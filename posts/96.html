<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/96" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/96" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="애플 인텔리전스, 혹은 단순한 실용주의" href="/post/2024-06-19-AppleIntelligenceorsimplypragmatism"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="애플 인텔리전스, 혹은 단순한 실용주의" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-AppleIntelligenceorsimplypragmatism_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="애플 인텔리전스, 혹은 단순한 실용주의" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">애플 인텔리전스, 혹은 단순한 실용주의</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="애플이 OpenAI에 전쟁 선포했다고" href="/post/2024-06-19-DidApplejustdeclarewaronOpenAI"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="애플이 OpenAI에 전쟁 선포했다고" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="애플이 OpenAI에 전쟁 선포했다고" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">애플이 OpenAI에 전쟁 선포했다고</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="딥러닝 그림으로 쉽게 이해하기, 제4부 순환 신경망" href="/post/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="딥러닝 그림으로 쉽게 이해하기, 제4부 순환 신경망" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="딥러닝 그림으로 쉽게 이해하기, 제4부 순환 신경망" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">딥러닝 그림으로 쉽게 이해하기, 제4부 순환 신경망</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">17<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LLM을 위한 지시어 파인 튜닝에 대한 포괄적인 소개" href="/post/2024-06-19-AComprehensiveIntroductiontoInstructionFine-TuningforLLMs"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LLM을 위한 지시어 파인 튜닝에 대한 포괄적인 소개" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-AComprehensiveIntroductiontoInstructionFine-TuningforLLMs_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LLM을 위한 지시어 파인 튜닝에 대한 포괄적인 소개" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">LLM을 위한 지시어 파인 튜닝에 대한 포괄적인 소개</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="작은 시간 혼합기 TTM IBM의 강력한 제로샷 예측 모델" href="/post/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="작은 시간 혼합기 TTM IBM의 강력한 제로샷 예측 모델" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="작은 시간 혼합기 TTM IBM의 강력한 제로샷 예측 모델" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">작은 시간 혼합기 TTM IBM의 강력한 제로샷 예측 모델</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="MLX로부터 GPT를 처음부터 만들어보기" href="/post/2024-06-19-GPTfromScratchwithMLX"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="MLX로부터 GPT를 처음부터 만들어보기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-GPTfromScratchwithMLX_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="MLX로부터 GPT를 처음부터 만들어보기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">MLX로부터 GPT를 처음부터 만들어보기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">41<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LSTM 구조를 사용하여 설명하는 어텐션" href="/post/2024-06-19-AttentionexplainedusingLSTMarchitecture"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LSTM 구조를 사용하여 설명하는 어텐션" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LSTM 구조를 사용하여 설명하는 어텐션" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">LSTM 구조를 사용하여 설명하는 어텐션</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AI 사용 사례는 근본적으로 다릅니다" href="/post/2024-06-19-AIUseCasesareFundamentallyDifferent"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AI 사용 사례는 근본적으로 다릅니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-AIUseCasesareFundamentallyDifferent_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AI 사용 사례는 근본적으로 다릅니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">AI 사용 사례는 근본적으로 다릅니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="고급 계산을 위한 클러스터 컴퓨터 활용 포괄적 가이드" href="/post/2024-06-19-UtilisingaClusterComputerforAdvancedComputationsAComprehensiveGuide"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="고급 계산을 위한 클러스터 컴퓨터 활용 포괄적 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-UtilisingaClusterComputerforAdvancedComputationsAComprehensiveGuide_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="고급 계산을 위한 클러스터 컴퓨터 활용 포괄적 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">고급 계산을 위한 클러스터 컴퓨터 활용 포괄적 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="신기한 여행 스탠포드의 매직 월드 크리에이터" href="/post/2024-06-19-WonderJourneyStanfordsMagicalWorldCreator"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="신기한 여행 스탠포드의 매직 월드 크리에이터" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-WonderJourneyStanfordsMagicalWorldCreator_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="신기한 여행 스탠포드의 매직 월드 크리에이터" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">신기한 여행 스탠포드의 매직 월드 크리에이터</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/81">81</a><a class="link" href="/posts/82">82</a><a class="link" href="/posts/83">83</a><a class="link" href="/posts/84">84</a><a class="link" href="/posts/85">85</a><a class="link" href="/posts/86">86</a><a class="link" href="/posts/87">87</a><a class="link" href="/posts/88">88</a><a class="link" href="/posts/89">89</a><a class="link" href="/posts/90">90</a><a class="link" href="/posts/91">91</a><a class="link" href="/posts/92">92</a><a class="link" href="/posts/93">93</a><a class="link" href="/posts/94">94</a><a class="link" href="/posts/95">95</a><a class="link posts_-active__YVJEi" href="/posts/96">96</a><a class="link" href="/posts/97">97</a><a class="link" href="/posts/98">98</a><a class="link" href="/posts/99">99</a><a class="link" href="/posts/100">100</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"애플 인텔리전스, 혹은 단순한 실용주의","description":"","date":"2024-06-19 03:19","slug":"2024-06-19-AppleIntelligenceorsimplypragmatism","content":"\n\n![ScreenShot](/assets/img/2024-06-19-AppleIntelligenceorsimplypragmatism_0.png)\n\n회사의 월드와이드 개발자 회의(WWDC)에서의 개회 발표는 제품에 AI를 통합하지 못한 실패에 집중했지만 이를 공식적으로 인정하지 않으려고 했습니다. 본질적으로 필요에 의한 덕목을 만드는 시도로, 그것은 빅테크 세계에서 쉽지 않습니다.\n\nApple의 기본적인 문제는 개인 정보를 존중하는 정책으로 이미지와 차별화 측면에서 이용하고 있는 점 때문에, Meta, Google, Microsoft 등 다른 기업들보다 운영 데이터가 훨씬 적다는 것입니다. 이미 2015년에 Apple의 개인 정보 보호 정책이 일부 데이터 과학자들이 회사의 취업 제안을 거절하고, 대신 다른 기업으로 가서 더 많은 자료를 활용할 수 있는 선택을 할 정도로 영향을 미치고 있다는 보도가 있었습니다. 그 이후로 상황은 변하지 않았지만, 한 가지 문제가 추가되었습니다: 매우 효율이 낮은 생성 알고리즘이 도입되어 데이터 요구량이 급증하게 되어 회사의 문제를 악화시키고 있습니다.\n\n이렇게 보았을 때, Apple은 어떻게 해야 할까요? 기본적으로 브랜드의 A와 놀아야 하고, 자사 모델을 Apple Intelligence로 명명하여 \"우리 같은 이들을 위한 AI\"로 위치시켜야 합니다. 다른 기술에 관심을 가지기 귀찮은 사람들을 위한 AI, 즉 몇 가지 기본 기능을 원하는 사람들을 위한 AI로 말이죠. 그리고 무엇보다도, 다른 브랜드의 기기를 소유한 사람들에게 열등함을 느끼지 말아야 합니다. 지루하고 실용적일지 모르지만... 그것이 반드시 나쁜 것은 아닙니다. Apple만이 할 수 있는 것처럼, 회사의 플랫폼을 끝없이 선보여서 참가자들을 피로하게 만든 후 남았습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n애플의 솔루션은 기본적인 AI로, 기기에서 개인 정보를 침해하지 않고 작동합니다. 기본적으로 간단한 기계 학습입니다. 하지만 iPhone의 프로세서가 그리 강력하지 않기 때문에, 특정 쿼리에 대한 두 번째 수준이 애플 서버에서 작동합니다. 애플 실리콘이 사용되며 애플 맵스의 경우처럼 쿼리를 수신하고 답변한 후 십 분 후에 삭제됩니다. 귀하의 데이터는 저장되지 않으며 회사 자체에서 액세스할 수 없습니다.\n\n이 모든 것은 매우 잘되고 있는데, Nvidia의 강력한 칩을 사용하지 않을 것이라고 결정한 회사가 경쟁 업체의 알고리즘 성능에 견줄만한 성과를 내지 못할 수 있다는 방법은 없습니다. 그에 대한 대응으로 애플 사용자들은 OpenAI의 수요에 따라 ChatGPT 4o에 액세스할 수 있게 되었습니다. 이를 통해 OpenAI에 상당한 수익을 창출하고 산업 리더로 올라설 수 있는 발판을 제공합니다. 사용자들이 요청을 하면, 그들의 기기는 그 정보가 애플의 벽을 넘어가게 된다는 경고를 보내며, 그 결과 OpenAI가 정보를 활용하는 용도에 노출되었다는 응답을 받게 될 것이지만 그냥 ChatGPT 앱을 열어서 프롬프트를 입력한 것과 유사한 답변을 받을 수 있습니다. 게다가, 애플은 사용자가 이를 활용하는 회사들과의 협정에 도달할 때 추가 알고리즘을 제공할 의사가 있다고 밝혔습니다. 그러니 일론 머스크가 이를 좋아하지 않는다고 해도 무슨 상관이죠?\n\n좋은 솔루션일까요? 아니죠, 실용주의입니다. 노인 시리의 이미지를 조금 꾸미는 수준으로 나아가 더 이상 어리석게 보이지 않도록 하고 (다행히 회사에게는 아직도 Alexa와 같이 더 한정적인 대안이 있습니다), 우리는 앱 간 작업 가능성을 구축합니다. 이는 사용자와 애플리케이션 개발자 모두에게 이득이 되는데 (WWDC임을 잊지 말아주세요, 그들의 대상은 그들이기 때문입니다), 우리는 어떤 앱이든 합리적인 AI에 액세스할 수 있는 플랫폼을 구축할 수 있습니다. 그것은 가치 제안을 향상시킬 수 있는 AI를 쉽게 액세스할 수 있도록 하는 API와 개발 도구로 이루어진 것입니다. 그리고 부차적으로 사용자들은 이러한 기능에 액세스하기 위해 기기를 업데이트하는 것을 권장받게 됩니다.\n\n한계를 이점으로 전환하는 측면에서 그리 나쁜 움직임은 아니나 다소 너무 자명한 것입니다. 그동안 애플은 미래의 AI 개발이 조금 더 지능적이되어야 하며, 학습의 한계를 해결하기 위해 엄청난 양의 데이터 대신 더 효율적인 모델을 선택하는 것을 기대하고 있습니다. 일단은 애플은 데이터 전쟁의 구경꾼일 뿐입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 마크다운 형식으로 표 태그를 변경해야 합니다.","ogImage":{"url":"/assets/img/2024-06-19-AppleIntelligenceorsimplypragmatism_0.png"},"coverImage":"/assets/img/2024-06-19-AppleIntelligenceorsimplypragmatism_0.png","tag":["Tech"],"readingTime":3},{"title":"애플이 OpenAI에 전쟁 선포했다고","description":"","date":"2024-06-19 03:18","slug":"2024-06-19-DidApplejustdeclarewaronOpenAI","content":"\n\n애플의 역사는 핵심 경쟁사를 생태계에 통합한 뒤 결국 그들을 앞질러서나 제거하는 양상을 보여줍니다.\n\nMac OS 하이 시에라(2017년)에서 \"메일, 연락처, 캘린더, 메시지 및 다른 앱과 함께 사용할\" \"인터넷 계정\"을 추가하는 인터페이스를 고려해보세요:\n\n![이미지](/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_0.png)\n\n일곱 년 후에는 이 패널을 서드파티 \"시리 소스\"로 대체하여 Apple Intelligence의 개인화된 유틸리티를 구동할 수 있다고 상상할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n실제로 Siri는 이미 사용하는 애플리케이션에서 \"학습\"할 수 있습니다:\n\n![image1](/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_1.png)\n\nmacOS Sequoia와 iOS 18이 출시되면 Siri를 통해 ChatGPT, Gemini 또는 다른 모델에 질문을 전달할 수 있을 것입니다. 아마도 Apple이 몇 년 전에 \"인터넷 계정\" 제공업체들과 유사한 방식으로 통합할 것으로 예상됩니다.\n\n![image2](/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n맥OS 소노마( macOS Sonoma)에서는 Twitter, Facebook 및 LinkedIn이 추가할 수있는 인터넷 계정에서 누락되어 있습니다. 아마도 메시지, 연락처 및 캘린더의 양방향 동기화가 중단된 통합이나 API 변경으로 이것이 발생한 것일 수 있습니다.\n\n그럼에도 불구하고 이것은 애플이 경쟁사를 OS 수준의 통합에 통합시켰다가 결국은 그것들을 오래된 기술로 만드는 추세를 보여줍니다.\n\nTwitter (지금은 X)와 LinkedIn의 경우, 공유 기능이 시스템 전체 기능으로 진화했습니다. 애플은 케임브리지 아날리티카 사건 이후 Facebook으로부터 거리를 둘 필요가 있었는데, 연락처 통합(지금은 연락처)을 계속 유지하면 신뢰성이 훼손될 것이었기 때문입니다. 이 전례는 애플이 AI 서비스의 통합에 대해 비슷한 방식을 채택할 수 있음을 시사합니다. 초기에는 더 열린 실험적인 방식으로 시작하여 뒤이어 자사의 소유 제공을 우선시하는 방식으로 전환할 수 있습니다.\n\n따라서 애플은 AI 경주를 개인 정보 보호를 고려한 환경에서 진행하고 있지만, 애플은 사용자 데이터 및 경험에 대한 통제를 유지하기 위해 필요할 때 OpenAI의 존재를 애플 운영 체제에서 사라지게 할 계획이라고 의심하지 않습니다. 애플 인텔리전스를 자사의 \"프라이빗 클라우드 컴퓨팅\"과 결합하면 ChatGPT 및 이 목록에 나타날 수 있는 다른 서비스를 필요할 때마다 떨어뜨릴 수 있는 능력을 제공받게 될 것이며, 이를 트위터, 페이스북 및 링크드인과 같은 서비스에 대해 한 것처럼 할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image 1](/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_3.png)\n\nWWDC 키노트에서, 멋진 크레이그 페데리기가 Siri의 지연 때문에 ChatGPT와 주목을 나눠야 한다는 사실에 자신을 젠더고 있을 것으로 의심했습니다 (상당히 큰 선두를 잃은 후).\n\n![image 2](/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_4.png)\n\n그럼에도 불구하고, 페데리기는 동시에 애플에 시간을 벌려주었습니다, 왜냐하면 이미 경쟁사의 능력을 흡수하는 방안이 이미 마련되어 있기 때문입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그게 바로 애플 인텔리전스가 의미하는 바에요.","ogImage":{"url":"/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_0.png"},"coverImage":"/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_0.png","tag":["Tech"],"readingTime":2},{"title":"딥러닝 그림으로 쉽게 이해하기, 제4부 순환 신경망","description":"","date":"2024-06-19 03:14","slug":"2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks","content":"\n\n저희의 그림으로 보여주는 딥러닝 여정 Part 4에 오신 것을 환영합니다! 오늘은 순환 신경망(Recurrent Neural Networks)에 대해 자세히 살펴보겠습니다. 입력, 출력, 활성화 함수 같은 익숙한 개념들에 대해 이야기할 건데, 조금씩 다른 면을 발견할 거에요. 그리고 이번이 여정의 첫 스탑이라면, 특히 Part 1과 Part 2를 읽어보시길 추천드립니다.\n\n순환 신경망(RNN)은 이전 상태에 의존하는 다음 위치에 영향을 받는 순서 기반 문제를 처리하기 위해 명시적으로 설계된 독특한 모델입니다.\n\n간단한 MIT 강의 예시로, 시간 tn에 특정 지점에 있는 공을 상상해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 우리가 볼의 방향을 예측하라는 요청을 받았다면, 추가 정보 없이는 추측의 일입니다. 볼은 아무 방향으로 움직일 수 있습니다.\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_1.png)\n\n하지만 만약 볼의 이전 위치에 대한 데이터가 제공된다면 어떨까요?\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 우리는 공이 오른쪽으로 계속 움직일 것이라고 자신 있게 예측할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_3.png)\n\n이 예측 시나리오는 우리가 순차적 문제라고 부르는 것입니다 — 여기서 답은 이전 데이터에 강력하게 영향을 받습니다. 이러한 순차적 문제는 모든 곳에 있으며, 과거 온도 데이터에 기반한 내일의 온도 예측부터 감정 분석, 명명된 개체 인식, 기계 번역 및 음성 인식을 포함한 다양한 언어 모델에 이르기까지 다양합니다. 오늘은 감정 탐지에 초점을 맞추어 시퀀스 기반 문제의 간단한 예제를 살펴보겠습니다.\n\n감정 탐지에서는 텍스트 조각을 가져와 해당 텍스트가 긍정적인지 부정적인지 여부를 결정합니다. 오늘은 영화 리뷰를 입력으로 받아 그것이 긍정적인지 아닌지를 예측하는 RNN을 구축할 것입니다. 따라서 이 영화 리뷰를 고려해 봅시다...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_4.png)\n\n우리의 신경망이 이것이 긍정적인 감정을 가지고 있다고 예측하길 원합니다.\n\n이것은 간단한 분류 문제처럼 들릴 수 있지만, 여기서 표준 신경망이 직면한 두 가지 주요 도전 과제가 있습니다.\n\n첫째, 우리는 가변 입력 길이를 다루고 있습니다. 표준 신경망은 길이가 다른 입력을 처리하는 데 어려움을 겪습니다. 예를 들어, 만약 우리가 세 단어로 이루어진 영화 리뷰로 신경망을 훈련한다면, 우리의 입력 크기는 세 개로 고정될 것입니다. 그러나 더 긴 리뷰를 입력하고 싶다면 어떻게 해야 할까요?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_5.png)\n\n위의 리뷰를 12개의 입력값으로 처리하는 데 어려워하고 처리하지 못할 수 있습니다. 지난 글들과 달리 입력값의 개수가 고정된 게 아닙니다(아이스크림 수익 모델은 온도와 요일 2개의 입력값이 있었습니다). 이 경우에는 모델이 유연하게 동작하고 얼마든지 많은 단어들에 적응할 수 있어야 합니다.\n\n또한 연속적인 입력값을 가지고 있습니다. 일반적인 신경망은 입력값의 방향성을 완전히 이해하지 못하는데, 이것은 여기서 중요합니다. 두 문장이 정확히 같은 단어를 포함할지라도 순서가 다르면 완전히 반대의 의미를 가질 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_6.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 도전에 직면할 때, 우리는 동적으로 입력을 순차적으로 처리할 수 있는 방법이 필요하다. 여기서 RNN이 빛을 발한다.\n\n이 문제에 접근하는 방법은 먼저 리뷰의 첫 단어 \"that\"을 처리하는 것이다:\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_7.png)\n\n그런 다음 이 정보를 사용하여 두 번째 단어 \"was\"를 처리한다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 위의 모든 정보를 사용하여 마지막 단어 \"현저한(phenomenal)\"을 처리하고 리뷰의 감정에 대한 예측을 제공해보겠습니다:\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_9.png)\n\n신경망을 구성하기 전에 입력에 대해 논의해야 합니다. 신경망에 입력되는 값은 숫자여야 합니다. 그러나 여기서의 입력값은 단어이므로 이러한 단어를 숫자로 변환해야 합니다. 이를 수행하는 여러 가지 방법이 있지만, 오늘은 기본적인 방법을 사용하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금은 10,000개의 단어로 이루어진 큰 사전이 있다고 상상해 봅시다. 우리는 (순진하게) 리뷰에 나오는 어떤 단어라도 이 10,000단어 사전 안에서 찾을 수 있다고 가정할 것입니다. 각 단어는 해당하는 숫자로 매핑되어 있습니다.\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_10.png)\n\n단어 \"that\"을 숫자들의 묶음으로 변환하려면, \"that\"이 매핑된 숫자를 확인해야 합니다...\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_11.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것을 10,000 개의 0으로 이루어진 행렬로 표현하되 8600번째 요소만 1인 형태로 나타내면 됩니다:\n\n![Matrix](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_12.png)\n\n비슷하게, 다음 두 단어 \"was\" (사전에서 9680번째 단어)와 \"phenomenal\" (사전에서 4242번째 단어)의 수치적 표현은 다음과 같습니다:\n\n![Matrix](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_13.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 우리는 단어를 신경망 친화적인 입력으로 변환하는 방법을 알아봤습니다.\n\n이제 우리의 주의를 신경망의 디자인으로 돌려봅시다. 간단히 설명하기 위해, 네트워크가 10,000개의 입력(= 1단어), 하나의 뉴런으로 이루어진 단일 은닉층 및 하나의 출력 뉴런을 가정해봅시다.\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_14.png)\n\n물론, 이것이 완전히 훈련된 신경망인 경우, 각 입력마다 연관된 가중치가 있고 뉴런들은 편향(bias) 항을 가지게 될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![신규 이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_15.png)\n\n이 네트워크에서 입력 가중치는 i가 입력을 나타내는 곳에 wᵢ로 표시됩니다. 숨겨진 레이어 뉴런의 편향 항은 bₕ로 나타냅니다. 숨겨진 레이어와 출력 뉴런을 연결하는 가중치는 wₕᵧ입니다. 마지막으로, 출력 뉴런의 편향은 y가 결과를 나타내므로 bᵧ로 표시됩니다.\n\n우리는 활성화 함수로 숨겨진 뉴런에 대해 쌍곡선 탄젠트 함수 (tanh)를 사용할 것입니다.\n\n![신규 이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_16.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n첫 번째 기사에서 다룬 내용을 다시 상기해보자면, tanh 함수는 입력값을 받아 -1부터 1 사이의 출력값을 생성합니다. 큰 양수 입력은 1에 가까워지고, 큰 음수 입력은 -1에 가까워집니다.\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_17.png)\n\n텍스트의 감정을 판단하기 위해, 우리는 출력 뉴런에서 시그모이드 활성화 함수를 사용할 수 있습니다. 이 함수는 숨겨진 레이어에서 출력을 받아 긍정적 감정의 확률을 나타내는 0부터 1까지의 값을 출력합니다. 1에 가까운 예측은 긍정적인 리뷰를 나타내며, 0에 가까운 예측은 긍정적이지 않을 확률이 높다는 것을 시사합니다.\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_18.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 활성화 함수들을 사용하면, 우리의 신경망은 다음과 같이 나타납니다:\n\n![neural network](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_19.png)\n\n이 신경망은 텍스트 입력을 받아 해당 텍스트가 긍정적인 감정을 가질 확률을 예측합니다. 위 예시에서, 신경망은 입력으로 \"that\"을 처리하고 이것이 긍정적인 경우일 확률을 예측합니다. 솔직히 말해서, \"that\"이라는 단어 자체로는 감정을 예측하는 데 큰 힌트를 주지는 않습니다. 이제 다음 단어를 신경망에 어떻게 통합할지 알아야 합니다. 이것이 순환 신경망의 순환적 측면이 작용하고, 기본 구조가 수정되는 시기입니다.\n\n리뷰의 두 번째 단어 \"was\"를 입력으로 넣기 위해, 위의 신경망을 정확하게 복사하여 새로 만듭니다. 그러나, 입력으로 \"that\" 대신 \"was\"를 사용합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Deep Learning Illustrated Part 4 Recurrent Neural Networks 20](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_20.png)\n\n앞 단어인 \"this\"에서의 정보도 이 신경망에서 사용하려고 합니다. 따라서, 이전 신경망의 은닉층에서의 출력을 가져와 현재 신경망의 은닉층으로 전달합니다:\n\n![Deep Learning Illustrated Part 4 Recurrent Neural Networks 21](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_21.png)\n\n이건 중요한 단계니까 천천히 살펴봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n첫 번째 기사에서 우리는 각 뉴런의 처리가 두 단계로 이루어진다는 것을 배웠어요: 합산과 활성화 함수 (이 용어가 무엇을 의미하는지 잘 모르겠다면 첫 번째 기사를 읽어보세요). 이러한 과정이 첫 번째 신경망에서 어떻게 이루어지는지 살펴봅시다.\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_22.png)\n\n첫 번째 신경망의 은닉층 뉴런에서 첫 번째 단계는 합산입니다:\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_23.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 입력값들을 각각의 가중치로 곱하고 편향 항을 모든 곱의 합에 더합니다:\n\n![equation1](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_24.png)\n\n이 방정식을 단순화하기 위해 입력 가중치를 wₓ로, 입력값을 x로 나타냅시다:\n\n![equation2](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_25.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그 다음, 2단계에서는 이 합계를 활성화 함수 tanh를 통해 전달합니다:\n\n![Image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_26.png)\n\n![Image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_27.png)\n\n이로써 첫 번째 신경망의 은닉층에서 출력 h₁을 생성합니다. 여기서 두 가지 옵션이 있습니다. h1을 출력 뉴론으로 전달하거나 다음 신경망의 은닉층으로 전달할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n(option 1) 만약 \"that\"에 대한 감성 예측을 하려면, h₁을 가져와서 출력 뉴런에 전달할 수 있습니다:\n\n![image1](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_28.png)\n\n출력 뉴런에 대해서는 합산 단계를 진행합니다...\n\n![image2](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_29.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e\u003cimg src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_30.png\" /\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n…그리고 이 합계에 시그모이드 함수를 적용합니다…\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e\u003cimg src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_31.png\" /\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n…이것으로 우리가 예측한 긍정적인 감정 값이 나옵니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Image 1](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_32.png)\n\nSo, y₁_hat here shows us the predicted probability that \"that\" has a positive sentiment.\n\nHowever, that's not what we aim for. Instead of passing h₁ to the output neuron, we transfer this information to the next neural network in the following way:\n\n![Image 2](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_33.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n신경망의 다른 부분과 마찬가지로 우리는 한 숨김층에서 다른 숨김층으로의 입력에 대한 입력 가중치 wₕₕ를 가지고 있습니다. 숨김층은 h₁을 h₁과 wₕₕ의 곱을 합산 단계에 추가함으로써 통합합니다. 따라서 두 번째 신경망 뉴런의 숨김층의 업데이트된 합산 단계는 다음과 같습니다:\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_34.png)\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_35.png)\n\n그런 다음이 합산은 tanh 함수를 통과합니다...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 두 번째 신경망의 은닉 레이어에서 출력되는 h₂를 생성합니다:\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_37.png)\n\n여기서 다시, h₂를 출력 뉴런을 통해 전달하여 감성 예측을 얻을 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_38.png)\n\n![Image 2](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_39.png)\n\n여기서, y₂_hat은 \"그것이\"가 긍정적인 감정을 가졌을 확률을 예측합니다.\n\n하지만 리뷰는 여기서 끝나지 않는다는 것을 알고 있습니다. 그래서 이전의 숨겨진 레이어 출력 값을 현재의 숨겨진 레이어에 전달하여 이 프로세스를 한 번 더 복제할 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_40.png)\n\nWe process the hidden layer neuron...\n\n![Image 2](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_41.png)\n\n...to an output, h₃:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 리뷰에서 마지막 단어이자 마지막 입력이기 때문에, 우리는 이 데이터를 외부 뉴런에 전달합니다...\n\n아래는 리뷰에서 마지막 단어이자 마지막 입력이기 때문에, 우리는 이 데이터를 외부 뉴런에 전달합니다...\n\n... 그리고 이를 통해 감정의 최종 예측을 제공합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_44.png\" /\u003e\n\n이 y₃_hat은 우리가 원하는 영화 리뷰의 감성으로, 처음에 그려 놓은 것을 얻는 방법입니다!\n\n\u003cimg src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_45.png\" /\u003e\n\n## 형식적 표현\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 다이어그램을 자세히 설명하면 다음과 같습니다:\n\n![다이어그램](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_46.png)\n\n각 단계마다 입력 x가 숨겨진 레이어를 통해 흐르고 출력 h를 생성합니다. 이 출력은 다음 신경망의 숨겨진 레이어로 이동하거나 감정 예측인 y_hat으로 이어집니다. 각 단계에는 가중치(weight)와 편향(bias) 용어가 포함되어 있습니다(다이어그램에는 편향이 표시되지 않음). 강조할 중요한 점은 모든 숨겨진 레이어를 하나의 조밀한 상자로 통합하고 있다는 것입니다. 모델에는 하나의 숨겨진 레이어와 단일 뉴런이 있는 것만 포함되어 있지만, 더 복잡한 모델에는 여러 숨겨진 레이어와 다수의 뉴런이 포함될 수 있고, 이러한 모든 요소가 이 상자(숨겨진 상태)로 압축됩니다. 이 숨겨진 상태는 숨겨진 레이어의 추상적인 개념을 담고 있습니다.\n\n본질적으로, 이것은 이 신경망의 단순화된 버전입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![딥러닝 일러스트레이티드 파트 4: 순환 신경망 (RNN)](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_47.png)\n\n이미지에서도 볼 수 있듯이 이 과정을 단순화한 다이어그램으로 표현할 수 있습니다:\n\n![딥러닝 일러스트레이티드 파트 4: 순환 신경망 (RNN)](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_48.png)\n\n이 프로세스의 본질은 출력값을 은닉층으로 순환해서 되돌린다는 점이며, 이것이 왜 순환 신경망이라고 불리는지에 대한 이유입니다. 이는 종종 교과서에서 신경망이 어떻게 표현되는지 보여주는 방식입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수학적으로 이 문제를 두 가지 기본 방정식으로 요약할 수 있어요:\n\n\n![equation1](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_49.png)\n\n\n첫 번째 방정식은 숨겨진 상태 내에서 발생하는 전체 선형 변환을 포함해요. 이 경우, 이 변환은 개별 뉴런 내에서의 tanh 활성화 함수예요. 두 번째 방정식은 출력 층에서 발생하는 변환을 나타내며, 이는 저희 예시에서 시그모이드 활성화 함수에 해당돼요.\n\n\n![equation2](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_50.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# RNN이 해결하는 문제 유형\n\n## 많은 것들을 하나로\n\n우리는 이전에 여러 입력(우리의 경우에는 리뷰 안의 모든 단어)이 RNN에 공급되는 시나리오를 논의했습니다. 그러면 RNN은 리뷰의 감정을 나타내는 단일 출력을 생성합니다. 각 단계마다 출력을 가질 수 있지만, 우리의 주된 관심사는 최종 출력에 있습니다. 왜냐하면 이는 전체 리뷰의 감정을 담고 있기 때문입니다.\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_51.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다른 예는 텍스트 완성입니다. 단어 문자열을 제공하면 RNN이 다음 단어를 예측하도록 원합니다.\n\n\n\u003cimg src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_52.png\" /\u003e\n\n\n## One-To-Many\n\n\n\u003cimg src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_53.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n원 대 다 문제의 고전적인 예는 이미지 캡션입니다. 여기서 하나의 입력은 이미지이고 출력은 여러 단어로 구성된 캡션이 됩니다.\n\n## 다 대 다\n\n![RNN Example](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_54.png)\n\n이 유형의 RNN은 기계 번역과 같은 작업에 사용됩니다. 예를 들어 영어 문장을 힌디어로 번역하는 작업입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 단점\n\n이제 RNN이 작동하는 방식을 자세히 살펴보았으니, 왜 그들이 널리 사용되지 않는지 살펴볼 가치가 있습니다 (스토리 전환!). 잠재력이 있음에도 불구하고, RNN은 특히 Vanishing Gradient Problem이라고 하는 것 때문에 교육 과정 중에 중요한 도전에 직면합니다. 이 문제는 RNN을 더 많이 펼치면서 더욱 악화되며, 결국 교육 과정을 복잡하게 만듭니다.\n\n이상적인 세상에서는 RNN이 현재 단계 입력과 이전 단계의 입력을 모두 동등하게 고려하는 것을 원합니다: \n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_55.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 실제로는 다음과 같이 보입니다:\n\n`\u003cimg src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_56.png\" /\u003e`\n\n각 단계는 약간 전 단계를 잊어버리는 경향이 있어서, 그 결과 단기 기억 문제인 사라지는 기울기 문제가 발생합니다. RNN이 더 많은 단계를 처리할수록, 이전 단계에서의 정보를 유지하는 데 어려움을 겪을 수 있습니다.\n\n입력이 세 개인 경우에는 이 문제가 그리 두드러지지 않습니다. 그렇다면 입력이 여섯 개인 경우는 어떨까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n첫 두 단계에서의 정보가 마지막 단계에서는 거의 없음을 발견했습니다. 이는 중요한 문제입니다.\n\n다음은 텍스트 완성 작업을 사용하여 이 점을 설명하는 예시입니다. 이 문장을 완성하는 것에 성공할 수 있는 RNN이 있습니다.\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_57.png)\n\n그러나 단어가 추가될수록 RNN은 다음 단어를 정확하게 예측하기 어려워 질 수 있습니다. 처음 단어에서 예측해야 할 단어 사이의 거리가 늘어나면서 RNN이 초기 단어들이 제공하는 문맥을 잊어버리기도 할 수 있기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_58.png\" /\u003e\n\n이것은 RNN이 이론상으로 훌륭해 보이지만 실제로는 종종 부족하다는 사실을 강조합니다. 단기 기억 문제를 해결하기 위해 우리는 Long Short-Term Memory (LSTM) 네트워크라고 불리는 특수 유형의 신경망을 사용합니다. 하지만 그것은 다음 파트에서 다루도록 하겠습니다. 그러니 기대해 주세요!\n\n# 보너스: Softmax 활성화 함수\n\n이전에 우리는 감성 예측을 다루는 다른, 훨씬 더 나은 방법에 대해 이야기했습니다. 출력 뉴런에 대한 활성화 함수를 결정했을 때로 돌아가보겠습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Deep Learning Illustrated](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_59.png)\n\n하지만 이번에는 조금 다른 것에 집중합니다. 순환 요소를 제외하고 기본 신경망에 초점을 맞춰 봅시다. 이제 우리의 목표는 무엇일까요? 영화 리뷰 전체가 아닌 단일 입력 단어의 감성을 예측하는 것입니다.\n\n이전에, 우리의 예측 모델은 입력이 양수일 확률을 출력하도록 목표로 했습니다. 이를 위해 출력 뉴런에서 시그모이드 활성화 함수를 사용하여 이를 성취했습니다. 이 함수는 긍정적 감정의 가능성에 대한 확률 값을 생성합니다. 예를 들어, \"terrible\"라는 단어를 입력하면, 우리 모델은 이상적으로 긍정적인 가능성이 낮음을 나타내는 낮은 값을 출력할 것입니다.\n\n![Deep Learning Illustrated](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_60.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 다시 한번 생각해보면, 이 결과가 크게 만족스럽지는 않습니다. 긍정적 감정의 낮은 확률은 반드시 부정적이라는 것을 의미하는 것은 아닙니다. 입력이 중립적이었을 수도 있습니다. 그렇다면 이를 어떻게 개선할 수 있을까요?\n\n다음을 고려해보세요. 만약 영화 리뷰가 긍정적인지, 중립적인지, 부정적인지 알고 싶다면 어떻게 될까요?\n\n그래서, 입력이 긍정적인지 예측하는 확률을 반환하는 하나의 출력 뉴런 대신, 세 개의 출력 뉴런을 사용할 수 있습니다. 각각이 리뷰가 긍정적, 중립적, 부정적일 확률을 예측할 것입니다.\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_61.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 네트워크의 각 뉴런에 시그모이드 함수를 적용하여, 확률을 출력하기 위해 단일 출력 뉴런 네트워크에서 사용한 것과 동일한 원리를 적용할 수 있습니다.\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_62.png)\n\n각 뉴런은 각자에게 해당하는 확률 값을 출력할 것입니다:\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_63.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 문제가 있습니다: 확률이 올바르게 합산되지 않습니다 (0.1 + 0.2 + 0.85 != 1) 그래서 이것은 그다지 좋은 해결책이 아닙니다. 출력 뉴런 모두에 시그모이드 함수를 간단히 적용하는 것으로는 문제를 해결할 수 없습니다. 세 개의 출력 간에 이러한 확률을 정규화하는 방법을 찾아야 합니다.\n\n여기서 우리의 무기에 강력한 활성화 함수를 소개합니다 — 소프트맥스 활성화. 소프트맥스 활성화 함수를 사용하면 우리의 신경망은 새로운 모습을 취합니다:\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_64.png)\n\n처음에는 복잡해 보일 수 있지만, 소프트맥스 함수는 실제로 매우 직관적입니다. 간단히 말해 출력 뉴런에서 출력 값 (y_hat)을 가져와 정규화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 이 세 개의 출력 뉴런에 대해서는 어떤 활성화 함수도 사용하지 않아야 한다는 것이 중요합니다. 출력 (y_hats)은 합산 단계 직후에 직접 얻는 결과가 될 것입니다.\n\n우리는 이 y_hat 출력을 소프트맥스 공식을 통해 정규화합니다. 이 공식은 긍정적인 감정의 확률 예측을 제공해줍니다:\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_65.png)\n\n마찬가지로, 부정적인 결과와 중립 결과의 확률 예측을 얻을 수도 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같이 작동해 봅시다. 예를 들어, \"terrible\"가 입력으로 주어지면 다음과 같은 y_hat 값들이 나타날 것입니다:\n\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_66.png)\n\n\n그런 다음이 값을 가져와 softmax 공식에 대입하여 \"terrible\"라는 단어가 긍정적인 함축을 가질 확률을 계산할 수 있습니다.\n\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_67.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 의미는 감성 뉴런에서 출력된 결합된 결과를 사용하면 \"terrible\"이 긍정적인 감정을 가진다는 확률이 0.05인 것을 의미합니다.\n\n만약 입력이 중립적인지의 확률을 계산하려면 비슷한 공식을 사용하되 분자를 바꿔야 합니다. 따라서, \"terrible\"이 중립적인지의 가능성은 다음과 같습니다:\n\n![probability formula](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_68.png)\n\n그리고 \"terrible\"이 부정적이라고 예측되는 확률은:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_69.png\" /\u003e\n\n그러면 뭐, 이제 확률이 1에 합쳐져서 모델이 더 설명 가능하고 논리적입니다.\n\n그래서 우리가 신경망에게 \"“terrible”라는 단어가 부정적인 감정이 있는 확률이 얼마나 되냐?\"고 물으면, 꽤 직관적인 답을 받게 됩니다. \"terrible\"이라는 단어가 부정적인 감정이라는 것에 대해 확률적으로 85%가 있다고 자신있게 말합니다. 이것이 바로 softmax 활성화 함수의 매력입니다!\n\n오늘은 여기까지! 우리는 순환 신경망과 소프트맥스 활성화 함수라는 두 가지 큰 주제를 다뤘습니다. 이것들은 나중에 더 깊게 다룰 많은 고급 개념들의 기초입니다. 그러니 시간을 내어 천천히 생각해보시고, 계속 질문하거나 의견을 나누고 싶다면 언제든지 LinkedIn에서 연락 주시거나 shreya.statistics@gmail.com으로 이메일 보내 주세요!","ogImage":{"url":"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_0.png"},"coverImage":"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_0.png","tag":["Tech"],"readingTime":17},{"title":"LLM을 위한 지시어 파인 튜닝에 대한 포괄적인 소개","description":"","date":"2024-06-19 03:09","slug":"2024-06-19-AComprehensiveIntroductiontoInstructionFine-TuningforLLMs","content":"\n\n지시 튜닝은 큰 언어 모델(LLM)의 능력을 특정 지시를 따르도록 개선하기 위해 사용되는 과정입니다. InstructGPT의 작업은 먼저 지시 미세 조정에 대한 작업을 소개했습니다.\n\nInstructGPT는 인간 지시를 더 잘 따르도록 GPT-3를 미세 조정하여 학습되었습니다. 인간이 모델의 응답을 평가한 데이터셋에서 GPT-3를 조정하는 것은 ChatGPT를 만드는 방향으로 큰 발전이었습니다.\n\n이 문서에서는 기존 LLM의 성능을 향상시키기 위해 지시 미세 조정하는 과정과 결과를 배우게 됩니다. 또한 미세 조정한 LLM의 성능을 평가하고 기본 모델과의 개선 정도를 양적으로 측정할 수 있는 중요한 지표에 대해 알게 될 것입니다.\n\n![image](/assets/img/2024-06-19-AComprehensiveIntroductiontoInstructionFine-TuningforLLMs_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 목차:\n\n- 지시사항 프롬프트를 사용한 LLMs 세밀조정\n- 세밀조정 과정\n- 지시 데이터 세트 준비\n- 지시 세밀조정 프로세스\n- 평가 및 성능 측정 지표\n\n# 1. 지시 프롬프트를 활용한 LLMs 세밀조정\n\n큰 LLMs 및 GPT3와 같은 기본 모델들은 프롬프트에 포함된 지시사항을 식별하고 올바르게 zero-shot 추론을 수행할 수 있지만, 더 작은 LLMs와 같은 다른 모델들은 작업을 수행하지 못할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, \"불어로 이 문장을 번역해주세요: '안녕, 어떻게 지내세요?'\"라는 지시를 받으면 능력있는 LLM은 비슷한 번역 예시를 볼 필요 없이 올바른 번역 \"Bonjour, comment ça va?\"를 생성할 수 있습니다.\n\n그러나 더 작은 LLM들이나 덜 포괄적인 훈련 데이터를 갖고 있는 LLM들은 또는 더 복잡한 작업에 대해서 적절한 작업을 제대로 수행하기 위해 안내 없이 어려움을 겪을 수 있습니다. 이를 해결하기 위해 one-shot 및 few-shot 추론 기술이 사용되는데, 여기서 한 번 또는 몇 가지 예제가 모델이 작업을 이해하는 데 도움이 됩니다.\n\n예시: One-Shot 추론\n\n- 프롬프트: \"이 문장을 독일어로 번역해주세요: '좋은 아침.' 예시: '어떻게 지내세요?' - ` 'Wie geht es dir?'\"\"\n- 모델 출력: \"Guten Morgen.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 모델은 제공된 예시를 사용하여 \"Good morning\"을 올바르게 번역하는 방법을 추론합니다.\n\n예시: Few-Shot 추론\n\n- 프롬프트: \"다음 문장을 프랑스어로 번역하십시오: '안녕히 가세요.' 예시: '안녕' -` 'Bonjour'. '고맙습니다' -` 'Merci'.\"\n- 모델 출력: \"Au revoir.\"\n\n몇 가지 예시를 제공함으로써, 모델은 번역 작업에 대한 더 나은 이해를 얻고 정확한 결과를 내놓습니다. 파인 튜닝은 LLM의 가중치를 업데이트하기 위해 레이블이 지정된 예시를 사용하여 기본 모델을 더 학습시키는 솔루션을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2. 지시 사항 세부 조정 과정\n\n이전에 대비해서, 선행 교육에서는 LLM을 자기 지도 학습을 통해 거대한 양의 비구조화된 텍스트 데이터를 사용하여 훈련했지만, 지시 사항 세부 조정은 레이블이 지정된 예제 데이터 세트를 사용하여 LLM의 가중치를 업데이트하는 지도 학습 과정입니다.\n\n레이블이 지정된 예제는 프롬프트-완료 쌍이며, 세부 조정 프로세스는 모델의 훈련을 확장하여 특정 작업에 대한 좋은 완료를 생성할 수 있는 능력을 향상시킵니다.\n\n다양한 작업에 대한 지시 사항 세부 조정 예시:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 텍스트 분류:\n\n   - 작업: 영화 리뷰의 감정 분류\n   - 프롬프트: \"이 리뷰의 감정을 분류하세요: '나는 이 영화를 정말 좋아했어요! 처음부터 끝까지 멋있었어요.'\"\n   - 완료: \"감정: 긍정적\"\n\n2. 텍스트 요약:\n\n   - 작업: 기사 요약\n   - 프롬프트: \"다음 기사를 요약하세요: '주식 시장은 역대급 성장을 보여주었으며, 주요 지수가 사상 최고치를 기록했습니다. 투자자들은 경제의 회복에 대해 낙관적입니다.'\"\n   - 완료: \"요약: 경제 회복에 대한 낙관주의 속에서 주식 시장이 사상 최고치를 경신했습니다.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 번역:\n\n- 작업: 영어 문장을 프랑스어로 번역하십시오.\n- 프롬프트: “다음 문장을 프랑스어로 번역하십시오: ‘The weather is nice today.’”\n- 완료: “Le temps est agréable aujourd’hui.”\n\n4. 질의응답:\n\n- 작업: 주어진 텍스트를 기반으로 질문에 답하십시오.\n- 프롬프트: “다음 글을 읽고 질문에 답하십시오: ‘중국의 만리장성은 세계에서 가장 유명한 구조물 중 하나입니다. 침입으로부터 보호하기 위해 건설되었습니다.’ 질문: 만리장성은 왜 지어졌습니까?”\n- 완료: “만리장성은 침입으로부터 보호하기 위해 건설되었습니다.”\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5. Named Entity Recognition (NER):\n\n- Task: 명명된 Entity(개체)를 식별하고 분류합니다. 예를 들어, 사람, 조직, 위치 등\n- 지시문: \"다음 문장에서 Entity(개체)를 식별하고 분류하세요: 'Barack Obama was born in Hawaii and served as the President of the United States.'\"\n- 완성: \"Barack Obama: 사람, Hawaii: 위치, President of the United States: 직책\"\n\n지시서 파인튜닝은 특정 지시에 대한 모델의 반응을 보여주는 예시를 사용하여 다양한 작업에서 모델의 성능을 향상시키는 데 특히 좋습니다. 지시서 파인튜닝의 가장 중요한 장점 중 세 가지는 다음과 같습니다:\n\n- 작업별 전문 지식: 레이블이 지정된 예시를 통해 특정 작업에 대해 직접 학습함으로써 모델이 해당 작업에서 높은 능숙도를 갖게 됩니다.\n- 향상된 정확도: 파인튜닝은 모델이 훈련된 작업에 대한 정확도를 크게 향상시키며 명확한 지침과 예시를 통해 학습합니다.\n- 문맥 처리 효율: 파인튜닝 후에는 프롬프트 내에서 여러 예시를 요구하지 않아도 되므로, 문맥 창에서 다른 관련 정보를 위한 공간을 절약할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 3. 지시 데이터 세트 준비\n\n지시 미세 조정 작업의 한 가지 어려움은 많은 공개 데이터 세트가 내용은 풍부하지만 지시 프롬프트로 사용하기에 적합하게 구조화되어 있지 않다는 것입니다. 예를 들어, 언어 모델 사전 훈련에 사용되는 데이터 세트는 특정 지시나 프롬프트 없이 원시 텍스트 단락으로 구성될 수 있습니다.\n\n이러한 어려움을 해결하기 위해 연구원들과 개발자들은 지시 프롬프트 데이터 세트로 변환하기 위해 기존 데이터 세트를 변환하는 미리 정의된 템플릿을 포함하는 라이브러리와 도구를 선별해 왔습니다. 예를 들어, 지시 프롬프트 라이브러리 및 예시를 포함한 Template Libraries, 예를 들면:\n\n- Hugging Face의 NLP Datasets: Hugging Face는 자연어 처리 (NLP) 데이터 세트의 방대한 컬렉션을 제공하며, 이 중 많은 데이터 세트에 미리 정의된 프롬프트 템플릿이 함께 제공됩니다. 이 템플릿을 사용하면 사용자가 원시 데이터 세트를 지시 기반 프롬프트 형식으로 변환할 수 있습니다.\n- OpenAI의 GPT Prompt Engineering: OpenAI는 지시 엔지니어링을 위한 자원과 도구를 제공하며, 특정 작업에 맞춘 프롬프트 라이브러리를 포함합니다. 이러한 라이브러리는 분류, 텍스트 생성 및 요약과 같은 작업을 위한 사용 준비가 완료된 템플릿을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 라이브러리와 도구를 사용하면 지시용 데이터 세트를 만들 수 있습니다:\n\n- Amazon 제품 리뷰 데이터 세트: 개발자는 Amazon 제품 리뷰 데이터 세트를 활용하여 언어 모델을 감정 분석이나 제품 분류를 위해 세밀하게 조정할 수 있습니다. \"이 리뷰의 감정을 분류하십시오\" 또는 \"제품 평가를 예측하십시오\"와 같은 프롬프트 템플릿을 적용하여, 개발자는 원시 리뷰를 세부 조정을 위한 지시 프롬프트로 변환할 수 있습니다.\n- Stanford Sentiment Treebank (SST): SST는 감정(긍정적 또는 부정적)으로 분류된 영화 리뷰를 포함하는 데이터셋입니다. 적절한 프롬프트 템플릿을 사용하면 연구자들은 SST를 감정 분석 세밀 조정 작업용 지시 프롬프트 데이터 세트로 변환할 수 있습니다.\n- CNN/Daily Mail 데이터 세트: 이 데이터 세트는 뉴스 기사와 글머리 기사 요약이 짝지어진 것입니다. \"이 기사에 대한 요약 생성\"과 같은 프롬프트 템플릿을 활용하여 개발자는 텍스트 요약 세밀 조정용 지시 데이터 세트를 준비할 수 있습니다.\n- WMT 번역 작업 데이터 세트: WMT(기계 번역 워크샵)은 기계 번역 모델을 훈련하기 위한 데이터 세트를 제공합니다. \"이 문장을 프랑스어로 번역하십시오\"와 같은 프롬프트 템플릿을 사용하여 연구자는 번역 세밀 조정 작업용 지시 프롬프트를 생성할 수 있습니다.\n\n# 4. 지시 세밀 조정 프로세스\n\n지시 데이터 세트를 준비했다면, 이를 훈련, 검증 및 테스트 세트로 나눕니다. 세밀 조정 중에는 훈련 데이터 세트에서 프롬프트를 선택하고 LLM에 전달하여 완성본을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM 완성 결과를 교육 데이터에 지정된 응답과 비교하여 표준 교차 엔트로피 함수를 사용하여 손실을 계산하고 역전파를 통해 모델 가중치를 업데이트하십시오.\n\n모델의 성능을 향상시키기 위해 여러 배치의 프롬프트-완성 쌍을 여러 번의 epoch 동안 반복합니다.\n\n# 5. 평가 및 성능 지표\n\n표준 지도 학습과 마찬가지로, 보유 검증 데이터 세트를 사용하여 LLM 성능을 측정하는 별도의 평가 단계를 정의하여 검증 정확도를 얻습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n미세 조정을 완료한 후, 테스트 정확도를 얻기 위해 홀드아웃 테스트 데이터셋을 사용하여 최종 성능 평가를 수행하십시오. 이때, 이메일 보완보상평가(BLEU) 및 ROUGE(Recall-Oriented Understudy for Gisting Evaluation)은 긴 통역모델 (LLM) 지침 미세 조정을 평가하는 데 사용되는 인기있는 두 평가 지표 중 하나입니다.\n\n- BLEU (이중 언어 평가 보조) 스코어:\n\n- 정의: 한 언어에서 다른 언어로 기계 번역된 텍스트의 품질을 평가하기 위한 지표로, 이를 인간이 만든 번역과 비교합니다.\n- 예시: 번역 작업에서 모델이 \"The weather is nice today\"을 정확하게 \"Le temps est agréable aujourd’hui\"로 번역했을 때, 인간 번역과 비교하여 BLEU 스코어가 높게 나타납니다.\n\n2. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) 스코어:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 정의: 자동 요약 및 기계 번역을 평가하는 메트릭스 세트입니다. 이는 모델 출력물과 참조 텍스트 간의 n-gram의 중첩을 측정합니다.\n- 예시: 요약 작업의 경우, 높은 ROUGE 점수는 모델이 생성한 요약과 인간이 작성한 요약 간에 높은 중첩이 있음을 나타냅니다.\n\n세밀 조정 과정은 기반 모델의 새 버전을 만들어내며 이를 보통 가르침 모델이라고 합니다. 이는 당신이 관심 있는 작업에 더 적합한 모델입니다.\n\n지시 프롬프트로 세밀 조정하는 것이 오늘날 LLM을 세밀 조정하는 가장 흔한 방법입니다. 본 문서에서는 이 중요한 주제에 대해 간략히 소개되었으니 이제 손을 더럽히고 LLM을 조금 만지작거리며 조정해보는 것이 시간입니다.\n\n## 만약 이 문서를 좋아하셨고 저를 지원하고 싶으시다면, 확인해주십시오:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 👏 이 이야기에 박수를 보내주세요 (50번 클랩!) 이 기사가 주목받을 수 있도록 도와주세요\n- To Data \u0026 Beyond 뉴스레터를 구독해주세요\n- 제 Medium 계정을 팔로우해주세요\n- 📰 제 Medium 프로필에서 더 많은 콘텐츠를 확인해주세요\n- 🔔 팔로우하기: LinkedIn | Youtube | GitHub | Twitter\n\n## 제 뉴스레터 'To Data \u0026 Beyond'를 구독하여 제 글을 완전히 그리고 일찍 볼 수 있습니다:\n\n## 데이터 과학과 AI 분야에서 커리어를 시작하고 방향을 모를 때 도움이 필요하신가요? 저는 데이터 과학 멘토링 세션과 장기적 커리어 멘토링을 제공합니다:\n\n- 멘토링 세션: [링크](https://lnkd.in/dXeg3KPW)\n- 장기적 멘토링: [링크](https://lnkd.in/dtdUYBrM)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Image](/assets/img/2024-06-19-AComprehensiveIntroductiontoInstructionFine-TuningforLLMs_1.png)","ogImage":{"url":"/assets/img/2024-06-19-AComprehensiveIntroductiontoInstructionFine-TuningforLLMs_0.png"},"coverImage":"/assets/img/2024-06-19-AComprehensiveIntroductiontoInstructionFine-TuningforLLMs_0.png","tag":["Tech"],"readingTime":7},{"title":"작은 시간 혼합기 TTM IBM의 강력한 제로샷 예측 모델","description":"","date":"2024-06-19 03:07","slug":"2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM","content":"\n\n![이미지](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_0.png)\n\nLLM(Large Language Models)에 대한 최신 연구를 따라가보면 주로 두 가지 주요 접근 방식을 볼 수 있어요:\n\n첫째, 연구자들은 가능한 가장 큰 모델을 구축하는 데 주력합니다. 단어 예측을 통한 사전 학습은 성능 향상에 중요한 역할을 합니다(그리고 수백만 달러가 소비되는 곳이기도 합니다!).\n\n둘째, 연구자들은 양자화와 같은 기술을 사용하여 작고 빠른 모델을 만들어냅니다 — 강력한 일반적인 성능을 유지하면서요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 일부 작업에서 더 작은 모델이 훨씬 큰 모델보다 더 나은 성과를 내는 흥미로운 일이 발생합니다. 예를 들어, Llama 3-8B는 MMLU 작업에서 더 큰 Llama 2-70B보다 우수한 성과를 냈습니다!\n\nIBM에서 소개한 Tiny Time Mixers (TTM)[1]은 두 번째 접근 방식을 따릅니다. 더 큰 SOTA 모델 — MOIRAI를 포함하여 —을 능가하는 가벼운 모델로, M4 데이터셋에서 우수한 성과를 거둡니다. 게다가, 이는 오픈 소스입니다!\n\n이 기사에서는 다음을 논의합니다:\n\n- TTM의 아키텍처 및 기능.\n- TTM을 특별하게 만드는 혁신적인 기능.\n- 다른 모델과의 벤치마킹 결과를 비교한 결과.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시작해요!\n\n# Enter Tiny Time Mixer (TTM)\n\nTTM의 주요 특징은 다음과 같습니다:\n\n- Non-Transformer Architecture: TTM은 Attention 메커니즘을 사용하지 않기 때문에 매우 빠릅니다. 완전 연결된 NN 계층만 사용합니다.\n- TSMixer Foundation: TTM은 아키텍처에서 TSMixer[2] (IBM의 혁신적인 시계열 모델)을 활용합니다.\n- 다양한 입력: 다변량 예측이 가능한 TTM은 추가 채널, 외부 변수 및 알려진 미래 입력을 수용하여 예측 다양성을 향상시킵니다.\n- 빠르고 강력함: TTM은 Monash 데이터 세트의 244백만개 샘플로 사전 훈련되었으며, 6대의 A100 GPU를 사용하여 8시간 이내에 훈련되었습니다.\n- 우수한 제로샷 예측: TTM은 사전 훈련되어 있으며, 미처 본 적 없는 데이터에 대한 우수한 제로샷 예측을 수행하여 큰 SOTA 모델을 능가합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n중요한 사항:\n\n# TTM 혁신\n\nTTM은 여러 혁신적인 기능을 소개합니다:\n\n- 다중 수준 모델링: TTM은 먼저 채널 독립적 방식(일변량 시퀀스)으로 사전 훈련을 받은 후, 세밀 조정 중에 여러 변수 종속성을 학습하기 위해 교차 채널 혼합을 사용합니다.\n- 적응형 패치 적용: 단일 패치 길이 대신 TTM은 서로 다른 레이어 간에 여러 패치 길이를 학습합니다. 각 시계열이 특정 패치 길이에서 최적으로 작동하기 때문에 적응형 패치는 모델이 다양한 데이터에 대해 더 잘 일반화되도록 도와줍니다.\n- 해상도 접두사 튜닝: 다른 주파수(예: 주간, 일별 데이터)는 전통적인 시계열 모델에 어려운 부분입니다. TTM은 시계열 주파수를 인코딩하기 위한 추가 임베딩 레이어를 사용하여 모델이 신호의 주파수에 따라 정확하게 예측을 조건부로 설정할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Tiny Time Mixers — 아키텍처\n\nTSMixer은 TTM의 전신입니다. TSMixer는 견고한 모델이지만, 기본 모델로 사용하거나 외부 변수를 처리하는 데 사용할 수는 없습니다.\n\nTTM은 TSMixer를 구성 요소로 사용하여 새로운 기능을 도입함으로써, 저자들이 보지 못한 데이터에 대해 일반화된 비-트랜스포머 모델을 만들었습니다.\n\nTTM의 아키텍처는 그림 1에 나와 있습니다. 우리는 두 단계, 사전 훈련(왼쪽)과 파인튜닝(오른쪽)에 대해 설명할 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_1.png)\n\n**의미론적 지식**: sl=context_size, fl=forecasting_length, c = 입력 기능의 채널 수, c’= 예측 채널의 수.\n\n## 사전 훈련\n\n- 사전 훈련 중에는 모델이 단변량 시계열로만 학습됩니다.\n- 먼저 개별 시계열을 정규화합니다. 마지막 출력은 역정규화됩니다 (표준적인 방법).\n- 패칭은 시계열에서 널리 성공한 기술이며 여기서도 사용됩니다. 단변량 시퀀스를 크기가 pl인 n 패치로 나눕니다.\n- TTM 백본 모듈은 적응형 패칭을 적용하고 패치를 크기 p에서 hf로 사상합니다. TTM 백본은 TTM의 핵심이며 나중에 자세히 설명하겠습니다.\n- TTM 디코더는 TTM 백본과 동일한 아키텍처를 갖고 있지만 훨씬 작아서 매개변수가 80% 적습니다.\n- 예측 선형 헤드에는 1개의 완전 연결 계층이 있으며 최종 예측을 생성합니다 (그런 다음 역정규화됨).\n- MSE 손실은 예측 기간 fl 동안 계산됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Feat-Tuning\n\n- 여기서는 TTM 백본이 동결되어 있고 TTM 디코더 및 Forecast 선형 헤드의 가중치만 업데이트됩니다.\n- 우리는 소수 데이터만으로 학습하는 후속 예측(few-shot forecasting) 또는 전체 데이터셋을 사용하는 후속 예측(full-shot forecasting)을 수행할 수 있습니다.\n- Feat-Tuning 단계에서는 다변량 데이터셋을 사용할 수 있습니다. 이 경우 TTM 디코더에서 채널 혼합이 활성화됩니다.\n- 선택적으로, 미래의 알려진 변수를 모델링하기 위해 외생 혼합 블록(그림 1에 나와 있음)을 활성화할 수도 있습니다.\n\n# TTM 백본\n\nTTM의 핵심 구성 요소는 TTM 백본입니다. 이는 Resolution Prefix Tuning과 Adaptive Patching을 가능하게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 컴포넌트를 자세히 살펴보자면 그 기능을 이해할 수 있어요 (그림 2에 표시됨):\n\n![이미지](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_2.png)\n\n- 임베딩 레이어는 크기 pl에서 패치를 투영하여 크기 hf의 입력 임베딩을 만든답니다.\n- Resolution Prefix Tuning 모듈은 시간-주파수/해상도를 나타내는 hf 크기의 임베딩을 만들고 이를 입력 임베딩에 연결합니다 (그림 2의 n=n+1 연산을 주목해주세요).\n- TTM 블록은 3개의 하위 모듈을 포함합니다: 패치 분할 모듈, 베니라 TSMixer 블록 및 패치 병합 블록:\n- 패치 분할 모듈은 패치 수를 K만큼 증가시키고 패치 길이를 다시 K만큼 감소시킵니다. 예를 들어, 첫 번째 수준에서 크기 [c,n, hf]의 입력은 [c, 4*n, hf//4]로 변화합니다.\n- TSMixer 블록이 변환된 입력에 적용되며 패치 병합 블록이 [c, 4*n, hf//4] 입력을 다시 [c,n, hf]로 변형합니다.\n\n# 외부 믹서\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n미래의 알려진 변수가 있는 경우, Exogenous Mixer를 활성화할 수 있습니다. 이 모듈은 Figure 3에 표시되어 있으며, TTM 아키텍처에서의 위치는 Figure 1에 표시되어 있습니다:\n\n![Exogenous Mixer](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_3.png)\n\nExogenous Mixer 블록은 간단합니다: 시계열의 미래 값(y3와 y4; Figure 3, 녹색)이 알려진 경우, 이를 사용하여 대상 변수(y1과 y2; Figure 4, 보라색)의 예측을 안내합니다.\n\n# TTM 교육 세부 정보 및 데이터세트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저자들은 다양한 문맥과 예측 길이에 대해 5가지 TTM 버전을 만들었습니다. 이는 (512,96), (512,192), (512, 336), (512,720), (96,24) 입니다.\n\n교육에 관해서, 저자들은 모델 사전 훈련을 위해 Monash 데이터베이스의 하위 집합(244k 샘플)을 사용했고, 파인튜닝 성능을 평가하기 위해 Informer 데이터셋을 사용했습니다. 또한, 저자들은 외부 혼합기 블록의 효과를 평가하고 알려진 미래 변수를 추가함으로써 성능이 얼마나 향상되는지 조사하기 위해 다른 데이터셋을 사용했습니다.\n\n이러한 데이터셋에 대해 더 자세한 내용은 원본 논문에서 확인할 수 있습니다. 아래는 (512,96) 변형을 위한 교육 하이퍼파라미터입니다:\n\n- pl(패치 길이) = 64\n- 백본 수준 수 = 6\n- 각 수준 당 TTM 블록 수 = 2\n- 배치 크기 = 3천\n- 에폭 = 20\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n학습 및 파인 튜닝 구성에 대한 자세한 내용은 원 논문을 참조해 주세요.\n\n# 평가 벤치마크\n\nTTM 대 최신 기법 모델\n\n그 다음, 저자들은 Zero-shot 및 5% Few-shot 버전의 TTM을 다른 최신 기법 모델과 비교했습니다. 이때 사용된 평가 메트릭은 MSE였습니다. 결과는 다음과 같은 표 1에서 확인할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 너무 인상적입니다:\n\n평균적으로, Few-shot TTM이 모든 다른 모델을 능가했습니다. 심지어 Zero-shot TTM이 일부 모델을 능가할 수 있었습니다! 기억하세요, Zero-shot TTM은 이러한 데이터에 대해 학습을 받지 않고 예측을 생성합니다.\n\n또한 TTM은 작년에 소개된 새로운 기반 시계열 모델인 GPT4TS를 앞서 나갔습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTMT 이외에도 다음으로 높은 순위의 모델은 GPT4TS, PatchTST 및 TSMixer입니다. 모두 패치(patching)를 활용합니다. 최근 시계열 예측 연구에서 패치(patching)가 매우 유익한 기술임이 입증되었습니다.\n\nTTM 대 foundation 모델\n\n저자들은 TTM을 독립적으로 평가하며 특히 GPT4TS와 LLMTime과 비교합니다.\n- LLMTime은 GPT-3과 LLaMa-2를 사용하여 시계열 예측을 위해 특정 수정을 가한 모델입니다.\n- GPT4TS는 다양한 작업(예측, 분류 등)을 위해 범용 시계열 모델이며, 기본 모델로 GPT-2를 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비교 결과는 표시됩니다. Table 2 (LLMTime)와 Table 3 (GPT4TS):\n\n![LLMTime](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_5.png)\n\n![GPT4TS](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_6.png)\n\nLLMTime은 제로샷 예측 시나리오에서 평가되었고, GPT4TS는 퓨샷 예측기로 동작했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 두 가지 비교에서 TTM이 명확한 승자입니다.\n- 게다가 TTM은 훨씬 빠르며 리소스를 상당히 적게 필요로 합니다. 이는 TTM이 GPT4TS와 같은 무거운 트랜스포머 계산을 사용하지 않기 때문에 예상된 결과입니다.\n\n## 외생 변수의 효과성\n\n현대 실제 세계 데이터셋은 가능한 경우 외생 변수를 사용하므로 예측 애플리케이션에서 이를 활용하는 것이 합리적입니다.\n\nTTM의 저자들은 이와 같은 변수를 사용함으로써 TTM이 어떻게 향상되는지 조사했습니다 (해당하는 경우). 구체적으로 제로샷 TTM, 일반 TTM 및 외생 변수를 사용하는 채널 혼합 (TTM-CM)을 비교했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그들은 TSMixer와 그 채널 혼합 변형을 평가했습니다. 결과는 다음과 같이 Table 4에 표시되어 있습니다:\n\n![Table 4](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_7.png)\n\n다시 한 번, 결과는 매우 흥미로워요: 먼저, TTM-CM이 1위를 차지하여 외생 변수가 모델에 도움이 되는 것을 의미합니다.\n\n채널 혼합 속성을 사용하는 TSMixer 변형은 2위를 차지했습니다. 또한, 제로-샷 TTM이 최악의 성능을 보입니다. 보조 변수가 있는 경우 모델 성능을 향상시키는 데 사용되어야 함이 명백합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Tiny Time Mixers 실무 활용\n\n모델 버전 512-96과 1024-96의 가중치를 HuggingFace에서 다운로드하여 다음과 같이 세밀 조정할 수 있습니다:\n\n```js\n!git clone https://github.com/IBM/tsfm.git\n!pip install transformers\n!pip install datasets\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tsfm_public.models.tinytimemixer.utils import (\n    count_parameters,\n    plot_preds,\n)\n\nfrom tsfm_public.models.tinytimemixer import TinyTimeMixerForPrediction\nfrom tsfm_public.toolkit.callbacks import TrackingCallback\n\nzeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\"ibm/TTM\", revision='main')\nfinetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\"ibm/TTM\", revision='main', head_dropout=0.0,dropout=0.0,loss=\"mse\")\n```\n\n따라서 transformers 라이브러리의 익숙한 Trainer 모듈을 사용하여 TTM을 세밀 조정할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfinetune_forecast_trainer = Trainer(\nmodel=finetune_forecast_model,\nargs=finetune_forecast_args,\ntrain_dataset=train_dataset,\neval_dataset=valid_dataset,\ncallbacks=[early_stopping_callback, tracking_callback],\noptimizers=(optimizer, scheduler))\n\n# Fine tune\nfinetune_forecast_trainer.train()\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_8.png\" /\u003e\n\npredictions_test = finetune_forecast_trainer.predict(test_dataset)\n\n이후에는 사적 데이터셋을 통해 예측을 받은 후 결과를 플롯합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 테이블 태그를 마크다운 형식으로 변경하도록 했습니다.\n\n\n\u003cimg src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_9.png\" /\u003e\n\n# 마무리 말씀\n\nTiny Time Mixer (TTM)은 다른 접근 방식을 따른 혁신적인 모델로서, 더 작지만 효율적인 모델들을 위한 길을 열어두었습니다.\n\n특히, TTM은 어텐션을 사용하지 않았고 여전히 강력한 시계열(Time Series) 기반 모델을 구축할 수 있다는 것을 입증했습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최초로 MLP만 사용한 메타러닝 기능을 갖춘 시계열 모델은 N-BEATS와 N-HITS였어요. 이 트렌드가 어떻게 이어지는지 한번 살펴봐요.\n\n최근에는 NLP 모델에서도 이러한 트렌드를 관측하고 있어요. 우리는 Mamba(State Space) xLSTM(기반 RNN)과 Hyena(CNN 기반)을 보았는데, 이들은 언어 모델이지만 트랜스포머는 아니며 다양한 벤치마크에서 인상적인 결과를 얻고 있어요.\n\n시계열 모델에 대한 이런 접근 방식이 어떻게 전개될지도 한번 살펴봅시다. 결국, 시계열에 대한 기초 모델 연구는 아직 새로운 상황이에요!\n\n# 읽어주셔서 감사합니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 제 LinkedIn 팔로우해 주세요!\n- 제 뉴스레터, AI Horizon Forecast를 구독해 주세요!\n\n## 참고 자료\n\n[1] Ekambaram 등, Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series (2024년 4월)\n\n[2] Ekambaram 등, TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting (2023년 6월)","ogImage":{"url":"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_0.png"},"coverImage":"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_0.png","tag":["Tech"],"readingTime":9},{"title":"MLX로부터 GPT를 처음부터 만들어보기","description":"","date":"2024-06-19 03:00","slug":"2024-06-19-GPTfromScratchwithMLX","content":"\n\n## MacBook에서 GPT-2 정의 및 훈련하기\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_0.png)\n\n이 게시물의 목표는 MLX, Apple 실리콘을 위한 Apple의 기계 학습 라이브러리를 사용하여 GPT-2를 처음부터 정의하고 훈련하는 과정을 안내하는 것입니다. 토크나이저에서 샘플링까지 모든 과정을 상세히 다루고자 합니다. Karpathy의 훌륭한 GPT 처음부터 튜토리얼 영감을 받아, 우리는 Shakespeare의 작품에 대해 모델을 훈련할 것입니다. 우리는 비어 있는 Python 파일로 시작하여 Shakespeare 스타일 텍스트를 작성할 수 있는 소프트웨어로 끝낼 것입니다. 그리고 이 모든 것을 훨씬 빠르게 가능하게 하는 MLX에서 모두 구축할 것입니다.\n\n본 게시물은 따라하며 체험하는 것이 가장 좋습니다. 코드는 아래 리포지토리에 포함되어 있으며 이를 열어 참조하는 것을 권장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 목차\n\n- 데이터 준비\n- GPT-2 코딩\n- 입력 임베딩\n- 위치 임베딩\n- 셀프 어텐션\n- 키, 쿼리 및 값\n- 멀티헤드 어텐션\n- MLP\n- 블록\n- 레이어 정규화 및 스킵 연결\n- 순방향 패스\n- 샘플링\n- 초기화\n- 훈련 루프\n- 참고 자료\n\n# 데이터 준비\n\nmlx를 설치하고 다음 임포트를 실행하십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport mlx.core as mx\nimport mlx.nn as nn\nimport mlx.optimizers as optim\nimport mlx.utils as utils\nimport numpy as np\nimport math\n```\n\nLLM 훈련의 첫 번째 단계는 큰 텍스트 데이터 코퍼스를 수집한 다음 토큰화하는 것입니다. 토큰화는 텍스트를 정수로 매핑하는 작업으로, LLM에 공급할 수 있습니다. 이 모델의 훈련 코퍼스는 셰익스피어의 작품들을 연결한 것입니다. 이는 대략 100만 글자이며 다음과 같습니다:\n\n```js\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n...\n```\n\n먼저 파일을 하나의 긴 문자열로 읽어 text 변수에 저장합니다. 그런 다음 set() 함수를 사용하여 텍스트에 있는 모든 고유한 문자를 얻어서 우리의 어휘가 됩니다. vocab을 출력하여 우리의 어휘에 있는 모든 문자를 하나의 문자열로 볼 수 있으며, 우리는 총 65개의 문자가 있어서 이것이 우리의 토큰이 될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 단어장 생성하기\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nvocab = sorted(list(set(text)))\nvocab_size = len(vocab)\n\nprint(''.join(vocab))\n# !$\u0026',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nprint(vocab_size)\n# 65\n\n\n생산 모델은 바이트 페어 인코딩과 같은 토큰화 알고리즘을 사용하여 하위 단어 청크의 더 큰 어휘를 생성할 것입니다. 오늘 우리의 초점은 아키텍처에 있기 때문에, 문자 수준의 토큰화를 계속할 것입니다. 다음으로, 단어장을 정수로 매핑하여 토큰 ID로 알려진 것으로 이동할 것입니다. 그런 다음 텍스트를 토큰으로 인코딩하고 문자열로 다시 디코딩할 수 있습니다.\n\n\n# 단어장을 정수로 매핑하기\nitos = {i:c for i,c in enumerate(vocab)} # int to string\nstoi = {c:i for i,c in enumerate(vocab)} # string to int\nencode = lambda x: [stoi[c] for c in x]\ndecode = lambda x: ''.join([itos[i] for i in x])\n\nprint(encode(\"hello world\"))\n# [46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\nprint(decode(encode(\"hello world\")))\n# hello world\n\n\n모든 문자 및 해당 어휘의 인덱스를 반복하여 숫자를 문자에 매핑하는 itos와 문자를 숫자에 매핑하는 stoi 사전을 생성하기 위해 enumerate() 함수를 사용합니다. 그런 다음 이러한 매핑을 사용하여 encode 및 decode 함수를 만듭니다. 이제 전체 텍스트를 인코딩하고 훈련 및 검증 데이터로 나눌 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n데이터 = 인코딩(텍스트)\n분할 = int(0.9 * len(데이터))\n훈련_데이터 = 데이터[:분할]\n검증_데이터 = 데이터[분할:]\n```\n\n현재 훈련 데이터는 토큰들의 매우 긴 문자열입니다. 그러나 이전 토큰들이 주어졌을 때 다음 토큰을 예측하는 모델을 훈련하려고 합니다. 따라서 데이터셋은 입력이 토큰 문자열이고 레이블이 올바른 다음 토큰인 예제로 구성되어야 합니다. 다음 토큰을 예측하는 데 사용되는 최대 토큰 수인 context length라는 모델 매개변수를 정의해야 합니다. 훈련 예제는 우리의 context length의 길이가 될 것입니다.\n\n처음 ctx_len+1 개의 토큰을 살펴봅시다.\n\n```js\nctx_len = 8\nprint(훈련_데이터[:ctx_len + 1])\n# [18, 47, 56, 57, 58,  1, 15, 47, 58]\n# x: [18, 47, 56, 57, 58,  1, 15, 47] | y: 58\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 예제는 인풋이 \"18, 47, 56, 57, 58, 1, 15, 47\"이고, 원하는 아웃풋이 \"58\"인 트레이닝 예제입니다. 이는 8 토큰의 컨텍스트를 가지고 있습니다. 그러나 생성 중에 필요한 7, 6, 5 ... 0개의 토큰만을 가지고 다음 토큰을 예측할 수 있도록 모델을 훈련시키고 싶습니다. 따라서 이 예제에 포함된 8개의 하위 예제를 고려합니다:\n\n```js\nctx_len = 8\nprint(train_data[:ctx_len + 1])\n# [18, 47, 56, 57, 58,  1, 15, 47, 58]\n# 8 sub examples\n# [18] --\u003e 47\n# [18, 47] --\u003e 56\n# [18, 47, 56] --\u003e 57\n# [18, 47, 56, 57] --\u003e 58\n# [18, 47, 56, 57, 58] --\u003e 1\n# [18, 47, 56, 57, 58, 1] --\u003e 15\n# [18, 47, 56, 57, 58, 1, 15] --\u003e 47\n# [18, 47, 56, 57, 58, 1, 15, 47] --\u003e 58\n```\n\n라벨은 간단히 왼쪽으로 이동한 인풋입니다.\n\n```js\nprint(\"inputs: \", train_data[:ctx_len])\nprint(\"labels: \", train_data[1:ctx_len+1]) # labels = inputs indexed 1 higher\n# inputs: [18, 47, 56, 57, 58,  1, 15, 47]\n# labels: [47, 56, 57, 58,  1, 15, 47, 58]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인덱스 0에서 입력은 18이고 라벨은 47입니다. 인덱스 1에서 입력은 인덱스 1을 포함하여 그 이전 모든 것, 즉 [18, 47]이고 라벨은 56입니다. 등등. 이제 라벨이 입력 순서에서 한 단계 상위로 색인화됨을 이해했으므로 데이터셋을 구축할 수 있습니다.\n\n```js\n# 훈련 및 검증 데이터 세트 생성\nctx_len = 8\nX_train = mx.array([train_data[i:i+ctx_len] for i in range(0, len(train_data) - ctx_len, ctx_len)])\ny_train = mx.array([train_data[i+1:i+ctx_len+1] for i in range(0, len(train_data) - ctx_len, ctx_len)])\nX_val = mx.array([val_data[i:i+ctx_len] for i in range(0, len(val_data) - ctx_len, ctx_len)])\ny_val = mx.array([val_data[i+1:i+ctx_len+1] for i in range(0, len(val_data) - ctx_len, ctx_len)])\n```\n\n우리는 데이터를 반복하고 입력(X)으로 ctx_len 크기의 청크를 가져와서 같은 청크를 라벨(y)로 하나 높은 색인에서 가져옵니다. 그런 다음 이 Python 리스트를 mlx 배열 객체로 만듭니다. 모델 내부로 mlx를 사용할 것이므로 입력을 mlx 배열로 만들고 싶습니다.\n\n그리고 한 가지 더. 훈련 중에 모델에 한 번에 하나의 예제만 전달하고 싶지 않습니다. 효율성을 위해 병렬로 여러 예제를 한꺼번에 전달하고 싶습니다. 이 예제 그룹을 우리의 배치라고 하며, 그룹 내의 예제 수가 배치 크기입니다. 따라서 훈련용 배치를 생성하는 함수를 정의합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef get_batches(X, y, b_size, shuffle=True):\n    if shuffle:\n        ix = np.arange(X.shape[0])\n        np.random.shuffle(ix)\n        ix = mx.array(ix)\n        X = X[ix]\n        y = y[ix]\n    for i in range(0, X.shape[0], b_size):\n        input = X[i:i+b_size]\n        label = y[i:i+b_size]\n        yield input, label\n```\n\n만약 shuffle=True라면, 데이터를 임의로 섞은 인덱스로 인덱싱하여 데이터를 섞습니다. 그런 다음 데이터 세트를 반복하고 입력 및 레이블 데이터 세트에서 배치 크기 청크를 반환합니다. 이 청크는 미니 배치로 알려져 있으며 병렬로 처리하는 예제를 쌓은 것입니다. 이러한 미니 배치는 모델 훈련 중에 우리의 입력이 될 것입니다.\n\n다음은 컨텍스트 길이가 8 인 4 개의 예제의 미니 배치 예제입니다.\n\n![2024-06-19-GPTfromScratchwithMLX_1.png](/assets/img/2024-06-19-GPTfromScratchwithMLX_1.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n32 개의 다음 토큰 예측 문제가 포함된 미니배치입니다. 모델은 입력의 각 토큰에 대해 다음 토큰을 예측하고 레이블은 손실을 계산하는 데 사용됩니다. 입력의 각 색인에 대한 다음 토큰이 포함된 것을 주목해주세요.\n\n이 텐서들의 형태가 복잡해질 것을 염두에 두시면 좋겠어요. 지금은 일단, 우리가 모델에 (배치 크기, ctx_len) 모양의 텐서를 입력할 것이라는 것만 기억해주세요.\n\n# 코딩 GPT-2\n\nGPT-2 아키텍처를 살펴보고 구현하려는 것에 대한 개요를 파악해봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_2.png)\n\n이게 혼란스러워 보이더라도 걱정하지 마세요. 우리는 바닥부터 꼭대기로 한 단계씩 구현할 거에요. 먼저 입력 임베딩을 구현하는 것부터 시작해봅시다.\n\n## 입력 임베딩\n\n입력 임베딩 레이어의 목적은 토큰 ID를 벡터로 매핑하는 것입니다. 각 토큰은 모델을 통해 전달될 때 그것에 대한 표현으로 사용될 벡터로 매핑됩니다. 각 토큰에 대한 벡터는 모델을 통해 전달되면서 정보를 축적 및 교환하고, 결국 다음 토큰을 예측하는 데 사용될 것입니다. 이러한 벡터들을 임베딩(embedding)이라고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n토큰 ID를 벡터로 매핑하는 가장 간단한 방법은 조회 테이블을 통해 할 수 있습니다. 각 토큰에 대한 임베딩 벡터가 있는 (vocab_size, n_emb) 크기의 행렬을 만듭니다. 이 행렬을 임베딩 가중치라고 합니다.\n\n![image](/assets/img/2024-06-19-GPTfromScratchwithMLX_3.png)\n\n다이어그램은 크기가 (65, 6)인 임베딩 레이어의 예시를 보여줍니다. 이는 어휘 사전에 65개의 토큰이 있고 각각이 길이가 6인 임베딩 벡터로 표현됨을 의미합니다. 입력된 시퀀스는 임베딩 가중치를 색인하여 각 토큰에 해당하는 벡터를 얻는 데 사용됩니다. 모델에 입력하는 미니배치를 기억하십니까? 원래 미니배치는 크기가 (batch_size, ctx_len)입니다. 임베딩 레이어를 통과한 후 크기는 (batch_size, ctx_len, n_emb)입니다. 각 토큰이 단일 정수가 아니라 길이가 n_emb인 벡터임을 의미합니다.\n\n이제 코드에서 임베딩 레이어를 정의해봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nn_emb = 6 # 파일 맨 위에 이러한 하이퍼파라미터를 추가할 수 있어요\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb)\n```\n\n우리의 구현을 정리하기 위한 클래스를 정의할 거예요. mlx의 기능을 활용하기 위해 nn.Module을 서브클래스화할 거예요. 그럼 init 함수에서는 슈퍼클래스 생성자를 호출하고 wte라고 불리는 토큰 임베딩 레이어를 초기화할 거예요.\n\n## 위치 임베딩\n\n다음은 위치 임베딩이에요. 위치 임베딩의 목적은 시퀀스에서 각 토큰의 위치에 대한 정보를 인코딩하는 거예요. 이걸 우리의 입력 임베딩에 추가해서 각 토큰의 완전한 표현을 얻을 수 있어요. 그 표현에는 시퀀스에서 토큰의 위치에 대한 정보가 담겨있어요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb) # 토큰 임베딩\n        self.wpe = nn.Embedding(ctx_len, n_emb) # 위치 임베딩\n```\n\n위치 임베딩은 토큰 임베딩과 동일한 방식으로 작동합니다. 하지만 각 토큰마다 행이 있는 것 대신, 각 가능한 위치 인덱스마다 행이 있습니다. 이는 임베딩 가중치의 모양이 (ctx_len, n_emb)가 됨을 의미합니다. 이제 GPT 클래스에 __call__ 함수를 구현해보겠습니다. 이 함수에는 모델의 forward pass가 포함될 것입니다.\n\n```python\n# 텐서 모양 주석\ndef __call__(self, x):\n    B, T = x.shape # (B = 배치 크기, T = ctx_len)\n    tok_emb = self.wte(x) # (B, T, n_emb)\n    pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)\n    x = tok_emb + pos_emb # (B, T, n_emb)\n```\n\n먼저, 입력의 차원을 B와 T 변수로 나누어 보다 쉽게 처리합니다. 시퀀스 모델링 맥락에서 B와 T는 일반적으로 \"배치\"와 \"시간\" 차원을 나타내는 약어로 사용됩니다. 이 경우, 시퀀스의 \"시간\" 차원은 컨텍스트 길이입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로 토큰 및 위치 임베딩을 계산합니다. 위치 임베딩의 경우 입력이 mx.arange(T)임에 유의하십시오. 이는 우리가 임베딩하고자 하는 위치인 0부터 T-1까지의 연속 정수 배열을 출력합니다. 임베딩 레이어를 통과한 후에는 각 위치에 대해 n_emb 길이의 벡터를 추출하므로 모양이 (T, n_emb)인 텐서가 생성됩니다. pos_emb가 tok_emb과 형태가 다르더라도 mlx가 브로드캐스트 또는 배치 차원을 통해 pos_emb을 복제하여 요소별 덧셈을 허용하기 때문에 두 값을 더할 수 있습니다. 마지막으로 덧셈을 수행하여 토큰의 새로운 표현을 얻습니다.\n\n## 셀프 어텐션\n\n지금까지 각 토큰의 표현 벡터는 독립적으로 계산되었습니다. 그들은 어떠한 정보를 교환할 기회도 가지지 못했습니다. 이는 주변 맥락에 따라 단어의 의미와 사용이 의존되므로 언어 모델링에서 직관적으로 나쁜 접근입니다. 셀프 어텐션은 이전 토큰으로부터 정보를 현재 토큰으로 통합하는 방법입니다.\n\n먼저, 가장 단순한 접근 방식을 살펴보겠습니다. 만약 각 토큰을 단순히 해당 표현 벡터의 평균으로 표현하고 그 이전의 모든 토큰의 벡터를 더한다면 어떨까요? 이렇게 하면 이전 토큰들로부터 정보를 현재 토큰의 표현에 담을 수 있습니다. 어떻게 보이게 될까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_4.png)\n\n하지만 self-attention은 for-loop를 사용하지 않습니다. 핵심 아이디어는 이전 토큰의 평균을 행렬 곱셈으로 얻을 수 있다는 것입니다!\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_5.png)\n\n입력 시퀀스를 특별한 행렬로 왼쪽부터 곱하면 원하는 결과를 얻을 수 있습니다. 이 행렬을 주목해보면 이는 어텐션 가중치라고 알려져 있습니다. 어텐션 가중치 행렬의 각 행은 주어진 토큰의 표현에 각 다른 토큰이 얼마나 많이 기여하는지를 나타냅니다. 예를 들어, 두 번째 행의 경우 [0.5, 0.5, 0, 0]입니다. 이것은 두 번째 행의 결과가 0.5*토큰1 + 0.5*토큰2 + 0*토큰3 + 0*토큰4, 즉 토큰1과 토큰2의 평균이 됨을 의미합니다. 어텐션 가중치는 하삼각 행렬입니다 (우상단 항목이 0). 이는 미래 토큰이 주어진 토큰의 표현에 포함되지 않도록 보장합니다. 이는 토큰이 이전 토큰과만 통신할 수 있도록 하며, 생성 중에 모델은 이전 토큰에만 액세스할 수 있는 것이 보장됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떻게 주의 집중 가중치 행렬을 생성할 수 있는지 살펴봅시다.\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_6.png)\n\n주의 가중치 행렬을 구성하는 배열을 만들고 오른쪽 상단 항목에 -inf를 넣은 다음 행별 softmax를 수행하면 원하는 주의 가중치를 얻을 수 있습니다. 이 작업을 수행하는 것으로 이 작업이 작동하는 방법을 확인하는 것이 좋습니다. 핵심은 (ctx_len, ctx_len) 크기의 배열을 가지고 각 행에 softmax를 수행하여 합계가 1이 되는 주의 가중치를 얻을 수 있다는 것입니다.\n\n이제 naive 자기 주의 영역을 벗어나 볼 수 있습니다. 이전 토큰을 간단히 평균화하는 대신 이전 토큰에 대한 임의의 가중 합계를 사용합니다. 임의의 행렬의 분포 소프트맥스를 수행할 때 어떻게 되는지 주목하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_7.png)\n\n각 행의 합이 1인 가중치를 계속 얻습니다. 훈련 중에는 왼쪽 행렬의 숫자를 학습하여 각 토큰이 다른 토큰의 표현에 얼마나 많이 참여하는지를 지정할 수 있습니다. 이것이 토큰이 서로에게 \"주의\"를 기울이는 방법입니다. 그러나 여전히 이 왼쪽 행렬이 어디에서 나왔는지 이해하지 못했습니다. 이러한 사전 소프트맥스 주의 가중치는 토큰 자체에서 계산되지만 간접적으로 세 개의 선형 변환을 통해 수행됩니다.\n\n## Keys, Queries, and Values\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_8.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리 시퀀스의 각 토큰은 3개의 새로운 벡터를 생성합니다. 이러한 벡터를 키(key), 쿼리(query) 및 값(value)라고합니다. 한 토큰의 쿼리 벡터와 다른 토큰의 키 벡터의 내적을 사용하여 두 토큰 간의 \"유사성\"을 측정합니다. 우리는 각 토큰과 각 다른 토큰 사이의 쌍별 유사성을 계산하고 싶어합니다. 따라서 쿼리 벡터(4x3)를 키 벡터의 전치(3x4)와 곱하여 원시 어텐션 가중치(4x4)를 얻습니다. 행렬 곱셈이 작동하는 방식으로 인해 원시 어텐션 가중치의 (i,j) 항목은 토큰 i의 쿼리와 토큰 j의 키의 내적 또는 두 가지 사이의 \"유사성\"이됩니다. 따라서 우리는 모든 토큰 간의 상호 작용을 계산했습니다. 그러나 과거 토큰이 미래 토큰과 상호 작용하는 것을 원하지 않기 때문에 상단 우측 항목들에 -inf 마스크를 적용하여 소프트맥스 후에 제로아웃되도록합니다. 그런 다음 행별 소프트맥스를 수행하여 최종 어텐션 가중치를 얻습니다. 이러한 가중치를 입력과 직접 곱하는 대신 값 프로젝션과 곱합니다. 결과적으로 새로운 표현이 생성됩니다.\n\n이제 우리는 주의를 개념적으로 이해했으니, 구현해 봅시다.\n\n```js\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n  \n```\n\n키, 쿼리 및 값 프로젝션 레이어를 정의하여 시작합니다. n_emb에서 진행하는 대신 n_emb에서 head_size로 프로젝션합니다. 아무것도 변경되지 않으며, 주의를 통해 계산된 새로운 표현이 차원 head_size가됨을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n    def __call__(self, x): # shapes commented\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n\n\n앞서로부터 전달받은 값으로 key, query, value를 계산한 뒤, 입력 모양을 미래의 편리함을 위해 변수 B, T, C로 나눕니다.\n\n\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\n\n\n이어서, 어텐션 가중치를 계산합니다. 키 텐서의 마지막 두 차원만 바꿔야 하므로 차원 변환은 마지막 두 차원에 대해서만 이루어집니다. 배치 차원은 여러 학습 예제를 병렬로 전달하기 위한 것뿐입니다. mlx의 전치 함수는 차원의 새로운 순서를 입력으로 받기 때문에, 마지막 두 차원을 전치하기 위해 [0, 2, 1]을 전달합니다. 그리고 여기에 주목할 점: 어텐션 가중치들은 head_size의 제곱근에 역수를 적용합니다. 이는 스케일드 어텐션이라 불리며, 목적은 Q와 K가 단위 분산을 가질 때, attn_weights도 단위 분산을 가지게 하는 것입니다. attn_weights의 분산이 높으면 softmax가 이 작은 값과 큰 값을 0 또는 1로 매핑하여 복잡성이 적은 표현을 얻도록 합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 단계는 미래 토큰에 주의를 기울이지 않는 인과 언어 모델링, 즉 토큰이 미래 토큰에 주의를 기울이지 않도록 마스크를 적용하는 것입니다.\n\n```js\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] \u003c indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\n```\n\n우리는 미리 설정한 ctx_len=4와 같은 다이어그램에서 indices 변수를 [0, 1, 2, 3]으로 설정하기 위해 mx.arange(4)을 사용한다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼 indices[:, None]와 같이 인덱스 값을 가진 열 벡터를 생성할 수 있어요. 마찬가지로 indices[None]을 사용하면 행 벡터를 얻을 수 있어요. 그리고 ` 비교를 수행할 때 mlx는 벡터들을 브로드캐스트합니다. 그 이유는 형태가 맞지 않아 요소별로 비교할 수 없기 때문이에요. 브로드캐스팅은 mlx가 부족한 차원에 따라 벡터를 복제한다는 것을 의미해요. 그 결과, (4, 4) 행렬 간의 요소별 비교가 이루어집니다. 그게 이해가 되죠. 참고로, 텐서 처리할 때 브로드캐스팅 세부 정보에 익숙해지는 것을 권장드립니다. 이 링크를 읽어보세요. 튜토리얼 등장횟수가 많을 거에요.\n\n요소별 비교 후, 다음 텐서가 남아 있어요:\n\n```js\n[[False,  True,  True,  True],\n [False, False,  True,  True],\n [False, False, False,  True],\n [False, False, False, False]]\n```\n\n이 텐서에 -1e9를 곱하면 값을 구할 수 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n[[-0e+00, -1e+09, -1e+09, -1e+09],\n [-0e+00, -0e+00, -1e+09, -1e+09],\n [-0e+00, -0e+00, -0e+00, -1e+09],\n [-0e+00, -0e+00, -0e+00, -0e+00]]\n```\n\n이제 추가적인 마스크가 있습니다. 이 행렬을 어텐션 가중치에 추가하여 모든 오른쪽 상단 항목을 매우 큰 음수로 만들 수 있습니다. 이렇게 하면 소프트맥스 연산 후에 이들이 0이 될 것입니다. 또한, _causal_mask 속성 이름에 \"_\"를 접두사로 추가하여 개인 변수로 표시합니다. 이것은 mlx에게 이것이 매개변수가 아니며 교육 중에 업데이트되지 않아야 함을 나타냅니다.\n\n```js\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] \u003c indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\n        attn_weights = attn_weights + self._causal_mask\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        o = (attn_weights @ V) # (B, T, head_size)\n```\n\n이제 최종 어텐션 가중치를 얻기 위해 행별로 softmax 처리하고 이러한 가중치를 값에 곱하여 출력을 얻을 수 있습니다. softmax에 axis=-1을 전달하여 행이 있는 마지막 차원을 따라 softmax를 수행하려는 것을 지정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최종 단계는 선형 투영 및 드롭아웃을 출력하는 것입니다.\n\n```js\n드롭아웃 = 0.1 # 파일 상단의 하이퍼파라미터와 함께 추가\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] \u003c indices[None] # 브로드캐스팅 트릭\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 투영\n        self.resid_dropout = nn.Dropout(드롭아웃)\n    def __call__(self, x):\n        B, T, C = x.shape # (배치 크기, ctx 길이, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\n        attn_weights = attn_weights + self._causal_mask\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        o = (attn_weights @ V) # (B, T, head_size)\n        o = self.c_proj(self.resid_dropout(o))\n        return o\n```\n\n출력 투영과 잔차 드롭아웃인 c_proj 및 resid_dropout 두 개의 새 계층을 추가했습니다. 출력 투영은 벡터를 원래 차원인 n_emb로 반환하는 역할을 합니다. 드롭아웃은 정규화 및 훈련 안정성을 위해 추가되었으며, 트랜스포머 블록을 쌓으면서 심층 네트워크를 구축하는 것이 중요합니다. 이것으로 하나의 어텐션 헤드를 구현하는 것이 끝났습니다!\n\n## 다중 헤드 어텐션\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하나의 주의 헤드만 있는 LLM보다는 여러 개의 주의 헤드를 병렬로 사용하고 그 출력을 연결하여 최종 표현을 만들곤 합니다. 예를 들어, 하나의 head_size=64를 가진 attention head가 있다고 가정해봅시다. 각 토큰에 대해 생성된 벡터는 64 차원입니다. 우리는 head_size=16인 4개의 병렬 attention head로 동일한 결과를 얻을 수 있습니다. 이들의 출력을 연결하여 16x4 = 64 차원의 출력을 생성할 수 있습니다. Multi-head attention은 모델이 더 복잡한 표현을 학습할 수 있도록 합니다. 각 head가 다른 projection 및 attention 가중치를 학습하기 때문입니다.\n\n```js\nn_heads = 4\nclass MultiHeadAttention(nn.Module): # 단순한 구현\n    def __init__(self):\n        super().__init__()\n        self.heads = [Attention(head_size // n_heads) for _ in range(n_heads)]\n    def __call__(self, x):\n        return mx.concatenate([head(x) for head in self.heads], axis=-1)\n```\n\n간단한 구현은 n_heads의 attention head 목록을 생성하고, 각각의 크기를 최종 head 크기로 나눈 것입니다. 그리고 각 헤드의 출력을 마지막 축을 기준으로 연결하는 것입니다. 그러나 이 구현은 비효율적이며 텐서의 속도를 활용하지 못합니다. 텐서의 성능을 활용한 multi-head attention을 구현해 봅시다.\n\n```js\nhead_size = 64 # 파일 상단에 넣기\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] \u003c indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 projection\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우선, 단일 헤드 어텐션 구현부터 시작해보겠습니다. __init__() 함수는 변경되지 않았어요. forward pass는 키(key), 쿼리(query), 값(value) 프로젝션을 생성하는 것으로 일반적으로 시작합니다.\n\n```js\nhead_size = 64 # 파일 맨 위에 배치\nn_heads = 8 # 파일 맨 위에 배치\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] \u003c indices[None] # 브로드캐스팅 트릭\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 프로젝션\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (배치 크기, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        mha_shape = (B, T, n_heads, head_size//n_heads)\n        K = mx.as_strided(K, (mha_shape)) # (B, T, n_heads, head_size//n_heads)\n        Q = mx.as_strided(Q, (mha_shape)) # (B, T, n_heads, head_size//n_heads)\n        V = mx.as_strided(V, (mha_shape)) # (B, T, n_heads, head_size//n_heads)\n```\n\n다음으로 수행해야 할 일은 헤드 수를 나타내는 새로운 차원을 추가하는 것이에요. 기존의 부자연스러운 구현에서는 각각 고유한 키, 쿼리 및 값 텐서를 가진 별도의 어텐션 객체를 사용했었지만, 이제 이 모든 요소를 하나의 텐서에 모두 가지고 있기 때문에 헤드를 위한 차원이 필요합니다. 우리가 원하는 새로운 모양을 mha_shape에 정의합니다. 그런 다음 각 텐서를 헤드 차원을 가지도록 재구성하기 위해 mx.as_strided()를 사용합니다. 이 함수는 파이토치의 view와 동등하며 mlx에게 이 배열을 다른 모양으로 다루도록 지시합니다. 그러나 아직 문제가 있어요. 이전과 같이 Q @ K_t(K의 마지막 2 차원을 전치한 K_t)를 곱하여 어텐션 가중치를 계산하려고 하면 다음과 같은 모양을 곱하게 됩니다.\n\n```js\n(B, T, n_heads, head_size//n_heads) @ (B, T, head_size//n_heads, n_heads)\n결과 모양: (B, T, n_heads, n_heads)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 (B, T, n_heads, n_heads) 모양의 텐서가 생성됩니다. 이는 올바른 결과가 아닙니다. 한 개의 헤드에서 우리의 어텐션 가중치는 (B, T, T) 모양이어야 합니다. 각 토큰 쌍 간의 상호 작용을 제공하기 때문에 이는 의미가 있습니다. 따라서 이제 우리의 모양은 똑같아야 하지만 헤드 차원이 추가되어야 합니다: (B, n_heads, T, T). 이를 위해 키, 쿼리 및 값의 차원을 변환하고, n_heads 차원을 2가 아닌 1로 만든 다음 재구성하는 방식으로 이를 달성합니다.\n\n```js\nhead_size = 64 # 파일 상단에 배치\nn_heads = 8 # 파일 상단에 배치\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] \u003c indices[None] # 브로드캐스팅 트릭\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 프로젝션\n        self.attn_dropout = nn.Dropout(dropout)\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (배치 크기, 문맥 길이, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        mha_shape = (B, T, n_heads, head_size//n_heads)\n        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1]) # (B, n_heads, T, T)\n        attn_weights = attn_weights + self._causal_mask[:T, :T]\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        attn_weights = self.attn_dropout(attn_weights)\n        o = (attn_weights @ V) # (B, n_heads, T, head_size//n_heads)\n        \n```\n\n이제 올바른 어텐션 가중치를 계산할 수 있습니다. 각 개별 어텐션 헤드의 크기로 어텐션 가중치를 조정합니다. 연결 이후의 크기인 head_size가 아닌 각 개별 어텐션 헤드의 크기로 어텐션 가중치를 조정합니다. 또한 어텐션 가중치에 드롭아웃을 적용합니다.\n\n마지막으로, 연결을 수행하고 출력 프로젝션 및 드롭아웃을 적용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nhead_size = 64 # 파일 상단에 넣어 둬요\nn_heads = 8 # 파일 상단에 넣어 둬요\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] \u003c indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 프로젝션\n        self.attn_dropout = nn.Dropout(dropout)\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (배치 크기, ctx 길이, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        mha_shape = (B, T, n_heads, head_size//n_heads)\n        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1]) # (B, n_heads, T, T)\n        attn_weights = attn_weights + self._causal_mask[:T, :T]\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        attn_weights = self.attn_dropout(attn_weights)\n        o = (attn_weights @ V) # (B, n_heads, T, head_size//n_heads)\n        o = o.transpose([0, 2, 1, 3]).reshape((B, T, head_size)) # 헤드 연결\n        o = self.c_proj(self.resid_dropout(o))\n        return o\n```\n\n모든 것을 하나의 텐서로 가지고 있기 때문에 형태 조작을 통해 연결을 수행할 수 있어요. 먼저, `transpose` 함수를 사용하여 `n_heads`를 두 번째로 마지막 차원으로 이동합니다. 그런 다음, 앞서 수행한 헤드 분할을 되돌리기 위해 원래 크기로 다시 형태를 변환합니다. 이는 각 헤드에서 최종 벡터를 연결하는 것과 동일합니다. 그리고 이게 멀티 헤드 어텐션에 대한 모든 것이에요! 가장 집중력이 필요한 구현 부분을 처리했어요.\n\n# MLP\n\n아키텍처의 다음 부분은 멀티레이어 퍼셉트론 또는 MLP입니다. 이는 2개의 쌓인 선형 레이어를 의미합니다. 여기에 말할 것은 많지 않아요, 이것은 표준 신경망입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.c_fc = nn.Linear(n_emb, 4 * n_emb)\n        self.gelu = nn.GELU()\n        self.c_proj = nn.Linear(4 * n_emb, n_emb)\n        self.dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        x = self.gelu(self.c_fc(x))\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n```\n\n입력을 받아 c_fc를 사용하여 고차원으로 프로젝션합니다. 그런 다음 gelu 비선형성을 적용하고 c_proj를 사용하여 임베딩 차원으로 다시 프로젝션합니다. 마지막으로 드롭아웃을 적용하고 반환합니다. MLP의 목적은 주의를 통해 벡터가 통신한 후 일부 계산을 허용하는 것입니다. 이러한 통신 레이어(주의) 및 계산 레이어(mlp)를 블록에 쌓겠습니다.\n\n# 블록\n\nGPT 블록은 주의가 뒤따르는 MLP로 구성됩니다. 이러한 블록은 구조를 깊게 만들기 위해 반복됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nclass Block(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mlp = MLP()\n        self.mha = MultiHeadAttention()\n    \n    def __call__(self, x):\n        x = self.mha(x)\n        x = self.mlp(x)\n        return x\n```\n\n이제 훈련 안정성을 향상시키기 위해 두 가지 기능을 추가해야 합니다. 아키텍처 다이어그램을 다시 살펴보겠습니다.\n\n## 레이어 정규화 및 스킵 연결\n\n![GPTfromScratchwithMLX_10](/assets/img/2024-06-19-GPTfromScratchwithMLX_10.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아직도 빨간색으로 표시된 구성 요소를 구현해야 합니다. 화살표는 skip 연결을 나타냅니다. 입력이 직접 변환되는 대신, 어텐션 및 MLP 레이어의 효과는 가산적입니다. 이들 결과는 입력에 직접적으로 대체하는 대신 추가됩니다. 이는 깊은 신경망의 훈련 안정성에 도움이 됩니다. 왜냐하면 역전파에서 덧셈 연산의 피연산자들은 합과 동일한 기울기를 받게 됩니다. 그러므로 기울기가 자유롭게 역방향으로 흐를 수 있어서 깊은 신경망을 괴롭히는 사라지거나 폭주하는 기울기와 같은 문제를 방지할 수 있습니다. 또한 레이어 정규화는 활성화 함수들이 정규 분포를 보이도록 하여 훈련 안정성을 돕습니다. 아래는 최종 구현입니다.\n\n```js\nclass Block(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mlp = MLP()\n        self.mha = MultiHeadAttention()\n        self.ln_1 = nn.LayerNorm(dims=n_emb)\n        self.ln_2 = nn.LayerNorm(dims=n_emb)\n    def __call__(self, x):\n        x = x + self.mha(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n```\n\n레이어 정규화는 멀티헤드 어텐션 및 MLP 이전에 적용됩니다. skip 연결은 x = x + ...와 같이 추가를 의미합니다.\n\n# Forward Pass\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n블록을 정의했으니, GPT-2의 전방 향 과정을 완료할 수 있습니다.\n\n```python\nn_layers = 3 # 파일 맨 위에 배치\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb) # 토큰 임베딩\n        self.wpe = nn.Embedding(ctx_len, n_emb) # 위치 임베딩\n        self.blocks = nn.Sequential(\n            *[Block() for _ in range(n_layers)],\n        ) # 트랜스포머 블록들\n        self.ln_f = nn.LayerNorm(dims=n_emb) # 최종 레이어 정규화\n        self.lm_head = nn.Linear(n_emb, vocab_size) # 출력 프로젝션\n    # 텐서 모양 주석\n    def __call__(self, x):\n        B, T = x.shape # (B = 배치 크기, T = ctx_len)\n        tok_emb = self.wte(x) # (B, T, n_emb)\n        pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)\n        x = tok_emb + pos_emb # (B, T, n_emb)\n        x = self.blocks(x) # (B, T, n_emb)\n        x = self.ln_f(x) # (B, T, b_emb)\n        logits = self.lm_head(x) # (B, T, vocab_size)\n        return logits\n```\n\nnn.Sequential을 사용해 블록을 담는 컨테이너를 만들어서 입력을 순차적으로 전달할 수 있습니다. 그런 다음 self.blocks(x)를 사용하여 모든 블록을 적용할 수 있습니다. 마지막으로 레이어 정규화를 적용하고 lm_head를 적용합니다. lm_head 또는 언어 모델링 헤드는 임베딩 차원에서 어휘 크기로 매핑하는 단순한 선형 레이어입니다. 모델은 어휘 내 각 단어에 대한 값이 포함된 벡터를 출력하며, 이를 로짓이라고 합니다. 로짓에 소프트맥스를 적용하여 어휘 전체에 대한 확률 분포를 얻을 수 있으며, 다음 토큰을 샘플링하거나 훈련 중 손실을 계산하는 데 사용할 수 있습니다. 학습을 시작하기 전에 구현해야 할 두 가지 사항만 더 남았습니다.\n\n# 샘플링\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n훈련이 완료된 후 모델에서 한 번 sampling하기 위해 generate 함수를 작성해야 합니다. 아이디어는 우리가 선택한 일련의 시퀀스로 시작하고, 그 다음 토큰을 예측하여 이를 시퀀스에 추가하는 것입니다. 그런 다음 새로운 시퀀스를 입력하고 다시 다음 토큰을 예측합니다. 이를 멈출 때까지 반복합니다.\n\n```js\n# GPT 클래스의 메서드\ndef generate(self, max_new_tokens):\n  ctx = mx.zeros((1, 1), dtype=mx.int32)\n```\n\n우리는 모델에 단일 토큰 'zero'로 프롬프트를 제공합니다. Zero는 새 줄 문자이므로 모델이 얼마나 셰익스피어와 유사한지 확인하고 싶으므로 세대를 시작하는 자연스러운 장소입니다. 참고로, (1, 1) 형태로 초기화하여 시퀀스 길이가 하나인 단일 배치를 시뮬레이션합니다.\n\n```js\n# GPT 클래스의 메서드\ndef generate(self, max_new_tokens):\n  ctx = mx.zeros((1, 1), dtype=mx.int32)\n  for _ in range(max_new_tokens):\n    logits = self(ctx[:, -ctx_len:]) # 마지막 ctx_len 문자열 전달\n    logits = logits[:, -1, :] # 다음 토큰에 대한 로짓 얻기\n    next_tok = mx.random.categorical(logits, num_samples=1)\n    ctx = mx.concatenate((ctx, next_tok), axis=1)\nreturn ctx\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 토큰에 대한 로짓을 얻으려면 마지막 ctx_len 문자열을 모델에 전달합니다. 그러나 모델 출력은 (B, T, vocab_size) 모양입니다. 왜냐하면 입력의 각 토큰에 대한 다음 토큰의 로짓을 예측하기 때문입니다. 학습 중에는 이를 전부 사용하지만 이제는 새 토큰을 샘플링하기 위해 마지막 토큰의 로짓만 원합니다. 이를 위해 로짓을 인덱싱하여 순서 차원인 첫 번째 차원에서 마지막 요소를 얻습니다. 그런 다음 mx.random.categorical() 함수를 사용하여 다음 토큰을 샘플합니다. 이 함수는 로짓을 softmax를 통해 확률 분포로 변환하고 확률에 따라 토큰을 무작위로 샘플링합니다. 마지막으로 새 토큰을 문맥에 연결하고 max_new_tokens 횟수만큼 프로세스를 반복합니다.\n\n# 초기화\n\n마지막으로 중요한 훈련 다이내믹스를 위해 가중치 초기화를 처리해야 합니다.\n\n```js\n# GPT의 방법\ndef _init_parameters(self):\n    normal_init = nn.init.normal(mean=0.0, std=0.02)\n    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저, 두 가지 서로 다른 nn.init.normal 함수를 정의합니다. 첫 번째는 모든 선형 및 임베딩 레이어를 초기화하는 함수입니다. 두 번째는 특히 잔여 투영인 선형 레이어를 초기화하는 함수이며, 이는 다중 헤드 어텐션과 MLP 내부의 마지막 선형 레이어를 말합니다. 이 특별한 초기화의 이유는 GPT-2 논문에 따르면 모델 깊이가 증가함에 따라 잔여 경로를 따라 누적을 확인하기 때문입니다 [2].\n\nmlx에서는 mx.update() 함수를 사용하여 모델의 매개변수를 변경할 수 있습니다. 문서를 확인해보면, 새로운 모델 매개변수의 완전한 또는 부분적인 사전을 예상합니다. 이 사전이 어떻게 구성되는지는 GPT 클래스 내에서 self.parameters()을 출력하여 확인할 수 있습니다.\n\n```js\n{'wte': {'weight': array([[-0.025084, -0.0197523, -0.0341617, ..., -0.0979123, -0.0830218, -0.0784692],\n       [-0.00777913, -0.117002, -0.0310708, ..., 0.0128591, 0.122941, 0.000414443],\n       [0.0240044, -0.0859084, 0.0253116, ..., 0.108967, 0.0767123, 0.0221565],\n       ...,\n       [0.050729, -0.04578, 0.0685943, ..., -0.0496998, -0.00350879, -0.00631825],\n       [0.00518804, 0.0499818, 0.0330045, ..., 0.0300661, 0.0431054, 0.000958906],\n       [-0.0323007, 0.0132046, 0.0208218, ..., -0.0785159, 0.00436121, -0.00726994]], dtype=float32)}, 'wpe': {'weight': array([[0.000797923, -0.0396898, -0.029047, ..., -0.0132273, 0.00684483, -0.0067624],\n       [-0.0247021, -0.0274349, 0.0310587, ..., -0.100099, 0.0301566, -0.0178732],\n       [0.0929172, -0.0468649, 0.0101506, ..., -0.0341086, -0.0516283, 0.0447596],\n       ...,\n       [-0.0508172, 0.0892201, -0.00183612, ..., -0.00341944, 0.023437, 0.0296461],\n       [0.0105829, 0.0688093, 0.146744, ..., -0.0836337, 0.0206679, 0.0184166],\n       [-0.00578717, -0.0606196, -0.0917056, ..., -0.0641549, -0.0490424, 0.0998114]], dtype=float32)}, 'blocks': {'layers': [{'mlp': {'c_fc': {'weight': array([[0.0169199, 0.00264431, 0.0316978, ..., -0.0596867, -0.0153549, 0.0176386],\n       ...\n```\n\n모든 모델 가중치를 mx.array로 포함하는 중첩된 사전입니다. 따라서 모델의 매개변수를 초기화하려면 새 매개변수로 이와 같은 사전을 구성하고 self.update()에 전달해야 합니다. 이를 위해 다음과 같이 수행할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# GPT의 방법\ndef _init_parameters(self):\n    normal_init = nn.init.normal(mean=0.0, std=0.02)\n    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))\n    new_params = []\n    for name, module in self.named_modules():\n        if isinstance(module, nn.layers.linear.Linear):\n            new_params.append((name + '.weight', normal_init(module.weight)))\n        elif isinstance(module, nn.layers.embedding.Embedding):\n            new_params.append((name + '.weight', normal_init(module.weight))\n```\n\nnew_params 라는 튜플 목록을 유지합니다. 이 목록에는 (parameter_name, new_value)의 튜플이 포함됩니다. 다음으로 self.named_modules()를 사용하여 model의 각 nn.Module 객체를 반복하며 (name, module) 튜플을 반환합니다. 루프 내에서 모듈 이름을 인쇄하면 다음과 같이 보입니다.\n\n```js\nlm_head\nblocks\nblocks.layers.4\nblocks.layers.3\nblocks.layers.3.ln_2\nblocks.layers.3.ln_1\nblocks.layers.3.mha\nblocks.layers.3.mha.resid_dropout\nblocks.layers.3.mha.c_proj\nblocks.layers.3.mha.attn_dropout\nblocks.layers.3.mha.c_attn\n...\nblocks.layers.0.mlp.dropout\nblocks.layers.0.mlp.c_proj\nblocks.layers.0.mlp.gelu\nblocks.layers.0.mlp.c_fc\nwpe\nwte\n```\n\nisinstance() 함수를 사용하여 linear 및 embedding 레이어를 찾은 다음 목록에 추가합니다. 예를 들어, \"blocks.layers.0.mlp.c_fc\"에 도달하는 경우, 이는 MLP의 첫 번째 linear 레이어입니다. 이 경우 첫 번째 if 문이 트리거되어 (\"block.layers.0.mlp.c_fc.weight\", [`초기화된 weight 값 여기에 추가`])의 튜플이 목록에 추가됩니다. 우리는 특정한 이 방법으로 가중치를 초기화하고자 하기 때문에 이름에 \".weight\"를 추가해야 합니다. 이제 잔류 투영 초기화를 처리해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\n# GPT의 메서드\ndef _init_parameters(self):\n    normal_init = nn.init.normal(mean=0.0, std=0.02)\n    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))\n    new_params = []\n    for name, module in self.named_modules():\n        if isinstance(module, nn.layers.linear.Linear):\n            if 'c_proj' in name: # 잔차 투영\n                new_params.append((name + '.weight', residual_init(module.weight)))\n            else:\n                new_params.append((name + '.weight', normal_init(module.weight)))\n            if hasattr(module, 'bias'):\n                new_params.append((name + '.bias', mx.zeros(module.bias.shape)))\n        elif isinstance(module, nn.layers.embedding.Embedding):\n            new_params.append((name + '.weight', normal_init(module.weight)))\n    self = self.update(utils.tree_unflatten(new_params))\n```\n\n선형 레이어인지 확인한 후 \"c_proj\"가 이름에 있는지 확인하고, 잔차 투영이라고 명명한 대로 특별한 초기화를 적용할 수 있습니다. 마지막으로 편향을 0으로 초기화해야 합니다.\n\n선형 브랜치 아래에 다른 if 문을 추가하여 nn.Module 객체가 편향 특성을 가지고 있는지 확인합니다. 그런 경우 해당 값을 0으로 초기화된 목록에 추가합니다. 마지막으로 튜플 목록을 중첩된 딕셔너리로 변환해야 합니다. 다행히 mlx에는 매개변수 딕셔너리를 처리하는 기능이 구현되어 있으며,이 목록을 중첩 된 매개변수 딕셔너리로 변환하기 위해 util.tree_unflatten() 함수를 사용할 수 있습니다. 이를 매개변수를 초기화하기 위해 update 메서드에 전달합니다. 이제 생성자에서 _init_parameters()를 호출할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb)  # 토큰 임베딩\n        self.wpe = nn.Embedding(ctx_len, n_emb)  # 위치 임베딩\n        self.blocks = nn.Sequential(\n            *[Block() for _ in range(n_layers)],\n        )  # 트랜스포머 블록들\n        self.ln_f = nn.LayerNorm(dims=n_emb)  # 최종 레이어 정규화\n        self.lm_head = nn.Linear(n_emb, vocab_size)  # 출력 프로젝션\n        self._init_parameters()  # \u003c-- 파라미터 초기화\n        # 초기화 시 전체 파라미터 수 출력\n        total_params = sum([p.size for n, p in utils.tree_flatten(self.parameters())])\n        print(f\"총 파라미터 수: {(total_params / 1e6):.3f}M\")\n    \n    # 텐서 모양 주석\n    def __call__(self, x):\n        B, T = x.shape  # (B = 배치 크기, T = ctx_len)\n        tok_emb = self.wte(x)  # (B, T, n_emb)\n        pos_emb = self.wpe(mx.arange(T))  # (T, n_emb)\n        x = tok_emb + pos_emb  # (B, T, n_emb)\n        x = self.blocks(x)  # (B, T, n_emb)\n        x = self.ln_f(x)  # (B, T, b_emb)\n        logits = self.lm_head(x)  # (B, T, vocab_size)\n        return logits\n    \n    def generate(self, max_new_tokens):\n        ctx = mx.zeros((1, 1), dtype=mx.int32)\n        for _ in range(max_new_tokens):\n            logits = self(ctx[:, -ctx_len:])\n            logits = logits[:, -1, :]\n            next_tok = mx.random.categorical(logits, num_samples=1)\n            ctx = mx.concatenate((ctx, next_tok), axis=1)\n        return ctx\n    \n    def _init_parameters(self):\n        normal_init = nn.init.normal(mean=0.0, std=0.02)\n        residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))\n        new_params = []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.layers.linear.Linear):\n                if 'c_proj' in name:\n                    new_params.append((name + '.weight', residual_init(module.weight)))\n                else:\n                    new_params.append((name + '.weight', normal_init(module.weight)))\n                if 'bias' in module:\n                    new_params.append((name + '.bias', mx.zeros(module.bias.shape)))\n            elif isinstance(module, nn.layers.embedding.Embedding):\n                new_params.append((name + '.weight', normal_init(module.weight))\n        self = self.update(utils.tree_unflatten(new_params))\n```\n\n생성자에 총 파라미터 수를 출력하는 코드를 추가했습니다. 마지막으로 훈련 루프를 구축할 준비가 되었습니다.\n\n# 훈련 루프\n\n모델을 훈련하기 위해서는 손실 함수가 필요합니다. 다음 토큰을 예측하므로 교차 엔트로피 손실을 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef loss_fn(model, x, y):\n    logits = model(x)\n    B, T, C = logits.shape # (batch_size, seq_len, vocab_size)\n    logits = logits.reshape(B*T, C)\n    y = y.reshape(B*T)\n    loss = nn.losses.cross_entropy(logits, y, reduction='mean')\n    return loss\n```\n\n먼저, 모델에서 로짓을 얻습니다. 그런 다음 로짓을 vocab_size 길이의 배열 목록으로 재구성합니다. 또한 정확한 토큰 ID 인 y를 동일한 길이로 재구성합니다. 그런 다음 내장된 교차 엔트로피 손실 함수를 사용하여 각 예제의 손실을 계산한 다음 이를 평균 내어 단일 값으로 얻습니다.\n\n```js\nmodel = GPT()\nmx.eval(model.parameters()) # 모델 파라미터 생성 (mlx는 게으른 평가)\nloss_and_grad = nn.value_and_grad(model, loss_fn)\nlr = 0.1\noptimizer = optim.AdamW(learning_rate=lr)\n```\n\n다음으로, 모델을 인스턴스화합니다. 그러나 mlx는 게으르게 평가되기 때문에 파라미터가 할당되고 생성되지 않습니다. 파라미터에 mx.eval을 호출하여 생성되도록 보장해야 합니다. 그런 다음 nn.value_and_grad()를 사용하여 손실 및 모델 파라미터의 그래디언트를 반환하는 함수를 얻을 수 있습니다. 이것이 우리가 최적화하는 데 필요한 모든 것입니다. 마지막으로 AdamW 옵티마이저를 초기화합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nnn.value_and_grad()에 대한 간단한 설명입니다. PyTorch를 사용해 보신 분이라면 loss.backward()를 사용할 것을 기대할 수 있습니다. 이 명령은 계산 그래프를 통과하며 모델 내 각 텐서의 .grad 속성을 업데이트합니다. 그러나 mlx의 자동 미분은 계산 그래프가 아닌 함수에 적용됩니다 [3]. 따라서 mlx에는 nn.value_and_grad()와 같이 함수를 입력받아 기울기 함수를 반환하는 내장 함수가 있습니다.\n\n이제 학습 루프를 정의해 보겠습니다.\n\n```js\nnum_epochs=20\nbatch_size=32\nfor epoch in range(num_epochs):\n    model.train(True)\n    running_loss = 0\n    batch_cnt = 0\n    for input, label in get_batches(X_train, y_train, batch_size):\n        batch_cnt += 1\n        loss, grads = loss_and_grad(model, input, label)\n        optimizer.update(model, grads)\n        running_loss += loss.item()\n        # 새로운 매개변수 및 옵티마이저 상태 계산\n        mx.eval(model.parameters(), optimizer.state)\n    avg_train_loss = running_loss / batch_cnt\n    model.train(False) # 평가 모드로 설정\n    running_loss = 0\n    batch_cnt = 0\n    for input, label in get_batches(X_val, y_val, batch_size):\n        batch_cnt += 1\n        loss = loss_fn(model, input, label)\n        running_loss += loss.item()\n    avg_val_loss = running_loss / batch_cnt\n    print(f\"Epoch {epoch:2} | train = {avg_train_loss:.4f} | val = {avg_val_loss:.4f}\")\n```\n\n외부 루프는 에포크를 거칩니다. 먼저 일부 모듈이 드롭아웃과 같이 학습 및 테스트 중에 다른 동작을 하는 경우가 있으므로 모델을 학습 모드로 설정합니다. 그런 다음 이전에 사용한 get_batches 함수를 사용하여 학습 데이터 배치를 반복합니다. 배치 단위로 손실과 기울기를 얻습니다. 그런 다음 모델과 기울기를 옵티마이저에 전달하여 모델 매개변수를 업데이트합니다. 마지막으로 매개변수 및 옵티마이저 상태가 업데이트되도록 mx.eval을 호출합니다(mx는 지연 평가를 수행하는 것을 기억하세요). 그런 다음 데이터의 평균 학습 손실을 계산하여 나중에 인쇄합니다. 이는 학습 데이터 한 번 통과입니다. 비슷하게 검증 손실을 계산하고 나서 에포크에서 평균 학습 및 검증 손실을 인쇄합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\ncompletion = decode(model.generate(1000)[0].tolist())\nprint(completion)\nwith open('completions.txt', 'w') as f:\n    f.write(completion)\n\n\n마지막으로, 모델에서 생성하는 코드를 추가합니다. 생성 결과는 여전히 (B, T) 형태이므로 0에서 색인화하여 1차원으로 만든 다음 mlx 배열을 Python 리스트로 변환해야 합니다. 그런 다음 앞서 설명한 decode 함수에 전달하고 파일에 쓸 수 있습니다.\n\n다음은 학습에 사용할 매개변수입니다 (이를 변경해보실 수 있습니다):\n\n\nctx_len = 128\nn_emb = 128\ndropout = 0.1\nhead_size = 128\nn_heads = 4 \nn_layers = 3 \nnum_epochs = 20\nbatch_size = 64\nlr = 1e-3\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 파일을 실행하여 훈련을 시작할 수 있습니다. 위의 설정으로 훈련을 진행하면 m2 맥북에서 약 10분이 걸렸어요. 지난 에포크에서 다음과 같은 훈련 손실을 얻었어요.\n\n```js\n에포크 19 | 훈련 = 1.6961 | 검증 = 1.8143\n```\n\n일부 출력을 살펴보겠습니다.\n\n```js\nGLOUCESTER:\nBut accomes mo move it.\n\nKING EDWARD:\nWhere our that proclaim that I curse, or I sprithe.\n\nCORIOLANUS:\nNot want:\nHis bops to thy father\nAt with hath folk; by son and fproathead:\nThe good nor may prosperson like it not,\nWhat, the beggares\nMore hath, when that made a,\nYour vainst Citizen:\nLet here are go in queen me and knife\nTo my deserved me you promise: not a fettimes,\nThat one the will not.\n\nCORIOLANUS:\nAnd been of queens,\nThou to do we best!\n\nJULIET:\nNot, brother recourable this doth our accuse\nInto fight!\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아주 작은 모델로 10분 동안의 훈련만으로 이 정도면 꽤 좋지 않나요? 문자를 예측하는데 셰익스피어 형식 같네요, 비록 무의미하긴 하지만요. 우리 모델과 실제 GPT-2의 유일한 차이는 이제 규모 뿐이에요! 이제 실험해보고 싶네요 — 다양한 설정을 시도해보거나 아키텍처를 잠시 건드려서 얼마나 낮은 손실을 달성할 수 있는지 확인해보세요.\n\n# 참고 문헌\n\n[1] Karpathy A (2015). Tiny Shakespeare [데이터 세트]. https://github.com/karpathy/char-rnn (MIT 라이선스)\n\n[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, 언어 모델은 비지도 멀티태스크 학습자입니다 (2019), OpenAI\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[3] Automatic Differentiation — mlx docs","ogImage":{"url":"/assets/img/2024-06-19-GPTfromScratchwithMLX_0.png"},"coverImage":"/assets/img/2024-06-19-GPTfromScratchwithMLX_0.png","tag":["Tech"],"readingTime":41},{"title":"LSTM 구조를 사용하여 설명하는 어텐션","description":"","date":"2024-06-19 02:58","slug":"2024-06-19-AttentionexplainedusingLSTMarchitecture","content":"\n\n![이미지](/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_0.png)\n\nNLP 엔지니어로서 자주 들어본 말 중 하나가 'Attention(주의)!‘입니다. 트랜스포머(Transformer)와 GPT에 대해 배우기 시작한 사람들에게는 혼란스러울 수 있습니다. 그러나 기계가 보다 긴 시퀀스에서도 맥락 정보를 유지하는 방법을 알아야 합니다. \n\n이 블로그에서는 먼저 LSTM이 무엇인지, 그 단점들은 무엇이었는지, 그리고 어떻게 attention 메커니즘이 이를 극복하는 데 도움이 되었는지 알아보겠습니다.\n\n# Attention이 필요한 이유?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n신경망 기술인 트랜스포머가 등장하기 전에 사용되던 기본 빌딩 블록은 인코더-디코더 LSTM 아키텍처였어요.\n\nLSTM 아키텍처는 주로 3개의 게이트로 구성돼 있어요. 각 게이트는 셀 상태로의 정보 흐름을 제어하는 역할을 합니다. 각 게이트와 그 기능에 대한 간단한 개요는 다음과 같아요:\n\n- 입력 게이트: 현재 입력에서 얼마나 많은 새로운 정보가 셀 상태에 추가돼야 하는지를 제어합니다. 현재 입력과 이전 숨겨진 상태를 가져와서 시그모이드 활성화 함수를 통과시켜 0과 1 사이의 값을 생성하고, 이 값을 현재 입력과 이전 숨겨진 상태를 통과시켜 생성된 후보 셀 상태에 곱해줍니다(이 값은 tanh 활성화 함수를 거칩니다).\n- 잊기 게이트: 이전 셀 상태 중 어느 부분을 유지하거나 잊을지를 결정합니다. 이전 숨겨진 상태와 현재 입력을 가져와서 시그모이드 활성화 함수를 통과시킵니다. 0과 1 사이의 결과값을 얻어 이전 셀 상태에 곱해줍니다. 이는 이전 셀 상태를 얼마나 유지할지를 결정합니다.\n- 출력 게이트: LSTM 셀의 출력과 다음 숨겨진 상태에 노출돼야 하는 셀 상태 얼마나 많은지를 결정합니다. 현재 입력과 이전 숨겨진 상태를 가져와서 시그모이드 활성화 함수를 통과시킵니다. 이 값은 현재 셀 상태의 tanh 값에 곱해져 다음 숨겨진 상태를 생성합니다.\n\n![LSTM 아키텍처를 이용한 어텐션](/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_1.png) \n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 표는 LSTM이 장기 의존성을 효과적으로 포착하기 위해 시간이 지남에 따라 셀 상태를 유지하고 조정할 수 있도록 함께 작동하는 게이트를 보여줍니다. 그러나 위 구조에서 입력의 문맥 길이가 증가하면 LSTM이 이러한 게이트에 모든 필요한 문맥을 저장하는 것이 어려워집니다. 예를 들어, \"자주 피우지 마세요, 이것은 당신의 폐에 강력한 영향을 미치고 더 심각한 합병증으로 이어질 것입니다.\" 라는 문장을 생각해보십시오. 위 문장에서 모델이 \"자주\"를 잊어버리면 문장 전체의 의미가 변경될 수 있습니다.\n\n그래서 이를 피하기 위해 어텐션(Attention)이 도입되었습니다. 이 기법을 도입한 주된 목적은 디코더가 다음 토큰을 예측할 때 인코더의 각 입력 토큰의 영향을 받도록 하는 것입니다. 이렇게 하면 입력의 문맥 길이에 관계없이 디코더가 모든 단어에 액세스할 수 있습니다. 유사도 점수에 따라 특정 입력 토큰이 다른 것보다 예측에 더 많은 영향을 미치도록 허용됩니다.\n\n# 어텐션은 어떻게 작동하나요?\n\n![LSTM 아키텍처를 사용하여 어텐션이 설명된 그림](/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 아키텍처 다이어그램에서는 영어에서 스페인어로 번역 작업에 인코더-디코더 모델이 사용됩니다. 인코더는 LSTM의 단일 레이어를 사용하며 해당 레이어에 2개의 LSTM 셀이 있습니다. LSTM이 다음 단어를 인코딩할 때, 각 LSTM 레이어에 동일한 가중치와 편향이 사용됩니다. 디코더에는 다른 가중치와 편향 세트가 있습니다. 이 인코더-디코더 아키텍처의 구분은 가변 길이의 입력과 출력을 번역하는 데 도움이 됩니다. 예를 들어, \"Let's go\"가 \"Vamos\"로 번역됩니다. 디코더는 'EOS' 토큰에 도달하거나 최대 출력 단어 제한에 도달했을 때 단어 생성을 중지합니다.\n\n인코더에서 나오는 컨텍스트 벡터는 해당 LSTM 레이어의 각 단어인 \"let's\"와 \"go\"에 대한 정보를 제공하는 추가 데이터와 함께 디코더로 전달됩니다.\n\n이를 위해 LSTM 인코더의 단기 메모리에서 가져온 값은 현재 디코더 상태와 코사인 유사도를 사용하여 비교됩니다. 마찬가지로, 두 번째로 펼쳐진 LSTM 네트워크의 출력은 현재 디코더 상태와 다시 계산됩니다. 이 유사도 점수는 각 입력 토큰에서 각 출력 토큰과 계산되며 이 점수는 Softmax 함수를 통과하여 확률로 변환됩니다.\n\nSoftmax 함수 공식:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_3.png)\n\n이 확률 점수는 각 입력 토큰이 해당 출력 토큰에 미치는 영향의 양을 나타냅니다. 그런 다음 인코더 출력은 해당 확률 점수에 따라 조정되어 인코더 출력의 가중 합인 컨텍스트 벡터를 계산합니다. 마지막으로 이 벡터는 현재 디코더 상태 벡터에 추가되어 다음 상태의 확률 점수를 생성하기 위해 완전 연결 네트워크를 통과합니다. 디코더 어휘에서 가장 높은 확률 점수를 가진 토큰이 예측되고, `EOS` 토큰이 예측되거나 출력 단어 제한이 도달할 때까지 디코더의 다음 레이어로 전달됩니다.\n\n이 아이디어를 통해 LSTM 모델의 필요성이 결국 사라지고 Transformer가 대세가 되었습니다.\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 어텐션(Attention)이 언어 모델이 입력에서 각 단어의 중요성을 이해하는 데 어떻게 도와주는지 설명합니다. 트랜스포머(Transformer) 아키텍처에서, 셀프-어텐션(Self-Attention)과 마스크드 셀프-어텐션(Masked Self-Attention)에 대해 이해할 수 있습니다.\n\n# 참고 자료\n\nStatquest: Joshua Starmer https://youtube.com/@statquest?si=sX9sq6vUnjzVbhCr\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” arXiv, August 1, 2023. https://doi.org/10.48550/arXiv.1706.03762.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLST Architecture Diagram – [Link](https://towardsdatascience.com/lstm-recurrent-neural-networks-how-to-teach-a-network-to-remember-the-past-55e54c2ff22e)","ogImage":{"url":"/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_0.png"},"coverImage":"/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_0.png","tag":["Tech"],"readingTime":4},{"title":"AI 사용 사례는 근본적으로 다릅니다","description":"","date":"2024-06-19 02:56","slug":"2024-06-19-AIUseCasesareFundamentallyDifferent","content":"\n\nAI를 통합하는 성공은 적절한 AI 사용 사례를 선택하는 데 크게 의존합니다. 이것은 알고리즘, 데이터 및 엔지니어링에 집중하기 전 제품 중심의 기술 전문가들에게 제시되는 관점입니다.\n\nAI 프로젝트가 종종 실패하는 이유는 실제로 시작조차 하지 않았기 때문입니다. 적절한 인간 문제를 해결하지 않거나 사용자 기대치를 충족할만한 최소 성능 기준을 충족하지 않았기 때문입니다.\n\nAI는 소프트웨어 버전의 덕트 테이프와 같습니다. 여러 가지에 유용하지만 모든 것에는 해당되지 않습니다. 마찬가지로, 훌륭한 제품은 사람들이 특정 작업을 아주 효과적으로 수행할 수 있도록 돕습니다. 그러나 AI는 여러 가지를 보통 잘합니다.\n\nAI가 특별히 잘 작동하는 사용 사례를 찾는 것은 효과적이지 않습니다. 이는 AI의 본성 상의 확률적 특성을 무시합니다. 대신, 중간 수준의 AI 성능을 갖는 사용 사례를 찾으면 더 낮은 리스크로 즉각적 가치를 제공할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전에 수행된 작업은 일반적인 컴퓨팅과 구글의 숙취 섬 비유를 통해 이 주제를 조사했습니다.\n\n중간 수준의 AI 성능을 효과적으로 활용하는 사용 사례들은 반복적인 성공 이력을 갖고 있습니다. 본문에서는 최종 사용자가 센서 퓨전, 생성형 AI, 자연어 처리, 컴퓨터 비전, 자율 로봇을 포함한 5가지 범주에서 제품 예시를 통해 AI를 경험하는 방법에 대해 설명합니다: \n\n- 센서 퓨전\n- 생성형 AI\n- 자연어 처리\n- 컴퓨터 비전\n- 자율 로봇\n\n# 센서 퓨전\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여러 센서에서 수집한 데이터를 결합하면 더 정확하고 완전한 표현을 얻을 수 있어요. 이러한 형태의 인공 지능은 하드웨어와 소프트웨어 제품에서 가장 일반적으로 사용돼요.\n\n한 가지 간단한 예는 보행자 카운터예요. 보행자 카운터는 운동을 즐겨하고 매일 운동을 할 수 있게 동기부여해 줘요. 하지만 인공 지능이 너무 우수하게 작동할 필요는 없어요. 그저 합리적으로 잘 작동하면 유용하게 활용할 수 있어요.\n\n소비자들에게는 정확도가 90%인 보행자 카운터(10걸음 중 1걸음 오류)가 일반적으로 매일의 피트니스 모니터링에 유용하게 사용돼요. 의도적인 디자인은 중간 정도의 인공 지능 성능을 보완할 수도 있어요. 예를 들어, 사용자에게 정확한 걸음 수가 중요하지 않고, 그저 1만걸음을 달성하거나 반지를 마무리하는 등의 목표에 도달하는 것이 중요하게 된답니다.\n\n기업들에게는 보행자 카운터가 특수 임상 연구와 같이 높은 정확도(95~99%)를 요구할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n둘 중 하나에 대한 경우에도 AI는 독특하게 가치가 있습니다. 왜냐하면 존재하는 것이 다른 대안들보다 낫기 때문입니다: (1) 자신의 걸음 수를 세는 사람 또는 (2) 아무 것도 아닌 것. 중간 AI 성능이 유용한 유사한 상황: 사진 태깅을 위한 얼굴 인식, 실시간 번역, 쇼핑 추천, 음성메시지 전사, 배터리 수명 최적화.\n\n모든 이들은 좁은 AI의 예시로, 종종 눈에 띄지 않지만 일상생활에서 자주 나타납니다. 좁은 AI는 특정 작업(걸음 수 세기)을 위해 설계되었으며, 특정 맥락(활동 추적) 내에서 사용할 목적으로 설계되었습니다. 이러한 AI 능력을 사용하기 위해 설계된 제약 조건들로 인해, 대규모 기업들에 의해 일반적으로 배포됩니다.\n\n# 생성적 AI\n\n생성적 AI의 본질은 같은 결과를 두 번 제공하지 않을 것이라는 것입니다. 이것은 일반적으로 실수 및 일부 예측할 수 없는 상황이 받아들여지는 사용 사례에서 잘 작동합니다: 미술, 음악, 글쓰기, 영화 및 엔터테인먼트, 게임 등.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nKrea.ai, Superside, 및 Microsoft cocreator 같은 제품들은 AI를 창의적인 협업자로 위치시켜, 스케치를 전문적인 구성물로 변환합니다. AI는 충분히 잘 작동하고 합리적으로 좋은 시각물을 생산하기만 하면 됩니다.\n\n## 초보자, 취미가 있는 사람들, 전문가\n\n생성적 AI는 대부분 초보자에게 도움이 되어 빠르게 아이디어를 현실로 만들어줍니다. 초보 사용자들은 또한 AI가 상대적으로 잘 일을 할 것을 믿습니다. 그러나 전문가들은 AI가 어렵게 표현할 독특한 비전을 가지고 있습니다. 그리고 취미가 있는 사람들(예술가, 작가, 음악가 등)은 창조 과정의 감정적인 고점과 저점에서 행복한 우연을 경험하는데, AI에 의존함으로써 이 인간적 경험의 보존을 도전받게 됩니다.\n\n## 다목적 AI \u0026 윤리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n생성 AI는 다목적입니다. 그러나 좁은 범위의 AI와 달리 사람들이 마음대로 조작할 수 없기 때문에 거의 아무도 상처받지 않도록 안전하게 설계된 다목적 AI는 드뭅니다.\n\n다목적 AI는 여러 응용 분야에서 유용하지만 의도치 않은 결과에 미치는 영향에 노출될 수도 있습니다. 다양한 산업 분야에서 사용되는 경우, 모델이 내용을 무단으로 재생산하거나 이전 예시를 재조합하여 새로운 콘텐츠를 생산하는 사례가 많이 있습니다.\n\n그러나 적절한 상황에서 적용된다면 성능이 중간인 생성 AI조차도 극도로 유용할 수 있습니다: (1) 일부 오류가 허용되는 경우, (2) 무작위성이 사용자 경험을 향상시킬 때, (3) 0에서 1로 가기를 원할 때, (4) 기존 작업을 빠르게 변형하는 것이 원본 콘텐츠를 작성하는 것보다 유용할 때.\n\n# 자연어 처리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금 가장 널리 사용되는 AI 형태입니다. 소비자 및 기업 사용 사례에서 언어는 만연하며, 텍스트 데이터는 디지턈 상호작용에서 가장 접근하기 쉽습니다.\n\nNLP로 달성하기 어려운 사용 사례들은 일반적으로 도메인 전문지식과 세심한 언어 이해를 필요로 합니다: 비꼬는 말 감지, 복잡한 이야기 이해, 전문가의 의사결정 능력이 필요한 새로운 상황에 적응하는 것 등.\n\nNLP로 쉽게 달성할 수 있는 사용 사례들은 두 가지 시각에서 볼 수 있습니다: (1) 전문가가 아닌 인턴을 위한 작업, (2) AI가 대부분의 작업을 처리한 후, 인간이 나머지 작업을 완료하는 상황.\n\n- 전문가가 아닌 인턴을 위한 작업. Notion은 전체 작업 공간에서 관련 키워드를 찾아줍니다, Otter.ai는 말을 텍스트로 전사해줍니다, SummarizeBot은 문서를 요약으로 요약해줍니다, Paperpile은 연구자들이 논문을 찾고 인용하는 데 도움을 줍니다, iOS는 앱에서 텍스트를 번역하고 음성 메시지를 전사합니다. 모두 도메인 전문지식이 필요하지 않은 작업으로 워크플로우를 향상시키는 작업입니다.\n- AI가 선처리를 한 뒤 사람이 확인하는 방식. 스팸 필터는 이메일을 스팸 또는 비 스팸으로 분류하는 간단한 이진 분류기입니다. 뉴스 기사도 카테고리별로 레이블을 붙일 수 있습니다: 스포츠, 기술, 엔터테인먼트, 정치, 기타... 이러한 작업은 사람에게 시간과 오류가 많이 발생하는 간단한 작업이지만, AI에게는 비교적 빠르고 간단합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 렌즈는 AI가 대량의 반복 작업을 효과적으로 처리하여 인간의 실수가 발생하기 쉬운 작업에 집중할 수 있게 해줄 수 있는 상황을 설명합니다.\n\n# 컴퓨터 비전\n\n세상의 대부분 데이터는 시각적이고 공간적입니다. 언어가 가장 흔하지만 많은 데이터가 이미지나 공간으로 표현됩니다. \n\n세상을 모델링하는 것은 매우 어렵습니다. 자율 주행(포부)은 지속적으로 약속되지만 실현하기는 어렵습니다(상용화되지 않음):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자율 주차는 구축하기 어려우며 도시 주차를 두려워하는 소수의 운전자들에게만 도움이 됩니다. 하지만 어떤 주차 공간이 충분히 큰지 예측하는 간단한 기능으로 시작할 수 있습니다. 자동화된 AI 유지보수 시스템을 만드는 것은 어려울 수 있습니다. 그러나 사용자는 장비 사용량을 감시하고 시각 검사를 통해 고장을 추적하는 단순한 AI 기능을 쉽게 활용할 수 있습니다. 간단한 피드백 루프에서 시작하여 점차 데이터가 허용하는 한 복잡성으로 나아갈 수 있습니다. AI 헤드라인들은 각자 비전을 가진 사용 사례들을 약속하는 경향이 있어서 역사적으로나 최근에도 혼합된 결과를 가져왔습니다. 하지만 점진적인 AI 솔루션은 아직도 즐거운 완성된 제품이 될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 자율 로봇\n\n인간-기계 협력은 로봇 공학 분야에서 철저히 연구되어 왔습니다. 이러한 개념은 이제 디지털 제품 경험으로 전이되어, 공동 조종사, 에이전트 경험, AI 파트너, AI 협업이라고 브랜드화되고 있습니다.\n\n## 자율 수준\n\n로봇 공학에서 정의된 6개의 자율 수준을 고려해보세요. 이 수준은 자율이 없는 수준 0(스팸 필터)에서 완전히 자율적인 수준 6(다양한 작업을 독립적으로 처리할 수 있는 개인 비서)까지 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금 대부분의 AI 시스템은 1~5단계 사이에 위치하며, 사용자 피드백이 있는 반자율적 AI 시스템이다. 이러한 시스템은 두 가지 상호적응 에이전트, 즉 사용 조건에 따라 정확도가 변하는 AI 시스템과 사용자의 작업 전문성 및 AI 정신 모델이 다양한 사용자로 인해 설계하기 어려운 과제입니다.\n\n## AI 시스템 성능 대 디자인\n\n비자율적 및 완전자율적 시스템 모두, 사용자의 정신 모델은 상대적으로 동일하며, AI가 합리적으로 잘 작동하거나 독립적으로 작동하는 것을 예상하거나 아예 그렇지 않도록 합니다. 완전자율적 시스템은 필요할 때만 사용자에게 알려주므로 AI 시스템 성능에 전적으로 의존합니다.\n\n그러나 지금의 대다수 사용 사례는 반자율적 AI를 사용합니다. 이러한 시스템은 (1) 시스템 성능을 기반으로 사용자 기대치를 중재하고, (2) 시스템에 대한 사용자 신뢰에 따라 다양한 작업 흐름을 허용하는 디자인을 주로 의존합니다. 예를 들어, 사용자는 AI에 독립적으로 작동하도록 신뢰할 수도 있고, AI 시스템의 투명성을 원해 일부 감시를 통해 개입할 때가 있을 수도 있으며, 아예 AI 없이 작업을 완료할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n성공적인 예 중 하나는 로봇 청소기인 룸바(Roomba)입니다. 사람들은 룸바를 자유롭게 이동시킬 수 있습니다(완전 자율), 멈춰있을 때 개입할 수도 있습니다(반자율), 혹은 룸바의 일정을 일시 중지하고 수동으로 공간을 청소할 수도 있습니다(비자율).\n\n일부 지저분한 지점을 놓치거나 멈추는 등의 적당한 AI 성능에도 불구하고, 룸바는 여전히 엄청난 편의성을 제공합니다. 로봇의 의도가 명확하기 때문에 사용자에게 특이한 간단한 인간-기계 제휴 관계입니다.\n\n시스템 성능이 향상되고 제품의 현실적인 가치를 위해 더 많이 사용됨에 따라, 사용자들은 시스템에 대한 신뢰도도 증가합니다.\n\n# AI 사용 사례는 근본적으로 다릅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI 성능이 빠르게 향상되고 있습니다. 하지만 전통적인 소프트웨어와는 달리, AI 결과는 본질적으로 결정론적이 아닙니다.\n\nAI 프로젝트의 도전 과제는 이미 시사되어 있는 해결책이 있는 것입니다. 성공적인 프로젝트는 먼저 문제를 효과적으로 판매할 때 주목받습니다. 하지만 가치 있는 사용자 문제부터 시작하는 대신, AI 프로젝트에 착수하기는 이미 AI로 결정되어 있는 것처럼 보입니다.\n\n이것은 사용 사례를 찾는 다른 방법을 제안하며, 확률적 시스템에 사용자의 필요성을 매칭시키는 것을 의미합니다.\n\nAI가 탁월하게 수행해야 하는 사용 사례는 기대를 충족시키지 못할 수 있습니다. AI를 모든 상황, 모든 사용자에게 항상 합리적으로 구현할 수 없는 높은 기준을 설정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n중간 수준의 AI 성능이 여전히 가치 있는 사용 사례를 찾는 것은 이미 사용자가 가치를 얻는 기본선 상에서 시작됩니다.\n\n정확도가 향상되고 환각이 줄어들면 사용자 경험도 자연스럽게 향상됩니다.\n\n이 기사의 서문을 확인해보세요:\n\n## 읽어주셔서 감사합니다! 여러분의 생각을 알고 싶어요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nElaine 씨는 디자인, 인공 지능, 신흥 기술 등에 대해 글을 씁니다. 더 많은 소식을 받으려면 팔로우해 주세요. 또는 LinkedIn에서 연락을 주세요.","ogImage":{"url":"/assets/img/2024-06-19-AIUseCasesareFundamentallyDifferent_0.png"},"coverImage":"/assets/img/2024-06-19-AIUseCasesareFundamentallyDifferent_0.png","tag":["Tech"],"readingTime":7},{"title":"고급 계산을 위한 클러스터 컴퓨터 활용 포괄적 가이드","description":"","date":"2024-06-19 02:54","slug":"2024-06-19-UtilisingaClusterComputerforAdvancedComputationsAComprehensiveGuide","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-UtilisingaClusterComputerforAdvancedComputationsAComprehensiveGuide_0.png\" /\u003e\n\n계산, 프로그래밍, 기계 학습, 신경망, 인공 지능 및 로보틱스를 위해 조립된 클러스터 컴퓨터를 사용하는 것은 몇 가지 단계가 필요합니다. 여기에 시작하는 데 도움이 되는 포괄적인 안내서가 있습니다:\n\n클러스터 설정\n\n하드웨어 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n클러스터 내의 각 컴퓨터(노드)가 네트워크에 올바르게 연결되었는지 확인해주세요.\n\n이더넷 또는 인피니밴드와 같은 고속 네트워킹 하드웨어를 사용해주세요. 모든 노드에 안정적인 전원 공급을 확인해주세요.\n\n밀집된 연산 중에 오버히팅을 방지하기 위해 적절한 냉각이 되어 있는지 확인해주세요.\n\n## 소프트웨어 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 운영 체제: 모든 노드에 Linux 배포판을 설치하세요. 고성능 컴퓨팅에서 널리 사용되는 운영 체제입니다.\n- 네트워킹: 모든 노드가 통신할 수 있도록 네트워킹을 구성하세요. 일반적으로 노드 간 비밀번호 없는 통신을 위해 SSH 키를 설정하는 것이 필요합니다.\n- 클러스터 관리 소프트웨어: Apache Hadoop, Kubernetes 또는 OpenMPI와 같은 클러스터 관리 소프트웨어를 설치하여 노드 간 작업을 관리하고 조율하세요.\n\n환경 설정하기\n\n분산 파일 시스템\n\n- HDFS: Hadoop 기반 설정에 대해, Hadoop 분산 파일 시스템(HDFS)을 설치하고 구성하여 노드 간 데이터를 관리하세요.\n- NFS/GlusterFS: Hadoop이 아닌 설정에 대해, NFS 또는 GlusterFS를 공유 저장소로 활용해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n소프트웨어 라이브러리 및 도구\n\n- 프로그래밍 언어: 필요한 프로그래밍 언어(예: Python, C++, Java)가 설치되어 있는지 확인합니다.\n- 라이브러리: 머신러닝 및 데이터 처리를 위한 관련 라이브러리를 설치합니다 (예: TensorFlow, PyTorch, Scikit-Learn, NumPy).\n- 컨테이너화: Docker 또는 Singularity를 사용하여 클러스터 전체에서 일관된 환경을 생성하고 배포합니다.\n\n작업 구성 및 실행\n\n## 작업 예약\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n작업 예약 소프트웨어인 SLURM, PBS 또는 Kubernetes를 사용하여 작업을 관리하고 리소스를 할당하세요.\n\n필요한 리소스를 지정하는 작업 스크립트를 작성하세요(예: 노드 수, CPU/GPU 요구 사항) 및 실행할 작업.\n\n프로그래밍 및 개발\n\n- 분산 컴퓨팅: 여러 노드를 활용할 수 있는 프로그램을 작성하세요. MPI (메시지 패싱 인터페이스) 애플리케이션의 경우 OpenMPI 또는 MPICH와 같은 라이브러리를 사용하세요.\n- 병렬 처리: Python의 Dask나 대용량 데이터 처리를 위한 Spark와 같은 병렬 처리 라이브러리를 사용하세요.\n- 머신 러닝 프레임워크: TensorFlow나 PyTorch와 같은 분산 훈련을 위해 프레임워크를 구성하세요. 이는 매개변수 서버와 워커 노드 설정을 포함할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특정 응용 프로그램\n\n기계 학습 및 신경망\n\n- 분산 훈련: TensorFlow의 tf.distribute.Strategy 또는 PyTorch의 torch.distributed를 사용하여 여러 노드에 걸쳐 훈련을 분산시킵니다.\n- 데이터 처리: 데이터 파이프라인이 모델에 데이터를 효율적으로 공급할 수 있도록 하고, 분산 데이터 저장 및 처리 시스템을 사용할 수 있습니다.\n\n인공 지능\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 추론: 대규모 추론 작업을 위해 클러스터 전체에 훈련된 모델을 배포합니다.\n- 모델 서빙: TensorFlow Serving 또는 TorchServe와 같은 도구를 사용하여 모델 서빙을 관리하고 확장합니다.\n\n로봇공학\n\n- 시뮬레이션: Gazebo와 같은 도구를 활용하여 복잡한 고품질 시뮬레이션을 클러스터를 활용하여 실행합니다.\n- 제어 알고리즘: 실시간으로 실행될 수 있는 고급 제어 알고리즘을 개발하고 클러스터 전체에 분산하여 배포합니다.\n\n모니터링 및 유지보수\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**모니터링 도구**\n\n- 리소스 모니터링: 프로메테우스, 그라파나 또는 갱글리아 같은 모니터링 도구를 사용하여 클러스터의 성능과 리소스 사용량을 추적합니다.\n- 로깅: ELK 스택(Elasticsearch, Logstash, Kibana)이나 플루언트드 같은 도구를 활용하여 중앙 집중식 로깅을 구현하여 모든 노드에서 로그를 수집하고 분석합니다.\n\n**유지 보수**\n\n소프트웨어 및 라이브러리를 최신 상태로 유지하여 보안과 호환성을 보장하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n중요한 데이터와 구성을 정기적으로 백업하여 데이터 손실을 예방하세요.\n\n예제 워크플로우\n\n다음은 클러스터에서 머신 러닝 모델을 학습하는 예제 워크플로우입니다:\n\n- 데이터 준비: HDFS 또는 다른 분산 파일 시스템을 사용하여 데이터를 저장하고 전처리합니다.\n- 환경 설정: Docker를 사용하여 모든 종속성이 포함된 컨테이너를 생성하고 노드 간에 배포합니다.\n- 작업 제출: SLURM을 사용하는 작업 스크립트를 작성하여 리소스를 요청하고 TensorFlow를 사용하여 분산 훈련 작업을 실행합니다.\n- 모델 훈련: 스크립트는 여러 노드 간에 훈련을 시작하며, 각 노드는 모델 훈련 프로세스의 일부를 실행합니다.\n- 모니터링: Grafana를 사용하여 훈련 진행 상황과 리소스 이용을 모니터링합니다.\n- 모델 배포: 훈련이 완료되면 TensorFlow Serving을 사용하여 모델을 대규모 추론에 배포합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n고급 계산을 위한 클러스터 컴퓨터를 설정하고 사용하는 것은 하드웨어와 소프트웨어의 신중한 계획과 구성을 필요로 합니다. 분산 컴퓨팅 프레임워크, 기계 학습 라이브러리 및 효과적인 모니터링 도구를 활용하여 클러스터 전반에 걸쳐 복잡한 계산과 AI 애플리케이션을 효율적으로 실행할 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-UtilisingaClusterComputerforAdvancedComputationsAComprehensiveGuide_0.png"},"coverImage":"/assets/img/2024-06-19-UtilisingaClusterComputerforAdvancedComputationsAComprehensiveGuide_0.png","tag":["Tech"],"readingTime":4},{"title":"신기한 여행 스탠포드의 매직 월드 크리에이터","description":"","date":"2024-06-19 02:53","slug":"2024-06-19-WonderJourneyStanfordsMagicalWorldCreator","content":"\n\n제 뉴스레터에서 최근 세계적인 Fei-Fei Li의 Stanford 연구소에서 만든 최신 모델에 대해 이야기했었죠. 그 모델은 명령으로 무한한 마법의 3D 세계를 만들어냈습니다.\n\n하지만 저는 그 모델의 깊은 부분까지 파헤치게 되었고, 그 모델에 너무 매료되어 세계에서 가장 진보된 텍스트/이미지-3D 모델이 어떻게 작동하는지 상세히 설명하고 싶어졌습니다.\n\n이 모델은 게임, 가상 현실, 증강 현실, 혼합 현실에 혁명을 일으킬 수 있을 뿐만 아니라, Fei Fei Li 자신이 시사한 대로, AI가 우리의 세계를 더 잘 이해할 수 있는 세계 모델을 만드는 데 도움이 될 수 있습니다.\n\n# 마법 창조하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nWonderJourney는 텍스트 설명 또는 이미지를 기반으로 무한하지만 일관된 3D 장면을 생성하는 AI 모델입니다.\n\n시각적으로 완벽하지는 않지만 비디오의 모든 것이 완전히 AI로 생성된 것이라는 아이디어는 정말 놀라운 것이죠.\n\n하지만 이것이 어떻게 작동하는 걸까요?\n\n## 모듈식 접근\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nWonderJourney는 세 가지 구성 요소로 나뉩니다:\n\n- LLM: 다음 장면을 생성하는 데 책임을 지는 대형 언어 모델(Large Language Model).\n- VSG(Visual Scene Generator): LLM의 다음 장면 텍스트 설명과 현재 장면 이미지를 입력으로 받아 다음 3D 장면을 생성하는 모델.\n- VLM validator: 새로 생성된 장면을 검사하고 품질이 충분히 높지 않은 경우 재시도를 요청하는 Vision Language Model.\n\n전체 모델의 표현은 다음과 같습니다:\n\n![WonderJourney Model](/assets/img/2024-06-19-WonderJourneyStanfordsMagicalWorldCreator_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이해하기 어려운 내용이죠. 다양한 기술적 구성 요소에 대해 설명하기 전에 완전한 예시를 보는 것이 좋습니다:\n\n- 사용자가 요청한 내용: \"산속의 아늑한 마을, 자갈길과 나무집이 있는 곳. 뒤쪽에 눈을 덮은 봉우리가 솟아 있습니다.\"\n- LLM이 새로운 장면 생성: \"아늑한 마을을 지나 숲으로 들어가면, 땅에 입체적인 그림자를 드리우는 덤불들이 보입니다.\"\n- Visual Scene Generator가 다음 3D 장면을 생성하며 마을을 멀리 뒤쪽에 밀어 넣어 숲으로 이동하는 느낌을 전달합니다.\n- VLM이 새로운 장면을 확인하고, 우리는 이 과정을 반복합니다.\n\n![이미지](/assets/img/2024-06-19-WonderJourneyStanfordsMagicalWorldCreator_1.png)\n\n이제 전체적인 파이프라인을 이해했으니, 생각해볼 점은 이 모든 것이 어떻게 작동하는 걸까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 다양한 모델 집합\n\n원더저니의 멋진 점 중 하나는 그 구성 요소들이 모듈식이라는 것입니다. 즉, 좋아하는 LLM, VSG 또는 VLM을 사용할 수 있다는 뜻이죠. 그러나 원더저니에는 몇 가지 중요한 모델이 포함되어 있습니다.\n\n따라서, 이해를 돕기 위해 3D 장면이 어떻게 생성되고 카메라의 역할에 대해 상세히 설명하면서 전반적인 내용을 설명하는 것이 좋을 것 같아요.\n\n## 2D에서 3D로, 그리고 그 반대로\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3D 장면을 논의할 때, 화상의 경우를 제외하고는 여전히 2D 화면에 표시해야 합니다 (가상 헤드셋 제외, WonderJourney의 사용 사례가 아닙니다).\n\n다시 말해, 전반적인 작업의 많은 부분은 3D 장면이 \"카메라\"의 2D 렌즈 (사용자의 화면)로부터 어떻게 보여질지 계산하는 것입니다.\n\n그러므로, 새로운 장면을 사용할 때마다, WonderJourney는 2D 이미지를 사용해서 새로운 3D 장면을 생성하지만, 결국 이 장면은 화면에 다시 2D로 투영됩니다.\n\n이를 알고 나면 이제 WonderJourney가 어떻게 작동하는지 이해할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 깊이, 포인트 클라우드 및 투영\n\n새로운 장면을 생성하는 전체 프로세스는 다음과 같습니다:\n\n![이미지](/assets/img/2024-06-19-WonderJourneyStanfordsMagicalWorldCreator_2.png)\n\n- 먼저, 모델은 초기 조건으로 텍스트 또는 이미지를 취하며, 사용자가 텍스트 설명만을 제공한 경우 이미지를 생성합니다.\n- 이 입력을 사용하여, 먼저 이미지에서 각 요소의 깊이를 추정하며, 즉, 각 객체가 실제로 얼마나 먼지 가까운지를 추정합니다 (예: 하늘은 항상 '멀리있음'으로 추정되어야 합니다).\n- 다음 단계는 깊이 추정으로부터 포인트 클라우드를 생성하는 것이며, WonderJourney는 이를 정제하여 요소 사이의 '날카로운' 경계를 보장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나, 저희는 여전히 현재 뷰를 작업 중이며 새로운 뷰가 필요합니다. 이를 위해 2D 화면(우리)에서 3D 장면을 볼 '카메라'는 뒤로 밀려서 이전 장면이 멀리 뒤쪽에 나타나도록 하여 '우리가 그것에서 멀어지고 있다'는 아이디어를 전달합니다. 이제 다음 장면에 나타날 현재 장면의 일부가 위치하고, 새 프레임의 그 부분이 렌더링됩니다.\n\n그런 다음, 이 부분적 렌더링과 다음 장면의 LLM(언어-이미지 모델) 설명을 사용하여 WonderJourney는 나머지 장면을 outpaint하는데, 이 경우에는 Stable Diffusion 모델을 사용하지만, 중요한 건 LLM의 새로운 장면이어야 하는 내용에 따라 조건이 부여된다는 것입니다.\n\n마지막으로, 새 이미지를 얻었으면, 우리는 간단히 깊이 추정 및 정제 프로세스를 반복하여 필요에 따라 여러 가지 대안적인 뷰를 생성하여 새로운 포인트 클라우드를 만듭니다.\n\n이 새로운 포인트 클라우드는 본질적으로 새로운 3D 뷰를 구축하며, VLM(시각 언어 모델)이 평가합니다. 품질 임계값을 충족하면, 해당 장면이 뷰어의 2D 화면에 투사되고, 프로세스가 반복됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 있어요, 끝없이 이어지고 항상 일관된 3D 여행이 마련되었습니다.\n\n# 모든 형태의 정복\n\n한 번 더, 학계는 사용자의 텍스트 또는 이미지 명령에 기반한 무한한 3D 장면을 생성할 수 있는 첫 번째 모델의 가능성을 확장했습니다.\n\n마지막으로, Fei Fei Li의 생각을 빌리자면, 강력한 3D 생성 모델을 구축하는 것은 AI에게 우리 세상에 대한 큰 공간적 이해력을 제공할 수 있으며, 이는 그들의 지능을 향상시키는 방법으로 활용될 수 있고, 누가 알겠으나, 구체적 AI 모델, 즉 인간형 로봇의 등장을 용이하게 할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현실 세계와 상호 작용할 수 있는 능력을 기계에 부여하면, 시뮬레이션 환경을 통해라도 사람들과 격차를 줄일 수 있을 것입니다. 우리는 관찰과 세상과의 상호 작용을 통해 배우기 때문에, 오늘날의 최첨단 AI 모델들을 뛰어넘는 능력을 갖게 됩니다, 어떻게 ChatGPT나 Claude가 감탄을 자아내는지와 상관없이요.\n\n우리가 이것이 옳은 길이라고 주장할 수는 없겠지만, 텍스트를 단순히 모델에 던지고 AGI로 성장할 것을 바라는 것보다는 훨씬 매력적으로 보입니다.","ogImage":{"url":"/assets/img/2024-06-19-WonderJourneyStanfordsMagicalWorldCreator_0.png"},"coverImage":"/assets/img/2024-06-19-WonderJourneyStanfordsMagicalWorldCreator_0.png","tag":["Tech"],"readingTime":4}],"page":"96","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"96"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>