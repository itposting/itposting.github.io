<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/25" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/25" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="ComfyUI 설치 가이드 Linux, Windows" href="/post/2024-06-22-InstallingComfyUILinuxWindows"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ComfyUI 설치 가이드 Linux, Windows" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-InstallingComfyUILinuxWindows_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ComfyUI 설치 가이드 Linux, Windows" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">ComfyUI 설치 가이드 Linux, Windows</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AI 아트 프롬프트의 구성 요소 분석" href="/post/2024-06-22-TheAnatomyofanAIArtPrompt"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AI 아트 프롬프트의 구성 요소 분석" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-TheAnatomyofanAIArtPrompt_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AI 아트 프롬프트의 구성 요소 분석" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">AI 아트 프롬프트의 구성 요소 분석</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AI 얼굴 스왑 배틀 PuLID vs InstantID vs FaceID" href="/post/2024-06-22-AIfaceswapbattlePuLIDvsInstantIDvsFaceID"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AI 얼굴 스왑 배틀 PuLID vs InstantID vs FaceID" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-AIfaceswapbattlePuLIDvsInstantIDvsFaceID_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AI 얼굴 스왑 배틀 PuLID vs InstantID vs FaceID" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">AI 얼굴 스왑 배틀 PuLID vs InstantID vs FaceID</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="파이썬을 사용한 신호처리를 위한 생성적 적대 신경망GAN 실습 가이드" href="/post/2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="파이썬을 사용한 신호처리를 위한 생성적 적대 신경망GAN 실습 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="파이썬을 사용한 신호처리를 위한 생성적 적대 신경망GAN 실습 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">파이썬을 사용한 신호처리를 위한 생성적 적대 신경망GAN 실습 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="초보자를 위한 디스코드에서 Midjourney 시작하기 단계별 가이드" href="/post/2024-06-22-GettingStartedwithMidjourneyonDiscordAStep-by-StepGuideforBeginners"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="초보자를 위한 디스코드에서 Midjourney 시작하기 단계별 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-GettingStartedwithMidjourneyonDiscordAStep-by-StepGuideforBeginners_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="초보자를 위한 디스코드에서 Midjourney 시작하기 단계별 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">초보자를 위한 디스코드에서 Midjourney 시작하기 단계별 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="NLP와 지식 그래프를 활용한 2024 최신 오피오이드 연구 방법" href="/post/2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="NLP와 지식 그래프를 활용한 2024 최신 오피오이드 연구 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="NLP와 지식 그래프를 활용한 2024 최신 오피오이드 연구 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">NLP와 지식 그래프를 활용한 2024 최신 오피오이드 연구 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Greg의 비디오에서 배우는 RAG 청킹 5단계 전략" href="/post/2024-06-22-FiveLevelsofChunkingStrategiesinRAGNotesfromGregsVideo"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Greg의 비디오에서 배우는 RAG 청킹 5단계 전략" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-FiveLevelsofChunkingStrategiesinRAGNotesfromGregsVideo_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Greg의 비디오에서 배우는 RAG 청킹 5단계 전략" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Greg의 비디오에서 배우는 RAG 청킹 5단계 전략</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Code Llama로 나만의 LLM 코딩 어시스턴트 만드는 방법 " href="/post/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Code Llama로 나만의 LLM 코딩 어시스턴트 만드는 방법 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Code Llama로 나만의 LLM 코딩 어시스턴트 만드는 방법 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Code Llama로 나만의 LLM 코딩 어시스턴트 만드는 방법 </strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="효율적인 HNSW 인덱싱 대규모 병렬 처리를 통한 인덱스 빌드 시간 단축 하는 방법" href="/post/2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="효율적인 HNSW 인덱싱 대규모 병렬 처리를 통한 인덱스 빌드 시간 단축 하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="효율적인 HNSW 인덱싱 대규모 병렬 처리를 통한 인덱스 빌드 시간 단축 하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">효율적인 HNSW 인덱싱 대규모 병렬 처리를 통한 인덱스 빌드 시간 단축 하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="일반 NLP 방법론 소개 기초부터 고급까지" href="/post/2024-06-22-IntroofGenericNLPmethods"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="일반 NLP 방법론 소개 기초부터 고급까지" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-IntroofGenericNLPmethods_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="일반 NLP 방법론 소개 기초부터 고급까지" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">일반 NLP 방법론 소개 기초부터 고급까지</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">15<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/21">21</a><a class="link" href="/posts/22">22</a><a class="link" href="/posts/23">23</a><a class="link" href="/posts/24">24</a><a class="link posts_-active__YVJEi" href="/posts/25">25</a><a class="link" href="/posts/26">26</a><a class="link" href="/posts/27">27</a><a class="link" href="/posts/28">28</a><a class="link" href="/posts/29">29</a><a class="link" href="/posts/30">30</a><a class="link" href="/posts/31">31</a><a class="link" href="/posts/32">32</a><a class="link" href="/posts/33">33</a><a class="link" href="/posts/34">34</a><a class="link" href="/posts/35">35</a><a class="link" href="/posts/36">36</a><a class="link" href="/posts/37">37</a><a class="link" href="/posts/38">38</a><a class="link" href="/posts/39">39</a><a class="link" href="/posts/40">40</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"ComfyUI 설치 가이드 Linux, Windows","description":"","date":"2024-06-22 21:48","slug":"2024-06-22-InstallingComfyUILinuxWindows","content":"\n\n친절한 마음으로 설치 방법 알려드립니다.\n\nLinux 또는 Windows에 ComfyUI를 수동으로 설치하세요.\n\n![이미지](/assets/img/2024-06-22-InstallingComfyUILinuxWindows_0.png)\n\n오랫동안 미게시 상태로 남아있었기 때문에 이를 새로 공유하려 합니다. 쉽게 설치할 수 있는 휴대용 Windows 버전이 있지만, 이 가이드에서는 수동 설치 방법을 다룹니다.\n\n## 전제조건\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPython 3.10\nGit\n\n## 단계 1: 레포지토리 복제하기\n\n명령 프롬프트(Windows)나 터미널(Linux)을 열고 레포지토리를 설치할 위치로 이동합니다. 윈도우의 C 드라이브 루트 디렉토리에 ai라는 폴더를 생성하고, 리눅스에서는 사용자의 홈 디렉토리에 해당 폴더를 생성합니다.\n\n![이미지](/assets/img/2024-06-22-InstallingComfyUILinuxWindows_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 Markdown 형식으로 표태그를 변경해주세요.\n\n\n\u003cimg src=\"/assets/img/2024-06-22-InstallingComfyUILinuxWindows_2.png\" /\u003e\n\n\nGit을 사용하여 레포지토리를 클론해주세요. 클론이 완료되면 현재 작업 디렉토리를 ComfyUI로 설정해주세요.\n\n```js\ngit clone https://github.com/comfyanonymous/ComfyUI.git\n```\n\n```js\ncd ComfyUI\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-22-InstallingComfyUILinuxWindows_3.png)\n\n![이미지](/assets/img/2024-06-22-InstallingComfyUILinuxWindows_4.png)\n\n## 단계 2: 가상 환경 만들기\n\n이제 가상 환경(venv)을 생성할 것입니다. 이 저장소의 설치 지침은 시스템 전역 수준에서 종속성을 설치하는 것을 제안하지만, 종속성 같은 것들은 다른 프로젝트들과 겹치는 영역을 피하기 위해 자체 venv 내에서 격리시키는 것이 더 나은 것 같아요. 다음 명령어를 사용할 거에요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\npython -m venv을 사용하여 새로운 가상 환경을 만들 수 있어요. 가상 환경의 이름을 venv로 지정할게요. 가장 간단하게 설정하기 위해요.\n\n```js\nREM: Windows용\n\nREM: 가상 환경 생성\npython -m venv venv\n\nREM: 가상 환경 활성화\nvenv\\Scripts\\activate.bat\n```\n\n```js\n# Linux용\n\n# 가상 환경 생성 (참고: python3 사용)\npython3 -m venv venv\n\n# 가상 환경 활성화\nsource venv/bin/activate\n```\n\n![이미지](/assets/img/2024-06-22-InstallingComfyUILinuxWindows_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-InstallingComfyUILinuxWindows_6.png)\n\n(venv)가 접두어로 있는 프롬프트를 볼 때, 활성화되었음을 의미합니다.\n\nvenv가 활성화된 상태에서는 설치된 파이썬 패키지가 venv 내에서만 작동합니다.\n\n## 단계 3: 의존성 설치\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저 Torch를 설치해 주세요. 몇 분 정도 소요될 거에요.\n\n```js\npip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n```\n\n다음으로 requirements.txt 파일에 나열된 나머지 종속성을 설치해 주세요:\n\n```js\npip install -r requirements.txt\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 4: ComfyUI 실행하기\n\nComfyUI를 실행하려면 먼저 venv가 활성화되어 있는지 확인해야 합니다.\n\n```js\nREM: Windows\n\nREM: venv를 활성화합니다.\nvenv\\Scripts\\activate.bat\n\nREM: comfyui 시작\npython main.py\n```\n\n```js\n# Linux\n\n# venv를 활성화합니다.\nsource venv/bin/activate\n\n# comfyui 시작\npython3 main.py\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n작업을 간편하게 하기 위해 위 두 가지를 배치 파일(Windows)이나 셸 스크립트(Linux) 안에 넣을 수 있습니다.\n\nWindows에서: 새 텍스트 파일을 만들고 launch.bat로 이름을 지정하십시오. 그 안에 다음과 같이 작성하십시오:\n\n```js\nREM: venv 활성화\ncall venv\\Scripts\\activate.bat\n\nREM: comfyui 시작\npython main.py --listen\n```\n\n파일을 저장한 후, launch.bat 파일을 더블 클릭하여 ComfyUI를 간편하게 시작할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n리눅스용: 새 텍스트 파일을 만들어 launch.sh라는 이름을 지어주세요. 그 안에 다음 내용을 넣어주세요:\n\n```bash\n#!/bin/bash\n\n# 가상 환경 활성화\nsource venv/bin/activate\n\n# comfyui 시작\npython3 main.py\n```\n\n파일을 저장해주세요. 다음으로 해당 파일을 실행 가능하게 만들기 위해 아래 명령어를 입력해주세요:\n\n```bash\nchmod +x launch.sh\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같이 입력하여 실행하세요:\n\n```js\n./launch.sh\n```\n\n또는 GUI에서 직접 실행할 수도 있지만, 파일 관리자에 따라 다를 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-22-InstallingComfyUILinuxWindows_0.png"},"coverImage":"/assets/img/2024-06-22-InstallingComfyUILinuxWindows_0.png","tag":["Tech"],"readingTime":4},{"title":"AI 아트 프롬프트의 구성 요소 분석","description":"","date":"2024-06-22 21:46","slug":"2024-06-22-TheAnatomyofanAIArtPrompt","content":"\n\n![이미지](/assets/img/2024-06-22-TheAnatomyofanAIArtPrompt_0.png)\n\n좋은 프롬프트를 작성하는 방법을 이해하면 찾고 있는 결과물을 얻는 데 도움이 됩니다.\n\nUI 도구를 사용하여 프롬프트를 작성할 수 있지만, 자신의 프롬프트를 변경, 세밀 조정하고 만드는 능력은 매우 중요합니다. 이러한 기술을 설명하는 용어가 심지어 있습니다 – 때로는 \"프롬프트 제작\" 또는 \"프롬프트 엔지니어링\"이라고도 합니다.\n\n물론 모든 지침을 따르지 않고도 놀라운 결과물을 얻을 수 있습니다. 단순한 단어나 구절에서 생성된 멋진 이미지를 보았습니다. 그러나 일관성을 원하고 결과물을 향상시키려면 AI가 언어 패턴에 어떻게 응답하는지 배워야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n나는 커뮤니티 포럼과 디스코드 채널에서 팔로우하는 AI 아티스트들이 이 기술을 숙달했으며, 그들이 어떻게 프롬프트를 작성하는지 공부하는 것이 나의 프롬프트 작성 능력 향상에 도움이 되었습니다.\n\n이 글에서 제가 하고 싶은 것은 제가 프롬프트를 작성할 때 사용하는 사고 과정을 보여드리고 싶습니다. 이를 특정 AI 아트 도구와 무관하게 작성 중이며, 다양한 도구들 사이에 문법 차이가 있을 수 있지만, 작성 방식은 대체로 비슷합니다. 아래 예시에서는 Midjourney에서 생성된 아트를 보여드리겠습니다.\n\n# 프롬프트 만들기\n\n나는 프롬프트의 해부학을 네 가지 구분으로 생각하고 특정한 순서로 배치하는 것을 좋아합니다 (순서가 출력물의 AI 우선 순위에 영향을 미침을 주의하세요).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 콘텐츠 유형\n- 설명\n- 스타일\n- 구성\n\n작업 절차에서 각각을 자세히 살펴보겠습니다.\n\n## 1. 콘텐츠 유형\n\n미술 작품을 만드는 방식을 고려할 때, 먼저 생각해야 할 것은 어떤 유형의 작품을 만들고 싶은지입니다. 사진, 드로잉, 스케치 또는 3D 렌더 중 어느 것인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n네 아래와 같이 시작하시면 됩니다.\n\n```js\n...사진\n```\n\n## 2. 설명\n\n설명은 주제, 주제 특성 및 환경/장면을 정의하는 것을 의미합니다. 형용사를 사용하여 더 구체적으로 설명할수록 결과물이 더 좋아집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단한 설명은 이렇게 생겼을 거에요…\n\n```js\n늑대 사진\n```\n\n결과물은 이렇게 나올 거에요…\n\n![image](/assets/img/2024-06-22-TheAnatomyofanAIArtPrompt_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 나은 설명은 환경/장면 설명과 함께 주제 속성을 추가하는 것입니다.\n\n```js\n안개 낀 숲 속에서 화난 전신 늑대 사진\n```\n\n그리고 이렇게 됩니다…\n\n![이미지](/assets/img/2024-06-22-TheAnatomyofanAIArtPrompt_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 이미지 URL과 함께 텍스트 설명을 포함하여 시각적 영감으로 활용할 수 있습니다.\n\n```js\n![Photograph of an angry full-bodied wolf in the foggy woods](http://www.wolfsite.com/wolf.jpg)\n```\n\n이 예시에서는 적용되지 않지만, 설명에 중요한 측면 중 하나는 포착하려는 시대나 역사적 시기입니다. 따라서 해당 이미지에 사람이나 건물이 있는 경우 \"언제\"라는 맥락이 중요할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\n원시 사회, 고대, 중세, 르네상스, 현대 세계, 현대, 미래\r\n```\r\n\r\n## 3. 스타일\r\n\r\n미술 스타일은 표현에 큰 영향을 미치는데, 스타일을 세 가지 하위 카테고리로 생각합니다:\r\n\r\n조명, 디테일 및 아트 스타일.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 조명에 사용할 수 있는 단어 몇 가지가 있어요:\n\n```js\n액센트 조명, 백라이트, 블랙라이트, 눈부신 빛, 양초 빛, 콘서트 조명, 황혼 광선, 직사광선, 황혼, 에디슨 전구, 전자 아크, 불, 형광등, 빛나는, 방사성 빛나는, 발광스틱, 용암 빛, 달빛, 자연 조명, 네온 램프, 나이트클럽 조명, 핵폐기물 빛나는, 양자 점 디스플레이, 스포트라이트, 스트로브, 햇빛, 자외선, 드라마틱 조명, 어두운 조명, 부드러운 조명\n```\n\n아트워크의 세부사항은 날카로움뿐만 아니라 특정 카메라 렌즈 또는 디지털 렌더링 엔진에서도 나타날 수 있어요.\n\n여기 세부사항에 사용할 수 있는 몇 가지 단어가 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n고도로 세밀하고 질감이 풍부한, 현실적인, 언리얼 엔진, 옥타네 랜더, 보케, Vray, 후디니 랜더, 퀵셀 메가스캔, 초점 깊이(또는 DoF), 아놀드 랜더, 8K UHD, 광선 추적, CGI, 루멘 반사, CGSociety, 초 현실적인, 부피적 안개, 오버글라즈, 아날로그 사진, 폴라로이드, 100mm, 필름 사진술, DSLR, Cinema4D, 스튜디오 품질\n\n\n미술 양식은 다른 기술들의 설명일 수도 있고, 역사적인 미술 장르로 정의될 수도 있어요.\n\n다음은 역사적 미술 양식을 위한 단어들입니다:\n\n\n추상, 중세 미술, 르네상스, 바로크, 로코코, 신고전주의, 낭만주의, 인상파, 후기 표현주의, 쿠비즘, 퓨처리즘, 아르트 데코, 추상표현주의, 현대, 팝 아트, 초현실주의, 판타지\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아름다운 기술과 자료에 관한 몇 가지 용어들입니다:\n\n```js\n디지털 아트, 디지털 페인팅, 컬러 페이지, 픽시브(애니메이션/만화 사이트) 주목 받은 작품, 아트스테이션에서 트렌드를 이끌고 있는 작품, 정밀한 선 화법, 타로 카드, 캐릭터 디자인, 컨셉 아트, 대칭, 황금비, 감동적인, 수상 경력, 반짝이는, 부드러운, 초현실주의, 신성한, 천상의, 우아한, 유화, 부드러운, 매혹적인, 미술\n```\n\n이제, 우리 늑대 프롬프트에 일부 스타일을 추가해보겠습니다.\n\n```js\n안개 낀 숲 속에서 분노한 전신 늑대의 사진, 황혼, 언리얼 엔진, 8K\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-22-TheAnatomyofanAIArtPrompt_3.png)\n\n8k 품질로 우드들이 매우 자세히 보이는 것을 확인할 수 있습니다. 셋인해 생산된 썩은 저녁 태양으로부터 발한 어둠 속에 밝기가 빛납니다. 늑대는 Epic 게임에서 모델링된 것처럼 보입니다.\n\n다음은 몇 가지 역사적 예술 스타일을 추가한 예시입니다:\n아래 프롬프트는 팝 아트의 역사적인 스타일을 포함하고 있습니다.\n\n```js\n안개 낀 숲 속에서 성난 전신 통통한 늑대의 사진, 팝 아트\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스타일이 결과물에 미치는 영향을 확인할 수 있습니다:\n\n![Style Influence](/assets/img/2024-06-22-TheAnatomyofanAIArtPrompt_4.png)\n\n## 아티스트 이름을 스타일로\n\n미술 스타일에 관한 가장 인기 있는 형태는 AI가 정말 좋아하는 아티스트 이름의 사용입니다. 스타일 변화를 탐구하기 위해 둘 이상의 아티스트 이름을 사용하는 것도 일반적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기를 클릭하면 Midjourney 커뮤니티가 유지하는 놀라운 예술가와 스타일 자원이 나옵니다.\n\n그래서 우리의 늑대 프롬프트를 확장해보자면, 자원 시트에서 2명의 아티스트를 선택했고, 포토그래픽 스타일로 돌아가기 위해 Unreal Engine을 제거했으며, 'sepia'를 색상으로 추가했습니다.\n\n보시다시피... 프롬프트 제작은 텍스트 입력을 계속 바꿔가며 결과물을 정제하고 다시 만드는 것이죠.\n\n```js\n알렉스 홀리-오를란델리에 의한 안개 낀 숲 속의 화난 늑대 사진, 바스티앙 르쿠프-드하름, 황혼, 세피아,\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 이것이 결과입니다:\n\n![image](/assets/img/2024-06-22-TheAnatomyofanAIArtPrompt_5.png)\n\n## 4. 구성\n\n남은 요소는 구성이며 이는 ...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n화면 비율, 카메라 뷰 및 해상도.\n\n특정 목적을 지정할 때 화면 비율은 정말 중요합니다. 배너를 만들고 있다면, 화면 보호기를 만들 때와는 다른 화면 비율을 사용해야 합니다.\n\n다양한 크기에 적용되는 다양한 화면 비율을 보여주는 훌륭한 자료입니다.\n\n![이미지](/assets/img/2024-06-22-TheAnatomyofanAIArtPrompt_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n카메라 시점은 이미지의 관점에 관한 것입니다. 여러분의 작품은 가까이서 찍은 사진이나 광각 사진, 피시아이, 등과 같이 어떤 시점을 선택할 건가요?\n\n중요한 질문은 시청자 시각이 무엇인가에 대한 것입니다.\n\n다음은 카메라 시점에 사용할 수 있는 몇 가지 단어입니다.\n\n\n울트라 광각, 광각, 항공 시점, 대규모 규모, 길거리 수준, 풍경, 파노라마, 보케, 피시아이, 네덜란드각, 저각도, 초장거리 샷, 장거리 샷, 가까이서 촬영, 극한 가까이서 촬영\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n상세, 품질 및 원하는 크기에 대한 해상도를 적용할 것입니다. 해상도에 사용할 수 있는 단어는 다음과 같습니다:\n\n```js\n높은 상세 정도, 초점 깊이 (또는 DoF), 4k, 8k UHD, 초실감적, 스튜디오 품질.\n```\n\n## 추가 도구 기반 매개변수\n\nMidJourney, Stable Diffusion 및 DALL-E 2와 같은 각 AI 생성기는 결과물을 더욱 정교하게 만들기 위한 추가 명령 매개변수를 갖게 됩니다. 예를 들어, Midjourney는 다음과 같은 추가 매개변수를 가지고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n--beta = 더 높은 품질의 출력을 위한 실험 알고리즘\n--no = 특정 객체를 포함하지 않도록 하는 부정적인 프롬프팅\n--s = stylize 인수는 스타일화 강도를 설정합니다\n--q = 출력물의 품질을 설정합니다\n--chaos = 출력물에 랜덤성을 추가하는 옵션을 포함합니다\n--seed = 특정 이미지에서 시작점을 설정합니다 (각 이미지에는 고유한 ID가 있음), 일관된 관점을 원하는 경우에 좋습니다\n```\n\n앞으로 이 도구 기반 명령어들을 사용하는 방법에 대해 더 많이 다룰 예정이니, 반드시 제를 팔로우해주세요.\n\n## 마치며\n\n이 프롬프트 생성 과정을 통해 도움이 되었기를 바랍니다. 프롬프트를 배열하는 방법을 이해하고 나면 정말 재미있을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요청 내용의 해부학 분석을 여기 안내해 드리겠어요:\n\n![The Anatomy of an AIArt Prompt](/assets/img/2024-06-22-TheAnatomyofanAIArtPrompt_7.png)\n\n제가 사용하는 프롬프트 기술과 차이를 알아들을 수 있다면 흥미롭게 생각하겠어요.\n\n프롬프트 작성은 계속 발전하는 기술이지만, 반드시 필요한 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 이 기사를 좋아하셨다면, 조금의 Medium 사랑을 표현해 주세요... 박수를 치거나 댓글을 달아 주시고, 반드시 팔로우해 주세요.\n\n또한, 제 추천 링크를 사용하여 회원이 되어 Medium에서 제 작품을 지원하고 무제한 액세스를 받을 수도 있습니다.","ogImage":{"url":"/assets/img/2024-06-22-TheAnatomyofanAIArtPrompt_0.png"},"coverImage":"/assets/img/2024-06-22-TheAnatomyofanAIArtPrompt_0.png","tag":["Tech"],"readingTime":6},{"title":"AI 얼굴 스왑 배틀 PuLID vs InstantID vs FaceID","description":"","date":"2024-06-22 21:45","slug":"2024-06-22-AIfaceswapbattlePuLIDvsInstantIDvsFaceID","content":"\n\n오늘은 ComfyUI 워크플로우를 사용하여 PuLID, InstantID 및 IP-Adapter의 FaceID-V2와 같은 세 가지 AI 얼굴 교체 기술을 비교해보려고 해요. 이러한 기술은 얼굴 인식, 얼굴 감지 및 얼굴 정렬을 위해 설계된 깊은 얼굴 분석 라이브러리 인 InsightFace를 기반으로 합니다. InsightFace는 상업적 라이센스가 필요하다는 점을 유의해 주세요.\n\nIP-Adapter FaceID는 이러한 기술 중에서 처음에 소개되었고, 그 뒤를 이어 InstantID가 나왔으며, 가장 최근에 PuLID가 나왔어요.\n\n각각의 프로그램은 얼굴 참조 이미지가 필요하기 때문에 교체된 얼굴의 효과는 제공하는 참조 이미지의 품질과 적합성에 크게 의존합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 세 가지 얼굴 교체 방법의 결과를 ComfyUI 워크플로우를 사용하여 비교할 것입니다. 여기서 다운로드할 수 있어요.\n\n먼저, 이 워크플로우의 사용 방법을 보여드리겠습니다. 그런 다음, 효과를 평가하기 위해 4 세트의 이미지를 생성하고 평가할 것입니다.\n\n![이미지](/assets/img/2024-06-22-AIfaceswapbattlePuLIDvsInstantIDvsFaceID_1.png)\n\n비디오 콘텐츠와 함께 Stable Diffusion에 더 자세히 파고들고 싶은 분들을 위해 이 글에 부가된 매력적인 비디오 튜토리얼을 확인하실 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# ComfyUI Workflow 설정하기\n\n1️⃣ 노드 다운로드 및 불러오기:\n\n- 필요한 모든 노드가 있는지 확인해주세요. 누락된 노드는 ComfyUI 관리자를 통해 설치한 후 ComfyUI를 재시작하면 됩니다.\n- 노드를 설치하기 전에 ComfyUI를 업그레이드하여 누락된 노드로 인한 문제를 피해주세요.\n\n![이미지](/assets/img/2024-06-22-AIfaceswapbattlePuLIDvsInstantIDvsFaceID_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2️⃣ 필요한 모델 설치하기:\n\n모델의 다운로드 주소와 저장 경로는 각 노드의 GitHub 홈페이지에 자세히 나와 있습니다.\n\n- PuLID: [GitHub 링크](GitHub Link)\n- InstantID: [GitHub 링크](GitHub Link)\n- IP-Adapter: [GitHub 링크](GitHub Link)\n\n예를 들어 PuLID의 경우, 미리 학습된 모델을 다운로드하여 ComfyUI/models/pulid/ 폴더에 위치시킵니다. 첫 실행 시 추가 모델을 자동으로 다운로드합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Workflow Structure\n\n1️⃣ Shared Nodes:\n\nAt the bottom, these nodes are common to PuLID, InstantID, and FaceID. They use identical dimensions for checkpoint, prompt, latent image, and fixed seed, facilitating easy comparison.\n\n![Image](/assets/img/2024-06-22-AIfaceswapbattlePuLIDvsInstantIDvsFaceID_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2️⃣ 모델 노드 그룹:\n\n더 위에, 세 개의 노드 그룹이 있습니다. 왼쪽부터 오른쪽으로: PuLID, InstantID, 그리고 IP-Adapter-FaceID입니다. 저자들은 이를 직관적으로 조직했습니다.\n\n![이미지](/assets/img/2024-06-22-AIfaceswapbattlePuLIDvsInstantIDvsFaceID_4.png)\n\n3️⃣ 참조 이미지 노드:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에 있는 \"Face\" 노드(\"Load Image\" 노드)는 세 기술 모두에 사용되는 참조 사진을 로드합니다.\n\n![이미지](/assets/img/2024-06-22-AIfaceswapbattlePuLIDvsInstantIDvsFaceID_5.png)\n\n# Workflow 실행 및 결과 비교\n\n## 진행 방법\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 모든 노드 및 모델이 올바르게 설치되고 구성되었는지 확인하십시오.\n- 워크플로를 가져오고 노드 연결 및 구성을 확인하십시오.\n- 참조 이미지를 로드하여 이미지 경로와 형식을 확인하십시오.\n- 워크플로를 실행하여 세 가지 기술을 위한 효과 이미지를 생성하십시오.\n- PuLID, InstantID 및 IP-Adapter FaceID의 차이, 장단점을 평가하기 위해 생성된 이미지를 비교하십시오.\n\n## 평가 기준\n\n다음 차원에 따라 네 개의 이미지 세트를 생성하고 다음과 같이 점수를 매기세요 (각각 1-3점):\n\n- 빠른 적합성: 이미지가 프롬프트 설명과 얼마나 잘 일치하는지.\n- 얼굴 밝기: 얼굴의 빛과 그림자의 자연스러움.\n- 얼굴 유사성: 생성된 이미지와 참조 이미지 사이의 닮은 정도.\n- 얼굴 세부 사항: 얼굴의 질감과 세부 풍부함.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 비교 분석\n\n💠 첫 번째 이미지 세트\n\n![First set of pictures](/assets/img/2024-06-22-AIfaceswapbattlePuLIDvsInstantIDvsFaceID_6.png)\n\n💠 두 번째 이미지 세트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-06-22-AIfaceswapbattlePuLIDvsInstantIDvsFaceID_7.png)\n\n💠Third set of pictures\n\n![Image 2](/assets/img/2024-06-22-AIfaceswapbattlePuLIDvsInstantIDvsFaceID_8.png)\n\n💠Fourth set of pictures\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-22-AIfaceswapbattlePuLIDvsInstantIDvsFaceID_9.png)\n\nFinally, let’s calculate the total score:\n\nFrom my observations:\n\n- InstantID scores the highest overall, followed closely by FaceID.\n- PuLID lags behind the other two.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nInstantID는 최고의 결과를 제공하지만, 가장 많은 자원을 사용합니다. 최종적으로 최선의 선택은 귀하의 특정한 요구 사항에 달렸습니다.\n\n귀하는 직접 테스트를 수행하고 결과를 댓글 섹션에서 공유하는 것을 장려합니다.\n\n💡 더 깊은 탐구를 원하십니까? 제 스테이블 디퓨전 컬렉션이 기다리고 있습니다.\n\n## 기사가 마음에 드셨나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그렇다면:\n\n- 댓글 남기기\n- 업데이트 팔로우하기\n- 무료 이메일 알림","ogImage":{"url":"/assets/img/2024-06-22-AIfaceswapbattlePuLIDvsInstantIDvsFaceID_0.png"},"coverImage":"/assets/img/2024-06-22-AIfaceswapbattlePuLIDvsInstantIDvsFaceID_0.png","tag":["Tech"],"readingTime":4},{"title":"파이썬을 사용한 신호처리를 위한 생성적 적대 신경망GAN 실습 가이드","description":"","date":"2024-06-22 21:43","slug":"2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython","content":"\n\n## 몇 줄의 코드로 신호 처리를 위한 생성형 딥러닝 모델을 만드는 방법\n\n![이미지](/assets/img/2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython_0.png)\n\n저는 연구에서 머신(딥)러닝을 많이 사용합니다. 이틀 전에, 생성적 적대 신경망(GAN)에 대해 작업하고 제 작업에 어떻게 적용할 수 있는지 살펴보았습니다.\n\n코드가 완성되면, 제가 Medium에 이 기사를 쓰기 시작했고, 항상 하는 것처럼 적절한 소개로 시작하는 최상의 문구를 찾으려 했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자, 이제부터 질문을 스스로에게 던지기 시작합니다:\n\n물론, 저는 내가 쓰는 것이 의미 있고 재미있기 때문에 독자들이 이것을 읽어야 한다고 믿습니다.\n\n하지만 사실은, 저는 시그널 처리를 사랑하고, 시그널 처리에 대해 글을 쓰는 것을 사랑하기 때문에 그렇게 합니다. 이 글은 제가 가장 사랑하는 두 가지에 대한 것입니다: 시그널 처리와 인공지능. 이 두 가지에 제 모든 사랑, 에너지, 열정을 쏟았으며 (사실 한 바다를 건너가서 연구까지 했습니다), 여러분이 이 주제를 흥미롭게 생각해주길 바랍니다.\n\n제목에서 짐작하실 수 있듯이, 우리는 시그널 처리를 위해 생성적 적대 신경망(Generative Adversarial Networks)을 사용할 것입니다. 이번에 할 게임은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 실험을 진행하고 있다고 상상해보세요. 이 실험의 설치는 생성기에 의해 이루어집니다. 이 생성기의 출력물은 시계열 데이터(즉, 신호)입니다.\n\n![이미지](/assets/img/2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython_1.png)\n\n이 실험이 비싸고 많은 에너지와 계산 노력이 필요하다고 상상해보세요. 우리는 결국 이 실험을 중단하고 싶습니다. 이를 위해서 생성기를 대리 생성기(surrogate)로 바꿔야 합니다.\n\n![이미지](/assets/img/2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희 대리 모델인 연핑크 뇌를 보십시오. 특히, 이 대리 모델은 기계 학습 모델입니다. 이름에서 알 수 있듯이, 이 기계 학습 모델은 적대적 생성 신경망(GAN)입니다.\n\n이 글은 이런 내용을 담고 있을 거에요:\n\n- 실험 구축하기: 우리는 통제된 데이터셋을 생성하고 설명할 거에요.\n- 기계 학습 모델 정의하기: 우리 GAN 모델의 특정 기능을 설명할 거에요.\n- 결과 탐색하기: 우리 생성 모델을 실행하고, 대리 모델을 사용하여 신호를 추출할 거에요.\n\n제가 기대되는 만큼 당신도 신나길 바라요. 함께해요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 1. 실험에 관하여\n\n전기/기계 공학자가 설정한 대부분의 신호는 사인 파형 신호입니다.*\n\n즉, 출력 신호는 이렇게 어떤 모양인 것입니다:\n\n![image](/assets/img/2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위와 같은 표를 Markdown 형식으로 변경합니다.\n\n| Variable | Description                              |\n|----------|------------------------------------------|\n| A        | 신호의 진폭                               |\n| omega    | 주파수                                   |\n| b        | 편향                                    |\n\n실제 세계 실험에서는 노이즈 요소가 있습니다.\n지금은 다양한 종류의 노이즈가 있으며, 각각 색상이 지정되어 있습니다(white noise, pink noise, blue noise, green noise 등). 가장 전형적인 노이즈 중 하나는 가우시안 백색 소음으로, 모든 주파수에서 존재하며 가우시안 분포를 갖는 노이즈입니다.\n\n따라서 대상 신호는 다음과 같이 보입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython_4.png)\n\nNow, in practice:\n\n- The mean is usually 0\n- The standard deviation can vary, but it is safe to assume it is 1 and fixed for our experiment.\n- Another constant can be considered to be in front of the noise factor as a sort of amplitude of the noise\n\nSo at the end of the day, it looks more like this:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython_5.png\" /\u003e\n\n이것이 바로 우리의 완벽한 세계, 아바타에서 말하는 대로 우리의 판도라입니다 😄\n\n현실에서는 상황이 조금 다를 수 있습니다.\n예를 들어, 우리가 진폭을 고정했다고 해봅시다. 그러나 그 진폭은 많이 변할 수 있습니다.\n예를 들어, 다음과 같이 말씀드릴 수 있습니다:\n\n- 진폭은 0.1부터 10까지의 범위 내에서 0.1씩 증가합니다.\n- 편향은 0.1부터 10까지의 범위 내에서 0.1씩 증가합니다.\n- 주파수는 1부터 2까지의 범위 내에서 0.001씩 증가합니다.\n- 잡음의 진폭은 고정되어 있으며 0.3입니다 (잡음의 랜덤성은 그 확률 분포에 있습니다)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 이러한 무작위성을 통합하려면 다음 코드 라인을 사용할 수 있습니다:\n\n다음은 일부 출력입니다:\n\n이 시점에서 목표가 충분히 명확해야 합니다:\n\n그러면 머신 러닝부터 시작합시다 🤗\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2. 머신 러닝에 대해\n\n저희가 사용하는 머신 러닝 모델은 Generative Adversarial Network (GAN)입니다.\n\nGAN에 대한 설명과 함께 신호 처리에 관한 이 기사를 정말 원합니다. 간단히 소개해보겠습니다. 주의: 어떤 사람들은 저보다 훨씬 잘 설명합니다 (이번에 Joseph Rocca에게 큰 찬사를 드립니다: Understanding Generative Adversarial Networks (GANs))\n\nGAN이 Deepfake에 사용되는 모델이라고 생각해보세요.\n이것은 이름에서 느낄 수 있듯이 생성 모델로, 생성 부분과 구별자를 훈련시켜 구현됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n생성 부분은 실제 모델과 가능한 가까운 모델을 생성하려고 노력합니다. 그게 전부라면 일반적인 인코더-디코더와 다르지 않을 것입니다. \"진짜 거래\"는 식별 부분의 존재입니다.\n\n식별 부분은 실제와 \"가짜\" (생성된 생성 모델에서 생성된) 인스턴스를 구분하려고 시도하는 분류기입니다.\n\n따라서 게임은 훈련 데이터 객체와 유사한 가짜 객체를 구축하려고 하는 생성 모델과 훈련 데이터 객체와 가짜 객체를 구분하려고 하는 식별 모델 사이의 경쟁입니다. 이 \"게임\"은 최소-최대 손실 함수와 Ian J. Goodfellow의 뛰어난 마음에 의해 만들어진 우아하면서 간단한 알고리즘으로 실현됩니다 (Generative Adversarial Nets 논문).\n\n지금, GAN의 매우 흔한 사용은 조건부 GAN입니다. 조건부 GAN은 특정 입력과 관련된 생성 모델입니다.\n입력이 문자열이라고 말해봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 출력물은 다음 이미지입니다:\n\n![Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython_6](/assets/img/2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython_6.png)\n\n이 예에서 모델은 더 단순하며 생성 모델은 특정 입력과 관련이 없습니다.\n이 생성 모델의 입력은 지금 잡음이므로 모델은 잡음에서 소스에서 생성된 것으로 추정되는 신호로 가려고 합니다.\n\n생성 모델의 구조는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 식별 모델의 아키텍처입니다:\n\n![discriminative model](/assets/img/2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython_8.png)\n\n생성 모델은 다음과 같이 설명할 수 있습니다:\nLSTM 모델은 무작위 잡음 벡터(3차원 벡터)를 입력으로 받고, 이상적으로는 원하는 신호인 300개의 요소로 이루어진 벡터를 출력합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image1](/assets/img/2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython_9.png)\n\nThe discriminative model distinguishes a real (from the training data) and a fake (generated by the generative model) output:\n\n![image2](/assets/img/2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython_10.png)\n\nThe hands-on implementation of this GAN is the following:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 입력의 길이는 모델의 매개변수입니다:\n\n```js\nLENGHT_INPUT = 300\n```\n\n그리고 노이즈 벡터의 차원은 latent_dim 매개변수입니다.\n\n이제 데이터셋을 생성해야 합니다. 이는 n개의 신호를 생성하는 함수를 작성하는 것을 의미합니다. 또한 주어진 차원의 n개의 무작위 노이즈 입력을 생성해야 하며, n개의 무작위 노이즈 신호를 사용하여 가짜 신호를 생성하는 코드를 작성해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, 저희는 기차 함수를 구축해야 합니다.\n\n이 코드는 우리의 생성 모델을 훈련할 것입니다. 또한 n_eval 단계마다 생성 모델의 진행 상황을 보여줄 것이고, 진짜 데이터와 가짜 데이터(여기서 가짜란 \"우리 모델에 의해 생성된\" 것을 말합니다)를 플로팅할 것입니다.\n\n# 3. 전체 코드\n\n전체 스크립트입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터셋을 생성합니다\n- GAN 모델을 구축합니다\n- GAN을 학습합니다\n\n다음과 같습니다:\n\n진행 상황을 보여드릴게요:\n\n이제 100,000개의 랜덤 신호를 생성해볼게요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대박이에요. 각 실험이 0.5달러 든다고 가정해보세요. 그러면 50,000달러를 \"절약\"한 셈이에요. 또한 각 실험이 1분이 걸린다고 가정해보세요. 그러면 70일을 \"절약\"한 셈이죠. 이것이 GANs 모델을 사용하는 목적입니다: \"시간과 노력을 절약하기 위해\".\n\n이제 100,000개의 실제 신호를 생성합시다.\n\n결과를 몇 개 그래프로 나타내 보겠습니다:\n\n## 4. 요약\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는:\n\n- 인공 지능과 신호 처리가 멋지다는 것을 확인하고, 그래서 이 둘을 합치기로 결정했습니다.\n- 우리는 신호 처리 시나리오를 만들었습니다. 여기서는 소음이 있는 사인 발생기가 있습니다. 이 사인에는 다른 진폭, 다른 주파수 및 다른 바이어스가 있을 수 있습니다.\n- 우리는 GAN 모델에 대해 간단히 설명했습니다. 모델의 생성 부분, 식별 부분 및 손실이 무엇인지 설명했습니다. 생성 모델의 입력은 3차원 노이즈이며, 출력은 학습 데이터 중 하나와 비슷한 신호입니다.\n- 우리는 GAN 모델을 훈련시키고 일부 무작위 신호를 생성했습니다.\n\n이 모델의 핵심 부분은 생성 능력이며, 훈련된 생성 모델은 시간, 비용 및 에너지를 절약할 수 있습니다. 이는 실험을 진행하는 대신 파이썬 환경에서 \"실행\" 버튼만 누르면 되기 때문입니다 🚀\n\n# 5. 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 이 기사를 좋아하셨고 머신 러닝에 대해 더 알고 싶거나, 단순히 저에게 질문을 하고 싶다면 아래 방법으로 연락해 주세요:\n\n1. LinkedIn에서 팔로우하기 - 모든 이야기를 공개하고 있습니다.\n2. 뉴스레터 구독하기 - 새로운 이야기에 대한 최신 정보를 받을 수 있으며, 궁금한 사항 또는 수정 사항을 받아 볼 수 있습니다.\n3. 추천 회원 가입하기 - 월별 \"이야기의 최대 개수\" 제한 없이 읽을 수 있으며, 최신 기술에 관한 저와 수천 명의 다른 머신 러닝 및 데이터 과학 최고의 작가들이 쓴 내용을 읽을 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython_0.png"},"coverImage":"/assets/img/2024-06-22-Hands-onGenerativeAdversarialNetworksGANforSignalProcessingwithPython_0.png","tag":["Tech"],"readingTime":7},{"title":"초보자를 위한 디스코드에서 Midjourney 시작하기 단계별 가이드","description":"","date":"2024-06-22 21:41","slug":"2024-06-22-GettingStartedwithMidjourneyonDiscordAStep-by-StepGuideforBeginners","content":"\n\n## Midjourney는 텍스트 프롬프트에서 AI가 생성한 이미지를 생성하는 흥미로운 도구이지만, Discord와 AI 이미지 생성에 처음 접하는 경우 과정이 다소 어렵게 느껴질 수 있습니다. 이 안내서는 Discord 설정부터 Midjourney를 사용하여 첫 AI 이미지를 생성하는 단계까지 안내해 드립니다.\n\n![이미지](/assets/img/2024-06-22-GettingStartedwithMidjourneyonDiscordAStep-by-StepGuideforBeginners_0.png)\n\n안녕하세요! AI 이미지를 생성하는 데 관심이 있는 많은 분들이 시작하기에 어려움, 혼란 또는 불확실함을 느낀다는 피드백을 받았습니다. 대부분의 사람들이 AI 이미지 생성에 대해 들어본 적이 있고 결과물을 본 적이 있어, 그 능력과 활용 방법에 궁금해하고 있습니다.\n\nAI 생성 이미지의 잠재적인 사용 범위는 매우 넓습니다. 개인 프로젝트에서부터 창의적 영감, 비즈니스 그래픽, 웹사이트 디자인 및 수동 소득 창출까지 가능성은 무한합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n작가로서 AI 도구를 자주 활용하는 저는 Midjourney AI를 가장 사용하기 편리하고 효율적인 이미지 생성기로 발견했습니다. Midjourney나 다른 AI 응용 프로그램과는 관련이 없으며 이들로부터 어떠한 보상도 받지 않지만, 여전히 제가 가장 선호하는 이 제품에 대한 초보자를 위한 안내서를 공유하고 싶었습니다. 게다가 Night Cafe AI를 사용하는 것도 좋아하는데, 이미지 생성을 위한 매일 무료 크레딧을 제공하기 때문입니다.\n\nMidjourney를 사용해보고 싶다면, 시작하는 방법에 대한 안내서가 여기 있습니다. Midjourney에서 discord를 통해 100개의 이미지를 생성한 후에 웹사이트 애플리케이션에 접근할 수 있다는 것을 기억해 주세요.\n\n이 글은 누구나 무료로 읽을 수 있습니다. AI가 생성한 글 내용이 포함되어 있어요. 자세한 안내서를 직접 작성하고 싶지만, ChatGPT의 도움을 받아 명확하고 유용한 안내서를 만드는 것이 더 효율적입니다. 유료 콘텐츠는 제가 직접 작성한 것이며 AI로 생성된 것이 아니라는 점을 참고해 주세요.\n\n## 초보자를 위한 Midjourney Discord 시작 가이드: 단계별 안내법\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 1: 디스코드 설정\n\n- 디스코드 계정 만들기:\n\n- 디스코드 웹사이트에 가서 오른쪽 상단의 \"로그인\"을 클릭합니다.\n- \"등록\"을 선택하고 필요한 세부 정보를 입력합니다(이메일, 사용자 이름, 비밀번호 및 생년월일).\n- 이메일 주소를 확인하려면 이메일로 전송된 링크를 따릅니다.\n\n2. 디스코드 앱 다운로드 (선택 사항):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 웹 브라우저를 통해 디스코드를 사용할 수 있지만 더 나은 경험을 위해 디스코드의 다운로드 페이지에서 컴퓨터나 휴대폰용 앱을 다운로드하세요.\n\n3. 디스코드에 로그인하기:\n\n- 디스코드 앱이나 웹사이트를 열고 새 계정 자격 증명을 사용하여 로그인하세요.\n\n## 단계 2: Midjourney 디스코드 서버 가입하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 초대 링크 받기: [여기를 클릭하여 Discord 초대 링크를 받아보세요](https://discord.com/invite/midjourney)\n\n- Midjourney 웹사이트를 방문하고 \"Beta 참여\" 버튼을 클릭하거나 위 링크를 사용해주세요.\n\n2. 서버에 참여하기:\n\n- 초대 링크를 클릭한 후 \"초대 수락\"을 클릭해주세요. 이렇게 하면 Midjourney Discord 서버로 이동됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Getting Started with Midjourney on Discord: A Step-by-Step Guide for Beginners](/assets/img/2024-06-22-GettingStartedwithMidjourneyonDiscordAStep-by-StepGuideforBeginners_1.png)\n\n## Step 3: Understanding the Server Layout\n\n- Familiarize Yourself with Channels:\n\n- On the left sidebar, you’ll see various channels. Channels are like rooms where specific topics are discussed.\n- Look for channels like #newbies, #announcements, and #rules to get started.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Getting Started with Midjourney on Discord - A Step-by-Step Guide for Beginners](/assets/img/2024-06-22-GettingStartedwithMidjourneyonDiscordAStep-by-StepGuideforBeginners_2.png)\n\n2. Read the Rules\n\n- In the #rules channel, read through the guidelines to understand what is expected of members.\n\n![Getting Started with Midjourney on Discord - A Step-by-Step Guide for Beginners](/assets/img/2024-06-22-GettingStartedwithMidjourneyonDiscordAStep-by-StepGuideforBeginners_3.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 4: 첫 번째 AI 이미지 생성하기\n\n- 뉴비 채널 찾기:\n\n- #newbies(예: #newbies-1, #newbies-2)라고 레이블된 채널을 찾아 클릭하세요.\n\n![이미지](/assets/img/2024-06-22-GettingStartedwithMidjourneyonDiscordAStep-by-StepGuideforBeginners_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. 봇 사용하기:\n\n화면 하단의 텍스트 입력 상자에 /imagine 명령어를 입력한 다음 생성하고 싶은 이미지에 대한 설명을 추가하세요.\n예를 들어, /imagine 일몰 시 산과 호수가 있는 고요한 풍경을 입력하세요.\n명령을 보내려면 \"Enter\" 키를 누르세요.\n\n3. 결과 기다리기:\n\nMidjourney 봇이 요청을 처리하고 프롬프트에 기반한 네 개의 이미지를 생성합니다. 이는 일반적으로 1분 이내에 완료됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Upscaling Images](/assets/img/2024-06-22-GettingStartedwithMidjourneyonDiscordAStep-by-StepGuideforBeginners_5.png)\n\n4. Upscaling Images:\n\n- Once the images are generated, you can choose to upscale (enhance) any of them for better quality. Click on the button under the image you like labeled “U1”, “U2”, “U3”, or “U4” (corresponding to the first, second, third, and fourth images).\n\n![Another Image](/assets/img/2024-06-22-GettingStartedwithMidjourneyonDiscordAStep-by-StepGuideforBeginners_6.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5. 다양성:\n\n- \"V1\", \"V2\", \"V3\", 또는 \"V4\"를 클릭하여 이미지의 변형을 만들 수도 있습니다.\n\n![Variations](/assets/img/2024-06-22-GettingStartedwithMidjourneyonDiscordAStep-by-StepGuideforBeginners_7.png)\n\n6. 이미지 저장하기:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 최종 확대된 이미지를 클릭한 후, 확대된 이미지를 오른쪽 클릭하여 \"이미지 다른 이름으로 저장...\"을 선택하여 디바이스에 다운로드하세요.\n\n![이미지](/assets/img/2024-06-22-GettingStartedwithMidjourneyonDiscordAStep-by-StepGuideforBeginners_8.png)\n\n## 단계 5: 커뮤니티 참여\n\n- 여러분의 작품 공유하기:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 적절한 채널에서 이미지를 공유하고 다른 사람들이 무엇을 만들고 있는지 확인해보세요.\n\n2. 도움 요청하기:\n\n- 궁금한 점이 있으면 #help 또는 #support 채널에서 질문할 수 있어요. 커뮤니티와 모더레이터들은 일반적으로 매우 지원적입니다.\n- 탐험하고 실험하기:\n- 다양한 프롬프트를 시도해보고, 여러 채널을 탐험하며, Midjourney의 가치를 극대화하기 위해 커뮤니티 이벤트에 참여해보세요.\n\n## 이러한 단계를 따라가면, Midjourney로 멋진 AI 생성 이미지를 만드는 길에 잘 진행될 거예요! 궁금한 점이 있다면 이 게시물에 댓글을 남겨주세요. 저가 답변해 드릴게요!","ogImage":{"url":"/assets/img/2024-06-22-GettingStartedwithMidjourneyonDiscordAStep-by-StepGuideforBeginners_0.png"},"coverImage":"/assets/img/2024-06-22-GettingStartedwithMidjourneyonDiscordAStep-by-StepGuideforBeginners_0.png","tag":["Tech"],"readingTime":5},{"title":"NLP와 지식 그래프를 활용한 2024 최신 오피오이드 연구 방법","description":"","date":"2024-06-22 21:38","slug":"2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch","content":"\n\n## 오피오이드 연구에서 인공지능과 지식 그래프 통합을 통한 혁신\n\n![이미지](/assets/img/2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch_0.png)\n\n오피오이드 위기는 전 세계적으로 수백만 명의 사람들에 영향을 미치는 엄청난 공중보건 위기입니다. 처방약물 오필리아, 헤로인, 펜타닐과 같은 합성 오피오이드의 광범위한 남용을 특징으로 하며, 이 위기는 중독, 과다복용, 사망률이 전례 없는 수준으로 이어지고 있습니다. 이 보건 위기의 복잡성과 규모는 새로운 통찰력을 발견하고 효과적인 개입책을 개발하기 위해 혁신적인 연구 방법이 필요합니다.\n\nNLP와 지식 그래프를 활용한 연구는 오피오이드 위기와의 전쟁에서 강력한 도구로 부상했습니다. 이런 최첨단 기술을 활용함으로써 연구자들은 과학문헌, 임상 노트, 의료 보고서, 그리고 소셜 미디어 토론 등 방대한 양의 비정형 데이터 저장소에 숨겨진 다양한 정보를 발굴할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNLP 기술은 비구조적 소스에서 구조화된 정보를 추출할 수 있게 해주어, 약물, 질병, 부작용과 같은 개체들 그리고 그들 사이의 관계를 식별하는 데 도움을 줍니다. 이 능력은 특히 오피오이드 연구 분야에서 여러 요소 간의 복잡한 관계를 이해하는 데 중요합니다.\n\n반면에 지식 그래프는 NLP를 통해 추출된 구조화된 정보를 통합하고 표현하는 강력한 프레임워크를 제공합니다. 개체들 간의 복잡한 관계를 모델링함으로써, 지식 그래프는 연구자들이 고급 질의, 추론, 그리고 지식 탐색 작업을 수행하고 독립된 데이터셋에 숨어있을 수 있는 통찰을 찾아내게 도와줍니다.\n\nNeo4j는 그래프 없이는 불가능한 연결된 데이터를 분석하는 데 사용됩니다. Neo4j는 연결된 데이터를 저장하고 처리하기 위해 특별히 설계된 네이티브 그래프 데이터베이스로, 모든 규모의 복잡한 생명과학 문제를 해결하는 데 도움이 됩니다.\n\nNLP와 지식 그래프의 결합은 중독에 기여하는 다양한 요인을 포괄적으로 이해함으로써 오피오이드 연구의 혁신을 약속합니다. 잠재적인 위험 요소를 식별하고 더 효과적인 예방 및 치료 전략을 개발하는 데 도움을 줄 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 게시물에서는 John Snow Labs의 Healthcare NLP 라이브러리에서 사전 학습된 모델을 사용하여 오피오이드 관련 엔티티 및 그들의 부작용을 추출하고, Neo4J를 사용하여 오피오이드 약물, 증상 및 연구 논문 간의 복잡한 관계를 나타내는 지식 그래프를 구축하는 방법에 대해 다룹니다. Neo4J를 사용하여 오피오이드 약물, 관련 조건 (효과) 및 관련 PubMed 논문과 같은 엔티티를 노드로, 그들의 관계를 엣지로 나타내면, Neo4J가 오피오이드의 부작용과 해당 약물 및 상태가 언급된 PubMed 기사에 대한 통찰을 제공할 수 있습니다. 전체적으로, Neo4J의 고도로 연결된 데이터를 처리할 수 있는 능력은 지식 관리부터 운영 분석 및 의사 결정 지원까지 다양한 의료 분야에서 가치 있는 도구로 작용합니다.\n\n먼저 Spark NLP 소개를 간단히 살펴보고, 좀 더 구체적인 결과를 통해 오피오이드 약물 분석에 대해 설명하겠습니다.\n\n## Spark NLP 및 LLM\n\n헬스케어 라이브러리는 John Snow Labs의 Spark NLP 플랫폼의 강력한 구성 요소로, 의료 분야 내에서 NLP 작업을 용이하게 하는 데 설계되었습니다. 이 라이브러리는 의료 데이터에 특화된 2,200개 이상의 사전 학습된 모델 및 파이프라인을 제공하여 정확한 정보 추출, 임상 및 의학적 개념을 위한 NER 및 텍스트 분석 능력을 제공합니다. 정기적으로 업데이트되고 첨단 알고리즘으로 구성된 이 헬스케어 라이브러리는 전자 의료 기록, 임상 노트 및 생물 의학 문헌과 같은 구조화되지 않은 의료 데이터 원본에서 보다 깊은 통찰을 제공하여 의료 전문가들에게 정보 처리를 간소화하고 강력한 통찰력을 제공하는 것을 목표로 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n존 스노우 랩의 GitHub 저장소는 협업 플랫폼 역할을 하며 사용자들이 스파크 NLP 및 관련 도구의 이해와 활용을 더욱 향상시키기 위해 코드 샘플, 튜토리얼 및 프로젝트와 같은 오픈 소스 자원에 액세스할 수 있는 곳입니다.\n\n존 스노우 랩은 또한 주기적으로 인증 교육을 제공하여 사용자들이 헬스케어 라이브러리 및 NLP 플랫폼의 다른 구성 요소를 활용하는 데 전문성을 쌓을 수 있도록 지원합니다.\n\n존 스노우 랩의 데모 페이지는 라이브러리의 기능을 탐색할 수 있는 사용자 친화적 인터페이스를 제공하여 사용자들이 상호작용적으로 다양한 기능과 모델을 테스트하고 시각화할 수 있도록 돕습니다. 이는 이러한 도구들이 의료 및 기타 분야의 실제 시나리오에 어떻게 적용될 수 있는지에 대한 보다 심층적인 이해를 돕습니다.\n\n## 아편 연구의 이유\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n옵이오이드 전염병은 모든 수명을 앗아가고 심각한 고통을 초래하여 공중보건 위기를 야기했습니다. 이 위기와의 싸움이 격렬해지면서, 연구원과 보건의료 전문가들은 더 심층적인 통찰을 얻고 옵이오이드 중독과 그에 미치는 넓은 영향에 대응할 수 있는 더 효과적인 전략을 개발하기 위해 혁신적인 방법을 모색하고 있습니다.\n\n아래 그래프(국립보건원(NIH) 제작)는 1990년대 후반의 비교적 낮은 수준에서 급격히 증가하여 2022년에 81,000명을 넘는 사망자 수로 끝나는 압도적인 모습을 그렸습니다. 남성과 여성에 대한 그래프의 명확한 선들은 남성 인구에 미치는 비대칭적인 영향을 보여주지만, 양측 모두가 수년 동안 옵이오이드 관련 사망자 수가 상당히 증가한 것을 보여줍니다.\n\n![그래프](/assets/img/2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch_1.png)\n\n## 임상 노트로부터 종양학 관련 개체 추출\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예명(entity) 인식(Named Entity Recognition, NER) 모델 또는 대규모 언어 모델(Large Language Models, LLMs)을 사용하여 비구조화된 텍스트 데이터에서 오피오이드 관련 정보를 추출하는 것은 분석에서 중요한 단계입니다. 전자 의료 기록(EHRs), 임상 노트 및 의학 문헌과 같은 비구조화 데이터 소스에는 많은 가치 있는 정보가 포함되어 있지만 구조화되지 않은 형식으로 제공되기 때문에 중요한 도전이 존재합니다. NER 모델은 특히 이러한 도전에 대응하기 위해 훈련되어 텍스트 내에서 개체를 사전 정의된 범주(라벨링)로 식별하고 분류하는 방식으로 작동합니다.\n\n오피오이드 연구의 맥락에서, NER 모델은 특정 약물 이름(예: 옥시코돈, 펜타닐)과 관련된 개체, 오피오이드 사용과 관련된 환자 증상(예: 호흡 억제, 변비), 그리고 진단, 치료, 동반 질환과 같은 기타 중요한 의학 용어를 인식하고 추출하는 데 훈련됩니다. 고급 NER 모델은 텍스트의 문맥과 의미를 이해하기 위해 심층 학습 기술과 LLMs에 기반한 정교한 알고리즘을 활용하여 개체 인식의 정확성을 크게 향상시킵니다.\n\n명명된 개체 간의 관계를 효율적으로 수립하기 위해 관계 추출을 사용하면 대규모 비구조화 텍스트 데이터를 다루는 데 도움이 됩니다. 이를 통해 인식된 개체와 그들 간의 관계가 포함된 구조화된 정보로 변환됩니다.\n\n비구조화 데이터에서 관련 있는 오피오이드 관련 개체들을 성공적으로 추출한 후, 다음 중요 단계는 이러한 개체들 간의 관계를 수립하는 것입니다. 여기서 지식 그래프(Knowledge Graphs)가 중요한 역할을 합니다. 지식 그래프는 추출된 데이터를 구조화된 형식으로 정리하고, 네트워크 형식의 다른 개체 간의 관계를 매핑합니다. 예를 들어 특정 오피오이드 약물을 해당하는 부작용과 연결하거나, 환자 증상을 특정 오피오이드 처방과 연관시키거나, 환자가 복용 중인 다른 약물과의 상호 작용을 강조할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n과정은 NER 모델이 텍스트 데이터에서 엔티티를 추출하는 것으로 시작합니다. 예를 들어, \"환자가 만성 통증으로 옥시코돈을 처받았지만 심한 변비와 호흡 억제 증상이 발생했습니다\"라는 임상 노트로부터 엔티티로 \"옥시코돈\", \"만성 통증\", \"변비\", \"호흡 억제\"를 식별할 것입니다. 이후, 지식 그래프는 \"알펜타닐\"과 \"신생아 호흡 억제 증후군\" 사이의 caused_by와 \"옥시코돈\"과 \"변비\" 및 \"호흡 억제\" 사이의 caused_by와 같은 관계를 설정할 것입니다.\n\n이 통합된 접근 방식은 데이터 분석을 향상시키는데 그치지 않고 오피오이드 연구의 복잡한 종속성을 포함한 종합적인 이해를 용이하게 합니다. NER 모델을 활용하여 엔티티 추출 및 관계 설정을 위한 관계 추출을 결합하고, 관계 매핑을 위해 지식 그래프를 활용함으로써 연구자들은 숨겨진 패턴을 발견하고 잠재적인 건강 결과를 예측하며, 표적화된 개입 방안을 개발할 수 있습니다. 이 방법론은 오피오이드 위기 대응의 중요한 발전을 나타내며, 오피오이드 사용 및 그에 따른 영향의 예방, 치료, 정책 결정을 위한 더 깊은 통찰력과 보다 효과적인 전략을 제공합니다.\n\n텍스트에서 정보 추출\n\nHealthcare NLP 라이브러리의 NER 모델을 사용하여 생성된 엔티티를 빠르게 시각화하는 능력은 생성된 결과를 조사하는 데 매우 유용한 기능입니다. Spark NLP Display는 Spark NLP로 생성된 추출된 및 레이블이 지정된 엔티티를 시각화하기 위한 오픈 소스 Python 라이브러리입니다. NerVisualizer는 추출된 네임드 엔티티를 강조하고 분석된 텍스트 위에 레이블을 장식으로 표시합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 예시에서는 오피오이드 관련 엔터티를 추출하기 위해 특별히 훈련된 모델(ner_opioid)이 두 가지 오피오이드 의약품을 식별했습니다: 만성 통증에 처방된 옥시코돈(Oxycodone)과 발작 통증에 사용되는 펜티넬 패치(Fentanyl patch). 시각화 도구는 당뇨병 관리에 사용되는 메트포르민(Metformin)과 고혈압에 사용되는 리시노프릴(Lisinopril)과 같은 비오피오이드 의약품을 식별했습니다. 의약품은 타입별(오피오이드 의약품 및 기타 의약품)로 명확하게 분류되었습니다. 또한 의약품의 의도된 용도 또는 관련 상황(일반 증상, 다른 질병)도 추출되어 레이블이 지정되었습니다.\n\n![이미지](/assets/img/2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch_2.png)\n\n관계 추출 시각화 도구는 (가상의) 42세 남성 환자의 의료 기록 및 현재 상태를 예시로 제공하며 오피오이드 사용에 초점을 맞춥니다. 이 도구는 환자가 만성 및 발작성 통증을 주요 불편으로 제기했고, 이를 오피오이드 진통제를 이용한 만성 통증 관리에 연결했습니다. 다이어그램은 환자의 과거 의료 기록을 포함하여 고혈압과 제2형 당뇨병을 보여주며 현재 의약품 처방 요령을 개요로 제시합니다. 이 시각적 표현은 환자의 증상, 의료 기록 및 의약품 사용 사이의 복잡한 관계를 효과적으로 포착하여 잠재적인 오피오이드 관련 합병증 및 추가 의료 관심이 필요한 영역에 대한 소중한 통찰력을 제공합니다.\n\n## 지식 그래프 결과\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사이퍼는 Neo4j의 그래프 쿼리 언어로, 그래프에서 데이터를 검색할 수 있는 SQL과 유사합니다. 다른 언어와 직관적인 유사성으로 인해 가장 쉽게 학습할 수 있는 그래프 언어입니다.\n\n다음 예제에서는 쿼리를 사용하여 모든 노드와 관계를 검색하여, 모든 노드(옵이오이드, 조건, 그리고 PubMed 논문 ID)의 전체 그림을 제공합니다.\n\n이 표현은 데이터 내에서 클러스터, 이상치, 그리고 패턴을 식별하는 데 유용합니다. 밀집된 영역은 노드 간의 높은 연결성과 상호 작용을 나타내어, 연구가 많이 이루어진 영역이나 높은 참조 주제를 시사할 수 있습니다. 반면 희소한 영역이나 고립된 노드는 연구가 적거나 특정 주제를 나타낼 수 있습니다.\n\n이러한 시각화는 연구에서 주요 연구 영역, 연구의 잠재적 공백, 그리고 데이터 관계의 전반적인 구조를 식별하는 데 유용합니다. 의료 조건, 약물 참조 및 학술 논문의 상호 연결된 성질을 이해하는 데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 시각화는 노드와 관계에 대한 더 자세한 정보를 제공하여 데이터의 포괄적인 매핑을 보여줍니다. Alfentanil은 합성, 단순 작용성 아편 진통제로, 펜타닐의 소분자 유도체로 분류됩니다.\n\n중심 노드인 alfentanil은 다양한 질환과 논문에 연결되어 있습니다. caused_by와 같은 관계는 alfentanil이 잠재적으로 일으킬 수 있는 의료 조건을 강조합니다.\n\narticle_of와 mentioned_in의 밀집된 네트워크는 이 데이터 집합 내 엔티티에 대한 포괄적인 연구 및 참고 자료의 유용성을 나타냅니다.\n\n이 시각화는 호흡기 질환과 해당 질환을 유발하거나 관련시킬 수 있는 다양한 아편 진통제 간의 관계에 초점을 맞추고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, 수족의 인정, 펜타닐 시트랏, 듀라게식, 코드인, 옥시코돈 등이 호산구 진통제이거나 호흡 기능을 악화시키거나 어떤 방식으로든 호흡 기능과 연결된 다른 물질일 수 있습니다.\n\n오른쪽의 결과 개요를 보면 총 27개의 노드가 있으며, 10개의 상태 노드, 17개의 호산구 노드 및 25개의 caused_by 관계가 있음을 알 수 있습니다.\n\n아래 예시에서 데이터베이스에서 쿼리를 실행하여 가장 유사한 상태를 가지는 호산구가 무엇인지 확인할 수 있습니다.\n\n쿼리는 호산구 간에 공유된 상태를 반환합니다. 예를 들어, 모르핀과 MS 콘틴은 만성 통증 질환 및 인지 저하와 같은 상태를 공유합니다. 반면 돌로핀과 메타돈은 인지 기능 장애와 불안 증상과 같은 상태를 공유합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 방법은 의료 전문가들이 공유된 부작용과 치료 목적에 대해 이해하는 데 도움이 될 수 있습니다. 또한 공유된 조건을 기반으로 의약품의 부작용을 예측하는 데도 도움이 될 수 있습니다.\n\n![이미지](/assets/img/2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch_3.png)\n\n## 결론\n\n요약하자면, NLP와 지식 그래프의 결합은 오피오이드 연구를 가속화하고 강화하는 강력한 방법을 제공합니다. 이러한 기술을 사용하여 바이오의학 정보의 대량을 효율적으로 추출, 정리 및 연결함으로써 연구자들은 다음을 할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 약물, 부작용 및 생물학적 경로 사이의 숨겨진 관계를 발견하세요.\n- 통증 관리를 위한 잠재적인 새로운 대상 식별\n- 유해한 약물 상호 작용을 예측하고 완화하세요.\n- 처방 관행과 환자 결과에 대한 패턴 분석\n\n아피오이드 위기가 공중 보건에 상당한 어려움을 줄 채 지속되는 가운데, 이러한 고급 컴퓨팅 기술을 활용하는 것은 점점 중요해집니다. 이 기술들은 연구 과정을 효율화할 뿐만 아니라 더 안전하고 효과적인 통증 관리 전략으로 이어질 수 있는 혁신적인 통찰력을 제공합니다.\n\nNLP 및 지식 그래프의 통합은 아피오이드 연구에서 이 복잡한 문제에 대응하는 우리의 능력을 크게 발전시키는 중요한 한걸음입니다. 이러한 기술이 계속 발전함에 따라, 이들은 근거에 기반한 정책 수립, 혁신적 치료법 개발, 그리고 궁극적으로 통증 관리 분야에서 환자 관리 개선에 더 큰 역할을 약속합니다.\n\n부록: 이 연구는 2024년 Neo4j Health Care \u0026 Life Sciences 워크샵에서 발표되었습니다. 전체 워크샵 비디오는 여기에서 확인할 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch_0.png"},"coverImage":"/assets/img/2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch_0.png","tag":["Tech"],"readingTime":8},{"title":"Greg의 비디오에서 배우는 RAG 청킹 5단계 전략","description":"","date":"2024-06-22 21:37","slug":"2024-06-22-FiveLevelsofChunkingStrategiesinRAGNotesfromGregsVideo","content":"\n\n# 소개\n\n대형 데이터 파일을 더 작은 세그먼트로 분할하는 것은 LLM 응용 프로그램의 효율성을 높이는 가장 중요한 단계 중 하나입니다. 목표는 LLM에게 특정 작업에 필요한 정보를 정확하게 제공하는 것이며, 그 이상의 정보는 제공하지 않는 것입니다.\n\n“나의 솔루션에서 적합한 청킹 전략은 무엇이어야 할까”는 고급 RAG 솔루션을 구축하는 동안 LLM 전문가가 필수적으로 결정해야 하는 초기적이고 근본적인 결정 중 하나입니다.\n\nGreg Kamradt는 비디오에서 다양한 청킹 전략을 개요로 제공합니다. 이러한 전략은 RAG 기반 LLM 어플리케이션 개발의 시작점으로 활용될 수 있습니다. 이들은 복잡성과 효과성을 기준으로 다섯 수준으로 분류되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 레벨 1: 고정 크기 청킹\n\n이것은 텍스트를 세그먼트로 분할하는 가장 단순하고 원시적인 방법입니다. 콘텐츠나 구조와 관계없이 텍스트를 지정된 문자 수의 청크로 분할합니다.\n\nLangchain과 llamaindex 프레임워크는 CharacterTextSplitter와 SentenceSplitter(문장을 기준으로 분할하는 것이 기본 설정) 클래스를 이용해 이러한 청킹 기술을 제공합니다. 몇 가지 기억해야 할 개념들 -\n\n- 텍스트가 어떻게 분할되는지: 한 문자씩\n- 청크 크기 측정 방법: 문자 수로\n- 청크 크기: 청크에 포함된 문자 수\n- 청크 중첩: 연속적인 청크에서 중복 데이터를 유지하기 위해 오버랩되는 문자 수\n- 구분자: 텍스트를 분할할 문자(기본 설정은 \"\")\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 레벨 2: 재귀 청킹\n\n고정 크기 청킹은 구현하기 쉽지만 텍스트의 구조를 고려하지 않습니다. 재귀 청킹은 대안을 제공합니다.\n\n이 방법에서는 텍스트를 계층적이고 반복적인 방식으로 더 작은 청크로 나누어 일련의 구분자를 사용합니다. 텍스트를 분할하는 초기 시도가 원하는 크기의 청크를 생성하지 못하는 경우, 해당 방법은 다른 구분자를 사용하여 결과 청크에서 자체를 재귀적으로 호출하여 원하는 청크 크기를 달성합니다.\n\nLangchain 프레임워크는 default 구분자(“\\n\\n”, “\\n”, “ “,””)를 사용하여 텍스트를 분할하는 RecursiveCharacterTextSplitter 클래스를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 레벨 3: 문서 기반 청킹\n\n이 청킹 방법에서는 문서를 그 내재적인 구조에 기반하여 분할합니다. 이 방법은 내용의 흐름과 구조를 고려하지만 명확한 구조가 없는 문서의 경우 효과적이지 않을 수 있습니다.\n\n- 마크다운 포맷의 문서: Langchain은 마크다운을 구분자로 사용하여 문서를 분할하는 MarkdownTextSplitter 클래스를 제공합니다.\n- Python/JS 코드 포함 문서: Langchain은 Python 프로그램을 클래스, 함수 등을 기반으로 분할하는 PythonCodeTextSplitter를 제공하며 RecursiveCharacterTextSplitter 클래스의 from_language 메서드를 통해 언어를 제공할 수 있습니다.\n- 테이블을 다루는 문서: 테이블을 처리할 때 1단계와 2단계에 기반하여 분할하면 행과 열 사이의 관계가 손실될 수 있습니다. 이 관계를 보존하기 위해 테이블 내용을 언어 모델이 이해할 수 있는 방식으로 형식화합니다 (예: HTML의 `table` 태그, `;`로 구분된 CSV 형식 등). 시맨틱 검색 중에 테이블에서 바로 포함된 내용과 일치시키는 것은 어려울 수 있습니다. 개발자들은 종종 추출 후 테이블을 요약하고 해당 요약의 임베딩을 생성하여 일치시키는 데 사용합니다.\n- 이미지를 포함하는 문서 (멀티 모달): 이미지와 텍스트의 임베딩은 서로 다를 수 있습니다 (CLIP 모델은 이를 지원). 가장 좋은 전략은 멀티 모달 모델(예: GPT-4 vision)을 사용하여 이미지의 요약을 생성하고 해당 임베딩을 저장하는 것입니다. Unstructured.io는 pdf 문서에서 이미지를 추출하기 위한 partition_pdf 메서드를 제공합니다.\n\n# 레벨 4: 시맨틱 청킹\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 세 가지 수준은 문서의 내용과 구조를 다루며 일정한 청크 크기의 값을 유지해야 한다. 이 청킹 방법은 임베딩에서 의미를 추출한 다음 이러한 청크 간의 의미적 관계를 평가하기 위한 것이다. 핵심 아이디어는 의미적으로 유사한 청크를 함께 유지하는 것입니다.\n\n![image](/assets/img/2024-06-22-FiveLevelsofChunkingStrategiesinRAGNotesfromGregsVideo_0.png)\n\nLlamindex에는 SemanticSplitterNodeParse 클래스가 있어 문서를 청크로 분할할 수 있습니다. 이는 청크 간의 맥락적 관계를 이용하여 문장 사이의 중단점을 임베딩 유사성을 이용해 적응적으로 선택합니다.\n\n알아둬야 할 몇 가지 개념들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- buffer_size: 초기 창(chunks)에 대한 윈도우를 결정하는 구성 가능한 매개변수\n- breakpoint_percentile_threshold: 다른 구성 가능한 매개변수. 청크를 분할할 임계값을 결정하는 값\n- embed_mode: 사용되는 임베딩 모델\n\n# 레벨 5: 주체적 청킹\n\n이 청킹 전략은 LLM을 사용하여 컨텍스트에 기반하여 청크에 포함해야 할 텍스트 양과 내용을 결정하는 가능성을 탐색합니다.\n\n초기 청크를 생성하려면 Proposals 개념을 사용하며, 이는 원시 텍스트에서 독립된 문장을 추출하는 논문에 기초합니다. Langchain은 이를 구현하기 위한 Proposal 검색 템플릿을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주장을 생성한 후, 이들은 LLM 기반 에이전트에게 공급됩니다. 이 에이전트는 주장이 기존 청크에 포함되어야 하는지 또는 새로운 청크를 생성해야 하는지를 결정합니다.\n\n![이미지](/assets/img/2024-06-22-FiveLevelsofChunkingStrategiesinRAGNotesfromGregsVideo_1.png)\n\n# 결론\n\n본 문서에서는 Langchain 및 Llamaindex 프레임워크에서 다양한 청킹 전략과 그 구현 방법을 탐색했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGreg가 작성한 코드를 찾으려면 다음 링크를 참조하세요: [Greg의 코드](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb)\n\nGenerative AI 및 기계 학습이 더 빠르게 발전하고 있기 때문에 이 기사를 지속적으로 업데이트할 것입니다. Generative AI 및 기계 학습의 발전에 대해 자주 쓰기 때문에 LinkedIn에서 저를 팔로우하십시오: [LinkedIn 프로필](https://www.linkedin.com/in/anurag-mishra-660961b7/)","ogImage":{"url":"/assets/img/2024-06-22-FiveLevelsofChunkingStrategiesinRAGNotesfromGregsVideo_0.png"},"coverImage":"/assets/img/2024-06-22-FiveLevelsofChunkingStrategiesinRAGNotesfromGregsVideo_0.png","tag":["Tech"],"readingTime":4},{"title":"Code Llama로 나만의 LLM 코딩 어시스턴트 만드는 방법 ","description":"","date":"2024-06-22 21:36","slug":"2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama","content":"\n\n이 실습에서는 무료로 사용할 수 있고 로컬 GPU에서 실행되는 AI 코드 어시스턴트를 구현할 예정입니다.\n\n챗봇에 질문을 하면 자연어로 답변하며 여러 프로그래밍 언어로 코드도 제공합니다.\n\n우리는 Hugging Face transformer 라이브러리를 사용하여 LLM을 구현하고 Chatbot 프론트 엔드에는 Streamlit을 사용할 것입니다.\n\n# LLM이 텍스트를 생성하는 방법은 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n디코더 전용 트랜스포머 모델인 GPT 계열은 주어진 입력 프롬프트에 대한 다음 단어를 예측하도록 훈련되었습니다. 이로 인해 텍스트 생성에 아주 능숙합니다.\n\n![이미지](/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_0.png)\n\n충분한 훈련 데이터가 제공된다면, 코드를 생성하는 것도 배울 수 있습니다. IDE에서 코드를 채우는 방식이나 챗봇으로 질문에 답변하는 방식으로 가능합니다.\n\nGitHub Copilot은 상용 예시로서 AI 페어 프로그래머의 한 예입니다. Meta AI의 Code Llama 모델은 유사한 능력을 갖추고 있지만 무료로 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 코드 람마란 무엇인가요?\n\n코드 람마는 Meta AI가 만들고 2023년 8월에 처음으로 출시한 코드 전용 LLM 계열의 특별한 제품입니다.\n\n![이미지](/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_1.png)\n\nMeta AI는 기본 모델 Llama 2(디코더 전용 Transformer 모델로 GPT-4와 유사함)을 시작으로, 대부분 코드로 이루어진 500B 토큰의 교육 데이터를 활용하여 추가 교육을 진행했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그 이후로 Code Llama에 대한 세 가지 버전이 네 가지 다른 크기로 제공됩니다.\n\nCode Llama 모델은 연구 및 상업적 사용을 위해 무료입니다.\n\n![이미지](/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_2.png)\n\n## Code Llama\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드 Llama는 코드 생성을 위한 기반 모델입니다. 코드 Llama 모델은 infill 목적으로 훈련되어 IDE 내에서 코드 완성을 위해 설계되었습니다.\n\n## 코드 Llama — Instruct\n\nInstruct 버전은 인간의 질문에 답변하기 위해 지시 데이터셋에 맞춰 세밀하게 조정되었습니다. 이는 ChatGPT와 유사합니다.\n\n## 코드 Llama — Python\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파이썬 버전은 추가 데이터셋인 100B 토큰의 파이썬 코드로 훈련되었습니다. 이 모델들은 코드 생성을 위해 의도되었습니다.\n\n# LLM 챗봇 코딩\n\n본 튜토리얼에서는 Instruct 버전 중 가장 작은 모델인 CodeLlama-7b-Instruct — hf를 사용할 것입니다. 이 모델은 자연어 질문에 답변하도록 세밀하게 튜닝되어 있기 때문에 챗봇으로 사용할 수 있습니다.\n\n가장 작은 모델조차도 여전히 7B 매개변수로 상당히 큽니다. 매개변수의 16비트 반정밀도를 사용하면, 모델은 약 14 GB의 GPU 메모리가 필요합니다. 4비트 양자화를 사용하면, 메모리 요구 사항을 약 3.5 GB 정도로 줄일 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 모델 구현하기\n\n우리는 먼저 Hugging Face에서 Code Llama 모델을 불러오고 주어진 프롬프트에 기반하여 텍스트를 생성할 ChatModel 클래스를 생성하는 것으로 시작하겠습니다.\n\n우리는 4비트 양자화를 위해 BitsAndBytesConfig를 사용하며, 모델을 로드하기 위해 AutoModelForCausalLM을 사용하고 입력 프롬프트로부터 토큰 임베딩을 생성하기 위해 AutoTokenizer를 사용합니다.\n\n```js\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nclass ChatModel:\n    def __init__(self, model=\"codellama/CodeLlama-7b-Instruct-hf\"):\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True, # 4비트 양자화 사용\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n        )\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model,\n            quantization_config=quantization_config,\n            device_map=\"cuda\",\n            cache_dir=\"./models\", # 모델을 models 폴더에 다운로드\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            model, use_fast=True, padding_side=\"left\"\n        )\r\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한, 사용자의 이전 입력 프롬프트와 AI가 생성한 응답을 저장하는 고정 길이의 히스토리 목록을 만듭니다. 이는 대화의 기억을 제공하여 LLM에게 대화의 기억을 부여하는 데 유용합니다.\n\n```js\nself.history = []\nself.history_length = 1\n```\n\nCode Llama은 사용자 프롬프트 앞에 시스템 프롬프트를 사용합니다.\n\n기본적으로, codellama-13b-chat 예제에서 시스템 프롬프트를 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nself.DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n당신은 코드와 소프트웨어 디자인에 대한 깊은 지식을 가진, 도움이 되는, 예의 바르고 정직한 도우미입니다. 항상 도움이 될 수 있는 답변을 해야 하며, 안전하고 신중해야 합니다. 답변에 해로운, 부정한, 인종 차별적, 성 차별적, 유해한, 위험한, 또는 불법적인 내용을 포함해서는 안 됩니다. 답변이 사회적으로 편향되거나 부정적이여선 안됩니다.\\n\\n만약 질문이 이해할 수 없거나 사실적으로 일관성이 없다면, 올바른 대답 대신 왜 잘못된 것인지 설명하세요. 만약 질문에 대한 대답을 모르면, 가짜 정보를 공유하지 말고 대신 말해주세요.\\\n        \"\"\"\n```\n\n이제 self.history에 현재 대화를 추가하는 함수를 구현해봅시다.\n\nLLM(어라운드  모델)은 한정된 문맥 길이를 가지고 있기 때문에 메모리에 정보를 한정적으로 보관할 수밖에 없습니다. 여기서는 self.history_length = 1 개의 질문과 대답만 최대한 보관합니다.\n\n```js\n    def append_to_history(self, user_prompt, response):\n        self.history.append((user_prompt, response))\n        if len(self.history) \u003e self.history_length:\n            self.history.pop(0)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내 우리는 입력 프롬프트에 기반한 텍스트를 생성하는 generate 함수를 구현합니다.\n\n각 LLM에는 훈련에 사용된 특정 프롬프트 템플릿이 있습니다. Code Llama의 경우 codellama-13b-chat의 프롬프트 템플릿을 참조로 사용했습니다.\n\n```js\n    def generate(\n        self, user_prompt, system_prompt, top_p=0.9, temperature=0.1, max_new_tokens=512\n    ):\n\n        texts = [f\"\u003cs\u003e[INST] \u003c\u003cSYS\u003e\u003e\\n{system_prompt}\\n\u003c\u003c/SYS\u003e\u003e\\n\\n\"]\n        do_strip = False\n        for old_prompt, old_response in self.history:\n            old_prompt = old_prompt.strip() if do_strip else old_prompt\n            do_strip = True\n            texts.append(f\"{old_prompt} [/INST] {old_response.strip()} \u003c/s\u003e\u003cs\u003e[INST] \")\n        user_prompt = user_prompt.strip() if do_strip else user_prompt\n        texts.append(f\"{user_prompt} [/INST]\")\n        prompt = \"\".join(texts)\n\n        inputs = self.tokenizer(\n            prompt, return_tensors=\"pt\", add_special_tokens=False\n        ).to(\"cuda\")\n\n        output = self.model.generate(\n            inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            pad_token_id=self.tokenizer.eos_token_id,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            top_p=top_p,\n            top_k=50,\n            temperature=temperature,\n        )\n        output = output[0].to(\"cpu\")\n        response = self.tokenizer.decode(output[inputs[\"input_ids\"].shape[1] : -1])\n        self.append_to_history(user_prompt, response)\n        return response\n```\n\n응답은 시스템 프롬프트와 사용자 프롬프트를 기반으로 합니다. 답변의 창의성은 top_p 및 temperature와 같은 매개변수에 따라 달라집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ntop_p를 사용하면 출력 토큰의 확률 값을 제한하여 너무 드물게 발생하는 토큰을 생성하는 것을 피할 수 있어요:\n\ntemperature를 사용하면 출력 토큰의 확률 분포를 평평하게 하거나 날카롭게 할 수 있어요:\n\n프론트엔드 애플리케이션을 진행하기 전에 ChatModel을 테스트해보죠.\n\n```js\nfrom ChatModel import *\n\nmodel = ChatModel()\nresponse = model.generate(\n    user_prompt=\"C++에서 hello world 프로그램을 작성해봐\", \n    system_prompt=model.DEFAULT_SYSTEM_PROMPT\n)\nprint(response)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n당신이 요청한 작업은 완료되었습니다. 이제 테이블 태그가 Markdown 형식으로 변경되었습니다.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\nimport streamlit as st\nfrom ChatModel import *\n\nst.title(\"Code Llama Assistant\")\n\n\n@st.cache_resource\ndef load_model():\n    model = ChatModel()\n    return model\n\n\nmodel = load_model()  # load our ChatModel once and then cache it\r\n```\n\n다음으로 generate 함수를 위한 모델 매개변수를 입력 제어하는 사이드바를 생성합니다.\n\n```js\r\nwith st.sidebar:\n    temperature = st.slider(\"온도\", 0.0, 2.0, 0.1)\n    top_p = st.slider(\"top_p\", 0.0, 1.0, 0.9)\n    max_new_tokens = st.number_input(\"max_new_tokens\", 128, 4096, 256)\n    system_prompt = st.text_area(\n        \"시스템 프롬프트\", value=model.DEFAULT_SYSTEM_PROMPT, height=500\n    )\r\n```\n\n그리고 챗봇 메시지 인터페이스를 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 채팅 기록 초기화\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\n# 앱 재실행시 기록된 채팅 메시지 표시\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\n# 사용자 입력 받기\nif prompt := st.chat_input(\"무엇이든 물어보세요!\"):\n    # 사용자 메시지를 채팅 기록에 추가\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    # 사용자 메시지를 채팅 메시지 컨테이너에 표시\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n\n    # 챗봇 응답을 채팅 메시지 컨테이너에 표시\n    with st.chat_message(\"assistant\"):\n        user_prompt = st.session_state.messages[-1][\"content\"]\n        answer = model.generate(\n            user_prompt,\n            top_p=top_p,\n            temperature=temperature,\n            max_new_tokens=max_new_tokens,\n            system_prompt=system_prompt,\n        )\n        response = st.write(answer)\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\r\n```\n\n스트림릿 앱을 streamlit run app.py로 실행하여 브라우저가 열립니다.\n\n이제 챗봇에 코딩 관련 질문을 할 수 있습니다.\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희는 Meta AI의 Code Llama LLM을 활용하여 AI 코딩 어시스턴트를 구현했어요. 그리고 Hugging Face의 transformer 라이브러리와 Streamlit을 사용해서 프론트엔드 애플리케이션을 만들었어요.\n\n6GB의 GPU 메모리를 갖춘 노트북으로는 4비트 양자화된 Code Llama 모델을 7B 매개변수와 함께 사용할 수밖에 없었어요. 더 큰 GPU를 사용하면 16비트 버전이나 더 큰 모델이 더 잘 작동할 것입니다.\n\nP.S. Code Llama로부터 제가 받은 농담보다 더 재미있는 농담들을 기대해봅니다 🤡.\n\n더 많은 LLM에 관심이 있으시다면, 최근에 공개된 오픈소스 모델에 대한 개요를 확인해보세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n[1] B. Rozière 외: Code Llama: 코드를 위한 오픈 기반 모델 (2023), arXiv:2308.12950\n\n# 자원\n\n- Streamlit 채팅 앱 예제: 기본 LLM 채팅 앱 구축\n- Hugging Face Code Llama gradio 구현: codellama-13b-chat\n- 이 문서의 전체 작업 코드: [https://github.com/leoneversberg/codellama-chatbot](https://github.com/leoneversberg/codellama-chatbot)","ogImage":{"url":"/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_0.png"},"coverImage":"/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_0.png","tag":["Tech"],"readingTime":9},{"title":"효율적인 HNSW 인덱싱 대규모 병렬 처리를 통한 인덱스 빌드 시간 단축 하는 방법","description":"","date":"2024-06-22 21:34","slug":"2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism","content":"\n\n## 소개\n\n검색 증강 생성 (RAG)을 위한 Generative AI (GenAI) 및 전자 상거래를 위한 추천과 같은 응용 프로그램에 의해 주도되는 벡터 데이터베이스 분야에서, HNSW (Hierarchical Navigable Small Worlds) 알고리즘은 근사 최근접 이웃 (ANN) 검색을 위해 상위 벡터 데이터베이스 제공 업체에서 많이 사용됩니다. 이 알고리즘은 빠른 검색과 좋은 검색률을 제공하지만 인덱스 빌드 시간이 길다는 점을 감수해야 합니다.\n\n인덱스 빌드 시간이 길면 확장성 제한, 운영 비용 증가, 개발자들을 위한 실험 감소, 그리고 동적 데이터셋의 느린 업데이트와 같은 여러 가지 도전에 직면하게 됩니다.\n\n본 블로그 포스트에서는 HNSW 인덱스를 빌드하는 데 오랜 시간이 걸리는 이유를 설명하고, 고도의 병렬성을 활용하여 CPU를 기반으로 한 기존 솔루션과 비교하여 인덱스 빌드 시간을 대략 85% 줄일 수 있는 솔루션을 제시합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## HNSW 개요\n\nHNSW는 쿼리의 근사 최근 이웃을 효율적으로 탐색하는 그래프 기반 알고리즘입니다. 그래프에는 계층적 노드(벡터 임베딩)가 있으며, 각 계층에는 이전 계층의 일부가 포함됩니다. 노드는 에지로 연결되어 있으며, 에지는 그들 사이의 거리를 나타내며, 거리는 코사인 유사성과 같은 메트릭으로 측정됩니다.\n\n상위 계층은 더 적은 노드와 더 긴 에지를 가지고 있어서 그래프의 먼 영역을 연결하고 벡터 공간을 빠르게 탐색할 수 있습니다. 가장 아래 계층에는 모든 벡터가 포함되어 있으며, 짧은 범위의 에지(인접 노드로의 연결)를 가지고 있어서 더 세부적인 검색이 가능합니다.\n\n그림 1은 HNSW 그래프 구조의 간단한 예를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism_0.png)\n\n인덱스를 구축하거나 업데이트하는 동안, 새 노드는 기존 노드로부터의 거리에 따라 삽입됩니다. 각 레이어마다 각 정점의 최근 이웃 목록이 유지되며, 해당 크기는 매개변수 ef_construction에 의해 결정됩니다.\n\n알고리즘은 이 목록의 노드를 반복하면서, 가장 가까운 이웃들과의 거리 계산을 수행하여 노드의 이웃이 쿼리에 더 가까운지 확인합니다. 그렇다면 해당 이웃은 목록에 추가할 후보로 고려됩니다.\n\n더 큰 ef_construction은 더 많은 후보를 추적하고 평가하여 실제 최근 이웃을 찾을 가능성을 높이며, 정확도를 향상시키지만 더 많은 거리 계산이 필요하기 때문에 인덱스 구축 시간이 늘어납니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 느린 인덱스 빌드 시간의 결과\n\n이전 섹션에서 볼 수 있듯이, HNSW 인덱스를 구축하려면 계층적 그래프에서 가장 가까운 이웃을 찾기 위해 많은 거리 계산이 필요합니다. 이로 인해 검색 대기 시간이 낮고 재현율이 좋아지지만, 이는 빌드 및 업데이트가 느린 그래프라는 희생을 갖게 됩니다.\n\n예를 들어, Figure 2에서 볼 수 있듯이, eBay의 경험에 따르면 1억 6천만 개의 벡터를 위해 HNSW 인덱스를 구축하는 데 3~6시간이 걸릴 수 있으며 데이터셋이 커질수록 빌드 시간이 빨리 증가합니다.\n\n이는 수십억 개의 실시간 리스트가 있는 이러한 기업에게 문제가 될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism_1.png\" /\u003e\n\n느린 인덱스 빌드 시간은 많은 도전을 야기할 수 있습니다: 확장성 문제, 운영 비용 증가, 개발자 응용 프로그램 실험 제한 및 동적 데이터 세트 업데이트 지연.\n\n확장성 문제\n\n인덱스 빌드 시간이 느릴수록 전자 상거래 및 RAG와 같은 애플리케이션의 확장성이 제한됩니다. 전자 상거래의 경우, 고객 기반과 제품 카탈로그 확장으로 인덱스 빌드 시간이 증가하여 관련 제품 추천의 서비스가 지연됩니다. RAG의 경우, 더 큰 데이터 세트는 높은 품질의 응답을 제공하지만, 느린 인덱스 빌드로 효율적으로 관리할 수 있는 데이터 양이 제한됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n운영 비용 증가\n\n인덱스 빌드 시간이 느린 경우 CPU 및 GPU와 같은 계산 리소스가 더 오래 사용됩니다. 이는 특히 리소스 사용에 비례한 비용이 발생하는 클라우드 컴퓨팅 환경에서 운영 비용을 증가시킵니다.\n\n개발자 응용 프로그램 실험 제한\n\n개발자들은 모델을 세밀하게 조정하고 응용 프로그램 성능을 개선하기 위해 빠르게 실험해야 합니다. 인덱스 빌드 시간이 길면 실험 횟수를 제한하고 그 실험을 평가하는 데 필요한 시간이 증가합니다. 이는 혁신을 늦추고 개선을 미루게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n동적 데이터셋의 느린 업데이트\n\n전자 상거래 및 RAG와 같은 애플리케이션에는 빈번히 업데이트되는 동적 데이터셋이 있습니다. 느린 인덱스 빌드는 새로운 데이터의 통합을 지연시켜 고갱도 비추천 및 응답의 만료로 이어지며, 이는 사용자 만족도 및 신뢰에 부정적인 영향을 미칠 수 있습니다.\n\n## 인덱스 빌드 시간을 줄이는 방법\n\n인덱스 빌드 시간을 줄이는 두 가지 효과적인 방법은 병렬 처리와 벡터 양자화입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n병렬 처리\n\n데이터 세트는 벡터 군집으로 분할되고 해당 군집 내에서 가장 가까운 이웃 거리 계산이 병렬로 수행됩니다. 병렬 처리는 거리 계산에 필요한 시간을 크게 줄여줍니다. 이는 인덱스 구축 과정 중 가장 시간이 많이 소요되는 부분입니다.\n\n그러나 대부분의 HNSW 인덱스 빌드 솔루션에서 사용하는 CPU는 제한된 병렬 처리 능력을 갖고 있으므로 더 높은 병렬 처리 능력을 갖춘 솔루션이 필요합니다.\n\n벡터 양자화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n벡터 양자화는 벡터를 압축하여 데이터 전송 당 더 많은 벡터를 포장할 수 있게 하며, 더 적은 비트에서 최근접 이웃 거리 계산을 수행하여 단순화합니다. 이로써 외부 메모리에서의 느린 메모리 접근 수를 줄이고 거리 계산 속도를 높일 수 있습니다.\n\n## 메모리 내 계산 (CiM) 연상 처리 - 유연한 대규모 병렬 처리\n\nGSI Technology의 메모리 내 계산 연상 처리 장치 (APU)는 백만 개의 비트 프로세서를 가진 기술로, 비트 수준에서 계산을 수행하여 유연한 양자화를 가능케 합니다. 이는 모든 크기의 데이터 요소에서 대규모 병렬 처리가 가능하도록 합니다.\n\nAPU를 사용하면 계산이 메모리 내에서 직접 수행되므로 프로세서와 메모리 간의 전통적인 병목 현상을 피할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAPU HNSW Index Build Process\n\n1. 양자화: 데이터 세트를 원하는 비트 길이로 압축합니다 (예: 특성 당 4비트).\n\n2. 클러스터링: K-평균 클러스터링을 사용하여 데이터 세트를 클러스터로 분할합니다.\n\n3. 할당: 각 정점을 클러스터 중심까지의 거리를 기반으로 가장 가까운 클러스터에 할당합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4. 데이터 불러오기: 여러 클러스터를 APU에 로드합니다.\n\n5. 최근접 이웃 탐색: APU에 로드된 클러스터에 할당된 정점들을 대상으로 최근접 이웃 탐색을 수행합니다.\n\n6. 단계 4와 5 반복: 모든 클러스터가 처리되고 모든 정점이 그래프에 삽입될 때까지 단계 4와 5를 반복합니다.\n\n7. 이웃들의 합집합: 각 정점에 대해 다수의 클러스터에서 가장 가까운 이웃들을 병합합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n8. 최적화: 연결 후, 간선을 양방향으로 만들고 각 정점이 중복된 간선을 제거하여 `= K`개의 이웃을 보유하도록 보장합니다.\n\nAPU는 수백만 비트 프로세서를 사용하여 5단계에서의 최근접 이웃 거리 계산을 병렬로 수행합니다. 이 대규모 병렬 처리는 유연한 양자화와 함께 거리 계산에 필요한 시간을 크게 줄여줍니다. 최근접 이웃 거리 계산을 수행하는 것은 인덱스를 구축하는 가장 시간이 많이 소요되는 부분입니다. 이를 줄이는 것이 인덱스를 빠르게 구축하는 데 가장 큰 영향을 미칠 것입니다.\n\n## 결과\n\nNvidia의 Figure 3에 따르면, 인텔 Xeon Platinum 8480CL CPU는 1억 개의 벡터를 위한 HNSW 인덱스를 구축하는 데 약 5,636초(약 1.5시간)가 걸리는 것을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Figure 4](/assets/img/2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism_2.png)\n\nFigure 4에 따르면 APU 시스템이 100 백만 벡터 HNSW 인덱스를 864초(약 15분)에 구축할 수 있습니다. 이는 Intel Xeon Platinum 8480CL CPU의 5,636초(약 1.5시간)에 비해 약 85%의 시간 단축입니다.\n\n또한, Figure 4는 APU 시스템이 약 2시간에 10억 벡터 HNSW 인덱스를 구축할 수 있다는 것을 보여주며, 이는 10억 벡터를 가정하여 eBay의 성능 개선을 비교했을 때 극적인 향상을 보입니다.\n\n![Figure 4](/assets/img/2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism_3.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 결론\n\nHNSW는 GenAI와 전자 상거래와 같은 애플리케이션에서 사용되는 벡터 유사성 검색에 중요한 알고리즘입니다. 높은 검색률과 빠른 검색을 제공하지만, 색인 빌드 시간이 길어지는 단점이 있습니다.\n\n색인 빌드 시간을 줄이는 주요 방법 중 하나는 병렬화입니다. 안타깝게도 대부분의 현재 솔루션은 한정된 병렬 처리 능력을 갖춘 CPU를 사용합니다.\n\n대량의 병렬 처리와 유연한 양자화를 통해 GSI Technology의 APU는 기존 CPU 기반 솔루션과 비교하여 색인 빌드 시간을 약 85% 줄일 수 있습니다. 이는 확장성을 향상시키고 운영 비용을 낮추며 빠른 개발자 애플리케이션 실험을 가능하게 하며, 동적 데이터 세트에 대한 적시적인 업데이트를 보장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAPU의 유연한 비트 수준 처리가 대량 병렬 처리를 제공하여 인덱스 빌드 시간을 크게 줄이는 방법에 대한 자세한 내용은 화이트페이퍼 \"효율적인 HNSW: 인덱스 빌드 시간을 85%로 단축\"을 읽어보세요.\n\nGSI 기술의 온프레미스 및 클라우드 기반 HNSW 인덱스 빌드 옵션에 대해 알아보려면 associativecomputing@gsitechnology.com 으로 문의하세요.","ogImage":{"url":"/assets/img/2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism_0.png"},"coverImage":"/assets/img/2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism_0.png","tag":["Tech"],"readingTime":6},{"title":"일반 NLP 방법론 소개 기초부터 고급까지","description":"","date":"2024-06-22 21:31","slug":"2024-06-22-IntroofGenericNLPmethods","content":"\n\n# NLP 배경 리뷰\n\n자연어 처리(NLP)는 광범위한 응용 분야를 갖고 있고 챗봇, 기계 번역과 같이 점점 더 다양한 제품들이 상용화되고 있습니다. 그러나 다양한 작업들은 각기 다른 요구 사항을 가지고 있어 하나의 표준 NLP 모델로는 이러한 요구 사항을 다루기 어렵습니다. 최근의 작업에서는 기계가 의도를 심층적으로 이해하고 자연스럽게 언어를 생성하기를 요구합니다. 딥러닝의 빠른 발전으로 복잡한 NLP 모델이 더 복잡한 문제들을 다룰 수 있게 되었습니다. NLP에 대한 미래는 밝아 보입니다. 그러나, 연구자들과 전문가들에게는 새로운 어려움의 산이 등장하고 있습니다. 더 효율적인 NLP 모델을 찾고 고품질의 데이터셋을 수집하고 주어진 계산 및 저장 자원의 제한을 다루기 위한 실행 가능한 해결책을 마련해야 하는 엄중한 도전에 직면하고 있습니다. 이러한 어려움은 한 번에 해결될 수 없습니다. 먼저 거대한 모델을 사전 학습하고 그 모델을 세밀하게 조정하거나 프롬프팅하는 두 단계 방법은 실행 가능하며 새로운 표준 방법이 되고 있습니다.\n\n개인적 취향으로 최근 NLP에서 사용되는 고급 방법들을 배우는 데 시간을 투자했습니다. NLP는 상당히 복잡한 문제로 여겨집니다. 이러한 방법들에 대한 기본 아이디어들은 다른 작업 영역에 적용될 수 있으며, IoT를 위한 편재 학습과 같은 다중 모드 학습이 있습니다.\n\n아래는 사전 학습, 세밀 조정, 프롬프트와 관련된 고전적 방법들을 정리해 보았습니다. 그 중에 BERT, GPT, 적응형 세밀 조정, 행동 세밀 조정, 파라미터 효율적 세밀 조정, 프롬프트 엔지니어링이 포함되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 사전 훈련 소개\n\n어노테이션 및 대규모 데이터셋의 수집은 비용이 많이 들거나 일부 특정 영역의 작업에는 불가능한 경우도 있습니다. 그러나 위키피디아와 같이 많은 양의 라벨이 없는 데이터가 많이 있다는 사실이 있습니다. 이에 따라 자연어 처리 분야에서 싼 비용으로 대규모 라벨이 없는 말뭉치를 사용하여 사전 훈련된 언어 모델(PLM)은 매우 큰 가치를 추가합니다. 이는 PLM이 하류 작업에 대해 빠르고 효과적으로 파인 튜닝될 수 있다는 것을 의미합니다. PLM은 효율적인 모델 초기화를 제공하며 다양한 작업에서 수렴을 가속화할 수 있습니다. 또한 PLM은 특정 영역에 대한 오버피팅을 어느 정도 피할 수 있습니다.\n\n일반적으로 PLM은 문맥 없는 임베딩과 문맥을 고려한 임베딩으로 나뉩니다. 문맥 없는 임베딩 방법은 매우 제한적인 문맥 정보를 갖고 있는 정적 단어 임베딩에 중점을 두며 상대적으로 간단한 모델로 구성됩니다. 이 방법에는 NNLM, word2vec (CBOW, Skip-Gram), Glove가 포함됩니다. 문맥을 고려한 임베딩 방법은 단어 토큰을 문맥 정보를 통합하여 매핑하는 것을 목표로 하며, CoVe, ELMo, GPT, BERT, XLNet이 이에 해당됩니다.\n\n![이미지](/assets/img/2024-06-22-IntroofGenericNLPmethods_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTransformer 아키텍처는 NLP 작업을 다루는 강력한 성능을 나타내어, 최근 PLM에서 Transformer 아키텍처를 활용하고 있다 [17]. BERT는 Transformer Encoder 아키텍처를 활용하며, 마스크된 언어 모델링을 특징으로 하며, 양방향 자동 인코딩 언어 모델이다 [18]. GPT는 Transformer Decoder 아키텍처를 활용하며, 단방향 자동 회귀 언어 모델이다 [19].\n\n\u003cimg src=\"/assets/img/2024-06-22-IntroofGenericNLPmethods_1.png\" /\u003e\n\n사전 훈련 작업은 클래식 언어 모델링, 마스크된 언어 모델링, 순열 언어 모델링, 노이즈 제거 오토인코더, 대조 학습으로 분류될 수 있다. 모델 아키텍처를 개선하거나 지식 그래프와 같은 더 많은 사전 정보를 통합함으로써, RoBERTa [20], ELECTRA [21], BART [22], T5 [23], XLNet, MPNet [24], ERNIE-BaiDu [25], Transformer-XL [26], DistilBERT [27], ALBERT [28], MobileBERT [29], ConvBERT [30], DeBERTa [31], BigBird [32]와 같이 다양한 BERT 또는 GPT 확장 모델이 출현하고 있다.\n\n# 파인튜닝 소개\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사전 학습된 임베딩을 활용한 이전 특성 추출과 비교했을 때, 사전 학습된 언어 모델(PLM)의 파인 튜닝은 자연어 처리의 다양한 하향 작업으로의 전이 학습에 더 효과적이고 강력함이 입증되었습니다. 일반적으로 채택된 파인 튜닝 절차는 첫 번째 단계로 레이블이 지정되지 않은 대규모 데이터를 기반으로 모델을 사전 학습하는 것입니다. 그 과정에서 가장 보편적으로 사용되는 방법은 가려진 언어 모델링(MLM) 접근 방식입니다. 두 번째 단계는 다운스트림 작업이나 도메인에 특화된 레이블이 지정된 데이터에서 PLM을 파인 튜닝하는 것이며, 이때 교차 엔트로피 손실 함수와 같은 표준 손실 함수가 사용됩니다.\n\nPLM은 비교적 고정되어 있고 계산 리소스가 더 많이 필요하며, 더 많은 데이터셋, 효율적인 모델 업데이트가 필요하지만, 파인 튜닝은 다양한 실용적 용도에 중요한 빠른 적응형 모델을 제공할 수 있습니다. 따라서 파인 튜닝은 귀중하며 최근 고급 방법은 파인 튜닝 성능을 더욱 향상시킬 수 있습니다. 예를 들어 적응형 파인 튜닝, 행동적 파인 튜닝, 매개변수 효율적 파인 튜닝, 텍스트 대 텍스트 파인 튜닝, 파인 튜닝 불안정성 완화 등이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n적응형 미세 조정은 PLM을 대상 데이터 분포에 더 적응하도록 전이하는 것을 의미합니다. PLM은 MLM(Masked Language Model) 접근 방식을 사용하여 관련 없는 레이블이 붙지 않은 데이터에 대해 미세 조정을 한 다음, 특정 작업 레이블이 붙은 데이터에 대해 교차 엔트로피 손실을 사용하여 모델을 미세 조정합니다. 대상 데이터 도메인으로의 전이는 특히 전문적인 다른 대상 데이터 분포에 유용하며, 이로 인해 최종 모델의 일반화 능력이 제한되기도 합니다.\n\n[이미지1]\n\n행동적 미세 조정은 주로 작업에 초점을 맞추어 PLM을 작업 레이블이 붙은 데이터에 대해 작업 관련 손실 함수를 사용하여 먼저 미세 조정한 후에 작업을 진행합니다. 이 중간 미세 조정 훈련은 높은 수준의 추론과 추론에 유익하며, 이름이 붙은 엔티티, 파라프레이징, 구문, 답변 문장 선택, 질문에 답변하기와 같은 작업에 적용됩니다.\n\n[이미지2]\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nParameter-efficient fine-tuning은 Downstream 작업의 계산 부담을 크게 줄이기 위해 대부분의 PLM을 고정시키고 소수의 매개변수만 미세 조정하는 것이다. 채택된 어댑터는 몇 개의 작은 레이어를 동결된 PLM에 삽입한다. 또는 PLM의 하위 매개변수를 미세 조정하고 나머지 큰 집합을 동결시킬 수도 있다.\n\nText-to-text fine-tuning은 가리거나 문맥연결 모델을 상위 자체 학습 레이어로 대체하여, 가리기 언어 모델을 자율적 모델로 이전시키는 것이다.\n\n작은 대상 데이터셋과 임의의 가중치 초기화는 미세 조정 모델의 심각한 불안정성을 유발할 수 있다. 따라서, PLM 모델을 중간으로 전송하여 대상 도메인이나 작업으로 불안정성을 완화할 수 있으며, 적대적 또는 신뢰 영역 기반 데이터 증강 방법도 유용하다.\n\n# Prompting 소개\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNLP 모델이 점점 더 정교해지면, 분명한 딜레마가 발생합니다. 한편에서는 수십억 개의 매개변수를 가진 보다 일반적이고 강력하지만 무거운 PLM이 존재하는 반면, 다른 한편에서 downstream 작업은 오직 독특하고 가벼우며 빠르게 적응하는 모델이 필요합니다. 한편에서는 많은 편향과 노이즈가 있는 주석이 되지 않은 원본 데이터가 많이 있지만, 다른 한편에서는 과연 소량이고 매우 비싼 과제별 주석 데이터만 사용할 수 있습니다. 한편에서는 대규모 데이터로 상당히 거대한 모델을 훈련시키는 것이 가능하며 고성능 컴퓨팅 센터를 통해 이것이 이루어집니다. 다른 한편에서는 downstream 작업이 한정된 계산 리소스를 제공할 수 있으며 스마트폰과 같은 엣지 컴퓨테이션만 가능합니다. 한편에서는 고전적인 PLM이 더 오랜 시간 동안 업데이트되며, 다른 한편에서는 downstream 작업이 요구 사항을 훨씬 더 빠르게 변경합니다. 요약하면, 이 중첩된 접근법은 방대한 범용 PLM과 유연한 가벼운 과제별 파인튜닝 모델로 이어지며 이러한 목표를 달성하는 데 점점 더 많은 비용이 듭니다. [40]은 이에 대처하기 위한 네 번째 패러다임, 즉 사전 훈련, 프롬프트, 예측을 요약했습니다. 회고적으로, 첫 번째 패러다임은 신경망 없이 특성 엔지니어링을 수동으로 해야 합니다. 두 번째 패러다임은 신경망을 활용하며 아키텍처 엔지니어링을 수동으로 해야 합니다. 세 번째 패러다임은 사전 훈련 및 파인튜닝 접근법을 활용하며 사전 훈련 및 파인튜닝을 위해 적절한 손실 함수를 찾기 위해 목적 함수 엔지니어링을 수동으로 해야 합니다. 네 번째 패러다임은 사전 훈련, 프롬프트, 예측 접근법을 활용하며 적절한 프롬프트를 찾기 위해 프롬프트 마이닝 엔지니어링을 필요로 합니다.\n\n프롬프팅 워크플로우는 4단계로 나눌 수 있습니다. 첫 번째는 프롬프트 구성으로, 필요한 작업을 템플릿에 매핑할 수 있으며, 채워진 템플릿은 PLM에 입력되어 빈 칸 프롬프트(템플릿 텍스트 문자열 중간의 미채워진 슬롯) 및 접두사 프롬프트(템플릿 텍스트 문자열 시작 부분의 미채워진 슬롯)을 포함한 답변을 생성합니다. 두 번째는 답변 구성으로, 생성된 답변을 필요한 레이블로 변환하는 맵 함수를 만들어낼 수 있습니다. 세 번째는 답변 예측으로, 실제로는 고유하게 채워진 템플릿이 PLM에 입력되어 해당하는 답변을 생성합니다. 네 번째는 답변 레이블 매핑으로, 해당하는 답변을 필요한 레이블에 매핑합니다. 요약하면, 원래 입력 x를 템플릿에 채워서 프롬프트 x'를 얻은 후, 일부 빈 칸 슬롯을 채워서 프롬프트 x'를 PLM에 입력하여 완전히 채워진 문자열 x''을 출력하며, 이것은 최종적으로 원하는 레이블 y로 매핑될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러므로 템플릿을 가져오는 것이 중요합니다. 즉, 프롬프트 엔지니어링으로 분류될 수 있는데, 이는 수동 템플릿 엔지니어링, 이산 프롬프트를 포함한 자동화된 템플릿 학습, 프롬프트 마이닝, 프롬프트 해석, 기울기 베이스 검색, 프롬프트 생성, 프롬프트 점수 및 접두사 튜닝, 이산 프롬프트로 초기화된 튜닝, 하드 소프트 프롬프트 혼합 튜닝이 추가로 포함됩니다. 더불어 다중 프롬프트는 프롬프트 성능을 향상시키고 결과를 확장할 수 있습니다. 프롬프트 앙상블, 프롬프트 증강, 프롬프트 구성, 프롬프트 분해를 포함합니다. 프롬프트를 생성하는 다양한 방법이 있지만, 최선의 방법을 결정하기 위한 직접 비교는 어렵습니다. 게다가 일부 실험에서는 템플릿의 약간의 변형이나 변경이 결과에 큰 차이를 일으킬 수 있습니다.\n\n프롬프트 [41-46]는 지식 탐색, 분류 기반 작업, 정보 추출, 자연어 처리에서의 추론, 질문에 대한 답변, 텍스트 생성, 텍스트 생성 자동 평가, 멀티모달 학습, 메타 응용 프로그램, 리소스 등과 같은 다양한 하류 작업에 널리 활용될 수 있습니다. 이러한 것들은 프롬프트의 일반화된 성능과 다양한 하류 작업에 대한 무감하임을 증명합니다. 이는 프롬프트 방법이 다양한 하류 작업을 완료할 수 있는 다양한 프롬프트 템플릿을 갖춘 효과적인 PLM을 실현할 수 있다는 것을 의미합니다. 흔히잘 알려진 피드백-\"답변\" 작업이 필요한 경우에, 일반적으로 고전적인 세밀한 튜닝 방법을 사용하여 PLM을 적응시키기 위해 특정 작업 영역 지식이 필요합니다. 반면 프롬프팅은 PLM이 학습한 지식을 활용합니다. 그렇기에 프롬프팅은 다양한 하류 작업의 모델 업데이트에 필요한 계산 부하를 줄이고, 모델 복사본을 저장할 필요가 없다는 것이고, 연습으로 특정 하류 작업에 대한 주석이 달린 데이터는 종종 매우 비싸며, 프롬프팅은 이 작업에 조금의 특정 데이터나 이전 지식으로 유익할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![IntroofGenericNLPmethods](/assets/img/2024-06-22-IntroofGenericNLPmethods_10.png)\n\n작업 특정 데이터 세트의 주석이 작고, 계산 리소스가 제한되며 엣지 노드의 저장 공간이 제한되어 실시간 응답이 필요하고, 다양한 하위 작업의 신속하게 변화하는 요구 사항이 고려될 때 사전 훈련 모델을 채택하고 그 후 적응 엔지니어링을 하는 것이 가장 좋은 방법으로 인정받고 있습니다. Transform 모델은 NLP에서 효과적이라는 것이 입증되었습니다. 따라서 BERT, GPT 및 그 이후에 축Enhanced한 모델들이 널리 사용되고 있습니다. 튜닝 작업은 사전 훈련된 모델을 특정 작업에 적응하는 데 초점을 맞춥니다. 따라서 더 높은 성능을 달성하기 위해 작업 데이터나 작업 도메인으로 이동하는 중간 단계를 채택하는 것이 선호됩니다. 삽입된 적응기를 사용하면 더 동적인 작업 요구 사항을 처리할 수 있습니다. 또는 프롬프트 엔지니어링을 통해 사전 훈련 모델에서 직접 필요한 정보를 추출할 수도 있습니다. 프롬프팅 방법은 인간 실무자에게 매우 직접적이고 이해하기 쉽지만, 일부 학자들은 하위 작업으로 세밀 조정하지 않으면 최고의 성능을 얻을 수 없다고 주장합니다.\n\n# 참고 자료\n\n1. Edunov, S.; Baevski, A.; Auli, M., Pre-trained language model representations for language generation. arXiv preprint arXiv:1903.09722 2019.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. Dong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y.; Gao, J.; Zhou, M.; Hon, H.-W., 통합 언어 모델 사전 훈련을 통한 자연어 이해 및 생성. Advances in Neural Information Processing Systems 2019, 32.\n\n3. Zhang, X.; Li, P.; Li, H., Ambert: 다중 단위 토큰화를 사용한 사전 훈련된 언어 모델. arXiv preprint arXiv:2008.11869 2020.\n\n4. Wu, S.; He, Y. Entity 정보로 언어 모델을 보강하여 관계 분류의 성능 향상, Proceedings of the 28th ACM international conference on information and knowledge management, 2019; pp 2361–2364.\n\n5. Gunel, B.; Du, J.; Conneau, A.; Stoyanov, V., 사전 훈련된 언어 모델 파인튜닝을 위한 지도 대조적 학습. arXiv preprint arXiv:2011.01403 2020.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n6. Qiu, X.; Sun, T.; Xu, Y.; Shao, Y.; Dai, N.; Huang, X., 자연어 처리를 위한 사전 학습 모델: 조사. Science China Technological Sciences 2020, 63 (10), 1872–1897.\n\n7. Esmaeilzadeh, A.; Taghva, K. 신경망 언어 모델 (NNLM) 및 BERT를 활용한 텍스트 분류: 경험적 비교, SAI Intelligent Systems Conference 논문집, Springer: 2021; pp 175–189.\n\n8. Church, K. W., Word2Vec. Natural Language Engineering 2017, 23 (1), 155–162.\n\n9. Rong, X., word2vec 매개변수 학습 해설. arXiv 사전 인쇄 arXiv:1411.2738 2014.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Ma, L.; Zhang, Y. (2015) Using Word2Vec to process big text data. 2015 IEEE International Conference on Big Data (Big Data), IEEE, pp. 2895–2897.\n- Pennington, J.; Socher, R.; Manning, C. D. (2014) Glove: Global vectors for word representation. Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543.\n- McCann, B.; Bradbury, J.; Xiong, C.; Socher, R. Learned in translation: Contextualized word vectors. Advances in neural information processing systems 2017, 30.\n- Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; Zettlemoyer, L. (2018) Deep contextualized word representations. arXiv:1802.05365. [Link](https://ui.adsabs.harvard.edu/abs/2018arXiv180205365P) (accessed February 01, 2018).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n14. Floridi, L.; Chiriatti, M., GPT-3: 그 특성, 범위, 한계 및 결과. Minds and Machines 2020, 30 (4), 681–694.\n\n15. Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K., Bert: 언어 이해를 위한 깊은 양방향 트랜스포머 사전 훈련. arXiv 사전 인쇄 arXiv:1810.04805 2018.\n\n16. Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R. R.; Le, Q. V., Xlnet: 언어 이해를 위한 일반화된 자기 회귀 사전 훈련. 신경 정보 처리 시스템의 진보 2019, 32.\n\n17. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; Polosukhin, I., Attention is all you need. 신경 정보 처리 시스템의 진보 2017, 30.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n18. Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K., BERT: Deep Bidirectional Transformers를 위한 사전 훈련. ArXiv 2019, abs/1810.04805.\n\n19. Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I., 생성 사전 훈련을 통한 언어 이해 개선. 2018.\n\n20. Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; Stoyanov, V., Roberta: 견고하게 최적화된 bert 사전 훈련 접근 방식. arXiv 사전 인쇄 arXiv:1907.11692 2019.\n\n21. Clark, K.; Luong, M.-T.; Le, Q. V.; Manning, C. D., Electra: 텍스트 인코더를 생성자가 아닌 판별자로 사전 훈련하는 방법. arXiv 사전 인쇄 arXiv:2003.10555 2020.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n22. Lewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; Mohamed, A.; Levy, O.; Stoyanov, V.; Zettlemoyer, L. 2019. \"Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.\" arXiv preprint arXiv:1910.13461 \n\n23. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; Liu, P. J. 2019. \"Exploring the limits of transfer learning with a unified text-to-text transformer.\" arXiv preprint arXiv:1910.10683 \n\n24. Song, K.; Tan, X.; Qin, T.; Lu, J.; Liu, T.-Y. 2020. \"Mpnet: Masked and permuted pre-training for language understanding.\" Advances in Neural Information Processing Systems, 33, 16857–16867.\n\n25. Wei, J.; Ren, X.; Li, X.; Huang, W.; Liao, Y.; Wang, Y.; Lin, J.; Jiang, X.; Chen, X.; Liu, Q. 2019. \"Nezha: Neural contextualized representation for Chinese language understanding.\" arXiv preprint arXiv:1909.00204\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n26. Dai, Z.; Yang, Z.; Yang, Y.; Carbonell, J.; Le, Q. V.; Salakhutdinov, R., Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860 2019.\n\n27. Sanh, V.; Debut, L.; Chaumond, J.; Wolf, T., DistilBERT, BERT의 압축 버전: 더 작고, 빠르고, 저렴하고 가벼움. arXiv preprint arXiv:1910.01108 2019.\n\n28. Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; Soricut, R., Albert: 자기지도 언어 표현의 가벼운 BERT. arXiv preprint arXiv:1909.11942 2019.\n\n29. Sun, Z.; Yu, H.; Song, X.; Liu, R.; Yang, Y.; Zhou, D., MobileBERT: 자원 제한 장치를 위한 콤팩트한 범용 BERT. arXiv preprint arXiv:2004.02984 2020.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n30. Jiang, Z.-H.; Yu, W.; Zhou, D.; Chen, Y.; Feng, J.; Yan, S., Convbert: Improving bert with span-based dynamic convolution. Advances in Neural Information Processing Systems 2020, 33, 12837–12848.\n\n31. He, P.; Liu, X.; Gao, J.; Chen, W., Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654 2020.\n\n32. Zaheer, M.; Guruganesh, G.; Dubey, K. A.; Ainslie, J.; Alberti, C.; Ontanon, S.; Pham, P.; Ravula, A.; Wang, Q.; Yang, L., Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems 2020, 33, 17283–17297.\n\n33. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; Zettlemoyer, L. In Deep Contextualized Word Representations, NAACL, 2018.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n34. Howard, J.; Ruder, S. Universal Language Model Fine-tuning을 위한 텍스트 분류(ACL, 2018).\n\n35. Dai, A. M.; Le, Q. V., 준지도학습 순차 학습. 신경 정보 처리 시스템의 발전 2015, 28.\n\n36. Phang, J.; Févry, T.; Bowman, S. R., Sentence encoders on stilts: 중간 레이블 데이터 작업에 대한 보조 훈련. arXiv 사전 인쇄 arXiv:1811.01088 2018.\n\n37. Rebuffi, S.-A.; Bilen, H.; Vedaldi, A., 잔여 어댑터를 사용한 다중 시각 도메인 학습. 신경 정보 처리 시스템의 발전 2017, 30.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n38. 아가자니안, A.; 제틀모이어, L.; 구프타, S., 내재적 차원이 언어 모델 파인튜닝의 효과를 설명합니다. arXiv 사전 인쇄 arXiv:2012.13255 2020.\n\n39. 주, C.; 청, Y.; 간, Z.; 선, S.; 골드스타인, T.; 리우, J., Freelb: 자연어 이해를 위한 향상된 적대적 훈련. arXiv 사전 인쇄 arXiv:1909.11764 2019.\n\n40. 리우, P.; 위안, W.; 부, J.; 장, Z.; 하야시, H.; 노이빅, G., 사전 학습, 프롬프트 및 예측: 자연어 처리에서 프롬프트 방법의 체계적 조사. arXiv 사전 인쇄 arXiv:2107.13586 2021.\n\n41. 양, Y.; 황, P.; 차오, J.; 리, J.; 린, Y.; 동, J. S.; 마, F.; 장, J., 적대적 예제 생성 및 강화를 위한 프롬프트 기반 접근 방법. arXiv 사전 인쇄 arXiv:2203.10714 2022.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n42. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.; Le, Q.; Zhou, D., Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 2022.\n\n43. Liang, S.; Zhao, M.; Schütze, H., Modular and Parameter-Efficient Multimodal Fusion with Prompting. arXiv preprint arXiv:2203.08055 2022.\n\n44. Hanna, M.; Mareček, D. In Analyzing BERT’s Knowledge of Hypernymy via Prompting, Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, 2021; pp 275–282.\n\n45. Abaho, M.; Bollegala, D.; Williamson, P.; Dodd, S., Position-based Prompting for Health Outcome Generation. arXiv preprint arXiv:2204.03489 2022.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n46. Wang, C.; Wang, J.; Qiu, M.; Huang, J.; Gao, M. In TransPrompt: Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021; pp 2792–2802.\n\n47. Lester, B.; Al-Rfou, R.; Constant, N., The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 2021.","ogImage":{"url":"/assets/img/2024-06-22-IntroofGenericNLPmethods_0.png"},"coverImage":"/assets/img/2024-06-22-IntroofGenericNLPmethods_0.png","tag":["Tech"],"readingTime":15}],"page":"25","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"25"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>