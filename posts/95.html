<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/95" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/95" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="MLX로부터 GPT를 처음부터 만들어보기" href="/post/2024-06-19-GPTfromScratchwithMLX"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="MLX로부터 GPT를 처음부터 만들어보기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-GPTfromScratchwithMLX_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="MLX로부터 GPT를 처음부터 만들어보기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">MLX로부터 GPT를 처음부터 만들어보기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">41<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LSTM 구조를 사용하여 설명하는 어텐션" href="/post/2024-06-19-AttentionexplainedusingLSTMarchitecture"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LSTM 구조를 사용하여 설명하는 어텐션" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LSTM 구조를 사용하여 설명하는 어텐션" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">LSTM 구조를 사용하여 설명하는 어텐션</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AI 사용 사례는 근본적으로 다릅니다" href="/post/2024-06-19-AIUseCasesareFundamentallyDifferent"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AI 사용 사례는 근본적으로 다릅니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-AIUseCasesareFundamentallyDifferent_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AI 사용 사례는 근본적으로 다릅니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">AI 사용 사례는 근본적으로 다릅니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="고급 계산을 위한 클러스터 컴퓨터 활용 포괄적 가이드" href="/post/2024-06-19-UtilisingaClusterComputerforAdvancedComputationsAComprehensiveGuide"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="고급 계산을 위한 클러스터 컴퓨터 활용 포괄적 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-UtilisingaClusterComputerforAdvancedComputationsAComprehensiveGuide_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="고급 계산을 위한 클러스터 컴퓨터 활용 포괄적 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">고급 계산을 위한 클러스터 컴퓨터 활용 포괄적 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="신기한 여행 스탠포드의 매직 월드 크리에이터" href="/post/2024-06-19-WonderJourneyStanfordsMagicalWorldCreator"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="신기한 여행 스탠포드의 매직 월드 크리에이터" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-WonderJourneyStanfordsMagicalWorldCreator_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="신기한 여행 스탠포드의 매직 월드 크리에이터" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">신기한 여행 스탠포드의 매직 월드 크리에이터</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="사고, 빠르고 느린, LLMs와 PDDL과 함께" href="/post/2024-06-19-ThinkingFastandSlowwithLLMsandPDDL"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="사고, 빠르고 느린, LLMs와 PDDL과 함께" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-ThinkingFastandSlowwithLLMsandPDDL_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="사고, 빠르고 느린, LLMs와 PDDL과 함께" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">사고, 빠르고 느린, LLMs와 PDDL과 함께</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">15<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label=" 변화가 다가오고 있어요" href="/post/2024-06-19-Changeiscoming"><div class="PostList_thumbnail_wrap__YuxdB"><img alt=" 변화가 다가오고 있어요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Changeiscoming_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt=" 변화가 다가오고 있어요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl"> 변화가 다가오고 있어요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="음성 쓰레기 분류 라즈베리 파이와 티처블 머신" href="/post/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="음성 쓰레기 분류 라즈베리 파이와 티처블 머신" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="음성 쓰레기 분류 라즈베리 파이와 티처블 머신" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">음성 쓰레기 분류 라즈베리 파이와 티처블 머신</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이의 AI 키트는 얼마나 좋은가요" href="/post/2024-06-19-HowgoodisRaspberryPisAIKit"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이의 AI 키트는 얼마나 좋은가요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-HowgoodisRaspberryPisAIKit_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이의 AI 키트는 얼마나 좋은가요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">라즈베리 파이의 AI 키트는 얼마나 좋은가요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="시그널 인텔리전스, 라즈베리 파이" href="/post/2024-06-19-SignalsIntelligenceTheRaspberryPi"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="시그널 인텔리전스, 라즈베리 파이" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-SignalsIntelligenceTheRaspberryPi_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="시그널 인텔리전스, 라즈베리 파이" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">시그널 인텔리전스, 라즈베리 파이</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/81">81</a><a class="link" href="/posts/82">82</a><a class="link" href="/posts/83">83</a><a class="link" href="/posts/84">84</a><a class="link" href="/posts/85">85</a><a class="link" href="/posts/86">86</a><a class="link" href="/posts/87">87</a><a class="link" href="/posts/88">88</a><a class="link" href="/posts/89">89</a><a class="link" href="/posts/90">90</a><a class="link" href="/posts/91">91</a><a class="link" href="/posts/92">92</a><a class="link" href="/posts/93">93</a><a class="link" href="/posts/94">94</a><a class="link posts_-active__YVJEi" href="/posts/95">95</a><a class="link" href="/posts/96">96</a><a class="link" href="/posts/97">97</a><a class="link" href="/posts/98">98</a><a class="link" href="/posts/99">99</a><a class="link" href="/posts/100">100</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"MLX로부터 GPT를 처음부터 만들어보기","description":"","date":"2024-06-19 03:00","slug":"2024-06-19-GPTfromScratchwithMLX","content":"\n\n## MacBook에서 GPT-2 정의 및 훈련하기\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_0.png)\n\n이 게시물의 목표는 MLX, Apple 실리콘을 위한 Apple의 기계 학습 라이브러리를 사용하여 GPT-2를 처음부터 정의하고 훈련하는 과정을 안내하는 것입니다. 토크나이저에서 샘플링까지 모든 과정을 상세히 다루고자 합니다. Karpathy의 훌륭한 GPT 처음부터 튜토리얼 영감을 받아, 우리는 Shakespeare의 작품에 대해 모델을 훈련할 것입니다. 우리는 비어 있는 Python 파일로 시작하여 Shakespeare 스타일 텍스트를 작성할 수 있는 소프트웨어로 끝낼 것입니다. 그리고 이 모든 것을 훨씬 빠르게 가능하게 하는 MLX에서 모두 구축할 것입니다.\n\n본 게시물은 따라하며 체험하는 것이 가장 좋습니다. 코드는 아래 리포지토리에 포함되어 있으며 이를 열어 참조하는 것을 권장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 목차\n\n- 데이터 준비\n- GPT-2 코딩\n- 입력 임베딩\n- 위치 임베딩\n- 셀프 어텐션\n- 키, 쿼리 및 값\n- 멀티헤드 어텐션\n- MLP\n- 블록\n- 레이어 정규화 및 스킵 연결\n- 순방향 패스\n- 샘플링\n- 초기화\n- 훈련 루프\n- 참고 자료\n\n# 데이터 준비\n\nmlx를 설치하고 다음 임포트를 실행하십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport mlx.core as mx\nimport mlx.nn as nn\nimport mlx.optimizers as optim\nimport mlx.utils as utils\nimport numpy as np\nimport math\n```\n\nLLM 훈련의 첫 번째 단계는 큰 텍스트 데이터 코퍼스를 수집한 다음 토큰화하는 것입니다. 토큰화는 텍스트를 정수로 매핑하는 작업으로, LLM에 공급할 수 있습니다. 이 모델의 훈련 코퍼스는 셰익스피어의 작품들을 연결한 것입니다. 이는 대략 100만 글자이며 다음과 같습니다:\n\n```js\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n...\n```\n\n먼저 파일을 하나의 긴 문자열로 읽어 text 변수에 저장합니다. 그런 다음 set() 함수를 사용하여 텍스트에 있는 모든 고유한 문자를 얻어서 우리의 어휘가 됩니다. vocab을 출력하여 우리의 어휘에 있는 모든 문자를 하나의 문자열로 볼 수 있으며, 우리는 총 65개의 문자가 있어서 이것이 우리의 토큰이 될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 단어장 생성하기\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nvocab = sorted(list(set(text)))\nvocab_size = len(vocab)\n\nprint(''.join(vocab))\n# !$\u0026',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nprint(vocab_size)\n# 65\n\n\n생산 모델은 바이트 페어 인코딩과 같은 토큰화 알고리즘을 사용하여 하위 단어 청크의 더 큰 어휘를 생성할 것입니다. 오늘 우리의 초점은 아키텍처에 있기 때문에, 문자 수준의 토큰화를 계속할 것입니다. 다음으로, 단어장을 정수로 매핑하여 토큰 ID로 알려진 것으로 이동할 것입니다. 그런 다음 텍스트를 토큰으로 인코딩하고 문자열로 다시 디코딩할 수 있습니다.\n\n\n# 단어장을 정수로 매핑하기\nitos = {i:c for i,c in enumerate(vocab)} # int to string\nstoi = {c:i for i,c in enumerate(vocab)} # string to int\nencode = lambda x: [stoi[c] for c in x]\ndecode = lambda x: ''.join([itos[i] for i in x])\n\nprint(encode(\"hello world\"))\n# [46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\nprint(decode(encode(\"hello world\")))\n# hello world\n\n\n모든 문자 및 해당 어휘의 인덱스를 반복하여 숫자를 문자에 매핑하는 itos와 문자를 숫자에 매핑하는 stoi 사전을 생성하기 위해 enumerate() 함수를 사용합니다. 그런 다음 이러한 매핑을 사용하여 encode 및 decode 함수를 만듭니다. 이제 전체 텍스트를 인코딩하고 훈련 및 검증 데이터로 나눌 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n데이터 = 인코딩(텍스트)\n분할 = int(0.9 * len(데이터))\n훈련_데이터 = 데이터[:분할]\n검증_데이터 = 데이터[분할:]\n```\n\n현재 훈련 데이터는 토큰들의 매우 긴 문자열입니다. 그러나 이전 토큰들이 주어졌을 때 다음 토큰을 예측하는 모델을 훈련하려고 합니다. 따라서 데이터셋은 입력이 토큰 문자열이고 레이블이 올바른 다음 토큰인 예제로 구성되어야 합니다. 다음 토큰을 예측하는 데 사용되는 최대 토큰 수인 context length라는 모델 매개변수를 정의해야 합니다. 훈련 예제는 우리의 context length의 길이가 될 것입니다.\n\n처음 ctx_len+1 개의 토큰을 살펴봅시다.\n\n```js\nctx_len = 8\nprint(훈련_데이터[:ctx_len + 1])\n# [18, 47, 56, 57, 58,  1, 15, 47, 58]\n# x: [18, 47, 56, 57, 58,  1, 15, 47] | y: 58\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 예제는 인풋이 \"18, 47, 56, 57, 58, 1, 15, 47\"이고, 원하는 아웃풋이 \"58\"인 트레이닝 예제입니다. 이는 8 토큰의 컨텍스트를 가지고 있습니다. 그러나 생성 중에 필요한 7, 6, 5 ... 0개의 토큰만을 가지고 다음 토큰을 예측할 수 있도록 모델을 훈련시키고 싶습니다. 따라서 이 예제에 포함된 8개의 하위 예제를 고려합니다:\n\n```js\nctx_len = 8\nprint(train_data[:ctx_len + 1])\n# [18, 47, 56, 57, 58,  1, 15, 47, 58]\n# 8 sub examples\n# [18] --\u003e 47\n# [18, 47] --\u003e 56\n# [18, 47, 56] --\u003e 57\n# [18, 47, 56, 57] --\u003e 58\n# [18, 47, 56, 57, 58] --\u003e 1\n# [18, 47, 56, 57, 58, 1] --\u003e 15\n# [18, 47, 56, 57, 58, 1, 15] --\u003e 47\n# [18, 47, 56, 57, 58, 1, 15, 47] --\u003e 58\n```\n\n라벨은 간단히 왼쪽으로 이동한 인풋입니다.\n\n```js\nprint(\"inputs: \", train_data[:ctx_len])\nprint(\"labels: \", train_data[1:ctx_len+1]) # labels = inputs indexed 1 higher\n# inputs: [18, 47, 56, 57, 58,  1, 15, 47]\n# labels: [47, 56, 57, 58,  1, 15, 47, 58]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인덱스 0에서 입력은 18이고 라벨은 47입니다. 인덱스 1에서 입력은 인덱스 1을 포함하여 그 이전 모든 것, 즉 [18, 47]이고 라벨은 56입니다. 등등. 이제 라벨이 입력 순서에서 한 단계 상위로 색인화됨을 이해했으므로 데이터셋을 구축할 수 있습니다.\n\n```js\n# 훈련 및 검증 데이터 세트 생성\nctx_len = 8\nX_train = mx.array([train_data[i:i+ctx_len] for i in range(0, len(train_data) - ctx_len, ctx_len)])\ny_train = mx.array([train_data[i+1:i+ctx_len+1] for i in range(0, len(train_data) - ctx_len, ctx_len)])\nX_val = mx.array([val_data[i:i+ctx_len] for i in range(0, len(val_data) - ctx_len, ctx_len)])\ny_val = mx.array([val_data[i+1:i+ctx_len+1] for i in range(0, len(val_data) - ctx_len, ctx_len)])\n```\n\n우리는 데이터를 반복하고 입력(X)으로 ctx_len 크기의 청크를 가져와서 같은 청크를 라벨(y)로 하나 높은 색인에서 가져옵니다. 그런 다음 이 Python 리스트를 mlx 배열 객체로 만듭니다. 모델 내부로 mlx를 사용할 것이므로 입력을 mlx 배열로 만들고 싶습니다.\n\n그리고 한 가지 더. 훈련 중에 모델에 한 번에 하나의 예제만 전달하고 싶지 않습니다. 효율성을 위해 병렬로 여러 예제를 한꺼번에 전달하고 싶습니다. 이 예제 그룹을 우리의 배치라고 하며, 그룹 내의 예제 수가 배치 크기입니다. 따라서 훈련용 배치를 생성하는 함수를 정의합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef get_batches(X, y, b_size, shuffle=True):\n    if shuffle:\n        ix = np.arange(X.shape[0])\n        np.random.shuffle(ix)\n        ix = mx.array(ix)\n        X = X[ix]\n        y = y[ix]\n    for i in range(0, X.shape[0], b_size):\n        input = X[i:i+b_size]\n        label = y[i:i+b_size]\n        yield input, label\n```\n\n만약 shuffle=True라면, 데이터를 임의로 섞은 인덱스로 인덱싱하여 데이터를 섞습니다. 그런 다음 데이터 세트를 반복하고 입력 및 레이블 데이터 세트에서 배치 크기 청크를 반환합니다. 이 청크는 미니 배치로 알려져 있으며 병렬로 처리하는 예제를 쌓은 것입니다. 이러한 미니 배치는 모델 훈련 중에 우리의 입력이 될 것입니다.\n\n다음은 컨텍스트 길이가 8 인 4 개의 예제의 미니 배치 예제입니다.\n\n![2024-06-19-GPTfromScratchwithMLX_1.png](/assets/img/2024-06-19-GPTfromScratchwithMLX_1.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n32 개의 다음 토큰 예측 문제가 포함된 미니배치입니다. 모델은 입력의 각 토큰에 대해 다음 토큰을 예측하고 레이블은 손실을 계산하는 데 사용됩니다. 입력의 각 색인에 대한 다음 토큰이 포함된 것을 주목해주세요.\n\n이 텐서들의 형태가 복잡해질 것을 염두에 두시면 좋겠어요. 지금은 일단, 우리가 모델에 (배치 크기, ctx_len) 모양의 텐서를 입력할 것이라는 것만 기억해주세요.\n\n# 코딩 GPT-2\n\nGPT-2 아키텍처를 살펴보고 구현하려는 것에 대한 개요를 파악해봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_2.png)\n\n이게 혼란스러워 보이더라도 걱정하지 마세요. 우리는 바닥부터 꼭대기로 한 단계씩 구현할 거에요. 먼저 입력 임베딩을 구현하는 것부터 시작해봅시다.\n\n## 입력 임베딩\n\n입력 임베딩 레이어의 목적은 토큰 ID를 벡터로 매핑하는 것입니다. 각 토큰은 모델을 통해 전달될 때 그것에 대한 표현으로 사용될 벡터로 매핑됩니다. 각 토큰에 대한 벡터는 모델을 통해 전달되면서 정보를 축적 및 교환하고, 결국 다음 토큰을 예측하는 데 사용될 것입니다. 이러한 벡터들을 임베딩(embedding)이라고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n토큰 ID를 벡터로 매핑하는 가장 간단한 방법은 조회 테이블을 통해 할 수 있습니다. 각 토큰에 대한 임베딩 벡터가 있는 (vocab_size, n_emb) 크기의 행렬을 만듭니다. 이 행렬을 임베딩 가중치라고 합니다.\n\n![image](/assets/img/2024-06-19-GPTfromScratchwithMLX_3.png)\n\n다이어그램은 크기가 (65, 6)인 임베딩 레이어의 예시를 보여줍니다. 이는 어휘 사전에 65개의 토큰이 있고 각각이 길이가 6인 임베딩 벡터로 표현됨을 의미합니다. 입력된 시퀀스는 임베딩 가중치를 색인하여 각 토큰에 해당하는 벡터를 얻는 데 사용됩니다. 모델에 입력하는 미니배치를 기억하십니까? 원래 미니배치는 크기가 (batch_size, ctx_len)입니다. 임베딩 레이어를 통과한 후 크기는 (batch_size, ctx_len, n_emb)입니다. 각 토큰이 단일 정수가 아니라 길이가 n_emb인 벡터임을 의미합니다.\n\n이제 코드에서 임베딩 레이어를 정의해봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nn_emb = 6 # 파일 맨 위에 이러한 하이퍼파라미터를 추가할 수 있어요\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb)\n```\n\n우리의 구현을 정리하기 위한 클래스를 정의할 거예요. mlx의 기능을 활용하기 위해 nn.Module을 서브클래스화할 거예요. 그럼 init 함수에서는 슈퍼클래스 생성자를 호출하고 wte라고 불리는 토큰 임베딩 레이어를 초기화할 거예요.\n\n## 위치 임베딩\n\n다음은 위치 임베딩이에요. 위치 임베딩의 목적은 시퀀스에서 각 토큰의 위치에 대한 정보를 인코딩하는 거예요. 이걸 우리의 입력 임베딩에 추가해서 각 토큰의 완전한 표현을 얻을 수 있어요. 그 표현에는 시퀀스에서 토큰의 위치에 대한 정보가 담겨있어요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb) # 토큰 임베딩\n        self.wpe = nn.Embedding(ctx_len, n_emb) # 위치 임베딩\n```\n\n위치 임베딩은 토큰 임베딩과 동일한 방식으로 작동합니다. 하지만 각 토큰마다 행이 있는 것 대신, 각 가능한 위치 인덱스마다 행이 있습니다. 이는 임베딩 가중치의 모양이 (ctx_len, n_emb)가 됨을 의미합니다. 이제 GPT 클래스에 __call__ 함수를 구현해보겠습니다. 이 함수에는 모델의 forward pass가 포함될 것입니다.\n\n```python\n# 텐서 모양 주석\ndef __call__(self, x):\n    B, T = x.shape # (B = 배치 크기, T = ctx_len)\n    tok_emb = self.wte(x) # (B, T, n_emb)\n    pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)\n    x = tok_emb + pos_emb # (B, T, n_emb)\n```\n\n먼저, 입력의 차원을 B와 T 변수로 나누어 보다 쉽게 처리합니다. 시퀀스 모델링 맥락에서 B와 T는 일반적으로 \"배치\"와 \"시간\" 차원을 나타내는 약어로 사용됩니다. 이 경우, 시퀀스의 \"시간\" 차원은 컨텍스트 길이입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로 토큰 및 위치 임베딩을 계산합니다. 위치 임베딩의 경우 입력이 mx.arange(T)임에 유의하십시오. 이는 우리가 임베딩하고자 하는 위치인 0부터 T-1까지의 연속 정수 배열을 출력합니다. 임베딩 레이어를 통과한 후에는 각 위치에 대해 n_emb 길이의 벡터를 추출하므로 모양이 (T, n_emb)인 텐서가 생성됩니다. pos_emb가 tok_emb과 형태가 다르더라도 mlx가 브로드캐스트 또는 배치 차원을 통해 pos_emb을 복제하여 요소별 덧셈을 허용하기 때문에 두 값을 더할 수 있습니다. 마지막으로 덧셈을 수행하여 토큰의 새로운 표현을 얻습니다.\n\n## 셀프 어텐션\n\n지금까지 각 토큰의 표현 벡터는 독립적으로 계산되었습니다. 그들은 어떠한 정보를 교환할 기회도 가지지 못했습니다. 이는 주변 맥락에 따라 단어의 의미와 사용이 의존되므로 언어 모델링에서 직관적으로 나쁜 접근입니다. 셀프 어텐션은 이전 토큰으로부터 정보를 현재 토큰으로 통합하는 방법입니다.\n\n먼저, 가장 단순한 접근 방식을 살펴보겠습니다. 만약 각 토큰을 단순히 해당 표현 벡터의 평균으로 표현하고 그 이전의 모든 토큰의 벡터를 더한다면 어떨까요? 이렇게 하면 이전 토큰들로부터 정보를 현재 토큰의 표현에 담을 수 있습니다. 어떻게 보이게 될까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_4.png)\n\n하지만 self-attention은 for-loop를 사용하지 않습니다. 핵심 아이디어는 이전 토큰의 평균을 행렬 곱셈으로 얻을 수 있다는 것입니다!\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_5.png)\n\n입력 시퀀스를 특별한 행렬로 왼쪽부터 곱하면 원하는 결과를 얻을 수 있습니다. 이 행렬을 주목해보면 이는 어텐션 가중치라고 알려져 있습니다. 어텐션 가중치 행렬의 각 행은 주어진 토큰의 표현에 각 다른 토큰이 얼마나 많이 기여하는지를 나타냅니다. 예를 들어, 두 번째 행의 경우 [0.5, 0.5, 0, 0]입니다. 이것은 두 번째 행의 결과가 0.5*토큰1 + 0.5*토큰2 + 0*토큰3 + 0*토큰4, 즉 토큰1과 토큰2의 평균이 됨을 의미합니다. 어텐션 가중치는 하삼각 행렬입니다 (우상단 항목이 0). 이는 미래 토큰이 주어진 토큰의 표현에 포함되지 않도록 보장합니다. 이는 토큰이 이전 토큰과만 통신할 수 있도록 하며, 생성 중에 모델은 이전 토큰에만 액세스할 수 있는 것이 보장됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떻게 주의 집중 가중치 행렬을 생성할 수 있는지 살펴봅시다.\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_6.png)\n\n주의 가중치 행렬을 구성하는 배열을 만들고 오른쪽 상단 항목에 -inf를 넣은 다음 행별 softmax를 수행하면 원하는 주의 가중치를 얻을 수 있습니다. 이 작업을 수행하는 것으로 이 작업이 작동하는 방법을 확인하는 것이 좋습니다. 핵심은 (ctx_len, ctx_len) 크기의 배열을 가지고 각 행에 softmax를 수행하여 합계가 1이 되는 주의 가중치를 얻을 수 있다는 것입니다.\n\n이제 naive 자기 주의 영역을 벗어나 볼 수 있습니다. 이전 토큰을 간단히 평균화하는 대신 이전 토큰에 대한 임의의 가중 합계를 사용합니다. 임의의 행렬의 분포 소프트맥스를 수행할 때 어떻게 되는지 주목하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_7.png)\n\n각 행의 합이 1인 가중치를 계속 얻습니다. 훈련 중에는 왼쪽 행렬의 숫자를 학습하여 각 토큰이 다른 토큰의 표현에 얼마나 많이 참여하는지를 지정할 수 있습니다. 이것이 토큰이 서로에게 \"주의\"를 기울이는 방법입니다. 그러나 여전히 이 왼쪽 행렬이 어디에서 나왔는지 이해하지 못했습니다. 이러한 사전 소프트맥스 주의 가중치는 토큰 자체에서 계산되지만 간접적으로 세 개의 선형 변환을 통해 수행됩니다.\n\n## Keys, Queries, and Values\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_8.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리 시퀀스의 각 토큰은 3개의 새로운 벡터를 생성합니다. 이러한 벡터를 키(key), 쿼리(query) 및 값(value)라고합니다. 한 토큰의 쿼리 벡터와 다른 토큰의 키 벡터의 내적을 사용하여 두 토큰 간의 \"유사성\"을 측정합니다. 우리는 각 토큰과 각 다른 토큰 사이의 쌍별 유사성을 계산하고 싶어합니다. 따라서 쿼리 벡터(4x3)를 키 벡터의 전치(3x4)와 곱하여 원시 어텐션 가중치(4x4)를 얻습니다. 행렬 곱셈이 작동하는 방식으로 인해 원시 어텐션 가중치의 (i,j) 항목은 토큰 i의 쿼리와 토큰 j의 키의 내적 또는 두 가지 사이의 \"유사성\"이됩니다. 따라서 우리는 모든 토큰 간의 상호 작용을 계산했습니다. 그러나 과거 토큰이 미래 토큰과 상호 작용하는 것을 원하지 않기 때문에 상단 우측 항목들에 -inf 마스크를 적용하여 소프트맥스 후에 제로아웃되도록합니다. 그런 다음 행별 소프트맥스를 수행하여 최종 어텐션 가중치를 얻습니다. 이러한 가중치를 입력과 직접 곱하는 대신 값 프로젝션과 곱합니다. 결과적으로 새로운 표현이 생성됩니다.\n\n이제 우리는 주의를 개념적으로 이해했으니, 구현해 봅시다.\n\n```js\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n  \n```\n\n키, 쿼리 및 값 프로젝션 레이어를 정의하여 시작합니다. n_emb에서 진행하는 대신 n_emb에서 head_size로 프로젝션합니다. 아무것도 변경되지 않으며, 주의를 통해 계산된 새로운 표현이 차원 head_size가됨을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n    def __call__(self, x): # shapes commented\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n\n\n앞서로부터 전달받은 값으로 key, query, value를 계산한 뒤, 입력 모양을 미래의 편리함을 위해 변수 B, T, C로 나눕니다.\n\n\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\n\n\n이어서, 어텐션 가중치를 계산합니다. 키 텐서의 마지막 두 차원만 바꿔야 하므로 차원 변환은 마지막 두 차원에 대해서만 이루어집니다. 배치 차원은 여러 학습 예제를 병렬로 전달하기 위한 것뿐입니다. mlx의 전치 함수는 차원의 새로운 순서를 입력으로 받기 때문에, 마지막 두 차원을 전치하기 위해 [0, 2, 1]을 전달합니다. 그리고 여기에 주목할 점: 어텐션 가중치들은 head_size의 제곱근에 역수를 적용합니다. 이는 스케일드 어텐션이라 불리며, 목적은 Q와 K가 단위 분산을 가질 때, attn_weights도 단위 분산을 가지게 하는 것입니다. attn_weights의 분산이 높으면 softmax가 이 작은 값과 큰 값을 0 또는 1로 매핑하여 복잡성이 적은 표현을 얻도록 합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 단계는 미래 토큰에 주의를 기울이지 않는 인과 언어 모델링, 즉 토큰이 미래 토큰에 주의를 기울이지 않도록 마스크를 적용하는 것입니다.\n\n```js\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] \u003c indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\n```\n\n우리는 미리 설정한 ctx_len=4와 같은 다이어그램에서 indices 변수를 [0, 1, 2, 3]으로 설정하기 위해 mx.arange(4)을 사용한다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼 indices[:, None]와 같이 인덱스 값을 가진 열 벡터를 생성할 수 있어요. 마찬가지로 indices[None]을 사용하면 행 벡터를 얻을 수 있어요. 그리고 ` 비교를 수행할 때 mlx는 벡터들을 브로드캐스트합니다. 그 이유는 형태가 맞지 않아 요소별로 비교할 수 없기 때문이에요. 브로드캐스팅은 mlx가 부족한 차원에 따라 벡터를 복제한다는 것을 의미해요. 그 결과, (4, 4) 행렬 간의 요소별 비교가 이루어집니다. 그게 이해가 되죠. 참고로, 텐서 처리할 때 브로드캐스팅 세부 정보에 익숙해지는 것을 권장드립니다. 이 링크를 읽어보세요. 튜토리얼 등장횟수가 많을 거에요.\n\n요소별 비교 후, 다음 텐서가 남아 있어요:\n\n```js\n[[False,  True,  True,  True],\n [False, False,  True,  True],\n [False, False, False,  True],\n [False, False, False, False]]\n```\n\n이 텐서에 -1e9를 곱하면 값을 구할 수 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n[[-0e+00, -1e+09, -1e+09, -1e+09],\n [-0e+00, -0e+00, -1e+09, -1e+09],\n [-0e+00, -0e+00, -0e+00, -1e+09],\n [-0e+00, -0e+00, -0e+00, -0e+00]]\n```\n\n이제 추가적인 마스크가 있습니다. 이 행렬을 어텐션 가중치에 추가하여 모든 오른쪽 상단 항목을 매우 큰 음수로 만들 수 있습니다. 이렇게 하면 소프트맥스 연산 후에 이들이 0이 될 것입니다. 또한, _causal_mask 속성 이름에 \"_\"를 접두사로 추가하여 개인 변수로 표시합니다. 이것은 mlx에게 이것이 매개변수가 아니며 교육 중에 업데이트되지 않아야 함을 나타냅니다.\n\n```js\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] \u003c indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\n        attn_weights = attn_weights + self._causal_mask\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        o = (attn_weights @ V) # (B, T, head_size)\n```\n\n이제 최종 어텐션 가중치를 얻기 위해 행별로 softmax 처리하고 이러한 가중치를 값에 곱하여 출력을 얻을 수 있습니다. softmax에 axis=-1을 전달하여 행이 있는 마지막 차원을 따라 softmax를 수행하려는 것을 지정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최종 단계는 선형 투영 및 드롭아웃을 출력하는 것입니다.\n\n```js\n드롭아웃 = 0.1 # 파일 상단의 하이퍼파라미터와 함께 추가\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] \u003c indices[None] # 브로드캐스팅 트릭\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 투영\n        self.resid_dropout = nn.Dropout(드롭아웃)\n    def __call__(self, x):\n        B, T, C = x.shape # (배치 크기, ctx 길이, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\n        attn_weights = attn_weights + self._causal_mask\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        o = (attn_weights @ V) # (B, T, head_size)\n        o = self.c_proj(self.resid_dropout(o))\n        return o\n```\n\n출력 투영과 잔차 드롭아웃인 c_proj 및 resid_dropout 두 개의 새 계층을 추가했습니다. 출력 투영은 벡터를 원래 차원인 n_emb로 반환하는 역할을 합니다. 드롭아웃은 정규화 및 훈련 안정성을 위해 추가되었으며, 트랜스포머 블록을 쌓으면서 심층 네트워크를 구축하는 것이 중요합니다. 이것으로 하나의 어텐션 헤드를 구현하는 것이 끝났습니다!\n\n## 다중 헤드 어텐션\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하나의 주의 헤드만 있는 LLM보다는 여러 개의 주의 헤드를 병렬로 사용하고 그 출력을 연결하여 최종 표현을 만들곤 합니다. 예를 들어, 하나의 head_size=64를 가진 attention head가 있다고 가정해봅시다. 각 토큰에 대해 생성된 벡터는 64 차원입니다. 우리는 head_size=16인 4개의 병렬 attention head로 동일한 결과를 얻을 수 있습니다. 이들의 출력을 연결하여 16x4 = 64 차원의 출력을 생성할 수 있습니다. Multi-head attention은 모델이 더 복잡한 표현을 학습할 수 있도록 합니다. 각 head가 다른 projection 및 attention 가중치를 학습하기 때문입니다.\n\n```js\nn_heads = 4\nclass MultiHeadAttention(nn.Module): # 단순한 구현\n    def __init__(self):\n        super().__init__()\n        self.heads = [Attention(head_size // n_heads) for _ in range(n_heads)]\n    def __call__(self, x):\n        return mx.concatenate([head(x) for head in self.heads], axis=-1)\n```\n\n간단한 구현은 n_heads의 attention head 목록을 생성하고, 각각의 크기를 최종 head 크기로 나눈 것입니다. 그리고 각 헤드의 출력을 마지막 축을 기준으로 연결하는 것입니다. 그러나 이 구현은 비효율적이며 텐서의 속도를 활용하지 못합니다. 텐서의 성능을 활용한 multi-head attention을 구현해 봅시다.\n\n```js\nhead_size = 64 # 파일 상단에 넣기\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] \u003c indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 projection\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우선, 단일 헤드 어텐션 구현부터 시작해보겠습니다. __init__() 함수는 변경되지 않았어요. forward pass는 키(key), 쿼리(query), 값(value) 프로젝션을 생성하는 것으로 일반적으로 시작합니다.\n\n```js\nhead_size = 64 # 파일 맨 위에 배치\nn_heads = 8 # 파일 맨 위에 배치\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] \u003c indices[None] # 브로드캐스팅 트릭\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 프로젝션\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (배치 크기, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        mha_shape = (B, T, n_heads, head_size//n_heads)\n        K = mx.as_strided(K, (mha_shape)) # (B, T, n_heads, head_size//n_heads)\n        Q = mx.as_strided(Q, (mha_shape)) # (B, T, n_heads, head_size//n_heads)\n        V = mx.as_strided(V, (mha_shape)) # (B, T, n_heads, head_size//n_heads)\n```\n\n다음으로 수행해야 할 일은 헤드 수를 나타내는 새로운 차원을 추가하는 것이에요. 기존의 부자연스러운 구현에서는 각각 고유한 키, 쿼리 및 값 텐서를 가진 별도의 어텐션 객체를 사용했었지만, 이제 이 모든 요소를 하나의 텐서에 모두 가지고 있기 때문에 헤드를 위한 차원이 필요합니다. 우리가 원하는 새로운 모양을 mha_shape에 정의합니다. 그런 다음 각 텐서를 헤드 차원을 가지도록 재구성하기 위해 mx.as_strided()를 사용합니다. 이 함수는 파이토치의 view와 동등하며 mlx에게 이 배열을 다른 모양으로 다루도록 지시합니다. 그러나 아직 문제가 있어요. 이전과 같이 Q @ K_t(K의 마지막 2 차원을 전치한 K_t)를 곱하여 어텐션 가중치를 계산하려고 하면 다음과 같은 모양을 곱하게 됩니다.\n\n```js\n(B, T, n_heads, head_size//n_heads) @ (B, T, head_size//n_heads, n_heads)\n결과 모양: (B, T, n_heads, n_heads)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 (B, T, n_heads, n_heads) 모양의 텐서가 생성됩니다. 이는 올바른 결과가 아닙니다. 한 개의 헤드에서 우리의 어텐션 가중치는 (B, T, T) 모양이어야 합니다. 각 토큰 쌍 간의 상호 작용을 제공하기 때문에 이는 의미가 있습니다. 따라서 이제 우리의 모양은 똑같아야 하지만 헤드 차원이 추가되어야 합니다: (B, n_heads, T, T). 이를 위해 키, 쿼리 및 값의 차원을 변환하고, n_heads 차원을 2가 아닌 1로 만든 다음 재구성하는 방식으로 이를 달성합니다.\n\n```js\nhead_size = 64 # 파일 상단에 배치\nn_heads = 8 # 파일 상단에 배치\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] \u003c indices[None] # 브로드캐스팅 트릭\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 프로젝션\n        self.attn_dropout = nn.Dropout(dropout)\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (배치 크기, 문맥 길이, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        mha_shape = (B, T, n_heads, head_size//n_heads)\n        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1]) # (B, n_heads, T, T)\n        attn_weights = attn_weights + self._causal_mask[:T, :T]\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        attn_weights = self.attn_dropout(attn_weights)\n        o = (attn_weights @ V) # (B, n_heads, T, head_size//n_heads)\n        \n```\n\n이제 올바른 어텐션 가중치를 계산할 수 있습니다. 각 개별 어텐션 헤드의 크기로 어텐션 가중치를 조정합니다. 연결 이후의 크기인 head_size가 아닌 각 개별 어텐션 헤드의 크기로 어텐션 가중치를 조정합니다. 또한 어텐션 가중치에 드롭아웃을 적용합니다.\n\n마지막으로, 연결을 수행하고 출력 프로젝션 및 드롭아웃을 적용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nhead_size = 64 # 파일 상단에 넣어 둬요\nn_heads = 8 # 파일 상단에 넣어 둬요\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] \u003c indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 프로젝션\n        self.attn_dropout = nn.Dropout(dropout)\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (배치 크기, ctx 길이, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        mha_shape = (B, T, n_heads, head_size//n_heads)\n        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1]) # (B, n_heads, T, T)\n        attn_weights = attn_weights + self._causal_mask[:T, :T]\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        attn_weights = self.attn_dropout(attn_weights)\n        o = (attn_weights @ V) # (B, n_heads, T, head_size//n_heads)\n        o = o.transpose([0, 2, 1, 3]).reshape((B, T, head_size)) # 헤드 연결\n        o = self.c_proj(self.resid_dropout(o))\n        return o\n```\n\n모든 것을 하나의 텐서로 가지고 있기 때문에 형태 조작을 통해 연결을 수행할 수 있어요. 먼저, `transpose` 함수를 사용하여 `n_heads`를 두 번째로 마지막 차원으로 이동합니다. 그런 다음, 앞서 수행한 헤드 분할을 되돌리기 위해 원래 크기로 다시 형태를 변환합니다. 이는 각 헤드에서 최종 벡터를 연결하는 것과 동일합니다. 그리고 이게 멀티 헤드 어텐션에 대한 모든 것이에요! 가장 집중력이 필요한 구현 부분을 처리했어요.\n\n# MLP\n\n아키텍처의 다음 부분은 멀티레이어 퍼셉트론 또는 MLP입니다. 이는 2개의 쌓인 선형 레이어를 의미합니다. 여기에 말할 것은 많지 않아요, 이것은 표준 신경망입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.c_fc = nn.Linear(n_emb, 4 * n_emb)\n        self.gelu = nn.GELU()\n        self.c_proj = nn.Linear(4 * n_emb, n_emb)\n        self.dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        x = self.gelu(self.c_fc(x))\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n```\n\n입력을 받아 c_fc를 사용하여 고차원으로 프로젝션합니다. 그런 다음 gelu 비선형성을 적용하고 c_proj를 사용하여 임베딩 차원으로 다시 프로젝션합니다. 마지막으로 드롭아웃을 적용하고 반환합니다. MLP의 목적은 주의를 통해 벡터가 통신한 후 일부 계산을 허용하는 것입니다. 이러한 통신 레이어(주의) 및 계산 레이어(mlp)를 블록에 쌓겠습니다.\n\n# 블록\n\nGPT 블록은 주의가 뒤따르는 MLP로 구성됩니다. 이러한 블록은 구조를 깊게 만들기 위해 반복됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nclass Block(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mlp = MLP()\n        self.mha = MultiHeadAttention()\n    \n    def __call__(self, x):\n        x = self.mha(x)\n        x = self.mlp(x)\n        return x\n```\n\n이제 훈련 안정성을 향상시키기 위해 두 가지 기능을 추가해야 합니다. 아키텍처 다이어그램을 다시 살펴보겠습니다.\n\n## 레이어 정규화 및 스킵 연결\n\n![GPTfromScratchwithMLX_10](/assets/img/2024-06-19-GPTfromScratchwithMLX_10.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아직도 빨간색으로 표시된 구성 요소를 구현해야 합니다. 화살표는 skip 연결을 나타냅니다. 입력이 직접 변환되는 대신, 어텐션 및 MLP 레이어의 효과는 가산적입니다. 이들 결과는 입력에 직접적으로 대체하는 대신 추가됩니다. 이는 깊은 신경망의 훈련 안정성에 도움이 됩니다. 왜냐하면 역전파에서 덧셈 연산의 피연산자들은 합과 동일한 기울기를 받게 됩니다. 그러므로 기울기가 자유롭게 역방향으로 흐를 수 있어서 깊은 신경망을 괴롭히는 사라지거나 폭주하는 기울기와 같은 문제를 방지할 수 있습니다. 또한 레이어 정규화는 활성화 함수들이 정규 분포를 보이도록 하여 훈련 안정성을 돕습니다. 아래는 최종 구현입니다.\n\n```js\nclass Block(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mlp = MLP()\n        self.mha = MultiHeadAttention()\n        self.ln_1 = nn.LayerNorm(dims=n_emb)\n        self.ln_2 = nn.LayerNorm(dims=n_emb)\n    def __call__(self, x):\n        x = x + self.mha(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n```\n\n레이어 정규화는 멀티헤드 어텐션 및 MLP 이전에 적용됩니다. skip 연결은 x = x + ...와 같이 추가를 의미합니다.\n\n# Forward Pass\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n블록을 정의했으니, GPT-2의 전방 향 과정을 완료할 수 있습니다.\n\n```python\nn_layers = 3 # 파일 맨 위에 배치\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb) # 토큰 임베딩\n        self.wpe = nn.Embedding(ctx_len, n_emb) # 위치 임베딩\n        self.blocks = nn.Sequential(\n            *[Block() for _ in range(n_layers)],\n        ) # 트랜스포머 블록들\n        self.ln_f = nn.LayerNorm(dims=n_emb) # 최종 레이어 정규화\n        self.lm_head = nn.Linear(n_emb, vocab_size) # 출력 프로젝션\n    # 텐서 모양 주석\n    def __call__(self, x):\n        B, T = x.shape # (B = 배치 크기, T = ctx_len)\n        tok_emb = self.wte(x) # (B, T, n_emb)\n        pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)\n        x = tok_emb + pos_emb # (B, T, n_emb)\n        x = self.blocks(x) # (B, T, n_emb)\n        x = self.ln_f(x) # (B, T, b_emb)\n        logits = self.lm_head(x) # (B, T, vocab_size)\n        return logits\n```\n\nnn.Sequential을 사용해 블록을 담는 컨테이너를 만들어서 입력을 순차적으로 전달할 수 있습니다. 그런 다음 self.blocks(x)를 사용하여 모든 블록을 적용할 수 있습니다. 마지막으로 레이어 정규화를 적용하고 lm_head를 적용합니다. lm_head 또는 언어 모델링 헤드는 임베딩 차원에서 어휘 크기로 매핑하는 단순한 선형 레이어입니다. 모델은 어휘 내 각 단어에 대한 값이 포함된 벡터를 출력하며, 이를 로짓이라고 합니다. 로짓에 소프트맥스를 적용하여 어휘 전체에 대한 확률 분포를 얻을 수 있으며, 다음 토큰을 샘플링하거나 훈련 중 손실을 계산하는 데 사용할 수 있습니다. 학습을 시작하기 전에 구현해야 할 두 가지 사항만 더 남았습니다.\n\n# 샘플링\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n훈련이 완료된 후 모델에서 한 번 sampling하기 위해 generate 함수를 작성해야 합니다. 아이디어는 우리가 선택한 일련의 시퀀스로 시작하고, 그 다음 토큰을 예측하여 이를 시퀀스에 추가하는 것입니다. 그런 다음 새로운 시퀀스를 입력하고 다시 다음 토큰을 예측합니다. 이를 멈출 때까지 반복합니다.\n\n```js\n# GPT 클래스의 메서드\ndef generate(self, max_new_tokens):\n  ctx = mx.zeros((1, 1), dtype=mx.int32)\n```\n\n우리는 모델에 단일 토큰 'zero'로 프롬프트를 제공합니다. Zero는 새 줄 문자이므로 모델이 얼마나 셰익스피어와 유사한지 확인하고 싶으므로 세대를 시작하는 자연스러운 장소입니다. 참고로, (1, 1) 형태로 초기화하여 시퀀스 길이가 하나인 단일 배치를 시뮬레이션합니다.\n\n```js\n# GPT 클래스의 메서드\ndef generate(self, max_new_tokens):\n  ctx = mx.zeros((1, 1), dtype=mx.int32)\n  for _ in range(max_new_tokens):\n    logits = self(ctx[:, -ctx_len:]) # 마지막 ctx_len 문자열 전달\n    logits = logits[:, -1, :] # 다음 토큰에 대한 로짓 얻기\n    next_tok = mx.random.categorical(logits, num_samples=1)\n    ctx = mx.concatenate((ctx, next_tok), axis=1)\nreturn ctx\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 토큰에 대한 로짓을 얻으려면 마지막 ctx_len 문자열을 모델에 전달합니다. 그러나 모델 출력은 (B, T, vocab_size) 모양입니다. 왜냐하면 입력의 각 토큰에 대한 다음 토큰의 로짓을 예측하기 때문입니다. 학습 중에는 이를 전부 사용하지만 이제는 새 토큰을 샘플링하기 위해 마지막 토큰의 로짓만 원합니다. 이를 위해 로짓을 인덱싱하여 순서 차원인 첫 번째 차원에서 마지막 요소를 얻습니다. 그런 다음 mx.random.categorical() 함수를 사용하여 다음 토큰을 샘플합니다. 이 함수는 로짓을 softmax를 통해 확률 분포로 변환하고 확률에 따라 토큰을 무작위로 샘플링합니다. 마지막으로 새 토큰을 문맥에 연결하고 max_new_tokens 횟수만큼 프로세스를 반복합니다.\n\n# 초기화\n\n마지막으로 중요한 훈련 다이내믹스를 위해 가중치 초기화를 처리해야 합니다.\n\n```js\n# GPT의 방법\ndef _init_parameters(self):\n    normal_init = nn.init.normal(mean=0.0, std=0.02)\n    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저, 두 가지 서로 다른 nn.init.normal 함수를 정의합니다. 첫 번째는 모든 선형 및 임베딩 레이어를 초기화하는 함수입니다. 두 번째는 특히 잔여 투영인 선형 레이어를 초기화하는 함수이며, 이는 다중 헤드 어텐션과 MLP 내부의 마지막 선형 레이어를 말합니다. 이 특별한 초기화의 이유는 GPT-2 논문에 따르면 모델 깊이가 증가함에 따라 잔여 경로를 따라 누적을 확인하기 때문입니다 [2].\n\nmlx에서는 mx.update() 함수를 사용하여 모델의 매개변수를 변경할 수 있습니다. 문서를 확인해보면, 새로운 모델 매개변수의 완전한 또는 부분적인 사전을 예상합니다. 이 사전이 어떻게 구성되는지는 GPT 클래스 내에서 self.parameters()을 출력하여 확인할 수 있습니다.\n\n```js\n{'wte': {'weight': array([[-0.025084, -0.0197523, -0.0341617, ..., -0.0979123, -0.0830218, -0.0784692],\n       [-0.00777913, -0.117002, -0.0310708, ..., 0.0128591, 0.122941, 0.000414443],\n       [0.0240044, -0.0859084, 0.0253116, ..., 0.108967, 0.0767123, 0.0221565],\n       ...,\n       [0.050729, -0.04578, 0.0685943, ..., -0.0496998, -0.00350879, -0.00631825],\n       [0.00518804, 0.0499818, 0.0330045, ..., 0.0300661, 0.0431054, 0.000958906],\n       [-0.0323007, 0.0132046, 0.0208218, ..., -0.0785159, 0.00436121, -0.00726994]], dtype=float32)}, 'wpe': {'weight': array([[0.000797923, -0.0396898, -0.029047, ..., -0.0132273, 0.00684483, -0.0067624],\n       [-0.0247021, -0.0274349, 0.0310587, ..., -0.100099, 0.0301566, -0.0178732],\n       [0.0929172, -0.0468649, 0.0101506, ..., -0.0341086, -0.0516283, 0.0447596],\n       ...,\n       [-0.0508172, 0.0892201, -0.00183612, ..., -0.00341944, 0.023437, 0.0296461],\n       [0.0105829, 0.0688093, 0.146744, ..., -0.0836337, 0.0206679, 0.0184166],\n       [-0.00578717, -0.0606196, -0.0917056, ..., -0.0641549, -0.0490424, 0.0998114]], dtype=float32)}, 'blocks': {'layers': [{'mlp': {'c_fc': {'weight': array([[0.0169199, 0.00264431, 0.0316978, ..., -0.0596867, -0.0153549, 0.0176386],\n       ...\n```\n\n모든 모델 가중치를 mx.array로 포함하는 중첩된 사전입니다. 따라서 모델의 매개변수를 초기화하려면 새 매개변수로 이와 같은 사전을 구성하고 self.update()에 전달해야 합니다. 이를 위해 다음과 같이 수행할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# GPT의 방법\ndef _init_parameters(self):\n    normal_init = nn.init.normal(mean=0.0, std=0.02)\n    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))\n    new_params = []\n    for name, module in self.named_modules():\n        if isinstance(module, nn.layers.linear.Linear):\n            new_params.append((name + '.weight', normal_init(module.weight)))\n        elif isinstance(module, nn.layers.embedding.Embedding):\n            new_params.append((name + '.weight', normal_init(module.weight))\n```\n\nnew_params 라는 튜플 목록을 유지합니다. 이 목록에는 (parameter_name, new_value)의 튜플이 포함됩니다. 다음으로 self.named_modules()를 사용하여 model의 각 nn.Module 객체를 반복하며 (name, module) 튜플을 반환합니다. 루프 내에서 모듈 이름을 인쇄하면 다음과 같이 보입니다.\n\n```js\nlm_head\nblocks\nblocks.layers.4\nblocks.layers.3\nblocks.layers.3.ln_2\nblocks.layers.3.ln_1\nblocks.layers.3.mha\nblocks.layers.3.mha.resid_dropout\nblocks.layers.3.mha.c_proj\nblocks.layers.3.mha.attn_dropout\nblocks.layers.3.mha.c_attn\n...\nblocks.layers.0.mlp.dropout\nblocks.layers.0.mlp.c_proj\nblocks.layers.0.mlp.gelu\nblocks.layers.0.mlp.c_fc\nwpe\nwte\n```\n\nisinstance() 함수를 사용하여 linear 및 embedding 레이어를 찾은 다음 목록에 추가합니다. 예를 들어, \"blocks.layers.0.mlp.c_fc\"에 도달하는 경우, 이는 MLP의 첫 번째 linear 레이어입니다. 이 경우 첫 번째 if 문이 트리거되어 (\"block.layers.0.mlp.c_fc.weight\", [`초기화된 weight 값 여기에 추가`])의 튜플이 목록에 추가됩니다. 우리는 특정한 이 방법으로 가중치를 초기화하고자 하기 때문에 이름에 \".weight\"를 추가해야 합니다. 이제 잔류 투영 초기화를 처리해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\n# GPT의 메서드\ndef _init_parameters(self):\n    normal_init = nn.init.normal(mean=0.0, std=0.02)\n    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))\n    new_params = []\n    for name, module in self.named_modules():\n        if isinstance(module, nn.layers.linear.Linear):\n            if 'c_proj' in name: # 잔차 투영\n                new_params.append((name + '.weight', residual_init(module.weight)))\n            else:\n                new_params.append((name + '.weight', normal_init(module.weight)))\n            if hasattr(module, 'bias'):\n                new_params.append((name + '.bias', mx.zeros(module.bias.shape)))\n        elif isinstance(module, nn.layers.embedding.Embedding):\n            new_params.append((name + '.weight', normal_init(module.weight)))\n    self = self.update(utils.tree_unflatten(new_params))\n```\n\n선형 레이어인지 확인한 후 \"c_proj\"가 이름에 있는지 확인하고, 잔차 투영이라고 명명한 대로 특별한 초기화를 적용할 수 있습니다. 마지막으로 편향을 0으로 초기화해야 합니다.\n\n선형 브랜치 아래에 다른 if 문을 추가하여 nn.Module 객체가 편향 특성을 가지고 있는지 확인합니다. 그런 경우 해당 값을 0으로 초기화된 목록에 추가합니다. 마지막으로 튜플 목록을 중첩된 딕셔너리로 변환해야 합니다. 다행히 mlx에는 매개변수 딕셔너리를 처리하는 기능이 구현되어 있으며,이 목록을 중첩 된 매개변수 딕셔너리로 변환하기 위해 util.tree_unflatten() 함수를 사용할 수 있습니다. 이를 매개변수를 초기화하기 위해 update 메서드에 전달합니다. 이제 생성자에서 _init_parameters()를 호출할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb)  # 토큰 임베딩\n        self.wpe = nn.Embedding(ctx_len, n_emb)  # 위치 임베딩\n        self.blocks = nn.Sequential(\n            *[Block() for _ in range(n_layers)],\n        )  # 트랜스포머 블록들\n        self.ln_f = nn.LayerNorm(dims=n_emb)  # 최종 레이어 정규화\n        self.lm_head = nn.Linear(n_emb, vocab_size)  # 출력 프로젝션\n        self._init_parameters()  # \u003c-- 파라미터 초기화\n        # 초기화 시 전체 파라미터 수 출력\n        total_params = sum([p.size for n, p in utils.tree_flatten(self.parameters())])\n        print(f\"총 파라미터 수: {(total_params / 1e6):.3f}M\")\n    \n    # 텐서 모양 주석\n    def __call__(self, x):\n        B, T = x.shape  # (B = 배치 크기, T = ctx_len)\n        tok_emb = self.wte(x)  # (B, T, n_emb)\n        pos_emb = self.wpe(mx.arange(T))  # (T, n_emb)\n        x = tok_emb + pos_emb  # (B, T, n_emb)\n        x = self.blocks(x)  # (B, T, n_emb)\n        x = self.ln_f(x)  # (B, T, b_emb)\n        logits = self.lm_head(x)  # (B, T, vocab_size)\n        return logits\n    \n    def generate(self, max_new_tokens):\n        ctx = mx.zeros((1, 1), dtype=mx.int32)\n        for _ in range(max_new_tokens):\n            logits = self(ctx[:, -ctx_len:])\n            logits = logits[:, -1, :]\n            next_tok = mx.random.categorical(logits, num_samples=1)\n            ctx = mx.concatenate((ctx, next_tok), axis=1)\n        return ctx\n    \n    def _init_parameters(self):\n        normal_init = nn.init.normal(mean=0.0, std=0.02)\n        residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))\n        new_params = []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.layers.linear.Linear):\n                if 'c_proj' in name:\n                    new_params.append((name + '.weight', residual_init(module.weight)))\n                else:\n                    new_params.append((name + '.weight', normal_init(module.weight)))\n                if 'bias' in module:\n                    new_params.append((name + '.bias', mx.zeros(module.bias.shape)))\n            elif isinstance(module, nn.layers.embedding.Embedding):\n                new_params.append((name + '.weight', normal_init(module.weight))\n        self = self.update(utils.tree_unflatten(new_params))\n```\n\n생성자에 총 파라미터 수를 출력하는 코드를 추가했습니다. 마지막으로 훈련 루프를 구축할 준비가 되었습니다.\n\n# 훈련 루프\n\n모델을 훈련하기 위해서는 손실 함수가 필요합니다. 다음 토큰을 예측하므로 교차 엔트로피 손실을 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef loss_fn(model, x, y):\n    logits = model(x)\n    B, T, C = logits.shape # (batch_size, seq_len, vocab_size)\n    logits = logits.reshape(B*T, C)\n    y = y.reshape(B*T)\n    loss = nn.losses.cross_entropy(logits, y, reduction='mean')\n    return loss\n```\n\n먼저, 모델에서 로짓을 얻습니다. 그런 다음 로짓을 vocab_size 길이의 배열 목록으로 재구성합니다. 또한 정확한 토큰 ID 인 y를 동일한 길이로 재구성합니다. 그런 다음 내장된 교차 엔트로피 손실 함수를 사용하여 각 예제의 손실을 계산한 다음 이를 평균 내어 단일 값으로 얻습니다.\n\n```js\nmodel = GPT()\nmx.eval(model.parameters()) # 모델 파라미터 생성 (mlx는 게으른 평가)\nloss_and_grad = nn.value_and_grad(model, loss_fn)\nlr = 0.1\noptimizer = optim.AdamW(learning_rate=lr)\n```\n\n다음으로, 모델을 인스턴스화합니다. 그러나 mlx는 게으르게 평가되기 때문에 파라미터가 할당되고 생성되지 않습니다. 파라미터에 mx.eval을 호출하여 생성되도록 보장해야 합니다. 그런 다음 nn.value_and_grad()를 사용하여 손실 및 모델 파라미터의 그래디언트를 반환하는 함수를 얻을 수 있습니다. 이것이 우리가 최적화하는 데 필요한 모든 것입니다. 마지막으로 AdamW 옵티마이저를 초기화합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nnn.value_and_grad()에 대한 간단한 설명입니다. PyTorch를 사용해 보신 분이라면 loss.backward()를 사용할 것을 기대할 수 있습니다. 이 명령은 계산 그래프를 통과하며 모델 내 각 텐서의 .grad 속성을 업데이트합니다. 그러나 mlx의 자동 미분은 계산 그래프가 아닌 함수에 적용됩니다 [3]. 따라서 mlx에는 nn.value_and_grad()와 같이 함수를 입력받아 기울기 함수를 반환하는 내장 함수가 있습니다.\n\n이제 학습 루프를 정의해 보겠습니다.\n\n```js\nnum_epochs=20\nbatch_size=32\nfor epoch in range(num_epochs):\n    model.train(True)\n    running_loss = 0\n    batch_cnt = 0\n    for input, label in get_batches(X_train, y_train, batch_size):\n        batch_cnt += 1\n        loss, grads = loss_and_grad(model, input, label)\n        optimizer.update(model, grads)\n        running_loss += loss.item()\n        # 새로운 매개변수 및 옵티마이저 상태 계산\n        mx.eval(model.parameters(), optimizer.state)\n    avg_train_loss = running_loss / batch_cnt\n    model.train(False) # 평가 모드로 설정\n    running_loss = 0\n    batch_cnt = 0\n    for input, label in get_batches(X_val, y_val, batch_size):\n        batch_cnt += 1\n        loss = loss_fn(model, input, label)\n        running_loss += loss.item()\n    avg_val_loss = running_loss / batch_cnt\n    print(f\"Epoch {epoch:2} | train = {avg_train_loss:.4f} | val = {avg_val_loss:.4f}\")\n```\n\n외부 루프는 에포크를 거칩니다. 먼저 일부 모듈이 드롭아웃과 같이 학습 및 테스트 중에 다른 동작을 하는 경우가 있으므로 모델을 학습 모드로 설정합니다. 그런 다음 이전에 사용한 get_batches 함수를 사용하여 학습 데이터 배치를 반복합니다. 배치 단위로 손실과 기울기를 얻습니다. 그런 다음 모델과 기울기를 옵티마이저에 전달하여 모델 매개변수를 업데이트합니다. 마지막으로 매개변수 및 옵티마이저 상태가 업데이트되도록 mx.eval을 호출합니다(mx는 지연 평가를 수행하는 것을 기억하세요). 그런 다음 데이터의 평균 학습 손실을 계산하여 나중에 인쇄합니다. 이는 학습 데이터 한 번 통과입니다. 비슷하게 검증 손실을 계산하고 나서 에포크에서 평균 학습 및 검증 손실을 인쇄합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\ncompletion = decode(model.generate(1000)[0].tolist())\nprint(completion)\nwith open('completions.txt', 'w') as f:\n    f.write(completion)\n\n\n마지막으로, 모델에서 생성하는 코드를 추가합니다. 생성 결과는 여전히 (B, T) 형태이므로 0에서 색인화하여 1차원으로 만든 다음 mlx 배열을 Python 리스트로 변환해야 합니다. 그런 다음 앞서 설명한 decode 함수에 전달하고 파일에 쓸 수 있습니다.\n\n다음은 학습에 사용할 매개변수입니다 (이를 변경해보실 수 있습니다):\n\n\nctx_len = 128\nn_emb = 128\ndropout = 0.1\nhead_size = 128\nn_heads = 4 \nn_layers = 3 \nnum_epochs = 20\nbatch_size = 64\nlr = 1e-3\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 파일을 실행하여 훈련을 시작할 수 있습니다. 위의 설정으로 훈련을 진행하면 m2 맥북에서 약 10분이 걸렸어요. 지난 에포크에서 다음과 같은 훈련 손실을 얻었어요.\n\n```js\n에포크 19 | 훈련 = 1.6961 | 검증 = 1.8143\n```\n\n일부 출력을 살펴보겠습니다.\n\n```js\nGLOUCESTER:\nBut accomes mo move it.\n\nKING EDWARD:\nWhere our that proclaim that I curse, or I sprithe.\n\nCORIOLANUS:\nNot want:\nHis bops to thy father\nAt with hath folk; by son and fproathead:\nThe good nor may prosperson like it not,\nWhat, the beggares\nMore hath, when that made a,\nYour vainst Citizen:\nLet here are go in queen me and knife\nTo my deserved me you promise: not a fettimes,\nThat one the will not.\n\nCORIOLANUS:\nAnd been of queens,\nThou to do we best!\n\nJULIET:\nNot, brother recourable this doth our accuse\nInto fight!\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아주 작은 모델로 10분 동안의 훈련만으로 이 정도면 꽤 좋지 않나요? 문자를 예측하는데 셰익스피어 형식 같네요, 비록 무의미하긴 하지만요. 우리 모델과 실제 GPT-2의 유일한 차이는 이제 규모 뿐이에요! 이제 실험해보고 싶네요 — 다양한 설정을 시도해보거나 아키텍처를 잠시 건드려서 얼마나 낮은 손실을 달성할 수 있는지 확인해보세요.\n\n# 참고 문헌\n\n[1] Karpathy A (2015). Tiny Shakespeare [데이터 세트]. https://github.com/karpathy/char-rnn (MIT 라이선스)\n\n[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, 언어 모델은 비지도 멀티태스크 학습자입니다 (2019), OpenAI\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[3] Automatic Differentiation — mlx docs","ogImage":{"url":"/assets/img/2024-06-19-GPTfromScratchwithMLX_0.png"},"coverImage":"/assets/img/2024-06-19-GPTfromScratchwithMLX_0.png","tag":["Tech"],"readingTime":41},{"title":"LSTM 구조를 사용하여 설명하는 어텐션","description":"","date":"2024-06-19 02:58","slug":"2024-06-19-AttentionexplainedusingLSTMarchitecture","content":"\n\n![이미지](/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_0.png)\n\nNLP 엔지니어로서 자주 들어본 말 중 하나가 'Attention(주의)!‘입니다. 트랜스포머(Transformer)와 GPT에 대해 배우기 시작한 사람들에게는 혼란스러울 수 있습니다. 그러나 기계가 보다 긴 시퀀스에서도 맥락 정보를 유지하는 방법을 알아야 합니다. \n\n이 블로그에서는 먼저 LSTM이 무엇인지, 그 단점들은 무엇이었는지, 그리고 어떻게 attention 메커니즘이 이를 극복하는 데 도움이 되었는지 알아보겠습니다.\n\n# Attention이 필요한 이유?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n신경망 기술인 트랜스포머가 등장하기 전에 사용되던 기본 빌딩 블록은 인코더-디코더 LSTM 아키텍처였어요.\n\nLSTM 아키텍처는 주로 3개의 게이트로 구성돼 있어요. 각 게이트는 셀 상태로의 정보 흐름을 제어하는 역할을 합니다. 각 게이트와 그 기능에 대한 간단한 개요는 다음과 같아요:\n\n- 입력 게이트: 현재 입력에서 얼마나 많은 새로운 정보가 셀 상태에 추가돼야 하는지를 제어합니다. 현재 입력과 이전 숨겨진 상태를 가져와서 시그모이드 활성화 함수를 통과시켜 0과 1 사이의 값을 생성하고, 이 값을 현재 입력과 이전 숨겨진 상태를 통과시켜 생성된 후보 셀 상태에 곱해줍니다(이 값은 tanh 활성화 함수를 거칩니다).\n- 잊기 게이트: 이전 셀 상태 중 어느 부분을 유지하거나 잊을지를 결정합니다. 이전 숨겨진 상태와 현재 입력을 가져와서 시그모이드 활성화 함수를 통과시킵니다. 0과 1 사이의 결과값을 얻어 이전 셀 상태에 곱해줍니다. 이는 이전 셀 상태를 얼마나 유지할지를 결정합니다.\n- 출력 게이트: LSTM 셀의 출력과 다음 숨겨진 상태에 노출돼야 하는 셀 상태 얼마나 많은지를 결정합니다. 현재 입력과 이전 숨겨진 상태를 가져와서 시그모이드 활성화 함수를 통과시킵니다. 이 값은 현재 셀 상태의 tanh 값에 곱해져 다음 숨겨진 상태를 생성합니다.\n\n![LSTM 아키텍처를 이용한 어텐션](/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_1.png) \n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 표는 LSTM이 장기 의존성을 효과적으로 포착하기 위해 시간이 지남에 따라 셀 상태를 유지하고 조정할 수 있도록 함께 작동하는 게이트를 보여줍니다. 그러나 위 구조에서 입력의 문맥 길이가 증가하면 LSTM이 이러한 게이트에 모든 필요한 문맥을 저장하는 것이 어려워집니다. 예를 들어, \"자주 피우지 마세요, 이것은 당신의 폐에 강력한 영향을 미치고 더 심각한 합병증으로 이어질 것입니다.\" 라는 문장을 생각해보십시오. 위 문장에서 모델이 \"자주\"를 잊어버리면 문장 전체의 의미가 변경될 수 있습니다.\n\n그래서 이를 피하기 위해 어텐션(Attention)이 도입되었습니다. 이 기법을 도입한 주된 목적은 디코더가 다음 토큰을 예측할 때 인코더의 각 입력 토큰의 영향을 받도록 하는 것입니다. 이렇게 하면 입력의 문맥 길이에 관계없이 디코더가 모든 단어에 액세스할 수 있습니다. 유사도 점수에 따라 특정 입력 토큰이 다른 것보다 예측에 더 많은 영향을 미치도록 허용됩니다.\n\n# 어텐션은 어떻게 작동하나요?\n\n![LSTM 아키텍처를 사용하여 어텐션이 설명된 그림](/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 아키텍처 다이어그램에서는 영어에서 스페인어로 번역 작업에 인코더-디코더 모델이 사용됩니다. 인코더는 LSTM의 단일 레이어를 사용하며 해당 레이어에 2개의 LSTM 셀이 있습니다. LSTM이 다음 단어를 인코딩할 때, 각 LSTM 레이어에 동일한 가중치와 편향이 사용됩니다. 디코더에는 다른 가중치와 편향 세트가 있습니다. 이 인코더-디코더 아키텍처의 구분은 가변 길이의 입력과 출력을 번역하는 데 도움이 됩니다. 예를 들어, \"Let's go\"가 \"Vamos\"로 번역됩니다. 디코더는 'EOS' 토큰에 도달하거나 최대 출력 단어 제한에 도달했을 때 단어 생성을 중지합니다.\n\n인코더에서 나오는 컨텍스트 벡터는 해당 LSTM 레이어의 각 단어인 \"let's\"와 \"go\"에 대한 정보를 제공하는 추가 데이터와 함께 디코더로 전달됩니다.\n\n이를 위해 LSTM 인코더의 단기 메모리에서 가져온 값은 현재 디코더 상태와 코사인 유사도를 사용하여 비교됩니다. 마찬가지로, 두 번째로 펼쳐진 LSTM 네트워크의 출력은 현재 디코더 상태와 다시 계산됩니다. 이 유사도 점수는 각 입력 토큰에서 각 출력 토큰과 계산되며 이 점수는 Softmax 함수를 통과하여 확률로 변환됩니다.\n\nSoftmax 함수 공식:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_3.png)\n\n이 확률 점수는 각 입력 토큰이 해당 출력 토큰에 미치는 영향의 양을 나타냅니다. 그런 다음 인코더 출력은 해당 확률 점수에 따라 조정되어 인코더 출력의 가중 합인 컨텍스트 벡터를 계산합니다. 마지막으로 이 벡터는 현재 디코더 상태 벡터에 추가되어 다음 상태의 확률 점수를 생성하기 위해 완전 연결 네트워크를 통과합니다. 디코더 어휘에서 가장 높은 확률 점수를 가진 토큰이 예측되고, `EOS` 토큰이 예측되거나 출력 단어 제한이 도달할 때까지 디코더의 다음 레이어로 전달됩니다.\n\n이 아이디어를 통해 LSTM 모델의 필요성이 결국 사라지고 Transformer가 대세가 되었습니다.\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 어텐션(Attention)이 언어 모델이 입력에서 각 단어의 중요성을 이해하는 데 어떻게 도와주는지 설명합니다. 트랜스포머(Transformer) 아키텍처에서, 셀프-어텐션(Self-Attention)과 마스크드 셀프-어텐션(Masked Self-Attention)에 대해 이해할 수 있습니다.\n\n# 참고 자료\n\nStatquest: Joshua Starmer https://youtube.com/@statquest?si=sX9sq6vUnjzVbhCr\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” arXiv, August 1, 2023. https://doi.org/10.48550/arXiv.1706.03762.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLST Architecture Diagram – [Link](https://towardsdatascience.com/lstm-recurrent-neural-networks-how-to-teach-a-network-to-remember-the-past-55e54c2ff22e)","ogImage":{"url":"/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_0.png"},"coverImage":"/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_0.png","tag":["Tech"],"readingTime":4},{"title":"AI 사용 사례는 근본적으로 다릅니다","description":"","date":"2024-06-19 02:56","slug":"2024-06-19-AIUseCasesareFundamentallyDifferent","content":"\n\nAI를 통합하는 성공은 적절한 AI 사용 사례를 선택하는 데 크게 의존합니다. 이것은 알고리즘, 데이터 및 엔지니어링에 집중하기 전 제품 중심의 기술 전문가들에게 제시되는 관점입니다.\n\nAI 프로젝트가 종종 실패하는 이유는 실제로 시작조차 하지 않았기 때문입니다. 적절한 인간 문제를 해결하지 않거나 사용자 기대치를 충족할만한 최소 성능 기준을 충족하지 않았기 때문입니다.\n\nAI는 소프트웨어 버전의 덕트 테이프와 같습니다. 여러 가지에 유용하지만 모든 것에는 해당되지 않습니다. 마찬가지로, 훌륭한 제품은 사람들이 특정 작업을 아주 효과적으로 수행할 수 있도록 돕습니다. 그러나 AI는 여러 가지를 보통 잘합니다.\n\nAI가 특별히 잘 작동하는 사용 사례를 찾는 것은 효과적이지 않습니다. 이는 AI의 본성 상의 확률적 특성을 무시합니다. 대신, 중간 수준의 AI 성능을 갖는 사용 사례를 찾으면 더 낮은 리스크로 즉각적 가치를 제공할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전에 수행된 작업은 일반적인 컴퓨팅과 구글의 숙취 섬 비유를 통해 이 주제를 조사했습니다.\n\n중간 수준의 AI 성능을 효과적으로 활용하는 사용 사례들은 반복적인 성공 이력을 갖고 있습니다. 본문에서는 최종 사용자가 센서 퓨전, 생성형 AI, 자연어 처리, 컴퓨터 비전, 자율 로봇을 포함한 5가지 범주에서 제품 예시를 통해 AI를 경험하는 방법에 대해 설명합니다: \n\n- 센서 퓨전\n- 생성형 AI\n- 자연어 처리\n- 컴퓨터 비전\n- 자율 로봇\n\n# 센서 퓨전\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여러 센서에서 수집한 데이터를 결합하면 더 정확하고 완전한 표현을 얻을 수 있어요. 이러한 형태의 인공 지능은 하드웨어와 소프트웨어 제품에서 가장 일반적으로 사용돼요.\n\n한 가지 간단한 예는 보행자 카운터예요. 보행자 카운터는 운동을 즐겨하고 매일 운동을 할 수 있게 동기부여해 줘요. 하지만 인공 지능이 너무 우수하게 작동할 필요는 없어요. 그저 합리적으로 잘 작동하면 유용하게 활용할 수 있어요.\n\n소비자들에게는 정확도가 90%인 보행자 카운터(10걸음 중 1걸음 오류)가 일반적으로 매일의 피트니스 모니터링에 유용하게 사용돼요. 의도적인 디자인은 중간 정도의 인공 지능 성능을 보완할 수도 있어요. 예를 들어, 사용자에게 정확한 걸음 수가 중요하지 않고, 그저 1만걸음을 달성하거나 반지를 마무리하는 등의 목표에 도달하는 것이 중요하게 된답니다.\n\n기업들에게는 보행자 카운터가 특수 임상 연구와 같이 높은 정확도(95~99%)를 요구할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n둘 중 하나에 대한 경우에도 AI는 독특하게 가치가 있습니다. 왜냐하면 존재하는 것이 다른 대안들보다 낫기 때문입니다: (1) 자신의 걸음 수를 세는 사람 또는 (2) 아무 것도 아닌 것. 중간 AI 성능이 유용한 유사한 상황: 사진 태깅을 위한 얼굴 인식, 실시간 번역, 쇼핑 추천, 음성메시지 전사, 배터리 수명 최적화.\n\n모든 이들은 좁은 AI의 예시로, 종종 눈에 띄지 않지만 일상생활에서 자주 나타납니다. 좁은 AI는 특정 작업(걸음 수 세기)을 위해 설계되었으며, 특정 맥락(활동 추적) 내에서 사용할 목적으로 설계되었습니다. 이러한 AI 능력을 사용하기 위해 설계된 제약 조건들로 인해, 대규모 기업들에 의해 일반적으로 배포됩니다.\n\n# 생성적 AI\n\n생성적 AI의 본질은 같은 결과를 두 번 제공하지 않을 것이라는 것입니다. 이것은 일반적으로 실수 및 일부 예측할 수 없는 상황이 받아들여지는 사용 사례에서 잘 작동합니다: 미술, 음악, 글쓰기, 영화 및 엔터테인먼트, 게임 등.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nKrea.ai, Superside, 및 Microsoft cocreator 같은 제품들은 AI를 창의적인 협업자로 위치시켜, 스케치를 전문적인 구성물로 변환합니다. AI는 충분히 잘 작동하고 합리적으로 좋은 시각물을 생산하기만 하면 됩니다.\n\n## 초보자, 취미가 있는 사람들, 전문가\n\n생성적 AI는 대부분 초보자에게 도움이 되어 빠르게 아이디어를 현실로 만들어줍니다. 초보 사용자들은 또한 AI가 상대적으로 잘 일을 할 것을 믿습니다. 그러나 전문가들은 AI가 어렵게 표현할 독특한 비전을 가지고 있습니다. 그리고 취미가 있는 사람들(예술가, 작가, 음악가 등)은 창조 과정의 감정적인 고점과 저점에서 행복한 우연을 경험하는데, AI에 의존함으로써 이 인간적 경험의 보존을 도전받게 됩니다.\n\n## 다목적 AI \u0026 윤리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n생성 AI는 다목적입니다. 그러나 좁은 범위의 AI와 달리 사람들이 마음대로 조작할 수 없기 때문에 거의 아무도 상처받지 않도록 안전하게 설계된 다목적 AI는 드뭅니다.\n\n다목적 AI는 여러 응용 분야에서 유용하지만 의도치 않은 결과에 미치는 영향에 노출될 수도 있습니다. 다양한 산업 분야에서 사용되는 경우, 모델이 내용을 무단으로 재생산하거나 이전 예시를 재조합하여 새로운 콘텐츠를 생산하는 사례가 많이 있습니다.\n\n그러나 적절한 상황에서 적용된다면 성능이 중간인 생성 AI조차도 극도로 유용할 수 있습니다: (1) 일부 오류가 허용되는 경우, (2) 무작위성이 사용자 경험을 향상시킬 때, (3) 0에서 1로 가기를 원할 때, (4) 기존 작업을 빠르게 변형하는 것이 원본 콘텐츠를 작성하는 것보다 유용할 때.\n\n# 자연어 처리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금 가장 널리 사용되는 AI 형태입니다. 소비자 및 기업 사용 사례에서 언어는 만연하며, 텍스트 데이터는 디지턈 상호작용에서 가장 접근하기 쉽습니다.\n\nNLP로 달성하기 어려운 사용 사례들은 일반적으로 도메인 전문지식과 세심한 언어 이해를 필요로 합니다: 비꼬는 말 감지, 복잡한 이야기 이해, 전문가의 의사결정 능력이 필요한 새로운 상황에 적응하는 것 등.\n\nNLP로 쉽게 달성할 수 있는 사용 사례들은 두 가지 시각에서 볼 수 있습니다: (1) 전문가가 아닌 인턴을 위한 작업, (2) AI가 대부분의 작업을 처리한 후, 인간이 나머지 작업을 완료하는 상황.\n\n- 전문가가 아닌 인턴을 위한 작업. Notion은 전체 작업 공간에서 관련 키워드를 찾아줍니다, Otter.ai는 말을 텍스트로 전사해줍니다, SummarizeBot은 문서를 요약으로 요약해줍니다, Paperpile은 연구자들이 논문을 찾고 인용하는 데 도움을 줍니다, iOS는 앱에서 텍스트를 번역하고 음성 메시지를 전사합니다. 모두 도메인 전문지식이 필요하지 않은 작업으로 워크플로우를 향상시키는 작업입니다.\n- AI가 선처리를 한 뒤 사람이 확인하는 방식. 스팸 필터는 이메일을 스팸 또는 비 스팸으로 분류하는 간단한 이진 분류기입니다. 뉴스 기사도 카테고리별로 레이블을 붙일 수 있습니다: 스포츠, 기술, 엔터테인먼트, 정치, 기타... 이러한 작업은 사람에게 시간과 오류가 많이 발생하는 간단한 작업이지만, AI에게는 비교적 빠르고 간단합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 렌즈는 AI가 대량의 반복 작업을 효과적으로 처리하여 인간의 실수가 발생하기 쉬운 작업에 집중할 수 있게 해줄 수 있는 상황을 설명합니다.\n\n# 컴퓨터 비전\n\n세상의 대부분 데이터는 시각적이고 공간적입니다. 언어가 가장 흔하지만 많은 데이터가 이미지나 공간으로 표현됩니다. \n\n세상을 모델링하는 것은 매우 어렵습니다. 자율 주행(포부)은 지속적으로 약속되지만 실현하기는 어렵습니다(상용화되지 않음):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자율 주차는 구축하기 어려우며 도시 주차를 두려워하는 소수의 운전자들에게만 도움이 됩니다. 하지만 어떤 주차 공간이 충분히 큰지 예측하는 간단한 기능으로 시작할 수 있습니다. 자동화된 AI 유지보수 시스템을 만드는 것은 어려울 수 있습니다. 그러나 사용자는 장비 사용량을 감시하고 시각 검사를 통해 고장을 추적하는 단순한 AI 기능을 쉽게 활용할 수 있습니다. 간단한 피드백 루프에서 시작하여 점차 데이터가 허용하는 한 복잡성으로 나아갈 수 있습니다. AI 헤드라인들은 각자 비전을 가진 사용 사례들을 약속하는 경향이 있어서 역사적으로나 최근에도 혼합된 결과를 가져왔습니다. 하지만 점진적인 AI 솔루션은 아직도 즐거운 완성된 제품이 될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 자율 로봇\n\n인간-기계 협력은 로봇 공학 분야에서 철저히 연구되어 왔습니다. 이러한 개념은 이제 디지털 제품 경험으로 전이되어, 공동 조종사, 에이전트 경험, AI 파트너, AI 협업이라고 브랜드화되고 있습니다.\n\n## 자율 수준\n\n로봇 공학에서 정의된 6개의 자율 수준을 고려해보세요. 이 수준은 자율이 없는 수준 0(스팸 필터)에서 완전히 자율적인 수준 6(다양한 작업을 독립적으로 처리할 수 있는 개인 비서)까지 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금 대부분의 AI 시스템은 1~5단계 사이에 위치하며, 사용자 피드백이 있는 반자율적 AI 시스템이다. 이러한 시스템은 두 가지 상호적응 에이전트, 즉 사용 조건에 따라 정확도가 변하는 AI 시스템과 사용자의 작업 전문성 및 AI 정신 모델이 다양한 사용자로 인해 설계하기 어려운 과제입니다.\n\n## AI 시스템 성능 대 디자인\n\n비자율적 및 완전자율적 시스템 모두, 사용자의 정신 모델은 상대적으로 동일하며, AI가 합리적으로 잘 작동하거나 독립적으로 작동하는 것을 예상하거나 아예 그렇지 않도록 합니다. 완전자율적 시스템은 필요할 때만 사용자에게 알려주므로 AI 시스템 성능에 전적으로 의존합니다.\n\n그러나 지금의 대다수 사용 사례는 반자율적 AI를 사용합니다. 이러한 시스템은 (1) 시스템 성능을 기반으로 사용자 기대치를 중재하고, (2) 시스템에 대한 사용자 신뢰에 따라 다양한 작업 흐름을 허용하는 디자인을 주로 의존합니다. 예를 들어, 사용자는 AI에 독립적으로 작동하도록 신뢰할 수도 있고, AI 시스템의 투명성을 원해 일부 감시를 통해 개입할 때가 있을 수도 있으며, 아예 AI 없이 작업을 완료할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n성공적인 예 중 하나는 로봇 청소기인 룸바(Roomba)입니다. 사람들은 룸바를 자유롭게 이동시킬 수 있습니다(완전 자율), 멈춰있을 때 개입할 수도 있습니다(반자율), 혹은 룸바의 일정을 일시 중지하고 수동으로 공간을 청소할 수도 있습니다(비자율).\n\n일부 지저분한 지점을 놓치거나 멈추는 등의 적당한 AI 성능에도 불구하고, 룸바는 여전히 엄청난 편의성을 제공합니다. 로봇의 의도가 명확하기 때문에 사용자에게 특이한 간단한 인간-기계 제휴 관계입니다.\n\n시스템 성능이 향상되고 제품의 현실적인 가치를 위해 더 많이 사용됨에 따라, 사용자들은 시스템에 대한 신뢰도도 증가합니다.\n\n# AI 사용 사례는 근본적으로 다릅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI 성능이 빠르게 향상되고 있습니다. 하지만 전통적인 소프트웨어와는 달리, AI 결과는 본질적으로 결정론적이 아닙니다.\n\nAI 프로젝트의 도전 과제는 이미 시사되어 있는 해결책이 있는 것입니다. 성공적인 프로젝트는 먼저 문제를 효과적으로 판매할 때 주목받습니다. 하지만 가치 있는 사용자 문제부터 시작하는 대신, AI 프로젝트에 착수하기는 이미 AI로 결정되어 있는 것처럼 보입니다.\n\n이것은 사용 사례를 찾는 다른 방법을 제안하며, 확률적 시스템에 사용자의 필요성을 매칭시키는 것을 의미합니다.\n\nAI가 탁월하게 수행해야 하는 사용 사례는 기대를 충족시키지 못할 수 있습니다. AI를 모든 상황, 모든 사용자에게 항상 합리적으로 구현할 수 없는 높은 기준을 설정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n중간 수준의 AI 성능이 여전히 가치 있는 사용 사례를 찾는 것은 이미 사용자가 가치를 얻는 기본선 상에서 시작됩니다.\n\n정확도가 향상되고 환각이 줄어들면 사용자 경험도 자연스럽게 향상됩니다.\n\n이 기사의 서문을 확인해보세요:\n\n## 읽어주셔서 감사합니다! 여러분의 생각을 알고 싶어요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nElaine 씨는 디자인, 인공 지능, 신흥 기술 등에 대해 글을 씁니다. 더 많은 소식을 받으려면 팔로우해 주세요. 또는 LinkedIn에서 연락을 주세요.","ogImage":{"url":"/assets/img/2024-06-19-AIUseCasesareFundamentallyDifferent_0.png"},"coverImage":"/assets/img/2024-06-19-AIUseCasesareFundamentallyDifferent_0.png","tag":["Tech"],"readingTime":7},{"title":"고급 계산을 위한 클러스터 컴퓨터 활용 포괄적 가이드","description":"","date":"2024-06-19 02:54","slug":"2024-06-19-UtilisingaClusterComputerforAdvancedComputationsAComprehensiveGuide","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-UtilisingaClusterComputerforAdvancedComputationsAComprehensiveGuide_0.png\" /\u003e\n\n계산, 프로그래밍, 기계 학습, 신경망, 인공 지능 및 로보틱스를 위해 조립된 클러스터 컴퓨터를 사용하는 것은 몇 가지 단계가 필요합니다. 여기에 시작하는 데 도움이 되는 포괄적인 안내서가 있습니다:\n\n클러스터 설정\n\n하드웨어 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n클러스터 내의 각 컴퓨터(노드)가 네트워크에 올바르게 연결되었는지 확인해주세요.\n\n이더넷 또는 인피니밴드와 같은 고속 네트워킹 하드웨어를 사용해주세요. 모든 노드에 안정적인 전원 공급을 확인해주세요.\n\n밀집된 연산 중에 오버히팅을 방지하기 위해 적절한 냉각이 되어 있는지 확인해주세요.\n\n## 소프트웨어 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 운영 체제: 모든 노드에 Linux 배포판을 설치하세요. 고성능 컴퓨팅에서 널리 사용되는 운영 체제입니다.\n- 네트워킹: 모든 노드가 통신할 수 있도록 네트워킹을 구성하세요. 일반적으로 노드 간 비밀번호 없는 통신을 위해 SSH 키를 설정하는 것이 필요합니다.\n- 클러스터 관리 소프트웨어: Apache Hadoop, Kubernetes 또는 OpenMPI와 같은 클러스터 관리 소프트웨어를 설치하여 노드 간 작업을 관리하고 조율하세요.\n\n환경 설정하기\n\n분산 파일 시스템\n\n- HDFS: Hadoop 기반 설정에 대해, Hadoop 분산 파일 시스템(HDFS)을 설치하고 구성하여 노드 간 데이터를 관리하세요.\n- NFS/GlusterFS: Hadoop이 아닌 설정에 대해, NFS 또는 GlusterFS를 공유 저장소로 활용해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n소프트웨어 라이브러리 및 도구\n\n- 프로그래밍 언어: 필요한 프로그래밍 언어(예: Python, C++, Java)가 설치되어 있는지 확인합니다.\n- 라이브러리: 머신러닝 및 데이터 처리를 위한 관련 라이브러리를 설치합니다 (예: TensorFlow, PyTorch, Scikit-Learn, NumPy).\n- 컨테이너화: Docker 또는 Singularity를 사용하여 클러스터 전체에서 일관된 환경을 생성하고 배포합니다.\n\n작업 구성 및 실행\n\n## 작업 예약\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n작업 예약 소프트웨어인 SLURM, PBS 또는 Kubernetes를 사용하여 작업을 관리하고 리소스를 할당하세요.\n\n필요한 리소스를 지정하는 작업 스크립트를 작성하세요(예: 노드 수, CPU/GPU 요구 사항) 및 실행할 작업.\n\n프로그래밍 및 개발\n\n- 분산 컴퓨팅: 여러 노드를 활용할 수 있는 프로그램을 작성하세요. MPI (메시지 패싱 인터페이스) 애플리케이션의 경우 OpenMPI 또는 MPICH와 같은 라이브러리를 사용하세요.\n- 병렬 처리: Python의 Dask나 대용량 데이터 처리를 위한 Spark와 같은 병렬 처리 라이브러리를 사용하세요.\n- 머신 러닝 프레임워크: TensorFlow나 PyTorch와 같은 분산 훈련을 위해 프레임워크를 구성하세요. 이는 매개변수 서버와 워커 노드 설정을 포함할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특정 응용 프로그램\n\n기계 학습 및 신경망\n\n- 분산 훈련: TensorFlow의 tf.distribute.Strategy 또는 PyTorch의 torch.distributed를 사용하여 여러 노드에 걸쳐 훈련을 분산시킵니다.\n- 데이터 처리: 데이터 파이프라인이 모델에 데이터를 효율적으로 공급할 수 있도록 하고, 분산 데이터 저장 및 처리 시스템을 사용할 수 있습니다.\n\n인공 지능\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 추론: 대규모 추론 작업을 위해 클러스터 전체에 훈련된 모델을 배포합니다.\n- 모델 서빙: TensorFlow Serving 또는 TorchServe와 같은 도구를 사용하여 모델 서빙을 관리하고 확장합니다.\n\n로봇공학\n\n- 시뮬레이션: Gazebo와 같은 도구를 활용하여 복잡한 고품질 시뮬레이션을 클러스터를 활용하여 실행합니다.\n- 제어 알고리즘: 실시간으로 실행될 수 있는 고급 제어 알고리즘을 개발하고 클러스터 전체에 분산하여 배포합니다.\n\n모니터링 및 유지보수\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**모니터링 도구**\n\n- 리소스 모니터링: 프로메테우스, 그라파나 또는 갱글리아 같은 모니터링 도구를 사용하여 클러스터의 성능과 리소스 사용량을 추적합니다.\n- 로깅: ELK 스택(Elasticsearch, Logstash, Kibana)이나 플루언트드 같은 도구를 활용하여 중앙 집중식 로깅을 구현하여 모든 노드에서 로그를 수집하고 분석합니다.\n\n**유지 보수**\n\n소프트웨어 및 라이브러리를 최신 상태로 유지하여 보안과 호환성을 보장하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n중요한 데이터와 구성을 정기적으로 백업하여 데이터 손실을 예방하세요.\n\n예제 워크플로우\n\n다음은 클러스터에서 머신 러닝 모델을 학습하는 예제 워크플로우입니다:\n\n- 데이터 준비: HDFS 또는 다른 분산 파일 시스템을 사용하여 데이터를 저장하고 전처리합니다.\n- 환경 설정: Docker를 사용하여 모든 종속성이 포함된 컨테이너를 생성하고 노드 간에 배포합니다.\n- 작업 제출: SLURM을 사용하는 작업 스크립트를 작성하여 리소스를 요청하고 TensorFlow를 사용하여 분산 훈련 작업을 실행합니다.\n- 모델 훈련: 스크립트는 여러 노드 간에 훈련을 시작하며, 각 노드는 모델 훈련 프로세스의 일부를 실행합니다.\n- 모니터링: Grafana를 사용하여 훈련 진행 상황과 리소스 이용을 모니터링합니다.\n- 모델 배포: 훈련이 완료되면 TensorFlow Serving을 사용하여 모델을 대규모 추론에 배포합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n고급 계산을 위한 클러스터 컴퓨터를 설정하고 사용하는 것은 하드웨어와 소프트웨어의 신중한 계획과 구성을 필요로 합니다. 분산 컴퓨팅 프레임워크, 기계 학습 라이브러리 및 효과적인 모니터링 도구를 활용하여 클러스터 전반에 걸쳐 복잡한 계산과 AI 애플리케이션을 효율적으로 실행할 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-UtilisingaClusterComputerforAdvancedComputationsAComprehensiveGuide_0.png"},"coverImage":"/assets/img/2024-06-19-UtilisingaClusterComputerforAdvancedComputationsAComprehensiveGuide_0.png","tag":["Tech"],"readingTime":4},{"title":"신기한 여행 스탠포드의 매직 월드 크리에이터","description":"","date":"2024-06-19 02:53","slug":"2024-06-19-WonderJourneyStanfordsMagicalWorldCreator","content":"\n\n제 뉴스레터에서 최근 세계적인 Fei-Fei Li의 Stanford 연구소에서 만든 최신 모델에 대해 이야기했었죠. 그 모델은 명령으로 무한한 마법의 3D 세계를 만들어냈습니다.\n\n하지만 저는 그 모델의 깊은 부분까지 파헤치게 되었고, 그 모델에 너무 매료되어 세계에서 가장 진보된 텍스트/이미지-3D 모델이 어떻게 작동하는지 상세히 설명하고 싶어졌습니다.\n\n이 모델은 게임, 가상 현실, 증강 현실, 혼합 현실에 혁명을 일으킬 수 있을 뿐만 아니라, Fei Fei Li 자신이 시사한 대로, AI가 우리의 세계를 더 잘 이해할 수 있는 세계 모델을 만드는 데 도움이 될 수 있습니다.\n\n# 마법 창조하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nWonderJourney는 텍스트 설명 또는 이미지를 기반으로 무한하지만 일관된 3D 장면을 생성하는 AI 모델입니다.\n\n시각적으로 완벽하지는 않지만 비디오의 모든 것이 완전히 AI로 생성된 것이라는 아이디어는 정말 놀라운 것이죠.\n\n하지만 이것이 어떻게 작동하는 걸까요?\n\n## 모듈식 접근\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nWonderJourney는 세 가지 구성 요소로 나뉩니다:\n\n- LLM: 다음 장면을 생성하는 데 책임을 지는 대형 언어 모델(Large Language Model).\n- VSG(Visual Scene Generator): LLM의 다음 장면 텍스트 설명과 현재 장면 이미지를 입력으로 받아 다음 3D 장면을 생성하는 모델.\n- VLM validator: 새로 생성된 장면을 검사하고 품질이 충분히 높지 않은 경우 재시도를 요청하는 Vision Language Model.\n\n전체 모델의 표현은 다음과 같습니다:\n\n![WonderJourney Model](/assets/img/2024-06-19-WonderJourneyStanfordsMagicalWorldCreator_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이해하기 어려운 내용이죠. 다양한 기술적 구성 요소에 대해 설명하기 전에 완전한 예시를 보는 것이 좋습니다:\n\n- 사용자가 요청한 내용: \"산속의 아늑한 마을, 자갈길과 나무집이 있는 곳. 뒤쪽에 눈을 덮은 봉우리가 솟아 있습니다.\"\n- LLM이 새로운 장면 생성: \"아늑한 마을을 지나 숲으로 들어가면, 땅에 입체적인 그림자를 드리우는 덤불들이 보입니다.\"\n- Visual Scene Generator가 다음 3D 장면을 생성하며 마을을 멀리 뒤쪽에 밀어 넣어 숲으로 이동하는 느낌을 전달합니다.\n- VLM이 새로운 장면을 확인하고, 우리는 이 과정을 반복합니다.\n\n![이미지](/assets/img/2024-06-19-WonderJourneyStanfordsMagicalWorldCreator_1.png)\n\n이제 전체적인 파이프라인을 이해했으니, 생각해볼 점은 이 모든 것이 어떻게 작동하는 걸까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 다양한 모델 집합\n\n원더저니의 멋진 점 중 하나는 그 구성 요소들이 모듈식이라는 것입니다. 즉, 좋아하는 LLM, VSG 또는 VLM을 사용할 수 있다는 뜻이죠. 그러나 원더저니에는 몇 가지 중요한 모델이 포함되어 있습니다.\n\n따라서, 이해를 돕기 위해 3D 장면이 어떻게 생성되고 카메라의 역할에 대해 상세히 설명하면서 전반적인 내용을 설명하는 것이 좋을 것 같아요.\n\n## 2D에서 3D로, 그리고 그 반대로\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3D 장면을 논의할 때, 화상의 경우를 제외하고는 여전히 2D 화면에 표시해야 합니다 (가상 헤드셋 제외, WonderJourney의 사용 사례가 아닙니다).\n\n다시 말해, 전반적인 작업의 많은 부분은 3D 장면이 \"카메라\"의 2D 렌즈 (사용자의 화면)로부터 어떻게 보여질지 계산하는 것입니다.\n\n그러므로, 새로운 장면을 사용할 때마다, WonderJourney는 2D 이미지를 사용해서 새로운 3D 장면을 생성하지만, 결국 이 장면은 화면에 다시 2D로 투영됩니다.\n\n이를 알고 나면 이제 WonderJourney가 어떻게 작동하는지 이해할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 깊이, 포인트 클라우드 및 투영\n\n새로운 장면을 생성하는 전체 프로세스는 다음과 같습니다:\n\n![이미지](/assets/img/2024-06-19-WonderJourneyStanfordsMagicalWorldCreator_2.png)\n\n- 먼저, 모델은 초기 조건으로 텍스트 또는 이미지를 취하며, 사용자가 텍스트 설명만을 제공한 경우 이미지를 생성합니다.\n- 이 입력을 사용하여, 먼저 이미지에서 각 요소의 깊이를 추정하며, 즉, 각 객체가 실제로 얼마나 먼지 가까운지를 추정합니다 (예: 하늘은 항상 '멀리있음'으로 추정되어야 합니다).\n- 다음 단계는 깊이 추정으로부터 포인트 클라우드를 생성하는 것이며, WonderJourney는 이를 정제하여 요소 사이의 '날카로운' 경계를 보장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나, 저희는 여전히 현재 뷰를 작업 중이며 새로운 뷰가 필요합니다. 이를 위해 2D 화면(우리)에서 3D 장면을 볼 '카메라'는 뒤로 밀려서 이전 장면이 멀리 뒤쪽에 나타나도록 하여 '우리가 그것에서 멀어지고 있다'는 아이디어를 전달합니다. 이제 다음 장면에 나타날 현재 장면의 일부가 위치하고, 새 프레임의 그 부분이 렌더링됩니다.\n\n그런 다음, 이 부분적 렌더링과 다음 장면의 LLM(언어-이미지 모델) 설명을 사용하여 WonderJourney는 나머지 장면을 outpaint하는데, 이 경우에는 Stable Diffusion 모델을 사용하지만, 중요한 건 LLM의 새로운 장면이어야 하는 내용에 따라 조건이 부여된다는 것입니다.\n\n마지막으로, 새 이미지를 얻었으면, 우리는 간단히 깊이 추정 및 정제 프로세스를 반복하여 필요에 따라 여러 가지 대안적인 뷰를 생성하여 새로운 포인트 클라우드를 만듭니다.\n\n이 새로운 포인트 클라우드는 본질적으로 새로운 3D 뷰를 구축하며, VLM(시각 언어 모델)이 평가합니다. 품질 임계값을 충족하면, 해당 장면이 뷰어의 2D 화면에 투사되고, 프로세스가 반복됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 있어요, 끝없이 이어지고 항상 일관된 3D 여행이 마련되었습니다.\n\n# 모든 형태의 정복\n\n한 번 더, 학계는 사용자의 텍스트 또는 이미지 명령에 기반한 무한한 3D 장면을 생성할 수 있는 첫 번째 모델의 가능성을 확장했습니다.\n\n마지막으로, Fei Fei Li의 생각을 빌리자면, 강력한 3D 생성 모델을 구축하는 것은 AI에게 우리 세상에 대한 큰 공간적 이해력을 제공할 수 있으며, 이는 그들의 지능을 향상시키는 방법으로 활용될 수 있고, 누가 알겠으나, 구체적 AI 모델, 즉 인간형 로봇의 등장을 용이하게 할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현실 세계와 상호 작용할 수 있는 능력을 기계에 부여하면, 시뮬레이션 환경을 통해라도 사람들과 격차를 줄일 수 있을 것입니다. 우리는 관찰과 세상과의 상호 작용을 통해 배우기 때문에, 오늘날의 최첨단 AI 모델들을 뛰어넘는 능력을 갖게 됩니다, 어떻게 ChatGPT나 Claude가 감탄을 자아내는지와 상관없이요.\n\n우리가 이것이 옳은 길이라고 주장할 수는 없겠지만, 텍스트를 단순히 모델에 던지고 AGI로 성장할 것을 바라는 것보다는 훨씬 매력적으로 보입니다.","ogImage":{"url":"/assets/img/2024-06-19-WonderJourneyStanfordsMagicalWorldCreator_0.png"},"coverImage":"/assets/img/2024-06-19-WonderJourneyStanfordsMagicalWorldCreator_0.png","tag":["Tech"],"readingTime":4},{"title":"사고, 빠르고 느린, LLMs와 PDDL과 함께","description":"","date":"2024-06-19 02:50","slug":"2024-06-19-ThinkingFastandSlowwithLLMsandPDDL","content":"\n\n\"ChatGPT가 실수를 할 수 있다는 사실을 확인해 주세요.\" 이제 프롬프트 바로 아래에 적혀 있고, ChatGPT가 날짜부터 전체 참조까지 아무 것이나 단정적으로 만들어 내는 것에 대해 익숙해졌습니다. 하지만 기본적인 추론에 대해서는 어떨까요? 인공지능(AI) 연구 초기에 나온 간단한 탑 재배치 작업을 살펴본다면, 대형 언어 모델(Large Language Models, LLM)이 어떻게 한계에 도달하는지 보여주고, 이에 대처하기 위해 계획 도메인 정의 언어(Planning Domain Definition Language, PDDL)와 상징적 해결사들을 소개하겠습니다. LLM은 본질적으로 확률적이므로, 이러한 도구가 미래의 AI 에이전트의 내장될 가능성이 높습니다. 이는 상식적인 지식과 날카로운 추론을 결합할 것입니다. 이 글에서 최대한 많은 정보를 얻으려면, VS Code의 PDDL 확장 프로그램과 planutils 플래너 인터페이스를 사용하여 직접 PDDL 환경을 설정하고 예제를 따라해 보세요.\n\n대형 언어 모델(LLM)에서는 각 문자가 응답의 이전 문자뿐만 아니라 사용자의 프롬프트의 모든 이전 문자에 대해 조건이 걸립니다. 거의 모든 것을 학습한 LLM은 신의 영역이 되었을 뿐만 아니라 재치도 갖추게 되었습니다. 그러나 LLM이 실제로 문제에 대해 생각하려 하지 않고 근본적으로 게으르다는 것을 깨닫는 데 오래 걸리지 않습니다. 이는 \"인공지능\" 분야의 고전적인 문제 도메인인 \"블록 세계\"에서 나오는 간단한 예제로 설명할 수 있습니다. 아래 그림에 나타난 것처럼, 왼쪽의 탑을 오른쪽의 탑으로 변환하는 작업을 고려해보세요.\n\n![image](/assets/img/2024-06-19-ThinkingFastandSlowwithLLMsandPDDL_0.png)\n\n여기에서 작업은 왼쪽의 탑을 오른쪽의 탑으로 바꾸는 것입니다. 로봇이 다음과 같은 기능이 있다고 가정합니다:\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- pickup `색깔`: 테이블의 아무 곳에 색깔이 `색깔`인 블록을 집어올립니다.\n- putdown `색깔`: 테이블의 아무 곳에 색깔이 `색깔`인 블록을 내려놓습니다.\n- unstack `색깔1` `색깔2`: 맨 위에 `색깔2`가 있는 탑에서 `색깔1` 색을 가진 블록을 집어올립니다.\n- stack `색깔1` `색깔2`: 맨 위에 `색깔2`가 있는 탑에 `색깔1` 색을 가진 블록을 위에 놓습니다.\n\n그래서 ChatGPT3.5는 이런 문제에 어떻게 대처하나요? 이것이 프롬프트입니다:\n\n![이미지](/assets/img/2024-06-19-ThinkingFastandSlowwithLLMsandPDDL_1.png)\n\nChatGPT는 주어진 구문 형식으로 필요한 조치 순서를 즉시 제공합니다. 놀랍죠! 하지만 기다려주세요. 올바른 조치는 처음 세 가지만 맞는 것 같습니다! 로봇은 먼저 파란색 블록을 탑의 맨 위에서 어디든 테이블로 옮기고, 그런 다음 녹색 블록도 동일한 작업을 합니다. 이제 모든 블록이 테이블 위에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![ThinkingFastandSlowwithLLMsandPDDL_2](/assets/img/2024-06-19-ThinkingFastandSlowwithLLMsandPDDL_2.png)\n\nChatGPT는 지금 파란 블록을 집어 빨간 블록 위에 쌓으라고 제안했습니다. 하지만 이것은 우리가 원하는 바가 전혀 아닙니다: 파란 블록은 바닥에 있어야 합니다. 또한 탑이 완전하지 않습니다. 우리가 도와보겠습니다:\n\n![ThinkingFastandSlowwithLLMsandPDDL_3](/assets/img/2024-06-19-ThinkingFastandSlowwithLLMsandPDDL_3.png)\n\nChatGPT는 단서를 받고 단순히 마지막에 추가했습니다. 이제 초록 블록이 파란 블록 위에, 그리고 파란 블록이 빨간 블록 위에 쌓여 있습니다. 출력을 올바른 형식으로 만들기 위해 더 많은 단서를 제공함으로써 해결 할 수 있을 수도 있지만, 우리는 모두 ChatGPT가 실제로 두 가지 동일하게 잘못된 솔루션 사이를 왔다갔다하며 순환하기 시작하는 것을 보았습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChatGPT4o는 조금 더 나은 작업을 수행하며, 동일한 프롬프트에 대해 다음 출력물을 생성합니다:\n\n```js\nunstack blue green\nputdown blue\nunstack green red\nputdown green\npickup blue\nstack green blue\npickup red\nstack red green\n```\n\n이 사실상 맞지만, 중간에 있는 \"pickup blue\"에 대해 문제가 발생합니다. \"You cannot 'stack green blue' after 'pickup blue' as the robot is still holding 'blue'\"라는 불만에 의해, ChatGPT-4o는 마지못해 다음과 같이 따릅니다:\n\n```js\nunstack blue green\nputdown blue\nunstack green red\nputdown green\npickup blue\nputdown blue (if needed, otherwise skip)\npickup green\nstack green blue\npickup red\nstack red green\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChatGPT-4o가 실제로 ChatGPT-3.5보다 큰 발전이었다는 것이 밝혀졌어요. 초기 프롬프트에 더 많은 정보를 추가하는 것도 도움이 된다는 것을 알게 되었어요. 그러나 위에 표시된 최적의 행동 순서를 ChatGPT에 얻게 하는 것은 매우 어렵고, 더 복잡한 제약 조건에는 \"한 번에 테이블에 블록을 두 개만 놓을 수 있다\"거나, 인간형 조립 작업에서 두 팔을 사용하는 것과 같은 것은 전혀 불가능하다는 것이 증명되었어요.\n\nLLM에게 일부 신용을 주자면, 문제가 사실 모호하다고 주장할 수도 있어요. 실제로 위의 행동들을 신중하게 생각해보면 더 많은 오해로 이어질 것이라고 생각돼요. 예를 들어, ChatGPT에게 한 블록을 집을 때 다른 블록을 집기 전에 첫 번째 블록을 놓기 전까지 다른 한에 놓을 수 없게 하는 것을 전혀 알려준 적이 없다는 점이요.\n\n# 계획 도메인 정의 언어\n\n그렇다면 어떻게 이러한 문제를 공식적으로 설명하여 마지막 불확실성의 조각을 제거할까요? 한 가지 방법은 인공지능 계획 커뮤니티에서 표준이 되어온 계획 도메인 정의 언어 (PDDL)입니다. PDDL은 1998년부터 존재해오고 지속적으로 영역을 확장해왔으며, 다양한 문제 해결자를 탄생시켰습니다. 현재 계획 커뮤니티가 다루고 있는 문제 유형을 감을 잡기 위해 2023년 국제 계획 대회 작업을 확인해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPDDL 정의는 두 개의 파일로 구성됩니다:\n\n- 도메인\n- 문제\n\n타워 예제를 사용하여 문제를 시작해봅시다:\n\n```js\n(define (problem blocks)(:domain blocksworld)\n\n(:objects\n    red green blue  - block\n    )\n\n(:init\n    (ontable red) ; 블록 red\n    (on green red) ; 블록 green\n    (on blue green)(clear blue) ; 블록 blue\n    (handempty)\n\n)\n\n(:goal (and\n    (on green blue)\n    (on red green)\n))\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPDDL은 s-표현식을 사용하는데, 이는 LISP에 익숙하지 않은 사람들에게는 조금 적응이 필요할 수 있어요. 일반적으로 \"A + B\"라고 말하는 대신에 \"+\" 연산자를 먼저 써서 (+ A B)와 같은 형태로 작성해야 합니다.\n\n위의 예시에서 마지막 부분에서 \"goal\"이 정의되는 부분을 보실 수 있어요. (on green blue)는 \"green on blue\"를 읽고, (on red green)은 \"red on green\"을 읽어요. 마찬가지로 \"and\" 연산자는 두 문장 모두가 참이어야 한다는 것을 보장합니다. 즉, \"red on green and green on blue\"가 됩니다. 괄호들은 또 다른 과거의 요구사항입니다: LISP는 \"List Processor\"의 약자로, 리스트를 Python의 튜플과 유사하게 정의합니다. 예를 들어, (:goal `list`))는 또 다른 리스트를 받아들이는데, 다시 두 개의 리스트를 포함합니다. 여기서 (and `list`)는 내장 연산자이지만 (on `list`)는 아닙니다. 이것이 도메인 파일의 필요성이고, 나중에 소개하도록 하겠습니다.\n\n목표뿐만 아니라, PDDL 문제에는 초기 조건 목록도 필요합니다. 이들은 술어들로 제공되며, True 또는 False만을 가질 수 있는 표현식입니다. PDDL은 이러한 술어들을 (:init `list`) 다음에 오는 리스트에 모아두는데, 여기에는\n\n- (ontable red)\n- (on green red)\n- (on blue green)\n- (clear blue)\n- (handempty)\n\n와 같은 조건들이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 형용사를 변수 또는 함수로 생각하는 것은 유혹적일 수 있습니다. 그러나 이들은 그 둘 다 아닙니다. 단순히 명제입니다. 예를 들어, 문제에 명제 (handempty)가 추가되었다면, 이는 참입니다. 거짓으로 변경하려면 문제에서 제거해야 합니다. 따라서 이는 두 가지 값 중 하나를 포함할 수 있는 변수가 아닙니다. 정의되어 있으면 참이고, 정의되어 있지 않으면 거짓입니다.\n\n나머지 모든 형용사들은 처음 봤을 때에는 함수처럼 보이지만, 이들도 단순히 명제일 뿐입니다. (ontable red)는 빨간 블록이 탁자 위에 앉아 있다고 알려줍니다. (on green red)는 녹색 블록이 빨간 블록 위에 있다고 알려주며, (on blue green)는 파란 블록이 녹색 블록 위에 있다고 시그널을 줍니다. (clear blue)는 파란 블록이 잡을 수 있다는 것을 나타냅니다.\n\n그러나 파라미터 역할을 할 수 없으면 PDDL은 상당히 쓸모가 없을 것입니다. :objects 목록은 형용사를 구성할 수 있는 모든 객체를 정의합니다. 조금 더 엄밀하게 만들기 위해 PDDL은 typing을 제공하기도 합니다. 여기서 \"-블록\"은 빨간색, 녹색, 파란색이 모두 블록 유형임을 나타냅니다. 이렇게 하면 위의 모든 형용사가 블록 유형의 객체와만 작동하므로 논리 오류를 방지하는 데 조금 더 수월해집니다. 또한, 해당 유형의 객체만 고려하여 해결책을 찾는 것이 더 쉬워지며, 이에 따라 필요한 동작만 고려하면 됩니다.\n\n형용사들은 도메인 설명서에서 정의됩니다. 도메인의 이름은 문제 설명서의 :domain 목록에 지정됩니다. 도메인 이름은 도메인 파일의 첫 줄에도 표시됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n(define (도메인 블록월드)\n\n(:requirements :typing :negative-preconditions)\n\n(:types block) \n\n(:predicates\n (on ?a ?b - block)\n (clear ?a - block)\n (holding ?a - block)\n (handempty)\n (ontable ?x - block)\n)\n\n(:action pickup ; 이 액션은 테이블에서 들어올 때만 사용됩니다\n:parameters (?x - block)\n:precondition (and (ontable ?x)\n    (handempty)\n    (clear ?x)\n   )\n:effect (and (holding ?x)\n    (not (handempty))\n    (not (clear ?x))\n    (not (ontable ?x))\n  )\n)\n(:action unstack ; 블록에서 들어올 때만 적합\n:parameters (?x ?y - block)\n:precondition (and (on ?x ?y)\n    (handempty)\n    (clear ?x)\n   )\n:effect (and (holding ?x)\n    (not (handempty))\n    (not (clear ?x))\n    (clear ?y)\n    (not (on ?x ?y))\n  )\n)\n\n(:action putdown\n:parameters (?x - block)\n:precondition (and (holding ?x)\n   )\n:effect (and (ontable ?x)\n    (not (holding ?x))\n    (handempty)\n    (clear ?x)\n  )\n)\n\n(:action stack\n:parameters (?x ?y - block)\n:precondition (and (holding ?x)\n    (clear ?y)\n   )\n:effect (and (on ?x ?y)\n    (not (holding ?x))\n    (handempty)\n    (not (clear ?y))\n    (clear ?x)\n  )\n)\n\n)\n\n이 두 파일을 problem.pddl 및 domain.pddl로 저장하면 VS Code 확장 프로그램을 사용하여 Figure 1에 표시된 것과 같은 계획을 생성할 수 있습니다. 그렇지 않으면 온라인 PDDL 편집기를 사용하고 LAMA 솔버를 사용할 수도 있습니다.\n\n블록월드 도메인을 위한 유형과 술어는 도메인 파일에서 매우 일찍 정의되어 있습니다:\n\n(:types block) \n\n(:predicates\n (on ?a ?b - block)\n (clear ?a - block)\n (holding ?a - block)\n (handempty)\n (ontable ?x - block)\n)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 사용된 유형 (block만)은 :types 목록에 제공됩니다. 술어는 :predicates에 입력되며 전체를 \"?로 표시된 자리 표시자\"가 사용합니다. 모든 술어가 실제로 위의 패턴을 따르는지 확인하려면 위에 정의된 문제를 확인할 수 있습니다.\n\n유의할 점은 타이핑이 이미 PDDL의 비표준 기능임을 의미합니다. 타입이 사용됨을 파서와 솔버에 알리려면 :requirements 목록의 맨 처음에 :typing이 제공됩니다.\n\n이제 작업을 사용하여 술어를 조작할 수 있습니다. :action은 매개변수, 선행 조건 및 효과로 구성된 목록입니다. 작업은 모든 선행 조건이 True인 경우에만 실행됩니다. 작업의 효과는 술어의 생성 또는 삭제입니다. 여기서는 :negative-preconditions를 추가하여 가능하게 된 (not `list`) 연산자를 사용합니다.\n\n```\n(:action pickup ; 이 작업은 테이블에서 선택하는 것만을 위한 작업입니다\n:parameters (?x - block)\n:precondition (and (ontable ?x)\n    (handempty)\n    (clear ?x)\n   )\n:effect (and (holding ?x)\n    (not (handempty))\n    (not (clear ?x))\n    (not (ontable ?x))\n  )\n)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n픽업 액션은 블록 오브젝트 하나의 매개변수를 갖고 있어요. 여기서는 동일한 오브젝트를 가리킬 때 ?x를 사용해요. 액션이 실행되려면 세 가지 조건이 참이어야 해요:\n\n- (ontable ?x)\n- (handempty)\n- (clear ?x)\n\n오브젝트는 테이블 위에 있어야 하고, 그리퍼는 비어 있어야 하며, 오브젝트는 집을 수 있어야 해요. ontable ?x와 clear ?x는 함수가 아니에요. 이것들은 문제의 :init 목록에 제공되었거나 런타임에 액션에 의해 생성되어야 하는 술어들이에요.\n\n여기서 :effect가 등장해요. 이 목록에는 액션이 실행된 후 참이 될 술어들이 포함되어 있어요. 요것들이죠:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- (?x을(를) 잡고)\n- (손이 비어 있지 않음)\n- (?x가 비어있지 않음)\n- (?x가 테이블 위에 없음)\n\n즉, 작업 (pickup ?x)을 성공적으로 실행하면 로봇이 ?x를 잡고 있게 되며, (손이 비어 있지 않음), (?x가 비어있지 않음) 및 (?x가 테이블 위에 없음)과 같은 예측이 삭제됩니다.\n\n이 설정은 계획을 만드는 방법에 대한 첫 번째 단서를 제공합니다. 원하는 결과로 이어지는 효과를 살펴보고, 동일한 방법으로 선행 조건을 충족하도록 하여 초기 상태에 이르기까지 도달할 수 있습니다. 또는 초기 조건 집합에 모든 가능한 작업을 적용하고 목표를 찾을 때까지 이러한 결과 상태를 처리할 수 있습니다. 이 두 가지 모두 어려운 문제로, 계획은 NP-어렵다고 합니다.\n\n이제 남은 작업을 살펴보겠습니다. 먼저, unstack은 블록 ?x를 블록 ?y에서 집습니다. 이를 위해서는 ?x가 ?y에 있고, 손이 비어 있으며, ?x가 비어 있어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n(:action unstack ; block을 가져오기에 적합\n:parameters (?x ?y - block)\n:precondition (and (on ?x ?y)\n    (handempty)\n    (clear ?x)\n   )\n:effect (and (holding ?x)\n    (not (handempty))\n    (not (clear ?x))\n    (clear ?y)\n    (not (on ?x ?y))\n  )\n)\n\n로봇이 블록 ?y를 들 수 있도록 되었습니다. 이에 따라 (handempty), (clear ?x), (on ?x ?y)이 삭제되었습니다. 또한 (clear ?y)가 True로 변경되어, 블록 ?y가 현재 들어올 수 있는 상태가 되었습니다. putdown 및 stack 동작도 동일한 패턴을 따릅니다.\n\n# PDDL 문제 해결\n\n여기서 설명한 계획 문제는 AI 분야의 가장 오래된 주제 중 하나이지만, 여전히 활발히 연구되고 있는 분야입니다. PDDL의 최신 버전에는 시간 및 양에 대한 추론 능력이 포함되어 있으며, 특정 계획 문제는 일부 알고리즘을 사용하여 더 잘 해결할 수 있습니다. 가장 일반적인 플래너 중 하나인 \"FastDownward\" (https://www.fast-downward.org/)는 PDDL 2.1 기능 대부분을 지원합니다. 대부분의 플래너 목록은 https://planning.wiki/ref/planners/atoz에서 찾을 수 있으며, 대부분의 플래너는 지원하는 요구 사항 목록을 표시합니다.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문제 설명을 공식화하는 것 외에도 AI 커뮤니티는 플래너 인터페이스를 통합하고 https://github.com/AI-Planning/planutils 프로젝트가 도커 컨테이너를 제공하여 다양한 플래너 중에서 선택할 수 있도록 하고 로컬 웹서버를 통해 사용 가능하게 합니다. 이 방법은 VSCode의 Planning Domain Definition Language 확장 기능에 통합되어 있습니다. 구문 강조 기능 뿐만 아니라 해당 도구를 사용하여 초기 및 목표 상태, 그리고 결과 계획을 시각화할 수 있습니다. 또한 블록 작업과 같은 자체 JavaScript 시각화를 추가할 수 있습니다. 이 예제는 여기에서 찾을 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-ThinkingFastandSlowwithLLMsandPDDL_4.png)\n\nLinux, Mac 또는 Windows에서 planutils를 Docker 컨테이너에 설정하는 것은 VS Code 확장 프로그램 안내에 따라 간단합니다. 그러나 Docker 컨테이너 내의 planutils의 Flask 서버에 액세스할 수 있도록 \"호스트 네트워킹\"을 활성화해야 합니다.\n\n#Planning and ChatGPT\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChatGPT와 달리, 계획자는 항상 올바른 작업 순서를 제시할 것입니다. 특정 문제에 대해 모든 술어 및 동작을 설정하는 것이 번거로워 보일 수 있지만, 실제 환경에서는 그렇게 하는 것이 무리하지 않습니다. 복잡한 여행 일정을 만들거나 긴 구매 주문을 처리하거나 식료품을 쇼핑하거나 복잡한 조립을 실행할 인간형 로봇을 만들기 위해 소프트웨어 에이전트를 구축한다면, 사용 가능한 API 종류를 잘 파악하고 그들이 작동하기 위해 필요한 것들(전제 조건)과 성공적으로 실행된 경우 무엇이 발생할지 알고 있어야 합니다. 그런 다음 계획 실행에는 (1) 세계의 상태를 기반으로 술어 목록을 작성하는 것, (2) 계획자가 하나씩 내뱉는 모든 동작을 호출하는 것, 그리고 (3) 실행 중 환경이 변경될 경우 선택적으로 다시 계획하는 것이 필요합니다. 이 프레임워크를 구현한 후 ChatGPT를 간단히 프롬프트하여 적합한 목표를 생성시킬 수 있습니다:\n\n![이미지](/assets/img/2024-06-19-ThinkingFastandSlowwithLLMsandPDDL_5.png)\n\n이 접근 방식은 결국 동일한 한계에 부딪힐 것이지만, 적절한 목표를 설정하는 것은 \"자동차 조립\"과 같이 수천 개의 동작이 필요할 수도 있는 목표에 대한 긴 목록보다 훨씬 처리가능합니다. 또한 ChatGPT가 심볼의 이름을 어떻게 지정할지 추측했습니다. 블록월드 예시가 매우 잘 문서화되어 있어서 색상을 사용하기로 선택한 것일 수도 있습니다.\n\n# 카운터와 시간을 고려한 계획들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지는 부울 술어에 대해 이야기했습니다. PDDL의 최신 버전은 양과 시간을 지원합니다. 아래 예시에서는 카운터 변수를 사용하여 초기값이 0이며 3에 도달해야 하는 문제가 제시됩니다.\n\n```js\n(define (problem say-hello-3-times) \n(:domain counter-test)\n\n(:init\n    (= (counter) 0)\n)\n\n(:goal\n    (and \n        (= (counter) 3)\n    )\n)\n)\n```\n\n이 문제를 해결할 액션을 제공하는 도메인은 다음과 같습니다:\n\n```js\n(define (domain counter-test)\n\n(:requirements :strips :fluents)\n\n(:functions\n        (counter)\n)\n\n(:action hello-world\n    :parameters ()\n    :precondition ()\n    :effect (and \n     (increase (counter) 1)\n    )\n)\n\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ncounter 함수는 :fluents에서 사용 가능한 functions에 정의되어 있습니다. \"Hello World\"를 출력할 수 있는 hello-world 동작은 그런 다음 (counter를 1 증가시키는) 효과를 제공합니다. PDDL fluents는 시간이 지남에 따라 변할 수 있는 상태 변수입니다. 이는 단순히 카운터 이상의 가치가 있습니다. 연료 수준, 에너지 소비, 또는 이커머스 카트에 있는 승객 또는 상품의 수에 대해 추론할 수 있게 합니다.\n\n따라오신 분들께서는 FastDownward 솔버가 :fluents 요구 사항을 처리할 수 없다는 것을 알 수 있을 것입니다. 이를 처리할 수 있는 계획자는 planutils를 통해 사용할 수 있는 Expressive Numeric Heuristic Search Planner (ENHSP)입니다. planutils 도커 컨테이너에서 planutils install enhsp-2020을 사용하여 설치하거나 온라인 편집기에서 EHSP 솔버를 사용할 수 있습니다.\n\n또한 각각의 동작에 타이밍을 연관시켜 계획을 시간 영역으로 확장할 수 있습니다. 이 내용은 VS Code PDDL 익스텐션 제작자의 이 튜토리얼에서 잘 설명되어 있습니다:\n\n# 조건부 효과\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n조건부 효과는 when 키워드를 사용하여 특정 조건에 따라 다른 결과를 지정할 수 있는 기능입니다 (:conditional-effects가 필요합니다). 트럭 적재 예제에서, 상태 표현식 requires-caution은 일반적인 경우 패키지가 부서지기 쉬울 때에만 True로 설정됩니다.\n\n```js\n(define (domain truck)\n\n(:requirements :strips  :conditional-effects)\n(:predicates \n  (in ?package ?truck)  \n  (empty ?truck)\n  (requires-caution ?truck)\n  (fragile ?object)\n  )\n\n(:action load-truck\n    :parameters (?package ?truck)\n    :precondition (and \n                    (empty ?truck))\n    :effect (and\n        (in ?package ?truck)\n        (when (fragile ?package)\n            (requires-caution ?truck))))\n)\n```\n\n이 도메인과 관련된 샘플 문제는 다음과 같습니다:\n\n```js\n(define (problem load-truck) \n(:domain truck)\n\n(:objects truck77 shipment123)\n(:init \n    (empty truck77) \n    (fragile shipment123))   \n\n(:goal (and (in shipment123 truck77)))\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nFastDownward를 사용하여이 문제를 해결할 수 있습니다. 불확실성을 구현하는 방법으로 'when'에 대해 생각하는 것은 유혹적입니다. 두 가지 상호 배타적 'when' 문을 사용할 수 있으며, 이를 통해 로봇 장애물을 모델링하는 방법은 인터넷에서 흔히 볼 수 있는 예입니다. 그러나 이는 실시간으로 술어를 평가하지 않기 때문에 실제로 말이 되지 않습니다. 동작의 다른 가지에 확률을 연결할 수 있도록 허용하는 PPDDL(Probabilistic PDDL)이라는 PDDL의 확률론적 버전도 존재합니다. 그러나 PPDDL도 동일한 문제를 가지고 있으며, 확률론적 방식으로 문제를 해결할 수 있게만 해주며 마치 Markov Decision Problem과 유사합니다.\n\n# 최신 트렌드\n\n최근에는 커뮤니티가 직접 Python으로 계획 문제를 구현하도록 이동하는 추세입니다. 예를 들어, PDDL로 가져오고 내보낼 수 있는 Unified Planning 라이브러리가 있으며 최신 솔루션과 통합되어 있으며 리플래닝과 계획 복구를 지원하며 리얼타임 애플리케이션에 중요한 기능입니다. Scikit-decide는이를 한 단계 더 나아가서, 심볼릭 계획 또는 강화 학습과 함께 사용할 수있는 계획 문제에 대한 통합 접근 방식을 제공합니다. 더 많은 정보를 원하시면 ICAPS-2024에서 튜토리얼을 확인해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n상징적 계획자들의 능력은 모든 가능성을 체계적으로 평가하여 수량과 시간에 대한 제약을 고려하여 올바른 작업 순서를 찾는 데 있습니다. 이 능력은 ChatGPT와 같은 LLM의 추론 능력과는 별개입니다. 이 둘의 시스템은 결국 같은 문제를 해결하도록 밀어붙이지만, 이 이분법은 또한 인간 경험을 모델링합니다. 우리의 정신 장치가 한계에 도달하면, 우리는 손전등이나 종이와 연필 또는 다른 도구를 사용하여 알고리즘을 고수하여 문제를 해결합니다.\n\nChatGPT가 훈련을 통해 일반적인 문제 해결자가 되는 것은 그다지 가능성이 낮습니다. 우리가 모든 가능한 영역을 포괄할 수 있는 예제를 제공하는 일은 너무나도 어렵기 때문이며, LLM은 정확성을 강제할 방법을 제공하지 않습니다. 이것은 비즈니스 애플리케이션이나 현실 세계에서 운영되는 로봇과 같은 제품 시스템에게 특히 중요합니다. 불행히도, 모든 솔버가 PDDL의 모든 기능을 지원하는 것은 아니며, 올바른 PDDL 정의와 솔버의 조합을 찾는 것은 어려울 수 있습니다.\n\nChatGPT를 상징적 계획과 보완하는 것 외에도, 오픈 월드 추론과 상징적 계획을 훨씬 더 밀접하게 통합할 수 있는 큰 기회가 있습니다. 예를 들어, LLM은 상식 정보를 사용하여 한 방향으로의 검색을 다른 방향보다 우선시하는 데 사용될 수 있습니다. 마찬가지로, 자연어의 PDDL 기호 및 술어는 OWL-ViT와 같은 도구를 사용하여 실제 세계의 이미지를 감지하는 데 사용될 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-ThinkingFastandSlowwithLLMsandPDDL_0.png"},"coverImage":"/assets/img/2024-06-19-ThinkingFastandSlowwithLLMsandPDDL_0.png","tag":["Tech"],"readingTime":15},{"title":" 변화가 다가오고 있어요","description":"","date":"2024-06-19 02:48","slug":"2024-06-19-Changeiscoming","content":"\n\n## Arondite가 만들고 있는 것을 주도하는 두 가지 큰 트렌드는 무엇인가요?\n\n![이미지](/assets/img/2024-06-19-Changeiscoming_0.png)\n\n이전 글에서 Arondite를 설립한 이유에 대해 소개했습니다. 이번 게시물에서는 우리가 발견한 가장 중요한 기술 트렌드 두 가지와 그들이 제시하는 새로운 기회와 도전에 대해 설명하겠습니다.\n\n### 두 가지 큰 트렌드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것들은:\n\n1. 로봇 및 자율 시스템에서의 침체 폭발 현상입니다.\n\n2. 그 시스템을 정의하는 인공 지능의 중요성이 점점 증가하고 있습니다.\n\n로봇 및 자율 시스템의 수가 빠르게 증가하면서 데이터 및 미션 관리의 복잡도가 기하급수적으로 증가하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한편, 엣지 컴퓨팅 및 전력 효율성의 지속적인 발전은 전자 전투에 빈번히 영향을 받는 전투 공간과 교차하고 있습니다. 결과적으로, 인공지능의 시스템 능력을 정의하는 능력이 가속화되고 있습니다.\n\nAI 기술의 발전이 로봇 및 자율 시스템의 영역으로 흘러들면서, 이러한 시스템들이 더욱 성능이 우수하고 신뢰성이 높아지고 있습니다. 이에 따라 이러한 시스템들은 더욱 가치가 있어지고 이미 진행 중인 이러한 시스템의 폭발적인 성장을 가속화시킵니다.\n\n이 두 가지 추세가 결합함으로써 지수적 변화가 발생하고, 컴퓨팅 분야에서 1970년대에 본 변화와 유사한 로봇 및 자율 시스템의 민주화가 바로 앞으로 다가오고 있음을 의미합니다.\n\n1960년대 후반에는 컴퓨터가 크고 비싼 장비였습니다. 그래서 군사 본부, 학술 연구소 및 대규모 기술 기업으로 한정되었습니다. 그러나 1970년대 후반에는 패러다임이 바뀌었습니다. 마찬가지로, 저희는 로봇 및 자율 시스템이 자신들의 개인용 컴퓨터 시대를 맞이하고 있다고 믿습니다. 컴퓨팅의 민주화가 새로운 기회와 도전을 가져왔던 것처럼, 로봇 및 자율 시스템의 민주화도 마찬가지입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기회와 도전\n\n이 변경의 결과가 깊은 영향을 줄 것으로 믿습니다. 사회는 AI 및 자율 시스템의 힘을 활용하면서도 보안, 유연성 및 인간의 통제 원칙을 희생하지 않을 것입니다. Arondite에서는 AI 이해와 통제, 데이터 복잡성 및 동적 팀워크라는 해결해야 할 세 가지 큰 도전을 보고 있습니다.\n\nAI 이해와 통제. 로봇 및 자율 시스템의 보다 광범위한 사용은 인류에게 새로운 가치를 제공하리라 약속하지만, 방어 분야에서는 도전도 가져옵니다. 우리가 구축한 AI는 감사 가능하고 이해 가능하며 검사 가능해야 합니다. 모델은 보호되어야 하며 그 유래를 이해해야 합니다. AI가 다양한 시스템에서 보다 널리 배포됨에 따라, 우리는 각각에 의미 있는 인간의 통제를 유지하고 제조업체가 다른 제조업체의 혼합 플릿을 통하여 심지어 표준을 강제로 적용해야 할 것입니다. 민주국가의 시민으로서, 우리는 우리 윤리가 우리의 공학적 결정에 스며들어가도록 보장해야 합니다. Arondite에서는 이 분야에서 리더십을 제공하는 것이 우리의 주요 책무 중 하나라고 믿고 있습니다.\n\n데이터 복잡성과 보안. 방어는 또한 데이터 관리와 임무 관리에서의 신생 복잡성에 맞서야 하며, 고객이 데이터와 AI를 활용하면서도 보안을 희생하지 않도록 해야 합니다. 이로 인해 몇 가지 중요한 질문이 제기됩니다. 어떻게 점점 더 복잡한 시스템의 혼합물 간에 쉬운 협업을 가능하게 할 수 있을까요? 얼마나 보안을 저해하지 않고 동맹국과 기업이 AI를 함께 개발할 수 있을까요? 어떻게 적절한 AI 모델이 올바른 위치에 배치되는지 보장할 수 있을까요? AI가 예상대로 작동 중이며 실제 세계 피드백을 자동으로 캡처하여 모델 신뢰성과 성능을 향상시킬 수 있을까요? 저희에게는 이러한 종류의 기능이 선택사양이 아니라 문제를 해결하는 중요한 부분임을 보증합니다. 이 없이는 복잡한 데이터 환경에서 신뢰를 유지하거나 고객에게 조직 전체로 제품을 안전하게 확장할 자신감을 줄 수 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**동적 팀 구성.** 변화 속도가 높아질수록, 인간-기계 팀도 동적으로 형성되고 재구성되어야 합니다. 로봇 시스템이 협업하는 것이 인간들이 협업하는 것만큼 쉬워져야 한다고 믿습니다. 이는 센서, 로봇 또는 자율 시스템과 연결해야 하는 사용자가 세계를 모델링할 수 있는 완전히 새로운 방법을 구축하는 것을 의미합니다. 사용자는 그런 다음 예기치 못한 작업을 수행하기 위해 다양한 기계들을 유연한 팀으로 결합할 수 있어야 합니다.\n\n이러한 도전에 직면하여, 우리는 인공지능으로 정의된 로봇 시스템들의 집단적인 능력을 발휘할 수 있도록 도와줄 것입니다. 결과적으로, 이는 국방을 포함한 기관들이 보안, 유연성 및 인간의 통제에 영향을 주지 않으면서 로봇 및 자율 시스템을 결합해 활용할 수 있게 만들 것입니다.\n\n**저자**\n\n윌 블라이스, 아론다이트의 공동 창립자이자 CEO","ogImage":{"url":"/assets/img/2024-06-19-Changeiscoming_0.png"},"coverImage":"/assets/img/2024-06-19-Changeiscoming_0.png","tag":["Tech"],"readingTime":3},{"title":"음성 쓰레기 분류 라즈베리 파이와 티처블 머신","description":"","date":"2024-06-19 02:46","slug":"2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine","content":"\n\n\n![이미지](/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_0.png)\n\n전 세계적으로 매년 20억 톤의 가정 폐기물이 생산됩니다. 이것은 우리 환경에 무거운 부담을 줍니다. 매립지와 환경에 끝내 찌르는 폐기물을 줄이기 위해 전 세계 주민들은 가정 폐기물을 분리 수거해야 합니다. 그리고 쓰레기 분리는 큰 비즈니스입니다. Fortunebusinessinsights.com에 따르면, 2019년 글로벌 폐기물 분류 장비 시장은 약 7억 달러였습니다. 그리고 2027년에는 18억 달러에 이를 것으로 예상됩니다.\n\n![이미지](/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_1.png)\n\n하지만 유감스럽게도 중국에서는 쓰레기 분리 시스템이 효과적이지 않습니다. 정책이 약하게 시행되며, 위반에 대한 처벌이 거의 없습니다. 더 나쁜 것은 쓰레기 분류가 혼란스럽습니다. 제 건물에는 섬유, 재활용, 음식물, 잔여 폐기물을 각각 수집하는 다섯 개의 용기가 있습니다. 각 라벨 아래에는 예시의 짧은 목록이 있습니다. 예를 들어, 와인과 플라스틱 병은 \"재활용\" 용기로 가야 합니다. 그리고 옷과 가방은 \"섬유\"에 속합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 목록은 모든 종류의 쓰레기를 다 다루기에는 너무 짧습니다. 나는 종종 우드 또는 금속판이 어느 컨테이너에 들어가야 하는지 모르기 때문에 컨테이너 앞에서 어리둥절해졌어요. 대부분의 경우, 결국 쓰레기는 \"잔여 폐기물\" 농푸에 들어갑니다. 이런 경우에는 누군가가 올바른 컨테이너를 알려주면 좋겠죠? 그리고 나는 Freethink의 이 YouTube 비디오를 보았습니다.\n\n이 비디오는 인공지능이 도와주는 로봇 쓰레기 분류 시스템을 보여줍니다. 이 시스템은 컴퓨터 비전을 사용하여 다양한 종류의 쓰레기를 인식한 다음 로봇 팔을 사용하여 분리합니다. 요즘에는 인공지능 컴퓨터 비전이 매우 정확합니다. 우리는 쓰레기 분류 문제에 대처하기 위해 완전히 활용해야 합니다.\n\n이 비디오에 영감을 받아 나는 집에 내 쓰레기 분류 라즈베리 파이를 만들었어요 (그림 1 및 비디오 2).\n\n시스템은 라즈베리 파이, 비디오 카메라 및 스피커로 구성되어 있습니다. 카메라가 쓰레기 물체를 \"보게 되면\", 라즈베리 파이는 스피커를 통해 쓰레기 카테고리를 말합니다. 컴퓨터 비전 모델은 Google의 Teachable Machine에서 훈련되었습니다. 이 기사에서 시스템을 설명합니다. 이 프로젝트의 코드는 여기 내 GitHub 저장소에 호스팅되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 Teachable Machine 모델 파일은 제 Google 드라이브에 호스팅되어 있습니다.\n\n# 1. 구성 요소\n\n이 프로젝트에서는 Raspberry Pi 4를 사용했습니다. RAM이 4GB이고 저장 용량이 64GB입니다. 또한 Pi에 USB 비디오 카메라와 USB 스피커를 연결했습니다 (그림 1 및 3). \n\n![이미지](/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n나는 Google의 Teachable Machine 프로젝트를 데스크톱 컴퓨터에서 작업했어. 그리고 나는 모델과 다른 파일을 내 집 네트워크를 통해 Raspberry Pi로 전송했어.\n\n# 2. Teachable Machine에서 쓰레기 분류 모델 훈련하기\n\n이 프로젝트에서, 나는 재활용 가능한(recyclable), 직물(textiles), 그리고 비어 있는(empty) 세 가지 클래스로 모델을 훈련했어. 나는 kaggle.com에서 보틀과 컵 데이터세트 그리고 의류 데이터세트(이 문서에서 설명함)를 발견했어. 나는 또한 처음 두 카테고리에 내 사진 몇 장을 추가했어. 비어 있는 카테고리에는 다양한 종류의 벽 사진을 포함했어. 이 마지막 클래스는 물체가 없을 때 시스템을 대기 상태로 만들고 싶기 때문에 필수적이야.\n\nTeachable Machine 웹사이트로 이동해서 이미지 프로젝트를 시작해봐. 내가 언급한 세 가지 클래스를 생성해. 각 클래스에 사진을 업로드해. 또는 위에서 제공한 내 공유 모델 파일을 열어볼 수도 있어.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_3.png)\n\n이제 Train Model 버튼을 클릭하여 훈련 프로세스를 시작할 수 있습니다. 훈련 이후 모델을 몇 장의 사진 또는 카메라를 사용하여 테스트할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_4.png)\n\n결과에 만족하셨다면 Export Model 버튼을 클릭하세요. Tensorflow 탭을 선택하고 Savedmodel 옵션을 선택한 후 Download my model 버튼을 클릭하세요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_5.png)\n\n당신의 브라우저가 모델이 준비되면 다운로드가 시작됩니다.\n\n# 3. 앱 작성\n\n호스트 컴퓨터에서 pi_garbage_classifier라는 폴더를 만듭니다. 다운로드한 파일을 프로젝트 폴더에 압축해제합니다. 이제 app.py라는 파일을 만들어 각 부분을 조합할 것입니다. 이것은 Teachable Machine의 코드 스니펫에 기반합니다 (Figure 6). 그러나 놀랍게도 샘플 코드는 잘못되었고 오도독도 못하다. keras_model.h5를 열려고 시도했지만 다운로드한 모델은 model.savedmodel이라는 이름입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기는 app.py의 코드입니다.\n\n```python\nimport cv2\nimport numpy as np\nfrom keras.models import load_model\nimport os\nimport time\nimport re\nimport sys\n\npathname = os.path.dirname(sys.argv[0])\n\nprint('sys.argv[0] =', sys.argv[0], \"pathname\", pathname)    \n\n# 모델 불러오기\nmodel = load_model(f'{pathname}/model.savedmodel')\n\n# 레이블을 labels.txt 파일에서 가져옵니다. 이것은 나중에 사용됩니다.\nlabels = open(f'{pathname}/labels.txt', 'r').readlines()\n\nwhile True:\n    time.sleep(5)\n\n    # CAMERA는 컴퓨터의 기본 카메라에 따라 0 또는 1이 될 수 있습니다.\n    camera = cv2.VideoCapture(0)\n    # 웹캠 이미지 불러오기\n    ret, image = camera.read()\n    camera.release()\n    \n    # 이미지를 (224-높이,224-너비) 픽셀로 크기 조정합니다.\n    image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_AREA)\n    \n    # 이미지를 윈도우에 표시합니다.\n    \n    image = np.asarray(image, dtype=np.float32).reshape(1, 224, 224, 3)\n    \n    image = (image / 127.5) - 1\n    \n    probabilities = model.predict(image)\n    \n    max_prob = np.max(probabilities)\n    label = re.sub(r'\\d+\\s+', '', labels[np.argmax(probabilities)]).strip()\n\n    print (label, f\"proba: {max_prob}\")\n    \n    if label != \"empty\" and max_prob \u003e 0.9:\n        os.system(f\"festival --tts {pathname}/voice/{label}.txt\")\n\n    keyboard_input = cv2.waitKey(1)\n    \n    if keyboard_input == 27:\n        break\n```\n\n내 코드에 메인 루프에 sleep 함수를 추가했습니다. 이를 통해 시스템은 5초마다 이미지를 캡처합니다. OpenCV는 캡처된 프레임을 버퍼링하기 때문에 각 반복에서 카메라를 초기화해야 합니다. 코드는 또한 예측 확률이 0.9보다 높으면 쓰레기 카테고리를 발표하기 위해 페스티벌 유틸리티를 사용합니다. 페스티벌은 텍스트 음성 변환 유틸리티입니다. 텍스트 파일을 읽어 내용을 읽어 줄 수 있습니다. 이 프로젝트에서는 voice라는 폴더를 만들고 발표 파일을 넣었습니다. 각 발표 파일에는 카테고리 이름만 작성했습니다. 읽기 편의를 위해 파일 내용을 수정하여 발표를 사용자 정의할 수 있습니다. \n\n그래서 최종적으로 폴더 구조는 다음과 같아야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_6.png\" /\u003e\n\n# 4. Raspberry Pi 설정하기\n\n저는 learn.adafruit.com에 있는 훌륭한 설정 안내서를 따랐어요. 먼저, Raspberry Pi Imager를 사용하여 마이크로 SD 카드에 Raspberry Pi OS (64비트)를 기록하세요. 고급 옵션에서 SSH를 활성화하고 사용자 이름과 암호를 설정하세요.\n\n카드를 Pi에 넣고 전원을 켜세요. 네트워크에 연결하고 다음 SSH 명령어로 로그인하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 만약 콘솔에서 \"REMOTE HOST IDENTIFICATION HAS CHANGED!\" 라고 나오면\n# 다음 명령어를 실행하여 리셋하세요\n# ssh-keygen -R raspberrypi.local\n\nssh pi@raspberrypi.local\n```\n\n라즈베리 파이 쉘이 준비되면, 아래 명령어를 하나씩 사용하여 필요한 소프트웨어를 설치하세요.\n\n```js\n# 라즈베리 파이에서 실행하세요\n\nsudo apt update\nsudo apt upgrade -y\n\n# 선택 사항\n# sudo apt install -y python3-pip\n\npip3 install --upgrade setuptools\n\npip3 install Pillow==9.2.0\n\npip install opencv-python\n\nRELEASE=https://github.com/PINTO0309/Tensorflow-bin/releases/download/v2.10.0/tensorflow-2.10.0-cp310-none-linux_aarch64.whl\n\nCPVER=$(python --version | grep -Eo '3\\.[0-9]{1,2}' | tr -d '.')\n\npip install $(echo \"$RELEASE\" | sed -e \"s/cp[0-9]\\{3\\}/CP$CPVER/g\")\n\nsudo apt install -y festival\n\n# 선택 사항\n# sudo reboot\n\nmkdir ~/pi_garbage_classifier\n```\n\n위의 마지막 명령어는 라즈베리 파이의 홈 디렉토리에 pi_garbage_classifier 라는 폴더를 만듭니다. 프로젝트 파일을 데스크톱 컴퓨터에서 이 폴더로 전송하려면 다음 명령어를 사용하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 호스트 데스크톱에서\n\ncd [your_desktop_pi_garbage_classifier_path]\nscp -r ./* pi@raspberrypi.local:~/pi_garbage_classifier/\n\n\n파일을 복사한 후에는 다음 명령어로 앱을 테스트하세요:\n\n\n# 라즈베리 파이에서\n\npython /home/pi/pi_garbage_classifier/app.py\n\n\n앱이 시작하는 데 몇 초 정도 걸립니다. 카메라 램프가 깜박일 때, 카메라 앞에 다양한 물체를 두어 시스템을 테스트할 수 있습니다. 물체의 클래스를 알리고 콘솔에 다음 출력이 나타날 것입니다 (그림 8).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_7.png)\n\n앱은 종료할 때까지 계속 실행될 거에요.\n\n# 5. 라즈베리 파이(Pi) 배포\n\n이제 시스템을 독립적으로 만들어 배포할 시간입니다. 앱을 Pi의 자동 시작 파일에 추가해야 합니다. rc.local과 cron 두 가지 방법을 시도해봤는데, 이 방법들은 데스크톱을 시작하기 전에 앱을 시작하기 때문에 USB 스피커를 활용할 수 없었어요. 그래서 이 포스트에서 해결책을 찾았어요. 시스템 자동 시작 파일에 관련된 것이죠. 따라서 이 명령어를 실행하여 파일을 편집해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsudo nano /etc/xdg/lxsession/LXDE-pi/autostart\n```\n\n다음 줄을 파일에 추가하세요.\n\n```js\n@python /home/pi/pi_garbage_classifier/app.py\n```\n\n이렇게 autostart 파일이 보이게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_8.png\" /\u003e\n\n프로그램을 종료하려면 Ctrl+X를 누르고 동일한 파일에 변경 사항을 저장하려면 Y를 누르세요. 다음 명령어를 사용하여 장치를 다시 부팅하세요.\n\n```js\nsudo reboot\n```\n\n부팅 후 시스템은 자동으로 작업을 수행해야 합니다. 이제 네트워크 케이블을 제거하고 Pi를 원하는 곳에 배치할 수 있습니다 (비디오 2). 이제 쓰레기의 목적지에 대해 확실하지 않을 때에는 항상 상담할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n이 기사에서는 시범용 쓰레기 분류 시스템을 구축하는 방법을 보여드렸어요. 설정하는 데 약 30분 가량 밖에 걸리지 않았죠. 아직까지는 두 가지 종류의 쓰레기와 전체 가정 쓰레기의 일부만 인식할 수 있지만 필수 구성 요소들은 모두 갖추고 있어요. 모델에 더 많은 객체와 클래스를 추가할 수 있습니다. 또한 이 프로젝트는 일종의 프레임워크입니다. 다시 말해, 새로운 모델을 훈련하고 Pi에 모델 파일을 교체하여 식물 분류 시스템이나 물체 인식기와 같이 완전히 다른 것으로 변환할 수 있어요.\n\nGoogle의 티처블 머신은 정말 우리에게 빛이 되어줬어요. 이전에는 서로 다른 화가들의 그림을 분석하는 데 사용했었죠. 물론 PyTorch나 Keras로 직접 컴퓨터 비전 모델을 사용자화할 수도 있어요. 하지만 그런 작업은 상당한 시간과 노하우를 요구할 수 있어요. 반면, 티처블 머신은 이 모든 것을 아주 간단하게 만들어줘요. 몇 번의 클릭만으로 과연성 있는 모델을 쉽게 얻을 수 있어요. 더불어 모델을 확장하고 수정하는 것도 쉬워요.\n\n하지만 생각해 볼 수 있는 작은 단점도 있어요. 티처블 머신은 인터페이스에서 모델 성능을 보여주지 않아요. 따라서 우리는 모델이 얼마나 잘 작동할지를 알 방법이 없어요. 더 많은 클래스를 추가할수록 모델 성능이 떨어지기 때문에 우리는 모델을 충분히 신뢰하고 배포하기 전에 모델의 정확도를 측정해야 해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 프로젝트를 시도해 보라고 권장합니다. 그리고 피드백을 주시면 좋을 것 같아요!","ogImage":{"url":"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_0.png"},"coverImage":"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_0.png","tag":["Tech"],"readingTime":9},{"title":"라즈베리 파이의 AI 키트는 얼마나 좋은가요","description":"","date":"2024-06-19 02:45","slug":"2024-06-19-HowgoodisRaspberryPisAIKit","content":"\n\n![image](/assets/img/2024-06-19-HowgoodisRaspberryPisAIKit_0.png)\n\n라즈베리 파이가 머신 러닝 기능을 갖춘 싱글 보드 컴퓨터 AI Kit을 출시했습니다. 최근 IPO 출시를 촉진하기 위한 것일 수도 있지만, 성능 데이터를 보면 AI 엣지 컴퓨팅 시장을 뒤흔들 것으로 보입니다.\n\n이 AI 키트는 Hailo-8L 가속기와 라즈베리 파이의 자체 M.2 HAT+ 보드로 구동됩니다.\n\n어쨌든, 이 $70의 AI 가속기는 정말 좋은 품질입니다. 아래 섹션에서 제시된 것처럼 임베디드 애플리케이션을 위한 강력한 가격을 얻을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Hailo-8L 소개:\n\n라즈베리 파이는 액셀러레이터 제공 업체로 Hailo를 선택했습니다. Hailo-8L은 최근 발표된 Hailo-8 칩의 간소화된 버전으로, 26TOPS를 제공합니다. Hailo-8L은 칭찬받을 만한 13TOPS를 제공하여 라즈베리 파이에 필요한 AI 성능을 제공합니다.\n\n![이미지](/assets/img/2024-06-19-HowgoodisRaspberryPisAIKit_1.png)\n\n최근 몇 주 사이에 TOPS 레코드가 빠르게 깎이고 있다는 글을 작성했습니다. 최근 AMD가 50TOPS로 TOPS 차트를 석권하고 있어, 13TOPS는 낮아 보일 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그런데 중요한 점을 고려해야 합니다. 이 내용은 임베디드 환경에서 사용될 것이며, 13TOPS는 컴퓨터 비전 분야의 엣지 애플리케이션에 큰 이점을 제공할 것입니다. 또한, 작성 시점에서 RISC-V Linux 랩탑은 단지 2TOPS를 제공합니다.\n\nHailo는 특히 저전력 엣지 AI 애플리케이션에 중점을 둔다는 사실을 알아두세요. 그들은 심지어 NVIDIA를 훌륭하게 이길 수 있습니다. 아래는 NVIDIA의 Jetson Nano 및 Xavier NX와 비교한 Hailo의 성능입니다.\n\n![Hailo Performance Comparison](/assets/img/2024-06-19-HowgoodisRaspberryPisAIKit_2.png)\n\n이 칩은 TensorFlow, TensorFlow Lite, Keras, PyTorch 및 ONNX를 포함한 다양한 AI 프레임워크를 널리 지원합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n상기 숫자 및 성능 이점을 고려하면, Hailo와 함께 Raspberry Pi를 선택하는 것이 명백해 보입니다.\n\n## Raspberry Pi 인공지능 키트 아키텍처 및 디자인\n\n현재 이 AI 키트는 x86 및 Arm 호스트 아키텍처를 모두 지원합니다.\n\nRaspberry Pi CEO인 Eben은 Apple 및 Qualcomm SoC의 AI PC를 위해 최근 출시된 통합 NPU와는 달리 별도의 가속기 보드를 계획적으로 준비했다고 전했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Raspberry Pi AI Kit](/assets/img/2024-06-19-HowgoodisRaspberryPisAIKit_3.png)\n\n라즈베리 파이는 이미 저렴한 40나노미터에서 IO 기능을 사용하여 분리된 아키텍처를 갖고 있습니다. CPU 및 GPU는 16나노미터에 있습니다. 코어는 16나노미터에 있지만 NPU를 추가하면 면적이 커져 비용이 많이 드는 문제가 발생할 수 있습니다.\n\n비 AI 응용 프로그램을 위해 라즈베리 파이 5를 사용하려는 사용자도 있을텐데요 ;-) 이들은 불필요한 NPU 다이 면적 비용을 위해 더 많은 돈을 지불해야 합니다. 이제 선택권이 생겼고 하드웨어 엔지니어들은 선택권을 좋아합니다. . .\n\n라즈베리 파이 3 버전 자체는 AI 응용 프로그램에 사용되었지만 AI 워크로드는 클라우드에서 실행되었습니다. 이 새로운 라즈베리 파이 5는 Hailo-8L을 사용하여 에지에서 작은 모델과 최적화된 LLM(로컬 모델 매니저)을 직접 수행할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 라즈베리 파이 AI 키트의 현재 상태:\n\n이 라즈베리 파이는 PCIe 2.0 인터페이스를 통해 Hailo-8L 가속기와 통신합니다. 그리고 PCIe 2.0은 라즈베리 파이 5 버전에만 있습니다. 따라서 AI 기능을 시도하려면 보드를 교체해야 합니다. 하지만, 심지어 라즈베리 파이 5의 M.2 HAT+도 약 80달러 정도에 구매할 수 있습니다.\n\n이 키트에는 Hailo-8L의 열을 효과적으로 방출하기 위한 사전 부착된 열 패드가 포함되어 있습니다. Hailo-8L은 소비전력은 적지만 상당히 더울 수 있습니다.\n\n라즈베리 파이는 이것이 많은 메이커 커뮤니티에게 새로운 영역임을 이해합니다. 따라서 사용자가 AI 기능을 개발하는 데 필요한 데모 및 소프트웨어 유틸리티를 제공할 계획입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하드웨어는 그대로 작동하지만 중요한 점은 소프트웨어가 아직 완전하지 않다는 것입니다. 그러나 몇 달 안에 상황이 변할 것입니다.\n\nTom's Hardware 팀이 리뷰 키트를 받았을 때 몇 가지 문제가 발생했습니다. Raspberry Pi OS 업데이트가 AI 기능 일부를 활성화할 예정이라는 것을 알았습니다. 현재 사용이 제한되지만, 이미 teddy bear를 포함한 이미지를 감지할 수 있었습니다. 따라서 현재는 약간의 노력이 필요합니다.\n\n이 데모는 속도 측정을 수행하지 않았지만, 팀은 몇 가지가 더 빠르다고 \"느꼈다\"고 언급했습니다. 미래에 Raspberry Pi가 필요한 경우에는 우리 자신의 모델을 실행할 수 있다고 언급했다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 사용 사례:\n\n라즈베리 파이 \u0026 헤일로는 이미지 인식을 포함한 컴퓨터 비전 하드웨어의 저전력 장점이 엣지 노드에서 중요하다고 주장합니다.\n\n이 회사는 또한 게임에 AI 기능을 추가하는 데 적합한 몇 가지 모델을 보유하고 있다고 합니다. — 음, 인간들...\n\n![이미지](/assets/img/2024-06-19-HowgoodisRaspberryPisAIKit_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nWindows 11이 라즈베리 파이가 목표로 하는 대상이 아니라서, 이 키트는 Microsoft Copilot 소프트웨어용이 아니라는데요. 그러나 일부 기능이 작동할 수 있는 가능성은 배제되지 않습니다. Copilot+ PC에 요구되는 40TOPS 중 13TOPS는 거의 1/3이므로 어떤 부분은 실제로 작동할 것으로 예상됩니다.\n\n## 결론\n\n양 회사 모두 메이커 커뮤니티가 자체 문제를 해결할 독특한 사용 사례를 개발할 것으로 기대하고 있습니다. 저 또한 라즈베리 파이 커뮤니티의 힘으로 인해 차이가 날 것이라고 믿습니다. 교실 프로젝트 외에도 많은 로봇 응용 프로그램이 새롭게 발견된 13TOPS AI 기능을 사용하기 시작할 것입니다.\n\n보다 원활한 플랫폼 작동을 위해 더 많은 라즈베리 파이 소프트웨어 업데이트를 기다려야 합니다. 흥미로운 사실은 라즈베리 파이 CEO가 Hailo와의 협력이 이로써 끝나지 않는다고 언급했다는 것입니다. . . .\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n에지 AI의 미래가 무엇을 가지고 있는지 정말 흥미로울 것입니다.\n\n더 많은 혁신 및 딥 테크 이야기를 보려면 박수를 보내 주시고 저를 팔로우해주세요. 매주 발송되는 혁신 스냅 소식지를 구독하여 혁신을 영감받으세요.\n\n![2024-06-19 How good is Raspberry Pi's AI Kit](/assets/img/2024-06-19-HowgoodisRaspberryPisAIKit_6.png)\n\n이 이야기는 Generative AI에 게재되었습니다. LinkedIn에서 저희와 연결하고 최신 AI 이야기에 대한 소식을 받으려면 Zeniteq를 팔로우하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최신 생성적 AI 뉴스와 업데이트를 받으려면 뉴스레터를 구독해주세요. 함께 AI의 미래를 만들어봐요!\n\n`\u003cimg src=\"/assets/img/2024-06-19-HowgoodisRaspberryPisAIKit_7.png\" /\u003e`","ogImage":{"url":"/assets/img/2024-06-19-HowgoodisRaspberryPisAIKit_0.png"},"coverImage":"/assets/img/2024-06-19-HowgoodisRaspberryPisAIKit_0.png","tag":["Tech"],"readingTime":5},{"title":"시그널 인텔리전스, 라즈베리 파이","description":"","date":"2024-06-19 02:43","slug":"2024-06-19-SignalsIntelligenceTheRaspberryPi","content":"\n\n\n![이미지](/assets/img/2024-06-19-SignalsIntelligenceTheRaspberryPi_0.png)\n\n라즈베리파이는 소형이면서도 효과적인 SIGINT 연구 스테이션을 제공합니다.\n\n미디엄 회원이 아니라면 substack를 통해 무료로 읽을 수 있습니다.\n\n신호 분석을 초보자의 관점에서 시작할 때, 시작하기가 어렵고 다루어야 할 정보가 많을 수 있습니다. 하드웨어 기반 시스템, 웹 기반 시스템이 있으며 무엇이라도 라디오 스펙트럼에서 보고 있는 것이죠?! 이렇게 하면 새로운 프로젝트가 재미있고 흥미로운 것으로 변할 수 있는데, 여기에 참여하는 동안 인내력과 동기부여를 빼앗기기도 합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-SignalsIntelligenceTheRaspberryPi_1.png\" /\u003e\n\n안녕하세요! 오늘은 주요 싱글 보드 컴퓨터를 사용하여 몇 가지 선택된 하드웨어와 결합하여 기본 SIGINT 수신 스테이션을 만드는 방법에 대해 이야기하려고 합니다. 이를 사용하여 항공기, 지역 신호, ISM 신호 및 올바른 안테나를 사용하면 우주 신호까지 감지할 수 있습니다. 기본 Raspbian OS를 사용하는 대신 전용 SIGINT에 포커싱된 OS를 사용할 것입니다. 하드웨어를 준비하고 시작해 봅시다!\n\n\u003cimg src=\"/assets/img/2024-06-19-SignalsIntelligenceTheRaspberryPi_2.png\" /\u003e\n\n이 프로젝트를 완료하려면 SD 카드에 플래싱하는 경험과 기본적인 Linux 경험이 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 드래곤 소개\n\n코로나 대유행 중에 만들어진 Dragon OS는 라디오 중심의 펜 테스트 및 분석 배포판으로, Kali Linux나 Parrot OS와 유사합니다. 다양한 신호 프로토콜과 하드웨어와 함께 작동할 수 있으며, Bluetooth, Wi-Fi, ISM, 전화 및 대부분의 라디오 스펙트럼을 싼 가격의 장치인 RTL-SDR 또는 블레이드RF 또는 hackRF 시스템과 같은 전용 SDR 장치를 사용하여 즉시 사용할 수 있도록 설정되어 있습니다.\n\n신호 기반 배포판 중 처음은 아니지만 쉽게 시작할 수 있는 것 중 하나이며 미리 설치된 다양한 도구와 표준 패키지 관리자로 초보자에게 매우 친숙합니다. 라즈베리 파이와 결합하면 매우 멋진 장비가 됩니다.\n\n# 요구 사항\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n라즈베리 파이 보드\n\n주변기기 (전원 공급 장치, SD 카드, 키보드 및 모니터 또는 SSH 액세스를 위한 장치)\n\nRTL-SDR 동글\n\n모니터 모드 Wi-Fi 카드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 운영 체제\n\n기본 Raspbian 배포본은 다양한 용도에 좋은 OS이지만, SIGINT 용도로는 다소 부족합니다. 적절히 구성할 수는 있지만, 이에는 추가 시간이 소요될 것입니다. 효율성이 중요한 우리에게 시간은 소중하니 이제 다른 옵션이 있습니다. 우리는 DragonOS를 사용할 것입니다. 이 OS는 라디오 스펙트럼을 탐색하는데 도움이 되는 다양한 유용한 도구로 구성되어 있어서 불필요한 노력 없이 사용할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-SignalsIntelligenceTheRaspberryPi_3.png)\n\n또한 다양한 소프트웨어 정의 라디오를 위한 사전 구성된 드라이버가 많이 제공됩니다. 이에는 RTL-SDR, bladeRF, hackRF 등이 포함되어 있습니다. 이는 대부분의 장치가 일반적으로 플러그 앤 플레이가 가능하다는 것을 의미합니다. 초보자에게 이는 이상적입니다. 대부분의 프로젝트를 쉽게 구성하도록 만들어주어 작업에 집중할 수 있게 해주며, 필요한 드라이버를 찾느라 고생할 필요가 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 설정하기\n\n우리의 Pi를 적절한 이미지로 구성하려면, 먼저 해당 이미지를 다운로드한 다음 SD 카드에 플래시해야 합니다. 하지만 먼저 OS 이미지가 필요합니다.\n\n최신 버전의 Dragon이 SourceForge 리포지토리에서 다운로드할 수 있습니다. 다음 링크에서 찾을 수 있습니다.\n\n![Image](/assets/img/2024-06-19-SignalsIntelligenceTheRaspberryPi_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 번 Sourceforge 페이지에 접속하면, 가장 최근 업데이트 날짜를 확인하여 최신 버전을 받고 있는지 확인할 수 있습니다. 우리가 사용하는 Raspberry Pi 모델의 올바른 이미지를 받는지도 확인해야 합니다. 다양한 모델 간에 소량의 차이가 있기 때문입니다. 그런 다음, 확인이 완료되면 다운로드 탭을 눌러 로컬에서 이용 가능하게 대기하면 됩니다.\n\n![Signals Intelligence The Raspberry Pi_5](/assets/img/2024-06-19-SignalsIntelligenceTheRaspberryPi_5.png)\n\n로컬 머신에 설치되면, SD 카드에 플래시가 필요합니다. 이를 위해 다른 프로젝트에서와 같이 Balena Etcher를 사용할 수 있습니다. 하지만 먼저 다운로드의 무결성을 확인하기 위해 MD5 합을 비교해야 합니다. Linux 사용자라면 다음 명령어로 터미널에서 이를 할 수 있습니다.\n\n```js\nmd5sum /다운로드한/파일의/경로\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n무결성을 확인하려면 표시된 출력물을 Sourceforge 웹사이트에 저장된 출력물과 비교해야 합니다. 일치하면 파일이 정상이며 준비된 상태입니다.\n\n최종 단계는 우리의 Raspberry Pi와 함께 사용할 수 있도록 OS를 SD 카드에 플래시하는 것입니다. 이는 상당히 간단한 단계이므로 자세히 설명하지는 않겠습니다. 그러나 문제가 발생하면 Balena 웹사이트를 사용하여 문제 해결할 수 있습니다.\n\n플래시가 완료되면 SD 카드를 Pi에 넣고 전원을 공급하고 부팅이 완료될 때까지 기다리면 됩니다. 설치가 올바르게 되었다면 데스크톱으로 부팅할 수 있어야 합니다. 이 시점에서 기본 사용자/비밀번호 조합을 변경했는지 확인해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n터미널에 들어가서 ls 명령어를 사용하여 사용 가능한 파일/프로그램을 검색해볼 수 있어요.\n\n```js\nls -la\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-SignalsIntelligenceTheRaspberryPi_7.png\" /\u003e\n\n# 마지막으로 함께 하고 싶은 말씀\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 요즘 만들어놓은 Raspberry Pi 보드를 활용해 기본적이면서도 광대한 신호 분석 스테이션으로 변신시켰습니다. 이제 여러분의 라디오 수신기 및 기본 센서 패키지의 범위에만 한정되어 있는 프로젝트입니다.\n\nDragon과 함께 미리 설치된 소프트웨어의 폭넓은 범위를 고려했을 때, 설치는 쉽지만 다음 단계가 무엇인지 궁금할 수 있습니다. 그러나 우리는 앞으로 몇 달 동안 Dragon OS에서 사용 가능한 일부 도구를 통해 작동하는 자습서를 통해 도움을 드릴 예정입니다.\n\nRaspberry Pi를 홈 빌트 프로젝트에 사용한 적이 있나요? 프로젝트 제안이 있거나 특정 주제를 다루는 자습서를 보고 싶으신가요? 댓글에 제안을 남겨주세요. 여러분의 소중한 의견을 기다리고 있습니다!\n\nMedium은 최근 일부 알고리즘 변경을 도입하여 이와 같은 기사의 발견 가능성을 개선했습니다. 이 변경 사항은 높은 품질의 콘텐츠가 보다 넓은 관객에게 도달하도록 하는 데 중요한 역할을 합니다. 여러분의 참여가 이를 실현하는 데 결정적인 역할을 하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 이 기사가 유익하고 정보가 많거나 재미있었다면, 우리는 친절히 당신의 지원을 보여주시도록 권장합니다. 이 기사에 박수를 보내면 작가에게 그들의 작품이 감사히 받아들여지고, 또한 그것을 이용할 수 있는 다른 사람들에게 그 가시성을 높이는 데 도움이 됩니다.\n\n🌟 이 기사를 즐겼다면? 우리의 작품을 지원하고 커뮤니티에 참여해보세요! 🌟\n\n💙 Ko-fi에서 저를 지원해주세요: Investigator515\n\n📢 독점 업데이트를 위해 OSINT 텔레그램 채널에 참여하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n📢 최신 이벤트 정보를 받으시려면 우리의 크립토 텔레그램을 팔로우해주세요\n\n🐦 트위터에서도 팔로우해주세요\n\n🟦 블루스카이에도 참여하실 수 있습니다!\n\n🔗 추천하는 기사들:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 뭐야 이건?! 로켓 엔진\n- OSINT 수사관을 위한 셀프 케어 \u0026 회복 가이드\n\n✉️ 이와 같은 콘텐츠를 더 보고 싶으세요? 이메일 업데이트 신청하세요","ogImage":{"url":"/assets/img/2024-06-19-SignalsIntelligenceTheRaspberryPi_0.png"},"coverImage":"/assets/img/2024-06-19-SignalsIntelligenceTheRaspberryPi_0.png","tag":["Tech"],"readingTime":5}],"page":"95","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"95"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>