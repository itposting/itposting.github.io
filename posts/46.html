<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/46" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/46" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="ROS 2 Humble Hawksbill에서 TurtleBot3 시뮬레이션 설정하기" href="/post/2024-06-20-SettingUpTurtleBot3SimulationinROS2HumbleHawksbill"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ROS 2 Humble Hawksbill에서 TurtleBot3 시뮬레이션 설정하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-SettingUpTurtleBot3SimulationinROS2HumbleHawksbill_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ROS 2 Humble Hawksbill에서 TurtleBot3 시뮬레이션 설정하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">ROS 2 Humble Hawksbill에서 TurtleBot3 시뮬레이션 설정하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="TRI의 로봇들이 오후에 새로운 조작 기술을 익힌 방법을 소개합니다" href="/post/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="TRI의 로봇들이 오후에 새로운 조작 기술을 익힌 방법을 소개합니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="TRI의 로봇들이 오후에 새로운 조작 기술을 익힌 방법을 소개합니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">TRI의 로봇들이 오후에 새로운 조작 기술을 익힌 방법을 소개합니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="인공지능과 로봇공학 사회에 미치는 영향과 수조 달러 규모의 비지니스 기회" href="/post/2024-06-20-AIandRoboticsImplicationsforSocietyandaMulti-TrillionDollarBusinessOpportunity"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="인공지능과 로봇공학 사회에 미치는 영향과 수조 달러 규모의 비지니스 기회" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-AIandRoboticsImplicationsforSocietyandaMulti-TrillionDollarBusinessOpportunity_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="인공지능과 로봇공학 사회에 미치는 영향과 수조 달러 규모의 비지니스 기회" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">인공지능과 로봇공학 사회에 미치는 영향과 수조 달러 규모의 비지니스 기회</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ROS2 겸손한 이미지 세분화" href="/post/2024-06-20-ROS2HumbleImageSegmentation"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ROS2 겸손한 이미지 세분화" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-ROS2HumbleImageSegmentation_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ROS2 겸손한 이미지 세분화" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">ROS2 겸손한 이미지 세분화</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="모바일 로봇, 더 많으면 더 좋다는 통신 원칙을 잘 활용하기" href="/post/2024-06-20-Mobilerobotsgetalegupfromamore-is-bettercommunicationsprinciple"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="모바일 로봇, 더 많으면 더 좋다는 통신 원칙을 잘 활용하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-Mobilerobotsgetalegupfromamore-is-bettercommunicationsprinciple_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="모바일 로봇, 더 많으면 더 좋다는 통신 원칙을 잘 활용하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">모바일 로봇, 더 많으면 더 좋다는 통신 원칙을 잘 활용하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기" href="/post/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이를 사용한 자동 번호판 인식" href="/post/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이를 사용한 자동 번호판 인식" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이를 사용한 자동 번호판 인식" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">라즈베리 파이를 사용한 자동 번호판 인식</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">30<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이가 이제 Hailo-8L M2 모듈로 AI를 처리할 준비가 되었습니다" href="/post/2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이가 이제 Hailo-8L M2 모듈로 AI를 처리할 준비가 되었습니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이가 이제 Hailo-8L M2 모듈로 AI를 처리할 준비가 되었습니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">라즈베리 파이가 이제 Hailo-8L M2 모듈로 AI를 처리할 준비가 되었습니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이 5 CPU 성능" href="/post/2024-06-20-RaspberryPi5CPUPerformance"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이 5 CPU 성능" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-RaspberryPi5CPUPerformance_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이 5 CPU 성능" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">라즈베리 파이 5 CPU 성능</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="가장 빠른 쿠버네티스 배포 방법일까요 라즈베리 파이 클러스터 구축에 대해 알아봅시다" href="/post/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="가장 빠른 쿠버네티스 배포 방법일까요 라즈베리 파이 클러스터 구축에 대해 알아봅시다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="가장 빠른 쿠버네티스 배포 방법일까요 라즈베리 파이 클러스터 구축에 대해 알아봅시다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">가장 빠른 쿠버네티스 배포 방법일까요 라즈베리 파이 클러스터 구축에 대해 알아봅시다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/41">41</a><a class="link" href="/posts/42">42</a><a class="link" href="/posts/43">43</a><a class="link" href="/posts/44">44</a><a class="link" href="/posts/45">45</a><a class="link posts_-active__YVJEi" href="/posts/46">46</a><a class="link" href="/posts/47">47</a><a class="link" href="/posts/48">48</a><a class="link" href="/posts/49">49</a><a class="link" href="/posts/50">50</a><a class="link" href="/posts/51">51</a><a class="link" href="/posts/52">52</a><a class="link" href="/posts/53">53</a><a class="link" href="/posts/54">54</a><a class="link" href="/posts/55">55</a><a class="link" href="/posts/56">56</a><a class="link" href="/posts/57">57</a><a class="link" href="/posts/58">58</a><a class="link" href="/posts/59">59</a><a class="link" href="/posts/60">60</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"ROS 2 Humble Hawksbill에서 TurtleBot3 시뮬레이션 설정하기","description":"","date":"2024-06-20 17:55","slug":"2024-06-20-SettingUpTurtleBot3SimulationinROS2HumbleHawksbill","content":"\n\n# 요구 사항 -\n\na. Ubuntu 22.04 (Jammy Jellyfish)\n\nb. ROS 2 Humble Hawksbill\n\n# 1. ROS 2 환경 변수 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nROS 2 설정 파일을 소스로 가져오면 ROS 2를 실행하는 데 필요한 여러 환경 변수가 설정됩니다.\n\nROS 2 명령에 액세스하려면 모든 터미널에 다음 명령을 입력하여 ROS2 Humble 환경을 소스화하세요:\n\n```js\nsource /opt/ros/humble/setup.bash\n```\n\n새 쉘을 열 때마다 설정 파일을 소스로 가져오지 않으려면 셸 시작 스크립트에 해당 명령을 추가할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```shell\necho \"source /opt/ros/humble/setup.bash\" \u003e\u003e ~/.bashrc\n```\n\n이 명령어를 .bashrc 파일에 추가한 후에는 해당 파일을 소스하세요:\n\n```shell\nsource ~/.bashrc\n```\n\n# 2. Gazebo 시뮬레이터 설치하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n쉘 스크립트에서 다음 명령을 입력하여 Gazebo11과 관련된 ROS 2 메타 패키지를 설치해보세요:\n\n```js\nsudo apt install gazebo11\nsudo apt install ros-humble-gazebo-ros-pkgs\n```\n\n# 3. ROS 2 의존 패키지 설치\n\n3.1. Cartographer\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n카토그래퍼는 여러 플랫폼과 센서 구성에서 2D 및 3D 실시간 동시 위치추적 및 매핑(SLAM)을 제공하는 시스템입니다.\n\nROS 2 Cartographer 패키지를 설치하려면 셸에서 다음 명령을 입력하세요:\n\n```js\nsudo apt install ros-humble-cartographer \nsudo apt install ros-humble-cartographer-ros\n```\n\n3.2. ROS 2를 위한 네비게이션 스택\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nROS 2 내비게이션 스택은 로봇이 시작 위치에서 목표 위치로 이동하는 데 도움을 주는 패키지 세트입니다.\n\nROS 2 내비게이션 스택 패키지를 설치하려면 셸에 다음 명령을 입력하세요:\n\n```js\nsudo apt install ros-humble-navigation2\nsudo apt install ros-humble-nav2-bringup\n```\n\n# 4. ROS2 Workspace 생성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n작업 공간은 모든 ROS 2 패키지를 포함하는 디렉토리 세트입니다.\n\n다음 명령어를 입력하여 새 작업 공간 디렉토리를 생성하세요:\n\n```js\nmkdir -p ~/turtlebot3_ws/src\n```\n\n# 5. Turtlebot3 패키지 설치\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5.1. 워크스페이스의 src 디렉토리 안으로 이동해주세요:\n\n```js\ncd ~/turtlebot3_ws/src\n```\n\n5.2. Turtlebot3 패키지를 클론해주세요:\n\n```js\ngit clone https://github.com/ROBOTIS-GIT/turtlebot3_simulations.git -b humble-devel\n\ngit clone https://github.com/ROBOTIS-GIT/turtlebot3.git -b humble-devel\n\ngit clone https://github.com/ROBOTIS-GIT/turtlebot3_msgs.git -b humble-devel\n\ngit clone https://github.com/ROBOTIS-GIT/DynamixelSDK.git -b humble-devel\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 6. 패키지 빌드하기\n\n작업 공간의 루트(turtlebot3_ws)에서 다음 명령을 사용하여 colcon을 이용해 ROS 2 패키지를 빌드하세요.\n\n```js\ncd ~/turtlebot3_ws\n\ncolcon build \n```\n\nturtlebot3_ws 작업 공간을 소스하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n~/turtlebot3_ws/install/setup.bash\n```\n\n## 7. 터틀봇3 가제보 시뮬레이션 실행하기\n\n작업 공간 (turtlebot3_ws) 안으로 이동한 후, 다음 명령을 실행하여 작업 공간을 소스하는 명령을 실행하세요.\n\n```js\n~/turtlebot3_ws/install/setup.bash\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTurtleBot3에는 버거, 와플, 와플_파이 세 가지 모델이 있습니다. 따라서 사용하기 전에 어떤 모델을 사용할지 설정해야 합니다.\n\n우분투에서 이를 수행하려면 export 명령어를 사용하여 사용할 모델을 지정합니다.\n\n다음과 같이 명령어를 셸에 입력하여 TURTLEBOT3_MODEL을 버거로 .bashrc 파일에 추가합니다. 그 후 .bashrc 파일을 재로드하세요:\n\n```js\necho 'export TURTLEBOT3_MODEL=burger' \u003e\u003e ~/.bashrc\nsource ~/.bashrc\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 7.1. Gazebo에서 Turtlebot3 시뮬레이션 월드 실행하기\n\n새 셸 창을 열고 turtlebot3 워크스페이스를 소스로 설정하세요:\n\n```js\nsource ~/turtlebot3_ws/install/setup.bash\n```\n\n다음 명령을 입력하여 Turtlebot3 로봇을 TurtleBot3 World에서 실행하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nros2 launch turtlebot3_gazebo empty_world.launch.py\n```\n\n다른 시뮬레이션 런치 파일을 실행하여 시뮬레이션 월드를 변경할 수 있어요:\n\n## TurtleBot3 World:\n\n```js\nros2 launch turtlebot3_gazebo turtlebot3_world.launch.py\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## TurtleBot3 집:\n\n```js\nros2 launch turtlebot3_gazebo turtlebot3_house.launch.py\n```\n\n## 7.2. Turtlebot3 텔레오퍼레이션 노드 실행하기\n\n새로운 쉘 창을 열고 turtlebot3 workspace에 소스를 입력하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsource ~/turtlebot3_ws/install/setup.bash\n```\n\n이 명령어를 사용하여 키보드로 TurtleBot3를 텔레오퍼레이션하려면 다음 명령어로 텔레오퍼레이션 노드를 실행하십시오:\n\n```js\nros2 run turtlebot3_teleop teleop_keyboard\n```","ogImage":{"url":"/assets/img/2024-06-20-SettingUpTurtleBot3SimulationinROS2HumbleHawksbill_0.png"},"coverImage":"/assets/img/2024-06-20-SettingUpTurtleBot3SimulationinROS2HumbleHawksbill_0.png","tag":["Tech"],"readingTime":4},{"title":"TRI의 로봇들이 오후에 새로운 조작 기술을 익힌 방법을 소개합니다","description":"","date":"2024-06-20 17:53","slug":"2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow","content":"\n\n## 작성자: Siyuan Feng, Ben Burchfiel, Toffee Albina, and Russ Tedrake\n\n![이미지](/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_0.png)\n\nTRI는 로봇이 시연을 통해 새로운 미술적 행동을 습득할 수 있는 새로운 접근 방식을 공개했습니다. 이것이 왜 중요한 새로운 기능인지, 이것이 가능하게 된 진전은 무엇이며, 다음으로 어디를 향해 나아가는지 살펴보겠습니다.\n\n# 소개\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTRI의 로봇공학 연구 부문의 미션은 신기술을 개발하여 다음 세대의 민첩한 로봇이 사람들을 지원하고 삶의 질을 향상시키는 데 기여하는 것입니다. 이를 위해서는 로봇이 구조화되지 않은 자연 환경에서 사람들과 함께 작동해야 합니다. 환장한 분들을 도와 요리 준비를 하거나 다친 사람을 의자에서 일어나게 하거나 작업 현장에서 일하는 사람들을 지원하는 것이 있을 수 있습니다. 미래의 로봇들은 세계 어디에서나 사람들의 삶에 엄청난 긍정적인 영향을 미칠 것입니다.\n\n다음 세대의 로봇은 유연하고 적응력이 있어야 합니다. 두 집, 두 작업 현장, 두 사람이 서로 같지 않기 때문에, 우리의 로봇은 오늘날보다 훨씬 더 다재다능해져야 합니다. 현재 로봇들은 임무를 수행할 수 있도록 면밀히 프로그래밍되어 있으며, 사람들은 명백한 모서리 사례를 예상하고 로봇에게 실수 대처 방법을 가르쳐야 합니다. 이 접근 방식은 통제된 환경에서 막대한 성공을 거두었으며, 현대의 공장 및 창고 자동화의 기초가 됩니다. 그러나 이 방식으로 로봇을 만드는 것은 로봇의 환경을 주의 깊게 모델링하고, 로봇의 행동 범위를 주의 깊게 설정하고, 모든 상황을 미리 예견해야 합니다. 이것은 앞으로 야생에서 작동하는 더 능숙한 미래 로봇을 위해 필요한 복잡성으로 확장할 수 없습니다.\n\n그 결과로 로보틱스 분야에서 인공지능과 기계 학습을 사용하는 로봇에 대한 관심이 급증했습니다. 이러한 접근 방식 중 많은 것이 전통적인 로봇공학의 측면을 AI와 융합하여 효과적으로 사용하지만, 중요한 것이 빠졌습니다: 로봇은 아직 주변의 물리적 세계를 제어하는 데 재능이 없습니다. 이제 우리는 사람들과 대화할 수 있는 로봇을 가졌지만 과자 봉지를 열 수 없는 것이든 신발끈을 매 줄 수 없는 로봇이 있습니다. 로봇은 똑똑해지고 있지만 아직 간단한 방식으로 세상과 상호 작용할 뿐입니다.\n\n이를 해결하기 위해 TRI는 최근 최첨단 생성 AI의 진보를 바탕으로 로봇에게 단 하루 만에 새로운 조작 능력을 가르칠 수 있는 능력을 개발했습니다. 동일한 로봇, 동일한 코드 및 동일한 설정을 사용하여 우리는 야채 껍질 벗기기, 핸드 믹서 사용, 스낵 준비 및 팬케이크 뒤집기와 같은 60가지 다른 민첩한 행동을 가르쳤습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금은 오늘의 가능성을 끌어올리기만 하는 것이 아니라 유연하고 적응 가능한 손재주를 가진 일반 목적의 로봇의 기반을 마련하기 위해 다양한 행동들의 광범위한 커리큘럼을 개발 중입니다.\n\n이를 가능하게 하는 비하인드 스토리를 자세히 알려드리는 것에 기쁨을 느낍니다.\n\n# 가르치는 방법\n\n새로운 행동을 가르치기 위해 인간 작업자가 로봇을 원하는 작업을 시연하는 방식으로 원격으로 작동시킵니다. 보통 몇 시간의 가르침이 필요하며, 이로 인해 수십 개에서 수백 개의 시연이 이루어집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 학습 방식\n\n어떤 특정 행동에 대한 집합의 시연이 수집되면, 로봇은 해당 행동을 자율적으로 수행하도록 학습합니다. 우리 프로세스의 핵심은 최근 이미지 생성 분야를 뒤흔든 생성적 AI 기술인 디퓨전(Diffusion)입니다. 최근 TRI와 우리 대학 파트너들인 송 교수 연구실에서 이 기술을 채택하여 직접적으로 로봇 행동을 생성하는 디퓨전 정책(Diffusion Policy)이라는 방법을 개발했습니다. 자연어에 의존한 이미지 생성이 아닌 센서 관측 및 선택적으로 자연어에 의존한 상황에 따른 로봇 행동을 생성하기 위해 디퓨전을 사용합니다.\n\n![이미지](/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_1.png)\n\n로봇 행동을 생성하는 데 디퓨전을 사용하는 것은 이전 방법들보다 세 가지 주요 이점을 제공합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 멀티 모달 시연에 대한 적합성. 이것은 인간 시연자들이 행동을 자연스럽게 가르치고 로봇이 혼란스러워하지 않아도 된다는 것을 의미합니다.\n- 고차원 행동 공간에 적합성. 이것은 로봇이 미래를 예측하고 단시간 행동을 피함에 도움이 되는 것을 가능하게 합니다.\n- 안정적이고 신뢰할 수 있는 훈련. 이것은 귀찮은 손조절이나 황금자료점을 찾아다니지 않고도 규모에 맞게 로봇을 훈련하고 신뢰할 수 있다는 것을 의미합니다.\n\n## 멀티 모달 행동\n\n대부분의 현실 세계 작업은 여러 가지 다른 방법으로 해결할 수 있습니다. 예를 들어 컵을 들어올릴 때 사람은 위쪽, 옆면 또는 심지어 아래에서 잡을 수 있습니다. 이 같은 현상, 행동의 멀티모달성은 이전까지 행동 학습 방법이 적응하기 어려웠던 것으로, 정상적인 인간 행동에서 널리 퍼져 있음에도 불구하고 어려움을 겪었습니다.\n\n로봇이 테이블 위에 앉아 있는 T자 모양 블록을 목표 위치로 밀어야 하는 간단한 예시를 고려해 보겠습니다. 로봇은 블록을 미끄러뜨려 이동해야 하며 다양한 면에 도달하기 위해 블록 주위로 이동해야 합니다. 이 작업은 본질적으로 멀티모달성을 포함하고 있으며 블록을 좌측이나 우측으로 이동하는 것이 대개 합리적입니다. 두 가지 동등한 올바른 행동 모드가 있습니다. 이 해결책은 단일 행동을 예측하는 대신 행동 분포를 학습하는 것입니다. 확산 정책은 이러한 분포를 더 안정적이고 견고하게 배우며, 이전 방법보다 멀티모달성을 훨씬 잘 포착합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_2.png)\n\n복합 모달 데모를 다룰 수 있는 능력은 복잡한 재치 있는 행동을 가르치는 데 중요한 역할을 한 것으로 입증되었으며, 이러한 유형의 복합성은 보편적입니다. 또한 우리의 로봇이 데이터 수집을 확장하는 동안 여러 선생님으로부터 쉽게 배울 수 있도록 합니다.\n\n## 고차원 행동\n\n확산은 고차원 출력 공간에 맞게 자연스럽게 적합합니다. 예를 들어 이미지를 생성하는 데는 수십만 개의 개별 픽셀을 예측해야 합니다. 로봇 공학에서 이것은 주요한 장점이며, 복잡한 다리를 가진 로봇에 우아하게 확산 기반 행동 모델을 확장할 수 있게 합니다. 또한 한 번의 단계가 아닌 의도된 행동 경로를 예측하는 능력을 제공합니다. 최근 작업 'DP, ACT'는 경로 예측이 종종 성능이 우수한 견고한 정책을 학습하기 위한 중요한 설계 기능임을 보여 주었습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 안정적인 교육\n\n확산 정책은 교육을 진행하는 데 괴로워하지 않을 만큼 굉장히 간단합니다. 새로운 행동을 가르치는 데에 많은 비용이 들고 번거로운 실제 세계 평가를 할 필요 없이 최상의 실행 체크포인트와 하이퍼파라미터를 찾는 과정을 요구하지 않습니다. 컴퓨터 비전이나 자연 언어 적용과 달리, AI 기반의 닫힌 루프 시스템은 오프라인 지표로 정확하게 평가되지 않을 수 없습니다. 이 시스템들은 일반적으로 로봇 공학에서 실제 하드웨어에서 평가를 필요로 합니다. 이는 현실 세계 평가 병목 현상으로 인해 광범위한 조정이나 하이퍼파라미터 최적화를 필요로 하는 모든 학습 파이프라인이 실용성을 잃게 만듭니다. 이러한 이유로 확산 정책은 뛰어난 안정성을 바탕으로 작동하여 이 어려움을 우회할 수 있게 해줍니다. 이것이 바로 우리에게 규모의 핵심을 제공해준 핵심적인 요소입니다.\n\n# 우리의 플랫폼\n\n## 원격 작동\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로봇을 인간의 데모를 통해 가르치기 때문에, 어려운 미묘한 동작을 가르치는 데 중요한 것은 좋은 원격 조작(teleoperation) 인터페이스입니다. 저희의 로봇 학습 방법은 텔레오퍼레이션 장치의 선택에 중립적이며, 조이스틱과 같은 다양한 저가의 인터페이스를 사용해왔습니다. 보다 미묘한 동작을 위해 바이매뉼 햅틱(haptic) 장치를 사용하여 가르칩니다. 이때 텔레오퍼레이션 장치와 로봇 사이에 위치-위치 결합이 있습니다. 위치-위치 결합은 입력 장치가 측정된 자세를 명령으로 보내고 로봇이 이러한 자세 명령을 토크를 사용한 운동 공간 제어로 추적한다는 것을 의미합니다. 로봇의 자세 추적 오류를 힘이 변환한 후 이 힘을 선생님이 느낄 수 있도록 다시 입력 장치로 보냅니다. 이를 통해 선생님은 힘을 통해 로봇과의 피드백 루프를 닫을 수 있으며, 우리가 가르치는 가장 어려운 기술 중 많은 부분에 필수적입니다.\n\n양 팔을 사용해 물체를 조작할 때 힘 피드백은 특히 중요합니다. 이를 실험적으로 확인할 수 있는 사례는 수동 핸드믹서와 같은 작동을 필요로 하는 장치를 조작하는 것입니다. 이는 이 피드백 없이 신뢰할 수 없는 방식으로 데모할 수 없었습니다.\n\n로봇이 양 손으로 도구를 잡을 때, 닫힌 기구 체인이 형성됩니다. 로봇과 도구의 특정 설정에 대해 시각적으로 관측할 수 없는 다양한 내부 힘 범위가 있습니다. 그리퍼를 떼어내는 것과 같은 특정 힘 구성은 불안정하며, 로봇의 움직임이 미끄러질 가능성이 높아집니다. 휴먼 데모 사람들이 햅틱 피드백에 접근할 수 없는 경우, 적절한 힘 제어를 느끼거나 가르칠 수 없습니다. 우리는 힙틱 피드백이 양 팔의 결합이 필요한 굵직한 미적 행동을 가르칠 때 데모 성공률을 개선하는 데 중요하다는 것을 발견했습니다.\n\n## 촉각 감지\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n장갑을 끼고 신발을 묶으려고 한 적이 있는 사람은 손의 감각이 얼마나 중요한지를 경험했을 것입니다. 꼼꼼한 작업을 할 때, 무슨 일이 일어나고 있는지 느낄 수 있다는 것은 성공에 중요한 추가 정보를 제공합니다. 우리는 로봇도 이와 마찬가지로 촉감을 갖는 것이 도움이 될 것이라고 믿습니다. 그래서 우리는 많은 플랫폼에서 TRI Soft-Bubble 센서를 사용하고 있습니다. 이러한 센서는 내부 카메라가 부풀어 오른 변형 가능한 외부 막을 관찰하는 것으로 구성되어 있습니다. 이러한 센서들은 희박한 힘 신호를 측정하는 것을 넘어 로봇이 접촉 패턴, 기하학, 미끄러짐 및 힘에 관한 공간적으로 밀집된 정보를 인식할 수 있도록 합니다.\n\n이 유형의 센서는 최근 몇 년간 보다 인기가 많아졌지만, 제공하는 정보를 잘 활용하는 것은 어려운 과제였습니다. 확산은 로봇이 이 시각-촉감 센서가 제공하는 풍부한 정보를 사용하는 자연스러운 방법을 제공합니다. 이러한 신호에 대해 조건부로 - 추가 입력으로 사용하는 것으로 - 우리는 이를 임의의 꼼꼼한 작업에 적용할 수 있도록 합니다.\n\n이 방향으로 진행한 초기 실험은 매우 유망합니다. 많은 경우에 접촉 감지를 추가하면 로봇의 재미있는 접촉 단계를 포함한 작업 수행 능력이 크게 향상됨을 발견하고 있습니다 😊\n\n\u003cimg src=\"/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_3.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 안전하고 성능이 우수한 제어\n\n고성능 로봇의 중간 제어 계층은 중대한 역할을 하며 종종 무시되는 부분입니다. 우리의 경우, 학습된 정책과 인간 데모 사용자는 10Hz로 로봇의 그리퍼에 대한 위치 및 방향 명령을 내립니다. 이러한 명령들은 중간 제어자에 의해 1kHz의 관절 토크 명령으로 업샘플링되고 변환됩니다. 중요한 점은 이 중간 제어자에는 로봇을 안전하게 보호하고 학습된 정책이나 인간 교사로부터 잠재적으로 위험한 상위 명령을 실행하지 못하도록 하는 안전 기능이 내장되어 있다는 것입니다.\n\n저희 방식은 작업 공간 제어에 근거하며 관절 레벨 명령에 대한 제약 조건이 있는 최적화 문제로 정의됩니다. 목표는 데모 사용자나 학습된 정책이 제공하는 고수준 명령을 따르면서 물리 및 충돌 회피와 같은 기타 안전 제약 조건을 준수하는 것입니다. 이 구현은 Drake Systems Framework를 활용하며 엄격한 분석과 테스트를 가능케 하여 시스템의 이 중요한 부분에 대해 확신할 수 있게 해줍니다. 우리는 이 구현을 향후 오픈 소스로 공개할 계획이며 이는 우리 시스템의 주요 성공 요소 중 하나로 여기고 있습니다.\n\n튼튼한 중간 제어자는 고품질의 행동 학습 파이프라인의 기초입니다. 이는 임피던스 제어 및 햅틱 피드백과 같은 임무 중요한 기능을 가능하게 하며 전반적인 시스템에 보다 훌륭한 안전 보호 기능을 제공하여 교사가 로봇을 물리적 한계까지 밀어낼 수 있도록 도와줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 현재 어디에 있나요\n\n로봇공학의 놀라운 새로운 시대에 들어서고 있습니다. 전문 로봇 공학자들이 몇 주의 시간을 투자해야 했던 일들을 누구나 이제 하루 만에 가르치고 성공을 기대할 수 있습니다. 이제는 조정 없이 작동하는 단일 파이프라인을 통해 복잡한 상호 작용을 가진 민첩한 행동들을 가르치는 것이 가능해졌습니다.\n\n더 많이 해야 할 일이 남아 있습니다. 현재 우리가 로봇에 새로운 기술을 가르칠 때, 그것은 부서지기 쉽습니다. 기술은 가르치는 데 사용된 상황과 유사한 상황에서는 잘 작동할 것이지만, 차이가 있는 상황에서 로봇은 고전할 것입니다. 실전에서 우리가 관찰하는 실패 케이스의 가장 흔한 원인은 다음과 같습니다:\n\n- 회복이 증명되지 않은 상태. 이는 너무 깨끗한 데모데이터로 인한 결과일 수 있습니다.\n- 카메라 시점 또는 배경의 중요한 변화.\n- 테스트 시간에 훈련 중 만나지 못한 피조작물.\n- 방해물, 예를 들어 훈련 중에 없던 중요한 혼잡.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼 해결책은 무엇일까요? 사람이 평생 경험을 토대로 배우듯이 점차 능력과 적응력이 향상되는 것처럼, 우리는 다양한 행동 교육과정이 유연하고 일반적인 로봇을 만드는 열쇠라고 믿습니다. 이를 위해 TRI는 신체 데이터에 기초한 강력한 행동 교육과정을 실제 로봇 그룹과 강력한 드레이크 시뮬레이션 스위트 모두에 투자하고 있습니다.\n\n지금까지 재치있는 조작 로봇 그룹에 60가지 이상의 행동을 가르쳤으며 올해 말까지 200가지 이상의 행동을 목표로 한 노력의 속도를 증가하고 있습니다. 이러한 행동은 도구 사용부터 가소성 물체 조작, 조심스러운 양손 조율까지 다양한 조작 시나리오에 걸쳐 있습니다.\n\n# 다음은\n\n다양한 기술 데이터셋에서 정책을 학습할 때 초기 성공의 조짐을 보았습니다. 한 예로, 상당히 정리된 장면에서 로봇에게 얼음이 담긴 머그잔을 싱크대로 비우도록 가르쳤습니다. 매우 혼잡한 상황에서 평가할 때, 이 데이터만 가지고 훈련된 정책은 거의 즉시 실패했습니다. 우리는 이 기술의 두 번째 버전을 얼음을 따르는 다른 15가지 작업과 함께 공동으로 가르쳤고, 로봇에게 원하는 행동에 대한 언어 설명을 조건으로 두었습니다. 어지럽힘에서 얼음을 붓는 동일한 시연자에게 접근할 수 있음에도 불구하고, 다중 작업 기술은 단일 작업 버전이 재앙적으로 실패한 매우 혼잡한 상황에서 성공했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 우리는 실제 로봇 데이터만으로는 진정으로 일반적인 손재주 로봇을 만들기에 충분하지 않을 것으로 예상합니다. 이 빈 공간을 채우기 위해, 우리는 Drake를 사용하여 강력한 시뮬레이션 전문 기술에 크게 의존하고 있습니다. Drake는 단단한 물체와 부드러운 물체 간 상세한 물리 상호작용을 정확하게 모델링할 수 있는 능력이 독특하며, 정교하고 복잡한 행동에 중요합니다.\n\n이 노력이 진행됨에 따라 성공의 확실한 징후 중 하나는 영사적으로 능숙한 행동 생성과 맥락 속 학습일 것입니다. 기존의 대형 언어 모델은 새로운 방식으로 개념을 구성하고 단일 예제에서 배울 수 있는 강력한 능력을 갖고 있습니다. 지난 한 해 동안 우리는 로봇이 의미론적으로 일반화하도록 함으로써 이를 가능케 하여 신규 물체로 채집 및 배치하는 것을 보았습니다. 다음 큰 이정표는 이 의미론적 능력을 높은 수준의 물리 지능과 창의성과 융합하는 동등한 강력한 대규모 행동 모델을 만드는 것입니다. 이러한 모델은 주변 세계와 풍부하게 상호작용하고 필요할 때 즉흥적으로 새로운 손재주 행동을 만들 수 있는 일반적인 목적의 로봇을 위해 중요할 것입니다.\n\n이 프로젝트에 관해 질문이 있거나 우리로부터 직접 듣고 싶으시다면, TRI LinkedIn 페이지에서 10월 4일 미동부로 1시부터 1시30분까지/태평양부로는 오전 10시부터 10시30분까지 LinkedIn Live Q\u0026A 세션에 참가해 주세요. TRI의 LinkedIn 페이지에서 이벤트에 등록하세요.\n\n# 감사의 글\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n에릭 쿠지노, 나빈 쿠퍼스와미, 파루트 샤와 같은 우리의 다른 동료인 알렉스 알스팩, 라레스 암브루스, 맥스 바지라챠리야, 앤드류 보울리우, 아디티아 바트, 이샨 찬드라트레야, 청 치, 릭 코리, 샘 크리시, 홍카이 다이, 리처드 데니토, 잭 팡, 아드리앙 가이돈, 그랜트 굴드, 쿠니마쓰 하시모토, 브랜든 해서웨이, 앨리슨 헨리, 피비 호건, 제나 홀만, 유타로 이시다, 토마스 콜라, 데일 맥코낸키, 이안 맥마혼, 캘더 필립스-그래프린, 고든 리처드슨, 찰리 리히터, 타로 타카하시, 파벨 톡마코프, 제러드 윌슨, 트리스탄 휘팅, 블레이크 울프와 같은 컨트리뷰터들의 힘든 노력 없이는 이 작업이 가능하지 않았을 것입니다.","ogImage":{"url":"/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_0.png"},"coverImage":"/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_0.png","tag":["Tech"],"readingTime":9},{"title":"인공지능과 로봇공학 사회에 미치는 영향과 수조 달러 규모의 비지니스 기회","description":"","date":"2024-06-20 17:52","slug":"2024-06-20-AIandRoboticsImplicationsforSocietyandaMulti-TrillionDollarBusinessOpportunity","content":"\n\n# 테크 엘리트: AI 및 점점 벌어지는 부의 격차\n\n포사이트 비로 Youtube 채널을 구독하세요: https://www.youtube.com/@ForesightBureau?sub_confirmation=1\n\n로봇에게 일자리를 빼앗길 걱정이 있나요? 혼자가 아닙니다. 저희의 'AI 직업 아포칼립스' 시리즈의 첫 번째 동영상에서는 자동화가 일으키는 위협으로부터 미래를 지키는 방법을 제안했습니다. 최신 비디오에서는 사회에 미칠 잠재적인 영향에 대해 심층적으로 살펴보며, 이 번창하고 있는 산업에 투자해야 하는 시기가 지금이라고 설명합니다. 왜냐하면 AI 산업이 수조 달러 규모의 기회로 성장하고 있기 때문입니다.\n\n![AI와 로봇공학이 사회와 수조 달러 비즈니스 기회에 미치는 영향](/assets/img/2024-06-20-AIandRoboticsImplicationsforSocietyandaMulti-TrillionDollarBusinessOpportunity_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시리즈의 이전 비디오에서는 자동화의 새로운 시대에 있어 가장 위험에 노출된 화이트 칼라 직업과 블루 칼라 직업이 무엇인지, 어떤 기업들이 인간형 로봇 산업을 지배할 것으로 보이며 해당 기업에 투자를 고려해야 하는 이유에 대해 다루었습니다. 또한 인간형 양다리로의 흥미로운 부상을 살펴보고 이들이 노동력을 대체할 것으로 예상되는 이유를 설명했습니다.\n\n아래 비디오에서는 AI와 로보틱스가 사회에 미칠 넓은 영향에 대해 다룹니다. 우리는 자동화가 기술 엘리트에게 혜택을 더 많이 줄 수 있고 이로 인해 부의 불평등이 더 커지고 사회적 격변을 야기하며, 기술적으로 우월한 소수의 손에 권력이 집중되는 가능성 등을 탐구합니다.\n\n물리는 오는 기계 경제에서 이윤을 얻을 수 있는 생존 전략 몇 가지를 제시하며 긍정적으로 마무리됩니다.\n\n![AI and Robotics Implications for Society and a Multi-Trillion Dollar Business Opportunity](/assets/img/2024-06-20-AIandRoboticsImplicationsforSocietyandaMulti-TrillionDollarBusinessOpportunity_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리를 소셜 미디어에서 팔로우해 주세요:\n\nYoutube: https://www.youtube.com/@ForesightBureau\nInstagram: https://www.instagram.com/foresight.bureau\nSubstack: https://substack.com/@foresightbureau\nTwitter/X: https://x.com/foresightbureau\nPodcast: https://foresightbureau.podbean.com\nLinkedin: https://bit.ly/ForesightBureauLI\nFacebook: https://bit.ly/ForesightBureauFB\nMedium: https://medium.com/@foresightbureau\nWebsite: https://foresightbureau.com\n\n면책 조항\n\n본 블로그는 오로지 즐거움을 위한 목적으로 작성되었습니다. 게시된 정보의 정확성이나 완전성을 보장하지 않습니다. 본 블로그를 사용함으로 인해 발생할 수 있는 손실이나 손해에 대해 우리는 책임을 지지 않습니다. 투자나 금융 조언으로 해석해서는 안 됩니다.","ogImage":{"url":"/assets/img/2024-06-20-AIandRoboticsImplicationsforSocietyandaMulti-TrillionDollarBusinessOpportunity_0.png"},"coverImage":"/assets/img/2024-06-20-AIandRoboticsImplicationsforSocietyandaMulti-TrillionDollarBusinessOpportunity_0.png","tag":["Tech"],"readingTime":2},{"title":"ROS2 겸손한 이미지 세분화","description":"","date":"2024-06-20 17:50","slug":"2024-06-20-ROS2HumbleImageSegmentation","content":"\n\n\n\u003cimg src=\"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_0.png\" /\u003e\n\n# 소개\n\n이 튜토리얼에서는 DeepLabV3 모델을 사용하여 이미지의 의미론적 세분화를 수행하는 ROS2 노드를 만들 것입니다. ResNet-101 백본을 사용한 이 모델은 의미론적 세분할 작업에 대한 최신 아키텍처입니다. 노드는 웹캠 피드를 구독하고 이미지를 처리한 후 세그멘테이션을 수행하고 세분화된 이미지를 게시할 것입니다.\n\n## 준비물\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 설치되어 있는지 확인하세요:\n\n- ROS2 (Humble)\n- PyTorch\n- OpenCV\n- cv_bridge (ROS 이미지와 OpenCV 이미지 간 변환을 위한)\n- torchvision (사전 학습 모델과 이미지 변환을 위한)\n\n## DeepLabV3\n\nDeepLabV3은 시맨틱 세그멘테이션 작업을 위해 설계된 최신 딥러닝 모델입니다. 시맨틱 세그멘테이션은 이미지의 각 픽셀을 미리 정의된 범주로 분류하는 작업을 포함합니다. 물체 감지와 달리 물체를 식별하고 주위에 바운딩 상자를 넣는 대신, 시맨틱 세그멘테이션은 장면에 대한 상세하고 픽셀 수준의 이해를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## DeepLabV3의 주요 기능\n\nResNet 백본: DeepLabV3은 특성 추출을 위해 ResNet-101을 사용합니다. ResNet-101은 잔차 학습 방식으로 유명한 강력하고 깊은 신경망으로, 사라져가는 그래디언트 문제를 해결하여 매우 깊은 네트워크를 효율적으로 훈련할 수 있으며 견고한 특성 추출을 보장합니다.\n\n## DeepLabV3 작동 방식\n\n- 입력 이미지: 고정된 크기(예: 512x512 픽셀)의 입력 이미지로 프로세스가 시작됩니다.\n- 특성 추출: 입력 이미지는 ResNet-101 백본을 통해 전달됩니다. 이 네트워크는 ImageNet과 같은 대규모 데이터셋에서 사전 훈련되어 다양한 시각적 특성에 대한 견고한 이해력을 갖추고 있습니다.\n- 어트러스 합성곱 레이어: 초기 특성 추출 후, 모델은 다양한 확장률을 가진 일련의 어트러스 합성곱을 적용합니다. 이 단계를 통해 모델은 다양한 크기의 객체를 분할하는 데 중요한 다중 스케일의 특성을 캡처할 수 있습니다.\n- 공간 피라미드 풀링: 어트러스 합성곱의 출력은 공간 피라미드 풀링 모듈로 전달됩니다. 이 모듈은 다양한 스케일에서 특성을 풀링하여 이미지의 풍부한 다중 문맥적 표현을 제공합니다.\n- 분할 맵: 마지막으로, 풀링된 특성은 원본 이미지 해상도로 업샘플링되고 최종 합성곱 레이어가 분할 맵을 생성합니다. 이 맵의 각 픽셀에는 클래스 레이블이 지정되어 이미지의 상세한 세그멘테이션이 이루어집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_1.png\" /\u003e\n\n## DeepLabV3\n\n- Architecture: DeepLabV3는 시멘틱 이미지 세그멘테이션을 위해 설계된 딥러닝 모델입니다. Dilated (확장된) 합성곱을 활용하여 수용 영역을 확대시키면서도 공간 해상도를 유지하는 방식으로 다중 스케일 문맥을 캡처합니다.\n- Backbone: 여기서 사용된 백본은 ResNet-101이며, 보다 복잡한 표현을 학습하는 데 도움이 되는 깊은 잔여 네트워크입니다.\n- 데이터 세트: 모델은 COCO 데이터 세트에서 사전 훈련을 받고 VOC 라벨로 세부 조정되어 이러한 데이터 세트에서 발견되는 공통 객체를 세분화할 수 있습니다.\n\n## 모델 로딩 코드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nself.model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', weights='DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1')\n```\n\n## 코드 단계\n\n- 초기화: 모델은 SegmentationNode 클래스의 __init__ 메서드에서 초기화됩니다. torch.hub.load 메서드는 미리 훈련된 deeplabv3_resnet101 모델을 불러옵니다.\n- 전처리: 입력 이미지는 모델의 입력 요구 사항과 일치하도록 크기 조정, 중앙 자르기 및 정규화됩니다.\n- 추론: 이미지가 수신될 때 콜백에서 모델은 전처리된 이미지 텐서에 대해 추론을 수행합니다.\n- 후처리: 모델의 출력은 각 픽셀의 클래스 점수를 포함하는 텐서입니다. 각 픽셀에서 가장 높은 점수가 클래스를 결정합니다. 그런 다음 결과는 PASCAL VOC 컬러 맵을 사용하여 컬러맵으로 변환된 세그멘테이션 이미지로 변환됩니다.\n\n![image](/assets/img/2024-06-20-ROS2HumbleImageSegmentation_2.png)  \n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 어플리케이션: 시맨틱 세그멘테이션을 위한 ROS2 노드\n\n실용적인 어플리케이션에서 DeepLabV3를 활용하기 위해 웹캠 이미지로부터 시맨틱 세그멘테이션을 수행하는 ROS2 (로봇 운영 시스템 2) 노드를 만들 수 있습니다. 다음은 단계별 개요입니다:\n\n- 노드 초기화: 웹캠 이미지 토픽을 구독하는 ROS2 노드를 초기화합니다.\n- 이미지 전처리: 들어오는 이미지를 DeepLabV3 모델이 필요로 하는 형식으로 변환합니다. 일반적으로 이미지 크기 조정 및 정규화를 수행합니다.\n- 모델 추론: 전처리된 이미지를 DeepLabV3 모델을 통해 전파하여 세그멘테이션 맵을 얻습니다.\n- 후처리: 세그멘테이션 맵을 원본 이미지 해상도에 맞게 업샘플링하고 클래스 레이블을 시각적으로 식별 가능한 색상으로 변환합니다.\n- 결과 게시: 세그멘트된 이미지를 시각화나 추가 처리를 위해 ROS2 토픽에 게시합니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_3.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 사용법\n\n이 모델은 웹캠에서 수신한 이미지를 세분화하는 데 사용되며, COCO 및 VOC 데이터 세트에서 학습한 클래스에 따라 장면에서 다양한 개체를 식별하고 색칠합니다. 이 세분화된 이미지는 그런 다음 ROS 주제에 발행됩니다.\n\n이 설정을 통해 물체 인식, 자율 탐사, 그리고 장면 이해를 포함한 여러 로보틱 응용 프로그램에 적합한 실시간 이미지 세분화가 가능합니다.\n\n## 자율 탐사\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n장애물 감지 및 회피:\n\n- 도시 환경: 자율 주행 자동차에서 시멘틱 세분화는 차로, 인도, 보행자, 차량 및 도시 풍경의 다른 중요 요소를 식별하는 데 도움이 됩니다. 이러한 요소를 세분화함으로써, 자율 주행 차량은 복잡한 환경에서 안전하게 탐색할 수 있습니다.\n- 실내 내비게이션: 실내에서 작동하는 로봇은 벽, 가구, 문 및 기타 장애물을 감지하기 위해 시멘틱 세분화를 사용할 수 있어 효과적으로 탐색과 경로를 계획할 수 있습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*YcvLdT_wB5wd6jfa6f3xWA.gif)\n\n## 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDeepLabV3는 새로운 atrous convolution과 공간 피라미드 풀링 기술을 통해 높은 정확도와 효율성을 제공하는 시멘틱 세그멘테이션에 강력한 도구입니다. DeepLabV3를 ROS2와 통합함으로써, 개발자들은 환경을 픽셀 수준에서 이해하고 상호 작용하는 지능적인 로봇 응용 프로그램을 만들 수 있습니다. DeepLabV3를 사용하면 자율 주행, 로봇 조작 또는 장면 이해와 같은 영역에서 고급 인식을 위한 새로운 가능성이 열립니다.","ogImage":{"url":"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_0.png"},"coverImage":"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_0.png","tag":["Tech"],"readingTime":4},{"title":"모바일 로봇, 더 많으면 더 좋다는 통신 원칙을 잘 활용하기","description":"","date":"2024-06-20 17:49","slug":"2024-06-20-Mobilerobotsgetalegupfromamore-is-bettercommunicationsprinciple","content":"\n\n## 한 연구에서 발견된바 미끄럼 방지 장치를 가지고 있는 것보다 지면 주변 상황에 대한 감각이 뛰어날 때 모바일 로봇에 더 도움이 된다고 해요.\n\n조지아 공과대학의 복잡한 레오로지학과 생체역학 연구소의 백시 총 박사 연구원이 말하길...\n\n![image](/assets/img/2024-06-20-Mobilerobotsgetalegupfromamore-is-bettercommunicationsprinciple_0.png)\n\n주변 환경에 대한 최소한의 인식력을 가진 로봇에 다리를 추가하면, 어려운 지형에서 로봇이 더 효율적으로 작동할 수 있다는 것을 저와 제 동료들이 발견했어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 수학자이자 엔지니어인 클로드 샤넌의 통신 이론에서 영감을 받았습니다. 먼 거리를 통해 신호를 전송하는 방법에 대해 그의 이론을 참고했습니다. 완벽한 전선을 구축하기 위해 엄청난 비용을 지출하는 대신, 샤넌은 잡음이 많은 통신 채널을 통해 정보를 신뢰할 수 있게 전달하는 데 중복성을 사용하는 것이 충분하다는 것을 보여 주었습니다. 우리는 화물을 로봇을 통해 운송하는 것에 대해 같은 접근법을 적용할 수 있을지 궁금해졌습니다. 즉, 우리가 \"잡음이 많은\" 지형을 통해 화물을 운송하려면, 쓰러진 나무와 큰 바위와 같은 지형에서 이를 로봇의 다리를 추가함으로써 센서와 카메라 없이 수행할 수 있을까요?\n\n대부분의 이동 로봇은 관성 센서를 사용하여 공간을 통해 어떻게 이동하는지 인식합니다. 우리의 주요 아이디어는 관성을 잊고 단순히 반복적으로 걸음을 내딛는 기능으로 대체하는 것입니다. 이를 통해 이론적 분석을 통해 추가 센싱 및 제어 없이도 신뢰할 수 있고 예측 가능한 로봇 운동(따라서 화물 운송)을 확인했습니다.\n\n우리의 가설을 검증하기 위해 우리는 천둥 벌레에 영감을 받은 로봇을 제작했습니다. 우리는 다리를 더 추가할수록 로봇이 추가 센싱이나 제어 기술 없이도 불균일한 표면을 효과적으로 이동할 수 있게 되는 것을 발견했습니다. 구체적으로, 우리는 일련의 실험을 수행했으며, 불균일한 자연 환경을 모방하는 지형을 구축하여로봇의 이동 성능을 평가했습니다. 우리는 다리 수를 6개로 시작하여 총 16개가 되기까지 2의 증분으로 다리 수를 점진적으로 늘려가며 로봇의 이동 성능을 평가했습니다.\n\n다리 수가 증가함에 따라 로봇은 센서 없이도 지형을 이동할 때 높은 민첩성을 보였습니다. 그 성능을 추가로 평가하기 위해 실제 지형에서 야외 테스트를 수행하여 더 현실적인 조건에서의 성능을 확인했는데, 그 결과도 좋았습니다. 많은 다리를 가진 로봇을 농업, 우주 탐사 및 수색 및 구조 작업에 사용할 수 있는 잠재력이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 왜 중요한가요\n\n음식, 연료, 건축 자재, 의약품을 운송하는 것은 현대 사회에 꼭 필요한 일이며, 효율적인 화물 교환이 상업 활동의 기초입니다. 여러 세기 동안 물자를 땅 위에서 운송하는 것은 도로와 추적선을 건설해야 했습니다. 그러나 도로와 추적선이 어디나 구비되어 있는 것은 아닙니다. 언덕 지역과 같은 곳은 화물에 제한적인 접근권을 가졌습니다. 로봇은 이러한 지역에서 화물을 운송하는 방법일 수 있습니다.\n\n# 이 분야에서 다른 연구는 무엇이 진행 중인가요\n\n다른 연구자들은 최근 몇 년 동안 더욱 민첩해진 인간형 로봇과 로봇 개를 개발해왔습니다. 이러한 로봇은 자신이 어디에 있는지와 자신 앞에 무엇이 있는지 정확한 센서에 의존하여 이를 알고, 그 뒤에 어떻게 항해할지에 대한 결정을 내립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 그들의 환경 인식에 대한 강한 의존으로, 예측할 수 없는 환경에서는 제한될 수 있습니다. 예를 들어, 수색 및 구조 작업에서 센서가 손상을 입을 수 있고 환경이 변할 수 있습니다.\n\n# 다음은 무엇인가요?\n\n저와 제 동료들은 연구에서 얻은 소중한 통찰을 가지고 농업 분야에 적용했습니다. 우리는 이 로봇을 사용하여 농지에서 효율적으로 잡초를 뽑는 회사를 설립했습니다. 이 기술을 계속 발전시키는 과정에서, 로봇의 디자인과 기능성에 초점을 맞추고 있습니다.\n\n우리는 백종 로봇의 기능적 측면을 이해하고 있지만, 계속되는 노력은 외부 센싱에 의존하지 않고 움직임에 필요한 최적의 다리 수를 결정하는 데 집중되어 있습니다. 우리의 목표는 비용 효율성과 시스템의 혜택을 유지하는 사이의 균형을 맞추는 것입니다. 현재 우리는 이 로봇이 효과적으로 작동하려면 최소 12개의 다리가 필요하다는 것을 보여 주었지만, 아직 이상적인 수를 조사 중에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사는 학술 전문가의 지식을 공유하기 위해 전논허 비영리 뉴스 기관인 더 컨버세이션(The Conversation)에서 제공한 것입니다. 저희에 대해 더 알고 싶으시거나 주간 과학 편집자 선정 기사를 구독하고 싶으시다면 저희 웹사이트를 방문해주세요.\n\n이 기사를 읽는 데 즐거우셨나요? 그렇다면 박수로 응원해주시고 이 페이지 우측 상단의 \"팔로우\" 버튼을 클릭해 주세요. 그렇게 하면 전문 연구자들이 쓴 저희 기사가 여러분의 피드에 자주 나타날 것입니다.\n\n저자는 NSF-Simons Southeast Center for Mathematics and Biology (Simons 재단 SFARI 594594), Georgia Research Alliance (GRA.VL22.B12), Army Research Office (ARO) MURI 프로그램, Army Research Office Grant W911NF-11-1-0514 및 던 가족 교수 자리로부터 자금 지원을 받았습니다. 저자와 동료들은 이 기사에서 다룬 연구와 관련한 하나 이상의 특허 출원을 진행 중입니다. 또한 저자와 동료들은 이 작업을 바탕으로 일부 원천 기업인 Ground Control Robotics, Inc.를 설립했습니다.","ogImage":{"url":"/assets/img/2024-06-20-Mobilerobotsgetalegupfromamore-is-bettercommunicationsprinciple_0.png"},"coverImage":"/assets/img/2024-06-20-Mobilerobotsgetalegupfromamore-is-bettercommunicationsprinciple_0.png","tag":["Tech"],"readingTime":3},{"title":"ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기","description":"","date":"2024-06-20 17:48","slug":"2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch","content":"\n\n\u003cimg src=\"/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png\" /\u003e\n\n이 튜토리얼에서는 PyTorch에서 미리 학습된 딥 러닝 모델을 사용하여 실시간 인간 포즈 추정을 위한 ROS 2 노드를 만들겠습니다. 이 노드는 웹캠 이미지를 구독하고 포즈 추정을 수행한 뒤 주석이 달린 이미지를 발행할 것입니다. 구현 세부 내용으로 들어가 봅시다.\n\n## 요구 사항\n\n시작하기 전에 다음이 설치되어 있는지 확인하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- ROS 2\n- Python 3\n- PyTorch\n- torchvision\n- OpenCV\n- cv_bridge (ROS 패키지로 ROS와 OpenCV 이미지 간 변환을 위한 것)\n\n모델: keypointrcnn_resnet50_fpn\n\n우리는 torchvision의 keypointrcnn_resnet50_fpn 모델을 사용합니다. 이 모델은 사람 자세 추정을 위해 설계되어 여러 신체 부위의 키포인트를 예측합니다. 여기에 이 모델의 구성 요소가 있습니다:\n\n- ResNet-50 백본: ResNet-50는 특징 추출기로 작용하는 합성곱 신경망입니다. 공간적 계층을 효과적으로 캡처하는 데 알려져 있습니다.\n- FPN (Feature Pyramid Network): FPN은 다중 스케일에서 특성 맵을 구축하여 감지 능력을 향상시킵니다.\n- Keypoint R-CNN: 이 Faster R-CNN의 변형은 바운딩 박스 외에도 키포인트를 감지하는 데 특화되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*LbBdaJJRpnNGRLdKExdzdw.gif\" /\u003e\n\n자율 이동 로봇(AMR) 및 로보틱스에서 실시간 포즈 추정의 응용\n\n포즈 추정은 로보틱스 분야에서 강력한 도구이며 자율 이동 로봇(AMR)의 성능을 크게 향상시킬 수 있습니다. 실시간 포즈 추정을 통합함으로써 로봇은 상황 인식, 인간과의 상호 작용, 다양한 작업에서의 성능을 향상시킬 수 있습니다. 다음은 주요 응용 분야 몇 가지입니다:\n\n- 협업 로봇 (Cobots)\n- 감시 및 보안\n- 제조 및 조립 라인\n- 내비게이션 및 장애물 회피\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계별 실행\n\n- 노드 초기화 및 구독\n\n우리는 필요한 라이브러리를 가져오는 것으로 시작합니다. 이에는 ROS 2 Python 클라이언트 라이브러리 (rclpy), ROS 메시지 종류 (Image), 이미지 변환을 위한 CvBridge, 그리고 딥러닝을 위한 PyTorch 및 torchvision이 포함됩니다.\n\n2. 노드 클래스 생성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPoseEstimationNode 클래스를 정의하고 Node를 상속합니다. 생성자에서는:\n\n- 노드를 pose_estimation_node으로 이름을 지정하여 초기화합니다.\n- 이미지를 수신하기 위해 /jetson_webcam 토픽에 구독합니다.\n- 이미지_pose 토픽에 주석 처리된 이미지를 게시할 발행자를 만듭니다.\n- ROS 및 OpenCV 이미지 간 변환을 위해 CvBridge를 초기화합니다.\n- torchvision에서 사전 학습된 자세 추정 모델 keypointrcnn_resnet50_fpn을 평가 모드로 설정하여 로드합니다.\n- 이미지를 텐서로 변환하기 위한 변환을 정의합니다.\n\n3. 수신된 이미지 처리\n\nlistener_callback 메서드에서 수신된 이미지를 처리합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 받은 이미지의 인코딩을 기록합니다.\n- CvBridge를 사용하여 이미지를 ROS 형식에서 OpenCV 형식으로 변환하고 다양한 이미지 인코딩을 처리합니다.\n\n4. 포즈 추정\n\n다음으로, OpenCV 이미지를 PIL 이미지로 변환하고 텐서로 변환하기 위한 변환이 적용됩니다. 이 텐서를 모델에 전달하여 예측을 얻고, torch.no_grad()를 사용하여 기울기 계산이 이루어지지 않도록 합니다.\n\n5. 이미지 주석 및 게시\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그런 다음 이미지에서 주요 지점 위치에 원을 그립니다. 이러한 주요 지점은 모델의 예측에서 추출되어 OpenCV로 그리기 위해 numpy 배열로 변환됩니다. 마지막으로 주석이 달린 이미지를 ROS 메시지로 변환하여 발행합니다.\n\n![이미지](/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_1.png)\n\n6. 노드 실행하기\n\nmain 함수는 ROS 2 Python 클라이언트 라이브러리를 초기화하고 노드의 인스턴스를 생성한 다음 종료될 때까지 작동하도록 유지하도록 되어 있습니다. 그 후에는 노드가 제거되고 ROS 2 컨텍스트가 종료됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Building a Pose Estimation Node with ROS 2](/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_2.png)\n\n# 결론\n\n본 튜토리얼을 따라하면 최신 딥러닝 기법을 사용하여 실시간 포즈 추정이 가능한 견고한 ROS 2 노드를 만들 수 있습니다. 이 설정은 인간-로봇 상호작용 및 감시를 포함한 다양한 로봇 응용 프로그램으로 확장할 수 있습니다.\n\n실시간 포즈 추정은 AMR 및 로봇의 능력에 새로운 차원을 추가하여 인간의 동작 및 자세를 이해하고 반응할 수 있습니다. 이 기능은 인간-로봇 상호작용을 향상시키며 안전성을 향상시키고 로봇이 자율적으로 또는 협업적으로 수행할 수 있는 작업 범위를 확장시킵니다. 기술이 계속 발전함에 따라 로봇학의 다양한 분야에서 더 많은 혁신적인 응용 프로그램이 예상됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# NVIDIA Jetson 플랫폼에서 ROS 2 및 인공지능을 이용한 로봇 응용 프로그램 구현\n\nhttps://developer.nvidia.com/blog/implementing-robotics-applications-with-ros-2-and-ai-on-jetson-platform-2/#ros_2_nodes_for_human_pose_estimation","ogImage":{"url":"/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png"},"coverImage":"/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png","tag":["Tech"],"readingTime":4},{"title":"라즈베리 파이를 사용한 자동 번호판 인식","description":"","date":"2024-06-20 17:43","slug":"2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi","content":"\n\n\n\u003cimg src=\"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_0.png\" /\u003e\n\n# 소개\n\n이 프로젝트의 목표는 Raspberry Pi 마이크로 컴퓨터를 사용하여 주차 장벽을 제어하기 위한 자동 번호판 인식 시스템을 설계하는 것입니다.\n\n왜 이 프로젝트를 하게 되었을까요?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어딘가에 참여 중인 프로젝트가 없는 Rpi가 하나 있고 카메라와 잠재적인 고민이 있는데요 ― 사무실 주차장에 자동 주차 장벽 제어 시스템이 없습니다. 그러니 이 프로젝트를 시작해보는 건 어떨까요?\n\n이 프로젝트의 목적은 생산에 적합한 안정적이고 경쟁력있는 솔루션을 만드는 것이 아니라, 한정된 장비를 사용하여 실제 문제를 위한 작동 제품을 만들면서 재미를 느끼는 것입니다. 그리고 그 이후에는 이 솔루션을 경량 엣지 디바이스에서 빠르게 작동하도록 최적화하는 재미도 봅시다)\n\n![image](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_1.png)\n\n일반적인 아이디어는 Rpi 카메라를 사용하여 일정 주기로 사진을 촬영하고, 이미지를 처리하여 차량 번호판을 감지하고 문자를 인식한 다음 데이터베이스에서 허용된 번호 목록과 비교하는 것입니다. 목록의 번호판과 일치한다면 장벽이 열릴 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기본적인 단계에서는 다음 도구를 사용할 것입니다: \n\n- 이미지 소스 — Raspberry Pi Camera 모듈 v2;\n- 번호판 검출기 — pyTorch를 사용하여 제공되는 Yolo v7;\n- 광필 인식 (OCR) — EasyOCR;\n- \"데이터베이스\" — Google 시트의 테이블;\n\n모든 처리 작업과 계산은 Raspberry Pi 4b에서 로컬로 실행되어야 하며, 이 솔루션은 자율적으로 작동해야합니다.\n\n![이미지](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n라즈베리 파이는 \"거의 실시간\"으로 Pi 카메라에서 프레임을 계속해서 읽습니다. 그런 다음, 사용자 정의 데이터셋 YOLOv7 모델을 미세 조정하여 번호판이 있는 영역을 감지합니다. 그 다음, 필요한 경우 이미지 전처리를 제공하고 EasyOCR 모델이 제공된 자르기된 프레임에서 번호를 감지합니다. 그런 다음 번호판 문자열을 \"데이터베이스\"에 저장된 번호판 중 어느 것과 일치하는지 확인하고 해당 작업을 실행합니다. 라즈베리 GPIO (General-Purpose Input-Output) - 제어 릴레이 스위치를 사용하여 주차장 장벽과 빛 등 추가 부하를 연결할 수 있습니다.\n\nGPIO 핀을 사용하면 입력 센서 (IR, PIR와 같은)를 연결하고 자동차가 감지됐을 때에만 카메라를 작동시킬 수 있습니다.\n\n이 작업은 여러 가지 방법으로 해결할 수 있습니다. 일부 방법은 특정 요구 사항과 사용 사례에 더 효율적이고 간편할 수 있습니다. 예를 들어, 모든 중요한 처리를 클라우드에서 수행하거나 GPU 기반 엣지 장치를 사용하거나 다른 모델을 사용할 수 있습니다. ONNX, TFLite 등을 사용하여 제공할 수도 있습니다. 그러나 이 프로젝트는 실험으로 진행되었고, 현재 사용 가능한 장비를 사용했으며, 쉬운 방법을 찾는 것이 아니었습니다 =)\n\n# 환경 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 하드웨어 디자인\n\n필수 하드웨어:\n\n- 카메라 — Raspberry Pi Camera 모듈 v2 (Sony IMX219 8MPx, 1080p30, 720p60)\n- 엣지 디바이스 — Raspberry Pi 4 모델 B 4GB (CPU: Broadcom BCM2711, 쿼드 코어 Cortex-A72 (ARM v8) 64비트 SoC @ 1.5GHz; RAM: 4GB LPDDR4–3200 SDRAM; 40핀 GPIO 헤더; 2.4 GHz/5.0 GHz 802.11ac Wi-Fi, 블루투스 5.0)\n- SD 카드 (8GB)\n- 전원 공급 장치 — 5V 3A USB-C\n\n![image](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n추가 내용:\n\n- 열흡수기, 냉각 팬\n- UPS\n- 디스플레이 (Waveshare 2.7인치 e-Paper HAT)\n- 외부 장치(장벽) 제어용 릴레이 / Raspberry HAT\n- 카메라 마운트 (\"카메라용 독특한 금속 와이어 마운트\" :))\n\n* 색깔 다시 채워주는 시간이 괜찮은 TFT 또는 OLED 유형의 화면을 사용하는 것이 좋지만, 그 당시에는 이 것만 사용할 수 있었습니다.\n\n[이미지](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n설정하기\n\nPyTorch를 사용하여 솔루션을 만들기로 결정했으므로, Arm 64비트(aarch64)용 pip 패키지만 제공되므로 64비트 버전의 OS(데비안 버전: 11 - “Bullseye”)를 설치해야 합니다.\n\n최신 arm64 라즈베리 파이 OS는 공식 사이트에서 다운로드할 수 있으며 rpi-imager를 통해 설치할 수 있습니다.\n\n설치가 완료되면 다음과 같아야 합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_5.png\" /\u003e\n\n라즈베리 파이에 SD 카드를 삽입한 후 부팅하면 다음과 같은 설정을 수행해야 합니다:\n\n/boot/config.txt 파일을 수정하여 카메라를 활성화합니다.\n\n```js\n# 이는 카메라와 같은 확장 기능을 사용하도록합니다.\nstart_x=1\n# 카메라 처리에 적어도 128M이 필요하며 더 크면 그대로 둘 수 있습니다.\ngpu_mem=128\n# 기존의 camera_auto_detect 줄을 주석 처리/삭제해야합니다. 이것은 OpenCV/V4L2 캡처에서 문제를 일으킵니다.\n#camera_auto_detect=1\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한 아마 I2C, SSH 및 VNC을 활성화하려고 할 것입니다. 이 작업은 raspi-config 또는 GUI에서 할 수 있습니다.\n\n![image](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_6.png)\n\n요구 사항 설치\n\n저는 Python 버전 3.9 및 3.10을 사용했습니다. 일부 경우에 따르면 3.11 버전이 더 빠르다고 보고되지만 아직 안정적인 PyTorch가 3.11에는 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`requirements.txt` 파일을 사용하여 pip 패키지 관리자를 통해 모든 필요한 라이브러리와 모듈을 설치하세요:\n\n\nmatplotlib\u003e=3.2.2\nnumpy\u003e=1.18.5\nopencv-python==4.5.4.60\nopencv-contrib-python==4.5.4.60\nPillow\u003e=7.1.2\nPyYAML\u003e=5.3.1\nrequests\u003e=2.23.0\nscipy\u003e=1.4.1\ntorch\u003e=1.7.0,!=1.12.0\ntorchvision\u003e=0.8.1,!=0.13.0\ntqdm\u003e=4.41.0\nprotobuf\u003c4.21.3\ntensorboard\u003e=2.4.1\npandas\u003e=1.1.4\nseaborn\u003e=0.11.0\neasyocr\u003e=1.6.2\n\n\n수동으로 직접 설치하거나 기존 환경에 구현할 경우 (하지 마세요 :)), 현재 OpenCV 버전에 문제가 있으므로 정확한 버전 4.5.4.60을 설치해야 합니다.\n\n모든 것이 올바르게 설치되었는지 확인하려면 `pip list` 명령어를 사용하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`\u003cimg src=\"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_7.png\" /\u003e`\n\n그럼, 하드웨어와 환경을 설정해놓았으니 코딩을 시작해봅시다.\n\n# 소프트웨어 설계\n\n이미지 캡쳐\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이미지 캡처를 위해 표준 picamera 라이브러리 대신 OpenCV를 사용하여 비디오 프레임을 스트리밍할 것입니다. 64비트 OS에서 picamera 라이브러리를 사용할 수 없고 그 속도도 느립니다. OpenCV는 직접 /dev/video0 장치에 액세스하여 프레임을 캡처합니다.\n\nOpenCV 카메라 읽기를 위한 사용자 정의 간단한 래퍼:\n\n```python\nclass PiCamera():\n    def __init__(self, src=0, img_size=(640,480), fps=36, rotate_180=False):\n        self.img_size = img_size\n        self.fps = fps\n        self.cap = cv2.VideoCapture(src)\n        #self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n        #self.cap.set(cv2.CAP_PROP_FPS, self.fps)\n        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.img_size[0])\n        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.img_size[1])\n        self.rotate_180 = rotate_180\n    def run(self):       \n        # 프레임 읽기\n        ret, image = self.cap.read()\n        if self.rotate_180:\n            image = cv2.rotate(image, cv2.ROTATE_180)\n        if not ret:\n            raise RuntimeError(\"프레임 읽기 실패\")\n        return image \n```\n\n여기서 카메라가 뒤집혀 있기 때문에 `image = cv2.rotate(image, cv2.ROTATE_180)`를 사용하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n버퍼 크기와 FPS 설정은 랙을 고치고 프레임 스트림을 적절하게 정렬하는 데 사용할 수 있습니다. 그러나 제 경우에는 카메라 제조사 및 프레임을 읽는 데 사용된 백엔드에 따라 달라서 작동하지 않습니다.\n\n카메라에서 이미지가 캡처된 후, 우리는 번호판을 감지하는 작업을 시작하여 이미지를 처리해야 합니다.\n\n번호판 감지 모듈\n\n이 작업에는 YOLOv7 사전 훈련된 모델을 사용할 것입니다. 이 모델을 사용하여 사용자 지정 번호판 데이터 세트에 대해 미세 조정할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nYOLOv7는 정확성과 속도 측면에서 최신 기술인 실시간 객체 감지 알고리즘입니다. COCO 데이터셋에 미리 학습되어 있습니다.\n\n이 알고리즘에 대한 자세한 내용은 다음 논문에서 확인할 수 있어요: YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.\n\n![이미지](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_8.png)\n\n공식 저장소에서 YOLOv7 레포지토리를 복제해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ngit clone https://github.com/WongKinYiu/yolov7.git\ncd yolov7\n```\n\n요로 요구 사항은 위에서 설치한 프로젝트 요구 사항에 이미 흡수되었습니다.\n\nFine-tuning을 위해 YOLOv7의 사전 훈련된 작은 버전인 이미지 크기 640을 적용하겠습니다.\n\n```js\n# 사전 훈련된 가중치 다운로드\n!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기본 사전 훈련된 물체 탐지:\n\n![image](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_9.png)\n\nNumberplate Detection Model training\n\n커스텀 데이터셋에 대한 모델 훈련은 꽤 간단하고 직관적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n친구야, 좋은 GPU를 이용해 Google Colab에서 모델 파인 튜닝을 진행할 거야.\n\n시작하기 전에 단일 번호판 클래스로 적절한 데이터셋을 생성하고 레이블을 지정해야 해.\n\n나의 데이터셋은 나만의 사진을 기반으로 부분적으로 만들었으며 AUTO.RIA Numberplate Dataset에서 일부를 활용했어 (이 멋진 분들에게 감사합니다!). 총 2000장의 이미지를 사용했어.\n\n레이블링은 Yolo 포맷으로 roboflow 서비스를 통해 진행했어.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```yaml\ntrain: dataset/train\nval: dataset/valid\n# Classes\nnc: 1  # number of classes\nnames: ['numberplate']  # class names\n```\n\n모델을 훈련하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```yaml\npython train.py --epochs 25 --workers 8 --device 0 --batch-size 32 --data data/numberplates.yaml --img 640 640 --cfg cfg/training/yolov7.yaml --weights 'yolov7-tiny.pt' --name yolov7_tiny_numberplates --hyp data/hyp.scratch.tiny.yaml\n``` \n\nBaseline으로 25회의 에포크가 충분하다고 결정했어요.\n\n![AutomaticNumberPlateRecognitionwithRaspberryPi_11.png](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_11.png)\n\n추론:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_12.png\" /\u003e\n\n프로젝트의 첫 번째 버전으로는 충분해 보이지만, 실제 응용 프로그램 중 발견된 특수 사례들을 통해 나중에 업데이트할 수 있습니다.\n\nYOLOv7 디텍터를 위한 추상적인 간단한 래퍼 클래스를 생성하였습니다:\n\n```js\nclass Detector():\n    def __init__(self, model_weights, img_size=640, device='cpu', half=False, trace=True, log_level='INFO', log_dir = './logs/'):\n        # 초기화\n        self.model_weights = model_weights\n        self.img_size = img_size\n        self.device = torch.device(device)\n        self.half = half  # half = device.type != 'cpu'  # half precision only supported on CUDA\n        self.trace = trace  # 모델을 Traced-모델로 변환\n        self.log_level = log_level\n        if self.log_level:\n            self.num_log_level = getattr(logging, self.log_level.upper(), 20) ##log_level 입력 문자열을 로깅 모듈이 허용하는 값 중 하나로 변환합니다. 값이 없다면 20 - INFO로 설정됩니다.\n            self.log_dir = log_dir\n            log_formatter = logging.Formatter(\"%(asctime)s %(message)s\")\n            logFile = self.log_dir + 'detection.log'\n            my_handler = RotatingFileHandler(logFile, mode='a', maxBytes=25 * 1024 * 1024,\n                                             backupCount=10, encoding='utf-8', delay=False)\n            my_handler.setFormatter(log_formatter)\n            my_handler.setLevel(self.num_log_level)\n            self.logger = logging.getLogger(__name__)  \n            self.logger.setLevel(self.num_log_level)\n            self.logger.addHandler(my_handler)\n        # YOLO 모델의 경로를 추가합니다. ('weights.pt')를 로드할 때마다, pytorch는 path 환경 변수(models/yolo)에서 모델 구성을 찾습니다.\n        yolo_folder_dir = str(Path(__file__).parent.absolute()) +\"\\yolov7\" #  모델 폴더 경로\n        sys.path.insert(0, yolo_folder_dir)\n        # 모델 로드\n        self.model = attempt_load(self.model_weights, map_location=self.device)  # FP32 모델 로드\n        # 모델을 Traced-모델로 변환\n        if self.trace:\n            self.model = TracedModel(self.model, self.device, self.img_size)\n        # if half:\n        #     model.half()  # to FP16\n        # 이름과 색상 가져오기\n        self.names = self.model.module.names if hasattr(self.model, 'module') else self.model.names\n        if len(self.names) \u003e 1:\n            self.colors = [[0, 255, 127]] + [[random.randint(0, 255) for _ in range(3)] for _ in self.names[1:]]\n        else:\n            self.colors = [[0, 255, 127]]\n        sys.path.remove(yolo_folder_dir)\n    def run(self, inp_image, conf_thres=0.25):\n        # 추론 실행\n        # 데이터 로드\n        dataset = LoadImage(inp_image, device=self.device, half=self.half)\n        t0 = time.time()\n        self.file_name, self.img, self.im0 = dataset.preprocess()\n        # 추론\n        t1 = time.time()\n        with torch.no_grad():  # 그래디언트를 계산하면 GPU 메모리 누수가 발생할 수 있습니다\n            self.pred = self.model(self.img)[0]\n        t2 = time.time()\n        # NMS 적용\n        self.pred = non_max_suppression(self.pred, conf_thres=conf_thres)\n        t3 = time.time()\n        # 검출 처리\n        bbox = None  # 최대 Confidence를 가진 검출 객체의 바운딩 상자\n        cropped_img = None  # 최대 Confidence를 가진 검출 객체를 자른 이미지\n        det_conf = None  # 최대 Confidence를 가진 검출 객체의 신뢰 수준\n        self.det = self.pred[0]  # pred[0] - NMX suppr는 이미지 당 1개의 텐서를 반환합니다;\n        if len(self.det):\n            # img_size에서 im0 크기로 상자 크기 조정\n            self.det[:, :4] = scale_coords(self.img.shape[2:], self.det[:, :4], self.im0.shape).round()\n            # 결과 출력\n            print_strng = \"\"\n            for c in self.det[:, -1].unique():\n                n = (self.det[:, -1] == c).sum()  # 클래스 당 검출\n                print_strng += f\"{n} {self.names[int(c)]}{'s' * (n \u003e 1)}\"  # 문자열에 추가\n            # 시간 출력(추론 + NMS)\n            print(\n                f'{print_strng} 검출. ({(1E3 * (t1 - t0)):.1f}ms)-데이터 로드, ({(1E3 * (t2 - t1)):.1f}ms)-추론, ({(1E3 * (t3 - t2)):.1f}ms)-NMS')\n            # 디버그 모드이면 결과를 파일에 기록\n            if self.log_level:\n                self.logger.debug(\n                    f'{self.file_name} {print_strng} 검출. ({(1E3 * (t1 - t0)):.1f}ms)-Load data, ({(1E3 * (t2 - t1)):.1f}ms)-Inference, ({(1E3 * (t3 - t2)):.1f}ms)-NMS')\n                if self.logger.getEffectiveLevel() == 10:  # 레벨 10 = 디버그\n                    gn = torch.tensor(self.im0.shape)[[1, 0, 1, 0]]  # 정규화 gain whwh\n                    for *xyxy, conf, cls in reversed(self.det):\n                        # 바운딩 박스와 함께 xywh 형식으로 검출 저장\n                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # 정규화된 xywh\n                        line = (int(cls), np.round(conf, 3), *xywh)  # 라벨 형식\n                        self.logger.debug(f\"{self.file_name} {('%g ' * len(line)).rstrip() % line}\")\n            # 최대 Confidence를 가진 검출 찾기:\n            indx = self.pred[0].argmax(0)[\n                4]  # pred[0] - NMX suppr는 이미지 당 1개의 텐서를 반환; argmax(0)[4] - conf는 [x1,y1,x2,y2,conf,cls]에서 indx 4를 가짐\n            max_det = self.pred[0][indx]\n            # 검출 바운딩 상자와 해당 자른 이미지 수집\n            bbox = max_det[:4]\n            cropped_img = save_crop(max_det[:4], self.im0)\n            cropped_img = cropped_img[:, :, ::-1] # BGR to RGB\n            det_conf = max_det[4:5]\n        print(f'검출 총 시간: {time.time() - t0:.3f}s')\n        return {'file_name': self.file_name, 'orig_img': self.im0, 'cropped_img': cropped_img, 'bbox': bbox,\n                'det_conf': det_conf}\n```    \n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n디버깅 목적을 위해 로깅 감지 데이터를 파일에 활성화할 수 있는 기능을 추가했습니다. 각 파일의 최대 크기는 25Mb이며 최대 10개의 파일을 저장한 후 덮어쓰기합니다.\n\n현재 작업에서는 감지기가 가장 높은 신뢰 점수를 가진 단일 감지만 반환하도록 설정해야 합니다. 감지기는 원본 이미지, 해당 경계 상자와 함께 자르기 감지된 영역, 신뢰 점수, 그리고 디버깅을 용이하게 하기 위해 각 이미지마다 생성된 고유한 이름을 출력합니다.\n\n번호판 영역 이미지 전처리\n\n일반적으로 다음 단계는 특정 이미지 전처리(예: RGB에서 그레이스케일로 변환, 노이즈 제거, 침식 + 팽창, 임계 처리, 히스토그램 평활화 등)를 수행하여 다음 OCR 단계를 위해 준비하는 것입니다. 전처리 작업은 OCR 솔루션 및 촬영 조건에 매우 의존하며 이에 맞게 조정됩니다. 그러나 EasyOCR로 이 기준 버전을 수행 중이며(나중에 사용자 지정 솔루션으로 대체해야 합니다), 저는 그레이스케일 변환 및 투영 프로필 방법을 이용한 기울기 보정이라는 두 가지 범용적인 단계로 전처리를 제한하기로 결정했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서는 평면 각도 보정을 사용하고 있지만 나중에는 원래의 번호판 모서리 탐지기와 호모그래피 계산 및 원근 변환을 사용한 보정으로 업데이트해야 합니다.\n\n```js\n# Skew Correction (projection profile)\ndef _find_score(arr, angle):\n    data = rotate(arr, angle, reshape=False, order=0)\n    hist = np.sum(data, axis=1)\n    score = np.sum((hist[1:] - hist[:-1]) ** 2)\n    return hist, score\n\ndef _find_angle(img, delta=0.5, limit=10):\n    angles = np.arange(-limit, limit+delta, delta)\n    scores = []\n    for angle in angles:\n        hist, score = _find_score(img, angle)\n        scores.append(score)\n    best_score = max(scores)\n    best_angle = angles[scores.index(best_score)]\n    print(f'Best angle: {best_angle}')\n    return best_angle\n\ndef correct_skew(img):\n    # correctskew\n    best_angle = _find_angle(img)\n    data = rotate(img, best_angle, reshape=False, order=0)\n    return data\n```\n\n![이미지](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_13.png)\n\n위 이미지 처리 단계 이후에는 인식을 위해 충분히 좋은 이미지로 간주할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n번호판 인식 (OCR)\n\n기준으로 EasyOCR 솔루션을 사용하기로 결정했어요. 쓰기 편하고 인식 정확도가 높아서 그리고 지루한 테서랙트에 비해 내가 알고 있는 괜찮은 대체재인 것 같아서요)\n\nEasyOCR을 이용한 번호판 인식을 위한 간단한 래퍼 클래스:\n\n```js\nclass EasyOcr():\n    def __init__(self, lang = ['en'], allow_list = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ', min_size=50, log_level='INFO', log_dir = './logs/'):\n        self.reader = easyocr.Reader(lang, gpu=False)\n        self.allow_list = allow_list\n        self.min_size = min_size\n        self.log_level = log_level\n        if self.log_level:\n            self.num_log_level = getattr(logging, log_level.upper(),\n                                         20)  ## log_level 입력 문자열을 로깅 모듈에서 허용하는 값 중 하나로 변환하고, 만약 없으면 20(INFO)으로 설정\n            self.log_dir = log_dir\n            # 로거 설정\n            log_formatter = logging.Formatter(\"%(asctime)s %(message)s\")\n            logFile = self.log_dir + 'ocr.log'\n            my_handler = RotatingFileHandler(logFile, mode='a', maxBytes=25 * 1024 * 1024,\n                                             backupCount=10, encoding='utf-8', delay=False)\n            my_handler.setFormatter(log_formatter)\n            my_handler.setLevel(self.num_log_level)\n            self.logger = logging.getLogger(__name__)  \n            self.logger.setLevel(self.num_log_level)\n            self.logger.addHandler(my_handler)\n\n    def run(self, detect_result_dict):\n        if detect_result_dict['cropped_img'] is not None:\n            t0 = time.time()\n            img = detect_result_dict['cropped_img']\n            img = ocr_img_preprocess(img)\n            file_name = detect_result_dict.get('file_name')\n            ocr_result = self.reader.readtext(img, allowlist = self.allow_list, min_size=self.min_size)\n            text = [x[1] for x in ocr_result]\n            confid = [x[2] for x in ocr_result]\n            text = \"\".join(text) if len(text) \u003e 0 else None\n            confid = np.round(np.mean(confid), 2) if len(confid) \u003e 0 else None   \n            t1 = time.time()\n            print(f'인식된 번호판: {text}, 신뢰도: {confid}.\\nOCR 총 시간: {(t1 - t0):.3f}s')\n            if self.log_level:\n                # 디버그 모드일 때 결과를 파일에 작성\n                self.logger.debug(f'{file_name} 인식된 번호판: {text}, 신뢰도: {confid}, OCR 총 시간: {(t1 - t0):.3f}s.')\n\n            return {'text': text, 'confid': confid}\n        else:\n            return {'text': None, 'confid': None}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n디버그 목적으로 Detector와 마찬가지로 OCR 데이터를 파일에 기록할 수 있는 기능도 추가되었다.\n\n인식 모듈은 인식된 문자열과 신뢰도 점수를 반환합니다.\n\n검증 및 조치\n\n검출된 번호판에서 성공적으로 인식된 텍스트를 가져왔으면, 이를 확인하고 일부 조치를 취해야 합니다. 번호판 확인 단계에서 가장 합리적인 일은 고객이 업데이트하는 데이터베이스를 사용하는 것입니다. 이 데이터베이스는 매번 또는 하루에 한 번씩 읽어서 로컬 저장소에 목록을 저장할 것입니다. 현재 기준 버전에서 데이터베이스를 설정하지 않고 주요 기능에 집중하기로 결정했습니다. 대신 Google Sheets를 예시로 사용할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_14.png\" /\u003e\n\n이 순간에는 아무런 조치 단계가 설정되어 있지 않습니다. 허용 목록에 있는 라이선스 번호 확인 결과만 표시됩니다. 하지만 라즈베리파이를 사용하면 GPIO 제어 릴레이 스위치를 통해 어떤 하중이든 매우 쉽게 작동시킬 수 있습니다.\n\n시각화\n\n해결책을 편안하게 모니터링하고 디버그할 수 있도록 시각화 모듈을 추가했습니다. 이 모듈은 번호판 인식 프로세스 표시, 입력 이미지 저장, 검출된 번호판이 있는 자르기된 영역 및 출력 결과 이미지 표시를 처리합니다. 또한, e-ink 스크린에 번호판 영역 및 인식된 텍스트를 표시하는 기능을 추가했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재, 편의를 위해 이미지는 압축된 JPG로 저장되며 로그 폴더에 10800개의 이미지가 한정된 양으로 저장됩니다(폴더 최대 크기 약 500Mb). 프로덕션 솔루션에서 시각화가 필요하지 않으며, 디버깅을 위해 이미지는 NumPy ndarrays나 이진 문자열에 저장하는 것이 더 좋습니다.\n\n```js\nclass Visualize():\n    def __init__(self, im0, file_name, cropped_img=None, bbox=None, det_conf=None, ocr_num=None, ocr_conf=None, num_check_response=None, out_img_size=(720,1280), outp_orig_img_size = 640, log_dir ='./logs/', save_jpg_qual = 65, log_img_qnt_limit = 10800):\n        self.im0 = im0\n        self.input_img = im0.copy()\n        self.file_name = file_name\n        self.cropped_img = cropped_img\n        self.bbox = bbox\n        self.det_conf = det_conf\n        self.ocr_num = ocr_num\n        self.ocr_conf = ocr_conf\n        self.num_check_response = num_check_response\n        self.out_img_size = out_img_size\n        self.save_jpg_qual = save_jpg_qual\n        self.log_dir = log_dir\n        self.imgs_log_dir = self.log_dir + 'imgs/'\n        os.makedirs(os.path.dirname(self.imgs_log_dir), exist_ok=True)\n        self.crop_imgs_log_dir = self.log_dir + 'imgs/crop/'\n        os.makedirs(os.path.dirname(self.crop_imgs_log_dir), exist_ok=True)\n        self.orig_imgs_log_dir = self.log_dir + 'imgs/inp/'\n        os.makedirs(os.path.dirname(self.orig_imgs_log_dir), exist_ok=True)\n        self.log_img_qnt_limit = log_img_qnt_limit\n\n        # Create blank image\n        h, w = self.out_img_size\n        self.img = np.zeros((h, w, 3), np.uint8)\n        self.img[:, :] = (255, 255, 255)\n\n        # Draw bounding box on top the image\n        if (self.bbox is not None) and (self.det_conf is not None):\n            label = f'{self.det_conf.item():.2f}'\n            color = [0, 255, 127]\n            plot_one_box(self.bbox, self.im0, label=label, color=color, line_thickness=3)\n\n        # Resize img width to fit the plot, keep origin aspect ratio\n        h0, w0 = im0.shape[:2]\n        aspect = w0 / h0\n        if aspect \u003e 1:  # horizontal image\n            new_w = outp_orig_img_size\n            new_h = np.round(new_w / aspect).astype(int)\n        elif aspect \u003c 1:  # vertical image\n            new_h = outp_orig_img_size\n            new_w = np.round(new_h * aspect).astype(int)\n        else:  # square image\n            new_h, new_w = outp_orig_img_size, outp_orig_img_size\n        self.im0 = cv2.resize(self.im0, (new_w, new_h), interpolation=cv2.INTER_AREA)\n        im0_h, im0_w = self.im0.shape[:2]\n\n        # Add original full image\n        im0_offset = 0\n        self.img[im0_offset:im0_h + im0_offset, im0_offset:im0_w + im0_offset] = self.im0\n\n        # Add cropped image with detected number bbox\n        if self.cropped_img is not None:\n            # Resize cropped img\n            target_width = int((w - (im0_w + im0_offset)) / 3)\n            r = target_width / self.cropped_img.shape[1]\n            dim = (target_width, int(self.cropped_img.shape[0] * r))\n            self.cropped_img = cv2.resize(self.cropped_img, dim, interpolation=cv2.INTER_AREA)\n            crop_h, crop_w = self.cropped_img.shape[:2]\n            # Add cropped img\n            crop_h_y1 = int(h/7)\n            crop_w_x1 = im0_w + im0_offset + int((w - (im0_w + im0_offset) - crop_w) / 2)\n            self.img[crop_h_y1:crop_h + crop_h_y1, crop_w_x1:crop_w + crop_w_x1] = self.cropped_img\n            # Add `_det` to filename\n            self.file_name = Path(self.file_name).stem + \"_det\" + Path(self.file_name).suffix\n\n        # Add ocr recognized number\n        if self.ocr_num is not None:\n            label = f\"{self.ocr_num} ({self.ocr_conf})\"\n            t_thickn = 2  # text font thickness in px\n            font = cv2.FONT_HERSHEY_SIMPLEX  # font\n            fontScale = 1.05\n            # calculate position\n            text_size = cv2.getTextSize(label, font, fontScale=fontScale, thickness=t_thickn)[0]\n            w_center = int((im0_w + im0_offset + w)/2)\n            ocr_w_x1 = int(w_center - text_size[0]/2)\n            ocr_h_y1 = int(crop_h_y1 + crop_h + 55)\n            org = (ocr_w_x1, ocr_h_y1)  # position\n            # Plot text on img\n            cv2.putText(self.img, label, org, font, fontScale,  color=(0, 0, 0), thickness=t_thickn, lineType=cv2.LINE_AA)\n\n        # Add number check response if in allowed list\n        if self.num_check_response == 'Allowed':\n            label = \"-=Allowed=-\"\n            fontColor = (0,255,0)\n        else:\n            label = \"-=Prohibited!=-\"\n            fontColor = (0,0,255)\n        t_thickn = 2  # text font thickness in px\n        font = cv2.FONT_HERSHEY_SIMPLEX  # font\n        fontScale = 1.05\n        # calculate position\n        text_size = cv2.getTextSize(label, font, fontScale=fontScale, thickness=t_thickn)[0]\n        w_center = int((im0_w + im0_offset + w) / 2)\n        response_w_x1 = int(w_center - text_size[0] / 2)\n        response_h_y1 = int(h*3/7) #TBD\n        org = (response_w_x1, response_h_y1)  # position\n        # Plot text on img\n        cv2.putText(self.img, label, org, font, fontScale, color=fontColor, thickness=t_thickn, lineType=cv2.LINE_AA)\n\n    def show(self):\n        # Show the image\n        cv2.imshow('image', self.img)\n\n    def save(self):\n        # Remove oldest file if reach quantity limit\n        if self.get_dir_file_quantity(self.imgs_log_dir) \u003e self.log_img_qnt_limit:\n            oldest_file = sorted([self.imgs_log_dir+f for f in os.listdir(self.imgs_log_dir)])[\n                0]  \n            os.remove(oldest_file)\n        # Write compressed jpeg with results\n        cv2.imwrite(f\"{self.imgs_log_dir}{self.file_name}\", self.img, [int(cv2.IMWRITE_JPEG_QUALITY), self.save_jpg_qual])\n\n    def save_input(self):\n        if self.input_img is not None:\n            # Remove oldest file if reach quantity limit\n            if self.get_dir_file_quantity(self.orig_imgs_log_dir) \u003e self.log_img_qnt_limit:\n                oldest_file = sorted([self.orig_imgs_log_dir+f for f in os.listdir(self.orig_imgs_log_dir)])[\n                    0]  \n                os.remove(oldest_file)\n            # Write compressed jpeg with results\n            cv2.imwrite(f\"{self.orig_imgs_log_dir}orig_inp_{self.file_name}\", self.input_img)\n\n    def save_crop(self):\n        if self.cropped_img is not None:\n            # Remove oldest file if reach quantity limit\n            if self.get_dir_file_quantity(self.crop_imgs_log_dir) \u003e self.log_img_qnt_limit:\n                oldest_file = sorted([self.crop_imgs_log_dir+f for f in os.listdir(self.crop_imgs_log_dir)])[\n                    0]  \n                os.remove(oldest_file)\n            # Write compressed jpeg with results\n            cv2.imwrite(f\"{self.crop_imgs_log_dir}crop_{self.file_name}\", self.cropped_img)\n\n    def display(self):\n        # Display img using e-ink display 176*264\n        disp_img = np.zeros((epd2in7.EPD_WIDTH, epd2in7.EPD_HEIGHT,3), np.uint8)\n        disp_img[:, :] = (255, 255, 255)\n        \n        if self.cropped_img is not None:\n            # Add cropped number\n            crop_resized = cv2.resize(self.cropped_img, (epd2in7.EPD_HEIGHT-4, 85), interpolation=cv2.INTER_AREA)\n            crop_resized_h, crop_resized_w = crop_resized.shape[:2]\n            crop_w_x1 = int(epd2in7.EPD_HEIGHT/2 - crop_resized_w/2)\n            disp_img[2:crop_resized_h+2, crop_w_x1:crop_resized_w+crop_w_x1] = crop_resized\n        \n        if\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지 이룩한 것을 시험해 봅시다. 정지 이미지에서의 탐지 및 인식 파이프라인:\n\n![image1](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_15.png)\n\n길거리에서 기기 카메라를 사용한 종단간 솔루션 테스트:\n\n![image2](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_16.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```\n![Image 1](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_17.png)\n\n![Image 2](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_18.png)\n\n![Image 3](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_19.png)\n\nPerformance\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 구성으로는 감지에 약 700~800ms, OCR 단계에 약 900~1200ms가 소요되며, 평균 FPS는 약 0.4~0.5입니다.\n\n![이미지](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_20.png)\n\n현재 주차 장벽 자동화 프로젝트에는 이러한 프레임 속도 값이 중요하지 않지만, 개선할 여지가 분명히 많습니다.\n\nhtop에서 CPU 활용률이 거의 100%에 가깝다는 것을 알 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_21.png)\n\n모든 테스트는 Raspberry Pi OS의 기본 설정으로 수행되었습니다. UI를 비활성화하고 기본적으로 활성화된 다른 백그라운드 서비스를 모두 제거하면 성능과 안정성이 높아집니다.\n\n보너스\n\n추가 조정 없이도 우리의 감지기 모듈은 LEGO 자동차의 번호판을 완벽하게 감지할 수 있다는 것이 밝혀졌습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지1](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_22.png)\n\n![이미지2](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_23.png)\n\n그래서 레고를 아들에게 빌려 Raspberry Pi Build Hat을 사용하여 나만의 주차장 바리어를 만들기로 결정했고, \"실제\" 조건에서 완전한 엔드 투 엔드 테스트를 제공하기로 했습니다.\n\nLEG 월드 햇 프로프라이어터리 라이브러리를 기반으로 한 Action 모듈용 간단한 랩퍼:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nclass Action():\n    def __init__(self):\n        self.motor = Motor('A')\n        self.motor.set_default_speed(25)\n        self.matrix = Matrix('B')\n        self.ok_color = [[(6, 10) for x in range(3)] for y in range(3)]\n        self.nok_color = [[(9, 10) for x in range(3)] for y in range(3)]\n        self.matrix.set_transition(2) #fade-in/out\n        self.matrix.set_pixel((1, 1), (\"blue\", 10))\n\n    def _handle_motor(self, speed, pos, apos):\n        print(\"Motor:\", speed, pos, apos)\n\n    def run(self, action_status):\n        while True:\n            if action_status[0] == 'Allowed':\n                self.matrix.set_pixels(self.ok_color)\n                time.sleep(1)\n                self.motor.run_for_degrees(-90, blocking=False)\n                time.sleep(5)\n                self.motor.run_for_degrees(90, blocking=False)\n                time.sleep(1)\n            elif action_status[0] == 'Prohibited':\n                self.matrix.set_pixels(self.nok_color)\n                time.sleep(3)\n            else:\n                self.matrix.clear()\n                self.matrix.set_pixel((1, 1), (\"blue\", 10))\n                time.sleep(1)\n                self.matrix.set_pixel((1, 1), (0, 10))\n                time.sleep(1)\n```\n\nMain 프로그램에서 action_status가 감지되고 변경될 때 메인 프로그램에서 액션을 트리거하여 병렬 스레드에서 이 모듈을 실행합니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_24.png\" /\u003e\n\nLEGO 번호판 중 하나를 Google 시트 \"데이터베이스\"에 추가했으므로 이제 모든 조각들을 함께 조합하여 실행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*ZHTFqk1E0pKLGAht0W_mnw.gif\" /\u003e\n\n# 최종 결론\n\n전반적으로 라즈베리 파이를 사용하여 주차장 장벽을 제어하기 위한 자동 번호판 인식 시스템을 완전히 구현하는 데 성공했습니다.\n\n강조해야 할 문제 중 하나는 처리 속도가 느린 관계로 이미지 지연이 발생할 수 있다는 점입니다. 카메라는 자체 버퍼가 있으며 이미지를 느린 속도로 캡처하는 동안 씬이 변경되어도 버퍼에서 여전히 \"이전\" 프레임을 읽는 문제가 있습니다. 현재 사용 사례에서는 그다지 중요하지 않지만 개선을 위해 전체 처리 시간과 거의 동일한 간격으로 프레임 스킵을 추가했습니다. 이렇게 하면 더 빠른 프레임 읽기와 버퍼의 정리가 가능하며 CPU의 부하를 줄일 수 있습니다. 그러나 지연 없이 거의 실시간 스무스한 이미지 스트리밍이 필요하다면 최상의 옵션은 카메라 읽기를 별도의 병렬 스레드로 설정하여 버퍼에서 가능한 최대 속도로 프레임을 읽도록 하는 것이며, 주 프로그램이 필요 시에만 이 프로세스에서 프레임을 가져 올 수 있도록 합니다. 그러나 파이썬에서 멀티 스레딩은 실제 다중 프로세스 처리가 아니라 아키텍처적 관점에서 코드를 보다 명확하게 조직화하고 실행하는 데 도움이 되는 시뮬레이션인 것을 기억해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 추가 단계\n\n- OCR. 현재 병목 현상인 OCR을 빠르게 처리할 수 있도록 개선해보세요. 나는 속도를 올리기 위해 작은 커스텀 RNN 기반 모델을 개발하기로 했습니다. 시간이 중요하지 않고 정확도만 필요한 경우 EasyOCR에서 다양한 모델을 사용하고 이를 여러분의 사례에 맞게 튜닝할 수 있습니다. 또는 WPOD-NET과 같은 다른 솔루션을 시도해볼 수도 있습니다. 또한 인식 품질을 향상시키는 중요한 포인트로는 정확한 사용 사례에 맞게 이미지 전처리를 조정하는 것이 있습니다.\n- Detector. 속도를 높이기 위해 카메라가 근거리에 있는 자동차에서만 작업해야 하는 경우 해상도가 높은 이미지가 필요하지 않습니다. 또 다른 옵션은 가능하다면 카메라와 차량의 위치가 대략 고정되어 있다면 전체 프레임이 아닌 번호판이 위치할 것으로 예상되는 영역만 캡처할 수 있습니다.\n\n나중에 이 두 모델 모두 전이 학습, 양자화, 가지치기 및 기타 방법을 사용하여 경량화하고 엣지 장치에서 더 빠르고 가벼운 작동이 가능하도록 할 수 있습니다.\n\n그러나 아무리 빠른 실시간 처리가 필수적이더라도 (물론 자동 주차 장벽에는 해당하지 않을 것입니다), 텐서 코어가 있는 장치가 없으면 NVIDIA Jetson과 같은 장치가 없으면 속도와 품질 사이에 항상 트레이드오프가 존재할 것입니다. CPU 전용 장치에서는 항상 속도와 품질 사이의 교환 관계가 발생할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 개선할 수 있는 다른 옵션이 있어요 — 현재 상황에서는 CPU를 24시간 7일 돌릴 필요가 없어요. 자동차가 다가올 때만 PIR 또는 IR 센서에 의해 카메라가 작동될 수 있어요.\n\n다음 번에 구현해보려고 하는 마지막 포인트 — 솔루션을 마이크로서비스로 전환하고 생산자-소비자 데이터 흐름 패턴을 구현할 거예요.\n\n그럼 이만 하겠습니다. 이 긴 지루한 프로젝트 구현 설명을 읽어 주셔서 감사해요.\n\n건강하게 지내시고 우크라이나를 응원해주세요 ❤.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 프로젝트에서 사용된 장비에 대한 링크:\n\n- Raspberry Pi 4 Model B 4GB\n- Raspberry Pi Camera Module v2\n- Raspberry Pi 4 Aluminum Case with Dual Cooling Fan\n- GeeekPi(52pi) Raspberry Pi UPS EP-0136\n- 264x176 2.7인치 E-Ink 디스플레이 HAT for Raspberry Pi\n- Raspberry Pi Build HAT\n- LEGO 3x3 컬러 라이트 매트릭스\n- LEGO 작은 테크닉 직각 모터","ogImage":{"url":"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_0.png"},"coverImage":"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_0.png","tag":["Tech"],"readingTime":30},{"title":"라즈베리 파이가 이제 Hailo-8L M2 모듈로 AI를 처리할 준비가 되었습니다","description":"","date":"2024-06-20 17:41","slug":"2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module","content":"\n\n## 가젯\n\n![라즈베리파이](/assets/img/2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module_0.png)\n\n인공지능은 매우 높은 에너지와 자원을 필요로 한다는 이미지를 가지고 있습니다. 그래서 강력하고 고용량의 기계에 전념되어 왔습니다. 많은 기업들이 클라우드 및 전용 솔루션에 솔루션을 선택하였습니다.\n\n이 상황을 극복하기 위한 것이 라즈베리 파이 AI 키트입니다. 잘 알려진 마이크로 컴퓨터와 함께 만들어진 제안서입니다. 라즈베리 파이의 최신 기술과 Hailo사의 M.2 HAT+가 함께 포함되어 있으며, 라즈베리 파이 5 위에 Hailo-8L AI 가속기 모듈이 사전에 조립되어 있습니다. 이 제품은 예상대로 70달러에 구매 가능합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n복잡한 AI 비전 앱을 만들 수 있게 하여, 실시간으로 실행되고 저 지연시간 및 저 전력 소비로 동작하는 앱도 가능해질 것입니다. Hailo-8L 공변처리기는 물체 감지, 의미 및 인스턴스 세분화, 자세 추정, 얼굴 태깅 등의 인공 신경망을 실행할 수 있습니다. 이로써 주CPU는 다른 작업을 처리할 수 있게 됩니다.\n\n그 결과 이 제안은 라즈베리 파이 사용자들의 요구를 충족할 수 있을 것입니다. 이 제품의 낮은 가격은 이 마이크로컴퓨터의 철학과 일치하며, 이제까지 존재하지 않았던 많은 기능과 새로운 기능을 제공할 수 있습니다.\n\n![2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module_1.png](/assets/img/2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module_1.png)\n\n이는 AI가 매우 흥미로운 영역 중 하나를 더 다루는데 도움이 될 것입니다. 소규모 프로젝트에 대해 이 솔루션이 매우 저렴하고, 이제 이미지를 인식하고 해석된 이미지에 대해 작용할 수 있는 능력을 지니고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n동일한 시간에, 이 새로운 기능을 통해 라즈베리 파이는 다시 한 번 독특한 위치에 있습니다. 사용자들은 더 완벽하고 훨씬 더 비싼 솔루션을 필요로하지 않고 매우 중요한 것에 대처할 수 있는 인공지능 솔루션에 접근할 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module_0.png"},"coverImage":"/assets/img/2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module_0.png","tag":["Tech"],"readingTime":2},{"title":"라즈베리 파이 5 CPU 성능","description":"","date":"2024-06-20 17:40","slug":"2024-06-20-RaspberryPi5CPUPerformance","content":"\n\n라즈베리 파이 기기들은 사용하기 정말 재미있어요. 가격도 저렴하고 성능도 좋아서 다양한 문제를 해결하는 방법에 대한 예제를 쉽게 찾을 수 있어요. Pi 5가 9월 28일에 발표되었을 때 저는 그것을 만져보고 싶었어요.\n\n![라즈베리 파이 5 CPU 성능](/assets/img/2024-06-20-RaspberryPi5CPUPerformance_0.png)\n\n저는 대부분의 프로젝트가 CPU에 의존적이기 때문에 CPU 성능을 측정하여 시작했어요. 아래에서 Pi 5를 최근의 Intel 13세대, SiPeed Lichee Pi 4A, Microsoft/Qualcomm SQ3, Apple M1, 이전 라즈베리 파이 모델 4와 3B, 그리고 열년 전의 Intel i7-4770K 칩과 비교해보았어요.\n\n벤치마크에는 다양한 환경에서 작성, 빌드 및 실행된 하나의 앱을 사용했어요. 이 앱은 간단합니다 - 상대방이 유능하다면 틱택토에서 이길 수 없다는 것을 증명합니다. 이는 알파/베타 가지치기 알고리즘을 사용하여 3가지 고유한 시작 수를 평가합니다. 6493개의 판 상태가 검토되었어요. 변형에는 다음이 포함돼요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 각 대상 CPU에 대한 원시 어셈블리 버전이 있습니다. 코드를 더 최적화할 수 있다고 생각하지만, 각 CPU의 명령어 세트와 레지스터를 활용하려고 노력했습니다. Arm32, Arm64, RISC-V 64 및 AMD64와 같은 다양한 변형이 있습니다.\n- 6502, 8080, 8086 및 RISC-V 64 CPU에 대한 원시 어셈블리 버전은 다양한 에뮬레이터에서 실행됩니다: NTVAO (6502 + Apple 1), NTVCM (8080 + CP/M 2.2), NTVDM (8086 + DOS 3.3), RVOS (RISC-V 64 + Linux). 모든 에뮬레이터는 C++로 작성되었으며 대상 플랫폼의 원시 컴파일러를 사용하여 생성되었습니다.\n- 알고리즘의 C++ 버전이 있습니다. 플랫폼의 기본 컴파일러를 사용했습니다 — Windows의 경우 Microsoft, MacOS의 경우 clang, Linux 배포판의 경우 Gnu를 사용했습니다. Gnu 및 clang는 Windows를 대상으로 할 수 있으며 Microsoft의 컴파일러보다 훨씬 빠른 코드(일반적으로 20% 이상)를 생성합니다. 하지만 대부분의 사람들이 기본값으로 사용할 것으로 생각하여 해당 컴파일러를 선택했습니다.\n\n![RaspberryPi5CPUPerformance_1](/assets/img/2024-06-20-RaspberryPi5CPUPerformance_1.png)\n\n![RaspberryPi5CPUPerformance_2](/assets/img/2024-06-20-RaspberryPi5CPUPerformance_2.png)\n\nRaspberry Pi 5는 이전 Pi 버전보다 상당히 빠릅니다. 이를 통해 많은 새로운 Pi 솔루션이 가능해질 것으로 기대됩니다. 기대되는 결과물이 무엇인지 기대됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n벤치마크 결과에 대한 몇 가지 참고 사항:\n\n- 모든 시간은 틱택토에서 이기지 못 한 것을 증명하는 한 번의 반복에 대한 밀리초로 표시됩니다.\n- Pi 5에 대해 5볼트 3암프 어댑터를 사용했습니다. 이 기기는 5볼트 5암프를 요구하며 제 어댑터로 화면의 알림 영역에 \"저전압\" 오류가 표시됩니다. 공식 어댑터는 Pi 5 공급 업체에서 구할 수 있지만 현재는 드물습니다. 그리고 5암프를 전달하는 일반 어댑터를 쉽게 찾을 수 없었습니다. 제 집에 있는 열두 개의 USB 어댑터는 모두 5V에서 1에서 3 암프를 제공합니다. 낮은 전력 어댑터를 인식하면 Pi 5가 언더클럭될 수 있기 때문에 실제 어댑터를 사용하면 벤치마크 시간이 개선될 수 있습니다. 업데이트: 공식 Pi 5 어댑터는 5.1V에서 5암프입니다. 이 어댑터를 사용하면 더 이상 \"저전압\" 경고를 받지 않습니다. 싱글 코어 성능은 동일하지만 3코어 성능은 11% 이상 향상되었습니다. 이를 반영하기 위해 위의 표에 새 열을 추가했습니다.\n- Pi 5는 Apple M1보다 두 배 정도 느립니다. 실제로 아주 빠릅니다.\n- 위에서 언급한 대로, AMD64용 Microsoft C++ 컴파일러는 clang과 Gnu 컴파일러보다 나쁩니다. 그러나 Arm64에 대한 성능 차이는 훨씬 더 커집니다. Microsoft/Qualcomm SQ3는 Arm64 어셈블러 코드를 M1과 대략 같은 속도로 실행하지만 C++ 앱은 현격히 느립니다. 이로 인해 Pi 5는 CPU가 덜 강력함에도 불구하고 윈도우 기기와 경쟁력이 높아졌습니다.\n- Intel i7-4770K는 Pi 5보다 10년 더 오래되었지만 Microsoft의 더 느린 컴파일러에도 불구하고 더 나은 성능을 발휘합니다.\n- 표에는 RVOS 에뮬레이터가 중첩된 버전을 실행하는 런타임이 나와있습니다. 여기서 Pi 5는 64비트 CPU를 32비트 기계에서 에뮬레이션하는 것이 비용이 많이 들기 때문에 이전 Pi 모델들보다 훨씬 우수한 성능을 보여줍니다. 64비트 OS가 탑재된 Pi 4는 32비트 OS보다 성능이 향상될 것이라고 생각합니다.\n\n이러한 벤치마크는 제게 중요한 시나리오에 대한 성능을 반영하며, Pi 5가 이전 버전보다 훨씬 빠르다는 것을 명백하게 보여줍니다. 이제 더 나은 전원 공급원을 찾아봐야겠어요.\n\n(참고: 틱택토 구현의 소스 코드는 https://github.com/davidly/ttt 에 있습니다. 에뮬레이터 코드는 https://github.com/davidly/ntvao, https://github.com/davidly/ntvcm, https://github.com/davidly/ntvdm, https://github.com/davidly/rvos 에서 찾을 수 있습니다.)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n(참고 2: 1970년대에 라디오 샥에서 구입한 부품들로 TTL 칩으로 만든 프로젝트에 전원을 공급하기 위해 5V 전원 공급기를 제작했습니다. 50년 후에 다시 5V 전원 공급기를 만들어야 할 것 같네요.)\n\n(참고 3: Anker USB 전원 공급기를 구입했는데, 5V를 4.5A로 공급할 수 있습니다. 그런데 라즈베리 파이 5에서 여전히 \"저전압 경고\" 알림이 표시되며 성능이 크게 향상되지 않습니다.)\n\n(참고 4: 라즈베리 파이 5에서 어셈블리 앱의 Arm32 버전을 실행했습니다. 1개와 3개의 스레드에 대한 시간은 각각 0.0919 및 0.0407입니다. Arm32는 Arm64보다 레지스터가 적기 때문에 느립니다만, 여전히 라즈베리 파이 4보다 약 2배 빠릅니다.)","ogImage":{"url":"/assets/img/2024-06-20-RaspberryPi5CPUPerformance_0.png"},"coverImage":"/assets/img/2024-06-20-RaspberryPi5CPUPerformance_0.png","tag":["Tech"],"readingTime":4},{"title":"가장 빠른 쿠버네티스 배포 방법일까요 라즈베리 파이 클러스터 구축에 대해 알아봅시다","description":"","date":"2024-06-20 17:38","slug":"2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster","content":"\n\n최근에 라즈베리 파이 4 싱글보드 컴퓨터를 사용하여 쿠버네티스 클러스터를 빠르고 쉽게 부트스트랩 할 수 있는 솔루션을 발견했어요. 이 솔루션은 다른 베어 메탈 클러스터에도 훌륭하게 작동해요.\n\n![이미지](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_0.png)\n\n# 문제점 - 왜 이것을 해야 할까요?\n\n쿠버네티스 클러스터를 설정하고 필요한 모든 단계를 수행하는 방법에 대한 이전 자습서를 확인할 수 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nk3sup를 사용하더라도 아직 시간이 많이 소요되고 오류가 발생하기 쉬운 것 같아요.\n\n# 더 나은 방법\n\nTalos-Linux 및 Kubernetes 및 Talos-Linux 커뮤니티의 놀라운 작업 덕분에 빠르고 쉬운 해결책을 얻을 수 있어요. (https://www.talos.dev/)\n\n## Talos Linux이란?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n태로스는 컨테이너를 최적화한 리눅스 배포판입니다. 쿠버네티스와 같은 분산 시스템을 위해 리눅스를 새롭게 상상해 만든 제품입니다. 실용성을 유지하면서 최대한 최소화된 디자인으로 제작되었습니다. 이러한 이유로 탈로스에는 몇 가지 고유한 기능들이 있습니다:\n\n- 변경할 수 없습니다\n- 원자적입니다\n- 일시적입니다\n- 최소화되어 있습니다\n- 기본적으로 보안이 설정되어 있습니다\n- 단일 선언 구성 파일과 gRPC API를 통해 관리됩니다\n\n탈로스는 컨테이너, 클라우드, 가상화, 그리고 베어 메탈 플랫폼에 배포할 수 있습니다.\n\n출처: https://www.talos.dev/v1.7/introduction/what-is-talos/\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 왜 Talos Linux를 사용해야 할까요?\n\n주로 API를 통해 제어할 수 있는 Kubernetes용 운영 체제입니다.\n\n- Kubernetes를 사용하려면 구성해야 할 우분투 또는 유사한 배포판을 사용할 필요가 없습니다. Talos는 오직 Kubernetes를 위해 만들어졌습니다!\n- Kubernetes 노드로 사용할 모든 장치는 Talos 이미지로 간편하게 로드됩니다.\n- 부팅 과정 이후, 모든 장치는 유지 보수 모드에서 시작되어 추가 명령을 실행할 준비가 됩니다.\n- talosctl 도구를 사용하여 Kubernetes 클러스터의 각 개별 노드가 어떻게 동작해야 하는지 구성할 수 있습니다.\n- 모든 노드에 SSH를 설정할 필요가 없습니다.\n- 모든 노드에 k3s 또는 유사한 것을 설치할 필요가 없습니다.\n- 제어 노드에서 토큰을 복사하고 작업자 노드를 설정하기 위해 초기화할 때 고려해야 하는 번거로움이 없습니다.\n- 호스트 컴퓨터로 kubectl 파일을 복사해야 하는 걱정을 할 필요가 없습니다.\n\n## Talos-Bootstrap\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTalos-Linux과 talosctl은 Kubernetes 클러스터 초기화를 매우 간단하게 만듭니다. Talos의 API 중심적인 특성으로 인해 다른 도구들은 이러한 API를 사용하여 초기화를 더욱 간단하게 할 수 있습니다.\n\naenix-io의 놀라운 팀에서 만든 도구인 talos-bootstrap이 있습니다.\n\n우리는 이 도구를 사용하여 기록 시간 안에 Kubernetes를 설정할 것입니다.\n\n## 파이들을 준비하세요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 명령어를 사용하여 Talos 이미지를 다운로드하세요. (https://www.talos.dev/v1.7/talos-guides/install/single-board-computers/rpi_generic/#download-the-image)\n\n```bash\ncurl -LO https://factory.talos.dev/image/ee21ef4a5ef808a9b7484cc0dda0f25075021691c8c09a276591eedb638ea1f9/v1.7.0/metal-arm64.raw.xz\nxz -d metal-arm64.raw.xz\n```\n\n라즈베리 파이 장치에 이미지를 플래싱하세요.\n\n저는 Balena Etcher를 사용하고 있지만 별도의 플래시 도구를 사용할 수도 있습니다. 한 장치마다 요렇게 하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다운로드한 탈로스 이미지를 라즈베리 파이용으로 선택해주세요.\n\n![Talos Image 1](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_1.png)\n\n![Talos Image 2](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_2.png)\n\n대상 드라이브를 선택하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 3](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_3.png)\n\nFinally flash the image\n\n![Image 4](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_4.png)\n\n![Image 5](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_5.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Talos Tools 준비 사항\n\n먼저 talosctrl 도구를 설치해야 합니다.\n\n```js\ncurl -sL https://talos.dev/install | sh\n```\n\n다음 단계에서는 talos-bootstrap 도구를 설치하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```bash\ncurl -LO https://github.com/aenix-io/talos-bootstrap/raw/master/talos-bootstrap\nchmod +x ./talos-bootstrap\nsudo mv ./talos-bootstrap /usr/local/bin/talos-bootstrap\n```\n\n## 보너스\n\n전체 과정을 간편화하기 위해 개발 컨테이너 환경을 만들었습니다.\n\n- devcontainers 확장 프로그램을 설치하세요 (https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers).\n- 위의 저장소를 복제하세요.\n- VSCode로 이 저장소를 열어보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 Macbook Pro M1에서 Orbstack을 Docker 런타임으로 사용하여 전체 프로세스를 테스트했어요. 그러나 MacOS와 Linux에서 다른 Docker 런타임을 사용해도 잘 작동해야 합니다.\n\n## 클러스터 부트스트랩\n\n클러스터를 부트스트랩하려면 다음 명령을 실행하세요:\n\n```js\ntalos-bootstrap install\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_6.png)\n\n![Image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_7.png)\n\nAfter some time, the talos-bootstrap should find your Raspberry Pi nodes in talos maintenance mode. Select your first node:\n\n![Image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_8.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼 역할을 선택할 수 있어요, 먼저 controlplane으로 시작할게요.\n\n![control node](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_9.png)\n\n해당 control 노드에 대한 호스트명을 입력해주세요.\n\n![hostname](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n탈로스를 설치할 디스크를 선택해주세요:\n\n![이미지](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_11.png)\n\n저는 HDD와 USB 스틱이 연결되어 있습니다. 더 큰 240GB 디스크를 선택했습니다.\n\n탈로스가 사용해야 할 네트워크 인터페이스를 선택해주세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Node custom address](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_12.png)\n\nSelect a custom address for your node.\n\n![Gateway address](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_13.png)\n\nSet your gateway address. In most cases, it's your router address.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n아래는 Markdown 형식으로 변경한 텍스트입니다.\n\n![이미지](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_14.png)\n\n기본 DNS 서버를 선택하세요.\n\n![이미지](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_15.png)\n\n다음 단계에서 클러스터에 대한 VIP(가상 공유 IP)를 선택할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n아래는 데모를 위해 두 개의 노드 클러스터를 설정한 주소입니다:\n\n- 192.168.2.81\n- 192.168.2.82\n\nVIP 주소로 192.168.2.240를 선택했습니다. 또한 이는 DHCP 범위를 벗어난 곳에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 탈로스와 함께 제공되는 항목입니다. 아래와 같이 설명되어 있습니다.\n\n원본\n\n간단히 말해서, 클러스터 액세스를 위한 주소를 정의할 수 있습니다. 이는 실제 노드 주소와는 독립적입니다. 탈로스-리눅스의 매우 편리한 기능입니다.\n\nKubernetes 엔드포인트를 사용자 정의하십시오:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같이 설정이 되었는지 확인해 주세요:\n\n![Configuration 1](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_17.png)\n\n다음 대화 상자에서 \"예\"를 선택해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_19.png)\n\n![image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_20.png)\n\n다이얼로그를 완료하면 talos-boostrap을 실행하는 경로에 kubeconfig 파일이 생성됩니다.\n\n![image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_21.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 테스트\n\n```js\nexport KUBECONFIG=./kubeconfig\nkubectl get nodes\n```\n\n다음과 같이 출력됩니다:\n\n```js\nNAME          STATUS   ROLES           AGE     VERSION\nk8s-control   Ready    control-plane   4m55s   v1.30.1\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n컨트롤 플레인이 성공적으로 생성되었습니다!\n\n## 워커 노드\n\n워커 노드의 경우, 대부분의 경우 작업 노드 역할을 선택하는 것을 제외하고 대부분 동일한 단계가 수행됩니다:\n\n![이미지](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_22.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_23.png)\n\n![image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_24.png)\n\n![image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_25.png)\n\n## Testing\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n워커 노드에 시간을 주고 다시 다음을 실행해보세요:\n\n```js\nexport KUBECONFIG=./kubeconfig\nkubectl get nodes\n```\n\n이렇게 하면 클러스터 내 양쪽 노드를 볼 수 있을 겁니다:\n\n![노드 이미지](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_26.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 요약\n\n지침을 읽어보면 모든 단계에 대해 설명이나 주석을 다 해 놓았기 때문에 클러스터 설정이 길어 보일 수 있지만, 제가 말하건대로 (모든 것이 순조롭게 진행된다면 🤞🏻) 5~10분 안에 설정이 완료됩니다. 🚀\n\n이제 완전히 작동하는 클러스터를 갖추었으므로 인그레스, 스토리지, 인증서 또는 flux-cd와 같은 다른 유용한 구성 요소를 설치할 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_0.png"},"coverImage":"/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_0.png","tag":["Tech"],"readingTime":8}],"page":"46","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"46"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>