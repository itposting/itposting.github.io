<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/81" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/81" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="관계형 데이터베이스에서 데이터 레이크하우스로 데이터 관리의 간단한 역사" href="/post/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="관계형 데이터베이스에서 데이터 레이크하우스로 데이터 관리의 간단한 역사" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="관계형 데이터베이스에서 데이터 레이크하우스로 데이터 관리의 간단한 역사" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">관계형 데이터베이스에서 데이터 레이크하우스로 데이터 관리의 간단한 역사</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="높은 영향력을 가진 데이터 거버넌스 팀" href="/post/2024-06-19-High-impactdatagovernanceteams"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="높은 영향력을 가진 데이터 거버넌스 팀" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-High-impactdatagovernanceteams_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="높은 영향력을 가진 데이터 거버넌스 팀" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">높은 영향력을 가진 데이터 거버넌스 팀</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="유튜브 트렌드 분석 파이프라인 Airflow, Spark, S3 및 Docker를 이용한 ETL" href="/post/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="유튜브 트렌드 분석 파이프라인 Airflow, Spark, S3 및 Docker를 이용한 ETL" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="유튜브 트렌드 분석 파이프라인 Airflow, Spark, S3 및 Docker를 이용한 ETL" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">유튜브 트렌드 분석 파이프라인 Airflow, Spark, S3 및 Docker를 이용한 ETL</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">25<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="1771 Technologies가 선보이는 Graphite Grid 소개" href="/post/2024-06-19-IntroducingGraphiteGridby1771Technologies"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="1771 Technologies가 선보이는 Graphite Grid 소개" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-IntroducingGraphiteGridby1771Technologies_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="1771 Technologies가 선보이는 Graphite Grid 소개" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">1771 Technologies가 선보이는 Graphite Grid 소개</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="분석력을 향상시키는 5가지 유용한 시각화 방법" href="/post/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="분석력을 향상시키는 5가지 유용한 시각화 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="분석력을 향상시키는 5가지 유용한 시각화 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">분석력을 향상시키는 5가지 유용한 시각화 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="여기에는 왜 당신의 친구들이 당신보다 더 많은 친구를 가지고 있는지에 대한 이유가 있습니다" href="/post/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="여기에는 왜 당신의 친구들이 당신보다 더 많은 친구를 가지고 있는지에 대한 이유가 있습니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="여기에는 왜 당신의 친구들이 당신보다 더 많은 친구를 가지고 있는지에 대한 이유가 있습니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">여기에는 왜 당신의 친구들이 당신보다 더 많은 친구를 가지고 있는지에 대한 이유가 있습니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="2일차 상호작용형 데이터 시각화  라이브러리 선택하기 " href="/post/2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="2일차 상호작용형 데이터 시각화  라이브러리 선택하기 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="2일차 상호작용형 데이터 시각화  라이브러리 선택하기 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">2일차 상호작용형 데이터 시각화  라이브러리 선택하기 </strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="SQL과 Power BI를 활용한 고객 세분화 보고서" href="/post/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="SQL과 Power BI를 활용한 고객 세분화 보고서" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="SQL과 Power BI를 활용한 고객 세분화 보고서" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">SQL과 Power BI를 활용한 고객 세분화 보고서</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="이상치에 대해 과하게 생각하지 마세요, 대신 t-분포를 사용해보세요" href="/post/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="이상치에 대해 과하게 생각하지 마세요, 대신 t-분포를 사용해보세요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="이상치에 대해 과하게 생각하지 마세요, 대신 t-분포를 사용해보세요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">이상치에 대해 과하게 생각하지 마세요, 대신 t-분포를 사용해보세요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">16<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="여러 브라우저 창 간에 실시간 WebSocket 데이터 공유하기" href="/post/2024-06-19-Sharingreal-timeWebSocketdataacrossmultiplebrowserwindows"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="여러 브라우저 창 간에 실시간 WebSocket 데이터 공유하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Sharingreal-timeWebSocketdataacrossmultiplebrowserwindows_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="여러 브라우저 창 간에 실시간 WebSocket 데이터 공유하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">여러 브라우저 창 간에 실시간 WebSocket 데이터 공유하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link posts_-active__YVJEi" href="/posts/81">81</a><a class="link" href="/posts/82">82</a><a class="link" href="/posts/83">83</a><a class="link" href="/posts/84">84</a><a class="link" href="/posts/85">85</a><a class="link" href="/posts/86">86</a><a class="link" href="/posts/87">87</a><a class="link" href="/posts/88">88</a><a class="link" href="/posts/89">89</a><a class="link" href="/posts/90">90</a><a class="link" href="/posts/91">91</a><a class="link" href="/posts/92">92</a><a class="link" href="/posts/93">93</a><a class="link" href="/posts/94">94</a><a class="link" href="/posts/95">95</a><a class="link" href="/posts/96">96</a><a class="link" href="/posts/97">97</a><a class="link" href="/posts/98">98</a><a class="link" href="/posts/99">99</a><a class="link" href="/posts/100">100</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"관계형 데이터베이스에서 데이터 레이크하우스로 데이터 관리의 간단한 역사","description":"","date":"2024-06-19 09:43","slug":"2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement","content":"\n\n## 데이터 관리의 진화 및 데이터 엔지니어링을 위한 솔루션\n\n비즈니스 인텔리전스와 데이터 엔지니어링은 산업 분야에서 비교적 새로운 분야입니다. 20세기 초부터 어느 정도 비즈니스 분석이 수행되어 왔습니다. 그러나 디지털 정보의 대량 분석이 필요한 건 정보 시대에만 발생한 문제입니다. 간단합니다 – 데이터를 소유(또는 수집)하고 그로부터 더 나은 통찰력을 얻는 사람들이 성공할 것입니다.\n\n데이터 엔지니어로써, 데이터 관리 접근 방식이 역사를 통해 어떻게 변화해왔는지, 데이터 처리와 관련된 문제를 어떻게 해결해 왔는지 이해하는 것은 흥미로운 일입니다. 역사에 대해 간단히 탐험해보고 데이터 관리 방식이 어떻게 시간이 지남에 따라 발전해 왔는지 알아보겠습니다.\n\n![이미지](/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 1980년대\n\n1980년대에는 대형 기업들이 관계형 SQL 데이터베이스를 사용하여 질의를 수행하여 통찰을 얻었습니다. SQL 기술이 1974년경에 처음 등장했기 때문에 이 주요 도구가 현재 50년이라는 점을 인식하는 것이 중요합니다. 기업들이 분석 질의를 위한 관계형 데이터 관리에서 제한을 경험하면서 비즈니스 데이터 웨어하우스라는 개념이 소개되었습니다.\n\n지금은 익숙하지만, 그 당시에는 이것이 상당한 혁신이었습니다. 이 접근 방식의 주요 성과는 다음과 같습니다:\n\n- 더 빠른 비즈니스 인텔리전스 (BI) 프로세스.\n- 구조화된 데이터와 효율적으로 작업할 수 있는 능력.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 주목할만한 단점들도 있었어요:\n\n- 반정형 및 비정형 데이터를 지원하지 않는 한계.\n- 규모와 속도에 대한 도전.\n- 대용량 데이터 크기를 처리할 때 장기간의 처리 시간이 발생할 수 있음.\n\n![이미지](/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_1.png)\n\n## 2000–2010년대\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2000년대에 대형 데이터가 등장하면서 전통적인 데이터 웨어하우스에 도전이 제기되었습니다. Google과 Yahoo와 같은 기업들은 구조화되지 않은 대량의 데이터를 생성했는데, 이는 구조화된 데이터 웨어하우스로는 효과적으로 처리할 수 없는 양이었습니다. 게다가 대규모의 구조화되지 않은 데이터(예: 이미지)가 필요한 머신 러닝의 등장은 기존 데이터 관리 시스템의 한계를 더욱 부각시켰습니다.\n\n이에 대응하여 데이터 레이크 개념이 소개되었는데, Google이 대형 데이터의 분산 처리를 위해 MapReduce를 선도했습니다. 이는 Hadoop의 개발로 이어지며, Hadoop 분산 파일 시스템(HDFS)을 저장 계층으로 사용하여 효율적인 데이터 저장 및 처리가 가능하게 했습니다. 이후에는 MapReduce와 이후에는 Spark를 사용하여 데이터를 처리할 수 있었습니다.\n\n데이터 레이크는 다음과 같은 장점을 제공했습니다:\n\n- 비용 효율적인 클라우드 저장소.\n- 머신 러닝(ML) 지원.\n- 유연한 데이터 저장.\n- 스트리밍 데이터 처리.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서, 테이블 태그를 마크다운 형식으로 변경하면 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2020년대\n\n관계형 데이터베이스를 사용하는 것에서 분산 저장 시스템을 활용해 대용량 데이터를 처리하는 것으로 발전했습니다. 대용량 데이터의 문제를 해결한 것 같았지만, 새로운 도전이 나타났습니다. 기업들은 여전히 데이터를 완전히 활용하기 어려워하며, 상당량의 데이터를 활용하지 못한 채 놓아두고 있습니다.\n\n2020년에 Databrick은 데이터 관리의 혁신적인 접근 방식인 데이터 레이크하우스(Data Lakehouse)에 대한 중요 논문을 발표했습니다. 이 방식은 데이터 웨어하우스와 데이터 레이크를 결합하여 클라우드 저장 서비스의 비용 효율성을 활용하면서 더 '웨어하우스'적인 방식으로 운영합니다. 데이터 레이크하우스는 BI 도구를 활용하고 데이터 과학/머신러닝 솔루션에 의존하는 대기업들에게 특히 유용합니다.\n\n이 접근을 옹호하는 회사가 Databricks이지만, 대부분의 도구가 오픈 소스이기 때문에 클라우드에서 이러한 솔루션을 독립적으로 구축하는 것도 가능합니다. 데이터 레이크하우스 시스템을 다룰 때는 저장 수준에서 적절한 기술을 갖추는 것이 중요합니다. 이러한 시스템은 table formats으로 언급되며, Apache Iceberg, Delta Lake, Apache Hudi와 같은 가장 일반적으로 사용되는 형식들이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기타 측면에서는 이전 솔루션과 유사하며 변환을 위해 Spark를 사용하는 것이 일반적입니다. 그러나 구체적인 툴킷은 회사마다 및 서로 다른 클라우드 제공업체 사이에 상당히 다를 수 있습니다. 데이터 레이크하우스는 지속적으로 발전하고 있으며 매년 새로운 기술이 등장하여 대규모 데이터를 처리하기가 더 쉬워지고 있습니다.\n\n![image](/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_3.png)\n\n# 결론\n\n요약하자면, 데이터 관리 솔루션의 진화에 대한 완전한 여정을 완료했습니다, 주로 분석을 위한 것입니다. 모든 접근 방식에는 고유의 사용 사례가 있으며 전 세계의 기업 및 팀에서 적극적으로 활용되고 있습니다. 내 의견으로는 데이터 웨어하우스가 가장 이해하기 쉬운 것으로, SQL 관계형 데이터베이스와 매우 유사합니다. 그러나 데이터 레이크하우스도 SQL을 언어 중 하나로 사용하지만 기술은 더 복잡합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n선택을 할 때는 다음 질문을 고려해 보세요:\n\n- 회사에서 개발 중인 ML 및 AI 도구가 있나요? 이들에게 데이터를 제공해야 할 필요가 있나요? 그렇지 않다면, 구조화된 데이터는 문제 해결에 충분하며 데이터 웨어하우스 솔루션을 사용할 수 있습니다.\n- 데이터의 크기를 고려해 보세요. 데이터 웨어하우스에서 저장 공간은 비실할 수 있습니다.\n- 마지막으로, 데이터 팀의 품질도 중요한 문제입니다. 데이터 레이크하우스를 사용하는 경우, 비용이 더 많이 드는 데이터 엔지니어 몇 명이 필요합니다.\n\n## 개인적인 반성:\n\n데이터 관리의 진화에 대한 간략한 탐험을 통해 다양한 접근 방법에 대해 더 잘 이해하실 수 있게 되었다고 믿습니다. 이 정보는 해결책을 비교하고 문제에 적합한 것을 선택할 때 필수적입니다.","ogImage":{"url":"/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_0.png"},"coverImage":"/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_0.png","tag":["Tech"],"readingTime":4},{"title":"높은 영향력을 가진 데이터 거버넌스 팀","description":"","date":"2024-06-19 09:40","slug":"2024-06-19-High-impactdatagovernanceteams","content":"\n\n## 영향을 주는 방법, 초점을 맞출 곳, 최고의 데이터 관리 팀에서 성공하기 위해 필요한 기술\n\n고영향력을 지닌 데이터 관리 팀에 대한 가장 좋은 비유는 원활하게 작동하는 주방과 유사합니다. 그들은 주방을 깨끗하게 유지하고 모든 칼이 날카롭고 모든 물건이 제 위치에 있도록 돕습니다. 이는 요리사들이 더 빨리 작업하게 하고 실수를 줄이며 나쁜 식품 위생 평가를 방지합니다.\n\n![고영향 데이터 관리 팀](/assets/img/2024-06-19-High-impactdatagovernanceteams_0.png)\n\n하지만 데이터 관리 팀은 선을 넘지 않아야 합니다. 조심하지 않으면 이들의 노력은 추가 부담처럼 보일 수 있고, 사업에 미미한 영향을 가져올 수 있는 정책과 프로세스를 도입할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 고영향 데이터 거버넌스 팀이 주로 초점을 맞춰야 할 영역 및 취해야 할 구체적인 단계를 살펴봅니다.\n\n![이미지](/assets/img/2024-06-19-High-impactdatagovernanceteams_1.png)\n\n데이터 거버넌스 팀이 어떻게 작동해야 하는지에 대해 일반화된 해결책은 없지만, 이러한 주요 주제들은 대부분의 주요 기업 채용 공고에서 요구 사항으로 계속해서 나타납니다.\n\n- 조직 전체에서 데이터 활용 가능하게 만들기\n- 데이터 품질을 유지하고 최우선 사항을 체계적으로 개선하는 데 도움\n- 소유권 모델을 통해 책임이 명확하게 보장\n- 리스크, 개인 정보 보호 및 규정 준수 관리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 조직 전체에서 데이터를 활용할 수 있도록\n\n문제는 거의 결코 데이터가 충분하지 않다는 것이 아닙니다. 오히려 기존 데이터가 활용되지 않거나 사람들이 찾을 수 없다는 문제입니다. 조직이 성장함에 따라 생성되는 데이터량은 상당히 증가합니다. 시간이 지남에 따라 이에는 여러 가지 영향이 있습니다.\n\n이것은 천천히 발생하다가 갑자기 전개됩니다.\n\n팀 간의 간헐적인 노력으로는 이 문제를 해결하기 어렵습니다. 데이터 거버넌스 팀은 이를 표준화하고 촉진하는 데 도움을 줄 수 있는 독특한 위치에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 중요한 데이터 자산 문서화\n\n데이터 관리 팀은 데이터 문서화에 대한 기대치를 설정하고 시행하여 후순위가 되지 않도록 해야 합니다. 예를 들어:\n\n- 데이터 카탈로그나 dbt yml 파일에 문서 작성이 이루어지도록 강제합니다. 이를 통해 팀간에 일관된 접근 방식을 구축할 수 있습니다.\n- 문서화는 ‘골드’ 표준 테이블에 필수로 이루어져야 하지만 ‘브론즈’와 ‘실버’는 선택 사항임을 명확히 합니다.\n- 잘 문서화된 테이블이나 필드의 명확한 예시를 제시합니다 (예: ‘세일즈포스 식별자에 의해 정의된 고객의 사용자 ID’와 같이 작성하며, ‘고객들의 사용자 ID’ 등과 같이 작성하지 않습니다).\n\n## 핵심 지표가 일관된 방식으로 정의되도록 보장하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떤 규모에 도달하면, 동일한 지표에 대한 서로 다른 정의를 반드시 마주하게 됩니다. 기본 데이터가 잘못되었다고 할지라도, 이는 의사 결정이 더뎌지고, 미팅에서는 지표가 올바른지 논의하는 데 시간을 보내게 만듭니다. 그 대신에 이에 대해 어떻게 대처할지에 대해 논의할 시간이 생깁니다.\n\n데이터 가버넌스 팀은 명확한 지침을 설정해야 합니다. 그러한 지침 중 하나는 다음과 같을 수 있습니다: \"주요 지표는 가시성을 향상하고 버전 관리하는 데 dbt에서 정의되어야 하며 BI 도구의 계산된 필드가 아니어야 합니다.\" 다른 지침으로는 주요 지표에 RAG(빨강, 주황, 초록) 임계값을 메트릭 정의와 함께 포함시켜 이를 한 곳에서 유지할 수 있도록 하는 것이 있습니다. 이는 해당 임계값이 위반될 경우 관련 이해 관계자에게 자동 경고를 설정할 수 있다는 장점이 있습니다.\n\n데이터 가버넌스 팀은 동일한 지표에 대한 다양한 정의를 찾고, 여러 정의를 발견하면 팀에 이를 통합하도록 장려해야 합니다. 이는 핵심 대시보드, Slack 채널 및 KPI가 사용되는 다른 위치를 정기적으로 평가함으로써 가장 잘 수행됩니다.\n\n## 데이터 검색 가능하게 하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 데이터 이용자들이 — 데이터 팀 내에서든 외부에서든 — 자신의 데이터를 찾지 못한다면, 그 데이터는 사실상 존재하지 않는 것과 다름없습니다. 필요한 데이터를 찾는 것이 마치 건초더미 속에서 바늘을 찾는 것처럼 느껴진다면, 문제의 일부 해결책은 툴셋을 업데이트하고 데이터 카탈로그를 도입하여 모든 데이터 자산을 발견할 수 있게 하는 것일 수 있습니다. 하지만 기본을 올바르게 설정하지 않는다면 어떤 도구도 도움이 되지 않을 것입니다.\n\n데이터 거버넌스 팀은 간단하지만 표준화된 규칙을 시행해야 합니다. 예를 들어, 데이터 자산에 대한 명확한 명명 규칙을 정하면 좋습니다. 대시보드의 경우 '⭐️ 제품 A/B 테스트 추적 [사업용 은행]'과 같이 명명 규칙을 사용하여 해당 대시보드가 중요(⭐️)하다는 것과 어떤 도메인에 속하는지 명확히 할 수 있습니다 (사업용 은행).\n\n사용 중인 데이터가 있는 곳에 가능한 가까이 위치시키는 것이 좋습니다. Looker와 같은 BI 툴을 사용하면 대부분의 데이터 이용자가 데이터를 소비할 수 있습니다. 각 도메인으로 진입할 수 있는 잘 구조화된 '홈페이지'를 만들어서 가장 중요한 대시보드에 대한 링크를 제공하는 것은 대시보드를 조직화하고 모든 사람이 쉽게 찾을 수 있도록 하는 좋은 방법입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 대시보드의 사용성 강화\n\n당신이 좋아하든 싫어하든, 얼마나 멋진 데이터 모델과 파이프라인을 만들든간에, 이를 소비하는 이해관계자들의 대시보드 경험은 데이터와 당신의 팀의 산출물에 대한 인식에 큰 영향을 미칠 것입니다. 대시보드를 제품처럼 다루어 외관, 느낌, 일관성, 그리고 성능을 강화해야 합니다.\n\nTypeform은 Looker의 개선 사항을 정리하면서 추가 단계를 거쳤습니다. 이들이 어떻게 그것을 수행했는지 여기에서 자세히 읽어보세요.\n\n![이미지](/assets/img/2024-06-19-High-impactdatagovernanceteams_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 불필요한 데이터 자산 제거\n\n데이터 자산의 확산은 데이터가 시간이 지남에 따라 사용하기 어려워지는 주요 이유 중 하나입니다. 데이터 스택을 정리하는 가장 효과적인 방법은 필요하지 않은 데이터 자산을 제거하는 것입니다. 필요하지 않은 데이터 자산의 명확한 정의는 없지만, 주의해야 할 신호는 다음과 같습니다.\n\n- 사용률이 낮은 대시보드\n- 다른 대시보드와 크게 중첩된 대시보드\n- 하류 종속성이 없는 데이터 모델\n- 다운스트림에서 사용되지 않는 데이터 모델의 열\n\n불필요한 데이터 스택을 정리하는 것은 쉬운 작업이 아니며, 가능하다면 사용되지 않는 테이블, 데이터 모델, 열 및 대시보드를 계속해서 제거하는 것에 투자해야 합니다. 그러나 많은 확장 중인 기업은 천 개 이상의 데이터 모델과 대시보드가 생기기 전까지는 뒷걸음치기를 하다가 이 문제에 대처해야 하는 경우가 많습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요번에 전해드릴 이야기는 Looker 대시보드의 수를 급격히 줄이고 자가 서비스를 거의 불가능하게 만들었던 회사의 이야기입니다.\n\n- 데이터 팀은 Looker 사용량을 위한 Looker 대시보드를 만들고 매월 #data-team 슬랙 채널로 보고서를 보냈습니다 (Looker를 통해 시스템 활동 탐색에서 이 데이터에 액세스할 수 있습니다). 보고서에는 a) 대시보드 수, b) 조회 수, c) 사용자 수, d) 임원 (VP/C-레벨) 사용자 수가 포함되어 있었습니다.\n- 대시보드를 제작한 도메인과 사용량으로 정렬할 수 있는 기능을 통해 사용하지 않는 대시보드와 삭제 가능한 대시보드 소유자를 빠르게 확인할 수 있었습니다.\n- 팀은 \"계층별 대시보드\" 개념을 도입했습니다. 계층 1 대시보드의 예시로는 보드 수준의 KPI 대시보드가 있을 수 있고, 계층 3 대시보드는 소규모 제품 팀용일 수 있습니다. 계층 1 대시보드에 대해 디자인 일관성, 피어 리뷰 요구 사항 및 사용 모니터링의 구체적인 요구 사항을 정했습니다.\n- 임원 사용자가 정기적으로 계층 1이 아닌 대시보드를 사용할 경우, 해당 대시보드는 계층 1 표준으로 검토되었습니다.\n\n이와 비슷한 단계를 데이터 모델을 정리하는 데 사용할 수 있습니다. 특히 dbt에서 잘 구성된 열 수준의 계보가 있는 경우 팀이 크게 혜택을 입었습니다. 이는 dbt에서 비즈니스 인텔리전스 도구까지 연장되는 정확하고 빠른 후속 영향을 평가하는 데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 중요한 데이터를 정의하세요\n\n규모에 관계없이, 데이터 거버넌스 팀의 의도가 얼마나 좋더라도 모든 데이터에서 높은 사용성을 시행하는 것은 불가능해집니다. 데이터 거버넌스 팀은 데이터 중요성이 어떻게 정의되어야 하며 이에 대한 영향이 무엇인지 명확하게 설정하는 것이 중요합니다. 예를 들어, '브론즈, 실버, 골드' 모델을 사용하고 실버 및 골드만 문서화 표준을 준수해야 한다고 결정할 수 있습니다.\n\n이 연습을 완료하는 동안 데이터를 체인으로 고려하고 하류 사용 사례에 되돌아갈 수 있도록하며 중요합니다.\n\n![Image](/assets/img/2024-06-19-High-impactdatagovernanceteams_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당신의 비즈니스 중요 데이터를 식별하는 방법을 안내하는 가이드를 읽어보세요. 비즈니스 중요 데이터 모델과 대시보드를 식별하는 실용적인 단계와 데이터에 대한 신뢰를 증진하는 방법을 알려드립니다.\n\n데이터 품질을 주의 깊게 살피면서 중요한 것을 체계적으로 개선하는 데 도움이 됩니다.\n\n데이터 팀이 직면하는 가장 큰 어려움으로 일관적으로 순위에 있던 것이 데이터 품질입니다. dbt는 최근 수천 명의 데이터 전문가들에게 최대 어려움에 대해 물었고, 57%의 표를 받아 데이터 품질이 가장 중요한 문제로 떠오르는군요.\n\n이에는 좋은 이유가 있습니다. 낮은 데이터 품질은 부정확한 결정, 시스템 장애 및 데이터에 대한 신뢰 손실로 이어질 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다행히 대부분의 데이터 전문가들은 검증된 데이터가 데이터 자산을 개발하는 일부분이라는 것에 익숙해졌습니다. 개별 자산에 적합하지만 개인의 해석에 많은 부분을 맡길 수 있습니다. 우리는 데이터 거버넌스 팀이 소유자 도메인 및 중요성과 같은 주요 차원을 횡단하여 데이터 품질을 체계적으로 평가하는 것을 권장합니다.\n\n## 데이터 품질 보고\n\n데이터 품질에 대한 보고를 하는 이유는 많지만 각 팀에게 자체 메트릭을 정의하도록 요청하는 것은 혼란을 초래하는 확실한 방법입니다. 데이터 품질을 일관되게 측정하고 전달하는 것은 비즈니스 결과에 여러 이점을 가져다 줄 수 있습니다:\n\n- 가동 시간 — 각 실행에서 성공적으로 통과하는 설치된 제어의 %는 무엇인가요?\n- 커버리지 — 필요한 제어가 설치된 데이터 자산의 %는 얼마나 되나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**품질 지표를 의미론적으로 유사한 영역으로 그룹화하면 비즈니스 사용 사례에 더 가까운 방식으로 데이터 품질에 대해 이야기할 수 있습니다. (1) 시기 적절성-데이터가 비즈니스와 합의된 SLA에 따라 신선하고 최신한가, (2) 정확성-모든 데이터가 사용 가능한가, (3) 완전성-데이터가 의미론적으로 올바른가, 그리고 (4) 일관성-가용 데이터가 시스템 전반에서 일관성이 있는가  우리는 또한 \"나쁨\", \"양호\", 또는 \"좋음\"이라고 하는 것을 명확히 정의하는 것을 제안합니다. 예를 들어, 50% 미만의 점수는 \"나쁨\"으로 표시되어 조치가 필요함을 의미합니다.**\n\n![이미지](/assets/img/2024-06-19-High-impactdatagovernanceteams_6.png)\n\n**이 수준의 통찰력을 통해 데이터 품질에 관한 질문을 시작할 수 있습니다. 예를 들어**\n\n**널 값 확인에 대한 좋은 커버리지를 가졌지만, 어떤 것이 실패하는지 조사하고, 고칠 수 있는지, 그리고 제거해야 하는지를 고려해야 합니다.**\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리의 어설션 테스트의 가동 시간이 크게 감소한 이유는 무엇일까요?\n\n저희가 일치성 테스트의 높은 가동 시간을 유지하고 있지만, 커버리지가 낮기 때문에 안전한 느낌을 준다고 생각하십니까?\n\n데이터 품질을 측정하는 방법에 대한 더 많은 지침은 \"데이터 품질 측정: 이론을 실무로\"라는 가이드를 참조해주세요.\n\n## 데이터 품질 기대치를 위한 가이드라인 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대부분의 상황에서는 데이터 사용 방식에 따라 기대 값을 명시하는 것이 좋습니다. 모든 자산에 동일한 기준을 적용하길 원하지 않을 수 있으므로 이를 권장합니다. 마찬가지로 ‘gold’ 자산에만 문서화를 강제할 수 있듯이, 데이터 거버넌스 팀은 데이터 테스트에 대한 기대 사항을 설정하는 역할을 해야 합니다. 비즈니스에 중요한 데이터, 노출되는 데이터 (예: 대시보드에 표시되는 데이터) 및 SLA(데이터가 제 시간에 도착하지 않으면 하류 영향이 있는 경우)를 고려하여 필요한 체크 항목을 평가하고 중요한 것을 측정할 수 있도록 돕습니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-High-impactdatagovernanceteams_7.png\" /\u003e\n\ndbt yml 파일은 이 메타데이터를 정의하기에 좋은 장소입니다. 이를 통해 사유 테스트(예: 사전 커밋 dbt 패키지에서 제공하는 check-model 태그)를 사용하여 각 데이터 모델이 필요한 중요도나 도메인 소유자 태그와 같은 메타데이터를 가지고 있는지 확인할 수 있습니다.\n\n# 수행 책임이 분명한 소유권 모델을 통해 책임을 보장하세요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 스택이 복잡해지면 한 사람이 모든 것을 머릿속에 간직하기 어려워집니다. 또한 문제를 발견한 사람이 그 문제를 해결할 적절한 사람이 아닌 경우가 더 많아집니다. 동시에 상하위 종속성의 수는 급증하여 적절한 상위 소유자를 찾거나 영향을받는 이해관계자에게 알릴 때 어려움을 겪게 됩니다.\n\n많은 데이터 팀에게 물어보지 않아도 꿈 같은 상황을 이해할 수 있습니다: 상위 생성자가 자신의 데이터의 품질을 소유하고 관리하며, 관련 데이터 팀이 책임을 질 수 있고, 이해관계자가 문제를 발견하는 날이 지나간 세상입니다.\n\n좋은 소유권은 말로만 하기 쉽지만, 실패한 소유권 이니셔티브가 부족하지 않습니다.\n\n데이터 거버넌스 팀은 명확히 정의된 역할과 책임을 갖는 일관된 소유권 모델을 구축하는 데 궁극적으로 책임이 있습니다. 단계별로 나누어서 누락된 부분을 파악하기 쉽게 만들어봅시다: (1) 메타데이터를 통합하고, (2) 관련 테스트로 문제를 감지하며, (3) 소유권을 할당하고, (4) 관련 인물에게 문제를 통지하여 그들이 대처할 수 있는 방법으로 행동을 취하도록 합시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![High impact data governance teams](/assets/img/2024-06-19-High-impactdatagovernanceteams_8.png)\n\n소유권은 기술적인 도전과 문화적인 측면 두 가지를 모두 고려해야 하는 문제입니다. 소유권 프로젝트를 성공적으로 이끌어내려면 두 측면에 모두 초점을 맞춰야 합니다.\n\n- 소유자에 대한 기대 설정 — 기대는 데이터 자산의 중요도와 연결되어야 합니다. 예를 들어, 무엇이 높은 심각도의 문제를 만들며 그 결정은 누가 하는지를 알아봅니다. 자세한 내용은 데이터 문제의 심각성 수준 설계를 시작하는 방법에서 확인할 수 있습니다.\n- 소유권 정의 — 소유권을 정의할 수 있는 장소는 dbt yml 파일부터 데이터 카탈로그, Confluence 페이지, 스프레드시트까지 다양합니다. 어디에 소유권을 정의해야 하는지 명확히 하고 모두가 동일한 방식으로 수행할 수 있도록 도와주세요.\n- 올바른 상황에서 올바른 사람에게 통보 — 데이터 소유권을 종합적으로 고려하길 권장합니다 — 상위팀이 소유한 데이터 소스부터 최종 사용자가 소유한 대시보드까지. 간단히 말해 우리의 권장사항을 다음 그룹으로 분류해보겠습니다: (1) 데이터 팀, (2) 상위팀, (3) 비즈니스 이해관계자.\n\n![High impact data governance teams](/assets/img/2024-06-19-High-impactdatagovernanceteams_9.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4. 문화 소유권 도전 극복하기 — 소유자로 할당받는 것과 책임을 져야 하는 것은 다릅니다. 소유권을 성공적으로 이행하는 것은 기술적인 도전만큼 문화적인 도전입니다. 사람들에게 영향을 미치고 책임을 지게 하는 것은 데이터 관리 팀에 대한 큰 이득입니다. 소유권에 대한 행동을 촉진하고 적절한 분야를 찾아 소유권을 정의함으로써 초기 승리를 증명하는 것은 기술적 구현만큼 중요합니다.\n\n우리의 가이드인 데이터 소유권: 현실적인 가이드를 읽어보세요. 데이터 팀, 상위 팀 및 비즈니스 이해자에게 소유권을 정의하고 활성화하기 위한 전체 도구 모음을 제공합니다.\n\n# 리스크, 개인정보 보호 및 규정 준수 관리\n\n회사의 수명주기에서 특정 시점, 일반적으로 데이터 관리 팀이 참여될 때, 관리해야 할 중요한 리스크가 있습니다. 이는 핀테크를 위한 규정 데이터, 곧 예정된 IPO를 위한 금융 데이터의 정확성, 또는 회사가 개인 식별 정보 데이터를 책임적으로 처리해야 한다는 일반적인 인식과 관련이 있을 수 있습니다.  \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당신의 역할은 데이터 거버넌스 전문가로 명확합니다: 회사가 의무를 준수하고 리스크를 최소화하면서도 데이터 팀의 업무를 느리게 만들지 않도록 보장하는 것입니다.\n\n다음은 이를 수행할 수 있는 몇 가지 방법입니다.\n\nPII 관련 데이터 – 대부분의 회사에서 사용자 이메일과 같은 일부 데이터는 모든 사람이 쉽게 쿼리할 수 없어야 합니다. 데이터가 PII로 태그되고 이 데이터 주변에 가드레일을 자동으로 시행하도록 강요하세요. 예를 들어, 접근 권한이 사용 사례별로 만 부여되고, 예를 들어 7일 후에 만료되는 원시 데이터가 있는 별도의 데이터 웨어하우스를 만드세요.\n\n사용자 데이터 삭제 요청 – 사용자 데이터를 처리하면 결국 사용자 데이터 삭제 요청을 만나게 될 것입니다. 이에 대해 조속히 생각할수록 처음 요청이 들어왔을 때 모든 사용자 데이터를 삭제하는 것이 더 쉬워집니다. 이러한 사고방식이 발생하기 전에 열어둔 적절한 도구, 예를 들어 열 수준의 계보,는 걸어두면 소요 시간이 크게 줄어듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인시던트 관리 프로세스 및 SLA 정의 - 어떤 소유권 모델이나 데이터 품질 점검을 하더라도 데이터 문제는 발생할 수 있습니다. 잘 정의된 인시던트 관리 프로세스에는 여러 가지 이점이 있습니다: 모두가 중요한 것에 대해 공유된 이해를 구축하는 데 도움이 되며, 문제에 대한 어떤 맥락도 없는 사람들을 쉽게 참여시킬 수 있습니다. 또한 지난 인시던트를 쉽게 되돌아보고 중요한 위반 사항에 대해 보고할 수 있습니다.\n\n인시던트 관리 테이블을 마크다운 형식으로 변경하십시오.\n\n# 데이터 가버넌스 전문가로서 탐색해야 할 핵심 기술\n\n데이터를 사용 가능하게 만들기, 품질을 보장하기, 소유권 구축하기, PII 및 리스크를 속도와 균형있게 조절하기 - 이것은 데이터 가버넌스 팀에게는 작은 일이 아닙니다! 데이터 가버넌스 전문가가 성공하기 위해 필요한 필수 기술은 아래와 같습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기술적 이해력 - 코드 기반에 기여하지 않고 직접 dbt 모델을 작성하지는 않더라도 데이터 팀이 사용하는 도구 및 제한 사항을 고수준으로 이해하면 유익합니다. 이렇게 하면 다른 팀의 고민을 더 잘 이해할 수 있으며 데이터 거버넌스 프로세스가 기존 워크플로에 어떻게 맞는지의 장단점을 파악할 수 있습니다.\n\n우선순위 균형 유지 - 모든 것이 항상 중요하다는 느낌일 수 있습니다. 그럼에도 불구하고 특정 이니셔티브에 우선순위를 두는 회사 전체의 상황을 주시하는 것이 여러분의 역할입니다. 예를 들어, 곧 예정된 IPO나 규정 위반과 같은 경우, 재무 데이터에 중점을 두는 것이 좋을 수 있으며 마케팅 및 제품 분야에는 덜 주의를 기울일 수 있습니다.\n\n공급업체 선정 프로세스 운영 - 위의 문제를 해결하는 데 도움이 되는 적어도 몇 가지 도구를 도입하고 싶을 것입니다. 데이터 카탈로그 및 데이터 관찰 도구와 같은 카테고리의 도구들에 주목해야 합니다. 도구에 투자를 결정하면, 잘 구조화된 프로세스를 운영하고 컨셉 증명을 계획하며 데이터 팀 구성원 모두가 주장을 듣게 해야 합니다. 회사가 이 카테고리의 도구를 처음 구매하는 경우일 수 있으므로 여러 개의 데모를 받고 기존 고객들과의 참조 검사를 진행하여 도구가 여러분에게 적합한지 확인하는 좋은 방법일 수 있습니다.\n\n조직적 지원 확보 - 누구도 즐거움을 위해 데이터 품질, 문서화 또는 소유권에 관심을 갖지 않습니다. 데이터 거버넌스 팀으로서 문서화, 데이터 품질 및 소유권이 왜 중요한지에 대한 꿈을 판매해야 합니다. 리더십 팀과 정기적으로 인사이트를 공유하는 것은 그들이 이에 투자해야 할 가치를 파악하고 게임에 참여하도록 하는 좋은 방법일 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 모두 모아보기\n\n데이터 거버넌스 팀의 역할은 부엌을 청소하고 칼이 날카로운지 확인하며 모든 것이 제자리에 있는지 확인하는 것입니다.\n\n이겈하는 것은 쉽지 않은 작업이며 우선 순위를 조정하고 이해 관계자를 연루시키며 벤더 선택 프로세스를 실행하는 것을 포함합니다. 이를 잘 수행할 때의 좋은 예로는 덴마크 핀테크 루나(Lunar)의 경우가 있습니다. 루나는 소유권부터 중요도 및 모니터링에 이르기까지 데이터 거버넌스 프레임워크를 성공적으로 롤아웃하고 C-레벨의 찬성을 받았습니다. 자세한 내용은 이곳에서 데이터 거버넌스를 프레임워크로 구축하는 데 영감을 얻어보십시오.\n\n요약:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 조직 전반에 걸쳐 데이터를 사용할 수 있도록 합니다 — 데이터가 잘 문서화되어 있고 주요 지표가 일관되게 정의되어 있으며 주요 데이터 자산이 발견 가능하며 대시보드가 잘 보이고 느껴지고 잘 수행될 수 있도록 합니다.\n- 데이터 품질을 계속해서 확인하여 가장 중요한 부분을 체계적으로 개선하도록 지원합니다 — 데이터 거버넌스 팀이 데이터 테스트를 작성하지는 않겠지만, 데이터 품질을 개선하도록 기대치를 강조하고 보고할 수 있습니다.\n- 소유권 모델을 통해 책임이 명확하게 보장합니다 — 소유권에 대한 주요 측면들, 소유권이 어떻게 정의되고 실행되며 각 팀이 자체적인 해석을 하지 않도록 데이터 거버넌스 팀이 중앙에서 정의해야 합니다.\n- 위험, 프라이버시 및 규정 준수를 관리합니다 — 종종 매력적이지 않은 분야로 여겨지지만, PII 데이터에 대한 불명확한 프로세스, 사용자 삭제 요청 및 사건들은 나중에 큰 문제를 일으킬 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-High-impactdatagovernanceteams_0.png"},"coverImage":"/assets/img/2024-06-19-High-impactdatagovernanceteams_0.png","tag":["Tech"],"readingTime":12},{"title":"유튜브 트렌드 분석 파이프라인 Airflow, Spark, S3 및 Docker를 이용한 ETL","description":"","date":"2024-06-19 09:37","slug":"2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker","content":"\n\n이 기사에서는 Apache Airflow와 PySpark를 사용하여 자동 ETL (추출, 변환, 로드) 파이프라인을 만드는 방법을 안내합니다. 이 파이프라인은 YouTube Data API에서 트렌드 비디오 데이터를 가져와 처리한 후 처리된 데이터를 S3에 저장할 것입니다.\n\nTwitter API를 사용한 파이프라인을 보여주는 Darshil Parmar의 YouTube 비디오를 시청한 후, 유사한 프로젝트에 도전하기로 영감을 받았습니다. 그러나 Twitter API의 가격 정책 변경으로 인해, 시청자가 YouTube Data API를 대체로 제안했고 이것이 제 흥미를 자극했습니다.\n\n프로젝트에 돌입하기 전에 두 가지 필수 사항이 있습니다:\n\n1. Youtube Data API 키 획득\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Google Developers Console을 방문해 주세요.\n- 새 프로젝트를 생성해 주세요.\n- \"YouTube Data API\"를 검색하고 활성화해 주세요.\n- 새 자격 증명을 생성하고 프로젝트에서 나중에 사용할 API 키를 복사해 주세요.\n\n자세한 지침은 YouTube Data API 시작 가이드를 참조해 주세요.\n\n2. AWS 액세스 키 ID 및 비밀 액세스 키 획득\n\n- AWS Management Console에 로그인해 주세요.\n- IAM(Identity and Access Management) 섹션으로 이동하고 새 사용자를 생성해 주세요.\n- 필요한 S3 액세스 정책을 부여하고 액세스 키를 생성해 주세요.\n- 프로젝트에서 사용할 액세스 키 ID와 비밀 액세스 키를 안전하게 저장해 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 실제 프로젝트를 시작하겠습니다! 준비됐나요 여러분!!\n\n![YouTube Trend Analysis Pipeline ETL with Airflow, Spark, S3, and Docker](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png)\n\n이 글은 4가지 주요 단계로 구성되어 있어요:\n\n- 소프트웨어 설치 및 설정\n- Youtube Data API에서 데이터 추출\n- PySpark를 사용하여 데이터 변환\n- AWS S3로 데이터 로드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 1. 소프트웨어 설치 및 설정:\n\n- VS Code — [VS Code 다운로드 및 설치](https://code.visualstudio.com/).\n- Docker Desktop — [Docker Desktop 다운로드 및 설치](https://www.docker.com/products/docker-desktop).\n- (선택사항) Windows Subsystem for Linux (WSL) — 데이터 엔지니어링에 사용되는 Apache Airflow 및 PySpark와 같은 많은 도구 및 라이브러리가 Unix 계열 시스템을 위해 개발되었습니다. 이러한 도구를 Windows에서 사용할 때 발생할 수 있는 호환성 문제를 피하기 위해 WSL을 통해 네이티브 Linux 환경에서 실행할 수 있습니다.\n  - ` 관리자 권한으로 PowerShell을 엽니다.\n  - ` 다음 명령을 실행하세요: wsl --install.\n  - ` 명령에 따라 WSL을 설치하고 Microsoft Store에서 Linux 배포판(예: Ubuntu)을 선택하세요.\n  - ` Linux 배포판에 사용자 이름 및 암호를 설정하세요.\n\n이 프로젝트를 실행하는 데 WSL이 반드시 필요한 것은 아닙니다. Docker Desktop은 Windows에서 네이티브로 실행될 수 있으며 Docker 자체가 관리하는 가벼운 Linux 가상 머신(VM)을 사용합니다. 그러나 Docker Desktop과 함께 WSL을 사용하면 Windows에서 직접 Linux 명령 및 작업을 실행할 수 있어 보다 네이티브한 개발 경험을 제공합니다.\n\n이제 설정을 시작해 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n부분 1 — 도커 이미지 만들기\n\n- 프로젝트용 새 폴더를 만들고 \"Airflow-Project\"로 이름을 지어주세요.\n- 해당 폴더에서 명령 프롬프트를 엽니다.\n- 명령 프롬프트에서 아래 명령을 실행하세요:\n\n```bash\ncode .\n```\n\n- 이 명령은 VS Code에서 해당 폴더를 프로젝트로 엽니다.\n- VS Code에서 \"dockerfile\"이라는 새 파일을 만들고 아래 코드를 붙여넣으세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nFROM apache/airflow:latest\n\n# 시스템 종속성을 설치하기 위해 루트 사용자로 전환합니다\nUSER root\n\n# git, OpenJDK를 설치하고 apt 캐시를 정리합니다\nRUN apt-get update \u0026\u0026 \\\n    apt-get -y install git default-jdk \u0026\u0026 \\\n    apt-get clean \u0026\u0026 \\\n    rm -rf /var/lib/apt/lists/*\n\n# Python 패키지를 설치하기 위해 airflow 사용자로 전환합니다\nUSER airflow\n\n# 필요한 Python 패키지를 설치합니다\nRUN pip install --no-cache-dir pyspark pandas google-api-python-client emoji boto3\n```\n\n이 Docker 파일은 프로젝트를 실행하는 데 필요한 모든 패키지를 포함하고 있어요.\n\n- 파일을 마우스 오른쪽 버튼으로 클릭하고 VS Code에서 \"이미지 빌드\" 옵션을 선택하세요. 이름을 입력하라는 프롬프트가 나타나면 \"airflow-project\"를 입력하세요. 이 명령은 Docker 이미지를 생성합니다. 그러나 이미지를 사용하려면 docker-compose.yml 파일을 생성하고 이미지를 사용하도록 구성해야 합니다.\n\n(재미있는 사실: 파일에서 Python 설치가 없는 이유 궁금하신가요? 실제로 Dockerfile에서 사용된 기본 이미지인 apache/airflow:latest에는 Python이 이미 설치되어 있어요. 왜냐하면 Airflow 자체가 Python으로 작성되어 있기 때문에 주로 워크플로 및 작업 정의에 Python을 사용합니다. 따라서 Dockerfile에서 별도로 Python을 설치할 필요가 없답니다!)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파트 2 — 도커 컴포즈 파일 생성하기\n\n도커 컴포즈를 사용하면 멀티 컨테이너 도커 애플리케이션을 쉽게 다룰 수 있습니다. 이를 통해 단일 명령으로 여러 도커 컨테이너를 정의하고 실행할 수 있으며 각 서비스의 환경 변수, 볼륨, 포트 및 기타 설정을 명확하고 조직적인 방식으로 구성할 수 있습니다. 도커 컴포즈를 사용하면 단일 명령어인 docker-compose up 또는 docker-compose down을 사용하여 여러 서비스를 쉽게 시작, 중지 및 관리할 수 있습니다.\n\n- \"docker-compose.yml\" 파일을 생성하고 다음 코드를 파일에 붙여넣습니다:\n\n```js\nversion: '3'\nservices:\n\n  airflowproject:\n    image: airflow-project:latest\n    environment:\n      - AWS_ACCESS_KEY_ID=your-aws-access-key\n      - AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key\n      - YOUTUBE_API_KEY=your-youtube-api-key\n    volumes:\n      - ./airflow:/opt/airflow\n    ports:\n      - \"8080:8080\"\n    command: airflow standalone\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 이제 파일을 마우스 오른쪽 버튼으로 클릭한 후 VS Code에서 'Compose Up' 옵션을 선택하세요. 환경을 설정하기 위해 클릭하세요.\n- 깜짝 놀랄 일이 벌어졌어요! 이 작업을 완료한 후에는 VS Code 프로젝트 디렉토리에 \"airflow\"라는 새 폴더가 나타날 수 있습니다.\n\nDocker 데스크톱을 열어서 모든 것이 올바르게 완료되었는지 확인하세요. 올바르게 완료된 경우 다음과 같은 화면이 표시됩니다.\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_1.png)\n\n- 이제 Airflow 프로젝트를 클릭하여 Airflow가 8080 포트에서 실행 중임을 나타내는 로그가 표시되는 화면을 엽니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_2.png)\n\n- 포트를 클릭하면 Airflow 로그인 페이지로 이동합니다. 이 링크를 처음 열어보는 경우 자격 증명을 제공해야 합니다.\n- 사용자 이름은 \"admin\"이고 비밀번호는 compose up 명령을 실행한 후 생성된 Airflow 폴더 내의 \"standalone_admin_password.txt\" 파일에 있습니다.\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_3.png)\n\n- 로그인 페이지에서 자격 증명을 입력한 후, 로컬 호스트에서 Airflow가 실행 중인 것을 확인할 수 있습니다. 다음과 같이 나타납니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_4.png\" /\u003e\n\n당신의 환경 설정 완료입니다! 휴―!!\n\n# 2. YouTube 데이터 API에서 데이터 추출하기:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Airflow 폴더 아래에 \"dags\"라는 이름의 폴더를 만들고, dags 폴더 아래에 \"youtube_etl_dag.py\"라는 파이썬 파일을 만듭니다.\n- 이제 \"youtube_etl_dag.py\" 파일에 다음을 import하세요.\n\n```js\nimport logging\nimport os\nimport re\nimport shutil\nfrom datetime import datetime, timedelta\n\nimport boto3\nimport emoji\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_date, udf\nfrom pyspark.sql.types import (DateType, IntegerType, LongType, StringType,\n                               StructField, StructType)\n\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n```\n\n- 이 프로젝트를 실행하는 데 위의 모든 라이브러리가 필요합니다(코드 작성을 시작하면 모두 유용해집니다)\n- VS Code에서 오류가 발생하는 것을 볼 수 있습니다. 그 이유는 모든 종속성이 도커에 설치되어 있지만 로컬 머신에는 설치되어 있지 않기 때문이므로 신경 쓰지 마십시오.\n- Airflow에서 구문 오류가 있으면 화면 상단에 표시되고, 논리 오류/예외는 Airflow 로그에서 확인할 수 있습니다.\n\n```js\n# DAG와 기본 인수 정의\ndefault_args = {\n    'owner': 'airflow',  # DAG 소유자\n    'depends_on_past': False,  # 과거 DAG 실행에 종속하는지 여부\n    'email_on_failure': False,  # 실패 시 이메일 알림 비활성화\n    'email_on_retry': False,  # 재시도 시 이메일 알림 비활성화\n    'retries': 1,  # 재시도 횟수\n    'retry_delay': timedelta(minutes=5),  # 재시도 간의 지연 시간\n     'start_date': datetime(2023, 6, 10, 0, 0, 0),  # 매일 자정(00:00) UTC에 실행\n}\n\ndag = DAG(\n    'youtube_etl_dag',  # DAG 식별자\n    default_args=default_args,  # 기본 인수 할당\n    description='간단한 ETL DAG',  # DAG 설명\n    schedule_interval=timedelta(days=1),  # 일별 스케줄 간격\n    catchup=False,  # 누락된 DAG 실행을 복구하지 않음\n)\n```  \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n매일 자정(0시)에 실행되는 DAG인 'youtube_etl_dag'을 정의하고 있습니다. 이 DAG은 Airflow에서 관리 및 트리거되며, VS Code에서 별도로 실행할 필요가 없습니다. Python 파일을 업데이트하면 Airflow에서 자동으로 변경 사항을 감지하고 반영할 것입니다.\n\n현재 Airflow에는 DAG이 표시되지만 아직 정의된 작업이 없어서 어떤 작업도 표시되지 않습니다. DAG를 기능적으로 만들기 위해 데이터 추출 작업을 만들어봅시다.\n\n```js\n# YouTube API에서 데이터를 추출하기 위한 Python callable 함수\ndef extract_data(**kwargs):\n    api_key = kwargs['api_key']\n    region_codes = kwargs['region_codes']\n    category_ids = kwargs['category_ids']\n    \n    df_trending_videos = fetch_data(api_key, region_codes, category_ids)\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    # DataFrame을 CSV 파일로 저장\n    df_trending_videos.to_csv(output_path, index=False)\n\ndef fetch_data(api_key, region_codes, category_ids):\n    \"\"\"\n    YouTube API에서 여러 나라와 카테고리의 인기 동영상 데이터를 가져옵니다.\n    \"\"\"\n    # 비디오 데이터를 저장할 빈 리스트를 초기화합니다.\n    video_data = []\n\n    # YouTube API 서비스 빌드\n    youtube = build('youtube', 'v3', developerKey=api_key)\n\n    for region_code in region_codes:\n        for category_id in category_ids:\n            # 각 지역 및 카테고리에 대해 next_page_token을 None으로 초기화\n            next_page_token = None\n            while True:\n                # 인기 동영상을 가져오기 위해 YouTube API에 요청을 보냅니다.\n                request = youtube.videos().list(\n                    part='snippet,contentDetails,statistics',\n                    chart='mostPopular',\n                    regionCode=region_code,\n                    videoCategoryId=category_id,\n                    maxResults=50,\n                    pageToken=next_page_token\n                )\n                response = request.execute()\n                videos = response['items']\n\n                # 각 비디오를 처리하고 데이터를 수집합니다.\n                for video in videos:\n                    video_info = {\n                        'region_code': region_code,\n                        'category_id': category_id,\n                        'video_id': video['id'],\n                        'title': video['snippet']['title'],\n                        'published_at': video['snippet']['publishedAt'],\n                        'view_count': int(video['statistics'].get('viewCount', 0)),\n                        'like_count': int(video['statistics'].get('likeCount', 0)),\n                        'comment_count': int(video['statistics'].get('commentCount', 0)),\n                        'channel_title': video['snippet']['channelTitle']\n                    }\n                    video_data.append(video_info)\n\n                # 결과의 더 많은 페이지가 있는 경우 다음 페이지 토큰을 가져옵니다.\n                next_page_token = response.get('nextPageToken')\n                if not next_page_token:\n                    break\n\n    return pd.DataFrame(video_data)\n\n# DAG를 위한 데이터 추출 작업 정의\nextract_task = PythonOperator(\n    task_id='extract_data_from_youtube_api',\n    python_callable=extract_data,\n    op_kwargs={\n        'api_key': os.getenv('YOUTUBE_API_KEY'),\n        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],\n        'category_ids': ['1', '2', '10', '15', '20', '22', '23']\n    },\n    dag=dag,\n)\n\nextract_task # 이 작업을 실행하도록 DAG를 설정함\n```\n\n이 코드에서 두 가지 주요 작업이 이루어지고 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- DAG에 extract_task라는 작업을 만들고 있습니다.\n- extract_task에서 호출되는 callable 함수인 extract_data를 정의하고 있습니다. 이 함수는 YouTube Data API에서 데이터를 가져와 \"Youtube_Trending_Data_Raw\"로 시작하는 CSV 파일에 pandas DataFrame을 사용하여 저장합니다.\n\nYouTube Data API 문서를 참조하여 API의 다른 부분에서 사용 가능한 데이터에 대해 자세히 이해할 수 있습니다. 우리는 트렌딩 비디오 데이터에 관심이 있으므로 API의 해당 부분에 집중할 것입니다. next_page_token은 모든 페이지에서 데이터를 검색하도록 보장합니다.\n\n코드를 수정한 후 Airflow 페이지에 변경 사항이 반영되어야 합니다. DAG를 수동으로 실행하려면 왼쪽 상단에 있는 실행 버튼을 클릭하시면 됩니다. 그래프에서 작업 상태 (대기, 실행 중, 성공 등)는 다른 색상으로 나타납니다. DAG가 실행 중일 때 로그를 보실 수도 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_5.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n런 버튼을 클릭하면 데이터를 가져오고 파일에 저장하는 데 시간이 걸립니다. 작업의 각 단계에서 그래프 색상이 변경되는 것을 볼 수 있을 거에요. 멋지죠? :)\n\n작업 상태가 성공을 나타내는 녹색으로 변하면, 새 파일인 \"Youtube-Trending-Data-Raw\"가 생긴 것을 확인할 수 있어요.\n\n우리의 Raw 데이터는 이렇게 생겼어요:\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 추출 작업이 완료되었습니다. 다음 작업으로 넘어가 봅시다!\n\n## 3. PySpark를 사용하여 데이터 변환하기:\n\n원시 데이터 파일을 살펴보면 데이터에 많은 해시태그와 이모지가 있는데, 이는 우리 프로젝트에는 필요하지 않습니다. 데이터를 전처리하고 정리하여 추가 분석에 유용하도록 만들어 봅시다.\n\n이 작업에 PySpark를 사용할 것입니다. PySpark는 대용량 데이터 세트를 처리하고 변환 작업을 수행하기 위해 설계된 강력한 프레임워크입니다. 데이터 세트가 특히 크지 않기 때문에 Pandas를 사용할 수도 있지만, 전에 PySpark를 사용한 적이 있어 이번에도 PySpark를 사용하기로 결정했습니다. 최근 PySpark를 공부하고 있으며, 이론을 공부하는 것보다 실제 구현이 더 흥미롭다고 느낍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# Python callable function to extract data from YouTube API\ndef extract_data(**kwargs):\n    api_key = kwargs['api_key']\n    region_codes = kwargs['region_codes']\n    category_ids = kwargs['category_ids']\n    \n    df_trending_videos = fetch_data(api_key, region_codes, category_ids)\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    # Save DataFrame to CSV file\n    df_trending_videos.to_csv(output_path, index=False)\n\ndef fetch_data(api_key, region_codes, category_ids):\n    \"\"\"\n    Fetches trending video data for multiple countries and categories from YouTube API.\n    Returns a pandas data frame containing video data.\n    \"\"\"\n    video_data = []\n\n    # Build YouTube API service\n    youtube = build('youtube', 'v3', developerKey=api_key)\n\n    for region_code in region_codes:\n        for category_id in category_ids:\n            # Initialize the next_page_token to None for each region and category\n            next_page_token = None\n            while True:\n                # Make a request to the YouTube API to fetch trending videos\n                request = youtube.videos().list(\n                    part='snippet,contentDetails,statistics',\n                    chart='mostPopular',\n                    regionCode=region_code,\n                    videoCategoryId=category_id,\n                    maxResults=50,\n                    pageToken=next_page_token\n                )\n                response = request.execute()\n                videos = response['items']\n\n                # Process each video and collect data\n                for video in videos:\n                    video_info = {\n                        'region_code': region_code,\n                        'category_id': category_id,\n                        'video_id': video['id'],\n                        'title': video['snippet']['title'],\n                        'published_at': video['snippet']['publishedAt'],\n                        'view_count': video['statistics'].get('viewCount', 0),\n                        'like_count': video['statistics'].get('likeCount', 0),\n                        'comment_count': video['statistics'].get('commentCount', 0),\n                        'channel_title': video['snippet']['channelTitle']\n                    }\n                    video_data.append(video_info)\n\n                # Get the next page token, if there are more pages of results\n                next_page_token = response.get('nextPageToken')\n                if not next_page_token:\n                    break\n\n    return pd.DataFrame(video_data)\n\ndef preprocess_data_pyspark_job():\n    spark = SparkSession.builder.appName('YouTubeTransform').getOrCreate()\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    df = spark.read.csv(output_path, header=True)\n    \n    # Define UDF to remove hashtag data, emojis\n    def clean_text(text):\n     if text is not None:\n        # Remove emojis\n        text = emoji.demojize(text, delimiters=('', ''))\n        \n        # Remove hashtag data\n        if text.startswith('#'):\n            text = text.replace('#', '').strip()\n        else:\n            split_text = text.split('#')\n            text = split_text[0].strip()\n        \n        # Remove extra double quotes and backslashes\n        text = text.replace('\\\\\"', '')  # Remove escaped quotes\n        text = re.sub(r'\\\"+', '', text)  # Remove remaining double quotes\n        text = text.replace('\\\\', '')  # Remove backslashes\n        \n        return text.strip()  # Strip any leading or trailing whitespace\n\n     return text\n    # Register UDF\n    clean_text_udf = udf(clean_text, StringType())\n\n    # Clean the data\n    df_cleaned = df.withColumn('title', clean_text_udf(col('title'))) \\\n                   .withColumn('channel_title', clean_text_udf(col('channel_title'))) \\\n                   .withColumn('published_at', to_date(col('published_at'))) \\\n                   .withColumn('view_count', col('view_count').cast(LongType())) \\\n                   .withColumn('like_count', col('like_count').cast(LongType())) \\\n                   .withColumn('comment_count', col('comment_count').cast(LongType())) \\\n                   .dropna(subset=['video_id'])\n    \n    # Generate the filename based on the current date\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'\n    \n    # Write cleaned DataFrame to the specified path\n    df_cleaned.write.csv(output_path, header=True, mode='overwrite')   \n\n\n# Define extract task for the DAG\nextract_task = PythonOperator(\n    task_id='extract_data_from_youtube_api',\n    python_callable=extract_data,\n    op_kwargs={\n        'api_key': os.getenv('YOUTUBE_API_KEY'),\n        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],\n        'category_ids': ['1', '2', '10', '15', '20', '22', '23']\n    },\n    dag=dag,\n)\n\n# Define preprocessing task for the DAG\npreprocess_data_pyspark_task= PythonOperator(\n    task_id='preprocess_data_pyspark_task',\n    python_callable=preprocess_data_pyspark_job,\n    dag=dag\n)\n\nextract_task \u003e\u003e preprocess_data_pyspark_task\n\n\n여기서는 이 코드가 하는 일을 설명해 드렸습니다.\n\n- \"preprocess_data_pyspark_task\"라는 작업을 만듭니다.\n- 이 작업은 preprocess_data_pyspark_job 함수를 호출합니다.\n- preprocess_data_pyspark_job 함수는 데이터를 정리합니다.\n- 그리고 정리된 데이터는 \"Transformed_Youtube_Data_currentDate\"라는 폴더에 저장됩니다.\n- 이 폴더 안에는 정리된 데이터가 담긴 \"part-\" 접두사가 붙은 새 CSV 파일이 생성됩니다.\n\n만약 Airflow를 보신다면 아래와 같이 첫 번째 작업에 새로운 작업이 추가된 것을 보실 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 우리가 변환한 데이터의 모습입니다:\n\n![Transformed Data](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_8.png)\n\n이 작업은 완료되었습니다. 이제 최종 작업으로 넘어갈 차례입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 4. S3로 데이터 로드하기:\n\n이 작업을 시작하기 전에 처음에 설정한 IAM 사용자를 사용하여 S3 버킷을 생성하고 버킷 이름을 메모해주세요.\n\n우리의 최종 코드입니다!\n\n```js\nimport logging\nimport os\nimport re\nimport shutil\nfrom datetime import datetime, timedelta\n\nimport boto3\nimport emoji\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_date, udf\nfrom pyspark.sql.types import (DateType, IntegerType, LongType, StringType,\n                               StructField, StructType)\n\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\n# DAG 및 기본 인자 정의\ndefault_args = {\n    'owner': 'airflow',  # DAG 소유자\n    'depends_on_past': False,  # 이전 DAG 실행에 의존 여부\n    'email_on_failure': False,  # 실패 시 이메일 알림 비활성화\n    'email_on_retry': False,  # 재시도 시 이메일 알림 비활성화\n    'retries': 1,  # 재시도 횟수\n    'retry_delay': timedelta(minutes=5),  # 재시도 사이 간격\n    'start_date': datetime(2023, 6, 10, 0, 0, 0),  # 매일 자정(00:00) UTC에 실행\n}\n\n# DAG 정의\ndag = DAG(\n    'youtube_etl_dag',  # DAG 식별자\n    default_args=default_args,  # 기본 인수 할당\n    description='간단한 ETL DAG',  # DAG 설명\n    schedule_interval=timedelta(days=1),  # 스케줄 간격: 매일\n    catchup=False,  # 누락된 DAG 실행을 복구하지 않음\n)\n\n# YouTube API에서 데이터를 추출하는 Python 유형의 함수\ndef extract_data(**kwargs):\n    api_key = kwargs['api_key']\n    region_codes = kwargs['region_codes']\n    category_ids = kwargs['category_ids']\n    \n    df_trending_videos = fetch_data(api_key, region_codes, category_ids)\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    # DataFrame을 CSV 파일로 저장\n    df_trending_videos.to_csv(output_path, index=False)\n\n# YouTube API에서 데이터 가져오는 함수\ndef fetch_data(api_key, region_codes, category_ids):\n    \"\"\"\n    YouTube API에서 여러 국가 및 카테고리의 트렌드 비디오 데이터를 가져옵니다.\n    비디오 데이터가 포함된 Pandas 데이터 프레임 반환.\n    \"\"\"\n    # 비디오 데이터를 보관할 빈 리스트 초기화\n    video_data = []\n\n    # YouTube API 서비스 빌드\n    youtube = build('youtube', 'v3', developerKey=api_key)\n\n    for region_code in region_codes:\n        for category_id in category_ids:\n            # 각 지역 및 카테고리마다 next_page_token을 None으로 초기화\n            next_page_token = None\n            while True:\n                # YouTube API에 트렌드 비디오를 가져오도록 요청\n                request = youtube.videos().list(\n                    part='snippet,contentDetails,statistics',\n                    chart='mostPopular',\n                    regionCode=region_code,\n                    videoCategoryId=category_id,\n                    maxResults=50,\n                    pageToken=next_page_token\n                )\n                response = request.execute()\n                videos = response['items']\n\n                # 각 비디오 처리 및 데이터 수집\n                for video in videos:\n                    video_info = {\n                        'region_code': region_code,\n                        'category_id': category_id,\n                        'video_id': video['id'],\n                        'title': video['snippet']['title'],\n                        'published_at': video['snippet']['publishedAt'],\n                        'view_count': video['statistics'].get('viewCount', 0),\n                        'like_count': video['statistics'].get('likeCount', 0),\n                        'comment_count': video['statistics'].get('commentCount', 0),\n                        'channel_title': video['snippet']['channelTitle']\n                    }\n                    video_data.append(video_info)\n\n                # 결과의 추가 페이지가 있는 경우 다음 페이지 토큰 가져오기\n                next_page_token = response.get('nextPageToken')\n                if not next_page_token:\n                    break\n\n    return pd.DataFrame(video_data)\n\n# PySpark 작업 전처리 함수\ndef preprocess_data_pyspark_job():\n    spark = SparkSession.builder.appName('YouTubeTransform').getOrCreate()\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    df = spark.read.csv(output_path, header=True)\n    \n    # 해시태그 데이터, 이모지 제거를 위한 UDF 정의\n    def clean_text(text):\n     if text is not None:\n        # 이모지 제거\n        text = emoji.demojize(text, delimiters=('', ''))\n        \n        # 해시태그 및 이후 모든 것 제거\n        if text.startswith('#'):\n            text = text.replace('#', '').strip()\n        else:\n            split_text = text.split('#')\n            text = split_text[0].strip()\n        \n        # 추가 이중 인용부호와 백슬래시 제거\n        text = text.replace('\\\\\"', '')  # 이스케이프된 따옴표 제거\n        text = re.sub(r'\\\"+', '', text)  # 남은 이중 인용부호 제거\n        text = text.replace('\\\\', '')  # 백슬래시 제거\n        \n        return text.strip()  # 선행 또는 후행 공백 제거\n\n     return text\n    # UDF 등록\n    clean_text_udf = udf(clean_text, StringType())\n\n    # 데이터 정리\n    df_cleaned = df.withColumn('title', clean_text_udf(col('title'))) \\\n                   .withColumn('channel_title', clean_text_udf(col('channel_title'))) \\\n                   .withColumn('published_at', to_date(col('published_at'))) \\\n                   .withColumn('view_count', col('view_count').cast(LongType())) \\\n                   .withColumn('like_count', col('like_count').cast(LongType())) \\\n                   .withColumn('comment_count', col('comment_count').cast(LongType())) \\\n                   .dropna(subset=['video_id'])\n    \n    # 현재 날짜를 기반으로 파일 이름 생성\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'\n    \n    # 정리된 DataFrame을 지정된 경로에 작성\n    df_cleaned.write.csv(output_path, header=True, mode='overwrite')   \n\n# S3로 데이터 업로드 함수\ndef load_data_to_s3(**kwargs):\n    bucket_name = kwargs['bucket_name']\n    today = datetime.now().strftime('%Y/%m/%d')\n    prefix = f\"processed-data/{today}\"\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    local_dir_path  = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'\n    upload_to_s3(bucket_name, prefix, local_dir_path)\n\ndef upload_to_s3(bucket_name, prefix, local_dir_path):\n    aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n    aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n\n    s3_client = boto3.client(\n        's3',\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key\n    )\n\n    for root, dirs, files in os.walk(local_dir_path):\n         for file in files:\n            if file.endswith('.csv'):\n                file_path = os.path.join(root, file)\n                s3_key = f\"{prefix}/{file}\"\n                logging.info(f\"Uploading {file_path} to s3://{bucket_name}/{s3_key}\")\n                s3_client.upload_file(file_path, bucket_name, s3_key)\n\n# DAG의 추출 작업 정의\nextract_task = PythonOperator(\n    task_id='extract_data_from_youtube_api',\n    python_callable=extract_data,\n    op_kwargs={\n        'api_key': os.getenv('YOUTUBE_API_KEY'),\n        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],\n        'category_ids': ['1', '2', '10', '15', '20', '22', '23']\n    },\n    dag=dag,\n)\n\n# DAG의 데이터 전처리 작업 정의\npreprocess_data_pyspark_task= PythonOperator(\n    task_id='preprocess_data_pyspark_task',\n    python_callable=preprocess_data_pyspark_job,\n    dag=dag\n)\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 저희가 만든 최종 작업인 load_data_to_s3_task를 소개합니다. 이 작업은 load_data_to_s3 함수를 호출하여 파일을 S3 버킷에 업로드합니다. 업로드가 잘 되었는지 확인하려면 S3 버킷의 내용을 확인하세요.\n\n마침내 우리의 Airflow는 이렇게 생겼습니다!\n\n![Airflow](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_9.png)\n\n이제 이 데이터를 Tableau나 다른 BI 도구에 연결하여 흥미로운 대시보드를 만들고 인사이트를 시각화해 보세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n함께 이 파이프라인을 따라 오면서 새로운 기술 몇 가지를 배웠으면 좋겠어요! 🚀 성공적으로 여기까지 왔다면 축하해요! 🎉 이 새롭게 얻은 지식이 데이터 엔지니어링에서의 향후 모험에 큰 도움이 되길 바래요!\n\n이 프로젝트의 Github 저장소를 첨부합니다:\n\n만약 이 글을 좋아하셨다면, 공유하고, 좋아요를 눌러주시고, 아래에 댓글을 남겨주시고 구독해주세요. 🎉👏📝\n\n커튼을 닫습니다! 🎭","ogImage":{"url":"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png"},"coverImage":"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png","tag":["Tech"],"readingTime":25},{"title":"1771 Technologies가 선보이는 Graphite Grid 소개","description":"","date":"2024-06-19 09:36","slug":"2024-06-19-IntroducingGraphiteGridby1771Technologies","content":"\n\n![그래픽그리드 소개](/assets/img/2024-06-19-IntroducingGraphiteGridby1771Technologies_0.png)\n\n# 약간의 역사\n\n2023년에 1771 Technologies를 설립할 때, 우리는 ‘그냥 작동하는’ 소프트웨어를 개발하기를 목표로 했습니다. 개발자를 위한 제품을 개발하고자 했으며, 복잡한 문제를 문제없이 해결하면서도 사용성과 기능성을 희생시키지 않는 것이 우리의 목표였습니다.\n\n우리 회사는 ‘무결한 간단함’이라는 용어의 살아있는 구현입니다. 우리의 미션은 가장 요구가 많은 사용 환경에도 견디는 신뢰할 수 있고 기능적인 소프트웨어를 만들어 사용자에게 최고의 품질을 제공하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Graphite Grid의 탄생\n\n당사의 솔루션이 완벽한 간편함을 제공한다는 약속을 지속적으로 이행함에 따라, 우리는 혁신적인 소프트웨어 제품인 Graphite Grid를 소개하게 되어 매우 기쁩니다. 이 React 데이터 그리드 라이브러리는 단순히 또 다른 도구가 아니라 성능 중심 소프트웨어에서의 게임 체인저입니다.\n\nGraphite Grid는 철저한 개발, 테스트 및 정제를 거쳐 귀하의 조직이 필요로 하는 유일한 JavaScript 데이터 그리드가 되도록 보장합니다. Graphite Grid를 사용하면 고객은 데이터의 복잡성에 적응하는 도구로 스마트한 결정을 내릴 수 있으며, 타협 없이 견고한 기반 위에 애플리케이션을 구축할 수 있습니다.\n\n# 여정이 시작됩니다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자바스크립트와 리액트 커뮤니티에서 소프트웨어 제품의 훌륭한 기능과 삶을 바꿀 기능에 대해 블로그 글을 쓰는 것은 새로운 일이 아닙니다. 그래서 우리는 소프트웨어가 설득하는 모든 작업을 하도록 선호합니다.\n\n전통적인 데이터 그리드의 제한에서 벗어나 개발 프로세스를 혁신하고 싶다면, 1771technologies.com을 방문하여 더 많은 정보를 알아보세요.\n\n당신이 여정에 머무르기를 바랍니다.","ogImage":{"url":"/assets/img/2024-06-19-IntroducingGraphiteGridby1771Technologies_0.png"},"coverImage":"/assets/img/2024-06-19-IntroducingGraphiteGridby1771Technologies_0.png","tag":["Tech"],"readingTime":2},{"title":"분석력을 향상시키는 5가지 유용한 시각화 방법","description":"","date":"2024-06-19 09:34","slug":"2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis","content":"\n\n아래는 Markdown 형식으로 표를 변경한 코드입니다.\n\n\n![Visualization](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_0.png)\n\n# 소개\n\nSeaborn은 오랫동안 사용되어 왔습니다.\n\n비전문가들도 강력한 그래픽을 구축할 수 있도록 도와주기 때문에, 데이터 시각화 라이브러리 중에서 가장 유명하고 많이 사용되는 것 중 하나라고 생각합니다. 또한 통계에 근거한 통찰력을 얻는 데 도움이 되기 때문입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 통계학자가 아닙니다. 데이터 과학에 관심을 가지고 있어서 그 분야에 대한 통계 개념을 배워 직무를 더 잘 수행하기 위해 노력하고 있어요. 그래서 히스토그램, 신뢰구간, 그리고 선형 회귀 분석에 매우 적은 양의 코드로 간단하게 접근할 수 있어서 정말 좋아해요.\n\nSeaborn의 구문은 매우 기본적입니다: sns.type_of_plot(data, x, y)요. 이 간단한 템플릿을 사용하여 막대 그래프, 히스토그램, 산점도, 선 그래프, 상자 그림 등 다양한 시각화를 만들 수 있어요.\n\n하지만 이 게시물은 그것들에 대해 이야기하려는 것이 아니에요. 여러분의 분석에 차이를 만들어 줄 다른 향상된 종류의 시각화에 대해 이야기할 거예요.\n\n어떤 시각화들이 있는지 함께 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 시각화\n\n이 시각화를 만들고 이 연습과 함께 코드를 작성하려면 seaborn을 import합니다. import seaborn as sns.\n\n여기에서 사용된 데이터셋은 Paulo Cortez가 작성하고 크리에이티브 커먼즈 라이센스하에 UCI 저장소에 기즐한 학생 성적 데이터입니다. 아래 코드를 사용하여 파이썬에서 바로 가져올 수 있습니다.\n\n```js\n# UCI Repo 설치\npip install ucimlrepo\n\n# 데이터셋 로딩\nfrom ucimlrepo import fetch_ucirepo \n  \n# 데이터셋 가져오기 \nstudent_performance = fetch_ucirepo(id=320) \n  \n# 데이터 (팬더스 데이터프레임 형식) \nX = student_performance.data.features \ny = student_performance.data.targets\n\n# 시각화를 위해 X와 Y 수집\ndf = pd.concat([X,y], axis=1)\n\ndf.head(3)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_1.png)\n\nNow let's talk about the 5 visualizations.\n\n## 1. Stripplot\n\nThe first plot picked is the stripplot. And you will quickly see why this is interesting. If we use this simple line of code, it will display the following viz.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 플롯\nsns.stripplot(data=df);\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_2.png\" /\u003e\n\n와우! 거의 Pandas의 `df.describe()`와 같은 차트입니다. x축에는 모든 수치 변수가 나타납니다. y축은 해당 변수의 범위 값입니다. 따라서 그림을 보면 몇 가지 흥미로운 통찰력을 빠르게 얻을 수 있습니다.\n\n- 시각적으로 적어도 이상치 실례가 적습니다.\n- 대부분의 수치 변수는 0에서 5 사이로 범위가 있습니다. 이는 데이터의 설명에 해당 변수가 범주화되었음을 보여줍니다. 따라서 여기서 가장 좋은 방법은 해당 변수를 범주형으로 변환하는 것입니다.\n- 이 데이터셋의 학생들은 15 ~ 23세 사이입니다.\n- 첫 번째 학기(G1)와 두 번째 학기(G2) 성적의 분포는 매우 유사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정말 대단하죠! 코드 한 줄로 생성된 이 플롯에서 우리가 얻은 정보의 양을 보세요!\n\n이 그래픽에 대해 더 탐구할 만한 다른 부분이 많이 있어요. 몇 가지 다른 인수를 추가하고 개선할 수 있어요. 실수와 최종 성적 G3에 대한 그래픽을 만들어서 학교별로 분리해 봅시다.\n\n이렇게요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_3.png\" /\u003e\n\n좋아요! 실패 횟수가 적은 학생들이 높은 성적을 받고 있는 것을 예상대로 확인할 수 있어요. 그리고 두 학교 모두 성적면에서 꽤 유사해 보여요.\n\n이제 다음으로 넘어가 봅시다.\n\n## 2. Catplot\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n음-음... 우리는 stripplot을 만들었지만, 그것은 숫자형 변수에만 해당해요. 그럼 범주형 변수는 어떨까요?\n\n여기서 catplot이 유용합니다. 이것은 카테고리별 관측치를 플롯으로 나타내줄 거에요. 학교별 성적을 확인하고 싶다면, 이렇게 간단해요.\n\n```js\n#CatPlot\nsns.catplot(data=df, x='school', y='G3');\n```\n\n![이미지](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기본적으로 카테고리별로 산점도가 표시됩니다. 그러나 kind 인수를 사용하여 막대, 상자, 바이올린과 같은 다른 유형으로 변경할 수 있습니다.\n\n```js\n#플롯\nsns.catplot(data=df, x='school', y='G3', kind='box');\nsns.catplot(data=df, x='school', y='G3', kind='bar');\n```\n\n![이미지](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_5.png)\n\n이제 상자 그림이 분포를 정확하게 보여주지 않는다는 점을 살펴봅니다. 이것은 요즘 이야기가 되고 있는 주제입니다. 사람들은 통계에 익숙하지 않은 사람들을 포함해 상자 플롯을 이해하는 데 어려움을 겪는다. 따라서 이 유형의 플롯에 대한 기본값은 산점도입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 백분위수 개념을 알고 상자 그림을 보는 데 편한 사람조차도 상자의 선안에 얼마나 많은 데이터가 있는지 파악하기 어려울 수 있습니다.\n\n그래서 seaborn 팀은 이 문제를 해결하려고 다음에 제시될 향상된 상자 그림을 사용했습니다.\n\n## 3. Boxenplot\n\nBoxenplot은 향상된 상자 그림입니다. 왜냐하면 상자 그림의 선분이 없으며 상자의 크기가 각 백분위수의 데이터 양에 따라 다르기 때문입니다. 한 개를 그려봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# BoxenPlot\nsns.boxenplot(data=df, x='traveltime', y='G3');\n```\n\n다음 그림을 보면 상자의 너비가 각 백분위별 관측치의 양에 따라 변하는 것을 볼 수 있습니다. 여행 시간 범주 1(15분 이하)은 대부분의 데이터 포인트가 중간인 10에서 14 사이에 있으며, 분포는 매우 정규와 유사합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_6.png\" /\u003e\n\nTravel Time == 1의 학점만을 분리해 보면 다음과 같은 그래픽을 볼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsns.displot(df.query('traveltime == 1'),\n              x='G3', kind='kde', aspect=2)\nplt.title('Distribution KDE of Final Grade on Travel Time == 1');\n```\n\n왼쪽의 첫 번째 상자 그림과 매우 관련이 있는 것을 관찰할 수 있습니다: 하단에서 격리된 몇 개의 점에 이어 거의 정규 분포에 가까운 분포가 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_7.png\" /\u003e\n\n실제로 상자 그림을 향상시킨 것입니다. 그러나 더 많은 그래픽 유형을 공부할 필요가 있습니다. 계속 전진하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4. lmplot\n\nlmplot은 Linear Model plot의 줄임말로, 데이터셋의 간단한 선형 모델을 시각화하는 가장 쉬운 방법입니다. Grade 1과 최종 학년 점수 간의 관계를 확인하고 싶다면, 다음 코드로 선형 모델을 시각화할 수 있습니다.\n\n```js\nsns.lmplot(data=df, x=\"G1\", y='G3')\n```\n\n결과가 표시됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_8.png\" /\u003e\n\n하지만 이 함수의 장점이 최고라고 할 수는 없어요. 우리는 색조를 추가하여 다중 수준 회귀를 모의할 수 있어요. 이 경우에는 두 성별이 유사한 성능을 발휘하고 있어서, 그 범주에 기반한 계층 선형 모델을 만드는 것이 의미가 없겠죠.\n\n```js\nsns.lmplot(data=df, x=\"G1\", y= 'G3', hue='sex')\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_9.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n게다가, 대상 변수와 비교하는 다양한 선형 모델을 추가하는 것도 간단합니다. 예를 들어, col 인수를 사용하여 서로 다른 학교에 대한 모델을 추가해봅시다.\n\n```js\nsns.lmplot(data=df, x=\"G1\", y='G3', hue='sex', col='school')\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_10.png\" /\u003e\n\n보다 깊은 분석을 위한 매우 유용한 플롯입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자, 이 글의 마지막 유형을 살펴보겠습니다.\n\n## 5. residplot\n\nresidplot은 선형 회귀의 잔차를 그리는 것입니다. 하지만, 그것이 왜 중요한지요?\n\n음, 선형 모델의 잔차로 하는 테스트 중 하나는 등분산성입니다. 즉, 잔차가 균일해야 한다는 것을 확인하는 것이죠. 우리가 선 (linear)을 따르는 관계를 분석한다면, 오류도 일정 범위 내에서 선을 따를 것이라는 것이 합리적입니다. 그래서 residplot을 볼 때에는 유사한 분산을 가진 직사각형 모양을 보고 싶습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsns.residplot(data=df, x=\"G1\", y= 'G3')\n```\n\nGrade 1로 최종 성적을 예측하는 선형 모델의 잔차를 보면 거의 균일한 집합을 볼 수 있지만 몇 가지 예외가 있습니다.\n\n![residual plot](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_11.png)\n\n# 떠나시기 전에\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n좋아요. 이제 우리는 분석에 사용할 수 있는 또 다른 5가지 그래픽 유형을 갖추었습니다.\n\n좋은 탐색적 데이터 분석은 시간이 걸립니다. 그 과정에서 많은 질문들이 나타나고 데이터에 대한 이해를 풍부하게 만들어줍니다. 그래서 더 많은 작업이 필요한 질문들에 대해 심층적으로 파고들기 위한 향상된 도구 몇 개를 가지고 있는 것이 중요합니다.\n\n- stripplot: 데이터 포인트를 전체로 시각화하는 데 도움이 됩니다. describe 함수 시각화와 유사합니다.\n- catplot: stripplot과 비슷하지만 범주에 대한 것입니다. 점, 막대, 상자와 같은 여러 형태로 나타낼 수 있습니다.\n- boxenplot: 상자 그림의 향상된 버전으로 \"수염에 얼마나 많은 데이터가 있는지\"와 같은 공백을 채웁니다.\n- lmplot: 두 변수에 대한 빠르고 간단한 선형 모델을 생성할 수 있습니다. hue 인수를 사용하여 다중 수준 회귀를 시각화하거나 col 인수로 facet grid를 생성할 수도 있습니다.\n- residplot: 선형 모델의 잔차를 살펴보고 잔차의 등분산성에 대한 이탈이 어디서 발생하는지 확인하는 데 유용합니다.\n\n이 내용이 마음에 들면 팔로우해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한, LinkedIn에서 나를 찾을 수 있습니다. \n\n# 참고문헌","ogImage":{"url":"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_0.png"},"coverImage":"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_0.png","tag":["Tech"],"readingTime":7},{"title":"여기에는 왜 당신의 친구들이 당신보다 더 많은 친구를 가지고 있는지에 대한 이유가 있습니다","description":"","date":"2024-06-19 09:31","slug":"2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou","content":"\n\n![이미지](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_0.png)\n\n친구 역설은 대부분의 사람이 평균적으로 자신의 친구보다 친구가 더 많은 사회 현상입니다. 때로는 이겁니다. 이것이 일반적으로 사실인지에 대한 사실은 인기 있는 주제에 대한 기사에서 확실히 나타나지 않습니다. 한번 조사해 보겠습니다!\n\n이것을 결정하는 것이 어려울 것이라고 생각했을 것 같았지만, 아마 이것이 발견되기까지 1991년까지 걸렸기 때문일지도 모릅니다. 스콧 펠드의 원래 논문인 \"왜 당신의 친구들은 당신보다 더 많은 친구를 갖고 있을까\"에서 그는 이것이 자존감 부족의 원인일 수도 있다고 제안했습니다. 그렇다고 합니다, 사람들이 친구의 친구들마다 명부를 작성하며 친구의 목록을 유지하지는 않거든요. 이것은 1991년에는 진실일지 모릅니다만, 이제 페이스북 시대에는 이것을 쉽게 할 수 있으며 아마 많은 사람들이 그렇게 하고 있을 것입니다.\n\n# 내 책에 얼마나 많은 얼굴이 있을까\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사실, 왜 하지 않을까요? 한번 해볼까요? 페이스북에는 374명의 친구가 있어요. 왜 그런지는 잘 모르겠어요. 어쨌든, 열 명의 친구를 무작위로 선택하여 친구들이 가진 친구의 수를 세 보겠습니다. 아래는 그 결과에요:\n\n```js\n친구 1 - 522\n친구 2 - 451 \n친구 3 - 735\n친구 4 - 397\n친구 5 - 2074\n친구 6 - 534\n친구 7 - 3607\n친구 8 - 237\n친구 9 - 1171\n친구 10 - 690\n```\n\n이들의 친구들이 가진 친구의 수 평균은 1042명입니다. 그러므로 내 친구들이 평균적으로 가지고 있는 친구의 수보다 내가 더 적은 친구를 가지고 있다는 사실인데요. 또한, 이 10명 중에는 제가 친구보다 친구가 더 많은 사람의 숫자가 한 명 빼고 열 명 중 아홉 명이 속해 있어요 — 안쓰럽고 외로운 친구 8번이네요.\n\n하지만 이것이 역설이 아니에요 — 특히 저를 알고 있으면요. 제 네트워크에서 다른 누군가를 한번 살펴볼까요? 친구 2라고 가정해볼게요. 친구 2는 451명의 친구가 있고, 페이스북은 사용자의 개인정보를 자연스럽게 보호하기 때문에 친구 2의 친구들의 프로필을 클릭하여 그들이 가지고 있는 친구의 수를 확인할 수 있어요. 친구 2의 열 명의 무작위 친구들이 가진 친구의 수는 각각 790, 928, 383, 73, 827, 1633, 202, 457, 860, 121입니다. 친구 2의 친구들이 가진 친구의 수 평균은 627명으로, 451명보다 많습니다. 또한, 이 중 여섯 명이 451명보다 많은 친구를 가지고 있어요. 따라서 친구 2는 대부분의 친구들보다 친구가 더 적은 상태에요. 내 친구 모두에 대해 이 연습을 반복한다면, 더 많은 친구를 가진 대부분의 친구들이 자기 친구보다 더 적은 친구를 가지고 있음을 계속해서 발견할 수 있을 거예요, 비록 대부분이 이미 제보다 친구가 더 많은 상태일지라도!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 역설적이라고 느껴질 것 같아요. 이게 왜 역설적으로 느껴지는지에 대한 직관은 다음과 같은 비유에서 옵니다. 자신의 키를 생각해보세요. 아마도 평균보다 키가 작을 수도 있죠. 아마도 자신보다 키가 작은 친구들이 있을 것입니다. 그래도 평균보다 키가 큰 사람이 적어도 절반은 있을 것을 기대합니다. 결국 이것이 \"평균\"에 대한 직관이죠. 그렇다면 왜 친구들에 대해서는 같은 이치가 적용되지 않을까요? 즉, 친구 2는 우리 친구들보다 친구가 적지만, 분명 많은 친구를 가진 사람들은 그들의 친구들보다 더 많은 친구를 가지고 있을 것입니다. 이는 사실이지만, 이러한 인기 있는 사람들은 극히 드물며, 이것이 친구 수와 키 사이의 차이입니다.\n\n전형적인 소셜 네트워크에서는 많은 사람들이 적은 친구를 가지고 있고, 적은 사람들이 많은 친구를 가지고 있습니다. 매우 인기 있는 사람들은 많은 사람들의 친구들 목록에 포함되어 있습니다. 그리고 인기 없는 사람들은 많은 사람들의 친구들 목록에 포함되지 않습니다. 다시 페이스북 친구 관계를 예로 들어보겠습니다. 분명히 제 페이스북 친구는 적은 편이에요. 그래서 제 친구들보다 더 많은 친구를 갖고 있는 사람들이 많을 것으로 보입니다. 그렇지만 - 여기서 가장 중요한 점은 - 그 사람들은 나와 제 적은 수의 친구를 자신들의 친구 목록에 포함시키지 않을 가능성이 높습니다. 친구 2와 나는 공통의 친구가 두 명뿐이죠. 그래서 친구 2의 친구들은 그들의 친구들이 가진 친구 수를 세어보면 나와 제 적은 수의 친구가 결과를 편향시키는 모습은 볼 수 없을 겁니다.\n\n# 그게 \"가라테 키드\"가 왜 외로웠는지 그 이유일까요?\n\n몇 가지 더 작은 예시를 살펴보면 모두와 그들 친구들을 네트워크에서 세어볼 수 있는데요. 가장 유명한 소셜 네트워크 (적어도 네트워크 이론가들에게는)는 자크리의 가라테 도장입니다. 이렇게 생겼다고 해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_1.png\" /\u003e\n\n이 그래프는 1970년대의 한 가라테 클럽에 있는 34명의 사람들과 수업 외에 상호 작용하는 친구들을 보여줍니다. 색상은 각 회원이 가진 친구의 수를 안내하는 데 사용됩니다. 한 사람이 한 명의 친구만 가지고 있고 또 다른 사람은 아홉 명의 친구를 가지고 있다는 것을 볼 수 있습니다! 이미 이 그래프는 손으로 세는 것이 너무 큽니다. 그러나 컴퓨터는 네트워크의 각 노드를 단계별로 통과하여 친구와 친구의 친구를 세는 데 도움을 줄 수 있습니다. 그래서 Zachary의 가라테 클럽에서는 다음과 같습니다.\n\n```js\n평균적으로 친구보다 친구가 더 적은 사람의 비율은 85.29%입니다.\n```\n\n```js\n자신의 대부분의 친구보다 친구가 적은 사람의 비율은 70.59%입니다.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![img](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_2.png)\n\n위의 히스토그램은 데이터를 자세히 보여줍니다. 많은 친구를 가진 소수의 사람들이 다시 한 번 중요한 특징을 보실 수 있습니다. 분명히 그 소수의 사람들은 그들의 친구들보다 더 많은 친구를 가지고 있을 것입니다. 그러나 이것이 핵심입니다 — 그런 사람들은 소수뿐입니다! \"친구의 친구\"의 전체 분포를 살펴보면 상당히 평평해지고, 많은 친구를 만나기가 더 가능해집니다. 아래의 두 히스토그램은 네트워크 내 각 사람의 평균 및 중앙값 친구 수를 보여줍니다. 어느 경우든 각 사람이 대략 아홉 명의 친구를 가지고 있다는 것에 유의하십시오. 이는 네트워크 내 대부분의 사람들보다 훨씬 많습니다! 실제로 34명 중 4명만이 아홉 명 이상의 친구를 가지고 있습니다.\n\n# 큰 소셜 네트워크에 대해 생각해보세요\n\n이제 페이스북으로 돌아가 봅시다. 이번에는 십 명의 개인으로부터 수집된 데이터를 사용할 것이며, 익명화되어 공개될 것입니다. 이 데이터는 다양한 사회적 관계를 가진 이 열 명과 연결된 약 4000명의 사람들을 포함합니다. 해당 소셜 네트워크는 다음과 같습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 마크다운 형식으로 변경합니다.\n\n\n![Network Details](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_3.png)\n\n우리는 이 네트워크의 친구와 친구의 친구도 세어볼 수 있습니다. 자세한 내용은 이렇습니다.\n\n![Details](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_4.png)\n\n요약하면, 결론은 이렇습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n평균적으로 친구보다 더 적은 친구를 갖고 있는 사람의 비율은 87.47%입니다.\n```\n\n```js\n자신의 대다수 친구보다 더 적은 친구를 갖고 있는 사람의 비율은 71.92%입니다.\n```\n\n매우 유사해요! 하지만 이것이 우연일 수도 있습니다. 결국 데이터 포인트는 두 개 뿐이죠. 모든 소셜 네트워크의 이 아이디어를 테스트하는 방법은 무엇인가요? 시뮬레이션!\n\n# 가짜로 만든다면 상관없어요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시뮬레이션은 어떤 것을 이해하기 위해 그것의 규모 모델을 연구하는 도구입니다. 바람 터널에서의 모형 비행기 날개는 대표적인 예입니다. 오늘날 많은 시뮬레이션은 컴퓨터 상에서 전적으로 이루어집니다. 소셜 네트워크의 맥락에서도 다양한 모형이 있지만, 순전히 게을러서 바바라시-알버트 모형이라 불리는 것을 선택할 것입니다. 왜냐하면 사용 중인 컴퓨터 패키지인 NetworkX에 이미 구현되어 있기 때문입니다. 만약 우리가 34명의 사람(카라테 클럽과 동일한 수)으로 이루어진 모조 소셜 네트워크를 생성한다면, 이렇게 보일 것입니다.\n\n![image](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_5.png)\n\n카라테 클럽과 그리 다르지 않게 보이지 않나요? 숫자 데이터도 유사합니다.\n\n![image](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 예시에서 중요한 숫자들은 다음과 같습니다.\n\n```js\n평균적으로 자기 친구들보다 더 적은 친구를 가진 사람들의 비율은 79.41%입니다.\n```\n\n```js\n자기 친구들 대부분보다 더 적은 친구를 가진 사람들의 비율은 70.59%입니다.\n```\n\n이것도 좋지만, 시뮬레이션의 진정한 매력은 여러 예시를 빠르게 확인할 수 있다는 점입니다. 위의 것은 단지 하나의 가짜 소셜 네트워크에 불과합니다. 우리의 결론에 대한 완전한 확신을 갖기 위해서 (물론 모델 가정에 상대적으로), 무작위로 생성된 다수의 소셜 네트워크에서 많은 시뮬레이션을 실행해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 모든 그래프를 시뮬레이션해 봐요!\n\n만약 34명의 사람으로 이루어진 10,000개의 랜덤 소셜 네트워크에서 위 연습을 반복한다면,\n\n```js\n평균적으로 자신의 친구들보다 친구 수가 적은 사람들의 비율은 79.31% 입니다.\n```\n\n```js\n자신의 대부분의 친구들보다 친구 수가 적은 사람들의 비율은 65.31% 입니다.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서 우리는 이제 34명의 사람이 있는 Barabási–Albert 모델과 같은 특성을 가진 어떤 소셜 네트워크에서도 친구 역설이 계속된다고 자신 있게 말할 수 있습니다. 하지만 쉽게 할 수 있는 다른 것은 네트워크의 사람 수를 바꾸어서 대규모 네트워크에서도 추세가 지속되는지 확인하는 것입니다. 사람 수를 늘리면 상황이 훨씬 나빠집니다. 소셜 네트워크의 사람 수가 증가함에 따라 자신보다 친구가 적은 사람들의 비율(대부분 또는 평균적으로)도 증가합니다.\n\n![이미지](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_7.png)\n\n# 멋져, 멋져. 우울한 소식 이외에는 없니?\n\n이러한 역설은 친구를 넘어서도 유지된다는 것이 증명되었습니다. 자신과의 비교에서 소득, 트위터 팔로워 및 얼마나 행복한지에 대해 친구들과 비교했을 때도 아마 당신은 부족하다고 생각될 것입니다. 하지만, 좋아, 이런 건 충분해요 - 이제 우리 모두 기분 나빠하게 느끼잖아요! 분명히 이런 모든 것에서 긍정적으로 얻을 수 있는 점이 있을 거에요, 그렇죠? 네!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n친구들이 당신보다 더 많은 연락처를 가지고 있기 때문에, 그들이 바이러스나 다른 커뮤니티를 통해 전파되는 것 등을 먼저 포착할 가능성이 더 높을 것입니다. 실제로 연구자들은 질병의 전파를 측정하기 위해 무작위로 선택된 사람들을 추적하는 대신에, 그 무작위로 선택된 사람들에게 친구의 이름을 말하고 그 친구를 추적하는 것이 훨씬 효율적임을 보였습니다! 연구에서, 친구 그룹은 처음 선택된 사람들보다 2주 정도 더 빨리 아프게 되었습니다. 친구 역설에 대해 발견될 대부분의 다른 응용 프로그램들이 아마도 있을 것입니다.\n\n그러나 마지막으로 물어보고 싶은 점이 하나 있습니다: 친구 역설은 어떤 소셜 네트워크에서든 반드시 발생해야 하는가요? 이에 대한 답은 예와 아니요입니다. 역설을 사용하는 그 평균에 대한 주장에 대해서는 수학적으로 증명될 수 있는 만큼 어떤 소셜 네트워크에도 해당된다는 것이 사실입니다. 즉, 평균적인 사람들이 자신의 친구들의 수보다 작거나 같다는 주장은 상상할 수 있는 모든 소셜 네트워크에 대해 참이다. 친구들이 다수를 차지한다는 주장을 사용할 경우(대다수의 친구가 엄격히 더 많은 친구를 가지고 있는 경우), 답은 아니요입니다. 키 요소는 인기있는 사람들의 존재 또는 비존재에 있습니다. 우리는 친구들보다 더 많은 친구를 가진 친구가 대부분인 완전히 무작위인 소셜 네트워크를 만들 수도 있습니다. 다음 네트워크를 고려해보세요. 다시 34명을 기준으로 한 네트워크입니다.\n\n![네트워크 그림](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_8.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n평균적으로 친구보다 친구가 더 많은 사람의 비율은 44.12%입니다.\n```\n\n```js\n친구들보다 더 적은 친구를 가진 사람들의 비율은 44.12%입니다.\n```\n\n친구들의 구체적인 분포는 차이를 명확히 보여줍니다. 친구의 수는 평균 주변으로 골고루 분포되어 있습니다. 이는 사람들의 키를 보는 것과 유사합니다. 대략 절반의 사람들은 자신의 친구보다 더 적은 친구를 가지고 있고, 나머지 반은 자신의 친구보다 더 많은 친구를 가지고 있으며, 나머지는 자신의 친구들의 평균치와 정확히 같은 수의 친구를 가지고 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_9.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n물론, 모두가 정확히 같은 수의 친구를 가지고 있다면, 그들은 친구들보다 친구 수가 똑같을 것이라는 것은 다소 명백할 것입니다! 그러나 친구들의 분포가 더 균등하게 퍼져 있을 때에도 이는 마찬가지입니다. 이것은 무엇을 시사합니까? 저는 이것이 평등한 공동체에 대한 호의적인 주장을 뒷받침한다는 것을 시사하며, 큰 수의 연결을 유지하는 과정에서 발생하는 복잡성을 수용하기 위해 친구 관계, 기업, Twitter 팔로워 등과 같은 계층적 네트워크가 자연스럽게 성장하는 것으로 보입니다. 그러나 이것은 다른 블로그 포스트의 주제입니다. 한편,\n\n이것을 공유하지 마세요. 그 대신에, 친구에게 공유하라고 말하세요.\n\n(이 기사에 대한 모든 시뮬레이션과 데이터는 아래의 GitHub에서 확인하실 수 있습니다.)","ogImage":{"url":"/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_0.png"},"coverImage":"/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_0.png","tag":["Tech"],"readingTime":8},{"title":"2일차 상호작용형 데이터 시각화  라이브러리 선택하기 ","description":"","date":"2024-06-19 09:30","slug":"2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary","content":"\n\n![Data visualisation](/assets/img/2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary_0.png)\n\n데이터 시각화, 이 주제는 다양한 산업에서 실험되어 왔고, 많은 분들이 이미 경험해 보셨을 거에요, 그런가요? 음, 여전히 많은 사람들이 자신의 데이터를 시각화하는 데 어려움을 겪고 있다는 사실에 놀라실 겁니다. 데이터를 시각화하는 차트를 만들기 위해서는 눈에 잘 띄고 원하는 모든 요소를 포함할 수 있는 새로운 코딩 기술이 필요합니다. 파이썬이라는 말을 들어본 적이 있나요? 코딩을 배우는 데에는 상당한 노력과 땀, 그리고 눈물이 필요해요. 하지만 우리는 좀 다르게 하고 싶어요.\n\n그래서 우리의 첫 번째 글에서는 업로드된 데이터를 처리하고 다른 웹 앱에 포함될 수 있는 멋진 대화 형 그래프를 토출하기를 원하는 에이전트를 원했다는 이야기를 나눴어요. 두 가지 선택지가 있었어요. 우리만의 그래프 라이브러리를 만드는 불편함을 감수할 건지, 아니면 이미 있는 오픈 소스를 사용할 건지 말이죠. 삶을 어렵게 만들 이유가 없죠? 그래서 우리는 오픈 소스 옵션을 선택했어요. 우리가 선택한 라이브러리를 보여드릴게요. 만약 데이터 시각화가 당신의 분야라면 꼭 한 번 확인해보세요...\n\n# 선택된 라이브러리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n생성된 차트가 상호작용적이 되려면 클라이언트 측(브라우저 내)에서 렌더링되어야 했습니다. 서버 측에서 차트를 생성하려면 차트를 정적 png 또는 jpeg 파일로 저장한 다음 해당 파일을 클라이언트에 제공하여 표시해야 합니다. 문제는 이러한 차트가 정적이 된다는 점입니다. ChatGPT가 데이터를로드하면 시각화 차트를 생성하는 방식과 유사합니다. 우리가 커뮤니티를 위해 만들고자 했던 것은 이것보다 더 뛰어난 것이었습니다. 그래서 몇 차례의 연구 끝에 우리는 결정을 내렸습니다.👇\n\n## ECharts\n\n# ECharts란?\n\nECharts는 멋진 오픈 소스 자바스크립트 데이터 시각화 라이브러리로, 강력한 렌더링 엔진을 제공하여 디자인이 우아하고 브라우저에서 표시될 때 상호작용적인 다양한 데이터 시각화 차트를 만들 수 있습니다. 📈🙌\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nECharts의 각 차트에는 다른 데이터 구조가 있어서 차트가 제대로 표시됩니다. 놀라운 다양한 차트뿐만 아니라 활기찬 커뮤니티도 자랑합니다, 그래서 지원이 필요할 때 언제나 단기간에 부족함이 없습니다 💪.\n\n아래의 예시들을 확인해보세요👇\n\n![Example 1](/assets/img/2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary_1.png)\n\n![Example 2](/assets/img/2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 이것이 AI 에이전트와 어떤 관련이 있는지 알아보겠어요.\n\n알겠어요, ECharts를 선택했죠? ✅ 다음 단계는 에이전트의 인프라를 설정해야 해요. 앱의 클라이언트 측에 전달할 수 있는 구성요소를 만들어야 해요. 이를 통해 ECharts가 제공하는 모든 차트 유형을 보여줄 수 있어요 🤝. 그래서 계획은 이렇습니다: 에이전트는 업로드된 데이터의 스키마에 따라 차트를 선택할 거예요. 그런 다음 차트가 필요로 하는 데이터 구조를 결정하고, 원시 데이터를 처리하여 해당 구조를 채우고, 마지막으로 브라우저에서 렌더링할 수 있는 형태로 패키지화해야 해요. 계획처럼 듣긴 하죠 🎯? AI 에이전트를 위한 시스템 아키텍처를 설계해 봅시다. Article 3에서 만납시다 🤝.\n\n![image](/assets/img/2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary_3.png)\n\n# 함께 참여하세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n계속 주목해주세요! 앞으로 30일 동안 매일 글을 게시할 예정이에요. 저희 진행 상황에 대한 업데이트도 있을 거예요 🚀. 만약 이 에이전트의 첫 사용자 중 한 명이 되고 싶다면 www.cubode.com 으로 이동해서 첫 액세스를 받으세요.","ogImage":{"url":"/assets/img/2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary_0.png"},"coverImage":"/assets/img/2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary_0.png","tag":["Tech"],"readingTime":3},{"title":"SQL과 Power BI를 활용한 고객 세분화 보고서","description":"","date":"2024-06-19 09:28","slug":"2024-06-19-CustomerSegmentationReportwithSQLandPowerBI","content":"\n\n이 프로젝트에서는 RFM(Recency, Frequency, Monetary) 개념을 사용하여 판매 데이터 세트를 분석할 것입니다. 저의 목표는 고객을 세분화하여 최상위 구매자와 무시되고 있을 수 있는 고객 및 그들을 유지하기 위해 타겟 광고 노력이 더 필요한 사람들을 식별하는 것입니다.\n\nRFM 보고서는 세 가지 주요 지표를 사용하여 고객을 세분화하는 방법입니다:\n\n1. Recency(최근성): 고객의 마지막 구매가 얼마나 오래되었나요?\n   \n2. Frequency(빈도): 고객이 얼마나 자주 구매하나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3: 금액: 고객이 구매에 얼마나 지출하는가?\n\n## 프로젝트 계획\n\n- 데이터 정리\n- 데이터셋 개요\n- 분석\n- 최근성, 빈도, 금액 (RFM) 보고서\n- RFM을 사용한 고객 세분화\n- Power BI 대시보드\n\n## 1. 데이터 정리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터를 조사한 결과 특정 기능에 누락된 값이 있는 것을 발견했어요. 그래서 이를 \"없음\"으로 대체해 주었어요. 다행히도, 저가 분석할 주요 기능들에는 누락된 값이 없었어요.\n\n```js\n-- 데이터 확인하기 --\nselect*\nfrom [dbo].[sales_data];\n\n-- 데이터 정리하기 --\n\n-- Addressline2, Postalcode 및 State 열의 누락된 값 찾기\nselect *\nfrom [dbo].[sales_data]\nwhere ADDRESSLINE2 IS NULL OR STATE IS NULL OR POSTALCODE IS NULL;\n\n-- 누락된 값 처리하기\nupdate [dbo].[sales_data]\nset ADDRESSLINE2 = case when ADDRESSLINE2 IS NULL then 'None' else ADDRESSLINE2 end,\n    STATE = case when STATE IS NULL then 'Unknown States' else STATE end,\n    POSTALCODE = case when POSTALCODE IS NULL then 'None' else POSTALCODE end;\n```\n\n![이미지](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_0.png)\n\n## 2. 데이터셋 개요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2.1 몇 가지 변수에서의 독특한 관찰\n\n회사는 \"Productline\" 열에서 나타나는 것처럼 열차, 오토바이, 비행기, 클래식 자동차 등의 판매에 특화되어 있습니다. 데이터는 2003년부터 2005년까지의 기간을 다룹니다.\n\n```js\n-- 일부 특성의 고유 관측치\nselect distinct STATUS from [dbo].[sales_data]\nselect distinct YEAR_ID from [dbo].[sales_data]\nselect distinct PRODUCTLINE from [dbo].[sales_data]\nselect distinct COUNTRY from [dbo].[sales_data]\nselect distinct DEALSIZE from [dbo].[sales_data]\nselect distinct TERRITORY from [dbo].[sales_data]\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_1.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1.2 각 연도별 고유한 월\n\n2005년의 월만 표시한 것은 그렇다고 하더라도, 2003년과 2004년은 둘 다 모든 12개월을 포함했다는 것을 기억하는 것이 중요합니다. 아래 출력에서 확인할 수 있듯이, 2005년 데이터는 5월까지만 있는 것을 알 수 있습니다.\n\n```js\n-- 각 연도별 고유한 월\nselect distinct MONTH_ID as Month_Id_2003  from [dbo].[sales_data]  where YEAR_ID = 2003 order by MONTH_ID ASC;\n\nselect MONTH_ID as Month_Id_2004 from [dbo].[sales_data]  where YEAR_ID = 2004 order by MONTH_ID ASC;\n\nselect distinct MONTH_ID  as Month_Id_2005 from [dbo].[sales_data]  where YEAR_ID = 2005 order by MONTH_ID ASC;\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_2.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2. 분석\n\n2.1 각 제품 라인에서 얼마나 벌고 있나요?\n\n클래식 자동차가 가장 많은 구매를 했고, 가장 적은 구매는 기차였습니다.\n\n```js\n--- 제품 라인별로 매출을 그룹화하여 계산해 봅시다\nselect PRODUCTLINE, SUM(sales) Revenue \nfrom   [dbo].[sales_data]\ngroup by PRODUCTLINE\norder by 2 desc\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_3.png\" /\u003e\n\n2.2 매년 매출액은 어떻게 되나요?\n\n결과를 통해 2004년이 가장 높은 매출을 기록했으며, 2003년이 이어서 나왔으며, 2005년의 매출액은 가장 낮았다는 것을 알 수 있습니다. 이 차이는 2005년이 단 5개월에 불과하기 때문에 전체 매출량이 2003년과 2004년의 연간 데이터와 비교하여 상대적으로 작기 때문입니다.\n\n```js\n-- 매년 매출액\nselect YEAR_ID, SUM(sales) Revenue \nfrom [dbo].[sales_data]\ngroup by YEAR_ID\norder by 2 desc \n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Customer Segmentation Report](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_4.png)\n\n2.3. What was the best month for sales in a specific year and how much was earned that month?\n\nIn 2003\n\nThere were more purchases in November and the next highest sales was October.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n-- 2003년 정보에 대하여\nselect MONTH_ID, sum(sales) as 수익, count(ordernumber) as 빈도 \nfrom [dbo].[sales_data]\nwhere YEAR_ID = 2003\ngroup by MONTH_ID\norder by 2 desc\n```\n\n![고객 세분화 보고서](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_5.png)\n\n2004년에도\n\n2004년에도 11월에 가장 높은 매출이 있었으며, 다음으로 10월에 높은 매출이 있었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n-- 2004 년도를 위한\nselect MONTH_ID, sum(sales) as Revenue, count(ordernumber) as Frequency \nfrom [dbo].[sales_data]\nwhere YEAR_ID = 2004\ngroup by MONTH_ID\norder by 2 desc\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_6.png\" /\u003e\n\n2005년도에\n\n2005년에는 최다 구매가 5월에 발생하여 올해의 판매 정점을 나타냈습니다. 세 연도 동안의 처음 다섯 개월 구매를 비교하면, 2005년의 판매량이 비교적 높고, 그 다음이 2004년이며, 가장 낮은 것은 2003년입니다. 이는 회사의 매출이 연도를 통틀어 꾸준히 증가하고 있다는 것을 시사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n-- 2005년 기준\nselect MONTH_ID, sum(sales) as 매출액, count(ordernumber) as 주문빈도\nfrom [dbo].[sales_data]\nwhere YEAR_ID = 2005\ngroup by MONTH_ID\norder by 2 desc\n```\n\n![이미지](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_7.png)\n\n2.4. 회사가 11월에 일반적으로 가장 높은 매출액을 기록하는 제품은 무엇인가요?\n\n11월에는 클래식 및 빈티지 자동차의 구매가 두드러지게 증가하여 2003년부터 2004년까지 전반적인 높은 매출에 크게 기여했습니다. 반면에 기차는 동일한 달에 회사의 수익 최소화를 보여주었습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n-- 2003년 11월\nselect MONTH_ID as November_2003, PRODUCTLINE, sum(sales) as Revenue, count(ordernumber) as Frequency \nfrom [dbo].[sales_data]\nwhere YEAR_ID = 2003 and MONTH_ID=11\ngroup by MONTH_ID, PRODUCTLINE\norder by 3 desc\n\n-- 2004년 11월\nselect MONTH_ID as November_2004,PRODUCTLINE, sum(sales) as Revenue, count(ordernumber) as Frequency \nfrom [dbo].[sales_data]\nwhere YEAR_ID = 2004 and MONTH_ID=11\ngroup by MONTH_ID, PRODUCTLINE\norder by 3 desc\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_8.png\" /\u003e\n\n## 4. 최근성, 주파수 및 금액 (RFM) 보고서\n\n최근성은 마지막 주문 또는 구매 이후 경과한 일 수를 나타내며, Euro Shopping Channel과 La Rochelle Gifts는 데이터에서 가장 최근 구매를 한 날짜에 해당합니다(최대 날짜). 최근성 값이 높을수록 마지막 구매 이후의 기간이 길어집니다. RFM 결과에 따르면 Toys of Finland. Co는 데이터 집합에서 현재 또는 최대 날짜로부터 100일 이상이 지난 마지막 구매를 했습니다.\n또한, 주문 사이의 평균 일 수를 살펴보면 고객들이 얼마나 자주 구매를 하는지에 대한 통찰을 제공합니다. Euro Shopping Channel은 평균 3일 간격으로 가장 꾸준한 고객 중 하나로 나타납니다.\n마찬가지로, 고객 당 금액 값을 검토하면 Euro Shopping Channel이 가장 많이 지출한 것을 확인할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sql\n-- RFM 분석\nselect CUSTOMERNAME,\n   sum(sales) as MonetaryValue,\n   avg(sales) as AvgMonetaryValue,\n   count(*) as Frequency_of_orders,\n   count(distinct convert(DATE, orderdate)) AS Number_of_unique_order_dates,\n   convert(INT, round(cast(datediff(dd, min(ORDERDATE), max(ORDERDATE)) as decimal)/(count(*)-1),0)) as [Aveg_day_between_orders],\n   datediff(DD,max(orderdate),( select max(orderdate) from [dbo].[sales_data])) as recency \nfrom [dbo].[sales_data]\ngroup by CUSTOMERNAME\nhaving count(*)\u003e1\norder by recency\n```\n\n![RFM Analysis](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_9.png)\n\n## 5.0 RFM 보고서 기반의 고객 세분화\n\n제공된 결과는 RFM 기준에 따른 고객 세분화에 대한 통찰을 제공합니다. rfm_recency=1에 속한 고객은 최근 구매 이후 지난 기간이 길어서, 데이터 세트에서 최대 날짜로부터 112일까지 구매를 한 Boards \u0026 Toys Co.와 같이 특징 지어집니다. 반면, rfm_recency=4는 최근 구매를 나타내며, 데이터 세트에서 가장 최근 구매가 14일 전인 Salzburg Collectables과 같은 상황입니다. 이는 rfm_recency=1 또는 2에 속한 고객이 마지막 주문 이후 상당한 시간이 지나자 \"잠재적 이탈 고객\"일 수 있다는 것을 나타냅니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비슷하게, rfm_monetary에 대해서는 소비가 높은 고객이 rfm_monetary=4로 분류되며, 소비가 적은 고객은 rfm_monetary=1로 분류됩니다. rfm_frequency에 대해, rfm_frequency=4인 고객은 가장 많은 주문을 보유하고 있으며, rfm_frequency=1은 가장 적은 주문을 가진 고객을 나타냅니다.\n\n이러한 RFM 카테고리를 분석하여, 최근 구매를 보유한 최고의 고객은(rfm_recency=4), 가장 높은 금액을 지출한 고객(rfm_monetary=4) 및 가장 많은 주문을 가진 고객(rfm_frequency=4)으로 식별할 수 있습니다.\n\n\n---고객을 네 가지 세분류로 분류합니다.\n\nwith customer_segment as\n(\n select \n   CUSTOMERNAME,\n   sum(sales) as MonetaryValue,\n   count(ordernumber) as Frequency_of_orders,\n   max (orderdate) as last_order_date,\n   (select max(orderdate) from [dbo].[sales_data]) as max_order_date,\n   datediff(dd, max(orderdate),(select max(orderdate) from [dbo].[sales_data])) as Recency \n from [dbo].[sales_data]\n group by CUSTOMERNAME \n)\nselect*,\n  NTILE(4) OVER (order by Recency desc) as rfm_recency,\n  NTILE(4) OVER (order by Frequency_of_orders) as rfm_frequency,\n  NTILE(4) OVER (order by MonetaryValue) as rfm_monetary\nfrom customer_segment \n\n\n![Customer Segmentation Report](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_10.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_11.png\" /\u003e\n\n5.1 상기 Segmentation을 기반으로 고객에게 Segment 이름 할당하기.\n고객 Segmentation을 고려할 때, 복합 코드가 444인 고객이 최고의 고객을 대표합니다. 첫 번째 4는 rfm_recency 그룹 4에 속한 고객을 나타내며 최근 구매를 의미합니다. 두 번째 4는 rfm_frequency 그룹 4에 속한 고객을 나타내어 많은 주문을 했음을 의미합니다. 마지막 4는 rfm_monetary 그룹 4에 속한 고객들을 나타내어 더 많은 돈을 소비했다는 것을 의미합니다.\n\n반대로, 복합 그룹이 111인 고객들은 오랜 시간 구매를 하지 않은, 주문 수가 적고 지금까지 가장 적은 금액을 소비한 고객들입니다.\n\n```js\n-- 우리 최고의 고객은 누구인가요?\n\nDROP TABLE IF EXISTS #customer_segment; -- 테이블 #rfm 생성\nwith customer_segment as\n(\n select \n CUSTOMERNAME,\n sum(sales) as MonetaryValue,\n avg(sales) as AvgMonetaryValue,\n COUNT(ordernumber) as Frequency,\n MAX(orderdate) as last_order_date,\n (select max(orderdate) from [dbo].[sales_data]) as max_oder_date,\n DATEDIFF(DD, MAX(orderdate),(select max(orderdate) from [dbo].[sales_data])) as Recency \n from [dbo].[sales_data]\n group by CUSTOMERNAME \n),\n\nrfm_segmentation as \n  (\n  select*,\n  NTILE(4) OVER (order by Recency desc)as rfm_recency,\n  NTILE(4) OVER (order by Frequency) as rfm_frequency,\n  NTILE(4) OVER (order by MonetaryValue) as rfm_monetary\n   --(NTILE(4) OVER (order by Recency)) + (NTILE(4) OVER (order by Frequency)) + (NTILE(4) OVER (order by AvgMonetaryValue)) AS CompositeScore\n  from customer_segment \n )\n\nselect \n c.*, rfm_recency + rfm_frequency + rfm_monetary as rfm_cell,\n CAST(rfm_recency as varchar)+ CAST(rfm_frequency as varchar)+ CAST(rfm_monetary as varchar) as rfm_cell_string\n\ninto #customer_segment -- 스크립트를 실행하면서 테이블을 생성하는 데 도움이 됩니다. 맨 위에 #rfm 테이블이 이미 생성되었습니다.\nfrom rfm_segmentation c\n\n\n\nselect CUSTOMERNAME, rfm_recency,rfm_frequency, rfm_monetary,rfm_cell_string,\n  CASE\n  when rfm_cell_string in (111, 112, 121, 122, 211, 212, 114, 141, 123, 131,132,141,142,221, 231,232,242,241) then '잃어버린 나 새로운 고객'\n  when rfm_cell_string in (222,223, 233,134,143,133,144,234,232,244) then '사라져가지만 돈을 많이 쓰는 고객' -- 회사는 그들이 떠나는 것을 방치해서는 안됩니다\n  when rfm_cell_string in (411,412,421,422,342,442,322,332,311) then '정기적이지만 소비가 적은 고객'\n  when rfm_cell_string in (424,423,432,324,414,324,331) then '좋은 고객들' \n  when rfm_cell_string in (444,443,434,344,333,334, 343,433) then '최고의 고객들' \n  end rfm_segment\n\nfrom #customer_segment\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![RFM Report with Power BI](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_12.png) \n\n![RFM Report with Power BI](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_13.png) \n\n## 6. Power BI를 사용한 RFM 보고서\n\nPower BI 대시보드를 통해 Euro Shopping Channel이 최고의 성과를 거두었음이 명확히 드러납니다. 해당 채널은 가장 많은 고유 주문을 하여 총 25개를 달성하였고, 또한 최근 구매자 중에서도 속해 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대시보드에서 주목할 만한 관찰 중 하나는 고객 세그먼트별 매출을 분석할 때 \"우수 고객\"으로 표시된 범주가 의외로 가장 낮은 매출을 보인다는 것입니다. 이는 세분화 규칙을 적용한 후 이 범주에 속하는 고객이 단 하나뿐이기 때문입니다. 따라서 이 한 명의 고객으로부터 발생한 총 매출이 다른 세그먼트에 속한 여러 고객들의 집계 매출보다 낮을 수밖에 없는 것입니다.\n\n![이미지](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_14.png)\n\n끝\n프로젝트를 읽어 주셔서 감사합니다.\nLinkedIn: www.linkedin.com/in/kofi-duodu-siriboe-a184021b2","ogImage":{"url":"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_0.png"},"coverImage":"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_0.png","tag":["Tech"],"readingTime":11},{"title":"이상치에 대해 과하게 생각하지 마세요, 대신 t-분포를 사용해보세요","description":"","date":"2024-06-19 09:24","slug":"2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead","content":"\n\n## R과 Brms를 사용한 베이지안 접근\n\n다수의 연구자들에게 이상값은 분석의 경로를 급진적으로 바꿀 수 있는 로그 파도와 동일하며 일부 예상된 효과를 \"혼란\"시킬 수 있습니다. 저는 \"극단적인 관측치\"라는 용어를 사용하여 이상값은 연구 중인 모집단의 일부가 아닌 관측치를 남깁니다. 예를 들어, 제 분야인 뇌 허혈 연구에서 이상값은 허혈이 있어야 하는 동물인데 실제로 허혈이 없는 동물이며, 극단적인 관측치는 다른 동물들과 매우 다른 작거나 큰 허혈이 있는 동물입니다.\n\n전통적 (빈도론적) 통계 모델은 가우시안 분포의 견고한 기반 위에 구축됩니다. 이에는 중심 한계 정리에 따라 모든 데이터 포인트가 중심 평균 주변에 예측 가능한 패턴으로 군집화될 것이라는 내재된 가정이 존재하는 중요한 제한 사항이 있습니다. 이것은 이상적으로는 플라톤의 이상 세계에서 사실일 수 있지만, 우리 연구자들인 생명 의학 분야의 우리는 이 가정에 의존하기 어려운 초기 샘플링(동물의 수) 때문에 도전적이라는 것을 알고 있습니다.\n\n가우시안 분포는 극단적인 관측치에 매우 민감하며, 그들의 사용은 이상적인 결과를 얻는 가장 좋은 방법은 극단적인 관측치를 제거하는 것이라는 오해를 유발합니다. 이에 대해 한번 심사위원 2로서 기사에서 언급했던 것처럼, \"문제는 귀하의 효과를 '가리게' 할 수 있는 극단적인 관측치가 아니라 귀하의 목적에 부적절하다고 믿는 통계 모델을 사용하고 있다는 사실입니다\".\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다행히도, 가우시안 모델의 가정에 얽매이지 않아도 된다는 것 알고 계시나요? 다른 선택지가 있으니까요, 바로 스튜던트 t-분포입니다. 저는 실제 생물의 응답을 포착하기 위한 좀 더 적응 가능한 수달로 보고 있어요. 스튜던트 t-분포는 우리 데이터가 어떤 맥락에서든 예상할 수 있는 정상 생물학적 응답인 극단적 관측치로 채워질 수 있음을 인정하기 위한 강력한 대안을 제공해줘요. 어떤 환자나 동물은 치료에 반응하지 않거나 과도하게 반응할 수 있고, 모델링 접근법이 이러한 반응을 스펙트럼의 일부로 인식하는 것은 귀중하답니다. 따라서 이 튜토리얼은 R의 brms 패키지를 통해 스튜던트 t-분포를 사용한 모델링 전략을 탐색합니다. 이 패키지는 베이지안 모델링에 강력한 동반자입니다.\n\n# 스튜던트 t-분포의 밑받침에 있는 것은 무엇인가요?\n\n스튜던트 t-분포는 꼬리가 두꺼운 가우시안 분포에 불과해요. 다시 말해, 가우시안 분포는 스튜던트 t-분포의 특별한 경우라고 할 수 있어요. 가우시안 분포는 평균(μ)과 표준 편차(σ)로 정의돼요. 그러나 스튜던트 t-분포는 자유도(DF)라는 추가 매개변수가 더해진다는 점이 다르죠. 이 자유도 매개변수는 분포의 \"두께\"를 제어하며, 평균으로부터 더 먼 사건에 더 높은 확률을 할당해줘요. 이 특성은 바이오의학과 같이 정상성 가정이 의심스러운 소량의 표본 크기에서 특히 유용해요. 자유도가 증가함에 따라 스튜던트 t-분포는 가우시안 분포에 가까워져요. 이를 밀도 플롯을 이용해 시각적으로 확인할 수 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-너무 생각하지 마세요 아웃라이어를 생각하지 마세요. 그 대신 스튜던트 t-분포를 사용하세요_0.png\" /\u003e\n\n그림 1에서 평균 주변의 언덕이 자유도가 줄어들면서 더 작아짐을 주의하십시오. 그 결과로 미리 수집되었던 확률이 두꺼운 꼬리로 이동했습니다. 이 특성은 스튜던트 t-분포가 아웃라이어에 대한 민감도를 줄여주는 것입니다. 이에 대한 자세한 내용은 블로그를 참조해주세요.\n\n# 필요한 패키지 로드\n\n필요한 라이브러리를 로드합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```r\nlibrary(ggplot2)\nlibrary(brms)\nlibrary(ggdist)\nlibrary(easystats)\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(ghibli)\n```\n\n# 탐색적 데이터 시각화\n\n그래서, 데이터 시뮬레이션을 건너뛰고 진지하게 할 차례입니다. 쥐들이 로타로드 테스트를 수행하는 중 획득한 실제 데이터로 작업할 것입니다.\n\n먼저, 데이터셋을 환경에 로드하고 해당 요소 수준을 설정합니다. 이 데이터셋에는 동물들의 ID, 그룹 변수(Genotype), 테스트가 수행된 두 가지 다른 날짜를 나타내는 지표(day), 같은 날짜에 대한 다양한 시도가 포함되어 있습니다. 이 문서에서는 시도 중 하나(Trial3)만 모델링합니다. 다른 시도들은 변이 모델링에 대한 미래 문서에서 저장할 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 처리를 고려하면 저희의 모델링 전략은 유전형과 일자를 Trial3의 분포의 범주형 예측변수로 사용할 것입니다.\n\n```R\ndata \u003c- read.csv(\"Data/Rotarod.csv\")\ndata$Day \u003c- factor(data$Day, levels = c(\"1\", \"2\"))\ndata$Genotype \u003c- factor(data$Genotype, levels = c(\"WT\", \"KO\"))\nhead(data)\n```\n\n![image](/assets/img/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead_1.png)\n\nGuilherme A. Franchi, 박사의 훌륭한 블로그 게시물에서 소개된 Raincloud 그림을 사용하여 데이터의 초기 전망을 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```R\nedv \u003c- ggplot(data, aes(x = Day, y = Trial3, fill=Genotype)) +\n  scale_fill_ghibli_d(\"SpiritedMedium\", direction = -1) +\n  geom_boxplot(width = 0.1,\n               outlier.color = \"red\") +\n  xlab('Day') +\n  ylab('Time (s)') +\n  ggtitle(\"Rorarod performance\") +\n  theme_classic(base_size=18, base_family=\"serif\")+\n  theme(text = element_text(size=18),\n        axis.text.x = element_text(angle=0, hjust=.1, vjust = 0.5, color = \"black\"),\n        axis.text.y = element_text(color = \"black\"),\n        plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5),\n        legend.position=\"bottom\")+\n  scale_y_continuous(breaks = seq(0, 100, by=20), \n                     limits=c(0,100)) +\n# Line below adds dot plots from {ggdist} package \n  stat_dots(side = \"left\", \n            justification = 1.12,\n            binwidth = 1.9) +\n# Line below adds half-violin from {ggdist} package\n  stat_halfeye(adjust = .5, \n               width = .6, \n               justification = -.2, \n               .width = 0, \n               point_colour = NA)\nedv\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-Do not over-think about outliers, use a Student-t distribution instead_2.png\" /\u003e\n\nFigure 2은 Guilherme A. Franchi, 박사의 원본과 다릅니다. 왜냐하면 우리는 한 가지가 아닌 두 가지 요인을 그래픽 표현하고 있기 때문입니다. 그러나 그래프의 성격은 같습니다. 빨간 점들에 주목해 주세요. 이들이 중심 경향의 평균을 특정 방향으로 기울이는 극단적인 관측값으로 간주될 수 있는 것입니다. 또한 분산이 다른 것을 관찰하며, 따라서 시그마도 모델링하는 것이 더 나은 추정치를 제공할 수 있습니다. 이제 우리의 작업은 brms 패키지를 사용하여 출력을 모델링하는 것입니다.\n\n# brms를 사용하여 통계 모델 맞추기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희는 Day 및 Genotype를 상호 작용하는 범주형 예측 변수로 설정하여 Trial 3의 분포에 모델을 맞추었어요. 먼저 자주 쓰이는 가우스 모델을 적합해볼게요. 이 모델은 빈도주의적 프레임워크에서의 일반적인 최소 제곱(OLS) 모델에 해당합니다. 왜냐하면 우리는 기본 평평한 brms 사전 분포를 사용하기 때문이죠. 이 기사에서는 사전 분포는 다루지 않겠지만, 약속드려요. 미래 블로그에서 다뤄볼 거에요.\n\n가우시안 모델 결과를 얻으면, 학생 t 모델의 큰 결과와 비교할 수 있어요. 그 다음, 데이터 분산의 차이를 고려하기 위해 방정식에 addsigma를 추가해요.\n\n## 가우시안 랜드에서 \"전형적인\" (빈도주의) 모델 적합\n\n우리의 가우시안 모델은 동질 분산성(3)의 전형적(그리고 종종 잘못된) 가정 하에 구축되었어요. 다시 말해, 우리는 모든 그룹이 동일한(또는 매우 유사한) 분산을 갖는다고 가정해요. 연구자로서 이를 본 적이 없어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```r\nGaussian_Fit1 \u003c- brm(Trial3 ~ Day * Genotype, \n           data = data, \n           family = gaussian(),\n           # reproducibility를 위한 seed\n           seed = 8807,\n           control = list(adapt_delta = 0.99),\n           # 모델을 내 노트북에 저장하기 위함\n           file    = \"Models/20240222_OutliersStudent-t/Gaussian_Fit1.rds\",\n           file_refit = \"never\")\n\n# 모델 비교를 위해 loo 추가\nGaussian_Fit1 \u003c- \n  add_criterion(Gaussian_Fit1, c(\"loo\", \"waic\", \"bayes_R2\"))\n```\n\n## 모델 진단\n\n계속하기 전에 실제 관측값과 모델의 예측을 비교하기 위해 몇 가지 간단한 모델 진단을 하는 것이 좋습니다. 이를 다양한 방법으로 수행할 수 있지만, 가장 흔한 방법은 전체 밀도를 그래픽으로 나타내는 것입니다. brms의 pp_check 함수를 사용하여 이를 달성할 수 있습니다.\n\n```r\nset.seed(8807)\n\npp_check(Gaussian_Fit1, ndraws = 100) +\n  labs(title = \"가우시안 모델\") +\n  theme_classic()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003ctable\u003e 태그를 Markdown 형식으로 바꿔보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 bayestestR 패키지 (4)의 describe_posterior 함수를 사용하여 결과를 확인해 봅시다:\n\n```js\ndescribe_posterior(Gaussian_Fit1,\n                   centrality = \"mean\",\n                   dispersion = TRUE,\n                   ci_method = \"HDI\",\n                   test = \"rope\",\n                   )\n```\n\n![그림](/assets/img/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead_4.png)\n\n여기서 'intercept'(절편)와 'GenotypeKO'(KO 동물의 같은 시간대 추정 차이)에 집중해 보겠습니다. WT 동물은 로타로드에서 약 37초를 보내는 반면, KO 동물은 한초도 안 되는 0.54초를 더 보냅니다. 이 분야의 연구자로서, 이 차이는 무의미하며 유전자형은 로타로드 성능에 영향을 미치지 않는다고 말할 수 있습니다. 또한, 모델에서 2.9인 Day의 영향도 무의미하다고 생각됩니다. 이러한 추정치를 brms의 훌륭한 conditional_effects 함수를 사용하여 쉽게 시각화할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```R\n# Convex hull을 위한 그래프를 생성합니다\nGaussian_CondEffects \u003c- \n  conditional_effects(Gaussian_Fit1)\n\nGaussian_CondEffects \u003c- plot(Gaussian_CondEffects, \n       plot = FALSE)[[3]]\n\nGaussian_CondEffects + \n  geom_point(data=data, aes(x = Day, y = Trial3, color = Genotype), inherit.aes=FALSE) +\n  Plot_theme +\n  theme(legend.position = \"bottom\", legend.direction = \"horizontal\")\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead_5.png\" /\u003e\n\nFigure 8에서 상호 작용 항목에 대한 추정치와 불확실성을 볼 수 있습니다. 여러 ggplot 요소로 플롯을 사용자 정의했으며, 원래 Quarto Notebook에서 확인할 수 있습니다. 날 1과 날 2 모두 유사한 불확실성을 보여줍니다. 날 1에 더 큰 분산이 있지만, 날 2에는 낮습니다. 이 점에 대해 이 기사 끝에 짧은 조각을 통해 다룰 예정입니다.\n\n이제 동일한 데이터를 학생 t 분포를 사용하여 모델링할 때 얼마나 우리의 이해가 얼마나 변하는지 살펴봅시다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 방문해 주셔서 감사합니다: 학생-t 분포를 사용한 모델 적합\n\n우리 `brms` 모델에서 학생-t 분포를 사용할 시간입니다.\n\n```js\nStudent_Fit \u003c- brm(Trial3 ~ Day * Genotype, \n           data = data, \n           family = student,\n           # 재현성을 위한 시드\n           seed = 8807,\n           control = list(adapt_delta = 0.99),\n           # 모델을 내 노트북에 저장하기 위함\n           file    = \"Models/20240222_OutliersStudent-t/Student_Fit.rds\",\n           file_refit = \"never\")\n\n# 모델 비교를 위한 loo 추가\nStudent_Fit \u003c- \n  add_criterion(Student_Fit, c(\"loo\", \"waic\", \"bayes_R2\"))\n```\n\n## 모델 진단\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전과 같이 모델 진단 결과를 그래프로 표현해봤어요:\n\n![Figure 9](/assets/img/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead_6.png)\n\nFigure 9에서 관측값과 예측값의 평균 모양과 정점이 일치하는 것을 볼 수 있어요. 우리 모델은 0 미만의 값을 예측하는 것으로 보입니다. 이는 현재는 건너뜁니다만 중요한 연구 이슈입니다. 그러나 이는 0 이하의 값에 대한 하한선을 설정하는 정보 전제나 분포 계열(예: log_normal`,hurdle_lognormal’, or `zero_inflated_poisson’)의 사용을 함축합니다. 경우에 따라 Andrew Heiss (5)가 이와 관련한 훌륭한 예시를 제공합니다.\n\n## student-t 분포의 결과 확인\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n후방 분포를 살펴보겠습니다:\n\n```js\ndescribe_posterior(Student_Fit,\n                   centrality = \"mean\",\n                   dispersion = TRUE,\n                   ci_method = \"HDI\",\n                   test = \"rope\",\n                   )\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead_7.png\" /\u003e\n\n이 모델 하에 우리의 추정치가 다소 변한 것을 볼 수 있어요. 절편(WT at 1 day)에 대한 추정치가 7초 감소했습니다. 그 이유는 무엇일까요? 초기에 발견한 극단 값들이 데이터의 중심 경향 측정에 덜 영향을 미치기 때문입니다. 따라서 이는 1일차 전형적인 WT 동물의 무게를 더 정확하게 측정한 것입니다. 또한, 일자별 영향이 상당히 증가한 것을 관찰할 수 있습니다. 거의 10초가 초기 가우시안 추정치보다 더 많이 소요되어요. 중요한 점은, KO 유전자형의 영향이 더 두드러지게 나타나며, 초기 가우시안 모델의 0.52보다 학생 t 모델에서 5.5로 약 10배 증가했습니다. 이 데이터의 맥락을 고려해볼 때, 두 모델 간의 대조가 뚜렷하다고 생각해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래픽적으로 조건부 효과를 확인해 보겠습니다. conditional_effects를 사용해요:\n\n```js\nStudent_CondEffects \u003c- \n  conditional_effects(Student_Fit)\n\nStudent_CondEffects \u003c- plot(Student_CondEffects, \n       plot = FALSE)[[3]]\n\nStudent_CondEffects + \n  geom_point(data=data, aes(x = Day, y = Trial3, color = Genotype), inherit.aes=FALSE) +\n  Plot_theme +\n  theme(legend.position = \"bottom\", legend.direction = \"horizontal\")\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead_8.png\" /\u003e\n\n더 나은 추정을 얻을 수 있을까요? 이 특정 예시에서는 가능하다고 생각해요. 처음부터 데이터의 분산 차이를 쉽게 알아차렸으며, 특히 첫째 날과 둘째 날의 시각화를 비교할 때 더욱 명확해졌어요. 학생 t-분포를 사용하여 추정치를 향상시켰고, residual variance를 예측하는 이분산성 모델을 개발하여 추정치를 더욱 개선했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요거 하나 남았네요.\n\n# 학생-t 분포를 사용하여 시그마 예측\n\n우리는 brms의 thebf 함수를 사용하여 반응 변수로 시그마를 포함시킵니다. 이 경우, 우리는 동일한 예측 변수인 Day와 Genotype을 사용하여이 매개 변수를 모델링할 것입니다.\n\n```js\nStudent_Mdl2 \u003c- bf (Trial3 ~ Day * Genotype,\n                     sigma ~ Day * Genotype)\n\nStudent_Fit2 \u003c- brm(\n           formula = Student_Mdl2,\n           data = data, \n           family = student,\n           # 재현성을 위한 시드\n           seed = 8807,\n           control = list(adapt_delta = 0.99),\n           # 모델을 내 노트북에 저장하기 위함\n           file    = \"Models/20240222_OutliersStudent-t/Student_Fit2.rds\",\n           file_refit = \"never\")\n\n# 모델 비교를 위해 loo 추가\nStudent_Fit2 \u003c- \n  add_criterion(Student_Fit2, c(\"loo\", \"waic\", \"bayes_R2\"))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 모델 진단\n\n![Figure 11](/assets/img/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead_9.png)\n\n피겨 11은 좋아보이지만 0 미만의 예측값이 조금 불편합니다. 이 경우, 이것이 추정치와 그들의 불확실성에 강력한 편향을 미치지는 않는다고 판단합니다. 그러나 이는 실제 연구를 할 때 고려할 부분입니다.\n\n## 예측 시그마를 사용한 학생-t 분포 결과 확인\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 우리는 사후 분포를 살펴보겠습니다.\n\n![image](/assets/img/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead_10.png)\n\n우리는 다른 두 적합된 모델에 비해 더 많은 매개변수를 볼 수 있습니다. 이는 시그마에 대한 응답이 이제 모델의 주효과로 포함되었기 때문입니다. 이 체계 아래에서 우리는 y절편이 가우시안 모델의 것에 더 가깝고 유전형 (GenotypeKO)의 영향이 반으로 줄어든 것을 볼 수 있습니다.\n\n그러나 주목해야 할 점이 하나 있습니다. 첫 번째 스튜던트-t 모델에서 y절편에 대한 불확실성은 24.1에서 37.4 사이였습니다. 그러나 마지막 모델에서 불확실성은 24.3에서 46.1로 증가했습니다. 즉, 서로 다른 분산을 고려할 때 이 (그리고 다른) 매개변수에 대한 확신이 줄어든다는 것을 의미합니다. 예를 들어, 일(day)의 경우, 1.2에서 18.9였던 범위가 -5.6에서 18.1로 변합니다. 이 경우, 이제 두 번째 날이 로터로드에 보낸 시간이 증가된다는 것에 대해 덜 확신할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 예제에서는 데이터의 다른 분산을 고려하면 결과에 대한 매우 다른 아이디어를 제공합니다.\n\n마지막으로, 우리는 로그 스케일에 표시된 시그마가 날짜와 유전형에 따라 의미 있는 변화가 있음을 확인할 수 있습니다:\n\n```js\nStudent_CondEffects2 \u003c- \n  conditional_effects(Student_Fit2)\n\nStudent_CondEffects2 \u003c- plot(Student_CondEffects2, \n       plot = FALSE)[[3]]\n\nStudent_CondEffects2 + \n  geom_point(data=data, aes(x = Day, y = Trial3, color = Genotype), inherit.aes=FALSE) +\n  Plot_theme +\n  theme(legend.position = \"bottom\", legend.direction = \"horizontal\")\n\n\nStudent_CondEffects3 \u003c- \n  conditional_effects(Student_Fit2, dpar = \"sigma\")\n\nStudent_CondEffects3 \u003c- plot(Student_CondEffects3, \n       plot = FALSE)[[3]]\n\nStudent_CondEffects3 + \n  Plot_theme +\n  theme(legend.position = \"bottom\", legend.direction = \"horizontal\")\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead_11.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![링크 텍스트](/assets/img/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead_12.png)\n\n두 번째 그래프에서 본 것은 시그마입니다. 이는 일자 및 유전형 간이 이 매개변수의 분산을 효과적으로 설명합니다. 특히 WT 쥐의 경우, 첫 번째 날에는 훨씬 더 불확실성이 높게 나타나는 반면, 두 번째 날에는 이 매개변수가 유사합니다.\n\n우리는 이 글을 샘플 외 예측을 위해 세 가지 모델을 비교하며 결론을 낼 수 있습니다.\n\n# 모델 비교\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nWAIC 기준을 사용하여 모델 비교를 수행합니다. WAIC는 외부 샘플 예측 오차를 추정하기 위해 사용됩니다. WAIC는 관측 데이터의 로그 우도와 유효 파라미터 수를 모두 고려하여 모델 적합성과 복잡성 사이의 균형을 제공합니다. 다른 기준과는 달리 WAIC는 포인트 추정 대신 파라미터의 사후 분포를 내재적으로 반영하므로 베이지안 분석에 특히 적합합니다.\n\n데이터 세트와 베이지안 모델이 주어졌을 때, WAIC는 다음과 같이 계산됩니다:\n\nWAIC = -2×(LLPD - pWAIC)\n\n여기서, LLPD는 로그 점별 예측 밀도로, 각 관측 데이터 지점에 대한 로그 우도를 후방 샘플 전체에 걸쳐 평균한 값입니다. WAIC는 유효 파라미터 수로, 로그 우도의 평균과 후방 샘플들 간 평균 로그 우도의 차로 계산됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n퍼포먼스 패키지의 compare_performance 함수를 사용합니다. easystats 환경에 포함된 일부 함수입니다 (4, 7, 8).\n\n```js\nFit_Comp \u003c- \n  compare_performance(\n    Gaussian_Fit1, \n    Student_Fit, \n    Student_Fit2, \n    metrics = \"all\")\n\nFit_Comp\n```\n\n이 출력 결과에 따르면, 아웃 오브 샘플 예측에서 시그마를 예측하는 우리의 Student-t 모델이 가장 적게 패널티를 받았습니다 (WAIC = 497). 이 모델에서 시그마에 대한 추정치가 없는 이유는 이것이 응답 변수로 포함되었기 때문입니다. 이 표는 또한 Student-t 모델이 가우시안 모델보다 잔차 분산 (시그마)이 적다는 것을 보여줍니다. 이는 분산이 예측 변수로 더 잘 설명된다는 것을 의미합니다. 동일한 결과를 그래프로 시각화할 수 있습니다.\n\n```js\nFit_Comp_W \u003c- \nloo_compare(\n Gaussian_Fit1, \n    Student_Fit, \n    Student_Fit2,  \n  criterion = \"waic\")\n\n# WAIC 그래프 생성\nFit_Comp_WAIC \u003c- \n  Fit_Comp_W[, 7:8] %\u003e% \n  data.frame() %\u003e% \n  rownames_to_column(var = \"model_name\") %\u003e% \n  \nggplot(\n  aes(x    = model_name, \n      y    = waic, \n      ymin = waic - se_waic, \n      ymax = waic + se_waic)\n  ) +\n  geom_pointrange(shape = 21) +\n  scale_x_discrete(\n    breaks=c(\"Gaussian_Fit1\", \n             \"Student_Fit\", \n             \"Student_Fit2\"), \n            \n    labels=c(\"Gaussian_Fit1\", \n             \"Student_Fit\", \n             \"Student_Fit2\") \n             \n    ) +\n  coord_flip() +\n  labs(x = \"\", \n       y = \"WAIC (score)\",\n       title = \"\") +\n  Plot_theme\n\nFit_Comp_WAIC\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead_13.png)\n\n그림 14는 우리의 마지막 모델이 샘플 외 예측에 대해 덜 처벌받는 것을 보여줍니다.\n\nGitHub 사이트에서이 게시물의 최신 버전을 찾을 수 있습니다. 이 여정이 유익했는지, 이 연습에 추가할 건설적인 의견이 있는지 알려주세요.\n\n* 별도로 표기하지 않은 경우, 모든 이미지는 R 코드를 사용하여 작성된 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n1. M. 아산울라, B. M. G. 키브리아, M. 샤킬, Normal and student´s t distributions and their applications (Atlantis Press, 2014; [링크](http://dx.doi.org/10.2991/978-94-6239-061-4)).\n\n2. P.-C. Bürkner, Brms: An r package for bayesian multilevel models using stan. 80 (2017), doi:10.18637/jss.v080.i01.\n\n3. K. 양, J. 투, T. 첸, Homoscedasticity: an overlooked critical assumption for linear regression. General Psychiatry. 32, e100148 (2019).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4. D. Makowski, M. S. Ben-Shachar, D. Lüdecke, bayestestR: 베이지안 프레임워크 내에서 효과 및 불확실성, 존재 및 중요성을 설명하는 책 (2019년). 4, 1541 페이지.\n\n5. A. Heiss, 베이지안 베타 및 제로 인플레이티드 베타 회귀 모형으로 비율을 모델링하는 안내서 (2021년), (http://dx.doi.org/10.59350/7p1a4-0tw75에서 다운로드 가능).\n\n6. A. Gelman, J. Hwang, A. Vehtari, 베이지안 모델에 대한 예측 정보 기준 이해. 통계 및 컴퓨팅. 24, 997–1016 페이지 (2013년).\n\n7. D. Lüdecke, M. S. Ben-Shachar, I. Patil, P. Waggoner, D. Makowski, Performance: 통계 모형의 평가, 비교 및 테스트를 위한 R 패키지. 6, 3139 페이지 (2021년).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n8. D. Makowski, M. Ben-Shachar, D. Lüdecke, bayestestR: 베이지안 프레임워크 내에서 효과 및 불확실성, 존재 및 유의성을 설명하는 방법. Journal of Open Source Software. 4, 1541 (2019).","ogImage":{"url":"/assets/img/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead_0.png"},"coverImage":"/assets/img/2024-06-19-Donotover-thinkaboutoutliersuseastudent-tdistributioninstead_0.png","tag":["Tech"],"readingTime":16},{"title":"여러 브라우저 창 간에 실시간 WebSocket 데이터 공유하기","description":"","date":"2024-06-19 09:22","slug":"2024-06-19-Sharingreal-timeWebSocketdataacrossmultiplebrowserwindows","content":"\n\n# 멀티 스크린 앱이나 여러 창에서 병행하여 실행되는 앱을 만들 때, 모든 연결된 참가자가 동일한 데이터를 공유할 때 트래픽을 많이 절약할 수 있습니다.\n\n스포일러: 우리는 로컬 스토리지를 사용하지 않습니다.\n\n# 내용:\n\n- 소개\n- 데모 비디오\n- 저장소\n- 백엔드\n- RPC 정의 파일\n- 프론트엔드\n- 자식 앱용 셸\n- 브라우저 창 간 위젯 이동\n- Chrome 개발 도구로 앱 검사하기\n- 성능을 더 높일 수 있을까?\n- neo.mjs 프로젝트 업데이트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 1. 소개\n\n우리의 데모 앱은 일부러 미니멀리즘한 디자인을 가지고 있습니다. 3가지 위젯(테이블, 파이 차트 및 막대 차트)이 있는 대시보드가 포함되어 있어요. 이 위젯들은 별도의 브라우저 창으로 분리할 수 있습니다.\n\n우리는 데이터 SharedWorker를 사용하여 모든 연결된 창에 대한 데이터를 한 번에 로드하고 있으며, \"스트리밍 모드\"를 사용하여 초당 60번의 새 데이터를 가져오고 있어요.\n\n또한 창을 변경하더라도 동일한 구성 요소 인스턴스를 재사용할 수 있도록 해주는 앱 SharedWorker를 사용하고 있습니다. 이를 통해 상태를 동기화하는 데 도움이 될 거예요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-19-Sharingreal-timeWebSocketdataacrossmultiplebrowserwindows_0.png)\n\n## 2. 데모 비디오\n\n이상적으로 큰 화면에서 비디오를 시청하세요.\n\n## 3. 저장소\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 저장소에서 찾을 수 있습니다:\n\n데모 코드는 MIT 라이선스를 사용하므로 마음대로 사용하고 확장할 수 있습니다. README 파일에는 앱을 로컬에서 실행하기 위한 필요한 단계가 포함되어 있습니다.\n\n온라인에서 직접 테스트해보려면 배포된 버전을 보고 싶다면 미리 알려주세요!\n\n# 4. 백엔드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 백엔드 코드를 가능한 간단하게 유지하려고 합니다.\n\n우리는 익스프레스와 ws를 사용하여 포트 3001에서 소켓 연결을 받을 수 있는 최소한의 서버를 만들고 있습니다. 또한 neo.mjs 코어를 우리의 Node.js 백엔드 코드에 가져와 내부 유틸리티 함수 및 클래스 구성 시스템을 사용할 수 있도록 합니다.\n\n전체 코드는 여기에 있습니다: ColorService.mjs\n\n우리의 서비스는 read() 메서드만 노출하며, 이 메서드는 우리가 opts 객체 내의 속성으로 전달하는 3가지 매개변수를 기반으로 무작위 데이터를 생성할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- amountColors\n- amountColumns\n- amountRows\n\n# 5. RPC 정의 파일\n\n실제 앱에서는 각 서비스 내에서 CRUD를 사용하고, 복잡성에 따라 사용 가능한 서비스로 JSON 파일을 동적으로 생성할 것입니다. 특정 사용자 역할이나 권한에 따라 다를 수 있습니다.\n\n우리의 프론트엔드 코드는 이 파일을 fetch()하여 원하는 네임스페이스를 우리에게 노출시킬 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n물론 원하신다면 async / await을 사용할 수도 있어요.\n\n여기서 좋은 점은 frontend 코드베이스 내에서 JavaScript 프라미스로 Service 메소드를 직접 사용할 수 있다는 거예요.\n\n진짜 편리하죠 :)\n\n\u003cimg src=\"/assets/img/2024-06-19-Sharingreal-timeWebSocketdataacrossmultiplebrowserwindows_1.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 애플리케이션 SharedWorker(프론트엔드 코드) 내에서 Colors.backend.ColorService.read()를 사용한다면, neo.mjs는 데이터 SharedWorker로 메시지를 보낼 것입니다. 관심사의 분리입니다. 데이터 워커는 소켓 연결이 이미 존재하는지 확인하고 필요한 경우 (재)연결합니다. 그런 다음 소켓을 통해 메시지를 보내고 백엔드 코드 내에서 ColorService.read()를 실행할 것입니다. 특정 소켓 연결 메시지의 응답은 App-Worker로 다시 전송됩니다. 초기 호출자에게 매핑된 후 우리의 Promise를 해결할 것입니다.\n\n개발자로서 여러분은 여기에서 일어나는 마법에 대해 걱정할 필요가 없습니다.\n\n# 6. 프론트엔드\n\nnpx neo-app을 사용하여 앱 셸을 자동으로 생성했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNeo 앱의 인덱스 파일은 MicroLoader 모듈만 포함하게 됩니다. 이 모듈은 우리 앱 폴더 안의 neo-config.json 파일을 가져와 neo 메인 스레드를 시작할 것입니다. 이 스레드는 워커 설정을 생성하고 완료되면 우리 앱 폴더 안의 app.mjs 파일을 동적으로 로드할 것입니다.\n\n우리 앱 SharedWorker에 로드되는 Viewport 파일:\n\nViewport에는 3개의 위젯과 HeaderToolbar가 직접 items 배열에 들어갑니다. 또한 view controller와 view model을 가져옵니다.\n\nNeo에서의 view model은 상태 제공자(다른 라이브러리/프레임워크에서는 \"store\"로 불림)로서, 자식 뷰가 데이터 속성에 바인딩할 수 있도록 허용합니다. 부모 체인 내에서 여러 데이터 속성에 바인딩할 수도 있습니다. 이 데모 앱에 중요한 부분은: 우리의 상태 트리가 모든 브라우저 창에서 작동한다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 7. 자식 앱을 위한 쉘\n\n주 앱과 마찬가지로 MicroLoader가 들어 있는 index 파일과 Viewport을 가져오는 app.mjs 파일이 있는 인덱스 파일이 있습니다:\n\nViewport은 완전히 비어 있어서, 주 앱을 로드하지 않고 앱을 열면 다음과 같이 나타날 것입니다:\n\n![이미지](/assets/img/2024-06-19-Sharingreal-timeWebSocketdataacrossmultiplebrowserwindows_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 8. 브라우저 창 간 위젯 이동\n\n저희 주 앱 ViewportController 내에서는 앱 SharedWorker에 연결된 각 브라우저 창에 대해 발생하는 connect 이벤트에 구독하고 있습니다.\n\n연결된 창이 주 앱 창이 아닌 경우 다음을 호출합니다:\n\n```javascript\nwidgetParent.remove(widget, false);\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 메인 창 안의 뷰포트에서 원하는 위젯(테이블, 파이 차트 또는 바 차트)를 제거하고 있어요. 두 번째 매개변수 false는 컴포넌트 인스턴스를 파괴하지 않도록 하는 플래그입니다.\n\nmainView.add(widget);\n\n그런 다음 위젯을 메인 뷰 → 다른 브라우저 창의 뷰포트에 추가하고 있어요. 동일한 컴포넌트 인스턴스를 재사용하기 때문에 최신 상태를 손쉽게 얻을 수 있어요.\n\n정말로 이렇게 간단해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n컴포넌트 인스턴스 재사용에 대한 주제는 메모리 누수를 줄이고 런타임 성능을 향상시키는 매우 강력한 기술이기 때문에 개별 블로그 게시물이 필요합니다.\n\n# 9. Chrome 개발 도구로 앱 검사하기\n\nNeo에서 단일 창 앱을 만들 때 프레임워크는 Dedicated Workers를 사용하며, 이를 주 창 콘솔에서 직접 확인할 수 있습니다.\n\n\"neo-config.json\" 파일에 \"useSharedWorkers”: true를 추가하면 프레임워크가 SharedWorkers로 전환되며, 브라우저 창 콘솔에서는 더 이상 확인할 수 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNeo.mjs는 Worker를 위한 추상화 계층을 제공하여 개발자의 API가 정확히 동일하게 유지됩니다. 우리는 언제든지 1줄의 구성을 변경할 수 있습니다. SharedWorker를 검사하려면 다음을 열어야 합니다:\n\nchrome://inspect/#workers\n\n![이미지](/assets/img/2024-06-19-Sharingreal-timeWebSocketdataacrossmultiplebrowserwindows_3.png)\n\n앱의 SharedWorker를 검사하고 콘솔에 다음 코드를 입력하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 10. 성능을 더 향상시킬 수 있을까요?\n\n데모 앱은 매우 빠르게 느껴지지만 아직은 성능을 최적화하지 않은 상태입니다. 그러므로 이 질문에 대한 답변은 명확히 \"네!\"입니다. 여전히 우리는 AmCharts 주요 스레드 애드온을 버전 5로 업그레이드(재작성)해야 합니다. 이에 우선 순위를 부여해야 할 필요가 있다면 미리 알려주세요.\n\n또한 여러 가지 앱 설정을 시험해볼 수도 있습니다. 예를 들어, 데이터 워커 대신 앱 워커 내에서 소켓 연결을 직접 생성하여 내부 포스트 메시지의 양을 줄일 수 있습니다.\n\n# 11. neo.mjs 프로젝트 업데이트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1.5년 만에 올린 첫 블로그 포스트였어요. 마음에 드셨으면 좋겣어요! 궁금한 점이 있으시면 언제든지 물어보세요.\n\n그동안 프레임워크는 많이 발전했어요. 가장 중요한 업데이트는 새 제품 웹사이트를 준비 중이라는 점이에요. 이 사이트에는 오랫동안 기다리셨던 자기 학습 섹션이 첫 번째 버전으로 포함될 예정이에요. 이를 완성하는 대략적인 시간은 한 달 정도 남았어요.\n\n우리는 아직 매주 목요일 오후 5시 30분(CEST 기준)에 무료 워크숍을 진행하고 있어요. 참여하고 싶다면 슬랙 채널에서 연락해주세요.\n\n회사에서 더 많은 도움이 필요하다면, neo.mjs 코어 팀은 전문 강사가 진행하는 인도주도 훈련(40시간, 6~12명 참가자)뿐만 아니라 전문 서비스(예: 차세대 앱 개발 지원)도 제공하고 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n피드백을 매우 환영합니다!\n\n진심으로, 즐거운 코딩하세요,\n토비아스","ogImage":{"url":"/assets/img/2024-06-19-Sharingreal-timeWebSocketdataacrossmultiplebrowserwindows_0.png"},"coverImage":"/assets/img/2024-06-19-Sharingreal-timeWebSocketdataacrossmultiplebrowserwindows_0.png","tag":["Tech"],"readingTime":6}],"page":"81","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"81"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>