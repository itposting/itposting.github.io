<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/79" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/79" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="제목 제로부터 dbt까지 스포티파이의 백만 개 플레이리스트 데이터를 분석하고 데이터 모델 구축하는 방법" href="/post/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="제목 제로부터 dbt까지 스포티파이의 백만 개 플레이리스트 데이터를 분석하고 데이터 모델 구축하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="제목 제로부터 dbt까지 스포티파이의 백만 개 플레이리스트 데이터를 분석하고 데이터 모델 구축하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">제목 제로부터 dbt까지 스포티파이의 백만 개 플레이리스트 데이터를 분석하고 데이터 모델 구축하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터, AI 서밋에서 얻은 교훈 파트 II" href="/post/2024-06-19-DataAISummitTakeawaysPartII"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터, AI 서밋에서 얻은 교훈 파트 II" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-DataAISummitTakeawaysPartII_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터, AI 서밋에서 얻은 교훈 파트 II" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터, AI 서밋에서 얻은 교훈 파트 II</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 " href="/post/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 </strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="DBT  Airflow  " href="/post/2024-06-19-dbtAirflow"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="DBT  Airflow  " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-dbtAirflow_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="DBT  Airflow  " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">DBT  Airflow  </strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">16<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 엔지니어를 위한 자료 구조와 알고리즘 면접 질문" href="/post/2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 엔지니어를 위한 자료 구조와 알고리즘 면접 질문" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 엔지니어를 위한 자료 구조와 알고리즘 면접 질문" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 엔지니어를 위한 자료 구조와 알고리즘 면접 질문</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="관계형 데이터베이스에서 데이터 레이크하우스로 데이터 관리의 간단한 역사" href="/post/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="관계형 데이터베이스에서 데이터 레이크하우스로 데이터 관리의 간단한 역사" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="관계형 데이터베이스에서 데이터 레이크하우스로 데이터 관리의 간단한 역사" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">관계형 데이터베이스에서 데이터 레이크하우스로 데이터 관리의 간단한 역사</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="높은 영향력을 가진 데이터 거버넌스 팀" href="/post/2024-06-19-High-impactdatagovernanceteams"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="높은 영향력을 가진 데이터 거버넌스 팀" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-High-impactdatagovernanceteams_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="높은 영향력을 가진 데이터 거버넌스 팀" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">높은 영향력을 가진 데이터 거버넌스 팀</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="유튜브 트렌드 분석 파이프라인 Airflow, Spark, S3 및 Docker를 이용한 ETL" href="/post/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="유튜브 트렌드 분석 파이프라인 Airflow, Spark, S3 및 Docker를 이용한 ETL" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="유튜브 트렌드 분석 파이프라인 Airflow, Spark, S3 및 Docker를 이용한 ETL" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">유튜브 트렌드 분석 파이프라인 Airflow, Spark, S3 및 Docker를 이용한 ETL</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">25<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="1771 Technologies가 선보이는 Graphite Grid 소개" href="/post/2024-06-19-IntroducingGraphiteGridby1771Technologies"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="1771 Technologies가 선보이는 Graphite Grid 소개" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-IntroducingGraphiteGridby1771Technologies_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="1771 Technologies가 선보이는 Graphite Grid 소개" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">1771 Technologies가 선보이는 Graphite Grid 소개</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="분석력을 향상시키는 5가지 유용한 시각화 방법" href="/post/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="분석력을 향상시키는 5가지 유용한 시각화 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="분석력을 향상시키는 5가지 유용한 시각화 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">분석력을 향상시키는 5가지 유용한 시각화 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/61">61</a><a class="link" href="/posts/62">62</a><a class="link" href="/posts/63">63</a><a class="link" href="/posts/64">64</a><a class="link" href="/posts/65">65</a><a class="link" href="/posts/66">66</a><a class="link" href="/posts/67">67</a><a class="link" href="/posts/68">68</a><a class="link" href="/posts/69">69</a><a class="link" href="/posts/70">70</a><a class="link" href="/posts/71">71</a><a class="link" href="/posts/72">72</a><a class="link" href="/posts/73">73</a><a class="link" href="/posts/74">74</a><a class="link" href="/posts/75">75</a><a class="link" href="/posts/76">76</a><a class="link" href="/posts/77">77</a><a class="link" href="/posts/78">78</a><a class="link posts_-active__YVJEi" href="/posts/79">79</a><a class="link" href="/posts/80">80</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"제목 제로부터 dbt까지 스포티파이의 백만 개 플레이리스트 데이터를 분석하고 데이터 모델 구축하는 방법","description":"","date":"2024-06-19 09:52","slug":"2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_0.png\" /\u003e\n\n앞으로 몇 주 동안, dbt (data build tool)를 사용하여 Spotify의 백만 개 플레이리스트 데이터셋을 엔드 투 엔드 분석 프로젝트로 변환하는 방법을 안내할 것입니다. 중소형 대형 실제 세계 원시 데이터를 상호 작용적인 데이터 모델로 변환하는 방법을 배우게 될 거에요. (어떤걸 🤣 기반으로 한 George Orwell의 하층층상 중간층에요)\n\n## 배울 내용\n\n- 30GB의 원시 JSON 데이터를 효율적이고 확장 가능하게 5GB Parquet 파일로 변환하기.\n- Parquet 파일을 심층적인 탐색과 분석을 위한 여러 dbt 모델로 변환하기.\n- 데이터 변환 프로세스에서 dbt를 사용하는 것이 왜 최선의 실천법인지 이해하기.\n- 데이터 무결성과 정확성을 보장하기 위해 각 dbt 모델 변경을 검증하는 방법에 대해 배우기 (스포일러: 오픈 소스 dbt 모델 코드 리뷰 도구인 Recce를 사용하세요).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 데이터로부터 중요한 질문에 답변해주세요\n\n매주 Recce LinkedIn 페이지에 스포티파이 데이터셋에 관한 두 가지 질문을 게시할 것입니다. 예를 들어,\n\n- 적어도 3곡의 테일러 스위프트 노래를 포함하는 재생 목록은 몇 개인가요?\n- 제이 체오의 인기 있는 상위 10곡은 무엇인가요?\n- BLACKPINK 💗과 Post Malone이 모두 포함된 재생 목록은 몇 개인가요?\n\n그 후에 투표를 가장 많이 받은 질문을 오픈 소스 저장소에 구현할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 누구를 위한 것인가요?\n\n비즈니스 BI 또는 ML을 위해 데이터 변환에 dbt를 사용하는 방법에 관심이 있는 모든 분들을 환영합니다. 뿐만 아니라, 데이터 또는 분석 엔지니어로 계속된 작업에 유용한 몇 가지 dbt 모베스트 사항을 함께 공유할 예정입니다.\n\n# 백만 플레이리스트 데이터 준비하기\n\n시작할 준비가 되셨나요? 멋지네요. 이 프로젝트에서는 스포티파이 백만 플레이리스트 데이터셋을 사용할 예정입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_1.png)\n\n## 데이터셋 다운로드\n\nSpotify 정책에 따라 등록하고 여기서 원시 데이터를 다운로드해야 합니다. 우리는 spotify_million_playlist_dataset.zip 파일을 사용할 거에요 (크기는 5.4 GB 👀).\n\n이 zip 파일은 31GB로 풀리니 충분한 공간이 있는지 확인해주세요! (나중에 Parquet으로 변환하면 용량이 줄어듭니다)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](https://miro.medium.com/v2/resize:fit:960/1*HbIZkLZc-9ClzgToTNLvsg.gif)\n\n데이터셋을 다운로드하고 압축 해제한 후, data 폴더에는 천 개의 분할된 JSON 파일로 구성되어 있음을 발견할 것입니다. 이러한 파일들은 다음과 같은 패턴으로 명명되어 있습니다:\n\n- mpd.slice.0–999.json\n- mpd.slice.1000–1999.json\n- …\n- mpd.slice.999000–999999.json\n\n분할된 JSON 파일 중 하나에서 플레이리스트 항목의 전형적인 예시는 다음과 같습니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![링크](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_2.png)\n\n데이터셋의 각 JSON 파일은 1,000개의 재생목록을 나타내며, 총 1백만 개의 재생목록이 포함되어 있습니다. 파일 접두사 \"mpd\"는 \"Million Playlist Dataset\"의 약자입니다.\n\nSpotify 팀은 이러한 JSON 파일의 무결성을 확인하고 MD5 체크섬을 사용하여 기본 통계를 계산하는 데 도움이 되도록 ./src 폴더에 스크립트를 제공했습니다. 아래 명령어로 기본 통계를 계산할 수 있습니다:\n\n```js\n$ python src/stats.py data\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSpotify의 README 문서에 따르면, 이 프로그램의 결과물은 'stats.txt' 내용과 일치해야 합니다. stats.py의 실행 시간은 노트북의 성능에 따라 다를 수 있으며, 30분을 초과할 수도 있습니다.\n\n# 초기 인사이트\n\n우리는 우선적으로 몇 가지 탐구를 시작해 초기 인사이트를 얻고 데이터셋을 더 잘 이해할 것입니다. 이 작업은 raw json을 사용하여 이루어질 것이지만, 더 고급 데이터 상호작용을 위해서는 데이터를 더 효율적인 형식으로 변환해야 할 것입니다. 이에 Parquet을 사용할 것이며 (자세한 내용은 아래에 소개되어 있음), 이는 dbt와 함께 사용하기에 이상적이며 raw 데이터를 변환하는 데 유용합니다.\n\nSpotify의 1000개 raw 데이터 파일로 되돌아가보죠. 모든 데이터 분석 처리 워크플로우에서 겪었던 노고와 눈물이 어떤 것이었는지 보여드리겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# DuckDB 및 jq\n\nDuckDB와 jq는 모두 JSON 데이터와 상호 작용하기 위한 훌륭한 도구입니다. 이 멋진 도구들을 설치하려면 선호하는 패키지 관리자를 사용하십시오. 예를 들어:\n\n```sh\n$ brew install duckdb\n$ brew install jq\n```\n\n## JSON 구조 이해하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 JSON 데이터를 빠르게 확인하기 위해 jq를 사용할 수 있습니다. 이후 보다 심층적인 분석을 위해 DuckDB를 활용할 수 있습니다. 1000개 파일 중 하나를 살펴보겠습니다:\n\n```js\n$ cd spotify_million_playlist_dataset/data\n$ jq 'keys' mpd.slice.0-999.json\n```\n\nMarkdown 양식으로 표를 변경했습니다:\n\n![표](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_3.png)\n\nJSON의 모든 조각은 두 개의 키만 포함하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 정보 — 이것은 단순히 JSON 파일의 메타데이터입니다.\n- 재생 목록 — 실제로 관심 있는 데이터\n\n아마도 \"재생 목록\"이 배열이라는 것을 짐작하실 수 있습니다. 따라서 재생 목록에서 첫 번째 요소를 살펴보겠습니다.\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_4.png)\n\n부분 재생 목록 데이터(첫 번째 트랙만 표시)는 다음과 같이 보일 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_5.png)\n\n# 덕DB로 분석하기\n\nJSON 데이터의 구조를 파악한 후에는 DuckDB를 사용하여 데이터를 분석할 수 있습니다. DuckDB는 SQL 데이터 유형을 JSON 파일 내에서 자동으로 감지하는 기능을 제공하므로 분석에 SQL 구문을 손쉽게 적용할 수 있습니다.\n\n## DuckDB 대화형 셸 열기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n터미널에 duckdb를 입력하면 PostgreSQL의 psql 및 SQLite 셸과 유사한 대화형 셸에 들어갈 수 있어요.\n\n```js\n$ duckdb\n```\n\n## DuckDB의 maximum_object_size 조정\n\n다음 명령을 실행하면 아래의 오류가 표시됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nSELECT * FROM read_json_auto('./mpd.slice.0-999.json');\n\n-- \"maximum_object_size\" of 16777216 bytes exceeded \n-- while reading file \"./mpd.slice.0-999.json\" (\u003e33554428 bytes).\n-- \"maximum_object_size\"을 늘려주세요.\n```\n\n이 오류는 밀리언 플레이리스트 데이터셋의 JSON 슬라이스가 DuckDB의 기본 maximum_object_size보다 크기 때문에 발생했습니다. 따라서 이를 조정하여 40MB로 설정해야 합니다 🫰:\n\n```js\nSELECT * \nFROM read_json_auto('./mpd.slice.0-999.json', maximum_object_size = 40000000); \n\n┌──────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n│         info         │                                                                            playlists                                                                             │\n│ struct(generated_o…  │ struct(\"name\" varchar, collaborative varchar, pid bigint, modified_at bjigint, num_tracks bigint, num_albums bigint, num_followers bigint, tracks struct(pos bi…  │\n├──────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n│ {'generated_on': 2…  │ [{'name': Throwbacks, 'collaborative': false, 'pid': 0, 'modified_at': 1493424000, 'num_tracks': 52, 'num_albums': 47, 'num_followers': 1, 'tracks': [{'pos': …  │\n└──────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n```\n\n## JSON 해제하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 컬럼 안에 중첩 구조인 재생 목록에만 관심이 있다는 것을 알고 있습니다. 그러므로 플레이리스트 열을 정규화하기 위해 UNNEST를 사용할 수 있습니다:\n\n```js\nSELECT UNNEST(playlists) \nFROM read_json_auto('./mpd.slice.0-999.json', maximum_object_size = 40000000) \nLIMIT 5;\n\n┌─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n│                                                                                    unnest(playlists)                                                                                    │\n│ struct(\"name\" varchar, collaborative varchar, pid bigint, modified_at bigint, num_tracks bigint, num_albums bigint, num_followers bigint, tracks struct(pos bigint, artist_name varch…  │\n├─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n│ {'name': Throwbacks, 'collaborative': false, 'pid': 0, 'modified_at': 1493424000, 'num_tracks': 52, 'num_albums': 47, 'num_followers': 1, 'tracks': [{'pos': 0, 'artist_name': Missy …  │\n│ {'name': Awesome Playlist, 'collaborative': false, 'pid': 1, 'modified_at': 1506556800, 'num_tracks': 39, 'num_albums': 23, 'num_followers': 1, 'tracks': [{'pos': 0, 'artist_name': …  │\n│ {'name': korean , 'collaborative': false, 'pid': 2, 'modified_at': 1505692800, 'num_tracks': 64, 'num_albums': 51, 'num_followers': 1, 'tracks': [{'pos': 0, 'artist_name': Hoody, 't…  │\n│ {'name': mat, 'collaborative': false, 'pid': 3, 'modified_at': 1501027200, 'num_tracks': 126, 'num_albums': 107, 'num_followers': 1, 'tracks': [{'pos': 0, 'artist_name': Camille Sai…  │\n│ {'name': 90s, 'collaborative': false, 'pid': 4, 'modified_at': 1401667200, 'num_tracks': 17, 'num_albums': 16, 'num_followers': 2, 'tracks': [{'pos': 0, 'artist_name': The Smashing …  │\n└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n```\n\nDuckDB는 UNNEST 함수에서 `recursive := true`와 같이 매우 편리한 옵션을 제공합니다. 이 옵션은 열을 재귀적으로 정규화합니다:\n\n```js\nSELECT UNNEST(playlists, recursive := true) \nFROM read_json_auto('./mpd.slice.0-999.json', maximum_object_size = 40000000) \nLIMIT 5;\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이렇게 하면 깊게 중첩된 JSON 데이터를 다루기가 매우 편리합니다.\n\n## JSON을 단일 표로 결합\n\n현재, 우리는 하나의 JSON 파일만 처리하고 있습니다. 만약 1,000개의 나누어진 JSON 파일을 모두 한 표로 합치고 싶다면 어떻게 해야 할까요?\n\nDuckDB는 여러 JSON 파일을 한 번에 읽을 수 있게 해주는 glob 구문을 제공합니다. `./mpd.slice.0-999.json`을 `./mpd.slice*.json`로 수정하면 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n테이블 태그를 Markdown 형식으로 변경하세요.\n\nCREATE TABLE playlists AS \nSELECT UNNEST(playlists , recursive:= true) \nFROM read_json_auto('./mpd.slice*.json', maximum_object_size = 40000000);\n```\n\n내 노트북(M3 MacBook)에서 playlists DuckDB 테이블을 만드는 데 30초가 걸렸어요. 이제 데이터를 Parquet으로 변환할 준비가 되었어요.\n\n## Parquet으로 변환\n\n변환 과정 중간에 메모리 부족 오류를 방지하기 위해 일부 임시 파일이 필요할 수 있습니다. DuckDB 쉘에서 계속하여, 먼저 다음을 실행하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nSET temp_directory='./tmp';\n```\n\n이제 DuckDB 테이블에서 플레이리스트를 Parquet 파일로 내보낼 준비가 되었습니다. copy 명령을 사용하여 .parquet 확장자를 갖는 파일을 지정하면 DuckDB가 자동으로 Parquet 파일로 내보내기를 원한다는 것을 알게 됩니다.\n\n```js\nCOPY playlist TO 'playlists.parquet';\n```\n\n쉽죠?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Parquet 대 JSON\n\n그래서, 왜 Parquet을 사용해야 할까요?\n\nParquet은 분석을 위한 우수한 파일 포맷으로, 컬럼 저장 방식을 통해 JSON에 비해 주목할만한 장점을 제공합니다. 이 설계는 데이터 압축 및 인코딩을 향상시켜 저장 공간을 줄이고 데이터 분석 워크플로우의 데이터 액세스 속도를 높이는데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n복잡한 중첩 데이터 구조도 지원하며 기존 데이터를 수정하지 않고 새 열을 추가할 수 있는 유연한 스키마 진화를 제공합니다. 이는 스키마 변경이 자주 발생하는 시나리오에 이상적인 형식이 됩니다.\n\n또한, 주요 데이터 웨어하우스와의 호환성을 통해 Parquet은 특히 dbt 사용자에게 매우 중요하며 데이터 통합 및 분석 워크플로우를 간소화합니다. 즐겨 사용하는 데이터 웨어하우스에서 쉽게 Parquet 파일을 가져오고 내보낼 수 있습니다.\n\nDuckDB와 jq를 사용하면 기가바이트의 JSON 데이터를 노트북에서 간단하게 분석할 수 있습니다.\n\n\"ON - YOUR - LAPTOP\"을 반복해보세요 💻\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 요약\n\n원본 데이터 세트는 1,000개의 JSON 파일로 이루어져 있으며 총 31GB입니다.\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_7.png)\n\n세 가지 간단한 DuckDB 쿼리를 실행한 후 1분의 처리 시간을 거쳐 단일 5.7GB Parquet 파일을 얻게 되어, 500% 개선이 이뤄졌습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_8.png)\n\n지금은 몇 초 안에 노트북으로 \"플레이리스트에 테일러 스위프트 노래가 몇 개 있는지?\"와 같은 질문에 빠르게 답변할 수 있습니다. 마음이 홀립니다.\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_9.png)\n\n이 데이터 분석 프로젝트의 첫 번째 부분은 여기까지입니다. 곧 두 번째 부분도 뵙겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 다음에는...\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_10.png)\n\n데이터셋을 Parquet 파일로 성공적으로 전환한 후, 다음 목표는 dbt의 파워를 활용하여 One Million Playlists 데이터셋에서 더 깊고 더 매력적인 분석적 인사이트를 발굴하는 것입니다.\n\n## 데이터에 소프트웨어 엔지니어링 최상의 실천 방법 적용\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n덕DB 셸은 데이터 집합을 대화식으로 분석할 수 있는 기능을 제공하지만, 우리의 SQL 변환에 보다 구조화되고 협업적이며 버전 관리된 접근이 필요함을 알 수 있습니다.\n\n여기서 dbt가 빛을 발합니다 🤩. dbt를 사용하면 데이터 변환을 코드로 처리할 수 있어 소프트웨어 엔지니어링 관행인 버전 관리, 코드 리뷰(Recce 빛나요 💖), 그리고 자동화된 테스트를 데이터 워크플로에 적용할 수 있습니다.\n\n## 함께 작업하기\n\n여러 SQL 쿼리를 논리적인 dbt 모델로 구성함으로써, 데이터 변환의 명확성과 유지 관리성을 향상시킬 뿐만 아니라, 데이터 팀이 서로 협력하여 서로의 작업을 점진적으로 빌드할 수 있습니다. 이 협업적인 접근은 데이터 모델이 견고하고 정확하며 최신 비즈니스 로직과 분석적 통찰을 반영하도록 보장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 신뢰할 수 있는 환경\n\n또한, dbt의 문서 기능을 사용하면 데이터 모델의 포괄적인 문서를 자동으로 생성하여 새 팀원들이 데이터 환경을 이해하기 쉽고 이해관계자들이 데이터 주도적 의사결정을 신뢰할 수 있게 합니다.\n\n## 데이터 주도적 개발\n\n요약하면, dbt는 SQL 변환을 효율적으로 관리할 수 있는 필수 도구와 함께 제공하여 협업적이고 반복적인 데이터 문화를 육성하는 데 도움이 되어, 오늘날의 데이터 주도적 세상에서 경쟁력을 유지하는 데 필수적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Part 2에서 만나요\n\n최신 소식을 받아보고 더 흥미로운 소식을 확인하려면 LinkedIn을 팔로우하세요! 🤩\n\n업데이트: Part 2가 이제 사용 가능합니다. 저의 샘플 프로젝트를 따라가면서 dbt가 데이터 프로젝트 모델링에 적합한 이유를 살펴보겠습니다. 아, 그리고 중요한 Spotify 플레이리스트 질문에 대해 답변도 해드립니다!\n\n# 파이프라인에서 더 많은 기사","ogImage":{"url":"/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_0.png"},"coverImage":"/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_0.png","tag":["Tech"],"readingTime":12},{"title":"데이터, AI 서밋에서 얻은 교훈 파트 II","description":"","date":"2024-06-19 09:50","slug":"2024-06-19-DataAISummitTakeawaysPartII","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-DataAISummitTakeawaysPartII_0.png\" /\u003e\n\n# 소개\n\n지난 주 데이터 및 AI 써밋에서 전체 세션을 빠르게 업로드해 준 Databricks에 큰 감사를 전합니다. 그들 모두를 살펴보는 것은 불가능하지만, 내가 확인한 몇 가지 가운데 내가 좋아하는 이야기는 다음과 같습니다.\n\n# Spark 업그레이드/마이그레이션\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대량 업그레이드와 마이그레이션은 플랫폼 팀에게 언제나 쉬운 작업이 아닙니다. 넷플릭스는 이 세션에서 모든 Spark 작업에 대한 인벤토리를 작성하고 마이그레이션 제어 및 자동화 플레인을 호스팅하며, 심지어 마이그레이션 프로세스에 유효성 검사 및 관측 기능을 통합하는 방법을 설명했습니다. 넷플릭스가 운영하는 규모를 고려하면, 그것은 매우 인상적입니다.\n\n우리는 이제까지 기관 전체적인 Spark 업그레이드를 하지 않았습니다(하지만 Databricks 런타임이 관련될 때 특히 그렇게 할 생각이 좋을 수도 있습니다). 하지만 이것은 어떻게 그겢을 달성할 수 있는 유용한 프레임워크를 제공합니다. 개별 팀이 업그레이드를 도울 수 있는 프로세스를 구축하고 적절한 관측 기능을 갖추어 각 팀이 너무 뒤처져 있지 않도록 확인하는 것이 의미가 있을지도 모릅니다. 우리 플랫폼 팀이 처리해야 할 다른 책임을 고려하면, 그렇게 하는 것이 관리하기 쉬워 보입니다.\n\n# 가드레일\n\n이전 게시물에서, 저는 Databricks 사용을 관리하는 데 중요한 컴퓨팅 정책의 중요성에 대해 썼습니다. 프로비던스는 Unity 마이그레이션 동안 훌륭한 일련의 가드레일을 소개했습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 클러스터 유형 제한\n- 대화식 클러스터 예약 불가\n- 운영 중 대화식 클러스터 미사용\n- 노드 수와 인스턴스 유형에 대한 제어 설정\n\n대화식 클러스터 예약을 허용하지 않는 것은 중요한 사항입니다. 모든 워크플로우는 이상적인 클러스터 구성을 위해 작업 클러스터를 사용해야 하며 대화식 클러스터처럼 일반적으로 보이는 유휴 기간을 피해야 합니다. (또한 작업 컴퓨팅이 대화식 컴퓨팅보다 저렴하다는 점도 말이죠).\n\n운영 중 대화식 클러스터를 사용하지 않는 것은 이전에 생각해보지 못한 부분이었는데, 합리적으로 보입니다. 하위 환경에서 생산 데이터를 읽기 위한 필요한 액세스가 있고 비상 시나리오를 위한 경우를 제외하고 별도의 운영 대화식 클러스터가 정말 필요할까요? 아마도 그렇지 않기 때문에 처음부터 허용하지 않는 것이 좋습니다.\n\n# 차원 모델링\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래, 당신은 아마 데이터 및 AI 서밋에서 차원 모델링 세션을 듣기 위해 참석하지 않았을 것 같지만, 컬럼형 데이터베이스를 적절히 설계할 때 장점이 있어요. 사실 테이블, 차원 테이블 및 다양한 유형의 천천히 변화하는 차원들 사이의 차이를 알아두면 정말 유용할 거예요. (여기서 네 가지만 있다고 생각했는데...)\n\n이 대화는 성능 대 개인 요구사항에 대한 더 나은 지원의 상충관계인 큰 하나의 테이블(OBT) 주변의 전형적인 사례를 언급하기에 잘 했어요. 기존 데이터 업데이트, 거버넌스 및 전반적인 중복성에 관해서는 OBT 접근 방식에는 단점이 있지만, 가끔 고려할 가치가 있어요.\n\n# 결론\n\n아마도 앞으로 몇 일 동안 더 많은 세션을 확인하게 될 것 같아요, 여기서 매우 훌륭한 시작이었어요. 다시 한 번 Databricks에 이러한 멋진 행사를 주최해준 것에 감사드려요.","ogImage":{"url":"/assets/img/2024-06-19-DataAISummitTakeawaysPartII_0.png"},"coverImage":"/assets/img/2024-06-19-DataAISummitTakeawaysPartII_0.png","tag":["Tech"],"readingTime":2},{"title":"특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 ","description":"","date":"2024-06-19 09:48","slug":"2024-06-19-DataEngineeringpipelineleveragingAirflowKafka","content":"\n\n# 소개\n\n이 문서에서는 Apache Airflow 및 Kafka(오픈 소스)를 활용하여 실시간 날씨 업데이트를 읽고 Twitter의 이벤트 스트림을 분석하여 대중의 반응을 이해하는 데이터 엔지니어링 파이프라인 아키텍처를 솔루션 디자인하는 방법에 대한 기본적인 개요를 제공합니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png)\n\n# ETL 기본 원칙\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nETL은 데이터 웨어하우스 또는 데이터 마트와 같은 분석 환경을 위해 데이터 획득 및 준비를 자동화하는 데이터 파이프라인 엔지니어링 방법론인 추출, 변환, 로드를 의미합니다. 이는 다양한 소스에서 데이터를 수집하고 표준 형식으로 편집한 다음 시각화, 탐색 및 모델링을 위한 새로운 환경으로 로드하는 것을 포함하며, 자동화 및 의사 결정을 지원합니다.\n\n# ELT 기본\n\nELT 프로세스는 추출, 로드 및 변환을 나타내며, 단곅하계의 단계 순서로 인한 독특한 차이로 인해 ETL과 다릅니다. ELT에서는 데이터가 데이터 레이크와 같은 목적지 환경으로 원본 형식 그대로 직접 로드됩니다. 이를 통해 목적지 플랫폼 내에서 필요에 따라 변환을 수행하고 동적으로 사용자 주도적 변경을 가능하게 합니다.\n\n# ETL과 ELT 비교\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nETL과 ELT의 차이점: ETL 파이프라인에서는 변환 작업이 목적지에 도달하기 전에 데이터 파이프라인 내에서 발생하는 반면, ELT는 변환 작업을 분리하여 목적지 환경에서 필요한 대로 수행할 수 있습니다. ETL은 엄격하고 목적이 명확하지만, ELT는 유연하며 빅 데이터 처리에 대한 셀프 서비스 분석을 제공합니다.\n\n# 데이터 수집 기술\n\n다양한 데이터 수집 기술에는 다음이 포함됩니다:\n\n- 완전 수집 대 부분 수집\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 전체 로딩: 데이터베이스에 초기 히스토리를 로드합니다. 추적 데이터는 새 창고에서 시작됩니다.\n- 점진적 로딩: 새 데이터를 삽입하거나 이미 로드된 데이터를 업데이트합니다. 거래 히스토리를 누적하는 데 사용됩니다. 데이터 양과 속도에 따라 일괄 또는 스트림 로드될 수 있습니다.\n\n정기 로딩 vs. 요청 로딩:\n\n- 정기 로딩: 매일 거래를 데이터베이스에 주기적으로 로드하며, 스크립트 작업에 의해 자동화됩니다.\n- 요청 로딩: 소스 데이터가 지정된 크기에 도달하거나 움직임, 소리 또는 임의의 변경 이벤트와 같은 다양한 이벤트에 의해 트리거됩니다.\n\n일괄 처리 vs. 스트림 로딩:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 배치 로딩: 시간별로 정의된 단위로 데이터를 로드하며 일반적으로 몇 시간에서 며칠 동안 누적됩니다.\n- 스트림 로딩: 데이터가 제공되는 즉시 실시간으로 로드됩니다.\n- 마이크로 배치 로딩: 즉시 처리를 위한 최근 데이터에 액세스합니다.\n\n푸시 대 수신 데이터 로딩:\n\n- 수신 방법: 클라이언트가 서버로부터 데이터를 요청합니다(예: RSS 피드, 이메일).\n- 푸시 방법: 클라이언트가 서버 서비스에 구독하여 데이터를 실시간으로 전달 받습니다(예: 푸시 알림, 즉각 메시징 서비스).\n\n# 데이터 파이프라인이란 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 파이프라인은 데이터의 이동 또는 수정에 특히 관련이 있어요. 이러한 파이프라인은 데이터를 한 곳이나 형식에서 다른 곳이나 형식으로 운송하는 것을 목표로 하며, 데이터를 추출하고 최종적으로 적재하기 위해 선택적 변환 단계를 통해 안내하는 시스템을 구성합니다.\n\n파이프라인을 통해 흐르는 데이터를 시각화하는 것은 데이터 패킷으로 표현할 수 있으며, 이는 데이터의 단위를 넓게 이야기합니다. 이러한 패킷은 단일 레코드나 이벤트에서 대량 데이터 수집물까지 다양할 수 있어요. 이 맥락에서 데이터 패킷은 파이프라인으로 흡수되기 위해 대기열에 정리되며, 데이터 파이프라인의 길이는 단일 패킷이 횡단하는 데 걸리는 시간을 의미합니다. 패킷 간의 화살표는 처리량 지연이나 연속 패킷 도착 사이의 시간을 나타냅니다.\n\n데이터 파이프라인 주요 성능 지표\n\n- 지연 시간: 데이터 패킷이 파이프라인을 통과하는 총 시간을 의미합니다. 지연 시간은 파이프라인 내 각 처리 단계에서 소요된 개별 시간의 합으로, 파이프라인 내 가장 느린 프로세스에 의해 제한됩니다. 예를 들어, 웹 페이지의 로딩 시간은 서버 속도에 따라 결정되며 인터넷 서비스 속도와 관계없이 서버 속도에 의해 제어됩니다.\n- 처리량: 이는 시간 단위당 파이프라인을 통해 처리될 수 있는 데이터 양을 의미합니다. 처리량을 증가시키는 것은 시간 단위당 더 많은 패킷을 처리하고 큰 상자를 차례차례 통과시키는 우리 친구 사슬 예시와 유사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 파이프라인 응용:\n\n- 간단한 복사 파이프라인: 파일 백업과 같이 데이터를 한 위치에서 다른 위치로 복사하는 작업을 포함합니다.\n- 데이터 레이크 통합: 분산된 래 데이터 소스를 데이터 레이크에 통합하는 작업입니다.\n- 거래 기록 이동: 거래 기록을 데이터 웨어하우스로 전송하는 작업입니다.\n- IoT 데이터 스트리밍: IoT 장치에서 데이터를 스트리밍하여 대시보드나 경보 시스템에서 정보를 제공하는 것을 말합니다.\n- 기계 학습용 데이터 준비: 기계 학습의 개발이나 제품화를 위해 래 데이터를 준비하는 작업입니다.\n- 메시지 보내기 및 받기: 이메일, SMS 또는 온라인 비디오 회의와 같은 애플리케이션을 포함합니다.\n\n# 주요 데이터 파이프라인 프로세스\n\n데이터 파이프라인 프로세스는 일반적으로 구조화된 일련의 단계를 따릅니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 추출: 하나 이상의 소스에서 데이터를 검색하는 과정입니다.\n- 투입: 추출된 데이터는 후속 처리를 위해 파이프라인에 투입됩니다.\n- 변환: 파이프라인 내의 선택적인 단계에서 데이터를 변환할 수 있습니다.\n- 로딩: 최종 단계는 변환된 데이터를 대상 시설로 로드합니다.\n- 스케줄링/트리거링: 작업을 필요에 따라 예약하거나 트리거하는 메커니즘입니다.\n- 모니터링: 전체 워크플로우가 효율적으로 작동하도록 모니터링됩니다.\n- 유지보수 및 최적화: 원활한 파이프라인 운영을 보장하기 위해 정기적인 유지보수 및 최적화 작업이 수행됩니다.\n\n# Apache Airflow\n\n- Python 기반의 오픈 소스 \"구성과 코드\" 플랫폼입니다. AirBNB에서 오픈 소스로 공개되었습니다.\n- 데이터 파이프라인 워크플로우를 작성, 예약 및 모니터링할 수 있습니다.\n- 확장 가능하며 병렬 컴퓨팅 노드를 지원하며 주요 클라우드 플랫폼과 통합됩니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 스케줄러: 예약된 워크플로우를 트리거합니다.\n- 실행기: 작업을 Worker에 할당하여 실행합니다.\n- 웹 서버: DAG 검사, 트리거 및 디버깅을 위한 대화형 UI를 호스팅합니다.\n- DAG 디렉토리: 스케줄러, 실행기 및 Worker가 액세스할 수 있는 DAG 파일을 저장합니다.\n- 메타데이터 데이터베이스: 각 DAG 및 해당 작업의 상태를 유지합니다.\n\nDAG 및 작업 라이프사이클\n\nDAG(유향 비순환 그래프)는 작업 간의 종속성과 실행 순서를 지정합니다. 'DAG'는 순환이나 사이클이 없는 관계를 나타내는 특정 유형의 그래프로, 노드와 간선으로 구성되며 방향성을 가진 간선이 노드 간의 흐름을 보여줍니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n작업 상태:\n\n- 상태 없음: 실행을 위해 대기 중인 작업.\n- 스케줄됨: 의존성에 따라 실행이 예약된 작업.\n- 제거됨: 실행이 시작된 이후에 사라진 작업.\n- 상류 작업 실패: 상류 작업에서 실패 발생.\n- 대기 중: 워커 가용성을 기다리는 작업.\n- 실행 중: 워커에 의해 실행 중인 작업.\n- 성공: 오류 없이 작업이 완료된 상태.\n- 실패: 실행 중에 오류가 발생한 작업.\n- 재시도 예정: 남은 재시도 횟수가 남아 있는 실패한 작업으로, 다시 예약됨.\n- 이상적인 작업 흐름: '상태 없음'에서 '스케줄됨'으로, '대기 중'으로, '실행 중'으로 이어져 '성공'으로 마무리됨.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_3.png)\n\nAirflow DAG 스크립트의 논리 블록\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 라이브러리 가져오기: 필요한 Python 라이브러리 가져오기.\n- DAG 인수: DAG에 대한 기본 인수(시작 날짜와 같은 것) 정의.\n- DAG 정의: 특정 속성을 사용하여 DAG 인스턴스화.\n- 작업 정의: DAG 내부의 개별 작업(노드) 정의.\n- 작업 파이프라인: 작업 간의 의존성을 지정하여 작업 간의 흐름을 구축.\n\n이러한 논리적 블록이 포함된 Python 스크립트 예제를 살펴보세요.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_4.png)\n\n# Kafka를 활용한 스트리밍 파이프라인 구축\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이벤트는 이벤트 스트리밍의 맥락에서 엔티티의 관찰 가능한 상태 업데이트를 설명하는 데이터를 의미합니다. 예시로는 자동차의 GPS 좌표, 방 온도, 또는 응용 프로그램의 RAM 사용량 등이 있습니다.\n\n이벤트는 다양한 형식으로 제공됩니다:\n\n- 텍스트, 숫자 또는 날짜와 같은 원시 유형\n- 값이 원시 또는 복합 유형인 키-값 쌍 형식(e.g., JSON, XML)\n- 타임스탬프가 포함된 시간 감도를 위한 키-값 형식\n\n한 소스에서 한 대상으로: 이벤트 스트리밍은 소스(센서, 데이터베이스, 응용 프로그램)가 실시간 이벤트를 지속적으로 생성하고 이를 대상지(파일 시스템, 데이터베이스, 응용 프로그램)로 전달하는 것을 의미합니다. 이 과정은 이벤트 스트리밍이라고 불립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은 출처에서 많은 대상으로: 다양한 통신 프로토콜(FTP, HTTP, JDBC, SCP)을 사용하는 여러 분산 이벤트 소스 및 대상을 관리하는 것은 도전이 될 수 있습니다. 이벤트 스트림 플랫폼(ESP)은 미들웨어로 작용하여 다양한 이벤트 기반 ETL의 처리를 간단하게 만듭니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_5.png)\n\nESP 구성 요소\n\n- 이벤트 브로커: 이벤트를 수신하고 소비하는 핵심 구성 요소입니다.\n- 이벤트 저장소: 받은 이벤트를 저장하여 대상이 비동기적으로 검색할 수 있도록 합니다.\n- 분석 및 쿼리 엔진: 저장된 이벤트를 쿼리하고 분석합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이벤트 브로커는 중요한 컴포넌트입니다. 이는 다음과 같은 구성 요소들을 포함합니다:\n\n- Ingester: 다양한 소스에서 이벤트를 효율적으로 수신합니다.\n- Processor: 직렬화, 역직렬화, 압축, 압축 해제, 암호화 및 복호화와 같은 작업을 수행합니다.\n- Consumption: 저장소에서 이벤트를 검색하고 구독된 대상에게 분배합니다.\n\n인기 있는 이벤트 처리 시스템 솔루션:\n\n- Apache Kafka\n- Amazon Kinesis\n- Apache Flink\n- Apache Spark\n- Apache Storm\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아파치 카프카: 독특한 기능과 광범위한 응용 시나리오를 갖춘 가장 인기 있는 ESP 중 하나입니다. 카프카는 분산 클라이언트-서버 모델을 따릅니다.\n\n- 서버 측: 효율적인 협업을 위해 ZooKeeper가 관리하는 여러 브로커로 구성됩니다.\n- 네트워크 통신: 클라이언트와 서버 간의 데이터 교환에 TCP를 활용합니다.\n- 클라이언트 측: CLI, 자바, 스칼라, REST API 및 타사 옵션을 포함한 다양한 클라이언트를 제공합니다.\n\n카프카의 인기 이유는?\n\n- 확장성: 데이터를 여러 브로커에 분산하여 확장성과 고 처리량을 보장합니다.\n- 높은 신뢰성: 안정성을 위해 여러 파티션과 복제를 사용합니다.\n- 영구적인 지속성: 이벤트를 영구적으로 저장하여 소비자의 편의에 맞게 사용할 수 있습니다.\n- 오픈 소스: 특정 요구 사항에 맞춰 사용자 정의가 가능하여 무료로 제공됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 카프카 아키텍처\n\n카프카 클러스터는 여러 브로커로 구성되어 있으며, 각 브로커는 이벤트를 수신, 저장, 처리 및 배포하는 역할을 합니다. ZooKeeper에 의해 조율되는 이러한 브로커들은 로그 또는 트랜잭션과 같은 특정 이벤트 유형을 저장하는 데이터베이스와 유사한 주제를 관리합니다.\n\n파티셔닝과 복제: 카프카는 장애 허용성과 병렬 이벤트 처리를 위해 파티셔닝과 복제를 사용합니다. 일부 브로커가 실패하더라도, 카프카는 주제 파티션을 운영 중인 브로커에 분산시킴으로써 지속성을 보장합니다.\n\nKafka CLI를 사용한 주제 관리: 카프카 명령줄 인터페이스는 카프카 클러스터 내에서 주제를 생성, 나열, 설명 및 삭제하는 기능을 제공합니다. 명령에는 정의된 파티션 및 복제로 주제를 생성하고 주제 및 구성에 대한 자세한 정보를 얻는 등의 작업이 포함됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_6.png)\n\n카프카 프로듀서\n\n카프카 프로듀서는 이벤트를 토픽 파티션에 발행하는 클라이언트 앱입니다. 이벤트는 선택적 파티셔닝을 위해 키와 연결될 수 있습니다. 프로듀서 CLI를 사용하면 프로듀서를 관리하고 지정된 토픽에 이벤트를 키와 함께 발행할 수 있습니다.\n\n컨슈머로 이벤트 읽기: 컨슈머는 토픽에 가입하고 저장된 이벤트를 읽어 순차적으로 오프셋을 유지합니다. 오프셋을 재설정함으로써 컨슈머는 처음부터 이벤트를 다시 재생할 수 있습니다. 카프카 컨슈머와 프로듀서는 독립적으로 작동하여 동기화 없이 이벤트를 저장하고 소비할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n끝까지 이어지는 이벤트: 날씨 파이프라인\n\n단계 1: 이벤트 소스 정의\n\n극단적인 날씨에 대한 대중의 반응을 이해하기 위해 날씨와 트위터 이벤트 스트림을 분석하고 싶다고 상상해보세요. 두 가지 주요 이벤트 소스를 활용할 것입니다:\n\n- IBM Weather API: JSON 형식의 실시간 및 예보 날씨 데이터를 제공합니다.\n- Twitter API: JSON 형식의 실시간 트윗 및 언급을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계 2: Kafka 토픽 구성\n\nKafka 클러스터에서 날씨 및 트위터 이벤트용 전용 토픽을 생성하여 데이터 흐름을 효율적으로 처리하기 위해 적절한 파티션 및 복제를 보장하세요.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_7.png)\n\n단계 3: 프로듀서 개발\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 이벤트 소스에 대해 특정한 프로듀서를 개발하세요. 이들은 JSON 데이터를 바이트로 직렬화하고 해당 Kafka 토픽으로 게시할 것입니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_8.png)\n\n단계 4: 컨슈머 구현\n\n날씨 및 Twitter 이벤트용 전용 컨슈머를 생성하세요. 이 컨슈머들은 Kafka 토픽에서 바이트를 역직렬화하여 JSON 데이터로 변환한 후 처리할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_9.png\" /\u003e\n\n단계 5: Persistence를 위한 DB Writer 통합\n\n이벤트 데이터를 관계형 데이터베이스에 쓰고 싶다면 DB writer를 사용하십시오. 이 구성 요소는 컨슈머에서 JSON 파일을 구문 분석하고 해당 데이터베이스 레코드를 생성합니다.\n\n```js\n#db writer EXAMPLE\nimport json\nimport sqlite3\n\ndef write_to_database(record):\n    connection = sqlite3.connect(\"event_database.db\")\n    cursor = connection.cursor()\n    cursor.execute(\"INSERT INTO events VALUES (?)\", (json.dumps(record),))\n    connection.commit()\n    connection.close()\n\nrecord = {\"event_type\": \"weather\", \"data\": {\"temperature\": 25, \"location\": \"NYC\"}}\nwrite_to_database(record)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n6단계: SQL을 사용한 데이터베이스 상호작용\n\n데이터베이스에 레코드를 쓰기 위해 SQL 삽입문을 사용하세요. 이 단계는 카프카 토픽에서 데이터를 영구 저장소 솔루션으로 전환하는 과정을 완료합니다.\n\n```js\n-- SQL 삽입 예시\nINSERT INTO events VALUES ('{\"event_type\": \"weather\", \"data\": {\"temperature\": 25, \"location\": \"NYC\"}');\n```\n\n7단계: 시각화 및 분석\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내, 수집하고 저장된 이벤트 데이터로부터 통찰력 있는 시각화와 분석을 위해 데이터베이스 레코드를 쿼리하세요. 수집된 이벤트 데이터로부터 가치 있는 통찰력을 얻기 위해 대시보드를 사용하는 것이 가장 좋습니다.\n\n이 end-to-end 파이프라인은 다양한 구성 요소의 원활한 통합을 보여주며, 이벤트 스트림을 관리하는 Kafka의 유연성과 강력함을 강조합니다.\n\n참고: 본 블로그 게시물에서 제공된 메모 및 정보는 \"ETL 및 쉘, Airflow 및 Kafka를 사용한 데이터 파이프라인\" 과정 중에 편집되었으며 개인적인 용도를 위한 기본 개요를 제공하기 위한 것입니다.","ogImage":{"url":"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png"},"coverImage":"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png","tag":["Tech"],"readingTime":10},{"title":"DBT  Airflow  ","description":"","date":"2024-06-19 09:45","slug":"2024-06-19-dbtAirflow","content":"\n\n![image](/assets/img/2024-06-19-dbtAirflow_0.png)\n\n요즘의 동적이고 경쟁적인 환경에서 기업은 데이터 기반 의사결정에 크게 의존하고 있습니다. 이를 실현하기 위해 조직은 믿을 수 있는 견고한 데이터 플랫폼과 고품질 데이터가 필요합니다. 이는 데이터를 효과적으로 수집하고 저장하는 시스템뿐만 아니라 데이터의 정확성과 신뢰성을 보장하는 것을 의미합니다.\n\n수백 개 또는 수천 개의 데이터 모델을 관리하는 것은 항상 간단한 과정은 아닙니다. 데이터 팀은 종종 데이터 자산을 효과적으로 구축, 테스트 및 유지하는 데 고민합니다. 플럼에서는 데이터 빌드 도구(dbt)를 활용하여 데이터 모델을 효과적으로 관리할 수 있는 CLI 도구에 의존합니다.\n\n그러나 dbt는 명령줄 인터페이스의 특성 때문에 도전적인 요소를 가지고 있습니다. 그렇다면, 제품 환경에 배포할 때 dbt의 장점을 팀이 어떻게 활용할 수 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ndbt Cloud가 당연한 선택처럼 보일 수 있지만, 모든 팀 또는 회사가 이 제품에 투자하기에 준비되어 있는 것은 아닙니다. 이러한 상황에서는 dbt 모델의 실행을 조율하고 그 사이의 종속성을 유지하는 대체 방법이 필요합니다.\n\n# 왜 자체 통합을 구축하기로 투자했는가\n\ndbt Cloud에 필요한 투자 외에도, 우리 팀은 특정 기능을 지원받길 원했습니다. 그러나 당시에는 플랫폼에서 이러한 기능 중 일부만 지원되었습니다. 아래에서 가장 기본적인 것들을 개요하겠습니다.\n\n![2024-06-19-dbtAirflow_1.png](/assets/img/2024-06-19-dbtAirflow_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- dbt 프로젝트 일정 설정: 우리는 dbt 프로젝트를 cron 작업 스타일로 일정을 잡고, 특정 시간 간격을 지정합니다.\n- 작업 세분화: 각 dbt 엔티티(모델, 테스트, 시드, 스냅샷을 포함)는 개별적으로 처리되어 필요한 경우에 개별적으로 트리거될 수 있도록 합니다.\n- dbt 의존성 유지: 또한, dbt 엔티티 간의 종속성을 유지해야 합니다. 예를 들어, 모델 A가 테스트를 포함하여 다른 모델 B의 상위 의존성으로 작용하는 경우 실행 순서를 유지해야 합니다.\n\n```js\n[dbt run A] -\u003e [dbt test A] -\u003e [dbt run B]\n```\n\n4. 다른 워크플로우 트리거: 특정 dbt 엔티티의 성공적인 완료 후에 특정 워크플로우를 시작해야 하며, 전체 dbt 프로젝트가 완료될 필요가 없는 경우가 있습니다. 또한, 두 dbt 엔티티 실행 사이에 특정 워크플로우를 시작해야할 경우가 있을 수 있습니다.\n\n```js\n[dbt run A] -\u003e [dbt와 무관한 워크플로우 실행] -\u003e [dbt run B]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5. 알림: 문제가 발생했을 때, 슬랙으로 알림을 받아야 합니다.\n\n6. 컨테이너화된 dbt 프로젝트 실행: 각각의 dbt 프로젝트는 자체 Docker 이미지를 가져야 합니다. 필요시 다른 dbt 버전에서 실행할 수 있는 유연성을 제공합니다.\n\n7. 모델 하위 집합 트리거링: dbt 엔티티는 dbt 태그를 기반으로 필터링될 수 있어서 필요한 특정 모델 그룹을 실행할 수 있습니다.\n\n8. 여러 일정 생성: 동일한 프로젝트를 서로 다른 일정으로 실행해야 할 수도 있습니다. 이는 모델 하위 집합을 트리거할 필요와 밀접하게 관려이 있습니다. 예를 들어, 모델을 시간별, 일별, 주간별로 태그 지정하여 해당 일정에 맞춰 실행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다양한 대안을 탐색해 본 결과, 천문학자가 개발한 Cosmos 패키지를 포함해 어떤 것도 당시의 우리 사용 사례와 명확한 요구 사항에 적합하다는 것을 입증하지 못했습니다.\n\n이러한 도전에 직면하자, 우리는 우리의 요구에 맞게 제작된 자체 Python 패키지를 개발하기로 전략적인 결정을 내렸습니다. 이 맞춤형 솔루션은 우리에게 dbt와 Airflow를 원활하게 통합할 수 있는 유연성과 기능이 제공되어, 데이터 팀이 효과적으로 데이터 모델을 관리하고 최적화할 수 있게 해줍니다.\n\n# 왜 Airflow를 선택했나요?\n\n데이터 파이프라인 Orchestration 전략을 설계할 때, 우리는 특정 간격(시간별, 일별 또는 주별)으로 작업을 예약하고 관리할 수 있는 능력을 지향했습니다. 팀이 Airflow 및 Google Cloud의 Cloud Composer 서비스에 익숙하고 기존 인프라가 있었기 때문에 Airflow를 선택하는 것이 우리의 요구에 적합한 자연스러운 선택이었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한 Airflow와 dbt는 Directed Acyclic Graphs (DAGs) 개념을 중심으로 하고 있어, dbt DAGs를 Airflow DAGs로 변환하여 Orchestration(오케스트레이션)할 수 있습니다.\n\nDAGs는 노드가 닫힌 순환 루프를 형성하지 않고 방향성을 가지고 연결된 작업 또는 데이터 모델을 나타내는 그래프입니다. dbt에서 DAGs는 데이터 모델 간의 관계와 종속성을 나타내고, Airflow에서는 DAGs는 데이터 파이프라인에 포함된 단계와 종속성을 시각화합니다.\n\n# 0부터 1까지: Cloud Composer에서 dbt 실행하기\n\n이 초기 단계에서 우리의 작업은 상대적으로 간단했습니다: Airflow를 사용하여 dbt 프로젝트를 실행하고 테스트하되, 각 모델, 테스트, 스냅샷 또는 시드를 개별 작업으로 설정할 필요가 없었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n필요한 것은 GitHub Actions를 사용하여 CI/CD 파이프라인을 설정하는 것이었습니다. 이 파이프라인은 관련된 dbt 프로젝트를 Cloud Composer 버킷으로 복사하고 Airflow 내에서 DAG를 구성합니다. 이 DAG에는 모델을 실행하는 하나의 작업 및 해당 작업을 테스트하는 다른 작업이 포함됩니다.\n\n![2024-06-19-dbtAirflow_2](/assets/img/2024-06-19-dbtAirflow_2.png)\n\n이것은 Airflow에서 dbt 프로젝트를 실행하는 가장 간단한 방법일 수 있습니다. DAG는 BashOperator로 생성된 두 개의 작업으로 구성됩니다. 기본적으로, 이는 로컬에서 dbt run 및 dbt test 명령을 실행하는 dbt CLI를 사용한 프로세스를 반영합니다.\n\n그러나 이 방법에는 여러 가지 제약이 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 만약에 dbt_run 작업 내에서 모델 실행이 실패한다면, 문제를 해결하기 위해 작업을 다시 실행해야 합니다. 이는 이전에 성공했던 모델들을 포함하여 모든 모델을 다시 실행해야 하므로 실행 시간과 비용이 증가하는 것을 의미합니다.\n- 모델 실행과 테스트는 완전히 독립적입니다. 모든 테스트는 워크플로우의 끝에 실행됩니다. 따라서 다른 모델들의 상위 종속성인 모델들에서 문제가 발생할 경우 너무 늦기 전까지 알아차리지 못할 수 있습니다.\n- 만약에 dbt_test 작업에서 모델 테스트가 실패한다면, 이 문제를 해결하기 위해 영향을 받는 모든 모델을 수정하기 위해 dbt_run과 dbt_test 작업을 검토하고 재실행해야 합니다.\n- Airflow 작업은 BashOperator를 사용하여 생성되며, Airflow 환경에 dbt 패키지 의존성이 설치되어 있어야 합니다. 이는 패키지 버전 호환성 문제로 인해 문제가 될 수 있으며, 특히 GCP의 Cloud Composer나 AWS의 MWAA와 같은 관리형 Airflow 서비스에서 더욱 그렇습니다.\n\n이 초기 시도는 Proof-of-Concept(PoC)로서의 느낌이 더 큰 노력입니다. 제한사항과 확장성 부족에도 불구하고, 팀의 기대에 부합하는 해결책을 개발할 자신감을 제공했습니다.\n\n우리의 다음 과제는 dbt 프로젝트를 Airflow DAG로 변환하여 각 dbt 엔티티가 해당하는 Airflow 작업으로 매핑되고 모든 종속성이 유지되도록 하는 것이었습니다.\n\n# dbt-airflow 빌딩\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ndbt DAG를 Airflow DAG로 변환하기 위해 첫 번째 작업은 dbt 엔터티에서 의존성을 추출하는 것이었습니다. 이는 데이터 모델 간의 관계를 구문 분석하고 DAG 내의 Airflow 작업에 매핑하는 것을 포함했습니다.\n\ndbt 의존성을 Airflow 작업으로 표현함으로써 데이터 워크플로를 원활하게 조정할 수 있었습니다. 작업(예: 모델 실행 또는 테스트)이 실패하면 분석가와 분석 엔지니어는 DAG의 나머지 부분을 해제하기 위해 기초 모델 문제에 대처해야 했습니다.\n\n이 접근 방식은 처음에는 방해적으로 보일 수 있지만, 물론 목표는 신뢰할 수 없는 데이터로 모델을 구축하는 것이 아닙니다. 그러나 초기 오류 감지는 우리 설계의 중요한 측면이었습니다. 이렇게 함으로써 비용을 최소화하고 계산 속도를 절약하며 데이터에 대한 신뢰를 유지할 수 있었습니다.\n\n## 반복 1: manifest.json 파일 활용\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n첫 번째 반복에서는 dbt가 생성한 manifest.json 파일을 읽는 파서를 개발했습니다. 이 메타데이터 파일은 dbt가 실행, 테스트 및 컴파일 중에 생성됩니다.\n\n이 파일의 내용을 사용하여 모든 dbt 엔티티와 해당 의존성을 추출할 수 있었습니다. 이 이정표는 dbt-airflow shaping의 탄생을 알리는 중요한 순간이었습니다.\n\n첫 릴리스는 manifest.json 파일에서 프로젝트를 읽고 모델 의존성을 추출하여 Airflow DAG 내의 TaskGroup으로 변환하는 능력을 갖추었습니다.\n\n이제 특정 작업이 실패할 때와 같이, 하류 의존성은 누군가 기본 문제를 해결할 때까지 일시 중지되어 DAG의 나머지 부분이 차단 해제됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 방법을 통해 문제가 발생했을 때 전체 모델 또는 테스트 세트를 다시 실행할 필요가 없어졌어요. 결과적으로, 이 전략은 Airflow에서 실행되는 dbt 프로젝트의 실행 및 유지에 관련된 비용을 크게 줄였어요.\n\n초기 Airflow DAG는 다음과 같았어요;\n\n```js\nimport functools\nfrom datetime import datetime \nfrom datetime import timedelta\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import PythonOperator\nfrom dbt_airflow.core.config import DbtAirflowConfig\nfrom dbt_airflow.core.config import DbtProfileConfig\nfrom dbt_airflow.core.config import DbtProjectConfig\nfrom dbt_airflow.core.task_group import DbtTaskGroup\nfrom dbt_airflow.operators.execution import ExecutionOperator\n\n\nwith DAG(\n    dag_id='test_dag',\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2),\n        },\n    on_failure_callback=functools.partial(\n        our_callback_function_to_send_slack_alerts\n    ),\n) as dag:\n\n    t1 = EmptyOperator(task_id='extract')\n    t2 = EmptyOperator(task_id='load')\n\n    tg = DbtTaskGroup(\n        group_id='transform',\n        dbt_project_config=DbtProjectConfig(\n            project_path=Path('/path/to/example_dbt_project/'),\n            manifest_path=Path('/path/to/example_dbt_project/target/manifest.json'),\n        ),\n        dbt_profile_config=DbtProfileConfig(\n            profiles_path=Path('/path/to/example_dbt_project/profiles'),\n            target='dev',\n        ),\n        dbt_airflow_config=DbtAirflowConfig(\n            execution_operator=ExecutionOperator.BASH,\n        ),\n    )\n\n    t1 \u003e\u003e t2 \u003e\u003e tg\n```\n\n프로젝트를 Cloud Composer에 배포하는 관점에서 이번 반복에서는 아무런 변경 사항이 없었어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Screenshot 1](/assets/img/2024-06-19-dbtAirflow_3.png)\n\nBy the end of this first iteration, we were able to meet half of the requirements we specified during ideation:\n\n![Screenshot 2](/assets/img/2024-06-19-dbtAirflow_4.png)\n\n## Iteration 2: Introducing Extra Tasks\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이번 반복에서는 초기 설계의 핵심 측면으로 초점을 옮겼습니다: 기존 모든 모델을 Airflow로 원활하게 마이그레이션할 수 있는 능력입니다.\n\nMachine Learning 워크플로우를 모델 실행 사이에 통합하여 하향 모델이 필요로 하는 데이터를 생성할 수 있도록 목표를 설정했습니다. 이 접근은 결과적으로 강하게 결합된 시스템을 만들어 내어 최선의 방법에 부합하지 않을 수 있지만, 이는 당시 실행 흐름을 대표했습니다. dbt-airflow가 이 흐름을 재현할 수 있도록 보장하는 것이 우선이었고, 이러한 구성 요소를 분리하는 구조 변경을 고려하기 전에 기술적 부채에 대한 대응은 그때의 주요 초점이 아니었습니다.\n\n그러나 이것은 다른 작업을 가속화할 수 있는 가치 있는 기능이었습니다. 예를 들어, 특정 dbt 모델에서 데이터에 의존하는 워크플로우는 해당 모델이 완료된 직후에 즉시 트리거될 수 있었고, 전체 DAG가 완료될 때까지 기다릴 필요가 없었습니다.\n\n기술적으로는 관심있는 dbt 프로젝트를 구문 분석한 후 생성된 Airflow 작업 이후에 Airflow Operator를 도입하려고 했습니다. 이를 달성하기 위해 Extra Task의 상위 또는 하위 작업을 명시해야 했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n'Extra Tasks'를 표현하기 위해 우리는 ExtraTask라는 객체를 개발했습니다. 이 객체는 렌더링된 dbt 프로젝트 내에서 이러한 작업을 소개하는 데 활용될 수 있습니다.\n\n```js\nfrom airflow.operators.python import PythonOperator\nfrom dbt_airflow.core.task import ExtraTask\n\n\nExtraTask(\n    task_id='test_task',\n    operator=PythonOperator,\n    operator_args={\n        'python_callable': lambda: print('Hello world'),\n    },\n    upstream_task_ids={\n        'model.example_dbt_project.int_customers_per_store',\n        'model.example_dbt_project.int_revenue_by_date',\n    },\n)\n```\n\n다음은 dbt-airflow의 Extra Task 기능을 활용하여 유명한 Sakila 프로젝트를 사용하여 생성된 더미 dbt 프로젝트를 렌더링하고 실행하는 예시 DAG입니다.\n\n```js\nimport functools\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import PythonOperator\nfrom dbt_airflow.core.config import DbtAirflowConfig\nfrom dbt_airflow.core.config import DbtProfileConfig\nfrom dbt_airflow.core.config import DbtProjectConfig\nfrom dbt_airflow.core.task import ExtraTask\nfrom dbt_airflow.core.task_group import DbtTaskGroup\nfrom dbt_airflow.operators.execution import ExecutionOperator\n\n\nwith DAG(\n    dag_id='test_dag',\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2),\n    },\n    'on_failure_callback': functools.partial(\n        our_callback_function_to_send_slack_alerts\n    ),\n) as dag:\n\n    t1 = EmptyOperator(task_id='extract')\n    t2 = EmptyOperator(task_id='load')\n\n    extra_tasks = [\n        ExtraTask(\n            task_id='test_task',\n            operator=PythonOperator,\n            operator_args={\n                'python_callable': lambda: print('Hello world'),\n            },\n            upstream_task_ids={\n                'model.example_dbt_project.int_customers_per_store',\n                'model.example_dbt_project.int_revenue_by_date',\n            },\n        ),\n        ExtraTask(\n            task_id='another_test_task',\n            operator=PythonOperator,\n            operator_args={\n                'python_callable': lambda: print('Hello world 2!'),\n            },\n            upstream_task_ids={\n                'test.example_dbt_project.int_customers_per_store',\n            },\n            downstream_task_ids={\n                'snapshot.example_dbt_project.int_customers_per_store_snapshot',\n            },\n        ),\n        ExtraTask(\n            task_id='test_task_3',\n            operator=PythonOperator,\n            operator_args={\n                'python_callable': lambda: print('Hello world 3!'),\n            },\n            downstream_task_ids={\n                'snapshot.example_dbt_project.int_customers_per_store_snapshot',\n            },\n            upstream_task_ids={\n                'model.example_dbt_project.int_revenue_by_date',\n            },\n        )\n    ]\n\n    tg = DbtTaskGroup(\n        group_id='my-dbt-project',\n        dbt_project_config=DbtProjectConfig(\n            project_path=Path('/path/to/example_dbt_project/'),\n            manifest_path=Path('/path/to/example_dbt_project/target/manifest.json'),\n        ),\n        dbt_profile_config=DbtProfileConfig(\n            profiles_path=Path('/path/to/example_dbt_project/profiles'),\n            target='dev',\n        ),\n        dbt_airflow_config=DbtAirflowConfig(\n            extra_tasks=extra_tasks,\n            execution_operator=ExecutionOperator.BASH,\n        ),\n    )\n\n    t1 \u003e\u003e t2 \u003e\u003e tg\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 번째 반복을 마칠 때, 우리는 더욱 우리가 목표로 한 최종 제품에 한 발짝 더 가까워졌어요.\n\n![이미지](/assets/img/2024-06-19-dbtAirflow_5.png)\n\n## 반복 3: 필터링 태그 및 다중 스케줄 생성\n\n이 반복에서, 우리의 목표는 특정 DAG 내에서 특정 dbt 엔티티의 하위 집합만 렌더링할 수 있게 하는 기능을 구현하는 것이었어요. 이는 여러 목적을 달성하기 위한 것이었죠:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 동일한 모델에 대해 시간별, 일별 또는 주간별로 다른 일정을 생성하려면 해당 태그가 지정된 모델만 선택하여 선택하면 됩니다.\n- 특정 이유로 특정 태그가 지정된 모델을 특정 DAG에 포함하지 않으려면 필터링을 해야 합니다.\n- 다른 워크플로에서 특정 시점에 일부 모델을 새로 고침해야 할 때 일부 모델만 렌더링해야 할 수 있습니다.\n\n이를 달성하기 위해, DbtAirflowConfig 객체에 'include_tags'와 'exclude_tags' 두 가지 추가 인수를 도입했습니다. 두 인수 모두 렌더링된 프로젝트에서 Entity를 포함하거나 제외하는 데 사용되는 dbt 태그와 일치하는 문자열 목록을 수용합니다.\n\n이 기능 추가의 주요 도전 과제는 필터링된 노드가 올바른 종속성을 유지하는 것이어서 예상보다 복잡했습니다.\n\n이 반복에서 우리는 이 프로젝트를 시작할 때 목표로 했던 최종 버전에 한 걸음 가까워졌습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-dbtAirflow_6.png\" /\u003e\n\n## 이터레이션 4: 컨테이너화된 프로젝트 실행\n\n에어플로우에서 BashOperator를 통해 dbt 명령을 실행할 때는 Airflow 환경 내에서 결과가 자료화되는 대상 시스템을 기준으로 dbt-core 및 해당 dbt 어댑터를 설치하는 것이 중요합니다.\n\n그러나 이 접근 방식은 패키지의 호환성 문제 등의 위험과 어려움을 야기할 수 있습니다. 특히 제공 업체의 다른 클라우드 서비스와의 통합을 용이하게하기 위해 미리 정의된 종속성을 함께 제공하는 클라우드의 관리형 Airflow 서비스인 클라우드 컴포저와 같은 서비스를 사용할 때 이 문제가 발생할 가능성이 높아집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고, 여러 개의 dbt 프로젝트를 관리하는 것은 서로 다른 dbt 버전에서 실행해야 할 수도 있습니다. 안타까운 점은 Airflow 환경에 dbt를 직접 설치하면 기술적으로 이것이 불가능하다는 것입니다.\n\n이러한 도전에 대처하고 버전 관리의 유연성을 보장하기 위해 우리는 dbt 프로젝트를 컨테이너화하고 k8s에서 실행하기로 결정했습니다. 이 방법은 다양한 사용 사례를 지원할뿐만 아니라 필요에 따라 dbt 버전을 업그레이드 또는 다운그레이드할 수 있는 유연성을 제공합니다.\n\n![이미지](/assets/img/2024-06-19-dbtAirflow_7.png)\n\n새 CI/CD 파이프라인이 트리거될 때마다, dbt 프로젝트를 포함하는 Docker 이미지가 빌드되어 Google Cloud의 Artifact Registry로 푸시됩니다. 그러나 dbt-airflow는 여전히 DAG를 렌더링하기 위해 manifest.json 파일에 의존하므로 전체 dbt 프로젝트를 복사하는 대신 manifest 파일만 Cloud Composer GCS 버킷으로 복사해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ndbt-airflow에서는 라이브러리를 확장하여 KubernetesPodOperator를 사용하여 Airflow Tasks를 실행하는 지원을 추가했어요. 이 기능을 통해 DbtAirflowConfig 내에서 execution_operator=ExecutionOperator.KUBERNETES를 지정할 수 있게 되었어요. 이 설정을 사용하면 TaskGroup 내의 각 작업이 자체 Kubernetes pod에서 실행되며 지정된 컨테이너 및 추가 구성을 활용할 수 있어요.\n\n```python\nimport functools\nfrom datetime import datetime \nfrom datetime import timedelta\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import PythonOperator\nfrom dbt_airflow.core.config import DbtAirflowConfig\nfrom dbt_airflow.core.config import DbtProfileConfig\nfrom dbt_airflow.core.config import DbtProjectConfig\nfrom dbt_airflow.core.task_group import DbtTaskGroup\nfrom dbt_airflow.operators.execution import ExecutionOperator\n\n\nwith DAG(\n    dag_id='test_dag',\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2),\n    },\n    'on_failure_callback': functools.partial(\n        our_callback_function_to_send_slack_alerts\n    ),\n) as dag:\n\n    t1 = EmptyOperator(task_id='extract')\n    t2 = EmptyOperator(task_id='load')\n\n    tg = DbtTaskGroup(\n        group_id='transform',\n        dbt_airflow_config=DbtAirflowConfig(\n            create_sub_task_groups=True,\n            execution_operator=ExecutionOperator.KUBERNETES,\n            operator_kwargs={\n                'name': f'dbt-project-1-dev',\n                'namespace': 'composer-user-workloads',\n                'image': 'gcp-region-docker.pkg.dev/gcp-project-name/ar-repo/dbt-project-1:latest',\n                'kubernetes_conn_id': 'kubernetes_default',\n                'config_file': '/home/airflow/composer_kube_config',\n                'image_pull_policy': 'Always',\n            },\n        ),\n        dbt_project_config=DbtProjectConfig(\n            project_path=Path('/home/project-1/'),  # 도커 컨테이너 내의 경로\n            manifest_path=Path('/home/airflow/gcs/dags/dbt/project-1/target/manifest.json'),  # Cloud Composer GCS 버킷에 있는 경로\n        ),\n        dbt_profile_config=DbtProfileConfig(\n            profiles_path=Path('/home/project-1/profiles/'),  # 도커 컨테이너 내의 경로\n            target=dbt_profile_target,\n        ),\n    )\n\n    t1 \u003e\u003e t2 \u003e\u003e tg\n```\n\n그리고 이 기능 추가로 프로젝트 초반에 지정된 초기 요구 사항 집필을 성공적으로 구현했어요.\n\n\u003cimg src=\"/assets/img/2024-06-19-dbtAirflow_8.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# dbt-airflow을 시작해 보세요\n\ndbt 프로젝트를 배포하고 일정을 예약하는 방법을 찾고 있다면, dbt-airflow를 사용하면 귀찮음을 덜 수 있습니다. Plum의 데이터팀은 이미 1년 이상 운영 중인 이 도구를 안정적이고 확장 가능하다고 입증했습니다. 그래서 여러분의 프로젝트에도 실제 가치를 제공할 수 있다고 자신합니다. 이 패키지는 플랫폼에 구애받지 않으며 dbt에서 지원하는 모든 대상과 함께 사용할 수 있습니다.\n\n이 프로젝트는 GitHub에서 유지되고 PyPI에서도 사용할 수 있습니다.\n\n또한 공식 문서에서 더 많은 세부 정보를 찾을 수 있습니다. 사실, 이 문서에서는 패키지가 제공하는 전체 기능 중 일부만 다루었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n프로젝트에 기여하는 것을 장려합니다. 무언가 부족한 부분이 있으면 참여해 주시기를 바랍니다.\n\n코딩 즐기세요,\n\nPlum Data Engineering Team ❤","ogImage":{"url":"/assets/img/2024-06-19-dbtAirflow_0.png"},"coverImage":"/assets/img/2024-06-19-dbtAirflow_0.png","tag":["Tech"],"readingTime":16},{"title":"데이터 엔지니어를 위한 자료 구조와 알고리즘 면접 질문","description":"","date":"2024-06-19 09:44","slug":"2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer","content":"\n\n데이터 엔지니어링 면접 준비는 데이터 구조와 알고리즘(DSA)에 대한 튼튼한 이해력이 필요합니다. 여러분이 잘 준비될 수 있도록, 자주 물어지는 DSA 면접 질문들을 정리한 목록을 제공해 드립니다. 연습 문제 링크가 포함되어 있어 다양한 주제를 다루며 다음 면접에 잘 준비될 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer_0.png)\n\n## 가장 자주 물어지는 DSA 문제 목록\n\n- 두 수의 합: [문제 링크](https://leetcode.com/problems/two-sum/description/)\n- 중복되는 문자가 없는 가장 긴 부분 문자열: [문제 링크](https://leetcode.com/problems/longest-substring-without-repeating-characters/)\n- 합이 k인 부분 배열: [문제 링크](https://leetcode.com/problems/subarray-sum-equals-k/description/)\n- 중복 숫자 찾기: [문제 링크](https://leetcode.com/problems/find-the-duplicate-number/description/)\n- 배열에서 Leaders: [문제 링크](https://www.geeksforgeeks.org/problems/leaders-in-an-array-1587115620/1)\n- 첫 번째 문자열에서 두 번째 문자열에 있는 문자 제거: [문제 링크](https://www.geeksforgeeks.org/remove-characters-from-the-first-string-which-are-present-in-the-second-string/)\n- 두 수 더하기: [문제 링크](https://leetcode.com/problems/add-two-numbers/description/)\n- 0과 1 문제: [문제 링크](https://leetcode.com/problems/ones-and-zeroes/description/)\n- 주식 매수와 매도의 최적 시기 II: [문제 링크](https://leetcode.com/problems/best-time-to-buy-and-sell-stock-ii/description/)\n- 최대 연속 1 개: [문제 링크](https://leetcode.com/problems/max-consecutive-ones/description/)\n- 끝에서 N번째 노드 제거: [문제 링크](https://leetcode.com/problems/remove-nth-node-from-end-of-list/description/)\n- 무작위 포인터가 있는 목록 복사: [문제 링크](https://leetcode.com/problems/copy-list-with-random-pointer/)\n- 문자열 뒤집기: [문제 링크](https://leetcode.com/problems/reverse-string/description/)\n- 첫 번째 누락된 양수: [문제 링크](https://leetcode.com/problems/first-missing-positive/description/)\n- 연속적인 1의 최대 개수 III: [문제 링크](https://leetcode.com/problems/max-consecutive-ones-iii/description/)\n- 홀수 짝수 연결 목록: [문제 링크](https://leetcode.com/problems/odd-even-linked-list/)\n- 목록 분할: [문제 링크](https://leetcode.com/problems/partition-list/description/)\n- 연결된 목록 순환이 있습니까? [문제 링크](https://leetcode.com/problems/linked-list-cycle/description/)\n- 유효한 괄호: [문제 링크](https://leetcode.com/problems/valid-parentheses/description/)\n- 부분 문자열의 모든 발생 제거: [문제 링크](https://leetcode.com/problems/remove-all-occurrences-of-a-substring/description/)\n- 첫 번째 나타나는 아닌 반복 문자 찾기: [문제 링크](https://www.geeksforgeeks.org/given-a-string-find-its-first-non-repeating-character/)\n- 목록에서 양수와 음수 수 세기: [문제 링크](https://www.geeksforgeeks.org/python-program-to-count-positive-and-negative-numbers-in-a-list/)\n- 목록에서 두 번째로 큰 숫자 찾기: [문제 링크](https://www.geeksforgeeks.org/python-program-to-find-second-largest-number-in-a-list/)\n- 최대 유효 괄호: [문제 링크](https://leetcode.com/problems/longest-valid-parentheses/description/)\n- 목록 뒤집기: [문제 링크](https://www.geeksforgeeks.org/python-reversing-list/)\n- 가장 긴 공통 접두어: [문제 링크](https://leetcode.com/problems/longest-common-prefix/)\n- 최대 부분 배열: [문제 링크](https://leetcode.com/problems/maximum-subarray/description/)\n- 평균 최대 부분 배열 I: [문제 링크](https://leetcode.com/problems/maximum-average-subarray-i/description/)\n- 배열의 곱: [문제 링크](https://leetcode.com/problems/product-of-array-except-self/description/)\n- 마지막 단어의 길이: [문제 링크](https://leetcode.com/problems/length-of-last-word/description/)\n- 문자열 뒤집기: [문제 링크](https://leetcode.com/problems/reverse-string/description/)\n- 목록 정렬: [문제 링크](https://leetcode.com/problems/sort-list/)\n- 최소 부분 배열 합: [문제 링크](https://leetcode.com/problems/minimum-size-subarray-sum/description/)\n- 최소 창 하위 문자열: [문제 링크](https://leetcode.com/problems/minimum-window-substring/description/)\n- 배열 K회 회전하기: [문제 링크](https://leetcode.com/problems/rotate-array/description/)\n- 과반 요소: [문제 링크](https://leetcode.com/problems/majority-element/description/)\n- 가장 긴 팰린드롬 부분 문자열: [문제 링크](https://leetcode.com/problems/longest-palindromic-substring/)\n- 엑셀 시트 열 제목: [문제 링크](https://leetcode.com/problems/excel-sheet-column-title/description/)\n- 애너그램 확인: [문제 링크](https://leetcode.com/problems/valid-anagram/description/)\n- 주식 매수 및 매도 최적 시기 III: [문제 링크](https://leetcode.com/problems/best-time-to-buy-and-sell-stock-iii/description/)\n- 오른쪽에서 가장 큰 요소로 요소 바꾸기: [문제 링크](https://leetcode.com/problems/replace-elements-with-greatest-element-on-right-side/description/)\n- 문장에서 가장 흔한 단어: [문제 링크](https://leetcode.com/problems/most-common-word/description/)\n- 주어진 합계를 갖는 부분 배열 찾기: [문제 링크](https://www.geeksforgeeks.org/find-subarray-with-given-sum/)\n- 단어 분리: [문제 링크](https://leetcode.com/problems/word-break/description/)\n- 문자열에서 첫 번째 발생지의 인덱스 찾기: [문제 링크](https://leetcode.com/problems/find-the-index-of-the-first-occurrence-in-a-string/description/)\n- 최대 연속 1 개: [문제 링크](https://leetcode.com/problems/max-consecutive-ones/description/)\n- 연속 배열: [문제 링크](https://leetcode.com/problems/contiguous-array/description/)\n- 주식 매수 및 매도 최적 시기: [문제 링크](https://leetcode.com/problems/best-time-to-buy-and-sell-stock/description/)\n- 쌍으로 노드 교환: [문제 링크](https://leetcode.com/problems/swap-nodes-in-pairs/)\n- 두 수 II: [문제 링크](https://leetcode.com/problems/two-sum-ii-input-array-is-sorted/description/)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 문제들을 연습함으로써, 데이터 엔지니어링 인터뷰에서 가장 일반적인 자료 구조와 알고리즘 문제에 대비할 준비가 더 잘 될 것입니다. 행운을 빕니다!","ogImage":{"url":"/assets/img/2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer_0.png"},"coverImage":"/assets/img/2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer_0.png","tag":["Tech"],"readingTime":6},{"title":"관계형 데이터베이스에서 데이터 레이크하우스로 데이터 관리의 간단한 역사","description":"","date":"2024-06-19 09:43","slug":"2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement","content":"\n\n## 데이터 관리의 진화 및 데이터 엔지니어링을 위한 솔루션\n\n비즈니스 인텔리전스와 데이터 엔지니어링은 산업 분야에서 비교적 새로운 분야입니다. 20세기 초부터 어느 정도 비즈니스 분석이 수행되어 왔습니다. 그러나 디지털 정보의 대량 분석이 필요한 건 정보 시대에만 발생한 문제입니다. 간단합니다 – 데이터를 소유(또는 수집)하고 그로부터 더 나은 통찰력을 얻는 사람들이 성공할 것입니다.\n\n데이터 엔지니어로써, 데이터 관리 접근 방식이 역사를 통해 어떻게 변화해왔는지, 데이터 처리와 관련된 문제를 어떻게 해결해 왔는지 이해하는 것은 흥미로운 일입니다. 역사에 대해 간단히 탐험해보고 데이터 관리 방식이 어떻게 시간이 지남에 따라 발전해 왔는지 알아보겠습니다.\n\n![이미지](/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 1980년대\n\n1980년대에는 대형 기업들이 관계형 SQL 데이터베이스를 사용하여 질의를 수행하여 통찰을 얻었습니다. SQL 기술이 1974년경에 처음 등장했기 때문에 이 주요 도구가 현재 50년이라는 점을 인식하는 것이 중요합니다. 기업들이 분석 질의를 위한 관계형 데이터 관리에서 제한을 경험하면서 비즈니스 데이터 웨어하우스라는 개념이 소개되었습니다.\n\n지금은 익숙하지만, 그 당시에는 이것이 상당한 혁신이었습니다. 이 접근 방식의 주요 성과는 다음과 같습니다:\n\n- 더 빠른 비즈니스 인텔리전스 (BI) 프로세스.\n- 구조화된 데이터와 효율적으로 작업할 수 있는 능력.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 주목할만한 단점들도 있었어요:\n\n- 반정형 및 비정형 데이터를 지원하지 않는 한계.\n- 규모와 속도에 대한 도전.\n- 대용량 데이터 크기를 처리할 때 장기간의 처리 시간이 발생할 수 있음.\n\n![이미지](/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_1.png)\n\n## 2000–2010년대\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2000년대에 대형 데이터가 등장하면서 전통적인 데이터 웨어하우스에 도전이 제기되었습니다. Google과 Yahoo와 같은 기업들은 구조화되지 않은 대량의 데이터를 생성했는데, 이는 구조화된 데이터 웨어하우스로는 효과적으로 처리할 수 없는 양이었습니다. 게다가 대규모의 구조화되지 않은 데이터(예: 이미지)가 필요한 머신 러닝의 등장은 기존 데이터 관리 시스템의 한계를 더욱 부각시켰습니다.\n\n이에 대응하여 데이터 레이크 개념이 소개되었는데, Google이 대형 데이터의 분산 처리를 위해 MapReduce를 선도했습니다. 이는 Hadoop의 개발로 이어지며, Hadoop 분산 파일 시스템(HDFS)을 저장 계층으로 사용하여 효율적인 데이터 저장 및 처리가 가능하게 했습니다. 이후에는 MapReduce와 이후에는 Spark를 사용하여 데이터를 처리할 수 있었습니다.\n\n데이터 레이크는 다음과 같은 장점을 제공했습니다:\n\n- 비용 효율적인 클라우드 저장소.\n- 머신 러닝(ML) 지원.\n- 유연한 데이터 저장.\n- 스트리밍 데이터 처리.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서, 테이블 태그를 마크다운 형식으로 변경하면 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2020년대\n\n관계형 데이터베이스를 사용하는 것에서 분산 저장 시스템을 활용해 대용량 데이터를 처리하는 것으로 발전했습니다. 대용량 데이터의 문제를 해결한 것 같았지만, 새로운 도전이 나타났습니다. 기업들은 여전히 데이터를 완전히 활용하기 어려워하며, 상당량의 데이터를 활용하지 못한 채 놓아두고 있습니다.\n\n2020년에 Databrick은 데이터 관리의 혁신적인 접근 방식인 데이터 레이크하우스(Data Lakehouse)에 대한 중요 논문을 발표했습니다. 이 방식은 데이터 웨어하우스와 데이터 레이크를 결합하여 클라우드 저장 서비스의 비용 효율성을 활용하면서 더 '웨어하우스'적인 방식으로 운영합니다. 데이터 레이크하우스는 BI 도구를 활용하고 데이터 과학/머신러닝 솔루션에 의존하는 대기업들에게 특히 유용합니다.\n\n이 접근을 옹호하는 회사가 Databricks이지만, 대부분의 도구가 오픈 소스이기 때문에 클라우드에서 이러한 솔루션을 독립적으로 구축하는 것도 가능합니다. 데이터 레이크하우스 시스템을 다룰 때는 저장 수준에서 적절한 기술을 갖추는 것이 중요합니다. 이러한 시스템은 table formats으로 언급되며, Apache Iceberg, Delta Lake, Apache Hudi와 같은 가장 일반적으로 사용되는 형식들이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기타 측면에서는 이전 솔루션과 유사하며 변환을 위해 Spark를 사용하는 것이 일반적입니다. 그러나 구체적인 툴킷은 회사마다 및 서로 다른 클라우드 제공업체 사이에 상당히 다를 수 있습니다. 데이터 레이크하우스는 지속적으로 발전하고 있으며 매년 새로운 기술이 등장하여 대규모 데이터를 처리하기가 더 쉬워지고 있습니다.\n\n![image](/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_3.png)\n\n# 결론\n\n요약하자면, 데이터 관리 솔루션의 진화에 대한 완전한 여정을 완료했습니다, 주로 분석을 위한 것입니다. 모든 접근 방식에는 고유의 사용 사례가 있으며 전 세계의 기업 및 팀에서 적극적으로 활용되고 있습니다. 내 의견으로는 데이터 웨어하우스가 가장 이해하기 쉬운 것으로, SQL 관계형 데이터베이스와 매우 유사합니다. 그러나 데이터 레이크하우스도 SQL을 언어 중 하나로 사용하지만 기술은 더 복잡합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n선택을 할 때는 다음 질문을 고려해 보세요:\n\n- 회사에서 개발 중인 ML 및 AI 도구가 있나요? 이들에게 데이터를 제공해야 할 필요가 있나요? 그렇지 않다면, 구조화된 데이터는 문제 해결에 충분하며 데이터 웨어하우스 솔루션을 사용할 수 있습니다.\n- 데이터의 크기를 고려해 보세요. 데이터 웨어하우스에서 저장 공간은 비실할 수 있습니다.\n- 마지막으로, 데이터 팀의 품질도 중요한 문제입니다. 데이터 레이크하우스를 사용하는 경우, 비용이 더 많이 드는 데이터 엔지니어 몇 명이 필요합니다.\n\n## 개인적인 반성:\n\n데이터 관리의 진화에 대한 간략한 탐험을 통해 다양한 접근 방법에 대해 더 잘 이해하실 수 있게 되었다고 믿습니다. 이 정보는 해결책을 비교하고 문제에 적합한 것을 선택할 때 필수적입니다.","ogImage":{"url":"/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_0.png"},"coverImage":"/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_0.png","tag":["Tech"],"readingTime":4},{"title":"높은 영향력을 가진 데이터 거버넌스 팀","description":"","date":"2024-06-19 09:40","slug":"2024-06-19-High-impactdatagovernanceteams","content":"\n\n## 영향을 주는 방법, 초점을 맞출 곳, 최고의 데이터 관리 팀에서 성공하기 위해 필요한 기술\n\n고영향력을 지닌 데이터 관리 팀에 대한 가장 좋은 비유는 원활하게 작동하는 주방과 유사합니다. 그들은 주방을 깨끗하게 유지하고 모든 칼이 날카롭고 모든 물건이 제 위치에 있도록 돕습니다. 이는 요리사들이 더 빨리 작업하게 하고 실수를 줄이며 나쁜 식품 위생 평가를 방지합니다.\n\n![고영향 데이터 관리 팀](/assets/img/2024-06-19-High-impactdatagovernanceteams_0.png)\n\n하지만 데이터 관리 팀은 선을 넘지 않아야 합니다. 조심하지 않으면 이들의 노력은 추가 부담처럼 보일 수 있고, 사업에 미미한 영향을 가져올 수 있는 정책과 프로세스를 도입할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 고영향 데이터 거버넌스 팀이 주로 초점을 맞춰야 할 영역 및 취해야 할 구체적인 단계를 살펴봅니다.\n\n![이미지](/assets/img/2024-06-19-High-impactdatagovernanceteams_1.png)\n\n데이터 거버넌스 팀이 어떻게 작동해야 하는지에 대해 일반화된 해결책은 없지만, 이러한 주요 주제들은 대부분의 주요 기업 채용 공고에서 요구 사항으로 계속해서 나타납니다.\n\n- 조직 전체에서 데이터 활용 가능하게 만들기\n- 데이터 품질을 유지하고 최우선 사항을 체계적으로 개선하는 데 도움\n- 소유권 모델을 통해 책임이 명확하게 보장\n- 리스크, 개인 정보 보호 및 규정 준수 관리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 조직 전체에서 데이터를 활용할 수 있도록\n\n문제는 거의 결코 데이터가 충분하지 않다는 것이 아닙니다. 오히려 기존 데이터가 활용되지 않거나 사람들이 찾을 수 없다는 문제입니다. 조직이 성장함에 따라 생성되는 데이터량은 상당히 증가합니다. 시간이 지남에 따라 이에는 여러 가지 영향이 있습니다.\n\n이것은 천천히 발생하다가 갑자기 전개됩니다.\n\n팀 간의 간헐적인 노력으로는 이 문제를 해결하기 어렵습니다. 데이터 거버넌스 팀은 이를 표준화하고 촉진하는 데 도움을 줄 수 있는 독특한 위치에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 중요한 데이터 자산 문서화\n\n데이터 관리 팀은 데이터 문서화에 대한 기대치를 설정하고 시행하여 후순위가 되지 않도록 해야 합니다. 예를 들어:\n\n- 데이터 카탈로그나 dbt yml 파일에 문서 작성이 이루어지도록 강제합니다. 이를 통해 팀간에 일관된 접근 방식을 구축할 수 있습니다.\n- 문서화는 ‘골드’ 표준 테이블에 필수로 이루어져야 하지만 ‘브론즈’와 ‘실버’는 선택 사항임을 명확히 합니다.\n- 잘 문서화된 테이블이나 필드의 명확한 예시를 제시합니다 (예: ‘세일즈포스 식별자에 의해 정의된 고객의 사용자 ID’와 같이 작성하며, ‘고객들의 사용자 ID’ 등과 같이 작성하지 않습니다).\n\n## 핵심 지표가 일관된 방식으로 정의되도록 보장하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떤 규모에 도달하면, 동일한 지표에 대한 서로 다른 정의를 반드시 마주하게 됩니다. 기본 데이터가 잘못되었다고 할지라도, 이는 의사 결정이 더뎌지고, 미팅에서는 지표가 올바른지 논의하는 데 시간을 보내게 만듭니다. 그 대신에 이에 대해 어떻게 대처할지에 대해 논의할 시간이 생깁니다.\n\n데이터 가버넌스 팀은 명확한 지침을 설정해야 합니다. 그러한 지침 중 하나는 다음과 같을 수 있습니다: \"주요 지표는 가시성을 향상하고 버전 관리하는 데 dbt에서 정의되어야 하며 BI 도구의 계산된 필드가 아니어야 합니다.\" 다른 지침으로는 주요 지표에 RAG(빨강, 주황, 초록) 임계값을 메트릭 정의와 함께 포함시켜 이를 한 곳에서 유지할 수 있도록 하는 것이 있습니다. 이는 해당 임계값이 위반될 경우 관련 이해 관계자에게 자동 경고를 설정할 수 있다는 장점이 있습니다.\n\n데이터 가버넌스 팀은 동일한 지표에 대한 다양한 정의를 찾고, 여러 정의를 발견하면 팀에 이를 통합하도록 장려해야 합니다. 이는 핵심 대시보드, Slack 채널 및 KPI가 사용되는 다른 위치를 정기적으로 평가함으로써 가장 잘 수행됩니다.\n\n## 데이터 검색 가능하게 하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 데이터 이용자들이 — 데이터 팀 내에서든 외부에서든 — 자신의 데이터를 찾지 못한다면, 그 데이터는 사실상 존재하지 않는 것과 다름없습니다. 필요한 데이터를 찾는 것이 마치 건초더미 속에서 바늘을 찾는 것처럼 느껴진다면, 문제의 일부 해결책은 툴셋을 업데이트하고 데이터 카탈로그를 도입하여 모든 데이터 자산을 발견할 수 있게 하는 것일 수 있습니다. 하지만 기본을 올바르게 설정하지 않는다면 어떤 도구도 도움이 되지 않을 것입니다.\n\n데이터 거버넌스 팀은 간단하지만 표준화된 규칙을 시행해야 합니다. 예를 들어, 데이터 자산에 대한 명확한 명명 규칙을 정하면 좋습니다. 대시보드의 경우 '⭐️ 제품 A/B 테스트 추적 [사업용 은행]'과 같이 명명 규칙을 사용하여 해당 대시보드가 중요(⭐️)하다는 것과 어떤 도메인에 속하는지 명확히 할 수 있습니다 (사업용 은행).\n\n사용 중인 데이터가 있는 곳에 가능한 가까이 위치시키는 것이 좋습니다. Looker와 같은 BI 툴을 사용하면 대부분의 데이터 이용자가 데이터를 소비할 수 있습니다. 각 도메인으로 진입할 수 있는 잘 구조화된 '홈페이지'를 만들어서 가장 중요한 대시보드에 대한 링크를 제공하는 것은 대시보드를 조직화하고 모든 사람이 쉽게 찾을 수 있도록 하는 좋은 방법입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 대시보드의 사용성 강화\n\n당신이 좋아하든 싫어하든, 얼마나 멋진 데이터 모델과 파이프라인을 만들든간에, 이를 소비하는 이해관계자들의 대시보드 경험은 데이터와 당신의 팀의 산출물에 대한 인식에 큰 영향을 미칠 것입니다. 대시보드를 제품처럼 다루어 외관, 느낌, 일관성, 그리고 성능을 강화해야 합니다.\n\nTypeform은 Looker의 개선 사항을 정리하면서 추가 단계를 거쳤습니다. 이들이 어떻게 그것을 수행했는지 여기에서 자세히 읽어보세요.\n\n![이미지](/assets/img/2024-06-19-High-impactdatagovernanceteams_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 불필요한 데이터 자산 제거\n\n데이터 자산의 확산은 데이터가 시간이 지남에 따라 사용하기 어려워지는 주요 이유 중 하나입니다. 데이터 스택을 정리하는 가장 효과적인 방법은 필요하지 않은 데이터 자산을 제거하는 것입니다. 필요하지 않은 데이터 자산의 명확한 정의는 없지만, 주의해야 할 신호는 다음과 같습니다.\n\n- 사용률이 낮은 대시보드\n- 다른 대시보드와 크게 중첩된 대시보드\n- 하류 종속성이 없는 데이터 모델\n- 다운스트림에서 사용되지 않는 데이터 모델의 열\n\n불필요한 데이터 스택을 정리하는 것은 쉬운 작업이 아니며, 가능하다면 사용되지 않는 테이블, 데이터 모델, 열 및 대시보드를 계속해서 제거하는 것에 투자해야 합니다. 그러나 많은 확장 중인 기업은 천 개 이상의 데이터 모델과 대시보드가 생기기 전까지는 뒷걸음치기를 하다가 이 문제에 대처해야 하는 경우가 많습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요번에 전해드릴 이야기는 Looker 대시보드의 수를 급격히 줄이고 자가 서비스를 거의 불가능하게 만들었던 회사의 이야기입니다.\n\n- 데이터 팀은 Looker 사용량을 위한 Looker 대시보드를 만들고 매월 #data-team 슬랙 채널로 보고서를 보냈습니다 (Looker를 통해 시스템 활동 탐색에서 이 데이터에 액세스할 수 있습니다). 보고서에는 a) 대시보드 수, b) 조회 수, c) 사용자 수, d) 임원 (VP/C-레벨) 사용자 수가 포함되어 있었습니다.\n- 대시보드를 제작한 도메인과 사용량으로 정렬할 수 있는 기능을 통해 사용하지 않는 대시보드와 삭제 가능한 대시보드 소유자를 빠르게 확인할 수 있었습니다.\n- 팀은 \"계층별 대시보드\" 개념을 도입했습니다. 계층 1 대시보드의 예시로는 보드 수준의 KPI 대시보드가 있을 수 있고, 계층 3 대시보드는 소규모 제품 팀용일 수 있습니다. 계층 1 대시보드에 대해 디자인 일관성, 피어 리뷰 요구 사항 및 사용 모니터링의 구체적인 요구 사항을 정했습니다.\n- 임원 사용자가 정기적으로 계층 1이 아닌 대시보드를 사용할 경우, 해당 대시보드는 계층 1 표준으로 검토되었습니다.\n\n이와 비슷한 단계를 데이터 모델을 정리하는 데 사용할 수 있습니다. 특히 dbt에서 잘 구성된 열 수준의 계보가 있는 경우 팀이 크게 혜택을 입었습니다. 이는 dbt에서 비즈니스 인텔리전스 도구까지 연장되는 정확하고 빠른 후속 영향을 평가하는 데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 중요한 데이터를 정의하세요\n\n규모에 관계없이, 데이터 거버넌스 팀의 의도가 얼마나 좋더라도 모든 데이터에서 높은 사용성을 시행하는 것은 불가능해집니다. 데이터 거버넌스 팀은 데이터 중요성이 어떻게 정의되어야 하며 이에 대한 영향이 무엇인지 명확하게 설정하는 것이 중요합니다. 예를 들어, '브론즈, 실버, 골드' 모델을 사용하고 실버 및 골드만 문서화 표준을 준수해야 한다고 결정할 수 있습니다.\n\n이 연습을 완료하는 동안 데이터를 체인으로 고려하고 하류 사용 사례에 되돌아갈 수 있도록하며 중요합니다.\n\n![Image](/assets/img/2024-06-19-High-impactdatagovernanceteams_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당신의 비즈니스 중요 데이터를 식별하는 방법을 안내하는 가이드를 읽어보세요. 비즈니스 중요 데이터 모델과 대시보드를 식별하는 실용적인 단계와 데이터에 대한 신뢰를 증진하는 방법을 알려드립니다.\n\n데이터 품질을 주의 깊게 살피면서 중요한 것을 체계적으로 개선하는 데 도움이 됩니다.\n\n데이터 팀이 직면하는 가장 큰 어려움으로 일관적으로 순위에 있던 것이 데이터 품질입니다. dbt는 최근 수천 명의 데이터 전문가들에게 최대 어려움에 대해 물었고, 57%의 표를 받아 데이터 품질이 가장 중요한 문제로 떠오르는군요.\n\n이에는 좋은 이유가 있습니다. 낮은 데이터 품질은 부정확한 결정, 시스템 장애 및 데이터에 대한 신뢰 손실로 이어질 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다행히 대부분의 데이터 전문가들은 검증된 데이터가 데이터 자산을 개발하는 일부분이라는 것에 익숙해졌습니다. 개별 자산에 적합하지만 개인의 해석에 많은 부분을 맡길 수 있습니다. 우리는 데이터 거버넌스 팀이 소유자 도메인 및 중요성과 같은 주요 차원을 횡단하여 데이터 품질을 체계적으로 평가하는 것을 권장합니다.\n\n## 데이터 품질 보고\n\n데이터 품질에 대한 보고를 하는 이유는 많지만 각 팀에게 자체 메트릭을 정의하도록 요청하는 것은 혼란을 초래하는 확실한 방법입니다. 데이터 품질을 일관되게 측정하고 전달하는 것은 비즈니스 결과에 여러 이점을 가져다 줄 수 있습니다:\n\n- 가동 시간 — 각 실행에서 성공적으로 통과하는 설치된 제어의 %는 무엇인가요?\n- 커버리지 — 필요한 제어가 설치된 데이터 자산의 %는 얼마나 되나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**품질 지표를 의미론적으로 유사한 영역으로 그룹화하면 비즈니스 사용 사례에 더 가까운 방식으로 데이터 품질에 대해 이야기할 수 있습니다. (1) 시기 적절성-데이터가 비즈니스와 합의된 SLA에 따라 신선하고 최신한가, (2) 정확성-모든 데이터가 사용 가능한가, (3) 완전성-데이터가 의미론적으로 올바른가, 그리고 (4) 일관성-가용 데이터가 시스템 전반에서 일관성이 있는가  우리는 또한 \"나쁨\", \"양호\", 또는 \"좋음\"이라고 하는 것을 명확히 정의하는 것을 제안합니다. 예를 들어, 50% 미만의 점수는 \"나쁨\"으로 표시되어 조치가 필요함을 의미합니다.**\n\n![이미지](/assets/img/2024-06-19-High-impactdatagovernanceteams_6.png)\n\n**이 수준의 통찰력을 통해 데이터 품질에 관한 질문을 시작할 수 있습니다. 예를 들어**\n\n**널 값 확인에 대한 좋은 커버리지를 가졌지만, 어떤 것이 실패하는지 조사하고, 고칠 수 있는지, 그리고 제거해야 하는지를 고려해야 합니다.**\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리의 어설션 테스트의 가동 시간이 크게 감소한 이유는 무엇일까요?\n\n저희가 일치성 테스트의 높은 가동 시간을 유지하고 있지만, 커버리지가 낮기 때문에 안전한 느낌을 준다고 생각하십니까?\n\n데이터 품질을 측정하는 방법에 대한 더 많은 지침은 \"데이터 품질 측정: 이론을 실무로\"라는 가이드를 참조해주세요.\n\n## 데이터 품질 기대치를 위한 가이드라인 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대부분의 상황에서는 데이터 사용 방식에 따라 기대 값을 명시하는 것이 좋습니다. 모든 자산에 동일한 기준을 적용하길 원하지 않을 수 있으므로 이를 권장합니다. 마찬가지로 ‘gold’ 자산에만 문서화를 강제할 수 있듯이, 데이터 거버넌스 팀은 데이터 테스트에 대한 기대 사항을 설정하는 역할을 해야 합니다. 비즈니스에 중요한 데이터, 노출되는 데이터 (예: 대시보드에 표시되는 데이터) 및 SLA(데이터가 제 시간에 도착하지 않으면 하류 영향이 있는 경우)를 고려하여 필요한 체크 항목을 평가하고 중요한 것을 측정할 수 있도록 돕습니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-High-impactdatagovernanceteams_7.png\" /\u003e\n\ndbt yml 파일은 이 메타데이터를 정의하기에 좋은 장소입니다. 이를 통해 사유 테스트(예: 사전 커밋 dbt 패키지에서 제공하는 check-model 태그)를 사용하여 각 데이터 모델이 필요한 중요도나 도메인 소유자 태그와 같은 메타데이터를 가지고 있는지 확인할 수 있습니다.\n\n# 수행 책임이 분명한 소유권 모델을 통해 책임을 보장하세요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 스택이 복잡해지면 한 사람이 모든 것을 머릿속에 간직하기 어려워집니다. 또한 문제를 발견한 사람이 그 문제를 해결할 적절한 사람이 아닌 경우가 더 많아집니다. 동시에 상하위 종속성의 수는 급증하여 적절한 상위 소유자를 찾거나 영향을받는 이해관계자에게 알릴 때 어려움을 겪게 됩니다.\n\n많은 데이터 팀에게 물어보지 않아도 꿈 같은 상황을 이해할 수 있습니다: 상위 생성자가 자신의 데이터의 품질을 소유하고 관리하며, 관련 데이터 팀이 책임을 질 수 있고, 이해관계자가 문제를 발견하는 날이 지나간 세상입니다.\n\n좋은 소유권은 말로만 하기 쉽지만, 실패한 소유권 이니셔티브가 부족하지 않습니다.\n\n데이터 거버넌스 팀은 명확히 정의된 역할과 책임을 갖는 일관된 소유권 모델을 구축하는 데 궁극적으로 책임이 있습니다. 단계별로 나누어서 누락된 부분을 파악하기 쉽게 만들어봅시다: (1) 메타데이터를 통합하고, (2) 관련 테스트로 문제를 감지하며, (3) 소유권을 할당하고, (4) 관련 인물에게 문제를 통지하여 그들이 대처할 수 있는 방법으로 행동을 취하도록 합시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![High impact data governance teams](/assets/img/2024-06-19-High-impactdatagovernanceteams_8.png)\n\n소유권은 기술적인 도전과 문화적인 측면 두 가지를 모두 고려해야 하는 문제입니다. 소유권 프로젝트를 성공적으로 이끌어내려면 두 측면에 모두 초점을 맞춰야 합니다.\n\n- 소유자에 대한 기대 설정 — 기대는 데이터 자산의 중요도와 연결되어야 합니다. 예를 들어, 무엇이 높은 심각도의 문제를 만들며 그 결정은 누가 하는지를 알아봅니다. 자세한 내용은 데이터 문제의 심각성 수준 설계를 시작하는 방법에서 확인할 수 있습니다.\n- 소유권 정의 — 소유권을 정의할 수 있는 장소는 dbt yml 파일부터 데이터 카탈로그, Confluence 페이지, 스프레드시트까지 다양합니다. 어디에 소유권을 정의해야 하는지 명확히 하고 모두가 동일한 방식으로 수행할 수 있도록 도와주세요.\n- 올바른 상황에서 올바른 사람에게 통보 — 데이터 소유권을 종합적으로 고려하길 권장합니다 — 상위팀이 소유한 데이터 소스부터 최종 사용자가 소유한 대시보드까지. 간단히 말해 우리의 권장사항을 다음 그룹으로 분류해보겠습니다: (1) 데이터 팀, (2) 상위팀, (3) 비즈니스 이해관계자.\n\n![High impact data governance teams](/assets/img/2024-06-19-High-impactdatagovernanceteams_9.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4. 문화 소유권 도전 극복하기 — 소유자로 할당받는 것과 책임을 져야 하는 것은 다릅니다. 소유권을 성공적으로 이행하는 것은 기술적인 도전만큼 문화적인 도전입니다. 사람들에게 영향을 미치고 책임을 지게 하는 것은 데이터 관리 팀에 대한 큰 이득입니다. 소유권에 대한 행동을 촉진하고 적절한 분야를 찾아 소유권을 정의함으로써 초기 승리를 증명하는 것은 기술적 구현만큼 중요합니다.\n\n우리의 가이드인 데이터 소유권: 현실적인 가이드를 읽어보세요. 데이터 팀, 상위 팀 및 비즈니스 이해자에게 소유권을 정의하고 활성화하기 위한 전체 도구 모음을 제공합니다.\n\n# 리스크, 개인정보 보호 및 규정 준수 관리\n\n회사의 수명주기에서 특정 시점, 일반적으로 데이터 관리 팀이 참여될 때, 관리해야 할 중요한 리스크가 있습니다. 이는 핀테크를 위한 규정 데이터, 곧 예정된 IPO를 위한 금융 데이터의 정확성, 또는 회사가 개인 식별 정보 데이터를 책임적으로 처리해야 한다는 일반적인 인식과 관련이 있을 수 있습니다.  \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당신의 역할은 데이터 거버넌스 전문가로 명확합니다: 회사가 의무를 준수하고 리스크를 최소화하면서도 데이터 팀의 업무를 느리게 만들지 않도록 보장하는 것입니다.\n\n다음은 이를 수행할 수 있는 몇 가지 방법입니다.\n\nPII 관련 데이터 – 대부분의 회사에서 사용자 이메일과 같은 일부 데이터는 모든 사람이 쉽게 쿼리할 수 없어야 합니다. 데이터가 PII로 태그되고 이 데이터 주변에 가드레일을 자동으로 시행하도록 강요하세요. 예를 들어, 접근 권한이 사용 사례별로 만 부여되고, 예를 들어 7일 후에 만료되는 원시 데이터가 있는 별도의 데이터 웨어하우스를 만드세요.\n\n사용자 데이터 삭제 요청 – 사용자 데이터를 처리하면 결국 사용자 데이터 삭제 요청을 만나게 될 것입니다. 이에 대해 조속히 생각할수록 처음 요청이 들어왔을 때 모든 사용자 데이터를 삭제하는 것이 더 쉬워집니다. 이러한 사고방식이 발생하기 전에 열어둔 적절한 도구, 예를 들어 열 수준의 계보,는 걸어두면 소요 시간이 크게 줄어듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인시던트 관리 프로세스 및 SLA 정의 - 어떤 소유권 모델이나 데이터 품질 점검을 하더라도 데이터 문제는 발생할 수 있습니다. 잘 정의된 인시던트 관리 프로세스에는 여러 가지 이점이 있습니다: 모두가 중요한 것에 대해 공유된 이해를 구축하는 데 도움이 되며, 문제에 대한 어떤 맥락도 없는 사람들을 쉽게 참여시킬 수 있습니다. 또한 지난 인시던트를 쉽게 되돌아보고 중요한 위반 사항에 대해 보고할 수 있습니다.\n\n인시던트 관리 테이블을 마크다운 형식으로 변경하십시오.\n\n# 데이터 가버넌스 전문가로서 탐색해야 할 핵심 기술\n\n데이터를 사용 가능하게 만들기, 품질을 보장하기, 소유권 구축하기, PII 및 리스크를 속도와 균형있게 조절하기 - 이것은 데이터 가버넌스 팀에게는 작은 일이 아닙니다! 데이터 가버넌스 전문가가 성공하기 위해 필요한 필수 기술은 아래와 같습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기술적 이해력 - 코드 기반에 기여하지 않고 직접 dbt 모델을 작성하지는 않더라도 데이터 팀이 사용하는 도구 및 제한 사항을 고수준으로 이해하면 유익합니다. 이렇게 하면 다른 팀의 고민을 더 잘 이해할 수 있으며 데이터 거버넌스 프로세스가 기존 워크플로에 어떻게 맞는지의 장단점을 파악할 수 있습니다.\n\n우선순위 균형 유지 - 모든 것이 항상 중요하다는 느낌일 수 있습니다. 그럼에도 불구하고 특정 이니셔티브에 우선순위를 두는 회사 전체의 상황을 주시하는 것이 여러분의 역할입니다. 예를 들어, 곧 예정된 IPO나 규정 위반과 같은 경우, 재무 데이터에 중점을 두는 것이 좋을 수 있으며 마케팅 및 제품 분야에는 덜 주의를 기울일 수 있습니다.\n\n공급업체 선정 프로세스 운영 - 위의 문제를 해결하는 데 도움이 되는 적어도 몇 가지 도구를 도입하고 싶을 것입니다. 데이터 카탈로그 및 데이터 관찰 도구와 같은 카테고리의 도구들에 주목해야 합니다. 도구에 투자를 결정하면, 잘 구조화된 프로세스를 운영하고 컨셉 증명을 계획하며 데이터 팀 구성원 모두가 주장을 듣게 해야 합니다. 회사가 이 카테고리의 도구를 처음 구매하는 경우일 수 있으므로 여러 개의 데모를 받고 기존 고객들과의 참조 검사를 진행하여 도구가 여러분에게 적합한지 확인하는 좋은 방법일 수 있습니다.\n\n조직적 지원 확보 - 누구도 즐거움을 위해 데이터 품질, 문서화 또는 소유권에 관심을 갖지 않습니다. 데이터 거버넌스 팀으로서 문서화, 데이터 품질 및 소유권이 왜 중요한지에 대한 꿈을 판매해야 합니다. 리더십 팀과 정기적으로 인사이트를 공유하는 것은 그들이 이에 투자해야 할 가치를 파악하고 게임에 참여하도록 하는 좋은 방법일 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 모두 모아보기\n\n데이터 거버넌스 팀의 역할은 부엌을 청소하고 칼이 날카로운지 확인하며 모든 것이 제자리에 있는지 확인하는 것입니다.\n\n이겈하는 것은 쉽지 않은 작업이며 우선 순위를 조정하고 이해 관계자를 연루시키며 벤더 선택 프로세스를 실행하는 것을 포함합니다. 이를 잘 수행할 때의 좋은 예로는 덴마크 핀테크 루나(Lunar)의 경우가 있습니다. 루나는 소유권부터 중요도 및 모니터링에 이르기까지 데이터 거버넌스 프레임워크를 성공적으로 롤아웃하고 C-레벨의 찬성을 받았습니다. 자세한 내용은 이곳에서 데이터 거버넌스를 프레임워크로 구축하는 데 영감을 얻어보십시오.\n\n요약:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 조직 전반에 걸쳐 데이터를 사용할 수 있도록 합니다 — 데이터가 잘 문서화되어 있고 주요 지표가 일관되게 정의되어 있으며 주요 데이터 자산이 발견 가능하며 대시보드가 잘 보이고 느껴지고 잘 수행될 수 있도록 합니다.\n- 데이터 품질을 계속해서 확인하여 가장 중요한 부분을 체계적으로 개선하도록 지원합니다 — 데이터 거버넌스 팀이 데이터 테스트를 작성하지는 않겠지만, 데이터 품질을 개선하도록 기대치를 강조하고 보고할 수 있습니다.\n- 소유권 모델을 통해 책임이 명확하게 보장합니다 — 소유권에 대한 주요 측면들, 소유권이 어떻게 정의되고 실행되며 각 팀이 자체적인 해석을 하지 않도록 데이터 거버넌스 팀이 중앙에서 정의해야 합니다.\n- 위험, 프라이버시 및 규정 준수를 관리합니다 — 종종 매력적이지 않은 분야로 여겨지지만, PII 데이터에 대한 불명확한 프로세스, 사용자 삭제 요청 및 사건들은 나중에 큰 문제를 일으킬 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-High-impactdatagovernanceteams_0.png"},"coverImage":"/assets/img/2024-06-19-High-impactdatagovernanceteams_0.png","tag":["Tech"],"readingTime":12},{"title":"유튜브 트렌드 분석 파이프라인 Airflow, Spark, S3 및 Docker를 이용한 ETL","description":"","date":"2024-06-19 09:37","slug":"2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker","content":"\n\n이 기사에서는 Apache Airflow와 PySpark를 사용하여 자동 ETL (추출, 변환, 로드) 파이프라인을 만드는 방법을 안내합니다. 이 파이프라인은 YouTube Data API에서 트렌드 비디오 데이터를 가져와 처리한 후 처리된 데이터를 S3에 저장할 것입니다.\n\nTwitter API를 사용한 파이프라인을 보여주는 Darshil Parmar의 YouTube 비디오를 시청한 후, 유사한 프로젝트에 도전하기로 영감을 받았습니다. 그러나 Twitter API의 가격 정책 변경으로 인해, 시청자가 YouTube Data API를 대체로 제안했고 이것이 제 흥미를 자극했습니다.\n\n프로젝트에 돌입하기 전에 두 가지 필수 사항이 있습니다:\n\n1. Youtube Data API 키 획득\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Google Developers Console을 방문해 주세요.\n- 새 프로젝트를 생성해 주세요.\n- \"YouTube Data API\"를 검색하고 활성화해 주세요.\n- 새 자격 증명을 생성하고 프로젝트에서 나중에 사용할 API 키를 복사해 주세요.\n\n자세한 지침은 YouTube Data API 시작 가이드를 참조해 주세요.\n\n2. AWS 액세스 키 ID 및 비밀 액세스 키 획득\n\n- AWS Management Console에 로그인해 주세요.\n- IAM(Identity and Access Management) 섹션으로 이동하고 새 사용자를 생성해 주세요.\n- 필요한 S3 액세스 정책을 부여하고 액세스 키를 생성해 주세요.\n- 프로젝트에서 사용할 액세스 키 ID와 비밀 액세스 키를 안전하게 저장해 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 실제 프로젝트를 시작하겠습니다! 준비됐나요 여러분!!\n\n![YouTube Trend Analysis Pipeline ETL with Airflow, Spark, S3, and Docker](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png)\n\n이 글은 4가지 주요 단계로 구성되어 있어요:\n\n- 소프트웨어 설치 및 설정\n- Youtube Data API에서 데이터 추출\n- PySpark를 사용하여 데이터 변환\n- AWS S3로 데이터 로드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 1. 소프트웨어 설치 및 설정:\n\n- VS Code — [VS Code 다운로드 및 설치](https://code.visualstudio.com/).\n- Docker Desktop — [Docker Desktop 다운로드 및 설치](https://www.docker.com/products/docker-desktop).\n- (선택사항) Windows Subsystem for Linux (WSL) — 데이터 엔지니어링에 사용되는 Apache Airflow 및 PySpark와 같은 많은 도구 및 라이브러리가 Unix 계열 시스템을 위해 개발되었습니다. 이러한 도구를 Windows에서 사용할 때 발생할 수 있는 호환성 문제를 피하기 위해 WSL을 통해 네이티브 Linux 환경에서 실행할 수 있습니다.\n  - ` 관리자 권한으로 PowerShell을 엽니다.\n  - ` 다음 명령을 실행하세요: wsl --install.\n  - ` 명령에 따라 WSL을 설치하고 Microsoft Store에서 Linux 배포판(예: Ubuntu)을 선택하세요.\n  - ` Linux 배포판에 사용자 이름 및 암호를 설정하세요.\n\n이 프로젝트를 실행하는 데 WSL이 반드시 필요한 것은 아닙니다. Docker Desktop은 Windows에서 네이티브로 실행될 수 있으며 Docker 자체가 관리하는 가벼운 Linux 가상 머신(VM)을 사용합니다. 그러나 Docker Desktop과 함께 WSL을 사용하면 Windows에서 직접 Linux 명령 및 작업을 실행할 수 있어 보다 네이티브한 개발 경험을 제공합니다.\n\n이제 설정을 시작해 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n부분 1 — 도커 이미지 만들기\n\n- 프로젝트용 새 폴더를 만들고 \"Airflow-Project\"로 이름을 지어주세요.\n- 해당 폴더에서 명령 프롬프트를 엽니다.\n- 명령 프롬프트에서 아래 명령을 실행하세요:\n\n```bash\ncode .\n```\n\n- 이 명령은 VS Code에서 해당 폴더를 프로젝트로 엽니다.\n- VS Code에서 \"dockerfile\"이라는 새 파일을 만들고 아래 코드를 붙여넣으세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nFROM apache/airflow:latest\n\n# 시스템 종속성을 설치하기 위해 루트 사용자로 전환합니다\nUSER root\n\n# git, OpenJDK를 설치하고 apt 캐시를 정리합니다\nRUN apt-get update \u0026\u0026 \\\n    apt-get -y install git default-jdk \u0026\u0026 \\\n    apt-get clean \u0026\u0026 \\\n    rm -rf /var/lib/apt/lists/*\n\n# Python 패키지를 설치하기 위해 airflow 사용자로 전환합니다\nUSER airflow\n\n# 필요한 Python 패키지를 설치합니다\nRUN pip install --no-cache-dir pyspark pandas google-api-python-client emoji boto3\n```\n\n이 Docker 파일은 프로젝트를 실행하는 데 필요한 모든 패키지를 포함하고 있어요.\n\n- 파일을 마우스 오른쪽 버튼으로 클릭하고 VS Code에서 \"이미지 빌드\" 옵션을 선택하세요. 이름을 입력하라는 프롬프트가 나타나면 \"airflow-project\"를 입력하세요. 이 명령은 Docker 이미지를 생성합니다. 그러나 이미지를 사용하려면 docker-compose.yml 파일을 생성하고 이미지를 사용하도록 구성해야 합니다.\n\n(재미있는 사실: 파일에서 Python 설치가 없는 이유 궁금하신가요? 실제로 Dockerfile에서 사용된 기본 이미지인 apache/airflow:latest에는 Python이 이미 설치되어 있어요. 왜냐하면 Airflow 자체가 Python으로 작성되어 있기 때문에 주로 워크플로 및 작업 정의에 Python을 사용합니다. 따라서 Dockerfile에서 별도로 Python을 설치할 필요가 없답니다!)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파트 2 — 도커 컴포즈 파일 생성하기\n\n도커 컴포즈를 사용하면 멀티 컨테이너 도커 애플리케이션을 쉽게 다룰 수 있습니다. 이를 통해 단일 명령으로 여러 도커 컨테이너를 정의하고 실행할 수 있으며 각 서비스의 환경 변수, 볼륨, 포트 및 기타 설정을 명확하고 조직적인 방식으로 구성할 수 있습니다. 도커 컴포즈를 사용하면 단일 명령어인 docker-compose up 또는 docker-compose down을 사용하여 여러 서비스를 쉽게 시작, 중지 및 관리할 수 있습니다.\n\n- \"docker-compose.yml\" 파일을 생성하고 다음 코드를 파일에 붙여넣습니다:\n\n```js\nversion: '3'\nservices:\n\n  airflowproject:\n    image: airflow-project:latest\n    environment:\n      - AWS_ACCESS_KEY_ID=your-aws-access-key\n      - AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key\n      - YOUTUBE_API_KEY=your-youtube-api-key\n    volumes:\n      - ./airflow:/opt/airflow\n    ports:\n      - \"8080:8080\"\n    command: airflow standalone\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 이제 파일을 마우스 오른쪽 버튼으로 클릭한 후 VS Code에서 'Compose Up' 옵션을 선택하세요. 환경을 설정하기 위해 클릭하세요.\n- 깜짝 놀랄 일이 벌어졌어요! 이 작업을 완료한 후에는 VS Code 프로젝트 디렉토리에 \"airflow\"라는 새 폴더가 나타날 수 있습니다.\n\nDocker 데스크톱을 열어서 모든 것이 올바르게 완료되었는지 확인하세요. 올바르게 완료된 경우 다음과 같은 화면이 표시됩니다.\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_1.png)\n\n- 이제 Airflow 프로젝트를 클릭하여 Airflow가 8080 포트에서 실행 중임을 나타내는 로그가 표시되는 화면을 엽니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_2.png)\n\n- 포트를 클릭하면 Airflow 로그인 페이지로 이동합니다. 이 링크를 처음 열어보는 경우 자격 증명을 제공해야 합니다.\n- 사용자 이름은 \"admin\"이고 비밀번호는 compose up 명령을 실행한 후 생성된 Airflow 폴더 내의 \"standalone_admin_password.txt\" 파일에 있습니다.\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_3.png)\n\n- 로그인 페이지에서 자격 증명을 입력한 후, 로컬 호스트에서 Airflow가 실행 중인 것을 확인할 수 있습니다. 다음과 같이 나타납니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_4.png\" /\u003e\n\n당신의 환경 설정 완료입니다! 휴―!!\n\n# 2. YouTube 데이터 API에서 데이터 추출하기:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Airflow 폴더 아래에 \"dags\"라는 이름의 폴더를 만들고, dags 폴더 아래에 \"youtube_etl_dag.py\"라는 파이썬 파일을 만듭니다.\n- 이제 \"youtube_etl_dag.py\" 파일에 다음을 import하세요.\n\n```js\nimport logging\nimport os\nimport re\nimport shutil\nfrom datetime import datetime, timedelta\n\nimport boto3\nimport emoji\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_date, udf\nfrom pyspark.sql.types import (DateType, IntegerType, LongType, StringType,\n                               StructField, StructType)\n\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n```\n\n- 이 프로젝트를 실행하는 데 위의 모든 라이브러리가 필요합니다(코드 작성을 시작하면 모두 유용해집니다)\n- VS Code에서 오류가 발생하는 것을 볼 수 있습니다. 그 이유는 모든 종속성이 도커에 설치되어 있지만 로컬 머신에는 설치되어 있지 않기 때문이므로 신경 쓰지 마십시오.\n- Airflow에서 구문 오류가 있으면 화면 상단에 표시되고, 논리 오류/예외는 Airflow 로그에서 확인할 수 있습니다.\n\n```js\n# DAG와 기본 인수 정의\ndefault_args = {\n    'owner': 'airflow',  # DAG 소유자\n    'depends_on_past': False,  # 과거 DAG 실행에 종속하는지 여부\n    'email_on_failure': False,  # 실패 시 이메일 알림 비활성화\n    'email_on_retry': False,  # 재시도 시 이메일 알림 비활성화\n    'retries': 1,  # 재시도 횟수\n    'retry_delay': timedelta(minutes=5),  # 재시도 간의 지연 시간\n     'start_date': datetime(2023, 6, 10, 0, 0, 0),  # 매일 자정(00:00) UTC에 실행\n}\n\ndag = DAG(\n    'youtube_etl_dag',  # DAG 식별자\n    default_args=default_args,  # 기본 인수 할당\n    description='간단한 ETL DAG',  # DAG 설명\n    schedule_interval=timedelta(days=1),  # 일별 스케줄 간격\n    catchup=False,  # 누락된 DAG 실행을 복구하지 않음\n)\n```  \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n매일 자정(0시)에 실행되는 DAG인 'youtube_etl_dag'을 정의하고 있습니다. 이 DAG은 Airflow에서 관리 및 트리거되며, VS Code에서 별도로 실행할 필요가 없습니다. Python 파일을 업데이트하면 Airflow에서 자동으로 변경 사항을 감지하고 반영할 것입니다.\n\n현재 Airflow에는 DAG이 표시되지만 아직 정의된 작업이 없어서 어떤 작업도 표시되지 않습니다. DAG를 기능적으로 만들기 위해 데이터 추출 작업을 만들어봅시다.\n\n```js\n# YouTube API에서 데이터를 추출하기 위한 Python callable 함수\ndef extract_data(**kwargs):\n    api_key = kwargs['api_key']\n    region_codes = kwargs['region_codes']\n    category_ids = kwargs['category_ids']\n    \n    df_trending_videos = fetch_data(api_key, region_codes, category_ids)\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    # DataFrame을 CSV 파일로 저장\n    df_trending_videos.to_csv(output_path, index=False)\n\ndef fetch_data(api_key, region_codes, category_ids):\n    \"\"\"\n    YouTube API에서 여러 나라와 카테고리의 인기 동영상 데이터를 가져옵니다.\n    \"\"\"\n    # 비디오 데이터를 저장할 빈 리스트를 초기화합니다.\n    video_data = []\n\n    # YouTube API 서비스 빌드\n    youtube = build('youtube', 'v3', developerKey=api_key)\n\n    for region_code in region_codes:\n        for category_id in category_ids:\n            # 각 지역 및 카테고리에 대해 next_page_token을 None으로 초기화\n            next_page_token = None\n            while True:\n                # 인기 동영상을 가져오기 위해 YouTube API에 요청을 보냅니다.\n                request = youtube.videos().list(\n                    part='snippet,contentDetails,statistics',\n                    chart='mostPopular',\n                    regionCode=region_code,\n                    videoCategoryId=category_id,\n                    maxResults=50,\n                    pageToken=next_page_token\n                )\n                response = request.execute()\n                videos = response['items']\n\n                # 각 비디오를 처리하고 데이터를 수집합니다.\n                for video in videos:\n                    video_info = {\n                        'region_code': region_code,\n                        'category_id': category_id,\n                        'video_id': video['id'],\n                        'title': video['snippet']['title'],\n                        'published_at': video['snippet']['publishedAt'],\n                        'view_count': int(video['statistics'].get('viewCount', 0)),\n                        'like_count': int(video['statistics'].get('likeCount', 0)),\n                        'comment_count': int(video['statistics'].get('commentCount', 0)),\n                        'channel_title': video['snippet']['channelTitle']\n                    }\n                    video_data.append(video_info)\n\n                # 결과의 더 많은 페이지가 있는 경우 다음 페이지 토큰을 가져옵니다.\n                next_page_token = response.get('nextPageToken')\n                if not next_page_token:\n                    break\n\n    return pd.DataFrame(video_data)\n\n# DAG를 위한 데이터 추출 작업 정의\nextract_task = PythonOperator(\n    task_id='extract_data_from_youtube_api',\n    python_callable=extract_data,\n    op_kwargs={\n        'api_key': os.getenv('YOUTUBE_API_KEY'),\n        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],\n        'category_ids': ['1', '2', '10', '15', '20', '22', '23']\n    },\n    dag=dag,\n)\n\nextract_task # 이 작업을 실행하도록 DAG를 설정함\n```\n\n이 코드에서 두 가지 주요 작업이 이루어지고 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- DAG에 extract_task라는 작업을 만들고 있습니다.\n- extract_task에서 호출되는 callable 함수인 extract_data를 정의하고 있습니다. 이 함수는 YouTube Data API에서 데이터를 가져와 \"Youtube_Trending_Data_Raw\"로 시작하는 CSV 파일에 pandas DataFrame을 사용하여 저장합니다.\n\nYouTube Data API 문서를 참조하여 API의 다른 부분에서 사용 가능한 데이터에 대해 자세히 이해할 수 있습니다. 우리는 트렌딩 비디오 데이터에 관심이 있으므로 API의 해당 부분에 집중할 것입니다. next_page_token은 모든 페이지에서 데이터를 검색하도록 보장합니다.\n\n코드를 수정한 후 Airflow 페이지에 변경 사항이 반영되어야 합니다. DAG를 수동으로 실행하려면 왼쪽 상단에 있는 실행 버튼을 클릭하시면 됩니다. 그래프에서 작업 상태 (대기, 실행 중, 성공 등)는 다른 색상으로 나타납니다. DAG가 실행 중일 때 로그를 보실 수도 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_5.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n런 버튼을 클릭하면 데이터를 가져오고 파일에 저장하는 데 시간이 걸립니다. 작업의 각 단계에서 그래프 색상이 변경되는 것을 볼 수 있을 거에요. 멋지죠? :)\n\n작업 상태가 성공을 나타내는 녹색으로 변하면, 새 파일인 \"Youtube-Trending-Data-Raw\"가 생긴 것을 확인할 수 있어요.\n\n우리의 Raw 데이터는 이렇게 생겼어요:\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 추출 작업이 완료되었습니다. 다음 작업으로 넘어가 봅시다!\n\n## 3. PySpark를 사용하여 데이터 변환하기:\n\n원시 데이터 파일을 살펴보면 데이터에 많은 해시태그와 이모지가 있는데, 이는 우리 프로젝트에는 필요하지 않습니다. 데이터를 전처리하고 정리하여 추가 분석에 유용하도록 만들어 봅시다.\n\n이 작업에 PySpark를 사용할 것입니다. PySpark는 대용량 데이터 세트를 처리하고 변환 작업을 수행하기 위해 설계된 강력한 프레임워크입니다. 데이터 세트가 특히 크지 않기 때문에 Pandas를 사용할 수도 있지만, 전에 PySpark를 사용한 적이 있어 이번에도 PySpark를 사용하기로 결정했습니다. 최근 PySpark를 공부하고 있으며, 이론을 공부하는 것보다 실제 구현이 더 흥미롭다고 느낍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# Python callable function to extract data from YouTube API\ndef extract_data(**kwargs):\n    api_key = kwargs['api_key']\n    region_codes = kwargs['region_codes']\n    category_ids = kwargs['category_ids']\n    \n    df_trending_videos = fetch_data(api_key, region_codes, category_ids)\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    # Save DataFrame to CSV file\n    df_trending_videos.to_csv(output_path, index=False)\n\ndef fetch_data(api_key, region_codes, category_ids):\n    \"\"\"\n    Fetches trending video data for multiple countries and categories from YouTube API.\n    Returns a pandas data frame containing video data.\n    \"\"\"\n    video_data = []\n\n    # Build YouTube API service\n    youtube = build('youtube', 'v3', developerKey=api_key)\n\n    for region_code in region_codes:\n        for category_id in category_ids:\n            # Initialize the next_page_token to None for each region and category\n            next_page_token = None\n            while True:\n                # Make a request to the YouTube API to fetch trending videos\n                request = youtube.videos().list(\n                    part='snippet,contentDetails,statistics',\n                    chart='mostPopular',\n                    regionCode=region_code,\n                    videoCategoryId=category_id,\n                    maxResults=50,\n                    pageToken=next_page_token\n                )\n                response = request.execute()\n                videos = response['items']\n\n                # Process each video and collect data\n                for video in videos:\n                    video_info = {\n                        'region_code': region_code,\n                        'category_id': category_id,\n                        'video_id': video['id'],\n                        'title': video['snippet']['title'],\n                        'published_at': video['snippet']['publishedAt'],\n                        'view_count': video['statistics'].get('viewCount', 0),\n                        'like_count': video['statistics'].get('likeCount', 0),\n                        'comment_count': video['statistics'].get('commentCount', 0),\n                        'channel_title': video['snippet']['channelTitle']\n                    }\n                    video_data.append(video_info)\n\n                # Get the next page token, if there are more pages of results\n                next_page_token = response.get('nextPageToken')\n                if not next_page_token:\n                    break\n\n    return pd.DataFrame(video_data)\n\ndef preprocess_data_pyspark_job():\n    spark = SparkSession.builder.appName('YouTubeTransform').getOrCreate()\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    df = spark.read.csv(output_path, header=True)\n    \n    # Define UDF to remove hashtag data, emojis\n    def clean_text(text):\n     if text is not None:\n        # Remove emojis\n        text = emoji.demojize(text, delimiters=('', ''))\n        \n        # Remove hashtag data\n        if text.startswith('#'):\n            text = text.replace('#', '').strip()\n        else:\n            split_text = text.split('#')\n            text = split_text[0].strip()\n        \n        # Remove extra double quotes and backslashes\n        text = text.replace('\\\\\"', '')  # Remove escaped quotes\n        text = re.sub(r'\\\"+', '', text)  # Remove remaining double quotes\n        text = text.replace('\\\\', '')  # Remove backslashes\n        \n        return text.strip()  # Strip any leading or trailing whitespace\n\n     return text\n    # Register UDF\n    clean_text_udf = udf(clean_text, StringType())\n\n    # Clean the data\n    df_cleaned = df.withColumn('title', clean_text_udf(col('title'))) \\\n                   .withColumn('channel_title', clean_text_udf(col('channel_title'))) \\\n                   .withColumn('published_at', to_date(col('published_at'))) \\\n                   .withColumn('view_count', col('view_count').cast(LongType())) \\\n                   .withColumn('like_count', col('like_count').cast(LongType())) \\\n                   .withColumn('comment_count', col('comment_count').cast(LongType())) \\\n                   .dropna(subset=['video_id'])\n    \n    # Generate the filename based on the current date\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'\n    \n    # Write cleaned DataFrame to the specified path\n    df_cleaned.write.csv(output_path, header=True, mode='overwrite')   \n\n\n# Define extract task for the DAG\nextract_task = PythonOperator(\n    task_id='extract_data_from_youtube_api',\n    python_callable=extract_data,\n    op_kwargs={\n        'api_key': os.getenv('YOUTUBE_API_KEY'),\n        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],\n        'category_ids': ['1', '2', '10', '15', '20', '22', '23']\n    },\n    dag=dag,\n)\n\n# Define preprocessing task for the DAG\npreprocess_data_pyspark_task= PythonOperator(\n    task_id='preprocess_data_pyspark_task',\n    python_callable=preprocess_data_pyspark_job,\n    dag=dag\n)\n\nextract_task \u003e\u003e preprocess_data_pyspark_task\n\n\n여기서는 이 코드가 하는 일을 설명해 드렸습니다.\n\n- \"preprocess_data_pyspark_task\"라는 작업을 만듭니다.\n- 이 작업은 preprocess_data_pyspark_job 함수를 호출합니다.\n- preprocess_data_pyspark_job 함수는 데이터를 정리합니다.\n- 그리고 정리된 데이터는 \"Transformed_Youtube_Data_currentDate\"라는 폴더에 저장됩니다.\n- 이 폴더 안에는 정리된 데이터가 담긴 \"part-\" 접두사가 붙은 새 CSV 파일이 생성됩니다.\n\n만약 Airflow를 보신다면 아래와 같이 첫 번째 작업에 새로운 작업이 추가된 것을 보실 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 우리가 변환한 데이터의 모습입니다:\n\n![Transformed Data](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_8.png)\n\n이 작업은 완료되었습니다. 이제 최종 작업으로 넘어갈 차례입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 4. S3로 데이터 로드하기:\n\n이 작업을 시작하기 전에 처음에 설정한 IAM 사용자를 사용하여 S3 버킷을 생성하고 버킷 이름을 메모해주세요.\n\n우리의 최종 코드입니다!\n\n```js\nimport logging\nimport os\nimport re\nimport shutil\nfrom datetime import datetime, timedelta\n\nimport boto3\nimport emoji\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_date, udf\nfrom pyspark.sql.types import (DateType, IntegerType, LongType, StringType,\n                               StructField, StructType)\n\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\n# DAG 및 기본 인자 정의\ndefault_args = {\n    'owner': 'airflow',  # DAG 소유자\n    'depends_on_past': False,  # 이전 DAG 실행에 의존 여부\n    'email_on_failure': False,  # 실패 시 이메일 알림 비활성화\n    'email_on_retry': False,  # 재시도 시 이메일 알림 비활성화\n    'retries': 1,  # 재시도 횟수\n    'retry_delay': timedelta(minutes=5),  # 재시도 사이 간격\n    'start_date': datetime(2023, 6, 10, 0, 0, 0),  # 매일 자정(00:00) UTC에 실행\n}\n\n# DAG 정의\ndag = DAG(\n    'youtube_etl_dag',  # DAG 식별자\n    default_args=default_args,  # 기본 인수 할당\n    description='간단한 ETL DAG',  # DAG 설명\n    schedule_interval=timedelta(days=1),  # 스케줄 간격: 매일\n    catchup=False,  # 누락된 DAG 실행을 복구하지 않음\n)\n\n# YouTube API에서 데이터를 추출하는 Python 유형의 함수\ndef extract_data(**kwargs):\n    api_key = kwargs['api_key']\n    region_codes = kwargs['region_codes']\n    category_ids = kwargs['category_ids']\n    \n    df_trending_videos = fetch_data(api_key, region_codes, category_ids)\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    # DataFrame을 CSV 파일로 저장\n    df_trending_videos.to_csv(output_path, index=False)\n\n# YouTube API에서 데이터 가져오는 함수\ndef fetch_data(api_key, region_codes, category_ids):\n    \"\"\"\n    YouTube API에서 여러 국가 및 카테고리의 트렌드 비디오 데이터를 가져옵니다.\n    비디오 데이터가 포함된 Pandas 데이터 프레임 반환.\n    \"\"\"\n    # 비디오 데이터를 보관할 빈 리스트 초기화\n    video_data = []\n\n    # YouTube API 서비스 빌드\n    youtube = build('youtube', 'v3', developerKey=api_key)\n\n    for region_code in region_codes:\n        for category_id in category_ids:\n            # 각 지역 및 카테고리마다 next_page_token을 None으로 초기화\n            next_page_token = None\n            while True:\n                # YouTube API에 트렌드 비디오를 가져오도록 요청\n                request = youtube.videos().list(\n                    part='snippet,contentDetails,statistics',\n                    chart='mostPopular',\n                    regionCode=region_code,\n                    videoCategoryId=category_id,\n                    maxResults=50,\n                    pageToken=next_page_token\n                )\n                response = request.execute()\n                videos = response['items']\n\n                # 각 비디오 처리 및 데이터 수집\n                for video in videos:\n                    video_info = {\n                        'region_code': region_code,\n                        'category_id': category_id,\n                        'video_id': video['id'],\n                        'title': video['snippet']['title'],\n                        'published_at': video['snippet']['publishedAt'],\n                        'view_count': video['statistics'].get('viewCount', 0),\n                        'like_count': video['statistics'].get('likeCount', 0),\n                        'comment_count': video['statistics'].get('commentCount', 0),\n                        'channel_title': video['snippet']['channelTitle']\n                    }\n                    video_data.append(video_info)\n\n                # 결과의 추가 페이지가 있는 경우 다음 페이지 토큰 가져오기\n                next_page_token = response.get('nextPageToken')\n                if not next_page_token:\n                    break\n\n    return pd.DataFrame(video_data)\n\n# PySpark 작업 전처리 함수\ndef preprocess_data_pyspark_job():\n    spark = SparkSession.builder.appName('YouTubeTransform').getOrCreate()\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    df = spark.read.csv(output_path, header=True)\n    \n    # 해시태그 데이터, 이모지 제거를 위한 UDF 정의\n    def clean_text(text):\n     if text is not None:\n        # 이모지 제거\n        text = emoji.demojize(text, delimiters=('', ''))\n        \n        # 해시태그 및 이후 모든 것 제거\n        if text.startswith('#'):\n            text = text.replace('#', '').strip()\n        else:\n            split_text = text.split('#')\n            text = split_text[0].strip()\n        \n        # 추가 이중 인용부호와 백슬래시 제거\n        text = text.replace('\\\\\"', '')  # 이스케이프된 따옴표 제거\n        text = re.sub(r'\\\"+', '', text)  # 남은 이중 인용부호 제거\n        text = text.replace('\\\\', '')  # 백슬래시 제거\n        \n        return text.strip()  # 선행 또는 후행 공백 제거\n\n     return text\n    # UDF 등록\n    clean_text_udf = udf(clean_text, StringType())\n\n    # 데이터 정리\n    df_cleaned = df.withColumn('title', clean_text_udf(col('title'))) \\\n                   .withColumn('channel_title', clean_text_udf(col('channel_title'))) \\\n                   .withColumn('published_at', to_date(col('published_at'))) \\\n                   .withColumn('view_count', col('view_count').cast(LongType())) \\\n                   .withColumn('like_count', col('like_count').cast(LongType())) \\\n                   .withColumn('comment_count', col('comment_count').cast(LongType())) \\\n                   .dropna(subset=['video_id'])\n    \n    # 현재 날짜를 기반으로 파일 이름 생성\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'\n    \n    # 정리된 DataFrame을 지정된 경로에 작성\n    df_cleaned.write.csv(output_path, header=True, mode='overwrite')   \n\n# S3로 데이터 업로드 함수\ndef load_data_to_s3(**kwargs):\n    bucket_name = kwargs['bucket_name']\n    today = datetime.now().strftime('%Y/%m/%d')\n    prefix = f\"processed-data/{today}\"\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    local_dir_path  = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'\n    upload_to_s3(bucket_name, prefix, local_dir_path)\n\ndef upload_to_s3(bucket_name, prefix, local_dir_path):\n    aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n    aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n\n    s3_client = boto3.client(\n        's3',\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key\n    )\n\n    for root, dirs, files in os.walk(local_dir_path):\n         for file in files:\n            if file.endswith('.csv'):\n                file_path = os.path.join(root, file)\n                s3_key = f\"{prefix}/{file}\"\n                logging.info(f\"Uploading {file_path} to s3://{bucket_name}/{s3_key}\")\n                s3_client.upload_file(file_path, bucket_name, s3_key)\n\n# DAG의 추출 작업 정의\nextract_task = PythonOperator(\n    task_id='extract_data_from_youtube_api',\n    python_callable=extract_data,\n    op_kwargs={\n        'api_key': os.getenv('YOUTUBE_API_KEY'),\n        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],\n        'category_ids': ['1', '2', '10', '15', '20', '22', '23']\n    },\n    dag=dag,\n)\n\n# DAG의 데이터 전처리 작업 정의\npreprocess_data_pyspark_task= PythonOperator(\n    task_id='preprocess_data_pyspark_task',\n    python_callable=preprocess_data_pyspark_job,\n    dag=dag\n)\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 저희가 만든 최종 작업인 load_data_to_s3_task를 소개합니다. 이 작업은 load_data_to_s3 함수를 호출하여 파일을 S3 버킷에 업로드합니다. 업로드가 잘 되었는지 확인하려면 S3 버킷의 내용을 확인하세요.\n\n마침내 우리의 Airflow는 이렇게 생겼습니다!\n\n![Airflow](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_9.png)\n\n이제 이 데이터를 Tableau나 다른 BI 도구에 연결하여 흥미로운 대시보드를 만들고 인사이트를 시각화해 보세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n함께 이 파이프라인을 따라 오면서 새로운 기술 몇 가지를 배웠으면 좋겠어요! 🚀 성공적으로 여기까지 왔다면 축하해요! 🎉 이 새롭게 얻은 지식이 데이터 엔지니어링에서의 향후 모험에 큰 도움이 되길 바래요!\n\n이 프로젝트의 Github 저장소를 첨부합니다:\n\n만약 이 글을 좋아하셨다면, 공유하고, 좋아요를 눌러주시고, 아래에 댓글을 남겨주시고 구독해주세요. 🎉👏📝\n\n커튼을 닫습니다! 🎭","ogImage":{"url":"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png"},"coverImage":"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png","tag":["Tech"],"readingTime":25},{"title":"1771 Technologies가 선보이는 Graphite Grid 소개","description":"","date":"2024-06-19 09:36","slug":"2024-06-19-IntroducingGraphiteGridby1771Technologies","content":"\n\n![그래픽그리드 소개](/assets/img/2024-06-19-IntroducingGraphiteGridby1771Technologies_0.png)\n\n# 약간의 역사\n\n2023년에 1771 Technologies를 설립할 때, 우리는 ‘그냥 작동하는’ 소프트웨어를 개발하기를 목표로 했습니다. 개발자를 위한 제품을 개발하고자 했으며, 복잡한 문제를 문제없이 해결하면서도 사용성과 기능성을 희생시키지 않는 것이 우리의 목표였습니다.\n\n우리 회사는 ‘무결한 간단함’이라는 용어의 살아있는 구현입니다. 우리의 미션은 가장 요구가 많은 사용 환경에도 견디는 신뢰할 수 있고 기능적인 소프트웨어를 만들어 사용자에게 최고의 품질을 제공하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Graphite Grid의 탄생\n\n당사의 솔루션이 완벽한 간편함을 제공한다는 약속을 지속적으로 이행함에 따라, 우리는 혁신적인 소프트웨어 제품인 Graphite Grid를 소개하게 되어 매우 기쁩니다. 이 React 데이터 그리드 라이브러리는 단순히 또 다른 도구가 아니라 성능 중심 소프트웨어에서의 게임 체인저입니다.\n\nGraphite Grid는 철저한 개발, 테스트 및 정제를 거쳐 귀하의 조직이 필요로 하는 유일한 JavaScript 데이터 그리드가 되도록 보장합니다. Graphite Grid를 사용하면 고객은 데이터의 복잡성에 적응하는 도구로 스마트한 결정을 내릴 수 있으며, 타협 없이 견고한 기반 위에 애플리케이션을 구축할 수 있습니다.\n\n# 여정이 시작됩니다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자바스크립트와 리액트 커뮤니티에서 소프트웨어 제품의 훌륭한 기능과 삶을 바꿀 기능에 대해 블로그 글을 쓰는 것은 새로운 일이 아닙니다. 그래서 우리는 소프트웨어가 설득하는 모든 작업을 하도록 선호합니다.\n\n전통적인 데이터 그리드의 제한에서 벗어나 개발 프로세스를 혁신하고 싶다면, 1771technologies.com을 방문하여 더 많은 정보를 알아보세요.\n\n당신이 여정에 머무르기를 바랍니다.","ogImage":{"url":"/assets/img/2024-06-19-IntroducingGraphiteGridby1771Technologies_0.png"},"coverImage":"/assets/img/2024-06-19-IntroducingGraphiteGridby1771Technologies_0.png","tag":["Tech"],"readingTime":2},{"title":"분석력을 향상시키는 5가지 유용한 시각화 방법","description":"","date":"2024-06-19 09:34","slug":"2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis","content":"\n\n아래는 Markdown 형식으로 표를 변경한 코드입니다.\n\n\n![Visualization](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_0.png)\n\n# 소개\n\nSeaborn은 오랫동안 사용되어 왔습니다.\n\n비전문가들도 강력한 그래픽을 구축할 수 있도록 도와주기 때문에, 데이터 시각화 라이브러리 중에서 가장 유명하고 많이 사용되는 것 중 하나라고 생각합니다. 또한 통계에 근거한 통찰력을 얻는 데 도움이 되기 때문입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 통계학자가 아닙니다. 데이터 과학에 관심을 가지고 있어서 그 분야에 대한 통계 개념을 배워 직무를 더 잘 수행하기 위해 노력하고 있어요. 그래서 히스토그램, 신뢰구간, 그리고 선형 회귀 분석에 매우 적은 양의 코드로 간단하게 접근할 수 있어서 정말 좋아해요.\n\nSeaborn의 구문은 매우 기본적입니다: sns.type_of_plot(data, x, y)요. 이 간단한 템플릿을 사용하여 막대 그래프, 히스토그램, 산점도, 선 그래프, 상자 그림 등 다양한 시각화를 만들 수 있어요.\n\n하지만 이 게시물은 그것들에 대해 이야기하려는 것이 아니에요. 여러분의 분석에 차이를 만들어 줄 다른 향상된 종류의 시각화에 대해 이야기할 거예요.\n\n어떤 시각화들이 있는지 함께 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 시각화\n\n이 시각화를 만들고 이 연습과 함께 코드를 작성하려면 seaborn을 import합니다. import seaborn as sns.\n\n여기에서 사용된 데이터셋은 Paulo Cortez가 작성하고 크리에이티브 커먼즈 라이센스하에 UCI 저장소에 기즐한 학생 성적 데이터입니다. 아래 코드를 사용하여 파이썬에서 바로 가져올 수 있습니다.\n\n```js\n# UCI Repo 설치\npip install ucimlrepo\n\n# 데이터셋 로딩\nfrom ucimlrepo import fetch_ucirepo \n  \n# 데이터셋 가져오기 \nstudent_performance = fetch_ucirepo(id=320) \n  \n# 데이터 (팬더스 데이터프레임 형식) \nX = student_performance.data.features \ny = student_performance.data.targets\n\n# 시각화를 위해 X와 Y 수집\ndf = pd.concat([X,y], axis=1)\n\ndf.head(3)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_1.png)\n\nNow let's talk about the 5 visualizations.\n\n## 1. Stripplot\n\nThe first plot picked is the stripplot. And you will quickly see why this is interesting. If we use this simple line of code, it will display the following viz.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 플롯\nsns.stripplot(data=df);\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_2.png\" /\u003e\n\n와우! 거의 Pandas의 `df.describe()`와 같은 차트입니다. x축에는 모든 수치 변수가 나타납니다. y축은 해당 변수의 범위 값입니다. 따라서 그림을 보면 몇 가지 흥미로운 통찰력을 빠르게 얻을 수 있습니다.\n\n- 시각적으로 적어도 이상치 실례가 적습니다.\n- 대부분의 수치 변수는 0에서 5 사이로 범위가 있습니다. 이는 데이터의 설명에 해당 변수가 범주화되었음을 보여줍니다. 따라서 여기서 가장 좋은 방법은 해당 변수를 범주형으로 변환하는 것입니다.\n- 이 데이터셋의 학생들은 15 ~ 23세 사이입니다.\n- 첫 번째 학기(G1)와 두 번째 학기(G2) 성적의 분포는 매우 유사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정말 대단하죠! 코드 한 줄로 생성된 이 플롯에서 우리가 얻은 정보의 양을 보세요!\n\n이 그래픽에 대해 더 탐구할 만한 다른 부분이 많이 있어요. 몇 가지 다른 인수를 추가하고 개선할 수 있어요. 실수와 최종 성적 G3에 대한 그래픽을 만들어서 학교별로 분리해 봅시다.\n\n이렇게요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_3.png\" /\u003e\n\n좋아요! 실패 횟수가 적은 학생들이 높은 성적을 받고 있는 것을 예상대로 확인할 수 있어요. 그리고 두 학교 모두 성적면에서 꽤 유사해 보여요.\n\n이제 다음으로 넘어가 봅시다.\n\n## 2. Catplot\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n음-음... 우리는 stripplot을 만들었지만, 그것은 숫자형 변수에만 해당해요. 그럼 범주형 변수는 어떨까요?\n\n여기서 catplot이 유용합니다. 이것은 카테고리별 관측치를 플롯으로 나타내줄 거에요. 학교별 성적을 확인하고 싶다면, 이렇게 간단해요.\n\n```js\n#CatPlot\nsns.catplot(data=df, x='school', y='G3');\n```\n\n![이미지](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기본적으로 카테고리별로 산점도가 표시됩니다. 그러나 kind 인수를 사용하여 막대, 상자, 바이올린과 같은 다른 유형으로 변경할 수 있습니다.\n\n```js\n#플롯\nsns.catplot(data=df, x='school', y='G3', kind='box');\nsns.catplot(data=df, x='school', y='G3', kind='bar');\n```\n\n![이미지](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_5.png)\n\n이제 상자 그림이 분포를 정확하게 보여주지 않는다는 점을 살펴봅니다. 이것은 요즘 이야기가 되고 있는 주제입니다. 사람들은 통계에 익숙하지 않은 사람들을 포함해 상자 플롯을 이해하는 데 어려움을 겪는다. 따라서 이 유형의 플롯에 대한 기본값은 산점도입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 백분위수 개념을 알고 상자 그림을 보는 데 편한 사람조차도 상자의 선안에 얼마나 많은 데이터가 있는지 파악하기 어려울 수 있습니다.\n\n그래서 seaborn 팀은 이 문제를 해결하려고 다음에 제시될 향상된 상자 그림을 사용했습니다.\n\n## 3. Boxenplot\n\nBoxenplot은 향상된 상자 그림입니다. 왜냐하면 상자 그림의 선분이 없으며 상자의 크기가 각 백분위수의 데이터 양에 따라 다르기 때문입니다. 한 개를 그려봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# BoxenPlot\nsns.boxenplot(data=df, x='traveltime', y='G3');\n```\n\n다음 그림을 보면 상자의 너비가 각 백분위별 관측치의 양에 따라 변하는 것을 볼 수 있습니다. 여행 시간 범주 1(15분 이하)은 대부분의 데이터 포인트가 중간인 10에서 14 사이에 있으며, 분포는 매우 정규와 유사합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_6.png\" /\u003e\n\nTravel Time == 1의 학점만을 분리해 보면 다음과 같은 그래픽을 볼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsns.displot(df.query('traveltime == 1'),\n              x='G3', kind='kde', aspect=2)\nplt.title('Distribution KDE of Final Grade on Travel Time == 1');\n```\n\n왼쪽의 첫 번째 상자 그림과 매우 관련이 있는 것을 관찰할 수 있습니다: 하단에서 격리된 몇 개의 점에 이어 거의 정규 분포에 가까운 분포가 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_7.png\" /\u003e\n\n실제로 상자 그림을 향상시킨 것입니다. 그러나 더 많은 그래픽 유형을 공부할 필요가 있습니다. 계속 전진하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4. lmplot\n\nlmplot은 Linear Model plot의 줄임말로, 데이터셋의 간단한 선형 모델을 시각화하는 가장 쉬운 방법입니다. Grade 1과 최종 학년 점수 간의 관계를 확인하고 싶다면, 다음 코드로 선형 모델을 시각화할 수 있습니다.\n\n```js\nsns.lmplot(data=df, x=\"G1\", y='G3')\n```\n\n결과가 표시됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_8.png\" /\u003e\n\n하지만 이 함수의 장점이 최고라고 할 수는 없어요. 우리는 색조를 추가하여 다중 수준 회귀를 모의할 수 있어요. 이 경우에는 두 성별이 유사한 성능을 발휘하고 있어서, 그 범주에 기반한 계층 선형 모델을 만드는 것이 의미가 없겠죠.\n\n```js\nsns.lmplot(data=df, x=\"G1\", y= 'G3', hue='sex')\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_9.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n게다가, 대상 변수와 비교하는 다양한 선형 모델을 추가하는 것도 간단합니다. 예를 들어, col 인수를 사용하여 서로 다른 학교에 대한 모델을 추가해봅시다.\n\n```js\nsns.lmplot(data=df, x=\"G1\", y='G3', hue='sex', col='school')\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_10.png\" /\u003e\n\n보다 깊은 분석을 위한 매우 유용한 플롯입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자, 이 글의 마지막 유형을 살펴보겠습니다.\n\n## 5. residplot\n\nresidplot은 선형 회귀의 잔차를 그리는 것입니다. 하지만, 그것이 왜 중요한지요?\n\n음, 선형 모델의 잔차로 하는 테스트 중 하나는 등분산성입니다. 즉, 잔차가 균일해야 한다는 것을 확인하는 것이죠. 우리가 선 (linear)을 따르는 관계를 분석한다면, 오류도 일정 범위 내에서 선을 따를 것이라는 것이 합리적입니다. 그래서 residplot을 볼 때에는 유사한 분산을 가진 직사각형 모양을 보고 싶습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsns.residplot(data=df, x=\"G1\", y= 'G3')\n```\n\nGrade 1로 최종 성적을 예측하는 선형 모델의 잔차를 보면 거의 균일한 집합을 볼 수 있지만 몇 가지 예외가 있습니다.\n\n![residual plot](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_11.png)\n\n# 떠나시기 전에\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n좋아요. 이제 우리는 분석에 사용할 수 있는 또 다른 5가지 그래픽 유형을 갖추었습니다.\n\n좋은 탐색적 데이터 분석은 시간이 걸립니다. 그 과정에서 많은 질문들이 나타나고 데이터에 대한 이해를 풍부하게 만들어줍니다. 그래서 더 많은 작업이 필요한 질문들에 대해 심층적으로 파고들기 위한 향상된 도구 몇 개를 가지고 있는 것이 중요합니다.\n\n- stripplot: 데이터 포인트를 전체로 시각화하는 데 도움이 됩니다. describe 함수 시각화와 유사합니다.\n- catplot: stripplot과 비슷하지만 범주에 대한 것입니다. 점, 막대, 상자와 같은 여러 형태로 나타낼 수 있습니다.\n- boxenplot: 상자 그림의 향상된 버전으로 \"수염에 얼마나 많은 데이터가 있는지\"와 같은 공백을 채웁니다.\n- lmplot: 두 변수에 대한 빠르고 간단한 선형 모델을 생성할 수 있습니다. hue 인수를 사용하여 다중 수준 회귀를 시각화하거나 col 인수로 facet grid를 생성할 수도 있습니다.\n- residplot: 선형 모델의 잔차를 살펴보고 잔차의 등분산성에 대한 이탈이 어디서 발생하는지 확인하는 데 유용합니다.\n\n이 내용이 마음에 들면 팔로우해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한, LinkedIn에서 나를 찾을 수 있습니다. \n\n# 참고문헌","ogImage":{"url":"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_0.png"},"coverImage":"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_0.png","tag":["Tech"],"readingTime":7}],"page":"79","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"79"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>