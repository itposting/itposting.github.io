<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/4" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/4" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-2d104a861d88ea21.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_buildManifest.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="합리적인 AI 프롬프트의 비밀 데이터와 함께 프롬프팅" href="/post/2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="합리적인 AI 프롬프트의 비밀 데이터와 함께 프롬프팅" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="합리적인 AI 프롬프트의 비밀 데이터와 함께 프롬프팅" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">합리적인 AI 프롬프트의 비밀 데이터와 함께 프롬프팅</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="챗지피티 대규모 업데이트 엑셀 실시간 대화형 분석 누군가가 그 뒤에 있는 새로운 모델을 공개했습니다 " href="/post/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="챗지피티 대규모 업데이트 엑셀 실시간 대화형 분석 누군가가 그 뒤에 있는 새로운 모델을 공개했습니다 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="챗지피티 대규모 업데이트 엑셀 실시간 대화형 분석 누군가가 그 뒤에 있는 새로운 모델을 공개했습니다 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">챗지피티 대규모 업데이트 엑셀 실시간 대화형 분석 누군가가 그 뒤에 있는 새로운 모델을 공개했습니다 </strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="언어 모델 보정 기법 확률 평가 향상하기" href="/post/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="언어 모델 보정 기법 확률 평가 향상하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="언어 모델 보정 기법 확률 평가 향상하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">언어 모델 보정 기법 확률 평가 향상하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AI 에이전트 능력 엔지니어링" href="/post/2024-06-20-AIAgentCapabilitiesEngineering"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AI 에이전트 능력 엔지니어링" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-AIAgentCapabilitiesEngineering_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AI 에이전트 능력 엔지니어링" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">AI 에이전트 능력 엔지니어링</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지" href="/post/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">18<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="인공지능의 마법 구슬 신경망이 인플레이션을 예측하는 방법" href="/post/2024-06-20-AIsCrystalBallHowNeuralNetworksForecastInflation"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="인공지능의 마법 구슬 신경망이 인플레이션을 예측하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-AIsCrystalBallHowNeuralNetworksForecastInflation_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="인공지능의 마법 구슬 신경망이 인플레이션을 예측하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">인공지능의 마법 구슬 신경망이 인플레이션을 예측하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="사함이 없이 나는 누구일까요" href="/post/2024-06-20-ngunitsinoakokungwalangmedalya"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="사함이 없이 나는 누구일까요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-ngunitsinoakokungwalangmedalya_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="사함이 없이 나는 누구일까요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">사함이 없이 나는 누구일까요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="산불에 대해 자세히 알아보기" href="/post/2024-06-20-DelvingintoWildfires"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="산불에 대해 자세히 알아보기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-DelvingintoWildfires_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="산불에 대해 자세히 알아보기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">산불에 대해 자세히 알아보기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Google의 프루프리드 한 번의 탭으로 작성 정확도를 높이는 AI-driven 기능" href="/post/2024-06-20-GooglesProofreadAI-DrivenTypingAccuracyinOneTap"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Google의 프루프리드 한 번의 탭으로 작성 정확도를 높이는 AI-driven 기능" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-GooglesProofreadAI-DrivenTypingAccuracyinOneTap_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Google의 프루프리드 한 번의 탭으로 작성 정확도를 높이는 AI-driven 기능" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Google의 프루프리드 한 번의 탭으로 작성 정확도를 높이는 AI-driven 기능</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="암호화폐 백테스팅 간단한 딥 러닝 전략의 놀라운 결과" href="/post/2024-06-20-Back-testingCryptocurrenciesAstonishingResultsfromaSimpleDeepLearningStrategy"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="암호화폐 백테스팅 간단한 딥 러닝 전략의 놀라운 결과" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-Back-testingCryptocurrenciesAstonishingResultsfromaSimpleDeepLearningStrategy_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="암호화폐 백테스팅 간단한 딥 러닝 전략의 놀라운 결과" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">암호화폐 백테스팅 간단한 딥 러닝 전략의 놀라운 결과</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><a class="link" href="/posts/1">1</a><a class="link" href="/posts/2">2</a><a class="link" href="/posts/3">3</a><a class="link posts_-active__YVJEi" href="/posts/4">4</a><a class="link" href="/posts/5">5</a><a class="link" href="/posts/6">6</a><a class="link" href="/posts/7">7</a><a class="link" href="/posts/8">8</a><a class="link" href="/posts/9">9</a><a class="link" href="/posts/10">10</a><a class="link" href="/posts/11">11</a><a class="link" href="/posts/12">12</a><a class="link" href="/posts/13">13</a><a class="link" href="/posts/14">14</a><a class="link" href="/posts/15">15</a><a class="link" href="/posts/16">16</a><a class="link" href="/posts/17">17</a><a class="link" href="/posts/18">18</a><a class="link" href="/posts/19">19</a><a class="link" href="/posts/20">20</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"합리적인 AI 프롬프트의 비밀 데이터와 함께 프롬프팅","description":"","date":"2024-06-20 18:31","slug":"2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData","content":"\n\n\u003cimg src=\"/assets/img/2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData_0.png\" /\u003e\n\n맞아요. 환각이나 가짜 증거는 더 이상 없어요. 이젠 순수한 사실들뿐이에요.\n\n그리고 제일 좋은 점은 무엇이냐면, 많이 변화시킬 필요가 없다는 거예요. 프롬프트하는 방식을 약간 바꾸는 것만으로 충분했어요.\n\n이게 뭔지 궁금하시다면, 이제 알아보도록 하죠. 함께 파헤쳐봐요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터 기반 프롬프팅\n\n여기서는 \"데이터를 활용한 프롬프팅\"이라고 부르겠어요. 이것에 대해 처음 들어보신다면, 기본 아이디어를 알려드리겠습니다:\n\n- 템플릿 사용: 당신의 작업을 설명하는 대신, ChatGPT에 응답 템플릿을 제공하세요.\n- 데이터 포함: 관련 데이터를 직접 프롬프트에 첨부하세요. URL, PDF 또는 간단한 텍스트일 수 있습니다.\n- 지시 추가: 연결된 데이터를 사용하여 AI에게 템플릿을 작성하도록 요청하세요.\n\n여기에 이 기술을 사용한 프롬프트의 예시가 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData_1.png\" /\u003e\n\n이제 우리가 기초를 알았으니까 이 프롬프트를 만드는 방법을 알아볼까요? (예를 들어 사용해서) 간단한 트윗을 학술 에세이로 바꾸려고 해요.\n\n## 1/ 데이터 찾기\n\n첫 번째 단계는 첫걸음을 내는 것입니다. 내 경우에는 데이터를 찾는 것이에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 블로그 게시물, PDF, 이미지 또는 ChatGPT에서 받아들일 수 있는 다른 형식일 수 있어요.\n\n나에게는 트윗이에요.\n\n## 2/ 데이터로 프롬프트를 제시해보세요\n\n다음 단계는 프롬프트를 작성하는 것이에요. 채워야 할 중요한 두 부분이 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 템플릿: 이것은 AI의 출력 형식을 안내합니다.\n- 데이터: 이것은 필요한 맥락과 제약을 제공합니다.\n\n원하는 경우 작업을 명확히 설명할 수 있습니다. 제 경우에는 트윗을 글로 변환하라고 언급했어요.\n\n![이미지](/assets/img/2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData_2.png)\n\n한 번 시도해보고 싶다면 전체 프롬프트는 여기에 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 텍스트를 친절하게 한국어로 번역해 드리겠습니다.\n\n\n| 파일 이름                                            | 설명                              |\n|-------------------------------------------------|--------------------------------|\n| 2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData_3.png | AIPrompt 데이터 사용 방법을 보여주는 이미지 |\n\n정말 멋지죠? 간단하면서도 마법처럼 잘 작동합니다.\n\n## 3/ 직접 해보기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 코드로 표를 마크다운 형식으로 바꿔보세요.\n\n즐거운 코딩 되세요!","ogImage":{"url":"/assets/img/2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData_0.png"},"coverImage":"/assets/img/2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData_0.png","tag":["Tech"],"readingTime":2},{"title":"챗지피티 대규모 업데이트 엑셀 실시간 대화형 분석 누군가가 그 뒤에 있는 새로운 모델을 공개했습니다 ","description":"","date":"2024-06-20 18:30","slug":"2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt","content":"\n\nGPT-4o가 출시된 지 며칠 만에, OpenAI가 ChatGPT 내에서 실시간 대화형 데이터 분석을 가능케 하는 또 다른 혁신적인 기능을 발표했습니다.\n\n![이미지](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_0.png)\n\n이 업데이트를 통해 사용자들은 이제 ChatGPT에서 데이터 파일을 직접 열 수 있어서 분석을 위한 Python 코드 실행이 원활해졌습니다. 이는 대규모 데이터셋을 병합하고 차트를 생성하며 손쉽게 결론을 요약하는 것을 포함합니다.\n\n![이미지](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 발전은 데이터 분석가의 핵심 작업을 주로 다룹니다. 많은 인터넷 사용자들이 이 발전에 흥분했습니다:\n\n![이미지](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_2.png)\n\n# 주요 기능\n\n1️⃣ Google 드라이브 및 Microsoft OneDrive로부터 직접 파일 업로드:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChatGPT는 이제 Google 시트, 문서, 슬라이드, 그리고 Microsoft 엑셀, 워드, PPT와 같은 파일을 Google 드라이브와 OneDrive에서 직접 업로드할 수 있습니다. 이를 통해 파일을 다운로드하고 다시 업로드하여 분석할 필요가 없어졌습니다.\n\n2️⃣ 대화형 테이블 및 차트:\n\n파일을 업로드하면 ChatGPT가 대화식 테이블을 만듭니다. 확장 버튼을 클릭하면 새로운 대화식 페이지가 열립니다. 예를 들어, 데이터를 월별로 그룹화하려면 데이터를 선택하고 \"월별 그룹화\"라고 입력하면 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터를 반올림하려면 한 번의 클릭으로 조정합니다:\n\n![image1](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_4.png)\n\nChatGPT는 분석된 데이터에서 시각적 차트를 생성할 수도 있습니다:\n\n![image2](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기능은 매일의 작업 흐름에 완벽하게 통합되어 생산성과 정확성을 향상시킵니다. 일부 사용자는 AI 기반 데이터 분석이 AI 코드 개발을 능가할 수 있다고 믿는데, 이는 OpenAI가 효과적으로 활용하고 있는 감정입니다.\n\n3️⃣차트의 사용자 정의 및 다운로드:\n\n사용자는 ChatGPT가 생성한 차트를 사용자 정의하고 발표 자료 및 문서로 다운로드할 수 있습니다. 예를 들어 특정 월에 지출을 결합하거나 데이터를 국가 또는 지역별로 분류하는 것이 간단합니다:\n\n![image](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시각화를 작성할 때, ChatGPT는 전반적인 추세에 대한 텍스트 요약을 제공하며 사용자는 수정된 색상과 같은 인터페이스 내에서 차트를 직접 편집한 후 다운로드할 수 있습니다:\n\n![ChatGPT](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_7.png)\n\nOpenAI는 ChatGPT가 클라우드 차트에 액세스할 수 있지만 기업 사용자 데이터를 교육에 사용하지 않을 것을 확약합니다. ChatGPT Plus 사용자는 추가 보안을 위해 개인정보 보호 기능을 활성화할 수 있습니다.\n\n# 새로운 모델 인사이트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최신 기능들은 데이터 분석을 위해 맞춤화된 새로운 모델에 의해 구동되는 것으로 알려져 있습니다.\n\n![2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_8.png](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_8.png)\n\n일부 사용자들은 ChatGPT 페이지의 Alpha Models 섹션에서 ADA V2(GPT-4)라는 모델을 발견했습니다.\n\n![2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_9.png](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_9.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# OpenAI의 우세함과 산업 영향\n\nOpenAI는 계속해서 트렌드 세터 역할을 하고, 종종 다른 기술 거물들을 가려버리곤 합니다. 구글의 최신 릴리스가 I/O 컨퍼런스에서 발표되었음에도 불구하고, OpenAI의 혁신들이 주목을 받으며 이 회사들 간의 치열한 경쟁을 강조하고 있습니다.\n\n![이미지](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_10.png)\n\n한 OpenAI 직원이 심지어 구글을 조롱했는데, 트윗은 빠르게 삭제되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_11.png)\n\nRegardless, the rivalry benefits users with continuous advancements.\n\nIt’s been an exciting week for AI enthusiasts and professionals alike.\n\n![Image 2](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_12.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n— by 公众号: 量子位\n\n# 참고 자료:\n\n[1] OpenAI Twitter\n\n[2] OpenAI ChatGPT 내 데이터 분석 개선\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[3] 트위터 사용자 @nanulled\n\n💡더 깊이 파고들고 싶나요? 내 ChatGPT 컬렉션이 여러분을 기다리고 있어요.\n\n## 기사가 마음에 드셨나요?\n\n그렇다면:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 댓글을 남겨주세요\n- 내 소식을 팔로우 해주세요\n- 무료 이메일 알림\n\n# 쉬운 영어로 🚀\n\nIn Plain English 커뮤니티의 일원이 되어 주셔서 감사합니다! 떠나시기 전에:\n\n- 꼭 박수와 작가를 팔로우해주세요 👏️️\n- 팔로우하기: X | LinkedIn | YouTube | Discord | Newsletter\n- 다른 플랫폼 방문하기: Stackademic | CoFeed | Venture | Cubed\n- 알고리즘 콘텐츠를 다루도록 강요하는 블로깅 플랫폼에 지친다면? Differ를 시도해보세요\n- PlainEnglish.io에서 더 많은 콘텐츠 확인하기","ogImage":{"url":"/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_0.png"},"coverImage":"/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_0.png","tag":["Tech"],"readingTime":5},{"title":"언어 모델 보정 기법 확률 평가 향상하기","description":"","date":"2024-06-20 18:26","slug":"2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments","content":"\n\n\n![Calibration Techniques for Language Models](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_0.png)\n\n언어 모델, 특히 대형 언어 모델(LLMs)은 인간과 유사한 언어를 이해하고 생성하는 능력으로 인공지능 분야를 혁신했습니다. 이러한 모델은 제로샷 설정에서 다양한 작업을 수행할 뿐만 아니라 맞춤식 프롬프트를 통해 놀라운 유연성과 다양성을 바탕으로 여러 도메인에서 탁월하게 유용합니다.\n\n그러나 그들의 효과적인 성능에도 불구하고, 이 모델들의 교정(calibration)은 종종 도전이 되는 핵심적인 측면 중 하나입니다 — 즉, 다양한 결과에 대한 확률이 그 결과가 정확할 가능성을 정확히 반영하는지 보장하는 것입니다.\n\n본 문서에서는 LLMs의 교정이 필요한 이유를 탐구하고, 그들의 확률 평가를 둘러싼 핵심 문제를 식별하며, 더 나은 모델 교정을 위한 현대적인 방법을 탐구합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## LLM 모델에서 교정이 왜 중요한 이유\n\nLLM의 본질은 정확성뿐만 아니라 올바른 신뢰 수준이 할당된 언어 기반 출력을 처리하고 생성하는 데 있습니다. 교정 — 즉 모델의 신뢰 수준을 정확성과 일치시키는 과정 — 는 다음과 같은 이유로 필수적입니다:\n\n- 신뢰할 수 있는 AI 의사 결정: 적절히 교정된 신뢰 점수는 사용자들이 AI가 내리는 결정을 믿고 의지할 수 있게 하며, 모델이 올바르거나 잘못될 가능성을 이해할 수 있게 합니다.\n\n- 위험 관리: 의료 진단이나 자율 주행과 같은 안전 중요 응용 프로그램에서 자신감이 넘치지만 부정확한 예측은 재앙적 결과로 이어질 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 모델 디버깅 및 개선: 보정은 개발자가 모델의 약점을 이해하고 이에 맞게 개선하는 데 도움이 될 수 있습니다.\n\n## LLM 확률 보정에서의 어려움\n\n대형 언어 모델은 종종 확률 보정에 영향을 미치는 여러 가지 어려움에 직면합니다:\n\n- 닫힌 모델 제한: 많은 대형 언어 모델은 로그 확률에 직접 접근할 수 있는 제한된 액세스를 가진 블랙 박스로 작동하여 확신도를 이해하고 조정하는 프로세스를 복잡하게 만듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 훈련 중의 불일치: Reinforcement Learning from Human Feedback (RLHF)와 같은 기술로 개선된 모델들은 어설프게 miscalibrated 될 수 있습니다. 논문 [1]에 따르면 가장 널리 사용되는 LLM들은 인간 피드백으로 강화학습된 모델들입니다 (RLHF-LLMs). 일부 연구에서는 RLHF-LLMs가 매우 잘 보정되지 않은 조건부 확률을 생성한다고 제안했습니다. 연구 결과는 RLHF-LLMs가 사용자 선호도에 근접하게 따라가기를 우선시하는 경향이 있어 잘 보정된 예측 생성보다는 낮은 보정을 낸다는 것을 보여줍니다. 이는 RLHF로 훈련된 모델들이 정확하고 신뢰할 수 있는 출력을 위해 필요한 확률 보정을 갖추지 못할 수 있는 주요 도전을 보여줍니다.\n\n- 작업별 보정 필요성: LLM의 일반적인 훈련은 일반적으로 특정 작업이나 도메인에 대해 조정되지 않았기 때문에, 특정 요구사항이나 응용 프로그램과 조율되도록 하기 위해 추가적인 보정이 필요합니다.\n\n## LLMs를 위한 고급 보정 방법\n\n보정 도전에 대응하기 위해 아래에서 논의된 여러 기술을 시도해볼 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_1.png)\n\n## Verbalized Confidence\n\n“Verbalized Confidence” refers to techniques where a Language Model (LLM) not only provides answers but also rates its confidence in its response explicitly. This approach involves the use of certain methodologies to obtain more reliable assessments of the model’s confidence in its answers.\n\n## Basic Implementation\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 간단한 형태로 표현된 신뢰 표현은 LLM에게 질문과 관련 있는 맥락을 제시한 후 명시적으로 신뢰 점수를 요청하는 것입니다. 이 직접적인 접근은 더 정교한 기술의 기반 역할을 합니다.\n\n## 향상된 신뢰 표현 기술\n\n- Chain-of-Thought (CoT) Prompting: CoT Prompting은 답변을 제공하기 전에 모델로부터 단계별 추론 과정을 유도하는 것을 포함합니다. 이 방법은 모델의 답변의 명확성과 풍부성을 향상시킬 뿐만 아니라 추론 단계에서 논리적 일관성을 관찰하여 신뢰 수준을 더 정확히 추정할 수 있습니다.\n- 다단계 신뢰 유도: 이 기술은 추론 또는 문제 해결 과정의 여러 단계에서 신뢰 점수를 캡처하여 신뢰 측정을 개선합니다. 최종 신뢰 수준은 개별 신뢰 점수의 곱으로 파생되어 확신의 합성 측정 값을 제공합니다.\n- 상위-K 답변 및 신뢰 점수: 하나의 답변 대신 모델이 여러 가능한 답변(상위-K 답변)을 생성하며 각각 개별적인 신뢰 점수와 함께 제시됩니다. 가장 높은 신뢰 점수를 가진 답변이 최종 답변으로 선택됩니다. 이 방법은 여러 가설을 평가하는 의사 결정 과정과 유사합니다.\n- 다양한 유도 기법: 여러 다양한 유도를 활용하면 신뢰 추정이 더 정확해질 수 있습니다. 다양한 유도는 서로 다른 어구, 맥락 또는 개념 각도에서 비롯될 수 있으며, 이는 모델의 평가를 편향된 또는 정보 부족으로부터 더 견고하게 만들어줍니다.\n- 숫자적 확률 대 언어 표현: 경우에 따라 모델이 정확성 가능성에 직접 연결된 숫자적 확률을 통해 자신의 신뢰를 표현합니다. 반대로, \"매우 가능성이 높음\" 또는 \"아마도 아님\"과 같은 언어적 표현도 사용될 수 있습니다.\n- 여러 가설과 함께 유도: 처음에 모델은 신뢰 등급 없이 여러 답변 후보를 생성합니다. 이후 상호 작용에서 각 답변의 정답 가능성을 평가합니다. 연구 결과는 이 방식으로 여러 가설을 평가하는 것이 극적으로 보정된다는 것을 나타냅니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_2.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델 간에 자신감을 효과적으로 표현하는 능력은 다양하며, 서로 다른 모델 아키텍처와 세대 간에 관찰되는 차이점이 있습니다.\n\n## 자기 일관성 기반 신뢰도\n\n자기 일관성 기반 신뢰도 방법은 동일한 쿼리에 대해 여러 응답을 생성하고 이러한 응답 사이의 일치를 분석함으로써 언어 모델의 신뢰도를 평가하는 복잡한 방법입니다. 이 기술은 다양한 조건에서 높은 일치가 나타날수록 응답의 정확도에 대한 높은 신뢰를 나타낸다는 아이디어에 기반합니다.\n\n여러 응답 생성하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델로부터 다양한 결과 스펙트럼을 얻기 위해 몇 가지 전략을 사용합니다:\n\n셀프-랜덤화: 이 방법은 동일한 질문을 다른 설정에서 여러 번 입력하는 것을 포함합니다. \"온도\" 매개변수를 조절하는 것이 일반적이며, 이는 모델의 응답 다양성을 다루기 위해 출력의 예측성 또는 랜덤성을 변화시킴으로써 작동합니다.\n\n프롬프트 왜곡: 질문의 요구사항을 다르게 해석할 수 있도록 문구를 변경하여 다양한 각도의 응답을 유도합니다. 이를 통해 모델의 강건성을 테스트하며, 문맥상 유사한 프롬프트지만 다르게 표현된 질문 사이에서 일관성을 유지하는지 확인합니다.\n\n잘못된 단서: 의도적인 오류나 오해를 유발하는 힌트를 프롬프트에 삽입하여 모델의 안정성을 평가합니다. 사람의 테스트와 유사하게, 이 방법은 모델이 자신감 있는 사람처럼 오해된 정보를 무시하고 정확하거나 일관된 응답을 유지할 수 있는지를 관찰합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n집계 전략\n\n결과를 종합하고 최종 신뢰 점수를 할당하기 위해 다양한 집계 전략을 고려할 수 있습니다:\n\n일관성 측정: 이는 모델이 다양한 조건에서 동일한 응답을 제공하는 정도를 검토하여 안정성과 신뢰성을 반영합니다.\n\n평균 신뢰도 (평균-신뢰도): 높은 일치도와 개별 신뢰 점수가 높은 답변에 더 많은 가중치를 부여하여 전반적인 신뢰도의 세밀한 측정을 제공하는 가중 평균이 계산됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n쌍-순위 전략: 모델의 상위-K 예측을 사용하는 시나리오에서 특히 유용한 이 전략은 모델 예측에서 순위 정보를 강조하여 가장 가능성이 높고 일관된 응답을 평가하는 데 도움이 됩니다.\n\n## 로짓 기반 접근 방식\n\n로짓 기반 캘리브레이션은 대형 언어 모델(Large Language Models, LLMs)이 하는 확률적 예측의 신뢰성을 향상시키는 중요한 기술입니다. 모델이 로그 확률과 같은 원시 점수를 출력할 때, 일반적으로 이러한 점수는 직접적으로 정확한 확률 분포로 변환되지 않습니다. 캘리브레이션 기술은 이러한 로짓을 조정하여 더 정확한 확률을 반영하며, 이것은 실제 응용 프로그램에서 강력한 의사 결정을 내리는 데 중요합니다. 아래에서는 로짓 기반 캘리브레이션에 사용되는 일부 방법에 대해 자세히 살펴봅니다:\n\n1. 토큰 간의 확신을 평균화하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n언어 모델의 예측에 대한 확신을 더 균일하게 추정하기 위한 일반적인 방법 중 하나는 토큰 간의 확신(로그 확률)을 평균화하는 것입니다. 이는 특정 응용 프로그램이나 데이터셋의 특성에 따라 모든 토큰 또는 선택적 하위 집합에 대해 수행될 수 있습니다. 결과는 모델의 확신에 대한 더 부드럽고 일반화된 측정으로, 어떤 단일 토큰의 변동성이 미치는 영향을 줄입니다.\n\n2. Platt 스케일링 (시그모이드)\n\nPlatt 스케일링 또는 시그모이드 보정은 원래 모델의 출력 로짓에 적용되는 로지스틱 회귀 모델입니다. 로짓 위에 시그모이드 함수를 피팅함으로써이 방법은 로짓을 보정된 확률로 변환합니다. 보정에는 일반적으로 'A'와 'B'로 표시되는 두 매개변수를 학습하는 과정이 포함됩니다. 이 매개변수는 로짓을 실제 관측된 확률과 보다 잘 일치시키기 위해 스케일링 및 이동시킵니다. 이 방법은 이진 분류 작업에 대한 단순성과 효과적임으로 인해 특히 유용합니다.\n\n![그림](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 등위 회귀\n\nPlatt 스케일링과는 다르게, 등위 회귀는 로짓과 확률 사이에 어떤 기능적 형태도 가정하지 않습니다. 이는 예측된 확률을 대상 확률과 일치시키기 위해 비감소 함수를 적합시키는 비모수적 접근 방식입니다. 이 조각별 상수 함수는 유연하며 로짓과 확률 사이의 관계가 더 복잡하거나 비선형적인 경우, 특히 일부 시나리오에서 실제 분포를 더 정확하게 반영할 수 있습니다.\n\n![image](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_4.png)\n\n4. 온도 조정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n온도 조정은 모델의 확신을 조정하는 사후 처리 기술로, 예측을 변경하지 않고 조절합니다. 소프트맥스 함수를 적용하기 전에 상수인 \"온도\"로 로짓을 나누는 방식으로 확률로 변환합니다. 최적의 온도는 보통 검증 데이터셋에서 교차 엔트로피 손실을 최소화하여 결정됩니다. 이 방법은 원래 로짓의 상대적 순서를 유지하면서 보정 프로세스에 미세한 영향을 미치므로 매력적입니다.\n\n# 대리 모델 또는 세부 조정 방법\n\n세부 조정은 특정 데이터와 목표를 사용하여 모델을 미세 조정하여 특정 작업에 보다 나은 준비를 시키는 고급 보정 접근 방식입니다. 더 신뢰할 수 있고 정확한 신뢰 점수를 제공하기 위해 모델을 미세 조정하는 여러 혁신적인 방법을 살펴보겠습니다.\n\n## 대리 모델을 활용한 신뢰 평가\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 가지 매력적인 방법은[2] 일반적으로 더 단순한 모델을 사용하여 주요 모델 (예: GPT-4)로부터 얻은 답변이 얼마나 믿을 만한지를 평가하는 것입니다:\n\n- 하는 일: 예를 들어, LLAMA2와 같은 보조 모델은 다른 모델인 GPT-4가 제공하는 답변의 대한 로그 확률을 얻기위해 동일한 프롬프트를 제공하고 GPT-4 모델 응답에 대한 점수를 추출할 때 사용될 수 있습니다.\n\n- 놀라운 효과성: 비록 보조 모델이 덜 강력할 수 있지만, 이 방법은 언어적 단서만 사용하는 것과 비교하여 더 나은 결과를 낳는 것으로 입증되었습니다(Area Under the Curve 또는 AUC를 기준으로 측정).\n\n## 불확실성 인식: R 튜닝\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nR-튜닝은 모델이 \"모르겠다\"라고 말해도 괜찮다는 것을 가르칩니다. 모델의 한계를 인식합니다. 파인튜닝 프로세스는 다음 단계로 구성됩니다.\n\n- 불확실성 식별: 모델의 답변이 흔들리거나 의문스러운 경우를 찾아내어 예측하고 실제 결과와 비교하여 학습 세트에서 찾습니다.\n\n- 확실히 훈련: 그런 다음 \"확실함\" 또는 \"불확실함\"으로 태그된 예제를 사용하여 모델을 가르치어 이러한 구별에서 배우도록 합니다. \"확실하다\" 또는 \"의심스럽다\"와 같은 구문을 사용하여 훈련 중에 자신감 수준을 표현하며, 토큰 생성부터 오류를 줄이는 데 집중합니다.\n\n![이미지](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## LITCAB: 작은 변화, 큰 영향\n\nLITCAB은 작지만 효과적인 보정 레이어를 소개합니다:\n\n- 간단한 추가: 모델 끝에 단일 선형 레이어를 추가하여 입력 텍스트에 따라 각 응답의 예측 확률을 조정합니다.\n\n- 효율적이고 효과적: 이 소규모 조정은 복잡성을 크게 늘리지 않고 모델의 판단력을 향상시킵니다. 원래 모델 크기 변화의 2% 미만만 변경됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## ASPIRE: 더 스마트한 모델 응답\n\n예측에 신뢰 점수를 할당하고 선택적 예측을 허용합니다. ASPIRE는 세 가지 단계로 구성됩니다:\n\n1. 작업별 튜닝: PEFT 기술을 사용하여 주요 모델을 변경하지 않으면서 특정 변환 가능한 매개변수를 수정하여 특정 작업에 대한 응답을 개선합니다.\n\n2. 답변 샘플링: 이러한 조정을 사용하여 각 질문에 대해 여러 잠재적인 답변을 생성하고 높은 가능성의 출력 시퀀스를 만들기 위해 빔 검색을 사용하며 생성된 출력 시퀀스가 참 값에 기반하여 올바른지 여부를 결정하기 위해 Rouge-L 메트릭을 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 자가평가 학습: 마지막으로, 모델이 자체적으로 답변이 맞거나 틀렸는지 판단하여 자체평가 능력을 향상시키는 다른 조절 세트를 소개합니다.\n\n![이미지](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_6.png)\n\n이러한 방법을 통해 언어 모델은 더 발전된 것뿐만 아니라 사용자의 맥락과 기대에 더 부합하게 되어, 더 신뢰성이 있고 문맥을 인식하는 상호작용을 이끌어냅니다.\n\n## 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대규모 언어 모델의 보정은 복잡하지만 중요한 작업으로, AI 응용 프로그램의 신뢰성과 안전성을 향상시킵니다. 위에서 논의한 다양한 혁신적인 방법을 사용하고 결합함으로써, 이러한 모델이 다양한 맥락에서 이해하고 상호 작용하는 방식을 혁신적으로 개선할 수 있습니다. 이는 높은 자신감과 정확성으로 결정을 내릴 수 있는 진정으로 지능적인 시스템을 위한 길을 열어줍니다.\n\n## 부록:\n\n- Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, 그리고 Christopher D. Manning이 제목을 붙인 “Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback,”\n- Llamas Know What GPTs Don’t Show: Surrogate Models for Confidence Estimation, Vaishnavi Shrivastava, Percy Liang, Ananya Kumar\n- R-Tuning: Instructing Large Language Models to Say ‘I Don’t Know’ Hanning Zhang♠∗, Shizhe Diao♠∗ 및 기타.\n- LITCAB: LIGHTWEIGHT LANGUAGE MODEL CALIBRATION OVER SHORT- AND LONG-FORM RESPONSES Xin Liu, Muhammad Khalifa, Lu Wang\n- Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement Gwenyth Portillo Wightman, Alexandra DeLucia 및 Mark Dredze\n- CAN LLMS EXPRESS THEIR UNCERTAINTY? AN EMPIRICAL EVALUATION OF CONFIDENCE ELICITATION IN LLMS Miao Xiong1∗, Zhiyuan Hu1, Xinyang Lu 및 기타.\n- ASPIRE 소개: LLMs에서 선택적 예측을 위한\n- Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers Moxin Li1, Wenjie Wang 및 기타.\n- 대규모 언어 모델로부터의 장문 생성 보정\nYukun Huang1, Yixin Liu 및 기타.\n\n![이미지](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 이야기는 Generative AI Publication에서 발행되었습니다.\n\n최신 AI 이야기를 따르려면 Substack, LinkedIn 및 Zeniteq에서 저희와 연락을 유지하세요. 함께 AI의 미래를 함께 만들어 보아요!\n\n![이미지](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_8.png)","ogImage":{"url":"/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_0.png"},"coverImage":"/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_0.png","tag":["Tech"],"readingTime":10},{"title":"AI 에이전트 능력 엔지니어링","description":"","date":"2024-06-20 18:23","slug":"2024-06-20-AIAgentCapabilitiesEngineering","content":"\n\n## 인공지능 에이전트를 위한 고수준 능력 엔지니어링 프레임워크 소개\n\n![이미지](/assets/img/2024-06-20-AIAgentCapabilitiesEngineering_0.png)\n\n## 소개\n\n최근 제가 발표한 '프롬프트 엔지니어링에서 에이전트 엔지니어링으로'라는 기사에서 나는 AI 에이전트 엔지니어링을 위한 프레임워크를 제안했습니다. 이 프레임워크는 AI 에이전트의 설계와 생성에 접근하기 위한 사고 모델을 제시합니다. 이 프레임워크는 다음 구조를 제안합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-AIAgentCapabilitiesEngineering_1.png\" /\u003e\n\n- AI 에이전트는 작업을 수행합니다.\n- 작업을 완료하려면 행동이 필요합니다.\n- 행동 수행에는 능력이 필요합니다.\n- 능력에는 필요한 숙련도 수준이 있습니다.\n- 필요한 숙련도 수준에는 기술 및 기법이 필요합니다.\n- 기술과 기법에는 오케스트레이션이 필요합니다.\n\n만약 해당 기사를 놓치셨거나 다시 참고하려면 여기서 찾을 수 있습니다.\n\n비록 직관적이지만, 보다 심오한 수준에서는 이 프레임워크가 포용하는 주제와 아이디어가 매우 방대합니다. 보다 포괄적인 프레임워크에서 제시된 개념을 탐구하는 것은 상당한 노력이 필요하며, 이 기사에서는 AI 에이전트 능력 엔지니어링 프레임워크에 초점을 맞추어 작업을 계속합니다. 이 프레임워크에 접근하는 방식은 주로 인지 및 행동 과학에 근간을 둔 개념을 확장하는 타분류적 마인드셋에 의존합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 인지 및 행동과학 기초\n\n다른 글에서 언급했듯이, 인간의 도구 및 기술 발전의 역사를 통해 우리는 자주 우리 자신을 빌려서 구축하고자 하는 대상이나 모델로 사용해왔습니다. AI에서의 하나의 사례는 인간 두뇌에서 영감을 받은 신경망입니다. AI 에이전트 기능에 대한 프레임워크를 구축하기 위해, 인지 및 행동과학에 대해 영감과 지침, 그리고 유용한 개념의 확장을 찾는 것은 자연스러운 일입니다. 먼저 이러한 과학이 무엇을 의미하는지 전반적인 이해를 해봅시다.\n\n인지 과학\n\n인지 과학은 마음과 그 과정의 다학제적인 연구로 심리학, 신경과학, 언어학, 그리고 인공지능과 같은 영역을 아우릅니다. 이는 인간이 어떻게 지각하고, 생각하며, 배우며, 기억하는지에 대한 중요한 통찰을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n행동 과학\n\n행동 과학은 인간의 인지 과정과 행동을 연구하는 학제간 분야로, 종종 개인과 환경 간의 행동 상호작용을 고려합니다. 심리학, 사회학, 인류학, 경제학과 같은 학문을 포함합니다.\n\nAI 에이전트가 수행할 수 있는 작업에 대한 기대치가 계속해서 높아지면서, 우리의 능력 프레임워크를 인지 및 행동 이론에 근간을 두는 것은 이러한 기대치를 충족하고 AI 에이전트가 인간과 유사한 능력으로 복잡한 작업을 수행할 수 있는 미래를 열어줄 견고한 기반을 제공해줄 것입니다.\n\n## AI 에이전트 능력 프레임워크\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"작은 세부 사항에 들어가기 전에 먼저 '능력'이라고 불리는 것들을 '작업'을 수행하기 위해 에이전트가 취해야 하는 '조치'를 구동하는 방식을 어떻게 범주화할 수 있는지 논의해보겠습니다. 일반적으로, 이러한 능력들은 인지, 사고, 행동 및 적응의 범주로 구성될 수 있습니다. 그리고 이를 통해 보다 구체적인 수준에서 이러한 범주에 속하는 예시 능력을 식별할 수 있습니다. 결과적인 프레임워크는 범주적으로 일관성 있는 것으로 보이지만, 다양한 능력과 범주 간의 함의된 관계가 대략적인 것임을 염두에 두세요. 실제로, 능력들은 프레임워크 전체에 걸쳐 서로 긴밀하게 얽혀 있으며 이 다차원성을 모델링하려고 하는 것은 현재 이 단계에서 그다지 유용하지 않게 느껴집니다. 아래는 범주를 구성하는 주요 범주 및 하위 범주의 시각적 표현이며, 곧 보게 될 범주 정렬이 없습니다.\n\n우리의 주요 초점은 LLM 중심의 AI 에이전트 엔지니어링에 의해 이끌린 것이지만, 이러한 프레임워크를 타인생 AI 및 로봇 분야로 확장할 수 있도록 미래 지향적이고 적용 가능한 개념을 통합하고 있습니다.\n\n마지막으로, 프레임워크에서 소명에 직접적으로 다뤄지지 않는 '자율성'은 특정 에이전트 또는 그 능력 중 하나에 대한 범용적인 특성으로 보다 적합합니다. 그렇다고 해서 자율성이 특정 작업에서 효과적으로 달성해야 하는 요구 사항은 아닙니다.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러한 기반을 마련한 후에 전체 프레임워크를 확장해 보겠습니다.\n\n# 인식\n\n에이전트가 환경에서 감각 정보를 습득, 해석 및 조직하는 능력을 포함합니다. 적절한 자극을 감지, 인식하고 이해함으로써 에이전트가 예상대로 작동할 수 있도록 합니다. 구체적인 능력의 예시:\n\n- 시각 처리: 이미지 및 물체 인식 및 처리\n- 텍스트 데이터 처리: 텍스트 인식 및 처리\n- 청각 처리: 음성 및 소리 인식 및 처리\n- 촉각 처리: 촉각 인식 및 처리\n- 후각 및 미각 처리: 향기 인식 및 처리\n- 감각 통합: 일관된 이해를 위해 다양한 감각 입력의 데이터 통합\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 고민중\n\n에이전트가 정보를 처리하고 개념을 형성하며 문제를 해결하고 결정을 내리며 지식을 적용할 수 있는 능력을 말합니다. 세분화된 능력의 예시는 다음과 같습니다:\n\n문맥적 이해와 인식\n\n- 문맥적 인식과 이해: 상황, 환경, 공간 및 시간적 맥락을 인지하고 이해함.\n- 자기인식 및 메타인지: 자기인식, 자기 모니터링, 자가 평가, 메타인지 지식\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주의와 실행 기능\n\n- 선택적 주의: 관련 데이터에 집중하면서 관련 없는 정보를 걸러내기\n- 분할 주의: 여러 작업이나 정보 원천을 동시에 처리하고 관리하기\n- 지속주의: 장기간 집중하고 노력하는 것\n- 계획: 특정 목표를 달성하기 위한 일련의 조치나 전략 수립\n- 의사 결정: 정보 분석, 옵션 평가 및 최적의 행동 결정\n- 억제 제어: 부적절하거나 원치 않는 행동이나 행동을 억제\n- 인지적 유연성: 두 가지 다른 개념에 대해 생각을 전환하거나 여러 개념을 동시에 고려하는 것\n- 감정 조절: 적절한 감정으로 감정 경험을 관리하고 대응하기\n\n기억\n\n- 단기 기억: 정보를 일시적으로 보유하고 조작하기\n- 작업 기억: 정보를 능동적으로 처리하고 조작하기\n- 장기 기억: 장기간 정보를 저장하고 검색하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이해와 분석\n\n- 논리적 추론: 형식 논리와 구조화된 규칙에 기반한 결론 도출\n- 확률적 추론: 확률과 통계 모델에 기반한 예측과 결정\n- 휴리스틱 추론: 해결책을 찾는 데 규칙 혹은 지름길을 적용\n- 귀납적 추론: 구체적 관찰로부터 일반화 도출\n- 타당적 추론: 일반 원칙이나 전제로부터 구체적 결론 도출\n- 추론적 추론: 관찰을 설명하기 위한 가설 형성\n- 유추적 추론: 이전 경험에 유사성을 찾아 문제 해결\n- 공간적 추론: 공간 관계에 대한 이해와 추론\n\n지식 활용과 응용\n\n- 의미적 지식: 범용 세계 지식과 개념을 이루는 특징 획득과 적용\n- 사건 지식: 특정 사건과 경험 지식 습득과 활용\n- 절차적 지식: 작업과 행동을 효율적으로 수행하는 방법 숙지\n- 선언적 지식: 사실적 정보 획득과 활용\n- 언어 이해: 언어를 이해하고 해석하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사회 및 감정 지능\n\n- 감정 인식: 감정을 감지하고 해석하기\n- 사회적 상호 작용: 사회적으로 적절한 방식으로 사람이나 다른 요소와 상호 작용하기\n- 공감: 다른 사람의 감정 상태를 이해하고 대응하기\n- 마음의 이해: 정신 상태, 의도 및 신념을 추론하고 이해하기\n- 사회적 지각: 사회적 단서와 맥락을 인식하고 이해하기\n- 관계 관리: 장기 관계를 관리하고 키우기\n\n창의성과 상상력\n\n- 아이디어 생성: 새로운 혁신적인 아이디어 도출하기\n- 예술적 창작: 음악, 시각 예술 및 문학과 같은 독창적인 예술 작품 창작하기\n- 상상력 있는 사고: 현재 현실을 넘어 새로운 가능성과 시나리오를 상상하고 표현하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 진행 중\n\n설명: 에이전트가 환경과 상호 작용하고 작업을 수행하는 능력을 포함합니다. 디지털 및 물리적 작업을 모두 포함합니다. 이러한 능력의 범주에는 의사 소통과 상호 작용도 포함되어 있어 사용자 및 다른 시스템과 의미 있는 상호 작용을 할 수 있습니다. 구체적인 능력의 예시는 다음과 같습니다:\n\n- 디지털 작업 실행: 특정 디지털 작업 수행, 출력 생성, 자동화, 문제 해결 작업, 결정 실행 및 응답 작업 포함\n- 물리적 작업 실행: 움직임 계획, 시작 및 조정, 감각 정보를 운동 작업과 통합하고, 물체를 잡고 다루는 것, 새로운 운동 기술 학습 및 적응\n- 인간과의 의사 소통과 상호 작용: 사용자와 의미 있는 대화를 나누며, 여러 언어를 다루고 대화 내용을 유지하는 것\n- 에이전트 및 시스템 간의 의사 소통과 상호 작용: 다른 AI 에이전트와 시스템과 효과적으로 의사 소통 및 조정, 프로토콜과 인터페이스를 사용하여 정보 교환, 작업 동기화 및 플랫폼 간 상호 작용 컨텍스트 유지하기.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내용: 에이전트가 새로운 정보, 경험 및 피드백에 기반하여 행동, 프로세스 및 감정적인 반응을 조정하고 발전시킬 수 있는 능력을 의미합니다. 명확히 말하자면, 여기서 우리는 에이전트의 운영 상태에서의 적응 및 학습 능력에 초점을 맞추고 있으며, 이는 기초 능력을 활성화하기 위한 맥락 내에서 발생하는 학습을 의미하지 않습니다. 우리의 프레임워크에서 이것은 도구 및 기법의 영역이 될 것입니다. 구체적인 능력의 예시는 다음과 같습니다:\n\n학습\n\n- 인지 학습: 인지 프로세스를 통해 지식을 습득하는 것\n- 모방 학습: 행동을 관찰하고 복제함으로써 새로운 기술과 행동을 습득하는 것\n- 경험적 학습: 경험을 통해 학습하고 반성하는 것\n\n적응과 발전\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 행동적 적응: 피드백이나 환경 변화에 대응하여 행동을 조절하는 것\n- 인지적 적응: 새로운 정보에 기반하여 인지 프로세스를 수정하는 것\n- 감정적 적응: 경험과 맥락에 기반하여 감정적 반응을 조절하는 것\n- 운동 적응: 연습과 피드백을 통해 운동 기술을 조정하는 것\n- 사회적 적응: 사회적 신호와 상호작용에 기반하여 사회적 행동을 수정하는 것\n- 진화: 시간이 지남에 따라 행동 및 인지 프로세스의 장기적인 변화와 개선\n\n이 문서는 책이 아닌 글로 쓰여졌으므로 이 예시 세부 기능에 대한 자세한 토론은 하지 않겠습니다. 이것이 철저한 내용이라고 믿고 싶지만, 최선의 시작점일 뿐입니다. 반복과 피드백을 통해 계속해서 수정, 개선하고, 더 넓은 채택에 적합한 안정적인 프레임워크로 나아갈 것입니다.\n\n이제 이 프레임워크의 실제 응용과 에이전트 엔지니어링 환경에서의 가치를 보여주는 몇 가지 예시를 살펴보겠습니다.\n\n## 실무에서의 AI 에이전트 능력 프레임워크\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI 에이전트 능력 프레임워크의 실용적 응용은 인지 및 행동과학 기반의 구조화된 개념을 활용하여 디자인 고민 프로세스를 촉진하는 데에 있습니다. 우리가 대리인들에게 원하는 능력을 구상하고 표현하는 다양한 방법을 고려할 때, 이 프레임워크는 능력 디자인 및 엔지니어링에 일관성과 포괄성을 제공하여 공통적인 기반을 확립하는 데 도움이 됩니다. 우리 AI 에이전트의 능력 수준에 대한 기대가 계속해서 높아지는 가운데, 이것은 특히 가치 있는 부분이 될 것입니다. 예시를 살펴보도록 합시다:\n\n고객 지원용 AI 에이전트\n\n고객 지원 및 맞춤 상품 추천을 제공하는 AI 에이전트를 고려해 봅시다. 이 프레임워크를 활용하여, 보다 세부적인 직무 및 시나리오 설명을 통해 더 생생한 그림을 그려봅시다.\n\n직무: 고객 지원 및 제품 추천에서 우수하고 공감적인 서비스를 제공하며, 판매 추세를 예측하고 매우 맞춤화된 상호작용을 위해 세부적인 맥락 요소를 포함하는 업무를 수행합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n상황: 활기 넘치는 온라인 고객 서비스 환경에서 우리의 AI 에이전트는 고객 쿼리를 해결하고 제품을 추천할 뿐만 아니라 요구를 예측하고 상호작용을 개인화하여 전반적인 고객 경험을 향상시키는 것이 그 임무입니다. 이 일은 다양한 조치와 능력을 포함합니다. 몇 년 전까지는 이러한 능력 중 일부를 구축하는 것은 전혀 불가능했을 것입니다. 이 작업을 위한 능력이 우리의 AI 에이전트 능력 프레임워크를 사용하여 효과적으로 표현될 수 있을까요? 이 작업의 실행 가능성을 확인하기 위해 노력하는 노력을 하겠습니다. 다음 개요가 포괄적인 것이 아님을 명심하면서 더 자세히 살펴보겠습니다:\n\n필요한 조치:\n\n- 고객 쿼리를 이해하고 해석합니다.\n- 정확하고 유용한 응답을 제공합니다.\n- 적절한 때 문제를 에스컬레이션합니다.\n- 고객 상호작용을 기반으로 판매 추세를 예측합니다.\n- 제품을 추천합니다.\n\n필요한 능력:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 지각\n\n- 텍스트 데이터 처리: 복잡한 문장과 우용어를 포함한 고객 질문을 인식하고 이해합니다.\n- 청각 처리: 시끄러운 환경에서도 말로된 질문을 필기하고 이해합니다.\n- 시각 처리: 비디오 지원 세션 중 시각 단서와 신체 언어를 해석합니다.\n\n2. 인식\n\n맥락 이해 및 인식:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 시간 인식: 계절적 추세와 피크 기간을 인식합니다.\n- 위치 인식: 지리적 위치 데이터를 이해합니다.\n- 개인 맥락 인식: 개별 고객, 그들의 이력 및 선호도를 이해합니다.\n\n기억:\n\n- 단기 기억: 최근 상호작용을 유지하기 위해 유지합니다.\n- 장기 기억: 과거 상호작용을 이용합니다.\n\n추론 및 분석:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 확률적 추론: 고객 상호작용에서 패턴을 식별하여 미래 행동을 예측합니다.\n- 연역적 논리: 논리적 프레임워크를 적용하여 문제 해결에 참여합니다.\n- 행동 분석: 고객 행동의 패턴을 이해하고 해석합니다.\n- 트렌드 분석: 현재 시장 트렌드와 계절별 데이터를 이해합니다.\n\n지식 활용과 적용\n\n- 의미 지식: 일반 세계 지식을 적용하여 질문을 이해하고 대답합니다.\n- 사건 지식: 관련 지원을 위해 특정 사건과 과거 경험을 활용합니다.\n- 선언적 지식: 정확한 응답을 위한 사실적 정보에 접근합니다.\n\n사회 및 감정지능\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 감정 인식: 고객의 감정을 감지하고 해석합니다.\n- 사회적 상호작용: 고객과 사회적으로 적절하게 소통합니다.\n- 마음의 이론: 고객의 요구를 추측하고 선제적으로 해결책을 제안합니다.\n- 관계 관리: 고객과의 확고한 관계를 구축하여 충성심을 유도합니다.\n\n창의성과 상상력\n\n- 상상력 있는 사고: 현재 문제를 넘어 새로운 가능성을 상상합니다.\n\n행동\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n디지털 상호작용:\n\n- 출력 생성: 빠르고 정확하며 맥락에 적합한 응답 생성.\n- 제품 추천 생성: 고객의 선호도 및 다른 관련 분석을 기반으로 제품을 추천.\n\n인간과의 소통 및 상호작용:\n\n- 대화의 연속성: 여러 상호작용 사이에서 맥락 유지.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n에이전트와 시스템 간 통신:\n\n- 에이전트간 협력: 다른 AI 시스템과 소통하여 행동을 동기화하고 통찰을 공유합니다.\n\n적응\n\n학습:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 체험 학습: 고객 행동에 대한 이해를 계속 향상시킵니다.\n\n적응:\n\n- 행동 적응: 피드백을 바탕으로 상호 작용 스타일을 조정합니다.\n- 인지 적응: 새로운 정보로 지식을 업데이트합니다.\n- 감정 적응: 감정적 반응을 조절합니다.\n\n이러한 통찰력 중 일부는 약간 놀라울 수도 있습니다. 예를 들어, AI 에이전트가 관계 관리를 기능으로 가지고 있어야 하는지, 또는 화면에 가상 몸체를 갖추고 비디오를 통해 '관측'할 수 있는 데이터 포인트의 새로운 배열에 대해 반응할 수 있는 AI 에이전트가 있어야 하는지에 대한 토의가 있을 수 있습니다. 확실히 개인 정보 보호 문제와 대응해야 할 문제가 많지만 완전히 배제해야 하는 개념은 아닙니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 기술과 기술을 통해 역량 구축하기\n\n이 문서는 기술과 기술을 평가하는 데 중점을 두지 않지만, 위의 연습을 거치고 나서 자연스럽게 나타나는 질문에 대해 다룰 것입니다. LLM이 대부분의 이러한 역량을 기본 제공해주는 도구를 제공하는 것은 아닌가요?\n\nLLM은 분명히 기술적 발전을 가파르게 이끌었지만, 간단한 답은 \"아니요\"입니다. 그리고 추론 및 분석 능력 같은 경우에는 LLM이 추론이나 분석같이 보이는 것을 인상적으로 시뮬레이션할 수 있지만, 인간의 그러한 역량에는 훨씬 미치지 못합니다. 요약하면, LLM은 이러한 많은 역량을 가능하게 하는 강력하지만 완전히 신뢰할 수 없는 단축키를 제공합니다. 이들은 인공 일반 지능(AGI) 아이디어 주변에 왜 그렇게 많은 관심이 있는지 설명해줄만한 매우 중요한 진화적 단계를 나타냅니다. 그것이 실제로 무엇을 의미하는지에 대한 정의는 논쟁의 대상이지만, 실현된다면 위에서 설명한 인지/행동 역량의 많은 부분을 활성화하는 기술 솔루션이 될 수 있을 것입니다.\n\n## 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI 에이전트 능력 엔지니어링 프레임워크가 사용자분의 AI 에이전트 능력을 정의하는 데 유용한 접근 방식으로 느껴지길 바랍니다. 인지 및 행동과학에서의 개념을 통합하여, 이 프레임워크는 AI 에이전트가 복잡한 작업을 수행하는 데 필요한 능력을 개발하는 데 도움이 되도록 지침을 제공합니다. 이 프레임워크는 비교적 밀집되어 있으며 시간이 지남에 따라 확실히 발전할 것입니다. 이 시점에서 주요 요점은 인지, 사고, 행동, 적응을 중심으로 한 정신적 모델입니다. 이 네 가지 고수준 개념만으로도 에이전트 능력을 효율적으로 조직화하고 개발하는 매우 견고한 기반을 제공합니다.\n\n이 글을 읽어 주셔서 감사합니다. 앞으로 이 프레임워크를 더 발전시키고 AI 에이전트 엔지니어링 프레임워크의 다른 측면을 확장할 예정입니다. 이 프레임워크나 기타 주제에 대해 더 자세히 논의하고 싶으시면 언제든지 LinkedIn을 통해 연락 주시기 바랍니다.\n\n별도로 언급되지 않는 한, 이 글의 모든 이미지는 저자가 찍은 것입니다.","ogImage":{"url":"/assets/img/2024-06-20-AIAgentCapabilitiesEngineering_0.png"},"coverImage":"/assets/img/2024-06-20-AIAgentCapabilitiesEngineering_0.png","tag":["Tech"],"readingTime":10},{"title":"이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지","description":"","date":"2024-06-20 18:20","slug":"2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV","content":"\n\n![Image](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png)\n\n객체 감지는 딥러닝의 한 분야 중 하나로, 많은 발전이 이루어졌습니다. 다양한 모델을 사용하여 우리는 사진에서 물체를 감지할 수 있고, 그 결과로 비디오에서도 물체를 감지할 수 있습니다. 요즘에는 웹캠 이미지를 사용한 실시간 객체 감지가 흔한 일입니다!\n\n이 튜토리얼에서는 TensorFlow를 사용하여 객체 감지 시스템을 구축할 것입니다. 구체적으로 TensorFlow Object Detection API를 사용할 것입니다. 단계별로 모든 필요한 종속성을 설치하고, TensorFlow Model Zoo의 사전 훈련된 모델을 살펴보고, 객체 감지기를 구축할 것입니다.\n\n다시 말해, 이 튜토리얼을 읽은 후에는...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- TensorFlow 기반 객체 검출기를 구축하기 위해 설치해야 하는 것을 알게 되었습니다.\n- 사전 훈련된 모델을 찾고 시스템에 다운로드하는 위치를 알고 있습니다.\n- 사진 및 비디오와 함께 사용할 수 있는 실제 객체 검출 시스템을 구축했습니다.\n\n그리고 이미지는 항상 수많은 말보다 많은 것을 전달합니다. 아래 시스템을 만들어 보세요!\n\n![이미지](https://miro.medium.com/v2/resize:fit:1200/1*qa_qXkly0MvO82-7uV7LpA.gif)\n\n한번 살펴보시죠!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 객체 탐지기 구축: 필수 조건\n\n텐서플로 Object Detection API를 사용하여 객체 탐지 시스템을 구축하려면 다음 세 가지 단계를 완료해야 합니다:\n\n- TensorFlow 및 OpenCV 설치하기. 우리는 TF 기능을 위해 TensorFlow가 필요하며, 이미지 I/O를 위해 OpenCV가 필요합니다. 보통 시스템에 이미 설치되어 있지만, 완전성을 위해 여기에 포함시켰습니다.\n- TensorFlow Object Detection API 설치하기. 이 추가 기능 세트는 별도로 설치해야 합니다. 어떻게 설치할 수 있는지 살펴보겠습니다.\n- TensorFlow Model Zoo에서 적절한 사전 학습된 모델 찾기. TensorFlow의 제작자들이 다양한 모델 아키텍처를 사용하여 사전 학습된 여러 모델을 TensorFlow Model Zoo에 넣었습니다. 간단히 살펴보고 모델을 선택할 것입니다.\n\n![이미지](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## TensorFlow와 OpenCV 설치하기\n\n실제 객체 탐지기를 구축하기 전에 TensorFlow와 OpenCV를 설치해야 합니다.\n\n여기서는 이미 시스템에 Python이 설치되어 있다고 가정합니다. 그렇지 않은 경우 먼저 Python을 설치해 주세요.\n\n요즘은 TensorFlow를 설치하는 것이 정말 쉽습니다. Python에 액세스할 수 있는 터미널에서 다음을 실행하면 됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 최신 버전의 pip가 필요합니다\npip install --upgrade pip\n\n# CPU 및 GPU용 현재 안정적인 릴리스\npip install tensorflow\n```\n\n먼저 pip를 최신 버전으로 업그레이드하고 TensorFlow를 설치합니다. 이제 CPU 또는 GPU 버전을 수동으로 지정해야 했던 것이 오늘에는 그렇지 않습니다. 그냥 tensorflow를 설치하면 GPU 버전이 정확하게 설정된 경우 GPU 버전이 자동으로 설치됩니다. 실제로 GPU와 CPU 사이를 자유롭게 전환할 수 있지만, 이에 대해서는 나중에 다시 이야기하겠습니다.\n\nOpenCV를 설치하는 것도 어렵지 않습니다: pip install opencv-python으로 해결할 수 있습니다.\n\n이제 기본 패키지가 설치되었으므로 TensorFlow Object Detection API를 살펴볼 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_2.png\" /\u003e\n\n강아지... 와우!\n\n## TensorFlow Object Detection API 설치\n\nGitHub에서 tensorflow/models에서 Object Detection API를 찾을 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이름에서 알 수 있듯이, 이것은 객체 감지 목적으로 사용할 수 있습니다. 특히, 사전 훈련된 모델을 로드하고 이미지 및 비디오에 경계 상자를 추가하는 기능을 제공합니다. 우리의 객체 감지 시스템이 이러한 API를 활용할 수 있다는 것은 우리가 모든 것을 직접 개발할 필요가 없다는 멋진 점입니다.\n\n나중에 사전 훈련된 모델을 살펴볼 것입니다. 먼저 Object Detection API를 설치해 봅시다. 이것은 시스템에 Git이 설치되어 있는 것을 가정합니다. 또한 protoc 명령을 실행할 수 있는지 확인해 주세요. 여기서 확인하는 방법을 찾아보세요.\n\n- 먼저 tensorflow/models 저장소 전체를 복제합니다. 한 단계 깊이만 복제하도록 주의하세요. 다음 명령을 실행하여 저장소를 복제하세요: git clone --depth 1 https://github.com/tensorflow/models\n- 이제 models/research/ 디렉토리로 이동한 다음 protoc object_detection/protos/*.proto --python_out=. 명령을 실행하세요.\n- 그런 다음 cp object_detection/packages/tf2/setup.py 명령을 사용하여 설정 파일을 현재 디렉토리로 복사합니다.\n- 마지막으로 python -m pip install 명령을 통해 Object Detection API를 pip를 통해 설치하세요.\n\n## TensorFlow 모델 동물원: 객체 감지용 사전 훈련된 모델\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희 물체 감지 시스템은 TensorFlow 모델 위에 구축될 예정이에요. 이 모델은 다양한 종류의 물체를 감지할 수 있어요. 이 모델을 훈련하는 과정은 다음과 같아요:\n\n- 다양한 물체가 포함된 많은 이미지를 수집하는 것\n- 이러한 이미지들에 레이블을 달아 모든 클래스가 균형을 이루도록 하는 것\n- 모델을 훈련하는 것\n\n이 과정은 많은 노력이 필요할 거예요. 다행히 TensorFlow 팀은 TensorFlow Detection Model Zoo에서 다양한 사전 훈련된 물체 감지 모델을 제공하고 있어요.\n\n이러한 물체 감지기는 이미 훈련이 완료되어 있으며 TensorFlow Object Detection API에서 이용할 수 있어요 (괄호 안에는 내부 모델 구조가 나와 있어요):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- CenterNet (HourGlass104, Resnet50 V1, Resnet101 V1, Resnet50 V2).\n- EfficientDet (D0, D1, D2, D3, D4, D5, D6, D7).\n- SSD (MobileNet V1 FPN, V2, V2 FPNLite; ResNet50 V1; Resnet101 V1).\n- Faster R-CNN (ResNet50; ResNet101; ResNet152; Inception ResNet V2).\n- Mask R-CNN (Inception ResNet V2).\n- ExtremeNet.\n\n당연히 직접 모델을 만드실 수도 있습니다. 하지만 이 강좌에서 다루지는 않습니다.\n\n오늘은 SSD MobileNet V2 FPNLite 640x640 모델을 사용할 것입니다. Zoo에서 원하는 모델을 선택하실 수 있지만, 이 사전 훈련된 모델은 용량이 20MB밖에 되지 않아서 빠른 인터넷 속도를 가진 많은 사람들이 다운로드할 수 있습니다.\n\n이제 우리의 디텍터를 만들어봅시다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_3.png\" /\u003e\n\n## 객체 탐지기 생성\n\n여기서는 객체 탐지 시스템을 구축하는 방법을 살펴보겠습니다. 이 과정은 세 가지 별개이지만 순차적인 단계로 나눌 수 있습니다:\n\n- 기반을 놓기. 이곳에서는 중요한 imports를 지정하고, 클래스를 정의하고, 초기화 작업을 설명하고, 준비 작업 정의를 작성할 것입니다.\n- 탐지 함수 작성. 이것이 탐지기의 핵심입니다. 이것은 일반적으로 탐지를 수행하고, 특히 이미지와 비디오에 대한 예측을 생성할 수 있게 합니다.\n- 탐지 호출 생성. 마지막으로, 우리의 탐지기가 준비되면 사용할 수 있도록 다음 추가 코드를 추가할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드 에디터를 열고 objectdetector.py와 같은 Python 파일을 생성해 주세요. 코드 작성 시작할까요?\n\n## 파트 1: 기반 구축하기\n\nTensorFlow Object Detection API를 기억하시나요? 이것은 물체 감지기를 구축하기 위한 TensorFlow 위의 프레임워크입니다. 다시 말해, 머신 러닝 모델을 만들기 위한 잘 알려진 라이브러리 위에 또 다른 층이란 의미죠. 이 API 위에 Object Detection API를 사용하는 물체 감지기 층을 추가할 계획입니다.\n\n이 TFObjectDetector의 기반을 구축하기 위해서는 Python 임포트 추가, 필요한 경우 GPU 비활성화, TFObjectDetector 클래스 작성 및 초기화, 물체 감지기를 위한 설정 메커니즘 작성, 마지막으로 몇 가지 도우미 함수를 작성해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 파이썬 라이브러리 가져오기\n\n첫 번째 코드는 항상 파이썬 라이브러리를 가져와야 합니다. 오늘도 마찬가지에요:\n\n```js\n# 모델 라이브러리 지정\nfrom object_detection.builders import model_builder\nfrom object_detection.utils import config_util\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import visualization_utils as viz_utils\nimport cv2\nimport numpy as np\nimport os\nimport tensorflow as tf\n```\n\nobject_detection 패키지에서 많은 함수를 가져왔네요 - 이는 TensorFlow Object Detection API를 나타냅니다. 모델 빌더를 사용하여 감지 모델(예: SSD MobileNet 모델)을 구축할 거에요. config_util을 사용하면 TensorFlow에 올바른 모델을 로드하도록 알려주는 구성을 로드할 수 있습니다. 클래스 이름을 나타내는 레이블은 label_map_util을 사용하여 로드할 수 있고, viz_utils는 이미지나 비디오에 경계 상자를 추가하는 데 유용하게 사용될 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nOpenCV (cv2)는 이미지의 입력 및 출력에 사용되며, NumPy (np)는 숫자 처리에 사용되고, os는 운영 체제 기능에 사용되며, 마지막으로 TensorFlow를 import합니다.\n\n## 필요한 경우 GPU 비활성화\n\n두 번째 단계는 GPU를 비활성화하는 것인데, 이것은 선택 사항입니다 — 다시 말해, 원할 경우에만 수행하십시오. 특히 GPU를 보유하고 있지만 구성이 잘못된 경우에 유용할 수 있습니다. 그때 CUDA 가시 장치를 환경에서 모두 지워야 합니다. TensorFlow의 GPU 버전을 사용하지 않는 경우에는 이 코드를 생략할 수 있습니다.\n\n```js\n# 필요한 경우 GPU 비활성화\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 클래스와 초기화자 만들기\n\n이제 진짜 작업을 시작할 시간입니다. 우리의 객체 탐지기의 모든 기능을 다루는 TFObjectDetector 클래스를 만들어봅시다.\n\n```js\n# 객체 탐지기 생성\nclass TFObjectDetector():\n```\n\n우리는 즉시 __init__ 정의를 추가합니다. 이는 클래스의 생성자를 나타내며 다시 말해, TFObjectDetector를 로딩하는 즉시 실행됩니다. 입력 값으로 다음을 받는다는 것을 주목하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 객체 검출에 대한 경로는 시스템에 설치된 Object Detection API의 TensorFlow 2.x 구성 파일 경로를 나타냅니다.\n- 실행 중인 모델의 모델 체크포인트 경로 (우리의 경우 SSD MobileNet 모델).\n- 텍스트 레이블에 클래스 ID를 매핑하는 사전을 구성할 수 있도록 하는 레이블 파일의 경로.\n- 모델 이름.\n\n⚠ 나중에 실제로 검출기를 사용할 때 상황에 맞게 입력값을 설정하는 방법을 설명할 것입니다.\n\n생성자에서는 여러 작업을 수행합니다. 우선, 입력값을 검출기 전체에 재사용할 수 있도록 많은 인스턴스 변수를 채웁니다. 또한 Object Detection API 폴더에 있는 파이프라인 구성을 로드하고, 우리 모델에 해당하는 구성 파일을 로드하며, 마지막으로 self.setup_model()을 호출합니다.\n\n이로써 모델의 설정 메커니즘을 시작하며, 지금 바로 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 객체 검출기 생성\nclass TFObjectDetector():\n\n  # 생성자\n  def __init__(self, path_to_object_detection='./models/research/object_detection/configs/tf2',\\\n    path_to_model_checkpoint='./checkpoint', path_to_labels='./labels.pbtxt',\\\n      model_name='ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'):\n    self.model_name = model_name\n    self.pipeline_config_path = path_to_object_detection\n    self.pipeline_config = os.path.join(f'{self.pipeline_config_path}/{self.model_name}.config')\n    self.full_config = config_util.get_configs_from_pipeline_file(self.pipeline_config)\n    self.path_to_model_checkpoint = path_to_model_checkpoint\n    self.path_to_labels = path_to_labels\n    self.setup_model()\n```\n\n## 설정 매커니즘\n\n설정 매커니즘은 모델을 백그라운드에서 설정하고 객체 검출기를 사용할 수 있게 만드는 역할을 담당합니다. 다음 단계로 구성됩니다:\n\n- __init__ 함수에서 로드된 모델 구성을 사용하여 모델을 빌드하는 과정.\n- 특정 상태로 모델을 복원하는 단계, 즉 훈련된 특정 상태로 모델을 복원합니다.\n- 예측을 생성하는 데 사용할 수있는 tf.function인 모델 검출 함수를 검색하는 단계.\n- 클래스 ID 및 텍스트 라벨 간의 매핑을 생성하는 단계으로, 라벨을 준비하는 과정입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 단계들의 실행을 setup_model() 정의로 그룹화해 봅시다. 이 정의는 위에서 지정된 __init__ 정의에서 호출되며, 따라서 우리의 객체 탐지기를 생성할 때 호출됩니다.\n\n```js\n  # 모델 설정\n  def setup_model(self):\n    self.build_model()\n    self.restore_checkpoint()\n    self.detection_function = self.get_model_detection_function()\n    self.prepare_labels()\n```\n\n그 다음으로 build_model()를 만들어 봅시다:\n\n```js\n  # 탐지 모델 빌드\n  def build_model(self):\n    model_config = self.full_config['model']\n    assert model_config is not None\n    self.model = model_builder.build(model_config=model_config, is_training=False)\n    return self.model\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 정의는 구성을 검색하고 존재하는지 확인한 뒤 모델을 빌드합니다. 이 모델은 인스턴스 변수에 할당되어 객체 탐지기 전반에 걸쳐 재사용될 수 있도록 합니다.\n\nrestore_checkpoint() 함수를 사용하면 TensorFlow Detection Model Zoo에서 제공하는 체크포인트 위치/상태로 모델을 되돌릴 수 있습니다.\n\n```js\n  # 모델로 체크포인트 복원\n  def restore_checkpoint(self):\n    assert self.model is not None\n    self.checkpoint = tf.train.Checkpoint(model=self.model)\n    self.checkpoint.restore(os.path.join(self.path_to_model_checkpoint, 'ckpt-0')).expect_partial()\n```\n\n그런 다음 탐지를 위한 tf.function을 생성할 수 있습니다. 이 함수는 모델을 활용하여 이미지를 전처리하고 예측을 생성한 후 감지를 처리하고 모든 것을 반환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n  # 탐지를 위한 tf.function 가져오기\n  def get_model_detection_function(self):\n    assert self.model is not None\n    \n    @tf.function\n    def detection_function(image):\n      image, shapes = self.model.preprocess(image)\n      prediction_dict = self.model.predict(image, shapes)\n      detections = self.model.postprocess(prediction_dict, shapes)\n      return detections, prediction_dict, tf.reshape(shapes, [-1])\n    \n    return detection_function\n```\n\n마지막으로, prepare_labels()라는 정의를 생성합니다. TensorFlow의 사람들에 의해 만들어졌으며 클래스 식별자를 텍스트 레이블로 매핑하는 책임이 있습니다. 이것은 이 인스턴스 변수로 설정됩니다.\n\n```js\n  # 레이블 준비\n  # 출처: https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\n  def prepare_labels(self):\n    label_map = label_map_util.load_labelmap(self.path_to_labels)\n    categories = label_map_util.convert_label_map_to_categories(\n        label_map,\n        max_num_classes=label_map_util.get_max_label_map_index(label_map),\n        use_display_name=True)\n    self.category_index = label_map_util.create_category_index(categories)\n    self.label_map_dict = label_map_util.get_label_map_dict(label_map, use_display_name=True)\n```\n\n## 도우미 함수들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지 우리는 객체 탐지기를 준비할 수 있는 기반을 만들었습니다. 이 부분을 완료하려면 두 가지 더 도와주는 함수를 만들기만 하면 됩니다. 첫 번째 함수는 키포인트 튜플을 재구성하고, 두 번째 함수는 이미지를 준비합니다. 즉, 이미지를 텐서로 변환해줍니다.\n\n```js\n  # Get keypoint tuples\n  # 출처: https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\n  def get_keypoint_tuples(self, eval_config):\n    tuple_list = []\n    kp_list = eval_config.keypoint_edge\n    for edge in kp_list:\n      tuple_list.append((edge.start, edge.end))\n    return tuple_list\n\n  \n  # Prepare image\n  def prepare_image(self, image):\n    return tf.convert_to_tensor(\n      np.expand_dims(image, 0), dtype=tf.float32\n    )\n```\n\n## 파트 2: 탐지 함수 작성\n\n와우, 이미 2부에 도착했네요! 이번에는 탐지 함수를 작성할 거예요. 더 자세히 말하면, 세 가지 정의를 만들 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 일반 탐지 기능입니다. 이 기능은 이미지 및 비디오 감지에 재사용할 수 있는 일반 탐지 코드를 포함하고 있습니다.\n- 이미지 감지입니다. 이 코드는 이미지 내 객체 감지를 위해 특히 사용됩니다.\n- 비디오 감지입니다. 이 코드는 비디오 내 객체 감지를 위해 사용됩니다.\n\n## 일반 탐지 기능\n\n첫 번째 정의는 일반 탐지 기능입니다. 이곳에서 일반은 이미지와 비디오에서 감지하는 기능을 공유한다는 뜻입니다. 다시 말해, 무의미하게 두 번 추가할 필요가 없는 것들을 포함합니다! 다음 세그먼트가 포함되어 있습니다:\n\n- 우선, 감지 함수가 None이 아닌지 확인합니다 (위의 Part 1에서). 이는 설정되어 있지 않으면 감지를 수행할 수 없음을 의미합니다.\n- 이미지를 복사하고 텐서로 변환하여 준비합니다. 그런 다음 예측이 포함된 사전 및 모양 정보를 가진 객체를 생성합니다.\n- 키포인트가 있는 경우 사용합니다.\n- Object Detection API에서 제공하는 viz_utils API를 사용하여 예측과 함께 바운딩 박스를 이미지에 추가합니다.\n- 마지막으로 바운딩 박스가 있는 이미지를 반환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\n# 객체 감지 실행\ndef detect(self, image, label_offset = 1):\n    # 감지 함수가 있는지 확인\n    assert self.detection_function is not None\n    \n    # 이미지 준비 및 예측 수행\n    image = image.copy()\n    image_tensor = self.prepare_image(image)\n    detections, predictions_dict, shapes = self.detection_function(image_tensor)\n\n    # 제공된 키포인트 사용\n    keypoints, keypoint_scores = None, None\n    if 'detection_keypoints' in detections:\n        keypoints = detections['detection_keypoints'][0].numpy()\n        keypoint_scores = detections['detection_keypoint_scores'][0].numpy()\n    \n    # 출력 이미지/프레임에 시각화 수행\n    viz_utils.visualize_boxes_and_labels_on_image_array(\n        image,\n        detections['detection_boxes'][0].numpy(),\n        (detections['detection_classes'][0].numpy() + label_offset).astype(int),\n        detections['detection_scores'][0].numpy(),\n        self.category_index,\n        use_normalized_coordinates=True,\n        max_boxes_to_draw=25,\n        min_score_thresh=.40,\n        agnostic_mode=False,\n        keypoints=keypoints,\n        keypoint_scores=keypoint_scores,\n        keypoint_edges=self.get_keypoint_tuples(self.full_config['eval_config']))\n    \n    # 이미지 반환\n    return image\n```\n\n## 이미지용 감지 함수\n\n이제 어떤 이미지 위의 객체를 감지하는 것이 쉽습니다. OpenCV를 사용하여 경로에서 이미지를 읽고, 일반적인 감지 정의를 호출한 후 결과를 출력 경로에 작성하는 것으로 간단하게 수행할 수 있습니다.\n\n```python\n# 폴더에서 이미지 예측\ndef detect_image(self, path, output_path):\n\n    # 이미지 로드\n    image = cv2.imread(path)\n\n    # 객체 감지 수행 및 출력 파일에 추가\n    output_file = self.detect(image)\n    \n    # 출력 파일을 시스템에 작성\n    cv2.imwrite(output_path, output_file)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 비디오용 Detect 함수\n\n비디오에서 객체를 감지하는 것은 조금 더 어렵지만 여전히 매우 쉽습니다. 비디오는 종종 초당 25프레임의 이미지로 이루어진 이미지 집합에 불과하다는 것을 상기해주세요. 이 특성을 활용하여 비디오에서 객체 감지를 수행할 것입니다!\n\n이 세그먼트는 다음 단계로 구성되어 있습니다:\n\n- 먼저 출력 비디오 라이터와 코덱을 설정합니다. 이를 통해 바운딩 박스가 그려진 각 프레임을 출력 비디오에 작성할 수 있습니다. 이것은 사실상 비디오 프레임을 바운딩 박스와 함께 한 프레임씩 재구성하는 것을 의미합니다.\n- 그런 다음 OpenCV의 VideoCapture 기능을 사용하여 경로에서 비디오를 읽습니다.\n- vidcap.read()를 사용하여 첫 번째 프레임(이미지)을 읽고 성공적으로 읽었는지 표시합니다. 프레임 수를 0으로 설정합니다.\n- 이제 프레임을 순환하며 감지를 수행하고(이것이 사실상 이미지에서의 감지임을 알아두세요!) 프레임을 출력 비디오에 작성합니다. 다음 프레임을 읽어 나가며, 더 이상 프레임을 읽을 수 없을 때(즉, frame_read != True가 될 때까지) 계속합니다.\n- 모든 프레임을 처리한 후 출력 비디오를 out.release()를 사용하여 해제합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n  # 폴더로부터 비디오를 예측합니다\n  def detect_video(self, path, output_path):\n    \n    # 코덱을 사용하여 출력 비디오 작성기를 설정합니다\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, 25.0, (1920, 1080))\n    \n    # 비디오를 읽어옵니다\n    vidcap = cv2.VideoCapture(path)\n    frame_read, image = vidcap.read()\n    count = 0\n    \n    # 각 프레임을 반복하면서 예측을 수행합니다\n    while frame_read:\n        \n      # 물체 감지를 수행하고 출력 파일에 추가합니다\n      output_file = self.detect(image)\n      \n      # 예측과 함께 프레임을 비디오에 작성합니다\n      out.write(output_file)\n      \n      # 다음 프레임 읽기\n      frame_read, image = vidcap.read()\n      count += 1\n        \n    # 비디오 파일을 릴리스합니다\n    out.release()\n```\n\n## 섹션 3: ​​감지 호출 생성\n\n1부 및 2부에서 TFObjectDetector 클래스를 작성하며 감지기를 완성했습니다. 이제 완료 되었으므로 호출해 보는 시간이 됐어요. 다음 코드로 호출할 수 있습니다.\n\n```js\nif __name__ == '__main__':\n  detector = TFObjectDetector('../../tf-models/research/object_detection/configs/tf2', './checkpoint', './labels.pbtxt', 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8')\n  detector.detect_image('./shop.jpg', './shopout.jpg')\n  detector.detect_video('./video.mp4', './videooutput.mp4')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 코드는 다음을 수행합니다:\n\n- 직접적으로 실행될 때, 즉 다른 클래스의 컨텍스트 내에서 실행되지 않을 때, 먼저 TFObjectDetector의 새 인스턴스를 생성합니다. 여기에서는 다음 정보를 전달합니다:\n- 클론된 TensorFlow 모델의 tf2 구성 폴더로의 절대 또는 상대 경로입니다.\n- 다운로드한 모델의 모델 체크포인트 폴더로의 절대 또는 상대 경로입니다. 사용하는 SSD MobileNet의 경우, 폴더를 해제하고 열면 ./checkpoint 폴더가 나타납니다. 거기를 참조하세요.\n- 클래스 인덱스와 레이블 이름 간의 매핑에 사용되는 레이블 파일의 절대 또는 상대 경로입니다. 없는 경우 TensorFlow Detection Model Zoo 모델 중 하나에 대해 여기에서 다운로드할 수 있습니다.\n- 모델의 이름입니다. 우리 경우에는 지정한 어려운 이름입니다. Model Zoo에서 다른 이름들 중 하나를 사용할 수도 있지만 그에 맞는 체크포인트를 사용해야 합니다.\n- ./shop.jpg라는 이미지에서 이미지 검출을 수행하고 결과물(즉, 바운딩 상자가 오버레이된 이미지)을 ./shopout.jpg에 저장합니다.\n- ./video.mp4라는 비디오에서 비디오 검출을 수행하고 출력물을 ./videooutput.mp4로 저장합니다.\n\n## 전체 모델 코드\n\n바로 코드로 이동하려는 사용자를 위해 전체 모델 코드는 내 Github 저장소에서 찾을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 객체 탐지기 실행하기\n\n이제 객체 탐지기를 실행한 결과를 살펴보겠습니다.\n\n이 사진들과 동영상은 Pexels 라이선스 하에 다운로드되어 사용되었습니다.\n\n## 사진에서\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_4.png)\n\n![Image 2](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_5.png)\n\n![Image 3](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_6.png)\n\n## On videos\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식으로 표시한 이미지 링크입니다.\n\n![이미지1](https://miro.medium.com/v2/resize:fit:1200/1*alxO3bGUDN3dzJnQ6wIoZQ.gif)\n\n![이미지2](https://miro.medium.com/v2/resize:fit:1200/1*hTy8jS8LYMGNPgJ6Q_jj2w.gif)\n\n## 요약\n\n머신 러닝에서 객체 감지에는 많은 유용한 사례가 있습니다. 이 튜토리얼을 통해 TensorFlow 객체 감지 API와 사전 훈련된 모델을 사용하여 이미지와 비디오에서 객체 감지를 수행하는 방법을 배웠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오늘의 글에서 무언가를 배워가셨나요? 궁금한 점, 의견 또는 제안이 있으면 언제든지 환영합니다. 읽어 주셔서 감사합니다!\n\n## 참고 자료\n\nTensorFlow, TensorFlow 로고 및 관련 상표는 Google Inc.의 상표입니다.\n\nTensorFlow. (2020, 9월 9일). TensorFlow/models. GitHub. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTensorFlow. (2020, 11). TensorFlow/models. GitHub. https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\n\nTensorFlow. (n.d.). TensorFlow/models. GitHub. https://github.com/tensorflow/models/tree/master/research/object_detection\n\n\"현대 합성곱 객체 탐지기의 속도/정확도 균형.\" 황재식, 라토드 비니트, 썬 첸, 주 만멍, 코라티카라 아니쉬, 파티 아드리아노, 피셔 이안, 우예노비치 세르게이, 송 양초, 과다라마 세르게이, 머피 케빈, CVPR 2017","ogImage":{"url":"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png"},"coverImage":"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png","tag":["Tech"],"readingTime":18},{"title":"인공지능의 마법 구슬 신경망이 인플레이션을 예측하는 방법","description":"","date":"2024-06-20 18:18","slug":"2024-06-20-AIsCrystalBallHowNeuralNetworksForecastInflation","content":"\n\n`\u003cimg src=\"/assets/img/2024-06-20-AIsCrystalBallHowNeuralNetworksForecastInflation_0.png\" /\u003e`\n\n인플레이션 예측에 대한 전통적인 방법은 과거 데이터와 복잡한 계량 경제 모델에 크게 의존하지만, 종종 빠르게 변화하는 경제 상황의 세세한 점을 포착하지 못할 때가 있습니다. (일부 우수한 모델을 제외하고) 이제는 신경망, 특히 LSTM 신경망이 예측 분석에 접근하는 방식을 혁신하고 있습니다.\n\n이 기사는 LSTM 모델을 핵심부터 만들고, 미국의 월별 인플레이션 지표 변화를 예측하는 데 사용하는 방법을 보여줍니다.\n\n# LSTM 부트캠프\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떤 것을 이해하는 가장 좋은 방법은 단순하게 생각하는 것입니다. 수학이나 복잡한 그래프가 필요하지 않고 순수한 직관과 논리만 있으면 돼요. 책을 읽고 있다고 상상해봐요. 한 장을 넘겨 다음 장으로 넘어갈 때, 이전 장의 중요한 내용을 기억해야 현재 장을 이해할 수 있어요. 이전 장에서의 정보를 상기시킬 수 있는 능력은 이야기를 따라가는 데 도움을 줍니다. 이제 컴퓨터가 이 책을 어떻게 읽을 수 있을지 생각해봐요.\n\n인간과 달리 컴퓨터는 새로운 정보를 처리할 때 이전 정보를 기억하는 데 어려움을 겪는 경향이 있어요 (미래에 우리를 지배하기 전에 컴퓨터들에게 우리의 이점을 남겨둘 수 있어 다행이죠). 여기서 장기 단기 기억망(LSTM) 네트워크가 등장합니다 — 이들은 당신이 책을 읽을 때와 같이 컴퓨터가 중요한 세부 정보를 시간을 초월하여 기억하는 데 도움을 줍니다. 그래서 LSTM 네트워크에서 중요한 키워드는 기억입니다. 하지만 LSTMs가 정말 무엇인가요?\n\n이들은 데이터 시퀀스를 처리하기 위해 설계된 특별한 종류의 인공 신경망입니다. 이들은 오랜 기간동안 정보를 기억하는 문제를 해결하기 위해 만들어졌는데, 일반적인 신경망은 이를 다루기 어렵습니다.\n\nLSTMs는 역사를 공부하면서 중요한 사건을 메모할 수 있는 노트북을 가지고 있는 것과 같아요. 언제든 필요할 때 이러한 노트를 다시 참고하여 이전 정보를 상기시킬 수 있습니다. 이 노트북이 바로 LSTM의 기억입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`LSTM`의 기능에 대해 이해해야 할 내용이 대부분입니다. 재미있는 세부사항은 전문가들에게 맡기고, 우리의 목표인 기계 학습 알고리즘을 사용한 인플레이션 예측으로 넘어가 보겠습니다.\n\n# LSTM을 이용한 인플레이션 예측\n\n우선, 분석하려는 데이터 유형을 이해해야 합니다. 미국 소비자 물가지수(CPI)는 도시 소비자가 일상적으로 소비하는 장바구니의 상품 및 서비스에 대한 가격 변동의 평균 변화를 측정하는 중요한 경제 지표입니다. 기본적으로 CPI는 음식, 의류, 주거, 연료, 교통, 의료 서비스 및 사람들이 일상적으로 구입하는 다른 상품 및 서비스에 대한 가격 변동을 모니터링하여 생활비 변동을 추적합니다.\n\n작업 계획은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 필요한 Python 라이브러리 및 미국 세인트루이스 연방준비은행에서 공개한 인플레이션(CPI) 데이터를 가져옵니다.\n- 데이터를 정리하고 훈련 세트와 테스트 세트로 분할합니다.\n- 설명 변수(예측 변수)를 선택합니다. 이 경우 미래 값을 예측하기 위해 과거 값을 사용하는 지연 변화가 사용됩니다. 이는 데이터에서 자기 상관성과 예측 가능성의 형태를 의미합니다.\n- 데이터를 훈련하고 테스트 데이터에서 예측합니다.\n- 정확도(적중률)와 평균 제곱근 오차(RMSE)를 사용하여 모델을 평가합니다.\n\n아래 코드를 사용하여 알고리즘을 구현하세요:\n\n```js\n# 필요한 라이브러리 가져오기\nimport pandas_datareader as pdr\nimport matplotlib.pyplot as plt\n\n# 히스토리컬 데이터의 시작 및 끝 날짜 설정\nstart_date = '1950-01-01'\nend_date = '2024-01-23'\n\n# 데이터프레임 생성 및 CPI 데이터 다운로드\ndata = pdr.DataReader('CPIAUCSL', 'fred', start_date, end_date)\n\n# CPI 데이터프레임에 nan 값이 있는지 확인\ncount_nan = data['CPIAUCSL'].isnull().sum()\n\n# 결과 출력\nprint('CPI 데이터프레임에 있는 nan 값의 수: ' + str(count_nan))\n\n# CPI를 연간 변화율로 변환\ndata = data.pct_change(periods=12, axis=0) * 100\n\n# 행에서 nan 값을 제거\ndata = data.dropna()\n\n# 라이브러리 가져오기\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nimport numpy as np\nimport pandas_datareader as pdr\nfrom sklearn.metrics import mean_squared_error\n\n# 데이터 전처리 함수 정의\ndef data_preprocessing(data, num_lags, train_test_split):\n    # 훈련을 위한 데이터 준비\n    x = []\n    y = []\n    for i in range(len(data) - num_lags):\n        x.append(data[i:i + num_lags])\n        y.append(data[i + num_lags])\n    # 데이터를 넘파이 배열로 변환\n    x = np.array(x)\n    y = np.array(y)\n    # 데이터를 훈련 및 테스트 세트로 분할\n    split_index = int(train_test_split * len(x))\n    x_train = x[:split_index]\n    y_train = y[:split_index]\n    x_test = x[split_index:]\n    y_test = y[split_index:]\n\n    return x_train, y_train, x_test, y_test\n\n# 훈련 및 테스트 값을 그래프로 그리는 함수 정의\n# 나머지 코드는 생략합니다.\n``` \n\n이 코드는 제가 최신 딥러닝 도서에서 가져온 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n플로팅 함수는 다음 차트를 제공해야 합니다:\n\n![차트](/assets/img/2024-06-20-AIsCrystalBallHowNeuralNetworksForecastInflation_1.png)\n\n이전에 정의한 함수를 사용하여 알고리즘을 평가해 봅시다:\n\n```js\n# 성능 평가\nprint('---')\nprint('정확도(학습) = ', round(calculate_accuracy(y_predicted_train, y_train), 2), '%')\nprint('정확도(테스트) = ', round(calculate_accuracy(y_predicted, y_test), 2), '%')\nprint('RMSE(학습) = ', round(np.sqrt(mean_squared_error(y_predicted_train, y_train)), 10))\nprint('RMSE(테스트) = ', round(np.sqrt(mean_squared_error(y_predicted, y_test)), 10))\nprint('상관관계(시계열 예측/학습) = ', round(np.corrcoef(np.reshape(y_predicted_train, (-1)), y_train)[0][1], 3))\nprint('상관관게(예측/테스트) = ', round(np.corrcoef(np.reshape(y_predicted, (-1)), np.reshape(y_test, (-1)))[0][1], 3))\nprint('---')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전 코드의 출력은 다음과 같습니다:\n\n```js\n---\nAccuracy Train =  62.58 %\nAccuracy Test =  70.51 %\nRMSE Train =  0.3287812546\nRMSE Test =  0.3275757807\nCorrelation In-Sample Predicted/Train =  0.522\nCorrelation Out-of-Sample Predicted/Test =  0.509\n---\n```\n\n예측 모델을 개발하고 배포할 때, 특히 금융과 같은 중요한 분야에서는 어떤 모델에 의존하기 전에 광범위한 연구를 수행하는 것이 중요합니다. 꼼꼼히 공부해보세요!","ogImage":{"url":"/assets/img/2024-06-20-AIsCrystalBallHowNeuralNetworksForecastInflation_0.png"},"coverImage":"/assets/img/2024-06-20-AIsCrystalBallHowNeuralNetworksForecastInflation_0.png","tag":["Tech"],"readingTime":5},{"title":"사함이 없이 나는 누구일까요","description":"","date":"2024-06-20 18:17","slug":"2024-06-20-ngunitsinoakokungwalangmedalya","content":"\n\n\n![Img](/assets/img/2024-06-20-ngunitsinoakokungwalangmedalya_0.png)\n\nMaraming tao ang nagsasabi na matalino raw ako, dahil marami akong natatanggap na award sa paaralan. Sinasabi nila na malayo raw ang mararating ko dahil masipag ako mag-aral. Sabi nila, magaling raw ako dahil marami akong natanggap na medalya noong ako ay nagtapos ng High School.\n\nIto ang mga salitang madalas kong marinig saanman. Mga salitang nakakataba ng puso. Para silang musika sa aking tenga na parang magpapatulog sa akin ng mahimbing kapag paulit-ulit kong pinapakinggan.\n\nNgunit sa sandaling iyon, hindi maiwasan na itanong sa sarili—sino nga ba ako kung walang medalya?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어릴 적에는 저의 꿈이 그냥 많은 장난감을 가지고 놀고, 잔디밭에서 놀고, 그네를 타고, 들판을 뛰어다니는 것이었어요. 모든 것에 능숙해질 필요가 있을 거라는 생각은 전혀 들지 않았고, 상을 받을 생각은 하지 않았어요. 그때의 제 삶은 단순했어요. 그냥 숙제를 해결할 수 있기만 하면 충분했어요. 수학 숙제를 살아남는 것만 하는 것이라도 말이에요.\n\n하지만 운명의 궁금한 일들이 일어나고, 삶의 경로가 달라지고, 운명이 변할 수 있다는 것을 깨달았어요. 고등학교에 입학하면서 적응하는 데 어려움을 겪었어요. 이곳은 제가 익숙한 환경이 아니었어요. 제가 지낼 예정이었던 종류의 사람들도 아니었어요. 그리고 그 때 나는 내가 다른 세계에 있다는 것을 깨달았어요.\n\n저는 학업적인 인정을 갈망해요.\n모든 사람들의 주목을 받길 갈망해요.\n\n“막 통과만 하면 될 거야”라는 말로 만족했던 어린이가 더 이상 실패를 겁내는 존재가 된 줄 누가 알았을까요? 장난감을 가지는 것만 꿈꿨던 어린이가 이제는 수상과 메달을 받는 꿈을 꿀 줄 누가 알았을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nelementary 학교 때부터 우수생이 되기 시작했는데 그 때는 신경 쓰지 않았어요. 그런데 고등학교에 들어서니 항상 앞서가려는 개처럼 달리기를 하듯 했어요. 대회에서 이기면 주위 사람들로부터 메달, 증명서, 그리고 칭찬을 받았죠.\n\n최고에 오르는 것은 정말로 피곤하고 지칩니다.\n\n난 다른 사람들이 무엇을 말할까 두려워하고, 그들의 기대치를 충족시킬 수 없을까 두려워하며, 실패를 두려워해서 그랬던 걸 깨달았어요.\n\n옛날에는 꿈이 간단했어요 - 탐험하고 배우고, 그 순간 속에서 즐거움을 찾는 거였죠. 그런데 어느 순간 모든 게 변해버렸어요. 다른 사람들의 높은 기대치에 부응하기 위해 우수하게 나아가고, 최고가 되어야 한다는 필요성이 저를 흡수했죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내가 한때 소중히 여기던 메달과 성취들이 이제는 무거운 짐이 되어, 끊임없이 성과를 거듭 증명하고 가치를 입증해야 하는 압박으로 나를 뒤덮고 있다. 내가 정말 내 인생을 살고 있는 것인지, 아니면 누군가의 성공 이상을 쫓고 있는 것인지 의심하기도 한다.\n\n나의 어린 시절에 즐거움을 찾았던 마음이 어디로 사라진 걸까? 엄청 어려워. 실패를 기다리는 사람들 속에서 갇혀 있는 것은 정말 힘들다. 완전히 포위당한 기분인데, 그들의 초상도 미달할까 두려움 속에서 숨이 막히는 듯하다.\n\n하지만 말하길, 모든 일엔 이유가 있다.\n\n메달과 인증서 없이 나를 알 수는 없지만, 왜 그러했는지는 안다. 내가 받은 각각의 메달에는 이야기가 있다. 이것은 내 목에 매다는 단순한 메달이 아니다. 이것은 내 정체성을 가진 존경스러운 것이다. 그것을 얻기 위해 내 눈물, 피, 땀을 바쳤기 때문이다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n나의 소중한 어린이, 널 내가 세운 높은 벽 안에 감췄지만 이제는 널 자유롭게 놓아줄게. 내가 너를 어두운 길로 이끌었던 점 용서해줘.","ogImage":{"url":"/assets/img/2024-06-20-ngunitsinoakokungwalangmedalya_0.png"},"coverImage":"/assets/img/2024-06-20-ngunitsinoakokungwalangmedalya_0.png","tag":["Tech"],"readingTime":3},{"title":"산불에 대해 자세히 알아보기","description":"","date":"2024-06-20 18:15","slug":"2024-06-20-DelvingintoWildfires","content":"\n\n# 머신 러닝 및 데이터로부터의 통찰\n\n![Wildfire](/assets/img/2024-06-20-DelvingintoWildfires_0.png)\n\n야생 산불 데이터를 심층적으로 분석한 후, FireVision의 일부인 머신 러닝 (ML) 모델을 구축했습니다. 빠르게 증가하는 산불 문제는 적절한 데이터와 지표가 있다면 매우 실행 가능하다는 확고한 믿음으로 나타났습니다. 산불에는 무작위성 요소가 있지만, 미친 듯한 일은 방법이 있습니다. 산불 데이터에서 확정적인 패턴이 존재합니다. 이러한 패턴은 우리에게 미래 산불의 측면을 예측하는 모델을 구축하는 것뿐만 아니라 앞으로 몇 년과 몇십 년 동안 대규모 산불의 위험을 어떻게 줄일지에 대해 고려할 수 있도록 합니다.\n\n지난 30년간의 역사적 산불 데이터로부터 얻은 일부 통찰을 시작으로, 2040년까지의 미래에 대해 ML 모델이 우리에게 말하는 내용에 대해 전환하겠습니다. 산불 이야기를 숫자와 차트 및 비디오 애니메이션 형식의 패턴을 사용하여 전달하고 앞으로 우리가 할 수 있는 일에 대해 언급할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 역사적 산불에서 배운 것들\n\n1992년부터 2020년까지 미국 농림부 산림 서비스에서 보고한 미국의 산불 기록은 훌륭합니다. 미 국에서 산불 피해의 주된 부담을 갖고 있는 11개 서부 연이은 주에서 산불이 계속해서 어떻게 진화하는지를 나타내는 차트를 사용하겠습니다.\n\n산불은 자연적 또는 인간적인 원인으로 발생할 수 있습니다. 주된 자연적 원인은 번개이며, 이는 우리가 ML 모델에 대해 이야기할 때 다시 다루도록 하겠습니다. 오픈 버닝부터 고장난 전선까지 총 열여섯 가지의 다양한 인간적 원인이 있습니다.\n\n자연적 원인은 2001년부터 2010년까지 미국 서부에서 발생한 모든 산불 중 36%를 차지하며, 태워진 면적의 68%를 차지했습니다. 그러나 그 다음 10년 동안(2011년부터 2020년)에는 자연적 원인이 산불의 23%와 면적의 57%만 차지했습니다. 인간적 원인은 현재 산불의 77%와 면적의 43%(각각 64%와 32%로부터 증가)를 책임지고 있습니다. 아래의 두 차트는 이를 설명하며, 중요한 추세를 강조하여 다시 한번 이 사실을 확인합니다: 인간 활동이 산불의 주요 원인으로 점점 더 강화되고 있다는 것을. 물론 이러한 사실은 우리에게 나아갈 방향을 제시합니다. 우리는 번개를 컨트롤할 수 없지만, 인간적 원인을 영향을 줄 수 있으며, 잠재적으로 미래 산불의 궤적을 수정할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![DelvingintoWildfires_1](/assets/img/2024-06-20-DelvingintoWildfires_1.png)\n\n![DelvingintoWildfires_2](/assets/img/2024-06-20-DelvingintoWildfires_2.png)\n\n자연 화재의 화룡력(화재 당 연소된 에이커로 양적화)은 원시 지역과 고 지대에서 더 자주 발생하며 급증했습니다. 자연 화재의 화룡력이 2001년부터 2010년과 2011년부터 2020년 사이에 2배 증가했습니다. 여름철 기온이 높고 건조한 기후로 인해 화재가 매우 커지고 격리하기 어려워졌다고 합니다. 인간이 일으킨 화재의 화룡력도 약 50% 증가했으며, 평균 화룡력은 여전히 자연 화재의 4분의 1 수준입니다. 이는 인간이 일으킨 화재가 일반적으로 도시 근처나 농장 주변 등에서 발생하여 더 쉽게 감지하고 진압할 수 있기 때문일 수 있습니다.\n\n![DelvingintoWildfires_3](/assets/img/2024-06-20-DelvingintoWildfires_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인간 활동으로 인한 화재 중에서 가장 강도가 높으며 증가 추세에 있는 것은 전력 송전선 및 장비로 인한 화재입니다. 전력 송전선이 야생지대를 통과하기 때문에 대부분의 다른 인간 활동으로 인한 화재에 비해 접근이 어렵기 때문에 이러한 특징이 나타나는 것으로 생각됩니다. 그렇지만 전력 송전은 미디어에서 과대 보도되는 화재로써 2%의 화재 및 3%의 태우어진 면적을 차지하고 있습니다(이전에는 각각 1% 및 2%였음). \n\n![이미지](/assets/img/2024-06-20-DelvingintoWildfires_4.png)\n\n![이미지](/assets/img/2024-06-20-DelvingintoWildfires_5.png)\n\n![이미지](/assets/img/2024-06-20-DelvingintoWildfires_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n서부 주들을 가로지르는 역사적인 산불에는 공간 패턴이 있을까요? 산불은 항상 매우 낮은 확률의 사건입니다. 환경이 좋아져도 — 예를 들어 따뜻하고 건조한 기간에 땅 위에 연소 가능한 연료가 많아져도 — 산불이 발생하지 않을 수 있습니다. 따라서 역사적인 기록은 산불 위험을 반영하지 않으며 공간적으로는 비교적 희소합니다. 그러나 1992년부터 2020년까지 약 30년 동안의 데이터를 살펴보면 그 안에 주목할 만한 패턴이 있습니다.\n\nFireVision Historical Maps의 아래 화재 크기 분포를 보면 대규모 산불은 일반적으로 해안과 인구 집중 지역에서 떨어진 곳에서 발생했으며, 아마도 더 높은 고도에서 발생했을 것입니다. 워싱턴, 오레곤, 캘리포니아 지역의 대부분의 산불은 작거나 중소형이었지만 남부 오레곤, 동부 오레곤/워싱턴 및 북부 캘리포니아를 제외하고는 그렇지 않습니다. 네바다와 유타 일부 지역은 흰색 공간으로 나타나는 것처럼 기록된 산불이 전혀 발생하지 않았습니다.\n\n다음 화재 원인 분포는 어느 정도로 화재 크기를 따릅니다. 해변과 인구 집중 지역에서 떨어진 곳 및 보다 높은 고도에서 발생한 대규모 산불들이 자연적인 원인으로 발화될 가능성이 높습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![2024-06-20-DelvingintoWildfires_8.png](/assets/img/2024-06-20-DelvingintoWildfires_8.png)\n\n화재의 특성은 과거 한 해 동안 어떻게 변했을까요? 다음 세 가지 차트(그리고 몇 가지 추가 데이터)는 2011년부터 2020년까지의 화재에 대해 몇 가지 중요한 사항을 강조합니다:\n\n- 소규모 화재(` 100 에이커 미만)가 주를 이룹니다. 실제로 모든 화재의 거의 97%를 차지합니다. 1% 조금 넘는 화재는 대규모(1000 에이커 이상)입니다.\n- 모든 규모의 화재가 여름에 절정을 이룹니다. 8월이 화재의 정점이었지만, 최근에 그 정점이 7월로 이동하는 것으로 보입니다.\n- 자연 발화 화재(주로 번개에 의해 발생)는 여름에 정점을 이루며, 10월부터 4월까지는 무시해도 될 정도입니다.\n\n![2024-06-20-DelvingintoWildfires_9.png](/assets/img/2024-06-20-DelvingintoWildfires_9.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Delving into Wildfires 10](/assets/img/2024-06-20-DelvingintoWildfires_10.png)\n\n![Delving into Wildfires 11](/assets/img/2024-06-20-DelvingintoWildfires_11.png)\n\n# 왜 머신 러닝을 사용해야 할까요?\n\n과거 산불 데이터를 살펴보고 직관을 개발하는 것은 유용하지만, 특정 장소에서 미래에 일어날 일을 예측하는 데 충분하지는 않습니다. 여기서 머신 러닝이 모델링 기술로 등장합니다. ML은 복잡한 상호작용에서 발생하는 패턴들이 있는 산불 특성과 같은 것들을 예측할 때 특히 유용할 수 있습니다. 이러한 패턴은 다양한 위치별 변수들의 복잡한 상호작용에서 발생합니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 지표면 연료\n- 수연 구조\n- 지형\n- 최근의 날씨 데이터를 기반으로 한 불난 날씨 지수(FWI) 및 그 구성 요소, 그리고 미래 연도를 위한 고해상도 기후 예측에 따른 것\n- 번개 충동 밀도 및 충동당 전력\n\n상호 작용은 복잡하여 특정 산불의 특성을 명확하게 결정할 수 있는 단일 변수(다른 모든 것을 일정하게 유지하면서)를 식별하는 것이 종종 어려울 수 있습니다. 이차 및 그 이상의 상호 작용이 지배합니다. 특히 심층 신경망은 이러한 유형의 문제를 모델링하는 데 최적으로 적합하며, 그것이 바로 우리가 구현한 방법입니다. FireVision 사용 설명서에서 ML 방법론 및 모델링에 대해 자세히 알아보세요.\n\n# 기계 학습에서 배운 것들\n\n## 산불 위험 지수\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 스크린샷은 2021년부터 2023년까지의 평균 산불 날씨 조건을 사용하여 7월에 서부 주에서 새로운 번개로 인한 산불 위험 지수(LFRI)를 20km 그리드로 보여줍니다. 이 지수는 특정 시기에 특정 위치의 산불 취약성(잠재적 산불 규모에 의한 측정, 다음 섹션 참조)과 번개 발생 밀도를 결합합니다. 더 높은 LFRI는 자연적인 원인에서 더 큰 산불이 발생할 확률이 높음을 나타냅니다.\n\n아래 보이는 패턴은 이전 섹션에서 실제 역사적 산불 규모의 패턴과 매우 유사합니다: 해안과 인구 중심지에서 멀리 떨어진 곳에서 더 높은 위험 지수 값이 발생하며, 서부 주 중앙에 집중되어 있습니다. 이는 예측 ML 모델의 결과이므로 우리는 더 이상 역사적 기록에 의존하지 않으며, 공백이 없습니다: FireVision Risk Maps를 사용하여 설명을 위해 20km x 20km 그리드 상의 모든 지점 위치를 평가했으며 해당 공간의 모든 지점에 위험 값이 주어졌습니다.\n\n이것이 화재 시즌의 정점에서 위험 분포인 경우, 이 위험 지수는 연중 어떻게 변하는가요? 아래 비디오 클립에서는 산불 날씨 데이터의 2021년부터 2023년 평균 및 2010년부터 2023년까지의 평균 번개 기후를 사용하여 5월부터 11월까지 이를 보여줍니다. 일반적으로 화재 시즌 시작 직전인 5월에는 서부 주의 동부 및 남동부 지역에서 위험이 이미 상당히 높습니다. 11월에는 정점을 훌쩍 넘은 후로 위험이 극히 낮아 보입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nML 모델의 매력은 우리가 관심을 두는 공간의 각 위치에서 화재 위험을 평가할 뿐만 아니라 미래를 예측할 수 있다는 점입니다. 기후가 변화하고 여름이 더 더워지고 건조해짐에 따라 화재 위험이 증가할 것으로 예상됩니다. 이미 지난 10년 동안 이에 대한 미리보기를 볼 수 있었습니다.\n\n다음 차트는 2040년까지 11개 서부 주의 7500개 위치에 대한 위험 분포의 체계적인 월별 보기를 제시합니다. 앞으로 나아갈수록 고위험 지역의 수가 증가하며, 2040년 7월에는 모든 위치 중 21%가 최고치에 도달할 것으로 예상됩니다(2023년 10%에서 상승).\n\n![화재 위험 차트](/assets/img/2024-06-20-DelvingintoWildfires_13.png)\n\nLFRI와 같은 화재 위험 지수는 대형 화재에 대해 보험 청약서를 작성하거나 위험 담보를 제거하는 등의 다양한 분야에서 활용할 수 있습니다. 또한 숲 얇게 하는 등의 우선 순위가 매겨진 완화 노력을 위한 위치 식별 및 자연 기반 탄소 크레딧의 영구성을 평가하는 데 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 두 개의 비디오 클립은 서부 미국 지역의 5월과 7월 간의 위험 지수 값이 현재부터 2040년까지 어떻게 변화하는지를 보여줍니다.\n\n마지막으로 한 가지 더 짚고 넘어가겠습니다. 번개로 인한 화재 위험 지수는 있지만 인재로 인한 화재 위험 지수는 왜 없을까요? 번개 기후학은 우리가 화재 발생 가능성을 모델링하는 핵심적인 간접적인 방법을 제공합니다. 우리는 이미 언급했듯이 번개는 우리가 통제할 수 없기 때문에 여기서의 발화 가능성은 이미 결정된 상태이며, 우리가 모델링하고 있는 시스템 외부에서 발생하는 것으로 생각할 수 있습니다. 반면에, 인재로 인한 화재는 대부분 우리가 통제할 수 있지만 발화 가능성의 추정은 어려운 것으로 알려져 있습니다 (비록 이론적으로 허용 가능한 수준으로 줄일 수는 있습니다).\n\n## 화재 취약성\n\n우리는 특정 위치에서 발화 사건이 발생했을 때 발생할 화재의 잠재적 크기에 따라 해당 위치의 산불 취약성을 양적으로 정량화합니다. 이것은 발화 사건이 발생했을 때 조건부 확률에 기반합니다. 취약성은 지표면의 연료 조건, 지형의 특성 및 최근 또는 예상되는 화재 기상의 영향을 받는 함수입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nThe susceptibility measure can be a powerful call to action. It is especially valuable information when it comes to human-caused fires, which as we’ve seen are becoming more dominant and largely in our control. If the fire susceptibility within a region is high, then it will require additional measures to proactively prevent ignition events such as faulty power lines, open burning or campfires in that region. Susceptibility numbers can also be used to drive targeted mitigation measures such as removal of forest debris and mechanical thinning of brush/forests.\n\nSimilar to the LFRI chart above, the chart below shows a systematic monthly view of the fire susceptibility distribution at 7500 locations across the western states between now and 2040. The susceptibility peaks in July, exceeding 1000 acres at 20% of all locations in 2023 and increasing to 37% in 2040.\n\n![Chart image](/assets/img/2024-06-20-DelvingintoWildfires_14.png)\n\nThe two video clips below (generated using the FireVision Susceptibility Maps) animate the evolution of fire susceptibility between now and 2040 for the months of May and July.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 화재 발생 원인\n\n다음 차트에서 생성된 월별 화재 원인의 분포는 FireVision Cause Maps를 사용하여 발화 이벤트에 영향을 받습니다. 차트는 간단히 20km 그리드 상의 모든 발화 위치에 대해 월별로 더 가능성있는 산불 원인을 나타냅니다. 번개로 인한 자연 산불은 여름에 훨씬 더 가능성이 크지만, 인간 활동에 의한 산불은 연중 다른 시기에 더 가능성이 큽니다.\n\n![DelvingintoWildfires_15](/assets/img/2024-06-20-DelvingintoWildfires_15.png)\n\n이 차트에서 명확하지 않은 것은 있지만, 이전에 언급한 바와 같이, 인간 활동이 해안 부근 및 인구 집중 지역 근처의 특정 지리적 영역에서 우세할 것이고, 자연적인 원인은 더 범람 지역 및 고도가 더 높은 지역에서 우세할 것이란 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 핵심 포인트\n\n- 인간 활동은 2011년부터 2020년까지 서부 11개 주에서 발생한 화재의 77%와 소실된 면적의 43%를 차지하며, 화재의 주요 원인으로 계속 증가하고 있습니다.\n- 자연 원인 중 번개는 더 범위가 넓은 지역에서 많은 대규모 화재를 유발하며 큰 피해를 입히고 있습니다.\n- 화재 규모는 이 세기의 첫째와 둘째 10년 사이에 최대 2배까지 증가하면서 기후 변화의 분명한 영향을 보여줍니다.\n- 전력 전송선으로 인한 화재는 증가하고 있지만, 여전히 화재의 2%와 이에 따른 면적의 3%에 불과합니다.\n- 기계 학습 모델은 서부 주에서 번개로 인한 화재 위험 지역 수가 현재부터 2040년에는 불정 상 시즌에 10%에서 21%로 두 배 증가할 것으로 예측합니다.\n- 모든 원인에서 대규모 화재에 높은 취약성이 있는 지역 수도 현재부터 2040년까지 거의 2배(20% 에서 37%) 증가할 것입니다.\n- 데이터와 모델 예측은 화재에 대한 행동으로 이어지는 단호한 요청입니다. 우리가 대응할 수 있는 것들이 많이 있습니다. 적절한 지표가 있다면 예방, 완화 및 위험 관리를 통해 많은 것을 할 수 있습니다.\n- 화재에 대한 행동을 촉구할 수 있는 두 가지 강력한 지역별 지표를 소개했습니다: 위험 지수 및 취약성 지표.","ogImage":{"url":"/assets/img/2024-06-20-DelvingintoWildfires_0.png"},"coverImage":"/assets/img/2024-06-20-DelvingintoWildfires_0.png","tag":["Tech"],"readingTime":9},{"title":"Google의 프루프리드 한 번의 탭으로 작성 정확도를 높이는 AI-driven 기능","description":"","date":"2024-06-20 18:13","slug":"2024-06-20-GooglesProofreadAI-DrivenTypingAccuracyinOneTap","content":"\n\n\n![Gboard](/assets/img/2024-06-20-GooglesProofreadAI-DrivenTypingAccuracyinOneTap_0.png)\n\nGboard은 모바일 기기용 Google 키보드로, 통계 디코딩을 활용하여 부드러운 타자 경험을 제공합니다. 자동 및 수동 오류 수정 기능을 갖추고 있어 사용자 친화적 상호 작용을 보장합니다. 대형 언어 모델 (LLMs)의 놀라운 능력을 활용하여 Gboard는 문장 및 단락 수준의 수정을 향상시켜 타자 경험을 혁신합니다.\n\nGoogle 연구팀이 제시한 새 논문 'Proofread: Fixes All Errors with One Tap'에서 Proofread를 소개합니다. Proofread는 서버 측 LLM을 기반으로 한 혁신적인 Gboard 기능으로서, 한 번의 탭으로 실시간으로 문장 및 단락을 수정할 수 있습니다. Pixel 8 장치에서 시작된 이 기능은 매일 수천 명의 사용자에게 혜택을 줍니다.\n\n![Proofread](/assets/img/2024-06-20-GooglesProofreadAI-DrivenTypingAccuracyinOneTap_1.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시스템은 데이터 생성, 메트릭 디자인, 모델 튜닝 및 모델 서빙으로 구성돼 있어요.\n\n![이미지](/assets/img/2024-06-20-GooglesProofreadAI-DrivenTypingAccuracyinOneTap_2.png)\n\n데이터 생성에 대해선, 정교한 오류 합성 프레임워크가 일반적인 키보드 오류를 통합하여 데이터셋을 생성하며, 사용자 입력을 모방합니다. 추가적인 단계에서 데이터 분포가 Gboard 도메인과 밀접하게 일치하도록 보장돼요.\n\n![이미지](/assets/img/2024-06-20-GooglesProofreadAI-DrivenTypingAccuracyinOneTap_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n메트릭스 설계에 대해: 다양한 관점에서 모델을 평가하기 위해 여러 메트릭스가 설계되었습니다. 긴 텍스트의 가능한 정답의 다양성을 고려하여, 주요 메트릭스에는 LLMs에 기반한 문법 오류와 의미적 일관성을 확인하는 사항이 포함되어 있습니다.\n\n모델 튜닝에 대해: InstructGPT에서 영감을 받아, 모델은 지도된 세밀 조정을 거친 후 보상 학습(RL) 튜닝을 진행합니다. RL 튜닝 단계에서 Global Reward 및 Direct Reward 기법을 활용하여 모델의 성능을 크게 향상시킵니다. 결과는 RL 튜닝이 문법 오류를 줄이는 데 효과적이며, PaLM2-XS 모델의 Bad 비율을 5.74% 감소시켰음을 보여줍니다.\n\n모델 서빙에 대해: 모델은 구름의 TPU v5에 배포되며 양자화, 버킷팅, 입력 분할 및 예측적 디코딩을 통해 최적화된 대기 시간을 달성합니다. 예측적 디코딩만으로 중앙값 대기 시간이 39.4% 감소합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 작업은 LLMs(Large Language Models)의 상당한 잠재력을 보여 주며, 고품질의 문장 및 단락 교정을 통해 타자 경험을 향상시킬 수 있다는 것을 보여줍니다. LLMs의 변화력을 강조하며 사용자 입력 상호작용에서의 LLMs의 변화력을 강조하며, 디바이스와의 상호 작용 방법을 근본적으로 개선할 것을 제안합니다.\n\n논문 Proofread: Fixes All Errors with One Tap은 arXiv에 게시되어 있습니다.\n\n저자: Hecate He | 편집자: Chain Zhang","ogImage":{"url":"/assets/img/2024-06-20-GooglesProofreadAI-DrivenTypingAccuracyinOneTap_0.png"},"coverImage":"/assets/img/2024-06-20-GooglesProofreadAI-DrivenTypingAccuracyinOneTap_0.png","tag":["Tech"],"readingTime":2},{"title":"암호화폐 백테스팅 간단한 딥 러닝 전략의 놀라운 결과","description":"","date":"2024-06-20 18:11","slug":"2024-06-20-Back-testingCryptocurrenciesAstonishingResultsfromaSimpleDeepLearningStrategy","content":"\n\n암호화폐 시장의 급격한 변동 속에서 백테스팅은 투자 전략을 검증하는 데 중요한 역할을 합니다. 본 연구는 비트코인, 이더리움, 바이낸스 코인, 솔라나 및 엑스알피와 같은 주요 암호화폐를 간단한 딥 러닝 모델을 활용해 평가하는 데 초점을 맞췄습니다. 놀랍게도, 이 방법은 다음과 같이 매우 뛰어난 성과 지표를 제시합니다:\n\n- 샤프 비율: 19.898, 비법적인 위험 조정 수익을 나타냅니다.\n- 소티노 비율: 114.442, 무시할 만한 하락 위험을 나타냅니다.\n- 베타: -0.131, 역시장 상관 관계를 보여줍니다.\n- 알파: 0.021, 주목할 만한 추세를 강조합니다.\n\n이 간단하고 효과적인 딥 러닝 모델에 의해 높은 성과를 이룬 이러한 결과는 전략적인 투자에 대한 깊은 통찰을 제공합니다.\n\n따라서, 우리는 바이낸스의 다섯 가지 암호화폐 쌍에 대해 (새 전략으로 불리는) 간단한 전략을 사용하여 백테스팅을 진행했습니다. 결과는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBTCUSDT\n\n![image1](/assets/img/2024-06-20-Back-testingCryptocurrenciesAstonishingResultsfromaSimpleDeepLearningStrategy_0.png)\n\n![image2](/assets/img/2024-06-20-Back-testingCryptocurrenciesAstonishingResultsfromaSimpleDeepLearningStrategy_1.png)\n\nETHUSDT\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBNBUSDT\n\nSOLUSDT\n\nXRPUSDT\n\n# ETH/USDT 백테스팅 성능 지표 분석\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단한 딥 러닝 전략을 사용하여 ETH/USDT의 백 테스팅 결과는 다음과 같은 성능 지표를 보여줍니다:\n\n- 평균 절대 오차 (MAE): 19.994, 예측 오차의 평균 크기를 나타냅니다.\n- 제곱근 평균 제곱 오차 (RMSE): 25.152, 예측 값과 실제 값 사이의 평균 제곱 차이의 제곱근을 나타냅니다.\n- R-제곱 (R²): 0.997, 예측 값과 실제 가격 사이에 매우 높은 상관 관계를 보여줍니다. 이는 모델이 변동성 거의 모두를 설명한다는 것을 의미합니다.\n- 평균 절대 백분율 오차 (MAPE): 0.0007, 예측 값과 실제 값 사이의 평균 퍼센트 오차를 나타냅니다.\n- 샤프 비율 (신 전략): 15.967, 새 전략의 우수한 리스크 조정 수익을 보여줍니다.\n- 소티노 비율 (신 전략): 116.496, 최소한의 하향 리스크를 강조합니다.\n- 베타 (신 전략): 0.048, 시장과의 낮은 양의 상관 관계를 보여줍니다.\n- 알파 (신 전략): 0.022, 전략이 시장 기준에 비해 우수한 성과를 보여주는 것을 나타냅니다.\n- 교차 검증 MAE: 661.709 ± 499.095, 모델의 예측 오차를 데이터의 다양한 하위 집합에 걸쳐 추정합니다.\n\n이러한 지표들은 이 백 테스트에서 적용된 딥 러닝 전략의 높은 성능과 견고성을 강조합니다.\n\n이것은 단지 예시일 뿐이며, 충분한 데이터가 있다면 바이낸스나 다른 거래소의 모든 심볼에서도 좋은 결과를 얻을 수 있을 것이라고 생각합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n리플레이 결과\n\n이 결과를 다시 확인하거나 새로 시도하려면 다음 코드를 사용할 수 있습니다:\n\n- 모델 만들기\n\n```js\nimport ccxt\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Dropout, Flatten\nimport tf2onnx\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# 바이낸스에서 데이터 다운로드하는 함수\ndef descargar_datos(symbol, timeframe='1d', start_date='2004-01-01T00:00:00Z', end_date='2024-01-01T00:00:00Z'):\n    exchange = ccxt.binance({'enableRateLimit': False})\n    since = exchange.parse8601(start_date)\n    end_date_timestamp = pd.to_datetime(end_date, utc=True)\n    all_data = []\n\n    while since \u003c end_date_timestamp.timestamp() * 1000:\n        ohlc = exchange.fetch_ohlcv(symbol, timeframe=timeframe, since=since)\n        all_data.extend(ohlc)\n        since = ohlc[-1][0] + 1  # 'since' 매개변수 1밀리초 증가\n\n    df = pd.DataFrame(all_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n    df.set_index('timestamp', inplace=True)\n\n    # 두 데이터 모두 타임존이 설정되어 있는지 확인하거나 필요한 경우 변환\n    if df.index.tz is None:\n        df.index = df.index.tz_localize('utc')\n    \n    df = df[df.index \u003c= end_date_timestamp]\n    print(df)\n    return df['close'].values\n\n# 데이터 불러오기\ndata = descargar_datos('ETH/USDT')\n\n# 데이터 정규화\nscaler = MinMaxScaler(feature_range=(0, 1))\ndata = scaler.fit_transform(data.reshape(-1, 1))\n\n# 시퀀스에서 샘플 생성하는 함수\ndef crear_muestras(dataset, pasos_de_tiempo=120):\n    X, y = [], []\n    for i in range(pasos_de_tiempo, len(dataset)):\n        X.append(dataset[i-pasos_de_tiempo:i, 0])\n        y.append(dataset[i, 0])\n    return np.array(X), np.array(y)\n\n# 훈련 및 테스트 데이터 준비\npasos_de_tiempo = 120\nX, y = crear_muestras(data, pasos_de_tiempo)\nX = X.reshape(X.shape[0], X.shape[1], 1)  # LSTM용 변경\n\n# 데이터 분할 (훈련에 80% 할당)\nsplit = int(0.8 * len(X))\nX_train, X_test = X[:split], X[split:]\ny_train, y_test = y[:split], y[split:]\n\n# 모델 훈련\n\nmodel = Sequential()\nmodel.add(Conv1D(filters=256, kernel_size=2, activation='relu',padding = 'same',input_shape=(X_train.shape[1],1)))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(LSTM(100, return_sequences = True))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100, return_sequences = False))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(units=1, activation = 'sigmoid'))\nmodel.compile(optimizer='adam', loss= 'mse' , metrics = [tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n\n# 조기 종료 설정\nearly_stopping = callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=10,\n    restore_best_weights=True,\n)\n# 최적의 모델 저장할 체크포인트 설정\ncheckpoint = ModelCheckpoint(\n    'best_model.h5', \n    monitor='val_loss', \n    save_best_only=True, \n    save_weights_only=False\n)\n\n# 300 에폭 학습\nhistory = model.fit(X_train, y_train, epochs = 300 , validation_data = (X_test,y_test), batch_size=32, callbacks=[early_stopping, checkpoint], verbose=2)\n\n# 훈련 이력 그래프\nplt.figure(figsize=(10, 5))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.plot(history.history['rmse'], label='Train RMSE')\nplt.plot(history.history['val_rmse'], label='Validation RMSE')\nplt.title('Model Training History')\nplt.xlabel('Epochs')\nplt.ylabel('Loss/RMSE')\nplt.legend()\nplt.savefig('ETHUSDT.png')  # 그래프 이미지 파일로 저장\n\n# 모델을 ONNX로 변환\nonnx_model, _ = tf2onnx.convert.from_keras(model, opset=13, output_path=\"model_ethusdt.onnx\")\nprint(\"ONNX 모델을 'model_ethusdt.onnx'로 저장했습니다.\")\n\n# 모델 평가\ntrain_loss, train_rmse = model.evaluate(X_train, y_train, verbose=0)\ntest_loss, test_rmse = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"train_loss={train_loss:.3f}, train_rmse={train_rmse:.3f}\")\nprint(f\"test_loss={test_loss:.3f}, test_rmse={test_rmse:.3f}\")\r\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 백테스팅\n\n```js\nimport ccxt\nimport pandas as pd\nimport numpy as np\nimport onnx\nimport onnxruntime as ort\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\nfrom sklearn.model_selection import TimeSeriesSplit\n\n# 바이낸스 거래소 인스턴스 생성\nbinance = ccxt.binance()\n\n# 시장 심볼 및 시간 간격 정의\nsymbol = 'ETH/USDT'\ntimeframe = '1d'\nlimit = 1000  # 120일 신뢰 구간 보장을 위한 충분한 데이터 다운로드\n\n# 역사적 데이터 다운로드\nohlcv = binance.fetch_ohlcv(symbol, timeframe, limit=limit)\n\n# 데이터를 판다스 DataFrame으로 변환\ndf = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\ndf['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n\n# 데이터를 CSV 파일로 저장\ndf.to_csv('binance_data.csv', index=False)\nprint(\"'binance_data.csv'에 다운로드 및 저장된 데이터\")\n\n# 다운로드한 데이터 로드\ndata = pd.read_csv('binance_data.csv')\n\n# 'timestamp' 열을 datetime 형식으로 변환\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\n\n# 정규화 값(정규화에 사용한 값에 맞게 조정)\nmin_close = data['close'].min()\nmax_close = data['close'].max()\n\n# 종가 데이터 정규화\ndata['close_normalized'] = (data['close'] - min_close) / (max_close - min_close)\n\n# ONNX 모델 로드\nmodel = onnx.load('model_ethusdt.onnx')\nonnx.checker.check_model(model)\n\n# 런타임 세션 생성\nort_session = ort.InferenceSession('model_ethusdt.onnx')\n\n# 모델에 주입할 데이터를 슬라이딩 윈도우로 준비\ninput_name = ort_session.get_inputs()[0].name\nsequence_length = 120  # 모델에 따라 조정\n\n# 예측값을 저장할 리스트 생성\npredictions_list = []\n\n# 예측 시작 날짜 설정\nstart_date = pd.Timestamp('2024-01-01')\nend_date = pd.Timestamp.today()\n\n# 날짜별 추론 실행\ncurrent_date = start_date\nwhile current_date \u003c= end_date:\n    # 현재 날짜 이전 120일 데이터 선택\n    end_idx = data[data['timestamp'] \u003c= current_date].index[-1]\n    start_idx = end_idx - sequence_length + 1\n\n    if start_idx \u003c 0:\n        print(f\"{current_date} 날짜에 대한 데이터 부족\")\n        break\n\n    # 정규화된 데이터 윈도우 추출\n    window = data['close_normalized'].values[start_idx:end_idx+1]\n\n    if len(window) \u003c sequence_length:\n        print(f\"{current_date} 날짜에 대한 데이터 부족\")\n        break\n\n    # 모델에 입력할 데이터 준비\n    input_window = np.array(window).astype(np.float32)\n    input_window = np.expand_dims(input_window, axis=0)  # 배치 크기 차원 추가\n    input_window = np.expand_dims(input_window, axis=2)  # 특성 차원 추가\n\n    # 추론 실행\n    output = ort_session.run(None, {input_name: input_window})\n    prediction = output[0][0][0]\n\n    # 예측값 역정규화\n    prediction = prediction * (max_close - min_close) + min_close\n\n    # 예측값 저장\n    predictions_list.append({'date': current_date, 'prediction': prediction})\n\n    # 날짜 증가\n    current_date += pd.Timedelta(days=1)\n\n# 예측값 리스트를 DataFrame으로 변환\npredictions_df = pd.DataFrame(predictions_list)\n\n# 예측값을 CSV 파일로 저장\npredictions_df.to_csv('predicted_data.csv', index=False)\nprint(\"'predicted_data.csv'에 저장된 예측값\")\n\n# 예측값과 실제 값 비교\ncomparison_df = pd.merge(predictions_df, data[['timestamp', 'close']], left_on='date', right_on='timestamp')\ncomparison_df = comparison_df.drop(columns=['timestamp'])\ncomparison_df = comparison_df.rename(columns={'close': 'actual'})\n\n# 오차 메트릭 계산\nmae = mean_absolute_error(comparison_df['actual'], comparison_df['prediction'])\nrmse = np.sqrt(mean_squared_error(comparison_df['actual'], comparison_df['prediction']))\nr2 = r2_score(comparison_df['actual'], comparison_df['prediction'])\nmape = mean_absolute_percentage_error(comparison_df['actual'], comparison_df['prediction'])\nprint(f'평균 절대 오차(MAE): {mae}')\nprint(f'제곱근 평균 제곱 오차(RMSE): {rmse}')\nprint(f'R^2 점수 (R2): {r2}')\nprint(f'평균 절대 백분율 오차(MAPE): {mape}')\n\n# 오차 밴드가 있는 그래프 그리기\nplt.figure(figsize=(14, 7))\nplt.plot(comparison_df['date'], comparison_df['actual'], label='실제 가격', color='blue')\nplt.plot(comparison_df['date'], comparison_df['prediction'], label='예측 가격', color='red')\nplt.fill_between(comparison_df['date'], comparison_df['prediction'] - mae, comparison_df['prediction'] + mae, color='gray', alpha=0.2, label='오차 밴드 (MAE)')\nplt.xlabel('날짜')\nplt.ylabel('가격')\nplt.title(f'{symbol} 가격 예측 대 비교')\nplt.legend()\nplt.savefig(f\"{symbol.replace('/', '_')}_price_prediction.png\")\nplt.show()\nprint(f\"'{symbol.replace('/', '_')}_price_prediction.png'로 그래프 저장됨\")\n\n# 잔차 분석\nresiduals = comparison_df['actual'] - comparison_df['prediction']\nplt.figure(figsize=(14, 7))\nplt.plot(comparison_df['date'], residuals, label='잔차', color='purple')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8)\nplt.xlabel('날짜')\nplt.ylabel('잔차')\nplt.title(f'{symbol} 예측 잔차')\nplt.legend()\nplt.savefig(f\"{symbol.replace('/', '_')}_residuals.png\")\nplt.show()\nprint(f\"'{symbol.replace('/', '_')}_residuals.png'로 잔차 그래프 저장됨\")\n\n# 상관관계 분석\ncorrelation = comparison_df['actual'].corr(comparison_df['prediction'])\nprint(f'실제 및 예측 가격 간 상관관계: {correlation}')\n\n# 예측 기반 투자 전략 시뮬레이션 (원본 전략)\ninvestment_df = comparison_df.copy()\ninvestment_df['strategy_returns'] = (investment_df['prediction'].shift(-1) - investment_df['actual']) / investment_df['actual']\ninvestment_df['buy_and_hold_returns'] = (investment_df['actual'].shift(-1) - investment_df['actual']) / investment_df['actual']\n\nstrategy_cumulative_returns = (investment_df['strategy_returns'] + 1).cumprod() - 1\nbuy_and_hold_cumulative_returns = (investment_df['buy_and_hold_returns'] + 1).cumprod() - 1\n\nplt.figure(figsize=(14, 7))\nplt.plot(investment_df['date'], strategy_cumulative_returns, label='전략 누적 수익', color='green')\nplt.plot(investment_df['date'], buy_and_hold_cumulative_returns, label='매수 및 보유 누적 수익', color='orange')\nplt.xlabel('날짜')\nplt.ylabel('누적 수익률')\nplt.title(f'{symbol} 투자 전략 대 매수 및 보유')\nplt.legend()\nplt.savefig(f\"{symbol.replace('/', '_')}_investment_strategy.png\")\nplt.show()\nprint(f\"'{symbol.replace('/', '_')}_investment_strategy.png'로 투자 전략 그래프 저장됨\")\n\n# 손실 전망\ninvestment_df['drawdown'] = strategy_cumulative_returns.cummax() - strategy_cumulative_returns\ninvestment_df['max_drawdown'] = investment_df['drawdown'].max()\n\nplt.figure(figsize=(14, 7))\nplt.plot(investment_df['date'], investment_df['drawdown'], label='Drawdown', color='red')\nplt.xlabel('날짜')\nplt.ylabel('Drawdown')\nplt.title(f'{symbol} 전략 Drawdown')\nplt","ogImage":{"url":"/assets/img/2024-06-20-Back-testingCryptocurrenciesAstonishingResultsfromaSimpleDeepLearningStrategy_0.png"},"coverImage":"/assets/img/2024-06-20-Back-testingCryptocurrenciesAstonishingResultsfromaSimpleDeepLearningStrategy_0.png","tag":["Tech"],"readingTime":12}],"page":"4","totalPageCount":71,"totalPageGroupCount":4,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"4"},"buildId":"FH3Qr-mLAesqA0X5IFRQr","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>