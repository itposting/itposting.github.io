<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/4" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/4" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/960f1fe994a0ab5c.css" as="style"/><link rel="stylesheet" href="/_next/static/css/960f1fe994a0ab5c.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-2d104a861d88ea21.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/O5JVeK6TDhbvSxBlchQwM/_buildManifest.js" defer=""></script><script src="/_next/static/O5JVeK6TDhbvSxBlchQwM/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="챗 시 PT와 인공 지능 시대에 문화 대혁명 이상의 재창조" href="/post/2024-05-27-ChatXiPTandtheReinventionofCulturalRevolutionIdealsintheAgeofArtificialIntelligence"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="챗 시 PT와 인공 지능 시대에 문화 대혁명 이상의 재창조" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-ChatXiPTandtheReinventionofCulturalRevolutionIdealsintheAgeofArtificialIntelligence_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="챗 시 PT와 인공 지능 시대에 문화 대혁명 이상의 재창조" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">챗 시 PT와 인공 지능 시대에 문화 대혁명 이상의 재창조</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="언급되지 않은 수퍼 비밀 프롬프트 도구 지금은 아무도 말하지 않는" href="/post/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="언급되지 않은 수퍼 비밀 프롬프트 도구 지금은 아무도 말하지 않는" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="언급되지 않은 수퍼 비밀 프롬프트 도구 지금은 아무도 말하지 않는" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">언급되지 않은 수퍼 비밀 프롬프트 도구 지금은 아무도 말하지 않는</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="기존의 GPT-4보다 뛰어난 성능을 자랑하는 스탠포드 대학팀의 대형 모델이 모바일폰에서도 구동될 수 있다는 점이 인기를 끌며 하룻밤 사이에 2천 회 이상 다운로드되었습니다" href="/post/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="기존의 GPT-4보다 뛰어난 성능을 자랑하는 스탠포드 대학팀의 대형 모델이 모바일폰에서도 구동될 수 있다는 점이 인기를 끌며 하룻밤 사이에 2천 회 이상 다운로드되었습니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="기존의 GPT-4보다 뛰어난 성능을 자랑하는 스탠포드 대학팀의 대형 모델이 모바일폰에서도 구동될 수 있다는 점이 인기를 끌며 하룻밤 사이에 2천 회 이상 다운로드되었습니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">기존의 GPT-4보다 뛰어난 성능을 자랑하는 스탠포드 대학팀의 대형 모델이 모바일폰에서도 구동될 수 있다는 점이 인기를 끌며 하룻밤 사이에 2천 회 이상 다운로드되었습니다</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ChatGPT를 활용한 스마트 대화를 위한 API 구현하기" href="/post/2024-05-27-ImplementanAPIforSmartConversationswithChatGPT"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ChatGPT를 활용한 스마트 대화를 위한 API 구현하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-ImplementanAPIforSmartConversationswithChatGPT_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ChatGPT를 활용한 스마트 대화를 위한 API 구현하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">ChatGPT를 활용한 스마트 대화를 위한 API 구현하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="스케치에서 프로페셔널 다이어그램으로 챗지피티를 활용한 플로우차트" href="/post/2024-05-27-FlowchartswithChatGPTfromSketchtoProfessionalDiagram"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="스케치에서 프로페셔널 다이어그램으로 챗지피티를 활용한 플로우차트" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-FlowchartswithChatGPTfromSketchtoProfessionalDiagram_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="스케치에서 프로페셔널 다이어그램으로 챗지피티를 활용한 플로우차트" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">스케치에서 프로페셔널 다이어그램으로 챗지피티를 활용한 플로우차트</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="깊은 CNN 뒤의 수학  AlexNet" href="/post/2024-05-27-TheMathBehindDeepCNNAlexNet"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="깊은 CNN 뒤의 수학  AlexNet" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="깊은 CNN 뒤의 수학  AlexNet" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">깊은 CNN 뒤의 수학  AlexNet</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">40<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="대규모 언어 모델에서 추론을 위한 인-컨텍스트 학습 방법" href="/post/2024-05-27-Retrieval-BasedIn-ContextLearningforReasoninginLargeLanguageModels"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대규모 언어 모델에서 추론을 위한 인-컨텍스트 학습 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-Retrieval-BasedIn-ContextLearningforReasoninginLargeLanguageModels_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대규모 언어 모델에서 추론을 위한 인-컨텍스트 학습 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">대규모 언어 모델에서 추론을 위한 인-컨텍스트 학습 방법</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="SOFTS 시리즈-코어 퓨전을 활용한 효율적인 다변수 시계열 예측" href="/post/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="SOFTS 시리즈-코어 퓨전을 활용한 효율적인 다변수 시계열 예측" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="SOFTS 시리즈-코어 퓨전을 활용한 효율적인 다변수 시계열 예측" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">SOFTS 시리즈-코어 퓨전을 활용한 효율적인 다변수 시계열 예측</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="강화 학습 딥 Q-네트워크" href="/post/2024-05-27-ReinforcementLearningDeepQ-Networks"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="강화 학습 딥 Q-네트워크" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="강화 학습 딥 Q-네트워크" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">강화 학습 딥 Q-네트워크</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">35<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="딥 러닝을 이용한 우울증 예측 NLP 기초 배우기" href="/post/2024-05-27-DepressionpredictionusingDeepLearninglearnbasicsofNLP"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="딥 러닝을 이용한 우울증 예측 NLP 기초 배우기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-DepressionpredictionusingDeepLearninglearnbasicsofNLP_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="딥 러닝을 이용한 우울증 예측 NLP 기초 배우기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">딥 러닝을 이용한 우울증 예측 NLP 기초 배우기</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><a class="link" href="/posts/1">1</a><a class="link" href="/posts/2">2</a><a class="link" href="/posts/3">3</a><a class="link posts_-active__YVJEi" href="/posts/4">4</a><a class="link" href="/posts/5">5</a><a class="link" href="/posts/6">6</a><a class="link" href="/posts/7">7</a><a class="link" href="/posts/8">8</a><a class="link" href="/posts/9">9</a><a class="link" href="/posts/10">10</a><a class="link" href="/posts/11">11</a></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"챗 시 PT와 인공 지능 시대에 문화 대혁명 이상의 재창조","description":"","date":"2024-05-27 14:33","slug":"2024-05-27-ChatXiPTandtheReinventionofCulturalRevolutionIdealsintheAgeofArtificialIntelligence","content":"\n\n## 의견 | 인공 지능 \u0026 중국\n\n![이미지](/assets/img/2024-05-27-ChatXiPTandtheReinventionofCulturalRevolutionIdealsintheAgeofArtificialIntelligence_0.png)\n\n2018년 초봄, 현재 TikTok을 보유한 베이징 소재의 바이트댄스의 창립자인 장이밍은 중국의 국가 미디어 감독 기관인 중국 국가 라디오와 텔레비전 행정국으로부터 희극적인 텍스트와 비디오 콘텐츠를 공유하는 소셜 미디어 앱인 네이한 드안지의 운영을 중단하라는 통지를 받았습니다. 이 명령은 \"음란한\" 및 \"부적절한\" 콘텐츠를 호스팅한 혐의로 발효되었습니다.\n\n장이밍은 어떻게 대응했을까요? - 물론, 그는 명령을 따르고 중국의 혁명적 전통주의에 따라 깊은 유감을 표현하여 공개 사과를 했습니다. \"지시 및 감독 당국의 지도와 기대를 늘 어기게 해서 유감스럽다\"고 그는 말했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현대 사회주의적 맥락에서 설정된 것이지만, 중국의 문화 대혁명(1966-1976) 기간 동안 열리던 전통적인 투쟁 세션을 많이 떠올리게 했어요. 이렇게 불리는 투쟁 세션은 비난 집회나 투쟁 모임으로, 국가 적대자로 몰되거나 반혁명적이며 반사회적인 사상이나 행동을 가진 개인들에 대한 공개적인 수치와 비날을 포함하고 있어요.\n\n공산당의 전통적인 정치적 올바름 규칙에 완전히 부응하여, 장이밍은 구제 조치 아홉 가지를 약속했어요. 이 중요 사항 중에는: 바이트댄스에서 공산당의 존재감을 강화하고 종업원들이 공산당과 정부의 관점에서 생각하도록 교육하는 것이 포함돼 있어요.\n\n6년 후 바이트댄스의 투쟁 세션 이후, 중국은 “처벌”의 양식으로 하이테크 국가 대만의 해안에서 조작을 발표하고 있어요, 믿기 어려우나 사실이며, 아마도 전체 자유 국가에 대한 투쟁 세션을 예측하기 위해서일 거예요. 그러나 1967년 투쟁 세션 피해자 본인인 시진핑 대통령의 메갈로마닉 코미디는 지리적 난장판의 비극적 현실을 넘어섭니다. 그는 나라 구멍마다 있는 모든 보안 카메라의 시야 안에서 인간 학문과 지성을 최대한의 잔인함으로 세뇌하고 억압하고자만 하지만 AI의 모든 유형의 나타남을 지배하고 통치하려 합니다. 이야기는 \"챗 시 PT\"에 대해 이루어지며, 이는 중국의 인공 지능 문화 대혁명이라고 레이블링 하고 싶은 중국의 지키반인에 의해 소개된 최근 AI 챗봇입니다. 새로운 GPT는 시진핑의 정치문화철학과 완전히 일치해요. 그것은 민주적 AI의 우리 아이디어를 복사하고 중국 정부의 과거 시점적 조직적인 선배의 완벽한 준수로 복속시키기 위한 무거운 대형 언어 모델(LLM)입니다. 지능적이거나 인공일지라도 중국의 정치적 편향 패러다임을 따라야 해요. 이것이 시진핑 대통령의 눈에는 \"바람직한 AI\"가 되는 이유입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-27-ChatXiPTandtheReinventionofCulturalRevolutionIdealsintheAgeofArtificialIntelligence_2.png\" /\u003e\n\n챗봇은 중국에서 빈민 민주주의 지식인의 현실적인 상태를 빈틈없이 보여주는 농담이 아니라 가장 쓴 목마음의 현실입니다. 중국 사이버스페이스 연구원이 개발한 Chat Xi PT는 중국 최고 인터넷 규제기관인 중국 사이버스페이스 행정부 (CAC) 아래에서 운영되며, Chat Xi PT의 주요 목적은 국가의 엄격한 콘텐츠 통제 아래에서 LLM 개발을 일방적으로 정리하는 것으로 보입니다. Chat Xi PT는 '새로운 시대의 중국 특색 사회주의에 대한 시진핑 사상'으로 알려진 시진핑의 개인적이고 정치적 문화철학을 대표하는 데이터셋을 학습하였으며 다른 기관인 CAC가 제공한 공식 문헌도 포함됩니다. 시진핑 주석은 중국 소유지로 보는 영역 내에서 어떠한 종류의 지식이든 그의 관점과 완전히 일치하도록 켄트라를 생성하기를 기대하고 있습니다. 국가 권력은 지능적인 콘텐츠에 의해 의문을 제기하거나 약화시키지 말아야 합니다.\n\n전반적인 맥락은 현대화되었지만, 리더의 말을 통해 지적 및 지능의 억압과 강제적 교육적 개념은 전통적인 문화 대혁명의 메커니즘과 일관성이 있습니다. 인간들이 마오쩐둥의 '홍책'으로 읽기를 강요해야 했던 전통적인 문화 대혁명의 메커니즘과 일관성이 있습니다. 그리고 CAC는 말 그대로 '마오쩍동 주석 명언집'이라고 더 공식적으로 붙여진 '홍책'에 Chat Xi PT에 실질적으로 교육을 놓치지 않았음을 보증합니다.\n\nChat Xi PT의 도입은 중국 정부가 AI를 전체로 동기화하려는 신호를 명확히 보여줍니다. 실제로 국가 프로파간다는 가짜 자유로운 환경을 연출하고 순응하는 AI 모델의 개발을 지원한다고 속이려 합니다. 그러나 실제로 CAC는 발전 AI에 엄격한 규칙을 도입하여 제공업체들이 자신들의 모델이 '핵심 사회주의 가치'를 내재하도록 보장하도록 요구합니다. 반면에 중국 사이버보안 협회는 정부 규제, 정책 문서 및 국가 매체 보고서에서 획득한 1억개의 데이터 엔트리로 이루어진 '개발자 지원'으로 명명한 공개 데이터베이스를 발표했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChat Xi PT는 현재 연구 단계에 있어 공개되지 않고, 초대된 사용자 그룹에 의해 테스트 중입니다. 그러나 최대한 빨리 챗봇을 보다 많은 사용자들이 이용할 수 있도록 계획 중에 있습니다. 중요한 질문은 공개적으로 출시될 때가 아닌, Chat Xi PT와 같은 AI가 누구에게 실제적으로 유용할 수 있는지입니다. 아니면, 그것이 중국의 국가 프로파간다의 자기 만족에 불과하며 현실 세계에서의 다른 적용 가능성이 없을까요?\n\n## 참고\n\nª 2024년 5월 20일 뉴욕 타임스의 Li Yuan 작성 “중국 기업, 국내 독재주의와 해외 적대에 직면” 기사\n\n이야기를 읽어주셔서 감사합니다. 다음에 다시 뵙겠습니다. 의견, 피드백 또는 제안 사항이 있으면 언제든지 댓글을 남겨주십시오! 그리고 만약 감사의 표시로 이 이야기에 최대한 클랩을 주고 싶다면, 환영합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저자 소개: 샘 바세히는 공학 박사 학위를 보유하고 있으며 생명 과학과 공학 석사 학위를 취득했으며 하버드 비즈니스 스쿨의 지속가능성 및 스탠포드 대학의 머신 러닝 포닥 강좌를 이수했습니다. 스웨덴 거주의 독일 국적자인 그는 주요 산업에 전략, 혁신 및 지속 가능한 미래에 대한 독립적인 전문가 자문을 제공하고 세계 경제 포럼의 자문자로 등재되어 있습니다. 이전에 AOL 타임 워너, AT Kearney, 델로이트 및 E\u0026Y에서 일한 경험이 있으며 스웨덴 언론인 조합에서 자주 기자 자격을 인정받은 넓은 출판 이력을 가지고 있습니다.\n\n저자의 다른 관련 기사들:","ogImage":{"url":"/assets/img/2024-05-27-ChatXiPTandtheReinventionofCulturalRevolutionIdealsintheAgeofArtificialIntelligence_0.png"},"coverImage":"/assets/img/2024-05-27-ChatXiPTandtheReinventionofCulturalRevolutionIdealsintheAgeofArtificialIntelligence_0.png","tag":["Tech"],"readingTime":4},{"title":"언급되지 않은 수퍼 비밀 프롬프트 도구 지금은 아무도 말하지 않는","description":"","date":"2024-05-27 14:32","slug":"2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet","content":"\n![이미지](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_0.png)\n\n상상해봐: 당신이 책상에 앉아 커피 한잔을 쥔 채로, AI에게 TODO를 도와달라고 요청할 준비를 한 씬을.\n\n프롬프트를 입력하고 엔터키를 누르면... 실망스러운 결과가 나옵니다. 당신이 원하는 대답이 아닐 때.\n\n이런 일이 벌어진 적이 있나요? 그렇다면 당신만이 아닙니다. 많은 사람들이 AI로부터 좋은 답변을 얻는 데 애를 먹습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 걱정하지 마세요. 이 문제를 해결해주는 도구를 찾았어요.\n\n그 도구는 입력에 기반한 매우 상세한 프롬프트를 작성합니다. 이 도구는 사용하실 수 있고, Claude를 만든 Anthropic 회사에서 만들었습니다.\n\n그러니 여러분의 커피를 가져오시고, 함께 사용하는 방법을 알아봐요!\n\n# 완벽한 프롬프트 작성하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리가 사용할 도구는 Anthropic Prompt Generator라고 합니다. 이를 이용하려면 계정을 만들어야 합니다.\n\n## 1/ Anthropic 계정 만들기\n\n우선 가장 먼저, 마법이 일어나는 곳인 계정을 설정해야 합니다.\n\nAnthropic 웹사이트에 가서 가입하세요. 쉽고 무료입니다! 로그인하면 아래 이미지처럼 보이는 대시보드가 나타납니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_1.png)\n\n## 2/ Describe What You Want\n\nNow, think about what you need from AI.\n\nAre you making a playlist? Finding recipes? Whatever it is, describe it clearly.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 기분에 따라 플레이리스트를 만들고 싶어요.\n\n![Image](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_2.png)\n\n## 3/ 툴이 매력적인 부분 보여줘\n\n설명을 입력한 후에, 툴이 마법을 부릅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단한 지침을 받아서 매우 상세한 프롬프트로 바꿉니다. 특별한 것은 추가할 필요 없어요; AI가 잘 알고 있어요.\n\n![Prompt image](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_3.png)\n\n받은 프롬프트에 만족하셨다면 좋아요. 하지만 여러분에게는 선택권이 항상 있어요. (사진에서 강조된대로) 프롬프트를 편집할 수도 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4/ 수정 및 정제\n\n가끔은 첫 시도가 완벽하지 않을 수 있어요. 괜찮아요! 여전히 프롬프트를 편집하여 원하는 대로 완성시킬 수 있어요.\n\n![image](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_4.png)\n\n프롬프트에 만족하셨다면, 오른쪽 상단의 '실행(Run)'을 클릭하기만 하면 돼요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 5/ 완벽한 프롬프트 사용하기\n\n실행을 클릭하면 프롬프트 변수를 채우라는 메시지가 나타납니다.\n\n이 도구는 이러한 매개변수를 가져와 생성된 프롬프트에 삽입하여 결과를 제공합니다.\n\n![image](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저의 경우, 저를 흥분시키기 위한 80년대 \u0026 90년대 노래로 구성된 플레이리스트가 만들어졌어요.\n\n관심이 있다면, 여기가 플레이리스트 목록입니다:\n\n- “Walking on Sunshine” by Katrina and The Waves\n- “Livin’ on a Prayer” by Bon Jovi\n- “Girls Just Want to Have Fun” by Cyndi Lauper\n- “I Wanna Dance with Somebody (Who Loves Me)” by Whitney Houston\n- “Don’t Stop Believin’” by Journey\n- “Sweet Child O’ Mine” by Guns N’ Roses\n- “Uptown Funk” by Mark Ronson ft. Bruno Mars\n- “Wannabe” by Spice Girls\n- “U Can’t Touch This” by MC Hammer\n- “Pump Up the Jam” by Technotronic\n- “Celebration” by Kool \u0026 The Gang\n- “The Power of Love” by Huey Lewis \u0026 The News\n- “Everybody (Backstreet’s Back)” by Backstreet Boys\n- “Twist and Shout” by The Beatles\n- “Dancing Queen” by ABBA\n\n# 마지막으로 생각할 것들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 완벽한 프롬프트를 만드는 것이 얼마나 쉬운지 알게 되었어요. 그리고 당신이 AI 상호작용에서 최상의 결과를 얻는 데 아무런 장애물이 없습니다.\n\n만약 이 안내서를 즐겼다면 매주 이와 같은 안내서를 작성하고, 구독자들의 인박스로 직접 보내드립니다. 즐겁고 무료이며 가치가 충만해요.\n\n![image](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_6.png)\n\n이 이야기는 Generative AI에 게시되어 있어요. LinkedIn에서 저희와 연결하고 최신 AI 이야기에 대한 소식을 받으려면 Zeniteq를 팔로우하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최신 뉴스 및 생성 AI에 대한 최신 정보를 받아보려면 뉴스레터를 구독하세요. 함께 AI의 미래를 만들어 봅시다!\n\n![Image](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_7.png)\n","ogImage":{"url":"/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_0.png"},"coverImage":"/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_0.png","tag":["Tech"],"readingTime":4},{"title":"기존의 GPT-4보다 뛰어난 성능을 자랑하는 스탠포드 대학팀의 대형 모델이 모바일폰에서도 구동될 수 있다는 점이 인기를 끌며 하룻밤 사이에 2천 회 이상 다운로드되었습니다","description":"","date":"2024-05-27 14:30","slug":"2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight","content":"\n\n![Octopus v2](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_0.png)\n\n대규모 모델 구현의 과정에서, 엔드사이드 AI는 매우 중요한 방향입니다.\n\n최근 스탠퍼드 대학의 연구자들이 출시한 Octopus v2는 개발자 커뮤니티로부터 큰 관심을 받으며 인기를 끌고 있습니다. 모델의 다운로드 횟수가 하룻밤 사이에 2천 건을 넘었습니다.\n\n200억 개의 파라미터를 갖는 Octopus v2는 정확도와 대기 시간 측면에서 GPT-4를 능가하며, 콘텍스트 길이를 95% 줄였습니다. 또한, Octopus v2는 Llama7B + RAG 구성보다 36배 빠릅니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은 네티즌들이 한탄했습니다: 디바이스 측 인공지능 에이전트 시대가 도래했습니다!\n\n![image](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_1.png)\n\n- 논문: 옥토퍼스 v2: 슈퍼 에이전트용 기기 내 언어 모델\n- 논문 주소: https://arxiv.org/abs/2404.01744\n- 모델 홈페이지: https://huggingface.co/NexaAIDev/Octopus-v2\n\n모델 개요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n옥토퍼스-V2-2B는 안드로이드 API에 맞게 설계된 20 억 개의 매개변수를 가진 오픈 소스 언어 모델로, 안드로이드 기기에서 원활하게 실행되며 안드로이드 시스템 관리에서 여러 기기 및 다양한 응용 프로그램의 조작까지 확장하는 데 사용됩니다.\n\n![이미지](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_2.png)\n\n일반적으로 검색 증강 생성 (RAG) 방법은 잠재적인 기능 매개변수에 대한 상세한 설명을 필요로 하며 (때로는 수만 개의 입력 토큰이 필요할 수도 있음), 이에 기반하여 옥토퍼스-V2-2B는 훈련 및 추론 단계에서 고유한 기능 토큰 전략을 도입하여 GPT-4와 유사한 성능 수준을 달성할 뿐만 아니라 추론 속도를 크게 향상시켜 RAG 기반 방법을 능가합니다. 이로 인해 엣지 컴퓨팅 기기에 특히 유용합니다.\n\n![이미지](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문어-V2-2B는 다양한 복잡한 시나리오에서 개별, 중첩 및 병렬 함수 호출을 생성할 수 있습니다.\n\n데이터 세트\n\n훈련, 검증 및 테스트 단계에서 고품질 데이터 세트를 사용하고 특히 효율적인 훈련을 달성하기 위해, 연구팀은 데이터 세트를 세 가지 주요 단계로 생성했습니다:\n\n- 관련 쿼리 및 관련 함수 호출 매개변수 생성;\n- 적절한 기능 구성요소에서 관련이 없는 쿼리 생성;\n- Google Gemini을 통한 이진 검증 지원.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_4.png)\n\nResearch team은 이 모델을 훈련하기 위해 20가지의 안드로이드 API 설명을 작성했습니다. 다음은 예시 안드로이드 API 설명입니다:\n\n```js\ndef get_trending_news (category=None, region='US', language='en', max_results=5):\n    \"\"\"\n    카테고리, 지역 및 언어에 기반한 트렌드 뉴스 기사를 가져옵니다.\n    Parameters:\n    - category (str, optional): 필터링할 뉴스 카테고리입니다. 모든 카테고리에 대해 기본값으로 None을 사용합니다. 선택적으로 제공할 수 있습니다.\n    - region (str, optional): 지역별 뉴스를 위한 ISO 3166-1 알파-2 국가 코드입니다. 기본값으로 'US'를 사용합니다. 선택적으로 제공할 수 있습니다.\n    - language (str, optional): 기사 언어를 위한 ISO 639-1 언어 코드입니다. 기본값으로 'en'을 사용합니다. 선택적으로 제공할 수 있습니다.\n    - max_results (int, optional): 반환할 기사의 최대 수입니다. 기본값으로 5를 사용합니다. 선택적으로 제공할 수 있습니다.\n    Returns:\n    - list [str]: 각각 기사를 나타내는 문자열의 목록입니다. 각 문자열은 기사 제목과 URL을 포함합니다.\n    \"\"\"\n```\n\n모델 개발 및 훈련\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 연구는 프레임워크에서 Google Gemma-2B 모델을 사전 학습 모델로 사용하며 두 가지 다른 훈련 방법을 채택합니다: 전체 모델 훈련과 LoRA 모델 훈련.\n\n전체 모델 훈련에서는 AdamW 옵티마이저를 사용하며 학습률은 5e-5로 설정되고 웜업 단계 수는 10으로, 선형 학습률 스케줄러가 사용됩니다.\n\nLoRA 모델 훈련은 전체 모델 훈련과 동일한 옵티마이저와 학습률 구성을 사용하며 LoRA 랭크는 16으로 설정되며, LoRA는 다음 모듈에 적용됩니다: q_proj, k_proj, v_proj, o_proj, up_proj, down_proj. 그 중 LoRA 알파 매개변수는 32로 설정됩니다.\n\n두 훈련 방법 모두 에포크 수는 3으로 설정됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 코드를 사용하면 단일 GPU에서 Octopus-V2-2B 모델을 실행할 수 있습니다.\n\n```js\nfrom transformers import AutoTokenizer, GemmaForCausalLM\nimport torch\nimport time\n\ndef inference(input_text):\n    start_time = time.time()\n    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    input_length = input_ids[\"input_ids\"].shape[1]\n    outputs = model.generate(\n        input_ids=input_ids[\"input_ids\"],\n        max_length=1024,\n        do_sample=False\n    )\n    generated_sequence = outputs[:, input_length:].tolist()\n    res = tokenizer.decode(generated_sequence[0])\n    end_time = time.time()\n    return {\"output\": res, \"latency\": end_time - start_time}\n\nmodel_id = \"NexaAIDev/Octopus-v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = GemmaForCausalLM.from_pretrained(\n    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n\ninput_text = \"Take a selfie for me with front camera\"\nnexa_query = f\"아래는 사용자쿼리입니다. 올바른 함수를 호출하고 함수를 호출하는 매개변수를 생성하십시오.\\n\\n쿼리: {input_text}\\n\\n응답:\"\nstart_time = time.time()\nprint(\"넥사 모델 결과:\\n\", inference(nexa_query))\nprint(\"latency:\", time.time() - start_time, \"초\")\n```\n\n평가\n\n벤치마크 테스트에서 Octopus-V2-2B는 단일 A100 GPU에서 \"Llama7B + RAG 솔루션\"보다 36배 빠른 탁월한 추론 속도를 보여주었습니다. 게다가, Octopus-V2-2B는 클러스터화된 A100/H100 GPU에 의존하는 GPT-4-turbo보다 168% 빠릅니다. 이 효율적인 개선은 Octopus-V2-2B의 기능 토큰 디자인에 기인합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Octopus-V2-2B](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_5.png)\n\n옥토퍼스-V2-2B는 속도뿐만 아니라 정확도 면에서도 우수하며, \"라마7B + RAG 솔루션\"을 31% 초과하는 함수 호출 정확도로 능가합니다. 옥토퍼스-V2-2B는 GPT-4 및 RAG + GPT-3.5와 비교 가능한 함수 호출 정확도를 달성합니다.\n\n![Learn More](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_6.png)\n\n관심 있는 독자들은 연구 내용에 대한 원본 논문을 읽어서 더 많이 알아볼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n참고: [https://mp.weixin.qq.com/s/qnFZOPLpdRxW42_cLUcImA](https://mp.weixin.qq.com/s/qnFZOPLpdRxW42_cLUcImA)\n\n기사가 마음에 드셨나요? 더 많은 학습을 원하신다면 제한 없이 읽을 수 있는 Medium 회원이 되어보세요. 이 링크를 통해 회원이 되면 추가 비용 없이 저를 지원해 주시게 됩니다. 미리 감사드리고 앞으로 또 만나요!\n","ogImage":{"url":"/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_0.png"},"coverImage":"/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_0.png","tag":["Tech"],"readingTime":6},{"title":"ChatGPT를 활용한 스마트 대화를 위한 API 구현하기","description":"","date":"2024-05-27 14:28","slug":"2024-05-27-ImplementanAPIforSmartConversationswithChatGPT","content":"\nChatGPT API를 활용하여 Node.js 및 Express로 대화형 앱을 개발하겠다고 시작하세요.\n\n![이미지](/assets/img/2024-05-27-ImplementanAPIforSmartConversationswithChatGPT_0.png)\n\nChatGPT API는 철저히 문서화되어 있지만 상호 작용하는 방법에 대해 고민해야 합니다. 기능이나 함수를 구현하려고 노력하면서 때로는 놓치기 쉬운 부분들이 있습니다. 또한 명시적으로 언급되지 않았지만 강하게 암시되어 있는 구현 방법도 있습니다. ChatGPT를 적극 활용하는 애플리케이션을 개발 중인데, 이를 통해 얻은 교훈과 빠른 시작 방법을 소개하겠습니다!\n\n# 팁 및 Best Practices\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChatGPT의 API는 메시지와 함께 반드시 전송해야 하는 3가지 주요 역할이 있습니다:\n\n- “system” 메시지는 모델의 출력에 대한 세밀한 제어를 제공합니다. 특정 “인격”을 가진 모델을 만들고 싶다면 “system” 역할을 사용하여 무엇을 해야 하는지 알려주세요.\n- “assistant”는 모델에서 이전 메시지를 계속적인 대화의 맥락으로 보낼 때 사용해야 합니다. 이를 순서대로 유지하는 것이 좋습니다.\n- “user” 메시지는 사용자가 자신의 질문이나 발언으로 제출한 내용입니다.\n\n개발 프로세스에 관한 내용:\n\n- 역할은 대화를 주제에 맞게 유지하고 일관된 사용자 경험을 만드는 데 중요합니다. ChatGPT에 무작위 역할을 사용하여 메시지를 보내면 응답은 받을 수 있지만 사용 사례에 최적화되지는 않을 수도 있습니다.\n- 대화의 이력을 유지하고 사용자의 대화 중에 다시 보내야 합니다. 순서를 유지하는 것이 중요합니다.\n- 환경이 핫 리로드와 함께 빠른 반복 작업을 허용하는지 확인해야 합니다. 시스템 프롬프트 및 데이터 이동을 빈번하게 작업하고 수정할 것입니다.\n- 대화의 초기 시스템 컨텍스트(시딩)를 설정할 때, 원하는 바를 구체적으로 명시해야 합니다! 프롬프트 엔지니어링에 대한 다른 많은 글들이 있으므로 초기 메시지가 적절한지 확인해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주의할 점:\n\n- 시스템 프롬프트/시딩 사용은 모델을 훈련시키지 않습니다! 단지 모델을 사용 사례에 편향시킬 뿐입니다.\n- 환청이란 모델이 정보를 날조하거나 부정확하게 제시하는 것을 의미합니다. LLM은 서로 자주 놓이는 단어를 연달아 놓는 데 능숙합니다. 전문 지식이 필요한 사용 사례를 탐색할 때 이 점을 염두에 두세요!\n\n모든 것을 염두에 두고, 초기 구현을 진행합시다!\n\n독자들이 대부분 NodeJS, Visual Studio Code와 같은 코드 편집기, Docker 및 터미널을 사용할 준비가 되어 있는 것으로 가정합니다. 그렇지 않은 경우, 초기 환경을 설정하는 것이 다소 혼란스러울 수 있습니다. 이 기사의 끝에 이 기사의 코드를 포함한 Github 링크를 첨부하겠으며, 컨테이너화를 건너뛰고 싶은 경우 도커 없이도 프로젝트를 실행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시작하려면 코드 저장소의 루트에서 터미널에서 프로젝트 구조를 만들어봅시다:\n\n```js\nnpx create-react-app web --template typescript\n```\n\n```js\nmkdir api api/src\ntouch docker-compose.yaml api/Dockerfile web/Dockerfile\ncd api\nnpm init -y\ntouch src/index.ts\n```\n\n이러한 간단한 Docker 구성을 채워넣어 빠르게 반복 작업을 할 수 있도록 할 거에요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼 API 및 웹 프로젝트를 실행할 docker-compose 파일에 간단한 구성을 추가하겠습니다.\n\ndocker-compose up으로 도커 구성이 작동하는지 확인한 후 API 작업을 시작하겠습니다. 먼저 API 프로젝트로 이동하여 다음 명령을 실행하세요:\n\n```js\nnpm i typescript nodemon node-ts express @types/express cors @types/cors openai\n```\n\n이러한 종속성은 API가 트래픽을 제공하는 데 도움이 되는 기본 라이브러리가 될 것입니다. 간단한 익스프레스 API를 만들어 시작해볼까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 번 이 작동하면 curl http://localhost/:3010을 실행할 수 있고, 우리의 API는 Hello World로 응답할 것입니다.\n\n![이미지](/assets/img/2024-05-27-ImplementanAPIforSmartConversationswithChatGPT_1.png)\n\n이제 사용자 대화를 ChatGPT API로 전달할 수 있는 API의 기본 구조가 갖춰졌습니다! 신중하게 진행하기 위해, 진행하기 전에 다루고 있는 개념을 모델링해야 합니다.\n\nChatGPT 위에 단순히 다른 UI를 올려놓는 것이 아닌 무언가를 구축하려면, ChatGPT가 이미 제공하지 않는 이 앱이 무엇을 할 것인지에 대한 개념이 필요합니다. ChatGPT를 다루는 일반적인 Best Practice 중 일부는 대화 시작 시 모델에 personas를 제공하는 것입니다. 모두가 AI를 어떻게 이끌어야 하는지를 이해하는 시간과 의지가 없는 것이 아닐 수도 있으니, 우리는 일반 비기술 사용자를 돕기 위해 그들이 대화를 나눌 수 있는 몇 가지 personas를 만들어주는 것이 가능할 것입니다. 그것이 이 애플리케이션의 기본 개념이 될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n페르소나 구현을 용이하게 하기 위해, 사용하고자 하는 몇 가지 페르소나를 빨리 작성하고 코드베이스의 다른 부분에서 접근할 수 있도록 만들 것입니다. 이를 통해 애플리케이션의 다른 곳에서 우리가 추구하는 기능을 구현하는 데 사용될 것입니다. Craig the Builder, Tom the Gardener, Kate the Bartender, Jen the Beautician 이렇게 4가지 페르소나를 만들어 봅시다.\n\n이제 이 작업이 완료되었으므로, 다음과 같이 매우 간단한 API 엔드포인트를 작성할 수 있습니다:\n\n![API 엔드포인트 이미지](/assets/img/2024-05-27-ImplementanAPIforSmartConversationswithChatGPT_2.png)\n\n이제 이것이 설정되었기 때문에 사용자들이 페르소나와 대화할 수 있는 서비스 레이어를 구현할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChatGPT과 상호 작용할 때 3가지 특정 단계를 반복합니다. 대화를 초기화하는 것, 사용자 입력을 받는 것, 그리고 모델 응답을 하는 것이죠. 하나의 사용자는 동시에 여러 대화를 나눌 수 있습니다. 대화 중 일부는 단순히 처음 메시지 하나와 모델 응답이 이어질 수도 있고, 일부는 더 긴 형태일 수도 있습니다. 이 대화에서 메시지의 주고받는 순서는 중요할 수 있습니다.\n\n![ChatGPT](/assets/img/2024-05-27-ImplementanAPIforSmartConversationswithChatGPT_3.png)\n\n이 정보를 바탕으로 API를 구축하는 방법을 찾아야 합니다. 구현을 진행하기 위해 전형적인 POST/PUT/DELETE HTTP 동작을 사용할 것입니다.\n\n- 사용자는 API에 대화를 초기화함으로써 관심 있는 대화 상대자의 개인 정보를 POST하는 방식으로 대화를 시작합니다. 대화를 나중에 이어나갈 수 있도록 프론트엔드에서 생성된 대화 ID를 포함해야 합니다.\n- 대화는 새로운 사용자 입력을 API에 PUT하는 것을 통해 계속됩니다. 메시지와 함께 대화 ID를 포함해야 하며, 이를 통해 이전 대화를 쉽게 추출하여 관련 맥락을 유지할 수 있어야 합니다.\n- DELETE를 통해 대화를 삭제할 수 있습니다. 사용자는 더 이상 보고 싶지 않은 대화 ID를 전송하여 대화를 삭제할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAPI 엔드포인트를 고려할 때, 우리는 저장소가 가져야 할 기능을 알 수 있습니다. 이 기사의 범위를 벗어나 데이터 저장 솔루션을 정의하는 대신, 구독할 수 있는 인터페이스를 정의하고 간단한 메모리 내 저장 구현을 실행하겠습니다.\n\n이 인터페이스는 대화를 지속시키는 서비스의 기반이 될 것입니다. ChatGPT를 백엔드로 사용하기 때문에, ChatGPT에 대한 기본 기능을 특히 구현해야 하므로 각각의 기본 클래스를 구현한 후 각 레이어에 필요한 기능에 대해 논의하겠습니다. 먼저, ChatGptConversationService의 뼈대를 구현할 것입니다.\n\n이를 위해 openai 노드 라이브러리를 사용할 것입니다. 환경에 API 키를 저장하며, 이는 도커 컨테이너를 통해 로드됩니다. 9번째 줄에서는 ChatGpt로부터 수신된 메시지를 그대로 저장하고, 전송 날짜와 함께 장식합니다. 전체 대화는 해당 대화가 초기화된 페르소나와 함께 저장되며, 현재까지 수신/송신된 모든 메시지가 포함됩니다. 이를 통해 새로운 메시지마다 전체 대화를 ChatGpt API로 전송하여 대화의 문맥을 유지할 수 있습니다. 이는 \"이 두 가지 옵션에 대해 더 말씀해주시겠어요?\"와 같은 후속 메시지를 보낼 때 LLM이 무엇을 의미하는지 알고 있는지 여부를 결정할 수 있습니다.\n\n다음으로 InMemoryConversationService를 구조화하고, 생성자에서 ChatGptConversationService를 인스턴스화하여 한 번에 생성되도록 할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 \"실제\" 서비스 계층을 구현한다면 데이터의 백업 저장소와 사용자 세분화 등을 포함하여 더 많은 작업을 수행할 것입니다. 그러나 우리는 이 예제 앱을 이용하여 실제로 프로덕션 환경으로 넘어가려는 것이 아니에요. 이제 InMemoryDB 서비스와 ChatGPT에서 각 메서드 구현을 동시에 살펴봐서 각 계층에 왜 세부 정보가 \"살아있는지\"에 대해 이야기할 수 있게 됩니다.\n\n## GET\n\n여기서 시작하기 원하는 첫 번째 일은 두 서비스 간의 GET을 구현하는 것입니다. ChatGPT를 시작으로 하여 InMemory 서비스에 이어 진행하겠습니다:\n\nChatGpt 서비스에서는, 단순히 대화의 ID를 가져와 Promise.resolve를 통해 반환합니다 (이 서비스가 언젠가 백업 저장소를 사용하도록 업그레이드되는 경우를 대비하여 여기서 promises를 사용하고 있습니다). 또한 사용자 소비용 대화를 가져올 때 역할 \"system\"을 필터링하여 사용자가 우리의 페르소나 프롬프트를 보지 못하도록 합니다. 또한 chatgpt의 역할을 프론트엔드에서 사용하고자 하는 값으로 다시 매핑합니다 (user/bot).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nInMemory 서비스는 이 경우에 많은 가치를 더하지 않지만, 메시지가 undefined 또는 null을 반환하는 경우 API 레이어에서 문제를 발생시킬 가능성 대신 빈 배열을 반환할 수 있습니다.\n\n이제 API 레이어에서 API URL에서 대화 ID를 추출하여 매우 쉽게 활용할 수 있습니다. 이것은 기존 대화 메시지와 현재 대화 메시지를 모두 가져오는 우리 목적에 완벽히 작동할 것입니다.\n\n# POST\n\n다음으로 새 대화를 생성하고 초기 페르소나/메시지를 구현할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 채팅 순서를 유지하기 위해 메시지가 전송된 시간을 유지하고자 합니다. 따라서 먼저 ChatGpt 서비스에서 메시지가 전송된 시간을 생성합니다. 그런 다음 \"시스템\" 역할을 사용하여 페르소나의 \"성격\" 및 \"사용자\" 역할로 전송된 메시지를 사용하여 ChatCompletionMessageParam을 인스턴스화하여 API 호출을 위한 메시지를 생성합니다. 이렇게하면 openai.chat.completions.create 메소드를 model/messages 매개변수와 함께 사용할 수 있습니다. 응답이 돌아오면 해당 응답을 대화 ID에 저장하고 나중에 메시지를 정렬하는 데 사용할 수 있도록 전송된 날짜를 메시지에 투영합니다. 그런 다음 API의 응답을 반환합니다.\n\nInMemory 서비스로 돌아와서, ChatGpt API가 실제로 응답했는지를 다시 확인하고, 그렇지 않은 경우에는 오류를 발생시킵니다. 그렇지 않으면 기본 ChatGpt 서비스 getConversation 메서드에서 전체 대화를 반환합니다. 이는 매번 메시지를 보낼 때 응답이 모든 메시지를 포함하며 프론트엔드 시각화를 쉽게 교체할 수 있음을 의미합니다.\n\n이 작업은 기존 경로 /conversation/:id의 POST 메서드에 추가해야합니다. 이렇게하면 페르소나와의 새로운 대화를 시작할 수 있습니다. 간단한 CURL 요청으로 테스트할 수 있습니다:\n\n\u003cimg src=\"/assets/img/2024-05-27-ImplementanAPIforSmartConversationswithChatGPT_4.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# PUT\n\n'새 대화를 시작하려면 초기 상태를 설정하고 초기 인격/시스템 프롬프트로 대화를 시작해야 해서 \"추가\"적인 작업이 많이 필요합니다. 반면에 \"진행중인\" 대화 메서드와 엔드포인트는 비교적 간단할 것입니다.\n\nChatGpt 서비스의 sendMessages 함수는 새 메시지를 저장하고 유지하기 위해 메시지를 먼저 저장소에 푸시한 다음 ChatGpt API로 보냅니다. 그리고 응답이 돌아오면 그것을 날짜 투영을 다루는 private 메서드인 newMessage를 사용하여 저장합니다. 이것은 서로 다른 메시지에서 공통적으로 사용되는 함수이기 때문에 날짜 투영을 처리해주죠. ChatGpt 서비스는 메시지 내용을 반환하지만, 이전 방식대로 API가 응답했는지를 확인하기 위해 사용할 것이므로 이를 통해 엔드포인트에서 전체 대화를 반환할 것입니다.\n\n이것을 API에서 간단하게 연결할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기존 루트인 /conversation/:id에 PUT 메서드 아래에 이 메소드가 추가될 것입니다. 이를 통해 페르소나와의 대화를 계속할 수 있게 될 것입니다. 아래 요청은 이전 대화를 계속할 것입니다:\n\n![이미지](/assets/img/2024-05-27-ImplementanAPIforSmartConversationswithChatGPT_5.png)\n\n이제 GET 액션을 \"테스트\"하는 것이 중요합니다. 이전 요청과 동일한 내용을 반환할 것입니다. 그러나 테스트 목적으로 명령어는 다음과 같습니다:\n\n\n# DELETE\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희는 삭제 작업이 꽤 간단합니다. ChatGpt 서비스는 메모리 저장소만을 사용하고 있으므로 대화 ID로 저장된 데이터를 삭제할 수 있습니다. 이를 아주 빠르게 구현할 수 있습니다:\n\nInMemory 서비스는 삭제 세부 정보를 ChatGpt 서비스에 위임하며, 대화가 저장소에 있는지 확인하고 있다면 삭제하고 해결된 \"true\" 프로미스를 반환하여 삭제를 구현할 것입니다. 삭제를 테스트하려면 curl을 실행하여 동일한 대화를 삭제한 후 가져올 수 있습니다:\n\n![Conversation Listing](/assets/img/2024-05-27-ImplementanAPIforSmartConversationswithChatGPT_6.png)\n\n# 대화 목록\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금 API의 유일한 미해결 사항은 기존 대화 목록을 나열할 수 있는 기능이 없다는 것입니다. 현재 우리가 가지고 있는 구현으로는 매우 쉽습니다.\n\n지금 우리는 저장소를 반환하고 Object.keys를 활용하여 프론트엔드 표현으로 매핑할 수 있습니다. 표시하고 싶은 정보에는 사용 중인 페르소나와 첫 번째 메시지가 포함되어 있어서 최소한 그것을 대화의 메모로 표시할 수 있습니다. 대화에 대한 요약기를 작성하는 것이 이상적일 수 있지만, 여기서는 제품으로 출시할 준비를 하고 있지는 않습니다!\n\nAPI도 복잡하지 않습니다! 새 경로를 추가할 것이지만, 상당히 직접적인 호출로 연결할 수 있습니다:\n\n지금까지 백엔드에서의 구현은 ChatGpt와 함께 작업하는 세부 정보를 어떻게 처리할 것인지에 대한 내용과 InMemory 서비스 인터페이스의 모양을 보여줍니다. ChatGpt 서비스의 세부 정보를 인지하지만 해당 정보를 API 레이어로 노출시키지는 않습니다. 이를 통해 서비스 계층의 입력/출력을 조정할 수 있는 옵션이 주어지므로, ChatGpt 서비스 구현에는 영향을 미치지 않으면서 서비스 계층의 입력/출력을 조정할 수 있고 그 반대도 가능합니다. ChatGpt 서비스의 기본 저장소 공급자를 조정하여 백엔드에 동기화되거나 InMemory 서비스에 사용자 계정 ID를 추가하여 사용자들이 서로의 대화에 접근을 제한할 수도 있습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 앱에 대한 기본 프론트 엔드 구현은 간단합니다. 우리가 필요로 하는 API 엔드포인트는 오로지 5개뿐입니다 (GET Persona, GET Conversation List, GET/POST/PUT Conversation Messages). 대화 삭제는 여기에서 원하는 목표에 절대적으로 필요하지 않을 것으로 생각됩니다.\n\n우선, 웹 프로젝트로 이동하여 앱을 구현하는 데 필요한 것들을 조립해 보겠습니다. API 엔드포인트를 모두 설정해야 하므로, 동일한 유형을 활용하여 API 레이어를 구축하겠습니다.\n\n이는 매우 간단한 구현이며 URL을 쉽게 교체할 수 있는 기능이 부족하지만 필요 시 빠르게 구현할 수 있습니다.\n\n여기서부터는 스타일 선택에 따라 다릅니다. UI 예시는 다음과 같을 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Screenshot](/assets/img/2024-05-27-ImplementanAPIforSmartConversationswithChatGPT_7.png)\n\n이를 몇 가지 간단한 리액트 컴포넌트를 사용하여 쉽게 구현할 수 있습니다. 이 UI와 API는 다음 저장소에서 React로 구현되었습니다: https://github.com/christopherbauer/chatgpt-example. 실행하려면 OPEN_AI_API_KEY 변수를 설정한 .env 파일을 추가하고 프로젝트를 보통대로 실행하면 됩니다.\n\n# 보너스\n\n매우 다른 챗봇 역할을 보려면 다른 페르소나인 JaSON the Robot을 매우 쉽게 구현하여 JSON 코드로만 응답하게 만들 수 있습니다. 이를 통해 응답을 실제 JSON으로 해석하여 앱에 통합 가능성을 개선할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 매우 간단한 변화로 여러분이 작업할 수있는 추가적인 페르소나를 제공합니다. 몇 가지 다른 페르소나를 추가하고 그들의 반응을 확인해보는 것을 즐겨보세요!\n\n\n\u003cimg src=\"/assets/img/2024-05-27-ImplementanAPIforSmartConversationswithChatGPT_8.png\" /\u003e\n\n\nChatGPT API를 사용하여 앱을 빌드하려면 역할에 대한 좋은 이해, 잘 구조화된 백엔드, 그리고 사용자 친화적인 프론트엔드가 필요합니다. API의 세 가지 주요 역할인 \"시스템\", \"어시스턴트\", \"사용자\"는 대화를 추적하고 의미 있는 상호 작용을 제공하는 데 중요합니다.\n\n도커, NodeJS, Express를 사용하여 견고한 프로젝트 기반을 구축하고 ChatGPT와의 트래픽을 처리하는 기본 API를 설정할 수 있습니다. 페르소나를 추가하면 사용자 상호 작용이 향상되어 AI가 더 관련성 있고 매력적으로 보입니다. 대화 기록을 유지하고 역할을 활용하여 다양한 요구 사항을 해결하는 서비스를 개발할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 문서는 프로젝트를 시작하고 personas를 정의하며 필요한 API 엔드포인트를 구축하는 주요 단계를 다루었습니다. 더 심층적으로 알아보고 싶은 사람들을 위해, 링크된 GitHub 저장소에는 완전한 코드베이스가 포함되어 있으며 설정하기 쉽습니다. 이 실용적인 방법을 통해 특정 요구 사항을 충족하기 위해 사용자 정의할 수 있는 예제 응용 프로그램을 탐색하고 확장할 수 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-27-ImplementanAPIforSmartConversationswithChatGPT_0.png"},"coverImage":"/assets/img/2024-05-27-ImplementanAPIforSmartConversationswithChatGPT_0.png","tag":["Tech"],"readingTime":11},{"title":"스케치에서 프로페셔널 다이어그램으로 챗지피티를 활용한 플로우차트","description":"","date":"2024-05-27 14:25","slug":"2024-05-27-FlowchartswithChatGPTfromSketchtoProfessionalDiagram","content":"\n프로페셔널한 플로차트 작성이 매우 힘들었던 2020년의 이야기는 이제 그만 해요. AI의 확산으로 종이에 간단한 플로차트를 스케치한 다음 Mermaid 코드로 변환하는 것이 매우 쉬워졌어요. 이 Mermaid 코드는 많은 온라인 편집기에 붙여넣기하여 멋진 편집 가능한 플로차트를 얻을 수 있답니다.\n\n간단한 예시가 있어요. 저는 자전거 타기와 운전하기 중에 선택하는 데 도움이 되는 작은 플로차트를 스케치했어요. 제 필기가 좋지 않고 선이 곧바르지 않다는 걸 주의해 주세요. 이건 전혀 문제가 되지 않아요.\n\n무료로 OpenAI 계정을 만들고 새로운 채팅을 시작하세요. 2024년 5월 기준으로 이미지를 입력으로 받을 수 있는 유일한 모델인 \"ChatGPT 4o\" 버전을 선택했는지 확인하세요. 걱정하지 마세요, ChatGPT 4o는 적당한 사용 수준에 대해 무료에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Flowchart](/assets/img/2024-05-27-FlowchartswithChatGPTfromSketchtoProfessionalDiagram_1.png)\n\n\n친절하게 ChatGPT에게 이미지를 Mermaid 코드로 변환해달라고 요청한 후 이미지를 업로드하세요. 세부 대화가 다음과 같았습니다:\n\n(나): 안녕하세요! 플로우차트 이미지를 업로드할 예정입니다. Mermaid 코드로 변환해 주세요.\n(ChatGPT): 네! 이미지를 업로드하시고, 저가 Mermaid 코드로 변환하는 데 도움을 드리겠습니다.\n(나): 이미지를 여기 올립니다. Mermaid 코드를 생성해 주세요. (이미지 업로드)\n(ChatGPT): (Mermaid 코드 제공)\n\n이 내용을 온라인 Mermaid 편집기에 복사하여 붙여넣었습니다. 구체적으로는 https://workflow.jace.pro를 선택했는데, 무료이며 간편합니다. 결과는 아래와 같습니다. 제 손으로 스케치한 이미지보다 훨씬 깔끔하게 보입니다!\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\n![Flowchart](/assets/img/2024-05-27-FlowchartswithChatGPTfromSketchtoProfessionalDiagram_2.png)\n\nHints\n\n- ChatGPT을 사용할 때는 소유권 데이터를 업로드하는 데 주의하세요. 기본적으로 OpenAI는 귀하의 프롬프트를 다시 사용하지 않지만, 귀하의 소유권 데이터는 여전히 어딘가의 서버로 전송되며 우리는 그에 대해 완전히 확신할 수 없습니다.\n- 플로우차트를 스케치하는 첫 번째 시도에서 블록을 건너뛰는 실수를 범했습니다. 그렇지만 ChatGPT가 내게 정확히 채워 넣어 줬어요. 놀랍죠!\n\n\n\n\n","ogImage":{"url":"/assets/img/2024-05-27-FlowchartswithChatGPTfromSketchtoProfessionalDiagram_0.png"},"coverImage":"/assets/img/2024-05-27-FlowchartswithChatGPTfromSketchtoProfessionalDiagram_0.png","tag":["Tech"],"readingTime":2},{"title":"깊은 CNN 뒤의 수학  AlexNet","description":"","date":"2024-05-27 14:19","slug":"2024-05-27-TheMathBehindDeepCNNAlexNet","content":"\n`\u003cimg src=\"/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_0.png\" /\u003e`\n\n합성곱 신경망(Convolutional Neural Networks, CNNs)은 주로 이미지와 같은 구조화된 배열 데이터를 처리하기 위해 고안된 깊은 신경망의 특수한 유형입니다. CNN은 이미지의 픽셀 데이터에서 직접 패턴을 인식함으로써 수동으로 특징을 추출하는 필요성을 제거합니다. CNN은 이미지 내에서 공간적 계층을 이해하는 데 특히 강력하며, 데이터를 패치 단위로 처리하는 학습 가능한 필터를 활용하여 픽셀 간의 공간적 관계를 보존합니다.\n\n이러한 네트워크는 대량의 시각 데이터가 포함된 작업에서 매우 효과적이며, 이미지 및 비디오 인식부터 실시간 물체 감지에 이르기까지 다양한 응용 프로그램에서 널리 사용됩니다. 얼굴 인식 기술과 자율 주행 차량과 같은 발전에 중요한 역할을 하는 기술입니다.\n\n본 문서에서는 컴퓨터 비전 분야에 큰 영향을 미친 혁신적인 CNN 아키텍처인 AlexNet을 살펴볼 것입니다. 다양한 시각 인식 작업에서 강력한 성능으로 유명한 AlexNet은 복잡한 이미지를 직접 해석하기 위해 깊은 학습을 활용합니다. 그 동작 뒤에 있는 수학 및 코드 프레임워크를 자세하게 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 목차\n\n## 1. 소개\n\n## 2. AlexNet 아키텍처 개요\n\n- 2.1. 일반적인 레이어 구조\n- 2.2. 출력 레이어와 Softmax 분류\n\n## 3. AlexNet 구성 요소 깊이 있는 분석\n\n- 3.1. ReLU 비선형성\n- 3.2. 여러 개의 GPU에서의 훈련\n- 3.3. 지역 반응 정규화\n- 3.4. 겹치는 풀링\n- 3.5. 완전 연결 레이어와 드롭아웃\n- 3.6. 드롭아웃\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 4: 훈련 과정 및 최적화\n\n  - 4.1: 확률적 경사 하강법 매개변수\n  - 4.2: 초기화\n  - 4.3: 학습률 조정 전략\n\n- 5: Python에서 AlexNet 구축\n\n  - 5.1: AlexNet 클래스\n  - 5.2: 조기 중지 클래스\n  - 5.3: 트레이너 클래스\n  - 5.4: 데이터 전처리\n  - 5.5: 모델 훈련 및 평가\n\n- 6: 결론\n\n- 추가 자료\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 1: 소개\n\n![이미지](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_1.png)\n\nAlexNet은 2012년 ImageNet 대규모 시각 인식 챌린지에서 우승한 이후 주목받게 된 혁신적인 딥 러닝 네트워크입니다. Alex Krizhevsky, Ilya Sutskever 및 Geoffrey Hinton이 개발한 AlexNet은 이전 최고의 26.2%에서 상위 5% 오류율을 15.3%로 크게 낮추어 이 분야에 새로운 기준을 제시했습니다. 이 업적은 복잡한 이미지 분류 작업을 대규모 데이터셋에서 처리하는 데 ReLU 활성화, GPU 가속 및 드롭아웃 정규화를 사용하는 CNN의 효과를 강조했습니다.\n\n이 모델은 대부분의 딥 러닝 CNN에서 표준이 된 여러 계층으로 구성되어 있습니다. 이에는 합성곱 계층, 최대 풀링, 드롭아웃, 완전히 연결된 계층 및 소프트맥스 출력 계층이 포함됩니다. 이 모델의 성공은 설계 및 훈련에 대한 창의적인 접근을 통해 더 깊은 네트워크 아키텍처의 실용성을 보여 주었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글에서는 AlexNet의 정교한 디자인과 수학 원리를 분석해보겠습니다. AlexNet의 훈련 절차와 최적화 기술에 대해서도 살펴볼 것이며, PyTorch를 사용하여 AlexNet을 처음부터 구축해볼 것입니다.\n\n# 2: AlexNet 아키텍처 개요\n\n![AlexNet Architecture](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_2.png)\n\n## 2.1: 일반적인 레이어 구조\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAlexNet의 아키텍처는 각 레이어가 이전 레이어의 출력을 바탕으로하여 특징을 체계적으로 추출하는 방식으로 설계되어 있습니다. 다음은 레이어 및 기능의 상세한 분해 내용입니다:\n\n입력 이미지\n모델은 227x227 픽셀로 조정된 입력 이미지를 처리합니다. 각 이미지는 표준 RGB 인코딩을 반영하기 위해 세 개의 채널 (빨강, 녹색, 파랑)을 가지고 있습니다.\n\n레이어 구성\n주로 8개의 주요 레이어로 구성되어 있으며, 이 중 5개는 합성곱 레이어이고 나머지 3개는 완전히 연결된 레이어입니다. 이러한 레이어 사이에는 활성화 함수, 정규화, 풀링 및 드롭아웃이 전략적으로 적용되어 학습 효율성을 향상시키고 과적합을 줄입니다.\n\n합성곱 레이어\n최초 레이어는 96개의 커널(필터)을 사용하며, 크기는 11x11x3이며 4픽셀의 스트라이드를 사용하여 입력 이미지와 함께 컨볼루션됩니다. 이 큰 스트라이드 크기는 네트워크를 계산적으로 효율적으로 만들어 첫 번째 레이어부터 출력 공간 볼륨 크기를 크게 줄입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n첫 번째 레이어의 출력은 두 번째 컨볼루션 레이어에 도달하기 전에 정규화와 맥스 풀링을 거칩니다. 이 레이어는 각각 크기가 5x5x48인 256개의 커널로 구성됩니다. 48개의 특성 맵은 이전 레이어에서 별도로 필터링된 출력에 해당하여 이 레이어가 효과적으로 특성을 섞을 수 있도록 합니다.\n\n세 번째 컨볼루션 레이어는 일반적으로 이전 레이어에서 유도된 특성 맵의 풍부함을 유지하는 데 도움이 되는 풀링이나 정규화를 따르지 않습니다. 256개의 크기가 3x3x384인 커널이 사용되며, 이는 두 번째 레이어의 출력과 직접 연결되어 네트워크가 복잡한 특성을 포착할 수 있는 능력을 향상시킵니다.\n\n네 번째 컨볼루션 레이어는 세 번째 레이어의 구성을 반영하지만 크기가 3x3x192인 384개의 커널을 사용하여 네트워크의 깊이를 향상시키면서 레이어의 공간적 차원을 변경하지 않습니다.\n\n마지막 컨볼루션 레이어는 크기가 3x3x192인 256개의 커널을 사용하며 맥스 풀링 레이어가 뒤따르며, 학습 중인 특성에 회전 및 위치 불변성을 제공하고 차원을 줄이는데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n완전 연결 레이어들\n첫 번째 완전 연결 레이어는 4096개의 뉴런을 가진 밀집 레이어입니다. 이 레이어는 이전 합성곱 레이어에서 평탄화된 결과(1차원 벡터로 변환됨)를 받아와 비선형 특징들의 결합을 학습하기 위해 고차원 공간에 투영합니다.\n\n두 번째 완전 연결 레이어도 4096개의 뉴런을 포함하며 드롭아웃 정규화가 적용됩니다. 드롭아웃은 학습 중에 일정 비율의 입력 유닛을 무작위로 0으로 설정하여 과적합을 방지하고, 네트워크가 어떤 작은 뉴런 집합에 의존하지 않는 더 견고한 특징을 학습하도록 장려합니다.\n\n마지막 완전 연결 레이어는 1000개의 뉴런으로 이루어져 있으며, 각각은 ImageNet 챌린지의 클래스에 대응합니다. 이 레이어는 클래스 예측에 중요하며, 일반적으로 분류 확률을 유도하기 위해 소프트맥스 함수를 사용합니다.\n\n## 2.2: 출력 레이어와 소프트맥스 분류\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAlexNet의 마지막 레이어는 3번째 완전 연결 레이어의 logits에 softmax 함수를 적용하여 1000가지 클래스 레이블에 대한 분포를 출력하는 softmax 회귀 레이어입니다.\n\n소프트맥스 함수는 다음과 같습니다:\n\n\n$$\n\\frac{e^{z_i}}{\\sum e^{z_i}}\n$$\n\n\n여기서 zi는 최종 완전 연결 레이어에서 각 클래스에 대한 로짓 또는 원시 예측 점수입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 레이어는 각 클래스의 스코어를 지수화하여 모든 클래스의 스코어 합에 대비하여 확률로 변환하여 가장 가능성이 높은 클래스를 강조합니다.\n\nSoftmax 레이어는 이러한 확률을 출력하는 것뿐만 아니라 훈련 중 교차 엔트로피 손실의 기초를 형성하여, 예측된 확률 분포와 실제 분포(진짜 레이블) 사이의 차이를 측정합니다.\n\n# 3: AlexNet 구성 요소의 심도 있는 분석\n\n## 3.1: ReLU 비선형성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n렐루(Rectified Linear Unit, ReLU)는 특히 AlexNet과 같은 CNN(합성곱 신경망)에서 표준 활성화 함수가 되었습니다. 이 간단한 함수는 시그모이드 또는 하이퍼볼릭 탄젠트 함수를 사용하는 네트워크와 비교하여 모델이 더 빨리 학습하고 효과적으로 수렴하게 합니다.\n\nReLU의 수학적 표현은 간단합니다:\n\n![ReLU Function](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_4.png)\n\n이 함수는 x가 양수인 경우 x를 출력하며, 그렇지 않으면 0을 출력합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식으로 변경한 글입니다.\n\n![Ramp Function](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_5.png)\n\n그래픽적으로, 이 함수는 양수 입력에 대해 선형적으로 증가하고 음수 입력에 대해 0입니다.\n\nSigmoid가 Tanh에 우세한 점\nReLU는 시그모이드와 같은 전통적인 활성화 함수보다 여러 이점이 있습니다:\n\n![Advantages of ReLU Over Sigmoid](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하이퍼볼릭 탄젠트와 달리, ReLU 함수는 신경망이 수렴하는 데 도움을 줍니다.\n\nSigmoid와 tanh 함수에서 발생하는 사라지는 기울기 문제를 해결함으로써, 신경망이 더 빨리 수렴할 수 있습니다. 이 문제는 입력이 매우 커지면 (양방향으로 크게) 기울기가 매우 작아지는 경우 발생합니다. 이 작은 기울기로 인해 역전파 중에 가중치에 대한 업데이트가 거의 이루어지지 않아 교육 속도가 현저히 느려집니다. 이와는 대조적으로 ReLU 함수의 기울기는 음수 입력에 대해 0이고 양수 입력에 대해 1입니다. 이로써 기울기 하강을 간단하게 만들고 가속시킵니다.\n\n활성화의 희소성을 촉진합니다. 입력 도메인의 절반에 대해 0을 출력하여 희소 데이터 표현을 내재시킴으로써, 일반적으로 Sigmoid 또는 tanh 함수로 생성되는 밀집 표현보다 희소 표현이 더 유익하다는 것으로 알려져 있습니다. 이는 대규모 이미지 인식 작업에서 특히 유익하며, 거기에 내재된 데이터 차원은 매우 높지만 정보가 상대적으로 낮을 때 더 유용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한 ReLU는 간단한 수학 연산을 포함합니다. 이 활성화 함수는 어떤 입력 값에 대해서도 단일 최댓값 연산이 필요하며, 시그모이드와 하이퍼볼릭 탄젠트는 계산상 더 복잡한 지수 함수를 포함하고 있어 계산이 더 많이 필요합니다. ReLU의 이러한 간단함은 특히 대규모 데이터셋에서 심층 신경망을 훈련할 때 매우 빠른 계산 성능을 제공합니다.\n\nReLU 함수의 음수 부분이 제로처리되기 때문에, 시그모이드나 하이퍼볼릭 탄젠트 함수와 같이 비선형 방식으로 변경되지 않는 출력의 문제를 피할 수 있습니다. 이 특성은 네트워크가 데이터를 더 깨끗하게 모델링하고 훈련 동력에서 잠재적인 문제점을 피할 수 있도록 합니다.\n\n## 3.2: 여러 GPU에서 훈련\n\n![image](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_8.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**AlexNet**은 병렬 GPU 학습을 활용한 선도적인 합성곱 신경망 중 하나였습니다. 깊고 계산량이 많은 아키텍처를 효율적으로 다룰 수 있었습니다. 이 네트워크는 두 개의 GPU에서 동시에 작동하여 성능과 실용성을 크게 향상시키는 중요한 설계 요소입니다.\n\n**레이어별 분배**\nAlexNet의 레이어는 두 개의 GPU 사이에 분배됩니다. 각 GPU는 합성곱 레이어의 뉴런 활성화(커널)의 절반을 처리합니다. 구체적으로, 세 번째 레이어의 커널은 두 번째 레이어의 모든 커널 맵에서 입력을 받지만, 네 번째와 다섯 번째 레이어는 동일한 GPU에 위치한 커널 맵으로부터만 입력을 받습니다.\n\n**GPU 간 통신**\nGPU는 병렬 연산 결과를 통합하기 위해 출력을 결합해야 하는 특정 레이어에서 통신해야 합니다. 이 GPU 간 통신은 병렬 계산 결과를 통합하는 데 필수적입니다.\n\n**선택적 연결성**\nAlexNet의 모든 레이어가 두 개의 GPU에 모두 연결되는 것은 아닙니다. 이 선택적 연결성을 통해 GPU간 전송되는 데이터 양이 줄어들어 통신 오버헤드를 줄이고 계산 효율성을 향상시킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 두 가지 GPU 사이에 데이터셋 뿐만 아니라 네트워크 모델을 분할하는 전략은 AlexNet이 단일 GPU에서 실행될 때보다 더 많은 매개변수와 큰 입력 크기를 처리할 수 있도록 합니다. 추가적인 처리 능력은 AlexNet이 6000만 개의 매개변수 및 대규모 이미지 분류 작업을 효율적으로 학습하기 위해 필요한 방대한 계산을 처리할 수 있게 합니다.\n\n더 큰 배치 크기로 학습하는 것은 여러 GPU로 가능케 됩니다. 더 큰 배치는 훈련 중에 더 안정적인 기울기 추정을 제공하여 깊은 신경망을 효율적으로 훈련하는 데 중요합니다. 여러 GPU를 사용하는 결과가 아니더라도 더 큰 배치 크기로 훈련하고 더 빠른 반복 시간을 가지는 능력은 오버피팅을 대항하는 데 도움이 됩니다. 네트워크는 더 다양한 데이터 집합을 짧은 시간 내에서 경험하며, 이는 훈련 데이터로부터 보이지 않는 데이터로 일반화하는 능력을 향상시킵니다.\n\n## 3.3: Local Response Normalization\n\nAlexNet의 Local Response Normalization (LRN)은 이미지 분류 작업에서 뛰어난 성능을 발휘하는 네트워크의 중요한 역할을 하는 정규화 전략입니다. 이 기술은 ReLU 비선형 활성화 함수의 출력에 적용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLRN 레이어는 이웃하는 뉴런들이 높은 활동을 보일 때 그 뉴런들의 반응을 억제함으로써 각 뉴런의 정규화된 출력을 계산합니다.\n\n특징 맵 i의 (x, y) 위치에 있는 뉴런의 활동 ax, yi를 고려할 때, 반응 정규화된 활동 bx, yi는 다음과 같습니다:\n\n\\[ b*{x,y}^{i} = a*{x,y}^{i} / \\left( k + \\alpha \\sum*{j=max(0,i-n/2)}^{min(N-1, i+n/2)}(a*{x,y}^{j})^2 \\right)^{\\beta} \\]\n\n여기서:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- ax, yi는 (x, y) 위치에 커널 i를 적용하고 ReLU 함수를 적용하여 계산한 뉴런의 활성입니다.\n- N은 레이어 내 커널의 총 수입니다.\n- 합은 동일한 공간 위치에서 n개의 이웃 커널 맵을 대상으로 하며, N은 커널의 총 수입니다.\n- k, α, β는 미리 정해진 하이퍼파라미터이며 (일반적으로 AlexNet에서 n=5, k=2, α=10e−4, β=0.75).\n- bx, yi는 뉴런의 정규화된 응답입니다.\n\n로컬 응답 정규화(LRN)는 생물학적 뉴런에서 발견된 측면 억제 개념에서 영감을 받은 인접한 뉴런들 사이의 형태의 지역 억제를 구현하는 데 사용됩니다. 이 억제는 여러 중요한 영역에서 중요한 역할을 합니다:\n\n활동 조절\nLRN은 주변 지원을 받지 않는 더 큰 활성화를 처벌함으로써 네트워크의 응답을 압도하는 단일 특성 맵을 방지합니다. 주변 활성화의 제곱 및 합을 통해 어떠한 특성도 결과에 지나치게 영향을 미치지 않도록 보장하며, 여러 입력에 대한 모델의 일반화 능력을 향상시킵니다.\n\n대비 정규화\n이웃들에 비해 두드러져 보이는 패턴을 강조함으로써 LRN은 시각 처리에서의 대비 정규화와 유사하게 기능합니다. 이 기능은 이미지의 중요한 지역적 특성을 효과적으로 강조하여 시각적 구분 과정을 지원합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n에러율 감소\nAlexNet에 LRN을 통합함으로써 ImageNet 분류 작업에서 상위 1 및 상위 5의 에러율을 줄이는 데 도움이 되었습니다. 이는 뉴런의 높은 활동 수준을 관리하여 네트워크의 전체적인 견고성을 향상시키는 데 도움이 됩니다.\n\n## 3.4: 오버랩핑 풀링\n\n오버랩핑 풀링은 합성곱 신경망(CNN)에서 사용되는 기술로, 입력 데이터의 공간 차원을 줄이고, 계산을 간단하게 만들며, 과적합을 제어하는 데 도움이 됩니다. 이는 일반적인 비오버랩핑(전통적) 맥스 풀링을 변경하여 풀링 창이 겹치도록합니다.\n\n전통적인 맥스 풀링\n전통적인 맥스 풀링에서 입력 이미지 또는 피처 맵은 풀링 필터의 크기와 일치하는 각기 다른 비오버랩핑 영역으로 나누어집니다. 각 영역에서 최대 픽셀 값이 결정되고 다음 레이어로 출력됩니다. 이 과정은 비오버랩핑 이웃에서 가장 중요한 피처를 선택하여 데이터 차원을 줄이는 데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, 2x2 풀링 크기(z)와 2픽셀 간격(stride s)을 가정하면, 필터는 입력 필드를 2픽셀씩 가로로 이동하고 2픽셀씩 세로로 이동합니다. 2의 간격은 필터가 처리하는 영역 간에 겹침이 없음을 보장합니다.\n\nAlexNet의 중첩 풀링\nAlexNet에서 사용되는 중첩 풀링은 스트라이드를 풀 크기보다 작게 설정하는 것을 의미합니다. 이 접근 방식을 사용하면 풀링 영역이 서로 겹칠 수 있으며, 동일한 픽셀이 여러 번의 풀링 작업에 포함될 수 있습니다. 이는 피쳐 맵의 밀도를 높이고 레이어를 통해 더 많은 정보를 유지하는 데 도움이 됩니다.\n\n예를 들어, 3x3 풀링 크기와 2픽셀 간격을 사용하는 경우를 생각해 봅시다. 이 구성은 풀링 필터가 더 크지만(3x3), 이미지나 피쳐 맵을 건너갈 때마다 2픽셀씩만 이동한다는 것을 의미합니다. 결과적으로 인접한 풀링 영역은 처리되는 열 또는 픽셀 행을 공유하며, 기능 통합을 향상시킵니다.\n\n## 3.5: 완전 연결 레이어 및 드롭아웃\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAlexNet의 아키텍처에서 컨볼루션 및 풀링 레이어를 거친 후, 네트워크의 고수준 추론은 완전 연결 레이어에 의해 수행됩니다. 완전 연결 레이어는 컨볼루션 레이어에서 특징 맵을 추출한 후 최종 분류로의 전환에 중요한 역할을 합니다.\n\n완전 연결 (FC) 레이어는 이전 레이어의 모든 뉴런을 가져와서 (다른 완전 연결 레이어의 출력인지, 또는 풀링 또는 컨볼루션 레이어에서 나온 평탄화된 출력인지에 관곂여) 각 뉴런을 포함하는 모든 뉴런에 연결합니다. AlexNet에서는 컨볼루션과 풀링 레이어를 거친 후 세 개의 완전 연결 레이어가 이어집니다.\n\nAlexNet의 처음 두 완전 연결 레이어는 각각 4096개의 뉴런을 가지고 있습니다. 이러한 레이어는 이전 레이어에서 식별한 지역화된 필터링된 특징을 전역적이고 고수준의 패턴으로 통합하는 데 중요합니다. 최종 완전 연결 레이어는 실제로 분류기 역할을 합니다. (이미지넷 데이터셋 기준으로 1000개의) 각 클래스 레이블에 대한 뉴런을 가지며 입력 이미지의 카테고리에 대한 네트워크의 예측을 출력합니다.\n\n이러한 레이어의 각 뉴런은 출력 레이어를 제외하고는 ReLU (활성화 함수)를 적용합니다. 출력 레이어에서는 softmax 함수를 사용하여 출력 로짓(각 클래스에 대한 원시 예측 점수)을 클래스에 대한 확률 분포로 매핑합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최종 풀링이나 합성층에서의 출력은 일반적으로 완전 연결층으로 전달되기 전에 평탄화 과정을 거칩니다. 이 과정은 2차원 특징 맵을 1차원 특징 벡터로 변환하여 전통적인 신경망 기법을 통해 처리할 수 있도록 합니다. 최종 층의 소프트맥스 함수는 이 네트워크를 통해 학습된 특징 조합에 기반하여 각 클래스 레이블에 확률을 할당하여 입력 이미지를 분류합니다.\n\n## 3.6: 드롭아웃\n\n드롭아웃은 신경망에서 오버피팅을 방지하는 정규화 기법으로, 특히 AlexNet과 같은 대규모 네트워크에서 효과적입니다. 오버피팅은 모델이 훈련 데이터에 특정한 패턴을 학습하지만 새로운 데이터에는 일반화되지 않을 때 발생합니다.\n\nAlexNet에서는 드롭아웃을 첫 번째 두 완전 연결층의 출력에 적용합니다. 이 층의 각 뉴런은 확률 p(일반적으로 0.5로 설정, 즉 50%)으로 \"드롭\"됩니다. 즉, 해당 뉴런은 일시적으로 네트워크에서 제거되며 모든 들어오는 및 나가는 연결도 함께 제거됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 Dropout의 수학과 코드를 심층적으로 알고 싶다면, 제 이전 글의 3.4절을 살펴보시기를 강력히 추천합니다:\n\n# 4: 훈련 과정과 최적화\n\n## 4.1: 확률적 경사 하강법 매개변수\n\nAlexNet에서는 훈련 중에 네트워크를 최적화하기 위해 확률적 경사 하강법(SGD)을 사용합니다. 이 방법은 손실 함수의 오차 기울기를 기반으로 네트워크의 가중치를 업데이트하며, 배치 크기, 모멘텀, 가중치 감쇠 등 매개변수의 효과적 조정이 모델의 성능과 수렴에 중요합니다. 오늘의 글에서는 Pytorch의 SGD 구현을 사용할 것이며, 이 인기있는 최적화 기법에 대해 고수준의 내용을 다룰 것입니다. 만약 낮은 수준의 내용, 수학적으로 분석하고 최적화 기법을 처음부터 구성하는 것에 관심이 있다면, 이 글을 참조해보세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 SGD의 주요 구성 요소와 AlexNet에서 사용된 설정에 대해 알아보겠습니다:\n\n배치 크기\n배치 크기는 모델의 가중치를 업데이트하는 데 사용되는 훈련 예제 수로, 손실 함수의 경사도를 계산하는 데 사용됩니다. AlexNet에서 배치 크기는 128로 설정되어 있습니다. 이 크기는 더 많은 메모리 및 계산을 필요로하는 더 큰 배치와 더 많은 예제를 토대로 계산되어 생긴 정확도 사이의 균형을 유지합니다.\n\n128의 배치 크기를 선택한 것은 경사도 추정을 안정화시켜 업데이트를 더 부드럽고 신뢰할 수 있게 만들어줍니다. 더 큰 배치는 경사도 계산에서의 노이즈를 줄여 각 업데이트에 대한 더 명확한 신호를 제공하지만, 더 많은 계산 리소스가 필요하며 때로는 훈련 데이터로부터 새로운 상황으로 효과적으로 일반화시키지 못할 수도 있습니다.\n\n모멘텀\nSGD의 모멘텀은 올바른 방향으로 업데이트를 가속시키고 옵티마이저가 취한 경로를 부드럽게 만들어줍니다. 이는 이전 업데이트 벡터의 일부분을 포함하여 업데이트 규칙을 수정합니다. AlexNet에서 모멘텀 값은 0.9로 설정되어 있어, 이전 업데이트 벡터의 90%가 현재 업데이트에 기여합니다. 이 높은 모멘텀 수준은 작지만 일관된 경사도와 함께 작동할 때 특히 손실 함수의 최솟값으로 수렴 속도를 높여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모멘텀을 사용하면 업데이트가 올바른 방향으로 이동하는 것뿐만 아니라 일관된 기울기를 가진 손실 함수의 위상을 따라 속도가 증가하는 것을 보장합니다. 이 측면은 어떠한 얕은 지역 최솟값이나 산점이 더 효과적으로 탈출하는 데 중요합니다.\n\n가중치 감쇠\n가중치 감쇠는 큰 가중치를 처벌하는 정규화 항으로 작용하여 가중치 값의 일부를 손실 함수에 추가함으로써 사용됩니다. AlexNet은 이 매개변수를 0.0005로 설정하여 가중치가 너무 커지는 것을 방지하고 네트워크의 많은 매개변수로 인해 과적합이 발생할 수 있는 것을 예방합니다.\n\nAlexNet과 같이 복잡한 모델에서 높은 용량으로 인해 과적합되기 쉬운 상황에서 가중치 감쇠는 필수적입니다. 가중치의 크기를 처벌함으로써, 가중치 가중 특성에 지나치게 의존하지 않도록 하는 일반화된 모델을 유도합니다.\n\nAlexNet의 가중치에 대한 업데이트 규칙은 다음과 같이 설명할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 표는 아래와 같이 만들 수 있습니다.\n\n\n![image](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_10.png)\n\n여기에서:\n\n- vt는 이전 단계의 모멘텀 강화 업데이트 벡터입니다.\n- μ (AlexNet의 경우 0.9)는 이전 업데이트의 영향을 강화하는 모멘텀 요소입니다.\n- ϵ은 업데이트 단계의 크기를 결정하는 학습률입니다.\n- ∇L은 가중치에 대한 손실 함수의 기울기를 나타냅니다.\n- λ (AlexNet의 경우 0.0005)는 큰 가중치에 대한 처벌로 과적합의 위험을 줄이는 가중치 감쇠 요소입니다.\n- w는 가중치 자체를 나타냅니다.\n\n이러한 설정은 네트워크가 효율적으로 학습하고 보도 및 보지 않은 데이터에 대해 견고한 성능을 달성하도록 도와줍니다. 이는 학습 속도와 정확도를 최적화하고 일반화 능력을 유지하는데 도움이 됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4.2: 초기화\n\n딥 신경망을 훈련하는 데 있어 가중치와 편향을 적절하게 초기화하고 학습 속도를 조심스럽게 조절하는 것이 매우 중요합니다. 이러한 요소들은 네트워크가 수렴하는 속도와 훈련 및 검증 데이터에 대한 전반적인 성능에 영향을 미칩니다.\n\n가중치 초기화\n\n![이미지](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_11.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAlexNet에서 컨볼루션 레이어의 가중치는 평균이 0이고 표준 편차가 0.01인 정규 분포에서 초기화됩니다. 이 좁은 표준 편차는 초기에 어떤 단일 뉴런도 출력을 지배하지 못하게하여 가중치 초기화의 균일한 스케일을 보장합니다.\n\n마찬가지로, 완전 연결 레이어의 가중치도 가우시안 분포에서 초기화됩니다. 이 분포의 분산에 특별히 주의하여 레이어 간 출력 분산을 일관되게 유지하는 것은 더 깊은 네트워크의 안정성을 유지하는 데 중요합니다.\n\n이 과정을 더 잘 이해하기 위해 Python으로 AlexNet의 초기화를 처음부터 구축해 봅시다:\n\n```python\nimport numpy as np\n\ndef initialize_weights(layer_shapes):\n    weights = []\n    for shape in layer_shapes:\n        if len(shape) == 4:  # 이것은 conv 레이어입니다: (out_channels, in_channels, filter_height, filter_width)\n            std_dev = 0.01  # conv 레이어용 표준 편차\n            fan_in = np.prod(shape[1:])  # in_channels, filter_height, filter_width의 곱\n        elif len(shape) == 2:  # 이것은 완전 연결 레이어입니다: (out_features, in_features)\n            # He 초기화: std_dev = sqrt(2. / fan_in)\n            fan_in = shape[1]  # 입력 피처의 수\n            std_dev = np.sqrt(2. / fan_in)  # ReLU를 유지하는 것이 권장되는 분산\n        else:\n            raise ValueError(\"잘못된 레이어 형태입니다: 4D(conv) 또는 2D(fc)여야 합니다\")\n\n        # 가우시안 초기화\n        weight = np.random.normal(loc=0, scale=std_dev, size=shape)\n        weights.append(weight)\n\n    return weights\n\n# 예시 사용법:\nlayer_shapes = [\n    (96, 3, 11, 11),  # Conv1 레이어: 96 필터, 3 입력 채널, 11x11 필터 크기\n    (256, 96, 5, 5),  # Conv2 레이어: 256 필터, 96 입력 채널, 5x5 필터 크기\n    (384, 256, 3, 3), # Conv3 레이어: 384 필터, 256 입력 채널, 3x3 필터 크기\n    (384, 384, 3, 3), # Conv4 레이어: 384 필터, 384 입력 채널, 3x3 필터 크기\n    (256, 384, 3, 3), # Conv5 레이어: 256 필터, 384 입력 채널, 3x3 필터 크기\n    (4096, 256*6*6),  # FC1 레이어: 4096 출력 피처, (256*6*6) 입력 피처\n    (4096, 4096),     # FC2 레이어: 4096 출력 피처, 4096 입력 피처\n    (1000, 4096)      # FC3 (출력) 레이어: 1000 클래스, 4096 입력 피처\n]\n\ninitialized_weights = initialize_weights(layer_shapes)\nfor idx, weight in enumerate(initialized_weights):\n    print(f\"Layer {idx+1} weights shape: {weight.shape} mean: {np.mean(weight):.5f} std dev: {np.std(weight):.5f}\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ninitialize_weights 함수는 각 레이어의 가중치 차원을 설명하는 튜플 목록을 가져옵니다. 컨볼루션 레이어는 네 가지 차원(필터 수, 입력 채널, 필터 높이, 필터 너비)을 기대하고, 완전히 연결된 레이어는 두 가지 차원(출력 피처, 입력 피처)을 기대합니다.\n\n컨볼루션 레이어에서는 표준 편차가 0.01로 고정되어 있으며, 한 개의 뉴런에 의한 지나친 출력을 방지하기 위해 원래 AlexNet 구성과 일치시킵니다.\n\n완전히 연결된 레이어에서는 He 초기화(ReLU 활성화 함수를 사용하는 레이어에 대한 좋은 방법론)를 사용합니다. 여기서 표준 편차는 sqrt(2/fan_in)으로 조정되어 출력 분산을 일정하게 유지함으로써 딥 네트워크에서 안정적인 학습을 촉진합니다.\n\nlayer_shapes에 정의된 각 레이어에 대해 Gaussian(정규) 분포의 평균인 0에서 초기화된 가중치가 계산됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n편향 초기화\n어떤 합성곱 레이어의 편향은 1로 설정되며 특히 ReLU 활성화 함수 뒤에 오는 레이어에서 사용됩니다. 이 초기화는 뉴런 출력 값을 ReLU 함수의 양수 범위로 밀어넣어 훈련 초기부터 활성화되도록 보장합니다. 다른 레이어의 편향은 중립적인 출력을 위해 0으로 초기화됩니다.\n\n일부 합성곱 레이어와 마찬가지로, 완전히 연결된 레이어의 편향도 1로 설정됩니다. 이 전략은 훈련 초기에 뉴런이 양수 활성화 상태에 있도록하여 훈련 시작 시 죽은 뉴런을 방지하는 데 도움이 됩니다.\n\n## 4.3: 학습률 조정 전략\n\nAlexNet은 초기 학습률을 0.01로 시작합니다. 이 비율은 기울기에 상당한 업데이트를 허용해 초기 진행을 신속하게 돕습니다만, 학습 과정이 발산할 위험이 없는 정도로 너무 높지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n훈련 중에 미리 정해진 지점에서 학습률을 10 배 감소시킵니다. 이 방식은 \"단계 감소\"라고 알려져 있습니다. AlexNet에서 이러한 조정은 일반적으로 검증 오류율이 크게 감소하지 않을 때 발생합니다. 이러한 지점에서 학습률을 감소시킴으로써 가중치 조정을 미세 조정하여 더 나은 수렴을 이끌어냅니다.\n\n더 높은 학습률로 시작하는 것은 모델이 잠재적인 국지 최솟값을 더 효과적으로 극복하도록 도와줍니다. 네트워크가 안정화되기 시작하면 학습률을 줄이는 것이 더 넓고 평평한 최솟값으로 안정화되게 도와줄 수 있습니다. 일반적으로 이는 새로운 데이터에 대한 일반화에 더 적합합니다.\n\n훈련이 진행됨에 따라 학습률을 낮추면 보다 세밀한 가중치 조정이 가능해집니다. 이 점진적인 정제는 모델이 훈련 데이터에 더 잘 맞도록 도와주며, 검증 데이터에 대한 성능도 향상시켜 모델이 훈련 예시만 외우는 것이 아니라 이를 통해 일반화를 실제로 학습하게 도와줍니다.\n\n# 5: Python에서 AlexNet 만들기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 섹션에서는 PyTorch를 사용하여 Python에서 AlexNet을 재현하는 단계별 프로세스를 자세히 설명하여 클래스 아키텍처, 초기 설정, 훈련 절차 및 평가 기술에 대한 통찰을 제공합니다.\n\n오늘 다룰 모든 코드가 포함된 이 Jupyter 노트북을 열어 두는 것을 권장합니다:\n\n## 5.1: AlexNet 클래스\n\n먼저 AlexNet의 메인 클래스를 구축하는 것부터 시작해 봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# PyTorch로 신경망을 생성하고 학습하기 위한 코드입니다.\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data.dataset import random_split\n\n# 운영 체제 확인을 위한 모듈\nimport platform\n\n# 데이터셋을 로드하고 변환하기 위한 torchvision\nimport torchvision\nimport torchvision.transforms as transforms\n\n# 학습률을 조정하기 위한 ReduceLROnPlateau 모듈\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# 숫자 연산을 위한 numpy\nimport numpy as np\n\n# 시각화를 위한 matplotlib\nimport matplotlib.pyplot as plt\n\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n```\n\n`AlexNet` 클래스는 `nn.Module`을 상속하여 PyTorch에서 모든 신경망 모듈의 기본 클래스인 `nn.Module`을 사용합니다. PyTorch에서 새로운 신경망 구조는 `nn.Module`을 서브클래싱하여 생성됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n초기화 메서드는 AlexNet 객체가 생성될 때 어떻게 구성되어야 하는지를 정의합니다. num_classes 매개변수를 선택적으로 받아 출력 클래스의 수를 유연하게 조절할 수 있으며, 기본값은 ImageNet 작업에 일반적인 1000입니다.\n\n특성 레이어\n\n```js\nself.features = nn.Sequential(\n  nn.Conv2d(3, 64, (kernel_size = 11), (stride = 4), (padding = 2)),\n  nn.ReLU((inplace = True)),\n  nn.MaxPool2d((kernel_size = 3), (stride = 2)),\n  nn.Conv2d(64, 192, (kernel_size = 5), (padding = 2)),\n  nn.ReLU((inplace = True)),\n  nn.MaxPool2d((kernel_size = 3), (stride = 2)),\n  nn.Conv2d(192, 384, (kernel_size = 3), (padding = 1)),\n  nn.ReLU((inplace = True)),\n  nn.Conv2d(384, 256, (kernel_size = 3), (padding = 1)),\n  nn.ReLU((inplace = True)),\n  nn.Conv2d(256, 256, (kernel_size = 3), (padding = 1)),\n  nn.ReLU((inplace = True)),\n  nn.MaxPool2d((kernel_size = 3), (stride = 2))\n);\n```\n\n여기에 AlexNet의 합성곱 레이어가 정의됩니다. nn.Sequential 컨테이너는 레이어 시퀀스를 감싸고, 데이터는 추가된 순서대로 이러한 레이어를 통과합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nnn.Conv2d(3, 64, (kernel_size = 11), (stride = 4), (padding = 2));\n```\n\n첫 번째 레이어는 2D 합성곱 레이어(nn.Conv2d)로, 입력 채널은 3개(RGB 이미지)이고, 출력 채널은 64개(특성 맵)이며, 커널 크기는 11x11, 스트라이드는 4이고, 양쪽에 2씩 패딩이 적용됩니다. 이 레이어는 입력 이미지를 처리하고 특성 추출을 시작합니다.\n\n```js\nnn.ReLU((inplace = True));\n```\n\n그런 다음 ReLU 활성화 함수를 통과합니다. 이는 비선형성을 도입하여 모델이 복잡한 패턴을 학습하도록 합니다. inplace=True 매개변수는 입력을 직접 수정하여 메모리를 절약하는 데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nnn.MaxPool2d((kernel_size = 3), (stride = 2));\n```\n\n맥스 풀링 레이어는 입력 특성 맵의 공간 차원을 줄여주어 모델이 입력 이미지의 특징의 위치에 더 견고해지도록 합니다. 이 레이어는 3x3 크기의 창과 2의 보폭을 사용합니다.\n\n추가의 nn.Conv2d와 nn.MaxPool2d 레이어가 뒤따르며 특성 표현을 더욱 정제하고 간결하게 만듭니다. 각각의 합성곱 레이어는 일반적으로 풀링을 통해 특성 맵의 차원을 줄이면서 특성 맵의 수를 증가시키는데, 이는 공간적인 입력으로부터 점진적으로 더 많은 의미 정보를 포함하는 특징으로 추상화하는 데 도움이 됩니다.\n\n적응형 풀링 및 분류기\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nself.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n```\n\navgpool은 특징 맵을 자동으로 6x6의 고정 크기로 풀링하며, 완전 연결 레이어의 입력 크기 요구 사항과 일치시키기 위해 필요하며, 네트워크가 다양한 입력 차원을 처리할 수 있도록 합니다.\n\n```js\nself.classifier = nn.Sequential(\n  nn.Dropout(),\n  nn.Linear(256 * 6 * 6, 4096),\n  nn.ReLU((inplace = True)),\n  nn.Dropout(),\n  nn.Linear(4096, 4096),\n  nn.ReLU((inplace = True)),\n  nn.Linear(4096, num_classes)\n);\n```\n\nclassifier라는 또 다른 순차적 컨테이너를 정의했습니다. 이 컨테이너에는 네트워크의 완전 연결 레이어가 포함되어 있습니다. 이 레이어들은 합성곱 레이어에 의해 추출된 추상적인 특징에 기초하여 최종 분류를 수행합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nnn.Dropout()은 각 forward 호출마다 입력 텐서의 요소 중 일부를 확률이 0.5로 임의로 0으로 만들어 과적합을 방지하는 데 도움이 됩니다.\n\nnn.Linear(256 _ 6 _ 6, 4096)은 적응형 풀링 레이어의 네트워크망 피처들을 4096 크기의 벡터로 재구성합니다. 학습된 가중치로 각 입력을 모든 출력에 연결합니다.\n\n마지막으로 nn.ReLU 및 nn.Dropout 호출은 학습 경로를 더 정제하여 비선형 활성화 지점과 정규화를 제공합니다. 최종 nn.Linear 레이어는 차원을 4096에서 num_classes로 줄여 각 클래스에 대한 원시 점수를 출력합니다.\n\nForward 메소드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n```\n\n`forward` 메서드는 네트워크의 순전파 실행을 지시합니다:\n\n- `x = self.features(x)`는 입력을 컨볼루션 레이어를 통해 처음의 특성 추출을 수행합니다.\n- `x = self.avgpool(x)`는 특성에 적응적 풀링을 적용하여 크기를 표준화합니다.\n- `x = torch.flatten(x, 1)`은 출력을 벡터로 평탄화하여 분류를 위해 준비합니다.\n- `x = self.classifier(x)`은 평탄화된 벡터를 분류기를 통해 각 클래스에 대한 예측을 생성합니다.\n\n## 5.2: 조기 중단(Class)\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n훈련 중인 머신 러닝 모델이 유효성 검사 손실이 개선되지 않을 때 훈련 프로세스를 중지하는 EarlyStopping 클래스를 사용합니다. 이 방법은 오버피팅을 방지하고 최적의 시점에 훈련을 중지하여 컴퓨팅 자원을 절약하는 데 중요합니다.\n\n```js\nclass EarlyStopping:\n    \"\"\"\n    성능이 향상되지 않을 때 훈련을 중지하는 얼리 스톱핑 클래스\n\n    Args:\n    -----\n        patience (int): 훈련을 중지하기 전 대기할 에폭 수\n        verbose (bool): True인 경우 손실이 개선되지 않는 각 에폭마다 메시지 출력\n        delta (float): 개선으로 간주할 모니터링 중량의 최소 변경량\n    \"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n\n    def __call__(self, val_loss):\n        \"\"\"\n        Args:\n        -----\n            val_loss (float): 모델 성능이 개선되었는지 확인하는 검증 손실\n\n        Returns:\n        --------\n            bool: 손실이 개선되지 않았으면 True, 그렇지 않으면 False를 반환\n        \"\"\"\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n        elif score \u003c self.best_score + self.delta:\n            self.counter += 1\n            if self.counter \u003e= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.counter = 0\n```\n\n초기화\n\n```js\ndef __init__(self, patience=7, verbose=False, delta=0):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nEarlyStopping 클래스는 작동을 구성하는 여러 매개변수로 초기화됩니다:\n\npatience은 훈련을 중지하기 전에 검증 손실이 향상되기를 기다릴 에포크 수를 결정합니다. 기본값으로 7로 설정되어 있어서 모델이 손실 경치(plateaus)를 극복할 여지를 줍니다.\n\nverbose는 클래스의 출력을 제어합니다. True로 설정하면 손실이 개선되지 않는 각 epoch에 대한 메시지를 인쇄하여 훈련 중 명확한 피드백을 제공합니다.\n\n델타는 손실 개선으로 간주되는 임계값을 설정하여 조기 중지 메커니즘의 민감도를 미세 조정하는 데 도움을 줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**호출 가능한 메서드**\n\n```python\ndef __call__(self, val_loss):\n    score = -val_loss\n\n    if self.best_score is None:\n        self.best_score = score\n    elif score \u003c self.best_score + self.delta:\n        self.counter += 1\n        if self.counter \u003e= self.patience:\n            self.early_stop = True\n    else:\n        self.best_score = score\n        self.counter = 0\n```\n\n`__call__` 메서드는 EarlyStopping 인스턴스를 함수처럼 사용할 수 있도록 해주어 교육 루프에 통합하는 과정을 간단하게 만듭니다. 현재 에포크의 검증 손실을 기반으로 모델의 성능이 향상되었는지를 평가합니다.\n\n이 메서드는 먼저 검증 손실을 최대화해야 하는 점수로 변환합니다. 손실을 부정하여 이루어진 점수(score = -val_loss)입니다. 이것은 낮은 손실이 더 좋다는 것을 의미합니다. 이 첫 평가(self.best_score가 None)일 경우, 메서드는 현재 점수를 초기 best_score로 설정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 점수가 self.best_score에 작은 델타를 더한 값보다 적으면, 의미 있는 개선이 없음을 나타내므로 counter가 증가합니다. 이 counter는 개선이 없는 epoch가 몇 번 경과했는지를 추적합니다. counter가 인내 임계값에 도달하면, 학습을 중단해야 함을 나타내는 early_stop 플래그가 트리거됩니다.\n\n반대로, 현재 점수가 개선되면, 메소드는 새 점수로 self.best_score를 업데이트하고 counter를 0으로 재설정하여 미래 개선을 위한 새 기준을 반영합니다.\n\n이 메커니즘은 의미 있는 개선이 없는 지정된 epoch 횟수 후에만 학습 프로세스가 중지되도록 보장하므로, 학습 단계를 최적화하고 과소적합 모델로 이어질 수 있는 조기 중지를 방지합니다. 인내와 델타를 조정함으로써 사용자는 학습 성능의 변화에 대해 조기 중지가 얼마나 민감한지를 조정할 수 있어서 특정 시나리오와 데이터셋에 맞게 사용자 정의할 수 있습니다. 이 맞춤 설정은 계산 자원과 시간의 제약 사항에 따라 최상의 모델을 얻기 위해 중요합니다.\n\n## 5.3: Trainer 클래스\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n트레이너 클래스는 전체 훈련 워크플로우를 포함하며, 에포크를 반복하고 훈련 루프를 관리하며 역전파를 처리하고 훈련 효율성과 효과를 최적화하기 위해 조기 중지 프로토콜을 구현합니다.\n\n```js\nclass Trainer:\n    \"\"\"\n    모델을 훈련하는 Trainer 클래스.\n\n    Args:\n    -----\n        model (nn.Module): 신경망 모델.\n        criterion (torch.nn.modules.loss): 손실 함수.\n        optimizer (torch.optim): 옵티마이저.\n        device (torch.device): 모델을 실행할 장치.\n        patience (int): 훈련을 중지하기 전까지 기다리는 에포크 수.\n    \"\"\"\n    def __init__(self, model, criterion, optimizer, device, patience=7):\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.device = device\n        self.early_stopping = EarlyStopping(patience=patience)\n        self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', patience=3, verbose=True, factor=0.5, min_lr=1e-6)\n        self.train_losses = []\n        self.val_losses = []\n        self.gradient_norms = []\n\n    def train(self, train_loader, val_loader, epochs):\n        \"\"\"\n        모델을 훈련합니다.\n\n        Args:\n        -----\n            train_loader (torch.utils.data.DataLoader): 훈련 데이터셋을 위한 DataLoader.\n            val_loader (torch.utils.data.DataLoader): 검증 데이터셋을 위한 DataLoader.\n            epochs (int): 모델을 훈련할 에포크 수.\n        \"\"\"\n        for epoch in range(epochs):\n            self.model.train()\n            for images, labels in train_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n\n                self.optimizer.zero_grad()\n                outputs = self.model(images)\n                loss = self.criterion(outputs, labels)\n                loss.backward()\n                self.optimizer.step()\n\n            self.train_losses.append(loss.item())\n\n            val_loss = self.evaluate(val_loader)\n            self.val_losses.append(val_loss)\n            self.scheduler.step(val_loss)\n            self.early_stopping(val_loss)\n\n            # 훈련 및 검증 손실 기록\n            print(f'에포크 {epoch+1}, 훈련 손실: {loss.item():.4f}, 검증 손실: {val_loss:.4f}')\n\n            if self.early_stopping.early_stop:\n                print(\"조기 중지\")\n                break\n\n    def evaluate(self, test_loader):\n        \"\"\"\n        테스트 데이터셋에 대해 모델을 평가합니다.\n\n        Args:\n        -----\n            test_loader (torch.utils.data.DataLoader): 테스트 데이터셋을 위한 DataLoader.\n\n        Returns:\n        --------\n            float: 테스트 데이터셋의 평균 손실.\n        \"\"\"\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for images, labels in test_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n\n                outputs = self.model(images)\n                loss = self.criterion(outputs, labels)\n                total_loss += loss.item()\n\n        return total_loss / len(test_loader)\n\n    def accuracy(self, test_loader):\n        \"\"\"\n        테스트 데이터셋에서 모델의 정확도를 계산합니다.\n\n        Args:\n        -----\n            test_loader (torch.utils.data.DataLoader): 테스트 데이터셋을 위한 DataLoader.\n\n        Returns:\n        --------\n            float: 테스트 데이터셋에서 모델의 정확도.\n        \"\"\"\n        self.model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for images, labels in test_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n\n                outputs = self.model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        return correct / total\n\n    def plot_losses(self, window_size=100):\n        # 이동 평균 계산\n        train_losses_smooth = self.moving_average(self.train_losses, window_size)\n        val_losses_smooth = self.moving_average(self.val_losses, window_size)\n\n        # 그래프 그리기\n        plt.plot(train_losses_smooth, label='훈련 손실')\n        plt.plot(val_losses_smooth, label='검증 손실')\n        plt.legend()\n        plt.grid()\n        plt.title('손실')\n\n    def moving_average(self, data, window_size):\n        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n```\n\n초기화\n\n```js\ndef __init__(self, model, criterion, optimizer, device, patience=7):\n    self.model = model\n    self.criterion = criterion\n    self.optimizer = optimizer\n    self.device = device\n    self.early_stopping = EarlyStopping(patience=patience)\n    self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', patience=3, verbose=True, factor=0.5, min_lr=1e-6)\n    self.train_losses = []\n    self.val_losses = []\n    self.gradient_norms = []\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTrainer 클래스는 인공 신경망 모델, 손실 함수, 옵티마이저 및 모델이 실행될 장치(CPU 또는 GPU)로 초기화됩니다. 이 설정을 통해 모든 모델 연산이 적절한 하드웨어로 전달되도록 보장됩니다.\n\n또한 조기 종료 및 학습률 감소 전략을 구성합니다:\n\n- 조기 종료: 검증 손실을 모니터링하고 주어진 에포크 수(인내심) 동안 개선이 없는 경우 훈련을 중지합니다.\n- ReduceLROnPlateau: 검증 손실의 개선이 멈추면 학습률을 줄이는데, 이는 가중치 공간에서 더 작은 단계를 밟아 모델을 세밀하게 조정하는 데 도움이 됩니다.\n\n여기서 train_losses와 val_losses는 각각 훈련 및 검증 단계의 에포크당 손실을 수집하여 성능 추적 및 나중 분석을 위해 사용됩니다. gradient_norms는 기울기의 크기를 저장하는 데 사용될 수 있으며, 디버깅 및 기울기가 소멸되거나 폭발하지 않도록 확인하는 데 유용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n훈련 방법\n\n```js\ndef train(self, train_loader, val_loader, epochs):\n    for epoch in range(epochs):\n        self.model.train()\n        for images, labels in train_loader:\n            images, labels = images.to(self.device), labels.to(self.device)\n\n            self.optimizer.zero_grad()\n            outputs = self.model(images)\n            loss = self.criterion(outputs, labels)\n            loss.backward()\n            self.optimizer.step()\n\n        self.train_losses.append(loss.item())\n\n        val_loss = self.evaluate(val_loader)\n        self.val_losses.append(val_loss)\n        self.scheduler.step(val_loss)\n        self.early_stopping(val_loss)\n\n        # 훈련 및 검증 손실 기록\n        print(f'Epoch {epoch+1}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')\n\n        if self.early_stopping.early_stop:\n            print(\"조기 종료\")\n            break\n```\n\n이 훈련 방법은 지정된 epoch 수동안 모델 훈련을 조정합니다. 데이터 일괄 처리, 역전파를 통한 모델 가중치 업데이트, 각 epoch 종료 시 검증 세트를 사용하여 모델 성능을 평가합니다.\n\n각 epoch 이후에 훈련 및 검증 손실을 기록하고 필요 시 학습률을 업데이트합니다. 조기 종료 조건이 트리거된 경우, 검증 손실을 평가한 후 조기 중지할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n평가 및 정확도 방법\n\n```python\ndef evaluate(self, test_loader):\n    self.model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(self.device), labels.to(self.device)\n\n            outputs = self.model(images)\n            loss = self.criterion(outputs, labels)\n            total_loss += loss.item()\n\n    return total_loss / len(test_loader)\n\ndef accuracy(self, test_loader):\n    self.model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(self.device), labels.to(self.device)\n\n            outputs = self.model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return correct / total\n```\n\n평가 방법은 주어진 데이터셋(일반적으로 검증 또는 테스트 세트)에서 모델의 성능을 평가하고 평균 손실을 반환합니다. 이 방법은 모델을 평가 모드로 설정하고 데이터셋을 반복하여 각 배치에 대해 손실을 계산하고 모든 배치를 통해 평균 손실을 계산합니다.\n\naccuracy는 예측된 레이블을 실제 레이블과 비교하여 주어진 데이터셋에서 모델의 정확도를 계산합니다. 이 방법은 평가 모드에서 데이터셋을 처리하고 모델의 예측을 사용하여 올바른 예측 수를 계산하고 정확도 백분율을 반환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시각화를 위한 유틸리티 메서드\n\n```js\ndef plot_losses(self, window_size=100):\n        # 이동 평균 계산\n        train_losses_smooth = self.moving_average(self.train_losses, window_size)\n        val_losses_smooth = self.moving_average(self.val_losses, window_size)\n\n        # 플롯\n        plt.plot(train_losses_smooth, label='훈련 손실')\n        plt.plot(val_losses_smooth, label='검증 손실')\n        plt.legend()\n        plt.grid()\n        plt.title('손실')\n\n    def moving_average(self, data, window_size):\n        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n```\n\n이 메서드는 모델이 과적합을 시작하거나 시간이 지남에 따른 손실 감소와 같은 추세를 더 명확하게 보여주기 위해 지정된 epoch 창을 통해 부드럽게 표시된 학습 및 검증 손실을 시각화합니다.\n\n## 5.4: 데이터 전처리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAlexNet 모델을 효과적으로 훈련시키려면 해당 모델의 입력 요구 사항에 맞게 데이터 전처리가 필요합니다. 구체적으로는 AlexNet이 원래 디자인된 차원과 정규화 표준에 부합해야 합니다.\n\n변환하기\n\n```js\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # AlexNet 호환을 위해 이미지 크기를 224x224로 조정\n    transforms.ToTensor(),  # 이미지를 PyTorch 텐서로 변환\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 텐서를 정규화\n])\n```\n\ntransforms.Resize((224, 224))는 이미지의 크기를 224x224 픽셀로 조정하여 AlexNet 모델에서 필요로 하는 입력 크기와 일치시키며, 모든 입력 이미지가 동일한 크기를 갖도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ntransforms.ToTensor()은 이미지를 PIL 형식이나 NumPy 배열에서 PyTorch 텐서로 변환합니다. PyTorch 모델은 입력을 텐서 형식으로 기대하기 때문에 이 과정은 필수적입니다.\n\ntransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))은 이미지 텐서를 정규화합니다. 이 구체적인 정규화는 세 개의 채널 (RGB)에 대해 평균과 표준 편차를 0.5로 조정하여 픽셀 값을 [-1, 1] 범위로 조정합니다. 이 단계는 입력을 표준화하여 모델의 학습 과정을 원활하게 만드는 데 중요합니다.\n\n데이터셋 로딩\n\n```js\ntrainset = torchvision.datasets.CIFAR10((root = \"./data\"), (train = True), (download = True), (transform = transform));\n\ntestset = torchvision.datasets.CIFAR10((root = \"./data\"), (train = False), (download = True), (transform = transform));\n\nclasses = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\");\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 CIFAR-10 데이터 세트를 훈련 및 테스트용으로 로드합니다. 미리 훈련된 모델을 학습하는 데 널리 사용되는 ImageNet 데이터 세트를 선택하지 않았을 수도 있습니다. ImageNet은 상당한 컴퓨팅 리소스와 오랜 학습 시간이 필요하므로 일반 노트북에서 시도하기를 권장하지 않습니다. 대신, CIFAR-10 데이터 세트를 선택했는데, 이 데이터 세트는 10가지 다른 클래스로 분산된 60,000장의 32x32 컬러 이미지를 포함하고 있습니다. 각 클래스당 6,000장의 이미지가 있습니다.\n\n참고: CIFAR-10 데이터 세트는 MIT 라이선스에 따라 오픈 소스로 사용할 수 있습니다. 이 라이선스는 상용 응용프로그램을 포함하여 다양한 용도로 자유롭게 사용할 수 있습니다.\n\n분할 및 데이터 로더\n\n```python\ntrain_split = 0.8\ntrain_size = int(train_split * len(trainset))\nval_size = len(trainset) - train_size\ntrain_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n훈련 데이터는 80%를 훈련용으로, 20%를 검증용으로 나눠놨어요. 이 방식은 모델을 보이지 않은 데이터로 튜닝하여 적절한 일반화 능력을 향상시키는 데 자주 사용됩니다.\n\n훈련, 검증 및 테스트 데이터셋을 배치 크기 64로 생성하기 위해 DataLoader 객체를 사용했어요. 훈련 데이터는 셔플링을 통해 무작위성을 보장하며, 이는 모델이 데이터의 순서에서 잘못된 패턴을 배우는 가능성을 줄여 더 효과적으로 학습하게 도와줍니다.\n\n데이터 시각화\n\n```js\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\ndef imshow(img):\n    img = img / 2 + 0.5  # 정상화해주세요\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\nimshow(torchvision.utils.make_grid(images[:5]))\nprint(' '.join('%5s' % classes[labels[j]] for j in range(5)))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저 이미지를 되돌리기 위해 (img = img / 2 + 0.5)를 사용합니다. 여기서 imshow는 텐서를 넘파이 배열로 변환하고, 차원 순서를 matplotlib.pyplot.imshow()에서 요구하는 형식으로 변경합니다.\n\n그런 다음, 데이터 세트에서 첫 번째 5개 이미지를 표시합니다:\n\n![이미지](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_12.png)\n\n## 5.5: 모델 훈련 및 평가\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내 AlexNet 모델의 훈련 환경을 설정했고, PyTorch를 사용하여 훈련 프로세스를 실행하고 테스트 데이터셋에서 모델의 성능을 평가했습니다.\n\n하지만 먼저, 성능 효율성을 극대화할 최상의 컴퓨팅 리소스(CPU 또는 GPU)를 사용하는지 확인해야 합니다.\n\n```js\n# 시스템의 운영 체제 확인\nif platform.system() == 'Darwin':  # Darwin은 macOS의 약칭입니다\n    try:\n        device = torch.device('cuda')\n        _ = torch.zeros(1).to(device)  # CUDA 사용 가능 여부에 따라 오류가 발생합니다\n    except:\n        device = torch.device('mps' if torch.backends.mps.is_built() else 'cpu')\nelse:\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n```\n\n여기서는 시스템이 macOS('Darwin')인지 식별하고 CUDA를 사용하도록 구성을 시도합니다. 일반적으로 macOS에서 NVIDIA GPU가 없어 CUDA를 사용할 수 없는 경우, Apple의 Metal Performance Shaders(MPS)를 사용할 수 있는 경우 MPS를 선택하거나 그렇지 않으면 CPU를 선택합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nmacOS 이외의 운영 체제에서는 CUDA를 직접 사용하려고 시도하고 CUDA를 사용할 수 없는 경우 CPU로 기본 설정됩니다.\n\n모델, 손실 함수 및 옵티마이저 초기화\n다음으로, AlexNet 모델을 초기화하고 계산 장치를 지정하고 손실 함수 및 옵티마이저를 설정합니다:\n\n```python\nmodel = AlexNet(num_classes=10).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n```\n\n10개 클래스로 AlexNet의 인스턴스가 생성되며, 즉시 지정된 장치(GPU 또는 CPU)로 전송됩니다. 이를 통해 모델의 모든 계산이 지정된 장치에서 수행되도록 보장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCrossEntropyLoss 함수는 다중 클래스 분류 문제에 대해 일반적으로 사용됩니다.\n\nSGD (확률적 경사 하강법) 옵티마이저는 모델의 매개변수, 학습률 0.01 및 모멘텀 0.9로 초기화됩니다. 이것들은 많은 시각 기반 작업에 대해 시작할 때 표준값입니다.\n\n모델 훈련\n모델은 지정된 epoch 수 동안 훈련되며 데이터를 배치로 처리하고 손실을 계산하며 역전파를 수행하고 검증 손실을 기반으로 조기 중지를 적용합니다:\n\n```js\ntrainer = Trainer(model, criterion, optimizer, device, (patience = 7));\ntrainer.train(train_loader, val_loader, (epochs = 50));\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ntrain 메서드는 훈련 및 검증 데이터 로더를 사용하여 모델을 50번의 에포크 동안 훈련시킵니다. 이 메서드는 데이터 로더에서 배치를 세심하게 처리하고 손실을 계산하며 가중치를 업데이트하기 위한 백프로파게이션을 수행하고, 검증 데이터셋을 사용하여 모델을 정기적으로 평가하여 검증 손실에 개선이 없을 경우 조기 중단을 구현합니다.\n\n모델 평가\n훈련 후에는 다음과 같이 테스트 세트에서 모델의 성능을 평가합니다.\n\n```js\ntest_loss = trainer.evaluate(test_loader)\nprint(f'Test Loss: {test_loss:.4f}')\n\naccuracy = trainer.accuracy(test_loader)\nprint(f'Test Accuracy: {accuracy:.2}')\n```\n\n마지막으로, 훈련 및 검증 손실을 시각화하여 모델의 학습 진행 상황을 모니터링합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ntrainer.plot_losses((window_size = 3));\n```\n\n이 코드는 plot_losses 메소드를 호출하여 훈련 및 검증 손실을 시각화합니다. 손실 값은 윈도우에 스무싱되어 있습니다 (이 경우 3개의 데이터 포인트로) 노이즈 없이 트렌드를 더 잘 시각화하기 위해. 이 코드를 실행하면 다음과 같은 손실을 기대할 수 있습니다:\n\n\u003cimg src=\"/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_13.png\" /\u003e\n\n위 그래프에서 보듯이, 모델 훈련은 21번의 epoch 후에 중지되었고 우리가 인내 값으로 7을 설정하고 14번째 epoch 이후에는 검증 손실이 개선되지 않았기 때문입니다. 이 설정은 교육 목적을 위해 만들어졌으므로 AlexNet을 능가하는 것이 목표가 아님을 명심해 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n개발자님, 친구 같은 톤으로 번역해드리겠습니다.\n\n에포크 수나 인내심을 늘려 검증 손실이 더 떨어질 수 있는지 확인해보는 것을 권장합니다. AlexNet의 성능을 향상시킬 수 있는 변경 및 업데이트 사항이 몇 가지 있습니다. 이 기사에서는 30분 시간 제한으로 인해 이러한 조정 사항을 다루지 않지만, 모델 성능을 개선할 수 있는 다양한 고급 기술을 탐색할 수 있습니다.\n\n더 많은 실험을 원하시는 분들을 위해 학습률 조정, 네트워크 아키텍처 조정, 더 고급 정규화 방법 사용 등 매개변수 조정을 시도해보세요. 더 최적화 및 세밀한 조정 기술들은 다음 기사에서 더 자세히 탐구할 수 있습니다:\n\n# 6: 결론\n\nAlexNet은 신경망 설계 및 학습 기술의 발전에서 중요한 모델로, 딥러닝 분야에서 중요한 이정표 역할을 하였습니다. ReLU 활성화, 겹치는 풀링, 그리고 GPU 가속 학습의 혁신적인 사용은 신경망의 효율성과 효과성을 현저히 향상시켜, 모델 아키텍처에 새로운 기준을 제시하였습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAlexNet으로의 드롭아웃 및 데이터 증강 기법 도입은 신경망의 오버피팅 문제를 해결하고 일반화 능력을 향상시켜 다양한 작업에 더 견고하고 다재다능하게 만들었습니다. 이러한 기술들은 현대 딥 러닝 프레임워크에서 핵심적인 역할을 하며, 다양한 후속 혁신에 영향을 미쳤습니다.\n\n## 추가 자료\n\n- Krizhevsky, A., Sutskever, I., \u0026 Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Advances In Neural Information Processing Systems. [링크](http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf)\n- LeCun, Y., Bengio, Y., \u0026 Hinton, G. (2015). Deep learning. Nature, 521(7553), 436–444. [링크](https://doi.org/10.1038/nature14539)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCristian Leo (2024). The Math Behind Convolutional Neural Networks, [link](https://medium.com/towards-data-science/the-math-behind-convolutional-neural-networks-6aed775df076)\n\n마지막까지 읽어주셔서 감사합니다! 이 글이 마음에 드셨다면 좋아요를 눌러 주시고 제 팔로우도 부탁드립니다. 앞으로도 비슷한 글을 정기적으로 업로드할 예정이에요. 제 목표는 가장 인기있는 알고리즘들을 처음부터 다시 만들어 기계 학습을 모두에게 접근하기 쉽게 만드는 것입니다.\n","ogImage":{"url":"/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_0.png"},"coverImage":"/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_0.png","tag":["Tech"],"readingTime":40},{"title":"대규모 언어 모델에서 추론을 위한 인-컨텍스트 학습 방법","description":"","date":"2024-05-27 14:17","slug":"2024-05-27-Retrieval-BasedIn-ContextLearningforReasoninginLargeLanguageModels","content":"\n\n대규모 언어 모델(LLM)은 소수의 입력-출력 예제만 제공되어도 혁신적인 능력을 보여주는 것으로 입증되었습니다. 적은 양의 예제만 제공되어도 LLM은 새로운 작업에 빠르게 적응할 수 있으며 어떠한 기울기 업데이트나 파인튜닝도 필요하지 않습니다. 이로써 LLM은 전례 없는 유연성과 일반화 능력을 나타낼 수 있었습니다.\n\n그러나 ICL에서 LLM의 성능은 제공된 인-컨텍스트 데모의 선택에 매우 민감합니다. 각 새로운 작업에 최적인 예제 세트를 수동으로 선택하는 것은 도전적이며 편향을 도입할 수 있습니다. 이에 대응하기 위해 RetICL이라는 신생 접근 방식은 사용자 지정된 각 입력 쿼리에 맞는 데모 세트를 동적으로 검색하는 방법을 제안합니다.\n\n본 기사에서는 RetICL의 주요 아이디어에 대해 깊이 파고들고, 현재 RetICL 시스템 및 응용 프로그램의 현황을 조사하며 이 희망적이지만 미발달 분야의 핵심 가정 몇 가지를 살펴볼 것입니다. RetICL이 LLM 추론을 향상시키는 데 성과를 보여주는 반면, 이러한 성과의 진정한 근원과 더 견고하고 충실한 RetICL 방법으로 나아가는 과정에 대한 여전히 미해결된 문제들이 존재함을 확인할 것입니다.\n\n강력한 에이전트 응용 프로그램을 가능하도록 하는 열쇠가 될 데이터 엔지니어링 도전이 될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Retrieval-Based ICL Overview](/assets/img/2024-05-27-Retrieval-BasedIn-ContextLearningforReasoninginLargeLanguageModels_0.png)\n\n# Retrieval-Based ICL 개요\n\nRetICL의 핵심 아이디어는 ICL에서 일반적으로 사용되는 고정된 인-컨텍스트 데모 세트를 새로운 입력 쿼리에 맞게 선택된 관련 예제로 동적 검색 프로세스로 대체하는 것입니다. 수동으로 선택된 정적 예제 세트에 의존하는 대신에 RetICL 시스템은 기존 지식 베이스 및 검색 모델을 활용하여 정보 쿼리의 문맥에 가장 적합한 데모를 실시간으로 찾아냅니다.\n\n이 접근 방식은 표준 ICL에 비해 여러 잠재적인 장점을 제공합니다. 첫째, 쿼리별 데모 검색을 통해 RetICL은 각 새로운 작업에 적응할 수 있도록 LLM에 보다 관련성 높은 정보를 제공할 수 있습니다. 둘째, RetICL은 고정 예제 세트를 선별하는 데 관여되는 수동적 노력과 잠재적인 편견을 줄입니다. 마지막으로 대규모 지식 베이스의 활용을 통해 RetICL 시스템은 단일 프롬프트에 맞게 구현할 수 있는 정보 범위를 상당히 확장할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 혜택을 고려하면 RetICL은 빠르게 연구 분야의 핫한 주제가 되었으며, 다양한 평가에서 제안된 시스템과 격려하는 결과가 증가하고 있습니다. 동시에 RetICL은 아직 발전 중인 분야로, 검색 시스템의 최적 디자인과 그 효과적인 원인에 대한 여러 개방적인 질문이 많이 있습니다.\n\n# RetICL 시스템의 주요 디자인 선택 사항\n\n기존의 RetICL 시스템은 다음과 같은 여러 중요한 차원을 통해 특징화될 수 있습니다:\n\n검색 모델: RetICL 시스템의 핵심 구성 요소는 주어진 쿼리에 대한 관련된 데모를 선택하는 데 사용되는 검색 모델입니다. 하나의 접근 방식은 \"벼림되어있는\" 검색기(예: 밀도가 높은 단락 검색)을 사용하는 것이며, 이들은 대규모 데이터세트에서 사전 훈련되어 텍스트를 임베딩으로 인코딩하고 유사한 예제를 효율적으로 찾습니다. 반대로, 일부 RetICL 시스템은 도메인별 데이터에서 검색기를 세밀하게 조정하여 대상 작업에 대한 검색 정밀도를 향상시킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n검색 목표: 데모를 선택할 때 중요한 질문 중 하나는 유사성 또는 다양성을 최적화할지 여부입니다. 유사성 기반 방법은 쿼리와 가장 의미론적으로 관련된 예제를 찾는 데, 다양성 기반 방법은 더 넓은 범위의 관련 정보를 포함하는 보완적인 데모 세트를 찾는 데 초점을 둡니다. 실제로 많은 RetICL 시스템은 두 가지 목표를 조합하여 사용합니다.\n\n검색 말뭉치: 또 다른 중요한 설계 선택은 검색에 사용되는 지식 베이스입니다. 일부 RetICL 시스템은 작업별 훈련 세트와 같은 정성화된 도메인별 말뭉치에 초점을 맞출 수 있으며, 다른 시스템은 Wikipedia와 같은 대규모 오픈 도메인 지식 베이스를 사용할 수 있습니다. 검색 말뭉치의 선택은 검색된 데모의 관련성과 품질에 큰 영향을 미칠 수 있습니다.\n\n검색 전략: 마지막으로, RetICL 시스템은 여러 검색된 데모를 통합하는 다른 전략을 사용할 수 있습니다. 가장 간단한 접근 방법은 단일 단계 검색으로, 상위 순위의 고정된 수의 데모가 함께 연결됩니다. 더 정교한 시스템은 반복적 검색을 사용할 수 있으며, 각 선택된 데모를 기반으로 쿼리 인코딩을 업데이트하여 후속 반복에서 더 다양한 예제를 찾을 수 있습니다.\n\n이러한 설계 선택의 많은 구성이 있지만 경험적 결과는 RetICL의 효과가 특정 도메인 및 모델링 목표에 크게 의존한다는 것을 시사합니다. 일반적인 해결책은 없을 것으로 보이며, 보다 체계적인 비교 및 제거 실험의 필요성을 도출합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 디자인 데모 전부터 학문위키\n\nRetICL에서 중요한 과제는 검색 모델 자체를 학습하는 것입니다. 어떻게 하면 직접적인 지도 없이 가장 정보가 풍부한 예시를 찾을 수 있을까요? 대부분의 기존 방법은 두 가지 범주로 나뉩니다:\n\nLM 기반 지도: 일반적인 전략 중 하나는 LLM이 금 예상을 생성하는 확률을 사용하여 후보 데모의 유용성을 대리로 하는 것입니다. 그런 다음 검색 모델은 이 확률을 최대화하는 데모를 선택하도록 학습됩니다. 이는 LLM 자체의 능력을 활용하여 관련 예시를 식별하는 데 도움이 됩니다.\n\n모델 없는 휴리스틱: 대안은 n-gram overlap 또는 entity matching과 같은 작업 무관한 휴리스틱을 사용하여 예시의 적절성을 추정하는 것입니다. LM 기반의 지도보다는 능력이 떨어지지만, 이러한 방법은 보다 효율적이고 검색 말뭉치에 적용 가능합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n훈련 데이터를 이러한 접근 방식 중 하나로 얻은 후, 리트리버는 일반적으로 대조적 학습이나 가르침과 같은 표준 순위 목표를 사용하여 최적화됩니다 [2]. 최근에는 점진적으로 더욱 어려운 부정적 예를 채굴하는 훈련 방법들이 리트리버의 견고성을 향상시키는 데 유망함을 보여주기도 했습니다 [2].\n\n# 적용 및 영향\n\nRetICL은 다양한 언어 작업에서 LLM 성능을 향상시키는 데 적용되었습니다. 일부 주목할만한 성과로는 다음이 있습니다:\n\n추론: RetICL은 복잡한 추론이 필요한 multi-hop 질문 응답 및 사실 검증 데이터셋에서 강력한 결과를 달성했습니다 [1,2]. 관련 사실과 설명을 검색함으로써 RetICL은 LLM이 더 정확하고 신뢰할 수 있는 추론 추적을 생성하도록 도울 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n질문 답변: 오픈 도메인 QA 벤치마크에서 RetICL은 대규모 지식 베이스에서 관련 문단을 동적으로 검색함으로써 표준 ICL을 능가했습니다. 이는 텍스트와 이미지에 대한 추론을 필요로 하는 멀티 모달 QA 작업을 포함합니다.\n\n텍스트 생성: 요약 및 대화와 같은 언어 생성 작업에서 RetICL은 문맥 내 예제를 제공하여 LLMs가 더 일관되고 관련성 있는 스타일에 맞는 출력으로 이끌어줄 수 있습니다.\n\n보다 일반적으로, RetICL은 새로운 도메인과 작업에 LLMs를 효과적으로 적응시키는 유망한 접근 방식을 제공하며, 파인튜닝의 계산 비용 없이 외부 지식을 실시간으로 활용하는 것을 학습함으로써 RetICL은 유연하고 확장 가능한 지식 통합을 LLMs로 가능하게 할 수 있습니다.\n\n동시에 RetICL은 여전히 중요한 한계에 직면하고 있습니다. RetICL 시스템의 성능은 검색에 사용된 지식 베이스의 관련성과 품질에 따라 달라집니다. 이러한 지식 베이스의 노이즈나 빈 곳은 환각적이거나 편향된 출력으로 이어질 수 있습니다. RetICL은 검색 단계에서 추가적인 계산 오버헤드를 도입하지만, 효율적인 인덱스 구조로 이를 완화할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# RetICL의 기반 조사\n\nRetICL은 흥미를 자아내지만, 최근의 연구에서는 이러한 시스템의 기반을 면밀히 조사하고 그 근본적인 추론 능력에 대한 진정한 본질을 의심하기도 시작했습니다.\n\n한 연구에서 ReAct, RetICL의 인기있는 프레임워크를 철저히 분석한 결과 [3], 입력 프롬프트의 다양한 요소를 주의 깊게 조절하여, 저자들은 ReAct의 특별한 성능이 본래 주장된 추론과 행동의 교차능력에서 나오지 않았음을 발견했습니다. 대신, 모델의 출력이 낮은 수준의 프롬프트 세부사항에 매우 민감하며 그 \"추론\"이 주로 얕은 검색이 아니라 의미 있는 심사숙고 대신 이루어졌다는 것을 발견한 것입니다 [3].\n\n구체적으로, 이 연구에서는 ReAct의 성능이 주로 주장된 추론과 행동 교차능력이라는 것에 의해 나오는 것이 아니라 제공된 예제 프롬프트의 선택에 매우 민감하다는 것을 발견했습니다. 예제 프롬프트가 체계적으로 변화할 때, ReAct의 출력은 주로 얕은 검색과 패턴 매칭에 더 의존하는 것처럼 보였으며, 실제로 언어 모델의 추론 능력을 증진하는 것보다는 오히려 예제에 대한 검색과 비슷한 패턴 매칭에 더 의존하는 것으로 나타났습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRetICL 방법이 언어 모델의 강력한 추론을 가능하게 하는지, 아니면 모델이 단순히 프롬프트 동안 제공된 특정 예제에 연결된 휴리스틱을 학습하는 것인지 의문이 제기됩니다.\n\n가장 중요한 점은 RetICL 시스템에서 얻은 이득의 기저를 이루는 예제 프롬프트의 선택이 중요한 요소로 보인다는 것입니다.\n\n이러한 결과는 RetICL 시스템의 획기적인 이득을 해석할 때 신중함이 필요함을 강조합니다.\n\nLLM의 추론 및 일반화 능력을 얼마나 향상시키는지에 따라, 그들이 진정으로 휴리스틱을 배우는지 알아봐야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음에는 더 많은 프롬프트와 순열에 걸쳐 RetICL 시스템을 보다 철저히 테스트하고 생성된 추론 추적에 대해 미세하게 평가하는 것이 필요합니다 [3].\n\n# 향후 방향\n\n앞으로 나아가면, RetICL을 향상시키고 효과적이며 견고하게 만들기 위한 많은 흥미로운 방법이 있습니다. 몇 가지 주요 방향은 다음과 같습니다:\n\n오픈 도메인 지식: 현재 RetICL 시스템은 주로 특정 작업과 관련된 정리된 지식 베이스에 의존합니다. 중요한 새로운 과제는 웹과 같은 대규모, 완전하게 구조화되지 않은 소스에서 관련 정보를 신뢰할 수 있게 얻을 수 있는 리트리버를 개발하는 것입니다. 이는 규모에 걸친 잡음, 불일치, 검색 효율성 등의 문제를 처리하는 것을 필요로 합니다 [1].\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지식 퓨전: 다른 기회는 여러 가지 모달리티와 지식 소스를 효과적으로 통합하는 것입니다. 예를 들어, 과학 질문에 답변하기 위한 RetICL 시스템은 교과서, 방정식, 다이어그램, 실험 데이터에서 정보를 결합해야 할 수도 있습니다. 이러한 다양한 정보 원천을 효과적으로 조화시키는 리트리버를 설계하는 것은 중요한 도전 과제입니다.\n\n신뢰할 만한 추론: RetICL 시스템에서 생성된 추론 기록의 해석 가능성과 신뢰성을 향상시키는 것이 그들의 안전한 배포를 위해 중요합니다. 이는 환각을 줄이기 위해 검색결과의 관련성을 향상시키고, 검색된 지식을 더 신뢰할 수 있는 방식으로 일관된 사고 연쇄로 운영할 수 있는 LLMs를 설계하는 것을 포함합니다.\n\n이론적 이해: 경험적으로, 유사한 예제를 검색하는 것이 문맥 내 학습을 개선하는 것이 강력하게 증명되었습니다. 그러나 이 현상의 이론적 기초는 여전히 모호합니다. LLMs의 적응을 돕는 검색의 이점이 왜 그리고 어떻게 지식을 가져오는지에 대한 수학적 모델을 개발하는 것은 미래 RetICL 시스템의 설계를 지도할 가치 있는 통찰을 제공할 수 있습니다.\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRetICL은 각 작업에 대해 관련된 데모를 동적으로 선택하여 LLM의 추론 능력을 향상시키는 유망한 패러다임으로 등장했습니다. 대규모 말뭉치의 지식을 활용하여 RetICL은 LLM이 새로운 도메인에 유연하게 적응하고 더 근거 있는 정확한 작업 완료를 생성하는 데 도움을 줄 수 있습니다.\n\n그러나 이 잠재력을 실현하기 위해서는 검색 모델, 교육 목표, 지식 원본 및 추론 시간 전략의 주요 설계 선택 사항을 주의 깊게 탐색해야 합니다. 또한 RetICL 시스템의 추론 능력을 검증하고 결과물이 충실하고 편향되지 않고 견고하다는 것을 보장하기 위해 추가적인 경험적 및 이론적 분석이 필요합니다.\n\n중요한 미해결 문제가 남아 있지만, RetICL의 신속한 진전은 LLM이 외부 지식과 자유로운 추론 및 생성을 가능하게 하는 미래를 엿보게 해줍니다. 이를 이루기 위해서는 핵심 검색 및 추론 능력을 확장하는 창의력과 한계와 잠재적 위험을 면밀히 검토하는 주의가 필요합니다. 앞으로의 도전적인 길이 기대됩니다.\n\n참고 문헌:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[1] Luo et al. (2023) “In-context Learning with Retrieved Demonstrations for Language Models: A Survey”\n\n[2] Rubin et al. (2022) “Learning to Retrieve Prompts for In-Context Learning”\n\n[3] Verma et al. (2024) “On the Brittle Foundations of ReAct Prompting for Agentic Large Language Models”","ogImage":{"url":"/assets/img/2024-05-27-Retrieval-BasedIn-ContextLearningforReasoninginLargeLanguageModels_0.png"},"coverImage":"/assets/img/2024-05-27-Retrieval-BasedIn-ContextLearningforReasoninginLargeLanguageModels_0.png","tag":["Tech"],"readingTime":7},{"title":"SOFTS 시리즈-코어 퓨전을 활용한 효율적인 다변수 시계열 예측","description":"","date":"2024-05-27 14:15","slug":"2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion","content":"\n## 새로운 MLP 기반 모델인 SOFTS는 혁신적인 STar Aggregate-Dispatch (STAD) 모듈을 활용하여 계산 복잡성을 이차 방정식에서 선형으로 줄여 놀라운 효율성과 확장성으로 다변량 시계열 예측에서 최첨단 성능을 달성합니다.\n\n'와우, 시계열은 중요하지만 어렵다! 그리고...' 라고 말하는 부분을 건너 뜁니다. 이는 독자가 시계열 예측의 섬세함을 알고 핵심을 쉽게 이해하길 원한다고 가정한 것이죠!\n\n이 논문은 무엇에 중점을 두나요?\n\n# 기여\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 논문의 주요 기여는 두 가지이며, \"SOFT\"와 \"STAD\"가 있습니다.\n\n## SOFT\n\nSOFT: Series-cOre Fused Time Series forecaster\n\n이는 다변량 시계열 예측을 위해 설계된 것으로, 채널 독립성과 상관관계를 균형있게 유지하기 위해 STAD 모듈을 사용합니다. 이는 채널 상호작용을 전역 중심 표현으로 집중시킴으로써 선형 복잡성으로 우수한 성능을 달성하는 데 도움이 되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## STAD\n\nSTAD: STar Aggregate Dispatch 모듈\n\nSTAD는 SOFT의 기초가 되는 곳입니다. 여기서 SOFT는 간단한 MLP 기반 모델입니다. STAD는 다변량 시계열의 채널 간 종속성을 포착하는 중앙 집중식 구조입니다. 결과는 이 방법이 효과적이고 확장 가능하다는 것을 보여줍니다.\n\n![이미지](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Reversible Instance Normalization\n\n연구자들은 이 방법을 사용했습니다. 이는 ITRANSFORMER 논문에서 채택되어 정규화를 하이퍼파라미터로 고려했기 때문입니다.\n\niTransformer의 Reversible Instance Normalization은 단순히 역 치수에 대해 주의와 피드포워드 네트워크를 적용하여 다변량 상관 관계를 포착하고 비선형 표현을 효과적으로 학습할 수 있게 합니다.\n\n## Series Embedding\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시리즈 임베딩은 패치 임베딩보다 덜 복잡합니다. 이를 전체 시리즈의 길이로 패치 길이를 설정하는 것과 같다고 말할 수 있습니다. 연구자들은 각 채널의 시리즈를 임베딩하기 위해 선형 투사를 사용했습니다:\n\n![이미지](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_1.png)\n\n## STar Aggregate Dispatch (STAD) 모듈\n\n여러 STAD 모듈을 사용하여 시리즈 임베딩을 미세 조정합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image1](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_2.png)\n\n## Linear Predictor\n\nAfter N layer of STAD, there is a linear predictor for our task (forecasting), if S_N is the output representation of layer n, the prediction will be as follows:\n\n![image2](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_3.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 별 집합 디스패치 모듈\n\nSTar Aggregate Dispatch (STAD) 모듘은 다변량 시계열 예측에서 채널 간 종속성을 포착하기 위해 설계된 중앙 집중식 메커니즘입니다. 주의 메커니즘과 같은 전통적인 분산 구조와 달리 STAD는 각 채널 쌍의 특성을 직접 비교하여 이차 복잡성을 초래하는 것이 아닌, 중앙 집중식 전략을 사용하여 이러한 복잡성을 선형으로 줄입니다. 모든 시리즈에서 정보를 집계하여 전역 중심 표현으로 변환한 다음 이 핵심 정보를 다시 개별 시리즈 표현에 보내어 비정상 채널에 대한 개선된 강건성을 갖는 효율적인 채널 상호 작용이 가능하게 합니다.\n\n이 중앙 집중 구조는 소프트웨어 엔지니어링에서의 별 모양 시스템에서 영감을 받았습니다. 여기서 중앙 서버가 직접 피어 간 통신이 아닌 정보를 집계하고 교환합니다. 이 설계를 통해 STAD는 채널 독립성의 혜택을 유지하면서 예측 정확도를 향상시키기 위해 필요한 상관 관계를 포착합니다. 채널 통계를 단일 핵심 표현으로 집계함으로써 STAD는 비정상적인 시계열에서 신뢰할 수 없는 상관 관계에 의존하는 위험을 완화합니다.\n\n![이미지](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n적용된 결과에 따르면 STAD 모듈은 기존의 최첨단 방법보다 우수한 성능을 보여주며, 그럼에도 불구하고 상당히 낮은 계산 요구량으로 그 성과를 이룹니다. 이는 많은 다른 트랜스포머 기반 모델들에 대한 도전이었던 채널 수가 많거나 긴 lookback 창을 가진 데이터셋에 대해 확장 가능하게 만듭니다. 게다가 STAD 모듈의 일반성 덕분에 다양한 트랜스포머 기반 모델들에서 어텐션 메커니즘을 대체로 사용할 수 있으며, 그 효율성과 효과를 한층 더 입증하고 있습니다.\n\nSTAD의 입력은 각 채널에 대한 시리즈 표현이며, MLP를 통해 처리한 후 풀링합니다 (여기서는 확률적 풀링을 사용합니다):\n\n![image](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_5.png)\n\n이제 우리는 코어 표현(O)을 계산했으며, 코어와 모든 시리즈의 표현을 퓨전합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_6.png)\n\nThe Repeat_Concat concatenates the core representation O to each series representation to get Fi. Then we give this Fi to another MLP and add the output to the previous hidden dimension to calculate the next one.\n\n- Note that there’s also a residual connection from the input to the output.\n\n# Results\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n메소드가 간단해 보이지만, 복잡성이 크게 줄었어요 (이차 함수에서 선형 함수로) 그런데 대단하죠 😅😉\n\n![이미지1](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_7.png)\n\n연구자들은 다양한 데이터셋을 실험하고 대부분의 선배들과 비교해보았는데, 아래에서 확인할 수 있어요:\n\n![이미지2](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_8.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그들은 다른 실험도 수행했지만, 이 문서를 너무 길게 만들지 않기 위해 원래 연구 논문을 읽는 것을 추천합니다.\n\n면책사항: 이 문서를 작성하는 데 Nouswise를 사용했는데, 이는 문서를 통해 정보를 찾을 수 있는 검색 엔진과 같은 것입니다. 보통 일반적으로 이용할 수는 없지만, 직접 저에게 연락하여 접근 권한을 부여받을 수 있습니다. 연락처는 X(이전 트위터) 또는 우리 디스코드 서버에 있을 수 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_0.png"},"coverImage":"/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_0.png","tag":["Tech"],"readingTime":5},{"title":"강화 학습 딥 Q-네트워크","description":"","date":"2024-05-27 14:10","slug":"2024-05-27-ReinforcementLearningDeepQ-Networks","content":"\n## Python을 사용하여 달에 착륙하는 셔틀 가르치기: Deep Q-Networks를 활용한 강화 학습의 수학적 탐구\n\n![Reinforcement Learning](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_0.png)\n\n강화 학습(RL)에서 Q-학습은 에이전트가 환경을 탐색하면서 누적 보상을 극대화하기 위한 정책을 학습하는 데 도움이 되는 기본 알고리즘입니다. 이를 통해 특정 상태에서 특정 작업을 수행했을 때 기대되는 유틸리티를 추정하는 작업-값 함수를 업데이트 함으로써 보상을 받고 미래 추정에 기반합니다 (이게 익숙하지 않으신가요? 걱정 마세요. 나중에 함께 자세히 살펴볼 겁니다).\n\n그러나 전통적인 Q-학습에는 도전 과제가 있습니다. 상태 공간이 확장됨에 따라 확장 가능성에 어려움을 겪으며 연속적인 상태 및 작업 공간을 갖는 환경에서 효과적이지 않습니다. 이때 Deep Q Networks (DQNs)가 나타납니다. DQNs는 Q-값을 근사하기 위해 신경망을 사용하여 에이전트가 보다 크고 복잡한 환경을 효과적으로 처리할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n본 기사에서는 Deep Q Networks에 대해 자세히 살펴보겠습니다. DQNs가 기존의 Q-learning의 한계를 극복하는 방법과 DQN을 구성하는 주요 구성 요소에 대해 탐구할 것입니다. 또한 처음부터 DQN을 구현하고 더 복잡한 환경에 적용하는 과정을 살펴볼 것입니다. 이 기사를 마치면 DQN이 어떻게 작동하는지 이해하고 도전적인 강화 학습 문제를 해결하는 데 사용하는 방법을 알게 될 것입니다.\n\n## 목차\n\n1: 전통적인 Q-러닝\n∘ 1.1: 상태와 행동\n∘ 1.2: Q-값\n∘ 1.3: Q-테이블\n∘ 1.4: 학습 과정\n\n2: Q-러닝에서 Deep Q-네트워크로\n∘ 2.1: 전통적인 Q-러닝의 한계\n∘ 2.2: 신경망\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3: Deep Q-Network의 해부학\n\n- 3.1: DQN의 구성요소\n- 3.2: DQN 알고리즘\n\n4: 처음부터 Deep Q-Network 구현하기\n\n- 4.1: 환경 설정\n- 4.2: 딥 신경망 구축\n- 4.3: 경험 재생 구현\n- 4.4: 타깃 네트워크 구현\n- 4.5: Deep Q-Network 훈련\n- 4.6: 모델 튜닝\n- 4.7: 모델 실행\n\n5: 결론\n참고 문헌\n\n# 1: 전통적인 Q-Learning\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Reinforcement Learning Deep Q Networks](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_1.png)\n\nQ-러닝은 환경에서 누적 보상을 극대화하기 위한 최적 조치를 학습하는 에이전트를 안내합니다. 딥 Q-네트워크에 집중하기 전에, 그 선구자인 Q-러닝 뒤에 있는 메커니즘을 간단히 검토하는 것이 좋습니다.\n\n## 1.1: 상태 및 조치\n\n미로를 탐색하는 로봇이라고 상상해보세요. 미로에서 차지하는 각 위치를 \"상태\"라고 합니다. 왼쪽, 오른쪽, 위 또는 아래로 이동하는 것과 같은 각각의 움직임을 \"조치\"라고 합니다. 목표는 결국 미로를 통해 최적 경로를 찾으려면 각 상태에서 어떤 조치를 취할지 결정하는 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 1.2: Q-Values\n\nQ-Learning의 핵심은 Q-값으로, 𝑄(𝑠, 𝑎)로 표시됩니다. 이 값은 특정 상태 s에서 특정 행동 a를 취한 후 더 나은 경로(정책)를 따를 때 기대되는 미래 보상을 나타냅니다.\n\nQ-값을 가이드북의 항목으로 생각해보세요. 각 가능한 이동의 장기적 이점을 평가하는 것입니다. 예를 들어 미로의 특정 위치에 있다고 가정했을 때 왼쪽으로 이동하는 경우, Q-값은 미래 보상 측면에서 그 이동이 얼마나 유익할지 알려줍니다. 더 높은 Q-값은 더 나은 이동을 나타냅니다.\n\n## 1.3: The Q-Table\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nQ-Learning은 Q-값을 추적하는 데 Q-테이블을 사용합니다. Q-테이블은 기본적으로 각 행이 상태에 해당하고 각 열이 행동에 해당하며 각 셀이 해당 상태-행동 쌍의 Q-값을 포함하는 대형 스프레드시트입니다.\n\nQ-테이블을 거대한 스프레드시트로 상상해보세요. 각 셀은 미로의 특정 위치에서 특정 이동을 하였을 때 잠재적 미래 보상을 나타냅니다. 환경에 대해 더 많이 배우면이 보상의 더 나은 추정치로이 스프레드시트를 업데이트합니다.\n\n## 1.4: 학습 과정\n\nQ-러닝의 학습 과정은 반복적입니다. 초기 상태 s에서 시작합니다. 그런 다음 작업 a를 결정합니다. 이 선택은 다음을 기반으로 할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 탐험: 효과를 발견하기 위해 새로운 조치를 시도합니다.\n- 개척: 가장 높은 알려진 Q-값을 갖는 조치를 선택하기 위해 기존 지식을 사용합니다.\n\n선택한 조치를 수행하고 보상 r을 관찰하며 다음 상태 s'로 이동합니다. Q-러닝 공식을 사용하여 상태-조치 쌍 (s, a)의 Q-값을 업데이트합니다:\n\n[이미지](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_2.png)\n\n여기에:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- α는 학습 속도로, 새로운 정보가 이전 정보를 얼마나 덮어쓸지를 결정합니다.\n- γ는 할인 요소로, 즉각적 보상을 먼 미래의 보상보다 더 가치 있게 여깁니다.\n- maxa′Q(s′,a′)는 다음 상태 s′에서 가능한 모든 행동 a′에 대해 최고의 Q값을 나타냅니다.\n\n매번 안내서를 업데이트하고 있다고 상상해 보세요. 각 이동 후에 성공적인지 실패인지에 대한 피드백(보상)을 받습니다. 그런 다음 새로운 정보를 반영하도록 가이드북의 등급(Q값)을 조정하여 미래의 결정을 더 잘 하게 됩니다.\n\nQ값이 수렴할 때까지 이 과정을 반복하면, 에이전트는 미로를 탐색하는 최적 정책을 학습한 것입니다. 시간이 흘러, 미로를 반복적으로 탐험하고 경험에 기반하여 가이드북을 업데이트함으로써 최상의 보상을 얻기 위한 최적의 움직임을 알려주는 포괄적인 전략을 개발하게 됩니다.\n\nQ-러닝에 대해 자세히 알아보려면 이 기사를 확인해 보세요: [링크](링크)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2: Q-Learning에서 Deep Q-Network로\n\n## 2.1: 전통적인 Q-Learning의 한계\n\nQ-Learning은 강화 학습에 대한 강력한 알고리즘이지만, 더 복잡한 환경에서 효과적으로 동작하는 데 제약 사항이 몇 가지 있습니다:\n\n확장성 문제: 전통적인 Q-Learning은 각 상태-행동 쌍이 Q-값에 매핑된 Q-테이블을 유지합니다. 상태 공간이 성장함에 따라, 특히 고차원 또는 연속적인 환경에서는 Q-테이블이 불필요하게 커져 메모리 비효율성과 학습 속도 저하를 초래합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이산 상태 및 행동 공간: Q-Learning은 상태와 행동이 이산적이고 유한한 환경에서 잘 동작합니다. 하지만 현실 세계의 많은 문제는 연속적인 상태와 행동 공간을 포함하고 있습니다. 이러한 전통적인 Q-Learning은 이러한 공간을 이산화하지 않고는 효율적으로 처리할 수 없으며, 이로 인해 정보 손실과 최적 정책의 하락을 초래할 수 있습니다.\n\n## 2.2: 신경망\n\n이제 신경망을 소개해 보겠습니다. 신경망은 딥 네트워크에서 중요한 역할을 하는데, 인간 두뇌의 구조와 기능을 모방하여 데이터로부터 복잡한 패턴을 학습할 수 있는 강력한 함수 근사기입니다. 신경망은 입력 데이터를 처리하고 가중치와 편향을 통해 변환하여 출력을 생성하는 연결된 노드(뉴런)의 계층으로 이루어져 있습니다.\n\n강화 학습의 맥락에서 신경망은 Q-함수를 근사화하는 데 사용될 수 있습니다. 이는 상태-행동 쌍을 Q-값에 매핑하는 데 도움이 되며, 특히 Q-테이블을 유지하는 것이 적절하지 않은 대규모나 연속적인 공간에서 상태와 행동 간에 일반화를 더 잘할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서, Deep Q-networks(DQNs)은 Q-Learning의 원리를 신경망의 함수 근사 능력과 결합시켜요. 그렇게 하면 전통적인 Q-learning의 주요 제약 사항을 다룰 수 있어요.\n\nDQNs은 Q-값을 테이블에 저장하는 대신 신경망을 사용하여 Q-함수를 근사해요. 이 네트워크는 상태를 입력으로 받아 가능한 모든 행동에 대한 Q-값을 출력해요. 환경에서의 경험으로 네트워크를 학습시켜 에이전트는 각 행동에 대한 예상 보상을 예측하도록합니다. 이를 통해 다양한 상태와 행동에 걸쳐 일반화할 수 있어요.\n\n체스를 배우는 것을 상상해보세요. 가능한 모든 체스판 구성과 각 동작에 대한 최상의 수를 외우는 대신(불가능한 일이죠), 전략과 원칙(예를 들어 보드 중앙을 제어하고 왕을 보호하는 것과 같은 것)을 배우게 됩니다. 비슷하게, DQN는 신경망을 통해 일반적인 패턴과 전략을 배우고 모든 가능한 상태를 외우지 않고도 정보를 바탕으로 결정할 수 있어요.\n\n신경망 사용은 DQN이 크거나 연속된 상태 공간을 다룰 수 있게 해요. 네트워크는 주요 특징을 잡아내는 상태 공간의 표현을 학습해 중요한 결정을 취할 수 있도록 해줍니다.\n\n큰 도시를 이동하려면 고려합시다. 모든 거리와 건물의 배치를 외우는 대신 표지판과 중요 도로를 인식해 길을 찾게 됩니다. DQN의 신경망도 비슷하게 작용하며, 에이전트가 복잡한 환경에서 이동하는 것을 돕는 상태 공간의 중요한 특징을 인식하도록 학습합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다양한 경험을 훈련함으로써 모델은 과거 경험에서 일반화하는 법을 배우게 됩니다. 즉, 에이전트는 배운 것을 새로운, 보지 못한 상태와 행동에 적용할 수 있어서 다양한 상황에서 더 적응력이 있고 효율적일 수 있습니다.\n\n# 3: 딥 Q-네트워크의 구성 요소\n\n## 3.1: DQN의 구성 요소\n\n딥 Q-네트워크 (DQN)가 어떻게 작동하는지 이해하려면 그 주요 구성 요소를 자세히 살펴보는 것이 중요합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3.1.1: 신경망\n\n![신경망](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_3.png)\n\nDQN의 핵심은 Q값을 위한 함수 근사기 역할을 하는 신경망입니다. 아키텍처는 일반적으로 다음과 같이 구성됩니다:\n\n입력 레이어: 에이전트의 \"눈\"으로 상상해보세요. 이 레이어는 환경으로부터 상태 표현을 받아들이는데, 마치 당신의 눈이 주변의 시각적 정보를 받아들이는 것과 유사합니다. 위의 이미지에서 왼쪽에 두 개의 노드가 있는 첫 번째 레이어입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nHidden Layers: 이러한 레이어들은 에이전트의 \"뇌\"로 생각할 수 있습니다. 눈을 통해 받은 정보를 다수의 사고 단계를 거쳐 처리하여 복잡한 특징과 패턴을 식별합니다. 마치 당신의 뇌가 세계를 처리하고 이해하는 방식과 비슷합니다. 위 이미지에서는 세 개의 노드가 있는 중간 레이어입니다.\n\nOutput Layer: 이는 에이전트의 \"의사 결정\" 부분과 같습니다. 입력 상태에 따라 모든 가능한 행동에 대한 Q 값(값함수)을 생성합니다. 당신이 보고 생각한 것에 기반하여 최선의 행동을 결정하는 방식과 유사합니다. 각 출력은 특정 행동을 취했을 때 기대되는 보상에 해당합니다. 위 이미지에서는 한 개의 노드를 가진 오른쪽의 마지막 레이어입니다.\n\n위 이미지는 간단한 피드포워드 신경망을 나타냅니다. 이는 신경망의 가장 기본적인 형태입니다. 이 구조는 기본적이지만 \"깊은\" 신경망은 아닙니다. 깊은 신경망으로 변환하기 위해서는 더 많은 은닉 레이어를 추가하여 신경망의 깊이를 증가시켜야 합니다. 또한, 다양한 아키텍처와 구성을 실험하여 더 발전된 모델을 개발할 수 있습니다. 각 레이어의 노드 수는 고정되지 않으며, 특정 훈련 데이터셋과 작업에 따라 다양합니다. 이러한 유연성을 통해 네트워크를 특정 목적에 더 잘 맞게 조정할 수 있습니다.\n\n신경망에 대해 더 알고 싶다면, 나는 아래의 글을 강력히 추천합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3.1.2: 경험 재생\n이제 목록의 다음 항목인 경험 재생으로 넘어가 봅시다. 이것은 DQNs에서 학습 과정을 안정화하고 향상시키는 기술입니다. 다음을 포함합니다:\n\n메모리 버퍼: 에이전트의 \"일기\"로 생각해보세요. 이것은 에이전트의 경험을 시간이 지남에 따라 저장합니다 (상태, 행동, 보상, 다음 상태, 완료), 마치 매일 당신이 무슨 일이 일어났는지 기록하는 것처럼.\n\n랜덤 샘플링: 훈련 중에 에이전트는 지난 경험을 배우기 위해 일기의 랜덤한 페이지를 넘깁니다. 이는 사건의 순서를 깨어주어 에이전트가 경험의 순서에 과적합되는 것을 방지하여 보다 견고하게 학습하도록 돕습니다.\n\n3.1.3: 타겟 네트워크\n마지막으로, 타겟 네트워크는 훈련을 위해 타겟 Q-값을 계산하는 데 사용되는 별도의 신경망입니다. 주 신경망과 동일한 구조를 가지고 있지만 주 신경망의 가중치가 정기적으로 업데이트되어 일치하도록 고정되어 있습니다. 에이전트를 위한 \"안정된 안내서\"로 생각해보세요. 주 신경망이 지속적으로 학습하고 업데이트되는 반면, 타겟 네트워크는 안정된 Q-값을 제공하여 훈련에 도움을 줍니다. 학습을 안정적이고 일관되게 유지하는 데 도움이 되는 신뢰할 수 있는, 주기적으로 업데이트되는 매뉴얼이 있는 것과 같습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 3.2: DQN 알고리즘\n\n이러한 구성 요소가 준비되면 DQN 알고리즘은 다음과 같은 몇 가지 중요한 단계로 개요를 제시할 수 있습니다:\n\n### 3.2.1: 순방향 전파\n\n먼저, 우리는 Q-values를 예측하는 데 중요한 순방향 전파로 시작합니다. 이러한 Q-values는 특정 상태에서 특정 행동을 취했을 때 기대되는 미래 보상을 저장합니다. 이 프로세스는 상태 입력부터 시작됩니다.\n\n#### 상태 입력\n\n에이전트는 환경에서 현재 상태 s를 관찰합니다. 이 상태는 에이전트의 현재 상황을 설명하는 특징 벡터로 표현됩니다. 상태를 에이전트 주변 세계의 스냅숏으로 생각해보세요. 눈이 주변을 둘러보는 것처럼 시각 장면을 촬영할 때와 유사합니다. 이 스냅숏에는 에이전트가 결정을 내리기 위해 필요한 모든 세부 정보가 포함되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nQ-Value Prediction\n이제 이 관측된 상태 s가 신경망으로 전달됩니다. 이 신경망은 여러 층을 통해 이 입력을 처리하고 Q-값 세트를 출력합니다. 각 Q-값은 가능한 작업 a에 해당하며, 매개 변수 θ는 네트워크의 가중치와 편향을 나타냅니다.\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_4.png)\n\n신경망을 에이전트 뇌의 복잡한 의사 결정 기계로 상상해보세요. 캡처(상태)를 받으면 이 정보를 여러 단계(층)를 통해 처리하여 다양한 작업에 대한 잠재적 결과(Q-값)를 찾습니다. 보이는 것을 바탕으로 취할 수 있는 다양한 작업을 고려해보는 것과 유사합니다.\n\n작업 선택\n그런 다음 에이전트는 가장 높은 Q-값을 가진 작업 a∗를 다음 움직임으로 선택하며, 이에 따라 탐욕적 작업 선택 정책을 따릅니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_5.png)\n\n이것은 모든 옵션을 심사한 후에 최선의 움직임을 결정하는 것과 유사합니다. 에이전트는 가장 높은 보상을 가져다줄 것으로 믿는 행동을 선택하며, 마치 당신이 보고 이해한 것을 기반으로 가장 유망한 길을 선택하는 것과 같습니다.\n\n3.2.2: 경험 재생\n다음으로, 우리는 학습 과정을 안정화하고 향상시키는 데 도움이 되는 경험 재생에 대해 이야기하겠습니다.\n\n경험 저장\n에이전트가 행동 a를 취하고 보상 r을 받은 후 새로운 상태 s′를 받으면, 이 경험을 (s, a, r, s′, done) 튜플로 저장하여 플레이백 버퍼에 저장합니다. 변수 done은 에피소드가 종료되었는지를 나타냅니다. 플레이백 버퍼를 에이전트가 경험을 기록하는 다이어리로 생각해보세요. 이는 당신이 하루 중 주목할 만한 사건을 메모하는 것과 유사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n샘플 미니배치\n훈련 중에는 경험의 미니배치가 임의로 선택되어 재생 버퍼에서 샘플링됩니다. 이 배치는 타겟 Q-값을 계산하고 손실을 최소화하여 네트워크를 업데이트하는 데 사용됩니다. 에이전트가 훈련할 때, 과거 경험을 학습하기 위해 일기장의 임의의 페이지를 넘겨보게 됩니다. 이 임의 샘플링은 사건의 순서를 깨고 다양한 학습 예제를 제공하며, 일기의 서로 다른 날짜를 검토하여 보다 넓은 시야를 얻는 것과 유사한 역할을 합니다.\n\n3.2.3: 역전파\n최종 단계는 역전파로, 이는 네트워크를 업데이트하여 예측을 개선합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n타겟 Q-값 계산\n미니 배치의 각 경험에 대해, 에이전트는 타겟 Q-값 y\\_를 계산합니다. 만약 다음 상태 s′가 종료 상태(즉, done이 true인 경우)라면, 타겟 Q-값은 간단히 보상 r입니다. 그렇지 않으면, 타겟 Q-값은 보상에 다음 상태 s′에서 타겟 네트워크 Qtarget에 의해 예측된 할인된 최대 Q-값을 더한 값입니다:\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_7.png)\n\n여기서 γ는 할인 계수(0 ≤ γ ≤ 1)입니다. 이 단계는 과거 경험에 기반해 미래를 계획하는 것과 같습니다. 경험이 여행(에피소드)을 끝낼 경우, 타겟은 받은 보상입니다. 계속된다면, 타겟에는 즉시와 미래 혜택을 모두 고려하여 행동을 계획하는 방식과 유사한 예상 미래 보상이 포함됩니다.\n\n손실 계산\n다음으로, 손실은 메인 네트워크에서 예측된 Q-값 Q(s_i, a_i; θ)과 타겟 Q-값 yi 사이의 평균 제곱 오차로 계산됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_8.png)\n\n손실을 계산하는 것은 예측이 실제 결과와 얼마나 차이가 나는지를 평가하는 것과 같습니다. 실제 결과와 비교하여 추측의 정확성을 확인하고 차이점을 주목하는 것과 같습니다.\n\n역전파 및 최적화\n마지막으로, 이 손실을 최소화하기 위해 역전파를 수행합니다. 계산된 손실은 네트워크를 통해 역전파되어 SGD(Stochastic Gradient Descent) 또는 Adam과 같은 최적화 알고리즘을 사용하여 가중치를 업데이트합니다. 이 프로세스는 손실을 최소화하기 위해 네트워크 매개변수 θ를 조정합니다:\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_9.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 α는 학습률을 나타내고, ∇θLoss는 네트워크 매개변수에 대한 손실의 그래디언트를 나타냅니다. 역전파는 실수로부터 배우는 것과 같습니다. 예측이 얼마나 잘못되었는지를 깨달았을 때 (손실), 전략(네트워크 가중치)을 조정하여 미래의 결정을 개선합니다. 피드백을 바탕으로 자신의 접근 방식을 미세 조정하여 다음에 더 나은 결과를 얻는 것과 비슷합니다.\n\n이 아키텍처를 사용하여 에이전트는 정책을 반복적으로 개선합니다. 시간이 지남에 따라 누적 보상을 극대화하는 조치를 취하는 것을 배웁니다. 신경망, 경험 재생 및 타겟 네트워크의 결합으로 DQN은 복잡한 고차원 환경에서 효과적으로 학습할 수 있습니다. 이 과정은 에이전트가 환경을 탐색하는 데 능숙해질 때까지 계속됩니다.\n\n# 4: 처음부터 Deep Q-Network 구현\n\n이 섹션에서는 처음부터 Deep Q-Network (DQN)의 구현을 안내합니다. 이 섹션의 끝에는 Python에서 DQN을 구축하고 훈련하는 방법을 명확히 이해하게 될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 OpenAI Gym의 Lunar Lander 환경을 사용할 것입니다. 이 환경에서의 목표는 달 착륙선을 조종하여 지정된 착륙 패드에 성공적으로 착륙하는 것입니다. 착륙선은 환경을 통해 비행할 때 추진기를 사용하여 움직임과 방향을 조절해야 합니다. 이 환경은 상업적으로 사용할 수 있습니다. 라이센스 및 사용 권한에 대한 자세한 내용은 OpenAI Gym GitHub 페이지에서 확인할 수 있습니다.\n\n오늘 다룰 모든 코드는 여기에서 찾을 수 있습니다:\n\n## 4.1: 환경 설정\n\n우리는 OpenAI Gym의 LunarLander 환경을 사용할 것이며, 이는 우리의 에이전트가 해결해야 할 어려운 문제를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport os\nimport pickle\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport random\nimport optuna\n```\n\n여기서 필요한 라이브러리들을 import 합니다. gym은 환경을 위해 사용되며, torch는 우리의 신경망을 구축하고 훈련하는 데 사용되며, collections, random, 및 optuna는 경험 재생과 하이퍼파라미터 최적화에 도움이 됩니다.\n\n```js\nenv = gym.make(\"LunarLander-v2\", (render_mode = \"rgb_array\"));\nstate_dim = env.observation_space.shape[0];\naction_dim = env.action_space.n;\n```\n\n우리는 LunarLander 환경을 초기화하고 상태 및 액션 공간의 차원을 가져옵니다. state_dim은 상태의 특징 수를 나타내고, action_dim은 가능한 액션 수를 나타냅니다.```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4.2: 딥 신경망 구축\n\n우리의 딥-NN에서는 DQN이라는 클래스를 생성할 것입니다. 이 클래스는 세 개의 완전 연결 계층을 가진 신경망을 정의합니다. 입력 계층은 상태 표현을 수신하며, 은닉 계층은 이 정보를 선형 변환과 ReLU 활성화 함수를 통해 처리하고, 출력 계층은 각 가능한 동작에 대한 Q-값을 생성합니다.\n\n먼저 코드를 확인한 다음 분석해 봅시다:\n\n```js\nclass DQN(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, action_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.2.1: 클래스 초기화\n\n```python\nclass DQN(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, action_dim)\n```\n\n우리는 DQN이라는 클래스를 정의했습니다. 이 클래스는 PyTorch의 모든 신경망 모듈에 사용되는 기본 클래스인 nn.Module을 상속받습니다. 이를 통해 우리는 PyTorch의 내장 함수와 기능을 활용할 수 있습니다.\n\n**init** 메서드는 객체의 속성을 초기화하는 특별한 메서드입니다. 우리의 경우에는 신경망의 레이어를 설정하게 됩니다. 완전 연결층 (Fully Connected Layers):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 세 개의 완전 연결 (선형) 레이어를 정의합니다:\n\n- self.fc1 = nn.Linear(state_dim, 128): 첫 번째 레이어는 입력 상태 차원 (상태의 피쳐 수)을 받아서 128개의 뉴런으로 매핑합니다.\n- self.fc2 = nn.Linear(128, 128): 두 번째 레이어는 첫 번째 레이어에서 나온 128개의 뉴런을 또 다른 128개의 뉴런으로 매핑합니다.\n- self.fc3 = nn.Linear(128, action_dim): 세 번째 레이어는 두 번째 레이어에서 나온 128개의 뉴런을 액션 차원 (가능한 액션 수)으로 매핑합니다.\n\n각 nn.Linear 레이어는 입력 데이터에 대해 선형 변환을 수행합니다:\n\n![이미지](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.2.2: Forward Method\n앞서 명시된 forward 메소드는 데이터가 네트워크를 통해 흐르는 방법을 정의합니다. 이 방법은 네트워크를 통해 데이터를 전달할 때 자동으로 호출됩니다.\n\n```python\ndef forward(self, x):\n    x = torch.relu(self.fc1(x))\n    x = torch.relu(self.fc2(x))\n    return self.fc3(x)\n```\n\n첫 번째 layer에서 입력 데이터 x는 첫 번째 fully connected layer (self.fc1)를 통해 전달됩니다. 그런 다음 출력은 ReLU (Rectified Linear Unit) 활성화 함수를 사용하여 변환됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nx = torch.relu(self.fc1(x));\n```\n\nReLU(Recified Linear Unit) 활성화 함수는 다음과 같이 정의됩니다:\n\n![ReLU activation function](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_11.png)\n\n모델에 비선형성을 도입하여 네트워크가 더 복잡한 기능을 학습할 수 있게합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 번째 레이어에서는 첫 번째 레이어의 출력이 두 번째 완전 연결 레이어 (self.fc2)를 통과하고 다시 ReLU 활성화 함수를 사용하여 변환됩니다:\n\n```js\nx = torch.relu(self.fc2(x));\n```\n\n마지막으로 출력 레이어에서는 두 번째 레이어의 출력이 활성화 함수 없이 세 번째 완전 연결 레이어 (self.fc3)를 통해 전달됩니다:\n\n```js\nreturn self.fc3(x);\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 레이어는 각 액션에 대한 최종 Q-값을 생성합니다. 각 값은 해당 상태에서 그 액션을 취했을 때의 예상 미래 보상을 나타냅니다.\n\n## 4.3: 경험 재생 구현\n\nReplayBuffer 클래스는 경험을 저장하고 샘플링하는 메커니즘을 제공하여 DQN에서 학습 과정을 안정화하고 개선하는 데 필수적입니다. 따라서 에이전트가 다양한 과거 경험 세트로부터 학습할 수 있도록 해주어 일반화하고 복잡한 환경에서 잘 수행할 수 있는 능력을 향상시킵니다.\n\n```js\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n\n    def push(self, state, action, reward, next_state, done):\n        self.buffer.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n        return state, action, reward, next_state, done\n\n    def __len__(self):\n        return len(self.buffer)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.3.1: 클래스 초기화\n\n```js\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n```\n\n**init** 메서드는 고정된 용량을 갖는 deque(덱, 이중 연결 리스트)를 초기화합니다. 덱은 양쪽 끝에서 효율적으로 항목을 추가하고 제거할 수 있는 자료 구조입니다. 빠른 양쪽 끝에서의 추가와 제거가 필요한 큐(queue)나 스택(stack)을 구현할 때 유용합니다.\n\nself.buffer = deque(maxlen=capacity)는 capacity만큼의 경험을 저장할 수 있는 deque를 생성합니다. 버퍼가 가득 차면 새로운 경험을 추가하면 가장 오래된 경험이 자동으로 제거됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.3.2: Push Method\n\n```python\ndef push(self, state, action, reward, next_state, done):\n    self.buffer.append((state, action, reward, next_state, done))\n```\n\n푸시 메서드는 버퍼에 새로운 경험을 추가합니다. 각 경험은 상태(state), 액션(action), 보상(reward), 다음 상태(next_state), 완료 여부(done)로 구성된 튜플입니다:\n\n- state: 현재 상태.\n- action: 에이전트가 취한 행동.\n- reward: 행동을 취한 후 받은 보상.\n- next_state: 행동을 취한 후 에이전트가 이동한 상태.\n- done: 에피소드가 종료되었는지를 나타내는 부울 값.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.3.3: 샘플 메서드\n\n```python\ndef sample(self, batch_size):\n    state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n    return state, action, reward, next_state, done\n```\n\n샘플 메서드는 버퍼에서 무작위로 일괄 경험을 검색합니다.\n\nrandom.sample(self.buffer, batch_size)는 버퍼에서 batch_size개의 경험을 무작위로 선택합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"zip(\\*random.sample(self.buffer, batch_size))\"은 경험 목록을 상태, 행동, 보상, 다음 상태 및 완료에 대한 별도의 튜플로 풀어낼 수 있습니다.\n\n이 메서드는 샘플된 경험들로 이러한 튜플을 반환합니다.\n\n4.3.4: Length Method\n\n```python\ndef __len__(self):\n    return len(self.buffer)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**len** 메서드는 버퍼에 저장된 현재 경험 수를 반환합니다.\n\n## 4.4: 타겟 네트워크 구현\n\n타겟 네트워크를 통해 안정적인 Q 값 세트를 제공하여 훈련을 위해 학습 프로세스를 안정화하고 복잡한 환경에서 에이전트의 성능을 향상시킵니다. 타겟 네트워크는 주 네트워크보다 덜 자주 업데이트되어 메인 네트워크의 가중치를 업데이트하는 데 사용되는 Q 값 추정치가 더 안정적임을 보장합니다.\n\nDQNTrainer라는 클래스 내에 타겟 네트워크를 구현할 것이며, 이 클래스는 DQN의 훈련 프로세스를 관리하고 주 및 타겟 네트워크, 옵티마이저 및 재생 버퍼를 포함합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nclass DQNTrainer:\n    def __init__(self, env, main_network, target_network, optimizer, replay_buffer, model_path='model/model.pth', gamma=0.99, batch_size=64, target_update_frequency=1000):\n        self.env = env\n        self.main_network = main_network\n        self.target_network = target_network\n        self.optimizer = optimizer\n        self.replay_buffer = replay_buffer\n        self.model_path = model_path\n        self.gamma = gamma\n        self.batch_size = batch_size\n        self.target_update_frequency = target_update_frequency\n        self.step_count = 0\n```\n\n- DQNTrainer 클래스 정의:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**init** 메서드는 학습에 필요한 다양한 구성 요소를 초기화합니다:\n\n- env: 에이전트가 작동하는 환경입니다.\n- main_network: 훈련 중인 주요 신경망입니다.\n- target_network: Q-값 추정을 안정화하는 데 사용되는 대상 신경망입니다.\n- optimizer: 주요 신경망의 가중치를 업데이트하는 데 사용되는 옵티마이저입니다.\n- replay_buffer: 경험을 저장하고 샘플링하는 버퍼입니다.\n- model_path: 훈련된 모델을 저장하고 로드하기 위한 경로입니다.\n- gamma: 미래 보상에 대한 할인 계수입니다.\n- batch_size: 각 훈련 단계에서 재생 버퍼에서 샘플링된 경험의 수입니다.\n- target_update_frequency: 대상 네트워크의 가중치를 주요 네트워크의 가중치에 맞게 업데이트하는 빈도입니다.\n- step_count: 훈련 중에 취한 단계 수를 추적하는 카운터입니다.\n\n  4.4.2: 모델 로딩\n\n```js\n# 모델이 있으면 로드\n        if os.path.exists(os.path.dirname(self.model_path)):\n            if os.path.isfile(self.model_path):\n                self.main_network.load_state_dict(torch.load(self.model_path))\n                self.target_network.load_state_dict(torch.load(self.model_path))\n                print(\"디스크에서 모델 로드됨\")\n        else:\n            os.makedirs(os.path.dirname(self.model_path))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 모델 경로의 디렉토리가 존재하는지 os.path.exists(os.path.dirname(self.model_path))를 사용하여 확인합니다. 저장된 모델이 있으면, 훈련을 멈춘 지점부터 계속하기 위해 불러옵니다:\n\n```js\nif os.path.isfile(self.model_path):\n    self.main_network.load_state_dict(torch.load(self.model_path))\n    self.target_network.load_state_dict(torch.load(self.model_path))\n    print(\"디스크에서 모델을 불러왔습니다\")\n```\n\ntorch.load는 load_state_dict를 사용하여 저장된 모델 가중치를 메인 및 타겟 네트워크에 불러옵니다. 모델 디렉토리가 존재하지 않는 경우, os.makedirs를 사용하여 만듭니다.\n\n## 4.5: 딥 Q-네트워크 훈련\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로, 우리는 DQN을 훈련하기 위한 학습 루프를 구현할 것입니다. 이 방법은 DQNTrainer 내에 이루어집니다. DQN을 위한 훈련 루프를 실행하며, 에이전트가 환경과 상호 작용하고 경험을 수집하며 네트워크를 업데이트하고 성능을 추적합니다.\n\n다음은 학습 루프의 코드입니다:\n\n```js\ndef train(self, num_episodes, save=True):\n    total_rewards = []\n    for episode in range(num_episodes):\n        state, _ = self.env.reset()\n        done = False\n        total_reward = 0\n\n        while not done:\n            self.env.render()  # 환경을 렌더링하기 위해 이 줄을 추가합니다\n            action = self.main_network(torch.FloatTensor(state).unsqueeze(0)).argmax(dim=1).item()\n            next_state, reward, done, _, _ = self.env.step(action)\n            self.replay_buffer.push(state, action, reward, next_state, done)\n            state = next_state\n            total_reward += reward\n\n            if len(self.replay_buffer) \u003e= self.batch_size:\n                self.update_network()\n\n        total_rewards.append(total_reward)\n        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n\n    if save:\n        torch.save(self.main_network.state_dict(), self.model_path)\n        print(\"모델을 디스크에 저장했습니다\")\n\n    self.env.close()\n    return sum(total_rewards) / len(total_rewards)\n```\n\ntrain 메서드는 지정된 에피소드 수에 대해 훈련 루프를 실행합니다. 이 루프는 에이전트가 경험을 쌓고 의사 결정 능력을 향상시키는 데 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.5.1: 훈련 루프\n우선 total_rewards를 빈 리스트로 초기화해 봅시다:\n\n```js\ntotal_rewards = [];\n```\n\n이제 훈련 루프를 만들어 봅시다:\n\n```js\nfor episode in range(num_episodes):\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 루프는 지정된 에피소드 수만큼 실행됩니다. 각 에피소드는 환경과의 완전한 상호작용 순서를 나타냅니다.\n\n4.5.2: 환경 재설정\n각 에피소드의 시작 시점에는 환경이 초기 상태로 재설정됩니다.\n\n```js\nstate, (_ = self.env.reset());\ndone = False;\ntotal_reward = 0;\n```\n\n- self.env.reset()은 환경을 초기화하고 초기 상태를 반환합니다.\n- done = False는 에피소드가 완료되지 않았음을 나타냅니다.\n- total_reward = 0은 현재 에피소드의 총 보상을 초기화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n### 4.4.3: Action Selection\n\n에이전트는 현재 상태를 기반으로 메인 네트워크를 사용하여 작업을 선택합니다.\n\n```python\naction = self.main_network(torch.FloatTensor(state).unsqueeze(0)).argmax(dim=1).item()\n```\n\ntorch.FloatTensor(state).unsqueeze(0)은 상태를 PyTorch 텐서로 변환하고 네트워크가 예상하는 입력 형태와 일치하도록 추가 차원을 추가합니다.\n\nself.main_network(...).argmax(dim=1).item()는 메인 네트워크가 예측한 가장 높은 Q 값으로 작업을 선택합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.5.4: 단계 및 저장 환경\n에이전트가 선택한 동작을 수행하고 보상 및 다음 상태를 관찰한 후, 해당 경험을 재생 버퍼에 저장합니다.\n\n```js\nnext_state, reward, done, _, (_ = self.env.step(action));\nself.replay_buffer.push(state, action, reward, next_state, done);\nstate = next_state;\ntotal_reward += reward;\n```\n\n- self.env.step(action)은 동작을 실행하고 다음 상태, 보상 및 에피소드 완료 여부를 반환합니다.\n- self.replay_buffer.push(...)는 재생 버퍼에 경험을 저장합니다.\n- state = next_state는 현재 상태를 다음 상태로 업데이트합니다.\n- total_reward += reward은 현재 에피소드의 보상을 누적합니다.\n\n  4.5.5: 네트워크 업데이트\n  재생 버퍼에 충분한 경험이 있을 경우, 네트워크가 업데이트됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nif len(self.replay_buffer) \u003e= self.batch_size:\n    self.update_network()\n```\n\n`if len(self.replay_buffer) \u003e= self.batch_size`은 replay buffer가 적어도 batch_size의 경험을 가지고 있는지 확인합니다.\n\nself.update_network()은 replay buffer에서 일괄적인 경험을 사용하여 네트워크를 업데이트합니다.\n\n4.5.6: 에피소드 종료\n총 보상은 각 에피소드의 끝에서 기록되고 출력됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ntotal_rewards.append(total_reward)\nprint(f\"에피소드 {episode}, 총 보상: {total_reward}\")\n```\n\ntotal_rewards.append(total_reward)는 현재 에피소드의 총 보상을 총 보상 목록에 추가합니다.\n\nprint(f\"에피소드 {episode}, 총 보상: {total_reward}\")은 에피소드 번호와 총 보상을 출력합니다.\n\n4.5.7: 모델 저장\n훈련 후, 모델은 디스크에 저장됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsave가 True이면:\n   torch.save(self.main_network.state_dict(), self.model_path)\n   print(\"모델이 디스크에 저장되었습니다.\")\n```\n\nif save:는 save 플래그가 True인지 확인합니다.\n\ntorch.save(self.main_network.state_dict(), self.model_path)는 메인 네트워크의 상태 딕셔너리를 지정된 파일 경로에 저장합니다.\n\n4.5.8: 평균 보상 반환\n마지막으로, 이 메서드는 환경을 닫고 모든 에피소드에 대한 평균 보상을 반환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nself.env.close();\nreturn sum(total_rewards) / len(total_rewards);\n```\n\nself.env.close()는 환경을 닫습니다.\n\nreturn sum(total_rewards) / len(total_rewards)는 평균 보상을 계산하고 반환합니다.\n\n## 4.6: 모델 튜닝\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내 훈련된 모델을 평가하고 튜닝하는 방법을 살펴보겠습니다. DQN의 성능을 향상시키기 위해 하이퍼파라미터를 최적화할 책임을 가질 Optimizer 클래스를 만들어보겠습니다.\n\n```js\nclass Optimizer:\n    def __init__(self, env, main_network, target_network, replay_buffer, model_path, params_path='params.pkl'):\n        self.env = env\n        self.main_network = main_network\n        self.target_network = target_network\n        self.replay_buffer = replay_buffer\n        self.model_path = model_path\n        self.params_path = params_path\n\n    def objective(self, trial, n_episodes=10):\n        lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n        gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n        target_update_frequency = trial.suggest_categorical('target_update_frequency', [500, 1000, 2000])\n\n        optimizer = optim.Adam(self.main_network.parameters(), lr=lr)\n        trainer = DQNTrainer(self.env, self.main_network, self.target_network, optimizer, self.replay_buffer, self.model_path, gamma=gamma, batch_size=batch_size, target_update_frequency=target_update_frequency)\n        reward = trainer.train(n_episodes, save=False)\n        return reward\n\n    def optimize(self, n_trials=100, save_params=True):\n        if not TRAIN and os.path.isfile(self.params_path):\n            with open(self.params_path, 'rb') as f:\n                best_params = pickle.load(f)\n            print(\"디스크에서 매개변수를 불러왔습니다\")\n        elif not FINETUNE:\n            best_params = {\n                'lr': LEARNING_RATE,\n                'gamma': GAMMA,\n                'batch_size': BATCH_SIZE,\n                'target_update_frequency': TARGET_UPDATE_FREQUENCY\n                }\n            print(f\"기본 매개변수 사용 중: {best_params}\")\n        else:\n            print(\"하이퍼파라미터 최적화 중\")\n            study = optuna.create_study(direction='maximize')\n            study.optimize(self.objective, n_trials=n_trials)\n            best_params = study.best_params\n\n            if save_params:\n                with open(self.params_path, 'wb') as f:\n                    pickle.dump(best_params, f)\n                print(\"매개변수를 디스크에 저장했습니다\")\n\n        return best_params\n```\n\n4.6.1: 클래스 정의\n\n```js\nclass Optimizer:\n    def __init__(self, env, main_network, target_network, replay_buffer, model_path, params_path='params.pkl'):\n        self.env = env\n        self.main_network = main_network\n        self.target_network = target_network\n        self.replay_buffer = replay_buffer\n        self.model_path = model_path\n        self.params_path = params_path\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**init** 메서드는 최적화에 필요한 다양한 구성 요소를 초기화합니다:\n\n- env: 에이전트가 작동하는 환경.\n- main_network: 주요 신경망.\n- target_network: 타겟 신경망.\n- replay_buffer: 경험을 저장하고 샘플링하는 버퍼.\n- model_path: 훈련된 모델을 저장하거나 불러오는 경로.\n- params_path: 최적의 하이퍼파라미터를 저장하거나 불러오는 경로.\n\n  4.6.2: 목적 메서드\n\n```js\ndef objective(self, trial, n_episodes=10):\n        lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n        gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n        target_update_frequency = trial.suggest_categorical('target_update_frequency', [500, 1000, 2000])\n\n        optimizer = optim.Adam(self.main_network.parameters(), lr=lr)\n        trainer = DQNTrainer(self.env, self.main_network, self.target_network, optimizer, self.replay_buffer, self.model_path, gamma=gamma, batch_size=batch_size, target_update_frequency=target_update_frequency)\n        reward = trainer.train(n_episodes, save=False)\n        return reward\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n목표 함수는 하이퍼파라미터에 대한 값을 제안하고 이러한 값을 사용하여 모델을 훈련합니다.\n\n- lr = trial.suggest_loguniform(`lr`, 1e-5, 1e-1): 범위 [1e-5, 1e-1] 내의 학습률을 제안합니다.\n- gamma = trial.suggest_uniform(`gamma`, 0.9, 0.999): 범위 [0.9, 0.999] 내의 할인 요인을 제안합니다.\n- batch_size = trial.suggest_categorical(`batch_size`, [32, 64, 128]): 지정된 목록에서 배치 크기를 제안합니다.\n- target_update_frequency = trial.suggest_categorical(`target_update_frequency`, [500, 1000, 2000]): 지정된 목록에서 대상 업데이트 빈도를 제안합니다.\n\n```js\noptimizer = optim.Adam(self.main_network.parameters(), (lr = lr));\n```\n\n여기서는 주어진 학습률로 Adam 옵티마이저를 설정합니다. Adam은 주로 신경망 훈련에 사용되는 최적화 알고리즘인 Adaptive Moment Estimation의 약자입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n신경망에서 각 매개변수에 대해 Adam은 손실 함수의 기울기를 계산합니다. 그것은 기울기의 지수 이동 평균 (첫 번째 모멘트로 표시되는 m)과 제곱 기울기 (두 번째 모멘트로 표시되는 v)를 추적합니다.\n\n이동 평균의 초기화 편향을 고려하기 위해 Adam은 첫 번째 및 두 번째 모멘트 추정치에 바이어스 보정을 적용합니다. 그런 다음 매개변수는 수정된 첫 번째 및 두 번째 모멘트를 사용하여 업데이트됩니다. 업데이트 규칙은 학습률과 모멘트를 통합하여 기울기의 크기와 방향을 모두 고려하는 방식으로 매개변수를 조정합니다.\n\n다음은 Adam에 대한 보다 포괄적인 기사입니다:\n\n```js\ntrainer = DQNTrainer(\n  self.env,\n  self.main_network,\n  self.target_network,\n  optimizer,\n  self.replay_buffer,\n  self.model_path,\n  (gamma = gamma),\n  (batch_size = batch_size),\n  (target_update_frequency = target_update_frequency)\n);\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 코드는 제안된 하이퍼파라미터로 theDQNTrainer 인스턴스를 초기화합니다.\n\n```js\nreward = trainer.train(n_episodes, (save = False));\n```\n\n마지막으로, 이 코드는 지정된 에피소드 수로 모델을 학습하고 평균 보상을 반환합니다.\n\n4.6.3: 최적화 메소드\n이 섹션에서는 모델의 성능을 극대화하는 조합을 효율적으로 찾을 수 있도록 하이퍼파라미터 공간을 체계적으로 탐색하는 파이썬 라이브러리인 Optuna를 사용하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\ndef optimize(self, n_trials=100, save_params=True):\n    if not TRAIN and os.path.isfile(self.params_path):\n        with open(self.params_path, 'rb') as f:\n            best_params = pickle.load(f)\n        print(\"디스크에서 매개변수를 불러왔습니다.\")\n    elif not FINETUNE:\n        best_params = {\n            'lr': LEARNING_RATE,\n            'gamma': GAMMA,\n            'batch_size': BATCH_SIZE,\n            'target_update_frequency': TARGET_UPDATE_FREQUENCY\n        }\n        print(f\"기본 매개변수를 사용합니다: {best_params}\")\n    else:\n        print(\"하이퍼파라미터를 최적화 중입니다.\")\n        study = optuna.create_study(direction='maximize')\n        study.optimize(self.objective, n_trials=n_trials)\n        best_params = study.best_params\n\n        if save_params:\n            with open(self.params_path, 'wb') as f:\n                pickle.dump(best_params, f)\n            print(\"매개변수를 디스크에 저장했습니다.\")\n\n    return best_params\n```\n\n`optimize` 메소드는 지정된 횟수의 시도에 대해 최적화 프로세스를 실행합니다.\n\n```python\nif not TRAIN and os.path.isfile(self.params_path):\n        with open(self.params_path, 'rb') as f:\n            best_params = pickle.load(f)\n        print(\"디스크에서 매개변수를 불러왔습니다.\")\n```\n\n학습이 필요하지 않은 경우 (TRAIN이 아닌 경우) 및 매개변수 파일이 존재하는 경우, 매개변수가 디스크에서 로드됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nelif not FINETUNE:\n    best_params = {\n        'lr': LEARNING_RATE,\n        'gamma': GAMMA,\n        'batch_size': BATCH_SIZE,\n        'target_update_frequency': TARGET_UPDATE_FREQUENCY\n    }\n    print(f\"기본 매개변수 사용 중: {best_params}\")\n```\n\n만약 파라미터 튜닝이 필요하지 않다면 (not FINETUNE), 기본 매개변수가 사용됩니다.\n\n```python\nelse:\n    print(\"하이퍼파라미터 최적화 중\")\n    study = optuna.create_study(direction='maximize')\n    study.optimize(self.objective, n_trials=n_trials)\n    best_params = study.best_params\n\n    if save_params:\n        with open(self.params_path, 'wb') as f:\n            pickle.dump(best_params, f)\n        print(\"매개변수를 디스크에 저장했습니다\")\n```\n\n하이퍼파라미터 최적화가 필요한 경우, Optuna를 사용하여 최적의 매개변수를 찾습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nstudy = optuna.create_study(direction='maximize')을 사용하면 목적 함수를 최대화하는 Optuna 스터디를 생성할 수 있어요.\n\nstudy.optimize(self.objective, n_trials=n_trials)은 지정된 횟수의 시행을 위한 최적화를 실행해요.\n\nsave_params를 True로 설정하면, 최적의 매개변수가 디스크에 저장돼요.\n\n다음은 Optuna를 깊이 들여다보는 포함한 다양한 세밀 조정 기법을 탐구한 멋진 기사에요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4.7: 모델 실행하기\n\n마지막으로, 모든 과정을 다시 한번 확인하고 코드를 실행해 봅시다!\n\n4.7.1: 훈련 및 파인튜닝 설정\n\n```js\nTRAIN = True\nFINETUNE = False\n\n# 다음 하이퍼파라미터를 설정하세요 (FINETUNE이 False인 경우)\nGAMMA = 0.99\nBATCH_SIZE = 64\nTARGET_UPDATE_FREQUENCY = 1000\nLEARNING_RATE = 1e-3\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTRAIN = True은 모델을 학습할지 여부를 나타냅니다. False로 설정하면 학습이 건너뛰어집니다.\n\nFINETUNE = False는 모델을 fine-tune할지 여부를 나타냅니다. True로 설정하면 기존 매개변수가 사용되고 fine-tune됩니다.\n\n만약 FINETUNE이 False인 경우, 다음 하이퍼파라미터를 설정합니다:\n\n- GAMMA = 0.99: 미래 보상에 대한 할인 계수입니다. 이는 즉시 보상에 비해 미래 보상이 얼마나 중요한지를 결정합니다.\n- BATCH_SIZE = 64: 각 학습 단계마다 재생 버퍼에서 샘플링된 경험의 수입니다.\n- TARGET_UPDATE_FREQUENCY = 1000: 타겟 네트워크의 가중치가 주요 네트워크의 가중치와 일치하도록 업데이트되는 빈도(스텝 단위).\n- LEARNING_RATE = 1e-3: 최적화기(optimizer)의 학습률로, 모델 가중치가 업데이트될 때 추정 오차에 따라 모델을 얼마나 변경할지를 제어합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.7.2: 네트워크 및 재생 버퍼 초기화\n\n```js\nmain_network = DQN(state_dim, action_dim);\ntarget_network = DQN(state_dim, action_dim);\ntarget_network.load_state_dict(main_network.state_dict());\ntarget_network.eval();\n\nreplay_buffer = ReplayBuffer(10000);\n```\n\nmain_network = DQN(state_dim, action_dim)은 지정된 상태 및 액션 차원으로 메인 네트워크를 초기화합니다.\n\ntarget_network = DQN(state_dim, action_dim)은 메인 네트워크와 동일한 구조로 대상 네트워크를 초기화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\ntarget_network.load_state_dict(main_network.state_dict()) 함수는 메인 네트워크의 가중치를 타겼 네트워크로 복사합니다.\n\ntarget_network.eval() 함수는 타겟 네트워크를 평가 모드로 설정합니다. 이는 추론 중에 드롭아웃과 배치 정규화와 같은 특정 레이어가 적절하게 동작하도록 합니다.\n\nreplay_buffer = ReplayBuffer(10000)은 10,000개의 경험을 저장할 수 있는 용량을 가진 리플레이 버퍼를 초기화합니다.\n\n4.7.3: 단계 카운트 설정\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nSTEP_COUNT = 0;\n```\n\nSTEP_COUNT = 0은 훈련 중 취한 단계 수를 추적하는 카운터를 초기화합니다.\n\n4.7.4: 옵티마이저 초기화 및 하이퍼파라미터 최적화\n\n```js\noptimizer = Optimizer(env, main_network, target_network, replay_buffer, f'{os.path.dirname(__file__)}/model/model.pth', f'{os.path.dirname(__file__)}/model/params.pkl')\nbest_params = optimizer.optimize(n_trials=2, save_params=True)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`optimizer = Optimizer(...)`은 환경, 네트워크, 리플레이 버퍼, 모델 경로 및 매개변수 경로로 Optimizer 클래스를 초기화합니다.\n\n`best_params = optimizer.optimize(n_trials=2, save_params=True)`는 최적의 하이퍼파라미터를 찾기 위해 최적화 프로세스를 실행합니다. 이 함수는 다음과 같은 기능을 수행합니다:\n\n- 지정된 횟수(n_trials=2)만큼 최적화를 실행합니다.\n- `save_params`가 True인 경우 최적의 하이퍼파라미터를 디스크에 저장합니다.\n\n  4.7.5: PyTorch Optimizer 및 DQN Trainer 생성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\noptimizer = optim.Adam(main_network.parameters(), lr=best_params['lr'])\ntrainer = DQNTrainer(env, main_network, target_network, optimizer, replay_buffer, f'{os.path.dirname(__file__)}/model/model.pth', gamma=best_params['gamma'], batch_size=best_params['batch_size'], target_update_frequency=best_params['target_update_frequency'])\ntrainer.train(1000)\n```\n\n`optimizer = optim.Adam(main_network.parameters(), lr=best_params['lr'])`은 최적의 하이퍼파라미터에서 학습률을 사용하여 Adam 옵티마이저를 생성합니다.\n\n`trainer = DQNTrainer(...)`는 환경, 네트워크, 옵티마이저, 리플레이 버퍼, 모델 경로 및 최적의 하이퍼파라미터로 DQNTrainer 클래스를 초기화합니다.\n\n`trainer.train(1000)`은 모델을 1000번의 에피소드 동안 훈련합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 요정의 훈련 초기 10 에피소드를 살펴보겠습니다:\n\n![agent training](https://miro.medium.com/v2/resize:fit:1200/1*ncnLXIRABedg4uKwVL0L5w.gif)\n\n여기서 모델은 서툴러서 무작위로 종종 비최적적인 결정을 내립니다. 요정이 환경을 탐험하고 기초를 배우기 때문에 이는 예상되는 현상입니다. 아직 보상을 극대화하기 위한 견고한 전략을 개발하지 못했습니다. 추가적인 훈련 에피소드를 거치면서 시간이 지남에 따라, 요정의 성능은 정책을 미세 조정하고 경험을 통해 배우면서 크게 향상되어야 합니다.\n\n이제 모델이 1000번 훈련된 후의 10개의 훈련 에피소드를 살펴봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](https://miro.medium.com/v2/resize:fit:1200/1*s4j6V4V-nLfkc18C2Z2-zA.gif)\n\n이것은 주목할 만한 개선입니다. 모델이 아직 NASA에 완성되지는 않았지만, 몇 가지 주요 향상 사항을 관찰할 수 있습니다:\n\n- 에이전트가 더 신중하고 전략적인 결정을 내립니다.\n- 환경을 더 효율적으로 탐색합니다.\n- 부적절한 조치의 빈도가 크게 감소했습니다.\n\n지속적인 훈련과 세밀한 조정을 통해, 에이전트의 성능은 더 개선될 것으로 예상되며, 최적의 행동에 더 가까워질 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 여러분의 차례입니다. 모델을 더 발전시켜 보세요. 하이퍼파라미터를 조정하거나 다른 모델 구조를 실험해 보세요. 창의성과 인내심으로 최선을 다하면 얼마든지 성과를 낼 수 있을 거에요. 곧 완벽하게 패치된 셔틀은 원활하게 착륙할 거예요!\n\n# 5: 결론\n\n딥 Q-네트워크를 구축, 훈련 및 평가하는 방법을 잘 이해하셨으니, 이제 다양한 환경에서 이 DQN을 테스트하고 다양한 도전에 적응하는 모습을 관찰해 보세요.\n\n에이전트의 성능을 향상시키기 위해 고급 기술을 구현하고 새로운 아키텍처를 탐험해 보세요. 예를 들어 다양한 하이퍼파라미터를 설정해보거나 다른 최적화 알고리즘(예: SGD 또는 Nadam)을 사용하거나 다른 미세조정 알고리즘을 사용해 볼 수 있어요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n- Sutton, R. S., \u0026 Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT Press.\n- Lin, L. J. (1992). “Self-improving reactive agents based on reinforcement learning, planning and teaching.” Machine Learning, 8(3–4), 293–321.\n- OpenAI. “LunarLander-v2.” OpenAI Gym. [링크](https://gym.openai.com/envs/LunarLander-v2/)\n- 버클리 AI 연구소 (BAIR). “Experience Replay.” [링크](https://bair.berkeley.edu/blog/2020/03/20/experiencereplay/)\n- Towards Data Science. “Reinforcement Learning 101: Q-Learning.” [링크](https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292)\n- Towards Data Science. “신경망 뒤의 수학.” [링크](https://towardsdatascience.com/the-math-behind-neural-networks-3a18b7f8d8dc)\n- Towards Data Science. “Adam Optimizer 뒤의 수학.” [링크](https://towardsdatascience.com/the-math-behind-adam-optimizer-3a18b7f8d8dc)\n- Towards Data Science. “Deep Neural Networks 맞춤화 뒤의 수학.” [링크](https://towardsdatascience.com/the-math-behind-fine-tuning-deep-neural-networks-3a18b7f8d8dc)\n\n마지막까지 읽어 주셔서 축하드립니다! 이 기사가 유익하고 즐거우셨기를 바랍니다. 만약 그렇다면, 박수를 남기고 더 많은 이런 기사를 보고 싶다면 저를 팔로우해 주세요. 앞으로 다루었으면 하는 주제나 기사에 대한 의견을 들을 수 있습니다. 피드백과 지원에 감사드립니다. 읽어 주셔서 감사합니다!\n","ogImage":{"url":"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_0.png"},"coverImage":"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_0.png","tag":["Tech"],"readingTime":35},{"title":"딥 러닝을 이용한 우울증 예측 NLP 기초 배우기","description":"","date":"2024-05-27 14:08","slug":"2024-05-27-DepressionpredictionusingDeepLearninglearnbasicsofNLP","content":"\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e\u003cimg src=\"/assets/img/2024-05-27-DepressionpredictionusingDeepLearninglearnbasicsofNLP_0.png\" /\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n# 이 논문에서는 우리가 어떻게 NLP 애플리케이션을 쉽게 만들 수 있는지 보여드리고 싶습니다.\n\n이 논문에서는 NLP의 기본을 배우게 될 것입니다.\n다음은 여러분이 배우게 될 내용입니다:\n\n1 — Tokenizer가 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2 — texts_to_sequences란 무엇인가요?\n\n3 — pad sequence란 무엇인가요?\n\n4 — Embedding이란 무엇인가요?\n\n5 — 예측 모델을 만들어 볼까요?\n\n이 코드는 텍스트 데이터를 포함하는 이진 NLP 데이터셋에 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 1 — 토크나이저란:\n\n기계는 숫자만 이해할 수 있다는 것을 우리 모두 알고 있습니다. 그래서 우리는 단어를 숫자로 변환해야 합니다.\n\n예를 들어, 만약 'hello world'를 기계가 이해할 수 있게 하려면 이렇게 숫자로 변환해야 합니다:\nhello는 0으로 표현\nworld는 1로 표현\n\n이를 수행하기 위해 토크나이저를 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n토크나이저를 사용하여 단어, 하위 단어, 문자를 숫자로 변환하고 각 단어를 숫자로 변환한 것을 토큰이라고 합니다.\n\n요약하자면, 토크나이저는 텍스트를 토큰으로 변환합니다.\n\n## 데이터셋\n\n우선 데이터셋을 가져와야 합니다. 이 논문에서 사용된 데이터셋은 아래 링크를 통해 찾을 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 프로그래밍 해봐요 :))\n\n데이터셋을 위한 변수를 정의해볼게요 :\n\n```js\ndataset = pd.read_csv(\"D:ITML projectPredict depressiondepression_dataset_reddit_cleaned.csv\");\n```\n\n이제 문장과 레이블을 위한 변수 두 개를 정의해야 해요 :\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsentences = dataset[\"clean_text\"];\nlabels = dataset[\"is_depression\"];\n```\n\n모델을 훈련하기 위해 훈련 데이터와 모델을 시험하고 최적화하는 테스트 데이터가 필요합니다.\n\n따라서 이제 데이터를 두 부분, 훈련 및 테스트용으로 분리해야 합니다.\n\n데이터는 7731개의 행(샘플)을 포함하고 있으며, 0부터 6000까지는 훈련 데이터로 정의하며, 즉 6000 이전의 모든 데이터는 훈련에, 그 이후의 모든 데이터는 테스트에 사용합니다:```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ntraining_size = 6000\n\ntraining_sentences = sentences[0:training_size]\ntesting_sentences = sentences[training_size:]\n\ntraining_labels = labels[0:training_size]\ntesting_labels = labels[training_size:]\n```\n\nTokenizer 작업을 해봅시다.\n\n```js\n'''\n여기서는 텐서플로우 토크나이저를 사용합니다.\n'''\n\nfrom keras.preprocessing.text import Tokenizer #Tokenizer 가져오기\n\nvocab_size = 10000 #토크나이저가 기대하는 단어의 개수\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token='\u003cOOV\u003e', lower=True)\ntokenizer.fit_on_texts(training_sentences) #단어를 숫자로 변환하기\n#word_index = tokenizer.word_index #각 단어의 숫자(토큰)를 표시\n# print(word_index)\n```\n\noov_token='`OOV`': 이 매개변수는 어휘에 없는 단어를 처리하는 데 도움을 줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nlower=True : 모든 단어를 소문자로 변환합니다.\n\n# 2 — texts_to_sequences\n\n이 방법을 사용하면 단어를 나타내는 모든 숫자가 순서로 변환됩니다.\n\n예를 살펴보겠습니다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsentence1 = '개는 좋은 동물이다'\nsentence2 = '내 이름은 오미드야'\n\ntokenizer = Tokenizer(num_words=10, oov_token='\u003cOOV\u003e', lower=True)\ntokenizer.fit_on_texts([sentence1, sentence2])\nword_index = tokenizer.word_index\nprint(word_index)\n\nsequences = tokenizer.texts_to_sequences([sentence1, sentence2])\nprint(sequences)\n\n'''\nOutput:\n{'\u003cOOV\u003e': 1, '은': 2, '개는': 3, '좋은': 4, '동물이다': 5, '내': 6, '이름은': 7, '오미드야': 8}\n[[3, 2, 4, 5], [6, 7, 2, 8]]\n'''\n```\n\n# 3 — 시퀀스 패딩\n\n모든 문장이 같은 길이를 가지고 있지 않기 때문에, 이를 처리하기 위해 시퀀스 패딩을 사용합니다.\n\n예를 들어 2개의 문장이 있는데, 하나는 3단어이고 다른 하나는 4단어를 가지고 있다고 가정해보겠습니다. 이런 상황에서, 패딩 시퀀스는 2x4 행렬을 만들어줍니다. 3단어를 가진 문장은 맨 끝이나 맨 처음 행렬 요소를 0으로 처리할 것입니다.\n\n예제로 살펴보겠습니다:```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom keras.preprocessing.sequence import pad_sequences\n\nsequences = tokenizer.texts_to_sequences([sentence1, sentence2]) #지난 코드와 비슷함\n\nsentences_padded = pad_sequences(sequences)\nprint(sentences_padded)\n\n'''\noutput:\n[[3 2 4 5 6]\n [0 7 8 2 9]]\n'''\n```\n\n더 많은 정보를 원하시면 문서를 읽어보세요.\n\n이제 우리의 주요 코드(우울증 예측)로 돌아가보겠습니다.\n\n이제 texts_to_sequences와 Pad sequences가 무엇인지 알았으니 이를 통해 데이터를 처리해봅시다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom keras.preprocessing.sequence import pad_sequences\n\nmax_length = 100 # tokenizer가 허용할 문장의 최대 길이\n\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length)\n```\n\n이제 데이터가 준비되었으니 모델을 만들 수 있지만, 그 전에 Embedding이 무엇인지 알아보겠습니다.\n\n# 4 — Embedding\n\nEmbedding은 단어를 벡터로 변환합니다. 이를 통해 모델은 단어 간의 관계를 이해할 수 있습니다.\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, '좋은'과 '나쁜'이라는 단어가 있다고 상상해보세요. 그런데 '나쁘지 않은'처럼 특정한 단어가 있는 경우에는 이 단어가 부정적인 느낌을 나타내는 '나쁜'과 연관되어 있음을 모델이 이해하도록 임베딩이 도움이 됩니다.\n\n# 5 — 예측 모델 만들기\n\n모델은 임베딩 레이어로 훈련되었으며, 그 후에는 글로벌 평균 풀링 1D, 24개의 밀집 (완전 연결) 레이어(relu 활성화 함수 사용) 및 마지막 레이어에 시그모이드 활성화 함수를 사용한 1개의 밀집 레이어로 구성되어 있습니다. 이를 10번의 epoch 동안 훈련시켰습니다.\n\n활성화 함수는 모델이 데이터를 더 잘 이해하도록 돕습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## ReLU 활성화 함수\n\nReLU는 값이 0보다 큰 경우에만 활성화되는 활성화 함수입니다:\n\nR(x) = max(0,x)\n\n## 시그모이드 활성화 함수\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\n![Image](/assets/img/2024-05-27-DepressionpredictionusingDeepLearninglearnbasicsofNLP_1.png)\n\nIf the labels of the data are binary (0 or 1) like the dataset we are using, we use the sigmoid activation function.\n\nIf the output of the sigmoid activation function (the last layer) is greater than 0.5, it is assigned the label 1. If it is lower than 0.5, it is assigned the label 0.\n\nIn summary:\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\n출력 결과 `0.5 — — → 1\n\n출력 결과 `0.5 — — -` 0\n\n## 코드\n\n```js\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dense, GlobalAveragePooling1D\n\nembedding_dim = 16 #임베딩 레이어의 차원\nmodel = Sequential([\n    Embedding(vocab_size, output_dim=embedding_dim, input_length=max_length),\n    GlobalAveragePooling1D(),\n    Dense(24, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nnum_epochs = 10\nhistory = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels))\n```\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 플롯\n\n10 epochs에서 모델의 진행 상황을 확인해 봅시다.\n\n```python\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['accuracy', 'loss'])\n```\n\n![image](/assets/img/2024-05-27-DepressionpredictionusingDeepLearninglearnbasicsofNLP_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 epoch마다 모델이 개선되었음을 확인할 수 있습니다. 정확도가 증가하고 손실이 감소했어요.\n\n## 모델 테스트\n\n이제 모델을 테스트해볼게요. 입력 텍스트에 texts_to_sequence 및 pad_sequences를 수행할 필요가 있다는 것을 잊지 마세요.\n\n```js\ntest_sentence = ['the life became so hard i can not take it any more i just wanna die ']\ntest_sentence = tokenizer.texts_to_sequences(test_sentence)\npadded_test_sentence = pad_sequences(test_sentence, maxlen=max_length)\nprint(model.predict(padded_test_sentence))\n\n'''\noutput :\n[[0.6440944]]\n'''\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n입력 텍스트 (test_sentence)에는 분명히 슬픈 감정이 있습니다. 모델의 출력값은 0.64로, 0.5보다 큽니다. 이전에 언급했듯이, 레이블 1로 할당되어 우울증이 긍정적이라는 것을 의미합니다.\n\n# GitHub:\n\n아래 링크를 통해 GitHub에서 코드에 접근할 수 있습니다.\n\n# 마지막 요청\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n읽어 주셔서 감사합니다. 즐거워하셨으면 좋겠어요!\n","ogImage":{"url":"/assets/img/2024-05-27-DepressionpredictionusingDeepLearninglearnbasicsofNLP_0.png"},"coverImage":"/assets/img/2024-05-27-DepressionpredictionusingDeepLearninglearnbasicsofNLP_0.png","tag":["Tech"],"readingTime":7}],"page":"4","totalPageCount":11,"totalPageGroupCount":1,"lastPageGroup":11,"currentPageGroup":0},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"4"},"buildId":"O5JVeK6TDhbvSxBlchQwM","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>