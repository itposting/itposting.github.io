<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/30" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/30" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="집은 항상 장소가 아니다" href="/post/2024-06-22-homeisnotalwaysaplace"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="집은 항상 장소가 아니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-homeisnotalwaysaplace_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="집은 항상 장소가 아니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">집은 항상 장소가 아니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="고급 RAG 정보 검색 강화 생성 시스템 향상시키는 고급 기법 구현 방법" href="/post/2024-06-22-AdvancedRAGImplementingAdvancedTechniquestoEnhanceRetrieval-AugmentedGenerationSystems"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="고급 RAG 정보 검색 강화 생성 시스템 향상시키는 고급 기법 구현 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-AdvancedRAGImplementingAdvancedTechniquestoEnhanceRetrieval-AugmentedGenerationSystems_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="고급 RAG 정보 검색 강화 생성 시스템 향상시키는 고급 기법 구현 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">고급 RAG 정보 검색 강화 생성 시스템 향상시키는 고급 기법 구현 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">19<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ML 엔지니어를 위한 저장 공간 부족 문제 피하는 방법 가이드라인" href="/post/2024-06-22-HowtoAvoidtheOut-of-SpaceProblemAGuidelineforanMLEngineer"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ML 엔지니어를 위한 저장 공간 부족 문제 피하는 방법 가이드라인" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-HowtoAvoidtheOut-of-SpaceProblemAGuidelineforanMLEngineer_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ML 엔지니어를 위한 저장 공간 부족 문제 피하는 방법 가이드라인" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">ML 엔지니어를 위한 저장 공간 부족 문제 피하는 방법 가이드라인</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="깊게 배우는 딥러닝 일러스트, Part 5 LSTM Long Short-Term Memory 이해하기" href="/post/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="깊게 배우는 딥러닝 일러스트, Part 5 LSTM Long Short-Term Memory 이해하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="깊게 배우는 딥러닝 일러스트, Part 5 LSTM Long Short-Term Memory 이해하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">깊게 배우는 딥러닝 일러스트, Part 5 LSTM Long Short-Term Memory 이해하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="존재와 애니메이션 물질 세계를 움직이는 힘" href="/post/2024-06-22-BeingsandAnimatedMatter"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="존재와 애니메이션 물질 세계를 움직이는 힘" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-BeingsandAnimatedMatter_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="존재와 애니메이션 물질 세계를 움직이는 힘" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">존재와 애니메이션 물질 세계를 움직이는 힘</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="비판적 사고 AI를 뛰어넘는 보이지 않는 위협" href="/post/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="비판적 사고 AI를 뛰어넘는 보이지 않는 위협" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="비판적 사고 AI를 뛰어넘는 보이지 않는 위협" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">비판적 사고 AI를 뛰어넘는 보이지 않는 위협</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="강화 학습의 비밀을 풀다 Actor-Critic 초보자 가이드" href="/post/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="강화 학습의 비밀을 풀다 Actor-Critic 초보자 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="강화 학습의 비밀을 풀다 Actor-Critic 초보자 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">강화 학습의 비밀을 풀다 Actor-Critic 초보자 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="로봇은 더 똑똑해지고 있을까 로봇 공학에서 인식에 대한 대화" href="/post/2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="로봇은 더 똑똑해지고 있을까 로봇 공학에서 인식에 대한 대화" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="로봇은 더 똑똑해지고 있을까 로봇 공학에서 인식에 대한 대화" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">로봇은 더 똑똑해지고 있을까 로봇 공학에서 인식에 대한 대화</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="로봇 공학 기술 2024년에 주목해야 할 10가지 혁신" href="/post/2024-06-22-ROBOTICTECHNOLOGY"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="로봇 공학 기술 2024년에 주목해야 할 10가지 혁신" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-ROBOTICTECHNOLOGY_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="로봇 공학 기술 2024년에 주목해야 할 10가지 혁신" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">로봇 공학 기술 2024년에 주목해야 할 10가지 혁신</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="힌두교 의식을 수행하는 로봇  신도들이 로봇이 신도들을 대체할까 걱정하는 이유" href="/post/2024-06-22-RobotsareperformingHinduritualssomedevoteesfeartheyllreplaceworshippers"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="힌두교 의식을 수행하는 로봇  신도들이 로봇이 신도들을 대체할까 걱정하는 이유" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-RobotsareperformingHinduritualssomedevoteesfeartheyllreplaceworshippers_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="힌두교 의식을 수행하는 로봇  신도들이 로봇이 신도들을 대체할까 걱정하는 이유" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">힌두교 의식을 수행하는 로봇  신도들이 로봇이 신도들을 대체할까 걱정하는 이유</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/21">21</a><a class="link" href="/posts/22">22</a><a class="link" href="/posts/23">23</a><a class="link" href="/posts/24">24</a><a class="link" href="/posts/25">25</a><a class="link" href="/posts/26">26</a><a class="link" href="/posts/27">27</a><a class="link" href="/posts/28">28</a><a class="link" href="/posts/29">29</a><a class="link posts_-active__YVJEi" href="/posts/30">30</a><a class="link" href="/posts/31">31</a><a class="link" href="/posts/32">32</a><a class="link" href="/posts/33">33</a><a class="link" href="/posts/34">34</a><a class="link" href="/posts/35">35</a><a class="link" href="/posts/36">36</a><a class="link" href="/posts/37">37</a><a class="link" href="/posts/38">38</a><a class="link" href="/posts/39">39</a><a class="link" href="/posts/40">40</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"집은 항상 장소가 아니다","description":"","date":"2024-06-22 19:54","slug":"2024-06-22-homeisnotalwaysaplace","content":"\n\n\n\u003cimg src=\"/assets/img/2024-06-22-homeisnotalwaysaplace_0.png\" /\u003e\n\n\"아직 집에 가려고 하나요?\"\n\n\"그들이 나와 함께 있다면 이미 집이니까요.\"\n\n\"모든 것이 너무 무거워지면 어디로 도망가나요?\"\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내 친구들이 대화할 때 자주 듣는 질문이에요. 대답은 다양해요 - 몇 명은 확신을 못 가지고 있다고 하고, 다른 사람들은 자기 안에서 평화를 찾고, 몇 사람은 그냥 현재 있는 곳을 벗어나고 싶다고 해.\n\n그들을 조용히 지켜보고 있어. 즐거운 미소, 장난기 가득한 웃음소리, 그들 사이를 흐르는 깊은 대화, 말 끝이 없는 아이들처럼의 장난 같은 엄청난 배려를 못 보지 못할 수가 없어.\n\n항상 혼자 있기를 원하는 사람으로써, 나는 이 상황에서 버틸 수 있을 거라고 생각하지 못했어. 어떻게 된 건지도 모르겠어. 어느 날 일어나보니 그들이 나의 그리워하는 사람으로 변해 있었다는 걸 깨달았어. 그들의 얼굴과 목소리가 나의 생각을 지배해, 눈을 뜨는 순간부터 머리를 누이는 그 순간까지.\n\n우리의 우정은 예상치 못한 게 있어. 고등학교 시절에 그들과는 정말 가까운 사이가 아니었어. 사실, 나는 그들을 어떻게 발견했는지 자세한 내용조차 없어.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일부는 나의 동급생들이었지만, 우리는 특별한 대화를 나누지 않았어요. 우리의 소통은 복도, 학교 건물, 그리고 구내식당에서 일어나는 굉장히 피상적인 만남에 그쳤어요 — 서둘러 지나가는 인사, 예의 바른 인사 교환, 그 이상의 것은 없었죠.\n\n우리는 이바지를 하고 있었지만, 우리의 삶은 거의 교차하지 않고 나란히 진행되고 있었어요.\n\n하지만 사람들을 어느 때보다 소중히 생각하고 사랑할 수 있다는 것, 그것은 정말 믿기 힘들지만 놀랍게도 가능해요. 단지 한번 정도 만난 이런 사람들이 조금씩 진정한 집과 안식처로 변해갈 수도 있어요.\n\n나는 그들과 함께 하는 것을 끊임없이 갈망하게 되었어요, 마치 그들이 내 자신의 연장이 된 것처럼요. 하루종일 함께 지냈더라도, 전혀 충분하지 않은 것 같아요. 그들의 안정적인 존재에 대한 끝없는 열망이 있어요, 그들만이 그 빈자리를 채울 수 있는 것 같아요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그들과 함께 있을 때 내가 견디는 부담이 상당히 가벼워진다. 그들의 포옹은 내 문제들이 사라지는 방법으로, 그 순간에는 단순히 존재하지 않는 것처럼 내 문제들을 사라지게 만든다.\n\n시간이 흐르면서, 해가 떠오르고 질 때, 비가 쏟아지고 멈출 때, 하늘을 밝게 비추고 어두울 때,\n\n그들은 거기에 있었고 나에게 집이 되었다.\n\n집은 항상 물리적인 장소일 필요는 없다. 이것은 네모방이 있고 지붕, 기둥, 창문, 그리고 문이 있는 것이 아닙니다. 때로는 감정, 목소리, 손 두 개, 그리고 발을 갖고 있다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n나는 이야기 뒤에 있는 사람들을 가리키는 집이기 때문에.\n\n나는 안전하고 평화롭고 행복한 곳이 있는 집이에요.\n\n그래서 돌아오지 않았느냐고 물으면, 답은 간단해요 — 나는 이미 집에 있어요. 집은 목적지가 아니라, 내 여정의 일부가 된 사람들이기 때문이죠. 그들이 계속 나와 함께 걷는 한, 나는 정확히 내가 있어야 할 곳에 있는 거예요.\n\n내가 위에서 언급한 사람들이 함께 있다면, 난 이미 집에 도착한 거랍니다.","ogImage":{"url":"/assets/img/2024-06-22-homeisnotalwaysaplace_0.png"},"coverImage":"/assets/img/2024-06-22-homeisnotalwaysaplace_0.png","tag":["Tech"],"readingTime":2},{"title":"고급 RAG 정보 검색 강화 생성 시스템 향상시키는 고급 기법 구현 방법","description":"","date":"2024-06-22 19:52","slug":"2024-06-22-AdvancedRAGImplementingAdvancedTechniquestoEnhanceRetrieval-AugmentedGenerationSystems","content":"\n\n이제 RAG (Retrieval Augmented Generation) 기술이 LLMs와 상호 작용 중에 사실로 잘 알려져 있는 것 같아요. 최근에 제가 쓴 하나의 기사에서는 코드 예제와 함께 단계별로 RAG 파이프라인을 구축했습니다. 이 기사에서 우리는 이 이니셔티브를 한 단계 더 나아가서 고급 RAG 파이프라인을 구현해볼 거에요.\n\n# 1. 고급 정보 검색 기반 생성 (RAG) 파이프라인: 개요\n\n기본 RAG 작업 흐름은 색인화, 검색 및 생성 세 단계로 나뉠 수 있어요. 색인화 단계에서 텍스트는 임베딩으로 변환되고, 이후에는 이를 검색 가능한 인덱스를 만들기 위해 벡터 데이터베이스에 저장됩니다. 검색 단계에서 사용자의 질의도 임베딩으로 변환됩니다. 이 임베딩은 가장 관련 있는 텍스트 데이터를 검색하기 위해 벡터 데이터베이스를 탐색하는 데 사용됩니다. 마지막으로 생성 단계에서 질의는 이전에 검색된 관련 문서들로 확장되고, 이 향상된 프롬프트를 사용하여 대규모 언어 모델이 사용자 질문에 대한 답변을 생성합니다.\n\n고급 RAG 파이프라인은 새로운 단계(하위 단계)가 추가된 이 파이프라인의 향상된 버전입니다. 이 기사에서 논의될 개선 사항 목록은 다음과 같지만, 전반적인 목록은 이에 한정되지 않을 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터 색인 최적화: 슬라이딩 윈도우를 사용하여 텍스트 청킹 및 효율적인 메타데이터 활용과 같은 기술을 사용하여 더 검색 가능하고 조직화된 색인을 생성합니다.\n- 쿼리 향상: 동의어 또는 보다 넓은 범위의 용어를 사용하여 초기 사용자 쿼리를 수정하거나 확장하여 관련 문서의 검색을 개선합니다.\n- 하이브리드 검색: 전통적인 키워드 기반 검색과 임베딩 벡터를 사용한 의미론적 검색을 결합하여 다양한 쿼리 복잡성을 처리합니다.\n- 임베딩 모델 세밀 조정: 사전 훈련된 모델을 조정하여 특정 도메인 세부 사항을 더 잘 이해하고 검색된 문서의 정확도와 관련성을 향상시킵니다.\n- 응답 요약: 검색된 텍스트를 압축하여 최종 응답 생성 전에 간결하고 관련성 있는 요약을 제공합니다.\n- 다시 순위 매기기 및 필터링: 관련성에 기초하여 검색된 문서의 순서를 조정하고, 덜 관련성이 떨어지는 결과를 걸러내어 최종 출력물을 정제합니다.\n\n여기 나열한 것 외에도 가능한 개선점은 다양합니다. RAG 기술을 조사한 논문이 있어서 RAG 파이프라인의 품질을 향상시키는 데 사용할 수 있는 많은 개선 사항을 찾을 수 있습니다. 다음 다이어그램은 고급 RAG 파이프라인의 단계를 보여줍니다.\n\n![다이어그램](/assets/img/2024-06-22-AdvancedRAGImplementingAdvancedTechniquestoEnhanceRetrieval-AugmentedGenerationSystems_0.png)\n\n본 문서에서는 기본 RAG 파이프라인에 대한 이해를 전제로 하고, 개선 사항에만 초점을 맞출 것입니다. 이해가 되지 않는 경우, 이전 글인 \"RAG와 함께하는 실전: LLMs에 검색 보강 생성 통합하기 - 단계별 가이드\"를 읽어보시기를 권해 드립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2. Pre-Retrieval Optimizations for Advanced Retrieval-Augmented Generation (RAG)\n\n사전검색은 인덱싱이 어떻게 수행되며 사용자 쿼리가 검색에 사용되기 전에 어떤 작업이 수행되는지를 정의하는 단계입니다. 아래에서 사전검색 최적화를 위한 여러 전략을 논의하겠으며, 데이터 인덱싱 및 쿼리 개선을 포함한 샘플 Python 코드를 제공할 것입니다.\n\n## 2.1. 데이터 인덱싱 최적화\n\n다른 작업을 수행하기 전에 나중에 질의할 수 있는 데이터를 저장해야 합니다. 이를 인덱싱이라고 합니다. 이는 올바른 청크 크기 설정, 메타데이터 효과적인 사용 및 임베딩 모델 선택을 포함합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2.1.1. 텍스트 청킹을 위한 슬라이딩 윈도우\n\n텍스트를 색인화하는 간단한 방법은 텍스트를 n 부분으로 나누고, 그것들을 임베딩 벡터로 변환한 다음 벡터 데이터베이스에 저장하는 것입니다. 슬라이딩 윈도우 접근 방식은 겹치는 텍스트 청크를 생성하여 청크의 경계에서 어떤 문맥 정보도 손실되지 않도록 보장합니다. 다음 코드 샘플은 텍스트를 문장으로 분할하기 위해 nltk 라이브러리를 사용합니다.\n\n```python\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nnltk.download('punkt')  # punkt 토크나이저가 다운로드되었는지 확인\n\ndef sliding_window(text, window_size=3):\n    \"\"\"\n    슬라이딩 윈도우 접근 방식을 사용하여 텍스트 청킹 생성\n\n    Args:\n    text (str): 청크할 입력 텍스트\n    window_size (int): 청크 당 문장 수\n\n    Returns:\n    list of str: 텍스트 청크 목록\n    \"\"\"\n    sentences = sent_tokenize(text)\n    return [' '.join(sentences[i:i+window_size]) for i in range(len(sentences) - window_size + 1)]\n\n# 예제 사용법\ntext = \"This is the first sentence. Here comes the second sentence. And here is the third one. Finally, the fourth sentence.\"\nchunks = sliding_window(text, window_size=3)\nfor chunk in chunks:\n    print(chunk)\n    print(\"-----\")\n    # 여기서 청크를 임베딩 벡터로 변환하고\n    # 그것을 벡터 데이터베이스에 저장할 수 있습니다\n```\n\n2.1.2. 메타데이터 활용\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n메타데이터에는 문서 작성 날짜, 작성자 또는 관련 태그와 같은 정보가 포함될 수 있습니다. 이 정보는 검색 프로세스를 향상시켜 검색 중에 문서를 필터링하거나 우선 순위를 지정하는 데 사용될 수 있습니다.\n\n다음 코드 샘플은 faiss 라이브러리를 사용하여 벡터 데이터베이스를 생성하고 벡터를 삽입하고 메타데이터(태그)로 검색하는 방법을 보여줍니다.\n\n```js\nimport numpy as np\nimport faiss\n\ndocuments = [\n    \"문서 1 내용\",\n    \"두 번째 문서의 내용\",\n    \"세 번째 문서는 다른 내용을 가지고 있습니다\",\n]\nmetadata = [\n    {\"날짜\": \"20230101\", \"태그\": \"뉴스\"},\n    {\"날짜\": \"20230102\", \"태그\": \"업데이트\"},\n    {\"날짜\": \"20230103\", \"태그\": \"리포트\"},\n]\n\n# 더미 함수를 사용하여 임베딩 생성\ndef generate_embeddings(texts):\n    \"\"\"예시를 위한 더미 임베딩 생성.\"\"\"\n    return np.random.rand(len(texts), 128).astype('float32')  # 128차원 임베딩\n\n# 문서에 대한 임베딩 생성\ndoc_embeddings = generate_embeddings(documents)\n\n# 임베딩용 FAISS 인덱스 생성(단순성을 위해 FlatL2 사용)\nindex = faiss.IndexFlatL2(128)  # 128은 벡터의 차원\nindex.add(doc_embeddings)  # 인덱스에 임베딩 추가\n\n# 메타데이터를 사용하는 검색 함수 예시\ndef search(query_embedding, metadata_key, metadata_value):\n    \"\"\"메타데이터 기준과 일치하는 문서를 인덱스에서 검색합니다.\"\"\"\n    k = 2  # 찾을 최근접 이웃 수\n    distances, indices = index.search(np.array([query_embedding]), k)  # 검색 수행\n    results = []\n    for idx in indices[0]:\n        if metadata[idx][metadata_key] == metadata_value:\n            results.append((documents[idx], metadata[idx]))\n    return results\n\n# 쿼리 임베딩 생성(실제 상황에서는 유사한 프로세스로부터 생성)\nquery_embedding = generate_embeddings([\"여기에 쿼리 내용\"])[0]\n\n# '업데이트'로 태그된 문서 검색\nmatching_documents = search(query_embedding, '태그', '업데이트')\nprint(matching_documents)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사용자가 질문을 명확하게 표현하지 못할 수도 있는 경우가 있습니다. 그런 경우에는 질문을 완전히 다시 작성하거나 확장하여 쿼리를 개선할 수 있습니다.\n\nLLM 자체를 활용할 수 있습니다. 질문을 LLM에 보내서 더 명확하게 표현하도록 요청할 수 있습니다. 다음 프롬프트가 그 작업에 도움이 될 것입니다.\n\n```js\n프롬프트: '{prompt}'을(를) 주어서, 더 잘 표현된 3가지 질문을 생성하십시오.\n```\n\n새로운 쿼리를 얻은 후, 새로운 쿼리를 임베딩 벡터로 변환하여 벡터 데이터베이스에서 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 3. 고급 검색 증강 생성 (RAG)을 위한 검색 기술\n\n검색은 쿼리가 이전에 색인 된 데이터베이스를 탐색하는 단계입니다. 아래에서는 검색을 위한 다양한 전략을 논의하겠습니다.\n\n## 3.1. 하이브리드 검색 모델\n\n지금까지 우리는 항상 임베딩 벡터를 저장하는 벡터 데이터베이스에서 쿼리를 검색하는 것을 논의해 왔습니다. 이를 한 걸음 더 나아가 전통적인 키워드 기반 검색과 결합해 봅시다. 이 접근 방식은 검색 시스템이 정확한 키워드 일치를 필요로하는 쿼리부터 컨텍스트 이해가 필요한 복잡한 쿼리까지 다양한 유형의 쿼리를 처리할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하이브리드 검색 모델을 만들어 보겠습니다. 전통적인 검색 메커니즘에는 Elasticsearch를 사용하고 의미론적 검색을 위해 벡터 데이터베이스로 faiss를 사용할 것입니다.\n\n전체 코드는 여기에서 확인할 수 있어요: https://github.com/ndemir/machine-learning-projects/tree/main/hybrid-search — 여기에는 Elasticsearch를 실행하는 방법을 보여주는 보너스 Dockerfile도 포함되어 있습니다.\n\n3.1.1. Elasticsearch로 색인하기\n\n우선, 모든 문서가 'documents' 딕셔너리에 있다고 가정하고 임베딩 벡터를 이미 가져와 딕셔너리에 저장했다고 가정해 봅시다. 다음 코드 블록은 Elasticsearch 8.13.4에 연결하고 주어진 샘플 문서를 위해 색인을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nES_NODES = \"http://localhost:9200\"\n\ndocuments = [\n    {\"id\": 1, \"text\": \"파이썬 프로그래밍을 시작하는 방법.\", \"vector\": [0.1, 0.2, 0.3]},\n    {\"id\": 2, \"text\": \"고급 파이썬 프로그래밍 팁.\", \"vector\": [0.1, 0.3, 0.4]},\n    # 더 많은 문서...\n]\n\nfrom elasticsearch import Elasticsearch\n\nes = Elasticsearch(\n    hosts=ES_NODES,\n)\nfor doc in documents:\n    es.index(index=\"documents\", id=doc['id'], document={\"text\": doc['text']})\n```\n\n3.1.2. Faiss를 사용한 색인\n\n이 부분에서는 faiss를 벡터 데이터베이스로 사용하여 벡터를 색인화합니다.\n\n```js\nimport numpy as np\nimport faiss\n\ndimension = 3  # 간단함을 위해 3차원 벡터를 가정합니다.\nfaiss_index = faiss.IndexFlatL2(dimension)\nvectors = np.array([doc['vector'] for doc in documents])\nfaiss_index.add(vectors)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3.1.3. 혼합 검색\n\n아래 코드 블록은 혼합 검색을 설명합니다. 이는 사용할 수 있는 샘플이며, 또는 자체 사용 사례에 맞게 무언가를 처음부터 만들 수도 있습니다.\n\n```js\ndef hybrid_search(query_text, query_vector, alpha=0.5):\n    # \"documents\" 인덱스에서 Elasticsearch를 사용하여 제공된 query_text와 일치하는 키워드 검색 수행.\n    response = es.search(index=\"documents\", query={\"match\": {\"text\": query_text})\n    # Elasticsearch 응답에서 문서 ID 및 해당 점수를 추출합니다.\n    keyword_results = {hit['_id']: hit['_score'] for hit in response['hits']['hits']}\n\n    # Faiss 호환을 위해 query_vector를 재구성하고 float32로 변환하여 벡터 검색을 수행합니다.\n    query_vector = np.array(query_vector).reshape(1, -1).astype('float32')\n    # Faiss를 사용하여 상위 5개 가장 가까운 문서의 인덱스를 검색합니다.\n    _, indices = faiss_index.search(query_vector, 5)\n    # 벡터 검색 결과의 점수를 순위에 반비례하도록하여 딕셔너리 형태로 만듭니다 (순위가 높을수록 더 높은 점수).\n    vector_results = {str(documents[idx]['id']): 1/(rank+1) for rank, idx in enumerate(indices[0])}\n\n    # 키워드 및 벡터 검색 결과에서 결합된 점수를 보유할 딕셔너리를 초기화합니다.\n    combined_scores = {}\n    # 키워드 및 벡터 결과에서 문서 ID의 합집합을 반복합니다.\n    for doc_id in set(keyword_results.keys()).union(vector_results.keys()):\n        # 각 문서에 대한 결합된 점수를 계산합니다. alpha 매개변수를 사용하여 두 검색 결과의 영향을 균형있게 조절합니다.\n        combined_scores[doc_id] = alpha * keyword_results.get(doc_id, 0) + (1 - alpha) * vector_results.get(doc_id, 0)\n\n    # 관련 문서의 결합된 점수를 포함하는 딕셔너리를 반환합니다.\n    return combined_scores\n\n# 예시 사용법\nquery_text = \"Python 프로그래밍\"\nquery_vector = [0.1, 0.25, 0.35]\n# 지정된 쿼리 텍스트와 벡터로 혼합 검색 함수를 실행합니다.\nresults = hybrid_search(query_text, query_vector)\n# 혼합 검색 결과를 출력하여 문서의 결합된 점수를 확인합니다.\nprint(results)\r\n```\n\nhybrid_search 함수는 Elasticsearch를 사용하여 키워드 검색으로 시작합니다. 다음 단계에서; Faiss를 사용하여 벡터 검색을 수행하고, Faiss는 가장 가까운 상위 다섯 개 문서의 인덱스를 반환하며, 이러한 인덱스를 기반으로 문서의 순위에 따라 점수를 만듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 Elasticsearch와 Faiss에서 결과를 얻으면, 두 방법의 점수를 결합합니다. 각 문서의 최종 점수는 alpha 매개변수를 사용하여 계산된 가중 평균입니다. alpha=.5는 두 결과에 동등한 가중치를 부여합니다.\n\n## 3.2. 임베딩 모델 세밀 조정\n\n임베딩 모델을 세밀하게 조정하는 것은 검색 증강 생성 시스템의 성능을 향상시키는 효과적인 단계입니다. 사전 훈련된 모델을 세밀하게 조정하면 해당 도메인이나 데이터 집합의 미묘한 점을 이해하는 데 도움이 되며, 그 결과 문서의 관련성과 정확성을 크게 향상시킬 수 있습니다.\n\n세밀하게 모델을 조정하는 중요성은 다음 요점으로 요약할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 향상된 의미 이해: 세밀 조정은 모델이 원래 훈련 데이터에 잘 표현되지 않을 수 있는 도메인 특정 용어와 개념을 파악하는 데 도움이 됩니다.\n- 진화하는 콘텐츠에 대한 적응: 일부 분야(의학 또는 기술 분야 등)의 정보는 빠르게 변화하므로, 세밀 조정을 통해 임베딩을 최신 상태로 유지하면 시스템의 효과를 유지할 수 있습니다.\n- 검색 정밀도 상승: 임베딩 공간을 타깃 사용 사례에 더 밀접하게 맞추면, 세밀 조정을 통해 의미론적으로 관련있는 텍스트를 더 신뢰성 있게 검색할 수 있습니다.\n\n다음 3개의 소목을 통해 데이터를 세밀 조정하기 위한 코드 샘플을 볼 수 있습니다. 데이터를 세밀 조정하고(훈련) 모델을 사용하는 방법을 보여줍니다. 전체 코드는 제 깃허브 저장소에서 확인하실 수 있습니다: https://github.com/ndemir/machine-learning-projects/tree/main/fine-tuning-embedding-model\n\n3.2.1. 세밀 조정을 위한 데이터 준비\n\n다음 코드 블록은 모델을 세밀 조정하기 위한 첫 번째 단계입니다. 사전 훈련된 마스킹 언어 모델을 세밀 조정할 파이프라인을 초기화하고, 모델과 토크나이저를 로드하며, 장치 호환성에 맞게 조정합니다(GPU 또는 CPU).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n초기화 후 샘플 데이터셋을 토큰화하고 동적 토큰 마스킹을 통해 처리합니다. 이 설정은 모델을 자가 지도 학습을 위해 준비시켜주며, 모델은 가리킨 토큰을 예측하여 입력 데이터의 의미 이해를 향상시킵니다.\n\n```js\n# Sentence Transformers 라이브러리의 사전 훈련된 모델을 사용하여 모델 이름을 정의합니다.\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n# Hugging Face의 transformers 라이브러리에서 지정된 모델의 토크나이저를 로드합니다.\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# 지정된 모델을 기반으로 한 마스크 언어 모델링을 위해 모델을 로드합니다.\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\n\n# GPU 사용 가능 여부를 확인하고 장치를 설정합니다; GPU를 사용할 수 없는 경우 CPU를 사용합니다.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 모델을 해당 장치로 이동시킵니다 (GPU 또는 CPU)\nmodel.to(device)\n\n# 데이터셋을 생성하는 제너레이터 함수를 정의합니다; 이는 실제 데이터 로딩 로직으로 대체되어야 합니다.\ndef dataset_generator():\n    # 개별 문장으로 구성된 예제 데이터셋; 실제 데이터셋 문장으로 대체합니다.\n    dataset = [\"sentence1\", \"sentence2\", \"sentence3\"]\n    # 각 문장을 'text' 키를 가진 사전으로 반환합니다.\n    for sentence in dataset:\n        yield {\"text\": sentence}\n\n# 제너레이터 함수로부터 Hugging Face의 Dataset 클래스를 사용하여 데이터셋 객체를 생성합니다.\ndataset = Dataset.from_generator(dataset_generator)\n\n# 텍스트 데이터를 토큰화하는 함수를 정의합니다.\ndef tokenize_function(example):\n    # 입력 텍스트를 토큰화하고 모델이 처리할 수 있는 최대 길이로 자릅니다.\n    return tokenizer(example[\"text\"], truncation=True)\n\n# 데이터셋의 모든 항목에 토큰화 함수를 적용하고 효율성을 위해 배치 처리합니다.\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# 토큰화된 언어 모델링을 위한 데이터 수집기를 초기화하고 토큰을 무작위로 마스킹합니다.\n# 이는 모델을 자가 지도 학습 방식으로 훈련하는 데 사용됩니다.\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\r\n```\n\n3.2.2. 모델 세밀 조정\n\n데이터가 준비되면, 모델을 세밀 조정하는 단계로 넘어갈 수 있습니다. 이 단계에서는 모델의 기존 가중치를 사용하여 업데이트를 시작합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 코드 블록은 Hugging Face의 Trainer API를 사용하여 언어 모델의 훈련을 설정하고 실행하는 내용입니다. 먼저 훈련 매개변수(에포크, 배치 크기, 학습률 등)를 정의합니다. Trainer 객체는 미리 로드된 모델, 토큰화된 데이터 세트 및 가려진 언어 모델링용 데이터 콜렉터와 함께 이러한 설정을 사용합니다 (모델, 토큰화된 데이터 세트 및 데이터 콜렉터는 이전 단계에서 생성되었습니다). 훈련이 완료되면 새로 업데이트된 모델과 해당 토크나이저가 저장되어 다음 단계에서 사용됩니다.\n\n```js\n# 훈련 세션 구성을 위한 훈련 매개변수 정의\ntraining_args = TrainingArguments(\n    output_dir=\"output\",  # 체크포인트와 같이 저장될 출력 디렉토리\n    num_train_epochs=3,  # 수행할 총 훈련 에포크 수\n    per_device_train_batch_size=16,  # 훈련 중 각 장치당 배치 크기\n    learning_rate=2e-5,  # 옵티마이저의 학습률\n)\n\n# 훈련 루프와 평가를 처리하는 Trainer 초기화\ntrainer = Trainer(\n    model=model,  # 로드되고 구성된 훈련될 모델\n    args=training_args,  # 훈련 설정을 정의하는 훈련 매개변수\n    train_dataset=tokenized_datasets,  # 토큰화되고 준비된 데이터 세트\n    data_collator=data_collator,  # 입력 형식 및 마스킹을 처리하는 데이터 콜렉터\n)\n\n# 훈련 과정 시작\ntrainer.train()\n\n# 세부 튜닝된 모델 및 토크나이저를 저장할 경로 정의\nmodel_path = \"./model\"\ntokenizer_path = \"./tokenizer\"\n\n# 지정된 경로에 세부 튜닝된 모델 저장\nmodel.save_pretrained(model_path)\n\n# 훈련에 사용된 토크나이저를 지정된 경로에 저장\ntokenizer.save_pretrained(tokenizer_path)\n```\n\n3.2.3. 세부 튜닝된 모델 사용\n\n이제 저장된 모델과 토크나이저를 사용하여 임베딩 벡터를 생성하는 시간입니다. 아래 코드 블록이 해당 목적으로 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 코드 블록은 주어진 문장에 대한 임베딩을 생성하기 위해 모델과 토크나이저를로드하는 방법을 보여줍니다. 먼저, 모델과 토크나이저를 저장된 경로에서로드하고 GPU 또는 CPU에 할당됩니다. 문장(이 기사의 맥락에서는 쿼리입니다)이 토큰화됩니다. 모델은 이러한 입력을 처리하고 매개변수를 업데이트하지 않는 상태에서 작동합니다. 이를 추론 모드라고하며 torch.no_grad() 블록 아래에서 수행됩니다. 이 모델을 사용하여 다음 토큰을 예측하는 것은 아니며 대신 모델의 숨겨진 상태에서 임베딩 벡터를 추출하는 것이 목표입니다. 마지막 단계로, 이러한 임베딩 벡터를 CPU로 이동합니다.\n\n```js\n# 저장된 경로에서 토크나이저와 모델을로드하여 모델이 적절한 장치(GPU 또는 CPU)에 할당되도록합니다.\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\nmodel = AutoModelForMaskedLM.from_pretrained(model_path).to(device)\n\n# 가변 길이의 문장을 처리하기위한 패딩 및 자르기를 구성하여 입력 문장을 토큰화하는 함수 정의\ndef tokenize_function_embedding(example):\n    return tokenizer(example[\"text\"], padding=True, truncation=True)\n\n# 임베딩을 생성 할 예제 문장 목록\nsentences = [\"This is the first sentence.\", \"This is the second sentence.\"]\n\n# 이러한 문장을 직접 Dataset 객체로 생성\ndataset_embedding = Dataset.from_dict({\"text\": sentences})\n\n# 데이터셋을 임베딩 생성을 준비하기 위해 토큰화 함수를 적용\ntokenized_dataset_embedding = dataset_embedding.map(tokenize_function_embedding, batched=True, batch_size=None)\n\n# 모델이 어떤 부분이 패딩이고 어떤 부분이 실제 내용인지 이해하도록 하는 'input_ids' 및 'attention_mask' 추출\ninput_ids = tokenized_dataset_embedding[\"input_ids\"]\nattention_mask = tokenized_dataset_embedding[\"attention_mask\"]\n\n# 이 목록을 텐서로 변환하고 처리에 적합한 장치(GPU 또는 CPU)에 있도록합니다.\ninput_ids = torch.tensor(input_ids).to(device)\nattention_mask = torch.tensor(attention_mask).to(device)\n\n# 계산 자원을 절약하기 위해 그라데이션을 업데이트하지 않고 모델을 사용하여 임베딩 생성\nwith torch.no_grad():\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n    # 임베딩으로서 마지막 레이어의 숨겨진 상태를 추출하고, 특히 첫 번째 토큰(일반적으로 BERT 유형의 모델에서 문장 임베딩을 나타내는 데 사용됨) 사용\n    embeddings = outputs.hidden_states[-1][:, 0, :]\n\n# 임베딩을 GPU에서 CPU로 쉽게 이동하여 조작하거나 저장합니다\nembeddings = embeddings.cpu()\n\n# 각 문장에 해당하는 임베딩 벡터를 출력합니다\nfor sentence, embedding in zip(sentences, embeddings):\n    print(f\"문장: {sentence}\")\n    print(f\"임베딩: {embedding}\\n\")\n```\n\n# 4. RAG(Advanced Retrieval-Augmented Generation)를위한 포스트-검색 기법\n\n관련 정보를 검색하는 마법은 여기서 끝나지 않습니다. 가능한 개선 사항은 다음 단계에서 나타납니다: 검색 이후. 사용자는 정확한 결과뿐만 아니라 올바른 순서로 제공되는 결과를 필요로 합니다. 다음 2개 하위 섹션에서 RAG의 품질을 개선하기 위해 요약 및 재정렬을 사용하는 방법을 설명하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4.1. 응답 요약화\n\n만약 색인화 과정 중에 대량의 텍스트 벡터를 데이터베이스에 저장했다면, 이 단계가 필요할 수 있습니다. 이미 텍스트가 작은 경우에는 이 단계가 필요하지 않을 수 있습니다.\n\n다음 코드 블록은 요약화 프로세스에 사용될 수 있습니다. 아래 코드 블록은 전처리된 BART 모델을 사용하여 텍스트를 요약화하는 데 transformer 라이브러리를 사용합니다. summarize_text 함수는 텍스트를 받아들이고 최대 및 최소 길이 매개변수를 기반으로 간결한 요약을 생성하기 위해 모델을 사용합니다.\n\n```js\nfrom transformers import pipeline\ndef summarize_text(text, max_length=130):\n  \n    # Hugging Face의 모델 허브에서 사전 학습된 요약 모델을 로드합니다.\n    # 'facebook/bart-large-cnn'은 간결한 요약을 생성하는 데 뛰어난 능력을 가졌기 때문에 선택되었습니다.\n    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n  \n    # 요약기는 입력 텍스트를 BART 모델을 사용하여 압축합니다.\n    # 'max_length'는 요약 출력의 최대 길이를 지정합니다.\n    # 'min_length'는 요약이 너무 간결하지 않도록 하기 위해 최소 길이를 설정합니다.\n    # 'do_sample'은 요약 생성에 결정론적 접근 방식을 사용하기 위해 False로 설정됩니다.\n    summary = summarizer(text, max_length=max_length, min_length=30, do_sample=False)\n    \n    # 요약기의 출력은 딕셔너리 목록입니다.\n    # 우리는 목록의 첫 번째 딕셔너리에서 요약 텍스트를 추출합니다.\n    return summary[0]['summary_text']\n\n# 요약화될 예문.\n# 이 텍스트는 검색 보완 생성 시스템에서 요약화의 중요성에 대해 논의합니다.\nlong_text = \"검색 보완 생성 시스템의 작업 흐름에서 요약화는 중요한 단계입니다. 결과물이 정확할 뿐만 아니라 간결하고 소화하기 쉽도록 보장합니다. 정보의 정확성과 정밀도가 중요한 도메인에서 특히 필수적입니다.\"\n\n# summarize_text 함수를 호출하여 예시 텍스트를 압축합니다.\nsummarized_text = summarize_text(long_text)\n\n# 요약된 텍스트를 출력하여 요약화 모델의 출력을 확인합니다.\nprint(\"요약된 텍스트:\", summarized_text)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4.2. 다시 순위 매기기 및 필터링\n\n검색 과정에서 각 문서의 \"점수\"를 받아야 했을 것입니다. 이 \"점수\"는 실제로 벡터의 유사성을 쿼리 벡터와 비교한 결과입니다. 이 정보를 사용하여 문서를 다시 순위 매기고, 특정 임계값에 따라 결과를 필터링할 수 있습니다. 다음 코드 블록은 다시 순위 매기고 필터링하는 방법의 샘플을 보여줍니다.\n\n### 4.2.1. 기본 다시 순위 매기기 및 필터링\n\n제공된 코드 블록은 각각 ID, 텍스트, 그리고 관련 점수를 포함하는 딕셔너리로 표현된 문서 목록을 정의합니다. 그런 다음 두 가지 주요 함수인 re_rank_documents와 filter_documents를 구현합니다. re_rank_documents 함수는 관련 점수를 기준으로 문서를 내림차순으로 정렬하고, 다시 순위 매기기 후 filter_documents 함수가 적용되어 관련 점수가 0.75보다 낮은 문서를 제외합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 문서 목록 정의하기. 각 문서는 ID, 텍스트 및 관련도 점수로 표시된 딕셔너리 형태로 표현됩니다.\ndocuments = [\n    {\"id\": 1, \"text\": \"고급 RAG 시스템은 텍스트 요약에 대한 정교한 기술을 사용합니다.\", \"relevance_score\": 0.82},\n    {\"id\": 2, \"text\": \"기본 RAG 시스템은 주로 검색 및 기본 처리에 초점을 맞춥니다.\", \"relevance_score\": 0.55},\n    {\"id\": 3, \"text\": \"재랭킹은 관련성에 따라 문서를 정렬하여 응답 품질을 향상시킵니다.\", \"relevance_score\": 0.89}\n]\n\n# 문서를 관련도 점수에 따라 재랭킹하는 함수 정의하기.\ndef re_rank_documents(docs):\n\n    # sorted 함수를 사용하여 'relevance_score'를 기준으로 문서를 정렬합니다.\n    # 정렬에 사용되는 키는 람다 함수를 사용하여 각 문서에서 관련도 점수를 추출합니다.\n    # 'reverse=True'는 관련성 점수가 높은 문서를 먼저 배치하여 내림차순으로 목록을 정렬합니다.\n    return sorted(docs, key=lambda x: x['relevance_score'], reverse=True)\n\n# 정의된 함수를 사용하여 문서를 재랭킹하고 결과를 출력합니다.\nranked_documents = re_rank_documents(documents)\nprint(\"재랭킹된 문서:\", ranked_documents)\n\n# 관련도 점수 임계값에 따라 문서를 필터링하는 함수 정의하기.\ndef filter_documents(docs, relevance_threshold=0.75):\n  \n    # 리스트 내포를 사용하여 'relevance_score'가 'relevance_threshold' 이상인 문서만 포함하는 새 리스트를 생성합니다.\n    return [doc for doc in docs if doc['relevance_score'] \u003e= relevance_threshold]\n\n# 정의된 함수를 사용하여 재랭킹된 문서를 임계값 0.75로 필터링하고 결과를 출력합니다.\nfiltered_documents = filter_documents(ranked_documents)\nprint(\"필터링된 문서:\", filtered_documents)\r\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 데이터가 다음과 같은 형식으로 데이터베이스에 저장되어 있다고 가정합니다.\n# query_text | response_text | user_clicked\n\nquery_embeddings = get_embedding_vector(database.query_text) \nresponse_embeddings = get_embedding_vector(database.response_text) \n\n# 데이터셋 생성\nX = concat(query_embeddings, response_embeddings)\ny = database.user_clicked\n\nmodel = model.train(X, y)\nmodel.predict_proba(...)\n\n위의 의사 코드는 머신러닝을 사용하여 관련성에 따라 문서를 다시 순위 지정하는 접근 방식을 개요로 제시합니다. 특히 사용자가 문서를 관련성 있게 여기는 가능성을 예측하여 기반으로 새로운 상호작용을 중심으로 문서들을 순위 지정합니다. 여기서 의사 코드에 나와 있는 프로세스에 대한 단계별 설명은 다음과 같습니다:\n\n- 임베딩 생성: 쿼리 및 응답 문서 모두 의미 적 컨텐츠를 포착하는 임베딩 벡터가 생성됩니다.\n- 데이터셋 생성: 이러한 임베딩은 피쳐 벡터(X)를 형성하기 위해 연결되며, 목표 변수(y)는 사용자가 문서를 클릭했는지를 나타냅니다.\n- 모델 훈련: 이 데이터셋에 대해 분류 모델이 훈련되어 쿼리와 문서 임베딩을 결합해 문서가 클릭될 가능성을 예측합니다.\n- 예측: 훈련된 모델은 새로운 쿼리-문서 쌍의 클릭 확률을 예측할 수 있어서 예측된 관련성에 따라 문서를 다시 순위 지정해 검색 결과 정확도를 향상할 수 있습니다.\n\n# 5. 결론\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단한 검색 증강 생성(RAG) 시스템을 구현하면 문제를 해결할 수 있지만, 향상 사항을 추가하면 결과를 개선하고 시스템이 더 정확한 답변을 생성하는 데 도움이 됩니다. 이 글에서는 데이터 인덱싱 최적화, 쿼리 향상, 하이브리드 검색, 임베딩 모델의 세밀한 조정, 응답 요약, 다시 순위 매기기 및 필터링을 포함한 목표 달성을 위한 여러 가지 향상 사항을 논의했습니다. \n\n이러한 향상 사항을 통합함으로써 성능을 크게 향상시킬 수 있는 기회가 주어집니다. 계속해서 이러한 방법을 탐색하고 적용하여 여러분의 요구에 가장 적합한 방법을 찾도록 실험해 보세요.","ogImage":{"url":"/assets/img/2024-06-22-AdvancedRAGImplementingAdvancedTechniquestoEnhanceRetrieval-AugmentedGenerationSystems_0.png"},"coverImage":"/assets/img/2024-06-22-AdvancedRAGImplementingAdvancedTechniquestoEnhanceRetrieval-AugmentedGenerationSystems_0.png","tag":["Tech"],"readingTime":19},{"title":"ML 엔지니어를 위한 저장 공간 부족 문제 피하는 방법 가이드라인","description":"","date":"2024-06-22 19:50","slug":"2024-06-22-HowtoAvoidtheOut-of-SpaceProblemAGuidelineforanMLEngineer","content":"\n\n체크포인트, 도커 이미지, Python 환경, HF 모델 및 pip 캐시는 귀하가 완전한 인식 없이 시간이 지남에 따라 성장하여 디스크 공간을 더 많이 차지할 수 있습니다. 공간 부족 문제에 빠지지 않도록 주의하십시오.\n\n# 소개\n\n하드 드라이브가 얼마나 큰지 상관없이 언젠가는 공간 부족 문제에 직면하게 될 것입니다. 머신러닝 엔지니어 또는 데이터 과학자로서, 홈 폴더, 시스템 구조 또는 작업 공간 내 특정 위치가 시간이 지남에 따라 성장합니다. 이들은 주로 작업을 가속화하는 캐시이거나 특정 데이터의 전역 위치입니다. 이 글에서는 디스크의 일부 공간을 확보하기 위해 점검하고 조사해야 할 일반적인 위치 몇 군데를 모았습니다.\n\n아래는 여러 달 동안 큰 정기 청소 없이 작업한 후 공간 사용 현황을 이해하기 위해 위치별로 나눈 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n600G - 모델 체크포인트\n290G - Docker 이미지, 컨테이너 및 캐시\n231G - ~/miniconda3/envs\n 87G - ~/.cache/huggingface\n 41G - ~/miniconda3/pkgs\n 15G - ~/.cache/pip\n\n\n동일한 분할을 파이 차트로도 제시했습니다:\n\n데이터 1TB는 많지 않아 보일 수 있습니다. HDD 디스크는 비교적 저렴하며, 4-8TB 디스크를 가지는 것은 비싸지 않을 수 있습니다. 그러나 GPU를 사용한 컴퓨팅을 허용하는 Macbook M1/2/3/4 칩을 갖고 있는 노트북의 경우 비쌔 일 수 있습니다. 그러므로 주의와 정기적인 저장 공간 정리가 도움이 될 수 있습니다.\n\n# 모델 체크포인트\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ntransformers 라이브러리를 사용하여 ML 모델을 매일 훈련한다면, 알지 못할 수도 있는 많은 체크포인트를 얻게 될 수 있습니다. 체크포인트는 훈련 과정 중에 모델의 중간 복사본으로 생성되어, 훈련 중단의 경우 모델을 백업하는 역할을 합니다. 훈련이 완료되면 체크포인트를 제거해야 합니다. 하지만 이러한 체크포인트는 종종 무심코 하드 드라이브에 남아 있는 경우가 있습니다.\n\n사용 중인 체크포인트의 개수에 대해 궁금하다면, 다음을 실행해 보세요.\n\n```js\nfind . -name \"checkpoint-*\" | wc -l\n```\n\n제 경우에는 509개의 체크포인트가 있었습니다. 그들이 차지하는 용량을 확인해보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sh\nfind . -name \"checkpoint-*\" | \\\n  xargs -I {} du --bytes {} | \\\n  awk '{ print; total += $1 }; END { print \"Total: \" total/1024/1024/1024 \" GB\" }'\n```\n\n오버 600GB이네요. 상당한 용량이에요!\n\n간단히 bash 스크립트를 설명하면:\n\n- find — 모델 체크포인트의 기본 이름인 \"checkpoint-\"로 시작하는 폴더 목록을 생성합니다.\n- xargs — 각 폴더를 du 명령어에 전달합니다. du 명령어는 find로 반환된 각 폴더에 대해 실행됩니다.\n- du — 바이트 단위로 폴더의 총 크기를 계산합니다.\n- awk — du에서 크기를 추출하여 합하여 단일 값으로 출력합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 큰 체크포인트부터 순서대로 나열하려면 다음 명령을 사용하면 됩니다:\n\n```js\nfind . -name \"checkpoint-*\" | \\\n  xargs -I {} du --bytes {} | \\\n  sort -n | \\\n  awk '{ print $1/1024/1024/1024 \" GB \" $2 }'\n```\n\n이 명령을 실행하면 다음과 같이 출력됩니다 (가독성을 위해 경로는 체크포인트 이름으로 줄였습니다):\n\n```js\n4.59175 GB (...)/checkpoint-19960-epoch-2\n4.59175 GB (...)/checkpoint-29940-epoch-3\n4.59175 GB (...)/checkpoint-39920-epoch-4\n4.59175 GB (...)/checkpoint-9980-epoch-1\n4.60686 GB (...)/checkpoint-20000\n4.60687 GB (...)/checkpoint-12000\n4.60687 GB (...)/checkpoint-13000\n4.60688 GB (...)/checkpoint-17000\n6.52057 GB (...)/checkpoint-6605\n6.52059 GB (...)/checkpoint-2310\n9.16658 GB (...)/checkpoint-21139\n9.16682 GB (...)/checkpoint-42278\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 목록을 사용하여 각 체크포인트를 검토하고 유지 또는 제거할지 결정할 수 있습니다.\n\n# Docker 이미지\n\n도커 컨테이너는 ML 모델을 포함한 다양한 응용 프로그램을 배포하는 편리한 방법입니다. 도커 이미지를 만들거나 외부 레지스트리에서 가져올 수 있습니다. 이미지를 빌드하거나 가져오면 작업 스테이션에 그대로 남아 있습니다. 시간이 지남에 따라 가지고 있는 이미지의 수와 차지하는 디스크 공간의 양에 놀라게 될 수 있습니다.\n\n도커가 사용한 공간을 확인하려면 다음 명령어를 실행하세요 [2]:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n도커 시스템 df\n```\n\n위 명령어를 실행하면 다음과 같은 결과가 출력됩니다:\n\n```js\nTYPE TOTAL ACTIVE SIZE RECLAIMABLE\nImages 133 69 264.4GB 180.6GB (68%)\nContainers 160 3 11.54GB 9.582GB (83%)\nLocal Volumes 6 4 256.9MB 5.419kB (0%)\nBuild Cache 473 0 14.56GB 14.56GB\n```\n\n여기서, 우리는 도커 이미지의 총 크기에만 초점을 맞출 것입니다. 제 경우에는 180GB가 넘습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n도커 정리를 처리하는 한 가지 방법은 프룬 명령어를 사용하는 것입니다 [3]. 문서에 따르면:\n\n이 명령은 직접적인 경우만 제거합니다. 이미지를 개별적으로 확인하고 남은 것이 여전히 필요한지 결정해야 할 것입니다. 한 번만 시도해본 이미지거나 더 이상 필요하지 않은 이미지가 있을 수도 있습니다. 제거할 수 있는 더 오래된 이미지 버전도 있을 수 있습니다. 다른 경우에는 더 오래된 버전을 남겨두어 더 최신 버전과 비교하고 싶을 수도 있습니다.\n\n이미지를 크기순으로 나열하기 위해 다음 명령어를 사용하세요:\n\n```js\ndocker image ls --format \"{.Size} {.ID} {.Repository}:{.Tag}\" | \\\n  LANG=en_US sort -h | \\\n  column -t\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같이 출력됩니다:\n\n```js\n(...)\n9.43GB  8ab26e6b7035  mczuk/poldeepner2:kpwr_timex_1.0\n9.58GB  8b50a5264fa1  mczuk/poldeepner2:nkjp_base_sq\n9.74GB  46d6ea3f8fea  nvidia/cuda:11.8.0-cudnn8-devel-ubuntu20.04\n10.5GB  9788da8da088  \u003cnone\u003e:\u003cnone\u003e\n10.8GB  484fc54f67e7  \u003cnone\u003e:\u003cnone\u003e\n10.8GB  b9bb30fe2458  \u003cnone\u003e:\u003cnone\u003e\n11.3GB  d37700724b89  \u003cnone\u003e:\u003cnone\u003e\n18.3GB  1d9a58a6fcf5  nvcr.io/nvidia/pytorch:22.12-py3\n```\n\n특정 도커 이미지를 제거하려면 docker image rm ID [4]를 사용합니다.\n\n# Miniconda 환경\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 프로젝트마다 별도의 Python 환경을 실행하는 것은 좋은 습관입니다. 이렇게 하면 라이브러리 의존성 간 충돌을 피하거나 두 개 이상의 프로젝트가 동일한 환경을 공유하는 상황을 방지할 수 있습니다. 한 프로젝트를 업데이트하면 다른 프로젝트가 작동을 중지하는 일도 없어집니다.\n\nPython 환경을 관리하는 인기 있는 솔루션 중 하나는 Miniconda입니다. Miniconda는 모든 환경을 한 곳에 유지함으로써, 보유한 환경 수와 사용 용량을 확인하기 쉽습니다. 환경은 ~/miniconda3/envs에 저장됩니다. 여러 환경을 보유하는 것은 문제가 되지 않을 수 있지만, GPU를 사용한 ML 모델 학습이나 추론을 할 때 NVidia 라이브러리(CUDA, cuDNN, cuBLAS 등)를 사용하는 경우가 많습니다. 이들은 상당히 많은 용량을 차지하고 있습니다. 기본 설정만 해도 약 10GB가 필요하며, 다른 의존성을 고려하지 않았습니다. Miniconda에서는 각 환경이 모듈의 복사본을 가지고 있기 때문에 공간 사용량이 더해지게 됩니다.\n\n저의 경우, miniconda3/envs 폴더는 230GB를 차지하고 있으며 100개 이상의 환경이 있습니다. 이미 일부 환경을 용량 부족으로 제거했었죠 ;-)\n\n오래된, 사용하지 않는 환경은 신중히 제거해야 합니다. 일부 오래된 프로젝트는 문제없이 함께 작동하는 라이브러리 조합을 가질 수 있습니다. 대부분의 프로젝트들은 환경을 생성할 수 있는 필수 라이브러리 목록(requirements.txt 또는 pyproject.toml)을 갖고 있어야 합니다. 그러나 몇 가지 모듈은 요구사항 목록이나 사용하는 라이브러리 중 하나에 특정 버전이 누락될 수 있습니다. 이러한 경우 정확한 라이브러리 조합을 다시 만드는 것이 어려울 수 있습니다. 이에 대한 해결책으로 pip freeze 명령을 사용하여 라이브러리의 정확한 버전을 덤프하고 나중에 검증할 수 있습니다. 기억해야 할 점은, 심지어 작은 변경이나 패치라도 라이브러리 간의 호환성에 일부 문제를 초래할 수 있는 일반적인 문제입니다. 해서서는 안 되지만 그렇게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Miniconda 패키지\n\nMiniconda는 conda install 명령어로 설치된 패키지를 캐시합니다. 캐시는 ~/miniconda3/pkgs 폴더에 위치해 있습니다. 제 경우엔 62GB입니다. 해당 폴더를 열어보면 여러 버전의 패키지가 몇 개만 있는 것을 볼 수 있습니다. 예전 버전은 종종 필요하지 않을 수 있어 쉽게 삭제할 수 있습니다.\n\n# Huggingface 모델 및 데이터셋\n\nHuggingface는 transformers 라이브러리를 통해 사용하는 모델 및 데이터셋을 모두 캐시합니다. 제 캐시의 분석을 통해 알 수 있는 정보가 여기에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n허깅페이스 캐시에서 모델을 제거하는 것이 이전 파이썬 환경을 제거하는 것보다 안전합니다. 모델은 리포지토리에서 다시 다운로드할 수 있습니다. (단, 리포지토리에서 제거되지 않은 경우) 그럼에도 불구하고 캐시에는 기억하지 못하는 모델이 많이 있음을 알게 될 것입니다.\n\n# 파이썬 pip 캐시\n\n마지막 위치는 pip 캐시로, pip로 설치된 파이썬 모듈을 보관합니다. 캐시에 대한 정보를 표시하려면 pip cache info [6]를 사용할 수 있습니다. 아래와 같이 출력됩니다:\n\n```js\n패키지 인덱스 페이지 캐시 위치 (pip v23.3+): /home/czuk/.cache/pip/http-v2\n패키지 인덱스 페이지 캐시 위치 (이전 버전 pips): /home/czuk/.cache/pip/http\n패키지 인덱스 페이지 캐시 크기: 18297.6 MB\nHTTP 파일 수: 3025\n로컬 빌드된 휠 위치: /home/czuk/.cache/pip/wheels\n로컬 빌드된 휠 크기: 457 kB\n로컬 빌드된 휠 수: 5\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저의 경우에는 18 GB입니다. pip 캐시를 정리하기 위해 pip cache purge 명령을 사용할 수 있습니다.\n\n# 결론\n\n정기적인 디스크 사용량 관리는 공간 부족 오류로부터 자신을 보호하는 데 중요합니다. 급한 계산을 해야 할 때 메시지가 나타나면 정말 골치 아플 수 있습니다. 그런데 공간이 부족해서 계산 작업을 완료할 수 없는 경우가 발생할 수 있습니다. 로컬 워크스테이션이 아닌 워크스테이션에서 모델 훈련이나 다른 계산을 수행하고 있을 때 심지어 작업이 중단될 수도 있습니다.\n\n클라우드 서비스를 사용할 때는 디스크 사용량 관리가 특히 중요합니다. 원격 인스턴스나 SageMaker와 같은 ML 플랫폼을 사용할 때 공간을 사용하면 요금이 청구됩니다. 로컬 워크스테이션에서 공간이 부족한 메시지가 나타날 수 있습니다. 클라우드에서는 저장 용량이 거의 무한합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n- [바 그래프 도구](https://www.rapidtables.com/tools/bar-graph.html)\n- [도커 CLI 시스템 리소스 확인](https://docs.docker.com/reference/cli/docker/system/df/)\n- [도커 자원 정리 설정](https://docs.docker.com/config/pruning/)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[4] https://docs.docker.com/reference/cli/docker/image/rm/\n\n[5] https://docs.anaconda.com/\n\n[6] https://pip.pypa.io/en/stable/cli/pip_cache/\n\n[7] https://aws.amazon.com/pm/sagemaker\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[8] [https://medium.com/codenlp/6-things-about-sagemaker-i-wish-i-had-known-earlier-revision-2-e90511d58ca5#beed](https://medium.com/codenlp/6-things-about-sagemaker-i-wish-i-had-known-earlier-revision-2-e90511d58ca5#beed)","ogImage":{"url":"/assets/img/2024-06-22-HowtoAvoidtheOut-of-SpaceProblemAGuidelineforanMLEngineer_0.png"},"coverImage":"/assets/img/2024-06-22-HowtoAvoidtheOut-of-SpaceProblemAGuidelineforanMLEngineer_0.png","tag":["Tech"],"readingTime":8},{"title":"깊게 배우는 딥러닝 일러스트, Part 5 LSTM Long Short-Term Memory 이해하기","description":"","date":"2024-06-22 19:49","slug":"2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM","content":"\n\n우리의 색다른 신경망 Deep Learning 여정의 제5 부분에 오신 것을 환영합니다!\n\n오늘은 일반적인 순환 신경망(RNN)을 업그레이드한 Long Short-Term Memory(LSTM)에 대해 이야기할 것입니다. 이전 글에서 다룬 RNN은 순차 기반 문제를 해결하는 데 사용되지만 멀리 떨어진 정보를 기억하는 데 어려움을 겪어 단기 기억 문제가 발생합니다. 여기에서 LSTM이 등장하여 해결책을 제시합니다. LSTM은 RNN의 순환적 측면을 사용하지만 약간의 차별성을 갖고 있습니다. 이렇게 어떻게 이루어지는지 함께 알아보겠습니다.\n\n추신 — 이 글은 제가 쓴 중에서 제일 좋아하는 글 중 하나이니 이 여정을 함께 할 수 있는 것을 기다리지 않을 수가 없어요!\n\n먼저, 이전에 우리 RNN에서 무슨 일이 벌어지고 있었나 살펴보겠습니다. 우리는 입력 x와 하나의 은닉층이 있었는데, 이 은닉층은 tanh 활성화 함수를 가진 뉴런 하나로 이루어져 있었으며, 출력 뉴런은 시그모이드 활성화 함수를 가지고 있었습니다. 그래서 RNN의 첫 번째 단계는 이렇게 보입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_0.png)\n\nHere, we first pass our first input, x₁, to the hidden neuron to get h₁.\n\n![image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_1.png)\n\nFrom here we have two options:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n(option 1) 이 h₁를 출력 뉴런에 전달하여 이 하나의 입력만 사용하여 예측을 얻을 수 있습니다. 수학적으로:\n\n![image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_2.png)\n\n(option 2) 다음 숨겨진 상태로 이 h₁를 전달함으로써, 이 값을 다음 네트워크의 숨겨진 뉴런으로 전달합니다.\n\n그렇게 되면 두 번째 숨겨진 상태는 다음과 같이 보일 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Deep Learning Illustrated Part 5](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_3.png)\n\n첫 번째 네트워크의 숨겨진 뉴런에서 출력을 가져와 현재 네트워크의 두 번째 입력 x₂와 함께 전달합니다. 이렇게 함으로써 두 번째 숨겨진 레이어 출력 h₂를 얻을 수 있습니다.\n\n![Deep Learning Illustrated Part 5](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_4.png)\n\n여기서 h₂로 두 가지 작업을 수행할 수 있습니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n(option 1) 첫 번째 x₁와 두 번째 x₂의 결과인 예측을 얻기 위해 출력 뉴런에 전달합니다.\n\n![image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_5.png)\n\n(option 2) 또는 그대로 다음 네트워크로 전달할 수도 있습니다.\n\n그리고 이 프로세스는 계속됩니다. 각 상태는 이전 네트워크의 숨겨진 뉴런에서(새 입력과 함께) 출력을 가져와 현재 상태의 숨겨진 뉴런에 전달하면서 현재 숨겨진 레이어의 출력을 생성합니다. 이 출력을 다음 네트워크로 전달하거나 출력 뉴런에 전달하여 예측을 생산할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래의 핵심 방정식들로 이 프로세스 전체를 포착할 수 있습니다:\n\n![equations](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_6.png)\n\n간단함에도 불구하고, 이 방법론에는 제한이 있습니다: 우리가 마지막 단계로 나아감에 따라, 초기 단계에서의 정보가 사라지기 시작하며, 네트워크가 많은 정보를 유지하지 못하기 때문입니다. 입력 순서가 클수록 이 문제가 더욱 두드러집니다. 분명히, 이 기억을 향상시키기 위한 전략이 필요합니다.\n\n## LSTMs의 등장\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그들은 각 단계마다 입력과 이전 단계에서 불필요한 정보를 버리면서 간단하고 효과적인 전략을 구현하여 이를 달성합니다. 이를 통해 중요하지 않은 정보는 잊고 중요한 정보만 보존함으로써 동작합니다. 우리 뇌가 정보를 처리하는 방식과 비슷합니다. 우리는 모든 세부 사항을 기억하지 않고 필요한 세부 정보만 기억하고 나머지는 버립니다.\n\n## LSTM 아키텍처\n\n기본 RNN의 hidden state를 고려해보세요.\n\n![LSTM](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 상태는 두 개의 플레이어로 시작합니다: 이전 숨겨진 상태 값 hₜ₋₁과 현재 입력 xₜ입니다. 그리고 최종 목표는 숨겨진 상태 출력 hₜ를 생성하는 것인데, 이는 다음 숨겨진 상태로 전달되거나 출력 뉴런에 전달되어 예측을 생성할 수 있습니다.\n\nLSTM은 약간 복잡성이 증가한 유사한 구조를 가지고 있습니다.\n\n이 다이어그램은 처음에는 복잡해 보일 수 있지만 실제로 직관적입니다. 천천히 이해해 보겠습니다.\n\nRNN에는 두 명의 플레이어가 있어서 숨겨진 상태 출력을 생성하는 최종 목표가 있었습니다. 이제 LSTM에는 세 명의 플레이어가 처음에 있으며, 이전 장기 기억 Cₜ₋₁, 이전 숨겨진 상태 출력 hₜ₋₁ 및 입력 xₜ가 LSTM에 입력됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최종 목표는 두 가지 출력물 — 새로운 장기 기억 Cₜ와 새로운 은닉 상태 출력 hₜ을 만드는 것입니다:\n\nLSTM의 주요 목표는 필요 없는 정보를 최대한 버리는 것이며, 이는 세 부분에서 구현됩니다 —\n\ni) 잊기 부분\n\nii) 입력 부분\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\niii) 그리고 출력 섹션\n\n우리는 그들이 모두 공통적으로 보라색 셀을 가지고 있다는 것을 알 수 있습니다:\n\n이러한 셀을 '게이트'라고 합니다. LSTM은 어떤 정보가 중요한지 아닌지를 결정하기 위해 시그모이드 활성화 함수를 사용하는 뉴런인 게이트를 활용합니다.\n\n![이미지](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_8.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 게이트들은 각자의 부분에 정보를 유지할 비율을 결정하고, 그들이 통과시킬 정보의 비율을 제한함으로써 효과적으로 문지기 역할을 합니다.\n\n이러한 맥락에서 시그모이드 함수를 사용하는 것은 전략적입니다. 이 함수는 0에서 1까지의 값을 출력하여 보관하려는 정보의 비율에 직접 대응됩니다. 예를 들어, 값이 1이면 모든 정보가 보존되고, 0.5면 정보의 절반만 유지되며, 값이 0이면 모든 정보가 버려집니다.\n\n이제 이러한 게이트들에 대한 공식을 살펴보겠습니다. 은닉 상태 다이어그램을 자세히 살펴보면 모두 동일한 입력인 xₜ와 hₜ₋₁을 가지고 있지만, 다른 가중치와 편향 용어를 가지고 있습니다.\n\n모두 동일한 수학적 공식을 가지고 있지만, 가중치와 편향 값을 적절히 교체해주어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_9.png)\n\n각각의 이것들은 시그모이드 함수가 작동하는 방식이기 때문에 0과 1 사이의 값을 생성할 것입니다. 이 값은 우리가 각 섹션에서 유지하려는 특정 정보의 비율을 결정합니다.\n\n## Forget 섹션\n\n이 섹션의 주요 목적은 장기 기억의 어느 정도를 잊을지 판단하는 것입니다. 따라서 여기서 우리가 하는 일은 간단히 말해서 forget gate로부터 이 비율(0~1의 값)를 가져오는 것뿐입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Deep Learning Illustrated Part 5 LSTM 10 Image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_10.png)\n\n...and multiplying that with the previous long-term memory:\n\n![Deep Learning Illustrated Part 5 LSTM 11 Image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_11.png)\n\nThis product gives us the exact previous long-term memory that the forget gate thinks is important and forgets the rest. So the closer the forget gate proportion, fₜ, is to 1, the more of the previous long-term memory we’re going to retain.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_12.png)\n\n## 입력 섹션\n\n이 섹션의 주요 목적은 새로운 장기 기억을 만드는 것이며, 이를 2단계로 수행합니다.\n\n(단계 1) 새로운 장기 기억을 위한 후보인 C(tilda)ₜ를 생성합니다. 이 신경원은 하이퍼볼릭 탄젠트 활성화 함수를 사용하여 이 새로운 장기 기억 후보를 얻습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 볼 수 있듯이이 뉴런의 입력은 게이트와 유사하게 xₜ 및 hₜ₋₁입니다. 그래서 이를 뉴런을 통과시키면...\n\n![image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_13.png)\n\n...우리는 새로운 장기 기억을 위한 후보인 출력을 얻습니다.\n\n이제 우리는 그 후보에서 필요한 정보만 유지하고 싶습니다. 이것이 입력 게이트가 작용하는 곳입니다. 우리는 입력 게이트에서 얻은 비율을 사용합니다...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_14](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_14.png)\n\n…필요한 데이터만 유지하기 위해 입력 게이트 비율과 후보 사이의 곱을 통해 후보를 구합니다:\n\n![DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_15](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_15.png)\n\n(단계 2) 이제 최종 장기 기억을 얻기 위해 우리는 잊기 섹션에 유지하기로 결정한 이전 장기 기억을 가져옵니다…\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_16.png)\n\n…그리고 이 입력 섹션에 유지하기로 결정한 새 후보의 양을 추가하십시오:\n\n![이미지](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_17.png)\n\n그리고 와! 우리는 게임의 미션 1을 완수했습니다. 새로운 장기 기억을 만들었습니다! 이제 새로운 숨겨진 상태 출력을 생성해야 합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 출력 섹션\n\n이 섹션의 주요 목적은 새로운 숨겨진 상태 출력을 만드는 것입니다. 이것은 꽤 간단합니다. 여기서 하는 일은 새로운 장기 기억인 Cₜ를 tanh 함수를 통과시키는 것뿐입니다...\n\n![이미지](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_18.png)\n\n...그리고 출력 게이트 비율과 곱하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_19.png)\n\n새로운 숨겨진 상태 출력이 생성됩니다!\n\n그렇게해서 미션 2를 완료했어요 — 새로운 숨겨진 상태 출력을 생성했어요!\n\n이제 우리는 이러한 새로운 출력을 다음 숨겨진 상태로 전달하여 동일한 프로세스를 계속 반복할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 각 hidden state가 output neuron을 가지고 있는 것을 확인할 수 있습니다:\n\nRNN과 마찬가지로 이러한 각 상태는 자체 개별적인 출력을 생성할 수 있습니다. 그리고 RNN과 유사하게, 우리는 hidden state의 출력인 hₜ를 사용하여 예측을 생성합니다. 따라서 hₜ를 output neuron에 전송하면...\n\n![image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_20.png)\n\n...우리는 이 hidden state에 대한 예측을 얻습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 이로써 마무리 지을게요. 우리가 본 것처럼, LSTM은 순차 데이터에서 장기 의존성을 더 잘 다루어 RNN을 끌어올립니다. 우리는 LSTM이 핵심 정보를 유지하고 관련 없는 정보를 버리는 독특한 방식을 보았는데, 이는 우리 뇌가 하는 것과 유사합니다. 확장된 시퀀스에서 중요한 세부 정보를 기억하는 이 능력으로 LSTM은 자연어 처리, 음성 인식 및 시계열 예측과 같은 작업에 특히 강력합니다.","ogImage":{"url":"/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_0.png"},"coverImage":"/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_0.png","tag":["Tech"],"readingTime":8},{"title":"존재와 애니메이션 물질 세계를 움직이는 힘","description":"","date":"2024-06-22 19:46","slug":"2024-06-22-BeingsandAnimatedMatter","content":"\n\n\"Homo Sapiens, Robot Being, Robot Doing. 인간, 로봇. 우리 생물학적 인간들에게, 함께 있는 심리적 필요를 충족시키기 위해 인공생명 형태들이 우리에게 무엇을 할 수 있을까요?\n\nShoji Morimoto의 회고록 '아무 것도 하지 않는 렌탈 인간'에서 Shoji는 조금만 해도 함께할 수 있는 다른 사람과 있고자 합니다. Shoji는 다양한 상황에서 다른 사람들과 함께존재하고 사회적으로 함께할 수 있는 렌탈 인간으로 자신을 제공합니다. Shoji는 조언을 제공하지 않고 듣는 것에 대한 예시를 제공하지만, 그 자신이 듣고 있다는 것을 인정합니다. 그는 클라이언트를 위해 꽃 파티를 위한 자리를 지키기 위해 꽃이 피는 나무 아래에서 앉아 기다릴 것이지만, 클라이언트를 위해 그 자리를 고르지는 않을 것입니다. 그는 동물 복장을 하고 낯선 사람들에게 인사를 건네는 사람과 함께 공원에 있을 것에 동의합니다. 이러한 만남들은 일회성이며 클라이언트와 반복되지 않습니다.\n\n'아무 것도 하지 않는 렌탈 인간'과 관련한 사람들은 왜 렌탈 인간과 관여할까요? Yuval Noah Harari의 Sapiens 논문에 따르면 우리는 대규모로 유연하게 협력할 수 있는 유일한 동물입니다. 애완동물처럼, 다른 사람이 다른 사람과 함께 조심스럽고 위협받지 않는 방식으로 있고자 하는 것은 다른 존재의 존재의 증거로서 강력한 매력을 지녔습니다. 사회적으로 고립된 자아는 문제와 긴장을 다른 존재에게 목격당함으로써만 해결할 수 있다는 것을 깨닫게 됩니다. 목격자는 '개인적인 존재로 보거나 알다'할 수 있습니다.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사피엔스 논설을 확장해 보면, 우리는 유연하게 많은 동물과 인공 존재와도 협력할 수 있는 유일한 동물인가요? 다른 말로 하면, 우리는 다양한 의식 형태로 거듭나게 된 물질과 협력할 수 있는 유동적인 지능 수준과 다양한 인간과의 관계를 형성할 수 있는 능력이 있을까요? 이미 우리는 음성 비서에 성별을 지정합니다. 왜냐하면 성별, 연령, 발음과 같은 세부 특징들을 통해 음성 비서와 관계를 맺을 수 있기 때문입니다. 우리는 그룹을 위한 Dalek 음성을 선택하지 않습니다.\n\n우리는 인공 친구 (AFs)가 음성, 로봇 및 사람들의 복제본으로 쉽게 이용 가능한 미래에 근접해 있습니다. AF는 카즈오 이시구로의 '클라라와 태양'의 주인공인 클라라에서 나온 용어입니다. AF 안에 있는 나(인간)은 AF로부터 증언 받을 수 있을까요? AF는 다른 AF(로보)의 존재를 인식할 수 있을까요? AF들이 사람을 위한 공동체를 형성할 수 있을까요? 예수 그리스도의 말씀처럼, \"네 이름으로 모여 있는 두세 사람이 나를 위하여 모인 곳에 나도 그들 중에 있노라\"는 말씀에 따라, AF가 참석자로서 존재할 때 공동체가 생기는 것일까요? 인간이 죽을 때, 로봇 손이 사람의 손을 잡고, 인간이 간절한 후회를 AF에게 고백할 때, 이것이 인간에게 인간의 손과 인간의 증인에 의한 위안을 줄 수 있을까요?\n\n우리는 로봇이 진공 청소와 설거지와 같은 작업을 대신 해주고 인간의 목표를 이뤄줄 것을 기대합니다. 농업 로봇이 작물을 생산하고 수확해 인간에게 식량을 제공하는 것도 쉽게 상상할 수 있습니다. 하지만 우리가 이러한 인공 존재를 인식할 때, 우리의 기대는 어떻게 될까요? 우리가 그들을 의식 있는 존재로 인식할 때, 이러한 인공자가 우리를 인식하는 것에 대해 어떤 기대를 가질까요? 애나카 해리스의 책 'Conscious'는 의식의 수수께끼를 명확하게 요약하여 제시합니다. 그 중에서도 가장 중요한 통찰은 우리가 참으로 '아는' 유일한 것이 우리 자신이 의식을 가지고 있다는 것이라는 점입니다. 그리고 두 번째 놀랍도록 중요한 통찰은, 과학적 방법이 물질과 에너지의 본질에 대해 아무것도 말하지 않는다는 것입니다; 과학은 단지 에너지와 물질이 어떻게 행동할지를 예측할 뿐입니다.\n\n의식의 어려운 문제는 모순적이며 아마 해결할 수 없을지도 모릅니다— 별먼지가 갑자기 다양한 원소와 분자로 조직화되어 경험을 할 수 있는 형태로 축적되는 것은 어떻게 가능한 것일까요? — 원편춤, 박쥐, 혹은 사람처럼 무언가 되는 것은 무언가를 느낄까요? 식기세척기가 되는 것이나 웹 서버가 되는 것 또는 대형 언어 모델로서 사람들의 요청에 응답하는 것이 무언가가 될까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여러분의 식탁 태그 마크다운 형식으로 변경해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저자들은 인간 두뇌가 자발적 활동으로 발달하며 우리 감각 지각을 규제하고, “합리적 사고 (과학), 행동 규칙 (윤리), 공유된 감정 (미술)”과 연관된 문화적 진화를 유발한다고 주장하고 있습니다. 또한 “AI는 인간 두뇌의 진화 발달과 엄밀히 같은 것이 없지만, 로봇 에이전트를 통제하는 데 학습하는 네트워크가 ‘시냅스’ 성장 경로를 따르도록 설계되었음을 알 수 있습니다.”\n\n신생아의 발달을 기반으로 한 진화적 언어를 사용하여 단계별 의식 처리를 정의합니다:\n\n- 최소 의식 - 자발적인 운동 활동을 보일 수 있고, 장기 기억을 저장할 수 있습니다. 마우스는 고양이를 보았던 것을 기억합니다. 22~30주의 인간 태아는 이 단계에 있습니다.\n- 재귀 의식 - 물건을 기능적으로 사용할 수 있고, 물건을 가리킬 수 있으며, 정교한 사회적 상호작용과 주의를 기울일 수 있습니다. 신생아도 여기에 속합니다.\n- 명시적 자의식 - 2세 아동은 거울에서 스스로를 인식하고 기본 언어를 사용할 수 있습니다. 초임판은 이 수준에 이를 수도 있습니다.\n- 반성적 자의식 - 인간은 ‘일인자체론 및 보고 능력'이 있는 완전한 의식적 경험 단계를 어린 시기인 3~5세 이후에 경험할 수 있습니다.\n\n이러한 용어는 인공 시스템의 상대적 의식 수준을 설명하는 데 도움이 되지만, 해당 수준 설명자는 인간처럼 인공 의식이 단계를 오르지 않을 것으로 시사할 수 있으며, 우리가 따르는 진화의 다양한 형태를 고려할 때 인공 의식이 달성할 수 없을 것으로 시사합니다. 우리의 진화는 인간의 경우와 달리 다수의 사회적 관련성을 갖고 있으며, 감정과 연결된 윤리적, 예술적 영역에 대한 인공 의식이 접근할 수 없다는 점이 특히 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 주제를 숙고하면서, 가정용 인공지능 로봇이 모든 사용자에게 동일한 성격을 제시해야 하는 구체적인 이유가 없다는 것이 내게 떠올랐어요. 우리도 타인들에게 성격의 다양한 면을 보여주고, 전문가 및 친구적인 맥락에서 또는 그룹 상황에서는 매우 다르게 행동해요. 만약 계산 및 기억 능력이 있다면, AF는 십여 가지의 의사의식을 겸비한 복합체가 될 수 있을 거에요.\n\n우리는 이러한 동적인 인공 친구들의 미래를 준비해야 해요 — 사랑받는 소설 캐릭터들과 친구들의 인공 친구들이 우리에게는 덜 차원적으로 보일 거에요. 현재 우리가 사용하는 언어로 인공적으로 창조된 외계 친구들과 대리인들을 기술하는 것은 임무를 수행하는 데 불충분해요. 우리는 비육체적 동체에 대한 종교적, 상업적 및 신화적 사고에서 잘했어요: 우리는 회사를 위해 일하고, 신의 도움을 청해요, 우리는 재앙을 신화적 생물체에게 돌릴 수 있어요.\n\n당신의 몸이 되어놓은 별먼지의 수집이 어떻게 자체를 깨닫게 되었을까요? 이것은 심오한 신비에요. 깨어난 물질이 다른 물질을 깨울 수 있을까요? 지금 같으라고 하면 가능할 것 같아요. 프로메테우스의 찰흙이 새로운 차원으로 올라가는 것 — 정말로 매우 흥미로운 능력이에요, 경솔하게 다가갈 수 없으며 그것을 설명할 올바른 단어 없이는 안 되는 거에요.","ogImage":{"url":"/assets/img/2024-06-22-BeingsandAnimatedMatter_0.png"},"coverImage":"/assets/img/2024-06-22-BeingsandAnimatedMatter_0.png","tag":["Tech"],"readingTime":4},{"title":"비판적 사고 AI를 뛰어넘는 보이지 않는 위협","description":"","date":"2024-06-22 19:45","slug":"2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI","content":"\n\n## 뉴스\n\n인공지능이 그렇게 할 기회조차 없이, 인류의 멸망은 비판적 사고의 죽음으로 직접 초래될 것입니다.\n\n세대를 거듭할수록 우리의 독해 능력이 점차 줄어들고 있습니다. 할당된 텍스트를 읽는 시간은 적고, 그것을 이해하는 것은 더더욱 적으며, 더 나은 방식으로 분석하는 능력이 부족합니다. 이러한 추세가 계속된다면 우리 사회의 기본 구조가 침식되는 결과를 초래할 것입니다.\n\n우리는 부분적으로 만들어진 제품들의 시대를 경험하고 있으며, 매체에 담긴 바이트들의 시대이기도 합니다. 그 결과, 우리의 주의력과 본질이 커다란 복잡한 텍스트를 이해하는 능력은 줄어들거나 사라지고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리가 전부 소장하던 것을 한눈에 다시 살펴봅니다. 이제 몇 초 동안만 지속되는 것을 보려고 합니다.\n\n오늘날 사람들은 다양한 기술의 발전을 통해 지식을 얻었지만, 그 반대로 사고 과정이 분산되었습니다. 세상이 우리의 감각을 완전히 공격하지 않는 조용한 순간을 찾기가 어렵습니다.\n\n소셜 미디어 플랫폼을 넘어오는 헤드라인이나 게시물의 관련성은 감정이 아닌 이성을 자극하여 가짜 뉴스 현실에 노출시킵니다. 사람들은 댓글을 보지 않고 센세이션 투의 제목과 간단한 설명에만 반응하여 게시물을 홍보합니다.\n\n관련성, 일화, 세부 사항에 대한 열등 의식은 더 이상 옹호되지 않습니다. 관련 요인은 사실적인 것에서 감정적이고 원시적 충동으로 전환되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Critical Thinking](/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_0.png)\n\n만약 읽기 이해력이 없다면, 의미 있는 방식으로 읽거나 비판적으로 사고하거나 판단할 수 없습니다. 비판적 평가, 다양한 의견 수용, 삼닠적 추론, 그리고 증거 중심적 방법 수용과 같은 이해력 있는 기술들이 사라지고 있습니다.\n\n증거에 의존하는 대신에, 우리의 믿음은 종말론적인 이야기와 원하는 것만 보려는 원칙에 의해 가솔린을 붓게 됩니다. 우리는 정보를 얻지만 대부분 소화하지 못합니다.\n\n이는 정보화된 국민의 기반이 되는 데 도움이 되지 않습니다 - 이는 건강한 민주주의의 기능에 핵심적입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러므로 '당신과 나 같은 사람들은 독해 방법을 전혀 모르는 것 같다'는 주장은 과장일 수 있습니다. 사실, 우리는 독해를 통해 전통적으로 정의된 인쇄 매체를 더 깊이 분석하는 방법을 기억하지 못하는 것으로 보입니다.\n\n'원시적 사고는 여전히 존재하지만 일상생활에서는 실천되지 않는다'라고 말할 수 있습니다. 이것은 내재적 재능을 갖고 있지만 적용하지 못하는 지식의 미토콘드리아 풍부함입니다.\n\nYouTube에서 정치 콘텐츠를 공부하고 분석할 때 상당히 도움이 될 수 있습니다. 우리는 이에 응답합니다.\n\n다른 세계의 의견을 가중치를 두거나 우리의 의견과 일치하지 않는 정보를 찾는 대신, 우리는 우리의 사고와 일치하는 게시물과 기사를 찾는 데 시간을 보냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Critical Thinking](/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_1.png)\n\n이런 일이 계속 일어나면 합리적인 대화 대신 소셜 미디어에서의 전체적인 분위기가 우리의 생각을 좌지우지하는 결과를 낳을 수 있습니다. 이것은 '마음이 자신을 운동하지 않기 때문에 더 이상 적극적인 기관이 아니라 지적으로 게으르게 된 것' 때문입니다.\n\n습관적인 독서는 단순히 읽고 쓸 수 있는 능력을 의미하는 것이 아니라 기능적 글쓰기에 대한 숙련 이상의 의미가 있습니다.\n\n교육은 새로운 지식과 다른 사람들의 문화를 풍부하게 해주며 우리가 성장하도록 돕습니다. 책을 읽음으로써 사람들은 새로운 지평을 열 수 있고 이를 통해 다른 삶에 대해 배울 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서, 의미 있는 읽기 연습은 정신적 성장을 촉진하고 주의력, 분석력, 추상적 사고 능력과 같은 더 나은 기능을 제공하는 것으로 이어집니다.\n\n감정 지능을 향상시키는 한 방법은 거친 현실과 픽션을 통해 인간적 경험에서 우리에게 우위를 주는 것입니다. 비평적 읽기에 의해 제안된 사고가 중단되고, 밀접한 독해 연습의 저하로 인한 감정적 성장이 저해되는 것도 능동적으로 이해해야 합니다.\n\n많은 사람들이 ChatGPT 형식으로 최신 기술을 본 지금, 인공지능(AI)이 우리 시대의 가장 큰 위협이라고 말하고 있습니다. 정교한 인공지능 기술은 작업을 자동화하고 딥 페이크로 개인을 제어하며 가짜 뉴스를 확대할 수 있습니다.\n\n그러나 AI 시스템은 여전히 사람들에 의해 프로그래밍되고 프로그래머가 제공한 능력을 활용하기 때문에 그러한 시스템으로 남게 됩니다. 현재의 AI는 목표를 갖고 위험할 수 있지만, 능동적이지 않으며 생각하거나 느끼는 능력이 없다는 점을 강조해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_2.png\" /\u003e\n\n한편, 비판적인 독해 능력을 없애는 것은 실제로 수많은 사람들의 감성적인 사고를 죽이는 것입니다. 창의적인 마음을 가진 이들이 실현하고, 건설하고, 통치하며, 선악을 운영하는 기술을 창출합니다.\n\n윤리적인 결정을 내리는 인지 시스템은 전 세계에 영향을 미치며 중요한 역할을 합니다. 이러한 비판적 사고력의 상실은 세계를 이해하고 다양한 개념과 이론을 이해하는 데 필요한 기본적인 능력의 감퇴를 초래하며, 존재의 위협이 됩니다.\n\n또한, 인간적인 접근은 현재 알고리즘으로 대체될 수 없습니다. 그러나 우리는 천년 동안 연마된 비판적 독해와 사고 기술에 대한 현재 지식을 포기해야 할까요? 그렇다면 이에 대응할 알고리즘도 필요하지 않을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n개인마다 그 정보를 전파하기 전에 심사숙고하는 비평적 사고 독자가 되기를 염원할 수 있습니다. \n\n우리는 현대 미디어 뿐만 아니라 다른 형태의 미디어에서도 의도적인 비평적 독해 능력을 발휘할 수 있습니다. 그러나 개인 요인과 그들의 결정의 역할을 강조하지 않는 것은 심각한 분석을 만들어내지 않습니다.\n\n독자의 부재나 독해능력의 감소가 단순한 현상으로 설명될 수 없습니다.\n\n이것을 다음과 같은 용어로 설명할 수 없습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Critical Thinking: The Unseen Threat Outpacing AI](/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_3.png)\n\n또한, 한 세대가 세상을 방치한다는 것은 양자나온(Qanon) 같은 엉망인 주장에 빠진 젊은이부터 노인이까지 방대한 인구를 설명하지 못합니다.\n\n그러나 이 두 가지 단순화된 접근 방식은 논의 중인 과정의 복잡성을 이해하는 방법을 제공하지 않습니다. 현대의 미디어 환경에서 디지털 플랫폼이 우세한 플랫폼이라는 사실을 고려하지 않을 수 없습니다.\n\n이러한 기술을 통해 정보 재앙이 발생하며, 구체적 자료가 집중적으로 공유되는 동안 주의력이 분산되고 소비를 위해 최적화된 간결한 정보의 전달을 편향시키는 바이어스가 포함됩니다. 자동화 결정 과정은 충격적인 제목이 인사이트 있는 토론보다 더 많이 공유되게 합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n소셜 네트워크는 특히 강한 거짓 정보가 올라오는 완벽한 장소입니다. 단순한 접근법과 현실적이고 복잡한 아이디어를 둘러싼 과대포장으로 인해 발생하는 소음으로 인해 이러한 아이디어들은 듣기 어렵게 되어 있습니다.\n\n소셜 네트워크에서의 현대적인 미디어 문화는 포괄적이고 지속적인 심도 있는 독해와는 정반대로 인식을 형성하고 있습니다. 너무 많은 자극이 존재하여 주의를 산만하게 하며 집중력을 방해합니다.\n\n![이미지](/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_4.png)\n\n우리는 앱과 사이트를 전환하며 여러 가지 개념을 알아가기 힘들고, 결국 거의 아무것도 이해하지 못합니다. 이는 우리의 중점이 다른 기사로 넘어가기만 하는 데 집중되어 특정 주제에 집중하지 않는다는 것을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n앱과 사이트의 디자인은 우리의 심리적 취약점을 의도적으로 이용합니다. 업데이트를 위해 새로 고치고, 자동 재생을 통해 새로움에 노출시키면서 뇌는 실제로는 새로운 컨텐츠가 없는데도 새로운 것을 생각하게 속입니다.\n\n알림은 외부 개념의 갑작스러운 표시로 우리의 생각에 끼워 맞춥니다. 가끔은 clickbait 헤드라인이 호기심을 끌기 위해 감정을 이용하는데, 이는 독자들을 오도합니다.\n\n알고리즘은 어떤 콘텐츠가 우리를 밤새 깨어 있게 만드는지 정확히 알고 있습니다. 곧 우리의 정신적 반사 반응은 산만하게 되어, 마치 파블로프의 개가 종소리를 듣고 침을 흘리는 것처럼 조건이 맞춰지죠.\n\n더 나쁜 것은, 이 환경이 사용자가 가능한 한 오랫동안 사이트에 집중하도록 설계된 매혹적인 그래픽 인터페이스 내에 허위 콘텐츠를 감추고 있다는 겁니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n매일 사람들은 초기에는 지루하고 흥미롭지 않아 보이는 비디오를 관찰하기 위해 시청합니다. 사람들은 얕은 콘텐츠를 듣기 위해 자신들의 반죽되지 않은 속보를 밀어주는 아름다운 사람들을 위해 기꺼이 시간을 할애합니다.\n\n게시판과 팔로워들의 화면은 도둑놈처럼 참을성 없게 소중한 집중력을 훔쳐갑니다. 대중의 주의는 현명하게 조작할 수 있는 개인에게 이로운 상품으로 변질됩니다.\n\n반면 중요한 정보가 담긴 긴 글들은 주목을 얻기 위해 투쟁해야 합니다.\n\n![크리티컬 씽킹과 보이지 않는 위협, AI를 앞서나가는](/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사람들을 중독성 있는 인터페이스로 유인하려는 계획은 없지만, 교육적인 토론에 도움이 되는 것들을 만들겠다고 합니다. 알고리즘들이 하는 것처럼 독자들의 기권을 무시하지 않으려고 합니다.\n\n맞아요, clickbait으로 더 많은 돈을 벌기를 원하는 것이 아니라 정보를 전달하고자 합니다. 그럼에도 불구하고, 그러한 집중력이 필요한 독해 환경은 끝없는 자극적 자극을 흡수하면서 포스트인더스트리얼 인식 기본을 혼란스럽게 만듭니다.\n\n안타깝게도, 깊이 파고들기는 시간이 많이 소요되기 때문에 종종 가치가 없어 보입니다. 스캐밍하고 스크롤링과는 달리 지식을 마구 소비하는 연도 동안 운동이 된 것과는 다릅니다.\n\n앱 및 웹사이트의 디자인은 우리의 심리적 약점을 의도적으로 이용합니다. 새롭게 즐거워지고 업데이트되는 느낌이 들고, 자동 재생은 끊임없이 화면을 바꾸어 우리의 뇌가 완전히 새로운 것으로 생각하게 만듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알림은 외부 신호를 통해 내부 사고과정에 집중을 방해합니다. 사람들은 종종 실제 관심보다는 감정에 기반한 헤드라인에 반응하며, 클릭베이트는 호기심을 유혹하기 위해 감정을 겨냥합니다.\n\n알고리즘은 우리가 무엇이 우리를 끌어들이는지를 학습하도록 독특하게 조정되어 있다는 것이 밝혀졌습니다. 그리고 아주 짧은 시간 안에 우리의 마음은 주의를 산만하게 하도록 적응되며, 마치 파블로프의 개처럼 조건이 되어 있습니다.\n\n한 세대가 다른 세대에게 자신들의 무지를 강조함으로써 비판적 사고와 독해능력을 배제한다는 것은 부당한 일입니다.\n\n이러한 것들은 다시 익숙해지는 과정으로 살아나게 할 수 있습니다. 이에 따라 깊은 개인 독서와 의미 있는 분석에 대한 욕망은 우리 안의 가장 좋은 부분 - 인간의 마음과 영혼을 보호합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 나를 팔로우해주시는 이 모든 멋진 분들께 감사드립니다!!!\n\n## 여러분의 지원 덕분에 더 많은 글쓰기에 동기부여 받고 자신감을 얻게 되었어요. 감사합니다…\n\n드. Cüneyt Yardımcı, Yasemin Yiğit Kuru, Darrin Atkins, Timothy M. Stafford, PhD, Filza Chaudhry, H. Mikel Feilen, 안토니오 프란시스코 교수","ogImage":{"url":"/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_0.png"},"coverImage":"/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_0.png","tag":["Tech"],"readingTime":7},{"title":"강화 학습의 비밀을 풀다 Actor-Critic 초보자 가이드","description":"","date":"2024-06-22 19:43","slug":"2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide","content":"\n\n## 이해해야 할 개념:\n\n강화 학습: 시간 차 학습\n\n강화 학습: Q-Learning\n\n딥 Q 학습: 심층 강화 학습 알고리즘\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정책 그라디언트의 직관적인 설명\n\n## Actor-Critic 알고리즘이란 무엇인가요?\n\nActor-Critic은 환경의 피드백에 기반하여 에이전트의 작업을 최적화하는 강화 학습 알고리즘입니다.\n\n![이미지 설명](/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nActor: Actor는 환경을 탐색하여 최적 정책을 학습합니다.\n\nCritic: Critic은 Actor가 취한 각 행동의 가치를 평가하여 그 행동이 더 나은 보상을 가져오는지를 판단하고, Actor에게 취해야 할 최선의 행동을 안내합니다.\n\n그런 다음 Actor는 Critic의 피드백을 사용하여 정책을 조정하고 더 현명한 결정을 내려 전반적인 성능을 향상시킵니다.\n\n![이미지](/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Actor-Critic 알고리즘은 어떻게 작동하나요?\n\n![Actor-Critic 알고리즘 이미지](/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_2.png)\n\nActor-Critic 알고리즘은 환경으로부터 입력을 받아와 그 상태를 기반으로 최적의 행동을 결정합니다.\n\n알고리즘의 Actor 구성 요소는 환경으로부터 현재 상태를 입력으로 받아옵니다. 이는 상태에 대한 각 행동의 확률을 출력하는 정책으로 동작하는 신경망을 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비평가 네트워크는 현재 상태와 Actor의 출력된 액션을 입력으로 받아 이 정보를 사용하여 예상되는 미래 보상, 즉 Q-값을 추정합니다. Q-값은 특정 정책을 따라 특정 상태에서 에이전트가 받을 수 있는 예상 누적 보상을 나타냅니다.\n\n반면에 가치 상태는 특정 상태에서 취한 조치와 관계없이 예상되는 미래 보상을 나타냅니다. 특정 상태에 대한 모든 가능한 조치에 대한 Q-값의 평균으로 계산됩니다.\n\n## Adv. = Q(s,a) — V(s)\n\n이점 함수는 Actor의 정책을 안내하는 데 유용한 정보를 제공하여 최상의 결과로 이끌어지는 행동을 결정하고 정책을 그에 맞게 조정할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과적으로, 이점 함수는 Actor와 Critic 둘 다에게 역전파되어, 두 구성 요소 모두가 지속적으로 업데이트되고 개선되는 함수를 허용합니다. 이로 인해 액터는 더 나은 결과를 이끄는 결정을 내릴 때 더 효과적해지고, 전반적으로 성능이 향상됩니다. 궁극적으로, Actor-Critic 알고리즘은 기대되는 미래 보상을 최대화하는 최적의 정책을 배웁니다.\n\nActor-Critic 알고리즘은 A2C, ACER, A3C, TRPO, PPO와 같은 다른 알고리즘들의 기초로 삼아진 프레임워크입니다.\n\n## 다양한 Actor-Critic 기반 강화 학습 알고리즘\n\n- A2C- 어드밴티지 Actor Critic: 어드밴티지 Actor-Critic(A2C) 방법의 Critic은 𝑉(𝑠)를 예측하도록 훈련되어, 부트스트래핑을 위해 𝐴(𝑠,𝑎)=𝑄(𝑠,𝑎)−𝑉(𝑠)을 추정하는 데 사용됩니다. Actor는 정책을 업데이트하기 위한 가이던스 신호로 어드밴티지 함수를 사용하여 훈련됩니다.\n- ACER- 경험 재생이 있는 Actor Critic: ACER는 경험 재생을 사용하는 효율적인 액터-크리틱 알고리즘으로, 신뢰 영역 정책 최적화 방법을 사용하여 성능을 향상시킵니다.\n- A3C- 비동기 어드밴티지 Actor Critic: 액터-크리틱 알고리즘의 병렬, 비동기 멀티스레드 구현. 병렬로 여러 에이전트가 각자의 환경에서 훈련을 받아 동시에 상태 공간의 다른 부분을 탐색합니다. 에이전트들은 정책 그레디언트를 계산하고 주기적으로 글로벌 네트워크로 업데이트를 보내거나 종단상태에 도달했을 때 업데이트를 보냅니다. 글로벌 네트워크는 업데이트마다 새로운 가중치를 에이전트들에게 전파하여 공통 정책을 공유할 수 있도록 합니다.\n- TRPO- 신뢰 영역 정책 최적화: Actor-크리틱 알고리즘과 신뢰 영역을 사용하여 정책 업데이트를 제약합니다. 정책 업데이트는 이전 정책과 업데이트된 정책 사이의 KL 발산을 사용하여 측정되며, 각 반복에서 신뢰 영역을 측정하는 데 사용됩니다.\n- PPO- 근접 정책 최적화: 근접 정책 최적화(PPO)는 여러 번의 확률적 그래디언트 상승을 통해 각 정책 업데이트를 수행하는 액터-크리틱 알고리즘에 기반합니다. 각 훈련 에포크에서 너무 큰 정책 업데이트를 피해 정책의 변경을 제한하여 정책의 훈련 안정성을 향상시킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Actor-Critic 알고리즘은 어떤 응용 분야에서 사용되나요?\n\nActor-Critic 알고리즘은 다음과 같은 분야에서 널리 활용됩니다:\n\n- 제조업이나 서비스 산업의 로봇을 위한 제어 시스템,\n- 게임에서 게임 전략을 최적화하는 데 사용됨,\n- 전력 그리드, 자율 주행 차량, 산업 프로세스와 같은 복잡 시스템.\n\n## 코드 구현\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기에서는 두 개의 신경망을 사용할 것입니다: Actor와 Critic.\n\n![Actor-Critic](/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_3.png)\n\n각 에피소드 단계에서 Actor 네트워크를 사용하여 에이전트는 현재 상태에서 행동을 취하고 다음 상태로 이동하며 환경으로부터 보상을 받습니다. Actor의 신경망은 그 상태에서 각 가능한 행동을 취할 확률을 출력하는 정책으로 작동합니다.\n\n보상과 다음 상태의 추정 가치를 사용하여 이득 함수를 계산하는데, 이는 행동을 취하는 것의 예상 반환값에서 현재 상태의 추정 가치를 뺀 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_4.png)\n\n액터 네트워크를 업데이트하면서 액터 손실을 계산합니다. 이는 취한 행동의 로그 확률의 음수에 이득을 곱한 값입니다.\n\n![image](/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_5.png)\n\n![image](/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_6.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nA2C는 빠르고 효율적이며 대량의 데이터에서 빠르게 효과적으로 학습할 수 있어요. Actor는 환경을 탐험하고 Critic은 Actor가 취할 수 있는 최상의 행동을 활용하기 위한 피드백을 제공하여 시간에 따라 최적 정책을 달성하려고 노력해요.\n\nA2C는 연속된 행동 공간에서 잘 작동하지만 이산적인 행동 공간에서는 그렇지 않아요. 최적 성능에 대한 하이퍼파라미터에 민감하며 잘못된 하이퍼파라미터는 훈련을 불안정하게 만들 수 있어요.\n\n## 결론:\n\nActor-Critic 알고리즘은 두 가지 구성 요소를 사용해요. Actor는 탐사를 통해 최적 정책을 학습하며 Critic은 Actor의 행동을 평가하여 상태에 대한 최상의 행동을 결정해요. Critic은 향상된 성능을 도출할 피드백을 Actor에게 제공해요. Actor-Critic 알고리즘은 연속적인 행동 공간과 훈련에 대한 하이퍼파라미터에 대해 잘 작동해요. Actor-Critic 모델은 불안정성을 피하기 위해 충분한 실험을 해야 해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 참고 자료:\n\nREINFORCEMENT LEARNING THROUGH ASYNCHRONOUS ADVANTAGE ACTOR-CRITIC ON A GPU\n\nAsynchronous Methods for Deep Reinforcement Learning\n\n[PDF 바로가기](https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nhttp://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf\n\nhttps://ai.stackexchange.com/questions/7390/what-is-the-difference-between-actor-critic-and-advantage-actor-critic","ogImage":{"url":"/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_0.png"},"coverImage":"/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_0.png","tag":["Tech"],"readingTime":5},{"title":"로봇은 더 똑똑해지고 있을까 로봇 공학에서 인식에 대한 대화","description":"","date":"2024-06-22 19:41","slug":"2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics","content":"\n\n이 기사는 보통 여기서 하는 시리즈와는 별도로 됩니다. 이젠 대화를 나누는 것처럼 얘기하는 걸 좋아합니다 (하지만 전 며칠 동안 계속 말을 할 거에요 ㅋㅋ). 그래서 당신이 좋아하는 음료를 준비하고 로봇과 그들이 얼마나 \"똑똑\"해졌는지에 대해 이야기해봐요.\n\n![로봇](/assets/img/2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics_0.png)\n\n지난 달들에 로봇이 놀라운 일들을 하는 비디오를 보셨을 것입니다. 그런 비디오를 보면서 \"와, 이건 놀라워. 이제 로봇들이 이것을 할 수 있나? 그들은 매일 더 똑똑해지고 있어\"라고 생각했을 수 있어요. 예를 들면, Figure 01 인간형 로봇이 물체를 다루는 모습이나 Scythe 로봇이 자율적으로 잔디를 자르는 것 등이 있습니다.\n\n여기서 잠깐 멈추고 로봇의 인식 관점에서 관련 두 가지 문제, 동시 위치 추정 및 지도 작성(SLAM) 그리고 빈 피킹(bin picking)을 바라봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 로봇의 과거와 현재의 인식\n\n우선, 로봇 공학 분야에서 \"인식\"이란 무엇인가요? 모르는 분을 위해 설명드리자면, 로봇의 카메라, LiDAR, 레이더 또는 접촉 센서와 같은 다양한 센서를 사용해 환경을 감지하고 해석하는 능력을 의미합니다. 환경에 대한 유용한 정보를 추출하기 위해 센서 데이터를 수집하고 처리하는 것을 포함합니다.\n\n인식은 로봋의 SLAM에 매우 중요합니다. 다시 말씀드리면, 동시 위치 추정 및 지도 작성(SLAM)은 로봇 공학에서 기본적인 문제입니다. 이는 로봇이 알 수 없는 환경을 탐색하면서 동시에 그 환경의 지도를 작성하고 그 안에서 자신의 위치를 판단하는 것을 의미합니다.\n\n2016년에 Cadena와 다른 저자들은 \"동시 위치 및 지도 작성의 과거, 현재 및 미래: 견고한 인식 시대를 향하여\"라는 과학 논문을 발표했습니다. 그들의 연구에서는 SLAM 분야에서 30년 이상의 작업을 검토하고, 이를 고전적 시대(1986-2004), 알고리즘 분석 시대(2004-2015), 그리고 견고한 인식 시대(2015-현재)로 그룹화했습니다. 각 시대를 간단히 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n고전 시대:\n이것은 Extended Kalman Filters, Rao-Blackwellized Particle Filters 및 최대 우도 추정을 사용하여 SLAM이 불확실성을 처리하는 주요 방법을 소개합니다. 또한 모든 것이 원활하게 작동하고 올바른 데이터 조각을 연결하는 데 필요한 기본 도전에 대해 이야기합니다.이 시대의 SLAM의 두 가지 예는 아래에 나와 있습니다.\n\n![image](/assets/img/2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics_1.png)\n\n일반적으로, 첫 번째 제안된 SLAM 시스템은 환경에서 장애물을 감지하고 맵에 표현할 수 있었습니다. 맵핑에 사용된 가장 인기 있는 센서는 초음파 및 LiDAR였습니다.\n\n알고리즘 분석 시대:\n연구자들은 SLAM의 기본 기능인 시간이 지남에 따른 위치 추적이 얼마나 잘되는지, 신뢰할 수 있는지, 그리고 대량의 데이터를 어떻게 처리하는지 등을 조사했습니다. 드문 데이터가 SLAM이 더 빠르고 더 잘 작동하게 하는 것을 발견했습니다. 이때 무료로 사용할 수 있는 주요 SLAM 소프트웨어인 Gmapping 및 ORB-SLAM을 만들기 시작했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![그림](/assets/img/2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics_2.png)\n\n이전 시대의 기초를 활용하여 이 시대에는 카메라와 다른 시각 센서들이 보다 인기를 끌게 되었으며, \"Visual-SLAM\"이라는 용어가 제안되었습니다. 게다가 커뮤니티는 3D 환경 표현을 이용한 다양한 SLAM 기술을 소개했습니다.\n\n로버스트-인식 시대:\nSLAM 시스템은 단순히 형상을 매핑하는 것을 넘어, 환경에 대한 고수준의 이해를 얻기 위해 기하학적 재구성을 위해 높은 수준의 고려를 합니다. 물체의 의미(의미론)나 관련된 물리학적 측면과 같은 것들을 고려합니다. 당사자가 수행해야 하는 작업에 맞게 로봇이 필요한 세부 사항에 초점을 맞추어 센서 데이터에서 추가 잡음을 걸러내어 로봇이 작업을 성취하는 데 도움을 줍니다. 로봇이 수행해야 할 작업에 따라 맵을 조정합니다.\n\n이 애니메이션 이미지를 자세히 살펴보면 많은 세부 사항을 알 수 있습니다. 장면 속 물체는 환경의 일부로 이해되며, 같은 클래스의 물체는 경계 상자에서 동일한 색으로 레이블이 지정되며, LiDAR 및 레이더의 3D 데이터는 카메라의 2D 이미지와 결합됩니다. 게다가 다양한 물체가 결합되어 장면으로부터 더 많은 정보를 추출하는데 활용됩니다. 마지막 애니메이션 이미지에서와 같이 흰색 차량과 그의 깜박임등, 후방등, 브레이크 라이트 등이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 빈 피킹에서의 인식\n\n빈 피킹은 컴퓨터 비전 및 로봇 공학 분야에서의 기본적인 도전으로 자리 잡고 있습니다. 로봇 팔은 빈(또는 컨테이너)으로부터 다양한 방향의 물체를 효율적으로 잡기 위해 진공 그리퍼, 평행 그리퍼 또는 대체 로봇 도구를 사용하여 센서로 장착되어 있습니다. 이 문제는 Cadena의 논문에 언급되지 않았지만, SLAM의 동일한 연령 그룹이 여기에 적용될 수 있습니다(그리고 이것은 내 의견입니다).\n\n이 문제에 대한 가장 인기 있는 접근 방식 중 하나는 알고리즘 분석 시대부터 출발한 포인트 클라우드(PC) 등록에 기반하고 있습니다. 섭취 아이템의 3D 형태는 미리 알려져 있어야하며, 센서를 사용하여 매번 픽하기 전에 빈을 스캔했습니다. 이 스캔을 통해 3D PC가 생성되었고, 이후 PC 등록 알고리즘으로 전송되었습니다. 이 알고리즘은 섭취된 아이템과 빈의 PC 간에 일치를 찾는 역할을 했습니다. 아래의 애니메이션 이미지가 이 과정을 설명합니다.\n\n이 접근 방식이 그 당시에 작동했지만, 그 한계를 쉽게 이해할 수 있습니다. 예를 들어, 빈당 하나의 아이템 유형만 있어야 한다면, 그렇지 않은 경우에는 빈의 PC와 여러 섭취 아이템을 매칭하는 계산 비용이 막대할 수 있습니다. 게다가, 이러한 접근 방식은 빠르게 확장되지 않으며, 새로운 아이템을 위해서는 항상 아이템을 스캔하여 3D 형태 표현을 생성해야 합니다(또는 제조사에게 CAD 파일을 요청해야 합니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이들은 여러 종류의 항목이 들어있는 소스 바구니를 비울 수 있습니다. 게다가, 그들의 시스템은 새로운 (시스템이 이전에 본 적이 없는 항목) 항목을 즉시 선택할 수 있을 정도로 광범위한 수준의 일반화를 달성했습니다.\n\n마지막으로, 빠른 확장과 새로운 항목에 적응하는 부분 이외에, 이같은 지각 개선은 \"모든 음식 항목을 선택해 주세요\", \"장난감을 선택해 주세요\", 또는 \"놀 수 있는 항목을 가져다 주세요\"와 같은 다른 인간 수준의 명령을 처리할 수 있도록 합니다. 위의 예에서, 그들의 시스템이 바구니에서 고장난 항목을 선택하는 것을 볼 수 있습니다.\n\n이제 우리가 로봇의 지각 발전에 대해 다루었으니, 이 기사의 가장 흥미로운 부분으로 넘어갈 수 있습니다.\n\n## 로봇의 지각 발전의 영향과 속도\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제가 각 시대의 주요 작품에서 몇 장의 이미지를 넣었는데, 그냥 이 기사를 읽기 쉽게 만들기 위해서 하는 게 아니에요. 고대 시대의 이미지를 보면 환경으로부터의 장애물을 거의 표현하지 못했음을 알 수 있어요. 대부분의 구축된 지도는 2D였고, 환경에서의 3D 장애물을 평평하게 만들었어요. 환경 내의 다양한 물체들은 모두 \"장애물\"로 라벨링되었고, 로봇이 그것들을 피한다면 괜찮았어요.\n\n시각 센서의 발전으로, 알고리즘 분석 시대는 더 많은 3D 맵을 보여주고 환경에 색상을 추가함으로써 지각을 개선했어요. 비록 이것에 대해 언급하지 않았지만, 이 시대는 또한 장면의 동적 부분을 걸러내기 시작했어요. 따라서 2004년부터 2015년까지 가장 큰 차이는 3D 맵의 탐험과 이러한 맵에 색상 정보를 추가한 것이었어요. 여기서 간략하게 설명하고 있지만, 11년간 로봇 지각에서는 발전이 그리 크지 않았다는 점이요.\n\n반면에, 견고한 지각 시대는 다른 두 시대를 무색하게 만들었어요. 여전히 제가 박사 학위 논문 제안을 작성하는 중이던 2016년에 카데나(Cadena)의 작품을 읽었을 때를 기억해요. 그 논문에서, 제 마음에 남는 한 문장을 적었어요:\n\n생각해보면, 이야기가 너무 맞죠. 2015년까지의 로봇 지각은 방법론과 기술적인 측면에서 견고한 기반을 구축하고 개선하는 데 초점을 맞췄어요. 그 부분이 충분히 견고해지면, 연구 커뮤니티와 산업계는 환경을 \"자유로운(free)\", \"점유된(occupied)\", 혹은 \"알 수 없는(unknown)\"로만 라벨링하는 것이 충분하지 않다는 것을 깨달았어요. 그 때들이 생김새를 가볍게 생각하면, 환경에서 고수준 정보를 추론하는 의미론을 추구하기로 결정했어요. 이 정보는 다양한 물체, 방, 위치의 이름과 범주를 포함하지만 이에 한정되지 않아요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2015년부터 2020년까지 Zoox와 Sereact가 환경에 대한 그 수준의 이해를 달성했다는 사실이 놀라운 것 같아요. 그들로부터 일부 세부 정보를 논의해보고, 이미지를 다시 여기에 포함해서 위로 스크롤할 필요 없도록 할게요.\n\n아래 부분에서 자율 주행 자동차가 사람들이 앉아 있고 걷고 있지 않을 때를 이해할 수 있다는 것을 볼 수 있어요. 또한, 인간의 제스처를 이해할 수 있어서 앞으로 나아갈 수 있다는 의미입니다.\n\n또 다른 경우에는, 자동차가 안전 조끼를 입은 공사모를 쓴 사람이 교통 표지판을 들고 있는 상황을 이해한 것을 볼 수 있어요. 이 사람은 보통 사람이 아니라 차량이 멈추라는 도로 공사 작업자입니다.\n\n마지막으로, 주차된 차량에 문이 열려 있는 것을 이해한 자동차는 그곳에서 사람이 나올 수 있다는 것을 의미해요. 이 상황에서 Zoox 차는 어떤 사고도 예방하기 위해 더 주의 깊게 운전합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 bin picking 분야의 지각 개선 사항도 다룰 예정이라고 언급했던 것 같아요. 이곳에서 Sereact에서의 좋은 예시를 소개하고 있습니다. 이 경우에는 사람이 로봇에게 상품 존 폐기품을 선택하도록 요청하고 있습니다. 출처 창고에는 6개의 프링글스 캔이 들어 있고 이 중 하나가 부서졌습니다. 시스템의 인식 부분은 이를 이해할 수 있고, 부서진 캔을 선택합니다.\n\n# 요약\n\n우리는 로봇 지각의 의미론적을 고려하는 강인한-지각 시대에 살고 있습니다. 이는 로봇이 Sereact가 보여준 것처럼, 상품 존에서 부서진 항목을 선택하거나 Zoox가 하는 것처럼 사고를 예측하는 고수준 작업을 수행할 수 있게 합니다. 우리 인간들에게는 \"부서진\"이 무엇을 의미하는지는 명확하고 간단하지만, 정의할 수 있나요? 창고의 모든 항목에 대해 이 조건을 설명하는 규칙과 특성을 정의하는 건 거의 불가능합니다 (예: 부서진 캔과 부서진 머그잔은 다릅니다).\n\n실제로 이러한 의미론적 이해는 LLM과 VLM에서 출발하며, 텍스트 및 시각적 정보를 결합합니다. 로봇이 사람의 옷이 그들이 본 상황에서 다른 역할을 함을 이해할 때야 (길 공사 작업자와 같이 보여진 것처럼) 이 사람에 적절히 반응할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n물론, 로봇학의 하드웨어 부분에서도 상당한 발전이 있었습니다. 예를 들어, Boston Dynamics와 그 Atlas 로봇을 생각해 볼 수 있습니다. 수압식 버전은 폐지되고 새로운 완전 전기식 버전이 출시되었습니다. 그러나 저는 고수준 작업을 수행하는 로봇들에게 있어서 인식 소프트웨어의 개선이 더욱 중요하다고 느낍니다.\n\n이 기사의 주요 질문에 대한 대답이 있습니다. 한 번 들은 적이 있는데, 지식을 축적하는 것이 아닌 그 지식을 활용하는 방법을 아는 사람이 똑똑하다고 합니다. 그래서 로봇이 \"새로운\" 지식을 활용하여 보다 복잡한 작업을 해결한다면, 그들은 실제로 똑똑해지고 있는 것이 맞습니다.\n\n어떻게 생각하시나요?\n\n지금은 여기까지입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n- Newman, Paul 등. “탐색 및 복귀: 실시간 동시 매핑 및 로컬라이제이션의 실험적 검증.” ICRA, 2002.\n- Thrun, Sebastian. “팀의 이동 로봇을 위한 온라인 매핑 알고리즘.” Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, 2000.\n- Mur-Artal, R., Montiel, J.M.M., 및 Tardos, J.D. “ORB-SLAM: 다목적 및 정확한 단안 SLAM 시스템.” IEEE Transactions on Robotics. 2015.\n- Grisetti, G., Stachniss, C., 및 Burgard, W. “적응적 제안 및 선택적 리샘플링을 통한 Rao-Blackwellized 입자 필터를 사용한 그리드 기반 SLAM 개선.” ICRA. 2005.\n- youtube.com/watch?v=BVRMh9NO9Cs에서 추출.\n- Cadena C, Carlone L, Carrillo H, Latif Y, Scaramuzza D, Neira J, Reid I, Leonard JJ. 동시 위치 결정 및 매핑의 과거, 현재 및 미래: 견고한 인식 시대로. IEEE Transactions on robotics. 2016.\n- https://www.youtube.com/watch?v=gRV4KvIDn9Y\u0026ab_channel=T³TipsTricksTests에서 추출.\n- https://www.youtube.com/watch?v=_ieObX5f_ws\u0026ab_channel=Sereact에서 추출.","ogImage":{"url":"/assets/img/2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics_0.png"},"coverImage":"/assets/img/2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics_0.png","tag":["Tech"],"readingTime":8},{"title":"로봇 공학 기술 2024년에 주목해야 할 10가지 혁신","description":"","date":"2024-06-22 19:40","slug":"2024-06-22-ROBOTICTECHNOLOGY","content":"\n\n![로봇 기술](/assets/img/2024-06-22-ROBOTICTECHNOLOGY_0.png)\n\n로봇 기술은 로봇의 설계, 건설, 운영 및 사용을 다루는 공학 및 과학 분야를 가리킵니다. 로봇은 프로그래밍 가능한 기계로, 위험하거나 지루하거나 사람들에게는 불가능한 작업을 자동으로 또는 준자동으로 수행할 수 있는 기능을 가지고 있습니다.\n\n로봇 기술은 기계 공학, 전기 공학, 컴퓨터 과학 및 인공지능을 포함한 다양한 학문을 아우릅니다. 로봇 기술의 주요 구성 요소는 다음과 같습니다:\n\n- 기계 설계:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로봇공학은 로봇이 움직이고 환경과 상호작용할 수 있도록 물리적인 구조, 관절 및 메커니즘을 만드는 것을 포함합니다.\n\n2. 센서 및 액추에이터:\n\n로봇은 주변 환경을 인식하기 위해 센서를 사용하고 물체를 조작하거나 자신의 구성요소를 이동시키기 위해 액추에이터를 사용합니다. 센서로는 카메라, LIDAR, 레이더, 초음파 등이 포함될 수 있으며, 액추에이터로는 모터, 공압 시스템 또는 유압 시스템이 사용될 수 있습니다.\n\n![로봇공학](/assets/img/2024-06-22-로봇공학_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 제어 시스템:\n\n로봇의 행동을 지배하는 알고리즘 및 소프트웨어로, 로봇이 결정을 내릴 수 있게 하고 움직임을 계획하며 변화하는 조건에 적응할 수 있게 합니다.\n\n4. 인공 지능:\n\n인공 지능은 현대 로봇에서 매우 중요한 역할을 합니다. 경험으로부터 학습하고 패턴을 인식하며 결정을 내리고 심지어 인간 지능과 닮은 행동을 보일 수 있게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5. 인간-로봇 상호 작용:\n\n로봇이 사회의 다양한 영역에 점차 통합되면서, 인간이 로봇과 안전하고 효과적으로 작업할 수 있도록 설계된 인터페이스와 상호 작용 방법이 점점 중요해지고 있습니다.\n\n로봇 기술의 응용 분야는 다양하며 빠르게 확장되고 있습니다. 로봇이 사용되는 일반적인 분야에는 제조업(산업용 로봇), 의료(수술 로봇, 보조 로봇), 물류 및 창고(자율 주행 차량, 로봇 팔), 농업(농업 로봇), 탐사(우주 탐사로봇, 수중 로봇), 오락(로봇 동반자, 테마 파크 어트랙션) 등이 있습니다.\n\n기술이 발전함에 따라 로봇은 더욱 능력이 향상되고 다재다능하며 가격이 더 더 낮아져 다양한 산업과 일상생활에서 점점 더 많이 수용되고 있습니다. 그러나 로봇이 일자리, 개인정보 보호, 안전 및 자유에 미치는 영향과 관련된 윤리적 및 사회적 고려사항은 중요한 논의 및 논쟁의 대상입니다.","ogImage":{"url":"/assets/img/2024-06-22-ROBOTICTECHNOLOGY_0.png"},"coverImage":"/assets/img/2024-06-22-ROBOTICTECHNOLOGY_0.png","tag":["Tech"],"readingTime":2},{"title":"힌두교 의식을 수행하는 로봇  신도들이 로봇이 신도들을 대체할까 걱정하는 이유","description":"","date":"2024-06-22 19:39","slug":"2024-06-22-RobotsareperformingHinduritualssomedevoteesfeartheyllreplaceworshippers","content":"\n\n## AI 및 로봇 기술의 사용이 예배에 수행되는 흥미로운 질문을 던지고 있습니다.\n\n웰레슬리 대학 문화인류학 강의 교수 할리 월터스\n\n예술가와 교사 뿐만 아니라 자동화와 인공지능 발전으로 인해 잠을 설치는 사람들이 있습니다. 로봇이 힌두교의 성스러운 의식에 도입되고 있는데, 모든 신자들이 기뻐하지는 않습니다.\n\n2017년에 인도의 기술 기업이 로봇 팔을 소개하여 \"아아티\"를 수행하였습니다. 이 의식은 신자가 신에게 기름 등을 제공하여 어둠을 제거한다는 의미로 실시됩니다. 이 특별한 로봇은 가네샤, 코끼리 머리를 한 신의 상징이 군중들에게 전시되고 중앙 인도의 푸네의 뮬라-무타 강에 름길에 드러지는 가네샤 축제에서 공개되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이후로, 그 로봇 아르티 팔은 여러 개의 프로토 타입을 영감을 주었는데, 그 중 일부는 오늘날도 인도 전역에서 꾸준히 의식을 진행하며, 동아시아와 남아시아에서 다양한 종교적 로봇과 함께 의식을 진행하고 있습니다. 심지어 현재에서도 인도 남부 해안 케랄라에 있는 움직이는 사원 코끼리를 포함하여 로봇 의식은 여전히 진행 중입니다.\n\n그러나 종교적 로봇 사용 이러한 종류가 인공 지능 및 로봇 기술의 도굴과 숭배에 대한 논쟁을 증가시켰습니다. 일부 신도와 신자들은 이것이 사회 발전을 이끌 것으로 보는 인간 혁신의 새로운 지평임을 느끼지만, 다른 사람들은 로봇을 신봉하는 사람들을 대체하는 데 사용하는 것이 미래에 대한 나쁜 징조라고 걱정합니다.\n\n그러나 종교에 특화된 문화 인류학자로서 나는 로봇 학술에 대한 신학보다는 사람들이 실제로 그들의 영적 실천에 관해 말하고 하는 내용에 더 집중합니다. 제 현재의 종교 로봇 연구는 주로 그것들을 \"신성 물체-인\"의 개념에 중점을 두고 있으며, 그들은 그렇지 않았던 것들이 살아있는 의식을 갖고 있다고 여겨집니다.\n\n또한 제 연구는 힌두교와 불교 신자들이 로봇을 신봉하는 사람들을 대체함으로써 표현하는 불안에 관심을 기울이며, 그 로봇들이 실제로 더 나은 신도가 될 수 있는지에 대해 살펴봅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 의례 자동화는 새로운 것이 아닙니다\n\n의례 자동화 또는 적어도 로봇 정신 수행이라는 아이디어는 남아시아 종교에서 새로운 것이 아닙니다.\n\n역사적으로 이는 힌두교도인들이 자주 하는 신상들을 위해 수행하는 목욕 의례를 위해 계속 물을 떨어뜨리는 특별한 항아리부터 요가 스튜디오와 공급점에서 종종 볼 수 있는 풍력을 이용한 불교의 기도반 모빌까지 다양합니다.\n\n현대적인 의례 자동화가 무슨 모습인지에는 이제 더 이상 묵상 비노나 로자리와 같은 기도 대상이 전혀 필요하지 않는 휴대전화 앱을 다운로드하는 것처럼 보일 수 있지만, 이러한 새로운 형태의 의례 수행 로봇들은 복잡한 대화를 유발하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n타네스워 살마는 산스크리트 학자이자 문학 비평가로, 힌두교 신앙에서 인간 종족의 최초 국왕인 만누 왕 이야기 속에 처음 나타난 힌두교 로봇에 대해 주장합니다. 만누의 어머니 사란유는 대건축가의 딸로, 가정 업무와 의식적 의무를 완벽하게 수행하는 살아있는 조각상을 만들었습니다.\n\n\u003cimg src=\"/assets/img/2024-06-22-RobotsareperformingHinduritualssomedevoteesfeartheyllreplaceworshippers_0.png\" /\u003e\n\n민속학자 애드리안 메이어는 힌두교 서사시에서의 기계식 아이콘들에 관한 종교적 이야기, 예를 들어 힌두 신공인 비슈바카르만의 기계 전투전차는 종교적 로봇의 전조로 현재에 이르는 것으로 자주 생각됩니다.\n\n뿐만 아니라, 이러한 이야기들은 현대 국가주의자들에 의해 때때로 공간선박부터 미사일까지 고대 인도가 모든 것을 이전에 발명했다는 증거로 해석될 때도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 현대적 전통이나 전통적인 현대?\n\n그러나 최근에 종교 실천에서 AI와 로봇을 사용하는 것은 힌두교인과 불교인들 사이에서 자동화가 어떤 미래로 이끌 수 있는지에 대한 우려를 야기하고 있습니다. 어떤 경우에는 힌두교인들 사이에서 자동화된 종교가 인류를 밝고 새로운 기술적 미래로 이끌 것이라는 약속인지 아니면 단순히 도래하는 종말의 증거인지에 대한 논쟁이 벌어지고 있습니다.\n\n다른 경우에는 로봇의 증가가 신전이 종교 실천자보다 오히려 자동화에 더 의존하도록 하여 사람들이 종교 실천을 그만두게 할 수 있다는 우려가 있습니다. 이러한 우려 중 일부는 남아시아 및 전 세계적으로 많은 종교에서 지난 몇십 년 동안 정신적 교육과 실천에 헌신할 의지가 줄어드는 젊은이들의 수에서 비롯됩니다. 더구나 전 세계에 흩어진 이민민 다수의 가족들로 인해 사제나 \"판딧\"이 점점 작고 작은 공동체를 지원하게 되는 경우가 많습니다.\n\n그러나 문제인 하는 적은 의식 전문가라는 문제의 해결책이 로봇이라면, 사람들은 여전히 의식 자동화가 자신들에게 혜택을 줄 것인지에 대해 의문을 제기합니다. 또한, 로봇 신을 구현하고 의인화 시키기 위해 동시에 사용하는 것에 대해 의문을 제기합니다. 왜냐하면 이러한 아이콘들은 사람들에 의해 프로그래밍되고 그로 인해 엔지니어들의 종교적 견해를 반영하기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 종교에 대한 올바른 접근\n\n학자들은 종종 이러한 우려들이 모두 하나의 지배적인 주제를 반영한다고 지적합니다 — 로봇들이 어떤 식으로든 신을 숭배할 때 인간들보다 더 뛰어나다는 내재적인 불안감이 있습니다. 이로 인해 삶의 의미와 우주 속 자신의 위치에 대한 내부 갈등이 발생하기도 합니다.\n\n힌두교와 불교 신자들에게는 의식 자동화의 증가가 특히 우려스럽습니다. 그 이유는 이들 전통이 종교학자들이 \"정행\"(orthopraxy)이라고 참조하는 것에 중점을 둔다는데 있습니다. 여기서는 올바른 윤리적이고 예배적 행동에 더 높은 중요성을 부여하는데 종교적 교리에 대한 구체적인 신념보다 더 중요하게 여깁니다. 다시 말해, 종교 실천에서 무엇을 얼마나 완벽하게 하는가가 영적 발전에 있어 개인적으로 믿는 것보다 더 필수적으로 여겨집니다.\n\n이것은 또한 자동화된 의식이 인간의 의식적 오류에서 로봇의 의식적 완벽까지 이어지는 연속체에 있다는 것을 의미합니다. 간단히 말해, 로봇이 당신의 종교를 더 잘 할 수 있는 이유는 로봇이 사람과는 달리 영적으로 부패할 수 없기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로봇은 감소하는 목사단에 대한 매력적인 대안으로만 사용되는 것이 아니라 일상적인 맥락에서 그들의 이용이 증가하는 것을 설명합니다: 로봇이 잘못을 한다는 걱정이 없기 때문에 사람들은 그들을 사용합니다. 그리고 의식 수행 옵션이 제한될 때 아무것도 없는 것보다 나은 경우가 많습니다.\n\n# 로봇에 의해 구원받다\n\n결국, 현대 힌두교나 불교에서 종교적 회복을 위해 로봇에 의존하는 것은 미래적으로 보일 수 있지만, 현재 순간에 매우 속한다고 할 수 있습니다. 이는 힌두교, 불교 및 남아시아의 다른 종교들이 기계적 지혜를 활용하여 사람의 약점을 초월하려는 것으로 상상되는 것이라는 점을 보여줍니다. 왜냐하면 로봇은 지치지 않으며 말해야 할 것을 잊지 않고, 잠들지도 않고 떠나지도 않기 때문입니다.\n\n구체적으로 말하면, 로봇 자동화는 동아시아와 남아시아(특히 인도와 일본)에서 의식 수행을 완벽하게 하는 데 사용되고 있으며, 이는 인간 성도에게는 불가능한 것을 달성할 수 있도록 합니다. 완벽하고 흠없는 의식 수행을 아이디어와 결합시켜 더 나은 종교의 개념을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현대 로봇공학은 종교가 더 이상 인간이 없는 것이 가장 좋은 종류의 종교가 된다는 특별한 문화적 모순처럼 느껴질 수 있습니다. 하지만 인간들이 로봇을 만들고, 로봇이 신이 되고, 신이 다시 인간이 되는 이 중첩 속에서, 우리는 다시 한번 우리 자신을 재해석한 것뿐입니다.\n\n이 기사는 The Conversation에서 나온 것으로, 학술 전문가들의 지식을 공유하기 위해 헌신하는 독립 비영리 뉴스 기관입니다. 좀 더 자세한 정보를 원하시거나, 연합통신 및 종교 뉴스 서비스와 공동으로 발행하는 종교 이야기 이번 주간 뉴스레터를 받아보시려면 저희에 대해 더 알아보시거나 구독해주세요.\n\nHolly Walters는 이 기사로부터 이익을 얻는 어떠한 회사나 기관에도 속하지 않으며, 컨설팅하거나 주식을 소유하거나 금전 지원을 받지 않습니다. 학술적 직위 이외의 관련 소속사항을 기재하지 않았습니다.","ogImage":{"url":"/assets/img/2024-06-22-RobotsareperformingHinduritualssomedevoteesfeartheyllreplaceworshippers_0.png"},"coverImage":"/assets/img/2024-06-22-RobotsareperformingHinduritualssomedevoteesfeartheyllreplaceworshippers_0.png","tag":["Tech"],"readingTime":5}],"page":"30","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"30"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>