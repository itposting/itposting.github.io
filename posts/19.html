<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/19" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/19" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="데이터 늪에서 데이터 마스터까지 Apache Iceberg와 함께하는 레이크하우스 혁명 " href="/post/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 늪에서 데이터 마스터까지 Apache Iceberg와 함께하는 레이크하우스 혁명 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 늪에서 데이터 마스터까지 Apache Iceberg와 함께하는 레이크하우스 혁명 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 늪에서 데이터 마스터까지 Apache Iceberg와 함께하는 레이크하우스 혁명 </strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="2024년에 데이터 엔지니어가 되기 위한 조건은 무엇일까" href="/post/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="2024년에 데이터 엔지니어가 되기 위한 조건은 무엇일까" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="2024년에 데이터 엔지니어가 되기 위한 조건은 무엇일까" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">2024년에 데이터 엔지니어가 되기 위한 조건은 무엇일까</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="기초부터 고급까지 빅데이터 전문가를 위한 Apache Hive 완벽 가이드" href="/post/2024-06-23-FromBasicstoAdvancedNavigatingApacheHiveforBigDataProfessionals"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="기초부터 고급까지 빅데이터 전문가를 위한 Apache Hive 완벽 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-FromBasicstoAdvancedNavigatingApacheHiveforBigDataProfessionals_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="기초부터 고급까지 빅데이터 전문가를 위한 Apache Hive 완벽 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">기초부터 고급까지 빅데이터 전문가를 위한 Apache Hive 완벽 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="PySpark 완벽 해설 explode와 collect_list 함수 사용 방법" href="/post/2024-06-23-PySparkExplainedTheexplodeandcollect_listFunctions"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="PySpark 완벽 해설 explode와 collect_list 함수 사용 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-PySparkExplainedTheexplodeandcollect_listFunctions_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="PySpark 완벽 해설 explode와 collect_list 함수 사용 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">PySpark 완벽 해설 explode와 collect_list 함수 사용 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 프로젝트의 코드 우수성을 위한 4가지 R 1부" href="/post/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 프로젝트의 코드 우수성을 위한 4가지 R 1부" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 프로젝트의 코드 우수성을 위한 4가지 R 1부" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 프로젝트의 코드 우수성을 위한 4가지 R 1부</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Fabric 완벽 마스터하기 실시간 주식 데이터 스트리밍 및 분석 방법" href="/post/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Fabric 완벽 마스터하기 실시간 주식 데이터 스트리밍 및 분석 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Fabric 완벽 마스터하기 실시간 주식 데이터 스트리밍 및 분석 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Fabric 완벽 마스터하기 실시간 주식 데이터 스트리밍 및 분석 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 아키텍처 간단 개요" href="/post/2024-06-23-DataArchitectureABriefOverview"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 아키텍처 간단 개요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-DataArchitectureABriefOverview_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 아키텍처 간단 개요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 아키텍처 간단 개요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">13<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유" href="/post/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석" href="/post/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="위성 이미지를 통한 그레이트 솔트레이크 축소 추적 Python 사용 방법" href="/post/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="위성 이미지를 통한 그레이트 솔트레이크 축소 추적 Python 사용 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="위성 이미지를 통한 그레이트 솔트레이크 축소 추적 Python 사용 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">위성 이미지를 통한 그레이트 솔트레이크 축소 추적 Python 사용 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">21<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><a class="link" href="/posts/1">1</a><a class="link" href="/posts/2">2</a><a class="link" href="/posts/3">3</a><a class="link" href="/posts/4">4</a><a class="link" href="/posts/5">5</a><a class="link" href="/posts/6">6</a><a class="link" href="/posts/7">7</a><a class="link" href="/posts/8">8</a><a class="link" href="/posts/9">9</a><a class="link" href="/posts/10">10</a><a class="link" href="/posts/11">11</a><a class="link" href="/posts/12">12</a><a class="link" href="/posts/13">13</a><a class="link" href="/posts/14">14</a><a class="link" href="/posts/15">15</a><a class="link" href="/posts/16">16</a><a class="link" href="/posts/17">17</a><a class="link" href="/posts/18">18</a><a class="link posts_-active__YVJEi" href="/posts/19">19</a><a class="link" href="/posts/20">20</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"데이터 늪에서 데이터 마스터까지 Apache Iceberg와 함께하는 레이크하우스 혁명 ","description":"","date":"2024-06-23 16:31","slug":"2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg","content":"\n\n\u003cimg src=\"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_0.png\" /\u003e\n\n우리의 데이터 인프라는 처음에 Amazon S3를 사용한 데이터 레이크와 Amazon Redshift를 사용한 데이터 웨어하우스의 조합으로 이루어져 있었습니다.\n\n이 구성은 대량의 데이터를 저장하고 분석할 수 있는 장점이 있었지만, 추가 저장 공간 및 유지보수 문제와 ACID 규칙 준수를 지원하지 않는 등 여러 가지 도전 과제가 있었습니다.\n\n# 목표\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n호수집 구조로 전환하는 목표는 데이터 레이크와 데이터 웨어하우스의 최상의 특징을 결합하는 것이었습니다. 이미 완전히 발달한 데이터 레이크가 있었기 때문에, 우리의 초점은 데이터 웨어하우스의 기능을 통합하는 데 있었습니다.\n\n## 데이터 레이크하우스와 데이터 레이크 및 데이터 웨어하우스의 차이는 무엇인가요?\n\n이름에서 알 수 있듯이 '데이터 레이크하우스'는 데이터 레이크와 데이터 웨어하우스의 최상의 특징을 결합합니다. 본질적으로 데이터 레이크하우스는 데이터 레이크의 기능을 확장하여 데이터 웨어하우스와 유사한 기능을 통합합니다. 데이터 레이크의 유연성, 확장 가능성 및 비용 효율성을 제공하는 한편, 데이터 웨어하우스와 주로 관련된 튼튼한 데이터 관리와 ACID (원자성, 일관성, 분리, 지속성) 트랜잭션을 제공하려고 합니다.\n\n# 왜 아파치 아이스버그를 선택했나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_1.png\" /\u003e\n\n- ACID 트랜잭션: ACID 트랜잭션을 지원하여 데이터 일관성과 신뢰성을 보장하고, 동시에 쓰기 및 읽기를 허용하여 데이터 오염이 발생하지 않습니다.\n- 비용 및 유지보수 감소: Redshift와 연관된 높은 저장 및 라이선스 비용을 최소화하며, 기본적으로 compaction 및 압축(zstd)을 지원합니다.\n- 성능 최적화: 메타데이터 가지치기, 파티셔닝 및 데이터 건너뛰기와 같은 기능을 통해 쿼리 성능을 크게 향상시킵니다.\n- 호환성: Apache Spark, Flink, Presto 등 여러 데이터 처리 엔진과 함께 작동하여 작업에 최적인 도구를 선택할 수 있는 유연성 제공.\n- Parquet, ORC, Avro와 같은 파일 형식 지원.\n- 기존의 AWS 생태계 및 Athena, Glue, Catalog, EMR 등과 시프트레이엘튼랏하는 탐바.\n- 성능 향상: 빠르게 쿼리되어 데이터를 효율적으로 검색할 수 있습니다.\n- 통합 데이터 처리: 일괄 및 스트리밍 데이터 처리에 대해 통합된 경험을 제공하여 실시간 및 기존 데이터의 원활한 통합 및 처리를 가능하게 합니다.\n\n\u003cimg src=\"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_2.png\" /\u003e\n\n# Iceberg 아키텍처:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_3.png)\n\n![Image 2](/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_4.png)\n\n# Apache Iceberg을 활용한 Lakehouse 전환 단계\n\n환경 설정:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 저장소 구성: Amazon S3와 같은 확장 가능한 저장소 솔루션을 설정하여 원본 데이터와 처리된 데이터를 저장하세요. 이미 저희와 같이 S3를 활용 중이라면 데이터 및 메타데이터를 저장할 대상 버킷을 정의하세요.\r\n- Iceberg 설치 및 구성: EMR을 사용 중이므로 Spark 세션을 생성할 때 Iceberg 관련 설정을 추가해야 합니다.\n\n```js\r\nspark = SparkSession.builder \\\n    .appName(\"user_device_data\") \\\n    .master(\"yarn\") \\\n    .config(\"spark.sql.defaultCatalog\", catalog) \\\n    .config(f\"spark.sql.catalog.{catalog}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog}.warehouse\",\n            \"\u003cYour S3 Warehouse Path\u003e\") \\\n    .config(\"spark.sql.catalog.glue_catalog.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(\"spark.sql.catalog.glue_catalog.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\r\n```\n\n데이터 이전:\n\n- 데이터레이크 — 기존 Parquet에서 Iceberg로 데이터 마이그레이션: 먼저 테이블을 만들었고, 기존 데이터레이크에서 데이터를 읽어와 Apache Spark를 사용하여 Iceberg 테이블에 기록함으로써 메타데이터가 올바르게 캡처되도록합니다.\n- 데이터웨어하우스 — Redshift 데이터를 Iceberg 형식으로 투입: Redshift에서 언로드한 데이터를 S3로 복사한 후, 데이터레이크와 동일한 접근 방식을 따랐습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같은 방법으로 인플레이스 마이그레이션을 수행할 수 있습니다:\n\n- add_files 사용\n- migrate 사용\n\n[기존 데이터 레이크를 Apache Iceberg를 사용한 트랜잭션 데이터 레이크로 마이그레이션하기](https://aws.amazon.com/blogs/big-data/migrate-an-existing-data-lake-to-a-transactional-data-lake-using-apache-iceberg/)\n\n기존 ETL 프로세스에서의 조정:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 우리는 모든 ETL 작업에 대한 싱크 구성을 변경하여 데이터 아이스버그 형식으로 쓰게 했습니다. 위에서 언급한 구성은 스파크 세션을 만들 때 사용되었습니다.\n\n데이터 거버넌스 및 메타데이터 관리:\n\nIceberg 테이블의 유지 보수 작업.\n\n- Compact : 우리는 다시 쓰고 결과 파일의 원하는 크기로 재작성하기 위해 rewriteDataFiles 절차를 실행합니다. 이것은 읽기 시간을 최적화하는 데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n('write.parquet.target-file-size-bytes '='52428800') \n# 약 이만큼의 바이트를 대상으로 생성된 파일의 크기를 제어합니다.\n\n2. 스냅샷 만료: 분석에 더 이상 필요하지 않은 데이터에 대해 스냅샷 만료를 실행하여 불필요한 저장 비용을 피합니다. 만료된 스냅샷과 연결된 매니페스트 목록, 매니페스트 및 데이터 파일은 여전히 유효한 스냅샷과 연관되어 있지 않은 한 스냅샷 삭제 시에 삭제됩니다.\n\n우리는 이 작업을 수행하기 위해 expireSnapshots 프로시저를 실행합니다.\n\n3. 오래된 메타데이터 파일 제거: Iceberg는 새 메타데이터 파일이 생성될 때 오래된 메타데이터 파일을 삭제하는 설정을 활성화할 수 있습니다. 또한 테이블이 보유해야 하는 메타데이터 파일 수를 설정할 수 있습니다. 우리는 그 수를 5로 설정했습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nwrite.metadata.delete-after-commit.enabled  true\nwrite.metadata.previous-versions-max 5\n```\n\n4. Orphan 파일 삭제 : Orphan 파일 제거를 위해 deleteOrphanFiles 절차를 실행하여 필요 없는 파일을 저장하지 않습니다. 이러한 파일들은 정기적인 정리 프로세스에서 선택되지 않습니다.\n\n쿼리 및 분석:\n\n- 쿼리 최적화: Iceberg는 메타데이터 가지치기(metadata pruning) 및 프리디케이트 푸시다운(predicate pushdown)과 같은 기능을 지원하여 쿼리 성능을 최적화할 수 있습니다. 데이터와 메타데이터 모두에 대한 쿼리 엔진으로 Athena를 사용하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n메타데이터 쿼리 치트 시트 : [https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-table-metadata.html](https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-table-metadata.html)\n\n- 실시간 분석 활성화: 스파크 ETL 프로세스에서 데이터 수집 및 업데이트가 발생하여 배치 및 스트리밍 데이터 처리에 통합된 경험을 제공합니다.\n\n# Apache Iceberg 구현의 장점\n\n비용 효율성:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 저장 비용 절감: Apache Iceberg는 기본 Z-표준 (zstd) 압축을 사용하여 저장 공간 요구 사항을 크게 줄입니다. 또한 파티션 분할을 지원하여 데이터 스캔을 제한하여 저장 공간 사용을 최적화합니다.\n- 중복 데이터 웨어하우징 솔루션 소거: Iceberg의 ACID 규정 준수로 기존 데이터 웨어하우징 솔루션이 불필요해집니다. Amazon S3의 저장 비용은 Redshift의 그것보다 상당히 낮기 때문에 Redshift 라이선싱 비용을 상당히 절약할 수 있습니다.\n\nApache Iceberg의 채택으로 일반 Parquet 형식으로 데이터를 저장하는 비용과 전체 S3 비용 모두 30%의 저장 비용 절감과 20%의 S3 비용 절감을 이끌었습니다.\n\n성능 향상:\n\n- 쿼리 성능 개선: Iceberg의 최적화된 데이터 관리 및 인덱싱으로 쿼리 성능과 데이터 검색 시간이 상당히 향상되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n역사적 데이터 수정:\n\n- 간편화된 데이터 업데이트: 이전에는 역사적 데이터를 수정하기 위해 작은 배치 작업을 작성해야 했습니다. 아이스버그를 사용하면 몇 가지 업데이트 명령을 실행함으로써 이를 달성할 수 있어, 프로세스가 간소화되었습니다.  \n\n접근 제어:\n\n- 간편화된 행 수준 접근: Redshift에서 서로 다른 국가에 대한 행 수준 접근을 제공하는 것은 복잡했습니다. 아이스버그를 통해 데이터 파티셔닝을 사용하여 특정 버킷에 대한 접근 정책을 쉽게 구현할 수 있어, 접근 관리가 간소화됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nApache Iceberg를 구현함으로써 비용 효율성, 향상된 성능, 간단화된 기존 데이터 수정 및 향상된 액세스 관리를 달성했습니다.\n\n# 결론\n\n여러 데이터셋을 Apache Iceberg로 이관하는 작업을 성공적으로 완료했으며 이미 상당한 비용 및 성능 이점을 확인하고 있습니다. 레이크하우스 아키텍처로의 전환은 데이터 레이크와 데이터 웨어하우스의 최상의 기능을 활용할 수 있게 해주어 더 효율적이고 확장 가능한 데이터 인프라를 구축하게 되었습니다.\n\n이 구현 기간 동안 빈말 야다브와 시바무 구프타에게 놀라운 헌신과 값진 기여에 진심으로 감사드립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사실, 한국어로 \"테이블\" 태그를 \"Markdown\" 형식으로 변환하면 되는 것 같아요.","ogImage":{"url":"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_0.png"},"coverImage":"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_0.png","tag":["Tech"],"readingTime":7},{"title":"2024년에 데이터 엔지니어가 되기 위한 조건은 무엇일까","description":"","date":"2024-06-23 16:28","slug":"2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024","content":"\n\n\u003cimg src=\"/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_0.png\" /\u003e\n\n만약 이 글을 읽고 있다면, 요즘 직업을 바꾸려고 고민 중이신 것 같군요. 아마도 소프트웨어 엔지니어링과 데이터베이스 디자인에 관심이 있으신 거겠죠. 당신의 배경이 무엇이든 중요하지 않아요 — 마케팅, 분석, 혹은 금융 분야에서 오셨든, 여러분도 할 수 있어요! 이 이야기는 데이터 공간에 빠르게 진입하는 방법을 찾아주기 위한 것이에요. 예전에 저도 똑같이 해서 그 뒤로 후회한 적이 없어요. 기술 분야, 특히 데이터는 매력과 혜택이 넘쳐나요. 선뜻 언급하지 않은 원격 근무와 선도 기업들로부터의 막대한 혜택 패키지도 말이죠. 파일과 숫자로 마법을 부릴 수 있다는 사실 그 자체가 멋진 거 아니겠어요? 이 글에서는 2~3개월 안에 완료할 수 있는 기술과 프로젝트들을 요약해서 소개할 거에요. 상상해보세요, 몇 달간의 노력만으로 첫 직장 면접 준비를 마치고 있을 수 있답니다.\n\n## 왜 데이터 엔지니어링이고 데이터 과학이 아닌가요?\n\n사실 왜 데이터 분석이나 데이터 과학이 아니고 데이터 엔지니어링을 선택하는 걸까요? 저는 이 역할의 본질에 답이 있다고 생각해요. 데이터 엔지니어가 되기 위해서는 소프트웨어 엔지니어링과 데이터베이스 디자인, 기계 학습(ML) 모델, 그리고 데이터 모델링과 비즈니스 인텔리전스(BI) 개발을 배워야 해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 엔지니어링은 DICE에 따르면 가장 빠르게 성장하는 직업입니다. 그들은 갭을 보여주기 위해 연구를 수행했습니다. 빨리 움직이세요.\n\n![이미지](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_1.png)\n\n데이터 과학자는 장기간 시장에서 \"가장 매혹적인\" 직업으로 여겨져 왔지만, 최근에 데이터 엔지니어의 부족이 있다는 것으로 보입니다. 이 분야에서 엄청난 수요가 있음을 확인할 수 있습니다. 이는 경험이 풍부하고 높은 자격을 갖춘 엔지니어들 뿐만 아니라 입문자도 포함됩니다. 지난 5년 동안 영국에서 데이터 엔지니어링은 가장 빠르게 성장하는 직업 중 하나였으며, 2023년 LinkedIn의 인기 있는 직업 목록에서 13위를 차지했습니다. 평균적으로 매주 약 4차례의 취업 면접 요청을 받습니다. 입문 데이터 엔지니어들은 훨씬 더 자주 초대될 것입니다.\n\n데이터 엔지니어링이 매우 복잡하기 때문에 급여와 혜택 package는 다른 기술 분야보다 훨씬 더 좋아보입니다. 실제로, 데이터가 잡다하고 지루한 데이터 조작 작업인 것처럼 보여 소프트웨어 엔지니어 중에는 데이터를 피하는 것을 선호하는 사람들도 많이 있습니다. 이로 인해 데이터 엔지니어링은 데이터 플랫폼 및 데이터 파이프라인 디자인 패턴을 배우려는 사람들에게 수익성 있는 목표가 되고 있습니다. 데이터 엔지니어링은 데이터 조작 및 프로세스 조정에 관한 것입니다. 데이터는 정제되고 테스트되고 승인되어 사용자에게 적시에 전달되어야 합니다. 이는 ML 및 BI가 그것을 많이 의존하는 이유입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 이야기에서는, 2~3개월 안에 달성할 수 있는 기술 세트와 가능한 프로젝트를 요약해 보려고 합니다. 상상해보세요, 몇 달 동안 적극적인 학습을 한 후에는 첫 직장 면접 준비가 완료되어 있게 됩니다.\n\n## 데이터 엔지니어링은 압도적으로 느껴질 수 있어요\n\nSTEM(과학, 기술, 공학, 수학) 배경 없이 데이터 엔지니어링에 뛰어드는 것은 매우 어려울 수 있어요. 코딩 자체가 쉬운 일이 아니에요. 데이터베이스 및 데이터 파이프라인 오케스트레이션은 처음부터 이해하기가 더욱 어려웠어요. 몇 년 전에 저는 양적 금융을 전공한 석사 학위를 받고 분석 매니저로 일했었어요. 코딩을 배우기로 결정한 날을 기억해요. 대학에서 얻은 수준처럼은 아니지만 실제 세계의 문제를 해결하기 위해 실무에서 나의 기술을 적용할 수 있도록 배우고자 한 것이죠.\n\n일상적으로 업무를 수행하면서 여가 시간에만 소프트웨어 엔지니어링을 배워야 했던 어려움이 있었던 것도 기억이 나네요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n나는 Fiverr과 PeoplePerHour에서 프로젝트를 찾아보며 기업들이 데이터에 대해 어떤 것을 필요로 하는지 살펴보았던 기억이 납니다. 지금 생각해보면 이것이 많은 고객들의 진정한 고통 포인트를 이해하는 데 많은 도움이 되었고 아마도 가장 효율적인 학습 방법이었을 것입니다.\n\n그래서 모든 데이터 실무자를 향한 첫 번째 조언은 믿음입니다.\n\n데이터 엔지니어링 분야에 진입하는 것은 압도적으로 느껴질 수 있지만 가치가 있습니다. 부끄러워하지 마시고 글을 쓰는 사람들에게 물어보세요. Medium은 그런 점에서 정말 좋은 곳입니다. 왜 취향에 맞는 주제를 확인하고 누구를 팔로우할지 확인해보지 않으세요?\n\n## 계획\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n쉬는 시간을 가져가서 진정으로 그것이 필요한지 생각해보세요. 만약 답이 '예' 라면 필요한 건 계획 뿐입니다. 이 곳에서 목표는 속도가 아닙니다. 가능하다면 아무런 고통 없이 \"데이터 엔지니어링에 진입하는 방법\"을 실현 가능한 해결책으로 기록하는 것이 이제의 목표입니다.\n\n지금은 단지 다음 몇 달에 집중하고자 하는 것과 복습해야 할 것들을 생각해봅시다.\n\n## 데이터 엔지니어링의 습관\n\n첫 두 주 동안 실제로 배우면서 이 습관을 습득하고자 할 것입니다. 조금씩 하되 꾸준히 합니다. 학습의 습관을 형성해야 합니다. 예를 들어, 저는 Google Professional Data Engineer 시험 준비를 하면서 이렇게 했습니다. 매일 아침에 체육관에서 사이클을 타면서 책을 읽었죠. 아침이 가장 생산적인 시간이기 때문에 그 때 진행했습니다. 이 2020년의 글은 여전히 유효합니다. 많은 것들이 실제로 변한 게 많지 않고, 학습은 주로 데이터 엔지니어링의 기본 원칙에 대해 집중했습니다. 물론, 제품 특정 질문이 많았지만, 이 글은 빠르게 학습하는 방법에 대한 지침서입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 엔지니어링은 다음과 같은 기술 영역들에 관련이 있어요:\n\n- ETL 및 데이터 추출\n- 데이터 조작과 데이터 모델링 (대개 SQL을 사용함)\n- 파이프라인 테스트\n- 데이터 테스트\n- 보고 및 비즈니스 인텔리전스\n- MLOps 및 기계 학습 파이프라인\n- 이 모든 것을 조율하는 것\n\n## 첫 1-2주: SQL\n\n일단 SQL에 집중해 볼게요. 목록에서 첫 번째 항목은 아니지만 저는 이것이 가장 보편적이라고 생각해요. SQL 방언은 데이터 모델링에서 널리 사용되어 왔기 때문에 이제 데이터 조작의 표준으로 간주될 수 있어요. 처음 두 주 동안 해야 할 것은 다양한 SQL 쿼리를 실행해보고 어떤 데이터 파이프라인에서 사용될 수 있는지 상상하는 거에요. 여기서 다시 정리하고 싶은 것들은 아마도 다음과 같을 거에요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- SQL을 사용하여 테이블을 만드는 방법\n- 공통 테이블 표현식을 사용하는 방법\n- SQL을 사용하여 데이터를 모킹하는 방법\n- 증분 전략을 사용하여 테이블을 업데이트하는 방법\n- 데이터 품질을 테스트하고 데이터를 정리하는 방법\n\n이러한 질문들은 압도될 수 있을지도 모르지만, 많은 훌륭하고 간단한 예제들이 있습니다. 이러한 예제들은 몇 가지 무료 데이터 웨어하우스 솔루션과 결합하여 비교적 간단하고 생산적인 샌드박스를 만드는 데 도움이 될 수 있습니다. 이에 대해 이전 이야기 중 하나에서 이야기했었습니다. SQL 관련해서는 일상적인 데이터 엔지니어링에 실제로 필요한 모든 것입니다.\n\n가장 어려운 주제인 MERGE와 같은 주제도 SQL이 CTE 내에 목된 데이터를 포함할 때 쉽게 설명될 수 있습니다:\n\n```js\ncreate temp table last_online as (\n    select 1 as user_id\n    , timestamp('2000-10-01 00:00:01') as last_online\n)\n;\ncreate temp table connection_data  (\n  user_id int64\n  ,timestamp timestamp\n)\nPARTITION BY DATE(_PARTITIONTIME)\n;\ninsert connection_data (user_id, timestamp)\n    select 2 as user_id\n    , timestamp_sub(current_timestamp(),interval 28 hour) as timestamp\nunion all\n    select 1 as user_id\n        , timestamp_sub(current_timestamp(),interval 28 hour) as timestamp\nunion all\n    select 1 as user_id\n        , timestamp_sub(current_timestamp(),interval 20 hour) as timestamp\nunion all\n    select 1 as user_id\n    , timestamp_sub(current_timestamp(),interval 1 hour) as timestamp\n;\n\nmerge last_online t\nusing (\n  select\n      user_id\n    , last_online\n  from\n    (\n        select\n            user_id\n        ,   max(timestamp) as last_online\n\n        from \n            connection_data\n        where\n            date(_partitiontime) \u003e= date_sub(current_date(), interval 1 day)\n        group by\n            user_id\n\n    ) y\n\n) s\non t.user_id = s.user_id\nwhen matched then\n  update set last_online = s.last_online, user_id = s.user_id\nwhen not matched then\n  insert (last_online, user_id) values (last_online, user_id)\n;\nselect * from last_online\n;\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 주 3–4: 현대 데이터 스택\n\n현대 데이터 스택 및 데이터 플랫폼 아키텍처 유형에 대한 몇 가지 이야기를 읽는 것을 추천합니다. 데이터 엔지니어링에서 사용할 수 있는 다양한 도구 및 프레임워크에 대한 전략적 개요를 제공하여, 채용 인터뷰 중에 기술에 능통하다는 것을 채용 담당자에게 알려줍니다. 모든 도구를 알 필요는 없지만, \"당신은 이 분야에 계십니까\"라는 질문은 잠재 고용주와의 첫 만남 중에 상상할 수 있는 가장 매혹적인 질문입니다. 여기서는 기술의 최근 이벤트(IPO, 합병 및 인수), 개발 및 새로운 도구에 대한 인식을 보여주고 싶습니다. DuckDB 또는 Polars와 같은 것에 대해 들었다고 언급하는 것만으로도 여러분이 호기심이 많고 열정적이라는 것을 사람들에게 알려줍니다.\n\n시장에 있는 다양한 데이터 도구로 쉽게 길을 잃을 수 있습니다. 눈송이(Snowflake)에 대해 이야기하고, 그 IPO가 얼마나 성공적이었는지를 언급했던 것을 기억합니다. 그것이 많은 도움이 되었거나 적어도 우리가 인터뷰어와 동일한 의견을 가졌다는 것 같아요. 우리는 현대 데이터 스택 및 그것을 현대적이고 견고하며 비용 효율적으로 만드는 요소들에 대해 토론했습니다. 간단히 말해서, 데이터를 다루기 위해 사용되는 도구 모음입니다. 데이터를 어떻게 처리할 것인가에 따라, 이러한 도구는 다음을 포함할 수 있습니다:\n\n- 관리되는 ETL/ELT 데이터 파이프라인 서비스\n- 클라우드 기반 관리되는 데이터 웨어하우스/데이터 레이크(데이터의 대상지)\n- 데이터 변환 도구\n- 비즈니스 인텔리전스 또는 데이터 시각화 플랫폼\n- 기계 학습 및 데이터 과학 기능\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n앞선 두 주 동안 SQL을 사용하여 데이터를 변환하고 조작하는 방법을 이미 배웠어요. 이제 이 전략적인 지식을 활용하는 방법을 알게 됐으니, 그에 맞게 활용해봐요.\n\n![이미지](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_2.png)\n\n## 5-6주차: 파이썬 기초\n\n지금은 파이썬에 대한 지식을 되짚거나 살짝 배우는 시기예요. 파이썬은 정말 배우기 쉬운 방법이죠. 스크립트 형식이라 코드를 읽기 쉽고 유용한 라이브러리가 많습니다. 이 모든 특징들로 인해 데이터 엔지니어링에서 프로그래밍 언어로 많이 선택되었어요. 반복문, 함수, 조건문, 오류 처리, 그리고 데이터 구조와 같은 기본적인 프로그래밍 개념에 초점을 맞출 거예요. 데이터 엔지니어링에서는 이런 것들을 자주 사용할 거라고 생각해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제안드리는 바는 데이터 API 및 요청을 통해 시작하는 것입니다. 이러한 지식을 클라우드 서비스와 결합하면 미래에 필요한 모든 ETL 프로세스를 위한 매우 좋은 기반이 마련됩니다.\n\n전형적인 데이터 파이프라인 [5]은 Python 함수(또는 오퍼레이터)의 연쇄이며 다음과 같이 보일 것입니다:\n\n\n![pipeline](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_3.png)\n\n\n우리는 Python 함수를 사용하여 데이터를 처리하며, 결과적으로 다음과 같은 파이프라인을 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_4.png)\n\nMore examples can be found here:\n\n## API requests\n\n이해API 요청 방법은 중요합니다. 이것은 ETL 서비스가 다른 서비스와 상호 작용하는 주요 방법이기 때문입니다. 즉, 데이터를 추출합니다. 데이터 엔지니어는 API 서비스에 요청을 보내어 데이터를 요청할 때 많이 사용합니다. 그런 다음 실제로 변환 (ETL)하는 데이터를 페이징하거나 스트림으로 응답합니다. 아래 예제를 고려해보십시오. 이 예제는 NASA 소행성 API에서 데이터를 추출하는 방법을 설명합니다. 이는 매우 간단한 예제이며 매우 배우기 쉬운 이유입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\n# nasa.py\nimport requests\nsession = requests.Session()\n\nurl=\"https://api.nasa.gov/neo/rest/v1/feed\"\napiKey=\"your_api_key\"\nrequestParams = {\n    'api_key': apiKey,\n    'start_date': '2023-04-20',\n    'end_date': '2023-04-21'\n}\nresponse = session.get(url, params=requestParams, stream=True)\nprint(response.status_code)\n```\n\n더 고급이고 실행 가능한 예제는 [여기 이야기](6)에서 찾을 수 있습니다.\n\n## 7–8 주차: 추출 — 적재\n\n파이썬과 SQL을 조금 배우면 실제 데이터를 추출하고 클라우드 어딘가에 저장할 수 있습니다. AWS, GCP 및 Azure와 같은 클라우드 서비스 제공업체들이 시장을 선도하고 있으며 그 중 적어도 하나에 익숙해지는 것이 필수적입니다. 그래서 이제 우리는 실제로 첫 번째 데이터 파이프라인을 만들고 싶어할 것입니다. 이것은 간단한 함수일 수 있습니다. NASA 소행성 데이터를 추출하여 AWS S3에 저장하는 것이다. 그게 다입니다! 아주 간단하지만 이것이 우리의 첫 번째 데이터 파이프라인이며 매일, 매 시간 등으로 실행되도록 예약할 수 있습니다. 서버리스 마이크로서비스로 배포하고 무료로 실행되어 클라우드 저장 공간에서 데이터를 추출 및 보존합니다. AWS 웹 UI를 사용하여 쉽게 배포할 수 있습니다. 그러나 서비스를 배포하는 더 선호되는 방법은 인프라스트럭처 코드입니다. 해당 주제는 본질적으로 이해하기 어려우며 초보자이신 경우 깊게 파고들지 않는 것이 좋습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 몇 주 동안은 명령줄 도구에 주로 초점을 맞추고 클라우드 기능을 배포하고 클라우드에서 리소스를 프로비저닝하는 몇 가지 트릭을 익히는 것을 추천합니다.\n\n간단한 AWS 람다처럼 ETL 서비스를 생성할 수 있습니다:\n\n```js\n# AWS CLI를 사용하여 패키지된 람다 배포:\naws \\\nlambda create-function \\\n--function-name etl-service-lambda \\\n--zip-file fileb://stack.zip \\\n--handler \u003c당신의 람다 핸들러 경로\u003e/app.lambda_handler \\\n--runtime python3.12 \\\n--role arn:aws:iam::\u003c당신의 AWS 계정 ID\u003e:role/my-lambda-role\n\n# # 이미 배포되었다면 업데이트를 위해 다음을 사용합니다:\n# aws --profile mds lambda update-function-code \\\n# --function-name mysql-lambda \\\n# --zip-file fileb://stack.zip;\n```\n\n예를 들어, AWS CLI를 사용하여 ETL 서비스를 호출할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\naws lambda invoke \\\n    --function-name etl-service-lambda \\\n    --payload '{ \"data\": \"value\" }' \\\n    response.json\n```\n\n일반적인 코드 예시와 람다 패키지를 사용한 예시는 여기 [7]에서 확인할 수 있습니다.\n\n이제 데이터가 클라우드에 저장되었으므로 데이터 웨어하우스 도구로 불러올 수 있습니다. 저는 1TB 데이터 스캔당 $5가 청구되는 BigQuery를 권장합니다. 테스트 데이터로 작업하게 되므로 비용이 전혀 발생하지 않습니다. 다음과 같이 외부 테이블을 만들 수 있습니다 [8]:\n\n```js\n# 데이터가 Google Cloud Storage에 저장되어 있다고 가정\nLOAD DATA INTO source.nasa_asteroids\nFROM FILES(\n  format='JSON',\n  uris = ['gs://nasa-asteroids-data/*']\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한 AWS CLI의 기본을 배우고 싶다면 이 기사를 시도해보실 수도 있습니다 [9]\n\n## 9-10주차\n\n이번에는 Python에서 단위 테스트의 기본을 배우고 ETL 서비스를 조정하는 방법에 대해 알아보고 싶어요. 이미 Python 함수를 만들어 API 호출을 수행하고 데이터를 추출하는 방법을 배웠죠. 아마도 우리는 Python for Data Engineers의 예제를 사용하여 Python에서 데이터를 변환하는 방법도 배웠을지도 모르겠어요. 이제는 적용한 데이터 변환 로직을 테스트하고 싶어요. 단위 테스트는 소프트웨어 엔지니어링에서 필수적인 기술이며 장기적으로 많은 시간을 절약해줍니다. 코드를 테스트하고 유지하는데 도움이 되죠. 요약하자면 Pytest 모듈의 기본을 배우고 싶습니다. 예를 들어, 아래와 같은 ETL 함수의 논리를 테스트할 수 있어야 합니다:\n\n```js\n# etl.py\ndef etl(item):\n    # 여기서 데이터 변환을 수행\n    return item.lower()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPython 스크립트를 실행하기만 하면 간단히 이 작업을 수행할 수 있습니다 [10]:\n\n```js\n# etl_test.py\nfrom etl import etl\n\ndef test_etl_returns_lowercase():\n    assert etl('SOME_UPPERCASE') == 'some_uppercase'\n```\n\nunittest 라이브러리를 사용한 또 다른 예시를 살펴보겠습니다:\n\n```js\n# ./prime.py\nimport math\n\ndef is_prime(num):\n    '''num이 소수인지 확인합니다.\n    '''\n    for i in range(2,int(math.sqrt(num))+1):\n        if num%i==0:\n            return False\n    return True\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nunittest을 사용하면 간단해요. 다음과 같이 테스트할 거예요:\n\n```js\n# ./test.py\nimport unittest\nfrom prime import is_prime\n\nclass TestPrime(unittest.TestCase):\n\n    def test_thirteen(self):\n        self.assertTrue(is_prime(13))\n```\n\n이제 테스팅의 기본을 알았으니, 함수들이 올바른 데이터를 반환하는지 확신할 수 있어요. 그러므로 다음 단계는 ETL 프로세스를 조정하는 추가적인 마이크로서비스를 배포하는 것이죠. 요약하면, 이는 간단한 AWS Lambda 함수거나 스케줄에 따라 우리의 ETL 서비스를 호출할 수 있는 다른 서버리스 애플리케이션이 될 수 있어요. 이것은 매우 간단하며 여기서 복잡하게 하고 싶지 않아요. 다른 Python Lambda 함수를 배포하고 매일 또는 매시간 실행되도록 스케줄을 지정할 거예요. 그것을 위해 AWS EventBridge 이벤트를 사용하고 cron 스케줄을 설정할 수 있어요. 우리의 Orchestator Lambda 코드는 아래와 같이 보일 거에요.\n\n```js\nimport json\nimport boto3\n \n# 다른 Lambda를 호출하기 위한 AWS Lambda 클라이언트\nclient = boto3.client('lambda')\n \ndef lambda_handler(event, context):\n \n    # 다른 Lambda로 전달할 데이터\n    data = {\n        \"ProductName\": \"iPhone SE\"\n    }\n \n    response = client.invoke(\n        FunctionName='arn:aws:lambda:eu-west-1:12345678:function:etl-service-lambda',\n        InvocationType='RequestResponse',\n        Payload=json.dumps(data)\n    )\n \n    response = json.load(response['Payload'])\n \n    print('\\n')\n    print(response)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 더 알고 싶다면, AWS Step Functions 및 Infrastructure as Code와 관련된 고급 튜토리얼이 있습니다.\n\n## 11–12 주차\n\nML 기본 지식을 배우세요. 데이터를 추출하고 ETL을 수행하는 방법 및 데이터 웨어하우스로 데이터를 로드하는 방법을 이미 알고 있습니다.\n\n아래 튜토리얼을 살펴보세요. 이 튜토리얼은 사용자 이탈을 다루고 행동 데이터를 사용하여 사용자 이탈을 예측하는 방법을 설명합니다. 몇 시간 만에 완료할 수 있지만 이탈에 대한 구체적인 내용을 깊게 파고들고 싶다면 더 많은 시간이 걸릴 수 있습니다. 모든 머신 러닝 모델을 알 필요는 없습니다. 우리는 머신 러닝 및 데이터 과학 분야에서 아마존 및 구글과 경쟁할 수는 없지만, 사용하는 방법을 알아야 합니다. 클라우드 서비스 제공 업체들이 제공하는 다양한 관리형 ML 서비스가 있으며, 그것들에 친숙해져야 합니다. 데이터 엔지니어들은 이러한 서비스를 위해 데이터 세트를 준비하며, 이에 대해 몇 개의 튜토리얼을 진행하는 것이 유용할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 결론\n\n데이터 엔지니어링을 빠르게 학습하고 이 분야의 전문가가 되어야 한다고 자신을 누르지 마세요. 많은 사람들에게는 특정 분야를 숙달하는 데 몇 년이 걸리기 때문에 주말에 학습하면서 몇 달 안에 이룰 수 있는 목표에 집중하는 것이 좋습니다. 데이터 엔지니어는 ETL/ELT 기술, 데이터 모델링에 대한 충분한 지식이 필요하며 적어도 Python에서 코딩할 수 있어야 합니다. 이 글에서는 데이터 엔지니어링을 가장 효율적으로 배울 수 있는 12주 계획을 개요로 설명했습니다. 즐기시기 바랍니다.\n\n## 추천 도서\n\n[1] https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[2] [Google Professional Data Engineer 시험을 2020년에 통과한 방법](https://towardsdatascience.com/how-i-passed-google-professional-data-engineer-exam-in-2020-2830e10658b6)\n\n[3] [초보자를 위한 고급 SQL 기술](https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488)\n\n[4] [데이터 플랫폼 아키텍처 유형](https://towardsdatascience.com/data-platform-architecture-types-f255ac6e0b7)\n\n[5] [데이터 파이프라인 디자인 패턴](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n[6] [Python for Data Engineers](https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd)\n\n[7] [Building a Batch Data Pipeline with Athena and MySQL](https://towardsdatascience.com/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c)\n\n[8] [When Your Stack is a Lake House](https://medium.com/towards-artificial-intelligence/when-your-stack-is-a-lake-house-6bcb17f9bff6)\n\n[9] [Mastering AWS CLI](https://medium.com/geekculture/mastering-aws-cli-5454ad5e685c)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[10] [Python을 사용한 데이터 파이프라인 테스트 안내](https://towardsdatascience.com/a-guide-to-data-pipeline-testing-with-python-a85e3d37d361)\n\n[11] [데이터 파이프라인 Orchestration](https://towardsdatascience.com/data-pipeline-orchestration-9887e1b5eb7a)","ogImage":{"url":"/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_0.png"},"coverImage":"/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_0.png","tag":["Tech"],"readingTime":14},{"title":"기초부터 고급까지 빅데이터 전문가를 위한 Apache Hive 완벽 가이드","description":"","date":"2024-06-23 16:26","slug":"2024-06-23-FromBasicstoAdvancedNavigatingApacheHiveforBigDataProfessionals","content":"\n\n![image](/assets/img/2024-06-23-FromBasicstoAdvancedNavigatingApacheHiveforBigDataProfessionals_0.png)\n\n아파치 하이브는 하둡을 위한 데이터 웨어하우징 및 SQL과 유사한 쿼리 언어입니다. 페이스북에서 개발되었으며, 현재 아파치 소프트웨어 재단의 일부이며 다양한 기관에서 대용량 데이터 처리를 위해 사용됩니다.\n\n혜택\n\n아파치 하이브는 특히 하둡 생태계 내 대용량 데이터 처리의 맥락에서 다른 도구들보다 여러 가지 이점을 제공합니다. 여기에는 몇 가지 주요 혜택이 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1. SQL과 유사한 인터페이스\n\na) 사용 편의성: 하이브는 HiveQL이라는 SQL과 유사한 쿼리 언어를 제공하여, 이미 SQL을 알고 있는 사용자에게 쉽게 접근할 수 있습니다. 이는 새로운 프로그래밍 언어나 쿼리 구문을 배워야 하는 다른 도구와 비교하여 학습 곡선을 낮춰줍니다.\n\nb) 선언형 언어: 사용자가 필요로 하는 데이터에 집중할 수 있도록 처리 로직의 세부사항을 신경 쓰지 않게 합니다.\n\n2. 확장성과 성능\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\na) 확장성: Hive는 Hadoop의 분산 저장소(HDFS) 및 처리(MapReduce, Tez, 또는 Spark)를 활용하여 대규모 데이터셋을 효율적으로 처리할 수 있도록 설계되었습니다.\n\nb) 성능: 파티셔닝, 버킷팅, 그리고 벡터화된 쿼리 실행과 같은 기술을 통해 Hive는 쿼리 성능을 크게 최적화할 수 있습니다.\n\n3. Hadoop 생태계와의 통합\n\na) 원활한 통합: Hive는 HDFS, YARN, HBase와 같은 다른 Hadoop 구성 요소뿐만 아니라 Flume와 Sqoop과 같은 데이터 수집 도구와도 원활하게 통합됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nb) 유연성: 하이브는 텍스트, 시퀀스, Avro, ORC 및 Parquet과 같은 다양한 데이터 형식을 지원하여 다양한 사용 사례에 유용합니다.\n\n4. 확장성\n\na) 사용자 정의 함수(UDFs): 하이브를 사용하면 사용자가 자바, 파이썬 또는 다른 언어로 사용자 정의 함수를 작성하여 기능을 확장할 수 있습니다. 이는 복잡한 처리 요구를 처리하는 유연성을 제공합니다.\n\nb) 확장성: 하이브의 아키텍처는 사용자 정의 SerDes(직렬화/역직렬화기) 및 입력-출력 형식을 추가하는 것을 지원하여 확장성을 향상시킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5. 데이터 읽기 시 구성 스키마\n\n- 구성 스키마에 대한 설명: 전통적인 데이터베이스가 쓰기 작업 중에 스키마를 강제하는 것과 달리, 하이브는 읽기 시에 스키마를 적용합니다. 이는 반정형 및 비구조화된 데이터를 다루는 데 유연성을 제공합니다.\n\n6. 비용 효율성\n\n- 오픈 소스: 하이브는 아파치 소프트웨어 재단의 오픈 소스 프로젝트로, 무료로 사용할 수 있고 개발 및 지원에 기여하는 대규모 커뮤니티가 존재합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nb) 상품 하드웨어: 하이브는 하둡에서 실행되는데, 이는 전통적인 데이터 웨어하우징 솔루션과 비교하여 인프라 비용을 줄이기 위해 상품 하드웨어에서 실행되도록 설계되었습니다.\n\n7. 비즈니스 인텔리전스와 분석\n\na) BI 도구 통합: 하이브는 Tableau, Power BI 및 Apache Superset과 같은 비즈니스 인텔리전스 도구와 쉽게 통합될 수 있으며, 데이터 시각화 및 분석을 용이하게 합니다.\n\nb) 즉석 쿼리: 하이브는 대규모 데이터셋에서 즉석 쿼리를 실행하는 데 적합하며, 탐색적 데이터 분석에 유용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n8. 트랜잭션 지원\n\na) ACID 트랜잭션: 하이브는 ACID (원자성, 일관성, 고립성, 지속성) 트랜잭션을 지원하여 데이터 무결성이 중요한 시나리오에서 신뢰성 있고 일관된 데이터 처리를 가능하게 합니다.\n\n9. 다양한 기능 세트\n\na) 인덱싱과 캐싱: 하이브는 쿼리 성능을 높이기 위한 인덱싱을 지원하며 쿼리 결과를 캐시하여 반복적인 쿼리에 대한 응답 시간을 개선할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nb) 조인 및 집계: 하이브는 데이터 분석에 필수적인 복잡한 조인 및 집계를 강력하게 지원합니다.\n\n10. 보안 및 접근 제어\n\na) 인증 및 권한 부여: 하이브는 Kerberos 인증을 지원하며, 세밀한 접근 제어를 위해 Apache Ranger 또는 Apache Sentry와 통합되어 데이터 보안과 규제 요구 사항을 준수합니다.\n\nb) 암호화: 데이터 암호화를 통해 데이터를 여유 및 전송 중 모두 안전하게 보호할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 아키텍처:\n\n1. 하이브 클라이언트: 사용자가 하이브와 상호 작용하는 인터페이스입니다. Command Line Interface (CLI), JDBC/ODBC 드라이버, 그리고 Apache Hue와 같은 웹 인터페이스가 포함됩니다.\n\n2. 하이브 서비스: 하이브서버2(쿼리 실행), 웹 기반 HCatalog인 WebHCat, 그리고 스키마 및 메타데이터 저장을 위한 메타스토어가 포함됩니다.\n\n3. 하이브 저장 및 연산: 데이터 저장은 일반적으로 HDFS(Hadoop 분산 파일 시스템)에 있으며, MapReduce, Tez, 또는 Spark를 사용하여 연산이 수행됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하이브에서의 데이터 모델링:\n\n1. 데이터베이스: 테이블의 네임스페이스.\n\n2. 테이블: 행과 열로 구성된 구조화된 데이터.\n\n3. 파티션: 테이블을 분할하여 쿼리와 관리를 쉽게 할 수 있는 세그먼트로 나눔.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n안녕하세요! 아래는 한국어로 번역한 내용입니다.\n\n고급 기능:\n\n1. 사용자 정의 함수(UDFs): 내장 함수로 처리할 수 없는 특정 작업을 위해 사용자가 작성한 사용자 지정 함수입니다.\n\n2. 조인 작업: 내부 조인, 외부 조인, 맵 사이드 조인을 지원하여 효율적인 처리를 도와줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 인덱스: 데이터를 스캔하는 양을 줄여 쿼리 성능을 향상시킵니다.\n\n4. 뷰: 쿼리 결과에 의해 생성된 가상 테이블입니다.\n\n성능 튜닝:\n\n1. 파티셔닝: 스캔하는 데이터 양을 제한하여 쿼리 성능을 향상시킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. Bucketing: 맵 쪽 조인을 더 효율적으로 돕습니다.\n\n3. Vectorization: 쿼리 성능을 향상시키기 위해 여러 행을 함께 처리합니다.\n\n4. Cost-Based Optimization (CBO): 통계를 사용하여 쿼리 실행 계획을 최적화합니다.\n\n사용 사례:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 웨어하우징: 대규모 데이터셋을 저장하고 분석합니다.\n\nETL 작업: 대용량 데이터에 대한 추출, 변환 및 로드 작업을 수행합니다.\n\n데이터 분석: 대규모 데이터의 일괄 처리 및 분석을 수행합니다.\n\n비즈니스 인텔리전스: 보고 및 분석을 위해 BI 도구와 통합됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다른 도구들과의 통합:\n\n1. Apache HBase: 저지연성 저장 및 검색을 위해 사용됩니다.\n\n2. Apache Spark: 빠른 인메모리 데이터 처리를 위해 사용됩니다.\n\n3. Apache Pig: 복잡한 데이터 변환을 위한 스크립팅 플랫폼입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4. Apache Flume: 데이터 수집을위한 도구입니다.\n\n5. Apache Sqoop: Hadoop과 관계형 데이터베이스 간의 데이터 전송을 위한 도구입니다.\n\n하이브의 주요 도전 과제\n\n아파치 하이브는 대용량 데이터 처리를위한 강력한 도구이지만 사용자가 하이브를 사용할 때 마주치는 일련의 도전 과제가 있습니다. 하이브를 사용하면 사용자가 마주칠 수있는 주요 문제 몇 가지는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1. 성능 이슈\n\na) 지연 시간: Hive는 배치 처리를 위해 설계되어 실시간 쿼리 요구에 대한 높은 지연 시간을 야기할 수 있습니다. 낮은 지연 시간 응용 프로그램에 적합하지 않을 수 있습니다.\n\nb) 쿼리 최적화: Hive 쿼리의 성능을 최적화하는 것은 복잡할 수 있으며 파티션, 버킷 및 인덱싱을 신중하게 설계해야 합니다.\n\nc) 소량 파일 문제: Hive는 소량의 많은 파일을 처리하는 데 성능이 저하될 수 있습니다. 대규모 데이터 세트를 효율적으로 처리하려면 소량의 파일을 큰 파일로 통합해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. 스키마 진화의 복잡성\n\n가) 스키마 변경: Hive에서 테이블 스키마를 변경하는 일은 번거로울 수 있습니다. 열을 추가하거나 삭제하거나 데이터 유형을 수정하고 메타데이터를 업데이트하는 등의 변경 사항은 호환성 문제를 야기할 수 있으며 신중한 관리가 필요합니다.\n\n나) 역호환성: 스키마를 변경할 때 역호환성을 보장하는 것은 특히 데이터 모델이 진화하는 환경에서 도전적일 수 있습니다.\n\n3. 자원 관리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\na) **리소스 경합:** 공유 Hadoop 클러스터에서 Hive 작업과 다른 Hadoop 워크로드(예: Spark 또는 HBase) 간의 리소스 경합은 성능 저하로 이어질 수 있습니다.\n\nb) **YARN 구성:** Hive 쿼리에 대한 최적의 자원 할당을 위해 YARN을 적절하게 구성하는 것은 복잡할 수 있고 근본적인 인프라를 깊이 이해해야 합니다.\n\n4. **디버깅 및 오류 처리**\n\na) **복잡한 로그:** Hive 쿼리의 디버깅은 종종 복잡하고 매우 상세한 로그를 통해 이루어지며, 이는 문제점을 정확히 파악하기 어렵게 만듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nb) **에러 메시지**: Hive 및 Hadoop 구성 요소에서 발생하는 에러 메시지는 알아보기 어려울 수 있고 문제를 해결하는 방법에 대한 명확한 안내를 제공하지 않을 수 있습니다.\n\n5. 실시간 처리 능력의 제한\n\na) 실시간 데이터 처리: Hive의 배치 지향적인 특성 때문에 실시간 데이터 처리 및 저지연 애플리케이션에는 부적합할 수 있습니다. Apache Kafka나 Apache Flink와 같은 대안들이 실시간 처리에 더 적합합니다.\n\n6. Hadoop 생태계에 대한 의존\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\na) 하둡 의존성: 하이브가 하둡 생태계와 긴밀히 통합되어 있기 때문에 하둡의 제한사항과 복잡성을 상속받습니다. 하둡 구성 요소를 업그레이드하거나 변경하면 하이브의 성능과 안정성에 영향을 줄 수 있습니다.\n\nb) 호환성 문제: 서로 다른 버전의 하이브, 하둡 및 다른 생태계 도구 간의 호환성을 유지하는 것은 어려울 수 있습니다.\n\n7. ETL 워크플로의 복잡성\n\na) ETL 파이프라인 복잡성: 하이브에서 복잡한 ETL 파이프라인을 설계하고 유지하는 것은 데이터 변환 및 정리 작업으로 인해 특히 어려울 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nETL에서의 오류 처리: ETL 워크플로우에서 견고한 오류 처리 및 복구 메커니즘을 보장하려면 추가적인 노력이 필요합니다.\n\n실제 프로젝트에서 작업할 때 고려해야 할 사항:\n\n1. Hive 환경 설정\n\na) 클러스터 설정: Hadoop 클러스터가 올바르게 설정되고 구성되었는지 확인해야 합니다. Hive는 HDFS 및 YARN과 같은 Hadoop 구성 요소에 의존합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nb) Hive 설치: 클러스터 노드에 Hive를 설치하세요. 간단한 HiveQL 쿼리를 실행하여 설치를 확인해보세요.\n\nc) 메타스토어 구성: Hive 메타스토어를 MySQL 또는 PostgreSQL과 같은 신뢰할 수 있는 RDBMS로 구성하세요. 이렇게 하면 성능과 신뢰성이 향상됩니다.\n\n2. 데이터 적재 및 저장\n\na) 데이터 로딩: LOAD DATA 명령을 사용하여 데이터를 Hive 테이블로 로드하세요. 또는 Apache Sqoop과 같은 도구를 사용하여 관계형 데이터베이스로부터 데이터를 가져올 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nb) 파티셔닝 및 버킷팅: 데이터 스키마를 계획하여 파티션과 버킷을 효과적으로 활용하세요. 이렇게 하면 스캔해야 하는 데이터 양을 줄여 쿼리 성능을 크게 향상시킬 수 있습니다.\n\nc) 데이터 포맷: 사용 사례에 따라 적합한 데이터 포맷(ORC, Parquet)을 선택하세요. ORC와 Parquet은 읽기 중심 워크로드에 대해 더 나은 압축률과 성능을 제공합니다.\n\n3. 효율적인 쿼리 작성\n\na) 일찍 필터링: 가능한 한 쿼리에서 필터를 가장 빨리 적용하여 처리해야 하는 데이터 양을 최소화하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nb) 적절한 조인 사용: 데이터셋의 크기에 따라 적절한 조인 유형(맵 사이드 조인, 브로드캐스트 조인)을 선택하세요.\n\nc) 전체 테이블 스캔 피하기: 전체 테이블 스캔을 피하려면 파티션 및 인덱스를 사용하세요.\n\nd) 결과 집합 제한: 개발 및 테스트 중에 특히 결과 행 수를 제한하는 LIMIT를 사용하세요.\n\n4. 성능 튜닝\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\na) 벡터화: 한 번에 여러 행을 처리할 수 있도록 벡터화를 활성화하여 쿼리 성능을 향상시킵니다.\n\nb) 비용 기반 최적화(CBO): Hive가 가장 효율적인 쿼리 실행 계획을 선택할 수 있도록 CBO가 활성화되어 있는지 확인합니다.\n\nc) 병렬 실행: 리듀서 및 맵 작업의 수를 조정하여 병렬성을 높입니다.\n\nd) 자원 할당: YARN 및 Tez/Spark를 구성하여 Hive 작업에 충분한 자원(메모리, CPU)을 할당합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5. 오류 처리 및 디버깅\n\na) 로그: 자세한 오류 메시지를 확인하려면 하둡 잡 트래커, HiveServer2 및 YARN의 로그를 확인하세요.\n\nb) 실행 계획 설명: 쿼리의 실행 계획을 이해하고 잠재적 병목 현상을 식별하려면 EXPLAIN을 사용하세요.\n\nc) 세션 변수: 디버깅을 위해 쿼리 실행 설정을 사용자 정의하려면 Hive 세션 변수(set 명령)를 사용하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n6. 보안 및 접근 제어\n\na) 인증: Hadoop 클러스터에서 안전한 인증을 위해 Kerberos를 구성합니다.\n\nb) 권한 부여: 세밀한 접근 제어 및 정책을 관리하기 위해 Apache Ranger 또는 Sentry를 사용하세요.\n\nc) 암호화: 데이터가 휴식 중이든 이동 중이든 보안 표준을 준수하기 위해 데이터를 암호화해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n7. 자동화 및 일정 설정\n\na) 워크플로우 자동화: Apache Oozie 또는 Apache Airflow를 사용하여 Hive 워크플로 및 ETL 프로세스를 자동화합니다.\n\nb) 일정 설정: 규칙적인 데이터 로드 및 ETL 작업을 일정에 맞춰 설정하여 데이터가 항상 최신 상태임을 보장합니다.\n\n8. 모니터링 및 유지보수\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\na) 모니터링 도구: 클러스터 건강 상태와 Hive 작업 성능을 추적하기 위해 Ambari, Cloudera Manager 또는 Grafana와 같은 모니터링 도구를 사용하세요.\n\nb) 메타데이터 관리: Hive 메타스토어 데이터베이스를 정기적으로 백업하고 오래된 메타데이터를 정리하여 성능 문제를 방지하세요.\n\nc) 업그레이드 및 패치: 최신 패치 및 업그레이드로 Hive 및 Hadoop 구성 요소를 업데이트하여 더 나은 성능과 보안을 제공하세요.\n\n9. 다른 도구들과 통합\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\na) 비즈니스 인텔리전스 (BI): 하이브를 Tableau, Power BI 또는 Apache Superset과 연결하여 데이터 시각화 및 보고를 할 수 있습니다.\n\nb) 데이터 사이언스: 하이브를 주피터 노트북이나 Apache Zeppelin과 같은 데이터 과학 플랫폼과 함께 사용하여 데이터 분석 및 기계 학습을 할 수 있습니다.\n\n10. 문서화와 모범 사례\n\na) 문서화: 하이브 설정에 대한 상세한 문서를 유지하며, 구성 세부 정보, 스키마 설계 및 ETL 워크플로우를 포함합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nb) Best Practices: Hive 쿼리 최적화, 데이터 모델링 및 리소스 관리에 대한 최상의 사례를 따르면 효율적이고 신뢰할 수 있는 작업이 보장됩니다.\n\nHIVE COMMANDS:\n\n![Hive Commands 1](/assets/img/2024-06-23-FromBasicstoAdvancedNavigatingApacheHiveforBigDataProfessionals_1.png)\n\n![Hive Commands 2](/assets/img/2024-06-23-FromBasicstoAdvancedNavigatingApacheHiveforBigDataProfessionals_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**다른 도구들과의 비교:**\n\n1. Hive vs. Pig: Apache Pig은 하둡에서 실행되며 ETL 작업에 적합하지만 Pig Latin이라는 절차적 스크립트 언어를 사용하며 SQL에 익숙한 사용자들에게는 직관적이지 않을 수 있습니다.\n\n2. Hive vs. HBase: HBase는 대규모 데이터셋에 대한 저레이턴시 실시간 읽기/쓰기 접근에 적합한 NoSQL 데이터베이스입니다. 반면 Hive는 배치 처리 및 분석 쿼리에 더 적합합니다.\n\n3. Hive vs. Spark SQL: Spark SQL은 인메모리 처리를 제공하여 특정 워크로드에 대해 더 빠른 쿼리 성능을 제공할 수 있습니다. 그러나 Hive는 더욱 성숙하며 하둡 생태계와 더 깊게 통합되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n테이블 태그를 마크다운 형식으로 변경하시면 됩니다.","ogImage":{"url":"/assets/img/2024-06-23-FromBasicstoAdvancedNavigatingApacheHiveforBigDataProfessionals_0.png"},"coverImage":"/assets/img/2024-06-23-FromBasicstoAdvancedNavigatingApacheHiveforBigDataProfessionals_0.png","tag":["Tech"],"readingTime":10},{"title":"PySpark 완벽 해설 explode와 collect_list 함수 사용 방법","description":"","date":"2024-06-23 16:24","slug":"2024-06-23-PySparkExplainedTheexplodeandcollect_listFunctions","content":"\n\nPySpark SQL은 Apache PySpark의 SQL을 위한 파이썬 인터페이스로, 데이터 변환 및 분석을 위한 강력한 도구 모음입니다. 데이터베이스 SQL 시스템에서 사용 가능한 가장 일반적인 유형의 작업을 모방하도록 만들어졌으며, Spark에서 사용 가능한 데이터프레임 패러다임을 활용하여 추가 기능을 제공할 수 있습니다.\n\n간단히 말하면, PySpark SQL은 개발자가 데이터를 효율적으로 조작하고 처리할 수 있는 다양한 기능을 제공합니다.\n\n이러한 기능 중에서, 특히 유용한 두 가지 함수는 데이터를 고유한 방식으로 변환하고 집계하는 능력으로 인해 주목할 만합니다. 이것들은 explode 및 collect_list 연산자입니다.\n\n본 문서에서는 이러한 각각의 기능이 정확히 무엇을 하는지 설명하고, 각각에 대한 사용 사례와 샘플 PySpark 코드를 보여드리겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Explode\n\nPySpark SQL의 `explode` 함수는 배열이나 맵과 같은 중첩 데이터 구조를 변환하고 평평하게 만드는 다목적 도구입니다. 이 함수는 중첩된 컬렉션을 포함하는 복잡한 데이터 세트를 다룰 때 특히 유용합니다. 중첩된 구조 내의 개별 요소를 분석하고 조작할 수 있도록 해줍니다.\n\nPyspark의 배열은 다른 컴퓨터 언어의 배열과 매우 유사합니다. 주로 동일한 유형의 요소를 특정 순서로 저장하는 데이터 구조로, 보통 연속적인 메모리 위치에 저장됩니다.\n\nSpark의 맵은 파이썬과 같은 언어의 사전과 동등합니다. 특정 키에 대한 값을 매우 빠르게 조회할 수 있는 키-값 쌍의 시리즈를 보유합니다. 나중에 `explode`를 사용하는 예제를 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어레이 컬럼에 적용되는 explode 함수는 어레이의 각 요소에 대해 새로운 행을 생성하고 해당 요소 값을 새 열에 저장합니다. 기본적으로 이 새 열은 col이라는 이름으로 지정되지만 별칭을 사용하여 사용자 정의 열 이름을 지정할 수 있습니다.\n\n마찬가지로, 맵 컬럼에 적용되는 explode 함수는 두 개의 새로운 열을 생성합니다: 키를 위한 열과 값을 위한 열. 기본적으로 이 열은 각각 key와 value로 지정되지만 다시 한 번 별칭을 사용하여 사용자 정의 열 이름을 제공할 수 있습니다.\n\n## Collect_list\n\nPySpark SQL의 collect_list 함수는 컬럼에서 값을 모아서 배열로 변환하는 집계 함수입니다. 기존에 다른 PySpark SQL 함수들을 사용하여 평탄화되거나 변환된 데이터를 재구성하거나 집계해야 하는 경우에 특히 유용합니다. 많은 면에서, 이 함수는 explode에 대한 보완 함수로 생각될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 함수는 데이터를 그룹화하기 위해 하나 이상의 열을 기반으로 한 그룹화 연산자(groupBy operator)와 함께 자주 사용됩니다.\n\n## 무료 Pyspark 개발 환경에 액세스하는 방법\n\n이 글의 코드를 따라하려면 PySpark 개발 환경에 액세스해야 합니다.\n\n귀하의 직장을 통해, 클라우드를 통해 또는 로컬 설치를 통해 PySpark에 액세스할 수 있다면 행운이겠죠. 그렇지 않은 경우, 아래 링크를 확인하여 Databricks Community Edition이라는 훌륭한 무료 온라인 PySpark 개발 환경에 액세스할 수 있는 방법에 대해 자세히 살펴보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDatabricks는 Apache Spark를 중심으로 구축된 데이터 엔지니어링, 머신 러닝 및 분석을 위한 클라우드 기반 플랫폼으로, 대용량 데이터 워크로드를 처리하기 위한 통합 환경을 제공합니다. Databricks의 설립자들은 Spark를 만들었으므로 이 분야에 대해 정통하다는 것을 알 수 있습니다.\n\n## 사용 사례 예시\n\n이제 explode 및 collect_list의 동작에 대해 조금 더 알게 되었으니, 이러한 함수들의 사용 사례를 살펴보겠습니다.\n\nexplode 함수\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nexplode 함수를 사용하여 배열을 변환하는 방법으로 시작하겠습니다. Spark에서 배열은 동일한 유형의 요소를 고정 크기의 순차적 컬렉션으로 저장하는 데이터 구조입니다.\n\n한 텍스트 열에 사람들의 이름을, 또 다른 배열 열에는 그들이 좋아하는 과일을 담은 PySpark 데이터프레임을 설정할 것입니다.\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode\n\n# 스파크 세션 초기화\nspark = SparkSession.builder.appName(\"ArrayExplodeExample\").getOrCreate()\n\n# 배열 열 \"fruits\"를 포함하는 DataFrame 생성\ndata = [\n    (\"John\", [\"apple\", \"banana\", \"cherry\"]),\n    (\"Mary\", [\"orange\", \"grape\"]),\n    (\"Jane\", [\"strawberry\", \"blueberry\", \"raspberry\"]),\n    (\"Mark\", [\"watermelon\"])\n]\n\n# 스키마 정의 및 DataFrame 생성\ndf = spark.createDataFrame(data, [\"name\", \"fruits\"])\n\n# 원래 DataFrame 표시\ndf.show(truncate=False)\n```\n\n이 데이터를 분석하는 많은 경우에서 \"name\"과 \"fruit\"의 각 기능 조합이 별도의 레코드로 존재하는 것이 훨씬 편리합니다. 이를 위해 explode 함수를 사용하여 이를 달성할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 배열 열을 펼치기 위해 explode 함수를 사용하세요\nexploded_df = df.withColumn(\"fruit\", explode(df.fruits))\n\n# 펼쳐진 DataFrame을 보여주세요\nexploded_df[\"name\",\"fruit\"].show(truncate=False)\n\n\n\n+----+----------+\n|name|fruit     |\n+----+----------+\n|John|apple     |\n|John|banana    |\n|John|cherry    |\n|Mary|orange    |\n|Mary|grape     |\n|Jane|strawberry|\n|Jane|blueberry |\n|Jane|raspberry |\n|Mark|watermelon|\n+----+----------+\n```\n\n이제 데이터는 일반적인 데이터테이블과 비슷하게 보이며, 추가 데이터프레임이나 SQL 작업을 수행하여 추가 분석을 수행할 경우 더 잘 정리되어 있습니다.\n\nPyspark에서 맵을 다루는 explode 사용법은 매우 유사합니다.\n\n```js\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, create_map, lit, col\nfrom pyspark.sql.types import MapType, StringType\n\n# Spark 세션 초기화\nspark = SparkSession.builder.appName(\"ExplodeExample\").getOrCreate()\n\n# 샘플 데이터\ndata = [\n    (\"Tom\", {\"Salary\": \"£5000\", \"Bonus\": \"£0\"}),\n    (\"Dick\", {\"Salary\": \"£2690\", \"Bonus\": None}),\n    (\"Harry\", {\"Salary\": \"£45000\", \"Bonus\": \"£20000\"})\n]\n\n# DataFrame 생성\ndf = spark.createDataFrame(data, [\"Name\", \"Remuneration\"])\n\n# 원본 DataFrame 보기\ndf.show(truncate=False)\n\n\n\n+-------+-----------------------------------+\n|Name   |Remuneration                       |\n+-------+-----------------------------------+\n|Tom    |{Salary -\u003e £5000, Bonus -\u003e £0}     |\n|Dick   |{Salary -\u003e £2690, Bonus -\u003e null}   |\n|Harry  |{Salary -\u003e £45000, Bonus -\u003e £20000}|\n+-------+-----------------------------------+\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nexplode 함수를 적용하면 키-값 쌍이 개별 레코드로 분할됩니다. 이전 예제와 마찬가지로, 이렇게 하면 더 나은 구성이 가능하여 추가 분석이 용이해집니다.\n\n```js\nremuneration_exploded = df.select(\n    col(\"Name\"),\n    explode(col(\"Remuneration\")).alias(\"key\", \"value\")\n)\n\n# 변환된 DataFrame 표시\nremuneration_exploded.show(truncate=False)\n\n\n+-------+------+-------+\n| Name  | key  | value |\n+-------+------+-------+\n|Tom    |Salary|£5000  |\n|Tom    |Bonus |   £0  |\n|Dick   |Salary|£2690  |\n|Dick   |Bonus | null  |\n|Harry  |Salary|£45000 |\n|Harry  |Bonus |£20000 |\n+-------+------+-------+\n```\n\n조금 더 복잡한 예제로 마무리해 보겠습니다. 다음과 같은 PySpark 데이터프레임이 있다고 가정해 봅시다.\n\n```js\n+----+-----------+-----------+\n|col1|     col2  |     col3  |\n+----+-----------+-----------+\n| a  | [1, 2, 3] | [4, 5, 6] |\n+----+-----------+-----------+\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 우리는 다음과 같은 출력을 얻고 싶습니다.\n\n```js\n+------+-----+-------+\n|col1  |col2  |col3  |\n+------+------+------+\n|   a  |   1  |   4  |\n|   a  |   2  |   5  |\n|   a  |   3  |   6  |\n+------+------+------+\n```\n\n이 작업은 생각보다 어렵습니다. 먼저, 입력 테스트 데이터를 생성해 봅시다.\n\n```js\ntestData = [('a',[1,2,3],[4,5,6]),]\n\ndf = spark.createDataFrame(data=testData, schema=['col1','col2','col3'])\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일단 보면 col2와 col3를 각각 터뜨릴 수 있다고 생각할 수 있지만, 여러분은 각 번 열을 번 열 씩만 터뜨릴 수 있다는 것이기 때문에 그렇게 하지 못할 거라고요. 한 번 해보세요, 그러면 제가 무슨 말을 하는지 이해하실 거에요.\n\n```js\ndf.select(\"col1\", explode(\"col2\").alias(\"col2\"), \"col3\").select(\"col1\",\"col2\", explode(\"col3\").alias(\"col3\")).show()\n```\n\n```js\n+----+----+----+\n|col1|col2|col3|\n+----+----+----+\n|   a|   1|   4|\n|   a|   1|   5|\n|   a|   1|   6|\n|   a|   2|   4|\n|   a|   2|   5|\n|   a|   2|   6|\n|   a|   3|   4|\n|   a|   3|   5|\n|   a|   3|   6|\n+----+----+----+\n```\n\n흠, 우리가 원하는 것과는 조금 다르네요. 원하는 대로 하려면, PySpark 배열의 경우 Python의 zip 연산과 동일한 작업을 수행하는 중간 단계가 필요합니다. Python zip 연산자는 두 가지 반복 가능한(iterable) 항목을 묶어줍니다. 예를 들어, 만약 우리가 아래와 같은 것이 있었다면,\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nnumbers = [1, 2, 3]\n\nletters = ['a', 'b', 'c']\n\nzipped = zip(numbers, letters)\n\nprint(list(zipped))\n\n[(1, 'a'), (2, 'b'), (3, 'c')]\n```\n\nArrays의 동등한 명령어는 arrays_zip이라고 부르기 쉽습니다. 그래서 먼저 그것을 사용하여 배열을 \"짜맞추고\", 그런 다음 폭파해야 합니다. Pyspark SQL이나 데이터프레임 작업을 사용하여 구현할 수 있습니다. 다음은 SQL에서의 해결 방법입니다.\n\n```js\nfrom pyspark.sql.functions import *\n\n# 입력 데이터의 데이터베이스 테이블 만들기\n\ndf.createOrReplaceTempView(\"test_table\")\n\nspark.sql(\"select col1, tmp.col2, tmp.col3 from (select col1, explode(tmp) as tmp from (select col1, arrays_zip(col2, col3) as tmp from test_table))\").show()\n\n+------+------+------+\n| col1 | col2 | col3 |\n+------+------+------+\n|   a  |   1  |   4  |\n|   a  |   2  |   5  |\n|   a  |   3  |   6  |\n+------+------+------+\n```\n\ncollect_list 함수\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ncollect_list 함수는 PySpark 데이터프레임 데이터를 레코드 단위로 저장하고 해당 데이터의 컬럼을 컬렉션으로 반환합니다. 이런 면에서, explode 함수의 반대 역할을 합니다. 간단한 예시로 설명드리겠습니다. 만약 다음과 같은 입력 데이터셋이 있다고 가정해봅시다.\n\n```js\ntestData = (['a'],['b'],['c']) \n\ndf = spark.createDataFrame(data=testData, schema = ['letter_column']) \n\ndf.printSchema() \n\ndf.show() \n\n+-------------+ \n|letter_column| \n+-------------+ \n|            a| \n|            b| \n|            c| \n+-------------+ \n```\n\n데이터에 collect_list를 적용한 결과는 다음과 같습니다.\n\n```js\nfrom pyspark.sql.functions import collect_list\n\ndf.select(collect_list(\"letter_column\").alias(\"letter_row\")).show()\n\n+----------+  \n|letter_row|  \n+----------+  \n| [a, b, c]| \n+----------+ \n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n보통은 데이터의 한 열만 다루는 것이 아니기 때문에, 좀 더 복잡한 문제를 고려해보면 다음과 같은 PySpark 데이터프레임이 있는 경우가 있습니다. 이는 3일 동안의 가스 및 전기의 도매 가격을 보여주는 데이터입니다.\n\n```js\n|연료       |      날짜| 가격| \n|-----------|----------|------| \n|Gas        |2019-10-11|121.56| \n|Gas        |2019-10-10|120.56| \n|Electricity|2019-10-11|100.00| \n|Gas        |2019-10-12|119.56| \n|Electricity|2019-10-10| 99.00| \n|Electricity|2019-10-12|101.00| \n```\n\n다음 형식의 새로운 데이터 세트를 반환하려고 합니다. 중요한 점은 각 연료의 가격이 왼쪽에서 오른쪽으로 날짜 순서대로 나열되어야 한다는 것입니다.\n\n```js\n|연료       |Price_hist              |\n|-----------|------------------------|\n|Electricity|[99.0, 100.0, 101.0]    |\n|Gas        |[120.56, 121.56, 119.56]|\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 입력 데이터 세트를 만들기 위한 코드를 작성할 것입니다.\n\n```js\ndata = [\n    (\"가스\", \"2019-10-11\", 121.56),\n    (\"가스\", \"2019-10-10\", 120.56),\n    (\"전기\", \"2019-10-11\", 100.00),\n    (\"가스\", \"2019-10-12\", 119.56),\n    (\"전기\", \"2019-10-10\", 99.00),\n    (\"전기\", \"2019-10-12\", 101.00)\n]\n\n# DataFrame 생성\ndf = spark.createDataFrame(data, [\"연료\", \"날짜\", \"가격\"])\n\n# DataFrame 표시\ndf.show()\n```\n\n이제 코드를 실행하면,\n\n```js\nfrom pyspark.sql.functions import collect_list\n\ndf.select(\"연료\", collect_list(\"가격\").alias(\"가격 이력\")).show(truncate=False)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 코드를 번역하였습니다.\n\n```js\n...\n...\nAnalysisException: [MISSING_GROUP_BY] 쿼리에 GROUP BY 절이 포함되어 있지 않습니다. GROUP BY를 추가하거나 OVER 절을 사용하여 윈도우 함수로 변환하십시오.;\nAggregate [Fuel#2, collect_list(Price#4, 0, 0) AS Price Hist#22]\n+- LogicalRDD [Fuel#2, Date#3, Price#4], false\n```\n\n이 부분은 좋지 않아요. 명확히, 연료 이름에 대해 어떤 종류의 그룹화를 수행해야 합니다. 운이 좋게도, collect_list 함수는 실제로 집계 함수이므로 원하는 결과를 얻기 위해 데이터프레임에서 사전 정렬 작업과 함께 agg 및 groupBy 작업을 사용할 수 있습니다. 다음을 실행하면 원하는 결과를 얻을 수 있습니다.\n\n```js\nfrom pyspark.sql.functions import collect_list\n\n# 날짜 순으로 정렬하여 가격을 날짜 순으로 정렬합니다\nsorted_df = df.sort(\"Fuel\", \"Date\")\n\n# 연료별로 그룹화하여 가격을 목록으로 수집합니다\nresult_df = sorted_df.groupBy(\"Fuel\").agg(collect_list(\"Price\").alias(\"Price_hist\"))\n\n# 결과 DataFrame 표시\nresult_df.show(truncate=False)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요구하는 출력은 다음과 같습니다.\n\n\n| Fuel       | Price_hist              |\n|------------|-------------------------|\n| Electricity| [99.0, 100.0, 101.0]    |\n| Gas        | [120.56, 121.56, 119.56]|\n\n\n## 요약\n\n본 문서에서는 PySpark SQL의 더 이상하고 유용한 데이터 조작 함수 두 가지를 소개했으며, 그들이 귀하에게 어떻게 가치있는지에 대한 사용 사례도 제시했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 데이터프레임에서 배열이나 사전 데이터 필드를 변환하여 별도 레코드로 넣어야 할 경우 explode 함수를 사용하세요.\n\ncollect_list 함수는 explode 함수의 반대로 생각할 수 있습니다. 이를 사용하여 개별 데이터프레임 레코드에서 항목을 컬렉션으로 집계하세요.\n\n이 내용이 마음에 들었다면, 아래 글들도 흥미롭게 보실 수 있을 것 같아요.","ogImage":{"url":"/assets/img/2024-06-23-PySparkExplainedTheexplodeandcollect_listFunctions_0.png"},"coverImage":"/assets/img/2024-06-23-PySparkExplainedTheexplodeandcollect_listFunctions_0.png","tag":["Tech"],"readingTime":11},{"title":"데이터 프로젝트의 코드 우수성을 위한 4가지 R 1부","description":"","date":"2024-06-23 16:22","slug":"2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1","content":"\n\n![이미지](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_0.png)\n\n# 소개\n\n어떤 데이터 과학 프로젝트도 성공에 중요한 키패드 요소는 고품질의 코드입니다. 단순한 데이터 분석부터 복잡한 머신 러닝 파이프라인에 이르기까지, 코드 품질은 프로젝트의 정확성, 효율성 및 유지 보수성을 보장하기 위해 항상 최우선 사항입니다. 잘 쓰여진 코드는 작업이 다른 사람(포함하여 미래의 본인)에 의해 쉽게 이해되고 수정 및 확장될 수 있도록 합니다. 버그와 오류의 가능성을 최소화하고 데이터 및 머신 러닝 프로젝트를 효율적이고 효과적이며 견고하게 만들어줍니다. 그러나 고품질의 코드를 작성하는 것은 항상 쉬운 일이 아닙니다.\n\n우리 모두는 낮은 품질의 코드를 본 적이 있습니다. 그리고 제가 '본다'고 말할 때, 실제로는 '쓴다'는 것을 의미합니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알고 계시죠: 빠른 분석과 증명 개념 모델링 연습에 도전하셨습니다. 그래서 데이터 세트를 CSV 파일로 덤프하고 노트북을 열어서 두 번 실행하면 오류를 발생시키는 암호화된 42개 셀을 만들었어요. 그 결과, 알수 없는 함수 이름, 덮어쓴 변수, 해독할 수 없는 차트, 결국 자신의 머리가 폭발하는지 EC2 인스턴스의 메모리가 폭발하는지 혼란의 소용돌이 속에 빠지게 되었어요.\n\n하지만 물론, 훌륭한 POC 모델은 충분히 작동하기 때문에 어디에 있게 될까요? 바로 프로덕션 환경이죠!\n\n저주받은 일이 생기면, 항상 그렇듯이, 몇 달 후에 다시 자신의 작업을 돌아보면서 정확히 무엇을 했는지 그리고 첫 번째로 어떻게 작동했는지 이해하려고 애를 쓰게 되는 상황이죠.\n\n네, 우리 모두 그런 경험을 해봤지만, 더 이상 그럴 필요가 없어요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 다부분 선언에서는 데이터 프로젝트를 위한 탁월한 코드를 작성하는 데 도움이 되는 4가지 개념(우연히도 모두 'R'로 시작)을 안내하겠습니다. 이 네 가지 R에 기반한 코드베이스를 구축하여 머신러닝 파이프라인 및 정신 건강을 모두 지킬 수 있기를 희망합니다!\n\n참고: 더 간단하게 하기 위해, 이 글의 범위는 데이터 프로젝트를 위한 파이썬 코드 개발로 한정되었지만, 일반적인 개념은 다른 언어로 확장 가능해야 합니다.\n\n# 첫 번째 R: 가독성\n\n![image](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n남이 읽을 걸 감안하고 코드를 작성하세요.\n\n아니, 더 나은 한 가지를 해볼게요:\n\n남이 읽을 걸 염두에 두고 코드를 작성하세요. 좋은 코드라면, 결국 누군가가 읽을 거에요.\n\n가독성은 다른 사람들(미래의 나 포함)이 코드를 읽는 시점과 장소에 관계없이 쉽게 이해하고 해석할 수 있어야 한다는 걸 의미해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 명확한 구조, 의미 있는 변수 이름, 객체 간 간단하고 논리적인 관계, 그리고 충분한 주석과 문서화가 포함됩니다. 가독성은 간단한 이유로 고품질 코드의 가장 중요한 측면입니다:\n\n코드는 쓰여지는 횟수보다 읽히는 횟수가 더 많습니다. 오늘 쓰고 있는 코드를 읽을 사람은 아마도 여러분들 중 어느 누군가가 여섯 달 뒤에 새로운 기능을 추가하거나 코드베이스에서 버그를 추적할 때 일 것입니다. 가독성을 우선시함으로써 여러분의 코드가 다른 사람들이 이해하기 쉬워지는데 그치지 않고, 장래의 여러분들이 정확히 무엇을 했는지 기억하려고 소중한 시간을 낭비하지 않아도 된다는 점에서 장래의 여러분의 시간을 아낄 수 있습니다.\n\n우리 모두가 조금 더 나은 가독성의 코드를 작성하는 데 시간을 투자한다면, 서로의 코드를 읽는 동안 머리를 긁는 시간을 줄일 수 있습니다. 따라서 높은 기준을 유지하고 동료에게 모범되는 행동을 보인다면, 개선된 효율성, 명확한 커뮤니케이션, 더 나은 협업을 통해 모두가 혜택을 받을 수 있습니다. 저는 개인적으로 대략 72%의 시간을 깨끗하고 가독성 있는 코드를 작성합니다. 여러분들은 제게 비해서 더 나은 결과를 얻을 수 있습니다!\n\n가독성 있는 코드를 작성하는 실용적인 팁은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 코딩에 들어가기 전에 잠깐 멈춰보세요\n\n![Image](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_2.png)\n\n코드를 작성하기 전에 생각하는 시간을 가지세요. 떠오르는 첫 번째 해결책이 문제에 대한 가장 좋은 해결책은 아닐 가능성이 높습니다. 올바른 알고리즘을 사용하고 계십니까? 가장 효율적인 데이터 구조를 사용하고 있나요? 필요 없는 중복 변수를 사용하고 있진 않나요? 코드로 작성하기 전에 단순한 영어로 논리를 설명할 수 있나요?\n\n문제를 개념화하지 않고 코드를 작성하면, 대부분 부적절하거나 완전히 잘못된 것을 작성하게 될 것입니다. 그러면 나중에 반드시 되돌아가서 수정해야 할 것입니다. 하지만 코드를 처음부터 다시 쓸 필요는 없습니다. 아니요, 아니요, 필요한 이유가 있기 때문에 이미 이러한 잘못된 해결책을 만드는 데 수십 시간을 투자했으니까요. 그러므로 처음부터 다시 시작하는 대신에 이제 작동하지 않는 솔루션에 수정을 강요하고, 그 외에 대비하기 위해 어떤 패치를 상위에 올려야 할지 고려해야 합니다. 그리고 그것이 아무도 39.5피트 막대로 만지기 싫어하는 나쁜 코드를 만들어내는 완벽한 요리 비법입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시간을 내어 생각해보세요. 무엇을 할 것인지 고려해보세요. 주변을 한번 두루 둘러보세요. 그리고 모두가 분명해지면, 뛰어들 준비가 돼 있나요?!\n\n## 명확하고 명시적인 변수와 함수 이름을 선택하세요\n\n![이미지](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_3.png)\n\n이번에 대해 내가 말할 것을 이미 알고 있으니 말할 필요는 없어요. 제목이 모두 설명해주고 있으니까요. 왜냐하면 매우 명확하고 명시적이기 때문이죠!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n안타깝게도, 저는 대부분의 경우 코드를 작성할 때 이렇게 작성하게 됩니다. 특히 급하게 할 때는요.\n\n```js\n# 나쁜 코드를 보여드리는 중\ndf = pd.read_csv('customer_data.csv')\ndf_2 = df[df['age'] \u003e 30]\n```\n\n하지만 여러분은 저처럼 하지 마세요. 객체와 변수(이 경우 판다스 데이터프레임)에 의미 있는 이름을 선택하세요.\n\n모든 데이터프레임을 df라고 부르는 것은 쉽지만 좋은 생각은 아닙니다. 이 예제에서는 데이터프레임 df에 어떤 데이터가 있고 df_2가 무엇을 나타내는지 즉시 알 수 없습니다. 조금 더 나아지도록 해보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 이 코드는 약간 더 나은 코드입니다\ncustomer_data = pd.read_csv('customer_data.csv')\ncustomer_over_30 = customer_data[customer_data['age'] \u003e 30]\n\n\n이 예제에서는 고객 데이터를 포함하는 원본 데이터프레임의 이름이 더 명확하고, 30세 이상인 고객을 참조하는 두 번째 데이터프레임인 customer_over_30이 더 명확합니다. 훨씬 나아 보이죠? \n\n하지만 과도하게 하는 것은 중요하지 않습니다. 변수 이름을 customers_over_30_active_today_purchased_under_100처럼 길게 지으면 안 됩니다. 이름은 이유당창기술상개발자의 에코메모리에 쉽게 들어맞을 수 있을만큼 짧아야 합니다. 코드를 읽고 변수를 for 루프와 if 문을 통해 추적할 다른 개발자는 빨리 잊지 않을 겁니다. 이름이 모호하면 추적해야 할 것이 무엇인지 곧 잊어버릴 것입니다. 이름이 너무 길 경우 그것이 무엇을 나타내는지 오용하거나 혼란스러울 수 있습니다.\n\n## PEP 8 코딩 스타일을 따르세요\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_4.png](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_4.png)\n\nPEP 8은 이미 알지 못하는 경우를 대비한 Python 코드의 공식 스타일 가이드입니다. 이것은 Python 코드의 가독성을 최대화하기 위한 서식 규칙 세트로, 네이밍 규칙, 공백 사용 여부, 모듈 가져오는 방법 및 순서 등이 포함됩니다.\n\n이 코드 단편은 PEP8 스타일 가이드를 따르지 않습니다.\n\n```python\n# 이 코드는 좋지 않아\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport pandas as pd\nfrom sklearn.model_selection import *\n\ndef loadData(file_path):\n    return pd.read_csv(file_path)\ndef splitData(df):\n    return train_test_split(df.drop('target',1),df['target'])\ndef train_Model(X_train,Y_train):\n    return RandomForestClassifier().fit(X_train,Y_train)\ndf = loadData('some_data.csv')\n\nX_train, X_test, Y_train, Y_test = splitData(df)\nmodel = trainModel(X_train, Y_train)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 많은 주황색 깃발이 있어요! 함수 이름이 snake_case로 되어 있지 않아요. 쉼표 뒤나 연산자 주변에 공백이 없어요. 함수들 사이에는 충분한 공백이나 너무 많은 공백이 있어서 코드를 읽기 어려워요. 전반적으로 말하면, 이 코드는 보기 싫은 코드에요.\n\n여기 PEP 8을 따르는 개선된 버전이 있어요.\n\n```js\n# 이 코드가 더 좋아요\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef load_data(file_path):\n    return pd.read_csv(file_path)\n\n\ndef split_data(df):\n    return train_test_split(df.drop('target', axis=1), df['target'])\n\n\ndef train_model(features_train, target_train):\n    return RandomForestClassifier().fit(features_train, target_train)\n\n\ndf = load_data('some_data.csv')\nfeatures_train, features_test, target_train, target_test = split_data(df)\nmodel = train_model(features_train, target_train)\r\n```\n\n함수 이름은 snake_case로 되어 있고, 쉼표 뒤와 연산자 주변에 공백이 있어요. 함수들 사이에는 두 줄의 공백이 있어요. 추가로 변수 이름이 약간 더 유의미하도록 변경되었어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPEP 8 스타일을 따르는 코드를 보장해주는 서식 지원 도구가 있습니다(린팅 및 서식 지정 부분을 참조하세요), 하지만 처음부터 깔끔하고 가독성이 좋은 코드를 작성하기 위한 가장 중요한 규칙을 알고 있어야 합니다.\n\n## 가능한 한 코드에 내부 문서를 추가하세요\n\n![image](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_5.png)\n\n깔끔한 코드는 문서의 자리 표시자가 아닙니다. 항상 코드에 두 가지 유형의 내부 문서를 추가해야 합니다: (1) 코드가 의도한 작업, (2) 코드를 사용하는 방법. 파이썬에서는 일반적으로 인라인 주석과 독스트링의 조합으로 제공됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기는 잘 문서화된 코드 예시입니다:\n\n```js\n# 이 코드가 훨씬 좋아졌어요\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n```\n\n```js\ndef load_data(file_path):\n    \"\"\"\n    csv 파일에서 데이터를 불러옵니다.\n\n    Parameters:\n        file_path (str): csv 파일의 경로.\n\n    Returns:\n        DataFrame: 불러온 데이터를 pandas DataFrame으로 반환합니다.\n    \"\"\"\n    \n    # pandas의 read_csv 함수를 사용하여 csv 파일에서 데이터를 불러옵니다.\n    loaded_data = pd.read_csv(file_path)\n    \n    return loaded_data\n\n\ndef split_data(df, test_size=0.2):\n    \"\"\"\n    데이터를 훈련 및 테스트 세트로 나누는 함수입니다.\n\n    Parameters:\n        df (DataFrame): 나눌 데이터.\n        test_size (float): 테스트 세트의 비율.\n\n    Returns:\n        X_train, X_test, y_train, y_test (DataFrame, DataFrame, Series, Series): \n            훈련 및 테스트 데이터가 특징과 대상으로 분할됩니다.\n    \"\"\"\n    \n    # sklearn의 train_test_split 함수를 사용하여 데이터를 \n    # 훈련 및 테스트 세트로 나눕니다.\n    X_train, X_test, y_train, y_test = \\\n      train_test_split(df.drop('target', axis=1), \n                       df['target'], \n                       test_size=test_size, \n                       random_state=42,\n                       )\n    \n    return X_train, X_test, y_train, y_test\n\n\ndef train_model(features_train, target_train):\n    \"\"\"\n    RandomForestClassifier 모델을 훈련시키는 함수입니다.\n\n    Parameters:\n        features_train (DataFrame): 훈련 특징.\n        target_train (Series): 훈련 대상.\n\n    Returns:\n        RandomForestClassifier: 훈련된 모델.\n    \"\"\"\n    \n    # RandomForestClassifier를 인스턴스화하고 훈련 데이터에 맞춥니다.\n    model = RandomForestClassifier().fit(features_train, target_train)\n    \n    return model\n\n\n# csv 파일에서 데이터를 불러옵니다.\ndf = load_data('some_data.csv')\n\n# 불러온 데이터를 훈련 및 테스트 세트로 나눕니다.\nfeatures_train, features_test, target_train, target_test = split_data(df)\n\n# 훈련 데이터로 RandomForestClassifier 모델을 훈련합니다.\nmodel = train_model(features_train, target_train)\n```\n\n직업 생활을 하면서 본 뛰어난 품질의 코드들은 대개 자세한 설명과 함께 함수 또는 클래스의 목적, 매개변수 및 반환 값에 대한 문서화 문자열(docstring)이 포함되어 있습니다(예시가 포함되어 있으면 더 좋습니다!). 그러나 코드 자체에 인라인 주석이 부족하다는 점에 항상 실망합니다. 이러한 주석은 특정 코드 라인의 의도를 명확히 하기에 중요하며, 코드베이스를 더 이해하기 쉽고 유지보수하기 용이하며, 무엇보다도 확장 가능하게 만들어 줍니다. 인라인 주석을 쓰는 데 너무 많다고 두려워하지 마세요. 코드를 인라인 문서화로 설명하는 데 공간이 너무 많을 순 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n좋은 실천 방법은 당신이 이루고자 하는 로직을 설명하는 한 줄씩 주석을 작성하고 이를 가짜 코드로 취급한 후 실제 코드가 이어지도록 하는 것일 수 있습니다. 예를 들어:\n\n```js\n# 초기화된 결과를 저장할 빈 리스트를 만듭니다\nresults = []\n\n# 0부터 10까지의 범위를 반복합니다\nfor i in range(10):\n    # 숫자가 짝수인지 확인합니다\n    if i % 2 == 0:\n        # 숫자가 짝수인 경우 결과 리스트에 추가합니다\n        results.append(i)\n\nprint(results)\n# 출력: [0, 2, 4, 6, 8]\n```\n\n여기서 실수를 발견했나요?\n\n0부터 10까지 반복하려 했지만 결과는 8까지만 보입니다. 무슨 일이 일어났을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오, 저희는 10을 포함하지 않는 range(10)을 사용했네요. 어이가 없네요!\n\n코드는 괜찮아요. 매우 가독성이 좋아요. 그 안에 명백한 오류가 없어요. 유일한 문제는 프로그래머가 그것을 수행하려고 의도한 대로 동작하지 않는다는 점이에요.\n\n여기서 문제의 심각성은 분명히 지나치게 과장되었지만, 이 간단한 예제를 사용하여 한 가지 사실을 설명하려고 해요.\n\n코드 라인은 프로그램이 하는 일을 보여줍니다. 인라인 코멘트는 프로그래머가 그것을 수행하려고 의도한 것을 보여줍니다. 의도와 현실 간에 불일치가 있는 경우 인라인 코멘트를 통해 코드의 맨 끝으로 스크롤하는 것보다 그 간극을 식별하고 깨다리는 데 훨씬 빨리 도와줄 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드에 주석과 문서를 작성하는 것은 항상 부담스러울 수 있습니다. 특히 이미 깨끗한 코드를 작성하는 데 많은 노력을 기울이고 있을 때 더 그렇습니다. 코드가 매우 가독성이 높고 이해하기 쉽다면, 무슨 일을 하는지 설명하는 주석을 작성할 필요가 없다고 주장할 수 있습니다. 간단한 경우에는 사실이 될 수 있지만, 다소 복잡한 알고리즘을 구현할 때는 문제가 생길 수 있습니다. 깨끗한 코드와 깨끗한 문서는 서로 다른 목적을 위해 필요하며 놀라운 데이터 프로젝트를 위해 둘 다 필요합니다.\n\n참고: 여기서 말씀드리는 것은 사용자 및 개발자가 코드 일부를 이해하는 데 도움이 되도록 함수 또는 클래스 수준에서 제공해야 하는 최소한의 내부 문서에 대한 것입니다. 이는 (가능한 경우) 모듈/패키지 수준의 (잠재적으로 외부) 문서와는 별개이며 사용자에게 어떻게 연결되는지에 대해 높은 수준에서 설명하는 데 필요한 것입니다.\n\n## 다른 사람에게 코드를 읽어보라고 요청하세요 (가능하다면)\n\n![이미지](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드가 가독성이 좋은지 알아보는 가장 좋은 방법은요... 맞죠, 당신이 예상한 대로: 그것을 읽어보는 겁니다!\n\n일반적으로 협업 중에는 코드 리뷰를 통해 이루어집니다(GitHub 및 Bitbucket의 풀 리퀘스트 또는 GitLab의 병합 리퀘스트를 통해). 여기서의 도전 과제는 사람들이 일반적으로 코드 리뷰를 하는 걸 좋아하지 않는다는 점입니다. 솔직히 말해서, 코드 리뷰는 지루할 수 있어요. 하지만, 코드 리뷰는 큰 선배 개발자들에 의한 코드 기여를 검토할 때 학습 경험이 될 수도 있고, 주니어 개발자들이 작성한 코드를 검토할 때 가르침의 기회가 될 수도 있어요.\n\n코드를 검토할 때는 긍정적이고 지지적인 태도를 가지는 것이 정말 중요해요. 여러분의 코멘트가 그것을 읽는 사람에게 영향을 줄 수 있다는 걸 명심하고, 가혹한 비판보다 건설적인 피드백을 제공하려 노력해 보세요. 기억하세요, 여러분은 개발자가 아니라 코드를 검토하는 중이에요.\n\n반면에 코드 리뷰 중에 피드백을 받을 때는 목표가 개인적으로 비판하는 게 아니라 코드의 품질을 향상시키는 데 있다는 것을 염두에 두세요. 자존심을 버리세요. 여러분과 여러분의 코드 둘 다 완벽하지 않다는 걸 기억하세요. 따라서 다른 시각을 수용할 수 있도록 열려 있으며, 이를 통해 훨씬 더 나은 해결책으로 이끌 수 있어요. 또한, 여러분의 시간과 피드백에 감사를 표하는 것을 잊지 말고 항상 감사해하세요. 그들은 자신들의 시간을 더 즐거운 일에 할애할 수도 있지만, 여러분과 여러분의 코드의 품질에 전념하기로 결정했기 때문이에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Linting 및 형식 지정 도구 사용\n\n\u003cimg src=\"/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_7.png\" /\u003e\n\nLinting 및 형식 지정은 코드를 더 읽기 쉽게 만드는 데 매우 도움이 될 수 있습니다 (또한 견고성을 높입니다). 그러나 이들은 정확히 무엇일까요?\n\nLinting은 코드를 분석하고 잠재적인 오류와 불일치를 식별하는 프로그램을 실행하는 프로세스입니다. 일반적으로 사용자 정의할 수 있는 일정한 코드 품질 수준을 유지하는 데 도움을 줍니다. 예를 들어, numpy를 가져왔지만 실제로 사용하지 않는 경우, 린터는 이를 감지하고 가져오기 문을 제거할 수 있도록 알려줍니다. 이는 잠재적인 오류와 \"음성\" 버그를 피하는 데 도움이 됩니다. 예를 들어 변수를 None과 비교할 때 x == None 대신 x is None을 사용하지 않아 코드가 실패하는 상황이 있었습니다. 코드가 왜 나를 싫어하는지 두 날 동안 골머리를 썼었죠. Linting은 그런 머리 아픔을 방지할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n포맷팅은 코드를 읽기 쉽고 세련되게 만드는 데 관련된 것이에요. 이는 코드를 특정한 미리 정의된 규칙이나 표준에 따라 조직화하거나 조정하는 것을 포함해요. 예를 들어, 슈퍼 멋진 데이터 처리 모듈을 작성했지만 보기에 정말 추한 느낌이었고, 다른 사람들과 공유하기가 조금 부끄러웠던 적이 있나요?\n\n코드 포맷팅은 그런 부분을 다루는 데 도움이 될 수 있어요.여러분의 코드가 깔끔하게 정리되면 좋은 점이죠.\n\n```js\n# 데이터 처리 함수의 극악적인 가상 예시...\n# 집에서 시도하지 마세요!\ndef process_data(\n    data_source,\n    filter_function,\n    transform_function,\n    group_function,\n    aggregate_function,\n    normalize_function,\n    save_function,\n    destination,\n):\n    data = load_data(data_source)\n    \n    data = filter_function(data)\n    data = transform_function(data)\n    data = group_function(data)\n    data = aggregate_function(data)\n    data = normalize_function(data)\n    save_function(data, destination)\n```\n\n코드의 형식을 잘 맞춰주면 이렇게 변할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인기 있는 코드 린터인 pylint와 flake8, 그리고 코드 가독성을 향상시키는 Black 같은 포매터 등이 있습니다. 저는 개인적으로 linting과 formatting을 위해 Ruff를 사용합니다. Ruff는 빠르고 매우 유연하여 코드 가독성을 유지하는 데 좋은 도구로 CI/CD 파이프라인에 추가할 수 있습니다. Ruff를 사용하는 방법에 대해 더 알고 싶다면 댓글에서 알려주세요. 나중에 이에 대한 글을 쓸지도 모르겠어요.\n\n## 지겹은 일은 AI 비서에게 맡기세요\n\n![AI assistants](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_8.png)\n\nAI 비서를 활용하면 수고스러운 코딩 작업을 자동화하고 동시에 코드를 더 가독성 있게 만들 수 있습니다. 아니, ChatGPT를 사용하여 코드를 작성하는 게 아닙니다. 간단한 유틸리티 함수나 판다스 데이터 작업이 필요하다면 AI에 맡기세요. 그렇게 하고 새로운 시간을 활용하여 푸어 오버 커피 한 잔을 즐기세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI를 사용하여 보일러플레이트 코드를 많이 작성하고 인라인 주석을 작성하며 코드에 대한 문서를 생성하는 것에 대해 이야기하고 있어요. GitHub Copilot과 같은 AI 어시스턴트는 타이핑하는 시간을 많이 절약할 수 있어요. 솔직히 말하면, 작성할 코드 중 많은 부분이 반드시 창의적이지 않아요. 이는 보일러플레이트 코드, 루틴 작업 및 반복 요소를 포함하고 있어요. 특히 유틸리티 함수에 대한 독스트링과 간단한 단위 테스트에 대해 이는 특히 사실이에요. Copilot은 이러한 일상적인 작업을 빠르게 처리할 수 있어요. 그래서 코드의 복잡하고 창의적인 측면에 더 많은 시간을 쏟을 수 있게 되고, 다른 사람 개발자들에게도 논리가 이해하기 쉽고 따라가기 쉽도록 만들어주게 되요.\n\n지금까지 Copilot을 사용하면서, 내가 의도한 대로 복잡한 논리가 들어간 탄탄한 코드를 작성하는 데 어려움이 있어요(아니면 내가 그냥 나쁜 프롬프트 엔지니어인지 모르겠어요). 그러나 인라인 주석, 독스트링 및 보일러플레이트 코드에 대한 자동 완성을 제안하는 데 놀랍도록 훌륭한 일을 합니다. 그래서 늘 뇌를 아끼지는 못하지만, 확실히 키보드 작업 시간을 많이 아낄 수 있고 — 보너스로 코드를 더 읽기 쉽게 만들어줍니다.\n\n어시스턴트로 사용하되, 그것을 지팡이로 만들지 않도록 주의하세요. 코드 작성을 위해 AI에 너무 의존하는 것에 대해 신중해지세요. 왜냐하면 어떤 근육이든 꾸준히 사용하지 않으면 그 근육은 분명히 약화되기 마련이기 때문이에요.\n\n저는 데이터 프로젝트의 코드 훌륭성을 위한 네 가지 R 중 첫 번째인 코드 가독성에 집중한 이 부분이 유용하고 실용적이었으면 좋겠어요. 여기서 공유한 팁들이 더 나은 코드를 작성하고 더 견고한 머신러닝 파이프라인을 구축하는 데 도움이 되기를 바랍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금 두 번째 R이 무엇을 의미하는지 추측할 수 있나요?\n\n다음 파트도 계속 기대해주세요!\n\n향후 게시글을 업데이트하려면 Medium에서 저를 팔로우해주십시오. 이 시리즈의 두 번째 부분 또한 포함될 예정입니다. 이 글에 대한 여러분의 생각을 듣고 싶으니 아래나 옆에 댓글을 남겨주세요. 또는 여러분의 장치에서 댓글 섹션이 있는 곳에 남겨주세요. 추가로 논의하고 싶은 질문이나 사항이 있으면 LinkedIn에서 연락주세요. 여러분을 만나 기뻐할 것입니다!\n\n기억하세요, 학습의 여정은 길고 계속됩니다. 기술을 능숙하게 유지하고 지식을 최신 상태로 유지하며 시야를 넓혀가세요. 더 나은 코드를 작성하고 독특하며 탁월한 데이터 및 머신러닝 프로젝트를 구축하는 데 건배해봅시다! 🍻","ogImage":{"url":"/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_0.png"},"coverImage":"/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_0.png","tag":["Tech"],"readingTime":14},{"title":"Fabric 완벽 마스터하기 실시간 주식 데이터 스트리밍 및 분석 방법","description":"","date":"2024-06-23 16:20","slug":"2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis","content":"\n\n\n![2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_0.png](/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_0.png)\n\n실시간 주식 시장 데이터를 관리하고 분석하기 위해 Fabric 이벤트 스트림을 사용하세요. 시간, 심볼, 가격 및 거래량과 같은 필드를 포함하는 이 데이터는 실시간 이벤트를 시뮬레이션하고 KQL 데이터베이스를 사용하여 분석하는 데 사용됩니다.\n\n## Fabrics에서 사용되는 구성 요소:\n\n- Fabric Event Stream\n- Azure Data Explorer (KQL Database)\n- Power BI\n- KQL Query\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_1.png)\n\n## 실시간 주식 데이터 설정 및 분석 단계\n\n- 워크스페이스 생성:\n\n1. Microsoft Fabric에 로그인: Microsoft Fabric에 로그인하고 Power BI를 선택합니다.\n2. 워크스페이스 생성: 메뉴 바에서 Workspaces를 선택하고 자신이 원하는 이름으로 새 워크스페이스를 만들어 Fabric 용량(Trial, Premium, 또는 Fabric)이 있는지 확인하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. 실시간 인텔리전스 기능 활성화: 파워 BI 포털의 왼쪽 하단에 있는 파워 BI 아이콘을 선택하고 실시간 인텔리전스 경험으로 전환하세요.\n\n3. KQL 데이터베이스 생성:\n\n- 이름을 선택하고 새 데이터베이스를 생성하세요.\n- OneLake 활성화: 데이터베이스 세부 정보 패널에서 OneLake에서 사용 가능하도록 설정하세요.\n\n![이미지](/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4. 이벤트스트림 만들기: 리얼타임 인텔리전스 메뉴에서 이벤트스트림(미리보기)를 선택하고 이름을 지정하세요. Microsoft Fabric의 이벤트스트림은 코드를 필요로하지 않고 실시간 이벤트를 캡처하고 변환하여 다양한 대상으로 라우팅합니다.\n\n5. 샘플 데이터 소스 추가:\n\n- 이벤트스트림 캔버스에서 새 출처를 추가하고 샘플 데이터를 선택하세요.\n- 샘플 데이터 소스의 이름을 지정하고 필요한 값으로 구성하세요.\n- 구성을 적용하려면 이벤트스트림을 게시하세요.\n\n![이미지](/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_4.png\" /\u003e\n\n6. **KQL Database를 Eventstream 대상으로 설정하기**\n\n- **대상 추가**: Eventstream 캔버스에서 대상 추가를 선택하고 KQL Database를 선택합니다.\n- **대상 구성**: KQL Database에 대한 세부 정보를 입력하고 데이터 삽입 모드를 선택하고 대상의 이름을 지정한 후 구성을 저장합니다.\n\n7. **이벤트 변환**: Eventstream 캔버스에서 그룹화와 같은 변환 이벤트를 추가하고 필요에 맞게 구성합니다. 해당 이벤트를 이벤트 스트림에 연결하고 변경 사항을 발행합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## KQL을 사용하여 데이터 분석하기\n\nKusto Query Language (KQL)은 KQL 데이터베이스에서 데이터를 조회하는 데 사용됩니다. 주식 데이터를 분석하는 몇 가지 예시 쿼리가 있습니다:\n\n- 평균 매수-매도 스프레드\nStockData_Table\n| extend bidPrice = todouble(bidPrice), askPrice = todouble(askPrice)\n| extend BidAskSpread = askPrice — bidPrice\n| summarize AvgBidAskSpread = round(avg(BidAskSpread),2) by symbol\n- 평균 거래량\nStockData_Table\n| extend volume = todouble(volume)\n| summarize AvgVolume = round(avg(volume),2) by symbol\n- 가격 변동률\nStockData_Table\n| extend lastSalePrice = todouble(lastSalePrice)\n| order by symbol, ['time'] asc\n| extend PrevPrice = prev(lastSalePrice, 1)\n| where isnotnull(PrevPrice)\n| extend PriceChangePercent = ((lastSalePrice — PrevPrice) / PrevPrice) * 100\n| summarize AvgPriceChangePercent = round(avg(PriceChangePercent),2) by symbol\n- 시장 점유율 백분율\nStockData_Table\n| extend marketPercent = todouble(marketPercent)\n| summarize AvgMarketSharePercent = avg(marketPercent) by symbol\n\n\u003cimg src=\"/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_5.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 시각화를 위한 Power BI 활용\n\n- KQL 데이터베이스에 연결: Power BI를 사용하여 KQL 데이터베이스에 연결하고 분석된 데이터를 기반으로 시각화를 생성하세요.\n- 대시보드 생성: 평균 입찰-요청 스프레드, 평균 거래량, 시장 점유율 등 핵심 성과 지표(KPI)를 시각화하기 위한 대화형 대시보드를 구성하세요.\n\n본 프로젝트는 Microsoft Fabric의 Eventstream을 활용하여 실시간 데이터 스트리밍, 분석, 시각화를 실현하며 KQL 및 Power BI를 사용하여 주식 시장 데이터에 대한 귀중한 통찰력을 제공합니다.","ogImage":{"url":"/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_0.png"},"coverImage":"/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_0.png","tag":["Tech"],"readingTime":4},{"title":"데이터 아키텍처 간단 개요","description":"","date":"2024-06-23 16:18","slug":"2024-06-23-DataArchitectureABriefOverview","content":"\n\n데이터 아키텍처는 성공적인 데이터 엔지니어링의 중요한 요소입니다. 조직 전반에서 데이터 수집, 저장, 처리 및 활용 방식의 기초를 제공합니다. 잘 설계된 데이터 아키텍처를 통해 기업은 원활한 데이터 통합, 높은 성능, 확장성 및 효과적인 데이터 거버넌스를 달성할 수 있습니다.\n\n이 블로그 포스트에서는 우수한 데이터 아키텍처의 핵심 원칙을 탐구하고 핵심 아키텍처 개념을 논의하며 견고하고 확장 가능한 데이터 시스템을 설계하는 데 도움이 되는 다양한 데이터 아키텍처 패턴을 살펴볼 것입니다.\n\n# 데이터 아키텍처란 무엇인가요?\n\n데이터 아키텍처는 조직 내에서 데이터가 수집, 저장, 관리되고 사용되는 방식을 개요로 설명하는 구조화된 프레임워크를 의미합니다. 데이터 흐름, 데이터 모델 및 데이터 처리에 사용되는 기술을 정의합니다. 견고한 데이터 아키텍처는 데이터가 다양한 비즈니스 요구에 대해 접근 가능하고 신뢰할 수 있으며 관련성이 있도록 보장하여 효율적인 의사 결정과 운영 효율성을 촉진합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 아키텍처는 기업 아키텍처의 하위 집합으로, 조직의 전체 IT 인프라 및 프로세스를 포괄합니다. 기업 아키텍처에는 비즈니스, 애플리케이션, 기술, 데이터 아키텍처와 같은 다양한 구성 요소가 포함됩니다. 기업 아키텍처의 더 넓은 맥락을 이해하면 데이터 아키텍처가 조직의 전체 전략 내에서 어떤 역할과 중요성을 발휘하는지를 이해할 수 있습니다.\n\n\\[이미지는 여기에 들어가는데, 이미지의 경로가 이상해서 제대로 표시되지 않았어요!]\n\n# 데이터 아키텍처의 진화\n\n과거 몇 10년 동안 데이터 아키텍처는 효율적인 데이터 분석의 필요성 증가와 데이터를 비즈니스 이익으로 활용하기 위한 데이터 전략의 현대화로 인해 상당한 변화를 겪었습니다. 이러한 진화는 각각이 특정 아키텍처 관행과 기술로 특징 지어지는 여러 중요한 단계로 나타납니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2000년 이전: 기업 데이터 웨어하우스 (EDW) 시대\n\n이 기간 동안 주목된 것은 기업 데이터 웨어하우스 (EDW)의 성공과 구현이었습니다. 조직은 다양한 소스로부터 데이터를 저장, 관리 및 분석할 수 있는 중앙 저장소를 생성하기 위해 노력했습니다. 주요 목표는 구조화되고 일관된 데이터 저장을 통해 비즈니스 인텔리전스 및 보고 요구를 지원하는 것이었습니다.\n\n특징:\n\n- 중앙 데이터 저장\n- 정의된 스키마를 가진 구조화된 데이터\n- 비즈니스 인텔리전스 및 보고에 초점을 맞춤\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한계:\n\n- 시행 및 유지 관리 비용 증가\n- 증가하는 데이터 양으로 인한 확장성 문제\n- 다양한 데이터 원본 통합의 어려움\n\n## 2000년부터 2010년까지: EDW 후기\n\n이 기간에는 전통적인 데이터 웨어하우스의 한계에 대한 대처 방향으로의 변화가 있었습니다. 조직들은 분산된 데이터와 데이터 실로에 대한 도전에 직면하며, 일관성 없고 조각난 분석을 유발했습니다. 데이터 마트가 인기를 얻게 되었는데, 이는 부서가 자체 데이터를 관리할 수 있게 해 주었지만, 더 많은 분열을 유발했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특징:\n\n- 데이터 웨어하우스 및 데이터 마트에 의존\n- 조각난 데이터 분석\n- 데이터 사일로의 등장\n\n한계:\n\n- 부서간 일관되지 않은 데이터 분석\n- 기업 전체 통찰력을 위한 데이터 통합의 어려움\n- 여러 데이터 시스템 유지비용 증가\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2010년부터 2020년까지: 논리 데이터 웨어하우스 (LDW) 시대\n\n논리 데이터 웨어하우스(LDW)는 단편화된 데이터 환경에 대한 해결책으로 등장했습니다. LDW는 데이터 웨어하우스, 데이터 마트 및 데이터 레이크를 포함한 다양한 저장 시스템에서의 데이터 접근을 통합하는 공통 의미 계층을 도입했습니다. 이 접근 방식은 더 통합된 데이터 분석과 향상된 데이터 접근성을 가능케 했습니다.\n\n주요 특징:\n\n- 공통 의미 계층을 통한 통합된 데이터 접근\n- 데이터 웨어하우스, 데이터 마트 및 데이터 레이크의 통합\n- 향상된 데이터 분석 기능\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n혜택:\n\n- 데이터 일관성 및 통합이 향상됩니다.\n- 확장성과 유연성이 향상됩니다.\n- 대규모 데이터 및 고급 분석에 대한 지원이 좋아집니다.\n\n제한 사항:\n\n- 여러 데이터 저장 시스템에 대한 계속된 의존이 필요합니다.\n- Semantic 레이어 관리의 복잡성이 증가합니다.\n- LDW를 구현하고 유지하는 데 전문 기술이 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2020년부터의 시대: 증강 데이터 분석과 활성 메타데이터의 시대\n\n현재와 미래의 데이터 아키텍처 단계는 인공 지능, 기계 학습 및 데이터 조정과 같은 첨단 기술에 의해 주도되는 증강 데이터 분석의 부상으로 특징 지어집니다. 이 시대는 데이터 접근의 민주화와 셀프 서비스 분석을 가능하게 하는 활성 메타데이터를 기반으로 합니다.\n\n특징:\n\n- AI와 기계 학습을 활용한 증강 데이터 분석\n- 메타데이터를 활용한 데이터 셀프 서비스\n- 고급 데이터 조정 및 추천 엔진\n- 적응적인 실천과 활성 메타데이터 분석\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n혜택:\n\n- 향상된 데이터 접근성 및 셀프 서비스 기능\n- 고급 분석을 통한 의사 결정 개선\n- 변화하는 비즈니스 요구에 대한 적응 능력 향상\n\n활성 메타데이터: 메타데이터는 오늘날 핵심 역할을 담당하며 데이터의 다양한 측면을 설명하고 지능적인 데이터 관리를 가능케 합니다. 네 가지 유형의 메타데이터가 있습니다:\n\n- 기술 메타데이터: 데이터 저장, 구조 및 처리에 관한 정보.\n- 운영 메타데이터: 데이터 워크플로우, 프로세스 및 사용에 대한 데이터.\n- 비즈니스 메타데이터: 데이터의 비즈니스 의미와 사용에 대한 맥락 정보.\n- 소셜 메타데이터: 데이터 상호작용 및 사용 패턴에서 유도된 인사이트.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n활성 메타데이터는 데이터를 설명하는 것뿐만 아니라 시스템 간 작업을 식별하여 더 동적이고 지능적인 데이터 작업을 용이하게 합니다.\n\n사용 사례:\n\n- 마스터 데이터 관리\n- 기업 간 데이터 교환\n- 애플리케이션 데이터 통합\n- 파트너 데이터 교환\n\n활성 메타데이터와 증강 분석으로 발전하는 것은 데이터 아키텍처의 능력을 전통적인 방식을 넘어서게 하여 다양한 사용 사례에 걸쳐 더 정교한 데이터 관리와 분석을 가능케 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 주요 아키텍처 원칙과 개념\n\n핵심 아키텍처 원칙과 개념을 이해하는 것은 효과적인 데이터 아키텍처를 설계하는 데 중요합니다. 이러한 원칙들은 견고하고 확장 가능하며 유연한 데이터 시스템을 구축하는 데 기반을 제공합니다.\n\n## 주요 기업의 원칙\n\n주요 기업들은 자체 아키텍처 원칙을 따라 자신들의 시스템의 효과성과 효율성을 보장합니다. 예를 들어:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAWS 웰 아키텍처 프레임워크에는 여섯 가지 핵심이 있습니다:\n\n- 운영 우수성\n- 보안\n- 신뢰성\n- 성능 효율성\n- 비용 최적화\n- 지속 가능성\n\n구글 클라우드의 클라우드 네이티브 아키텍처를 위한 다섯 가지 원칙은 다음과 같습니다:\n\n- 자동화를 위한 디자인\n- 상태 관리에 똑똑하게\n- 관리형 서비스 우선\n- 깊이 우선 방어 실천\n- 항상 아키텍처 구축중임.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 프레임워크와 \"데이터 엔지니어링 기본\"과 같은 다른 리소스에서 영감을 받아, 여기에는 좋은 데이터 아키텍처를 설계하는 데 필수적인 몇 가지 조직화된 원칙과 개념이 있습니다.\n\n## 도메인과 서비스\n\n도메인은 데이터 아키텍처가 지원하는 특정 영역의 비즈니스 또는 지식입니다. 서비스는 도메인 내에서 특정 작업을 수행하는 기능입니다.\n\n혜택:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 조직적 명확성: 명확히 정의된 도메인과 서비스는 데이터와 프로세스를 조직화하는 데 도움이 됩니다.\n- 집중된 개발: 팀은 특정 서비스에 집중하여 효율성과 전문성을 향상시킬 수 있습니다.\n\n## 분산 시스템\n\n분산 시스템은 서로 다른 네트워크 컴퓨터에 위치한 구성 요소가 통신하고 조정하여 공통 목표를 달성하는 시스템입니다. 확장 가능하고 강인한 데이터 아키텍처를 구축하는 데 필수적입니다.\n\n혜택:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 확장성: 더 많은 노드를 추가하여 부하 처리 용이.\n- 신뢰성: 중복된 노드는 일부 노드가 실패해도 시스템 가용성을 보장합니다.\n\n## 확장성과 탄력성\n\n확장성은 시스템이 자원을 추가함으로써 증가하는 작업 양을 처리하는 능력입니다. 탄력성은 수요에 따라 자원을 동적으로 확장 또는 축소하는 능력입니다.\n\n혜택:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 비용 효율성: 사용한 리소스에 대해서만 지불하세요.\n- 성능 최적화: 피크 시간에는 성능을 유지하고 낮은 사용량 기간에는 비용을 최적화하세요.\n\n## 가용성 및 신뢰성\n\n가용성은 시스템이 운영 및 접근 가능한 시간의 백분율입니다. 신뢰성은 시스템이 시간이 지남에 따라 올바르고 일관되게 작동하는 능력입니다.\n\n주요 지표:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 가동 시간: 시스템이 작동 중인 시간의 백분율.\n- 고장 사이의 평균 시간 (MTBF): 시스템 고장 사이의 평균 시간.\n- 복구에 소요되는 평균 시간 (MTTR): 고장 난 시스템을 수리하는 데 걸리는 평균 시간.\n\n전략:\n\n- 중복성: 고장 시 대체 시스템을 구현.\n- 장애 조치 기구: 고장 시 대기 시스템으로 자동 전환.\n\n## 이벤트 주도형 아키텍처\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이벤트 주도 아키텍처는 시스템이 이벤트나 상태 변경에 대응하는 설계 패러다임입니다. 이 접근 방식은 높은 결합도를 갖추고, 유연성과 확장성을 촉진합니다.\n\n활용 사례:\n\n- 실시간 분석\n- 사물 인터넷 시스템\n- 통지 시스템\n\n## 사용자 액세스: 단일 vs. 멀티테넌트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSingle-tenant 아키텍처는 각 고객에게 소프트웨어 및 데이터베이스의 전용 인스턴스를 할당하며, 멀티테넌트 아키텍처는 다중 고객 사이에서 리소스를 공유합니다.\n\n고려해야 할 사항:\n\n- 성능: Single-tenant는 더 나은 성능을 제공할 수 있지만, 멀티테넌트는 비용 효율적입니다.\n- 보안: 멀티테넌트 시스템은 다른 고객들을 위한 데이터 격리 및 보안을 보장해야 합니다.\n- 유지 보수: 멀티테넌트 시스템은 중앙에서 유지 및 업데이트하기 쉽습니다.\n\n이러한 원칙을 준수하고 이러한 개념을 이해하여 현대 비즈니스 환경의 요구 사항을 충족하면서 확장 가능하고 신뢰할 수 있으며 비용 효율적인 데이터 아키텍처를 설계할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 주요 데이터 아키텍처 패턴\n\n지난 수십 년간 데이터 아키텍처에서는 효과적으로 데이터를 관리하고 활용하기 위한 필수 요소로 여러 가지 핵심 패턴이 나타났습니다. 각 패턴에는 강점, 약점 및 이상적인 사용 사례가 있습니다. 여기에서 이러한 패턴을 간단히 살펴보고, 다양한 맥락에서 어떻게 적용할 수 있는지 설명하겠습니다.\n\n## 1. 데이터 웨어하우스\n\n데이터 웨어하우스는 구조화된 데이터를 저장하고 보고 및 분석하기 위해 설계된 중앙 저장소입니다. 쿼리 성능과 데이터 집계에 최적화되어 있어 비즈니스 인텔리전스 애플리케이션에 이상적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 해당 데이터 아키텍처에 대한 간략한 설명입니다:\n\n특징:\n\n- 중앙 집중 저장: 여러 출처에서 수집한 데이터를 한 곳에 저장합니다.\n- 구조화된 데이터: 데이터가 체계적으로 구성되어 일반적으로 스키마를 따릅니다.\n- 분석 최적화: 빠른 쿼리 및 데이터 검색을 위해 설계되었습니다.\n\n혜택:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 높은 성능: 복잡한 쿼리와 데이터 분석에 최적화되어 있습니다.\n- 데이터 통합: 다양한 소스에서 데이터를 통합하여 통일된 뷰를 제공합니다.\n- 일관성: 스키마 강제화를 통해 데이터 품질과 일관성을 준수합니다.\n\n사용 사례:\n\n- 비즈니스 인텔리전스 및 보고\n- 과거 데이터 분석\n- 다중 시스템에서 데이터 통합\n\n## 2. 데이터 레이크\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nData lakes는 방대한 양의 원시, 비구조화 및 반구조화 데이터를 저장합니다. 데이터 웨어하우스와 달리 데이터 레이크는 데이터에 엄격한 스키마를 부과하지 않아 데이터 수집 및 저장에 더 많은 유연성을 제공합니다.\n\n![DataArchitectureABriefOverview_2.png](/assets/img/2024-06-23-DataArchitectureABriefOverview_2.png)\n\n특징:\n\n- 원시 데이터 저장: 데이터를 원시 형식으로 저장합니다.\n- 확장성: 대량의 데이터를 처리할 수 있습니다.\n- 스키마형 읽기: 데이터 저장 시가 아닌 데이터를 읽을 때 스키마가 적용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n장점:\n\n- 유연성: 구조화된, 비구조화된 및 반구조화된 데이터를 포함한 다양한 데이터 유형을 저장할 수 있습니다.\n- 확장성: 대규모 데이터 작업을 처리하기에 적합합니다.\n- 비용 효율성: 데이터 웨어하우스와 비교하여 대량의 데이터를 저장하는 데 비용이 저렴할 수 있습니다.\n\n도전:\n\n- 데이터 관리: 적절한 관리 없이 데이터 레이크는 데이터 스왐프가 될 수 있습니다.\n- 성능: 데이터 웨어하우스와 비교하여 쿼리 성능이 느릴 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사용 사례:\n\n- 대규모 데이터 분석\n- 기계 학습 및 데이터 과학\n- 로그 및 이벤트 데이터 저장\n\n## 3. 현대 데이터 스택\n\n현대 데이터 스택은 데이터 통합, 변환, 저장 및 분석을 용이하게 하는 일련의 모듈식 클라우드 기반 도구의 집합을 가리킵니다. 이 방식은 기민성, 확장성 및 사용 편의성을 강조합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 마크다운 형식으로 변경한 내용입니다.\n\n\n![Data Architecture Overview](/assets/img/2024-06-23-DataArchitectureABriefOverview_3.png)\n\nComponents:\n\n- Data Ingestion: Tools like Fivetran or Stitch for extracting and loading data.\n- Data Transformation: Tools like dbt (data build tool) for transforming data within the warehouse.\n- Data Storage: Cloud data warehouses like Snowflake or Google BigQuery.\n- Data Visualization: Tools like Looker, Tableau, or Mode for data exploration and visualization.\n\nBenefits:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 모듈성: 쉽게 교체하거나 업그레이드할 수 있는 유연한 구성 요소입니다.\n- 확장성: 데이터 양과 사용량에 맞게 확장되는 클라우드 기반 솔루션입니다.\n- 사용 편의성: 사용자 친화적인 인터페이스와 자동화로 광범위한 기술 전문 지식이 필요하지 않습니다.\n\n사용 사례:\n\n- 데이터 분석 솔루션의 신속한 배포\n- 민첩한 데이터 관리 및 변환\n- 셀프 서비스 분석 및 보고\n\n## 4. 통합 배치 및 스트리밍 아키텍처\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n통합 배치 및 스트리밍 아키텍처는 실시간 및 일괄 데이터를 하나의 프레임워크에서 처리하는 것을 목표로 합니다. 이 접근 방식은 데이터 처리 파이프라인을 간소화하고 일괄 및 스트리밍 데이터를 관리하기 위한 별도 시스템의 복잡성을 줄입니다.\n\n![이미지](/assets/img/2024-06-23-DataArchitectureABriefOverview_4.png)\n\n주요 아키텍처:\n\n- 람다 아키텍처: 데이터 흐름을 별도 경로로 분리하여 일괄 및 스트리밍 처리를 결합합니다.\n- 카파 아키텍처: 모든 데이터를 스트림으로 처리함으로써 람다 아키텍처를 간소화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 람다 아키텍처:\n\n- Batch Layer: 대규모의 과거 데이터를 처리합니다.\n- Speed Layer: 즉각적인 인사이트를 위해 실시간 데이터를 처리합니다.\n- Serving Layer: 배치 및 스피드 레이어의 출력을 통합된 결과로 병합합니다.\n\n장점:\n\n- 포괄적인 데이터 처리: 과거 및 실시간 데이터를 처리합니다.\n- 장애 허용성: 배치 재처리를 통해 데이터 정확성을 보장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n도전:\n\n- 복잡성: 별도의 배치 및 스트리밍 시스템을 관리하는 것은 어려울 수 있습니다.\n\n## Kappa 아키텍처:\n\n- 통합형 스트림 처리: 모든 데이터를 스트림으로 처리하여 아키텍처를 간소화합니다.\n- 재처리: 데이터 스트림을 재처리하여 과거 분석을 수행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n혜택:\n\n- 간단함: 단일 데이터 처리 모델을 사용하여 복잡성을 줄임.\n- 유연성: 관리 및 확장이 더 쉬움.\n\n## 5. 데이터 레이크하우스\n\n데이터 레이크하우스는 데이터 레이크와 데이터 웨어하우스의 기능을 결합하여, 데이터 레이크의 유연성과 데이터 웨어하우스의 성능 및 관리 기능을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-DataArchitectureABriefOverview_5.png\" /\u003e\n\n특징:\n\n- 통합 스토리지: 구조화된 및 구조화되지 않은 데이터를 모두 저장합니다.\n- ACID 트랜잭션: 데이터 작업에 대한 원자성, 일관성, 격리 및 내구성을 지원합니다.\n- 스키마 강제: 스키마를 읽거나 쓸 때 스키마에 대한 강제를 허용합니다.\n\n혜택:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 유연성: 다양한 데이터 유형과 워크로드를 처리할 수 있습니다.\n- 성능: 고성능 쿼리 및 데이터 관리를 위해 최적화되었습니다.\n- 데이터 관리: 견고한 데이터 관리 및 거버넌스 기능을 제공합니다.\n\n사용 사례:\n\n- 통합 데이터 분석 및 보고\n- 기계 학습 및 AI 워크로드\n- 실시간 및 일괄 데이터 처리\n\n## 6. 데이터 매쉬\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 메시는 도메인 주도 설계의 원칙을 데이터 관리에 적용한 분산형 데이터 아키텍처로, 중앙 집중식 데이터 레이크나 데이터 웨어하우스에서 데이터 소유 및 책임을 특정 비즈니스 도메인과 일치시키는 분산 모델로 전환합니다.\n\n![이미지](/assets/img/2024-06-23-DataArchitectureABriefOverview_6.png)\n\n특징:\n\n- 도메인 중심: 데이터는 해당 도메인이 가장 잘 아는 곳에서 관리됩니다.\n- 제품으로서의 데이터: 각 도메인은 데이터를 제품으로 취급하여 품질과 사용성을 보장합니다.\n- 셀프 서비스 데이터 인프라: 도메인이 자체적으로 데이터를 관리할 수 있도록 필요한 도구와 플랫폼을 제공합니다.\n- 연합된 지배 체계: 중앙 집중식 정책과 표준을 분산 방식으로 시행합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n혜택:\n\n- 확장성: 비즈니스가 성장함에 따라 조직이 데이터 아키텍처를 확장하는 것을 가능하게 합니다.\n- 민첩성: 더 빠르고 유연한 데이터 관리와 분석을 가능하게 합니다.\n- 소유권: 도메인 내에서 데이터 소유권과 책임을 강조합니다.\n\n사용 사례:\n\n- 다양한 데이터 요구 사항을 가진 대규모 조직\n- 데이터 거버넌스 및 품질을 향상시키려는 기업\n- 데이터 인프라를 확장하려는 조직\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 7. 데이터 패브릭\n\n데이터 패브릭은 온프레미스 및 클라우드 환경을 횡단하며 이종 데이터 원본 및 응용 프로그램을 연결하여 통합 데이터 환경을 만드는 구조적 방법론입니다. 일관된 데이터 관리와 거버넌스를 보장하며 심층적인 데이터 접근과 통합을 제공하여 데이터 환경의 일관성을 유지하고자 합니다.\n\n![데이터 아키텍처 개요](/assets/img/2024-06-23-DataArchitectureABriefOverview_7.png)\n\n특징:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 통합 액세스: 모든 데이터 원본에 대한 단일 액세스 포인트를 제공합니다.\n- 통합: 다양한 데이터 환경을 연결하여 데이터 이동 및 통합을 가능하게 합니다.\n- 자동화: AI 및 기계 학습을 활용하여 데이터 관리 작업을 자동화합니다.\n- 거버넌스: 데이터 풍경 전반에 걸쳐 데이터 품질, 보안 및 규정 준수를 보장합니다.\n\n혜택:\n\n- 일관성: 조직 전반에서 일관된 데이터를 보장합니다.\n- 효율성: 데이터 실로를 줄이고 데이터 액세스를 간소화합니다.\n- 민첩성: 신속한 데이터 통합 및 이동을 용이하게 합니다.\n\n사용 사례:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 하이브리드 데이터 환경을 가진 조직\n- 데이터 통합 및 관리를 개선하고자 하는 기업\n- 데이터 지배와 준수를 보장해야 하는 비즈니스\n\n# 추가 자료\n\n- Ross Pettit의 “Utility from Value Add 구분하기”\n- Joshua Klahr의 “현대 데이터 아키텍처의 여섯 가지 원칙”\n- Snowflake의 “데이터 웨어하우스 아키텍처란?” 웹 페이지\n- Erik Bernhardsson의 “소프트웨어 인프라 2.0: 워크리스트”\n- Etai Mizrahi의 “데이터 빚을 앞서가는 방법”\n- Neal Ford의 “전략 대 전술: SOA와 무관한 수구의 함정”\n- Dustin Lange 등의 “Deequ로 규모에 맞게 테스트 데이터 품질 유지하기”\n- IBM Education의 “3계층 아키텍처”\n- TOGAF 프레임워크 웹사이트\n- Prukalpa의 “CDOs가 2021년 주목해야 할 상위 5가지 데이터 트렌드”\n- Alexey Makhotkin의 “240 개의 테이블과 문서가 없다고?”\n- Molly Vorwerck의 “관측 가능 데이터의 궁극적인 체크리스트”\n- Apache Flink Roadmap의 “통합 분석: 일괄 및 스트리밍이 만나다; SQL 이상”\n- Martin Fowler의 “Utility Vs Strategic Dichotomy”\n- Ben Lorica 등의 “데이터 레이크하우스란?”\n- Thor Olavsrud의 “데이터 관리를 위한 프레임워크인 데이터 아키텍처란?”\n- Casber Wang의 “오픈 데이터 생태계란 무엇이며 왜 계속해서 존재하는가?”\n- Laszlo Sragner의 “MLOps가 잘못된 이유는 무엇인가?”\n- Chris Riccomini의 “데이터 메시가 뭐길래”\n- Martin Fowler의 “누가 아키텍트가 필요한가”\n- Zachman 프레임워크 위키피디아 페이지\n- Prukalpa의 “현대 데이터 플랫폼의 구성 요소”\n- Zhamak Dehghani의 “단일 데이터 레이크를 분산 데이터 메시로 나아가는 방법”\n- Zhamak Dehghani의 “데이터 메시 원칙 및 논리 아키텍처”\n- Iman Samizadeh의 “큰 데이터용 람다와 카파 두 데이터 처리 아키텍처 간단 소개”\n- Hussein Danish의 “원칙에 따른 데이터 엔지니어링, 제1부: 아키텍처 전망”\n- Jay Kreps의 “로그: 소프트웨어 엔지니어가 실시간 데이터의 통일된 추상화에 대해 알아야 할 사항”\n\n이것들은 저가 블로그 글 작성 중 살펴본 일부 자료입니다. 이들은 데이터 아키텍처의 개념과 모범 사례에 대한 추가 통찰력을 제공하고 깊게 이해하는 데 도움이 될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 요약\n\n이 블로그 포스트에서는 학습 중에 만난 데이터 아키텍처의 기본 원칙 중 일부를 개요로 설명하고 간략히 주요 데이터 아키텍처 패턴을 논의했습니다.\n\n이 상세한 데이터 아키텍처 패턴을 이해함으로써, 조직의 요구에 가장 적합한 아키텍처를 결정하고 효과적으로 구현하는 방법에 대해 정보를 얻을 수 있습니다.\n\n만일 내가 어떤 원칙이나 아키텍처를 잘못 이해하고 있다면 말씀해 주십시오. 배우는 것에 기쁨을 느낄 테니까요.","ogImage":{"url":"/assets/img/2024-06-23-DataArchitectureABriefOverview_0.png"},"coverImage":"/assets/img/2024-06-23-DataArchitectureABriefOverview_0.png","tag":["Tech"],"readingTime":13},{"title":"윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유","description":"","date":"2024-06-23 16:15","slug":"2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy","content":"\n\n## GenAI은 윤리적인 어려움이 있습니다. 데이터 리더들은 이를 어떻게 해결해야 할까요? 이 기사에서는 윤리적 AI의 필요성과 데이터 윤리가 AI 윤리임을 고려합니다.\n\n![이미지](/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png)\n\n기술 경쟁에서 미래 성공의 중요한 요소로 빠르게 이동하는 것은 항상 그렇습니다.\n\n안타깝게도, 너무 빨리 움직이는 것은 무시해선 안 될 위험이 기다리고 있다는 것을 의미하기도 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n옛날부터 전해져 내려오는 이야기입니다. 한 순간에는 선사 시대 모기 유전자를 서열 진행하고, 다음 순간에는 공룡 테마 파크를 개장하고 세계 최초의 실패한 하이퍼루프를 설계하고 있죠 (하지만 분명히 마지막은 아닙니다).\n\nGenAI에 관해서는 인생이 예술을 모방한다고 할 수 있어요.\n\n우리가 AI를 알려진 양으로 생각하고 싶어도, 이 기술의 창조자조차도 그 작동 방식에 대해 완전히 확신하지 못하는 것이 현실입니다.\n\nUnited Healthcare, 구글, 심지어 캐나다 법원 같은 곳에서 발생한 여러 고프로 AI 사태를 고려할 때, 우리가 어디서 잘못되었는지 다시 생각해 봐야 할 때입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이해를 돕기 위해 명확하게 말씀드리겠습니다. GenAI(및 AI 전반)는 결국 각 산업에서 중요한 역할을 하게 될 것으로 믿습니다. 공학 업무를 가속화하거나 일반적인 질문에 답변하는 데 이르기까지 모든 산업에서 중요한 역할을 하게 될 것입니다. 그러나 AI의 잠재적 가치를 실현하기 위해서는 먼저 AI 애플리케이션을 개발하는 방식과 데이터 팀이 그 역할에서 어떤 영향을 끼치는지에 대해 비판적으로 사고해야 합니다.\n\n본 글에서는 AI의 세 가지 윤리적 고민, 데이터 팀의 참여 방식, 그리고 데이터 리더로서 당신이 오늘 할 수 있는 일로 내일을 위한 보다 윤리적이고 신뢰할 수 있는 AI를 전달하는 방법에 대해 알아볼 것입니다.\n\n# AI 윤리의 세 가지 층위\n\n전 뉴욕타임스 데이터 및 인사이트 부문의 전 SVP인 동료인 Shane Murray와 대화를 나누던 중, 그는 처음으로 실제 윤리적 진퇴양난에 직면한 상황 중 하나를 공유해 주었습니다. 뉴욕타임스에서 금융 인센티브를 위한 기계 학습 모델을 개발하던 중, 할인을 결정할 수 있는 기계 학습 모델의 윤리적 영향에 대한 토론이 제기되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일단 할인 코드를 위한 머신 러닝 모델은 모든 것을 고려할 때 꽤 무해한 요청처럼 보였습니다. 그러나 몇 개의 할인 코드를 자동화하는 것이 얼마나 순박한 것인지라고 생각할 수 있는지에 상관없이, 그 비즈니스 문제에서 인간적 공감을 없애는 행위는 팀에게 여러 윤리적 고려사항을 만들었습니다.\n\n단순하지만 기존에는 인간적인 활동이라고 여겨진 것들을 자동화하려는 경쟁은 순전히 실용적인 결정인 것처럼 보입니다 — 효율성을 향상시키는지 아닌지의 단순한 이분법일 뿐입니다. 그러나 인간의 판단을 어떤 부분에서든 배제하면, AI가 관련되었는지 여부와 상관없이, 그 과정의 인간적 영향을 직접적으로 관리하려는 능력도 함께 상실하게 됩니다.\n\n그것은 실제 문제입니다.\n\n인공지능 개발에서 주요한 윤리적 고려사항은 세 가지가 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1. 모델 편향\n\n뉴욕 타임즈에서의 토론 핵심에 다가가는 항목입니다. 모델 자체가 어떤 의도치 않은 결과를 가져다 줄 수 있어서 한 사람을 다른 사람에 비해 유리하게 하거나 불리하게 할 수 있습니까?\n\n이곳에서의 도전 과제는 모든 다른 사항이 동일하다면, 모든 상호작용에 대해 공정하고 중립적인 결과를 일관되게 제공할 수 있도록 GenAI를 설계하는 것입니다.\n\n2. AI 사용\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI의 윤리적 고려 중 가장 존중받는 부분은 해당 기술이 어떻게 활용될지, 그 사용 사례가 회사나 사회 전반에 어떤 영향을 미칠지를 이해하는 것입니다.\n\n이 AI는 윤리적인 목적을 위해 설계되었습니까? 그 사용이 누군가에게 직접적이거나간접적으로 피해를 주지는 않는지? 그리고 궁극적으로 이 모델이 장기적으로 순수한 선을 제공할 것인가?\n\n쥬라기 공원의 첫 번째 장면에서 이안 말콤 박사가 말한 것처럼, 단지 무언가를 만들 수 있다고 해서 반드시 만들어야 한다는 뜻은 아닙니다.\n\n3. 데이터 책임성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, 데이터 팀들에게 가장 중요한 고려 사항(또한 이 글에서 제가 대부분의 시간을 할애할 부분): 데이터 자체가 AI가 책임 있게 구축되고 활용되는 데 어떻게 영향을 미치는지에 대한 문제입니다.\n\n이 고려 사항은 우리가 사용하는 데이터를 이해하고, 어떤 상황에서 안전하게 사용할 수 있는지, 그에 따른 위험 요소가 무엇인지를 다룹니다.\n\n예를 들어, 우리는 데이터가 어디에서 왔는지, 어떻게 획득되었는지를 알고 있나요? 특정 모델에 공급되는 데이터에 개인정보 문제가 있는가요? 개인들을 피해로 치는 데 위험에 노출시키는 개인 데이터를 활용하고 있나요?\n\n알 수 없는 데이터 위에 훈련된 LLM에서 안전하게 진행해도 괜찮을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNew York Times가 OpenAI에 대한 소송에서 강조한 대로, 우리는 첫째로 이 데이터를 사용할 권리가 있는 걸까요?\n\n데이터의 품질이 중요한 역할을 하는 곳이기도 합니다. 주어진 모델을 피드하는 데이터의 신뢰성을 신뢰할 수 있을까요? 퀄리티 문제가 AI 제품에 도달할 경우 잠재적인 결과는 무엇일까요?\n\n따라서, 이러한 윤리적인 문제들을 전체적으로 쳐다보았으니, 이제 데이터 팀이 이에 대해 책임을 져야 하는 이유를 살펴봅시다.\n\n# 데이터 팀이 AI 윤리에 대해 책임져야 하는 이유\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 팀과 관련된 모든 윤리적 AI 고려사항 중에서 가장 중요한 것은 데이터 책임 문제입니다.\n\n마찬가지로 GDPR이 비즈니스와 데이터 팀이 함께 데이터 수집 및 활용 방식을 재고하도록 강제했던 것처럼, GenAI는 기업이 어떤 워크플로우를 자동화할 수 있는지를 재고하도록 강제할 것입니다.\n\n우리 데이터 팀으로서 어떤 AI 모델의 구축에 영향을 미치려는 책임이 절대적이지만, 그 설계 결과에 직접적으로 영향을 끼칠 수는 없습니다. 그러나 그 모델에서 잘못된 데이터를 거를 수 있다면, 그 설계 결함에 따른 위험을 완화하는 데 많은 도움이 될 수 있습니다.\n\n모델 자체가 우리의 통제 영역을 벗어나 있다면, 할지 말지에 대한 본질적인 질문은 전혀 다른 달에 있는 것입니다. 우리는 그 문제점을 발견하면 지적할 책임이 있지만, 마무리로 말하자면, 우리가 탑승하든 말든, 로켓은 발사될 것입니다.\n가장 중요한 것은 로켓이 안전하게 발사되도록 하는 것입니다. (아니면 비행기 몸통을 훔치는 것도...)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서 데이터 엔지니어들의 삶의 모든 영역과 마찬가지로, 우리가 시간과 노력을 투자하고 싶은 곳은 최대한 많은 사람들에게 가장 직접적인 영향을 미칠 수 있는 곳입니다. 그 기회는 데이터 자체에 있다는 것이 사실입니다.\n\n# 데이터 팀이 데이터 책임성에 신경 써야 하는 이유\n\n말해봐야할 것 같지만 너무 당연한 것 같지만, 그래도 말하겠습니다:\n\n데이터 팀은 데이터가 AI 모델로 어떻게 활용되는지에 대한 책임을 져야 합니다. 왜냐하면 단순히 말하자면 그들만이 그것을 할 수 있는 유일한 팀이기 때문입니다. 물론, 준수 팀, 보안 팀, 심지어 법률 팀들이 윤리가 무시될 때 책임을 져야 할 수 있습니다. 그러나 얼마나 많은 책임을 공유할 수 있더라도, 그 팀들이 결국은 데이터 팀만큼 데이터를 동일한 수준에서 이해하지는 못할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n소프트웨어 엔지니어링 팀이 OpenAI나 Anthropic과 같은 써드파티 LLM을 사용하여 앱을 만든다고 상상해보세요. 그러나 그들은 그들의 애플리케이션에 실제로 필요한 데이터 외에도 위치 데이터를 추적하고 저장하고 있다는 것을 깨닫지 못했습니다. 그들은 모델을 구동하기 위해 전체 데이터베이스를 활용합니다. 올바른 논리 결함이 있으면 나쁜 행위자가 그 데이터세트에 저장된 데이터를 사용하여 임의의 개인을 추적하는 프롬프트를 손쉽게 공학적으로 만들 수 있습니다. (이것은 오픈 소스 LLM과 폐쇄 소스 LLM 사이의 긴장 관계입니다.)\n\n또는 소프트웨어 팀이 위치 데이터에 대해 알고 있지만 위치 데이터가 실제로 근삿값일 수 있다는 것을 깨닫지 못했습니다. 그들은 그 위치 데이터를 사용하여 16세 소년을 단순히 거리에 있는 피자 헛(Pizza Hut)이 아닌 어둡고 좁은 골목으로 안내하는 AI 매핑 기술을 만들 수 있습니다. 물론 이러한 유형의 오류는 의지적이 아니지만 데이터가 활용되는 방식에 내재된 의도하지 않은 위험을 강조합니다.\n\n이러한 예시와 다른 사례들은 윤리적 AI에 관한 문제에서 데이터 팀의 역할을 강조합니다.\n\n# 그렇다면, 데이터 팀이 윤리적으로 유지하는 방법은 무엇일까요?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대부분의 경우, 데이터 팀은 모델이 작동하도록 대략적이고 대리 데이터를 다루는 데 익숙합니다. 그러나 AI 모델을 제공하는 데이터의 경우, 실제로 훨씬 더 높은 수준의 검증이 필요합니다.\n\n소비자를 위해 틈을 메우기 위해 데이터 팀은 데이터 관행과 그 관행이 조직 전반과 어떻게 관련되는지 신중히 살펴봐야 합니다.\n\nAI의 위험을 완화하는 방법을 고려할 때, 아래는 데이터 팀이 미래에 더 윤리적인 방향으로 AI를 움직이기 위해 취해야 할 3단계입니다.\n\n# 1. 회의 참석 자리를 얻으세요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 팀은 타조가 아닙니다 - 머리를 모래속에 파묻고 문제가 사라지기를 기대할 수 없습니다. 데이터 팀이 리더십 테이블에 자리를 놓기 위해 싸워온 것처럼, 데이터 팀은 AI 테이블에 자리를 얻을 수 있도록 옹호해야 합니다.\n\n어떤 데이터 품질 위기든, 지구가 이미 타버린 후에 뛰어든다면 충분하지 않습니다. GenAI에 고유하게 내재된 종말적 위험을 다룰 때, 우리 자신의 개인적 책임에 대해 선제적으로 대처하는 것이 이전보다 중요합니다.\n\n그들이 당신이 테이블에 앉을 수 있도록 허락하지 않는다면, 바깥에서 교육하는 책임이 있습니다. 당신의 힘을 다하여 훌륭한 발견, 거버넌스 및 데이터 품질 솔루션을 제공하여 그 팀들에게 정보를 제공하면서 책임 있는 결정을 내릴 수 있도록 해야 합니다. 그들에게 사용할 것, 언제 사용할 것, 그리고 당신 팀의 내부 프로토콜로 유효성을 검증할 수 없는 써드파티 데이터 사용의 위험성을 가르쳐 주세요.\n\n이것은 단지 비즈니스 문제가 아닙니다. 유나이티드 헬스케어와 브리티시 컬럼비아 주가 말할 것처럼, 많은 경우, 이들은 실제 사람들의 삶과 생계에 관한 것입니다. 그러니, 우리가 그 관점으로 운영하고 있는지 확인합시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2. RAG과 같은 방법론을 활용하여 더 책임감 있는 - 그리고 신뢰할 수 있는 - 데이터 조직화하기\n\n우리는 종종 RAG(검색 증강 생성) 같은 방법론을 언급하며 인공 지능으로부터 가치를 창출하는 자원으로 생각합니다. 그러나 그것은 그 AI를 구축하고 사용하는 방식을 보호하는 자원이기도 합니다.\n\n예를 들어, 모델이 소비자를 대상으로 하는 채팅 앱에 공급하기 위해 개인 고객 데이터에 액세스하는 경우를 상상해보십시오. 올바른 사용자 프롬프트가 모델이 동작하여 심각한 PII가 폭로되어 나쁜 행위자가 그것을 취할 수도 있습니다. 그래서 데이터가 어디에서 왔는지 확인하고 제어하는 능력은 그 AI 제품의 무결성을 보호하는 데 중요합니다.\n\n책임있는 데이터 팀은 RAG와 같은 방법론을 활용하여 준수되고 더 안전하며 모델에 적합한 데이터를 신중하게 조직화하여 많은 위험을 완화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI 개발에 RAG(빨강, 주황, 초록) 접근 방식을 취하면 너무 많은 데이터를 처리할 때 발생할 수 있는 위험을 최소화하는 데 도움이 됩니다 — 우리의 위치 데이터 예시에서 언급된 대로.\n\n그것이 실제로 어떻게 보이는지 궁금하시죠? 예를 들어 Netflix와 같은 미디어 회사라고 가정해봅시다. 이 회사는 고객 데이터의 일부를 활용하여 맞춤형 추천 모델을 만들어야 합니다. 그런 다음 해당 사용 사례에 대한 구체적이고 제한된 데이터 포인트를 정의하고, 그 데이터를 유지하고 유효성을 검사할 책임자가 누구인지, 어떤 상황에서 데이터를 안전하게 사용할 수 있는지, 그리고 시간이 지남에 따라 그 AI 제품을 개발하고 유지할 가장 적합한 사람이 누구인지를 보다 효과적으로 정의할 수 있게 됩니다.\n\nData lineage과 같은 도구는 데이터의 출처를 신속하게 확인하여 팀이 어떤 시점에서 데이터가 어디에서 유래되고 사용되는지 또는 잘못 사용되는지를 확인할 수 있도록 도와줄 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 3. 데이터 신뢰도를 우선시하세요\n\n데이터 제품에 대해 이야기할 때 자주 하는 말이 있습니다. \"쓰레기 데이터를 넣으면 쓰레기 데이터가 나온다,\" 하지만 GenAI의 경우에는 그 말이 조금 모자라다고 볼 수 있습니다. 실제로 쓰레기 데이터가 AI 모델로 들어가면 쓰레기만 나오는 것이 아니라 실제로 사람에게도 영향을 미치는 결과가 나올 수 있습니다.\n\n그래서 모델로 공급되는 데이터를 제어하기 위해 RAG 아키텍처가 필요한만큼, Pinecone과 같은 벡터 데이터베이스에 연결되는 강력한 데이터 관찰 기능이 필요합니다. 데이터가 실제로 깨끗하고 안전하며 신뢰할 수 있는지 확인하기 위해 이 연결이 중요합니다.\n\nAI를 시작하는 고객들로부터 가장 많이 들은 불만 중 하나는, 제품용 AI를 준비하는 것은, 벡터 데이터 파이프라인으로 인덱스를 적극적으로 모니터링하지 않는다면 데이터의 신뢰성을 검증하는 것이 거의 불가능하다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대부분의 경우, 데이터 및 AI 엔지니어들이 데이터에 문제가 발생했음을 알게 되는 유일한 방법은 모델이 나쁜 프롬프트 응답을 내뱉을 때입니다. 그때에는 이미 너무 늦은 시점이죠.\n\n# 현재가 가장 좋은 때입니다\n\n2019년에 우리 팀이 데이터 관찰 카테고리를 만들었던 것은 데이터의 신뢰성과 신뢰의 필요성을 높인 동일한 과제입니다.\n\n오늘날 AI가 일상적으로 의지하는 다양한 프로세스 및 시스템을 바꿔놓을 것이라고 약속하는 가운데, 데이터 품질의 도전과 더 중요한 것으로서의 윤리적 영향이 더욱 심각해지고 있습니다.","ogImage":{"url":"/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png"},"coverImage":"/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png","tag":["Tech"],"readingTime":8},{"title":"사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석","description":"","date":"2024-06-23 16:14","slug":"2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo","content":"\n\nMichelangelo은 Uber 내부 ML-as-a-service 플랫폼으로, 기업의 요구를 충족하기 위해 AI를 확장하고 머신 러닝을 민주화하는 서비스입니다. 데이터 관리, 모델 학습, 평가, 배포, 예측 및 모니터링 기능을 하며 전통적인 머신 러닝 모델부터 딥 러닝까지 다양한 모델을 다룰 수 있습니다. Michelangelo은 오랫동안 Uber에서 사용 중이며 여러 Uber 데이터 센터에 배포되어 있습니다. 이 기사에서는 Michelangelo의 개발 동기와 아키텍처에 대해 자세히 살펴보겠습니다.\n\nMichelangelo 이전에 Uber는 scikit-learn, R 등과 같은 온라인에서 사용 가능한 오픈 소스 도구를 사용했으나 ML의 영향력은 소수의 데이터 과학자와 엔지니어가 주로 오픈 소스 도구를 사용하여 구축할 수 있는 한도를 벗어날 수 없었습니다. 따라서 Uber는 규모화된 훈련 및 예측 데이터를 관리하기 위한 신뢰성 있고 일관된, 재현 가능한 파이프라인을 구축하고자 했습니다. 가장 중요한 것은 실험 비교 및 모델을 프로덕션 환경에 배포하는 과정이 명확히 정립되지 않았습니다. Uber의 엔지니어링 팀은 해당 프로젝트에 특정한 사용자 정의 서빙 컨테이너를 만들어야 했습니다. Michelangelo는 이러한 문제를 해결하기 위해 팀 간 워크플로우를 표준화하고 엔지니어들이 쉽게 규모에 맞게 머신 러닝 시스템을 구축하고 운영할 수 있도록 하는 엔드 투 엔드 시스템을 통해 설계되었습니다. Michelangelo은 주로 아이디어에서 첫 프로덕션 모델로의 경로 축소와 그 이후 빠른 반복에 초점을 맞추었습니다. UberEATS의 사용 사례를 통해 전체 프로세스를 더 자세히 알아보겠습니다.\n\nUberEATS에는 Michelangelo에서 실행되는 여러 머신 러닝 모델이 있습니다. 배달 시간 예측, 검색 순위, 음식점 순위 등을 다루는 모델들이 포함되어 있습니다. 배송 시간 모델은 주문이 발생하기 전에 음식이 준비되고 배달되는 데 얼마나 걸릴지 예측합니다. 그렇다면 UberEATS는 어떻게 작동할까요? 고객이 주문하면 레스토랑으로 전송되어 처리됩니다. 레스토랑은 주문을 확인하고 식사를 준비해야 하며, 주문의 복잡성과 레스토랑의 혼잡 정도에 따라 필요한 시간이 달라집니다. 식사가 준비되기 직전에 Uber 딜리버리 파트너가 음식을 가져가도록 지시됩니다. 그런 다음, 딜리버리 파트너는 레스토랑에 도착하고 음식을 가져와 고객의 위치로 운전하고(경로, 교통 등에 따라 달라집니다) 고객의 집으로 걸어가 배달을 완료해야 합니다. 이 복잡한 프로세스의 총 소요 시간을 예측하고 프로세스의 각 단계에서 이러한 시간까지 재계산하는 것이 목표입니다.\n\nUberEATS의 데이터 과학자들은 end-to-end delivery 시간을 예측하기 위해 그라디언트 부스트된 의사 결정 트리 회귀 모델을 사용합니다. 모델의 features로는 하루 중 시간, 배달 위치, 평균 식사 준비 시간 등이 포함됩니다. 이러한 모델들은 Michelangelo 모델 서빙 컨테이너에 배포되어 Uber의 데이터 센터 전체에 통합되며, UberEATS 마이크로서비스에 의해 네트워크 요청을 통해 호출됩니다. 이러한 예측은 고객이 식사가 준비되고 배달되는 동안에 표시됩니다. Michelangelo은 Uber 내부에서 개발된 구성 요소와 오픈 소스 시스템의 혼합을 사용하여 구축되었습니다. XGBoost, Tensorflow, Spark 등의 오픈 소스 구성 요소가 사용되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMichelangelo의 데이터 관리 구성 요소는 온라인과 오프라인 파이프라인으로 나뉩니다. 오프라인 파이프라인은 일괄 모델 훈련 및 일괄 예측 작업에 사용되며, 온라인 파이프라인은 온라인, 낮은 지연 시간 예측을 수행합니다. 또한 Michelangelo에는 기능 스토어가 있어 팀이 기계 학습 문제용 다른 기능 세트를 공유하고 발견할 수 있습니다. Uber의 모든 데이터는 먼저 HDFS 데이터 레이크에 저장되며, 오프라인 파이프라인에서 기능을 계산하기 위해 액세스할 수 있으며 온라인에 배포된 모델은 HDFS에 저장된 데이터에 액세스할 수 없습니다. 따라서 온라인 모델용 기능은 미리 계산되어 Cassandra 기능 스토어에 저장되어서 예측 시 낮은 대기 시간으로 읽을 수 있습니다.\n\n앞에서 말했듯이 Uber는 중앙 집중식 기능 스토어를 만드는 데 높은 우선순위를 둡니다. Uber의 팀은 기능을 생성하고 관리하고 다른 사람과 공유할 수 있기 때문에 새로운 기능을 추가하는 것이 쉬워지며 기능 스토어에 기능이 있으면 사용하기 매우 쉬워집니다. Michelangelo에는 모델러가 훈련 및 예측 시 모델로 보내는 기능을 선택, 변환 및 결합하는 데 사용하는 DSL(도메인 특화 언어)이라는 것이 있습니다. DSL은 모델 구성 자체의 일부이며, 훈련 시점과 예측 시점에 적용되어 동일한 최종 기능 집합이 두 경우에 모델로 생성되어 보내지도록 보장합니다.\n\nUber는 수억 개의 샘플을 처리할 수 있는 분산 모델 훈련 시스템을 사용하며 의사결정 트리, 신경망 및 선형 모델과 같은 알고리즘을 위한 빠른 반복을 위해 작은 데이터 세트로 축소할 수도 있습니다. 모델 구성은 모델 유형, 초매개변수, 데이터 소스 참조 및 계산 리소스 요구 사항을 지정하며, 이것은 훈련 작업을 구성하는 데 사용됩니다. 모델 훈련 후 성능 메트릭이 계산되고 모델 평가 보고서에 통합됩니다. 훈련이 완료된 후, 원래 구성, 학습된 매개변수 및 평가 보고서가 분석 및 배포를 위해 모델 저장소에 저장됩니다. Michelangelo는 모든 모델 유형에 대한 초매개변수 검색을 지원하며 모든 훈련 작업은 API 및 워크플로우 도구를 통해 관리됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 마이셀란젤로가 어떻게 모델을 학습하고 평가하며 배포하는지에 대해 설명한 내용입니다. 마이셀란젤로는 특정 사용 사례에 이상적인 모델에 이르기까지 수백 개의 모델을 학습합니다. 이러한 수백 개의 모델은 엔지니어들을 최적의 성능을 보이는 모델 구성으로 안내하며, 학습된 모델을 추적하고 평가하고 비교하는 것이 마이셀란젤로에서 주요 관심사입니다. 모델 훈련 시, 마이셀란젤로에 저장된 각 모델은 학습자, 모델 구성, 정확도 측정, 학습된 매개변수 등의 정보가 포함된 버전화된 객체로 카산드라 모델 저장소에 저장됩니다. 이러한 정보는 웹 UI 또는 API를 통해 쉽게 확인하고 비교할 수 있습니다. 마이셀란젤로는 또한 모델이 동작하는 이유를 이해하고 필요한 경우 디버깅하는 데 도움이 되는 시각화 도구를 제공합니다. 특성 보고서에는 각 특성이 모델의 중요도에 따라 부분 의존 플롯 및 분포 히스토그램이 표시됩니다.\n\n배포를 위해 마이셀란젤로는 UI 또는 API를 통해 모델 배포를 관리하는 엔드 투 엔드 지원을 제공합니다. 오프라인 모델은 요청에 따라 또는 정기적인 일정에 따라 실행되는 스파크 작업에서 실행되는 컨테이너에 배포됩니다. 온라인 모델은 수신 요청을 기반으로 예측하는 서비스 클러스터에 배포됩니다. 두 경우 모두, 모델 아티팩트는 ZIP 아카이브로 패키징되어 우버의 데이터 센터 각 위치에 복사됩니다. 예측 컨테이너는 자동으로 디스크에서 새 모델을 로드하고 예측 요청을 처리하기 시작합니다. 우버의 여러 팀은 마이셀란젤로의 API를 통해 정기적인 모델 재훈련과 배포를 예약하기 위한 자동화 스크립트를 보유하고 있습니다. 모델이 배포된 후, 데이터 파이프라인 또는 클라이언트 서비스에서 로드된 특성을 기반으로 예측을 수행합니다. 온라인 모델의 경우 예측은 네트워크를 통해 클라이언트 서비스로 반환되며, 오프라인 모델의 경우 예측 결과는 하이브(데이터 웨어하우스)에 다시 기록되어 SQL 기반 쿼리 도구를 통해 직접 액세스할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 번에 여러 모델을 동일한 서빙 컨테이너에 배포할 수 있습니다. 이를 통해 이전 모델에서 새로운 모델로의 쉬운 전환과 모델의 A/B 테스트를 병행할 수 있습니다. 서빙 시간에는 모델이 해당 태그로 최근에 배포된 모델을 사용하여 식별됩니다. 온라인 모델의 경우 확장을 위해 예측 서비스 클러스터에 더 많은 호스트가 추가되고 로드 밸런서가 부하를 분산시킵니다. 오프라인 예측의 경우 더 많은 Spark 실행자가 추가되며 Spark가 병렬성을 관리합니다. 예측을 모니터링하기 위해 Michelangelo는 자동으로 로그를 기록하고 선택적으로 생성된 예측의 일부를 보류하고 나중에 해당 예측을 데이터 파이프라인이 생성한 레이블과 비교합니다. 앞으로 Uber는 AutoML, 분산 딥 러닝 및 기타 도구 및 서비스를 추가하여 기존 시스템을 강화할 계획입니다.\n\n```js\n참고 자료:\n1. Uber 엔지니어링 블로그\n```","ogImage":{"url":"/assets/img/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo_0.png"},"coverImage":"/assets/img/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo_0.png","tag":["Tech"],"readingTime":5},{"title":"위성 이미지를 통한 그레이트 솔트레이크 축소 추적 Python 사용 방법","description":"","date":"2024-06-23 16:11","slug":"2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython","content":"\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*HOgBDjBrNhopFc2jBrRXVw.gif\" /\u003e\n\n# 목차\n\n- 🌅 위대한 솔트 레이크 수축 문제 소개\n- 💾 Landsat-8 이미지 다운로드\n- 📈 통계 파일로부터 위대한 솔트 레이크 지역의 시계열\n- ⚙️ Landsat-8 이미지 처리\n- 🗺️ 위대한 솔트 레이크 이미지 시각화\n- 🎥 위대한 솔트 레이크 수축의 타임 랩스\n- 📉 분류된 이미지로부터 위대한 솔트 레이크 지역의 시계열\n- ⚖️ 통계 파일 및 이미지에서의 시계열 비교\n- 📄 결론\n- 📚 참고문헌\n\n## 🌅 위대한 솔트 레이크 수축 문제 소개\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n유타 주의 미국 대소염소인, 위도 메리칼크 지역 치축하고 있어요. 여러 보고서에 의하면, 이 호수는 1986년 최대 규모 대비 30% 이상으로 줄어들었다고 합니다. 이 호수의 수위가 저하되는 이유로 기후 변화와 농업용 수분 분할 등이 언급되었습니다.\n\n기후 변화 요인에 대해 보도된 바에 의하면 강수량 패턴이 변화되고 온도가 상승함에 따라 스노우팩이 감소하고 호수로의 유입량이 줄어들고 있다고 합니다.\n\n두 번째 이유에 대해, 최근 몇 년 동안 도시 및 농업 지역이 확장되어 왔습니다. 도시 및 농업 분야의 수요 증가는 물 수위의 하락에 더해진 요인이 되었습니다.\n\n장기적인 하강은 생태계에 심각한 영향을 미치며, 특히 공기와 물의 질에 영향을 미칩니다. 호법 지역의 노출은 먼지와 염분을 방출함으로써 공기와 물의 질에 대한 위험을 증가시킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n상황이 동적이며 감소하는 수위를 추적하는 논의가 계속되고 있기 때문에 저는 이 게시물을 작성하기로 결정했습니다. 우리가 위성 이미지를 사용하여 호수 표면적의 변화를 모니터링하는 방법을 보여주기 위해서입니다. 이것은 축소의 지표로 사용됩니다.\n\n저는 2014년부터 2023년까지 Landsat-8에 의해 촬영된 모든 이미지를 사용하여 구글 Colab에서 Python을 사용하여 그들을 분석했습니다. 지난 어떤 기간 동안 호수 표면적의 시계열을 추출하거나 심지어 이 접근 방법을 다른 호수에 적용하는 것에 관심이 있다면, 이 블로그 게시물은 여러분을 위한 것입니다.\n\n## 💾 Landsat-8 이미지 다운로드\n\nLandsat-8 이미지를 다운로드하기 위해 AρρEEARS라는 웹 앱을 사용했습니다. 이 웹 앱을 통해 관심 영역(AOI)에 대한 위성 이미지를 다운로드할 수 있습니다. 이미지는 자르고 메타데이터 및 통계 파일로 제공됩니다. AOI에 대한 폴리곤을 그리고 제품을 선택하고 시작 및 종료 날짜를 선택하기만 하면 됩니다. 이미 이 웹 앱에 관한 게시물을 작성해 놓았습니다. 여기서 확인해보세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 게시물에서는 Great Salt Lake 주변에 폴리곤을 그리고 Landsat-8 ARD 제품을 선택하여 시작 시간과 종료 시간을 2014년부터 2023년까지 여름(6월, 7월 및 8월) 기간으로 설정했습니다. 여름 동안에만 촬영된 이미지를 고려한 결정은 두 가지 이유로부터 나왔어요: (1) 여름 동안 보다 맑은 이미지를 더 유추할 수 있는 가능성이 더 높고, (2) 이는 계산 시간을 제한하기 때문입니다. 이 여름 동안의 모든 연도에 대한 Landsat-8 이미지를 다운로드하므로 결과(호수의 추정 표면적)는 서로 다른 연도 간에 비교할 수 있어요.\n\n데이터 정책: USGS 웹사이트에 따르면, \"USGS에서 다운로드한 Landsat 데이터에는 제한이 없으며, 원하는 대로 사용하거나 재배포할 수 있습니다\" (링크) 및 \"LP DAAC로부터 획득한 LP DAAC의 모든 현재 데이터와 제품은 재사용, 판매 또는 재배포에 대한 제한이 없습니다\" (링크).\n\n## 📈 통계 파일에서 Great Salt Lake 지역의 시계열\n\nAρρEEARS에서 요청을 제출한 후, 호수의 표면적을 추정할 두 가지 옵션이 있습니다. 옵션 1은 통계 CSV 파일에 보고된 미리 계산된 물 픽셀 수를 사용하는 것이며, 옵션 2는 분류된 이미지를 처리하여 물 픽셀을 계산하는 것인데, 이는 추후 설명하겠습니다. 옵션 1을 따르려면 AρρEEARS에서 \"L08-002-QA-PIXEL-CU-Statistics-QA.csv\" 및 \"L08-002-QA-PIXEL-CU-lookup.csv\" 두 파일을 다운로드하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 파일을 빠르게 살펴보겠습니다:\n\n![이미지1](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_0.png)\n\n![이미지2](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_1.png)\n\n첫 번째 파일인 \"L08-002-QA-PIXEL-CU-Statistics-QA.csv\"에서는 각 이미지(행)에 대해 각 클래스(열)의 픽셀 수가 보고됩니다. 이러한 열 이름을 해독하려면 두 번째 파일인 \"L08-002-QA-PIXEL-CU-lookup.csv\"이 필요합니다. 'Water'로 레이블된 열을 필터링하면 물 픽셀에 대한 해당 클래스 이름(열)을 찾을 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Great Salt Lake](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_2.png)\n\n수집된 코드들 중 물 클래스로 고려된 코드는 21890, 21952, 22146, 22208, 54662, 54724, 54918, 54980입니다. 따라서 \"L08–002-QA-PIXEL-CU-Statistics-QA.csv\" 파일에서 이 열들만 선택하고 픽셀 수를 합산하면 지난 10년간의 Great Salt Lake 지역의 시계열을 시각화할 수 있습니다. 이를 위한 Python 스크립트를 작성해 봅시다:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ncsv_file_path = 'L08-002-QA-PIXEL-CU-Statistics-QA.csv'\n\n# 물 클래스를 위한 열 이름\nselected_columns = ['21890', '21952', '22146','22208','54662','54724','54918','54980']\n\n# CSV 파일을 pandas DataFrame으로 읽기\ndf = pd.read_csv(csv_file_path)\n\n# 'date' 열을 날짜 형식으로 변환\ndf['date'] = pd.to_datetime(df['Date'])\n\n# 물 열 선택\nselected_data = df[['Date'] + selected_columns]\n\n# 각 행의 합산 계산\nselected_data['sum'] = selected_data[selected_columns].sum(axis=1)\n\n# 플롯\nplt.figure(figsize=(10, 6))\nplt.plot(selected_data['Date'], selected_data['sum'], marker='o', linestyle='-', color='b')\n\nplt.title('시간에 따른 물 픽셀의 합산')\nplt.ylabel('합계')\nplt.grid(True)\n\nlocator = mdates.MonthLocator() \nplt.gca().xaxis.set_major_locator(locator)\n\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n출력 결과는:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Tracking The Great Salt Lake's Shrinkage Using Satellite Images Python](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_3.png)\n\n만약 AOI에서 각 이미지로 분류된 픽셀의 물의 수를 그래프로 그린다면, 각 Landsat 이미지가 전체 호수를 커버할 수 있는 것을 보장할 수 없습니다. 게다가 우리는 흐린 픽셀들을 걸러냅니다. 다시 말해, Landsat 이미지가 전체 호수를 커버하더라도, 만약 호수 위에 구름이 있다면, 깨끗한 픽셀의 물로 분류되는 숫자는 전체 호수의 표면을 과소 평가할 수 있습니다. 이 두 문제의 예시로 각 이미지를 고려해 보겠습니다.\n\n![Example Image 1](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_4.png)\n\n여기서 보듯이, 왼쪽 이미지(2015년 7월 1일)는 전체 호수를 커버하지 않고 일부가 잘린 상태입니다. 이 이미지를 기반으로 호수의 표면적을 계산하는 것은 오해를 불러일으킬 수 있으며, 시계열에서 상당한 하강을 보여줄 수 있습니다. 오른쪽 이미지(2015년 6월 22일)에 대해선, 전체 호수를 커버하고 있지만, 물 픽셀의 수가 표면적을 계산하기 위한 정확한 정보를 제공할 수 없을 수 있습니다. 보여진 것처럼, 흐린 픽셀이 걸러져 나가면서 호수의 표면적이 과소 평가되는 문제가 발생합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한, 시계열 그래프가 각 이미지의 총 물 픽셀을 표시하여 전반적인 감소 추세를 관측하기 어렵게 만듭니다.\n\n이러한 문제를 해결하려면 코드를 약간 수정해야 합니다. 먼저, 각 이미지의 물 픽셀 수를 그리는 대신 데이터를 집계하고 각 연도별로 보고된 최대 물 픽셀 수를 얻어야 합니다. 둘째, 다음 변환을 사용하여 제곱 킬로미터 (sqKm)로 호수 면적을 그릴 것입니다:\n\n호수 면적 (Km²) = (랜드셧 픽셀 크기) x (물 픽셀 수) / 10⁶\n\n* 랜드셧 픽셀 크기 = 30m x 30m\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom scipy.stats import linregress\n\n\ncsv_file_path = 'L08-002-QA-PIXEL-CU-Statistics-QA.csv'\n\n# 물 픽셀을 위한 열 이름\nselected_columns = ['21890', '21952', '22146', '22208', '54662', '54724', '54918', '54980']\n\n# CSV 파일을 pandas DataFrame으로 읽기\ndf = pd.read_csv(csv_file_path)\n\n# 'date' 열을 datetime 형식으로 변환\ndf['date'] = pd.to_datetime(df['Date'])\n\n# 'date' 열에서 연도 추출\ndf['year'] = df['date'].dt.year\n\n# 열 선택\nselected_data = df[['year'] + selected_columns]\n\n# 각 행의 합 계산\nselected_data['sum'] = selected_data[selected_columns].sum(axis=1)\n\n# 각 연도의 최대 값을 계산\nmax_values_per_year = selected_data[['year','sum']].groupby('year').max()\n\n# 면적으로 변환 (평방 킬로미터)\nmax_values_per_year['area'] = max_values_per_year['sum'] * 30 * 30 / 1000000\n\n# 플로팅\nplt.figure(figsize=(12, 6))\nplt.bar(max_values_per_year.index, max_values_per_year['area'], color='lightblue', label='연도별 최대 면적')\nplt.plot(max_values_per_year.index, max_values_per_year['area'], marker='o', linestyle='-', color='b', label='추세선')\n\n# 추세선을 위한 선형 회귀\nslope, intercept, _, _, _ = linregress(max_values_per_year.index, max_values_per_year['area'])\ntrend_line = slope * max_values_per_year.index + intercept\n\n# 추세선 방정식 표시\nplt.plot(max_values_per_year.index, trend_line, color='red', linestyle='--', label=f'추세선: y = {slope:.4f}x + {intercept:.4f}')\n\nplt.xticks(fontweight='bold')\nplt.yticks(fontweight='bold')\n\nplt.title('연도별 대소염소 지역 최대 면적')\nplt.ylabel('최대 면적 (평방 킬로미터)', fontsize=12, fontweight='bold')\nplt.grid(True)\nplt.legend(loc='center')\nplt.show()\n```\n\n위의 플롯을 참고해주세요.\n\n우리가 데이터를 집계하는 코드 수정이 도움이 된 것으로 보입니다. 그러나 결과는 아직 정확하지 않을 수 있습니다. 왜냐하면 우리는 각 연도에 촬영된 모든 이미지에서 대소염소 호수의 최대 면적을 추출하기 때문입니다. 예를 들어, 2015년에 가장 명확한 이미지가 15%의 구름을 가지고 있다고 가정해 봅시다. 구름이 걸러지면 호수의 표면적을 15% 과소 평가하게 됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 문제를 해결하기 위해서는, 호수 위에서 찍힌 원본 분류된 이미지들과 작업해야 합니다. 구체적으로는, 각 이미지마다 수집된 픽셀 중 물로 분류된 것을 오버레이하여 다운로드해야 합니다. 이 과정을 거치면 각 연도별로 가장 정확한 구름 없는 호수 이미지를 작성할 수 있으며 표면적을 정확하게 추출할 수 있게 될 것입니다. 이 방법에 대해 다음 섹션에서 다룰 예정입니다.\n\n## ⚙️ Landsat-8 이미지 처리\n\n이번과 다음 섹션에서는 통계 파일이 아닌 실제 Landsat-8 분류된 이미지를 다루게 됩니다.\n\nAρρEEARS에서 또는 API를 통해 L08.002_QA_PIXEL_CU_doy`YYYYMMDD`_aid0001와 같이 명명된 파일들을 수동으로 다운로드할 수 있습니다. API를 사용하려는 경우 검색 필드에 'QA_Pixel'을 입력하여 파일을 필터링한 후 AρρEEARS에서 모든 파일을 선택하고 '다운로드'를 클릭하세요. 그런 다음 아래 코드를 실행하여 API를 통해 파일을 다운로드할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계 1: 폴더 만들기\n\n```python\nimport os\n\n# 폴더 이름 목록\nfolders = ['QA', 'Clear_Image', 'Map', 'Animation']\n\nbase_path = '/content'  \n\n# 폴더 생성\nfor folder in folders:\n    folder_path = os.path.join(base_path, folder)\n    os.makedirs(folder_path, exist_ok=True)\n    print(f\"'{folder}' 폴더가 '{folder_path}'에 생성되었습니다.\")\n```\n\n단계 2: 토큰 설정\n\n```python\nimport os\nimport requests\n\nresponse = requests.post('https://appeears.earthdatacloud.nasa.gov/api/login', auth=('여러분의 사용자명', '여러분의 비밀번호'))\ntoken_response = response.json()\ntoken = token_response['token']\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3단계: 텍스트 파일에 저장된 URL을 읽고 파일을 다운로드합니다:\n\n```js\ntxtfile = 'Great-Salt-Lake-download-list.txt' \nfilenames = [] # 파일 이름을 저장하는 리스트\nurls = [] # 파일 이름이 없는 URL을 저장하는 리스트\n\nwith open(txtfile) as file:\n    for line in file:\n        url = line.strip() # 줄 바꿈 문자 제거\n        filename_index = url.find('L08.002') # 파일 이름의 인덱스 가져오기\n        url_without_filename = url[:filename_index-1] # 파일 이름이 없는 URL 가져오기\n        print(url_without_filename)\n        filename = url[filename_index:] # 파일 이름 가져오기\n        print(filename)\n\n        response = requests.get(url_without_filename,\n                                headers={'Authorization': 'Bearer {0}'.format(token)},\n                                allow_redirects=True,\n                                stream=True)\n        dest_dir = \"/content/QA\"\n        filepath = os.path.join(dest_dir, filename)\n        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\n        # 파일을 목적지 디렉토리에 쓰기\n        with open(filepath, 'wb') as f:\n          for data in response.iter_content(chunk_size=8192):\n            f.write(data)\n```\n\n이 코드는 폴더를 생성하고 토큰을 설정하며 QA 폴더에 AppEARS 서버에서 분류된 Landsat-8 이미지를 다운로드합니다.\n\n첫 번째 단계에서는 QA, Clear_Image, Map 및 Animation 네 가지 폴더를 만들었습니다. \"QA\" 폴더는 원본 분류된 이미지를 저장하기 위한 것이고, \"Clear_Image\" 폴더는 각 연도별로 물 위 구름이 없는 Great Salt Lake 이미지를 저장하는 용도이고, \"Map\"은 물 위 구름이 없는 이미지를 바탕으로한 Great Salt Lake 맵을 저장하는 곳이며, \"Animation\" 폴더는 타임랩스 파일을 저장하는 용도입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 단계를 올바르게 따르면 디렉토리에 \"QA_Pixel\" 파일이 있어야 합니다:\n\n![이미지](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_6.png)\n\n이제 디렉토리에 이미지가 분류되었으므로 이러한 이미지를 오버레이하고 각 연도별로 물로 분류된 픽셀을 유지하고 이러한 래스터 레이어를 Geotiff 파일에 저장해야 합니다. 이 작업은 다음과 같이 수행할 수 있습니다:\n\n```js\nimport os\nimport numpy as np\nimport rasterio\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport pandas as pd\n\n# 유지할 QA 값 정의\nqa_values = [21890, 21952, 22146, 22208, 54662, 54724, 54918, 54980]\n\n# 폴더 내의 모든 파일 나열\nqa_folder = '/content/QA'\nall_files = os.listdir('/content/QA')\n\n# 연도별로 루프 실행\nfor year in range(2014, 2024):\n    print(year)\n\n    # 물 픽셀을 저장할 빈 배열 초기화\n    water_pixels = None\n\n    # 조건에 따라 파일 필터링\n    QA_list = [file for file in all_files if str(year) in file]\n    for QA_filename in QA_list:\n        if QA_filename.endswith('.tif'):\n            date_str = QA_filename.split('_')[4][3:]\n            date_obj = datetime.strptime(date_str, '%Y%j')\n            date = date_obj.date()\n            print('Processing date:', date)\n\n            # QA tif 파일 읽기\n            qa_file = os.path.join(qa_folder, QA_filename)\n\n            with rasterio.open(qa_file) as qa_src:\n                # tif 파일에서 데이터 및 메타데이터 읽기\n                qa_data = qa_src.read(1)\n\n                # 모든 값이 nan인 경우 건너뛰기\n                qa_data_zeros = np.all(qa_data == 1)\n\n                if qa_data_zeros:\n                    continue\n\n                profile = qa_src.profile\n\n                # 물값을 2로 대체하고 나머지는 NaN으로 변경\n                qa_data = qa_data.astype(float)\n                qa_data[np.isin(qa_data, qa_values)] = 2\n                qa_data[~np.isin(qa_data, [2])] = np.nan\n\n                # 물 픽셀을 배열에 추가\n                if water_pixels is None:\n                    water_pixels = qa_data.copy()\n                else:\n                    water_pixels = np.nanmax(np.stack([water_pixels, qa_data]), axis=0)\n\n                # 물 픽셀을 갖는 새 래스터 생성\n                profile.update(count=1)\n                with rasterio.open(f'/content/Clear_Image/Great_Salt_Lake_{year}.tif', 'w', **profile) as dst:\n                    dst.write(water_pixels.reshape(profile['height'], profile['width']), 1)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스크립트는 물 픽셀을 나타내는 QA 값을 정의한 후 지정된 QA 폴더의 파일을 나열합니다. 다음으로, 스크립트는 각 연도별로 물 픽셀을 저장할 배열을 초기화하고 2014년부터 2023년까지의 연도를 반복합니다. 각 연도의 QA 파일에 대해, 파일을 읽고 물 값을 임의의 숫자로 바꾼 다음 나머지 값을 NaN으로 설정합니다. 그런 다음 물 픽셀의 배열을 업데이트하고 물 픽셀이 있는 새 래스터를 생성하여 'Clear_Image' 폴더에 Geotiff 파일로 씁니다.\n\n다음 애니메이션은 스크립트가 Great Salt Lake에서 촬영한 Landsat-8 이미지의 물 픽셀을 유지하여 2021년의 구름 없는 이미지를 생성하는 방법을 보여줍니다.\n\n![애니메이션](https://miro.medium.com/v2/resize:fit:1400/1*DM_1YNYFpMGwXhNRYHC5yA.gif)\n\n코드를 실행한 후에는 각 연도에 대해 Great Salt Lake의 완전하고 구름 없는 이미지가 디렉토리에 저장됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Great Salt Lake Images](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_7.png)\n\n## 🗺️ Visualization of The Great Salt Lake Images\n\nWe will first visualize the raster file to examine the cloud-free images and see how the lake's surface area changes from 2014 to 2023 before calculating its total area.\n\n```python\nimport os\nimport matplotlib.pyplot as plt\nimport rasterio\n\ndef plot_raster_files(folder_path):\n    # Get a list of all raster files in the folder\n    raster_files = [f for f in os.listdir(folder_path) if f.endswith('.tif')]\n    raster_files.sort()\n    \n    # Set up subplots\n    rows, cols = 2, 5  \n    fig, axes = plt.subplots(rows, cols, figsize=(15, 8))\n\n    # Loop through raster files and plot them in subplots\n    for i, file_name in enumerate(raster_files):\n        date_str = file_name.split('_')[3][0:5]\n        file_path = os.path.join(folder_path, file_name)\n        print(file_path)\n\n        # Open the raster file\n        with rasterio.open(file_path) as src:\n            # Read raster data\n            raster_data = src.read(1)\n\n            # Plotting\n            ax = axes[i // cols, i % cols]\n            cmap = ListedColormap(['white', 'blue'])\n            ax.imshow(raster_data, cmap=cmap, vmin=0, vmax=2)\n            ax.set_title(date_str)\n            ax.set_xticks([])\n            ax.set_yticks([])\n\n    # Adjust layout for better visualization\n    plt.tight_layout()\n    plt.show()\n\n# Specify the folder path containing GeoTIFF images \nGeotiff_folder = '/content/Clear_Image'\nplot_raster_files(Geotiff_folder)\n```\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과는 다음과 같습니다:\n\n\u003cimg src=\"/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_8.png\" /\u003e\n\n먼저, 이미지가 전체 호수를 포함하고 구름이 없는 것을 볼 수 있습니다. 이는 우리가 통계 파일을 기반으로 호수 영역의 시계열을 그리는 데 논의한 문제를 해결했습니다. 두 번째로, 최대 면적이 2016 또는 2017에 발생했으며, 최소 면적은 2022에 발생했음을 알 수 있습니다.\n\n## 🎥 Great Salt Lake 수축 타임랩스\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGreat Salt Lake의 표면적 변화를 동적으로 시각화하기 위해, 우리는 호수의 개별 지도를 내보내고 이를 연속으로 배치하여 2014년부터 2023년까지의 타임랩스를 만들 수 있습니다. 이를 수행하는 방법은 다음과 같습니다:\n\n단계 1: 구름이 없는 GeoTIFF 파일에서 개별 지도를 내보냅니다.\n\n```python\n# 폴더 내의 모든 파일 나열\nGeotiff_folder = '/content/Clear_Image'\nall_files = os.listdir(Geotiff_folder)\n\n# 파일 순회\nfor Lake_filename in all_files:\n    if Lake_filename.endswith('.tif'):\n      date_str = Lake_filename.split('_')[3][0:4]\n\n      # 래스터 파일 열기\n      with rasterio.open(os.path.join(Geotiff_folder,Lake_filename)) as src:\n        # 래스터 데이터 읽기\n        raster_data = src.read(1)\n        cmap = ListedColormap(['white','blue'])\n        plt.imshow(raster_data,cmap=cmap, interpolation='nearest', vmin=0, vmax=2)\n        plt.title(f'Great Salt Lake {date_str}')\n        # 축 제거\n        plt.axis('off')\n        # 그림을 파일로 저장\n        plt.savefig(f'/content/Map/{date_str}.png')\n```\n\n단계 2: 이러한 지도를 병합하여 타임랩스를 작성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom PIL import Image\nimport os\nimport glob\n\nfolder_path = '/content/Map'\noutput_gif = os.path.join('/content/Animation', 'Lake_animation.gif')\n\n# 폴더 내 모든 jpg 파일의 정렬된 목록을 가져옵니다\nfile_list = sorted(glob.glob(os.path.join(folder_path, '*.png'))\n\n# 이미지의 원하는 차원을 설정합니다\nwidth, height = 700, 500\n\n# 이미지를 읽고 리스트에 저장합니다\nframes = []\nfor file in file_list:\n    frame = Image.open(file)\n\n    # 이미지 크기 조정\n    frame = frame.resize((width, height), Image.ANTIALIAS)\n\n    # 이미지에 흰색 배경 추가\n    background = Image.new('RGB', frame.size, (255, 255, 255))\n    background.paste(frame)\n    frame = background.convert('RGB')\n\n    # 전역 색상 표를 사용하여 이미지를 P 모드로 변환\n    frame = frame.convert('P', palette=Image.ADAPTIVE, colors=256)\n    frames.append(frame)\n\n\nframes[0].save(output_gif, save_all=True, append_images=frames[1:],\n        duration=500,  # 밀리초로 지속 시간 설정\n        loop=0,        # 루프 수 설정 (0은 무한을 의미)\n        optimize=True)\n```\n\n이외에도 다양한 기능을 사용하여 이미지를 수정 및 GIF 애니메이션을 만들 수 있습니다. 위 코드에서 설정한 이미지 처리를 참고해 보세요.\n\n시간 흐름은:\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*HOgBDjBrNhopFc2jBrRXVw.gif\" /\u003e\n\n## 📉 분류된 이미지로부터 그레이트 솔트 레이크 지역의 시계열 데이터입니다\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n선택지 1인 통계 파일을 기반으로 호수 표면적의 시계열 데이터를 추출한 섹션과 유사하게, 이제 흐릿하지 않은 Landsat-8 이미지에서 표면적을 추정할 수 있습니다. 다음 스크립트에서는 흐릿하지 않은 GeoTIFF 파일을 읽어서 물 픽셀을 세고, 제곱 킬로미터로 표면적을 변환하고, 이를 시계열로 나타내어 추세를 분석합니다.\n\n```js\nimport os\nimport re\nimport rasterio\nimport matplotlib.pyplot as plt\nfrom scipy.stats import linregress\n\nfolder_path = \"/content/Clear_Image\"\nvalue_to_count = 2\n\nfiles = [f for f in os.listdir(folder_path) if f.endswith('.tif')]\nfiles.sort()\n\ndata = {'dates': [], 'pixel_counts': []}\n\nfor file in files:\n    with rasterio.open(os.path.join(folder_path, file)) as src:\n        array = src.read(1) \n        pixel_count = (array == value_to_count).sum()\n        date = file.split('_')[3][0:4]\n        data['dates'].append(date)\n        data['pixel_counts'].append(pixel_count)\n\n# 데이터프레임 생성\ndf = pd.DataFrame(data)\n\n# 면적으로 변환 (제곱 킬로미터)\ndf['area'] = df['pixel_counts'] * 30 * 30 / 1000000\n\n# 그래픽 표현\nplt.figure(figsize=(12, 6))\nplt.bar(df['dates'], df['area'], color='lightblue', label='Max Area Per Year')\nplt.plot(df['dates'], df['area'], marker='o', linestyle='-', color='b', label='Trend Line')\n\n# 추세선을 위한 선형 회귀\nslope, intercept, _, _, _ = linregress(df.index, df['area'])\ntrend_line = slope * df.index + intercept\n\n# 추세 방정식 표시\nplt.plot(df.index, trend_line, color='red', linestyle='--', label=f'Trend Line: y = {slope:.1f}x + {intercept:.1f}')\n\nplt.xticks(fontweight='bold')\nplt.yticks(fontweight='bold')\n\nplt.title('Maximum Area of The Great Salt Lake Per Year')\nplt.ylabel('Max Area (SqKm)', fontsize=12, fontweight='bold')\nplt.grid(True)\nplt.legend(loc='center')\nplt.show()\n```\n\n위 코드를 실행하면 아래와 같은 플롯이 생성됩니다:\n\n\u003cimg src=\"/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_9.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 도표와 통계 파일을 기반으로 한 플롯 간의 주요 차이점은 이 플롯이 우리가 원본 Landsat-8 이미지에서 추정할 수 있는 호수의 가장 정확한 표면적을 나타낸다는 것입니다. 이 플롯은 2014년부터 2016년까지는 상당한 변화가 없고, 2017년과 2019년에는 작은 확장(약 4800 제곱킬로미터에서 약 5300 제곱킬로미터)을 보여주지만, 2019년부터 2022년에는 상당한 감소(약 5300 제곱킬로미터에서 약 4100 제곱킬로미터)가 있었습니다. 2022년부터 2023년에는 다시 그래프 상에서 증가함(약 4100 제곱킬로미터에서 약 4700 제곱킬로미터)이 관찰되었는데, 이는 작년에 받은 상당한 우천량으로 인해 젖은 해가 된 것으로 보입니다.\n\n## ⚖️ 통계 파일 및 이미지로 추출된 시계열 비교\n\n이전에 언급했듯이 QA 통계 파일을 기반으로 표면적을 분석하면 호수 위의 구름이 있는 픽셀에 대한 필터를 보상하지 않기 때문에 해당 지역을 과소평가하는 경향이 있습니다. 통계 파일을 통해 계산된 표면적과 분류된 Landsat-8 이미지에서 추정한 표면적을 비교해 보기 위해 두 가지를 동일한 레이아웃에 플롯하여 통계 파일만을 의존했을 때 얼마나 과소평가가 발생하는지 살펴봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 스크립트는 각 접근 방법에서 보고된 물 픽셀의 수를 제곱 킬로미터 단위의 표면적으로 변환하고 이를 시각화합니다. 결과 그래프는 다음과 같습니다:\n\n![그래프](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_10.png)\n\n해당 그래프는 각 접근 방법에서 계산된 표면적을 보여줍니다: 주황색 선과 빨간 막대는 통계 파일을 나타내며(옵션 1), 파란색 선과 막대는 구름이 없는 이미지를 나타냅니다(옵션 2). 그림에서 볼 수 있듯이, 통계 파일을 사용하면 호수 표면적의 증가 또는 감소의 일반적인 패턴을 추적할 수 있습니다. 그러나 앞서 언급한 대로, 통계 파일은 2015년에는 1000 제곱 킬로미터에서 2020년에는 150 제곱 킬로미터로 이르는 과소평가로 이어집니다. 2020년에 통계와 구름이 없는 이미지에서 계산된 표면적 간의 미세한 차이는 2020년에 캡처된 랜드섯 이미지 중 하나가 거의 구름이 없고 호수 대부분을 포함했기 때문에, 통계 파일과 구름이 없는 추정치 사이의 픽셀 수가 비슷하다는 점을 설명합니다. 반면, 2015년의 큰 차이는 2015년 랜드섯-8이 캡처한 최상의 맑은 이미지가 여전히 다양한 구름이 있는 픽셀을 가지고 있거나 전체 호수를 커버하지 않았기 때문입니다.\n\n## 📄 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위성 이미지의 응용 중 하나는 물체나 풍경의 변화를 모니터링하는 것입니다. 이 연구에서는 Landsat-8에 의해 캡처된 이미지를 사용하여 2014년부터 2023년까지 대연몰의 표면 면적 변동을 분석했습니다. 이 지역은 지난 수십 년 동안 상당한 수축이 있어 지켜지고 있습니다. 두 가지 다른 방법으로 측정된 호수의 면적을 비교하여 우리의 분석에서 작은 오류가 어떻게 오해와 부정확한 정보로 이어질 수 있는지 강조해 보았습니다. 이 분석 결과, 2019년부터 2022년까지 대연몰의 상당한 수축, 2014년부터 2022년까지 10% 수축이 있었습니다. 흥미로운 점은 2023년의 다습한 해가 대연몰의 회복에 중요한 역할을 한 것으로 나타났습니다. 그 해에 10% 증가했습니다. 그러나 2023년을 넘어서의 회복은 불확실하며, 2024년에 추가 개선이 보장되지는 않습니다.\n\n## 📚 참고 자료\n\n2014년부터 2023년까지의 Landsat-8 이미지는 2024/02/20에 https://lpdaac.usgs.gov에서 AppEEARS를 사용하여 검색되었습니다. 이는 미국 지질조사원(USGS) 지구 자원 관측 및 과학(EROS) 센터가 운영하는 NASA EOSDIS Land Processes Distributed Active Archive Center (LP DAAC)에서 유지되었습니다.\n\n📱 더 많은 유익한 콘텐츠를 위해 다른 플랫폼에서 저와 연결하세요! LinkedIn, ResearchGate, Github, Twitter.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 링크를 통해 해당 게시물을 확인할 수 있습니다:","ogImage":{"url":"/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_0.png"},"coverImage":"/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_0.png","tag":["Tech"],"readingTime":21}],"page":"19","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"19"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>