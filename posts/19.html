<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/19" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/19" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유" href="/post/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석" href="/post/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="위성 이미지를 통한 그레이트 솔트레이크 축소 추적 Python 사용 방법" href="/post/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="위성 이미지를 통한 그레이트 솔트레이크 축소 추적 Python 사용 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="위성 이미지를 통한 그레이트 솔트레이크 축소 추적 Python 사용 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">위성 이미지를 통한 그레이트 솔트레이크 축소 추적 Python 사용 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">21<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="마케팅 전략을 지원하는 데이터 시각화 사용법" href="/post/2024-06-23-UsingDataVisualizationstoSupportMarketingStrategies"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="마케팅 전략을 지원하는 데이터 시각화 사용법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-UsingDataVisualizationstoSupportMarketingStrategies_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="마케팅 전략을 지원하는 데이터 시각화 사용법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">마케팅 전략을 지원하는 데이터 시각화 사용법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="2024년 데이터 사이언스를 효과적으로 배우는 방법" href="/post/2024-06-23-HowtoEffectivelyLearnDataSciencein2024"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="2024년 데이터 사이언스를 효과적으로 배우는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-HowtoEffectivelyLearnDataSciencein2024_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="2024년 데이터 사이언스를 효과적으로 배우는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">2024년 데이터 사이언스를 효과적으로 배우는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="해적 위협 이해하기 데이터 기반 해적 분석 방법" href="/post/2024-06-23-UnderstandingMaritimeThreatsAData-drivenApproachtoPiracyAnalysis"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="해적 위협 이해하기 데이터 기반 해적 분석 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-UnderstandingMaritimeThreatsAData-drivenApproachtoPiracyAnalysis_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="해적 위협 이해하기 데이터 기반 해적 분석 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">해적 위협 이해하기 데이터 기반 해적 분석 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Microsoft Power BI에서 Python Seaborn으로 간단한 Pairplot 시각화하기 방법" href="/post/2024-06-23-CreatingsimplePairplotvisualsusingPythonSeaborninMicrosoftPowerBI"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Microsoft Power BI에서 Python Seaborn으로 간단한 Pairplot 시각화하기 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-CreatingsimplePairplotvisualsusingPythonSeaborninMicrosoftPowerBI_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Microsoft Power BI에서 Python Seaborn으로 간단한 Pairplot 시각화하기 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Microsoft Power BI에서 Python Seaborn으로 간단한 Pairplot 시각화하기 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 분석에서 절대 빼먹으면 안 되는 것들" href="/post/2024-06-23-WaitYouForgotSomethinginYourDataAnalysis"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 분석에서 절대 빼먹으면 안 되는 것들" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-WaitYouForgotSomethinginYourDataAnalysis_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 분석에서 절대 빼먹으면 안 되는 것들" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 분석에서 절대 빼먹으면 안 되는 것들</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="처음 데이터 분석가로 일하기 전에 알았더라면 좋았을 5가지 사항" href="/post/2024-06-23-5ThingsIWishIKnewBeforeMyFirstJobasaDataAnalyst"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="처음 데이터 분석가로 일하기 전에 알았더라면 좋았을 5가지 사항" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-5ThingsIWishIKnewBeforeMyFirstJobasaDataAnalyst_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="처음 데이터 분석가로 일하기 전에 알았더라면 좋았을 5가지 사항" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">처음 데이터 분석가로 일하기 전에 알았더라면 좋았을 5가지 사항</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="위성 이미지 시각화를 위한 Streamlit 앱 만들기 단계별 가이드" href="/post/2024-06-23-CreatingaStreamlitAppforSatelliteImageryVisualizationAStep-by-StepGuide"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="위성 이미지 시각화를 위한 Streamlit 앱 만들기 단계별 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-CreatingaStreamlitAppforSatelliteImageryVisualizationAStep-by-StepGuide_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="위성 이미지 시각화를 위한 Streamlit 앱 만들기 단계별 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">위성 이미지 시각화를 위한 Streamlit 앱 만들기 단계별 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">16<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><a class="link" href="/posts/1">1</a><a class="link" href="/posts/2">2</a><a class="link" href="/posts/3">3</a><a class="link" href="/posts/4">4</a><a class="link" href="/posts/5">5</a><a class="link" href="/posts/6">6</a><a class="link" href="/posts/7">7</a><a class="link" href="/posts/8">8</a><a class="link" href="/posts/9">9</a><a class="link" href="/posts/10">10</a><a class="link" href="/posts/11">11</a><a class="link" href="/posts/12">12</a><a class="link" href="/posts/13">13</a><a class="link" href="/posts/14">14</a><a class="link" href="/posts/15">15</a><a class="link" href="/posts/16">16</a><a class="link" href="/posts/17">17</a><a class="link" href="/posts/18">18</a><a class="link posts_-active__YVJEi" href="/posts/19">19</a><a class="link" href="/posts/20">20</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유","description":"","date":"2024-06-23 16:15","slug":"2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy","content":"\n\n## GenAI은 윤리적인 어려움이 있습니다. 데이터 리더들은 이를 어떻게 해결해야 할까요? 이 기사에서는 윤리적 AI의 필요성과 데이터 윤리가 AI 윤리임을 고려합니다.\n\n![이미지](/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png)\n\n기술 경쟁에서 미래 성공의 중요한 요소로 빠르게 이동하는 것은 항상 그렇습니다.\n\n안타깝게도, 너무 빨리 움직이는 것은 무시해선 안 될 위험이 기다리고 있다는 것을 의미하기도 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n옛날부터 전해져 내려오는 이야기입니다. 한 순간에는 선사 시대 모기 유전자를 서열 진행하고, 다음 순간에는 공룡 테마 파크를 개장하고 세계 최초의 실패한 하이퍼루프를 설계하고 있죠 (하지만 분명히 마지막은 아닙니다).\n\nGenAI에 관해서는 인생이 예술을 모방한다고 할 수 있어요.\n\n우리가 AI를 알려진 양으로 생각하고 싶어도, 이 기술의 창조자조차도 그 작동 방식에 대해 완전히 확신하지 못하는 것이 현실입니다.\n\nUnited Healthcare, 구글, 심지어 캐나다 법원 같은 곳에서 발생한 여러 고프로 AI 사태를 고려할 때, 우리가 어디서 잘못되었는지 다시 생각해 봐야 할 때입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이해를 돕기 위해 명확하게 말씀드리겠습니다. GenAI(및 AI 전반)는 결국 각 산업에서 중요한 역할을 하게 될 것으로 믿습니다. 공학 업무를 가속화하거나 일반적인 질문에 답변하는 데 이르기까지 모든 산업에서 중요한 역할을 하게 될 것입니다. 그러나 AI의 잠재적 가치를 실현하기 위해서는 먼저 AI 애플리케이션을 개발하는 방식과 데이터 팀이 그 역할에서 어떤 영향을 끼치는지에 대해 비판적으로 사고해야 합니다.\n\n본 글에서는 AI의 세 가지 윤리적 고민, 데이터 팀의 참여 방식, 그리고 데이터 리더로서 당신이 오늘 할 수 있는 일로 내일을 위한 보다 윤리적이고 신뢰할 수 있는 AI를 전달하는 방법에 대해 알아볼 것입니다.\n\n# AI 윤리의 세 가지 층위\n\n전 뉴욕타임스 데이터 및 인사이트 부문의 전 SVP인 동료인 Shane Murray와 대화를 나누던 중, 그는 처음으로 실제 윤리적 진퇴양난에 직면한 상황 중 하나를 공유해 주었습니다. 뉴욕타임스에서 금융 인센티브를 위한 기계 학습 모델을 개발하던 중, 할인을 결정할 수 있는 기계 학습 모델의 윤리적 영향에 대한 토론이 제기되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일단 할인 코드를 위한 머신 러닝 모델은 모든 것을 고려할 때 꽤 무해한 요청처럼 보였습니다. 그러나 몇 개의 할인 코드를 자동화하는 것이 얼마나 순박한 것인지라고 생각할 수 있는지에 상관없이, 그 비즈니스 문제에서 인간적 공감을 없애는 행위는 팀에게 여러 윤리적 고려사항을 만들었습니다.\n\n단순하지만 기존에는 인간적인 활동이라고 여겨진 것들을 자동화하려는 경쟁은 순전히 실용적인 결정인 것처럼 보입니다 — 효율성을 향상시키는지 아닌지의 단순한 이분법일 뿐입니다. 그러나 인간의 판단을 어떤 부분에서든 배제하면, AI가 관련되었는지 여부와 상관없이, 그 과정의 인간적 영향을 직접적으로 관리하려는 능력도 함께 상실하게 됩니다.\n\n그것은 실제 문제입니다.\n\n인공지능 개발에서 주요한 윤리적 고려사항은 세 가지가 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1. 모델 편향\n\n뉴욕 타임즈에서의 토론 핵심에 다가가는 항목입니다. 모델 자체가 어떤 의도치 않은 결과를 가져다 줄 수 있어서 한 사람을 다른 사람에 비해 유리하게 하거나 불리하게 할 수 있습니까?\n\n이곳에서의 도전 과제는 모든 다른 사항이 동일하다면, 모든 상호작용에 대해 공정하고 중립적인 결과를 일관되게 제공할 수 있도록 GenAI를 설계하는 것입니다.\n\n2. AI 사용\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI의 윤리적 고려 중 가장 존중받는 부분은 해당 기술이 어떻게 활용될지, 그 사용 사례가 회사나 사회 전반에 어떤 영향을 미칠지를 이해하는 것입니다.\n\n이 AI는 윤리적인 목적을 위해 설계되었습니까? 그 사용이 누군가에게 직접적이거나간접적으로 피해를 주지는 않는지? 그리고 궁극적으로 이 모델이 장기적으로 순수한 선을 제공할 것인가?\n\n쥬라기 공원의 첫 번째 장면에서 이안 말콤 박사가 말한 것처럼, 단지 무언가를 만들 수 있다고 해서 반드시 만들어야 한다는 뜻은 아닙니다.\n\n3. 데이터 책임성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, 데이터 팀들에게 가장 중요한 고려 사항(또한 이 글에서 제가 대부분의 시간을 할애할 부분): 데이터 자체가 AI가 책임 있게 구축되고 활용되는 데 어떻게 영향을 미치는지에 대한 문제입니다.\n\n이 고려 사항은 우리가 사용하는 데이터를 이해하고, 어떤 상황에서 안전하게 사용할 수 있는지, 그에 따른 위험 요소가 무엇인지를 다룹니다.\n\n예를 들어, 우리는 데이터가 어디에서 왔는지, 어떻게 획득되었는지를 알고 있나요? 특정 모델에 공급되는 데이터에 개인정보 문제가 있는가요? 개인들을 피해로 치는 데 위험에 노출시키는 개인 데이터를 활용하고 있나요?\n\n알 수 없는 데이터 위에 훈련된 LLM에서 안전하게 진행해도 괜찮을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNew York Times가 OpenAI에 대한 소송에서 강조한 대로, 우리는 첫째로 이 데이터를 사용할 권리가 있는 걸까요?\n\n데이터의 품질이 중요한 역할을 하는 곳이기도 합니다. 주어진 모델을 피드하는 데이터의 신뢰성을 신뢰할 수 있을까요? 퀄리티 문제가 AI 제품에 도달할 경우 잠재적인 결과는 무엇일까요?\n\n따라서, 이러한 윤리적인 문제들을 전체적으로 쳐다보았으니, 이제 데이터 팀이 이에 대해 책임을 져야 하는 이유를 살펴봅시다.\n\n# 데이터 팀이 AI 윤리에 대해 책임져야 하는 이유\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 팀과 관련된 모든 윤리적 AI 고려사항 중에서 가장 중요한 것은 데이터 책임 문제입니다.\n\n마찬가지로 GDPR이 비즈니스와 데이터 팀이 함께 데이터 수집 및 활용 방식을 재고하도록 강제했던 것처럼, GenAI는 기업이 어떤 워크플로우를 자동화할 수 있는지를 재고하도록 강제할 것입니다.\n\n우리 데이터 팀으로서 어떤 AI 모델의 구축에 영향을 미치려는 책임이 절대적이지만, 그 설계 결과에 직접적으로 영향을 끼칠 수는 없습니다. 그러나 그 모델에서 잘못된 데이터를 거를 수 있다면, 그 설계 결함에 따른 위험을 완화하는 데 많은 도움이 될 수 있습니다.\n\n모델 자체가 우리의 통제 영역을 벗어나 있다면, 할지 말지에 대한 본질적인 질문은 전혀 다른 달에 있는 것입니다. 우리는 그 문제점을 발견하면 지적할 책임이 있지만, 마무리로 말하자면, 우리가 탑승하든 말든, 로켓은 발사될 것입니다.\n가장 중요한 것은 로켓이 안전하게 발사되도록 하는 것입니다. (아니면 비행기 몸통을 훔치는 것도...)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서 데이터 엔지니어들의 삶의 모든 영역과 마찬가지로, 우리가 시간과 노력을 투자하고 싶은 곳은 최대한 많은 사람들에게 가장 직접적인 영향을 미칠 수 있는 곳입니다. 그 기회는 데이터 자체에 있다는 것이 사실입니다.\n\n# 데이터 팀이 데이터 책임성에 신경 써야 하는 이유\n\n말해봐야할 것 같지만 너무 당연한 것 같지만, 그래도 말하겠습니다:\n\n데이터 팀은 데이터가 AI 모델로 어떻게 활용되는지에 대한 책임을 져야 합니다. 왜냐하면 단순히 말하자면 그들만이 그것을 할 수 있는 유일한 팀이기 때문입니다. 물론, 준수 팀, 보안 팀, 심지어 법률 팀들이 윤리가 무시될 때 책임을 져야 할 수 있습니다. 그러나 얼마나 많은 책임을 공유할 수 있더라도, 그 팀들이 결국은 데이터 팀만큼 데이터를 동일한 수준에서 이해하지는 못할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n소프트웨어 엔지니어링 팀이 OpenAI나 Anthropic과 같은 써드파티 LLM을 사용하여 앱을 만든다고 상상해보세요. 그러나 그들은 그들의 애플리케이션에 실제로 필요한 데이터 외에도 위치 데이터를 추적하고 저장하고 있다는 것을 깨닫지 못했습니다. 그들은 모델을 구동하기 위해 전체 데이터베이스를 활용합니다. 올바른 논리 결함이 있으면 나쁜 행위자가 그 데이터세트에 저장된 데이터를 사용하여 임의의 개인을 추적하는 프롬프트를 손쉽게 공학적으로 만들 수 있습니다. (이것은 오픈 소스 LLM과 폐쇄 소스 LLM 사이의 긴장 관계입니다.)\n\n또는 소프트웨어 팀이 위치 데이터에 대해 알고 있지만 위치 데이터가 실제로 근삿값일 수 있다는 것을 깨닫지 못했습니다. 그들은 그 위치 데이터를 사용하여 16세 소년을 단순히 거리에 있는 피자 헛(Pizza Hut)이 아닌 어둡고 좁은 골목으로 안내하는 AI 매핑 기술을 만들 수 있습니다. 물론 이러한 유형의 오류는 의지적이 아니지만 데이터가 활용되는 방식에 내재된 의도하지 않은 위험을 강조합니다.\n\n이러한 예시와 다른 사례들은 윤리적 AI에 관한 문제에서 데이터 팀의 역할을 강조합니다.\n\n# 그렇다면, 데이터 팀이 윤리적으로 유지하는 방법은 무엇일까요?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대부분의 경우, 데이터 팀은 모델이 작동하도록 대략적이고 대리 데이터를 다루는 데 익숙합니다. 그러나 AI 모델을 제공하는 데이터의 경우, 실제로 훨씬 더 높은 수준의 검증이 필요합니다.\n\n소비자를 위해 틈을 메우기 위해 데이터 팀은 데이터 관행과 그 관행이 조직 전반과 어떻게 관련되는지 신중히 살펴봐야 합니다.\n\nAI의 위험을 완화하는 방법을 고려할 때, 아래는 데이터 팀이 미래에 더 윤리적인 방향으로 AI를 움직이기 위해 취해야 할 3단계입니다.\n\n# 1. 회의 참석 자리를 얻으세요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 팀은 타조가 아닙니다 - 머리를 모래속에 파묻고 문제가 사라지기를 기대할 수 없습니다. 데이터 팀이 리더십 테이블에 자리를 놓기 위해 싸워온 것처럼, 데이터 팀은 AI 테이블에 자리를 얻을 수 있도록 옹호해야 합니다.\n\n어떤 데이터 품질 위기든, 지구가 이미 타버린 후에 뛰어든다면 충분하지 않습니다. GenAI에 고유하게 내재된 종말적 위험을 다룰 때, 우리 자신의 개인적 책임에 대해 선제적으로 대처하는 것이 이전보다 중요합니다.\n\n그들이 당신이 테이블에 앉을 수 있도록 허락하지 않는다면, 바깥에서 교육하는 책임이 있습니다. 당신의 힘을 다하여 훌륭한 발견, 거버넌스 및 데이터 품질 솔루션을 제공하여 그 팀들에게 정보를 제공하면서 책임 있는 결정을 내릴 수 있도록 해야 합니다. 그들에게 사용할 것, 언제 사용할 것, 그리고 당신 팀의 내부 프로토콜로 유효성을 검증할 수 없는 써드파티 데이터 사용의 위험성을 가르쳐 주세요.\n\n이것은 단지 비즈니스 문제가 아닙니다. 유나이티드 헬스케어와 브리티시 컬럼비아 주가 말할 것처럼, 많은 경우, 이들은 실제 사람들의 삶과 생계에 관한 것입니다. 그러니, 우리가 그 관점으로 운영하고 있는지 확인합시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2. RAG과 같은 방법론을 활용하여 더 책임감 있는 - 그리고 신뢰할 수 있는 - 데이터 조직화하기\n\n우리는 종종 RAG(검색 증강 생성) 같은 방법론을 언급하며 인공 지능으로부터 가치를 창출하는 자원으로 생각합니다. 그러나 그것은 그 AI를 구축하고 사용하는 방식을 보호하는 자원이기도 합니다.\n\n예를 들어, 모델이 소비자를 대상으로 하는 채팅 앱에 공급하기 위해 개인 고객 데이터에 액세스하는 경우를 상상해보십시오. 올바른 사용자 프롬프트가 모델이 동작하여 심각한 PII가 폭로되어 나쁜 행위자가 그것을 취할 수도 있습니다. 그래서 데이터가 어디에서 왔는지 확인하고 제어하는 능력은 그 AI 제품의 무결성을 보호하는 데 중요합니다.\n\n책임있는 데이터 팀은 RAG와 같은 방법론을 활용하여 준수되고 더 안전하며 모델에 적합한 데이터를 신중하게 조직화하여 많은 위험을 완화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI 개발에 RAG(빨강, 주황, 초록) 접근 방식을 취하면 너무 많은 데이터를 처리할 때 발생할 수 있는 위험을 최소화하는 데 도움이 됩니다 — 우리의 위치 데이터 예시에서 언급된 대로.\n\n그것이 실제로 어떻게 보이는지 궁금하시죠? 예를 들어 Netflix와 같은 미디어 회사라고 가정해봅시다. 이 회사는 고객 데이터의 일부를 활용하여 맞춤형 추천 모델을 만들어야 합니다. 그런 다음 해당 사용 사례에 대한 구체적이고 제한된 데이터 포인트를 정의하고, 그 데이터를 유지하고 유효성을 검사할 책임자가 누구인지, 어떤 상황에서 데이터를 안전하게 사용할 수 있는지, 그리고 시간이 지남에 따라 그 AI 제품을 개발하고 유지할 가장 적합한 사람이 누구인지를 보다 효과적으로 정의할 수 있게 됩니다.\n\nData lineage과 같은 도구는 데이터의 출처를 신속하게 확인하여 팀이 어떤 시점에서 데이터가 어디에서 유래되고 사용되는지 또는 잘못 사용되는지를 확인할 수 있도록 도와줄 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 3. 데이터 신뢰도를 우선시하세요\n\n데이터 제품에 대해 이야기할 때 자주 하는 말이 있습니다. \"쓰레기 데이터를 넣으면 쓰레기 데이터가 나온다,\" 하지만 GenAI의 경우에는 그 말이 조금 모자라다고 볼 수 있습니다. 실제로 쓰레기 데이터가 AI 모델로 들어가면 쓰레기만 나오는 것이 아니라 실제로 사람에게도 영향을 미치는 결과가 나올 수 있습니다.\n\n그래서 모델로 공급되는 데이터를 제어하기 위해 RAG 아키텍처가 필요한만큼, Pinecone과 같은 벡터 데이터베이스에 연결되는 강력한 데이터 관찰 기능이 필요합니다. 데이터가 실제로 깨끗하고 안전하며 신뢰할 수 있는지 확인하기 위해 이 연결이 중요합니다.\n\nAI를 시작하는 고객들로부터 가장 많이 들은 불만 중 하나는, 제품용 AI를 준비하는 것은, 벡터 데이터 파이프라인으로 인덱스를 적극적으로 모니터링하지 않는다면 데이터의 신뢰성을 검증하는 것이 거의 불가능하다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대부분의 경우, 데이터 및 AI 엔지니어들이 데이터에 문제가 발생했음을 알게 되는 유일한 방법은 모델이 나쁜 프롬프트 응답을 내뱉을 때입니다. 그때에는 이미 너무 늦은 시점이죠.\n\n# 현재가 가장 좋은 때입니다\n\n2019년에 우리 팀이 데이터 관찰 카테고리를 만들었던 것은 데이터의 신뢰성과 신뢰의 필요성을 높인 동일한 과제입니다.\n\n오늘날 AI가 일상적으로 의지하는 다양한 프로세스 및 시스템을 바꿔놓을 것이라고 약속하는 가운데, 데이터 품질의 도전과 더 중요한 것으로서의 윤리적 영향이 더욱 심각해지고 있습니다.","ogImage":{"url":"/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png"},"coverImage":"/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png","tag":["Tech"],"readingTime":8},{"title":"사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석","description":"","date":"2024-06-23 16:14","slug":"2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo","content":"\n\nMichelangelo은 Uber 내부 ML-as-a-service 플랫폼으로, 기업의 요구를 충족하기 위해 AI를 확장하고 머신 러닝을 민주화하는 서비스입니다. 데이터 관리, 모델 학습, 평가, 배포, 예측 및 모니터링 기능을 하며 전통적인 머신 러닝 모델부터 딥 러닝까지 다양한 모델을 다룰 수 있습니다. Michelangelo은 오랫동안 Uber에서 사용 중이며 여러 Uber 데이터 센터에 배포되어 있습니다. 이 기사에서는 Michelangelo의 개발 동기와 아키텍처에 대해 자세히 살펴보겠습니다.\n\nMichelangelo 이전에 Uber는 scikit-learn, R 등과 같은 온라인에서 사용 가능한 오픈 소스 도구를 사용했으나 ML의 영향력은 소수의 데이터 과학자와 엔지니어가 주로 오픈 소스 도구를 사용하여 구축할 수 있는 한도를 벗어날 수 없었습니다. 따라서 Uber는 규모화된 훈련 및 예측 데이터를 관리하기 위한 신뢰성 있고 일관된, 재현 가능한 파이프라인을 구축하고자 했습니다. 가장 중요한 것은 실험 비교 및 모델을 프로덕션 환경에 배포하는 과정이 명확히 정립되지 않았습니다. Uber의 엔지니어링 팀은 해당 프로젝트에 특정한 사용자 정의 서빙 컨테이너를 만들어야 했습니다. Michelangelo는 이러한 문제를 해결하기 위해 팀 간 워크플로우를 표준화하고 엔지니어들이 쉽게 규모에 맞게 머신 러닝 시스템을 구축하고 운영할 수 있도록 하는 엔드 투 엔드 시스템을 통해 설계되었습니다. Michelangelo은 주로 아이디어에서 첫 프로덕션 모델로의 경로 축소와 그 이후 빠른 반복에 초점을 맞추었습니다. UberEATS의 사용 사례를 통해 전체 프로세스를 더 자세히 알아보겠습니다.\n\nUberEATS에는 Michelangelo에서 실행되는 여러 머신 러닝 모델이 있습니다. 배달 시간 예측, 검색 순위, 음식점 순위 등을 다루는 모델들이 포함되어 있습니다. 배송 시간 모델은 주문이 발생하기 전에 음식이 준비되고 배달되는 데 얼마나 걸릴지 예측합니다. 그렇다면 UberEATS는 어떻게 작동할까요? 고객이 주문하면 레스토랑으로 전송되어 처리됩니다. 레스토랑은 주문을 확인하고 식사를 준비해야 하며, 주문의 복잡성과 레스토랑의 혼잡 정도에 따라 필요한 시간이 달라집니다. 식사가 준비되기 직전에 Uber 딜리버리 파트너가 음식을 가져가도록 지시됩니다. 그런 다음, 딜리버리 파트너는 레스토랑에 도착하고 음식을 가져와 고객의 위치로 운전하고(경로, 교통 등에 따라 달라집니다) 고객의 집으로 걸어가 배달을 완료해야 합니다. 이 복잡한 프로세스의 총 소요 시간을 예측하고 프로세스의 각 단계에서 이러한 시간까지 재계산하는 것이 목표입니다.\n\nUberEATS의 데이터 과학자들은 end-to-end delivery 시간을 예측하기 위해 그라디언트 부스트된 의사 결정 트리 회귀 모델을 사용합니다. 모델의 features로는 하루 중 시간, 배달 위치, 평균 식사 준비 시간 등이 포함됩니다. 이러한 모델들은 Michelangelo 모델 서빙 컨테이너에 배포되어 Uber의 데이터 센터 전체에 통합되며, UberEATS 마이크로서비스에 의해 네트워크 요청을 통해 호출됩니다. 이러한 예측은 고객이 식사가 준비되고 배달되는 동안에 표시됩니다. Michelangelo은 Uber 내부에서 개발된 구성 요소와 오픈 소스 시스템의 혼합을 사용하여 구축되었습니다. XGBoost, Tensorflow, Spark 등의 오픈 소스 구성 요소가 사용되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMichelangelo의 데이터 관리 구성 요소는 온라인과 오프라인 파이프라인으로 나뉩니다. 오프라인 파이프라인은 일괄 모델 훈련 및 일괄 예측 작업에 사용되며, 온라인 파이프라인은 온라인, 낮은 지연 시간 예측을 수행합니다. 또한 Michelangelo에는 기능 스토어가 있어 팀이 기계 학습 문제용 다른 기능 세트를 공유하고 발견할 수 있습니다. Uber의 모든 데이터는 먼저 HDFS 데이터 레이크에 저장되며, 오프라인 파이프라인에서 기능을 계산하기 위해 액세스할 수 있으며 온라인에 배포된 모델은 HDFS에 저장된 데이터에 액세스할 수 없습니다. 따라서 온라인 모델용 기능은 미리 계산되어 Cassandra 기능 스토어에 저장되어서 예측 시 낮은 대기 시간으로 읽을 수 있습니다.\n\n앞에서 말했듯이 Uber는 중앙 집중식 기능 스토어를 만드는 데 높은 우선순위를 둡니다. Uber의 팀은 기능을 생성하고 관리하고 다른 사람과 공유할 수 있기 때문에 새로운 기능을 추가하는 것이 쉬워지며 기능 스토어에 기능이 있으면 사용하기 매우 쉬워집니다. Michelangelo에는 모델러가 훈련 및 예측 시 모델로 보내는 기능을 선택, 변환 및 결합하는 데 사용하는 DSL(도메인 특화 언어)이라는 것이 있습니다. DSL은 모델 구성 자체의 일부이며, 훈련 시점과 예측 시점에 적용되어 동일한 최종 기능 집합이 두 경우에 모델로 생성되어 보내지도록 보장합니다.\n\nUber는 수억 개의 샘플을 처리할 수 있는 분산 모델 훈련 시스템을 사용하며 의사결정 트리, 신경망 및 선형 모델과 같은 알고리즘을 위한 빠른 반복을 위해 작은 데이터 세트로 축소할 수도 있습니다. 모델 구성은 모델 유형, 초매개변수, 데이터 소스 참조 및 계산 리소스 요구 사항을 지정하며, 이것은 훈련 작업을 구성하는 데 사용됩니다. 모델 훈련 후 성능 메트릭이 계산되고 모델 평가 보고서에 통합됩니다. 훈련이 완료된 후, 원래 구성, 학습된 매개변수 및 평가 보고서가 분석 및 배포를 위해 모델 저장소에 저장됩니다. Michelangelo는 모든 모델 유형에 대한 초매개변수 검색을 지원하며 모든 훈련 작업은 API 및 워크플로우 도구를 통해 관리됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 마이셀란젤로가 어떻게 모델을 학습하고 평가하며 배포하는지에 대해 설명한 내용입니다. 마이셀란젤로는 특정 사용 사례에 이상적인 모델에 이르기까지 수백 개의 모델을 학습합니다. 이러한 수백 개의 모델은 엔지니어들을 최적의 성능을 보이는 모델 구성으로 안내하며, 학습된 모델을 추적하고 평가하고 비교하는 것이 마이셀란젤로에서 주요 관심사입니다. 모델 훈련 시, 마이셀란젤로에 저장된 각 모델은 학습자, 모델 구성, 정확도 측정, 학습된 매개변수 등의 정보가 포함된 버전화된 객체로 카산드라 모델 저장소에 저장됩니다. 이러한 정보는 웹 UI 또는 API를 통해 쉽게 확인하고 비교할 수 있습니다. 마이셀란젤로는 또한 모델이 동작하는 이유를 이해하고 필요한 경우 디버깅하는 데 도움이 되는 시각화 도구를 제공합니다. 특성 보고서에는 각 특성이 모델의 중요도에 따라 부분 의존 플롯 및 분포 히스토그램이 표시됩니다.\n\n배포를 위해 마이셀란젤로는 UI 또는 API를 통해 모델 배포를 관리하는 엔드 투 엔드 지원을 제공합니다. 오프라인 모델은 요청에 따라 또는 정기적인 일정에 따라 실행되는 스파크 작업에서 실행되는 컨테이너에 배포됩니다. 온라인 모델은 수신 요청을 기반으로 예측하는 서비스 클러스터에 배포됩니다. 두 경우 모두, 모델 아티팩트는 ZIP 아카이브로 패키징되어 우버의 데이터 센터 각 위치에 복사됩니다. 예측 컨테이너는 자동으로 디스크에서 새 모델을 로드하고 예측 요청을 처리하기 시작합니다. 우버의 여러 팀은 마이셀란젤로의 API를 통해 정기적인 모델 재훈련과 배포를 예약하기 위한 자동화 스크립트를 보유하고 있습니다. 모델이 배포된 후, 데이터 파이프라인 또는 클라이언트 서비스에서 로드된 특성을 기반으로 예측을 수행합니다. 온라인 모델의 경우 예측은 네트워크를 통해 클라이언트 서비스로 반환되며, 오프라인 모델의 경우 예측 결과는 하이브(데이터 웨어하우스)에 다시 기록되어 SQL 기반 쿼리 도구를 통해 직접 액세스할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 번에 여러 모델을 동일한 서빙 컨테이너에 배포할 수 있습니다. 이를 통해 이전 모델에서 새로운 모델로의 쉬운 전환과 모델의 A/B 테스트를 병행할 수 있습니다. 서빙 시간에는 모델이 해당 태그로 최근에 배포된 모델을 사용하여 식별됩니다. 온라인 모델의 경우 확장을 위해 예측 서비스 클러스터에 더 많은 호스트가 추가되고 로드 밸런서가 부하를 분산시킵니다. 오프라인 예측의 경우 더 많은 Spark 실행자가 추가되며 Spark가 병렬성을 관리합니다. 예측을 모니터링하기 위해 Michelangelo는 자동으로 로그를 기록하고 선택적으로 생성된 예측의 일부를 보류하고 나중에 해당 예측을 데이터 파이프라인이 생성한 레이블과 비교합니다. 앞으로 Uber는 AutoML, 분산 딥 러닝 및 기타 도구 및 서비스를 추가하여 기존 시스템을 강화할 계획입니다.\n\n```js\n참고 자료:\n1. Uber 엔지니어링 블로그\n```","ogImage":{"url":"/assets/img/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo_0.png"},"coverImage":"/assets/img/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo_0.png","tag":["Tech"],"readingTime":5},{"title":"위성 이미지를 통한 그레이트 솔트레이크 축소 추적 Python 사용 방법","description":"","date":"2024-06-23 16:11","slug":"2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython","content":"\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*HOgBDjBrNhopFc2jBrRXVw.gif\" /\u003e\n\n# 목차\n\n- 🌅 위대한 솔트 레이크 수축 문제 소개\n- 💾 Landsat-8 이미지 다운로드\n- 📈 통계 파일로부터 위대한 솔트 레이크 지역의 시계열\n- ⚙️ Landsat-8 이미지 처리\n- 🗺️ 위대한 솔트 레이크 이미지 시각화\n- 🎥 위대한 솔트 레이크 수축의 타임 랩스\n- 📉 분류된 이미지로부터 위대한 솔트 레이크 지역의 시계열\n- ⚖️ 통계 파일 및 이미지에서의 시계열 비교\n- 📄 결론\n- 📚 참고문헌\n\n## 🌅 위대한 솔트 레이크 수축 문제 소개\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n유타 주의 미국 대소염소인, 위도 메리칼크 지역 치축하고 있어요. 여러 보고서에 의하면, 이 호수는 1986년 최대 규모 대비 30% 이상으로 줄어들었다고 합니다. 이 호수의 수위가 저하되는 이유로 기후 변화와 농업용 수분 분할 등이 언급되었습니다.\n\n기후 변화 요인에 대해 보도된 바에 의하면 강수량 패턴이 변화되고 온도가 상승함에 따라 스노우팩이 감소하고 호수로의 유입량이 줄어들고 있다고 합니다.\n\n두 번째 이유에 대해, 최근 몇 년 동안 도시 및 농업 지역이 확장되어 왔습니다. 도시 및 농업 분야의 수요 증가는 물 수위의 하락에 더해진 요인이 되었습니다.\n\n장기적인 하강은 생태계에 심각한 영향을 미치며, 특히 공기와 물의 질에 영향을 미칩니다. 호법 지역의 노출은 먼지와 염분을 방출함으로써 공기와 물의 질에 대한 위험을 증가시킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n상황이 동적이며 감소하는 수위를 추적하는 논의가 계속되고 있기 때문에 저는 이 게시물을 작성하기로 결정했습니다. 우리가 위성 이미지를 사용하여 호수 표면적의 변화를 모니터링하는 방법을 보여주기 위해서입니다. 이것은 축소의 지표로 사용됩니다.\n\n저는 2014년부터 2023년까지 Landsat-8에 의해 촬영된 모든 이미지를 사용하여 구글 Colab에서 Python을 사용하여 그들을 분석했습니다. 지난 어떤 기간 동안 호수 표면적의 시계열을 추출하거나 심지어 이 접근 방법을 다른 호수에 적용하는 것에 관심이 있다면, 이 블로그 게시물은 여러분을 위한 것입니다.\n\n## 💾 Landsat-8 이미지 다운로드\n\nLandsat-8 이미지를 다운로드하기 위해 AρρEEARS라는 웹 앱을 사용했습니다. 이 웹 앱을 통해 관심 영역(AOI)에 대한 위성 이미지를 다운로드할 수 있습니다. 이미지는 자르고 메타데이터 및 통계 파일로 제공됩니다. AOI에 대한 폴리곤을 그리고 제품을 선택하고 시작 및 종료 날짜를 선택하기만 하면 됩니다. 이미 이 웹 앱에 관한 게시물을 작성해 놓았습니다. 여기서 확인해보세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 게시물에서는 Great Salt Lake 주변에 폴리곤을 그리고 Landsat-8 ARD 제품을 선택하여 시작 시간과 종료 시간을 2014년부터 2023년까지 여름(6월, 7월 및 8월) 기간으로 설정했습니다. 여름 동안에만 촬영된 이미지를 고려한 결정은 두 가지 이유로부터 나왔어요: (1) 여름 동안 보다 맑은 이미지를 더 유추할 수 있는 가능성이 더 높고, (2) 이는 계산 시간을 제한하기 때문입니다. 이 여름 동안의 모든 연도에 대한 Landsat-8 이미지를 다운로드하므로 결과(호수의 추정 표면적)는 서로 다른 연도 간에 비교할 수 있어요.\n\n데이터 정책: USGS 웹사이트에 따르면, \"USGS에서 다운로드한 Landsat 데이터에는 제한이 없으며, 원하는 대로 사용하거나 재배포할 수 있습니다\" (링크) 및 \"LP DAAC로부터 획득한 LP DAAC의 모든 현재 데이터와 제품은 재사용, 판매 또는 재배포에 대한 제한이 없습니다\" (링크).\n\n## 📈 통계 파일에서 Great Salt Lake 지역의 시계열\n\nAρρEEARS에서 요청을 제출한 후, 호수의 표면적을 추정할 두 가지 옵션이 있습니다. 옵션 1은 통계 CSV 파일에 보고된 미리 계산된 물 픽셀 수를 사용하는 것이며, 옵션 2는 분류된 이미지를 처리하여 물 픽셀을 계산하는 것인데, 이는 추후 설명하겠습니다. 옵션 1을 따르려면 AρρEEARS에서 \"L08-002-QA-PIXEL-CU-Statistics-QA.csv\" 및 \"L08-002-QA-PIXEL-CU-lookup.csv\" 두 파일을 다운로드하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 파일을 빠르게 살펴보겠습니다:\n\n![이미지1](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_0.png)\n\n![이미지2](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_1.png)\n\n첫 번째 파일인 \"L08-002-QA-PIXEL-CU-Statistics-QA.csv\"에서는 각 이미지(행)에 대해 각 클래스(열)의 픽셀 수가 보고됩니다. 이러한 열 이름을 해독하려면 두 번째 파일인 \"L08-002-QA-PIXEL-CU-lookup.csv\"이 필요합니다. 'Water'로 레이블된 열을 필터링하면 물 픽셀에 대한 해당 클래스 이름(열)을 찾을 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Great Salt Lake](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_2.png)\n\n수집된 코드들 중 물 클래스로 고려된 코드는 21890, 21952, 22146, 22208, 54662, 54724, 54918, 54980입니다. 따라서 \"L08–002-QA-PIXEL-CU-Statistics-QA.csv\" 파일에서 이 열들만 선택하고 픽셀 수를 합산하면 지난 10년간의 Great Salt Lake 지역의 시계열을 시각화할 수 있습니다. 이를 위한 Python 스크립트를 작성해 봅시다:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ncsv_file_path = 'L08-002-QA-PIXEL-CU-Statistics-QA.csv'\n\n# 물 클래스를 위한 열 이름\nselected_columns = ['21890', '21952', '22146','22208','54662','54724','54918','54980']\n\n# CSV 파일을 pandas DataFrame으로 읽기\ndf = pd.read_csv(csv_file_path)\n\n# 'date' 열을 날짜 형식으로 변환\ndf['date'] = pd.to_datetime(df['Date'])\n\n# 물 열 선택\nselected_data = df[['Date'] + selected_columns]\n\n# 각 행의 합산 계산\nselected_data['sum'] = selected_data[selected_columns].sum(axis=1)\n\n# 플롯\nplt.figure(figsize=(10, 6))\nplt.plot(selected_data['Date'], selected_data['sum'], marker='o', linestyle='-', color='b')\n\nplt.title('시간에 따른 물 픽셀의 합산')\nplt.ylabel('합계')\nplt.grid(True)\n\nlocator = mdates.MonthLocator() \nplt.gca().xaxis.set_major_locator(locator)\n\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n출력 결과는:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Tracking The Great Salt Lake's Shrinkage Using Satellite Images Python](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_3.png)\n\n만약 AOI에서 각 이미지로 분류된 픽셀의 물의 수를 그래프로 그린다면, 각 Landsat 이미지가 전체 호수를 커버할 수 있는 것을 보장할 수 없습니다. 게다가 우리는 흐린 픽셀들을 걸러냅니다. 다시 말해, Landsat 이미지가 전체 호수를 커버하더라도, 만약 호수 위에 구름이 있다면, 깨끗한 픽셀의 물로 분류되는 숫자는 전체 호수의 표면을 과소 평가할 수 있습니다. 이 두 문제의 예시로 각 이미지를 고려해 보겠습니다.\n\n![Example Image 1](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_4.png)\n\n여기서 보듯이, 왼쪽 이미지(2015년 7월 1일)는 전체 호수를 커버하지 않고 일부가 잘린 상태입니다. 이 이미지를 기반으로 호수의 표면적을 계산하는 것은 오해를 불러일으킬 수 있으며, 시계열에서 상당한 하강을 보여줄 수 있습니다. 오른쪽 이미지(2015년 6월 22일)에 대해선, 전체 호수를 커버하고 있지만, 물 픽셀의 수가 표면적을 계산하기 위한 정확한 정보를 제공할 수 없을 수 있습니다. 보여진 것처럼, 흐린 픽셀이 걸러져 나가면서 호수의 표면적이 과소 평가되는 문제가 발생합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한, 시계열 그래프가 각 이미지의 총 물 픽셀을 표시하여 전반적인 감소 추세를 관측하기 어렵게 만듭니다.\n\n이러한 문제를 해결하려면 코드를 약간 수정해야 합니다. 먼저, 각 이미지의 물 픽셀 수를 그리는 대신 데이터를 집계하고 각 연도별로 보고된 최대 물 픽셀 수를 얻어야 합니다. 둘째, 다음 변환을 사용하여 제곱 킬로미터 (sqKm)로 호수 면적을 그릴 것입니다:\n\n호수 면적 (Km²) = (랜드셧 픽셀 크기) x (물 픽셀 수) / 10⁶\n\n* 랜드셧 픽셀 크기 = 30m x 30m\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom scipy.stats import linregress\n\n\ncsv_file_path = 'L08-002-QA-PIXEL-CU-Statistics-QA.csv'\n\n# 물 픽셀을 위한 열 이름\nselected_columns = ['21890', '21952', '22146', '22208', '54662', '54724', '54918', '54980']\n\n# CSV 파일을 pandas DataFrame으로 읽기\ndf = pd.read_csv(csv_file_path)\n\n# 'date' 열을 datetime 형식으로 변환\ndf['date'] = pd.to_datetime(df['Date'])\n\n# 'date' 열에서 연도 추출\ndf['year'] = df['date'].dt.year\n\n# 열 선택\nselected_data = df[['year'] + selected_columns]\n\n# 각 행의 합 계산\nselected_data['sum'] = selected_data[selected_columns].sum(axis=1)\n\n# 각 연도의 최대 값을 계산\nmax_values_per_year = selected_data[['year','sum']].groupby('year').max()\n\n# 면적으로 변환 (평방 킬로미터)\nmax_values_per_year['area'] = max_values_per_year['sum'] * 30 * 30 / 1000000\n\n# 플로팅\nplt.figure(figsize=(12, 6))\nplt.bar(max_values_per_year.index, max_values_per_year['area'], color='lightblue', label='연도별 최대 면적')\nplt.plot(max_values_per_year.index, max_values_per_year['area'], marker='o', linestyle='-', color='b', label='추세선')\n\n# 추세선을 위한 선형 회귀\nslope, intercept, _, _, _ = linregress(max_values_per_year.index, max_values_per_year['area'])\ntrend_line = slope * max_values_per_year.index + intercept\n\n# 추세선 방정식 표시\nplt.plot(max_values_per_year.index, trend_line, color='red', linestyle='--', label=f'추세선: y = {slope:.4f}x + {intercept:.4f}')\n\nplt.xticks(fontweight='bold')\nplt.yticks(fontweight='bold')\n\nplt.title('연도별 대소염소 지역 최대 면적')\nplt.ylabel('최대 면적 (평방 킬로미터)', fontsize=12, fontweight='bold')\nplt.grid(True)\nplt.legend(loc='center')\nplt.show()\n```\n\n위의 플롯을 참고해주세요.\n\n우리가 데이터를 집계하는 코드 수정이 도움이 된 것으로 보입니다. 그러나 결과는 아직 정확하지 않을 수 있습니다. 왜냐하면 우리는 각 연도에 촬영된 모든 이미지에서 대소염소 호수의 최대 면적을 추출하기 때문입니다. 예를 들어, 2015년에 가장 명확한 이미지가 15%의 구름을 가지고 있다고 가정해 봅시다. 구름이 걸러지면 호수의 표면적을 15% 과소 평가하게 됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 문제를 해결하기 위해서는, 호수 위에서 찍힌 원본 분류된 이미지들과 작업해야 합니다. 구체적으로는, 각 이미지마다 수집된 픽셀 중 물로 분류된 것을 오버레이하여 다운로드해야 합니다. 이 과정을 거치면 각 연도별로 가장 정확한 구름 없는 호수 이미지를 작성할 수 있으며 표면적을 정확하게 추출할 수 있게 될 것입니다. 이 방법에 대해 다음 섹션에서 다룰 예정입니다.\n\n## ⚙️ Landsat-8 이미지 처리\n\n이번과 다음 섹션에서는 통계 파일이 아닌 실제 Landsat-8 분류된 이미지를 다루게 됩니다.\n\nAρρEEARS에서 또는 API를 통해 L08.002_QA_PIXEL_CU_doy`YYYYMMDD`_aid0001와 같이 명명된 파일들을 수동으로 다운로드할 수 있습니다. API를 사용하려는 경우 검색 필드에 'QA_Pixel'을 입력하여 파일을 필터링한 후 AρρEEARS에서 모든 파일을 선택하고 '다운로드'를 클릭하세요. 그런 다음 아래 코드를 실행하여 API를 통해 파일을 다운로드할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계 1: 폴더 만들기\n\n```python\nimport os\n\n# 폴더 이름 목록\nfolders = ['QA', 'Clear_Image', 'Map', 'Animation']\n\nbase_path = '/content'  \n\n# 폴더 생성\nfor folder in folders:\n    folder_path = os.path.join(base_path, folder)\n    os.makedirs(folder_path, exist_ok=True)\n    print(f\"'{folder}' 폴더가 '{folder_path}'에 생성되었습니다.\")\n```\n\n단계 2: 토큰 설정\n\n```python\nimport os\nimport requests\n\nresponse = requests.post('https://appeears.earthdatacloud.nasa.gov/api/login', auth=('여러분의 사용자명', '여러분의 비밀번호'))\ntoken_response = response.json()\ntoken = token_response['token']\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3단계: 텍스트 파일에 저장된 URL을 읽고 파일을 다운로드합니다:\n\n```js\ntxtfile = 'Great-Salt-Lake-download-list.txt' \nfilenames = [] # 파일 이름을 저장하는 리스트\nurls = [] # 파일 이름이 없는 URL을 저장하는 리스트\n\nwith open(txtfile) as file:\n    for line in file:\n        url = line.strip() # 줄 바꿈 문자 제거\n        filename_index = url.find('L08.002') # 파일 이름의 인덱스 가져오기\n        url_without_filename = url[:filename_index-1] # 파일 이름이 없는 URL 가져오기\n        print(url_without_filename)\n        filename = url[filename_index:] # 파일 이름 가져오기\n        print(filename)\n\n        response = requests.get(url_without_filename,\n                                headers={'Authorization': 'Bearer {0}'.format(token)},\n                                allow_redirects=True,\n                                stream=True)\n        dest_dir = \"/content/QA\"\n        filepath = os.path.join(dest_dir, filename)\n        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\n        # 파일을 목적지 디렉토리에 쓰기\n        with open(filepath, 'wb') as f:\n          for data in response.iter_content(chunk_size=8192):\n            f.write(data)\n```\n\n이 코드는 폴더를 생성하고 토큰을 설정하며 QA 폴더에 AppEARS 서버에서 분류된 Landsat-8 이미지를 다운로드합니다.\n\n첫 번째 단계에서는 QA, Clear_Image, Map 및 Animation 네 가지 폴더를 만들었습니다. \"QA\" 폴더는 원본 분류된 이미지를 저장하기 위한 것이고, \"Clear_Image\" 폴더는 각 연도별로 물 위 구름이 없는 Great Salt Lake 이미지를 저장하는 용도이고, \"Map\"은 물 위 구름이 없는 이미지를 바탕으로한 Great Salt Lake 맵을 저장하는 곳이며, \"Animation\" 폴더는 타임랩스 파일을 저장하는 용도입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 단계를 올바르게 따르면 디렉토리에 \"QA_Pixel\" 파일이 있어야 합니다:\n\n![이미지](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_6.png)\n\n이제 디렉토리에 이미지가 분류되었으므로 이러한 이미지를 오버레이하고 각 연도별로 물로 분류된 픽셀을 유지하고 이러한 래스터 레이어를 Geotiff 파일에 저장해야 합니다. 이 작업은 다음과 같이 수행할 수 있습니다:\n\n```js\nimport os\nimport numpy as np\nimport rasterio\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport pandas as pd\n\n# 유지할 QA 값 정의\nqa_values = [21890, 21952, 22146, 22208, 54662, 54724, 54918, 54980]\n\n# 폴더 내의 모든 파일 나열\nqa_folder = '/content/QA'\nall_files = os.listdir('/content/QA')\n\n# 연도별로 루프 실행\nfor year in range(2014, 2024):\n    print(year)\n\n    # 물 픽셀을 저장할 빈 배열 초기화\n    water_pixels = None\n\n    # 조건에 따라 파일 필터링\n    QA_list = [file for file in all_files if str(year) in file]\n    for QA_filename in QA_list:\n        if QA_filename.endswith('.tif'):\n            date_str = QA_filename.split('_')[4][3:]\n            date_obj = datetime.strptime(date_str, '%Y%j')\n            date = date_obj.date()\n            print('Processing date:', date)\n\n            # QA tif 파일 읽기\n            qa_file = os.path.join(qa_folder, QA_filename)\n\n            with rasterio.open(qa_file) as qa_src:\n                # tif 파일에서 데이터 및 메타데이터 읽기\n                qa_data = qa_src.read(1)\n\n                # 모든 값이 nan인 경우 건너뛰기\n                qa_data_zeros = np.all(qa_data == 1)\n\n                if qa_data_zeros:\n                    continue\n\n                profile = qa_src.profile\n\n                # 물값을 2로 대체하고 나머지는 NaN으로 변경\n                qa_data = qa_data.astype(float)\n                qa_data[np.isin(qa_data, qa_values)] = 2\n                qa_data[~np.isin(qa_data, [2])] = np.nan\n\n                # 물 픽셀을 배열에 추가\n                if water_pixels is None:\n                    water_pixels = qa_data.copy()\n                else:\n                    water_pixels = np.nanmax(np.stack([water_pixels, qa_data]), axis=0)\n\n                # 물 픽셀을 갖는 새 래스터 생성\n                profile.update(count=1)\n                with rasterio.open(f'/content/Clear_Image/Great_Salt_Lake_{year}.tif', 'w', **profile) as dst:\n                    dst.write(water_pixels.reshape(profile['height'], profile['width']), 1)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스크립트는 물 픽셀을 나타내는 QA 값을 정의한 후 지정된 QA 폴더의 파일을 나열합니다. 다음으로, 스크립트는 각 연도별로 물 픽셀을 저장할 배열을 초기화하고 2014년부터 2023년까지의 연도를 반복합니다. 각 연도의 QA 파일에 대해, 파일을 읽고 물 값을 임의의 숫자로 바꾼 다음 나머지 값을 NaN으로 설정합니다. 그런 다음 물 픽셀의 배열을 업데이트하고 물 픽셀이 있는 새 래스터를 생성하여 'Clear_Image' 폴더에 Geotiff 파일로 씁니다.\n\n다음 애니메이션은 스크립트가 Great Salt Lake에서 촬영한 Landsat-8 이미지의 물 픽셀을 유지하여 2021년의 구름 없는 이미지를 생성하는 방법을 보여줍니다.\n\n![애니메이션](https://miro.medium.com/v2/resize:fit:1400/1*DM_1YNYFpMGwXhNRYHC5yA.gif)\n\n코드를 실행한 후에는 각 연도에 대해 Great Salt Lake의 완전하고 구름 없는 이미지가 디렉토리에 저장됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Great Salt Lake Images](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_7.png)\n\n## 🗺️ Visualization of The Great Salt Lake Images\n\nWe will first visualize the raster file to examine the cloud-free images and see how the lake's surface area changes from 2014 to 2023 before calculating its total area.\n\n```python\nimport os\nimport matplotlib.pyplot as plt\nimport rasterio\n\ndef plot_raster_files(folder_path):\n    # Get a list of all raster files in the folder\n    raster_files = [f for f in os.listdir(folder_path) if f.endswith('.tif')]\n    raster_files.sort()\n    \n    # Set up subplots\n    rows, cols = 2, 5  \n    fig, axes = plt.subplots(rows, cols, figsize=(15, 8))\n\n    # Loop through raster files and plot them in subplots\n    for i, file_name in enumerate(raster_files):\n        date_str = file_name.split('_')[3][0:5]\n        file_path = os.path.join(folder_path, file_name)\n        print(file_path)\n\n        # Open the raster file\n        with rasterio.open(file_path) as src:\n            # Read raster data\n            raster_data = src.read(1)\n\n            # Plotting\n            ax = axes[i // cols, i % cols]\n            cmap = ListedColormap(['white', 'blue'])\n            ax.imshow(raster_data, cmap=cmap, vmin=0, vmax=2)\n            ax.set_title(date_str)\n            ax.set_xticks([])\n            ax.set_yticks([])\n\n    # Adjust layout for better visualization\n    plt.tight_layout()\n    plt.show()\n\n# Specify the folder path containing GeoTIFF images \nGeotiff_folder = '/content/Clear_Image'\nplot_raster_files(Geotiff_folder)\n```\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과는 다음과 같습니다:\n\n\u003cimg src=\"/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_8.png\" /\u003e\n\n먼저, 이미지가 전체 호수를 포함하고 구름이 없는 것을 볼 수 있습니다. 이는 우리가 통계 파일을 기반으로 호수 영역의 시계열을 그리는 데 논의한 문제를 해결했습니다. 두 번째로, 최대 면적이 2016 또는 2017에 발생했으며, 최소 면적은 2022에 발생했음을 알 수 있습니다.\n\n## 🎥 Great Salt Lake 수축 타임랩스\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGreat Salt Lake의 표면적 변화를 동적으로 시각화하기 위해, 우리는 호수의 개별 지도를 내보내고 이를 연속으로 배치하여 2014년부터 2023년까지의 타임랩스를 만들 수 있습니다. 이를 수행하는 방법은 다음과 같습니다:\n\n단계 1: 구름이 없는 GeoTIFF 파일에서 개별 지도를 내보냅니다.\n\n```python\n# 폴더 내의 모든 파일 나열\nGeotiff_folder = '/content/Clear_Image'\nall_files = os.listdir(Geotiff_folder)\n\n# 파일 순회\nfor Lake_filename in all_files:\n    if Lake_filename.endswith('.tif'):\n      date_str = Lake_filename.split('_')[3][0:4]\n\n      # 래스터 파일 열기\n      with rasterio.open(os.path.join(Geotiff_folder,Lake_filename)) as src:\n        # 래스터 데이터 읽기\n        raster_data = src.read(1)\n        cmap = ListedColormap(['white','blue'])\n        plt.imshow(raster_data,cmap=cmap, interpolation='nearest', vmin=0, vmax=2)\n        plt.title(f'Great Salt Lake {date_str}')\n        # 축 제거\n        plt.axis('off')\n        # 그림을 파일로 저장\n        plt.savefig(f'/content/Map/{date_str}.png')\n```\n\n단계 2: 이러한 지도를 병합하여 타임랩스를 작성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom PIL import Image\nimport os\nimport glob\n\nfolder_path = '/content/Map'\noutput_gif = os.path.join('/content/Animation', 'Lake_animation.gif')\n\n# 폴더 내 모든 jpg 파일의 정렬된 목록을 가져옵니다\nfile_list = sorted(glob.glob(os.path.join(folder_path, '*.png'))\n\n# 이미지의 원하는 차원을 설정합니다\nwidth, height = 700, 500\n\n# 이미지를 읽고 리스트에 저장합니다\nframes = []\nfor file in file_list:\n    frame = Image.open(file)\n\n    # 이미지 크기 조정\n    frame = frame.resize((width, height), Image.ANTIALIAS)\n\n    # 이미지에 흰색 배경 추가\n    background = Image.new('RGB', frame.size, (255, 255, 255))\n    background.paste(frame)\n    frame = background.convert('RGB')\n\n    # 전역 색상 표를 사용하여 이미지를 P 모드로 변환\n    frame = frame.convert('P', palette=Image.ADAPTIVE, colors=256)\n    frames.append(frame)\n\n\nframes[0].save(output_gif, save_all=True, append_images=frames[1:],\n        duration=500,  # 밀리초로 지속 시간 설정\n        loop=0,        # 루프 수 설정 (0은 무한을 의미)\n        optimize=True)\n```\n\n이외에도 다양한 기능을 사용하여 이미지를 수정 및 GIF 애니메이션을 만들 수 있습니다. 위 코드에서 설정한 이미지 처리를 참고해 보세요.\n\n시간 흐름은:\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*HOgBDjBrNhopFc2jBrRXVw.gif\" /\u003e\n\n## 📉 분류된 이미지로부터 그레이트 솔트 레이크 지역의 시계열 데이터입니다\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n선택지 1인 통계 파일을 기반으로 호수 표면적의 시계열 데이터를 추출한 섹션과 유사하게, 이제 흐릿하지 않은 Landsat-8 이미지에서 표면적을 추정할 수 있습니다. 다음 스크립트에서는 흐릿하지 않은 GeoTIFF 파일을 읽어서 물 픽셀을 세고, 제곱 킬로미터로 표면적을 변환하고, 이를 시계열로 나타내어 추세를 분석합니다.\n\n```js\nimport os\nimport re\nimport rasterio\nimport matplotlib.pyplot as plt\nfrom scipy.stats import linregress\n\nfolder_path = \"/content/Clear_Image\"\nvalue_to_count = 2\n\nfiles = [f for f in os.listdir(folder_path) if f.endswith('.tif')]\nfiles.sort()\n\ndata = {'dates': [], 'pixel_counts': []}\n\nfor file in files:\n    with rasterio.open(os.path.join(folder_path, file)) as src:\n        array = src.read(1) \n        pixel_count = (array == value_to_count).sum()\n        date = file.split('_')[3][0:4]\n        data['dates'].append(date)\n        data['pixel_counts'].append(pixel_count)\n\n# 데이터프레임 생성\ndf = pd.DataFrame(data)\n\n# 면적으로 변환 (제곱 킬로미터)\ndf['area'] = df['pixel_counts'] * 30 * 30 / 1000000\n\n# 그래픽 표현\nplt.figure(figsize=(12, 6))\nplt.bar(df['dates'], df['area'], color='lightblue', label='Max Area Per Year')\nplt.plot(df['dates'], df['area'], marker='o', linestyle='-', color='b', label='Trend Line')\n\n# 추세선을 위한 선형 회귀\nslope, intercept, _, _, _ = linregress(df.index, df['area'])\ntrend_line = slope * df.index + intercept\n\n# 추세 방정식 표시\nplt.plot(df.index, trend_line, color='red', linestyle='--', label=f'Trend Line: y = {slope:.1f}x + {intercept:.1f}')\n\nplt.xticks(fontweight='bold')\nplt.yticks(fontweight='bold')\n\nplt.title('Maximum Area of The Great Salt Lake Per Year')\nplt.ylabel('Max Area (SqKm)', fontsize=12, fontweight='bold')\nplt.grid(True)\nplt.legend(loc='center')\nplt.show()\n```\n\n위 코드를 실행하면 아래와 같은 플롯이 생성됩니다:\n\n\u003cimg src=\"/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_9.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 도표와 통계 파일을 기반으로 한 플롯 간의 주요 차이점은 이 플롯이 우리가 원본 Landsat-8 이미지에서 추정할 수 있는 호수의 가장 정확한 표면적을 나타낸다는 것입니다. 이 플롯은 2014년부터 2016년까지는 상당한 변화가 없고, 2017년과 2019년에는 작은 확장(약 4800 제곱킬로미터에서 약 5300 제곱킬로미터)을 보여주지만, 2019년부터 2022년에는 상당한 감소(약 5300 제곱킬로미터에서 약 4100 제곱킬로미터)가 있었습니다. 2022년부터 2023년에는 다시 그래프 상에서 증가함(약 4100 제곱킬로미터에서 약 4700 제곱킬로미터)이 관찰되었는데, 이는 작년에 받은 상당한 우천량으로 인해 젖은 해가 된 것으로 보입니다.\n\n## ⚖️ 통계 파일 및 이미지로 추출된 시계열 비교\n\n이전에 언급했듯이 QA 통계 파일을 기반으로 표면적을 분석하면 호수 위의 구름이 있는 픽셀에 대한 필터를 보상하지 않기 때문에 해당 지역을 과소평가하는 경향이 있습니다. 통계 파일을 통해 계산된 표면적과 분류된 Landsat-8 이미지에서 추정한 표면적을 비교해 보기 위해 두 가지를 동일한 레이아웃에 플롯하여 통계 파일만을 의존했을 때 얼마나 과소평가가 발생하는지 살펴봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 스크립트는 각 접근 방법에서 보고된 물 픽셀의 수를 제곱 킬로미터 단위의 표면적으로 변환하고 이를 시각화합니다. 결과 그래프는 다음과 같습니다:\n\n![그래프](/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_10.png)\n\n해당 그래프는 각 접근 방법에서 계산된 표면적을 보여줍니다: 주황색 선과 빨간 막대는 통계 파일을 나타내며(옵션 1), 파란색 선과 막대는 구름이 없는 이미지를 나타냅니다(옵션 2). 그림에서 볼 수 있듯이, 통계 파일을 사용하면 호수 표면적의 증가 또는 감소의 일반적인 패턴을 추적할 수 있습니다. 그러나 앞서 언급한 대로, 통계 파일은 2015년에는 1000 제곱 킬로미터에서 2020년에는 150 제곱 킬로미터로 이르는 과소평가로 이어집니다. 2020년에 통계와 구름이 없는 이미지에서 계산된 표면적 간의 미세한 차이는 2020년에 캡처된 랜드섯 이미지 중 하나가 거의 구름이 없고 호수 대부분을 포함했기 때문에, 통계 파일과 구름이 없는 추정치 사이의 픽셀 수가 비슷하다는 점을 설명합니다. 반면, 2015년의 큰 차이는 2015년 랜드섯-8이 캡처한 최상의 맑은 이미지가 여전히 다양한 구름이 있는 픽셀을 가지고 있거나 전체 호수를 커버하지 않았기 때문입니다.\n\n## 📄 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위성 이미지의 응용 중 하나는 물체나 풍경의 변화를 모니터링하는 것입니다. 이 연구에서는 Landsat-8에 의해 캡처된 이미지를 사용하여 2014년부터 2023년까지 대연몰의 표면 면적 변동을 분석했습니다. 이 지역은 지난 수십 년 동안 상당한 수축이 있어 지켜지고 있습니다. 두 가지 다른 방법으로 측정된 호수의 면적을 비교하여 우리의 분석에서 작은 오류가 어떻게 오해와 부정확한 정보로 이어질 수 있는지 강조해 보았습니다. 이 분석 결과, 2019년부터 2022년까지 대연몰의 상당한 수축, 2014년부터 2022년까지 10% 수축이 있었습니다. 흥미로운 점은 2023년의 다습한 해가 대연몰의 회복에 중요한 역할을 한 것으로 나타났습니다. 그 해에 10% 증가했습니다. 그러나 2023년을 넘어서의 회복은 불확실하며, 2024년에 추가 개선이 보장되지는 않습니다.\n\n## 📚 참고 자료\n\n2014년부터 2023년까지의 Landsat-8 이미지는 2024/02/20에 https://lpdaac.usgs.gov에서 AppEEARS를 사용하여 검색되었습니다. 이는 미국 지질조사원(USGS) 지구 자원 관측 및 과학(EROS) 센터가 운영하는 NASA EOSDIS Land Processes Distributed Active Archive Center (LP DAAC)에서 유지되었습니다.\n\n📱 더 많은 유익한 콘텐츠를 위해 다른 플랫폼에서 저와 연결하세요! LinkedIn, ResearchGate, Github, Twitter.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 링크를 통해 해당 게시물을 확인할 수 있습니다:","ogImage":{"url":"/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_0.png"},"coverImage":"/assets/img/2024-06-23-TrackingTheGreatSaltLakesShrinkageUsingSatelliteImagesPython_0.png","tag":["Tech"],"readingTime":21},{"title":"마케팅 전략을 지원하는 데이터 시각화 사용법","description":"","date":"2024-06-23 16:09","slug":"2024-06-23-UsingDataVisualizationstoSupportMarketingStrategies","content":"\n\n## R을 사용하여 호텔 예약 회사를 위한 데이터 시각화 작업을 합니다.\n\n![image](/assets/img/2024-06-23-UsingDataVisualizationstoSupportMarketingStrategies_0.png)\n\n# 시나리오\n\n저는 호텔 예약 회사의 주니어 데이터 분석가로서 호텔 예약 데이터를 정리하고 `ggplot2`로 시각화를 생성하여 데이터를 분석하고 시각화를 통해 데이터의 다양한 측면을 제시하는 작업을 맡게 되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로, 다음 단계를 살펴봅시다 👇🏽\n\n## 단계 1: 데이터 가져오기\n\n저는 RStudio에 데이터 세트를 가져와서 비즈니스 업무를 시작했어요.\n\n코드:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```R\nhotel_bookings \u003c- read.csv(\"hotel_bookings.csv\")\n```\n\n## 단계 2: ggplot2 패키지 설치 및 불러오기\n\nggplot2를 사용하려면 tidyverse 패키지를 설치하고 로드해야 했어요.\n\n코드:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n``` r\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n```\n\n### Step 3: Making different Charts\n\n#### Chart 1:\n\nA stakeholder was interested in developing promotions based on different market segments, but first he needed to know how many of the transactions were occurring for each market segment and if it was dependent on the type of hotel.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해당 코드는 Markdown 형식으로 변경해야 합니다. \n\n코드:\n\n```js\nggplot (data = hotel_bookings) + geom_bar (mapping = aes (x=market_segment, fill = market_segment)) + facet_wrap(~hotel)\n```\n\n시각화:\n\n\u003cimg src=\"/assets/img/2024-06-23-UsingDataVisualizationstoSupportMarketingStrategies_1.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 시각화를 한 후, 차트가 깔끔하지 않고 내 발견물을 명확하게 전달하지 못한다는 것을 깨달았어요. 그래서 다음 차트를 제작하게 되었어요.\n\n## 차트 2:\n\n이해를 돕기 위해 이 시각화를 정리하고 주석을 추가해야 했어요. 제목, 부제목, 그리고 데이터가 cover하는 기간을 보여주는 캡션을 추가했어요. 더 나아가 x-축 포인트를 정리하고, y-축 레이블을 \"예약 횟수\"로 변경하여 정말 명확하게 만들었어요.\n\n코드:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nggplot (data = hotel_bookings) + geom_bar (mapping = aes (x=market_segment, fill = market_segment)) + facet_wrap(~hotel) +\n\ntheme(axis.text.x = element_text (angle = 90)) +\nlabs(title = \"호텔 예약: 호텔별 시장 세그먼트 비교\", subtitle = \"두 종류의 호텔 샘플\", caption = \"데이터 출처: 2015-2017\", y=\"예약 건수\")\n```\n\n시각화:\n\n\u003cimg src=\"/assets/img/2024-06-23-UsingDataVisualizationstoSupportMarketingStrategies_2.png\" /\u003e\n\n## 차트 3:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또 다른 이해관계자가 말했습니다. \"일찍 예약하는 사람들을 대상으로 하고 싶고, 아이를 둔 사람들은 미리 예약해야한다는 가설이 있습니다.\" 데이터를 탐색한 후, 제가 기대한 것과는 다르게 나타나서 더 나아가 산포도를 만들었습니다. 산포도를 통해 가설이 부정확했음을 확인하고 결과를 보고했습니다.\n\n코드:\n\n```js\nggplot (data = hotel_bookings) + geom_point (mapping = aes (x = lead_time, y = children))\n```\n\n시각화:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-UsingDataVisualizationstoSupportMarketingStrategies_3.png\" /\u003e\n\n## Chart 4:\n\n이 차트를 위해 먼저 데이터를 필터링하여 \"City Hotel\" 호텔 유형의 \"Online TA\" 시장 세그먼트만 포함하도록 했습니다. 이 새로운 데이터 프레임의 이름을 \"onlineta city hotels\"로 지었습니다.\n\n코드:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```r\nonlineta_city_hotels \u003c- hotel_bookings %\u003e%\nfilter(hotel == \"City Hotel\" \u0026 market_segment == \"Online TA\")\n```\n\n새 데이터 프레임에서 내 시각화를 만들었고, 차트는 아이들과 함께하는 예약의 리드 타임도 보여줍니다.\n\n코드:\n\n```r\nggplot(data = onlineta_city_hotels) + geom_point(mapping = aes(x = lead_time, y = children))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시각화:\n\n![시각화](/assets/img/2024-06-23-UsingDataVisualizationstoSupportMarketingStrategies_4.png)\n\n## 차트 5:\n\n다음 차트는 어떤 게스트 그룹이 가장 많은 주말 밤을 예약하는지 알아내어 새 마케팅 캠페인에서 해당 그룹을 대상으로 할 수 있도록 하는 것이 목적이었습니다. 내 이해관계자의 가설은; 아이가 없는 게스트가 가장 많은 주말 밤을 예약한다는 것입니다. 데이터를 시각화한 결과, 그녀의 가설이 실제로 정확하다는 것을 보여 주었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 바뀐 마크다운 형식입니다:\n\n\nThe Code:\n\n```js\nggplot (data = hotel_bookings) + geom_point (mapping = aes (x = stays_in_weekend_nights, y = children))\n```\n\nThe Visual:\n\n![Visualization](/assets/img/2024-06-23-UsingDataVisualizationstoSupportMarketingStrategies_5.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터 개요\n\n저는 사용한 호텔 예약 데이터는 Nuno Antonio, Ana Almeida 및 Luis Nunes가 Data in Brief, Volume 22, February 2019에 게재한 논문인 'Hotel Booking Demand Datasets' (https://www.sciencedirect.com/science/article/pii/S2352340918315191)에서 원래 비롯되었습니다.\n\n이 데이터는 Thomas Mock과 Antoine Bichat에 의해 2020년 2월 11일 주간 #TidyTuesday를 위해 다운로드 및 정리되었습니다 (https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md).","ogImage":{"url":"/assets/img/2024-06-23-UsingDataVisualizationstoSupportMarketingStrategies_0.png"},"coverImage":"/assets/img/2024-06-23-UsingDataVisualizationstoSupportMarketingStrategies_0.png","tag":["Tech"],"readingTime":4},{"title":"2024년 데이터 사이언스를 효과적으로 배우는 방법","description":"","date":"2024-06-23 16:07","slug":"2024-06-23-HowtoEffectivelyLearnDataSciencein2024","content":"\n\n## 데이터 과학 직업 준비를 위한 자기 학습 안내서\n\n![이미지](/assets/img/2024-06-23-HowtoEffectivelyLearnDataSciencein2024_0.png)\n\n항상 스스로 배우는 사람이었어요. 프로그래밍과 비즈니스에 대한 모든 지식은 가르쳐주는 사람이나 멘토 없이 얻었어요. 어렵기는 했지만, 스스로 배우는 여정의 모든 단계를 즐겨왔어요.\n\n당신은 어떠신가요? 당신도 스스로 학습하는 길을 걷고 계신가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n친구야, 너에게 흥미로운 이야기 하나를 전하고 싶어. 오늘날의 셀프 러닝은 내 초창기때와 비교하면 정말 산만해졌어. 핵심은? - AI 도구, 특히 ChatGPT야.\n\n그것은 도우미, 멘토, 그리고 선생님까지 모두 한 곳에 모였다고 생각하면 돼. 그러니, 2024년에 우리의 학습 여정을 살릴 수 있는 ChatGPT를 활용해보는 건 어때?\n\n이 기사에서는 \"시작해야 할 주제\"와 \"해결 방법\"에 대해 이야기하고, 추가로 몇 가지 정말 유용한 팁도 덤으로 제공할 거야.\n\n이 여정에 도전할 준비가 됐니? 함께 뛰어드는 거야!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 기초 이해하기\n\n여기서는 모든 주제를 설명하지 않을 것이에요. 대신 데이터 과학 학습을 시작하는 데 필요한 사항을 지적할 거에요.\n\n## 1. 통계:\n\n이제, 자기 학습자들이 데이터 과학에 뛰어들 때 자주 하는 큰 실수에 대해 이야기해봐요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자주 그들은 \"통계\"를 건너뛰고 Python, SQL 또는 다른 기술적인 것으로 바로 가곤 합니다.\n\n하지만 여기 내 조언이 있어요: \"통계\"로 시작하세요.\n\n정말 시간을 들여 철저히 익히고 다른 영역으로 뛰어들기 전에 미리 갖추세요. 멋진 데이터 마법사가 되는 관건적인 첫걸음이거든요.\n\n이게 왜 중요한지 궁금하시다구요? 제 글을 확인해 보세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n🎯 통계를 위한 할 일 체크리스트:\na. 확률 이론\nb. 기술통계\nc. 추론 통계\nd. 통계 기계 학습\n\n📚 자료: 데이터 과학자를 위한 실용 통계: 면접 대비 노트\n\n## 2. 프로그래밍 기술\n\n통계에 감을 잡았다면요? 좋아요! 다음 단계는 데이터 과학 스킬을 향상시키기 위한 프로그래밍 언어를 선택하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 선택할 수 있는 좋은 두 가지 옵션이 있어요: Python과 R.\n\n그럼, 어떤 것을 선택해야 할까요? 편안한 느낌이 드는 것을 선택하시면 돼요.\n\n하지만 제 의견을 물어본다면, Python을 선택하는 게 정말 좋은 선택이라고 말할 거예요.\n\n왜냐면? — Python은 너무 다재다능하고 익히기 쉬운데요. 게다가 Python은 다양한 라이브러리가 채워져 있어요. 이들은 데이터 처리, 복잡한 계산을 하거나 다른 작업을 수행할 때 일을 훨씬 더 쉽게 만들어주는 도구상자와 같아요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한편, R은 정말 대단해요. 특히 통계에 관심이 많다면 더욱 좋아할 거예요. 통계 분석을 위한 언어로 R을 선택하는 것이 좋죠. 하지만 기억하세요, R은 주로 통계에 관한 것에 집중되어 있어요.\n\n그래서, 필요한 것을 고려하고 당신에게 맞는 선택을 해보세요.\n\n만약 데이터 과학 분야에서 만능인이 되고 싶다면, 파이썬이 최선의 선택일지도 몰라요!\n\n🎯 파이썬을 위한 To-do 체크리스트:\na. 파이썬 기초\nb. Pandas와 NumPy 라이브러리 (데이터프레임 기초 및 연산)\nc. 시각화 (Matplotlib 및 Seaborn 라이브러리)\nd. 데이터 수집 (BeautifulSoup, Scrapy, Selenium 또는 Requests 라이브러리)\ne. 에러 처리와 디버깅\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n📚 자료: 초보자를 위한 파이썬 데이터 분석 50일: 궁극의 도전서\n\n## 3. EDA — 데이터 가공 및 시각화\n\n파이썬 세상에 첫 발을 내딛었군요 — 축하합니다! 그렇다면, 다음 단계를 이야기해보겠습니다.\n\n데이터 과학에 입문한 사람으로써, 데이터에 뛰어들어 눈을 뜨게 하는 통찰을 찾는 것만이 전부인 줄 착각하기 쉽습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대부분의 시간을 데이터 분석이나 복잡한 모델 작성에 소비하는 것이 유혹적일 수 있습니다.\n\n하지만 가장 기본적으로 먼저 배울 것이 있습니다: EDA 또는 탐색적 데이터 분석입니다.\n\nEDA는 모든 회사의 데이터 과학 작업의 기초입니다.\n\n데이터를 정리, 요약, 변환 및 시각화하는 것이 포함됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 작업들은 모델을 구축하는 것만큼 화려하지 않을지도 모릅니다만, 그들은 중요합니다.\n\n사실, 당신과 같은 초보자에게 있어서 EDA를 숙달하는 것은 이 분야에서 첫 직장을 얻는 데 필수적인 단계입니다.\n\n트렌드 분석과 모델링은 데이터 과학의 부분이지만 이러한 작업은 주로 경험이 풍부한 전문가들에게 맡겨집니다. 그러므로 EDA에 대해 훌륭해지는 데 집중하세요 - 이것이 데이터 과학에서 성공적인 출발을 하는 데 필요한 열쇠입니다!\n\n효과적인 EDA를 수행하는 방법을 배우고 싶다면, 이 문서를 읽어보세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n📝 제 블로그 글:\na. 효과적인 탐색적 데이터 분석 (EDA) 진행 방법\nb. 데이터 시각화의 가장 효과적인 방법\n\n🎯 EDA를 위한 할 일 체크리스트:\na. 데이터 요약\nb. 데이터 정제\nc. 데이터 변형\nd. 데이터 시각화\n\n📚 자료: 파이썬으로 하는 데이터 분석 50일: 초보자를 위한 궁극의 도전서\n\n## 4. SQL (데이터 조작 및 추출)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파이썬에 이어 프로그래밍 세계에서 또 다른 중요한 요소인 SQL(구조화된 질의 언어)이 있습니다.\n\nSQL을 마스터했다면, 놀랍게도 직업 기회의 보물 창고를 열었습니다!\n\nSQL은 모든 산업에서 열광적으로 수요되는 기술입니다. 데이터베이스를 질의하고 조작하는 데 가장 많이 사용되는 도구입니다.\n\nSQL 쿼리를 읽고 쓰며 최적화하는 능력은 데이터를 추출하고 조정하는 데 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정말 데이터 게임을 끌어올리는 기술이에요!\n\n🎯 SQL을 위한 할 일 체크리스트:\na. 주요 6가지 구문: (SELECT, FROM, WHERE, GROUP BY, HAVING, 그리고 ORDER BY)\nb. 조인 및 CTE(Common Table Expressions)\nc. 윈도우 함수(Window Functions)\nd. 저장된 프로시저(Stored Procedures)\n\n📚 자료: 최소 기능 SQL 패턴\n\n📝 내 글:\na. 데이터 위저드를 위한 SQL \"CASE WHEN\" 마스터하는 궁극의 안내서\nb. SQL \"윈도우 함수\" 마스터하기: 궁극의 안내서\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n좋아요! 우리가 이야기한 모든 스킬을 습득했다면, \"데이터 분석가\"가 될 준비가 된 것이야.\n\n하지만 한 가지 주의할 점 — PowerBI나 Tableau와 같은 시각화나 보고서 생성 도구를 배우는 걸 잊지 마세요. 그것들이 중요하거든!\n\n하지만, \"데이터 과학자\"나 \"데이터 엔지니어\"가 되고 싶다구요? 그건 좀 다른 이야기에요. 추가적으로 더 고급스러운 스킬이 필요할 거에요. 이제 그것에 대해 자세히 알아볼까요?\n\n# 고급 스킬\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서부터는 학습 경로가 조금 더 어렵고 복잡해집니다. 왜냐하면 이러한 주제들은 초보자에게 적합하지 않기 때문이죠.\n\n이러한 기술들을 습득하려면 선형 대수, 미적분 및 이전의 계산 이론 지식과 같은 수학적 주제에 대한 보다 나은 이해가 필요합니다. 함께 공부해봅시다.\n\n## 1. 머신러닝\n\n기초를 마스터했으니 이제 데이터 스크래핑, 정제 및 통계 분석과 같은 기술을 갖추었습니다. 함께 하면 됩니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알려줬던 대로 진행하면 돼요: 원시 데이터를 유용한 정보로 변환하는 일.\n\n다음 큰 단계는?\n\n이 데이터를 활용해 더 깊은 통찰을 제공하고 스마트한 비즈니스 선택을 이끌어내는 모델을 만드는 것이에요.\n\n여기서 \"머신 러닝\"이 지향하는 바가 나옵니다. 이는 컴퓨터에게 사고하고 데이터로부터 학습하는 법을 가르치는 것이 중요한데, 마치 인간과 같죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여정은 단순한 선형 회귀부터 복잡한 신경망(즉, 딥러닝)과 같은 다양한 알고리즘을 이해하는 것을 포함합니다.\n\n이 개념들은 어렵게 느껴질 수 있지만, 그것들은 최첨단 기술입니다.\n\n도전을 받아들이고 배우세요, 그렇지 않으면 뒤처지기 쉬울 수 있습니다. 선택은 당신의 것입니다!\n\n🎯 기계 학습을 위한 투두 체크리스트:\na. 피처 엔지니어링\nb. 지도 및 비지도학습\nc. 회귀 알고리즘 (선형 회귀, 로지스틱 회귀 등)\nd. 분류 알고리즘 (로지스틱 분류, SVM, 나이브 베이즈 등)\ne. 클러스터링 알고리즘 (주로 K-means)\nf. 딥러닝 개념 (인공 신경망, 합성곱 신경망, 순환 신경망, 트랜스포머, 파이토치/텐서플로 기초)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n📚 자료: 작업 중입니다. 2개월 후(2024년 시작) 여기에서 사용할 수 있게 될 것입니다 — 알림을 받기 위해 구독 유지해주세요.\n\n## 2. 모델 평가\n\n머신 러닝 모델을 만들었을 때 얼마나 잘 수행되고 있는지 궁금할 것입니다.\n\n내부에서 이 모델이 어떻게 작동하는지는 상당히 복잡할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그것이 모델을 평가하는 중요성이 들어갑니다.\n\n데이터 과학에서 모델이 제대로 작동하는지 확인하는 것이 중요합니다. 이는 '모델 평가 방법'을 반드시 알아야 한다는 것을 의미합니다.\n\n이것은 당신의 모델을 이해하고 향상시키는 데 필수적입니다!\n\n이것에 대해 이해해야 하는 정보는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- \"다양한 상황에 가장 적합한 평가 방법은 무엇인가,\"\n- \"모델을 평가하는 방법은 무엇인가,\" 그리고 마지막으로,\n- \"이러한 평가를 어떻게 해석해야 하는가.\"\n\n이 정보는 당신이 원하는 목표를 달성하기 위해 모델을 개선하는 데 도움이 될 것입니다.\n\n제 논문에서 기초를 배우세요:\n최상의 모델 평가 방법 선택 및 사용 시기: 궁극의 안내서\n\n🎯 모델 평가를 위한 할 일 체크리스트:\na. 혼동 행렬\nb. 정밀도, 재현율 및 F1 점수\nc. 교차 검증\nd. 과적합, 과소적합\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n📚 리소스: 작업 중이에요. 2개월 후에 여기서 이용 가능할 거예요 (2024년부터 시작) — 알림을 받으려면 구독하세요.\n\n이제, 데이터 과학자가 되고 싶은 이들을 위한 두 가지 고급 주제에 대해 이야기해 볼게요.\n\n지금까지 오셨다면, 이제 거의 데이터 과학자로서의 여정을 시작할 준비가 거의 끝났어요.\n\n하지만 더 기다려 주세요. 기초를 넘어서, 데이터 엔지니어들에게 중요한 고급 주제가 하나 더 있어요. 그것은 바로:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 3. Big Data Technologies:\n\n데이터 엔지니어의 역할에 대해 알아봅시다.\n\n그들의 주요 업무는 무엇일까요?\n\n데이터의 '공학' 측면을 처리하는 것입니다. 이는 다양한 원본에서 데이터 수집 및 자동화된 프로세스 설정과 같은 작업을 포함합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기본적으로, 그들은 모든 이 데이터를 한 곳에 수집하기 위해 데이터 흐름이나 파이프라인을 구축합니다. 여기서 '빅 데이터 기술'에 대해 배우는 것이 중요해집니다.\n\n'빅 데이터'라고 하는 이유는 무엇일까요?\n\n오늘날의 세상은 데이터로 넘쳐나고 있으며, 그 양이 엄청나기 때문에 '빅 데이터'라고 불리는 것입니다.\n\n이를 관리하기 위해서는 여러 기술에 익숙해져야 합니다. 알아듣기에는 많은 것처럼 들릴 수도 있지만요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 여기 친절한 조언이 있어요: 이러한 기술을 배울 때, 기본적인 개념을 이해하는 데 집중해 보세요.\n\n기본 개념은 항상 변화하고 진화하지만 기술과는 달리 변하지 않습니다.\n\n이러한 방식으로 기술과 함께 성장하고 발전하기 위한 튼튼한 기반을 마련할 수 있을 거예요.\n\n제 논문에서 더 많은 정보를 찾아보세요:\n데이터 엔지니어링에 진입하는 방법: 2024\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n🎯 빅데이터 기술을 위한 할 일 체크리스트:\na. 빅데이터 소개\nb. 분산 시스템\nc. 하둡 (맵 리듀스)\nd. 스파크\ne. 클라우드 컴퓨팅\n\n📚 자료: 데이터 엔지니어링 분야로의 진입 방법: 2024\n\n## 👉 참고:\n\n![이미지](/assets/img/2024-06-23-HowtoEffectivelyLearnDataSciencein2024_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n📚 지금 eBook 다운로드하세요: 데이터 과학 학습을 위한 ChatGPT\n\n## 2024년 1월 15일까지 새해 특별 할인 이벤트:\n\n50% 할인 혜택\n할인 코드: \"NEWYEAR50\"\n\n# ChatGPT 프롬프트를 원하는 대로 사용할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 1. 개념 명확히하기\n\n\"[주제]에 대한 개념을 [기본/중급/고급] 지식을 가진 사람도 이해할 수 있는 방식으로 설명해주세요. 복잡한 측면을 단순화하고 더 관련성 있게 만들기 위해 [선택: 비유/예시/둘 다]를 제공해주세요.\n\n추가 지시사항: [선택 사항: 간단한 역사, 응용, 주제의 중요성을 명시하실 수 있음]\"\n\n## 2. 실습 문제\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n“[파이썬 코드 예시/통계 문제 해결책]을(를) 위한 예제를 제공할 수 있을까요? 이 작업은 [초급/중급/고급] 기술을 가진 사람에게 적합해야 합니다. 코드에 주석이나 단계별 설명을 포함하여 사고 과정을 명확히 해주세요.\n\n세부 내용: [데이터 세트, 알고리즘, 통계 방법 또는 사용할 라이브러리와 같은 구체적인 요구 사항을 포함하십시오]”\n\n## 3. 알고리즘 설명\n\n“[알고리즘 이름]에 대한 자세한 설명을 제공해 주실 수 있을까요? 이 설명에는 [작동 원리, 사용 사례, 장단점 및/또는 다른 유사한 알고리즘과의 비교]을 포함해야 합니다. 알고리즘에 대한 [기본적/중급적/고급적] 이해를 가진 사람을 대상으로 설명해 주세요.”\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4. 코드 디버깅\n\n언어/프레임워크: [예: 파이썬, 자바스크립트, 리액트]\n코드 설명: 코드가 의도한 작업을 간략히 설명해주세요.\n문제 설명: 마주치는 문제를 명확히 설명해주세요 (예: 오류 메시지, 예상치 못한 출력, 성능 문제).\n코드 스니펫: [여기에 코드 스니펫 삽입해주세요. 문제에 관련된 내용이 간결하고 명료하게 포함되어야 합니다.]\n\n이전 시도: [선택 사항: 이미 시도한 해결 방법을 언급해주세요.]\n구체적인 질문: [선택 사항: 디버깅 문제와 관련된 구체적인 질문을 하세요.]\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 여기까지!\n\n좋아요, 스스로 학습 로드맵을 모두 설정했네요. 하지만 기억해주세요, 이러한 기술들을 학습한다고 해서 즉시 취업할 수 있는 것은 아닙니다. 이외에도 해야 할 일이 있어요:\n\n- 먼저, 실제 프로젝트에 뛰어들어 보세요. 여러분의 모든 기술을 GitHub이나 여러분 자신의 블로그와 같은 한 곳에서 쇼케이스해 보세요. 여행 일지를 만드는 것처럼요.\n- 다음으로, 각각의 채용 공고에 맞는 이력서를 작성하세요. 회사들이 이력서를 걸러내는 데 사용하는 ATS(응시자 추적 시스템)에 맞게 작성하세요. 계속 지원하고 노력한 만큼 결과가 있을 거에요.\n- 이제 AI에 대해 이야기해 보겠습니다. 오늘날의 세상에서 AI 활용이 필수적이에요. 적어도 기본적인 \"프롬프트 엔지니어링\"에 대해 이해해 보세요. 이것은 여러분을 돋보이게 할 수 있는 기술이에요.\n  여기서 배우세요: ChatGPT - 프롬프트 엔지니어링 예술을 마스터하는 \"최종 가이드\"\n- 소통은 중요해요, 특히 데이터 과학에서요. 복잡한 데이터를 간단하게 설명해야 할 때가 많죠. 그래서 이야기 스킬을 향상시켜 보세요 - 이건 금이에요.\n- 그리고 LinkedIn을 잊지 마세요. 여러분의 작업을 활발하게 공유하세요. 이것은 채용 담당자들이 여러분을 알아채는 좋은 방법이며, 취업 활동을 더 원활하게 만들어 줄 거에요.\n\n그래서 여기서요. 계속 학습하고 호기심을 가져요, 분명히 성공할 거에요. 여러분의 여정에서 모두 최고의 행운을 빕니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 만약 제 글을 즐기신다면, 저를 지원해주세요:\n\n🛒 제 Gumroad 샵 방문하기: [https://codewarepam.gumroad.com/](https://codewarepam.gumroad.com/)\n\n제 베스트셀러 eBook: Top 50+ ChatGPT Personas for Custom Instructions","ogImage":{"url":"/assets/img/2024-06-23-HowtoEffectivelyLearnDataSciencein2024_0.png"},"coverImage":"/assets/img/2024-06-23-HowtoEffectivelyLearnDataSciencein2024_0.png","tag":["Tech"],"readingTime":9},{"title":"해적 위협 이해하기 데이터 기반 해적 분석 방법","description":"","date":"2024-06-23 16:05","slug":"2024-06-23-UnderstandingMaritimeThreatsAData-drivenApproachtoPiracyAnalysis","content":"\n\n![Understanding Maritime Threats](/assets/img/2024-06-23-UnderstandingMaritimeThreatsAData-drivenApproachtoPiracyAnalysis_0.png)\n\n해상 해적 활동은 글로벌 배송 및 해안 지역 커뮤니티에 대한 중요한 우려 사항이며, 해상에서의 공격은 인간의 생명 뿐만 아니라 경제 활동에도 위협을 가합니다. 공격은 해상에서 활동하는 범죄 그룹들에 의해 실시되는 납치, 무장 강도, 납치 등 다양한 불법 활동을 포함합니다. 이러한 공격은 선원의 생명을 위협하며, 세계 경제에 영향을 미치는 국제 무역 노선 및 해상 상업을 방해하여 전 세계 경제에 영향을 줍니다.\n\n해적 사건의 패턴 및 기본 요인을 이해하는 것은 효과적인 대책을 개발하고 해상 안보를 보장하는 데 중요합니다. 본문에서는 20년 이상의 기간에 걸친 포괄적 데이터세트를 활용하여 해상 해적 공격의 분석 및 시각화에 대해 탐구합니다. 지리적 분포, 시간적 추세, 사회 경제 지표와의 상관 관계를 탐색함으로써, 우리는 바다에서의 해적 공격의 역학 및 해양 안전과 안보에 대한 함의를 밝혀내고자 합니다.\n\n# 데이터세트 가져오기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요즘, CC BY 4.0 DEED로 라이선스 된 이 데이터셋을 발견했어요. 이 데이터셋은 해상 해적 공격에 대한 정보를 제공하며, 1993년부터 2020년까지 7,500건 이상의 해적 공격에 대한 정보와 국가 데이터(Wolrd Bank를 주원천으로 함) 및 지리적 데이터를 포함하고 있어요. 이 데이터는 2021년에 발표된 데이터 논문으로, 해적 공격을 이해하고 예방하기 위해 반해적 조직, 연구자 및 상업 기업에서 사용될 것을 목적으로 하고 있어요. 데이터셋의 일반적인 개념은 기록된 해적 공격을 관련 국가와 해당 사건 발생 시의 사회 경제적 상황과 연결하는 것이에요.\n\n보다 구체적으로, 데이터셋은 세 가지 테이블로 구성되어 있어요:\n\n- pirate_attacks, 각 기록된 공격에 대한 데이터를 제공\n- country_indicators, 관련 국가의 사회 및 경제 데이터(연도별)\n- country_codes, 사용된 국가 코드를 국가 이름과 지역과 연결하는 테이블\n\nMarkdown 포맷으로 표를 변경하면서 다음과 같이 표시할 수 있어요:\n\n![Understanding Maritime Threats: A Data-driven Approach to Piracy Analysis](/assets/img/2024-06-23-UnderstandingMaritimeThreatsAData-drivenApproachtoPiracyAnalysis_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 데이터 세트의 각 열에 대한 간단한 설명입니다:\n\n- 첫 번째 이미지:\n![](/assets/img/2024-06-23-UnderstandingMaritimeThreatsAData-drivenApproachtoPiracyAnalysis_2.png)\n\n- 두 번째 이미지:\n![](/assets/img/2024-06-23-UnderstandingMaritimeThreatsAData-drivenApproachtoPiracyAnalysis_3.png)\n\n- 세 번째 이미지:\n![](/assets/img/2024-06-23-UnderstandingMaritimeThreatsAData-drivenApproachtoPiracyAnalysis_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPlotly와 잘 어울리기 때문에 분석 작업에 Jupyter Lab을 사용했어요. Python에서 시각화를 만드는 데 좋아하는 Plotly와 함께하는 것이 좋았어요. 데이터셋을 주피터 노트북(Jupyter Notebook)으로 가져오기 위해서 먼저 데이터셋 파일(CSV 파일)이 프로젝트 디렉토리에서 접근 가능한지 확인해야 해요. 그런 다음 다음과 같이 할 수 있어요:\n\n```js\nimport pandas as pd\n\npirate_attacks = pd.read_csv('pirate_attacks.csv')\ncountry_indicators = pd.read_csv('country_indicators.csv')\ncountry_codes = pd.read_csv('country_codes.csv')\n```\n\n데이터셋을 가져온 후, 데이터프레임을 확인해볼 수 있어요:\n\n```js\npirate_attacks\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해적_공격 데이터 프레임의 각 특성에 대한 널 값의 백분율도 확인했습니다:\n\n```js\nnan_percentage = pirate_attacks.isna().mean() * 100\nprint(\"각 열의 NaN 백분율:\")\nprint(nan_percentage)\n```\n\n![이미지](/assets/img/2024-06-23-UnderstandingMaritimeThreatsAData-drivenApproachtoPiracyAnalysis_5.png)\n\n일부 열에는 상당한 비율의 널 값이 있습니다. 따라서 이러한 속성을 중심으로 분석하는 것은 의미가 없습니다. 예를 들어 vessel_type, time 및 attack_description에 대한 확인은 귀찮습니다. 왜냐하면 이러한 속성의 대부분 값이 널입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 탐색적 데이터 분석\n\nPlotly는 제가 가장 선호하는 파이썬 시각화 라이브러리이기 때문에 이 게시물의 모든 시각화는 Plotly로 만들어졌습니다.\n\n이 분석은 세 가지 섹션으로 나뉘어 있습니다:\n\n- 공간 분석\n- 시계열 분석\n- 상관 분석\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 1. 공간 분석\n\n데이터를 시각화한 지도는 만드는 것이 가치가 있어요.\n\n```python\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Plotly Express를 사용하여 산포도 그리기\nfig = px.scatter_mapbox(pirate_attacks, lat='latitude', lon='longitude', \n                        title='해적 공격: 위도와 경도',\n                        zoom=2, height=600)\n\n# Mapbox를 사용하여 지도 레이아웃 업데이트\nfig.update_layout(mapbox_style='open-street-map')\nfig.update_layout(margin=dict(l=0, r=0, t=40, b=0))\n\n# 그래프 표시\nfig.show()\n```\n\n가장 가까운 국가별 공격 횟수를 보여주는 막대 그래프도 만들었어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nimport plotly.express as px\n\n# pirate_attacks 데이터를 가장 가까운 국가와 연도별로 그룹화하여 각 그룹의 공격 횟수 계산\npirate_attacks['date_datetime'] = pd.to_datetime(pirate_attacks['date'])\nyearly_country_counts = pirate_attacks.groupby(['nearest_country', pirate_attacks['date_datetime'].dt.year]).size().reset_index(name='num_attacks')\n\n# 각 국가의 연간 평균 공격 횟수 계산\navg_yearly_country_counts = yearly_country_counts.groupby('nearest_country')['num_attacks'].mean().reset_index()\navg_yearly_country_counts = avg_yearly_country_counts.sort_values(by='num_attacks', ascending=False)\n\n# 국가 지표와 병합하여 국가 이름 가져오기\ndf_top_10_countries = pd.merge(avg_yearly_country_counts.head(10), country_codes[['country', 'country_name']],\n                                left_on='nearest_country', right_on='country', how='left')\n\n# Plotly Express를 사용하여 바 차트 생성\nfig = px.bar(df_top_10_countries, x='num_attacks', y='country_name', orientation='h',\n             title='평균 공격 횟수가 높은 상위 10개 국가',\n             labels={'num_attacks': '연간 평균 공격 횟수', 'country_name': '국가'})\nfig.update_layout(height=600, yaxis_categoryorder='total ascending')\n\n# 차트 보이기\nfig.show()\n```\n\n인도네시아는 연간 평균 공격 횟수가 가장 높은 국가입니다. 기타 국가로는 예멘, 나이지리아, 말레이시아 등이 있습니다.\n\n데이터 포인트를 DBSCAN을 이용해 클러스터링하는 것이 흥미로웠습니다. 특히, DBSCAN 클러스터링을 위해 'eps' 매개변수를 설정했는데, 이는 동일 클러스터에 속하는 두 점 사이의 최대 거리를 정의합니다. 0.05도로 설정했는데, 이는 지구 상의 위치에 따라 약 5km 정도가 되며, 다시 말해 클러스터 내 임의의 두 점 간 최대 거리는 5km보다 작아야 합니다.\n\n또한 'min_samples' 매개변수를 50으로 설정했는데, 이는 클러스터에 속해야 하는 최소한의 점 수를 정의합니다. 다시 말해, 모든 형성된 클러스터는 적어도 50개의 점을 가져야 하며, 이와 거리가 5km 이상 떨어진 다른 점들은 이상점으로 간주됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\nfrom sklearn.cluster import DBSCAN\nimport numpy as np\n\n# NaN 값을 포함하는 행 삭제\npirate_attacks.dropna(subset=['latitude', 'longitude'], inplace=True)\n\n# DBSCAN을 위해 위도 및 경도 열을 라디안으로 변환\npirate_attacks['lat_rad'] = np.radians(pirate_attacks['latitude'])\npirate_attacks['lon_rad'] = np.radians(pirate_attacks['longitude'])\n\n# lat_rad 및 lon_rad 열을 numpy 배열로 연결\ncoordinates = pirate_attacks[['lat_rad', 'lon_rad']].values\n\n# DBSCAN 클러스터링 수행\ndbscan = DBSCAN(eps=0.05, min_samples=50)  # 필요에 따라 eps 및 min_samples 조정\ndbscan.fit(coordinates)\n\n# DataFrame에 클러스터 레이블 추가\npirate_attacks['cluster'] = dbscan.labels_\n\n# Plotly Express를 사용하여 클러스터가 다르게 색칠된 산점도 생성\nfig = px.scatter_mapbox(pirate_attacks, lat='latitude', lon='longitude', color='cluster',\n                        title='해적 공격 클러스터', zoom=2, height=600,\n                        color_continuous_scale=px.colors.qualitative.Light24)\n\n# Mapbox를 사용하여 맵 레이아웃 업데이트\nfig.update_layout(mapbox_style='open-street-map')\nfig.update_layout(margin=dict(l=0, r=0, t=40, b=0))\n\n# 플롯 표시\nfig.show()\n```\n\n클러스터링 결과, 지도 시각화에서 모든 이상치를 포함한 20개의 클러스터가 생성되었습니다. 가장 많은 사건 수를 가진 상위 5개 클러스터는 다음과 같습니다:\n\n- 말라카 해협/싱가포르 해협/인도네시아 지역으로 2,699개의 포인트\n- 나이지리아와 토고의 다양한 항구(라고스, 로메 등)로 925개의 포인트\n- 아덴만, 붉은 바다, 소말리아 지역으로 685개의 포인트\n- 방글라데시의 치타공 항구로 509개의 포인트\n- 남인도의 다양한 항구(첸나이, 코친, 카키나다 등)로 237개의 포인트\n\n또한 중요한 점은 상위 클러스터 중 하나가 이상치 클러스터임을 언급해야 합니다. 이는 데이터의 상당 부분(957개 관측치)이 특정 패턴을 나타내지 않고 어느 정도 무작위로 분포되어 있음을 의미합니다. 특히, 아프리카 북동 해안을 따라 시각적으로 이상치 포인트를 확인할 수 있지만, 이러한 포인트들은 희소하며 이전에 설정된 DBSCAN 클러스터링 매개변수에 따라 명확한 클러스터를 형성하지 못합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2. 시계열 분석\n\n지도에 표시된 데이터는 상당 기간에 걸쳐 있습니다. 따라서, 공격 수가 시간이 지남에 따라 어떻게 변화했는지 확인하는 것도 중요합니다. 이를 위해 월 단위로 시간에 따른 공격 수를 그래프로 표시합니다.\n\n```js\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# 'date' 열을 날짜 타입으로 변환하고 인덱스로 설정하기\npirate_attacks['date_datetime'] = pd.to_datetime(pirate_attacks['date'])\npirate_attacks.set_index('date_datetime', inplace=True)\n\n# 월별로 공격 수 집계\nmonthly_attacks = pirate_attacks.resample('M').size()\n\n# 추세 구성 요소에 대한 플롯 생성\nfig_trend = go.Figure(go.Scatter(x=result.trend.index, y=result.trend, mode='lines', name='Trend'))\nfig_trend.update_layout(title='Trend Component',\n                        xaxis_title='Date',\n                        yaxis_title='Number of Attacks')\n\n# 플롯 보이기\nfig_trend.show()\n```\n\n추가로, 시계열의 계절성 구성 요소를 플롯합니다. 계절성 구성 요소는 데이터에서 고정된 시간 간격에서 발생하는 반복적이고 주기적인 변동 또는 패턴을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 계절 분해를 수행합니다\nresult = seasonal_decompose(monthly_attacks, model='additive')\n\n# 계절 성분을 위한 그래프를 생성합니다\nfig_seasonal = go.Figure(go.Scatter(x=result.seasonal.index, y=result.seasonal, mode='lines', name='Seasonal'))\nfig_seasonal.update_layout(title='계절 성분',\n                           xaxis_title='날짜',\n                           yaxis_title='공격 수')\n\n# 그래프를 표시합니다\nfig_seasonal.show()\n```\n\n해당 계절 성분은 4월과 5월에 해적 공격이 현저히 많은 것을 나타내며, 월평균 대비 약 네 번의 추가 공격이 있습니다. 이러한 계절성은 주로 이 달에 나타나는 유리한 날씨 조건으로 인해 해적 활동이 용이해지는 것일 수 있습니다.\n\n또한, 해양 생물, 어업 선박 및 기타 해양 활동의 계절 이동 패턴은 해적 행동에 영향을 미칠 수 있습니다. 그러나 이러한 계절성을 설명하는 다른 요인으로는 경제적, 문화적 또는 정치적 사건 등이 있을 수 있습니다.\n\n## 3. 상관 분석\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내, 해적 사건과 국가 지표 간의 잠재적 상관 관계를 탐색하기 위해 세 개의 데이터 프레임을 하나로 통합하는 새로운 통합 데이터 프레임을 만들었습니다.\n\n다음은 Markdown 형식으로 표(tag)를 변경한 코드입니다:\n\n\n# 'date' 열에서 연도를 추출\npirate_attacks['year'] = pirate_attacks['date_datetime'].dt.year\n\n# nearest_country와 year로 데이터를 그룹화하고 각 그룹의 공격 횟수를 계산\ngrouped_pirate_attacks = pirate_attacks.groupby(['nearest_country', 'year']).size().reset_index(name='num_attacks')\n\n# 'nearest_country'와 'year'를 기준으로 pirate_attacks와 country_indicators를 병합\ndf = pd.merge(grouped_pirate_attacks, country_indicators, left_on=['nearest_country', 'year'], right_on=['country', 'year'])\n\n# 'country'를 기준으로 결과 DataFrame을 country_codes와 병합\ndf = pd.merge(df, country_codes, on='country')\n\n\n그런 다음, 가장 많은 사건이 발생한 국가인 인도네시아에 중점을 두어 인도네시아의 국가 지표와 해적 공격 횟수에 대한 상관 관계 행렬을 만들었습니다.\n\n\ndf = df[df['nearest_country'] == 'IDN']\n\n# 상관 관계 행렬 계산\ncorrelation_matrix = df[['num_attacks', 'corruption_index', 'homicide_rate', 'GDP',\n                         'total_fisheries_per_ton', 'total_military', 'population',\n                         'unemployment_rate', 'totalgr', 'industryofgdp']].corr()\n\n# Plotly를 사용하여 히트맵 생성\nfig = go.Figure(data=go.Heatmap(z=correlation_matrix.values,\n                                 x=correlation_matrix.columns,\n                                 y=correlation_matrix.columns,\n                                 colorscale='RdYlBu'))\n\n# 레이아웃 업데이트\nfig.update_layout(title='선택한 변수의 상관 관계 행렬',\n                  xaxis_title='변수',\n                  yaxis_title='변수',\n                  height=800,\n                  width=800\n                 )\n\n# 플롯 표시\nfig.show()\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Understanding Maritime Threats](/assets/img/2024-06-23-UnderstandingMaritimeThreatsAData-drivenApproachtoPiracyAnalysis_6.png)\n\n인도네시아의 경우, 상관 행렬은 흥미로운 결과를 나타냈습니다. 공격 횟수와 살인 비율 간에는 양의 상관 관계(0.49)가 있어 나라 내 해적 사건의 빈도와 폭력 범죄 수준 사이에 관련성을 보여줍니다.\n\n그 외 양의 상관 관계가 있는 변수로는 실업률(0.33)과 총 군사(0.27)가 있습니다. 이러한 양의 상관 관계는 범죄율, 고용 기회 및 군사 존재와 같은 사회 경제적 요소와 해적 활동 사이의 잠재적 연결을 나타냅니다.\n\n# 제 생각\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제공된 데이터셋을 사용하여 해적 공격의 분석 및 시각화는 해당 사건과 관련된 양식 및 요소에 대한 유용한 통찰을 제공할 수 있습니다. 데이터셋의 탐색을 통해 해적 활동의 지리적 핫스팟, 계절적 추이 및 사회경제 지표와의 상관 관계가 나타납니다. 이를 통해 기관은 해적 발생을 모델링하거나 예측하여 선원 및 해안 지역 사회를 안전하게 보호하고 안전한 배상 노선을 결정할 수 있습니다. 해적 사건을 주도하는 기본 요인을 이해함으로써 정책 결정자와 해양 당국은 해적을 근절하고, 해상 안보를 증진하며, 취약한 지역에서 해양 활동의 안전을 향상시키기 위한 보다 효과적인 전략과 개입을 개발할 수 있습니다.\n\n# 참고 자료 \n\nBenden, P., Feng, A., Howell, C., \u0026 Dalla Riva G. V. (2021). Crime at Sea: A Global Database of Maritime Pirate Attacks (1993–2020). Journal of Open Humanities Data, 7: 19, pp. 1–6. DOI: https://doi.org/10.5334/johd.39\n\n✨독서해 주셔서 감사합니다!✨\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n💼 Upwork에서 함께 일해요!\n\n💌 Medium이나 LinkedIn에서 함께 하세요!","ogImage":{"url":"/assets/img/2024-06-23-UnderstandingMaritimeThreatsAData-drivenApproachtoPiracyAnalysis_0.png"},"coverImage":"/assets/img/2024-06-23-UnderstandingMaritimeThreatsAData-drivenApproachtoPiracyAnalysis_0.png","tag":["Tech"],"readingTime":11},{"title":"Microsoft Power BI에서 Python Seaborn으로 간단한 Pairplot 시각화하기 방법","description":"","date":"2024-06-23 16:04","slug":"2024-06-23-CreatingsimplePairplotvisualsusingPythonSeaborninMicrosoftPowerBI","content":"\n\n시본(Seaborn): 시본은 파이썬에서 통계 플로팅에 사용되는 라이브러리입니다. 시본은 맷플롯립을 기반으로 구축되어 있으며, 통계 플롯을 보다 매력적으로 만드는 멋진 시각화에 널리 사용됩니다. 시본에 대해 더 알고 싶으신가요?\n\nPower BI에서 파이썬 설정하기:\n\n단계 01: Power BI Desktop을 엽니다. 파일 → 옵션 및 설정 → 옵션 → Python 스크립팅으로 이동합니다.\n\n단계 02: Python 설치 여부를 확인하려면 'Python 설치 방법' 링크를 클릭하여 Python 설치를 안내하는 단계를 따르세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Python Power BI Step 1](/assets/img/2024-06-23-CreatingsimplePairplotvisualsusingPythonSeaborninMicrosoftPowerBI_0.png)\n\nTo connect to Python from Power BI:\n1. Click on “Get Data” under “Data Sources”.\n2. Select the “Python Script” connector.\n3. Click OK.\n4. A Popup window will appear, allowing you to write your Python Script.\n\n![Python Power BI Step 2](/assets/img/2024-06-23-CreatingsimplePairplotvisualsusingPythonSeaborninMicrosoftPowerBI_1.png)\n\n![Python Power BI Step 3](/assets/img/2024-06-23-CreatingsimplePairplotvisualsusingPythonSeaborninMicrosoftPowerBI_2.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 번 matplotlib과 seaborn 라이브러리를 가져온 후 Seaborn을 사용하여 데이터를 로드합니다. 데이터 세트의 이름은 \"tips\"이지만, 제가 \"Data\"로 이름을 변경했습니다. 로컬에 저장된 데이터 세트를 읽고 싶다면 pandas 라이브러리를 가져와서 pd.read_csv()를 사용하여 파일을 읽을 수 있습니다. Python 스크립트 작성이 완료되면 확인을 클릭해주세요. 데이터를 Power BI로 로드하세요.\n\n![이미지](/assets/img/2024-06-23-CreatingsimplePairplotvisualsusingPythonSeaborninMicrosoftPowerBI_3.png)\n\n시각화 캔버스에서 Python Visual을 선택해주세요.\n\n![이미지](/assets/img/2024-06-23-CreatingsimplePairplotvisualsusingPythonSeaborninMicrosoftPowerBI_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPython Visual을 클릭하면 Microsoft Power BI의 Python Script 편집기에 필요한 시각화를 만들 Python 스크립트를 작성해야 합니다.\n\n![image](/assets/img/2024-06-23-CreatingsimplePairplotvisualsusingPythonSeaborninMicrosoftPowerBI_5.png)\n\n아래 스크린샷은 Power BI 대시보드에 완벽하게 맞춘 Pairplot을 보여줍니다.\n\n![image](/assets/img/2024-06-23-CreatingsimplePairplotvisualsusingPythonSeaborninMicrosoftPowerBI_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이렇게 하면 Seaborn과 matplotlib을 사용하여 멋진 시각화를 쉽게 만들 수 있어요.\n\nPower BI에 대한 더 많은 통찰력을 얻으려면 저를 팔로우해 주세요. 친구들과 공유하는 걸 잊지 마세요.\n\n감사합니다,\n\n수만트 시슬라","ogImage":{"url":"/assets/img/2024-06-23-CreatingsimplePairplotvisualsusingPythonSeaborninMicrosoftPowerBI_0.png"},"coverImage":"/assets/img/2024-06-23-CreatingsimplePairplotvisualsusingPythonSeaborninMicrosoftPowerBI_0.png","tag":["Tech"],"readingTime":2},{"title":"데이터 분석에서 절대 빼먹으면 안 되는 것들","description":"","date":"2024-06-23 16:02","slug":"2024-06-23-WaitYouForgotSomethinginYourDataAnalysis","content":"\n\n대도시에서 배달 사업을 운영하는 소유자라고 상상해보세요. 귀하의 전달 직원은 모두 자전거를 타고 배달을 하고 있습니다. 자전거 사고로 발생하는 응급실 방문을 기반으로 한 연구 결과를 읽었습니다. \"사이클링 중 부상을 입은 사람들의 데이터에 따르면, 헬멧을 착용한 사람 중 57%가 부상을 입었으며 착용하지 않은 사람은 43%입니다.\" 귀하의 직원들에게 어떤 것을 권장하겠습니까?\n\n사고 발생 시 헬멧이 뇌를 심각하게 보호한다는 근거를 따를 수도 있고, 헬멧 착용으로 인해 직원들이 무모하게 움직이는 것으로 생각하여 헬멧 착용을 금지할 수도 있습니다. (그런데 헬멧을 쓰는 사람들이 더 위험한 행동을 하지는 않습니다. \"위험 보상\" 이론은 최선이 아니라는 점입니다.) 연구를 살펴보면, 1,000명의 자전거 타는 사람 중 133명(13%)이 사고에 연루되었으며 그 중 70명(사고에 연루된 사람 중 53%)이 응급실에 입원했습니다. 응급실에 입원한 사람 중 40명(57%)은 헬멧을 착용하고 있었고 30명(47%)은 착용하지 않았습니다.\n\n이제 커피 마시는 사람이라고 상상해보세요. 아침 신문을 읽는 중에 커피를 놓고 의사에게 전화하고 싶어지는 기사를 보게 됩니다. 기사는 커피와 췌장암의 연관성에 대한 연구 결과를 요약합니다. \"연구, 커피 섭취와 췌장암 연결,\"라는 대문자 제목이 눈에 띕니다. \"통계적 연관성은 커피가 암을 유발한다는 것을 증명하지는 않지만, 연구 그룹의 리더인 하버드 대학의 브라이언 맥마헌 박사는 연구 결과가 명백해지자 며칠 전 커피를 그만 먹었다고 말했습니다. 전화 인터뷰에서 그는 다른 사람들에게 조언해야 한다고 자신하지 않겠다고 말했습니다,\" 기사에는 이렇게 써 있습니다.\n\n자, 충분합니다. 만약 하버드 대학의 맥마헌 박사가 자신의 연구로 인해 커피를 먹지 않는다면, 그에게 무언가가 있는 것이 분명합니다. 그렇죠?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 음…\n\n일부 연구에 불쑥 스며들거나 연구 설계에서 찾지 않았을 때 발생하는 흥미로운 편향이 있습니다. 이것은 \"버크슨의 편향(Berkson's Bias)\"이라고 불립니다. 이것은 데이터에서 존재할 수 있는 원인과 결과 사이의 관계를 보여줌으로써 결과에 영향을 미치는 유형의 편향이지만 실제 세계에는 없을 수도 있습니다. 아니요, 당신이 이런 걸 보는 게 아니에요. 네, 당신의 데이터는 유효해요. 단지 데이터 수집 방법을 계획하는 과정에서 어떤 일이 벌어져서 이러한 잘못된 연관성이 나타난 것일 뿐입니다.\n\nJoseph Berkson은 1946년에 이 편향을 최초로 설명했으며, 과학자들은 여전히 논문이 거절되거나 철회되거나 치명적으로 비평당하기도 합니다. 예를 들어 커피와 췌장암과 관련된 논문과 같은 경우입니다. 일어나는 현상은 연구 대상 군이 일반 대중과 비슷하지 않으므로 결론을 내릴 수 있는 것은 연구 대상 군과 유사한 사람들에게로 제한되고 전체 대중에는 해당되지 않습니다. 또는, 연구 대상 군에 다른 사람들을 분석에 포함시키지 않아 수학적으로 문제가 발생하는 것일 수 있습니다.\n\n## 헬멧 착용은 안전합니다, 아이들여\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n픽셔널 자전거 헬멧 예시로, 자전거 타는 전체 인구 즉, 1,000명에 대해 알아볼게요. 더욱 명료하게 하기 위해 이를 점으로 나열해 볼게요:  \n\n- 자전거 타는 사람은 1,000명이에요.\n- 그 중 75%가 헬멧을 쓰고 있어요. 그렇게 되면 750명이 헬멧을 쓰고 있는 거죠.\n- 25%는 헬멧을 쓰고 있지 않아요. 그러면 250명의 사람이 헬멧을 안 쓰고 있어요.\n- 750명 중 헬멧을 쓰고 있는 사람 중 100명 (13%)이 사고를 당했어요.\n- 헬멧을 쓰지 않은 250명 중 33명 (13%)이 사고를 당했어요.\n- 두 그룹 모두 사고 위험이 동일해요.\n- 100명의 헬멧을 쓰고 사고를 당한 피해자 중 40명 (40%)이 응급실로 가야 했고, 나머지 60명은 가지 않았어요.\n- 33명의 헬멧을 쓰지 않은 사고 피해자 중 30명 (91%)이 응급실로 가야했지만, 3명은 가지 않았어요.\n- 응급실 방문 위험은 헬멧을 쓰지 않은 그룹에서 높아요.\n- 응급실에 들어간 70명의 피해자 중 40명 (57%)이 헬멧을 썼고, 나머지 30명 (43%)은 헬멧을 쓰지 않았어요.\n- 응급실에서의 헬멧 착용 위험은 모든 사이클리스트에 대해 높아요.\n\n마지막에 한 것을 보셨나요? 이것이 올바른 결론입니다. 사고 위험은 같지만, 응급실 방문 위험은 헬멧을 쓰지 않은 그룹이 더 높아요 (91% 대 40%). 그러나 분석은 ER의 마지막 70명만을 살펴보았으며 전체 사이클리스트 인구에 대해 고려되지 않았어요.\n\n## Go Ahead and Drink Your Joe\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n커피와 췌장암에 관한 연구에 따르면, 연구자들은 연구 대상을 일반 대중이 아닌 위장 질환 클리닉에서 선정했습니다. 그들은 클리닉에서의 췌장암 환자를 선택하고, 다른 질환으로 클리닉에 있는 사람들과 매칭시켰습니다. 결과적으로, 췌장암을 앓는 사람들이 커피를 더 많이 마신다고 보고되었습니다. 다른 질환을 앓는 사람들은 더 적게 커피를 마셨습니다. 혹은, 편집장에게 보낸 편지 중 하나에서는 다음과 같이 언급되었습니다:\n\n이후 연구가 진행된 분석에 따르면, 연구자들이 실험군의 식이 제한을 고려하지 못했을 가능성이 있습니다. 그들은 이미 위장 질환으로 클리닉에 방문한 사람들이었고, 그 중 많은 사람들은 치료 요법으로 커피를 마시지 않고 있었습니다. 더구나, 연구는 클리닉에 있지 않은 병이 없는 많은 커피 마시는 사람들을 포함하지 않았습니다.\n\n요약하자면, 이 연구는 연구 대상과 유사한 사람들에게만 일반화될 수 있으며, 일반 대중에게는 적용할 수 없습니다.\n\n## 무슨 일이 있는 걸까?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n건강 관련 분야에서 참가자를 모집하지만 연구 결과를 대중에 일반화하려고 하는 경우에는 이러한 한계가 있을 수 있습니다. 연구 대상이 특정 질병 또는 상태의 최악의 경우만 분석하는 경우, 결과를 일반화하기 어렵습니다. 그리고 원인과 결과를 연관시키지만 해당 원인에 노출된 전체 인구를 고려하지 않는다면 선택편향이 발생할 수 있습니다.\n\n아래는 이에 대한 그래픽 표현입니다:\n\n![](/assets/img/2024-06-23-WaitYouForgotSomethinginYourDataAnalysis_0.png)\n\n왼쪽에는 노출과 결과 사이의 약한 양의 상관 관계가 있습니다. 오른쪽에는 노출과 결과 사이의 강한 음의 상관 관계가 있습니다. 두 경우 모두 동일한 데이터 세트에서 나왔지만, 오른쪽의 경우에는 몇 가지 데이터 포인트가 누락된 선택 기준이 있었습니다. 사고 부상으로 응급실에 가지 않아도 되는 사람들이나 위장 증상으로 클리닉에 가야 하는 필요가 없는 사람들을 제외하는 경우, 연관성을 인위적으로 만들어낼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 그래서요?\n\n데이터와 증거를 해석하는 방법에 주의하세요. 가장 뛰어난 두뇌도 연구에서 선택 편향이나 다른 편향에 빠질 수 있습니다. 혼동 요인에 대해서는 말도 안 돼죠. 신뢰하되 검증하는 걸 잊지 마세요... 아니면 적어도 누군가가 검증을 할 때까지 기다리세요.\n\n커피를 즐기세요.\n\n만약 방금 읽은 내용이 마음에 들었고 일반적인 Medium 기사도 좋은데, 우리의 작업을 지원하고 싶다면 멤버십을 획득하고 우리의 작업을 지원해 보세요! 자세한 내용은 여기를 클릭해보세요: [https://epiren.medium.com/membership](https://epiren.medium.com/membership) 감사합니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRené F. Najera, MPH, DrPH은 공중보건 의사이자 역학학자이며, 아마추어 사진작가이자 러닝/사이클링/수영 애호가입니다. 그는 남편이자 아버지이며, \"전반적으로 훌륭한 남자\"로 알려져 있습니다. 그는 공중보건 센터의 소장으로 일하거나, 지역 타코 가게에서 타코를 먹거나, 버지니아 북부의 한 대학에서 교수로 재직하며, 거기서 국제 및 지역보건학부의 부교수로 지정된 세계 최고의 공중보건 학교에서 가르칩니다. 이 블로그 글의 모든 의견은 나회자 박사의 개인 의견이며, 그의 직장, 친구, 가족 또는 지인들의 의견을 반영하는 것은 아닙니다.","ogImage":{"url":"/assets/img/2024-06-23-WaitYouForgotSomethinginYourDataAnalysis_0.png"},"coverImage":"/assets/img/2024-06-23-WaitYouForgotSomethinginYourDataAnalysis_0.png","tag":["Tech"],"readingTime":5},{"title":"처음 데이터 분석가로 일하기 전에 알았더라면 좋았을 5가지 사항","description":"","date":"2024-06-23 16:01","slug":"2024-06-23-5ThingsIWishIKnewBeforeMyFirstJobasaDataAnalyst","content":"\n\n\n![Image](/assets/img/2024-06-23-5ThingsIWishIKnewBeforeMyFirstJobasaDataAnalyst_0.png)\n\n첫 데이터 분석가 직무에 5개월 차인데, 앞으로 다른 사람이 알려주었으면 하는 놀라운 점들이 많아요! 아래에 나열해서, 제가 처음에 알았더라면 더 나은 준비를 할 수 있었을 텐데요.\n\n# 첫 데이터 분석가로 일을 시작하기 전에 알았으면 좋았을 5가지 사항\n\n# 1. 만약 팀에서 유일한 기술 직군인 경우, 동료들이 당신이 하는 일을 이해하지 못할 수도 있어요. 상황에 따라 괜찮을 수도 있지만, 비극적일 수도 있어요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n면접 때, 팀 내에서 당신이 유일한 기술 담당자인 지 판단해 보세요. 만약 당신이 유일한 기술 담당자라면, 이게 괜찮을 수도 있어요 — 동료들이 당신이 학습 중이며 전문 분야를 벗어난 특정 작업이 있을 수 있다는 것을 이해하고, 당신을 지원하고 당신의 편에 서 있을 매니저도 존재한다면요. 하지만, 여전히 첫 번째 기술 직무에서 '독립 작업'을 권하지는 않을 거예요.\n\n유일한 기술 담당자가 되는 것은 치명적일 수 있어요. 부서에서 당신을 유일한 기술 리더로 원하는 바로모든 회의와 프로젝트에 참여하게 될 수도 있어요. 당신은 직무 범위를 넘어서는 작업을 수행하기를 기대받을 수도 있습니다. 이것이 당신의 전문 분야가 아니거나 급여 수준을 벗어나는 일일지라도요. 가장 최악의 경우에는 '기술'과 관련한 모든 문제가 발생할 때에 당신이 모든 책임을 져야 할 수도 있습니다. 심지어 시스템을 설정하지 않았거나 제대로 교육받지 않았거나 당신이 그 이내에 해당 사항을 알지 못했더라도요.\n\n만약 당신이 혼자라면, 다른 팀의 멘토를 찾거나 (LinkedIn이나 다른 플랫폼을 통해) 온라인 멘토를 발견하여 지원을 받으세요. 저는 특히 Maven Analytics를 통해 LinkedIn 데이터 분석 커뮤니티가 매우 지원적이라고 생각해요.\n\n반면에, 기술팀이나 데이터 팀의 일부가 되어 있다면, 더 경험이 많은 기술 인들로부터 배울 수 있고, 그들을 멘토로 활용할 수 있어요. 또한, 다른 직원이 기술 요청을할 때 당신이 속한 티켓 시스템을 통해 제출할 수 있어요. 이를 통해 당신과 팀원들이 과제를 나눠 수행할 수 있어요. 이는 데이터/기술 요청을 담당하는 사람이 한 명에게 모든 부담이 집중되지 않게 도와줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2. 많은 시간을 데이터 정리에 할애하게 될 거에요.\n\n데이터 분석 과정에서 대부분 깔끔하고 정리된 데이터세트를 분석합니다.\n\n데이터 분석가로서 업무를 하다 보면, 정리되지 않은 혼란스러운 데이터들을 다루게 될 거에요. 예전 설문조사 데이터 중에 필드가 제대로 형식화되어 있지 않거나 값이 제대로 대문자로 표시되지 않은 경우, 그리고 엑셀 수식이 오류를 일으키는 경우가 있습니다. 동료들이 데이터의 소스나 정확성을 모르는 경우에 데이터 시각화를 위한 데이터 모델을 만들라며 요청을 받을 수도 있습니다.\n\n준비를 위해: Excel 스프레드시트 데이터를 정리하는 연습을 하세요. 데이터 정리에 관련된 기사와 YouTube 영상을 살펴보세요. ChatGPT에게 오류가 있는 더미 Excel 스프레드시트를 생성하도록 요청하고, Excel 수식을 사용하여 데이터를 정리하는 연습을 해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 3. 사람들은 항상 당신의 숫자들에 만족스러워 하지 않을 수 있어요.\n\n비즈니스에서 모두가 \"데이터 기반 통찰력\"을 원한다고 말합니다. 이것은 그들이 자신의 주장을 뒷받침해주는 데이터 포인트를 원한다는 것을 의미합니다. 때로는 동료나 고객들이 당신의 숫자에 만족하지 않을 수 있습니다. 그들은 고객 만족도 평균을 계산해 달라고 요청할 수도 있고, 숫자가 예상보다 낮을 때 실망할 수 있습니다. 그들에게 메신저를 쏘지 말라고 말하더라도 실망할 수 있고, 제품을 유리하게 표현할 수 있는 다른 지표를 찾을 수도 있습니다.\n\n\"메신저를 쏘지 말라\" 상황을 미리 예방하기 위해 데이터 프로젝트를 수행하기 전에 상사와 명확한 명세를 확립하세요. 어떤 값을 계산할 것인지, 어떻게 계산할 것인지 등을 정하세요. 이러한 명세에 대해 상사나 관련 이해 관계자들로부터 승인을 받고, 정확한 연도 대비 분석을 위해 보고서 표준화의 중요성을 강조하세요. 예를 들어, 매년 서로 다르게 계산한 평균값을 보고하고 있다면 조직이 더 나아지고 있는지 아니면 나빠지고 있는지 알 수 없을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 4. 직무 설명은 매일 하는 일과 매우 다를 수 있습니다.\n\n첫 데이터 분석가 역할에 면접을 봤을 때, 직무 설명서에는 SQL과 Microsoft Power BI를 사용할 것이라고 나와 있었습니다. 그러나 실제로는, 매일 엑셀을 주로 사용하여 즉시 데이터 요청에 응답하고 기술적 배경이 필요 없는 행정 업무를 수행합니다.\n\n이것은 #1이 사실일 경우 더 그럴 수 있습니다 - 만약 팀에서 당신이 유일한 기술 직군이라면요. 이전 기술 직군과의 시간적 격차가 있었다면, 당신에게 할당된 행정/기술 업무가 백로그로 쌓일 것입니다.\n\n이것은 괜찮을 수도 있습니다 - 여전히 학습 중이고, 자극적인 프로젝트를 수행하고, 이력서에 어떤 지표들을 기록하고 있다면요. 반면에, 특정 고급 기술을 습득할 것으로 생각했지만 매일 엑셀에서 작업하고 있다면 당황스러울 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 5. 팀에 다른 기술 인력이 있다고 해서 그들이 당신을 교육할 시간이나 전문 지식을 가지고 있다는 것은 아닙니다.\n\n이것을 이해하려면 면접 중에 다음 질문들을 해보세요:\n\n- \"이 팀/이 부서/이 조직에 다른 기술 인력이 있나요?\"\n\n만약 있다면:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- \"그들의 일상 업무는 무엇인가요?\"\n- \"그들의 업무 범위가 제 업무와 중복되기도 하나요?\"\n- \"이전에 같은 역할을 한 사람이 그들과 어떻게 상호 작용했나요? 얼마나 자주 만나셨나요?\"\n- \"저를 안내하고 이 플랫폼을 사용하는 방법을 알려줄 분은 그들이 맡게 될 것인가요?\"\n- \"이 직무에서 멘토링을 받을 수 있는 기회가 있나요? 이 조직에서 어떻게 멘토를 찾을 수 있을까요?\"\n\n# 총정리하자면: Google, ChatGPT, Stack Overflow, 그리고 Reddit이 당신의 가장 친한 친구가 될 것입니다. 어떤 기술 직무에서든 이 말이 사실입니다! 그러니 구글링에 익숙해지세요.\n\n나는 팀 내 유일한 기술 담당자이며, 이것이 나의 첫 번째 기술 직무입니다. 많은 것을 배웠지만, 훈련을 위한 시간을 찾으려고 하고, 적절한 분석 방법을 습득하고, 가이드가 되는 사람이 없이 데이터 소스의 타당성을 확인하려고 하니 압도되는 느낌을 받습니다 (적어도 조금은 가이드해 줄 사람이 없네요!)\n\n어떤 기술 직무에서든 예상치 못한 일들이 벌어질 것입니다. 당신은 팀에서 선정된 '똑똑한 사람'입니다. 이는 당신이 '마스터 구글러'임을 의미합니다. 다른 사람보다 더 많이 알 필요는 없지만, 문제를 해결할 때 까지 구글 검색하는 끈기가 필요합니다!","ogImage":{"url":"/assets/img/2024-06-23-5ThingsIWishIKnewBeforeMyFirstJobasaDataAnalyst_0.png"},"coverImage":"/assets/img/2024-06-23-5ThingsIWishIKnewBeforeMyFirstJobasaDataAnalyst_0.png","tag":["Tech"],"readingTime":4},{"title":"위성 이미지 시각화를 위한 Streamlit 앱 만들기 단계별 가이드","description":"","date":"2024-06-23 15:59","slug":"2024-06-23-CreatingaStreamlitAppforSatelliteImageryVisualizationAStep-by-StepGuide","content":"\n\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*DZ2rEeyOTUZ2bWq8FnvJ2w.gif)\n\n# Table of Contents\n\n- 🌟 Introduction\n- 📌 Setup\n- 💾 Design the pages\n- 🌍 Functions for Map Visualization\n- 📄 Conclusion\n- 📚 References\n\n## 🌟 Introduction\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n공유는 우리 삶의 필수적인 부분이 되었습니다. 매 순간 X, Instagram, TikTok 등 다양한 플랫폼에 수많은 사진과 비디오가 업로드되며, 이는 다른 사람들과 순간을 공유하기 위한 것입니다. 코딩도 예외가 아니며, 우리가 코드를 다른 사람들과 공유할 수 있는 플랫폼 중 하나가 Streamlit입니다. 저는 이전에 다양한 방법을 사용하여 위성 이미지를 다운로드하고 시각화하는 것에 관한 여러 포스트를 게시해 왔습니다. 이번 포스트에서는 인증 설정이 필요 없는 Streamlit 앱을 개발하는 방법을 보여 드리겠습니다. 이 앱은 지구상의 어떤 지점과 기간에 캡처된 Sentinel-2의 사용 가능한 이미지를 나열하고, 사용자들이 해당 목록에서 이미지를 선택하고 해당 위치의 RGB 이미지와 씬 분류를 시각화할 수 있게 합니다. 이 포스트는 다른 사람들과 공유하기 위해 이 앱을 개발하는 단계별 가이드입니다. 더 알고 싶다면 계속 읽어보세요!\n\n## 📌 설정\n\nStreamlit을 사용하여 어떤 앱을 만들든, 첫 번째 단계는 임의의 이름을 가진 폴더를 만들고 그 안에 두 가지를 설정하는 것입니다. 첫 번째는 우리의 주요 코드를 포함할 빈 Python 스크립트입니다. 두 번째로 옵션인데, .streamlit이라는 하위 폴더를 만드는 것입니다. 이 하위 폴더는 웹 앱에 로그인하는 데 사용되는 사용자 이름과 비밀번호(자격 증명)을 저장하는 데 사용됩니다. 앱에 로그인 페이지를 원치 않는 경우 이 하위 폴더를 무시할 수 있습니다. 그렇지 않으면 메모장을 열어 아래 형식으로 사용자 이름과 비밀번호를 작성하고 secrets.toml이라는 이름으로 저장하세요.\n\n```js\n[passwords]\n# 규칙을 따라 입력하세요: username = \"password\"\nMahyar = \"abc123\"\n```  \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 설정 단계를 따르면 폴더에 이러한 파일이 있어야 합니다:\n\n![Image 1](/assets/img/2024-06-23-CreatingaStreamlitAppforSatelliteImageryVisualizationAStep-by-StepGuide_0.png)\n\n![Image 2](/assets/img/2024-06-23-CreatingaStreamlitAppforSatelliteImageryVisualizationAStep-by-StepGuide_1.png)\n\n💾 페이지 디자인하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 단계는 사용자 경험을 향상시키기 위해 앱에 추가할 요소를 고려하는 것입니다. 사용자를 안내하는 텍스트 정보, 앱 실행에 필요한 사용자로부터 필요한 정보, 그리고 단계별로 앱을 실행하는 버튼을 포함합니다. 사용자가 어떤 위치와 기간의 위성 이미지를 볼 수 있도록 하는 것이 목표이므로, 다음 요소들을 페이지에 포함해야 합니다:\n\n- 로그인 페이지\n- 환영 메시지\n- 페이지 제목\n- 위성 이름에 관한 정보\n- 위치 및 위치 주변의 버퍼\n- 검색 기간\n- 구름 덮인 정도\n\n세 개의 버튼: 이용 가능한 목록을 가져오는 버튼, 목록을 다운로드하는 버튼, 선택한 이미지를 시각화하는 버튼.\n모든 이 요소들은 다음과 같은 메인 스크립트의 라인을 사용하여 구현할 수 있습니다:\n\nPart 1: 필수 라이브러리 및 사용자 인증\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom st_files_connection import FilesConnection\nfrom pystac_client import Client\nfrom odc.stac import load\nimport hmac\n\n\ndef check_password():\n    \"\"\"Returns `True` if the user had a correct password.\"\"\"\n\n    def login_form():\n        \"\"\"Form with widgets to collect user information\"\"\"\n        with st.form(\"Credentials\"):\n            st.text_input(\"Username\", key=\"username\")\n            st.text_input(\"Password\", type=\"password\", key=\"password\")\n            st.form_submit_button(\"Log in\", on_click=password_entered)\n\n    def password_entered():\n        \"\"\"Checks whether a password entered by the user is correct.\"\"\"\n        if st.session_state[\"username\"] in st.secrets[\n            \"passwords\"\n        ] and hmac.compare_digest(\n            st.session_state[\"password\"],\n            st.secrets.passwords[st.session_state[\"username\"]],\n        ):\n            st.session_state[\"password_correct\"] = True\n            del st.session_state[\"password\"]  # Don't store the username or password.\n            del st.session_state[\"username\"]\n        else:\n            st.session_state[\"password_correct\"] = False\n\n    # Return True if the username + password is validated.\n    if st.session_state.get(\"password_correct\", False):\n        return True\n\n    # Show inputs for username + password.\n    login_form()\n    if \"password_correct\" in st.session_state:\n        st.error(\"😕 User not known or password incorrect\")\n    return False\n\n\nif not check_password():\n    st.stop()\n``` \n\n이 코드는 위성 이미지를 시각화하는 Streamlit 앱을 설정합니다. 웹 앱을 구축하는 데 필요한 필수 라이브러리를 가져옵니다. 데이터 조작 및 숫자 연산을 위해 pandas 및 numpy, 파일 연결을 위한 st_files_connection, 보안 비밀번호 비교를 위해 HMAC를 포함하여 필요한 라이브러리를 가져옵니다.\n\ncheck_password() 함수는 사용자 인증을 처리합니다. 입력된 자격 증명을 HMAC를 사용하여 저장된 자격 증명과 비교하여 확인하는 login_form()을 생성합니다. 주요 check_password() 함수는 로그인 프로세스의 상태를 관리하고 로그인이 실패하는 경우 오류 메시지를 표시합니다. 사용자가 인증되지 않은 경우 앱을 중지합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\n# Main Streamlit app starts here\nst.write(\"스트림릿에 오신 것을 환영합니다! 위성 시각화 앱입니다...\")\n\n# Display Title\nst.title(\"위성 지도 포털\")\nst.markdown(\"아래 데이터를 입력하세요.\")\n\n# Initialize session state for date_labels and user_date\nif 'date_labels' not in st.session_state:\n    st.session_state.date_labels = []\n\nif 'data' not in st.session_state:\n    st.session_state.data = None\n\nif 'user_date' not in st.session_state:\n    st.session_state.user_date = None\n\nif 'user_date_index' not in st.session_state:\n    st.session_state.user_date_index = 0\n\ncollections=[\"sentinel-2-l2a\"]\ncolumns = ['collection', 'start_date', 'end_date', 'min_cloud_cover', 'max_cloud_cover', 'longitude', 'latitude','buffer']\n\nwith st.form(key=\"test\"):\n    \n    collection=st.selectbox(\"컬렉션*\",options=collections,index=None)\n    start_date = st.date_input(label=\"시작 날짜*\")\n    end_date = st.date_input(label=\"끝 날짜*\")\n    max_cloud_cover  = st.number_input(label=\"최대 구름 덮인 정도*\",value=10)\n    longitude=st.number_input(label=\"경도*\", format=\"%.4f\",value=-119.7513)\n    latitude=st.number_input(label=\"위도*\", format=\"%.4f\",value=37.2502)\n    buffer=st.number_input(label=\"버퍼 (0.01 = 1 km)*\", format=\"%.2f\",value=0.01)\n\n    # Mark Mandatory fields\n    st.markdown(\"**필수 입력란*\")\n\n    submit_button_run = st.form_submit_button(label=\"실행\")\n    submit_button_list = st.form_submit_button(label=\"가능한 이미지 목록 표시\")\n    submit_button_viz = st.form_submit_button(label=\"시각화\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`C:\\Users\\Streamlit` streamlit run streamlit_app_authen_sentinel_2.py 실행하면 브라우저에서 로그인 및 주요 페이지를 볼 수 있습니다:\n\n부분 1: 로그인 페이지\n\n![이미지](/assets/img/2024-06-23-CreatingaStreamlitAppforSatelliteImageryVisualizationAStep-by-StepGuide_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 제2부: 메인 페이지\n\n![이미지](/assets/img/2024-06-23-CreatingaStreamlitAppforSatelliteImageryVisualizationAStep-by-StepGuide_3.png)\n\n## 🌍 지도 시각화 기능\n\n지금까지 로그인 페이지를 만들고 메인 페이지를 디자인했습니다. 현재 메인 페이지에서는 사용자 입력을 수집하고 아직 기능이 구현되지 않은 세 개의 버튼이 있습니다. 이 섹션에서는 이 버튼을 작동하도록 코드를 완성할 것입니다. 첫 번째 버튼은 \"실행\"이며, 이 버튼은 사용자 입력 매개변수를 사용하여 클라우드 데이터베이스를 검색하고 DataFrame에 있는 사용 가능한 이미지를 표시하는 기능을 합니다. 이 버튼에 기능을 추가하기 위해 두 가지 추가 부분(Part 3 및 Part 4)을 포함했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPart 3: 검색 기능 정의\n\n```python\ndef search_satellite_images(collection=\"sentinel-2-l2a\",\n                            bbox=[-120.15,38.93,-119.88,39.25],\n                            date=\"2023-06-01/2023-06-30\",\n                            cloud_cover=(0, 10)):\n    \n    # 검색 클라이언트 정의\n    client=Client.open(\"https://earth-search.aws.element84.com/v1\")\n    search = client.search(collections=[collection],\n                            bbox=bbox,\n                            datetime=date,\n                            query=[f\"eo:cloud_cover\u003c{cloud_cover[1]}\", f\"eo:cloud_cover\u003e{cloud_cover[0]}\"])\n\n    # 일치하는 항목 수 출력\n    print(f\"찾은 이미지 수: {search.matched()}\")\n\n    data = load(search.items(), bbox=bbox, groupby=\"solar_day\", chunks={})\n\n    print(f\"데이터에 있는 날짜 수: {len(data.time)}\")\n\n    return data\n\ndef get_bbox_with_buffer(latitude=37.2502, longitude=-119.7513, buffer=0.01):\n    \n    min_lat = latitude - buffer\n    max_lat = latitude + buffer\n    min_lon = longitude - buffer\n    max_lon = longitude + buffer\n    \n    bbox = [min_lon, min_lat, max_lon, max_lat]\n    return bbox\n```\n\nPart 3은 사용자가 지정한 매개변수에 따라 STAC API에 연결하여 위성 이미지를 검색하는 search_satellite_images 함수를 정의하는 것으로 시작합니다. 해당 함수는 일치하는 데이터를 반환합니다. 다른 함수 get_bbox_with_buffer는 지정된 위도, 경도 주변에 특정 버퍼를 설정하여 경계 상자를 계산합니다.\n\nPart 4: 실행 버튼\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 이러한 열이 있는 빈 DataFrame을 만듭니다\r\ndf = pd.DataFrame(columns=columns)\r\n\r\nif \"mdf\" not in st.session_state:\r\n    st.session_state.mdf = pd.DataFrame(columns=df.columns)\r\n\r\n\r\n# 새 데이터\r\nwith st.form(key=\"test\"):\r\n    \r\n    collection=st.selectbox(\"collection*\",options=collections,index=None)\r\n    start_date = st.date_input(label=\"start_date*\")\r\n    end_date = st.date_input(label=\"end_date*\")\r\n    max_cloud_cover  = st.number_input(label=\"max_cloud_cover*\",value=10)\r\n    longitude=st.number_input(label=\"longitude*\", format=\"%.4f\",value=-119.7513)\r\n    latitude=st.number_input(label=\"latitude*\", format=\"%.4f\",value=37.2502)\r\n    buffer=st.number_input(label=\"buffer (0.01 = 1 km)*\", format=\"%.2f\",value=0.01)\r\n\r\n    # 필수 사항 표시\r\n    st.markdown(\"**필수*\")\r\n\r\n    submit_button_run = st.form_submit_button(label=\"Run\")\r\n    submit_button_list = st.form_submit_button(label=\"List Available Images\")\r\n    submit_button_viz = st.form_submit_button(label=\"Visualize\")\r\n\r\n    if submit_button_run:\r\n        new_df=pd.DataFrame(\r\n            [\r\n                {   \r\n                    \"collection\": collection,\r\n                    \"start_date\":start_date.strftime(\"%Y-%m-%d\"),\r\n                    \"end_date\": end_date.strftime(\"%Y-%m-%d\"),\r\n                    \"max_cloud_cover\":max_cloud_cover,\r\n                    \"longitude\": longitude,\r\n                    \"latitude\": latitude,\r\n                    \"buffer\": buffer,\r\n\r\n                }\r\n\r\n            ]\r\n        )\r\n        \r\n        st.session_state.mdf = pd.concat([st.session_state.mdf, new_df], axis=0)\r\n        st.dataframe(st.session_state.mdf)\r\n        st.success(\"Your request successfully submitted!\")\r\n\r\n        data = search_satellite_images(collection=collection,\r\n                                       date=f\"{start_date}/{end_date}\",\r\n                                       cloud_cover=(0, max_cloud_cover),\r\n                                       bbox=get_bbox_with_buffer(latitude=latitude, longitude=longitude, buffer=buffer))\r\n        st.session_state.data = data\r\n\r\n        date_labels = []\r\n        # 시간 단계 수 결정\r\n        numb_days = len(data.time)\r\n        # 각 시간 단계 반복\r\n        for t in range(numb_days):\r\n            scl_image = data[[\"scl\"]].isel(time=t).to_array()\r\n            dt = pd.to_datetime(scl_image.time.values)\r\n            year = dt.year\r\n            month = dt.month\r\n            day = dt.day\r\n            date_string = f\"{year}-{month:02d}-{day:02d}\"\r\n            date_labels.append(date_string)\r\n        \r\n        st.session_state.date_labels= date_labels\r\n\n\r\n두 번째 부분에서 주 어플리케이션 로직은 빈 DataFrame을 초기화하고 세션 상태에 DataFrame이 있는지 확인합니다.\r\n\r\n“Run” 버튼을 클릭하면 사용자 입력 값으로 새 DataFrame 항목을 생성하고 세션 상태를 업데이트하며 업데이트된 DataFrame을 표시합니다. 그런 다음 search_satellite_images를 호출하여 데이터를 가져 오고 이 데이터를 처리하여 날짜 레이블을 추출하고 형식을 맞추어 세션 상태에 저장합니다. 이러한 설정을 통해 사용자는 매개 변수를 입력하고 위성 이미지를 검색하며 결과를 시각화 할 수 있습니다.\r\n\r\n이 시점에서 Streamlit 앱을 실행하면 지정된 위치 및 날짜에 대해 다음과 같은 테이블이 표시됩니다:\r\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 \"Run\" 버튼을 활성화한 후 \"List Available Images\" 버튼을 활성화하는 단계입니다. 이 버튼을 클릭하면 사용자가 지정한 위치, 날짜 및 구름 양에 따라 Sentinel-2에 의해 촬영된 사용 가능한 이미지가 표시됩니다.\n\n파트 5: List Available Images 버튼\n\n```js\n    if submit_button_list:\n        user_date=st.selectbox(\"Available Images*\",options=st.session_state.date_labels,index=None)\n        if user_date:\n            st.session_state.user_date = user_date\n            st.session_state.user_date_index = user_date.index()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"List Available Images\" 버튼을 클릭하면 이 코드가 위성 이미지의 사용 가능한 날짜를 나열하는 드롭다운 메뉴를 생성합니다. 사용자가 날짜를 선택하면 해당 날짜와 인덱스를 저장하는 세션 상태를 업데이트하여 사용자의 선택에 기반한 추가 작업을 가능하게 합니다.\n\n이제 앱을 다시 실행하고 \"List Available Images\" 버튼을 클릭하면 사용 가능한 이미지를 나열하는 드롭다운 메뉴가 표시됩니다. 이 중 하나를 선택해주세요:\n\n![이미지](/assets/img/2024-06-23-CreatingaStreamlitAppforSatelliteImageryVisualizationAStep-by-StepGuide_5.png)\n\n사용 가능한 이미지 중 하나를 선택한 후 마지막 단계는 \"Visualize\" 버튼을 완료하고 활성화하는 것입니다. \"Visualize\" 버튼의 목표는 사용자가 지정한 위치와 버퍼에 대해 선택한 위성 이미지를 표시하는 것입니다. 시각화에 관한 이 메시지에서 설명된 코드를 사용했지만 오른쪽에 장면 분류의 파이 차트를 표시하는 추가 기능을 추가했습니다. 이를 위해 각 장면 클래스의 개수를 반환하는 함수를 정의하고 시각화 버튼에서 사용해야 합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파트 6: Count 함수\n\n```js\ndef count_classified_pixels(data,num):\n    \n    scl_image = data[[\"scl\"]].isel(time=num).to_array()\n \n    # 분류된 픽셀 수 세기 \n    count_saturated = np.count_nonzero(scl_image == 1)        # 포화 또는 결함\n    count_dark = np.count_nonzero(scl_image == 2)             # 어두운 지역 픽셀\n    count_cloud_shadow = np.count_nonzero(scl_image == 3)     # 구름 그림자\n    count_vegetation = np.count_nonzero(scl_image == 4)       # 식물\n    count_soil = np.count_nonzero(scl_image == 5)             # 노출된 토양\n    count_water = np.count_nonzero(scl_image == 6)            # 물\n    count_clouds_low= np.count_nonzero(scl_image == 7)        # 낮은 확률 구름 / 분류되지 않은 구름\n    count_clouds_med = np.count_nonzero(scl_image == 8)       # 중간 확률 구름\n    count_clouds_high = np.count_nonzero(scl_image == 9)      # 높은 확률 구름\n    count_clouds_cirrus = np.count_nonzero(scl_image == 10)   # 시르러스 구름\n    count_clouds_snow = np.count_nonzero(scl_image == 11)     # 눈\n\n    counts = {\n    '어둠/밝은 영역': count_cloud_shadow +count_dark+count_clouds_low+count_clouds_med+count_clouds_high+count_clouds_cirrus +count_clouds_snow +count_saturated,\n    '식물': count_vegetation,\n    '노출된 토양': count_soil,\n    '물': count_water,\n    }\n\n    return counts\n```\n\n파트 7: 시각화 버튼\n\n```js\nif submit_button_viz:\n        \n        date_string_title= f\"타겟 지역의 Sentinel-2 이미지\"\n        fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))  \n\n        rgb = st.session_state.data[[\"red\", \"green\", \"blue\"]].isel(time=st.session_state.user_date_index).to_array()\n        rgb.plot.imshow(robust=True, ax=axs[0])\n        axs[0].axis('off')  # 축 눈금과 레이블 숨기기\n        axs[0].set_title(date_string_title)\n\n        # 데이터 준비 \n        counts = count_classified_pixels(st.session_state.data, st.session_state.user_date_index )\n        labels = list(counts.keys())\n        values = list(counts.values())\n        colors = ['DarkGrey', 'chartreuse', 'DarkOrange', 'cyan']\n        explode = (0.3, 0.1, 0.1, 0.1)  # 첫 번째 조각 튀어나오기\n\n        # 파이 차트 그리기 \n        axs[1].pie(values, labels=labels, colors=colors, autopct='%1.0f%%', startangle=140, explode=explode)\n        axs[1].legend(labels, loc='best', bbox_to_anchor=(1, 0.5))\n        axs[1].axis('equal')  # 파이 차트를 원으로 만들기\n        axs[1].set_title('클래스 분포')\n\n        # 그림을 Streamlit에 표시\n        st.pyplot(fig)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"Visualize\" 버튼을 클릭하면 코드가 관심 영역(AOI) 위의 Sentinel-2 이미지를 나타내는 제목이 있는 플롯을 설정합니다. 두 개의 서브플롯이 나란히 만들어집니다. RGB 이미지가 선택된 날짜의 세션 상태 데이터에서 추출되어 시각적 명확성을 높이기 위해 첫 번째 서브플롯에 축 레이블 없이 표시됩니다. 모든 버튼이 활성화된 상태에서 앱을 한 번 더 실행해 봅시다:\n\n![이미지](/assets/img/2024-06-23-CreatingaStreamlitAppforSatelliteImageryVisualizationAStep-by-StepGuide_6.png)\n\n![이미지](/assets/img/2024-06-23-CreatingaStreamlitAppforSatelliteImageryVisualizationAStep-by-StepGuide_7.png)\n\n구름 양, 날짜 및 좌표의 기본값을 변경하여 시각화에 어떻게 영향을 주는지 확인해 보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 📄 결론\n\n이 가이드에서는 지구상의 어떤 지점에서든 언제든 위성 이미지를 시각화하기 위한 Streamlit 애플리케이션을 만드는 과정을 안내했습니다. 우리는 앱을 안전하게 보호하기 위해 로그인 페이지를 설정한 후 사용자 입력을 수집하는 주 페이지를 디자인하는 것으로 시작했습니다. \"실행\" 버튼 기능을 구현하여 사용 가능한 위성 이미지를 검색하고, \"사용 가능한 이미지 목록\" 버튼을 클릭하면 사용 가능한 이미지의 드롭다운 메뉴가 표시되고, \"시각화\" 버튼을 클릭하여 선택한 위성 이미지를 표시할 수 있도록 했습니다. 이 인터랙티브한 방식은 사용자들이 코드를 커뮤니티와 쉽게 공유하고 위성 이미지를 효과적으로 탐색하고 다양한 시나리오를 탐구하여 인사이트를 얻을 수 있도록 합니다.\n\n## 📚 참고 자료\n\nhttps://github.com/stac-utils/pystac-client/blob/main/docs/quickstart.rst\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nhttps://www.element84.com/earth-search/examples/\n\nCopernicus Sentinel 데이터 [2024]에 대한 센티넬 데이터\n\nCopernicus 서비스 정보 [2024]에 대한 코퍼니커스 서비스 정보.\n\n📱더 많은 흥미로운 콘텐츠를 보기 위해 다른 플랫폼에서 저와 연결하세요! LinkedIn, ResearchGate, Github 및 Twitter.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 링크를 통해 확인할 수 있는 관련 게시물이 있습니다:","ogImage":{"url":"/assets/img/2024-06-23-CreatingaStreamlitAppforSatelliteImageryVisualizationAStep-by-StepGuide_0.png"},"coverImage":"/assets/img/2024-06-23-CreatingaStreamlitAppforSatelliteImageryVisualizationAStep-by-StepGuide_0.png","tag":["Tech"],"readingTime":16}],"page":"19","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"19"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>