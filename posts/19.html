<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/19" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/19" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="Python과 SQL로 데이터 마스터하기 4가지 전략적 사용 사례를 통한 효율성과 보안 강화" href="/post/2024-06-23-DataMasterywithPythonandSQLUnleashingEfficiencyandSecuritythrough4StrategicUseCases"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Python과 SQL로 데이터 마스터하기 4가지 전략적 사용 사례를 통한 효율성과 보안 강화" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-DataMasterywithPythonandSQLUnleashingEfficiencyandSecuritythrough4StrategicUseCases_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Python과 SQL로 데이터 마스터하기 4가지 전략적 사용 사례를 통한 효율성과 보안 강화" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Python과 SQL로 데이터 마스터하기 4가지 전략적 사용 사례를 통한 효율성과 보안 강화</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="SQL에서 Anti-Join과 Semi-Join 쉽게 이해하기" href="/post/2024-06-23-Anti-JoinSemi-JoininSQL"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="SQL에서 Anti-Join과 Semi-Join 쉽게 이해하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-Anti-JoinSemi-JoininSQL_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="SQL에서 Anti-Join과 Semi-Join 쉽게 이해하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">SQL에서 Anti-Join과 Semi-Join 쉽게 이해하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">1<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="PostgreSQL 테이블 파티셔닝 가이드 효율적인 데이터 관리 방법" href="/post/2024-06-23-GuidetoPostgreSQLTablePartitioning"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="PostgreSQL 테이블 파티셔닝 가이드 효율적인 데이터 관리 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-GuidetoPostgreSQLTablePartitioning_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="PostgreSQL 테이블 파티셔닝 가이드 효율적인 데이터 관리 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">PostgreSQL 테이블 파티셔닝 가이드 효율적인 데이터 관리 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ChatGPT를 이용한 플레이어 이탈 예측 방법" href="/post/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ChatGPT를 이용한 플레이어 이탈 예측 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ChatGPT를 이용한 플레이어 이탈 예측 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">ChatGPT를 이용한 플레이어 이탈 예측 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">21<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="지금 바로 탐험하세요 AI가 지원하는 PDF 채팅 동반자 AskToPDF" href="/post/2024-06-23-ExploreAskToPDFnowYourAI-poweredPDFChatCompanion"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="지금 바로 탐험하세요 AI가 지원하는 PDF 채팅 동반자 AskToPDF" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-ExploreAskToPDFnowYourAI-poweredPDFChatCompanion_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="지금 바로 탐험하세요 AI가 지원하는 PDF 채팅 동반자 AskToPDF" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">지금 바로 탐험하세요 AI가 지원하는 PDF 채팅 동반자 AskToPDF</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="SQL로 MLflow 모델 구축하기 머신러닝 라이프사이클 관리 쉽게 하는 방법" href="/post/2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="SQL로 MLflow 모델 구축하기 머신러닝 라이프사이클 관리 쉽게 하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="SQL로 MLflow 모델 구축하기 머신러닝 라이프사이클 관리 쉽게 하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">SQL로 MLflow 모델 구축하기 머신러닝 라이프사이클 관리 쉽게 하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="IPL 통계 분석을 위한 고급 SQL 쿼리 완벽 가이드" href="/post/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="IPL 통계 분석을 위한 고급 SQL 쿼리 완벽 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="IPL 통계 분석을 위한 고급 SQL 쿼리 완벽 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">IPL 통계 분석을 위한 고급 SQL 쿼리 완벽 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">18<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Snowflake의 미래 부여가 결국 실패할 이유" href="/post/2024-06-23-WhyyourSnowflakefuturegrantswilleventuallybreak"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Snowflake의 미래 부여가 결국 실패할 이유" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-WhyyourSnowflakefuturegrantswilleventuallybreak_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Snowflake의 미래 부여가 결국 실패할 이유" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Snowflake의 미래 부여가 결국 실패할 이유</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 늪에서 데이터 마스터까지 Apache Iceberg와 함께하는 레이크하우스 혁명 " href="/post/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 늪에서 데이터 마스터까지 Apache Iceberg와 함께하는 레이크하우스 혁명 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 늪에서 데이터 마스터까지 Apache Iceberg와 함께하는 레이크하우스 혁명 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 늪에서 데이터 마스터까지 Apache Iceberg와 함께하는 레이크하우스 혁명 </strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="2024년에 데이터 엔지니어가 되기 위한 조건은 무엇일까" href="/post/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="2024년에 데이터 엔지니어가 되기 위한 조건은 무엇일까" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="2024년에 데이터 엔지니어가 되기 위한 조건은 무엇일까" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">2024년에 데이터 엔지니어가 되기 위한 조건은 무엇일까</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><a class="link" href="/posts/1">1</a><a class="link" href="/posts/2">2</a><a class="link" href="/posts/3">3</a><a class="link" href="/posts/4">4</a><a class="link" href="/posts/5">5</a><a class="link" href="/posts/6">6</a><a class="link" href="/posts/7">7</a><a class="link" href="/posts/8">8</a><a class="link" href="/posts/9">9</a><a class="link" href="/posts/10">10</a><a class="link" href="/posts/11">11</a><a class="link" href="/posts/12">12</a><a class="link" href="/posts/13">13</a><a class="link" href="/posts/14">14</a><a class="link" href="/posts/15">15</a><a class="link" href="/posts/16">16</a><a class="link" href="/posts/17">17</a><a class="link" href="/posts/18">18</a><a class="link posts_-active__YVJEi" href="/posts/19">19</a><a class="link" href="/posts/20">20</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"Python과 SQL로 데이터 마스터하기 4가지 전략적 사용 사례를 통한 효율성과 보안 강화","description":"","date":"2024-06-23 16:46","slug":"2024-06-23-DataMasterywithPythonandSQLUnleashingEfficiencyandSecuritythrough4StrategicUseCases","content":"\n\n\n![Data Mastery with Python and SQL: Unleashing Efficiency and Security through 4 Strategic Use Cases](/assets/img/2024-06-23-DataMasterywithPythonandSQLUnleashingEfficiencyandSecuritythrough4StrategicUseCases_0.png)\n\n# 소개\n\n데이터 분석과 관리는 현대 기업 운영의 필수 구성 요소입니다. 데이터의 힘을 효과적으로 이용하기 위해 전문가들은 프로그래밍 언어와 도구의 조합을 활용하여 효율적인 데이터 처리, 조작 및 분석을 실현합니다. 이 기사에서는 데이터 분석가와 과학자들이 효과적인 의사 결정을 내리기 위해 널리 사용되는 두 가지 기본 언어인 Python과 SQL의 놀라운 기능을 탐색합니다.\n\nPython은 다양한 데이터 관련 도전 과제에 대처하기 위한 다양한 라이브러리와 프레임워크를 제공한다는 사실은 널리 알려져 있습니다. Python과 SQL은 함께 강력한 조합을 이루어 데이터 전문가들이 데이터의 최대 잠재력을 발휘하고 데이터베이스를 보다 효과적으로 탐색할 수 있도록 지원합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 Python과 SQL의 효과와 시너지를 보여주는 네 가지 구별된 사용 사례를 살펴봅니다. 각 사용 사례는 Python의 유연성과 SQL의 질의 능력이 결합된 능력이 돋보이는 고유한 시나리오를 대표합니다.\n\n자세히 알아보겠습니다!\n\n# 사용 사례\n\n사용 사례 1: Python으로 작성된 SQL 쿼리의 가독성 향상\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n웹 API를 사용하여 GridDB와 같은 클라우드 데이터베이스에 연결하고 SQL 쿼리를 실행하여 데이터를 검색하는 상황을 상상해보세요. JSON 페이로드를 수용하는 API 엔드포인트를 위한 HTTP 요청 본문을 작성할 때 SQL 쿼리를 요청 본문에 포함해야 합니다. 그러나 실제로는 다양한 어려움이 있을 수 있습니다.\n\n실제로 사용되는 SQL 쿼리는 점점 복잡해지며 적절한 들여쓰기, 줄 바꿈을 포함하고 코드를 읽기 쉽도록 형식화하는 것이 어려워질 수 있습니다. 또한 VScode 또는 Jupyter와 같은 Python 노트북에서 쿼리를 한 줄로 작성해야 하는 경우, 코드 기능을 설명하는 유용한 주석 행을 추가하는 것이 불가능해집니다. 이러한 쿼리가 포함된 노트북은 장기적으로 유지 및 디버깅하기 어려워집니다.\n\n아래 솔루션은 여러 줄에 걸쳐 SQL 쿼리를 작성할 수 있게 해 주어 코드의 가독성과 유지보수성을 향상시킵니다. 적절한 줄 바꿈과 들여쓰기를 사용하여 복잡한 쿼리를 쉽게 구성하고 이해할 수 있으며 명확성을 희생시키지 않고 처리할 수 있습니다.\n\n여기 Python에서 SQL 쿼리를 작성하는 더 좋은 방법이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsql_query1 = (f\"\"\"\n-- 지역, 카테고리 및 판매 등급별로 판매 데이터를 분류하는 쿼리\n\nSELECT region,\n    CASE WHEN lower(category) LIKE 'a%' THEN 'Category A'\n         WHEN lower(category) LIKE 'b%' THEN 'Category B'\n         WHEN lower(category) LIKE 'c%' THEN 'Category C'\n         ELSE '기타 카테고리'\n    END AS category_classification,\n    CASE WHEN subquery.total_sales BETWEEN 1 AND 1000 THEN '낮은 판매량' \n         WHEN subquery.total_sales BETWEEN 1001 AND 5000 THEN '중간 판매량'\n         WHEN subquery.total_sales \u003e 5000 THEN '높은 판매량'\n    END AS sales_classification\nFROM Sales_Dataset\nJOIN (\n    SELECT region, SUM(sales) AS total_sales\n    FROM Sales_Dataset\n    GROUP BY region\n) AS subquery\nON Sales_Dataset.region = subquery.region\nGROUP BY 1, 2\nORDER BY 3 DESC\n\"\"\")\n```\n\n그런 다음 아래와 같이 파이썬의 JSON 라이브러리를 사용하여 요청 본문에 전달할 수 있습니다 -\n\n```js\nimport json\n# 요청 본문 작성\nrequest_body = json.dumps([\n    {\n        \"type\": \"sql-select\",\n        \"stmt\": sql_query1\n    }\n])\n\n# 작성된 요청 본문 유효성 검사\nprint(request_body)\n```\n\nJSON 물체를 생성하며, 해당 물체에는 작업 유형(\"sql-select\")과 SQL 쿼리문(stmt)가 포함됩니다. json.dumps() 함수를 사용하여 파이썬 사전을 JSON 문자열 표현으로 변환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로 requests 라이브러리를 사용하여 요청을 게시할 수 있습니다.\n\n```js\ndata_req1 = requests.post(url, data=request_body, headers=header_obj)\n```\n\n## 사용 사례 2: 텍스트 열에서 해시태그 추출\n\nTikTok와 Instagram과 같은 플랫폼의 소셜 미디어 분석을 수행할 때, API를 통해 데이터를 추출하고 Azure나 Redshift와 같은 데이터베이스에 저장하는 것이 일반적입니다. 그러나 API 응답에는 종종 콘텐츠가 문자열로 제공되며, 해시태그가 비디오 제목 전체에 흩어져 있는 경우가 많습니다. 이를 해결하기 위해 다음 쿼리를 사용하여 비디오 제목과 같은 텍스트 열에서 해시태그를 추출할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sql\nselect * from \n(SELECT distinct TRIM(SPLIT_PART(title, '#', num)) AS hashtag\nFROM social_media_video_info\nCROSS JOIN (\n    SELECT 1 AS num UNION ALL\n    SELECT 2 AS num UNION ALL\n    SELECT 3 AS num UNION ALL\n    SELECT 4 AS num UNION ALL\n    SELECT 5 AS num UNION ALL\n    SELECT 6 AS num UNION ALL\n    SELECT 7 AS num UNION ALL\n    SELECT 8 AS num UNION ALL\n    SELECT 9 AS num UNION ALL\n    SELECT 10 AS num\n) AS nums\nWHERE num \u003c= LENGTH(title) - LENGTH(REPLACE(title, '#', '')) + 1\n  AND TRIM(SPLIT_PART(title, '#', num)) \u003c\u003e ''\n) \nwhere hashtag not like '% %'\n```\n\n하위 쿼리는 다음 단계를 수행합니다:\n\n- 'social_media_video_info' 테이블의 'title' 열을 '#' 문자로 구분자로 사용하여 분할합니다.\n- SPLIT_PART(title, '#', num) 함수는 지정된 \"num\" 위치에서 '#'로 구분된 \"title\" 열의 일부를 추출합니다.\n- TRIM() 함수는 추출된 부분에서 선행 또는 후행 공백을 제거합니다.\n- DISTINCT 키워드는 고유한 해시태그만 선택됨을 보장합니다.\n- 부하 쿼리 \"nums\"와의 CROSS JOIN은 숫자 1에서 10까지를 가진 임시 결과 집합을 생성합니다.\n- 조건 num `= LENGTH(title) — LENGTH(REPLACE(title, ‘#’, ‘’)) + 1은 \"title\" 열의 해시태그 최대 수에 따라 분할이 이루어짐을 보장합니다.\n- 조건 TRIM(SPLIT_PART(title, ' # ', num)) `` ‘’는 빈 해시태그를 필터링합니다.\n- 요약하면 쿼리는 'social_media_video_info' 테이블의 'title' 열에서 '#'를 구분자로 사용하여 해시태그를 추출합니다. 고유하며 비어있지 않은 해시태그만 선택하며 결과에서 공백이 포함된 해시태그는 제외합니다. 이 쿼리는 제목 당 최대 10개의 해시태그만 고려합니다.\n\n## Use Case 3: Python에서 미래 및 폐기 경고 억제하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 몇 줄의 코드는 프로그램 실행 중 미래 경고와 사용이 중단된 경고를 억제하는 데 목적이 있습니다.\n\n```js\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning) \n```\n\n현재 라이브러리 버전과 호환되는 코드를 사용하고 있고 잠재적인 문제나 사용이 중단된 기능에 대해 알림을 받고 싶지 않은 경우 유용할 수 있습니다.\n\n경고는 종종 라이브러리의 미래 버전에서의 잠재적인 문제나 변경 사항에 대한 유용한 정보를 제공합니다. 경고가 발생하는 근본적인 문제를 완전히 무시하는 대신에 그에 대응하고 해결하는 것이 일반적으로 권장됩니다. 'warnings' 모듈은 파이썬 코드에서 어떻게 경고가 처리되는지 제어할 수 있는 simplefilter() 옵션도 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 사용 사례 4: 가능한 경우 매개변수화된 쿼리 사용\n\nPython을 사용하여 SQL 쿼리를 실행할 때, SQL 문에 값을 직접 포함하는 대신 매개변수화된 쿼리나 준비된 문장을 사용하는 것이 좋습니다. 이렇게 함으로써 SQL 인젝션 공격을 방지하고 데이터 유형을 올바르게 처리할 수 있습니다.\n\n만약 당신의 애플리케이션이나 스크립트가 아래의 select 쿼리를 사용한다고 가정해봅시다 -\n\n```js\nSELECT * FROM \nTABLE \nWHERE \nCOLUMN1 IN ('abcd')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 공격자가 데이터베이스에 악의적인 값을 삽입하려고 한다면, 그들은 다음 쿼리를 사용하여 이를 수행할 수 있는 구멍을 이용할 수 있습니다. 아래는 공격자가 '선택 쿼리'에 삽입 문을 추가하여 데이터베이스에 불필요한 값들을 주입하는 방법의 기본 예시입니다. 아래 회색 부분은 공격자가 제공한 악의적인 입력으로, 결과적으로 한 번에 2개의 쿼리가 실행되는 것입니다 - 1. 선택하고 2. 삽입하다.\n\n![이미지](/assets/img/2024-06-23-DataMasterywithPythonandSQLUnleashingEfficiencyandSecuritythrough4StrategicUseCases_1.png)\n\n'테이블 삽입'에만 해당하는 것은 아닙니다. 공격자는 다른 선택, 업데이트, 삭제 또는 심지어 테이블 삭제를 실행할 수 있습니다. 'DROP TABLE'이 얼마나 재앙을 초래할 수 있는지 상상해보세요!\n\nSQL 주입은 입력을 살균화하거나 매개변수화함으로써 방지할 수 있습니다. 각각에 대한 자세한 내용을 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 매개변수화\n\n매개변수화는 수동으로 또는 파이썬에서 사용할 수 있는 패키지를 통해 수행할 수 있습니다. 설문 응용 프로그램을 사용한다고 상상해보세요. 사용자가 채우기 위해 설문 조사를 보내는 상황입니다. 사용자들은 제공된 두 개의 텍스트 상자에 세부 정보를 입력하도록 요청받습니다. 사용자가 두 텍스트 상자에 세부 정보를 입력할 때마다 아래와 같이 백그라운드에서 Insert SQL 쿼리가 실행된다고 가정해 봅시다 -\n\ntheVal1 = 설문 텍스트 상자 1에서 가져옴\ntheVal2 = 설문 텍스트 상자 2에서 가져옴\n\n아래는 백그라운드에서 실행되는 응용 프로그램 코드입니다 -\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n  sql = \"INSERT INTO TABLE VALUES ('\" + theVal1 + \"','\" + theVal2 + \"')\"\n```\n\n만약 1번 사용자가 텍스트 상자 1에 A3를 입력하고 텍스트 상자 2에 A4를 입력한다면, 백엔드에서 실행되는 쿼리는 다음과 같을 것입니다 -\n\n```js\nINSERT INTO TABLE VALUES ('A3','A4')\n```\n\n만약 두 번째 사용자가 해커라면 굉장히 교활한 사용자일 겁니다. 이 사용자는 테이블 구조와 백엔드 쿼리를 이해하고 있다면, 이를 악의적으로 이용하여 추가적인 레코드를 삽입할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사용자가 텍스트 상자 1에 값 A1을 입력하고 텍스트 상자 2에 다음 값을 입력한다고 가정해 봅시다.\n\n```js\nA2 ');INSERT INTO TABLE VALUES ('B1','B2\n```\n\n본질적으로 발생하는 일은 값이 백엔드 쿼리에 추가되어 아래와 같이 됩니다.\n\n```js\nINSERT INTO TABLE VALUES ('A1','A2');INSERT INTO TABLE VALUES ('B1','B2')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서, 이 해킹 사용자에 의해 2개의 레코드가 삽입될 것입니다.\n\n당신의 테이블은 세 개의 값이 있을 것입니다. 첫 번째 사용자가 삽입한 값 1개와 두 번째 사용자가 삽입한 값 2개 -\n\n![image](/assets/img/2024-06-23-DataMasterywithPythonandSQLUnleashingEfficiencyandSecuritythrough4StrategicUseCases_2.png)\n\n## 입력 살균 처리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n산소화는 입력 값의 특수 문자를 이스케이핑하여 수행할 수 있습니다. 이 작업은 대상 컨텍스트(예: SQL 쿼리)에서 특별한 의미를 가지는 문자를 해당하는 이스케이프 표현으로 대체하거나 인코딩하는 것을 포함합니다. 예를 들어 SQL에서는 작은따옴표(') 문자가 흔히 두 번 반복하여 이스케이프됩니다(그것을 대체함으로써 문자열에서 작은따옴표를 2개로 대체). 다시 말해, 쿼리에 입력 값을 넣기 전에 값을 수동으로 이스케이프할 수 있습니다. 이를 위해 str.replace를 사용할 수 있습니다.\n\n응용 프로그램 코드는 동일한 상태로 유지하면서 다음에 보여지는 몇 개의 문자열 대체 문을 추가합니다 -\n\ntheVal1 = 설문조사 텍스트상자1에서 가져옴\ntheVal2 = 설문조사 텍스트상자2에서 가져옴\n\n```js\nescapedVal1 = theVal1.replace(\"'\", \"''\")\nescapedVal2 = theVal2.replace(\"'\", \"''\")\nsql = \"INSERT INTO TABLE VALUES ('\" + escapedVal1 + \"','\" + escapedVal2 + \"')\"\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해커가 악성 레코드를 삽입하려고 시도할 때 사용자의 삽입문과 함께 삽입이 됩니다. 아래와 같이 보일 거에요 -\n\n```js\nINSERT INTO TABLE VALUES ('A1','A2'');INSERT INTO TABLE VALUES (''B1'',''B2')\n```\n\n테이블에 삽입된 값은 아래와 같을 거에요 -\n\n![이미지](/assets/img/2024-06-23-DataMasterywithPythonandSQLUnleashingEfficiencyandSecuritythrough4StrategicUseCases_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서 백엔드 테이블에서 사용자 중 한 명이 삽입문을 실행하려고 시도한 사실을 확인할 수 있을 것입니다. 입력 변수를 이스케이핑하는 것만으로 SQL 인젝션을 효과적으로 막았습니다.\n\n더 나은 방법은 psycopg2, pyodbc, sqlite3 또는 SQLAlchemy와 같은 Python 라이브러리를 사용하는 것입니다. 이러한 SQL 어댑터들은 다른 기능들과 함께 매개변수화된 쿼리를 지원하는 내장 기능을 갖추고 있습니다.\n\n# 마무리\n\n본 문서에서는 SQL 쿼리를 다룰 때 Python 프로그래밍 기술을 향상시키기 위한 네 가지 실용적인 사용 사례를 살펴보았습니다. Python으로 작성된 SQL 쿼리의 가독성을 향상시키기 위한 Use Case 1부터 시작하여 쿼리 포맷팅과 들여쓰기와 같은 기술을 활용함으로써 코드를 더 체계적이고 이해하기 쉽도록 만들 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nUse Case 2로 넘어가면서, 텍스트 열에서 해시태그를 추출하는 방법에 대해 살펴보았습니다. SQL 및 문자열 조작 함수를 활용하여 관련 해시태그를 효과적으로 추출하고 데이터 분석 프로세스를 강화하는 방법을 배웠습니다.\n\nUse Case 3에서는 Python에서 미래 및 폐기 경고 메시지를 억제하는 중요성을 다뤘습니다. `warnings` 모듈을 활용하여 더 깨끗한 Python 출력과 오류 없는 코드 실행을 보장할 수 있어, 불필요한 방해요소와 호환성 문제를 피할 수 있습니다.\n\n마지막으로, Use Case 4에서는 보안을 강화하고 성능을 개선하며 SQL 인젝션 공격을 방지하기 위해 처리된 코드와 매개변수화된 쿼리를 사용하는 중요성을 강조했습니다.\n\n이러한 사용 사례를 이해하고 구현함으로써 Python 개발자와 데이터 분석가는 SQL 쿼리 실행 및 최적화 기술을 향상시켜 훨씬 견고하고 효율적인 코드를 작성할 수 있습니다. 실제 상황에서 이러한 기술을 적용하면 더 깨끗한 워크플로우를 만들고 효율적이고 안전하게 실행 가능한 통찰력을 도출할 수 있을 것입니다.","ogImage":{"url":"/assets/img/2024-06-23-DataMasterywithPythonandSQLUnleashingEfficiencyandSecuritythrough4StrategicUseCases_0.png"},"coverImage":"/assets/img/2024-06-23-DataMasterywithPythonandSQLUnleashingEfficiencyandSecuritythrough4StrategicUseCases_0.png","tag":["Tech"],"readingTime":9},{"title":"SQL에서 Anti-Join과 Semi-Join 쉽게 이해하기","description":"","date":"2024-06-23 16:45","slug":"2024-06-23-Anti-JoinSemi-JoininSQL","content":"\n\n## Anti-Join 및 Semi-Join 이해하기 - 예제와 함께\n\n### Anti-Join\n\nAnti-Join은 테이블 A에 있는 행 중 테이블 B에 없는 행을 얻는 경우입니다.\n\n예를 들어, 주문을 한 번도 하지 않은 고객을 식별하려면 Anti-Join을 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 테이블 A를 고객이고 테이블 B를 주문으로 고려해 봅시다.\n\n```js\nSELECT c.customer_id, c.customer_name\nFROM customers c\nLEFT JOIN orders o\non c.customer_id = o.customer_id\nWHERE o.customer_id IS NULL;\n```\n\n# Semi-Join\n\n세미 조인은 테이블 B에서 조건이 일치하는 경우에만 테이블 A에서 행을 반환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, 주문을 한 고객만 식별하고 싶다면 이 세미 조인을 활용할 수 있습니다.\n\n```js\nSELECT customer_id\nFROM orders \nWHERE customer_id IN (SELECT customer_id FROM customers);\n```\n\n# 요약\n\n- Anti-Join: 두 번째 테이블에 일치하는 행이 없는 첫 번째 테이블의 행을 검색합니다.\n- Semi-Join: 두 번째 테이블에서 적어도 일치하는 행이 하나 이상 있는 첫 번째 테이블의 행을 검색합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n행복한 학습되세요! 화이팅!\n\n만약 제 글이 도움이 되셨다면, 클랩 버튼을 눌러서 지지를 보여주시고 나중을 위해 글을 저장하는 것을 잊지 마세요.","ogImage":{"url":"/assets/img/2024-06-23-Anti-JoinSemi-JoininSQL_0.png"},"coverImage":"/assets/img/2024-06-23-Anti-JoinSemi-JoininSQL_0.png","tag":["Tech"],"readingTime":1},{"title":"PostgreSQL 테이블 파티셔닝 가이드 효율적인 데이터 관리 방법","description":"","date":"2024-06-23 16:43","slug":"2024-06-23-GuidetoPostgreSQLTablePartitioning","content":"\n\n\n![2024-06-23-GuidetoPostgreSQLTablePartitioning](/assets/img/2024-06-23-GuidetoPostgreSQLTablePartitioning_0.png)\n\nPostgreSQL는 강력한 오픈 소스 관계형 데이터베이스 관리 시스템으로 대규모 및 복잡한 데이터 세트를 관리하기 위한 다양한 고급 기능을 제공합니다. 이 중 하나가 테이블 파티셔닝 기능입니다. 이 기능을 사용하면 대규모 테이블을 더 작고 관리하기 쉬운 파티션으로 나눌 수 있습니다.\n\n# 테이블 파티셔닝이란?\n\n테이블 파티셔닝은 대규모 테이블을 더 작고 관리하기 쉬운 청킹된 파티션으로 나누는 데이터베이스 디자인 기술입니다. 각 파티션은 본질적으로 원본 데이터의 하위 집합을 저장하는 별도의 테이블입니다. 이 기술을 사용하면 대규모 데이터 세트에 대한 쿼리 성능과 데이터 관리를 크게 향상시킬 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파티셔닝은 날짜 열이나 값 범위와 같은 하나 이상의 열을 기준으로 수행될 수 있습니다. 예를 들어, 레코드의 날짜를 기반으로 테이블을 파티션할 수 있으며, 각 파티션은 특정 날짜 범위의 데이터를 나타냅니다. 데이터를 쿼리할 때 PostgreSQL은 쿼리에 관련이 없는 파티션을 빠르게 제거하여 빠른 쿼리 실행을 가능하게 합니다.\n\n# 테이블 파티셔닝의 장점\n\n- 향상된 쿼리 성능: 파티셔닝을 통해 데이터베이스가 데이터를 특정 파티션으로 빠르게 좁힐 수 있어 쿼리 중 스캔해야 하는 데이터 양이 줄어들어 더 빠른 쿼리 실행 시간을 가능하게 합니다, 특히 대규모 데이터 집합의 경우에는 특히 유용합니다.\n- 쉬운 데이터 관리: 테이블 파티셔닝을 통해 대규모 데이터 집합을 더 작고 관리하기 쉬운 파티션으로 분할하여 쉽게 관리할 수 있습니다. 데이터 아카이빙, 데이터 삭제, 백업 및 복원 작업과 같은 작업을 단순화할 수 있습니다.\n- 향상된 데이터 로딩 및 인덱싱: 파티션된 테이블에 데이터를로드할 때 프로세스를 병렬화할 수 있어 더 빠른 데이터 삽입이 가능합니다. 또한 파티션된 테이블에있는 인덱스는 더 효율적일 수 있으며 더 작은 데이터 하위 집합만 다루면 되기 때문에 데이터 처리 속도가 빨라질 수 있습니다.\n- 비용 효율적인 스토리지: 파티셔닝을 통해 오래된 데이터나 덜 액세스되는 데이터를 더 저렴한 스토리지 미디어에 저장할 수 있으며, 자주 액세스되는 데이터는 더 빠른 스토리지 장치에 유지할 수 있습니다.\n\n# PostgreSQL에서의 파티셔닝 방법\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPostgreSQL는 다음과 같은 다양한 분할 방법을 제공합니다:\n\n- 범위 분할 (Range Partitioning)\n- 목록 분할 (List Partitioning)\n- 해시 분할 (Hash Partitioning)\n\n## 범위 분할 (Range Partitioning)\n\n범위 분할은 특정 열의 지정된 값 범위를 기반으로 데이터를 분할하는 테이블 분할의 일종입니다. 이는 시계열 데이터나 자연적인 순서를 갖는 데이터를 다룰 때 유용합니다. 각 파티션은 고유한 값 범위를 나타내며, 그 범위 내에 속하는 데이터는 해당 파티션에 저장됩니다. 범위 분할을 사용하면 특정 범위 내의 데이터를 효율적으로 검색할 수 있어 쿼리 성능이 향상됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음과 같은 구조로 sales 테이블의 예제를 고려해 봅시다.\n\n\nCREATE TABLE sales (\n    sale_id SERIAL PRIMARY KEY,\n    sale_date DATE,\n    product_id INT,\n    quantity INT,\n    amount NUMERIC\n) partition by range (sale_date);\n\n\nsale_date 열을 기준으로 한 판매 데이터에 대한 범위 분할 테이블을 생성하기 위해서는 다음 단계를 따라해야 합니다:\n\n파티션 생성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 날짜 범위를 나타내는 개별 테이블을 만들 것입니다. 데모를 위해 \"sales_january,\" \"sales_february,\" 그리고 \"sales_march\" 세 개의 파티션을 만들 것입니다.\n\n```js\nCREATE TABLE sales_january PARTITION OF sales\n    FOR VALUES FROM ('2023-01-01') TO ('2023-02-01');\n\nCREATE TABLE sales_february PARTITION OF sales\n    FOR VALUES FROM ('2023-02-01') TO ('2023-03-01');\n\nCREATE TABLE sales_march PARTITION OF sales\n    FOR VALUES FROM ('2023-03-01') TO ('2023-04-01');\n```\n\n제약 설정하기\n\n각 파티션에 제약 조건을 정의하여 데이터가 올바른 파티션으로 라우팅되도록 보장해야 합니다. 이 예제에서는 각 파티션의 sale_date 열에 대해 CHECK 제약 조건을 사용할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nALTER TABLE sales_january ADD CONSTRAINT sales_january_check\n    CHECK (sale_date \u003e= '2023-01-01' AND sale_date \u003c '2023-02-01');\n\nALTER TABLE sales_february ADD CONSTRAINT sales_february_check\n    CHECK (sale_date \u003e= '2023-02-01' AND sale_date \u003c '2023-03-01');\n\nALTER TABLE sales_march ADD CONSTRAINT sales_march_check\n    CHECK (sale_date \u003e= '2023-03-01' AND sale_date \u003c '2023-04-01');\n```\n\n파티션에 데이터 삽입\n\n이제 sales 테이블에 데이터를 삽입할 수 있고, PostgreSQL은 sale_date를 기준으로 데이터를 적절한 파티션으로 자동으로 라우팅할 것입니다:\n\n```js\nINSERT INTO sales (sale_date, product_id, quantity, amount)\nVALUES ('2023-01-15', 101, 5, 100.00);\n\nINSERT INTO sales (sale_date, product_id, quantity, amount)\nVALUES ('2023-02-20', 102, 10, 200.00);\n\nINSERT INTO sales (sale_date, product_id, quantity, amount)\nVALUES ('2023-03-10', 103, 8, 150.00);\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파티션에서 데이터 조회하기\n\n데이터를 조회할 때, PostgreSQL은 WHERE 절을 기반으로 관련 파티션에만 자동으로 액세스합니다.\n\n```js\n-- 1월의 판매 데이터 검색\nSELECT * FROM sales WHERE sale_date \u003e= '2023-01-01' AND sale_date \u003c '2023-02-01';\n\n-- 2월의 판매 데이터 검색\nSELECT * FROM sales WHERE sale_date \u003e= '2023-02-01' AND sale_date \u003c '2023-03-01';\n\n-- 3월의 판매 데이터 검색\nSELECT * FROM sales WHERE sale_date \u003e= '2023-03-01' AND sale_date \u003c '2023-04-01';\n```\n\n이러한 쿼리는 적절한 파티션에만 액세스하므로 쿼리 성능이 향상됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# PostgreSQL에서의 List Partitioning\n\n리스트 파티셔닝은 PostgreSQL에서의 다른 종류의 테이블 파티셔닝 방법으로, 데이터가 특정 열의 값에 기반하여 파티션으로 분할되는 방식입니다. 값의 범위를 사용하는 범위 파티셔닝과 달리, 리스트 파티셔닝은 각 파티션에 대한 특정 값을 정의할 수 있게 합니다. 이 파티셔닝 기술은 데이터를 구별되고 서로 겹치지 않는 세트로 분류할 수 있는 경우에 유용합니다.\n\n다음과 같은 구조를 가진 제품 테이블의 예시를 살펴봅시다:\n\n```js\nCREATE TABLE products (\n    product_id SERIAL PRIMARY KEY,\n    category TEXT,\n    product_name TEXT,\n    price NUMERIC\n) partition by list(category);\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제품 데이터를 카테고리 열을 기반으로 한 테이블 분할 테이블을 만들기 위해서는 다음 단계를 따라야합니다:\n\n분할 생성\n\n각 분할을 나타내는 개별 테이블을 만들어야 합니다. 각 분할은 특정 카테고리의 제품을 커버하도록 설계됩니다. 데모를 위해 \"전자제품\", \"의류\", \"가구\" 세 가지 분할을 만들어 보겠습니다.\n\n```js\nCREATE TABLE electronics PARTITION OF products\n    FOR VALUES IN ('전자제품');\n\nCREATE TABLE clothing PARTITION OF products\n    FOR VALUES IN ('의류');\n\nCREATE TABLE furniture PARTITION OF products\n    FOR VALUES IN ('가구');\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제약 조건 설정\n\n리스트 분할은 특정 값에 기반을 두기 때문에 CHECK 제약 조건이 필요하지 않습니다. 그러나 적절한 테이블에 행을 추가하여 파티션을 올바르게 설정해야 합니다.\n\n파티션에 데이터 삽입\n\n이제 제품 테이블에 데이터를 삽입할 수 있으며, PostgreSQL은 카테고리에 따라 데이터를 자동으로 해당 파티션으로 라우팅합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nINSERT INTO products (category, product_name, price)\nVALUES ('Electronics', 'Smartphone', 500.00);\n\nINSERT INTO products (category, product_name, price)\nVALUES ('Clothing', 'T-Shirt', 25.00);\n\nINSERT INTO products (category, product_name, price)\nVALUES ('Furniture', 'Sofa', 800.00);\n```\n\n파티션에서 데이터 쿼리하기\n\n데이터를 쿼리할 때, PostgreSQL은 WHERE 절을 기반으로 관련 파티션에 자동으로 액세스합니다.\n\n```js\n-- 전자제품 제품 검색\nSELECT * FROM products WHERE category = 'Electronics';\n\n-- 의류 제품 검색\nSELECT * FROM products WHERE category = 'Clothing';\n\n-- 가구 제품 검색\nSELECT * FROM products WHERE category = 'Furniture';\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n포스트그리스큐엘에서는 목록 파티셔닝이란 열의 특정 값에 따라 데이터를 관리하고 쿼리하는 데 유용한 기술입니다. 카테고리나 기타 고유한 집합을 기준으로 데이터를 파티션으로 나누면, 목록 파티셔닝을 통해 빠른 데이터 검색과 효율적인 데이터 관리가 가능해집니다.\n\n# 해시 파티셔닝 PostgreSQL\n\n해시 파티셔닝은 PostgreSQL에서 사용되는 테이블 파티셔닝의 한 유형으로, 데이터를 지정한 열의 해시 값에 기반하여 파티션으로 나누는 방식입니다. 특정 값이나 범위를 사용하는 범위 또는 목록 파티셔닝과 달리, 해시 파티셔닝은 해시 함수를 사용하여 데이터를 파티션 간에 균일하게 분배합니다. 이 파티셔닝 기술은 데이터를 균등하게 분산시켜 부하 분산을 달성하려는 경우 유용합니다.\n\n주문 테이블의 구조를 가진 예시를 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nCREATE TABLE orders (\n    order_id SERIAL PRIMARY KEY,\n    order_date DATE,\n    customer_id INT,\n    total_amount NUMERIC\n) partition by hash(customer_id);\n```\n\n주문 데이터를 고객 ID 열을 기반으로 한 해시 파티션 테이블을 만들려면 다음 단계를 따라야 합니다.\n\n파티션 생성\n\n각 파티션을 나타내는 개별 테이블을 만들어야 합니다. 각 파티션은 특정 해시 값 범위를 나타냅니다. 예제로, 세 개의 파티션을 만들어 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\nCREATE TABLE orders_1 PARTITION OF orders\r\n    FOR VALUES WITH (MODULUS 3, REMAINDER 0);\r\n\r\nCREATE TABLE orders_2 PARTITION OF orders\r\n    FOR VALUES WITH (MODULUS 3, REMAINDER 1);\r\n\r\nCREATE TABLE orders_3 PARTITION OF orders\r\n    FOR VALUES WITH (MODULUS 3, REMAINDER 2);\r\n```\r\n\r\n이 예시에서는 HASH() 함수를 사용하여 customer_id 열의 해시 값에 기반하여 데이터를 분할해야 함을 지정합니다. MODULUS와 REMAINDER를 사용하여 분할의 수(이 경우 3)와 각 분할의 나머지 값을 지정합니다.\r\n\r\n분할에 데이터 삽입하기\r\n\r\n이제 주문 테이블에 데이터를 삽입하면 PostgreSQL이 customer_id의 해시 값을 기반으로 적절한 분할로 데이터를 자동으로 라우팅합니다:\r\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nINSERT INTO orders (order_date, customer_id, total_amount) \nVALUES ('2023-01-15', 101, 500.00);\n\nINSERT INTO orders (order_date, customer_id, total_amount) \nVALUES ('2023-02-20', 102, 600.00);\n\nINSERT INTO orders (order_date, customer_id, total_amount) \nVALUES ('2023-03-10', 103, 700.00);\n```\n\n파티션에서 데이터 조회하기\n\n데이터를 조회할 때 PostgreSQL은 customer_id의 해시 값에 기반하여 적절한 파티션에 자동으로 액세스합니다.\n\n```js\n-- customer_id 101에 대한 주문 검색\nSELECT * FROM orders WHERE customer_id = 101;\n\n-- customer_id 102에 대한 주문 검색\nSELECT * FROM orders WHERE customer_id = 102;\n\n-- customer_id 103에 대한 주문 검색\nSELECT * FROM orders WHERE customer_id = 103;\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPostgreSQL에서의 해시 파티셔닝은 지정한 열의 해시 값에 기반하여 데이터를 파티션 간에 고르게 분산시키는 유용한 기술입니다. 해시 함수를 활용하여 데이터를 균일하게 분산시키는 해시 파티셔닝은 부하 분산을 실현하고 쿼리 성능을 향상시킵니다.\n\nPostgreSQL 테이블 파티셔닝은 대규모 데이터 집합의 성능 및 관리를 현저히 향상시킬 수 있는 강력한 기능입니다. 데이터를 작은 파티션으로 나누어 쿼리 성능을 최적화하고 데이터 관리를 간소화하며 효율적인 데이터 로딩 및 색인화를 달성할 수 있습니다. 파티셔닝 전략을 설계할 때는 데이터와 쿼리 패턴을 고려하여 가장 적합한 파티셔닝 방법을 선택하세요. 올바른 구현으로 PostgreSQL에서 대규모 데이터 처리에 변화를 줄 수 있는 게임 체인저가 될 수 있습니다.\n\n파티셔닝 시작!\n\n다른 블로그도 살펴보세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n읽어주셔서 감사합니다!\n\n데이터, AI, 스타트업, 리더십, 글쓰기 및 문화에 관한 내용을 올립니다.\n\n다음 블로그도 기대해주세요!!","ogImage":{"url":"/assets/img/2024-06-23-GuidetoPostgreSQLTablePartitioning_0.png"},"coverImage":"/assets/img/2024-06-23-GuidetoPostgreSQLTablePartitioning_0.png","tag":["Tech"],"readingTime":9},{"title":"ChatGPT를 이용한 플레이어 이탈 예측 방법","description":"","date":"2024-06-23 16:39","slug":"2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT","content":"\n\n## 저 코드 머신 러닝 플랫폼을 활용한 데이터 과학 | ACTABLE AI\n\n![이미지](/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_0.png)\n\n# 소개\n\n게임 산업에서 기업들은 플레이어를 유치하는 것 뿐만 아니라 특히 인게임 마이크로 트랜잭션에 의존하는 프리투플레이 게임에서 가능한 한 오랫동안 유지시키려고 노력합니다. 이러한 마이크로 트랜잭션은 종종 인게임 화폐 구매를 포함하며, 플레이어가 진행 또는 사용자 정의를 위한 아이템을 획득하고 게임 개발을 지원합니다. 중단하는 플레이어 수를 나타내는 이탈률을 모니터링하는 것이 중요합니다. 이는 높은 이탈율은 수입 손실을 의미하며, 이는 개발자와 관리자의 스트레스 수준이 증가하게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사는 특정 모바일 앱에서 획득한 데이터를 기반으로 한 실제 데이터셋의 사용을 탐색하며, 사용자들이 플레이한 레벨에 중점을 둡니다. 기계 학습을 활용하여 기술 현장에서 중요한 역할을 하며 인공 지능(AI)의 기초를 형성한 이후, 기업들은 자신들의 데이터에서 가치 있는 통찰을 얻을 수 있습니다.\n\n하지만, 기계 학습 모델을 구축하는 것은 일반적으로 코딩과 데이터 과학 전문 지식이 필요하므로, 많은 사람들 및 자원이 부족한 소규모 기업들에게는 접근하기 어렵습니다. 이러한 도전에 대처하기 위해, 로우코드와 노코드 기계 학습 플랫폼이 나타나기 시작했으며, 기계 학습과 데이터 과학 과정을 간소화하여 방대한 코딩 지식이 필요하지 않도록 하는 것을 목표로 합니다. Einblick, KNIME, Dataiku, Alteryx, Akkio와 같은 플랫폼의 예시가 있습니다.\n\n이 기사에서는 하나의 로우코드 기계 학습 플랫폼을 사용하여 사용자가 게임을 중단할지 예측할 수 있는 모델을 훈련하는 방법을 다룹니다. 뿐만 아니라 결과 해석 및 모델 성능을 향상시키는 데 사용할 수 있는 기술에 대해 탐구합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사의 나머지 부분은 다음과 같이 구성되어 있습니다:\n\n- 플랫폼\n- 데이터셋\n- 탐색적 데이터 분석\n- 분류 모델 훈련\n- 모델 성능 향상\n- 새로운 특징 생성\n- 새로운 (개선된) 분류 모델 훈련\n- 생산 환경에 모델 배포\n- 결론\n\n# 플랫폼\n\n전체 공개 - 이 기사 작성 시점에 제가 Actable AI의 데이터 과학자인 사실을 알려드립니다. 따라서 이 기사에서는 해당 플랫폼을 사용할 예정입니다. 또한, 저는 ML 라이브러리에 새로운 기능을 구현하고 유지보수하는 일에 관여하고 있어서, 이 플랫폼이 실제 문제에 대해 어떻게 대응하는지 궁금했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n플랫폼은 전통적인 분류, 회귀 및 세분화 애플리케이션을 위한 여러 인기있는 머신 러닝 방법을 제공합니다. 시계열 예측, 감성 분석 및 인과 추론과 같은 일부 일반적이지 않은 도구도 이용할 수 있습니다. 또한, 결측 데이터를 보완할 수 있으며 데이터 세트의 통계를 계산하고(특성 간 상관 관계, 분산 분석(ANOVA) 등), 막대 차트, 히스토그램, 워드 클라우드와 같은 도구를 사용하여 데이터를 시각화할 수 있습니다.\n\nGoogle Sheets 애드온도 제공되어 스프레드시트 내에서 직접 분석과 모델 훈련을 할 수 있습니다. 다만, 이 애드온에서는 최신 기능이 지원되지 않을 수 있으니 참고 바랍니다.\n\n![image](/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_1.png)\n\n핵심 라이브러리는 GitHub에서 오픈 소스로 제공되며 AutoGluon 및 scikit-learn과 같은 잘 알려진 신뢰할 수 있는 프레임워크로 구성되어 있습니다. 이는 기존의 오픈 소스 솔루션을 활용하는 다른 관련 플랫폼과 유사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나, 이에 대한 질문이 생깁니다: 대부분의 도구들이 이미 사용 가능하고 무료로 제공되는데, 왜 이러한 플랫폼을 사용해야 하는지요?\n\n가장 중요한 이유는 이러한 도구들이 Python과 같은 프로그래밍 언어에 대한 지식이 필요하다는 것입니다. 일반적으로 코딩에 익숙하지 않은 사람은 사용하기 어렵거나 불가능할 수 있습니다. 따라서 이러한 플랫폼은 프로그래밍 명령어의 형태로가 아닌 GUI(그래픽 사용자 인터페이스) 형식으로 모든 기능을 제공하려고 합니다.\n\n더 경험이 많은 전문가들은 또한 시간을 절약할 수 있을 뿐만 아니라 쉽게 사용할 수 있는 그래픽 인터페이스를 통해 지원 도구와 기법의 정보를 제공할 수도 있습니다. 일부 플랫폼은 익숙하지 않았던 도구들을 제공하거나 데이터 작업 시 유용한 경고(예: 데이터 누출의 존재 - 모델이 볼 수 없는 데이터의 생산 환경에 배포될 때 사용할 수 없는 특징에 액세스할 수 있는 경우)를 제공할 수도 있습니다.  \n\n이러한 종류의 플랫폼을 사용하는 또 다른 이유는 모델을 실행할 하드웨어도 제공하기 때문입니다. 따라서 자신의 컴퓨터나 GPU(Graphical Processing Units)와 같은 구성 요소를 구매하고 유지 관리할 필요가 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터셋\n\n게임 회사가 제공한 데이터셋은 이곳에서 확인할 수 있으며 CC BY-SA-4 라이센스가 적용되어있어 적절한 크레딧이 제공된다면 공유 및 수정이 허용됩니다. 이 데이터셋은 789,879개의 행(샘플)을 가지고 있어 과적합과 같은 영향을 줄여줄 것으로 예상됩니다.\n\n이 데이터셋은 개인이 모바일 앱에서 플레이한 각 레벨에 관한 정보를 포함하고 있습니다. 예를 들어, 플레이한 시간, 플레이어가 레벨에서 승리했는지 패배했는지, 레벨 번호 등에 대한 정보가 있습니다.\n\n사용자 ID도 포함되어 있지만 원래 플레이어의 신원을 드러내지 않도록 익명화되었습니다. 일부 필드도 제거되었지만, 이 데이터셋은 이 기사에서 고려된 ML 플랫폼에서 제공된 도구가 플레이어의 이탈을 예측하는 데 유용할 수 있는지 확인하는 견고한 기초를 제공할 것으로 예상됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 기능의 의미는 다음과 같습니다:\n\n- Churn: 플레이어가 게임을 2주 이상 플레이하지 않았을 경우 '1', 그렇지 않으면 '0'\n- ServerTime: 레벨이 플레이된 서버의 타임스탬프\n- EndType: 레벨이 종료된 이유 ('승리'일 경우 주로 게임에서 이겼을 때, '패배'일 경우 게임에서 졌을 때)\n- LevelType: 레벨의 유형\n- Level: 레벨 번호\n- SubLevel: 하위 레벨 번호\n- Variant: 레벨 변형\n- Levelversion: 레벨 버전\n- NextCar: 사용되지 않음 (플랫폼이 한 라벨만 포함하는 기능을 처리하는 방법 확인용으로 포함됨)\n- AddMoves: 추가 이동 횟수\n- DoubleMana: 사용되지 않음 (플랫폼이 한 라벨만 포함하는 기능을 처리하는 방법 확인용으로 포함됨)\n- StartMoves: 레벨 시작 시 사용 가능한 이동횟수\n- ExtraMoves: 구매한 추가 이동 횟수\n- UsedMoves: 플레이어가 사용한 이동 횟수\n- UsedChangeCar: 사용되지 않음 (플랫폼이 한 라벨만 포함하는 기능을 처리하는 방법 확인용으로 포함됨)\n- WatchedVideo: 비디오를 시청했는지 여부, 추가 이동 제공\n- BuyMoreMoves: 플레이어가 추가 이동을 구매한 횟수\n- PlayTime: 레벨 플레이에 소요된 시간\n- Scores: 플레이어가 달성한 점수\n- UsedCoins: 레벨에서 사용된 총 코인 수\n- MaxLevel: 플레이어가 도달한 최대 레벨\n- Platform: 장치 유형\n- UserID: 플레이어의 ID\n- RollingLosses: 플레이어의 연속적인 패배 횟수\n\n# 탐색적 데이터 분석\n\n학습 전 첫 번째 단계는 탐색적 데이터 분석(EDA)을 통해 데이터를 이해하는 것입니다. EDA는 데이터를 요약, 시각화하고 주요 특성을 이해하는 데이터 분석 방법론입니다. 목표는 데이터로부터 통찰력을 얻고, 어떠한 패턴, 추세, 이상 현상 또는 존재할 수 있는 이슈(예: 결측값)를 식별하여 사용될 특성 및 모델을 확인하는 데 도움이 되는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주요 이유들을 확인해 보면서 레벨이 종료된 이유에 대해 살펴보겠습니다:\n\n![Image](/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_2.png)\n\n위 이미지를 보면 레벨 종료의 주된 이유(EndType으로 표시됨)는 플레이어가 게임에서 패배한 경우(63.6%)가 승리한 경우의 35.2%에 비해 더 많다는 것을 알 수 있습니다. 또한 UsedChangeCar 열은 모든 행에 동일한 값을 포함하고 있어 쓸모없어 보입니다.\n\n매우 중요한 점은 우리의 대상값이 매우 불균형하다는 것입니다. 처음 10,000행 중에서 63개의 샘플만이 (데이터의 0.6%) Churn 값이 1(즉, 플레이어가 이탈함)을 가지고 있다는 것입니다. 이것은 염두에 둘 필요가 있습니다. 왜냐하면 우리의 모델이 Churn에 대해 0의 값을 예측하는 데 매우 편향될 수 있기 때문입니다. 모델이 정확도와 같은 몇 가지 지표에 대해 매우 좋은 값을 얻을 수 있기 때문에, 이 경우에는 가장 일반적인 클래스를 선택하는 더미 모델이 99.4%의 정확도로 정답을 맞출 것입니다! 이에 대해 Baptiste Rocca와 Jason Brownlee의 두 훌륭한 기사에서 더 읽어보시기를 권합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아쉽게도 Actable AI는 SMOTE(합성 소수 샘플링 기법)를 통해 불균형 데이터를 처리하거나 클래스 가중치 또는 다른 샘플링 전략을 사용하는 방법을 아직 제공하지 않습니다. 이는 최적화를 위해 선택된 지표에 주의를 기울여야 한다는 것을 의미합니다. 위에서 언급한 대로, 정확도는 한 클래스의 샘플이 올바르게 레이블링되지 않아도 높은 비율을 달성할 수 있는 경우라면 최선의 선택이 아닐 것입니다.\n\n또 다른 유용한 분석 유형은 특징들 간의 상관 관계, 특히 예측자 특징과 대상 특징 간의 상관 관계입니다. 이를 수행하기 위해 '상관 분석' 도구를 사용할 수 있으며, 해당 결과는 Actable AI 플랫폼에서 직접 확인할 수 있습니다:\n\n![HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_3](/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_3.png)\n\n위 차트에서 파란 막대는 특징이 Churn과 양의 상관 관계가 있는 경우를 나타내며 값이 1인 경우이고, 주황색 막대는 음의 특징 상관 관계를 나타냅니다. 상관 관계는 -1에서 1 사이에 있음을 주의해야 합니다. 양의 값은 두 특징이 함께 변화하는 경향이 있다는 것을 나타내며(예: 둘 다 증가하거나 감소), 음의 상관 관계는 한 특징이 증가하거나 감소할 때 다른 특징이 반대로 변화한다는 것을 나타냅니다. 따라서 상관 관계의 크기(음의 부호를 무시한)가 아마도 가장 중요한 사항일 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특정 수준을 잃은 플레이어들이 (가장 위쪽의 파란 막대) 막적회전에 민감하다는 것이나, 반대로 특정 수준을 이긴 플레이어들은 계속해서 플레이하는 경향이 있다는 등 여러 가지 교훈이 있습니다 (세 번째 오렌지 막대). 그러나 값이 상당히 낮다는 것도 주목해야 합니다. 이는 이러한 특징이 목표와 상관 관계가 약한 것을 의미합니다. 이는 모델이 더 정확한 예측을 수행하기 위해 더 중요한 정보를 포착하는 새로운 기능을 만들어 사용하는 특성 엔지니어링을 수행해야 할 것으로 예상됩니다. 특성 엔지니어링은 이 글의 뒷부분에서 자세히 다룰 것입니다.\n\n하지만 새로운 특성을 생성하기 전에 데이터셋의 원래 특성만 사용하여 어떤 성능을 달성할 수 있는지 살펴보는 것도 좋은 방법입니다. 따라서 다음 단계는 더 흥미로운 것으로, 모델을 훈련시켜 어떤 성능을 달성할 수 있는지 알아보는 것이 될 것입니다.\n\n# 분류 모델 훈련\n\n사용자가 플레이를 중단할지 여부를 예측하고 싶기 때문에, 이는 여러 레이블 중 하나를 선택해야 하는 분류 문제입니다. 우리의 경우, 문제는 두 가지 레이블 중 하나('1'은 '이탈', '0'은 '이탈하지 않음'에 해당)를 할당하는 것을 포함하므로, 이는 이진 분류 문제로 만듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 프로세스는 주로 AutoGluon 라이브러리를 통해 수행되며, 이 라이브러리는 자동으로 여러 모델을 학습한 다음 가장 우수한 성능을 달성한 모델을 선택합니다. 이렇게 하면 각각의 모델을 수동으로 학습하고 그 성능을 비교할 필요가 없어집니다.\n\nActable AI 플랫폼에서 설정해야 할 여러 매개변수가 있으며, 제가 선택한 옵션은 아래에 나와 있습니다:\n\n![이미지](/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_4.png)\n\n모델의 최적화를 위해 사용할 메트릭도 선택할 수 있습니다. 저는 수신자 조작 특성 (ROC) 아래 영역 (AUC ROC) 곡선을 사용했습니다. 이는 이전에 논의된 클래스 불균형 문제에 대해 훨씬 민감하지 않기 때문입니다. 값은 0부터 1까지의 범위를 가지며 (1일수록 완벽한 점수입니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일정 시간이 지난 후에는 결과가 생성되어 표시되며, 여기서도 볼 수 있습니다. 여러 가지 다른 측정 항목이 계산되며, 이는 좋은 실천 방식일 뿐만 아니라 각 측정 항목이 모델 성능의 특정 측면에 집중하기 때문에 우리가 모델을 실제로 이해하려면 거의 필수적입니다.\n\n표시된 첫 번째 메트릭은 최적화 메트릭으로, 값이 0.675입니다:\n\n![이미지](/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_5.png)\n\n이것은 크게 좋지는 않지만, EDA 중에 특성이 대상과 상관성이 약한 것을 상기하면, 성능이 별로 두드러지지 않는 것은 놀라운 일이 아닙니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 결과는 결과를 이해하는 중요성을 강조합니다. 보통 0.997 (즉, 99.7%) 정확도에 대해 매우 만족스러워할 것입니다. 그러나 이는 앞서 언급한 것처럼 데이터 세트의 심각한 불균형 때문이 대부분이므로 그다지 중요하지 않습니다. 한편, 정밀도와 재현율과 같은 점수는 기본적으로 임계값 0.5를 기반으로 하며, 이는 우리의 응용 프로그램에 가장 적합하지 않을 수 있습니다.\n\nROC 및 정밀도-재현율 곡선도 표시되는데, 이것들 또한 성능이 약간 부족함을 명확히 보여줍니다:\n\n이러한 곡선들은 최종 응용 프로그램에서 사용할 임계값을 결정하는 데도 유용합니다. 예를 들어, 거짓 양성의 수를 최소화하려면 모델이 더 높은 정밀도를 얻는 임계값을 선택하고 해당하는 재현을 확인할 수 있습니다.\n\n얻은 최상의 모델에서 각 피처의 중요성을 확인할 수도 있습니다. 이는 AutoGluon을 통해 순열 중요성을 사용하여 계산됩니다. 결과의 신뢰성을 결정하기 위해 P-값도 표시됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_6.png\" /\u003e\n\n아마도 놀랍지 않게도, 가장 중요한 기능은 EndType입니다(레벨이 종료된 이유를 보여주는데, 승리나 패배와 같은 요인), 그 다음은 MaxLevel(사용자가 플레이 한 가장 높은 레벨로, 숫자가 높을수록 플레이어가 게임에 매우 집중하고 활발하다는 것을 의미합니다).\n\n반면에, UsedMoves(플레이어가 수행한 움직임의 수)는 사실상 쓸모없지만, StartMoves(플레이어가 사용할 수있는 움직임의 수)는 실제로 성능을 해칠 수 있습니다. 이것도 논리적입니다. 사용된 움직임 수와 플레이어가 사용할 수있는 움직임 수가 그 자체로는 높은 정보를 가지고 있지 않기 때문에, 두 값간의 비교가 훨씬 유용할 것입니다.\n\n또한 각 클래스(이 경우 1 또는 0)의 추정 확률을 살펴볼 수 있습니다. 이 확률은 예측된 클래스를 도출하기 위해 사용되며(기본적으로 가장 높은 확률을 가진 클래스가 예측된 클래스로 할당됩니다):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_7.png\" /\u003e\n\nAI의 설명 가능성은 모델 동작을 이해하는 데 더 중요해지고 있으며, 이로 인해 Shapley 값과 같은 도구가 인기를 끌고 있습니다. 이러한 값은 기능이 예측된 클래스의 확률에 미치는 기여를 나타냅니다. 예를 들어, 첫 번째 행에서 RollingLosses 값이 36인 경우, 해당 플레이어가 게임을 계속하는 클래스 (클래스 0)의 확률이 감소함을 볼 수 있습니다.\n\n반대로, 이는 다른 클래스(즉, 플레이어가 이탈하는 클래스 1)의 확률이 증가함을 의미합니다. 이는 RollingLosses 값이 높을수록 플레이어가 연속해서 많은 레벨을 잃었고, 따라서 좌절하여 게임을 그만 둘 가능성이 높다는 것을 의미합니다. 반면, RollingLosses 값이 낮을수록 부정적인 클래스(즉, 플레이어가 게임을 그만 두지 않을 가능성)의 확률이 일반적으로 향상됩니다.\n\n언급한 바와 같이 여러 모델이 훈련되고 평가된 후, 그 중에서 최적의 모델이 선택됩니다. 흥미로운 점은 이 경우 최고의 모델이 LightGBM임과 동시에 가장 빠른 모델 중 하나인 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마크다운 형식으로 테이블 태그를 변경해보세요.\n\n# 모델 성능 향상\n\n지금 이 시점에서는 모델의 성능을 향상시킬 수 있습니다. 아마도 가장 쉬운 방법 중 하나는 '품질 최적화' 옵션을 선택하고 얼마나 나아질 수 있는지 확인하는 것입니다. 이 옵션은 일반적으로 성능을 향상시키는 몇 가지 매개변수를 구성하며, 단단한 학습 시간의 비용으로 이루어집니다. 다음 결과를 얻었습니다 (여기에서도 확인할 수 있습니다):\n\n\u003cimg src=\"/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_9.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다시 ROC AUC 지표에 초점을 맞추면, 성능이 0.675에서 0.709로 향상되었습니다. 이렇게 간단한 변경으로 성능이 상당히 향상된 것입니다. 하지만 여전히 이상적인 수준에서는 멀리 떨어져 있습니다. 더 나은 성능을 위해 우리가 할 수 있는 다른 방법이 있을까요?\n\n# 새로운 기능 만들기\n\n이전에 논의한 대로, 이것을 특성 공학을 사용하여 수행할 수 있습니다. 이것은 기존 기능에서 새로운 기능을 만드는 것을 의미하며, 이러한 새로운 기능은 더 강력한 패턴을 포착하고 예측할 변수와 더 높은 상관 관계를 갖도록 할 수 있습니다.\n\n우리의 경우, 데이터 세트의 기능은 사용자가 플레이한 레벨에 관한 정보에 대한 값만을 갖기 때문에 범위가 상당히 좁습니다. 따라서, 시간을 거쳐 레코드를 요약함으로써 더 전반적인 전망을 얻는 것이 매우 유용할 수 있습니다. 이렇게 함으로써 모델은 사용자의 역사적인 추세에 대한 지식을 가질 수 있을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, 플레이어가 사용한 추가 움직임의 수를 확인하여 경험한 난이도를 측정할 수 있습니다. 추가 움직임이 거의 필요하지 않았다면, 레벨이 너무 쉬웠을 수도 있습니다. 반대로 많은 숫자는 레벨이 너무 어려웠을 수도 있습니다.\n\n또한, 플레이어가 게임에 몰입하고 참여하고 있는지 확인하기 위해 이전 몇 날 동안 게임을 플레이한 시간을 확인하는 것도 좋은 아이디어일 것입니다. 게임을 별로 하지 않았다면, 그들이 흥미를 잃고 곧 그만둘 수도 있다는 의미일 수 있습니다.\n\n유용한 특징들은 서로 다른 도메인에서 다양하기 때문에 현재 작업과 관련된 정보를 찾는 것이 중요합니다. 예를 들어, 연구 논문, 사례 연구 및 기사를 찾거나 해당 분야에서 일한 회사나 전문가들의 조언을 구하면 가장 일반적인 특징, 특징 간의 관계, 잠재적인 함정 및 유용할 것으로 예상되는 새로운 특징들에 대해 숙련된 지식이 있는 사람들로부터 도움을 받을 수 있습니다. 이러한 접근 방식은 시행착오를 줄이고 특징 엔지니어링 프로세스를 가속화하는 데 도움이 됩니다.\n\n최근 대규모 언어 모델(LLMs)의 발전과 (예: ChatGPT를 들어본 적이 있을지도 모르죠...), 그리고 특징 엔지니어링 프로세스가 경험 부족한 사용자에게는 다소 어려울 수 있기 때문에, LLMs가 어떤 특징을 생성할 수 있는 아이디어를 제공하는 데 어떤 지원이 될 수 있는지 궁금했습니다. 저는 이를 테스트해보았고, 다음 결과가 나왔습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_10.png\" /\u003e\n\n차트 GPT의 답변은 실제로 아주 좋습니다. 또한 앞서 언급한 바와 같이 여러 시간 기반 특성을 가리키기도 합니다. 물론 필요한 정보가 없을 경우 모든 제안된 기능을 구현할 수 없을 수도 있음을 염두에 두세요. 게다가 이것은 망상에 빠지기 쉬우며, 따라서 완전히 정확한 답변을 제공하지 못할 수도 있습니다.\n\nChatGPT로부터 더 관련성 높은 응답을 받으려면 사용 중인 특성을 지정하거나 프롬프트를 활용하는 등의 방법을 사용할 수 있습니다. 그러나 이는 본 문서의 범위를 벗어나므로 독자들에게 연습 과제로 남겨두겠습니다. 그럼에도 불구하고, LLMs는 시작 단계로 고려할 수 있지만, 논문, 전문가 등보다 신뢰할 만한 정보를 얻기 위해 노력하는 것이 강력히 권장됩니다.\n\nActable AI 플랫폼에서는 상당히 잘 알려진 SQL 프로그래밍 언어를 사용하여 새로운 기능을 만들 수 있습니다. SQL에 익숙하지 않은 사용자를 위해 ChatGPT를 활용하여 쿼리를 자동으로 생성하는 방법이 유용할 수 있습니다. 그러나 내가 제한된 실험을 통해 이 방법의 신뢰성은 다소 일관되지 않을 수 있다는 것을 발견했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n원하는 결과를 정확하게 계산하기 위해 의도한 출력물을 수동으로 확인하여 검증하는 것이 좋습니다. SQL Lab에서 쿼리를 실행한 후에 나타나는 테이블을 확인하여 이를 확인할 수 있습니다. Actable AI는 SQL 코드를 작성하고 실행하는 인터페이스입니다.\n\n다음은 새로운 열을 생성하는 데 사용한 SQL 코드입니다. 다른 기능을 만들고 싶다면 이것을 참고하여 시작할 수 있을 것입니다.\n\n```js\nSELECT \n    *,\n    SUM(\"PlayTime\") OVER UserLevelWindow AS \"time_spent_on_level\",\n    (a.\"Max_Level\" - a.\"Min_Level\") AS \"levels_completed_in_last_7_days\",\n    COALESCE(CAST(\"total_wins_in_last_14_days\" AS DECIMAL)/NULLIF(\"total_losses_in_last_14_days\", 0), 0.0) AS \"win_to_lose_ratio_in_last_14_days\",\n    COALESCE(SUM(\"UsedCoins\") OVER User1DayWindow, 0) AS \"UsedCoins_in_last_1_days\",\n    COALESCE(SUM(\"UsedCoins\") OVER User7DayWindow, 0) AS \"UsedCoins_in_last_7_days\",\n    COALESCE(SUM(\"UsedCoins\") OVER User14DayWindow, 0) AS \"UsedCoins_in_last_14_days\",\n    COALESCE(SUM(\"ExtraMoves\") OVER User1DayWindow, 0) AS \"ExtraMoves_in_last_1_days\",\n    COALESCE(SUM(\"ExtraMoves\") OVER User7DayWindow, 0) AS \"ExtraMoves_in_last_7_days\",\n    COALESCE(SUM(\"ExtraMoves\") OVER User14DayWindow, 0) AS \"ExtraMoves_in_last_14_days\",\n    AVG(\"RollingLosses\") OVER User7DayWindow AS \"RollingLosses_mean_last_7_days\",\n    AVG(\"MaxLevel\") OVER PastWindow AS \"MaxLevel_mean\"\nFROM (\n    SELECT\n        *,\n        MAX(\"Level\") OVER User7DayWindow AS \"Max_Level\",\n        MIN(\"Level\") OVER User7DayWindow AS \"Min_Level\",\n        SUM(CASE WHEN \"EndType\" = 'Lose' THEN 1 ELSE 0 END) OVER User14DayWindow AS \"total_losses_in_last_14_days\",\n        SUM(CASE WHEN \"EndType\" = 'Win' THEN 1 ELSE 0 END) OVER User14DayWindow AS \"total_wins_in_last_14_days\",\n        SUM(\"PlayTime\") OVER User7DayWindow AS \"PlayTime_cumul_7_days\",\n        SUM(\"RollingLosses\") OVER User7DayWindow AS \"RollingLosses_cumul_7_days\",\n        SUM(\"PlayTime\") OVER UserPastWindow AS \"PlayTime_cumul\"\n    FROM \"game_data_levels\"\n    WINDOW\n        User7DayWindow AS (\n            PARTITION BY \"UserID\"\n            ORDER BY \"ServerTime\"\n            RANGE BETWEEN INTERVAL '7' DAY PRECEDING AND CURRENT ROW\n        ),\n        User14DayWindow AS (\n            PARTITION BY \"UserID\"\n            ORDER BY \"ServerTime\"\n            RANGE BETWEEN INTERVAL '14' DAY PRECEDING AND CURRENT ROW\n        ),\n        UserPastWindow AS (\n        PARTITION BY \"UserID\"\n        ORDER BY \"ServerTime\"\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        )\n) AS a\nWINDOW\n    UserLevelWindow AS (\n        PARTITION BY \"UserID\", \"Level\"\n        ORDER BY \"ServerTime\"\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ),\n    PastWindow AS (\n        ORDER BY \"ServerTime\"\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ),\n    User1DayWindow AS (\n        PARTITION BY \"UserID\" \n        ORDER BY \"ServerTime\" \n        RANGE BETWEEN INTERVAL '1' DAY PRECEDING AND CURRENT ROW\n    ),\n    User7DayWindow AS (\n        PARTITION BY \"UserID\"\n        ORDER BY \"ServerTime\"\n        RANGE BETWEEN INTERVAL '7' DAY PRECEDING AND CURRENT ROW\n    ),\n    User14DayWindow AS (\n        PARTITION BY \"UserID\"\n        ORDER BY \"ServerTime\"\n        RANGE BETWEEN INTERVAL '14' DAY PRECEDING AND CURRENT ROW\n    )\nORDER BY \"ServerTime\";\n```\n\n이 코드에서 '윈도우'는 고려할 시간 범위를 정의하기 위해 생성되어 마지막 날, 지난 주 또는 지난 2주와 같은 것을 나타냅니다. 해당 범위 내에 속한 레코드가 기능 계산 중에 사용되며, 이는 주로 게임에서 플레이어의 여정에 대한 일부 역사적 맥락을 제공하기 위해 의도되었습니다. 전체 기능 목록은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- level_time_spend: 사용자가 레벨을 플레이하는 데 소요된 시간을 나타냅니다. 레벨의 난이도를 나타냅니다.\n- levels_completed_last_7_days: 사용자가 지난 7일 동안(1주일) 완료한 레벨 수를 나타냅니다. 난이도, 인내력 및 게임에 대한 몰입도를 보여줍니다.\n- total_wins_last_14_days: 사용자가 레벨을 이긴 총 횟수\n- total_losses_last_14_days: 사용자가 레벨을 진 총 횟수\n- win_to_lose_ratio_last_14_days: 승리 횟수와 패배 횟수의 비율 (total_wins_last_14_days/total_losses_last_14_days)\n- used_coins_last_1_days: 이전 날 사용된 코인 수. 레벨 난이도 및 플레이어가 게임 내 통화를 사용할 의지를 보여줍니다.\n- used_coins_last_7_days: 지난 7일간 사용된 코인 수 (1주일)\n- used_coins_last_14_days: 지난 14일간 사용된 코인 수 (2주)\n- extra_moves_last_1_days: 사용자가 이전 날에 사용한 추가 움직임 수. 레벨 난이도를 나타냅니다.\n- extra_moves_last_7_days: 사용자가 지난 7일 동안 사용한 추가 움직임 수 (1주)\n- extra_moves_last_14_days: 사용자가 지난 14일 동안 사용한 추가 움직임 수 (2주)\n- rolling_losses_mean_last_7_days: 사용자가 지난 7일 동안 누적으로 경험한 평균 손실 횟수 (1주). 레벨 난이도를 보여줍니다.\n- max_level_mean: 모든 사용자가 달성한 최고 레벨의 평균.\n- max_level: 사용자가 지난 7일 동안(1주) 달성한 최고 레벨. max_level_mean과 결합하여 플레이어의 다른 플레이어들에 대한 진행 상황을 보여줍니다.\n- min_level: 사용자가 지난 7일 동안(1주) 플레이한 최소 레벨\n- play_time_cumul_7_days: 사용자가 지난 7일 동안(1주) 플레이한 총 시간. 플레이어의 게임 몰입도를 나타냅니다.\n- play_time_cumul: 사용자가 플레이한 총 시간(첫 번째 기록부터)\n- rolling_losses_cumul_7_days: 지난 7일 동안(1주) 누적된 롤링 손실 총 횟수. 레벨의 난이도를 나타냅니다.\n\n새로운 기능의 값을 계산할 때는 이전 레코드만 사용하는 것이 중요합니다. 다시 말해, 향후 관측치의 사용은 피해야 합니다. 왜냐하면 모델이 상용 환경에 배포될 때 미래 값에 액세스할 방법이 없기 때문입니다.\n\n생성된 피처에 만족하셨다면 테이블을 새 데이터 세트로 저장하고 (희망적으로) 성능이 향상된 새 모델을 실행할 수 있습니다.\n\n# 새로운 (희망적으로 향상된) 분류 모델 훈련\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n새로운 열이 유용한지 확인할 시간이에요. 이전과 동일한 단계를 반복할 수 있어요. 유일한 차이점은 이제 추가 기능을 포함하는 새 데이터 세트를 사용한다는 것이에요. 기존 모델과 공정한 비교를 위해 동일한 설정을 사용하여 원본 모델과 다음 결과와 함께 최적화합니다(여기서도 확인할 수 있습니다):\n\n![image](/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_11.png)\n\n0.918의 ROC AUC 값은 원래 값인 0.675와 비교했을 때 크게 향상되었어요. 심지어 품질에 최적화된 모델(0.709)보다 더 나아요! 이는 데이터를 이해하고 더 풍부한 정보를 제공할 수 있는 새로운 기능을 만드는 중요성을 보여줍니다.\n\n이제 어떤 새로운 기능이 실제로 가장 유용했는지 확인하는 것이 흥미롭겠죠. 다시 한번 특징 중요도 표를 확인할 수 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![그림](/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_12.png)\n\n최근 두 주간의 총 패배 횟수가 매우 중요한 것으로 보입니다. 이는 게임에서 더 자주 패배할수록 플레이어가 좌절하고 플레이를 그만둘 가능성이 더 높아진다는 논리적입니다.\n\n모든 사용자의 평균 최대 레벨도 중요한 것으로 보입니다. 다시 한번, 플레이어가 다수의 다른 플레이어와 얼마나 멀리 떨어져 있는지를 결정하는 데 사용될 수 있습니다. 평균보다 훨씬 높은 level은 플레이어가 게임에 깊게 몰두해 있다는 것을 나타내고, 평균보다 훨씬 낮은 값은 플레이어가 여전히 충분히 동기 부여받지 못했을 수 있다는 것을 나타낼 수 있습니다.\n\n이것들은 우리가 만들었을 수 있는 몇 가지 간단한 기능에 불과합니다. 성능을 더 향상시킬 수 있는 다른 기능들이 있을 수 있습니다. 독자에게 어떤 다른 기능들이 만들어질 수 있는지 확인해 보도록 남겨두겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지난과 동일한 시간 제한으로 품질에 최적화된 모델을 훈련시키는 것은 성능을 향상시키지 않았습니다. 그러나 더 많은 피처를 사용하고 있기 때문에 최적화에 더 많은 시간이 필요할 수 있기 때문에 이해할 만한 일입니다. 여기서 시간 제한을 6시간으로 늘리면 성능이 실제로 0.923(AUC 기준)로 향상된 것을 확인할 수 있습니다:\n\n![이미지](/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_13.png)\n\n또한 정밀도 및 재현율과 같은 몇 가지 메트릭이 여전히 상당히 낮다는 점에 유의해야 합니다. 그러나 이는 0.5의 분류 임계값을 가정했기 때문일 수 있습니다. 임계값을 곡선을 클릭하여 변경할 수 있지만 AUC는 임계값에 독립적이며 성능을 더 포괄적으로 나타낼 수 있습니다. 앞서 언급했듯이 AUC는 불균형 데이터셋을 기반으로 훈련하는 동안 최적화 메트릭으로 사용될 때 특히 유용합니다.\n\n훈련된 모델의 AUC 성능은 다음과 같이 요약할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n|                         Model                           | AUC (ROC) |\n|--------------------------------------------------------|-----------|\n| Original features                                      |     0.675 |\n| Original features + optim. for quality                 |     0.709 |\n| Engineered features                                    |     0.918 |\n| Engineered features + optim. for quality + longer time |     0.923 |\n\n\n# 프로덕션 환경에 모델 배포하기\n\n새로운 데이터에서 실제로 모델을 사용할 수 없다면 좋은 모델이 있어도 쓸모가 없습니다. 머신러닝 플랫폼은 훈련된 모델을 사용하여 미래의 보이지 않는 데이터에 대한 예측을 생성하는 기능을 제공할 수 있습니다. 예를 들어, Actable AI 플랫폼은 API를 통해 모델을 사용하여 플랫폼 외부 데이터에 사용할 수 있게 해주며, 모델을 내보내거나 원시 값을 삽입하여 즉시 예측을 얻을 수 있습니다.\n\n그러나 모델을 주기적으로 미래 데이터에 테스트하는 것은 매우 중요합니다. 모델이 여전히 예상대로 작동하는지 확인하기 위해 필요합니다. 실제로 더 최신 데이터로 모델을 다시 훈련해야 할 수도 있습니다. 이는 특징(예: 피처 분포)이 시간이 지남에 따라 변경될 수 있어 모델의 정확도에 영향을 미칠 수 있기 때문입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, 회사에서 새로운 정책을 도입할 수 있으며 이는 고객 행동에 영향을 미칠 수 있습니다(긍정적 또는 부정적). 그러나 새로운 변화를 반영하는 기능이 없는 경우 모델은 새로운 정책을 고려하지 못할 수 있습니다. 만일 그런 심각한 변화가 있지만 모델에 정보를 제공할 수 있는 기능이 없다면, 고려해볼 가치가 있는 것은 두 모델을 사용하는 것입니다: 하나는 이전 데이터를 훈련시키고 사용하는 데에 특화되고, 다른 하나는 최신 데이터를 훈련시키고 사용하는 데에 특화하는 것입니다. 이렇게 함으로써 모델이 단일 모델로 잡기 어려운 서로 다른 특성을 가진 데이터에서 작동할 수 있도록 보장할 수 있습니다.\n\n# 결론\n\n본 글에서는 사용자가 모바일 앱에서 각 레벨을 하면서 생성된 정보를 포함하는 실제 데이터 세트를 사용하여, 플레이어가 2주 후에 게임을 그만둘지를 예측할 수 있는 분류 모델을 훈련시켰습니다.\n\n데이터 탐색부터 모델 훈련, 피처 엔지니어링까지 전반적인 처리 파이프라인을 고려했습니다. 결과의 해석에 대한 토론과 어떻게 향상시킬 수 있는지가 제공되었으며, 0.675에서 0.923으로 가치를 향상시키는 방법을 탐색했습니다(1.0이 최대값입니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n새로 만들어진 기능들은 비교적 간단합니다. 더 많은 기능들이 있을 수 있지만, Feature Normalisation과 Standardisation 같은 기술들 역시 고려해볼 만 합니다. 여기와 여기에서 유용한 자료들을 찾을 수 있습니다.\n\nActable AI 플랫폼에 관해서, 제가 조금 편협한 의견을 가지고 있을 수도 있지만, 이 플랫폼이 데이터 과학자와 머신 러닝 전문가에 의해 수행되어야 하는 좀 더 성가신 프로세스들을 간단화할 수 있다고 생각합니다. 다음과 같은 우수한 측면이 있습니다:\n\n- Core ML 라이브러리는 오픈 소스이기 때문에 좋은 프로그래밍 지식을 가진 사람이라면 안전하게 사용할 수 있습니다. 또한 Python을 알고 있는 사람이라면 누구나 사용할 수 있습니다.\n- Python을 모르거나 코딩에 익숙하지 않은 사람들을 위해 GUI를 통해 쉽게 분석 및 시각화를 할 수 있는 방법을 제공합니다.\n- 플랫폼을 사용하기 시작하는 것이 너무 어렵지 않습니다. 기술적인 정보가 너무 많아서 지식이 부족한 사람들이 사용을 꺼려할 정도로 사용자를 압도하지 않습니다.\n- 무료 계층을 통해 공개적으로 사용 가능한 데이터셋에 대한 분석을 실행할 수 있습니다.\n- 이 글에서 고려된 분류 이외에도 다양한 도구가 제공됩니다.\n\n그럼에도 불구하고, 개선할 부분들이 몇 가지 존재하며, 몇 가지 측면들이 개선되어야 할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 무료 티어는 개인 데이터에서 ML 모델을 실행할 수 없습니다.\n- 사용자 인터페이스가 다소 오래된 느낌이 있습니다.\n- 일부 시각화 요소가 명확하지 않고 해석하기 어려울 수 있습니다.\n- 어플리케이션이 가끔 반응이 느릴 수 있습니다.\n- 불균형 데이터를 지원하지 않습니다.\n- 이 플랫폼의 최대 잠재력을 발휘하려면 데이터 과학 및 기계 학습에 대한 일부 지식이 여전히 필요합니다 (다른 플랫폼에서도 마찬가지일 수 있습니다).\n\n다음 기사에서는 다른 플랫폼들을 사용하여 그들의 장단점을 파악하고, 각 플랫폼에 최적인 사용 사례를 확인할 것입니다.\n\n그 때까지 이 기사가 흥미로운 내용이었길 바랍니다! 피드백이나 질문이 있으시면 언제든지 자유롭게 남겨주세요!\n\n이 기사에 대한 생각이 있으신가요? LinkedIn에서 메시지를 보내거나 직접 연락 주십시오!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저를 팔로우해주세요! 이렇게 하면 미래의 기사 발행 알림을 받을 수 있습니다.\n\n![Image](/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_14.png)\n\n이 기사를 작성한 저자는 작성 시점에 Actable AI의 데이터 과학자였습니다.","ogImage":{"url":"/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_0.png"},"coverImage":"/assets/img/2024-06-23-HowtoPredictPlayerChurnwithSomeHelpFromChatGPT_0.png","tag":["Tech"],"readingTime":21},{"title":"지금 바로 탐험하세요 AI가 지원하는 PDF 채팅 동반자 AskToPDF","description":"","date":"2024-06-23 16:38","slug":"2024-06-23-ExploreAskToPDFnowYourAI-poweredPDFChatCompanion","content":"\n\n문서 상호 작용의 혁명적인 여정을 시작하세요. AskToPDF는 PDF 문서와 상호 작용하는 방법을 간소화하기 위해 설계된 새로운 AI 애플리케이션입니다. 세심한 개발과 정제 작업을 거친 끝에, 저는 AskToPDF가 모든 사람들에게 www.asktopdf.com에서 사용 가능하다는 것을 기쁘게 알려드립니다!\n\n최초 제공 기간 동안 무료로 제공되며, 또한 데이터베이스 채팅을 위한 새로운 AI 애플리케이션을 계획 중입니다. 아이디어는 데이터베이스나 스키마에 연결한 다음 자연 언어로 데이터베이스와 대화를 시작하는 것입니다. 이 새로운 generateSQL 애플리케이션에 대한 더 많은 업데이트를 받으시려면 저를 팔로우해주세요. 개발에 사용된 모든 기술/전략에 관련된 이야기를 게시할 예정입니다.\n\nAskToPDF란 무엇인가요?\n\nAskToPDF는 문서를 더 이상 전에 없던 방식으로 찾아볼 수 있도록 하는 혁신적인 AI 기반 PDF 채팅 앱입니다. 고급 언어 모델 (LLM) 기술을 기반으로, AskToPDF를 사용하면 어떤 PDF 문서든 업로드하고 쉽게 정보를 조회할 수 있습니다. 강의 노트를 검토하는 학생, 프로젝트 보고서를 재방문하는 전문가 또는 문서 내용에 대해 궁금한 사용자라면 AskToPDF가 도움을 줄 준비가 되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-23-ExploreAskToPDFnowYourAI-poweredPDFChatCompanion_0.png)\n\nAskToPDF는 어떻게 작동하나요?\n\nAskToPDF는 첨단 검색 증강 생성(RAG) 기술을 활용하여 업로드된 PDF를 심층적으로 이해합니다. 한 번 업로드하면 문서와 관련된 질문을 입력하기만 하면 AskToPDF가 나머지를 처리해줍니다. 저희 지능형 시스템은 문서를 스캔하여 관련 정보를 검색하고 실시간으로 간결하고 정확한 답변을 생성합니다.\n\nAskToPDF를 선택하는 이유는 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 효율성: 모든 페이지를 읽지 않고도 긴 문서에서 즉각적인 답변을 얻을 수 있습니다.\n- 정확성: 문서 내용에서 직접 유도된 정확한 답변을 얻을 수 있습니다.\n- 접근성: 사용자 친화적인 웹 인터페이스를 통해 언제 어디서나 AskToPDF에 접속할 수 있습니다.\n\n지금 AskToPDF 커뮤니티에 가입하세요!\n\nAskToPDF를 통해 문서 상호작용의 미래를 경험해보세요. 학자, 전문가, 또는 단순히 궁금한 사람이라면, AI 기술을 활용한 답변 시스템은 PDF 탐색을 원활하고 생산적으로 만들어줍니다.\n\n지금 www.asktopdf.com을 방문하여 AskToPDF가 문서 탐색 경험을 어떻게 변화시킬지 살펴보세요. AskToPDF를 통해 쉽고 빠르게 질문하고 답변을 얻는 즐거움을 느껴보시죠!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI를 활용하여 문서를 탐색하는 힘을 발휘해보세요 - 오늘 AskToPDF를 시도해보세요!","ogImage":{"url":"/assets/img/2024-06-23-ExploreAskToPDFnowYourAI-poweredPDFChatCompanion_0.png"},"coverImage":"/assets/img/2024-06-23-ExploreAskToPDFnowYourAI-poweredPDFChatCompanion_0.png","tag":["Tech"],"readingTime":2},{"title":"SQL로 MLflow 모델 구축하기 머신러닝 라이프사이클 관리 쉽게 하는 방법","description":"","date":"2024-06-23 16:37","slug":"2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement","content":"\n\n## MLflow 생태계에 SQL 모델을 통합하는 단계별 안내서\n\n![이미지](/assets/img/2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement_0.png)\n\n## 소개\n\n기계 학습의 끊임없이 진화하는 풍경에서, 모델의 끝-끝 수명주기를 원할하게 관리해야 하는 필요성이 중요해졌습니다. 이러한 복잡한 프로세스를 단순화하는 오픈 소스 플랫폼인 MLflow가 나타납니다. 이 포괄적인 안내서에서는 SQL 기반 모델과 MLflow의 기능을 융합하는 과정을 탐색할 것입니다. 우리의 주요 목표는 두 가지입니다: 첫째, 간단한 SQL 기반 모델을 사용하여 MLflow의 기본 원리를 실습적으로 이해하는 것이며, 둘째로, MLflow의 모델 저장소 내에 SQL 쿼리를 캡슐화하는 흥미로운 도전에 대응하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## SQL 기반 모델의 중요성\n\nSQL(구조화된 쿼리 언어) 기반 모델은 현실 세계 비즈니스 시나리오에서 독특하게 중요하며 순위 매기기, 추천 시스템 및 데이터 필터링 작업에서 중추적 역할을 합니다. SQL 기반 모델이 중요한 역할을 하는 몇 가지 추가 도메인을 살펴보겠습니다:\n\n- 재고 관리: SQL 데이터베이스는 재고 수준, 재주문 점 및 공급망 데이터를 추적합니다. SQL 쿼리는 재고 수준을 모니터링하고 재고 보충 알림을 생성하며 재고 순환을 최적화하는 데 도움을 줍니다.\n- 고객 세분화 및 타겟 마케팅: 마케터들은 SQL 기반 모델을 활용하여 고객 베이스를 세분화하고 타겟 마케팅 캠페인을 설계합니다. 이러한 모델은 고객 인구 통계, 구매 이력 및 온라인 행동을 분석하여 특정 선호도를 갖는 고객 세그먼트를 식별함으로써 맞춤형 마케팅 전략을 구현할 수 있습니다.\n- 공급망 최적화: 복잡한 공급망을 관리하는 기업들은 SQL 모델을 사용하여 재고 수준을 최적화하고 물류를 최적화하며 비용을 최소화합니다. SQL 쿼리는 공급업체 성과, 수요 예측 및 생산 일정을 분석하여 적시에 납품하고 효율적인 운영을 보장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## MLflow이란 무엇인가요?\n\nMLflow는 Databricks에서 개발한 오픈 소스 플랫폼으로, 기계 학습 수명주기의 end-to-end 관리를 용이하게 합니다. 실험 추적, 코드를 재현 가능한 실행으로 패키징하고 모델을 공유하고 배포하는 도구를 제공하며, 모든 것을 통합된 프레임워크 내에서 처리합니다. 데이터 과학자, 기계 학습 엔지니어, 또는 비즈니스 분석가이든, MLflow는 기계 학습 프로젝트를 구성하고 협력하는 구조화된 방식을 제공합니다.\n\nMLflow의 주요 구성 요소:\n\n- 추적: MLflow의 추적 구성 요소를 사용하면 중요한 메트릭, 매개 변수 및 다른 실행에 연결된 아티팩트를 기록하고 모니터링할 수 있습니다.\n- 프로젝트: MLflow의 프로젝트 기능을 사용하면 코드, 종속성 및 환경 사양을 재사용 가능한 형식으로 패키징할 수 있습니다. 이를 통해 실험을 재현 가능하게 하여, 개발에서 제품화로의 전환 시에 일관성이 깨지지 않도록 할 수 있습니다.\n- 모델: 훈련된 기계 학습 모델의 모음입니다. 서로 다른 모델을 나타내는 서로 다른 책을 개인 서재로 상상해보세요. MLflow는 여러분이 이러한 모델을 패키징, 구성 및 관리할 수 있게 해주어 나중에 쉽게 찾아서 사용할 수 있습니다.\n- 모델 레지스트리: 모델 레지스트리를 정리된 서재로 생각해보세요. 여러분이 훈련된 기계 학습 모델의 서로 다른 버전을 저장하고 관리할 수 있는 곳입니다. 여러분이 좋아하는 책의 다른 버전이 있는 책장이 있는 것처럼 생각해보세요. MLflow의 모델 레지스트리는 모델이 시간이 지남에 따라 진화하는 것을 추적하여 필요할 때 쉽게 찾아서 사용할 수 있도록 해줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMLflow은 scikit-learn 및 TensorFlow와 같은 다양한 모델 라이브러리를 조화롭게 통합하여 배포를 최적화하고 모델 알고리즘 변경에 대한 우려를 덜어주는 통합 플랫폼으로 빛난다. MLflow를 사용하면 기업은 다양한 알고리즘 실험을 통해 일정한 배포 프로세스를 준수하는 동시에 유연성을 얻을 수 있습니다. 이 플랫폼은 라이브러리 간의 차이를 추상화하여 개발에서 배포로의 수동 변환 없이 원활한 전환을 가능하게 합니다. 이 유연성은 알고리즘 잠금을 회피하고 새로운 혁신에 적응하며, 배포 파이프라인을 중앙 집중화하여 유지 관리 부담을 줄입니다.\n\n![이미지](/assets/img/2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement_1.png)\n\n## Mflow에 SQL 기반 모델 통합하는 이유\n\nMLflow는 Python 모델 및 TensorFlow 아티팩트를 관리하는 데 뛰어나지만, 모델 컨텍스트 내에서 SQL 쿼리를 수용하는 것은 원래 지원되지 않습니다. 이 분리는 종종 기관이 SQL 기반 및 Python 모델을 배포하기 위해 별도의 시스템을 유지하도록 강요하여 유지 보수 노력과 추적 불일치가 증가하게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해당 도전을 극복하기 위해, 우리는 파이썬 함수 내에서 SQL 쿼리의 논리를 캡슐화하는 천재적인 전략을 곧 발표할 예정이에요. 이를 통해, 우리는 SQL 기반 모델을 MLflow의 생태계에 원활하게 통합하여 효율적인 저장, 버전 관리, 배포를 가능하게 합니다.\n\n이 통합은 전통적인 기계 학습 알고리즘을 넘어 플랫폼의 능력을 확장하여, 데이터 과학 작업에서 SQL 쿼리의 파워를 활용하려는 기관들에게 가치 있는 자산이 됩니다. 이는 모델 라이프사이클 관리에 대한 통합된 접근 방식을 제공하며, 동일한 프레임워크 내에서 SQL 및 Python 기반 모델을 효율적으로 결합하여, 데이터 분석 및 모델 개발에 SQL 전문 지식을 필요로 하는 기관에 대응합니다. 이는 MLflow의 유틸리티를 확장하여 더 넓은 범위의 모델링 작업을 처리하고, 모델 개발 라이프사이클 전반에 걸쳐 개선된 효율성과 협업을 촉진합니다.\n\n## 단계별 구현\n\n문제 진단: 에어비앤비를 고려해봅시다. 웹 사이트에서 사용자 경험을 향상시키기 위해, 에어비앤비는 숙박 정보의 랭킹 모델을 최적화하고자 합니다. 고급 기계 학습 알고리즘으로 진입하기 전에, 그들은 SQL 기반 모델을 벤치마크로 구축하고자 합니다. 주요 목표는 사용자가 특정 지역에 대한 요청을 제출할 때, 웹 사이트의 리스트를 평균 리뷰 점수를 기반으로 순위를 매기는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 차례로 수행할 단계입니다:\n\n1. 모의 데이터 생성: 먼저 Airbnb 리스트 데이터를 모방하는 모의 데이터 세트를 생성할 것입니다. 이 데이터 세트에는 리스트 ID, 지역, 리뷰 점수, 생성 날짜 및 방의 수가 포함됩니다. 이 데이터 생성을 통해 모델에 대한 대표적인 데이터 세트로 작업할 수 있게 됩니다.\n\n방금 생성한 데이터 세트를 간략히 살펴보겠습니다:\n\n![데이터세트](/assets/img/2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. 랭킹 함수 정의하기: 지역별로 목록을 랭킹하는 함수를 정의할 거에요. 이 함수는 지역(지역)을 입력으로 받아와서 SQL 쿼리를 사용하여 리뷰 점수를 기반으로 내림차순으로 필터링하고 정렬할 거에요.\n\n함수에 대해 빠르게 테스트를 해 보겠습니다:\n\n3. MLflow를 위한 함수 랩핑: 이 랭킹 함수를 MLflow에 통합하기 위해서는 MLflow의 규칙을 따라 랩핑할 필요가 있어요. mlflow.pyfunc.PythonModel을 상속하는 RankingModel이라는 파이썬 클래스를 만들 거에요. 이 클래스에는 스파크 세션을 초기화하는 predict 메서드가 포함되며 입력에서 지역을 추출하고 랭킹 함수를 호출할 거에요.\n\n4. 모델 테스트: RankingModel이 예상대로 작동하는지 확인하기 위해 빠른 테스트를 실행할 거에요. 이 테스트에서 모델의 인스턴스를 만들고 모델 입력(지역)을 정의하고 predict 메서드를 호출하여 순위가 매겨진 목록을 얻을 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4. 모델 등록: 성공적인 테스트 후, 앞으로 사용할 모델을 MLflow에 등록합니다. 이 단계에는 MLflow 런을 정의, 테스트 데이터로 predict() 메서드를 호출, predict 함수의 시그니처 추론, 모델 아티팩트를 MLflow에 로깅, 그리고 모델 레지스트리에 모델 등록이 포함됩니다.\n\n등록된 모델을 살펴보겠습니다.\n\n![등록된 모델](/assets/img/2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement_3.png)\n\n이제 등록 후에 다른 노트북이나 콘텍스트에서 이 모델을 로드하고 해당 지역을 입력으로 예측 메서드를 호출하면 그 지역에 대한 리뷰 점수별로 순위가 매겨진 목록 ID가 반환됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5. 등록된 모델 로드 및 사용하기: 마지막으로 등록된 모델을 다른 컨텍스트나 노트북에서 로드하고 예측에 사용하는 방법을 보여드리겠습니다. 이를 통해 모델을 다양한 애플리케이션에 원활하게 통합할 수 있습니다.\n\n이러한 단계를 따르면 SQL 기반 모델을 성공적으로 구축했고, MLflow와 통합했으며, 다양한 데이터 기반 하위 응용 프로그램에서 사용할 수 있게 될 것입니다.\n\n## 결론\n\n마지막으로, 우리는 SQL 모델을 성공적으로 작성하고 MLflow에 통합했습니다. 이 모델은 이제 등록되어 다른 서비스의 엔드포인트로 제공되어 리뷰 점수 기반 순위 목록에 대한 확장 가능하고 효율적인 솔루션으로 사용할 준비가 되었습니다. 이 접근 방식은 MLflow의 다양한 유형의 모델을 통합하여 통일된 프레임워크 내에서 다루는 다양성을 보여주며, Python 기반 모델에서 SQL 기반 모델로의 능력을 확장합니다. 전체 노트북은 여기에서 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사가 유익하고 재미있었으면 좋겠습니다. 좋아요로 감사를 표현해 주시고, 댓글로 피드백을 공유해 주세요.\n\n# 추가 정보\n\n제가 작성한 PySpark 튜토리얼 컬렉션을 소개합니다. 이 튜토리얼은 PySpark의 다양한 측면을 마스터하는 데 도움이 되도록 설계되었습니다. 제가 다음 기사에서 우선적으로 다루기를 원하는 구체적인 주제나 기술이 있으면 자유롭게 제안해 주세요. 여러분의 피드백은 귀중합니다. 만약 이 PySpark 튜토리얼이 유익하고 도움이 되었다면, 더 깊은 내용의 컨텐츠를 제공하는 Medium에서 저를 팔로우하시기를 권장합니다. PySpark의 세계로의 여정을 즐기세요! 즐거운 학습과 코딩되세요!","ogImage":{"url":"/assets/img/2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement_0.png"},"coverImage":"/assets/img/2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement_0.png","tag":["Tech"],"readingTime":6},{"title":"IPL 통계 분석을 위한 고급 SQL 쿼리 완벽 가이드","description":"","date":"2024-06-23 16:34","slug":"2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics","content":"\n\u003cimg src=\"/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_0.png\" /\u003e\n\n# 소개:\n\n상상해보세요. BCCI(인도크리켓통제위원회)에서 IPL 2150의 데이터 분석가로 고용되었다고 가정해 봅시다. 네, 2150 년에도 데이터 분석가는 여전히 높은 수요가 있고, AI가 아직 모든 일자리를 대체하지는 않았습니다. 누가 생각했겠습니까? 아마도 AI는 여전히 크리켓의 규칙을 이해하려고 노력 중일지도 모릅니다! 그런데, 이 프로젝트에서는 2150 년 자료가 제공되지 않습니다. 그래서 신경 쓰지 마세요.\n\n당신의 팀 매니저가 IPL 시즌 전체 기록을 포함하는 여러 CSV 파일을 손에 쥐고 여러분에게 접근합니다. 그들은 여러분에게 포괄적인 분석을 수행하고 이 데이터를 Postgres(RDBMS)로 이전하여 팀 내에서 더 효율적인 데이터 관리를 요청합니다.\n모든 데이터 집합과 마찬가지로, 도메인 지식은 데이터 분석가가 효과적으로 데이터 분석을 수행하는 데 중요합니다. IPL 크리켓에 익숙하지 않다면, 분석을 진행하기 전에 데이터 집합의 열을 먼저 살펴봄으로써 도메인 지식을 얻는 것이 좋습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터 세트:\n\nCSV 파일에는 아래 그림에 표시된 6개의 테이블이 포함되어 있으며, 이를 pgAdmin (postgreSQL의 RDMS)에서 다음 데이터베이스 스키마처럼 변환해야 합니다.\n실제 시나리오에서 기업은 일반적으로 CSV에서 SQL 데이터베이스로의 전환보다 DBMS(데이터베이스 관리 시스템)를 직접 사용합니다. 그러나 우리의 SQL 프로젝트 목적으로 이 전환이 수행되었습니다.\n\n\u003cimg src=\"/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_1.png\" /\u003e\n\n제약 조건:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 주어진 스키마에 포함된 기본 키 (Primary Key)와 외래 키 (Foreign Key) 제약 조건\n- out_type은 'caught', 'caught and bowled', 'bowled', 'stumped', 'retired hurt', 'keeper catch', 'lbw', 'run out', 'hit wicket', 또는 NULL(실제 null이 아닌 문자열 형태) 값만 가질 수 있습니다.\n- role_desc는 'Player', 'Keeper', 'CaptainKeeper' 또는 'Captain' 값만 가질 수 있습니다.\n- toss_name은 'field' 또는 'bat' 값만 가질 수 있습니다.\n- win_type은 'wickets', 'runs', 또는 NULL(실제 null이 아닌 문자열 형태) 값만 가질 수 있습니다.\n- ball_by_ball 테이블의 runs_scored 값은 0에서 6 사이여야 합니다.\n- ball_by_ball 테이블의 innings_no 값은 1 또는 2만 가능합니다.\n\n# 분석적 질문 및 해결책:\n\n자세한 분석을 위해 팀 매니저가 다음 작업을 할당하고 명확한 경로를 제시했습니다.\n\nQ 1: 주어진 데이터베이스 스키마에 따라 CSV 파일을 생성하고, 모든 제약 조건과 테이블 간 관계가 올바르게 반영되도록 하고, 그 후 pgAdmin에 가져오세요. (다른 RDBMS를 사용하는 경우, 모든 쿼리에 대해 구문을 조정하십시오)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해결책: 지정된 데이터베이스 스키마와 일치하도록 필요한 테이블을 적절한 제약 조건과 관계와 함께 생성하고 해당 CSV 파일에서 데이터를 가져옵니다.\n\n```js\n-- 해결책 1:\n-- 테이블을 생성할 때 위의 데이터베이스 스키마에 따라\n-- 필요한 제약 조건 및 관계 키를 추가해주세요\n\n-- venue 테이블 생성\ncreate table if not exists venue(\n venue_id int,\n venue_name varchar(50) not null,\n city_name varchar(50) not null,\n country_name varchar(50) not null,\n constraint pk_venue_venue_id primary key (venue_id)\n);\n-- venue.csv 파일에서 값 가져오기\ncopy venue\nfrom 'D:\\Downloads\\A Portfolio Projects\\SQL Projects\\IPL Analysis\\Dataset CSV\\venue.csv'\ndelimiter ','\ncsv header;\n\n-- team 테이블 생성\ncreate table if not exists team(\n team_id int,\n team_name varchar(50) not null,\n constraint pk_team_team_id primary key(team_id)\n);\n-- team.csv 파일에서 값 가져오기\ncopy team\nfrom 'D:\\Downloads\\A Portfolio Projects\\SQL Projects\\IPL Analysis\\Dataset CSV\\team.csv'\ndelimiter ','\ncsv header;\n\n-- player 테이블 생성\ncreate table if not exists player(\n player_id int,\n player_name varchar(50) not null,\n dob date not null,\n batting_hand varchar(50) not null,\n bowling_skill varchar(50) not null,\n country_name varchar(50) not null,\n constraint pk_player_player_id primary key(player_id)\n);\n-- player.csv 파일에서 값 가져오기\ncopy player\nfrom 'D:\\Downloads\\A Portfolio Projects\\SQL Projects\\IPL Analysis\\Dataset CSV\\player.csv'\ndelimiter ','\ncsv header;\n\n-- match 테이블 생성\ncreate table if not exists match(\n match_id int primary key,\n season_year int not null,\n team1 int not null references team(team_id),\n team2 int not null references team(team_id),\n venue_id int not null references venue(venue_id),\n toss_winner int not null references team(team_id),\n match_winner int not null references team(team_id),\n toss_name varchar(50) not null check(toss_name in ('field', 'bat')),\n win_type varchar(50) not null check(win_type in ('wickets', 'runs', 'NULL')),\n man_of_match int not null references player(player_id),\n win_margin int not null\n)\n-- match.csv 파일에서 값 가져오기\ncopy match\nfrom 'D:\\Downloads\\A Portfolio Projects\\SQL Projects\\IPL Analysis\\Dataset CSV\\match.csv'\ndelimiter ','\ncsv header;\n\n-- player_match 테이블 생성\ncreate table if not exists player_match(\n playermatch_key bigint primary key,\n match_id int not null references match(match_id),\n player_id int not null references player(player_id),\n role_desc varchar(50) not null check(role_desc in ('Player', 'Keeper', 'CaptainKeeper', 'Captain')),\n team_id int not null references team(team_id)\n);\n-- player_match.csv 파일에서 값 가져오기\ncopy player_match\nfrom 'D:\\Downloads\\A Portfolio Projects\\SQL Projects\\IPL Analysis\\Dataset CSV\\player_match.csv'\ndelimiter ','\ncsv header;\n\n-- ball_by_ball 테이블 생성\ncreate table if not exists ball_by_ball(\n match_id int not null references match(match_id),\n innings_no int not null check(innings_no\u003c3 and innings_no\u003e0),\n over_id int not null,\n ball_id int not null,\n runs_scored int not null check(runs_scored\u003c=6 and runs_scored\u003e=0),\n extra_runs int not null,\n out_type varchar(50) not null check(out_type in ('caught', 'caught and bowled', 'bowled', 'stumped', 'retired hurt', 'keeper catch', 'lbw', 'run out', 'hit wicket', 'NULL')),\n striker int not null references player(player_id),\n non_striker int not null references player(player_id),\n bowler int not null references player(player_id),\n constraint pk_ball_by_ball_id primary key(match_id, innings_no, over_id, ball_id)\n)\n-- ball_by_ball.csv 파일에서 값 가져오기\ncopy ball_by_ball\nfrom 'D:\\Downloads\\A Portfolio Projects\\SQL Projects\\IPL Analysis\\Dataset CSV\\ball_by_ball.csv'\ndelimiter ','\ncsv header;\n```\n\n질문 2: 생성한 테이블에서 각 경기장 마다 스코어된 평균 달성량을 찾으려면 스타디움에서 경기당 평균 달성량(두 팀의 총점)을 계산해야 합니다.\n총 점수를 계산하려면 ball_by_ball 테이블에서 runs_scored 및 extra_runs를 합산해야 합니다.\n\n해결책: 각 경기장에서 스코어된 평균 달성량을 찾으려면 다음 단계를 따라야 합니다. 먼저 각 경기장에서 플레이된 총 경기수를 계산하고, 그 다음 각 경기장에서 스코어된 총 점수를 결정합니다. 마지막으로 총 점수를 플레이된 경기수로 나누어 각 경기장의 경기 당 평균 달성량을 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sql\n-- 2단계: 각 구장에서의 경기 수 계산\nwith\nno_of_match_per_venue as\n (\n  select v.venue_id, v.venue_name, count(match_id) as no_of_matches\n  from match m\n  join venue v\n  on v.venue_id=m.venue_id\n  group by v.venue_id, v.venue_name\n ),\n-- 2단계: 각 구장에서의 총 득점 계산\ntotal_run_per_venue as\n (\n  select v.venue_id, sum(b.runs_scored+b.extra_runs) as total_run\n  from ball_by_ball b\n  join match m\n  on m.match_id = b.match_id\n  join venue v\n  on v.venue_id = m.venue_id\n  group by v.venue_id\n )\n-- 마지막으로 위의 두 임시 테이블을 사용하여\n-- 각 구장에서의 경기 당 평균 득점을 계산합니다\nselect  npv.venue_name, tpv.total_run, npv.no_of_matches,\nround(tpv.total_run/npv.no_of_matches::numeric,3) as avg_run\nfrom no_of_match_per_venue npv\njoin total_run_per_venue tpv\non npv.venue_id = tpv.venue_id\norder by avg_run desc;\n```\n\n![2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_2.png](/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_2.png)\n\n질문 3: 평균으로 경기 당 가장 많은 볼을 친 선수들을 찾고 상위 10명으로 제한하십시오.\n스트라이커로서 해당 선수가 등록된 경우 선수가 공을 쳤다고 간주합니다.\n\n해결책: 먼저 각 선수가 참가한 총 경기 수를 계산해야 합니다. 그 다음 각 선수가 스트라이커로서 받은 총 볼 수를 확인해야 합니다. 마지막으로 경기 당 평균으로 가장 많은 볼을 친 상위 10명의 선수를 식별할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n-- 솔루션 3:\n-- 단계 1: player_match 테이블에서 플레이어가 참가한 경기 수를 세기\nwith num_of_match_by_player as\n(\nselect player_id, count(match_id) as no_of_match from player_match\ngroup by player_id\n),\n-- 단계 2: 공격수로서 플레이어가 참가한 총 볼 수 계산\ntotal_ball_played_by_player as\n(\nselect striker, count(ball_id) as total_ball_played from ball_by_ball\ngroup by striker\n)\n-- 최종적으로 플레이어 당 평균 한 경기에서 가장 많이 볼을 친 상위 10명을 계산\nselect player_id, player_name, avg_ball_played from\n(\nselect \\*,\n-- 동률이 있는 경우를 포함하기 위해 rank 함수 사용\nrank() over(order by avg_ball_played desc) from\n(\n-- 평균 계산\nselect p.player_id, p.player_name,\n(tp.total_ball_played/mp.no_of_match) as avg_ball_played\nfrom num_of_match_by_player mp, total_ball_played_by_player tp, player p\nwhere mp.player_id = tp.striker\nand\np.player_id = mp.player_id\n)\n)\nwhere rank\u003c=10; -- 상위 10개 가져오기\n\n![](/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_3.png)\n\nQ 4: 가장 빈도가 높은 6타자를 찾아보세요.\n즉, 플레이어가 차지한 볼 중에서 가장 높은 비율로 6점을 친 플레이어를 찾으세요. 플레이어 ID, 플레이어 이름, 플레이어가 6점을 얻은 횟수, 차진 볼 수, 6의 비율을 출력하세요.\n\n솔루션: 먼저 각 플레이어가 차진 볼 수를 계산합니다. 그런 다음, 각 플레이어가 친 6점을 결정합니다. 마지막으로 각 플레이어의 6의 비율을 계산하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sql\n-- Solution 4:\n-- 각 세션에서 각 선수가 참가한 공의 수 계산\nwith ball_by_player as(\n select striker, count(ball_id) as ball_played from ball_by_ball\n group by striker\n),\n-- 각 선수가 기록한 6점 수 계산\nsix_by_player as(\n select striker, count(ball_id) as no_of_six from ball_by_ball\n where runs_scored = 6\n group by striker\n)\n-- 최종 비율 얻기\nselect p.player_id, p.player_name, bp.ball_played, sp.no_of_six,\nround((sp.no_of_six::numeric/bp.ball_played),2) as fraction\nfrom ball_by_player as bp, six_by_player as sp, player as p\nwhere bp.striker = sp.striker and bp.striker = p.player_id\norder by fraction desc;\n```\n\n\u003cimg src=\"/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_4.png\" /\u003e\n\nQ 5: 각 시즌에서 가장 많은 득점을 기록한 상위 3 타자 및 가장 많은 wickets를 따낸 상위 3 볼러 player_ids를 찾아보세요. Output (season_year, batsman, runs, bowler, wickets). 여기서 batsman 및 bowler는 선수들의 player_ids입니다. 동점인 경우 더 낮은 player_id를 먼저 출력합니다. season_year (날짜가 빠른 순)와 rank(특정 시즌에 더 많은 득점 및 wickets를 기록한 타자와 볼러)로 정렬합니다. (no_of_seasons\\*3)개의 행이 있을 것입니다.\n\nSolution: 먼저, 각 시즌에서 가장 많은 wickets를 기록한 상위 3 타자를 식별합니다. 다음으로, 각 시즌에서 가장 많은 wickets를 기록한 상위 3 볼러를 결정합니다. 마지막으로, 이러한 결과를 결합하여 최종 목록을 얻습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n-- 솔루션 5:\n-- 먼저 각 시즌에서 각각 가장 많은 횟수의 릴리를 기록한 상위 3명의 타자를 찾습니다.\nwith top_batsman as\n (\n select *,\n rank() over(partition by season_year order by run desc, striker) from\n  (\n  select m.season_year, b.striker, p.player_name, sum(runs_scored) as run\n  from ball_by_ball as b, match as m, player as p\n  where b.match_id = m.match_id and p.player_id = b.striker\n  group by m.season_year, b.striker, p.player_name\n  )\n ),\n-- 그리고 두 번째로, 각 시즌에서 각각 가장 많은 볼을 기록한 상위 3명의 볼러를 찾는다.\ntop_bowlers as(\n select *,\n rank() over(partition by season_year order by wicket desc, bowler) from\n  (\n  select m.season_year, b.bowler, p.player_name, count(out_type) as wicket\n  from ball_by_ball as b, match as m, player as p\n  where b.match_id = m.match_id and p.player_id = b.bowler\n  and b.out_type not in ('run out', 'retired hurt')\n  group by m.season_year, b.bowler, p.player_name\n  )\n )\n-- 위 두 가지를 조인하여 최종 결과를 얻으세요\nselect tbt.season_year, tbt.striker, tbt.player_name, tbt.run, tbo.bowler, tbo.player_name, tbo.wicket\nfrom top_batsman as tbt, top_bowlers as tbo\nwhere tbt.rank=tbo.rank and tbt.rank\u003c=3 and tbo.rank\u003c=3 and tbt.season_year = tbo.season_year\norder by season_year;\n```\n\n\u003cimg src=\"/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_5.png\" /\u003e\n\n질문 6: 각 경기에서 최대 파트너십 득점을 달성한 선수의 ID를 찾기 위한 SQL 쿼리를 작성하세요. 결과에는 (match_id, player1, runs1, player2, runs2)가 포함되어야 하며, 파트너십 득점의 내림차순으로 정렬되어야 합니다. 동점의 경우 match_id가 오름차순으로 정렬되어야 합니다. runs1이 항상 runs2보다 큰지 확인하고, runs1과 runs2가 동일한 경우 player1_id가 player2_id보다 커야 합니다. extra_runs는 포함되어서는 안 됩니다. 서로 다른 선수가 동일한 파트너십 득점을 여러 번 달성하는 경우 각 경기의 여러 행이 존재할 수 있습니다.\n\n솔루션:\n단계 1- partnership이라는 공통 테이블 표현(CTE)을 사용하여 각 파트너십(경기 ID 및 연결된 선수 ID로 식별)이 가져온 총 득점(extr):\n단계 2- 다른 CTE인 striker_run_contributed는 각 파트너십에서 스트라이커가 기여한 총 득점을 계산합니다.\n단계 3- CTE final_table은 파트너십 득점을 스트라이커의 득점 기여와 결합하고, 비 스트라이커의 득점을 계산합니다. 각 경기의 최고 파트너십 득점만 포함하도록 필터링합니다.\n단계 4- 주 쿼리는 결과를 선택하고 정렬하여 더 높은 득점자가 항상 먼저 나오고 run1이 항상 run2보다 크도록 합니다. 두 선수가 동일한 득점인 경우 더 높은 ID를 가진 선수가 먼저 표시됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n-- 질문 5:\n-- 각 경기에서 가장 많은 협력 득점을 얻은 플레이어들의 ID 찾기?\n-- 한 경기에 여러 개의 행이 있을 수 있습니다.\n-- 출력 (match_id, player1, runs1, player2, runs2),\n-- 협력 득점의 내림차순으로(동점인 경우 match_id는 오름차순으로 비교).\n-- 각 행에서 run1 \u003e run2\n-- runs1=runs2인 경우 player1_id \u003e player2_id. 참고: extra_runs는 계산하지 않아야 함\n-- 솔루션\n\nwith partnership as\n(\n select match_id, striker, non_striker, p_id, p_run from\n (\n  select *,\n  sum(runs_scored) over(partition by match_id, p_id order by match_id) as p_run,\n  row_number() over(partition by match_id, p_id order by match_id) as rank\n  from(\n   select b.match_id, b.runs_scored, b.striker, b.non_striker,\n   case when striker\u003cnon_striker then concat(non_striker,' ',striker)\n   else concat(striker, ' ', non_striker)\n   end as p_id\n   from ball_by_ball as b\n  )\n ) where rank=1\n order by p_run desc, match_id asc\n),\nstriker_run_contributed as\n (\n select b.match_id, b.striker, b.non_striker, sum(b.runs_scored) as striker_run\n from ball_by_ball as b\n group by b.match_id, b.striker, b.non_striker\n  ),\nfinal_table as\n(\n select p.match_id, p.striker, p.non_striker, sr.striker_run, (p.p_run-sr.striker_run) as non_striker_run,\n p.p_run\n from partnership as p, striker_run_contributed as sr\n where p.match_id = sr.match_id and p.striker = sr.striker and p.non_striker = sr.non_striker\n and p.p_run = (select max(p_run) from partnership as pt where pt.match_id = p.match_id)\n order by p.p_run desc, p.match_id asc\n  )\nselect match_id,\ncase when (striker_run = non_striker_run and striker\u003enon_striker) then striker\n  when striker_run\u003enon_striker_run then striker\n  else non_striker end as player_1,\ncase when (striker_run\u003enon_striker_run) then striker_run\n  else non_striker_run end as run1,\ncase when (striker_run = non_striker_run and striker\u003enon_striker) then non_striker\n  when striker_run\u003enon_striker_run then non_striker\n  else striker end as player_2,\ncase when (striker_run\u003enon_striker_run) then non_striker_run\n  else striker_run end as run2,\np_run as total_partnership\nfrom final_table;\n```\n\n\u003cimg src=\"/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_6.png\" /\u003e\n\n질문 7: 이닝 유형이 wickets인 모든 경기에서 득점이 6점 미만인 이닝 ID를 찾으세요.\n출력 (match_id, innings_no, over_id). 참고: 이닝에서 득점된 점수에는 extra_runs도 포함됨.\n\n솔루션: 먼저 ball_by_ball 테이블과 이긴 경기 정보를 포함하는 match 테이블을 조인한 후, 득점이 6점 미만인 경우에 해당하는 over_id를 가져옵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sql\n-- 질문 7:\n-- 이닝 종료로 승리한 경기 중에서 6 미만의 점수를 기록한 이닝 ID를 찾아주세요.\n-- 출력 (match_id, innings_no, over_id). 참고: 이닝에서 기록된 점수에는 추가 점수도 포함됩니다.\n\n-- 해결 방법 7:\n\nselect b.match_id, b.innings_no, b.over_id\nfrom ball_by_ball as b\njoin match as m on m.match_id = b.match_id\nwhere win_type = 'wickets'\ngroup by b.match_id, b.innings_no, b.over_id\nhaving sum(b.runs_scored) + sum(extra_runs) \u003c 6\n```\n\n![이미지](/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_7.png)\n\nQ 8: 2013 시즌에서 가장 많은 홈런을 친 상위 5명의 타자 나열하기?\n출력 (player_name).\n\n해결 방법: ball_by_ball 테이블을 match 테이블과 연결하여 시즌 연도를 얻고, player 테이블과 연결하여 선수명을 얻습니다. 2013년에 홈런을 세어 상위 5명을 제한하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sql\n-- 질문 8:\n-- 2013 시즌에서 가장 많은 홈런을 친 상위 5명의 타자를 나열하십시오.\n-- 알파벳순으로 동점이 발생했을 경우를 고려하십시오. 결과 (선수 이름).\n\n-- 해결책 8:\nselect p.player_name from ball_by_ball as b, match as m, player as p\nwhere (b.match_id = m.match_id and b.striker = p.player_id)\nand (m.season_year = 2013 and b.runs_scored = 6)\ngroup by b.striker, p.player_name order by count(runs_scored) desc limit 5\n```\n\n![2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_8.png](/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_8.png)\n\nQ 9: 2013 시즌에서 가장 낮은 스트라이크 비율(평균 당 탈아웃당한 볼의 수)으로 상위 5명의 볼러를 나열하십시오. 알파벳순으로 동점이 발생했을 경우를 고려하십시오. 결과 (선수 이름).\n\n해결책: 우선 2013년에 각 선수가 얼마나 많은 아웃을 기록했는지를 계산하십시오. 'NULL', 'retired hurt', 'run out'과 같은 out_type은 볼러로 카운트되지 않습니다. 그래서 데이터 분석가는 데이터 세트에 대한 도메인 지식을 어느 정도 알고 있는 것이 중요합니다. 그런 다음 각 볼러가 한 공을 던진 횟수를 계산하십시오. 마지막으로 평균 비율을 구하고, 비율이 높을수록 볼러로서의 스트라이크 비율이 낮습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sql\n-- 질문 9: 2013 시즌에서 볼링 스트라이크율(얻은 퍼스트볼당 볼이 던져진 평균 수)이 가장 낮은 5명의 볼러를 나열하십시오. 알파벳순으로 동률 발생 시 이름순으로 정렬하십시오. 결과값은 (선수 이름)으로 출력합니다.\n\n-- 해결책 9:\nwith wicket as\n(\n select b.bowler, p.player_name, count(out_type) as no_of_wicket\n from ball_by_ball as b, player as p, match as m\n where (b.bowler = p.player_id and b.match_id = m.match_id)\n and (b.out_type not in ('NULL', 'retired hurt', 'run out')\n and m.season_year = 2013)\n group by b.bowler, p.player_name\n),\nballs as\n(\n select b.bowler, p.player_name, count(ball_id) as no_of_ball\n from ball_by_ball as b, player as p, match as m\n where (b.bowler = p.player_id and b.match_id = m.match_id)\n and m.season_year = 2013\n group by b.bowler, p.player_name\n\n)\nselect b.player_name, b.no_of_ball/w.no_of_wicket as ratio from wicket as w, balls as b\nwhere w.bowler = b.bowler\norder by ratio desc , b.player_name limit 5;\n```\n\n![이미지](/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_9.png)\n\nQ 10: 각 나라(적어도 한 명의 선수가 아웃 처리됨)별로 어떤 경기에서 볼 아웃된 선수의 수를 찾아내십시오? 결과값은 (나라 이름, 수)으로 출력합니다. 여기서 나라는 선수의 속한 국적입니다.\n\n해결책: 볼링 백볼 테이블을 선수 테이블과 조인하여 국가 이름을 얻고, out_type = \"볼드\"로 필터링합니다. 적어도 한 명의 선수가 있는 각 나라별로 볼드 아웃된 선수의 수를 그룹화하여 계산합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sql\n-- 질문 10:\n-- 적어도 한 명의 선수가 볼을 던진 나라마다\n-- 임의의 경기에서 볼 처리를 받은 플레이어의 수를 찾으세요.\n-- 출력 (country_name, count). 여기서 나라는 선수의 국적입니다.\n\n-- 해결 방법:\nselect p.country_name, count(striker) no_of_bowled_out\nfrom ball_by_ball as b, player as p\nwhere p.player_id = b.striker and b.out_type = 'bowled'\ngroup by p.country_name having count(striker) \u003e 0 order by no_of_bowled_out desc\n```\n\n![이미지](/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_10.png)\n\nQ 11: ‘푸네’에서 진행된 임의의 경기에서 적어도 백을 득점한 오른손 타자의 이름을 나열해주세요? 출력 (player_name, run).\n\n해결 방법:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sql\n-- 질문 11:\n-- 'Pune'에서 플레이된 모든 경기 중에서 적어도 한 번 센추리를 기록한 우포수 선수들의 이름을 나열하십시오. player_name을 알파벳순으로 출력하십시오.\n\n-- 해결책:\nselect p.player_name, sum(runs_scored) as run\nfrom ball_by_ball as b, match as m, venue as v, player as p\nwhere (b.match_id = m.match_id and m.venue_id = v.venue_id\n    and p.player_id = b.striker and v.city_name = 'Pune'\n    and p.batting_hand = 'Right-hand bat')\ngroup by b.striker, p.player_name having sum(runs_scored)\u003e=100\norder by run desc, p.player_name;\n```\n\n![이미지](/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_11.png)\n\n보너스 질문:\n자체 해결해보기-\n적어도 한 번의 경기를 이겨온 모든 팀에 대한 승률을 찾으십시오(모든 시즌에 걸쳐). 팀 이름으로 알파벳순으로 결과를 정렬하십시오. 출력 (team_name, win_percentage).\n팀의 승률은 = (팀이 이긴 경기수 / 팀이 플레이한 총 경기수) \\* 100로 계산될 수 있습니다.\n참고: 소수점 셋째 자리까지 백분율로 계산하십시오.\n\n# 결론:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단히 말씀드리자면, SQL을 사용하여 IPL 통계에 뛰어들어 본 것은 정말 즐거운 경험이었어요! 선수, 팀 및 경기에 관한 멋진 통찰력을 발견하여 트렌드와 우수한 성적을 눈에 띄게하기 쉬웠습니다.\n\n저의 Github 저장소를 참조하여 SQL 쿼리, 질문 및 데이터셋을 이용할 수 있습니다.\n\n도움이 되었기를 바라며 다시 한번 감사합니다.\n","ogImage":{"url":"/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_0.png"},"coverImage":"/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_0.png","tag":["Tech"],"readingTime":18},{"title":"Snowflake의 미래 부여가 결국 실패할 이유","description":"","date":"2024-06-23 16:32","slug":"2024-06-23-WhyyourSnowflakefuturegrantswilleventuallybreak","content":"\n\n\u003cimg src=\"/assets/img/2024-06-23-WhyyourSnowflakefuturegrantswilleventuallybreak_0.png\" /\u003e\n\n여기 설정이 있어요...\n\n방금 MyFirstRole이라는 새 역할을 만들었습니다.\n\n```js\ncreate role MyFirstRole;\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n계획은 이 역할에 대해 MyDatabase 라는 데이터베이스에 대해 완전한 읽기 액세스 권한을 부여하는 것입니다. MyDatabase 내의 모든 스키마의 모든 현재 테이블 및 뷰를 선택할 수 있도록하고 싶습니다.\n\n```js\ngrant usage on database MyDatabase to role MyFirstRole;\ngrant usage on all schemas in database MyDatabase to role MyFirstRole;\ngrant select on all tables in database MyDatabase to role MyFirstRole;\ngrant select on all views in database MyDatabase to role MyFirstRole;\n```\n\n위 명령문을 설정하면 우리의 역할은 이제 MyDatabase의 모든 항목을 선택할 수 있습니다.\n\n그러나 시간이 흐른다면 데이터베이스에 새로운 테이블 및 뷰가 생성될 것입니다. 새로운 개체가 생성될 때마다 MyFirstRole에게 수동으로 선택 권한을 부여하는 것은 비현실적입니다. 다행히도 미래의 권한 부여가 이 문제를 해결해 줄 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n미래 스키마에서 MyDatabase 내의 향후 역할에 대한 사용 권한을 MyFirstRole 역할에 부여합니다.\n미래 테이블에서 MyDatabase 내의 향후 테이블에 대한 선택 권한을 MyFirstRole 역할에 부여합니다.\n미래 뷰에서 MyDatabase 내의 향후 뷰에 대한 선택 권한을 MyFirstRole 역할에 부여합니다.\n```\n\n이제 MyFirstRole은 앞으로 생성되는 테이블이나 뷰에서 select 문을 실행할 수 있습니다.\n\n모든 것이 잘 되고 예상대로 작동하고 있습니다. MyFirstRole에 대한 select 권한을 수동으로 업데이트할 필요가 없으며, 새롭게 생성된 테이블과 뷰에 액세스를 부여해야 하는 경우를 제외하고는요. 이때, MyDatabase에 제한적인 액세스를 갖는 새로운 역할인 MySecondRole을 생성해야 할 때가 올 것입니다.\n\nMyDatabase에는 MySchema라는 스키마가 있다고 가정합시다. MySecondRole은 MySchema 내의 현재 및 향후 테이블 및 뷰에서만 선택할 수 있고 다른 작업은 제한되어야 한다고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n조금만 더 해야겠어요...\n\n```js\ncreate role MySecondRole;\ngrant usage on database MyDatabase to role MySecondRole;\ngrant usage on schema MyDatabase.MySchema to role MySecondRole;\ngrant select on all tables in schema MyDatabase.MySchema to role MySecondRole;\ngrant select on future tables in schema MyDatabase.MySchema to role MySecondRole;\ngrant select on all views in schema MyDatabase.MySchema to role MySecondRole;\ngrant select on future views in schema MyDatabase.MySchema to role MySecondRole;\n```\n\n다시 한 번, MySecondRole에 대한 새 테이블 및 뷰가 MySchema에 생성되는 것처럼 모든 것이 예상대로 작동하는 것처럼 보입니다. 그러나 몇 주 후에 MyFirstRole이 MySecondRole에 대한 그랜트 이후에 생성된 테이블 또는 뷰에 더 이상 액세스 권한이 없다는 것을 깨닫게 됩니다.\n\n그리고 이것이 미래의 권한이 결국 망가지게 되는 이유입니다...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-WhyyourSnowflakefuturegrantswilleventuallybreak_1.png\" /\u003e\n\n간단한 영어로, MyFirstRole은 데이터베이스 수준에서 미래 객체에 권한을 부여받았습니다. ... 데이터베이스에서 미래 테이블을 부여했습니다. 그와 반대로, MySecondRole은 스키마 수준에서 미래 부여를 받았습니다. ... 스키마 수준의 부여는 동일한 데이터베이스 내에서 데이터베이스 수준의 부여보다 우선합니다. 이는 MyFirstRole이 MySecondRole의 미래 부여가 실행된 시점에 테이블 및 뷰에 대한 미래 부여가 무효화되었음을 의미합니다.\n\n이에 대한 해결책은 없습니다. 동일한 데이터베이스에서 데이터베이스 및 스키마 수준에서 미래 부여가 동시에 존재하는 세계에서 살 수 없습니다. 해결책은 데이터베이스의 액세스 패턴을 사전에 파악하고 이에 맞게 부여하는 것입니다. 액세스 요구 사항이 복잡하고 여러 역할 간에 맞춤형인 경우 스키마 수준에서 미래 부여가 적합합니다. 더 간단하고 단일 목적의 데이터베이스(스테이징이 좋은 예시입니다)의 경우에는 데이터베이스 수준의 부여를 사용할 수도 있습니다.\n\n읽어 주셔서 감사합니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n안녕하세요! 부담 가지지 마시고 언제든지 인사해주세요 👋\n\n트위터 — https://x.com/jduran9987\n\n링크드인 — https://www.linkedin.com/in/jonathan-duran-80974a183/","ogImage":{"url":"/assets/img/2024-06-23-WhyyourSnowflakefuturegrantswilleventuallybreak_0.png"},"coverImage":"/assets/img/2024-06-23-WhyyourSnowflakefuturegrantswilleventuallybreak_0.png","tag":["Tech"],"readingTime":3},{"title":"데이터 늪에서 데이터 마스터까지 Apache Iceberg와 함께하는 레이크하우스 혁명 ","description":"","date":"2024-06-23 16:31","slug":"2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg","content":"\n\n\u003cimg src=\"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_0.png\" /\u003e\n\n우리의 데이터 인프라는 처음에 Amazon S3를 사용한 데이터 레이크와 Amazon Redshift를 사용한 데이터 웨어하우스의 조합으로 이루어져 있었습니다.\n\n이 구성은 대량의 데이터를 저장하고 분석할 수 있는 장점이 있었지만, 추가 저장 공간 및 유지보수 문제와 ACID 규칙 준수를 지원하지 않는 등 여러 가지 도전 과제가 있었습니다.\n\n# 목표\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n호수집 구조로 전환하는 목표는 데이터 레이크와 데이터 웨어하우스의 최상의 특징을 결합하는 것이었습니다. 이미 완전히 발달한 데이터 레이크가 있었기 때문에, 우리의 초점은 데이터 웨어하우스의 기능을 통합하는 데 있었습니다.\n\n## 데이터 레이크하우스와 데이터 레이크 및 데이터 웨어하우스의 차이는 무엇인가요?\n\n이름에서 알 수 있듯이 '데이터 레이크하우스'는 데이터 레이크와 데이터 웨어하우스의 최상의 특징을 결합합니다. 본질적으로 데이터 레이크하우스는 데이터 레이크의 기능을 확장하여 데이터 웨어하우스와 유사한 기능을 통합합니다. 데이터 레이크의 유연성, 확장 가능성 및 비용 효율성을 제공하는 한편, 데이터 웨어하우스와 주로 관련된 튼튼한 데이터 관리와 ACID (원자성, 일관성, 분리, 지속성) 트랜잭션을 제공하려고 합니다.\n\n# 왜 아파치 아이스버그를 선택했나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_1.png\" /\u003e\n\n- ACID 트랜잭션: ACID 트랜잭션을 지원하여 데이터 일관성과 신뢰성을 보장하고, 동시에 쓰기 및 읽기를 허용하여 데이터 오염이 발생하지 않습니다.\n- 비용 및 유지보수 감소: Redshift와 연관된 높은 저장 및 라이선스 비용을 최소화하며, 기본적으로 compaction 및 압축(zstd)을 지원합니다.\n- 성능 최적화: 메타데이터 가지치기, 파티셔닝 및 데이터 건너뛰기와 같은 기능을 통해 쿼리 성능을 크게 향상시킵니다.\n- 호환성: Apache Spark, Flink, Presto 등 여러 데이터 처리 엔진과 함께 작동하여 작업에 최적인 도구를 선택할 수 있는 유연성 제공.\n- Parquet, ORC, Avro와 같은 파일 형식 지원.\n- 기존의 AWS 생태계 및 Athena, Glue, Catalog, EMR 등과 시프트레이엘튼랏하는 탐바.\n- 성능 향상: 빠르게 쿼리되어 데이터를 효율적으로 검색할 수 있습니다.\n- 통합 데이터 처리: 일괄 및 스트리밍 데이터 처리에 대해 통합된 경험을 제공하여 실시간 및 기존 데이터의 원활한 통합 및 처리를 가능하게 합니다.\n\n\u003cimg src=\"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_2.png\" /\u003e\n\n# Iceberg 아키텍처:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_3.png)\n\n![Image 2](/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_4.png)\n\n# Apache Iceberg을 활용한 Lakehouse 전환 단계\n\n환경 설정:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 저장소 구성: Amazon S3와 같은 확장 가능한 저장소 솔루션을 설정하여 원본 데이터와 처리된 데이터를 저장하세요. 이미 저희와 같이 S3를 활용 중이라면 데이터 및 메타데이터를 저장할 대상 버킷을 정의하세요.\r\n- Iceberg 설치 및 구성: EMR을 사용 중이므로 Spark 세션을 생성할 때 Iceberg 관련 설정을 추가해야 합니다.\n\n```js\r\nspark = SparkSession.builder \\\n    .appName(\"user_device_data\") \\\n    .master(\"yarn\") \\\n    .config(\"spark.sql.defaultCatalog\", catalog) \\\n    .config(f\"spark.sql.catalog.{catalog}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog}.warehouse\",\n            \"\u003cYour S3 Warehouse Path\u003e\") \\\n    .config(\"spark.sql.catalog.glue_catalog.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(\"spark.sql.catalog.glue_catalog.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\r\n```\n\n데이터 이전:\n\n- 데이터레이크 — 기존 Parquet에서 Iceberg로 데이터 마이그레이션: 먼저 테이블을 만들었고, 기존 데이터레이크에서 데이터를 읽어와 Apache Spark를 사용하여 Iceberg 테이블에 기록함으로써 메타데이터가 올바르게 캡처되도록합니다.\n- 데이터웨어하우스 — Redshift 데이터를 Iceberg 형식으로 투입: Redshift에서 언로드한 데이터를 S3로 복사한 후, 데이터레이크와 동일한 접근 방식을 따랐습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같은 방법으로 인플레이스 마이그레이션을 수행할 수 있습니다:\n\n- add_files 사용\n- migrate 사용\n\n[기존 데이터 레이크를 Apache Iceberg를 사용한 트랜잭션 데이터 레이크로 마이그레이션하기](https://aws.amazon.com/blogs/big-data/migrate-an-existing-data-lake-to-a-transactional-data-lake-using-apache-iceberg/)\n\n기존 ETL 프로세스에서의 조정:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 우리는 모든 ETL 작업에 대한 싱크 구성을 변경하여 데이터 아이스버그 형식으로 쓰게 했습니다. 위에서 언급한 구성은 스파크 세션을 만들 때 사용되었습니다.\n\n데이터 거버넌스 및 메타데이터 관리:\n\nIceberg 테이블의 유지 보수 작업.\n\n- Compact : 우리는 다시 쓰고 결과 파일의 원하는 크기로 재작성하기 위해 rewriteDataFiles 절차를 실행합니다. 이것은 읽기 시간을 최적화하는 데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n('write.parquet.target-file-size-bytes '='52428800') \n# 약 이만큼의 바이트를 대상으로 생성된 파일의 크기를 제어합니다.\n\n2. 스냅샷 만료: 분석에 더 이상 필요하지 않은 데이터에 대해 스냅샷 만료를 실행하여 불필요한 저장 비용을 피합니다. 만료된 스냅샷과 연결된 매니페스트 목록, 매니페스트 및 데이터 파일은 여전히 유효한 스냅샷과 연관되어 있지 않은 한 스냅샷 삭제 시에 삭제됩니다.\n\n우리는 이 작업을 수행하기 위해 expireSnapshots 프로시저를 실행합니다.\n\n3. 오래된 메타데이터 파일 제거: Iceberg는 새 메타데이터 파일이 생성될 때 오래된 메타데이터 파일을 삭제하는 설정을 활성화할 수 있습니다. 또한 테이블이 보유해야 하는 메타데이터 파일 수를 설정할 수 있습니다. 우리는 그 수를 5로 설정했습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nwrite.metadata.delete-after-commit.enabled  true\nwrite.metadata.previous-versions-max 5\n```\n\n4. Orphan 파일 삭제 : Orphan 파일 제거를 위해 deleteOrphanFiles 절차를 실행하여 필요 없는 파일을 저장하지 않습니다. 이러한 파일들은 정기적인 정리 프로세스에서 선택되지 않습니다.\n\n쿼리 및 분석:\n\n- 쿼리 최적화: Iceberg는 메타데이터 가지치기(metadata pruning) 및 프리디케이트 푸시다운(predicate pushdown)과 같은 기능을 지원하여 쿼리 성능을 최적화할 수 있습니다. 데이터와 메타데이터 모두에 대한 쿼리 엔진으로 Athena를 사용하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n메타데이터 쿼리 치트 시트 : [https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-table-metadata.html](https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-table-metadata.html)\n\n- 실시간 분석 활성화: 스파크 ETL 프로세스에서 데이터 수집 및 업데이트가 발생하여 배치 및 스트리밍 데이터 처리에 통합된 경험을 제공합니다.\n\n# Apache Iceberg 구현의 장점\n\n비용 효율성:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 저장 비용 절감: Apache Iceberg는 기본 Z-표준 (zstd) 압축을 사용하여 저장 공간 요구 사항을 크게 줄입니다. 또한 파티션 분할을 지원하여 데이터 스캔을 제한하여 저장 공간 사용을 최적화합니다.\n- 중복 데이터 웨어하우징 솔루션 소거: Iceberg의 ACID 규정 준수로 기존 데이터 웨어하우징 솔루션이 불필요해집니다. Amazon S3의 저장 비용은 Redshift의 그것보다 상당히 낮기 때문에 Redshift 라이선싱 비용을 상당히 절약할 수 있습니다.\n\nApache Iceberg의 채택으로 일반 Parquet 형식으로 데이터를 저장하는 비용과 전체 S3 비용 모두 30%의 저장 비용 절감과 20%의 S3 비용 절감을 이끌었습니다.\n\n성능 향상:\n\n- 쿼리 성능 개선: Iceberg의 최적화된 데이터 관리 및 인덱싱으로 쿼리 성능과 데이터 검색 시간이 상당히 향상되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n역사적 데이터 수정:\n\n- 간편화된 데이터 업데이트: 이전에는 역사적 데이터를 수정하기 위해 작은 배치 작업을 작성해야 했습니다. 아이스버그를 사용하면 몇 가지 업데이트 명령을 실행함으로써 이를 달성할 수 있어, 프로세스가 간소화되었습니다.  \n\n접근 제어:\n\n- 간편화된 행 수준 접근: Redshift에서 서로 다른 국가에 대한 행 수준 접근을 제공하는 것은 복잡했습니다. 아이스버그를 통해 데이터 파티셔닝을 사용하여 특정 버킷에 대한 접근 정책을 쉽게 구현할 수 있어, 접근 관리가 간소화됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nApache Iceberg를 구현함으로써 비용 효율성, 향상된 성능, 간단화된 기존 데이터 수정 및 향상된 액세스 관리를 달성했습니다.\n\n# 결론\n\n여러 데이터셋을 Apache Iceberg로 이관하는 작업을 성공적으로 완료했으며 이미 상당한 비용 및 성능 이점을 확인하고 있습니다. 레이크하우스 아키텍처로의 전환은 데이터 레이크와 데이터 웨어하우스의 최상의 기능을 활용할 수 있게 해주어 더 효율적이고 확장 가능한 데이터 인프라를 구축하게 되었습니다.\n\n이 구현 기간 동안 빈말 야다브와 시바무 구프타에게 놀라운 헌신과 값진 기여에 진심으로 감사드립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사실, 한국어로 \"테이블\" 태그를 \"Markdown\" 형식으로 변환하면 되는 것 같아요.","ogImage":{"url":"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_0.png"},"coverImage":"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_0.png","tag":["Tech"],"readingTime":7},{"title":"2024년에 데이터 엔지니어가 되기 위한 조건은 무엇일까","description":"","date":"2024-06-23 16:28","slug":"2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024","content":"\n\n\u003cimg src=\"/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_0.png\" /\u003e\n\n만약 이 글을 읽고 있다면, 요즘 직업을 바꾸려고 고민 중이신 것 같군요. 아마도 소프트웨어 엔지니어링과 데이터베이스 디자인에 관심이 있으신 거겠죠. 당신의 배경이 무엇이든 중요하지 않아요 — 마케팅, 분석, 혹은 금융 분야에서 오셨든, 여러분도 할 수 있어요! 이 이야기는 데이터 공간에 빠르게 진입하는 방법을 찾아주기 위한 것이에요. 예전에 저도 똑같이 해서 그 뒤로 후회한 적이 없어요. 기술 분야, 특히 데이터는 매력과 혜택이 넘쳐나요. 선뜻 언급하지 않은 원격 근무와 선도 기업들로부터의 막대한 혜택 패키지도 말이죠. 파일과 숫자로 마법을 부릴 수 있다는 사실 그 자체가 멋진 거 아니겠어요? 이 글에서는 2~3개월 안에 완료할 수 있는 기술과 프로젝트들을 요약해서 소개할 거에요. 상상해보세요, 몇 달간의 노력만으로 첫 직장 면접 준비를 마치고 있을 수 있답니다.\n\n## 왜 데이터 엔지니어링이고 데이터 과학이 아닌가요?\n\n사실 왜 데이터 분석이나 데이터 과학이 아니고 데이터 엔지니어링을 선택하는 걸까요? 저는 이 역할의 본질에 답이 있다고 생각해요. 데이터 엔지니어가 되기 위해서는 소프트웨어 엔지니어링과 데이터베이스 디자인, 기계 학습(ML) 모델, 그리고 데이터 모델링과 비즈니스 인텔리전스(BI) 개발을 배워야 해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 엔지니어링은 DICE에 따르면 가장 빠르게 성장하는 직업입니다. 그들은 갭을 보여주기 위해 연구를 수행했습니다. 빨리 움직이세요.\n\n![이미지](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_1.png)\n\n데이터 과학자는 장기간 시장에서 \"가장 매혹적인\" 직업으로 여겨져 왔지만, 최근에 데이터 엔지니어의 부족이 있다는 것으로 보입니다. 이 분야에서 엄청난 수요가 있음을 확인할 수 있습니다. 이는 경험이 풍부하고 높은 자격을 갖춘 엔지니어들 뿐만 아니라 입문자도 포함됩니다. 지난 5년 동안 영국에서 데이터 엔지니어링은 가장 빠르게 성장하는 직업 중 하나였으며, 2023년 LinkedIn의 인기 있는 직업 목록에서 13위를 차지했습니다. 평균적으로 매주 약 4차례의 취업 면접 요청을 받습니다. 입문 데이터 엔지니어들은 훨씬 더 자주 초대될 것입니다.\n\n데이터 엔지니어링이 매우 복잡하기 때문에 급여와 혜택 package는 다른 기술 분야보다 훨씬 더 좋아보입니다. 실제로, 데이터가 잡다하고 지루한 데이터 조작 작업인 것처럼 보여 소프트웨어 엔지니어 중에는 데이터를 피하는 것을 선호하는 사람들도 많이 있습니다. 이로 인해 데이터 엔지니어링은 데이터 플랫폼 및 데이터 파이프라인 디자인 패턴을 배우려는 사람들에게 수익성 있는 목표가 되고 있습니다. 데이터 엔지니어링은 데이터 조작 및 프로세스 조정에 관한 것입니다. 데이터는 정제되고 테스트되고 승인되어 사용자에게 적시에 전달되어야 합니다. 이는 ML 및 BI가 그것을 많이 의존하는 이유입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 이야기에서는, 2~3개월 안에 달성할 수 있는 기술 세트와 가능한 프로젝트를 요약해 보려고 합니다. 상상해보세요, 몇 달 동안 적극적인 학습을 한 후에는 첫 직장 면접 준비가 완료되어 있게 됩니다.\n\n## 데이터 엔지니어링은 압도적으로 느껴질 수 있어요\n\nSTEM(과학, 기술, 공학, 수학) 배경 없이 데이터 엔지니어링에 뛰어드는 것은 매우 어려울 수 있어요. 코딩 자체가 쉬운 일이 아니에요. 데이터베이스 및 데이터 파이프라인 오케스트레이션은 처음부터 이해하기가 더욱 어려웠어요. 몇 년 전에 저는 양적 금융을 전공한 석사 학위를 받고 분석 매니저로 일했었어요. 코딩을 배우기로 결정한 날을 기억해요. 대학에서 얻은 수준처럼은 아니지만 실제 세계의 문제를 해결하기 위해 실무에서 나의 기술을 적용할 수 있도록 배우고자 한 것이죠.\n\n일상적으로 업무를 수행하면서 여가 시간에만 소프트웨어 엔지니어링을 배워야 했던 어려움이 있었던 것도 기억이 나네요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n나는 Fiverr과 PeoplePerHour에서 프로젝트를 찾아보며 기업들이 데이터에 대해 어떤 것을 필요로 하는지 살펴보았던 기억이 납니다. 지금 생각해보면 이것이 많은 고객들의 진정한 고통 포인트를 이해하는 데 많은 도움이 되었고 아마도 가장 효율적인 학습 방법이었을 것입니다.\n\n그래서 모든 데이터 실무자를 향한 첫 번째 조언은 믿음입니다.\n\n데이터 엔지니어링 분야에 진입하는 것은 압도적으로 느껴질 수 있지만 가치가 있습니다. 부끄러워하지 마시고 글을 쓰는 사람들에게 물어보세요. Medium은 그런 점에서 정말 좋은 곳입니다. 왜 취향에 맞는 주제를 확인하고 누구를 팔로우할지 확인해보지 않으세요?\n\n## 계획\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n쉬는 시간을 가져가서 진정으로 그것이 필요한지 생각해보세요. 만약 답이 '예' 라면 필요한 건 계획 뿐입니다. 이 곳에서 목표는 속도가 아닙니다. 가능하다면 아무런 고통 없이 \"데이터 엔지니어링에 진입하는 방법\"을 실현 가능한 해결책으로 기록하는 것이 이제의 목표입니다.\n\n지금은 단지 다음 몇 달에 집중하고자 하는 것과 복습해야 할 것들을 생각해봅시다.\n\n## 데이터 엔지니어링의 습관\n\n첫 두 주 동안 실제로 배우면서 이 습관을 습득하고자 할 것입니다. 조금씩 하되 꾸준히 합니다. 학습의 습관을 형성해야 합니다. 예를 들어, 저는 Google Professional Data Engineer 시험 준비를 하면서 이렇게 했습니다. 매일 아침에 체육관에서 사이클을 타면서 책을 읽었죠. 아침이 가장 생산적인 시간이기 때문에 그 때 진행했습니다. 이 2020년의 글은 여전히 유효합니다. 많은 것들이 실제로 변한 게 많지 않고, 학습은 주로 데이터 엔지니어링의 기본 원칙에 대해 집중했습니다. 물론, 제품 특정 질문이 많았지만, 이 글은 빠르게 학습하는 방법에 대한 지침서입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 엔지니어링은 다음과 같은 기술 영역들에 관련이 있어요:\n\n- ETL 및 데이터 추출\n- 데이터 조작과 데이터 모델링 (대개 SQL을 사용함)\n- 파이프라인 테스트\n- 데이터 테스트\n- 보고 및 비즈니스 인텔리전스\n- MLOps 및 기계 학습 파이프라인\n- 이 모든 것을 조율하는 것\n\n## 첫 1-2주: SQL\n\n일단 SQL에 집중해 볼게요. 목록에서 첫 번째 항목은 아니지만 저는 이것이 가장 보편적이라고 생각해요. SQL 방언은 데이터 모델링에서 널리 사용되어 왔기 때문에 이제 데이터 조작의 표준으로 간주될 수 있어요. 처음 두 주 동안 해야 할 것은 다양한 SQL 쿼리를 실행해보고 어떤 데이터 파이프라인에서 사용될 수 있는지 상상하는 거에요. 여기서 다시 정리하고 싶은 것들은 아마도 다음과 같을 거에요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- SQL을 사용하여 테이블을 만드는 방법\n- 공통 테이블 표현식을 사용하는 방법\n- SQL을 사용하여 데이터를 모킹하는 방법\n- 증분 전략을 사용하여 테이블을 업데이트하는 방법\n- 데이터 품질을 테스트하고 데이터를 정리하는 방법\n\n이러한 질문들은 압도될 수 있을지도 모르지만, 많은 훌륭하고 간단한 예제들이 있습니다. 이러한 예제들은 몇 가지 무료 데이터 웨어하우스 솔루션과 결합하여 비교적 간단하고 생산적인 샌드박스를 만드는 데 도움이 될 수 있습니다. 이에 대해 이전 이야기 중 하나에서 이야기했었습니다. SQL 관련해서는 일상적인 데이터 엔지니어링에 실제로 필요한 모든 것입니다.\n\n가장 어려운 주제인 MERGE와 같은 주제도 SQL이 CTE 내에 목된 데이터를 포함할 때 쉽게 설명될 수 있습니다:\n\n```js\ncreate temp table last_online as (\n    select 1 as user_id\n    , timestamp('2000-10-01 00:00:01') as last_online\n)\n;\ncreate temp table connection_data  (\n  user_id int64\n  ,timestamp timestamp\n)\nPARTITION BY DATE(_PARTITIONTIME)\n;\ninsert connection_data (user_id, timestamp)\n    select 2 as user_id\n    , timestamp_sub(current_timestamp(),interval 28 hour) as timestamp\nunion all\n    select 1 as user_id\n        , timestamp_sub(current_timestamp(),interval 28 hour) as timestamp\nunion all\n    select 1 as user_id\n        , timestamp_sub(current_timestamp(),interval 20 hour) as timestamp\nunion all\n    select 1 as user_id\n    , timestamp_sub(current_timestamp(),interval 1 hour) as timestamp\n;\n\nmerge last_online t\nusing (\n  select\n      user_id\n    , last_online\n  from\n    (\n        select\n            user_id\n        ,   max(timestamp) as last_online\n\n        from \n            connection_data\n        where\n            date(_partitiontime) \u003e= date_sub(current_date(), interval 1 day)\n        group by\n            user_id\n\n    ) y\n\n) s\non t.user_id = s.user_id\nwhen matched then\n  update set last_online = s.last_online, user_id = s.user_id\nwhen not matched then\n  insert (last_online, user_id) values (last_online, user_id)\n;\nselect * from last_online\n;\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 주 3–4: 현대 데이터 스택\n\n현대 데이터 스택 및 데이터 플랫폼 아키텍처 유형에 대한 몇 가지 이야기를 읽는 것을 추천합니다. 데이터 엔지니어링에서 사용할 수 있는 다양한 도구 및 프레임워크에 대한 전략적 개요를 제공하여, 채용 인터뷰 중에 기술에 능통하다는 것을 채용 담당자에게 알려줍니다. 모든 도구를 알 필요는 없지만, \"당신은 이 분야에 계십니까\"라는 질문은 잠재 고용주와의 첫 만남 중에 상상할 수 있는 가장 매혹적인 질문입니다. 여기서는 기술의 최근 이벤트(IPO, 합병 및 인수), 개발 및 새로운 도구에 대한 인식을 보여주고 싶습니다. DuckDB 또는 Polars와 같은 것에 대해 들었다고 언급하는 것만으로도 여러분이 호기심이 많고 열정적이라는 것을 사람들에게 알려줍니다.\n\n시장에 있는 다양한 데이터 도구로 쉽게 길을 잃을 수 있습니다. 눈송이(Snowflake)에 대해 이야기하고, 그 IPO가 얼마나 성공적이었는지를 언급했던 것을 기억합니다. 그것이 많은 도움이 되었거나 적어도 우리가 인터뷰어와 동일한 의견을 가졌다는 것 같아요. 우리는 현대 데이터 스택 및 그것을 현대적이고 견고하며 비용 효율적으로 만드는 요소들에 대해 토론했습니다. 간단히 말해서, 데이터를 다루기 위해 사용되는 도구 모음입니다. 데이터를 어떻게 처리할 것인가에 따라, 이러한 도구는 다음을 포함할 수 있습니다:\n\n- 관리되는 ETL/ELT 데이터 파이프라인 서비스\n- 클라우드 기반 관리되는 데이터 웨어하우스/데이터 레이크(데이터의 대상지)\n- 데이터 변환 도구\n- 비즈니스 인텔리전스 또는 데이터 시각화 플랫폼\n- 기계 학습 및 데이터 과학 기능\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n앞선 두 주 동안 SQL을 사용하여 데이터를 변환하고 조작하는 방법을 이미 배웠어요. 이제 이 전략적인 지식을 활용하는 방법을 알게 됐으니, 그에 맞게 활용해봐요.\n\n![이미지](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_2.png)\n\n## 5-6주차: 파이썬 기초\n\n지금은 파이썬에 대한 지식을 되짚거나 살짝 배우는 시기예요. 파이썬은 정말 배우기 쉬운 방법이죠. 스크립트 형식이라 코드를 읽기 쉽고 유용한 라이브러리가 많습니다. 이 모든 특징들로 인해 데이터 엔지니어링에서 프로그래밍 언어로 많이 선택되었어요. 반복문, 함수, 조건문, 오류 처리, 그리고 데이터 구조와 같은 기본적인 프로그래밍 개념에 초점을 맞출 거예요. 데이터 엔지니어링에서는 이런 것들을 자주 사용할 거라고 생각해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제안드리는 바는 데이터 API 및 요청을 통해 시작하는 것입니다. 이러한 지식을 클라우드 서비스와 결합하면 미래에 필요한 모든 ETL 프로세스를 위한 매우 좋은 기반이 마련됩니다.\n\n전형적인 데이터 파이프라인 [5]은 Python 함수(또는 오퍼레이터)의 연쇄이며 다음과 같이 보일 것입니다:\n\n\n![pipeline](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_3.png)\n\n\n우리는 Python 함수를 사용하여 데이터를 처리하며, 결과적으로 다음과 같은 파이프라인을 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_4.png)\n\nMore examples can be found here:\n\n## API requests\n\n이해API 요청 방법은 중요합니다. 이것은 ETL 서비스가 다른 서비스와 상호 작용하는 주요 방법이기 때문입니다. 즉, 데이터를 추출합니다. 데이터 엔지니어는 API 서비스에 요청을 보내어 데이터를 요청할 때 많이 사용합니다. 그런 다음 실제로 변환 (ETL)하는 데이터를 페이징하거나 스트림으로 응답합니다. 아래 예제를 고려해보십시오. 이 예제는 NASA 소행성 API에서 데이터를 추출하는 방법을 설명합니다. 이는 매우 간단한 예제이며 매우 배우기 쉬운 이유입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\n# nasa.py\nimport requests\nsession = requests.Session()\n\nurl=\"https://api.nasa.gov/neo/rest/v1/feed\"\napiKey=\"your_api_key\"\nrequestParams = {\n    'api_key': apiKey,\n    'start_date': '2023-04-20',\n    'end_date': '2023-04-21'\n}\nresponse = session.get(url, params=requestParams, stream=True)\nprint(response.status_code)\n```\n\n더 고급이고 실행 가능한 예제는 [여기 이야기](6)에서 찾을 수 있습니다.\n\n## 7–8 주차: 추출 — 적재\n\n파이썬과 SQL을 조금 배우면 실제 데이터를 추출하고 클라우드 어딘가에 저장할 수 있습니다. AWS, GCP 및 Azure와 같은 클라우드 서비스 제공업체들이 시장을 선도하고 있으며 그 중 적어도 하나에 익숙해지는 것이 필수적입니다. 그래서 이제 우리는 실제로 첫 번째 데이터 파이프라인을 만들고 싶어할 것입니다. 이것은 간단한 함수일 수 있습니다. NASA 소행성 데이터를 추출하여 AWS S3에 저장하는 것이다. 그게 다입니다! 아주 간단하지만 이것이 우리의 첫 번째 데이터 파이프라인이며 매일, 매 시간 등으로 실행되도록 예약할 수 있습니다. 서버리스 마이크로서비스로 배포하고 무료로 실행되어 클라우드 저장 공간에서 데이터를 추출 및 보존합니다. AWS 웹 UI를 사용하여 쉽게 배포할 수 있습니다. 그러나 서비스를 배포하는 더 선호되는 방법은 인프라스트럭처 코드입니다. 해당 주제는 본질적으로 이해하기 어려우며 초보자이신 경우 깊게 파고들지 않는 것이 좋습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 몇 주 동안은 명령줄 도구에 주로 초점을 맞추고 클라우드 기능을 배포하고 클라우드에서 리소스를 프로비저닝하는 몇 가지 트릭을 익히는 것을 추천합니다.\n\n간단한 AWS 람다처럼 ETL 서비스를 생성할 수 있습니다:\n\n```js\n# AWS CLI를 사용하여 패키지된 람다 배포:\naws \\\nlambda create-function \\\n--function-name etl-service-lambda \\\n--zip-file fileb://stack.zip \\\n--handler \u003c당신의 람다 핸들러 경로\u003e/app.lambda_handler \\\n--runtime python3.12 \\\n--role arn:aws:iam::\u003c당신의 AWS 계정 ID\u003e:role/my-lambda-role\n\n# # 이미 배포되었다면 업데이트를 위해 다음을 사용합니다:\n# aws --profile mds lambda update-function-code \\\n# --function-name mysql-lambda \\\n# --zip-file fileb://stack.zip;\n```\n\n예를 들어, AWS CLI를 사용하여 ETL 서비스를 호출할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\naws lambda invoke \\\n    --function-name etl-service-lambda \\\n    --payload '{ \"data\": \"value\" }' \\\n    response.json\n```\n\n일반적인 코드 예시와 람다 패키지를 사용한 예시는 여기 [7]에서 확인할 수 있습니다.\n\n이제 데이터가 클라우드에 저장되었으므로 데이터 웨어하우스 도구로 불러올 수 있습니다. 저는 1TB 데이터 스캔당 $5가 청구되는 BigQuery를 권장합니다. 테스트 데이터로 작업하게 되므로 비용이 전혀 발생하지 않습니다. 다음과 같이 외부 테이블을 만들 수 있습니다 [8]:\n\n```js\n# 데이터가 Google Cloud Storage에 저장되어 있다고 가정\nLOAD DATA INTO source.nasa_asteroids\nFROM FILES(\n  format='JSON',\n  uris = ['gs://nasa-asteroids-data/*']\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한 AWS CLI의 기본을 배우고 싶다면 이 기사를 시도해보실 수도 있습니다 [9]\n\n## 9-10주차\n\n이번에는 Python에서 단위 테스트의 기본을 배우고 ETL 서비스를 조정하는 방법에 대해 알아보고 싶어요. 이미 Python 함수를 만들어 API 호출을 수행하고 데이터를 추출하는 방법을 배웠죠. 아마도 우리는 Python for Data Engineers의 예제를 사용하여 Python에서 데이터를 변환하는 방법도 배웠을지도 모르겠어요. 이제는 적용한 데이터 변환 로직을 테스트하고 싶어요. 단위 테스트는 소프트웨어 엔지니어링에서 필수적인 기술이며 장기적으로 많은 시간을 절약해줍니다. 코드를 테스트하고 유지하는데 도움이 되죠. 요약하자면 Pytest 모듈의 기본을 배우고 싶습니다. 예를 들어, 아래와 같은 ETL 함수의 논리를 테스트할 수 있어야 합니다:\n\n```js\n# etl.py\ndef etl(item):\n    # 여기서 데이터 변환을 수행\n    return item.lower()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPython 스크립트를 실행하기만 하면 간단히 이 작업을 수행할 수 있습니다 [10]:\n\n```js\n# etl_test.py\nfrom etl import etl\n\ndef test_etl_returns_lowercase():\n    assert etl('SOME_UPPERCASE') == 'some_uppercase'\n```\n\nunittest 라이브러리를 사용한 또 다른 예시를 살펴보겠습니다:\n\n```js\n# ./prime.py\nimport math\n\ndef is_prime(num):\n    '''num이 소수인지 확인합니다.\n    '''\n    for i in range(2,int(math.sqrt(num))+1):\n        if num%i==0:\n            return False\n    return True\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nunittest을 사용하면 간단해요. 다음과 같이 테스트할 거예요:\n\n```js\n# ./test.py\nimport unittest\nfrom prime import is_prime\n\nclass TestPrime(unittest.TestCase):\n\n    def test_thirteen(self):\n        self.assertTrue(is_prime(13))\n```\n\n이제 테스팅의 기본을 알았으니, 함수들이 올바른 데이터를 반환하는지 확신할 수 있어요. 그러므로 다음 단계는 ETL 프로세스를 조정하는 추가적인 마이크로서비스를 배포하는 것이죠. 요약하면, 이는 간단한 AWS Lambda 함수거나 스케줄에 따라 우리의 ETL 서비스를 호출할 수 있는 다른 서버리스 애플리케이션이 될 수 있어요. 이것은 매우 간단하며 여기서 복잡하게 하고 싶지 않아요. 다른 Python Lambda 함수를 배포하고 매일 또는 매시간 실행되도록 스케줄을 지정할 거예요. 그것을 위해 AWS EventBridge 이벤트를 사용하고 cron 스케줄을 설정할 수 있어요. 우리의 Orchestator Lambda 코드는 아래와 같이 보일 거에요.\n\n```js\nimport json\nimport boto3\n \n# 다른 Lambda를 호출하기 위한 AWS Lambda 클라이언트\nclient = boto3.client('lambda')\n \ndef lambda_handler(event, context):\n \n    # 다른 Lambda로 전달할 데이터\n    data = {\n        \"ProductName\": \"iPhone SE\"\n    }\n \n    response = client.invoke(\n        FunctionName='arn:aws:lambda:eu-west-1:12345678:function:etl-service-lambda',\n        InvocationType='RequestResponse',\n        Payload=json.dumps(data)\n    )\n \n    response = json.load(response['Payload'])\n \n    print('\\n')\n    print(response)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 더 알고 싶다면, AWS Step Functions 및 Infrastructure as Code와 관련된 고급 튜토리얼이 있습니다.\n\n## 11–12 주차\n\nML 기본 지식을 배우세요. 데이터를 추출하고 ETL을 수행하는 방법 및 데이터 웨어하우스로 데이터를 로드하는 방법을 이미 알고 있습니다.\n\n아래 튜토리얼을 살펴보세요. 이 튜토리얼은 사용자 이탈을 다루고 행동 데이터를 사용하여 사용자 이탈을 예측하는 방법을 설명합니다. 몇 시간 만에 완료할 수 있지만 이탈에 대한 구체적인 내용을 깊게 파고들고 싶다면 더 많은 시간이 걸릴 수 있습니다. 모든 머신 러닝 모델을 알 필요는 없습니다. 우리는 머신 러닝 및 데이터 과학 분야에서 아마존 및 구글과 경쟁할 수는 없지만, 사용하는 방법을 알아야 합니다. 클라우드 서비스 제공 업체들이 제공하는 다양한 관리형 ML 서비스가 있으며, 그것들에 친숙해져야 합니다. 데이터 엔지니어들은 이러한 서비스를 위해 데이터 세트를 준비하며, 이에 대해 몇 개의 튜토리얼을 진행하는 것이 유용할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 결론\n\n데이터 엔지니어링을 빠르게 학습하고 이 분야의 전문가가 되어야 한다고 자신을 누르지 마세요. 많은 사람들에게는 특정 분야를 숙달하는 데 몇 년이 걸리기 때문에 주말에 학습하면서 몇 달 안에 이룰 수 있는 목표에 집중하는 것이 좋습니다. 데이터 엔지니어는 ETL/ELT 기술, 데이터 모델링에 대한 충분한 지식이 필요하며 적어도 Python에서 코딩할 수 있어야 합니다. 이 글에서는 데이터 엔지니어링을 가장 효율적으로 배울 수 있는 12주 계획을 개요로 설명했습니다. 즐기시기 바랍니다.\n\n## 추천 도서\n\n[1] https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[2] [Google Professional Data Engineer 시험을 2020년에 통과한 방법](https://towardsdatascience.com/how-i-passed-google-professional-data-engineer-exam-in-2020-2830e10658b6)\n\n[3] [초보자를 위한 고급 SQL 기술](https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488)\n\n[4] [데이터 플랫폼 아키텍처 유형](https://towardsdatascience.com/data-platform-architecture-types-f255ac6e0b7)\n\n[5] [데이터 파이프라인 디자인 패턴](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n[6] [Python for Data Engineers](https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd)\n\n[7] [Building a Batch Data Pipeline with Athena and MySQL](https://towardsdatascience.com/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c)\n\n[8] [When Your Stack is a Lake House](https://medium.com/towards-artificial-intelligence/when-your-stack-is-a-lake-house-6bcb17f9bff6)\n\n[9] [Mastering AWS CLI](https://medium.com/geekculture/mastering-aws-cli-5454ad5e685c)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[10] [Python을 사용한 데이터 파이프라인 테스트 안내](https://towardsdatascience.com/a-guide-to-data-pipeline-testing-with-python-a85e3d37d361)\n\n[11] [데이터 파이프라인 Orchestration](https://towardsdatascience.com/data-pipeline-orchestration-9887e1b5eb7a)","ogImage":{"url":"/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_0.png"},"coverImage":"/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_0.png","tag":["Tech"],"readingTime":14}],"page":"19","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"19"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>