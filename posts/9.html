<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/9" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/9" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/960f1fe994a0ab5c.css" as="style"/><link rel="stylesheet" href="/_next/static/css/960f1fe994a0ab5c.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-2d104a861d88ea21.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/kNTo-t2jvQG5kfHDWIcB-/_buildManifest.js" defer=""></script><script src="/_next/static/kNTo-t2jvQG5kfHDWIcB-/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="사용 기반 API 요금 청구 및 미터링을 위한 실시간 분석 솔루션" href="/post/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="사용 기반 API 요금 청구 및 미터링을 위한 실시간 분석 솔루션" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="사용 기반 API 요금 청구 및 미터링을 위한 실시간 분석 솔루션" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">사용 기반 API 요금 청구 및 미터링을 위한 실시간 분석 솔루션</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 폭풍 속을 해마하는 여정 정교한 레이크하우스 플랫폼 구축하기" href="/post/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 폭풍 속을 해마하는 여정 정교한 레이크하우스 플랫폼 구축하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 폭풍 속을 해마하는 여정 정교한 레이크하우스 플랫폼 구축하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 폭풍 속을 해마하는 여정 정교한 레이크하우스 플랫폼 구축하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">13<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="DBT 증분 전략과 동등성" href="/post/2024-05-27-DBTIncrementalStrategyandIdempotency"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="DBT 증분 전략과 동등성" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-DBTIncrementalStrategyandIdempotency_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="DBT 증분 전략과 동등성" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">DBT 증분 전략과 동등성</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 품질 관리의 과거, 현재, 그리고 미래 2024년에 알아야 할 테스트, 모니터링, 그리고 데이터 관찰 가능성" href="/post/2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 품질 관리의 과거, 현재, 그리고 미래 2024년에 알아야 할 테스트, 모니터링, 그리고 데이터 관찰 가능성" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 품질 관리의 과거, 현재, 그리고 미래 2024년에 알아야 할 테스트, 모니터링, 그리고 데이터 관찰 가능성" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 품질 관리의 과거, 현재, 그리고 미래 2024년에 알아야 할 테스트, 모니터링, 그리고 데이터 관찰 가능성</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="어떻게 Apache Airflow에서 2000개 이상의 DBT 모델을 조율하는지" href="/post/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="어떻게 Apache Airflow에서 2000개 이상의 DBT 모델을 조율하는지" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="어떻게 Apache Airflow에서 2000개 이상의 DBT 모델을 조율하는지" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">어떻게 Apache Airflow에서 2000개 이상의 DBT 모델을 조율하는지</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="이미지를 클러스터링하는 방법" href="/post/2024-05-27-HowtoClusterImages"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="이미지를 클러스터링하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-HowtoClusterImages_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="이미지를 클러스터링하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">이미지를 클러스터링하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">13<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="칠레의 공공 안전 문제의 정치적 측면" href="/post/2024-05-27-ThepoliticaldimensionofthepublicsecurityprobleminChile"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="칠레의 공공 안전 문제의 정치적 측면" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-ThepoliticaldimensionofthepublicsecurityprobleminChile_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="칠레의 공공 안전 문제의 정치적 측면" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">칠레의 공공 안전 문제의 정치적 측면</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="시간에 따른 변화를 시각화하는 멋진 전략들" href="/post/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="시간에 따른 변화를 시각화하는 멋진 전략들" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="시간에 따른 변화를 시각화하는 멋진 전략들" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">시간에 따른 변화를 시각화하는 멋진 전략들</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="따뜻한 기후에서 온 러너들이 더 더운 봄 마라톤에서 더 잘 달리나요" href="/post/2024-05-27-DoRunnersFromWarmClimatesDoBetterinHotSpringMarathons"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="따뜻한 기후에서 온 러너들이 더 더운 봄 마라톤에서 더 잘 달리나요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-DoRunnersFromWarmClimatesDoBetterinHotSpringMarathons_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="따뜻한 기후에서 온 러너들이 더 더운 봄 마라톤에서 더 잘 달리나요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">따뜻한 기후에서 온 러너들이 더 더운 봄 마라톤에서 더 잘 달리나요</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="NBA 네트워크 분석 Neo4j를 활용한 연결하기" href="/post/2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="NBA 네트워크 분석 Neo4j를 활용한 연결하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="NBA 네트워크 분석 Neo4j를 활용한 연결하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">NBA 네트워크 분석 Neo4j를 활용한 연결하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><a class="link" href="/posts/1">1</a><a class="link" href="/posts/2">2</a><a class="link" href="/posts/3">3</a><a class="link" href="/posts/4">4</a><a class="link" href="/posts/5">5</a><a class="link" href="/posts/6">6</a><a class="link" href="/posts/7">7</a><a class="link" href="/posts/8">8</a><a class="link posts_-active__YVJEi" href="/posts/9">9</a><a class="link" href="/posts/10">10</a><a class="link" href="/posts/11">11</a></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"사용 기반 API 요금 청구 및 미터링을 위한 실시간 분석 솔루션","description":"","date":"2024-05-27 12:58","slug":"2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering","content":"\n\n![Real-Time Analytics Solution](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_0.png)\n\nDisclaimer: The author of this article is a Developer Advocate at Redpanda, which is a critical component of the solution discussed. The author also brings prior expertise in API Management and Apache Pinot to the table. Hence, the proposed solution is a combination of these technologies aimed at solving a prevalent problem.\n\nAn API business refers to a company that packages its services or functionalities as a set of API (Application Programming Interface) products. These APIs can be sold to new and existing customers, who can then integrate these functionalities into their own applications. The company can generate revenue by charging these customers based on their usage of the APIs.\n\nA company operating an API business needs a data infrastructure component to track API call volume and bill consumers accordingly.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 게시물에서는 Apache APISIX, Redpanda 및 Apache Pinot를 사용하여 실시간 API 사용 추적 솔루션을 구축하기 위한 참조 아키텍처를 제시합니다. 이 게시물은 \"어떻게\"보다는 \"왜\"에 중점을 두었습니다. 이를 솔루션 설계 연습으로 간주하고 심층 튜토리얼이 아니라는 것을 고려해 주세요. 저는 솔루션 패턴을 청사진으로 추출하여 향후 프로젝트에서 재사용할 수 있도록 돕고자 합니다.\n\n다른 말 없이 시작해 봅시다.\n\n# APIs 및 API 관리\n\nAPI 및 API 관리 개념에 대해 처음이라면, 먼저 간단히 소개해 드리겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n디지털 비즈니스에서 API는 비즈니스 작업에 프로그래밍 방식으로 액세스할 수 있도록 해줘서 인간을 제외할 수 있어요. 이러한 비즈니스 작업에는 주문 생성, 자금 이체, CRM에서 고객 주소 업데이트 등이 포함될 수 있어요.\n\n비즈니스에서 전형적인 API 환경에는 세 가지 당사자가 관련돼요:\n\n- 백엔드 시스템 — 비즈니스 작업을 실행하는 시스템이에요.\n- 소비자 — 비즈니스 작업을 사용하려는 1차 및 3차 애플리케이션이에요.\n- API 프록시 — 프록시로서 작용하며 중간자 역할을 하는 요소에요.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAPI의 역할은 내부 비즈니스 시스템을 소비자로부터 분리하여, 소비자가 백엔드 시스템의 복잡성을 처리할 필요 없이 이를 제공하는 것입니다. 이러한 방식으로, API는 추상화 계층 역할을 합니다. API는 HTTP를 포함한 다양한 통신 프로토콜을 통해 작동하며, RESTful 및 GraphQL API 스타일을 볼 수 있습니다.\n\n운영 중에는 조직이 API 수명주기의 다양한 단계를 관리하기 위해 전체 수명주기 API 관리 플랫폼을 사용합니다. API 프록시 디자인, 배포, 런타임 정책 참여 및 모니터링과 같은 API 수명주기의 각 단계에 대한 전용 구성 요소를 번들로 제공하는 API 관리 플랫폼이 있습니다. 이 문서의 문맥에서 Apache APISIX를 사용할 것이며, 이는 Apache 라이선스 하에 배포되는 오픈 소스 API 관리 플랫폼입니다.\n\n그렇다고 해서 여기서 구축하는 솔루션이 APISIX와 무조건적으로 결합된 것은 아닙니다. 시장에서 구할 수 있는 대부분의 전체 수명주기 API 관리 제품과 통합할 수 있습니다. 단, 적합한 통합 인터페이스를 제공해야 합니다.\n\n![Real-Time Analytics Solution for Usage-Based API Billing and Metering](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_2.png)\n\n번역 시 일부 용어는 컨텍스트에 맞게 번역되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# API를 활용하여 수익을 창출하는 방법\n\n이제, API를 활용하여 수익을 창출하는 방법에 대해 살펴봅시다. 즉, 사용량에 따라 소비자에게 요금을 부과하는 수익 모델을 찾아보는 것입니다.\n\n더 나은 설명을 위해 현실적인 예시를 들어보겠습니다.\n\n부동산 감정 회사를 고려해보세요. 이 회사는 주택 구매자와 판매자에게 즉각적인 부동산 평가를 제공합니다. 이 평가는 우편번호, 부동산 유형, 지역과 같은 간단한 요소를 기반으로 합니다. 현재, 이 회사는 웹 기반 사용자 인터페이스만 제공하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_3.png)\n\n비지니스 운영을 확장하고 더 많은 고객을 유치하며 새로운 시장 세그먼트에 진입하기 위해 이 회사는 API 비지니스로 진출하기로 결정했습니다. 이 말은 평가 엔진을 API 제품 세트로 패키징하여 새로운 및 기존 소비자에게 판매하고 그들의 API 호출 사용량에 따라 청구할 것을 의미합니다.\n\n이를 위해 먼저 평가 엔진을 분리하고 API 관리 플랫폼 뒤에 배치하여 달성합니다. 이를 통해 소비자들이 일련의 API를 통해 기능에 액세스할 수 있게 됩니다.\n\n![image](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_4.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n평가 API는 부동산 회사, 은행, 보험사, 정부 등 다양한 분야의 잠재 고객들을 유치할 것입니다:\n\n- 부동산 회사 — 주택 구매자를 위한 정확한 평가값 제공.\n- 은행 — 모기지 승인 전 주택 평가.\n- 보험 제공업자 — 주택 및 내용 보험에 대한 더 정확한 견적 제공.\n- 정부 — 부동산 세금 쉽게 계산.\n\n## API 수익화 모델\n\n이 회사는 어떻게 수익을 창출할까요? 평가 API를 API 제품 세트로 포장하여 구독 계층과 함께 판매하세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가입 등급은 소비자가 매달 고정된 API 호출 횟수를 사용할 수 있는 할당량입니다. 그 할당량을 초과하면 사용자는 제한을 받거나 초과 사용량에 대해 요금을 지불해야 합니다.\n\n예를 들어, 가치평가 API는 다음과 같이 세 가지 가입 등급으로 제공될 수 있습니다.\n\n- 브론즈: 매달 10,000건의 요청\n- 골드: 매달 100,000건의 요청\n- 플래티넘: 매달 무제한의 요청\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n고객은 예상 사용량에 따라 다양한 티어 중에서 선택하여 API에 가입할 수 있습니다. 한 달의 끝에 회사는 실제 사용량을 기반으로 고객에게 청구할 것입니다.\n\n우리의 목표는 각 고객의 API 사용량을 효율적이고 신뢰할 수 있는 방법으로 측정하는 것입니다.\n\n# 솔루션 계획\n\n이제 우리가 해결하려는 문제를 이해했으니, 구현에 들어가기 전에 몇 가지 설계 결정사항을 설명해 드리겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## KPI 지표\n\n첫 번째 단계는 솔루션으로부터 기대하는 KPI 또는 지표를 식별하는 것입니다. 특히 다음 다섯 가지에 관심이 있습니다.\n\n- API 사용량 — 시간당 소비자 당 API 호출 횟수\n- API 지연 — 느린, 느린 API를 식별하기 위한 종단 간 지연 시간\n- 고유 사용자 — API 당 고유 호출 수는?\n- 지리적 사용량 분포 — 주로 API 사용자가 어디에서 왔는가?\n- 오류 수 — 호출 오류가 많으면 백엔드에 문제가 있다는 것을 의미합니다.\n\n이상적으로 이런 것들이 모두 이렇게 시각화된 대시보드에서 보고 싶습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 마크다운 형식으로 표시 변환한 코드입니다.\n\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_6.png)\n\n## 이해 관계자\n\n두 번째 디자인 결정은 솔루션 이해 관계자 - 이러한 지표를 전달해야 하는 대상. 주로 세 가지 당사자가 있습니다.\n\n고객 및 협력사 - 소비자는 실시간 대시 보드에서 할당량 사용량과 청구 추정을 확인하는 것을 좋아합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAPI 운영 팀 - 이 팀은 API 관리 인프라를 관리합니다. API의 건강 정보에 특히 관심이 있으며, 지연시간, 처리량, 오류 등을 주로 다룹니다.\n\nAPI 제품 팀 - 이 팀은 API 제품을 소유하고 있습니다. 그들은 소비자의 인구 통계, API 중에서 더 인기 있는 것이 무엇인지 등을 확인하기 위해 즉시 실험을 실행하고 싶어 합니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_7.png)\n\n## 일괄 처리 또는 실시간 처리?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최종 디자인 결정으로, 실시간 및 일괄 메트릭 사이에 80:20의 분할을 설정하겠습니다.\n\n데이터는 시간이 경과함에 따라 가치를 잃습니다. 데이터를 빨리 처리할수록 적절한 조치를 취할 수 있습니다. 그래서 우리는 API 트래픽 및 헬스 메트릭을 실시간으로 처리하겠습니다.\n\n소비자 API 키가 유출된 상황을 생각해보세요. 악의적인 API 클라이언트가 훔친 키나 인증 토큰을 사용하여 소비자를 대신해 API를 호출할 수 있습니다. 시스템은 이 API 키에서의 급격한 트래픽 증가를 감지하여 비정상으로 식별하고 키를 차단하면서 소비자에게 경보를 보낼 수 있습니다. 경보를 받은 소비자는 즉시 API 키를 재발급하여 비용을 최소화할 수 있습니다.\n\n그러나 모든 사용 사례가 실시간 처리를 필요로 하는 것은 아닙니다. 어떤 사용 사례는 자연스럽게 일괄 처리에 적합합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 고객을 위한 월별 사용량에 기반한 청구 보고서.\n- 업무팀을 위한 주간 API 건강 보고서.\n- 제품팀을 위한 매일 API 트래픽 보고서.\n\n# 구현\n\n지금은 이 기사의 중간 지점에 도달했으며 지금까지의 토론을 바탕으로 다음 솔루션 아키텍처를 제시합니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_8.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다이어그램이 복잡하고 많은 알 수없는 기술이 있어서 알겠습니다. 그래서, 세 개의 레이어로 나누어서 각각에 대해 자세히 설명하겠습니다.\n\n## 데이터 수집\n\n이전에 언급했듯이, API 관리 시스템은 디자인 및 런타임 측면에서 트래픽 모양 만들기, 인증 및 구독 관리와 같은 API 라이프사이클 관리 작업을 수행하는 여러 이동 부품을 가지고 있습니다. 이에 대한 추가 측면도 있습니다.\n\n![image](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_9.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나, 우리가 가장 관심 있는 구성 요소는 API 게이트웨이입니다. 이 곳은 모든 API 트래픽이 백엔드로 흐르는 곳입니다.\n\n우리의 첫 번째 작업은 API 게이트웨이에서 접촉점을 확인하는 것입니다. 이를 통해 왕복하는 API 요청과 응답을 수집할 수 있습니다. 그런 다음 이 정보를 실시간으로 분석용 데이터 저장소로 이동시키는 데이터 파이프라인을 구축할 것입니다. 이를 통해 향후 질의를 용이하게 할 것입니다.\n\n그러나, 이 쓰기 경로를 구현할 때 직접 데이터를 기본 데이터 저장소에 쓰는 것은 여러 문제점을 야기할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAPIM 시스템과 분석 인프라 사이의 강한 결합 - 나중에 새 APIM 공급업체로 전환할 때 분석 인프라의 상당 부분을 다시 작성해야 할 수도 있습니다.\n\n동기 쓰기 - 운영 중에 두 시스템 모두 사용 가능해야 하므로 분석 시스템을 유지보수 목적으로 중지하기 어려울 수 있습니다.\n\n확장 가능한 데이터 수집 - API 게이트웨이의 돌발적인 트래픽 급증으로 인해 분석 시스템이 과부하되어 두 시스템 모두 협조하여 확장해야 할 수 있습니다.\n\n이로 인해 APIM과 분석 인프라를 분리하는 방법을 모색할 필요가 있습니다. Apache Kafka와 같은 스트리밍 데이터 플랫폼은 API 게이트웨이에서 높은 처리량 데이터 스트림을 낮은 지연 시간으로 수신할 수 있으므로 여기에 적합할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해당 솔루션에서는 성능과 간편함 측면에서 Kafka보다 우월한 Redpanda, Kafka API 호환 스트리밍 데이터 플랫폼을 사용할 것입니다. 하지만 만약 Kafka만 사용하길 원한다면 괜찮습니다. 해당 솔루션은 두 기술에 모두 매끄럽게 작동합니다.\n\nRedpanda를 중심으로 한 데이터 파이프라인은 다음과 같이 구성됩니다:\n\n![Redpanda Data Pipeline](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_11.png)\n\nRedpanda의 추가로 두 시스템이 분리되었고 쓰기 경로가 비동기로 동작합니다. 이는 분석 시스템이 유지 보수를 위해 오프라인 상태로 들어갈 수 있고, 중단된 지점부터 다시 재개할 수 있도록 합니다. 게다가 Redpanda는 갑작스러운 트래픽 급증을 흡수하여 분석 시스템이 과부하를 받거나 API 게이트웨이에 맞춰 스케일링할 필요가 없도록 해줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 APISIX와 Redpanda 사이의 연결을 어떻게 만들어야 할지 궁금할 것입니다. 다행히도, APISIX는 Kafka를 위한 내장 데이터 싱크를 제공합니다. 게이트웨이로 API 요청이 발생하고 응답이 반환될 때, 이 싱크는 실시간으로 Kafka 토픽에 레코드를 발행합니다. 우리는 Redpanda와 함께 이 싱크를 사용할 수 있습니다. 왜냐하면 Redpanda가 Kafka API와 호환되기 때문입니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_12.png)\n\nAPISIX는 개별 API 호출을 JSON 이벤트로 형식화하고 지연 시간, HTTP 상태 및 타임스탬프와 같은 중요한 메트릭을 포함시킵니다. 이러한 정보들은 분석 데이터 저장소에서 관련 있는 차원으로 매핑될 것입니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_13.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAPI 관리 플랫폼에 Kafka 싱크가 없는 경우 어떻게 할까요? 그럼 대안으로 API 게이트웨이의 HTTP 액세스 로그를 Kafka로 스트림 처리할 수도 있습니다. 이를 위해 Filebeat나 유사한 도구를 사용할 수 있습니다.\n\n## 분석 데이터베이스\n\n이제 Redpanda에 API 호출 이벤트가 랜딩되고 있으니, 다음 단계는 적합한 분석 데이터베이스 기술을 식별하는 것입니다.\n\nOLTP 데이터베이스, 키-값 저장소 또는 데이터 웨어하우스가 될 수 있을까요? 다음 기대 기준에 따라 각각을 평가해 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 스트리밍 데이터 수집 - 실시간 데이터 원본인 Kafka에서 가져와야 합니다. 여기서는 배치 데이터 로딩이 없어야 합니다. 스트리밍 수집은 더 높은 데이터 신선도를 보장합니다.\n- 낮은 지연 쿼리 - 쿼리 지연 시간은 하위 초 범위 내여야 하며 사용자를 위한 분석 대시보드를 만족시켜야 합니다.\n- 높은 쿼리 처리량 - 사용자를 대상으로 하는 분석 대시보드에서 동시에 발생하는 쿼리를 처리할 수 있어야 하며 지연 시간을 훼손하지 않아야 합니다.\n\n위의 모든 조건을 충족하는 분석 데이터베이스로 Apache Pinot을 선택했습니다.\n\nApache Pinot은 실시간 분산 OLAP 데이터베이스로, 스트리밍 데이터에서 OLAP 워크로드를 처리하도록 설계되어 극히 낮은 지연 시간과 높은 동시성을 제공합니다. Pinot은 Kafka와 원활하게 통합되어 Kafka 주제에서 데이터가 생성될 때마다 실시간 수집을 허용합니다. 수집된 데이터는 색인이 작성되고 열 형식으로 저장되어 효율적인 쿼리 실행을 가능케 합니다.\n\n아키텍처에서 Pinot을 사용하면 엔드 투 엔드 데이터 파이프라인은 다음과 같이 보입니다. Pinot은 Redpanda와 API 호환성으로 원활하게 통합됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_14.png\" /\u003e\n\n파이프라인에 스트림 프로세서가 필요한가요? 정말 필요한지는 사용 사례에 따라 다릅니다.\n\n스트림 프로세서 대신 Redpanda의 Wasm 기반 인브로커 데이터 변환을 사용하여 API 이벤트 페이로드에서 민감한 필드를 제거할 수 있습니다. 그러나 아파치 Flink와 같은 상태 보유형 스트림 프로세서는 다음과 같을 때 파이프라인에 더 많은 가치를 더할 수 있습니다:\n\n- 실시간 결합 및 보강이 필요할 때 — 핀오토로 전달할 추가 차원이 필요하며, 이는 여러 스트림을 결합하여 파생할 수 있습니다. 예: IP 지오코딩.\n- 알림 — 사용량의 이상 현상을 기반으로 알림을 트리거하고 하류 이벤트 주도형 워크플로를 실행합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Serving layer\n\n우리의 분석 데이터 파이프라인이 이제 완성되었습니다. 모든 파이프라인 구성 요소는 데이터 인프라 구조층에 있습니다. 필요하다면 Pinot 쿼리 콘솔에 액호크 SQL 쿼리를 실행하여 메트릭을 생성할 수 있습니다.\n\n그러나 솔루션의 모든 이해 관계자/사용자가 그렇게 하길 원하는 것은 아닙니다. 우리는 각 사용자 그룹에게 메트릭을 직관적이고 편안하게 찾을 수 있는 방식으로 제시해야 합니다. 이것이 우리가 분석의 마지막 단계인 서빙 레이어를 구현하는 곳입니다.\n\n\u003cimg src=\"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_15.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리의 우선순위는 소비자들입니다. 그들은 사용량과 청구 예상을 시각화하는 실시간 대시보드가 필요합니다. 이를 위해 Streamlit과 같은 프레임워크를 활용하여 Python 기반 데이터 어플리케이션을 개발할 수 있습니다. Pinot Python 드라이버 pinotdb를 사용하면 애플리케이션과 Pinot 쿼리 환경을 연결할 수 있습니다.\n\nBI 및 즉석 탐색이 필요한 사용자 그룹, 특히 API 제품 소유자는 Tableau와 Apache Superset과 같은 선호하는 BI 도구를 연동할 수 있는 Pinot의 ODBC 인터페이스를 사용할 수 있습니다.\n\n일괄 작업을 위해 Pinot는 Presto나 Trino와 같은 쿼리 연합 엔진에 Pinot 커넥터를 통해 연동할 수 있습니다.\n\n# 요약\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 파이프라인 구현 단계 순서를 나열하여 글을 마무리해 봅시다.\n\n1. Redpanda 클러스터를 프로비저닝하고 토픽을 생성하고 ACL을 설정합니다.\n2. APISIX에서 Kafka 싱크를 구성합니다.\n3. Pinot 스키마와 테이블을 생성합니다.\n4. 필요에 따라 데이터를 가공합니다.\n5. 대시보드를 생성하거나 연결합니다.\n\n이 솔루션은 비즈니스 자체에서 호스팅하고 관리하는 자체 호스팅 배포 모델을 전제로 합니다. 그러나 동일한 설계 원칙이 이 도구들의 호스팅 버전을 선택해도 적용될 수 있다는 점을 알아두는 것이 중요합니다. 아키텍처의 각 구성 요소는 호스팅 서비스로 대체될 수 있으며, 이를 통해 다양한 배포 전략에 대응할 수 있는 유연한 해결책이 될 수 있습니다.\n\n이전에 언급했듯이, 이 글은 \"어떻게\"보다는 \"왜\"에 대해 주로 다루고 있습니다. 목표는 정확한 실행 방법보다는 근본적인 해결책 패턴을 이해하는 것입니다. 이 글을 다음 실시간 분석 프로젝트의 청사진으로 삼아 보세요. 필요에 따라 다른 기술을 통합하여 조정할 수 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_0.png"},"coverImage":"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_0.png","tag":["Tech"],"readingTime":11},{"title":"데이터 폭풍 속을 해마하는 여정 정교한 레이크하우스 플랫폼 구축하기","description":"","date":"2024-05-27 12:55","slug":"2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform","content":"\n![이미지](/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_0.png)\n\n목요일 오후 늦은 시간, 우리는 깨달음을 얻었습니다. 어두운 사무실에서 깜박이는 화면들을 둘러싸고, 두 명의 예리한 데이터 분석가 동료와 데이터 과학자로 구성된 헌신적인 팀, 그리고 나, 데이터 엔지니어는 PostgreSQL과 MySQL 데이터베이스에서 데이터를 조인하려고 깊이 파고들고 있었습니다. 이 작업은 간단할텐데 어쩌면 서로 다른 부서에서 오는 불일치된 데이터 구조와 충돌하는 스키마로 애를 쓰고 있었습니다. 이러한 이질적인 데이터 세트를 수동으로 맞추려고 할수록 복잡성이 압도되는 느낌이었습니다. 혼돈스러운 정보 동기화 시도마다 실패할 때마다 공기는 분노로 더 두꺼워졌습니다. \"이렇게 일할 수는 없어,\" 살짝 중얼거렸던 저의 목소리엔 스트레스가 느껴졌습니다. 우리의 현재 시스템이 현실에 부합하지 않다는 것은 명백했습니다—우리는 흩어진 데이터 정복 뿐만 아니라 이 넓은 데이터 정글을 이해할 수 있는 통합 플랫폼이 필요했던 것입니다. 이 순간이 우리에게 중대한 변화가 필수적이라는 것을 알게 된 시점이었고, 큰 변화가 곧 찾아올 것임을 알게 된 시점이었습니다.\n\n이 여정을 시작하면서, 저희는 우리 회사의 의사 결정 프로세스의 기반이 되는 견고한 데이터 플랫폼 아키텍처를 만들었습니다. 이 블로그 글에서는 다양한 데이터를 단일하고 강력한 분석 엔진으로 통합하는 것뿐만 아니라 지속적으로 발전하고 시간이 지남에 따라 더 많은 데이터 소스를 통합하는 유연하고 확장 가능한 데이터 인프라를 구축하는 과정에서 우리가 직면한 인사이트와 도전에 대해 탐구할 것입니다.\n\n대용량의 원시 데이터를 원래 형식 그대로 저장할 수 있는 유연성으로 기업들에게 빠르게 적응하고 효율적으로 확장할 수 있는 도구로써 데이터 레이크가 부각되었습니다. 그러나 계속 진행함에 따라 데이터 관리에 더 구조적인 접근이 필요하다는 것을 깨달았고, 그로 인해 레이크하우스 구조를 채택하게 되었습니다. 이 하이브리드 모델은 데이터 레이크의 확장성과 유연성을 데이터 웨어하우스의 관리 기능과 결합하여 데이터 전략을 향상시킵니다. 이 이야기는 이러한 기술을 활용하기 위해 우리가 취한 실질적인 단계를 살펴보며, 데이터 레이크를 레이크하우스 프레임워크로 통합함으로써 데이터 주도 기업에게 혁신적인 자산이 될 수 있는 방법을 밝힐 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 레이크하우스 이전의 데이터 전략\n\n레이크하우스를 개발하기 전에, 데이터 관리는 간단했지만 우리의 요구사항이 증가함에 따라 비효율적으로 변했습니다. 처음에는 BI용 Apache Superset을 사용하여 3개의 주요 데이터 소스를 관리했는데, 처음에는 최소한의 복잡성으로 우리의 요구사항을 충족시켰습니다.\n\n그러나 우리의 데이터 요구사항이 증가함에 따라 시스템의 한계가 나타나기 시작했습니다. 두 가지 다른 소스에서 데이터를 조인해야 할 필요가 발생했을 때 중대한 도전이 발생했습니다. 당시 우리의 솔루션은 매우 효율적이지 못했습니다: 필요한 데이터를 한 소스에서 다른 소스로 수동으로 복제했습니다. 이 프로세스는 시간이 많이 걸릴 뿐만 아니라 데이터를 동기화하기 위해 빈번한 업데이트가 필요했기 때문에 오류를 발생시킬 가능성도 있었습니다.\n\n또한 다양한 팀과 프로젝트가 발전함에 따라 Superset 내에서 여러 데이터셋이 생성되었는데, 각각이 특정한 분석 요구에 맞게 조정되었습니다. 불행히도, 이로 인해 여러 데이터셋에 중복 변환 요소가 코딩되어 복잡성이 증가했을 뿐만 아니라 이러한 변환을 유지하고 업데이트하는 것이 점점 더 부담스러워졌습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 데이터 아키텍처 결정: Lake, Warehouse 또는 Lakehouse?\n\n저희 데이터 인프라에 적합한 아키텍처를 선택하는 것은 중요한 결정이었습니다. 세 가지 주요 옵션 중에서 선택을 고민했습니다: 데이터 레이크, 데이터 웨어하우스 및 레이크하우스. 각각에 대한 간단한 개요를 살펴보겠습니다:\n\n• **데이터 레이크**: 데이터 레이크는 원시 형식으로 방대한 양의 데이터를 저장합니다. 다양한 소스에서 대량의 다양한 데이터를 처리하는 데 이상적이며 높은 유연성과 확장성을 제공합니다. 그러나 구조화된 데이터 환경의 처리 효율성 일부가 부족합니다.\n\n• **데이터 웨어하우스**: 이것은 질의 및 분석에 최적화된 구조화된 형식으로 데이터를 저장하는 시스템입니다. 데이터 웨어하우스는 구조화된 데이터에 대한 빠른 쿼리 성능에 뛰어나지만 변경사항 및 새로운 데이터 유형의 수용에 있어서 덜 유연할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n• Lakehouse: 데이터 레이크와 데이터 웨어하우스의 장점을 결합한 하이브리드 모델입니다. 데이터 레이크의 넓은 저장 공간과 유연성을 제공하면서 데이터 웨어하우스의 효율적인 쿼리 기능을 갖추고 있습니다.\n\n신중한 고려 끝에 저희는 여러 가지 이유로 레이크하우스 아키텍처를 도입하기로 결정했습니다:\n\n1. 유연성: 레이크하우스 아키텍처는 필요한 적응성을 제공했습니다. 기존의 데이터 웨어하우스는 새로운 데이터 소스나 유형을 빠르게 통합하는 것이 어려워 변경에 제한이 있고 느립니다.\n\n2. 단순화된 아키텍처: 처음에는 기존의 ETL 프로세스를 데이터 웨어하우스와 데이터 레이크에 별도로 구축하는 것을 고려했지만, 두 개의 별도 시스템을 유지할 명확한 이유를 찾지 못했습니다. 레이크하우스 모델은 강력한 쿼리 및 저장 기능을 하나의 보다 관리하기 쉬운 시스템으로 통합한 간소화된 접근 방식을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 주요 구성 요소에 대한 개요입니다:\n\nOur Data Lakehouse stack\n\n우리의 레이크하우스 아키텍처는 AWS 기술과 오픈 소스 솔루션의 최선을 활용하여 데이터를 효율적으로 관리하고 분석하는 것을 목표로 합니다.\n\n이미지를 Markdown 형식으로 변경했습니다.\n\n\n![Lakehouse Overview](/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_1.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**데이터 저장:**\n\n우리는 데이터 저장을 위해 AWS S3를 활용하며, 환경을 개발 및 프로덕션용 2개의 전용 버킷으로 구성합니다. 메달리온 아키텍처를 채택하여 각 버킷 내에 bronze, silver 및 gold 세 가지 독립적인 레이어(폴더)를 설정했습니다. 각 레이어는 데이터 관리 수명주기에서 특정 목적을 제공합니다. 메달리온 아키텍처는 데이터를 세 개의 레이어로 분류하는 레이크하우스 시스템에 사용되는 계층화된 데이터 처리 모델입니다:\n\n- Bronze Layer (Raw Layer): 이 기본 레벨에서는 다양한 소스로부터 도착한 대로 데이터를 정확히 저장하여 JSON, CSV 등의 원래 형식으로 보존합니다. 이 레이어는 주로 데이터 엔지니어링 팀이 디버깅 및 데이터 무결성 확인을 위해 액세스하는 데 중요하며 데이터 과학자가 초기 인사이트를 얻고 데이터 품질을 측정하기 위해 탐색 분석을 시작하는 중요한 역할을 합니다. 초기 인사이트는 더 나은 데이터 처리 전략을 안내하는 데 중요합니다.\n- Silver Layer (Cleansed Layer): 데이터가 실버 레이어로 이동하면 필요한 클렌징 및 변환 프로세스를 수행합니다. 여기서 우리는 불일치를 수정하고 데이터를 풍부하게하여 구체적인 비즈니스 규칙을 적용하여 구조화되고 유용하게 만듭니다. 우리의 분석 엔지니어는 이 클렌징 된 데이터와 작업하여 복잡한 변환을 실행하고 내부 분석을 이끄는 자세한 보고서를 생성합니다. 더 나아가 이 레이어는 우리의 데이터 과학자가 정교한 모델을 구축하는 데 의존하는 정돈된 데이터 환경을 제공합니다.\n- Gold Layer (Aggregated Layer): 이 곳에서 데이터는 가장 높은 가치를 얻으며, 비즈니스 수준의 집계 및 핵심 성능 지표로 변환됩니다. 신속한 검색 및 고속 분석을 위해 최적화된 골드 레이어는 주로 의사 결정자를 위해 액세스됩니다. 이들은 회사 전반의 전략 및 운영에 영향을 미치는 실행 가능한 인사이트를 위해 정제된 데이터에 의존합니다. 더불어 이 레이어는 기업 수준의 보고서 및 대시보드의 기반 역할을 합니다.\n\n실버 및 골드 레이어에서는 쿼리 성능을 최적화하기 위해 데이터 파일을 Parquet 파일로 저장합니다. Parquet의 효율적인 열 지향 저장 형식 덕분에 쿼리 성능이 최적화됩니다. 또한, 이러한 Parquet 파일 위에서 Apache Iceberg를 활용하여 레이크하우스 아키텍처에 여러 가지 중요한 기능을 제공합니다. Apache Iceberg를 사용하면 데이터 레이크를 전통적인 데이터베이스처럼 다루되 더 큰 유연성과 확장성을 갖습니다. 스냅샷, 트랜잭션, 업서트 및 삭제와 같은 복잡한 작업을 지원함으로써 데이터 레이크를 더 동적이고 다재다능한 시스템으로 변환할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 카탈로그:\n\n저희의 데이터 카탈로그 관리에는 비용 효율성, 다른 AWS 서비스와의 깊은 통합, 그리고 직관적인 메타데이터 관리 기능으로 인해 AWS Glue Catalog를 선택했습니다. AWS Glue Catalog는 중앙 메타데이터 저장소로 기능하며, 이를 통해 각종 AWS 서비스 간의 데이터 자산을 보다 쉽게 관리하고 접근할 수 있습니다. AWS Glue Crawler를 활용하여 S3에 저장된 데이터를 자동으로 발견하고 분류하여 데이터 카탈로그 테이블을 손쉽게 생성하고 업데이트할 수 있습니다.\n\n하지만 AWS Glue Catalog는 운영 요구에 맞게 데이터 검색을 용이하게 하는 측면에서 제한이 있음을 인지하고 있습니다. 잘 통합되어 비용 효율적이지만 세련된 데이터 카탈로그의 세부 기능 중 일부를 지원하지 않습니다. 특히 대규모 데이터 작업에 필수적인 향상된 검색 및 발견 도구와 같은 기능을 지원하지 않습니다. 이는 모델링을 위해 다양한 데이터세트에 빠르게 액세스해야 하는 데이터 과학자, 비즈니스 결정에 신속한 통찰을 얻어야 하는 데이터 분석가, 그리고 철저한 데이터 탐색에 의존하여 포괄적인 보고서를 작성하는 비즈니스 인텔리전스 전문가를 포함한 여러 팀 멤버에 영향을 줄 수 있습니다. 앞으로는 저희 조직의 중요한 역할들의 요구를 충족시키기 위해 데이터 검색을 지원하는 더 편리하고 포괄적인 데이터 카탈로그 솔루션을 탐구할 계획입니다.\n\n데이터 액세스 및 쿼리 엔진:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAWS Athena는 주요 쿼리 엔진으로 사용되며 AWS Glue 카탈로그와 원활하게 통합됩니다. 이 간편한 설정을 통해 데이터 레이크를 효과적으로 쿼리할 수 있어 Athena는 우리 데이터 아키텍처의 중요한 구성 요소입니다. Athena를 사용하는 주요 장점 중 하나는 비용 효율성입니다. Athena는 쿼리 중 스캔된 데이터 양에 따라 요금이 부과되기 때문에 현재 우리의 쿼리는 과도한 데이터 양을 스캔하지 않아 비용을 상당히 낮게 유지할 수 있었습니다.\n\n그러나 우리는 데이터 레이크 사용을 확대함에 따라(특히 애플리케이션 내 차트를 통한 직접 데이터 쿼리 통합이 예정된) Athena와 관련된 비용이 증가할 수 있다는 점을 알고 있습니다. 이러한 잠재적 시나리오에 대비하기 위해 Trino로 전환을 고려 중이며, 이는 EKS에서 실행되어 AWS Glue 메타스토어에 연결될 것입니다. Athena와 Trino 사이의 기본적인 유사성으로 인해 이 마이그레이션은 간단할 것으로 예상됩니다.\n\n현재 Athena에서 두 가지 서로 다른 워크그룹을 활용하고 있습니다 - SQL 쿼리를 위한 하나와 Spark(Python) 연산을 위한 다른 하나입니다. 앞으로, 우리는 이러한 설정을 세분화하여 변환, 고객 분석 등과 같은 다양한 비즈니스 요구에 대해 별도의 워크그룹을 생성하여 운영 효율성과 비용 관리를 향상시킬 계획입니다.\n\n데이터 거버넌스:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAWS Lake Formation은 저희 데이터 거버넌스에 중요한 역할을 합니다. 레이크하우스 아키텍처에서 데이터 보안과 권한 관리를 크게 향상시킵니다. 이는 PHI 및 민감한 데이터를 다루는 데 핵심적인 엄격한 접근 제어를 시행하는 데 도움이 됩니다.\n\n강력한 접근 제어를 위한 LF-Tags 구현: 데이터가 안전하게 액세스되고 엄격한 정책을 준수하는 데 필요한 접근 권한을 정교하게 제어하기 위해, 우리는 데이터베이스 및 테이블 수준에서 권한을 세밀하게 제어하기 위해 LF-Tags를 활용합니다. 우리의 태그 전략은 체계적으로 설계되어 있으며, 데이터베이스는 일반적으로 태그가 지정되며, 더 구체적인 요구 사항에 따라 테이블 수준에서 권한을 관리합니다. 우리가 사용하는 가능한 태그에는 다음과 같은 것들이 있습니다:\n\n- 환경: dev, prod\n- 부서: app, internal, devops, hr, customers, infra, sales, ds\n- PHI: true\n- 데이터 레이크 레이어: gold, silver, bronze\n- 클라이언트 대면: true (데이터를 고객에게 노출할 수 있는지 여부를 나타냄)\n\n각 데이터베이스와 테이블에 여러 태그를 적용하여 세밀한 역할 기반 접근 제어를 가능하게 합니다. 예를 들어, 우리 애플리케이션에서 데이터를 쿼리하는 고객을 위한 역할은 client_facing: true, data_lake_layer: gold, 그리고 environment: prod와 같은 태그 조합을 통해 액세스 권한을 부여받을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최초에 AWS Lake Formation을 설정하는 것은 간단하지 않았습니다. 이 플랫폼은 강력하지만 직관적이지 않았고, 권한 행동을 우리의 거버넌스 요구에 맞게 조정하는 데 상당한 노력과 시간이 걸렸습니다. 이러한 도전을 극복하기 위해서는 가파른 학습 곡선이 필요했는데, 다양한 구성을 실제로 실험해야 했고, 우리가 필요로 하는 상세한 액세스 제어를 효과적으로 구현하고 관리하는 방법을 이해해야 했습니다.\n\n![이미지](/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_2.png)\n\n데이터 수집:\n\n우리는 단순하게 사용할 수 있고 기관 전체의 데이터를 수집해야 할 필요가 있는 미래의 요구를 예측하여 다양한 커넥터를 지원하는 플랫폼을 찾았습니다. 두 플랫폼인 Airbyte(오픈 소스 솔루션)와 Rivery(SaaS 솔루션)을 비교하는 POC를 진행한 후, 몇 가지 설득력있는 이유로 Airbyte를 선택했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저, 저희의 결정은 데이터 양에 따라 청구되지 않는 비용 효율적인 솔루션에 더 기울였습니다. 저희는 증가하는 비용을 걱정하지 않고 자유롭게 데이터를 가져오길 선호했습니다. 게다가 Airbyte의 개발 도구는 특히 인상적이었습니다. 플랫폼의 Connector Builder SDK는 Connector Builder UI와 로우코드 커넥터 개발 환경이 모두 포함되어 있어 필요한 간편함과 유연성을 제공했습니다. 이 기능 덕분에 우리는 우리의 특정 요구에 맞게 데이터 커넥터를 쉽게 구축하고 맞춤화할 수 있었습니다.\n\nAirbyte는 대규모이자 활발한 커뮤니티를 자랑하지만 제품에 몇 가지 어려움을 겪었습니다. 처음에는 데이터 수집 속도가 느렸습니다. 특히 PostgreSQL에서 20GB를 2일 이상으로 전송하는 데 시간이 걸렸습니다. 먼저 AWS-data-lake 목적지를 사용해 보았지만 느리고 지속적인 동기화를 지원하지 않았습니다. 이 문제를 해결하기 위해 이 문제를 고치기위한 pull request를 제출했지만 3개월이 걸렸습니다. 더 나은 해결책을 찾기위해 여러 다른 목적지를 실험해 보았습니다. Parquet을 사용할 때 타임스탬프가 struct로 형식화되는 짜증나는 문제가 있는 S3 목적지가 있었습니다. 이 구체적인 문제는 2년째 해결되지 않은 상태로 있는데, 이는 지원 측면에서 중요한 차이점을 보여줍니다. 유망한 Iceberg 목적지는 AWS Glue Catalog를 지원하지 않았습니다. 그래서 AWS-Glue 목적지를 시도했지만 JSON 출력만 지원해서 비효율적이라는 것을 발견했습니다.\n\n최종적으로 이러한 옵션 중 어느 것도 우리의 요구 사항을 완전히 충족시키지 못했기 때문에, 우리는 자체적으로 사용자 정의 AWS-data-lake 목적지를 개발하기로 결정했습니다. 우리는 원래 코드를 복제하고 우리의 요구 사항에 맞게 특별히 맞춤화하여 데이터 수집 프로세스를 크게 향상시킨 맞춤형 솔루션을 만들었습니다.\n\n이러한 어려움에도 불구하고 Airbyte는 효과적으로 우리의 요구 사항을 모두 충족시켰습니다. 오늘날, Airbyte를 사용하여 약 15가지 다른 데이터 소스를 데이터 레이크에 성공적으로 통합했으며, 데이터 수집 능력과 전체 데이터 전략을 크게 향상시켰습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 처리:\n\n저희의 데이터 처리 워크플로우는 dbt Core에 의해 강력하게 주도되며, ELT (추출, 로드, 변환) 접근 방식을 사용합니다. 모든 데이터가 브론즈층에서 시작되어 점진적으로 실버층과 골드층으로 변환됩니다.\n\n저희는 dbt Athena 어댑터를 사용하고 있으며, SQL 및 Python (PySpark) 모델을 지원합니다. 이 다양성은 더 복잡한 변환을 효과적으로 처리하는 데 중요합니다. dbt-Athena 어댑터는 활기찬 커뮤니티의 혜택을 받고 있으며, 정기적인 업데이트로 계속 발전하고 있습니다. 처음에는 Python Athena 통합을 채택하는 데 약간 주저했었는데, 그 당시의 혁신성과 제한된 추적 레코드 때문이었습니다. 그러나 철저한 테스트와 유효성 검사를 거친 후에는 어떠한 문제도 발생하지 않았고, 안정성과 효율성을 확인하며 우리의 프로덕션 환경에 성공적으로 구현했습니다.\n\ndbt에서 테이블 속성을 구성하는 것은 직관적이고 유연하며, 우리의 데이터 관리 능력을 크게 향상시킵니다. 예를 들어, 증분 테이블을 널리 사용하는데, 이는 새 데이터 또는 변경된 데이터만 효율적으로 가져오는 데 중요합니다. Iceberg 테이블 형식을 활용하여 병합 증분 전략을 채택하면 데이터세트를 중복 처리 없이 원활하게 업데이트할 수 있습니다. 또한, dbt에서 데이터 파티션을 관리하는 것도 간단해졌습니다. 파티션은 테이블 속성 내에서 직접 선언할 수 있습니다. 아래는 우리의 골드층 테이블에 대한 테이블 속성 구성 예시입니다. 우리가 재료화 전략, 파티셔닝 및 데이터 형식을 어떻게 지정하는지 보여주고 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n{ config(\n    materialized='incremental',\n    incremental_strategy='merge',\n    unique_key=['session_id'],\n    partitioned_by=['day'],\n    table_type='iceberg',\n    format='parquet'\n) }\n\n\n우리는 dbt에 대해 매우 만족하고 있습니다; 이 도구는 우리의 데이터 변환을 어떻게 관리하는지에 혁명을 일으켰습니다. 이 도구는 견고한 버전 관리, 코드의 재사용성 및 데이터 흐름의 명확한 문서화를 제공하여 복잡한 변환 작업을 관리하고 레이크하우스 아키텍처 전반에서 데이터 무결성을 유지하는 것을 크게 간소화합니다.\n\ndbt와의 성공을 토대로, 우리는 현재 데이터 관리 역량을 더욱 강화하기 위해 새로운 도구를 탐색 중입니다. Montara.io가 강력한 기능 세트를 제공하여 워크플로우를 최적화하는 우리의 dbt Git 리포지토리와 직접 통합되었습니다. Montara는 자동 CI/CD, 팀원들이 dbt 전문 지식이 적은 경우에도 모델을 작성하고 테스트할 수 있는 사용자 친화적인 UI를 제공하며 데이터 계보 표시, 데이터 카탈로그 및 관찰 가능성과 같은 가치 있는 도구를 제공합니다.\n\nMontara에 감명을 받았습니다; 이 도구는 우리의 dbt 워크플로우를 크게 간소화시켜 팀 전체에서 데이터 변환을 보다 접근 가능하고 관리하기 쉽게 만듭니다. 이 도구가 비교적 새로운 것이며 여전히 발전 중이므로 가끔씩 일부 문제와 기능의 빈틈을 겪기도 하지만, 우리의 경험은 전반적으로 매우 긍정적입니다. Montara 팀은 우수한 지원을 제공하며 우리와 긴밀히 협력하여 발생하는 어려움을 신속하게 해결하고 계속되는 제품 향상에 우리의 피드백을 통합합니다. 이 협력적인 접근은 문제를 신속하게 해결할 뿐만 아니라 Montara.io가 우리의 데이터 인프라 요구와 완벽하게 일치하도록 발전하도록 보장합니다.\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n분석 및 BI 도구:\n\nApache Superset은 저희가 선택한 분석 및 비즈니스 인텔리전스 도구로, 오픈 소스와 강력한 데이터 시각화 능력으로 유명합니다. 다른 BI 도구와 비교했을 때 유연성과 비용 효율성을 강점으로 삼아 Superset을 선택했습니다. 다양한 사용자 정의 옵션과 사용자 친화적 인터페이스를 통해 우리 팀은 대시보드와 보고서를 자신들의 필요에 맞게 맞춤화할 수 있으며, 특히 Athena를 주 데이터 원본으로 사용하는 저희 독특한 분석 환경에 특히 적합합니다.\n\n데이터 분석가들은 저희 회사의 다양한 부서를 위한 대시보드를 만들기 위해 주로 Superset을 사용합니다. 더불어, Superset의 기능을 활용하여 차트를 애플리케이션에 직접 임베드하여 고객에게 유용한 통찰을 제공합니다.\n\n현재, 일부 차트는 브론즈 계층의 데이터에 직접 접근하여 실시간으로 변환 작업을 수행합니다. 그러나 더 이상 원시 데이터 쿼리의 부하를 줄이기 위해 이 접근 방식을 수정 중이며, 최종적으로 LF-tags를 사용하여 브론즈 계층의 액세스를 제한할 계획입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSuperset은 대시보딩에 편리하고 효과적인 도구라고 생각하지만, 시간이 지남에 따라 생성된 데이터셋이 증가하면 어느 정도 어수석해질 수 있습니다. Superset의 각 데이터셋은 개별적으로 구성되어 있어 대시보드의 수와 복잡성이 증가함에 따라 중복과 관리 도전이 발생할 수 있습니다. 그럼에도 불구하고, 이러한 어려움에도 불구하고, Superset은 우리의 요구 사항을 잘 충족시켜 주며 조직 전반에서 데이터를 시각화하고 상호 작용하는 다재다능한 플랫폼을 제공합니다.\n\n오케스트레이션과 워크플로우 관리\n\nApache Airflow는 데이터 환경 내에서 워크플로우를 조정하고 관리하는 데 중요한 역할을 합니다. 오픈 소스 도구인 Airflow는 유연성, 확장성, 그리고 강력한 커뮤니티 지원을 제공하여 우리의 운영 요구에 필수적인 요소를 제공합니다. Airflow를 활용하여 모든 데이터 파이프라인이 데이터 레이크로 정확하게 트리거되어 데이터의 신선도와 신뢰성을 유지하도록 합니다.\n\n현재, 저희는 저희 레이크하우스 운영에 필수적인 세 가지 주요 DAGs (방향이 있는 비순환 그래프)를 관리하고 있습니다. 첫 번째 DAG는 AirbyteOperator를 활용하여 동기화를 위해 필요한 모든 업무를 트리거하여 브론즈 레이어에 데이터를 효율적으로 삽입하는 작업을 담당합니다. 두 번째 DAG는 dbt 변환을 실행하여 데이터를 처리하고 실버 및 골드 레이어로 옮기는 업무를 담당합니다. 세 번째 DAG는 전체 워크플로를 감독하며 데이터 처리의 원활한 흐름을 유지하기 위해 순차적으로 삽입 DAG 및 이후에 dbt DAG를 트리거합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한, 이러한 워크플로우 내에 Slack 알림을 통합했습니다. 이 설정은 DAG(작업 방향성 비순환 그래프) 실패 시 실시간 알림을 제공하여 지속적인 운영 및 데이터 무결성 유지를 위해 즉각적인 모니터링과 대응이 가능하게 합니다.\n\n## 결론: 전략적 이점을 위한 데이터 활용\n\n마지막으로, 데이터 레이크하우스 아키텍처를 구축하고 정제하는 우리의 여정은 도전적이고 보람찼습니다. 우리는 데이터 관리 역량을 혁신한 여러 도구와 기술을 성공적으로 통합하여, 다양한 데이터를 통합하여 동적 의사 결정을 지원하는 견고한 분석 엔진으로 변화시켰습니다. Apache Superset, AWS S3, AWS Glue Catalog, Apache Airflow, 그리고 dbt를 활용한 우리의 사용은 복잡한 데이터 과제에 대응하기 위해 첨단 기술을 채용하는 데 드러난 우리의 의지를 보여줍니다.\n\n이러한 도구들은 우리의 운영 효율을 향상시키는데 그치지 않고 회사 전반에서 더 통찰력있는 데이터 분석과 보고의 길을 열었습니다. 우리의 데이터 인프라를 계속 발전시키면서, 우리는 데이터 능력을 더욱 향상시킬 수 있는 새로운 기술과 방법을 탐구하는 데 헌신하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 비슷한 여정을 걸어가고 있는 독자들로부터의 피드백과 질문을 환영합니다. LinkedIn에서 저와 연락하셔서 더 자세한 토론을 나누거나 아이디어를 교환해 주세요.\n\n","ogImage":{"url":"/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_0.png"},"coverImage":"/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_0.png","tag":["Tech"],"readingTime":13},{"title":"DBT 증분 전략과 동등성","description":"","date":"2024-05-27 12:53","slug":"2024-05-27-DBTIncrementalStrategyandIdempotency","content":"\n\n![Screenshot](/assets/img/2024-05-27-DBTIncrementalStrategyandIdempotency_0.png)\n\n# 배경\n\n안녕하세요, 저는 데이터 엔지니어인 Todd입니다. 저는 Nowcast에서 데이터 온보딩에 주로 관여하고 있습니다. 이 기술 블로그에서는 Nowcast에서의 ETL 파이프라인 디자인의 간략한 역사를 소개하고, Airflow와 DBT의 \"Incremental Models\" 사이에서 발생한 문제를 설명하고 우리가 개발한 해결책을 소개하겠습니다.\n\n# Python으로 ETL\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n역사적으로 Nowcast에서는 ETL 파이프라인을 Python을 사용하여 작성했습니다. 이 파이프라인은 AWS S3, Athena, RDBMS 등에 저장된 데이터에 변환을 적용하는 많은 Python 스크립트로 구성되어 있었습니다. 우리는 이러한 스크립트를 포함하는 도커 이미지를 작성하여 ECR에 업로드하고, Airflow에서 ECS 작업을 호출했습니다. 이러한 스크립트는 보통 데이트와 같은 파티션 필드를 매개변수로 사용하여 멱등성이 있도록 설계되었습니다. 즉, 2024-01-01을 전달하면 2024-01-01의 데이터가 처리되었습니다.\n\n이러한 스크립트 중 하나를 호출할 때, 실제로 실행되는 명령은 아래와 같이 보일 것입니다. 이때 데이트 매개변수는 Airflow에서 관리됩니다:\n\n```js\npython transform_data.py 2024-01-01 --some --other --arguments\n```\n\n# Airflow\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAirflow은 Nowcast에서 많은 해동안 사용되어온 스케줄링 및 워크플로우 관리 도구입니다. 기본적으로 두 가지로 사용되고 있어요:\n\n1. 작업 스케줄러\n2. 작업 의존성 관리\n\n역사적으로 Airflow는 매일 실행되며 여러 Python 스크립트에 '실행 날짜' 매개변수를 전달하여 데이터를 처리합니다. 문제가 발생하거나 특정 기간의 작업을 다시 실행해야 할 때는 Airflow DAG에서 해당 작업을 다시 실행할 수 있습니다. 예를 들어, 2024년 01월 01일에 어떤 데이터 변환 스크립트가 실패하면 문제를 식별하고 수정한 후 해당 스크립트를 다시 실행할 수 있어요. 이는 스크립트가 한 번에 하나의 파티션만 처리하고 날짜를 매개변수로 입력받기 때문에 가능한 일입니다.\n\n# DBT에서 ETL\n\n2022년 말쯤 Python ETL 플로우를 Snowflake로 이전하기 시작했습니다. 그 결과 더 빠르고 저렴하며 깨끗한 파이프라인이 만들어졌어요. 우리는 파이프라인 실행 도구로 DBT를 사용하기로 결정했습니다 — DBT는 SQL 위에 위치한 레이어로 DB 모델 정의, 템플릿, 의존성 관리 및 데이터 회귀 테스트와 같은 다양한 기능을 포함하고 있어요. 빠르고 효율적으로 ETL 파이프라인을 구축하는 데 매우 유용한 도구입니다. 파이썬에서는 파이프라인의 각 변환을 스크립트로 작성하지만, DBT에서는 템플릿화된 SQL CTAS 쿼리로 작성됩니다. 이 쿼리들은 복잡한 수천 줄의 코드로 이루어진 스크립트와 비교했을 때 매우 읽기 쉽습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBest practise in DBT is to use Incremental Models:\n\n# Incremental Models\n\nIncremental models are an efficient way of defining how to (incrementally) add data to our SQL models — consider we have a table that describes credit card transactions — we can make a DBT model (CTAS) that looks something like this:\n\n```js\n{\n materialized=\"table\"\n}\nselect\n transaction_id,\n transaction_date,\n user_id,\n store_name_description,\n transaction_amount\nfrom { ref('external_table_transaction') }\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 코드는 table external_table_transaction에서 거래 데이터를 불러오는 테이블을 만듭니다. 문제는이 쿼리를 다시 실행할 때마다 전체 테이블을 다시로드한다는 것입니다. 테이블에 데이터가 많아질수록 쿼리가 느려지고 비용이 많이 발생합니다. 이 문제의 해결책은 증분 모델을 사용하는 것입니다:\n\n```js\n{\n config(\n materialized=\"incremental\",\n unique_key=[\"transaction_id\"],\n incremental_strategy=\"delete+insert\",\n )\n}\nselect\n transaction_id,\n transaction_date,\n user_id,\n store_name_description,\n transaction_amount\nfrom { ref('external_table_transaction') }\n {- if is_incremental() }\n where transaction_date = (select max(transaction_date) + 1 as next_date from { this })\n{- endif }\n```\n\n여기에서 우리는 DBT를 강력하게 만드는 일부 매크로/템플릿 기능을 볼 수 있습니다. 이제 기본적으로 하는 것은 테이블의 최신 데이터보다 1일이 더 늦은 거래 데이터만 external_table_transaction에서 로드해야 한다는 것입니다. 이것은 간단하면서도 강력합니다. 업데이트마다 계속 커지는 수십억 개의 데이터 행 처리 대신 이제 이전에 볼 수 없던 행만 처리하면 됩니다. 그리고 필요하다면 전체 갱신으로 테이블을 다시로드할 수 있는 옵션도 있습니다.\n\n# 문제\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n점진적 모델은 매우 매력적입니다 — 수학적으로 아름답고 데이터 스트림을 다룰 때 매우 잘 작동합니다. 문제는 처리하려는 데이터를 제어해야 할 때 발생합니다 — 점진적 모델은 특정 파티션만 다시 실행할 수 없으며 대신에 증분 모델의 규칙에 따라 데이터를 로드합니다. 이론적으로는 문제가 되지 않을 수도 있지만, 점진적 모델이 이상적인 환경에서 실행된다면 모든 데이터가 정확히 한 번만 로드될 것입니다 — 하지만 현실은 복잡합니다 — DAG가 깨지고, 데이터가 늦게 전달되거나 아예 제공되지 않는 경우가 발생하며 때로는 역사적 기록을 다시 로드해야 할 때가 있습니다. 게다가 Airflow 파이프라인이 어떤 이유로든 실패할 경우 DBT 작업이 Airflow 실행과 동기화되지 않을 수 있습니다. Nowcast로 마이그레이션한 이후 DBT를 사용하면서 경험한 점진적 모델과 관련된 이슈 목록이 아래에 나와 있습니다:\n\n- 한 파이프라인에서 수리가 진행된 것이 있었는데, 이는 2년 전으로 거슬러 올라가야 했으므로 역사적 데이터를 로드해야 했는데 (증분) 데이터 파이프라인이 역사적 재실행을 처리할 수 없어서 즉시 처리해야 했습니다.\n- 다른 DAG에서 상류 이슈로 3일 동안 깨졌으며, 3일 동안 데이터가 로드되지 않았고, DAG가 4일째 실행될 때 1일부터 데이터를 로드했으므로 동기화가 맞지 않았습니다.\n- 세 번째 파이프라인에서 상류 스킵 날짜(데이터가 빠진 날)가 발생했고, 점진적 모델은 데이터를 로드하기 위해 데이터에서 최대 날짜에 `1`을 추가하는 방식으로 처리했으나 해당 날짜가 나타나지 않아 데이터가 로드되지 않은 채로 수동 처리가 필요해졌습니다.\n\n하지만 우리는 단순히 점진적 모델을 포기할 수 없습니다 — 일부 파이프라인은 수십억 개의 행을 처리해야 하므로, 테이블을 대량으로 처리할 쿼리를 작성하면 느리고 비용이 많이 소요될 것입니다.\n\n# 동형성(idempotency) 및 분할의 중요성.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n점진적 모델의 주요 문제는 이 모델이 멱등성을 갖지 않으며 특정 파티션에 대해 실행 구성이 불가능하다는 것입니다. 우리가 ETL 파이프라인에 대해 예전에 채택한 방식은 멱등 스크립트가 여러 번 다시 실행할 수 있는 횟수에 제한이 없는 것이었습니다. 과거 데이터에 문제가 발생하면 특정 파티션을 다시 생성할 수 있었고, 스크립트가 멱등성을 가졌기 때문에 특정한 날짜를 여러 번 실행해도 문제가 발생하지 않았습니다. 하지만 점진적 모델은 데이터의 특정 파티션을 다시 실행할 수 있는 능력이 없으며, 대신 모든 데이터를 스트림처럼 처리하여 보지 않은 데이터만을 로드합니다. 다시 말해 특정 규칙을 충족하는 데이터를 로드하는 것이죠.\n\n우리가 Airflow라는 스케줄링 도구를 사용하고 있기 때문에 데이터 파이프라인은 어떤 종류의 시간적 분할과 일치해야 합니다. 시간별, 일별, 주별, 월별 등 다양한 분할 방식이 될 수 있지만 중요한 점은 Airflow가 어떤 일정에 따라 실행되고 있다는 것입니다. 만약 과거 Airflow 작업을 다시 실행한다면 해당 작업을 호출할 때 해당하는 시간적 파티션에 맞게 실행되기를 기대하지만, 점진적 모델은 항상 앞으로만 '보기' 때문에 과거의 파티션에 대해 구성되지 않습니다. 이것은 Airflow에서 작업을 실행할 때 예상하는 것과는 다릅니다.\n\n하루마다 실행되는 2개의 Airflow DAG를 고려해보죠. 하나의 DAG는 매개변수로 날짜를 사용하여 해당 파티션만 실행하는 작업을 가지고 있습니다. 다른 DAG는 점진적 모델을 사용하며 실행할 때 보지 않은 데이터를 처리합니다. 둘 다 정상적으로 실행될 때 이전에 보지 못한 일별 데이터를 처리하게 되며 두 DAG는 동일하게 동작합니다. 하지만 문제가 발생하여 특정 날짜인 2024년 1월 1일을 다시 로드해야 할 때는 어떨까요? 파티션화된 DAG는 예상대로동작하여 2024년 1월 1일을 다시 실행할 것이지만, 점진적 모델은 Airflow에 전달되는 날짜와 관계없이 이전에 본 적 없는 데이터만을 로드할 것입니다.\n\n점진적 모델의 한계에 대해 논평한 댓글에서는:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단히 말하면 - Airflow와 같은 일정 관리 도구를 사용할 때 시간 분할을 기대하는 경우, 점진적 모델이 잘 작동하지 않습니다.\n\n# 해결책\n\n해결책은 간단합니다 - DBT 변수를 사용할 수 있습니다. 또한 점진적 모델의 기능을 완전히 포기할 필요가 없습니다. 하나 이상의 변수를 추가하여 하나 이상의 분할에 명시적으로 실행할 수 있습니다:\n\n```js\n{- set target_date = var(\"target_date\", \"\") }\n{\n config(\n materialized=\"incremental\",\n unique_key=[\"transaction_id\"],\n incremental_strategy=\"delete+insert\",\n )\n}\nselect\n transaction_id,\n transaction_date,\n user_id,\n store_name_description,\n transaction_amount\nfrom { ref('external_table_transaction') }\n{- if target_date != \"\" }\n where transaction_date = '{ target_date }'\n{- else }\n {- if is_incremental() }\n where transaction_date = (select max(transaction_date) + 1 as next_date from { this })\n {- endif }\n{- endif }\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 DBT 모델에 `target_date`라는 새 매개변수를 추가합니다. `target_date`가 정의되지 않은 경우 모델은 증분 동작으로 실행되지만, 변수가 전달된 경우 지정된 파티션에 대해 실행됩니다. 이 모델 구조화 방식은 Airflow에서 호출될 때 훨씬 더 잘 작동합니다.\n\n게다가, 이 모델은 이제 멱등성이 생겼습니다. 즉, 원본 데이터가 동일한 경우 동일한 쿼리와 매개변수로 실행하고 동일한 결과를 얻을 수 있습니다. 반면 증분 모델의 경우 로드된 데이터는 테이블 내용 및 상위 스트림에서 발생한 변경 내용에 따라 달라집니다.\n\n이 솔루션은 병렬, 증분 및 파티션화의 3가지 모드를 효과적으로 제공합니다. 따라서 Airflow와 DBT의 의도된 증분 전략과 잘 어울리며 이를 사용할 경우 잘 작동합니다. 아래와 같이 인수 없이 DBT를 실행하면 증분 모델을 사용할 것입니다:\n\n```js\ndbt run --select my_model\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n명시적으로 새로 고침을 실행하면 대량 적재가 발생합니다:\n\n```js\ndbt run --select my_model --full-refresh\n```\n\n그리고 추가한 target_date 매개변수를 전달하면 특정 파티션에 대해서만 실행되도록 할 수 있습니다:\n\n```js\ndbt run --select my_model  --vars \"{target_date : '2024-01-01'}\"\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 Airflow가 전달되는 날짜 매개변수를 제어할 수 있는 명령으로 돌아왔어요. 이렇게 하면 훨씬 더 부드러운 통합이 가능해요!\n\n# 참고 자료\n\n이 문제를 연구하는 데 사용된 다음 문서들입니다:\n\nDBT — 증분성의 한계에 대해\nMedium — DBT와 Airflow를 사용한 멱등데이터 파이프라인\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Nowcast의 엔지니어링\n\n만약 DBT에서 데이터 파이프라인을 구축하는 방법에 대해 알고 싶으면 아래 링크를 사용하여 친목을 돈 미팅을 예약해보세요. '문의 사항'란에 'Todd와 이야기하고 싶어요'라고 작성해주세요.\n\nNowcast는 현재 데이터 엔지니어를 채용 중입니다! 관심이 있으시면 [여기에서 지원하세요](application_link).\n","ogImage":{"url":"/assets/img/2024-05-27-DBTIncrementalStrategyandIdempotency_0.png"},"coverImage":"/assets/img/2024-05-27-DBTIncrementalStrategyandIdempotency_0.png","tag":["Tech"],"readingTime":7},{"title":"데이터 품질 관리의 과거, 현재, 그리고 미래 2024년에 알아야 할 테스트, 모니터링, 그리고 데이터 관찰 가능성","description":"","date":"2024-05-27 12:51","slug":"2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024","content":"\n\n## 데이터 환경이 진화하고 있으며, 데이터 품질 관리도 함께 발전해야 합니다. 다음은 AI 시대에 데이터 품질 관리가 향하는 방향과 세 가지 일반적인 접근 방식에 대한 정보입니다.\n\n![이미지](/assets/img/2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024_0.png)\n\n서로 다른 용어일까요? 같은 문제에 대한 독특한 접근 방식일까요? 아니면 다른 것일까요?\n\n그리고 더 중요한 것은 — 모두 세 가지가 정말 필요한가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 엔지니어링에서처럼, 데이터 품질 관리도 초속으로 진화하고 있어요. 기업에서 데이터와 AI의 급부상으로 인해, 현대 비즈니스에 있어 데이터 품질은 제로 데이 위험이 되었고 데이터 팀이 해결해야 할 핵심 문제가 되었어요. 중첩 용어가 많아서 어떻게 모두 맞는지 또는 맞는지 여부가 항상 명확하지 않아요.\n\n그러나 몇몇이 주장하는 것과는 달리, 데이터 품질 모니터링, 데이터 테스트 및 데이터 가시화는 데이터 품질 관리에 대한 대안적인 접근 방식도 아니고, 상충되는 것도 아니에요. 이것들은 하나의 해결책의 보완적 요소들이에요.\n\n이 글에서, 이 세 가지 방법론의 구체적인 내용, 각각이 어디에서 가장 잘 작동하며, 어디서 약점이 있는지, 그리고 2024년에 데이터 신뢰를 증진할 수 있는 데이터 품질 실무를 최적화하는 방법에 대해 살펴볼게요.\n\n# 현대 데이터 품질 문제 이해하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 솔루션을 이해하기 전에 문제를 이해해야 합니다. 시간이 지남에 따라 어떻게 변화했는지 알아야 합니다. 다음 유사성을 고려해 봅시다.\n\n상상해보세요. 당신이 지역 수도 공급을 책임지는 엔지니어라고 상상해봅시다. 당신이 이 직무를 맡을 때, 그 도시에는 단 1,000 명의 주민이 있었습니다. 그러나 도시 아래에 금이 발견되자, 당신의 1,000 명 주민들의 작은 커뮤니티가 1,000,000 명의 진정한 도시로 변모했습니다.\n\n이것이 당신이 하는 일에 어떻게 영향을 미칠까요?\n\n먼저, 작은 환경에서는 실수 포인트가 상대적으로 적습니다. 파이프가 고장나면, 근본 원인을 냉동 파이프, 누군가가 수도관에 파고들면, 일반적인 몇 가지 원인 중 하나로 좁힐 수 있고, 1~2 명의 직원이 리소스로 문제를 빠르게 해결할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n100만 명의 신규 거주민을 디자인하고 유지하기 위한 뱀과 같은 파이프라인, 수요 충족을 위해 필요한 광란스러운 속도, 그리고 팀의 한계적인 능력(및 가시성) 때문에 예상했던 모든 문제를 찾아 해결하거나 감시해야 할 수 있는 능력이 더 이상 동일하지 않습니다. \n\n현대 데이터 환경도 마찬가지입니다. 데이터 팀은 금광을 발견했고 이해 관계자들은 그 발전 상황에 참여하고 싶어합니다. 데이터 환경이 커질수록 데이터 품질 유지가 더 어려워지며 전통적인 데이터 품질 방법이 덜 효과적일 수 있습니다.\n\n그들의 주장이 완전히 틀렸다고 할 수는 없습니다. 하지만 그것만으로 충분하지는 않습니다.\n\n# 그래서 데이터 모니터링, 테스트 및 관찰의 차이는 무엇일까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n매우 명확하게, 이러한 방법 중 각각은 데이터 품질에 대응하려는 시도입니다. 따라서, 당신이 해결해야 할 문제가 그것이라면, 이 중 하나는 원칙적으로 그 문제를 확인할 것입니다. 하지만, 이 모두가 데이터 품질 솔루션이라는 것은 실제로 데이터 품질 문제를 해결해주지 않을 수 있다는 뜻입니다.\n\n이러한 솔루션들이 언제 어떻게 사용되어야 하는지는 그것보다는 조금 더 복잡합니다.\n\n가장 간단하면서, 데이터 품질을 문제로 생각할 수 있고, 테스트 및 모니터링을 품질 문제를 식별하는 방법으로 생각할 수 있으며, 데이터 가시성은 더 품질 문제를 해결할 수 있는 더 심도 있는 가시성과 해결 기능을 결합하고 확장하는 다양하고 포괄적인 접근 방식으로 생각할 수 있습니다.\n\n더 간단히 말하여, 모니터링과 테스팅은 문제를 확인하고, 데이터 가시성은 문제를 확인하고 해결책을 제시함으로써 실질적인 대응이 가능합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 데이터 관찰이 데이터 품질 성숙도 곡선에서 어디에 위치하는지 시각화하는 빠른 그림이 있습니다.\n\n![Data Quality Maturity Curve](/assets/img/2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024_1.png)\n\n이제 각 방법에 대해 조금 더 자세히 알아보겠습니다.\n\n# 데이터 테스트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 품질에 대한 전통적인 두 가지 방법 중 첫 번째는 데이터 테스트입니다. 데이터 품질 테스트(또는 간단히 데이터 테스트)는 사용자 정의 제약 조건이나 규칙을 사용하여 데이터 집합 내에서 특정 알려진 문제를 식별하는 감지 방법으로, 데이터 무결성을 확인하고 특정 데이터 품질 기준을 보장합니다.\n\n데이터 테스트를 생성하기 위해 데이터 품질 소유자는 SQL이나 dbt와 같은 모듈화된 솔루션을 활용하여 특정 문제(예: 과도한 널 비율 또는 잘못된 문자열 패턴)를 감지하는 일련의 수동 스크립트를 작성할 것입니다.\n\n데이터 요구 사항 — 따라서 데이터 품질 요구 사항도 — 가 매우 작은 경우, 많은 팀이 간단한 데이터 테스트에서 필요한 것을 충분히 얻을 수 있을 것입니다. 그러나 데이터가 커지고 복잡해지면, 새로운 데이터 품질 문제에 직면하게 되고 이를 해결하기 위한 새로운 능력이 필요해질 것입니다. 그 시간은 빨리 다가오게 될 것입니다.\n\n데이터 테스트는 데이터 품질 프레임워크의 필수 구성 요소로 남을 것이지만, 몇 가지 중요한 영역에서 제한이 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 깊은 데이터 지식이 필요합니다 — 데이터 테스트에는 데이터 엔지니어가 1) 품질을 정의하기 위해 충분한 전문 분야 지식이 필요하고, 2) 데이터가 어떻게 실패할 수 있는지에 대한 충분한 지식이 필요합니다.\n- 알 수 없는 문제에 대한 검토가 불가능합니다 — 데이터 테스트는 예상되는 문제에 대해서만 알려줄 뿐, 예상치 못한 사건에 대해서는 알려주지 않습니다. 특정 문제를 커버하기 위해 테스트가 작성되지 않은 경우, 테스트는 해당 문제를 발견할 수 없습니다.\n- 확장성이 없습니다 — 30개 테이블에 대해 10개의 테스트를 작성하는 것은 3,000개 테이블에 대해 100개의 테스트를 작성하는 것과 많은 차이가 있습니다.\n- 제한된 가시성 — 데이터 테스트는 데이터 자체만을 테스트하므로 문제가 데이터, 시스템 또는 해당 시스템을 제공하는 코드와 관련이 있는지 알려줄 수 없습니다.\n- 해결 방법이 없습니다 — 데이터 테스트로 문제를 감지해도, 이를 해결하는 데나 영향을 받는 내용을 이해하는 데는 도움이 되지 않습니다.\n\n어떤 규모에 있어서도, 테스트는 데이터에서 \"불!\"이라고 외치는 것과 같다. 그 후 아무도 어디서 이것을 본 것인지 알려주지 않고 걷어나가는 데이터 버전입니다.\n\n# 데이터 품질 모니터링\n\n데이터 품질 모니터링은 데이터 품질에 대한 또 다른 전통적이면서 다소 세련된 접근 방식으로, 수동 임계값 설정 또는 머신러닝을 통해 계속해서 모니터링하고 데이터에서 숨어있는 알 수 없는 이상 현상을 식별하는 영구적인 솔루션입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, 데이터가 제때 도착했나요? 예상했던 행 수를 얻었나요?\n\n데이터 품질 모니터링의 주요 이점은 알려지지 않은 알려지지 않은 사항에 대해 보다 넓은 범위의 커버리지를 제공하며, 모든 데이터셋마다 테스트를 작성하거나 복제하여 공통 문제를 수동으로 식별해야 하는 데이터 엔지니어들을 해방시켜줍니다.\n\n어느 면에서는, 데이터 품질 모니터링이 테스트보다 전체적인 측면에서 더 ganzonden입니다. 시간이 흘러도 해당 메트릭을 비교하고 팀이 이미 알려진 문제의 데이터에 대한 단일 단위 테스트에서 보지 못할 패턴을 발견할 수 있도록 해줍니다.\n\n유감스럽게도, 데이터 품질 모니터링은 몇 가지 중요한 측면에서 부족함이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 컴퓨팅 비용 증가 - 데이터 품질 모니터링은 비용이 많이 듭니다. 데이터 테스트와 마찬가지로 데이터 품질 모니터링은 데이터를 직접 쿼리하지만, 알려지지 않은 알려지지 않은 사항을 식별하기 위해 넓게 적용되어야 하므로 큰 컴퓨트 비용이 듭니다.\n- 가치창출 시간이 느림 - 모니터링 임계값은 머신 러닝으로 자동화할 수 있지만, 먼저 각 모니터를 직접 구축해야 합니다. 이는 데이터 환경이 시간이 지남에 따라 확장됨에 따라 각 문제에 대해 많은 양의 코딩을 하고 그 모니터를 수동으로 확장해야 한다는 것을 의미합니다.\n- 제한된 가시성 - 데이터가 다양한 이유로 손상될 수 있습니다. 테스트와 마찬가지로 모니터링은 데이터 자체만을 살펴보기 때문에 이상 사항이 발생했음을 알려줄 뿐, 그 이유를 알려주지는 않습니다.\n- 해결책이 없음 - 모니터링은 테스트보다 더 많은 이상 사항을 감지할 수는 있지만, 여전히 어떤 것이 영향을 받았는지, 누가 그것을 알아야 하는지 또는 그 중 어느 것이 중요한지를 알려줄 수 없습니다.\n\n게다가, 데이터 품질 모니터링이 경고를 전달하는 데에만 더 효과적일 뿐 관리하지는 않는다는 점 때문에 여러분의 데이터 팀은 시간이 지남에 따라 실제로 데이터 신뢰성을 향상시키기보다 경보 피로를 경험할 가능성이 훨씬 더 큽니다.\n\n# 데이터 관측성\n\n이것이 데이터 관측성입니다. 위에서 언급된 방법들과는 달리 데이터 관측성은 종합적인 공급업체 중립적 솔루션을 의미하며, 확장 가능하고 실행 가능한 완전한 데이터 품질 커버리지를 제공하도록 설계되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n소프트웨어 엔지니어링의 최고의 실천 방법을 모티브로 한 데이터 관찰은 데이터 품질 관리의 종단간 AI 지원 접근법으로, 데이터 품질 문제에 대한 \"무엇, 누가, 왜, 어떻게\"를 단일 플랫폼 내에서 해결하기 위해 설계되었습니다. 이는 기존 데이터 품질 방법의 한계를 보완하기 위해 테스트와 완전 자동화된 데이터 품질 모니터링을 결합하여 단일 시스템으로 확장한 후, 그것을 데이터, 시스템 및 코드 수준으로 확장하여 데이터 환경을 커버합니다.\n\n중요 사건 관리 및 해결 기능 (자동 열 수준 라인형 및 경보 프로토콜과 같은)과 결합된 데이터 관찰은 데이터 팀이 수집부터 사용까지 데이터 품질 문제를 감지, 분류 및 해결할 수 있도록 돕습니다.\n\n더불어 데이터 관찰은 데이터 엔지니어, 분석가, 데이터 소유자 및 이해 관계자를 포함한 팀 간 협업을 촉진하여 교차 기능적 가치를 제공하도록 설계되었습니다.\n\n데이터 관찰은 전통적인 데이터 품질 실무의 단점을 다음 네 가지 핵심 방식으로 해결합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 견고한 사건 분류 및 해결 - 가장 중요한 것은 데이터 관찰성이 사건을 빨리 해결할 수 있는 리소스를 제공합니다. 태깅 및 경보 외에도 데이터 관찰성은 자동 열 수준 계보를 통해 원인 분석 프로세스를 빠르게 처리하여 팀이 영향을 받은 것, 누가 알아야 하는지, 고치러 가야 할 곳을 한 눈에 볼 수 있도록 돕습니다.\n- 완벽한 가시성 - 데이터 관찰성은 데이터 소스를 초월하여 인프라, 파이프라인 및 데이터 이동 및 변환하는 포스트 인게스션 시스템까지 확대하여 회사 전반의 도메인 팀을 위해 데이터 문제를 해결합니다.\n- 가치 실현 속도 향상 - 데이터 관찰성은 ML 기반 모니터를 사용하여 설정 프로세스를 완전히 자동화하고 코딩이나 임계값 설정 없이 즉시 커버리지를 제공하여 환경에 따라 시간이 경과함에 따라 자동으로 확장되는 커버리지를 빠르게 얻을 수 있습니다 (사용자가 정의한 테스트가 쉬워지는 사용자 정의 테스트를 위한 커스텀 인사이트 및 단순화된 코딩 도구도 포함됨).\n- 데이터 제품 건강 추적 - 데이터 관찰성은 전통적인 테이블 형식을 벗어나 모니터링 및 건강 추적을 확장하여 특정 데이터 제품이나 중요 자산의 건강 상태를 모니터링, 측정 및 시각화합니다.\n\n# 데이터 관찰성과 AI\n\n우리는 모두 \"쓰레기를 넣으면 쓰레기가 나온다\"는 말을 들어본 적이 있을 것입니다. 그런데, 그 말은 AI 애플리케이션에 대해 두 배로 참된 것입니다. 그러나 AI는 단순히 출력물을 제공하기 위해 더 나은 데이터 품질 관리가 필요한 것뿐만 아니라 진화하는 데이터 자산의 확장성을 극대화하기 위해 데이터 품질 관리 자체가 AI에 의해 지원되어야 합니다.\n\n데이터 관찰성은 기업 데이터 팀이 AI에 신뢰할 수 있는 데이터를 효과적으로 제공할 수 있도록 해주는 사실상 유일한 데이터 품질 관리 솔루션이자 가능성이라고 볼 수 있습니다. 이를 달성하는 한 가지 방법은 AI로 구동되는 솔루션이기도 하기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI를 활용하여 모니터 생성, 이상 징후 감지, 원인 분석을 통해 데이터 관찰력은 실시간 데이터 스트리밍, RAG 아키텍처 및 기타 AI 사용 사례를 위한 초스케일 데이터 품질 관리를 가능하게 합니다.\n\n# 그렇다면, 2024년에는 데이터 품질이 어떻게 변화할까요?\n\n기업 및 그 이상을 위한 데이터 에스테이트가 계속 발전함에 따라, 전통적인 데이터 품질 방법으로는 데이터 플랫폼이 망가질 수 있는 모든 방법을 감시할 수 없거나 그 문제를 해결하는 데 도움을 줄 수 없습니다.\n\n특히 AI 시대에는 데이터 품질이 비즈니스 리스크뿐만 아니라 존립적인 리스크이기도 합니다. 모델로 공급되는 데이터의 전부를 신뢰할 수 없다면, AI의 출력도 신뢰할 수 없게 됩니다. AI의 빠른 규모에서는 전통적인 데이터 품질 방법으로는 이러한 데이터 자산의 가치나 신뢰성을 보호하는 데 충분하지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n효과적인 데이터 품질 관리를 위해서는 테스트와 모니터링이 하나로 통합된 플랫폼에 솔루션이 필요합니다. 이 솔루션은 데이터 환경 전체를 객관적으로 모니터링하고 데이터 팀이 문제를 신속히 해결할 수 있는 자원을 제공해야 합니다.\n\n다시 말해, 최신 데이터 팀이 필요한 것은 데이터 관측성입니다.\n\n첫 번째 단계. 감지하기. 두 번째 단계. 해결하기. 세 번째 단계. 성공하기.","ogImage":{"url":"/assets/img/2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024_0.png"},"coverImage":"/assets/img/2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024_0.png","tag":["Tech"],"readingTime":8},{"title":"어떻게 Apache Airflow에서 2000개 이상의 DBT 모델을 조율하는지","description":"","date":"2024-05-27 12:49","slug":"2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow","content":"\n\n![How we orchestrate 2000 DBT models in Apache Airflow](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_0.png)\n\n요즘에는 DBT (Data Build Tool)가 매우 표현력 있는 SQL과 Jinja 템플릿을 사용하여 변환을 선언하는 방식을 통해 다양한 처리 엔진에 연결되는 데이터 변환 워크플로우로 자리 잡았습니다. 이에 더불어 DBT는 문서 작성, 테스트, 그리고 기본 기능을 확장하는 커뮤니티 제작 패키지에 대한 좋은 지원을 제공합니다. ELT에서의 T를 훨씬 더 쉽고 즐겁게 만들었습니다.\n\nDBT Core는 모델 간의 계보를 다루지만, 프로덕션 환경에서 실행되어야 하는 위치와 시기에 대한 솔루션을 제공하지 않습니다. 다시 말해, 오케스트레이션은 기본적으로 제공되지 않습니다.\n\n본 글에서는 Airflow를 활용하여 DBT Core 프로젝트를 오케스트레이션하는 방법을 살펴볼 수 있습니다. 이를 통해 데이터 분석가 및 심지어 제품 소유자도 자신만의 데이터 모델을 생성하고 유지할 수 있는 직관적인 파이프라인을 만들었습니다. SQL과 Git의 기본 지식만 있으면, 비즈니스의 다양한 사람들이 몇 분 만에 자신의 모델을 Airflow DAG로 전환하여 분산 및 확장 가능한 환경에서 실행할 수 있습니다. 이는 경보, 데이터 품질 테스트, 그리고 내장된 액세스 제어와 함께, Airflow DAG가 무엇인지 알 필요 없이 UI에서 상호 작용할 수 있습니다 😄\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주요 부분으로 나눠 보겠습니다:\n\n- Mono vs Multi DAG 접근 방식\n- 프로젝트 구조 및 DAG 레이아웃\n- DAG 생성 파이프라인\n- DBTOperator를 생성한 방법과 이유\n- 결론 및 앞으로의 계획\n\n# Mono vs Multi DAG 접근 방식\n\n이 문제에 대한 직관적인 방법은 전체 DBT 프로젝트를 \"하나의 큰 DAG\"로 모델링하는 것입니다. 이는 DBT 계보를 고려하여 작업을 연결하기 쉽게 만들어주며 Airflow에서 전체 DBT 프로젝트의 멋진 계보 뷰를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 Monon DAG 방식에는 이 프로젝트를 시작할 때 우리에게 중요한 몇 가지 단점이 있습니다:\n\n- DAG 레벨에서 일정이 설정되므로 프로젝트 전체가 동일한 일정으로 실행됩니다. 이는 프로젝트 전체에서 모델에 대해 다른 SLA가 있는 경우 문제가 될 수 있습니다.\n- 큰 DAG는 탐색하기 어려울 수 있습니다. 프로젝트에 2000개 이상의 모델이 있는 경우 이 거대한 DAG를 통해 길을 찾는 것은 분석가나 비즈니스 사용자들에게 특히 Airflow에 익숙하지 않은 사람들에게 도전이 될 수 있습니다.\n- 접근 제어에 효율적이지 않습니다. 서로 다른 팀이 DBT 프로젝트의 다른 부분을 소유하고 있기 때문에 Airflow에서도 이 분리를 활용해야 합니다. 예를 들어, 당신의 팀만 당신의 모델을 수동으로 트리거하거나 완전한 새로 고침을 수행할 수 있어야 합니다. 하나의 큰 DAG만 있는 것은 전체 프로젝트에 대한 하나의 접근 제어 계층을 의미합니다.\n- 모델 실패의 경우 알림을 분할하기가 어려울 수 있습니다. 다시 말하지만, 모델 실패의 경우 관련 팀에만 알림을 보내기를 원했습니다.\n\n매우 중요한 참고: DBT는 여러 프로젝트를 네이티브로 지원하기 전에 프로젝트를 시작했습니다. DBT 코어에서 완전히 지원되지 않았지만, DBT 매쉬는 프로젝트를 분할하고 프로젝트 당 하나의 DAG를 갖는 경험을 보다 수월하게 만들 수 있는 방법일 수 있습니다.\n\n## DBT 프로젝트를 여러 DAG로 분리하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서 언급한 문제를 해결하기 위해, 우리는 조직에 맞는 그룹화 규칙에 따라 프로젝트를 다른 DAG로 나누기로 결정했습니다. 이를 통해 프로젝트의 다른 부분에 대해 서로 다른 SLA를 가질 수 있고, DAG 수준에서 액세스 제어 및 콜백 함수에서 알림/알림 대상을 다르게 설정할 수 있습니다. 또한, 팀은 자신들의 DAG만 쉽게 필터링할 수 있으며, Airflow에서 모델을 더 잘 탐색할 수 있습니다.\n\n그러나, 어떤 모델을 어떤 DAG에 그룹화할지 결정하는 방법 및 종속 DAG를 어떻게 연결할지에 대한 자연스러운 질문들이 제기됩니다.\n\n이 중요한 질문들은 우리를 오늘날의 솔루션을 개발하는 데 이끌었습니다. 여기에서 언급해야 할 중요한 점은 Airflow에서 DBT 라인어지 전체를 볼 수 있는 것이 우리에게 그다지 중요하지 않다는 점입니다. 우리는 데이터 탐색을 위해 Datahub를 사용하며, 이는 매우 좋은 라인어지보기를 제공합니다. 따라서, 우리는 Airflow를 가능한 가장 효율적인 방법으로 모델 실행을 관리하기 위한 도구로 사용하기로 결정했으며, 데이터 발견 도구로 사용하지는 않기로 했습니다.\n\n# 프로젝트 구조 및 DAG 배치\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전에 언급된 질문들을 고려할 때, 우리는 모델 그룹 개념을 고안해냈습니다. 모델 그룹은 서로 깊게 관련된 데이터 변환의 집합입니다. 예를 들어 함께 새로 고쳐져야하고 단위로서만 의미가 있는 같은 데이터 마트의 테이블들입니다. 또한, 이러한 테이블들은 단일 팀에 의해 소유되고 유지보수됩니다. 모델 그룹은 비즈니스 목표를 달성하기 위해 고안되었습니다. 중간 변환 수행 및 테이블 그룹 준비, 데이터 마트 생성, KPI 계산 등을 수행합니다.\n\n따라서, 우리는 각 모델 그룹 당 하나의 DAG를 가지기로 결정했습니다. 모델들이 서로 밀접하게 관련되어 있고 함께 스케줄되어야하기 때문입니다.\n\n아래에 제시된 최소주의 프로젝트 구조는 레이아웃을 이해하는 데 도움이 될 수 있습니다.\n\n![이미지](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음과 같이 설명해보겠습니다:\n\n- dbt_project.yml: 이것은 프로젝트의 루트에 있는 일반 dbt_project 파일입니다. 여기에는 특별한 내용이 없습니다.\n- deployment.yml: 이 파일에는 배포할 모델 그룹을 등록합니다. 즉, 모델 그룹을 DAG로 변환하기 위한 작업을 수행합니다. 실행 일정, 태그, 소유자 등을 지정합니다. 다음과 같이 보일 것입니다:\n\n```js\n# deployment.yml\n---\nmodel_groups:\n  - name: model_group_a # 폴더의 이름입니다.\n    schedule: 0 0 * * * # DAG 일정입니다.\n    owner: Team_A # Airflow에서 DAG의 소유자 (역할)입니다.\n    tags: [tag1, tag2] # Airflow DAG용 태그입니다.\n    description: 추가 변환을 위해 테이블을 준비합니다. # DAG 설명입니다.\n\n  - name: model_group_b\n    schedule: 0 2 * * *\n    owner: Team_A\n    tags: [tag1, tag2]\n    description: 여러 테이블을 조인하여 데이터 마트를 생성합니다.\n```\n\n- model_group_a 및 model_group_b: SQL 모델(동일한 방식으로 DBT Python 모델도 작동합니다)이 포함된 폴더입니다. 이 예제에서는 model_group_a의 model2.sql이 종속성으로 model_group_a의 model1.sql을 참조한다고 가정합니다. 모델 그룹은 DBT 프로젝트의 폴더이며 모델을 포함합니다. 원하는 만큼 모델을 넣을 수 있으며 하위 폴더에 대한 DAG 생성도 허용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAirflow에서는 이 구조가 다음과 같이 보일 것입니다.\n\n이 구조를 통해 몇 가지 중요한 점을 보장할 수 있습니다:\n\n- 의존하는 DAG는 센서로 연결됩니다: 이를 통해 각 모델 그룹이 다른 일정에 따라 실행되도록하고 동시에 실패가 하향으로 전파되는 것을 방지할 수 있습니다. 센서 검사에 실패하면 하향 모델은 건너뛰게 됩니다. 여기 중요한 점은 우리가 기본 Airflow 외부 작업 센서를 분기시켜야 했단 점입니다. 이는 우리가 상류 모델 실행의 최종 상태를 확인하려고 했기 때문입니다. 기본 센서는 특정 실행 날짜만 터치할 수 있기 때문입니다.\n- 동일한 DAG 내에서 실행의 계통은 dbt-test 작업을 기반으로 합니다: 이는 데이터 품질 오류가 하향으로 전파되는 것을 방지하여, 데이터 품질 문제가 계속 악화되는 눈덩이 효과를 피할 수 있습니다.\n- 각 DAG (모델 그룹)에는 소유자가 있습니다: 이는 해당 DAG에서 수동 작업(전체 갱신 실행 트리거, 작업 지우기 등)을 취할 수 있는 사람이 적합한 팀 멤버뿐이라는 것을 의미합니다.\n- DAG의 수와 크기는 유연하며, DBT 프로젝트 레이아웃을 따릅니다: 모든 DAG가 모델 그룹을 기준으로 동적으로 생성되기 때문에, 그 크기나 세분성은 원하는 대로 조절할 수 있습니다. DBT 프로젝트 안의 모델 그룹에 포함된 모델 수가 DAG 레이아웃을 지배합니다.\n\n# DAG 생성 파이프라인\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 팀원이 DBT 저장소에서 PR을 생성하는 순간부터 어떤 일이 발생하는지 살펴보겠습니다. 간단히 말해서, DBT 프로젝트의 배포 파이프라인은 다음과 같습니다.\n\n![DBT 프로젝트 배포 파이프라인](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_2.png)\n\n매 PR에 CI 단계로서 아래 두 가지 매우 중요한 요소가 도입되었습니다:\n\n- 조직 거버넌스 요구 사항 확인: 각 모델은 소유자와 적절한 태그, 설명 등을 가져야 합니다. 이는 매우 중요한데, 데이터 카탈로그를 풍부하고 의미 있는 것으로 만들어주기 때문입니다.\n- 스테이징 환경에서 업데이트된 모델 실행: 이를 통해 도입되는 변경 사항이 업데이트된 모델 및 하위 종속성에 대한 성공적인 실행을 보장합니다. DBT CI 실행을 위한 우리의 스테이징 영역에는 생산 모델의 대표적인 샘플이 포함되어 있어 CI 테스트 실행 비용을 최소화합니다. DBT 모델을 CI에서 적절히 테스트하는 것은 별도의 포스트가 필요하며 이에 대해 별도의 글이 필요할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPR이 병합되면 DAG 생성 프로세스가 시작됩니다. 이 프로세스는 DBT manifest.json 파일을 구문 분석하여 전체 그래프를 가져오는 방식으로 작동합니다. 그런 다음 deployment.yaml에 정의된 모델 그룹 규칙에 따라 다른 DAG가 생성됩니다.\n\n여기서 중요한 개념은 DBT manifest를 구문 분석할 때 \"내부\" 및 \"외부\" 모델을 구분하는 것입니다. 내부 모델은 해당 모델 그룹에 포함된 모델이며, 외부 모델은 주어진 모델 그룹 외부의 종속성입니다. 이 구분을 통해 외부 최신 작업 센서인 ExternalLatestTaskSensor를 사용하여 적절한 센서를 할당할 수 있습니다. 이 센서는 Airflow 외부 작업 센서의 파생 버전입니다. 우리는 메타데이터 데이터베이스 쿼리를 수정하여 상위 작업의 최신 상태를 가져와서 (실행 날짜별로 정렬) 센서가 상위 작업의 최신 dbt-test 결과를 확인할 수 있도록 했습니다.\n\n따라서 각 모델 그룹은 개별 일정에 따라 실행될 수 있도록 센서로 연결됩니다. 우리가 고려한 다른 옵션은 TriggerDagRunOperator를 사용하는 것이었지만, 이는 상위 최상위 모델에서만 일정을 설정할 수 있도록 했습니다.\n\n이미지 소스:\n![How we orchestrate 2000 DBT models in Apache Airflow](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n흐름그림(DAGs)을 생성하는 작업 자체는 Jinja를 사용한 템플릿화를 통해 이루어집니다. 결국 우리는 단지 Python 파일들을 생성하는 것이니까요 😃. 특정 DAG에 포함할 모델들, 그들의 \"내부\" 선조 및 \"외부\" 모델 의존성(센서)을 결정하면 됩니다.\n\n마지막으로 생성 작업이 완료되면, DAG와 DBT 프로젝트 자산은 Airflow의 자산 버킷에 푸시됩니다. 거기서 다른 프로세스(Airflow에서 실행 중)가 이를 가져갈 것입니다. Airflow 측면에서 작동 방식을 알고 싶다면, 저의 Airflow 글을 참조해주세요.\n\n# 우리가 DBTOperator를 만든 방법과 이유\n\nAirflow에서 DBT를 실행 중이라면 BashOperator를 사용하여 dbt 명령을 실행하거나, 그 작업을 처리할 DBTOperator를 생성할 수 있습니다. 후자의 옵션은 전자보다 많은 이점을 가지고 있으며, 왜 여러분이 자체 DBTOperator를 만들어야 하는지 설명하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희는 airflow-dbt 프로젝트의 오픈 소스 구현을 사용하여 DBTOperator 여정을 시작했습니다. 몇 달 동안 잘 사용해 왔지만, 우리만의 Operator를 만드는 것이 가장 좋을 것이라는 것을 깨달았습니다.\n\n우리는 서브프로세스 명령이 아닌 DBT 프로그래밍 방식의 호출을 사용하고 싶었습니다. 이는 실행 결과를 더 잘 처리하는 방법을 제공하며 또한 모범 사례를 준수합니다. dbt cli를 위한 Python 진입점을 사용한 후 코드가 더 깔끔하고 가독성이 향상되었습니다.\n\n가장 중요한 것은 DBT Orchestration 솔루션의 명백한 제한 사항을 해결하고자 했습니다, 특히 버그 수정을 위한 수동 개입을 처리할 때입니다. 이러한 제한 사항 중 일부는 아래에 나열되어 있습니다.\n\n## 증분 모델의 스키마 변경\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDBT에서 기본으로 제공하는 on_schema_change 옵션 중 우리 문제를 해결하는 데는 거의 모든 경우에서 부가 정보를 백필할 필요가 있기 때문에 문제 해결이 되지 않았습니다. 예를 들어 열이 추가될 때 정보를 백필해야 하는 경우가 대부분이었습니다. 그래서 스키마 변경 시 유일한 옵션은 전체 새로 고침을 트리거하는 것이었습니다. 우리는 예상된 소스 스키마 변경으로 인해 많은 모델이 실패했고, 그 당시 \"트리거\"를 하려면 Snowflake에서 테이블을 삭제해야 했습니다 😅.\n\n물론, 이것은 이상적이지 않습니다. 그래서 우리가 처음으로 구현한 것 중 하나는 사용자 지정 DBTOperator에서 실행이 실패한 후 dbt-run 실행 로그를 구문 분석하여 실패가 스키마 변경으로 인한 것인지 감지하면 --full-refresh 플래그를 전달하여 해당 모델을 자동으로 다시 트리거하는 기능이었습니다. 이 간단한 기능 덕분에 DBT 모델의 일일 유지 보수 시간이 단축되었습니다.\n\n## 대규모 모델이나 전체 새로 고침의 초기 처리\n\n가끔 아주 큰 모델의 초기 처리를 할 때나 여러 이유로 수동으로 전체 새로 고침을 트리거할 때, Snowflake DBT Warehouse를 과부하시키는 경우가 있습니다. 그를 피하기 위해 DBTOperator에 기능을 만들어서 해당 모델을 실행하는 데 사용하는 웨어하우스를 동적으로 변경하고 크기(소형, 중형, 대형 등)를 설정하는 기능을 만들었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이렇게 함으로써, 모든 기본적인 작은 증분 모델을 동시에 동일한 DBT Warehouse에서 실행할 수 있으면서, 전용 리소스가 할당된 격리된 Warehouse에서 대규모 실행을 수행할 수 있습니다. 이는 Snowflake 쿼리 실행 대기열의 증가를 방지합니다.\n\n또한, 데이터 분석가들이 Airflow 인터페이스에서 직접 전체 새로 고침을 트리거할 수 있게하여, 테이블을 삭제할 필요가 없습니다. DBTOperator가 하는 일은 적절할 때 dbt run 명령에 --full-refresh 플래그를 전달하는 것 뿐입니다.\n\n## 모델 그룹에서 개별 모델 수동 트리거\n\n때로는 데이터 분석가들이 DAG의 모델 그룹에서 하나 또는 두 개의 모델만 실행하도록 트리거해야 할 필요가 있습니다. 가끔 이러한 실행은 지정된 모델의 전체 새로 고침이어야 할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 문제를 해결하기 위해, Airflow DAG의 매개변수로 사용 가능한 옵션을 만들어 선택한 분석가가 DAG에서 특정 모델만 트리거할 수 있도록 했습니다. 그 특정 DagRun에 선택되지 않은 다른 모든 모델은 건너뛰게 됩니다. 이 접근 방식은 한 두 개의 모델만 실행해야 할 때 모든 모델을 실행하여 리소스를 낭비하는 것을 방지합니다.\n\nAirflow의 clear task 옵션을 사용하는 것도 해결책이지만, 사용자가 전체 리프레시를 실행하거나 그 모델을 실행하는 데이터 웨어하우스를 변경해야 하는 경우에는 제한적입니다. Airflow에서 작업을 지우기만 해서 매개변수로 실행을 사용자 정의할 수는 없습니다. 이 사용자 정의 옵션을 통해 분석가가 더 정확하게 그들의 요구 사항을 지정할 수 있어 모델 실행의 효율성과 유연성을 향상시킬 수 있습니다.\n\n## 모델 수정 후 하류 종속성의 트리거\n\n우리의 Airflow-DBT 구조에서 모델 그룹에 따라 많은 DAG가 있으며, 일부 모델은 4~5개의 DAG로 구성된 긴 종속성 체인을 가지고 있습니다. 그 체인의 첫 번째 DAG에서 모델 실행(실행 또는 테스트)이 실패하면 다른 DAG에서의 모든 하류 모델이 오류 전파를 방지하기 위해 건너뛰게 됩니다. 첫 번째 모델이 수정된 후에도 우리는 모든 하류 DAG를 다시 실행할 수 있도록 어떻게 보장할 수 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전에는 이 작업을 수동으로 처리했었어요 😞. 데이터 분석가들은 모델 수정이 적용된 후 재시작해야 하는 하향 DAG들을 계속 추적해야 했어요. 이 과정은 시간이 많이 소요되고 오류가 발생하기 쉬웠어요.\n\n이 문제를 해결하기 위해, 우리는 Airflow DAG에서 사용되는 DBTOperator의 사용자 정의 로직에 의해 구동되는 하향 트리거 옵션을 만들었어요. DAG 실행 시 이 값을 설정하면 모든 모델이 성공하면 DAG는 자동으로 모든 하향 종속성을 인지하고 해당 종속성들의 DagRun을 트리거합니다. 이를 통해 버그 수정 후 DAG를 수동으로 트리거하는 프로세스가 불필요해졌어요.\n\n우리는 dbt ls 명령을 사용하여 종속성 그래프에 있는 모델을 나열하는데 구현이 간단했어요. 그런 다음, 해당 모델을 DAG와 매핑하고 Airflow의 trigger_dag() 함수를 사용하여 하향 실행을 자동으로 트리거했어요.\n\n더 중요한 것은, 이 프로세스가 \"체인 반응\"으로 자동으로 계속된다는 것이에요: 트리거된 DAG는 완료되면 하향 종속성도 트리거하도록 인수 플래그를 받아 이 프로세스는 체인에서 마지막 DAG가 완료될 때까지 계속됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## DAG 매개변수를 통한 DBT 실행 인터페이스\n\nDBTOperator에 위에서 언급한 솔루션들을 구현한 후, 우리는 또한 DAG 매개변수를 생성하여 일부 구성을 사용자에게 노출시켜 수동 실행을 사용자 정의할 수 있도록 했습니다.\n\n![image](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_4.png)\n\n이로써 데이터 분석가들과 분석 엔지니어들의 일상에 큰 변화가 생겼습니다. 이제 필요할 때 수동 실행을 완전히 사용자 정의할 수 있게 되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 모든 매개변수는 이전에 설명한 대로 DAG를 자동으로 생성할 때 템플릿에 동적으로 추가됩니다. 따라서 예를 들어, 해당 모델 그룹의 사용 가능한 모델을 사용하여 Models 드롭다운을 채우게 됩니다.\n\n또한 DAG 생성 파이프라인에서 사용되는 \"매개변수 주입\" 방법은 매우 확장 가능하여 미래에 필요에 따라 더 많은 매개변수를 생성할 수 있습니다. \n\n# 결론과 앞으로의 방향\n\n저는 이 게시물이 Airflow에서 DBT 오케스트레이션에 대한 다른 관점을 제공할 수 있기를 바랍니다. 이 구현은 2년이 지난 후에도 여전히 우리의 요구를 충족시키지만, 완벽하거나 이상적이지는 않으며 개선할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비슷한 오픈 소스 구현으로는 뛰어난 Astronomer Cosmos 프로젝트를 찾을 수 있어요. 여기서 재미있는 기능은 각 모델에 대한 실행 및 테스트를 결합하기 위해 작업 그룹을 사용한다는 점이에요 (우리도 그랬죠 😍) 그리고 프로젝트 구성을 통해 매우 쉽고 깔끔하게 DbtDag를 선언하는 방식이에요.\n\n프로젝트를 둘 이상의 DAG로 분할하는 것도 가능해요, 생성자가 dbt select 인수를 허용하기 때문에요. 따라서 태그를 전달하고 다른 태그에 따라 프로젝트를 분할할 수 있어요. 그러나 저는 DAG 간의 가능한 상호 작용 (모델 참조)을 어떻게 다루는지는 분명하지 않아요. DBT 조정 여정을 시작한다면 매우 깔끔한 추상화를 제공하기 때문에 꼭 확인해보세요.\n\n지금은 우리 앞에 있는 것이 데이터 계약의 구현이며 그것이 우리가 DBT와 상호 작용하는 방식에 큰 영향을 미치고 있어요. 소스 시스템과 데이터 레이크의 테이블 사이를 잇기 위해 계약을 사용함으로써 연결자(데이터 추출기)의 프로비저닝과 업무 지식이 필요하지 않은 초기(기본) 변환(유형 캐스팅, 열 이름 표준화, 복잡한 필드의 언네스팅)을 수행하는 DBT 모델을 자동화할 수 있어요. 결과적으로, 이전에 설명한 일부 모델 그룹은 데이터 계약을 기반으로 완전히 자동화된 방식으로 생성되고 있어요.\n\nDBT-Airflow 구현에 대해 더 논의할 준비가 되어 있고 커뮤니티가 이 문제를 해결하는 방법에 대해 듣는 것에 매우 열려 있어요. 따라서 유사한 구현이 있다면 어떻게 하는지 알려주세요 😆. 진정으로 높은 가치를 제공하는 멋진 솔루션을 만들 수 있는 것은 연결된 커뮤니티 덕분이에요.","ogImage":{"url":"/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_0.png"},"coverImage":"/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_0.png","tag":["Tech"],"readingTime":12},{"title":"이미지를 클러스터링하는 방법","description":"","date":"2024-05-27 12:46","slug":"2024-05-27-HowtoClusterImages","content":"\n\n![FiftyOne](https://miro.medium.com/v2/resize:fit:1400/1*b6uzxatq8ELEOu-1SjMmGg.gif)\n\n# FiftyOne, Scikit-learn 및 Feature Embeddings을 사용하기\n\n2024년 깊은 학습의 계산 집약적인 환경에서 \"클러스터\"라는 단어는 주로 GPU 클러스터를 논할 때 가장 자주 나타납니다. 이는 매우 최적화된 행렬 곱셈 기계의 대규모 컬렉션으로, 동등하게 거대한 생성 모델을 훈련시키기 위해 설정된 것입니다.\n모두가 더 크고 더 나은 모델을 훈련시키고, AI 모델 성능의 한계를 끌어올리며, 최신의 구조적 진보를 자료에 적용하는 데 주력합니다.\n\n그런데 더 나은 모델을 구축하려고 할 때 더 중요할 수 있는 다른 유형의 클러스터가 있는데요. 저는 CPU나 TPU, 또는 다른 어떤 종류의 하드웨어에 대해 언급하는 게 아닙니다. 심지어 모델 훈련 과정에 대해서도 말하는 게 아닙니다. 저는 데이터를 깊이 이해하는 데 도움이 되는 옛날의 비지도 학습 작업인 클러스터링을 말하는 것입니다. 끝내 우리에게 예측력이 흐르는 원천이기 때문이죠.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-05-27-HowtoClusterImages_0.png)\n\nIn this blog, we’ll cover the basics of clustering and show you how to structure your visual data using the open-source machine learning libraries Scikit-learn and FiftyOne!\n\n# What is Clustering?\n\n![Image](/assets/img/2024-05-27-HowtoClusterImages_1.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 클러스터링의 기본 요소\n\n바닥에 펼쳐진 다양한 모양과 크기의 레고 블록이 많이 있다고 상상해보세요. 이제 레고 블록을 정리해야 할 때, 모든 블록을 보관할 충분히 큰 용기가 없다는 것을 깨닫게 됩니다. 운좋게도, 각각이 거의 동일한 수의 조각을 수납할 수 있는 작은 상자 네 개를 찾을 수 있습니다. 단순히 임의로 레고를 각 상자에 넣고 하루를 끝내도 좋습니다. 하지만, 그 다음에 특정 조각을 찾으러 갈 때, 그것을 찾기 위해 꽤나 많은 시간이 걸릴 것입니다.\n\n대신 비슷한 조각을 동일한 상자에 넣는 것이 훨씬 많은 시간과 고민을 덜어줄 것입니다. 그러나 Legos를 어떤 기준으로 상자에 넣을 것입니까? 다른 색깔에 상자를 할당할 것입니까? 아니면 하나의 상자에 모든 네모난 조각을, 다른 하나에는 원형 조각을 넣을 것입니까? 실제 사용하는 Legos에 달렸습니다! 요약하자면, 이것이 클러스터링입니다.\n\n보다 형식적으로 말하면, 클러스터링 또는 군집 분석은 데이터 포인트를 그룹화하기 위한 기술입니다. 클러스터링 알고리즘은 다수의 개체를 입력 받아 각 개체에 대한 할당을 출력합니다. 그러나 분류와 달리 클러스터링은 개체를 사전 설정된 버킷으로 강제로 분류하는 클래스 목록으로 시작하지 않습니다. 오히려 데이터를 통해 주어진 버킷을 발견하려고 시도합니다. 다시 말해, 클러스터링은 데이터에서 구조를 발견하는 데 관한 것이며, 이미 존재하는 구조에서 레이블을 예측하는 것이 아닙니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로 강조해야 할 점은 클러스터링이 레이블을 예측하는 것과는 관련이 없다는 점입니다. 분류, 탐지 및 분할 작업과는 달리 클러스터링 작업에 대한 'ground truth' 레이블이 없습니다. 이러한 알고리즘을 비지도학습이라고 하며, 지도학습과 자기지도학습과 대비됩니다.\n\n클러스터링은 교육이 필요하지 않습니다. 클러스터링 알고리즘은 데이터 포인트(객체)의 특징을 입력으로 받아 이러한 특징을 사용하여 객체를 그룹으로 분할합니다. 성공적일 때, 이러한 그룹은 독특한 특성을 강조하여 데이터 구조를 파악할 수 있게 해줍니다.\n\n💡 이는 클러스터링이 데이터를 탐색하는 데 매우 강력한 도구임을 의미합니다 - 특히 데이터가 레이블이 없는 경우에 유용합니다!\n\n## 클러스터링은 어떻게 작동합니까?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주의 깊게 살펴보셨다면 클러스터링과 클러스터링 알고리즘 사이에 섬세한 차이를 발견할 수 있을 겁니다. 이는 클러스터링이 여러 기법을 포괄하는 더 큰 범주이기 때문이에요!\n\n클러스터링 알고리즘은 몇 가지 다른 유형으로 나눠집니다. 이들은 클러스터 멤버십을 할당하는데 사용하는 기준에 의해 구분됩니다. 가장 일반적인 클러스터링 유형 중 일부는 다음과 같아요:\n\n1. 중심 기반 클러스터링: 예를 들어, K-평균 및 평균 변이 클러스터링과 같은 기법들이 속합니다. 이러한 방법들은 각 클러스터를 정의하기 위해 중심점을 찾으려고 합니다. 클러스터 내의 점들 간 일정한 일관성 개념을 최대화하는 중심점을 살펴봐요. 이 클러스터링 유형은 대규모 데이터셋에 잘 확장되지만 이상점과 무작위 초기화에 민감합니다. 종종 여러 번 실행하고 가장 나은 결과를 선택합니다. K-평균과 같은 기법이 고차원 데이터에서 어려움을 겪을 수 있고, 고차원 축소 기법과 함께 사용할 때 구조를 더 잘 발견할 수 있는 것을 발견할 거에요. 아래에서 두 가지를 어떻게 함께 사용하는지 설명하겠습니다.\n\n2. 밀도 기반 클러스터링: DBSCAN, HDBSCAN 및 OPTICS와 같은 기법은 특징 공간이 얼마나 희소하거나 밀집되어 있는지에 기반하여 클러스터를 선택합니다. 개념적으로, 이 알고리즘은 고밀도 영역을 클러스터로 취급하고, 특징 공간에서 포인트가 충분히 퍼져있을 때 클러스터를 분리합니다. DBSCAN과 같이 간단한 밀도 기반 기법은 데이터가 밀접하게 위치하지 않을 수 있는 고차원 데이터에서 작업하는 데 어려움을 겪을 수 있습니다. 그러나 HDBSCAN과 같은 더 정교한 기법은 이러한 한계를 극복하고 고차원 특징에서 탁월한 구조를 발견할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n계층적 군집화: 이러한 기술은 다음 중 하나를 추구합니다:\n\n- 개별 점부터 시작하여 군집을 반복적으로 결합하여 더 큰 복합체를 구성하거나\n- 하나의 군집에 있는 모든 객체로 시작하여 군집을 작은 구성요소로 반복적으로 분할하는 방식\n\n데이터셋이 커질수록 구성기술은 계산적으로 소모적이 되지만, 작고 중간 규모의 데이터셋 및 저차원 특징에 대해서는 성능이 꽤 뛰어날 수 있습니다.\n\n📚 가장 일반적으로 사용되는 10가지 이상의 군집화 알고리즘에 대한 포괄적인 논의를 원한다면, Scikit-learn의 직관적이고 잘 쓰여진 가이드를 확인해보세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 어떤 기능에 클러스터링 해야 할까요?\n\n우리는 이 글에서 시작한 레고 블록들에 대한 토론 중, 특징들(길이, 너비, 높이, 곡률 등)을 독립적인 엔티티로 생각할 수 있는 데이터 테이블의 열로 볼 수 있습니다. 이 데이터를 정규화하여 각 특징이 다른 특징들을 지배하지 않도록 한 후, 각 레고 블록에 대한 수치값의 행을 특징 벡터로 클러스터링 알고리즘에 전달할 수 있습니다. 클러스터링은 이와 같은 많은 응용 분야를 가졌으며, 가벼운 전처리가 된 수치값이나 시계열 데이터로 작동합니다.\n\n이미지와 같은 구조화되지 않은 데이터는 몇 가지 간단한 이유로 이 프레임워크에 매끄럽게 들어가지 않습니다:\n\n- 이미지는 크기(가로세로 비율과 해상도)가 다를 수 있습니다.\n- 원시 픽셀 값은 매우 노이즈가 있을 수 있습니다.\n- 픽셀 간의 상관 관계는 매우 비선형 일 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리가 모든 이미지 크기를 재정렬하고 표준화하며, 픽셀 값 정규화를 하고, 노이즈 제거를 하고, 다차원 배열을 \"특성 벡터\"로 평평하게 만들면, 이러한 처리된 픽셀 배열을 특성으로 다루는 것은 비지도 클러스터링 알고리즘에 많은 양의 스트레스를 줄 수 있습니다. 이는 MNIST와 같은 간단한 데이터 세트에 대해서는 작동할 수 있지만, 실제로는 종종 선택사항이 아닙니다.\n\n다행히도, 깊은 신경망이라 불리는 강력한 비선형 함수 근사 도구들이 있습니다! 이미지 도메인에 주목한다면, CLIP 및 DINOv2와 같은 모델이 있습니다. 이러한 모델의 출력은 입력 데이터의 의미 있는 표현이며, 이미지 분류와 같은 특정 작업을 위해 훈련된 모델들도 있습니다. 일반적으로 네트워크의 끝에서 두 번째 층의 출력을 취합니다. 또한 변수적 오토인코더(VAE) 네트워크도 있으며, 일반적으로 중간 층에서 표현을 취하는 것이 일반적입니다!\n\n💡다양한 모델에는 서로 다른 아키텍처가 있으며, 서로 다른 데이터 세트에서 훈련되었으며 서로 다른 작업을 향해 훈련되었습니다. 이러한 모든 요소는 모델이 학습하는 특성의 유형에 영향을 줍니다. 당신의 과제를 해보세요 📚:)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 설정 및 설치\n\n지금까지 배경 지식을 살펴보았으니, 그 이론을 실제로 적용하여 비구조적 데이터를 구조화하는 클러스터링 방법을 배워봅시다. 이를 위해 가장 흔한 클러스터링 알고리즘을 구현한 scikit-learn과 unstructured data의 관리 및 시각화를 간소화하는 fiftyone이라는 두 개의 오픈 소스 머신 러닝 라이브러리를 활용할 것입니다:\n\n```js\npip install -U scikit-learn fiftyone\n```\n\nFiftyOne Clustering Plugin은 우리의 일상을 더욱 쉽게 만들어줍니다. 이 플러그인은 scikit-learn의 클러스터링 알고리즘과 이미지 사이의 연결 조직 역할을 하며, FiftyOne App 내에서 간단한 UI로 이러한 작업을 모두 래핑합니다. 플러그인을 CLI에서 설치할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nfiftyone 플러그인을 https://github.com/jacobmarks/clustering-plugin 에서 다운로드하세요.\n\n또한 CLIP 모델을 사용하여 이미지 피처를 생성할 수 있게 해주는 OpenAI의 CLIP GitHub 저장소와, 이러한 피처를 2D로 시각화하기 위해 Uniform Manifold Approximation and Projection (UMAP)이라는 차원 축소 기술을 적용할 수 있게 해주는 umap-learn 라이브러리를 두 가지 더 필요로 할 것입니다:\n\n```js\npip install umap-learn git+https://github.com/openai/CLIP.git\n```\n\n이 두 라이브러리가 꼭 필요한 것은 아닙니다. FiftyOne Model Zoo에서 임베딩을 노출하는 임의의 모델로 피처를 생성하고, PCA나 tSNE와 같은 대체 기술로 차원 축소를 수행할 수도 있습니다.\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n필요한 모든 라이브러리를 설치한 후에, Python 프로세스에서 관련 FiftyOne 모듈을 가져와 FiftyOne Dataset Zoo에서 데이터셋을 로드하세요 (또는 원하는 데이터를 사용하세요!). 이 가이드에서는 MS COCO 데이터셋의 검증 분할 (5,000 샘플)을 사용할 것입니다:\n\n```python\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\n# FiftyOne 모듈 import 및 Zoo에서 데이터셋 로드\ndataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\")\n\n# 라벨을 삭제하여 라벨이 없는 데이터로 시작하는 시뮬레이션\ndataset.select_fields().keep_fields()\n\n# 이름 변경하고 데이터베이스에 유지\ndataset.name = \"clustering-demo\"\ndataset.persistent = True\n\n# 데이터셋을 시각화하기 위해 앱 실행\nsession = fo.launch_app(dataset)\n```\n\n만약 Jupyter Notebook에서 작업 중이라면, auto=False를 전달하고 session.url이 가리키는 곳에 브라우저 탭을 열어주세요 (일반적으로 http://localhost:5151/) 앱을 전체 화면에서 볼 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-05-27-HowtoClusterImages_2.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 기능 생성하기\n\n이제 우리가 데이터를 가지고 있으니, 클러스터링에 사용할 기능을 생성해야 합니다. 이 튜토리얼에서는 두 가지 다른 기능을 살펴볼 것입니다: CLIP Vision Transformer에 의해 생성된 512차원 벡터 및 이러한 고차원 벡터를 UMAP 차원 축소 루틴을 통해 실행하여 생성된 2차원 벡터.\n\nFiftyOne 샘플 컬렉션에 차원 축소를 실행하기 위해, FiftyOne Brain의 `compute_visualization()` 함수를 사용할 것입니다. 해당 함수는 method 키워드 인수를 통해 UMAP, PCA 및 tSNE를 지원합니다. 우리는 데이터셋의 `compute_embeddings()` 메서드를 사용하여 CLIP 임베딩을 생성한 다음 이를 명시적으로 차원 축소 루틴에 전달할 수 있습니다. 그러나 대신, `compute_visualization()`에 CLIP로 임베딩을 계산하도록 암시적으로 지시하고 이러한 임베딩을 필드 \"clip_embeddings\"에 저장한 다음 이를 사용하여 2D 표현을 얻을 수 있습니다:\n\n```js\nres = fob.compute_visualization(\n  dataset,\n  (model = \"clip-vit-base32-torch\"),\n  (embeddings = \"clip_embeddings\"),\n  (method = \"umap\"),\n  (brain_key = \"clip_vis\"),\n  (batch_size = 10)\n);\ndataset.set_values(\"clip_umap\", res.current_points);\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nbrain_key 인자를 사용하면 이러한 결과에 이름으로 프로그래밍적으로 또는 FiftyOne 앱에서 앞으로 액세스할 수 있습니다. 마지막 줄은 생성된 2D 벡터의 배열을 가져와 데이터셋의 새 필드 \"clip_umap\"에 저장합니다.\n\n앱을 새로 고침하고 Embeddings 패널을 열면 데이터셋의 2D 표현을 볼 수 있습니다. 플롯의 각 점이 단일 이미지에 해당합니다:\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/0*vsnStl9tEHaj90yr.gif)\n\n## 클러스터 계산 및 시각화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희의 특징 벡터를 사용하면 FiftyOne 클러스터링 플러그인을 사용하여 데이터에 구조를 부여할 수 있습니다. FiftyOne 앱에서 키보드의 역따옴표 키를 누른 후 compute_clusters를 입력하십시오. 드롭다운 목록에서 항목을 클릭하여 클러스터링 모달을 엽니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*YG-ldErYMMZd8gV2lNSXPA.gif)\n\n클러스터링 실행 결과에 액세스하기 위해 run_key(앞에서 본 brain_key와 유사함)를 입력하십시오. 이렇게 하면 input 폼이 동적으로 업데이트되는 것을 확인할 수 있습니다. 이 시점에서 클러스터링할 특징과 사용할 클러스터링 알고리즘을 선택해야 합니다!\n\n클러스터링 방법으로 \"kmeans\"를 선택하고 특징 벡터로 \"clip_umap\"을 선택하십시오. 클러스터 수를 20으로 설정하고 다른 모든 매개변수는 기본값을 사용하십시오. 엔터 키를 눌러 클러스터링 알고리즘을 실행하십시오. 몇 초만 기다리면 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n컴퓨팅이 완료되면 샘플에 새로운 필드가 추가된 것을 알 수 있습니다. 이 필드에는 정수의 문자열 표현이 포함되어 있고, 해당 샘플이 할당된 클러스터를 나타냅니다. 이 값들을 직접 필터링하고 샘플 그리드에서 한 클러스터씩 볼 수 있습니다:\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*mptQ960wfczV7HWhnWC--w.gif)\n\n더 흥미로운 점은 우리의 임베딩 플롯에서 이 클러스터 레이블에 의해 색칠된 것입니다:\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*-vJwIeVRsfFs2y4_DPbzqA.gif)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n클러스터링된 데이터를 이런 식으로 시각화하면 클러스터링 루틴을 간단히 점검할 수 있고 데이터의 구조에 대한 직관적인 시각을 제공할 수 있습니다. 이 예시에서는 테디 베어들의 클러스터가 다른 데이터와 꽤나 잘 분리된 것을 볼 수 있습니다. 또한, 이 클러스터링 루틴은 농장 동물과 코끼리, 얼룩말과 같은 더 이국적인 동물들 사이의 경계를 찾아냈습니다.\n\n이제, 클러스터링 실행을 새로 만들어보세요. 클러스터의 수를 30개로 늘려볼 거에요 (이 새로운 필드에서 임베딩을 색칠하는 걸 잊지 마세요). 약간의 임의성에 따라 (이 루틴의 초기화가 모두 무작위임에 주의하세요), 이제는 코끼리와 얼룩말이 각자의 클러스터를 차지할 가능성이 높아질 것입니다.\n\n초기 클러스터 세트로 돌아와서, 임베딩 플롯에서 마지막 영역 중 하나를 살펴봐봅시다. 축구를 하는 사람들의 이미지 몇 장이 주로 테니스 이미지의 클러스터에 속해있는 것을 확인할 수 있습니다. 이는 2D 차원 축소된 벡터들을 임베딩 벡터 그 자체 대신에 우리의 클러스터링 루틴으로 전달했기 때문입니다. 2D 투영은 시각화에 도움이 되지만, UMAP과 같은 기술은 구조를 상당히 잘 보존하는데, 상대적 거리는 정확히 보존되지 않고 일부 정보가 손실됩니다.\n\n만약 대신 CLIP 임베딩을 직접 클러스터링 계산에 동일한 하이퍼파라미터로 전달한다면, 이러한 축구 이미지들은 다른 축구 이미지와 함께 피리스비나 야구와 같은 다른 필드 스포츠들과 함께 같은 클러스터에 할당됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*2cjbVS8JBKe8CNyyTzBQRQ.gif)\n\n주요 포인트는 고차원 특성이 저차원 특성보다 나은 것도 그 반대도 아니라는 점입니다. 각 선택에는 어떤 대가가 따릅니다. 이것이 다른 기술, 하이퍼파라미터 및 특성을 실험해보아야 하는 이유입니다.\n\n이것을 더 명확하게 보여주기 위해 HDBSCAN을 클러스터링 알고리즘으로 사용해보겠습니다. 이 알고리즘은 클러스터 수를 지정할 수 없게 하며, 대신 min_cluster_size 및 max_cluster_size와 같은 매개변수를 사용하며 클러스터를 병합할 때 기준을 명시합니다. 우리는 CLIP 임베딩을 특성으로 사용할 것이며, 대략적인 시작점으로 10부터 300까지의 요소로 이루어진 클러스터 만 원한다고 말할 것입니다. 클러스터가 너무 크면 도움이 되지 않을 수 있고, 너무 작으면 신호가 아닌 잡음에 반응할 수 있습니다. 특정 값은 물론 데이터셋에 따라 다를 것입니다!\n\n클러스터 레이블에 따라 색을 입히면 결과가 조금 어수선해 보입니다. 그러나 각 클러스터에 대한 이미지를 개별적으로 살펴보면 데이터셋에서 매우 흥미로운 샘플 집합을 식별한 것을 확인할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*hKcCc7r-OCrFsgfwMT-WYA.gif)\n\n💡 HDBSCAN의 경우, 모든 백그라운드 이미지에 \" -1 \" 레이블이 지정됩니다. 이러한 이미지는 최종 클러스터 중 어느 것에도 병합되지 않습니다.\n\n## 클러스터링 실행 추적하기\n\n다양한 피처 조합, 클러스터링 기술 및 하이퍼파라미터를 테스트하면서 특정 군집 세트를 생성하는 데 사용한 \"구성\"을 추적하고 싶을 수 있습니다. 다행히도 FiftyOne Clustering 플러그인은 사용자 지정 실행을 사용하여 이 모든 작업을 처리합니다. 플러그인은 run_key로 실행을 선택하고 모든 실행 매개변수를 앱에서 멋지게 서식이 있는 출력으로 볼 수 있는 get_clustering_run_info 연산자를 노출합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-05-27-HowtoClusterImages_3.png)\n\n데이터셋의 `get_run_info()` 메서드에 run_key를 전달하여 이 정보에 프로그래밍 방식으로 액세스할 수도 있어요!\n\n## GPT-4V로 클러스터 레이블링하기\n\n지금까지 우리의 클러스터는 단지 번호만 있었는데, 이는 우리가 사용한 일종의 정리 수단이에요. 그러나 만약 우리가 데이터셋에서 특정 특성에 따라 클러스터를 형성한다면, 그 특성을 식별하고 샘플을 대강 레이블로 사용해야 할 거에요. 단순하게 우리는 각각의 클러스터를 개별적으로 살펴보고 주어진 클러스터 내의 이미지만 선택하고 시각화해서 클러스터를 직접 태그해 볼 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n혹시… 우리가 모두가 함께 할 수 있는 멀티모달 대형 언어 모델을 사용해보는 건 어떨까요? FiftyOne Clustering 플러그인은 GPT-4V의 멀티모달 이해 능력을 활용하여 각 클러스터에 개념적 레이블을 제공합니다.\n\n이 기능을 사용하려면 OpenAI API 키 환경 변수가 필요합니다(필요한 경우 계정을 생성할 수 있음). 아래와 같이 설정할 수 있습니다:\n\n```js\nexport OPENAI_API_KEY=sk-...\n```\n\n이 기능은 label_clusters_with_gpt4v 연산자를 통해 제공되며, 각 클러스터에서 이미지 다섯 개를 무작위로 선택하여 GPT-4V에 과제별 프롬프트와 함께 공급하고 결과를 처리합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n클러스터의 개수에 따라 (GPT-4V는 느릴 수 있으며, 이는 클러스터 수의 선형적인 비례로 확장됩니다) 작업 실행을 위임하고 싶을 수 있습니다. 이를 위해 운영자 모달에서 상자를 선택하고 다음 명령어를 사용하여 작업을 시작할 수 있습니다:\n\n```js\nfiftyone delegated launch\n```\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*qkTocuUvrFm2ye8nDRuQNA.gif\" /\u003e\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 워크스루에서는 scikit-learn과 FiftyOne을 사용하여 인기있는 클러스터링 알고리즘과 심층 신경망을 결합하여 비구조화된 데이터에 구조를 부여하는 방법을 다뤘어요. 그 과정에서 특징 벡터, 알고리즘 및 선택하는 하이퍼파라미터가 클러스터링 계산의 최종 결과에 큰 영향을 미칠 수 있다는 점을 알 수 있었어요. 클러스터가 무엇을 선택하고 데이터의 구조를 얼마나 잘 식별하는지에 대해서도 그의 결과가 영향을 줄 수 있어요.\n\n이러한 클러스터링 루틴을 데이터에 적용한 후에 몇 가지 중요한 질문이 생기게 돼요:\n\n- 이러한 클러스터링 실행을 어떻게 양적으로 비교하고 대조할 수 있을까요?\n- 여러 클러스터링 실행에서 얻은 통찰력을 종합하여 데이터를 더 잘 이해하는 데 도움이 될까요?\n- 이러한 통찰력을 활용하여 더 나은 모델을 훈련하는 데 어떻게 사용할까요?\n\n이러한 질문에 답하면 클러스터링의 장점을 누리는 데 도움이 될 거예요. 만약 이 게시물을 즐겼고 이러한 후속 주제를 다루길 원한다면 알려주세요 👋!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 다음에는 무엇을 해야 할까요\n\n더 심층적으로 클러스터링 세계로 들어가고 싶다면, 다음 몇 가지 방법을 살펴보세요:\n\n- 임베딩 모델 선택: 이 가이드에서는 CLIP, 시맨틱 기반 모델을 사용했습니다. 허깅페이스의 Transformers 라이브러리나 OpenCLIP와 같은 다른 시맨틱 모델을 사용하면 어떻게 달라지는지 확인해보세요. 또한 ResNet50와 같은 \"픽셀 및 패치\" 컴퓨터 비전 모델이나 DINOv2와 같은 자기 지도 학습 모델을 사용할 때 어떻게 변하는지 살펴보세요.\n- 클러스터링 하이퍼파라미터: 이 가이드에서는 클러스터 수에 거의 손 대지 않았습니다. 이 수를 증가하거나 감소시킬 때 결과가 달라질 수 있습니다. k-평균 클러스터링과 같은 몇 가지 기술에서는 최적의 클러스터 수를 추정하기 위해 사용할 수 있는 휴리스틱이 있습니다. 여기서 멈추지 마시고 다른 하이퍼파라미터들도 실험해보세요!\n- 개념 모델링 기법: 이 가이드에서 제공된 내장 개념 모델링 기법은 GPT-4V와 약간의 가볍게 힌트를 사용하여 각 클러스터의 핵심 개념을 식별합니다. 이것은 오픈 엔드 문제를 다루는 한 가지 방법일 뿐입니다. 이미지 캡션 및 주제 모델링을 사용하거나 자신만의 기법을 만들어보세요!\n\n이 글이 마음에 들었고 FiftyOne의 활기찬 오픈 소스 커뮤니티와 소통하고 싶다면:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 🎉 FiftyOne Slack 커뮤니티에 거의 3,000명의 AI 애호가 및 실무자들과 함께 참여해보세요!\n- 🎉 AI 모임 네트워크의 12,000명 이상에 가입하여 다가오는 이벤트에 대한 소식을 받아보세요!\n- 💪 FiftyOne 프로젝트를 오픈소스인 GitHub에서 기여해보세요!\n","ogImage":{"url":"/assets/img/2024-05-27-HowtoClusterImages_0.png"},"coverImage":"/assets/img/2024-05-27-HowtoClusterImages_0.png","tag":["Tech"],"readingTime":13},{"title":"칠레의 공공 안전 문제의 정치적 측면","description":"","date":"2024-05-27 12:45","slug":"2024-05-27-ThepoliticaldimensionofthepublicsecurityprobleminChile","content":"\n\n칠레는 항상 중간-높은 수준의 발전, 민주주의 및 안전을 갖춘 국가로 알려져 왔습니다. 그러나 최근에는 이 세 가지 요소가 약화되고 있습니다.\n\n2013년에 칠레의 GDP 인당는 19,196달러(인플레이션 조정)였지만, 2023년에는 16,849달러입니다. 다시 말해, 오늘날의 칠레 경제는 10년 전보다 나쁩니다.\n\n민주주의 측면에서 정치 참여와 정치 경쟁이 늘어난 것은 사실입니다. 그러나 동시에 민주가치, 정당에 대한 신뢰, 일반적으로 민주주의 기관 및 민주주의의 반응성(기관의 대중 요구에 대한 민감성)은 더 약해졌습니다. 실제로 The Economist의 민주주의 지수에서 칠레는 2013년에 7.81의 종합 점수를 기록했으나, 2023년에는 6.62로 하락하여 1.19 포인트 또는 약 15.2% 감소한 것으로 나타났습니다. 또한, V-Dem에 따르면, 칠레의 \"반응성\" 점수는 2013년에 0.75였지만, 2023년에는 0.60으로 하락했습니다. 마지막으로, CEP 조사에 따르면, 2013년에 정당에 대한 신뢰는 단 5%에 불과했으나, 2023년에는 약 2%로 감소하여 국가의 정치 기관에 대한 불신이 확대되고 있다는 점을 강조합니다.\n\n그리고 안전 상황도 좋지 않습니다. 2013년에는 10만 명당 고의적 살인이 3.16건이었지만, 2023년에는 10만 명당 6.3건으로 크게 증가했습니다. 범죄 증가의 주요 원인은 다양한 기관의 취약성에 기인합니다. 예를 들면, 정보 수집 시스템의 약점, 교정 시설 내 수감자 통제의 미비, 안전 기관 간 협조의 부재, 테러에 대한 전략 부재, 통제되지 않은 국경, 체포 및 유죄 판결의 확률 저하, 그리고 초기 유아 시기부터의 사회 예방 정책 부재 등이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위기에 대응하기 위해 행정부와 의회가 노력해 왔습니다. 그러나 그들의 행동에는 항상 공화주의적 감각의 부재가 우세한 것 같습니다. 결과적으로 경제 성장을 촉진하는 정책은 연기되고, 안보 조치는 제때에 시행되지 않았으며, 정치 체계는 아직 개혁되지 않았습니다. 따라서 칠레가 갇혀 있는 구멍을 벗어날 수 있는 능력은 제도적 인센티브보다는 성실함의 예와 현 로직을 변화시킬 수 있는 공화주의 가치에 안내되는 위대한 지도자들에 달려 있습니다. 이것은 바로 아렌트가 \"세상에 없던 것을 삽입하는 능력\"으로 묘사한 인간의 행동입니다. 그러나 현재 논리를 변화시킬 수 있는 행동의 예다운을 제공하는 공화주의 덕목이 이를 안내해야 합니다.\n\n이러한 덕목을 기르고 문제를 해결할 수 있을까요? 네, 그러나 먼저 우리 지도자들은 이 문제가 단순히 법률 변경으로 해결할 수 없다는 것을 인식해야 합니다. 각자의 변화로 시작되는 문화적 변화가 필요합니다. 이는 용기, 용감함, 궁극적으로는 칠레에 대한 애정을 요구합니다.\n\n![이미지](/assets/img/2024-05-27-ThepoliticaldimensionofthepublicsecurityprobleminChile_0.png)","ogImage":{"url":"/assets/img/2024-05-27-ThepoliticaldimensionofthepublicsecurityprobleminChile_0.png"},"coverImage":"/assets/img/2024-05-27-ThepoliticaldimensionofthepublicsecurityprobleminChile_0.png","tag":["Tech"],"readingTime":2},{"title":"시간에 따른 변화를 시각화하는 멋진 전략들","description":"","date":"2024-05-27 12:44","slug":"2024-05-27-AwesomeStrategiestoVisualizeChangewithTime","content":"\n이 기사는 두 시점을 대조하는 효과적인 전략을 시각화하는 방법을 요약하고, 영감을 주는 그래픽 예시(소스 코드 링크 포함)로 설명합니다.\n\n더 많은 흥미로운 데이터 시각화는 DataBrewer.co에서 찾아볼 수 있습니다. 이는 R 및 Python에서 데이터 분석과 데이터 시각화 기술을 공유하는 훌륭한 플랫폼입니다.\n\n# 두 시간대 비교에 선분 사용하기\n\n다음 그래프는 각 나라의 1800년과 2015년 인간의 수명 기대값, 인구 규모, 그리고 일인당 GDP를 비교합니다. 포인트는 다른 나라의 특정 시점을 나타내며, 선분은 각 나라에서 변화의 시각적 궤적을 제공합니다. 더불어 연한 회색 사각형의 사용은 두 시기 간의 차이를 강조하는 데 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![그래프](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_0.png)\n\n이 그래프는 Lisa Charlotte Rost가 Datawrapper로 원래 만들었습니다. 이 작품을 R ggplot2로 재현한 것을 찾으려면 여기를 확인하세요.\n\n# 방향별 변경 강조를 위해 화살표 사용하기\n\n선분 외에도 화살표는 한 시점에서 다른 시점으로의 변화 방향을 강조하는 특징적인 요소입니다. 아래 그래프는 2000년부터 2020년까지 국회에서 여성들이 보유한 의석 변화를 보여주기 위해 이 전략을 사용하고 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_1.png)\n\n# 연속적인 변화를 나타내는 선 활용하기\n\n선은 데이터 포인트의 연속적인 변화를 직관적으로 보여주는 지표입니다. 아래 그래프는 1880년대부터 2000년대까지 주요 서양 국가들에서 흡연 인기의 동향을 보여주며, 미국, 독일 및 프랑스의 변화를 선명한 색상으로 강조했습니다.\n\n![이미지](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_2.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 나라에서 시간에 따라 변화 추적하는 것 이외에도, 평균 추세선과 변동성 리본을 추가하여 시각 정보를 풍부하게 할 수 있습니다. 다음 그림에서는 각 나라의 인간 기대수명 변화를 나타냅니다. 대륙별로, 중앙 두꺼운 선은 평균 수명을 보여주고, 음영 처리된 리본은 이 평균 추세 주변의 변동성을 나타냅니다. 여기에는 평균을 중심으로 평균을 중심으로 한 표준편차의 값이 있습니다.\n\n![이미지](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_3.png)\n\n# 각 시점에서의 총 값들을 나타내는 리본 사용하기\n\n위의 선 그래프는 각 나라의 동적인 변화를 보여주었습니다. 각 시간별 y 값의 누적 합이 관심사일 때, 영역 그래프를 사용하여 이 목표를 달성할 수 있습니다. 영역 그래프는 기본적으로 쌓인 선 플롯이며, 선들 사이를 채운 색깔로 연속적인 밴드나 리본을 형성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 그래프는 지난 200년 동안 전 세계에서 미국으로의 이민 인구 변동을 나타냅니다. 각 시간 지점마다 국가별 및 총 인구 수를 보여줍니다.\n\n![Migration Population Dynamics](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_4.png)\n\n위 그래프는 Mirko Lorenz가 Datawrapper를 사용하여 처음 만들었습니다. 이 작품을 R ggplot2를 이용하여 ggalluvial 패키지를 사용해 재현한 것을 여기에서 찾아볼 수 있습니다.\n\n면적/알류비움 그래프의 변형인 스트림 플롯은 미적 강화를 더해줍니다. 다음 그래프는 지난 4개 십년 동안 영화 장르의 인기 변화를 보여줍니다. 멋진 R ggstream 패키지를 사용하여 만든 이 작품의 세부 사항은 David Sjoberg의 훌륭한 블로그 기사를 확인하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Heatmap](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_5.png)\n\n# 시간 변화를 시각화하는 멋진 전략\n\n히트맵은 시간에 따른 변화를 보여주는 매력적인 도구로, 숫자 값을 색상 척도에 매핑합니다. 아래 그림은 백신 도입 전후 미국 주별 여덟 가지 감염병의 연간 발병 건수를 시각화한 것입니다. 백신이 병의 확산을 통제하는 능력을 인상적으로 보여줍니다.\n\n![Yearly Incidences](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_6.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n표는 원래 Tynan DeBold와 Dov Friedman에 의해 WSJ에서 발표되었습니다. R ggplot2에서 이 작업의 재현을 보려면 여기를 확인하십시오.\n\n색상이 변화의 전반적인 추세를 효과적으로 반영하지만 단순한 y-축과 비교했을 때 정확한 숫자 값에 대해 해석하기가 쉽지 않습니다. 히트맵에 축을 한 방향 또는 다른 방향으로 보강하는 것이 도움이 될 수 있습니다. 다음 히트맵은 지난 200년 동안의 월별 태양 흑점 활동을 보여줍니다. 히트맵 옆에 동기화된 선 플롯이 있어 y-축을 따라 태양 흑점 활동의 변화를 연속적으로 나타내어 흑점 활동 변동을 보다 직관적으로 해석할 수 있습니다.\n\n![이미지](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_7.png)\n\n# 애니메이션을 통한 변화 시각화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2007년 한스 로슬링이 전설적인 TED 토크 '당신이 본 적 없는 최고의 통계'에서 데이터 애니메이션을 유명하게 만들었습니다. 그럼에도 오늘날에도 데이터를 애니메이션으로 렌더링하는 것은 많은 데이터 분석가들에게 끔찍한 작업으로 여겨집니다. 그러나 괜찮아요! 멋진 R gganimate 패키지를 사용하면 정적 플롯을 애니메이션으로 렌더링하는 것은 시간 변수를 기반으로 데이터 집합을 \"faceting\"하는 추가 단일 코드 라인을 추가하는 것만큼 간단할 수 있습니다.\n\n다음의 애니메이션 인구 피라미드는 독일의 과거(어두운 색상)와 모의 데이터를 사용한 미래(밝은 색상)의 인구 구조를 보여줍니다. R ggplot2를 사용하여 이 애니메이션을 만드는 지침을 자세히 알고 싶다면 여기를 확인하세요.\n\n위에 소개된 모든 그래픽은 R ggplot2에서 만들었습니다. 시각화 및 데이터 과학 분야에서 패러다임을 바꾸는 도구입니다. 만일 당신이 시스템적이고 재미있는 방법으로 ggplot2를 배우고 싶다면, 내 책으로 ggplot2에서 만들어진 코믹 스타일 ebook을 확인해 보는 것을 권유합니다. 나의 책에서 배운 내용을 놀랍도록 빠르게 당신만의 멋진 시각화로 변환하는 것을 경험할 수 있을 거예요!\n","ogImage":{"url":"/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_0.png"},"coverImage":"/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_0.png","tag":["Tech"],"readingTime":4},{"title":"따뜻한 기후에서 온 러너들이 더 더운 봄 마라톤에서 더 잘 달리나요","description":"","date":"2024-05-27 12:41","slug":"2024-05-27-DoRunnersFromWarmClimatesDoBetterinHotSpringMarathons","content":"\n![DoRunnersFromWarmClimatesDoBetterinHotSpringMarathons](/assets/img/2024-05-27-DoRunnersFromWarmClimatesDoBetterinHotSpringMarathons_0.png)\n\n2024 보스턴 마라톤은 따뜻했습니다.\n\n미친 듯이 더웠던 것은 아니었습니다. 하지만 홉킨턴에서 출발할 때 기온은 50도 후반에서 60도 이하였고, 마침내 보스턴 시내 코플리 광장에 도착하기 전에 기온이 60도 후반까지 상승하였습니다.\n\n날씨가 더 나빴던 해가 있었을 것이 분명하지만, 올해의 레이스는 완주자에게 부정적인 영향을 미치었습니다. 러너들과 대화를 나누었거나 레이스 리포트를 읽었거나, 완주 시간을 한 번 확인해 보았다면 그것이 명백하게 나타납니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 이 경기에 대한 토의 중에 내 관심을 끈 주장을 여러 사람이 했어요:\n\n따뜻한 날씨가 매우 재앙이었던 이유는 많은 러너들이 겨울에서 벗어났기 때문이라는 주장이 있었어요. 그 주장의 논리적 연역은 오히려 따뜻한 기후에서 온 러너들이 올해의 경기에서 더 나은 성적을 거뒀어야 한다는 것이에요.\n\n그런데 이게 정말 사실일까요?\n\n경기 시기는 나무랄데 없는 실험을 제공하는데요. 국내 일부 지역이 따뜻하고 일부 지역이 추운 초봄에 열리기 때문이죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 질문에 대한 통찰을 얻기 위해 데이터를 찾아보겠습니다.\n\n# 우리가 사용할 데이터는 무엇인가요?\n\n이 질문에 접근하기 위해 우리는 두 종류의 데이터가 필요합니다: 선수들이 얼마나 잘 또는 못했는지를 알려주는 자료와, 마라톤 이전 훈련 중이었던 날씨에 대한 정보가 필요합니다.\n\n보스턴 선수 경기 협회(BAA)는 경주 결과를 다운로드할 수 있도록 제공하며, 저는 이 다운로드 가능한 결과 목록으로 시작했습니다. 이는 어떤 중요한 인구 통계 정보(나이, 성별, 우편번호)와 성적 데이터(완주 시간)를 제공했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 완주 시간 자체만으로는 정보가 부족합니다. BAA는 분할 데이터를 제공하지만 검색 결과에서만 확인할 수 있습니다. 자세한 분할 결과(5km 단위)는 확인할 수 있지만 한 번에 25개씩만 볼 수 있습니다. 간편함을 위해 여기 결과 세트에서 중간 지점의 분할 결과를 가져왔습니다.\n\n이러한 결과는 1,000 단위로 볼 수 있기 때문에 스크레이핑하는 것이 훨씬 빨랐고 효율적이었습니다. 중간 지점 분할 결과를 사용하여 각 선수가 레이스 후반부를 완주하는 데 걸린 시간을 계산할 수 있었습니다. 이를 통해 각 러너가 두 번째 절반에서 얼마나 느려졌는지(혹은 그렇지 않았는지)를 계산할 수 있었습니다.\n\n마지막으로, 일부 날씨 데이터가 필요합니다. BAA는 각 러너의 우편번호를 제공합니다. 유효한 우편번호가 있는 미국 러너들을 분리하여 Python 패키지 pgeocode를 사용하여 위도와 경도를 확인했습니다. 이를 통해 Open Meteo를 통해 날씨 데이터를 수집할 수 있었습니다.\n\n4월 1일부터 4월 15일까지 경주 직전 2주간의 데이터를 수집했는데, 이 기간에는 매일 최저, 최고, 평균 온도를 포함했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 정리 및 준비를 마친 후에 해당 데이터를 Kaggle의 공개 데이터 세트로 공유했습니다. 여러분은 생 데이터를 확인하고 직접 분석해보세요.\n\n# 우리의 작업 가설은 무엇인가요?\n\n우리가 시험하려는 기본 개념은 올해 따뜻한 보스턴 마라톤에서 따뜻한 기후에서 온 러너들이 서늘한 기후에서 온 러너들보다 더 잘 해냈을지에 대한 것입니다.\n\n이 가설은 일차적으로 어떤 러너들은 4월 초에 비교적 따뜻한 기후에 사는 반면, 다른 사람들은 상대적으로 서늘한 기후에 사는 것을 전제로 합니다. 이를 테스트하는 것은 꽤 단순합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 우편번호별 날씨 데이터를 살펴보고, 러너들이 더 따뜻하고 시원한 기후로 분산되는 경향을 판단할 수 있습니다.\n\n이 가설의 두 번째이자 더 복잡한 부분은 선수가 일반적인 기대에 비해 얼마나 잘 성과했는지를 결정하는 것입니다.\n\n이상적인 경우, 모든 선수들을 조사하여 그들의 목표 페이스가 무엇이었는지를 결정한 다음, 실제 완주 시간과 비교할 수 있을 것입니다. 그러나, 그것은 선택지가 아닙니다.\n\n내가 찾은 가장 가까운 대리인은 레이스의 전반부와 후반부 사이의 페이스 변동을 살펴보는 것이었습니다. 아마도, 레이스의 첫 번째 절반에서의 러너의 페이스는 그들이 희망했던 경기 시간을 어느 정도 나타낼 것으로 예상됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 번째 반에 선수가 속도를 줄였는지 여부에 따라 그들이 그 기대치를 충족했는지 결정할 수 있습니다.\n\n코스의 성격은 첫 번째 절반에서 크게 내려가고 두 번째 절반에 여러 개의 언덕이 있는 것으로, 대부분의 달리기 선수들이 좋은 해에 음의 분할을 달리기 어려울 것으로 예상됩니다.\n\n하지만 날씨가 일부 선수들에게 차이를 준다면, 더운 날씨에 더 익숙한 선수들보다 그들은 보다 큰 양의 양분으로 달리게 될 것입니다.\n\n# 4월 초에 온도 변화는 얼마나 큽니까?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 가설의 첫 부분을 검토하기 위해, 관련 기상 데이터의 지도부터 살펴볼 수 있습니다.\n\n분석의 일환으로, 저는 이 데이터를 Tableau Public에 공유하고 여러 시각화를 만들었습니다. 중요한 상호작용식 시각화를 미디엄에 삽입할 수 없지만, 아래에 화면 캡처본을 포함했습니다.\n\n이 지도와 다른 시각화와 상호작용하려면 Tableau Public의 대시보드를 방문해 주세요.\n\n![2024-05-27-DoRunnersFromWarmClimatesDoBetterinHotSpringMarathons_1.png](/assets/img/2024-05-27-DoRunnersFromWarmClimatesDoBetterinHotSpringMarathons_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 지도는 2024 보스턴 마라톤 참가자들이 사는 각 우편번호를 나타냅니다. 색깔은 4월 1일부터 15일까지의 평균 최저 기온을 보여줍니다.\n\n국가 전역에 약간의 차이가 있으며 남동부에서 기온이 훨씬 높습니다. 특히 플로리다와 텍사스는 60도대의 평균 최저 기온을 기록했습니다 (드물게는 70도 이하).\n\n미국 남동부의 다른 지역은 50도대의 기온을 보였습니다. 남부 캘리포니아와 중앙-대서양 지역 등 일부 다른 장소는 40도 후반에 도달했거나 50도 근처까지 올랐습니다. 그러나 4월 초에는 전국 대부분의 지역이 중반 40도대 이하에 있었습니다.\n\n기온의 범위는 다양하지만, 대부분의 참가자들은 지도의 서쪽에 있는 시원한 곳에서왔습니다 — 뉴 잉글랜드, 중서부 및 태평양 북서부. 보스턴 마라톤 2024년 참가자 분석을 통해 참가자들이 사는 위치에 대한 아이디어를 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 히스토그램은 완주자 수를 나타내며, 그 기준은 우편 번호의 평균 최저 온도입니다.\n\n대다수의 러너들은 4월 초에 35도에서 45도 사이의 지역에 거주합니다. 그러나 50도에서 60도의 저녁 최저 온도를 기록하는 지역 출신의 소수 그룹도 있습니다.\n\n비록 이것이 그다지 더운 온도는 아니지만, 15일 평균이며 저녁 최저 온도입니다. 어떤 날은 평균보다 높은 온도일 수 있고, 러너들이 최저 온도보다 나름 높은 기온에서 운동했을 가능성이 높습니다.\n\n그러나 우리 작은 실험의 잠재적인 단점 중 하나입니다. 만약 따뜻한 기후 출신의 러너가 충분하지 않다면, 비교를 위한 신뢰할 수있는 데이터셋을 확보할 수 없을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 최고 기온과 일일 평균 기온을 살펴봐서 다른 결과를 얻을 수 있는지도 확인해봤어요. 이 온도들은 분명히 높았지만, 최소와 최대 사이의 간격은 꽤 안정적이었어요 — 대략 10도 차이가 나거나 그 이상은 나지 않았어요. 그래서 세 가지 통계 중 아무거나 사용하고 마지막에 비슷한 분석을 얻을 수 있어요.\n\n# 러너들은 얼마나 느려졌나요?\n\n일단 온도를 일단 떠나서, 데이터가 러너들이 얼마나 느렸는지에 대해 무엇을 말하고 있을까요?\n\n먼저 전반적인 상황을 살펴보겠습니다. 러너들은 이 경기에서 얼마나 긍정적인 스플릿을 보였을까요? 아래 히스토그램이 분포를 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n왼쪽에는 매우 적은 수의 러너들(이 데이터셋의 약 17,000 중 500명 가량)이 음수 스플릿을 달성했습니다. 더 많은 비율의 러너들이 작은 양의 양수 스플릿(0에서 5%)을 달렸지만, 그래도 소수의 러너들이었습니다.\n\n이를 이해를 돕기 위해, 만약 한 러너가 첫 절반을 1시간 30분에 달렸고 5%의 양수 스플릿을 보인다면, 그 러너는 3시간 4분 30초에 도착할 것입니다.\n\n그 이후 더 큰 그룹은 5%에서 10%의 양수 스플릿을 달렸으며, 1시간 30분을 시작으로 3시간 9분에 마침을 지었습니다.\n\n하지만 대부분의 러너들은 그들의 속도가 10% 이상 증가한 것을 보였고, 많은 경우 25%를 훨씬 초과했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n달리는 중에 달리기 속도가 느려지는 것에 영향을 미치는 것은 어떤 요소일까요? 모든 사람들이 좋아하는 언덕 이외에요!\n\n# 첫 반의 시간과 양수 스플릿 간 상관 관계\n\n데이터를 살펴보면, 어떤 게 바로 눈에 띕니다. 레이스의 첫 반을 완주하는 데 걸린 시간과 레이스 도중에 얼마나 느려졌는지 사이에 명확한 양수 상관 관계가 있다는 것이죠.\n\n분명히 말하자면—이것은 잃어버린 시간의 양에 관한 것이 아닙니다. 이 관계는 속도와 잃어버린 시간의 백분율 사이에 있습니다. 그래서 더 느린 러너는 더 빠른 러너보다 더 많이 느려졌다는 것이죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 산점도는 35세 이하의 남성 러너들을 강조합니다. y축은 레이스 전반을 완주하는 데 걸린 시간(초)을 나타내고, x축은 후반전에서의 속도 변화를 퍼센트로 보여줍니다.\n\n조금 더 빠른 러너 중 일부는 상당히 느려진 것으로 보이며 — 그래프 우측 하단에 작은 그룹이 있습니다 — 반면에 보다 느린 러너들은 후반전에서 10% 이상의 양수 분할을 보는 경향이 있습니다.\n\n그래프 상에서 5,400초(1시간 30분) 주변에 일종의 갭 또는 공백이 있습니다. 이것은 Boston 재검토를 위해 첫 번째 절반을 완주하는 러너가 달리게 될 속도입니다. 기억하세요, 이 시각적 자료는 35세 이하의 남성을 대상으로 합니다.\n\n이 러너들의 대다수는 0%에서 10% 사이의 양수 분할을 보았습니다. 6,000초(1시간 40분) 이상을 시작한 보다 느린 러너들 중에서도, 대다수가 10% 이상의 양수 분할을 보았습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 표는 나이 그룹과 성별에 따라 분해된 전반부와 속도 변화 비율 간의 상관 계수를 보여줍니다.\n\n```js\n| Age Group   |   Men |   Women |\n|-------------|-------|---------|\n| Under 35    | 0.148 |   0.097 |\n| 35-39       | 0.2   |   0.16  |\n| 40-44       | 0.217 |   0.122 |\n| 45-49       | 0.205 |   0.185 |\n| 50-54       | 0.19  |   0.196 |\n| 55-59       | 0.168 |   0.199 |\n| 60-64       | 0.066 |   0.113 |\n| 65-69       | 0.176 |   0.069 |\n```\n\n이 상관 관계들은 아주 강한 것은 아니지만 모두 양의 관계입니다. 시각화 결과와 결합하면, 선수들이 경험하는 초기 속도와 감속 비율 사이에 어떤 긍정적인 연관성이 있다는 것이 명백합니다.\n\nTableau에서 시각화를 다양한 성별 및 연령 그룹을 더 자세히 탐색해볼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 날씨가 얼마나 중요한가요?\n\n따라서 시작점으로 삼는다면, 날씨가 러너의 양분에 미치는 영향이 더 큰지 작은지 어떤지요?\n\n적어도 이 경우에는 따뜻한 기후에서 오는 것이 따뜻한 날씨에서 성능이 향상된다는 명확한 증거가 많지 않다는 사실이 밝혀졌습니다.\n\n위의 시각적 표는 연령 그룹, 성별 및 평균 최저 기온에 따른 속도 변화의 백분율을 설명합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 색은 서로 다른 온도를 나타냅니다. 왼쪽에서 오른쪽으로 증가하는 온도입니다. 왼쪽 세 막대는 쌀쌀한 날씨입니다. 네 번째 막대는 쌀쌀하고 따뜻한 사이에 있으며 가장 오른쪽(노란색) 막대는 따뜻한 날씨(평균 최저 온도 60도 이상)를 나타냅니다.\n\n따뜻한 훈련 날씨가 도움이 된다면, 막대가 오른쪽으로 이동할수록 줄어들어야 합니다.\n\n35세 이하의 젊은 여성들은 그렇게 보이는 것 같습니다. 하지만 40-44세 여성들은 반대로 행동합니다. 전반적으로, 애매한 상황 같습니다. 때로는 온난한 날씨에서 러너가 더 잘 할 때도 있고, 때로는 변화가 없을 때도 있으며, 때로는 더 나쁠 때도 있습니다.\n\n드롭다운 상자를 사용하여 남성 결과로 전환하면 비슷한 혼합된 결과를 얻을 수 있습니다. 명확한 방향이 나타나지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 산점도는 각 러너의 성별과 나이 대비 평균 최저 온도의 변화량을 보여줍니다. 이 시각화는 35세 미만의 여성에만 초점을 맞추고 있습니다.\n\nTableau에서 이 시각화를 확인하면 성별 및 연령 그룹의 다양한 필터를 사용해볼 수 있습니다.\n\n추세선은 약간 하향 추세가 있지만 이 그래프의 R² 값(35세 미만의 여성 대상)은 0.05 미만으로 매우 약합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n| Age Group   |    Men |   Women |\n|-------------|--------|---------|\n| Under 35    | -0.052 |  -0.071 |\n| 35-39       | -0.001 |   0.024 |\n| 40-44       | -0.043 |   0.022 |\n| 45-49       |  0.020 |  -0.013 |\n| 50-54       |  0.008 |  -0.084 |\n| 55-59       |  0.003 |   0.024 |\n| 60-64       |  0.055 |   0.013 |\n| 65-69       |  0.023 |   0.131 |\n\nAnd if you look at this full table of correlation coefficients — between average min temperature and percent change in pace — there’s a real mixed bag between weak negative correlations and weak positive correlations.\n\nI looked at the same tables based on correlations involving the max temp and the mean daily temp, and neither one had stronger correlations. If you limit the results to faster runners who likely qualified on time, the relationship disappears altogether in many cases.\n\nBased on this data, it’s hard to conclude that living in a warm climate has a significant impact on how well a runner performs in a warm, spring marathon — compared to a runner living in a cool climate.\n\n\n\n\n여기 평균 최저 온도와 속도 변화 비율 사이의 상관 계수에 대한 전체 표를 보면, 약한 음의 상관 관계와 약한 양의 상관 관계 사이에 혼합된 결과가 있습니다.\n\n최대 온도 및 평균 일일 온도와 관련된 상관 관계를 기반으로 한 동일한 표를 살펴보았는데, 강한 상관 관계는 더 강해지지 않았습니다. 시간에 맞게 자격을 취득한 빠른 러너에 제한해도, 많은 경우에는 관련성이 완전히 사라집니다.\n\n이 데이터를 기반으로, 따뜻한 기후에서 살면 따뜻한 봄 마라톤에서 어떤 선수가 성과에 미치는 영향이 서늘한 기후에서 사는 선수와 비교해 유의미한 차이가 있는지 결론을 내리기 어렵습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 가능한 설명과 추가 질문\n\n이것은 날씨가 중요하지 않다는 것을 의미합니까?\n\n아니요, 물론 그렇지 않습니다. 날씨가 성능에 영향을 미친다는 연구가 많이 있고, 따뜻한 날씨에 적응하는 것이 그 성능 저하에 대한 일부 보호 이점을 제공한다는 것이 확인되었습니다.\n\n올해 초에 미국 올림픽 트라이얼 마라톤에 대한 시리즈를 썼습니다. 그 시리즈에서 인용한 연구에 따르면, 보스턴에서 올해 경험한 것과 같은 60도대의 온도는 상당한 성능 저하를 일으킬 수 있습니다. 또한 비슷한 따뜻한 날씨에서 훈련하는 것이 도움이 된다고 생각하는 것은 완전히 논리적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n올해 보스턴 마라톤 데이터의 결과 (또는 그 반대)를 설명하는 것은 무엇일까요?\n\n누가 알겠어요, 하지만 여기에 네 가지 가능한 (일부) 설명이 있습니다.\n\n첫째, 4월 초에는 날씨가 그다지 따뜻하지 않습니다. 미국의 가장 따뜻한 지역인 플로리다와 텍사스는 온도가 60도에 도달하는 것이 전부입니다. 다른 '따뜻한' 지역도 50도대에 불과합니다. 이는 더위에 완전히 적응하기에 충분히 따뜻하지 않을 수 있습니다.\n\n만약 온도가 일관되게 70도 이상이었다면, 아마 이러한 러너들은 서늘한 날씨 러너들과 비교했을 때 더 나은 성적을 올릴 수도 있었을 것입니다. 현재 상황에서는, 따뜻한 날씨 스포츠 선수들이 아직 완전한 열 적응을 이루지 못했을 수도 있습니다. 4월 대신 5월에 경주를 했다면 결과가 달라졌을지도 모릅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n둘째, 따뜻한 기후에서는 많은 선수들이 활동하지 않습니다. 만약 50도와 60도가 어떤 식으로든 적응할만큼 충분히 따뜻한 범위로 가정한다면, 여전히 안정적인 분포를 만들기 위해 충분한 규모의 인구가 필요합니다.\n\n나이와 성별로 나누어 보면, 여전히 시원한 기후의 2,000명 이상의 선수들이 있지만, 따뜻한 기후에는 약 200명 정도만 있습니다. 그 작은 그룹에는 더 많은 무작위성이 있을 것이며, 이로 인해 기후의 가능한 영향이 흐릿해질 수 있습니다. 따라서 데이터의 사용 가능한 부분에 잠재적인 문제가 있고, 더 많은 데이터가 있다면 개선될 수 있습니다.\n\n셋째, 미국의 선수들만으로 인구를 좁혔습니다. 이는 정확한 날씨 데이터를 수집하기 위해 최상의 위치 데이터를 가지고 있었기 때문입니다.\n\n국제적인 도시와 국가를 위도와 경도로 변환하는 도구 세트를 찾을 수 있었을 것이지만, 조금 더 추가 작업이 필요했을 것입니다. 이렇게 하면 데이터 집합이 확대되었을 것이며, 아마도 더 다양한 온도 범위를 제시할 수도 있었을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그만큼 중남미 및 남미 북부 지역 출신의 완주자들이 상당히 많았습니다.\n\n마지막으로, 내가 사용한 지표(전반에서 후반으로의 페이스 변화율)는 결국 별로 유용하지 않았을지도 모릅니다.\n\n저는 이것을 사용했는데, 기대치에 대한 선수의 성적을 가장 잘 측정할 수 있는 방법이라고 생각했습니다. 그러나 그것은 그 유용성을 제한할 수 있는 훈련, 페이스 조절 경험, 날씨를 고려해 페이스를 조정하는 선택 등 많은 방해 변수들이 있습니다.\n\n# 탐구할 가치 있는 추가 질문들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제가 이 글을 한국어로 번역해 드리겠습니다.\n\n자신만의 데이터를 탐색하고 싶은 경우, 아래 두 가지 방법이 있습니다:\n\n- 기술적으로 능숙하지 않은 경우, Tableau Public을 방문하여 이 데이터셋을 기반으로 한 시각화를 살펴볼 수 있습니다.\n- 데이터 분석 능력이 있는 경우, Kaggle에서 직접 데이터셋과 상호 작용할 수 있습니다.\n\n자신만의 결론을 도출하거나, 제 분석에 대한 생각이 있다면 언제든지 응답을 남겨주세요. 의견을 듣고 싶습니다.\n\n이 분석을 통해, 이 특정 질문에 대해 살펴볼 다른 방법이 하나 더 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n성별과 연령대별로 데이터를 분류하는 대신, 아마도 시간 제한을 채워야 하는 완주자들을 분리하고 싶어요. 첫 번째 절반 경주 속도를 기반으로, 해당 성별 및 연령대 그룹에 대한 보스턴 예선 시간을 충족할 것으로 예상되는 러너들을 식별할 수 있어요.\n\n그러면 각기 다른 기후에서 더 많은 러너 그룹을 남길 수 있고, 아마 더 중요한 결과를 만들어 낼 수도 있을 거예요.\n\n덤으로 떠오른 관련 질문 중 하나는, 추가 데이터 수집이 필요한 질문이지만, 바로 '보스턴에서 전형적인 양분이 어떤지' 일거예요. 러너들이 보통 얼마나 느려지는가, 그리고 그것은 경주 날씨에 얼마나 변할까요?\n\n이런 질문에 관심이 있거나, 마라톤에 관한 다른 데이터 기반 이야기에 관심이 있으시다면, 반드시 이메일 업데이트를 구독하세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 열렬한 러너이자 데이터 괴짜입니다. 올해의 보스턴 마라톤에는 커트라인에 걸려 참가하지 못했지만, 2025년에는 반드시 참가할 예정입니다. 제 활동을 계속해서 지켜볼 수 있는 방법을 알려드릴게요:\n\n- 저의 훈련 소식을 알고 싶다면 Running with Rock을 팔로우해주세요.\n- 마라톤 훈련 계획을 선정하는 데 도움이 될 수 있는 팁을 확인해보세요.\n- Strava에서 저를 쫓아보세요! 🏃‍♂️\n","ogImage":{"url":"/assets/img/2024-05-27-DoRunnersFromWarmClimatesDoBetterinHotSpringMarathons_0.png"},"coverImage":"/assets/img/2024-05-27-DoRunnersFromWarmClimatesDoBetterinHotSpringMarathons_0.png","tag":["Tech"],"readingTime":11},{"title":"NBA 네트워크 분석 Neo4j를 활용한 연결하기","description":"","date":"2024-05-27 12:40","slug":"2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j","content":"\n## 두 NBA 선수 사이의 가장 짧은 링크를 찾는 동안... 그리고 그래프 데이터베이스로 놀아보기.\n\n![Image](/assets/img/2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_0.png)\n\n# 소개\n\nNBA 경기를 관람하다가 해설자들이 가리키는 선수들 간의 예상치 못한 연결에 흥미를 느낀 적이 있나요? 이러한 즉흥적인 발언에 영감을 받아 우리는 프로젝트에 착수했습니다. 영화 역할을 통해 이질적인 배우들을 연결하는 '케빈 베이컨의 여섯 단계'의 개념과 마찬가지로 NBA 선수들을 비슷한 방식으로 연결하려고 노력했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 블로그 포스트에서는 우리가 그래프 데이터베이스를 탐험한 과정과 다른 세대의 NBA 선수들 간의 관계를 발견하는 데 Neo4j를 어떻게 활용했는지에 대해 안내해드릴 거에요.\n\n![이미지](https://miro.medium.com/v2/resize:fit:996/0*ZjUZ-n48dZTqe14R.gif)\n\n# 프로젝트 개요\n\n우리의 목표는 명확했습니다: 서로 다른 NBA 선수들 간의 가장 짧은 경로를 식별하는 것이었어요. 이를 달성하기 위해 NBA 드래프트 데이터와 함께 작업하여, Neo4j를 사용해 복잡한 선수 간의 연결을 매핑하는 데 도움을 받았어요. 이 작업은 단순히 숫자들을 연산하는 데 그치는 것이 아니라, 데이터 안에 숨겨진 매력적이고 가끔은 꽤 재미있는 이야기들을 발견하는 데 관한 것이었어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 이 프로젝트를 복제하는 데 필요한 단계를 간략하게 설명합니다:\n\n- 필요한 데이터 확보하기;\n- 데이터를 노드와 엣지로 포맷팅하기;\n- Neo4j 데이터베이스 인스턴스 구성하기;\n- 데이터를 Neo4j로 가져오기;\n- 데이터와 상호 작용하기 위해 Cypher 표기법 활용하기.\n\n# 데이터 획득과 처리\n\n대부분의 데이터 애호가들처럼, 우리는 NBA 드래프트 데이터에 초점을 맞춘 Kaggle에서 데이터셋을 확보했습니다. 그런 다음 Python을 사용하여 이 데이터셋을 처리했습니다. 이 과정은 데이터를 그래프 데이터베이스에 필요한 형식에 맞게 변환하는 것을 포함했습니다: 엔티티를 노드로 분할하고 연결을 엣지로 나누는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](https://miro.medium.com/v2/resize:fit:608/0*eaiZYXxaq4pFOsiH.gif)\n\nAfter some basic processing, we can input the data to the Neo4j instance using the neo4j python package.\n\nTo increase the complexity of our ‘Six Degrees’ game though, we decided to only consider relationships of players with the team they were initially drafted from, rather than all the teams they played for throughout their career. A good challenge always adds a bit of spice!\n\n# Why Graph Databases?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단히 말해서, 그래프 데이터베이스는 데이터를 노드에 저장하고 노드 간의 관계를 엣지를 사용하여 표현합니다. 이를 통해 NBA 선수 연결과 같이 복잡하게 연결된 데이터를 시각화하고 분석하는 데 적합합니다.\n\n이 프로젝트에 구조화된 데이터베이스를 사용할 수도 있었지만, 몇 가지 이유로 그래프 데이터베이스를 선택했습니다. 그 이유 중 일부는 다음과 같습니다: 데이터 포인트 간의 복잡한 연결을 처리하는 데 더 적합하다는 점; 데이터가 얼마나 크고 복잡하더라도 빠르고 효율적으로 유지한다는 점; 무엇보다도 이러한 데이터베이스의 시각적인 특성 덕분에 선수 간의 연결을 이해하고 확인하기 쉽습니다. ‘Six Degrees’와 같은 프로젝트에서 두 선수 간의 가장 짧은 링크를 찾는 경우, 그래프 데이터베이스가 정말 빛을 발합니다.\n\n# Neo4j 사용하기\n\n이 프로젝트에 사용한 도구는 Neo4j였습니다. 우리는 Neo4j의 무료 샌드박스를 활용했는데, 이는 비용 없이 이용할 수 있는 작고 임시적인 데이터베이스입니다. 인스턴스를 설정하고 그 가능성을 발견하고 싶다면 위 링크를 통해 시도해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음과 같이 노드가 생성되었습니다:\n\n- Player: NBA 선수를 나타냅니다. (예: Jalen Brunson)\n- Team: NBA 프랜차이즈를 나타냅니다. (예: Dallas Mavericks)\n- Organization: 선수가 선발된 대학, 대학교 및 국제 기관을 나타냅니다. (예: Villanova)\n- Draft Class: 실제 드래프트 연도를 나타냅니다. (예: 2018)\n\n노드 간의 연결은 다음 엣지를 통해 설정됩니다:\n\n- DRAFTED_BY: 선수와 프랜차이즈를 연결합니다. (예: Jalen Brunson ↔ Dallas Mavericks)\n- IS_OF_DRAFT_SEASON: 선수와 드래프트 클래스를 연결합니다. (예: Jalen Brunson ↔ 2018)\n- IS_OF_ORG: 선수와 기관을 연결합니다. (예: Jalen Brunson ↔ Villanova)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 텍스트를 번역하면 다음과 같습니다.\n\n네오4j 데이터베이스를 채우고 아래와 같이 시각적으로 매력적인 그래프를 만들 수 있었어요! 다음과 같은 Cypher 구문을 사용하여 데이터베이스를 쿼리할 수 있어요:\n\n```js\n# 제일로 브런슨(Jalen Brunson)의 연결을 보여줘 (id: 1628973)\nMATCH (n:Player {id: 1628973})\nRETURN n\n```\n\n\u003cimg src=\"/assets/img/2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_1.png\" /\u003e\n\n하나의 선수의 연결을 시각화하고 그리는 것은 비교적 쉽지만, 대량의 개체를 처리할 때 복잡해집니다. 여기서 그래프 데이터베이스의 강점이 정말로 드러나는 거죠!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식으로 변환된 표입니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*2jWbvPxLe9SzQw_YTXR11A.gif)\n\n# 결과 및 발견\n\n몇 가지 간단한 쿼리를 사용하여 데이터 세트를 탐색할 수 있습니다:\n\n```js\n# 유형별 그룹화된 모든 엣지 수 세기\nMATCH ()-[relationship]-\u003e()\nRETURN TYPE(relationship) AS type, COUNT(relationship) AS amount\nORDER BY amount DESC;\n\n# 유형별 그룹화된 모든 노드 수 세기\nMATCH (n)\nRETURN labels(n)[0] AS type, COUNT(*) AS amount\nORDER BY amount DESC;\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 결과 요약입니다:\n\n### 노드 (8,900)\n\n- Player: 7,884\n- Organization: 903\n- DraftClass: 74\n- Team: 39\n\n### 연결 (24,320)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- IS_OF_DRAFT_SEASON: 8,454\n- DRAFTED_BY: 8,001\n- IS_OF_ORG: 7,865\n\n하지만 가장 중요한 쿼리 — 이 블로그 포스트의 주요 목표인 — 두 Player 엔티티 사이의 가장 짧은 경로를 보여주는 쿼리입니다!\n\n여러분… 그 시간입니다! 🥁\n\n```js\n# 두 선수 사이의 가장 짧은 경로를 보여줍니다\nMATCH path=shortestPath(\n  (p1:Player {id: \"PLAYER-ID-1\"})-[*]-(p2:Player {id: \"PLAYER-ID-2\"})\n)\nRETURN path\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 쿼리를 서로 다른 선수 ID에 대해 실행하여 NBA 선수들 간의 흥미로운 관계를 볼 수 있습니다. 어떤 관계는 간단하지만, 다른 것들은 조금 복잡할 수도 있어요:\n\n- 조쉬 하트와 돈테 디빈첸조 모를 사이에 빌라노바로 연결된 관계를 볼 수 있어요. 대학 시절 함께 뛰었기 때문이에요:\n\n![image](/assets/img/2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_2.png)\n\n- 또한, 동일한 드래프트 클래스를 공유한 선수들, 즉 르브론 제임스와 듀에인 웨이드는 2003년 드래프트 클래스를 통해 연결되어 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_3](/assets/img/2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_3.png)\n\n- 서로 매우 다른 세대에서 나왔지만 동일한 팀에 드래프트된 선수들도이 연결을 공유합니다. 즉, D'Angelo Russel과 Jerry West는 둘 다 레이커스에 드래프트되었습니다.\n\n![2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_4](/assets/img/2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_4.png)\n\n지금까지 간단한 연결을 본 것처럼, 재미있는 부분은 LeBron James와 Kobe Bryant 사이의 연결과 같은 놀라운 연결을 찾는 것입니다. 이 두 거장이 직접적인 연결을 공유하지 않기 때문에, 그들을 연결하는 경로가 반드시 있어야 합니다. 어떻게 될까요? 🥁🥁🥁🥁🥁\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_5.png)\n\nEver wondered how Zydrunas Ilgauskas, the Lithuanian center drafted by the Cavaliers in ’96, played a key role in linking LeBron and Kobe?\n\n![gif](https://miro.medium.com/v2/resize:fit:784/0*eblTJD0vmN4IyzQ9.gif)\n\nBut if you’re a Cavaliers fan — or simply a curious individual who researched the 1996 NBA Draft, you’ll find that the Cavaliers had more than one pick; they had three. So, how can we show all possible shortest paths between Kobe and LeBron? We would need to modify our query slightly:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 레브론 제임스(2544)와 코비 브라이언트(977) ID 매치\nMATCH path=allShortestPaths(\n  (p1:Player {id: 2544})-[*]-(p2:Player {id: 977})\n)\nRETURN path\n```\n\n결과:\n\n\u003cimg src=\"/assets/img/2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_6.png\" /\u003e\n\n# 추가 정보: 몇 가지 추가적 탐색 데이터 분석\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 나아가 Bleacher Report에 따르면 역사상 가장 우수한 드래프트 클래스는 1984 년의 것으로, Michael Jordan, John Stockton, Charles Barkley, Hakeem Olajuwon 등의 이름이 소개되었습니다.\n\n![이미지](/assets/img/2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_7.png)\n\n모든 연도에서, 어떤 팀이 가장 많은 선수들을 섭외했을까요? 다음 쿼리가 이 질문에 대한 답을 제시합니다:\n\n```js\n# 각 팀이 섭외한 선수의 수를 세는 쿼리\nMATCH (t:Team)\u003c-[:DRAFTED_BY]-(p:Player)\nRETURN t.team_name AS Team, count(p) AS Drafts\nORDER BY Drafts DESC\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과에 따르면 사크라멘토 킹스가 최상의 위치를 차지했습니다(508명), 뒤를 이어 애틀랜타 호크스(489명)와 뉴욕 닉스(473명)가 있습니다. 킹스의 드래프트 픽 분포를 자세히 살펴봅시다...\n\n```js\n# 킹스가 올해 드래프트한 모든 선수 및 그들의 드래프트 클래스 가져오기\nMATCH path = (t:Team {team_name: 'Kings'})\n\u003c-[:DRAFTED_BY]-\n(p:Player)\n-[:IS_OF_DRAFT_SEASON]-\u003e\n(d:DraftClass)\nRETURN path\n```\n\n이 쿼리는 킹스에 의해 드래프트된 모든 선수와 해당 드래프트 클래스를 보여주는 멋진 그래프를 반환합니다.\n\n![2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_8](/assets/img/2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_8.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오른쪽에 보시다시피 킹스가 픽을 하지 않은 드래프트 클래스들이 있습니다. NBA 창시 프랜차이즈인 킹스 팀이 픽을 하지 않은 것은 가능한 일일까요? 네, 70년대 초 이전에는 킹스 프랜차이즈가 로열스로 알려졌기 때문입니다. 1970년 이전에 이 프랜차이즈의 모든 드래프트 픽은 로열스 팀 노드에 할당되었습니다.\n\n# 결론\n\n우리는 Neo4j를 사용하여 NBA 선수 관계를 매핑하여 'Six Degrees' 게임에 데이터 중심 접근법을 적용했습니다. 이 프로젝트는 데이터 분석뿐만 아니라 농구의 상호 연결된 세계를 탐험하는 것이었습니다!\n\n이러한 다양한 관계를 탐색하는 것은 압도적이면서 매혹적일 수 있습니다. 저는 최종 이미지로 마무리하며, 이 그래프가 얼마나 멋질 수 있는지를 보여주는데, 그 복잡성과 아름다움에서 불꽃놀이를 연상케 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```json\n경로 = (p:Player) -[:DRAFTED_BY|IS_OF_DRAFT_SEASON*1..2]-\u003e (t)\nWHERE (t:Team) OR (t:DraftClass) OR ((:Player) -[:IS_OF_ORG]-\u003e (t))\nRETURN path\nLIMIT 500\n```\n\n![이미지](/assets/img/2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_9.png)\n\n# 감사의 말\n\n이 프로젝트의 MVP인 João Pedro Boufleur에게 특별한 찬사를 보냅니다. 그의 기여는 이 모험의 성공에 필수적이었습니다.```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n- Kaggle: NBA 데이터베이스\n- Neo4j 샌드박스\n- Neo4j Cypher 매뉴얼\n","ogImage":{"url":"/assets/img/2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_0.png"},"coverImage":"/assets/img/2024-05-27-NBANetworkAnalysisConnectingtheDotswithNeo4j_0.png","tag":["Tech"],"readingTime":8}],"page":"9","totalPageCount":11,"totalPageGroupCount":1,"lastPageGroup":11,"currentPageGroup":0},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"9"},"buildId":"kNTo-t2jvQG5kfHDWIcB-","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>