<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/9" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/9" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_buildManifest.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="검색 증강 생성RAG을 최적화하는 4가지 전략" href="/post/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="검색 증강 생성RAG을 최적화하는 4가지 전략" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="검색 증강 생성RAG을 최적화하는 4가지 전략" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">검색 증강 생성RAG을 최적화하는 4가지 전략</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">17<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="대형 언어 모델LLMs로 Next-Token 예측을 분류로 전환하는 방법" href="/post/2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대형 언어 모델LLMs로 Next-Token 예측을 분류로 전환하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대형 언어 모델LLMs로 Next-Token 예측을 분류로 전환하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">대형 언어 모델LLMs로 Next-Token 예측을 분류로 전환하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기" href="/post/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="약물 발견의 출발점으로서 방대한 화학 공간을 구조 기반 가상 스크리닝하기 위한 방법" href="/post/2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="약물 발견의 출발점으로서 방대한 화학 공간을 구조 기반 가상 스크리닝하기 위한 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="약물 발견의 출발점으로서 방대한 화학 공간을 구조 기반 가상 스크리닝하기 위한 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">약물 발견의 출발점으로서 방대한 화학 공간을 구조 기반 가상 스크리닝하기 위한 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="BERT 미세 조정으로 텍스트 분류하는 방법" href="/post/2024-06-23-FinetuneBERTfortextclassification"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="BERT 미세 조정으로 텍스트 분류하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="BERT 미세 조정으로 텍스트 분류하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">BERT 미세 조정으로 텍스트 분류하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="자율 에이전트의 흥망성쇠 그 발전과 한계 " href="/post/2024-06-23-TheRiseandFallofAutonomousAgents"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="자율 에이전트의 흥망성쇠 그 발전과 한계 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="자율 에이전트의 흥망성쇠 그 발전과 한계 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">자율 에이전트의 흥망성쇠 그 발전과 한계 </strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="대형 언어 모델LLM로 민감한 데이터 처리하는 방법 총정리" href="/post/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대형 언어 모델LLM로 민감한 데이터 처리하는 방법 총정리" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대형 언어 모델LLM로 민감한 데이터 처리하는 방법 총정리" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">대형 언어 모델LLM로 민감한 데이터 처리하는 방법 총정리</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LangChain, Semantic Kernel, AutoGen 최신 비교 및 분석" href="/post/2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LangChain, Semantic Kernel, AutoGen 최신 비교 및 분석" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LangChain, Semantic Kernel, AutoGen 최신 비교 및 분석" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">LangChain, Semantic Kernel, AutoGen 최신 비교 및 분석</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="대형 언어 모델LLM을 위한 토큰 마스킹 전략들" href="/post/2024-06-23-TokenMaskingStrategiesforLLMs"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대형 언어 모델LLM을 위한 토큰 마스킹 전략들" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대형 언어 모델LLM을 위한 토큰 마스킹 전략들" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">대형 언어 모델LLM을 위한 토큰 마스킹 전략들</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">18<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="반품 및 환불 처리를 위한 챗봇 활용 엔터프라이즈 워크플로우 통합 방법" href="/post/2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="반품 및 환불 처리를 위한 챗봇 활용 엔터프라이즈 워크플로우 통합 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="반품 및 환불 처리를 위한 챗봇 활용 엔터프라이즈 워크플로우 통합 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">반품 및 환불 처리를 위한 챗봇 활용 엔터프라이즈 워크플로우 통합 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><a class="link" href="/posts/1">1</a><a class="link" href="/posts/2">2</a><a class="link" href="/posts/3">3</a><a class="link" href="/posts/4">4</a><a class="link" href="/posts/5">5</a><a class="link" href="/posts/6">6</a><a class="link" href="/posts/7">7</a><a class="link" href="/posts/8">8</a><a class="link posts_-active__YVJEi" href="/posts/9">9</a><a class="link" href="/posts/10">10</a><a class="link" href="/posts/11">11</a><a class="link" href="/posts/12">12</a><a class="link" href="/posts/13">13</a><a class="link" href="/posts/14">14</a><a class="link" href="/posts/15">15</a><a class="link" href="/posts/16">16</a><a class="link" href="/posts/17">17</a><a class="link" href="/posts/18">18</a><a class="link" href="/posts/19">19</a><a class="link" href="/posts/20">20</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"검색 증강 생성RAG을 최적화하는 4가지 전략","description":"","date":"2024-06-23 19:38","slug":"2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG","content":"\n\n# 개인 데이터 및 개인 인프라를 활용한 고급 AI 솔루션\n\n![image](/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_0.png)\n\n머신 러닝 모델을 최적화하기 위해 권장 사항을 몇 번이나 따라보았는데, 여전히 특정 요구 사항과 잘 맞지 않는 솔루션에 부딪힌 적이 몇 번이나 있나요? 대답을 알아요: 많은, 아니면 모든 경우에 해당할 거예요. 모든 것이 데이터에 달려 있기 때문이죠. 특정 상황에 가장 적합한 방법을 찾을 때까지 테스트하고 실패하고 또 다시 테스트해야만 해요. 이 기사에서는 고급 AI 솔루션을 위해 개인 데이터와 개인 인프라를 활용하여 검색 증강 생성(Retrieval-Augmented Generation, RAG)을 최적화하는 네 가지 전략을 제시합니다.\n\n이전 기사에서는 LLaMA 3 같은 공개 모델에 개인 지식을 포함시키기 위해 검색 증강 생성(RAG) 전략을 활용하는 방법을 설명했습니다. RAG를 사용하면 개인 데이터를 개인 인프라에 저장하면서도 민감한 정보를 다른 사람들과 공유하지 않고 사용할 수 있습니다. RAG를 활용하는 장점은 명백하지만, 구현에는 여러 부분에서 중요한 조정이 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## RAG Recap\n\n저는 이전 글에서 설명한 RAG에 대한 간단한 개요부터 시작하겠습니다. 두 가지 주요 프로세스가 있습니다. 첫 번째는 \"데이터 수집 프로세스\"로, 다양한 소스에서 데이터를 수집하여 텍스트로 변환한 후 작은, 일관된 및 의미론적으로 관련 있는 부분으로 분할하고 결과를 벡터 데이터베이스에 저장합니다. 두 번째는 \"추론 프로세스\"로, 사용자 쿼리로 시작하여 첫 번째 프로세스의 결과를 사용하여 관련 데이터 부분을 식별하고 마지막으로 모델의 컨텍스트를 풍부하게하여 출력을 얻습니다.\n\n다음 다이어그램에서 두 프로세스의 자세한 내용을 볼 수 있습니다:\n\n![다이어그램](/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지난 기사에서는 ColdF 라는 가상의 회사에서 데이터를 사용하여 \"데이터 수집\" 및 \"추론\" 프로세스를 생성했습니다. 이 기사에서는 이러한 프로세스의 결과를 평가하고 최적화하는 기본적인 방법을 설명하겠습니다.\n\n## 개선 구성 요소\n\n먼저, 우리의 RAG 프로세스에서 중요한 지점을 식별하는 것부터 시작해봅시다:\n\n- 청킹 접근: 의미 있고 맥락적으로 관련 있는 데이터 세그먼트를 보장하도록 청킹 크기를 최적화합니다.\n- 임베딩 모델: 의미 표현을 개선하기 위해 모델 선택 및 세부 조정을 수행합니다.\n- 벡터 검색 방법: 효과적인 유사성 측정 및 검색 매개변수 선택합니다.\n- 모델에 피드할 최종 프롬프트: 효율적인 결과 품질을 개선하기 위해 효과적인 프롬프트를 작성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## RAG 파이프라인에서의 A/B 테스팅\n\n각 개선 구성 요소를 식별한 후, 전략은 각 구성 요소의 두 가지 다른 구성을 가진 두 버전을 비교하여 어떤 것이 더 나은 성능을 발휘하는지 결정합니다. 이는 두 버전을 실행하고 미리 정의된 지표에 대한 성능을 측정하는 것을 포함합니다. 하지만 성능을 어떻게 측정할까요? 그리고 어떤 지표를 사용해야 할까요? 이에 대한 답변으로 우리는 “RAGAS: 자동화된 검색 증강 생성 평가”¹ 논문을 사용합니다. 이 논문은 세 가지 주요 지표를 제안합니다:\n\n- 충실도: 답변에 있는 정보가 문맥에 제공된 정보와 일치하는지 확인합니다. 답변이 충실하다면 그 안에 있는 모든 내용이 문맥에서 직접 찾거나 추론할 수 있습니다. 예를 들어, 문맥이 “5월에 리스본을 방문했을 때, 앨리스와 나는 알파마, 바이로 알토, 벨렘 타워 그리고 다른 여러 곳으로 갔다”이고 답변이 “5월에 앨리스는 알파마, 바이로 알토, 벨렘 타워 그리고 다른 여러 곳으로 갔다”라면 문맥은 추출된 모든 내용을 지지하므로 충실도 점수는 100%입니다. 그러나 답변이 “5월에 앨리스는 알파마와 상 호르헤 성에 갔다”라면 답변에서 추출된 두 내용 중 (예: “앨리스는 알파마로 갔다” 및 “앨리스는 상 호르헤 성에 갔다”) 하나만이 문맥에서 지지되므로 충실도 점수는 50%입니다.\r\n- 답변 관련성: 생성된 답변이 완전하고 직접적으로 질문에 대답하는지 확인합니다. 정보가 정확하든 아니든 관련성이 있습니다. 예를 들어, 질문이 “포르투갈의 수도는 무엇인가?”이고 답변이 “리스본은 포르투갈의 수도입니다”라면 이 답변은 질문에 직접 대답하므로 관련성이 있습니다. 답변이 “리스본은 다양한 명소가 많은 아름다운 도시입니다”라면 부분적으로 관련성이 있을 수 있지만 질문에 대답하는 데 직접 필요한 정보가 아닌 추가 정보가 포함되어 있습니다. 이 지표는 답변이 집중되고 핵심을 유지하도록 보장합니다.\r\n- 문맥 관련성: 문맥에서 제공된 정보가 질문에 대답하는 데 얼마나 도움이 되는지 확인합니다. 필요한 것만 포함되고 불필요한 관련 없는 정보는 제거되므로 질문에 직접적으로 도움이 되지 않는 부가 정보를 제거합니다. 예를 들어, 질문이 “5월에 앨리스가 리스본에서 어떤 장소를 방문했나요?”이고 문맥이 “5월에 리스본을 방문했을 때, 앨리스는 알파마, 바이로 알토, 벨렘 타워 그리고 다른 여러 곳으로 갔다”이면 이 문맥은 앨리스가 5월에 어떤 장소를 방문했는지에 대한 필수 정보만을 제공하므로 매우 관련성이 높습니다. 그러나 문맥이 “5월에 리스본을 방문했을 때, 앨리스는 많은 흥미로운 사람을 만나 맛있는 음식을 먹었으며 다양한 장소를 갔다”이면 이 문맥은 질문에 대답하는 데 필요하지 않은 부가 정보를 포함하고 있으므로 관련성이 없는 것으로 간주됩니다. 이 지표는 질문에 대답하는 데 도움이 되는 정보만 포함되어 불필요한 자세를 피하도록 보장합니다. 이 지표는 문맥 정밀도로도 불립니다.\n\n이 논문은 또한 이러한 지표가 LLM을 통해 완전 자동화된 방식으로 측정될 수 있다는 방법에 대해 설명합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 평가에서 사용할 라이브러리인 Ragas는 이러한 주요 메트릭의 진화를 보여주며 새로운 메트릭을 추가했습니다:\n\n- Context Recall: 이 메트릭은 문맥과 실제 답변 간의 일치 정도를 측정합니다. Context Relevance와 마찬가지로 생성된 답변 대신 실제 답변을 사용합니다. 이 메트릭을 얻으려면 참 값이 필요합니다. 이러한 전략의 효과를 평가하기 위해 ColdF 데이터를 바탕으로 실제 답변이 포함된 10개의 질문 세트를 준비했습니다.\n\nFaithfulness와 Answer Relevance는 생성기 메트릭으로서, 각각 환영과 답변이 질문과 얼마나 직접적인지를 측정합니다.\n\nContext Relevance와 Context Recall은 검색기 메트릭으로, 벡터 데이터베이스에서 올바른 데이터 청크를 검색하고 필요한 모든 정보를 얻는 능력을 측정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전에 제시한 네 가지 메트릭을 평가하려면 질문, 생성된 답변, 맥락 및 실제 답변이 필요합니다.\n\nLangChain을 사용하여 RAG 프로세스를 구현할 것입니다. 코드를 실행하려면 Python이 설치되어 있어야 하며(version 3.11.9) 다음 라이브러리가 필요합니다:\n\n- ollama==0.2.1\n- chromadb==0.5.0\n- transformers==4.41.2\n- torch==2.3.1\n- langchain==0.2.0\n- ragas==0.1.9\n\n다음은 LangChain을 사용한 코드 스니펫입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\n# 필요한 라이브러리 및 모듈 가져오기\nfrom langchain.embeddings.base import Embeddings\nfrom transformers import BertModel, BertTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer, RobertaModel, RobertaTokenizer\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_text_splitters import MarkdownHeaderTextSplitter\nimport requests\nfrom langchain_chroma import Chroma\nfrom langchain import hub\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.chat_models import ChatOllama\nfrom operator import itemgetter\n\n# DPRQuestionEncoder를 사용하여 사용자 지정 임베딩 클래스 정의\nclass DPRQuestionEncoderEmbeddings(Embeddings):\n    show_progress: bool = False\n    \"\"\"tqdm을 설치해야 하는지 여부 표시합니다.\"\"\"\n    \n    def __init__(self, model_name: str = 'facebook/dpr-question_encoder-single-nq-base'):\n        # 지정된 모델 이름으로 토크나이저와 모델 초기화\n        self.tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(model_name)\n        self.model = DPRQuestionEncoder.from_pretrained(model_name)\n        \n    def embed(self, texts):\n        # texts가 리스트인지 확인\n        if isinstance(texts, str):\n            texts = [texts]\n        \n        embeddings = []\n        if self.show_progress:\n            try:\n                from tqdm import tqdm\n                iter_ = tqdm(texts, desc=\"임베딩 중\")\n            except ImportError:\n                logger.warning(\n                    \"tqdm을 가져올 수 없어 진행률 표시줄을 표시할 수 없습니다. \"\n                    \"`pip install tqdm`으로 설치하세요.\"\n                )\n                iter_ = texts\n        else:\n            iter_ = texts\n\n        for text in iter_:\n            # 입력 텍스트 tokenize\n            inputs = self.tokenizer(text, return_tensors='pt')\n            # 모델을 사용하여 임베딩 생성\n            outputs = self.model(**inputs)\n            # 임베딩 추출하고 리스트로 변환\n            embedding = outputs.pooler_output.detach().numpy()[0]\n            embeddings.append(embedding.tolist())\n        \n        return embeddings\n    \n    def embed_documents(self, documents):\n        return self.embed(documents)\n    \n    def embed_query(self, query):\n        return self.embed([query])[0]\n\n# 프롬프트 생성을 위한 템플릿 정의\ntemplate = \"\"\"\n### CONTEXT\n{context}\n\n### QUESTION\nQuestion: {question}\n\n### INSTRUCTIONS\nCONTEXT 마크다운 텍스트를 사용하여 사용자의 질문에 답변하세요.\n간결하고 명료한 답변을 제공하세요.\n질문에 답변하기 위해 CONTEXT에 근거만 사용하세요.\nCONTEXT에 필요한 정보가 없는 경우 'NONE'을 반환하세요.\n\"\"\"\n\n# 템플릿을 사용하여 ChatPromptTemplate 인스턴스 생성\nprompt = ChatPromptTemplate.from_template(template)\n\n# URL에서 텍스트 데이터 가져오기\nurl = \"https://raw.githubusercontent.com/cgrodrigues/rag-intro/main/coldf_secret_experiments.txt\"\nresponse = requests.get(url)\nif response.status_code == 200:\n    text = response.text\nelse:\n    raise Exception(f\"파일을 가져오는 데 실패했습니다: {response.status_code}\")\n\n# 마크다운 텍스트를 분할할 헤더 정의\nheaders_to_split_on = [\n    (\"#\", \"Header 1\")\n]\n\n# 지정된 헤더로 MarkdownHeaderTextSplitter 인스턴스 생성\nmarkdown_splitter = MarkdownHeaderTextSplitter(\n    headers_to_split_on, strip_headers=False\n)\n\n# 마크다운 splitter를 사용하여 텍스트 분할\ndocs_splits = markdown_splitter.split_text(text)\n\n# Chat 모델 초기화\nllm = ChatOllama(model=\"llama3\")\n\n# 사용자 지정 임베딩을 사용하여 문서에서 Chroma vector store 생성\nvectorstore = Chroma.from_documents(documents=docs_splits, embedding=DPRQuestionEncoderEmbeddings())\n\n# Vector store에서 retriever 생성\nretriever = vectorstore.as_retriever()\n\n# 문서 형식 지정을 위한 함수 정의\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# 검색 보완 생성 (RAG) chain 생성\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n    | {\"answer\": prompt | llm | StrOutputParser(), \n       \"context\": itemgetter(\"context\")}\n)\n\n# 질문으로 RAG chain 실행\nresult = rag_chain.invoke(\"Who led the Experiment 1?\")\nprint(result)\r\n```\r\n\r\n위 코드의 끝에 RAG Chain이 정의되어 있으며 이 코드를 사용하여 메트릭을 평가할 수 있습니다:\r\n\r\n```js\r\n# 필요한 라이브러리 및 모듈 가져오기\nimport pandas as pd\nfrom datasets import Dataset\nfrom ragas import evaluate\nfrom ragas.metrics import (\n        context_precision,\n        faithfulness,\n        answer_relevancy,\n        context_recall\n)\nfrom langchain_community.chat_models import ChatOllama\n\ndef get_questions_answers_contexts(rag_chain):\n    \"\"\" 질문 및 답변 목록을 읽고 ragas 데이터셋을 반환합니다. \"\"\"\n    # 파일 URL\n    url = 'https://raw.githubusercontent.com/cgrodrigues/rag-intro/main/coldf_question_and_answer.psv'\n\n    # URL에서 파일 가져오기\n    response = requests.get(url)\n    data = response.text\n   \n    # 데이터를 줄 단위로 분할\n    lines = data.split('\\n')\n\n    # 각 줄을 파이프 기호로 나누어 튜플 생성\n    rag_dataset = []\n\n    for line in lines[1:10]: # 처음 10개 질문만\n        if line.strip():  # 공백이 아닌지 확인\n            question, reference_answer = line.split('|')\n            result = rag_chain.invoke(question)\n            generated_answer = result['answer']\n            contexts = result['context']\n\n            rag_dataset.append({\n                \"question\": question,\n                \"answer\": generated_answer, \n                \"contexts\": [contexts], \n                \"ground_truth\": reference_answer\n            })\n\n          \n    rag_df = pd.DataFrame(rag_dataset)\n    rag_eval_datset = Dataset.from_pandas(rag_df)\n    \n    # ragas 데이터셋 반환\n    return rag_eval_datset\n\ndef get_metrics(rag_dataset):\n    \"\"\" RAG Dataset에 대해 신의성, 답변 관련성, 컨텍스트 정밀도, \n        컨텍스트 재현율 메트릭 계산 \"\"\"\n    # 계산할 메트릭 목록\n    metrics = [\n        faithfulness,\n        answer_relevancy,\n        context_precision,\n        context_recall\n    ]\n        \n    # LLaMA 3 모델을 사용할 지역 ChatOllama\n    langchain_llm =  ChatOllama(model=\"llama3\")\n    langchain_embeddings = DPRQuestionEncoderEmbeddings('facebook/dpr-question_encoder-single-nq-base')\n\n    # 메트릭 반환\n    results = evaluate(rag_dataset, metrics=metrics, llm=langchain_llm, embeddings=langchain_embeddings)\n    return results\n\n# RAG 데이터셋 가져오기\nrag_dataset = get_questions_answers_contexts(rag_chain)\n\n# 메트릭 계산\nresults = get_metrics(rag_dataset)\nprint(results)\r\n```\r\n\r\n위 코드를 실행하여 결과를 살펴보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```json\r\n{\n  '신뢰성': 0.8611, \n  '답변 관련성': 0.8653, \n  '맥락 정밀도': 0.7778, \n  '맥락 회수율': 0.8889\n}\r\n```\n\n\u003cimg src=\"/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_2.png\" /\u003e\n\n이미 언급된 것처럼, 첫 번째 두 지표(예: 신뢰성 및 답변 관련성)는 생성 과정과 관련이 있습니다. 이는 이러한 지표를 향상시키기 위해서는 언어 모델이나 모델에 입력되는 프롬프트를 변경해야 한다는 것을 의미합니다. 마지막 두 지표(예: 맥락 정밀도 및 맥락 회수율)는 검색과 관련이 있으며, 이는 이러한 지표를 향상시키기 위해서는 문서가 저장되고 색인화되며 선택되는 방식을 개선해야 한다는 것을 의미합니다.\n\n## 청킹 접근 방식\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n청킹 접근 방식은 데이터가 검색을 위해 최적의 세그먼트로 분할되도록 보장합니다. 이 패러다임은 다양한 청크 크기를 실험하여 너무 작아서 문맥이 빠진다거나 너무 크면 (검색 시스템을 압도하는) 문제가 발생하지 않도록 균형을 찾는 것을 포함합니다. 베이스라인에서는 각 실험을 기준으로 문서를 청크로 분할합니다. 그렇기 때문에 실험의 일부가 희석되어 최종 임베딩에 포함되지 않을 수 있습니다. 이 상황을 해결하기 위한 한 가지 가능한 접근 방식은 부모 문서 검색기를 사용하는 것입니다. 이 방법은 특정 관련 문서 단편이나 문단뿐만 아니라 그들의 부모 문서도 검색합니다. 이 접근 방식은 관련 단편 주변의 문맥이 보존되도록 보장합니다. 아래 코드는 이 접근 방식을 테스트할 때 사용되었습니다:\n\n```js\n# 필요한 라이브러리 및 모듈 가져오기\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\n# 부모 문서 검색기 생성\nparent_document_retriever = ParentDocumentRetriever(\n    vectorstore = Chroma(collection_name=\"parents\", \n                         embedding_function=DPRQuestionEncoderEmbeddings('facebook/dpr-question_encoder-single-nq-base')),\n    docstore = InMemoryStore(),\n    child_splitter = RecursiveCharacterTextSplitter(chunk_size=200),\n    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500),\n)\n\nparent_document_retriever.add_documents(docs_splits)\n\n\n# 검색 보강 생성 (RAG) 체인 생성\nrag_chain_pr = (\n    {\"context\": parent_document_retriever | format_docs, \"question\": RunnablePassthrough()}\n    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n    | {\"answer\": prompt | llm | StrOutputParser(), \n       \"context\": itemgetter(\"context\")}\n)\n\n# RAG 데이터세트 가져오기\nrag_dataset = get_questions_answers_contexts(rag_chain_pr)\n\n# 메트릭스 계산\nresults = get_metrics(rag_dataset)\nprint(results)\r\n```\n\n결과는 다음과 같습니다:\n\n```js\r\n{\n  'faithfulness': 0.6667, \n  'answer_relevancy': 0.4867, \n  'context_precision': 0.7778, \n  'context_recall': 0.6574\n}\r\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 성능 향상에 기여하지 않는 것을 보여줍니다. 감소하는 컨텍스트 회상은 회수 과정이 제대로 작동하지 않고 컨텍스트에 완전한 정보가 없음을 나타냅니다. 충실성 및 답변 관련성 지표의 변화는 풍부하지 않은 컨텍스트에서 나옵니다. 이 경우, 청킹 및 회수를 위한 다른 방법을 평가해 볼 수 있습니다.\n\n## 포함 모델\n\n포함 모델은 텍스트 청크를 밀집 벡터 표현으로 변환합니다. 서로 다른 모델은 다양한 주제에서 훈련될 수 있으며 때로는 임베딩을 개선할 수 있습니다. 임베딩 방법의 선택은 계산 효율성과 임베딩 품질 사이의 균형을 고려해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 Dense Passage Retrieval (“facebook/dpr-question_encoder-single-nq-base”), Sentence-BERT (“paraphrase-MiniLM-L6-v2”), 또는 Chroma의 기본 모델 (“all-MiniLM-L6-v2”)과 같은 다양한 임베딩 모델을 비교합니다. 각 모델은 강점을 갖고 있으며 도메인 특정 데이터에서 평가하여 가장 정확한 의미 표현을 제공하는지를 결정하는 데 도움이 됩니다.\n\n임베딩 모델을 변경하기 위해 새로운 클래스 \"SentenceBertEncoderEmbeddings\"를 정의하는 것이 필요합니다. 이 새로운 클래스는 모델인 Sentence-BERT 모델을 구현합니다. 이 새로운 클래스는 우리 이전 임베딩인 \"DPRQuestionEncoderEmbeddings\"를 대체할 것이며, 이것은 아래의 코드로 Sentence-BERT 모델을 테스트하는 데 사용한 코드입니다:\n\n```js\n코드 내용\n```\n\n결과는 요렇습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n{\n  'faithfulness': 0.5278, \n  'answer_relevancy': 0.5306, \n  'context_precision': 0.5556, \n  'context_recall': 0.7997\n}\n```\n\n\u003cimg src=\"/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_4.png\" /\u003e\n\n이 경우, 인코더의 변경은 메트릭스에서 성능이 저하된 것을 의미합니다. 이는 DPR이 Sentence-BERT보다 검색 정확도가 더 높기 때문에, 정확한 문서 검색이 중요한 경우에는 DPR이 더 적합하다는 것을 의미합니다. Sentence-BERT로 전환할 때 '신뢰성' 및 '답변 관련성' 메트릭스의 상당한 하락은 높은 검색 정밀성을 필요로 하는 작업에 적합한 임베딩 모델을 선택하는 중요성을 강조합니다.\n\n## 벡터 검색 방법\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n벡터 검색 방법은 유사성 측정을 기반으로 가장 관련성 높은 청크를 검색합니다. 흔한 방법으로는 유클리드 (L2) 거리, 코사인 유사도 등이 있습니다. 이 검색 방법을 변경하면 최종 출력 품질을 향상시킬 수 있습니다.\n\n다음은 코드입니다:\n\n```js\n# 필요한 라이브러리 및 모듈 가져오기\nimport pandas as pd\nfrom datasets import Dataset\nfrom ragas import evaluate\nfrom ragas.metrics import (\n        context_precision,\n        faithfulness,\n        answer_relevancy,\n        context_recall\n)\nfrom langchain_community.chat_models import ChatOllama\n\n# 문서에서 Chroma 벡터 저장소 생성\n# 사용자 정의 임베딩을 사용하고 코사인 유사도 검색으로 변경\nvectorstore = Chroma.from_documents(collection_name=\"dist\", \n                                    documents=docs_splits, \n                                    embedding=DPRQuestionEncoderEmbeddings(), \n                                    collection_metadata={\"hnsw:space\": \"cosine\"})\n\n# 벡터 저장소로부터 리트리버 생성\nretriever = vectorstore.as_retriever()\n\n# 검색 증강 생성 (RAG) 체인 생성\nrag_chain_dist = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n    | {\"answer\": prompt | llm | StrOutputParser(), \n       \"context\": itemgetter(\"context\")})\n\n# RAG 데이터 세트 가져오기\nrag_dataset = get_questions_answers_contexts(rag_chain_dist)\n\n# 메트릭 계산\nresults = get_metrics(rag_dataset)\nprint(results)\n```\n\n이것이 결과입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n{\n  'faithfulness': 0.9444, \n  'answer_relevancy': 0.8504, \n  'context_precision': 0.6667, \n  'context_recall': 0.8889\n}\n```\n\n![Image](/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_5.png)\n\n'충실성'에서의 개선은 벡터 검색에 코사인 유사도를 사용하여 검색된 문서를 쿼리와 더 잘 일치시키는 것을 나타냅니다. '문맥 정밀도'가 감소했지만 전체적으로 높은 '충실성'과 '문맥 회수율'은 이 맥락에서 코사인 유사도가 보다 효과적인 벡터 검색 방법이라는 것을 보여주며, 검색 성능을 최적화하기 위한 벡터 검색 방법의 선택의 중요성을 뒷받침합니다.\n\n## 모델에 피드 할 최종 프롬프트\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최종 프롬프트 구성은 검색된 데이터를 모델의 쿼리에 통합하는 것을 의미합니다. 프롬프트의 작은 변화는 결과에 큰 영향을 미칠 수 있어 시행 착오 과정이 될 수 있습니다. 프롬프트 내의 예시를 제공하면 모델이 더 정확하고 관련성 높은 결과물을 생성할 수 있습니다.\n\n## 결론\n\n검색 증강 생성(Retrieval-Augmented Generation, RAG) 파이프라인을 최적화하는 것은 특정 데이터와 애플리케이션 컨텍스트에 매우 의존적인 반복적인 과정입니다. 본 논문에서는 네 가지 주요 전략을 탐구했습니다: 청킹 접근 방식의 개선, 임베딩 모델의 선택과 세밀한 튜닝, 효과적인 벡터 검색 방법의 선택, 정확한 프롬프트 작성. 이러한 구성 요소 각각이 RAG 시스템의 성능 향상에 중요한 역할을 합니다.\n\n결과는 일반적인 해결책이 없음을 강조했습니다. 예를 들어, 우리의 맥락에서 Dense Passage Retrieval (DPR)은 Sentence-BERT보다 우수한 성과를 보였지만, 이는 다른 데이터셋이나 요구 사항에 따라 달라질 수 있습니다. 마찬가지로, 벡터 검색에서 코사인 유사도로 전환하면 더 나은 충실도와 콘텍스트 회수가 나타나는 것으로 나타났는데, 검색 과정의 미묘한 변경이 영향을 미치는 것을 보여주었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRAG 파이프라인을 최적화하는 여정은 지속적인 테스트, 실패로부터 교훈을 얻고, 정보에 기반한 조정을 포함합니다. 이 반복적인 방식을 받아들이면, AI 솔루션을 더 효과적으로 사용자의 요구에 맞게 맞춤화할 수 있습니다. 성공의 열쇠는 데이터를 이해하고, 다양한 전략을 실험해보며, 끊임없이 프로세스를 개선하는 데 있습니다.\n\n내 프로필과 이메일 목록을 구독하여 최신 작업을 업데이트 받아보세요. 함께하면, AI 최적화의 복잡성을 탐험하고 데이터 기반 솔루션의 완전한 잠재력을 발휘할 수 있습니다. \n\n## 참고 자료\n\n[1] Es, S., James, J., Espinosa-Anke, L., \u0026 Schockaert, S. (2023). RAGAS: Automated Evaluation of Retrieval Augmented Generation. Exploding Gradients, CardiffNLP, Cardiff University, AMPLYFI.","ogImage":{"url":"/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_0.png"},"coverImage":"/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_0.png","tag":["Tech"],"readingTime":17},{"title":"대형 언어 모델LLMs로 Next-Token 예측을 분류로 전환하는 방법","description":"","date":"2024-06-23 19:36","slug":"2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs","content":"\n\n![이미지](/assets/img/2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs_0.png)\n\n대규모 언어 모델(Large Language Models, LLMs)은 방대한 양의 인터넷 데이터로 훈련되어 다양한 자연어 작업을 수행할 수 있습니다. 그 중 하나인 분류는 주제를 미리 정의된 레이블로 분류하는 지도 학습 작업입니다. 제로샷 및 퓨샷 분류는 인기 있는 기술로, LLMs가 훈련 데이터 없이 또는 몇 가지 예제로 분류 작업을 수행할 수 있습니다. 그러나 보다 정확도를 높이기 위해 가이드 미세 조정을 통해 LLMs의 성능을 향상시킬 수 있음이 입증되었습니다.\n\n# 가이드 미세 조정 LLMs\n\n가이드 미세 조정을 위한 일반적인 방법은 질문-답변 쌍으로 구성된 데이터셋을 작성하는 것입니다. 사전 훈련된 LLMs는 이러한 쌍을 사용하여 지도 학습 방식으로 추가로 미세 조정됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당신은 지난 포스트에서 이 접근법을 확인할 수 있어요.\n\n선택적으로, 데이터셋을 좋아하는 조합과 덜 선호하는 조합의 쌍으로 구성된 직접 선호도 최적화(DPO)를 사용하여 성능을 더 개선할 수 있어요. 상위 순위의 오픈소스 LLMs가 이러한 접근법 중 하나를 사용하여 훈련되는 것은 놀라운 일이 아니에요.\n\n# LLMs는 딥 뉴럴 네트워크입니다\n\n직관적으로, 이러한 방법은 LLMs가 토큰을 기반으로 하는 아키텍처를 사용한다는 사실을 활용해요. 지시된 데이터와 선호도 데이터셋 모두에서 텍스트 쌍이 토큰으로 변환됩니다. 교차 엔트로피 손실과 디코더만을 사용하는 오토레그레시브 속성을 이용하여, LLMs의 가중치가 업데이트되는데, 레이블 토큰은 입력 토큰으로부터 복사되지만 하나씩 밀려서 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주어진 것을 감안하면 우리는 어휘를 분류 라벨로 교체할 수 있습니다! 다음 토큰을 어휘에서 예측하는 대신, 앞선 문맥 토큰에서 분류 작업의 범주를 예측하는 데 관심이 있습니다. 이것은 일반적으로 lm_head로 구현되는 LLMs의 헤드를 변경함으로써 가능합니다. 텍스트 생성에서, lm_head는 (임베딩 차원, 어휘 크기)의 모양을 가지고 있습니다. 분류를 위해 우리는 이것을 (임베딩 차원, 분류 수)로 수정합니다.\n\n# 분류 작업을 위해 LLMs 학습하기\n\n## 모델 및 토크나이저 불러오기\n\n이 접근 방식이 작동하는지 확인하기 위한 실험을 수행해 봅시다. 먼저, HuggingFace에서 사전 훈련된 LLM 및 해당 토크나이저를 사용하여 시작하겠습니다. 본 연구에서는 경량화된 38억 개의 파라미터 모델 microsoft/Phi-3-mini-4k-instruct를 선택했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"microsoft/Phi-3-mini-4k-instruct\"\ndevice_map = \"auto\"\ntrust_remote_code = True\n\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name, device_map=device_map, trust_remote_code=trust_remote_code\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side = \"right\"\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\n```\n\n우리의 분류 실험을 간단히 하기 위해 이진 분류 작업을 선택할 것입니다. 나중에 여러 클래스로 확장할 수 있음을 알고 있습니다. 이진 분류에서 클래스 수는 두 개이므로 torch.nn.Linear(hidden_size, 2)입니다. to(\"cuda:`number`\") 함수는 이 레이어가 할당된 GPU 기기를 지정합니다. 이 레이어가 model_name = \"auto\"을 사용하여 초기로드된 모델이 있는 동일한 기기에 할당되었는지 확인해주세요.\n\n## Modify LLMs Head\n\n```python\n# 모델 수정 및 세밀 조정\nhidden_size = 3072\nmodel.lm_head = torch.nn.Linear(hidden_size, 2).to(\"cuda:3\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또 다른 흥미로운 실험은 fe-fine-tuning을 위해 필요한 레이어를 결정하는 것입니다. 이 게시물에서는 모델이 모든 다른 레이어에서 토큰의 문맥적 의미를 학습했다는 가정에 기반하여, 마지막 블록의 마지막 정규화 레이어에 초점을 맞출 것입니다. 이 레이어는 lm_head 이전의 끝에서 두 번째 레이어입니다.\n\n먼저, 모든 레이어의 Weight를 'param.requires_grad = False'로 지정하여 동결시킵니다. 그리고 나서 마지막 블록의 마지막 정규화 레이어를 찾아 'param.requires_grad = True'로 가중치를 조정 가능하도록 변경합니다. HuggingFace의 모델 클래스에서는 아래 코드 스니펫에서 보여주는 대로 dot 연산을 사용하여 어떤 레이어로든 이동할 수 있습니다.\n\n```js\n# 마지막 블록과 마지막 정규화 레이어만 fine-tune\nfor param in model.parameters():\n    param.requires_grad = False\n\n# 마지막 정규화 레이어만 fine-tune\nlast_block = model.model.layers[-1].to(\"cuda:3\")\nfinal_norm = model.model.norm.to(\"cuda:3\")\n\nfor param in final_norm.parameters():\n    param.requires_grad = True\n```\n\n## Cross Entropy Loss 정의\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전 게시물에서 lm_head의 가장 중요한 로짓이 문장의 마지막 토큰과 관련이 있다고 설명했습니다. 이는 self-attention 메커니즘에서 기인하는데, 마지막 토큰은 이전 모든 문맥 토큰들로부터의 주의 점수를 가지고 있습니다. 따라서 우리는 logits[:, -1, :]를 사용합니다.\n\n```python\nimport torch\n\ndef calculate_loss_batch(input_batch, target_batch, model):\n    input_batch, target_batch = input_batch.to(\"cuda\"), target_batch.to(\"cuda\")\n    logits = model(input_batch).logits[:, -1, :]  # 마지막 출력 토큰의 로짓\n    loss = torch.nn.functional.cross_entropy(logits, target_batch).to(\"cuda\")\n    return loss\n```\n\n## 데이터셋 준비\n\n데이터셋은 텍스트와 레이블 두 개의 리스트로 구성되어 있습니다. 먼저, 토크나이저를 사용하여 텍스트 문자열을 토큰화된 ID로 변환합니다. 토크나이저는 정의된 최대 길이보다 긴 토큰을 자르거나 최대 길이보다 짧은 경우 패딩합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\necoding = tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n)\n```\n\n그 다음으로, torch에서 DataLoader 객체를 사용하여 데이터를 모델 튜닝을 위해 반복적으로 공급합니다. BinaryClassification Dataset 객체는 torch의 Dataset 객체를 상속하며, 이 데이터셋을 DataLoader 객체로 로드합니다.\n\n```js\nclass BinaryClassificationDataset(Dataset):\n      pass\n\ndataset = BinaryClassificationDataset(texts, labels, tokenizer, max_length)\n\n# DataLoader\ndataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n```\n\n1단계와 2단계를 합쳐서, 이것이 완전한 구현입니다. 메인 함수에 예제를 제공하여 BinaryClassificationDataset 객체를 인스턴스화하고 DataLoader 객체를 만들어 데이터를 생성하는 방법을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\n\nclass BinaryClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].squeeze(0)  # Remove batch dimension\n        attention_mask = encoding['attention_mask'].squeeze(0)  # Remove batch dimension\n\n        return {\n            'input_ids': input_ids.long(),\n            'attention_mask': attention_mask.long(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    texts = [\"Hello, this is a sample sentence.\", \"Another sample text for classification.\"]\n    labels = [0, 1]\n\n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    # Define dataset\n    max_length = 128\n    dataset = BinaryClassificationDataset(texts, labels, tokenizer, max_length)\n\n    # DataLoader\n    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n\n    # Iterate through the dataloader\n    for batch in dataloader:\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        labels = batch['labels']\n\n        print(\"Input IDs:\", input_ids)\n        print(\"Attention Mask:\", attention_mask)\n        print(\"Labels:\", labels)\n      \n```\n\n## 모델 훈련\n\n손실 함수를 정의하고 데이터셋을 준비한 후, 모델 훈련을 시작할 수 있습니다. calculate_loss_loader 함수는 losses를 0으로 초기화합니다. 지정된 배치 수(num_batches)를 지정하면, calculate_loss_batch 함수를 사용하여 각 배치의 손실을 계산합니다. DataLoader 객체를 사용하여 calculate_loss_batch 함수에 배치 데이터를 반복적으로 제공하고, 각 반복에 대해 손실을 누적합니다. 재현성을 위해 torch.manual_seed(1234)를 사용합니다. 이 실험에서 0.68의 손실을 달성했습니다.\n\n```python\ndef calculate_loss_loader(data_loader, model, num_batches=None):\n    losses = 0.\n    if len(data_loader) == 0:\n        return \"Please provide dataset, data loader is empty.\"\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches = min(num_batches, len(data_loader))\n    for index, batch in enumerate(data_loader):\n        if index \u003c num_batches:\n            loss = calculate_loss_batch(\n              batch[\"input_ids\"], batch[\"labels\"], model\n            )\n            losses += loss.item()\n        else:\n            break\n    return total_loss / num_batches\n\n\ntorch.manual_seed(1234) # 재현성을 위해\nwith torch.no_grad():\n    train_loss = calculate_loss_loader(dataloader, model, num_batches=2)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 트레이드오프\n\n다음 토큰 예측에서 트랜스포머를 처음 원리를 통해 분류 예측으로 활용할 수 있다는 것은 흥미롭습니다. 미리 훈련된 트랜스포머 모델은 언어 패턴의 풍부한 표현을 포착합니다. 그러나 가벼운 모델이지만 38 억 개의 매개변수를 가지고 있습니다. 독자들은 정확성과 교육 그리고 추론 처리량(초당 생성된 토큰 수) 사이의 트레이드오프를 인식해야 합니다. 저는 기준선으로 더 작은 모델을 시작하는 것을 제안합니다. 예를 들어, 6700만 개의 매개변수를 가진 distilbert-base-uncased 모델을 사용할 수 있습니다. 또는 XGBoost와 같은 더 전통적인 머신러닝 모델을 시도할 수도 있습니다.\n\n# 결론\n\n본 블로그 포스트에서는 처음 원리를 사용하여 다음 토큰 예측 문제를 분류 레이블 예측으로 변환하는 방법을 보여드렸습니다. 이 데모를 통해 LLM의 복잡한 구조를 해체하고 도메인별 문제에 동일한 개념을 적용하는 지식을 습득할 수 있기를 희망합니다. 전체 분류 작업에 대해 트랜스포머가 최적화된 솔루션이 아닐 수 있으며, 다른 더 작은 모델과 데이터를 학습하고 평가해야 함을 상기해야 합니다. 읽어 주셔서 감사합니다. 행복한 학습 되세요!","ogImage":{"url":"/assets/img/2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs_0.png"},"coverImage":"/assets/img/2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs_0.png","tag":["Tech"],"readingTime":9},{"title":"서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기","description":"","date":"2024-06-23 19:35","slug":"2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification","content":"\n\n데이터와 컴퓨터 프로그램의 세계에서 머신 러닝이라는 개념은 어려운 문제 같을 수도 있어요. 복잡한 수학과 이해하기 어려운 개념이 가득한 것 같죠.\n\n그래서 오늘은 여기서 멈추어서, 제 MLBasics 시리즈의 새로운 이슈를 통해 모든 것이 어떻게 작동하는지 기본적인 사항을 살펴보고 싶어요.\n\n오늘의 안건은 서포트 벡터 머신을 이해하는 것이에요.\n\n이 강력한 도구는 데이터를 명확한 범주로 분류하는 데 도움이 되지만...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떻게 동작하는 건가요?\n\nSupport Vector Machines 모델을 간단히 설명해 보겠습니다👇🏻\n\n# Support Vector Machine이란?\n\nSupport Vector Machine (SVM)은 두 가지 다른 클래스로 데이터 포인트를 가장 잘 분리하는 초평면을 찾으려는 지도 학습 알고리즘입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 문제는 이를 수행할 수 있는 무한한 수의 초평면이 존재한다는 점이 어렵습니다. 그래서 SVM의 목표는 클래스를 최대 여백으로 가장 잘 분리하는 초평면을 식별하는 것입니다.\n\n![image](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png)\n\n# SVM의 주요 개념\n\n더 깊이 파고들기 전에, 몇 가지 핵심 용어를 이해해 보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Support Vectors(서포트 벡터): 이들은 초평면에 가장 가까운 데이터 포인트로, 초평면의 위치와 방향에 큰 영향을 미칩니다.\n- 여백(Margin): 여백은 초평면과 각 클래스에서 가장 가까운 데이터 포인트 사이의 거리입니다. 더 큰 여백은 분류기의 일반화를 더 잘 시킬 것입니다.\n- 초평면(Hyperplane): 2차원 공간에서 데이터를 두 부분으로 나누는 선입니다. 고차원에서는 평면이나 고차원의 유사 구조체입니다.\n\n![이미지](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_1.png)\n\n# SVM이 작동하는 방식\n\n두 종류의 데이터 포인트가 있는 데이터셋을 상상해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 파란색 🔵\n- 노란색 🟨\n\n새 데이터 포인트를 파란색 또는 노란색 중 하나로 분류하고 싶습니다. 주요 과제는 두 클래스를 분리할 수 있는 다양한 하이퍼플레인이 존재한다는 것인데, 그런 다음 큰 질문이 있습니다:\n\n어떻게 최적의 하이퍼플레인을 찾을까요?\n\n![이미지](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 좋은 초평면은 두 클래스로부터 최대 거리를 가지는 것입니다. 이는 가능한 다양한 초평면을 찾고 두 클래스로부터 최대 거리를 가지는 것을 선택함으로써 수행됩니다.\n\n# SVM 뒤에 숨겨진 수학적 직관\n\n데이터를 분류하는 방법을 이해하기 위해 수학적 측면을 살펴보겠습니다.\n\n점곱은 하나의 벡터를 다른 벡터에 따라 투영하는 것을 말합니다. 그래서 우리는 한 쪽의 점과 다른 쪽의 초평면이 어디에 있는지 결정하는 데 활용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n임의의 점 X를 고려해 보면:\n\n- 만약 X⋅W ` c 이면 — 이것은 양성 샘플입니다.\n- 만약 X⋅W ` c 이면 — 이것은 음성 샘플입니다.\n- 만약 X⋅W = c 이면 — 이것은 결정 경계 상에 있습니다.\n\n쉽죠?\n\n그러니까 조금 되감아보고 이 방정식들이 어디에서 왔는지 이해해 봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## #1. 하이퍼플레인을 찾는 방법 결정\n\n우리가 “분리선”을 얻기 위해, 서포트 벡터와 하이퍼플레인 사이의 거리 d를 먼저 계산할 수 있습니다. 여유 공간은 하이퍼플레인으로부터 가장 가까운 서포트 벡터까지의 거리의 두 배이며, 이 여유 공간 내에는 어떤 점도 있어서는 안 됩니다.\n\n## #2. 거리 “d” 투영\n\n거리 d는 두 서포트 벡터 사이의 차이를 하이퍼플레인의 법선 벡터 w의 방향으로 투영하면 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_3.png)\n\n여러분 중 많은 분들이 여기에 도착한 방법을 모르실 것 같아요. 그래서 한 발 물러나서 이 함수가 처음부터 무엇을 의미하는지 더 잘 이해해보도록 해요.\n\nA와 B라는 두 벡터가 있다고 상상해봅시다. 그들 사이에 θ도를 생성해요. 이 스칼라 곱을 사용하여 A가 B 위에 떨어지는 투영을 쉽게 찾을 수 있어요.\n\n즉, A의 B에 대한 투영을 찾을 수 있어요. A와 B 벡터를 알면 다음 수식에서도 확인할 수 있듯이요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서 이제 우리가 이 기본 원리를 이해했으니, SVM 모델로 돌아가 봅시다. SVM에 동일한 수학적 개념을 적용할 수 있습니다. 여기서 A는 지원 벡터 머신으로 정의된 벡터이고 B는 우리가 분할 초평면의 법선 벡터입니다.\n\n## #3. 제약 조건 정의하기\n\n이제 여백을 활용하여 제약 조건을 정의할 수 있습니다. 최대 여백 초평면이 (2D 예제에서) 선 방정식을 따라야 한다는 것을 알고 있습니다.\n\n이것은 초평면에 놓인 것은 양수 값을 가질 것이며(양쪽 초평면에 해당), 그 아래에 있는 것은 음수 값을 가질 것입니다(음쪽 초평면에 해당).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 두 초평면 사이의 간격을 \"마진\"이라고 합니다.\n\n## SVM의 마진\n\n마진은 SVM에서 중요한 개념으로, 초평면 주변에 데이터 포인트가 없는 버퍼 영역으로 작용합니다. 이 마진이 넓을수록 모델이 보이지 않는 데이터에 대해 일반화할 수 있으며, 과적합 가능성을 줄입니다.\n\n양수 또는 음수로 점을 분류하기 위해 초평면과의 상대적 위치를 기반으로 결정 규칙을 설정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 한 쪽에 있는 점들은 한 범주로 분류됩니다 (파란색 🔵)\n- 다른 한 쪽에 있는 점들은 반대 범주에 속합니다 (노란색 🟨).\n\n마진을 최대화함으로써 SVM은 의사결정 경계를 최적으로 배치하여 가능한 높은 신뢰도로 클래스를 분리합니다.\n\n그러면 어떻게 최대화할까요?\n\n# 최적화와 제약 사항\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSVM은 여백을 최대화하기 위한 최적화 문제를 해결하는 것을 포함합니다. 이는 선택한 초평면이 각 클래스의 가장 가까운 데이터 포인트에서 충분한 거리를 유지하도록 하는 것을 의미합니다. 이를 서포트 벡터라고 합니다.\n\n이미 이전에 발견한 선 방정식을 기반으로 한 분류 알고리즘이 있습니다. 그래서 출력을 다음과 같이 정의할 수 있습니다:\n\n- +1 또는 🔵는 양쪽의 데이터를 나타냅니다.\n- -1 또는 🟨는 음쪽의 데이터를 나타냅니다.\n\n![이미지](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 여전히 w 벡터와 b 매개변수를 찾아야 합니다.\n\n그래서... 어떻게 할까요?\n\n마진 경계에 위치하는 서포트 벡터는 우리의 양의 및 음의 초평면 내에 포함되어 있기 때문에 다음 제약 조건을 만족합니다.\n\n그래서 이를 쉽게 일반화할 수 있어요...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 일반 제약 조건 방정식\n\n모든 데이터 포인트 (x, y)가 마진을 넘어가지 않도록 하기 위해, 모든 데이터 포인트에 대한 제약 조건은 다음과 같이 요약될 수 있습니다:\n\n![equation](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_5.png)\n\n그리고 수행할 단계가 하나 더 남았습니다...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 최적화 목표\n\n이제 일반적인 제약 방정식을 가지고 있으므로, 벡터 w의 절대값을 최소화하면서 제약 조건을 충족시킬 수 있습니다.\n\n이는 다음과 같이 수학적으로 정의될 수 있습니다:\n\n![equation](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 최적화 문제를 해결함으로써, 클래스 간의 최상의 분리를 보장하는 최대 마진을 가지는 초평면을 정의하는 벡터 w와 b의 최적값을 찾을 수 있습니다.\n\n# 결론\n\n서포트 벡터 머신은 데이터 과학자의 무기 중 강력한 도구로, 이진 분류에 효과적인 방법을 제공합니다.\n\n클래스 간의 간격을 최대화하는 데 초점을 맞추면, SVM은 새로운 데이터에 대해 잘 일반화되는 견고한 분류기를 생성하여, 오버피팅의 위험을 줄입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSVM의 수학적 기반은 최적 초평면의 식별을 보장하여 다양한 분류 작업에 신뢰할 수 있는 선택지로 만듭니다.\n\n복잡한 데이터셋을 다루거나 모델 성능을 향상시키려는 경우, SVM에 대한 이해와 구현은 머신러닝 도구상자를 크게 향상시킬 수 있습니다.\n\nMLBasics 이슈를 좋아하셨나요? 그렇다면 DataBites 뉴스레터를 구독하여 최신 소식을 받아보세요!\n\n내용을 메일로 받아보실 수 있습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_7.png](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_7.png)\n\nX, Threads, LinkedIn에서도 만나볼 수 있어요! 거기서는 머신러닝, SQL, Python, 데이터 시각화에 관한 일일 치트시트를 올려요.\n\n다른 멋진 글도 여기 한번 확인해보세요! 😄","ogImage":{"url":"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png"},"coverImage":"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png","tag":["Tech"],"readingTime":6},{"title":"약물 발견의 출발점으로서 방대한 화학 공간을 구조 기반 가상 스크리닝하기 위한 방법","description":"","date":"2024-06-23 19:33","slug":"2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery","content":"\n\n\n![Structure-based virtual screening](/assets/img/2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery_0.png)\n\n가상 스크리닝은 유형의 분자 제약을 넘어서 널리 합성 가능한 화합물 억양 개 중에서 선도물을 찾을 수 있게 합니다. 현재 약물 설계를 위한 여러 방법들이 있지만, 아직 완벽한 방법은 없기 때문에 새로운 접근법의 개발은 새로운 목표와 도전이 이 분야에 오는 한 계속될 것입니다.\n여기서는 구조 기반 가상 스크리닝 접근법에 집중하고 있으며, 널리 사용되고 있는 새로운 기계 학습 기술과 시너지를 이루고 있는 것을 성공적으로 소개하고 있습니다.\n\n# 거대 라이브러리 도킹\n\n거대 라이브러리 도킹 방법은 여기서 논의된 다른 방법 중 약물 개발에 가장 오랫동안 사용된 방법 중 하나입니다. 높은 활성 물질 수가 이 방법으로 발견되어 왔으며, 서브나노 몰 활동까지 초점이 맞춰졌습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 초대형 라이브러리 도킹에 필요한 두 가지 필수 입력은 대상 구조와 스크리닝 데이터범입니다.\n\n단백질의 고해상도 X-선 및 cryoEM 구조 및 실험적인 것과 유사한 인공지능으로 얻은 구조들이 현재 사용 가능합니다. 여기서 가장 중요한 것 중 하나는 모델 성능을 벤치마킹하고 최적화하여 연구된 리간드와 단백질 간의 주요 상호작용을 복제하는 것이며, 이는 원래의 결정학 모델에서 발생하는 상호작용과 유사합니다.\n\n구조 기반 가상 스크리닝에서 이제 가장 자주 사용되는 데이터범 유형은 온디맨드 생성 라이브러리와 상용 인실리코 빌딩 블록에 기반한 집중 데이터범입니다. 첫 번째 데이터범은 빠른 아날로그 액세스를 제공하여 구조-활성-관계를 얻고 히트-투-리드 생성을 가속화할 수 있습니다. 그들의 단점은 특정 화합물종을 함유한 분자의 제한된 가용성입니다. 집중 데이터범은 원하는 프레임을 함유하는 대규모 라이브러리를 생성함으로써 이 제한을 해소합니다.\n\n도킹 모델이 교정된 후 분자 라이브러리를 가상으로 스크리닝할 수 있습니다. 분자들과 단백질 결합부위의 상호작용은 점수 함수를 사용하여 계산됩니다. 얻은 예측은 필터링되고 클러스터링됩니다. 이러한 필터는 주요 도킹 함수에서 놓친 문제 있는 특징을 포착하고, 알려진 리간드와의 상이함을 보장하며, 우선순위가 있는 화합물들 사이에서 다양성을 촉진할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, 획득한 리간드-생물학적 타겟 복합체를 시각적으로 검사합니다. 표적 단백질 결합 부위와 주요 상호작용을 형성하는 분자들은 후속 실험적 검증을 위해 선택됩니다.\n\n거대 라이브러리 도킹의 주요 문제는 최상위 등수 분자들 사이에 거짓 양성 결과가 누적된다는 점입니다. 상호작용 지문, 보다 정확한 리간드 변형 설명, 및 수용체 유연성은 이 상황을 극복하는 데 도움이 될 수 있습니다. 또한, 분자 도킹과 분자 역학 자유 에너지 계산의 조합과 같은 보다 엄격한 계산 방법을 사용하여 거대 화면에서 상위 순위 분자들의 작은 집합을 다시 점수 매기는 것으로 이 문제를 해결할 수 있습니다.\n\n지금은 딥러닝 기반 도킹 방법이 활발히 개발되어 거대 라이브러리 스크리닝의 속도와 정확도를 향상시키고 있지만, 유사하지 않은 타겟에 일반화하고 물리적으로 유효한 리간드 구조를 생성하는 모델의 능력에 대한 우려가 여전히 있습니다.\n\n# 기계 학습 가속 가상 스크리닝\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가상 스크리닝 파이프라인에 머신 러닝 방법을 통합하면 가장 큰 사용 가능한 온디맨드 라이브러리를 효율적으로 탐색할 수 있습니다. 이 접근 방식은 100배까지 다양한 화합물을 축소하고 동시에 최상위 점수화된 분자를 풍부하게 만들기로 입증되었습니다.\n\n머신 러닝 가속 가상 스크리닝 파이프라인의 본질적인 부분은 다음과 같은 순차적 단계에 따라 진행됩니다. 먼저, 관심 대상과의 관련 도킹 점수에 기반하여 온디맨드 라이브러리 샘플의 해당 훈련 세트가 생성됩니다. 그런 다음, 머신 러닝 모델을 훈련시키고 나머지 라이브러리의 점수를 예측하는 데 사용됩니다. 유망한 분자들은 머신 러닝 모델의 더 깊은 개선(액티브 러닝) 또는 실험적 평가에 사용됩니다. 원하는 결과가 달성될 때까지 이러한 작업들은 반복될 수 있습니다.\n\n머신 러닝 가속 가상 스크리닝의 주요 단점이 몇 가지 있습니다. 첫째로, 더 정교한 알고리즘으로 인해 상당한 계산 비용이 필요하며 도킹 점수 기능의 정확도가 낮아 실제 잠재성을 충분히 발휘하지 못할 수 있습니다. 둘째로, 사용 가능한 데이터셋이 특정 유형의 화합물이나 대상으로 편향될 수 있어 새로운, 보지 못한 화합물에 대한 모델의 일반화 능력이 제한됩니다. 셋째로, 일부 최상위 점수 분자가 생물학적 실험에서 무효일 가능성이 있지만, 이들이 훈련 세트에 포함된다면 머신 러닝 모델이 가상 스크린의 높은 거짓 양성률을 전파하거나 심지어 증가시킬 수 있습니다.\n\n**# 프래그먼트 기반 가상 스크리닝**\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n분자 단편의 내재적으로 낮은 분자 복잡성은 그들이 대상 단백질 결합 부위를 보완하고 vitro에서 테스트될 가능성을 높입니다. 그러나 대형 분자와는 달리 단편은 높은 효능이나 선택성을 갖지 않으며, 추후 광범위한 화학적 발전이 필요하여 선도 후보를 얻기 위해. 새로운 단편 기반 가상 선별 방법으로 이 과정을 최적화하는 것이 가능해졌습니다.\nV-SYNTHES 방법은 대규모 화합물 모델링 도전을 작은 단계로 분해하여 현재 명시적 도킹으로 달성할 수 있는 것보다 몇 차원 더 큰 라이브러리 탐색으로 탐색합니다. 이러한 방법은 대형 라이브러리 도킹의 주요 한계인 화학 공간 크기에 선형적으로 비례하는 컴퓨팅 리소스의 상당한 증가를 다룹니다.\nV-SYNTHES에는 라이브러리 준비, 열거, 도킹 및 히트 선택의 반복적 단계가 포함됩니다. 준비 과정에서 단편 유사 화합물 라이브러리가 생성되며 (\"최소 열거 라이브러리\"(MEL)), 사용된 화학 라이브러리 공간 전체에 대한 모든 반응의 골조-신톤 조합의 모든 가능성을 대표합니다. 다음 단계에서, MEL 화합물은 대상 수용체에 도킹됩니다. 최상의 점수를 받은 신톤이 선택되고 해당 골조에 초점을 맞춘 화학 라이브러리는 반복적인 열거와 도킹이 수행됩니다. 반복 횟수는 화학 공간에서의 전체 화합물을 나타내는 분자가 완성될 때까지 되풀이됩니다. 마지막으로 라이브러리의 최종 열거된 하위 집합에 도킹 스크린을 수행하고 최우선 히트는 방해 화합물, 물리-화학적 특성, 약물 유사성, 독창성 및 화학 다양성에 대한 후처리 필터링을 거칩니다. 그 결과 화합물 세트가 합성되어 실험적으로 테스트됩니다.\n최근 제안된 기계 학습 방법인 FRAME은 구조 안내 방식을 사용하여 단편의 점진적 발전을 기반으로 합니다. 여기서 딥 러닝은 초기 단편에서 결합된 단백질 구조에 기초하여 적합한 성장 벡터를 식별하고 SE(3)-등변 신경망 활용을 통해 다양한 화학적 발전을 평가합니다. FRAME의 주요 단점은 제한된 합성 접근성으로 이어질 수 있다는 것입니다.\n\n저희 Chemspace는 Enamine REAL Space에 접근하여 V-SYNTHES를 사용하여 속도와 효과면에서 가장 효과적인 구조 기반 가상 리간드 스크리닝 방법을 사용합니다. 저희 전문가들이 최단 시간 내에 당신이 원하는 대상에 대한 최상의 히트 화합물을 기가 스케일 라이브러리에서 검색하는 데 도와드릴 것입니다. 본 서비스에 대해 자세히 읽을 수 있습니다.\n\n본 검토는 다음 논문을 바탕으로 작성되었습니다: J. Carlsson et al., Curr. Opin. Struct. Biol. 2024; https://doi.org/10.1016/j.sbi.2024.102829","ogImage":{"url":"/assets/img/2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery_0.png"},"coverImage":"/assets/img/2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery_0.png","tag":["Tech"],"readingTime":4},{"title":"BERT 미세 조정으로 텍스트 분류하는 방법","description":"","date":"2024-06-23 19:32","slug":"2024-06-23-FinetuneBERTfortextclassification","content":"\n\n섬세 조정은 대형 언어 모델이 사용자 지정 데이터에 적응하고 텍스트 분류와 같은 하향 작업을 잘 수행할 수 있도록 돕는 중요한 기술입니다.\n\n본 문서는 섬세 조정의 기본에 초점을 맞추고, LORA, QLORA 등 다른 기술에 대해 깊게 다루지는 않습니다. 시작하는 가장 좋은 방법은 BERT로 실험을 해보는 것입니다.\n\n주로 두 가지 방법으로 이 작업을 수행할 수 있습니다:\n\n- 허깅페이스 트레이너 API 사용: 사용하기 쉽지만 매우 사용자 정의가 어려움\n- PyTorch 사용: 트레이너보다 조금 어려우나 프로세스에 대한 더 많은 사용자 정의와 제어를 제공합니다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터셋\n\n우리는 Hugging Face에서 제공하는 Yelp Reviews 데이터셋을 사용할 예정입니다. 이 데이터셋은 다음 두 열로 구성되어 있습니다:\n\n- 레이블: 1부터 5까지의 별표가 부여된 등급입니다.\n- 텍스트: 리뷰 내용입니다.\n\n저희의 목표는 리뷰 텍스트로부터 별의 개수를 예측할 수 있는 모델을 훈련하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n휍핑페이스 트레이너 API를 사용하여 파인튜닝하기\n\n- 모든 라이브러리를 설치하세요 :\n\n```js\n!pip install --upgrade transformers datasets evaluate huggingface_hub torch\n```\n\n참고: 이 라이브러리들의 최신 버전을 항상 사용하도록 하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. 데이터셋 로드: Hugging Face에서 제공하는 datasets 라이브러리를 사용하여 데이터셋을 로드할 수 있어요.\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"yelp_review_full\")\n```\n\n데이터셋을 확인해봐요. 어떤 데이터를 다루게 될지 알아봅시다.\n\n```python\ndataset[\"train\"][1]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리 데이터가 어떻게 보이는지 확인해보세요.\n\n```js\n{'label': 1,\n 'text': \"안타깝게도 Dr. 골트버그의 환자로서 느끼는 좌절은 뉴욕의 다른 많은 의사들과 겪어온 경험의 반복입니다 - 좋은 의사, 하지만 최악의 스태프. 그의 스텝은 단순히 전화를 받지 않는 것 같습니다. 답변을 받으려면 보통 반복적인 전화로 2시간이 걸립니다. 누가 그런 시간을 가진 사람이며 누가 그것과 소통하길 원하겠습니까? 다른 많은 의사들과도 이 문제를 겪어왔고, 이해가 안 가네요. 사무원이 있고 의료 필요가 있는 환자가 있는데, 왜 전화를 받는 사람이 없는 건지요? 이해할 수 없고, 신경질만 나게 합니다. Dr. 골트버그에게 2점을 주어야 하는 점이 유감입니다.\"}\n```\n\n3. 토크나이저를 로드하고 텍스트를 토큰화하는 함수를 만들어보세요:\n\n```js\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n토크나이저는 텍스트를 입력_ids, 토큰_유형_ids 및 어텐션_마스크로 이해할 수 있는 세 개의 열로 변환합니다.\n\n데이터셋에서 작은 배치를 만들기(선택 사항)\n\n```js\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n```\n\n4. 모델 불러오기:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n```\n\n분류할 레이블 수를 초기화하려면 num_labels 매개변수를 사용하세요.\n\n5. 훈련 인수 초기화\n\n```python\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nhttps://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments에서 제공되는 매개변수에 대한 자세한 정보를 확인할 수 있습니다.\n\n6. 메트릭 계산 함수 설정:\n\n```js\nimport numpy as np\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n```\n\n7. 학습 시작:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom transformers import Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()\n```\n\n트레이닝을 시작하려면 wandb 키를 입력하라는 프롬프트가 나타납니다. 키를 입력하면 트레이닝 프로세스가 시작됩니다. 트레이닝이 완료되면 아래와 같은 결과를 보게 될 것입니다.\n\n![트레이닝 결과](/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png)\n\n선택적으로 노트북에서 허깅페이스로 모델을 저장할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\nfrom huggingface_hub import login\nlogin()\nmodel.push_to_hub(\"HuggingfaceUsername/yourModelName\")\n```\n\n8. 추론 실행:\n\n모델을 테스트하려면 PyTorch를 사용할 수 있습니다.\n\n```js\nimport torch\nimport torch.nn.functional as F\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"HuggingfaceUsername/yourModelName\")\ns=\"The was awesome and I loved it\"\ntt=tokenizer(s,return_tensors=\"pt\", padding=True, truncation=True)\n```\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델을 평가 모드로 설정하면 더 이상 가중치를 업데이트할 필요가 없어지고, 이제 분류 작업에 사용할 수 있습니다.\n\n```python\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(**tt)\n```\n\n결과를 확인해보겠습니다.\n\n```python\nSequenceClassifierOutput(loss=None, logits=tensor([[-2.3995, -2.0111, -0.8381,  2.4683,  2.8968]]), hidden_states=None, attentions=None)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 중요한 변수는 로짓 변수입니다. 이 경우 로짓은 텍스트가 특정 클래스에 속할 확률을 나타냅니다. 현재 로짓은 이해하기 어려운 형식으로 표시됩니다. 이를 이해할 수 있는 형식으로 변환하려면 이해할 수 있는 숫자로 변환해야 합니다.\n\n```js\nlogits = outputs.logits\nprint(\"로짓:\", logits)\n\n# 소프트맥스를 사용하여 로짓을 확률로 변환합니다\nprobabilities = F.softmax(logits, dim=-1)\nprint(\"확률:\", probabilities)\n\n# 예측된 클래스를 결정합니다\npredicted_class = torch.argmax(probabilities, dim=-1)\nprint(\"예측된 클래스:\", predicted_class.item())\n```\n\n여기서 출력은 4입니다.\n\nPyTorch를 사용한 파인 튜닝\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델이 이해할 수 있도록 몇 가지 전처리 단계가 필요합니다.\n\n- 열 삭제\n\n```js\ntokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\ntokenized_datasets.set_format(\"torch\")\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n```\n\n2. 데이터로더(Dataloader) 생성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport torch\nfrom torch.utils.data import DataLoader\ntraindataloader=DataLoader(small_train_dataset,batch_size=8,shuffle=True)\ntestdataloader=DataLoader(small_eval_dataset,batch_size=8)\n```\n\n3. 모델을 다운로드하고 GPU에 로드해주세요.\n\n```js\nfrom transformers import AutoModelForSequenceClassification\nmodel=AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n```\n\n4. 옵티마이저(optimizer)와 학습률 스케줄러(learning rate scheduler)를 생성하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom torch.optim import AdamW, SGD\nfrom transformers import get_scheduler\noptimizer = SGD(model.parameters(), lr=5e-5)\nnum_epochs = 3\nnum_training_steps = num_epochs * len(traindataloader)\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n```\n\n원하는 옵티마이저와 학습률 스케줄러를 조정하여 가장 적합한 것을 선택할 수 있어요.\n\n5. 학습 및 평가\n\n모델을 model.train()을 사용하여 학습 모드로 설정해주세요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in traindataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n```\n\n```js\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\nmodel.eval()\nfor batch in testdataloader:\n    b = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**b)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n\nmetric.compute()\n```\n\n제 Kaggle 노트북에서 스크립트를 확인할 수 있습니다. https://www.kaggle.com/code/exterminator11/finetune-bert. 행운을 빕니다!","ogImage":{"url":"/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png"},"coverImage":"/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png","tag":["Tech"],"readingTime":8},{"title":"자율 에이전트의 흥망성쇠 그 발전과 한계 ","description":"","date":"2024-06-23 19:30","slug":"2024-06-23-TheRiseandFallofAutonomousAgents","content":"\n\n2023년 ChatGPT가 인기를 끌자, 창조적 AI 공간에서 골드 러시 분위기가 등장했습니다. 전 세계적으로 사람들은 미래에 대한 AI의 변혁적 잠재력을 인식했습니다. 이 골드 러시적 마인드셋은 우리를 핵심 질문으로 이끕니다: 여기에는 골드가 어디에 있을까요?\n\n다른 말로 하면, 미래에는 어떤 일이 일어날까요? 보통 우리에게는 미래로 멀리 전망하는 것이 어려우며, 다가오는 변화를 예측하기 위해 짧은 시간대에 초점을 맞추는 것이 더 좋습니다. 그러나 창조적 AI 분야에서는 이것이 다르다고 보이죠. 우리가 향하는 방향을 알 수는 있겠지만, 다음에 무엇이 올지 예측하는 것은 널리 어렵습니다.\n\n2023년 4월, AutoGPT와 BabyAGI와 같은 자율 에이전트 워크플로우가 GitHub에서 인기를 얻기 시작했습니다. 인기가 폭증하며, AutoGPT는 딱 한 달 만에 5만 명 이상의 프로그래머들의 주목을 끌었습니다.\n\n![이미지](/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 자율 에이전트 워크플로 이해\n\n자율 에이전트 워크플로의 구체적인 내용을 알아보기 전에 먼저 '에이전트'가 무엇을 의미하는지 명확히 해보겠습니다. '에이전트'라는 용어는 컴퓨터 과학의 맥락에서 20세기로 거슬러 올라가는 뿌리를 가지며, 1980년대 후반에 큰 인기를 얻었습니다.\n\n일반적으로, 에이전트는 \"행동\"이 가능한 개체입니다 (‘agency' 개념에서 파생됨). 에이전트는 다음과 같은 세 가지 주요 기능으로 정의될 수 있습니다:\n\n- 지각: 센서 또는 텍스트 입력을 통해.\n- 결정: 인식에 기반한 결정을 내림.\n- 행동: 그 결정에 기반한 행동을 실행함.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2023년 초에 발표된 논문 'ReAct: 언어 모델에서의 추론과 실행의 시너지 효과'는 특히 대규모 언어 모델의 맥락에서 마일스톤을 달성했습니다. 인기 있는 프레임워크인 LangChain은 ReAct 로직을 구현하여 자료와 관련된 테마에 상당 부분의 리포지토리를 할애했습니다. 이 접근 방식은 에이전트 논리를 보다 넓은 관중에게 접근 가능하게 만들면서 에이전트 논리의 최신 발전을 통합했습니다.\n\n에이전트를 뛰어넘게 하는 것은 에이전트에게 일련의 도구를 제공하여 대규모 언어 모델의 훈련 데이터의 한계를 극복하는 능력입니다. 이러한 도구는 본질적으로 소프트웨어 함수로, 에이전트가 API 요청(예: Google 검색 쿼리 실행), 웹 사이트 읽기, 프로젝트 보드에 액세스, 계산 수행, 데이터베이스에서 SQL 쿼리 실행 또는 보호된 환경에서 코드 작성 및 실행과 같은 작업을 수행할 수 있게 합니다. 도구는 대규모 모델이 외부 세계와 상호작용할 수 있게 합니다. 모델은 원하는 결과를 달성하기 위해 실행할 함수를 결정할 수 있습니다.\n\nReAct 로직은 각 언어 모델 프로세스 단계에서 'Thought', 'Act/Action' 및 'Observation' 요소를 포함합니다. 'Thought'는 추론을 통해 다음 작업과 그 근간에 있는 결정을 향상시킵니다. 'Action'은 어떤 도구를 사용할지와 어떤 매개변수를 사용할지를 지정하며, 'Observation'은 'Action'에 따라 도구 실행 결과를 포함합니다. 초기 조사에 대한 답변을 위해 이러한 단계들이 필요한 정보가 수집될 때까지 반복됩니다.\n\n![이미지](/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# OpenAI에서 구현됨\n\nOpenAI의 중요한 역할은 에이전트의 잠재력에 대한 인식으로 두드러지며, 이를 기능으로 통합하여 ChatGPT 플러그인 스토어를 열어두었습니다. 또한, GPT-XX-0631 이후의 OpenAI 모델 업데이트는 JSON 출력을 더 잘 제공할 수 있도록 지속적으로 개선되었습니다. 이 향상은 도구, 플러그인 또는 기능을 신뢰할 수 있게 실행하는 데 주로 필수적입니다. 그 이후 API는 또한 GPT 모델 쿼리에서 반환된 텍스트 블록과 별도로 함수의 정의 및 실행을 허용합니다. 일반적으로 JSON 출력은 함수 실행에 필요한 매개변수를 정의하므로, 전통적인 오픈 소스 프로젝트의 경우 (MistralAI와 같이 API 클라이언트에서 함수 호출을 제공하는 몇 가지 예외를 제외하고) 출력에서 추출해야 합니다.\n\n# 자율 에이전트 워크플로우\n\n자율 에이전트로 돌아와서, 클래식 ReAct 에이전트가 특정 작업이나 질문을 다루는 데 설계되었다면, 자율 에이전트 워크플로우는 한 걸음 더 나아갑니다. 작은 작업이 아니라 넓은 목표가 초기 입력으로 표현됩니다. 일반적으로 단일 프로세스 단계는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- (LLM) - 작업/목표를 하위 작업으로 분해합니다.\n- 작업을 작업 풀에 추가합니다.\n- (LLM) - 작업 풀을 우선 순위로 나열합니다.\n- 가장 중요한 작업을 선택합니다.\n- (LLM-Agent) - 가장 중요한 작업을 완료합니다.\n- (LLM-Agent) - 다음 작업을 정의합니다.\n\n다시 말해, '에이전트'들은 이제 '프로젝트 관리 에이전트'에 의해 조정되며 큰 작업을 목표를 달성할 때까지 처리 가능한 작은 작업으로 나누어줍니다. 이 프로세스는 새로운 도구를 만들고, 새로운 에이전트를 조정하고, 정보를 단기 또는 장기 기억에 저장하는 것을 포함합니다. 이 이터레이션 프로세스는 이론적으로 거의 모든 작업을 해결할 수 있습니다. 사용자들은 대형 언어 모델 (GPT-4)에 의해 생성된 창의적인 솔루션을 보고했습니다. 워크플로는 웹사이트나 프로필을 생성할 때 캡차를 우회하기 위해 작업에 막혔는데, '인간'을 위해 작업 플랫폼 (Fiverr)에 작업을 게시하여 문제를 해결했습니다.\n\n# 다중 에이전트 협업 워크플로\n\n우리는 독립적인 에이전트 워크플로가 개별 에이전트의 능력을 활용하여 특정 기능을 수행하는 능력을 증진함으로써 복잡한 작업을 해결할 수 있는 능력을 크게 향상시켰다는 것을 보았습니다. 그러나 이러한 기술의 진정한 잠재력은 독립적인 에이전트보다는 다중 에이전트의 협력 노력에 더 명백해집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다중 에이전트 협업은 특화된 역량을 갖춘 각종 에이전트들이 협력하여 단일 에이전트 시스템으로는 해결할 수 없는 복잡한 목표를 달성하는 것을 포함합니다. 이 방식은 여러 에이전트의 집단적 강점을 활용하여 도전적인 문제에 대한 더 정교하고 확장 가능하며 유연한 해결책을 제공합니다. 성공적인 다중 에이전트 협업의 핵심은 다양한 에이전트의 행동을 조율하고 공통 목표를 향한 효과적인 의사 소통과 협력을 보장하는 능력에 있습니다.\n\nAutoGen은 대규모 언어 모델(LLMs)을 활용하는 워크플로의 조율, 최적화 및 자동화를 촉진하는 Microsoft의 프레임워크입니다. AutoGen의 핵심 아이디어는 특화된 역할과 기능을 갖춘 각 에이전트들이 더 효과적으로 협력할 수 있는 시스템 설계를 용이하게 하는 것입니다. 이러한 에이전트들은 자율적으로 작업을 수행하거나 서로 간 대화를 나누며, 프로그래밍 및 사람 사용자 또는 다른 도구로부터의 입력을 바탕으로 결정을 내릴 수 있습니다. 이는 단일 에이전트나 순수히 인간 팀이 다루기 어려운 복잡한 문제를 해결하는 더 동적이고 유연한 방식을 가능케 합니다.\n\nAutoGen은 두 가지 주요 기능을 통해 이를 실현합니다:\n\n- 개발자가 특화된 기능과 역할을 갖춘 에이전트들의 집합을 정의함으로써, 이러한 에이전트들의 모듈화와 재사용성을 강화합니다.\n- 에이전트 간 상호작용 행동을 위한 프레임워크를 수립하여, 미리 정의된 프로토콜 또는 자동화된 채팅을 통해 효과적으로 소통하고 협력할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_2.png)\n\n# 도전과 제한 사항\n\n클래식 ReAct 에이전트와 비슷한 원칙으로 디자인된 에이전트들은 능력에 제한이 있습니다. 이 제한은 토큰 창문 제약 때문에 발생하는데, 여기서는 어떻게 그리고 언제 사용해야 하는지에 대한 설명이 정의될 수 있는 기능의 수가 제한됩니다. 단일 작업 또는 도구로 구비된 에이전트는 만족스러운 결과를 산출할 수 있지만, 도구의 수가 증가함에 따라 신뢰성이 감소합니다.\n\n토큰 창문의 확장이 있더라도, \"중간\" 문제가 지속됩니다. 즉, 입력 프롬프트의 중간에 위치한 정보가 덜 주목을 받습니다. 따라서 16,000 토큰을 초과하는 문맥 창문 내에 수백 개의 도구를 수용할 수 있는 잠재력이 있더라도 대부분은 여전히 간과됩니다. 게다가, 의사 결정은 여전히 과정에서 중요한 도전 과제입니다. 제한된 도구 수가 있더라도, MultiActionAgents는 적합한 도구를 선택하거나 모든 가능한 도구를 활용해야 한다고 믿는 것에 어려움을 겪습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 자율 에이전트의 과제\n\n자율 에이전트 개발이 미진한 이유 중 하나는 그들이 만들어내는 오픈 루프 시스템에 따른 비용 때문입니다. 보통 이러한 워크플로우는 가장 효율적인 해결책으로 이끌어주지 않습니다. 대신 계속하여 새로운 작업을 정의합니다. 오케스트레이션 에이전트들은 작업 풀을 모니터링하는 데 어려움을 겪어야 합니다. 한 번 이상으로 정의된 작업이 없도록 보장하는 것이 중요합니다. 동시에 최선의 결과를 얻으려면 최신 모델인 GPT-4를 사용해야 하며, 간단한 목표조차도 수천 번의 LLM 호출을 유발할 수 있어 빠르게 비용이 증가할 수 있습니다.\n\n뿐만 아니라, 시스템은 종종 막다른 곳에 이르게 되어 폐쇄 루프를 만들어냅니다:\n\n- 작업 A는 작업 B와 C에 의해 완료되어야 하며,\n- 그리고 작업 C의 분할 결과로써 작업 A가 다시 정의되는 일이 발생합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델 업데이트의 관찰 가능한 저하와 게으름으로 문제가 악화되고 있습니다. 저는 연구를 수행하고 발견물로부터 PowerPoint 프레젠테이션을 작성할 수 있는 반자동 워크플로우를 개발했었는데, GPT-3.5 turbo로 시도 중 3번 중 2번은 성공했습니다. 하지만 GPT-4에서는 이러한 결과를 재현할 수 없었습니다.\n\nLangChain의 미리 정의된 agent + 도구킷에서도 비슷한 상황이 나타납니다. 예를 들어, Pandas 및 SQL agent는 2023년 중반까지 신뢰할 수 있는 결과를 제공했지만 이제 80%의 경우에 오류로 이어지고 있습니다. 이는 agent의 인기가 점차 감소하거나 적어도 침체로 이어지고 있다는 것을 보여줍니다.\n\n# 해결 방안\n\n덜 발전된 모델일지라도 실행 로직과 프롬프트 엔지니어링을 통해 개선의 여지가 있습니다. 한 가지 전략은 MultiActionAgent를 사용하는 대신 agent가 수행할 수 있는 작업 범위를 좁히는 것입니다. 각 도구에 대해 SingleActionAgent가 생성되고, 적합한 SingleActionAgent를 선택하는 Routing Agent가 지정됩니다. 이 방법은 GPT-3.5 turbo와 같이 강력하지 않은 모델에도 만족스러운 결과를 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 이 방법은 이전의 종단 간 에이전트 전략과 비교해 개발 노력이 상당히 많이 필요합니다. 더 비용 효율적인 모델을 활용하도록 솔루션을 분해하면 재정적으로 한 번에 더 많은 LLM 호출을 할 수 있습니다. 이를 통해 주요 투표의 실행이나 자가 비평 방법을 사용하여 하나의 LLM 호출에 대한 고려 사항의 반복적인 유효성 검증이 가능해집니다.\n\n프로세스를 분할하는 것이 가장 합리적인 해결책 중 하나로 보입니다. 현재 에이전트 워크플로우는 한 프롬프트 접근 방식에 의존하며, 결정, 문제 해결 및 추가 프로세스는 반복적으로 해결되고 동일한 프롬프트로 이루어지도록 의도되어 있습니다. 프롬프트 엔지니어링에 익숙한 사람들은 특정 상황에는 특정 프롬프트가 필요하다는 것을 알고 있습니다.\n\n# 전망\n\n현재 직면한 문제들이 시간이 지남에 따라 완화될 것입니다. 더 강력한 모델이 시장에 등장하고 오늘날의 최고 모델들이 더 저렴해질 것입니다. 시장이 많은 비판하는 점진적인 악화를 수정할 수 있는 능력을 가지고 있다고 확신합니다. 필요한 곳에는 해결책이 따를 것입니다. 의심의 여지 없이, 생성모델의 다음 성취는 우리의 능력을 혁신할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 사용성이 매우 낮고 신뢰할 수 없지만, 미래는 모든 종류의 프로세스에서 활용될 AI 엔티티(에이전트)에 의해 형성될 것입니다. 컴퓨터 과학 분야의 최근 논문은 에이전트 프로세스(시행착오 학습)에 보상 학습 모델을 통합하는 해결책을 탐구하고 있습니다. 이는 에이전트가 보상 모델을 정의하고 훈련시킨 다음, 극도로 복잡한 작업을 해결하는 도구로 사용할 수 있다는 것을 의미합니다.\n\n에이전트와 자율 에이전트는 의심의 여지 없이 인공 일반 지능(AGI) 달성을 향한 여정에서 중요한 역할을 할 것입니다.\n\n## 출처:\n\n- Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., \u0026 Zhou, E. (2023). The Rise and Potential of Large Language Model Based Agents: A Survey. arXiv:2309.07864에서 검색됨\n- Yao, S., et al. (2022). REACT: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629에서 검색됨\n- LangChain. (n.d.). 에이전트 유형: REACT. LangChain 문서에서 확인됨\n- Significant-Gravitas. (n.d.). AutoGPT. GitHub에서 확인됨\n- Microsoft Research. (n.d.). AutoGen: 다음 세대 대형 언어 모델 애플리케이션 활성화. Microsoft Research 블로그에서 확인됨\n- Microsoft Research. (n.d.). AutoGen 프로젝트 페이지. Microsoft Research 프로젝트에서 확인됨","ogImage":{"url":"/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_0.png"},"coverImage":"/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_0.png","tag":["Tech"],"readingTime":8},{"title":"대형 언어 모델LLM로 민감한 데이터 처리하는 방법 총정리","description":"","date":"2024-06-23 19:28","slug":"2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels","content":"\n\n## 목차\n\n소개\n\n데이터 민감도는 무엇을 정의하고 있으며 누가 정의하고 있나요?\n데이터 익명화와 익명변환은 무엇인가요?\n민감한 데이터를 처리하기 위해 AI를 활용하는 것이 특별한 이유는 무엇인가요?\n\n실습 안내 — LLM 기반 데이터 프로파일러 구현\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로컬 LLM 설정\n1. 도커를 사용하여 모델 서버 설정하기\n2. 프롬프트 빌드하기\nAzure OpenAI 설정\n\n고수준 솔루션 아키텍처\n결론\n참고 자료\n\n예상되는 하루 평균 데이터 생성량은 328.77 백만 테라바이트입니다. 대부분의 데이터는 데이터 주도형 애플리케이션으로 흐르며 매초마다 처리되고 풍부해집니다. 주요 제품들 사이에서 LLM의 채택과 통합이 확대되어 텍스트 데이터 활용의 사용 사례와 이점이 더욱 증가했습니다.\n\n대규모 데이터를 처리하는 조직은 민감한 데이터 처리 요구 사항을 준수하는 데 어려움을 겪습니다. 이는 데이터 보안이나 데이터 법률 및 규정을 준수하는 것과 관련이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n민감한 데이터 침해의 직접적 및 간접적 영향은 특히 민감한 데이터가 관련될 때 기관에 중대한 재정적 결과를 초래할 수 있습니다. 이는 즉각적인 비용 영향을 넘어서 해당 기관의 고객 기반의 신뢰와 충성을 흔들어놓을 수 있습니다.\n\n![Image](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_0.png)\n\n# 무엇이 그리고 누가 데이터의 민감성을 정의할까요?\n\n민감한 데이터는 데이터 보호와 개인 정보 보호 맥락에서 중요한 개념입니다. 높은 수준에서 민감한 데이터는 비밀유지되고 무단 접근으로부터 보호되어야 하는 정보로 이루어져 있습니다. 민감한 데이터의 중요성은 그것을 보호하기 위해 마련된 법률 및 규정들에 의해 강조됩니다. 예를 들어, EU의 일반 데이터 보호 규정(GDPR)과 캘리포니아 소비자 개인 정보 보호 법(CCPA) 등 다양한 국가에서 적용되는 법률 및 규정이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n민감한 데이터에는 다음과 같은 다양한 정보 카테고리가 포함됩니다:\n\n- 개인 식별 가능 정보 (일반 데이터 보호 규정)\n- 보호된 건강 정보 (건강 보험 이동성 및 책임성 법)\n- 교육 정보 (가족 교육 권리 및 개인 정보 보호 법)\n- 기밀 비즈니스 정보\n- 금융 정보\n- 고용 정보\n- 법적 정부 발급 정보\n- ...\n\n게다가 GDPR은 특히 개인 식별 가능 정보(PII)와 관련하여 민감한 개인 데이터의 특별한 카테고리를 다음과 같이 명시하고 있습니다:\n\n- 인종이나 민족 출신\n- 정치적 견해\n- 종교나 철학적 신념\n- 노동 조합 가입\n- 유전자 정보\n- 생체 인식 정보\n- 건강 정보\n- 성생활이나 성적 취향\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n법적 프레임워크에 따라 민감한 데이터가 무엇인지에 대한 명확한 설명과 경계가 존재하지만, 기관이 운영중인 도메인이나 섹터에 따라 이러한 내용은 더 많은 민감한 기밀 정보를 포함할 수 있습니다. 이로써 기관은 자체 데이터 기밀성 분류를 정의하고 해당 분류를 데이터 민감도 도구에 통합하거나 사용자 정의 솔루션에 통합하는 것으로 나아갈 것입니다.\n\n# 데이터 익명화와 익명도\n\n익명화된 데이터란 특정 개인과 연결되지 않는 데이터를 의미하며, 추가보조 데이터를 통해서도 그러한 연결이 불가능합니다. 완전히 익명화된 데이터는 GDPR과 같은 개인정보 보호 규정의 적용 범위에서 벗어납니다.\n\n추가 정보와 함께 특정 개인에게 속한다고 할 수 있는 데이터는 익명화된 것이라고 합니다. 이는 원본 데이터를 가짜 식별자로 대체하는 것만으로 달성될 수 있으며, 필요한 경우 되돌릴 수도 있습니다. 예를 들어, 암호화는 데이터 익명화의 한 방법입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n개인정보 의사화는 GDPR의 제 4조에 정의되어 있습니다:\n\n개인정보 익명화 및 의사화는 민감한 데이터 식별 후 수행되는 작업입니다. 데이터를 익명화 또는 의사화해야 하는지는 특정 사용 사례 및 미래 어느 시점에서 익명화를 반대로 해야 하는지 여부에 따라 다릅니다.\n\n![Image](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_1.png)\n\nLLM은 데이터 익명화에서 일부 역할을 합니다. 텍스트 데이터에 마스크를 적용할 수 있지만 별도의 복잡성 없이는 모든 요구 사항, 특히 식별 해제와 관련된 요구 사항을 모두 충족시킬 수 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에 제시된 정의는 나중에 이 용어들을 참조할 때 충분한 간략한 개요를 제공합니다. 주제를 더 깊게 파헤치지 않아도 됩니다. 많은 기사들이 이것을 자세히 다루고 있고, 나는 다른 기사 중 하나에서 이를 더 깊이 설명합니다.\n\n# 민감한 데이터를 처리하는 데 인공 지능을 활용하는 것이 무엇이 특별한가요?\n\n민감한 텍스트 데이터는 대용량 텍스트 필드와 문서에 간접적으로 포함될 수 있어 휴리스틱 기술을 사용해서 감지하기 어려울 수 있습니다. 이러한 기술과 방법들은 주로 미리 정의된 규칙과 패턴(예: 명명된 개체 인식)을 의존하므로 그 능력이 제한될 수 있습니다.\n\n대부분의 경우 중요한 민감한 데이터는 문장 안에서 미묘하게 제시되지 않습니다. 메시징 애플리케이션, 고객 지원 서비스, 이메일 등에서 생성된 데이터는 전체 텍스트의 맥락 안에 민감한 데이터가 포함될 수 있습니다. 이러한 복잡한 상황은 데이터 마스커들이 구문 분석된 텍스트를 기억하고 맥락을 이해해야 한다는 것을 요구합니다. 그러면 휴리스틱 기술의 사용이 방해되어 모든 개인정보 보호 요구 사항을 충족시키기에 부적합하다고 판단됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 실습 안내 — LLM-Powered 데이터 식별기 구현\n\n적절한 프롬프트를 제공하면 고급 LLM 모델은 훈련 데이터를 사용하여 문장 내의 민감한 데이터를 식별하고 가리는 데 성공할 수 있습니다. 이 예제에서는 Llama2 Model과 Azure의 GPT4 Model을 사용하여 데이터 민감도 식별기를 설정하는 방법을 시험해보겠습니다. LangChain 프레임워크를 사용하여 모델을 검색하고 프롬프트를 제공할 것입니다.\n\n우리는 모델을 로드하고 응용 프로그램을 실행하는 데 사용할 환경을 설정하는 것부터 시작하겠습니다.\n\n이를 위해 먼저 Python 3.8 이상이 설치되어 있는지 확인하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Local LLM Setup\n\n이 섹션에서는 데이터 민감도 감지를 위한 로컬 Llama2 모델을 준비하고 실행하는 데 필요한 단계를 나열합니다.\n\n## 1. 도커를 사용하여 모델 서버 설정\n\nLangChain과 통신할 수 있는 모델 서버를 설정하기 위해 도커를 사용할 것입니다. 모델을 로컬로 설정하는 다른 방법도 있으며, 여기에서 더 많은 정보를 찾을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우선 도커가 설치되어 실행 중인지 확인한 후, 아래 명령을 실행하여 이미지를 다운로드하고 모델 서버를 설정하세요.\n\n```js\ndocker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n```\n\n서버가 작동 중인지 확인하려면 브라우저에서 http://localhost:11434 링크를 열면 \"Ollama is running\" 문구가 표시됩니다.\n\n컨테이너가 실행되면 모델을 설치하세요. 다양한 모델을 테스트하고 싶다면 여기서 확인할 수 있는 목록을 확인해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n도커 실행 -it 올라마 올라마 실행 llama2\n```\n\n설치가 완료되면 모델을 로드하고 Python 스크립트를 설정할 것입니다.\n\n먼저 LangChain을 설치해 봅시다.\n\n```js\npip install langchain\npip install langchain-community\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스크립트에서 필요한 패키지를 가져오세요.\n\n```js\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_community.llms import Ollama\n```\n\n모델을 불러오세요.\n\n```js\nllm = Ollama(model=\"llama2\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2. 프롬프트 작성\n\nLLM이 이미 문장의 내장된 맥락을 이해할 수 있는 기능을 갖추고 있다고 생각할 때, 과제가 의도대로 실행되는 것을 보장하는 방식으로 프롬프트를 구성하는 것이 중요합니다.\n\n예를 들어, 다음 사항을 LLM이 보장해야 합니다:\n\n- 문맥을 이해하는 데 키워드 일치만 의존하지 않고 민감한 정보를 식별하는 능력\n- * 데이터 보호 법과 규정 (GDPR, CCPA 등)을 준수하는 감지\n- 모델이 정밀도와 재현율 사이의 균형을 유지\n- 문장 구조가 변경되지 않고 감지된 섹션이 처리되는 것을 보장\n- 추가적인 내용 없이 제공된 데이터만 반환되는 것을 보장\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ntemplate = \"\"\"\n민감한 데이터 식별자 및 마스킹 기능이 있습니다. \n텍스트에서 민감한 정보를 식별하고 \"****\"를 사용하여 마스킹하는 기능이 가능합니다. \n민감한 데이터는 종종 명시적으로 언급되지 않을 수 있으며, 텍스트의 맥락에 내재될 수도 있습니다(예: 건강, 금융, 주소 등의 주제)\n개인 식별 데이터가 감지되고 마스킹되도록 하십시오.\nGDPR, CCPA 및 HIPA와 같은 데이터 보호 법률 및 규정을 고려하도록 하십시오.\n입력 텍스트가 수정되거나 변경되지 않도록 하고 감지된 민감 정보만 마스킹하도록 하십시오.\n마스킹한 정보에 대한 높은 신뢰도를 보장하십시오.\n반환된 내용에는 필요한 마스킹이 적용된 입력 텍스트 이외의 내용이 포함되어서는 안 됩니다.\n민감한 텍스트가 감지되지 않으면 추가 콘텐츠 없이 입력값을 그대로 반환하십시오.\n\n문장:\n{sentence}\n\"\"\"\n\noutput_parser = StrOutputParser()\n\n# 설정 구문\nprompt = PromptTemplate.from_template(template)\n\n# 체인 생성\nchain = prompt | llm | output_parser\n```\n\n요청은 모델이 지시된대로 작업을 성공적으로 실행할 수 있도록 필요한 요구사항을 정의합니다. 국가에 따라 준수해야하는 특정 요구 사항에 대한 추가 매개 변수화가 가능합니다. 다음과 같은 것이 추가될 수 있습니다.\n\n- 어떤 법률을 준수해야 하는지 명시하는 것은 국가에 따라 중요합니다.\n- 민감한 데이터 마스킹 시 운영할 때의 신뢰도\n\n첫 번째 예제를 실행해 봅시다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsentence = \"\"\"\n지난 주, 여름 계획을 논의하는 동안,\n마이크는 바칼리로의 솔로 여행을 드디어 떠날 것을 시사했어요\n그는 보너스를 받은 후에 모아놓은 돈으로요.\n그의 보너스로 1만 달러 이상을 받았어요\n\"\"\"\n\n# 감지 실행\nresponse = chain.invoke({'sentence':sentence})\n\n# 최종 응답 출력\nprint(response)\r\n```\n\n![이미지](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_2.png)\n\nLlama2 모델의 응답은 민감한 데이터를 식별하고 지시에 따라 가리는 능력이 있었습니다. 모든 이름이 가려지고, \"보너스\"와 그 금액도 가렸으므로, 해당 모델이 숫자가 민감한 금융 정보와 관련되어 있는 것을 감지할 수 있었음을 나타냅니다.\n\n참고사항\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n* 데이터 보호 법률과 규정이 정기적으로 업데이트되므로, 모델은 최신 버전에 액세스할 수 없습니다.\n\n** 응용 프로그램은 제공된 법률 및 규정 문서의 특정 버전에서 실행되도록 RAG (검색 증강 생성)을 활용하도록 조정될 수 있습니다.\n\n# Azure OpenAI 설정\n\nAzure OpenAI의 GPT4 모델을 사용하려면 Azure OpenAI 서비스를 생성해야 합니다. Microsoft은 여기에서 리소스를 생성하는 단계에 대해 명확한 설명서를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n리소스를 생성한 후에는 배포로 이동하여 기본 버전을 사용하여 gpt4 모델을 배포합니다.\n\n![image](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_3.png)\n\n모델이 성공적으로 배포되면 API에 액세스하기 위해 사용된 Azure OpenAI 키를 검색하는 것을 잊지 마세요.\n\nAzure OpenAI와 작업하기 위해 필요한 LangChain 패키지를 설치해봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\npip install langchain-openai\n```\n\n로컬 설정을 구축할 때와 같은 단계를 따라가되, Azure OpenAI와 작업하기 위해 import 및 프롬프트 템플릿을 조정해야 합니다.\n\n```js\nimport os\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# 필요한 환경 변수 추가\nos.environ[\"OPENAI_API_VERSION\"] = \u003cAPI VERSION\u003e\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \u003cYour AZURE OPENAI KEY\u003e\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \u003cYour AZURE  OPENAI ENDPOINT\u003e\n\n# 프롬프트 템플릿 정의\ntemplate = \"\"\"\n당신은 민감한 데이터 식별자 및 마스커입니다.\n텍스트에서 민감한 정보를 식별하고 \"****\"를 사용하여 마스킹하는 능력이 있습니다.\n민감한 데이터는 텍스트의 맥락 속에 내장될 수 있으며 항상 명시적으로 언급되지 않을 수도 있습니다(예: 건강, 재정, 주소와 관련된 주제 등).\n개인 식별 가능 데이터가 감지되고 마스킹되었는지 확인하세요.\n데이터 보호법과 규정(GDPR, CCPA, HIPA 등)을 고려해 감지가 이뤄지도록 하세요.\n입력 텍스트가 변경되거나 수정되지 않고 감지된 민감한 정보만 마스킹되도록 하세요.\n마스킹된 민감한 정보에 대해 높은 신뢰도를 보장하세요.\n반환된 콘텐츠에 요구된 마스킹이 적용된 입력 텍스트 이외의 것이 포함되어서는 안 됩니다.\n민감한 텍스트가 감지되지 않은 경우에는 추가 내용 없이 입력을 그대로 반환하세요.\n\n문장:\n{sentence}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\n# 모델 선택\nllm = AzureChatOpenAI(\n    azure_deployment=\"gpt4\",\n)\n\n# 체인 설정\nchain = prompt | llm\n\nsentence = \"\"\"\n지난주 여름 계획을 논의하던 중, \n마이크가 발리로의 단독 여행을 드디어 가기로 하는 듯하다는 신호를 주었어요.\n보너스가 들어와서 이제 막 그 여행을 위해 돈 모았다고 하더라구요.\n그는 1만 달러 이상의 보너스를 받았어요.\n\"\"\"\n\n# LLM 실행\nresponse = chain.invoke({'sentence':sentence})\n\nprint(response.content)\n```\n\n\u003cimg src=\"/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_4.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3번의 테스트를 거친 후에도 출력 결과로 나온 것은 모델이 민감한 정보를 식별하고 필요한 곳에서 이를 가려 주었지만 문장의 내용을 변경하지 않았다는 것을 보여줬어요.\n\n다른 예시를 사용해 봅시다:\n\n```js\nsentence = \"\"\"\n에마와의 통화 중, 그녀가 내년에 월 $3000으로 인상된 임대료 때문에 이사를 가겠다고 가벼운 말투로 언급했어. 그녀가 이 세부 정보를 비공개로 유지한 걸 알았어. 어떤 재정적 걱정 때문일 수도 있어.\n\"\"\"\n```\n\n![AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_5.png 이미지](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 모델 모두 제공된 예시에서 민감한 데이터를 정확하게 식별할 수 있었습니다. 그러나 이 예시에는 상대적으로 짧은 텍스트만 포함되어 있었습니다. 보다 긴 텍스트는 특히 민감한 데이터가 보다 나중에 노출되는 경우에 결과에 더 큰 영향을 줄 수 있습니다. 모델이 숫자와 해당 문맥 간의 연결을 놓칠 수 있기 때문입니다.\n\n다음 섹션에서는 휴리스틱과 LLMs를 모두 활용하는 민감도 감지의 고수준 하이브리드 솔루션 접근 방식을 논의할 것입니다.\n\n# 고수준 솔루션 아키텍처\n\n효율적이고 신뢰할 수 있는 민감한 데이터 식별 LLM 애플리케이션을 개발하는 것은 이미 중요한 이정표입니다. 그러나 이러한 애플리케이션을 데이터 아키텍처에 배치하는 것은 신중히 고려되어야 합니다. 대량 및 속도가 빠른 텍스트 데이터를 보유한 기업은 해결책이 비효율적이 되어 큰 비용과 노력을 필요로 할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다이어그램은 각 단계에서 다양한 구성 요소에 의해 텍스트 데이터가 처리되는 방법을 단계별로 보여줍니다. 이를 통해 완전히 준수된 데이터 상태를 달성할 수 있습니다.\n\n이러한 해결책을 구축할 때 고려해야 할 중요한 질문들:\n\n- 사용 사례가 LLM을 배포하고 유지하는 과부하를 정당화하는가? 휴리스틱 기술이 충분한가?\n- 시장에 사용 사례에 대한 기존 솔루션이 있는가?\n- 데이터의 대기 시간과 가용성 요구 사항을 충족할 것인가?\n- 데이터는 컨텍스트 내에서 중요 데이터를 포함하고 있는가?\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기존 휴리스틱 접근 방식을 활용하면 민감한 데이터를 감지하는 데 도움이 될 수 있지만, 문장의 맥락 속에 간접적으로 나타나는 민감한 데이터를 표준 방법으로 감지하기 어려운 상황에서는 어려움이 있습니다. 큰 양의 텍스트를 이해하는 내재 기능을 갖춘 LLM(Large Language Models)은 민감한 데이터 감지 및 분류 문제에 대처하기 위한 차세대 도구로 기능할 수 있습니다.\n\n본 문서에서는 LLM이 이 문제에 대해 대상화될 수 있는 예시를 보여주었습니다. 예시의 프롬프트는 LLM이 과업의 일반적 요구 사항을 충족할 수 있는 능력이 있음을 입증했습니다. 휴리스틱과 LLM 간의 혼합 접근 방식 도입은 데이터 솔루션 아키텍처에 보여진 것처럼 더 나은 결과를 보장하고 추가적인 안전장치를 제공할 수도 있습니다. 이 문서는 LLM을 사용하여 민감한 데이터를 처리하는 가능성에 대한 일부 조감도를 보여주었고, 일부 추가적인 사용 사례와 가능성은 다음과 같습니다:\n\n- RAG를 위해 데이터 카탈로그 메타데이터 통합\n- 민감도 수준에 대한 분류 도입\n- 도메인별 민감한 데이터 지식 통합\n- …\n\n- 새 이야기를 게시할 때 알림을 받으려면 구독해주세요.\n- LinkedIn에서 언제든지 연락 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 민감한 데이터 처리에 대한 보다 자세한 아키텍처에 관심이 있다면 — 내 다른 기사들을 여기에서 확인해보세요.\n\n# 참고 자료\n\n유럽 의회 및 이사회 2016년 4월 27일 제 2016/679 규정 (일반 데이터 보호 규정) (EEA와 관련된 텍스트)에 관한 자연인의 보호에 대한 데이터 처리 및 그와 같은 데이터의 자유 이동의 처리 및 95/46/EC 지침의 폐지. General Data Protection Regulation (GDPR) — 공식 법적 텍스트 (gdpr-info.eu)에서 확인할 수 있습니다.\n\n데이터 침해 비용 2023 IBM. 다음 위치에서 확인 가능: https://www.ibm.com/security/data-breach (2024년 3월 1일 열람).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n| Source | Title | Date |\n|--------|-------|------|\n| LangChain | [Website](https://www.langchain.com) | - |\n| California Consumer Privacy Act (CCPA) | State of California — Department of Justice — Office of the Attorney General | March 13, 2024. [Link](https://oag.ca.gov/privacy/ccpa) |\n| Taylor, P. | Data Growth Worldwide 2010–2025 | November 16, 2023. Statista [Link](https://www.statista.com/statistics/871513/worldwide-data-created/) |","ogImage":{"url":"/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_0.png"},"coverImage":"/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_0.png","tag":["Tech"],"readingTime":11},{"title":"LangChain, Semantic Kernel, AutoGen 최신 비교 및 분석","description":"","date":"2024-06-23 19:27","slug":"2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore","content":"\n\nJane Huang과 Kirk Li가 씀\n\n이 기사에서는 대형 언어 모델 (LLMs)을 활용한 응용 프로그램을 개발하기 위한 다양한 전략을 비교 분석하며, OpenAI의 Assistant API, LangChain, Semantic Kernel, AutoGen 등과 같은 프레임워크를 아우르고 있습니다. LLMs의 동적인 환경에서는 적절한 프레임워크를 선택하는 것이 이러한 모델을 응용 프로그램에 매끄럽게 통합하기 위해 중요합니다. 다행히 LLM을 백엔드로 하는 시스템을 구축하는 데는 처음부터 시작할 필요가 없습니다.\n\n![이미지](/assets/img/2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore_0.png)\n\nOpenAI의 Assistant API는 응용 프로그램 내에서 AI 어시스턴트를 개발하는 데 도움이 되는 강력한 도구로 부상했습니다. 제공하는 편의성에도 불구하고 일부 유경험 개발자들은 비용과 실제 서비스에서의 관측 가능성 문제에 대해 우려를 표명하며, 잠재적인 단점에 대해 거론했습니다. Assistant API는 개발 노력을 크게 줄이지만, 가격 모델의 장기적 지속 가능성에 대한 불확실성이 남아있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대조적으로 LangChain, Semantic Kernel 및 AutoGen과 같은 대체 프레임워크는 개발자들에게 AI 응용 프로그램에 대한 제어와 유연성을 제공합니다. 이러한 대안들은 각각 특정 선호도와 프로젝트 요구 사항을 고려한 선택지를 제시합니다. 오늘날 사용 가능한 또 다른 주목할 만한 옵션은 SDK를 활용하지 않거나 OpenAI에 Assistant API로 복잡성을 맡기지 않고 작동하는 \"자체 구축\" 솔루션입니다. 이러한 선택지는 유연성뿐만 아니라 필요한 개발 노력 수준에서도 차이를 나타냅니다. 다양한 대안을 제공함으로써, 본 글은 개발자들이 자신의 프로젝트에 대한 독특한 요구 사항과 포부에 부합하는 판단력을 가지도록 돕고자 합니다.\n\n오늘날, 우리의 초점은 LangChain, Semantic Kernel, AutoGen과 같은 프레임워크가 제공하는 옵션에 주변합니다. 이러한 프레임워크는 각각 다른 선호도와 프로젝트 요구 사항을 고려한 것입니다. 이 글의 저자들은 이 글에서 논의된 프레임워크의 어떤 측면에 대해서도 새로운 것을 주장하지 않음을 유의해주십시오. 이 내용들은 링크를 통해 공개된 문서에서 출처를 얻은 것으로, 저자들이 다양한 프레임워크에 대한 학습 및 프로젝트 경험을 요약한 것입니다. 인공 지능 기술의 급격한 발전으로 인해, 본 글이 항상 시간에 따른 최신 발전을 포함하지 못할 수 있다는 점을 인식하는 것이 중요합니다.\n\n일반적으로, LangChain과 Semantic Kernel은 LLMs를 응용 프로그램에 통합하는 공통 목표를 가지고 있지만 접근 방식과 기능에서 차이가 있습니다. LangChain은 메모리와 컨텍스트 창을 명시적으로 구성해야 하지만 Assistant API는 이러한 측면을 자동화합니다. OpenAI의 Assistant API는 개발 노력을 최소화하는 반면, LangChain과 Semantic Kernel과 같은 프레임워크는 AI 응용 프로그램에 대한 심층적인 이해와 제어를 원하는 개발자들에게 매력적입니다. 이러한 프레임워크들은 AI 모델과 기존 코드 간의 간극을 메우는 SDK를 제공함으로써 돋보입니다. 이러한 SDK는 실제 세계 조치와 AI 응답의 통합을 용이하게 하여, 복잡한 비즈니스 프로세스를 자동화할 수 있는 완전 자동화된 AI 에이전트 구축에 이상적인 솔루션입니다. 플러그인, 도구 및 콘넥터를 통한 확장성은 다양한 기존 코드를 원활하게 연결함으로써 다른 공급 업체의 AI 서비스를 통합할 때 유연성을 제공합니다.\n\n반면, AutoGen은 다중 에이전트 프레임워크로 위치하며, LangChain의 단일 에이전트 초점과는 다릅니다. 이는 다중 에이전트 협업을 특징으로 하는 애플리케이션을 생성할 수 있어, 복잡한 에이전트 상호작용을 지향하는 개발자들을 위한 다재다능한 옵션을 제공합니다. 이러한 차이를 이해하는 것은 프로젝트 요구 사항과 원하는 협업 기능에 따라 이러한 프레임워크 중에서 선택하는 개발자들에게 중요합니다. 2024년 1월 말에, LangChain의 창시자들은 에이전트 실행 시간을 맞추기 위해 설계된 또 다른 다중 에이전트 워크플로인 LangGraph를 소개했습니다. 이 출시는 AutoGen과 비교했을 때 마음의 모델에서 상당한 변화를 제시합니다. 핵심적인 차이점은 프레임워크가 에이전트를 구성하는 방식에 있습니다. LangGraph는 고유한 에이전트 및 이들의 전이 확률을 명확하게 정의하는 방식을 촉진하며, 그것들을 그래프로 묘사합니다. 이에 반해, AutoGen은 이 과정을 더 \"대화\"로 보고 있습니다. 더불어, LangGraph는 LangChain 생태계에 원활하게 통합되어 있습니다. 이 통합을 통해 사용자들은 모든 LangChain 통합을 활용하고 LangSmith 감시 기능을 활용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리의 비교 분석을 시작하기 위해, 테이블 1에 명시된 여러 프레임워크의 기본적인 특성을 자세히 살펴보겠습니다. (화면의 너비 제한으로 인해 현재 웹페이지에서 보이지 않는 전체 내용을 볼 수 있도록 스크롤 막대를 드래그해주세요). 이 분석에서는 특히 세 가지 최고로 인정받는 프레임워크를 비교합니다. 특정 작업을 위해 개발자들이 개발 프로세스 중에 활용할 수 있는 흥미로운 특화된 라이브러리들인 가이던스, 가드레일, 람마 인덱스, 타입챗과 같은 추가로 흥미로운 라이브러리들이 있습니다. 그러나 이 기사의 목적상 이러한 라이브러리들을 자세히 다루지는 않겠습니다.\n\n## 테이블 1: 기본 특성 개요\n\n## 테이블 2: 샘플 레슨\n\n인터넷에는 소개에 관한 많은 유익한 수업들이 온라인으로 찾아볼 수 있습니다. 몇 가지 예시는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 프레임워크의 구성 요소\n\n이제, 더 깊이 탐구하기 위해 표 3-13에 나타난 프레임워크의 다양한 구성 요소를 면밀히 검토하고 비교해 보겠습니다.\n\n## 표 3: 구성 요소 개요: 작업 조율\n\n## 표 4: 구성 요소 개요: 메모리 관리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 테이블 5: 구성 요소 개요: 재사용 가능한 구성 요소\n\n## 테이블 6: 구성 요소 개요: 프롬프트 템플릿\n\n## 테이블 7: 구성 요소 개요: 문서 로더\n\n## 테이블 8: 구성 요소 개요: 문서 변환 및 분할\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 테이블 9: 구성 요소 개요: 호출 순서 구성\n\n## 테이블 10: 구성 요소 개요: 벡터 저장소\n\n## 테이블 11: 구성 요소 개요: 검색기\n\n## 테이블 12: 구성 요소 개요: 모델 입출력\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 테이블 13: 구성 요소 개요: 데이터 연결\n\n# 결론\n\nLLM(언어 모델 라이브러리)의 환경이 계속 발전함에 따라, 복잡한 AI 애플리케이션을 구축하려는 개발자들에게는 프레임워크 선택이 중요한 결정이 됩니다. Assistant API의 간편한 편리성이나 LangChain, LangGraph, Semantic Kernel, AutoGen과 같은 프레임워크가 제공하는 세밀한 제어라는 각 옵션은 각각의 장점과 고려해야 할 사항이 있습니다. 어떤 SDK를 사용할지 결정하는 것은 특정한 요구 사항, 선호도, 그리고 개발자의 목표뿐만 아니라 수행 중인 프로젝트의 성격에 달려 있습니다. 일반적인 해결책이 아니라 다양한 SDK들을 조화롭게 결합하여 사용하는 것이 종종 최적의 해결책일 수 있습니다. Semantic Kernel과 AutoGen의 원활한 통합에 대해 탐구한 John Maeda의 흥미로운 블로그 게시물과 함께, Matthew Bolanos는 오픈AI 어시스턴트를 통합하고 있으며 오픈AI 어시스턴트를 활용한 시각 등을 설명하는 \"Semantic Kernel의 미래: OpenAI 어시스턴트,\" \"OpenAI 어시스턴트: Semantic Kernel과 오픈AI 어시스턴트 사용에 대한 첫인상,\" 그리고 \"OpenAI 어시스턴트: 템플릿화된 어시스턴트 지시의 힘\" 시리즈를 Microsoft의 플랫폼에 발표하고 있습니다. Microsoft은 이미 OpenAI 어시스턴트 API를 사용하는 실험적인 구현을 갖고 있으나, 팀은 어떠한 모델로 만들어진 에이전트도 수용할 수 있는 에이전트 인터페이스의 완전한 추상화를 목표로 하고 있습니다. 이를 위해 Microsoft의 Semantic Kernel 팀 구성원들은 AutoGen팀의 연구 결과를 활용하여 에이전트가 팀으로 협업하는 시나리오를 포함한 다양한 경험을 수용할 수 있는 추상화를 개발하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더욱 풍부한 대화를 위해 LangChain은 프레임워크와 OpenAI 어시스턴트 간 상호작용을 명료하게 설명하는 포괄적인 문서를 보급했습니다. Gagan Bansal은 OpenAI 어시스턴트를 AutoGen에 통합하는 것을 탐구함으로써 대화에 기여했으며, GPTAssistantAgent에 대한 통찰을 통해 이에 대해 자세히 논의했습니다. 이러한 동적인 환경에서 다양한 SDK 간의 협업 가능성에 대해 정보를 가지고 있는 것이 AI 애플리케이션에서 대형 언어 모델의 전체 잠재력을 이용하는 데 중요합니다.\n\nCasey Doyle가 작업을 검토하는 데 도움을 준 데 대해 감사드립니다.","ogImage":{"url":"/assets/img/2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore_0.png"},"coverImage":"/assets/img/2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore_0.png","tag":["Tech"],"readingTime":5},{"title":"대형 언어 모델LLM을 위한 토큰 마스킹 전략들","description":"","date":"2024-06-23 19:24","slug":"2024-06-23-TokenMaskingStrategiesforLLMs","content":"\n\n## 다양한 언어 모델에서 사용되는 다양한 가리기 기술, 그 이점 및 Pytorch를 사용하여 낮은 수준에서 작동하는 방법에 대해 자세히 알아보세요.\n\n![이미지](/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_0.png)\n\n토큰 마스킹은 분류 변형 및 생성 모델에서 언어 모델을 훈련하는 데 널리 사용되는 전략입니다. BERT 언어 모델이 소개했으며 많은 변형(RoBERTa, ALBERT, DeBERTa 등)에서 사용되었습니다.\n\n그러나 토큰 마스킹은 텍스트 손상이라는 큰 그룹 내의 전략입니다. BART 연구 논문에서는 다양한 텍스트 손상 전략을 사용하여 인코더-디코더 생성 모델을 훈련하는 많은 실험이 수행되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_1.png\" /\u003e\n\n텍스트 손상에 대한 다양한 기술을 논의하기 전에, 대형 언어 모델(Large Language Models, LLMs)의 모든 텍스트 손상 방법에 대한 표준 개념에 대해 이야기하겠습니다.\n\n# 지도 학습에서 자기 지도 학습으로\n\n대규모 양의 텍스트가 언어 모델의 초기 교육에 사용되며, 모델이 언어를 올바르게 표현하도록 학습하고, 이 지식을 그 매개 변수 가중치에 암묵적으로 저장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 방대한 양의 텍스트는 학습을 위한 레이블이 있어야 합니다. 모델 입력 데이터를 처리한 후 참조 데이터를 사용하여 교차 엔트로피를 계산해야 합니다. 그러나 이렇게 많은 데이터에 주석을 다는 것은 현실적이지 않습니다. 따라서 우리는 자동 레이블 생성을 찾게 되었고, 지도 문제를 자가지도 문제로 전환하게 되었습니다.\n\n이 경우, 손상된 시퀀스는 모델의 학습 입력으로 작용하고, 기존 시퀀스의 전체 또는 일부가 학습 데이터의 레이블로 작용합니다. 이는 모델의 성격(인코더 또는 인코더-디코더)에 따라 다릅니다.\n\n# 손상 확률\n\n자동 레이블을 사용하여, 모델은 데이터에 주석을 달지 않고 각 학습 예제와 연결된 레이블을 학습합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n텍스트 손상(특히 토큰 마스킹, 토큰 삭제 및 텍스트 인필링에서)에서 각 단어는 일반적으로 15–20% 정도의 확률에 따라 손상될 것입니다. 이 확률은 모델이 각 문장의 맥락을 학습할 수 있도록 낮게 유지됩니다.\n\n문장 순서 바꾸기 또는 문서 회전과 같은 일부 텍스트 손상 기술은 특정 확률로 단어를 손상시키지 않습니다. 이로 인해 다른 손상 기술과 호환되도록 할 수 있습니다. 아래에서 논의할 것과 같이요.\n\n# 분류와 생성 사이의 차이점\n\n텍스트 손상을 통해 언어 모델을 학습할 때, 레이블은 분류 모델(인코더만)인지 생성 모델(인코더-디코더)인지에 따라 달라집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n분류 모델에서는 레이블을 사용하여 입력의 오염된 영역에만 주의를 기울입니다. 따라서 만약 문장 전체에서 단어가 마스킹되었다면, 레이블은 초기 시퀀스가 되어 오염된 시퀀스에만 주의를 기울입니다.\n\n생성 모델의 경우, 모델이 텍스트를 연속적으로 생성할 수 있어야 하므로 출력 레이블은 초기 정상 시퀀스가 되며 전체 시퀀스 자체에 주의를 기울입니다.\n\n# 설정\n\n이제 우리는 텍스트 오염으로 언어 모델을 학습할 때의 공통점을 간단히 소개했으니, 텍스트를 손상시키는 다양한 기술과 각 경우에 코드 예시를 제시하는 것을 논의해보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 서로 다른 전략들이 어떻게 작동하는지 보기 위해 코드 예제에서 문서로 시작하겠습니다. 우리는 전처리에 매우 유용한 여러 자연어 처리 도구를 갖춘 Stanford NLP에서 개발된 라이브러리인 Stanza를 사용할 것입니다.\n\n```js\nimport stanza\nstanza.download('en')\n\n# 예제에서 사용되는 텍스트\ntext = \"헌팅턴병은 유전적인 신경퇴행성 질환으로 헌팅틴 유전자 내 다형성 CAG 반복이 확장되면서 발생합니다. 번역 초기 인자 4E-BP의 인산화는 번역 조절의 변경으로 이어져서 원치 않는 단백질 합성과 신경 기능에 영향을 줍니다. 돌연변이 헌팅틴(mhtt) 유전자의 전사 결과는 잘 알려지지 않았습니다. 발병 연령의 가변성은 성인과 소아형 헌팅턴병을 분리하는 중요한 요소입니다. 고령 유전자, 어머니의 보호(즉, 과도한 아버지의 유전), 우수한 노화 유전자 및 환경 임계치가 고려되는 요소입니다. 분자 병인학에 중점을 둔 부분으로는 운동 장애, 인지 장애 및 신경 정신 장애가 포함됩니다. 진단 부분도 고려되었습니다. 이는 유전자 검사 및 주요 및 이차 증상을 포함합니다. 헌팅턴병의 유전학과 병리학에도 주목합니다.\"\n\n# 각 다른 문장을 리스트 요소로 얻기 위해 Stanza 모델을 사용할 것입니다.\nnlp = stanza.Pipeline('en', use_gpu=False)\ndoc = nlp(text)\nsentences = [sentence.text for sentence in doc.sentences]\n```\n\n# 토큰 가림\n\nBERT는 이 전략을 도입했는데, 이는 첫 번째이자 가장 잘 알려진 시퀀스 손상 전략입니다. 입력 시퀀스를 무작위 단어로 가려서 훈련 중에 레이블로 사용될 단어를 손상하는 것으로 구성되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n분류 모델에서는 Huggingface transformers에서 DataCollatorForLanguageModeling 클래스를 직접 사용하여 BERT 또는 RoBERTa와 같은 모델을 학습할 수 있는 필요한 레이블을 생성할 수 있습니다.\n\n```js\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling\nimport torch\n\ndef load_dataset_mlm(sentences, tokenizer_class=AutoTokenizer, \n                     collator_class=DataCollatorForLanguageModeling, \n                     mlm=True, mlm_probability=0.20):\n    tokenizer = tokenizer_class.from_pretrained('google-bert/bert-base-uncased')\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True, \n                       truncation=True)\n    \n    # 랜덤 마스킹 설정\n    data_collator = collator_class(\n        tokenizer=tokenizer, \n        mlm=mlm,  \n        mlm_probability=mlm_probability \n    )\n\n    \"\"\"콜레이터는 텐서들 튜플을 기대하므로 입력 텐서들을 분리한 다음\n    첫 번째 차원을 삭제하고 튜플로 전달해야 합니다.\"\"\"\n    tuple_ids = torch.split(inputs['input_ids'], 1, dim=0)\n    tuple_ids = list(tuple_ids)\n    for tensor in range(len(tuple_ids)):\n        tuple_ids[tensor] = tuple_ids[tensor].squeeze(0)\n    tuple_ids = tuple(tuple_ids)\n    \n    # 각 문장의 input_ids, attention_masks 및 레이블 가져오기\n    batch = data_collator(tuple_ids)\n    return batch['input_ids'], inputs['attention_mask'], batch['labels']\n\n\ninput_ids, attention_mask, labels = load_dataset_mlm(sentences)\n\n\"\"\"\ninput_ids[0]:\ntensor([  101, 16364,  1005,  1055,   103,  2003,  1037,   103, 10976,  3207,\n          103, 25284,   103, 25426, 16870,  4295,  3463,  2349,  2000,   103,\n         1997, 26572, 18078,  6187,  2290, 17993,  1999,  1996,  5933,  7629,\n          103,   103,   102,     0,     0])\n\nattention_mask[0]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n\nlabels[0]:\ntensor([ -100,  -100,  -100,  -100,  4295,  -100,  -100, 11265,  -100,  -100,\n         6914,  -100,  8285,  -100,  2389,  -100,  -100,  -100,  -100,  4935,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         4962,  1012,  -100,  -100,  -100))\n\n\"\"\"\n```\n\n생성된 input_ids에는 원본 텍스트의 각 토큰에 대한 정수 번호가 있습니다. 특별한 토큰은 마스킹된 단어를 나타내며 (BERT에서는 이 토큰이 103입니다), 이 특별한 토큰은 사용하는 언어 모델에 따라 다르며 서로 다른 토크나이저는 서로 다른 주의 마스크 식별자를 반환합니다.\n\n또한, Huggingface는 모델 내에서 다른 작업을 수행하여 고유 토큰에 대해 다른 작업을 지정하므로 \"-100\"으로 표시된 토큰은 모델에서 무시해야 함을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBART와 같은 생성 모델의 경우, DataCollatorForLanguageModeling 클래스를 사용하여 토큰 마스킹 전략을 구현할 수 있습니다. 그러나 생성 모델에 맞게 태그를 조정하기 위해 작은 변경을 도입해야 합니다.\n\n```js\nfrom transformers import BartTokenizer, DataCollatorForLanguageModeling\nimport torch\n\ndef load_dataset_mlm(sentences, tokenizer_class=BartTokenizer, \n                     collator_class=DataCollatorForLanguageModeling, \n                     mlm=True, mlm_probability=0.20):\n    tokenizer = tokenizer_class.from_pretrained('facebook/bart-base')\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True, \n                       truncation=True)\n    \n    # 랜덤 마스킹 구성\n    data_collator = collator_class(\n        tokenizer=tokenizer, \n        mlm=mlm,  # 마스크된 언어 모델링을 위해 True\n        mlm_probability=mlm_probability  # 각 토큰이 마스킹될 확률\n    )\n\n    \"\"\"Collator는 텐서의 튜플을 예상하므로 입력 텐서를 분할하고 첫 번째 차원을 제거한 후 튜플로 전달해야합니다.\"\"\"\n    tuple_ids = torch.split(inputs['input_ids'], 1, dim=0)\n    tuple_ids = list(tuple_ids)\n    for tensor in range(len(tuple_ids)):\n        tuple_ids[tensor] = tuple_ids[tensor].squeeze(0)\n    tuple_ids = tuple(tuple_ids)\n    \n    # 각 문장에 대한 input_ids, attention_masks 및 labels 가져오기\n    batch = data_collator(tuple_ids)\n    batch['labels'] = inputs['input_ids']\n    return batch['input_ids'], inputs['attention_mask'],  batch['labels']\n\ninput_ids, attention_mask, labels = load_dataset_mlm(sentences)\n\n\"\"\"\ninput_ids[0]:\ntensor([    0, 38831,  2577,  1054,    18,  2199,    16,    10, 14913, 28904,\n         5777,  3693, 32226, 38868,  2199,   775,   528,     7,  2919,     9,\n        48052,   636,   230,  3450, 35315,    11,     5, 50264, 50264, 50264,\n            4,     2])\n\nattention_mask[0]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1])\n\nlabels[0]:\ntensor([    0, 38831,  2577,  1054,    18,  2199,    16,    10, 14913, 28904,\n         5777,  3693, 32226, 38868,  2199,   775,   528,     7,  2919,     9,\n        48052,   636,   230,  3450, 35315,    11,     5,  8217, 24276, 10596,\n            4,     2])\n\"\"\"\n```\n\n여기서 각 입력 토큰은 마스크 여부에 관계없이 해당하는 토큰을 레이블링하며, 이는 분류 모델과는 달리 모델이 제공된 시퀀스에 기반한 텍스트 시퀀스를 생성할 수 있어야 하기 때문입니다. BART의 경우, 각 마스크를 나타내는 토큰은 ID 50264를 갖습니다.\n\n# 토큰 삭제\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 전략은 마스킹에 대해 다른 접근 방식을 사용합니다. 특정 확률로 텍스트의 원래 시퀀스에서 단어가 제거되어 모델은 빈칸과 해당 위치를 찾아야 합니다. 표준 마스킹은 마스크가 이미 모델의 입력에서 지정되어 있기 때문에 위치를 학습하지 않습니다.\n\n```js\ndef token_deletion(sentences, tokenizer_class=BartTokenizer, collator_class=DataCollatorForLanguageModeling, \n                 mlm=True, mlm_probability=0.20):\n    tokenizer = tokenizer_class.from_pretrained('facebook/bart-base')\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n    \n    data_collator = collator_class(\n        tokenizer=tokenizer, \n        mlm=mlm,\n        mlm_probability=mlm_probability \n    )\n\n    tuple_ids = torch.split(inputs['input_ids'], 1, dim=0)\n    tuple_ids = list(tuple_ids)\n    for tensor in range(len(tuple_ids)):\n        tuple_ids[tensor] = tuple_ids[tensor].squeeze(0)\n    tuple_ids = tuple(tuple_ids)\n \n    batch = data_collator(tuple_ids)\n\n    # We use the initial inputs as labels\n    batch['labels'] = batch['input_ids'].clone()\n    \n    # We remove tokens with mask identifier and thus make token deletion\n    # Change the value to the mask identifier of the specific token model\n    # It is necessary to know the identifier of the mask token for \n    # that specific model\n    mask = batch['input_ids'] != 50264\n    initial_size = batch['input_ids'].size(1)\n    total_sentences = batch['input_ids'].size(0)\n\n    # When we remove the specific token, we must fill with the padding \n    # token otherwise the tensor size is not respected.\n    for i in range(total_sentences):\n        new_tensor = batch['input_ids'][i][mask[i]]\n        new_tensor = F.pad(new_tensor, (0, initial_size - new_tensor.size(0)), value=1)\n        batch['input_ids'][i] = new_tensor\n        attention_mask = batch['input_ids'][i] == 1\n        inputs['attention_mask'][i][attention_mask] = 0\n        \n    return batch['input_ids'], inputs['attention_mask'], batch['labels']\n\ninput_ids, attention_mask, labels = token_deletion(sentences)\n\n\"\"\"\ninput_ids[0]:\ntensor([    0, 38831,  2577,  1054,  2199, 14913, 28904,  3693, 32226, 38868,\n         2199,   775,   528,     7,  2919,     9, 23404,   636,   230, 35315,\n           11,     5, 24276, 10596,     4,     2,     1,     1,     1,     1,\n            1,     1])\n\nattention_mask[0]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 0, 0, 0, 0, 0])\n\nlabels[0]:\ntensor([    0, 38831,  2577,  1054, 50264,  2199, 50264, 50264, 14913, 28904,\n        50264,  3693, 32226, 38868,  2199,   775,   528,     7,  2919,     9,\n        23404,   636,   230, 50264, 35315,    11,     5, 50264, 24276, 10596,\n            4,     2])\n\n\"\"\"\n```\n\nBART를 사용하여 Token Deletion을 훈련할 때, 일부 텍스트 생성 벤치마킹은 질문 응답, 요약 생성 작업 및 대화 작업에 긴 시퀀스를 사용할 때 약간의 개선이 나타납니다.\n\n# 텍스트 채워넣기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n텍스트 인필링은 토큰 마스킹과 비슷합니다. 특정 확률로 원본 텍스트에 마스크를 씌우게 됩니다. 이 경우에는 마스킹이 하나의 단어 이상을 덮을 수 있다는 차이가 있습니다. BART에서 텍스트 인필링을 적용할 때, 람다 값이 3인 포아송 분포를 사용하여 마스킹을 수행합니다. 이는 평균적으로 문장에서 텍스트가 마스킹될 때마다 세 개의 단어가 하나의 토큰 마스크로 마스킹됨을 의미합니다. 그러나 확률 분포이기 때문에 더 많거나 더 적은 개수의 마스킹된 단어가 있을 수도 있습니다.\n\n![image](/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_2.png)\n\n저희는 NumPy 라이브러리와 우리 언어 모델에 특화된 토크나이저를 사용하여 텍스트 인필링을 구현할 것입니다. 아래는 예시 코드입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 문장 순열\n\n문장 순열에서 모델 입력 시퀀스에 맞는 문장 수(소형 모델에서 입력 시퀀스는 512에서 1024 토큰 사이)를 고려하는 것이 매우 중요합니다. 순서 테스트 후, 시퀀스에 맞게 문장 수를 결정한 다음, 그것들을 리스트나 배열로 나누어야 하고, 예시 코드에서와 같이 중복되지 않는 방식으로 무작위로 선택해야 합니다.\n\n```js\n# 주어진 \"문장\" 세트 중 첫 \"number_sentences\"를 선택하고 그 문장들을 무작위로 반환합니다.\ndef sentence_permutation(sentences, number_sentences):\n    new_sentences = sentences[:number_sentences]\n    random.shuffle(new_sentences)\n    new_sentences = sentence_joiner(new_sentences)\n    return new_sentences\n\ndef permuted_data_generation(sentences: list, total_sentences: int):\n    training_sentences = []\n    training_labels = []\n    sentences_copy = sentences.copy()\n    # 문장 목록의 크기에서 1을 뺀 횟수만큼 sentence_permutation을 적용하여 \n    # 텍스트의 각 새 문장 예제를 얻고 가장 오래된 문장을 제거합니다.\n    for _ in range(len(sentences)-total_sentences+1):\n        new_sentences = sentence_permutation(sentences_copy, total_sentences)\n        joined_sentences = sentence_joiner(sentences_copy[:total_sentences])\n        sentences_copy = sentences_copy[1:]\n        training_sentences.append(new_sentences)\n        training_labels.append(joined_sentences)\n\n    return training_sentences, training_labels\n\n\ndef permutation_training(sentences: list, sentences_labels: list, \n                         tokenizer_class=BartTokenizer, \n                         collator_class=DataCollatorForLanguageModeling, \n                         mlm=True, mlm_probability=0.0):\n    # permuted 문장으로부터 input_ids와 attention mask를 얻습니다\n    input, attention_mask, _ = load_dataset_mlm(sentences, tokenizer_class, collator_class, mlm, mlm_probability)\n    \n    # 원본 문장으로부터 라벨 가져오기\n    labels, _, _ = load_dataset_mlm(sentences_labels, tokenizer_class, collator_class, mlm, mlm_probability)\n\n    return input.squeeze(0), attention_mask.squeeze(0), labels.squeeze(0)\n\ninput_ids, attention_mask, labels = permutation_training(training_sentences, training_labels_sentences)\n\n\"\"\"\ninput_ids[0]:\ntensor([    0, 38831,  2577,  1054,    18,  2199, ...\n\nattention_mask[0]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, ...\n\nlabels[0]:\ntensor([    0, 38831, 2577, 1054, 18, 2199, ...\n\"\"\"\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 예제에서는 모델에 데이터를 입력할 때 원래 순서에서 먼저 나온 문장을 제거한 후, 주어진 문장 개수에 따라 문장 순열을 수행하기 전에 새로운 문장을 추가합니다. 이렇게 함으로써 입력 시퀀스의 문장을 다시 정렬하더라도 각 새로운 예제마다 새로운 문장이 나타나는 문맥 창을 유지하고 가장 오래된 문장을 삭제합니다.\n\n# 문서 회전\n\n문서 회전을 적용하려면 사용된 각 배치의 차원을 고려해야 합니다. 패딩을 적용하는 경우 패딩은 문서의 나머지 부분과 함께 회전되지 않고 원래 위치를 유지해야합니다.\n\n```js\ndef sentence_joiner(sentences: list):\n  return ' '.join(sentences)\n\n# 이 함수를 사용하여 토크나이저에 입력 데이터를 형성할 수 있는 원하는 만큼의 문장을 모을 수 있습니다.\ndef rotated_data_generation(sentences: list, total_sentences: int):\n  training_sentences = []\n  sentences_copy = sentences.copy()\n  for _ in range(len(sentences)-total_sentences+1):\n    new_sentences = sentences_copy[:total_sentences]\n    new_sentences = sentence_joiner(new_sentences)\n    sentences_copy = sentences_copy[1:]\n    training_sentences.append(new_sentences)\n  return training_sentences\n\n# 이전 함수에서 회전된 문장을 적용합니다.\ndef document_rotation_training(sentences, tokenizer_class=BartTokenizer):\n  tokenizer = tokenizer_class.from_pretrained('facebook/bart-base')\n  tokens = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n  tokens['input_ids'] = tokens['input_ids'].squeeze(0)\n  tokens['labels'] = tokens['input_ids'].clone()\n \n  iterations = tokens['input_ids'].size(0)\n  for i in range(iterations):\n    # 어텐션 마스크를 가져와 리스트로 변환합니다.\n    attention_mask = tokens['attention_mask'][i].tolist()\n    # 패딩이 시작되는 위치를 계산합니다.\n    if 0 in attention_mask:\n      padding_start_position = attention_mask.index(0)\n    else:\n      padding_start_position = False\n    # 패딩이 있는 경우 문서의 나머지 부분과 함께 회전되지 않도록 패딩 위치를 고려합니다.\n    if padding_start_position:\n      random_token = torch.randint(1, padding_start_position-1, (1,))\n      tokens['input_ids'][i] = torch.cat((tokens['input_ids'][i][0].unsqueeze(0), \n                                      tokens['input_ids'][i][random_token.item():padding_start_position-1],\n                                      tokens['input_ids'][i][1:random_token.item()],\n                                      tokens['input_ids'][i][padding_start_position-1:-1],\n                                      tokens['input_ids'][i][-1].unsqueeze(0)), 0)\n                                        \n    # 패딩이 없는 경우 패딩을 고려하지 않고 문서를 회전합니다.\n    else:\n      random_token = torch.randint(1, tokens['input_ids'].size(0)-1, (1,))\n      tokens['input_ids'][i] = torch.cat((tokens['input_ids'][i][0].unsqueeze(0),\n                                      tokens['input_ids'][i][random_token.item():-1],\n                                      tokens['input_ids'][i][1:random_token.item()],\n                                      tokens['input_ids'][i][-1].unsqueeze(0)), 0)\n  return tokens['input_ids'], tokens['attention_mask'].squeeze(0), tokens['labels']\n\ndata = rotated_data_generation(sentences, 3)\ninput_ids, attention_mask, labels = document_rotation_training(data)\n\n\"\"\"\ninput_ids[2]:\ntensor([    0,  2433,    61,    32,   551,    88,  1316,    32,    12,  4138,\n        15557, 47605,     6, 22835,  2591,   939,     4,   242, 10079, 38422,\n         9235,     6, 10295, 22540, 14819,     8,  3039, 11543,     4,   347,\n        37347,  8457,     9, 41419,  8217,  1054,    36,   119, 49491,    43,\n        10596, 37118,    32,    45,   157,   684,     4, 41058,  4484,     9,\n         1046,     9, 23808,    16,    41,   505,  3724,     9, 18073,    18,\n         2199, 18246,  4194,     8, 13430,  3505,     4,    20,     2,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n\nattention_mask[2]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])\n\nlabels[2]:\ntensor([    0,   347, 37347,  8457,     9, 41419,  8217,  1054,    36,   119,\n        49491,    43, 10596, 37118,    32,    45,   157,   684,     4, 41058,\n         4484,     9,  1046,     9, 23808,    16,    41,   505,  3724,     9,\n        18073,    18,  2199, 18246,  4194,     8, 13430,  3505,     4,    20,\n         2433,    61,    32,   551,    88,  1316,    32,    12,  4138, 15557,\n        47605,     6, 22835,  2591,   939,     4,   242, 10079, 38422,  9235,\n            6, 10295, 22540, 14819,     8,  3039, 11543,     4,     2,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n\n\"\"\"\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n짧은 텍스트 시퀀스는 문서 회전 및 문장 순열 기술을 의미 없게 만듭니다. 반면에 다른 언급된 방법들(토큰 마스킹, 토큰 삭제 및 텍스트 채우기)은 짧고 긴 텍스트 시퀀스에서 도움이 될 수 있습니다.\n\n시퀀스 순열과 마찬가지로 각 데이터 입력마다 가장 오래된 문장을 제거하고 새로운 문장을 추가하여 문맥 윈도우를 유지할 수 있습니다.\n\n# 결론\n\n본 글은 시퀀스 왜곡으로 언어 모델을 학습시키는 다양한 방법에 대해 논의했습니다. 이들은 가장 유명하지만 대부분의 모델은 토큰 마스킹만을 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요약하면, 가장 효과적인 전략은 텍스트를 변경하는 대신 텍스트를 손상시키는 것입니다. 그러나 이 두 가지 접근 방식은 모델 훈련 중에 결합될 수 있으며, BART의 경우 Text Infilling 및 Sentence Permutation을 사용하여 흥미로운 결과를 얻을 수 있었습니다.\n\n이 훈련 방식은 인코더 또는 인코더-디코더 트랜스포머 모델에서 사용할 수 있습니다. 내가 아는 한, 이 접근 방식을 사용하는 디코더 전용 모델은 없습니다. 왜냐하면 그런 경우에는 자기 회귀 언어 모델링 또는 인과 언어 모델링이 사용됩니다. 이는 GPT 모델과 같이 자기 어텐션 메커니즘 없이 인코더를 사용할 때 각 토큰의 예측이 이전 토큰에만 의존하며 그 다음 토큰에는 의존하지 않기 때문입니다. BERT와 같은 인코더 전용 모델에서는 양방향 어텐션이 존재하여 각 토큰이 이전 및 이후 토큰에 따라 어느 위치에 있어야 하는지 예측할 수 있습니다.\n\n미래 글에서는 어떻게 인과 언어 모델링이 작동하는지와 더 고급 시퀸스 손상 기술에 대해 깊이 알아볼 것입니다.\n\n![이미지](/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_3.png)\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n행복한 코딩하세요!","ogImage":{"url":"/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_0.png"},"coverImage":"/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_0.png","tag":["Tech"],"readingTime":18},{"title":"반품 및 환불 처리를 위한 챗봇 활용 엔터프라이즈 워크플로우 통합 방법","description":"","date":"2024-06-23 19:23","slug":"2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds","content":"\n\n\u003cimg src=\"/assets/img/2024-06-23-엔터프라이즈워크플로우통합챗봇을활용한반품및환불.png\" /\u003e\n\n고객 서비스 랜드스케이프가 급격히 변화하고 있습니다. 기업들은 영업을 최적화하고 고객 경험을 높이기 위해 AI 솔루션을 채택하고 있습니다. 기본 챗봇은 FAQ에 대한 답변으로 흔히 사용되고 있지만, 반품이나 환불 처리와 같이 복잡한 시나리오에 대해 어려움을 겪는 경우가 많습니다. 이러한 복잡한 상황에서 가장 큰 수익률을 얻을 수 있는 영역입니다.\n\n# 기본 챗봇 응용\n\nLLM 챗봇은 대부분 고객 서비스에 사용되며, 기본 지원을 제공하고 자주 묻는 질문에 답변합니다. 이러한 챗봇은 일반적으로 응답 생성을 위해 소스 문서나 HTML 페이지의 라이브러리에 의존합니다. 이 문서들은 청크(chunk) 단위로 분리되어 벡터 저장소(Vector Store)에 저장됩니다. 사용자가 질문을 하면 벡터 저장소에서 유사도 검색이 수행되어 관련 문서 청크가 검색되고, 이후 OpenAI와 같은 LLM으로 응답을 생성하기 위해 전송됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds_1.png)\n\n챗봇이 도움이 될 수는 있지만, 고객 서비스 자동화의 진정한 가치는 고객 지원에서 정례적으로 발생하는 복잡한 상황을 해결할 수 있는 능력에 있습니다.\n\n# 챗봇을 활용하여 복잡한 워크플로우 문제 해결하기\n\n자연어 처리 및 생성 기술의 놀라운 발전에도 불구하고, LLM을 기반으로 한 챗봇은 여전히 제품 반품 또는 환불과 같이 복잡한 상황에서 특히 고객이 만족스러운 경험을 얻는 데 어려움을 겪고 있습니다. 왜 그런지 살펴보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003ctable\u003e\n\u003ctr\u003e\n\u003cth\u003eIssue\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eContextual Blindspots\u003c/td\u003e\n\u003ctd\u003eLLMs (Large Language Models) may sound human-like, but often miss important context. For instance, a chatbot might not realize a prior promise for faster shipping, causing frustration.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eIsolated Operations\u003c/td\u003e\n\u003ctd\u003eTraditional LLM chatbots are often disconnected from essential systems like order management or knowledge bases. This means a chatbot may not access specific booking details when a customer wants to change a flight.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eInconsistent Service\u003c/td\u003e\n\u003ctd\u003eLLMs can provide conflicting responses, especially in complex situations with strict policies. This can lead to confusion and distrust, like differing information on warranty policies.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLanguage Barriers\u003c/td\u003e\n\u003ctd\u003eMost LLM chatbots support only one language, limiting help for a global customer base. For example, an English-only chatbot may not effectively assist Spanish-speaking customers.\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\n# Why Processing Refunds can be Challenging for AI (Even the Super Smart Ones!)\n\nEven highly intelligent AI systems face difficulties when it comes to handling refund processes. Here are some essential capabilities that chatbots need to possess for successfully managing complex tasks like refunds:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 많은 규칙을 따라야 합니다: 온라인으로 셔츠를 주문했지만 잘못된 사이즈로 도착했어요. 환불 정책에 따르면 구매 후 30일 이내에 원래 영수증이 있는 새 제품만 환불을 받을 수 있을 수도 있습니다. AI는 이 모든 규칙과 함께 주문 세부 정보 (날짜, 사이즈 등)와 교환 사유 (잘못된 사이즈)를 이해해야 올바른 결정을 내릴 수 있어야 해요. 이 모든 것을 동시에 이해하는 것은 수 많은 퍼즐 조각을 맞추는 것과 같아요!\n- 당신의 말을 이해하기: 교환에 관해 AI에게 “이 셔츠는 너무 커!” 라고 메시지를 보낼 수도 있어요. AI의 자연어 처리 능력은 당신이 환불 정책에 정확한 단어를 사용하지 않아도 (예: “잘못된 사이즈”) 당신이 무슨 의미인지 파악하는 데 능숙해야 해요. 이는 용어가 많은 문자 메시지를 번역하는 것과 같아요 — AI는 당신의 말 뒤에 감추어진 의도를 이해해야 해요.\n- 다른 시스템과 대화하기: 환불을 처리하려면 AI가 여러 컴퓨터 프로그램을 확인해야 할 수도 있어요. 세부 정보를 확인하기 위해 주문 내역을 확인해야 하거나 적용 가능한 규칙을 확인하기 위해 환불 정책 데이터베이스를 참고하거나 환불을 시작하기 위해 금융 시스템에 연결해야 할 수도 있어요. 이 모든 것을 처리해야 하는 것은 여러 공을 질러야 하는 것과 같아요 — AI는 이러한 다양한 시스템과의 소통을 처리하여 환불을 올바르게 처리해야 해요.\n- 상황에 적응하기: 환불 처리를 위한 AI 시스템은 “왜 환불을 요청하시나요?” 나 “제품이 손상되었나요, 아니면 생각이 바뀌었나요?”와 같이 명확히 질문하여 상황에 적응해야 해요. 세부 사항을 확인한 후 자신의 방식으로 접근을 조정해야 해요. 하자가 있는 경우, 사진을 요청하고 전액 환불을 처리할 수 있어야 해요. 생각이 바뀐 경우, 반품 안내나 재설치 수수료가 있는 라벨을 제공하거나 당신에게 가장 잘 맞는 옵션을 제안하며 교환이나 상품권을 제공할 수 있어야 해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은 기업들이 처음에는 기존 문서에서 FAQ를 처리하기 위해 챗봇을 도입하지만, 실제 기회는 챗봇이 복잡한 업무 흐름을 관리할 수 있는 능력에 있습니다. 이러한 시나리오는 챗봇이 특정 정책을 조회하고 내부 시스템과 상호 작용하며 명확한 질문을 하고 다양한 상황에 적응하며 필요할 때 실시간 대화 대상에게 매끄럽게 연결되어야 합니다. 이것이 다음 12개월 동안 가장 중요한 챗봇 혁신이 일어날 곳이며, 기업에 가장 높은 ROI를 제공할 것입니다.","ogImage":{"url":"/assets/img/2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds_0.png"},"coverImage":"/assets/img/2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds_0.png","tag":["Tech"],"readingTime":4}],"page":"9","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"9"},"buildId":"kkckKPyxcjJp_o8wb2unW","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>