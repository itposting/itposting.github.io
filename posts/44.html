<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/44" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/44" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="지식 그래프로 RAG 어플리케이션의 정확성 향상하기" href="/post/2024-06-20-EnhancingtheAccuracyofRAGApplicationsWithKnowledgeGraphs"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="지식 그래프로 RAG 어플리케이션의 정확성 향상하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-EnhancingtheAccuracyofRAGApplicationsWithKnowledgeGraphs_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="지식 그래프로 RAG 어플리케이션의 정확성 향상하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">지식 그래프로 RAG 어플리케이션의 정확성 향상하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="컴퓨터 프로그래머이자 15년간 ChatGPT에 대해 전문적으로 썼어요 - 전문 작가들이 괜찮다는 법을 이야기해 드릴게요" href="/post/2024-06-20-ImaComputerProgrammerandWroteProfessionallyaboutChatGPTfor15YearsHeresHowProWritersCanBeOkay"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="컴퓨터 프로그래머이자 15년간 ChatGPT에 대해 전문적으로 썼어요 - 전문 작가들이 괜찮다는 법을 이야기해 드릴게요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-ImaComputerProgrammerandWroteProfessionallyaboutChatGPTfor15YearsHeresHowProWritersCanBeOkay_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="컴퓨터 프로그래머이자 15년간 ChatGPT에 대해 전문적으로 썼어요 - 전문 작가들이 괜찮다는 법을 이야기해 드릴게요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">컴퓨터 프로그래머이자 15년간 ChatGPT에 대해 전문적으로 썼어요 - 전문 작가들이 괜찮다는 법을 이야기해 드릴게요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">13<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="합리적인 AI 프롬프트의 비밀 데이터와 함께 프롬프팅" href="/post/2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="합리적인 AI 프롬프트의 비밀 데이터와 함께 프롬프팅" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="합리적인 AI 프롬프트의 비밀 데이터와 함께 프롬프팅" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">합리적인 AI 프롬프트의 비밀 데이터와 함께 프롬프팅</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="챗지피티 대규모 업데이트 엑셀 실시간 대화형 분석 누군가가 그 뒤에 있는 새로운 모델을 공개했습니다 " href="/post/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="챗지피티 대규모 업데이트 엑셀 실시간 대화형 분석 누군가가 그 뒤에 있는 새로운 모델을 공개했습니다 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="챗지피티 대규모 업데이트 엑셀 실시간 대화형 분석 누군가가 그 뒤에 있는 새로운 모델을 공개했습니다 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">챗지피티 대규모 업데이트 엑셀 실시간 대화형 분석 누군가가 그 뒤에 있는 새로운 모델을 공개했습니다 </strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="언어 모델 보정 기법 확률 평가 향상하기" href="/post/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="언어 모델 보정 기법 확률 평가 향상하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="언어 모델 보정 기법 확률 평가 향상하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">언어 모델 보정 기법 확률 평가 향상하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AI 에이전트 능력 엔지니어링" href="/post/2024-06-20-AIAgentCapabilitiesEngineering"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AI 에이전트 능력 엔지니어링" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-AIAgentCapabilitiesEngineering_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AI 에이전트 능력 엔지니어링" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">AI 에이전트 능력 엔지니어링</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지" href="/post/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">18<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="인공지능의 마법 구슬 신경망이 인플레이션을 예측하는 방법" href="/post/2024-06-20-AIsCrystalBallHowNeuralNetworksForecastInflation"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="인공지능의 마법 구슬 신경망이 인플레이션을 예측하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-AIsCrystalBallHowNeuralNetworksForecastInflation_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="인공지능의 마법 구슬 신경망이 인플레이션을 예측하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">인공지능의 마법 구슬 신경망이 인플레이션을 예측하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="사함이 없이 나는 누구일까요" href="/post/2024-06-20-ngunitsinoakokungwalangmedalya"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="사함이 없이 나는 누구일까요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-ngunitsinoakokungwalangmedalya_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="사함이 없이 나는 누구일까요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">사함이 없이 나는 누구일까요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="산불에 대해 자세히 알아보기" href="/post/2024-06-20-DelvingintoWildfires"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="산불에 대해 자세히 알아보기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-DelvingintoWildfires_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="산불에 대해 자세히 알아보기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">산불에 대해 자세히 알아보기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/41">41</a><a class="link" href="/posts/42">42</a><a class="link" href="/posts/43">43</a><a class="link posts_-active__YVJEi" href="/posts/44">44</a><a class="link" href="/posts/45">45</a><a class="link" href="/posts/46">46</a><a class="link" href="/posts/47">47</a><a class="link" href="/posts/48">48</a><a class="link" href="/posts/49">49</a><a class="link" href="/posts/50">50</a><a class="link" href="/posts/51">51</a><a class="link" href="/posts/52">52</a><a class="link" href="/posts/53">53</a><a class="link" href="/posts/54">54</a><a class="link" href="/posts/55">55</a><a class="link" href="/posts/56">56</a><a class="link" href="/posts/57">57</a><a class="link" href="/posts/58">58</a><a class="link" href="/posts/59">59</a><a class="link" href="/posts/60">60</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"지식 그래프로 RAG 어플리케이션의 정확성 향상하기","description":"","date":"2024-06-20 18:36","slug":"2024-06-20-EnhancingtheAccuracyofRAGApplicationsWithKnowledgeGraphs","content":"\n\n## 네오포스트 및 LangChain을 사용하여 RAG 애플리케이션에서 지식 그래프를 구축하고 정보를 검색하는 실용적 가이드\n\n![Enhancing the Accuracy of RAG Applications With Knowledge Graphs](/assets/img/2024-06-20-EnhancingtheAccuracyofRAGApplicationsWithKnowledgeGraphs_0.png)\n\n그래프 검색 보강 생성 (GraphRAG)은 전통적인 벡터 검색 검색 방법에 강력한 보충으로 인기를 얻고 있습니다. 이 접근 방식은 그래프 데이터베이스의 구조화된 성격을 활용하여 데이터를 노드와 관계로 구성함으로써 회수된 정보의 깊이와 맥락을 향상시킵니다.\n\n![Enhancing the Accuracy of RAG Applications With Knowledge Graphs](/assets/img/2024-06-20-EnhancingtheAccuracyofRAGApplicationsWithKnowledgeGraphs_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래프는 다양하고 서로 연결된 정보를 체계적으로 표현하고 저장하는 데 탁월합니다. 복잡한 관계와 속성을 다양한 데이터 유형에 걸쳐 손쉽게 캡처할 수 있습니다. 반면에 벡터 데이터베이스는 고차원 벡터를 통해 비구조화된 데이터를 처리하는 데 강점을 가지고 있어서 구조화된 정보에 어려움을 겪을 수 있습니다. RAG 애플리케이션에서는 구조화된 그래프 데이터와 비구조화된 텍스트를 통한 벡터 검색을 결합하여 양쪽의 장점을 모두 활용할 수 있습니다. 이것이 이 블로그 포스트에서 증명할 내용입니다.\n\n## 지식 그래프가 훌륭하지만, 그것을 어떻게 만들까요?\n\n지식 그래프를 구축하는 것은 보통 가장 어려운 단계입니다. 이 작업에는 데이터 수집과 구조화가 필요하며, 해당 도메인과 그래프 모델링에 대한 심층적인 이해가 필요합니다.\n\n이 프로세스를 간소화하기 위해 우리는 LLM(Large Language Models)을 실험해오고 있습니다. 언어와 맥락에 대한 깊은 이해력을 갖춘 LLM은 지식 그래프 생성 프로세스의 중요한 부분을 자동화할 수 있습니다. 이 모델들은 텍스트 데이터를 분석하여 엔티티를 식별하고, 그들의 관계를 이해하며, 어떻게 그들을 최상의 그래프 구조로 표현할 지 제안할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 실험의 결과로 LangChain에 그래프 구성 모듈의 첫 번째 버전을 추가했습니다. 이 블로그 게시물에서 이를 시연할 예정입니다.\n\n해당 코드는 GitHub에서 확인할 수 있습니다.\n\n## Neo4j 환경 설정\n\nNeo4j 인스턴스를 설정해야 합니다. 이 블로그 게시물의 예제를 따라 진행하십시오. 가장 쉬운 방법은 Neo4j Aura에서 무료 인스턴스를 시작하는 것입니다. 이는 Neo4j 데이터베이스의 클라우드 인스턴스를 제공합니다. 다른 방법으로는 Neo4j 데스크톱 애플리케이션을 다운로드하고 로컬 데이터베이스 인스턴스를 생성하여 로컬 Neo4j 데이터베이스를 설정할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nos.environ[\"OPENAI_API_KEY\"] = \"sk-\"\nos.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\nos.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\nos.environ[\"NEO4J_PASSWORD\"] = \"password\"\n\ngraph = Neo4jGraph()\n```\n\n또한, 이 블로그 게시물에서 OpenAI 모델을 사용할 예정이므로 OpenAI 키를 제공해야 합니다.\n\n# 데이터 수집\n\n이 데모에서는 엘리자베스 1세의 위키백과 페이지를 사용할 것입니다. LangChain 로더를 사용하여 위키백과 문서를 손쉽게 가져와 분할할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\n# 위키백과 항목 읽기\r\nraw_documents = WikipediaLoader(query=\"Elizabeth I\").load()\r\n# 청킹 전략 정의\r\ntext_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\r\ndocuments = text_splitter.split_documents(raw_documents[:3)\r\n```\r\n\r\n이제 검색된 문서를 기반으로 그래프를 구성할 시간입니다. 이를 위해 그래프 데이터베이스에 지식 그래프를 구축하고 저장하는 데 큰 도움이 되는 LLMGraphTransformermodule을 구현했습니다.\r\n\r\n```js\r\nllm=ChatOpenAI(temperature=0, model_name=\"gpt-4-0125-preview\")\r\nllm_transformer = LLMGraphTransformer(llm=llm)\r\n\r\n# 그래프 데이터 추출\r\ngraph_documents = llm_transformer.convert_to_graph_documents(documents)\r\n# Neo4j에 저장\r\ngraph.add_graph_documents(\r\n  graph_documents, \r\n  baseEntityLabel=True, \r\n  include_source=True\r\n)\r\n```\r\n\r\n지식 그래프 생성 체인이 사용할 LLM을 정의할 수 있습니다. 현재 OpenAI 및 Mistral에서 함수 호출 모델만 지원하고 있지만, 향후 LLM 선택을 확장할 계획입니다. 이 예시에서는 최신 GPT-4을 사용합니다. 생성된 그래프의 품질은 사용하는 모델에 크게 의존하는 것을 주의해야 합니다. 이론적으로는 항상 능력 있는 모델을 사용하는 것이 좋습니다. LLM 그래프 트랜스포머는 그래프 문서를 반환하며, 이를 add_graph_documents 메서드를 통해 Neo4j로 가져올 수 있습니다. baseEntityLabel 매개변수는 각 노드에 추가적인 __Entity__ 라벨을 할당하여 색인화 및 쿼리 성능을 향상시킵니다. include_source 매개변수는 노드를 원본 문서에 연결하여 데이터 추적 및 문맥 이해를 용이하게 합니다.\r\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n네오4j 브라우저에서 생성된 그래프를 확인할 수 있습니다.\n\n![그래프](/assets/img/2024-06-20-EnhancingtheAccuracyofRAGApplicationsWithKnowledgeGraphs_2.png)\n\n이 이미지는 생성된 그래프의 일부분을 나타냅니다.\n\n# RAG를 위한 혼합 검색\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래프 생성 후, 우리는 RAG 애플리케이션을 위한 벡터 및 키워드 색인과 그래프 검색을 결합한 하이브리드 검색 접근 방식을 사용할 것입니다.\n\n![이미지](/assets/img/2024-06-20-EnhancingtheAccuracyofRAGApplicationsWithKnowledgeGraphs_3.png)\n\n이 다이어그램은 사용자가 질문을 제기하면 RAG 검색기로 이어지는 검색 프로세스를 보여줍니다. 이 검색기는 키워드 및 벡터 검색을 사용하여 구조화되지 않은 텍스트 데이터를 검색하고 이를 지식 그래프에서 수집한 정보와 결합합니다. Neo4j는 키워드 및 벡터 색인이 모두 있는 기능을 제공하기 때문에 단일 데이터베이스 시스템으로 세 가지 검색 옵션을 구현할 수 있습니다. 이러한 소스에서 수집된 데이터는 LLM(언어 모델)에 공급되어 최종 답변을 생성하고 전달합니다.\n\n## 구조화되지 않은 데이터 검색기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"Neo4jVector.from_existing_graph\" 메서드를 사용하여 문서에 키워드 및 벡터 검색을 추가할 수 있어요. 이 방법은 \"Document\"로 레이블이 지정된 노드를 대상으로 하는 하이브리드 검색 접근 방식을 위해 키워드 및 벡터 검색 인덱스를 구성합니다. 또한, 누락된 경우 텍스트 임베딩 값을 계산합니다.\n\n```js\nvector_index = Neo4jVector.from_existing_graph(\n    OpenAIEmbeddings(),\n    search_type=\"hybrid\",\n    node_label=\"Document\",\n    text_node_properties=[\"text\"],\n    embedding_node_property=\"embedding\"\n)\n```\n\n그런 다음에 유사성 검색 방법을 사용하여 벡터 인덱스를 호출할 수 있어요.\n\n## Graph Retriever\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한편, 그래프 검색을 구성하는 것은 좀 더 복잡하지만 더 많은 자유를 제공합니다. 이 예제에서는 전체 텍스트 색인을 사용하여 관련 노드를 식별하고 해당 직접 이웃들을 반환할 것입니다.\n\n![image](/assets/img/2024-06-20-EnhancingtheAccuracyofRAGApplicationsWithKnowledgeGraphs_4.png)\n\n그래프 검색기는 입력에서 관련 엔티티를 식별하여 시작합니다. 단순화를 위해, 우리는 LLM에 개인, 조직 및 위치를 식별하도록 지시합니다. 이를 달성하기 위해 새롭게 추가된 with_structured_output 방법을 사용한 LCEL을 사용할 것입니다.\n\n```js\n# 텍스트에서 엔터티 추출\nclass Entities(BaseModel):\n    \"\"\"엔터티에 대한 정보 식별.\"\"\"\n\n    names: List[str] = Field(\n        ...,\n        description=\"텍스트에 나타나는 모든 사람, 조직 또는 비즈니스 엔터티들\",\n    )\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"시스템\",\n            \"텍스트에서 조직 및 개인 엔터티를 추출 중입니다.\",\n        ),\n        (\n            \"사용자\",\n            \"다음 입력에서 정보를 추출하기 위해 주어진 형식을 사용하세요: {질문}\",\n        ),\n    ]\n)\n\nentity_chain = prompt | llm.with_structured_output(Entities)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n테스트해 봅시다:\n\n```js\nentity_chain.invoke({\"question\": \"Amelia Earhart가 어디에서 태어났나요?\"}).names\n# ['Amelia Earhart']\n```\n\n좋아요, 이제 질문에서 엔티티를 감지할 수 있게 되었으니, 전체 텍스트 인덱스를 사용하여 그들을 지식 그래프에 매핑해 봅시다. 먼저, 전체 텍스트 인덱스를 정의하고 약간의 철자 오류를 허용하는 전체 텍스트 쿼리를 생성하는 함수를 만들어야 합니다. 이 과정에 대해서는 자세히 다루지 않겠습니다.\n\n```js\ngraph.query(\n    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n\ndef generate_full_text_query(input: str) -\u003e str:\n    \"\"\"\n    주어진 입력 문자열에 대한 전체 텍스트 검색 쿼리를 생성합니다.\n\n    이 함수는 전체 텍스트 검색에 적합한 쿼리 문자열을 생성합니다.\n    입력 문자열을 단어로 분할하고 각 단어에 유사성 임계값(~2개의 변경된 문자)을 추가한 후, AND 연산자를 사용하여 결합합니다.\n    사용자 질문에서 엔티티를 데이터베이스 값에 매핑하는 데 유용하며, 일부 철자 오류를 허용합니다.\n    \"\"\"\n    full_text_query = \"\"\n    words = [el for el in remove_lucene_chars(input).split() if el]\n    for word in words[:-1]:\n        full_text_query += f\" {word}~2 AND\"\n    full_text_query += f\" {words[-1]}~2\"\n    return full_text_query.strip()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 모든 것을 함께 적용해 봅시다.\n\n```js\n# Fulltext index query\ndef structured_retriever(question: str) -\u003e str:\n    \"\"\"\n    질문에 언급된 엔터티의 인근 엔터티를 수집합니다.\n    \"\"\"\n    result = \"\"\n    entities = entity_chain.invoke({\"question\": question})\n    for entity in entities.names:\n        response = graph.query(\n            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n            YIELD node,score\n            CALL {\n              MATCH (node)-[r:!MENTIONS]-\u003e(neighbor)\n              RETURN node.id + ' - ' + type(r) + ' -\u003e ' + neighbor.id AS output\n              UNION\n              MATCH (node)\u003c-[r:!MENTIONS]-(neighbor)\n              RETURN neighbor.id + ' - ' + type(r) + ' -\u003e ' +  node.id AS output\n            }\n            RETURN output LIMIT 50\n            \"\"\",\n            {\"query\": generate_full_text_query(entity)},\n        )\n        result += \"\\n\".join([el['output'] for el in response])\n    return result\n```\n\nstructured_retriever 함수는 사용자 질문에서 엔터티를 감지하여 시작합니다. 그런 다음 감지된 엔터티를 반복하고 해당 노드의 인근을 검색하기 위해 Cypher 템플릿을 사용합니다. 이제 테스트해 봅시다!\n\n```js\nprint(structured_retriever(\"Who is Elizabeth I?\"))\n# Elizabeth I - BORN_ON -\u003e 7 September 1533\n# Elizabeth I - DIED_ON -\u003e 24 March 1603\n# Elizabeth I - TITLE_HELD_FROM -\u003e Queen Of England And Ireland\n# Elizabeth I - TITLE_HELD_UNTIL -\u003e 17 November 1558\n# Elizabeth I - MEMBER_OF -\u003e House Of Tudor\n# Elizabeth I - CHILD_OF -\u003e Henry Viii\n# 그리고 더 많은 정보가 출력됩니다...\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 최종 검색기\n\n처음에 언급했던대로, 우리는 비구조화 및 그래프 검색기를 결합하여 LLM에 전달할 최종 컨텍스트를 생성할 것입니다.\n\n```js\ndef retriever(question: str):\n    print(f\"검색 쿼리: {question}\")\n    structured_data = structured_retriever(question)\n    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n    final_data = f\"\"\"구조화된 데이터:\n{structured_data}\n비구조화된 데이터:\n{\"#문서 \". join(unstructured_data)}\n    \"\"\"\n    return final_data\n```\n\n우리는 Python을 다루고 있으므로, 간단하게 f-string을 사용하여 출력을 연결할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# RAG Chain 정의\n\nRAG의 검색 구성 요소를 성공적으로 구현했습니다. 이제, 통합된 하이브리드 검색기가 제공하는 컨텍스트를 활용하여 응답을 생성하는 프롬프트를 소개하여 RAG 체인의 구현을 완료합니다.\n\n```js\ntemplate = \"\"\"다음 컨텍스트만을 기반으로 질문에 답해보세요:\n{context}\n\n질문: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nchain = (\n    RunnableParallel(\n        {\n            \"context\": _search_query | retriever,\n            \"question\": RunnablePassthrough(),\n        }\n    )\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n```\n\n마지막으로, 하이브리드 RAG 구현을 테스트해볼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nchain.invoke({\"question\": \"엘리자베스 1세가 속한 집은 무엇인가요?\"})\n# 검색 쿼리: 엘리자베스 1세는 어떤 집에 속했나요?\n# '엘리자베스 1세는 Tudor 왕조에 속했습니다.'\n\n```\n\n또한 질의 재작성 기능을 적용하여 RAG 체인이 대화 형태에 적응하도록 했습니다. 후속 질문을 효율적으로 검색하기 위해 벡터 및 키워드 검색 방법을 사용하기 때문에 후속 질문을 재작성하여 검색 프로세스를 최적화해야 합니다.\n\n```js\nchain.invoke(\n    {\n        \"question\": \"언제 태어났나요?\",\n        \"chat_history\": [(\"엘리자베스 1세가 어떤 집에 속했나요?\", \"Tudor 왕조\")],\n    }\n)\n# 검색 쿼리: 엘리자베스 1세는 언제 태어났나요?\n# '엘리자베스 1세는 1533년 9월 7일에 태어났습니다.'\n\n```\n\n질문이 언제 태어났나요?가 처음에 언제 엘리자베스 1세가 태어났나요?로 재작성된 것을 관찰할 수 있습니다. 재작성된 쿼리가 관련 context를 검색하여 질문에 대답하는 데 사용되었습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# RAG 애플리케이션을 쉽게 개선하기\n\nLLMGraphTransformer의 도입으로 지식 그래프를 생성하는 과정이 더 매끄럽고 접근하기 쉬워졌습니다. 지식 그래프가 제공하는 심층성과 맥락으로 여러분의 RAG 애플리케이션을 향상시키려는 누구에게나 이제 보다 쉬워졌습니다. 이것은 시작에 불과하며 앞으로 많은 개선 사항이 계획되어 있습니다.\n\nLLMs로 그래프를 생성하는 데 대한 통찰, 제안 또는 질문이 있으시다면 망설이지 말고 연락해 주세요.\n\n코드는 GitHub에서 확인할 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-20-EnhancingtheAccuracyofRAGApplicationsWithKnowledgeGraphs_0.png"},"coverImage":"/assets/img/2024-06-20-EnhancingtheAccuracyofRAGApplicationsWithKnowledgeGraphs_0.png","tag":["Tech"],"readingTime":10},{"title":"컴퓨터 프로그래머이자 15년간 ChatGPT에 대해 전문적으로 썼어요 - 전문 작가들이 괜찮다는 법을 이야기해 드릴게요","description":"","date":"2024-06-20 18:32","slug":"2024-06-20-ImaComputerProgrammerandWroteProfessionallyaboutChatGPTfor15YearsHeresHowProWritersCanBeOkay","content":"\n\n![alt text](/assets/img/2024-06-20-ImaComputerProgrammerandWroteProfessionallyaboutChatGPTfor15YearsHeresHowProWritersCanBeOkay_0.png)\n\n2023년 1월에 ChatGPT가 공개 된 지 세 달 후에는 이미 기업들이 작가들을 불필요하다고 생각하고 있음을 알 수 있었습니다. 몇몇 클라이언트들이 사라졌는데, 그 중 아무도 ChatGPT를 사용 중이라고 말해 주지 않았습니다.\n\n또한 내가 작가로 일을 찾는 데 꾸준히 신뢰할 수 있는 방법 중 하나인 나의 차가운 아웃리치에 대한 직접적이고 모욕적인 댓글들도 있었어요. \"오, ChatGPT가 이제 그런 걸 할 수 있어.\"\n\n그 후로, 나는 내가 판매하는 프리랜싱 플랫폼에서 ChatGPT의 훌륭함에 대해 글을 쓰도록 제안을 받았습니다. 이 회사는 세계적으로 프리랜서 일을 타격할 것으로 예상되었던 \"AI-파멸\"에 대응하여 콘텐츠 전략을 강화하기로 결정했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그들의 전략 중 일부는 ChatGPT 및 다양한 GenAI 도구에 대해 수많은 기사를 쓰는 것이 포함되어 있었고, 이로 인해 이러한 주제에 대한 SEO 트래픽을 이용하여 방문자들을 자신들의 프리랜서 서비스로 유도했습니다.\n\n기술 작성에 특화된 제가 17년 동안 코더였기 때문에 전문가로서 특히 고급 기술 내용에 대한 주요 AI 작성자가 되었습니다.\n\n# 작가로서, 뭔가를 알고 있는 척하지 마세요\n\n2019년, 저는 프리랜서 작가로서 성공을 꿈꾸는 절박한 소설 작가였습니다. 저는 성공을 이루기 위해 무엇을 해야 하는지 배울 수 있도록 Writing Cooperative라는 출판물을 꾸준히 참고했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수익이 많은 작가들로부터 가장 흔히 본 조언 중 하나는 전문화하는 것이었습니다. 저는 $0.25/단어 이상을 받는 프로 작가들의 기사에서 반복해서 읽었습니다. 그 당시에는 사랑하는 일을 하면서 $0.25/단어를 받는다는 것이 꿈 같았습니다. 하지만 오늘날에는 당연한 수준입니다.\n\n나는 '테크 작가'가 되기를 의도한 적이 없었습니다. 어쩌다 보니 그 방향으로 빠져들었습니다. 사람들이 나의 글을 발견하면서 나의 기술적 배경도 알게 되었고, 나에게 기술 주제에 대해 글을 쓰라는 요청을 했습니다. 나는 그 주제들 중 많은 것에 익숙하지 않았지만, 나의 고객들은 기술 분야 사람들이 작문이 서툰 경향이 있다는 걸 알아주었기 때문에 나에게 투자해 주었습니다. 나는 그 간극을 메워주었고, 기술적인 개념을 비전문가들에게도 전달할 수 있는 기술적인 사람이었습니다.\n\n그러한 요청들은 나를 안드로이드 개발, 메타버스, 핀테크, 암호화폐(심오한 주제들이 실린 곳이었습니다), 다양한 SaaS 도구, 스타트업, 백엔드 기술, 여러 프로그래밍 언어(중에는 전에 들어본 적도 없는 것이 있었습니다) 등에 대해 글을 쓰게 이끌었습니다.\n\n또한 AI에 대해 글을 써보게 되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n고스트라이터로 일찍 배운 것은 뭔가를 알고 있다고 속이는 것은 좋지 않은 글을 쓰도록 이끈다는 것이었습니다. 전문가가 글을 쓰면 사람들은 글쓰기보다는 전문성을 더 감상합니다. 고스트라이터가 하는 주요 실수 중 하나는 그들의 화려한 문장이 정확도보다 중요하다고 생각하는 것입니다. 하지만 그렇지 않습니다.\n\n테크니컬 라이터로써, 정확성은 당신의 생명줄입니다.\n\n고객들이 종종 내가 하는 말 중 하나는, \"나는 글을 쓸 때 뭔가를 알고 있다고 속이지 않으므로 멍청한 질문을 많이 할 거야. 당신이 전문가입니다. 난 그저 작가일 뿐이죠\" 라는 것입니다.\n\n사람들은 그것을 존중합니다. 이 태도로 몇 년 동안 일을 유지해 왔습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# AI의 내부를 배우기\n\n같은 태도는 나를 AI의 내부로 이끌었어요 — 현재 AI로 간주되는 것이 무엇인지에 대해 모두가 동의하지 않기 때문에.\n\n테크니컬 라이터로, 저는 무언가의 섬세하고 궁극적인 부분, 그것이 어떻게 작동하는지, 그리고 왜 그것이 작동하는지를 알고 싶어해요. 누군가가 GPT에 대해 쓰라고 부탁했을 때, 제가 스스로에게 묻는 첫 번째 질문은 \"GPT가 무엇을 의미하는지?\" 였어요.\n\n그것이 AI, 머신 러닝, 그리고 모델 아키텍처에 대해 배우는 멋진 토끼굴로 이끈거든요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChatGPT이 \"작동\"하는 방식을 완전히 이해하게 되어서, 그것의 응답에 대한 신비를 풀어주고, 이 기계가 이유할 능력이 전혀 없다는 것이 밝혀졌어요. 그렇죠.\n\n여기에 전체 논문에 대해 자세히 다루진 않겠습니다(어차피 자격이 없어서요), 그리고 아래 정의들은 간결함을 위해 어느 정도 부정확할 수 있습니다.\n\nGPT란 다음을 의미합니다:\n\n- Generative: 것을 \"생성\"합니다.\n- Pretrained: 야생으로 방출되기 전에 데이터에서 패턴을 찾아 \"학습\"하는 \"훈련\" 과정을 거칩니다. 이 \"사전 훈련\" 이후, 수많은(수백만?) 답변 중에서 사람들이 \"올바른\" 답변인지 틀린 답변인지를 \"미세 조정\"합니다. 전체 교육 과정은 답변에 \"가중치\"를 더하여, 시스템이 통계 분석을 기반으로 더 정확하게 올바른 답변을 예측할 수 있도록 합니다. 다시 말해, 만약 사람이 90%의 정확도로 한 답변을 올바르다고 표시했다면, 시스템은 그것이 \"정답\"이라 \"배웠다\"는 것입니다. 만약 사전 훈련 데이터가 90%의 시간을 같은 부정확성을 가졌다면, AI 시스템은 그 답변을 \"정답\"으로 고려할 것입니다. 바보 같죠?\n- Transformer: 시스템이 데이터를 순차적으로 처리하는 대신 병렬로 처리할 수 있는 AI 아키텍처를 가리킵니다. (Transformer 아키텍처는 구글 엔지니어들에 의해 만들어진 시대의 획기였는데 지금 보고 있는 금 러시로 이끌었습니다.) 예를 들어, \"bat\"이라는 단어는 포유류나 막대기를 의미할 수 있습니다. \"The bat was hungry\"라는 맥락에서 \"bat\"이 \"포유류\"라는 것을 추론할 수 있습니다. Transformer 아키텍처는 데이터를 동시에(\"병렬로\") 처리하여, 순차적으로 처리할 때보다 훨씬 더 맥락을 추출할 수 있습니다. 이것이 Transformer 아키텍처가 (그리고 ChatGPT뿐만 아니라 다른 Transformer 아키텍처들도 있습니다) 일반적으로 맥락을 올바르게 이해하는 이유입니다. (IBM Technology 유튜브 채널이 지금까지 발견한 것 중에 Transformer와 다른 AI 개념에 대한 가장 좋은 설명을 제공합니다.)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n앞서 말한 것만으로도 몇 가지 경고 신호를 눈치채셨을지도 모르겠어요. ChatGPT에 대해 더 많은 기사를 쓰면서 저는 느끼기 시작한 몇 가지를 여기서 설명해 드릴게요:\n\n- 모든 것은 확률 공식에 따라 작동합니다. 무수히 많은 데이터에 기반하여 \"사전 훈련된\" 시스템은 통계 분석을 사용하여 문장에서 다음으로 가장 가능성이 높은 단어를 결정합니다. 많은 사람들이 ChatGPT가 마치 감각이 있는 것처럼 느껴졌을 정도로 '사운드'가 믿음직스럽다고 생각했을 거에요. 하지만 사실은 수학적 공식을 바탕으로 동작하고 있는 거예요. 여기서 경고 신호는 바로 이겁니다. 아무것도 진정한 창작물이 아니다. 전부 유도물이에요. (그리고, 말그대로, 대규모 언어 모델의 '채팅' 버전은 더 '수다스러운' 소리를 내도록 특별히 훈련되었습니다. 이 모든 것은 매우 교묘하고 매우 신선하다.)\n\n- 이러한 시스템의 본능적 편견이 생성된 콘텐츠에서 나타나기 시작하는 데 오래 걸리지 않았어요. 이미지 생성 도구는 대개 다른 아키텍처를 사용하지만, 확률 공식의 개념은 비슷하며, 편향적인 심지를 일관되게 생성했고, 때로는 인종 차별적이고 성 차별적인 이미지를 생성하기도 했어요. 다시 한 번 강조하지만: 아무것도 창작된 게 없어요. 전부 유도물이죠.\n\n- OpenAI - ChatGPT 뒤에 있는 회사 - 는 열린 것이 아니라는 것을 드러냈어요. Microsoft는 최초 출시 이후 100억 달러를 투자하여 OpenAI에 깊이 뛰어들었습니다. 20년간의 경쟁 끝에 구글을 마침내 이길 것을 갈망하는 Microsoft의 절박함이 분명합니다. ChatGPT 출시 이후 Microsoft는 구글의 검색 엔진 시장 점유율을 정복할 것이라고 확신했어요. 그렇지는 않았죠.\n\n저는 Microsoft/Google의 경쟁을 여러 해간 지켜봤어요. ChatGPT 이후 Microsoft CEO 사티아 나델라의 행동은 사회의 이익보다는 개인적 복수심과 우월함에 기반한 충동적인 행동이라고 생각해요.\n\n요점은요: 폐쇄적인 회사가 사람들을 직접 훈련하여 언어 모델을 만든 다음, 그 언어 모델이 통계적으로 가능성 높은 방식으로 답변을 건진다는 점입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n변환된 표를 Markdown 형식으로 바꿔주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 작가들이 이해해야 할 중요한 측면은 AI가 사고할 수없다는 것입니다. 이것을 이해해야 합니다. AI는 생각하지 않습니다. 데이터셋과 \"미세 조정\"에 기반한 통계적으로 가능성이 높은 답변을 반복합니다.\n\n우리가 ChatGPT에 공급한 방대한 양의 데이터는 대부분의 시간 정확해 보이게끔 만듭니다. 이 방대한 양의 데이터를 분석하는 것은 실제로 유용한 진전이며 그에는 활용 방법이 있습니다.\n\n# 잘못 정의된 단어가 혹평을 일으키기도 합니다\n\nAI와 ChatGPT를 이해하기 위해 하는 작업 중, AI를 높은 위치에 두고 있는, 그러나 부정확한 단어를 반복해서 만나게 되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n“신경망”과 “뉴런”: 이 단어들은 조심스럽게 다뤄야 하는 가장 위험한 용어 중 하나입니다. 왜냐하면 인공지능을 인간 사고와 관련된 것으로 위치시킨다는 점에서 그렇습니다 — 하지만 그렇지 않습니다.\n\n옥스퍼드는 “신경망”을 “인간 뇌와 신경계를 모방한 컴퓨터 시스템”으로 잘못 정의하고 있습니다.\n\n이 정의는 놀라울 정도로 어렵습니다. 왜냐하면 뇌에 대해 전문가들조차 뇌가 어떻게 작동하는지에 대해 의견이 분분합니다. 빠른 구글 검색만으로도 인정받는 전문가들 간에 얼마나 많은 논란이 있는지 쉽게 확인할 수 있습니다. 신경과학자, 심리학자, 정신과의사들 사이에 상반되는 의견을 찾을 수 있습니다.\n\n만약 뇌를 연구하는 사람들이 인간 뇌가 어떻게 작동하는지 명확하고 확실하게 설명할 수 없다면, 대기업 기술 회사들은 어떻게 할 수 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n안녕하세요! 번역해 드리겠습니다.\n\n그것은 불가능합니다.\n\n하지만 이해해야 했습니다. 그래서 더 심층적으로 파고들어가 보니, 다른 정의와 부딪혔습니다. 신경망은 \"뉴런(neurons)\"으로 구성되어 있다고 합니다. 놀랍게도 이 \"뉴런(neurons)\"은 \"뇌 속 뉴런을 모방한 것\"이었습니다.\n\n토끼굴에서 벗어나는 데 얼마나 오랜 시간이 걸렸는지 모르겠습니다. 하지만 마침내 벗어났습니다.\n\n저는 기술에 흥미를 갖는 사람입니다. 회로 기판을 분해하며 자랐습니다. 누군가 기계에 대해 이야기할 때, 제게는 그것을 어떻게 만들 것인지 알고 싶습니다. 이러한 정의들은 어떻게 신경망을 만들지에 대해 말해 주지 않았습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은 연구 끝에 제가 개발한 정의는 다음과 같습니다:\n\n- 컴퓨터 과학에서의 뉴런은 여러 개의 가중치 입력을 처리하여 비선형 변환을 통해 하나의 출력을 생성하는 계산 단위입니다.\n\n- 비선형 변환이란 출력이 입력의 변화에 직접 비례하지 않고, 입력 매개변수가 변경될 때 출력의 복잡한 수정이 가능한 것을 의미합니다.\n\n- 신경망은 상호 연결된 인공 뉴런으로 구성된 계산 모델로, 각 뉴런은 여러 개의 가중치 입력을 처리하여 비선형 변환을 통해 하나의 출력을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그게 더 좋아 보이죠? 실행 가능하니까요. 한정된 내용이고요. 수수께끼가 없어요. \n\n그리고 사실적입니다.\n\nAI와 ML에 관한 글을 쓸 때 원래 위의 정의들을 사용했었지만 미래에 다른 사람들에 의해 수정될 수 있으니 위와 같이 붙여 놓았어요.\n\n신경망과 뉴런을 그들의 계산적 의미로 정의하면 그들을 이상한, 페라오의 의미에서 벗어나게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI는 가중 입력을 처리하고 단일 출력을 생성하는 도구인 'AI'를 올바른 위치에 배치합니다.\n\n\"가중 입력\"에 대해 말하자면, 가중치는 사전 훈련, 편향 등에서 나옵니다.\n\n# '환각'은 환각이 아닙니다. 버그입니다.\n\n'환각'이라는 용어가 잘못 정의되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGenAI의 환각 성향은 잘 알려져 있습니다. 그 환각은 법정 문서에서 가짜 사례를 사용해 벌금을 부과받은 변호사들이 여러 명 생겼습니다.\n\n\"환각\"이란 단어는 AI가 실제로 가진 창의력을 강조하기 위한 것입니다.\n\n환각은 환각이 아닙니다. 그것들은 소프트웨어 버그입니다. 그리고 그것들은 고칠 수 없습니다. 그 이유는 다음과 같습니다:\n\n소프트웨어를 작성할 때, 개발자들은 일반적으로 \"제어 구조\"라고 하는 일련의 프로세스를 논리적으로 따릅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n만약 입력값이 \"1\"이면\n  ' 사용자가 \"1\"을 입력한 경우에만 이 코드가 실행됩니다.\n  Print(\"1을 입력했습니다\")\n그렇지 않으면\n  ' 사용자가 \"1\" 이외의 다른 값을 입력한 경우에 이 코드가 실행됩니다.\n  Print(\"1을 입력하지 않았습니다\")\n끝\n```\n\n위 예제에서 \"if...then...else\" 구조는 프로그램의 흐름을 제어하기 때문에 \"제어 구조\"라고 합니다. 누군가 \"1\"을 입력하면 첫 번째 코드 섹션이 실행됩니다. 다른 값을 입력하면 두 번째 섹션이 실행됩니다.\n\n휴대폰과 컴퓨터에 설치된 멋진 앱들의 기반이 되는 다른 제어 구조들도 있습니다.\n\n다층 신경망은 다르게 작동합니다. 제가 너무 단순하게 설명했다면 죄송합니다. 프로그래밍에 대한 논문을 쓰는 것이 목적이 아니라 고수준 개념을 설명하여 이해하는 데 도움을 주는 것이 목표입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n생성적 AI에서는 무수히 많은 입력이 동시에 처리되기 때문에 신경망 내부에서 무슨 일이 일어나고 있는지의 흐름을 포착하는 것이 불가능합니다. 이를 늦추면 AI의 기본 구조가 맥락을 포착하지 못하게 되어 정확한 답변을 제공하는 능력이 감소합니다.\n\n\"한꺼번에 일어나는 모든 것\"이라는 문제로, 아무도 AI의 내부를 해체해서 고쳐야 하는 부분을 파악할 수 없습니다.\n\n왜 고장났는지 아무도 모릅니다. 그리고 아무도 디버깅할 수 없습니다.\n\n널리 알려진 대로, 생성적 AI는 재정과 같이 높은 정확도를 요구하는 상황에서는 전혀 사용할 수 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 정말? 그가 진짜 마법이라고 말했다고요? 어머나요.\n\n# 정확도가 필요한 작업에서는 ChatGPT를 피하십시오 — 특히 수학 관련 작업\n\n비코더들을 당황하게 만드는 또 다른 주요 개념은 언어 모델과 수학입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대형 언어 모델은 수학 계산을 수행할 수 없어요. 물리적으로 불가능한 일인데, 그 이유는 컴퓨터의 특정 부분에 입력 값을 연산할 수 있는 곳으로 보내지 않기 때문이에요.\n\n컴퓨터에서 \"숫자\"와 \"텍스트\"는 핵심 개념이에요. 컴퓨터 프로세서에는 수학 계산을 처리하는 특정 섹션이 존재하는데, 그것이 산술 논리 장치(ALU)라고 불려요.\n\nOpenAI의 대표 Greg Brockman이 ChatGPT 4를 시연할 때, \"TaxGPT\"라는 귀여운 예제를 보여줬어요. 그는 누군가의 세금 계산을 하도록 요청하고, 이후 \"계산기에 연결되어 있지 않다\"며 \"머리 계산\"을 하고 있는 것이라고 언급했어요.\n\n실례지만 - 뭐라구요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n앞서 언급한 대로, ChatGPT는 문장에서 다음 단어를 예측합니다. 이 단어들을 이해하지는 않습니다. ChatGPT는 1, \"1\" 또는 \"one\" 사이의 차이를 이해하지 못합니다. 만약 인터넷에 \"1 + 1 = 2\"와 같은 패턴을 따르는 충분한 텍스트가 있다면, ChatGPT는 \"1 + 1 =\"으로 시작하는 텍스트 라인 끝에 \"2\"를 출력할 가능성이 높습니다.\n\n아마도.\n\n확실하지는 않습니다.\n\n\"정신적인 계산\"은 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n계산기가 없어요.\n\n여러분, 세금 계산에는 챗지피티(ChatGPT)가 아니라 회계 도구를 사용해주세요. 세금 당국은 챗지피티 데모를 믿었다고 해도 관대해주지 않을 거에요.\n\n# 더 나은 프롬프트는 아무런 영향을 미치지 않았어요 — 결국 챗지피티 사용을 그만뒀어요\n\n챗지피티에 대해 기사를 쓰는 일을 받았던 때에는 대략적인 구상을 만들거나 질문에 답변하는 데 널리 사용했었어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n초반에는 열정 가득했지만, 그 결과물을 거의 사용하지 않게 되었다. 사용했을 때도 많이 편집했다.\n\n내가 글을 쓸 때 프로그래밍 주제를 배우는 데 도움이 되었다. 그래도 그 결과물을 확인하고, 가끔 비현실적인 부분이 있어서, 직접 튜토리얼을 참고하느니 ChatGPT를 사용하는 것이 시간을 절약할 수 있는지 의문이 들었다.\n\n전문 작가로서 나의 작업에 부정확함이 들어갈 수 없었다. 그래서 ChatGPT가 생성한 모든 것을 구글링했다. 그것은 종종 틀렸다.\n\n전문가들의 조언을 따라 \"더 나은 프롬프트 만들기\"를 시도했다. JSON(JavaScript Object Notation)이라 불리는 \"프로그래밍 스타일\" 언어를 사용한 프롬프트를 작성하기도 했는데, ChatGPT가 이해하기 쉬울 것이라 생각했다. (JSON은 텍스트를 구조화된 방식으로 작성할 수 있게 해주어 프로그래밍 작업에 더 적합하다.)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n변동이 거의 없었네요.\n\n어떤 경우에도 ChatGPT는 저가 누구나 제한을 넘을 수 없었어요.\n\n결국, 덜 사용하기 시작했고, 몇 달 전에 프로 구독을 취소하고 나서부터 거의 사용하지 않았어요.\n\n한편으로 Claude.ai를 발견했는데, OpenAI보다 윤리적인 면에서 높게 평가되는 회사가 만들었어요. 그럼에도 결국에는 동일하게 실망스러웠고, 저도 그 도구를 거의 사용하지 않게 되었어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# ChatGPT은 비작가들에게 유용하며, Canva와 CapCut은 비디자이너들에게 유용합니다.\n\n만약 ChatGPT의 글쓰기가 너무 형편없었다면 — 정말 그랬습니다 — 제게는 이 질문이 들떴어요, \"왜 글쓰기를 판매하기가 더 어려운 느낌일까?\" 2023년은 비즈니스 측면에서는 그리 나쁘지 않았습니다 — 대부분은 AI에 대해 쓰기로 그만큼 많은 돈을 받았기 때문이죠 (이것도 아이러니하죠) — 그러나 정말로 몇몇 클라이언트를 잃었고, 새로운 클라이언트를 확보하는 것이 어려워진 느낌이었습니다.\n\n내 자신을 클라이언트의 입장에 두고 몇 가지를 깨달았습니다. 스스로에게 물었죠, \"SEO 기사를 위해 글쓰기 작업에 $250을 지불할 의사가 있을까?\"\n\n그 대답은 명확했습니다. 아니에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그것은 깨워주는 순간이었어요. 내가 내 작업에 대해 지불하길 원하지 않았다면, 내 고객들은 왜 할 것이라고 생각할까?\n\n답은 간단했어요: 돈을 지불한 가치.\n\nChatGPT는 지루하고 단조로운 반복적이고 지루한 글쓰기를 테이블에서 제외했어요. 그리고 많은 작가들 - 나 자신 포함 - 그런 종류의 콘텐츠를 작성하고, 아마도 과금하고 있었을 수도 있어요.\n\n많은 회사들이 필사적으로 만들고 싶어하는 “SEO 목적 콘텐츠”들은 바로 ChatGPT가 제공하는 것이에요. 그리고 이건 무료로 제공돼요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아무도 그런 것을 읽는 것을 좋아하지 않는다고 생각해요. 하지만 Google 순위를 성스럽게 여기는 절박한 많은 CEO들과 대화를 나눈 적이 있어요. 그들은 그 순위를 위해 모든 것을 쓸 것이라고 말해요. 그들에게는 트래픽을 가져다줄 뿐인 기사 내용이 무엇이든 상관이 없어요.\n\n사실, 클라이언트 중 한 명이 그렇게 말한 적이 있어요. (그들은 아직도 ChatGPT가 아닌 저에게 글쓰기 요금을 지불하고 있어요.)\n\n\"SEO 목적용 콘텐츠\"는 또한 낮게 매달린 열매예요. 그것을 원하는 기업들은 사고리더십에 크게 신경을 쓰지 않으며 콘텐츠에 많은 돈을 지불하고 싶어하지 않아요.\n\n저에게 글쓰기 요금을 지불하는 일에 대해서는 — SEO 콘텐츠로 통하는 쓰레기에 대해 $250를 지불하지 않을 테지만, 제 분야에서 리더로서의 위치를 강조하고 비즈니스 문을 여는 기사라면 두 배나 네 배의 금액을 지불할 의향이 충분해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다양한 품질의 콘텐츠입니다.\n\n나는 ChatGPT를 사랑하는 Canva와 CapCut 두 도구를 비교합니다. 나는 프로 디자이너가 아니며 확실히 비디오 편집자도 아닙니다(아직은). 이러한 도구들은 나의 소셜 미디어 프로필을 위해 \"냄새나지 않는\" 빠르고 조잡한 콘텐츠를 만들 수 있게 해줍니다— 그리고 돈을 크게 쓰지 않아도 됩니다. 나는 내 브랜드를 구축하는 초기 단계에 있습니다. 나에게는 정량이 정직합니다. 매일 비디오를 만들면 비디오 편집이 비십니다.\n\n그래서 나는 Canva를 그래픽용으로, CapCut을 비디오 편집용으로 사용합니다.\n\n프로 디자이너와 비디오 편집자는 아마도 내 소셜 미디어 콘텐츠를 보고 싫어할 것입니다. 그러나 그것은 우리 모두가 공존할 수 있는 행복한 중간지대입니다. 언젠가 부유하고 돈이 많이 번 다면, 누군가에게 일을 맡기겠지요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금은 프로급 디자인 및 비디오 편집 작업을 제공하는 사람들의 올바른 타깃 시장이 아닙니다.\n\n# 혹평은 문제가 아니에요 — 문장의 퀄리티만큼 중요해요\n\n저급한, 대량생산, 대량콘텐츠를 제공하는 클라이언트들을 살릴 수 없어요. 솔직히 그런 고객들과 관련된 일을 하는 게 귀찮아요.\n\n그런 종류의 글을 쓰는 게 싫고, 제가 하는 글은 훨씬 더 즐겁답니다. 퀄리티가 훨씬 높아서 도전적이지만 보람도 커요. 그리고 더 많이 벌어들일 수 있죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"나는 '싼 가격' 경쟁을 그만뒀어. 그건 그냥 맨 밑바닥으로 달려가는 경주라고 생각해.\n\n최고의 고객은 이상적이지만 더 어렵게 찾을 수 있어. 그러나 그런 고객은 존재해. 그런 고객을 만나기 위한 방법은 자신의 품질을 면밀히 살펴보고, 탁월하지 않다면 개선하는 것이다.\n\n그런 다음 사이에 있는 사람들 — 중상위 고객들이 있어: 그들은 괜찮은 콘텐츠를 존중하지만 여전히 ChatGPT가 그들을 위해 만들어 줄 수 있다고 바라는 사람들이다. 왜냐면 사람들의 후기로 그렇게 들었기 때문이야.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n과장된 혹평으로 인해 잠재적 고객에게 제품을 판매하기 어려워졌어요. 하지만 매출 사이클이 어렵게 되는 건 이 과장이라는 것을 알아야 해요. 품질 때문은 아니에요. 만약 매출 사이클을 개선한다면 여전히 중간 계층 고객을 끌어들일 수 있는 기회가 있어요.\n\n네, 이 새로운 생태계에서 우리는 더욱 열심히 팔아야 합니다.\n\nAI가 전문 작가의 작품과 비슷할 리 없다는 걸 확신할 수 있어요. 저는 이것에 한푼의 의심도 없어요. 이걸 확실히 믿게 되는 데는 1년 반이 걸렸죠. 확신을 갖게 되면, 제 판매 능력도 향상되었어요. 누군가 앞에 서서 \"챗지피티로는 그 퀄리티를 얻을 수 없어요\" 혹은 \"챗지피티 콘텐츠로 브랜드를 훼손하게 될 거라고 절대적으로 확신해요\" 라고 확신을 갖고 전할 수 있게 되었거든요.\n\n다른 사람들에게 이것들을 설득하기 위해선 이런 사실에 대해 개인적 확신을 얻어야 했어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 목표는 예술가와 작가들을 위해 힘을 실어주는 것입니다. 매일 올라오는 격려가 담긴 콘텐츠를 받고 싶다면, 즐겨 사용하는 소셜 미디어 채널에서 저를 팔로우해주세요.","ogImage":{"url":"/assets/img/2024-06-20-ImaComputerProgrammerandWroteProfessionallyaboutChatGPTfor15YearsHeresHowProWritersCanBeOkay_0.png"},"coverImage":"/assets/img/2024-06-20-ImaComputerProgrammerandWroteProfessionallyaboutChatGPTfor15YearsHeresHowProWritersCanBeOkay_0.png","tag":["Tech"],"readingTime":13},{"title":"합리적인 AI 프롬프트의 비밀 데이터와 함께 프롬프팅","description":"","date":"2024-06-20 18:31","slug":"2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData","content":"\n\n\u003cimg src=\"/assets/img/2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData_0.png\" /\u003e\n\n맞아요. 환각이나 가짜 증거는 더 이상 없어요. 이젠 순수한 사실들뿐이에요.\n\n그리고 제일 좋은 점은 무엇이냐면, 많이 변화시킬 필요가 없다는 거예요. 프롬프트하는 방식을 약간 바꾸는 것만으로 충분했어요.\n\n이게 뭔지 궁금하시다면, 이제 알아보도록 하죠. 함께 파헤쳐봐요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터 기반 프롬프팅\n\n여기서는 \"데이터를 활용한 프롬프팅\"이라고 부르겠어요. 이것에 대해 처음 들어보신다면, 기본 아이디어를 알려드리겠습니다:\n\n- 템플릿 사용: 당신의 작업을 설명하는 대신, ChatGPT에 응답 템플릿을 제공하세요.\n- 데이터 포함: 관련 데이터를 직접 프롬프트에 첨부하세요. URL, PDF 또는 간단한 텍스트일 수 있습니다.\n- 지시 추가: 연결된 데이터를 사용하여 AI에게 템플릿을 작성하도록 요청하세요.\n\n여기에 이 기술을 사용한 프롬프트의 예시가 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData_1.png\" /\u003e\n\n이제 우리가 기초를 알았으니까 이 프롬프트를 만드는 방법을 알아볼까요? (예를 들어 사용해서) 간단한 트윗을 학술 에세이로 바꾸려고 해요.\n\n## 1/ 데이터 찾기\n\n첫 번째 단계는 첫걸음을 내는 것입니다. 내 경우에는 데이터를 찾는 것이에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 블로그 게시물, PDF, 이미지 또는 ChatGPT에서 받아들일 수 있는 다른 형식일 수 있어요.\n\n나에게는 트윗이에요.\n\n## 2/ 데이터로 프롬프트를 제시해보세요\n\n다음 단계는 프롬프트를 작성하는 것이에요. 채워야 할 중요한 두 부분이 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 템플릿: 이것은 AI의 출력 형식을 안내합니다.\n- 데이터: 이것은 필요한 맥락과 제약을 제공합니다.\n\n원하는 경우 작업을 명확히 설명할 수 있습니다. 제 경우에는 트윗을 글로 변환하라고 언급했어요.\n\n![이미지](/assets/img/2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData_2.png)\n\n한 번 시도해보고 싶다면 전체 프롬프트는 여기에 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 텍스트를 친절하게 한국어로 번역해 드리겠습니다.\n\n\n| 파일 이름                                            | 설명                              |\n|-------------------------------------------------|--------------------------------|\n| 2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData_3.png | AIPrompt 데이터 사용 방법을 보여주는 이미지 |\n\n정말 멋지죠? 간단하면서도 마법처럼 잘 작동합니다.\n\n## 3/ 직접 해보기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 코드로 표를 마크다운 형식으로 바꿔보세요.\n\n즐거운 코딩 되세요!","ogImage":{"url":"/assets/img/2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData_0.png"},"coverImage":"/assets/img/2024-06-20-TheSecrettoFoolproofAIPromptsPromptingWithData_0.png","tag":["Tech"],"readingTime":2},{"title":"챗지피티 대규모 업데이트 엑셀 실시간 대화형 분석 누군가가 그 뒤에 있는 새로운 모델을 공개했습니다 ","description":"","date":"2024-06-20 18:30","slug":"2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt","content":"\n\nGPT-4o가 출시된 지 며칠 만에, OpenAI가 ChatGPT 내에서 실시간 대화형 데이터 분석을 가능케 하는 또 다른 혁신적인 기능을 발표했습니다.\n\n![이미지](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_0.png)\n\n이 업데이트를 통해 사용자들은 이제 ChatGPT에서 데이터 파일을 직접 열 수 있어서 분석을 위한 Python 코드 실행이 원활해졌습니다. 이는 대규모 데이터셋을 병합하고 차트를 생성하며 손쉽게 결론을 요약하는 것을 포함합니다.\n\n![이미지](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 발전은 데이터 분석가의 핵심 작업을 주로 다룹니다. 많은 인터넷 사용자들이 이 발전에 흥분했습니다:\n\n![이미지](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_2.png)\n\n# 주요 기능\n\n1️⃣ Google 드라이브 및 Microsoft OneDrive로부터 직접 파일 업로드:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChatGPT는 이제 Google 시트, 문서, 슬라이드, 그리고 Microsoft 엑셀, 워드, PPT와 같은 파일을 Google 드라이브와 OneDrive에서 직접 업로드할 수 있습니다. 이를 통해 파일을 다운로드하고 다시 업로드하여 분석할 필요가 없어졌습니다.\n\n2️⃣ 대화형 테이블 및 차트:\n\n파일을 업로드하면 ChatGPT가 대화식 테이블을 만듭니다. 확장 버튼을 클릭하면 새로운 대화식 페이지가 열립니다. 예를 들어, 데이터를 월별로 그룹화하려면 데이터를 선택하고 \"월별 그룹화\"라고 입력하면 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터를 반올림하려면 한 번의 클릭으로 조정합니다:\n\n![image1](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_4.png)\n\nChatGPT는 분석된 데이터에서 시각적 차트를 생성할 수도 있습니다:\n\n![image2](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기능은 매일의 작업 흐름에 완벽하게 통합되어 생산성과 정확성을 향상시킵니다. 일부 사용자는 AI 기반 데이터 분석이 AI 코드 개발을 능가할 수 있다고 믿는데, 이는 OpenAI가 효과적으로 활용하고 있는 감정입니다.\n\n3️⃣차트의 사용자 정의 및 다운로드:\n\n사용자는 ChatGPT가 생성한 차트를 사용자 정의하고 발표 자료 및 문서로 다운로드할 수 있습니다. 예를 들어 특정 월에 지출을 결합하거나 데이터를 국가 또는 지역별로 분류하는 것이 간단합니다:\n\n![image](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시각화를 작성할 때, ChatGPT는 전반적인 추세에 대한 텍스트 요약을 제공하며 사용자는 수정된 색상과 같은 인터페이스 내에서 차트를 직접 편집한 후 다운로드할 수 있습니다:\n\n![ChatGPT](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_7.png)\n\nOpenAI는 ChatGPT가 클라우드 차트에 액세스할 수 있지만 기업 사용자 데이터를 교육에 사용하지 않을 것을 확약합니다. ChatGPT Plus 사용자는 추가 보안을 위해 개인정보 보호 기능을 활성화할 수 있습니다.\n\n# 새로운 모델 인사이트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최신 기능들은 데이터 분석을 위해 맞춤화된 새로운 모델에 의해 구동되는 것으로 알려져 있습니다.\n\n![2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_8.png](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_8.png)\n\n일부 사용자들은 ChatGPT 페이지의 Alpha Models 섹션에서 ADA V2(GPT-4)라는 모델을 발견했습니다.\n\n![2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_9.png](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_9.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# OpenAI의 우세함과 산업 영향\n\nOpenAI는 계속해서 트렌드 세터 역할을 하고, 종종 다른 기술 거물들을 가려버리곤 합니다. 구글의 최신 릴리스가 I/O 컨퍼런스에서 발표되었음에도 불구하고, OpenAI의 혁신들이 주목을 받으며 이 회사들 간의 치열한 경쟁을 강조하고 있습니다.\n\n![이미지](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_10.png)\n\n한 OpenAI 직원이 심지어 구글을 조롱했는데, 트윗은 빠르게 삭제되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_11.png)\n\nRegardless, the rivalry benefits users with continuous advancements.\n\nIt’s been an exciting week for AI enthusiasts and professionals alike.\n\n![Image 2](/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_12.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n— by 公众号: 量子位\n\n# 참고 자료:\n\n[1] OpenAI Twitter\n\n[2] OpenAI ChatGPT 내 데이터 분석 개선\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[3] 트위터 사용자 @nanulled\n\n💡더 깊이 파고들고 싶나요? 내 ChatGPT 컬렉션이 여러분을 기다리고 있어요.\n\n## 기사가 마음에 드셨나요?\n\n그렇다면:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 댓글을 남겨주세요\n- 내 소식을 팔로우 해주세요\n- 무료 이메일 알림\n\n# 쉬운 영어로 🚀\n\nIn Plain English 커뮤니티의 일원이 되어 주셔서 감사합니다! 떠나시기 전에:\n\n- 꼭 박수와 작가를 팔로우해주세요 👏️️\n- 팔로우하기: X | LinkedIn | YouTube | Discord | Newsletter\n- 다른 플랫폼 방문하기: Stackademic | CoFeed | Venture | Cubed\n- 알고리즘 콘텐츠를 다루도록 강요하는 블로깅 플랫폼에 지친다면? Differ를 시도해보세요\n- PlainEnglish.io에서 더 많은 콘텐츠 확인하기","ogImage":{"url":"/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_0.png"},"coverImage":"/assets/img/2024-06-20-ChatGPTHugeUpdateReal-TimeInteractiveAnalysisofExcelSomeoneRevealstheNewModelBehindIt_0.png","tag":["Tech"],"readingTime":5},{"title":"언어 모델 보정 기법 확률 평가 향상하기","description":"","date":"2024-06-20 18:26","slug":"2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments","content":"\n\n\n![Calibration Techniques for Language Models](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_0.png)\n\n언어 모델, 특히 대형 언어 모델(LLMs)은 인간과 유사한 언어를 이해하고 생성하는 능력으로 인공지능 분야를 혁신했습니다. 이러한 모델은 제로샷 설정에서 다양한 작업을 수행할 뿐만 아니라 맞춤식 프롬프트를 통해 놀라운 유연성과 다양성을 바탕으로 여러 도메인에서 탁월하게 유용합니다.\n\n그러나 그들의 효과적인 성능에도 불구하고, 이 모델들의 교정(calibration)은 종종 도전이 되는 핵심적인 측면 중 하나입니다 — 즉, 다양한 결과에 대한 확률이 그 결과가 정확할 가능성을 정확히 반영하는지 보장하는 것입니다.\n\n본 문서에서는 LLMs의 교정이 필요한 이유를 탐구하고, 그들의 확률 평가를 둘러싼 핵심 문제를 식별하며, 더 나은 모델 교정을 위한 현대적인 방법을 탐구합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## LLM 모델에서 교정이 왜 중요한 이유\n\nLLM의 본질은 정확성뿐만 아니라 올바른 신뢰 수준이 할당된 언어 기반 출력을 처리하고 생성하는 데 있습니다. 교정 — 즉 모델의 신뢰 수준을 정확성과 일치시키는 과정 — 는 다음과 같은 이유로 필수적입니다:\n\n- 신뢰할 수 있는 AI 의사 결정: 적절히 교정된 신뢰 점수는 사용자들이 AI가 내리는 결정을 믿고 의지할 수 있게 하며, 모델이 올바르거나 잘못될 가능성을 이해할 수 있게 합니다.\n\n- 위험 관리: 의료 진단이나 자율 주행과 같은 안전 중요 응용 프로그램에서 자신감이 넘치지만 부정확한 예측은 재앙적 결과로 이어질 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 모델 디버깅 및 개선: 보정은 개발자가 모델의 약점을 이해하고 이에 맞게 개선하는 데 도움이 될 수 있습니다.\n\n## LLM 확률 보정에서의 어려움\n\n대형 언어 모델은 종종 확률 보정에 영향을 미치는 여러 가지 어려움에 직면합니다:\n\n- 닫힌 모델 제한: 많은 대형 언어 모델은 로그 확률에 직접 접근할 수 있는 제한된 액세스를 가진 블랙 박스로 작동하여 확신도를 이해하고 조정하는 프로세스를 복잡하게 만듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 훈련 중의 불일치: Reinforcement Learning from Human Feedback (RLHF)와 같은 기술로 개선된 모델들은 어설프게 miscalibrated 될 수 있습니다. 논문 [1]에 따르면 가장 널리 사용되는 LLM들은 인간 피드백으로 강화학습된 모델들입니다 (RLHF-LLMs). 일부 연구에서는 RLHF-LLMs가 매우 잘 보정되지 않은 조건부 확률을 생성한다고 제안했습니다. 연구 결과는 RLHF-LLMs가 사용자 선호도에 근접하게 따라가기를 우선시하는 경향이 있어 잘 보정된 예측 생성보다는 낮은 보정을 낸다는 것을 보여줍니다. 이는 RLHF로 훈련된 모델들이 정확하고 신뢰할 수 있는 출력을 위해 필요한 확률 보정을 갖추지 못할 수 있는 주요 도전을 보여줍니다.\n\n- 작업별 보정 필요성: LLM의 일반적인 훈련은 일반적으로 특정 작업이나 도메인에 대해 조정되지 않았기 때문에, 특정 요구사항이나 응용 프로그램과 조율되도록 하기 위해 추가적인 보정이 필요합니다.\n\n## LLMs를 위한 고급 보정 방법\n\n보정 도전에 대응하기 위해 아래에서 논의된 여러 기술을 시도해볼 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_1.png)\n\n## Verbalized Confidence\n\n“Verbalized Confidence” refers to techniques where a Language Model (LLM) not only provides answers but also rates its confidence in its response explicitly. This approach involves the use of certain methodologies to obtain more reliable assessments of the model’s confidence in its answers.\n\n## Basic Implementation\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 간단한 형태로 표현된 신뢰 표현은 LLM에게 질문과 관련 있는 맥락을 제시한 후 명시적으로 신뢰 점수를 요청하는 것입니다. 이 직접적인 접근은 더 정교한 기술의 기반 역할을 합니다.\n\n## 향상된 신뢰 표현 기술\n\n- Chain-of-Thought (CoT) Prompting: CoT Prompting은 답변을 제공하기 전에 모델로부터 단계별 추론 과정을 유도하는 것을 포함합니다. 이 방법은 모델의 답변의 명확성과 풍부성을 향상시킬 뿐만 아니라 추론 단계에서 논리적 일관성을 관찰하여 신뢰 수준을 더 정확히 추정할 수 있습니다.\n- 다단계 신뢰 유도: 이 기술은 추론 또는 문제 해결 과정의 여러 단계에서 신뢰 점수를 캡처하여 신뢰 측정을 개선합니다. 최종 신뢰 수준은 개별 신뢰 점수의 곱으로 파생되어 확신의 합성 측정 값을 제공합니다.\n- 상위-K 답변 및 신뢰 점수: 하나의 답변 대신 모델이 여러 가능한 답변(상위-K 답변)을 생성하며 각각 개별적인 신뢰 점수와 함께 제시됩니다. 가장 높은 신뢰 점수를 가진 답변이 최종 답변으로 선택됩니다. 이 방법은 여러 가설을 평가하는 의사 결정 과정과 유사합니다.\n- 다양한 유도 기법: 여러 다양한 유도를 활용하면 신뢰 추정이 더 정확해질 수 있습니다. 다양한 유도는 서로 다른 어구, 맥락 또는 개념 각도에서 비롯될 수 있으며, 이는 모델의 평가를 편향된 또는 정보 부족으로부터 더 견고하게 만들어줍니다.\n- 숫자적 확률 대 언어 표현: 경우에 따라 모델이 정확성 가능성에 직접 연결된 숫자적 확률을 통해 자신의 신뢰를 표현합니다. 반대로, \"매우 가능성이 높음\" 또는 \"아마도 아님\"과 같은 언어적 표현도 사용될 수 있습니다.\n- 여러 가설과 함께 유도: 처음에 모델은 신뢰 등급 없이 여러 답변 후보를 생성합니다. 이후 상호 작용에서 각 답변의 정답 가능성을 평가합니다. 연구 결과는 이 방식으로 여러 가설을 평가하는 것이 극적으로 보정된다는 것을 나타냅니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_2.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델 간에 자신감을 효과적으로 표현하는 능력은 다양하며, 서로 다른 모델 아키텍처와 세대 간에 관찰되는 차이점이 있습니다.\n\n## 자기 일관성 기반 신뢰도\n\n자기 일관성 기반 신뢰도 방법은 동일한 쿼리에 대해 여러 응답을 생성하고 이러한 응답 사이의 일치를 분석함으로써 언어 모델의 신뢰도를 평가하는 복잡한 방법입니다. 이 기술은 다양한 조건에서 높은 일치가 나타날수록 응답의 정확도에 대한 높은 신뢰를 나타낸다는 아이디어에 기반합니다.\n\n여러 응답 생성하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델로부터 다양한 결과 스펙트럼을 얻기 위해 몇 가지 전략을 사용합니다:\n\n셀프-랜덤화: 이 방법은 동일한 질문을 다른 설정에서 여러 번 입력하는 것을 포함합니다. \"온도\" 매개변수를 조절하는 것이 일반적이며, 이는 모델의 응답 다양성을 다루기 위해 출력의 예측성 또는 랜덤성을 변화시킴으로써 작동합니다.\n\n프롬프트 왜곡: 질문의 요구사항을 다르게 해석할 수 있도록 문구를 변경하여 다양한 각도의 응답을 유도합니다. 이를 통해 모델의 강건성을 테스트하며, 문맥상 유사한 프롬프트지만 다르게 표현된 질문 사이에서 일관성을 유지하는지 확인합니다.\n\n잘못된 단서: 의도적인 오류나 오해를 유발하는 힌트를 프롬프트에 삽입하여 모델의 안정성을 평가합니다. 사람의 테스트와 유사하게, 이 방법은 모델이 자신감 있는 사람처럼 오해된 정보를 무시하고 정확하거나 일관된 응답을 유지할 수 있는지를 관찰합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n집계 전략\n\n결과를 종합하고 최종 신뢰 점수를 할당하기 위해 다양한 집계 전략을 고려할 수 있습니다:\n\n일관성 측정: 이는 모델이 다양한 조건에서 동일한 응답을 제공하는 정도를 검토하여 안정성과 신뢰성을 반영합니다.\n\n평균 신뢰도 (평균-신뢰도): 높은 일치도와 개별 신뢰 점수가 높은 답변에 더 많은 가중치를 부여하여 전반적인 신뢰도의 세밀한 측정을 제공하는 가중 평균이 계산됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n쌍-순위 전략: 모델의 상위-K 예측을 사용하는 시나리오에서 특히 유용한 이 전략은 모델 예측에서 순위 정보를 강조하여 가장 가능성이 높고 일관된 응답을 평가하는 데 도움이 됩니다.\n\n## 로짓 기반 접근 방식\n\n로짓 기반 캘리브레이션은 대형 언어 모델(Large Language Models, LLMs)이 하는 확률적 예측의 신뢰성을 향상시키는 중요한 기술입니다. 모델이 로그 확률과 같은 원시 점수를 출력할 때, 일반적으로 이러한 점수는 직접적으로 정확한 확률 분포로 변환되지 않습니다. 캘리브레이션 기술은 이러한 로짓을 조정하여 더 정확한 확률을 반영하며, 이것은 실제 응용 프로그램에서 강력한 의사 결정을 내리는 데 중요합니다. 아래에서는 로짓 기반 캘리브레이션에 사용되는 일부 방법에 대해 자세히 살펴봅니다:\n\n1. 토큰 간의 확신을 평균화하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n언어 모델의 예측에 대한 확신을 더 균일하게 추정하기 위한 일반적인 방법 중 하나는 토큰 간의 확신(로그 확률)을 평균화하는 것입니다. 이는 특정 응용 프로그램이나 데이터셋의 특성에 따라 모든 토큰 또는 선택적 하위 집합에 대해 수행될 수 있습니다. 결과는 모델의 확신에 대한 더 부드럽고 일반화된 측정으로, 어떤 단일 토큰의 변동성이 미치는 영향을 줄입니다.\n\n2. Platt 스케일링 (시그모이드)\n\nPlatt 스케일링 또는 시그모이드 보정은 원래 모델의 출력 로짓에 적용되는 로지스틱 회귀 모델입니다. 로짓 위에 시그모이드 함수를 피팅함으로써이 방법은 로짓을 보정된 확률로 변환합니다. 보정에는 일반적으로 'A'와 'B'로 표시되는 두 매개변수를 학습하는 과정이 포함됩니다. 이 매개변수는 로짓을 실제 관측된 확률과 보다 잘 일치시키기 위해 스케일링 및 이동시킵니다. 이 방법은 이진 분류 작업에 대한 단순성과 효과적임으로 인해 특히 유용합니다.\n\n![그림](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 등위 회귀\n\nPlatt 스케일링과는 다르게, 등위 회귀는 로짓과 확률 사이에 어떤 기능적 형태도 가정하지 않습니다. 이는 예측된 확률을 대상 확률과 일치시키기 위해 비감소 함수를 적합시키는 비모수적 접근 방식입니다. 이 조각별 상수 함수는 유연하며 로짓과 확률 사이의 관계가 더 복잡하거나 비선형적인 경우, 특히 일부 시나리오에서 실제 분포를 더 정확하게 반영할 수 있습니다.\n\n![image](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_4.png)\n\n4. 온도 조정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n온도 조정은 모델의 확신을 조정하는 사후 처리 기술로, 예측을 변경하지 않고 조절합니다. 소프트맥스 함수를 적용하기 전에 상수인 \"온도\"로 로짓을 나누는 방식으로 확률로 변환합니다. 최적의 온도는 보통 검증 데이터셋에서 교차 엔트로피 손실을 최소화하여 결정됩니다. 이 방법은 원래 로짓의 상대적 순서를 유지하면서 보정 프로세스에 미세한 영향을 미치므로 매력적입니다.\n\n# 대리 모델 또는 세부 조정 방법\n\n세부 조정은 특정 데이터와 목표를 사용하여 모델을 미세 조정하여 특정 작업에 보다 나은 준비를 시키는 고급 보정 접근 방식입니다. 더 신뢰할 수 있고 정확한 신뢰 점수를 제공하기 위해 모델을 미세 조정하는 여러 혁신적인 방법을 살펴보겠습니다.\n\n## 대리 모델을 활용한 신뢰 평가\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 가지 매력적인 방법은[2] 일반적으로 더 단순한 모델을 사용하여 주요 모델 (예: GPT-4)로부터 얻은 답변이 얼마나 믿을 만한지를 평가하는 것입니다:\n\n- 하는 일: 예를 들어, LLAMA2와 같은 보조 모델은 다른 모델인 GPT-4가 제공하는 답변의 대한 로그 확률을 얻기위해 동일한 프롬프트를 제공하고 GPT-4 모델 응답에 대한 점수를 추출할 때 사용될 수 있습니다.\n\n- 놀라운 효과성: 비록 보조 모델이 덜 강력할 수 있지만, 이 방법은 언어적 단서만 사용하는 것과 비교하여 더 나은 결과를 낳는 것으로 입증되었습니다(Area Under the Curve 또는 AUC를 기준으로 측정).\n\n## 불확실성 인식: R 튜닝\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nR-튜닝은 모델이 \"모르겠다\"라고 말해도 괜찮다는 것을 가르칩니다. 모델의 한계를 인식합니다. 파인튜닝 프로세스는 다음 단계로 구성됩니다.\n\n- 불확실성 식별: 모델의 답변이 흔들리거나 의문스러운 경우를 찾아내어 예측하고 실제 결과와 비교하여 학습 세트에서 찾습니다.\n\n- 확실히 훈련: 그런 다음 \"확실함\" 또는 \"불확실함\"으로 태그된 예제를 사용하여 모델을 가르치어 이러한 구별에서 배우도록 합니다. \"확실하다\" 또는 \"의심스럽다\"와 같은 구문을 사용하여 훈련 중에 자신감 수준을 표현하며, 토큰 생성부터 오류를 줄이는 데 집중합니다.\n\n![이미지](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## LITCAB: 작은 변화, 큰 영향\n\nLITCAB은 작지만 효과적인 보정 레이어를 소개합니다:\n\n- 간단한 추가: 모델 끝에 단일 선형 레이어를 추가하여 입력 텍스트에 따라 각 응답의 예측 확률을 조정합니다.\n\n- 효율적이고 효과적: 이 소규모 조정은 복잡성을 크게 늘리지 않고 모델의 판단력을 향상시킵니다. 원래 모델 크기 변화의 2% 미만만 변경됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## ASPIRE: 더 스마트한 모델 응답\n\n예측에 신뢰 점수를 할당하고 선택적 예측을 허용합니다. ASPIRE는 세 가지 단계로 구성됩니다:\n\n1. 작업별 튜닝: PEFT 기술을 사용하여 주요 모델을 변경하지 않으면서 특정 변환 가능한 매개변수를 수정하여 특정 작업에 대한 응답을 개선합니다.\n\n2. 답변 샘플링: 이러한 조정을 사용하여 각 질문에 대해 여러 잠재적인 답변을 생성하고 높은 가능성의 출력 시퀀스를 만들기 위해 빔 검색을 사용하며 생성된 출력 시퀀스가 참 값에 기반하여 올바른지 여부를 결정하기 위해 Rouge-L 메트릭을 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 자가평가 학습: 마지막으로, 모델이 자체적으로 답변이 맞거나 틀렸는지 판단하여 자체평가 능력을 향상시키는 다른 조절 세트를 소개합니다.\n\n![이미지](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_6.png)\n\n이러한 방법을 통해 언어 모델은 더 발전된 것뿐만 아니라 사용자의 맥락과 기대에 더 부합하게 되어, 더 신뢰성이 있고 문맥을 인식하는 상호작용을 이끌어냅니다.\n\n## 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대규모 언어 모델의 보정은 복잡하지만 중요한 작업으로, AI 응용 프로그램의 신뢰성과 안전성을 향상시킵니다. 위에서 논의한 다양한 혁신적인 방법을 사용하고 결합함으로써, 이러한 모델이 다양한 맥락에서 이해하고 상호 작용하는 방식을 혁신적으로 개선할 수 있습니다. 이는 높은 자신감과 정확성으로 결정을 내릴 수 있는 진정으로 지능적인 시스템을 위한 길을 열어줍니다.\n\n## 부록:\n\n- Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, 그리고 Christopher D. Manning이 제목을 붙인 “Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback,”\n- Llamas Know What GPTs Don’t Show: Surrogate Models for Confidence Estimation, Vaishnavi Shrivastava, Percy Liang, Ananya Kumar\n- R-Tuning: Instructing Large Language Models to Say ‘I Don’t Know’ Hanning Zhang♠∗, Shizhe Diao♠∗ 및 기타.\n- LITCAB: LIGHTWEIGHT LANGUAGE MODEL CALIBRATION OVER SHORT- AND LONG-FORM RESPONSES Xin Liu, Muhammad Khalifa, Lu Wang\n- Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement Gwenyth Portillo Wightman, Alexandra DeLucia 및 Mark Dredze\n- CAN LLMS EXPRESS THEIR UNCERTAINTY? AN EMPIRICAL EVALUATION OF CONFIDENCE ELICITATION IN LLMS Miao Xiong1∗, Zhiyuan Hu1, Xinyang Lu 및 기타.\n- ASPIRE 소개: LLMs에서 선택적 예측을 위한\n- Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers Moxin Li1, Wenjie Wang 및 기타.\n- 대규모 언어 모델로부터의 장문 생성 보정\nYukun Huang1, Yixin Liu 및 기타.\n\n![이미지](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 이야기는 Generative AI Publication에서 발행되었습니다.\n\n최신 AI 이야기를 따르려면 Substack, LinkedIn 및 Zeniteq에서 저희와 연락을 유지하세요. 함께 AI의 미래를 함께 만들어 보아요!\n\n![이미지](/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_8.png)","ogImage":{"url":"/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_0.png"},"coverImage":"/assets/img/2024-06-20-CalibrationTechniquesforLanguageModelsEnhancingProbabilityAssessments_0.png","tag":["Tech"],"readingTime":10},{"title":"AI 에이전트 능력 엔지니어링","description":"","date":"2024-06-20 18:23","slug":"2024-06-20-AIAgentCapabilitiesEngineering","content":"\n\n## 인공지능 에이전트를 위한 고수준 능력 엔지니어링 프레임워크 소개\n\n![이미지](/assets/img/2024-06-20-AIAgentCapabilitiesEngineering_0.png)\n\n## 소개\n\n최근 제가 발표한 '프롬프트 엔지니어링에서 에이전트 엔지니어링으로'라는 기사에서 나는 AI 에이전트 엔지니어링을 위한 프레임워크를 제안했습니다. 이 프레임워크는 AI 에이전트의 설계와 생성에 접근하기 위한 사고 모델을 제시합니다. 이 프레임워크는 다음 구조를 제안합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-AIAgentCapabilitiesEngineering_1.png\" /\u003e\n\n- AI 에이전트는 작업을 수행합니다.\n- 작업을 완료하려면 행동이 필요합니다.\n- 행동 수행에는 능력이 필요합니다.\n- 능력에는 필요한 숙련도 수준이 있습니다.\n- 필요한 숙련도 수준에는 기술 및 기법이 필요합니다.\n- 기술과 기법에는 오케스트레이션이 필요합니다.\n\n만약 해당 기사를 놓치셨거나 다시 참고하려면 여기서 찾을 수 있습니다.\n\n비록 직관적이지만, 보다 심오한 수준에서는 이 프레임워크가 포용하는 주제와 아이디어가 매우 방대합니다. 보다 포괄적인 프레임워크에서 제시된 개념을 탐구하는 것은 상당한 노력이 필요하며, 이 기사에서는 AI 에이전트 능력 엔지니어링 프레임워크에 초점을 맞추어 작업을 계속합니다. 이 프레임워크에 접근하는 방식은 주로 인지 및 행동 과학에 근간을 둔 개념을 확장하는 타분류적 마인드셋에 의존합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 인지 및 행동과학 기초\n\n다른 글에서 언급했듯이, 인간의 도구 및 기술 발전의 역사를 통해 우리는 자주 우리 자신을 빌려서 구축하고자 하는 대상이나 모델로 사용해왔습니다. AI에서의 하나의 사례는 인간 두뇌에서 영감을 받은 신경망입니다. AI 에이전트 기능에 대한 프레임워크를 구축하기 위해, 인지 및 행동과학에 대해 영감과 지침, 그리고 유용한 개념의 확장을 찾는 것은 자연스러운 일입니다. 먼저 이러한 과학이 무엇을 의미하는지 전반적인 이해를 해봅시다.\n\n인지 과학\n\n인지 과학은 마음과 그 과정의 다학제적인 연구로 심리학, 신경과학, 언어학, 그리고 인공지능과 같은 영역을 아우릅니다. 이는 인간이 어떻게 지각하고, 생각하며, 배우며, 기억하는지에 대한 중요한 통찰을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n행동 과학\n\n행동 과학은 인간의 인지 과정과 행동을 연구하는 학제간 분야로, 종종 개인과 환경 간의 행동 상호작용을 고려합니다. 심리학, 사회학, 인류학, 경제학과 같은 학문을 포함합니다.\n\nAI 에이전트가 수행할 수 있는 작업에 대한 기대치가 계속해서 높아지면서, 우리의 능력 프레임워크를 인지 및 행동 이론에 근간을 두는 것은 이러한 기대치를 충족하고 AI 에이전트가 인간과 유사한 능력으로 복잡한 작업을 수행할 수 있는 미래를 열어줄 견고한 기반을 제공해줄 것입니다.\n\n## AI 에이전트 능력 프레임워크\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"작은 세부 사항에 들어가기 전에 먼저 '능력'이라고 불리는 것들을 '작업'을 수행하기 위해 에이전트가 취해야 하는 '조치'를 구동하는 방식을 어떻게 범주화할 수 있는지 논의해보겠습니다. 일반적으로, 이러한 능력들은 인지, 사고, 행동 및 적응의 범주로 구성될 수 있습니다. 그리고 이를 통해 보다 구체적인 수준에서 이러한 범주에 속하는 예시 능력을 식별할 수 있습니다. 결과적인 프레임워크는 범주적으로 일관성 있는 것으로 보이지만, 다양한 능력과 범주 간의 함의된 관계가 대략적인 것임을 염두에 두세요. 실제로, 능력들은 프레임워크 전체에 걸쳐 서로 긴밀하게 얽혀 있으며 이 다차원성을 모델링하려고 하는 것은 현재 이 단계에서 그다지 유용하지 않게 느껴집니다. 아래는 범주를 구성하는 주요 범주 및 하위 범주의 시각적 표현이며, 곧 보게 될 범주 정렬이 없습니다.\n\n우리의 주요 초점은 LLM 중심의 AI 에이전트 엔지니어링에 의해 이끌린 것이지만, 이러한 프레임워크를 타인생 AI 및 로봇 분야로 확장할 수 있도록 미래 지향적이고 적용 가능한 개념을 통합하고 있습니다.\n\n마지막으로, 프레임워크에서 소명에 직접적으로 다뤄지지 않는 '자율성'은 특정 에이전트 또는 그 능력 중 하나에 대한 범용적인 특성으로 보다 적합합니다. 그렇다고 해서 자율성이 특정 작업에서 효과적으로 달성해야 하는 요구 사항은 아닙니다.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러한 기반을 마련한 후에 전체 프레임워크를 확장해 보겠습니다.\n\n# 인식\n\n에이전트가 환경에서 감각 정보를 습득, 해석 및 조직하는 능력을 포함합니다. 적절한 자극을 감지, 인식하고 이해함으로써 에이전트가 예상대로 작동할 수 있도록 합니다. 구체적인 능력의 예시:\n\n- 시각 처리: 이미지 및 물체 인식 및 처리\n- 텍스트 데이터 처리: 텍스트 인식 및 처리\n- 청각 처리: 음성 및 소리 인식 및 처리\n- 촉각 처리: 촉각 인식 및 처리\n- 후각 및 미각 처리: 향기 인식 및 처리\n- 감각 통합: 일관된 이해를 위해 다양한 감각 입력의 데이터 통합\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 고민중\n\n에이전트가 정보를 처리하고 개념을 형성하며 문제를 해결하고 결정을 내리며 지식을 적용할 수 있는 능력을 말합니다. 세분화된 능력의 예시는 다음과 같습니다:\n\n문맥적 이해와 인식\n\n- 문맥적 인식과 이해: 상황, 환경, 공간 및 시간적 맥락을 인지하고 이해함.\n- 자기인식 및 메타인지: 자기인식, 자기 모니터링, 자가 평가, 메타인지 지식\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주의와 실행 기능\n\n- 선택적 주의: 관련 데이터에 집중하면서 관련 없는 정보를 걸러내기\n- 분할 주의: 여러 작업이나 정보 원천을 동시에 처리하고 관리하기\n- 지속주의: 장기간 집중하고 노력하는 것\n- 계획: 특정 목표를 달성하기 위한 일련의 조치나 전략 수립\n- 의사 결정: 정보 분석, 옵션 평가 및 최적의 행동 결정\n- 억제 제어: 부적절하거나 원치 않는 행동이나 행동을 억제\n- 인지적 유연성: 두 가지 다른 개념에 대해 생각을 전환하거나 여러 개념을 동시에 고려하는 것\n- 감정 조절: 적절한 감정으로 감정 경험을 관리하고 대응하기\n\n기억\n\n- 단기 기억: 정보를 일시적으로 보유하고 조작하기\n- 작업 기억: 정보를 능동적으로 처리하고 조작하기\n- 장기 기억: 장기간 정보를 저장하고 검색하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이해와 분석\n\n- 논리적 추론: 형식 논리와 구조화된 규칙에 기반한 결론 도출\n- 확률적 추론: 확률과 통계 모델에 기반한 예측과 결정\n- 휴리스틱 추론: 해결책을 찾는 데 규칙 혹은 지름길을 적용\n- 귀납적 추론: 구체적 관찰로부터 일반화 도출\n- 타당적 추론: 일반 원칙이나 전제로부터 구체적 결론 도출\n- 추론적 추론: 관찰을 설명하기 위한 가설 형성\n- 유추적 추론: 이전 경험에 유사성을 찾아 문제 해결\n- 공간적 추론: 공간 관계에 대한 이해와 추론\n\n지식 활용과 응용\n\n- 의미적 지식: 범용 세계 지식과 개념을 이루는 특징 획득과 적용\n- 사건 지식: 특정 사건과 경험 지식 습득과 활용\n- 절차적 지식: 작업과 행동을 효율적으로 수행하는 방법 숙지\n- 선언적 지식: 사실적 정보 획득과 활용\n- 언어 이해: 언어를 이해하고 해석하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사회 및 감정 지능\n\n- 감정 인식: 감정을 감지하고 해석하기\n- 사회적 상호 작용: 사회적으로 적절한 방식으로 사람이나 다른 요소와 상호 작용하기\n- 공감: 다른 사람의 감정 상태를 이해하고 대응하기\n- 마음의 이해: 정신 상태, 의도 및 신념을 추론하고 이해하기\n- 사회적 지각: 사회적 단서와 맥락을 인식하고 이해하기\n- 관계 관리: 장기 관계를 관리하고 키우기\n\n창의성과 상상력\n\n- 아이디어 생성: 새로운 혁신적인 아이디어 도출하기\n- 예술적 창작: 음악, 시각 예술 및 문학과 같은 독창적인 예술 작품 창작하기\n- 상상력 있는 사고: 현재 현실을 넘어 새로운 가능성과 시나리오를 상상하고 표현하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 진행 중\n\n설명: 에이전트가 환경과 상호 작용하고 작업을 수행하는 능력을 포함합니다. 디지털 및 물리적 작업을 모두 포함합니다. 이러한 능력의 범주에는 의사 소통과 상호 작용도 포함되어 있어 사용자 및 다른 시스템과 의미 있는 상호 작용을 할 수 있습니다. 구체적인 능력의 예시는 다음과 같습니다:\n\n- 디지털 작업 실행: 특정 디지털 작업 수행, 출력 생성, 자동화, 문제 해결 작업, 결정 실행 및 응답 작업 포함\n- 물리적 작업 실행: 움직임 계획, 시작 및 조정, 감각 정보를 운동 작업과 통합하고, 물체를 잡고 다루는 것, 새로운 운동 기술 학습 및 적응\n- 인간과의 의사 소통과 상호 작용: 사용자와 의미 있는 대화를 나누며, 여러 언어를 다루고 대화 내용을 유지하는 것\n- 에이전트 및 시스템 간의 의사 소통과 상호 작용: 다른 AI 에이전트와 시스템과 효과적으로 의사 소통 및 조정, 프로토콜과 인터페이스를 사용하여 정보 교환, 작업 동기화 및 플랫폼 간 상호 작용 컨텍스트 유지하기.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내용: 에이전트가 새로운 정보, 경험 및 피드백에 기반하여 행동, 프로세스 및 감정적인 반응을 조정하고 발전시킬 수 있는 능력을 의미합니다. 명확히 말하자면, 여기서 우리는 에이전트의 운영 상태에서의 적응 및 학습 능력에 초점을 맞추고 있으며, 이는 기초 능력을 활성화하기 위한 맥락 내에서 발생하는 학습을 의미하지 않습니다. 우리의 프레임워크에서 이것은 도구 및 기법의 영역이 될 것입니다. 구체적인 능력의 예시는 다음과 같습니다:\n\n학습\n\n- 인지 학습: 인지 프로세스를 통해 지식을 습득하는 것\n- 모방 학습: 행동을 관찰하고 복제함으로써 새로운 기술과 행동을 습득하는 것\n- 경험적 학습: 경험을 통해 학습하고 반성하는 것\n\n적응과 발전\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 행동적 적응: 피드백이나 환경 변화에 대응하여 행동을 조절하는 것\n- 인지적 적응: 새로운 정보에 기반하여 인지 프로세스를 수정하는 것\n- 감정적 적응: 경험과 맥락에 기반하여 감정적 반응을 조절하는 것\n- 운동 적응: 연습과 피드백을 통해 운동 기술을 조정하는 것\n- 사회적 적응: 사회적 신호와 상호작용에 기반하여 사회적 행동을 수정하는 것\n- 진화: 시간이 지남에 따라 행동 및 인지 프로세스의 장기적인 변화와 개선\n\n이 문서는 책이 아닌 글로 쓰여졌으므로 이 예시 세부 기능에 대한 자세한 토론은 하지 않겠습니다. 이것이 철저한 내용이라고 믿고 싶지만, 최선의 시작점일 뿐입니다. 반복과 피드백을 통해 계속해서 수정, 개선하고, 더 넓은 채택에 적합한 안정적인 프레임워크로 나아갈 것입니다.\n\n이제 이 프레임워크의 실제 응용과 에이전트 엔지니어링 환경에서의 가치를 보여주는 몇 가지 예시를 살펴보겠습니다.\n\n## 실무에서의 AI 에이전트 능력 프레임워크\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI 에이전트 능력 프레임워크의 실용적 응용은 인지 및 행동과학 기반의 구조화된 개념을 활용하여 디자인 고민 프로세스를 촉진하는 데에 있습니다. 우리가 대리인들에게 원하는 능력을 구상하고 표현하는 다양한 방법을 고려할 때, 이 프레임워크는 능력 디자인 및 엔지니어링에 일관성과 포괄성을 제공하여 공통적인 기반을 확립하는 데 도움이 됩니다. 우리 AI 에이전트의 능력 수준에 대한 기대가 계속해서 높아지는 가운데, 이것은 특히 가치 있는 부분이 될 것입니다. 예시를 살펴보도록 합시다:\n\n고객 지원용 AI 에이전트\n\n고객 지원 및 맞춤 상품 추천을 제공하는 AI 에이전트를 고려해 봅시다. 이 프레임워크를 활용하여, 보다 세부적인 직무 및 시나리오 설명을 통해 더 생생한 그림을 그려봅시다.\n\n직무: 고객 지원 및 제품 추천에서 우수하고 공감적인 서비스를 제공하며, 판매 추세를 예측하고 매우 맞춤화된 상호작용을 위해 세부적인 맥락 요소를 포함하는 업무를 수행합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n상황: 활기 넘치는 온라인 고객 서비스 환경에서 우리의 AI 에이전트는 고객 쿼리를 해결하고 제품을 추천할 뿐만 아니라 요구를 예측하고 상호작용을 개인화하여 전반적인 고객 경험을 향상시키는 것이 그 임무입니다. 이 일은 다양한 조치와 능력을 포함합니다. 몇 년 전까지는 이러한 능력 중 일부를 구축하는 것은 전혀 불가능했을 것입니다. 이 작업을 위한 능력이 우리의 AI 에이전트 능력 프레임워크를 사용하여 효과적으로 표현될 수 있을까요? 이 작업의 실행 가능성을 확인하기 위해 노력하는 노력을 하겠습니다. 다음 개요가 포괄적인 것이 아님을 명심하면서 더 자세히 살펴보겠습니다:\n\n필요한 조치:\n\n- 고객 쿼리를 이해하고 해석합니다.\n- 정확하고 유용한 응답을 제공합니다.\n- 적절한 때 문제를 에스컬레이션합니다.\n- 고객 상호작용을 기반으로 판매 추세를 예측합니다.\n- 제품을 추천합니다.\n\n필요한 능력:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 지각\n\n- 텍스트 데이터 처리: 복잡한 문장과 우용어를 포함한 고객 질문을 인식하고 이해합니다.\n- 청각 처리: 시끄러운 환경에서도 말로된 질문을 필기하고 이해합니다.\n- 시각 처리: 비디오 지원 세션 중 시각 단서와 신체 언어를 해석합니다.\n\n2. 인식\n\n맥락 이해 및 인식:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 시간 인식: 계절적 추세와 피크 기간을 인식합니다.\n- 위치 인식: 지리적 위치 데이터를 이해합니다.\n- 개인 맥락 인식: 개별 고객, 그들의 이력 및 선호도를 이해합니다.\n\n기억:\n\n- 단기 기억: 최근 상호작용을 유지하기 위해 유지합니다.\n- 장기 기억: 과거 상호작용을 이용합니다.\n\n추론 및 분석:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 확률적 추론: 고객 상호작용에서 패턴을 식별하여 미래 행동을 예측합니다.\n- 연역적 논리: 논리적 프레임워크를 적용하여 문제 해결에 참여합니다.\n- 행동 분석: 고객 행동의 패턴을 이해하고 해석합니다.\n- 트렌드 분석: 현재 시장 트렌드와 계절별 데이터를 이해합니다.\n\n지식 활용과 적용\n\n- 의미 지식: 일반 세계 지식을 적용하여 질문을 이해하고 대답합니다.\n- 사건 지식: 관련 지원을 위해 특정 사건과 과거 경험을 활용합니다.\n- 선언적 지식: 정확한 응답을 위한 사실적 정보에 접근합니다.\n\n사회 및 감정지능\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 감정 인식: 고객의 감정을 감지하고 해석합니다.\n- 사회적 상호작용: 고객과 사회적으로 적절하게 소통합니다.\n- 마음의 이론: 고객의 요구를 추측하고 선제적으로 해결책을 제안합니다.\n- 관계 관리: 고객과의 확고한 관계를 구축하여 충성심을 유도합니다.\n\n창의성과 상상력\n\n- 상상력 있는 사고: 현재 문제를 넘어 새로운 가능성을 상상합니다.\n\n행동\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n디지털 상호작용:\n\n- 출력 생성: 빠르고 정확하며 맥락에 적합한 응답 생성.\n- 제품 추천 생성: 고객의 선호도 및 다른 관련 분석을 기반으로 제품을 추천.\n\n인간과의 소통 및 상호작용:\n\n- 대화의 연속성: 여러 상호작용 사이에서 맥락 유지.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n에이전트와 시스템 간 통신:\n\n- 에이전트간 협력: 다른 AI 시스템과 소통하여 행동을 동기화하고 통찰을 공유합니다.\n\n적응\n\n학습:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 체험 학습: 고객 행동에 대한 이해를 계속 향상시킵니다.\n\n적응:\n\n- 행동 적응: 피드백을 바탕으로 상호 작용 스타일을 조정합니다.\n- 인지 적응: 새로운 정보로 지식을 업데이트합니다.\n- 감정 적응: 감정적 반응을 조절합니다.\n\n이러한 통찰력 중 일부는 약간 놀라울 수도 있습니다. 예를 들어, AI 에이전트가 관계 관리를 기능으로 가지고 있어야 하는지, 또는 화면에 가상 몸체를 갖추고 비디오를 통해 '관측'할 수 있는 데이터 포인트의 새로운 배열에 대해 반응할 수 있는 AI 에이전트가 있어야 하는지에 대한 토의가 있을 수 있습니다. 확실히 개인 정보 보호 문제와 대응해야 할 문제가 많지만 완전히 배제해야 하는 개념은 아닙니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 기술과 기술을 통해 역량 구축하기\n\n이 문서는 기술과 기술을 평가하는 데 중점을 두지 않지만, 위의 연습을 거치고 나서 자연스럽게 나타나는 질문에 대해 다룰 것입니다. LLM이 대부분의 이러한 역량을 기본 제공해주는 도구를 제공하는 것은 아닌가요?\n\nLLM은 분명히 기술적 발전을 가파르게 이끌었지만, 간단한 답은 \"아니요\"입니다. 그리고 추론 및 분석 능력 같은 경우에는 LLM이 추론이나 분석같이 보이는 것을 인상적으로 시뮬레이션할 수 있지만, 인간의 그러한 역량에는 훨씬 미치지 못합니다. 요약하면, LLM은 이러한 많은 역량을 가능하게 하는 강력하지만 완전히 신뢰할 수 없는 단축키를 제공합니다. 이들은 인공 일반 지능(AGI) 아이디어 주변에 왜 그렇게 많은 관심이 있는지 설명해줄만한 매우 중요한 진화적 단계를 나타냅니다. 그것이 실제로 무엇을 의미하는지에 대한 정의는 논쟁의 대상이지만, 실현된다면 위에서 설명한 인지/행동 역량의 많은 부분을 활성화하는 기술 솔루션이 될 수 있을 것입니다.\n\n## 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI 에이전트 능력 엔지니어링 프레임워크가 사용자분의 AI 에이전트 능력을 정의하는 데 유용한 접근 방식으로 느껴지길 바랍니다. 인지 및 행동과학에서의 개념을 통합하여, 이 프레임워크는 AI 에이전트가 복잡한 작업을 수행하는 데 필요한 능력을 개발하는 데 도움이 되도록 지침을 제공합니다. 이 프레임워크는 비교적 밀집되어 있으며 시간이 지남에 따라 확실히 발전할 것입니다. 이 시점에서 주요 요점은 인지, 사고, 행동, 적응을 중심으로 한 정신적 모델입니다. 이 네 가지 고수준 개념만으로도 에이전트 능력을 효율적으로 조직화하고 개발하는 매우 견고한 기반을 제공합니다.\n\n이 글을 읽어 주셔서 감사합니다. 앞으로 이 프레임워크를 더 발전시키고 AI 에이전트 엔지니어링 프레임워크의 다른 측면을 확장할 예정입니다. 이 프레임워크나 기타 주제에 대해 더 자세히 논의하고 싶으시면 언제든지 LinkedIn을 통해 연락 주시기 바랍니다.\n\n별도로 언급되지 않는 한, 이 글의 모든 이미지는 저자가 찍은 것입니다.","ogImage":{"url":"/assets/img/2024-06-20-AIAgentCapabilitiesEngineering_0.png"},"coverImage":"/assets/img/2024-06-20-AIAgentCapabilitiesEngineering_0.png","tag":["Tech"],"readingTime":10},{"title":"이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지","description":"","date":"2024-06-20 18:20","slug":"2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV","content":"\n\n![Image](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png)\n\n객체 감지는 딥러닝의 한 분야 중 하나로, 많은 발전이 이루어졌습니다. 다양한 모델을 사용하여 우리는 사진에서 물체를 감지할 수 있고, 그 결과로 비디오에서도 물체를 감지할 수 있습니다. 요즘에는 웹캠 이미지를 사용한 실시간 객체 감지가 흔한 일입니다!\n\n이 튜토리얼에서는 TensorFlow를 사용하여 객체 감지 시스템을 구축할 것입니다. 구체적으로 TensorFlow Object Detection API를 사용할 것입니다. 단계별로 모든 필요한 종속성을 설치하고, TensorFlow Model Zoo의 사전 훈련된 모델을 살펴보고, 객체 감지기를 구축할 것입니다.\n\n다시 말해, 이 튜토리얼을 읽은 후에는...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- TensorFlow 기반 객체 검출기를 구축하기 위해 설치해야 하는 것을 알게 되었습니다.\n- 사전 훈련된 모델을 찾고 시스템에 다운로드하는 위치를 알고 있습니다.\n- 사진 및 비디오와 함께 사용할 수 있는 실제 객체 검출 시스템을 구축했습니다.\n\n그리고 이미지는 항상 수많은 말보다 많은 것을 전달합니다. 아래 시스템을 만들어 보세요!\n\n![이미지](https://miro.medium.com/v2/resize:fit:1200/1*qa_qXkly0MvO82-7uV7LpA.gif)\n\n한번 살펴보시죠!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 객체 탐지기 구축: 필수 조건\n\n텐서플로 Object Detection API를 사용하여 객체 탐지 시스템을 구축하려면 다음 세 가지 단계를 완료해야 합니다:\n\n- TensorFlow 및 OpenCV 설치하기. 우리는 TF 기능을 위해 TensorFlow가 필요하며, 이미지 I/O를 위해 OpenCV가 필요합니다. 보통 시스템에 이미 설치되어 있지만, 완전성을 위해 여기에 포함시켰습니다.\n- TensorFlow Object Detection API 설치하기. 이 추가 기능 세트는 별도로 설치해야 합니다. 어떻게 설치할 수 있는지 살펴보겠습니다.\n- TensorFlow Model Zoo에서 적절한 사전 학습된 모델 찾기. TensorFlow의 제작자들이 다양한 모델 아키텍처를 사용하여 사전 학습된 여러 모델을 TensorFlow Model Zoo에 넣었습니다. 간단히 살펴보고 모델을 선택할 것입니다.\n\n![이미지](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## TensorFlow와 OpenCV 설치하기\n\n실제 객체 탐지기를 구축하기 전에 TensorFlow와 OpenCV를 설치해야 합니다.\n\n여기서는 이미 시스템에 Python이 설치되어 있다고 가정합니다. 그렇지 않은 경우 먼저 Python을 설치해 주세요.\n\n요즘은 TensorFlow를 설치하는 것이 정말 쉽습니다. Python에 액세스할 수 있는 터미널에서 다음을 실행하면 됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 최신 버전의 pip가 필요합니다\npip install --upgrade pip\n\n# CPU 및 GPU용 현재 안정적인 릴리스\npip install tensorflow\n```\n\n먼저 pip를 최신 버전으로 업그레이드하고 TensorFlow를 설치합니다. 이제 CPU 또는 GPU 버전을 수동으로 지정해야 했던 것이 오늘에는 그렇지 않습니다. 그냥 tensorflow를 설치하면 GPU 버전이 정확하게 설정된 경우 GPU 버전이 자동으로 설치됩니다. 실제로 GPU와 CPU 사이를 자유롭게 전환할 수 있지만, 이에 대해서는 나중에 다시 이야기하겠습니다.\n\nOpenCV를 설치하는 것도 어렵지 않습니다: pip install opencv-python으로 해결할 수 있습니다.\n\n이제 기본 패키지가 설치되었으므로 TensorFlow Object Detection API를 살펴볼 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_2.png\" /\u003e\n\n강아지... 와우!\n\n## TensorFlow Object Detection API 설치\n\nGitHub에서 tensorflow/models에서 Object Detection API를 찾을 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이름에서 알 수 있듯이, 이것은 객체 감지 목적으로 사용할 수 있습니다. 특히, 사전 훈련된 모델을 로드하고 이미지 및 비디오에 경계 상자를 추가하는 기능을 제공합니다. 우리의 객체 감지 시스템이 이러한 API를 활용할 수 있다는 것은 우리가 모든 것을 직접 개발할 필요가 없다는 멋진 점입니다.\n\n나중에 사전 훈련된 모델을 살펴볼 것입니다. 먼저 Object Detection API를 설치해 봅시다. 이것은 시스템에 Git이 설치되어 있는 것을 가정합니다. 또한 protoc 명령을 실행할 수 있는지 확인해 주세요. 여기서 확인하는 방법을 찾아보세요.\n\n- 먼저 tensorflow/models 저장소 전체를 복제합니다. 한 단계 깊이만 복제하도록 주의하세요. 다음 명령을 실행하여 저장소를 복제하세요: git clone --depth 1 https://github.com/tensorflow/models\n- 이제 models/research/ 디렉토리로 이동한 다음 protoc object_detection/protos/*.proto --python_out=. 명령을 실행하세요.\n- 그런 다음 cp object_detection/packages/tf2/setup.py 명령을 사용하여 설정 파일을 현재 디렉토리로 복사합니다.\n- 마지막으로 python -m pip install 명령을 통해 Object Detection API를 pip를 통해 설치하세요.\n\n## TensorFlow 모델 동물원: 객체 감지용 사전 훈련된 모델\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희 물체 감지 시스템은 TensorFlow 모델 위에 구축될 예정이에요. 이 모델은 다양한 종류의 물체를 감지할 수 있어요. 이 모델을 훈련하는 과정은 다음과 같아요:\n\n- 다양한 물체가 포함된 많은 이미지를 수집하는 것\n- 이러한 이미지들에 레이블을 달아 모든 클래스가 균형을 이루도록 하는 것\n- 모델을 훈련하는 것\n\n이 과정은 많은 노력이 필요할 거예요. 다행히 TensorFlow 팀은 TensorFlow Detection Model Zoo에서 다양한 사전 훈련된 물체 감지 모델을 제공하고 있어요.\n\n이러한 물체 감지기는 이미 훈련이 완료되어 있으며 TensorFlow Object Detection API에서 이용할 수 있어요 (괄호 안에는 내부 모델 구조가 나와 있어요):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- CenterNet (HourGlass104, Resnet50 V1, Resnet101 V1, Resnet50 V2).\n- EfficientDet (D0, D1, D2, D3, D4, D5, D6, D7).\n- SSD (MobileNet V1 FPN, V2, V2 FPNLite; ResNet50 V1; Resnet101 V1).\n- Faster R-CNN (ResNet50; ResNet101; ResNet152; Inception ResNet V2).\n- Mask R-CNN (Inception ResNet V2).\n- ExtremeNet.\n\n당연히 직접 모델을 만드실 수도 있습니다. 하지만 이 강좌에서 다루지는 않습니다.\n\n오늘은 SSD MobileNet V2 FPNLite 640x640 모델을 사용할 것입니다. Zoo에서 원하는 모델을 선택하실 수 있지만, 이 사전 훈련된 모델은 용량이 20MB밖에 되지 않아서 빠른 인터넷 속도를 가진 많은 사람들이 다운로드할 수 있습니다.\n\n이제 우리의 디텍터를 만들어봅시다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_3.png\" /\u003e\n\n## 객체 탐지기 생성\n\n여기서는 객체 탐지 시스템을 구축하는 방법을 살펴보겠습니다. 이 과정은 세 가지 별개이지만 순차적인 단계로 나눌 수 있습니다:\n\n- 기반을 놓기. 이곳에서는 중요한 imports를 지정하고, 클래스를 정의하고, 초기화 작업을 설명하고, 준비 작업 정의를 작성할 것입니다.\n- 탐지 함수 작성. 이것이 탐지기의 핵심입니다. 이것은 일반적으로 탐지를 수행하고, 특히 이미지와 비디오에 대한 예측을 생성할 수 있게 합니다.\n- 탐지 호출 생성. 마지막으로, 우리의 탐지기가 준비되면 사용할 수 있도록 다음 추가 코드를 추가할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드 에디터를 열고 objectdetector.py와 같은 Python 파일을 생성해 주세요. 코드 작성 시작할까요?\n\n## 파트 1: 기반 구축하기\n\nTensorFlow Object Detection API를 기억하시나요? 이것은 물체 감지기를 구축하기 위한 TensorFlow 위의 프레임워크입니다. 다시 말해, 머신 러닝 모델을 만들기 위한 잘 알려진 라이브러리 위에 또 다른 층이란 의미죠. 이 API 위에 Object Detection API를 사용하는 물체 감지기 층을 추가할 계획입니다.\n\n이 TFObjectDetector의 기반을 구축하기 위해서는 Python 임포트 추가, 필요한 경우 GPU 비활성화, TFObjectDetector 클래스 작성 및 초기화, 물체 감지기를 위한 설정 메커니즘 작성, 마지막으로 몇 가지 도우미 함수를 작성해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 파이썬 라이브러리 가져오기\n\n첫 번째 코드는 항상 파이썬 라이브러리를 가져와야 합니다. 오늘도 마찬가지에요:\n\n```js\n# 모델 라이브러리 지정\nfrom object_detection.builders import model_builder\nfrom object_detection.utils import config_util\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import visualization_utils as viz_utils\nimport cv2\nimport numpy as np\nimport os\nimport tensorflow as tf\n```\n\nobject_detection 패키지에서 많은 함수를 가져왔네요 - 이는 TensorFlow Object Detection API를 나타냅니다. 모델 빌더를 사용하여 감지 모델(예: SSD MobileNet 모델)을 구축할 거에요. config_util을 사용하면 TensorFlow에 올바른 모델을 로드하도록 알려주는 구성을 로드할 수 있습니다. 클래스 이름을 나타내는 레이블은 label_map_util을 사용하여 로드할 수 있고, viz_utils는 이미지나 비디오에 경계 상자를 추가하는 데 유용하게 사용될 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nOpenCV (cv2)는 이미지의 입력 및 출력에 사용되며, NumPy (np)는 숫자 처리에 사용되고, os는 운영 체제 기능에 사용되며, 마지막으로 TensorFlow를 import합니다.\n\n## 필요한 경우 GPU 비활성화\n\n두 번째 단계는 GPU를 비활성화하는 것인데, 이것은 선택 사항입니다 — 다시 말해, 원할 경우에만 수행하십시오. 특히 GPU를 보유하고 있지만 구성이 잘못된 경우에 유용할 수 있습니다. 그때 CUDA 가시 장치를 환경에서 모두 지워야 합니다. TensorFlow의 GPU 버전을 사용하지 않는 경우에는 이 코드를 생략할 수 있습니다.\n\n```js\n# 필요한 경우 GPU 비활성화\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 클래스와 초기화자 만들기\n\n이제 진짜 작업을 시작할 시간입니다. 우리의 객체 탐지기의 모든 기능을 다루는 TFObjectDetector 클래스를 만들어봅시다.\n\n```js\n# 객체 탐지기 생성\nclass TFObjectDetector():\n```\n\n우리는 즉시 __init__ 정의를 추가합니다. 이는 클래스의 생성자를 나타내며 다시 말해, TFObjectDetector를 로딩하는 즉시 실행됩니다. 입력 값으로 다음을 받는다는 것을 주목하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 객체 검출에 대한 경로는 시스템에 설치된 Object Detection API의 TensorFlow 2.x 구성 파일 경로를 나타냅니다.\n- 실행 중인 모델의 모델 체크포인트 경로 (우리의 경우 SSD MobileNet 모델).\n- 텍스트 레이블에 클래스 ID를 매핑하는 사전을 구성할 수 있도록 하는 레이블 파일의 경로.\n- 모델 이름.\n\n⚠ 나중에 실제로 검출기를 사용할 때 상황에 맞게 입력값을 설정하는 방법을 설명할 것입니다.\n\n생성자에서는 여러 작업을 수행합니다. 우선, 입력값을 검출기 전체에 재사용할 수 있도록 많은 인스턴스 변수를 채웁니다. 또한 Object Detection API 폴더에 있는 파이프라인 구성을 로드하고, 우리 모델에 해당하는 구성 파일을 로드하며, 마지막으로 self.setup_model()을 호출합니다.\n\n이로써 모델의 설정 메커니즘을 시작하며, 지금 바로 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 객체 검출기 생성\nclass TFObjectDetector():\n\n  # 생성자\n  def __init__(self, path_to_object_detection='./models/research/object_detection/configs/tf2',\\\n    path_to_model_checkpoint='./checkpoint', path_to_labels='./labels.pbtxt',\\\n      model_name='ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'):\n    self.model_name = model_name\n    self.pipeline_config_path = path_to_object_detection\n    self.pipeline_config = os.path.join(f'{self.pipeline_config_path}/{self.model_name}.config')\n    self.full_config = config_util.get_configs_from_pipeline_file(self.pipeline_config)\n    self.path_to_model_checkpoint = path_to_model_checkpoint\n    self.path_to_labels = path_to_labels\n    self.setup_model()\n```\n\n## 설정 매커니즘\n\n설정 매커니즘은 모델을 백그라운드에서 설정하고 객체 검출기를 사용할 수 있게 만드는 역할을 담당합니다. 다음 단계로 구성됩니다:\n\n- __init__ 함수에서 로드된 모델 구성을 사용하여 모델을 빌드하는 과정.\n- 특정 상태로 모델을 복원하는 단계, 즉 훈련된 특정 상태로 모델을 복원합니다.\n- 예측을 생성하는 데 사용할 수있는 tf.function인 모델 검출 함수를 검색하는 단계.\n- 클래스 ID 및 텍스트 라벨 간의 매핑을 생성하는 단계으로, 라벨을 준비하는 과정입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 단계들의 실행을 setup_model() 정의로 그룹화해 봅시다. 이 정의는 위에서 지정된 __init__ 정의에서 호출되며, 따라서 우리의 객체 탐지기를 생성할 때 호출됩니다.\n\n```js\n  # 모델 설정\n  def setup_model(self):\n    self.build_model()\n    self.restore_checkpoint()\n    self.detection_function = self.get_model_detection_function()\n    self.prepare_labels()\n```\n\n그 다음으로 build_model()를 만들어 봅시다:\n\n```js\n  # 탐지 모델 빌드\n  def build_model(self):\n    model_config = self.full_config['model']\n    assert model_config is not None\n    self.model = model_builder.build(model_config=model_config, is_training=False)\n    return self.model\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 정의는 구성을 검색하고 존재하는지 확인한 뒤 모델을 빌드합니다. 이 모델은 인스턴스 변수에 할당되어 객체 탐지기 전반에 걸쳐 재사용될 수 있도록 합니다.\n\nrestore_checkpoint() 함수를 사용하면 TensorFlow Detection Model Zoo에서 제공하는 체크포인트 위치/상태로 모델을 되돌릴 수 있습니다.\n\n```js\n  # 모델로 체크포인트 복원\n  def restore_checkpoint(self):\n    assert self.model is not None\n    self.checkpoint = tf.train.Checkpoint(model=self.model)\n    self.checkpoint.restore(os.path.join(self.path_to_model_checkpoint, 'ckpt-0')).expect_partial()\n```\n\n그런 다음 탐지를 위한 tf.function을 생성할 수 있습니다. 이 함수는 모델을 활용하여 이미지를 전처리하고 예측을 생성한 후 감지를 처리하고 모든 것을 반환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n  # 탐지를 위한 tf.function 가져오기\n  def get_model_detection_function(self):\n    assert self.model is not None\n    \n    @tf.function\n    def detection_function(image):\n      image, shapes = self.model.preprocess(image)\n      prediction_dict = self.model.predict(image, shapes)\n      detections = self.model.postprocess(prediction_dict, shapes)\n      return detections, prediction_dict, tf.reshape(shapes, [-1])\n    \n    return detection_function\n```\n\n마지막으로, prepare_labels()라는 정의를 생성합니다. TensorFlow의 사람들에 의해 만들어졌으며 클래스 식별자를 텍스트 레이블로 매핑하는 책임이 있습니다. 이것은 이 인스턴스 변수로 설정됩니다.\n\n```js\n  # 레이블 준비\n  # 출처: https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\n  def prepare_labels(self):\n    label_map = label_map_util.load_labelmap(self.path_to_labels)\n    categories = label_map_util.convert_label_map_to_categories(\n        label_map,\n        max_num_classes=label_map_util.get_max_label_map_index(label_map),\n        use_display_name=True)\n    self.category_index = label_map_util.create_category_index(categories)\n    self.label_map_dict = label_map_util.get_label_map_dict(label_map, use_display_name=True)\n```\n\n## 도우미 함수들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지 우리는 객체 탐지기를 준비할 수 있는 기반을 만들었습니다. 이 부분을 완료하려면 두 가지 더 도와주는 함수를 만들기만 하면 됩니다. 첫 번째 함수는 키포인트 튜플을 재구성하고, 두 번째 함수는 이미지를 준비합니다. 즉, 이미지를 텐서로 변환해줍니다.\n\n```js\n  # Get keypoint tuples\n  # 출처: https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\n  def get_keypoint_tuples(self, eval_config):\n    tuple_list = []\n    kp_list = eval_config.keypoint_edge\n    for edge in kp_list:\n      tuple_list.append((edge.start, edge.end))\n    return tuple_list\n\n  \n  # Prepare image\n  def prepare_image(self, image):\n    return tf.convert_to_tensor(\n      np.expand_dims(image, 0), dtype=tf.float32\n    )\n```\n\n## 파트 2: 탐지 함수 작성\n\n와우, 이미 2부에 도착했네요! 이번에는 탐지 함수를 작성할 거예요. 더 자세히 말하면, 세 가지 정의를 만들 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 일반 탐지 기능입니다. 이 기능은 이미지 및 비디오 감지에 재사용할 수 있는 일반 탐지 코드를 포함하고 있습니다.\n- 이미지 감지입니다. 이 코드는 이미지 내 객체 감지를 위해 특히 사용됩니다.\n- 비디오 감지입니다. 이 코드는 비디오 내 객체 감지를 위해 사용됩니다.\n\n## 일반 탐지 기능\n\n첫 번째 정의는 일반 탐지 기능입니다. 이곳에서 일반은 이미지와 비디오에서 감지하는 기능을 공유한다는 뜻입니다. 다시 말해, 무의미하게 두 번 추가할 필요가 없는 것들을 포함합니다! 다음 세그먼트가 포함되어 있습니다:\n\n- 우선, 감지 함수가 None이 아닌지 확인합니다 (위의 Part 1에서). 이는 설정되어 있지 않으면 감지를 수행할 수 없음을 의미합니다.\n- 이미지를 복사하고 텐서로 변환하여 준비합니다. 그런 다음 예측이 포함된 사전 및 모양 정보를 가진 객체를 생성합니다.\n- 키포인트가 있는 경우 사용합니다.\n- Object Detection API에서 제공하는 viz_utils API를 사용하여 예측과 함께 바운딩 박스를 이미지에 추가합니다.\n- 마지막으로 바운딩 박스가 있는 이미지를 반환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\n# 객체 감지 실행\ndef detect(self, image, label_offset = 1):\n    # 감지 함수가 있는지 확인\n    assert self.detection_function is not None\n    \n    # 이미지 준비 및 예측 수행\n    image = image.copy()\n    image_tensor = self.prepare_image(image)\n    detections, predictions_dict, shapes = self.detection_function(image_tensor)\n\n    # 제공된 키포인트 사용\n    keypoints, keypoint_scores = None, None\n    if 'detection_keypoints' in detections:\n        keypoints = detections['detection_keypoints'][0].numpy()\n        keypoint_scores = detections['detection_keypoint_scores'][0].numpy()\n    \n    # 출력 이미지/프레임에 시각화 수행\n    viz_utils.visualize_boxes_and_labels_on_image_array(\n        image,\n        detections['detection_boxes'][0].numpy(),\n        (detections['detection_classes'][0].numpy() + label_offset).astype(int),\n        detections['detection_scores'][0].numpy(),\n        self.category_index,\n        use_normalized_coordinates=True,\n        max_boxes_to_draw=25,\n        min_score_thresh=.40,\n        agnostic_mode=False,\n        keypoints=keypoints,\n        keypoint_scores=keypoint_scores,\n        keypoint_edges=self.get_keypoint_tuples(self.full_config['eval_config']))\n    \n    # 이미지 반환\n    return image\n```\n\n## 이미지용 감지 함수\n\n이제 어떤 이미지 위의 객체를 감지하는 것이 쉽습니다. OpenCV를 사용하여 경로에서 이미지를 읽고, 일반적인 감지 정의를 호출한 후 결과를 출력 경로에 작성하는 것으로 간단하게 수행할 수 있습니다.\n\n```python\n# 폴더에서 이미지 예측\ndef detect_image(self, path, output_path):\n\n    # 이미지 로드\n    image = cv2.imread(path)\n\n    # 객체 감지 수행 및 출력 파일에 추가\n    output_file = self.detect(image)\n    \n    # 출력 파일을 시스템에 작성\n    cv2.imwrite(output_path, output_file)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 비디오용 Detect 함수\n\n비디오에서 객체를 감지하는 것은 조금 더 어렵지만 여전히 매우 쉽습니다. 비디오는 종종 초당 25프레임의 이미지로 이루어진 이미지 집합에 불과하다는 것을 상기해주세요. 이 특성을 활용하여 비디오에서 객체 감지를 수행할 것입니다!\n\n이 세그먼트는 다음 단계로 구성되어 있습니다:\n\n- 먼저 출력 비디오 라이터와 코덱을 설정합니다. 이를 통해 바운딩 박스가 그려진 각 프레임을 출력 비디오에 작성할 수 있습니다. 이것은 사실상 비디오 프레임을 바운딩 박스와 함께 한 프레임씩 재구성하는 것을 의미합니다.\n- 그런 다음 OpenCV의 VideoCapture 기능을 사용하여 경로에서 비디오를 읽습니다.\n- vidcap.read()를 사용하여 첫 번째 프레임(이미지)을 읽고 성공적으로 읽었는지 표시합니다. 프레임 수를 0으로 설정합니다.\n- 이제 프레임을 순환하며 감지를 수행하고(이것이 사실상 이미지에서의 감지임을 알아두세요!) 프레임을 출력 비디오에 작성합니다. 다음 프레임을 읽어 나가며, 더 이상 프레임을 읽을 수 없을 때(즉, frame_read != True가 될 때까지) 계속합니다.\n- 모든 프레임을 처리한 후 출력 비디오를 out.release()를 사용하여 해제합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n  # 폴더로부터 비디오를 예측합니다\n  def detect_video(self, path, output_path):\n    \n    # 코덱을 사용하여 출력 비디오 작성기를 설정합니다\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, 25.0, (1920, 1080))\n    \n    # 비디오를 읽어옵니다\n    vidcap = cv2.VideoCapture(path)\n    frame_read, image = vidcap.read()\n    count = 0\n    \n    # 각 프레임을 반복하면서 예측을 수행합니다\n    while frame_read:\n        \n      # 물체 감지를 수행하고 출력 파일에 추가합니다\n      output_file = self.detect(image)\n      \n      # 예측과 함께 프레임을 비디오에 작성합니다\n      out.write(output_file)\n      \n      # 다음 프레임 읽기\n      frame_read, image = vidcap.read()\n      count += 1\n        \n    # 비디오 파일을 릴리스합니다\n    out.release()\n```\n\n## 섹션 3: ​​감지 호출 생성\n\n1부 및 2부에서 TFObjectDetector 클래스를 작성하며 감지기를 완성했습니다. 이제 완료 되었으므로 호출해 보는 시간이 됐어요. 다음 코드로 호출할 수 있습니다.\n\n```js\nif __name__ == '__main__':\n  detector = TFObjectDetector('../../tf-models/research/object_detection/configs/tf2', './checkpoint', './labels.pbtxt', 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8')\n  detector.detect_image('./shop.jpg', './shopout.jpg')\n  detector.detect_video('./video.mp4', './videooutput.mp4')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 코드는 다음을 수행합니다:\n\n- 직접적으로 실행될 때, 즉 다른 클래스의 컨텍스트 내에서 실행되지 않을 때, 먼저 TFObjectDetector의 새 인스턴스를 생성합니다. 여기에서는 다음 정보를 전달합니다:\n- 클론된 TensorFlow 모델의 tf2 구성 폴더로의 절대 또는 상대 경로입니다.\n- 다운로드한 모델의 모델 체크포인트 폴더로의 절대 또는 상대 경로입니다. 사용하는 SSD MobileNet의 경우, 폴더를 해제하고 열면 ./checkpoint 폴더가 나타납니다. 거기를 참조하세요.\n- 클래스 인덱스와 레이블 이름 간의 매핑에 사용되는 레이블 파일의 절대 또는 상대 경로입니다. 없는 경우 TensorFlow Detection Model Zoo 모델 중 하나에 대해 여기에서 다운로드할 수 있습니다.\n- 모델의 이름입니다. 우리 경우에는 지정한 어려운 이름입니다. Model Zoo에서 다른 이름들 중 하나를 사용할 수도 있지만 그에 맞는 체크포인트를 사용해야 합니다.\n- ./shop.jpg라는 이미지에서 이미지 검출을 수행하고 결과물(즉, 바운딩 상자가 오버레이된 이미지)을 ./shopout.jpg에 저장합니다.\n- ./video.mp4라는 비디오에서 비디오 검출을 수행하고 출력물을 ./videooutput.mp4로 저장합니다.\n\n## 전체 모델 코드\n\n바로 코드로 이동하려는 사용자를 위해 전체 모델 코드는 내 Github 저장소에서 찾을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 객체 탐지기 실행하기\n\n이제 객체 탐지기를 실행한 결과를 살펴보겠습니다.\n\n이 사진들과 동영상은 Pexels 라이선스 하에 다운로드되어 사용되었습니다.\n\n## 사진에서\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_4.png)\n\n![Image 2](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_5.png)\n\n![Image 3](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_6.png)\n\n## On videos\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식으로 표시한 이미지 링크입니다.\n\n![이미지1](https://miro.medium.com/v2/resize:fit:1200/1*alxO3bGUDN3dzJnQ6wIoZQ.gif)\n\n![이미지2](https://miro.medium.com/v2/resize:fit:1200/1*hTy8jS8LYMGNPgJ6Q_jj2w.gif)\n\n## 요약\n\n머신 러닝에서 객체 감지에는 많은 유용한 사례가 있습니다. 이 튜토리얼을 통해 TensorFlow 객체 감지 API와 사전 훈련된 모델을 사용하여 이미지와 비디오에서 객체 감지를 수행하는 방법을 배웠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오늘의 글에서 무언가를 배워가셨나요? 궁금한 점, 의견 또는 제안이 있으면 언제든지 환영합니다. 읽어 주셔서 감사합니다!\n\n## 참고 자료\n\nTensorFlow, TensorFlow 로고 및 관련 상표는 Google Inc.의 상표입니다.\n\nTensorFlow. (2020, 9월 9일). TensorFlow/models. GitHub. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTensorFlow. (2020, 11). TensorFlow/models. GitHub. https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\n\nTensorFlow. (n.d.). TensorFlow/models. GitHub. https://github.com/tensorflow/models/tree/master/research/object_detection\n\n\"현대 합성곱 객체 탐지기의 속도/정확도 균형.\" 황재식, 라토드 비니트, 썬 첸, 주 만멍, 코라티카라 아니쉬, 파티 아드리아노, 피셔 이안, 우예노비치 세르게이, 송 양초, 과다라마 세르게이, 머피 케빈, CVPR 2017","ogImage":{"url":"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png"},"coverImage":"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png","tag":["Tech"],"readingTime":18},{"title":"인공지능의 마법 구슬 신경망이 인플레이션을 예측하는 방법","description":"","date":"2024-06-20 18:18","slug":"2024-06-20-AIsCrystalBallHowNeuralNetworksForecastInflation","content":"\n\n`\u003cimg src=\"/assets/img/2024-06-20-AIsCrystalBallHowNeuralNetworksForecastInflation_0.png\" /\u003e`\n\n인플레이션 예측에 대한 전통적인 방법은 과거 데이터와 복잡한 계량 경제 모델에 크게 의존하지만, 종종 빠르게 변화하는 경제 상황의 세세한 점을 포착하지 못할 때가 있습니다. (일부 우수한 모델을 제외하고) 이제는 신경망, 특히 LSTM 신경망이 예측 분석에 접근하는 방식을 혁신하고 있습니다.\n\n이 기사는 LSTM 모델을 핵심부터 만들고, 미국의 월별 인플레이션 지표 변화를 예측하는 데 사용하는 방법을 보여줍니다.\n\n# LSTM 부트캠프\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떤 것을 이해하는 가장 좋은 방법은 단순하게 생각하는 것입니다. 수학이나 복잡한 그래프가 필요하지 않고 순수한 직관과 논리만 있으면 돼요. 책을 읽고 있다고 상상해봐요. 한 장을 넘겨 다음 장으로 넘어갈 때, 이전 장의 중요한 내용을 기억해야 현재 장을 이해할 수 있어요. 이전 장에서의 정보를 상기시킬 수 있는 능력은 이야기를 따라가는 데 도움을 줍니다. 이제 컴퓨터가 이 책을 어떻게 읽을 수 있을지 생각해봐요.\n\n인간과 달리 컴퓨터는 새로운 정보를 처리할 때 이전 정보를 기억하는 데 어려움을 겪는 경향이 있어요 (미래에 우리를 지배하기 전에 컴퓨터들에게 우리의 이점을 남겨둘 수 있어 다행이죠). 여기서 장기 단기 기억망(LSTM) 네트워크가 등장합니다 — 이들은 당신이 책을 읽을 때와 같이 컴퓨터가 중요한 세부 정보를 시간을 초월하여 기억하는 데 도움을 줍니다. 그래서 LSTM 네트워크에서 중요한 키워드는 기억입니다. 하지만 LSTMs가 정말 무엇인가요?\n\n이들은 데이터 시퀀스를 처리하기 위해 설계된 특별한 종류의 인공 신경망입니다. 이들은 오랜 기간동안 정보를 기억하는 문제를 해결하기 위해 만들어졌는데, 일반적인 신경망은 이를 다루기 어렵습니다.\n\nLSTMs는 역사를 공부하면서 중요한 사건을 메모할 수 있는 노트북을 가지고 있는 것과 같아요. 언제든 필요할 때 이러한 노트를 다시 참고하여 이전 정보를 상기시킬 수 있습니다. 이 노트북이 바로 LSTM의 기억입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`LSTM`의 기능에 대해 이해해야 할 내용이 대부분입니다. 재미있는 세부사항은 전문가들에게 맡기고, 우리의 목표인 기계 학습 알고리즘을 사용한 인플레이션 예측으로 넘어가 보겠습니다.\n\n# LSTM을 이용한 인플레이션 예측\n\n우선, 분석하려는 데이터 유형을 이해해야 합니다. 미국 소비자 물가지수(CPI)는 도시 소비자가 일상적으로 소비하는 장바구니의 상품 및 서비스에 대한 가격 변동의 평균 변화를 측정하는 중요한 경제 지표입니다. 기본적으로 CPI는 음식, 의류, 주거, 연료, 교통, 의료 서비스 및 사람들이 일상적으로 구입하는 다른 상품 및 서비스에 대한 가격 변동을 모니터링하여 생활비 변동을 추적합니다.\n\n작업 계획은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 필요한 Python 라이브러리 및 미국 세인트루이스 연방준비은행에서 공개한 인플레이션(CPI) 데이터를 가져옵니다.\n- 데이터를 정리하고 훈련 세트와 테스트 세트로 분할합니다.\n- 설명 변수(예측 변수)를 선택합니다. 이 경우 미래 값을 예측하기 위해 과거 값을 사용하는 지연 변화가 사용됩니다. 이는 데이터에서 자기 상관성과 예측 가능성의 형태를 의미합니다.\n- 데이터를 훈련하고 테스트 데이터에서 예측합니다.\n- 정확도(적중률)와 평균 제곱근 오차(RMSE)를 사용하여 모델을 평가합니다.\n\n아래 코드를 사용하여 알고리즘을 구현하세요:\n\n```js\n# 필요한 라이브러리 가져오기\nimport pandas_datareader as pdr\nimport matplotlib.pyplot as plt\n\n# 히스토리컬 데이터의 시작 및 끝 날짜 설정\nstart_date = '1950-01-01'\nend_date = '2024-01-23'\n\n# 데이터프레임 생성 및 CPI 데이터 다운로드\ndata = pdr.DataReader('CPIAUCSL', 'fred', start_date, end_date)\n\n# CPI 데이터프레임에 nan 값이 있는지 확인\ncount_nan = data['CPIAUCSL'].isnull().sum()\n\n# 결과 출력\nprint('CPI 데이터프레임에 있는 nan 값의 수: ' + str(count_nan))\n\n# CPI를 연간 변화율로 변환\ndata = data.pct_change(periods=12, axis=0) * 100\n\n# 행에서 nan 값을 제거\ndata = data.dropna()\n\n# 라이브러리 가져오기\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nimport numpy as np\nimport pandas_datareader as pdr\nfrom sklearn.metrics import mean_squared_error\n\n# 데이터 전처리 함수 정의\ndef data_preprocessing(data, num_lags, train_test_split):\n    # 훈련을 위한 데이터 준비\n    x = []\n    y = []\n    for i in range(len(data) - num_lags):\n        x.append(data[i:i + num_lags])\n        y.append(data[i + num_lags])\n    # 데이터를 넘파이 배열로 변환\n    x = np.array(x)\n    y = np.array(y)\n    # 데이터를 훈련 및 테스트 세트로 분할\n    split_index = int(train_test_split * len(x))\n    x_train = x[:split_index]\n    y_train = y[:split_index]\n    x_test = x[split_index:]\n    y_test = y[split_index:]\n\n    return x_train, y_train, x_test, y_test\n\n# 훈련 및 테스트 값을 그래프로 그리는 함수 정의\n# 나머지 코드는 생략합니다.\n``` \n\n이 코드는 제가 최신 딥러닝 도서에서 가져온 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n플로팅 함수는 다음 차트를 제공해야 합니다:\n\n![차트](/assets/img/2024-06-20-AIsCrystalBallHowNeuralNetworksForecastInflation_1.png)\n\n이전에 정의한 함수를 사용하여 알고리즘을 평가해 봅시다:\n\n```js\n# 성능 평가\nprint('---')\nprint('정확도(학습) = ', round(calculate_accuracy(y_predicted_train, y_train), 2), '%')\nprint('정확도(테스트) = ', round(calculate_accuracy(y_predicted, y_test), 2), '%')\nprint('RMSE(학습) = ', round(np.sqrt(mean_squared_error(y_predicted_train, y_train)), 10))\nprint('RMSE(테스트) = ', round(np.sqrt(mean_squared_error(y_predicted, y_test)), 10))\nprint('상관관계(시계열 예측/학습) = ', round(np.corrcoef(np.reshape(y_predicted_train, (-1)), y_train)[0][1], 3))\nprint('상관관게(예측/테스트) = ', round(np.corrcoef(np.reshape(y_predicted, (-1)), np.reshape(y_test, (-1)))[0][1], 3))\nprint('---')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전 코드의 출력은 다음과 같습니다:\n\n```js\n---\nAccuracy Train =  62.58 %\nAccuracy Test =  70.51 %\nRMSE Train =  0.3287812546\nRMSE Test =  0.3275757807\nCorrelation In-Sample Predicted/Train =  0.522\nCorrelation Out-of-Sample Predicted/Test =  0.509\n---\n```\n\n예측 모델을 개발하고 배포할 때, 특히 금융과 같은 중요한 분야에서는 어떤 모델에 의존하기 전에 광범위한 연구를 수행하는 것이 중요합니다. 꼼꼼히 공부해보세요!","ogImage":{"url":"/assets/img/2024-06-20-AIsCrystalBallHowNeuralNetworksForecastInflation_0.png"},"coverImage":"/assets/img/2024-06-20-AIsCrystalBallHowNeuralNetworksForecastInflation_0.png","tag":["Tech"],"readingTime":5},{"title":"사함이 없이 나는 누구일까요","description":"","date":"2024-06-20 18:17","slug":"2024-06-20-ngunitsinoakokungwalangmedalya","content":"\n\n\n![Img](/assets/img/2024-06-20-ngunitsinoakokungwalangmedalya_0.png)\n\nMaraming tao ang nagsasabi na matalino raw ako, dahil marami akong natatanggap na award sa paaralan. Sinasabi nila na malayo raw ang mararating ko dahil masipag ako mag-aral. Sabi nila, magaling raw ako dahil marami akong natanggap na medalya noong ako ay nagtapos ng High School.\n\nIto ang mga salitang madalas kong marinig saanman. Mga salitang nakakataba ng puso. Para silang musika sa aking tenga na parang magpapatulog sa akin ng mahimbing kapag paulit-ulit kong pinapakinggan.\n\nNgunit sa sandaling iyon, hindi maiwasan na itanong sa sarili—sino nga ba ako kung walang medalya?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어릴 적에는 저의 꿈이 그냥 많은 장난감을 가지고 놀고, 잔디밭에서 놀고, 그네를 타고, 들판을 뛰어다니는 것이었어요. 모든 것에 능숙해질 필요가 있을 거라는 생각은 전혀 들지 않았고, 상을 받을 생각은 하지 않았어요. 그때의 제 삶은 단순했어요. 그냥 숙제를 해결할 수 있기만 하면 충분했어요. 수학 숙제를 살아남는 것만 하는 것이라도 말이에요.\n\n하지만 운명의 궁금한 일들이 일어나고, 삶의 경로가 달라지고, 운명이 변할 수 있다는 것을 깨달았어요. 고등학교에 입학하면서 적응하는 데 어려움을 겪었어요. 이곳은 제가 익숙한 환경이 아니었어요. 제가 지낼 예정이었던 종류의 사람들도 아니었어요. 그리고 그 때 나는 내가 다른 세계에 있다는 것을 깨달았어요.\n\n저는 학업적인 인정을 갈망해요.\n모든 사람들의 주목을 받길 갈망해요.\n\n“막 통과만 하면 될 거야”라는 말로 만족했던 어린이가 더 이상 실패를 겁내는 존재가 된 줄 누가 알았을까요? 장난감을 가지는 것만 꿈꿨던 어린이가 이제는 수상과 메달을 받는 꿈을 꿀 줄 누가 알았을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nelementary 학교 때부터 우수생이 되기 시작했는데 그 때는 신경 쓰지 않았어요. 그런데 고등학교에 들어서니 항상 앞서가려는 개처럼 달리기를 하듯 했어요. 대회에서 이기면 주위 사람들로부터 메달, 증명서, 그리고 칭찬을 받았죠.\n\n최고에 오르는 것은 정말로 피곤하고 지칩니다.\n\n난 다른 사람들이 무엇을 말할까 두려워하고, 그들의 기대치를 충족시킬 수 없을까 두려워하며, 실패를 두려워해서 그랬던 걸 깨달았어요.\n\n옛날에는 꿈이 간단했어요 - 탐험하고 배우고, 그 순간 속에서 즐거움을 찾는 거였죠. 그런데 어느 순간 모든 게 변해버렸어요. 다른 사람들의 높은 기대치에 부응하기 위해 우수하게 나아가고, 최고가 되어야 한다는 필요성이 저를 흡수했죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내가 한때 소중히 여기던 메달과 성취들이 이제는 무거운 짐이 되어, 끊임없이 성과를 거듭 증명하고 가치를 입증해야 하는 압박으로 나를 뒤덮고 있다. 내가 정말 내 인생을 살고 있는 것인지, 아니면 누군가의 성공 이상을 쫓고 있는 것인지 의심하기도 한다.\n\n나의 어린 시절에 즐거움을 찾았던 마음이 어디로 사라진 걸까? 엄청 어려워. 실패를 기다리는 사람들 속에서 갇혀 있는 것은 정말 힘들다. 완전히 포위당한 기분인데, 그들의 초상도 미달할까 두려움 속에서 숨이 막히는 듯하다.\n\n하지만 말하길, 모든 일엔 이유가 있다.\n\n메달과 인증서 없이 나를 알 수는 없지만, 왜 그러했는지는 안다. 내가 받은 각각의 메달에는 이야기가 있다. 이것은 내 목에 매다는 단순한 메달이 아니다. 이것은 내 정체성을 가진 존경스러운 것이다. 그것을 얻기 위해 내 눈물, 피, 땀을 바쳤기 때문이다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n나의 소중한 어린이, 널 내가 세운 높은 벽 안에 감췄지만 이제는 널 자유롭게 놓아줄게. 내가 너를 어두운 길로 이끌었던 점 용서해줘.","ogImage":{"url":"/assets/img/2024-06-20-ngunitsinoakokungwalangmedalya_0.png"},"coverImage":"/assets/img/2024-06-20-ngunitsinoakokungwalangmedalya_0.png","tag":["Tech"],"readingTime":3},{"title":"산불에 대해 자세히 알아보기","description":"","date":"2024-06-20 18:15","slug":"2024-06-20-DelvingintoWildfires","content":"\n\n# 머신 러닝 및 데이터로부터의 통찰\n\n![Wildfire](/assets/img/2024-06-20-DelvingintoWildfires_0.png)\n\n야생 산불 데이터를 심층적으로 분석한 후, FireVision의 일부인 머신 러닝 (ML) 모델을 구축했습니다. 빠르게 증가하는 산불 문제는 적절한 데이터와 지표가 있다면 매우 실행 가능하다는 확고한 믿음으로 나타났습니다. 산불에는 무작위성 요소가 있지만, 미친 듯한 일은 방법이 있습니다. 산불 데이터에서 확정적인 패턴이 존재합니다. 이러한 패턴은 우리에게 미래 산불의 측면을 예측하는 모델을 구축하는 것뿐만 아니라 앞으로 몇 년과 몇십 년 동안 대규모 산불의 위험을 어떻게 줄일지에 대해 고려할 수 있도록 합니다.\n\n지난 30년간의 역사적 산불 데이터로부터 얻은 일부 통찰을 시작으로, 2040년까지의 미래에 대해 ML 모델이 우리에게 말하는 내용에 대해 전환하겠습니다. 산불 이야기를 숫자와 차트 및 비디오 애니메이션 형식의 패턴을 사용하여 전달하고 앞으로 우리가 할 수 있는 일에 대해 언급할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 역사적 산불에서 배운 것들\n\n1992년부터 2020년까지 미국 농림부 산림 서비스에서 보고한 미국의 산불 기록은 훌륭합니다. 미 국에서 산불 피해의 주된 부담을 갖고 있는 11개 서부 연이은 주에서 산불이 계속해서 어떻게 진화하는지를 나타내는 차트를 사용하겠습니다.\n\n산불은 자연적 또는 인간적인 원인으로 발생할 수 있습니다. 주된 자연적 원인은 번개이며, 이는 우리가 ML 모델에 대해 이야기할 때 다시 다루도록 하겠습니다. 오픈 버닝부터 고장난 전선까지 총 열여섯 가지의 다양한 인간적 원인이 있습니다.\n\n자연적 원인은 2001년부터 2010년까지 미국 서부에서 발생한 모든 산불 중 36%를 차지하며, 태워진 면적의 68%를 차지했습니다. 그러나 그 다음 10년 동안(2011년부터 2020년)에는 자연적 원인이 산불의 23%와 면적의 57%만 차지했습니다. 인간적 원인은 현재 산불의 77%와 면적의 43%(각각 64%와 32%로부터 증가)를 책임지고 있습니다. 아래의 두 차트는 이를 설명하며, 중요한 추세를 강조하여 다시 한번 이 사실을 확인합니다: 인간 활동이 산불의 주요 원인으로 점점 더 강화되고 있다는 것을. 물론 이러한 사실은 우리에게 나아갈 방향을 제시합니다. 우리는 번개를 컨트롤할 수 없지만, 인간적 원인을 영향을 줄 수 있으며, 잠재적으로 미래 산불의 궤적을 수정할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![DelvingintoWildfires_1](/assets/img/2024-06-20-DelvingintoWildfires_1.png)\n\n![DelvingintoWildfires_2](/assets/img/2024-06-20-DelvingintoWildfires_2.png)\n\n자연 화재의 화룡력(화재 당 연소된 에이커로 양적화)은 원시 지역과 고 지대에서 더 자주 발생하며 급증했습니다. 자연 화재의 화룡력이 2001년부터 2010년과 2011년부터 2020년 사이에 2배 증가했습니다. 여름철 기온이 높고 건조한 기후로 인해 화재가 매우 커지고 격리하기 어려워졌다고 합니다. 인간이 일으킨 화재의 화룡력도 약 50% 증가했으며, 평균 화룡력은 여전히 자연 화재의 4분의 1 수준입니다. 이는 인간이 일으킨 화재가 일반적으로 도시 근처나 농장 주변 등에서 발생하여 더 쉽게 감지하고 진압할 수 있기 때문일 수 있습니다.\n\n![DelvingintoWildfires_3](/assets/img/2024-06-20-DelvingintoWildfires_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인간 활동으로 인한 화재 중에서 가장 강도가 높으며 증가 추세에 있는 것은 전력 송전선 및 장비로 인한 화재입니다. 전력 송전선이 야생지대를 통과하기 때문에 대부분의 다른 인간 활동으로 인한 화재에 비해 접근이 어렵기 때문에 이러한 특징이 나타나는 것으로 생각됩니다. 그렇지만 전력 송전은 미디어에서 과대 보도되는 화재로써 2%의 화재 및 3%의 태우어진 면적을 차지하고 있습니다(이전에는 각각 1% 및 2%였음). \n\n![이미지](/assets/img/2024-06-20-DelvingintoWildfires_4.png)\n\n![이미지](/assets/img/2024-06-20-DelvingintoWildfires_5.png)\n\n![이미지](/assets/img/2024-06-20-DelvingintoWildfires_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n서부 주들을 가로지르는 역사적인 산불에는 공간 패턴이 있을까요? 산불은 항상 매우 낮은 확률의 사건입니다. 환경이 좋아져도 — 예를 들어 따뜻하고 건조한 기간에 땅 위에 연소 가능한 연료가 많아져도 — 산불이 발생하지 않을 수 있습니다. 따라서 역사적인 기록은 산불 위험을 반영하지 않으며 공간적으로는 비교적 희소합니다. 그러나 1992년부터 2020년까지 약 30년 동안의 데이터를 살펴보면 그 안에 주목할 만한 패턴이 있습니다.\n\nFireVision Historical Maps의 아래 화재 크기 분포를 보면 대규모 산불은 일반적으로 해안과 인구 집중 지역에서 떨어진 곳에서 발생했으며, 아마도 더 높은 고도에서 발생했을 것입니다. 워싱턴, 오레곤, 캘리포니아 지역의 대부분의 산불은 작거나 중소형이었지만 남부 오레곤, 동부 오레곤/워싱턴 및 북부 캘리포니아를 제외하고는 그렇지 않습니다. 네바다와 유타 일부 지역은 흰색 공간으로 나타나는 것처럼 기록된 산불이 전혀 발생하지 않았습니다.\n\n다음 화재 원인 분포는 어느 정도로 화재 크기를 따릅니다. 해변과 인구 집중 지역에서 떨어진 곳 및 보다 높은 고도에서 발생한 대규모 산불들이 자연적인 원인으로 발화될 가능성이 높습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![2024-06-20-DelvingintoWildfires_8.png](/assets/img/2024-06-20-DelvingintoWildfires_8.png)\n\n화재의 특성은 과거 한 해 동안 어떻게 변했을까요? 다음 세 가지 차트(그리고 몇 가지 추가 데이터)는 2011년부터 2020년까지의 화재에 대해 몇 가지 중요한 사항을 강조합니다:\n\n- 소규모 화재(` 100 에이커 미만)가 주를 이룹니다. 실제로 모든 화재의 거의 97%를 차지합니다. 1% 조금 넘는 화재는 대규모(1000 에이커 이상)입니다.\n- 모든 규모의 화재가 여름에 절정을 이룹니다. 8월이 화재의 정점이었지만, 최근에 그 정점이 7월로 이동하는 것으로 보입니다.\n- 자연 발화 화재(주로 번개에 의해 발생)는 여름에 정점을 이루며, 10월부터 4월까지는 무시해도 될 정도입니다.\n\n![2024-06-20-DelvingintoWildfires_9.png](/assets/img/2024-06-20-DelvingintoWildfires_9.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Delving into Wildfires 10](/assets/img/2024-06-20-DelvingintoWildfires_10.png)\n\n![Delving into Wildfires 11](/assets/img/2024-06-20-DelvingintoWildfires_11.png)\n\n# 왜 머신 러닝을 사용해야 할까요?\n\n과거 산불 데이터를 살펴보고 직관을 개발하는 것은 유용하지만, 특정 장소에서 미래에 일어날 일을 예측하는 데 충분하지는 않습니다. 여기서 머신 러닝이 모델링 기술로 등장합니다. ML은 복잡한 상호작용에서 발생하는 패턴들이 있는 산불 특성과 같은 것들을 예측할 때 특히 유용할 수 있습니다. 이러한 패턴은 다양한 위치별 변수들의 복잡한 상호작용에서 발생합니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 지표면 연료\n- 수연 구조\n- 지형\n- 최근의 날씨 데이터를 기반으로 한 불난 날씨 지수(FWI) 및 그 구성 요소, 그리고 미래 연도를 위한 고해상도 기후 예측에 따른 것\n- 번개 충동 밀도 및 충동당 전력\n\n상호 작용은 복잡하여 특정 산불의 특성을 명확하게 결정할 수 있는 단일 변수(다른 모든 것을 일정하게 유지하면서)를 식별하는 것이 종종 어려울 수 있습니다. 이차 및 그 이상의 상호 작용이 지배합니다. 특히 심층 신경망은 이러한 유형의 문제를 모델링하는 데 최적으로 적합하며, 그것이 바로 우리가 구현한 방법입니다. FireVision 사용 설명서에서 ML 방법론 및 모델링에 대해 자세히 알아보세요.\n\n# 기계 학습에서 배운 것들\n\n## 산불 위험 지수\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 스크린샷은 2021년부터 2023년까지의 평균 산불 날씨 조건을 사용하여 7월에 서부 주에서 새로운 번개로 인한 산불 위험 지수(LFRI)를 20km 그리드로 보여줍니다. 이 지수는 특정 시기에 특정 위치의 산불 취약성(잠재적 산불 규모에 의한 측정, 다음 섹션 참조)과 번개 발생 밀도를 결합합니다. 더 높은 LFRI는 자연적인 원인에서 더 큰 산불이 발생할 확률이 높음을 나타냅니다.\n\n아래 보이는 패턴은 이전 섹션에서 실제 역사적 산불 규모의 패턴과 매우 유사합니다: 해안과 인구 중심지에서 멀리 떨어진 곳에서 더 높은 위험 지수 값이 발생하며, 서부 주 중앙에 집중되어 있습니다. 이는 예측 ML 모델의 결과이므로 우리는 더 이상 역사적 기록에 의존하지 않으며, 공백이 없습니다: FireVision Risk Maps를 사용하여 설명을 위해 20km x 20km 그리드 상의 모든 지점 위치를 평가했으며 해당 공간의 모든 지점에 위험 값이 주어졌습니다.\n\n이것이 화재 시즌의 정점에서 위험 분포인 경우, 이 위험 지수는 연중 어떻게 변하는가요? 아래 비디오 클립에서는 산불 날씨 데이터의 2021년부터 2023년 평균 및 2010년부터 2023년까지의 평균 번개 기후를 사용하여 5월부터 11월까지 이를 보여줍니다. 일반적으로 화재 시즌 시작 직전인 5월에는 서부 주의 동부 및 남동부 지역에서 위험이 이미 상당히 높습니다. 11월에는 정점을 훌쩍 넘은 후로 위험이 극히 낮아 보입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nML 모델의 매력은 우리가 관심을 두는 공간의 각 위치에서 화재 위험을 평가할 뿐만 아니라 미래를 예측할 수 있다는 점입니다. 기후가 변화하고 여름이 더 더워지고 건조해짐에 따라 화재 위험이 증가할 것으로 예상됩니다. 이미 지난 10년 동안 이에 대한 미리보기를 볼 수 있었습니다.\n\n다음 차트는 2040년까지 11개 서부 주의 7500개 위치에 대한 위험 분포의 체계적인 월별 보기를 제시합니다. 앞으로 나아갈수록 고위험 지역의 수가 증가하며, 2040년 7월에는 모든 위치 중 21%가 최고치에 도달할 것으로 예상됩니다(2023년 10%에서 상승).\n\n![화재 위험 차트](/assets/img/2024-06-20-DelvingintoWildfires_13.png)\n\nLFRI와 같은 화재 위험 지수는 대형 화재에 대해 보험 청약서를 작성하거나 위험 담보를 제거하는 등의 다양한 분야에서 활용할 수 있습니다. 또한 숲 얇게 하는 등의 우선 순위가 매겨진 완화 노력을 위한 위치 식별 및 자연 기반 탄소 크레딧의 영구성을 평가하는 데 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 두 개의 비디오 클립은 서부 미국 지역의 5월과 7월 간의 위험 지수 값이 현재부터 2040년까지 어떻게 변화하는지를 보여줍니다.\n\n마지막으로 한 가지 더 짚고 넘어가겠습니다. 번개로 인한 화재 위험 지수는 있지만 인재로 인한 화재 위험 지수는 왜 없을까요? 번개 기후학은 우리가 화재 발생 가능성을 모델링하는 핵심적인 간접적인 방법을 제공합니다. 우리는 이미 언급했듯이 번개는 우리가 통제할 수 없기 때문에 여기서의 발화 가능성은 이미 결정된 상태이며, 우리가 모델링하고 있는 시스템 외부에서 발생하는 것으로 생각할 수 있습니다. 반면에, 인재로 인한 화재는 대부분 우리가 통제할 수 있지만 발화 가능성의 추정은 어려운 것으로 알려져 있습니다 (비록 이론적으로 허용 가능한 수준으로 줄일 수는 있습니다).\n\n## 화재 취약성\n\n우리는 특정 위치에서 발화 사건이 발생했을 때 발생할 화재의 잠재적 크기에 따라 해당 위치의 산불 취약성을 양적으로 정량화합니다. 이것은 발화 사건이 발생했을 때 조건부 확률에 기반합니다. 취약성은 지표면의 연료 조건, 지형의 특성 및 최근 또는 예상되는 화재 기상의 영향을 받는 함수입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nThe susceptibility measure can be a powerful call to action. It is especially valuable information when it comes to human-caused fires, which as we’ve seen are becoming more dominant and largely in our control. If the fire susceptibility within a region is high, then it will require additional measures to proactively prevent ignition events such as faulty power lines, open burning or campfires in that region. Susceptibility numbers can also be used to drive targeted mitigation measures such as removal of forest debris and mechanical thinning of brush/forests.\n\nSimilar to the LFRI chart above, the chart below shows a systematic monthly view of the fire susceptibility distribution at 7500 locations across the western states between now and 2040. The susceptibility peaks in July, exceeding 1000 acres at 20% of all locations in 2023 and increasing to 37% in 2040.\n\n![Chart image](/assets/img/2024-06-20-DelvingintoWildfires_14.png)\n\nThe two video clips below (generated using the FireVision Susceptibility Maps) animate the evolution of fire susceptibility between now and 2040 for the months of May and July.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 화재 발생 원인\n\n다음 차트에서 생성된 월별 화재 원인의 분포는 FireVision Cause Maps를 사용하여 발화 이벤트에 영향을 받습니다. 차트는 간단히 20km 그리드 상의 모든 발화 위치에 대해 월별로 더 가능성있는 산불 원인을 나타냅니다. 번개로 인한 자연 산불은 여름에 훨씬 더 가능성이 크지만, 인간 활동에 의한 산불은 연중 다른 시기에 더 가능성이 큽니다.\n\n![DelvingintoWildfires_15](/assets/img/2024-06-20-DelvingintoWildfires_15.png)\n\n이 차트에서 명확하지 않은 것은 있지만, 이전에 언급한 바와 같이, 인간 활동이 해안 부근 및 인구 집중 지역 근처의 특정 지리적 영역에서 우세할 것이고, 자연적인 원인은 더 범람 지역 및 고도가 더 높은 지역에서 우세할 것이란 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 핵심 포인트\n\n- 인간 활동은 2011년부터 2020년까지 서부 11개 주에서 발생한 화재의 77%와 소실된 면적의 43%를 차지하며, 화재의 주요 원인으로 계속 증가하고 있습니다.\n- 자연 원인 중 번개는 더 범위가 넓은 지역에서 많은 대규모 화재를 유발하며 큰 피해를 입히고 있습니다.\n- 화재 규모는 이 세기의 첫째와 둘째 10년 사이에 최대 2배까지 증가하면서 기후 변화의 분명한 영향을 보여줍니다.\n- 전력 전송선으로 인한 화재는 증가하고 있지만, 여전히 화재의 2%와 이에 따른 면적의 3%에 불과합니다.\n- 기계 학습 모델은 서부 주에서 번개로 인한 화재 위험 지역 수가 현재부터 2040년에는 불정 상 시즌에 10%에서 21%로 두 배 증가할 것으로 예측합니다.\n- 모든 원인에서 대규모 화재에 높은 취약성이 있는 지역 수도 현재부터 2040년까지 거의 2배(20% 에서 37%) 증가할 것입니다.\n- 데이터와 모델 예측은 화재에 대한 행동으로 이어지는 단호한 요청입니다. 우리가 대응할 수 있는 것들이 많이 있습니다. 적절한 지표가 있다면 예방, 완화 및 위험 관리를 통해 많은 것을 할 수 있습니다.\n- 화재에 대한 행동을 촉구할 수 있는 두 가지 강력한 지역별 지표를 소개했습니다: 위험 지수 및 취약성 지표.","ogImage":{"url":"/assets/img/2024-06-20-DelvingintoWildfires_0.png"},"coverImage":"/assets/img/2024-06-20-DelvingintoWildfires_0.png","tag":["Tech"],"readingTime":9}],"page":"44","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"44"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>