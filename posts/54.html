<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/54" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/54" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="2024년을 위한 SQL에서 재귀 CTE 사용 방법 알아야 할 모든 것" href="/post/2024-06-20-HowtoUseRecursiveCTEsinSQLAllYouNeedToKnowin2024"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="2024년을 위한 SQL에서 재귀 CTE 사용 방법 알아야 할 모든 것" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-HowtoUseRecursiveCTEsinSQLAllYouNeedToKnowin2024_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="2024년을 위한 SQL에서 재귀 CTE 사용 방법 알아야 할 모든 것" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">2024년을 위한 SQL에서 재귀 CTE 사용 방법 알아야 할 모든 것</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="마법같은 메이지 글로벌 후크의 마법을 풀어보세요" href="/post/2024-06-20-UnleashtheMagicofMageGlobalHooks"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="마법같은 메이지 글로벌 후크의 마법을 풀어보세요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-UnleashtheMagicofMageGlobalHooks_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="마법같은 메이지 글로벌 후크의 마법을 풀어보세요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">마법같은 메이지 글로벌 후크의 마법을 풀어보세요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="아파치 드루이드 아키텍처" href="/post/2024-06-20-TheArchitectureofApacheDruid"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="아파치 드루이드 아키텍처" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-TheArchitectureofApacheDruid_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="아파치 드루이드 아키텍처" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">아파치 드루이드 아키텍처</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Sridhar Ramaswamy가 누구이며, 이것이 Snowflake에 미치는 의미는 무엇인가요" href="/post/2024-06-20-WhoisSridharRamaswamyandwhatdoesthismeanforSnowflake"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Sridhar Ramaswamy가 누구이며, 이것이 Snowflake에 미치는 의미는 무엇인가요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-WhoisSridharRamaswamyandwhatdoesthismeanforSnowflake_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Sridhar Ramaswamy가 누구이며, 이것이 Snowflake에 미치는 의미는 무엇인가요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Sridhar Ramaswamy가 누구이며, 이것이 Snowflake에 미치는 의미는 무엇인가요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 엔지니어링 세계를 여행하며 발견한 것들" href="/post/2024-06-20-MyjourneyintheDataEngineeringworld"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 엔지니어링 세계를 여행하며 발견한 것들" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-MyjourneyintheDataEngineeringworld_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 엔지니어링 세계를 여행하며 발견한 것들" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 엔지니어링 세계를 여행하며 발견한 것들</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 수집 - 파트 1 아키텍처 패턴" href="/post/2024-06-20-DataIngestionPart1ArchitecturalPatterns"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 수집 - 파트 1 아키텍처 패턴" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-DataIngestionPart1ArchitecturalPatterns_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 수집 - 파트 1 아키텍처 패턴" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 수집 - 파트 1 아키텍처 패턴</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="왜 매년 이 10일에 데이터 파이프라인이 실패할 것이며 그리고 대처하는 방법" href="/post/2024-06-20-WhyYourDataPipelinesWillFailOnThese10DaysEveryYearAndWhatToDoAboutIt"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="왜 매년 이 10일에 데이터 파이프라인이 실패할 것이며 그리고 대처하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-WhyYourDataPipelinesWillFailOnThese10DaysEveryYearAndWhatToDoAboutIt_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="왜 매년 이 10일에 데이터 파이프라인이 실패할 것이며 그리고 대처하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">왜 매년 이 10일에 데이터 파이프라인이 실패할 것이며 그리고 대처하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="3분 안에 Airflow와 DAG 이해하기" href="/post/2024-06-20-UnderstandingAirflowandDAGsin3minutes"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="3분 안에 Airflow와 DAG 이해하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-UnderstandingAirflowandDAGsin3minutes_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="3분 안에 Airflow와 DAG 이해하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">3분 안에 Airflow와 DAG 이해하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Snowflake를 위한 CICD 및 DevOps 포괄적인 가이드" href="/post/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Snowflake를 위한 CICD 및 DevOps 포괄적인 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Snowflake를 위한 CICD 및 DevOps 포괄적인 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Snowflake를 위한 CICD 및 DevOps 포괄적인 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Chronon, 에어비앤비의 기계 학습 피처 플랫폼, 이제 오픈 소스로 공개합니다" href="/post/2024-06-20-ChrononAirbnbsMLFeaturePlatformIsNowOpenSource"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Chronon, 에어비앤비의 기계 학습 피처 플랫폼, 이제 오픈 소스로 공개합니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-ChrononAirbnbsMLFeaturePlatformIsNowOpenSource_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Chronon, 에어비앤비의 기계 학습 피처 플랫폼, 이제 오픈 소스로 공개합니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Chronon, 에어비앤비의 기계 학습 피처 플랫폼, 이제 오픈 소스로 공개합니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/41">41</a><a class="link" href="/posts/42">42</a><a class="link" href="/posts/43">43</a><a class="link" href="/posts/44">44</a><a class="link" href="/posts/45">45</a><a class="link" href="/posts/46">46</a><a class="link" href="/posts/47">47</a><a class="link" href="/posts/48">48</a><a class="link" href="/posts/49">49</a><a class="link" href="/posts/50">50</a><a class="link" href="/posts/51">51</a><a class="link" href="/posts/52">52</a><a class="link" href="/posts/53">53</a><a class="link posts_-active__YVJEi" href="/posts/54">54</a><a class="link" href="/posts/55">55</a><a class="link" href="/posts/56">56</a><a class="link" href="/posts/57">57</a><a class="link" href="/posts/58">58</a><a class="link" href="/posts/59">59</a><a class="link" href="/posts/60">60</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"2024년을 위한 SQL에서 재귀 CTE 사용 방법 알아야 할 모든 것","description":"","date":"2024-06-20 15:40","slug":"2024-06-20-HowtoUseRecursiveCTEsinSQLAllYouNeedToKnowin2024","content":"\n\n## 데이터 과학\n\n![이미지](/assets/img/2024-06-20-HowtoUseRecursiveCTEsinSQLAllYouNeedToKnowin2024_0.png)\n\n![이미지](/assets/img/2024-06-20-HowtoUseRecursiveCTEsinSQLAllYouNeedToKnowin2024_1.png)\n\n공통 테이블 표현식인 CTE는 SQL에서 가장 강력하고 널리 사용되는 도구 중 하나입니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCTE(공통 테이블 식)를 사용하면 복잡한 쿼리를 간소화하고 가독성이 좋고 유지보수가 쉬운 SQL 쿼리를 작성할 수 있습니다.\n\nCTE를 사용하면 쿼리 결과에서 임시 테이블을 생성할 수 있습니다. 사실, 복잡한 서브쿼리를 간단한 CTE로 분해할 수도 있어요.\n\nCTE에 대해 더 알아보려면 제 이전 이야기 중 하나를 읽어보세요.\n\n그러나 CTE를 더 강력하게 만드는 것은 계층 데이터를 분석하는 능력입니다. CTE는 재귀를 지원하여 서로 다른 엔티티 간의 관계를 분석하기 위해 자신을 참조할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단한 시나리오로 중첩된 인형 예시를 들어보겠습니다 —\n\n![인형 사진](/assets/img/2024-06-20-HowtoUseRecursiveCTEsinSQLAllYouNeedToKnowin2024_2.png)\n\n세트 안에 몇 개의 인형이 있는지 정확히 알고 싶을 때, 동일한 세트를 계속해서 살펴봐야 하며 가장 작은 인형을 찾아야 합니다. 가장 작은 인형은 내부에 더 작은 인형이 없는 인형으로, 이것이 세트 내 인형 수를 세는 중단 조건이 됩니다.\n\n재귀 공통 테이블 표현식(CTE)은 정확히 동일한 논리를 따르며 여러 레이어가 서로 아래에 존재하는 데이터 집합을 탐색하는 데 도움을 줍니다. 재귀 CTE의 각 반복에서는 특정 중단 조건을 만날 때까지 데이터 구조의 한 수준을 탐색합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현실에서는 소셜 미디어 네트워크(예: 페이스북의 친구 목록), 슬랙의 채팅 스레드, 회사의 직원 구조(고위 경영진부터 개별 직원까지)와 같은 계층적 데이터 유형을 볼 수 있습니다. 재귀 CTE는 이러한 데이터를 다룰 때 매우 유용할 수 있습니다.\n\n# 재귀 CTE의 기본 사항\n\n모든 재귀 CTE에는 두 가지 주요 부분이 포함됩니다 —\n\n- 앵커 부분 — 주 쿼리 또는 초기 쿼리라고 할 수 있습니다. 따라서 재귀 부분에서 참조할 수 있는 시작점입니다.\n- 재귀 부분 — 재귀 CTE의 두 번째 부분으로, 앵커 부분을 참조하고 일정 조건을 충족하는 한 반복적으로 실행됩니다. 따라서 한 번 반복의 결과가 다음 반복의 입력으로 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 두 가지 기본 내용을 처음에 이해하지 못해도 전혀 괜찮아요!\n\n다음 두 사용 사례를 탐험하면서 예제로 설명해드릴 거예요. 이 두 가지 사용 사례에서 재귀 CTE를 사용할 수 있어요.\n\n# 계층적 데이터 다루기\n\n계층 데이터는 이름에서 알 수 있듯이 트리 형식이거나 부모-자식 관계로 구성된 데이터를 포함하고 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기관에서는 관리자 - 직원 또는 폴더 - 하위 폴더 구조와 같은 데이터 유형을 일반적으로 관찰할 수 있습니다.\n\n따라서 계층 구조에서 항목은 그 아래에 있는 모든 다른 항목의 '상위 항목'입니다.\n\n재귀 공통 테이블 식(CTE)은 이러한 종류의 데이터에서 통찰력을 얻는 데 매우 유용할 수 있습니다. 자기 자신을 참조하는 CTE를 활용하여 전체 계층 구조를 검색할 수 있습니다.\n\n이 개념을 이해하는 데 가장 좋은 예시를 살펴보겠습니다. 조직적인 계층 구조의 매우 흔한 예시를 살펴보죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 표에 표시된 대로 직원 테이블을 가지고 있고 각 목록이 관리자부터 직원까지의 경로를 나타내는 쉼표로 구분된 목록을 얻고 싶다고 가정해보세요.\n\n![Employee Table](/assets/img/2024-06-20-HowtoUseRecursiveCTEsinSQLAllYouNeedToKnowin2024_3.png)\n\n다음 쿼리를 사용하여 이 입력 데이터를 다시 생성할 수 있습니다.\n\nMySQL Workbench에 있는 analyticswithsuraj는 스키마 이름입니다. 여러분의 스키마로 대체할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nDROP TABLE IF EXISTS analyticswithsuraj.employee;\nCREATE TABLE analyticswithsuraj.employee (\n    EmployeeID VARCHAR(10),\n    EmployeeName VARCHAR(50),\n    ManagerID VARCHAR(10)\n);\n\n-- Insert sample data\nINSERT INTO analyticswithsuraj.employee VALUES (1, 'John', NULL);\nINSERT INTO analyticswithsuraj.employee VALUES (2, 'Jane', 1);\nINSERT INTO analyticswithsuraj.employee VALUES (3, 'Bob', 1);\nINSERT INTO analyticswithsuraj.employee VALUES (4, 'Alice', 2);\nINSERT INTO analyticswithsuraj.employee VALUES (5, 'Charlie', 2);\nINSERT INTO analyticswithsuraj.employee VALUES (6, 'David', 3);\nINSERT INTO analyticswithsuraj.employee VALUES (7, 'Eva', 3);\n```\n\n문제에 돌아가서... 완전한 해결책을 보여드릴게요. 그리고 그 후에 설명을 따라 읽어볼게요.\n\n```js\nWITH RECURSIVE RecursiveCTE AS (\n    SELECT\n        EmployeeID,\n        EmployeeName,\n        ManagerID,\n        EmployeeName AS Path -- 초기 경로는 직원의 ID입니다\n    FROM\n        analyticswithsuraj.employee\n    WHERE\n        ManagerID IS NULL -- 앵커 부분\n\n    UNION ALL\n\n    SELECT\n        e.EmployeeID,\n        e.EmployeeName,\n        e.ManagerID,\n        concat_ws(',', rc.Path, e.EmployeeName) AS Path\n    FROM\n        analyticswithsuraj.employee e\n    JOIN\n        RecursiveCTE rc ON e.ManagerID = rc.EmployeeID -- 재귀 부분\n)\n```\n\nMySQL Workbench에서 재귀 CTE를 작성하려면 CTE 이름 앞에 RECURSIVE 키워드를 입력해야 합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCaveat: 재귀 CTE의 기본 개념을 읽으신다면, 솔루션은 두 부분으로 이뤄져야 합니다.\n\n첫 번째는 시작점인 앵커 부분입니다. 재귀 쿼리에서 참조할 수 있는 시작점으로, 아래와 같이 간단한 SELECT 문으로 표현됩니다.\n\n```js\n    SELECT\n        EmployeeID,\n        EmployeeName,\n        ManagerID,\n        EmployeeName AS Path -- 초기 경로는 직원의 ID만 포함합니다\n    FROM\n        analyticswithsuraj.employee\n    WHERE\n        ManagerID IS NULL -- 앵커 부분\n```\n\n여기서는 직원의 전체 계층 구조를 저장할 추가적인 Path 열을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 앵커 쿼리는 항상 ManagerID가 NULL 인 직원만 포함합니다. 즉, 그들 위에 매니저가 없는 직원만을 선택합니다.\n\n재귀 CTE의 두 번째 부분은 재귀 부분으로, 직원 이름을 선택하고 Path 열의 값을 업데이트하여 아래와 같이 쉼표로 구분된 경로를 만듭니다.\n\n```js\n    UNION ALL\n\n    SELECT\n        e.EmployeeID,\n        e.EmployeeName,\n        e.ManagerID,\n        concat_ws(', ', rc.Path, e.EmployeeName) AS Path\n    FROM\n        analyticswithsuraj.employee e\n    JOIN\n        RecursiveCTE rc ON e.ManagerID = rc.EmployeeID -- Recursive part\n```\n\n보시다시피 이 재귀 부분은 원래의 직원 테이블에서 데이터를 조회하고 안에 쓰여진 CTE와 조인합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nUNION ALL은 앵커 쿼리와 순환 쿼리의 결과를 결합하는 데 사용됩니다.\n\n최종적으로는 이 순환 CTE를 간단한 SELECT 문으로 쿼리할 수 있습니다.\n\n```js\nSELECT\n    EmployeeID,\n    EmployeeName,\n    ManagerID,\n    Path\nFROM\n    RecursiveCTE;\n```\n\n![이미지](/assets/img/2024-06-20-HowtoUseRecursiveCTEsinSQLAllYouNeedToKnowin2024_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과적으로 각 레코드마다 직원의 완전한 계층 구조를 볼 수 있습니다. 여기에는 John이 최종 관리자로 나타나며 그의 상사가 없는 것을 나타내고, Jane과 Bob이 그의 직속 보고자들이라는 것을 보여줍니다.\n\n입력 테이블에서 4번째 레코드에서 Jane이 Alice의 관리자이고 John이 Jane의 관리자인 것을 볼 수 있으므로 Alice의 전체 계층은 'John, Jane, Alice'입니다.\n\n그러나 데이터는 단순한 계층보다 복잡할 수 있습니다. Facebook 친구 목록이나 다른 소셜 네트워킹 데이터와 같이 요롯한 경우도 있습니다. 재귀 CTE가 어떻게 활용될 수 있는지 살펴보겠습니다.\n\n# 네트워크 데이터 처리하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n네트워크 데이터는 이름에서 알 수 있듯이 사물, 엔티티 및 사람들의 네트워크에 대한 것입니다. 아래 그림에서 보이는 것처럼 네트워크 데이터를 노드(블록)와 노드를 연결하는 엣지(선)로 시각화할 수 있습니다.\n\n![그림](/assets/img/2024-06-20-HowtoUseRecursiveCTEsinSQLAllYouNeedToKnowin2024_5.png)\n\n노드는 사용자 또는 엔티티를 나타내며, 엣지는 그들 사이의 관계를 나타냅니다.\n\n간단히 말해서, 두 노드 사이에 엣지(선)가 있다면, 두 노드는 직접적으로 서로 연결되어있다는 것을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 테이블 `network_connections`에 표시된 11명의 친구들 중 파올로와 연결된 사람들을 분석하고 있습니다. 파올로의 친구와 친구의 친구들을 파악하고 싶습니다.\n\n```js\nDROP TABLE IF EXISTS alldata.network_connections;\nCREATE TABLE alldata.network_connections (\n    source_node VARCHAR(50),\n    target_node VARCHAR(50)\n);\n\nINSERT INTO alldata.network_connections (source_node, target_node) VALUES\n('Paolo', 'David'),\n('Paolo', 'Anna'),\n('David', 'Mark'),\n('Anna', 'Peter'),\n('Samar', 'Patrik'),\n('Mark', 'Vivan'),\n('Patrik', 'Maya'),\n('Julia', 'Robert');\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알다타는 MySQL Workbench에서의 스키마 이름입니다. 이를 여러분의 스키마 이름으로 바꿔 사용하실 수 있습니다.\n\n이전 계층 데이터의 예제에서 이미 앵커 부분과 재귀 부분을 설명했으니, 여기서 바로 솔루션으로 넘어가겠습니다.\n\n```js\nWITH RECURSIVE NetworkCTE AS (\n    -- 시작 노드를 선택하는 앵커 부분\n    SELECT source_node,\n            target_node\n    FROM network_connections\n    WHERE source_node = 'Paolo' -- 다른 사람의 네트워크를 보려면 여기를 변경하세요\n    \n    UNION ALL\n    \n    -- 연결된 노드를 선택하는 재귀 부분\n    SELECT nc.source_node,\n            nc.target_node\n    FROM network_connections nc\n    JOIN NetworkCTE n ON nc.source_node = n.target_node\n)\n\nSELECT * FROM NetworkCTE;\n```\n\n코드에서 설명한 대로, 앵커 부분은 먼저 모든 레코드를 가져와 Paolo를 소스 노드로 하는 것 즉, Paolo의 직접적인 친구들을 모두 가져옵니다. 반면 재귀 부분은 Paolo의 친구의 친구를 모두 가져옵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서 Paolo의 네트워크를 최종적으로 다음과 같이 볼 수 있습니다.\n\n![image](/assets/img/2024-06-20-HowtoUseRecursiveCTEsinSQLAllYouNeedToKnowin2024_7.png)\n\n네트워크 데이터 분석의 다른 사용 사례들은 공급망 산업에서 서로 다른 개체간의 의존 관계를 분석하거나 재무 거래 분석에 계정을 통해 자금 이동을 추적하는 데 사용될 수 있습니다.\n\n더 많은 재귀 CTE를 효과적으로 사용할 수 있는 예시가 있다면 댓글에 언급하지 않을래요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글이 유용하고 정보가 풍부하게 느껴졌으면 좋겠어요!\n\n공통 테이블 표현식(CTE)은 SQL에서 널리 사용되며 재귀 CTE는 특수한 경우입니다. 이 간편한 글에서 재귀 CTE를 사용하여 계층적 및 네트워크 데이터를 다루는 방법을 알아보았어요.\n\n여기서 실제 예시를 통해 배운 것처럼, 특정 시나리오에서 SQL에서 재귀 CTE가 게임 체인저가 될 수 있다는 점을 알게 되었어요. 동일한 내용에 대한 다른 통찰이 있으면 댓글로 자유롭게 공유해주시기 바랍니다!\n\n💡 저를 팔로우하고 제 이메일 목록에 가입하여 데이터 과학, SQL, Python 및 취업 검색 팁에 관한 다른 글을 더 이상 놓치지 않도록 해보세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n읽어 주셔서 감사합니다!","ogImage":{"url":"/assets/img/2024-06-20-HowtoUseRecursiveCTEsinSQLAllYouNeedToKnowin2024_0.png"},"coverImage":"/assets/img/2024-06-20-HowtoUseRecursiveCTEsinSQLAllYouNeedToKnowin2024_0.png","tag":["Tech"],"readingTime":8},{"title":"마법같은 메이지 글로벌 후크의 마법을 풀어보세요","description":"","date":"2024-06-20 15:39","slug":"2024-06-20-UnleashtheMagicofMageGlobalHooks","content":"\n\n\n![Global hooks in Mage](/assets/img/2024-06-20-UnleashtheMagicofMageGlobalHooks_0.png)\n\n# TLDR\n\nMage의 글로벌 훅은 API 작업 앞뒤에서 사용자 정의 코드를 실행할 수 있는 강력한 기능입니다. 이를 통해 응용 프로그램의 다양한 구성 요소 간에 기능을 확장하거나 외부 시스템과 통합하거나 데이터를 유효성 검사하는 유연성을 제공합니다. 대상 조건과 비동기 실행을 통해, 글로벌 훅은 세밀한 제어와 성능 최적화를 제공합니다.\n\n![Global hooks GIF](https://miro.medium.com/v2/resize:fit:1400/1*OirhBHxPRCvHwConOgnGgQ.gif)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 개요\n\n- 글로벌 훅이란?\n- 왜 글로벌 훅을 사용해야 하나요?\n- 글로벌 훅 생성하기\n- 결론\n\n# 글로벌 훅이란?\n\n매지(Mage)의 글로벌 훅은 애플리케이션 실행 주기 중 특정 시점에 사용자 정의 코드를 실행할 수 있는 강력한 기능입니다. 이 훅은 데이터 유효성 검사, 변환 또는 외부 시스템과의 통합과 같은 다양한 작업을 수행하는 데 사용할 수 있습니다. 글로벌 훅은 반복적인 작업을 자동화하거나 애플리케이션의 여러 구성 요소에 걸쳐 특정 비즈니스 규칙을 강제 적용해야 할 때 특히 유용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n메이지(Mage)의 글로벌 훅(Global Hooks)은 파이프라인 실행 중 두 가지 다른 시점에서 트리거될 수 있습니다:\n\n- 블록 완료 전(pre-completion of a Block): 해당 훅은 파이프라인의 특정 블록이 실행되기 전에 실행됩니다. 이를 통해 해당 블록이 데이터를 처리하기 전에 데이터 유효성 검사, 변환 또는 데이터 풍부화와 같은 작업을 수행할 수 있습니다.\n- 블록 완료 후(post-completion of a Block): 해당 훅은 파이프라인의 특정 블록 실행이 완료된 후에 실행됩니다. 이를 통해 해당 블록의 출력을 기반으로 외부 시스템 통합, 로깅, 감사 또는 하류 프로세스 트리거와 같은 작업을 수행할 수 있습니다.\n\n이 두 실행 지점을 활용하여, 글로벌 훅(Global Hooks)은 데이터 파이프라인의 기능을 확장하는 유연한 방법을 제공합니다. 특정 요구 사항에 따라 블록 실행 전후에 사용자 정의 코드를 실행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 전역 후크의 필요성은 무엇인가요?\n\n개발자들에게 전역 후크는 애플리케이션의 기능을 확장하는 강력하고 유연한 방법을 제공하여 코드 재사용과 유지 보수성을 증진시킵니다. 특정 지점에서 사용자 정의 코드의 실행을 허용함으로써, 전역 후크는 다른 구성 요소 사이에서 반복적으로 동일한 논리를 수동으로 구현해야 하는 문제를 해결합니다. 이 중앙 집중식 접근 방식은 시간과 노력을 절약해주며, 일관성을 유지하고 개발자가 핵심 응용프로그램 논리에 집중할 수 있게 하면서 로깅, 보안 또는 외부 시스템 통합과 같은 사업을 가로지르는 문제를 쉽게 통합할 수 있도록 지원합니다.\n\n전역 후크는 여러 구성 요소나 모듈에 걸쳐 있는 로깅, 오류 처리, 인증 및 권한 부여와 같은 사업을 가로지르는 문제를 구현하는 데 유용할 수 있습니다. 또한, 중앙 집중점을 제공함으로써 외부 서비스나 API와의 통합을 용이하게 할 수 있습니다. 게다가, 전역 후크는 애플리케이션 내에서 비즈니스 규칙, 데이터 유효성 검사 및 변환 논리를 일관되게 강화할 수 있습니다. 많은 구성 요소와 데이터 흐름이 있는 복잡한 애플리케이션의 경우, 전역 후크는 공유 기능을 캡슐화함으로써 코드 재사용과 유지 보수성을 증진시킬 수 있습니다.\n\n후크를 만들기 전에, 개발자들은 전역 후크 설정을 켜놓았는지 확인해야 합니다. Mage Project Overview 페이지에서 왼쪽 탐색 메뉴에서 설정을 선택하십시오. 설정 페이지에 들어가서 전역 후크를 켭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![마법의 마법을 발휘하라](/assets/img/2024-06-20-UnleashtheMagicofMageGlobalHooks_1.png)\n\n# 훅 만들기\n\n만약 여러분이 팀 내에서 다양한 파이프라인을 소유하고 있고, 파이프라인 이름에 우리의 이니셜을 접두어로 쓰고 싶어서 쉽게 식별할 수 있도록 하고 싶다면 어떻게 해야 할까요?\n\n- 먼저 mage를 열고 새 파이프라인을 만드세요\n- mage가 없다면 시작하는 방법에 대한 문서를 확인하세요\n- 파이프라인 편집기 페이지에 들어간 후, 아래 그림과 같이 데이터 로더 블록을 선택한 후, 파이썬 위에 마우스를 올리고 일반 (템플릿 없음) 옵션을 클릭하세요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-UnleashtheMagicofMageGlobalHooks_2.png\" /\u003e\n\n- 해당 블록이 로드되면 내용을 삭제한 뒤 아래 코드를 추가해 주세요.\n\n```js\nimport os\nfrom datetime import datetime\n\nfrom mage_ai.settings.repo import get_repo_path\n\n@data_loader\ndef load_data(*args, **kwargs):\n    with open(os.path.join(get_repo_path(), f'ping_{datetime.utcnow().timestamp()}'), 'w') as f:\n        f.write('')\n\n    payload = kwargs.get('payload', {})\n    name = payload.get('name')\n\n    payload['name'] = f'cff_{name}'\n\n    return payload\n```\n\n- 접두사 'cff'를 본인의 이니셜로 바꾸거나 코드를 그대로 둘 수 있습니다.\n- Shift + Enter를 눌러 블록을 실행하세요 (블록은 `본인의 이니셜_None`을 반환해야 합니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMage는 개발자들이 대부분의 요구 사항을 선택하거나 전환할 수 있는 사용하기 쉬운 Global Hooks 사용자 인터페이스를 개발했습니다. 주요 파이프라인 페이지의 왼쪽 네비게이션 팝업에서 글로벌 후크를 선택하여 Global Hooks UI로 이동해 봅시다.\n\n- + 새 글로벌 후크 추가 선택\n- Global Hooks UI로 이동하면 후크에 고유한 이름, 리소스 유형 및 작업 유형을 생성하세요.\n- 각 후크에는 Mage의 API 호출인 리소스 유형을 선택해야 합니다. 파이프라인 리소스를 선택하면 후크가 파이프라인 API 호출에 영향을 미칩니다.\n- 새 파이프라인을 만들 때도 이 후크를 사용하므로 작업 유형을 만들기로 선택합니다.\n\n![이미지](/assets/img/2024-06-20-UnleashtheMagicofMageGlobalHooks_3.png)\n\n- 위에서 언급한 대로 후크는 작업이 시작되기 전이나 후에 실행됩니다. 이 경우 파이프라인 이름 앞에 이니셜을 삽입하면 작업이 시작되기 전에 실행됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-20-UnleashtheMagicofMageGlobalHooks_4.png)\n\n- 다음으로 실행 드롭다운을 클릭하고 후크를 위해 파이프라인을 선택하세요.\n- 후크의 스냅샷을 생성하려면 '스냅샷 업데이트' 버튼을 클릭하세요. 스냅샷이 유효할 때 개발자들은 초록색 스냅샷 유효 표시를 볼 수 있습니다.\n\n![이미지](/assets/img/2024-06-20-UnleashtheMagicofMageGlobalHooks_5.png)\n\n- 프로젝트에서 블록 이름으로 데이터를 추출할 블록을 추가하세요.\n- 병합될 데이터의 출력 유형을 선택하세요.\n- 마지막으로 화면 하단의 '변경 사항 저장'을 클릭하여 후크를 저장하세요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-UnleashtheMagicofMageGlobalHooks_6.png\" /\u003e\n\n- 이제 파이프라인 생성 페이지로 이동하여 + 새로 만들기 버튼을 클릭하세요.\n- 파이프라인을 위한 새로운 이름을 입력하세요.\n- 생성을 클릭하세요.\n\n\u003cimg src=\"/assets/img/2024-06-20-UnleashtheMagicofMageGlobalHooks_7.png\" /\u003e\n\n위에서 안내한 대로 Global Hook을 구성했다면, 데이터 로더 블록에서 제공한 이니셜로 모든 파이프라인 이름이 시작됨을 확인할 수 있을 것입니다. 회사는 파이프라인을 생성하고 소유하는 개발자들이 쉽게 파이프라인을 찾을 수 있도록 명명 규칙을 만들 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-20-UnleashtheMagicofMageGlobalHooks_8.png)\n\n# 결론\n\n마지막으로, 글로벌 후크는 개발자들에게 애플리케이션을 강화하고 사용자 정의 기능을 원활하게 통합할 수 있는 유연하고 확장 가능한 기능을 제공하는 강력한 기능입니다. 사용자 정의 코드를 애플리케이션 라이프사이클의 특정 시점에서 실행할 수 있도록 허용함으로써, 글로벌 후크는 코드 중복, 교차 관심사 및 외부 시스템 통합과 같은 일반적인 문제를 해결합니다. 데이터 유효성 검사 및 변환부터 로깅 및 모니터링까지, 글로벌 후크는 개발자가 프로젝트 전반에 걸쳐 일관되게 다양한 사용 사례를 실행할 수 있도록 합니다. 또한, 특정 구성 요소를 대상으로 하거나 비동기 실행과 같은 기능은 세밀한 제어와 성능 최적화를 제공합니다. Mage의 글로벌 후크에 대한 자세한 정보는 문서를 참조하십시오.\n","ogImage":{"url":"/assets/img/2024-06-20-UnleashtheMagicofMageGlobalHooks_0.png"},"coverImage":"/assets/img/2024-06-20-UnleashtheMagicofMageGlobalHooks_0.png","tag":["Tech"],"readingTime":5},{"title":"아파치 드루이드 아키텍처","description":"","date":"2024-06-20 15:36","slug":"2024-06-20-TheArchitectureofApacheDruid","content":"\n\n## 하둡이 모든 문제를 해결할 수 있는 때\n\n# 목차\n\n- 시작\n- 아키텍처\n- 실시간 노드\n- 역사적 노드\n- 브로커 노드\n- 코디네이터 노드\n\n# 소개\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이번 주에는 가장 유명한 실시간 OLAP 시스템 중 하나인 Apache Druid에 대해 자세히 살펴볼 예정이에요. 어떻게 작동하는지 궁금했던 적이 있나요? 이 블로그 글은 'Druid — A Real-time Analytical Data Store' 논문을 읽은 후에 작성되었어요.\n\n참고: 이 논문은 2014년에 발표되었기 때문에 Druid의 일부 세부 사항이 변경/업데이트되었을 수 있어요.\n\n# 시작부터\n\n2004년, 구글이 산업에서 가장 영향력 있는 논문 중 하나인 'MapReduce: Simplified Data Processing on Large Clusters'를 발표했어요. 구글은 MapReduce를 도입하여 상용 컴퓨터를 사용하여 대규모 데이터 처리를 달성했어요. 그 뒤로 Hadoop 프로젝트가 짧은 시간 내에 나왔고, HDFS와 MapReduce 프레임워크는 많은 기업에 대한 대용량 데이터 분석의 기반을 마련했어요. 대용량 데이터의 저장 및 처리를 가능케 한 Hadoop 시스템이 있지만, 일부 단점이 있고 모든 요구 사항을 충족시키지 못할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n메타마켓(지금은 Rill이라 불림)은 마케터들이 마케팅 인사이트에 더 쉽게 접근하고 상호 작용하며 시각화하는 데 도움을 주는 회사입니다. 메타마켓 제품은 고도로 병행적인 환경에서 쿼리 성능과 데이터 가용성에 대한 보장이 필요합니다. 그들은 곧 하둡이 필요한 것을 지원할 수 없다는 것을 깨달았습니다. 오픈 소스 솔루션을 많이 연구한 후, 그들은 사용 가능한 솔루션 이상이 필요하다는 것을 깨달았습니다. 그래서 대용량 데이터에 대한 실시간 분석을 제공하는 데이터 저장소인 Druid를 만들었습니다.\n\n메타마켓 초기에는 사용자가 이벤트 스트림을 탐색할 수 있는 대시보드 솔루션에 초점을 맞추고 있었습니다. 대시보드 아래의 데이터는 사용자가 상호 작용할 수 있는 속도로 빠르게 처리되고 반환되어야 했습니다. 쿼리 지연 시간 외에도 시스템은 고 가용성(다운 타임이 비즈니스에 해를 입힐 수 있음) 및 병행성(많은 사용자가 동시에 제품을 사용함)이 필요합니다. 이것이 메타마켓이 Druid를 스스로 개발해야 하는 이유입니다.\n\n# 아키텍처\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/0*LEShd3zbfB-0RWim.gif)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDruid은 공유하지 않는 아키텍처를 갖고 있어요. Druid 클러스터에는 다양한 종류의 노드가 있어요. 각 노드 유형은 일련의 책임을 맡고 있어요. 아래 섹션에서는 Druid의 노드 유형에 대한 자세한 내용을 다룰 거예요.\n\n# 실시간 노드\n\n실시간 노드는 이벤트 스트림을 수집하고 쿼리하는 작업을 맡고 있어요. 이벤트를 즉시 쿼리할 수 있도록 처리할 수 있어요. 그들은 Zookeeper(드루이드의 상태 관리 구성 요소)에 상태 및 책임 있는 데이터를 알려줄 거예요. 실시간 노드는 이벤트를 위한 로우 저장소처럼 작동해요. 이러한 노드는 모든 수신 이벤트를 위해 인메모리 인덱스 버퍼를 유지해요. 이러한 인덱스는 데이터가 수집될 때 점진적으로 생성돼요. 이 인덱스는 직접 쿼리될 수 있어요.\n\n메모리 제한 때문에 실시간 노드는 인메모리 인덱스를 디스크에 두 가지 방식으로 유지해요: 주기적으로 또는 최대 행 임계값을 초과했을 때. 이 프로세스는 로우 저장 형식을 메모리에서 열 기반 저장 형식으로 변환해요. 그런 다음, 디스크의 데이터는 변경 불가능하게 저장돼요. 실시간 노드는 로컬로 유지되는 모든 데이터를 찾는 백그라운드 작업을 예약할 거예요. 이 작업은 이 인덱스를 병합하고 특정 시간 범위의 모든 수신된 이벤트를 포함하는 변경 불가능한 데이터 블록을 빌드해요. 논문에서 이 병합된 데이터를 \"세그먼트\"라고 부르고 있어요. 나중에, 실시간 노드는 이 세그먼트를 원격 저장소(드루이드에서 딥 스토리지라고 불리는)에 업로드할 거예요. S3나 HDFS와 같은 원격 저장소가 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Real-time nodes](https://miro.medium.com/v2/resize:fit:1400/0*kVg94BJBzWdZYS96.gif)\n\n실시간 노드들은 주로 Kafka와 같은 메시지 버스를 사용합니다. 프로듀서는 데이터를 Kafka 토픽으로 전송하고, 그런 다음 실시간 노드가 Kafka 토픽에서 데이터를 읽어들이게 됩니다. Kafka와 같은 중간 계층이 있는 경우 몇 가지 장점이 있습니다:\n\n- 이벤트 버퍼: Kafka는 이벤트 오프셋을 유지하여 실시간 노드가 현재 데이터 소비 위치를 알 수 있도록 합니다. 노드는 메모리 버퍼에 지속적으로 저장할 때마다 오프셋을 업데이트하며, 디스크에서도 이 정보를 저장합니다. 장애 발생 시 디스크가 여전히 사용 가능하다면, 실시간 노드는 디스크에 저장된 오프셋을 사용하여 해당 커밋된 오프셋부터 토픽을 계속 읽을 수 있습니다. 이는 복구 시간을 줄이는 데 도움이 됩니다.\n- 단일 데이터 엔드포인트:\n\n![Apache Druid Architecture](/assets/img/2024-06-20-TheArchitectureofApacheDruid_0.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 중복된 소비자: 여러 실시간 노드가 카프카 토픽으로부터 동일한 이벤트 세트를 수신하여 중복된 이벤트 스트림을 생성할 수 있습니다. 실시간 노드 1개가 실패할 경우, 다른 노드가 데이터가 수신되도록 보장합니다.\n- 부하분산된 소비자: 여러 실시간 노드가 각각 스트림 파티션을 수신합니다. 따라서 시스템은 수신 처리량을 확장할 수 있습니다.\n\n# 과거 노드\n\n과거 노드는 실시간 노드에서 세그먼트를 로드하고 제공합니다. 세그먼트(실시간 노드에서)는 S3 또는 HDFS와 같은 저장소에 원격으로 저장되며, 노드의 로컬 디스크는 캐시로 사용됩니다. 실시간 노드와 마찬가지로, 과거 노드는 Zookeeper에 온라인 상태와 제공하는 데이터를 알립니다. Druid는 Zookeeper에게 과거 노드에게 세그먼트를 로드하고 삭제하는 방법에 대한 지침을 보냅니다. 이 지침에는 세그먼트가 딥 스토리지에 위치하고 세그먼트를 해제하고 처리하는 방법에 대한 정보도 포함됩니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*YonIcYDmR1RAZM9XQ31Slg.gif)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n역사 노드는 데이터 서빙을 위해 깊은 저장소에서 특정 세그먼트를 다운로드합니다. 다운로드하기 전에 먼저 로컬 캐시를 확인합니다. 필요한 세그먼트가 캐시에 없는 경우 깊은 저장소에서 다운로드합니다. 다운로드 후에는 해당 상태를 Zookeeper에 알립니다. 역사 노드는 변경 불가능한 데이터만 처리하므로 세그먼트에서 읽기를 실행할 때 일관성을 보장할 수 있습니다. 변경 불가능성은 또한 Druid가 데이터 수정 여부에 신경 쓰지 않고 병렬화를 효율적으로 달성할 수 있도록 합니다.\n\n![이미지](/assets/img/2024-06-20-TheArchitectureofApacheDruid_1.png)\n\n역사 노드는 서로 다른 티어로 그룹화될 수 있습니다. 사용자는 각 티어마다 다른 성능 및 장애 허용 매개변수를 구성할 수 있습니다. 티어 노드의 목적은 더 중요한 세그먼트에 따라 더 높거나 낮은 우선 순위로 분배되도록하는 것입니다. 예를 들어, 사용자는 고 CPU 및 메모리를 가진 \"핫\" 티어의 역사 노드를 설정할 수 있습니다. \"핫\" 클러스터는 더 자주 액세스되는 데이터를 더 많이 다운로드할 것입니다. 또한 \"콜드\" 클러스터에는 덜 자주 액세스되는 세그먼트만 포함될 것입니다.\n\n# 브로커 노드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*75f72U25fy68ZFcgnVHLxg.gif)\n\n브로커 노드는 \"적절한\" 쿼리를 필요한 데이터를 가진 히스토리컬 및 리얼타임 노드로 경로 지정합니다. 브로커 노드는 Zookeeper에서 메타데이터를 읽어 어떤 세그먼트가 쿼리 가능하며 해당 세그먼트가 어디에 위치하는지를 가리킵니다. 쿼리가 리얼타임 및 히스토리컬 노드에서 결과를 필요로 할 때, 브로커는 결과를 합쳐서 호출자에게 반환하기 전에 결합합니다.\n\n이러한 노드는 LRU 전략을 사용한 캐시 구현이 있습니다. 캐시는 로컬 힙 메모리나 Memcached와 같은 외부 스토어를 사용할 수 있습니다. 브로커 노드가 쿼리를 받으면 해당 쿼리를 세트의 세그먼트로 매핑합니다. 쿼리 결과는 이미 캐시에 존재할 수 있기 때문에 다시 처리할 필요가 없습니다. 캐시에 존재하지 않는 결과에 대해서는 브로커 노드가 올바른 노드로 쿼리를 전달합니다:\n\n- 히스토리컬 노드의 결과의 경우, 브로커는 미래 사용을 위해 세그먼트 단위로 이러한 결과를 캐싱합니다.\n- 리얼타임 노드의 결과는 브로커가 캐싱하지 않습니다. 이렇게 함으로써 쿼리가 항상 결과의 신선도를 보장하는 리얼타임 노드에 의해 처리되도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n브로커 노드는 쿼리를 처리 노드로 라우팅하기에 중요합니다. 이를 위해 세그먼트 노드 매핑을 위해 Zookeeper와 통신해야 합니다. Zookeeper 장애가 발생한 경우, 브로커 노드는 클러스터의 최신 상태를 사용합니다 (이전 성공적인 Zookeeper 통신에서의 마지막 메타데이터). 브로커 노드는 클러스터 상태가 장애 전과 동일하다고 가정할 것입니다.\n\n# 코디네이터 노드\n\n이러한 노드는 역사적 노드의 데이터 관리 및 분배를 담당합니다. 코디네이터 노드는 역사적 노드에게 새로운 데이터를 로드하도록 지시하거나 오래된 데이터를 삭제하며 데이터를 복제하고 로드 밸런스를 맞추기 위해 데이터를 이동합니다. 코디네이터 노드에게는 리더 선출 프로세스가 있어 코디네이터 작업을 실행하는 노드를 결정하며, 다른 노드는 백업으로서 대체 역할을 합니다.\n\n코디네이터 노드는 주기적으로 실행되어 클러스터의 현재 상태를 결정합니다. 그런 다음 예상 상태와 실제 상태를 비교하여 결정을 내립니다. (Kubernetes와 유사하죠?). 앞에서 언급한 노드 유형과 마찬가지로, 코디네이터 노드는 현재 클러스터 정보를 얻기 위해 Zookeeper와 통신합니다. 또한 이러한 노드는 추가 운영 매개변수 및 구성을 저장하는 MySQL 데이터베이스와 연결되어 있습니다. MySQL 데이터베이스에는 세그먼트가 클러스터에서 생성되고 삭제되며 복제되는 방식을 제어하는 규칙 테이블이 저장됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n룰 테이블에는 규칙이 포함되어 있습니다. 이 규칙은 역사적 세그먼트가 클러스터에서로드되고 삭제되는 방식을 제어합니다. 이러한 규칙을 통해 코디네이터가 다음을 알 수 있습니다:\n\n- 세그먼트를 다른 역사 노드 티어에 할당하는 방법\n- 각 티어에 세그먼트의 복제본이 어떻게 존재해야 하는지\n- 언제 세그먼트를 삭제해야 하는지\n\n코디네이터 노드는 또한 세그먼트의 분배를 제어함으로써 클러스터의 균형을 유지합니다. 더 나아가, 코디네이터 노드는 역사 노드에 동일한 세그먼트의 복사본을로드하여 장애 허용성과 가용성을 향상시킬 수 있습니다. 복제본의 수는 설정 가능합니다.\n\n# 저장 형식\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n드루이드(Druid)의 테이블은 타임스탬프가 찍힌 이벤트의 컬렉션으로, 일반적으로 각 세그먼트는 5~10백만 행 정도로 파티셔닝되어 있어요. 세그먼트는 기본 저장 단위이며, 복제 및 분배는 세그먼트 수준에서 수행됩니다. 모든 테이블에는 Druid가 필요로 하는 타임스탬프 열이 항상 있어요. Druid는 이 열을 데이터 분배 및 유지 정책에 사용하기 때문이에요. 세그먼트는 데이터 소스 식별자, 데이터 간격 및 버전에 의해 식별됩니다. 더 늦은 버전의 세그먼트에는 더 최신 데이터가 있어요. Druid에서 읽기 작업은 항상 최신 버전의 세그먼트에서 특정 시간 범위의 데이터를 읽어와요. 세그먼트는 컬럼 형식으로 원격 저장소에 저장되며, 이를 통해 더 효율적인 CPU 사용이 가능합니다. 필요한 데이터만 로드되기 때문이죠. Druid에는 다양한 데이터 형식을 지원하기 위한 여러 가지 컬럼 타입이 있어요. Druid는 디스크나 메모리에서 데이터를 더 효율적으로 압축하기 위해 다른 데이터 형식에 다양한 압축 방식을 적용할 거에요.\n\n# 마무리\n\n드루이드(Druid) - 실시간 분석 데이터 스토어 논문을 읽은 후의 제 메모가 여기까지에요. 여러분이 가치를 느끼셨으면 좋겠습니다. 만약 다른 실시간 OLAP 시스템에 대해 읽고 싶으시다면 링크드인의 Apache Pinot에 관한 제 글을 확인해보세요: Apache Pinot, 링크드인의 실시간 OLAP 시스템을 한 눈에!\n\n다음 블로그에서 만나요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n[1] Fangjin Yang, Eric Tschetter, Xavier Léauté, Nelson Ray, Gian Merlino, Deep Ganguli, Druid — A Real-time Analytical Data Store\n\n내 뉴스레터는 주간 블로그 스타일 이메일로, 저보다 더 똑똑한 사람들로부터 배운 것들을 기록합니다.\n\n그러니 저와 함께 배우고 성장하고 싶다면 여기에서 구독하세요: https://vutr.substack.com.","ogImage":{"url":"/assets/img/2024-06-20-TheArchitectureofApacheDruid_0.png"},"coverImage":"/assets/img/2024-06-20-TheArchitectureofApacheDruid_0.png","tag":["Tech"],"readingTime":7},{"title":"Sridhar Ramaswamy가 누구이며, 이것이 Snowflake에 미치는 의미는 무엇인가요","description":"","date":"2024-06-20 15:34","slug":"2024-06-20-WhoisSridharRamaswamyandwhatdoesthismeanforSnowflake","content":"\n\n## 서문\n\n많은 사람들처럼, 나도 우연히 데이터에 발을 담궜어요. 그러나 굉장히 특이한 점은 투자 은행에서 경력을 시작했다는 것이죠. 데이터가 새로운 석유인 만큼, 계속해서 파고들다 보니 결국 그 데이터-석유를 발굴했어요.\n\n데이터 다루기에 능숙해지면서 우리가 처해있는 생태계에 대한 기술적 이해를 얻게 되었어요. 그러나 투자 은행과 같은 산업에서 매크로 경제 주도 요인과 추세는 여전히 저에게는 매우 흥미롭답니다. 그래서 저는 Snowflake가 기업 제국을 구축하는 흥미로운 사례 연구라고 생각해요.\n\n스리다르 라마스와미는 매력적인 인물이고, 나가는 CEO인 프랭크 슬룻먼과 완전히 다릅니다. CEO의 역할 외에도, 그들이 가진 공통점은 가치 창출 머신이라는 점이에요. 우리는 상당히 심각한 추세와 매우 심각한 사람들과 마주하고 있어요. 이 글은 가벼운 마음으로 쓰인 것이 아니에요. 그러나 데이터 산업이 빠르게 변화하는 시대에, 누가 진군을 이끌고 있는지 이해하는 것이 공평하다고 생각해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 소개\n\n몇 주 전 기업 데이터 클라우드 서비스의 선두 주자인 Snowflake가 Frank Slootman이 CEO 직을 물러나 회장으로 남을 것을 발표했습니다. 그 자리에는 Sridhar Ramaswamy가 들어가게 되었습니다.\n\nSridhar Ramaswamy는 상대적으로 데이터 분야에 새로 온 신입사원입니다. Snowflake에 합류하기 전에는 Google에서 일하다가 2019년 Neeva를 설립했는데, 이는 프라이버시에 중점을 둔 검색 엔진이었습니다. Neeva는 2023년 5월 Snowflake에 인수되었습니다.\n\nSnowflake에 대한 이러한 변화가 의미하는 것을 이해하기 위해선, Snowflake의 놀라운 여정을 살펴보는 것이 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터 창고 트렌드의 등장\n\n스노우플레이크는 티에리 크루안과 베누아 다지빌에 의해 2010년에 설립되었습니다. 둘 다 기술에 능통하며 데이터 창고 분야에서 경험을 가지고 있었습니다. \"클라우드 데이터 창고\"라는 개념은 존재하지 않았습니다. 대신, 데이터 기능은 비즈니스 애널리스트가 도구를 사용하여 소스 시스템에서 데이터를 추출하는 것으로 제한되어 있었습니다. 더 기술적인 사용자에게는 \"엔터프라이즈 데이터 창고\", 오라클, 골든게이트 등이 도와주었으며 이들은 전부 온프레미스에 배포되어 있었습니다.\n\n클라우드의 확산은 데이터를 중앙 집중화할 수 있는 기회를 가져왔습니다. 클라우드 기반 애플리케이션을 대거 추가함으로써 기업은 사실상 인터넷 호스팅 서비스, 즉 클라우드에서 데이터를 생성하기 시작했습니다. 더불어 AWS, Azure 및 GCP에 의해 클라우드에서 호스팅되는 인프라는 데이터에 상대적으로 쉽게 액세스할 수 있게 되었습니다. 데이터를 저장할 중앙 위치를 가지는 것은 덜 간단했지만, 빅쿼리와 레드시프트와 같은 솔루션은 물론 존재했습니다.\n\n이것이 첫 번째 중요한 트렌드로 이어집니다 — 클라우드 서비스의 증가 및 '일상적인 비즈니스' 애플리케이션이 클라우드(크기가 큰 \"C\")에서 존재하게 되는 전환입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한, 현재 “데이터 레이크”로 알려진 것이나 S3와 같은 오브젝트 스토어는 이미 존재했지만, 데이터 저장소로 사용하기에는 기능이 제한되어 있었습니다. 또는, 그렇게 생각했지요. Snowflake의 최초 버전은 S3 위에 구축되었지만, 현재는 다른 클라우드 제공 업체에서 호스팅할 수 있으며 따라서 오브젝트 스토어를 지원합니다. 클라우드 버전의 Oracle을 위한 원천 자재는 이미 존재했습니다. 우리가 놓친 것은 기술과 노력이었습니다.\n\n이 사실은 실제로 최근 Skiers in Data 컨퍼런스에서 Upsolver의 Roy에 의해 강조된 두 번째 중요한 트렌드로 이어집니다. 이는 스노우플레이크와 다른 클라우드 데이터 창고 제공 업체의 데이터 웨어하우징 기술 대안으로 Duck DB와 같은 오픈 테이블 형식 및 강력한 쿼리 엔진의 등장입니다.\n\n# 무자비한 실행\n\n2014년 6월, Snowflake은 CEO로 Bob Muglia를 영입했습니다. Bob의 배경은 당시 마이크로소프트의 부사장이었으며 20년 이상 근무한 경험이 있었습니다. Snowflake에 합류하기 전에 Muglia는 스티브 발머에 직접 보고하는 네 명 중 한 명이었으며 \"서버 및 도구\" 부문을 담당했습니다. 그는 결국 현재 Microsoft의 CEO 인 Satya Nadella에 의해 대체되었으며 Muglia의 작업을 기반으로 Microsoft Azure를 본질적으로 구축했습니다. 위키피디아에서:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBob의 임무 기간 동안 데이터 웨어하우징 분야에서 엄청난 기술 혁신이 일어났어요. 예전에는 대규모 기업 데이터 웨어하우스를 지원하기 위해 존재했던 기술이 이제는 더 작은 응용 프로그램으로도 도입되었어요.\n\n티에리와 베누아는 스노우플레이크 초기에 가장 어려웠던 도전 과제가 다중 테넌시였다고 썼어요. 구조화된 데이터 수십억 행을 처리할 수 있는 시스템을 만드는 것은 수십 년간 일어나고 있었지만, 기업뿐만 아니라 중소기업에게도 잘 작동하는 추가적인 추상화 레이어가 있는 다중 테넌트 시스템은 어떨까요?\n\n저도 정확한 통계를 직접 보진 않았지만, Bob의 임기 동안 스노우플레이크는 작은 회사에서 시작하여 수천 명의 고객을 확보했어요. 그들은 수십 개국으로 확장되어 온프레미스 데이터 웨어하우스의 대체제로 필요한 기능 전체를 갖추게 되었어요. 이러한 양상은 AWS(RedShift), 마이크로소프트(Synapse) 및 GCP(BigQuery)에서도 비슷하게 반복되었지만 각기 다른 성공 수준을 거뒀어요.\n\n동시에, 데이터브릭스는 우연히도 같은 방향으로 나아갔어요. 이 회사는 처음에는 클라우드에서 효율적으로 Spark를 실행할 수 있는 장소를 제공하기 위해 설립되었지만, .delta(그들의 \"오픈\" 테이블 형식)을 통해 스노우플레이크에 이어 클라우드 데이터 웨어하우징 대안으로 성장하였어요. 데이터브릭스의 진화에 대해서는 다른 별도의 작품으로 충분히 써볼만한 매력적인 이야기라 해도 과언이 아닐 거예요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2018년까지 Snowflake는 IPO를 준비 중이었습니다. [ ]와 같은 투자자들과 데이터 회사 기준에선 거대한 설치 베이스로, 완벽한 스톰이 몰고 있었습니다.\n\n# CEO 대기 중: Frank Slootman\n\n위키피디아에서 발췌:\n\nSlootman의 첫 번째 경영직은 1995년 Compuware에서 이루어졌습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n프랭크는 2007년 Data Domain의 성공적인 IPO 이후 기업 세계에서 가치 창출자로서의 명성을 이미 확립했지만, 실제로 ServiceNow 이후 전설적인 존재로 변모했습니다. ServiceNow IPO는 블록버스터이자 성공이었으며, Google에 따르면 지난 5년간 220% 상승했습니다.\n\n그래서 Slootman 장이 도착하기 전에 이미 여러 면에서 쓰여진 것이었습니다. 그의 도착은 블러브에 부응했습니다. Forbes에서 발췌한 이 문장은 Snowflake의 처음 몇 날을 잘 요약합니다:\n\nSnowflake IPO는 미친 짓이었습니다. 주식은 240USD 정도에 상장되었고, 몇 주 만에 400USD까지 상승하여 시가총액이 1000억 달러 이상이 되었습니다 (기억해두어야 할 것은 Databricks의 마지막 비공개 가치가 500억 달러 정도였다는 것입니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-20-WhoisSridharRamaswamyandwhatdoesthismeanforSnowflake_1.png)\n\n의사록가들은 IPO 이후 기간 동안 주가가 조작되어 기존 스노우플레이크 주주가 최대한의 가치를 실현할 수 있게 했다고 주장할 것입니다. 결국, 2021년 당시 매출은 단지 약 500백만 달러에 불과했으며, 시장 자본화 수익비율(또는 \"매출 배수\"는 2020/2021년 당시 약 100배 정도로 엄청나게 높은 값을 나타냅니다.\n\n![이미지](/assets/img/2024-06-20-WhoisSridharRamaswamyandwhatdoesthismeanforSnowflake_2.png)\n\n여러 면에서, 이것이 슬룻만이 고용된 이유였습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n러시아가 우크라이나를 침공하자, 그들의 주가는 급격히 하락하여 몇 달 동안 거의 60%의 가치를 잃었습니다. 다가오는 해에는 AI를 둘러싼 경쟁이 가열될 것이며, Frank은 은퇴할 예정입니다. 이어서 우리는 Sridhar Ramaswamy에 대해 이야기할 것입니다.\n\n# 스노우플레이크가 니바를 인수\n\n2023년 5월, 스노우플레이크가 1억 5천만 달러에 소규모 스타트업인 Neeva를 인수했습니다.\n\n![이미지](/assets/img/2024-06-20-WhoisSridharRamaswamyandwhatdoesthismeanforSnowflake_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n획득 배경은 LLM-Powered Search를 통해 스노우플레이크 사용자가 영어만 사용하여 데이터베이스의 데이터를 더 쉽게 쿼리할 수 있다는 것이었습니다. 정말 멋지죠, 하지만 정말 혁명적인 변화는 아니었습니다.\n\n2023년이 지나면서, 우리는 스노우플레이크의 미션을 상기시키게 되었습니다:\n\n이것은 매우 대담한 비전입니다. 스노우플레이크가 무글리아 아래 기능을 구축하고, 스룻먼 아래 강력히 상용화함에 따라, 라마스외미가 CEO로 발표되면 스노우플레이크가 다시 한 번 건설을 시작하려고 합니다.\n\n# 스리다르 라마스외미가 스노우플레이크에게 뜻하는 바\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSnowflake은 2024년 2월 Sridhar가 책임을 맡은 후 몇 달 동안 매우 대담한 몇 가지 움직임을 해 왔습니다.\n\n## 데이터 플레인 통합\n\n데이터 작업에 종사하는 사람들에게 첫 번째 중요한 포인트는 Snowflake가 \"데이터 플레인\"을 통합하기 시작한다는 점입니다.\n\n\"데이터 플레인\"은 Orchestra에서 데이터와 직접 상호작용하는 응용 프로그램 계층으로 지칭하는 영역입니다. 예를 들어, 데이터를 이동하거나 정리하는 응용 프로그램입니다. Snowflake에는 이미 모든 데이터가 포함되어 있기 때문에 많은 기능을 그 안에 구축하는 것이 합리적입니다. 예를 들어,\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Snowflake Cortex를 사용한 이상 탐지\n- ACCESS_HISTORY 뷰를 사용한 완전한 엔드 투 엔드 선조 트래킹\n- ELT 통합 제로\n- 데이터 품질 테스트 모니터\n\n이는 기본적으로 Snowflake 시장이 더 이상 실제로는 없다는 관련 사측 데이터 관측 도구 공급업체들에 압력을 가할 것입니다.\n\n이는 마침내 GCP가 데이터 카탈로그, Dataplex 및 선조 주변에 매우 유사한 기능을 출시하는 시기와 일치합니다.\n\n## AI 생성과 민주화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스노플레이크는 Snowpark 컨테이너 서비스를 제공하고 Snowflake Cortex를 통해 거의 모든 LLM에 액세스할 수 있습니다. 이를 통해 데이터 전문가들은 Snowflake에 있는 데이터를 활용하여 LLM을 구동하는 애플리케이션을 쉽게 배포할 수 있게 됩니다.\n\n이것은 다른 AI 업체들이 하는 일과 비교했을 때 상당히 상상력이 부족한 움직임입니다. 이러한 서비스들은 Microsoft의 Open AI나 AWS/Anthropic의 Claude와 같은 클라우드 기반 LLM 서비스 위에 얇은 랩핑을 한 것입니다.\n\n그러나 Document AI와 같은 몇 가지 기능과 결합하면 Snowflake가 조직 내에서 AI를 활용하는 데 필요한 모든 것을 갖고 있다는 것을 깨닫게 될 것입니다. 사람들이 Cortex를 그 AI의 보약으로 사용할 것인지는 아직 미지수입니다. \"AI를 최대한 활용하기\"는 다양한 형태로 나타날 것이며, \"데이터 최대한 활용하기\"와 마찬가지로 다양한 형태로 나타날 것입니다.\n\n예를 들어, 기본 분석에 대한 \"좋은\"것이란 작동하는 데이터 파이프라인과 대시보드일 수 있습니다. 그러나 더 성숙한 기업에 대한 좋은 사용 사례는 실시간 스트리밍 데이터 파이프라인을 통해 실시간으로 학습하고 예측을 제공하는 기계 학습 모델을 용이하게 하는 것일 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스노우플레이크를 처음 사용하는 경우에 사용할 수 있는 곳은 미묘한 차이가 있을 수 있습니다. 사실 두 번째로 사용할 때는 사용하지 않을 수도 있어요. 실제로 제가 알기로는 그렇게 사용하는 사람이 많지 않아요. 코텍스에서 모든 모델을 제공할 수 있는 것은 그냥 \"스노우플레이크를 사용하면 된다\"는 말로 이어지지 않는다는 걸 명심해주세요. 하지만 AI 관련 작업 중에 어떤 일이 벌어지고 있는지에 대해서는 스노우플레이크를 통해 이 컴퓨팅을 수행할 것이라고 확신할 수 있어요.\n\n## 저장소와 컴퓨팅 분리 지속\n\n\"데이터를 스노우플레이크로 가져오는 것\"이 최근 가장 큰 이익이자 단점으로 작용하고 있어요.\n\n데이터가 스노우플레이크에 들어간 다음에는 데이터를 효과적으로 사용할 수 있다는 약속에 비해, 데이터의 진입 및 이탈 비용은 전 세계의 데이터 아키텍트들, 제 자신을 포함해 괴롭히고 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n물론, 스노우플레이크는 저장소와 컴퓨팅을 분리하여 이를 기능으로 성공적으로 홍보했지만, 이 개념 자체는 새로운 것이 아닙니다.\n\n아파치 아이스버그와 다른 오픈 테이블 형식에 대한 관심이 높아지면, External Tables와 같은 제품들은 추가 투자를 받게 될 것입니다. Orchestra에서의 우리의 이해에 따르면, 스노우플레이크 제품팀이 아이스버그를 매우 심각하게 받아들이고 외부 테이블을 아이스버그의 우수한 기능으로 인해 폐기할 것으로 예상됩니다.\n\n아키텍처 열렬론자와 스노우플레이크 사용자들에게는 아이스버그의 완벽한 지원으로 인해 데이터 유입 및 유출에 대한 반대 이유가 사라지게 될 것입니다. 그러나 스노우플레이크와 아이스버그의 변화 속도를 고려할 때, 아이스버그와 다른 오픈 테이블 형식의 지원이 스노우플레이크의 프로프리어터리 플랫폼과 동일한 수준으로 제공되는 것은 매우 적합하지 않을 것입니다. 그럼에도 불구하고 참고할 만한 사항입니다.\n\n## 계속되는 사용자 경험에 대한 집중력\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSnowflake가 Snowflake 대 Databricks 논쟁에서 중요한 이유 중 하나는 그들의 사용자 인터페이스와 사용자 경험이 최고라는 것입니다.\n\n이 제품은 정말로 무한한 탄력성을 가진 데이터 웨어하우스입니다.\n\n사용자들이 AI 사용 사례에 집중하는 시대에, 데이터 엔지니어링 및 과학 팀들은 기술적 업무와 비즈니스 운영 사이의 간극을 좁힐 필요가 있을 것입니다.\n\n비전문가들도 이제까지 본 적이 없는 수준으로 데이터 플랫폼에 찾아올 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이는 접근성이 높고 사용하기 쉬운 UI가 이제 언제보다 더 중요해질 것을 의미합니다. 따라서, 우리는 Snowflake가 UI와 Anomaly Detection과 같은 시스템에 계속 투자할 것으로 예상됩니다. 이러한 시스템은 높은 복잡성을 단 한 줄의 코드로 추상화하여 누구나 이해할 수 있게 해줍니다.\n\n# 결론: 데이터 분야의 경쟁은 막 시작되었습니다.\n\n레드시프트 대안으로 시작한 것이 데이터 플랫폼이 되었습니다.\n\nSnowflake는 초기 몇 년 동안 AWS에서 운영되었으며 Redshift의 인기를 크게 앞서갔습니다. 잘 알려진 사실은 Amazon 세일즈 대표들이 Redshift 대신 Snowflake를 홍보하는 것을 선호했다는 것입니다. 물론, Snowflake를 통한 간접적인 사용에 대한 이익이 Redshift보다 낮았지만, Sales 및 Marketing에 Snowflake가 지출한 비용으로 인해 거대한 양의 거래가 이루어졌습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과적으로 Redshift의 기능을 받아들여 기존의 인프라 레이어 이상으로 발전했습니다. 카탈로그, 계보, 알림 및 데이터 품질과 같은 기능은 데이터 창고에 항상 포함되어야 했고, 벤처 투자가 2021년 대비 줄어들고 있는 상황에서 Snowflake는 데이터 영역의 다양한 스타트업을 겨냥하고 있습니다.\n\n한편으로, 라마스와미의 가장 큰 과제는 데이타브릭스의 가장 큰 경쟁사로서의 위치를 공고히 하는 것일 것입니다. 데이타브릭스는 노트북 개념으로 시작하여 데이터 엔지니어링으로 나아갔는데, Snowflake는 그 반대로 하려고 하고 있습니다.\n\n데이타 엔지니어 또는 창고 전문가로서, 왜 즐겁고 흥미로운 머신 러닝 워크로드가 Snowflake 외부에 있어야 할까요? 왜 데이터를 이탈시켜 다른 팀에 전달해야 할까요?\n\n이러한 기능 세트를 확장하는 것이 Snowflake의 앞으로 몇 년 동안 가장 큰 우선 순위입니다. 이 점에는 매우 직관적인 장점이 있습니다. 데이터를 이동시킨 후 정제하고 제공하며 탐색한 다음, 만족할 때까지 — 그것을 기반으로 머신 러닝과 인공 지능을 활용하기 시작하면 됩니다. 이것에는 매력이 있습니다. 데이터 전문가로서의 우리 여정 중 많은 부분과 유사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 방식의 단점은 공통 분모에 맞춰진다는 것입니다. 데이브릭스는 실시간 머신러닝을 위해 낮은 지연 시간으로 대용량 데이터를 다루던 사람들로부터 시작되었습니다. 여기에는 a fortiori 요소가 있는데, 현대적인 형태로 표현하면 \"만약 당신이 렌치를 잡을 수 있다면, 당신은 닷지볼도 잡을 수 있다\"라는 문구가 있습니다.\n\n자체 인프라에 저장된 데이터에서 스파크를 실행하는 것이 대용량 데이터에서 실시간 머신러닝 작업을 수행하는 방법으로 입증되었습니다. 따라서 당신은 데이브릭스를 사용하여 더 간단한 작업을 수행할 수 있습니다. Snowflake에서 단순한 작업을 잘 처리할 수 있지만, 낮은 지연 시간이나 대용량 데이터를 필요로 하는 작업을 실행하기에 적합하거나 비용 효율적인지는 아닙니다.\n\n물론, 사용의 편의성을 간과할 수 없습니다. Snowflake를 사용하는 것은 쉽고 앞으로도 계속 쉬울 것입니다. 이것은 데이브릭스의 능력을 가진 미사일과 비교했을 때 그들의 약점입니다.\n\nIPO 후 시기와 비교했을 때 주가가 하락하고 있지만, Ramaswamy에게는 조금이나마 여유가 있어야 할 것입니다. 데이터 및 인공지능 무기 경쟁은 얼마나 막바지인지에 대한 의문이 없습니다. Ramaswamy의 가장 큰 우선 과제는 인공지능 기능의 핵무기를 개발하는 것입니다. 데이터 전문가로써 우리에게는 그것을 사용하는 비용이 혜택을 초과하는지 여부가 관건일 것입니다. 🚀","ogImage":{"url":"/assets/img/2024-06-20-WhoisSridharRamaswamyandwhatdoesthismeanforSnowflake_0.png"},"coverImage":"/assets/img/2024-06-20-WhoisSridharRamaswamyandwhatdoesthismeanforSnowflake_0.png","tag":["Tech"],"readingTime":10},{"title":"데이터 엔지니어링 세계를 여행하며 발견한 것들","description":"","date":"2024-06-20 15:32","slug":"2024-06-20-MyjourneyintheDataEngineeringworld","content":"\n\n![image](/assets/img/2024-06-20-MyjourneyintheDataEngineeringworld_0.png)\n\n나에 대해\n\n안녕하세요, 저는 프리얀카입니다. 저는 비은행 금융 회사(NBFC)에서 시니어 데이터 엔지니어로 일하고 있어요. 지오인포매틱스(MTech) 학위를 가지고 있으며, 지오인포매틱스 엔지니어링에서 데이터 엔지니어링으로 전향했어요. 이 블로그에서는 이 분야로의 나의 여정과 비슷한 전환을 고려하는 사람들에게 도움이 될 만한 자료를 공유할 거에요.\n\n데이터 엔지니어링 분야로 진입하는 방법\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 위성 데이터 분석에 중점을 둔 원격 감지 업체에서 경력을 시작했는데, R, Python 및 일부 SQL을 활용하는 경험을 쌓았습니다. 우리는 소규모로 데이터 엔지니어링을 활용했지만, 코딩에 더 많은 관심을 갖게 되었습니다. 이 열정이 저를 대담한 결정을 내리게끔 만들었습니다: 일자리 제안 없이도 나가서 Python 개발자나 코딩 분야의 경력을 쫓기로 했습니다.\n\n여정을 시작할 때 데이터 엔지니어의 역할에 대해서는 완전히 모르고 있었습니다. LinkedIn에 자주 소개되는 데이터 분석가와 데이터 과학자는 알고 있었지만, 데이터 엔지니어링은 의문스러웠습니다. 통계와 모델 제작에 노출되어 있던 저는 처음에 무료 온라인 자료를 활용하여 데이터 과학 역할에 대비하기 시작했습니다. 초기 면접에서 종종 통과했지만, 케이스 스터디 부분에서는 어려움을 겪었습니다.\n\n그때 LinkedIn에 데이터 엔지니어의 채용 공고가 눈에 띄었는데, 이 포지션에는 Python, SQL, AWS 및 데이터 웨어하우징 개념 등이 요구되었습니다. 이것이 내 관심사와 부합한다고 느끼면서, 나는 이러한 분야를 집중적으로 공부하기로 했습니다. 나중에 사용한 자료에 대해 논의할 것입니다.\n\n면접에서 여러 차례 거절을 당했지만, 조금씩 데이터 엔지니어링이 무엇인지와 전형적으로 무엇이 물어보는지를 배우게 되었습니다. 특정 데이터 엔지니어링 면접 중에 SQL과 Python에 초점을 맞춘 초기 라운드를 성공적으로 통과했습니다. 두 번째 라운드에서는 제 이전 프로젝트, SQL, AWS 서비스, Python뿐만 아니라 의도치 않게 Docker에 대해 다뤄야 했습니다. Docker에 대해 익숙하지 않았고 이것이 심사에서 나를 배척할 것이라 두려워했지만, 놀랍게도 제안을 받았습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내 데이터 엔지니어링 경력의 시작이었습니다. 일을 시작하기 전에 2~3 주가 있었는데, 그 기간 동안 가능한 많은 지식을 습득하는 데 전념했습니다. 처음에는 분야가 낯설어 학습 곡선이 가파르게 느껴졌습니다. 저는 자주 늦은 밤과 주말을 일하면서 일을 해내기 위해 노력했어요. 점차적으로 데이터 웨어하우스와 데이터 마트를 처음부터 구축하고, 프로젝트를 주도하고, 전체 파이프라인을 관리하며, 심지어 제 작업에 대한 상을 받기도 했어요.\n\n나중에는 도메인에서 계속 진보하기 위해 직장을 옮겼고, 민감하고 많은 양의 데이터를 다루는 방식으로 일을 하게 되었습니다. 지오인포매틱스에서 데이터 엔지니어링으로의 여정은 도전적이면서 보람 있었고, 이 분야에서 성공을 거둘 수 있도록 도와준 자료와 전략을 공유할 수 있어 기쁩니다.\n\n나의 자료\n\n저는 꾸준히 무료 강좌를 활용하고 Udemy의 할인 이벤트를 이용하여 추가 강좌를 구매했습니다. 다음은 사용한 자료의 목록입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPython-\n\n- 100 Days of Code: The Complete Python Pro Bootcamp- [link](https://website.com)\n- Corey Schafer [link](https://website.com)\n- sentdex [link](https://website.com)\n- Telusko [link](https://website.com)\n\nSQL-\n\n- kudvenkat [link](https://website.com)\n- Sumit Mittal [link](https://website.com)\n- Namaste SQL [link](https://website.com)\n- SQL Tutorial [link](https://website.com)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하둡과 빅 데이터 -\n\n- 학습 저널 - 🔗\n\nAirflow -\n\n- Apache Airflow 완전 실무 입문 - 🔗\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAWS-\n\n- Ultimate AWS Certified Solutions Architect Associate SAA-C03 -🔗\n- AWS Certified Data Engineer Associate 2024 — Hands On!- 🔗\n- Ultimate AWS Certified Developer Associate 2024 NEW DVA-C02- 🔗\n- DataEng Uncomplicated -🔗\n- Complete Coding — Master AWS Serverless — 🔗\n\nKafka-\n\n- Apache Kafka Series — Learn Apache Kafka for Beginners v3 -🔗\n- The Big Data Show — 🔗\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSpark-\n\n- [Spark documentation](link)\n- [Apache Spark 3.5 Tutorial with Examples](link)\n- [Apache Spark 3 — Spark Programming in Python for Beginners](link)\n- [Taming Big Data with Apache Spark and Python — Hands On!](link)\n- [Apache Spark 3 — Beyond Basics and Cracking Job Interviews](link)\n- [GeekCoders](link)\n- [MANISH KUMAR](link)\n\nData Warehousing concept-\n\n- [The Data Warehouse Toolkit (Book by Ralph Kimball)](link)\n- [Tech Coach](link)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDSA-\n\n- 데이터 구조 및 알고리즘 DSA | Python+Javascript LEETCODE- 🔗\n- 데이터 구조 튜토리얼- 🔗\n- DSA 튜토리얼- 🔗\n- 파이썬을 활용한 데이터 구조 및 알고리즘 입문자를 위한 전체 과정-🔗\n\n시스템 디자인-\n\n- ByteByteGo- 🔗\n- Exponent-🔗\n- Gaurav Sen-🔗\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결론\n\n마지막으로, 데이터 엔지니어링으로의 전환에 사용한 자료는 현재의 면접에 여전히 적용 가능합니다. 이 자원들 대부분은 무료이거나 매우 저렴하며, 이를 통해 대부분의 면접을 성공적으로 통과할 수 있었습니다. 제가 제품 기반 기업에서 일자리를 얻지 못했지만, 성공의 열쇠는 자료 그 자체가 아니라 꾸준한 연습에 있다는 것을 깨달았습니다. 꾸준히 연습하고 실패를 두려워하지 말고, 각 면접은 학습의 기회임을 기억하세요.\n\n제 경험상, 모든 기업이 구인 설명서에서 언급된 모든 기술을 사용하는 것은 아닙니다. 예를 들어, 면접 중에 Docker와 Kafka에 대해 물어봤지만, 입사한 후 해당 기업에서는 사용하지 않았습니다. 그러므로 먼저 Python, SQL, AWS(또는 기타 클라우드 플랫폼) 및 데이터 웨어하우징 개념에 집중하세요. 이러한 영역에서 자신감을 얻은 후 지원하고 면접을 진행하세요. 기술적 경험은 귀중하므로 가능한 경우 프로젝트에 참여하여 기술을 향상시키세요. 계속 배우고 연습하며 면접 프로세스를 두려워하지 마세요. 이 모든 것이 여정의 일부입니다.","ogImage":{"url":"/assets/img/2024-06-20-MyjourneyintheDataEngineeringworld_0.png"},"coverImage":"/assets/img/2024-06-20-MyjourneyintheDataEngineeringworld_0.png","tag":["Tech"],"readingTime":4},{"title":"데이터 수집 - 파트 1 아키텍처 패턴","description":"","date":"2024-06-20 15:30","slug":"2024-06-20-DataIngestionPart1ArchitecturalPatterns","content":"\n\n두 편의 기사를 통해 데이터 수집에 대해 철저히 탐구해 보겠습니다. 데이터 수집은 운영 및 분석 세계를 연결하는 기본적인 프로세스입니다. 다양한 출처에서 데이터를 수집하여 원래의 운영 환경인 '운영 평면'에서 분석 평면 또는 '분석 평면'으로 운송하는 것이 중요합니다. 이 전환이 분석 권한의 완전한 잠재력을 발휘하는 데 필수적입니다.\n\n이 권한의 본질은 다양한 데이터 소스를 기반으로 데이터 기반 통찰력을 생성하고 인공 지능 모델을 구현하는 능력입니다. 조직의 분석 능력은 종종 효과적으로 분석할 수 있는 데이터 소스의 수와 직접적으로 관련이 있습니다. 따라서 올바른 데이터 수집 전략을 선택하는 것이 중요합니다. 이러한 전략은 CRM, ERP, 금융 시스템과 같은 표준 운영 응용 프로그램부터 IoT 센서, API, 스크래핑된 문서, 이미지, 비디오와 같은 다양한 형식까지의 관련 데이터 소스를 처리할 수 있는 견고함이 있어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n보다 넓은 시야를 가지고 보면, 데이터 수집은 하나의 요소에 불과하지만, 조직 내에서 더 큰 데이터 플랫폼 퍼즐의 중요한 구성 요소인 것이 분명해집니다. 일반적으로 이 데이터 플랫폼은 디지털 변혁 이니셔티브의 중심 역할을 하며, 조직이 비즈니스 목표를 달성하는 데 도움을 줍니다. 데이터 플랫폼은 다양한 아키텍처 패턴과 다양한 도구로 구성되어 있으며, 각각이 그 기능성과 효율성에 중요한 역할을 합니다.\n\n데이터 수집에 헌정된 두 개의 글 중 첫 번째 글에서는 적합한 데이터 수집 기술의 선택을 안내하는 아키텍처 패러다임을 탐구합니다. 제 목표는 각 패턴을 그 본질로 단순하게 요약하고, 데이터 수집 프로세스에 대한 전략적인 함의를 밝히는 것입니다. 이러한 패턴들을 제시하는 목적은, 이론적으로 가장 간단하지만 절대 필수적인 작업인 데이터를 분석적 프레임워크 내에 통합하는 데 종종 복잡성을 초래하는걸 극복하는 데 있습니다. 데이터 수집의 전략적 중요성을 인식하는 것은 조직의 데이터 에코시스템의 광범위한 범위 내에서 데이터를 순차적으로 사용하기 위한 효과적인 전환을 보장하는 데 필수적입니다.\n\n# 패턴 1: 통합된 데이터 저장소\n\n저희가 살펴볼 첫 번째 아키텍처 접근 방식은 통합된 데이터 저장소 패턴입니다. 이곳에서 단일 저장 시스템이 운영 응용 프로그램 요구 사항과 분석 처리를 모두 처리합니다. 일반적으로 이 시스템은 관계형 데이터베이스 관리 시스템(RDBMS)입니다. 이러한 설정에서는 같은 데이터베이스가 일상적인 운영 및 데이터 분석에 모두 사용되어, 서로 다른 저장 솔루션 간의 데이터 전송이 필요 없어집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-20-DataIngestionPart1ArchitecturalPatterns_2.png)\n\n이 접근 방식 안에서 두 가지 주요 하위 패턴이 있습니다:\n\n- 가상화 — 이것은 데이터베이스 내 작업 테이블 위에 분석적 시각을 제공하는 가상 데이터베이스 레이어 또는 뷰를 생성하는 것을 포함합니다. 이는 데이터를 물리적으로 변경하거나 복제하지 않고도 분석적 렌즈를 통해 데이터를 '볼' 수 있는 방법입니다.\n- 복제 및 변환 — 여기서 작업 데이터는 분석에 보다 적합한 형식으로 복제됩니다. 이는 저장 프로시저, 머티얼라이즈드 뷰 또는 작업 응용 프로그램의 저장 레이어 내에서 구현될 수 있으며, 효율적인 분석 쿼리를 위해 최적화된 데이터의 병렬 버전을 생성합니다.\n\n이 모델은 데이터 관리의 간소화와 원시 데이터의 가용성을 제공하지만, 상당한 제한 사항이 있습니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터 통합 도전 과제 — 이 모델은 복수의 물리적 데이터베이스로부터 데이터를 통합하는 데 어려움을 겪는데, 단일 저장 시스템에 의존하기 때문입니다. 이를 극복하기 위해 연결된 서버나 크로스 데이터베이스 쿼리와 같은 기술을 활용할 수 있지만, 추가 복잡성을 도입하기 쉽고 일반적으로 선호되지 않습니다.\n- 시스템 간 간섭 가능성 — 동시에 운영 및 분석 프로세스가 동작 중인 동일한 데이터베이스는 상호 간섭을 일으켜 부하를 증가시키고 운영 응용프로그램과 분석 처리의 성능을 저하시킬 수 있습니다.\n- 성능 트레이드오프 — 효율적인 트랜잭션 처리를 우선시 하는 온라인 트랜잭션 처리(OLTP) 시스템과 복잡한 쿼리 처리에 최적화된 온라인 분석 처리(OLAP) 시스템이 서로 다른 최적화 요구 사항을 갖는 것은, 두 작업에 모두 최적이 되기 어렵다는 것을 의미합니다.\n- 강하게 결합됨 — 통합된 데이터 저장소 패턴은 운영 및 분석 영역 간 강한 상호 연결성을 유발하여 양쪽 영역 중 하나에서 유연성이 제한되거나 없을 수 있습니다.\n\n이러한 제약 사항을 고려하면, 대규모 데이터셋을 처리하거나 여러 물리적 데이터 소스를 다룰 때 통합된 데이터 저장소 접근 방식은 일반적으로 권장되지 않습니다. 복잡성이 너무 커지지 않는 강력한 데이터베이스에서 운영되는 소규모 응용프로그램에는 적합할 수 있습니다.\n\n# 패턴 2: 데이터 가상화\n\n초기 패턴을 기반으로, 데이터 가상화 접근 방식은 특수화된 소프트웨어를 활용하여 다수의 기존 데이터 소스 위에 가상 데이터 레이어를 구축합니다. 이 중간 계층을 통해 원본 데이터 소스에 부분적으로 처리되는 쿼리를 실행하여 결과를 통합하여 분석을 위한 일관된 데이터 집합을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Table](/assets/img/2024-06-20-DataIngestionPart1ArchitecturalPatterns_3.png)\n\n\n이 방식의 주요 이점은 다음과 같습니다:\n\n- 거의 실시간 데이터 액세스 - 데이터가 분석용 데이터베이스로 물리적으로 이동되지 않고 소스에서 직접 쿼리되기 때문에 이 패턴은 신속한 데이터 사용 가능성을 제공하며 실시간에 근접합니다.\n- 지능형 캐싱 - 데이터 가상화 시스템은 일반적으로 고급 캐싱 기능으로 설계되어 소스 시스템에 대한 수요를 최소화하고 성능을 최적화할 수 있습니다.\n\n그러나 이 방식은 몇 가지 우려 사항도 도입합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 소스 시스템 제한 사항 - 소스 데이터베이스가 특정 쿼리 유형에 최적화되지 않은 경우에는 성능 병목 현상이 발생할 수 있습니다. 특히 쿼리 실행에 소스 응답에 의존하는 경우 가상 레이어로 확장될 수 있습니다.\n- 네트워크 오버헤드 - 다양한 네트워크 존에 분산된 데이터 소스와 인터페이스하는 가상화 레이어는 지연을 겪을 수 있어 전반적인 성능에 영향을 줄 수 있습니다.\n- 역사적 데이터 추적 - 가상 레이어는 데이터를 내재적으로 저장하지 않으므로 데이터 수집 타임라인을 통해 \"시간 여행\"이라고 일반적으로 언급되는 역사적 분석에 대한 도전을 제공합니다.\n\n특정 데이터 가상화 솔루션이 이러한 문제를 해결하는 고유한 메커니즘을 제공할 수 있다는 점을 강조하고 싶습니다. 이러한 중요한 아키텍처 결정을 포함한 경우에는 귀하의 특정 인프라에서 데이터 가상화 솔루션을 철저히 테스트하는 것이 가장 좋습니다. 이를 통해 해당 기능과 한계를 이해하고 적절한 확장 및 세밀한 조정을 통해 통합 및 분석 프로세스를 최적화할 수 있습니다.\n\n## 패턴 3: ETL\n\nETL은 Extract, Transform, Load의 약어로 데이터 처리에서 잘 알려진 패러다임을 나타냅니다. 먼저 데이터를 원본에서 수집하여 (추출), 이후 ETL 서버에서 정제하고 (변환) 마지막으로 정제된 결과를 분석 중심 데이터베이스에 넣는 과정을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-DataIngestionPart1ArchitecturalPatterns_4.png\" /\u003e\n\n여러 해 동안 다양한 ETL 도구 제공업체가 이 방법을 지원했습니다. 이들은 다양한 전문 변환 기술 및 디자인 스타일을 제공했습니다. 보편적인 스타일은 사용자가 직관적인 비주얼 워크플로우 내에서 추출, 변환 및 적재 작업을 상호 연결할 수 있는 그래픽 인터페이스를 포함합니다. 이러한 프로세스는 종종 스크립팅이나 직접 SQL 쿼리를 통해 더 많이 사용자 정의할 수 있습니다.\n\nETL의 주요 이점은 다음과 같습니다:\n\n- 중앙 집중식 논리 — ETL 프로세스는 전체 변환 논리를 단일하고 관리 가능한 환경에서 통합할 수 있어 데이터 수집 뿐만 아니라 데이터를 분석 요구에 맞게 가공하는 데 도움이 됩니다.\n- 사용자 친화적 디자인 — ETL 도구의 비주얼적 성격은 데이터 변환 프로세스를 민주화하여 다양한 기술 수준의 사용자가 데이터 파이프라인 작성에 참여할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 ETL에는 단점이 있습니다. 이러한 단점으로 대체 모델이 등장했습니다:\n\n- 특정 업체에 의존 — ETL 도구 의존은 벤더 락인 형태로 이어질 수 있으며, 현재 도구에서 요금을 변경하거나 기능을 중단하는 경우 다른 플랫폼으로의 전환을 비싸고 복잡하게 만들 수 있습니다.\n- 성능 제약 — ETL 변환은 지정된 서버에서 실행되는데, 현대 데이터 웨어하우스에서 제공되는 고성능 컴퓨팅 자원과 비교할 수 없을 수 있으며, 잠재적으로 병목 현상이 발생할 수 있습니다. 이 상황은 역설을 제시합니다: 쿼리 실행을 위한 매우 효율적인 데이터 웨어하우스 엔진이 있더라도 ETL 서버에 의해 전체 파이프라인 처리량이 상당히 느린 속도로 처리되면서 제한됩니다.\n- 불명확한 데이터 이력 — 시각적 구성 요소로 간소화된 ETL 변환은 종종 데이터 변환의 복잡성을 숨기며, 데이터의 이동(데이터 이력)을 이해하고 감사하기 어렵게 만들 수 있습니다.\n- 한정된 확장성 — ETL 도구는 폭넓은 접근성을 위해 설계되었지만, 데이터 플랫폼이 성장할 때 필수적인 확장 및 산업화를 위한 강력한 기능을 갖추지 못할 수 있으며(DataOps 프레임워크에서 설명한 내용과 같음), 이는 데이터 플랫폼이 성장하는 상황에서 중요합니다.\n- 강성 — ETL 도구가 고유한 데이터 수집 요구 사항을 수용하지 못할 때 융통성이 제한되며, 기술 부채에 기여하는 해결책으로 이어질 수 있습니다.\n\nETL 패턴의 이러한 일반적인 제한사항은 특정 ETL 공급업체에 의해 해결될 수 있습니다. 특히 ETL 도구가 특정 클라우드 DWH를 위해 설계된 포괄적인 스위트에 통합되었을 때, 속도와 성능과 관련된 문제가 완화될 수 있습니다. 그러나, ETL 도구의 발전 궤적을 파악하여 계속해서 데이터 입수 요구 사항과 같은 성장하는 데이터 볼륨이나 신종 데이터 소스 등과 일치하도록 하는 것이 중요합니다.\n\n# 패턴 4: ELT\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nELT은 ETL의 기본 단계를 공유하면서 이러한 프로세스를 재구성하고 재정의합니다. ELT에서는 다음과 같은 과정이 수행됩니다:\n\n- EL — 먼저 추출 및 적재 작업이 수행되어 원시 데이터를 즉시 변환 없이 데이터 플랫폼으로 직접 전송합니다.\n- T — 변환은 그 후에 발생하여 원시 데이터를 실행 가능한 통찰력으로 변환합니다. 중요한 것은 변환 작업이 추출 및 적재와 독립적으로 다른 일정으로 동작할 수 있다는 점입니다.\n\n![이미지](/assets/img/2024-06-20-DataIngestionPart1ArchitecturalPatterns_5.png)\n\n이 재구성된 프로세스는 여러 ETL 제약 사항을 해결합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 더 많은 유연성 - 추출/로딩과 변환 도구를 분리함으로써 다양한 데이터 유형 및 변환 표준에 알맞은 도구를 선택할 수 있는 능력이 높아집니다.\n- 성능 일치 - 변환은 데이터 플랫폼 내에서 이루어지며, 완전한 컴퓨팅 능력을 활용하여 대규모 데이터 세트를 분산 컴퓨팅 엔진으로 처리하기에 특히 효과적입니다.\n- 확장성 향상 - ELT의 내재된 유연성은 자동화 및 확장성에서 뛰어난 변환 도구를 선택할 수 있도록 돕습니다.\n\n이러한 개선점이 있음에도 불구하고 ELT 모델은 새로운 복잡성을 도입합니다:\n\n- 다양한 도구의 관리 - 추출, 로딩, 변환에 대해 다른 도구를 사용하는 것은 라이선스, 가격, 업데이트 주기 및 지원 구조를 관리하기 위해 엄격한 관리가 필요합니다.\n- 조율 도전 - 보다 다양한 툴킷은 성공적인 데이터 추출 및 로딩 이후에 변환을 계속 진행할 수 있도록 Directed Acyclic Graphs(DAG)를 기반으로 한 복잡한 조율이 필요합니다.\n\nELT 패턴은 그 유연성 때문에 개인적으로 좋아하지만, 다양한 도구 환경을 관리하고 복잡한 조율 전략을 수립할 의지가 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 새로운 패턴 등장\n\n확립된 패턴 이상으로, 새로운 방법론과 패턴이 계속해서 등장하고 있습니다. 이 부분에서는 'Push' 및 스트림 처리와 같은 두 가지 새로운 패턴에 대해 논의합니다.\n\n## Push (vs Pull)\n\n이전에 언급된 전통적인 패턴들은 주로 \"Pull\" 유형으로, 여기서는 분석 평면이 운영 평면에서 데이터를 적극적으로 검색합니다. 그에 반해, \"Push\" 방법론은 흐름을 뒤집습니다: 운영 평면이 변경이 발생하는 즉시 데이터를 분석 평면으로 '밀어넣는' 방식으로 작동합니다. 이러한 변경은 생성(Create), 검색(Read), 업데이트(Update), 삭제(Delete) (CRUD) 작업과 같습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Data Ingestion](/assets/img/2024-06-20-DataIngestionPart1ArchitecturalPatterns_6.png)\n\n푸시 방식은 스트리밍 아키텍처 내에서 자주 발견되지만 그것에 국한되지는 않습니다. 기본적으로 운영 평면은 분석 평면이 지정한 엔드포인트로 데이터 전송을 시작하는 것을 의미합니다. 이러한 설정은 일반적으로 개발 팀이 푸시 메커니즘을 구현하도록 요구하며, 별도의 구성 요소를 통해 또는 기존의 운영 응용 프로그램을 강화하여 구현될 수 있습니다.\n\n이 접근 방식의 주요 이점은 분석 팀이 데이터 가치 변환에 집중할 수 있도록 하여 데이터 수집 파이프라인을 구축하는 것에 주의를 기울이지 않게 한다는 것입니다. 그러나 결국 두 가지 주요 단점이 있습니다:\n\n- 전용 애플리케이션 개발팀이 필요함 - 사전 패키지 소프트웨어, 소프트웨어 서비스(SaaS) 제공물 또는 IoT 장치와 같은 외부 하드웨어 때문에 이러한 팀이 존재하지 않거나 쉽게 사용할 수 없는 경우 문제가 될 수 있습니다. 이러한 상황에서는 분석 환경으로의 푸시를 용이하게 하는 전문화된 '데이터 통합 팀'을 설립하는 것이 필요할 수 있지만, 이는 신속하게 병목현상으로 변할 수 있습니다.\n- 푸시 실패 처리 - 풀 방식 아키텍처는 일반적으로 푸시 아키텍처보다 파이프라인 중단에 대한 강한 내성을 나타냅니다. 풀 실패의 경우 분석 플랫폼이 프로세스를 다시 시작할 수 있습니다. 그러나 푸시가 실패하는 경우 분석 플랫폼은 누락된 푸시 메시지에 대해 알지 못할 수 있습니다. 이러한 단점을 극복하기 위해 푸시 기반 파이프라인은 주로 고가용성 스트리밍 아키텍처에 통합되어 있으며 동시 운영 및 강력한 가용성을 위해 설계되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n푸시 패턴은 고도의 소프트웨어 개발 성숙도를 갖춘 조직이나 상용 솔루션을 구매할 때 데이터 푸시 기능을 협상할 수 있는 조직에 가장 적합합니다. 이것이 불가능한 상황에서는 푸시를 다른 데이터 수집 패턴과 결합하여 데이터 통합이 원할하고 효율적으로 이루어지도록 하는 것이 바람직할 수 있습니다.\n\n## 스트림 처리\n\n스트림 처리 또는 이벤트 스트리밍으로도 알려진 스트림 처리는 데이터가 생성됨과 동시에 지속적으로 흐르는 것을 의미하며, 실시간 처리와 분석을 가능케하여 즉각적인 통찰력을 제공합니다. 이러한 시스템은 즉각적인 의사 결정 작업에 중요하며, 금융 거래, 실시간 분석, IoT 모니터링과 같은 활동을 위해 고용량, 저지연 처리를 지원합니다.\n\n![이미지](/assets/img/2024-06-20-DataIngestionPart1ArchitecturalPatterns_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스트림 처리와 분석을 결합할 때 두 가지 접근법이 엿보입니다:\n\n- 스트리밍을 위한 ELT(또는 ETL) 적응 - 이는 실시간 이벤트를 추출하여 데이터 플랫폼으로 로드함으로써 기존 데이터 워크플로우를 특별하거나 전문화된 스트리밍 컨슈머를 통해 현대적 데이터 원천을 유지함을 내포합니다.\n- 스트리밍 캐시 활용 - 중앙집중식이고 내구성 있는 스트리밍 캐시는 이벤트 데이터의 고성능 저장소로 작용합니다. 일부 혁신적인 패턴은 이러한 캐시를 분석적으로 활용하여 공유 데이터 저장소의 현대적이고 효율적인 변형을 만듭니다. 여기에 중요한 고려 사항은 스트리밍 데이터를 정적 데이터 원천과 통합하는 것이며, 스트리밍 캐시를 통해 전달되지 않을 수도 있습니다.\n\n스트림 데이터와 정적 데이터의 통합은 KAPPA 및 LAMBDA와 같은 데이터 아키텍처 패턴에서 설계되어 있습니다. 이 두 가지 아키텍처는 필요할 때 두 세계를 통합할 수 있는 방법을 제공합니다.\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 수집 방법의 전략적 통합은 데이터 분석 분야의 발전하는 풍경에서 중추적인 위치를 차지하고 있습니다. 이 글은 통합 데이터 저장소, 데이터 가상화, ETL 및 ELT라는 네 가지 주요 데이터 수집 패턴을 강조했습니다. 각각은 각기 독특한 장점과 제약이 있습니다. 이러한 패턴을 분석하면서, 통합 데이터 저장소의 간결함과 제한된 확장 가능성, 데이터 가상화의 거의 실시간 기능과 성능에 대한 잠정적인 비용, ETL의 중앙 집중 제어와 잠재적인 병목 현상 및 강제성, 그리고 ELT의 유연성과 확장 가능성이 균형을 이루면서 조정 문제에 직면하는 것을 볼 수 있습니다.\n\n또한, 신생 스트림 처리 패러다임은 산업이 실시간 분석으로의 전환을 역설합니다. 이러한 방법들은 비교적 신생이지만, 정보 생성의 끊임없는 속도에 대응하여 좀 더 즉각적이고 동적인 데이터 처리 방식으로 나아가게 됩니다.\n\n제 다음 글에서는 데이터 플랫폼에 적합한 데이터 수집 도구를 선택하는 데 더 심층적으로 다룰 것입니다. 이 과정에서 중요한 고려 사항은 해당 도구가 통합될 구조적 패턴입니다.\n\n질문이 있으신가요? 피드백이 필요하신가요? LinkedIn에서 저와 연결하거나 직접 Jan@Sievax.be로 연락해 주세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사는 데이터 우수성을 향한 당신을 안내하는 컨설팅 기업 Sievax에서 자랑스럽게 제공합니다. 더 알고 싶으신가요? 저희 웹사이트를 방문해보세요! 데이터 전략 마스터 클래스를 제공하여 데이터 전략의 세계를 깊게 이해할 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-20-DataIngestionPart1ArchitecturalPatterns_0.png"},"coverImage":"/assets/img/2024-06-20-DataIngestionPart1ArchitecturalPatterns_0.png","tag":["Tech"],"readingTime":10},{"title":"왜 매년 이 10일에 데이터 파이프라인이 실패할 것이며 그리고 대처하는 방법","description":"","date":"2024-06-20 15:28","slug":"2024-06-20-WhyYourDataPipelinesWillFailOnThese10DaysEveryYearAndWhatToDoAboutIt","content":"\n\n취업 준비 중이세요? 무료 5페이지 프로젝트 기획 안내서를 활용하여 개인 프로젝트를 개발하여 경쟁력을 확보하세요.\n\n오래 전에 역사 채널에서 아직 역사를 보여주던 시절, 내가 가장 좋아했던 쇼 중 하나는 '사람들 사라진 뒤의 삶'이었습니다. 다른 어떤 역사 채널의 프로그램과 달리 이 시리즈는 한 명의 사람도 등장하지 않습니다. 대신, 시리즈는 어떤 일이 발생할지를 상상합니다. 어느 날 사람들이 그저 사라진다면 문명에 어떤 일이 벌어질지를 상상합니다.\n\n각 에피소드가 1시간이었기 때문에, 이 프로그램은 미래로 꽤 멀리 확장될 수 있었습니다. 각 세그먼트는 동일하게 시작했는데, \"(숫자) 년 후 사람들이 없는 세상\"에서부터 시작했습니다. 나는 포기된 센트럴 파크가 잡초로 덮여지는 시간이 빠르게 흘러가는 것부터 고장난 펌프로 인해 수조에 갇힌 물고기들에게 불가피한 일이 벌어지는 것까지 모든 것을 지켜보았습니다.\n\n이 프로그램의 주요 주장은, 인류 문명의 복잡성과 규모에도 불구하고, 대부분의 인프라가 방치되면 30일 이내에 실패할 것이라는 것이었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시간은 자동화된 코드 또는 데이터 엔지니어링 파이프라인에도 유사한 영향을 미칠 수 있어요. 상류 검사를 추가하거나 단위 테스트를 끝없이 견디더라도, 때로는 지구가 계속 회전하고 시간이 선형적으로 흘러가기로 결정했기 때문에 파이프라인이 실패할 수 있어요.\n\n내 생각에, 신중한 엔지니어들도 예견해야 할 세 가지 중요한 시간 제약 사항이 있어요:\n\n- 일광 절약 시간제 (양일 모두)\n- 윤년\n- \"31\" 문제\n\n![파이프라인 실패 이유와 대처 방법](/assets/img/2024-06-20-WhyYourDataPipelinesWillFailOnThese10DaysEveryYearAndWhatToDoAboutIt_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# DST\n\n데이터 엔지니어로 거의 3년을 일한 후에는, 논란이 되는 미국의 일광 절약 시간 조정과 관련된 최악의 날들을 쉽게 기억할 수 있어요.\n\n일정이 엉망이 되고, 조정이 다시 고려되어야 하며, 더 심각한 것은 모든 것이 놀랍지 않다는 것이예요.\n\n특히 어렵고 힘든 일광 절약 시간이 지난 후, 나는 이제 그런 끔찍한 날들 중 하나에도 데이터가 계속 흐를 수 있도록 견고한 파이프 라인을 구축하기 위해 취하는 조치에 대해 썼어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이번 과제를 해결하는 것은 코딩 조치와 설계적 사고의 결합이 필요합니다.\n\n일광 절약시간제 대책 중 하나는 개발할 때 확고한 습관을 길러야 합니다. 가장 중요한 것은 환경이나 시간 변화에도 일정한 출력을 유지할 수 있도록 논리를 포함한 stateful 파이프라인을 구축하는 것입니다.\n\n게다가, 메타데이터에 적절하게 접근하고 가시성을 확보하는 것은 예상치 못한 실행을 신뢰성 있게 추적하고 처리하며, 필요한 경우 중복을 제거할 수 있도록 도와줍니다.\n\n특정 조건에 대한 안전장치가 없는 시나리오에 직면했을 때, 예를 들어 시간 변경과 같은 상황에는 프로세스 전반에 걸쳐 투명성을 높이는 것이 가장 좋은 대안입니다. 더 많은 로그, 더 많은 타임스탬프, 더 명확하고 더 자주 발생하는 경고가 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 조정이나 재구축이 필요하다면, 가능한 빨리 알아내는 것이 좋습니다. 이미 한 시간 늦거나 빠르기 때문에요.\n\n내가 한 말을 인용해보면:\n\n일관된 피플라인에 대한 DST 영향에 대해 좀 더 자세히 알아보려면, 더 읽어보세요:\n\n## 또 다른 네 해\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDST만큼 빈번하지는 않지만 동일하게 귀찮은 문제인 윤년도 있습니다. 일할 때 하류로부터 시간에 기반한 논리를 수정할 때 \"4년 후에 나를 감사해줄 사람이 있을 거야\"라고 말했습니다.\n\n기술적으로는 이 사안을 내버려둘 수도 있었고 파이프라인은 3월 1일에 작동했을 것입니다. 그러나 다운타임과 같은 지표에 대한 평가를 받을 때는 가능한 정확해질 필요가 있습니다.\n\n숫자가 개인 및 팀 업적에 어떤 영향을 미치는지를 고려하면, 데이터를 일관되고 신뢰성 있게 제공할 수 있는 능력은 팀의 신뢰성과 조직 내 영향력을 향상(또는 유지)시키는 데 도움이 됩니다.\n\nPython 스크립트에서 윤년을 고려하면 많은 번거로운 if/elif 논리가 포함된 이상한 과정으로 느껴질 것 같습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적으로 사용 사례에 따라 그렇지 않습니다.\n\n사실, 2024년 윤일에 푸시한 수정은 5 줄 미만이었습니다.\n\n문제의 핵심은 Python의 datetime 모듈이 일과 날짜만으로 연도를 유추할 수 없다는 것입니다.\n\n따라서 월/일 형식만 사용하는 파이프라인이 있다면:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndt_var = date_var.strptime(\"%m/%d\")\n\nif dt_var.day == datetime.now():\n  \n  # 뭔가를 수행합니다\n```\n\n2월 29일에는 파이프라인이 3월 1일로 기본 설정됩니다.\n\n그러나 연도를 제공하면 datetime 모듈이 올바른 하루 증분으로 증가시키는 데 필요한 모든 컨텍스트를 가지고 있습니다. 예를 들어 2/29/24는 올바른 증분입니다.\n\n```js\ndt_var = date_var.strptime(\"%m/%d/%y\")\n\nif dt_var.day == datetime.now():\n  \n  # 뭔가를 수행합니다\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n리프 데이 수정은 몇 분 동안 머리를 갈구었다가 과도하게 생각해서 자신을 탓한 일 중 하나입니다.\n\n일반적으로 가능하고 논리적인 경우, 완전한 날짜 문자열을 포함하십시오.\n\n변수를 정의한 후에는 항상 다양한 날짜 부분에 액세스할 수 있습니다.\n\n# 31 문제\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로 가장 흔하고 복잡한 날짜 문제를 남겼습니다.\n\n이 문제를 '31 문제'라고 부릅니다.\n\n최근 파일 이름 문자열에 포함된 날짜를 검색해야 했던 파이프라인의 문제를 해결하고 있었습니다. 예를 들어 \"내 파일 2024년 3월 10일\" 같은 형식이었습니다.\n\n이를 위해 이전 타임스탬프 변수를 기반으로 월을 추론했지만, 불행히도 그렇게 할 수밖에 없었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom datetime import datetime\n\nprev = datetime.now() - timedelta(days=1)\n\nprev_day = prev.day\n\nprev_month = prev.month\n\nif f\"{prev_month}/{prev_day}\" in file_str:\n  \n  # Proceed\n```\n\n이 스크립트를 새로운 달인 4월의 첫째 날에 실행했을 때 출력이 \"April 31\"로 나왔던 이유는 datetime.now()이 추가 일이 있는 경우 prev에 제시될 때 현재 월을 검색하도록 알고 있는 게 아니기 때문입니다. \n\n이 문제를 해결하기 위한 두 단계가 필요합니다.\n\n- 스크립트에 31일이 있는 월을 알려주는 논리 작성\n- 이전 논리에 따라 이전 월 값 또는 현재 월 값을 반환할지 확인하는 논리 작성\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같이 접근 방식을 취합니다:\n\n```js\n# 31일 함수\n\ndef month_31(month: int):\n\n  if month in [1, 3, 5, 7, 8, 10, 12]:\n    day = 31\n\n    return day\n\n  else:\n    day = 30\n\n    return day\n\ndef return_prev_month(day: str):\n  \n  if day in list(range(29, 32)):\n    \n    month_days = month_31(month_int)\n  \n    if month_days == 30 or month_days == 31:\n        \n        month = prev.month\n\n        return month\n\n    else:\n\n        month = now.month\n\n        return month\n```\n\n저에게는 이것이 매 스크립트에 넣어야 하는 논리가 아닙니다. 하지만 조금 장황한 것이기는 하지만, 이는 바로 문제를 해결하고, “31 문제\"에 견딜 수 있게 파이프라인을 1년 중 7개월 동안 더 견고하게 만드는 데 도움이 되었습니다.\n\n데이터 수집의 여러 측면과 마찬가지로 날짜는 민감한 구성 요소입니다. 파이프라인이 실행 중에 많은 단계에서 실패할 수 있을 때, 저에게는 시간의 경과와 특정 날짜의 도착을 예상하고 최적화하는 것이 합리적으로 보입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비록 날짜 논리가 ETL 또는 데이터 인프라의 가장 흥미로운 측면은 아니지만, 그것은 시간이 지남에 따라 악화될 수 있는 그런 섬세한 문제 중 하나에요.\n\n더군다나 버전이 만료되는 가능성, 플랫폼 변경 및 조직적 선호도의 변화가 더해지면 파이프라인이 망가질 수도 있어요.\n\n사람 없는 세상(Life After People)과는 달리, 데이터 파이프라인의 천천히 떨어지는 변화는 지켜보기가 재미없을 거에요.\n\n도와주시길 원해요. 이 블로그 외에서 어떻게 도울 수 있는지 알려주실 3가지 질문에 답해주시면 됩니다. 모든 응답자에게 무료 선물이 제공됩니다.","ogImage":{"url":"/assets/img/2024-06-20-WhyYourDataPipelinesWillFailOnThese10DaysEveryYearAndWhatToDoAboutIt_0.png"},"coverImage":"/assets/img/2024-06-20-WhyYourDataPipelinesWillFailOnThese10DaysEveryYearAndWhatToDoAboutIt_0.png","tag":["Tech"],"readingTime":5},{"title":"3분 안에 Airflow와 DAG 이해하기","description":"","date":"2024-06-20 15:27","slug":"2024-06-20-UnderstandingAirflowandDAGsin3minutes","content":"\n\n![Understanding Airflow and DAGs in 3 minutes](/assets/img/2024-06-20-UnderstandingAirflowandDAGsin3minutes_0.png)\n\n안녕하세요! 이 기사에서는 Apache Airflow가 무엇인지 그리고 전통적인 ETL(Extract-Transform-Load) 워크플로우 내에서 어떤 문제를 해결하는지 예를 통해 알아보겠습니다. 또한, Directed Acyclic Graphs (DAGs)가 Airflow 내에서 작업을 구현하는 데 어떻게 활용되는지 살펴볼 것입니다.\n\n예시: 매일 새벽 12시에 벤더 서버에서 매출 데이터 파일을 검색하여 변환한 후 데이터 웨어하우스에 로드하는 ETL 작업이 있다고 상상해보세요. cron 스케줄러를 사용하여 이 작업을 프로덕션 서버에서 성공적으로 트리거합니다.\n\n문제 1: 만약 어느 날 이 작업이 실패한다면 어떻게 될까요? 문제는 추출, 변환 또는 로딩 프로세스에서 발생한 것일까요? 문제 해결을 위해 흩어진 로그를 통해 쥐잡이 게임이 되어버립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문제 2: 이제 10에서 50개의 ETL 작업을 관리하는 시나리오로 확장해 봅시다. 의존 관계를 추적하고 단일 작업 실패가 혼란을 일으킬 수 있는 것을 방지하는 것은 어려운 과제가 됩니다.\n\n문제 3: 게다가, 한 작업이 다른 작업을 트리거해야 하는 경우가 있고, 그들의 타이밍이 동기화되어 있지 않다면 어떻게 해야 할까요? 생산 서버와 크론 작업만을 이용해서 이러한 복잡성을 관리하는 것은 가리지 않은 눈으로 고양이들을 몰고 다니는 것과 같습니다.\n\nAirflow를 당신의 명령 센터로 생각해보세요. 여기서 ETL 작업을 시각화하고 손쉽게 관리할 수 있습니다. DAG를 사용하여 Airflow는 작업의 순서를 지도화하여 의존 관계가 명확하고 실패가 격리되도록 합니다. DAG가 무엇인지 이해해 봅시다...\n\nDIRECTED ACYCLE GRAPH(DAG):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nApache Airflow의 DAG는 각 작업을 나타내는 작업 단위로 구성되며 작업 간의 관계가 워크플로를 정의합니다.\n\n위의 예에서는 세 가지 작업으로 구성된 \"SALES_ETL_DAG\"를 개념적으로 구성할 수 있습니다:\n\n- 작업 A: 공급 업체 서버에서 Sales 파일에서 데이터 추출\n- 작업 B: 추출한 데이터 변환\n- 작업 C: 변환된 데이터를 데이터 웨어하우스에 로드\n\n(참고: 이러한 모든 작업은 Python으로 작성할 수 있으며 Airflow 자체가 Python으로 개발되었으며 주로 Python과 함께 작동합니다)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n실행 순서는 중요합니다: 작업 A는 성공적으로 완료되어야 작업 B가 시작할 수 있고, 작업 B는 완료되어야 작업 C가 시작할 수 있습니다. 이 순차적인 흐름은 의존성을 존중하고 작업이 역행 없이 단방향으로 진행되도록 보장합니다 — 이것이 DAG의 \"Directed\" 측면입니다.\n\n이 구조 내에는 순환 의존성이나 루프가 없습니다 : 각 작업은 의존성에 의해 정의된 구조적인 순서대로 실행되어 정돈된 작업 진행이 보장됩니다 — 이는 DAG가 \"Acyclic\"임을 보장합니다.\n\nDAG 표현: 작업은 노드로 표시되고 작업 간의 의존성은 이러한 노드 사이의 방향성 있는 에지로 표시됩니다 — 이는 워크플로우의 \"Graph\" 표현을 형성합니다.\n\n![Understanding Airflow and DAGs](/assets/img/2024-06-20-UnderstandingAirflowandDAGsin3minutes_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAirflow은 작업의 종속성과 지정된 논리에 따라 작업을 예약하고 모니터링하는 DAGs를 사용하여 복잡한 워크플로우를 쉽게 관리하고 시각화할 수 있습니다.\n\nAirflow은 작업을 예약하고 모니터링하는 것뿐만 아니라 실패한 작업을 다시 시도하고 각 단계를 로깅하며 잠재적인 문제에 대한 경고를 제공합니다. 데이터 작업에 대한 제어 탑을 가지고 있는 것과 같으며 다양한 기술과 서비스와 매끄럽게 통합됩니다.\n\n요약하면, Airflow은 그냥 다른 도구가 아니라 현대 데이터 엔지니어링에 필수적인 자산입니다. ETL 프로세스에 질서, 신뢰성 및 효율성을 제공하여 팀이 혁신에 집중할 수 있도록 도와줍니다. Airflow에 대한 이 개요가 유익하고 통찰력 있었기를 바랍니다.\n\nAirflow 환경을 처음부터 설정하고 기본 ETL 파이프라인을 구현해 보고 싶다면 제 다른 글을 확인해보세요. 여기 링크입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 중요한 것은 테이블 태그를 마크다운 형식으로 변경하는 것입니다.","ogImage":{"url":"/assets/img/2024-06-20-UnderstandingAirflowandDAGsin3minutes_0.png"},"coverImage":"/assets/img/2024-06-20-UnderstandingAirflowandDAGsin3minutes_0.png","tag":["Tech"],"readingTime":3},{"title":"Snowflake를 위한 CICD 및 DevOps 포괄적인 가이드","description":"","date":"2024-06-20 15:25","slug":"2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide","content":"\n\n데이터 엔지니어링 및 데이터베이스 관리의 세계에서는 지속적 통합/지속적 배포(CI/CD) 실천이 민첩하고 신뢰성 있으며 효율적인 개발 워크플로우에 중요합니다. 성장 중인 클라우드 기반 데이터 웨어하우징 플랫폼인 Snowflake는 확장성, 적응성, 우수한 성능으로 유명합니다. 그러나 Snowflake를 위해 CI/CD를 구현하는 것은 표준화된 실천 방식과 특화된 도구 부재로 인한 독특한 도전에 직면하고 있습니다. 더하여 데이터베이스 프로젝트에 특화된 DevOps 및 CI/CD 워크플로에 대한 모범 사례를 상세히 설명하는 종합적인 문서 부재 문제가 있습니다.\n\n이러한 도전에도 불구하고, 최근 Snowflake 내에서 유망한 발전이 있었으며, 새로운 기능 도입으로 DevOps 및 CI/CD 실천에 대한 명확한 지침과 도구 제공에 대한 가능성을 시사하고 있습니다. 이에 따라 Snowflake 사용자를 위한 DevOps 프로세스 표준화 접근 방식과 향상된 문서 작성을 간소화하기 위한 기대가 커지고 있습니다.\n\n이 기사에서는 최근 기능 및 모범 사례를 활용한 Snowflake용 CI/CD 및 DevOps 설정의 총체적 데모에 대해 심층적으로 다룰 것입니다.\n\n# 소개: SQL Server에서 Snowflake로의 간극을 메꾸는 것\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSQL Server 출신이신 분으로서 Snowflake로 전환하면서, 데이터베이스 객체를 관리하기 위한 표준이 없다는 점이 큰 장벽이었습니다. SQL Server의 SSDT(SQL Server Data Tools) 접근 방식과 DACPAC(Data-tier Application Component Package) 파일은 데이터베이스 변경 관리(DCM)를 선언적으로 처리하는 방법을 제공했지만, Snowflake에는 이와 같은 표준화된 접근 방식이 없었습니다. 초기 접근 방식은 Terraform과 같은 도구나 Schemachange 또는 Flyway와 같은 명령중심의 DCM 솔루션에 의존하는 경향이 있었는데, 이는 모든 사람들의 선호에 부합하지 않을 수 있습니다.\n\n다행히도 Snowflake은 CREATE OR ALTER, EXECUTE IMMEDIATE FROM, Snowflake CLI 및 Git 통합과 같은 선언적 DCM을 위한 기본 블록을 도입하고 있습니다. 이러한 발전을 통해 Snowflake에 대한 CI/CD에 대해 더 구조화되고 효율적인 접근 방식이 가능해졌습니다.\n\n## Snowflake의 CI/CD를 위한 Building Blocks 활용하기\n\nSnowflake의 최근 기능은 CI/CD 및 DevOps 실천을 효과적으로 구현하기 위한 기초를 제공합니다. 이러한 기본 블록은 배포 프로세스를 자동화하고 협업을 강화하며 환경 간 일관성을 보장하는 사용자들을 능력을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_0.png\" /\u003e\n\n1. CREATE OR ALTER\n\n스노우플레이크의 CREATE OR ALTER 기능을 사용하면 현재 상태에 대해 걱정할 필요없이 데이터베이스 테이블의 원하는 상태를 정의할 수 있습니다. 이 선언적 접근 방식은 데이터베이스 객체의 관리를 단순화하고 원활한 배포 워크플로우를 용이하게 합니다.\n\n2. EXECUTE IMMEDIATE FROM\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n#### 1. EXECUTE IMMEDIATE FROM command\n\nThe EXECUTE IMMEDIATE FROM command allows you to execute SQL statements stored in external files or URLs. This feature is crucial for automating deployment tasks and orchestrating deployment processes within Snowflake.\n\n#### 2. Snowflake CLI\n\nSnowflake CLI allows developers to run SQL queries, ad-hoc queries, or SQL query files effortlessly using the `snow sql` command. This functionality improves development workflows, making it easier to execute and manage queries efficiently within Snowflake environments.\n\n#### 3. Git Integration\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSnowflake와 Git 저장소를 통합하면 코드의 중앙 집중식 데이터 원천이 제공되어 협업 및 버전 제어 기능을 강화할 수 있습니다. 개발자들은 Snowflake 환경 내에서 변경 사항을 추적하고 브랜치를 관리하며 Git 워크플로를 신속하게 활용할 수 있습니다. 이는 팀워크를 촉진하고 효율적인 배포 파이프라인을 용이하게합니다.\n\n# Snowflake를 위한 CI/CD 구현: 단계별 안내\n\n![이미지](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_1.png)\n\n## 사전 요구 사항:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Snowflake 계정 (최소 sysadmin 액세스 권한)\n- GitHub 계정\n- Git 저장소 (관리자 액세스 권한)\n\n## 실제 데모 플로우:\n\nSnowflake를 위한 CI/CD를 설정하는 단계를 따라해보세요:\n\n- 데이터베이스 및 스키마 생성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_2.png\" /\u003e\n\n다른 환경에서도 같은 작업을 수행하세요.\n\n참고: 데이터베이스 및 스키마는 기본적으로 변경되지 않는 객체이므로 Terraform과 같은 인프라 코드(IAC) 도구를 통해 이상적으로 관리됩니다. 그러나 이 데모에서는 수동으로 생성합니다.\n\n2. Snowflake를 Git 리포지토리에 연결하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nStep 2.1: GitHub에서 개인 액세스 토큰(PAT)을 생성합니다.\n\nGitHub 계정의 Settings → Developer Settings → Personal Access Tokens → Tokens (classic)으로 이동하여 PAT을 생성합니다.\n\nStep 2.2: GitHub PAT을 저장할 Secret을 생성합니다.\n\n![GitHub PAT 이미지](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nStep 2.3: Snowflake 내에서 Git API 통합을 생성하세요.\n\n![이미지](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_4.png)\n\nStep 2.4: Snowflake 객체 및 구성을 저장할 Git 저장소를 설정하세요.\n\n![이미지](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 배포를 위한 서비스 계정 및 기타 Snowflake 객체 생성\n\n배포를 위해 서비스 계정 및 필요한 기타 Snowflake 객체를 설정하세요. 적절한 권한 및 접근 제어를 보장해주세요.\n\n![이미지](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_6.png)\n\n4. 로컬 개발\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희 지역 개발 환경 설정에 대해 Snowflake 프로젝트의 폴더 계층 구조를 아래 이미지처럼 구성하는 것을 권장합니다. 각 스키마마다 별도의 폴더가 할당되며, 각 스키마 폴더 내에서는 객체 유형에 따라 하위 폴더로 객체를 더 구성할 수 있습니다.\n\n뿐만 아니라, 루트 수준에 스크립트 폴더를 포함하는 것을 제안합니다. 이를 통해 사전 배포 및 사후 배포 작업을 수용하여 DACPAC이 데이터베이스 프로젝트를 구성하는 방식과 유사하게 잘 구성되고 쉽게 탐색할 수 있는 구조를 유지할 수 있습니다.\n\n테이블을 생성할 때 Snowflake의 CREATE OR ALTER 문을 사용하는 간편함과 효율성을 보여 드릴 수 있습니다. 이를 통해 새 테이블을 생성하거나 기존 테이블을 수정하는 등 테이블 스키마를 효과적으로 관리할 수 있습니다. CREATE OR REPLACE로 이미 정의된 테이블의 경우, CREATE OR ALTER로의 전환은 빠르게 조정할 수 있어 배포 파이프라인과의 원활한 호환성을 보장할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_8.png\" /\u003e\n\n또한 Jinja 템플릿을 통합하여 배포 프로세스를 확장할 수 있습니다. 이를 통해 유연성과 확장성을 제공합니다. 예를 들어 데이터베이스 이름을 매개변수화함으로써 배포 중에 대상 환경을 동적으로 선택할 수 있으며, 이는 다양한 배포 시나리오에서 프로세스를 간소화합니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_9.png\" /\u003e\n\n다시 한번 제시된 폴더 계층 구조를 강조하면, 프로젝트 폴더 안에서 sf_deploy_dev.sql과 sf_deploy_tst.sql 두 가지 필수 스크립트를 찾을 수 있습니다. 이 스크립트는 Snowflake 객체의 배포를 조정하는 배포 진입점으로 작용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기에 그들의 구조를 보여주는 샘플 스니펫이 있어요:\n\n\n![Snowflake CLI](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_10.png)\n\n\n5. 로컬 Snow CLI 사용\n\n내 경험상 Snowflake CLI를 전체 배포 파이프라인에 뛰어들기 전에 로컬에서 미리 테스트하는 것이 유익합니다. 이를 통해 만든 서비스 계정을 사용하여 연결을 확인하고 올바른 액세스 권한을 확인하며 쉬운 디버깅 및 테스트를 용이하게 할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저 Snow CLI를 설치해야 합니다. Snow CLI는 Snowflake 문서에 제공된 설치 지침을 따라 간단히 설치할 수 있어요.\n\n![Snowflake CLI](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_11.png)\n\n설치가 완료되면 CLI 사용 및 명령어에 대한 안내를 위해 문서를 참조할 거에요. 우리의 경우 배포 워크플로에서 두 가지 SQL 명령어를 주로 활용할 거에요.\n\n먼저 ALTER GIT REPOSITORY 명령어를 사용하여 링크된 Git 저장소에서 업데이트를 가져올 거에요. 이를 통해 Snowflake 환경이 코드베이스의 최신 변경 사항과 동기화되도록 할 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n표시되어 있는 내용을 아래와 같이 번역해 드리겠습니다.\n\n\n![2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_12](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_12.png)\n\n그 후에는 EXECUTE IMMEDIATE FROM 명령어를 사용하여 외부 파일이나 URL에 저장된 SQL 문을 실행할 것입니다. 우리의 경우에는 이 명령어를 사용하여 대상 데이터베이스로 객체를 배포할 것이며, 배포 스크립트 경로를 참조하게 됩니다. 예를 들면 sf_deploy_dev.sql과 같습니다.\n\n![2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_13](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_13.png)\n\nSnowflake CLI 명령어에서는 계정, 사용자, 창고와 같은 입력값을 지정하여 연결을 설정할 것입니다. 이후에는 이러한 입력값들을 GitHub 비밀에 저장하여 보안 및 편의성을 갖출 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n6. GitHub Workflow를 만들기\n\nGitHub Actions를 구성하여 대상 환경으로의 배포를 자동화합니다.\n\n7. GitHub Secrets 채우기\n\nGitHub에 필요한 시크릿과 환경 변수를 채워 넣어, 워크플로우 실행 중에 Snowflake 및 Git 리포지토리에 안전하게 액세스합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n리포지토리에서 Settings → Secrets and Variables → Actions → New repository secret로 이동하세요.\n\n![이미지](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_14.png)\n\n8. 테스트 워크플로우\n\n수동으로 또는 자동화된 이벤트(예: main으로 푸시/머지)를 트리거하여 구성된 GitHub 워크플로우를 유효성 검사하고, 배포 작업의 성공적인 실행과 올바른 처리를 확인하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Snowflake에서 유의할 사항 및 권장 사항\n\n## 유의할 사항:\n\n1. CREATE OR ALTER TABLE 사용 시 제한 사항\n\n- 이 명령의 제한 사항은 특히 특정 위치에 열을 삽입해야 할 때 제한적일 수 있습니다.\n- 테이블에 대한 제약은 이 명령을 사용하여 뷰나 저장 프로시저와 같은 다른 데이터베이스 객체를 변경할 수 없다는 것을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. Snowsight에서 CREATE OR ALTER 동작\n\n- 명령이하는 작업과 Snowsight에 표시되는 것 사이의 불일치는 정말 혼란스러울 수 있습니다. CREATE OR ALTER를 통해 테이블이 생성되었더라도, Snowsight에서는 정의상 CREATE OR REPLACE로 표시됩니다. 이것이 Snowflake의 의도인지 또는 버그인지 확실하지 않습니다.\n\n3. YAML 파이프라인에서의 단일 데이터베이스 연결\n\n- 모든 환경에서 동일한 데이터베이스 연결을 사용하여 일관성을 유지하는 것이 실용적으로 보일 수 있지만, Snowflake가 이 접근 방식을 권장하거나 요구하는지를 고려하는 것이 중요합니다. 테스트(TST) 및 프로덕션(PRD)과 같이 더 높은 환경으로 전개하더라도, Git 저장소가 연결된 개발(dev) 환경과 일치하는 단일 연결을 유지합니다. 이 선택은 EXECUTE IMMEDIATE와 같은 SQL 명령의 올바른 실행을 보장합니다. 그러나 Snowflake가 각 환경에 대해 별도의 Git 저장소 연결을 설정하여 다른 데이터베이스 연결 사용을 가능하게 하는 것을 필수로 하는지는 명확하지 않습니다. Snowflake의 권장 사항에 대한 명확한 설명이 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 권장사항:\n\n1. 배포 프로세스 중 더 나은 가시성을 위한 강화된 로깅 기능.\n\n- 현재 배포 중 로그에서는 마지막 EXECUTE IMMEDIATE 명령의 출력만 캡처되어 전체 배포 프로세스의 가시성이 제한됩니다.\n- Snowflake이 배포 중 수행하는 모든 작업을 포괄적으로 보여주는 강화된 로깅 기능을 구현하는 것이 유익할 것입니다. 이렇게 하면 배포 활동을 더 잘 추적하고 문제 해결 및 감사에 도움이 될 수 있습니다.\n\n2. Snowflake에 의한 배포 artifacts의 개발\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 높은 환경으로의 배포 프로세스를 간단히 하기 위해서는 Snowflake에 의한 배포 아티팩트 또는 배포 패키지의 개발을 탐색하는 것이 좋습니다.\n- DACPAC(배포용 아티팩트로 빌드되는 SSDT의 접근 방식과 유사하게, Snowflake에서는 배포 아티팩트를 생성하고 관리하기 위한 간소화된 메커니즘을 제공할 수 있습니다.\n- 이 접근 방식은 Snowflake가 다른 환경으로의 배포를 위해 필요한 SQL 스크립트, 구성 및 메타데이터를 포함하는 배포 패키지 또는 아티팩트를 생성하는 것을 포함할 것입니다.\n\nSnowflake에 대한 CI/CD 및 DevOps 실천 방법을 구현하면, 기관은 데이터 엔지니어링 워크플로우에서 민첩성, 협업 및 자동화를 채택할 수 있습니다. 이 가이드에서 소개된 Snowflake의 최신 기능과 모베스트 프랙티스를 활용하여, 팀은 배포 프로세스를 최적화하고 오류를 최소화하며 데이터 중심 이니셔티브의 시장 진입 시간을 가속화할 수 있습니다.\n\nSnowflake가 계속 발전함에 따라, 현대 데이터 엔지니어링의 빠르게 변화하는 지형에서 선도하기 위해 CI/CD를 채택하는 것이 중요해집니다.","ogImage":{"url":"/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_0.png"},"coverImage":"/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_0.png","tag":["Tech"],"readingTime":9},{"title":"Chronon, 에어비앤비의 기계 학습 피처 플랫폼, 이제 오픈 소스로 공개합니다","description":"","date":"2024-06-20 15:22","slug":"2024-06-20-ChrononAirbnbsMLFeaturePlatformIsNowOpenSource","content":"\n\n테이블 태그를 Markdown 형식으로 변경해주세요.\n\nA feature platform that offers observability and management tools, allows ML practitioners to use a variety of data sources, while handling the complexity of data engineering, and provides low latency streaming.\n\n![ChrononAirbnbsMLFeaturePlatformIsNowOpenSource_0](/assets/img/2024-06-20-ChrononAirbnbsMLFeaturePlatformIsNowOpenSource_0.png)\n\nBy: Varant Zanoyan, Nikhil Simha Raprolu\n\n![ChrononAirbnbsMLFeaturePlatformIsNowOpenSource_1](/assets/img/2024-06-20-ChrononAirbnbsMLFeaturePlatformIsNowOpenSource_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAirbnb에서는 우리의 ML Feature Platform 인 Chronon을 오픈 소스로 공개하게 된 것을 기쁘게 생각합니다. 우리 커뮤니티 Discord 채널에 참여하여 함께 대화해보세요.\n\n우리는 Stripe와 함께 이 프로젝트의 초기 채택자 및 공동 유지 보수자로서 이 공지를 함께 전하게 되어 기쁘게 생각합니다.\n\n이 블로그 글은 Chronon의 주요 동기와 기능에 대해 다루고 있습니다. Chronon의 핵심 개념에 대한 개요는 이전 게시물을 참조해주세요.\n\n# 배경\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChronon은 ML 실무자들의 공통된 고민을 해소하고자 만들어졌어요: 모델링에 대부분의 시간을 쏟는 대신 모델을 구축하는 데 필요한 데이터를 관리하는 일에 시간을 많이 써야 했기 때문이죠.\n\nChronon 이전에는, 실무자들은 일반적으로 다음 두 가지 방법 중 하나를 사용했어요:\n\n- 오프라인-온라인 복사: ML 실무자들은 데이터 웨어하우스에서 모델을 학습한 뒤 온라인 환경에서 해당 기능을 복제하는 방법을 찾았죠. 이 방법의 이점은 데이터 웨어하우스 전체, 즉 데이터 소스와 대규모 데이터 변환에 사용되는 강력한 도구를 활용할 수 있다는 것이에요. 그러나 이 방법은 온라인 추론에 모델 기능을 제공하는 명확한 방법을 제공하지 않아 일관성 및 레이블 유출로 모델 성능에 심각한 영향을 미치는 단점이 있어요.\n- 기록하고 대기: ML 실무자들은 모델 추론을 실행할 온라인 서빙 환경에서 사용 가능한 데이터로 시작합니다. 관련 특징을 데이터 웨어하우스에 기록합니다. 충분한 데이터가 축적되면, 해당 로그를 사용하여 모델을 학습하고 같은 데이터로 서빙합니다. 이 방법의 이점은 일관성이 보장되며 유출 가능성이 낮다는 것이에요. 그러나 이 방법의 주요 단점은 대기 시간이 길어질 수 있어서 변화하는 사용자 행동에 빠르게 대응하는 능력을 저해할 수 있다는 것이죠.\n\nChronon 접근 방식은 양쪽을 모두 최대한 활용할 수 있게 해줘요. Chronon을 사용하면 ML 실무자들은 특징을 한 번 정의하면 됩니다. 이를 통해 오프라인 흐름은 모델 학습을 위한 것뿐만 아니라 온라인 흐름에서 모델 추론에도 사용됩니다. 또한, Chronon은 특징 연결, 가시성 및 데이터 품질을 위한 강력한 도구, 특징 공유 및 관리도 제공해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 작동 방식\n\n아래에서는 Chronon의 대부분의 기능을 구동하는 주요 구성 요소를 살펴보며 빠른 시작 가이드에서 파생된 간단한 예제를 사용합니다. 이 예제를 실행하려면 해당 가이드를 따를 수 있습니다.\n\n온라인 대규모 소매업체라고 가정해 보겠습니다. 사용자들이 구매를 하고 나중에 상품을 반품하는 사기 벡터를 감지했습니다. 주어진 거래가 사기적인 반품으로 이어질 가능성을 예측하는 모델을 훈련하고자 합니다. 사용자가 체크아웃 과정을 시작할 때마다 이 모델을 호출할 것입니다.\n\n## 기능 정의\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n구매 데이터: 구매 로그 데이터를 사용자 수준으로 집계하여이 사용자가 플랫폼에서 이전에 수행한 활동을 볼 수 있습니다. 구체적으로 다양한 시간 창을 통해 이전 구매 금액의 SUM, COUNT 및 AVERAGE를 계산할 수 있습니다.\n\n```js\nsource = Source(\n    events=EventSource(\n        table=\"data.purchases\", # 이는 매일 일괄 업데이트되는 데이터 웨어하우스의 로그 테이블을 가리킵니다.\n        topic=\"events/purchases\", # 스트리밍 소스 토픽\n        query=Query(\n            selects=select(\"user_id\",\"purchase_price\"), # 우리가 관심 있는 필드를 선택합니다.\n            time_column=\"ts\") # 이벤트 시간\n    ))\n\nwindow_sizes = [Window(length=day, timeUnit=TimeUnit.DAYS) for day in [3, 14, 30]] # 아래에서 사용할 몇 가지 창 크기를 정의합니다.\n\nv1 = GroupBy(\n    sources=[source],\n    keys=[\"user_id\"], # 사용자별 집계 중\n    online=True,\n    aggregations=[Aggregation(\n            input_column=\"purchase_price\",\n            operation=Operation.SUM,\n            windows=window_sizes\n        ), # 다양한 창에서 구매 가격의 합\n        Aggregation(\n            input_column=\"purchase_price\",\n            operation=Operation.COUNT,\n            windows=window_sizes\n        ), # 다양한 창에서 구매 횟수\n        Aggregation(\n            input_column=\"purchase_price\",\n            operation=Operation.AVERAGE,\n            windows=window_sizes\n        ), # 다양한 창에서 사용자별 평균 구매\n        Aggregation(\n            input_column=\"purchase_price\",\n            operation=Operation.LAST_K(10),\n        ), # 목록으로 집계된 마지막 10개 구매 가격\n    ],\n)\n```\n\n이렇게하면 `GroupBy`가 생성되어 `purchases` 이벤트 데이터를 사용자별 주요 키로 계속하여 시간 범위별로 다양한 필드를 집계하여 유용한 기능으로 변환합니다.\n\n이는 사용자 수준에서 구매 기록 데이터를 유용한 기능으로 변환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n유저 데이터: 유저 데이터를 피처로 변환하는 것은 조금 더 간단합니다. 주로 집계를 수행할 필요가 없기 때문입니다. 이 경우, 원본 데이터의 주요 키와 피처의 주요 키가 동일하므로 행을 대상으로 집계를 수행하는 대신 단순히 열 값을 추출할 수 있습니다:\n\n```js\nsource = Source(\n    entities=EntitySource(\n        snapshotTable=\"data.users\", # 모든 사용자의 일별 스냅샷을 포함하는 테이블을 가리킴\n        query=Query(\n            selects=select(\"user_id\",\"account_created_ds\",\"email_verified\"), # 관심 있는 필드를 선택\n        )\n    ))\n\nv1 = GroupBy(\n    sources=[source],\n    keys=[\"user_id\"], # 주요 키는 소스 테이블의 주요 키와 동일함\n    aggregations=None, # 이 경우 집계나 윈도우를 정의할 필요가 없음\n    online=True,\n) \n```\n\n이를 통해 `user_id`를 주요 키로 사용하여 `data.users` 테이블로부터 차원을 추출하는 `GroupBy`가 생성됩니다.\n\n피처를 결합하기: 다음으로, 이전에 정의된 피처를 백필링되어 모델 훈련에 사용되고 모델 추론을 위해 온라인으로 제공될 수 있는 단일 뷰로 결합해야 합니다. Join API를 사용하여 이를 달성할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리의 사용 사례에서 피처가 올바른 타임스탬프로 계산되는 것이 매우 중요합니다. 모델이 체크아웃 플로우를 시작할 때 실행되기 때문에, 온라인 추론에서 모델이 볼 것과 일치하는 피처 값이 모델 훈련을 위해 논리적으로 일치하도록 백필에 해당 타임스탬프를 사용하고 싶습니다.\n\n아래는 정의가 어떻게 보이는지에 대한 예시입니다. 기존에 정의된 피처와 returns(반품)이라는 다른 피처 세트를 API의 right_parts 부분에 결합하는 것을 확인할 수 있습니다.\n\n```js\nsource = Source(\n    events=EventSource(\n        table=\"data.checkouts\",\n        query=Query(\n            selects=select(\"user_id\"), # 다양한 GroupBy를 함께 결합하기 위해 사용되는 기본 키\n            time_column=\"ts\",\n            ) # 피처 값을 계산하기 위해 사용되는 이벤트 시간\n    ))\n\nv1 = Join(\n    left=source,\n    right_parts=[JoinPart(group_by=group_by) for group_by in [purchases_v1, returns_v1, users]] # 세 가지 GroupBy를 포함\n)\n```\n\n# 백필/오프라인 계산\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에 나와 있는 Join 정의로 사용자가 할 수 있는 첫 번째 작업은 역사적인 피쳐 값들을 모델 훈련을 위해 생성하기 위해 백필을 실행하는 것입니다. Chronon은 몇 가지 주요 이점을 갖고 이 백필을 수행합니다:\n\n- 시점 정확도: 위의 Join에서 \"left\" 쪽으로 사용된 소스를 주목해보세요. 이 소스는 각 행마다 해당 특정 체크아웃의 논리적 시간에 해당하는 \"ts\" 타임스탬프를 포함하는 \"data.checkouts\" 소스 위에 구축되어 있습니다. 모든 피쳐 계산은 해당 타임스탬프를 기준으로 창 정확성이 보장됩니다. 따라서 이전 사용자 구매의 1개월 합계에 대한 경우, 왼쪽 소스에서 제공된 타임스탬프를 기준으로 각 행이 사용자에 대해 계산될 것입니다.\n- Skew 처리: Chronon의 백필 알고리즘은 매우 편향된 데이터셋을 처리하는 데 최적화되어 있어, 귀찮은 OOM과 작업 멈춤을 피할 수 있습니다.\n- 계산 효율 최적화: Chronon은 백엔드에 직접 여러 최적화를 적용할 수 있어 계산 시간과 비용을 줄일 수 있습니다.\n\n# 온라인 계산\n\nChronon은 온라인 피쳐 계산을 위한 많은 복잡성을 추상화합니다. 위의 예에서 해당 피쳐가 일괄 피쳐인지 또는 스트리밍 피쳐인지에 따라 피쳐를 계산할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일괄 기능 (예: 위의 사용자 기능)\n\n사용자 기능은 기존의 일괄 테이블 위에 구축되어 있기 때문에, Chronon은 매일 배치 작업을 실행하여 새 데이터가 일괄 데이터 저장소에 도착하면 새 기능 값을 계산하고 이를 온라인 KV 저장소에 업로드하여 제공합니다.\n\n스트리밍 기능 (예: 위의 구매 기능)\n\n구매 기능은 소스에 스트리밍 구성 요소를 포함한 소스에 기초로 구축되어 있습니다. 이는 소스에 \"주제\"가 포함되어 있음으로 나타납니다. 이 경우 Chronon은 실시간 업데이트를 위해 배치 업로드와 함께 스트리밍 작업도 실행합니다. 배치 작업은 다음을 담당합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 값 시딩하기: 긴 윈도우의 경우, 스트림을 되감아서 모든 원시 이벤트를 재생하는 것은 실용적이지 않을 수 있습니다.\n- \"윈도우의 중간\"을 압축하고 꼬리 정확도 제공하기: 정확한 윈도우 정확도를 위해서는, 윈도우의 머리와 꼬리에서 원시 이벤트가 필요합니다.\n\n그러면 스트리밍 작업이 업데이트를 KV 스토어에 기록하여, 추출 시 특성 값을 최신 상태로 유지합니다.\n\n# 온라인 서빙 / 추출 API\n\nChronon은 낮은 대기 시간으로 특성을 추출하기 위한 API를 제공합니다. 우리는 개별 GroupBys(즉, 위에서 정의한 사용자 또는 구매 특성)에 대한 값이나 Join에 대한 값 중 하나를 가져올 수 있습니다. 다음은 Join에 대한 하나의 해당 요청과 응답의 예시입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n// 사용자=123에 대한 모든 기능을 검색 중\nMap\u003cString, String\u003e keyMap = new HashMap\u003c\u003e();\nkeyMap.put(\"user\", \"123\")\nFetcher.fetch_join(new Request(\"quickstart_training_set_v1\", keyMap));\n// 샘플 응답 (기능 이름과 값의 맵)\n'{\"purchase_price_avg_3d\":14.2341, \"purchase_price_avg_14d\":11.89352, ...}'\n```\n\n사용자 123에 대한 모든 기능을 가져오는 Java 코드입니다. 반환 형식은 기능 이름과 기능 값의 맵입니다.\n\n위 예제는 Java 클라이언트를 사용합니다. 쉬운 테스트와 디버깅을 위해 Scala 클라이언트와 Python CLI 도구도 있습니다:\n\n```js\nrun.py --mode=fetch -k '{\"user_id\":123}' -n quickstart/training_set -t join\n\n\u003e {\"purchase_price_avg_3d\":14.2341, \"purchase_price_avg_14d\":11.89352, ...}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nrun.py CLI 도구를 활용하여 상단의 Java 코드와 동일한 가져오기 요청을 수행할 수 있어요. run.py는 Chronon 워크플로우를 빠르게 테스트하는 편리한 방법이에요.\n\n또 다른 옵션은 이러한 API를 서비스로 래핑하여 REST 엔드포인트를 통해 요청하는 것이에요. 이 방법은 Airbnb 내에서 Ruby와 같은 Java 이외의 환경에서 기능을 가져오기 위해 사용돼요.\n\n# 온라인-오프라인 일관성\n\nChronon은 온라인-오프라인 정확도에 도움을 주는 것뿐만 아니라 이를 측정하는 방법도 제공해요. 측정 파이프라인은 온라인 가져오기 요청의 로그에서 시작돼요. 이 로그에는 요청의 기본 키와 타임스탬프가 포함되어 있습니다. 그리고 가져온 기능 값도 포함돼요. 그런 다음, Chronon은 키와 타임스탬프를 왼쪽 측으로 하는 Join backfill로 전달하고, 계산 엔진에게 기능 값을 backfill하도록 요청해요. 그런 다음 backfill된 값을 실제 가져온 값과 비교하여 일관성을 측정해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 다음은 무엇인가요?\n\n오픈 소스는 Stripe와 넓은 커뮤니티와 함께 나아갈 흥미로운 여정의 첫걸음일 뿐입니다.\n\n저희의 비전은 ML 실무자들이 자신들의 데이터를 활용하는 가장 최선의 결정을 내릴 수 있도록 돕고, 이 결정을 실행하는 과정을 가능한 쉽게 만들어 주는 플랫폼을 만드는 것입니다. 현재 우리의 로드맵을 구성하는 데 사용 중인 몇 가지 질문입니다:\n\n반복과 계산 비용을 얼마나 더 낮출 수 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n크로논은 이미 Airbnb 및 Stripe와 같은 대기업이 처리하는 데이터 규모에 적합하게 구축되어 있습니다. 그러나 우리의 컴퓨트 엔진을 최적화하여 컴퓨트 비용을 줄이고 새로운 기능을 만들고 실험하는 \"시간 비용\"을 줄일 수 있는 방법이 항상 있습니다.\n\n새로운 기능을 작성하는 것을 얼마나 더 쉽게 만들 수 있을까요?\n\n기능 엔지니어링은 인간이 도메인 지식을 표현하여 모델이 활용할 수 있는 신호를 생성하는 과정입니다. 크로논은 NLP를 통합하여 ML 전문가들이 이러한 기능 아이디어를 자연어로 표현하고 작동하는 기능 정의 코드를 생성할 수 있도록 하여 이터레이션을 시작할 수 있도록 지원할 수 있습니다.\n\n기술적 장벽을 낮추어 기능 생성을 보다 쉽게 하는 것은 결과적으로 ML 전문가들과 가치 있는 도메인 전문 지식을 보유한 파트너들 간의 새로운 협력을 열 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델 유지 관리 방식을 개선할 수 있을까요?\n\n사용자 행동 변경으로 모델 성능이 변화할 수 있습니다. 왜냐하면 모델이 훈련된 데이터가 현재 상황에 적용되지 않을 수 있기 때문입니다. 우리는 이러한 변화를 감지하고 이에 대응하는 전략을 조기에 예방적으로 만들어내는 플랫폼을 상상합니다. 이를 위해 모델을 재학습하거나 새로운 기능을 추가하거나 기존 기능을 수정하거나 그 이상의 조합으로 전략을 개발할 수 있습니다.\n\n플랫폼 자체가 ML 실무자가 최상의 모델을 구축하고 배포하는 데 도움이 되는 지능형 에이전트가 될 수 있을까요?\n\n플랫폼 레이어로 수집하는 메타데이터가 많아지면, 일반적인 ML 어시스턴트로서 플랫폼이 더욱 강력해질 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n새로운 데이터로 실험을 자동으로 실행하여 모델을 개선하는 방법을 식별하는 플랫폼을 만드는 목표를 언급했습니다. 이러한 플랫폼은 ML 전문가들이 \"이 사용 사례를 모델링할 때 가장 유용한 특성은 무엇인가요?\" 또는 \"이 목표에 관한 신호를 캡처하는 특성을 만들 수 있는 어떤 데이터 소스가 있을까요?\"와 같은 질문을 할 수 있도록하여 데이터 관리에도 도움이 될 수 있습니다. 이러한 종류의 질문에 답변할 수 있는 플랫폼은 지능적 자동화의 다음 수준을 나타냅니다.\n\n# 시작하기\n\n다음은 시작하는 데 도움이 되는 리소스 또는 팀에 적합한지 평가하는 데 도움이 되는 리소스입니다.\n\n- Github 프로젝트, Chronon 웹 사이트 및 특히 빠른 시작 가이드를 확인해보세요.\n- 커뮤니티 디스코드 채널에 들러보세요. 에어비앤비 및 Stripe 팀은 Chronon이 여러분의 스택에 어떻게 맞을지에 대해 여러분과 채팅하는 데 열정적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이런 종류의 작업에 관심이 있으세요? 여기에서 열려 있는 역할을 확인해보세요 — 채용 중입니다.\n\n# 감사의 말\n\n후원자: Henry Saputra Yi Li Jack Song\n\n기여자: Pengyu Hou Cristian Figueroa Haozhen Ding Sophie Wang Vamsee Yarlagadda Haichun Chen Donghan Zhang Hao Cen Yuli Han Evgenii Shapiro Atul Kale Patrick Yoon","ogImage":{"url":"/assets/img/2024-06-20-ChrononAirbnbsMLFeaturePlatformIsNowOpenSource_0.png"},"coverImage":"/assets/img/2024-06-20-ChrononAirbnbsMLFeaturePlatformIsNowOpenSource_0.png","tag":["Tech"],"readingTime":10}],"page":"54","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"54"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>