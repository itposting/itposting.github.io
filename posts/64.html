<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/64" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/64" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="시계열 회귀 및 교차 검증 깔끔한 접근 방식" href="/post/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="시계열 회귀 및 교차 검증 깔끔한 접근 방식" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="시계열 회귀 및 교차 검증 깔끔한 접근 방식" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">시계열 회귀 및 교차 검증 깔끔한 접근 방식</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="내가 내부로 2 흥행 예상 금액이 8500만 달러에 달할 이유 AI 예측으로 보는 안목" href="/post/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="내가 내부로 2 흥행 예상 금액이 8500만 달러에 달할 이유 AI 예측으로 보는 안목" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="내가 내부로 2 흥행 예상 금액이 8500만 달러에 달할 이유 AI 예측으로 보는 안목" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">내가 내부로 2 흥행 예상 금액이 8500만 달러에 달할 이유 AI 예측으로 보는 안목</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="기계 학습을 위한 재생 커널 힐버트 공간" href="/post/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="기계 학습을 위한 재생 커널 힐버트 공간" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="기계 학습을 위한 재생 커널 힐버트 공간" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">기계 학습을 위한 재생 커널 힐버트 공간</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="L1 및 L2 정규화를 분석적 및 확률적 관점에서 이해하기" href="/post/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="L1 및 L2 정규화를 분석적 및 확률적 관점에서 이해하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="L1 및 L2 정규화를 분석적 및 확률적 관점에서 이해하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">L1 및 L2 정규화를 분석적 및 확률적 관점에서 이해하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="영화 제목이 점점 길어지고 있는 걸까요 통계 분석 해보기" href="/post/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="영화 제목이 점점 길어지고 있는 걸까요 통계 분석 해보기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="영화 제목이 점점 길어지고 있는 걸까요 통계 분석 해보기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">영화 제목이 점점 길어지고 있는 걸까요 통계 분석 해보기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="이상치 탐지를 위한 3가지 간단한 통계적 방법" href="/post/2024-06-19-3SimpleStatisticalMethodsforOutlierDetection"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="이상치 탐지를 위한 3가지 간단한 통계적 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-3SimpleStatisticalMethodsforOutlierDetection_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="이상치 탐지를 위한 3가지 간단한 통계적 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">이상치 탐지를 위한 3가지 간단한 통계적 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="MLOps - PyTest를 사용한 데이터 검증" href="/post/2024-06-19-MLOpsDataValidationwithPyTest"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="MLOps - PyTest를 사용한 데이터 검증" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-MLOpsDataValidationwithPyTest_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="MLOps - PyTest를 사용한 데이터 검증" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">MLOps - PyTest를 사용한 데이터 검증</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="콜모고로프-아놀드 네트워크 혹평인가, 딥 러닝 혁명인가" href="/post/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="콜모고로프-아놀드 네트워크 혹평인가, 딥 러닝 혁명인가" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="콜모고로프-아놀드 네트워크 혹평인가, 딥 러닝 혁명인가" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">콜모고로프-아놀드 네트워크 혹평인가, 딥 러닝 혁명인가</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">16<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="OpenAI Assistant API와 Streamlit을 사용하여 도우미 만들기" href="/post/2024-06-19-CreatinganAssistantwithOpenAIAssistantAPIandStreamlit"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="OpenAI Assistant API와 Streamlit을 사용하여 도우미 만들기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-CreatinganAssistantwithOpenAIAssistantAPIandStreamlit_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="OpenAI Assistant API와 Streamlit을 사용하여 도우미 만들기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">OpenAI Assistant API와 Streamlit을 사용하여 도우미 만들기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="대규모 언어 모델Large Language Models, LLMs 이해하기" href="/post/2024-06-19-UnderstandingLargeLanguageModelsLLMs"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대규모 언어 모델Large Language Models, LLMs 이해하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-UnderstandingLargeLanguageModelsLLMs_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대규모 언어 모델Large Language Models, LLMs 이해하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">대규모 언어 모델Large Language Models, LLMs 이해하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/61">61</a><a class="link" href="/posts/62">62</a><a class="link" href="/posts/63">63</a><a class="link posts_-active__YVJEi" href="/posts/64">64</a><a class="link" href="/posts/65">65</a><a class="link" href="/posts/66">66</a><a class="link" href="/posts/67">67</a><a class="link" href="/posts/68">68</a><a class="link" href="/posts/69">69</a><a class="link" href="/posts/70">70</a><a class="link" href="/posts/71">71</a><a class="link" href="/posts/72">72</a><a class="link" href="/posts/73">73</a><a class="link" href="/posts/74">74</a><a class="link" href="/posts/75">75</a><a class="link" href="/posts/76">76</a><a class="link" href="/posts/77">77</a><a class="link" href="/posts/78">78</a><a class="link" href="/posts/79">79</a><a class="link" href="/posts/80">80</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"시계열 회귀 및 교차 검증 깔끔한 접근 방식","description":"","date":"2024-06-19 20:22","slug":"2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach","content":"\n\n![Image](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_0.png)\n\n시계열 예측 방법은 항상 발전하고 있습니다. ARIMA가 오랫동안 기초를 이루어 왔지만, 머신러닝 모델도 큰 약속을 보여줍니다. 다양한 산업 분야에서 자료를 시간에 따라 더 정확하게 모델링할 수 있는 경우가 있습니다. 이 글에서는 그 중 하나인 매출 예측을 다루어 보겠습니다. 소중한 시간을 아끼기 위해, 바로 본문으로 넘어가겠습니다.\n\n# 코드\n\n이 글에서 모든 것을 재현하는 코드는 제 GitHub 저장소에서 찾을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터 세트\n\n이 연습에서는 Kaggle에서 Samuel Cortinhas가 공개한 CC0: Public 도메인으로 제공되는 시계열 데이터 연습 데이터 세트를 사용합니다. 이 데이터 세트는 10년(2010년부터 2019년) 동안의 모의 시계열 데이터를 포함하며 날짜, 상점 ID, 제품 ID 및 매출 기능이 포함되어 있습니다. 이 분석에서는 회귀 구성 요소에 초점을 맞추기 위해 단일 상점과 제품을 선택했습니다.\n\n![](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_1.png)\n\n# 시계열 분석\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 0: 설정하기\n\n나는 데이터 탐색과 회귀를 위해 다음 패키지들을 사용할 것입니다. 로딩하기 전에 아래 명령어를 사용하여 설치할 수 있습니다: install.packages(\"package_name\").\n\n```js\n# 필요한 라이브러리 로딩하기\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tidymodels)\nlibrary(modeltime)\nlibrary(timetk)\nlibrary(viridis)\n``` \n\n## Step 1: 날짜 및 해당할 수 있는 모든 것들!!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n날짜는 제가 가장 좋아하는 변수입니다. 하나의 날짜 열은 많은 정보를 담고 있어요. 이 시나리오에서는 판매와의 관계를 탐색하기 위해 날짜 열에서 새로운 특징들을 만들 거에요. 하지만 먼저, 날짜 열을 문자열로만 사용하는 것보다는 as.Date()를 사용하여 정리할 거에요.\n\n```r\ndata \u003c- data %\u003e%\n  mutate(date = as.Date(date, format = \"%m/%d/%Y\"))\n```\n\n다음으로 회귀 분석을 위해 이 날짜 열에서 새로운 특징들을 만들 거에요. Lubridate 패키지는 이 작업을 간단하게 만들어 주는 편리한 함수들로 구성돼 있어요.\n\n```r\n# 시간과 관련된 요소를 포함하기 위해 데이터 전처리\ndf \u003c- data %\u003e%\n  mutate(\n    year = year(date),\n    semester = factor(semester(date)),\n    quarter = factor(quarter(date)),\n    day_in_week = factor(wday(date, label = TRUE)),\n    week_in_year = factor(week(date)),\n    day_in_year = factor(yday(date)),\n    month = factor(month(date, label = TRUE))\n  )\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 데이터가 다음과 같이 보입니다:\n\n![image](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_2.png)\n\n## 단계 2: 탐색적 데이터 분석\n\n단일 날짜 열에서 생성된 모든 이러한 새로운 흥미로운 기능들과 매출과의 관골을 탐색해 볼 것입니다. 연간 계절성부터 시작하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndf %\u003e%\n  ggplot(aes(date, sales)) +\n  geom_line(alpha = 1, size = 1, color = \"darkblue\") +  \n  theme_bw() +\n  labs(title = \"일별 매출 분포 변화\", x = \"날짜\", y = \"매출\") +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"2 years\") +  \n  scale_y_continuous(labels = scales::comma) \n```\n\n\u003cimg src=\"/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_3.png\" /\u003e\n\n매출 데이터에는 명확한 계절성과 특정한 추세가 있습니다. 이제 요일과의 관계를 살펴보겠습니다.\n\n```js\ndf %\u003e%\n  ggplot(aes(day_in_week, sales, color = day_in_week)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.1) +\n  theme_bw() +\n  scale_colour_viridis_d() +\n  labs(title = \"요일별 일일 매출 분포\", x = \"요일\", y = \"매출\") +\n  scale_x_discrete() +\n  scale_y_continuous(labels = scales::comma)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_4.png)\n\n이제 월별 분포를 살펴보겠습니다.\n\n```js\ndf |\u003e \n  ggplot(aes(month, sales)) +\n  geom_violin(color = \"darkgreen\") +  \n  geom_jitter(alpha = 0.2, aes(color = sales)) +  \n  theme_light() +\n  geom_smooth(method = \"loess\", se = FALSE) +  \n  scale_colour_viridis_c() +\n  labs(title = \"Daily Sales Distribution by Month\", x = \"\", y = \"Sales\", color= \"Sales\")\n```\n\n![Image](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_5.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 모델링으로 넘어가겠습니다.\n\n## 단계 3: 데이터 분할 및 교차 검증 설정\n\ntidymodels에서 제공하는 initial_time_split() 함수를 사용하여 데이터를 학습 및 테스트 세트로 나누겠습니다.\n\n```R\ndf_split \u003c- df |\u003e \n  initial_time_split(prop=0.9)  # 90% 데이터를 학습에, 10%를 테스트에 할당\n\ndf_train \u003c- training(df_split)\ndf_test \u003c- testing(df_split)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것이 분할의 모습입니다:\n\n\u003cimg src=\"/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_6.png\" /\u003e\n\n교차 검증은 입력 데이터의 하위 집합에서 모델을 학습시키고 나머지 데이터에서 평가하여 모델을 평가하는 데 사용할 수 있습니다. 일반적인 교차 검증 방법은 데이터 포인트를 무작위로 선택하여 학습 데이터 집합에 할당합니다. 그러나 이 방법은 데이터가 순차적이어야하기 때문에 시계열에 적합하지 않습니다. Timetk 패키지에는 시계열 데이터 세트에 대한 교차 검증 폴드를 특별히 생성하고 해당 분할을 시각화하는 데 사용할 수있는 멋진 함수인 time_series_cv()가 있습니다.\n\n```R\n# 교차 검증 분할 생성\ndf_folds \u003c- \n  time_series_cv(\n    df_train, \n    initial = \"3 years\", \n    assess = \"1 year\", \n    skip = \"6 months\",\n    slice_limit = 5)  \n\n# 분할 시각화\nplot_time_series_cv_plan(df_folds, date, sales)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_7.png\" /\u003e\n\n## 단계 4: 모델링을 위한 데이터 준비\n\n레시피는 새로운 예측 변수를 만들고 모델에서 필요한 몇 가지 전처리를 수행할 수 있는 객체입니다. 저는 auto ARIMA 및 랜덤 포레스트 두 모델을 시도하고 싶기 때문에 두 레시피 객체를 만들 것입니다. 레시피()의 첫 번째 단계는 회귀 분석을 위한 공식입니다.\n\n```js\nrecipe_autoarima \u003c- \n  recipe(sales ~ date,\n         data = df_train)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 번째 랜덤 포레스트 레시피에는 step_holiday()를 사용하여 date 데이터를 공휴일에 대한 하나 이상의 이진 지표 변수로 변환하는 기능을 추가할 것입니다. 또한 step_rm()을 사용하여 date 열을 제거하고 step_dummy()를 사용하여 모든 명목 예측 변수를 더미 변수로 변환할 것입니다.\n\n```js\nrecipe_rf \u003c- \n  recipe(sales ~ ., data = df_train) |\u003e\n  step_holiday(date, holidays = timeDate::listHolidays(\"US\")) |\u003e  \n  step_rm(date) |\u003e  \n  step_dummy(all_nominal_predictors())\n```\n\n다음은 recipe 객체가 보이는 모습입니다:\n\n![Recipe Object](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_8.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n레시피 객체는 학습 및 테스트 데이터셋에서 반복 가능한 단계 시퀀스를 만들어주어 긴 피처 엔지니어링 코드를 작성하지 않고도 사용할 수 있습니다. 저는 이를 피처 엔지니어링의 단축키로 생각해요!\n\n아래 명령은 레시피 객체가 데이터셋을 업데이트하고 새로 생성된 열을 이해하는 데 사용될 수 있습니다.\n\n```js\nrecipe_rf |\u003e prep() |\u003e bake(new_data = NULL)\n```\n\n## 단계 5: 모델 사양 및 재표본화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n레시피를 만들면, 각각의 두 모델에 대한 명세서를 작성할 것입니다. ARIMA에 Modeltime을 사용하고 랜덤 포레스트에는 Tidymodels를 사용할 예정이에요. 이것들은 일관된 단계적 흐름으로 여러 모델에 대한 명세를 만드는 훌륭한 방법을 제공합니다.\n\n```js\n# Auto ARIMA 모델 명세\nauto_arima_spec \u003c- arima_boost() |\u003e \n  set_mode(\"regression\") |\u003e \n  set_engine('auto_arima_xgboost')\n\n# Random Forest 모델 명세\nrf_spec \u003c- \n  rand_forest(trees = 500) |\u003e  \n  set_mode(\"regression\") |\u003e \n  set_engine(\"ranger\")\n```\n\n## 단계 6: Workflow 세트\n\nworkflow()는 전처리, 모델링, 후처리 요청을 함께 묶을 수 있는 객체입니다. 여러 모델과 리샘플링을 사용할 때, workflow 세트를 사용하는 것이 더 편리하다고 발견했어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 recipe 객체와 해당하는 모델 사양을 workflow_set()을 사용하여 결합할 것입니다. 기본적으로 cross 파라미터는 TRUE로 설정되어 있어서 각 recipe 객체가 각 모델 사양과 일치하도록 합니다. 이 경우에는 각 recipe 객체가 해당하는 모델 사양과 일치하도록 FALSE로 설정할 것입니다.\n\n```js\nworkflowset_df \u003c- \n  workflow_set(\n    list(recipe_autoarima, recipe_rf),\n    list(auto_arima_spec, rf_spec),\n    cross=FALSE\n  )\n```\n\n이것이 workflow 객체입니다. 현재 결과가 없지만, 교차 검증이 완료되면 결과를 저장하기 위한 자리 표시자가 있습니다.\n\n![그림](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_9.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 7: 모델 학습 시간입니다\n\n지금까지 새로운 특성을 만들고 데이터를 전처리하고 모델 사양을 만들고 워크플로 세트를 구축했습니다. 이제 훈련 세트 및 리샘플링 폴드를 사용하여 모델을 적합시키겠습니다. 결과를 나중에 분석할 df_results에 결과를 저장할 것입니다.\n\n```js\ndf_results \u003c-\n  workflow_map(\n    workflowset_df,\n    \"fit_resamples\",\n    resamples = df_folds\n  )\n```\n\n이렇게 하면 workflowset_df의 모든 워크플로에 대해 루프를 반복하고 각각에 fit_resamples 함수를 적용하여 df_folds 교차 검증 객체를 사용합니다. 각 실행의 결과는 df_results의 해당 모델 행 아래에 저장됩니다. 결과 열이 이제 교차 검증 결과로 채워졌음을 유의하십시오. 이러한 결과는 필요한 경우 unnest()를 사용하여 추출할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_10.png)\n\n결과는 tidymodels의 정말 멋진 기능인 autoplot() 명령을 사용하여 신속하게 시각화할 수도 있습니다.\n\n```js\nautoplot(df_results)\n```\n\n![image](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_11.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 리샘플에서 랜덤 포레스트가 ARIMA와 비교했을 때 더 낮은 RMSE(평균 제곱근 오차)와 R-제곱을 보여주고 있습니다.\n\n## 단계 8: 테스트 데이터 적합\n\n이제 df_results의 결과를 순위 매겨 가장 낮은 RMSE를 기준으로 가장 성능이 좋은 모델을 식별하겠습니다. 해당 모델의 ID 및 매개변수를 검색하고 이 최적화된 모델을 훈련 데이터에 맞출 것입니다. 그런 다음, 이 모델을 사용하여 테스트 데이터셋에서 판단하고 새로운 지표를 확인하겠습니다.\n\n```r\n# 최고의 rmse를 가진 workflow의 ID 가져오기\nbest_workflow_id \u003c- df_results %\u003e%\n  rank_results(rank_metric = \"rmse\") %\u003e%\n  head(1) %\u003e%\n  pull(wflow_id)  \n\n## best_workflow_id와 관련된 매개변수 가져오기\nbest_params \u003c- df_results %\u003e%\n  extract_workflow_set_result(id = best_workflow_id) %\u003e%\n  select_best(metric = \"rmse\")  \n\n## best_workflow_id와 관련된 workflow 가져오기\nbest_workflow \u003c- df_results %\u003e%\n  extract_workflow(id = best_workflow_id)  \n\n# 최적화된 매개변수로 workflow 완성\nfinalized_workflow \u003c- finalize_workflow(best_workflow, best_params) \n\nfinalized_workflow\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 최종 워크플로우의 모습입니다:\n\n![image](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_12.png)\n\n이제 이 워크플로우를 사용하여 fit() 및 augment()를 사용하여 테스트 데이터 세트에서 예측하겠습니다. 이 예측을 기반으로 지표를 측정할 것입니다.\n\n```R\npredictions \u003c- finalized_workflow %\u003e%\n  fit(df_train) %\u003e%\n  augment(df_test)  \n\n## 평가 지표 계산\nevaluation_metrics \u003c- metric_set(rmse, mae, rsq)\nresults \u003c- evaluation_metrics(predictions, truth = sales, estimate = .pred) \nprint(results)  \n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 최종 지표가 표시되는 모습입니다:\n\n![Final Metrics](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_13.png)\n\n정말 잘 했어요!\n\n# 성공했어요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사를 통해 tidymodels, modeltime 및 timetk가 시계열 회귀 모델을 구축하는 강력한 프레임워크를 제공하는 방법에 대해 명확해졌으면 좋겠고, 여러분이 한 번 시도해볼 것으로 바랍니다! 이 기사는 한 가게와 제품 ID를 위해 만들어 졌지만, 이것은 어떤 가게와 제품 조합에 대해 이를 복제할 수 있는 Shiny 웹 애플리케이션을 구축하기 위한 좋은 사례가 될 수 있습니다. 즐거운 코딩하세요!\n\n# 코드\n\n이 기사의 모든 내용을 다시 만들기 위한 코드는 제 GitHub 저장소에서 찾을 수 있습니다.\n\n마음껏 찾아주세요. LinkedIn에서 저를 찾아보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n글에서 언급되지 않는 한, 모든 이미지는 저자가 찍은 것입니다.","ogImage":{"url":"/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_0.png"},"coverImage":"/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_0.png","tag":["Tech"],"readingTime":10},{"title":"내가 내부로 2 흥행 예상 금액이 8500만 달러에 달할 이유 AI 예측으로 보는 안목","description":"","date":"2024-06-19 20:19","slug":"2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI","content":"\n\n## 인공지능, 영화 및 예측 AI\n\n저는 AI가 미래에 대해 우리에게 말해 줄 수 있는지에 광신 중이었습니다. 아니죠, Mystic Meg와 그녀의 싸이킥 프렌즈 네트워크가 인공지능 덕분에 직장을 잃게 될 거라는 건 아니에요 (비록 그들도 그런 일이 오리라고는 예상 못했을 거에요)! 이건 초능력이나 마술 같은 건 아니고, 심지어 X-파일 의사과학도 아닌 거에요; 단지 대형 언어 모델을 사용해 훈련 데이터로부터의 기대와 패턴을 기반으로 다음에 무엇이 올지 예측하고, 우리의 집합적 의견이 무엇일지 예측하는 거예요.\n\n## 인공지능의 수비 크리스탈 볼 뒤에 있는 과학\n\n이를 뒷받침하는 베일러 대학의 실질적인 연구가 있어요 (아래서 다룰 거에요). 작년에 배팅 회사에서 GPT를 디자인해 파워 백을 제공하는 데 도움을 줄 것을 요청받았는데, 제 개인 윤리적 이유로 거절했지만, 이게 AI가 산업에서 활용되기 시작하는 방식이며, 재무 및 보험 서비스를 포함하며 AI가 주식 시장 추세를 예측하고 보험 가입 정책에 이벤트 발생 가능성을 평가할 수 있어요. AI는 법률 분야에서 법적 사건 결과를 예측하는 데 사용될 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 이야기꾼으로서, 가장 흥분되는 분야는 엔터테인먼트 산업입니다. LLMs는 시대정신에 입각합니다. AI가 관객들과 연결될 운명인 이야기를 알려줄 수 있다고 믿습니다. 사실, 그것에 돈을 걸 준비가 되어 있습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*ok4AW4Fpjz7zpIjV665AOQ.gif)\n\n## 직감 이상: 왜 인사이드 아웃 2가 박스 오피스에서 내가 거는 걸까요\n\n만약 AI가 옳다면, 인사이드 아웃 2는 2024년 최초로 개봉 주말에 8500만 달러를 벌어들일 영화가 될 것이고, 저는 입을 헐겁니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여름 박스 오피스가 힘든 시작을 했지만, ChatGPT는 Inside Out 2가 오프닝 주말에 뒤이어 개봉하는 Dune Part Two나 Godzilla X Kong: The New Empire 같은 주요 2024년 영화들을 누르고 $85백만을 달성할 것이라고 말했습니다.\n\n![이미지](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_0.png)\n\n이 성과는 Pixar에게 매우 필요한 승리가 될 것이며, 특히 이전 영화인 Elemental과 Lightyear의 성과가 좋지 않은 점을 고려할 때 더욱 중요할 것입니다.\n\n사실, 이토록 자신한다면, 만약 AI가 틀리다면 이 기사의 첫 주 수입 전액을 베팅하고, 그 금액을 자선 단체에 기부하겠습니다. 그것은 나름대로 큰 제안이죠: Medium과 독자들 덕분에 내가 인기 있는 이야기로 매달 최대 $850를 벌 수 있습니다. 그러니 이 기사를 계속 주목하고 공유해 주시고, 제가 책임을 질 수 있도록 도와주세요. 아이들의 정신적 안녕을 위한 자선 단체를 제안해 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# LLMs의 예측 능력 뒤에 숨은 과학\n\n하지만 내가 내일 Inside Out 2의 성공을 확신하는 이유는 뭘까요? 경제라는 것과 AI가 생성한 텍스트의 지루함 때문입니다. 사실, 일상적인 것이라고 생각되는 것은 대중의 의견, 감정 및 태도에 의해 상당히 영향을 받는 사건들을 평가할 때 자산이 되기도 합니다.\n\nAI 텍스트의 진부한 쓰기는 AI의 가치를 제공하는 중요한 도구로 사용되고 있다. 경제 교수들은 예측 가능성을 발견했으며 이 때문에 AI가 이미 확립된 패턴에 기반한 신뢰할 수 있고 일관된 예측을 제공할 수 있는 가치 있는 도구가 되고 있다는 것입니다. 이러한 이유로 AI는 가치 있는 도구로 사용됩니다. 이렇게요:\n\n## 미래 전망에서 제공됨\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n베일러 대학교의 반 팜 교수와 스콧 커닝햄 교수가 일반 경제학에 게재된 논문에서 미래 이벤트를 예측하는 흥미로운 방법을 발견했다고 발표했어요! ChatGPT가 '사실'을 만들어내면서 이용한 것인데요. 이들은 \"미래 서사\"란 개념을 사용했어요: AI에게 \"미래에서 일어난 사건을 공유하는 캐릭터들이 나오는 이야기\"를 출력하도록 요청했대요. 이 이야기들은 ChatGPT의 훈련 데이터 수집 이후에 일어난 사건들을 담고 있었어요.\n\n## 베일러 대학교 연구 결과로 알아본 예측 AI의 통찰\n\n그들은 가상의 미래 시점을 사용함으로써 ChatGPT의 제약사항을 우회하고 (팩트를 추정하는 것에 망설임을 보이는 특성) 놀랄만큼 정확한 2022년 아카데미 수상자를 예측하는 데 성공했어요. 결과를 즉시 확인하기 위해 실제 미래 이벤트를 예측하는 데 사용하지는 않았어요 (2022년 아카데미 시상식은 AI의 훈련 데이터 범위 밖이었지만 그 당시 연구자들에게 발생했습니다). 그러나 트럼프 허시 돈 판결 이전, 이들의 실험 결과를 반복해보았어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n심사단이 판단을 내릴 때까지 ChatGPT가 훈련 데이터 외에서 새로운 정보에 접근하는지를 Bing 브라우저를 통해 확인하기 위해 실험을 실행했습니다. Pham과 Cunningham의 동의 없이입니다. 트럼프 판결은 ChatGPT와 실험자 모두 알지 못할 이벤트였습니다. 그럼에도 불구하고 그것이 정확하다고 판단했습니다 (100회 중 100회의 채팅에서 트럼프에 대해 유죄 판결을 내렸습니다).\n\n## 한 단어씩 미래 예측하기\n\n이 방법은 ChatGPT의 텍스트를 절차적으로 생성하는 기본 메커니즘을 활용하며, 즉 단어에서 단어로 전개(predictively from word to word)합니다. API에서 Temperature와 Top-P 값과 같은 매개변수를 사용하여 통찰력 있는 기능을 어떻게 세밀하게 조정할 수 있는지에 대해 궁금해합니다. 예를 들어, AI에게 \"다음으로 가능성이 높은 단어\"로 간주하도록 지시하는 경우 (Temperature), 또는 AI가 선택할 수 있는 토큰 풀을 증가시켜 최소한의 토큰 집합에 대한 누적 확률에 따라 선택할 수 있도록 하는 것(Top-P). 그러나 이는 향후 연구를 위한 분야입니다 (이에 관심 있는 학자들 협업 가능할까요?).\n\n# 미래 예측 \"미래 서술\" 프롬프트란 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기본적으로, 특정 문장이 어떻게 끝날 것인지를 ChatGPT에 예측하도록 하는 대신, 미래적인 시각에서 해당 문장을 완성하도록 합니다. \"이미 다음 주인 것처럼 했을 때, 임의의 주제에 대해 무엇을 쓰게 될 가능성이 높을까요?\" \n\n![이미지](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_2.png)\n\n만약 AI가 문장을 완성시킬 수 있다면, 과정 생성의 기능으로 우리가 한 결론에 도달하는 것은 그리 어려운 일이 아닐 것입니다. \n\nPham과 Cunningham이 고려하지 않았을지도 모를 다른 요소가 있는데, 그것은 LLMs가 언어와 감정이 주요 요소인 예측 분석에서 더 나은 성과를 낼 수 있다는 점입니다. 사람들의 행동이 단어에 영향을 받는 상황에 LLMs가 최적이지 않을 수도 있지만, 소행성이 언제 충돌할지 예측하는 데보다는 알맞을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 정치 캠페인과 선거 결과\n- 시장 동향과 소비자 행동\n- 법적 사안 결과\n- 홍보 및 미디어 영향\n- 영화 박스 오피스 성적\n\n## 엔터테인먼트에서 예측적 AI 분석의 광범위한 함의\n\n그래서 '인사이드 아웃 2'의 성공을 예측할 때, 우리는 데이터에만 의존하는 것이 아니라 집단 문화 의식에도 접근하고 있습니다. 이러한 함의는 거대합니다 — 우리는 AI가 콘텐츠의 창작에 영향을 끼칠 것으로 생각했지만, AI가 청중들이 접근할 콘텐츠를 미리 예견할 수 있다면, 영화와 TV 프로그램에 대한 우리의 반응은 어떨까요? 제가 이전에 상상한 것은 창의적 분야에서 AI의 최적 활용이 개인화였습니다; 큰 이야기를 작고 소소하게 만드는 것입니다. 그러나 아마도 보다 보편적으로 공감할 수 있는 콘텐츠를 만드는 데 있을지도 모릅니다?\n\n# 비슷한 마음: 집단 의식과 LLMs\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChatGPT은 우리의 커뮤니케이션의 공통점이자 \"하이브 마인드\"라고 할 수 있어요. Pham과 Cunningham이 발견한 예측 분석 능력은 '군중의 지혜'라고 알려진 원리에 근거할 수 있어요. 이 원리는 더 큰 그룹의 사람들이 가장 정확한 해결책으로 수렴하는 경향이 있다는 것입니다. 우리는 다양하고 분산된 그룹의 집합된 교육적 추측이 개인의 정확도를 능가할 수 있다는 것을 알고 있어요.\n\n![이미지](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_3.png)  \n\n여기서 LLM은 집단 지성의 수단으로 작용하며, 방대한 훈련 데이터로부터 집단의 감정을 분별하고 어떤 것이 문화적·사회적 대화의 보다 넓은 서술 내에 어떻게 들어맞을지를 파악할 수 있어요. 기본적으로 AI는 텍스트로 표현된 대중 의견의 공통점을 예측해요. AI는 대중 의식을 열린 책처럼 읽을 수 있어요. 그것은 훈련을 받았거든요.\n\n![이미지](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_4.png)  \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리의 의견 및 어느 정도 우리의 행동은 실현 가능합니다. 우리는 우리의 말과 예술, 의사 소통으로 계산할 수 있습니다. 언어는 AI 기반 시장에서 가장 가치 있는 상품입니다.\n\n![Image](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_5.png)\n\n## AI 예측에서 군중의 지혜\n\nAI는 우리 일반적인 시각, 보편적인 의견 및 공통 이해를 울리는 반향합니다. 그것이 바로 조화론이 의미하는 것입니다: LLMs가 사회의 가치와 일치하는지 확인하는 것입니다. AI는 일반적인 답변을 식별하여 우리의 필요성을 예상하고, 가장 넓은 사용자 스펙트럼을 고려하는 솔루션을 제공합니다. AI의 가장 큰 약점인 예측 가능하고 일반적이며 모두와 아무도와 같은 소리를 내지만, 합의를 예측할 때 가장 큰 강점이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_6.png)\n\n## 공생 관계: AI와 인사이드 아웃 2\n\n사실, 인사이드 아웃 2를 예측하는 데에 특히 호흡이 맞는 이유 중 하나가 있습니다. 첫 번째 영화를 본 사람이라면 집단 합의와 마음이 어떻게 작용하는지에 대한 상징적 수준으로 이해할 수 있을 것입니다. 마치 교육 데이터(기억)로 이루어진 거대한 컴퓨터 콘솔처럼 보이죠. 이는 ChatGPT와 같은 AI가 언어와 감정을 처리하고 이해하여 거의 인간 같은 방식으로 예측하고 응답하는 과정과 대칭성을 갖고 있습니다.\n\n## 자선의 내기: AI 예측을 시험해보기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내 기사의 수익을 AI의 예측에 따라 걸어놓음으로써, 나는 이 기술의 실용적 응용에 대한 내 자신감을 증명하고 있어. 독자들과 함께 재미있고 영향력 있는 방법이 되며, 좋은 일을 지원하는 것이다.\n\n더 높은 수익을 기대해 읽고 공유해주세요! (팁 하나, 특정 텍스트를 강조하여 X를 선택하여 트윗할 수 있어요).\n\n만약 내가 틀리더라도, 적절한 어린이 자선단체가 내 내기로부터 이익을 받을 것이야. 이 기사의 수익이 Inside Out 2가 열릴 때 300달러를 초과할 경우, 결과와 관계없이 기부할 거라고. 결과를 보려면 팔로우하기를 잊지 말아주세요!\n\n🌟 당신의 지원은 매우 감사히 받아들이고, 이 기사들이 계속되도록 도와줄 거예요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n👍 만일 이 글이 유익했다면 박수로 응원을 표현해 주세요\n\n💡 Medium에서 한 번에 긴 누른 버튼으로 최대 50번 박수를 줄 수 있어요!\n\n🤝 이 글을 당신의 소셜 미디어나 LinkedIn에 공유해 주시면 감사하겠습니다\n\n🐦 텍스트를 강조하고 X를 선택하여 당신이 좋아하는 부분을 트윗할 수 있어요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n💬 기부할 어린이 자선 단체가 있나요? 아래 댓글에서 추천해 주세요 — 여러분의 추천을 듣고 싶어요!\n\n# Jim the AI Whisperer는 누구일까?\n\nAI를 해소하고 모든 사람이 접근할 수 있도록 하는 것이 제 목표입니다. AI의 잠재력에 열정적이며 여러분과 제 발견을 공유하는 것을 즐깁니다.\n\n## 함께 소통해요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n개인 코칭에 관심이 있거나 제 서비스를 이용하고 싶다면 언제든지 저에게 연락해주세요. 팟캐스트, 인터뷰 및 언론 활동에도 참여할 수 있습니다. 그리고 제 작품을 지원하고 싶다면 'Buy Me a Coffee' 페이지도 확인해보세요.\n\n![image](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_7.png)\n\n## 업데이트 및 참여 유지하기\n\n저, 인공지능 전문가 Jim의 새로운 소식을 놓치지 않고 받고 싶다면 구독해주세요. 인공지능 분야의 뜨거운 소식을 놓치지 않도록 최신 정보를 제공할 것을 약속드립니다. 함께 보다 나은 세상을 만들어나갑시다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Jim the AI Whisperer가 쓴 다음 Medium 기사를 즐기실 수도 있습니다:\n\n![Medium Article](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_8.png)\n\n이 이야기는 Generative AI에서 발행되었습니다. LinkedIn에서 저희와 연락하고 최신 AI 이야기를 계속 업데이트 받으려면 Zeniteq를 팔로우하세요.\n\n최신 뉴스 및 생성적 AI에 관한 업데이트를 받으려면 뉴스레터를 구독하세요. 함께 AI의 미래를 함께 만들어봐요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_9.png\" /\u003e\n","ogImage":{"url":"/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_0.png"},"coverImage":"/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_0.png","tag":["Tech"],"readingTime":8},{"title":"기계 학습을 위한 재생 커널 힐버트 공간","description":"","date":"2024-06-19 20:17","slug":"2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning","content":"\n\n# 커널\n\n머신 러닝(ML)에서 커널 기계는 SVM과 같은 선형 분류기를 사용하여 비선형 문제를 해결합니다. 이를 위해 커널 함수를 활용하여 데이터를 더 높은 차원의 공간으로 암시적으로 매핑하여 선형으로 분리할 수 있습니다.\n\nX를 비어 있지 않은 집합이라고 하고, 커널 k가 다음과 같이 정의됩니다:\n\n![커널 함수](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 정의에 따르면, 커널 함수 k: X x X → ℝ은 데이터 포인트 x와 x' 사이의 유사성을 측정합니다. k가 유효한 커널이 되려면, k(x, x') = ⟨φ(x), φ(x')⟩ 조건을 만족해야 합니다. 여기서 φ는 입력 공간 X에서 특징 공간 H로의 매핑이며, ⟨・ ,・⟩는 힐베르트 공간 H에서의 내적을 나타냅니다. 이는 커널 값이 특징 공간에서 데이터 포인트의 표현의 내적과 동일함을 의미합니다.\n\n하지만, 다른 많은 머신 러닝 알고리즘과 달리 입력 데이터를 특징 벡터로 명시적으로 변환해야 하는 기능 맵을 사용하지 않고, 커널 방법은 사용자가 지정한 커널 함수를 통해 직접 유사성을 계산합니다. 이 커널 함수는 잠재적으로 무한 차원의 고차원 특징 공간에서 작동하더라도, 커널 트릭을 통해 특징 표현을 명시적으로 계산하는 것을 피할 수 있습니다. 이 기술은 이러한 계산에 수반되는 상당한 계산 비용을 피할 수 있도록 도와줍니다. 이러한 암묵적 작업을 통해 커널 방법은 복잡한 데이터 관계를 효율적으로 분석하고, 효과적인 패턴 인식 및 분석을 용이하게 합니다.\n\n## 함수 공간\n\n함수 분석에서, f (・)는 함수 자체를 나타내며, f(x)는 입력 x에서 함수가 취하는 특정 값을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_1.png\" /\u003e\n\n입력 x를 사용하는 함수 f를 고려해 봅시다. 여기서 x는 (x₁, x₂)로 정의된 벡터입니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_2.png\" /\u003e\n\n함수 f(・)는 ℝ²를 ℝ에 매핑하는 함수 공간의 원소입니다. 이 예제에서는 이 함수 공간에서 f(・)를 ℝ³로 표현할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_3.png)\n\nGiven the function\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_4.png)\n\nThe linear functional f can be represented as:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_5.png\" /\u003e\n\n보통 임의의 함수는 피처 𝜙(𝑥)의 선형 조합으로 나타낼 수 있습니다. x에서 f를 평가하는 𝑓(𝑥)는 피처 공간에서의 내적입니다:\n\n\u003cimg src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_6.png\" /\u003e\n\n예를 들어, x = (-1, 4)에서 평가된 f는\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_7.png)\n\n## 무한 차원의 특성 공간\n\n이 개념은 자연스럽게 무한 차원의 특성 공간으로 확장됩니다. 예를 들어, 우리는 지수 함수 eˣ를 이용하여 그의 테일러 급수 표현을 확장할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_8.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 e³가 되면\n\n![Image 1](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_9.png)\n\n여기서 피처 공간은 무한한 차원에 있습니다.\n\n![Image 2](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 평가 기능\n\n평가 기능은 함수를 입력으로 받아 스칼라(하나의 숫자)를 출력으로 생성하는 특별한 유형의 함수입니다. 좀 더 간단히 말하면, 일반적인 함수가 숫자를 숫자로(또는 벡터를 벡터로) 매핑하는 반면, 함수적으로는 함수를 숫자로 매핑합니다.\n\nX의 x에서의 평가 기능 L은 다음과 같이 정의됩니다.\n\n\n\u003cimg src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_11.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 정의는 𝐿이 x에 의해 매개변수화된 함수로, 입력으로 함수 f를 사용하고 실수 ℝ을 생성하는 기능입니다.\n\n선형 함수는 아래와 같이 벡터 덧셈 및 스칼라 곱셈 연산을 보존하는 기능입니다:\n\n![이미지](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_12.png)\n\n# 이중 공간\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n벡터 공간 V의 쌍대 공간은 V 상의 모든 선형 함수인 집합입니다.\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_13.png)\n\n친숙한 3차원 공간을 고려해보겠어요. ℝ³로 표시되며, 각 점은 세 가지 구성 요소 (x, y, z)를 가진 벡터로 나타낼 수 있어요. 이것이 바로 우리의 원래 벡터 공간 V에요. ℝ³에서의 선형 함수는 3차원 벡터를 입력으로 받아 하나의 실수를 출력하는 선형 매핑입니다. 예를 들어, 선형 함수는 다음과 같이 정의될 수 있어요: f(x, y, z) = 2x + 3y + 4z. ℝ³의 쌍대 공간인 V*는 ℝ³에서의 모든 가능한 선형 함수의 집합이에요. 이 쌍대 공간의 각 함수는 3차원 벡터로 고유하게 표현될 수 있어요. 예를 들어, 함수 f(x, y, z) = 2x + 3y + 4z는 벡터 (2, 3, 4)로 나타낼 수 있어요.\n\n# Riesz Representation Theorem\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n리즈 표현 정리는 힐버트 공간에서의 모든 연속 선형 함수 L이 F의 고정된 요소와의 내적으로 표현될 수 있다는 것을 명시합니다. 공식적으로, 힐버트 공간 F에서 어떤 연속 선형 함수 𝐿에 대해,\n\n\n![리즈 표현 정리](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_14.png)\n\n\n리즈 표현 정리는 선형 함수의 추상적 세계를 보다 익숙한 내적 개념으로 이어주어, 선형 함수를 내적의 기하학적 직관을 사용해 이해하고 조작할 수 있게 합니다. 본질적으로, 리즈 표현 정리는 힐버트 공간 H의 모든 연속 선형 함수에 대해, 그 함수를 완전히 나타내는 동일한 H 내의 고유 요소가 존재함을 명시합니다. 이는 선형 함수를 고정된 \"대표\" 벡터와의 내적으로 생각할 수 있다는 것을 의미합니다. 이는 힐버트 공간에서 선형 함수와 내적 사이의 근본적인 연결을 확립합니다. 복제 커널 힐버트 공간(RKHS)의 맥락에서, 이 개념은 함수의 평가가 커널 함수와의 내적을 사용하여 계산될 수 있음을 보여주기 위해 확장됩니다.\n\n# 복제 커널\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n평가 기능과 Riesz 표현 정리를 결합하면 다음을 얻을 수 있습니다:\n\n![식 15](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_15.png)\n\n어떤 함수 f ∈ F가 x에서 평가되면 f를 F 내의 고유 기능 kₓ와의 내적으로 표현할 수 있습니다. 여기서 kₓ는 다음과 같이 쓸 수 있습니다:\n\n![식 16](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_16.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n테이블 태그를 마크다운 형식으로 변경해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 결국 커널의 개념으로 돌아가게 됩니다.\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_19.png)\n\n요약하면, 평가 기능 Lₓ는 k( ⋅ , x )로 표현할 수 있습니다.\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_20.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내부 곱으로 이를 계산할 수 있습니다. 이 함수는 Hilbert 공간 𝐻의 커널 함수와의 내적으로 계산됩니다. 함수 𝑘(⋅, 𝑥) 또는 kₓ는 재생 커널로 불립니다.\n\n그러므로,\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_21.png)\n\n여기에 사용된 표기법은 모두 동일한 객체를 가리킵니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_22.png)\n\n# 복제 커널 힐버트 공간 (RKHS)\n\n복제 커널 힐버트 공간(RKHS)은 각 지점에서의 평가가 연속 선형 기능적인 함수들의 힐버트 공간입니다. 이는 공간 안의 임의의 함수 f와 임의의 점 𝑥에 대해 𝑓(𝑥) = ⟨𝑓,𝑘(⋅, 𝑥)⟩가 성립하는 커널 함수 k가 존재한다는 의미입니다. 커널 함수 𝑘(⋅,𝑥)는 복제 커널로 알려져 있으며, 함수와 특징 공간 간의 다리 역할을 하여 함수를 특정 지점에서 평가할 수 있게 합니다.\n\n본질적으로, f(x)는 f와 복제 커널 함수 k(⋅, x)의 내적으로 나타낼 수 있습니다. 함수 k(⋅, x)는 복제 커널이라고 불립니다. 개념적으로, 평가 함수는 f와 x의 특징 공간 표현의 내적으로 계산할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 예시\n\n푸리에 변환은 다음과 같은 형태를 가지고 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_23.png\" /\u003e\n\n놀랍게도, 우리가 커널의 재현 특성을 논의할 때에도 똑같은 형태를 가지고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Reproducing Kernel Hilbert Space for Machine Learning](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_24.png)\n\nFunctional analysis has traditionally been utilized to examine the characteristics of transformational functions like the Fourier transform. To demonstrate this, let's take a look at an aperiodic pulse function.\n\n![Aperiodic Pulse Function](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_25.png)\n\nThe Fourier transform of the aperiodic pulse function 𝑓(𝑥) is:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_26.png)\n\n예를 들어,\n\n![이미지](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_27.png)\n\n아래 예시에서 우리는 함수(1, cos(𝑥), cos(2𝑥), cos(3𝑥), ...)를 기저로 사용하여 계단 함수를 나타냅니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 코싸인 함수로 특징 공간에 나타낸 특징들을 보여줍니다. 이것이 우리가 왜 함수를 사용하여 특징 공간을 모델링할 수 있는지 효과적으로 보여줍니다.\n\n# 무한 차원 RKHS\n\n두 번째 예제에서는 가우시안 함수로 구성된 특징 공간을 설명하겠습니다. 아래 f(x)를 고려해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_29.png\" /\u003e\n\n'𝑓(𝑥)'가 어떻게 커널 'k'를 복제하는지 알아봅시다.\n\n\u003cimg src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_30.png\" /\u003e\n\n이를 위해 '𝑓(𝑥)'를 특정 포인트의 커널들의 선형 결합으로 표현합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_31.png\" /\u003e\n\n이 예시에서는 가우시안 커널을 사용할 것입니다. 이 커널은 다음과 같이 정의됩니다:\n\n\u003cimg src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_32.png\" /\u003e\n\n우리는 𝑓(𝑥)를 k(・, xᵢ)의 선형 조합으로 표현할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_33.png)\n\n이 과정에서는 가우시안 커널을 직접적으로 다룹니다. 아래의 함수 f는 다음으로 구성됩니다:\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_34.png)\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_35.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_36](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_36.png)","ogImage":{"url":"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_0.png"},"coverImage":"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_0.png","tag":["Tech"],"readingTime":8},{"title":"L1 및 L2 정규화를 분석적 및 확률적 관점에서 이해하기","description":"","date":"2024-06-19 20:13","slug":"2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews","content":"\n\n## 기계 학습과 수학\n\n![이미지](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_0.png)\n\n기계 학습을 공부할 때 L1 및 L2 정규화를 알게 될 것입니다. 많은 멋진 블로그들이 시각화를 통해 이러한 개념을 직관적으로 설명합니다. 그러나 L1 및 L2 정규화를 해석 및 확률적 관점에서 자세히 설명하는 블로그는 거의 없습니다. 그래서 나는 두 관점 모두로 이 두 가지 정규화에 대해 쓰기로 결심했습니다. 이 블로그에서는 L1 및 L2 정규화를 상세한 수학적 유도 및 시각화와 함께 소개하여 이러한 개념을 잘 이해할 수 있도록 도와드리겠습니다.\n\n## 목차\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 정규화 개요\n  - L1 정규화\n    - 2.1 L1 정규화의 분석적 유도\n    - 2.2 L1 정규화의 확률적 유도\n\n- L2 정규화\n  - 3.1 L2 정규화의 분석적 유도\n  - 3.2 L2 정규화의 확률적 유도\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 1. 규제의 개요\n\n우선, 규제의 개념을 다시 살펴봅시다. 구체적으로 이해하기 위해 두 가지 차원을 가진 소량의 데이터를 예로 들어봅시다 [1].\n\n![이미지](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_1.png)\n\n보시다시피, 이 데이터는 비선형입니다. 데이터가 비선형이기 때문에 간단한 선형 회귀가 이 데이터에 적합하지 않을 것이라고 쉽게 추측할 수 있습니다. 이 경우, 비선형 데이터를 나타내기 위해 다항 회귀를 고려해 보겠습니다. 규제의 중요성을 이해하기 위해 우리는 15차 다항 회귀를 사용하여, 즉, 과도하게 복잡한 함수를 사용하여 데이터를 예측합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마크다운 형식으로 변경된 문구입니다.\n\n\n![Understanding L1 and L2 regularization with analytical and probabilistic views](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_2.png)\n\nscikit-learn 라이브러리의 PolynomialFeatures 및 Ridge 클래스를 사용하여 데이터를 적합시킵니다. 결과는 아래와 같습니다.\n\n![Understanding L1 and L2 regularization with analytical and probabilistic views](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_3.png)\n\n왼쪽 그림은 정규화가 없는 다항 회귀를 보여주며, 오른쪽 그림은 정규화가 적용된 다항 회귀를 보여줍니다. 정규화가 없는 다항 회귀는 원래 데이터에 비해 지나치게 복잡한 함수를 사용했기 때문에 데이터에 과적합되었습니다. 반면, 정규화가 적용된 다항 회귀는 과적합을 방지하면서 모델의 복잡성을 줄일 수 있어서 정규화가 적용되었다면 정규화가 없는 것보다 더 나은 적합을 할 수 있습니다. 일반적으로, 위의 예시처럼 모델이 과적합되는 것을 방지하기 위해 정규화를 사용합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n정규화를 어떻게 할까요? 이론적으로는 정규화 용어를 목적 함수에 추가하여 이를 기반으로 매개변수를 최적화합니다. 아래에서 보여지는 것처럼요.\n\n\u003cimg src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_4.png\" /\u003e\n\n정규화 용어는 계수 값이 증가하지 않을 때를 위해 패널티를 부과합니다. 그럼 계수에 패널티를 부여하는 이유는 무엇일까요? 직관적으로 이해하려면 이전 예제의 계수를 살펴보겠습니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_5.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 줄은 정규화 없이 다항 회귀의 계수를 나타내고, 아래 줄은 정규화(L2)를 적용한 다항 회귀의 계수를 나타냅니다. 정규화 없이 다항 회귀는 더 큰 계수 값을 갖습니다. 직관적으로, 모델이 더 큰 계수 값을 갖는다는 것은 모델이 변화를 크게 일으킬 수 있다는 것을 의미합니다. 따라서, 정규화 없이 모델은 주어진 데이터와 더 정확하게 맞을 수 있지만 일반적이지는 않습니다. 한편, 정규화를 적용한 모델은 상대적으로 계수 값이 작기 때문에 주어진 데이터를 더 일반적으로 맞출 매개변수를 탐색할 수 있습니다.\n\n지금까지 정규화의 개념과 그 효과에 대해 이해했습니다. 이제 정규화 뒤의 이론적 배경을 자세히 살펴보겠습니다.\n\n## 2. L1 정규화\n\nL1 정규화[2]는 계수의 절댓값 또는 계수의 l1-노름을 정규화 용어로 추가합니다. L1 정규화는 계수의 특징 선택에 도움을 줍니다. 즉, 관련 없는 독립 변수의 수를 줄일 수 있습니다. 구체적으로, L1 정규화를 적용한 회귀 모델은 Least Absolute Shrinkage and Selection Operator(Lasso) 회귀라고 불립니다. L1 정규화의 공식은 아래와 같습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_6.png)\n\nwhere w is the parameter. From now on, we will learn how to solve this problem.\n\n## 2.1 Analytical derivation of L1 regularization\n\nHow can we optimize the L1 regularization formula? To solve it analytically, this formula can be seen as constraint optimization with Lagrange multipliers.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Understanding L1 and L2 regularization](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_7.png)\n\n마지막 방정식을 보시면, L1 정규화 공식과 동일합니다. 다음으로, 유도된 라그랑지안을 기반으로 매개변수를 어떻게 해석적으로 최적화할 수 있는지 알아보겠습니다. 안타깝게도, L1 정규화에 대한 닫힌 솔루션을 얻을 수는 없습니다. 왜냐하면 정규화 항을 미분할 수 없기 때문입니다. 이 사실을 아래 그림에서 확인할 수 있습니다. 두 매개변수 함수가 있다고 가정하고, L1 정규화 항을 다음과 같이 나타낼 수 있습니다:\n\n![L1 Regularization Term](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_8.png)\n\n가장자리에서 도함선을 계산할 때, 오른쪽과 왼쪽에서 서로 다른 값이 나오기 때문에, 가장자리에서 미분할 수 없습니다 ([7]에서 더 많은 수학적 세부사항을 확인하실 수 있습니다). 몇 가지 예외적인 상황을 제외하고 닫힌 형태의 솔루션을 찾는 것은 어렵지만, 행렬 X가 직교하고 매개변수 수가 하나인 경우 닫힌 형태의 솔루션을 찾을 수 있습니다 [3]. 그러나 이러한 상황은 실제 분석에서 드물게 발생합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼, 파라미터를 어떻게 찾아야 할까요? 라쏘 문제를 해결하는 가장 일반적인 방법은 서브그래디언트와 좌표 하강법입니다. scikit-learn의 구현에서는 라쏘 문제를 최적화하기 위해 좌표 하강법을 사용하므로 좌표 하강법을 배워봅시다 [4].\n\n좌표 하강법은 간단한 아이디어입니다. n 차원 함수 f가 있다고 가정했을 때, 우리는 f를 각 파라미터 차원을 반복적으로 최소화하여 최소화합니다. 수학적 정의를 살펴봅시다.\n\n![image](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_9.png)\n\n이름에서 알 수 있듯이, 우리는 파라미터를 개별적으로 계산하고 이전 값에 기반하여 업데이트합니다. 이 과정은 수렴하거나 설정한 최대 반복 횟수에 도달할 때까지 계속됩니다. 정말 간단하지 않나요? 이제 라쏘 공식의 구체적인 예제를 살펴봅시다. 평균 제곱 오차(MSE)로 된 라쏘 문제를 고려해봅시다. 따라서 공식은 다음과 같을 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_10.png\" /\u003e\n\n포함 된 좌표 감소 방법을 사용하기 때문에, 다른 비대상 매개변수를 고정한 채 각 매개변수에 대해 이 공식을 최소화해야 합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_11.png\" /\u003e\n\n마지막 공식은 조금 까다롭습니다 (적어도 저에게는 그렇습니다). 각 항목의 차원을 고려할 때, X의 전치 행렬의 i번째 행만 i번째 경사에 관련되어 있음을 이해할 수 있습니다. i번째 매개변수의 경사를 정식화하려면 위의 공식을 재정의해야 합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_12.png\" /\u003e\n\n파라미터 업데이트 방정식을 얻으려면 XB 항을 분해해야 합니다. XB의 i번째 열과 다른 열로 나눕니다. 보시다시피, 파라미터 업데이트 공식을 유도할 수 있습니다. L1 정규화 항은 어떠신가요? 이를 해결하기 위해 소프트 쓰레스홀딩을 소개할 것입니다. L1 항을 미분할 수 없기 때문에, 우리는 서브그레디언트(subgradients)와 부분도함수(subdifferentials)를 사용하여 이를 근사할 것입니다. 서브그레디언트(subgradients)의 개념에 대해서는 [5]의 이론을 확인해볼 수 있습니다. 서브그레디언트를 사용하여 다음을 유도할 수 있습니다:\n\n\u003cimg src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_13.png\" /\u003e\n\nLasso 문제에서 L1 정규화 항을 대체하면 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래에 있는 표를 Markdown 형식으로 변경해주세요.\n\n\n![Understanding L1 and L2 regularization](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_14.png)\n\n주어진 마지막 방정식을 재정립하여 𝛽를 구할 수 있습니다:\n\n![Understanding L1 and L2 regularization](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_15.png)\n\n부호 및 최댓값 함수를 사용하여 조건부 분기를 구성할 수 있습니다. 이제 우리는 수렴할 때까지 이 최종 공식을 반복하여 파라미터를 업데이트합니다. 구체적인 예시를 풀어보겠습니다. 시각화를 위해 두 개의 매개변수로 함수를 최적화하고 바이어스 항이 없다고 가정합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_16](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_16.png)\n\n정규화 강도로 𝜆 = 1을 사용하고 유도한 방정식에 따라 매개변수 값을 업데이트합니다.\n\n이 경우 수렴이 빠르게 진행되는 것을 확인할 수 있습니다. 계수 값은 scikit-learn 구현과 거의 같습니다.\n\n```js\n# 스크래치에서 좌표 하강\nb0 = 1.00, b1 = 2.00\n\n# scikit-learn\nb0 = 1.11, b1 = 2.04\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n실제 케이스에서는 Cython을 사용하여 좌표 하강을 계산하는 scikit-learn 구현을 사용해야 합니다. 그들의 구현은 원래의 Python 코드보다 훨씬 더 빠릅니다. 추후에는 함께 사용한 코드를 공유할 예정이에요.\n\n## 2.2 L1 규제의 확률적 유도\n\n확률적 측면에서 L1 규제에 대해 자세히 알아보기 전에 MAP(최대 사후 확률) 추정치를 알아야 합니다. MAP 추정 및 최대 우도 추정치 사이의 차이를 배워봅시다. 이미 알고 계신 분들은 다음 섹션을 건너뛸 수 있습니다.\n\n필수 사항 — 최대 우도 추정 및 MAP 추정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리가 다중 선형 함수를 갖고 있다고 가정하고, 관측된 데이터 점과 예측된 값 사이의 오차가 평균 0 및 표준 편차 𝜎를 따르는 정규 분포를 따른다고 가정합니다. 가정된 분포 하에서 오차가 발생하는 확률 밀도, 즉 우도는 다음과 같이 유도할 수 있습니다:\n\n여기서 데이터 점의 개수를 n이라 하고 매개변수의 수를 p라고 합니다. 이 확률 밀도를 최대화하는 매개변수를 찾고 싶은데, 이를 최대 우도 추정(Maximum Likelihood Estimation, MLE)이라고 합니다. MLE 공식은 다음과 같이 쓸 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적으로 로그를 취하여 곱셈을 합으로 변경합니다. 빈도주의 통계학에서는 매개변수를 상수값으로 간주하지만 알 수 없으므로 미분을 사용하여 우도를 최대화하는 매개변수를 찾습니다.\n\n반면에 베이즈 정리에서는 매개변수를 확률 변수로 취할 수 있습니다. 베이즈 정리를 적용하여 우도를 다음과 같이 볼 수 있습니다:\n\n![image](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_19.png)\n\n사후 확률은 우도와 사전 확률에 비례합니다. 이 설정에서는 MLE와 같이 우도를 최대화하는 대신 사후 확률을 최대화해야 합니다. 사후 확률을 최대화하는 것을 최대 사후 확률 추정이라고 하며 MAP 추정이라고 합니다. 다음과 같이 정의할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_20.png\" /\u003e\n\n이전 예제에 적용할 때 MLE와 대조적인 점을 살펴보면:\n\n\u003cimg src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_21.png\" /\u003e\n\nMAP 추정을 하는 데는 사전 확률이 필요하다는 것을 알 수 있습니다. 이를 위해 어떤 확률 분포든 사용할 수 있습니다. 이제 L1 정규화로 돌아가보겠습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n확률적 유도: 라플라스 사전을 사용한 L1 정칙화\n\n우리가 사전으로 라플라스 분포를 선택하면, MAP 추정은 L1 정칙화 공식이 됩니다. 이를 유도해 봅시다! 라플라스 사전은 아래와 같은 모양을 가진 확률 분포 중 하나입니다.\n\n![image](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_22.png)\n\n지수 함수의 지수항이 L1 정칙화 항과 유사함을 알 수 있습니다. 이제, 우리는 MAP 추정에서 사전 확률로 평균 0을 가진 라플라스 사전을 대입합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 마지막 공식이 L1 정규화와 동일함을 확인할 수 있습니다! 직관적으로 라플라스 분포의 모양이 가우시안 분포보다 훨씬 더 날카롭습니다. 이는 아래의 해석적 유도 부분에 표시된 L1 정규화 용어와 유사합니다.\n\n아래는 마지막 공식이 L1 정규화와 동일함을 확인할 수 있습니다! 직관적으로 라플라스 분포의 모양이 가우시안 분포보다 훨씬 더 날카롭습니다. 이는 아래의 해석적 유도 부분에 표시된 L1 정규화 용어와 유사합니다.\n\n지금까지 해석적 및 확률적 관점에서 L1 정규화 유도를 이해했습니다. 다음에는 L2 정규화에 대해 알아보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 3. L2 정규화\n\nL2 정규화는 계수의 제곱값 또는 계수의 L2-노름을 정규화 항으로 추가합니다. L2 정규화는 작은 계수를 유도하는 데 도움이 됩니다. L2 정규화가 적용된 회귀 모델을 Ridge 회귀라고 합니다. L2 정규화의 수식은 아래와 같습니다.\n\n![수식](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_25.png)\n\n## 3. 1 L2 정규화의 해석적 도출\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nL1 정규화와 마찬가지로 L2 정규화 문제를 라그랑주 승수를 사용한 제한 최적화로 생각해 볼 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_26.png)\n\n마지막 방정식은 L2 정규화 공식과 같습니다. L1 정규화와 대조적으로 이 공식은 미분 가능합니다. 따라서 새로운 개념을 도입할 필요가 없습니다. 그저 미분하면 되죠!\n\n![이미지](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_27.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 L2 정규화에 대한 닫힌 형태의 해를 얻었습니다. 이를 구현하고 scikit-learn ridge 결과와 비교해 봅시다.\n\n```python\n# 샘플 데이터\nX = np.random.randn(100, 2)\nbeta = np.array([2, 3]).reshape(1, 2)\nY = X @ beta.T + np.random.normal(beta.shape[0])\n\nlam = 1.0\ninv_mat = np.linalg.inv(X.T @ X + np.eye((X.T @ X).shape[0]))\n\nridge_coef = inv_mat @ X.T @ Y\nprint(ridge_coef.reshape(-1))\n# [1.998, 2.937]\n\nridge = Ridge(alpha=1.0)\nridge.fit(X, Y)\nprint(ridge.coef_.reshape(-1))\n# [1.979, 2.973]\n```\n\n거의 같은 결과를 얻을 수 있다는 것을 확인할 수 있습니다.\n\n## 3.2 L2 정규화의 확률론적 유도\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMAP 추정을 다시 고려해 봅시다. L1 정칙화를 유도할 때는 사전 분포로 라플라스 분포를 사용합니다. L2 정칙화의 경우, 평균이 0인 가우시안 분포를 사전 분포로 활용합니다.\n\n![image](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_28.png)\n\n지수 함수의 거듭제곱 항이 L2 정칙화 항과 유사하다는 것을 알 수 있습니다. 이제 MAP 추정에서 평균이 0인 가우시안 사전 확률로 대체합니다.\n\n![image](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_29.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막 공식이 L2 정규화와 동일한 것을 확인할 수 있습니다. 직관적으로 가우시안 분포의 형태는 라플라스 사전보다 부드러운 곡선을 가집니다. 따라서 이는 L2 정규화 용어와도 유사합니다.\n\n![이미지](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_30.png)\n\n마지막으로, 이 블로그에서 사용된 코드를 공유하겠습니다.\n\n이 블로그에서는 L1 및 L2 정규화의 자세한 유도를 해석적 및 확률론적 관점을 통해 소개했습니다. 이 블로그가 정규화의 수학적 배경을 이해하는 데 도움이 되길 바랍니다. 읽어 주셔서 감사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 참고 자료\n\n[1] Underfitting vs.Overfitting, scikit-learn 문서\n\n[2] Manfredi, V., Lecture 12: Regularization, 웨즐리안 대학 강의\n\n[3] https://stats.stackexchange.com/questions/17781/derivation-of-closed-form-lasso-solution\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[4] Tibshirani, R., Coordinate Descent, Carnegie Mellon 대학 강의\n\n[5] Giba, L., Subgradient Descent Explained, Step by Step, MLC\n\n[6] Kang, B., 정규화의 확률적 해석, Bounded Rationality\n\n[7] https://math.dartmouth.edu/opencalc2/cole/lecture21.pdf","ogImage":{"url":"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_0.png"},"coverImage":"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_0.png","tag":["Tech"],"readingTime":12},{"title":"영화 제목이 점점 길어지고 있는 걸까요 통계 분석 해보기","description":"","date":"2024-06-19 20:09","slug":"2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis","content":"\n\n![이미지](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_0.png)\n\n이 글은 주간 뉴스레터인 'Stat Significant'에서 영화, 음악 및 TV에 관한 데이터 중심의 에세이를 다룬 것입니다.\n\n## 소개: 긴 영화 제목의 여름\n\n2024년 여름은 긴 영화 제목의 해입니다. 대형 흥행 영화의 계절이 시작되었으며 Godzilla x Kong: The New Empire로 시작해 세계 제국의 왕국이 박스 오피스 막대를 넘겨준 후 Fury Road: A Mad Max Saga로 아이가 가장 많이 벌어들인 영화로 자리를 내줬습니다. 이전에는 단순한 명명 체계 - Fear, The Fugitive, The Firm, Dave -가 있었지만 지금은 겉으로는 역동적인 단어들이 깔린 알파벳 수프로 대체되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼에도 불구하고, 이러한 장황한 제목들은 신형 할리우드 광고의 산물 이상의 것이다. 오히려, 이러한 명명 체계들은 박스 오피스 수익의 축소에 대응하고 고품질 가정 엔터테인먼트 옵션들로부터의 경쟁에 적응하는 영화 산업을 반영한다. 듣기로는, 영화 관객들은 집을 나와야만 하는데 길고 감정적인 제목들이 필요하다고 한다.\n\n그래서 오늘은 영화 명명의 트렌드, 산업 역학, 이러한 트렌드를 움직이는 과장된 단어들, 그리고 특정 구두점의 부상을 조사해 볼 것이다. Stat Significant Literary Universe (SSLU)에서 또 다른 멋진 에피소드를 위해 준비하시라: \"Rise of the Long-Winded Movie Title: A Stat Significant Saga Part 1 (of 1): The Legend of The Kingdom of an Ailing Movie Industry.\"\n\n## 영화 제목들이 더 기네요?\n\n2006년의 'Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan'는 예산이 $20백만 이상인 프로젝트들 중에서 가장 긴 영화 제목의 기록을 보유하고 있다(인플레이션 조정 포함) — 비록 이 영화가 대부분의 장황한 이름과는 뚜렷한 차이를 보이고 있다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_1.png)\n\n보랏의 여정을 따라가는 영화 제목은 주인공의 이름 이후에 나오는 모든 것이 지나치게 무의미하다는 것을 보여줍니다. 반면에 다른 긴 제목들은 서투른 기업 마케팅의 산물이며 (얼마나 많은 맥킨지 컨설턴트들이 불어난 장미: 송버드와 스네이크스 의 이름을 지을 때 도움을 주었는지 아무도 모릅니다.) 이렇게 길게 늘이는 제목들은 관객들에게 알려진 이야기 (지적 재산권 ❤️)을 제공하면서도 약간의 새로움을 제공합니다 - 그것들은 같지만 다릅니다.\n\n당연히 할리우드의 프랜차이즈가 가장 긴 영화 제목 목록을 지배하며, 이 중 대부분은 만화책 네이밍 규칙을 활용하고 있습니다.\n\n![이미지](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_2.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n할리우드의 시리즈 엔터테인먼트에 대한 열광은 2000년대 초반, 스파이더맨 2, 반지의 제왕 시리즈 및 해리포터 시리즈의 대성공을 통해 시작되었습니다. 2010년대와 2020년대에 이 추세가 계속되며 할리우드는 시리즈 스토리텔링을 더욱 받아들이면서 제목의 길이가 점점 더 길어졌습니다.\n\n![이미지](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_3.png)\n\n장르별 제목 길이를 살펴보면, 어드벤처, 판타지 및 패밀리와 같은 시리즈 친화적 형식이 일반적으로 더 많은 영숫자 문자를 사용한다는 것을 알 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 데이터 포인트는 사실과 같이 더 많은 내용을 제공해요:\n\n- 마블이 엔터테인먼트를 망쳤어요\n- 넷플릭스도 그렇죠\n- 밥 아이거가 이에 기여했을지도 몰라요\n- A24는 현대 시네마의 구원자에요\n- 크리스토퍼 놀란도 그렇죠\n\n영화는 예전에는 훌륭했었어요 (독자님같이 14살이었을 때 주로), 하지만 이제는 나빠졌고, 긴 제목은 이 매체의 예술적 몰락을 또 다른 지표로 보여줍니다. 세상이 절대 변하지 않았다면, 이런 문제가 없었을 거에요 — 영화 제목은 최대 세 음절로 유지됐을 거예요.\n\n우리는 이러한 사고방식을 계속할 수 있어요(무기한으로), 냉소적인 태도에 빠지며 Flubber, The Game, Titanic, Scream 2 같은 간결한 제목들의 시대를 그리워할 수 있지만, 만약 우리가 그렇지 않았다면 어떨까요? 불평 대신에 이 계속해서 늘어나는 제목들의 힘을 주는 구문과 문구를 탐험하며 긴 제목체계의 황당함을 받아들였다면 어떨까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 긴 영화 제목의 구성 요소: 흔한 단어 및 문장 부호\n\nRinger의 \"Big Picture\" 팟캐스트는 영화 산업의 예술적 및 상업적 트렌드에 대한 깊은 분석을 제공합니다. 이 쇼의 주인공인 션 페너시는 산업이 지적 재산권(IP)에 점점 의존하고 있다는 점에 공개적으로 경멸을 품고 있는 열렬한 영화 애호가입니다. 그는 어리석은 IP에 대해 이야기할 때마다, Rebel Moon — Part Two [Colon] The Scargiver처럼 영화 제목에 포함된 \"콜론\"을 명시적으로 언급합니다.\n\n![2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_5](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_5.png)\n\n이 쇼 몇 주 동안 들은 후, 페너시의 섬세한 항의 행위가 집에 닿기 시작했습니다: 왜 많은 영화들(일부는 심지어 시퀄도 아닌 것들)이 콜론을 사용하는 걸까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n영화 제목의 단어와 문장부호 사용을 검토할 때, 콜론의 압도적인 숫자를 볼 수 있습니다. 다른 긴 제목에서 나오는 용어는 영화가 시리즈 내에 있는 위치를 나타내거나 새로운 장을 강조합니다. 그러나 현대 할리우드의 주인공은 콜론입니다. 영화 산업은 항상 시퀄에 의존해 왔지만, 이 구두점의 증가된 사용은 영화 마케팅의 새로운 방향을 강조하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_7.png](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_7.png)\n\n모든 영화는 한 번뿐인 경험, 일순간의 사건으로, 긴 (아마도 끝없는) 이야기의 단일 장을 제공합니다. 수퍼맨 2와 스파이더맨 3 시대는 끝났고, 대신 수퍼맨: 강철의 사나이와 스파이더맨: 홈 노 웨이가 등장했습니다. 많은 면에서, 이 두 점은 아메리칸 영화관이 오랜 시간 동안 지켜온 매력 요소인데, 이는 연재물 콘텐츠에 현혹된 사람들로 인해 비난의 대상이 되었습니다. 하지만 넷플릭스와 데이비드 자스로브를 탓할 수도 있습니다.\n\n## 마지막 생각: 우리는 더 많은 콜론이 필요한가요?\n\n![2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_8.png](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_8.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 계속 확장되는 영화 제목들의 근본적인 질문이 존재합니다: 스튜디오들은 이러한 제목들로 어떤 것을 이루려고 하는 걸까요? 왜 할리우드는 이러한 메시지를 채택하는 걸까요?\n\n이러한 긴 제목들의 목적을 더 잘 이해하기 위해 최근 개봉한 몇 편의 샘플을 살펴보겠습니다:\n\n- Godzilla x Kong: The New Empire: 거대한 도마뱀이 거대한 원숭이와 싸움을 벌일 것이며, 그것은 황홀할 것입니다.\n- Kingdom of the Planet of the Apes: 이 영화는 플래닛 오브 더 에이프 시리즈의 또 다른 장면으로, 보통 크기의 원숭이들이 독재 정부로 조직되는 것을 다룰 것입니다.\n- The Garfield Movie (2024) 그리고 이 전작 Garfield: The Movie (2004): 이 영화들은 가필드에 대한 영화일 것입니다 (저는 확신합니다).\n\n이러한 명명 선택은 영화 산업의 결과로, 극장으로 사람들을 유혹할 수 있는 것은 이벤트로 만드는 경우에 한정됩니다. 스트리밍과의 경쟁에서, 영화는 일련화로 향하고 동시에 이러한 부분들을 놓칠 수 없는 이벤트로 구성하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n너무 크게 자란 도마뱀과 거대한 원숭이가 \"새로운 제국\"(무엇을 의미하는지는 모르지만) 배경 속에서 싸운다고 주장하는 것은 영화를 스펙터클로 변신시킵니다. 이 영화는 시퀄에 그치는 것이 아니라, 여러분이 좋아했던 이전 시리즈와 너무 다르지도 않습니다. 이 마케팅 전략이 사람들을 유인하여 집을 떠나 1인당 $22를 내고 이 원숭이와 도마뱀의 결투를 집에서 $3.99(또는 무료!)에 보는 대신 극장에서 관람하도록 만드는 것 같아요!\n\n올해 미국 박스 오피스는 지난해 파업과 슈퍼히어로 콘텐츠의 부재에 따라 25% 감소한 상황입니다 (마블은 2024년에 한 편의 영화만 출시할 예정입니다). 이렇게 쓰는 것이 마음 아프지만, 점 이상이 필요하다는 주장이 있을 수 있습니다. 극장에 사람들을 모아서 그들에게 영화의 매력(예: 니콜 키드먼 광고와 편한 좌석)을 다시 알려주고, 이 관객들이 향후 작품들의 예고편을 샘플링할 수 있게 해, 그들이 관람하러 극장에 다시 오게 합니다. 이것은 플라이휠을 시작할 영화가 없으면 관람 습관이 사라진다는 것을 의미합니다. 어쩌면 우리는 어리석은 콜론으로 가득한 제목을 가진 일련화된 마블 컨텐츠를 충분히 필요로 하지 않을까요.\n\n\"25년까지 살아남아라\"는 엔터테인먼트 산업에서 흔한 속담이 되었습니다. 이 시기에 파업 후 콘텐츠가 데뷔할 것이며 모든 것이 완벽해지리라고 예상됩니다(더 이상 문제는 없을 것입니다!).\n\n우리도 이 말을 채택해야 할지 모르겠습니다. 하지만 우리만의 특별한 변화와 함께: \"25년까지 버텨라: 상황은 나아진다: 새로운 제국(희망적으로)\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만화, 음악 및 TV에 대한 더 많은 데이터 중심 에세이를 읽고 싶다면, 내 뉴스레터 Stat Significant를 확인해보세요.\n\n데이터 및 통계에 대해 이야기 나누고 싶으세요? 흥미로운 데이터 프로젝트가 있으신가요? 혹시 인사를 전하고 싶으신가요? daniel@statsignificant.com 으로 이메일 보내주세요.","ogImage":{"url":"/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_0.png"},"coverImage":"/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_0.png","tag":["Tech"],"readingTime":6},{"title":"이상치 탐지를 위한 3가지 간단한 통계적 방법","description":"","date":"2024-06-19 20:07","slug":"2024-06-19-3SimpleStatisticalMethodsforOutlierDetection","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-3SimpleStatisticalMethodsforOutlierDetection_0.png\" /\u003e\n\n데이터 과학자의 일 중 중요한 부분은 데이터를 정제하고 전처리하는 것입니다. 이 과정 중 하나인 이상치 탐지와 제거는 매우 중요합니다. 대규모의 이상치, 급증, 그리고 나쁜 데이터는 정확한 기계 학습 모델을 학습하는 데 방해가 될 수 있기 때문에, 이상치를 적절하게 처리하는 것이 중요합니다.\n\n하지만 데이터 과학자들이 항상 이상치를 식별하기 위해 격리 숲 또는 국소 이상치 요소처럼 기계 학습 모델을 사용하는 것은 아닙니다. 제 데이터 과학 경력에서 배운 한 가지는 간단한 해결 방법이 효과적이면 그것을 사용해야 한다는 것입니다.\n\n이번에는 대부분의 시간에 잘 동작하는 이상치를 탐지하는 데 유용한 3가지 간단한 통계적 솔루션을 제공하고자 합니다. 또한 이를 Python에서 어떻게 수행하는지도 보여드릴 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 1. Z 점수\n\nZ 점수는 표준 점수로도 알려져 있으며, 특이값을 감지하는 데 사용되는 잘 알려진 방법 중 하나입니다. 기본적으로 어떤 데이터 포인트가 평균에서 몇 개의 표준 편차만큼 떨어져 있는지를 나타냅니다.\n\n어떤 데이터셋의 특정 데이터 포인트의 Z 점수는 다음과 같이 계산됩니다:\n\n![](/assets/img/2024-06-19-3SimpleStatisticalMethodsforOutlierDetection_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어디서:\n\n- Z는 Z 점수 값입니다.\n- x는 데이터 포인트입니다.\n- μ는 데이터 집합의 평균입니다.\n- σ는 표준 편차입니다.\n\n따라서 Z 점수가 4인 경우 데이터 포인트가 평균보다 4 표준 편차 위에 있음을 의미합니다. Z 점수가 -4인 경우에는 평균보다 4 표준 편차 아래에 있습니다.\n\nZ 점수의 경우, 일반적으로 3 이상 또는 -3 미만의 값은 이상치로 간주됩니다. 그러나이 기준은 유연하며 프로그래머에 따라 조정할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같이 Python과 scipy.stats 패키지를 사용하여 데이터프레임 열의 각 값의 z 점수를 계산하는 간단한 방법이 있어요:\n\n```python\nfrom scipy import stats\n\ndf[\"z_score\"] = stats.zscore(df[\"column_of_interest\"])\n```\n\n데이터셋의 각 값에 대한 z 점수를 얻은 후에는 이상값을 걸러낼 수 있어요:\n\n```python\ndf_clean = df[(df[\"z_score\"] \u003c= 3) \u0026 (df[\"z_score\"] \u003e= -3)]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nz 점수의 단점 중 하나는 이상치 탐지 방법임에도 불구하고 이상치에 민감하다는 것입니다. 데이터 세트에 매우 큰 이상치가 있는 경우 평균을 왜곡할 수 있습니다 (평균은 이상치에 민감하기 때문입니다). 평균이 왜곡되면 더 작지만 여전히 관련 있는 이상치를 잡지 못할 수 있습니다.\n\n## 2. IQR\n\nIQR (사분위 범위)는 평균 대신 중앙값을 기준점으로 사용하기 때문에 z 점수보다 견고합니다.\n\n![이미지](/assets/img/2024-06-19-3SimpleStatisticalMethodsforOutlierDetection_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 IQR 값을 구했으니, 다른 점들이 이상값인지 판별하는 기준점으로 사용될 것입니다. Q1 값보다 1.5 * IQR 이상 또는 Q3 값보다 1.5 * IQR 이하인 모든 점은 이상치로 간주됩니다.\n\n이 경우, 46.5(27 + 13 * 1.5)보다 큰 값 또는 -5.5(14 - 13 * 1.5)보다 작은 값은 이상치로 간주됩니다.\n\n파이썬에서 numpy의 percentile 및 scipy stats의 IQR 함수를 사용하여 이를 계산하는 방법은 다음과 같습니다:\n\n```python\nfrom scipy.stats import iqr\nimport numpy as np\n\n# 데이터의 IQR 구하기\niqr_data = iqr(df[\"column_of_interest\"])\n# 범위를 벗어나는 값을 얻기 위한 참조점 계산 (1.5 * IQR)\niqr_lim = 1.5 * iqr_data\n\n# 상위 (Q3 또는 75번째 백분위수)와 하위사분위 (Q1 또는 25번째 백분위수) 계산\nq1 = np.percentile(df[\"column_of_interest\"], 25)\nq3 = np.percentile(df[\"column_of_interest\"], 75)\n\n# 사분위수와 IQR*1.5를 사용하여 상한선과 하한선 결정\n상한선 = q3 + iqr_lim\n하한선 = q1 - iqr_lim\n\n# 상한선보다 작거나 하한선보다 큰 값은 이상치로 간주하고 제거\ndf_clean_iqr = df[(df[\"column_of_interest\"] \u003e= lower_limit) \n\u0026 (df[\"column_of_interest\"] \u003c= upper_limit)]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서 볼 수 있듯이 데이터 세트의 IQR을 계산하려면 z 점수보다 몇 가지 더 많은 단계/코드 줄이 필요합니다. 또한 각 데이터 포인트에 대한 가시적인 \"점수\"를 얻지 못하므로 얼마나 이상하게 큰지를 나타내지 않습니다. 여기서는 어떤 것이 범위를 벗어났는지 여부만을 알 수 있습니다.\n\n그러나 중앙값을 사용하기 때문에 평균보다 쉽게 왜곡되지 않아서 데이터 세트의 이상치에 덜 민감합니다.\n\n# 3. 수정된 z 점수\n\n수정된 z 점수는 z 점수와 IQR의 측면을 모두 고려하여 표준 z 점수의 더 견고한 버전을 만듭니다. 데이터 점이 얼마나 \"멀리 떨어져\" 있는지를 대략적으로 알려주는 점수를 제공하면서도 이상치에 민감하지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nz 점수와 수정된 z 점수의 주요 차이점은 수정된 z 점수가 평균 대신 중앙값을 기준점으로 사용한다는 것입니다. 표준 편차가 평균과 직접적으로 관련되어 있기 때문에, 수정된 z 점수는 정확한 표준 편차를 측정하지 않습니다. 그러나 중위수 절대 편차(MAD)를 사용하여 표준 편차를 근사하려고 합니다.\n\n![image](/assets/img/2024-06-19-3SimpleStatisticalMethodsforOutlierDetection_3.png)\n\n여기서,\n\n- 0.6745는 표준 편차의 중위수에 해당하는 값을 근사화하는 데 사용되는 상수입니다.\n- xi는 조사하는 데이터 포인트입니다.\n- x͂는 데이터셋의 중앙값입니다.\n- MAD는 데이터셋의 중위수 절대 편차입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n중앙값 절대 편차를 계산하려면 데이터 세트의 중앙값에서 각 데이터 포인트를 빼면 됩니다. 이 뺄셈의 절대값을 취하세요. 마지막으로, 이러한 절대 차이의 중앙값을 취하면 됩니다.\n\n일반적으로 수정된 z 점수에서는 점수가 ` 3.5 또는 `-3.5 인 값이 이상치로 간주됩니다.\n\n수정된 z 점수의 계산 방법과 Python에서의 실제 예제에 대한 더 자세한 정보가 필요하시면 수정된 z 점수에 관한 제 글을 참조해주세요:\n\nPython에서 수정된 z 점수를 구현하는 방법은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 이 함수는 값을 취하고 데이터 집합을 취하여 하나의 값을 위한 수정된 z 점수를 반환합니다.\ndef compute_mod_z_score(value,df):\n    # 데이터 집합의 MAD(흑백절대편차) 계산 (관심 있는 열)\n    med_abs_dev = (np.abs(df[\"column_of_interest\"] - \n                  df[\"column_of_interest\"].median())).median()\n    const = 0.6745\n    mod_z = (const * (value - df[\"column_of_interest\"].median()) \n            / med_abs_dev)\n    return mod_z\n\n# 위의 함수를 전체 열에 적용하여 수정된 Z 점수를 모든 데이터 점에 대해 얻습니다.\ndf[\"mod_zscore\"]=df[\"column_of_interest\"].apply(compute_mod_z_score,df=df)\n```\n\n수정된 Z 점수의 주요 단점은 덜 알려져 있으며 MAD와 같은 변수를 사용하기 때문에 설명하기가 조금 더 복잡하고 어려울 수 있다는 것입니다. 또한 저는 아는 한 Python 라이브러리 중에 수정된 Z 점수를 계산하는 것이 없습니다.\n\n# 결론\n\n보시다시피, 이상치 탐지의 각 통계적 방법마다 이점과 단점이 있습니다. 제가 일하는 곳에서는 이를 모두 사용했지만 다른 데이터 집합과 사용 사례에 대해 사용했습니다. 데이터를 탐색하여 문제에 접근하는 방법을 알아야 한다는 점의 중요성을 강조할 수 없습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 데이터셋이 급격한 변동이 있는 경우에는 수정된 z 점수나 IQR이 가장 적합할 수 있습니다. 가장 간단하고 설명하기 쉬운 해결책을 찾고 있다면 z 점수 / 표준 점수를 선택하는 것이 좋습니다. \n\n언제나 여러분만의 테스트를 실행하고 다른 데이터 과학자들이나 결과에 혜택을 받을 수 있는 관련 이해당사자들과 상의하는 것이 중요합니다.\n\n# 읽어주셔서 감사합니다","ogImage":{"url":"/assets/img/2024-06-19-3SimpleStatisticalMethodsforOutlierDetection_0.png"},"coverImage":"/assets/img/2024-06-19-3SimpleStatisticalMethodsforOutlierDetection_0.png","tag":["Tech"],"readingTime":5},{"title":"MLOps - PyTest를 사용한 데이터 검증","description":"","date":"2024-06-19 20:05","slug":"2024-06-19-MLOpsDataValidationwithPyTest","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-MLOpsDataValidationwithPyTest_0.png\" /\u003e\n\n# 소개\n\nMLOps 파이프라인에서는 가능한 한 많은 단계를 자동화하려고 노력합니다. 프로그래머의 직접적인 개입으로 발생할 수 있는 오류의 수를 최소화하는 것이 목표입니다. 또한 데이터셋 유효성 검사에 유의하는 것도 중요합니다. 누구나 기계 학습의 제1 규칙에 대해 익숙할 것입니다: 쓰레기를 넣으면 쓰레기가 나옵니다. 우리가 개발하는 모델이 얼마나 정교하든, 데이터셋의 관리가 제대로 이루어지지 않으면 높은 확률로 나쁜 결과를 얻을 것입니다.\n\n이 기사에서는 PyTest를 사용하여 데이터셋에 대한 자동 검증을 수행하는 방법을 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 Deepnote을 사용하여 이 기사의 스크립트를 실행하고 있어요. Deepnote은 협업형 데이터 과학 프로젝트와 프로토타이핑에 좋은 클라우드 노트북 서비스에요.\n\n## ETL에 대하여\n\n처음으로 머신 러닝에 접근하는 사람들은 대부분 Kaggle에서 볼 수 있는 도전 과제를 해결해야 하는데요. 이러한 도전 과제에서는 거의 항상 시간이 지나도 변하지 않는 정적인 데이터셋을 다루게 됩니다. 하지만 실제 세계에서는 이것이 완전히 사실이라고 할 수 없어요.\n\n실제 머신 러닝 제품을 개발할 때에는 데이터가 지속적으로 변할 수 있어요. 그 결과 데이터는 데이터 추출, 데이터 변환, 데이터 로딩의 초기 단계를 거쳐 얻어지게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 세 단계는 일반적으로 ETL 이라는 약어로 요약됩니다. 간단히 말해서, 데이터 수집을 수행하여 모델 훈련을 진행할 충분한 데이터 양을 확보해야 한다고 상상해보세요. 데이터를 어딘가에서 추출해야 하는데, 예를 들어 스크래핑하거나 오픈 소스 데이터가 어떻게 도움이 될 수 있는지 분석해야 합니다(추출).\n\n데이터는 다양한 형식으로 제공될 수 있습니다. CSV 파일 몇 개, JSON 파일, 그리고 몇 개의 txt 파일을 모았을 수도 있습니다. 따라서 데이터를 균일하게 변환해야 합니다.\n\n마지막으로, 데이터 과학자들이 쉽게 사용할 수 있도록 데이터를 쉽게 활용할 수 있어야 합니다. 예를 들어 다운로드하기 쉽도록 시스템에 업로드할 수 있어야 합니다(예: Hugging Face, AWS).\n\n![이미지](/assets/img/2024-06-19-MLOpsDataValidationwithPyTest_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 ETL 최상의 실천 방법에 대해 읽을 수 있습니다.\n\n## 무엇이 잘못될 수 있을까요?\n\n이제 데이터가 수집되는 방법을 알게 되었으니, 데이터 과학자가 데이터 유효성 검사를 다루어야 하는 이유와 방법에 대해 이해해 봅시다. 데이터셋에서 몇 가지 잘못된 부분이 생길 수 있습니다.\n\n- 주변 세계는 동적이며 변하기 때문에 데이터의 분포도 변합니다. 티셔츠 가격에 대해 예측하는 모델을 생각해보세요. XXL 사이즈는 아무도 구매하지 않았기 때문에 매우 낮은 가격으로 예측되었습니다. 그러나 세대가 지남에 따라 사람들이 점점 키가 커지기 때문에 미래에는 큰 사이즈에 더 중요성을 부여하는 모델을 다시 훈련해야 할 수도 있습니다.\n- 소스 데이터에 변경 사항이 있었지만 우리에게 알려지지 않았습니다. ETL 파이프라인을 담당하는 팀이 영화 평점 시스템을 변경하여 1에서 5점까지 범위로 구성된 시스템에서 10점까지의 시스템으로 전환했습니다.\n- ETL 중에 데이터 흡수에 버그가 있었을 수 있습니다. 실수가 있고 cm로 표현된 데이터에서 km로 표현된 데이터로 변경되었을 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 유효성 검사는 데이터를 분할한 후(splitting data into train and test)에 수행할 수도 있고 그 전에도 할 수 있습니다. 어디서 하는 것이 가장 좋은지 명확하지 않으며 두 가지 방법에 대한 장단점이 있습니다.\n\n## PyTest 소개\n\nPyTest는 다양한 종류의 테스트를 실행하는 데 널리 사용되는 파이썬 라이브러리입니다. 일반적으로 코드 베이스 내에 tests라는 폴더를 생성하고 여기에 실행하려는 여러 테스트 파일들을 수집합니다. 각 파일의 이름은 test_xx.py와 같이 지정됩니다. 따라서 tests 폴더 안에서 test_data.py 또는 test_model.py와 같은 파일들을 생성할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n테스트를 수행하는 데 주요한 Python 명령어는 assert입니다. 이 명령어는 특정 조건이 충족되었는지 확인하고, 그렇지 않으면 오류가 발생합니다. 조건 뒤에 문자열로 오류를 정의할 수 있습니다. 예시를 살펴보겠습니다.\n\nPyTest는 파일 내에서 감지된 모든 테스트 함수를 실행하고, 모든 단언문이 True를 반환하는지 확인합니다. 그렇지 않을 경우 터미널에 실패한 테스트를 표시합니다. 테스트 함수의 예시는 다음과 같습니다.\n\n여기서 첫 번째 문제가 발생합니다. 이전 함수에서 주어진 입력의 값은 무엇인가요? 테스트 단계에서 이러한 변수를 어떻게 지정할까요? 우리는 픽스처(fixtures)에서 도움을 받습니다!\n\n## PyTest의 픽스처(Fixtures)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은 경우 (위의 경우와 같이) 테스트에는 단언을 만들기 위한 입력 데이터(예: 데이터)가 필요합니다. 이 입력 데이터는 PyTest의 fixtures를 사용하여 제공할 수 있습니다. Fixtures를 사용하면 더 이상 할당할 필요 없이 테스트 내에서 사용될 변수를 선언할 수 있습니다. 그러나 fixtures를 정의하는 함수는 테스트 함수의 입력 변수와 동일한 이름을 가져야 합니다. 예를 살펴봅시다.\n\n위의 코드 블록에서 보는 것처럼, 우리는 data(함수 이름을 따름)라는 fixture를 구현하여 df라는 데이터프레임을 출력값으로 반환합니다.\n따라서 test_data_length 테스트에서는 입력 데이터가 fixture의 값을 취할 것이므로 df 데이터프레임과 일치할 것입니다.\n\nFixture의 범위를 지정할 수 있으므로, fixture가 파괴될 때를 결정할 수 있습니다. 예를 들어, 범위가 \"session\"인 경우 동일한 fixture가 전체 세션 동안 유지됩니다. 이것은 첫 번째 테스트가 데이터 값을 변경할 수 있고, 그 값을 두 번째 테스트로 전달할 수 있도록 합니다.\n\n대신 \"function\" 범위를 사용하면, 각 테스트가 fixture의 새로운 및 변경되지 않은 복사본을 입력으로 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPyTest 문서에서는 다양한 범위에 대해 읽을 수 있어요.\n\n머신러닝 데이터셋에 대한 테스트 작성은 전통적인 소프트웨어 엔지니어링에 대한 테스트 작성보다 복잡할 수 있어요. 전통적인 소프트웨어에서는 각 기능에 대해 기대 출력이 있기 때문에 테스트가 기대한 것과 다른 결과를 반환하면 명백히 오류가 있다고 볼 수 있어요.\n\n반면, 데이터셋에서는 무엇을 기대해야 하는지 확신할 수 없어요. 예를 들어 데이터셋에서 기능 \"키\"의 평균이 1.70cm일 것으로 가정해봅시다. 하지만 테스트 결과 평균이 \"1.75\"라고 나타나면 어떻게 해야 할까요? 오류가 있는 걸까요? 아니면 정말로 키가 큰 사람들의 데이터를 추가해서 평균을 높인 것일까요?\n\n그래서 데이터셋에서 할 수 있는 몇 가지 간단한 결정론적 테스트부터 시작해보고, 확정적이지 않은 테스트에 대해도 살펴보겠어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 결정론적 테스트\n\n결정론적 테스트 작성은 매우 간단합니다. 데이터셋에서 무엇이 결정론적인가요? 예를 들어, 열의 수가 정확하게 X여야 하거나 행의 수가 N 이상이어야 충분한 데이터가 있는 것과 같은 경우입니다.\n\n범주형 변수의 경우, 값이 특정 범위 내에 있는지 확인할 수 있습니다. 예를 들어, \"색상\" 특성이 [빨강, 초록, 파랑] 값만을 가질 수 있는지 확인할 수 있습니다.\n\n이러한 유형의 테스트를 위한 예제 파일을 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 코드에서는 다음과 같은 함수들을 찾을 수 있습니다:\n\n- data: 이 함수에서는 데이터프레임을 포함하는 변수를 노출하는 fixture가 있습니다.\n- test_column_presence_and_type: 이 함수에서는 [age, salary, name, genre] 네 개의 열이 데이터셋에 존재하고 올바른 유형인지 확인합니다.\n- test_class_names: 이 함수는 장르 값이 알려진 값들 중에 있는지 확인합니다. 예상치 않은 값들을 찾지 않도록 보장합니다.\n- test_column_ranges: 여기서는 숫자 변수들이 특정 범위에 있는지 확인합니다. 예를 들어, 나이는 절대 음수가 될 수 없습니다!\n\n## 확률론적 테스트\n\n확률론적 테스트에서 우리가 하고 싶은 것은 불확실성을 고려하여 값들을 측정하는 것입니다. 불확실성에 대해 이야기할 때 확률과 통계가 관련되며, 우리는 여기서도 그들을 활용할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 작업 중인 데이터셋의 값을 평가하기 위해 이전 버전과 비교하는 것이 일반적인 관행입니다.\n\n데이터셋에서 수행할 수 있는 몇 가지 예시 확인 사항은 다음과 같습니다:\n\n- 이상값의 존재 확인\n- 하나 이상의 열 값 분포 확인\n- 하나 이상의 열 또는 모든 열과 목표 열(예측 대상) 사이의 상관 관계 확인\n- 다양한 열의 평균 및 표준 편차 확인\n\n이미 언급한 바와 같이, 결정론적 테스트에서는 통계가 사용되며 일반적으로 과거 데이터가 예시로 취해져 현재 데이터와 비교됩니다. 따라서 가설 검정이 어떻게 작동하는지 이해하고, 이러한 비교를 위해 어떻게 사용할 수 있는지 이해하는 것이 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글에서는 가설 검정의 기본 개념을 간단히 살펴볼 예정이에요. 더 깊이 알고 싶으시다면 이 영상을 보시는 걸 추천해요:\n\n가설 검정을 다룰 때는 항상 대립 가설에 대한 귀무 가설을 검정하게 되어요.\n\n- 귀무 가설 (H_0): 과학 커뮤니티에서 널리 받아들여지는 가정이에요. 저희의 경우, 데이터에 관한 가정일 수 있어요.\n- 대립 가설 (H_a): 저의 새로운 가설로 받아들여지길 원하는 것으로, 귀무 가설과 반대되는 가설이에요. 제가 새로운 가설을 받아들여지게 하려면 제 가설을 확인하는 데이터를 제시해야 하죠. 이렇게 하면 새로운 가설이 옳다는 것을 모두를 설득하는 것이 더 쉬워져요.\n\n대표적인 예시는:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 영가설 (H_0): 두 개의 샘플은 동일한 평균을 가진 정규 분포에서 나온 것이다.\n- 대립가설: 두 개의 샘플은 서로 다른 평균을 가진 정규 분포에서 나왔다.\n\n가정에 따라 사용할 수 있는 다양한 통계 검정 방법이 있습니다. 각 통계 검정은 가정과 관련이 있습니다. 따라서 올바른 검정을 선택하는 것이 매우 중요합니다. 올바른 통계 검정을 선택하는 데 도움이 될 수 있는 적절한 논문을 알려드리겠습니다.\n\n이 예제에서는 t-검정을 사용할 것입니다.\n\n우리가 해야 할 일은, 샘플을 시작으로 알려진 공식을 사용하여 테스트 통계량이라는 값을 계산하는 것입니다. 테스트 통계량으로부터 곡선 아래 영역에 해당하는 p-값이라는 다른 값을 계산합니다 (나중에 자세히 살펴보겠습니다). p-값이 미리 선택한 임계값 (알파)보다 크면 우리는 영가설을 기각할 수 없으며 영가설은 여전히 진실이 유지됩니다. 그 대신에 p-값이 작으면 영가설을 기각할 수 있고 새로운 (대립) 가설을 주장할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당연히 이 거절에 대한 신뢰도는 미리 선택한 임계값에 의해 결정됩니다. 일반적인 임계값은 0.1, 0.5 및 0.001입니다. 이 값이 작을수록 더 확신이 있습니다.\n\n![이미지](/assets/img/2024-06-19-MLOpsDataValidationwithPyTest_2.png)\n\n아직 완전히 이해가 안 된다면 걱정하지 마세요, 이 전체 설명은 몇 줄의 Python 코드로 번역됩니다! t-테스트에 대한 Scipy 함수는 직접 검정 통계값과 p-값을 반환합니다. 우리가 해야 할 일은 알파 값을 선택하고 결정만 내리는 것뿐입니다.\n\n기계 학습에서는 참조 데이터 세트를 보유하고 이를 새로 얻은 데이터 세트와 비교하여 데이터 분포가 동일한지 이해하는 것이 최적일 것입니다. 유감스럽게도 우리가 사용 가능한 데이터 세트가 그리 많지 않기 때문에 보통 테스트 데이터 세트를 훈련 데이터 세트와 비교하는 것이 일반적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자주 수행하는 테스트 중 하나는 두 기능이 동일한 확률 분포에서 나왔는지 확인하는 것입니다. 물론, 테스트 데이터 세트에서 사용하는 열이 교육 데이터와 동일한 분포를 가져야 합니다. 그렇지 않으면 모델이 학습한 패턴은 테스트에서 완전히 쓸모 없게 될 것입니다!\n\n이를 위해 Kolmogorov-Smirnov 테스트라는 테스트를 사용할 수 있습니다. 이 테스트는 scipy 라이브러리에서도 제공됩니다.\n\n이 시점에서 PyTest로 이러한 확인을 구현할 수 있어야 합니다.\n\n사실 데이터 세트의 다른 열에 대해 여러 가설 검정을 실행할 때, 선택한 알파에 본페로니 교정이 필요합니다. 이에 대해 다음 기사에서 살펴볼 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n이 기사에서는 데이터 입력 파이프라인의 주요 구성 요소인 ETL에 대해 이야기했습니다. ETL은 추출(extraction), 변환(transformation), 로드(load)의 약어입니다. 또한 데이터 과학자가 작업 중인 데이터를 유효성 검사하는 중요성에 대해 이야기했습니다. 이 유효성 검사는 우리가 사전에 예측한 예상 출력을 알고 있는 결정론적 테스트로 수행되거나, 우리의 가정을 통계적 테스트로 확인할 수 있는 비결정론적 테스트로 이루어집니다. 이러한 테스트는 모두 모든 데이터 과학자에게 매우 중요한 도구인 PyTets를 사용하여 실행되며, 이는 코드를 깨끗하게 유지하고 코드 내의 오류를 최소화하는 데 도움이 됩니다.\n\n이 기사가 마음에 든다면 Medium에서 제 팔로우 하세요! 😁\n\n💼 Linkedin ️| 🐦 X (Twitter) | 💻 Website","ogImage":{"url":"/assets/img/2024-06-19-MLOpsDataValidationwithPyTest_0.png"},"coverImage":"/assets/img/2024-06-19-MLOpsDataValidationwithPyTest_0.png","tag":["Tech"],"readingTime":8},{"title":"콜모고로프-아놀드 네트워크 혹평인가, 딥 러닝 혁명인가","description":"","date":"2024-06-19 20:01","slug":"2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution","content":"\n\n## 더 좋은 해석 가능성, 작은 네트워크 크기 및 학습 가능한 활성화 함수가 MLPs를 무너뜨리게 될까요?\n\n# 주요 내용 (기사 개요)\n\nSubstack 그룹 채팅, LinkedIn 등에서 생각을 공유해 준 모든 분들께 감사드립니다. 거기서 우리가 가지는 대화들을 모두 사랑하고, 앞으로도 계속 많은 대화를 이어갈 예정입니다.\n\nKolmogorov–Arnold Networks 및 그들이 과학적 기능을 모델링하는 데 특히 유리할 수 있는 잠재력에 대해 많은 이야기가 되었습니다. 본 기사에서는 KANs 및 그들이 새로운 세대의 딥 러닝에서의 타당성에 대해 탐구할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![KANs 이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_0.png)\n\n먼저, KANs와 그들을 가능하게 하는 이론에 대해 간단히 살펴보겠습니다.\n\n콜모고로프-아놀드 표현 정리: 모든 무서운 방정식과 정의를 건너뛰고 간단한 설명으로 해결합시다. KART는 여러 입력을 가진 연속 함수는 단일 입력 (사인이나 제곱과 같은)의 간단한 함수들을 결합하고 더하는 것으로 생성할 수 있다는 것을 말합니다. 예를 들어, 다중 변수 함수 f(x,y)= x*y는 ( (x + y)² — (x² +y²) ) / 2로 표현할 수 있습니다. 이는 덧셈, 뺄셈 및 제곱 연산(단일 입력 함수)만 사용합니다. 실제 KART는 뺄셈을 덧셈으로 다시 구성하는 것이 포함되지만 여기서는 이를 간단하게 유지하겠습니다.\n\nKANs (콜모고로프-아놀드 네트워크) - 기존의 MLP (다층 퍼셉트론)과 달리 고정된 노드 활성화 함수를 갖는 MLPs와 달리, KANs는 가장자리에 학습 가능한 활성화 함수를 사용하여 선형 가중치를 비선형 가중치로 대체합니다. 이는 KANs를 보다 정확하고 해석 가능하게 만들어주며, 특히 학문적 응용 및 일상생활에서 자주 발견되는 희소한 합성 구조와 관련이 있는 함수에 유용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_1.png)\n\n와우, 구성 요소 구조가 뭔지 궁금하셨나요? 제처럼 gawaar(무지한 사람)이라면 \"희소한 구성 요소 구조\"라는 말에 혼란스러울 수 있습니다. 그럼 이건 뭐길래요, 그렇게 중요한가요? 간단히 말하자면- 특정 함수가 몇 개의 간단한 함수로 구성되어 있고 각 함수가 입력 변수 중 일부에만 의존할 때 그 함수는 희소한 구성 요소 구조를 갖고 있습니다. 예를 들어, 함수 f(x, y, z) = sin(x) * exp(y) + z는 구성 요소가 희소합니다.\n\n- 세 가지 간단한 함수인 sin(x), exp(y), z로 이루어져 있습니다.\n- 각 간단한 함수는 하나의 입력 변수(x, y, z 중 하나)에만 의존합니다.\n\n비교적으로, f(x, y, z) = x² * y³ + sin(x + y + z)와 같은 함수는 덜 희소하다고 볼 수 있습니다. 왜냐하면 더 복잡한 연산(x² * y³와 같은)이 필요하며 sin 함수에서 모든 세 가지 입력 변수를 결합해야하기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수학적으로 엄격한 정의를 원하는 경우, \"딥러닝의 기초: 계산 가능한 함수의 복합 희박성\"에서 발견한 정의는 다음과 같습니다.\n\n그들의 중요성을 요약하면, 저자들은 과학과 현실에서 만나는 대부분의 함수가 더 단순한 구조를 갖고 있어서, 그것들을 모델링하기 위해 MLP에 대안으로 KAN이 매력적이라고 언급합니다. 아래 표는 이 주장을 뒷받침하는 것으로 보입니다.\n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_2.png)\n\nKAN과 스플라인- KAN의 또 다른 흥미로운 특성은 스플라인의 활용입니다. 우리의 목적을 위해, 스플라인은 곡선에 맞게 구부러질 수 있는 유연한 자 등처럼 생각해야 합니다. 이들은 부드럽게 연결된 여러 다항 조각들로 이루어져 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](https://miro.medium.com/v2/resize:fit:576/0*9JUhBPlynwlgU86F.gif)\n\n스플라인을 활성화 함수로 사용함으로써 KAN은 입력 변수 간의 복잡한 관계를 학습할 수 있으면서 해석 가능성과 지역적인 제어를 유지할 수 있습니다. 뒤에 나오는 기사에서는 스플라인에 대해 더 자세히 설명하겠습니다.\n\nKAN에는 장단점이 있습니다. 먼저, 세 가지 주목할 만한 단점이 있습니다.\n- 연구 부족 - 새로운 아이디어인만큼 이해하지 못하는 것은 용서될 수 있지만, 새로운 아이디어를 살펴볼 때는 항상 이를 기억하는 것이 중요합니다. AI 연구 분야에서 천재아이디어에서 비실용적인 물건이 된 사례가 흔한 일입니다. 우리가 알기론 KAN에는 특정한 시점을 넘어서 발전하기 어렵게 하는 근본적인 장애물이 있을 수도 있습니다.\n- 시장 적합성 - 현재는 트랜스포머와 NN에 특화된 하드웨어가 만들어지고 있습니다. 이러한 개발은 KAN에 대한 강력한 선택 편향을 만들 수 있으며, 결국 그들의 수용을 방해할 수 있습니다 (이 훌륭한 의견을 제공해준 Patrick McGuinness에게 감사드립니다).\n- 훈련 속도가 느림 - KAN 훈련은 NN보다 10배 느립니다. 작업 중이신 내용에 따라 이는 그들을 부정할 수도 있고 큰 문제가 되지 않을 수도 있습니다. 수용/연구가 더 많아짐에 따라 해결되기도 할 것입니다 (혹시 여러분 중 누가 이 문제를 해결하게 된다면 좋겠네요). 하지만 현재로서는 더 많은 주류 방향으로의 수용을 막을 것이며 이는 규모 측면에서 주도되고 있는 곳에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nKANs은 여러 가지 장점을 가지고 있습니다:\n\n- 향상된 정확도: 다양한 작업에 대해 MLP보다 적은 매개변수로 더 낮은 RMSE 손실을 달성할 수 있습니다. 저자들은 몇 가지 놀라운 결과를 보여주며 Deepmind를 이기는 것을 포함합니다-\n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_3.png)\n\n- 유리한 스케일링 법칙: 더 빠른 신경 스케일링 법칙을 나타내며, 모델 크기가 커짐에 따라 성능 향상이 더 큽니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_4.png)\n\n- Catastrophic forgetting을 완화: 일반 MLPs는 이전에 학습한 내용을 새로운 내용으로 덮어쓰기 때문에 NNs가 이전 입력을 '잊어버리는' 문제를 발생시킵니다. KANs는 지속적인 학습을 용이하게 하는 지역 가용성을 활용합니다. 제가 Spines에 대해 언급할 때 이 기계적 측면에 대해 자세히 이야기하겠습니다. tl;dr 섹션에서- 1년 전 대화에서 일반 NNs의 주요 제한 사항으로 이를 지적한 첫 번째 사람으로 Dr. Bill Lambos에게 특별한 감사를 전하고 싶습니다. 그의 예측과 계산 신경과학자이자 생물학자(최초의 AI 전문가)로서의 통찰력은 여러 차례 옳았습니다.\n\n![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_5.png)\n\n- 설명 가능성- KANs는 더 설명 가능하며, 특정 분야에 매우 유리합니다. 저희 그룹 채팅 참가자 중 한 명이 그룹 채팅에서 다음과 같이 말했습니다-\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 상호작용성 - 사람들이 KAN과 조작하여 다양한 결과를 달성할 수 있는 근본적인 수준에서 상호작용할 수 있습니다. 네트워크 내에서 도메인 전문 지식을 입력하는이 사용법은 나에게 매우 유망해 보이며, 더 많은 사람들이 그에 대해 이야기하지 않는 것에 놀랍습니다. 제 반응이 과하게 반응인가요? 당신의 생각을 듣고 싶어요\n\n![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_6.png)\n\n이러한 장점들은 미래 탐사에 매력적인 전망을 제공합니다.\n\n이 기사의 나머지 부분은 KAN이 작동하는 속성을 조사할 것입니다. 기대하는 대로, 논의할 많은 포인트가 있으며, 때로는 이 기사가 약간 압도적일 수 있습니다. 이 분해 방법을 다루기 위해 우리는 적어도 느림을 준다는 구태의 없는 말을 사용할 것입니다. 우리는 현재의 신경망 기반 아키텍처에 대한 심층 학습의 이론적 기초와 그 파급 효과로 시작합시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 다양한 컨설팅 및 자문 서비스를 제공합니다. 함께 일할 수 있는 방법을 탐색하고 싶다면 여기 있는 내 소셜 미디어를 통해 저에게 연락하거나 이 이메일에 답장해주세요.\n\n15만 명 이상의 기술 리더와 함께하고 AI의 가장 중요한 아이디어에 대한 통찰을 무료 뉴스레터인 \"AI Made Simple\"을 통해 받아보세요.\n\n# 이론적 기반\n\n## A. 유니버설 근사 정리와 딥 러닝\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nUniversal Approximation Theorem (UAT)는 시그모이드 활성화 함수를 사용하는 뉴럴 네트워크가 한정된 수의 뉴런을 가진 단일 은닉층을 포함하여, 임의의 정확도로 제한된 세트의 연속 함수를 근사화할 수 있다고 말합니다. 우리가 해야 할 일은 비선형 함수의 다양한 조합을 계속해서 쌓아가는 것 뿐입니다. 그렇게 함으로써 우리가 원하는 함수의 근사치를 얻을 수 있습니다. (ML 모델에 데이터를 제공할 때, 우리는 모든 특징과 그들의 타겟과의 관계를 모델링하는 함수를 원합니다.)\n\n![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_7.png)\n\n나중에 다른 비선형성으로 확장되었으며, 이것이 왜 큰 신경망이 잘 작동하는지에 중요한 역할을 합니다. 큰 신경 네트워크 → 더 좋은 복잡한 비선형 관계를 모델링하기 위한 능력 → 연속 함수에 대한 더 가까운 근사값.\n\n계속하기 전에, 많은 사람들이 종종 간과하는 중요한 점이 있습니다. 모든 데이터셋/도메인이 연속 함수로 모델링될 수 있는 것은 아니라는 것입니다. 예를 들어, 더욱 울퉁불퉁한 의사결정 경계를 가진 특정 데이터셋이 있습니다. 이러한 경우, 신경망은 보다 매끄러운 의사결정 경계를 선호하므로 랜덤 포레스트와 같은 트리 기반 알고리즘이 더 나은 선택일 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 논문의 부록에서 저자들은 위 시각화에 관한 다음 문장을 제시했습니다-\n\n이제 이산적인 결정 공간을 다루는 매우 중요한 사용 사례도 있습니다. 여기서 신경망과 그래디언트 기반 방법은 대단히 Yamcha'ed 됩니다. 정부 검열에 대항하기 위한 제네바 프로젝트가 좋은 예입니다. 인공지능은 정부 검열을 회피하기 위해 창의적인 방법으로 4가지 기능을 결합하려고합니다. 이에 대해 자세히 알아 보려면 저희의 기사인 \"AI를 사용하여 검열에 맞서는 방법\" (이 프로젝트에 대한 깊은 살펴 보기) 또는 \"인터넷을 다시 제어하는 방법\" (인터넷의 기관적 조작의 영향을 줄이기 위해 적용할 수있는 몇 가지 AI 기술에 대한 살펴 보기)를 읽으실 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것이 중요한 이유는 신경망 및 그 구현에 대한 토론에서 종종 간과되는 세세한 부분이기 때문에 강조하고 있습니다. KAN(콜모고로프-아놀드 Representation Theorem)이 더 일반화되면서 비슷한 일이 발생할 수 있다는 걱정이 듭니다. 이 중요한 포인트를 언급했으니, KAN의 이야기로 넘어가겠습니다.\n\n## B. 콜모고로프-아놀드 Representation Theorem 이해하기\n\n콜모고로프-아놀드 Representation Theorem(KAT)은 바운드 영역에서의 임의의 연속 다변수 함수를 연속 단변수 함수의 유한 조합과 덧셈 연산으로 표현할 수 있다는 것을 제시합니다. 이 정리는 원칙적으로 고차원 함수를 학습하는 것을 단변수 함수의 다항수만 학습하는 것으로 줄일 수 있다는 것을 시사합니다. (비록 이러한 1D 함수 중 일부는 매끄럽지 않거나 프랙탈이거나 학습할 수 없을 수도 있습니다.)\n\n\u003cimg src=\"/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_10.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최근 연구에 따르면, 과학 응용 프로그램에서 많이 사용되는 다변량 함수들은 매끄럽고 희소한 구조를 가지며, 효과적인 실제 구현을 가능하게 합니다. 이것이 왜 KAN이 우리 과학계 사람들 사이에서 인기가 높아 보이는지 알겠죠. 이 부분은 이해하셨으니, 이제 KART의 매우 흥미로운 함의로 넘어가 봅시다.\n\n## C. KAN은 차원의 저주에 강하게 대응할 수 있을 것입니다\n\n차원의 저주란 입력 차원이 증가함에 따라 주어진 정확도를 달성하기 위해 필요한 데이터 포인트 또는 모델 파라미터의 급격한 증가를 가리킵니다. 이 문제는 고차원에서 데이터 포인트가 서로 점점 더 희소하고 멀어지기 때문에 발생하며, 기저 관계를 캡처하기 위해 더 많은 데이터와 모델 복잡성이 필요해집니다.\n\nKAN은 단변량 함수에 의존하고 합성 구조를 활용할 수 있기 때문에 차원의 저주를 완화할 수 있는 잠재력이 있습니다. 이것은 정말 멋지지만, 저는 현재 더 큰 규모와 더 많은 다양성이 통합된 상황에서 어떻게 잘 유지될지에 대해 의심하고 있습니다. 그럼에도 불구하고, 그들이 어떻게 진전될지에 대해 조심스럽게 낙관적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기반이 마련되었으니 이제 실제 KAN 구조와 그 작동을 이루는 다른 부분들에 대해 이야기해 보겠습니다.\n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_11.png)\n\n# 콜모고로프-아놀드 네트워크(KANs): 심층적인 탐구\n\n## A. KANs의 아키텍처\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n페이퍼의 2.2절에는 많은 정의와 표기법이 나와 있어서, 한 걸음씩 세밀히 설명해보겠습니다. 먼저, 일부 배경 정보-\n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_12.png)\n\n가장 중요한 부분은 마지막 강조 부분입니다- 우리는 더 넓고 깊은 KANs가 필요합니다 (smh, 이제는 인공지능 모델을 몸매 비하하는 걸 믿을 수가 없네). 그렇다면 우리가 KANs를 어떻게 강화할 수 있을까요? 임의로 큰 신경망의 비밀은 내일이 없는 것처럼 레이어를 쌓을 수 있는 능력에 있습니다. 그저 마음대로 레이어를 쌓아올리기가 KANs로는 문제인 이유는, 우리가 레이어에 대응하는 유사한 개념이 부족하기 때문입니다. 그래서, KAN 레이어에 대한 정의를 찾아보고, 이것을 확장하여 더 큰 chonky bois를 얻어보겠습니다.\n\n우리는 KAN 레이어를 다음과 같이 정의할 수 있습니다-\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_13.png)\n\n더 깊은 KAN을 만들어봅시다.\n\nKAN의 형태는 배열로 표시됩니다: [n0, n1, …, nL], 여기서 ni는 i번째 레이어의 노드 수를 의미합니다. 차원이 n0인 입력 벡터 x0가 주어지면, L개 레이어로 구성된 KAN 네트워크는 출력을 다음과 같이 계산합니다:\n\nKAN(x) = (ΦL−1 ◦ ΦL−2 ◦ … ◦ Φ1 ◦ Φ0)x. 이는 출력이 입력 레이어부터 시작하여 각 레이어의 활성화 함수를 순차적으로 적용하여 얻어진다는 의미입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같이 다시 작성할 수도 있습니다.\n\n![Image 1](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_14.png)\n\n우리의 작업은 미분 가능하기 때문에 backprop으로 KAN을 학습할 수 있습니다.\n\n![Image 2](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_15.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nKAN 레이어를 최적화하는 중요한 몇 가지 팁이 있어요.\n\n잔차 활성화 함수. 우리는 잔차 연결과 비슷한 기저 함수 𝑏⁢(𝑥)(residual connections)를 포함하여 활성화 함수 𝜙⁢(𝑥)가 기저 함수 𝑏⁢(𝑥)과 스플라인 함수의 합으로 정의됩니다:\n\n𝜙⁢(𝑥)=𝑤⁢(𝑏⁢(𝑥)+spline⁢(𝑥)).(2.10)\n\n우리는 다음과 같이 설정합니다- 𝑏⁢(𝑥)=silu⁢(𝑥)=𝑥/(1+𝑒^−𝑥)(2.11)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적으로, spline⁢(𝑥)는 B-스플라인의 선형 조합으로 매개화됩니다. 이 조합은 다음과 같습니다:\n\n![B-spline formula](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_16.png)\n\n초기화: \"각 활성화 함수는 spline⁢(𝑥)≈0으로 초기화됩니다. 𝑤는 Xavier 초기화에 따라 초기화되며, 이 초기화는 MLP의 선형 레이어를 초기화하는 데 사용됩니다.\"\n\n스플라인 격자의 업데이트: \"입력 활성화에 따라 각 격자를 업데이트하여, 활성화 값이 훈련 중 고정된 영역을 벗어나는 문제를 해결합니다.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nKAN이 기본적으로 더 복잡하기 때문에, 동일 크기의 MLP보다 매우 많은 매개변수를 가지게 됩니다. 간단히 말하자면, 다음과 같은 네트워크를 가정해 봅시다.\n1) 깊이가 L인,\n2) 너비가 n0 = n1 = · · · = nL = N 인 층,\n3) 각 spline이 주로 k(일반적으로 k = 3)차이며 G 구간(G + 1개의 그리드 포인트)에서 \n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_17.png)\n\n하지만, KAN은 보통 MLP보다 훨씬 작은 𝑁이 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 매개변수를 줄임으로써,\n- 더 나은 일반화를 달성합니다\n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_18.png)\n\n- 해석 가능성을 용이하게 함- 더 작은 네트워크는 읽고 확인하기 쉽습니다.\n\n지금까지 오셨으면, 확실히 KANs에서 많이 보이는 스플라인을 알아차릴 수 있었을 것입니다. 그리고 당신은 틀리지 않았습니다- 스플라인은 KANs의 매우 중요한 부분입니다. 그래서 다음으로 그것들에 대해 논의할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## B. 스플라인 매개 변수화 및 KANs\n\n여러 점을 통과하는 매끄러운 곡선을 그리려고 상상해보세요. 컴퓨터에게 원하는 대로 곡선을 그리도록 하는 방법은 무엇일까요? 여기서 B-스플라인이 등장합니다. 이들은 본질적으로 \"매듭(knots)\"이라고 불리는 점 세트 위에 정의된 부드럽고 조각으로 나뉘는 다항식 곡선입니다.\n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_19.png)\n\n이러한 매듭은 곡선을 세그먼트로 나누고, 각 세그먼트 내에서 B-스플라인은 특정 차수의 다항식입니다. 매력은 이러한 세그먼트들이 매끄럽게 연결되어 원하는 수준의 부드러움을 가진 연속적인 곡선을 만든다는 데 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nB-스플라인은 매우 멋지기 때문에 그들의 유연성에서 Camavinga의 스타일을 띠고 있습니다. 매우 부드러운 곡선을 원한다면 더 높은 차수의 B-스플라인을 사용해야 합니다. 곡선의 모양에 대한 유연성과 제어를 더 원한다면 매듭의 위치를 조절할 수 있습니다. 이러한 이유로 B-스플라인은 다양한 함수를 표현하는 데 놀라운 유연성을 제공합니다. 그래서 다음 번에 \"문화인\" 여러분들 중 한 분이 선호하는 애니메이션, AI 또는 심지어 포토샵으로 가공한 모델에 대한 갈망이 생길 때, B 스플라인의 신에게 감사하는 것을 잊지 마세요.\n\nB-스플라인은 KANs에서 중요한 역할을 합니다. 왜냐하면 엣지에서 배우는 활성화 함수를 표현하는 강력하고 해석 가능한 방식을 제공하기 때문입니다. ReLU나 시그모이드와 같은 고정된 활성화 함수 대신 KANs는 이러한 활성화 함수를 근사하기 위해 B-스플라인을 사용합니다. 이것은 여러 가지 이점을 제공합니다:\n\n- 정확성: B-스플라인은 고정성이 높은 복잡한 비선형 함수를 높은 정확도로 근사할 수 있습니다.\n- 지역 제어: B-스플라인의 계수를 변경하면 조정된 매듭 부근에서만 곡선의 모양에 영향을 줍니다. 이는 전체 네트워크 구조에 영향을 미치지 않고 활성화 함수의 동작을 세세하게 제어할 수 있게 합니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/0*OhuCnxJme6EnYIWX.gif)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 해석 가능성: B-스플라인은 구간 다항식으로 구성되어 있기 때문에 블랙박스 신경망보다 시각적으로 직관적이며 이해하기 쉽습니다. 이는 학습된 활성화 함수가 더 해석 가능하며 과학적 발견을 용이하게 합니다. \"우리는 KAN이 매듭 이론(4.3절)과 응축물질 물리의 상 전이 경계(4.4절) 모두를 (재)발견할 수 있다는 것을 입증합니다. KAN은 정확성(마지막 절)과 해석 가능성(이 절)으로 인해 AI+과학의 기초 모델이 될 수 있습니다.\"\n\nSplines를 최상의 성능으로 작동하기 위해서는 그들을 지원하는 것이 필요합니다. 이것이 Grid Extension 기술이 필요한 곳입니다.\n\n## C. Splines 개선을 위한 Grid Extension\n\nGrid Extension 기술은 KAN의 중요한 구성 요소로, 스플라인에 내재된 제한된 해상도를 해결합니다. 스플라인은 유한한 점 그리드 상에 정의되어 있으므로 정확도는 그리드의 밀도에 따라 달라집니다. Grid Extension 기술은 모델이 기반이 되는 함수의 점점 미세한 세부사항들을 학습할 수 있도록 훈련 중에 그리드 밀도를 증가시킴으로써 이를 해결합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_20.png)\n\n이 프로세스는 기존의 더 거친 스플라인에 새로운, 더 세밀한 스플라인을 맞추는 것을 포함합니다. 이를 통해 모델이 데이터 분포의 변화에 적응하고 이전에 배운 지식을 버리지 않고 정확성을 향상시킬 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_21.png)\n\nKANs에 대한 해석 가능성이 얼마나 중요한지 고려할 때, 가능한 한 우리의 KANs가 해석 가능하도록 보장할 수 있는 방법에 대해 이야기해 보겠습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## D. 해석 가능성을 위한 KAN 단순화\n\nKAN의 기본 아키텍처는 해석 가능성을 촉진하지만 활성화 함수의 많은 수로 모델을 이해하기 어렵게 만들 수 있습니다. 이를 해결하기 위해 여러 가지 단순화 기술이 사용됩니다:\n\n- 희소화 규제 (L1 및 엔트로피): 희소화 규제는 활성화 함수의 크기를 벌칙으로 삼아 희소한 표현을 촉진하며, 중요한 관계만을 나타내는 일부 활성화 함수만 값이 큰 방향으로 유도합니다. 이는 모델이 학습한 가장 중요한 관계를 식별하는 데 도움이 됩니다.\n- 가지치기 기술: 가지치기 기술은 중요도 점수에 기초하여 네트워크에서 노드를 자동으로 제거하여 아키텍처를 더 단순화하고 매개변수 수를 줄입니다.\n- 심볼화: 심볼화는 사용자가 활성화 함수를 삼각, 지수 또는 로그 함수와 같은 기호적 표현으로 변환할 수 있게 합니다. 이 단계는 활성화 함수의 숫자적 표현을 보다 쉽게 해석 가능한 기호적 공식으로 변환하기 위해 인간의 전문 지식을 활용합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_22.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 해석가능성의 주요 이점은 사람들(사용자, 전문가 등)이 프로세스/훈련 중간에서 AI를 결정으로 이끌 수 있도록 하는 것입니다. 이 상호작용은 AI 솔루션을 구축하는 가장 간과된 측면 중 하나이며, 대부분의 팀이 이를 구축하는 데 아무것도 하지 않거나 거의 하지 않습니다. 그러나 제대로 하면 게임 체인저가 될 수 있습니다. 그러니 KAN과 그들의 상호작용을 어떻게 만들 수 있는지에 대해 이야기해보겠습니다.\n\n## E. 대화형 KAN\n\nKAN의 주요 이점 중 하나는 그들의 고유한 상호작용성입니다. 시각화 도구와 상징적 조작 기능을 활용하여 사용자는 학습된 표현을 개량하고 근본적인 관계에 대한 보다 깊은 통찰력을 얻기 위해 모델과 협업할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 대화식 방식을 통해 사용자는 모델의 학습 과정을 안내할 수 있습니다.\n\n- 활성화 함수 시각화: 사용자는 활성화 함수를 시각화하여 잠재적인 상징적 표현을 식별하고 모델의 의사 결정 과정을 이해할 수 있습니다.\n- 활성화 함수 수동 설정: 사용자는 도메인 지식을 기반으로 특정 활성화 함수를 상징적 공식에 수동으로 맞출 수 있어 모델의 해석가능성과 정확성을 향상시킬 수 있습니다.\n- 네트워크 반복적으로 개선: 사용자는 모델 구조, 하이퍼파라미터 및 정규화 전략을 반복적으로 조정하여 정확성과 상호 운용성 사이의 균형을 최적화할 수 있습니다.\n\n모든 이러한 것들이 함께 결합하여 우리에게 KANs를 제공합니다- 결함이 있고 재미있으며 극도로 흥미로운 것입니다. 저에게는 KANs가 MLP의 가장 근본적인 문제 중 일부를 진정으로 다루는 신선한 시도를 제공한다고 생각됩니다. 그래서 이 논문은 모든 인정을 받아야 한다고 생각합니다. KANs가 쓸모 없거나 너무 특정한 제약이 있다고 해도- 우리는 KANs를 더 자세히 연구함으로써 많은 통찰을 얻을 것이라고 생각합니다.\n\n그리고 그로써 코끼리 번식이 해결되었다고 생각해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 이 기사가 마음에 드셨고 공유하고 싶으시다면 아래 가이드라인을 참조해 주세요.\n\n이것으로 이 기사를 마치겠습니다. 여러분의 시간을 감사히 여깁니다. 언제나처럼, 저와 함께 일하길 희망하시거나 제 다른 작품을 확인해 보고 싶다면, 제 링크는 이 이메일/게시물의 맨 끝부분에 있을 거예요. 그리고 만약 이 글에서 가치를 발견했다면, 더 많은 사람들과 공유해 주시면 감사하겠습니다. 여러분과 같은 입소문 추천이 제 성장을 돕는데 큰 도움이 됩니다.\n\n![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_24.png)\n\n저는 정보를 제공하고 유용하며, 부당한 영향으로부터 독립된 작품을 만드는 데 많은 노력을 기울였습니다. 제 글을 지원하고 싶으시다면, 이 뉴스레터의 유로 구독자가 되는 것을 고려해 주세요. 이를 통해 더 많은 노력을 기울여 쓰기/연구를 할 수 있고, 더 많은 사람들에게 도달하며, 제 치명적인 초콜릿 우유 중독을 지원할 수 있습니다. 매주 10만 명이 넘는 독자들에게 인공지능 연구와 엔지니어링의 가장 중요한 아이디어들을 대중화하는 데 도와주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n초콜릿 우유를 사주실래요?\n\n부탁드립니다~\n\nPS- 우리는 \"자신의 능력에 맞게 지불\"하는 모델을 따릅니다. 자세한 내용 및 여러분에게 적합한 방법을 찾는 데 도움이 될 포스트를 확인해보세요.\n\n나는 주기적으로 미니 업데이트를 Microblogging 사이트인 Twitter(https://twitter.com/Machine01776819), Threads(https://www.threads.net/@iseethings404), TikTok(https://www.tiktok.com/@devansh_ai_made_simple)에서 공유하고 있어요. 제 학습 내용을 계속 알고 싶다면 팔로우해주세요.\n\n# 언제든지 연락주세요~\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 링크를 사용하여 다른 콘텐츠를 확인하거나 튜터링에 대해 더 알아보거나 프로젝트에 대해 연락하거나 인사를 전하실 수 있습니다.\n\n기술, AI 및 기계 학습에 대한 작은 단편들은 여기에서 확인하세요.\n\nAI 뉴스레터 - [artificialintelligencemadesimple.substack.com](https://artificialintelligencemadesimple.substack.com/)\n\n제 할머니가 좋아하는 기술 뉴스레터 - [codinginterviewsmadesimple.substack.com](https://codinginterviewsmadesimple.substack.com/)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내 다른 기사들도 확인해보세요. : [https://rb.gy/zn1aiu](https://rb.gy/zn1aiu)\n\n내 유튜브 채널: [https://rb.gy/88iwdd](https://rb.gy/88iwdd)\n\nLinkedIn에서 연락해요. 함께 소통해요: [https://rb.gy/m5ok2y](https://rb.gy/m5ok2y)\n\n내 인스타그램: [https://rb.gy/gmvuy9](https://rb.gy/gmvuy9)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 트위터: [https://twitter.com/Machine01776819](https://twitter.com/Machine01776819)","ogImage":{"url":"/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_0.png"},"coverImage":"/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_0.png","tag":["Tech"],"readingTime":16},{"title":"OpenAI Assistant API와 Streamlit을 사용하여 도우미 만들기","description":"","date":"2024-06-19 19:58","slug":"2024-06-19-CreatinganAssistantwithOpenAIAssistantAPIandStreamlit","content":"\n\n## 단계별 가이드\n\n![이미지](https://miro.medium.com/v2/resize:fit:1200/1*bX5eqE7EUmnwxWuqjZDzIQ.gif)\n\n# OpenAI Assistant API\n\n최근 OpenAI가 새로운 기능을 소개했습니다. 이들은 Assistant API와 같이 에이전트와 같은 아키텍처를 보여줍니다. OpenAI에 따르면:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 발전은 희망적이지만, 아직 LangChain을 따라가지 못합니다. LangChain은 자연어 입력을 처리하고 문맥 기반 액션을 실행하는 더 유연한 LLM을 활용하여 에이전트 형태의 시스템을 만들 수 있습니다.\n\n하지만, 이것은 시작에 불과합니다.\n\n높은 수준에서 Assistant API와 상호 작용하는 것은 루프로 상상할 수 있습니다:\n\n- 사용자 입력을 받으면 LLM이 호출되어 응답을 제공할지 또는 특정 조치를 취할지를 결정합니다.\n- LLM의 결정이 쿼리에 대한 답변으로 충분하다면 루프가 종료됩니다.\n- 만약 행동이 새로운 관찰로 이어진다면, 이 관찰은 프롬프트에 포함되고 LLM이 다시 호출됩니다.\n- 그런 다음 루프가 다시 시작됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-CreatinganAssistantwithOpenAIAssistantAPIandStreamlit_0.png)\n\n안타깝게도 발표된 장점에도 불구하고, API에 대한 문서는 특히 사용자 정의 함수 호출 및 Streamlit와 같은 프레임워크를 사용한 앱 구축과 관련하여 제대로 작성되지 않았다고 생각했습니다.\n\n이 블로그 포스트에서는 OpenAI Assistant API 및 사용자 정의 함수 호출을 사용하여 Streamlit 인터페이스와 함께 AI 어시스턴트를 구축하는 방법을 안내해드리겠습니다. 이를 통해 Assistant API를 효과적으로 사용하고자 하는 분들께 도움이 될 것입니다.\n\n# 사용 사례: 세금 계산 어시스턴트\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 블로그 포스트에서는 간단한 예제를 보여드리겠습니다: 주어진 수익에 기반한 세금을 계산할 수 있는 AI 어시스턴트입니다. Langchain 사용자들은 \"세금 계산\" 도구를 가진 에이전트를 생성함으로써 이를 쉽게 이해할 수 있습니다.\n\n이 도구에는 필요한 계산 단계와 LLM이 수익 또는 세금과 관련된 질문이 있을 때 도구를 호출해야 하는지를 알려주는 잘 설계된 프롬프트가 포함될 것입니다.\n\n그러나 이 프로세스는 OpenAI 어시스턴트 API와 정확히 동일하지는 않습니다. OpenAI의 문서에 따르면 코드 해석기와 파일 검색 도구는 직접적으로 간단한 방식으로 사용할 수 있지만, 사용자 정의 도구는 약간 다른 방식으로 접근해야 합니다.\n\n```js\nassistant = client.beta.assistants.create(\n  name=\"데이터 시각화자\",\n  description=\"당신은 아름다운 데이터 시각화를 만드는 데 뛰어나십니다. .csv 파일에 있는 데이터를 분석하며 트렌드를 이해하고 해당 트렌드에 관련된 데이터 시각화를 제시합니다. 또한 관찰된 트렌드에 대한 간단한 텍스트 요약을 공유합니다.\",\n  model=\"gpt-4o\",\n  tools=[{\"type\": \"code_interpreter\"}],\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 단계씩 세부 내용을 살펴보겠습니다. 다음을 목표로 합니다:\n\n- 주어진 수익에 기반한 세금을 계산하는 함수 정의하기.\n- 이 함수를 사용하는 도구 개발하기.\n- 이 도구에 액세스하고 세금 계산이 필요할 때 호출할 수 있는 어시스턴트 만들기.\n\n# 어시스턴트 통합을 위한 세금 계산 함수\n\n다음 단락에서 설명하는 세금 계산 도구는 이 글에서 논의된 API를 사용하는 방법을 보여주기 위한 예시로 설계되었음을 유념해 주세요. 실제 세금 계산에 사용해서는 안 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음과 같이 조각별 함수를 고려해 보세요. 이 함수는 주어진 매출에 대한 세금 값을 반환합니다. 입력이 간단한 구문 분석을 위해 문자열로 설정되어 있음을 유의하세요:\n\n```js\ndef calculate_tax(revenue: str):\n    try:\n        revenue = float(revenue)\n    except ValueError:\n        raise ValueError(\"매출은 숫자의 문자열 표현이어야 합니다.\")\n\n    if revenue \u003c= 10000:\n        tax = 0\n    elif revenue \u003c= 30000:\n        tax = 0.10 * (revenue - 10000)\n    elif revenue \u003c= 70000:\n        tax = 2000 + 0.20 * (revenue - 30000)\n    elif revenue \u003c= 150000:\n        tax = 10000 + 0.30 * (revenue - 70000)\n    else:\n        tax = 34000 + 0.40 * (revenue - 150000)\n\n    return tax\n```\n\n다음으로, 비서(assistant)를 정의합니다:\n\n```js\nfunction_tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"calculate_tax\",\n            \"description\": \"유로로 주어진 매출에 대한 세금을 가져옵니다.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"revenue\": {\n                        \"type\": \"string\",\n                        \"description\": \"유로로 연간 매출\"\n                    }\n                },\n                \"required\": [\"revenue\"]\n            }\n        }\n    }\n]\n\n# 비서(assistant) 정의\nassistant = client.beta.assistants.create(\n    name=\"Assistant\",\n    instructions=\"\",\n    tools=function_tools,\n    model=\"gpt-4o\",\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제, 주요한 포인트에 대해서 얘기해볼게요:\n\n어시스턴트가 \"calculate_tax\"가 호출될 때 어떻게 함수를 사용하는지 알고 계신가요? 이 부분은 OpenAI 어시스턴트에서 문서화가 잘 되어 있지 않아, 많은 사용자들이 처음 사용할 때 혼동을 겪을 수 있어요. 이를 해결하기 위해, 응담 스트림(response stream)에서 다양한 이벤트를 관리하기 위한 EventHandler를 정의해야 합니다. 특히 \"calculate_tax\" 도구가 호출될 때의 이벤트를 어떻게 처리하는지에 대해 명확히 알아둬야 해요.\n\n```js\n    def handle_requires_action(self, data, run_id):\n        tool_outputs = []\n\n        for tool in data.required_action.submit_tool_outputs.tool_calls:\n            if tool.function.name == \"calculate_tax\":\n                try:\n                    # 도구 매개변수에서 수익 추출\n                    revenue = ast.literal_eval(tool.function.arguments)[\"revenue\"]\n                    # 세금을 계산하는 calculate_tax 함수 호출\n                    tax_result = calculate_tax(revenue)\n                    # 필요한 형식에 맞게 도구 출력을 추가\n                    tool_outputs.append({\"tool_call_id\": tool.id, \"output\": f\"{tax_result}\"})\n                except ValueError as e:\n                    # 세금 계산 시 발생하는 모든 오류 처리\n                    tool_outputs.append({\"tool_call_id\": tool.id, \"error\": str(e)})\n        # 모든 도구 출력을 동시에 제출\n        self.submit_tool_outputs(tool_outputs)\n```\n\n위 코드는 다음과 같이 동작해요: 동작이 필요한 각 도구 호출에 대해:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- \"calculate_tax\" 함수 이름을 확인합니다.\n- 툴 매개변수에서 수익 값을 추출합니다.\n- 수익을 이용하여 calculate_tax 함수를 호출하여 세금을 계산합니다. (여기서 실제 상호작용이 이루어집니다.)\n- 모든 툴 호출을 처리한 후, 수집된 결과를 제출합니다.\n\n# 보조 인공지능과 대화하기\n\n다음은 OpenAI가 문서화한 표준 단계를 따라 보조 인공지능과 상호작용할 수 있습니다. 따라서 이 섹션에서는 많은 세부 정보를 제공하지 않겠습니다:\n\n- 스레드 생성: 이는 사용자와 보조 인공지능 간의 대화를 나타냅니다.\n- 사용자 메시지 추가: 이는 스레드에 추가되는 텍스트 및 파일을 포함할 수 있습니다.\n- 실행 생성: 보조 인공지능과 연관된 모델 및 도구를 활용하여 응답 생성합니다. 이 응답은 다시 스레드에 추가됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 코드 조각은 특정 사용 사례에서 어시스턴트를 실행하는 방법을 보여줍니다: 코드는 스레드 ID 및 어시스턴트 ID를 사용하는 특정 매개변수를 설정하여 어시스턴트와의 스트리밍 상호작용을 설정합니다. EventHandler 인스턴스는 스트림 중 이벤트를 관리합니다. stream.until_done() 메서드는 모든 상호작용이 완료될 때까지 스트림을 유지합니다. with 문은 스트림이 적절히 닫히도록 보장합니다.\n\n```js\n  with client.beta.threads.runs.stream(thread_id=st.session_state.thread_id,\n                                         assistant_id=assistant.id,\n                                         event_handler=EventHandler(),\n                                         temperature=0) as stream:\n        stream.until_done()\n```\n\n# Streamlit 인터페이스\n\n여기서 내 게시물을 마칠 수 있지만, Streamlit 포럼(예: 이 포스트)에서 사용자들이 터미널에서는 정상 작동하지만 인터페이스에서 스트리밍이 작동하지 않는다는 수많은 문의를 발견했습니다. 이것이 나로 하여금 더 깊이 파고들도록 유도했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스트리밍을 앱에 성공적으로 통합하려면, 앞서 언급한 EventHandler 클래스의 기능을 확장해야 합니다. 특히 텍스트 생성, 텍스트 델타 처리 및 텍스트 완료를 중점적으로 다루어야 합니다. 채팅 히스토리를 관리하면서 Streamlit 인터페이스에 텍스트를 표시하기 위해 필요한 세 가지 주요 단계는 다음과 같습니다:\n\n- 텍스트 생성 처리 (on_text_created): 어시스턴트의 각 응답마다 새로운 텍스트 상자를 초기화하고 표시하여 이전 작업의 상태를 반영하도록 UI를 업데이트합니다.\n- 텍스트 델타 처리 (on_text_delta): 어시스턴트가 텍스트를 생성할 때 현재 텍스트 상자를 동적으로 업데이트하여 전체 UI를 새로 고치지 않고도 점진적으로 변경할 수 있도록 합니다.\n- 텍스트 완료 처리 (on_text_done): 새로운 빈 텍스트 상자를 추가하여 각 상호작용 세그먼트를 완료하고, 다음 상호작용을 준비합니다. 또한, 대화 세그먼트를 chat_history에 기록합니다.\n\n예를 들어, 텍스트 델타를 관리하는 다음 코드 조각을 살펴봅시다:\n\n```python\ndef on_text_delta(self, delta: TextDelta, snapshot: Text):\n    \"\"\"\n    텍스트 델타가 생성될 때의 핸들러\n    \"\"\"\n    # 최신 텍스트 상자를 지웁니다.\n    st.session_state.text_boxes[-1].empty()\n    \n    # 새로운 텍스트가 있으면, 어시스턴트 텍스트 목록의 마지막 요소에 추가합니다.\n    if delta.value:\n        st.session_state.assistant_text[-1] += delta.value\n    \n    # 업데이트된 어시스턴트 텍스트를 최신 텍스트 상자에 다시 표시합니다.\n    st.session_state.text_boxes[-1].info(\"\".join(st.session_state[\"assistant_text\"][-1]))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 코드는 세 가지 주요 작업을 수행합니다:\n\n- 최신 텍스트 상자 지우기: 최신 텍스트 상자의 내용을 지워 새 입력을 준비합니다 (st.session_state.text_boxes[-1]).\n- 델타 값을 도우미 텍스트에 추가: 새 텍스트 (delta.value)가 있는 경우, 이를 st.session_state.assistant_text[-1]에 저장된 지속적인 도우미 텍스트에 추가합니다.\n- 업데이트된 도우미 텍스트 다시 표시: 지금까지 축적된 모든 도우미 텍스트의 내용을 반영하기 위해 최신 텍스트 상자의 내용을 업데이트합니다 (st.session_state[\"assistant_text\"][-1]).\n\n# 결론\n\n이 블로그 포스트에서는 OpenAI Assistant API와 Streamlit을 사용하여 세금을 계산할 수 있는 AI 도우미를 만드는 방법을 보여주었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 Assistant API의 능력을 강조하기 위해 이 간단한 프로젝트를 수행했어요. 문서가 다소 불명확하더라도, 목표는 모호한 부분을 명확하게 하고 Assistant API를 사용하고자 하는 분들에게 일부 지침을 제공하는 것이었습니다. 이 게시물이 도움이 되었으면 좋겠고, 이 강력한 도구로 더 많은 가능성을 탐험하도록 격려하길 바랍니다.\n\n공간 제약으로 인해 불필요한 코드 조각을 포함하지 않으려고 노력했어요. 그러나 필요한 경우, 제 Github 저장소를 방문하여 전체 구현 내용을 확인해주세요.","ogImage":{"url":"/assets/img/2024-06-19-CreatinganAssistantwithOpenAIAssistantAPIandStreamlit_0.png"},"coverImage":"/assets/img/2024-06-19-CreatinganAssistantwithOpenAIAssistantAPIandStreamlit_0.png","tag":["Tech"],"readingTime":8},{"title":"대규모 언어 모델Large Language Models, LLMs 이해하기","description":"","date":"2024-06-19 19:56","slug":"2024-06-19-UnderstandingLargeLanguageModelsLLMs","content":"\n\n![UnderstandingLargeLanguageModelsLLMs_0](/assets/img/2024-06-19-UnderstandingLargeLanguageModelsLLMs_0.png)\n\n대형 언어 모델(LLMs)은 현대 인공 지능의 중요한 요소로 자리 잡아, 기계가 인간 언어를 이해하고 생성하는 방식을 혁신하고 있습니다. 챗봇과 가상 비서부터 고급 연구 도구에 이르기까지, LLMs는 다양한 분야에서 혁신을 주도하고 있습니다. 이 글은 LLMs의 복잡성을 탐구하여 그 개발, 기술 기반, 응용 및 전망을 살펴봅니다.\n\n# 대형 언어 모델이란?\n\nLLMs는 자연어 텍스트를 처리하고 생성하기 위해 설계된 인공 지능 모델의 하위 집합입니다. 이러한 모델은 수억에서 수조에 이르는 많은 매개변수를 특징으로 하며, 이를 통해 인간과 유사한 텍스트를 높은 정확성과 일관성으로 생성하고 이해할 수 있습니다. LLMs에서의 \"대형\"이라는 용어는 이러한 모델을 훈련하기 위해 필요한 방대한 양의 데이터와 계산 능력을 가리킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# LLMs의 진화\n\nLLMs의 여정은 1960년대 ELIZA와 같은 초기 자연 언어 처리(NLP) 모델을 시작으로 했습니다. ELIZA는 간단한 패턴 매칭 기술을 사용했습니다.\n\n![이미지](/assets/img/2024-06-19-UnderstandingLargeLanguageModelsLLMs_1.png)\n\n1990년대에는 딥 러닝의 등장으로 패러다임이 전환되었습니다. 이 기계 학습 분야는 인간 뇌 구조를 모방한 인공 신경망을 활용하여 방대한 양의 데이터에서 학습합니다. 이로써 더 정교한 언어 모델의 개발이 가능해졌습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1997년에 핵심적인 개발이 이루어졌는데, Long Short-Term Memory (LSTM) 네트워크가 소개되었습니다. 텍스트와 같은 순차 데이터에 어려움을 겪는 전통적인 신경망과는 달리, LSTMs는 장기 의존성을 다룰 수 있어 문장 내 맥락을 파악할 수 있었습니다. 이로 인해 보다 큰 데이터셋에서 언어 모델을 훈련하고 언어의 미묘한 측면을 캡처할 수 있게 되었습니다.\n\n하지만, 신경망과 딥러닝이 2010년대에 도입되면서 중대한 발전이 이루어졌습니다.\n\nLLM 진화의 주요 이정표는 다음과 같습니다:\n- 단어 임베딩 (2013): Word2Vec과 같은 모델은 단어를 고차원 공간에서 연속적인 벡터로 표현하는 단어 임베딩 개념을 소개하여 의미론적 관계를 포착했습니다.\n- Sequence-to-Sequence 모델 (2014): Seq2Seq와 같은 모델의 발전은 특히 번역 작업에서 유용한 입력-출력 쌍 처리를 더 잘할 수 있게 해주었습니다.\n- Attention 메커니즘 (2017): Vaswani 등이 제안한 Transformer 모델은 어텐션 메커니즘을 통합하여 다양한 단어의 중요성을 가중치로 고려할 수 있는 NLP 혁명을 일으켰습니다.\n- Generative Pre-trained Transformers (GPT, 2018–2024): OpenAI의 GPT 시리즈는 GPT-4가 1.5조 개의 파라미터를 자랑하며 중대한 발전을 이루었습니다. 이러한 모델은 텍스트 생성, 번역 및 요약에서 놀라운 능력을 보여주었습니다.\n\n# 대형 언어 모델 예제\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 개발되어 사용 중인 유명한 LLM 중 일부를 살펴보겠습니다.\n\n- GPT: GPT의 전체 명칭은 Generative pre-trained Transformer입니다. Open AI가 개발한 이 모델은 Chat GPT(Open AI에서 출시)에서 GPT-4 모델을 사용하고 있으며 들어보셨을 것입니다.\n- BERT: BERT의 전체 명칭은 Bidirectional Encoder Representations from Transformers입니다. 구글이 개발한 이 큰 언어 모델은 자연어 처리와 관련된 다양한 작업에 주로 사용됩니다. 또한, 특정 텍스트에 대한 임베딩을 생성하거나 다른 모델을 훈련시키는 데 사용될 수도 있습니다.\n- RoBERTa: RoBERTa의 전체 명칭은 Robustly Optimized BERT Pretraining Approach입니다. 변형기 아키텍처의 성능을 향상시키기 위한 시도 중 하나로, RoBERTa는 Facebook AI Research에 의해 개발된 BERT 모델의 향상된 버전입니다.\n- BLOOM: 다양한 기관과 연구자들이 협력하여 개발한 최초의 다국어 언어 모델인 BLOOM은 GPT 아키텍처와 유사합니다.\n\n![이미지](/assets/img/2024-06-19-UnderstandingLargeLanguageModelsLLMs_2.png)\n\n# NLP와 LLM의 차이\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNLP은 자연 언어 처리의 알고리즘 개발으로 이루어진 인공 지능(인공지능) 분야입니다. NLP는 알고리즘 및 기술로 이루어진 LLM보다 더 넓은 범위의 분야입니다. NLP는 기계 학습 및 언어 데이터 분석이라는 두 가지 접근 방식을 가지고 있습니다. NLP의 응용 분야는 다음과 같습니다.\n\n- 자동화된 루틴 작업\n- 검색 기능 개선\n- 검색 엔진 최적화\n- 대규모 문서 분석 및 정리\n- 소셜 미디어 분석\n\n반면에, LLM은 대규모 언어 모델로, 인간과 유사한 텍스트에 더 특화되어 있으며, 콘텐츠 생성 및 맞춤형 추천을 제공합니다.\n\n# 대규모 언어 모델의 장점은 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대형 언어 모델(LLM)은 여러 이점을 가지고 있어 다양한 응용 분야에서 널리 사용되고 성공을 거두는데 이바지합니다:\n\n- LLM은 제로샷 학습을 수행할 수 있어 명시적으로 훈련받지 않은 작업에 대해 일반화할 수 있습니다. 이 능력은 추가적인 훈련 없이 새로운 응용분야나 시나리오에 대응할 수 있도록 해줍니다.\n- LLM은 방대한 양의 데이터를 효율적으로 처리할 수 있어 언어 번역이나 문서 요약과 같은 방대한 텍스트 말뭉치에 대한 심도 있는 이해가 필요한 작업에 적합합니다.\n- LLM은 특정 데이터셋이나 도메인에 대해 미세 조정이 가능하며, 특정 사용 사례나 산업에 계속적인 학습과 적응이 가능합니다.\n- LLM은 코드 생성부터 콘텐츠 생성까지 여러 언어 관련 작업을 자동화할 수 있어 프로젝트의 전략적이고 복잡한 측면을 위해 인력을 확보할 수 있습니다.\n\n# LLM은 어떻게 작동하나요?\n\nLLM은 신경망 아키텍처를 기반으로 하며, 특히 Transformer 아키텍처에 기반합니다. 이러한 모델의 주요 구성 요소는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 임베딩: 단어 또는 토큰을 의미와 관계를 담은 밀집 벡터로 변환합니다.\n- 셀프 어텐션 메커니즘: 이 메커니즘은 모델이 입력 텍스트의 관련 부분에 초점을 맞추도록 하며, 다양한 단어의 중요성을 동적으로 가중치를 부여합니다.\n- 피드 포워드 레이어: 이러한 레이어는 셀프 어텐션 메커니즘의 출력을 처리하여 의미 있는 표현을 생성합니다.\n- 대규모 말뭉치에서의 훈련: LLM(Large Language Models)은 방대한 양의 텍스트 데이터에서 훈련되며, 비감독 또는 준지도 학습을 통해 언어의 패턴, 구조 및 뉘앙스를 학습합니다.\n- 파인튜닝: 사전 훈련 이후 모델은 종종 특정 작업이나 데이터셋에 대해 파인튜닝되어 특정 응용 프로그램에서의 성능을 향상시킵니다.\n\nTransformer 아키텍처에 대한 자세한 내용은 여기에서 읽어보세요:\n\n# LLM의 응용\n\nLLM의 다양한 용도로 인해 다양한 분야에서 채택되고 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 자연어 이해 (NLU): LLMs는 감성 분석, 엔티티 인식, 언어 번역과 같은 작업에서 뛰어나며 인간 언어를 더 잘 이해할 수 있게 합니다.\n- 텍스트 생성: 창의적 글쓰기와 코드 생성부터 이메일과 보고서 작성까지, LLMs는 일관된 문맥적인 텍스트를 생성할 수 있습니다.\n- 대화형 에이전트: Siri, Alexa 및 고객 서비스의 챗봇과 같은 가상 비서들은 LLMs를 활용하여 자연스럽고 효과적인 상호작용을 제공합니다.\n- 내용 요약: LLMs는 대량의 정보를 간결한 요약으로 정리하여 정보 검색과 지식 관리를 도와줍니다.\n- 코드 생성: GitHub Copilot과 같은 도구는 LLMs를 활용하여 개발자들에게 코드 조각을 제안하고 함수를 완성하는 데 도움을 줍니다.\n\n# 대형 언어 모델 실험\n\n## 최신 모델과 함께 직접 체험하기\n\n대형 언어 모델의 능력을 직접 탐구하고 싶은 분들을 위해 몇 가지 접근 가능한 플랫폼과 모델이 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- OpenAI의 GPT-4: OpenAI는 GPT 모델의 여러 버전에 대한 액세스를 제공하며, 이 중 GPT-4는 사용자가 API를 통해 실험할 수 있습니다. 사용자는 OpenAI의 플랫폼(chat.openai.com 등)에 가입하여 응용 프로그램을 작성하거나 모델과 상호 작용하여 텍스트 생성, 질문에 답변 또는 코딩 능력을 이해할 수 있습니다.\n- Anthropics의 Claude 2: Anthropics는 안전과 유틸리티에 초점을 맞춘 Claude 2라는 흥미로운 모델을 제공합니다. 대화형 인공지능에 대한 독특한 관점을 제공하며 다양한 응용 프로그램을 위해 자사 플랫폼을 통해 액세스할 수 있습니다.\n- Hugging Face의 오픈 모델: 인기 있는 AI 모델 허깅페이스는 GPT와 BERT 모델의 변형을 포함한 다양한 오픈 소스 모델을 호스팅합니다. 열정가와 개발자는 이러한 모델을 텍스트 생성, 감성 분석 등 다양한 작업에 활용할 수 있습니다. 사용자 친화적인 인터페이스로 초보자도 손쉽게 실험을 시작할 수 있습니다.\n- Google Bard: Google은 자사의 LLM인 Bard로 경쟁에 뛰어들었습니다. 글 작성 언어 모델인 Bard는 작성 시점을 기준으로 한계적으로 공개되거나 테스트 중일 수 있지만, 대화형 인공지능 및 언어 이해 분야에서 중요한 역할을 약속합니다.\n\n# 도전과 윤리적 고려 사항\n\n그들의 능력에도 불구하고, LLM은 여러 가지 도전과 윤리적 고려 사항을 안고 있습니다:\n\n- 편향성과 공정성: LLM은 훈련 데이터에 존재하는 편향을 지속하거나 심지어 확대시킬 수 있어 공정하지 않거나 유해한 결과를 초래할 수 있습니다.\n- 잘못된 정보: LLM이 현실적인 텍스트를 생성할 수 있는 능력으로 인해 잘못된 정보의 퍼지와 딥 페이크의 생성에 대한 우려가 제기됩니다.\n- 자원 집약적: LLM의 훈련 및 배포는 상당한 계산 자원이 필요하므로 높은 에너지 소비와 환경 영향을 초래할 수 있습니다.\n- 데이터 프라이버시: LLM의 훈련에 사용되는 방대한 데이터셋은 종종 민감한 정보를 포함하고 있어 데이터 프라이버시와 보안 문제를 제기합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 향후 방향\n\nLLM의 미래는 희망적입니다. 현재의 한계를 해결하고 새로운 지평을 탐구하기 위해 계속된 연구에 집중되어 있습니다:\n\n- 효율성 향상: 연구자들은 LLM의 계산 부담을 줄이기 위해 더 효율적인 아키텍처와 교육 방법을 개발하고 있습니다.\n- 향상된 이해: 향후 모델은 더 깊은 언어 이해를 달성하고, 추론, 설명, 지식 생성 능력을 향상시키는 데 주력하고 있습니다.\n- 다중 모달 모델: 텍스트와 이미지, 오디오 등 다른 데이터 유형을 통합하여 시각적 질문 응답과 같은 복잡한 작업을 수행할 수 있는 포괄적인 AI 시스템을 만들고 있습니다.\n- 윤리적 AI: LLM이 책임 있게 사용되고 편향이 최소화되며 남용을 방지하기 위해 윤리적 가이드라인과 프레임워크 개발에 중점을 둡니다.\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대형 언어 모델은 인공 지능의 중요한 발전을 나타내며, 다양한 산업 분야에서 혁신적인 잠재력을 제공합니다. 연구가 진행됨에 따라 초점은 점차 더 효율적이고 윤리적이며 지적인 모델을 만들어 인간들과 자연스럽게 상호작용하며 의미 있는 방식으로 우리의 능력을 확장할 것입니다. LLM의 복잡성을 이해하는 것은 점점 더 디지털 세계에서 그들의 힘을 책임있고 효과적으로 활용하기 위해 중요합니다.","ogImage":{"url":"/assets/img/2024-06-19-UnderstandingLargeLanguageModelsLLMs_0.png"},"coverImage":"/assets/img/2024-06-19-UnderstandingLargeLanguageModelsLLMs_0.png","tag":["Tech"],"readingTime":7}],"page":"64","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"64"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>