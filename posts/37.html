<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/37" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/37" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="미래를 여는 열쇠 IoT와 홀로그램 기술의 최전선 이해하기" href="/post/2024-06-22-UnlockingtheFutureUnderstandingIoTandItsHolographicFrontier"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="미래를 여는 열쇠 IoT와 홀로그램 기술의 최전선 이해하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-UnlockingtheFutureUnderstandingIoTandItsHolographicFrontier_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="미래를 여는 열쇠 IoT와 홀로그램 기술의 최전선 이해하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">미래를 여는 열쇠 IoT와 홀로그램 기술의 최전선 이해하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">1<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="오픈 소스 맥박산소측정기, 직접 만들어보기" href="/post/2024-06-22-OpenSourcePulseOximeter"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="오픈 소스 맥박산소측정기, 직접 만들어보기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-OpenSourcePulseOximeter_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="오픈 소스 맥박산소측정기, 직접 만들어보기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">오픈 소스 맥박산소측정기, 직접 만들어보기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 프로젝트에서 벽에 부딪혔다면 이제 어떻게 해야 할까요" href="/post/2024-06-22-YouveHitaWallinYourDataProjectNowWhat"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 프로젝트에서 벽에 부딪혔다면 이제 어떻게 해야 할까요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-YouveHitaWallinYourDataProjectNowWhat_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 프로젝트에서 벽에 부딪혔다면 이제 어떻게 해야 할까요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 프로젝트에서 벽에 부딪혔다면 이제 어떻게 해야 할까요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="본 글에서는 Python, SQL, Power BI를 사용해 특정 소프트웨어 엔지니어링 분야의 국가별 연봉 및 평점 분석 프로젝트를 다루었습니다" href="/post/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="본 글에서는 Python, SQL, Power BI를 사용해 특정 소프트웨어 엔지니어링 분야의 국가별 연봉 및 평점 분석 프로젝트를 다루었습니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="본 글에서는 Python, SQL, Power BI를 사용해 특정 소프트웨어 엔지니어링 분야의 국가별 연봉 및 평점 분석 프로젝트를 다루었습니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">본 글에서는 Python, SQL, Power BI를 사용해 특정 소프트웨어 엔지니어링 분야의 국가별 연봉 및 평점 분석 프로젝트를 다루었습니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="SQL에서 중앙값 계산하는 5가지 방법" href="/post/2024-06-22-5WaysToCalculateMedianInSQL"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="SQL에서 중앙값 계산하는 5가지 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-5WaysToCalculateMedianInSQL_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="SQL에서 중앙값 계산하는 5가지 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">SQL에서 중앙값 계산하는 5가지 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 분석 효율성을 높이는 방법 esProc SPL 종합 가이드" href="/post/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 분석 효율성을 높이는 방법 esProc SPL 종합 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 분석 효율성을 높이는 방법 esProc SPL 종합 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 분석 효율성을 높이는 방법 esProc SPL 종합 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">43<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="SQL 윈도우 함수 탐구 ROW_NUMBER, RANK, DENSE_RANK 사용 방법" href="/post/2024-06-22-ExploringSQLWindowFunctionsROW_NUMBERRANKandDENSE_RANK"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="SQL 윈도우 함수 탐구 ROW_NUMBER, RANK, DENSE_RANK 사용 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-ExploringSQLWindowFunctionsROW_NUMBERRANKandDENSE_RANK_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="SQL 윈도우 함수 탐구 ROW_NUMBER, RANK, DENSE_RANK 사용 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">SQL 윈도우 함수 탐구 ROW_NUMBER, RANK, DENSE_RANK 사용 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="DuckDB 빅데이터 업계의 떠오르는 스타" href="/post/2024-06-22-DuckDBTheRisingStarintheBigDataLandscape"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="DuckDB 빅데이터 업계의 떠오르는 스타" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-DuckDBTheRisingStarintheBigDataLandscape_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="DuckDB 빅데이터 업계의 떠오르는 스타" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">DuckDB 빅데이터 업계의 떠오르는 스타</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 마법사를 위한 CASE WHEN 완벽 마스터 가이드 SQL 최종 안내서" href="/post/2024-06-22-TheUltimateGuidetoMasteringCASEWHENinSQLforDataWizards"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 마법사를 위한 CASE WHEN 완벽 마스터 가이드 SQL 최종 안내서" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-TheUltimateGuidetoMasteringCASEWHENinSQLforDataWizards_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 마법사를 위한 CASE WHEN 완벽 마스터 가이드 SQL 최종 안내서" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 마법사를 위한 CASE WHEN 완벽 마스터 가이드 SQL 최종 안내서</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="SQL에서 임시 테이블의 강력한 기능 탐구하기" href="/post/2024-06-22-ExploringthePowerofTemporaryTablesinSQL"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="SQL에서 임시 테이블의 강력한 기능 탐구하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-ExploringthePowerofTemporaryTablesinSQL_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="SQL에서 임시 테이블의 강력한 기능 탐구하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">SQL에서 임시 테이블의 강력한 기능 탐구하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/21">21</a><a class="link" href="/posts/22">22</a><a class="link" href="/posts/23">23</a><a class="link" href="/posts/24">24</a><a class="link" href="/posts/25">25</a><a class="link" href="/posts/26">26</a><a class="link" href="/posts/27">27</a><a class="link" href="/posts/28">28</a><a class="link" href="/posts/29">29</a><a class="link" href="/posts/30">30</a><a class="link" href="/posts/31">31</a><a class="link" href="/posts/32">32</a><a class="link" href="/posts/33">33</a><a class="link" href="/posts/34">34</a><a class="link" href="/posts/35">35</a><a class="link" href="/posts/36">36</a><a class="link posts_-active__YVJEi" href="/posts/37">37</a><a class="link" href="/posts/38">38</a><a class="link" href="/posts/39">39</a><a class="link" href="/posts/40">40</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"미래를 여는 열쇠 IoT와 홀로그램 기술의 최전선 이해하기","description":"","date":"2024-06-22 17:56","slug":"2024-06-22-UnlockingtheFutureUnderstandingIoTandItsHolographicFrontier","content":"\n\n\u003ctable\u003e 태그를 Markdown 형식으로 변경해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n의료 분야에서는 홀로그래픽 시뮬레이션이 정교한 수술을 가상 환경에서 연습할 수 있도록 함으로써 수술 기술과 의료 교육을 변화시킬 수 있습니다. 소매업에서 홀로그래픽 디스플레이를 활용하면 제품을 구매하기 전에 집 안에서 아이템을 볼 수 있어 쇼핑 경험을 향상시킬 수 있습니다. 홀로그래픽 IoT는 대화형 시뮬레이션과 가상 현장 여행을 제공함으로써 교육 혁신의 가능성을 제공할 수 있습니다.\n\nIoT와 홀로그래픽 기술이 함께 제공하는 매력적인 새로운 가능성이 있지만 단점도 있습니다. 기술적 도전, 개인정보 문제 및 윤리적 문제를 해결하려면 어려움이 있습니다. 그러나 창의적 사고와 적절한 안전 대책으로 이러한 장애물을 극복할 수 있어 더 통합되고 지능적이며 몰입적인 미래로의 문을 열 수 있습니다.\n\n요약하면, IoT와 홀로그래픽 기술의 결합은 우리의 디지털 환경에서 통신과 참여를 재정의합니다. 우리는 변형되는 시너지를 받아들이면서 상상력이 유일한 한계인 미래로 나아가는 여정을 떠납니다. IoT와 홀로그래픽 기술의 힘을 활용하여 보다 연결된 사회를 만들며 생산성을 증가시키고 삶의 질을 향상시킬 수 있습니다.\n\n이 글은 알로카 페레라가 작성했습니다.","ogImage":{"url":"/assets/img/2024-06-22-UnlockingtheFutureUnderstandingIoTandItsHolographicFrontier_0.png"},"coverImage":"/assets/img/2024-06-22-UnlockingtheFutureUnderstandingIoTandItsHolographicFrontier_0.png","tag":["Tech"],"readingTime":1},{"title":"오픈 소스 맥박산소측정기, 직접 만들어보기","description":"","date":"2024-06-22 17:55","slug":"2024-06-22-OpenSourcePulseOximeter","content":"\n\n이 프로젝트에서 사용된 항목\n\n전체 장치는 44mm x 30mm 퍼프 보드에 부착된 Arduino Nano를 중심으로 설계되었습니다. 먼저, 센서의 VIN, GND, SDA 및 SCL 핀에 전선을 납땜한 다음, 그 전선은 침대 부품 아래를 따라 Arduino Nano로 연결됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로, OLED에 대한 커넥터를 Nano에 연결한 다음 디스플레이에 연결합니다. 마지막으로, 전체 전자 어셈블리를 하우징에 밀어 넣고 몇 개의 3mm 나사로 고정합니다.\n\n![이미지](/assets/img/2024-06-22-OpenSourcePulseOximeter_2.png)\n\n전자기기가 삽입된 후에 OLED 화면을 상단 부분에 연결하고 나머지 샤시에 몇 개의 3mm 나사로 고정합니다. 뚜껑을 부드럽게 위아래로 움직여 움직임을 테스트할 수 있습니다.\n\n포함된 스케치는 사용자의 현재 심박수와 산소 포화도를 표시하기 위해 몇 가지 작업을 수행합니다. 업로드하려면 필요한 라이브러리를 설치하고 도구 메뉴에서 Arduino Nano를 보드 목록에서 선택한 다음 업로드를 클릭하면 됩니다. 스케치 자체는 OLED 및 MAX30102를 초기화하고 에러가 발생할 경우 보고합니다. 그런 다음 센서를 보정하기 위해 100개의 값을 읽고 이를 표시하기 시작합니다. 장치는 그런 다음 25개의 새 값들을 읽고 이들로 이동 평균을 계산하는 루프에 진입합니다. 마지막으로 값이 유효한지 확인하고 유효하다면 화면에 인쇄합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n펄스 옥시미터를 사용하려면 손가락 끝을 센서 위에 올려놓고 부드럽게 뚜껑을 닫으세요. 그런 다음 전원 공급원을 꽂고 데이터가 표시될 때까지 기다리기만 하면 됩니다.\n\n![이미지](/assets/img/2024-06-22-OpenSourcePulseOximeter_3.png)\n\n![이미지](/assets/img/2024-06-22-OpenSourcePulseOximeter_4.png)\n\n![이미지](/assets/img/2024-06-22-OpenSourcePulseOximeter_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 펄스 옥시미터 코드\n\nC/C++\n\n```js\n/*\n  하드웨어 연결 (Breakoutboard to Arduino):\n  -5V = 5V (3.3V 사용 가능)\n  -GND = GND\n  -SDA = A4 (또는 SDA)\n  -SCL = A5 (또는 SCL)\n  -INT = 연결하지 않음\n\n  MAX30105 브레이크아웃은 5V 또는 3.3V I2C 로직을 처리할 수 있습니다. 보드를 5V로 전원 공급하는 것을 권장하지만 3.3V에서도 작동합니다.\n*/\n\n#include \u003cWire.h\u003e\n#include \"MAX30105.h\"\n#include \"spo2_algorithm.h\"\n#include \"SSD1306Ascii.h\"\n#include \"SSD1306AsciiWire.h\"\n\nMAX30105 particleSensor;\nSSD1306AsciiWire oled;\n\n#define MAX_BRIGHTNESS 255\n\n#if defined(__AVR_ATmega328P__) || defined(__AVR_ATmega168__)\n// 아두이노 Uno에는 50개의 IR LED 데이터 및 빨간 LED 데이터를 32비트 형식으로 저장할 충분한 SRAM이 없습니다.\n// 이 문제를 해결하기 위해 샘플 데이터의 16비트 MSB가 잘립니다. 샘플이 16비트 데이터로 변환됩니다.\nuint16_t irBuffer[50]; // 적외선 LED 센서 데이터\nuint16_t redBuffer[50];  // 빨간색 LED 센서 데이터\n#else\nuint32_t irBuffer[50]; // 적외선 LED 센서 데이터\nuint32_t redBuffer[50];  // 빨간색 LED 센서 데이터\n#endif\n\nint32_t spo2; // SPO2 값\nint8_t validSPO2; // SPO2 계산이 유효한지를 나타내는 표시기\nint32_t heartRate; // 심박수 값\nint8_t validHeartRate; // 심박수 계산이 유효한지를 나타내는 표시기\n\nvoid setup()\n{\n  Serial.begin(115200); // 초당 115200비트로 시리얼 통신 초기화:\n\n  oled.begin(\u0026Adafruit128x64, 0x3C);\n  oled.setFont(Arial14);\n\n  // 센서 초기화\n  if (!particleSensor.begin(Wire, I2C_SPEED_FAST)) // 기본 I2C 포트, 400kHz 속도 사용\n  {\n    Serial.println(F(\"MAX30105를 찾을 수 없습니다. 배선/전원을 확인하세요.\"));\n    while (1);\n  }\n\n  particleSensor.setup(55, 4, 2, 200, 411, 4096); // 이러한 설정으로 센서 구성\n}\n\nvoid loop()\n{\n\n  // 처음 50개 샘플을 읽고 신호 범위를 결정합니다.\n  for (byte i = 0 ; i \u003c 50 ; i++)\n  {\n    while (particleSensor.available() == false) // 새 데이터가 있는지 확인\n      particleSensor.check(); // 새 데이터가 있는지 확인\n\n    redBuffer[i] = particleSensor.getRed();\n    irBuffer[i] = particleSensor.getIR();\n    particleSensor.nextSample(); // 이 샘플은 완료되었으므로 다음 샘플로 이동\n    Serial.print(F(\"빨강=\"));\n    Serial.print(redBuffer[i], DEC);\n    Serial.print(F(\", 적외선=\"));\n    Serial.println(irBuffer[i], DEC);\n  }\n\n  // 처음 50개 샘플(총 4초의 샘플) 후에 심박수 및 SpO2 계산\n  maxim_heart_rate_and_oxygen_saturation(irBuffer, 50, redBuffer, \u0026spo2, \u0026validSPO2, \u0026heartRate, \u0026validHeartRate);\n\n  // MAX30102에서 지속적으로 샘플을 채취합니다. 1초마다 심박수와 SpO2를 계산합니다.\n  while (1)\n  {\n    // 메모리에 처음 25개 세트의 샘플을 버리고 마지막 25개 세트의 샘플을 맨 위로 이동합니다.\n    for (byte i = 25; i \u003c 50; i++)\n    {\n      redBuffer[i - 25] = redBuffer[i];\n      irBuffer[i - 25] = irBuffer[i];\n    }\n\n    // 심박수를 계산하기 전에 25개 세트의 샘플을 채취합니다.\n    for (byte i = 25; i \u003c 50; i++)\n    {\n      while (particleSensor.available() == false) // 새 데이터가 있는지 확인\n        particleSensor.check(); // 새 데이터가 있는지 확인\n\n      redBuffer[i] = particleSensor.getRed();\n      irBuffer[i] = particleSensor.getIR();\n      particleSensor.nextSample(); // 이 샘플은 완료되었으므로 다음 샘플로 이동\n      Serial.print(F(\"빨강=\"));\n      Serial.print(redBuffer[i], DEC);\n      Serial.print(F(\", 적외선=\"));\n      Serial.print(irBuffer[i], DEC);\n\n      Serial.print(F(\", 심박수=\"));\n      Serial.print(heartRate, DEC);\n\n      Serial.print(F(\", 심박수유효=\"));\n      Serial.print(validHeartRate, DEC);\n\n      Serial.print(F(\", SPO2=\"));\n      Serial.print(spo2, DEC);\n\n      Serial.print(F(\", SPO2유효=\"));\n      Serial.println(validSPO2, DEC);\n      \n    }\n\n    // 25개의 새로운 샘플을 수집한 후 HR 및 SP02를 재계산합니다.\n    maxim_heart_rate_and_oxygen_saturation(irBuffer, 50, redBuffer, \u0026spo2, \u0026validSPO2, \u0026heartRate, \u0026validHeartRate);\n    printToScreen();\n  }\n}\n\nvoid printToScreen() {\n  oled.clear();\n  oled.setCursor(0,0);\n  if(validSPO2 \u0026\u0026 validHeartRate) {\n    oled.print(F(\"HR: \")); oled.println(heartRate, DEC);\n    oled.print(F(\"SPO2: \")); oled.println(spo2, DEC);\n  } else {\n    oled.print(F(\"유효하지 않습니다\"));\n  }\n}\n```","ogImage":{"url":"/assets/img/2024-06-22-OpenSourcePulseOximeter_0.png"},"coverImage":"/assets/img/2024-06-22-OpenSourcePulseOximeter_0.png","tag":["Tech"],"readingTime":5},{"title":"데이터 프로젝트에서 벽에 부딪혔다면 이제 어떻게 해야 할까요","description":"","date":"2024-06-22 17:53","slug":"2024-06-22-YouveHitaWallinYourDataProjectNowWhat","content":"\n\n![img](/assets/img/2024-06-22-YouveHitaWallinYourDataProjectNowWhat_0.png)\n\n우리 모두가 익숙한 시나리오를 경험했을 것입니다: SQL 쿼리를 개발하거나 데이터 파이프라인을 구축하거나 예측 모델을 작성 중이면서 최종 출력 데이터 집합이라고 생각되는 지점에서 코드를 실행하면 의도하지 않은 결과가 나오는 것이다. 레코드 수가 이상하게 반환된다거나, 필드가 올바르게 채워지지 않는다거나, 모델이 예상치 못한 결과를 생성할 수도 있습니다.\n\n어떤 문제가 발생했음을 알지만 코드에는 문제가 발생한 원인에 대한 오류 메시지나 명확한 표시가 없을 때 어떻게 문제 해결에 접근해야 할까요? \n\n지난 6년 이상의 다양한 분석 업무에서 얻은 경험으로 말씀드리면, 새로운 개발 문제를 해결하며 자주 벽에 부딪힐 것이라고 확신합니다. 그러나 이러한 장애물을 극복하고 더 강인한 전문가로 나아가기 위해 채택한 효과적인 전략이 있습니다. 다음 섹션에서는 내게 가장 성공적으로 입증된 몇 가지 기술을 공유하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## \"재부팅을 시도해 보셨나요?\"\n\n저는 컴퓨터를 껐다 켜는 것을 의미하는 게 아니라, 오히려 뇌를 재설정하는 것을 말하고 있어요. 다시 말해, 몇 분 동안 책상에서 멀어지는 걸 시도해 보세요.\n\n이게 얼마나 효과가 좋은지 믿을 수 없을 정도로 도움이 됐던 적이 많아요. 때로는 간단한 산책만으로도 문제에 대한 해결책이나 새로운 접근 방법이 스스로 떠오르곤 해요. 너무 오래 동안 같은 문제를 바라보고 있으면, 문제의 한 부분에만 집중하다가 더 큰 그림을 잊고 만다는 점이 쉽게 발생할 수 있어요.\n\n예를 들어, 파이썬 함수를 사용해서 데이터 포인트를 올바르게 출력하려는 문제를 겪고 있을 수도 있어요. 하지만 더 중요한 것은, 함수가 왜 필요한지, 어떤 종속성이 있는지, 그리고 함께 상호작용해야 하는 다른 데이터들이 무엇인지에 대한 초점을 잃었을 수 있다는 거죠. 때때로 이러한 방법이 문제를 해결하는 길로 다시 돌아가는 가장 좋은 방법일 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 가정을 다시 바라보기\n\n제 개인적인 경험으로 보면, 제 경력에서 부딪힌 대부분의 장애물은 데이터나 코드에 대해 잘못된 가정을 한 것에서 비롯되었습니다.\n\n어느 한 역할에서, 퇴사한 다른 데이터 분석가가 만든 대시보드 지표를 복제하는 업무를 맡았습니다. 이전 SQL과 해당 지표를 생성하는 데 사용된 Power BI 워크북에 접근할 수 있었습니다. 나는 비슷한 방식으로 비즈니스 로직을 복제했지만, 결과물을 출력했을 때 약 5% 정도 다르게 나타났습니다. 문법의 정확성, 필드 호출, 및 조인에서 의도하지 않은 데이터 필터링을 철저히 검토하기 위해 상당한 시간을 보냈지만, 이러한 접근 방식들 중 어느 것도 작동하지 않았습니다. 더 철저한 검토를 거친 후, 우리 둘 다 다른 원본 테이블에서 필드명이 customer_active로 필터링하는 것으로 밝혀졌으며, 이 두 필드는 백엔드에서 서로 다르게 계산되었습니다.\n\n위의 장애물의 근본 원인은 내가 호출한 필드에 대해 가정을 한 점이었습니다. 내가 생각했던 대로 데이터를 쿼리했지만, 잘못된 데이터에 대한 내 가정이 결국 우리 둘 간의 지표와의 5% 차이의 원인이었습니다. 되돌아보면, 차이가 발생하는 곳을 이해하기 위해 소스 테이블로 역추적하는 것이 더 나은 방법이었을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적으로 이 단계에 있을 때 다음을 고려해보세요:\n\n- 작업 중인 데이터의 기본 정보는 무엇인가요? 즉, 각 행이 나타내는 내용은 무엇인가요? 예를 들어 매일 스냅샷되는 고객 테이블과 작업 중이라면 — 테이블의 기본 정보는 고객과 날짜입니다. 의도한 테이블의 기본 정보가 실제로 출력되는 것인지 확인하세요.\n- 검토 중인 코드가 전체 그림을 나타내는가요? 데이터가 상류 어딘가에 잘못된 부분이 있는지 이해해야 할 필요가 있나요? 파이프라인 또는 데이터 수집 문제가 있을 수 있나요?\n- 비즈니스와 관련된 x 테이블의 문맥을 정확히 이해하고 있나요? 관련 데이터가 시스템과 프로세스 관점에서 어떻게 유래되는지 이해하는 것이 중요합니다. 명확하지 않다면 전문가와 협업하세요.\n\n## 코드의 개별 세그먼트를 분리하세요\n\nSQL 작업 시, 나의 즐겨찾는 방법은 코드 세그먼트를 공통 테이블 표현으로 분리하고 각각을 개별적으로 쿼리하는 것입니다. 이 형식은 처음에 DBT를 사용할 때 발견했고, 그들의 문서에서 더 많은 정보를 확인할 수 있습니다. 다음과 같은 예시를 살펴보세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nWITH customer_acq AS (\n  SELECT\n  *\n  , CASE\n      WHEN first_order_date IS NOT NULL\n        THEN first_order_date\n      ELSE salesforce_date_created\n    END AS acquisition_date\n  FROM customers\n)\n\n, main AS (\n  SELECT\n  *\n  FROM customer_acq\n  WHERE acquisition_date \u003e= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)\n)\n\nSELECT * FROM main\n```\n\n마지막 줄에 있는 SELECT * FROM main 부분이 불필요해 보일 수 있지만, 이 구조를 통해 코드 세그먼트의 문제 해결을 쉽게 할 수 있음을 보증할 수 있어요. 여기서 마지막 줄에서 우리는 각 CTE의 출력을 쿼리하여 각각에서 무엇이 잘못되고 있는지 확인할 수 있어요. 이 기술은 특히 쿼리의 출력이 의도치 않게 0개의 레코드를 반환할 때 매우 효과적으로 증명되었어요. 여기서 \"main\"이라는 마지막 단어를 다른 CTE 이름으로 변경함으로써 모든 레코드를 필터링하는 원인 CTE를 결정할 수 있어요.\n\nCTE가 쿼리 최적화에 부담을 준다는 것은 일반적인 오해입니다. 실제로 (적어도 눈에 띄는 차이를 만들 만큼만이라도) 현대 클라우드 데이터 웨어하우스 공급업체와 함께 작업하는 경우 CTE가 쿼리 최적화에 미칠 영향은 상당하지 않아요.\n\n## 하나의 예시 레코드 격리하기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기술을 사용하여 원본 시스템의 한 예제 레코드, 원본 데이터베이스 테이블 및 코드의 출력에서 하나의 예제 레코드를 살펴보고자 합니다. 이를 통해 A.) 원본 시스템에서 데이터베이스로 데이터가 이동하는 과정에 어떤 문제가 있는지와 B.) 적용한 로직이 이 한 레코드에 어떤 영향을 미치는지를 결정할 수 있습니다. 하나를 분리함으로써 우리는 개별 입력이 개발한 코드를 통해 어떻게 흐르는지 더 명확하게 이해할 수 있습니다. 이 단계에서 다음 사항을 평가할 수 있습니다:\n\n- 이 레코드가 의도한 대로 코드를 통해 흐르고 있는가?\n- 그렇지 않다면, 이 레코드가 제대로 작동하는 다른 레코드와 어떤 점이 다른가요?\n\n위에서 언급한 기술과 결합하여, 코드의 개별 부분에서 개별 레코드를 분리할 수도 있습니다. 예를 들어, 위에서 정의한 CTE를 사용하여 코드의 각 세그먼트를 통해 특정 레코드가 어떻게 출력되는지 확인해보세요. 여기에서는 코드 중 어떤 섹션이 레코드를 잘못 조작하고 있는지를 정확히 식별할 수 있어야 합니다.\n\n## 더 이상 추측하지 마세요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문제 명세서에 대해 포괄적인 시각을 가져야 합니다. 대시보드가 올바르게 채워지지 않는 상황에 처해 있을 때, 소규모 수정(필터 조건 변경 등)을 하고 대시보드를 새로고침하여 최선을 바라는 습관에 빠지기 쉬울 수 있습니다. 이것은 마치 주사위를 던지는 것과 다를바 없으며, 여러 계층의 문제를 효과적으로 해결하는 방법이 아닙니다.\n\n문제 명세서를 해체하고 다시 구성해보세요. 다음과 같은 질문을 스스로에게 던져보세요:\n\n- 어떤 비즈니스 질문에 답하려고 하는가요?\n- 이에 대한 답변을 위해 어떤 데이터가 사용 가능한가요? 각 관련 데이터 테이블이 무엇을 나타내는지 알아야 합니다. 최소 가치 제품을 제공하기 위해 필요한 데이터와 사용 가능한 데이터에 대한 명확한 가이드라인을 가져야 합니다.\n- 소스 데이터 위에 BI 레이어를 구축하기 위해 필요한 변확 필요한 변환은 무엇인가요?\n- 사용 중인 데이터 소스 또는 데이터 파이프라인의 미묘한 점은 무엇인가요?\n- 데이터 품질 — 널 값이 어디에 존재하나요? 데이터의 지연에 대한 우려사항은 없나요?\n\n이러한 질문에 대답할 수 없다면, 도움을 요청하거나 데이터에 대한 팀원 또는 전문가와 소통하는 것도 괜찮습니다. 데이터에 대한 친숙함은 가치 있는 해결책을 제공하는 데 중요하므로 필요하다면 비즈니스 컨텍스트를 더 잘 이해하기 위해 필요한 시간을 투자하는 것이 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n시니어리티 레벨에 관계없이, 발전하는 과정에서 어려움을 겪게 될 것입니다. 때때로 이미 알고 있는 지식을 잊고 문제에 새로운 시각으로 접근해야 합니다. 예상치 못한 결과나 어려움에 부딪힐 때는 체계적인 방법을 채택하고 차분한 마음가짐을 유지하는 것이 중요합니다.\n\n어떤 데이터 프로젝트에서도 어려움에 부딪히는 것은 실패의 징후가 아니라 성장의 기회입니다. 이러한 전략을 적용하고 프로젝트에 대해 종합적인 시각을 유지함으로써 문제 해결 능력을 향상시키고 더욱 신뢰할 수 있고 효율적인 결과를 이룰 수 있습니다. 그러니 다음 번 데이터 프로젝트에서 어려움을 겪을 때는 이 기술들을 포용하고 시각을 재설정하며, 각 어려움이 더 강력한 데이터 전문가로 발전하는 한 걸음 다가가는 과정임을 기억해주세요.","ogImage":{"url":"/assets/img/2024-06-22-YouveHitaWallinYourDataProjectNowWhat_0.png"},"coverImage":"/assets/img/2024-06-22-YouveHitaWallinYourDataProjectNowWhat_0.png","tag":["Tech"],"readingTime":5},{"title":"본 글에서는 Python, SQL, Power BI를 사용해 특정 소프트웨어 엔지니어링 분야의 국가별 연봉 및 평점 분석 프로젝트를 다루었습니다","description":"","date":"2024-06-22 17:52","slug":"2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry","content":"\n\n현재 정보 시대에서 데이터 분석의 중요성은 이전보다 더 커졌어요. 이 글에서는 Python, SQL 및 Power BI를 사용하여 진행한 데이터 분석 프로젝트에 대해 자세히 설명할 거예요. Kaggle에서 얻은 데이터셋은 특정 소프트웨어 직종의 채용 공고, 채용자 회사의 평가 비율, 그리고 직원들에게 제공되는 최저, 평균 및 최고 연봉 범위 등의 데이터를 포함하고 있어요. 이 프로젝트는 기존 데이터를 정리하고 구성하는 데 초점을 맞추며 필요한 데이터를 추출하고 이를 시각화하여 통계 비율을 생성합니다. 이 프로젝트의 주요 목표는 나만의 로드맵을 개요화하여 완전한 데이터 엔지니어링 프로젝트를 만드는 것이에요.\n\n이 프로젝트에서:\n\n- Python을 사용한 데이터 정리와 구성,\n- Oracle SQL을 사용한 데이터베이스 관리 및 쿼리, 그리고 일부 데이터 업데이트,\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 시각화 및 의미 있는 보고서 작성에 Power BI를 사용합니다.\n\n프로젝트에서 사용된 각 기술이 어떻게 통합되었는지와 이 프로세스의 끝에서 얻은 결과를 단계별로 공유할 것입니다. 이 글이 데이터 분석 프로젝트에 참여하거나 관심을 가지고 있는 모든 분들에게 유용하길 바랍니다.\n\n프로젝트 로드맵 및 사용된 기술\n\n이 프로젝트에서는 구체적인 목표를 달성하기 위한 단계별 로드맵을 개요로 작성했습니다. 이제, 프로젝트에서 사용한 Python, SQL 및 Power BI 기술을 자세히 설명하고 이들을 어떻게 통합했는지 시작하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_0.png)\n\n프로젝트에서 사용된 모든 코드는 GitHub 링크(https://github.com/Revealis)를 통해 확인할 수 있습니다.\n\n1. Python: 데이터 정리 및 분석\n\n프로젝트의 첫 단계에서는 Python을 사용하여 데이터세트를 정리하고 예비 분석을 수행했습니다. Python의 강력한 라이브러리 중 하나인 Pandas를 사용하여 데이터를 정리하고 예비 분석을 수행하며 오류를 해결했습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. SQL: 데이터베이스 관리 및 쿼리\n\n프로젝트의 두 번째 단계에서는 SQL을 사용하여 데이터를 저장하고 빠르게 액세스했습니다. SQL의 강력한 쿼리 기능 덕분에 대량 데이터 세트에서 복잡한 쿼리를 수행했고, 전송 단계에서 문제가 발생한 열을 재검토하며 필요한 작업을 수행했습니다.\n\n3. Power BI: 데이터 시각화 및 보고\n\n마지막 단계에서는 Power BI를 사용하여 SQL 데이터베이스에서 데이터를 검색하고 분석된 데이터를 시각화했습니다. Power BI의 대화식 시각화 도구 덕분에 데이터를 더욱 이해하기 쉽고 인상적으로 제시할 수 있었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1. 파이썬: 데이터 정제 및 분석\n\n첫 번째 단계로 캐글에서 데이터셋을 다운로드합시다. (https://www.kaggle.com/datasets/imbishal7/glassdoor-salary)\n\n먼저, 데이터셋을 파이썬으로 불러오고 내용을 확인해봅시다. 먼저, Pandas 라이브러리를 import 해주세요:\n\n![Dataset](https://www.example.com/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로, Python에서 데이터셋을 읽어봅시다:\n\n```python\nimport pandas as pd\nfile_path = 'glassdoor-salaries.csv'\ndata = pd.read_csv(file_path)\ndata.info()\n```\n\n코드를 실행했는데, 실행 후에 CSV 파일에서 호환성 오류가 발생했습니다. 불량 행을 건너뛰도록 데이터셋을 다시 읽으려고 했습니다:\n\n```python\nimport pandas as pd\nfile_path = 'glassdoor-salaries.csv'\ndata = pd.read_csv(file_path, delimiter='\\t', on_bad_lines='skip')\nprint(data.head())\nprint(data.info())\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과에서 추가 열을 식별했고 데이터 유형을 확인했습니다:\n\n![이미지](/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_2.png)\n\n![이미지](/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_3.png)\n\n추가 및 불필요한 열을 삭제하는 코드에 한 줄을 추가했습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndata = data.drop(columns=['Unnamed: 9'])\n```\n\n그런 다음, 다시 확인하기 위해 CSV 파일을 원시 상태로 읽어 오류가 있는 열이 있는지 확인했습니다:\n\n```js\nimport csv\n\nfile_path = 'glassdoor-salaries.csv'\nwith open(file_path, 'r', encoding='utf-8') as file:\n    reader = csv.reader(file, delimiter='\\t')\n    for i, row in enumerate(reader):\n        if len(row) != 9: \n            print(f' Bad line: {i+1}, Content: {row}')\n```\n\n![이미지](/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_4.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 출력을 확인했고, 오직 열 이름만 잘못된 행으로 계산되었다는 것을 알았습니다. 그래서 첫 번째 행을 \"헤더\"로 표시했습니다. 먼저 데이터셋의 특수 문자를 공백으로 대체했습니다. 그런 다음 데이터셋에서 누락된 값을 식별하고 해당 열을 수정했습니다:\n\n```js\nimport pandas as pd\n\ndata = pd.read_csv('glassdoor-salaries.csv',header=0, delimiter='\\t')\npd.set_option('display.max_column', None)\npd.set_option('display.max_rows', None)\ndata = data.replace('\\xa0', ' ', regex=True) \ndata = data.drop(columns=['Unnamed: 9'])\nmean_rating = data['company_rating'].mean()\ndata['company_rating'].fillna(mean_rating, inplace=True)\nif 'company' in data.columns:\n    data['company'] = data['company'].fillna(\"No Company\")\n```\n\n그런 다음 단위 열을 더 읽기 쉽게 만드는 함수를 작성했습니다:\n\n```js\ndef chg_unit(unit):\n    if unit == ' / yr':\n        return 'Yearly'\n    elif unit == ' / mo':\n        return 'Monthly'\n    elif unit == ' / hr':\n        return 'Hourly'\ndata['unit'] = data['unit'].apply(chg_unit)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 프로세스 후에 데이터셋의 낮은, 평균 및 높은 급여 열의 숫자 데이터가 통화 단위와 결합되어 분석 과정에서 문제가 될 수 있습니다. 따라서 통화 단위를 추출하여 \"통화\"라는 다른 열에 할당했습니다:\n\n```js\ndef clmn_currency(x):\n    units = x.split(' ')\n    if len(units) == 1:\n        return x[0]\n    else:\n        return units[0]\ndata['currency'] = data['median'].apply(clmn_currency)\n```\n\n```js\ndef format_salary(x):\n    x = str(x)\n    unit = clmn_currency(x)\n    x = x.replace(unit,'')\n    x = x.replace(',','')\n    x = x.replace('M','000000')\n    x = x.replace('K','000')\n    return x              \n\ndata['median'] = data['median'].apply(format_salary)\ndata['low'] = data['low'].apply(format_salary)\ndata['high'] = data['high'].apply(format_salary)\n```\n\n또한 열에 있는 수백만, 수천 등을 나타내는 문자를 숫자로 변환했습니다. SQL 전의 마지막 단계는 정리된 데이터를 저장하는 것이었습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndata.to_csv('cleaned_glassdoor-salaries.csv', index=False)\n```\n\n2. SQL: 데이터베이스 관리 및 쿼리\n\n두 번째 단계에서는 정제된 데이터를 Oracle SQL 데이터베이스로 전송하였습니다. 첫 번째 단계로, SQL Developer를 사용하여 `employee_data`라는 테이블을 생성해보겠습니다:\n\n\u003cimg src=\"/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_5.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로, 컴퓨터에 텍스트 파일을 생성하여 SQL 명령을 작성하고, 데이터를 정리한 경로, 데이터가 로드될 대상 테이블, 파일 내에서 데이터가 구분되는 방법을 포함하여 저장하세요:\n\n![이미지](/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_6.png)\n\n그런 다음, 컴퓨터의 시작 메뉴에서 명령줄을 열고 저장된 텍스트 파일 안에 있는 SQL 명령을 실행하세요. 명령줄에 작성한 코드에는 사용자 이름, 비밀번호, 데이터베이스 문자열 (TNS 이름) 및 데이터베이스에 연결할 파일의 경로가 포함됩니다:\n\n![이미지](/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 단계를 거친 후에 내가 데이터를 테이블에 넣어 두었음을 확인했다. DML을 사용하여 쿼리했을 때, 데이터를 CSV 파일로 변환했기 때문에 다시 오류가 발생했다. 통화를 확인했을 때, 특수 문자가 데이터베이스 문자 설정 때문에 나오지 않았음을 발견했다.\n\n페이지를 마크다운 형식으로 변경해주세요:\n\n|Header1|Header2|Header3|\n|-------|-------|-------|\n|Data1  |Data2  |Data3  |\n\nSQL 명령어로 문제를 해결하고 세계 각국의 통화를 국가 코드로 변경했다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그런 다음, 평균 열을 쿼리하고 정렬했을 때 문자에 오류가 있는 것을 발견했습니다:\n\n![이미지](/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_10.png)\n\n이 열 및 기타 열의 오류를 수정하기 위해 SQL의 REPLACE 명령을 사용하여 불필요한 문자를 공백으로 대체했습니다:\n\n![이미지](/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_11.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 작업을 수행한 후에는 문제가 있는지 확인하기 위해 열을 확인하고 필요한 최종 조정을 했습니다. 회사 열에서 문제가 있는 열을 삭제하는 대신 효율성을 위해 이름을 바꿔주었습니다:\n\n![Company Column](/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_12.png)\n\n계산에서 혼란을 방지하기 위해 Round 명령을 사용하여 평점 열을 소수점 한 자리로 반올림했습니다:\n\n![Rating Column](/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_13.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 파워 BI: 데이터 시각화와 보고서\n\n파워 BI 부분에서는 프로젝트의 시각화와 마지막 단계로, 데이터를 가져와서 내 테이블에서 데이터가 들어왔는지 확인했습니다. 필요한 필터를 만들고, 오른쪽 메뉴에서 시각화할 데이터를 선택하고, 보고서에 맞게 그래프를 조정했습니다:\n\n![이미지](/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_14.png)\n\n그런 다음 차트를 만들기 시작했고, 첫 번째 차트에서는 회사들이 직무 역할에 대한 고용 비율을 보여주는 파이 차트를 만들었습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_15.png\" /\u003e\n\n참고: 숫자의 단위는 터키어로 제공됩니다. 이를 설정 섹션에서 사용자 정의할 수 있습니다.\n\n제가 두 번째 차트를 만들었고 회사 수의 분포를 국가별로 막대 차트로 조사했습니다:\n\n\u003cimg src=\"/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_16.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 세 번째 차트를 만들었고, 기업들의 평가 분포율을 막대 그래프를 사용하여 분석했어요:\n\n![Company Rating Distribution Rates](/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_17.png)\n\n네 번째 차트를 만들었는데, 이 차트에는 통화 단위를 선택하여 전문직의 평균 연봉을 플롯했어요. 이 표에 카드를 추가하여 정확한 수치를 볼 수도 있어요. 우리는 표 옆의 슬라이서로 원하는 통화를 선택할 수 있어요:\n\n![Average Salaries of Professions by Currency Units](/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_18.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최종 차트에서 이해를 돕기 위해 행렬을 사용했습니다. 이 표에서는 회사들이 직무별로 제공하는 결제 방법의 분포 백분율을 조사했습니다:\n\n| 구분 | 시간당 | 월별 | 연간 |\n|---|---|---|---|\n| 엔지니어 | 20% | 50% | 30% |\n| 디자이너 | 30% | 40% | 30% |\n| 분석가 | 25% | 45% | 30% |\n| 기타 | 20% | 50% | 30% |\n\n프로젝트의 종합 평가\n\n얻은 결과\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 클린징: 파이썬을 사용하여 데이터셋에서 누락된 데이터와 오류 데이터를 성공적으로 정리했습니다. 이 단계는 분석을 위한 견고한 기반을 마련했습니다.\n\n데이터 분석: SQL 쿼리를 통해 데이터베이스의 데이터에 복잡한 분석을 수행할 수 있었습니다. 이러한 분석을 통해 조정해야 하는 불필요한 행과 열에 대한 유용한 정보를 얻을 수 있었습니다.\n\n데이터 시각화: Power BI를 사용하여 만든 시각화를 통해 데이터를 더 명확하고 효과적으로 표현할 수 있었습니다. 이러한 시각화는 다음 단계의 결정을 지원하는 명확하고 설득력 있는 결과를 제공했습니다.\n\n## 이 포괄적인 데이터 엔지니어링 프로젝트와 로드맵 그리고 전적으로 제 자신의 코드에 관심을 가져 주셔서 감사합니다.","ogImage":{"url":"/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_0.png"},"coverImage":"/assets/img/2024-06-22-DataAnalysisProjectUsingPythonSQLandPowerBISalaryandRatingAnalysisofCertainSoftwareEngineeringFieldsbyCountry_0.png","tag":["Tech"],"readingTime":10},{"title":"SQL에서 중앙값 계산하는 5가지 방법","description":"","date":"2024-06-22 17:50","slug":"2024-06-22-5WaysToCalculateMedianInSQL","content":"\n\n![이미지](/assets/img/2024-06-22-5WaysToCalculateMedianInSQL_0.png)\n\n3년 전, 데이터 과학의 세계에 들어갈 것 같았을 때, 여러 가지가 너무 압도적으로 보였어요. 딱 한 번의 인생 안에 배워야 할 것들이 너무 많았죠! 데이터 과학자가 되기 위해 필요한 기술을 탐험하면서 저를 안심시켜준 것 중 하나는 통계학이었어요. 저는 고등학교 때부터 대학 시절까지 통계학을 좋아했거든요. 통계학을 배울 때 우리가 초기에 배운 것 중 하나는 데이터의 평균, 중앙값 그리고 최빈값을 계산하는 것이었어요. 전문 용어로는 중심 경향성 측정값이라고 부르기도 해요.\n\n# 빠른 복습\n\n평균: \"평균치\"라고도 불리며, 모든 데이터 점수를 더하고 데이터 점수의 수로 나누어 찾을 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n중앙값: 중간 숫자; 모든 데이터 포인트를 정렬하고 중간에 있는 숫자를 선택하여 찾음 (또는 두 개의 중간 숫자가 있는 경우, 그 두 숫자의 평균을 취함).\n\n최빈값: 가장 빈도가 높은 숫자 - 즉, 가장 자주 발생하는 숫자.\n\n![이미지](/assets/img/2024-06-22-5WaysToCalculateMedianInSQL_1.png)\n\nMeasure of Central Tendency에 대한 마이크로블로그를 보유하고 있습니다. 여기에서 그들의 필요성과 사용 사례가 더 자세히 설명되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nhttps://www.linkedin.com/posts/subhralina-nayak_data-statistics-datascience-activity-6977655040274522112-bvR6?utm_source=share\u0026utm_medium=member_desktop\n\n# SQL: 나의 진정한 사랑\n\n말할 것도 없이, 나는 \"SQL이 모든 것보다 중요\" 운동의 기수입니다!\n\n데이터 분석에 필수인 최고의 도구가 SQL이죠. SQL을 사용하면 가능한 모든 분석을 수행할 수 있습니다. 하지만 중심 경향성 측정은 할 수 있나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n네, 당연히 가능해요!\n\n평균을 계산하는 것은 매우 간단해요. AVG() 집계 함수를 사용할 수 있어요.\n\n최빈값을 계산하는 것은 각 카테고리의 레코드 수를 세고, MAX() 함수를 사용하여 가장 많은 수의 그룹을 구할 수 있어요.\n\n하지만, 중앙값을 계산하는 것은 약간 까다로울 수 있어요. 그렇지만 중앙값을 SQL에서 계산하는 것은 여전히 가능해요. 한 가지 방법이 아니라, 5가지 방법으로 가능해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# SQL을 사용하여 중앙값 계산해보기\n\n먼저, 중앙값을 계산하는 데 사용되는 수학 공식을 이해해야 합니다.\n\nN개의 요소를 가진 데이터셋이 있을 때, 작은 순서대로 정렬된 경우,\n\n중앙값 = (N+1)/2번째 요소, 만약 N이 홀수인 경우\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n중앙값 = (N/2번째 요소 + (N/2 + 1)번째 요소)/2, 단, N이 짝수인 경우\n\n위의 공식에 따르면 데이터셋은 정렬되어 있어야 하며 홀수와 짝수 개수의 데이터포인트를 고려해야 합니다.\n\n## 변수와 OFFSET — FETCH를 사용하여\n\n```js\nDECLARE @c BIGINT = (SELECT COUNT(*) FROM sales.order_items)\n\nSELECT AVG(list_price) AS \"Median\"\nFROM (\n SELECT list_price\n FROM sales.order_items\n ORDER BY list_price\n OFFSET (@c - 1)/2 ROWS\n FETCH NEXT 1 + (1 - @c%2) ROWS ONLY\n) data\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n변수 @c는 sales.order_items 테이블의 전체 행 수로 할당됩니다. 내부 서브쿼리는 목록 가격을 오름차순으로 정렬합니다. OFFSET (@c - 1)/2 ROWS 절은 첫 번째 절반의 행 (단 @c가 홀수인 경우) 또는 정확히 중간 행 (단 @c가 짝수인 경우)을 건너뜁니다. FETCH NEXT 1 + (1 - @c%2) ROWS ONLY 절은 @c가 홀수인 경우 다음 행을 가져오거나 @c가 짝수인 경우 추가 행을 가져오지 않습니다. 마지막으로 외부 쿼리는 검색된 행의 평균을 계산하여 중앙값을 구합니다.\n\n## ROW_NUMBER() 윈도우 함수 사용하기\n\n```js\nSELECT AVG(list_price) AS \"Median\"\nFROM\n(\n   SELECT list_price,\n      ROW_NUMBER() OVER (ORDER BY list_price ASC, order_id ASC) AS RowAsc,\n      ROW_NUMBER() OVER (ORDER BY list_price DESC, order_id DESC) AS RowDesc\n   FROM sales.order_items\n) data\nWHERE\n   RowAsc IN (RowDesc, RowDesc - 1, RowDesc + 1)\n```\n\n위 쿼리는 먼저 각 행에 대해 목록 가격과 주문 ID에 따라 오름차순 및 내림차순으로 순서를 지정합니다. 서브쿼리는 목록 가격 및 해당하는 오름차순 및 내림차순 순서의 행 번호를 검색합니다. 외부 쿼리는 오름차순 순서의 행 번호가 내림차순 순서의 행 번호와 동일하거나 하나 차이 나는 경우의 목록 가격의 평균을 계산합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## ORDER BY, MIN() 및 MAX() 함수 사용\n\n```js\nSELECT (\n(SELECT MAX(list_price)\nFROM (\n SELECT list_price\n FROM sales.order_items\n ORDER BY list_price\n OFFSET (SELECT COUNT(*)\n FROM sales.order_items) / 2 ROWS\n FETCH NEXT 1 ROW ONLY) AS BottomHalf) +\n(SELECT MIN(list_price)\nFROM (\n SELECT list_price\n FROM sales.order_items\n ORDER BY list_price DESC\n OFFSET (SELECT COUNT(*)\n FROM sales.order_items) / 2 ROWS\n FETCH NEXT 1 ROW ONLY) AS TopHalf)\n) / 2 AS Median\n```\n\n데이터셋을 list_price를 기준으로 두 부분 (상위 및 하위)으로 분할합니다. 내부 서브쿼리는 각각 list 가격의 상위 50%와 하위 50%를 찾습니다. MAX(list_price) 및 MIN(list_price) 함수는 각각 각 반의 최대값과 최소값을 검색합니다. 최종 결과는 하위 반의 최댓값과 상위 반의 최솟값을 더한 다음 그 합을 2로 나누어 중앙값을 얻습니다.\n\n## NTILE() 윈도우 함수 사용\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nSELECT MAX(list_price) AS \"중앙값\"\nFROM (\n SELECT list_price,\n NTILE(4) OVER(ORDER BY list_price) AS Quartile \n FROM sales.order_items\n) X\nWHERE Quartile = 2\n```\n\nNTILE 함수는 결과 집합을 4개의 동일한 부분(사분위수)으로 나눕니다. ORDER BY list_price는 목록 가격을 오름차순으로 정렬합니다. 외부 쿼리는 Quartile이 2인 서브쿼리에서 최대 목록 가격을 선택하는데, 이는 중간값인 두 번째 사분위수를 나타냅니다.\n\n## PERCENTILE_CONT() 윈도우 함수 사용하기\n\n```js\nSELECT DISTINCT PERCENTILE_CONT(0.5) \n  WITHIN GROUP (ORDER BY list_price) OVER() AS \"중앙값\"\nFROM sales.order_items\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPERCENTILE_CONT 함수는 그룹 내에서 주어진 열의 특정 백분위수를 계산합니다. 0.5 인수는 우리가 50번째 백분위수인 중앙값을 찾고 싶다는 것을 나타냅니다. ORDER BY list_price 절은 중앙값을 계산하기 전에 목록 가격이 오름차순으로 정렬되도록 합니다. DISTINCT 키워드는 중앙값이 동일한 값이 여러 개 있는 경우에도 전체 중앙값에 대해 한 결과만 얻도록 합니다. OVER() 함수는 어떤 분할도 없이 사용되므로 PERCENTILE_CONT가 전체 결과 집합에 적용됩니다.\n\n참고: 모든 쿼리는 MS SQL Server에서 작성되었습니다.\n\n이 쿼리들은 SQL에서 중앙값을 계산하는 다양한 방법을 제공하며, 그 효과는 데이터셋 크기와 사용 중인 데이터베이스 시스템에 따라 달라질 수 있습니다. 데이터셋에서 가장 효율적이고 정확한 중앙값 계산 방법을 찾기 위해 이 쿼리들을 특정 데이터셋에서 테스트하는 것이 좋은 실천법입니다.\n\nSQL에서 중앙값을 계산하는 다른 방법이 있을 것으로 확신합니다. 발견하면 코멘트로 남겨주시기 바랍니다. 위에서 소개한 방법 중에서 여러분의 즐겨찾는 또는 선호하는 방법을 알려주세요. 이 블로그가 여러분의 SQL 역량을 향상시키는 데 도움이 되기를 바랍니다. 네, \"모든 것은 SQL로 시작한다\"고 말씀합니다.","ogImage":{"url":"/assets/img/2024-06-22-5WaysToCalculateMedianInSQL_0.png"},"coverImage":"/assets/img/2024-06-22-5WaysToCalculateMedianInSQL_0.png","tag":["Tech"],"readingTime":5},{"title":"데이터 분석 효율성을 높이는 방법 esProc SPL 종합 가이드","description":"","date":"2024-06-22 17:41","slug":"2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL","content":"\n\n## esProc SPL(Structured Process Language)을 통해 애플리케이션 비용을 줄이고 일반적인 데이터 분석 도전 과제를 해결하는 방법을 발견하세요\n\n![이미지](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_0.png)\n\n현재의 디지털 시대에서 데이터의 만능성은 부인될 수 없습니다. 데이터 관리의 선두주자인 SQL(Structured Query Language)이 많은 사람들에게 신뢰받는 동반자로 자리 잡고 있습니다. 그러나 효율적인 데이터 관리 영역에서 유일한 경쟁자인가요?\n\nesProc SPL(Structured Process Language)이라는 챌린저가 나타나 데이터와 상호 작용하는 방식을 혁신하겠다고 약속합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n함께 참여하여 SPL의 세계를 해독하고 그 잠재적 이점을 탐험해 보세요.👇🏻\n\n# esProc SPL이란 무엇인가요?\n\n먼저, esProc SPL이 무엇인지 설명하겠습니다.\n\n구조화된 및 반구조화된 데이터의 계산 및 처리 엔진으로, esProc SPL은 분석 데이터베이스나 데이터 컴퓨팅 미들웨어로 사용될 수 있으며, 주로 오프라인 일괄 작업 및 온라인 쿼리와 같은 두 가지 데이터 분석 시나리오에 적용됩니다. 시장에서 흔히 볼 수 있는 일반적인 분석 데이터베이스와는 달리, esProc SPL은 SQL 시스템이나 NoSQL 기술(MongoDB, HBase와 같은)이 아니며, 대신 자체 생성한 SPL (Structured Process Language) 구문을 채택합니다. 이는 기존의 데이터 처리 기술과 비교하여 코딩이 간단하고 실행 효율이 높습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nesProc SPL이 해결하는 고통점은 무엇인가요?\n\nSPL은 주로 어렵게 작성하고 실행이 느리며 운영 및 유지 관리가 어려운 데이터 문제를 해결합니다. 아래에 몇 가지 예시가 있습니다.\n\n현재 많은 산업 분야에서는 배치 작업을 수행해야 하는데, 이 작업은 일반적으로 평소 업무 시간이 끝난 밤에 이루어집니다. 따라서 고정된 시간 창이 있고, 이 작업은 반드시 해당 창 안에서 완료되어야 하는데, 그렇지 않으면 정상적인 업무에 영향을 미칩니다. 그러나 업무가 축적됨에 따라 배치 작업의 압력이 커지게 됩니다. 고정된 시간 창 때문에 때로는 비즈니스와 데이터 양이 증가함에 따라 배치 작업이 해당 시간 창 내에서 완료되지 못할 수도 있습니다, 특히 월/년 말과 같은 중요한 날짜에는 더 그렇습니다.\n\n보고서를 조회할 때 항상 몇 가지 중요한 보고서가 쿼리하기 어렵고, 세 분 또는 두 분 또는 심지어 그 이상 걸리는 경우가 있습니다. 최적화를 여러 차례 시도해도 쿼리 효과가 크게 향상되지 않으면 사용자들은 화를 내게 됩니다. 가끔은 보고서를 쿼리하는 사용자 수가 증가하고, 선택한 시간 범위가 길어지면 결과를 찾기가 더욱 어려워집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n업무에서는 매우 긴, 극도로 복잡한 SQL 코드를 종종 볼 수 있습니다. 이러한 코드는 N개의 계층으로 중첩되어 있을 뿐만 아니라, 문을 작성하는 데 수백 줄이 필요합니다. 이러한 코드는 작성하기 힘들 뿐만 아니라 수정하기도 불가능하며, 몇 일 후에는 프로그래머조차 헷갈릴 수 있습니다. 저장 프로시저에 대해서는 상황이 더 나빠며, 때로는 저장 프로시저의 크기가 수십 또는 수백 KB에 달하는 경우도 있어 작성하고 유지하기가 매우 어렵습니다.\n\n일부 복잡한 계산은 저장 프로시저로 작성할 수 있지만 JAVA보다 간단하지만, 저장 프로시저는 애플리케이션을 이관할 수 없으며, 이에 지나치게 의존하면 애플리케이션 프레임워크에 문제가 발생할 수 있습니다. 애플리케이션의 확장이 불가능해지거나 결합도가 높아진다는 등의 문제가 발생할 수 있습니다.\n\n현재는 다양한 종류의 데이터 소스가 있으며, 데이터베이스만해도 많은 종류가 존재합니다. 게다가 NoSQL, 텍스트, 엑셀, JSON과 같은 다른 데이터 소스도 많이 있습니다. 이러한 다양한 데이터 소스를 혼합하여 사용하려면 더 어려워집니다. 특히, 모든 데이터를 데이터베이스로 가져와야 한다면, 실시간 데이터 처리가 떨어지며 데이터베이스 공간을 차지하여 데이터베이스에 더 많은 압력을 가하게 됩니다. 반대로 데이터베이스로 가져오지 않으면, 하드 코딩을 해야 하므로 매우 어려울 수 있습니다. 결과적으로 궁지에 몰릴 수 있습니다.\n\n일반적으로 esProc SPL은 느린 일괄 작업, 느린 쿼리, 데이터베이스 압력이 높은 경우, SQL 코드 작성과 유지 보수가 어려운 경우, 다양한 데이터 소스의 혼합 계산, 불합리한 응용 프레임워크 등의 문제를 해결할 수 있는 능력을 갖추고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래에 더 많은 문제들을 나열해보겠습니다:\n\n- 시간 창 안에 맞지 않는 느린 일괄 작업은 특히 중요한 요약 날짜에 부담감을 주고 있습니다.\n- 쿼리/보고서를 위해 몇 분을 기다려야 하는 경우, 업무 인원들은 화를 내게 됩니다.\n- 더 많은 동시성, 쿼리 시간 간격이 길어질수록 데이터베이스가 크래시됩니다.\n- 수십 KB의 N-레벨 중첩 SQL 또는 저장 프로시저들은 몇 일 지나면 프로그래머 자신도 헷갈립니다.\n- RDB/NoSQL/파일/JSON/웹 등과 같은 수십 개의 데이터 원천, 원천간 혼합 계산이 매우 필요합니다.\n- 핫 데이터와 콜드 데이터를 별도의 데이터베이스로 분리하면 전체 데이터에 대한 실시간 쿼리가 어렵습니다.\n- 저장 프로시저에 너무 의존하면 응용 프로그램을 이전할 수 없으며, 프레임워크를 조정하기 어렵습니다.\n- 데이터베이스에 너무 많은 중간 테이블이 존재하여 저장소 및 자원을 고갈시키지만 그것들을 삭제하기는 무서워합니다.\n- 기업에서 끊임없는 보고서 수요가 있어, 인력 비용을 어떻게 경감할 수 있을까요?\n- …\n\n물론 esProc SPL 대상 시나리오에 대한 대응 기술들도 언제든 사용 가능합니다. 그러니, esProc SPL의 대응 기술은 무엇인가요?\n\n가장 중요한 것은 OLAP 시나리오에 적용되는 SQL 구문을 사용하는 데이터베이스와 데이터 웨어하우스입니다. 예를 들어, MySQL, PostgreSQL, Oracle, DB2 등과 같은 일반 관계형 데이터베이스; 하둡 위의 데이터 웨어하우스인 Hive, Spark SQL; Snowflake와 같은 새로운 MPP 및 클라우드 데이터 웨어하우스; Oracle의 ExaData와 같은 상용 올인원 데이터베이스 머신 등이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n개발 언어 관점에서 SPL은 Python, Scala, Java 및 Kotlin과 같은 일부 데이터 분석 및 통계 기술을 대체할 수 있습니다.\n\n상기 기술들과 비교했을 때, esProc SPL은 저 코드, 고 성능, 가벼움, 다양성의 장점을 가지고 있습니다. 특히 복잡한 계산을 구현하는 데 있어 Python 및 SQL보다 SPL이 더 간결하고 간단하며, 즉 SPL 코드가 더 짧습니다. esProc SPL은 높은 성능 알고리즘과 고성능 스토리지를 제공하여 빠르게 실행하는 것이 가능하며, 독립적으로 사용하거나 응용 프로그램에 통합하여 여러 데이터 소스에서 직접 계산할 수 있어 더 가볍고 개방적입니다. SPL은 전통적인 계산 능력뿐만 아니라 행렬, 적합 및 심지어 AI 모델링과 같은 다양한 기능을 제공하여 대부분의 데이터 작업을 쉽게 처리할 수 있어 더 다재다능합니다.\n\nSPL의 주요 대응 기술은 여전히 SQL입니다. 결국 데이터 분석 분야에서 가장 널리 사용되는 기술 중 하나입니다. 그래서 esProc SPL은 SQL 이상의 무엇을 제공할까요?\n\n현대 복잡한 비즈니스 및 빅데이터 관점에서 SQL을 살펴보면 SQL의 계산 및 설명 능력이 부족하다는 것을 알게 될 것입니다. 필요한 데이터 유형 및 계산 기능(예: 순서화 된 계산)이 부족한 SQL은 종종 복잡한 계산을 다층 중첩 및 우회적 방법으로 구현해야만 하고, 이런 방식은 두 가지 문제를 발생시킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n첫 번째로 개발 비용에 대해 이야기해 보겠습니다. 실무에서는 종종 천 줄이 넘는 SQL 코드를 볼 수 있습니다. 한 번 계산 로직이 약간 복잡해지면 긴 코드와 다층 중첩 코드를 작성해야 하며, 이는 쓰기와 디버깅이 어렵다는 것뿐만 아니라, 프로그래머 자신도 시간이 지난 후에 작성한 코드가 무슨 뜻인지 이해하지 못할 수 있습니다. 이는 개발 비용 증가로 이어질 수밖에 없습니다. 반면 SPL은 풍부한 데이터 유형과 계산 기능을 제공하여 계산 및 설명 능력을 크게 향상시킵니다. 더불어 좀 더 유연한 구문 시스템을 제공하는 SPL은 단계적 코딩을 지지하며, \"다단계\"의 자연스러운 사고에 따라 복잡한 계산 로직을 구현할 수 있어 쉽게 코딩하고 디버그할 수 있으며 개발 비용을 크게 줄일 수 있습니다.\n\n두 번째로 복잡한 SQL 코딩으로 인한 성능 문제에 대해 이야기해 보겠습니다. SQL로는 구현할 수 없는 효율적인 방법이 있더라도 느린 알고리즘을 사용해야 할 수 있습니다. 원하는 성능 지수를 달성하려면 추가 하드웨어를 더해야 하므로 하드웨어 비용이 증가합니다. SPL은 높은 성능 알고리즘(및 저장소)을 캡슐화하고 동일한 성능을 달성하는 데 더 적은 하드웨어가 필요하므로 하드웨어 비용이 효과적으로 감소합니다. SPL은 덜 많은 하드웨어로 SQL 성능을 달성하거나 능가하는 많은 사례를 찾을 수 있습니다.\n\nSQL(데이터베이스)의 컴퓨팅 시스템은 닫혀 있고 데이터는 데이터베이스로 로드된 후에만 계산될 수 있으며, 보통 데이터베이스를 독립적으로 배포할 수 있기 때문에 부풀어 오른 무겁고 무겁은 프레임워크로 이어질 수 있습니다. 더욱이 SQL의 컴퓨팅 능력은 실제로 불완전하며, SQL은 일부 복잡한 시나리오를 독립적으로 처리하는 데 적합하지 않기 때문에 Python, Java와 같은 다른 기술을 채용해야 합니다. 그러나 이러한 완전히 다른 기술은 기술 스택의 복잡성을 증가시킬 수 있습니다. 무겁고 부풀어 오른 프레임워크와 복잡한 기술 스택은 O\u0026M 비용을 크게 증가시킵니다. 반면 SPL은 컴퓨팅 능력에서 더 개방적이며 다양한 데이터 원본에서 직접 계산하고 독립적 또는 통합적으로 사용할 수 있습니다. 그리고 그 프레임워크가 가벼워집니다. 또한 SPL은 복잡한 계산을 쉽게 구현할 수 있도록 포괄적인 기능을 제공하며, 다른 기술 없이 대부분의 작업을 수행할 수 있습니다. 그 기술 스택은 간단하게 됩니다. 가벼운 프레임워크와 간단한 기술 스택은 O\u0026M 비용을 낮춥니다.\n\n![image](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSPL은 Java보다도 상당한 장점을 지니고 있어요. Java는 기능이 많은 프로그래밍 언어로, 이론상으로는 어떤 데이터 계산 작업도 처리할 수 있어요. 그러나 너무 네이티브한 특성과 필요한 계산 라이브러리 부족으로 모든 계산 작업을 처음부터 개발해야 하여 구현이 어렵다는 제약이 있어요.\n\n특히 고성능 알고리즘의 경우 Java에서 구현하기가 더 어려워요. 프로그래머들이 최선을 다해 문제를 해결하려고 하더라도 성능이 매우 취약해요. 그 결과 개발 비용은 높아지고 성능은 낮아지는데요. 따라서 하드웨어 비용도 증가하게 되요.\n\n또한 Java는 실제 사용에서 몇 가지 단점을 가지고 있어요. 예를 들어, 컴파일된 언어로써 핫 스왑을 달성하는 것이 어렵고, 다른 애플리케이션/모듈이 함께 주 애플리케이션과 함께 배포되어야 하는 경우 서로 강하게 결합되는 문제가 발생해요. 이러한 단점들은 자주 변경되는 데이터 분석 시나리오에 상당한 부정적 영향을 미치게 돼요. 이러한 단점을 피하기 위해 프로그래머들은 SQL과 Java를 함께 사용하곤 해요. SQL이 많은 계산에 더 간단하고 편리하기 때문이죠. 그러나 이렇게 함으로써 SQL 문제만 해결되지 않을 뿐더러 새로운 Java 문제가 발생해서 복잡한 기술 스택, 사용하기 어려움, 그리고 높은 유지보수 비용을 초래하게 되요.\n\n한편 SPL은 이러한 문제들이 없어요. SPL은 풍부한 계산 라이브러리를 제공하여 계산 작업을 쉽게 구현할 수 있으며, 성능을 보장하기 위해 많은 저복잡도 알고리즘을 제공해요. 또한 해석형 언어로, SPL은 자연스럽게 핫 스왑을 지원하고 결합 문제는 결코 발생하지 않아요. SQL과 비교하면 SPL은 Java에 비해 상당한 장점을 지니고 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Table](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_2.png)\n\n파이썬에서는 구조화된 데이터 처리에 문제가 있습니다. 파이썬(Pandas)은 풍부한 연산 라이브러리를 제공하고 많은 간단한 계산에 대해 파이썬으로 쉽게 코딩할 수 있어 Python이 기본적으로 SQL과 동등하다고 할 수 있습니다. 그러나 약간 복잡한 계산에 대해 Python으로 코딩하는 것은 어렵고 개발 비용이 여전히 높습니다.\n\n또한, 파이썬은 대규모 데이터 처리에 취약합니다. 외부 저장소에 대해 커서 타입을 제공하지 않기 때문에 메모리 용량을 초과하는 데이터를 처리하는 것은 번거롭고 비효율적이며 하드웨어 비용이 증가할 수 있습니다.\n\n뿐만 아니라, 파이썬은 버전 문제를 내포하고 있습니다. 다른 버전 간의 호환성 문제로 사용 및 운영 비용이 높아지게 되며 파이썬은 통합력이 매우 떨어져 응용 프로그램에서 결합하기 어려워 별도의 서비스를 배포해야 하는 경우가 많습니다. 이로 인해 운영 및 유지보수 비용이 증가하게 됩니다. 자바와 마찬가지로 파이썬은 실무에서 종종 SQL과 함께 사용되며, 기존의 SQL 문제는 여전히 존재하면서 새로운 문제가 발생합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서 개발 및 성능 측면에서 SPL의 장점에 대해 많이 설명했습니다. 더불어 SPL은 많은 다른 이점을 갖고 있습니다: 버전 문제 없음; 우수한 통합으로 애플리케이션에 매끄럽게 통합이 가능함; 완전한 기능 제공; 간단한 기술 스택; 다른 기술에 의존할 필요 없음; 운영 및 유지보수 비용 절감.\n\n![이미지](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_3.png)\n\n전반적으로 SPL은 SQL, Java 및 Python과 비교하여 개발, 하드웨어 및 운영 및 유지보수 비용 모두에서 수십 배의 비용 절감을 이룰 수 있습니다.\n\n# 사례 소개\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 esProc SPL의 실제 응용효과입니다.\n\n먼저 두 가지 일괄 작업 사례를 살펴보겠습니다.\n\n## 보험 회사의 자동차 보험 일괄 작업\n\n이것은 보험 회사의 자동차 보험에 대한 일괄 작업 시나리오로, 사용자에게 보험 갱신을 상기시키기 위해 과거 보험을 새 보험과 관련시켜야 합니다. 데이터 양이 많아서 정책 테이블에는 3500만 개의 데이터 행, 세부 사항 테이블에는 1억 2300만 개의 데이터 행이 포함되어 있습니다. 여러 가지 연관 방법이 있기 때문에 따로 처리해야 하며, 계산이 매우 복잡합니다. 보험 회사는 원래 informix의 저장 프로시저를 사용하여 계산했으며, 30일간의 새 보험을 연관시키는 데 112분이 걸렸습니다. 시간이 더 길면 계산이 어려울 것입니다. 이로 인해 성능 문제가 발생합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nesProc SPL을 사용하면 동일한 작업을 수행하는 데 17분 만에 끝낼 수 있어요. 이는 6.5배의 증가이며 코드 양도 1800줄에서 500줄로 줄어드는 걸 확인할 수 있어요. 이전에 말했듯이, SPL을 사용하면 쉽게 코드를 작성할 수 있고 실행 속도도 더 빨라져요.\n\n사례 상세정보: 오픈소스 SPL은 보험 회사의 일괄 처리 작업 시간을 2시간에서 17분으로 최적화했어요\n\n# 은행의 대출 계약 일괄 작업\n\n이 또한 일괄 작업 시나리오에 해당돼요. 은행은 AIX 및 DB2(은행의 표준 설정)이 장착된 작은 기계에서 이 작업을 수행하며 \"기업 대출 계약 세부 정보\" 저장 프로시저를 실행하는 데 1.5시간이 걸렸어요. 이 계산에는 48개의 SQL 단계, 매우 복잡한 다중 테이블 관련 및 3300줄의 코드가 포함돼 있어요. 이 일괄 작업은 전체 은행의 일괄 작업의 일부이기 때문에 이 일괄 작업이 느리다면 전반적인 일괄 작업 프로세스를 늦추기 때문에 긴급히 최적화해야 할 필요가 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nesProc SPL을 사용하여 동일한 작업을 수행할 때는 10분만에 처리할 수 있으며, 속도를 8.5배 빠르게 하며 코드 양을 3300줄에서 500줄로 줄입니다. 속도 향상의 주된 이유는 SPL의 순서 처리와 다목적 순회와 같은 기능을 활용한 것입니다.\n\n사례 세부 정보: 오픈 소스 SPL은 은행 대출 동의의 일괄 처리 속도를 10배 이상 높입니다.\n\n이제 두 가지 온라인 쿼리 사례를 살펴보겠습니다.\n\n# 모바일 뱅킹: 다중 동시 계좌 조회\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n은행에서는 고객들에게 현재 계좌 정보를 조회할 수 있는 모바일 뱅킹을 통해 서비스를 제공하고 있습니다. 이 서비스는 대규모 동시 접속이 필요하며 실시간 처리가 요구됩니다. Hadoop은 동시 접근 요구를 충족할 수 없기 때문에, 은행은 6개 노드로 구성된 ES 클러스터를 쿼리 백엔드로 구축했습니다. 백엔드는 요구 사항을 충족하지만 실시간으로 연관을 짓지는 못하며, 지점 코드가 변경되면 데이터를 업데이트하는 데 몇 시간이 소요됩니다. 업데이트 기간 동안 서비스를 중단해야 하므로 사용자의 정상적인 조회에 영향을 줍니다.\n\nesProc SPL을 도입한 후, 상세 데이터는 계좌 번호별로 순서대로 저장됩니다. SPL의 외부 저장소 인덱스 및 정렬 기술을 이용하여 계좌 번호별 정보를 빠르게 액세스하고, 메모리에서 지점 코드와 연관시킬 수 있습니다. 이렇게 하면 실시간 조회와 실시간 연관을 구현할 뿐만 아니라 고 동시성을 처리할 수 있습니다. 마지막으로, esProc SPL은 1개 노드에 6개 ES 노드의 효과를 실현하며, 지점 정보 업데이트에 대기 시간이 없는 실시간 연관을 구현합니다.\n\n케이스 세부 정보 : 오픈 소스 SPL은 은행 모바일 계정에 대한 사전 연관 쿼리를 실시간 연관으로 변경하는 계산\n\n\n은행의 고객 중 고유 대출 클라이언트 수 계산\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n은행의 대출 업무에는 대출 잔액, 담보 유형, 고객 유형, 대출 방식 등 여러 지표가 관련되어 있습니다. 수백 가지 지표가 임의로 조합되어 쿼리되며, 따라서 계산 규모가 방대하며, 동시 쿼리 수가 증가함에 따라 계산 난이도도 증가합니다. 이 계산 시나리오에서는 2000만 행의 대형 테이블과 더 큰 상세 테이블에 대한 연관, 필터링 및 집계 계산이 필요하며, 각 페이지는 거의 200개의 지표를 계산해야 하며, 10개의 동시성은 2000개 이상의 지표를 동시 계산할 수 있습니다. 이러한 대규모 계산 규모에 대해 Oracle을 사용하여 하루 전에 계산해야 하지만, 사전 계산은 사용자의 실시간 쿼리 요구를 충족시키지 못합니다.\n\n은행이 SPL을 채용한 후에는 정렬 병합, 부울 차원 순서, 멀티 스레드 병렬 컴퓨팅 등을 통해 지표의 계산이 실시간으로 구현되어 성능 요구 사항이 충족되며, 총 2000개 지표와 10개의 동시성에 대한 계산 시간이 3초 미만이 됩니다. 데이터를 미리 준비할 필요가 없으며 어떤 레이블 조합도 즉시 선택할 수 있고 실시간으로 쿼리 결과를 얻을 수 있습니다.\n\n사례 세부 정보: 오픈 소스 SPL은 은행 사전 계산 고정 쿼리를 실시간 유연한 쿼리로 최적화합니다.\n\n오프라인 배치 작업과 온라인 쿼리 사이에는 큰 차이가 있습니다. 전자는 종종 많은 양의 데이터와 매우 복잡한 계산 로직이 포함되지만 동시성 쿼리가 관련되지 않으며 실시간 계산이 필요하지 않습니다. 지정된 시간 내에 계산을 완료하면 됩니다. 후자는 그 반대로, 대량의 동시성 숫자가 관련되며 높은 실시간성이 필요하며 처리하기가 어려울 수 있습니다. esProc SPL은 두 시나리오에 모두 적합합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사실, 개발 효율성과 성능 외에도 esProc SPL은 응용 프레임워크 측면에서도 효과적으로 작동합니다. 아래에 두 가지 관련 사례가 있습니다.\n\n# 은행의 BI 시스템에서의 프런트 엔드 데이터베이스\n\n이 은행은 중앙 데이터 웨어하우스를 보유하고 있습니다. 이 웨어하우스는 전체 은행의 모든 데이터 작업을 담당하며, 오버로드되었기 때문에 BI 시스템에 5개의 동시성을 할당하지만 여전히 요구 사항을 충족시키지 못합니다.\n\n이 문제를 해결하기 위해 BI 시스템을 위한 전용 프런트 엔드 데이터베이스(은행에서는 프런트 엔드 머신이라고 부릅니다)를 구축해야 합니다. 그러나 데이터베이스를 구축할 때 문제가 발생합니다. 즉, 데이터 웨어하우스에서 고빈도 데이터만 데이터베이스로 가져오면 다른 데이터를 조회할 수 없어 비즈니스 요구 사항에 대응할 수 없습니다. 데이터를 모두 데이터베이스로 가져오면 비현실적(너무 비용이 많이든다)이며 이는 사실상 데이터 웨어하우스를 재구축하는 것과 같습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nesProc SPL을 사용하여 프런트엔드 머신을 구축할 때는 고빈도 데이터만 프런트엔드 머신으로 가져오면 되므로 반복적인 구성을 피할 수 있습니다. esProc은 가장 높은 빈도로 발생하는 데이터 계산 작업을 처리하며, 몇 가지 낮은 빈도의 데이터 계산 작업은 자동으로 중앙 데이터 웨어하우스로 라우팅되어 앞서 언급한 두 가지 문제가 해결됩니다. 이러한 자동 라우팅 기능은 여기서 핵심 역할을 합니다.\n\n# 보험 회사 - 외부 데이터베이스 저장 프로시저\n\n이 경우는 esProc SPL을 Vertica 저장 프로시저로 사용하는 것입니다. 캐나다 보험 회사인 고객이 Vertica, MySQL, Access와 같은 데이터베이스에서 겪는 주요 문제는 두 가지입니다. 하나는 Vertica가 저장 프로시저를 지원하지 않기 때문에 복잡한 계산을 실행해야 하는 경우 Java 하드코딩을 통해 구현해야 하며, 이는 매우 어렵습니다. 또 다른 하나는 여러 데이터 소스의 혼합 계산에 관련된 경우 MySQL과 같은 데이터를 먼저 Vertica로 로드해야 하는데, 이는 장황하고 실시간이 아니며, 로딩 후 데이터베이스가 부풀어 오르며, 결국 일부 데이터는 영구 저장할 필요가 없습니다.\n\nesProc SPL로 이러한 두 가지 문제를 해결할 수 있습니다. 먼저 Vertica 외부 저장 프로시저로 작용하면서 esProc은 원래 하드 코딩이 필요한 모든 계산을 맡아 처리하므로 구현이 단순해지며 더 효율적입니다. 둘째, esProc SPL의 다중 소스 계산 능력을 통해 다양한 데이터 소스에서 직접 혼합 계산을 수행하여 모든 데이터를 하나의 데이터베이스로 로드할 필요가 없게 되어 데이터가 실시간이 되고 효율이 높아지며, Vertica를 가볍게 유지할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 예시들을 통해 esProc SPL의 적용 가능한 시나리오와 효과에 대해 기본적으로 이해했습니다. 물론 여기서는 주어지지 않은 더 많은 케이스들이 있습니다.\n\n# 왜 esProc SPL이 더 잘 작동하는가\n\n위의 예시들에서 우리는 코드 효율성과 컴퓨팅 성능 측면에서 SPL이 SQL보다 명확한 장점을 가지고 있다는 것을 확인할 수 있습니다. 왜 그럴까요? SPL에는 특별한 점이 있나요?\n\n하지만, 질문은 오히려 다른 방향에서 물어봐야 합니다. 즉, SQL이 잘 작동하지 않는 이유는 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSQL로 작성하는 것이 어려운 이유를 설명하는 예제부터 시작해 보겠습니다: 주식이 최대 연속 상승을 유지하는 일 수를 계산해 보겠습니다.\n\n```js\nSELECT MAX(ContinuousDays)\nFROM (SELECT COUNT(*) ContinuousDays\n    FROM (SELECT SUM(UpDownTag) OVER ( ORDER BY TradeDate) NoRisingDays\n            FROM (SELECT TradeDate,\n                    CASE WHEN Price\u003eLAG(price) OVER ( ORDER BY TradeDate)\n                        THEN 0 ELSE 1 END UpDownTag\n                FROM Stock ) )\n        GROUP BY NoRisingDays )\n```\n\nSQL은 네 단계로 중첩된 코드를 채택하고 있어서 전반적으로 더 복잡합니다. 이 문제는 저희 회사의 채용 시험 문제로 사용되었었는데, 합격률은 20% 미만이었습니다. 이 문제가 너무 어려웠기 때문에 우리는 수정했습니다: 명령문을 제공하고 목적을 찾도록 요청했지만, 아쉽게도 합격률은 여전히 높지 않았습니다. 이것이 우리에게 말해주는 것은 무엇일까요? 복잡한 계산 로직이 조금이라도 어려워지면, SQL을 이해하고 작성하기 어려워진다는 것을 말해줍니다!\n\n사실, 이 문제는 자연스러운 사고를 따라 간단한 단계로 쉽게 해결할 수 있습니다: i) 거래일별로 레코드를 정렬하기; ii) 연속 상승과 하락의 간격을 얻기; iii) 최대 연속 상승 간격의 일 수를 세기. 그러나 SQL은 순서화된 작업을 충분히 지원하지 않으며 순서화된 그룹핑을 직접 제공하지 않기 때문에 다중 중첩 및 우회적인 방식을 사용해야 하며, 결과적으로 SQL 코드를 이해하고 작성하는 데 어려움을 겪게 됩니다. 이 계산 문제는 드문 문제가 아니며, 현실에서 더 복잡한 계산이 많이 있으며, 수천 줄이 넘는 SQL 코드들이 정확히 이러한 문제를 해결하기 위해 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 빠른 실행이 어려운 이유를 설명하는 또 다른 예제를 살펴보겠습니다: 1억 개의 데이터 행 중 상위 10개 가져오기.\n\n```js\nSELECT TOP 10 * FROM Orders ORDER BY Amount DESC\n```\n\n이 코드가 복잡하지 않지만 ORDER BY 키워드가 포함되어 있어 모든 데이터에 대한 대규모 정렬을 먼저 수행하고 TOP 10을 가져오게 됩니다. 대규모 정렬은 메모리와 외부 저장소 간의 여러 번의 교환을 수반하기 때문에 매우 느립니다. 이 문은 얕은 의미에 따라 실행된다면 효율성이 매우 낮을 것입니다. 사실 완전한 정렬 없이 훨씬 빠른 방법이 있습니다. 단지 10개의 가장 큰 수를 포함하는 집합을 유지하면 되며 데이터를 한 번만 통과하여 결과를 얻을 수 있습니다. 그러나 SQL은 이러한 알고리즘을 설명할 수 없습니다. 이 경우 데이터베이스의 옵티마이저만 의존할 수 있습니다. 간단한 계산의 경우 대부분의 데이터베이스가 실제로 최적화할 수 있으며 완전한 정렬을 수행하지 않아 빠르게 계산할 수 있습니다. 그러나 계산이 복잡해지면 최적화 엔진은 혼란스러워지고 느린 알고리즘을 실행해야 합니다. 예를 들어 다음 코드는 그룹 내 상위 10개를 가져오기 위한 것입니다:\n\n```js\nSELECT * FROM (\n      SELECT *, ROW_NUMBER() OVER (PARTITION BY Area ORDER BY Amount DESC) rn \n      FROM Orders ) \nWHERE rn\u003c=10\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 SQL 코드와 이전 것 사이에는 큰 차이가 있어요. 이 코드는 서브쿼리와 윈도우 함수를 사용하여 간접적인 방법으로 계산을 구현해야 합니다. 이러한 복잡한 계산을 위해 데이터베이스의 최적화 엔진이 최적화할 수 없어 정렬을 수행해야 하므로 매우 느립니다. 실제 테스트에서 Oracle에서 그룹화된 하위 집합의 TopN을 계산하는 것이 전체 집합의 TopN을 계산하는 것보다 21배 느리다는 것을 발견했습니다. 우리는 그룹 내 TopN을 계산할 때 성능이 약간만 떨어져야 하는 것으로 생각했지만, 실제 결과는 우리가 기대한 것과는 거리가 멀었습니다. 따라서 Oracle은 아마도 그룹 내 TopN을 계산할 때 정렬을 수행하여 성능이 급격히 저하되었을 것으로 생각되며, 이로 인해 옵티마이저가 실패한 것입니다.\n\n그렇다면 SPL은 이를 어떻게 해결할까요?\n\n첫 번째 예제에 대한 해결책은 다음과 같습니다:\n\n```js\nStock.sort(TradeDate).group@i(Price\u003cPrice[-1]).max(~.len())\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSPL은 순서대로 그룹화를 제공합니다. 구현 아이디어는 이전 SQL 코드와 같지만 매우 간결하게 표현됩니다.\n\n두 번째 예제의 경우:\n\n![이미지](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_4.png)\n\nSPL은 TopN을 세트를 반환하는 집계 연산으로 간주하여 전체 정렬을 피합니다. 전체 집합 또는 그룹화된 하위 집합의 경우 구문이 유사하며 우회적인 접근이 필요하지 않습니다. 대부분의 경우, 간단하게 작성하면 빠르게 실행됩니다. 코드를 간단하게 작성하면 빠르게 실행되고, 반대로 코드를 지나치게 복잡하게 작성하면 빠르게 실행되지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다른 유사성을 찾아서 1+2+3+...+100을 계산해보자. 일반 사람들은 가장 흔한 방법을 선택할 것이다. 즉, 1+2=3, 3+3=6, 6+4=10...; 반면에 가우스는 1+100=101, 2+99=101..., 50+51=101로 발견하고 총 50개의 같은 합(101)이 있음을 발견했고 이를 통해 50에 101을 곱해서 결과를 빠르게 구할 수 있었다. 우리가 초등학교에서 이를 읽은 적이 있기 때문에 이 이야기가 누구에게나 새로운 것은 아닐 것이다. 우리는 가우스가 이처럼 뛰어난 해결책을 떠올렸다고 생각하지만, 우리는 하나의 사실을 넘어가는 것이 쉽다는 것을 잊기 쉽다: 가우스 시대에는 이미 곱셈이 존재했다. 우리는 곱셈이 덧셈보다 나중에 만들어진 것을 알고 있으며 곱셈이 그 시대에 발명되지 않았다면 가우스가 얼마나 똑똑하더라도 이 문제를 해결할 방법을 이렇게 빨리 찾을 수 없었을 것이다. 이제 우리 주제로 돌아와 SQL은 덧셈만 있는 산술체계와 같다. 연속적인 덧셈 문제를 해결하고 싶다면 하나씩 더해야 하므로 코드가 길어지고 효율적인 계산이 이루어지지 않는다. 반면에 SPL은 곱셈의 발명과 같이, 코드를 단순화하고 성능을 향상시키는 기술로 볼 수 있다.\n\nSQL의 어려움은 관계 대수에서 비롯된다. 이러한 이론적 결함은 공학적인 방법으로 해결할 수 없다. SPL은 \"이산 데이터셋 모델\"에 기초하고 있으며 더 풍부한 데이터 유형 및 기본 연산을 제공하므로 보다 강력한 표현 능력이 있다.\n\n이 말은 가우스만큼 똑똑한 프로그래머만이 SPL을 사용할 수 있는 것을 의미합니까?\n\n사실은 그렇지 않습니다. SPL은 일반 프로그래머들을 위해 만들어진 것이며 대부분의 경우, 자연스러운 사고를 따르기만 하면 올바른 코드를 작성할 수 있습니다. 반면에 SQL을 사용하면 계산이 다소 복잡해지면 우회적인 방법으로 구현해야 할 때가 많아서 숙련된 프로그래머가 아니라면 해결하기 어려울 수 있습니다. 이 의미에서 SPL은 SQL보다 간단합니다. 그러나 SPL을 마스터하고 싶다면 더 배워야 합니다. 걱정 마세요, SPL 관련 지식에 대해 이미 어느 정도 배운 것이 있습니다. 나머지를 모르는 것은 똑똑한 프로그래머가 이미 지식 포인트(많지 않음)를 요약했기 때문에 그것들을 배우기만 하면 됩니다. 이러한 지식을 마스터하면 복잡한 문제를 손쉽게 해결할 수 있을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n실제로 SQL로 처리할 수 없는 많은 시나리오가 있습니다. 몇 가지 예제를 살펴보겠습니다:\n\n전자 상거래 분야의 사용자 행동 변환을 위한 퍈널 분석에서는 각 이벤트(페이지 브라우징, 검색, 장바구니에 담기, 주문하기 및 결제) 후의 사용자 이탈률을 계산해야 합니다. 이 분석은 이러한 이벤트가 특정 시간 창 안에서 완료되고 특정 순서로 발생할 때에만 효과적입니다. SQL에서 이러한 복잡한 순서 관련 계산을 설명하는 것은 번거롭습니다. 코드를 작성해도 효율적이지 않을 뿐더러 최적화하는 일은 훨씬 어렵습니다.\n\n상기 언급한 복잡한 다단계 배치 잡 케이스에서는 대용량 데이터에서 일부 복잡한 절차를 커서(커서)의 도움으로 수행해야 합니다. 그러나 커서를 읽는 것은 느리며 병렬로 계산할 수 없어 컴퓨팅 자원이 낭비됩니다. 다단계 절차적 계산 중에는 중간 결과를 반복해서 버퍼링해야 하며 지정된 시간 창 내에 배치 잡을 완료하지 못하는 매우 낮은 효율성과 실패를 초래합니다.\n\n대용량 데이터에서 다중 인덱스 계산을 수행할 때 한 번에 수백 개의 인덱스를 계산하고 상세 데이터를 여러 차례 사용해야하며 연관이 또한 관련될 때, SQL에서는 데이터를 반복적으로 탐색해야 합니다. 큰 테이블 연관, 조건 필터링, 그룹화 및 집계, 중복 제거 등의 혼합 계산이 포함되며 높은 동시성으로 실시간 계산도 필요합니다. 이러한 계산을 SQL에서 구현하는 것은 어려운 일입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n…\n\n여유 공간의 제한 때문에 전자 상거래 퍈널 계산을 위한 SQL 코드만 아래에 제시되었습니다. 코드를 보시면 복잡성을 느끼실 수 있을 겁니다.\n\n```js\nwith e1 as (\n select uid,1 as step1,min(etime) as t1\n from event\n where etime\u003e= to_date('2021-01-10') and etime\u003cto_date('2021-01-25')\n and eventtype='eventtype1' and …\n group by 1),\ne2 as (\n select uid,1 as step2,min(e1.t1) as t1,min(e2.etime) as t2\n from event as e2\n inner join e1 on e2.uid = e1.uid\n where e2.etime\u003e= to_date('2021-01-10') and e2.etime\u003cto_date('2021-01-25')\n and e2.etime \u003e t1 and e2.etime \u003c t1 + 7\n and eventtype='eventtype2' and …\n group by 1),\ne3 as (\n select uid,1 as step3,min(e2.t1) as t1,min(e3.etime) as t3\n from event as e3\n inner join e2 on e3.uid = e2.uid\n where e3.etime\u003e= to_date('2021-01-10') and e3.etime\u003cto_date('2021-01-25')\n and e3.etime \u003e t2 and e3.etime \u003c t1 + 7\n and eventtype='eventtype3' and …\n group by 1)\nselect\n sum(step1) as step1,\n sum(step2) as step2,\n sum(step3) as step3\nfrom\n e1\n left join e2 on e1.uid = e2.uid\n left join e3 on e2.uid = e3.uid\n```\n\n이것은 세 단계의 퍼널 계산입니다. SQL은 순서와 관련된 계산을 부족하게 가지고 있으며 완전히 집합 지향적이지 않습니다. 여러 하위 쿼리로 우회하고 반복적으로 JOIN해야하는 등 쓰기와 이해하기 어렵고, 성능이 낮으며 최적화하기 어렵습니다. 여기에는 세 단계 퍼널만 제시되었지만 더 많은 단계가 필요한 경우 더 많은 서브쿼리를 추가해야 하므로 난이도가 높아집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대조적으로, SPL 코드는 훨씬 간단합니다:\n\n![이미지](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_5.png)\n\nSPL은 주문 관련 계산을 제공하며 보다 체계적인 집합 중심적입니다. 코드는 자연스러운 생각에 따라 직접 작성되어 간단하고 효율적입니다. 또한, 이 코드는 단계 수에 상관없이 퍼널을 처리할 수 있으며 수정해야 하는 것은 매개변수 뿐입니다.\n\n이것은 단순화된 실제 사례입니다 (원래 SQL 코드는 거의 200줄이 있었습니다). 사용자는 Snowflake의 중간 서버(4*8=32코어와 동등)에서 3분 후에 결과를 얻지 못했지만, 12코어, 1.7G 저사양 서버에서 SPL 코드를 실행하여 20초 미만에 결과를 얻었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서 언급했듯이, SPL은 덧셈을 기반으로 한 곱셈의 발명과 동등하며 사실, SPL은 여러 \"곱셈\"을 만들어 냅니다. 아래에는 SPL의 고성능 알고리즘 중 일부가 나와 있으며, 그 중 많은 알고리즘은 SPL의 독자적인 발명품입니다.\n\n![이미지](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_6.png)\n\n예를 들어, 다목적 순회 알고리즘은 한 번의 순회로 여러 작업의 효과를 달성할 수 있으며, 포인터로 사용되는 외래 키는 외래 키 필드를 외래 키가 가리키는 레코드의 주소로 매핑하여 이 레코드를 다시 사용할 때 더 효율적으로 만들어 줍니다. 더블 증분 분할은 급격히 확장되는 데이터 규모에 적응하여 데이터를 저장하고 액세스하는 데 매우 효율적으로 만들어 줍니다.\n\n더 많은 정보: [성능 최적화 — 서문](링크)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러면, Java도 잘 작동하지 않는 이유는 무엇인가요?\n\n이전에 언급했듯이, Java는 너무 원시적이며 필요한 데이터 유형과 계산 라이브러리가 부족하기 때문에 모든 계산 작업을 처음부터 해야 하며 이는 매우 번거롭습니다. 예를 들어, Java에서 그룹화 및 집계 작업을 구현하려면 10줄 이상의 코드가 필요합니다. Java 8의 Stream이 이러한 유형의 작업을 어느 정도 단순화하지만 약간 복잡한 계산에 대해 여전히 구현하기가 매우 어렵습니다(SQL과 비교하면 큰 차이가 있음).\n\nJava에서는 성능 요구 사항이 높은 계산을 위해 코드를 작성하는 것이 더 어렵습니다. 예를 들어, 큰 정렬 알고리즘을 사용하지 않는 TopN 작업, 더 효율적인 HASH 조인 알고리즘, 정렬 병합 알고리즘 등이 있습니다. 이러한 알고리즘 자체를 구현하는 것이 어렵고 Java는 계산 라이브러리가 부족하기 때문에 많은 응용 프로그래머들이 이를 어떻게 해결해야 하는지 알지 못하고 상대적으로 간단하지만 느린 알고리즘을 사용해야 합니다. 결과적으로, 계산 속도는 SQL보다 느리고 이러한 문제를 해결하는 것조차 어렵습니다.\n\n큰 데이터를 처리하는 성능은 대부분 데이터 IO와 관련이 있습니다. IO 비용이 너무 높으면 연산 속도가 빨라도 작동하지 않습니다. 효율적인 IO는 주로 특별히 최적화된 저장 체계에 의존합니다. 그러나 안타깝게도 Java는 널리 사용되는 효율적인 저장 체계가 없으며 일반적으로 텍스트 파일이나 데이터베이스를 사용하여 데이터를 저장합니다. 데이터베이스 인터페이스의 성능은 매우 나쁩니다. 텍스트 파일은 약간 나아지지만 데이터 유형을 구문 분석하는 데 너무 많은 시간이 소요되어 성능이 낮아집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 실제로 Java의 단점이 고려된다면, 핫 스왑이 어렵고 강한 결합력 등 Java는 이전 문제를 해결하기 위해 SQL을 넘어설 수도 없는 상태에 미치지 못할 것입니다.\n\n그렇다면 왜 Python은 여전히 잘 작동하지 못하나요?\n\nJava를 분석하면 Python의 많은 단점이 유사하다는 것을 기본적으로 알 수 있습니다. 예를 들어, 인접 참조, 정렬된 그룹화, 위치 계산, 비등치 그룹화와 같이 비교적 복잡한 계산을 구현하기 어렵다는 것입니다.\n\n외부 저장 처리 메커니즘 없이 대용량 데이터 처리를 위해 Python에서 구현하는 것은 매우 어려울 것입니다. 게다가 Python은 진정한 병렬 처리를 지원하지 않습니다. Python 자체의 병렬 처리는 사실상 가짜로, 실제로는 CPU에 대한 직렬 처리이거나 직렬 처리보다 더 느리기 때문에 현대 멀티 코어 CPU의 장점을 활용하기 어려울 수 있습니다. Python의 주요 인터프리터인 CPython 인터프리터에는 Global Interpreter Lock이 있습니다. 이 잠금은 Python 코드를 실행하기 전에 얻어야 하며, 즉 여러 CPU 스레드가 동시에 작업하는 경우에도 하나의 스레드만 코드를 실행하고 여러 스레드는 번갈아가며 코드를 실행할 수밖에 없습니다. 그러나 여러 스레드 실행은 문맥 전환 및 잠금 메커니즘 처리와 같은 복잡한 트랜잭션을 포함하므로 성능은 향상되지 않고 오히려 감소됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파이썬은 한 프로세스에서 간단한 멀티스레드 병렬 처리 메커니즘을 활용할 수 없어, 많은 프로그래머들이 복잡한 멀티프로세스 병렬 처리 방법을 채택해야 합니다. 프로세스 자체의 비용과 관리가 훨씬 더 복잡하며, 병렬 처리의 효과는 여러 스레드보다 비교할 수 없이 낮습니다. 게다가 프로세스 간 통신 또한 매우 복잡합니다. 때로는 직접 통신을 포기하고 집계 결과를 전달하기 위해 파일 시스템을 사용해야 하며, 이는 상당한 성능 저하로 이어집니다.\n\n자바와 같이, 파이썬은 고성능 컴퓨팅을 위한 효율적인 저장 체계를 제공하지 않아, 오픈 포맷 파일이나 데이터베이스를 사용해야 하므로 성능이 낮아집니다. 많은 경우 파이썬은 SQL과 함께 사용되지만, SQL의 문제를 해결하지 못합니다. 파이썬의 버전 및 통합 등의 문제를 고려한다면, 파이썬이 실제로 작동하지 않는다는 결론에 도달할 수 있습니다.\n\n# 기술적 특성\n\n위에서 언급한대로, esProc SPL에서 코딩이 간단하고 실행이 빠른 이유를 설명했습니다. 즉, SPL의 저 코드 및 고 성능입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금 esProc SPL의 기술적 특성을 좀 더 자세히 살펴보겠습니다.\n\n![이미지](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_7.png)\n\n현재 esProc는 순수한 Java로 개발된 소프트웨어이며 JDK1.8 이상 버전의 JVM 환경에서 모든 운영 체제에서 실행할 수 있습니다. 일반적인 VM 및 컨테이너를 포함합니다.\n\n일반적인 설치 후 esProc는 1G 미만의 공간을 차지하며, 대부분은 참조된 타사 외부 데이터 소스 드라이버 패키지가 차지합니다. esProc의 핵심 패키지는 15M 미만으로 Android에서도 실행할 수 있게 만들어 줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nJVM을 제외하고 esProc은 운영 환경에 대한 다른 엄격한 요구 사항이 없습니다. 하드 디스크 및 메모리 용량 요구 사항은 컴퓨팅 작업과 관련되어 있으며 다양한 작업에 따라 크게 달라질 수 있습니다. 동일한 컴퓨팅 작업을 수행할 때, esProc은 일반 데이터베이스보다 적은 하드웨어 자원을 요구하는 경우가 많습니다 (특히 분산 데이터베이스의 경우). 메모리 용량을 늘리고, 더 높은 주파수와 코어 수를 가진 CPU를 선택하며, SSD를 활용하면 컴퓨팅 성능 향상에 긍정적인 영향을 미칠 수 있습니다.\n\n![Image](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_8.png)\n\n프레임워크 가장 왼쪽에는 업무 데이터베이스와 전통적인 데이터 웨어하우스(있는 경우)가 있습니다. 실제로 다양한 유형의 데이터 소스가 있을 수 있으며, 이러한 데이터를 \"데이터 응고\"를 통해 esProc의 고성능 파일 저장소로 변환하여 더 높은 성능을 달성할 수 있습니다.\n\n물론, 데이터 변환은 필수적이지 않습니다. 더 높은 데이터 실시간성이 필요한 시나리오의 경우, esProc은 데이터 소스가 제공하는 인터페이스(예: JDBC)를 통해 데이터를 실시간으로 읽고 계산할 수 있습니다. 그러나 esProc이 데이터 인터페이스의 성능에 개입할 수 없기 때문에, 서로 다른 데이터 소스에 대해 다른 컴퓨팅 성능이 발생할 수 있으며 성능 저하도 발생할 수 있습니다. 사용자가 실시간성과 컴퓨팅 성능 모두에 요구 사항이 있는 경우, 데이터 소스의 핫 데이터를 읽어 고정되지 않은 과거의 콜드 데이터를 esProc의 고성능 저장소로 변환하여 모든 데이터를 esProc의 혼합 컴퓨팅 능력을 활용하여 동시에 처리할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nesProc의 고성능 파일 저장소에 대해 아래에서 자세히 소개하겠습니다.\n\n프레임워크 중간 부분에는 실제 데이터 처리를 담당하는 esProc Server가 있습니다. esProc Server를 사용하면 분산 방식으로 여러 노드를 배치하여 클러스터 컴퓨팅을 지원하며, 부하 분산 및 오류 허용 메커니즘을 지원합니다. esProc의 클러스터 크기는 비교적 작으며(32개 노드 이하), 리소스가 계산용(관리 및 스케줄링이 아님)으로 사용됩니다. 많은 경우에 구현된 것처럼 esProc는 기존 기술(MPP/HADOOP)의 클러스터 컴퓨팅 시나리오를 단일 노드를 통해 처리하며 성능이 더 우수하여 계산 능력이 부족할 걱정이 없습니다.\n\nesProc의 SPL 스크립트는 각 클러스터 노드에 배포되며, 스크립트는 esProc의 IDE에서 원격으로 개발 및 디버깅할 수 있으며 디버깅 중에 데이터는 로컬 노드로 저장되거나 다운로드되지 않으므로 보안상 더 안전합니다.\n\nesProc은 JDBC/RESTful와 같은 표준 인터페이스를 캡슐화하여 애플리케이션에서 호출할 수 있습니다. Java 애플리케이션의 경우 esProc는 직접 통합되어 애플리케이션 내부에서 계산 엔진으로 사용될 수 있습니다. Java가 아닌 애플리케이션의 경우 다른 인터페이스를 통해 esProc를 호출할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n전체 프레임워크 관점에서 보면, esProc은 간단하고 효율적인 데이터 처리 능력을 제공하는 것에 더해, 독립적으로 또는 통합하여 사용할 수 있는 유연성이 뛰어납니다. 더불어 비교적 가벼운 연산 방식으로 esProc은 가벼운 사용감을 제공하며, 전통적인 분산 기술처럼 무겁지 않습니다.\n\n개발 및 디버깅 측면에서 esProc SPL은 간단하고 쉽게 사용할 수 있는 개발 환경을 제공합니다.\n\n![image](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_9.png)\n\nIDE에서는 단계별로 코드를 작성하고, 오른쪽에 있는 결과 패널에서 각 단계의 실행 결과를 실시간으로 확인할 수 있습니다. 또한 디버그, 스텝인, 중단점 설정 등 다양한 편집 및 디버깅 기능이 있습니다. 사용하기 쉬운 편집 및 디버깅 기능은 저코딩을 위해 꼭 필요하며, SQL (및 저장 프로시저)과는 매우 다르며 개발 비용을 크게 절감할 수 있습니다. 이러한 기능들로 esProc SPL은 데스크탑 분석에 자주 사용되며 매우 편리합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSPL은 특별히 설계된 구문 시스템으로, 점진적 계산을 자연스럽게 지원하며 복잡한 프로세스 작업에 특히 적합합니다. 또한 SPL은 SQL보다 루프, 분기, 프로시저, 서브프로그램과 같은 보다 완전한 프로그래밍 능력을 자랑합니다. 각 작업 단계에서 SPL은 변수를 정의하지 않고 이전 단계의 결과를 셀 이름으로 참조할 수 있습니다 (물론 변수도 지원됩니다).\n\n![Image](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_10.png)\n\n게다가, SPL은 매우 다양한 구조화된 데이터 계산 라이브러리를 제공하여 문자열, 날짜 및 시간 처리, 수학적 계산, 파일 및 데이터베이스 읽기/쓰기, JSON/XML 다층 데이터 지원, 그룹화/루프/정렬/필터링/연관/집합/순서 있는 계산을 수행할 수 있습니다. 특히 시퀀스(테이블 시퀀스)를 위해 제공된 루프 함수는 집합 연산을 크게 단순화할 수 있습니다. SPL은 또한 대량 데이터 계산을 위한 커서, 하드 디스크 반복 탐색을 줄이는 채널, 병렬 및 분산 컴퓨팅 메커니즘을 제공합니다. 뿐만 아니라, SPL은 AI를 위한 모델링 및 예측 함수와 MongoDB, Elasticsearch, HBase, HDFS, Influxdb 등 수십 가지 데이터 소스를 위한 외부 라이브러리 함수를 제공합니다.\n\n현재 SPL은 400개 이상의 함수를 제공하며 각 함수에는 여러 옵션이 포함되어 있습니다. 이는 수천 개의 라이브러리 함수에 해당합니다. 이러한 함수들은 프로시저, 루프, 분기와 같은 완벽한 프로그래밍 언어 함수와 함께, SPL이 다양한 데이터 처리 작업을 수행할 수 있도록 합니다. 이는 SPL의 다재다능성 특징의 전형적인 표현입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nesProc SPL은 매우 높은 통합성을 자랑합니다. Java로 개발된 SPL은 독립적으로 실행되거나 응용 프로그램에 심층적으로 통합되어 인애플리케이션 컴퓨팅 엔진으로 작동할 수 있으며, 마이크로 서비스, 엣지 컴퓨팅, 보고서 데이터 준비와 같은 시나리오에서 중요한 역할을 수행할 수 있습니다.\n\n[이미지](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_11.png)\n\n뛰어난 통합성은 가벼운 특징을 반영합니다. esProc SPL은 항상 독립된 서버가 필요하지 않습니다(데이터베이스와는 매우 다름), 대신 JAR 파일을 임베딩하는 것만으로 응용 프로그램에 강력한 컴퓨팅 성능을 제공할 수 있습니다. 게다가 JAR 파일은 수십 메가바이트 크기만으로 매우 작고 가벼우며, 언제 어디서든 사용할 수 있습니다. 심지어 Android 폰에서도 실행할 수 있습니다.\n\nesProc SPL은 수십 가지 데이터 소스를 지원하며 혼합 컴퓨팅 능력을 자랑합니다. 다수의 데이터 소스를 데이터베이스에 데이터를로드하지 않고 직접 계산할 수 있습니다. 실시간 데이터 처리 뿐만 아니라 다양한 데이터 소스의 장점을 완전히 유지할 수 있습니다. 예를 들어, RDB는 계산 능력이 뛰어나지만 IO 효율이 낮습니다. RDB에 일부 계산을 수행한 후 SPL이 나머지 계산을 수행하게 할 수 있습니다. MongoDB는 동적 다층 데이터를 저장하기에 적합하며, SPL을 사용하여 다층 데이터를 직접 처리할 수 있습니다. 파일 시스템은 읽기 및 쓰기 효율이 더 높을 뿐만 아니라 사용이 더 유연합니다. SPL은 파일을 기반으로 직접 계산할 수 있고, 병렬 컴퓨팅의 효과를 최대로 발휘할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_12.png)\n\nesProc SPL은 여러 데이터 소스를 지원하여 그 유연성을 한 번 더 보여줍니다. 또한, esProc SPL에 메타데이터가 없기 때문에 여러 데이터 소스에 직접 액세스할 수 있고 혼합 계산도 수행할 수 있으며, 따라서 esProc SPL은 가벼워졌습니다. 이는 한 번 더 그 가벼운 특성을 보여주는 것입니다.\n\n현재, esProc SPL은 다음과 같은 데이터 유형을 처리할 수 있습니다:\n\n- 구조화된 텍스트: txt/csv\n- 일반 텍스트, 문자열 분석\n- 엑셀 파일의 데이터\n- 다층 구조의 텍스트: json, xml\n- 구조화된 데이터: 관계형 데이터베이스\n- 다층 구조 데이터: bson\n- KV 유형 데이터: NoSQL\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특히 esProc은 json 및 xml과 같은 다층 구조 데이터를 강력하게 지원하여 전통적인 데이터베이스를 크게 능가합니다. 따라서 esProc은 mongodb와 kafka와 같은 json과 유사한 데이터 소스와 잘 작동하며, HTTP/Restful 및 마이크로서비스와 데이터를 쉽게 교환하고 계산 서비스를 제공할 수 있습니다.\n\n또한, esProc은 Excel 파일에서 데이터를 쉽게 계산할 수 있습니다. 그러나 esProc은 Excel의 형식 처리에 능숙하지 않으며, 이미지, 오디오 및 비디오와 같은 데이터 처리도 잘하지 못합니다.\n\n뿐만 아니라, esProc SPL은 자체 효율적인 데이터 파일 저장을 제공합니다. 개인 데이터 형식은 높은 성능뿐만 아니라 비즈니스 범주별로 데이터를 파일 시스템 트리 디렉토리에 저장할 수 있도록 합니다.\n\n현재 SPL은 bin 파일 및 복합 테이블 두 가지 파일 저장 형식을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBin 파일은 기본적인 이진 데이터 형식으로, 압축 기술을 채택하여 공간 점유도가 낮고 빠른 읽기를 제공합니다. 데이터 타입을 저장하여 데이터 형식을 구문 분석할 필요가 없어 더 빠른 읽기가 가능하며, 증가형 분할 메커니즘을 지원하여 데이터를 추가할 수 있어 병렬 컴퓨팅을 통한 구분 전략을 구현하기 매우 쉽습니다. 이를 통해 컴퓨팅 성능을 향상시킬 수 있습니다.\n\n복합 테이블은 더 복잡한 저장 구조를 제공합니다. 특히 혼합 행 방식 및 열 방식 저장을 지원하며, 정렬된 저장이 압축률과 위치 지정 성능을 향상시킵니다. 더 효율적인 지능형 인덱스를 지원하며, 기본 테이블 및 하위 테이블의 통합을 지원하여 저장 공간과 연관을 효과적으로 줄입니다. 또한 증가형 분할 메커니즘을 지원하여 병렬 컴퓨팅을 구현하기 쉬워지며 컴퓨팅 성능을 향상시킬 수 있습니다.\n\n파일 기반 저장은 데이터베이스를 필요로하지 않고 메타데이터가 없기 때문에 SPL의 사용이 더 유연하고 더 효율적이며 가벼우며 저렴하여 빅데이터 시대의 요구에 보다 적합합니다.\n\n![이미지](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_13.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nesProc는 전통적인 데이터 웨어하우스의 \"데이터 저장소(warehouse)\" 개념이 없으며, 메타데이터 개념도 없습니다. 또한, 일정한 주제의 데이터를 통합 관리하는 개념이 존재하지 않습니다. esProc에서 \"데이터베이스 내부\"나 \"데이터베이스 외부\"라는 개념도 없고, \"데이터를 데이터베이스로 가져오기(import)\"하거나 \"데이터베이스에서 가져오기(export)\"하는 동작도 없습니다.\n\nesProc는 접근 가능한 모든 데이터 원본을 esProc의 데이터로 취급하고 직접 계산할 수 있습니다. 데이터를 계산하기 전에 데이터베이스에 가져오는 작업은 필요하지 않으며, 계산 후 의도적으로 데이터베이스로 내보내는 작업도 필요하지 않습니다. 결과는 해당 인터페이스를 통해 대상 데이터 원본에 기록할 수 있습니다.\n\nesProc는 일반 데이터 원본에 대한 접근 인터페이스를 캡슐화하며, 다양한 데이터 원본은 기본적으로 동일한 논리적 상태를 갖습니다. 유일한 차이점은 서로 다른 데이터 원본이 다른 접근 인터페이스를 가지며, 서로 다른 인터페이스에는 서로 다른 성능이 있습니다. 이러한 인터페이스들은 데이터 원본 공급 업체에 의해 제공되므로, esProc는 이러한 기능과 성능에 개입할 수 없습니다.\n\nesProc는 데이터를 저장하기 위해 특수 형식 파일 (바이너리 파일 및 복합 테이블)을 설계하고 더 많은 기능과 성능을 달성하기 위해 이 파일들을 파일 시스템에 저장합니다. 이러한 파일들은 기술적으로 esProc가 소유하지 않으며, 파일 포맷은 공개되어 있으며(접근 코드가 오픈 소스로 공개됨), 이러한 데이터 파일에 액세스할 수 있는 모든 응용 프로그램은 공개된 사양 또는 오픈 소스 코드를 통해 이를 읽고 쓸 수 있습니다. 물론 SPL에서 직접 읽고 쓰는 것이 더 편리합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 의미에서 esProc와 외부 데이터 원본 간의 데이터 교환 시 \"데이터베이스로 가져오기\" 또는 \"데이터베이스에서 내보내기\"와 같은 작업이 없습니다. 그러나 외부 데이터를 esProc 형식 파일로 변환하여 더 많은 기능과 더 나은 성능을 얻거나 esProc 형식 파일을 외부 데이터로 변환하여 다른 응용 프로그램에서 사용하는 것과 같이 데이터 변환 작업이 발생할 수 있습니다. 이러한 모든 변환 작업은 SPL을 사용하여 수행할 수 있습니다.\n\n![image](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_14.png)\n\n원칙적으로 esProc는 데이터를 관리하지 않으며 데이터 보안에 대한 책임이 없습니다. 어느 정도로는 esProc에 보안 메커니즘이 없고 필요하지 않다고 말할 수 있습니다.\n\n지속적인 데이터의 보안은 원칙적으로 데이터 원본의 책임입니다. esProc 형식 파일의 경우, 많은 파일 시스템이나 VM이 완전한 보안 메커니즘(액세스 제어, 암호화 등)을 제공하며 이를 직접 활용할 수 있습니다. esProc의 클라우드 버전은 또한 계산하기 전에 S3와 같은 객체 저장 서비스에서 데이터를 검색하고 이들의 보안 메커니즘을 활용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내장된 esProc은 주 Java 애플리케이션과 동일한 프로세스에서 실행되며 주 애플리케이션에 대한 컴퓨팅 서비스만 제공합니다. esProc은 외부 서비스 인터페이스를 제공하지 않으므로 보안 및 권한 문제가 없습니다. 독립적인 서비스 프로세스의 esProc은 표준 TCP/IP 및 HTTP를 사용하여 통신하며 전문적인 네트워크 보안 제품을 통해 모니터링 및 관리할 수 있으며 구체적인 보안 조치는 이러한 제품의 책임입니다.\nesProc은 계산에 특화되어 있으며 영속적 저장의 신뢰성에 대한 책임이 없습니다. 이에 관련된 전문 기술 및 제품도 있습니다. esProc은 이러한 기술 및 제품과 함께 작동할 수 있도록 표준 규격을 준수하려고 노력합니다. 예를 들어, 데이터는 매우 신뢰할 수 있는 객체 저장소에 영속화될 수 있고, 이를 위해 esProc은 해당 인터페이스를 제공하여 이러한 데이터 원본에 액세스하여 계산을 수행할 수 있도록 합니다.\n\nesProc은 전문적인 컴퓨팅 기술이지만 전문적인 보안 기능을 제공하지는 않습니다. esProc의 철학은 다른 전문 보안 기술과 함께 작동하는 것입니다.\n\n위에서 esProc SPL의 몇 가지 기술적 특성을 소개했습니다. 이제 더 많은 시나리오에서 적용된 esProc SPL 솔루션을 살펴봅시다.\n\n# 더 많은 솔루션\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터 주도형 마이크로서비스 구현\n\n마이크로서비스는 응용 프로그램 단에서 데이터 처리가 필요하며, 관련 처리 기술이 필요합니다. 데이터베이스가 강력한 컴퓨팅 능력을 가지고 있지만, 응용 프로그램 단에 포함되기 어려워 하드 코딩이 종종 필요합니다. Java/ORM은 충분히 구조화된 컴퓨팅 라이브러리가 부족하여 데이터 처리 개발이 어려워지고, 핫 스왑을 달성하지 못하기 때문에 마이크로서비스의 요구 사항을 충족하기 어렵습니다.\n\n마이크로서비스에서 데이터 계산을 구현하기 위해 Java/ORM을 SPL로 교체하면 이러한 문제를 효율적으로 해결할 수 있습니다. SPL은 풍부한 컴퓨팅 라이브러리와 민첩한 구문을 갖추어 개발을 크게 단순화할 수 있습니다. SPL은 오픈 시스템이며, 실시간으로 어떤 소스의 데이터도 처리할 수 있습니다. SPL은 해석 실행되어 핫 스왑을 자연스럽게 지원합니다. 뛰어난 알고리즘과 병렬 메커니즘을 갖춘 SPL은 컴퓨팅 성능을 보장합니다. 따라서 SPL은 마이크로서비스에서 이상적인 컴퓨팅 엔진입니다.\n\n더 많은 정보를 원하신다면: Open-source SPL Rings down the Curtain on ORM를 방문해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 저장 프로시저 교체\n\n저장 프로시저의 단점들은 오랜 역사를 갖고 있습니다. 특히, 저장 프로시저는 수정과 디버그가 어려우며 이동성이 없습니다; 저장 프로시저를 컴파일하는 데는 높은 권한이 필요하여 보안 문제를 일으킵니다; 여러 애플리케이션이 공유된 저장 프로시저를 사용하면 애플리케이션 간의 강력한 결합을 야기할 수 있습니다. 안타깝게도, 우리는 더 나은 해결책이 없어서(하드 코딩 비용이 너무 높기 때문에) 이러한 단점들을 감내해야 합니다.\n\nSPL은 복잡한 구조화된 데이터 연산을 위해 특별히 설계되었으며, 저장 프로시저의 우수한 대체품이 될 수 있으며 외부 데이터베이스 저장 프로시저의 효과를 달성할 수 있습니다. SPL은 다단계 계산을 지원하며, 저장 프로시저와 같은 복잡한 계산에 자연스럽게 적합합니다. SPL 스크립트는 자연적으로 이동할 수 있으며, 스크립트는 데이터베이스의 읽기 권한만을 요구하며 데이터베이스 보안 문제를 일으키지 않습니다; 다른 애플리케이션의 스크립트는 서로 다른 디렉토리에 저장되어 애플리케이션 간의 결합을 일으키지 않습니다.\n\n더 많은 정보는 다음을 참조하십시오:  [안녕, 저장 프로시저 - 사랑과 미움의 대상](Goodbye, Stored Procedures — the Love/Hate Thing)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터베이스에서 중간 테이블을 제거하세요\n\n데이터베이스를 사용할 때는, 쿼리 성능을 향상시키거나 개발을 간소화하기 위해 많은 중간 테이블이 생성되며, 이 중간 테이블의 수는 시간이 지남에 따라 계속 증가합니다. 이러한 테이블은 많은 공간을 차지하여 데이터베이스가 지나치게 중복되고 부풀어 있게 만들며, 각 어플리케이션에서 동일한 중간 테이블에 대한 액세스는 강한 결합을 유발시키며 중간 테이블을 관리하기가 어렵습니다.\n\n데이터베이스에 중간 테이블을 저장하는 목적은 후속 계산에 데이터베이스의 계산 능력을 활용하는 것입니다. 위에서 언급한 문제를 해결하기 위해 중간 테이블을 데이터베이스 외부에 배치하고 파일로 저장하며, 후속 계산은 SPL에서 구현할 수 있습니다. 외부 중간 테이블(파일)은 관리가 더 쉽고, 다른 디렉토리에 저장하면 어플리케이션 간 결합 문제가 발생하지 않습니다. 이 방법을 통해 데이터베이스의 부하를 완전히 줄일 수 있으며, 데이터베이스를 배포할 필요 없이도 수행할 수 있습니다.\n\n자세한 정보는 아래 링크를 방문해 주세요: [오픈 소스 SPL: 데이터베이스로부터 수만 개의 중간 테이블을 제거합니다](https://www.example.com)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 계속되는 보고서 개발 요구 사항을 처리하세요\n\n보고서/BI 개발은 데이터 준비와 데이터 표현 두 단계로 진행됩니다. 그러나 보고서 도구/BI 도구는 표현 단계의 문제만 해결할 뿐 데이터 준비에 대해서는 아무것도 할 수 없습니다. 복잡한 데이터 준비를 위해서는 SQL/저장 프로시저/자바 하드코딩이 유일한 선택이며, 이는 개발과 유지보수가 어려우며 비용이 높습니다. 우리는 종종 계속되는 보고서 개발 요구 사항에 직면하며, 저비용으로 빠르게 대응하기가 어려운 경우가 많습니다. 개발 비용을 높이는 주된 요인은 데이터 준비입니다.\n\nSPL의 도움을 받아 보고서 표현과 데이터 소스 사이에 컴퓨팅 레이어를 추가함으로써 데이터 준비 문제를 해결할 수 있습니다. SPL은 보고서의 데이터 준비를 간단하게 만들어주며, 보고서 도구의 계산 기능 부족을 보완하여 보고서 개발 효율을 포괄적으로 향상시킵니다. 보고서의 데이터 준비가 도구로 구현되면, 보고서 표현과 데이터 준비 양쪽 모두가 저비용으로 계속되는 보고서 개발 요구 사항을 신속하게 처리할 수 있습니다.\n\n자세한 내용은 방문해주십시오: 오픈 소스 SPL이 보고서 응용 프로그램을 최적화하고 계속되는 보고서 개발 요구를 처리합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 프론트엔드 계산을 구현하기 위한 프로그래밍 가능한 데이터 라우팅\n\n너무 많은 비즈니스를 맡으면 데이터 웨어하우스가 심한 작업 부담을 겪을 수 있습니다. 따라서, 고주파 계산 작업의 일부를 프론트엜드 애플리케이션으로 옮겨 부담을 분산해야 합니다. 그러나, 프론트엔드 계산에 사용할 수 있는 기술이 거의 없습니다. 데이터베이스를 사용해 프론트엔드 계산을 구현하면 데이터 동기화 문제가 발생할 수 있습니다. 즉, 고주파 데이터만 프론트엔드 데이터베이스로 가져오면 모든 쿼리 요청을 처리할 수 없지만, 전체 데이터를 프론트엔드 데이터베이스에 복사하면 반복된 구축과 막대한 작업을 해결해야 합니다.\n\nSPL은 고주파 데이터를 SPL 파일로 변환하여 저장함으로써 이 문제를 해결할 수 있으며, SPL의 고성능 계산 능력으로 응용 프로그램에 효율적인 컴퓨팅 서비스를 제공할 수 있습니다. 또한, SPL은 지능형 데이터 라우팅 기능을 제공합니다. 응용 프로그램이 저주파 데이터를 쿼리하면 SPL이 자동으로 쿼리 요청을 데이터 웨어하우스로 라우팅하여 반복된 구축의 높은 비용을 피하고 유연하고 효율적인 컴퓨팅 능력을 낮은 구현 비용으로 최대한 발휘할 수 있습니다.\n\n더 많은 정보를 원하시면 아래 링크를 방문해주세요: Routable computing engine implements front-end database\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 혼합 계산을 통한 실시간 HTAP 구현\n\nHTAP 요구 사항은 다양한 데이터베이스에 대량의 데이터가 저장된 후에 실시간 쿼리를 수행할 수 없는 문제의 결과입니다. 실시간 쿼리를 구현할 때, HTAP 데이터베이스는 현재 다음과 같은 문제에 직면하게 됩니다: i) 기존의 프로덕션 데이터베이스가 HTAP 데이터베이스가 아니기 때문에 프로덕션 데이터베이스를 대체해야 하는데, 이는 높은 위험을 유발할 것입니다; ii) SQL의 계산 능력이 충분하지 않고, 과거 데이터가 잘 구성되지 않아 성능이 낮아지는 문제가 발생합니다; iii) 데이터베이스의 계산 능력이 다양한 데이터 소스의 장점을 활용하기에 너무 제한적이며, 모든 데이터를 하나의 데이터베이스로로 로드하는 복잡한 ETL 프로세스는 실시간 성능이 나빠진다.\n\nSPL은 다양한 데이터 소스에 대한 혼합 계산을 지원하고 실시간 분석을 달성할 자연스러운 능력을 갖고 있습니다. 계산 특성에 따라 잘 구성된 과거 추월 데이터를 파일로 저장함으로써 계산 성능을 향상시키고, 트랜잭션 핫 데이터는 여전히 프로덕션 데이터베이스에 저장되어 실시간으로 읽을 수 있습니다. 이러한 능력을 통해 SPL은 프로덕션 시스템을 변경하지 않고 효율적인 HTAP 효과를 달성할 수 있으며, 위험과 비용을 최소화할 수 있습니다. SPL은 오픈, 다중 소스 혼합 계산 능력을 통해 저위험, 고성능 및 강력한 실시간 HTAP를 지원합니다.\n\n더 많은 정보를 원하시면, 다음을 방문해 주세요: HTAP 데이터베이스는 HTAP 요구 사항을 처리할 수 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 파일 계산을 통한 레이크하우스 구현\n\n레이크하우스를 구현하기 위해서는 저장 및 계산 능력이 모두 필요합니다. 다시 말해, 노출 데이터를 완전히 보존할 수 있는 능력과 강력한 계산 능력이 필요합니다. 디비로 레이크하우스를 구현하는 것은 계산만 할 수 있고 저장은 할 수 없는 난처한 상황에 직면하게 됩니다. 디비는 강력한 제약 조건을 가지고 있어서, 비준수 데이터를 저장할 수 없어 데이터의 모든 기능을 보존할 수 없습니다. 게다가 복잡한 ETL 프로세스는 매우 비효율적입니다. 디비는 매우 강하게 닫혀있는 성격을 가지고 있어서, 디비 데이터에서만 계산을 할 수 있어 다양한 원시 데이터 원본을 직접 계산하거나 실시간 혼합 계산을 수행할 수 없습니다.\n\nSPL은 진정한 레이크하우스를 구현할 수 있습니다. 원시 데이터는 파일 시스템에 직접 저장될 수 있어 데이터의 무결성을 보존할 수 있습니다.\n\nSPL은 더욱 개방적이며, 어떤 유형의 데이터든지 직접 계산할 수 있습니다. txt, csv, json과 같은 개방형 형식 파일 데이터에 대해서 SPL은 직접 계산하고, 다른 유형의 데이터에 대해서도 실시간 혼합 계산을 수행할 수 있습니다. 데이터의 계산과 구성은 동기적으로 이루어질 수 있어 데이터를 더욱 효율적으로 활용하며 재계산 시에도 높은 성능을 유지할 수 있습니다. 단계적 접근법은 레이크하우스를 구현하는 올바른 방법입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 많은 정보를 원하시면 다음을 방문해주세요: 현재 Lakehouse는 거짓 제안과 같습니다.\n\n# FAQ\n\n# esProc은 오픈 소스 또는 데이터베이스 기술을 기반으로 하나요?\n\n위에서 주로 SQL과 같은 기존 기술의 단점을 자세히 분석했는데, 이러한 단점은 주로 그 뒤에 있는 이론 체계로 인한 것입니다. 만약 이러한 이론을 그대로 따른다면, 이러한 결점을 근본적으로 해결하는 것은 불가능합니다. 그래서 우리는 새로운 컴퓨팅 모델인 이산 데이터 집합을 발명하고, 이 모델을 기반으로 한 esProc SPL을 개발했습니다. 모든 것이 새로워서 산업에 참고할 수 있는 관련 이론과 엔지니어링 제품이 없으므로, 우리는 처음부터 개발해야 했고, 모델에서 코드까지의 모든 부분은 처음부터 만들어졌습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 에스프록은 어디에 배포할 수 있나요?\n\n에스프록은 완전히 Java로 개발되어 있기 때문에 JVM이 장착된 모든 환경에 배포할 수 있습니다. VM, 클라우드 서버, 컨테이너를 비롯한 환경에 배포할 수 있습니다. 실제로 에스프록은 독립적으로 사용하거나 응용 프로그램에 통합할 수 있습니다. 독립적으로 사용할 때는 별도의 에스프록 서버를 실행해야 하며 분산 클러스터를 구축할 수 있습니다. 응용 프로그램에 통합될 때는 jar 형식으로 삽입되어 응용 프로그램의 일부(계산 엔진)로 간주됩니다.\n\n# 어떻게 응용 프로그램이 에스프록을 호출할까요?\n\n에스프록은 표준 JDBC 드라이버를 제공하므로 Java 응용 프로그램에 직접적으로 신속하게 통합할 수 있습니다. .net/Python과 같은 Java가 아닌 응용 프로그램의 경우 ODBC/HTTP/RESTful 인터페이스를 통해 호출할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 다른 프레임워크와 esProc를 통합할 수 있을까요?\n\nesProc는 전통적인 데이터베이스처럼 독립적인 서비스 프로세스로 실행될 수 있습니다. 또한 esProc는 표준 JDBC 드라이버와 HTTP 서비스를 제공하여 애플리케이션이 호출할 수 있습니다. 따라서 JDBC를 통해 SPL 문을 보내야 하는 Java 애플리케이션에서 SPL 스크립트를 실행할 수 있습니다. esProc의 스크립트 코드를 호출하는 것은 관계형 데이터베이스의 저장 프로시저를 호출하는 것과 동등합니다. Java 애플리케이션이 아닌 경우에는 HTTP/Restful 메커니즘을 통해 esProc에서 제공하는 컴퓨팅 서비스에 액세스할 수 있습니다.\n\nJava에서 개발된 애플리케이션에서 esProc는 완전히 포함될 수 있습니다. 즉, 모든 컴퓨팅 기능이 JDBC 드라이버에 캡슐화되어 주 응용 프로그램과 동일한 프로세스에서 외부 독립적인 서비스 프로세스에 의존하지 않고 실행될 수 있습니다.\n\nesProc는 순수한 Java로 개발된 소프트웨어이기 때문에 다양한 Java 프레임워크 및 애플리케이션 서버에 완전하고 원활하게 통합될 수 있습니다. Spring, Tomcat과 같은 프레임워크에서 esProc는 사용자가 작성한 Java 애플리케이션과 동일한 논리적 상태를 갖습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n계산 유형의 프레임워크인 Spark과 같은 곳에 esProc를 완벽하게 통합할 수 있지만 실용적으로는 그다지 의미가 없습니다. esProc는 계산하기 전에 데이터를 SPL 특정 데이터 객체로 변환해야 하는데, 이는 시간이 많이 소요되며 처음부터 있던 계산 프레임워크의 데이터 객체를 무의미하게 만들어 두 유형의 데이터 객체의 장점을 결합할 수 없게 만들어 실패로 이어집니다. 이러한 계산 프레임워크의 핵심은 데이터 객체(예: Spark의 RDD)입니다. 만약 그러한 데이터 객체를 더 이상 사용할 수 없다면, 해당 계산 프레임워크 자체가 의미가 없어집니다. esProc의 계산 능력은 보통의 계산 프레임워크보다 훨씬 뛰어나기 때문에 더 이상 이러한 프레임워크를 사용할 필요가 없습니다.\n\n특히 스트림 계산 프레임워크(예: Flink)의 경우, esProc가 통합될 수 있더라도 역할을 할 수 없습니다. esProc는 독립적으로 여러 차례 스트림 계산 시나리오에 활용되어 왔고, 전혀 스트림 계산 프레임워크의 지원이 필요하지 않습니다. 동일한 계산 양에 대해 esProc는 일반적인 스트림 계산 프레임워크보다 크게 낮은 리소스를 소비하며 더 많은 기능을 제공합니다.\n\n# 기존 데이터베이스를 기반으로 esProc를 실행할 수 있나요?\n\n네, 물론 가능합니다! esProc는 데이터베이스, 텍스트, 엑셀, JSON/XML, 웹 서비스 등 수십 가지 데이터 원본을 지원합니다. 게다가, esProc는 데이터베이스와 텍스트와 같은 다른 데이터 원본 간의 연관 및 혼합 작업을 수행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 데이터 집중적 작업(대부분의 대용량 데이터 작업이 해당됨)의 경우, 데이터베이스로부터 데이터를 읽는 것은 데이터베이스의 느린 I/O 성능 때문에 많은 시간이 소요됩니다. esProc의 계산 시간이 매우 짧더라도, 전체 시간이 여전히 매우 길어져 성능 요구 사항을 충족시키지 못할 수 있습니다. 따라서 고성능을 요구하는 시나리오에서는 데이터베이스로부터 대량의 콜드 데이터를 esProc의 고성능 파일로 이동해야만 최적의 성능을 얻을 수 있습니다. 남은 작은 양의 핫 데이터는 여전히 데이터베이스에 저장되어 있을 수 있으며, esProc의 다중 소스 혼합 컴퓨팅 능력을 통해 실시간 전체 데이터 조회를 쉽게 구현할 수 있습니다.\n\n# esProc는 데이터를 어디에 저장합니까?\n\nesProc는 데이터를 파일로 저장하며, 오픈 텍스트 형식을 지원하며 고성능의 개인 파일 형식을 제공합니다. 파일은 더욱 개방적이고 유연하기 때문에 파일을 기반으로 한 고성능 저장 스키마를 설계하고 병렬 컴퓨팅을 통해 계산 성능을 향상시키기가 더 쉽습니다. esProc는 로컬 파일 시스템 및 NFS를 포함한 모든 OS의 파일 시스템을 지원합니다. 따라서 esProc는 저장 및 계산을 분리하는 능력을 자연히 갖추고 있으며, 하드 디스크를 직접 운영해야 하는 데이터베이스와 달리 저장 및 계산을 분리하는 것이 어려운 데이터베이스와는 달리 파일 시스템을 우회하여 운영할 필요가 없습니다.\n\n# esProc의 고가용성을 어떻게 보장할까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nesProc는 여러 노드가 협력하여 작동할 수 있는 분산 컴퓨팅을 지원합니다. 그러나 실제로는 대부분의 작업을 처리하고 높은 동시성 시나리오를 제외한 대부분의 작업 요청 속도를 단일 노드를 통해 충분히 처리 할 수 있기 때문에 분산 컴퓨팅이 드물게 사용됩니다.\n\nesProc의 다가오는 클라우드 에디션(사설 배포 지원)은 자동 탄력적 컴퓨팅을 지원할 것입니다. 데이터 요청량이 증가하면 새 VM이 자동으로 활성화되어 계산에 참여하고, 데이터 요청량이 감소하면 유휴 상태의 VM이 자동으로 종료됩니다.\n\n내장된 esProc는 주 애플리케이션에 대한 컴퓨팅 서비스만 제공하며 외부 서비스를 제공하지 않거나 외부 서비스의 신뢰성에 대한 책임을 지지 않습니다. 외부 서비스의 신뢰성은 주 애플리케이션과 프레임워크가 책임집니다.\n\n독립 프로세스의 esProc은 핫 스탠바이 메커니즘을 지원하며, JDBC는 현재 작동 중인 서비스 프로세스 중 부하가 적은 프로세스를 선택하여 계산을 수행합니다. esProc의 분산 컴퓨팅도 내결함성을 제공합니다. 그러나 esProc의 설계 목표가 대규모 클러스터가 아니기 때문에 계산 중 노드 오류가 발견되면 작업이 실패로 처리됩니다. esProc의 내결함성은 노드 오류가 감지되면 클러스터가 새 작업을 수용할 수 있도록 하는 것으로 제한되어, 소규모 클러스터에 적합합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nesProc의 서비스 프로세스는 현재 실패 후 자동 복구 기능을 제공하지 않으며, 실패는 관리자가 처리해야 합니다. 그러나 감시 프로세스를 설계하여 자동 기능을 구현하는 것은 어렵지 않습니다.\n\nesProc의 탄력 있는 컴퓨팅 메커니즘(클라우드 버전)은 VM을 할당할 때 현재 실패한 노드를 피함으로써 일정한 수준의 고가용성을 실현할 수 있습니다.\n\n# esProc 기능 확장 방법\n\nesProc는 Java로 작성된 소프트웨어이며, Java로 작성된 정적 함수를 호출하기 위한 인터페이스를 제공하여 esProc의 기능을 확장할 수 있습니다. 또한 esProc는 사용자 정의 함수 인터페이스를 공개하여 응용 프로그래머가 Java로 새로운 함수를 작성하고 이를 esProc로 이동하여 SPL에서 사용할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# esProc의 약점은 무엇인가요?\n\nRDB와 비교했을 때:\n\nesProc의 메타데이터 기능은 비교적 미숙합니다.\n\nesProc는 DBMS가 아니며 전통적인 의미에서 메타데이터 개념이 없으며 데이터는 대부분 파일 형태로 저장, 관리 및 사용됩니다. 대부분의 작업은 데이터 원본(파일)에 접근하여 시작되므로 데이터베이스에 비해 간단한 계산에는 약간 더 귀찮을 수 있지만 복잡한 계산에는 더 유리합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nHadoop/MPP과 비교할 때:\n\nHadoop/MPP 데이터베이스 클러스터의 규모는 비교적 큽니다 (MPP 클러스터의 노드 수에는 제약이 있을 수 있지만), 그리고 해당 클러스터의 활용, 운영 및 관리에 대한 성숙한 경험이 이미 존재합니다. 반면에, esProc 클러스터는 주로 소규모에서 중규모를 타겟팅하며, 일반적으로 몇 개에서 몇십 개의 노드를 가리킵니다. 그럼에도 불구하고, Hadoop/MPP에 비해 실용적인 경험이 부족합니다. 실제로, esProc 클러스터는 거의 사용되지 않는데, 그 이유는 고객이 종종 원래 클러스터의 효과를 달성하거나 심지어 뛰어넘을 수 있는 경우가 많아서, 이는 esProc 클러스터 활용에 대한 상대적인 경험 부족으로 이어집니다.\n\nPython과 비교할 때:\n\n우리는 현재 esProc SPL의 AI 기능을 개발하고 있으며, AI 모델링 및 예측 기능을 점차 향상시키고 있지만, 이러한 기능은 아직 Python의 다양한 AI 알고리즘 라이브러리만큼 풍부하지는 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# SPL과 SQL 간의 호환성은 어때요?\n\nesProc는 SQL 시스템의 연산 엔진이 아니며, 현재는 대량의 데이터를 포함하지 않는 간단한 SQL만 지원하며 성능을 보장할 수 없습니다. 대규모 데이터 처리 시나리오에서는 esProc가 SQL을 지원하지 않는 것으로 간주할 수 있으며 물론 어떤 SQL 저장 프로시저와도 호환되지 않을 것입니다.\n\nesProc는 앞으로 SQL을 지원하는 이중 엔진을 개발할 예정이지만, 여전히 높은 성능과 대규모 데이터 처리 요구를 충족시키기 어렵습니다. esProc로 기존 SQL 코드를 쉽게 이관하도록만 하기 때문입니다.\n\n# SQL을 SPL로 자동 변환해 주는 도구가 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSQL의 널리 사용으로, 특히 기존 시스템을 최적화할 때, SQL을 SPL로 자동 변환하여 마이그레이션 비용을 줄일 수 있는지 궁금해질 수밖에 없습니다.\n\n안타깝게도, 그럴 수 없습니다.\n\n데이터베이스는 SPL의 데이터 원본으로 사용될 수 있지만, 데이터베이스에 기반을 둔 고성능 구현은 어렵기 때문에(SQL의 저장 방식 때문에), 데이터베이스에 기반한 SQL을 SPL로 자동 변환하는 것은 불가능합니다. 더 중요한 것은, 충분한 설명 능력의 부족으로 인해, SQL은 많은 고성능 알고리즘을 구현할 수 없습니다. 이 경우 SQL을 강제로 SPL로 변환하면 기능은 보장되지만, 성능은 보통 훨씬 나쁠 수 있습니다.\n\n또한, SPL은 SQL만큼 강력한 자동 최적화 메커니즘을 제공하지 않습니다. 수십 년에 걸쳐 발전한 뒤, 많은 데이터베이스가 강력한 최적화 엔진을 갖추고 있습니다. 상대적으로 간단한 느린 SQL 문을 직면했을 때 엔진은 그들의 진정한 의도를 \"추측\"하고, 이를 자동으로 최적화하여 고성능 방식으로 실행할 수 있습니다(위에서 언급한 TopN과 같은); 반면, SPL의 자동 최적화 기능은 부족하며, 우리는 데이터베이스 공급 업체만큼 최적화 경험이 풍부하지 않기 때문에, 진술의 의도를 \"추측\"할 능력이 없으며, 코드를 직접 실행해야 합니다. 이 경우 우리는 고성능을 달성하기 위해 저복잡도 코드를 작성하는 우리 프로그래머들에게만 의존할 수밖에 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# SPL 배우기는 얼마나 어려운가요?\n\nSPL은 로우 코드와 고성능을 목표로 합니다. SPL 구문은 Java보다 쉽고 훨씬 간단하며, 몇 시간 안에 마스터할 수 있고 몇 주 안에 능숙해질 수 있습니다. 어려운 것은 최적화 알고리즘을 설계하는 것입니다. 다행히도 배우기 어렵지 않고, 우리는 그 지식 포인트들을 요약하여 고정된 루틴으로 정리했습니다. 이 루틴을 따르기만 하면, 마스터할 수 있고 숙달된 전문가가 될 수 있습니다.\n\n- SPL 프로그래밍 — 서문\n- 성능 최적화 — 서문\n- SQL과 비교했을 때 SPL이 더 어려운가요?\n\n# 성능 최적화 프로젝트를 어떻게 시작할까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대부분의 프로그래머들은 SQL로 생각하는 방법에 익숙하지만 고성능 알고리즘에 대해 친숙하지 않습니다. 그들은 한두 가지 시나리오를 통해 이해할 수 있도록 훈련받아야 합니다. 성능 최적화 루틴은 많지 않으며 (수십 개), 흔히 사용되는 루틴은 10개 미만입니다. 이러한 루틴을 경험하면 알고리즘 설계와 구현이 그리 어렵지 않다는 것을 깨닫게 될 것입니다. 처음 2-3가지 시나리오는 사용자가 우리 엔지니어의 도움을 받아 구현할 것입니다. 이 과정에서 우리는 사용자들에게 문제 해결 방법을 직접 제시하는 대신에 어떻게 문제를 해결할 수 있는지 가르쳐줄 것입니다.\n\n![이미지](/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_15.png)\n\n자세한 정보를 보려면 다음을 방문하세요: SQL (그리고 저장 프로시저) 실행 속도가 너무 느릴 때 어떻게 해야 합니까?\n\n# 요약\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내, esProc SPL의 데이터 분석 엔진으로의 장점을 요약해 보겠습니다.\n\n높은 성능\n\n실제로 esProc SPL은 대용량 데이터를 처리하는 성능이 평균적으로 기존 솔루션보다 1~2개 수준 높기 때문에 성능 우위가 매우 명백합니다.\n\n효율적인 개발\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSPL의 유연한 구문과 풍부한 컴퓨팅 라이브러리, 그리고 절차적 특성을 통해 복잡한 알고리즘을 자연스러운 사고에 따라 구현할 수 있어 개발 효율이 향상됩니다.\n\n유연하고 개방적인\n\nesProc SPL은 여러 데이터 소스의 혼합 계산을 지원하며, 다중 데이터 소스의 장점을 효과적으로 활용하면서 최소 비용으로 HTAP를 구현할 수 있습니다. 게다가, esProc SPL은 응용 프로그램에 통합되어 진정으로 개방적이고 유연합니다.\n\n리소스 절약\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n높은 컴퓨팅 능력을 지원하는 esProc SPL은 비즈니스 목표를 달성하는 데 더 적은 하드웨어로 가능합니다 (단일 기계가 클러스터와 견줄 수 있습니다), 그래서 더 환경 친화적인 기술입니다.\n\n날카로운 비용 절감\n\n이러한 장점이 비용에 반영되면 개발, 하드웨어, 운영 및 유지보수 비용을 X 배 절감할 수 있습니다.\n\n추가 질문이 있거나 다른 확장 기능을 알고 계신다면 언제든지 댓글을 남겨주세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 Medium 뉴스레터를 구독하시면 최신 소식을 받으실 수 있습니다. 저의 콘텐츠를 받아보실 걸 약속드립니다! \n\nTwitter와 LinkedIn에서도 만나보실 수 있어요!\n\n원본 논문 보기:\n\nesProcess SPL 더 많은 내용은:","ogImage":{"url":"/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_0.png"},"coverImage":"/assets/img/2024-06-22-UnlockingDataAnalysisEfficiencyAComprehensiveGuidetoesProcSPL_0.png","tag":["Tech"],"readingTime":43},{"title":"SQL 윈도우 함수 탐구 ROW_NUMBER, RANK, DENSE_RANK 사용 방법","description":"","date":"2024-06-22 17:39","slug":"2024-06-22-ExploringSQLWindowFunctionsROW_NUMBERRANKandDENSE_RANK","content":"\n\nSQL 세계에서 윈도우 함수는 현재 행과 관련된 테이블 행 집합을 대상으로 계산을 수행할 수 있는 강력한 도구입니다. 이 글에서는 세 가지 필수 SQL 윈도우 함수인 ROW_NUMBER, RANK 및 DENSE_RANK에 대해 알아볼 것입니다. 이러한 함수를 사용하면 결과 세트 내에서 행에 고유한 번호나 순위를 할당하여 데이터에서 가치 있는 통찰을 추출하기가 쉬워집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반 구문:\nROW_NUMBER() OVER (ORDER BY column)\n\n- ORDER BY는 결과 집합을 정렬하는 데 사용되는 열 또는 표현식을 지정합니다.\n\nROW_NUMBER() OVER (PARTITION BY column ORDER BY column)\n\n- PARTITION BY는 지정된 열을 기준으로 결과 집합을 분할하는 선택적 절입니다. 순위가 각 파티션 내에서 별도로 적용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n회사 직원들에게 급여 순으로 내림차순으로 새 직원 ID를 할당해 봅시다:\n\n```js\nSELECT * , ROW_NUMBER() OVER(ORDER BY salary DESC) AS new_employee_id\nFROM employees;\n```\n\n![이미지](/assets/img/2024-06-22-ExploringSQLWindowFunctionsROW_NUMBERRANKandDENSE_RANK_1.png)\n\n직원들을 급여에 따라 내림차순으로 순위를 매기고, 'position' 열을 기준으로 결과 집합을 파티션으로 나눠 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\nSELECT * , ROW_NUMBER() OVER(PARTITION BY position ORDER BY salary DESC) AS employee_rank\nFROM employees;\n```\n\n![Exploring SQL Window Functions](/assets/img/2024-06-22-ExploringSQLWindowFunctionsROW_NUMBERRANKandDENSE_RANK_2.png)\n\n## 2. RANK Function\n\nThe RANK() function assigns a unique rank to each row based on the values in one or more columns. Rows with the same values receive the same rank, and the next rank is skipped. It’s useful when you want to create a ranking with gaps.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적인 구문:\n\nRANK() OVER (ORDER BY column)\n\nRANK() OVER (PARTITION BY column ORDER BY column)\n\n우리는 직원들을 급여에 따라 내림차순으로 순위 매겨볼게요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sql\nSELECT * , RANK() OVER(ORDER BY salary DESC) AS salary_rank\nFROM employees;\n```\n\n![Exploring SQL Window Functions](/assets/img/2024-06-22-ExploringSQLWindowFunctionsROW_NUMBERRANKandDENSE_RANK_3.png)\n\n## 3. DENSE_RANK Function: 동일한 순위를 가진 항목 그룹화하기\n\nDENSE_RANK() 함수는 동일한 순위를 가진 항목을 함께 그룹화하고 싶을 때 유용합니다. 동일한 값들을 갖는 행들은 동일한 순위를 부여받으며, 다음 순위가 건너뛰어지지 않습니다. 이 함수는 순위에 빈칸이 없는 순위를 만들고 싶을 때 유용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적인 구문:\n\n\nDENSE_RANK() OVER (ORDER BY column)\n\nDENSE_RANK() OVER (PARTITION BY column ORDER BY column)\n\n\n만약 \"titles\"라는 테이블이 있고 \"title\"과 \"price\"라는 열이 있다고 가정해보겠습니다.\n책 제목을 가격순으로 순위를 매기고 동일한 가격을 가진 제목들을 그룹화하려면:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nSELECT title, price, DENSE_RANK() OVER(ORDER BY price DESC) as 'rank'\nFROM titles;\n```\n\n![Exploring SQL Window Functions](/assets/img/2024-06-22-ExploringSQLWindowFunctionsROW_NUMBERRANKandDENSE_RANK_4.png)\n\n‘type’ 열을 기반으로 결과 집합을 파티션으로 나눠 봅시다.\n\n```js\nSELECT title, price, type, DENSE_RANK() OVER(PARTITION BY type ORDER BY price DESC) as 'rank'\nFROM titles;\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-22-ExploringSQLWindowFunctionsROW_NUMBERRANKandDENSE_RANK_5.png)\n\n가정하고 있는 것은 \"titles\"라는 테이블이 \"title\"과 \"ytd_sales\"라는 열을 가지고 있다는 것입니다. 성과가 좋은 책을 식별하기 위해 다음 쿼리를 사용할 수 있습니다:\n\n```js\nSELECT title, ytd_sales, DENSE_RANK() OVER(ORDER BY ytd_sales DESC) as 'rank'\nFROM titles;\n```\n\n![image](/assets/img/2024-06-22-ExploringSQLWindowFunctionsROW_NUMBERRANKandDENSE_RANK_6.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n‘type’ 열을 기반으로 결과 집합을 파티션으로 나눠 봅시다.\n\n```js\nSELECT title, ytd_sales, type, DENSE_RANK() OVER(PARTITION BY type ORDER BY ytd_sales DESC) as 'rank'\nFROM titles;\n```\n\n![참고 이미지](/assets/img/2024-06-22-ExploringSQLWindowFunctionsROW_NUMBERRANKandDENSE_RANK_7.png)\n\n# 세 가지 함수를 모두 결합하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n직원의 급여와 함께 직원 목록이 있는 시나리오를 고려해보세요. 각 직원에 고유한 직원 ID를 할당하고, 급여순으로 순위를 매기고, 촘촘한 순위를 부여하려고 합니다.\n\n```js\nSELECT \n    first_name, last_name, position, salary,\n    ROW_NUMBER() OVER (ORDER BY salary) AS employee_id,\n    RANK() OVER (ORDER BY salary) AS salary_rank,\n    DENSE_RANK() OVER (ORDER BY salary) AS dense_salary_rank\nFROM employees;\n```\n\n이 예제에서는 세 윈도우 함수를 한 쿼리에서 모두 사용합니다:\n\n- ROW_NUMBER()는 각 직원에게 고유한 employee_id를 할당합니다.\n- RANK()는 직원들을 급여에 따라 순위를 매기되, 동일한 급여의 경우 갭을 둡니다.\n- DENSE_RANK()는 직원들을 급여에 따라 순위를 매기되, 동일한 급여의 경우 갭을 두지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 쿼리는 직원 데이터의 종합적인 정보를 제공합니다. 고유 식별자와 두 가지 유형의 급여 순위가 포함되어 있습니다.\n\n## 결론\n\nROW_NUMBER, RANK, DENSE_RANK와 같은 SQL 윈도우 함수는 데이터 분석 및 보고에 불가결한 도구입니다. 이들은 결과 집합 내에서 행에 고유한 번호나 순위를 할당하여 데이터에서 통찰을 도출하는 작업을 더 쉽게 만들어 줍니다.\n\n# SQL 기초 개념\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n감사합니다! 더 많은 콘텐츠는 SQL Fundamentals에서도 찾아볼 수 있어요! 🚀💫","ogImage":{"url":"/assets/img/2024-06-22-ExploringSQLWindowFunctionsROW_NUMBERRANKandDENSE_RANK_0.png"},"coverImage":"/assets/img/2024-06-22-ExploringSQLWindowFunctionsROW_NUMBERRANKandDENSE_RANK_0.png","tag":["Tech"],"readingTime":5},{"title":"DuckDB 빅데이터 업계의 떠오르는 스타","description":"","date":"2024-06-22 17:38","slug":"2024-06-22-DuckDBTheRisingStarintheBigDataLandscape","content":"\n\n# 빅 데이터의 현재 상태\n\n지난 십 년 동안 빅 데이터 환경은 놀라운 변화를 겪었습니다. 이 빠르게 발전하는 분야에서 오늘날 사용 가능한 도구의 숫자는 엄청납니다 — 십 년 전과 비교하면 약 9배나 다양합니다.\n\n이 다양성은 데이터 웨어하우스, 데이터 레이크, 레이크하우스로만 한정되지 않고, ETL (추출, 변환, 적재)에서 ELT (추출, 적재, 변환), EL (추출, 적재) 등의 처리 방법론까지 확대되었습니다. 선택의 폭증은 하나의 기술에 대한 전문가가 되는 일이 어렵다는 점을 복합화시키는 것으로, 결국 시간은 우리 모두에게 동등합니다!\n\nSQL이 다양하고 다양한 이 생태계에서 데이터 변환의 선택 도구로 자리 잡았으며, 많은 데이터 처리 플랫폼에서 선택 언어로 자리잡았음을 입증하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다양한 도구 중 하나인 DuckDB가 큰 인기를 끌고 있어요! 알아맞히셨죠... DuckDB!\n\n# DuckDB가 뭔지, 그리고 왜 신경 써야 할까요!?\n\nDuckDB는 빠르게 인기를 얻고 있는 인프로세스 SQL 분석 엔진입니다. 그 인기를 증명하는 인상적인 통계들이 있어요:\n\n- PyPI에서 매달 170만 다운로드\n- GitHub에서 13,800개의 스타를 기록하며 Postgres와 같은 관심을 절반의 시간에 도달합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-DuckDBTheRisingStarintheBigDataLandscape_0.png\" /\u003e\n\n- DuckDB는 DB-Engines의 트렌드 보고서에 의하면 Snowflake와 유사한 두 년간의 기간 동안 인기가 급증했습니다. 다음 몇 년 동안 주류가 될 것으로 전망되며, 현재 전통적인 데이터 웨어하우스에서 처리되는 일부 페이로드를 대체하는 것으로 잘 준비되어 있는 것으로 보입니다.\n\n\u003cimg src=\"/assets/img/2024-06-22-DuckDBTheRisingStarintheBigDataLandscape_1.png\" /\u003e\n\nDuckDB의 오픈 소스 성격은 영구적인 MIT 라이선스로 보장되어 있어 매력을 더합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# DuckDB의 주요 이점\n\n- 설치의 용이성: DuckDB를 실행하는 것은 brew install duckdb(이것은 DuckDB CLI를 설치합니다)처럼 매우 간단합니다.\n- 낮은 복잡성: 서버가 없다는 점 (DuckDB는 바이너리일 뿐입니다)로, 자격 증명, 액세스 제어 목록, 방화벽 구성 등과 같은 복잡한 작업이 필요하지 않습니다.\n- 보편적 호환성: 매우 적은 종속성을 가지고 DuckDB는 이식성을 대표합니다. 심지어 브라우저에서도 실행할 수 있습니다!\n- DataFrame 통합: DuckDB의 Python 라이브러리는 Pandas DataFrames를 쿼리할 수 있는 능력을 갖고 있어 게임 체인저입니다! 직접 쿼리할 수 없는 어떤 시스템과도 통합 계층 또는 '접착제' 역할을 하여 데이터 처리의 변환 단계를 용이하게 합니다.\n- 확장 기능: DuckDB는 유연한 확장 메커니즘을 가지고 있어 JSON 및 Parquet에서 데이터를 직접 읽거나 S3에서 데이터를 직접 읽는 경우와 같은 유연성을 제공합니다. 이 능력은 개발자들의 경험을 크게 향상시킵니다.\n- 안정성과 효율성: DuckDB는 메모리 제한을 초과하는 작업 부담을 처리하도록 설계되었습니다 (일부 제한 사항이 있음). 이것은 분석 데이터셋이 사용 가능한 RAM보다 훨씬 크지만 디스크에 맞는 작은 경우에 특히 관련이 있으며, 이는 '싼' 및 쉽게 이용 가능한 하드웨어(예: 랩톱)를 사용하여 분석을 완료할 수 있도록 해줍니다.\n\n# 실제 데이터 파이프라인에서의 DuckDB\n\n클라우드 기반 시스템과 비교할 때 DuckDB는 최소한의 요구 사항과 비용 효율성으로 두드러지게 나타납니다. DuckDB는 클라우드 계정, 할당량 또는 추가 비용이 필요하지 않습니다. 개발자의 랩톱에서 제품 설정까지 DuckDB의 일관성은 데이터가 스테일하거나 잘못될 경우 시간이 지남에 따라 발생하는 클라우드 기반 솔루션과는 대조적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDuckDB의 간편한 실행은 데이터를 계산 노드로 이동하거나 VM/작업 오케스트레이션, 장애 처리와 같은 분산 시스템에서 볼 수 있는 일반적인 도전 과제를 우회합니다. Apple의 M1 SoC로 구동되는 클라우드 기반 또는 유사한 현대 기계의 성능은 DuckDB의 유용성을 더욱 향상시킵니다. 이는 대규모 데이터셋을 처리하는 단일 기계 처리 시나리오를 가능케 하죠. 실제로 매일 TB 단위의 데이터를 처리해야 하는 고객은 매우 적고, 이들은 모든 공개 클라우드에서 사용 가능한 CPU 성능보다 더 많은 CPU 성능이 필요하게 될 것입니다.\n\n# SQL 구문 설탕\n\nDuckDB의 상대적인 신선함은 GROUP BY ALL, SELECT * EXCLUDE, ASOF JOINS 등과 같은 새로운 SQL 구문 개선을 도입할 수 있는 유연성을 제공합니다. 이러한 추가 기능은 SQL 쿼리를 더 직관적이고 가독성 있게 만들어줍니다. 아래 코드 스니펫을 살펴보세요:\n\n```js\n-- ANSI SQL에서 여러 필드로 그룹화\nSELECT country, city, region, postal_code, AVG(price) AS avg_price\nFROM customers\n-- 집계되지 않는 필드는 여기서 반복되어야 함\nGROUP BY country, city, region, postal_code;\n\n-- DuckDB에서 모두를 기준으로 그룹화\nSELECT country, city, region, postal_code, AVG(price) AS avg_price\n-- 필드는 한 번만 나열됨. 코드 유지 관리가 더 쉬워집니다\nGROUP BY ALL;\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n-- 'email' 필드를 제외한 모든 데이터를 ANSI SQL에서 쿼리합니다.\nSELECT country, city, region, postal_code, address, phone_number\n  /*, email*/\nFROM customers;\n\n-- 'email' 필드를 제외한 모든 데이터를 DuckDB에서 쿼리합니다.\nSELECT * EXCLUDE (email) FROM customers;\n```\n\n```js\n-- '대략적으로' 동일한 타임스탬프를 조인하는 것을 고려합니다.\n-- ANSI SQL에서는 보통 이를 버킷화해야 합니다.\n-- DuckDB에서는 ASOF JOIN을 사용하여 동일한 결과를 더 간편하고 효율적으로 얻을 수 있습니다.\nSELECT events.id, events.ts, events.val, metadata.details\nFROM events\nASOF JOIN metadata USING(id, ts);\n```\n\n# Pandas 데이터프레임 통합\n\n특히 Python 생태계에서 DuckDB의 중요한 장점은 Pandas 데이터프레임과의 원활한 통합 기능입니다. 이 기능은 다양한 소스에서 다양한 데이터셋을 병합하는 프로세스를 간소화하여 데이터 분석 및 변환 작업을 용이하게 합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, Jupyter Notebook에서 다음을 수행할 수 있습니다 (영화 추천 시스템 데이터셋을 기반으로):\n\n```js\n# 의존성 설치\n%pip install --quiet duckdb\n%pip install --quiet jupysql\n%pip install --quiet duckdb-engine\n%pip install --quiet pandas\n%pip install --quiet matplotlib\n%pip install --quiet psycopg2-binary\n%pip install --quiet dash\n%pip install --quiet plotly\n\nimport duckdb\nimport pandas as pd\n\n# jupysql 로드 및 구성\n%load_ext sql\n%config SqlMagic.autopandas = True\n%config SqlMagic.feedback = False\n%config SqlMagic.displaycon = False\n%config SqlMagic.named_parameters=True\n\n# 로컬 DuckDB 인스턴스에 연결\n%sql duckdb:///\n\n# DuckDB를 사용하여 원격 파일 조회 활성화 (예: S3)\n%%sql\nINSTALL httpfs;\nLOAD httpfs;\n\n# S3 액세스 키 구성\nSET s3_region = '...';\nSET s3_access_key_id = '...';\nSET s3_secret_access_key = '...';\n\n# 원격 Postgres 데이터베이스에 연결\nATTACH 'dbname=DATABASE user=USER host=HOST password=PASSWORD connect_timeout=10' AS postgres (TYPE postgres, READ_ONLY);\n\n# 쿼리를 실행하고 데이터프레임에 저장\n%%sql\ndf \u003c\u003c SELECT \n    t1.movieId,\n    t1.title,\n    t1.genres,\n    t2.userId,\n    t2.rating,\n    t3.tag\n  # Postgres의 테이블 쿼리\n  FROM postgres.public.movies AS t1\n  # DuckDB의 테이블과 조인\n  INNER JOIN ratings AS t2 USING (movieId)\n  # S3의 JSON 데이터셋과 조인\n  INNER JOIN 's3://S3-BUCKET/tags.json' AS t3 USING (userId, movieId)\n\n# 마지막으로 다른 쿼리에서 데이터프레임을 참조\n%%sql\nby_genres \u003c\u003c SELECT genres, COUNT(*) AS cnt \n             FROM df\n             GROUP BY ALL\n             ORDER BY 2 DESC\n             LIMIT 5;\n\n# 또는 변형된 데이터셋 플롯\nimport plotly.express as px\nfig = px.pie(by_genres,\n             values='cnt',\n             names='genres',\n             title='Top 5 movie genres')\nfig.show()\n```\n\n# 결론\n\nDuckDB의 개요를 통해 이 도구가 대용량 데이터 영역에서 다재다능하고 효율적이며 사용자 친화적인 잠재력을 갖고 있음을 강조했습니다. 상대적으로 새로운 참가자로서, 데이터 엔지니어와 소프트웨어 개발자들의 변화하는 요구에 부합하는 솔루션을 가능하게 하고 간극을 좁히는 독특한 위치에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 일반적인 데이터 처리 사용 사례를 다루는 데 그 유틸리티와 다양성에 대해 자신이 있습니다. DuckDB와 관련한 여러분의 경험과 통찰력에 대해 듣고 싶습니다!\n\n다음에 또 만나요 - 계속해서 쿼리해 보세요! 🦆","ogImage":{"url":"/assets/img/2024-06-22-DuckDBTheRisingStarintheBigDataLandscape_0.png"},"coverImage":"/assets/img/2024-06-22-DuckDBTheRisingStarintheBigDataLandscape_0.png","tag":["Tech"],"readingTime":6},{"title":"데이터 마법사를 위한 CASE WHEN 완벽 마스터 가이드 SQL 최종 안내서","description":"","date":"2024-06-22 17:36","slug":"2024-06-22-TheUltimateGuidetoMasteringCASEWHENinSQLforDataWizards","content":"\n\n## CASE WHEN 매직: SQL 기술을 즉시 변화시키세요!\n\n![이미지](/assets/img/2024-06-22-TheUltimateGuidetoMasteringCASEWHENinSQLforDataWizards_0.png)\n\nSQL의 세계에서 조건부 논리를 마스터하는 것은 체스 선수가 전략적 움직임을 배우는 것과 조금 비슷합니다.\n\n이 핵심에는 CASE WHEN 문이 있습니다. SQL Server, Oracle 및 Snowflake와 같은 환경에서 데이터와 의사 결정을 형성할 수 있는 다재다능한 도구입니다. 이 문서는 다양한 시나리오에서 CASE WHEN을 활용하는 데 필수적인 가이드입니다. 여러분이 데이터 과학자를 희망하는 초보자이든 경험이 풍부한 전문가이든 데이터 조작 기술을 향상시키는 데 도움이 될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 1. CASE WHEN 이해하기\n\nSQL의 CASE WHEN 문은 프로그래밍 언어의 if-else 논리와 유사한 조건식입니다.\n\n이는 SQL 쿼리 내에서 조건부 체크를 가능케 하며, 특정 기준에 따라 데이터를 동적으로 조작하는 방법을 제공합니다.\n\n구문 개요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nCASE\n    WHEN condition1 THEN result1\n    WHEN condition2 THEN result2\n    ...\n    ELSE resultN\nEND\n```\n\n# 2. SQL Server에서의 기본 사용 사례\n\nSQL Server에서 시작해봅시다.\n\nSQL Server에서는 CASE WHEN이 쿼리에서 조건부 로직에 대한 기본 도구로 작용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특정 조건에 따라 데이터를 분류하거나 변환할 수 있습니다. 이 기능은 데이터를 특정 기준에 따라 세분화하거나 레이블을 지정해야 하는 상황에 특히 유용합니다. 예를 들어, 판매 금액을 높음, 중간, 낮음과 같이 다른 수준으로 분류할 때 활용할 수 있습니다.\n\n판매 데이터베이스가 있다고 가정하고 판매를 다양한 수준으로 분류하고 싶다면 다음과 같이 CASE WHEN을 사용할 수 있습니다:\n\n```js\nSELECT \n    SaleAmount,\n    CASE \n        WHEN SaleAmount \u003e 1000 THEN '높음'\n        WHEN SaleAmount BETWEEN 500 AND 1000 THEN '중간'\n        ELSE '낮음'\n    END AS SaleLevel\nFROM Sales;\n```\n\n이 쿼리는 판매 금액을 기준으로 높음, 중간, 낮음 수준으로 판매를 분류합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 3. Oracle의 고급 사용\n\nOracle SQL은 CASE WHEN의 기능을 확장합니다.\n\nOracle의 CASE WHEN 구현은 다양성을 더욱 확대합니다.\n\n여러 조건에 따라 다른 계산 또는 변환을 적용하는 등 더 복잡한 결정 프로세스에 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 능력은 고객 상태나 구매 금액과 같은 다른 열 값에 따라 데이터 필드에 할인이나 사용자 정의 계산을 적용하는 것과 같은 시나리오에서 특히 유용합니다.\n\n고객 데이터베이스를 다루고 있고 고객 상태와 구매 금액에 따라 할인을 적용하려는 경우를 상상해보세요. 다음은 이를 수행하는 방법입니다:\n\n```js\nSELECT \n    CustomerID,\n    PurchaseAmount,\n    CASE \n        WHEN CustomerStatus = 'VIP' AND PurchaseAmount \u003e 1000 THEN PurchaseAmount * 0.8\n        WHEN CustomerStatus = 'Regular' AND PurchaseAmount \u003e 1000 THEN PurchaseAmount * 0.9\n        ELSE PurchaseAmount\n    END AS FinalAmount\nFROM Customers;\n```\n\n이 쿼리는 VIP 고객에게는 20% 할인을 적용하고, 1000달러 이상 구매시 일반 고객에게는 10% 할인을 적용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 4. Snowflake에서 CASE WHEN을 사용하여 값 합산하기\n\nSnowflake는 클라우드 데이터 플랫폼 기능으로 유명한데, CASE WHEN도 지원합니다.\n\nSnowflake는 조건부 집계를 위해 CASE WHEN을 지원합니다. 이 기능은 하나의 쿼리 내에서 서로 다른 범주나 기준에 따라 값들을 조건적으로 합산해야 하는 상황에 유용합니다.\n\n특히 이 기능은 데이터를 세심하게 요약하는 데 유용한데, 예를 들어 동일한 데이터 세트 내에서 서로 다른 비용 유형별 총 비용을 계산하는 경우 등에 활용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTravel 및 Supplies와 같은 다른 유형의 비용을 카테고리별로 합산하려는 시나리오를 생각해보세요. 쿼리는 다음과 같을 수 있습니다:\n\n```js\nSELECT \n    SUM(CASE WHEN ExpenseType = 'Travel' THEN Amount ELSE 0 END) AS TotalTravelExpense,\n    SUM(CASE WHEN ExpenseType = 'Supplies' THEN Amount ELSE 0 END) AS TotalSuppliesExpense\nFROM Expenses;\n```\n\n이 쿼리는 여행 및 용품에 대한 총 비용을 별도로 계산합니다.\n\n# 5. PostgreSQL 및 다중 조건 처리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPostgreSQL는 인기있는 오픈 소스 데이터베이스로, CASE WHEN에 강력한 지원을 제공합니다.\n\nPostgreSQL에서 CASE WHEN은 쿼리 내에서 여러 조건을 처리하는 데 능숙합니다. 이 기능은 다양한 기준을 충족해야 하는 경우에 결과가 달라지는 상세한 데이터 분석과 조작에 중요합니다.\n\n여러 조건에 따라 데이터를 자세히 분해하거나 분류해야 하는 시나리오에서 가치 있는데, 즉 여러 입력 데이터 범위에 따라 다른 값이나 조치를 할당해야 하는 경우입니다.\n\n예를 들어, 학생 성적 데이터 세트를 분석하고 성적 점수를 할당하려고 한다고 가정해 봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nSELECT \n    StudentID,\n    Grade,\n    CASE \n        WHEN Grade = 'A' THEN 4\n        WHEN Grade = 'B' THEN 3\n        WHEN Grade = 'C' THEN 2\n        WHEN Grade = 'D' THEN 1\n        ELSE 0\n    END AS GradePoints\nFROM StudentGrades;\n```\n\n이 쿼리는 문자 학점에 따라 학점을 할당합니다.\n\n# 6. CASE WHEN으로 동적 열 이름 생성\n\nCASE WHEN의 독특한 응용은 열 이름을 동적으로 지정하는 데 있습니다. 이는 보고서 작성이나 다양한 스키마 요구 사항을 처리할 때 특히 유용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n판매 데이터를 분석하고 연도에 따라 동적 열 이름을 갖는 보고서를 생성하려고 한다고 가정해봅시다. SQL Server에서 다음 예시를 참고하세요:\n\n```js\nSELECT \n    CustomerID,\n    SUM(CASE WHEN Year = 2021 THEN Amount ELSE 0 END) AS [Sales_2021],\n    SUM(CASE WHEN Year = 2022 THEN Amount ELSE 0 END) AS [Sales_2022]\nFROM Sales\nGROUP BY CustomerID;\n```\n\n위 쿼리에서는 각 연도별로 고객당 총 판매 금액을 보여주기 위해 'Sales_2021', 'Sales_2022'라는 동적 열을 생성합니다.\n\n# 7. 성능 고려사항\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCASE WHEN은 강력하지만 대규모 데이터셋에서 쿼리 성능에 미치는 영향을 고려하는 것이 중요합니다.\n\n효율적인 인덱싱 및 쿼리 최적화로 잠재적인 지연을 완화할 수 있습니다.\n\n고객 구매 행동을 분석하는 대규모 전자 상거래 데이터베이스를 고려해보세요. 수백만 개의 행이 포함된 쿼리에 CASE WHEN을 사용하면 성능에 영향을 줄 수 있습니다. 다음은 샘플 쿼리입니다:\n\n```js\nSELECT \n    CustomerID,\n    TotalPurchases,\n    CASE \n        WHEN TotalPurchases \u003e 1000 THEN 'High Value'\n        ELSE 'Regular'\n    END AS CustomerType\nFROM Purchases\nWHERE TotalPurchases \u003e 1000;\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 상황에서 'Purchases' 테이블이 수백만 개의 행을 포함하는 경우, CASE WHEN 문은 쿼리의 속도를 늦출 수 있습니다. 특히 `TotalPurchases`가 인덱싱되지 않은 경우에는 더 그렇습니다.\n\n성능을 개선하기 위해 중요한 열에 인덱스를 설정하거나 쿼리를 더 작고 관리하기 쉬운 부분으로 분할하는 것이 좋습니다.\n\n# 제한 사항과 대안\n\nCASE WHEN은 중첩된 쿼리나 저장 프로시저가 더 효율적일 수 있는 복잡한 논리 구조에서 제한 사항이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## i. 복잡한 논리 처리:\n\nCASE WHEN은 복잡한 논리나 여러 조건으로 인해 읽기 어렵고 다루기 어려워질 수 있습니다.\n\n복잡한 결정 트리나 다수의 중첩 조건이 필요한 시나리오에는 적합하지 않습니다.\n\n## ii. 성능 문제:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대규모 데이터 세트에서는 CASE WHEN을 사용하면 특히 수백만 개의 행을 대상으로 한 계산에서 쿼리 성능이 느려질 수 있습니다.\n\n특정 사용 사례에 최적화된 다른 SQL 구조물이나 함수보다 효율성이 떨어질 수 있습니다.\n\n## iii. 집계 제어가 제한됨:\n\nCASE WHEN은 여러 열 또는 테이블을 포함하는 복잡한 집계에 이상적이지 않을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음과 같은 추가 서브쿼리나 조인이 필요할 수 있습니다. 그러나 이러한 경우 쿼리를 복잡하게 만들 수 있고 실행 속도를 늦출 수 있습니다.\n\n## iv. 대안적 SQL 구조:\n\nIF/ELSE 문: 일부 SQL 환경에서는 저장 프로시저 내에서 IF/ELSE 문을 사용하여 더 복잡한 로직을 처리할 수 있습니다.\n\n저장 프로시저 및 함수: 복잡한 로직에 대해서는 해당 로직을 저장 프로시저나 함수로 캡슐화하는 것이 더 효율적이고 유지보수하기 쉬울 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n창 함수: 고급 데이터 분석을 위해 창 함수는 행 집합에 대한 작업에 특히 더 강력하고 효율적인 접근 방식을 제공할 수 있습니다.\n\n## v. SQL 이외의 대안:\n\n데이터 처리 파이프라인에서 스크립팅: 때로는 데이터 처리 스크립트(예: Python, R)에서 복잡한 조건 로직을 처리하는 것이 SQL 외부에서 처리하는 것보다 더 효율적일 수 있습니다.\n\n데이터 변환 도구 활용: 데이터 변환 도구(예: ETL 도구)는 복잡한 데이터 조작을 처리하는 더 직관적이고 효율적인 방법을 제공하는 경우도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## vi. Best Practices Consideration:\n\nCASE WHEN이 주어진 작업에 가장 적합한 도구인지를 평가하는 것이 매우 중요합니다. 이때 양식의 복잡성과 데이터 집합의 크기를 함께 고려해야 합니다.\n\n정기적으로 SQL 쿼리를 검토하고 리팩토링하여 데이터와 요구 사항이 발전함에 따라 효율적이고 유지 관리가 용이한 쿼리일 수 있도록 합니다.\n\n이러한 한계와 대안을 이해함으로써 다양한 데이터 조작 및 분석 시나리오에서 SQL을 더 효과적이고 효율적으로 사용할 수 있습니다. 이는 데이터 전문가들이 특정 요구 사항에 맞는 올바른 도구를 선택하도록 도와주며, 단숨함, 성능, 유지 관리성을 균형 있게 고려하게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 9. 실제 응용 분야\n\n데이터 과학에서, CASE WHEN은 데이터 클리닝, 분류 및 피쳐 엔지니어링에 응용됩니다. 예를 들어 고객 행동을 분류하거나 일관성 없는 데이터 항목을 정리하는 데 사용됩니다.\n\na. 데이터 클리닝: 예를 들어, 성별의 불일치하는 표현이 포함된 데이터셋이 있다고 가정해 봅시다 (예: 'M', 'Male', 'F', 'Female'). 이러한 값을 표준화하기 위해 CASE WHEN을 사용할 수 있습니다:\n\n```js\nSELECT \n    CASE \n        WHEN Gender IN ('M', 'Male') THEN '남성'\n        WHEN Gender IN ('F', 'Female') THEN '여성'\n        ELSE '기타'\n    END AS 표준화된성별\nFROM Users;\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nb. 고객 행동 분류: 소매 데이터셋에서 구매 빈도에 따라 고객을 분류합니다:\n\n```sql\nSELECT \n    CustomerID,\n    CASE \n        WHEN PurchaseCount \u003e 50 THEN '자주 구매하는 고객'\n        WHEN PurchaseCount BETWEEN 10 AND 50 THEN '가끔 구매하는 고객'\n        ELSE '거의 구매하지 않는 고객'\n    END AS BuyerType\nFROM CustomerPurchases;\n```\n\nc. 기계 학습 모델을 위한 피처 엔지니어링: 연령과 소득에 기반하여 신용 위험을 예측하는 머신 러닝 모델을 위한 새로운 피처를 생성합니다:\n\n```sql\nSELECT \n    Age,\n    Income,\n    CASE \n        WHEN Age \u003c 30 AND Income \u003c 50000 THEN '낮은 위험'\n        WHEN Age \u003e= 30 AND Income \u003e= 50000 THEN '높은 위험'\n        ELSE '보통 위험'\n    END AS RiskCategory\nFROM CustomerData;\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 예시들은 CASE WHEN의 실용성과 다양성을 보여줍니다. 특히 데이터 클리닝, 고객 행동 분석, 예측 모델링을 위한 피처 엔지니어링과 같은 데이터 과학 응용 프로그램에서 이러한 기능을 활용하기에 유용합니다.\n\n## 10. 모범 사례\n\n- 조건을 간단하고 가독성 있도록 유지합니다.\n- 지나치게 복잡한 중첩 CASE WHEN 문을 피합니다.\n- 대규모 데이터셋에서 성능을 테스트합니다.\n\n## 11. 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCASE WHEN은 SQL에서 조건부 데이터 조작을 위한 강력한 도구입니다. 다양한 SQL 환경에서의 유연성은 데이터 과학 전문가들에게 귀중한 도구로 만들어 줍니다.\n\n여기서 소개된 개념과 예제를 이해하고 적용함으로써 데이터 조작 능력을 향상시킬 수 있습니다. 데이터 과학 여정을 생산적이고 매혹적으로 만드는 데 도움이 될 것입니다.\n\nCASE WHEN에 대한 이 탐구는 SQL 기능을 배우는 것 이상의 의미가 있습니다. 데이터 처리 능력에 유연성과 효율성을 더해주는 도구로 데이터 과학 여정을 강화시키는 것입니다.\n\n이러한 개념을 계속해서 탐구하고 적용할수록 각 쿼리는 단순히 한 가지 명령이 아니라 데이터 과학의 기술을 습득하는 한 걸음이라는 것을 기억해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n계속 실험하고, 학습하고, 무엇보다 중요한 것은 여정을 즐기는 것이에요!\n\n⭐️ 내 Gumroad 샵: [codewarepam의 Gumroad](https://codewarepam.gumroad.com/)\n\n무료 eBook, AI 트렌드, 그리고 데이터 과학 사례 연구를 꾸준히 받고 싶다면, 지금 구독하세요!\n\n# 베스트셀러 eBook:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTop 50+ ChatGPT Personas for Custom Instructions\n\n# 무료 eBook:\n\n- Prompt Engineering 예술 습득하기\n- 데이터 과학자를 위한 Top 50+ 효과적인 ChatGPT 프롬프트\n- 완벽한 AI 예술 프롬프트 습득하기: Top 50+ 프롬프트\n- Top 200+ 정교하게 만들어진 프롬프트\n- 데이터 지망생을 위한 도메인 마스터리","ogImage":{"url":"/assets/img/2024-06-22-TheUltimateGuidetoMasteringCASEWHENinSQLforDataWizards_0.png"},"coverImage":"/assets/img/2024-06-22-TheUltimateGuidetoMasteringCASEWHENinSQLforDataWizards_0.png","tag":["Tech"],"readingTime":9},{"title":"SQL에서 임시 테이블의 강력한 기능 탐구하기","description":"","date":"2024-06-22 17:35","slug":"2024-06-22-ExploringthePowerofTemporaryTablesinSQL","content":"\n\nSQL은 가장 인기 있는 관계형 데이터베이스 관리 시스템 중 하나로, 데이터 조작 및 분석을 향상시키는 다양한 기능을 제공합니다. 임시 테이블 사용이 그 중 하나입니다.\n\n![image](/assets/img/2024-06-22-ExploringthePowerofTemporaryTablesinSQL_0.png)\n\n임시 테이블은 세션 내에서 중간 결과 집합을 저장하고 조작할 수 있는 방법을 제공하여 유연성과 성능 향상을 제공합니다. 이 기사에서는 임시 테이블의 세계로 뛰어들어 SQL에서의 이점과 실제 사용 사례를 탐색해 보겠습니다.\n\n# 1. 임시 테이블 이해하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n임시 테이블은 그 이름 그대로 임시로 존재하며 특정 세션에 바인딩된 테이블입니다. 중간 결과를 저장하거나 복잡한 데이터 조작을 수행하는 데 유용합니다. 임시 테이블은 MySQL 임시 디렉토리에 저장되며 시스템에 의해 자동으로 생성된 고유한 이름을 갖습니다. 생성 세션 내에서만 표시되고 액세스할 수 있으며 세션이 종료될 때 자동으로 삭제됩니다.\n\n# 2. 임시 테이블 생성\n\nMySQL에서 임시 테이블을 만드는 것은 간단합니다. 아래 데이터를 임시 테이블에 저장하려는 예제를 살펴봅시다:\n\n```js\n-- 이 쿼리 결과를 임시 테이블로 표시하고 싶어요.\n-- (책 제목과 출판사 국가를 보여줍니다.)\n\nSELECT title, country\nFROM titles\nLEFT JOIN publishers\nUSING (pub_id)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploringthePowerofTemporaryTablesinSQL_1.png\" /\u003e\n\n```js\n-- 임시 테이블 'titles_publishers'을 생성합니다.\n\nCREATE TEMPORARY TABLE titles_publishers\nSELECT title, country\nFROM titles\nLEFT JOIN publishers\nUSING (pub_id)\n\n-- 임시 테이블 'titles_publishers'에서 모든 데이터를 검색합니다.\n\nSELECT *\nFROM titles_publishers;\n```\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploringthePowerofTemporaryTablesinSQL_2.png\" /\u003e\n\n# 3. 임시 테이블 사용하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n-- 'The' 으로 시작하는 책 제목과 출판사 국가를 표시합니다.\n\nSELECT *\nFROM titles_publishers\nWHERE title LIKE 'The%';\n```\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploringthePowerofTemporaryTablesinSQL_3.png\" /\u003e\n\n# 4. 임시 테이블과 일반 테이블 결합하기\n\n임시 테이블을 일반 테이블과 결합할 수 있는 기능은 여러 가지 강력한 측면 중 하나입니다. 이를 통해 임시 데이터를 기존 데이터와 결합하여 복잡한 쿼리를 작성할 수 있습니다. 다음 예제를 살펴보세요:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n-- 'The'으로 시작하는 책 제목과 출판 국가를 표시해주세요.\n\nSELECT titles_publishers.title, publishers.country\nFROM titles_publishers\nLEFT JOIN publishers\nON titles_publishers.publisher_id = publishers.publisher_id\nWHERE titles_publishers.title LIKE 'The%';\n```\n\n![Exploring the Power of Temporary Tables in SQL](/assets/img/2024-06-22-ExploringthePowerofTemporaryTablesinSQL_4.png)\n\n## 5. 임시 테이블 삭제하기\n\n임시 테이블은 세션이 끝나면 자동으로 삭제되지만, 필요한 경우에는 세션이 끝나기 전에 명시적으로 삭제할 수도 있습니다. 임시 테이블을 삭제하려면 DROP TABLE 문을 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n테이블 titles_publishers 삭제;\n```\n\n# 결론\n\nMySQL의 임시 테이블은 중간 결과 세트를 저장하고 조작하는 강력하고 다재다능한 솔루션을 제공합니다. 데이터 조작 작업에서 성능과 유연성을 향상시킵니다.\n\n임시 테이블을 사용하면 MySQL에서 데이터 조작 기능을 향상시키고 더 효율적이고 효과적인 데이터 분석을 수행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# SQL 기초\n\n시간을 내주셔서 감사합니다! 🚀\nSQL 기초에서 더 많은 콘텐츠를 찾아보실 수 있어요 💫","ogImage":{"url":"/assets/img/2024-06-22-ExploringthePowerofTemporaryTablesinSQL_0.png"},"coverImage":"/assets/img/2024-06-22-ExploringthePowerofTemporaryTablesinSQL_0.png","tag":["Tech"],"readingTime":3}],"page":"37","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"37"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>