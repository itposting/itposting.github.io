<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/37" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/37" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="Spark 심화 학습 Delta 테이블에서 Z-Ordering과 Data Skipping 사용법" href="/post/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Spark 심화 학습 Delta 테이블에서 Z-Ordering과 Data Skipping 사용법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Spark 심화 학습 Delta 테이블에서 Z-Ordering과 Data Skipping 사용법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Spark 심화 학습 Delta 테이블에서 Z-Ordering과 Data Skipping 사용법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="BigQuery에서 함수형 데이터 엔지니어링 가이드" href="/post/2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="BigQuery에서 함수형 데이터 엔지니어링 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="BigQuery에서 함수형 데이터 엔지니어링 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">BigQuery에서 함수형 데이터 엔지니어링 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">16<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="품질 엔지니어를 위한 RAG 사용 가이드" href="/post/2024-06-22-RAGforQualityEngineers"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="품질 엔지니어를 위한 RAG 사용 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-RAGforQualityEngineers_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="품질 엔지니어를 위한 RAG 사용 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">품질 엔지니어를 위한 RAG 사용 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">13<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 메시에 대한 도전 과제 및 해결책  3부" href="/post/2024-06-22-ChallengesandSolutionsinDataMeshPart3"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 메시에 대한 도전 과제 및 해결책  3부" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-ChallengesandSolutionsinDataMeshPart3_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 메시에 대한 도전 과제 및 해결책  3부" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 메시에 대한 도전 과제 및 해결책  3부</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label=" Docker로 Apache Spark와 Minio를 사용하여 Apache Iceberg 활용하는 방법" href="/post/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker"><div class="PostList_thumbnail_wrap__YuxdB"><img alt=" Docker로 Apache Spark와 Minio를 사용하여 Apache Iceberg 활용하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt=" Docker로 Apache Spark와 Minio를 사용하여 Apache Iceberg 활용하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl"> Docker로 Apache Spark와 Minio를 사용하여 Apache Iceberg 활용하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="모든 문제를 해결하는 하나의 계층  시맨틱 레이어란 무엇인가" href="/post/2024-06-22-SemanticLayerOneLayertoServeThemAll"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="모든 문제를 해결하는 하나의 계층  시맨틱 레이어란 무엇인가" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="모든 문제를 해결하는 하나의 계층  시맨틱 레이어란 무엇인가" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">모든 문제를 해결하는 하나의 계층  시맨틱 레이어란 무엇인가</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">20<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Rockset이 OpenAI에 인수되다 사용자들에게 어떤 의미가 있을까" href="/post/2024-06-22-RocksetIsAcquiredbyOpenAIWhatDoesItMeanforItsUsers"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Rockset이 OpenAI에 인수되다 사용자들에게 어떤 의미가 있을까" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-RocksetIsAcquiredbyOpenAIWhatDoesItMeanforItsUsers_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Rockset이 OpenAI에 인수되다 사용자들에게 어떤 의미가 있을까" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Rockset이 OpenAI에 인수되다 사용자들에게 어떤 의미가 있을까</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="파이썬과 Streamlit으로 멀티페이지 금융 대시보드 만들기 처음부터 끝까지 완성하기" href="/post/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="파이썬과 Streamlit으로 멀티페이지 금융 대시보드 만들기 처음부터 끝까지 완성하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="파이썬과 Streamlit으로 멀티페이지 금융 대시보드 만들기 처음부터 끝까지 완성하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">파이썬과 Streamlit으로 멀티페이지 금융 대시보드 만들기 처음부터 끝까지 완성하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">15<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Z-점수를 이용해 연령대별 달리기 성능을 비교할 수 있을까요" href="/post/2024-06-22-CanWeUseZ-ScorestoCompareRunningPerformancesBetweenAgeGroups"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Z-점수를 이용해 연령대별 달리기 성능을 비교할 수 있을까요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-CanWeUseZ-ScorestoCompareRunningPerformancesBetweenAgeGroups_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Z-점수를 이용해 연령대별 달리기 성능을 비교할 수 있을까요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Z-점수를 이용해 연령대별 달리기 성능을 비교할 수 있을까요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="파이썬을 사용한 탐색적 데이터 분석 EDA 방법" href="/post/2024-06-22-ExploratoryDataAnalysisEDAUsingPython"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="파이썬을 사용한 탐색적 데이터 분석 EDA 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="파이썬을 사용한 탐색적 데이터 분석 EDA 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">파이썬을 사용한 탐색적 데이터 분석 EDA 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">18<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/21">21</a><a class="link" href="/posts/22">22</a><a class="link" href="/posts/23">23</a><a class="link" href="/posts/24">24</a><a class="link" href="/posts/25">25</a><a class="link" href="/posts/26">26</a><a class="link" href="/posts/27">27</a><a class="link" href="/posts/28">28</a><a class="link" href="/posts/29">29</a><a class="link" href="/posts/30">30</a><a class="link" href="/posts/31">31</a><a class="link" href="/posts/32">32</a><a class="link" href="/posts/33">33</a><a class="link" href="/posts/34">34</a><a class="link" href="/posts/35">35</a><a class="link" href="/posts/36">36</a><a class="link posts_-active__YVJEi" href="/posts/37">37</a><a class="link" href="/posts/38">38</a><a class="link" href="/posts/39">39</a><a class="link" href="/posts/40">40</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"Spark 심화 학습 Delta 테이블에서 Z-Ordering과 Data Skipping 사용법","description":"","date":"2024-06-22 17:20","slug":"2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables","content":"\n\n\u003cimg src=\"/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_0.png\" /\u003e\n\n만약 스파크가 말할 수 있다면, 데이터 엔지니어들에 대해 말할 것이고 그들을 우울하게 만들고 예측할 수 있는 미래에 불안감을 주게 될 거에요! 🫢😬\n\n말이지, 이런 데이터 엔지니어들 (내 자신을 포함해서 👀)은 스파크가 문제를 해결할 수 있다고 가정하는데 그들은 해결하지 못해서 불안해해요 (안 코딩해도 돼서 말이지… 내가 말하는 건 나의 경우에요🙈).\n\n하지만 스파크는 실제로 문제를 해결해 주거든요! 그 중 하나는 Z-Ordering을 적용하여 쿼리 실행 속도를 10-12배 빠르게 하는 문제를 해결하는 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Z-Ordering\n\n하루에 \"n\" 번 쿼리되는 델타 테이블이 있다고 해보죠. 이 쿼리들은 실행될 때마다 특정 열을 기준으로 필터링됩니다. 🤔\n\nSpark은 이미 적절한 파티션 수를 얻기 위해 적응형 쿼리 최적화 (AQE)를 사용하고 있으며, 이를 통해 쿼리의 성능을 높이기 위해 프레디케이트 푸시다운을 수행합니다.\n\n하지만 스파크는 여전히 최고 조절에 이르지 않았습니다! 우리는 스파크가 자신의 능력을 발휘할 수 있도록 도와주어야 합니다😤! 특정 열에 필터를 적용할 것이라는 점은 이미 알고 있기 때문에, 해당 열에 Z-Ordering을 활성화할 수 있습니다 🤨\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n컬럼에서 Z-Ordering을 활성화하면 다음과 같은 효과가 있습니다:\n\n- 지정된 열을 기준으로 테이블을 재분할합니다.\n- 지정된 열을 기준으로 파티션 내에서 정렬됩니다.\n\n더 잘 이해하기 위해 예시를 살펴보겠습니다: 🤓\n\n## 시나리오\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 가지 흥미로운 \"이름\" 열을 제외한 몇 가지 흥미없는 열을 포함한 델타 테이블을 생각해보세요. 이미 Spark의 AQE 이후 최적화된 파티션이 4개 있습니다.\n\n![이미지](/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_1.png)\n\n- 해당 쿼리를 테이블에서 실행하자마자, Spark는 델타 트랜잭션 로그의 최소-최대 값들을 확인하고 \"필요한 데이터가 이 파일에 포함되어 있는가\"라고 물을 것입니다. 🧐\n- 따라서 Spark는 처음 3개 파티션을 확인하지만, \"Dan\"보다 더 작은 이름은 없기 때문에 마지막 파티션을 확인하지 않을 것입니다. 이것이 바로 데이터 스킵(Data Skipping)이라고 합니다. 😲\n\n![이미지](/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 그러나 이러한 종류의 데이터 스킵은 'Name' 열에 대해 최적화되지 않았기 때문에 불규칙한 데이터 스킵이라고 할 수 있습니다.\n- 'Name' 열에 Z-Order를 적용해보고 무슨 일이 일어나는지 확인해 봅시다! ✨\n\n![image](/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_3.png)\n\n![image](/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_4.png)\n\n- 이제 위의 쿼리를 실행하면 Spark는 첫 번째 파티션만 확인하므로 데이터 스킵을 완벽하게 수행합니다. 😉\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Z-Ordering 구문:\n\n데이터의 Z-Ordering을 하려면 ZORDER BY 절에 정렬할 열을 지정합니다.\n\n```js\nOPTIMIZE table_name ZORDER BY column_name\n```\n\n## Z-Ordering의 이점:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Z-ordering은 상당한 속도 향상을 가져올 수 있으며, 쿼리 실행 시간을 분 단위에서 초 단위로 줄일 수 있어요🤯\n- 작업 태스크를 실행하는 스파크 워커 노드의 코어에서 균형을 유지하는 적절한 파티션 수⚖️\n\n## Z-Ordering과 관련된 도전:\n\n- 파티션 열을 선택하는 과정은 복잡합니다 🥴\n- Z-Ordering 작업은 비용이 많이 발생하며 더 오래 걸린다는 것을 염두에 두세요 ⏱️\n- Z-Ordering이 활성화된 테이블에서 동시적인 쓰기는 불가능합니다\n\n혜택과 도전을 따져보시면, 사용할지 말지 고민이 될 거예요. 😵‍💫\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이타브릭스도 이러한 딜레마를 느꼈고, 그들은 최첨단 솔루션을 개발했어요: 리퀴드 클러스터링 🥂\n\n다음 블로그에서 리퀴드 클러스터링에 대해 다뤄볼 예정이에요, 기대해주세요!\n\n만약 블로그가 마음에 들었다면 👏 손뼉을 치셔서 이 내용이 모든 데이터 엔지니어에게 전달되도록 해주세요.\n\n읽어 주셔서 감사합니다! 😁","ogImage":{"url":"/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_0.png"},"coverImage":"/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_0.png","tag":["Tech"],"readingTime":3},{"title":"BigQuery에서 함수형 데이터 엔지니어링 가이드","description":"","date":"2024-06-22 17:17","slug":"2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery","content":"\n\n\n\u003cimg src=\"/assets/img/2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery_0.png\" /\u003e\n\n# 소개\n\n## 함수형 데이터 엔지니어링이란 무엇인가요?\n\n함수형 데이터 엔지니어링은 수학과 소프트웨어 엔지니어링의 함수 개념을 반영한 데이터 작업 방식입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n막심 보셰만(Maxime Beauchemin)은 Apache Airflow 및 Apache Superset의 창립자로, 2018년 발표한 'Functional Data Engineering — a modern paradigm for batch data processing'에서 데이터에 대한 기능적 엔지니어링 원칙의 목표와 실제 적용을 설명했습니다.\n\n명확한 목표와 제약 조건을 정의함으로써, 이 접근 방식은 데이터에서 명확함, 재현성, 안정성, 신뢰성 및 추적 가능성을 달성하려는 목표를 갖습니다. 실제적으로, 이는 도구, 개발, 테스트 및 자동화를 간소화하고, 어떠한 조직, 팀 또는 프로젝트에 대한 데이터 관련 작업의 확장 가능하고 효율적인 기초를 구축하는 데 도움이 될 수 있습니다.\n\n# 동기\n\n데이터 공간에서 우리는 지속적으로 엔트로피와 싸움을 벌이고 있습니다. 데이터 소스 및 도구의 증식, 데이터 번성, 문서화 흩어짐, 일관되지 않고 잘못 계획된 명명 규칙, 스파게티처럼 얽힌 의존성 맵 등. 시스템은 뒤죽박죽으로 변화할 수 있습니다.하지만 이 저항할 수 없는 진전의 반대에 에너지와 의지를 투입하지 않는 한 요구되는 엔트로피 증가에는 버팁을 세우게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단함은 엔트로피를 막기 위해 우리가 사용하는 무기입니다.\n\n간단함은 기능적 데이터 엔지니어링의 존재 이유인데요: 아키텍처의 간단함; 프로세스의 간단함; 실행의 간단함; 쿼리 및 비용의 간단함이 중요합니다. 이러한 간단함은 증가하는 데이터 원본, 증가하는 데이터 양, 성장 중인 분산 팀 및 신속하고 신뢰할 수 있는 데이터로 의사결정을 내리고 모델을 학습하며 비즈니스 목표를 지원하는 수요를 다룰 때 필요합니다.\n\n비용과 복잡성의 증가, 신뢰성과 정확성의 감소를 피하기 위해서는 데이터 처리, 통합 및 관리에 일관된, 규율적인 방식이 필요하며, 이러한 목표를 달성하는데 도움이 되는 것이 바로 기능적 데이터 엔지니어링 접근 방식입니다.\n\n이 기사에서는 Google Cloud 데이터 스택에서 BigQuery를 중심으로 이를 어떻게 구현할 수 있는지 소개하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 상황\n\n함수형 데이터 엔지니어링의 원리를 이해하기 위해서는 해당 원리들이 유도된 수학 및 소프트웨어 엔지니어링 원리로 시작하는 것이 유익합니다.\n\n## 함수란 무엇인가요?\n\n수학적 함수의 정의부터 시작해보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떤 함수에 대해서 입력과 출력 사이에 1:1 매핑이 있습니다. 이건 직관적으로 이해되지만, 애플리케이션에서 '요소'가 실제로 무엇인지 정의하는 것이 중요할 것입니다.\n\n## 함수형 프로그래밍이란?\n\n함수형 프로그래밍의 정의로 넘어가겠습니다:\n\n함수는 다른 함수들로 구성될 수 있으며, 값들을 다른 값들로 매핑합니다. 좀 더 자세히 읽어보면:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n동일한 입력이 제공되면 항상 동일한 출력을 반환하는 함수를 의미합니다 (즉, 결정적인 함수입니다).\n\n## 데이터 파티셔닝\n\n함수형 데이터 엔지니어링의 기본 원칙을 살펴보기 전에 파티션된 데이터의 개념을 소개하는 것이 중요합니다.\n\n실제로, 데이터 파티셔닝은 데이터를 원래 수신된 시간을 기준으로 별도의 파티션으로 물리적으로 분리합니다. 이렇게 함으로써 하향 작업이 특정 쿼리나 작업에 필요한 파티션만을 대상으로 쿼리할 수 있도록 만들어줍니다. 이는 파티션 프루닝이라 불리는 작업입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파티셔닝 시간 단위는 데이터의 양과 속도에 따라 다를 수 있지만, 많은 경우 도착 날짜별로 데이터를 파티션으로 나누는 것이 합리적입니다. 이 기사에서는 날짜를 기반으로 한 데이터 배치를 다루고 있다고 가정하겠습니다.\n\n수학적 정의에 관련해서, '요소'라는 개념을 보면 입력 데이터 소스에서 날짜로 구분된 단일 파티션을 고려할 수 있으며, 이는 한 입력 파티션이 항상 정확히 하나의 출력 파티션에 매핑된다는 것을 의미합니다.\n\n# 해결책\n\n기능적 데이터 엔지니어링의 본질은 어떤 데이터 엔지니어링 작업도 입력 데이터, 변환 로직 및 출력 데이터로 축소하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![AGuidetoFunctionalDataEngineeringinBigQuery_1](/assets/img/2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery_1.png)\n\n입력 데이터, 변환 로직 및 출력 데이터를 분리하는 것이 매우 중요합니다. 로직은 시간이 지남에 따라 변경되므로 필요에 따라 새로운 로직을 기존 데이터에 적용하여 출력 데이터 테이블 파티션을 재구성할 수 있어야 합니다.\n\n## 1. 영구적이고 변경 불가능한 입력 데이터 파티션\n\n첫 번째 기본 원칙은 데이터 소스 파티션이 시스템에 들어오면 영구적이며 결코 삭제될 수 없다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 날짜 파티션은 변경할 수 없는 객체로 간주되며, 행을 UPDATE, DELETE 또는 INSERT할 수 없습니다.\n\n이는 변경되는 로직이 계속하여 변경되지 않은 입력 데이터에서 작동하기 때문에 견고한 기반을 제공해줍니다. 이로써 논리 변경의 효과를 격리시킬 수 있습니다.\n\n## 2. 결정론적이고 아이덴포턴트한 버전 관리된 논리\n\n결정론적 논리는 동일한 입력을 주면 항상 똑같은 응답을 반환하는 함수를 의미합니다. 이는 LLMs의 특정 상황에서 흥미로울 것입니다. 왜냐하면 LLM은 본성상 결정론적이 아니기 때문에 동일한 프롬프트라도 다른 응답을 반환합니다. 그러나 응답 구조 자체가 결정론적이라면 LLM 응답을 여전히 기능적 데이터 엔지니어링 워크플로에 활용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아이덴포턴시는 두 번째 중요한 제약 사항으로, 반복된 논리적 실행이 초기 실행에서 발생한 최종 상태를 변경하지 않는다는 것을 본질적으로 의미합니다. 간단한 INSERT 문을 고려해 보겠습니다: 이를 여러 번 실행하면 의도적으로든 우연히든 중복된 행이 생성되는 원치 않는 부작용이 발생합니다. 이는 INSERT가 아이덴포턴트 작업이 아님을 의미합니다. 아이덴포턴시는 특정 날짜 파티션을 작성하거나 덮어쓰기만 할 수 있도록 작업의 허용된 출력을 제한함으로써 달성됩니다.\n\n부작용(즉, 외부 상태를 변경하지 않는)이 없는 결정론적이고 아이덴포턴트한 작업은 일반적으로 순수 작업이라고 합니다.\n\n재현성을 달성하기 위해 버전 제어는 중요한 능력입니다: 논리적 변경으로 인해 잘못된 최종 상태가 발생하면 이전 버전의 로직으로 되돌아가거나 출력 테이블을 다시 작성하는 것이 간단하게 되어야 합니다. 결과 데이터를 이전 버전으로 롤백하여 출력 데이터를 이전 버전으로 복원하는 효과적인 방법과 거의 동일해야 합니다.\n\n결과 데이터를 직접 버전 관리하는 것과는 다르지만, 영구적이고 변경할 수 없는 입력 데이터에 버전 관리된 로직을 적용하는 것은 출력 데이터 파티션을 버전 관리하는 것과 사실상 동일해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 3. 변경할 수 없는, 교체 가능한, 분할된 출력 데이터\n\n출력된 데이터 파티션은 변경할 수 없는 상태로 유지되며, 한 번 생성되면 데이터 파티션을 변경할 수 없습니다. 이는 UPDATE, DELETE, INSERT 또는 MERGE 작업이 허용되지 않음을 의미합니다. 즉, 행은 데이터 파티션 내에서 변이될 수 없습니다. 이는 배포된 변환 로직을 통해 출력 데이터 추적 가능성을 보존하여 파티션 및 행 수준의 추적이 가능하게 합니다.\n\n입력 및 출력 데이터 제약 사항 간의 주요 차이점은 출력 데이터 파티션은 영구적이지 않지만 교체 가능하다는 점입니다. 이들은 전체적으로 생성되거나 덮어쓸 수 있지만, 변환 실행 로직의 일부로 업데이트하거나 삭제할 수 없습니다.\n\n날짜 파티션 필드의 선택은 중요하며, 트레이드오프가 필요할 수 있습니다. 출력 데이터를 입력 데이터와 동일한 필드로 분할함으로써, 메타데이터 쿼리만을 기반으로 모니터링 및 자동화 작업을 진행할 수 있습니다. 이는 효율적이고 예측 가능하며 기본 데이터의 잠재적으로 비싼 스캔이 필요하지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 구현 로직\n\n이 기술의 실제 응용을 이해하기 위해서는 변환을 실행하기 위해 필요한 논리적 상호 작용을 명확히해야 합니다. 변환 논리가 출력 데이터 파티션을 생성하거나 대체하기 위해 실행되어야 할 작업을 결정하는 실행 논리를 제공하며, 실행 트리거는 실행 논리가 실행되는 시점을 결정합니다.\n\n![image](/assets/img/2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery_2.png)\n\n실행 트리거는 요청에 따라, 일정에 따라 또는 이벤트 기반(예: 새로운 들어오는 데이터 감지)일 수 있으며, 실행 논리는 그 후에 변환 논리가 어떻게 배포되며 필요 시 어떤 출력 데이터 파티션이 덮어쓰여야 하는지를 결정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n점진적 접근 방식은 메타데이터를 확인하여 입력 데이터 파티션 날짜가 아직 출력 데이터에 없는 경우를 식별하고, 그런 다음 변환 로직을 적용하여 새로운 출력 데이터 파티션을 만드는 것을 의미합니다. 그러나 이 작업은 새로 도착한 날짜 파티션에 대해서만 수행됩니다.\n\n더 복잡한 실행 로직을 구현할 수도 있습니다. 예를 들어, 늦게 도착한 데이터를 고려하여 최근 날짜 파티션의 추가 수를 덮어쓰거나, 특정 날짜 이전 또는 이후의 모든 출력 날짜 파티션을 덮어쓸 수도 있습니다.\n\n기술적 구현 옵션에는 천천히 변하는 차원에 대한 고려 사항이 있지만, 이는 이 기본 구현 범위를 벗어납니다.\n\n# 실행\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 제약 조건과 구현 논리를 고려할 때, 이 패러다임을 BigQuery에서 구현하는 운영 프레임워크의 각 측면을 정의하는 것은 실제로 매우 간단합니다. BigQuery 및 관련 기술에 대한 맥락을 고려하면, 다음 기사는 우리가 사용할 수있는 Google Cloud 리소스에 대한 넓은 개요를 제공합니다:\n\n이제 BigQuery에서 Functional Data Engineering 워크플로우를 구축, 실행 및 관리하기 위해 필요한 모든 리소스의 구현 옵션을 고려하고 평가하며, 다음 영역을 다루고 있습니다:\n\n- 인바운드 데이터\n- 변환 논리\n- 데이터 증가\n- 출력 데이터\n- 실행 논리\n- 실행 트리거\n- 모니터링\n- 버전 관리\n\n## 1. 인바운드 데이터\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은 BigQuery 데이터를 가져오는 메커니즘이 있지만, 입력 데이터가 분할되어야 하고 영구적이며 변경될 수 없다는 제약 조건으로 인해 다음 옵션이 선호됩니다:\n\n날짜별 테이블\n\n날짜별 테이블은 데이터를 효율적인 쿼리 패턴을 위해 서로 다른 물리적 파티션으로 분리하는 기본적인 방법입니다. 이를 함수형 데이터 엔지니어링 워크플로우의 입력 데이터로 사용할 수 있지만, 상류 프로세스는 변형 불가 제약 조건을 준수하고 데이터 파티션이 로드 후 변경되지 않도록 구성되어야 합니다.\n\n이를 지원하기 위한 메커니즘 중 하나는 흡수 시간에 의한 파티셔닝이며, 특정 데이터 집합 권한을 설정하여 데이터가 삭제되지 않고 영구적이 되도록 해야 합니다. 파티션된 테이블은 BigQuery 사용자 인터페이스에서 간단한 몇 번의 클릭으로 삭제할 수 있으므로 이를 방지하기 위한 보호장치가 마련되어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n날짜별 샤딩된 테이블\n\n날짜별 샤딩된 테이블은 Google에서 관리하는 가져오기 작업(예: GA4 BigQuery 내보내기)을 할 때 자주 만날 수 있습니다. 이들은 날짜별로 파티셔닝된 테이블과 유사하지만, 실제로는 접두사를 공유하고 날짜를 기반으로 한 접미사를 가진 기술적으로 별개의 테이블입니다. 이들의 주요 이점은 날짜별로 파티셔닝된 테이블에서 가능한 효율적인 쿼리 패턴을 달성할 수 있는 와일드카드 및 _TABLE_SUFFIX를 결합하여 질의할 수 있다는 것입니다.\n\n저장소 폴더 구조\n\nGoogle Cloud Storage와 BigQuery 외부 테이블을 결합한 접근 방식은 데이터 상호작용과 관리 사이의 분리를 제공하면서 불변성과 영속성 제약 조건을 달성하기에 매우 견고하고 효과적입니다. GCS 버킷에 파일로 저장된 데이터는 직접 쿼리할 수 있지만 BigQuery에서 삭제할 수는 없어 데이터 삭제에 대한 추가적인 보안 계층을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n_FILE_NAME 가상 열을 변환 로직에 포함시킬 수 있습니다. 이는 추가 입력 메타데이터를 제공하기 위해 구문 분석될 수 있습니다. 이는 잘 설계된 날짜 기반 폴더 구조(서로 다른 파일 유형을 별도 계층으로 구분)와 결합되어, 이 방법은 날짜별 파티셔닝 또는 날짜별 샤딩된 테이블이 달성할 수 있는 효율적인 쿼리 패턴을 복제하는 데 사용될 수 있습니다.\n\n## 2. 변환 로직\n\nBigQuery 뷰는 임의의 SQL을 데이터의 기저에있는 개요 데이터의 일시적인 뷰로 작성할 수 있는 유용한 자원입니다. SQL 기반의 변환은 출력 스키마와 데이터를 결정하며, 검사 가능한 스키마를 가지고 있어 매우 유용할 수 있지만 실제로는 널리 남용됩니다.\n\n쿼리시 뷰는 항상 모든 기저 데이터를 쿼리하고 다시 계산하므로 매우 비효율적입니다. 뷰는 종종 주기적으로 전체 하위 테이블을 다시 빌드하는 데 사용되며, 이는 불필요하게 비싼 방법이며, 입력 데이터 양이 증가함에 따라 비용이 증가할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 Table Functions을 사용하여 날짜 분할 기능을 추가하면 Views의 대부분의 긍정적인 특성을 활용할 수 있습니다.\n\nTable Functions\n\nTable Functions은 Views와 논리적으로 동일하지만 한 가지 중요한 측면에서 다릅니다. Table Functions은 인수를 취할 수 있으며, 이러한 인수는 SQL 쿼리에서 변수로 사용할 수 있습니다. 이것은 보기를 사용하면서도 날짜 파티션 하위 집합을 쿼리할 수 있는 강력한 기능을 제공합니다.\n\n이 인수들은 Table Function SQL의 구조적 부분(예: 열 또는 테이블 이름)을 형성할 수 없다는 점에 유의하십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n날짜 범위 테이블 함수\n\n날짜 범위 테이블 함수는 정확히 두 개의 DATE 인수인 start_date와 end_date를 사용하는 테이블 함수의 구현에 불과합니다. 테이블 함수 SQL이 우리의 영구적이고 변경할 수 없는 입력 데이터를 참조할 때, 쿼리 문에 WHERE 절을 추가하여 이러한 날짜 사이의 파티션 범위에 대한 데이터만 쿼리하고 반환할 수 있습니다.\n\n날짜 범위 테이블 함수를 연결하여 변환 로직을 별도의 단계로 분리할 수 있으며, 이로 인해 독립적으로 개발, 쿼리 및 테스트할 수 있게 됩니다.\n\n다음 예제 논리 구조는 날짜 범위 테이블 함수의 시퀀스에서 구성됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n외부 쿼리가 최종 Table Function(TF12)을 호출할 때, 쿼리 데이터 범위를 start_date 및 end_date 인수로 전달하면 해당 인수가 Table Functions의 일련의 순서를 통해 상위로 전파되며 입력 데이터에서 특정 날짜 범위만 쿼리됩니다. 변환 로직은 이후 이 날짜 범위에만 적용되고, Table Function은 변환된 데이터 분할을 반환하여 검사, 분석 또는 출력 테이블의 기존 분할에 덮어쓸 준비가 됩니다.\n\n## 3. 데이터 증강\n\n변환 로직은 일반적으로 데이터 구조 변경, 조인, 집계 및 계산을 포함하나, BigQuery는 입력 데이터에서 출력 데이터 분할로 전달되는 데이터를 더욱 증강하는 강력한 도구 세트를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사용자 정의 함수 (UDF)\n\n사용자 정의 함수는 강력하고 모듈식이며 재사용 가능한 기능으로 로직을 캡슐화하는 깔끔한 메커니즘입니다. SQL 또는 Javascript로 작성한 후에 SQL 쿼리 내에서 사용하여 데이터에 사용자 정의 분류, 계산 또는 기타 작업을 추가할 수 있습니다.\n\nBigQuery ML: AI 함수\n\nBigQuery ML (머신 러닝)에는 강력한 ML 모델을 바로 호출할 수 있게 하는 여러 내장 함수가 있습니다. 이 함수들은 입력부터 출력으로 흘러가는 데이터를 개선할 수 있도록 ML.UNDERSTAND_TEXT, ML.TRANSLATE, ML.ANNOTATE_IMAGE, ML.TRANSCRIBE 및 ML.PROCESS_DOCUMENT을 포함합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 경우에 대해 번역하면 다음과 같습니다.\n\n이러한 것들에는 경우에 따라 상당한 추가 비용이 소요되며, API 속도 제한을 피하고 일괄 처리 전략이 필요할 수도 있습니다.\n\n또한 ML.ANNOTATE_IMAGE 및 ML.PROCESS_DOCUMENT는 Google Cloud Storage 버킷에 호스팅된 파일로 분석해야 하며, BigQuery에서는 Object Table이 객체 인벤토리 역할을 하게 됩니다.\n\nBigQuery ML: 생성적 AI 기능\n\nBigQuery ML.GENERATE_TEXT 함수를 사용하면 간단한 SQL 쿼리를 통해 Google LLMs(gemini-1.5-flash, gemini-1.5-pro, gemini-pro, gemini-pro-vision,text-bison,text-bison-32,text-unicorn)에 직접 액세스할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기능을 사용하면 추가 비용이 발생할 수 있으며 안정적인 성능을 얻기 위해 일괄 처리 전략이 필요할 수도 있습니다.\n\n생성 AI 함수는 본질적으로 결정론적이지 않기 때문에 순수한 기능 데이터 엔지니어링 워크플로에 허용되지 않을 수 있습니다. 그러나 응답 구조가 결정론적인 경우, 이러한 경우처럼 변활 논리에 실제로 포함될 수 있으며 LLM의 응답은 출력 데이터에서 액세스할 수 있습니다. 다만 출력 데이터 파티션이 덮어쓰기되면 포함된 응답은 이전 응답과 다를 수 있습니다.\n\n원격 함수\n\n원격 함수는 Cloud Functions에 대한 액세스 포인트를 제공하여 다양한 언어로 사용자 정의 코드를 작성하고 외부 시스템과 상호 작용한 다음 BigQuery에서 응답을 인라인으로 반환할 수 있습니다. 이를 통해 BigQuery의 영역을 데이터 이상 및 자동화 사용 사례에 대한 Google 및 외부 API의 광범위한 세계에 확장할 수 있습니다. 그러나 Cloud Functions은 추가 개발, 인프라, 모니터링, 권한 및 디버깅 오버헤드가 따르므로 내장 또는 사용자 정의 함수를 사용하는 것이 더 좋습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4. 출력 데이터\n\n정확하고 신뢰할 수 있는 출력 데이터를 작성하고 유지하는 것이 기능 데이터 엔지니어링 워크플로우의 주요 목표입니다. 이 출력 데이터는 일반적으로 다음과 같은 형태를 띕니다:\n\n- 날짜별 테이블\n\n날짜별 테이블은 기능적 데이터 엔지니어링에서 선호하는 출력 구조입니다. 필요에 따라 클러스터링된 고카디널리티 열로 구성되며, 추가 필터링이 적용될 것으로 예상되는 열로 클러스터링될 수 있습니다. 파티션 가지치기를 통해 효율적인 쿼리 수행이 가능하며 (필요한 특정 데이터 파티션만 쿼리함), 쿼리할 데이터양이 줄어듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파티셔닝된 테이블 메타데이터는 INFORMATION_SCHEMA.PARTITIONS 뷰를 통해 조회할 수도 있습니다. 이는 모니터링 및 자동화 활동에 중요합니다.\n\n파티셔닝된 테이블은 최대 10,000개의 파티션을 가질 수 있으며, 따라서 날짜별로 분할된 테이블은 하루 날짜 파티션을 27년 이상 보유할 수 있습니다.\n\n클라우드 스토리지 버킷\n\n출력 데이터는 EXPORT DATA 문을 사용하여 다양한 파일 및 압축 형식으로 Google Cloud Storage 버킷에 내보낼 수도 있습니다. 그런 다음 이 데이터는 BigQuery에서 외부 테이블을 사용하여 압축 파일의 일반적으로 더 낮은 비용의 저장소인 Cloud Storage에서 액세스할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGoogle Cloud Storage 버킷 내용은 Object Tables을 사용하여 BigQuery에서 나열할 수 있습니다. 이를 통해 모니터링 및 자동화 기능의 기초로 활용해볼 수 있습니다. 그러나 모니터링 및 자동화는 URI(전역적으로 고유한 파일 경로)에서 날짜 메타데이터를 구문 분석하는 것에 따라 약간 복잡할 수 있습니다.\n\n또한 데이터를 Amazon S3 버킷이나 Azure Blob Storage로 내보내어 안전한 크로스 클라우드 데이터 통합을 할 수도 있습니다.\n\n## 5. 실행 로직\n\n기능 데이터 엔지니어링 워크플로우의 논리적 실행은 두 가지 핵심 BigQuery 기능에 의해 주도됩니다: 리소스 메타데이터에 액세스하기 위한 INFORMATION_SCHEMA 뷰 및 원하는 결과에 따라 기능 작업을 실행하기 위한 PROCEDURES입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 메타데이터 비교\n\n입력 및 출력 데이터가 파티션 열을 공유하도록 제한했다면, 입력 데이터와 출력 데이터 간의 메타데이터 비교는 차이점을 식별하는 매우 간단한 방법을 제공합니다. 또한 이는 모니터링 및 자동화를 구축하는 효율적인 기반으로 작용하며, 상태를 외부 저장소(예: 최신 실행 결과를 저장하는 테이블)에 저장할 필요가 없고 기본 데이터를 쿼리할 필요도 없습니다. 이것은 운영 비용을 예측 가능하고 낮출 수 있습니다.\n\n기능 캡슐화\n\n입력 데이터 테이블의 날짜 파티션 또는 샤드 배열을 반환하는 데 필요한 SQL을 수동으로 작성하는 것은 가능하지만, 우리의 선호하는 접근 방식은 그 자체로 기능적입니다. SQL 사용자 정의 함수(UDF)를 활용하여 SQL 컴파일 및 프로시저를 실행하기 위한 프로시저를 사용함으로써 자동화에 기능적 데이터 엔지니어링 접근을 지원하는 일부 유틸리티 함수에 액세스할 수 있습니다. 파티션된 테이블의 최신 날짜 파티션을 가져오기 위한 예시 기능 사용 코드는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nDECLARE source_table_id STRING;\nDECLARE last_partition_date DATE;\n\nSET source_table_id = 'myproject.mydataset.mypartitioned_table';\nCALL [project].[region].get_last_partition_date (source_table_id, last_partition_date) \n```\n\n이 예시에서 get_last_partition_date를 성공적으로 실행한 후, source_table_id로 참조된 테이블에 유효한 날짜 분할이 있는 경우, get_last_partition_date DATE 변수는 최신 날짜 값을 설정하고 후속 논리 단계에서 사용할 수 있습니다.\n\n비교적 작은 범위의 유사한 메타데이터 함수를 사용하여 실행 로직을 간단하게 표현하고 자동화 활동에 대한 신뢰할 수 있는 기반으로 사용할 수 있습니다.\n\n실행 모드\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기능 데이터 엔지니어링 플로우의 개발, 디버깅, 업데이트 및 유지 관리를 도와주는 여러 실행 모드가 있습니다. 첫 번째 두 가지는 다음과 같습니다:\n\n- 전체 테이블 새로고침 — 첫 번째부터 마지막 파티션까지 모든 데이터 덮어쓰기\n- 증분 새로고침 — 새 데이터가 도착했을 때 가장 최근의 n개 파티션을 덮어쓰기\n\n이러한 것들이 기본 실행 모드이지만, 추가적으로 도움이 되는 몇 가지 실행 모드가 있으며, 가속화되고 효율적인 개발 및 디버깅을 지원합니다:\n\n- 날짜 범위 새로고침 — 특정 날짜 범위 내의 모든 파티션 덮어쓰기\n- 최신 정보 새로고침 — 특정 날짜까지의 파티션 덮어쓰기\n- 시작 날짜 새로고침 — 특정 날짜부터 파티션 덮어쓰기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 실행 모드의 조합은 함수형 데이터 엔지니어링 흐름의 실행에 대한 세밀한 제어를 제공합니다.\n\n함수형 데이터 조작\n\n출력 날짜 파티션을 덮어쓰는 데 하드 코딩된 DML(Data Manipulation Language)을 사용할 수 있지만, 이렇게 하면 스크립트가 사용 사례별로 제한되며, 코드를 복사하고 편집하지 않으면 재사용할 수 없습니다.\n\n메타데이터 함수에도 동일한 방식을 적용하고 SQL UDF(User-Defined Functions), 프로시저(Procedures), 트랜잭션을 결합하여 날짜 파티션을 덮어쓰는 함수를 방한 방법으로 구축합니다. 이는 덮어쓰기 함수가 사실 DELETE 및 INSERT DML 문으로 구성되지만, 이것이 TRANSACTION 내에서 발생한다는 사실은 어떤 실패가 발생하더라도 롤백되므로 트랜잭션이 방한적이라는 것을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기능 권한\n\n함수 및 프로시저에 대한 액세스 권한을 허용할 때 주의해야 할 두 가지 중요한 기능이 있습니다:\n\n- 사용자 권한 — 사용자는 이메일, Google 그룹 또는 서비스 계정을 통해 지정된 데이터 세트에서 함수를 실행할 수 있는 권한을 부여받을 수 있습니다.\n- 루틴 권한 — 루틴에는 지정된 데이터 세트에서 함수를 실행할 수 있는 권한이 부여될 수 있습니다.\n\n즉, 사용자는 외부 프로젝트의 단일 데이터 세트에 대한 BigQuery Viewer 액세스를 부여받을 수 있으며, 해당 데이터 세트에 대한 권한만 있으면 데이터 세트 함수와 그에 허용된 종속 함수를 성공적으로 실행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n6. 트리거 로직\n\n실행 함수는 데이터의 신선도에 따라 여러 가지 다른 메커니즘을 통해 트리거될 수 있습니다.\n\n요청 시\n\n실행 함수는 필요에 따라 호출될 수 있으며, 후속 활동 직전에 BigQuery 콘솔에서 배포된 함수를 실행함으로써 실행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**예약된 쿼리**\n\n예약된 쿼리는 주기적인 일정에 맞게 실행 함수를 트리거하는 가벼운 방법으로, 실패 시 내장된 이메일 알림(간단한 경보 시스템을 위해 Slack로 자동 전달될 수 있음)이 제공됩니다. 쿼리 레이블을 추가하면 특정 쿼리 레이블을 기반으로 필터링하여 처리량 및 추정 비용별로 분석할 수 있습니다.\n\n예약된 쿼리는 복잡한 코드를 작성하기에는 권장되지 않으며, 해당 코드는 BigQuery에서 볼 수 없습니다. 예약된 쿼리의 결과를 테이블에 삽입하는 것은 비멱득 연산이며, 따라서 기능형 데이터 엔지니어링 흐름에서 피해야 합니다. 쓰기/잘라내기 연산은 멱득 연산이지만 전체 테이블 덮어쓰기의 처리 비용이 시간이 지남에 따라 증가하므로 일반적으로 비효율적입니다.\n\n이벤트 기반 트리거\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n빅쿼리에 데이터가 도착할 때 함수나 쿼리를 트리거하는 간단하고 기본 메커니즘이 없습니다. 이상적으로는 PubSub 주제를 테이블이나 데이터 세트에 연결하여 새 테이블이나 파티션에 도착할 때 트리거되고, 다른 임의의 함수 실행을 트리거할 수 있으면 좋을 것입니다. 그러나 현재는 이를 외부 도구 없이 실행할 수 없으므로 일반적으로 출력 데이터의 원하는 데이터 신선도와 일치하는 빈도로 예약된 실행 쿼리를 배포합니다.\n\n## 7. 모니터링\n\n빅쿼리에서 데이터 워크플로우를 모니터링하는 것은 다양한 방법을 결합하여 구현할 수 있습니다.\n\n메타데이터 모니터링\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nThe INFORMATION_SCHEMA views can be used to monitor resource metadata in order to manage and optimize resource, partition, and storage infrastructure.\n\nLog Monitoring\n\nThe INFORMATION_SCHEMA.JOBS view is used to monitor query patterns, compute volume and associated costs, and to isolate unnecessary compute-related expenses.\n\nQuery Labelling\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n테이블 태그를 Markdown 형식으로 변경해 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저장된 쿼리는 BigQuery 내에서 버전 관리를 구현하는 초보자용 방법이지만, 정의된 프로세스를 엄격히 준수하여 사용해야 합니다. 현재 BigQuery에서 코드 버전을 관리하는 강인한 독립형 솔루션이 아닙니다.\n\nGit 저장소 통합\n\n지금까지 BigQuery에서 코드를 버전 관리하는 것은 매우 어려웠지만, Git 저장소 통합이 현재 시험 중이므로 주목해 주세요! 일부 테스팅을 거친 후에 이 새로운 (그리고 매우 환영받는) BigQuery 기능을 함수형 데이터 엔지니어링 워크플로에 어떻게 사용하는지에 대한 업데이트를 올릴 수 있을 것입니다.\n\n# 구현\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기까지 오신 여러분은 BigQuery에서의 기능적 데이터 엔지니어링에 관심이 있는 분들이시군요... 멋지네요!\n\n우리가 구현하고 확장하며 기능적 데이터 엔지니어링 워크플로우를 관리하기 위해 사용하는 기능 라이브러리에 대한 문서는 여기에서 확인할 수 있습니다: bqtools.\n\n또한, 이 기능적 데이터 엔지니어링 접근 방식을 활용하여 Google Analytics 4 데이터의 자동 전처리 및 증강을 실행하는 첫 번째 제품을 개발했습니다. 자세한 내용은 여기에서 확인할 수 있습니다: Decode Data for GA4.\n\n이 자료에서 여러분이 BigQuery 여정에서 도움이 될만한 유용한 자료가 있다면, 저희에게 문의해 주세요. 도움이 필요한 사항이 있다면 언제든지 연락해 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n행복한 기능성 데이터 엔지니어링!\n\n🔎 변환 플로우에 대해:\n\n저희는 기업 및 기관들이 Google Cloud에서 빅쿼리를 중심으로 데이터 기능을 설계, 배포 및 관리할 수 있도록 지원합니다.\n\n빅쿼리 프로그래밍에 대한 깊은 전문 지식을 가지고 있기 때문에, 기능적 데이터 엔지니어링, 빠른 능력 개발 및 비용/보안 감사를 지원하기 위해 빅쿼리 기능을 확장하는 라이브러리를 구축합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 내부 원격 함수 라이브러리에서 배포할 수도 있고, 사용 사례에 맞춰 BigQuery 기능을 확장하기 위해 사용자 정의 함수를 개발할 수도 있습니다.\n\n당신의 BigQuery 여정이 어디에 있든 우리가 도와드릴 수 있어요. 여기까지 읽으셨다면 직접 jim@transformationflow.io 로 연락하시거나 여기에서 상담 예약해주세요! 🔎","ogImage":{"url":"/assets/img/2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery_0.png"},"coverImage":"/assets/img/2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery_0.png","tag":["Tech"],"readingTime":16},{"title":"품질 엔지니어를 위한 RAG 사용 가이드","description":"","date":"2024-06-22 17:14","slug":"2024-06-22-RAGforQualityEngineers","content":"\n\n## RAG 만들기는 쉽지만, 품질 있는 RAG 만들기는 어려워요\n\n![image](/assets/img/2024-06-22-RAGforQualityEngineers_0.png)\n\n검색 확장 생성(RAG)은 대규모 언어 모델(LLMs)의 기능을 확장하는 일반적인 패턴이 되었습니다.\n\n이론적으로 RAG는 간단합니다(컨텍스트 창에 데이터를 추가하기만 하면 됩니다!) 하지만 실제로는 복잡합니다. 상자 다이어그램 너머에는 고급 청킹 전략, 재랭킹, 다중 쿼리 리트리버, 작은 데이터부터 큰 데이터 검색, 가상 문서 임베딩, 사전 임베딩 데이터 보강, 동적 라우팅, 맞춤 임베딩 모델... 등이 숨어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n초기 파이프라인을 설정하는 것은 빠르고 쉽지만, 프로덕션 수준의 품질에 도달하는 것은 상당히 복잡합니다. 신중한 고려 없이 RAG 시스템은 부정확하거나 관련성이 없는 정보를 반환할 수 있습니다. 비효율적으로 비싼 리소스를 소비하거나 프로덕션 규모의 소스 데이터로 확장할 때 병목 현상이 발생할 수도 있습니다.\n\nRAG 시스템의 품질을 효과적으로 평가하고 효율적으로 이해하는 것은 각 개별 구성 요소가 전체 RAG 파이프라인을 만드는 과정을 이해하는 데 필요합니다. 이러한 각 부분에 대한 설계 결정은 품질에 영향을 미치며, RAG 애플리케이션을 배포하려는 모든 사람이 알아야 합니다.\n\n이 글에서는 테스트와 품질 관점에서 RAG 개념과 패턴에 대한 소개를 제공합니다. RAG가 왜 가치 있는지에 대한 소개로 시작하여, 프로덕션 품질의 RAG를 구축하는 데 내재된 많은 설계 결정이 어떻게 향상되는지에 대해 설명합니다. 이 소개는 특정 평가 방법과 기법에 대해 논의하기 전에 필요한 기초를 제공할 것입니다.\n\n# LLM의 한계\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRAG 파이프라인을 이해하려면, RAG가 대응하려는 LLM의 한계를 먼저 이해해야 합니다.\n\nLLM의 핵심은 간단합니다: 프롬프트를 보내면 응답을 받습니다.\n\nLLM이 응답을 반환하려면 모델과 추론 계산을 실행해야 합니다. 이 계산은 입력을 수백억 개 또는 수조의 매개변수와 결합하는 것을 포함합니다. 이는 비용이 많이 드는 계산입니다.\n\nLLM을 호출하는 것만큼이나 LLM을 훈련시키는 것은 훨씬 어렵습니다. 훈련은 모델 내 매개변수에 대한 최적값을 결정하는 과정입니다. 최상의 가중치를 계산하는 데 사용되는 다양한 알고리즘이 있지만, 모두 특정 입력에서 모델을 실행하고 오차를 계산한 후에 조정을 하는 반복적 과정을 포함합니다. 이 과정은 많은 횟수로 많은 입력에서 계속되며, 결국 훈련된 모델을 얻게 됩니다. 모델 추론은 몇 초만에 완료될 수 있지만, 모델 훈련은 광범위한 GPU 클러스터에서도 몇 주가 걸릴 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-RAGforQualityEngineers_1.png\" /\u003e\n\n엄청난 교육 비용은 새로운 정보를 LLM에 통합하는데 병목 현상을 야기합니다. 대부분의 기업은 모델을 교육할 자원이 없으며, 단순히 사적 데이터로 교육하여 LLM에 \"새로운 정보를 추가\"할 수 없습니다. 대신, 잘 자금을 지원받는 대규모 기술 기업은 대형 공개 데이터 세트에서 일반 목적의 기반이 되는 모델을 교육하고, 이러한 모델은 RAG와 같은 보조 프로세스로 새로운 능력 및 정보가 부가됩니다.\n\n구체적으로 RAG는 새로운 모델을 교육하는 높은 비용을 우회하는 방식으로 LLM이 추가적인 지식에 접근하도록 하는 것을 목표로 합니다.\n\n# RAG 기본 원리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRAG가 실제로 작동하는 방식에 대해 알아봅시다.\n\nLLM에 전송된 프롬프트는 제한된 길이인 context window을 가지고 있습니다. Context window은 토큰(단어에 대한 대략적인 동등물) 단위로 측정됩니다. Context window의 크기는 보통 1K, 4K 또는 그 이상의 토큰으로 표시되지만 더 큰 context window이 사용 가능해지고 있습니다(예: Gemini 1.5 Pro의 128K).\n\n많은 사람들이 직관적으로 context window은 할 수 있는 가장 긴 질문이라고 생각하지만, 이는 context window에 대한 한정적인 사고 방식입니다. LLM의 작동 방식으로 인해, context window에 제공된 정보는 LLM이 응답을 생성하는 동안 LLM에게 사용 가능합니다. 따라서 추가 정보를 제공하는 데 사용될 수 있습니다. 일반적으로 이를 in-context learning 이라고 합니다.\n\n따라서, 우리는 context window을 사용하여 LLM이 질문에 답변하기 위해 필요한 새로운 지식을 제공할 수 있습니다. 예를 들어, 우리가 상조에 관한 회사 정책에 대해 물어보는 프롬프트를 만들고, context window 내에서 이에 관한 전체 회사 안내서(상조에 관한 섹션 포함)를 넣는다면 새로운 정보를 제공하여 LLM이 응답할 수 있게 할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해당 솔루션은 이미 관련 정보가 있고 그 정보가 문맥 창 안에 쏙 들어갈 수 있는 경우 간단합니다. 불행히도 항상 그런 것은 아닙니다. 따라서 우리는 우리의 프롬프트에 관련 정보만 검색 및 다운 선택할 수 있는 메커니즘이 필요합니다.\n\n무식한 접근법은 프롬프트에서 용어를 검색하여 관련할 수 있는 데이터 전체에서 주변 텍스트를 복사한 후 이를 프롬프트에 추가하는 것입니다.\n\n이 간단한 키워드 검색 형태의 RAG는 LLM 응답을 향상시킬 수 있으며 어떤 맥락에서는 유용할 수 있지만 가끔식 거짓 양성 (다른 맥락에서 사용된 키워드) 때문에 문제가 될 수도 있습니다. 다행히도 우리는 텍스트의 의미에 맞추어 일치시키는 의미 검색을 활용하여 더 나은 결과를 얻을 수 있습니다.\n\n구체적으로, 우리는 포지블리 관련 데이터 청크에서 임베딩 모델을 활용하여 임베딩을 생성한 다음 이 임베딩을 통해 데이터를 검색하여 우리의 프롬프트에 관련된 데이터를 찾을 수 있습니다. 이 방법은 매우 단순화된 접근법이지만, 진정한 RAG와 같은 결과를 얻기 시작하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# RAG 및 임베딩\n\n간단한 키워드 검색보다 RAG가 제공하는 이점을 이해하려면 임베딩 모델의 목적과 성격을 이해해야 합니다. 이는 그 자체로 심도 있는 주제이지만, RAG를 이해하는 데 중요합니다.\n\n임베딩 모델은 우리의 원래 LLM과 유사하지만, 새로운 콘텐츠를 생성하는 대신 입력을 벡터(숫자 목록)로 축소합니다. 임베딩 모델은 매우 큰 숫자 목록입니다. 임베딩 모델이 생성하는 벡터는 일반적으로 768 또는 1536 숫자(차원)지만, 다른 크기의 벡터도 존재합니다.\n\n임베딩 모델에 의해 생성된 벡터는 단순히 임의의 숫자 세트가 아니라 모델에 따라 입력 데이터의 의미를 요약한 것입니다. 이 벡터는 다른 모델에게는 의미가 없지만 \"유사한\" 텍스트는 같은 모델에서 유사한 벡터를 생성할 것입니다. \"유사하다\"는 단순히 \"동일한 키워드를 가지고 있다\" 이상을 의미합니다. 임베딩 모델은 구조화되지 않은 데이터로부터 보다 심층적인 의미를 추출하기 위해 특별히 훈련되었습니다. 예를 들어 \"남자 말이 날지 않는다\"와 \"날개가 있는 사나이가 말 타고 있다\"는 비슷한 단어를 가지고 있지만 같은 모델에서는 서로 멀리 떨어진 벡터를 생성할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n벡터의 멋진 점은 그들에게 수학 연산을 수행할 수 있다는 것입니다. 빠른 수학이죠. 수백만 개의 벡터를 검색하여 비교적 짧은 시간 안에 유사한 벡터를 찾을 수 있습니다. (여기에 사용된 일부 알고리즘이 있습니다.)\n\n이제 우리의 RAG 파이프라인 조각들이 모두 마련되었으니, 단계별로 진행해봅시다.\n\n첫 네 단계는 한 번에 수행되거나 소스 데이터가 변경될 때 업데이트됩니다. 다섯 번부터 여덟 번까지의 단계는 각 추론 요청마다 수행됩니다:\n\n- 모든 가능성 있는 데이터를 수집합니다. - 이토록 많은 데이터가 있어서 우리의 프롬프트의 컨텍스트 창에 쏙 들어가기 불가능합니다.\n- 이 데이터를 더 작은 조각으로 나눕니다 (나중에 자세히 설명하겠습니다).\n- 그런 다음 각 조각을 임베딩 모델을 통해 실행하여 조각의 의미를 포함하는 벡터를 생성합니다.\n- 해당 벡터를 벡터 데이터베이스에 저장합니다.\n- 각 추론 요청으로: 프롬프트를 받으면, 그 프롬프트를 조각낸 소스 데이터와 동일한 임베딩 모델을 통해 실행하여 다른 벡터 (프롬프트 벡터 또는 쿼리 벡터라고 함)를 생성합니다.\n- 우리의 벡터 데이터베이스에서 우리의 프롬프트 벡터와 유사한 벡터를 검색합니다. 반환된 벡터들은 생 데이터를 키워드로 검색했다면 얻었을 것보다 더 나은 매치일 것입니다.\n- 식별된 관련 벡터를 (선택적으로) 재정렬하고, 그런 다음 상위 벡터 각각의 생 데이터를 반환합니다.\n- 원시 데이터는 초기 프롬프트와 결합되어 LLM으로 보내집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_2.png)\n\n여기, 우리 LLM은 이제 새로운 독점 데이터를 모두 훈련한 것처럼 작동하며, 기본 모델 훈련을 비용 문제로 수행할 필요가 없습니다.\n\n이론상으로는 이렇게 작동해야 합니다. 그러나 실제로는 이 너무 단순한 파이프라인은 여러분의 제품 요구를 만족시키지 못할 가능성이 높으며, 우수한 품질의 제품에 도달하려면 RAG 파이프라인을 준비하려면 특정 애플리케이션의 요구를 충족시키기 위해 다양한 부분을 적응, 개선, 교체 또는 확장해야 할 것입니다.\n\n# RAG 디자인과 품질\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서는 RAG를 소개하지만, RAG는 실제로 매우 복잡할 수 있으며, 이러한 실제 세계의 복잡성은 응용 프로그램의 품질에 영향을 미칠 수 있습니다. RAG 파이프라인 내에서 사용 가능한 일부 구현 도전, 품질 위험 및 대안을 이해하기 위해 각 단계를 살펴보겠습니다.\n\n## #1—관련 데이터 수집, 적재 및 풍부화\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_3.png)\n\n시작부터 모든 \"가능성 있는 데이터\"를 찾아야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 RAG 다이어그램에서 1번 아이콘은 여러 소스에서 데이터를 수집하고, 정리하고, 변환하며, 익명화하고, 토큰화하는 데이터 파이프라인(또는 파이프라인 세트!)일 가능성이 높습니다.\n\n일부 파이프라인은 특히 텍스트 이외의 형식을 가진 원시 데이터의 경우 매우 복잡해질 수 있습니다. 예를 들어, 일부 파이프라인은 대량의 스캔된 물리 문서를 처리하기 위해 OCR 기술을 널리 활용합니다.\n\n데이터 파이프라인의 복잡성은 데이터 파이프라인 테스트의 모든 과제가 따라옵니다.\n\n가장 잘 구현된 RAG 파이프라인도 소스 데이터가 심지어 벡터 DB로 전달되지 않는다면 완전히 실패할 수 있으며, 이 데이터의 다양성, 속도 및 양에 따라 RAG의 이 단계는 복합적이며 많은 응용프로그램 품질 문제의 원인이 될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적인 데이터 파이프라인 활동에 추가로, RAG는 데이터 풍부화를 통해 혜택을 얻을 수 있습니다. 종종, 다른 시스템(또는 사람들)은 소스 데이터에 대한 맥락을 알고 있어서 그 의미를 평가하는 데 엄청난 도움이 될 수 있습니다. 예를 들어, 고객 데이터베이스에는 다른 시스템에서 제공하는 태그나 주석과 같은 관련 정보를 추가하여 데이터를 풍부화할 수 있습니다. 종종, 다른 생성 모델이나 자연어 처리(NLP)가 더 깨끗하거나 요약된 메타데이터를 생성하는 데 사용됩니다. 모두 \"임베딩 생성\" 전의 \"전처리\"로 생각해보세요. 그리고 제대로 수행된다면, 검색 품질을 크게 향상시킬 수 있습니다.\n\n당신이 RAG 검색 시스템의 품질을 평가하고 있다면, 데이터가 실제로 어떻게 소스되고 흡수되는지 이해하는 데 시간을 투자하는 것이 가치가 있습니다. 훌륭한 AI 부분에 도착하기 전에 RAG 파이프라인에 도달하기 전에 데이터가 어떻게 가져오고 처리되는지 알아두세요.\n\n## #2—Chunking\n\n![2024-06-22-RAGforQualityEngineers_4](/assets/img/2024-06-22-RAGforQualityEngineers_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터가 수신되고 임베딩 모델을 실행하기 전에는 데이터를 diskrete pieces로 나눠야 합니다. 그렇다면 데이터를 어떻게 분할할지 어떻게 결정하나요? 이를 chunking strategy라고 합니다.\n\n최적의 크기는 얼마나 크거나 작아야 할까요? 청크들은 서로 중첩되어야 할까요? 페이지, 단락 또는 일정한 길이로만 나누는 것 이외에 더 스마트한 분할 방법이 있을까요? 비표준 형식의 데이터(code, JSON 등)는 어떻게 chunk해야 할까요?\n\n이러한 질문들은 chunking strategy가 답하려고 노력하는 것이며 완벽한 해결책은 없습니다. 서로 다른 전략은 서로 다른 타협점을 갖습니다. 일부는 간단하고 빠르게 구현할 수 있으며, 보통 결과를 제공합니다. 다른 전략은 더 복잡하고 관련이 깊습니다. 더 나은 히트율과 LLM 응답 품질을 제공할 수 있습니다. 데이터를 너무 거칠게 나누면 의미 없는 데이터로 context window를 채우거나 다른 관련 청크를 밀어내거나 의미 있는 일치를 얻을 수 없을 정도로 일반적인 임베딩을 생성할 수 있습니다. 너무 세밀하게 나누면 관련 데이터를 잘라낼 수 있습니다.\n\n이 문서에서는 다섯 가지 chunking 범주를 탐구합니다: 고정 크기, 재귀, 문서 기반, 의미 기반, 그리고 AI를 사용한 Agentic(체킹에 인공 지능 사용, 멋지죠!)입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은 기타 접근 방식이 있습니다. 예를 들어, 작은 것에서 큰 것으로 검색을 최적화하려면 작은 청크를 사용하지만 각 청크는 큰 부모 청크에 연결되어 있어서 삽입될 컨텍스트 모델에 검색됩니다. 콘텍스트 인식 청킹은 문서의 성격에 대해 기존 지식을 활용하여 문서를 논리적 청크로 적절하게 분할합니다.\n\n위 목록은 아마 완전하지 않을 수 있지만, RAG 구현자가 사용할 수 있는 다양한 옵션과 애플리케이션 전체의 품질에 적합하고 조정된 청킹 전략의 중요성을 보여줍니다. Pinecone 블로그에서 이러한 전략 중 많은 내용을 자세히 다루고 있습니다.\n\n## #3—임베딩 모델 선택과 구성\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n임베딩을 생성하는 데 사용할 수 있는 여러 모델이 있습니다. 서로 다른 모델은 다양한 상황에서 더 나은 또는 나쁘게 수행할 수 있습니다. 일부 모델은 일반 사용을 위해 사전 훈련되어 있고 일부는 특정 도메인(예: 의학 기록)에 대해 세밀하게 조정되어 있습니다. 또한 응용 프로그램에서 처리하는 특정 데이터에 대해 자체 임베딩 모델을 세밀하게 조정할 수도 있습니다.\n\n또한 많은 모델이 다른 크기로 제공되며(임베딩 생성 비용 및 시간에 영향을 미침), 다른 입력 길이(처리 가능한 최대 청크 크기) 및 다른 출력 벡터 차원(높은 차원 = 더 정확하지만 더 많은 공간 요구와 느린 속도)으로 제공됩니다.\n\n일부 임베딩 모델은 API를 통해만 액세스할 수 있습니다(예: OpenAI 임베딩 엔드포인트), 다른 모델은 완전한 오픈 소스로 제공되어 로컬에서 다운로드하고 실행하거나 클라우드 공급업체에 호스팅할 수 있습니다.\n\n응용 프로그램 내에서 다른 데이터 경로에 대해 다른 임베딩 모델을 사용하는 것도 가능합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적으로 좋은 임베딩 모델은 RAG 응용 프로그램에 충분할 수 있지만, 일부는 특정 임베딩 모델이나 사용자 지정된 모델을 사용하여 혜택을 얻을 수 있습니다.\n\n임베딩 전략에 대한 설계 고려 사항과 해당 선택의 품질 특성을 알면 응용 프로그램의 평가 요구 사항과 접근 방식에 대한 통찰력을 제공할 것입니다. 임베딩 모델 선택을 평가하는 더 깊은 논의는 여기에서 확인할 수 있습니다.\n\n## #5—쿼리 처리 및 임베딩\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수신한 쿼리에 임베딩 모델을 정확히 실행해야 한다는 규칙은 없습니다. 실제로 이 쿼리를 최적화하여 애플리케이션의 전반적인 품질을 향상시킬 수 있는 다양한 방법이 있습니다. 이는 특히 쿼리가 사람 사용자로부터 직접 제출되었으며 모호하고 애매한 쿼리일 경우에 더욱 참된 것입니다.\n\n애플리케이션의 특성 또는 의도에 대한 추가 지식을 통해 LLM 또는 전통적인 논리를 사용하여 쿼리를 축소하거나 다시 작성하는 것이 가능할 수 있습니다. 다시 말해, 의도된 것이 아닌 실제로 묻는 것이었던 쿼리를 재작성하는 방식으로 쿼리를 재구성할 수 있습니다.\n\n쿼리 처리의 고급 형태인 HyDE도 있습니다. 여기서는 가상 문서를 작성하여 유사한 문서(답변에서 답변)를 벡터 검색하고 임베딩 및 쿼리 검색(질문에서 답변)을 하는 것대신 사용할 수 있습니다.\n\n또 다른 옵션은 쿼리를 여러 관련된 쿼리로 분할하고 각각을 병렬로 실행한 다음 결과를 결합하는 것입니다. 이는 처리 비용이 듬성들지만 검색 품질을 향상시킬 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특정 사용 사례에 따라 사용자 지정 쿼리 처리가 필요할 수 있으며 응용 프로그램의 품질과 동작에 큰 영향을 미칠 수 있습니다.\n\n## #4, #6—Vector DB 및 Vector Search\n\n![image](/assets/img/2024-06-22-RAGforQualityEngineers_7.png)\n\n벡터 검색은 빠르지만, 쿼리와 유사한 임베딩을 찾기 위해 벡터 DB를 검색하는 데에는 시간 (그리고 돈) 비용이 소요될 수 있습니다. 이 비용을 최소화하는 한 가지 방법은 의미 캐싱입니다. 의미 캐싱에서는 임베딩이 처음 검색된 후 응답이 캐시되어, 향후 유사한 검색이 캐시로부터 직접 데이터를 반환하게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n물론, 캐싱은 복잡성을 증가시킵니다 (그리고 컴퓨터 과학에서의 두 번째 어려운 문제 중 하나입니다—다른 하나의 이름을 기억하지 못하겠군요). 캐싱은 성능을 향상시킬 수 있지만, 오래된 캐시는 변동성 있는 소스 데이터 환경에서 특히 응답 품질을 해치는 요인이 될 수 있습니다.\n\n## #7—재랭킹\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_8.png)\n\n위에서 설명한 내용에서 우리는 단순히 우리의 벡터 검색으로 반환된 모든 관련 데이터를 컨텍스트 창에 채울 수 있다고 가정했습니다. 이것은 명백히 간소화된 내용이며, 반환된 모든 벡터 중 어느 것이 컨텍스트 창에 포함될 것인지를 결정하기 위한 과정이 있어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n검색 결과를 콘텍스트 창 안에 맞출 수 있을 때에도, 많은 연구에서 콘텍스트 Stuffing(콘텍스트 창 채우기)가 LLM 회상을 부정적으로 영향을 줄 수 있다는 것을 지적합니다. 이는 중간에서 사라지는 문제를 도입하여 응답 품질(회상은 LLM이 콘텍스트 창에 있는 정보를 사용하는 능력입니다)에 영향을 줄 수 있습니다.\n\n해결책은 초기 벡터 검색 후에 추가 단계로 재랭킹을 추가하는 것입니다.\n\n재랭킹의 TLDR(요약): 임베딩 모델은 속도에 최적화되어 있으며 많은 문서에 대해 실행되어야 하므로 빠릅니다. 재랭킹 모델(또는 교차 인코더)이라 불리는 다른 모델은 느리지만 정확도에 최적화되어 있습니다. 그래서 빠르고 부정확한 임베딩 모델을 사용하여 임베딩을 생성한 다음, 작은 집합에서 최고 품질의 문서를 찾기 위해 느리고 정확한 모델을 사용합니다. 느린 정확한 검색에서 가장 일치하는 결과는 콘텍스트 창에서 우선순위를 갖습니다.\n\n다시 말하지만, 이보다 더 많은 내용이 있지만, 재랭킹의 본질은 바로 이것입니다. Pinecone 블로그에서 더 자세한 설명을 찾을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다시 순위를 매기면 RAG에서 반환된 데이터의 관련성을 크게 향상시킬 수 있습니다. 컨텍스트 창에서 더 관련성이 높은(또는 무관련성이 적은) 데이터는 응답 품질을 향상시킬 것입니다. 그러나 복잡성과 지연이 증가하지만, 품질의 트레이드오프는 많은 RAG 응용 프로그램에서 가치 있는 요소일 수 있습니다.\n\n## 큰 컨텍스트 창 vs. RAG\n\n우리는 마침내 LLM을 호출하는 지점에 도달했지만, 프롬프트 엔지니어링에 대해 이야기하기 전에 RAG와 큰 컨텍스트 창 간의 관계에 대해 언급할 시간을 가져야 합니다.\n\nLLM 기술은 빠르게 발전하고 있으며 개선의 한 가지 측면은 컨텍스트 창의 크기입니다. 한 가지 대표적인 예는 2024년 2월에 출시된 Gemini 1.5 Pro이며, 128K 컨텍스트 창을 제공하며(공개적으로 출시되지 않음) 최대 백만(!!!) 토큰까지 확장할 수 있는 옵션이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일부 사람들은 100만 토큰 컨텍스트 창이 RAG 파이프라인을 사용할 때 사용되지 않을 것이라고 예상했습니다. 그러나 실제로는 그렇지 않습니다. 이 블로그에서는 RAG가 왜 유용하며 (심지어 필수적이기도 한) 거대한 컨텍스트 창을 사용할 때도 필요한 이유에 대해 설명합니다. (스포일러: 비용, 지연 시간 및 회수 품질)\n\n대규모 컨텍스트 모델은 유용하며, LLMs가 많은 사실들 간에 종합적인 결론을 요구하는 쿼리에 응답하는 데 도움이 될 수 있습니다 (이러한 사실들이 RAG를 통해 선별되어 있는지 여부는 상관 없음).\n\n큰 컨텍스트 창과 RAG 간의 관계는 계속 발전할 것이며, RAG를 구현하고 테스트하는 사람들은 응용 프로그램 품질에 미치는 이러한 트레이드오프와 그들의 영향을 이해해야 합니다.\n\n## #8—프롬프트 생성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![2024-06-22-RAGforQualityEngineers_9](/assets/img/2024-06-22-RAGforQualityEngineers_9.png)\n\n벡터DB에서 관련 데이터를 많이 받아 재설정하고, LLM(Large Language Model)의 문맥 창 안에 맞는 적절한 데이터 세트로 마무리했습니다. 그럼 이제 어떻게 해야 할까요? 받은 데이터를 초기 질문 뒤에 밀어 넣고 끝내면 될까요?\n\nLLM을 다뤄본 사람이라면 알 수 있듯이, 그것만큼 간단한 일이 아닙니다. LLM은 강력할 수 있지만, 변덕스럽고 짜증을 유발할 수도 있습니다. 당신의 프롬프트에 작은 세부 사항이 응담 품질에 상당한 영향을 미칠 수 있다는 것이 밝혀졌습니다. 프롬프트의 단어 선택, 데이터 순서, 사용하는 어조, \"시간을 들이다\"와 같은 제안, 심리적 언어 사용까지 모두 LLM 응답 품질에 영향을 미칠 수 있습니다. 프롬프트를 자동으로 생성하는 최적 프롬프트 생성 전략이 있습니다. ...맞아, 프롬프트 생성에 특별히 훈련된 다른 모델을 사용하는 것입니다. 이것은 신속히 진화하는 프롬프트 엔지니어링 분야의 일부입니다.\n\n최상의 품질의 응답을 생성할 정확한 프롬프트 템플릿은 보통 모델 및 응용 프로그램에 따라 다르며 종종 실험과 시행착오가 필요할 수 있습니다. RAG의 이 보이지 않는 작은 세부 사항이 가지는 품질 영향을 감안할 때, 적용된 특정 프롬프트 엔지니어링은 시스템의 다른 부분과 마찬가지로 면밀히 평가되고 심사되어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# RAG 시스템의 측정 및 평가\n\n우리는 RAG 파이프라인의 주요 구성 요소들을 살펴보고 (간략하게) 이들이 애플리케이션 품질에 미치는 영향에 대해 이야기해 보았습니다. 이것은 소개였지만, 이러한 유형의 애플리케이션의 내부 작업 및 품질 도전에 대한 통찰력을 제공해야 합니다. RAG에 대해 더 깊이 파고드는 많은 훌륭한 기사, 블로그, 논문이 있습니다. 시작할 때 하나만 선택한다면, \"Retrieval-Augmented Generation for Large Language Models—A Survey\"를 읽어보세요.\n\n주요 교훈: RAG를 구현할 때 선택할 수 있는 옵션과 선택지가 많으며, 각각에는 품질에 대한 대가와 영향이 있습니다. 이러한 선택들 중 일부는 직접적으로 평가될 수 있고, 일부는 전반적인 검색이나 응답 품질에 영향을 미칩니다. 이러한 선택 각각과 이들이 당신의 RAG 시스템에 어떤 영향을 미칠 수 있는지 이해하는 것이 전반적인 애플리케이션의 제품 품질을 달성하는 데 중요합니다.\n\n당연한 다음 질문은 다음과 같습니다: 좋아, 그런데 RAG를 어떻게 평가할까요? 개방형 자유형식 응답의 품질을 어떻게 측정할까요? 어떤 지표를 사용하여 실제로 측정할 수 있을까요? 이러한 평가를 자동화할 수 있을까요, 그리고 어느 수준에서 할 수 있을까요? LLM은 본질적으로 비결정론적이고 그들이 소비하는 데이터도 본질적으로 불안정할 때 품질을 어떻게 보장할까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이런 건들로 이루어진 큰 질문들이에요. 우리는 ARC나 HellaSwag 같은 프레임워크를 이용한 모델 평가, LLM-as-a-judge와 같은 접근 방식, 바늘 찾기 테스트와 같은 테스트, 어려움, 신뢰성, 그리고 관련성과 같은 측정 항목, Ragas와 LlamaIndex와 같은 도구 등의 주제를 다룰 거에요.\n\n하지만, 이 모든 재미로움은 다음 블로그를 기다려야 해요.\n\n본 글에 대한 기술적 피드백으로 Etienne Ohl와 Jack Bennetto에게 특별히 감사드려요.","ogImage":{"url":"/assets/img/2024-06-22-RAGforQualityEngineers_0.png"},"coverImage":"/assets/img/2024-06-22-RAGforQualityEngineers_0.png","tag":["Tech"],"readingTime":13},{"title":"데이터 메시에 대한 도전 과제 및 해결책  3부","description":"","date":"2024-06-22 17:11","slug":"2024-06-22-ChallengesandSolutionsinDataMeshPart3","content":"\n\n'연합된 계산 기반 거버넌스'는 안전하고 신뢰할 수 있으며 상호 운용 가능한 데이터 메시를 보장합니다. 상호 운용성으로부터 추가 가치는 종종 \"전체가 부분의 합보다 더 크다\"는 구절로 요약됩니다. HTTP와 같은 표준 프로토콜, 효율적인 데이터 전송 메커니즘, 그리고 구성 요소의 포괄적인 버전 관리와 같은 기술적 측면을 설정하는 것이 상호 운용성을 활성화하는 데 중요하다면, 여기서 우리의 초점은 전체적인 데이터 일관성과 호환성 유지에 있을 것입니다.\n\n상호 운용성을 참으로 지원하려면, 우리는 모든 데이터 제품에서 내용을 포괄하는 일관된 혹은 일관성 있는 모델을 보장해야 합니다. 이 모델은 새로운 데이터 제품이 추가됨에 따라 동적으로 업데이트되어야 합니다. 나의 삼부작 시리즈의 마지막 글에서는 본래 체계적이며 최신으로 유지되는 종합적인 관점을 유지하는 실용적인 방법을 보여드리도록 하겠습니다.\n\n거버넌스는 종종 발전을 억제할 수 있는 엄격한 규칙으로 여겨집니다. 즉시적인 혜택 없이 부가 작업을 추가하는 부담으로 여겨집니다. 거버넌스 프로세스를 자동화함으로써(즉 '계산 기반'으로 만들면) 데이터 메시 내에서 그 적용을 효율적으로 만들 수 있지만, 프로젝트나 제품 비용은 그대로 남아 있을지도 모릅니다.\n\n그렇다면, 우리는 어떻게 거버넌스를 변경하여 분산된 팀에 가치를 제공하고 동시에 참여를 촉진할 수 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 지배 규칙을 유용한 프레임워크로 변환하고 커뮤니티 기반의 아이디어와 시장 기반의 메커니즘을 활용함으로써 이를 달성할 수 있습니다. 이러한 전략은 참여와 가이드라인 준수를 효과적으로 장려할 수 있습니다.\n\n## 가이드로 연방화\n\n세계적인 인터넷과 그 개발에서 얻을 수 있는 교훈을 고려해봅시다. 웹은 모든 정보를 일관되고 구조적으로 조직화하기 위한 중앙 기관이나 카탈로그 없이 운영됩니다.\n\n예전에야후는 웹의 정보를 구조화하는 데 중앙에서 분류 체계를 작성하고 정리하는 것이 옳은 방식이라고 믿었습니다. 그러나 구글은 분산된 자동화 검색 기능으로 월드 와이드 웹의 정보를 보다 획기적으로 처리하여 더 큰 성공을 거두었습니다. 구글의 방식은 작업을 분배하고 자동화를 통해 아래에서 위로 데이터를 합치는 것을 지원했기 때문에 더욱 효과적이었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n웹의 비구조화된 다양한 정보는 대기업의 명확성 부족을 반영하기도 하지만, 우리는 여전히 조직 내에서 선제적으로 행동할 기회가 있습니다. 그러나 Google 검색 엔진과 같은 하위 검색 기능은 마술처럼 누락된 메타데이터를 생성하거나 일관성 없는 데이터를 일관성 있게 만들어 줄 수 없습니다. 따라서 데이터가 일관되게 정의되고 비즈니스 컨텍스트를 위한 메타데이터가 데이터 제품에 통합되도록 선제적으로 행동하고 보장하는 것이 중요합니다.\n\n비즈니스 도메인에서 정보 다양성을 유도하여 통합된 하나로 집계할 수 있도록 해야 합니다. 데이터 메쉬는 다중 비즈니스 도메인 경계를 가로지르는 폴리세미에 대해 말하지만 중앙에서 모델링해야 하는 점이 모호합니다. 이를 구체화하고 중앙 집중화와 분산화 사이의 적절한 균형을 이룰 수 있는 모델링이 어떻게 구현될 수 있는지 살펴봅시다.\n\n프로세스 월드(운영 시스템)에서는 두 가지 접근 방식이 나왔습니다. 이전에 생성된 정보의 고립된 영역(엔터프라이즈 응용 프로그램 통합 또는 EAI)을 다시 통합하거나 처음부터 이러한 영역이 만들어지는 것을 방지하는 것(도메인 주도 설계 또는 DDD)입니다. DDD는 \"Bounded Contexts\"의 전체 인터페이스를 모델링하기 위해 \"Context Maps\"를 참조합니다. EAI는 응용 프로그램 간 마찰없는 데이터 교환을 가능하게 하는 중심 요소인 \"Canonical Data Model\"을 참조합니다. 분석적인 세계에서 Ralph Kimball은 공유 비즈니스 차원을 가진 데이터 웨어하우스 버스 아키텍처를 소개하고, Dan Linstedt는 비즈니스를 세밀하게 대표하고 비즈니스 키를 사용하는 교차 도메인 허브의 필요성을 강조하는 데이터 보트 모델을 옹호했습니다. 모든 접근 방식은 정보 고립 영역의 급증을 방지하기 위해 비즈니스 개념의 핵심을 포착하는 기업 데이터 모델의 필요성을 강조합니다.\n\n이러한 포괄적 모델 개발의 불가능성에 대한 이야기가 많이 나오지만, 많은 프로젝트에서 실용적인 접근 방식이 성공을 거두었습니다. 중요한 것은 도메인별 데이터 모델을 통합할 온톨로지(프레임워크)를 수립하는 것입니다. DDD 용어로 말하면, \"Context Maps\"를 통해 Bounded Contexts의 명확하게 인식된 중첩 목록이 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다시 말해, 핵심 비즈니스 컨셉에 대한 지표를 제공하고 개요를 제공하는 고수준 비즈니스 모델입니다. 개별 도메인 모델의 상세한 디자인은 온톨로지의 범위를 벗어납니다. 온톨로지는 다수의 도메인 모델을 통합하고 비즈니스 도메인 전반에 걸쳐 유효한 공통점과 추상화를 설명합니다. 다의어를 정의하는 것뿐만 아니라 공통 비즈니스 용어, 객체 및 관계를 식별하는 데 중점을 둡니다.\n\n여러 테스트된 모델 패턴과 산업 모델이 솔리드한 기반이 됩니다. 이를 통해 바퀴를 재창조하는 것을 피할 수 있습니다. 그러나 최종 해결책으로서가 아닌 시작점으로 활용하세요. 공통 업계 모델이 아닌 고유한 비즈니스 요구사항을 대표하는 것이 중요합니다.\n\n때로는 기업 모델이 이미 존재하지만 프로젝트에서 활발하게 활용되지 않을 수 있습니다. 이러한 자료를 활용하여 특정 컨텍스트에 맞추도록 '활성화'를 시도하세요. 반면에, 존 지일즈는 \"냉장고 속 코끼리\"라는 책에서 \"어떻게 '충분한' 기업 데이터 모델을 직접 만드는가\"란 특정한 챕터에서 처음부터 새로운 온톨로지를 만드는 실용적인 접근 방법을 소개합니다.\n\n기존 모델을 활용하고 새로운 모델을 만드는 두 가지 접근법을 균형 있게 조화시키는 것은 많은 프로젝트에서 긍정적인 결과를 이끌어냈습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서 발견된 풍부한 조언들을 완전히 다 다루지는 못하지만, 두 가지 주요 아이디어를 강조하겠습니다:\n\n- 온톨로지에서 비즈니스 참여 활성화\n비즈니스 정보를 성공적으로 통합하기 위해서는, 여러 영역을 중심으로 조정할 수 있다는 오해를 피해야 합니다. 그러나 중앙 감독 없이는 조정될 수 없습니다. 그리고 중앙화된 IT 데이터 모델러 팀은 종종 각각의 비즈니스 도전에 대한 구체적인 지식이 부족할 수 있습니다. 따라서, 비즈니스 전문가들을 참여시키는 것이 중요합니다.\n연합 접근 방식을 유지하기 위해서, 모델링 팀은 IT 데이터 모델링 전문가들이 중재하는 모든 비즈니스 영역의 전문가들을 포함해야 합니다. 그 목표는 비즈니스에 대한 통합적이고 고수준의 이해를 달성하는 것입니다. 기억하세요, 데이터 모델링은 근본적으로 비즈니스 모델링이라는 것을.\n\n- 고수준 유지 및 세부 사항 연방화\nOntology는 고수준의 공통점에 관한 것입니다. 핵심 개체 및 관계를 작업하기 위해 추상 모델링 패턴을 사용하세요, 예를 들어 다음과 같은 것들.\n\n| 주체 \u0026 역할: 비즈니스 역할을 수행하는 개인 또는 조직 (예: 고객, 에이전트, 공급업체).\n| 위치: 지리적 위치, 건물 또는 지역.\n| 이벤트: 사고와 같은 중요한 이벤트 또는 '애플리케이션이 생성됨'과 같은 루틴 이벤트.\n| 문서: 물리적 또는 전자 문서, 계약서 또는 신분증의 스캔 이미지, 또는 모든 유형의 데이터 파일을 포함합니다.\n| 합의: 종종 형식적으로 문서화된 당사자 간의 계약.\n| 계정: 레코드의 일반적인 표현.\n| 작업: 계획된 또는 실제 작업 항목.\n| 자원 / 자산: 회사 건물, 컴퓨터 또는 차량과 같은 자산.\n| 제품: 고객에게 제공되는 상품 및 서비스.\n\n- 이러한 모델링 패턴은 다음과 같은 관계를 가지고 있습니다.\n제품 -` 주체가 활용하는 -` 합의를 체결한 -` 계정을 용이하게 하는 합의 -` 위치에 유지된 -` 자원의 장소이고, 그렇게하여야 된다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n희망을 통해 일반적인 개념을 제공했기를 바랍니다 — 참조 도서에서 더 자세하고 유용한 조언을 찾을 수 있을 거에요. 일반적인 패턴은 비즈니스 개념을 설명하는 상향식 프레임워크를 제공하여 다양한 영역간의 이해를 통합하는 데 도움이 됩니다. 자세한 비즈니스 도메인 모델은 상향식으로 개발된 상위 온톨로지에 명시적으로 연결되는 분산 도메인 팀에 의해 아래에서 위로 보완되어야 합니다. 이는 다양한 관점을 조화시키기 위한 강력한 토론 없이는 성공할 수 없는 창의적인 과정입니다.\n\n도메인 주도 설계는 Shared Kernel, Customer/Supplier Dev Teams, Conformist, Anticorruption Layer, Separate Ways, Open Host Service, 또는 Published Language과 같은 이 과정을 중재하는 실용적인 패턴을 제공합니다. 많은 용어와 설명이 주로 기능적 통합으로부터 비롯되었다 하더라도, 그 원칙들은 데이터 관리와 모델링에도 적용됩니다.\n\n전반적으로, 모든 데이터 생산자는 상세한 비즈니스 도메인 데이터 모델을 온톨로지에 매핑해야 합니다. 이 정보는 데이터 메시에 발행된 데이터 제품에 캡슐화된 메타데이터로 제공됩니다. 모든 가능한 데이터 제품의 메타데이터에서 파생된 데이터 메시는 언제든지 소비자를 위한 최신의 기업용 데이터 모델을 제공할 수 있습니다.\n\n# 참여를 활발하게 하는 동기부여\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 메시 내에서 효율적인 거버넌스는 참여자들이 데이터 기여를 온톨로지에 맞추도록 장려하며, 기업 전체의 이익과 개별 또는 부서별 목표를 균형 있게 조율합니다.\n\n기업 당사자들이 적극 참여하도록 어떻게 동기부여할 수 있을까요?\n\n## 커뮤니티 및 오픈 소스 원칙\n\n오픈 소스 운동에서 배울 점이 있습니다. 오픈 소스 운동의 철학에 부합하여, 데이터 모델의 특정 부분의 소유권이 아닌 일관된 전체적인 협력에 대한 작업이 중요합니다. 협업은 정보를 공유하는 것이며, 따라서 데이터 모델에 대한 모든 변경 사항은 시간이 지남에 따라 공개되고 문서화되어야 합니다. 거버넌스 프로세스는 이러한 공유를 가장 간단하고 보상적으로 만들어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오픈 소스 개발에서는 Git과 같은 코드 저장소를 활용하여 배포 및 업데이트 작업을 투명하게 처리하고 코드의 전체 내용을 분산하여 업데이트할 수 있습니다. 개발자들은 제품 상태에 대한 포괄적인 개요를 언제든 확인할 수 있습니다. 풀 리퀘스트를 사용하면 코드 변경에 대한 다양한 관점을 토론하고 중재할 수 있는 가치 있는 플랫폼을 구축할 수 있습니다.\n\n이와 유사하게, 데이터 메시를 활용하여 기업 데이터 모델의 분산 작업에 동일한 기능을 제공할 수 있습니다. 이 시리즈의 제2부에서는 데이터 자체에 메타데이터를 캡슐화하는 방법을 소개하여 전체 온톨로지에 맞게 자립적인 데이터 제품을 만들었습니다. 데이터 모델의 변경 사항은 이후 새로운 데이터 원자로 스트리밍되거나 변환으로 추가될 수 있습니다. 발행된 업데이트는 신속하게 기업 데이터 모델의 최신 상태를 생성하기 위해 소비될 수 있습니다. 데이터 모델에 대한 상충되는 변화나 다른 관점은 조정되고 토론될 수 있습니다. 온톨로지는 전반적 일관성을 보장하며 모든 변경 사항은 원본 데이터 모델로 귀결할 수 있는 투명하고 추적 가능한 형태로 남습니다. 이 접근법은 분산 데이터 모델링 팀 간의 협업을 촉진합니다.\n\n기업 내에서 데이터 커뮤니티를 형성하면 회사의 성공에 대한 공헌도 높일 수 있습니다. 데이터 메시는 가치 있는 데이터를 공유할 수 있도록 모두가 동참할 수 있는 협업 환경으로 작용합니다. 종종, 애플리케이션 소유자들은 생성하는 데이터가 기업의 다른 맥락에서 실질적인 추가 가치를 창출할 수 있다는 사실을 충분히 인식하지 못합니다. 이러한 데이터 제품은 온톨로지와 조화를 이룬 경우, 생산자가 자세한 응용 프로그램을 이해하지 않고도 다른 비즈니스 맥락에서 원활하게 활용할 수 있습니다. 개별 데이터 제품마다 이중 데이터 계약을 작성하는 대신 애플리케이션 소유자는 기업과 단일 가상 계약을 체결합니다. 이를 통해 제품 소유자의 데이터를 발행하고 다양한 비즈니스 영역에서 활용하여 전반적인 가치와 효율성을 향상시킬 수 있습니다.\n\n## 데이터 제품 시장\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알truistic 동기 외에도, 시장 기반 메커니즘은 기업 내 분산 모델링 노력에 참여를 동기부여할 수 있습니다. 이러한 메커니즘은 데이터 제품을 위한 고객을 찾는 데 도움을 주는 것뿐만 아니라 데이터의 명확한 정의와 구조화를 장려합니다.\n\n참여 동기:\n\n- 생산자와 고객에게 부가 가치 제공\n생산자는 기업 내에서 자신의 응용 프로그램의 중요성과 가시성을 높여 혜택을 얻습니다. 반면에 고객은 자신의 데이터 요구를 효율적으로 충족시켜주는 것으로, 생산자와 직접 소통할 수 있는 명확한 개요를 얻을 수 있습니다.\n- 참여를 간소화하고 민주화\n프로세스를 간소화하기 위해 자동화된, 셀프 서비스 데이터 패브릭을 구현합니다. 정확한 정보 매핑에 대한 긴 조정은 데이터 제품을 빠르게 출시하고 싶은 생산자들에게 장벽이 될 수 있습니다. 이 프로세스를 간소화함으로써 참여를 장려하여 지각된 장벽을 줄입니다.\n\n마켓플레이스에 제공되는 데이터 제품의 경우, 시장 메커니즘을 활용하여 데이터 제품의 가치를 결정할 수 있습니다. 본문에서 상세한 가치 평가 방법에 대해 다룰 수 없지만, 사용자 사용량을 추적하고 평가 및 리뷰와 같은 소비자 피드백을 가능하게 함으로써 간단하면서도 효과적인 접근 방법이 있습니다. 이러한 메커니즘들은 참여를 장려할 뿐만 아니라 데이터 제품의 품질을 평가하고 개선하기 위한 가치 있는 통찰을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기업 데이터 모델을 만드는 것이 어려운 일로 여겨졌지만, 이제는 고품질 데이터 제품을 위한 자체 지속 경쟁으로 변모했습니다.\n\n다음은 데이터 메쉬에서 '연방형 컴퓨터 관리'의 개선 내용을 요약한 것입니다:\n\n- 거버넌스 프로세스를 접근하기 쉽고, 마찰이 적도록 만듭니다.\n- 비즈니스 도메인 팀에 데이터 모델링 활동을 연방화하지만, 프레임워크로서의 안내적 온톨로지를 제공합니다.\n- 데이터 모델에 대한 협업 작업을 허용하고, 변경 사항을 모두 데이터 메쉬를 통해 투명하게 게시합니다.\n- 데이터 메쉬를 통해 데이터 시장을 제품으로서 활용할 수 있게 배치하여 투자 수익률(ROI)을 얻을 수 있도록 합니다.\n\n이로써 데이터 메쉬의 도전과 해결책에 관한 세 번의 시리즈가 마무리됩니다. 종합적으로, Zhamak Dehghani의 정의에 따라 데이터 메쉬를 개선하기 위해 다음과 같은 조언을 제공했습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n부분 1\n\n- 데이터 메시의 운영 및 분석 데이터 평면 사이의 큰 차이를 줄입니다.\n\n부분 2\n\n- '데이터 제품'을 스마트 데이터 구조로 구현하여 전체 데이터 계보와 비즈니스 컨텍스트를 포함하도록 하고, 자체 생성이 가능한 새로운 '슈퍼 객체'로 사용하지 않도록 합니다.\n- 데이터 메시의 하부에 데이터 제품 인프라를 제공하여 느슨하게 결합된 운영 및 분석 시스템 간에 데이터 제품을 원활하게 교환할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파트 3\n\n- '활성화'하면서 연방화된 기업 데이터 모델링을 위한 거버넌스 프로세스를 활성화하고, 쉽게 접근 가능한 서비스를 통해 개방형 협업 활성화.\n- 데이터 메쉬를 시장에서 데이터 제품으로 사용되는 하부 구조로 배치하여 참여자에게 투자 수익을 보장합니다.","ogImage":{"url":"/assets/img/2024-06-22-ChallengesandSolutionsinDataMeshPart3_0.png"},"coverImage":"/assets/img/2024-06-22-ChallengesandSolutionsinDataMeshPart3_0.png","tag":["Tech"],"readingTime":8},{"title":" Docker로 Apache Spark와 Minio를 사용하여 Apache Iceberg 활용하는 방법","description":"","date":"2024-06-22 17:09","slug":"2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker","content":"\n\n\u003cimg src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png\" /\u003e\n\n이 게시물에서는 Apache Iceberg(데이터 테이블 형식), 분산 처리 엔진인 Apache Spark, 고성능 객체 저장 솔루션인 Minio를 결합하는 방법에 대해 탐구합니다. 주요 초점은 이러한 구성 요소를 Docker 컨테이너 내에 설정하여 통제된 환경에서 격리된 환경을 제공하는 데 있습니다. 이러한 기술을 결합하여 ACID 트랜잭션, 스키마 진화 및 Minio 내에서 효율적인 데이터 파티셔닝을 통한 효율적인 데이터 관리 기능을 확보할 수 있습니다.\n\n# 아키텍처 개요\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*gM-qHwR03S6IEh32mgJpjA.gif\" /\u003e  \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 컴포넌트 개요 이론적 개요📖\n\n실제 구현에 들어가기 전에, 사용할 기술들인 Apache Iceberg, Apache Spark, 그리고 Minio에 대한 간단한 이론적 개요를 살펴보겠습니다:\n\n![이미지](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_1.png)\n\n- Apache Iceberg: 차세대 데이터 테이블 형식\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터 레이크 및 데이터 웨어하우스용으로 설계됨: Iceberg는 대규모 데이터의 복잡성을 처리하도록 특별히 구축되었습니다. 데이터 레이크와 데이터 웨어하우스 내에서 강력한 데이터 관리 기능을 제공하면서 분석 워크로드의 성능을 최적화합니다.\n- ACID 트랜잭션: Iceberg는 ACID(원자성, 일관성, 고립성, 지속성) 트랜잭션을 지원하여 동시 쓰기 시나리오에서도 데이터 무결성과 일관성을 보장합니다. 이는 동일한 데이터에 여러 애플리케이션이나 프로세스가 동시에 쓰기를 하는 경우에 데이터 유효성을 유지하고 데이터 손상을 방지하는 데 중요합니다.\n- 스키마 진화: Iceberg는 기존 데이터 형식과 달리 데이터 테이블의 스키마를 손실 없이 진화시킬 수 있습니다. 이를 통해 분석 요구 사항이나 데이터 소스가 시간이 지남에 따라 변경될 때 데이터 구조를 조정할 수 있습니다. 기존 데이터를 다시 작성하지 않고도 열을 추가, 제거 또는 수정할 수 있습니다.\n- 타임 트래블 쿼리: Iceberg는 타임 트래블 쿼리를 지원하여 언제든지 과거 데이터 스냅샷에 액세스할 수 있습니다. 이는 변경 사항 감사, 분석 파이프라인 디버깅 및 역사적 분석에 매우 유용합니다. Iceberg는 데이터 버전을 추적하여 필요한 경우 특정 버전의 테이블을 검색할 수 있습니다.\n- 효율적인 파티셔닝: Iceberg는 특정 열을 기준으로 데이터 테이블을 파티션하여 읽기 성능과 데이터 관리를 최적화합니다. 해당 파티션 값에 따라 데이터 파일을 지능적으로 저장하므로 Spark가 특정 쿼리에 대한 관련 데이터만 효율적으로 스캔할 수 있습니다. 이는 쿼리 속도를 크게 향상시킵니다.\n- 데이터 조직 및 압축: Iceberg는 자동으로 Parquet 또는 ORC와 같은 효율적인 파일 형식으로 데이터를 구성하여 효율적인 데이터 압축과 열 액세스를 용이하게 합니다. 또한 데이터 압축을 수행하여 저장 공간을 최소화하고 시간이 지남에 따라 읽기 성능을 향상시킵니다.\n\n2. Apache Spark: 통합 분석 엔진\n\n![이미지](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_2.png)\n\n- 대규모 데이터 처리: Apache Spark는 대규모 데이터셋을 클러스터로 효과적으로 처리하는 강력한 오픈 소스 분산 처리 엔진입니다. 관계형 데이터베이스, NoSQL 데이터베이스, CSV 파일 및 Minio와 같은 객체 저장소를 포함한 다양한 데이터 원본을 지원합니다.\n- 인메모리 처리: Spark는 인메모리 계산을 활용하여 성능을 향상시키며, 자주 액세스되는 데이터를 디스크 기반 솔루션에 비해 더 빠른 처리를 위해 메모리에 유지합니다.\n- 구조적, 반구조적 및 비구조적 데이터: Spark는 CSV, JSON과 같은 구조적 데이터, XML과 같은 반구조적 데이터, 텍스트와 같은 비구조적 데이터를 포함한 다양한 데이터 형식을 처리할 수 있습니다. 이러한 유연성으로 인해 현대 데이터 생태계에서 다양한 데이터 유형을 다루는 데 이상적입니다.\n- 머신러닝 및 스트림 처리: 전통적인 분석 이상으로 Spark는 머신러닝 파이프라인 및 실시간 데이터 처리까지 확장됩니다. Spark는 TensorFlow, PyTorch와 같은 인기있는 머신러닝 라이브러리를 효율적인 모델 훈련 및 배포를 위해 통합합니다. 또한 Apache Flink와 같은 스트림 처리 프레임워크를 지원하여 거의 실시간 데이터 분석을 수행할 수 있습니다.\n- Iceberg와의 원활한 통합: Spark는 네이티브 Iceberg 지원을 제공하여 Spark 애플리케이션에서 Iceberg 테이블을 직접 읽고 쓰기 및 쿼리할 수 있습니다. 이를 통해 Spark 워크플로우 내에서 데이터 관리를 간편하게 할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. Minio: 고성능 객체 저장 서버\n\n![Minio Image](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_3.png)\n\n- 오픈소스 및 비용 효율적: Minio는 무료 및 오픈소스 객체 저장 솔루션으로, 대형 클라우드 공급업체가 제공하는 프로프리어터리 객체 저장 서비스에 대안으로 경제적입니다. 자체 데이터 저장 인프라를 관리하고 데이터 레이크 또는 데이터 웨어하우스 내에서 해당 기능을 활용할 수 있습니다.\n- 확장성 및 성능: Minio는 확장성을 고려하여 제작되었습니다. 데이터 저장 필요가 증가함에 따라 더 많은 노드를 Minio 클러스터에 쉽게 추가할 수 있어 증가하는 데이터 양을 처리할 수 있습니다. 또한 효율적인 데이터 액세스를 제공하여 Spark 애플리케이션에 대한 효율적인 데이터 액세스를 제공합니다.\n- S3 호환성: Minio는 Amazon S3와 API 호환성이 있어서 S3와 작동하는 기존 도구 및 애플리케이션과의 원활한 통합이 가능합니다. 이는 전통적인 클라우드 객체 저장에서 더 경제적인 온프레미스 솔루션으로의 원활한 전환을 용이하게 합니다.\n- 내구성 및 신뢰성: Minio는 데이터 중복 및 복제 메커니즘을 제공하여 데이터가 장애에 대비하여 보호되도록 합니다. 다중 노드 또는 저장 장치 간의 데이터 복제를 구성하여 하드웨어 문제 발생 시 데이터 손실 위험을 최소화할 수 있습니다.\n\n4. Docker 통합\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_4.png\" /\u003e\n\n도커는 구성 요소의 실행을 격리하고 관리하는 데 사용할 수있는 컨테이너화 플랫폼을 제공합니다. 이 아키텍처에서 도커가 어떻게 적합한지 살펴보겠습니다:\n\n- **도커 이미지**: 각 서비스(Spark Master, Spark Worker 및 Minio)를 위한 도커 이미지를 만들 수 있습니다. 필요한 모든 종속성(Spark, Minio 이진 파일, Iceberg 라이브러리 등)을 포함하여 일관된 환경을 제공하고 다양한 기기에 배포를 단순화합니다.\n- **도커 콤포즈**: 도커 콤포즈와 같은 도구를 사용하여 모든 서비스의 구성 및 배포를 함께 관리할 수 있습니다. 이 도구는 서비스, 종속성 및 환경 변수를 단일 YAML 파일에 정의하여 전체 환경 설정 프로세스를 간편화합니다.\n\n도커를 활용하면 이동성이 뛰어나고 격리된 개발 또는 프로덕션 환경을 구축할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 도커 컴포즈 구성\n\n아래는 Minio, Spark 마스터 및 Spark 워커 서비스를 설정하는 Docker Compose 파일입니다.\n\n```js\nversion: '3.9'\nservices:\n  minio:\n    image: minio/minio\n    container_name: minio\n    environment:\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    command: server /data --console-address \":9001\"\n    volumes:\n      - minio_data:/data\n\nspark-master:\n    image: bitnami/spark:latest\n    container_name: spark-master-minio-iceberg\n    environment:\n      - SPARK_MODE=master\n      - SPARK_SUBMIT_ARGS=--packages org.apache.iceberg:iceberg-spark3-runtime:0.12.0\n    ports:\n      - \"7077:7077\"\n      - \"8080:8080\"\n  spark-worker:\n    image: bitnami/spark:latest\n    container_name: spark-worker-minio-iceberg\n    environment:\n      - SPARK_MODE=worker\n      - SPARK_MASTER_URL=spark://spark-master:7077\n      - SPARK_SUBMIT_ARGS=--packages org.apache.iceberg:iceberg-spark3-runtime:0.12.0\n    depends_on:\n      - spark-master\n    ports:\n      - \"8081:8081\"\nvolumes:\n  minio_data:\n```\n\n# Docker Compose 파일 설명\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Minio 서비스: Minio 서버를 실행하여 포트 9000(API)과 9001(콘솔)을 노출합니다. Minio 데이터는 minio_data라는 이름의 Docker 볼륨에 저장됩니다.\n- Spark Master 서비스: Delta Lake 및 Iceberg를 위한 패키지가 포함된 Spark 마스터 노드를 실행합니다. Spark UI 및 마스터 통신을 위한 포트 7077 및 8080이 노출됩니다.\n- Spark Worker 서비스: Spark 워커 노드를 실행하고 Spark 마스터에 연결됩니다. Delta Lake 및 Iceberg를 위한 패키지가 포함되어 있습니다.\n\n# 이미지 빌드\n\n다음 명령을 실행하여\n\n```js\ndocker-compose up -d\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 서비스가 정상적으로 작동 중이거나 문제가 발생한 경우 알림이 표시됩니다 (문제가 발생하지 않기를 희망합니다 😄)\n\n![Image 5](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_5.png)\n\n![Image 6](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_6.png)\n\n먼저 http://localhost:9001/browser를 방문하여 아래 이미지를 확인해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_7.png\" /\u003e\n\n모든 서비스가 실행되면 Apache Iceberg를 사용하여 Apache Spark 및 Minio로 읽기와 쓰기를 하는 간단한 데이터 파이프라인을 생성해 봅시다.\n\n# Spark 작업 설정 및 실행하기\n\n다음의 Python 코드는 Minio와 상호 작용하고 Iceberg를 사용하여 Spark를 사용하는 방법을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Minio 클라이언트 초기화\n\n제공된 자격 증명으로 Minio 서버에 연결합니다.\n\n```js\nfrom minio import Minio\nclient = Minio(\n    \"127.0.0.1:9000\",\n    access_key=\"your_admin_name_account\",\n    secret_key=\"your_password\",\n    secure=False\n)\n```\n\n## 설명\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Minio 초기화: 이 코드는 Minio 클라이언트를 초기화하여 127.0.0.1의 9000 포트에서 실행 중인 Minio 서버에 연결합니다. 액세스 키와 시크릿 키로 minioadmin을 사용하고, SSL을 사용하지 않는 연결이므로 secure를 False로 설정합니다.\n\n# 버킷 관리\n\n버킷이 존재하는지 확인하고, 존재하지 않으면 생성합니다.\n\n```js\nminio_bucket = \"my-first-bucket\"\nfound = client.bucket_exists(minio_bucket)\nif not found:\n    client.make_bucket(minio_bucket)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 설명\n\n- Bucket 존재 여부 확인: bucket_exists 메서드는 Minio에 my-first-bucket이라는 이름의 버킷이 이미 있는지 확인합니다.\n- Bucket 생성: 버킷이 존재하지 않는 경우 make_bucket 메서드를 사용하여 my-first-bucket이라는 새 버킷을 만듭니다.\n\n# 파일 업로드\n\nMinio에 지정된 버킷에 CSV 파일을 업로드합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n목적지_파일 = 'data.csv'\n원본_파일 = './data/data.csv'  # 프로젝트 폴더에이 파일이 존재하는지 확인하세요\nclient.fput_object(minio_bucket, 목적지_파일, 원본_파일)\r\n```\n\n## 설명\n\n- 파일 업로드: fput_object 메서드는 로컬 파일 ./data/data.csv를 Minio 버킷 my-first-bucket으로 객체 이름이 data.csv인 파일로 업로드합니다.\n\n## 코드 실행 후🎦\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*rZUjqaNCls7_73eojd2RQw.gif)\n\n# SparkSession 구성\n\nSpark를 Iceberg 및 Minio를 스토리지 백엔드로 사용하도록 구성합니다.\n\n```js\nfrom pyspark.sql import SparkSession\niceberg_builder = SparkSession.builder \\\n    .appName(\"iceberg-spark-minio-example\") \\\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.iceberg:iceberg-hive-runtime:1.5.0\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n    .config(\"spark.sql.catalog.spark_catalog.warehouse\", f\"s3a://{minio_bucket}/iceberg_data/\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"your_admin_name_account\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"your_pasword\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .enableHiveSupport()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 설명\n\n- SparkSession 빌더: Iceberg 및 Minio를 사용하기 위해 구성된 Spark 세션 빌더를 생성합니다.\n- 애플리케이션 이름: \"iceberg-spark-minio-example\"로 애플리케이션 이름을 설정합니다.\n- JAR 패키지: Hadoop AWS 및 Iceberg에 필요한 JAR를 포함합니다.\n- Spark 확장: Spark SQL을 위해 Iceberg 확장 기능을 활성화합니다.\n- 카탈로그 구성: 카탈로그 유형을 Hadoop으로 구성하고, 웨어하우스 위치를 Minio 버킷을 가리키는 S3 경로로 설정합니다.\n- Minio 자격 증명: Minio의 액세스 및 시크릿 키를 설정합니다.\n- 엔드포인트 구성: S3 엔드포인트를 로컬 Minio 서버를 가리키도록 구성하고, S3 URI에 대한 경로 스타일 액세스를 활성화합니다.\n\n# Iceberg용 SparkSession 빌드\n\nIceberg 테이블과 상호 작용하기 위한 Spark 세션을 빌드합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\niceberg_spark = iceberg_builder.getOrCreate()\n```\n\n## 설명\n\n- SparkSession 생성: getOrCreate 메서드는 지정된 구성으로 Spark 세션을 초기화합니다.\n\n# 데이터 로드\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스파크 데이터프레임으로 CSV 파일을 읽습니다.\n\n```js\ndf = iceberg_spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(source_file)\n```\n\n## 설명\n\n- CSV 파일 읽기: ./data/data.csv 파일을 스파크 데이터프레임으로 로드합니다. header 옵션은 첫 번째 행을 열 이름으로 사용하도록 설정되었고, inferSchema 옵션은 데이터 유형을 자동으로 추정하도록 설정되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 아이스버그 테이블 위치 정의\n\n미니오 내 아이스버그 테이블의 위치를 지정합니다.\n\n```js\niceberg_table_location = f\"s3a://{minio_bucket}/iceberg_data/default\"\n```\n\n## 설명\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Iceberg Table 위치: Minio에 Iceberg 테이블 데이터를 저장하는 데 사용되는 S3 경로를 정의합니다. 경로는 버킷 이름과 하위 디렉터리 iceberg_data/default을 사용하여 구성됩니다.\n\n# Iceberg 테이블에 쓰기\n\nDataFrame 데이터를 Minio의 Iceberg 테이블에 씁니다.\n\n```js\ndf.write \\\n    .format(\"iceberg\") \\\n    .mode(\"append\") \\\n    .saveAsTable(\"iceberg_table_name\")  # Iceberg 테이블의 이름\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 설명\n\n- DataFrame 작성: Iceberg 형식을 사용하여 append 모드로 iceberg_table_name이라는 Iceberg 테이블에 DataFrame을 작성합니다.\n\n## 코드 실행 후🎦\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*oUTGnwgU4yk2nG31-kuIWA.gif\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Iceberg 테이블에서 읽기\n\nIceberg 테이블에서 데이터를 읽어 스키마와 몇 가지 샘플 레코드를 표시합니다.\n\n```js\niceberg_df = iceberg_spark.read.format(\"iceberg\").load(f\"{iceberg_table_location}/iceberg_table_name\")\niceberg_df.printSchema()\niceberg_df.show()\n```\n\n## 설명\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 아이스버그 테이블 읽기: 아이스버그 테이블에서 데이터를 읽어와 DataFrame에로드합니다.\n- 스키마 출력: DataFrame의 스키마를 표시합니다.\n- 데이터 표시: DataFrame에서 몇 가지 샘플 레코드를 표시합니다.\n\n## 코드 실행 후🎦\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*qnK1mFZ6ecqTXBGCtEWi0w.gif\" /\u003e\n\n# 전체 코드🖥️\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom minio import Minio\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\n# Minio 클라이언트 및 버킷 생성\nclient = Minio(\n    \"127.0.0.1:9000\",\n    access_key=\"minioadmin\",\n    secret_key=\"minioadmin\",\n    secure=False\n)\n\nminio_bucket = \"my-first-bucket\"\n\nfound = client.bucket_exists(minio_bucket)\nif not found:\n    client.make_bucket(minio_bucket)\n\ndestination_file = 'data.csv'\nsource_file = './data/data.csv' ## 프로젝트 폴더 내에 파일이 있어야 함\n\n# Minio에 파일 업로드\nclient.fput_object(minio_bucket, destination_file, source_file,)\n\n# Iceberg와 Minio 설정을 사용한 SparkSession 빌더 생성\niceberg_builder = SparkSession.builder \\\n    .appName(\"iceberg-concurrent-write-isolation-test\") \\\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.iceberg:iceberg-hive-runtime:1.5.0\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n    .config(\"spark.sql.catalog.spark_catalog.warehouse\", f\"s3a://{minio_bucket}/iceberg_data/\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .enableHiveSupport()\n\n# Iceberg를 위한 SparkSession 생성\niceberg_spark = iceberg_builder.getOrCreate()\n\n# Iceberg 테이블을 Minio에 쓰기\ndf.write \\\n    .format(\"iceberg\") \\\n    .mode(\"append\") \\\n    .saveAsTable(\"iceberg_table_name\")  # Iceberg 테이블 이름\n\n# Iceberg 테이블에서 데이터 읽기\niceberg_df = iceberg_spark.read.format(\"iceberg\").load(f\"{iceberg_table_location}/iceberg_table_name\")\n\n# 데이터프레임 스키마 및 데이터 출력\nprint(\"**************************\")\nprint(\"This the Dataframe schema \")\nprint(\"**************************\")\niceberg_df.printSchema()\n\nprint(\"**************************\")\nprint(\"******Dataframe Data******\")\nprint(\"**************************\")\niceberg_df.show()\n```\n\n# GitHub\n\n프로젝트 링크\n\n# Summary\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 설정을 사용하면 Spark 및 Iceberg로 대규모 데이터를 효율적으로 관리하고 처리할 수 있고, 확장 가능한 객체 저장소로 Minio를 사용할 수 있습니다. Docker를 활용하면 이러한 구성 요소의 배포와 관리가 간편하고 재현 가능해집니다.\n\n즐거운 학습이 되길 바래요 😉","ogImage":{"url":"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png"},"coverImage":"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png","tag":["Tech"],"readingTime":14},{"title":"모든 문제를 해결하는 하나의 계층  시맨틱 레이어란 무엇인가","description":"","date":"2024-06-22 17:05","slug":"2024-06-22-SemanticLayerOneLayertoServeThemAll","content":"\n\n## 기술과 비즈니스 간의 간극을 메우다\n\n![이미지](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_0.png)\n\n- 시멘틱 레이어는 복잡한 데이터 구조와 비즈니스 용어 사이의 다리 역할을 하며, 데이터의 통합된 보기를 제공하여 접근을 간소화하고 조직적 의사결정의 일관성을 보장합니다.\n- 시멘틱 레이어를 활용하면 데이터 거버넌스를 개선하고 AI 통합을 용이하게 하여 신뢰할 수 있고 투명한 분석 및 정보에 기초한 결정이 가능해지며, 조직 프로세스를 효율성과 적응성을 높일 수 있습니다.\n\n# 목차\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 소개\n\n시맨틱 레이어는 현대 데이터 관리에서 점점 더 중요한 요소가 되고 있습니다. 이러한 레이어의 부재는 제한된 데이터 가용성, 보고서 상의 불일치 및 잘못된 의사 결정으로 이어지며 IT 자원에 부담을 가중시킵니다. 데이터 민주화에 대한 노력은 다양한 비즈니스 인텔리전스 도구와 데이터 소스의 증가로 어려움이 더해져 종종 일관되지 않은 분석 결과와 거버네스 문제를 야기합니다.\n\n시맨틱 레이어를 도입함으로써 이러한 문제를 해결할 수 있습니다. 비즈니스 데이터의 일관된 표현으로 작용하면서 복잡한 데이터 구조를 익숙한 비즈니스 용어로 변환함으로써 조직 전체에서 데이터의 통합된 관점이 만들어지며, 접근이 간소화되고 일관성이 보장됩니다. 전문가들은 데이터 엔지니어링과 비즈니스 분석 사이의 간극을 좁히는 중요성을 강조하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사는 시맨틱 레이어의 정의와 기능을 살펴보고 기업 데이터를 조직화하고 추상화하여 의사 결정 프로세스를 용이하게 하는 중추적 역할을 설명합니다. 시맨틱 레이어를 구현하는 다양한 이점을 탐구하며 데이터 일관성, 거버넌스 및 민첩성이 향상되는 등의 이점을 논의합니다. 현대 기업 데이터와 AI 관리의 복잡성을 다루면서, 시맨틱 레이어는 운영 효율성 향상과 더 많은 정보에 기반한 의사 결정 능력을 약속하는 중심 요소로 등장합니다.\n\n# 시맨틱 레이어가 필요한 이유는?\n\nMicah Horner (TimeXtender)는 간결히 설명합니다. 그는 시맨틱 레이어가 없으면 조직이 데이터를 효과적으로 활용하는 데 제한이 발생한다고 주장합니다:\n\n- 제한된 데이터 접근 및 사용: 데이터 격리와 복잡성은 비기술적 사용자가 데이터에 액세스하기 어렵게 만들며, 시맨틱 레이어 없이는 정보 기반 결정과 데이터 기반 이니셔티브가 저해됩니다.\n- 통일된 데이터 언어 부재: 부서 간 다른 용어들은 혼란과 오해를 야기하며, 시맨틱 레벨이 없으면 비즈니스 목표를 조정하기 어렵게 합니다.\n- 보고 및 분석의 일관성 부재: 일관되지 않은 데이터 정의와 계산으로 인해 모순된 결과와 신뢰성 없는 결정이 발생하며, 비용이 발생합니다.\n- IT 부담 증가: 시맨틱 레이어가 없으면 IT 팀은 데이터 액세스 요청 및 문제에 너무 많은 시간을 할애하며, 전략적 이니셔티브로부터 자원을 분산시킵니다.\n- 제한된 민첩성과 확장성: 수동 데이터 통합 프로세스로 인해 비즈니스 변화에 대응하거나 영업을 확장하는 능력이 제한되어 시장 변화에 대한 대응 능력이 제한됩니다.\n- 데이터 거버넌스 및 규정 준수 위험: 일관되지 않은 데이터 관리는 기업 거버넌스와 규정 준수에 위험을 초래하며, 법적 및 재정적 결과가 발생할 수 있습니다.\n- 소실된 통찰과 경쟁 우위: 접근할 수 없는 데이터와 공유된 이해 부재로 인해 시장에서의 놓친 통찰과 경쟁 우위가 감소합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nKieran O’Driscoll(AtScale), Kyle Hale and Soham Bhatt (Databricks)가 블로그 글에서 대부분의 기업이 여전히 데이터 민주화에 어려움을 겪고 있다고 설명합니다.\n\n의사결정자에게 데이터를 제공하는 것은 특히 대규모 조직에서 어려운 과제입니다. 기업의 절반 이상이 세 개 이상의 비즈니스 인텔리전스 도구를 사용하며, 데이터 과학자와 애플리케이션 개발자는 각자 선호하는 도구를 가지고 있습니다.\n\n다양한 도구와 쿼리 언어는 상반되는 분석 결과를 내놓습니다. 서로 다른 데이터 복사본이나 Tableau Hyper Extracts, Power BI Premium Imports, 또는 SSAS와 같은 OLAP 솔루션을 사용하는 여러 사업 부서들은 이 문제를 더욱 악화시킵니다.\n\n다양한 마트, 데이터 웨어하우스 및 보고 도구에 데이터를 저장하면 단일 진실의 버전을 유지하는 것이 어렵습니다. 이로 인해 데이터 이동, ETL, 보안 및 복잡성이 증가하며 데이터 거버넌스 문제가 발생하고 잠재적으로 오래된 데이터에 의존하게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAtScale은 기업이 의사소통의 간극을 좁히기 위해 시멘틱 레이어가 필요한 이유를 매우 명확하게 설명하는 예를 사용합니다.\n\nDonald Farmer(AtScale)은 데이터 엔지니어와 비즈니스 분석가 간의 심각한 도전 과제를 강조하고 시멘틱 레이어가 이 간극을 메우는 데 도움이 되는 이유에 대해 설명합니다.\n\n많은 조직은 데이터 엔지니어(코드 기반 환경을 선호하는)와 비즈니스 분석가(비코드 인터페이스를 선호하는) 간의 연결 부재로 데이터 기반 의사결정에 어려움을 겪고 있습니다. 이러한 불일치는 비효율성, 일관되지 않은 데이터 정의 및 부정확한 의사결정을 초래합니다. 견고한 시멘틱 레이어는 엔지니어를 위한 API와 분석가를 위한 직관적 인터페이스를 제공함으로써 이러한 간극을 메울 수 있습니다. 이는 조직의 기술과 기술을 최적화합니다.\n\n시멘틱 레이어는 다양한 작업 스타일을 수용하는 통합 플랫폼을 제공하며 협업, 데이터 거버넌스 및 혁신을 강화합니다. 엔지니어가 데이터에 프로그래밍적으로 액세스하고 조작하는 데 사용할 수 있는 견고한 API를 제공하여 워크플로에 원활하게 통합됩니다. 동시에, 기술 전문지식이 많이 필요하지 않는 분석가를 위한 직관적 인터페이스를 제공합니다. 이 통합된 기반은 일관된 정의와 지표를 보장하며 더 나은 의사소통과 조정을 촉진합니다. 공유 작업 영역, 버전 관리 및 주석 기능과 같은 기능은 협업과 지식 공유를 촉진하며 조직이 효과적으로 데이터를 활용하고 비즈니스 가치를 실현할 수 있도록 돕습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 시맨틱 레이어란 무엇인가요\n\n위키피디아에서는 시맨틱 레이어를 다음과 같이 정의합니다:\n\nAtScale에서는 시맨틱 레이어를 다음과 같이 정의합니다:\n\n그리고 추가 설명...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_1.png)\n\nLulit Tesfaye (Enterprise Knowledge) defines the Semantic Layer as follows:\n\nand further …\n\nSummarizing these definitions, the semantic layer can be defined as follows:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시맨틱 레이어는 복잡한 기업 데이터를 친숙한 비즈니스 용어로 번역하고 서로 다른 데이터 원본을 매핑하고 데이터 관계를 관리하여 통합된 보기를 제공합니다. 사용자를 위해 데이터 모델을 간소화하며 구조화된, 비구조화된 및 반구조화된 데이터를 포함합니다. 메타데이터 레이어 역할을 하여 콘텐츠의 관리와 분석을 향상시킵니다. 데이터베이스와 최종 사용자 사이에 추상화 레이어 역할을 하며 일관된 데이터 뷰를 제공하고 SQL 지식 없이 직관적인 쿼리를 지원합니다. 또한 접근 제어, 데이터 품질 보증 및 정책 강제를 통해 데이터 관리를 지원합니다.\n\n# 시맨틱 레이어가 비즈니스 요구를 어떻게 지원하는가\n\n시맨틱 레이어는 데이터의 통합된 보기를 제공하여 일관된 액세스와 쿼리를 가능하게 합니다. 사용자 경험을 향상시키고 조직의 효율성을 높이며 기업 전반에 걸친 분석에 표준화된 접근 방법을 제공하여 다수의 혜택을 얻을 수 있습니다:\n\n- 진실의 단일 원천: 데이터는 원본에 관계 없이 표준 형식으로 제공되어 사용자가 다양한 도구와 기술로 분석할 수 있습니다. 시맨틱 레이어를 사용하는 기업은 단일 데이터원을 제한되지 않고 부서 간 분석을 수행할 수 있습니다.\n- 간소화된 데이터 액세스: 시맨틱 레이어는 복잡한 데이터 구조의 간소화된 통합 보기를 제공하며 사용자가 심도 있는 기술 지식 없이 데이터에 액세스하고 이해하는 것을 용이하게 합니다. 이로써 기업에서 데이터 액세스를 민주화할 수 있습니다.\n- 분석과 AI의 민주화: 데이터 분석이 확대됨에 따라 모든 요구 사항에 대해 단일 BI 또는 ML 플랫폼에 의존하는 것은 비현실적입니다. 시맨틱 레이어 플랫폼은 다양한 데이터 플랫폼, 프로토콜 및 도구를 연결하여 데이터와 사용을 분리하고 분석 및 ML의 민주화를 가능하게 합니다. [계속]\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 시맨틱 레이어 구현\n\n기업은 다양한 소스에서 데이터를 추상화하고 그 문맥을 이해하며 실행 가능한 통찰을 추출하는 도구가 필요합니다. 이를 통해 모든 사용자에 대한 데이터 문해력을 제고할 수 있습니다. 현대적인 \"범용\" 시맨틱 레이어 플랫폼은 시맨틱 레이어의 원래 강점을 강화하여 거버넌스 중앙화를 실현하고 비즈니스 지향적 데이터 관점을 촉진합니다.\n\n## 시맨틱 레이어의 구성 요소\n\nMicah Horner (TimeXtender)는 시맨틱 레이어의 구성 요소를 설명하며 복잡한 데이터 구조를 직관적인 사용자 경험으로 손쉽게 전환하는 방법을 소개합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1. 데이터 수집\n\n- 여러 출처에서 데이터 수집: 데이터베이스, 스프레드시트, API 등에서 데이터를 수집하여 중요한 정보를 중앙 집중화하여 후속 처리 및 언어 계층 생성을 시작합니다.\n\n2. 데이터 준비\n\n- 데이터 변환 및 준비: 수집된 데이터를 정리, 유효성 검사하고 변환하여 정확성과 분석에 사용하기 쉬운 신뢰할 수 있는 데이터 세트를 만듭니다.\n- 데이터의 차원 모델링: 데이터를 차원과 사실로 구조화하여 복잡한 관계를 단순화하며 의미 있는 통찰력을 제공하는 언어 계층 구축에 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 데이터 전달\n\n- 의미론적 레이어: 기술적 데이터를 비즈니스 친화적 용어로 번역하여 데이터를 이해하기 쉽고 모든 사용자에게 관련성 있게 만드는 의미 모델을 생성합니다.\n- 데이터 제품: 부서별 모델(데이터 제품)을 개발하여 맞춤형 데이터 액세스를 제공하여 각 팀이 필요로 하는 데이터를 과다하게 받지 않고 얻을 수 있도록 합니다.\n\nDavid P. Mariani(AtScale)은 A16Z 데이터 스택의 메트릭 레이어에서 변환 서비스를 활용한 범용 의미론적 레이어의 구현을 주장합니다. 데이터 모델링, 워크플로우 관리 및 권한 및 보안 등의 기능을 수행합니다. 이 레이어는 적절하게 조정되었을 때 다음과 같은 중요한 이점을 제공합니다:\n\n- 어떤 분석 도구에서도 접근할 수 있는 기업 메트릭과 계층적 차원을 위한 단일 진실의 소스 생성\n- 수월하게 업데이트하거나 새로운 메트릭을 정의하고 데이터의 도메인별 뷰를 설계하며 새로운 원시 데이터 자산을 통합하는 민첩성 제공\n- 분석 성능을 최적화하고 클라우드 자원 소비를 모니터링 및 최적화\n- 액세스 제어, 정의, 성능 및 자원 소비와 관련된 거버넌스 정책 강화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_2.png)\n\n성공은 사용자가 의미적 확장 없이 자유롭게 혁신할 수 있는 중앙에서 관리되는 의미론적 레이어를 활용하는 데 달려 있어요.\n\n메트릭 레이어: 메트릭 레이어는 기업 메트릭의 진실된 단일 원천으로서, 다양한 분석 도구에서 접근할 수 있습니다. 이는 BI 도구, 응용 프로그램, 역방향 ETL 및 데이터 과학 도구를 위한 메트릭 저장소를 제공하며, 설계 및 변경 관리가 그 기능의 일부입니다. 효과적인 메트릭 레이어는 일관성, 효율성 및 사용자 경험이 원활하도록 하기 위한 쿠레이션, 변경 관리, 탐색 및 서비스 기능이 필요합니다.\n\n데이터 모델링: 데이터 모델링은 데이터 레이크하우스나 창고의 논리적 데이터 개념을 물리적 구조에 매핑하는 과정을 말합니다. 비주얼 프레임워크나 코드 기반 언어를 사용할 수 있습니다. 중요한 활동에는 데이터를 \"분석용으로 준비함\", 일관된 차원 정의, 메트릭 설계가 포함됩니다. 이는 비즈니스 의미론을 데이터 모델에 포함시키며, 복합적인 분석 접근을 통해 일관성, 거버넌스 및 혁신을 촉진합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n워크플로우 관리: 워크플로우 관리는 의미론적 레이어를 위한 물리적 변환을 조율하여 비용과 성능을 최적화합니다. 사용자는 데이터 집계물 생성이 필요한 최소한의 쿼리 대기 시간을 요구하는데, 이는 클라우드 규모의 데이터로 인한 것입니다. 성능 관리는 자동으로 생성물을 처리하며, 클라우드 및 노동 비용을 고려하면서 동적으로 적응합니다. 의미론적 레이어 데이터를 활용하여 워크플로우 관리는 성능과 비용을 최적화합니다.\n\n자격 및 보안: 의미론적 레이어의 자격 및 보안은 쿼리 시에 데이터 거버넌스 정책을 동적으로 시행하여 사용자가 올바른 데이터에 액세스하는 것을 보장합니다. 여러 자격과 일관된 정의를 관리함으로써 신뢰와 무결성을 유지합니다. 성능 최적화는 사용자의 자격 및 유즈 케이스 우선순위를 고려합니다. 실시간 정책 시행이 중요한데, 의미론적 레이어를 넘어 더 넓은 보안 서비스를 제공합니다.\n\n## 모던 데이터 스택 내 의미론적 레이어 통합\n\nDavid P. Mariani은 또한 모던 데이터 스택의 레이어가 주변 레이어와 원활하게 통합되어야 한다고 언급합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n의미 계층은 데이터 플랫폼, 분석 및 출력 계층, 그리고 메타데이터 및 서비스 계층과 깊게 통합되어야 합니다.\n\n클라우드 데이터 플랫폼에서의 범용 의미 계층은 데이터를 데이터 웨어하우스나 레이크하우스에 중앙 집중화합니다. 하이브리드/멀티 클라우드 설정에서는 플랫폼 간 쿼리를 위해 데이터 가상화가 필요합니다. 효율적인 워크플로 관리는 다양한 데이터 플랫폼 아키텍처와의 밀접한 통합을 필요로 합니다:\n\n쿼리 엔진 오케스트레이션: 고객으로부터 쿼리를 플랫폼별 SQL로 동적으로 변환하며, 플랫폼 고유 기능에 최적화되고 의미 모델에서 논리적-물리적 매핑을 반영합니다.\n\n변환 오케스트레이션: 뷰를 물리적 테이블로 재질화하며, 데이터 플랫폼 내에서 성능 및 비용을 최적화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n### 워크플로 오케스트레이션: \n데이터 플랫폼 내에서 새로운 데이터 또는 메타데이터를 생성하며, 사용자 또는 AI/ML 상호 작용을 기반으로 합니다.\n\n### 사용자 정의 함수 (UDF): \n클라우드 데이터 플랫폼의 함수 라이브러리를 활용하여 분석 및 출력을 수행하여 시맨틱 레이어 기능을 향상시킵니다.\n\n### 메타데이터 및 지원 서비스: \n시맨틱 레이어는 메타데이터 및 지원 서비스와 통합하여 데이터 패브릭 생태계의 다양한 도구들과 협력합니다.\n\n- 시맨틱 레이어는 기업용 데이터 카탈로그 도구에 메트릭 및 데이터 모델 검색을 위한 메타데이터 및 계보를 공유해야 합니다.\n- 시맨틱 레이어는 다른 도구로부터 메타데이터를 가져와 시맨틱 데이터 모델을 자동화하고 표준화하기 위한 능력이 있어야 합니다.\n- 시맨틱 레이어에는 사용자 액세스, 가동 시간 및 시스템 성능을 관리하기 위한 모니터링 엔드포인트가 있어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## AtScale과 Databricks를 활용한 의미론적 레이크하우스 구축\n\nAtScale과 Databricks의 협업은 물리적 테이블에 추상화 레이어를 제공하여 의미론적 레이크하우스를 만들고 있습니다. 이를 통해 엔티티, 속성 및 조인을 정의하여 데이터 소비를 단순화하여 분석가와 최종 사용자에게 비즈니스 친화적인 뷰를 제공합니다.\n\nAtScale의 의미론적 레이어는 분석 도구와 Databricks 레이크하우스 사이에 위치하여 데이터를 추상화하여 쉽게 소비할 수 있도록 합니다. Hive SQL, SSAS 큐브 또는 웹 서비스를 통해 연결하여 최적화된 SQL 실행을 위해 쿼리를 Databricks로 전달하여 성능과 확장성을 보장합니다.\n\nAtScale의 Universal Semantic Layer는 쿼리 패턴을 식별하고 자동으로 집계를 관리하는 자율 성능 최적화를 사용합니다. 이로써 수동 노력을 없애고 Delta Lake에서 \"Diamond Layer\" 집계를 생성하여 BI 보고 성능을 향상시키고 분석 데이터 파이프라인과 엔지니어링을 단순화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n툴에 독립적인 의미론적 레이크하우스를 만들기\n\nDatabricks 레이크하우스 플랫폼은 데이터, 분석 및 AI 워크로드를 통합합니다. AtScale의 의미론적 레이크하우스는 BI 및 AI/ML을 지원하여 Tableau, Power BI, Excel 및 Looker를 통해 일관된 사용을 가능하게 하는 툴에 독립적인 의미론적 레이어를 확장합니다.\n\n![image](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_3.png)\n\nAtScale의 유니버설 의미론적 레이어는 BI 및 AI/ML 팀을 통합하여 기업 데이터에 일관된 액세스를 제공합니다. 엑셀을 사용하는 비즈니스 사용자와 노트북을 사용하는 데이터 과학자들이 Databricks 레이크하우스의 모든 기능을 활용할 수 있도록 보장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터 분석의 의미론적 층\n\n조직이 민첩성을 유지하고 신속하게 정보된 결정을 내리길 원하는 경우, 데이터 분석의 유연성은 매우 중요합니다. 의미론적 층은 데이터 층을 시각화 층에서 추상화함으로써 이러한 유연성을 가능케하며 핵심적인 역할을 합니다. 이 추상화를 통해 분석가는 차트, 표 및 그래프를 포함한 데이터의 다른 관점을 쉽게 만들고 분석 필드 및 측정 항목을 손쉽게 변경할 수 있습니다. 결과적으로, 분석가는 빠르게 새로운 통찰을 얻을 수 있게 되며 기업이 변화하는 시장 조건에 빠르게 대응할 수 있도록 전략을 조정하는 데 도움을 줄 수 있습니다.\n\n리처드 마카라(Reconfigured)는 의미론적 층을 갖는 기업들이 사용 가능한 데이터가 업데이트되는 즉시에 액세스할 수 있기 때문에 의사결정에서 더 유연해지고 있다고 언급합니다. 이 실시간 데이터 액세스는 비즈니스 환경의 변화에 적극적으로 대응하고 새로운 기회와 잠재적 위험을 식별하는 데 도움을 줄 수 있습니다. 이러한 유연성을 활용할 수 있는 능력은 투자하는 회사들이 경쟁 우위를 확보하고 현대의 빠르게 변화하는 비즈니스 환경에서 정보에 기반한 결정을 내리는 데 중요합니다. 이는 사용자들이 다음과 같은 일들을 수행할 수 있게 합니다:\n\n- 데이터 표현 방법을 빠르고 쉽게 변경\n- 비즈니스 요구 사항의 변화에 대응\n- 다양한 소스에서 데이터에 쉽게 액세스하고 분석\n- 기존 보고서에서 쉽게 보이지 않는 추세, 이상점 및 다른 통찰력 식별 \n- 협업 데이터 분석 프로젝트에 더 쉽게 참여\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 분석의 유연성이 기업이 비즈니스 환경 변화에 신속히 대응하고 데이터 분석 및 보고서 작성에서 혁신을 유발하는 데 도움이 됩니다. 다양한 데이터 원본과 분석 요구 사항을 수용하는 다재다능한 도구를 제공하여, 기업은 경쟁력을 향상시키고 데이터 가치를 극대화할 수 있습니다.\n\n의미 계층은 데이터 분석을 강화시켜 동일한 데이터 원본에 관계 없이 통합 데이터 원천을 제공함으로써 다부서 간 다재다능한 분석을 가능하게 합니다. 비즈니스 요구 사항의 변화에 따라 데이터 모델을 쉽게 조정할 수 있어 사용자를 방해하지 않고 유연하게 대응할 수 있습니다. 추상화 능력은 복잡한 보고서와 시각화물을 간단하게 작성할 수 있도록 도와 분석적 창의력을 촉진합니다. 데이터 원본의 기술적 제약이 분석을 제한하지 않아 유연성이 더욱 향상됩니다. 사용자는 데이터에 효율적으로 액세스하고 분석하여 의사 결정을 강화할 수 있습니다. 외부 소스와 맞춤형 데이터 계층의 통합은 다차원 분석을 용이하게 합니다.\n\n요컨대, 의미 계층은 현대 기업이 데이터 분석을 미래 지향적으로 준비하고 효율성을 향상시키며 정보에 기반한 의사 결정을 가능하게 함으로써 기업을 강화합니다.\n\n## 데이터 분석에서 의미 계층의 혜택\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSean Leslie (data.world)은 의미론적 레이어가 데이터 분석에 도움이 되는 여러 이유 중에 다음을 강조합니다:\n\n데이터 통합 및 추상화를 단순화합니다: 의미론적 레이어는 다양한 소스에서 데이터를 통합하여 통일된 관점을 제공하고, 데이터 통합을 간소화하여 데이터 분석가와 비즈니스 사용자가 효율적으로 데이터를 조합하고 접근할 수 있도록 합니다.\n\n데이터 이해도와 접근성을 향상시킵니다: 의미론적 레이어는 공통의 비즈니스 어휘를 사용하여 기술적 데이터와 비즈니스 사용자를 연결하여 셀프 서비스 분석 및 BI 도구를 통해 쉬운, 비즈니스에 맞는 데이터 탐색 및 분석을 가능하게 합니다.\n\n데이터 지배 및 보안을 용이하게 합니다: 의미론적 레이어는 데이터 일관성을 위해 비즈니스 규칙을 적용하고 데이터 무결성을 유지하며 역할에 따라 액세스 컨트롤을 시행하여 안전하고 규정 준수된 데이터 액세스를 보장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 의미 계층이 데이터 거버넌스에 미치는 영향\n\n리처드 마카라 (Reconfigured)는 데이터 거버넌스가 데이터 보안, 가용성, 사용 가능성 및 무결성을 관리하는 과정을 지칭한다고 설명합니다. 이는 데이터 자산을 관리하기 위한 정책, 절차 및 통제 방법을 수립하는 것을 포함합니다. 데이터 거버넌스의 목적은 비즈니스 가치 증대, 리스크 감소 및 규정 준수를 보장하는 것입니다.\n\n데이터 거버넌스는 정책과 통제를 통해 올바른 데이터 관리를 보장합니다. 그 중요성은 데이터 무결성, 규정 준수, 보안 유지 및 효과적인 의사 결정을 원활하게 하는 데 있습니다:\n\n- 규정 준수: 데이터 거버넌스는 GDPR, HIPAA 등 관련 법률 및 규정을 준수하는 것을 보장합니다.\n- 신뢰할 수 있는 데이터: 데이터 거버넌스는 데이터 표준화 및 메타데이터 관리 정책을 수립하는 데 도움을 줄 수 있으며, 이는 더 정확한 데이터와 더 나은 분석으로 이어질 수 있습니다.\n- 더 나은 의사 결정: 데이터 거버넌스는 믿을 만한 데이터를 생성하는 데 도움을 줄 수 있어 더 나은 비즈니스 결정을 내릴 수 있게 합니다.\n- 리스크 완화: 데이터 거버넌스 정책이 있을 때 조직은 데이터 손실, 데이터 유출 및 관련 문제와 관련된 리스크를 완화할 수 있습니다.\n- 데이터 보안: 데이터 거버넌스에는 권한이 없는 또는 악의적인 접근을 방지하는 데이터 보안 정책이 포함될 수 있습니다.\n- 데이터 정책 강화: 강력한 데이터 거버넌스를 통해 조직은 각 기능 및 사업 단위 전반에서 데이터 정책의 준수를 보장할 수 있습니다.\n- 비용 절감: 효과적인 데이터 거버넌스를 통해 부정확한 데이터 의사 결정, 신뢰할 수 없는 분석 및 데이터 중복으로 인한 비용을 줄일 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 거버넌스는 조직 내 데이터 품질과 데이터 관리에 상당한 영향을 미칩니다. 빅데이터와 생성적 AI의 급격한 증가로 인해 그 중요성은 높아지고 있으며, 성장하는 정보 양을 효과적으로 처리하기 위한 적응 가능한 데이터 전략이 필요합니다.\n\n## 어떻게 의미론적 레이어가 데이터 거버넌스를 개선하는가\n\n의미론적 레이어는 다양한 소스로부터의 복잡한 데이터를 비즈니스 용어로 단순화하여 조직 전반에 걸쳐 일관된 이해를 보장합니다. 데이터 정의를 중앙 집중화하여 모델링을 단순화하고 변화를 효율적으로 관리할 수 있도록 합니다. 보고 및 분석을 위한 단일 액세스 포인트로 작용하여 데이터 검색을 향상시킵니다. 이는 데이터 거버넌스를 통해 일관성, 표준화, 효율성을 촉진함으로써 데이터 품질, 정확성, 보안 및 규제 준수를 효과적으로 관리합니다:\n\n데이터의 명확한 정의: 의미론적 레이어는 데이터의 일관된 이해를 보장하여 명확한 정의와 문맥을 통해 일관성 부족으로 인한 혼돈과 오류를 제거합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n향상된 데이터 품질: 시맨틱 레이어는 데이터 일관성과 정확성을 보장하여 에러를 최소화하고 데이터 품질을 향상시킵니다. 표준화된 정의는 결정 신뢰성, 효율성, 고객 경험을 향상시키며 에러를 방지하고 정확한 데이터 관리와 검색을 용이하게 합니다.\n\n데이터 모델링의 유연성: 시맨틱 레이어는 데이터 모델링을 간소화하여 여러 소스에 대한 통일된 모델을 제공하며 일관성을 보장하고 진화하는 비즈니스 요구 사항에 적응하며 관리와 유지 보수 프로세스를 간소화하여 전체 데이터 관리 효율성을 향상시킵니다.\n\n제어된 접근: 시맨틱 레이어는 정확한 데이터 접근 제어를 가능케 하여 인가된 사용자가 미리 정의된 규칙에 따라 데이터를 처리할 수 있도록 하여 관리와 데이터 보안을 보장합니다.\n\n변경 관리: 시맨틱 레이어는 데이터 정의를 중앙 집중화하여 변경 관리를 간소화하고 여러 소스 간에 수동 업데이트를 제거하여 효율성을 향상시키고 일관성을 보장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n감사 기능: 시맨틱 레이어는 데이터 접근을 규제하여 감사 기능을 향상시키며, 사용자 활동 추적을 가능하게 하고 책임 소지 및 데이터 사용 및 관리의 투명성을 촉진합니다.\n\n효율적인 워크플로우: 시맨틱 레이어는 데이터 분석, 공유 및 협업을 위한 통합 환경을 제공하여 작업 흐름을 최적화하고 중복을 최소화하며 전반적인 효율성을 향상시킵니다.\n\n데이터 일관성: 시맨틱 레이어는 정의된 규칙과 표준에 따라 통제되어 일관된 데이터 접근성과 무결성을 보장하므로 조직 전반에 걸쳐 보다 정확하고 신뢰할 수 있는 통찰력을 얻을 수 있습니다.\n\n향상된 데이터 계보: 시맨틱 레이어는 데이터 계보 추적을 가능하게 하여 소스에서 비즈니스 개념 매핑을 단순화하고 변환을 투명하게 문서화합니다. 이로써 조기 오류 탐지를 통해 데이터 품질, 규정 준수 및 개선된 의사 결정을 보장하여 거버넌스를 강화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간소화된 데이터 감사: 의미론적 레이어를 통해 더 나은 데이터 거버넌스를 위해 세밀한 수준에서 데이터를 추적하고 감사할 수 있습니다. 이를 통해 데이터의 오류와 불일치를 식별하기가 더 쉬워집니다.\n\n간소화된 데이터 거버넌스: 의미론적 레이어는 메타데이터와 모델을 중앙 집중화하여 데이터 거버넌스를 간소화하며 정책 이행, 자동화된 품질 점검, 일관된 기준을 유지하는 데 도움을 줍니다. 이는 모든 출처에서의 데이터 정확성과 효율성을 향상시키고 리스크를 완화합니다.\n\n개인정보 보호 및 보안 강화: 점점 더 많은 데이터가 처리되고 수집되는 상황에서 데이터 보안과 개인정보 보호의 필요성도 더욱 커졌습니다. 의미론적 레이어는 데이터에 대한 액세스를 승인된 인원만 가능하게 해 개인정보 보호를 강화합니다.\n\n거버넌스 및 감사: 범용적인 의미론적 레이어를 사용하면 변경 이력을 기록하고 명확한 소유권을 파악할 수 있습니다. 또한 새로운 메트릭을 정의할 수 있는 사용자를 명시하거나 제한하는 것이 더 쉬워집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n향상된 규정 준수: 더 나은 데이터 거버넌스를 통해 의미론적 계층은 모든 데이터가 규정 요구 사항에 따라 처리되도록 보장하여 데이터 침해의 위험을 줄입니다.\n\n의미론적 계층을 구현하면 데이터 거버넌스에 상당한 혜택이 있습니다. 데이터의 통합된 표시를 제공함으로써 관리 프로세스를 간소화하고 데이터 품질과 보안을 향상시켜 효율적인 데이터 관리와 상식적인 의사 결정을 위한 필수적인 도구가 됩니다.\n\n## 의미론적 계층으로 AI 강화하기\n\nAtScale의 David P. Mariani는 선도적인 데이터 기관이 진단형, 예측형, 권고형 분석을 통합하는 AI와 함께 증강 분석에 중점을 두고 있다고 지적하며, 데이터 이동을 줄이며 효율성을 향상시키기 위해 데이터 과학/기계 학습 플랫폼을 통합한다고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이문 찌리(Progress)는 기업 데이터와 생성 AI를 통합하는 것이 신뢰성, 투명성, 보안을 향상시키며 데이터 품질과 확장성을 향상시킬 수 있다고 강조했습니다. 의미론적 레이어를 활용하면 기업은 오류를 줄이고 신뢰성을 보장하며 거버넌스 표준을 준수하면서 혁신적인 비용 절감 기회를 창출하고 의사 결정 및 운영 효율성을 향상시킬 수 있습니다. 의미론적 레이어의 두 가지 주요 이점:\n\n\\[ \\begin{aligned} \n1. 생성 AI의 결과물에서 환각 감소: \\end{aligned} \\]\n생성 AI 모델은 종종 인간의 추론과 이해 부족으로 인해 알려진 환각이라고 불리는 잘못된 답변을 생성합니다. 이러한 오류는 15-20%의 확률로 발생하며 의미론적 데이터 플랫폼을 사용하여 데이터를 맥락화하고 조화시킴으로써 감소할 수 있습니다. 적절한 데이터 정리, 정리 및 모델링은 AI의 정확성을 향상시키는 데 필수적입니다.\n\n\\[ \\begin{aligned} \n2. 생성 AI의 결과물의 신뢰성과 신뢰성 향상: \\end{aligned} \\]\n생성 AI 모델은 종종 비관련, 부정확 또는 편향적인 결과물을 생성하는데, 이는 중요한 비즈니스 결정에 문제를 일으킬 수 있습니다. 기업 의미론적 데이터 시스템과 생성 AI를 통합함으로써 기업은 AI 결과물의 정확성과 신뢰성을 향상시킬 수 있습니다. 사설 의미론적으로 태그된 데이터를 사용해 생성 AI는 조직의 고유한 맥락에 대한 깊은 통찰력을 얻게 됩니다. 이 통합으로 인해 AI 시스템은 실시간 업데이트된 데이터에 접근하여 \"훈련 데이터 차단\" 문제를 해결하고 생성된 답변의 전반적인 품질을 향상할 수 있습니다.\n\n큐브는 대규모 언어 모델(LLM)에 대한 핵심적인 맥락을 제공하는 의미론적 레이어가 AI 기반 데이터 경험에서 중요하다고 강조합니다. 데이터를 의미 있는 비즈니스 정의로 조직하고 쿼리 인터페이스를 제공하여 LLM이 데이터의 맥락을 이해하도록 보장함으로써 오류와 환각을 줄이고 혁신적인 AI 응용프로그램을 가능하게 하며 쿼리 프로세스를 간소화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLMs은 혁신적이지만, \"입력이 쓰레기면 출력도 쓰레기\" 문제 때문에 정확한 결과물을 생성하는 데 한계가 있습니다. LLMs는 환각을 일으킬 수 있습니다. 그들에게 단순히 데이터베이스 스키마를 공급하는 것만으로 올바른 SQL을 생성하는 데 충분하지 않습니다. 데이터 컨텍스트를 포함하여 데이터를 개념적으로 이해하기 위해 의미론적 계층이 필요합니다. 이 계층은 쿼리를 위해 데이터를 비즈니스 정의로 구성하고, LLM이 이를 통해 쿼리할 수 있도록하여 정확성을 보장합니다. 따라서 의미론적 계층은 LLM이 환각하는 문제를 해결함으로써 필요한 컨텍스트를 제공하고, 쿼리와 데이터 출력의 정확성을 보장합니다.\n\n![이미지](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_4.png)\n\nLLMs와 의미론적 계층을 결합하면 새로운 AI 주도 데이터 경험을 제공합니다. 이러한 계층은 AI 에이전트에 필수적인 컨텍스트를 제공하여 정확한 데이터 쿼리를 가능하게 하고, 조직이 사용자 정의 LLM 애플리케이션을 구축할 수 있도록 돕습니다. 데이터 웨어하우스 상단에 위치한 의미론적 계층은 AI 기술과의 원활한 통합을 촉진합니다.\n\n![이미지](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n의미론적 계층 데이터 모델은 LLM이 데이터를 이해하고 올바른 쿼리를 생성하기 위한 맥락으로 사용하는 구조와 정의를 제공합니다. 복잡한 조인 및 계산을 추상화하여, 의미론적 계층은 비즈니스 수준 용어를 기반으로 한 단순화된 인터페이스를 제공하여 오류를 줄이고 환각을 방지합니다.\n\n아르투르 키둔노프(Cube)는 모든 BI 도구 간 데이터 소비를 효율적으로 만들어 실수를 줄이고 일관된 데이터 원천과 신뢰를 유지하는 통합된 의미론적 계층의 중요성을 강조합니다. 모든 데이터 환경에 대한 메트릭 및 메타데이터를 정의하여 BI 소프트웨어부터 AI 도구에 이르기까지 다양한 플랫폼에서 데이터에 접근할 수 있도록 보장합니다. 이 다양성은 데이터 전달의 진화하는 풍경을 수용하고, 차세대 데이터 주도 애플리케이션을 지원합니다.\n\n일관된 데이터 없이는 AI도 불가능합니다: 고품질의 데이터는 AI에게 방대한 데이터셋으로부터 신뢰할 수 있는 통찰력을 제공할 수 있게 해줍니다. 통합된 의미론적 계층은 이 맥락에서 중요한 역할을 하며, 비즈니스 컨텍스트와 정의를 제공하여 AI 도구가 오류를 방지할 수 있습니다. AI는 비즈니스 사용자를 위해 자연어 쿼리를 통해 데이터 큐레이션과 민주화를 용이하게하는 정의 및 코드 개선을 제안하여 의미론적 계층을 향상시킬 수 있습니다.\n\n적용된 AI의 부상: AI의 보급은 대규모 플랫폼뿐만 아니라 도메인별 응용 프로그램으로도 확대되면서, 일관되고 정확한 데이터를 위해 통합된 의미론적 계층이 필요해졌습니다. 의미론적 계층은 입력이 정확하고 관련성이 있으며 일관되도록 보장합니다. 이는 AI 주도 경험에서 정확한 결과와 경쟁 우위를 위해 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI에 준비된 범용 의미론적 층의 장점: AI에 준비된 범용 의미론적 층은 다양한 데이터 플랫폼을 연결하고, 데이터 민주화를 촉진하며 고객을 대상으로 하는 애플리케이션을 지원하는 데 필수적입니다.\n\nPatrk Liu Tran(Validio)은 의미론적 층이 AI 및 LLM과 혁신적인 데이터 경험을 창출하려는 기업들로부터 많은 관심을 받고 있다고 강조하고 있습니다. 중요한 목표 중 하나는 자연어 쿼리를 LLM에 가능하게 함으로써 데이터 검색을 간소화하고 분석가들을 하찮은 작업으로부터 해방시키는 것입니다.\n\n의미론적 층과 LLM을 통합하면 정확도가 최대 300%까지 개선됩니다. 메트릭스를 미리 정의하고 잘못된 가정의 위험을 줄임으로써 정확성이 높아집니다. 의미론적 층을 사용하면 LLM이 합의된 비즈니스 메트릭스에 따라 작동하여 정밀성을 향상시킵니다. LLM이 보다 보편화되면 데이터 중심적 기관들에게 의미론적 층의 중요성이 점점 더 분명해지고 있습니다. 이러한 접근법은 더욱 효율적이고 정확한 데이터 처리 및 분석을 약속하며, 결과적으로 더 나은 의사 결정 및 운영 효율성을 가능하게 합니다.\n\n# 미래 지향 및 신흥 트렌드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nTomasz Tunguz(Theory Ventures)은 시맨틱 모델이 중요한 추세로 떠오를 것이라며, 조직간의 정의를 통일하여 사람의 이해와 대형 언어 모델의 의미 합성을 위한 단순화된 분석을 개선하고 재사용성과 구성 가능성을 향상시킬 것이라고 언급했습니다.\n\nTimeXTender에 따르면 시맨틱 레이어 기술은 지속적으로 발전하며 데이터 관리 전략에 영향을 미치고 다양한 산업에서 새로운 응용 가능성을 열어줄 것입니다.\n\nAtScale은 시맨틱 레이어가 계속해서 중요성을 갖게 될 것으로 보며, 조직 간의 정의를 통일하여 분석을 단순화하고 재사용성을 촉진하며 인간의 이해와 대규모 언어 모델의 의미 합성을 가능하게 합니다.\n\nGenAI와 같은 책임 있는 AI 도입은 투명성과 편향과 같은 윤리적 측면을 우선시합니다. AI 관리자와 같은 신생 직무들은 윤리적인 실행을 보장합니다. 기업은 점점 데이터를 제품으로 취급하며 맞춤형 비즈니스 결과를 위해 재사용 가능한 데이터 제품을 만들어냅니다. 생성적 AI와 대규모 언어 모델(LLMs)의 통합은 데이터 탐색을 변형시키며, MDX 생성은 쿼리 유연성을 높이고 자연 언어 인터페이스는 데이터 접근을 민주화합니다. 예측과 이상 탐지와 같은 AI 기능들은 의사 결정력을 강화하여 분석 접근성을 넓히고 통찰력을 심화시켜 혁신을 이끌어갈 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n의미적 레이어는 현대 데이터 관리의 다양한 도전에 대한 중심적인 해결책이 됩니다. 데이터의 통합된 관점을 제공하고 액세스를 간소화함으로써, 데이터의 제한된 가용성과 일관성 없는 보고 문제를 해결합니다. 또한 이러한 통합은 개선된 데이터 거버넌스를 촉진하고 AI 통합을 용이하게 하여 조직의 효율성과 유연성을 증가시킵니다. 의미적 레이어의 중요성은 데이터 분석을 혁신하고 AI 애플리케이션을 지원하여 보다 정보화된 의사 결정 과정을 가능케하는 변화력 있는 잠재력에 있습니다. 기업이 점점 데이터 중심적인 세계의 복잡성에 대처함에 따라, 의미적 레이어는 데이터 자원을 관리하고 활용하는 데 더 큰 유연성과 효과성을 제공하는 기반 요소입니다.\n\n# 참고문헌\n\nAtScale 및 Databricks를 활용한 의미적 레이크하우스 구축\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**범용 의미 계층의 힘**\n\n두 세계를 하나로: 데이터 엔지니어와 비즈니스 애널리스트 결합하기\n\n의미 계층이란 무엇인가요?\n\n데이터 전략에서 의미 계층이 차지하는 위치\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시맨틱 레이어가 데이터 거버넌스에 미치는 영향\n\n데이터 시각화에서 시맨틱 레이어 사용의 이점\n\n시맨틱 레이어에 대한 궁극적인 가이드\n\n유니버설 시맨틱 레이어로 현대 비즈니스 인텔리전스를 재구축하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현대 데이터 스택의 의미론적 레이어\n\n의미론적 레이어란 무엇이며, 데이터를 지식으로 변환하는 방법은 무엇인가요?\n\n의미론적 레이어를 구현하는 세 가지 방법\n\n의미론적 레이어란 무엇인가요? (구성 요소 및 기업 애플리케이션)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAtScale은 AI, GenAI 모델을 위한 의미론적 레이어 지원 추가\n\n의미론적 레이어의 의미\n\n의미론적 레이어: AI 기반 데이터 경험의 중추\n\n의미론적 레이어는 AI 기반 분석을 위한 빠져있던 조각입니다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기업을 위한 생성 AI의 혜택과 의미론적 데이터 통합\n\n만약 범용 의미론적 레이어가 없다면 실시간 AI 경험을 진화시킬 수 없습니다\n\n의미론적 레이어 101: 데이터 팀이 데이터보다는 지표에 집중해야 하는 이유\n\n기업 분석과 생성 AI를 위한 2024 의미론적 레이어 혁신\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현대 데이터 인프라를 위한 신흥 아키텍처","ogImage":{"url":"/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_0.png"},"coverImage":"/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_0.png","tag":["Tech"],"readingTime":20},{"title":"Rockset이 OpenAI에 인수되다 사용자들에게 어떤 의미가 있을까","description":"","date":"2024-06-22 17:03","slug":"2024-06-22-RocksetIsAcquiredbyOpenAIWhatDoesItMeanforItsUsers","content":"\n\n2024년 6월 21일, OpenAI가 데이터 색인 및 쿼리 기능으로 유명한 실시간 분석 데이터베이스인 Rockset을 인수했다고 발표했습니다. 이 인수는 Rockset 사용자들에게 중요한 변화를 알립니다. 사용자들은 이 플랫폼을 이탈해야 하는 제한된 시간을 가지고 있으며 다음 단계에 대해 궁금해하고 있습니다. 본 문서는 Rockset 사용자들이 이 전환을 안내하며, OpenAI가 왜 이러한 결정을 내렸는지, 즉각적으로 필요한 조치는 무엇인지, 그리고 Rockset 사용자들과 실시간 분석 요구 사항을 위한 이상적인 대안으로 어떤 솔루션이 적합한지에 대해 통찰을 제공할 것입니다.\n\n# OpenAI가 Rockset을 인수한 이유는 무엇인가요?\n\nOpenAI는 Rockset의 기술을 통합하여 제품 전반에 걸쳐 검색 인프라를 강화하려고 합니다. 이는 인공지능 업계에서 실시간 데이터 액세스와 처리의 중요성을 명확하게 보여 주는 지표입니다. 게다가 Rockset 인수를 통해 OpenAI는 실시간 분석 전문가 팀을 흡수하여 OpenAI의 능력을 계속 강화할 것입니다.\n\n# Rockset 사용자가 해야 할 첫 번째 일\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRockset 사용자 분들에게 시한부가 닥쳐오고 있습니다. Rockset의 자세한 FAQ에 따르면, 계약 없는 월간 요금제 사용자들은 2024년 9월 30일까지 오프보딩을 진행해야합니다. 계약된 고객들은 Rockset 계정 팀과 협력하여 적절한 오프보딩 계획을 개발할 수 있지만, 모든 고객들은 빠르게 Rockset 대체안을 찾아야 합니다. 인수가 계획된 상황에서, 이제는 Rockset 사용자들이 다음 단계를 취해야 할 때입니다.\n\n![이미지](/assets/img/2024-06-22-RocksetIsAcquiredbyOpenAIWhatDoesItMeanforItsUsers_0.png)\n\nRockset 사용자들은 다음 단계를 시작해야 합니다:\n\n- 현재 사용량과 요구 사항을 평가하십시오: 솔루션을 평가하기 전에 무엇을 찾고 있는지 알아두는 것이 좋습니다. 이는 많은 시간을 절약할 수 있습니다.\n- 비슷하거나 더 나은 기능을 제공하는 대체 플랫폼 목록 작성 시작: 기존에 Rockset을 어떻게 사용했느냐에 따라 비즈니스의 요구 사항이 단순할 수도 복잡할 수도 있습니다. 각 플랫폼은 장단점이 있습니다. 비즈니스에 중요한 성능과 능력을 제공할 수 없는 솔루션을 평가하는데 소중한 시간을 낭비하지 않도록 어떤 플랫폼이 무엇을 할 수 있어야 하는지 알아두는 것이 중요합니다.\n- 작업 중단 없이 마이그레이션 과정을 계획하기 시작: 오픈 소스든 상용 솔루션이든, 솔루션과 함께 제공되는 지원이나 커뮤니티를 평가하는 것이 중요합니다. 성공적인 POC를 진행할 수 있는 옆에서 지원해줄 파트너를 찾거나 24시간 내내 문제 해결을 도와줄 활성 Slack 커뮤니티를 찾는 것은 마이그레이션이 원활히 진행되도록 하는 데 도움이 될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Rockset 사용자를 위한 대안\n\nRockset 사용자가 다음 단계를 계획할 때, 각 합당한 대안을 탐색하는 것이 중요합니다. 사용 사례와 성능 요구에 따라, 다양한 플랫폼이 원하는 능력을 제공할 수 있습니다. 고려할만한 몇 가지 옵션은 다음과 같습니다:\n\n# 오픈 소스 실시간 분석 SQL 워크로드를 위한:\n\n- Apache Druid: Druid는 대규모 데이터에서 실시간 및 일괄 처리 쿼리를 하위 초 단위로 제공하는 고성능 실시간 분석 데이터베이스입니다.\n- ClickHouse: ClickHouse는 고속 오픈 소스 열 지향 데이터베이스 관리 시스템으로, SQL 쿼리를 사용하여 실시간으로 분석 데이터 보고서를 생성할 수 있습니다.\n- StarRocks: 확장 가능한 JOIN 쿼리를 실행하고 정규화 파이프라인 없이 실시간 분석을 제공하기에 적합합니다. StarRocks는 기본적으로 실시간 데이터 업서트를 지원하며, 열 지향 저장소에서 직접 가변 데이터로 두 번째 수준의 데이터 신선도를 제공할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 프로프리업 관리형 솔루션을 통한 실시간 분석 SQL 워크로드:\n\n- Imply: 엔터프라이즈 지원을 통한 클라우드 상의 관리형 Apache Druid.\n- CelerData: StarRocks 프로젝트의 주도 및 유지보수를 지원하는 클라우드 관리형 StarRocks.\n\n# 오픈 소스 벡터 검색(VectorDB)을 위한:\n\n- Weaviate: Weaviate는 객체 및 벡터를 저장하는 오픈 소스 벡터 데이터베이스로, 클라우드 네이티브 데이터베이스의 내결함성 및 확장성과 함께 벡터 검색을 구조화된 필터링과 결합할 수 있습니다.\n- Milvus: 차세대 AI 애플리케이션을 위한 클라우드 네이티브 벡터 데이터베이스 및 스토리지\n- Qdrant: 다음 세대 AI를 위한 고성능 대규모 벡터 데이터베이스.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 관리되는 벡터 검색 (VectorDB):\n\n- SingleStore: SQL 능력 이외에도 SingleStore는 관리되는 벡터 검색 기능을 제공하여 양종의 워크로드에 대한 포괄적인 솔루션을 제공합니다.\n- Zilliz: Milvus 뒤의 회사인 Zilliz는 Milvus의 혜택을 추가 지원 및 유지 보수로 제공하는 관리되는 벡터 검색 서비스를 제공합니다.\n- Pinecone: 배포 및 벡터 검색 애플리케이션의 스케일링을 간소화하고 높은 가용성과 성능을 보장하는 완전히 관리되는 벡터 검색 플랫폼입니다.\n\n전환을 해야 하는 시긴데요. 중요한 인프라가 그대로 유지되고 작동되도록 해야 합니다. 이 플랫폼마다 고유한 장점이 있으며, 당신의 특정 요구 사항을 기반으로 평가하여 성공적인 마이그레이션이 이루어질 것입니다.\n\n# 왜 StarRocks가 Rockset 이주민들에게 가장 좋은 다음 단계인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은 Rockset 사용자들이 실시간 분석 요구를 위해 채택했습니다. 따라서 특히 현재 실시간 분야에서 한 명의 선도 업체 중 하나를 언급하는 것이 중요합니다: StarRocks. 실시간 분석을 위한 강력하고 효율적인 대안을 찾는 Rockset 사용자들에게 StarRocks는 매력적인 선택지를 제시합니다. 이유는 다음과 같습니다:\n\n- 확장 가능한 JOIN 쿼리: StarRocks를 사용하면 확장 가능한 JOIN 쿼리를 실행할 수 있으며 데노멀라이제이션 파이프라인이 필요없이 실시간 분석을 제공하여 데이터 처리를 간소화하고 성능을 향상시킵니다.\n- 실시간 데이터 업서트: Rockset에서 StarRocks로 전환할 때 데이터 신선도를 유지할 수 있습니다.\n- 우수한 성능: 컬럼형 스토리지, 벡터화 및 SIMD를 활용하여 StarRocks는 Rockset보다 우수한 성능을 달성하며 저장 공간의 일부만 사용하여 비용 효율적인 솔루션이 됩니다.\n- 오픈 소스 커뮤니티: Apache 라이선스를 받은 리눅스 재단 프로젝트인 StarRocks는 거대하고 성장 중인 글로벌 커뮤니티가 언제든지 도와줄 준비가 되어 있습니다.\n\n# 다음 단계\n\nOpenAI에 의한 Rockset 인수는 사용자들에게 도전과 기회를 제시합니다. 전환은 어렵게 느껴질 수 있지만, 우수한 성능과 확장 가능성을 제공하는 플랫폼으로 업그레이드할 기회이기도 합니다. StarRocks의 실시간 분석 성능에 대해 더 알아보고 이주하는 동안 지원을 받으려면 Slack 커뮤니티에 가입하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막 순간까지 기다리지 말고 오늘 Rockset에서의 원활한 이전을 보장하려면 이전 계획을 시작하세요.","ogImage":{"url":"/assets/img/2024-06-22-RocksetIsAcquiredbyOpenAIWhatDoesItMeanforItsUsers_0.png"},"coverImage":"/assets/img/2024-06-22-RocksetIsAcquiredbyOpenAIWhatDoesItMeanforItsUsers_0.png","tag":["Tech"],"readingTime":4},{"title":"파이썬과 Streamlit으로 멀티페이지 금융 대시보드 만들기 처음부터 끝까지 완성하기","description":"","date":"2024-06-22 17:01","slug":"2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch","content":"\n\n\u003cimg src=\"/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_0.png\" /\u003e\n\n개인화된 작업 공간을 상상해보세요. 여기서는 상품, ETF, 과소평가된 주식, 그리고 변동성 있는 암호화폐 시장을 각각 별도의 탭에서 동시에 모니터링할 수 있습니다. 이러한 설정은 데이터를 손쉽게 접근할 수 있는 것 이상을 제공합니다. 표준 도구들이 제공하는 유연성 부족으로 자주 허용되지 않는 방식으로 이 데이터를 통합하고 상호작용할 수 있게됩니다. 여러분의 대시보드를 구축함으로써, 관련성 있는 지표를 강조함으로써 유연성을 얻을 수 있고, 사용자 정의 필터를 적용하고, 심지어 시장 변동에 실시간으로 반응하는 고급 분석 도구를 통합할 수 있습니다.\n\n투자 결정에 중요한 고유한 지표들을 간과할 수 있는 범용 인터페이스에 의존할 필요가 없습니다. 여러분의 대시보드를 생성함으로써, 데이터 분석 과정을 통제하고 시장에 대한 더 깊은 이해를 기르는 전략적인 과정이 됩니다. 이제 이 혁신적인 도구를 만드는 것을 시작해봅시다.\n\n데이터 작업에 파이썬보다 더 나은 것이 무엇인가요? 그리고 데이터를 분석하는 데 가장 사용자 친화적인 방법이 무엇인가요? 우리는 파이썬에 대해 더 많은 이해가 있고, Streamlit을 소개할게요. 이 Streamlit은 데이터 분석을 위한 대화형 웹 애플리케이션을 빠르고 쉽게 구축할 수 있는 강력한 오픈 소스 파이썬 라이브러리입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 난이도의 개발자를 대상으로 설계된 Streamlit은 데이터 스크립트를 공유 가능한 웹 앱으로 쉽게 변환할 수 있도록 도와줍니다.\n\n# Streamlit을 특별하게 만드는 요소는 무엇인가요?\n\n1. 사용 용이성: Streamlit의 매력은 그 간단함에 있습니다. 직관적인 Python 코드로 앱을 작성하므로 HTML, CSS 또는 JavaScript와 같은 복잡한 웹 기술을 알 필요가 없습니다. 이 사용자 친화적인 방식은 웹 프로그래밍 경험이 부족한 데이터 과학자와 분석가들에게 앱 개발의 가능성을 열어줍니다.\n\n2. 빠른 프로토타이핑: Streamlit을 사용하면 코드를 수정하면 해당 변경 사항이 앱 인터페이스에 자동으로 업데이트되어 이터레이션 과정이 매우 신속해집니다. 이 기능을 이용하면 앱을 동적으로 조정하고 실시간으로 결과를 확인할 수 있어 빠른 프로토타이핑과 실험에 매우 유용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 다양한 내장 위젯: Streamlit에는 슬라이더, 체크박스, 드롭다운과 같은 다양한 내장 위젯이 포함되어 있어 데이터와 상호 작용하기가 매우 쉽습니다. 이러한 요소들은 코드를 최소한으로 사용하여 추가할 수 있어 사용자가 표시된 데이터나 수행 중인 계산을 조작할 수 있게 해줍니다.\n\n4. 데이터 시각화 지원: Streamlit은 Matplotlib, Seaborn, Plotly와 같은 주요 Python 데이터 시각화 라이브러리와 완벽하게 통합되어 있습니다. 이 통합을 통해 차트, 지도 및 그래프를 쉽게 앱에 통합하여 데이터를 더욱 매력적이고 정보를 제공하는 방식으로 시각화할 수 있습니다.\n\n자, 그럼 더 이상의 말이 필요 없죠. 이를 만들어 봅시다. 먼저, 필요한 것을 이해해봅시다: 사용자로서, 주요 재정 데이터를 한 곳에 표시하고 4가지 관심 영역(Crypto, ETF, 주식, 상품) 간에 쉽게 탐색할 수 있기를 원합니다. 아래는 이 모든 세그먼트에 대한 요구 사항입니다.\n\n- Cryptos: 시가 총액을 기준으로 상위 500개의 가상 화폐에 대한 실시간 가격 및 이전 종가, 시장 변동 폭, 7일 변동 사항을 표시합니다.\n- ETFs: 펀드의 성과 지표: 현재 가격, 52주 최고 및 최저가, 연 별 ETF 수익률(%), 배당 수익률 및 ETF의 실제 옵션에 대한 정보입니다.\n- Stocks: 주식의 현재 가격을 수집하고 주요 지표인 주가수익(P/E) 비율, 주당순이익(EPS)을 함께 제시합니다. 그것을 기반으로 설정한 임계값과 함께, 주식의 공정 가치를 계산하여 현재 주식 가치와 공정 주식 가치 간의 차이를 보여줍니다(차이가 충분히 크다면, 아마도 가격이 저평가된 주식일 것입니다).\n- Commodities: 현재 시장 가격, 과거 추이 (그래프), 판매 단위를 표시합니다(주식과는 다르게 각 상품에는 다른 측정 단위가 있으며, 이전 종가로부터의 가격 변동).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"알았다고 했잖아!\", 버니 맥의 캐릭터가 \"오션스 13”에서 강조했던 것처럼. 코드 작업을 시작해봐요.\n\n새로운 파이썬 프로젝트를 만들고 중요한 몇 가지 파일을 포함해보세요: ETF 목록과 코드로 분석할 주식 목록이 필요할 거에요. 두 파일 모두 준비해 놨어요 (그동안 가지고 있었고, 관심이 있는 분들께 DM을 통해 공유할 수 있어요).\n\n# Streamlit 환경 설정하기\n\nStreamlit과 데이터 처리 및 시각화를 위해 pandas와 matplotlib/plotly와 같은 필요한 라이브러리를 설치해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\npip install streamlit pandas matplotlib plotly\n```\n\n메인 스크립트(streamlit_app.py)를 만들고 각 대시보드 패널을 위한 개별 스크립트를 작성하세요. 앱을 구성하기 위해 Streamlit의 레이아웃 기능을 활용해주세요:\n\n- 주요 네비게이션: st.sidebar.radio 또는 st.sidebar.selectbox를 사용하여 다른 금융 세그먼트 간에 탐색할 수 있게 사이드바를 활용하세요.\n- 대시보드 콘텐츠: 암호화폐, ETF, 주식 및 상품 모듈에 있는 각 페이지 함수(app())에서 데이터를 표시하는 데 필요한 표, 차트 및 상호 작용 위젯을 설정해주세요.\n\n우리의 streamlit_app.py 파일은 프로젝트의 루트 폴더에 있어야 합니다. 또한 대시보드 페이지가 있는 pages 폴더가 필요합니다 (각 대시보드마다 한 페이지씩).\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n## 내 Streamlit 어플리케이션 폴더 구조:\n- **pages/**: 대시보드의 여러 페이지를 위한 디렉토리\n    - **__init__.py**: 'pages'를 파이썬 패키지로 만듦\n    - **commodities.py**: 상품 대시보드 모듈\n    - **cryptos.py**: 암호화폐 대시보드 모듈\n    - **etfs_value.py**: ETF 대시보드 모듈\n    - **underpriced_stocks.py**: 저평가 주식 대시보드 모듈\n- **stramlit_app.py**: 주 Streamlit 어플리케이션 파일\n\n\n주 Streamlit 어플리케이션 코드부터 시작해봅시다. (특정 대시보드 페이지에 대한 중요 부분은 이미 이전 게시물 몇 개에서 살펴봤습니다.)\n\n```python\nimport streamlit as st\nfrom pages import commodities, cryptos, etfs_value, underpriced_stocks\n\n# 페이지 딕셔너리\npages = {\n    \"상품\": commodities,\n    \"암호화폐\": cryptos,\n    \"ETFs 가치\": etfs_value,\n    \"저평가 주식\": underpriced_stocks\n}\n\nst.sidebar.title('Navigation')\nchoice = st.sidebar.radio(\"페이지 선택:\", list(pages.keys()))\n\npage = pages[choice]\npage.app()  # 각 모듈이 페이지를 실행하기 위한 app 함수를 갖고 있다고 가정\n```\n\n앱 구조를 보면 매우 간단합니다. 필요한 대시보드 페이지와 Streamlit 라이브러리를 import하고, 페이지 딕셔너리를 작성하고, 사이드바 제목을 설정하고, 선택 메커니즘을 구현합니다. 이를 통해 사용자가 앱의 다른 섹션으로 이동할 수 있도록 합니다. 마지막으로 page.app() 메서드를 호출하여 사용자의 선택에 따라 적절한 페이지를 동적으로 로드합니다. 이 방법은 각 페이지 모듈 내의 특정 기능과 연결되어 대시보드를 렌더링하는 데 사용됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n작은 이정표를 달성했어요: 더 큰 목표를 향해 나아가요.\n대시보드 페이지 만들기\n\npages/commodities.py\n\n```js\nimport streamlit as st\nimport yfinance as yf\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 상품, 측정 단위 및 이름 정의\ncommodities_info = {\n    \"CL=F\": {\"unit\": \"배럴\", \"name\": \"원유 (WTI)\"},\n    \"BZ=F\": {\"unit\": \"배럴\", \"name\": \"브렌트 원유\"},\n    \"NG=F\": {\"unit\": \"mmBtu\", \"name\": \"천연가스\"},\n    \"HO=F\": {\"unit\": \"갤런\", \"name\": \"난방유\"},\n    // (중략)\n}\n\n@st.cache  # 데이터 캐싱을 통해 과도한 API 호출을 방지합니다\ndef fetch_commodity_data(tickers, period=\"6d\", interval=\"1d\"):\n    try:\n        data = yf.download(tickers, period=period, interval=interval)\n        return data\n    except Exception as e:\n        st.error(f\"상품 데이터 검색에 실패했습니다: {str(e)}\")\n        return pd.DataFrame()  # 실패 시 빈 DataFrame 반환\n\ndef app():\n    st.title(\"상품 대시보드\")\n\n    // (중략)\n\nif __name__ == \"__main__\":\n    app()\r\n```\n\n이 Streamlit 앱은 선택한 기간 동안 상품 가격과 변동을 보여주는 대시보드를 표시하는 데 사용됩니다. 사용자들은 특정 상품, 시간대 및 데이터 세부 사항을 기반으로 사용자 정의로 표시를 조정할 수 있습니다. 특정 라이브러리를 가져온 후, commodities_info라는 딕셔너리를 만들었습니다. 이 딕셔너리는 관심 상품을 정의하며, 해당 시장 티커 심볼, 측정 단위 및 이름을 포함합니다. 이 딕셔너리는 특정 상품을 티커 심볼로 참조하면서 사용자 친화적인 이름과 단위를 표시하는 데 앱 전반에서 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 부분은 매우 중요합니다: @st.cache 데코레이터를 사용하여 함수 호출 결과를 캐싱하는 중요한 임무를 수행하고 있습니다. 이를 통해 입력을 기반으로 함수 호출 결과를 캐싱함으로써 yfinance에 대한 API 호출 수를 줄여 대역폭을 절약할 뿐만 아니라 첫 로드 후 사용자 상호작용 속도도 높일 수 있습니다.\n\n- tickers: 상품 기호 목록.\n- period: 데이터를 가져올 시간 기간을 지정하는 문자열 (기본값은 \"6d\" 또는 6일). 대시보드에서 사용자가 더 많은 내용을 볼 수 있도록 변경할 수 있습니다.\n- interval: 데이터 포인트의 정밀도 (기본값은 \"1d\" 또는 매일). 대시보드에서도 사용자 정의가 가능합니다.\n\n## 앱 기능\n\n이 함수는 주요 응용 프로그램 인터페이스를 정의합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- st.title(\"상품 대시보드\"): 대시보드의 제목을 설정합니다.\n- 사이드바 입력란을 통해 사용자는 데이터의 기간과 간격을 선택하고 어떤 상품을 표시할지 선택할 수 있습니다.\n\n## 데이터 로드 및 표시\n\n상품이 선택된 경우, 앱은 fetch_commodity_data 함수를 사용하여 데이터를 검색합니다. 성공적인 데이터 검색은 최신 및 이전 종가를 사용하여 변동률을 계산하는 데이터 처리를 트리거합니다. 이 데이터는 그런 다음 데이터 프레임에 표시됩니다. 그 후에는 시각적인 그래픽을 위한 플로팅 함수를 정의하고 있습니다 (이 대시보드에서 그래프와 플롯만 사용 가능합니다. 다른 대시보드의 경우 주식/ETF/암호화폐의 수가 100보다 많아 시스템에 불필요한 부하가 될 수 있습니다).\n\n## 실행 시작점\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nif __name__ == \"__main()\":\n    app()\n```\n\n이 줄은 스크립트가 직접 실행되었는지 확인한 후 streamlit 애플리케이션을 시작하는 app() 함수를 호출합니다.\n\n축하드립니다! 네가 만든 대시보드 페이지 중 첫 번째를 성공적으로 만들었어요!\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n앗, 분할 취소를 요청해주셨군요. 물론이죠, 질문이 있으시면 언제든지 물어보세요! 😉\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMarkdown 형식으로 표를 나타냅니다.\n\n파일 pages/underpriced_stocks.py\n\n```js\nimport streamlit as st\nimport yfinance as yf\nimport pandas as pd\nimport requests\n\n# API 액세스를 위한 상수\nAPI_KEY = 'Your API Key'\nBASE_URL = 'https://financialmodelingprep.com/api/v3'\n\n@st.cache_resource\ndef fetch_sp500_tickers():\n    \"\"\"\n    API를 사용하여 현재 S\u0026P 500 소속 티커를 가져옵니다.\n    \"\"\"\n    url = f\"{BASE_URL}/sp500_constituent?apikey={API_KEY}\"\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            data = response.json()\n            tickers = [item['symbol'] for item in data]\n            return tickers\n        else:\n            st.error(f\"티커를 가져오지 못했습니다: HTTP 상태 코드 {response.status_code}\")\n            return []\n    except Exception as e:\n        st.error(f\"요청 실패: {e}\")\n        return []\n\n@st.cache\ndef fetch_stock_data(tickers):\n    \"\"\"\n    주어진 티커에 대한 주식 데이터를 가져와 주식이 저평가되었는지 계산합니다.\n    \"\"\"\n    data = []\n    for symbol in tickers:\n        stock = yf.Ticker(symbol)\n        try:\n            info = stock.info\n            if 'currentPrice' in info and 'trailingEps' in info:\n                current_price = info['currentPrice']\n                eps = info['trailingEps']\n                pe_ratio = info.get('trailingPE', float('inf'))  # 사용 가능한 경우 trailing P/E 사용\n\n                # 목표 P/E 비율 가정\n                target_pe = 15\n                fair_value = eps * target_pe\n\n                underpriced = current_price \u003c fair_value\n                price_gap = ((fair_value - current_price) / current_price) * 100 if current_price else 0\n\n                data.append({\n                    'Symbol': symbol,\n                    'Current Price': current_price,\n                    'EPS': eps,\n                    'Fair Market Value': fair_value,\n                    'Underpriced': '네' if underpriced else '아니요',\n                    'Price Gap (%)': round(price_gap, 2)\n                })\n        except Exception as e:\n            print(f\"{symbol}에 대한 데이터를 가져오지 못했습니다: {e}\")\n\n    return pd.DataFrame(data)\n\ndef app():\n    \"\"\"\n    S\u0026P 500 주식 및 저평가 상태를 표시하는 Streamlit 앱입니다.\n    \"\"\"\n    st.title(\"S\u0026P 500 주식 분석\")\n\n    tickers = fetch_sp500_tickers()\n\n    if tickers:\n        st.write(\"S\u0026P 500 회사의 티커가 로드되었습니다.\")\n        df = fetch_stock_data(tickers)\n        if not df.empty:\n            st.dataframe(df)\n        else:\n            st.write(\"제공된 티커에 대한 데이터를 찾을 수 없습니다.\")\n    else:\n        st.write(\"주식 티커를 로드할 수 없습니다. API 설정 및 네트워크 연결을 확인해주세요.\")\n\nif __name__ == \"__main__\":\n    app()\n\n```\n\n암호화폐처럼 동일한 방식으로: 필요한 라이브러리 가져오기, financialmodelingprep 라이브러리에서 SP500 티커 가져오기 및 yfinance에서 데이터 가져오기: 각 주식별로 데이터를 가져오는 동안 limitation에 도달하는 것을 피하기 위해 이 두 작업을 서로 다른 소스 사이에 분리했습니다. financialmodelingprep의 최소 결제 요금제(매월 19.99 미국 달러)에는 분당 300회의 호출 제한이 있으므로 우리가 주식을 하나씩 가져올 때 쉽게 이 제한에 도달할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nyfinance에서 무엇을 얻고 있습니까? 적절한 기준을 설정하고 특정 주식이 성장 잠재력이 있는지 고려하는 데 도움이 되는 여러 가지 지표 목록을 얻고 있습니다.\n현재 가격: 주식의 최신 거래 가격입니다.\nEPS (주당 수익): 회사가 주당 주식에 대해 벌어들이는 돈을 나타냅니다.\n목표 P/E 비율: 이는 많은 가치 투자자들을 위한 전형적인 기준인 15로 설정됩니다. 여기서는 해당 주식의 이익에 기초하여 합리적인 가격으로 간주될 수 있는 것을 예상하기 위해 사용됩니다. 목표 P/E 15는 성장과 가치 속성을 균형 있게 고려할 수 있는 중도 기준으로 선택되었습니다. 브로드 산업 범위에 역사적으로 적용되었던 산업에 대해 사용된 보수적인 수치로, 오버밸류된 시장에서 상대적 가치 평가가 낮은 주식을 식별하는 데 도움을 줄 수 있습니다.\n공정시장가치 계산: EPS * 목표 P/E로 계산됩니다. 이는 주식이 목표 P/E 비율인 15로 가치 평가되었다면 해당 주식의 공정 가치를 나타냅니다. 낮은 P/E는 주식이 수익에 비해 저평가되었을 수 있다는 것을 시사할 수 있습니다.\n저평가 여부 확인: 만일 현재 시장 가격이 계산된 공정시장가치보다 낮다면 해당 주식이 저평가되었다고 간주됩니다.\n가격 격차(%): 공정시장가치와 현재 가격 사이의 백분율 차이를 보여주며, 주식 가격이 추정된 공정 가치에 도달하기 위해 얼마나 증가해야 하는지를 나타냅니다.\n\n![Financial Dashboard](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_3.png)\n\n우리는 4개 중 3개를 얻었습니다: 마지막은 ETF 분석 대시보드입니다.\n\n파일: pages/etfs_value.py\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nimport streamlit as st\nimport yfinance as yf\nimport pandas as pd\n\nst.set_page_config(layout=\"wide\")\n\n@st.cache_resource(ttl=300, show_spinner=True)\ndef fetch_options_data(symbol):\n    \"\"\" Yahoo Finance에서 ETF 심볼의 옵션 데이터를 가져옵니다. \"\"\"\n    etf = yf.Ticker(symbol)\n    try:\n        expiration_dates = etf.options\n        options_info = []\n        for expiration_date in expiration_dates:\n            options_chain = etf.option_chain(expiration_date)\n            puts = options_chain.puts\n            calls = options_chain.calls\n            options_info.append({\n                '만기일': expiration_date,\n                '풋 옵션 개수': len(puts),\n                '콜 옵션 개수': len(calls)\n            })\n        return options_info\n    except Exception as e:\n        st.error(f\"{symbol}에 대한 옵션 데이터를 가져올 수 없습니다: {e}\")\n        return []\n\ndef format_assets(assets):\n    \"\"\" 큰 숫자를 읽기 쉬운 형식으로 변환합니다. \"\"\"\n    if assets \u003e= 1e9:\n        return f\"{assets / 1e9:.2f}B\"\n    elif assets \u003e= 1e6:\n        return f\"{assets / 1e6:.2f}M\"\n    return str(assets)\n\n@st.cache_data(show_spinner=True)\ndef fetch_data(symbol):\n    \"\"\" Yahoo Finance에서 ETF에 대한 금융 데이터 및 메트릭을 가져옵니다. \"\"\"\n    etf = yf.Ticker(symbol)\n    info = etf.info\n    options_info = fetch_options_data(symbol)\n    return {\n        '이름': info.get('longName', 'N/A'),\n        '최신 가격': f\"${info.get('previousClose', 'N/A')}\",\n        '52주 최고가': f\"${info.get('fiftyTwoWeekHigh', 'N/A')}\",\n        '52주 최저가': f\"${info.get('fiftyTwoWeekLow', 'N/A')}\",\n        '1년 수익률': f\"{info.get('ytdReturn', 'N/A') * 100:.2f}%\" if info.get('ytdReturn') is not None else \"N/A\",\n        '3년 수익률': f\"{info.get('threeYearAverageReturn', 'N/A') * 100:.2f}%\" if info.get('threeYearAverageReturn') is not None else \"N/A\",\n        '5년 수익률': f\"{info.get('fiveYearAverageReturn', 'N/A') * 100:.2f}%\" if info.get('fiveYearAverageReturn') is not None else \"N/A\",\n        '총 자산': format_assets(info.get('totalAssets', 'N/A')),\n        '배당 수익률': f\"{info.get('yield', 'N/A') * 100:.2f}%\" if info.get('yield') is not None else \"N/A\",\n        '평균 거래량': info.get('averageVolume', 'N/A'),\n        '옵션 상세정보': \"; \".join([f\"만기일: {opt['만기일']}, 풋: {opt['풋 옵션 개수']}, 콜: {opt['콜 옵션 개수']}\" for opt in options_info]),\n    }\n\ndef app():\n    \"\"\" ETF 분석을 표시하는 Streamlit 애플리케이션의 진입점입니다. \"\"\"\n    st.title(\"ETF 분석\")\n    refresh_button = st.button(\"데이터 새로고침\")\n\n    if refresh_button:\n        st.experimental_rerun()\n\n    file_path = \"etfs.txt\"\n    try:\n        with open(file_path, 'r') as file:\n            symbols = [line.strip().upper() for line in file.readlines()]\n            data = [fetch_data(symbol) for symbol in symbols]\n            df = pd.DataFrame(data)\n            st.table(df)\n    except FileNotFoundError:\n        st.error(\"ETF 심볼 파일을 찾을 수 없습니다. 현재 디렉토리에 'etfs.txt' 파일이 있는지 확인해 주세요.\")\n\nif __name__ == \"__main__\":\n    app()\r\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터를 가져와서 서식을 지정한 후 Streamlit의 st.table() 함수를 사용하여 각 ETF의 주요 지표를 명확하고 조직적으로 보여줍니다. 이 테이블에는 최신 가격, 올해의 최고가와 최저가, 수익률, 총 자산, 배당 수익률 및 자세한 옵션 데이터와 같은 세부 정보가 포함되어 있습니다.\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_4.png)\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_5.png)\n\nETF의 사용 가능한 옵션 수가 다르기 때문에 테이블의 높이와 가시성에 영향을 줍니다. 그래서 이 대시보드의 스크린샷을 2개 두었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래요, 우리 최종 대시보드가 준비되었어요. 터미널을 열고 마법의 열쇠를 입력해볼까요? \"알라딘의 비밀 금고 여는 방법\"이 아니라 이런 모습을 하겠죠.\n\n```js\nstreamlit run streamlit_app.py\n```\n\n그리고, 와! 대시보드가 실행 중이에요.\n\n당신의 IDE 터미널에서 대시보드에 액세스하는 URL을 확인할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_6.png)\n\n로컬 URL 링크를 클릭하세요. 기본 브라우저에서 페이지를 열고 성취 결과를 확인할 수 있을 겁니다.\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_7.png)\n\n테이블에 대해 \"csv로 다운로드\", 검색 및 전체화면 옵션이 제공되었는지 확인하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_8.png\" /\u003e\n\n요약하자면 — 우리는 큰 한걸음을 내디디었어요: 어떤 데이터 표시 소스의 UI 한계에 국한되어 사용자일 뿐이었던 것으로부터, 이제 우리는 직접 대시보드를 개발할 수 있게 되었어요: 신뢰할 만한 정보 소스를 찾아내고, 원하는 형태로 정보를 제공받을 수 있도록 결정하고, 그에 맞게 조작할 수 있게 되었죠. 너무 복잡하지 않죠, 아마도 자기 계발과 금융 교육 여정에서 다음 목표에 도달하기 위한 단계에 또 다른 발걸음인 것 같아요. 코딩에 행운을 빕니다!\n\n# 쉽게 말해보면 🚀\n\nIn Plain English 커뮤니티의 일원이 되어 주셔서 감사합니다! 떠나시기 전에:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 글쓴이를 클립하고 팔로우 해주세요! 👏️️\n- 팔로우하기: X | LinkedIn | YouTube | Discord | 뉴스레터\n- 다른 플랫폼 방문하기: Stackademic | CoFeed | Venture | Cubed\n- PlainEnglish.io 에서 더 많은 콘텐츠를 확인하세요!","ogImage":{"url":"/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_0.png"},"coverImage":"/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_0.png","tag":["Tech"],"readingTime":15},{"title":"Z-점수를 이용해 연령대별 달리기 성능을 비교할 수 있을까요","description":"","date":"2024-06-22 16:58","slug":"2024-06-22-CanWeUseZ-ScorestoCompareRunningPerformancesBetweenAgeGroups","content":"\n\n![image](/assets/img/2024-06-22-CanWeUseZ-ScorestoCompareRunningPerformancesBetweenAgeGroups_0.png)\n\n다른 연령대 간 경주 결과를 효과적으로 비교하려면 어떻게 해야 할까요?\n\n그것이 제가 계속 탐구하고 있는 질문입니다.\n\n우리는 나이가 들수록 느리게 움직이게 됩니다. 어떤 사람들에게는 더 큰 영향을 미치지만 60세 남성이 25세 남성과 레벨한 경쟁을 할 수 없다는 것이 결론입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마스터 러너들을 위해 흥미로운 것을 유지하고 포용적인 러닝 커뮤니티를 유지하기 위해 많은 노력이 기울여졌습니다. 그것을 가능하게 하는 시스템인 연령 등급화라는 것을 개발하는 데 많은 노력이 기울여졌습니다. 최근 몇 주 동안, 몇 가지 대안을 탐색하기 위해 데이터를 사용해보았습니다.\n\n이전에 백분위수를 다른 비교 방법으로 제안했습니다. 이 주제에 대한 후속 기사는 여기에서 읽을 수 있습니다.\n\n잠재력이 있는 부분도 있지만 일부 단점도 있습니다. 그 주제로 나중에 돌아가서 그 중 몇 가지를 해결하고 상황을 개선해보겠습니다.\n\n그러나 오늘은 또 다른 비교 가능한 방법, 즉 Z-점수에 집중하고 싶습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 통계 도구가 다른 종류의 러너들의 경주 결과를 효과적으로 비교하는 데 도움이 될까요? 한번 살펴보고 알아봅시다.\n\n# Z-스코어란 무엇인가요?\n\n먼저, 기본적인 통계를 살펴봅시다. 개별 경주 결과의 z-스코어를 계산하기 전에 몇 가지 중요한 개념을 알 필요가 있습니다.\n\n샘플 경주 결과가 있다고 가정해 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n런너를 '전형적'이거나 '평균적'이라고 표현하는 다양한 방법이 있습니다. 우리는 평균을 사용할 것입니다. 평균을 계산하려면 모든 결과를 더한 다음 결과의 개수로 나누면 됩니다.\n\n우리 경우, 35세 미만의 남성들의 360,075개의 경주 결과를 포함한 샘플이 있습니다. 만약 수학을 해본다면 (저는 Python의 Pandas 패키지를 사용하여 컴퓨터에 일을 시켰습니다),이 그룹의 평균 완주 시간은 4:16:36입니다 (나중에 샘플에 대해 더 언급하겠습니다).\n\n그러나 이것은 이야기의 일부에 불과합니다. 많은 러너가 4:16 정도의 완주 시간을 기록하지만, 이 샘플에서 가장 빠른 러너는 2:03:45로 완주했습니다. 다른 사람들은 6, 7 또는 8시간이 걸렸습니다.\n\n분산과 편차라는 다른 기본 개념이 있습니다. 개별 결과가 평균에서 얼마나 벗어나 있는가? 결과들은 밀접하게 뭉쳐 있거나 퍼져 있나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수학적인 내용에는 들어가지 않고, 우리는 판다스를 사용하여 이 샘플의 표준 편차를 계산할 수 있습니다: 54:07. 이는 주어진 경주 결과와 평균 완주 시간 사이의 평균 거리를 나타냅니다.\n\nZ-점수는 특정 결과가 평균으로부터 얼마나 떨어져 있는지를 이해하는 표준화된 방법입니다 — 해당 샘플의 표준 편차를 기반으로 합니다.\n\n만약 한 러너가 평균(3:22:29)보다 54:07 빨리 완주했다면, 해당 결과의 z-점수는 -1일 것입니다 — 평균보다 한 표준 편차 떨어진 값입니다. 만약 한 러너가 두 배 빠르게(2:28:22) 완주했다면, 그것은 평균보다 두 배의 표준 편차 떨어진 값일 것입니다.\n\n위의 시각화는 이 개념을 설명해 줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n히스토그램은 360,075개의 개별 레이스 결과를 나타내며 각 5분 간격에 속하는 결과의 백분율을 보여줍니다 (즉, 3:55에서 4:00 사이).\n\n점선은 평균 값이 있는 곳 뿐만 아니라 평균 값의 한 표준 편차 위 및 아래, 그리고 두 표준 편차 위 및 아래를 나타냅니다.\n\n평균 값으로부터 멀어질수록 해당 시간을 뛰어넘은 참가자가 줄어든다는 것을 알 수 있습니다.\n\n이 개념을 다른 샘플, 예를 들어 다른 연령대에 적용해보면 실제 평균 값과 표준 편차가 달라질 수 있지만, 일반적인 원칙은 유지될 것입니다. 평균 값에서 두 표준 편차 떨어진 곳에서 레이스를 마친 선수는 한 표준 편차 떨어진 곳에서 레이스를 마친 선수보다 훨씬 적을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 잠재적으로 개별 결과가 평균 이하 얼마나 떨어져 있는지를 측정하여 \"좋은\" 결과를 비교하는 방법을 제공할 수 있습니다.\n\n# 각 연령 그룹의 평균과 표준 편차 계산\n\n첫 번째 단계는 많은 데이터를 수집하는 것입니다 — 이미 수행했습니다. 여기서 더 많은 내용을 읽을 수 있습니다. 하지만, 여기에 짧게 설명하겠습니다.\n\n이 분석을 용이하게 하기 위해, 샘플로 사용할 일련의 경주를 식별했습니다. 이는 2010년부터 2019년까지 미국에서 9월, 10월 또는 11월에 개최된 500명 이상의 완주자가 있는 모든 마라톤을 포함합니다. 그런 다음, 각 완주자의 성별, 연령 및 완주 시간을 수집했는데 — 총 2,017,493개의 결과가 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n몇 가지 정리와 준비 작업을 한 후, 이 결과를 Pandas 데이터프레임에 로드하여 쉬운 분석이 가능하게 했어요. 이 시리즈를 마치면 Kaggle에 완전한 데이터셋을 공유할 예정이에요. 만약 여러분이 자체적으로 분석을 하고 싶다면 이 데이터셋을 사용해보세요.\n\nPandas로 결과를 처리하면 성별 및 연령대로 결과를 그룹화하고 각 그룹의 평균과 표준 편차를 계산하는 것이 상당히 간단해집니다. 참고로, 나는 Boston Marathon 참가 자격을 얻기 위해 BAA가 사용하는 연령 구간을 사용하고 있어요. 그리고 샘플이 너무 작기 때문에 80세 이상의 러너는 포함하지 않았어요.\n\n위의 시각화는 각 연령대별 평균 완주 시간을 나타냅니다. 빨간 점은 남성이고, 초록 점은 여성입니다.\n\n여기서 놀라운 점은 없어요. 오른쪽으로 이동할수록 평균 완주 시간이 더 느려집니다. 어린 러너들 사이에는 큰 차이가 없지만, 50대와 60대로 갈수록 그 차이가 뚜렷해집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n평균적으로, 동일 연령대의 남성보다 여성이 느린 속도로 완주합니다. 그러나 연령에 따른 추세는 여성과 남성 모두 유사합니다.\n\n위의 시각화 자료는 각 그룹의 표준 편차를 보여줍니다.\n\n이건 정확히 내가 예상한 것과는 조금 다르네요 — 그리고 이는 문제가 될 수도 있습니다.\n\n각 그룹 사이의 표준 편차는 거의 비슷합니다. 노인 남성을 제외하고, 모든 그룹의 표준 편차는 50분에서 55분 사이에 있습니다. 세 가지 노인 남성 연령대 중에서는 약간 더 높습니다(55분에서 60분까지).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 정확히 무엇을 기대했는지 확신할 수 없지만, 이것이 어떤 방식으로든 평균과 함께 조정될 것으로 생각했습니다. 아래에서 어떻게 진행되는지 확인해볼게요. 그러나 한 그룹이 다른 그룹보다 더 극단적인 z-점수를 가지거나 다른 그룹과 비교할 때 과대평가될 수도 있다는 느낌이 듭니다.\n\n# 개별 결과에 정규화된 Z-점수 적용하기\n\n각 그룹에 대한 평균과 표준 편차를 계산한 후, 결과의 일부(2019)를 취하여 각 결과에 대한 z-점수를 계산했습니다.\n\n실제 실습 중에 어떻게 작용하는지 몇 가지 레이스를 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 미네아폴리스의 트윈시티 마라톤을 시작으로 하겠습니다. 여기 주요 10명의 완주자들을 z-점수로 정리해 봤어요. 비교를 위해 2020년 연령 등급 표에 따른 연령 등급 점수도 포함했어요.\n\n```js\n| 성별    | 나이  | 완주 시간 | z-점수 | 연령 등급 점수 |\n|---------|------|----------|---------|--------------|\n| F       | 40   | 02:34:07 | -2.55  | 89.12        |\n| F       | 27   | 02:31:29 | -2.54  | 88.5         |\n| F       | 24   | 02:32:49 | -2.51  | 87.73        |\n| F       | 30   | 02:35:50 | -2.45  | 86.03        |\n| F       | 33   | 02:36:34 | -2.44  | 85.69        |\n| F       | 31   | 02:38:46 | -2.4   | 84.44        |\n| F       | 26   | 02:40:08 | -2.37  | 83.72        |\n| F       | 37   | 02:40:24 | -2.37  | 84.42        |\n| F       | 30   | 02:40:13 | -2.37  | 83.68        |\n| F       | 29   | 02:41:13 | -2.35  | 83.16        |\n```\n\n여기서 눈에 띄는 것은 이 10명의 완주자가 모두 여성이라는 점이에요. 그들은 높은 연령 등급을 가지고 있으며 훌륭한 완주 시간을 보여주지만... 비슷한 시간을 보이는 남성이 전혀 없다는 것이 납득하기 어렵네요.\n\n두 번째 예로, 작은 규모의 대회인 뉴저지의 애틀랜틱 시티 마라톤을 살펴보겠습니다 (그리고 이 마라톤이 거친 첫 번째 마라톤입니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n| Gender | Age | Finish  | zScore | Age Grade |\r\n|--------|-----|---------|--------|-----------|\r\n| F      | 28  | 02:42:48| -2.32  | 82.35     |\r\n| M      | 35  | 02:19:15| -2.27  | 87.84     |\r\n| M      | 32  | 02:21:46| -2.13  | 85.83     |\r\n| M      | 61  | 03:00:04| -2.06  | 83.75     |\r\n| F      | 41  | 03:07:27| -1.92  | 73.72     |\r\n| M      | 34  | 02:33:10| -1.92  | 79.66     |\r\n| M      | 56  | 02:58:22| -1.91  | 80.66     |\r\n| M      | 56  | 03:01:43| -1.84  | 79.17     |\r\n| F      | 41  | 03:12:30| -1.83  | 71.79     |\r\n| F      | 24  | 03:11:47| -1.76  | 69.91     |\r\n\n\r\n여기에는 결과에서 남성과 여성이 섞여 있습니다. 그러나 상위 두 결과를 살펴보세요.\r\n\r\n최고의 결과는 z-점수가 -2.32인 28세 여성으로, 2시간 42분을 달렸습니다. 이는 훌륭한 시간입니다 (그녀는 첫 번째 여성 완주자이자 총 완주자 중 네 번째이지만, 이것이 2:19를 달린 35세 남성보다 더 나은 결과일까요?\r\n\r\n여기 마지막 예시 — 버지니아주 리치먼드 마라톤을 살펴보겠습니다.\r\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n| Gender   |   Age | Finish   |   zScore |   Age Grade |\n|----------|-------|----------|----------|-------------|\n| F        |    23 | 02:36:19 |    -2.44 |       85.77 |\n| F        |    30 | 02:36:30 |    -2.44 |       85.67 |\n| F        |    28 | 02:40:08 |    -2.37 |       83.72 |\n| F        |    29 | 02:43:31 |    -2.3  |       81.99 |\n| F        |    23 | 02:47:03 |    -2.24 |       80.26 |\n| F        |    36 | 02:47:54 |    -2.23 |       80.38 |\n| F        |    35 | 02:48:45 |    -2.21 |       79.76 |\n| F        |    26 | 02:49:08 |    -2.2  |       79.27 |\n| F        |    28 | 02:49:29 |    -2.19 |       79.1  |\n| F        |    28 | 02:50:19 |    -2.17 |       78.72 |\n\n\n여기서 다시 한번 말해요, 상위 10명의 완주자는 모두 여성입니다. 상위 두 명은 (2:36) 꽤 인상적하지만, 여전히 이 목록에 들어갈만한 남성이 없다는 것은 믿기 어렵습니다.\n\n이 여성들은 모두 젊으며, 35~39세 그룹을 넘는 사람은 한 명도 없습니다.\n\n조금 더 자세히 살펴보기 위해, 리치먼드 마라톤에서 z-점수로 상위 열 명의 남성입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n| 성별 | 나이 | 완주 시간 | z-점수 | 나이 등급 |\n|------|------|-----------|--------|-----------|\n| M    | 31   | 02:19:43  | -2.17  | 87.07     |\n| M    | 25   | 02:20:54  | -2.15  | 86.34     |\n| M    | 30   | 02:21:34  | -2.14  | 85.93     |\n| M    | 24   | 02:22:00  | -2.13  | 85.67     |\n| M    | 35   | 02:27:14  | -2.11  | 83.08     |\n| M    | 25   | 02:24:14  | -2.09  | 84.34     |\n| M    | 55   | 02:48:50  | -2.08  | 84.45     |\n| M    | 22   | 02:25:32  | -2.06  | 83.59     |\n| M    | 26   | 02:26:26  | -2.05  | 83.08     |\n| M    | 42   | 02:36:17  | -2.01  | 81.55     |\n``` \n\n그래서 몇몇 정말 우수한 남성분들이 있었어요. 그렇지만 31살 남성분이 2시간 19분만에 완주했는데도 z-점수가 -2.17밖에 되지 않았어요. 우연히도, 그게 2시간 50분에 완주한 여성 10위보다 뒤에 오네요.\n\n여기에는 조금 더 다양한 연령대가 있어요(55살과 42살), 하지만 이 완주자들의 대부분은 35세 미만의 남성분들이에요. \n\n# z-점수를 사용하는 데 문제점\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 문제의 가장 분명한 문제는 이 측정 방법이 여성의 결과를 과대평가하기 쉽다는 것입니다 — 특히 젊은 여성의 결과를 말이죠. 큰 규모의 두 마라톤에서 전체 상위 열 명 리스트가 모두 여성으로 차지되는 경우가 있었습니다.\n\n이게 왜 발생하는 걸까요?\n\n각 연령 그룹별 평균과 표준 편차를 생각해 보십시오.\n\n35세 미만 여성의 평균 완주 시간은 4시간 43분 20초입니다. 이는 동일 연령대 남성보다 27분 늦고, 55-59세 이상의 모든 남성 연령 그룹보다도 느립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n동시에, 그들의 표준 편차(51:59)는 가장 낮은 편 중 하나입니다. 동일 연령의 남성들보다 약 두 분 더 낮습니다.\n\n이것들이 결합되면, 최고의 여성들이 평균보다 훨씬 낮은 점수를 얻을 수 있는 여지가 많아지며, 다른 연령 그룹에 비해 낮은 z-점수를 달성할 수 있습니다.\n\n해당 시점(2019년)에서, 마라톤 남성들의 세계 신기록은 2:01:39로, (2018년 베를린에서 일리우드 킵초게가 달성했습니다). 이는 -2.51의 z-점수를 달성할 것입니다.\n\n한편, 그때의 여성 세계 기록은 2:14:04로, (2019년 시카고에서 브리짓 코스게이가 달성했습니다). 이는 -2.86의 z-점수를 달성할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 시스템에서는 35세 미만의 여성에게 큰 내부적 이점이 있습니다. 2:30으로 달리는 여성이라도 Kipchoge의 세계 기록 아래에있는 z-점수(-2.55)를 달성할 수 있습니다.\n\n이 시각화는 각 연령 그룹의 달리기자 중 얼마나 많은 비율이 z-점수가 -2(파란색) 미만 또는 -1과 -2 사이 (보라색)인지 보여줍니다.\n\n그룹이 여전히 작지만, 35세 미만의 여성 중 2(1.27%) 미만의 점수를 받은 여성이 남성(0.79%)보다 더 많습니다.\n\n35~39세에는 -2 이하의 점수를 받은 여성(0.89%)이 남성(0.38%)보다 약 두 배 더 많습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그런 노릇하며 60대와 70대 러너 사이에서도 재밌는 일들이 벌어지고 있어요. 하지만 이 그룹들이 너무 작아서 개별 레이스 결과에 명확히 나타나지 않을 수도 있어요.\n\n# 그래서 Z-점수는 레이스 결과를 이해하는 데 유용할까요?\n\n만약 연령 등급을 완전한 대체품으로 찾고 있다면, 그렇지 않다고 말할 거예요.\n\n이것은 불균형적이고, 일부 그룹이 다른 그룹보다 우위를 가지고 있다는 것이 분명해요. 젊은 여성들 사이에서 높은 평균 완주 시간은 그들이 그 평균 아래서 끝내고 낮은 Z-점수를 받을 공간이 훨씬 더 많다는 것을 명백히 보여줘요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n개념적으로, 일반적인 비교와 판단을 하려면 유용한 것 같아요. 어떤 러너가 평균값보다 표준 편차 하나나 둘 미만인지 아는 것은 그들이 얼마나 뛰어난지와 그 결과가 얼마나 어려운지에 대한 일반적인 감각을 제공해줍니다.\n\n하지만 이것을 극단적인 경우에 더 나은 비교를 하기 위해 보정하는 좋은 방법은 없다고 생각해요. \n\n나이 등급은 보정에 문제가 있을 수 있지만, 이 시스템을 개선된 것으로 보지는 않아요.\n\n어쩌면 다른 사람이 이 데이터를 가지고 더 나은 결과를 얻을 수 있을지도 모르겠지만, 저는 일단 지금은 이걸 잠시 내려놓고 리스트에서 지우려고 해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 그럼 다음은 무엇인가요?\n\n지금까지 우리는 두 가지 대안을 살펴봤어요 — 백분위수와 z-점수.\n\n앞으로 나아가서, 이 두 가지 대안이 나이 등급과 어떻게 비교되는지 다시 한 번 살펴보고 싶어요. 또한 여러분이 직접 결과를 측정하고 비교할 수 있는 계산기를 만들어 공유하고 싶어요.\n\n그 다음으로, 2023년 데이터로 업데이트하고, 백분위수 시스템을 약간 수정하여 2023년 최신 나이 등급 테이블과 비교하고 싶어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 시리즈를 마무리 짓는 것이 좋을 것 같아요. 만약 여러분이 자신의 분석을 원하신다면, 전체 데이터셋을 Kaggle에 공유할 예정이니까 참고해주세요.\n\n그 중에 하나라도 관심이 있다면, 다음 몇 개의 글을 받아보시려면 이메일 업데이트 구독을 확실하게 해주세요. 저는 다음 2~3주 안에 글을 올릴 예정이에요.\n\n만약 이 분석을 돕는 피드백이나 아이디어가 있다면, 응답을 남겨주세요. 항상 두 번째(또는 세 번째, 네 번째) 의견을 가지고 있으면 도움이 되죠!\n\n저는 열정적인 달리기 사랑자이자 데이터 좋아하는 사람입니다. 이제 40살이 되었는데, 연령대별 결과를 비교하는 것이 특히 저에게 흥미로운 분야에요. 제 활동을 계속 지켜보려면 아래와 같이 따라갈 수 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 제 훈련에 대해 알아보려면 Running with Rock을 팔로우하세요\n- 마라톤 훈련 계획을 선택하는 팁을 읽어보세요\n- Strava에서 제를 스토킹하세요","ogImage":{"url":"/assets/img/2024-06-22-CanWeUseZ-ScorestoCompareRunningPerformancesBetweenAgeGroups_0.png"},"coverImage":"/assets/img/2024-06-22-CanWeUseZ-ScorestoCompareRunningPerformancesBetweenAgeGroups_0.png","tag":["Tech"],"readingTime":10},{"title":"파이썬을 사용한 탐색적 데이터 분석 EDA 방법","description":"","date":"2024-06-22 16:56","slug":"2024-06-22-ExploratoryDataAnalysisEDAUsingPython","content":"\n\n## Python에서 탐색적 데이터 분석 및 데이터 시각화에 대한 기본 예제\n\n![image](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_0.png)\n\n# 소개\n\n탐색적 데이터 분석(EDA)는 데이터 분석 과정에서 중요한 단계로, 데이터셋을 더 잘 이해하기 위한 작업입니다. EDA를 통해 데이터의 주요 특징, 변수 간의 관계, 문제에 관련이 있는 변수들을 이해할 수 있습니다. 또한 EDA를 통해 데이터에서 누락된 값, 중복된 값, 이상값 및 오류를 식별하고 처리할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 게시물에서는 Pandas, Numpy, Matplotlib 및 Seaborn과 같은 다양한 라이브러리를 사용하여 EDA(탐색적 데이터 분석)를 수행할 때 Python을 사용합니다. Pandas는 데이터 조작 및 분석을 위한 라이브러리입니다. Numpy는 숫자 계산을 위한 라이브러리입니다. Matplotlib 및 Seaborn은 데이터 시각화를 위한 라이브러리입니다.\n\n이 프로젝트에서는 2023년 Goodreads Choice Awards 베스트 북을 분석할 것입니다.\n\n![image](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_1.png)\n\n분석을 수행하고 연습을 시작하려면 Kaggle에서 이 데이터셋을 찾을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터 이해하기\n\n## 필요한 라이브러리 가져오기\n\n```js\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\n## 데이터셋을 판다스 데이터프레임에 불러오기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\n# CSV 파일에 데이터가 저장되어 있으므로, Pandas 함수를 사용하여 CSV 파일을 읽을 것입니다.\ndf = pd.read_csv('Good_Reads_Book_Awards_Crawl_2023_12_27_11_14.csv')\n\ndf.sample(5) # 데이터 샘플을 표시합니다\n\n```\n\n![image](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_2.png)\n\n다음으로 데이터셋에서 일부 불필요한 열을 제거할 것이며, 이 단계는 선택 사항입니다. 사용하지 않을 열을 제거하여 DataFrame의 크기를 줄이는 것이 좋습니다.\n\n```python\n# 사용하지 않을 열은 source_URL, Book Description, About the Author입니다\ndf.drop(['source_URL','Book Description','About the Author'], axis=1, inplace=True)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 데이터프레임 확인하기\n\n이제 각 열의 데이터 유형을 확인하고 숫자 열의 요약을 확인하여 다음 단계를 결정할 수 있습니다.\n\n```js\ndf.info()\n```\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n.info()를 통해 데이터 세트는 누락된 값이 없는 상태로 괜찮아 보입니다. 또한 데이터 세트의 형태(컬럼 수: 12, 행 수: 299) 및 각 컬럼의 데이터 유형과 같은 정보를 제공해줍니다.\n\n```js\ndf.describe()\n```\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_4.png)\n\n.describe() 메서드는 DataFrame의 숫자형 열에 대한 요약 통계를 제공합니다. 각 숫자형 열의 평균값, 중간값, 표준 편차, 최솟값 및 최댓값을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Int/Float 크기를 축소하고 데이터 유형 할당하기\n\n데이터의 외관과 숫자 구성을 식별했다면, 데이터 분석의 후속 단계를 결정할 수 있습니다. .info()로부터 데이터의 크기인 28.2 KB와 각 열의 데이터 유형을 알 수 있습니다. .describe() 메서드는 각 열의 최솟값과 최댓값, 그리고 평균값과 같은 숫자 열의 통계를 보여줍니다.\n\n이 결과를 통해 Number of Ratings와 Number of Reviews와 같이 여전히 숫자 열이어야 하는 열이 누락되어 있는 것을 알 수 있습니다. 이러한 열들은 천 단위 구분자로 쉼표 “,”를 사용합니다. Readers Choice Votes와 같이 일반 서식으로 저장된 열은 천 단위 구분자가 없습니다. 숫자를 쉼표로 구분하지 않고 일반 숫자를 사용하는 이유는 식별자일 수도 있고 여러 자리 숫자일 수도 있어 숫자를 분리하는 것이 적절하지 않을 수 있기 때문입니다. 따라서 쉼표를 제거하고 공백으로 대체해야 합니다.\n\n```js\nnumeric_columns = ['Number of Ratings', 'Number of Reviews']\n\n# 해당 열에서 캐릭터 쉼표를 제거하고 Int32로 변환\nfor column in numeric_columns:\n    df[column] = df[column].replace(',', '', regex=True).astype('int32')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 방법은 위의 열에서 모든 쉼표를 제거할 것입니다. .astype(`int32`) 이 부분은 남겨두셔도 좋습니다. 왜냐하면 판다스가 자동으로 데이터 유형을 int64로 할당해줄 것이기 때문입니다. 여기서 64와 32는 각 행당 메모리 양이나 비트 수를 나타냅니다.\n\n그대로 둘 수 있지만, 데이터프레임을 더 효율적으로 만들기 위해 이러한 값을 낮은 값으로 다운캐스팅하고 있습니다. 아래 숫자형 열의 범위를 살펴보면:\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_5.png)\n\n각 숫자형 열의 가장 작은 값과 가장 큰 값을 보여줍니다. 예를 들어 'Readers Choice Votes' 열에서, 가장 작은 값은 935이고 가장 큰 값은 397565입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n값의 범위를 알면 해당 값을 저장하는 데 필요한 비트를 결정할 수 있습니다. 참고로:\n\n- Int8 변수는 -128에서 127까지의 값을 저장할 수 있습니다.\n- Int16 변수는 -32,768에서 32,767까지의 값을 저장할 수 있습니다.\n- Int32 변수는 -2,147,483,648에서 2,147,483,647까지의 값을 저장할 수 있습니다.\n- Int64 변수는 -9,223,372,036,854,775,808에서 9,223,372,036,854,775,807까지의 값을 저장할 수 있습니다.\n\nInt32가 값 범위에 딱 맞는 옵션입니다. Int64를 사용할 수도 있지만 더 많은 메모리를 사용하고 DataFrame을 덜 효율적으로 만든다는 점에서 적절하지 않습니다.\n\n실수의 경우에는 약간 다릅니다. 실수는 데이터가 저장할 수 있는 소수 자릿수에 실제로 영향을 줍니다. Float16는 4자리 소수를 저장하고, Float32는 8자리 소수를 저장하며, Float64는 16자리 소수를 저장할 수 있습니다. DataFrame에서 많은 소수 자릿수를 사용할 필요는 없지만 원래 값과 동일하게 유지하고 싶은 경우 Float16을 사용하는 것이 가장 좋은 선택입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 각 열의 값을 범위로 알고 있으므로 해당 열을 올바른 데이터 유형으로 할당할 것입니다.\n\n또한 일부 열은 텍스트 값을 저장합니다. 해당 열의 데이터 유형을 문자열 또는 범주로 할당할 수 있습니다. 판다스 문서에 따르면 범주형 데이터 유형은 소수의 다른 값으로 구성된 문자열 변수(예: 성별, 사회 계급, 혈액형, 국가 소속 등)에 유용합니다. 이 정의에 따라 'Column Readers Choice Category'가 범주형 데이터 유형을 사용하는 가장 적합한 선택입니다.\n\n```js\n# 나머지 열을 올바른 데이터 유형으로 변환합니다.\nconvert_dict = {'Readers Choice Votes': 'int32',\n                'Readers Choice Category': 'category',\n                'Title': 'string',\n                'Author': 'string',\n                'Total Avg Rating': 'float16',\n                'Number of Pages': 'int16',\n                'Edition': 'category',\n                'First Published date': 'datetime64[ns]',\n                'Kindle Price': 'float16'}\ndf = df.astype(convert_dict)\n```\n\n'Kindle Version and Price' 열의 경우 이미 가격 데이터를 저장하는 'Kindle Price'라는 다른 열이 있으므로 가격을 제거할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```jsx\n#텍스트에서 통화를 분리하여 새 열에 넣습니다\ndf['Kindle Version'] = df['Kindle Version and Price'].str.extract('([a-zA-Z ]+)', expand=False).str.strip()\n\n#열을 올바른 데이터 유형으로 변경합니다\ndf['Kindle Version'] = df['Kindle Version'].astype('category')\n\n#이전 열을 제거합니다\ndf = df.drop('Kindle Version and Price', axis=1)\n```\n\n이제 데이터프레임을 다시 확인해보겠습니다:\n\n```jsx\ndf.info()\n```\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndf.describe()\n```\n\n![Exploratory Data Analysis using Python](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_7.png)\n\n```js\ndf.sample(10)\n```\n\n![Exploratory Data Analysis using Python](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_8.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최근 결과에서 보듯이 데이터프레임의 메모리 크기를 크게 줄일 수 있습니다. 데이터 유형을 변경하고 정수 및 부동 소수점을 다운캐스팅하여 메모리 크기를 줄이는 방법입니다. 이 단계는 선택 사항이지만 대규모 데이터셋을 다룰 때 매우 유용할 수 있습니다.\n\n# 데이터 분석 및 시각화\n\n## 카테고리 분포\n\n첫 번째 분석은 데이터셋 내의 다양한 카테고리별 책 분포를 찾는 것입니다. 그런 다음 seaborn 모듈을 사용하여 시각화할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ncat_counts = df['독자 선호 카테고리'].value_counts()\nprint(cat_counts)\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=cat_counts.index, y=cat_counts.values, palette='Blues_d')\nplt.title('카테고리별 분포')\nplt.xlabel('카테고리')\nplt.ylabel('책 수')\nplt.xticks(rotation=30, ha='right')\nplt.show()\n```\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_9.png\" /\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_10.png\" /\u003e\n\n우리 데이터는 모든 카테고리에 고르게 분포되어 있습니다. 다만, 데뷔 소설 카테고리만 19권의 책이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 각 카테고리별 투표, 평점, 리뷰, 페이지 및 가격 분포를 분석할 것입니다. 이 분포를 시각화하기 위해 상자 그림을 사용할 것입니다.\n\n```js\nfig, axes = plt.subplots(3, 2, figsize=(16, 18), sharey=False, sharex=True)\n\n# 첫 번째 플롯: 독자 선호 투표 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Readers Choice Votes', palette='Set3', ax=axes[0, 0])\naxes[0, 0].set_title('각 카테고리별 독자 선호 투표 분포')\naxes[0, 0].set_ylabel('투표')\n\n# 두 번째 플롯: 평균 평점 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Total Avg Rating', palette='Set3', ax=axes[0, 1])\naxes[0, 1].set_title('각 카테고리별 평균 평점 분포')\naxes[0, 1].set_ylabel('평균 평점')\n\n# 세 번째 플롯: 평가 수 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Number of Ratings', palette='Set3', ax=axes[1, 0])\naxes[1, 0].set_title('각 카테고리별 평가 수 분포')\naxes[1, 0].set_ylabel('평가 수')\n\n# 네 번째 플롯: 리뷰 수 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Number of Reviews', palette='Set3', ax=axes[1, 1])\naxes[1, 1].set_title('각 카테고리별 리뷰 수 분포')\naxes[1, 1].set_ylabel('리뷰 수')\n\n# 다섯 번째 플롯: 페이지 수 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Number of Pages', palette='Set3', ax=axes[2, 0])\naxes[2, 0].set_title('각 카테고리별 페이지 수 분포')\naxes[2, 0].set_ylabel('페이지 수')\n\n# 여섯 번째 플롯: 킨들 가격 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Kindle Price', palette='Set3', ax=axes[2, 1])\naxes[2, 1].set_title('각 카테고리별 킨들 가격 분포')\naxes[2, 1].set_ylabel('킨들 가격')\n\nfor ax in axes[2, :]:\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha='right')\n    \nfig.tight_layout()\nplt.show()\n```\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_11.png\" /\u003e\n\n대부분의 분포가 편향되어 있으며, 몇몇 카테고리에서 극단값이 있습니다. 평균 평점 분포는 데이터가 정규 분포되어 있습니다. 편향된 데이터의 중심 경향성을 측정하는 가장 좋은 방법은 중앙값을 사용하는 것입니다. 중앙값은 극단값(이상치)에 민감하지 않기 때문에 적합합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 카테고리별 분석\n\n이제 투표, 평점, 리뷰, 페이지 및 가격별로 카테고리를 조사하여 2023년 가장 인기 있는 카테고리를 찾아보겠습니다.\n\n```js\n# 집계할 열을 결정합니다.\naggregations = {'Readers Choice Votes': 'sum', \n                'Total Avg Rating': 'mean',\n               'Number of Ratings': 'sum',\n               'Number of Reviews': 'sum',\n                'Number of Pages': 'median',\n                'Kindle Price': 'median',\n               }\n\n# 책 분야별로 그룹화합니다.\ncategory_vote = df.groupby('Readers Choice Category').agg(aggregations).sort_values('Readers Choice Votes', ascending=False)\n\n# 각 카테고리의 총 투표, 총 평점 및 총 리뷰의 백분율을 계산합니다.\ntotal_votes = category_vote['Readers Choice Votes'].sum()\ntotal_ratings = category_vote['Number of Ratings'].sum()\ntotal_reviews = category_vote['Number of Reviews'].sum()\npercent_of_total_votes = (category_vote['Readers Choice Votes'] / total_votes) * 100\npercent_of_total_ratings = (category_vote['Number of Ratings'] / total_ratings) * 100\npercent_of_total_reviews = (category_vote['Number of Reviews'] / total_reviews) * 100\n\n# 새로운 Votes, Ratings 및 Reviews의 데이터프레임을 생성합니다.\nresult_df = pd.DataFrame({\n    'Votes (합산)': category_vote['Readers Choice Votes'], \n    '% 투표': percent_of_total_votes, \n    '평균 평점': category_vote['Total Avg Rating'].round(2),\n    'Number of Ratings': category_vote['Number of Ratings'],\n    '% 총 평점': percent_of_total_ratings.round(2),\n    'Number of Reviews': category_vote['Number of Reviews'],\n    '% 총 리뷰': percent_of_total_reviews.round(2),\n    'Median Pages': category_vote['Number of Pages'],\n    'Median Kindle Price': category_vote['Kindle Price'].round(2)\n    })\n\n# 가장 많이 투표된 카테고리 찾기\nmax_voted_cat = result_df['Votes (합산)'].idxmax()\nmax_votes = result_df['Votes (합산)'].max()\navg_rat = result_df.loc[max_voted_cat, '평균 평점']\n\n# 가장 많이 평가된 카테고리 찾기\nmax_rated_cat = result_df['Number of Ratings'].idxmax()\nmax_rates = result_df['Number of Ratings'].max()\npct_max_rates = result_df['% 총 평점'].max()\n\n# 가장 많이 리뷰된 카테고리 찾기\nmax_reviewed_cat = result_df['Number of Reviews'].idxmax()\nmax_reviews = result_df['Number of Reviews'].max()\npct_max_reviews = result_df['% 총 리뷰'].max()\n\n# 결과 출력\nprint(f\"'{max_voted_cat}' 카테고리가 2023년 가장 많이 투표된 카테고리로 선정되었습니다. {max_votes:,}표를 획득했습니다.\")\nprint(f\"'{max_rated_cat}' 카테고리가 2023년 가장 많이 평가된 카테고리로 선정되었습니다. 평균 평점은 {format(avg_rat, '.2f')}이며, 평가 횟수는 {max_rates:,}회이며, 전체 평점의 {format(pct_max_rates, '.2f')}%를 차지하고 있습니다.\")\nprint(f\"'{max_reviewed_cat}' 카테고리가 2023년 가장 많이 리뷰된 카테고리로 선정되었습니다. 리뷰 수는 {max_reviews:,}개이며, 전체 리뷰의 {format(pct_max_reviews, '.2f')}%를 차지하고 있습니다.\")\n\nresult_df\n```\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_12.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로는 데이터를 시각화하여 더 나은 이해와 시각화를 하려고 합니다.\n\n```js\nfig, axes = plt.subplots(3, 2, figsize=(16, 18), sharey=False)\n\n# 첫 번째 그래프\nsns.barplot(x=result_df.index, y=result_df['Votes (sum)'], palette='Blues_d', order=result_df.index, ax=axes[0, 0])\naxes[0, 0].set_title('각 카테고리별 독자 투표수')\naxes[0, 0].set_ylabel('투표수')\naxes[0, 0].set_xticklabels(labels=result_df.index, rotation=30, ha='right')\n\n# 두 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Avg Ratings', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Avg Ratings'], palette='Blues_d', order=result_df_sorted.index, ax=axes[0, 1])\naxes[0, 1].set_title('각 카테고리별 평균 평점')\naxes[0, 1].set_ylabel('평균 평점')\naxes[0, 1].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\n# 세 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Number of Ratings', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Number of Ratings'], palette='Blues_d', order=result_df_sorted.index, ax=axes[1, 0])\naxes[1, 0].set_title('각 카테고리별 평점 수')\naxes[1, 0].set_ylabel('평점 수')\naxes[1, 0].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\n# 네 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Number of Reviews', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Number of Reviews'], palette='Blues_d', order=result_df_sorted.index, ax=axes[1, 1])\naxes[1, 1].set_title('각 카테고리별 리뷰 수')\naxes[1, 1].set_ylabel('리뷰 수')\naxes[1, 1].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\n# 다섯 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Median Pages', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Median Pages'], palette='Blues_d', order=result_df_sorted.index, ax=axes[2, 0])\naxes[2, 0].set_title('각 카테고리별 평균 페이지 수')\naxes[2, 0].set_ylabel('페이지 수')\naxes[2, 0].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\n# 여섯 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Median Kindle Price', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Median Kindle Price'], palette='Blues_d', order=result_df_sorted.index, ax=axes[2, 1])\naxes[2, 1].set_title('각 카테고리별 평균 킨들 가격')\naxes[2, 1].set_ylabel('킨들 가격 ($)')\naxes[2, 1].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\nplt.tight_layout()\nplt.show()\n```\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_13.png\" /\u003e\n\n그럼 여기까지입니다. 평균 평점이 가장 높지 않은에도 불구하고, Romance이 2023년에 가장 인기 있는 책 카테고리로 선정되었습니다. 다른 카테고리보다 투표, 평점 및 리뷰에서 우위를 차지했습니다. Ratings와 Reviews에서 두 번째로 높은 등수를 차지하며 숫자가 두 배 더 많습니다. 한편, Humor와 History \u0026 Biography는 2023년에 가장 인기 없는 두 가지 책 카테고리로 순위했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 카테고리의 가격은 매우 유사한 편이지만, Romance과 Romantasy는 모든 카테고리 중에서 가장 낮은 중간 가격을 가지고 있습니다. 이에도 불구하고 투표 수, 평점 및 리뷰 수가 많습니다.\n\n## 상관 관계 찾기\n\n이제 질문이 등장합니다. 투표 수, 리뷰, 평점 또는 페이지 수와 가격 사이에 어떤 상관 관계가 있는지 알아볼까요? 페이지 수가 많으면 평점이 높을까요? 아니면 낮은 가격 카테고리일수록 더 많은 리뷰와 높은 평점을 받게 될까요? 알아보도록 하겠습니다.\n\n```js\n# 열 할당\ncolumns_of_interest = ['리뷰 수', '평가 수', '페이지 수', '총평균 평점', '최다선정 표', '킨들 가격']\n\n# 상관 행렬 계산\ncorrelation_matrix = df[columns_of_interest].corr()\n\n# 상관 행렬 표시\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\nplt.title('상관 행렬')\nplt.xticks(rotation=30, ha='right')\nplt.show()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_14.png\" /\u003e\n\n이 행렬을 통해 독자 선호투표와 리뷰 수와 평가 수 간에 높은 상관 관계가 있다는 것을 알 수 있습니다. 높은 리뷰와 평점은 높은 투표와 관련이 있습니다. 페이지 수와 가격은 투표, 평점, 리뷰 사이에 강한 연결이 없습니다. 즉, 책의 가격과 두께가 투표, 평점, 리뷰에 실제로 영향을 미치지 않는다는 것을 의미합니다.\n\n## 책으로 분석하기\n\n2023년 가장 투표를 많이 받은 책이 무엇인지 확인해 보는 시간입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nmost_voted_books = df[['Title', 'Readers Choice Category', 'Readers Choice Votes', 'Total Avg Rating', 'Number of Ratings', 'Number of Reviews', 'Number of Pages']].sort_values(by=['Readers Choice Votes', 'Number of Ratings', 'Number of Reviews'], ascending=False).head(20)\n\nplt.figure(figsize=(14, 6))\nsns.barplot(x=most_voted_books['Title'], y=most_voted_books['Readers Choice Votes'], data=most_voted_books, palette='Blues_d')\nplt.title('2023년 가장 많이 투표된 책')\nplt.xlabel('책 제목')\nplt.ylabel('투표 수')\nplt.xticks(rotation=30, ha='right')\nplt.show()\n\nmost_voted_books\n```\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_15.png\" /\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_16.png\" /\u003e\n\n그래서 이겨낸 책이 결정되었습니다. Fourth Wing은 2023 독자 선호 투표에서 가장 인기 있는 책으로 우승을 차지했습니다. 2위인 Yellowface보다 거의 2배 많은 표를 받아 거의 백만 개의 평가를 받았습니다. 로맨티지 카테고리에 해당하는 투표 중 절반 이상을 차지했어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n이제 모든 카테고리에서 수상자들을 살펴보겠습니다.\n\n```js\nmax_votes_index = df.groupby('Readers Choice Category')['Readers Choice Votes'].idxmax()\ntitles_with_max_votes = df.loc[max_votes_index, ['Readers Choice Category', 'Title', 'Readers Choice Votes', 'Total Avg Rating', 'Number of Ratings', 'Number of Reviews', 'Number of Pages']].sort_values('Readers Choice Votes', ascending=False)\ntitles_with_max_votes\n```\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_18.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로, 2023년에 각 월별로 발행된 책의 수를 알아내기 위해 barplot을 사용하여 분석할 예정입니다.\n\n```js\nimport calendar\ndf['First Published date'] = pd.to_datetime(df['First Published date'])\n\n# 2023년에 발행된 책만 추출합니다.\nbooks_2023 = df[df['First Published date'].dt.year == 2023]\n\n# 각 월별 발행된 책의 수를 계산합니다.\nbooks_per_month = books_2023.groupby(books_2023['First Published date'].dt.month)['Title'].count().reset_index()\nbooks_per_month['Month'] = books_per_month['First Published date'].apply(lambda x: calendar.month_abbr[x])\n\nplt.figure(figsize=(14, 8))\nsns.barplot(data=books_per_month, x='Month', y='Title', palette='Blues_d')\nplt.title('2023년에 발행된 책의 분포')\nplt.xlabel('월')\nplt.ylabel('발행된 책의 수')\nplt.show()\n\nbooks_per_month[['Month','Title']]\n```\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_19.png\" /\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_20.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 그래프를 통해 우리는 11월에 가장 적은 책이 출간되었음을 발견했습니다. 반면에 9월과 1월은 가장 많은 책이 출간된 달이었습니다.\n\n# 결론\n\n우리의 분석을 통해 2023년에 어떤 카테고리가 가장 인기가 많고 가장 인기가 적은지를 결정했습니다. 또한 페이지, 투표, 평점 및 리뷰 사이의 연결 여부를 확인하기 위해 분포 분석과 상관 분석을 수행했습니다.\n\n내가 방금 보여준 것은 Python을 데이터 분석과 시각화 도구로 사용하는 놀라운 방법의 일감입니다. 이 기사에서는 데이터 집합을 더 깊게 이해할 수 있도록 중요한 기본 단계를 다루었는데, 이를 위해 분석을 위한 pandas와 시각화를 위한 matplotlib/seaborn과 같은 라이브러리를 사용했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 글을 읽어 주셔서 감사합니다. 읽으시는 데 즐거움을 느끼셨기를 바라고 파이썬에서의 탐색적 데이터 분석을 이해하는 데 도움이 되었기를 바랍니다.\n\n여기서 저의 전체 코드를 제 Github에서 찾아볼 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_0.png"},"coverImage":"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_0.png","tag":["Tech"],"readingTime":18}],"page":"37","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"37"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>