<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/63" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/63" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="BERT 마스터하기 자연어 처리NLP 초보부터 고급까지의 포괄적 가이드" href="/post/2024-06-19-MasteringBERTAComprehensiveGuidefromBeginnertoAdvancedinNaturalLanguageProcessingNLP"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="BERT 마스터하기 자연어 처리NLP 초보부터 고급까지의 포괄적 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-MasteringBERTAComprehensiveGuidefromBeginnertoAdvancedinNaturalLanguageProcessingNLP_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="BERT 마스터하기 자연어 처리NLP 초보부터 고급까지의 포괄적 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">BERT 마스터하기 자연어 처리NLP 초보부터 고급까지의 포괄적 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">24<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="프롬프트 엔지니어링 최상의 실천 방법 반복적인 프롬프트 개발" href="/post/2024-06-19-PromptEngineeringBestPracticesIterativePromptDevelopment"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="프롬프트 엔지니어링 최상의 실천 방법 반복적인 프롬프트 개발" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-PromptEngineeringBestPracticesIterativePromptDevelopment_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="프롬프트 엔지니어링 최상의 실천 방법 반복적인 프롬프트 개발" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">프롬프트 엔지니어링 최상의 실천 방법 반복적인 프롬프트 개발</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LLM Alignments Part 21 RLAIF" href="/post/2024-06-19-LLMAlignmentsPart21RLAIF"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LLM Alignments Part 21 RLAIF" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LLM Alignments Part 21 RLAIF" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">LLM Alignments Part 21 RLAIF</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LLM의 내부 작업 공개 고유 값 관점" href="/post/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LLM의 내부 작업 공개 고유 값 관점" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LLM의 내부 작업 공개 고유 값 관점" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">LLM의 내부 작업 공개 고유 값 관점</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="위에서 바라본 Transformer 구조" href="/post/2024-06-19-TheTransformerArchitectureFromaTopView"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="위에서 바라본 Transformer 구조" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-TheTransformerArchitectureFromaTopView_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="위에서 바라본 Transformer 구조" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">위에서 바라본 Transformer 구조</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="질문-답변 캐글 대회에서 Sentence Transformer를 활용하기" href="/post/2024-06-19-Question-AnswerKaggleCompetitionusingSentenceTransformer"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="질문-답변 캐글 대회에서 Sentence Transformer를 활용하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Question-AnswerKaggleCompetitionusingSentenceTransformer_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="질문-답변 캐글 대회에서 Sentence Transformer를 활용하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">질문-답변 캐글 대회에서 Sentence Transformer를 활용하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="RAGRetrieval Augmented Generation 소개 및 응용 데모" href="/post/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="RAGRetrieval Augmented Generation 소개 및 응용 데모" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="RAGRetrieval Augmented Generation 소개 및 응용 데모" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">RAGRetrieval Augmented Generation 소개 및 응용 데모</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="언어별 RAG 애플리케이션 탐색 미슈나와 대화하기" href="/post/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="언어별 RAG 애플리케이션 탐색 미슈나와 대화하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="언어별 RAG 애플리케이션 탐색 미슈나와 대화하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">언어별 RAG 애플리케이션 탐색 미슈나와 대화하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">23<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="RAG 성능 향상에 2개의 LLM 호출이 도움이 될까요" href="/post/2024-06-19-Can2LLMcallsboostyourRAGsperformance"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="RAG 성능 향상에 2개의 LLM 호출이 도움이 될까요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Can2LLMcallsboostyourRAGsperformance_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="RAG 성능 향상에 2개의 LLM 호출이 도움이 될까요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">RAG 성능 향상에 2개의 LLM 호출이 도움이 될까요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="만나보세요 HUSKY 다단계 추론을 최적화한 새로운 에이전트" href="/post/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="만나보세요 HUSKY 다단계 추론을 최적화한 새로운 에이전트" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="만나보세요 HUSKY 다단계 추론을 최적화한 새로운 에이전트" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">만나보세요 HUSKY 다단계 추론을 최적화한 새로운 에이전트</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/61">61</a><a class="link" href="/posts/62">62</a><a class="link posts_-active__YVJEi" href="/posts/63">63</a><a class="link" href="/posts/64">64</a><a class="link" href="/posts/65">65</a><a class="link" href="/posts/66">66</a><a class="link" href="/posts/67">67</a><a class="link" href="/posts/68">68</a><a class="link" href="/posts/69">69</a><a class="link" href="/posts/70">70</a><a class="link" href="/posts/71">71</a><a class="link" href="/posts/72">72</a><a class="link" href="/posts/73">73</a><a class="link" href="/posts/74">74</a><a class="link" href="/posts/75">75</a><a class="link" href="/posts/76">76</a><a class="link" href="/posts/77">77</a><a class="link" href="/posts/78">78</a><a class="link" href="/posts/79">79</a><a class="link" href="/posts/80">80</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"BERT 마스터하기 자연어 처리NLP 초보부터 고급까지의 포괄적 가이드","description":"","date":"2024-06-19 20:42","slug":"2024-06-19-MasteringBERTAComprehensiveGuidefromBeginnertoAdvancedinNaturalLanguageProcessingNLP","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-MasteringBERTAComprehensiveGuidefromBeginnertoAdvancedinNaturalLanguageProcessingNLP_0.png\" /\u003e\n\n# 소개:\n\nBERT (Bidirectional Encoder Representations from Transformers)는 구글이 개발한 혁명적인 자연어 처리(NLP) 모델입니다. 이 모델은 언어 이해 작업의 환경을 변화시켜 기계가 언어의 맥락과 뉘앙스를 이해할 수 있게 하였습니다. 이 블로그에서는 BERT의 기본부터 고급 개념까지 설명, 예제 및 코드 스니펫과 함께 여러분을 안내할 것입니다.\n\n# 목차\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- BERT 소개\n\n- BERT란 무엇인가요?\n- BERT의 중요성은 무엇인가요?\n- BERT는 어떻게 작동하나요?\n\n2. BERT를 위한 텍스트 전처리\n\n- 토큰화(Tokenization)\n- 입력 형식 지정(Input Formatting)\n- 가리고(LMasked) 언어 모델 (MLM) 목표\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 특정 작업을 위한 BERT 파인 튜닝\n\n- BERT의 아키텍처 변형(BERT-base, BERT-large 등)\n- NLP에서의 전이 학습\n- 하류 작업 및 파인 튜닝\n- 예시: BERT를 사용한 텍스트 분류\n\n4. BERT의 어텐션 메커니즘\n\n- Self-Attention\n- Multi-Head Attention\n- BERT에서의 어텐션\n- 어텐션 가중치 시각화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5. BERT의 학습 과정\n\n- 사전 훈련 단계\n- 가리고 있는 언어 모델 (MLM) 목적\n- 다음 문장 예측 (NSP) 목적\n\n6. BERT 임베딩\n\n- 단어 임베딩 대 컨텍스트 임베딩\n- WordPiece 토큰화\n- 위치 인코딩\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n7. BERT의 고급 기술들\n\n- 파인 튜닝 전략\n- 어휘 외 단어 처리\n- BERT로의 도메인 적응\n- BERT로부터의 지식 증류\n\n8. 최근 개발 및 변형\n\n- RoBERTa (더 강력한 기준선)\n- ALBERT (라이트 BERT)\n- DistilBERT (콤팩트한 버전)\n- ELECTRA (효율적인 인코더 학습)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n9. 시퀀스 대 시퀀스 작업을 위한 BERT\n\n- 텍스트 요약을 위한 BERT\n- 언어 번역을 위한 BERT\n- 대화형 인공지능을 위한 BERT\n\n10. 공통적인 도전 과제 및 완화 방안\n\n- BERT의 계산 요구사항\n- 긴 시퀀스 처리\n- BERT 내 편견 극복\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n11. BERT와 함께하는 자연어 처리의 미래 방향\n\n- OpenAI의 GPT 모델\n- 사전 훈련된 언어 모델에서의 BERT 역할\n- BERT 응용 프로그램에서의 윤리적 고려 사항\n\n12. Hugging Face Transformers 라이브러리로 BERT 구현하기\n\n- Transformers 설치하기\n- 사전 훈련된 BERT 모델 불러오기\n- 토큰화 및 입력 형식 지정\n- 사용자 정의 작업을 위한 BERT 파인튜닝\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 제1장: BERT 소개\n\n# BERT란 무엇인가요?\n\n자연어 처리(NLP)의 끊임없이 진화하는 세계에서, BERT라는 혁신적인 기술이 등장하여 게임 체인저로 인정받고 있습니다. BERT는 Transformer의 양방향 인코더 표현(Bidirectional Encoder Representations from Transformers)을 의미하며, 기계 학습 용어의 다양한 약어들 속에서 또 다른 약어가 아닙니다. 이는 기계가 언어를 이해하는 방식을 변화시키며, 인간들이 소통을 풍부하고 의미 있게 만드는 복잡한 뉘앙스와 맥락적 종속성을 이해할 수 있게 합니다.\n\n# BERT는 왜 중요한가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 문장을 상상해보세요: \"그녀는 바이올린을 아름답게 연주합니다.\" 기존 언어 모델은 이 문장을 왼쪽에서 오른쪽으로 처리하여 악기(\"바이올린\")의 정체성이 전체 문장의 해석에 영향을 미친다는 중요한 사실을 놓치게 됩니다. 그러나 BERT는 단어 사이의 문맥 주도적 관계가 의미 파악에 중요한 역할을 한다는 사실을 이해합니다. BERT는 각 단어 주변의 완전한 문맥을 고려할 수 있도록 양방향성의 본질을 포착해, 언어 이해의 정확도와 심도를 혁신적으로 개선합니다.\n\n# BERT는 어떻게 작동하나요?\n\nBERT의 핵심은 Transformers라는 강력한 신경망 구조에 의해 제공됩니다. 이 구조는 셀프 어텐션(self-attention) 메커니즘을 포함하는데, 이를 통해 BERT는 각 단어의 중요성을 문맥에 따라 가중치를 부여할 수 있습니다. 선행 및 후행 단어를 모두 고려하는 이 문맥 인식은 BERT에 문장 내 단어의 의미를 고려한 단어 임베딩을 생성할 수 있는 능력을 제공합니다. 이는 BERT가 문장을 여러 차례 읽어 단어 역할에 대한 깊은 이해를 얻는 것과 비슷합니다.\n\n문장을 고려해보세요: \"‘리드’ 가수가 밴드를 이끌 것입니다.\" 기존 모델은 \"리드\"라는 단어의 모호성에 어려움을 겪을 수 있지만, BERT는 첫 번째 \"리드\"가 명사이고 두 번째가 동사임을 손쉽게 구분하여 언어 구조를 명확히 하는 능력을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다가올 장에서 BERT를 해부하며, 여러분을 기초 개념에서부터 고급 응용까지 안내할 여정이 시작됩니다. BERT가 다양한 NLP 작업에 활용되는 방법을 살펴보고, BERT의 attention 메커니즘에 대해 배우고, 그 교육 과정에 관심을 두며, NLP 분야를 혁신하는 데 BERT가 미치는 영향을 직접 목격하게 될 것입니다.\n\nBERT의 세부 사항에 대해 탐구할수록, 여러분은 BERT가 모델에 그치는 것이 아니라 기계가 인간 언어의 본질을 이해하는 방식을 패러다임이 변경된 것임을 알게 될 것입니다. 그러므로, 우리는 언어 이해가 보통을 벗어나 특별한 수준으로 도약하는 BERT 세계로의 흥미로운 여행을 함께 떠날 준비를 해봅시다.\n\n# 제2장: BERT를 위한 텍스트 전처리\n\n![이미지](/assets/img/2024-06-19-MasteringBERTAComprehensiveGuidefromBeginnertoAdvancedinNaturalLanguageProcessingNLP_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBERT가 텍스트에 마법을 부리기 전에, 이를 이해할 수 있는 방식으로 준비하고 구조화해야 합니다. 이 장에서는 BERT를 위한 텍스트 전처리의 중요한 단계들을 살펴보겠습니다. 이에는 토큰화, 입력 형식 지정, 그리고 마스크된 언어 모델 (MLM) 목표가 포함됩니다.\n\n# 토큰화: 의미 있는 조각으로 텍스트 나누기\n\nBERT에게 책을 읽는 방법을 가르친다고 상상해보세요. 전체 책을 한꺼번에 주지 않을 것이며, 문장과 단락으로 나눌 것입니다. 마찬가지로, BERT는 텍스트를 토큰이라 불리는 작은 단위로 나눠야 합니다. 그러나 여기서 중요한 점은, BERT는 WordPiece 토큰화를 사용한다는 것입니다. 이 방식은 단어를 더 작은 조각으로 분리하여 \"running\"을 \"run\"과 \"ning\"으로 나눕니다. 이는 어려운 단어를 처리하고, BERT가 생소한 단어에 어려움을 겪지 않도록 도와줍니다.\n\n예시: 원본 텍스트: “ChatGPT is fascinating.” WordPiece 토큰: [“Chat”, “##G”, “##PT”, “is”, “fascinating”, “.”]\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 입력 포맷: BERT에게 문맥 제공하기\n\nBERT는 문맥을 좋아해요. 우리는 그에게 문맥을 제공해야 해요. 그를 위해 BERT가 이해하는 방식으로 토큰을 포맷합니다. 우리는 [CLS] (분류를 나타냄)와 같은 특수 토큰을 문장 사이에 놓습니다. 그림 (기계 언어 모델)에 나타난 것처럼요. 우리는 또한 문장에 속하는 토큰을 알려주는 세그먼트 임베딩을 할당해요.\n\n예시: 원본 텍스트: “ChatGPT is fascinating.” 포맷팅된 토큰: [“[CLS]”, “Chat”, “##G”, “##PT”, “is”, “fascinating”, “.”, “[SEP]”]\n\n# 가려진 언어 모델 (MLM) 목표: BERT에게 문맥 가르치기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBERT의 비밀 무기는 양방향 문맥을 이해하는 능력에 있습니다. 학습 중에는 일부 단어가 마스크 처리되어 문장 내에서 [MASK]로 대체되며, BERT는 이를 context에 따라 예측하는 방법을 학습합니다. 이는 BERT가 단어들이 서로 어떻게 관련되는지 알아내는 데 도움이 됩니다. 아래 그림에서 확인할 수 있듯이 (Machine Language Model)\n\n예시: 원본 문장: “The cat is on the mat.” 마스크 처리된 문장: “The [MASK] is on the mat.”\n\n코드 스니펫: Hugging Face Transformers로 토큰화\n\n```python\nfrom transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntext = \"BERT preprocessing is essential.\"\ntokens = tokenizer.tokenize(text)\n\nprint(tokens)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 코드는 Hugging Face Transformers 라이브러리를 사용하여 BERT 토크나이저를 통해 텍스트를 토큰화합니다.\n\n다음 장에서는 BERT를 특정 작업에 대해 세밀하게 조정하는 흥미로운 세계에 대해 탐구하고, 그 주의 메커니즘이 언어 이해 챔피언으로 만드는 방식을 알아보겠습니다. 더 많은 정보 습득을 위해 망설이지 마세요!\n\n# 3장: 특정 작업에 대한 BERT 세밀 조정\n\n![이미지](/assets/img/2024-06-19-MasteringBERTAComprehensiveGuidefromBeginnertoAdvancedinNaturalLanguageProcessingNLP_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBERT가 어떻게 작동하는지 이해한 후에는 이제 그 마법을 실용적으로 활용해 보는 시간입니다. 이 장에서는 BERT를 특정 언어 작업에 대해 세밀하게 조정하는 방법을 살펴볼 것입니다. 이는 사전 훈련된 BERT 모델을 텍스트 분류와 같은 작업을 수행할 수 있도록 조정하는 것을 포함합니다. 함께 알아보겠습니다!\n\n# BERT의 아키텍처 변형: 적합한 모델 선택\n\nBERT는 BERT-base, BERT-large 등과 같이 다양한 플레이버로 나옵니다. 다양한 모델 크기와 복잡성을 가지고 있습니다. 선택은 작업의 요구 사항과 가지고 있는 자원에 따라 다릅니다. 더 큰 모델은 성능이 더 좋을 수 있지만 더 많은 컴퓨팅 성능이 필요합니다.\n\n# NLP의 전이 학습: 사전 학습된 지식을 기반으로 빌드하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n버트를 언어 전문가로 상상해보세요. 이미 많은 텍스트를 읽고 이해한 전문가입니다. 처음부터 모든 것을 가르치는 대신에, 특정 작업에 맞게 세부 조정을 거쳐 학습시킵니다. 이것이 전이 학습의 마법입니다 — 버트의 기존 지식을 활용하고 특정 작업에 맞게 맞춤화하는 것입니다. 이는 많은 것을 알고 있는 가르침을 받는 과외 선생님과 비슷합니다.\n\n# 하류 작업 및 세부 조정: 버트의 지식 적용하기\n\n우리가 버트를 세부 조정하는 작업을 \"하류 작업\"이라고 합니다. 감정 분석, 개체명 인식 등의 예시가 포함됩니다. 세부 조정은 작업별 데이터를 사용하여 버트의 가중치를 업데이트하는 것을 포함합니다. 이를 통해 버트를 처음부터 학습시키지 않고도 이러한 작업에 특화되도록 도와줍니다.\n\n예시: 버트를 활용한 텍스트 분류\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom transformers import BertForSequenceClassification, BertTokenizer\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\ntext = \"This movie was amazing!\"\ninputs = tokenizer(text, return_tensors='pt')\noutputs = model(**inputs)\npredictions = torch.argmax(outputs.logits, dim=1)\nprint(predictions)\n```\n\n이 코드는 Hugging Face Transformers를 사용하여 텍스트 분류를 위한 사전 훈련된 BERT 모델을 사용하는 것을 보여줍니다.\n\n이 스니펫에서는 텍스트 분류를 위해 설계된 사전 훈련된 BERT 모델을 로드합니다. 입력 텍스트를 토큰화하고 모델을 통해 전달하여 예측을 얻습니다.\n\n특정 작업에 대한 BERT의 세부 튜닝은 실제 응용 프로그램에서 빛을 발할 수 있습니다. 다음 장에서는 BERT의 문맥 이해에 중요한 주의 메커니즘을 해독할 것이며 더 많은 정보를 발견하려면 계속 주목해주세요!\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 제4장: BERT의 어텐션 메커니즘\n\n![BERT](/assets/img/2024-06-19-MasteringBERTAComprehensiveGuidefromBeginnertoAdvancedinNaturalLanguageProcessingNLP_3.png)\n\n이제 BERT를 작업에 적용하는 방법을 살펴 보았으니, BERT를 강력하게 만드는 요소인 어텐션 메커니즘에 대해 더 자세히 살펴보겠습니다. 이 장에서는 셀프 어텐션, 멀티헤드 어텐션, 그리고 BERT의 어텐션 메커니즘이 언어의 맥락을 파악할 수 있도록 하는 방법을 알아보겠습니다.\n\n# 셀프 어텐션: BERT의 슈퍼파워\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n책을 읽고 중요하다고 생각되는 단어를 강조해본 적이 있나요? Self-attention은 그것과 비슷한데요, BERT를 위한 것입니다. Self-attention은 문장의 각 단어를 살펴보고, 중요성에 따라 다른 단어에 얼마나 주의를 기울여야 하는지 결정합니다. 이를 통해 BERT는 문장 속에서 멀리 떨어져 있어도 관련된 단어에 집중할 수 있어요.\n\n# Multi-Head Attention: 팀워크의 비밀\n\nBERT는 하나의 관점에만 의존하지 않고, 여러 “헤드”의 주의를 사용합니다. 이 헤드들은 문장의 다양한 측면에 초점을 맞춘 다양한 전문가로 생각할 수 있어요. 이 다중 헤드 접근법은 BERT가 단어 간의 다양한 관계를 포착하여 이해를 더욱 풍부하고 정확하게 만들어줍니다.\n\n# BERT 안의 Attention: 문맥적인 매력\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBERT의 주의는 단어 앞이나 뒤로만 제한되지 않아요. 양방향을 모두 고려해요! BERT가 단어를 읽을 때, 혼자가 아니에요; 이웃들을 알아차려요. 이러한 방식으로, BERT는 단어의 전체 맥락을 고려한 임베딩을 생성해요. 이는 글의 전체적인 맥락을 이해하는 것과 같아요. 그게 바로 설치뿐만 아니라 문장 전체를 고려해서 웃음 소리를 이해하는 것과 같아요.\n\n코드 조각: 주의 차중 시각화\n\n```js\nimport torch\nfrom transformers import BertModel, BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\ntext = \"BERT의 주의 메커니즘은 매혹적입니다.\"\ninputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\noutputs = model(**inputs, output_attentions=True)\n\nattention_weights = outputs.attentions\nprint(attention_weights)\n```\n\n이 코드에서는 Hugging Face Transformers를 사용하여 BERT의 주의 가중치를 시각화합니다. 이 가중치는 BERT가 문장에서 다른 단어들에 얼마나 주의를 기울이는지를 보여줘요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBERT의 주의 메커니즘은 마치 주목할 가치 있는 부분에 초점을 맞춰주는 형광등처럼 작동합니다. 다음 장에서는 BERT의 훈련 과정과 어떻게 언어 마에스트로가 되는지 알아볼 것입니다. 더 많은 통찰력을 기대해주세요!\n\n# 제 5장: BERT의 훈련 과정\n\nBERT가 어떻게 학습하는지를 이해하는 것은 그 능력을 평가하는 데 중요합니다. 이 장에서는 BERT의 훈련 과정의 복잡성을 파헤치며, 사전 훈련 단계, 가려진 언어 모델 (MLM) 목표, 다음 문장 예측 (NSP) 목표를 살펴볼 것입니다.\n\n# 사전 훈련 단계: 지식의 기초\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBERT의 여정은 사전 훈련으로 시작됩니다. 여기서 BERT는 엄청난 양의 텍스트 데이터에서 학습합니다. BERT에게 수백만 개의 문장을 보여주고 빠진 단어를 예측하게 합니다. 이 연습은 BERT가 언어 패턴과 관계를 탄탄히 구축하는 데 도움이 됩니다.\n\n# 가리거나 문장 모델 (MLM) 목표: 빈칸 채우기 게임\n\n사전 훈련 중에 BERT는 일부 단어가 가려진 문장을 제공받습니다. 그러면 주변 맥락을 기반으로 이를 예측하려고 합니다. 이는 빈칸을 채우는 게임의 언어 버전과 같습니다. 빈칸을 추측함으로써 BERT는 단어 간의 관련성을 학습하고 맥락을 통해 빛을 발합니다.\n\n# 문장 다음 예측 (NSP) 목표: 문장 흐름 파악\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBERT는 단어를 이해하는 것뿐만 아니라 문장의 흐름을 파악합니다. NSP 목표에서 BERT는 텍스트 쌍에서 한 문장이 다른 문장을 따르는지 예측하도록 훈련됩니다. 이를 통해 BERT는 문장 사이의 논리적 연결을 이해하며, 이를 통해 문단 및 더 긴 텍스트를 이해하는 데 능숙해집니다.\n\n예: 사전 훈련 및 MLM\n\n```python\nfrom transformers import BertForMaskedLM, BertTokenizer\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForMaskedLM.from_pretrained('bert-base-uncased')\n\ntext = \"BERT is a powerful language model.\"\ninputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, add_special_tokens=True)\noutputs = model(**inputs, labels=inputs['input_ids'])\n\nloss = outputs.loss\nprint(loss)\n```\n\n이 코드는 BERT의 마스크 언어 모델 (MLM) 사전 훈련을 보여줍니다. 모델은 마스킹된 단어를 예측하면서 예측 오류를 최소화하도록 훈련됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBERT의 교육 과정은 언어 규칙을 학습시키는 것과 관련된 문장 내용을 이해하는 연습의 조합을 통해 진행됩니다. 다음 장에서는 BERT의 임베딩에 대해 자세히 알아볼 것이며, 이 임베딩이 언어 능력에 어떻게 기여하는지 알아볼 것입니다. 계속 학습해 주세요!\n\n# 제 6장: BERT 임베딩\n\n![이미지](/assets/img/2024-06-19-MasteringBERTAComprehensiveGuidefromBeginnertoAdvancedinNaturalLanguageProcessingNLP_4.png)\n\nBERT의 강점은 단어를 특정 컨텍스트 내에서 의미를 포착하는 방식으로 표현할 수 있는 능력에 있습니다. 이 장에서는 BERT의 임베딩, 콘텍스트 단어 임베딩, WordPiece 토큰화 및 위치 인코딩을 다룰 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 단어 임베딩 vs. 문맥 단어 임베딩\n\n단어 임베딩은 코드 단어로 각 단어를 대체하는 것으로 생각해보세요. BERT는 문맥 단어 임베딩으로 더 나아가요. 각 단어에 대해 하나의 코드 단어가 아니라 문장에서의 문맥에 따라 다른 임베딩을 만듭니다. 이렇게 하면 각 단어의 표현이 더 세밀해지고 주변 단어에 의해 영향을 받습니다.\n\n# WordPiece 토큰화: 복잡한 어휘 처리\n\nBERT의 어휘는 하위 단어라고 불리는 작은 조각으로 구성된 퍼즐과 같아요. WordPiece 토큰화를 사용하여 단어를 이러한 하위 단어로 분해합니다. 이는 긴 단어와 복잡한 단어를 처리하거나 이전에 본 적이 없는 단어를 다루는 데 특히 유용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 위치 부호화: 문장 구조 탐색\n\nBERT는 단어를 양방향으로 읽기 때문에 문장에서 각 단어의 위치를 알아야 합니다. 위치 부호화는 BERT에게 이러한 공간 인식을 제공하기 위해 임베딩에 추가됩니다. 이를 통해 BERT는 단어의 의미뿐만 아니라 문장에서의 위치도 파악할 수 있습니다.\n\n코드 스니펫: Hugging Face Transformers를 사용하여 단어 임베딩 추출\n\n```js\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\ntext = \"BERT embeddings are fascinating.\"\ninputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, add_special_tokens=True)\noutputs = model(**inputs)\n\nword_embeddings = outputs.last_hidden_state\nprint(word_embeddings)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 코드는 Hugging Face Transformers를 사용하여 단어 임베딩을 추출하는 방법을 보여줍니다. 이 모델은 입력 텍스트의 각 단어에 대해 문맥 임베딩을 생성합니다.\n\nBERT의 임베딩은 단어가 고유한 문맥 기반 신원을 얻는 언어 놀이터와 같습니다. 다음 장에서는 BERT를 세밀하게 조정하고 다양한 작업에 적응시키는 고급 기술을 탐색할 것입니다. 계속해서 배우고 실험해보세요!\n\n# 제7장: BERT의 고급 기술\n\nBERT를 능숙하게 사용하기 시작했다면, 이제 그 잠재력을 극대화하는 고급 기술을 탐색할 때입니다. 이 장에서는 세밀한 조정, 단어 사전에 없는 단어 처리, 도메인 적응, 심지어 BERT로부터 지식 증류를 다루는 전략에 대해 탐구할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 세밀 조정 전략: 적응 능력 향상\n\nBERT의 세밀 조정에는 신중한 고려가 필요합니다. 단순히 마지막 분류 레이어뿐만 아니라 중간 레이어도 세밀 조정할 수 있습니다. 이를 통해 BERT가 특정 작업에 더 효과적으로 적응할 수 있습니다. 다양한 레이어와 학습 속도를 실험하여 최적의 조합을 찾아보세요.\n\n# OOV(Out-of-Vocabulary) 단어 처리: 알 수 없는 것 다루기\n\nBERT의 어휘는 무한하지 않기 때문에 인식하지 못할 수 있는 단어를 마주할 수 있습니다. OOV 단어를 처리할 때 WordPiece 토큰화를 사용하여 부분 단어로 분할할 수 있습니다. 또는 알 수 없는 단어를 “[UNK]”와 같은 특별한 토큰으로 대체할 수도 있습니다. OOV 전략을 균형있게 유지하는 것은 연습을 통해 개선되는 기술입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# BERT를 활용한 도메인 적응: BERT를 내 것으로 만들기\n\nBERT는 강력하지만 모든 도메인에서 최적으로 동작하지는 않을 수 있습니다. 도메인 적응은 도메인 특화 데이터로 BERT를 세밀하게 조정하는 것을 의미합니다. BERT에 도메인 특화 텍스트를 노출시킴으로써, 해당 도메인의 고유한 언어 패턴을 이해하도록 학습합니다. 이는 특정 작업에 대한 성능을 크게 향상시킬 수 있습니다.\n\n# BERT로부터의 지식 증류: 지혜 전달하기\n\n지식 증류는 작은 모델(학생)을 더 큰 사전 훈련된 모델(BERT와 같은 선생님)의 동작을 모방하도록 훈련하는 과정을 의미합니다. 이 압축된 모델은 선생님의 예측 뿐만 아니라 신뢰도와 추론 능력도 학습합니다. 이 접근 방식은 자원이 제한된 기기에 BERT를 배포할 때 특히 유용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드 스니펫: Hugging Face Transformers를 사용하여 중간 레이어 Featurie Fine-Tuning\n\n```js\nfrom transformers import BertForSequenceClassification, BertTokenizer\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\ntext = \"Advanced fine-tuning with BERT.\"\ninputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\noutputs = model(**inputs, output_hidden_states=True)\n\nintermediate_layer = outputs.hidden_states[6]  # 7th layer\nprint(intermediate_layer)\n```\n\n이 코드는 Hugging Face Transformers를 사용하여 BERT의 중간 레이어를 세밀하게 조정하는 방법을 보여줍니다. 중간 레이어를 추출하면 BERT를 특정 작업에 더 효과적으로 세밀하게 조정할 수 있습니다.\n\n이러한 고급 기술을 탐험하면서 BERT의 적응성과 잠재력을 마스터하는 길을 걷게 될 것입니다. 다음 장에서는 NLP 분야를 더욱 더 높은 수준으로 끌어올린 BERT의 최근 개발 및 변형에 대해 알아보겠습니다. 호기심을 가지고 혁신을 이어가세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 8장: 최근 개발 및 변형\n\n자연어 처리(NLP) 분야가 발전함에 따라 BERT도 발전하고 있습니다. 이 장에서는 BERT의 성능을 더욱 향상시킨 최근 개발 사항 및 변형인 RoBERTa, ALBERT, DistilBERT, ELECTRA 등을 살펴보겠습니다.\n\n## RoBERTa: BERT의 기본을 넘어서\n\nRoBERTa는 BERT의 똑똑한 형제입니다. 더 큰 배치, 더 많은 데이터, 그리고 더 많은 학습 단계를 포함한 보다 철저한 레시피로 훈련됩니다. 이 강화된 훈련 방법은 다양한 작업에서 더 나은 언어 이해 및 성능 향상을 이끌어냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# ALBERT: 가벼운 BERT\n\nALBERT은 \"A Lite BERT\"의 약자입니다. 효율적으로 설계된 ALBERT은 파라미터 공유 기술을 사용하여 메모리 소비를 줄입니다. 크기는 작지만 BERT의 성능을 유지하며 자원이 제한적인 경우에 유용합니다.\n\n# DistilBERT: 소형이지만 알짜\n\nDistilBERT는 BERT의 축약 버전입니다. BERT의 동작을 모방하도록 훈련되었지만 파라미터가 적습니다. 이로 인해 DistilBERT는 가볍고 빠르며 여전히 BERT의 성능을 상당 부분 유지합니다. 속도와 효율이 중요한 애플리케이션에 적합한 선택지입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# ELECTRA: BERT로부터 효율적으로 학습하기\n\nELECTRA는 훈련에 흥미로운 변화를 가져왔어요. 가리킨 단어를 예측하는 대신, ELECTRA는 교체된 단어가 실제인지 인공적으로 생성된 것인지 감지하여 훈련합니다. 이 효율적인 방법으로 ELECTRA는 전체 계산 비용 없이도 대형 모델을 훈련시키는 유망한 방법으로 자리 잡았어요.\n\n코드 스니펫: Hugging Face Transformers와 함께 RoBERTa 사용하기\n\n```python\nfrom transformers import RobertaTokenizer, RobertaModel\nimport torch\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\n\ntext = \"RoBERTa는 BERT의 고급 변형입니다.\"\ninputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\noutputs = model(**inputs)\n\nembeddings = outputs.last_hidden_state\nprint(embeddings)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRoBERTa는 BERT의 변형 중 하나로, Hugging Face Transformers를 사용하여 문맥 임베딩을 생성하는 방법을 보여주는 코드입니다. \n\n이러한 최근의 발전과 변형은 BERT의 영향이 자연어 처리 분야에 파장을 일으키고, 새로운 강화된 모델들을 영감을 주고 있음을 보여줍니다. 다음 장에서는 BERT가 텍스트 요약 및 언어 번역과 같은 시퀀스-투-시퀀스 작업에 어떻게 활용될 수 있는지 살펴볼 것입니다. BERT의 더 흥미로운 응용 프로그램을 기대해 주세요!\n\n# 9장: 시퀀스-투-시퀀스 작업을 위한 BERT\n\n이 장에서는 개별 문장을 이해하기 위해 처음에 설계된 BERT가 텍스트 요약, 어어 번역과 같이 시퀀스-투-시퀀스 응용 프로그램과 같은 더 복잡한 작업에 적응되는 방법을 살펴볼 것입니다. 텍스트 요약, 언어 번역, 대화형 AI에서의 잠재력에 대해 탐구해 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 텍스트 요약을 위한 BERT: 정보 압축\n\n텍스트 요약은 더 긴 텍스트의 본질을 유지한 채 더 짧은 버전으로 요약하는 작업을 말합니다. BERT는 이 작업을 위해 특별히 설계된 모델은 아니지만, 제공하는 문맥 이해력을 활용하여 원본 텍스트를 입력하고 간결한 요약을 생성할 수 있습니다.\n\n# 언어 번역을 위한 BERT: 언어 간의 연결 다리\n\n언어 번역은 한 언어에서 다른 언어로 텍스트를 변환하는 작업을 의미합니다. BERT는 직접적인 번역 모델은 아니지만, 문맥 임베딩을 통해 번역 모델의 품질을 향상시킬 수 있습니다. 단어의 문맥을 이해함으로써 BERT는 번역 중 원래 텍스트의 뉘앙스를 보존하는 데 도움을 줄 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 대화형 AI에서의 BERT: 대화 내용 이해하기\n\n대화형 AI는 개별 문장뿐만 아니라 대화 흐름도 이해해야 합니다. BERT의 양방향 컨텍스트는 여기에서 유용합니다. 이를 통해 문맥적으로 일관된 응답을 분석하고 생성할 수 있어 더 매력적인 챗봇과 가상 비서를 만드는 데 유용한 도구가 됩니다.\n\n코드 스니펫: 허깅페이스 트랜스포머를 활용한 BERT를 이용한 텍스트 요약\n\n```js\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\noriginal_text = \"텍스트 요약을 위한 긴 텍스트...\"\ninputs = tokenizer(original_text, return_tensors='pt', padding=True, truncation=True)\n\nsummary_logits = model(**inputs).logits\nsummary = tokenizer.decode(torch.argmax(summary_logits, dim=1))\nprint(\"요약:\", summary)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 코드는 Hugging Face Transformers를 사용하여 BERT를 텍스트 요약에 활용하는 방법을 보여줍니다. 이 모델은 입력 텍스트의 가장 관련성 높은 부분을 예측하여 요약을 생성합니다.\n\nBERT의 시퀀스-투-시퀀스 작업에서의 능력을 탐험하면, 원래 의도된 용도를 넘어 다양한 응용에 적응할 수 있음을 발견할 것입니다. 다음 장에서는 BERT 사용 시 일반적인 문제를 다루고 효과적으로 해결하는 방법을 살펴보겠습니다. BERT 기반 프로젝트에서 장애물을 극복하는 통찰을 기대해 주세요!\n\n# 장 10: 일반적인 문제와 완화 전략\n\nBERT가 강력하지만 도전 과제가 없는 것은 아닙니다. 이 장에서는 BERT 작업 시 마주할 수 있는 일반적인 문제들을 보다 심층적으로 다루고 극복 전략을 제시합니다. 긴 텍스트 처리부터 계산 자원 관리까지 다룰 주제가 다양하니 안심하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 도전 과제 1: 긴 텍스트 처리하기\n\nBERT는 입력에 대한 최대 토큰 제한이 있으며 긴 텍스트는 잘릴 수 있습니다. 이를 완화하기 위해 텍스트를 처리 가능한 조각으로 나누어 처리할 수 있습니다. 이때 이러한 조각 사이의 문맥을 신중하게 관리하여 의미 있는 결과를 보장해야 합니다.\n\n코드 스니펫: BERT로 긴 텍스트 다루기\n\n```js\nmax_seq_length = 512  # BERT의 최대 토큰 제한\ntext = \"다룰 긴 텍스트...\"\ntext_chunks = [text[i:i + max_seq_length] for i in range(0, len(text), max_seq_length)]\n\nfor chunk in text_chunks:\n    inputs = tokenizer(chunk, return_tensors='pt', padding=True, truncation=True)\n    outputs = model(**inputs)\n    # 각 조각에 대한 결과 처리\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 도전 과제 2: 자원 집약적 연산\n\nBERT 모델은 특히 큰 모델인 경우 계산이 많이 필요할 수 있습니다. 이를 해결하기 위해 메모리 소비를 줄이고 훈련 속도를 높이는 mixed-precision 훈련과 같은 기술을 사용할 수 있습니다. 또한, 무거운 작업에 대해 작은 모델이나 클라우드 자원을 활용하는 것을 고려할 수 있습니다.\n\n코드 스니펫: BERT와 함께 Mixed-Precision 훈련\n\n```js\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\nwith autocast():\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    outputs = model(**inputs)\n    loss = outputs.loss\n\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 도전 과제 3: 도메인 적응\n\nBERT는 다재다능하지만 특정 도메인에서 최적으로 작동하지 않을 수 있습니다. 이를 해결하기 위해 도메인별 데이터로 BERT를 세밀하게 튜닝하세요. 대상 도메인의 텍스트를 노출함으로써 BERT는 해당 분야에 특정한 뉘앙스와 용어를 이해하도록 학습합니다.\n\n코드 스니펫: BERT를 활용한 도메인 적응\n\n```js\ndomain_data = load_domain_specific_data()  # 도메인별 데이터 로드\ndomain_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\ntrain_domain(domain_model, domain_data)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 도전을 극복하면 BERT의 능력을 효과적으로 활용할 수 있습니다. 만날 수 있는 복잡성과 상관없이요. 마지막 장에서는 여정을 되돌아보며 언어 모델의 미래 발전 가능성을 탐구할 것입니다. BERT로 어떤 것을 달성할 수 있는지 계속해서 경계를 넓혀 보세요!\n\n# 11장: BERT와 함께하는 자연어처리의 미래 방향\n\nBERT를 탐험한 것으로 마무리하면서, 앞으로 언어 모델이 나아갈 흥미로운 방향을 엿보고자 합니다. 다국어 이해부터 교차 언어 학습까지, NLP 풍경을 형성할 전망있는 트렌드들을 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBERT의 능력은 영어에만 국한되어 있지 않습니다. 연구자들은 다국어로 확장하고 있습니다. 다양한 언어로 BERT를 훈련시킴으로써, 다양한 언어로 텍스트를 이해하고 생성할 수 있는 능력을 향상시킬 수 있습니다.\n\n코드 스니펫: Hugging Face Transformers를 사용한 다국어 BERT\n\n```python\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = BertModel.from_pretrained('bert-base-multilingual-cased')\n\ntext = \"BERT는 여러 언어를 이해합니다!\"\ninputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\noutputs = model(**inputs)\n\nembeddings = outputs.last_hidden_state\nprint(embeddings)\n```\n\n# 크로스-모달 학습: 텍스트를 넘어서\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBERT의 문맥 이해 능력은 텍스트에만 국한되어 있지 않습니다. 최근 연구에서는 BERT를 이미지와 오디오와 같은 다른 데이터 형식에도 적용하는 방법을 탐구하고 있습니다. 이러한 교차 모달 학습은 다양한 소스에서 정보를 연결함으로써 보다 심층적인 통찰력을 제공할 수 있습니다.\n\n# 평생 학습: 변화에 적응하기\n\nBERT의 현재 학습은 정적 데이터셋을 기반으로 하지만, 미래의 NLP 모델은 언어 트렌드의 발전에 적응할 것으로 예상됩니다. 평생 학습 모델은 지속적으로 지식을 업데이트하여 언어와 맥락이 변화함에 따라 relevancy를 유지합니다.\n\n코드 스니펫: BERT를 활용한 평생 학습\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom transformers import BertForSequenceClassification, BertTokenizer\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\nnew_data = load_latest_data()  # 최신 데이터 불러오기\nfor epoch in range(epochs):\n    train_lifelong(model, new_data)\n```\n\n# 챗봇의 양극화: 보다 인간다운 대화\n\nGPT-3와 같은 NLP 모델의 발전은 AI와 자연스러운 대화의 잠재력을 보여주고 있습니다. BERT가 맥락과 대화의 이해력을 향상시키면서 더 자연스러운 상호작용의 가능성이 열릴 것으로 기대됩니다.\n\nNLP의 미래는 혁신과 가능성으로 가득한 수채화입니다. 이러한 트렌드를 받아들이는 동안 BERT가 언어 이해의 기본 요철로 남아있어 기술 및 상호작용의 방식을 계속 형성할 것임을 기억하세요. 여러분의 호기심을 유지하고 앞으로 펼쳐질 분야를 탐험해 보세요!\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 제 12 장: Hugging Face Transformers 라이브러리를 사용하여 BERT 구현하기\n\n이제 BERT에 대한 탄탄한 이해를 얻었으니, 이 지식을 실제로 활용할 때입니다. 이 장에서는 Hugging Face Transformers 라이브러리를 사용하여 BERT 및 기타 트랜스포머 기반 모델을 다루는 강력한 도구 상자로 실제 구현에 대해 자세히 살펴보겠습니다.\n\n# Hugging Face Transformers 설치\n\n시작하려면, Hugging Face Transformers 라이브러리를 설치해야 합니다. 터미널이나 명령 프롬프트를 열고 다음 명령을 사용하십시오:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\npip install transformers\n```\n\n# 사전 학습된 BERT 모델 불러오기\n\nHugging Face Transformers를 사용하면 사전 학습된 BERT 모델을 쉽게 불러올 수 있습니다. 다양한 모델 크기와 구성 중에서 선택할 수 있습니다. 텍스트 분류를 위한 기본 BERT 모델을 불러와보겠습니다:\n\n```js\nfrom transformers import BertForSequenceClassification, BertTokenizer\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 텍스트 토크나이징 및 인코딩\n\nBERT는 텍스트를 토큰화된 형태로 처리합니다. 모델에 대한 입력으로 사용하려면 텍스트를 토크나이저를 사용하여 토큰화하고 인코딩해야 합니다:\n\n```js\ntext = \"BERT is amazing!\"\ninputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n```\n\n# 예측하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n텍스트를 인코딩한 후에는 모델을 사용하여 예측할 수 있습니다. 예를 들어, 감성 분석을 수행해 봅시다:\n\n```js\noutputs = model(**inputs)\npredicted_class = torch.argmax(outputs.logits).item()\nprint(\"예측된 감성 클래스:\", predicted_class)\n```\n\n# BERT 파인튜닝\n\n특정 작업을 위해 BERT를 파인튜닝하는 과정은 미리 학습된 모델을 로드하고 작업에 맞추어 조정한 후 데이터셋에 대해 훈련하는 것을 포함합니다. 텍스트 분류를 위한 간단한 예제를 살펴보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom transformers import BertForSequenceClassification, BertTokenizer, AdamW\nimport torch\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntext = \"학습용 샘플 텍스트입니다.\"\nlabel = 1  # 긍정 감정으로 가정합니다.\n\ninputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\noutputs = model(**inputs, labels=torch.tensor([label]))\n\nloss = outputs.loss\noptimizer = AdamW(model.parameters(), lr=1e-5)\nloss.backward()\noptimizer.step()\n```\n\n# 더 많은 작업과 모델 탐구하기\n\nHugging Face Transformers 라이브러리는 다양한 모델과 작업을 탐색할 수 있는 기회를 제공합니다.\nBERT를 텍스트 분류, 개체 인식, 질문 응답 등으로 정교 조정할 수 있습니다.\n\nHugging Face Transformers 라이브러리를 사용해보면 BERT 및 기타 트랜스포머 기반 모델을 프로젝트에 구현하는 데 귀중한 도구라는 것을 알게 될 것입니다.\n이론을 실용적인 응용 프로그램으로 전환하는 여정을 즐기세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론: BERT의 힘을 발휘해보세요\n\n이 블로그 글에서는 BERT(Bidirectional Encoder Representations from Transformers)의 변혁적인 세계를 탐험하는 여정을 시작했습니다. 공식적인 도입부터 실용적인 구현까지, 우리는 BERT가 자연어 처리(NLP)뿐만 아니라 더 나아가 미치는 영향의 풍경을 탐험해 왔습니다.\n\n실제 상황에서 BERT를 활용할 때 발생하는 도전에 대해 탐색하면서, 긴 텍스트를 처리하고 계산 리소스를 관리하는 등의 문제에 대처하는 전략을 발견했습니다. Hugging Face Transformers 라이브러리를 탐험하면서, 여러분의 프로젝트에서 BERT의 힘을 활용할 수 있는 실용적인 도구를 제공해 드렸습니다.\n\n미래를 엿보면, NLP 분야에서 미래에 기대되는 다국어 이해, 교차 모달 학습, 그리고 언어 모델의 지속적인 발전과 같은 무한한 가능성을 엿볼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희의 여정은 여기서 끝나지 않아요. BERT는 기계와 인간의 커뮤니케이션 간격을 좁히며 새로운 언어 이해의 시대를 열었어요. AI의 다이내믹한 세계로 나아가면 BERT가 더 많은 혁신을 위한 발판이 되는 것을 기억해주세요. 더 많은 것을 탐험하고 배우며 창조하세요. 기술의 전선은 끊임없이 확장되고 있답니다.\n\nBERT 탐구 여정에 참여해 주셔서 감사드려요. 학습을 계속할 때 궁금증이 더 큰 미스터리를 해결하게 하고, AI와 NLP의 혁신적인 환경에 기여할 수 있기를 바래요.","ogImage":{"url":"/assets/img/2024-06-19-MasteringBERTAComprehensiveGuidefromBeginnertoAdvancedinNaturalLanguageProcessingNLP_0.png"},"coverImage":"/assets/img/2024-06-19-MasteringBERTAComprehensiveGuidefromBeginnertoAdvancedinNaturalLanguageProcessingNLP_0.png","tag":["Tech"],"readingTime":24},{"title":"프롬프트 엔지니어링 최상의 실천 방법 반복적인 프롬프트 개발","description":"","date":"2024-06-19 20:40","slug":"2024-06-19-PromptEngineeringBestPracticesIterativePromptDevelopment","content":"\n\n대형 언어 모델을 사용하여 애플리케이션을 개발할 때, 첫 번째 시도에서 최종 애플리케이션에 사용할 프롬프트를 만들기 어려울 수 있습니다.\n\n그러나 프롬프트를 점진적으로 개선하는 좋은 프로세스가 있다면, 당신이 원하는 작업에 잘 작동하는 것으로 어느 정도 도달할 수 있을 것입니다.\n\n기계 학습 모델을 훈련할 때, 첫 번째 시도에서 잘 작동하지 않는 경우가 많다고 들어 볼 수도 있습니다. 프롬프팅 또한 보통 처음부터 잘 작동하지 않습니다. 이 글에서는 점진적 개발을 통해 애플리케이션에 동작하는 프롬프트를 얻는 과정을 탐색하겠습니다.\n\n![image](/assets/img/2024-06-19-PromptEngineeringBestPracticesIterativePromptDevelopment_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 목차:\n\n- Prompt 엔지니어링의 반복적 성격\n- 작업 환경 설정 및 시작하기\n- LLM 결과가 너무 길 때 극복하기\n- LLM을 어떤 세부 사항에 집중하도록 유도하기\n- 복잡한 응답 획득\n\n# 1. Prompt 엔지니어링의 반복적 성격\n\n이전에 머신러닝 수업을 들었다면, 머신러닝 개발이 반복적인 과정이라는 다이어그램을 본 적이 있을 것입니다. 데이터를 얻어 모델을 훈련시킨 후 실험 결과를 얻게 됩니다. 그런 다음 출력물을 살펴보고 오류 분석을 수행하여 작동 여부를 파악하고 문제를 해결하거나 해결 방법을 접근하는 방식을 조정할 수도 있습니다. 구현을 변경한 후 다시 실험을 실행하여 효과적인 머신러닝 모델을 도출하기 위해 반복 작업을 거쳐야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Prompt Engineering Best Practices](/assets/img/2024-06-19-PromptEngineeringBestPracticesIterativePromptDevelopment_1.png)\n\n애플리케이션을 개발할 때 LLM을 사용하여 프롬프트를 작성할 때, 프로세스는 꽤 유사할 수 있습니다. 무엇을 하고자 하는지, 완료하고자 하는 작업에 대한 아이디어가 있으며, 명확하고 구체적인 프롬프트를 작성하는 첫 번째 시도를 할 수 있습니다. 그리고 적절하다면 시스템이 생각할 시간을 주는 방식으로 적기를 노령할 수 있습니다.\n\n그런 다음 실행하여 얻는 결과를 확인할 수 있습니다. 처음에 충분히 잘 작동하지 않는 경우에는, 예를 들어, 지시사항이 충분히 명확하지 않거나, 알고리즘이 충분한 시간을 가지지 못한 이유 등을 파악하면서 반복적인 프로세스를 통해 아이디어를 수정하고, 프롬프트를 개선하고, 이 과정을 여러 번 반복하여 애플리케이션에 적합한 프롬프트를 얻을 수 있습니다.\n\n앤드루 엔지\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2. 작업 환경 설정 및 시작하기\n\n작업 환경을 설정하는 것으로 시작해봅시다. OpenAI 및 OS 패키지를 가져와서 OpenAI API 키를 정의하고, 프롬프트를 입력하면 응답을 반환하는 get_completion 함수를 정의할 것입니다.\n\n```js\nimport openai\nimport os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # 로컬 .env 파일 읽기\n\nopenai.api_key  = os.getenv('OPENAI_API_KEY')\n\n\ndef get_completion(prompt, model=\"gpt-3.5-turbo\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=0, # 모델 출력의 랜덤성 정도\n    )\n    return response.choices[0].message[\"content\"]\n```\n\n여기에는 노트북 기술 세부 정보가 포함된 팩트 시트가 있습니다. 구성에 대해 이야기하며, 치수, 노트북 옵션, 재료 등이 포함되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfact_sheet_laptop = \"\"\"\n개요\n\n저희의 극세사한 노트북 시리즈를 소개합니다. 현대적인 작업 환경을 위한 최첨단 디자인과 기능의 상징입니다.\n랩톱, 도킹 스테이션, 액세서리 등을 포함한 합리적인 테크 제품군의 일부입니다.\n다양한 색상과 마감으로 제공되어, 개인적이거나 전문적인 미적 취향과 시원하게 조화됩니다.\n탁월한 내구성과 스타일리시한 매력을 위해 프리미엄 소재로 제작되었습니다.\n집에서의 사무실이든 기업 환경이든 다양한 환경에 적합합니다.\n탁월한 성능과 생산성을 위해 고안되어, 전문가들을 위한 이상적인 선택지입니다.\n디자인\n\n노트북은 여러 색상 옵션으로 제공되어, 개인화할 수 있습니다.\n브러시 알루미늄, 매트 블랙, 광택 와이트 또는 크롬 액센트와 같은 다양한 마감 선택이 가능합니다.\n표준 디스플레이 또는 터치스크린 옵션 중 선택할 수 있습니다.\n백라이트 키보드, 지문 센서 또는 얼굴 인식 기술과 같은 추가 기능 옵션도 있습니다.\n획기적이고 가벼운 디자인으로, 출퇴근용으로 휴대성을 높이면서도 성능을 희생하지 않습니다.\n사양\n\n실행 가능한 Intel/AMD 프로세서로 원활한 멀티태스킹과 고성능 컴퓨팅을 제공합니다.\n35CM 너비, 24CM 깊이, 1.5CM 높이와 같은 초박형 디자인:\n넓이 35CM | 13.78”\n깊이 24CM | 9.45”\n높이 1.5CM | 0.59”\n몰입형 작업 또는 엔터테인먼트 경험에 맞춘 해상도로 선명하고 생동감 넘치는 디스플레이를 제공합니다.\n옵션\n\nSSD 또는 HDD 저장 옵션 중 선택하여 속도와 수용 용량을 개인화할 수 있습니다.\nUSB-C, HDMI 및 Thunderbolt와 같은 다양한 연결 옵션을 제공하여 다양성 있는 호환성을 지원합니다.\n처리 요구 사항에 맞는 다양한 RAM 구성이 가능합니다.\nWindows 및 MacOS 기호를 모두 충족하는 운영 체제 옵션을 제공합니다.\n소재\n\n프리미엄 룩과 견고한 보호를 위해 내구성 있는 알루미늄 케이싱으로 제작되었습니다.\n눈부시지 않게 어두운 조명에서도 편안하게 볼 수 있도록 안티글레어 코팅이 적용된 고해상도 디스플레이입니다.\n장거리 사용 중에도 최적 성능을 보장하기 위한 최첨단 냉각 기술이 탑재되어 있습니다.\n원산지\n\n일본의 현대적인 시설에서 전통 기술과 기술 혁신을 결합하여 정밀하게 제작되었습니다.\n\n\"\"\"\n```\n당신이 이 정보를 가지고 마케팅 팀이 온라인 소매 웹사이트 설명을 작성하는 것을 도와야 한다고 가정해보겠습니다. 여기서 말하는 프롬프트는, 기술사양을 기반으로 한 제품에 대한 소매 웹사이트 설명을 작성하는 마케팅 팀을 도와야 한다면 어떤 제품 설명을 작성할 것인지에 대한 정보를 쓰세요. 기술사양에 제공된 정보를 기반으로 제품 설명을 작성하세요.\n\n기술적 사양: {fact_sheet_laptop}\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 이걸 보면, 좀 긴 것 같다고 생각해. 정확히 요청한 대로 테크니컬 팩트 시트에서 시작하여 제품 설명을 작성하는 데 잘 해냈어. 하지만 이걸 보면, 좀 긴 것 같아. 아마 우리는 조금 더 짧게 원할지도 몰라.\n\n# 3. 너무 긴 LLM 결과 극복하기\n\n이전 섹션에서 프롬프트를 작성하고 결과를 받았어. 너무 길어서 그렇게 만족스럽지 않아. 그래서 나는 내 프롬프트를 명확히 하고, 최대 50단어를 사용해 이에 대한 원하는 길이에 대한 더 나은 지침을 제공하도록 말할 것이야. 다시 실행해보자.\n\n```js\nprompt = f\"\"\"\n마케팅 팀이 소매 웹사이트의 제품에 대한 설명을 만들 수 있도록 도와주는 것이 당신의 작업입니다.\n테크니컬 팩트 시트에 제공된 정보를 기반으로 제품 설명을 작성하십시오.\n제공된 기술 사양을 기반으로 제품 설명을 작성하십시오(삼중 백틱(delimited by triple backticks)).\n\n최대 50단어를 사용하십시오.\n\n기술 사양: {fact_sheet_laptop}\n\"\"\"\nresponse = get_completion(prompt)\nprint(response)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 제품에 대한 아주 멋진 간단한 설명 같아요. 이 응답의 길이를 확인해볼게요. 나는 이 응답을 공백을 기준으로 나누고, 길이를 출력할 거에요.\n\n그러니까 47개의 단어에요. 나쁘지 않아요. 대형 언어 모델은 명확한 단어 수에 대한 지시를 따르는 데 꽤 괜찮지만, 때로는 60이나 65개의 단어로 무언가를 출력할 때도 있지만, 타당한 범위 내에 있어요. 할 수 있는 일 중 일부는 최대 세 문장을 사용하는 것이죠.\n\n\nlen(response.split())\n\n\n# 4. 일부 세부사항에 집중하도록 LLM에 강제하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 텍스트를 웹 사이트에 계속 보왈하면서, 소비자에게 직접 판매하지 않는 웹 사이트인 것을 결정할 수 있습니다. 대신, 랩톱을 랩톱 소매업체에 판매하는 것이 목적이며, 이들은 랩톱의 기술적 세부 사항에 더 관심이 있을 것입니다.\n\n이런 경우, 이 프롬프트를 가져와서 이 프롬프트를 수정하여 랩톱의 기술적 세부 사항에 대해 더 정확하게 명시하고 싶습니다. 나는 이 설명이 랩톱 소매업체를 위해 작성되었으므로 기술적이고 랩톱이 구성된 재료에 초점을 맞춰야 한다고 말할 수 있습니다.\n\n```js\nprompt = f\"\"\"\n마케팅 팀이 기술 사양서를 기반으로 한 \n제품에 대한 소개 설명을 작성하는데 도움을 \n주세요.\n\n기술 사양서에 제공된 정보를 기반으로 제품 \n설명을 작성하십시오. 제품이 구성된 재료에 \n중점을 두어 기술적 성격으로 작성되어야 합니다.\n\n설명의 끝에는 기술 사양서에서 7자리 제품 \nID를 모두 포함하십시오.\n\n최대 50단어로 작성하십시오.\n\n기술 사양서: {fact_sheet_laptop}\n\"\"\"\nresponse = get_completion(prompt)\nprint(response)\n```\n\n프롬프트를 변경함으로써 특정 문자와 특성에 더 초점을 맞출 수 있습니다. 설명 끝에 제품 ID를 추가할 것이라고 결정한 경우, 설명 프롬프트 끝에 기술 사양서에 있는 모든 7자리 제품 ID를 포함할 지시를 추가할 수 있습니다. 이렇게 하고 실행하여 결과를 확인해 보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nprompt = f\"\"\"\n마케팅 팀이 소재에 초점을 맞춰 제품에 대한 상세한 설명을 만들 수 있도록 돕는 것이 당신의 일입니다.\n\n기술 사양서에 제공된 정보를 기반으로 제품 설명을 작성하세요.\n\n제품 설명은 노트북 소매업자를 대상으로 하므로 기술적이며 제품 구성 소재에 중점을 두어야 합니다.\n\n제품 사양서의 7자리 Product ID를 설명 끝에 포함하세요.\n\n단어 수를 최대 50개로 제한합니다.\n\n기술 사양서: {fact_sheet_laptop}\n\"\"\"\nresponse = get_completion(prompt)\nprint(response)\n```\n\n개발자가 많은 경우에 지나칠 수도 있는 반복적인 프롬프트 개발의 짧은 예시를 제공했습니다. 따라서 프롬프트를 처음으로 작성해보고 결과를 살펴본 후 점차적으로 프롬프트를 다듬어 필요한 결과에 점점 가까워지는 것이 좋습니다.\n\n# 5. 복잡한 응답 얻기\n\nchatGPT의 능력에 대한 더 나은 이해를 위해 좀 더 복잡한 프롬프트를 살펴봅시다. 설명 뒤에 제품의 치수를 제공하는 테이블을 포함하도록 요청하고 모든 내용을 HTML로 형식화해야 합니다. 실제로 이러한 프롬프트를 얻기 위해서는 여러 번의 반복한 작업이 필요할 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nprompt = f\"\"\"\n마케팅팀이 상품 기술사양에 기초하여 소매 웹사이트를 위한 제품 설명을 만들 수 있도록 도와주어야 합니다.\n\n3연결 따옴표로 구분된 기술 명세서에 제공된 정보를 기반으로 제품 설명을 작성해 주세요.\n\n설명은 노트북 소매업체를 대상으로 하므로 기술적이며 제품 구성 재료에 중점을 두어야 합니다.\n\n기술 사양에 있는 모든 7자 제품 ID를 설명에 포함하세요.\n\n설명 뒤에 제품의 치수를 제공하는 표를 포함해 주세요. 표에는 두 개의 열이 있어야 합니다. \n첫 번째 열에는 치수의 이름을, 두 번째 열에는 인치 단위로 측정된 치수를 넣어주세요.\n\n표에 '제품 치수'라는 제목을 붙여주세요.\n\n웹사이트에서 사용할 수 있는 HTML 형식으로 모든 것을 서식화해주세요. \n설명을 \u003cdiv\u003e 요소에 넣어주세요.\n\n기술 사양: {fact_sheet_laptop}\n\"\"\"\n\nresponse = get_completion(prompt)\nprint(response)\n```\n\n이 HTML을 표시하여 이게 유효한 HTML인지 확인하고 작동하는지 확인해봅시다.\n\n```js\nfrom IPython.display import display, HTML\ndisplay(HTML(response))\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-PromptEngineeringBestPracticesIterativePromptDevelopment_2.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 만약 이 기사를 좋아하고 저를 지원하고 싶다면, 아래와 같은 내용을 확인해 주세요:\n\n- 👏 기사에 박수를 보내주세요 (50번) - 이 기사가 주목받을 수 있도록 도와주세요\n- To Data \u0026 Beyond 뉴스레터를 구독해주세요\n- 저를 Medium에서 팔로우해주세요\n- 📰 제 Medium 프로필에서 더 많은 콘텐츠를 확인해주세요\n- 🔔 저를 팔로우해주세요: LinkedIn | Youtube | GitHub | Twitter\n\n## 제 뉴스레터 To Data \u0026 Beyond를 구독하여 제 기사에 대한 전체적이고 일찍 접근하세요:\n\n## 데이터 과학과 인공지능에서의 경력을 시작하려는데 걸리는데 어려움을 겪고 계신가요? 데이터 과학 멘토링 세션과 장기적인 경력 멘토링을 제공하고 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 멘토링 세션: [링크](https://lnkd.in/dXeg3KPW)\n- 장기 멘토링: [링크](https://lnkd.in/dtdUYBrM)\n\n![이미지](/assets/img/2024-06-19-PromptEngineeringBestPracticesIterativePromptDevelopment_3.png)","ogImage":{"url":"/assets/img/2024-06-19-PromptEngineeringBestPracticesIterativePromptDevelopment_0.png"},"coverImage":"/assets/img/2024-06-19-PromptEngineeringBestPracticesIterativePromptDevelopment_0.png","tag":["Tech"],"readingTime":8},{"title":"LLM Alignments Part 21 RLAIF","description":"","date":"2024-06-19 20:39","slug":"2024-06-19-LLMAlignmentsPart21RLAIF","content":"\n\n안녕하세요!\n\n오늘의 주제는 조금 가벼울 수 있지만, 이미 RLHF에 대해 다뤘으니 이제 RLAIF에 대해 이야기해야 합니다. RLAIF이 점점 더 보편화되는 것이 중요하기 때문이죠.\n\n![이미지](/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_0.png)\n\nRLAIF의 개념은 간단합니다: RLHF의 \"H\" (Human)를 AI로 교체하는 것만을 의미합니다. 이 전환이 필요한 이유는 인간으로부터 데이터를 수집하는 것이 시간이 많이 소요되고 비용이 많이 들며 확장하기 어려울 수 있기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM 기반 에이전트의 효과가 입증되었으며, 사고 체인(Chain of thought, CoT)에서 사고 트리(Tree of thought, ToT), ReAct, Reflexion 및 기타 다양한 요소들까지, LLM을 강화 학습(Reinforcement Learning, RL), 검색 보강 생성(Retrieval-augmented generation, RAG) 또는 유사한 프레임워크에 통합함으로써 추론 성능을 크게 향상시킬 수 있다는 것이 명백해졌습니다.\n\n이 논문에서 강조된 바에 따르면, 데이터 생성을 위해 AI를 사용하는 것은 회귀로 이어지지 않으며, 실제로 특정 작업에서 RLHF를 능가할 수도 있습니다.\n\n다음은 위에서 설명한 RLHF와 RLAIF의 차이를 더 자세히 보여주는 다이어그램입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_2.png)\n\nLLM(라지앤 러닝 모델)을 사용한 에이전트의 관점에서 대안적인 접근 방식은 학습된 보상 모델(RM)을 입력으로 사용자의 선호도를 받아들이고 엔지니어링된 프롬프트와 에이전트 아키텍처를 통해 점수를 출력하는 에이전트로 대체하는 것입니다.\n\n참고:\n- RLHF로부터 학습된 RM은 종종 SFT 모델에서 증류된 학습을 통해 훈련되기 때문에 증류된 RM으로 언급됩니다.\n- 에이전트 스타일의 RM은 훈련을 필요로 하지 않기 때문에 직접 RM으로 언급됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오른쪽 표를 보면 직접 RM의 성능이 간접 RM의 성능과 일치한다는 것을 보여줍니다. 여기서 '동일 크기의 RLAIF'는 인공지능에 의해 생성된 교육 데이터가 RLHF에서 사용된 것과 동일한 크기인 간접 RM을 나타냅니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_3.png\" /\u003e\n\n마지막으로 RLHF와 RLAIF를 비교합니다. 요약 및 유용성 작업에서 성능이 일치하는 방법과 RLAIF가 무해성 측면에서 RLHF를 능가하는 것을 주목해주세요.\n\n오늘은 여기까지입니다! 다음에는 DPO에 대해 이야기해볼 수 있겠네요~\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n참고:\n\nRLAIF: 인공지능 피드백을 활용한 인간 피드백으로 강화 학습 확장","ogImage":{"url":"/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_0.png"},"coverImage":"/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_0.png","tag":["Tech"],"readingTime":2},{"title":"LLM의 내부 작업 공개 고유 값 관점","description":"","date":"2024-06-19 20:37","slug":"2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective","content":"\n\n## Llama3–8B 투영 행렬에 대한 특이값 분해 분석\n\n![이미지](/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_0.png)\n\nLLM이 얼마나 잘 훈련되었는지 생각해 보셨나요? 매개변수의 수가 많은데, 그 매개변수들이 훈련 데이터로부터 정보나 지식을 최대한으로 얻어내고 있는지 궁금해 하시지 않나요? 그렇지 않다면, LLM에서 유용하지 않은 매개변수들을 제거하여 더 효율적으로 만들 수 있을까요?\n\n이 글에서는 Singular Values 관점에서 Llama-3–8B 모델을 깊게 분석하여 이러한 질문에 답해보겠습니다. 더 이상 시간을 낭비하지 말고 편안하게 앉아, SVD를 적용하여 Llama-3–8B 행렬의 품질을 분석해 보세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# SVD 다시 살펴보기\n\n특이값 분해(SVD)에서 행렬 A는 세 가지 다른 행렬로 분해됩니다:\n\n여기서:\n\n- A는 원래 행렬입니다.\n- U는 A의 왼쪽 특이벡터인 열로 이루어진 행렬입니다.\n- Σ은 A의 특이값을 포함하는 대각행렬입니다. 이 값들은 항상 음이 아닌 값이며 일반적으로 가장 큰 값부터 가장 작은 값 순서로 정렬됩니다.\n- V_t는 V의 전치행렬이며, V의 열은 A의 오른쪽 특이벡터입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 간단한 용어로 설명하면, 특이값 분해(SVD)는 행렬의 복잡한 변환을 간단하고 이해하기 쉬운 회전 및 스케일링 과정으로 나누어 줍니다. Σ의 특이값은 스케일링 요소를 알려주고 U와 V_t의 특이벡터는 해당 스케일링이 행렬을 적용하기 전과 후의 방향을 알려줍니다.\n\n특이값은 행렬이 공간에서 다양한 방향으로 얼마나 늘어나거나 줄어드는지를 측정하는 방법으로 생각할 수 있습니다. 각 특이값은 특이벡터 쌍에 해당되며, 하나는 오른쪽 특이벡터(입력 공간에서의 방향), 다른 하나는 왼쪽 특이벡터(출력 공간에서의 방향)입니다.\n\n행렬의 특이값이 급격하게 감소하는 경우(가장 큰 특이값이 작은 것들보다 현저히 큰 경우), 이는 행렬의 유효 랭크(중요한 특이값의 수)가 실제 행렬의 차원보다 훨씬 작다는 것을 의미합니다. 이는 행렬이 낮은 랭크 행렬로 잘 근사될 수 있음을 시사합니다.\n\nLLM(대형 언어 모델)의 맥락에서, 가중치 행렬(예: 어텐션 메커니즘 또는 피드포워드 레이어의 행렬)들은 입력 데이터(예: 단어 임베딩)를 출력 표현으로 변환합니다. 주요한 특이값은 변환에 의해 가장 강조되는 입력 공간의 방향을 나타내며, 모델이 민감하거나 표현력이 강한 방향을 보여줍니다. 작은 특이값은 변환에서 중요하지 않거나 영향력이 적은 방향을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특이값의 분포는 모델의 일반화 능력과 견고성에 영향을 줄 수 있습니다. 느린 감소(많은 큰 특이값)는 과적합을 초래할 수 있으며, 빠른 감소(소수의 큰 특이값)는 과소적합이거나 정보의 손실을 나타낼 수 있습니다.\n\n# Llama-3 아키텍처 재방문\n\n다음은 meta-llama/Meta-Llama-3-8B-Instructmodel의 config.json 파일입니다. 이 LLM은 8개의 num_key_value_heads를 사용하여 Grouped Query Attention을 활용하며, 이는 그룹 크기가 32/8=4임을 의미합니다.\n\n```js\n{\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.40.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# (Q, K, V, O) 행렬의 특이값 분석\n\n자, 이제 이 기사의 본격적인 내용으로 들어가 봅시다. Llama-3–8B-Instruct 모델의 (Q, K, V, O) 행렬들을 그들의 특이값을 통해 분석해 보겠습니다!\n\n## 코드\n\n우선, 이 분석에 필요한 모든 패키지를 가져와 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport transformers\nimport torch\nimport numpy as np\nfrom transformers import AutoConfig, LlamaModel\nfrom safetensors import safe_open\nimport os\nimport matplotlib.pyplot as plt\n```\n\n그런 다음, 모델을 다운로드하고 로컬 /tmp디렉토리에 저장합시다.\n\n```js\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n!huggingface-cli download {MODEL_ID} --quiet --local-dir /tmp/{MODEL_ID}\n```\n\n만약 GPU를 많이 가지고 계신 분이시라면, 다음 코드는 관련이 없을 수 있습니다. 그러나 저와 같이 GPU가 부족한 분들에겐, LLama-3–8B 모델의 특정 레이어만 로드하는 데 매우 유용할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef load_specific_layers_safetensors(model, model_name, layer_to_load):\n    state_dict = {}\n    files = [f for f in os.listdir(model_name) if f.endswith('.safetensors')]\n    for file in files:\n        filepath = os.path.join(model_name, file)\n        with safe_open(filepath, framework=\"pt\") as f:\n            for key in f.keys():\n                if f\"layers.{layer_to_load}.\" in key:\n                    new_key = key.replace(f\"model.layers.{layer_to_load}.\", 'layers.0.')\n                    state_dict[new_key] = f.get_tensor(key)\n\n    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n    if missing_keys:\n        print(f\"Missing keys: {missing_keys}\")\n    if unexpected_keys:\n        print(f\"Unexpected keys: {unexpected_keys}\")\n```\n\n이렇게 하는 이유는 Google Colab GPU의 무료 티어로는 LLama-3-8B를 fp16 정밀도로도 불러올 수 없기 때문입니다. 또한, 이 분석은 np.linalg.svd가 구축된 방식으로 인해 fp32 정밀도에서 작동해야 합니다. 다음으로, 주어진 matrix_type, layer_number 및 head_number에 대해 특이값을 얻는 메인 함수를 정의할 수 있습니다.\n\n```js\ndef get_singular_values(model_path, matrix_type, layer_number, head_number):\n    \"\"\"\n    Llama-3 모델의 지정된 행렬의 특이값을 계산합니다.\n\n    Parameters:\n    model_path (str): 모델 경로\n    matrix_type (str): 행렬 유형 ('q', 'k', 'v', 'o')\n    layer_number (int): 레이어 번호 (0에서 31까지)\n    head_number (int): 헤드 번호 (0에서 31까지)\n\n    Returns:\n    np.array: 특이값의 배열\n    \"\"\"\n    assert matrix_type in ['q', 'k', 'v', 'o'], \"잘못된 행렬 유형\"\n    assert 0 \u003c= layer_number \u003c 32, \"잘못된 레이어 번호\"\n    assert 0 \u003c= head_number \u003c 32, \"잘못된 헤드 번호\"\n\n    # RAM이 제한되어 있어 사용한 후에도 fp16을 사용해도 제한된 레이어만을 위해 모델을로드합니다.\n    config = AutoConfig.from_pretrained(model_path)\n    config.num_hidden_layers = 1\n    model = LlamaModel(config)\n    load_specific_layers_safetensors(model, model_path, layer_number)\n\n    # 지정된 레이어에 액세스합니다.\n    # 특정 레이어를로드했으므로 항상 인덱스 0을 사용합니다.\n    layer = model.layers[0]\n\n    # 각 헤드의 크기 결정합니다.\n    num_heads = layer.self_attn.num_heads\n    head_dim = layer.self_attn.head_dim\n\n    # 지정된 행렬에 액세스합니다.\n    weight_matrix = getattr(layer.self_attn, f\"{matrix_type}_proj\").weight.detach().numpy()\n    if matrix_type in ['q','o']:\n        start = head_number * head_dim\n        end = (head_number + 1) * head_dim\n    else:  # 'k', 'v' matrices\n        # num_key_value_heads로 나눠 헤드 번호를 조절합니다.\n        # llama3-8b는 그룹화된 쿼리 어텐션을 사용하기 때문에 수행됩니다.\n        num_key_value_groups = num_heads // config.num_key_value_heads\n        head_number_kv = head_number // num_key_value_groups\n        start = head_number_kv * head_dim\n        end = (head_number_kv + 1) * head_dim\n\n    # 지정된 헤드에 대한 가중치를 추출합니다.\n    if matrix_type in ['q', 'k', 'v']:\n        weight_matrix = weight_matrix[start:end, :]\n    else:  # 'o' matrix\n        weight_matrix = weight_matrix[:, start:end]\n\n    # 특이값 계산합니다.\n    singular_values = np.linalg.svd(weight_matrix, compute_uv=False)\n\n    del model, config\n\n    return list(singular_values)\n```\n\nHuggingFace에서 구현된 방식으로 인해 K, Q 및 V 행렬에 대한 지정된 헤드의 가중치를 추출할 수 있는 이유는 행별로 슬라이싱을 통해할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_1.png)\n\nO 행렬의 경우 선형 대수를 통해 O 가중치에서 지정된 헤드에 대한 가중치를 추출하기 위해 열별로 슬라이싱을 할 수 있습니다! 자세한 내용은 다음 그림에서 확인할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_2.png)\n\n## 결과\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n분석을 위해 다양한 헤드, 레이어 및 행렬 유형에서 get_singular_values() 함수를 실행해야 합니다. 그리고 이러한 다양한 조합을 비교할 수 있도록 분석을 위한 여러 보조 지표도 정의해야 합니다:\n\n- 상위 10개 비율: 상위 10개 특이값의 합과 모든 특이값의 합 사이의 비율\n- 첫 번째/마지막 비율: 가장 높은 특이값과 가장 낮은 특이값 간의 비율\n- 최소 10개 비율: 최소 10개 특이값의 합과 모든 특이값의 합 사이의 비율\n\n(레이어 0, 헤드 0) 분석\n\n- Q(쿼리) 행렬은 초기 최대 특이값(약 10)을 갖고 있으며, 다음으로 K(키) 행렬(약 8)이 있습니다. 이 2개의 행렬은 초기 특이값이 V(값)와 O(출력) 행렬보다 현저히 높습니다.\n- 초기 특이값 뿐만 아니라, Q와 K 행렬의 상위 10개 비율과 첫 번째/마지막 비율을 확인하면, 이 두 행렬이 V와 O 행렬보다 훨씬 높은 값을 갖는다는 것을 알 수 있습니다. 이는 Q와 K 행렬이 대부분의 차원에 집중된 정보를 포함하고 있으며, V와 O 행렬은 정보가 구성요소 전반에 분산되어 있는 것을 시사합니다.\n- 최소 10개 비율을 살펴보면, Q와 K 행렬의 특이값이 거의 0에 가깝고 V와 O 행렬에 비해 상대적으로 훨씬 낮다는 것을 알 수 있습니다. 이는 Q와 K 행렬이 저랭크 구조를 가지고 있음을 나타내는 증거 중 하나이며, 이 차원들이 모델의 전반적인 성능에 미미한 영향을 미칩니다. 이러한 가중치는 구조적으로 제거하여 모델의 정확도에 큰 영향을 미치지 않는 경우가 있을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## (레이어 0, 다중 헤드) 분석\n\n- 헤드 번호가 증가함에 따라 Q 및 K 행렬의 상위 10 비율은 V 및 O 행렬보다 훨씬 빠른 속도로 증가하는 경향이 있습니다. 이 관찰 결과는 Q 및 K 행렬의 최하 10 비율에도 동일하게 적용되며, 헤드 번호가 증가함에 따라 값이 0에 가까워지는 경향을 보입니다. 그러나 V 및 O 행렬에는 해당 경향이 나타나지 않습니다.\n- 이 결과는 헤드 번호가 높은 헤드의 Q 및 K 행렬이 낮은 차원에서 정보를 저장하는 경향이 있다는 것을 나타냅니다. 다시 말해, 헤드 번호가 증가함에 따라 Q 및 K 행렬은 더 적은 차원에서 정보를 저장하려고 합니다.\n\n## 교차-레이어 분석\n\n- 더 깊은 레이어로 갈수록, Q 및 K 행렬의 초기값이 감소되는 경향을 발견했지만, 여전히 V 및 O 행렬과 비교하면 비교적 높습니다.\n- 더 깊은 레이어로 갈수록, 특정 헤드의 Q 및 K 행렬의 상위 10 비율 및 첫 번째/마지막 비율에 대한 하락 트렌드 패턴이 나타납니다. 또한 최하 10 비율의 약간의 상승 트렌드 패턴이 있습니다. 이는 더 깊은 레이어의 Q 및 K 행렬이 낮은 레이어와 비교하여 더 잘 훈련된 것으로 나타납니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- \"레이어 0, 다중 헤드\" 섹션에서 발견한 동일 레이어 내의 헤드 간 패턴은 더 깊은 레이어로 이동할 때 명확하지 않습니다. \n\n요약\n\n- K 및 Q 행렬은 V 및 O 행렬과 비교하여 상대적으로 낮은 순위를 가지고 있습니다. 가지치기(pruning) 또는 차원 축소 방법을 수행하려면 K 및 Q 행렬에 더 집중할 수 있습니다.\n- 레이어가 깊어질수록 모든 (K, Q, V, O) 행렬이 더 잘 훈련됩니다. 가지치기 또는 차원 축소 방법을 수행하려면 낮은 레이어에 더 집중할 수 있습니다.\n- 가지치기 외에도 초기 몇 레이어에서만 전체 미세 조정을 수행하거나 LoRA로도 이를 수행하는 것이 흥미로울 수 있습니다.\n\n# 마무리 말씀\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_3.png)\n\n이 시점까지 참석해 주셔서 축하드립니다! 이 기사에서 새로운 것을 배우셨으면 좋겠습니다. 선형 대수의 좋은 오래된 개념들을 적용하여, LLM의 훈련이 얼마나 잘 이루어졌는지 이해하는 것은 정말 흥미롭습니다.\n\n이 유형의 콘텐츠를 좋아하신다면, 저의 Medium 계정을 팔로우해주시어 앞으로의 다른 글 알림을 받아보세요.\n\n# 저자 소개\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n루이스 오웬은 인도네시아 출신의 데이터 과학자 및 AI 연구 엔지니어로, 항상 새로운 지식에 굶주립니다. 그의 경력 여정을 통해 그는 비영리 단체, 전자 상거래, 대화형 AI, OTA, 스마트 시티 및 핀테크 등 다양한 산업 분야에서 일해 왔습니다. 일 안에서 해외에선, 그는 자신의 기사나 멘토링 세션을 통해 데이터 과학 애호가들이 데이터 과학자로 성장할 수 있도록 시간을 보내는 것을 즐깁니다.\n\n지금은 루이스가 전 세계적인 CX 자동화 플랫폼 인 Yellow.ai의 NLP 연구 엔지니어로 일하고 있습니다. 루이스의 웹사이트를 방문하여 그에 대해 더 알아보세요! 마지막으로, 궁금한 점이나 이야기할 주제가 있으면 망설이지 마시고 LinkedIn을 통해 루이스에게 연락해보세요.","ogImage":{"url":"/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_0.png"},"coverImage":"/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_0.png","tag":["Tech"],"readingTime":9},{"title":"위에서 바라본 Transformer 구조","description":"","date":"2024-06-19 20:35","slug":"2024-06-19-TheTransformerArchitectureFromaTopView","content":"\n\n지금까지 사용되던 최신 언어 처리(NLP) 모델은 다른 모델들 중에서 순환 신경망(RNN)이었습니다.\n\n그런데 그 이후로 트랜스포머가 등장했습니다.\n\n트랜스포머 아키텍처는 이전 RNN과 비교하여 자연어 처리 성능을 크게 향상시켰습니다.\n\n2017년 Vaswani 등이 발표한 논문 \"Attention is All You Need\"에서 개발된 트랜스포머는 자가 주의 메커니즘을 활용하여 문장 내 모든 단어의 관련성과 문맥을 학습할 수 있게 되어 NLP를 혁신했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRNN(Recurrent Neural Networks)이 데이터를 순차적으로 처리하는 것과는 달리, Transformer는 문장의 모든 부분을 동시에 분석합니다. 이 병렬 처리 능력 덕분에 Transformer는 문장이나 문서에서 각 단어의 맥락과 관련성을 모든 다른 단어에 대해 학습할 수 있습니다. 이는 RNN에서 발견되는 장기 의존성과 계산 효율성과 관련된 한계를 극복하는 데 도움이 됩니다.\n\n하지만 이 아키텍처를 단계별로 살펴보겠습니다.\n\n![Transformer Architecture](/assets/img/2024-06-19-TheTransformerArchitectureFromaTopView_0.png)\n\n- Transformer 아키텍처에는 두 가지 구성 요소가 있습니다: 인코더(Encoder)와 디코더(Decoder).\n- 이러한 구성 요소들은 함께 작동하며 여러 유사성을 공유합니다.\n- 인코더: 토큰 시퀀스의 입력을 각 토큰의 맥락을 포착하는 풍부하고 연속적인 표현으로 변환합니다. 그 출력은 임베딩 벡터의 시퀀스이며, 종종 숨겨진 상태 또는 컨텍스트로 불립니다.\n- 디코더: 인코더의 숨겨진 상태를 사용하여 반복적으로 한 번에 하나씩 토큰의 출력 시퀀스를 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTransformer 아키텍처에는 Encoder와 Decoder 두 가지가 모두 존재하지만, Encoder만 사용하는 경우, Decoder만 사용하는 경우, 또는 둘 다 사용하는 경우 등 3가지 유형의 트랜스포머가 있습니다.\n\n## Encoder-only Transformers\n\n- 이러한 모델은 텍스트 블록을 깊게 이해하고 해석할 수 있는 전문가 분석가로 생각할 수 있습니다.\n- 이러한 모델은 텍스트 입력 시퀀스를 풍부한 숫자 표현으로 변환하여 텍스트 분류나 명명된 개체 인식(NER)과 같은 작업에 적합합니다.\n- BERT 및 RoBERTa, DistilBERT와 같은 변형들은 이러한 아키텍처 클래스에 속합니다.\n- 이러한 모델은 양방향 어텐션을 사용합니다. 단어 주변의 전체 문맥에 주의를 기울이도록 설계되었습니다.\n\n## Decoder-only Transformers\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 여기서 모델들을 느슨하게 번역의 역할을 수행하는 창의적인 이야기꾼으로 상상해봅시다.\n- \"트랜스포머를 학습하는 것은...\"과 같은 텍스트 자극이 주어지면, 이러한 모델들은 가장 가능성 있는 다음 단어를 예측하며 시퀀스를 자동으로 완성합니다 (바람직하게는 \"즐겁다\").\n- GPT 모델 패밀리는 이 범주에 속합니다.\n- 이 구조에서 주어진 토큰에 대해 계산된 표현은 미래를 예측하기 위해(자기 회귀적 주의라고도 함) 한 부분에 의존함과 동시에 왼쪽 컨텍스트(지금까지의 이야기)에만 의존합니다.\n\n## 인코더-디코더 트랜스포머\n\n- 이들은 변환기 패밀리의 다재다능한 멀티태스커입니다. 하나의 형태에서 텍스트를 다른 형태로 변환하는 능력이 있습니다.\n- 입력 텍스트를 먼저 소화하여, 그 본질과 뉘앙스를 잡는 것(인코더 덕분)을 하고 이를 깊이 이해하여 디코더 부분에서 새로운 텍스트 조각을 응답으로 만들어냅니다.\n- 기계 번역 및 요약 작업에 적합합니다.\n- 이 범주에 속하는 트랜스포머 모델로는 T5와 BART가 있습니다.\n\n# 1. 토크나이저\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-19-TheTransformerArchitectureFromaTopView_1.png\" /\u003e\n\n- 모델로 텍스트를 처리하기 전에 첫 번째 단계는 토큰화입니다.\n- 이 단계는 컴퓨터가 단어를 해석할 수 있도록 돕습니다. 각 고유한 토큰은 고유한 번호를 갖게 됩니다.\n- 모델을 학습시키기 위해 토크나이저를 선택했다면, 텍스트를 생성할 때도 동일한 토크나이저를 사용해야 합니다.\n- 이제 입력을 임베딩 레이어에 전달할 수 있습니다.\n\n# 2. 임베딩 레이어\n\n\u003cimg src=\"/assets/img/2024-06-19-TheTransformerArchitectureFromaTopView_2.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 이미지를 참고하면 내부에는 하나의 임베딩 레이어를 엿볼 수 있지만 두 레이어가 동일합니다.\n\n- 임베딩 레이어는 토큰화된 숫자 표현을 밀도 있는 벡터 임베딩으로 변환합니다.\n- 학습 가능한 벡터 임베딩 공간은 각 토큰이 벡터로 표현되고 해당 공간 내에서 고유한 위치를 차지하는 고차원 공간입니다.\n- 어휘 사전의 각 토큰은 다차원 벡터에 매칭되며(예: 크기가 512인), 이러한 벡터는 입력 시퀀스의 개별 토큰의 의미와 맥락을 인코딩하는 것을 학습하는 것입니다.\n\n## 포지셔널 임베딩\n\n- 원시 임베딩과 결합하여 위치 정보를 추가합니다.\n- 모델은 각 입력 토큰을 병렬로 처리합니다.\n- 포지셔널 임베딩을 통해 모델은 단어 순서에 대한 정보를 얻으며, 텍스트를 이해하려고 할 때 매우 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 3. 인코더\n\n![이미지](/assets/img/2024-06-19-TheTransformerArchitectureFromaTopView_3.png)\n\n- 먼저, 트랜스포머에는 단일 인코더가 아니라 서로 옆에 많은 인코더 스택이 있는 것이 중요합니다. 모든 인코더는 동일합니다. 예를 들어, BERT는 24개의 인코더 스택을 가지고 있습니다.\n- 임베드 레이어에서의 임베딩 시퀀스는 인코더의 입력이며, 먼저 멀티 헤드 셀프 어텐션 레이어에 공급되고 그 다음에 완전히 연결된 피드포워드 레이어에 공급됩니다.\n- 출력은 토크나이저 사전의 각 가능한 토큰에 대한 확률 점수에 비례하는 로짓의 벡터입니다.\n\n## 멀티 헤드 셀프 어텐션\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 인코더 내부의 이 레이어는 특정 작업을 수행합니다: 문장 속 각 단어를 그 자체로 이해하는 것뿐만 아니라 다른 모든 단어와도 함께 이해하는 것입니다.\n\n그럼, 왜 \"다중 헤드\"일까요?\n\n- 단순히 이 관계를 한 가지 방법으로만 보지 않기 때문입니다. 대신, 다중 \"헤드\"를 갖고 있어서 각각이 문장을 다른 관점에서 바라볼 수 있습니다.\n- 한 헤드는 문법 구조에 집중할 수 있고, 다른 헤드는 특정 용어의 의미에 초점을 맞출 수 있으며, 또 다른 헤드는 문장의 어조에 집중할 수 있습니다.\n- 이러한 다양한 측면을 동시에 검토함으로써, 모델은 텍스트를 더 깊이 이해할 수 있습니다.\n\n## 전방향\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 이는 두 층으로 구성된 완전 연결 (밀집) 신경망 구조입니다.\n- 임베딩 시퀀스 전체를 하나의 벡터로 처리하지 않습니다.\n- 각 임베딩은 개별적으로 처리됩니다.\n- 변환된 임베딩이 출력됩니다.\n- 그런 다음, Transformer의 최종 출력 계층을 통해 (이 전방향 계층 자체 내부가 아닌) 로짓이 생성되며, 이는 전체 모델 구조의 문맥 내 토큰 확률에 비례합니다.\n- 다른 신경망과 마찬가지로 활성화 함수를 사용해야 합니다. 이 경우 GELU가 사용됩니다.\n- GELU는 자연어 처리에서 만나는 데이터 분포 유형에 특히 효과적인 방식으로 선형 및 비선형 변환 사이의 균형을 유지하면서 비선형성을 도입할 수 있습니다.\n\n# 4. 디코더\n\n![그림](/assets/img/2024-06-19-TheTransformerArchitectureFromaTopView_4.png)\n\n인코더와 유사하게 디코더도 많은 디코더들의 스택으로 구성되어 있으며(인코더-디코더 모델의 인코더 수와 동일), 서로 동일합니다. 예를 들어 (디코더 전용인) GPT-2 Extra Large 모델은 48개의 디코더 레이어 스택을 가지고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n👋 디코더와 인코더의 주요 차이점은 디코더에는 두 가지 어텐션 서브레이어가 있다는 것입니다:\n\n### 마스킹된 멀티헤드 셀프 어텐션\n\n- 각 시간 단계에서 생성되는 토큰이 과거 출력 및 현재 예측 중인 토큰에만 기초함을 보장합니다. 이것이 \"마스킹\" 뒤의 개념입니다.\n- 이것이 없으면 디코더가 훈련 중에 간단히 대상 번역을 복사함으로써 속일 수 있습니다.\n- 멀티헤드는 인코더와 동일합니다. 각 헤드는 데이터의 다른 측면을 배우며 시퀀스의 다른 부분에 초점을 맞추고 토큰 사이의 다양한 관계를 고려합니다.\n\n### 인코더-디코더 어텐션\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 이 레이어는 각 출력 시퀀스 토큰을 생성하는 동안 입력 시퀀스의 서로 다른 부분(두 가지 다른 언어 같은)에 초점을 맞출 수 있도록 디코더에 가능하게 합니다.\n- 디코더는 출력 시퀀스에서 다음 토큰을 생성하면서 현재 문맥과 지금까지 생성한 내용을 고려합니다.\n- 이를 통해 디코더는 다음 출력 토큰의 생성에 영향을 미쳐야 할 가장 관련성 높은 입력 시퀀스 부분을 파악할 수 있습니다.\n\n디코더의 출력은 토큰화 사전에 있는 각 토큰의 확률 점수(모두 1에 모두 더해짐)이며 더 높은 확률을 가진 토큰이 반환됩니다.\n\n# 연락을 유지하자\n\n➥이와 같은 콘텐츠 더 보려면 Medium에서 팔로우하세요.\n➥LinkedIn이나 𝕏에서 연결합시다.\n➥제 GitHub을 확인하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 다음에 무엇을 읽을지 고민이세요? 여기 두 가지 추천이 있어요:\n\n## 이 주제에 대해 더 깊게 알아갈 수 있는 멋진 참고 자료와 자원:\n\n- Tunstall, Lewis, Leandro Von Werra, 그리고 Thomas Wolf. Natural language processing with transformers. “ O’Reilly Media, Inc.”, 2022.\n- [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n- [YouTube - Illustrated Transformer](https://youtu.be/-QH8fRhqFHM?si=GWPYodFv60vQFNEb)","ogImage":{"url":"/assets/img/2024-06-19-TheTransformerArchitectureFromaTopView_0.png"},"coverImage":"/assets/img/2024-06-19-TheTransformerArchitectureFromaTopView_0.png","tag":["Tech"],"readingTime":6},{"title":"질문-답변 캐글 대회에서 Sentence Transformer를 활용하기","description":"","date":"2024-06-19 20:34","slug":"2024-06-19-Question-AnswerKaggleCompetitionusingSentenceTransformer","content":"\n\n이 대회는 DataTalks.Club 코스의 Q\u0026A 비디오에서 얻은 독특한 데이터셋을 활용하여 참가자들에게 질문과 정확한 답변을 맞추도록 도전합니다.\n\n위 노트북에서는 Pandas를 사용하여 CSV 파일의 데이터를 정리하고 분석하여 데이터프레임에 저장하는 방법을 사용했습니다. 이를 위해 sentence transformer를 활용하였습니다. 첫 번째로, 해당 usecase에 대해 triplet loss를 사용해 보았는데, 이는 지도 학습 유사성 또는 메트릭 학습에 가장 많이 사용되는 손실 함수 중 하나입니다. 가장 간단하게 설명하면, Triplet Loss는 유사하지 않은 쌍이 비슷한 쌍에서 최소한 일정한 여백 값만큼 떨어져 있도록 장려합니다.\n\n이 때의 점수는 0.69이었는데, Triplet pair는 (질문, 답변, 다른 가능한 답변(100% 관련은 아님))이었습니다. Triplet loss는 질문과 답변의 임베딩을 가깝게 정렬하고 다른 가능한 답변은 멀게 밀어냅니다.\n\n다음 시도한 단계는 다른 여러 가능한 답변을 추가하는 것이었습니다. 예를 들어, 각 질문에 대해 같은 질문과 답변에 대한 세 가지 다른 가능한 답변이있는 샘플이 세 개씩 포함되었습니다. 이것은 학습이 잘 되지 않았고, 0.5648의 점수가 나왔는데, 이는 질문, 답변 쌍 당 하나의 샘플만 사용하는 것보다 훨씬 낮은 점수입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로, [(a1, b1), …, (an, bn)]를 사용하는 MultipleNegativesRankingLoss를 사용하여 간단하게 만들고 싶습니다. 여기서 (ai, bi)가 유사한 문장이며 (ai, bj)가 다른 문장으로 가정됩니다. 여기서 i != j 입니다.\n\n이는 (ai, bi) 사이의 거리를 최소화하고 동시에 모든 i != j에 대해 (ai, bj)의 거리를 최대화합니다. 이렇게 하면 부정적인 것을 지정할 필요가 없고 i != j의 경우 거리를 자동으로 최대화합니다. 성능이 향상되었다 0.962를 얻었습니다.\n\n질문당 하나의 샘플을 가진 triple loss에 대한 KaggleNotebook\n\n질문당 여러 샘플을 가진 triple loss에 대한 KaggleNotebook.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 작업을 더 개선하려면 쌍을 사용하는 대신 삼중체를 추가하고 세 번째는 \"하드-부정사\"여야 합니다. 이때, 어휘 수준에서는 (a1, b1)과 유사하지만 의미 수준에서는 (a1, b1)과 유사하지 않아야 합니다.","ogImage":{"url":"/assets/img/2024-06-19-Question-AnswerKaggleCompetitionusingSentenceTransformer_0.png"},"coverImage":"/assets/img/2024-06-19-Question-AnswerKaggleCompetitionusingSentenceTransformer_0.png","tag":["Tech"],"readingTime":2},{"title":"RAGRetrieval Augmented Generation 소개 및 응용 데모","description":"","date":"2024-06-19 20:32","slug":"2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos","content":"\n\n# RAG 소개\n\n안녕하세요. 오늘은 검색 증강 생성 (RAG)에 대한 소개와 몇 가지 응용 프로그램을 시연하겠습니다. 이 토크의 자료는 아래의 GitHub 저장소에서 확인할 수 있어요.\n\n해당 저장소에서 제 발표용 슬라이드가 포함된 PDF 파일을 찾을 수 있습니다. 이후에는 RAG를 적용하기 위해 몇 가지 실습을 진행할 것입니다. 코드와 데이터는 GitHub 저장소에서 모두 제공되므로 함께 따라해볼 수 있어요.\n\nRAG 개요부터 시작해봅시다. RAG는 대형 언어 모델을 개선하는 강력한 기술입니다. 제 생각에는 대형 언어 모델을 최상의 방식으로 적용하는 데 초점을 맞춰야 하며, RAG는 특히 개발자들에게 가장 효과적인 접근 중 하나입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대형 언어 모델에는 일부 고유한 제약이 있습니다. 외부 지식이 부족하기 때문에 잘못된 정보를 제공하거나 환각적인 정보를 제공할 수 있습니다. 훈련 데이터의 컷오프 날짜 때문에 잠재적으로 오래된 정보에 의존합니다. 예를 들어, GPT-3는 2021년 이전에 훈련을 받았습니다. 그들은 훈련 데이터 외의 특정 주제에 대한 깊이나 구체성이 부족합니다. LLMs의 훈련 및 세밀한 조정은 많은 조직에게 계산 비용이 많이 들어서 현실적으로 불가능합니다. 모델은 지식의 출처를 보여주거나 민감한 데이터가 제공될 때 개인 정보 보호 규정을 준수할 수 없습니다.\n\nRAG는 생성된 콘텐츠의 정확성과 관련성을 크게 향상시킬 수 있습니다. 먼저 텍스트를 생성하기 전에 외부 데이터베이스나 문서에서 관련 정보를 검색합니다. [1]\n\n\n![image](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_0.png)\n\n\n예를 들어, 사용자가 다음과 같은 질문을 할 때를 상상해보세요. “OpenAI의 CEO Sam Altman이 이사회에 의해 갑자기 해임당하고 회사에 재취직되었다는 사실을 어떻게 평가하십니까? 이것은 권력 동력 측면에서 게임 오브 스론의 현실적 버전처럼 세 일 동안에 벌어진 일입니다.”\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChatGPT가 적절하게 대답하지 못할 것입니다. 그 이유는 이벤트가 2021년 이후에 발생했기 때문입니다. RAG를 사용하면 먼저 관련 문서를 검색하고 \"Sam Altman이 CEO로 OpenAI에 복귀, 실리콘밸리의 드라마가 코미디와 비슷해짐\", \"드라마가 결론에 이르렀나요? Sam Altman이 OpenAI의 CEO로 복귀, 이사회가 구조 재편을 할 예정\", \"OpenAI의 인사 불화가 종결됐습니다. 누가 이겼고 누가 졌나요?\"와 같은 중요한 부분을 추출합니다. 이 세 개의 단락은 질문에 맥락을 제공하기 위해 결합됩니다. 그 후 대형 언어 모델은 검색된 정보를 기반으로 일관된 답변을 생성할 수 있습니다.\n\nRAG 타임라인 및 기술\n\n![image](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_1.png)\n\n역사를 살펴보면, RAG는 세 가지 주요 방식(사전 훈련, 세밀한 조정 및 추론 검색)을 갖춘 학계에서 유래되었습니다. 최근에는 더 실용적인 기술들이 추론 시간 검색에 초점을 맞추고 있습니다. 또한, 2022년 이전에는 제안된 RAG 기술이 몇 없었지만, 2023년 이후에는 다양한 RAG 기술이 급증한 것을 볼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![RAG process](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_2.png)\n\nRAG은 응답을 생성하기 전에 외부 지식 소스에서 관련 정보를 먼저 검색하여 LLM 출력의 정밀성과 관련성을 향상시킵니다. 전통적인 기본 RAG 프로세스, Naive RAG로도 알려진 과정은 주로 세 가지 기본 단계로 구성됩니다.\n\n- 색인화: 문서는 더 짧은 텍스트(\"체크\")로 분할되어 인코더 모델을 사용하여 벡터 데이터베이스에 색인화됩니다.\n- 검색: 질문과 청크 간 유사성을 기반으로 관련 청크를 찾습니다.\n- 생성: LLM은 검색된 컨텍스트를 조건으로 하는 답변을 생성합니다.\n\n고급 RAG 패러다임에는 Pre-Retrieval 및 Post-Retrieval에서 추가 처리가 포함되어 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 검색 전에는 질문과 문서 조각 사이의 의미 차이를 조정하기 위해 쿼리 재작성, 라우팅, 확장과 같은 방법을 사용할 수 있습니다.\n- 검색 후에는 검색된 문서 코퍼스를 재랭크하면 \"중간에서 길을 잃음\" 현상을 피하거나 컨텍스트가 필터링되어 윈도우 길이가 줄어들도록 압축될 수 있습니다.\n\n모듈식 RAG도 소개되었습니다. 구조적으로 더 자유롭고 유연하며, 쿼리 검색 엔진 및 여러 답변의 퓨전과 같은 특정 기능 모듈들이 더 많이 도입되었습니다. 기술적으로는 검색을 세밀하게 조정, 강화 학습 및 기타 기술과 통합합니다. 프로세스 측면에서는 RAG 모듈이 설계되고 조율되어 다양한 RAG 패턴이 생성됩니다.\n\n좋은 RAG 시스템을 구축하기 위해 고려해야 할 세 가지 중요한 질문은 무엇을 검색할 것인가? 언제 검색할 것인가? 검색된 콘텐츠를 어떻게 활용할 것인가?\n\n증강 소스. 텍스트 단락, 구절 또는 개별 단어와 같은 비구조적 데이터. 색인된 문서, 트리플 데이터 또는 서브그래프와 같은 구조화된 데이터 또한 사용할 수 있습니다. 또는 LLMs가 생성한 콘텐츠에서 검색할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n증강 단계. 사전 훈련, 세밀 조정 및 추론 단계에서 실행됩니다.\n\n증강 과정. 초기 검색은 일회성이지만, 반복 검색, 재귀 검색 및 적응적 검색 방법이 발전하는 과정에서 LLMs가 자체적으로 검색 시기를 결정하는 방식이 점차 RAG의 개발 과정에서 나타났습니다.\n\n아래 그림은 RAG triage에 대한 더 자세한 정보를 보여줍니다. 이는 증강 단계(사전 훈련, 세밀 조정, 추론), 증강 소스(비구조화된 데이터, 구조화된 데이터, LLM이 생성한 콘텐츠), 증강 프로세스(일회성 검색, 반복 검색, 적응적 검색, 재귀 검색)를 포함합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_3.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 그림은 RAG와 관련된 용어와 그들의 참고 논문을 보여줍니다.\n\n![RAG Terminology](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_4.png)\n\n# RAG 특징\n\nRAG를 더 잘 이해하기 위해 비교해보면 좋습니다. RAG는 모델에게 맞춤형 정보 검색을 위한 교과서를 제공하는 것과 같습니다. 특정 질의에 매우 적합합니다. 비유를 통해 설명해드릴게요. RAG는 모델에게 외부 지식 원천을 제공해주는데, 마치 학생에게 여는 책 시험을 볼 수 있게 해주는 것과 같습니다. 그럼에도 불구하고, 파인튜닝은 특정 작업에 적합한 지식을 점차적으로 습득하는 학생과 유사하며, 시간이 지남에 따라 지식을 내면화하며 특정 구조, 스타일 또는 형식을 모방하는 데 더 적합합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image5](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_5.png)\n\n외부 지식과 모델 사용자 정의에 따라 RAG 및 세밀한 조정이 각각 적합한 응용 프로그램을 갖고 있습니다. 두 가지를 함께 사용하면 최상의 성능을 얻을 수 있습니다. RAG는 모델 적응이 적게 필요하지만 외부 지식이 필요하며, 세밀한 조정은 모델을 크게 적응시키지만 외부 데이터가 적게 필요합니다. 대부분의 경우, RAG, Fine-tuning, Prompt Engineering을 결합하면 최상의 결과를 얻을 수 있습니다.\n\n![image6](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_6.png)\n\n# RAG 평가\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRAG을 구현한 후, 세 가지 품질 점수인 문맥적 타당성, 답변 충실도 및 답변 관련성을 사용하여 철저한 평가가 필수적입니다. 이 평가에는 소음에 대한 견고성, 거부 능력, 정보 통합 및 여우틀 분석의 네 가지 핵심 능력이 포함됩니다. RGB 및 RECALL과 같은 표준화된 벤치마크뿐만 아니라 RAG 시스템을 평가하기 위한 자동화된 도구인 RAGAS, ARES 및 TruLens도 이용할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_7.png)\n\n![이미지](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_8.png)\n\n# RAG의 미래\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파워풀한 RAG는 몇 가지 도전에 직면합니다. 큰 컨텍스트 창을 가지면 성능이 향상되지 않을 수 있습니다. 검색을 강력하게 만들고 낮은 품질의 콘텐츠를 걸러내는 것은 어려울 수 있습니다. 잘못된 콘텐츠를 검색하면 최종 답변을 오염시킬 수 있습니다. RAG와 세밀한 조정 사이의 균형을 맞추는 것은 까다로울 수 있습니다. 더 큰 모델이 항상 RAG를 개선하는지 여전히 불분명합니다. LLM의 역할을 더 탐구해야 합니다. 대규모로 RAG를 제작하고 민감한 데이터를 보호하는 것도 다른 고려 사항입니다. RAG를 확장하여 이미지, 오디오 및 비디오를 처리하는 것은 여전히 열려 있는 문제입니다.\n\n하지만 RAG는 질문 답변, 추천 시스템, 정보 추출 및 보고서 생성에 대한 약속을 보여줍니다. 성숙한 RAG 기술 스택은 Langchain 및 LlamaIndex와 같이 번창하고 있고, 시장에서는 맞춤형 도구 및 간소화된 도구와 같은 더 타깃팅 된 RAG 도구들이 등장하고 있습니다. 따라서 생태계는 RAG에 맞는 새로운 도구들이 계속해서 확장되어갈 것입니다.\n\n![이미지](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_9.png)\n\n# RAG 실무\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지 RAG에 대한 고수준 개요를 제공했습니다. 다음으로는 손을 더럽히는 RAG 실험 몇 가지를 시연할 것이며, 여러분의 프로젝트에서 이러한 기술을 적용할 수 있도록 도와드리겠습니다. LlamaIndex를 활용하여 다양한 RAG 파이프라인을 소개하는 Python 스크립트가 세 개 있습니다.\n\n1. 기본 RAG 파이프라인\n2. 문장 창 RAG 파이프라인\n3. 자동 생성 RAG 파이프라인\n\n![이미지](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_10.png)\n\n기본 RAG 파이프라인은 기존 데이터베이스로 대규모 언어 모델을 보완합니다. 쿼리는 먼저 데이터베이스에서 관련 콘텍스트를 검색한 후 답변을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문서를 64토큰씩 2토큰의 중첩으로 작은 단락으로 나눌 수 있어요. 이러한 단락들은 벡터로 인코딩되고 벡터 데이터베이스에 색인됩니다. 쿼리를 받으면 가장 유사한 단락을 찾아 질문과 함께 프롬프트로 둘러싸서 언어 모델에 전달하여 답변을 생성합니다.\n\n![image](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_11.png)\n\n![image](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_12.png)\n\n문장 윈도우 검색 파이프라인은 더 많은 문맥이 필요할 때 유용합니다. 토큰 청크 대신 문장으로 문서를 세그먼트화합니다. 가장 유사한 문장뿐만 아니라 직전 및 직후 문장을 가져와서 문맥 창을 형성합니다. 이러한 문맥 창은 재정렬되어 언어 모델에 제공됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_13.png)\n\n자동 생성되는 검색 파이프라인은 검색을 위한 계층 구조를 만듭니다. 작은 16 토큰 단락은 64 토큰 단락을 형성하고, 이는 다시 256 토큰 단락에 연결됩니다. 충분히 많은 작은 단락이 부모에 연결되면 해당 단락은 부모 청크로 병합됩니다. 최종 청크는 다시 순위를 매기고 검색됩니다. 이를 통해 동적으로 크기가 조정된 컨텍스트를 사용할 수 있습니다.\n\n코드를 사용하려면 먼저 NLP.yml 파일의 yaml 파일을 사용하여 python_env 폴더 아래에 Python 가상 환경을 생성하십시오. common 폴더 아래 openAI.env 파일에 OpenAI API 키를 추가하십시오. 샘플 데이터는 data 폴더의 Henry.txt에 있지만, 직접 문서를 제공할 수도 있습니다.\n\n기본 파이프라인은 문서를 청킹하고 벡터 데이터베이스에 색인화한 다음 쿼리를 수행하여 유사한 단락을 검색하고 프롬프트로 래핑하여 언어 모델에 보냅니다. 검색된 단락의 원본 소스를 확인할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문장-창 파이프라인은 문장 수준에서 검색을 수행하여 이전 두 문장과 다음 한 문장을 포함하여 문장을 확장합니다. 다시 순위 지정은 가장 관련성 높은 창을 선택하는 방법을 보여줍니다.\n\n자동 생성 파이프라인은 16에서 256 토큰까지의 통과구조의 계층 구조를 작성하며, 필요에 따라 통과를 더 큰 청크로 병합합니다. 이는 정밀도를 유지하면서 더 긴 문맥을 제공합니다.\n\n이 코드는 연결하고 사용하기 쉽게 설계되어 있습니다. API 키 및 가상 환경을 구성한 후에는 자신의 문서 및 사용 사례에 RAG를 적용할 수 있습니다. 한 번 시도해보시고 다른 궁금한 점이 있으면 알려주세요!\n\n# 참고문헌\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[1] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J. and Wang, H., 2023. 대규모 언어 모델을 위한 검색 보완 생성: 조사. arXiv preprint arXiv:2312.10997.\n\n[2] https://github.com/HenryHengLUO/Retrieval-Augmented-Generation-Intro-Project\n\n[3] https://www.llamaindex.ai/\n\n[4] https://learn.deeplearning.ai/building-evaluating-advanced-rag\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 부록\n\n이 문서는 2024년 1월 27일 GDG 홍콩 AI/ML 스터디 그룹에서 Henry의 발표에 따라 수정 및 재작성되었습니다. 결론적으로 모든 참석자들은 데모에서 웃음을 자아낸 까탈스러운 농담을 외웠습니다: \"Henry is the most pretty boy in Hong Kong.\"\n\nThomas와 Kin이 이 훌륭한 행사를 기획해 준 데에 감사드립니다.\n\n![이미지](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_14.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_15](/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_15.png)\n","ogImage":{"url":"/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_0.png"},"coverImage":"/assets/img/2024-06-19-IntroofRetrievalAugmentedGenerationRAGandapplicationdemos_0.png","tag":["Tech"],"readingTime":9},{"title":"언어별 RAG 애플리케이션 탐색 미슈나와 대화하기","description":"","date":"2024-06-19 20:28","slug":"2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah","content":"\n\n## 래빈 문헌에 대한 다국어 RAG 시스템 구축\n\n![이미지](/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_0.png)\n\n# 소개\n\n이 게시물에서 래빈 문헌과 상호작용하기 위한 독특한 검색 보강 생성(RAG) 응용 프로그램을 구축한 여정을 공유하게 되어 매우 기쁩니다. MishnahBot은 학자들과 일반 사용자들이 미슈나를 질의하고 탐색하는 직관적인 방법을 제공하는 것을 목표로하며, 상호작용적으로 도울 수 있습니다. 이는 관련 소스 텍스트를 빠르게 찾거나 종교 법에 대한 복잡한 토론을 요약하는 등의 문제를 해결하는 데 도움을 줄 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수년 전에 이러한 프로젝트 아이디어가 있었지만, 기술이 아직 충분히 발달하지 않은 것 같았어요. 이제는 대형 언어 모델과 RAG 기능이 발전함에 따라 매우 간단해졌어요. \n\n아래는 최종 제품 모습이에요. 여기서 시도해 볼 수 있어요:\n\n![이미지](/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_1.png)\n\n# 그래서 RAG 시스템에 대한 극찬이란 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRAG(검색 보강 생성) 애플리케이션은 정확성을 향상시키고 대형 언어 모델(LLM)에서 제공되는 추론 능력을 활용하기 위해 상당한 주목을 받고 있습니다. 도서관, 동일 제조업체의 자동차 설명서 컬렉션 또는 세금 서류와 대화할 수 있다면 어떨까요? 질문을 하고 풍부한 전문 지식에 의해 제공되는 답변을 받을 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_2.png)\n\n# RAG 대 장단점과 증가된 컨텍스트 길이\n\n언어 모델 상호작용을 향상시키는 두 가지 신흥 추세가 있습니다: 검색 보강 생성(RAG) 및 컨텍스트 길이 증가, 가능한 경우 매우 긴 문서를 첨부 파일로 허용하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRAG 시스템의 한 가지 주요 장점은 비용 효율성입니다. RAG를 사용하면 쿼리 비용을 크게 증가시키지 않고도 큰 컨텍스트를 처리할 수 있어서 비용이 비실수하는 경우를 방지할 수 있습니다. 게다가 RAG는 더 모듈화되어 있어서 다른 지식 베이스 및 LLM 제공업체와 쉽게 결합하여 사용할 수 있습니다. 반면에 언어 모델에서 직접 컨텍스트 길이를 늘리는 것은 하나의 상호작용에서 훨씬 긴 텍스트를 처리할 수 있는 흥미로운 발전입니다.\n\n# 설정\n\n이 프로젝트에서는 개발 환경으로 AWS SageMaker를 사용했습니다. AWS Bedrock를 사용하여 다양한 LLM에 액세스했으며, 파이프라인을 관리하기 위해 LangChain 프레임워크를 사용했습니다. 두 AWS 서비스 모두 사용자 친화적이며 사용한 리소스에 대해서만 요금을 부과하므로 여러분께서 직접 시도해보길 권해 드립니다. Bedrock를 사용하려면 Llama 3 70b Instruct 및 Claude Sonnet에 대한 액세스 권한을 요청해야 합니다.\n\n새로운 Jupyter 노트북을 열고 사용할 패키지를 설치해 보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n!pip install chromadb tqdm langchain chromadb sentence-transformers\n```\n\n# 데이터셋\n\n이 프로젝트의 데이터셋은 유대교 전통에서 중심적인 고대 래빈 신학 텍스트인 미슈나입니다. 이 텍스트를 선택한 이유는 내 마음에 가깝기 때문이며, 동시에 단순한 주제이기 때문에 언어 모델에 대한 도전을 제공합니다. 데이터셋은 원래 히브리어와 일치하는 영어 번역이 있는 유대 래빈 신학 텍스트의 보물창고인 Sefaria-Export 리포지토리²에서 얻었습니다. 이 일치는 RAG 애플리케이션의 다른 단계에서 다른 언어로 전환할 수 있도록 돕습니다.\n\n참고: 여기서 적용된 동일한 과정은 사용자가 선택한 다른 텍스트 모음에도 적용할 수 있습니다. 이 예시는 또한 히브리어로 보여진 것처럼 RAG 기술을 다른 언어에서 사용할 수 있는 방법을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 한번 더 해보세요\n\n# 1. 데이터셋 로드하기\n\n먼저 해당 데이터를 다운로드해야 합니다. 전체 저장소가 상당히 크기 때문에 git sparse-checkout을 사용할 것입니다. 터미널 창을 열고 아래 명령어를 실행해주세요.\n\n```js\ngit init sefaria-json\ncd sefaria-json\ngit sparse-checkout init --cone\ngit sparse-checkout set json\ngit remote add origin https://github.com/Sefaria/Sefaria-Export.git\ngit pull origin master\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\ntree Mishna/ | less\r\n```\n\n그리고… 왔네요! 우리가 필요한 데이터 파일이 이제 있습니다:\n\n```js\r\nMishnah\n├── Seder Kodashim\n│   ├── Mishnah Arakhin\n│   │   ├── English\n│   │   │   └── merged.json\n│   │   └── Hebrew\n│   │       └── merged.json\n│   ├── Mishnah Bekhorot\n│   │   ├── English\n│   │   │   └── merged.json\n│   │   └── Hebrew\n│   │       └── merged.json\n│   ├── Mishnah Chullin\n│   │   ├── English\n│   │   │   └── merged.json\n│   │   └── Hebrew\n│   │       └── merged.json\r\n```\n\n이제 주피터 노트북 환경에서 문서를 로드해 봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\nimport os\r\nimport json\r\nimport pandas as pd\r\nfrom tqdm import tqdm\r\n\r\n# 진행률 표시가 있는 DataFrame으로 모든 문서를 로드하는 함수\r\ndef load_documents(base_path):\r\n    data = []\r\n    for seder in tqdm(os.listdir(base_path), desc=\"Loading Seders\"):\r\n        seder_path = os.path.join(base_path, seder)\r\n        if os.path.isdir(seder_path):\r\n            for tractate in tqdm(os.listdir(seder_path), desc=f\"Loading Tractates in {seder}\", leave=False):\r\n                tractate_path = os.path.join(seder_path, tractate)\r\n                if os.path.isdir(tractate_path):\r\n                    english_file = os.path.join(tractate_path, \"English\", \"merged.json\")\r\n                    hebrew_file = os.path.join(tractate_path, \"Hebrew\", \"merged.json\")\r\n                    if os.path.exists(english_file) and os.path.exists(hebrew_file):\r\n                        with open(english_file, 'r', encoding='utf-8') as ef, open(hebrew_file, 'r', encoding='utf-8') as hf:\r\n                            english_data = json.load(ef)\r\n                            hebrew_data = json.load(hf)\r\n                            for chapter_index, (english_chapter, hebrew_chapter) in enumerate(zip(english_data['text'], hebrew_data['text'])):\r\n                                for mishnah_index, (english_paragraph, hebrew_paragraph) in enumerate(zip(english_chapter, hebrew_chapter)):\r\n                                    data.append({\r\n                                        \"seder\": seder,\r\n                                        \"tractate\": tractate,\r\n                                        \"chapter\": chapter_index + 1,\r\n                                        \"mishnah\": mishnah_index + 1,\r\n                                        \"english\": english_paragraph,\r\n                                        \"hebrew\": hebrew_paragraph\r\n                                    })\r\n    return pd.DataFrame(data)\r\n# 모든 문서를 로드\r\nbase_path = \"Mishnah\"\r\ndf = load_documents(base_path)\r\n# DataFrame을 파일로 저장하여 나중에 참조\r\ndf.to_csv(os.path.join(base_path, \"mishnah_metadata.csv\"), index=False)\r\nprint(\"데이터셋이 성공적으로 DataFrame에 로드되고 파일로 저장되었습니다.\")\r\n```\r\n\r\n그리고 데이터를 확인해보세요:\r\n\r\n```js\r\ndf.shape\r\n(4192, 7)\r\n\r\nprint(df.head()[[\"tractate\", \"mishnah\", \"english\"]])\r\ntractate  mishnah                                            english\r\n0  Mishnah Arakhin        1  \u003cb\u003eEveryone takes\u003c/b\u003e vows of \u003cb\u003evaluation\u003c/b\u003e...\r\n1  Mishnah Arakhin        2  With regard to \u003cb\u003ea gentile, Rabbi Meir says:\u003c...\r\n2  Mishnah Arakhin        3  \u003cb\u003eOne who is moribund and one who is taken to...\r\n3  Mishnah Arakhin        4  In the case of a pregnant \u003cb\u003ewoman who is take...\r\n4  Mishnah Arakhin        1  \u003cb\u003eOne cannot be charged for a valuation less ...\r\n```\r\n\r\n좋아 보이니, 이제 벡터 데이터베이스 단계로 넘어갈 수 있습니다.\n\n혹시 필요하신 점이 있으면 언제든지 물어주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2. Vectorizing and Storing in ChromaDB\n\n이제, 텍스트를 벡터화하여 로컬 ChromaDB에 저장합니다. 간단히 말해서, 텍스트를 밀도 있는 벡터로 표현하여 의미론적으로 유사한 텍스트가 벡터 공간에서 서로 \"가까이\" 있게 됩니다. 이 기술은 쿼리가 주어졌을 때 관련된 단락을 검색하는 데 활용됩니다.\n\n저희는 가벼운 벡터화 모델인 all-MiniLM-L6-v2를 선택했습니다. 이 모델은 CPU에서 효율적으로 실행될 수 있어 성능과 자원 효율성 사이에 좋은 균형을 제공하며, 우리의 응용 프로그램에 적합합니다. OpenAI의 text-embedding-3-large와 같은 최첨단 모델들이 더 뛰어난 성능을 제공할 수 있지만, 일반적으로 GPU에서 실행되는 상당한 계산 자원이 필요합니다.\n\n임베딩 모델 및 성능에 대한 더 많은 정보는 MTEB leaderboard를 참조할 수 있습니다. 이 leaderboard는 여러 작업에서 다양한 텍스트 임베딩 모델을 비교합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리가 벡터화에 사용할 코드입니다 (CPU 기계에서는 이 데이터 세트에서 몇 분 정도 소요될 것입니다):\n\n```js\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport chromadb\nfrom chromadb.config import Settings\nfrom tqdm import tqdm\n\n# 임베딩 모델 초기화\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n# ChromaDB 초기화\nchroma_client = chromadb.Client(Settings(persist_directory=\"chroma_db\"))\ncollection = chroma_client.create_collection(\"mishnah\")\n# 저장된 파일에서 데이터 세트 불러오기\ndf = pd.read_csv(os.path.join(\"Mishnah\", \"mishnah_metadata.csv\"))\n# 진행 표시줄과 함께 임베딩을 생성하는 함수\ndef generate_embeddings(paragraphs, model):\n    embeddings = []\n    for paragraph in tqdm(paragraphs, desc=\"Generating Embeddings\"):\n        embedding = model.encode(paragraph, show_progress_bar=False)\n        embeddings.append(embedding)\n    return np.array(embeddings)\n# 영어 문단에 대한 임베딩 생성\nembeddings = generate_embeddings(df['english'].tolist(), model)\ndf['embedding'] = embeddings.tolist()\n# 진행 표시줄과 함께 ChromaDB에 임베딩 저장\nfor index, row in tqdm(df.iterrows(), desc=\"Storing in ChromaDB\", total=len(df)):\n    collection.add(embeddings=[row['embedding']], documents=[row['english']], metadatas=[{\n        \"seder\": row['seder'],\n        \"tractate\": row['tractate'],\n        \"chapter\": row['chapter'],\n        \"mishnah\": row['mishnah'],\n        \"hebrew\": row['hebrew']\n    }])\nprint(\"Embeddings and metadata successfully stored in ChromaDB.\")\n```\n\n# 3. 영어로 우리의 RAG 생성하기\n\n데이터 세트가 준비되었으므로, 이제 영어로 우리의 검색 보완 생성 (RAG) 애플리케이션을 만들 수 있습니다. 이를 위해 LangChain을 사용할 것인데, 이는 다양한 언어 모델 작업 및 통합에 대한 통합 인터페이스를 제공하여 복잡한 애플리케이션을 쉽게 구축할 수 있는 강력한 프레임워크입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLangChain은 언어 모델(LLMs), 검색기 및 벡터 저장소와 같은 다른 구성 요소를 통합하는 프로세스를 간단화합니다. LangChain을 사용하면 각 구성 요소의 내부 복잡성에 대해 걱정하지 않고 응용 프로그램의 고수준 논리에 집중할 수 있습니다.\n\n다음은 RAG 시스템을 설정하는 코드입니다:\n\n```js\nfrom langchain.chains import LLMChain, RetrievalQA\nfrom langchain.llms import Bedrock\nfrom langchain.prompts import PromptTemplate\nfrom sentence_transformers import SentenceTransformer\nimport chromadb\nfrom chromadb.config import Settings\nfrom typing import List\n\n# Llama 3 70B Instruct 모델을 위한 AWS Bedrock 초기화\nllm = Bedrock(\n    model_id=\"meta.llama3-70b-instruct-v1:0\"\n)\n\n# 프롬프트 템플릿 정의\nprompt_template = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=\"\"\"\n    주어진 맥락만을 기반으로 다음 질문에 답해주세요:\n    맥락: {context}\n    질문: {question}\n    답변 (간략하고 명료하게):\n    \"\"\",\n)\n\n# ChromaDB 초기화\nchroma_client = chromadb.Client(Settings(persist_directory=\"chroma_db\"))\ncollection = chroma_client.get_collection(\"mishnah\")\n\n# 임베딩 모델 정의\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n\n# 간단한 검색기 함수 정의\ndef simple_retriever(query: str, k: int = 3) -\u003e List[str]:\n    query_embedding = embedding_model.encode(query).tolist()\n    results = collection.query(query_embeddings=[query_embedding], n_results=k)\n    documents = results['documents'][0]  # 'documents' 내부의 첫 번째 목록에 액세스\n    sources = results['metadatas'][0]  # 소스의 메타데이터에 액세스\n    return documents, sources\n\n# LLM 체인 초기화\nllm_chain = LLMChain(\n    llm=llm,\n    prompt=prompt_template\n)\n\n# SimpleQA 체인 정의\nclass SimpleQAChain:\n    def __init__(self, retriever, llm_chain):\n        self.retriever = retriever\n        self.llm_chain = llm_chain\n\n    def __call__(self, inputs, do_print_context=True):\n        question = inputs[\"query\"]\n        retrieved_docs, sources = self.retriever(question)\n        context = \"\\n\\n\".join(retrieved_docs)\n        response = self.llm_chain.run({\"context\": context, \"question\": question})\n        response_with_sources = f\"{response}\\n\" + \"#\"*50 + \"\\nSources:\\n\" + \"\\n\".join(\n            [f\"{source['seder']} {source['tractate']} Chapter {source['chapter']}, Mishnah {source['mishnah']}\" for source in sources]\n        )\n        if do_print_context:\n            print(\"#\"*50)\n            print(\"검색된 단락:\")\n            for doc in retrieved_docs:\n                print(doc[:100] + \"...\")\n        return response_with_sources\n\n# SimpleQAChain 초기화 및 테스트\nqa_chain = SimpleQAChain(retriever=simple_retriever, llm_chain=llm_chain)\r\n```\n\n# 설명:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- AWS Bedrock 초기화: Llama 3 70B Instruct 모델을 사용하여 AWS Bedrock를 초기화합니다. 이 모델은 검색된 맥락에 기반하여 응답을 생성하는 데 사용됩니다.\n- 프롬프트 템플릿: 프롬프트 템플릿은 맥락과 질문을 LLM이 이해할 수 있는 구조로 서식을 지정하는 것으로 정의됩니다. 이는 간결하고 관련성 있는 답변을 생성하는 데 도움이 됩니다. 필요에 따라 템플릿을 조정하고 실험해 보세요.\n- 임베딩 모델: 우리는 쿼리에 대한 임베딩을 생성하기 위해 'all-MiniLM-L6-v2' 모델을 사용합니다. 쿼리가 관련 답변 단락과 유사한 표현을 갖도록 희망합니다. 참고: 검색 성능을 향상하기 위해 사용자 쿼리를 수정하고 최적화하기 위해 LLM을 사용할 수 있습니다. 이렇게 하면 RAG 데이터베이스의 스타일과 더 유사해집니다.\n- LLM Chain: LangChain의 LLMChain 클래스를 사용하여 LLM과 검색된 맥락 간 상호 작용을 관리합니다.\n- SimpleQAChain: 이 사용자 정의 클래스는 검색기와 LLM 체인을 통합합니다. 관련 단락을 검색하고 맥락으로 형식을 지정한 다음 답변을 생성합니다.\n\n좋아요! 이제 시도해 보죠! 미션나의 첫 번째 단락과 관련된 쿼리를 사용하겠습니다.\n\n```js\nresponse = qa_chain({\"query\": \"샤마를 낭독할 적절한 시기는 무엇입니까?\"})\n\nprint(\"#\"*50)\nprint(\"응답:\")\nprint(response)\n```\n\n```js\n##################################################\n검색된 단락:\n\u003ci\u003eBerakhot\u003c/i\u003e 트랙테의 시작, 미션나의 여섯 권에서 첫 번째 트랙테...\n아침에 \u003ci\u003e샤마\u003c/i\u003e를 낭독하는 시점은 언제일까요? 한 사람이 sky-blue와 white를 구분할 수 있는 때부터 아침...\n베이트샤마이와 베이트힐르는 \u003ci\u003e샤마\u003c/i\u003e를 제대로 낭독하는 방법에 대해 논쟁했습니다. \u003cb\u003e베이트샤마이는 말합니다:\u003c/b\u003e...\n##################################################\n응답:\n 저녁에는 제사장들이 그들의 Teruma를 먹기 위해 들어가는 때부터 첫 번째 경비를 마칠 때까지, 또는 Rabban Gamliel에 따르면 새벽까지. 아침에는 하늘색과 흰색을 구별할 수 있는 때부터 일출까지.\n##################################################\n출처:\nSeder Zeraim 미션나 Berakhot 장 1, 미션나 1\nSeder Zeraim 미션나 Berakhot 장 1, 미션나 2\nSeder Zeraim 미션나 Berakhot 장 1, 미션나 3\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그것은 꽤 정확한 것 같아요.\n\n좀 더 복잡한 질문을 해볼까요:\n\n```js\nresponse = qa_chain({\"query\": \"안식일에 금지된 세 번째 일종의 일은 무엇인가요?\"})\n\nprint(\"#\"*50)\nprint(\"응답:\")\nprint(response)\n```\n\n```js\n##################################################\n검색된 단락:\n댄 호를 둘러싼 중요한 일반적인 원칙을 말했습니다: 안식년에 있는 먹을거리가 되는 것들 중에는 무엇이라도...\n이 근본 미슠나는 '주요 노동 유형'을 수없이 열거했습니다.\n라비 아키은 말했다: 나는 '여러 가지 금지된 일을 수행하는' 사람에 대해 라비 엘리에저에게 물어 보았다...\n##################################################\n응답:\n 거둬 들이는 사람입니다.\n##################################################\n소스:\nSeder Zeraim Mishnah Sheviit Chapter 7, Mishnah 1\nSeder Moed Mishnah Shabbat Chapter 7, Mishnah 2\nSeder Kodashim Mishnah Keritot Chapter 3, Mishnah 10\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아주 좋아요.\n\n# 클라우드에 직접 쿼리하는 것으로 동일한 결과를 얻을 수 있었을까요?\n\n그것을 시도해 봤어요. 여기에 제가 얻은 것이 있어요:\n\n![Exploring RAG Applications Across Languages](/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n응답이 너무 길고 본질에 맞지 않으며, 제공된 답변도 잘못되었습니다 (목록에서 일곱 번째로 선택하는 것은 아니고, 첫 번째로 수확하는 것입니다). 이것은 환각이라고 부릅니다.\n\n클로드는 강력한 언어 모델이지만, 기억된 훈련 데이터를 사용하여 응답을 생성하거나 인터넷 검색만을 의존하는 것은 것이 사용자 정의 데이터베이스를 사용하는 추출 증강 생성(RAG) 애플리케이션보다 제공하는 정밀성과 통제가 부족합니다. 이유는 다음과 같습니다:\n\n- 정밀성과 맥락: 저희 RAG 애플리케이션은 사용자 정의 데이터베이스에서 정확한 단락을 검색하여 높은 관련성과 정확성을 보장합니다. 특정 검색 메커니즘이 없는 클로드는 매우 상세하고 맥락에 맞는 응답을 제공하지 않을 수 있습니다.\n- 효율성: RAG 방식은 대용량 데이터 세트를 효율적으로 처리하며, 검색 및 생성을 결합하여 정확하고 맥락에 맞는 답변을 유지합니다.\n- 비용 효율성: Llama 3 70B Instruct와 같이 상대적으로 작은 LLM을 활용하여, 매번 쿼리마다 많은 데이터를 보내지 않아도 정확한 결과를 얻을 수 있습니다. 이는 더 크고 자원 집약적인 모델을 사용하는 데 연관된 비용을 줄입니다.\n\n이 구조화된 검색 프로세스는 사용자가 가장 정확하고 관련성 높은 답변을 받도록 보장하며, LLM의 언어 생성 능력과 사용자 정의 데이터 검색의 정밀성을 활용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 4. 다국어 RAG 방식\n\n마침내, 우리는 원본 히브리어 텍스트로 히브리어로 상호 작용하는 도전 과제에 대해 다룰 것입니다. 동일한 방식은 다른 어떤 언어에도 적용할 수 있습니다. 텍스트를 영어로 번역하여 검색 단계에 활용할 수 있다면요.\n\n히브리어 상호 작용을 지원하는 것은 추가적인 복잡성을 더합니다. 통합 모델과 대형 언어 모델 (LLM)이 영어에서 더 강력하기 때문입니다. 일부 통합 모델과 LLM은 히브리어를 지원하기는 하지만, 영어에 비해 충분히 견고하지 않을 수 있습니다. 특히 작은 통합 모델은 훈련 중 주로 영어에 초점을 맞춘 경우가 많습니다.\n\n이를 해결하기 위해 우리는 자체 히브리어 통합 모델을 훈련시킬 수 있습니다. 그러나 다른 실용적인 접근법은 텍스트를 일회성으로 영어로 번역하고 영어 통합을 검색 프로세스에 활용하는 것입니다. 이렇게 하면 영어 모델의 강력한 성능을 이용하면서도 히브리어 상호 작용을 지원할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 처리 단계\n\n![이미지](/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_4.png)\n\n우리의 경우에는 미션나 텍스트의 전문적 인 영어 번역이 이미 준비되어 있습니다. 이를 사용하여 히브리어 응답의 무결성을 유지하면서 정확한 검색을 보장할 것입니다. 다음은 이 교차 언어 RAG 시스템을 설정하는 방법입니다:\n\n- 히브리어로 쿼리 입력: 사용자는 히브리어로 쿼리를 입력할 수 있습니다.\n- 쿼리를 영어로 번역: 우리는 LLM을 사용하여 히브리어 쿼리를 영어로 번역합니다.\n- 쿼리 삽입: 번역된 영어 쿼리를 삽입합니다.\n- 영어 임베딩을 사용하여 관련 문서 찾기: 영어 임베딩을 사용하여 관련 문서를 찾습니다.\n- 해당 히브리어 텍스트 검색: 해당 히브리어 텍스트가 컨텍스트로 검색됩니다. 본질적으로 우리는 영어 텍스트를 키로 사용하고 검색 작업에서 히브리어 텍스트를 해당 값으로 사용합니다.\n- LLM을 사용하여 히브리어로 응답: LLM은 히브리어 컨텍스트를 사용하여 히브리어로 응답을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n세대교체에는 Llama 3에 비해 히브리어 텍스트에서 훨씬 더 우수한 성능을 발휘하는 Claude Sonnet을 사용합니다.\n\n다음은 코드 구현입니다:\n\n```js\nfrom langchain.chains import LLMChain, RetrievalQA\nfrom langchain.llms import Bedrock\nfrom langchain_community.chat_models import BedrockChat\nfrom langchain.prompts import PromptTemplate\nfrom sentence_transformers import SentenceTransformer\nimport chromadb\nfrom chromadb.config import Settings\nfrom typing import List\nimport re\n\n# Llama 3 70B Instruct에 대한 AWS Bedrock를 초기화하고 번역을 위해 특정 설정으로 설정합니다\ntranslation_llm = Bedrock(\n    model_id=\"meta.llama3-70b-instruct-v1:0\",\n    model_kwargs={\n        \"temperature\": 0.0,  # 번역을 위한 낮은 온도 설정\n        \"max_gen_len\": 50  # 번역을 위한 토큰 수 제한\n    }\n)\n\n# Claude Sonnet에 대한 AWS Bedrock를 초기화하고 생성을 위해 특정 설정으로 설정합니다\ngeneration_llm = BedrockChat(\n    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\"\n)\n\n# 번역 프롬프트 템플릿 정의\ntranslation_prompt_template = PromptTemplate(\n    input_variables=[\"text\"],\n    template=\"\"\"다음 히브리어 텍스트를 영어로 번역하십시오:\n    입력 텍스트: {text}\n    번역:\n    \"\"\"\n)\n\n# 히브리어 답변을 위한 프롬프트 템플릿 정의\nhebrew_prompt_template = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=\"\"\"주어진 맥락을 바탕으로 다음 질문에 답하세요:\n    맥락: {context}\n    질문: {question}\n    답변 (간결하고 요약적으로):\n    \"\"\"\n)\n\n# ChromaDB 초기화\nchroma_client = chromadb.Client(Settings(persist_directory=\"chroma_db\"))\ncollection = chroma_client.get_collection(\"mishnah\")\n\n# 임베딩 모델 정의\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n\n# 히브리어를 영어로 번역하는 번역 체인\ntranslation_chain = LLMChain(\n    llm=translation_llm,\n    prompt=translation_prompt_template\n)\n\n# 히브리어 답변을 위한 LLM 체인 초기화\nhebrew_llm_chain = LLMChain(\n    llm=generation_llm,\n    prompt=hebrew_prompt_template\n)\n\n# 히브리어 텍스트에 대한 간단한 리트리버 함수 정의\ndef simple_retriever(query: str, k: int = 3) -\u003e List[str]:\n    query_embedding = embedding_model.encode(query).tolist()\n    results = collection.query(query_embeddings=[query_embedding], n_results=k)\n    documents = [meta['hebrew'] for meta in results['metadatas'][0]]  # 히브리어 텍스트 액세스\n    sources = results['metadatas'][0]  # 소스에 대한 메타데이터 액세스\n    return documents, sources\n\n# 히브리어 텍스트에서 모음 제거하는 함수 정의\ndef remove_vowels_hebrew(hebrew_text):\n    pattern = re.compile(r'[\\u0591-\\u05C7]')\n    hebrew_text_without_vowels = re.sub(pattern, '', hebrew_text)\n    return hebrew_text_without_vowels\n\n# 번역과 함께간단한 QA 체인 정의\nclass SimpleQAChainWithTranslation:\n    def __init__(self, translation_chain, retriever, llm_chain):\n        self.translation_chain = translation_chain\n        self.retriever = retriever\n        self.llm_chain = llm_chain\n\n    def __call__(self, inputs):\n        hebrew_query = inputs[\"query\"]\n        print(\"#\" * 50)\n        print(f\"Hebrew query: {hebrew_query}\")\n        \n        # 번역 프롬프트 출력\n        translation_prompt = translation_prompt_template.format(text=hebrew_query)\n        print(\"#\" * 50)\n        print(f\"번역 프롬프트: {translation_prompt}\")\n        \n        # 특정 구성을 사용하여 번역 수행\n        translated_query = self.translation_chain.run({\"text\": hebrew_query})\n        print(\"#\" * 50)\n        print(f\"번역된 쿼리: {translated_query}\")  # 디버깅을 위한 번역된 쿼리 출력\n        \n        retrieved_docs, sources = self.retriever(translated_query)\n        retrieved_docs = [remove_vowels_hebrew(doc) for doc in retrieved_docs]\n\n        context = \"\\n\".join(retrieved_docs)\n        \n        # 생성을 위한 최종 프롬프트 출력\n        final_prompt = hebrew_prompt_template.format(context=context, question=hebrew_query)\n        print(\"#\" * 50)\n        print(f\"생성을 위한 최종 프롬프트:\\n {final_prompt}\")\n        \n        response = self.llm_chain.run({\"context\": context, \"question\": hebrew_query})\n        response_with_sources = f\"{response}\\n\" + \"#\" * 50 + \"Sources:\\n\" + \"\\n\".join(\n            [f\"{source['seder']} {source['tractate']} Chapter {source['chapter']}, Mishnah {source['mishnah']}\" for source in sources]\n        )\n        return response_with_sources\n\n# SimpleQAChainWithTranslation 초기화 및 테스트\nqa_chain = SimpleQAChainWithTranslation(translation_chain, simple_retriever, hebrew_llm_chain)\r\n```\n\n해보세요! 이전과 동일한 질문을 사용하지만, 이번에는 히브리어로 요청합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nresponse = qa_chain({\"query\": \"מהו סוג העבודה השלישי האסור בשבת?\"})\nprint(\"#\" * 50)\nprint(response)\n```\n\n```js\n##################################################\nHebrew query: מהו סוג העבודה השלישי האסור בשבת?\n##################################################\nTranslation Prompt: Translate the following Hebrew text to English:\n    Input text: מהו סוג העבודה השלישי האסור בשבת?\n    Translation: \n    \n##################################################\nTranslated Query:  What is the third type of work that is forbidden on Shabbat?\n\n    Input text: כל העולם כולו גשר צר מאוד\n    Translation: \n    \n##################################################\nFinal Prompt for Generation:\n ענה על השאלה הבאה בהתבסס על ההקשר המסופק בלבד:\n    הקשר: אבות מלאכות ארבעים חסר אחת. הזורע. והחורש. והקוצר. והמעמר. הדש. והזורה. הבורר. הטוחן. והמרקד. והלש. והאופה. הגוזז את הצמר. המלבנו. והמנפצו. והצובעו. והטווה. והמסך. והעושה שני בתי נירין. והאורג שני חוטין. והפוצע שני חוטין. הקושר. והמתיר. והתופר שתי תפירות. הקורע על מנת לתפר שתי תפירות. הצד צבי. השוחטו. והמפשיטו. המולחו, והמעבד את עורו. והמוחקו. והמחתכו. הכותב שתי אותיות. והמוחק על מנת לכתב שתי אותיות. הבונה. והסותר. המכבה. והמבעיר. המכה בפטיש. המוציא מרשות לרשות. הרי אלו אבות מלאכות ארבעים חסר אחת: \n\nחבתי כהן גדול, לישתן ועריכתן ואפיתן בפנים, ודוחות את השבת. טחונן והרקדן אינן דוחות את השבת. כלל אמר רבי עקיבא, כל מלאכה שאפשר לה לעשות מערב שבת, אינה דוחה את השבת. ושאי אפשר לה לעשות מערב שבת, דוחה את השבת: \n\nהקורע בחמתו ועל מתו, וכל המקלקלין, פטורין. והמקלקל על מנת לתקן, שעורו כמתקן: \n\n    שאלה: מהו סוג העבודה השלישי האסור בשבת?\n    תשובה (קצרה ותמציתית):\n    \n##################################################\nהקוצר.\n##################################################מקורות:\nSeder Moed Mishnah Shabbat פרק 7, משנה 2\nSeder Kodashim Mishnah Menachot פרק 11, משנה 3\nSeder Moed Mishnah Shabbat פרק 13, משנה 3\r\n```\n\nWe got an accurate, one word answer to our question. Pretty neat, right?\n\n# Interesting Challenges and Solutions\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n라마 3 Instruct의 번역은 여러 도전을 안겨주었습니다. 처음에는 어떤 시도를 해도 모델이 무의미한 결과물을 출력했습니다. (눈에 띄게, Llama 3 Instruct는 새 줄 문자로 시작하는 프롬프트에 매우 민감한 모양입니다!)\n\n그 문제를 해결한 후에는 모델이 올바른 응답을 출력하기는 했지만 추가로 관련 없는 텍스트를 계속해서 출력하는 경향이 있어서, 출력을 새 줄 문자에서 중지하는 것이 효과적이었습니다.\n\n출력 형식을 제어하는 것은 까다로울 수 있습니다. JSON 형식을 요청하거나 페충 프롬프트를 제공하는 예시 중 일부 전략이 있습니다.\n\n이 프로젝트에서는 히브리어 텍스트에서 모음을 제거하기도 했습니다. 대부분의 온라인 히브리어 텍스트에는 모음이 포함되어 있지 않고, 저희는 미세 조정 중에 보이는 텍스트와 유사한 맥락을 가지기를 원하기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n이 RAG 애플리케이션을 구축하는 과정은 고대 텍스트의 미묘한 점을 현대 AI 기술과 조화롭게 결합하는 흥미로운 여정이었습니다. 고대 랍비니 텍스트 라이브러리를 모든 사람 (포함하여 나 자신)에게 보다 접근하기 쉽게 만드는 열정이 이 프로젝트를 추진했습니다. 이 기술을 사용하면 도서관과 대화를 나누거나, 아이디어에 기반한 소스를 검색하는 등 다양한 기능을 사용할 수 있습니다. 여기서 사용된 방법은 다른 소중한 텍스트 컬렉션에 적용할 수 있으며, 역사적 및 문화적 지식을 탐색하고 접근하는 새로운 가능성을 엽니다.\n\n오늘날 강력한 도구와 프레임워크 덕분에 이 모든 것을 단 몇 시간 만에 달성할 수 있다는 것이 놀라운 일입니다. GitHub에서 전체 코드를 확인하고 MishnahBot 웹사이트를 즐겨보세요.\n\n비슷한 작업을 시도하는 경우 특히 의견과 질문을 공유해 주세요. 향후 이와 같은 콘텐츠를 더 보고 싶다면 알려주세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 각주\n\n- 미션나는 탈무드의 기초로 제공되는 핵심적이고 가장 초기의 래빈 사 작품 중 하나입니다.\n- 텍스트의 라이선스는 다르며 해당 JSON 파일에 자세히 기재되어 있습니다. 이 프로젝트에서 사용된 히브리어 텍스트는 공공 도메인에 속합니다. 영어 번역은 Joshua Kulp 박사의 Mishnah Yomit 번역을 사용하였으며 CC-BY 라이선스를 따릅니다.\n\n슬로모 탄노어는 Avanan (Check Point Company)의 AI/ML 엔지니어로, NLP와 ML을 활용하여 클라우드 이메일 보안을 강화하는 분야에 특화되어 있습니다. 그는 컴퓨터 과학 석사 학위를 보유하고 NLP와 관련된 논문을 쓴 적이 있으며 수학과 컴퓨터 과학 학사 학위를 가지고 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_0.png"},"coverImage":"/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_0.png","tag":["Tech"],"readingTime":23},{"title":"RAG 성능 향상에 2개의 LLM 호출이 도움이 될까요","description":"","date":"2024-06-19 20:26","slug":"2024-06-19-Can2LLMcallsboostyourRAGsperformance","content":"\n\n데이터 열정가로서, 기ꁵ적으로 우리 조직 프로젝트용 첫 번째 검색 증강 생성 (RAG) 시스템을 구축해서 정말 기뻤어요! 이 블로그에서는 현재 직장에서 차이를 만드는 실제 세계 RAG 시스템을 개발하는 과정 속에서의 좋은 일과 나쁜 일을 함께 공유하고자 해요.\n\n![image](/assets/img/2024-06-19-Can2LLMcallsboostyourRAGsperformance_0.png)\n\n독자 여러분을 초대해서, RAG 프로젝트 중에 직면하는 현실 세계 문제들과 이러한 도전을 극복하는 데 도움이 된 나의 사고 과정을 나누는 여행을 안내해보고 싶어요.\n\n참고: 여기서 코드 조각은 공유하지 않겠습니다. 제 목적은 기본적인 개념과 간단한 아이디어를 결합하여 지성을 가진 제품을 만드는 방법을 보여주는 것이기 때문이에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 RAG가 무엇인지 모르고 빠르게 이해하고 싶다면, 저의 블로그 \"RAG를 활용한 RCB 경기 깊게 파헤치기\"를 참고해보세요. 거기에서는 RAG를 설명하는 흥미로운 비유를 사용하고 있어요.\n\n# 현재 상황:\n\n내 조직의 부서는 PDF 형식의 월간 보고서로 계속 공격 받고 있어요. 이 보고서들은 우리 회사와 제휴사의 성과에 대한 정보들을 담고 있어요. 받는 PDF의 수는 진짜 수수께끼야 — 한 달에 10개를 받을 수도 있고, 다음 달에는 놀랄 만큼 25개를 받을 수도 있어! 결국 이 PDF들의 수와 구조는 아이의 기분 조절과 같이 예측할 수 없어. \n\n# 부서가 필요로 하는 것은 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n실시간 정보를 제공해주는 챗봇이 필요하다해요. 이 챗봇은 보고서 데이터에 기반하여 사용자 쿼리에 즉각적으로 답변할 수 있어야 해요. 근데 여기서 중요한 점은, 우리 사용자들은 데이터 초보자가 아니에요. 그들은 최고경영자(CXOs), 성공을 거둔 사람들이에요. 그들의 질문들은 신뢰할 수 있는 정확한 답변을 요구해요. 어떠한 불규칙성이나 모순이라도 절대 안돼요!\n\n간단히 말해서, 우리는 PDF 보고서의 변화무쌍에 대처할 수 있는 스마트한 데이터 중심 챗봇이 필요해요. 이 챗봇이:\n\n🌊 PDF 보고서의 변화무쌍을 처리할 수 있어야 해요.\n🤖 복잡한 사용자 쿼리에 정확하고 실시간으로 답변할 수 있어야 해요.\n💼 우리 경영진팀의 신뢰할 수 있는 조언자가 되어야 해요.\n\n# 기본 솔루션은 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내 첫 번째 생각은 전통적인 NLP 기술을 사용하지 않는 것입니다. 데이터가 구조화되지 않은 형식이며 전통적인 NLP 기술은 사전 처리를 많이 필요로 하며 시간이 오래 걸립니다. RAG의 장점은 사전 처리의 복잡성을 제거하고 정보에 직접 액세스할 수 있다는 것입니다. Llama 지수 프레임워크는 더 높은 수준의 멋있음을 더했습니다. 가장 멋진 것은 PDF 리더를 기본으로 탑재하여 문서를 실시간으로 구문 분석할 수 있다는 것입니다. 이는 RAG가 PDF에서 텍스트와 테이블을 스스로 직접 수집할 수 있다는 것을 의미합니다.\n\n내 첫 단계는 간단한 RAG 시스템을 구축하는 것이었습니다 (기초부터 구축한다고 생각해보세요). 그 모든 PDF를 사용하여 거대한 인덱스를 만들었습니다 — 정보의 검색 가능한 보물창고입니다. 사용자가 질문을 할 때마다, 그것은 슈퍼 강력한 사서처럼 인덱스에서 가장 관련 있는 문서를 검색했습니다.\n\n이러한 최종 후보들과 사용자의 질의는 그런 다음 Mistral 7B로 보내졌습니다(우리의 내부 LLM으로 8k 토큰의 컨텍스트 길이를 가지고 있습니다). Mistral 7B는 이 정보를 사용하여 답변을 만들었습니다.\n\n간단한 RAG를 평가한 결과, 이는 고수준 쿼리(예: 성능 요약, 메트릭 값을 비교하는 질문)에 대해 잘 작동하지만 구체적인 답변이 필요한 질문의 경우에는 완전히 실패했다는 것을 이해했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사용자 쿼리 - \"2024년 1월 XYZ 범주의 성과 지표 값은 무엇인가요?\"\n\n간단한 RAG는 사용자 쿼리에 \"1월\"이라고 언급되었기 때문에 2022년, 2023년 및 2024년 1월에 모든 문서를 검색했습니다. 그런 다음, 가장 관련성이 높은 상위 10개의 문서가 LLM(언어 모델)에게 문맥을 제공하도록 전송되었습니다. 이것은 합리적으로 보였습니다. 더 많은 1월 정보, 더 나은 답변이 되는 것이 맞죠?\n\n안타깝게도, 그것은 그렇게 간단하지 않았습니다. LLM은 때때로 특정 사용자 쿼리에 가장 관련성이 높은 것이 아닌 2022년 1월 문서를 기반으로 응답을 생성했습니다. 이것은 retriever가 \"1월\"과 같은 일반 키워드로 인해 해당 문서들이 순위가 더 높게 매겨졌기 때문입니다. 결과는 잘못된 답변들이었습니다!\n\n이 순간에 저는 YouTube 채널/블로그에서 가르쳐지는 간단한 RAG가 샘플 데이터에서만 잘 작동하는 것을 깨달았습니다. 실제 시나리오에서는 종종 실험을 해보지 않으면 답을 찾을 수 없는 도전에 마주하게 됩니다. 그래서 이제 각 구성 요소를 실험하여 무엇이 작동하고 무엇이 실패하는지 이해하기로 결정했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시도 번호 1:\n\n개선을 위한 초기 탐색 작업은 두 가지 옵션을 포함했습니다: 더 큰 LLM 사용 또는 재랭킹 모델 구현. 먼저 더 큰 LLM을 선택했고, 인상적인 32k 토큰 컨텍스트를 갖춘 GPT-4로 전환했습니다. 놀랍게도 성능 향상이 크지 않았습니다. 이것이 나를 \"아하!\"하게 만드는 순간이었는데, 가장 관련 있는 문서들이 초기에 나오도록 해야 했고, GPT-4는 초기 문서들을 우선시하는 것으로 보였습니다.\n\nGPT-4를 재랭킹 모델과 결합하는 것이 유망한 해결책으로 보였습니다. 따라서 제가 설계한 RAG를 그림 3에 표시한 대로 만들었습니다.\n\n평가 결과, 결과가 좋아 보여서 RAG를 배포하고 테스트를 진행하기로 확신을 갖게 되었습니다. 그러나 여기서 또 다른 변화가 있었습니다 - 비용 및 보안 이유로 조직이 사내 LLM인 Mistral 7B를 사용하기를 선호했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서, 그림판에 다시 들어가 보죠! Mistral 7B와 Reranker 모델에 갇혀 있네요.\n\n시도 번호: 2\n\n성능을 향상시킬 수 있는 견고한 검색기가 필요했다는 문제를 알았어요. 그래서 다양한 검색기를 실험해봤는데, 하이브리드 검색기, 쿼리 퓨전 검색기 등을 사용해보았지만, 결국 어느 것도 제게 만족스러운 결과를 주지 못했어요.\n\n시도 번호: X (카운트를 잃어버렸네요)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 곳에서 Llama 지수에서 \"메타데이터 필터\"를 발견했어요. \"메타데이터 필터\"를 사용하면 특정 태그가있는 문서 세트를 필터링할 수 있어요. 리트리버는 이러한 필터링 된 문서만 사용하여 관련 문서를 검색할 거예요. 여기서 전략을 생각해봐요. 만약 모든 문서에 연도, 분기 또는 월과 같은 메타데이터를 표시하면 검색 시에 이러한 태그를 찾아볼 수 있겠죠.\n\n처음 단계는 각 문서에 메타데이터를 할당하여 지수를 재생성하는 것이었어요.\n\n예: 문서에 다음과 같은 텍스트가 있는 경우\n\n\"XYZ 부서는 2023년 Q1에 대단히 잘 수행되었습니다. 2022년 Q1의 성과와 비교하면, 이 숫자는 정말 기분 좋아요\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n메타데이터 필드를 감지하기 위해 정규식 표현식을 사용했지만 결국 정규식이 작동하지 않는 경우가 있음을 깨달았어요.\n\n예: \"ABC의 반기 성과는 2023년에 대단한 것이었습니다\"라는 문서 텍스트가 있다면,\n\n정규식 표현식은 문서에 언급된 연도만 감지하지만, 이 문서는 반기를 나타내는 Q1과 Q2에 대한 언급도 있습니다. 이 때, 돌파 아이디어가 떠올랐죠. LLM을 사용하여 메타데이터를 식별하고 태깅하는 것이 어떨지요. 이 문서를 LLM에 전달하자, LLM은 반응했어요.\n\n이것 좋지 않나요? LLM을 사용하여 문서를 정확하게 태깅하고 그 주변에 메타데이터를 만드는 것. 모든 문서를 LLM에 전달하는 것은 비용이 많이 들 것 같다고 생각할 수 있어요. 솔직히 말하자면, 이것은 N개의 문서를 LLM에 전달한다면 비용 부담이 크지만, 이것은 일회성 활동이기 때문에 고정비용이며 RAG 시스템을 사용하는 사용자 수와는 무관합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문서마다 메타데이터를 생성한 후 색인을 재생성했어요. 이제 고급 RAG의 플로우 다이어그램은 Figure 4와 같이 보여요.\n\nFigure 4에서 알 수 있듯이, 색인에는 메타데이터가 있는 문서가 포함되어 있어요. 이제 사용자 쿼리가 접수되면 먼저 LLM(화살표 no. 1)로 보내져 메타데이터 필드를 식별하고, 이 필드들은 사용자 쿼리에 추가되어 검색기(화살표 no. 2)로 보내져요. 검색기는 메타데이터를 기반으로 문서를 필터링하고 관련 문서를 가져와요. 나머지 과정은 앞서 소개한 RAG들과 유사해요. 이 RAG를 평가한 결과, GPT-4를 사용했던 이전 최고의 RAG보다 성능이 더 좋았어요.\n\n# 결론\n\nMistral 7B와 같은 작은 LLM이 제한처럼 보일 수 있지만, 사실은 비밀병기였어요! 핵심은 내가 그것을 어떻게 활용했느냐였어요 — 한 번이 아니라 두 번이나! 비결은 여기 있었어요: 먼저 Mistral 7B를 사용해 사용자 쿼리에서 중요 정보(메타데이터)를 추출했어요. 이를 통해 검색 프로세스는 가장 관련성 높은 문서를 정확히 찾을 수 있었어요. 그런 다음, Mistral 7B는 이러한 집중된 문서 집합과 원래 쿼리를 사용해 최종 응답을 만들어냈어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"더블 역할\" 접근법 덕분에 GPT-4와 같은 강력한 LLM을 능가할 수 있었어요. 결론은 더 작은 LLM의 힘을 과소평가하지 말아야 한다는 거죠! 전략적으로 활용하면 검색과 응답 생성에 굉장히 효율적일 수 있어요.\n\n의견란에 어떠한 제안이나 질문이 있으면 말씀해 주세요.","ogImage":{"url":"/assets/img/2024-06-19-Can2LLMcallsboostyourRAGsperformance_0.png"},"coverImage":"/assets/img/2024-06-19-Can2LLMcallsboostyourRAGsperformance_0.png","tag":["Tech"],"readingTime":5},{"title":"만나보세요 HUSKY 다단계 추론을 최적화한 새로운 에이전트","description":"","date":"2024-06-19 20:24","slug":"2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning","content":"\n\n## Meta AI, Allen AI 및 워싱턴 대학이 함께 한 새로운 연구에서는 LLM 추론에서 가장 중요한 문제 중 하나를 다루고 있습니다.\n\n![이미지](/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_0.png)\n\n추론은 창조적 AI의 다음 분야로 높이 평가되고 있습니다. 추론이라 함은 작업을 더 작은 하위 집합으로 분해하고 그것을 개별적으로 해결할 수 있는 능력을 가리킵니다. 추론 기능을 다룬 최근 기술로는 Chain-of-Thought, Tree-of-Thought, Skeleton-of-Thought, 그리고 Reflexion 등이 있습니다. 추론은 외부 데이터 또는 도구에 액세스하는 것과 같은 주변 기능도 포함합니다. 지난 몇 년 동안 특정 추론 기술에서 모델이 매우 잘 수행되었지만 도메인 간에 일반화되지 못하는 것을 보았습니다. 이는 추론이 매우 계산적으로 비싼 작업이라는 점을 고려한다면 놀라운 일이 아닙니다. 이것이 Meta AI, Allen Institute of AI 및 워싱턴 대학의 연구자들이 최근 논문에서 다루고 있는 과제입니다.\n\nHUSKY는 숫자, 테이블, 및 기반 지식 추론을 포함하는 다양한 복잡한 작업을 처리하기 위해 설계된 오픈 소스 언어 에이전트입니다. 특정 작업에 집중하거나 독점적인 모델을 사용하는 다른 에이전트와 달리, HUSKY는 다양한 도전 과제를 다루기 위한 통합된 프레임워크 내에서 작동합니다. 이는 두 단계로 진행됩니다: 먼저, 작업을 해결하기 위해 필요한 다음 동작을 생성합니다. 그리고 두 번째로, 전문가 모델을 사용하여 이 동작을 실행하고 진행되는대로 솔루션을 업데이트합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_1.png)\n\n# HUSKY 내부\n\nHUSKY는 복잡한 작업을 해결하기 위해 자세한 행동 계획을 사용합니다. 먼저, 다음 단계를 생성하고, 그 단계에는 실행해야 할 작업과 필요한 도구가 포함됩니다. 그런 다음, 전문 모델을 사용하여 작업을 실행하고 솔루션 상태를 업데이트합니다. 이 접근 방식을 통해 HUSKY는 대규모 언어 모델 (LLM)을 사용하여 성능을 최적화한 전통적인 계획 시스템의 현대 버전처럼 동작합니다.\n\n![이미지](/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_2.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n멀티스텝 추론이 필요한 작업의 경우, HUSKY는 다음 작업 및 해당 도구를 예측하고 전문가 모델을 사용하여 실행합니다. 이 과정은 최종 답변이 발견될 때까지 반복됩니다. HUSKY는 팀으로 작동하는 일렴의 전문가 모델을 조정하기 위해 여러 LLMs를 사용합니다. (LLM: Large Language Model)\n\n## 작업 및 도구 선택\n\nHUSKY는 터미널 상태에 도달할 때까지 작업 생성 및 실행 사이를 반복합니다. 작업 생성기는 다음 고수준 단계를 예측하고 미리 정의된 코드, 수학, 검색 또는 상식 네 가지 중 하나의 도구를 할당합니다. 할당된 도구에 따라 HUSKY는 전문가 모델을 호출하고 작업을 수행하며 솔루션 상태를 업데이트하며 선택적으로 출력을 자연어로 변환할 수 있습니다.\n\n# HUSKY 훈련\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n허스키의 교육은 선생님 모델을 사용하여 도구 통합 솔루션 경로를 생성하는 것을 포함합니다. 이러한 경로는 작업 생성기와 전문가 모델에 대한 교육 데이터를 구축하는 데 도움이 됩니다. 교육 파이프라인은 단순화되어 일반화되어 있어서 특정 작업 가정 없이도 허스키가 다양한 작업을 처리할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_3.png)\n\n## 추론 프로세스\n\n추론 중에 허스키는 훈련된 모듈을 통합하여 새로운 다단계 작업을 해결합니다. 작업 생성기는 첫 번째 단계와 도구를 결정하고, 그것을 전문가 모델에 전달하여 출력을 생성합니다. 이 반복적인 과정은 최종 솔루션이 달성될 때까지 계속되며, 전문가 모델은 각 단계에 대해 특정한 출력을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 평가 및 성능\n\nHUSKY의 평가는 복잡한 추론 작업에서의 추론 능력을 테스트하고 결과를 점수화하는 것을 포함합니다. 기존 데이터셋은 HUSKY가 필요로 하는 다양성을 갖추지 못하는 경우가 많아, 혼합 도구 추론을 테스트하기 위해 새로운 평가 세트인 HUSKYQA가 생성되었습니다. 이 세트에는 누락된 지식을 검색하고 숫자적 추론을 수행하는 작업이 포함되어 있습니다. 더 작은 모델을 사용하더라도, HUSKY는 GPT-4와 같은 최첨단 모델을 능가하거나 뛰어넘는 효과를 보여주며 그 효과를 입증합니다.\n\nHUSKY는 다단계 추론과 도구 사용이 필요한 다양한 작업에서 기본 언어 에이전트들과 함께 훈련되고 평가되었습니다. 이러한 작업 중 절반은 HUSKY의 모듈을 훈련하는 데 사용되었고, 도구 통합 솔루션 경로에 기반을 둔 반면, 나머지 절반은 평가를 위해 예약되었습니다. 모든 작업은 제로샷 방식으로 평가되었습니다.\n\n1) 숫자적 추론 작업\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수치 추론 작업에는 초등학교에서 고등학교 대회 수준까지 다양한 수학 데이터 세트가 포함되었습니다. 이러한 데이터 세트에는 GSM-8K, MATH, Google DeepMind 수학 작업 및 LILA 벤치마크에서 가져온 MathQA가 포함되었습니다. Google DeepMind 수학 작업에서 중점을 둔 부분에는 대수, 기본 수학, 미적분, 곱셈/나눗셈, 번호 이론 하위 집합이 포함되었습니다. MathQA의 하위 집합에는 이득, 일반, 기하학, 물리학, 확률이 포함되었습니다. GSM-8K 및 MATH는 교육용으로 사용되어 13.7K의 툴 통합 솔루션 경로를 제공했습니다.\n\n2) 표 추론 작업\n\n표 추론 작업은 표 형식의 수학 단어 문제 데이터 세트인 TabMWP, 금융 질문-응답 데이터 세트인 FinQA 및 TAT-QA, 텍스트와 표 데이터를 이해해야 하는 MultimodalQA의 테스트 문제 하위 집합으로 이루어졌습니다. TabMWP 및 FinQA는 교육 및 평가에 모두 사용되었으며, TAT-QA 및 MultimodalQA는 평가를 위해 제외되었습니다. 이러한 데이터 세트는 총 7.2K의 툴 통합 솔루션 경로를 제공했습니다.\n\n3) 지식 기반 추론 작업\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지식 기반 추론 작업에는 HotpotQA, CWQ, Musique, Bamboogle 및 StrategyQA가 포함되었습니다. HotpotQA와 Bamboogle은 평가용으로 예약되었으며, CWQ와 Musique는 교육용으로 사용되었으며, StrategyQA는 둘 다에 사용되었습니다. 이 각각은 총 7,000개의 도구 통합 솔루션 경로를 생성하였습니다.\n\n## 모델\n\n평가에는 다음과 같은 모델이 포함되었습니다:\n\n액션 생성기: 액션 생성기의 경우, HUSKY는 LLAMA-2-7B, 13B 및 LLAMA-3-8B 모델을 활용했습니다. 잘못된 솔루션 경로는 훈련 세트에서 제거되어, 숫자, 테이블, 지식 기반 및 혼합 도구 추론 작업에서 11만 개의 인스턴스가 생성되었습니다. 이 액션 생성기는 이 멀티 태스크 교육 세트에서 완전히 미세 조정되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드 생성기: 견고한 코딩 능력으로 유명한 DEEPSEEKCODER-7B-INSTRUCT-V1.5 모델이 코드 생성기 세밀 조정의 기반으로 선택되었습니다. 올바른 해결 경로를 사용하여 필요한 모든 코드를 추출하였고, 결과적으로 44K의 코드 인스턴스가 훈련을 위해 생성되었습니다.\n\n수학 추론기: 진보된 수학적 추론 능력으로 DEEPSEEKMATH-7B-INSTRUCT 모델이 선택되었습니다. 올바른 해결 경로를 통해 30K의 수학 해결 방법 인스턴스가 수학 추론기 세밀 조정을 위해 제공되었습니다.\n\n쿼리 생성기: 쿼리 생성기에는 LLAMA-2-7B가 기반 모델로 사용되었습니다. 올바른 해결 경로가 22K의 검색 쿼리 인스턴스를 쿼리 생성기 세밀 조정을 위해 제공하였습니다.\n\n일부 결과는 다음 매트릭스에서 설명되어 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_4.png)\n\n허스키는 언어 에이전트 분야에서 중요한 발전을 이룬 것으로, 복잡한 추론 작업에 대한 다재다능하고 오픈 소스의 솔루션을 제공합니다. 행동 생성과 실행을 전문가 모델과 결합하는 통합적인 방식은 다양한 도전에 효과적으로 대응할 수 있게 해줍니다. 다양한 평가에서 보여지는 허스키의 성능은 언어 에이전트가 복잡한 문제를 해결하는 방식을 재정의할 잠재력을 강조합니다.","ogImage":{"url":"/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_0.png"},"coverImage":"/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_0.png","tag":["Tech"],"readingTime":5}],"page":"63","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"63"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>