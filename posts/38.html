<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/38" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/38" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="스트림 프로세싱 엔진과 스트리밍 데이터베이스의 차이점" href="/post/2024-06-22-DifferencesBetweenStreamProcessingEnginesandStreamingDatabases"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="스트림 프로세싱 엔진과 스트리밍 데이터베이스의 차이점" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-DifferencesBetweenStreamProcessingEnginesandStreamingDatabases_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="스트림 프로세싱 엔진과 스트리밍 데이터베이스의 차이점" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">스트림 프로세싱 엔진과 스트리밍 데이터베이스의 차이점</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="SQL에서는 쉽지만 NoSQL에서는 어려운 다대일 관계 설정 방법" href="/post/2024-06-22-Many-to-ManyOneRelationshipsareSimpleinSQLbutHardinNoSQL"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="SQL에서는 쉽지만 NoSQL에서는 어려운 다대일 관계 설정 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-Many-to-ManyOneRelationshipsareSimpleinSQLbutHardinNoSQL_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="SQL에서는 쉽지만 NoSQL에서는 어려운 다대일 관계 설정 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">SQL에서는 쉽지만 NoSQL에서는 어려운 다대일 관계 설정 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="GA4 BigQuery Export 스키마와 구조 이해하기" href="/post/2024-06-22-UnderstandingtheGA4BigQueryExportSchemaandStructure"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="GA4 BigQuery Export 스키마와 구조 이해하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-UnderstandingtheGA4BigQueryExportSchemaandStructure_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="GA4 BigQuery Export 스키마와 구조 이해하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">GA4 BigQuery Export 스키마와 구조 이해하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="파이썬 Enum 모듈 완벽 가이드 마스터하기 위한 모든 것" href="/post/2024-06-22-MasterPythonEnumModuleAComprehensiveGuide"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="파이썬 Enum 모듈 완벽 가이드 마스터하기 위한 모든 것" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-MasterPythonEnumModuleAComprehensiveGuide_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="파이썬 Enum 모듈 완벽 가이드 마스터하기 위한 모든 것" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">파이썬 Enum 모듈 완벽 가이드 마스터하기 위한 모든 것</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">17<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Airflow, DuckDB, Streamlit으로 StarCraft 2 데이터 탐험하기" href="/post/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Airflow, DuckDB, Streamlit으로 StarCraft 2 데이터 탐험하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Airflow, DuckDB, Streamlit으로 StarCraft 2 데이터 탐험하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Airflow, DuckDB, Streamlit으로 StarCraft 2 데이터 탐험하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">27<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Spark 심화 학습 Delta 테이블에서 Z-Ordering과 Data Skipping 사용법" href="/post/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Spark 심화 학습 Delta 테이블에서 Z-Ordering과 Data Skipping 사용법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Spark 심화 학습 Delta 테이블에서 Z-Ordering과 Data Skipping 사용법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Spark 심화 학습 Delta 테이블에서 Z-Ordering과 Data Skipping 사용법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="BigQuery에서 함수형 데이터 엔지니어링 가이드" href="/post/2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="BigQuery에서 함수형 데이터 엔지니어링 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="BigQuery에서 함수형 데이터 엔지니어링 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">BigQuery에서 함수형 데이터 엔지니어링 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">16<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="품질 엔지니어를 위한 RAG 사용 가이드" href="/post/2024-06-22-RAGforQualityEngineers"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="품질 엔지니어를 위한 RAG 사용 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-RAGforQualityEngineers_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="품질 엔지니어를 위한 RAG 사용 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">품질 엔지니어를 위한 RAG 사용 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">13<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 메시에 대한 도전 과제 및 해결책  3부" href="/post/2024-06-22-ChallengesandSolutionsinDataMeshPart3"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 메시에 대한 도전 과제 및 해결책  3부" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-ChallengesandSolutionsinDataMeshPart3_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 메시에 대한 도전 과제 및 해결책  3부" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 메시에 대한 도전 과제 및 해결책  3부</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label=" Docker로 Apache Spark와 Minio를 사용하여 Apache Iceberg 활용하는 방법" href="/post/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker"><div class="PostList_thumbnail_wrap__YuxdB"><img alt=" Docker로 Apache Spark와 Minio를 사용하여 Apache Iceberg 활용하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt=" Docker로 Apache Spark와 Minio를 사용하여 Apache Iceberg 활용하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl"> Docker로 Apache Spark와 Minio를 사용하여 Apache Iceberg 활용하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/21">21</a><a class="link" href="/posts/22">22</a><a class="link" href="/posts/23">23</a><a class="link" href="/posts/24">24</a><a class="link" href="/posts/25">25</a><a class="link" href="/posts/26">26</a><a class="link" href="/posts/27">27</a><a class="link" href="/posts/28">28</a><a class="link" href="/posts/29">29</a><a class="link" href="/posts/30">30</a><a class="link" href="/posts/31">31</a><a class="link" href="/posts/32">32</a><a class="link" href="/posts/33">33</a><a class="link" href="/posts/34">34</a><a class="link" href="/posts/35">35</a><a class="link" href="/posts/36">36</a><a class="link" href="/posts/37">37</a><a class="link posts_-active__YVJEi" href="/posts/38">38</a><a class="link" href="/posts/39">39</a><a class="link" href="/posts/40">40</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"스트림 프로세싱 엔진과 스트리밍 데이터베이스의 차이점","description":"","date":"2024-06-22 17:32","slug":"2024-06-22-DifferencesBetweenStreamProcessingEnginesandStreamingDatabases","content":"\n\n\u003cimg src=\"/assets/img/2024-06-22-DifferencesBetweenStreamProcessingEnginesandStreamingDatabases_0.png\" /\u003e\n\n리얼타임 분석 분야에서 빠르게 발전하는 가운데, 지난 10년간 다양한 스트림 처리 엔진이 등장했습니다. 주목할 만한 예시로는 Apache Storm, Apache Flink, Apache Samza 등이 있습니다. 이러한 엔진들은 다양한 기업에서 널리 받아들여져 실시간 처리 및 분석 애플리케이션에 상당한 지원을 제공하고 있습니다.\n\n지난 몇 년 동안 새롭고 흥미로운 혁신이 등장했습니다: 스트리밍 데이터베이스. PostgreSQL 플러그인으로 개발된 초기 솔루션이었던 PipelineDB를 시작으로, 카프카를 위해 디자인된 Confluent의 KsqlDB, 그리고 최근에 등장한 오픈 소스인 RisingWave - 분산 SQL 스트리밍 데이터베이스입니다. 이러한 시스템들은 꾸준히 수용과 인기를 얻어왔습니다.\n\n그럼 스트림 처리 엔진과 스트리밍 데이터베이스를 서로 교환해서 사용할 수 있을까요? 이 기사는 각각의 설계 원칙에 대해 알아보며, 이러한 흥미로운 기술들의 차이점, 유사성, 사용 사례, 그리고 잠재적인 미래 전망을 탐구합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 디자인 원칙\n\n스트림 처리 엔진과 데이터베이스는 데이터 스트림 처리에 중요한 기능을 제공합니다. 그러나 사용자 상호 작용 인터페이스 및 데이터 저장 옵션 측면에서 디자인 원칙에서 뚜렷한 차이가 있습니다. 이러한 기술이 등장한 역사적 배경을 명확히 이해하기 위해 각각의 독특한 특성에 대해 자세히 알아보기 전에 관심을 기울여야 합니다.\n\n# 스트림 처리 엔진의 진화\n\n데이터베이스 시스템은 60년 이상의 시간 동안 연구되어 왔지만, 배치 처리 및 스트림 처리를 포괄하는 컴퓨팅 엔진은 비교적 최근의 혁신입니다. 현대 스트림 처리로의 여정은 2004년 구글의 MapReduce 논문 발표로 시작되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMapReduce는 최고의 성능을 위해 상품용 기계 네트워크를 효율적으로 최적화하는 것을 목표로 했습니다. 이 높은 목표는 MapReduce에서 두 가지 주요 기능, Map과 Reduce,을 갖춘 세련된 저수준 프로그래밍 인터페이스의 도입으로 이어졌습니다. 이러한 설계는 유경험 프로그래머에게 핵심 기능에 직접 액세스할 수 있게 해주어 특정 비즈니스 논리를 구현하고 프로그램 병렬 처리를 제어하며 다른 복잡한 세부사항을 자체적으로 관리할 수 있도록 했습니다.\n\nMapReduce를 독특하게 만든 점은 데이터 저장을 원격 분산 파일 시스템과 같은 외부 시스템에 위임하여 처리에만 초점을 맞춘 것입니다. 이 분할은 오늘날의 스트림 처리 엔진에 영향을 주고 스트리밍 데이터베이스와의 중요한 차이점 및 시너지 파악을 위한 무대를 설정했습니다.\n\n회사 내에서 MapReduce를 성공적으로 활용하기 위해 다음 세 가지 중요 사전 조건이 필요했습니다:\n\n- 회사는 상당량의 데이터와 관련 비즈니스 시나리오를 내부에 보유해야 합니다.\n- 회사는 상품용 기계에 충분히 액세스할 수 있어야 합니다.\n- 회사는 숙련된 소프트웨어 엔지니어 집단을 고용해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 세 가지 조건은 맵리듀스가 2004년에 소개될 때 대부분의 기업들이 달성하기 어려웠던 벽이었습니다. 소셜 네트워크와 모바일 인터넷의 폭발적인 성장으로 2010년 이후에야 첫 번째 조건이 충족되기 시작했습니다. 이러한 변화로 주요 기업들은 스트림 처리 기술에 관심을 기울이고, 두 번째와 세 번째 전제 조건을 충족하기 위해 크게 투자했습니다. 이 투자는 2010년쯤부터 시작된 스트림 처리 엔진의 발전 시대를 의미했습니다. 이 기간 동안 Apache Storm, Apache Samza, Apache Flink 등 다수의 탁월한 스트림 처리 엔진이 등장했습니다.\n\n이 신생 스트림 처리 엔진들은 맵리듀스 디자인 패턴의 핵심 원칙을 완전히 수용했습니다. 구체적으로 다음과 같습니다:\n\n- 사용자에게 저수준 프로그래밍 인터페이스 노출;\n- 데이터 스토리지 제어 양보.\n\n이 디자인은 빅데이터 시대에 적합하게 개조되었습니다. 일반적으로 대량 데이터 처리를 필요로 하는 기술 기업들은 자체 데이터 센터와 전문 엔지니어링 팀을 보유했습니다. 이 기업들이 필요로 한 것은 성능 향상과 더 다채로운 프로그래밍 패러다임이었습니다. 전문 엔지니어링 팀과 함께하면 보통 분산 파일 시스템을 배포하여 방대한 데이터 스토리지를 처리할 수 있었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 2015년 이후 클라우드 컴퓨팅 기술의 혁신적인 발전으로 풍경이 변화하기 시작했습니다. 기술적 배경이 부족한 기업들조차도 스트림 처리 기술에 접근하고자 했습니다. 이 수요에 대응하여 스트림 처리 엔진은 SQL을 채택해 간결하고 폭넓게 인정받는 프로그래밍 언어가 되었습니다. 이 적응으로 인해 더 많은 사용자들이 스트림 처리 기술의 혜택을 누릴 수 있게 되었습니다. 그 결과, 오늘날 주요한 스트림 처리 엔진은 사용자 상호작용에 계층적 접근 방식을 제공하며 Java와 Scala와 같은 낮은 수준의 프로그래밍 인터페이스와 더 접근성 있는 고수준의 SQL 프로그래밍 인터페이스를 제공합니다.\n\n# 사용자 상호작용 인터페이스\n\n현대의 스트림 처리 엔진은 Java와 Scala와 같은 낮은 수준의 프로그래밍 인터페이스뿐만 아니라 SQL 및 Python과 같은 고수준 인터페이스도 제공합니다. 이러한 인터페이스는 병렬성과 같은 다양한 시스템 런타임 세부 정보를 노출하여 사용자가 응용프로그램의 설령 미묘한 측면들을 제어할 수 있게 합니다. 다른 한편으로, 스트리밍 데이터베이스는 주로 SQL 인터페이스를 특징으로 하며 런타임 복잡성을 단순화합니다. 일부는 파이썬 및 자바와 같은 언어로 사용자 정의 함수(UDF)를 제공하여 표현 능력을 향상시킵니다. 이러한 사용자 상호작용 인터페이스의 다양성은 두 가지 주요 균형을 유발합니다.\n\n1. 유연성과 사용 편의성 사이의 균형 유지\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n능숙한 프로그래머들은 Java와 Scala와 같은 저수준 인터페이스가 제공하는 풍부한 표현 능력을 누릴 수 있습니다. Turing-complete 언어인 Java, Scala 및 유사한 언어는 이론적으로 사용자가 어떤 로직이든 표현할 수 있도록 합니다. 기존 Java 및 Scala 라이브러리를 보유한 기업에게는 특히 유리합니다.\n\n그러나 이러한 접근법은 Java나 Scala에 익숙하지 않은 사람들을 막을 수 있으며 복잡함 때문에 시간이 많이 소요될 수 있습니다. 시스템의 사용자 정의 API를 마스터하는 것은 기술적인 사용자에게도 어려울 수 있습니다.\n\n스트리밍 데이터베이스에서 더 높은 수준의 SQL 인터페이스를 통해 사용 편의성을 강조하면 몇 가지 사용 가능한 도전 과제가 있습니다. 첫 번째는 SQL 지원의 완성도로, 단순한 DML (데이터 조작 언어) 문이 아닌 복잡한 DDL (데이터 정의 언어) 작업이 필요할 수 있습니다. 사용자들은 종종 롤, 사용자, 인덱스 생성 또는 삭제와 같은 복잡한 작업이 필요합니다. 두 번째는 SQL 생태계의 지원으로, DBeaver나 pgAdmin과 같은 관리 도구와의 호환성을 포함하여 추가 노력이 필요할 수 있습니다.\n\n저수준 인터페이스는 기술 중심 사용자에게 더 많은 유연성을 제공하며, 스트리밍 데이터베이스의 SQL 인터페이스는 비즈니스 중심 사용자를 위해 특별히 설계되어 사용 편의성을 강조합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. 유연성과 성능의 균형\n\n낮은 수준의 프로그래밍 인터페이스의 가용성은 사용자가 이전 지식을 활용하여 시스템 성능을 최적화할 수 있게 합니다. 심층적인 프로그래밍 전문 지식과 비즈니스 논리 이해력을 갖춘 사람들은 종종 낮은 수준의 인터페이스를 통해 탁월한 성능을 얻을 수 있습니다. 그러나 이에는 회사가 엔지니어링 팀을 구축하기 위해 더 많은 투자를 해야 할 수도 있습니다.\n\n대조적으로, 고수준 프로그래밍 인터페이스를 사용할 때 계층적 스트림 처리 엔진은 성능 단점을 겪을 수 있습니다. 캡슐화는 성능 오버헤드를 야기하며 캡슐화 계층이 더 많을수록 성능이 더 나빠집니다. 스트림 처리 엔진의 중간 계층은 하위 디자인이 고수준 논리에 어둡게 되어 성능 손실을 야기할 수 있습니다. 비교적으로, SQL 인터페이스와 최적화 기능만을 제공하는 스트리밍 데이터베이스는 더 높은 성능 수준에 도달할 수 있습니다.\n\n요약하면, 낮은 수준의 인터페이스를 갖춘 스트림 처리 엔진은 기술 중심 사용자가 성능 이점을 얻을 수 있도록 해줍니다. 반면 SQL 인터페이스를 제공하는 스트리밍 데이터베이스는 비즈니스 중심 사용자를 위해 맞춤화되어 있으며 복잡한 기술 전문 지식이 필요 없이도 더 높은 성능 경계를 달성할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터 저장\n\n사용자 상호 작용 인터페이스 외에도 스트림 처리 엔진과 스트리밍 데이터베이스 사이의 가장 중요한 차이점은 시스템이 데이터를 저장하는지 여부입니다. 스트림 처리에서 데이터 저장의 유무는 성능, 오류 복구, 확장성 및 사용 사례와 같은 시스템의 여러 측면에 직접적으로 영향을 미칩니다.\n\n스트림 처리 엔진에서 데이터 입력 및 출력은 일반적으로 HDFS와 같은 원격 분산 파일 시스템과 같은 외부 시스템에서 발생합니다. 반면, 스트리밍 데이터베이스는 계산 능력 뿐만 아니라 데이터 저장 기능도 포함하고 있습니다. 이는 적어도 두 가지 작업을 수행할 수 있다는 것을 의미합니다: 1) 입력을 저장하고, 2) 출력을 저장합니다.\n\n데이터 입력 측면에서 데이터 저장은 중요한 성능 이점을 제공합니다. 예를 들어, 데이터 입력을 저장하는 경우 성능상의 이점이 있습니다. 메시지 큐(예: Kafka)에서의 데이터 스트림과 원격 데이터베이스(예: MySQL)에 저장된 테이블을 결합하는 단순한 스트림 처리 엔진의 시나리오를 고려해 보겠습니다. 스트림 처리 엔진이 데이터 저장 기능이 없는 경우, 새로운 데이터 항목이 스트림에 들어올 때마다 연산을 수행하기 전에 원격 데이터베이스에서 데이터를 가져와야 합니다. 이 접근 방식은 초기 분산 스트림 처리 엔진에서 채택되었습니다. 시스템 아키텍처를 간단화하는 장점이 있지만, 성능이 급격히 저하된다는 단점이 있습니다. 서로 다른 시스템 간의 데이터 접근은 명백히 높은 지연 시간을 유발하며, 지연 시간에 민감한 스트림 처리 엔진에 높은 지연 작업을 도입하면 성능이 떨어지게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스트림 처리 시스템 내부에 데이터를 저장(또는 캐싱)하면 시스템 간 데이터 접근을 피할 수 있어 성능을 향상시킬 수 있습니다. 내장 저장 기능을 갖춘 스트림 처리 시스템인 스트리밍 데이터베이스에서는 사용자가 필요에 따라 메시지 대기열에서 데이터 스트림을 가져와 원격 데이터베이스의 테이블과 결합해야 할 때 해당 테이블을 직접 스트리밍 데이터베이스로 복제할 수 있습니다. 이렇게 하면 모든 접근이 내부 작업으로 처리되어 효율적인 처리가 가능해집니다. 물론 이것은 단순화된 예시일 뿐입니다. 실제 시나리오에서는 원격 데이터베이스의 대규모 또는 동적으로 변화하는 테이블과 같은 문제가 발생할 수 있으나 이에 대해 자세히 다루지는 않겠습니다.\n\n출력 측면에서 출력 결과를 저장하는 것은 상당한 이점을 제공할 수 있습니다. 간단히 말해 네 가지 주요 장점은 다음과 같습니다:\n\n- 데이터 스택 아키텍처를 단순화: 계산을 위해 별도의 스트림 처리 엔진과 데이터 저장 및 쿼리 응답을 위한 별도의 저장 시스템을 사용하는 대신 스트리밍 데이터베이스와 같이 단일 시스템이 계산, 저장 및 쿼리 응답을 처리할 수 있습니다. 이 방식은 데이터 스택을 크게 단순화하고 종종 비용 절감을 이루어냅니다.\n- 계산 리소스 공유 용이: 스트림 처리 엔진에서는 계산 결과가 입력 데이터를 사용하고 계산을 수행한 후 외부 시스템으로 내보내기 때문에 그 결과를 시스템 내에서 직접 재사용하기가 어렵습니다. 내장 저장 기능을 갖춘 스트리밍 데이터베이스는 계산 결과를 내부적으로 머티얼라이징된 뷰로 저장해 다른 계산이 이 리소스에 직접 액세스하고 재사용할 수 있게 해줍니다.\n- 데이터 일관성 보장: 스트림 처리 엔진은 처리에 대한 정확히 한 번 문맥을 보장할 수 있지만 결과 액세스 일관성을 보장할 수는 없습니다. 스트림 처리 엔진은 저장소가 없기 때문에 계산 결과를 하류 저장 시스템으로 가져와야 합니다. 상류 엔진은 결과의 버전 정보를 하류 시스템으로 출력하여 하류 시스템의 사용자에게 보여지는 일관된 결과를 얻도록 해야 합니다. 이는 일관성 보장 부담을 사용자에게 지우게 됩니다. 반면, 저장 기능을 갖춘 스트리밍 데이터베이스는 시스템 내에 계산 진행 상태와 결과 버전 정보를 관리하여 사용자가 다중 버전 컨트롤을 통해 항상 일관된 결과를 보게끔 해줍니다.\n- 프로그램 해석 용이성 향상: 프로그램 개발 중에는 반복적인 수정과 정확성 검증이 흔합니다. 정확성을 검증하는 공통 접근 방법은 입력 및 출력을 얻어와 계산 논리가 기대에 부응하는지 수동으로 검증하는 것입니다. 이 작업은 배치 처리 엔진에서는 비교적 간단하지만 스트림 처리 엔진에서는 더 복잡합니다. 스트림 처리 엔진에서는 입력 및 출력이 동적으로 변하기 때문에 엔진이 저장소가 없고 정확성을 검증하기 위해 상류 메시지 소스, 하류 결과 저장 시스템 및 스트림 처리 엔진을 횡단해야 합니다. 게다가 사용자는 계산 진행 정보를 인식해야 해 복잡한 작업이 됩니다. 이에 비해 스트리밍 데이터베이스에서는 계산 결과가 데이터베이스 내에 저장되므로 상류 시스템에서 입력을 얻은 후(일반적으로 Kafka와 같은 메시지 대기열로 직접 오프셋을 가져오는 방식) 단일 시스템 내에서 결과를 검증하기만 하면 됩니다. 이는 개발 효율을 크게 향상시킵니다.\n\n물론 공짜 점심은 없으며 소프트웨어 개발은 단순해 보이는 해결책을 제공하지 않습니다. 스트림 처리 엔진과 비교했을 때 저장 기능을 갖춘 스트리밍 데이터베이스는 아키텍처, 리소스 활용, 일관성 및 사용자 경험에서 많은 장점을 제공합니다. 하지만 그에 따라 어떤 것을 희생해야 할까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 가지 중요한 희생은 소프트웨어 설계의 복잡성입니다. MapReduce의 설계를 기억해보세요. MapReduce의 개념은 오라클과 IBM과 같은 거대 기업이 전체 데이터베이스 시장을 지배하던 시절에 뛰어났습니다. 왜냐하면 이들은 많은 보통 컴퓨터를 사용하는 기술 기업을 위해 대규모 병렬 컴퓨팅을 가능케 하는 간단한 모델을 효과적으로 활용했기 때문입니다.\n\nMapReduce는 데이터베이스의 저장 및 연산 통합 개념을 직접 역전시켰으며, 연산을 독립적인 제품으로 분리했습니다. 이를 통해 사용자는 프로그래밍을 통해 연산을 확장할 수 있게 되었습니다. 다시 말해, MapReduce는 간소화된 아키텍처를 통해 대규모 수평 스케일링을 실현했으며, 높은 수준의 전문 기술 사용자의 유능성에 의존했습니다.\n\n그러나 시스템이 데이터 저장을 필요로 하는 경우, 모든 것이 더욱 복잡해집니다. 그러한 시스템을 사용 가능하게 만들기 위해 개발자는 고가용성, 장애 복구, 동적 스케일링, 데이터 일치성 및 기타 다양한 도전에 대한 섬세한 설계와 구현을 고려해야 합니다. 다행히도 지난 10년 동안 대규모 데이터베이스와 스트림 처리의 이론과 실무가 크게 발전해왔습니다. 따라서 지금은 스트리밍 데이터베이스의 등장에 이상적인 시기라고 할 수 있습니다.\n\n# 활용 사례\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n계산 모델에서 유사성을 공유하고 있지만, 스트리밍 처리 엔진과 스트리밍 데이터베이스는 여전히 고유한 응용 및 기능으로 이어지는 세세한 점들이 있습니다. 이러한 기술들은 응용 분야에서 중첩되어 있지만, 최종 사용자의 초점에서 차이가 나타납니다.\n\n스트림 처리 엔진은 일반적으로 기계 중심의 작업과 더 일치하며 데이터 저장 기능이 부족하며 종종 계산 결과 소비를 위해 하류 시스템과 통합이 필요합니다. 반면에, 스트리밍 데이터베이스는 더 많은 인간 상호 작용을 고려하여 저장 및 임의 쿼리 지원을 제공함으로써 직접적인 인간 참여를 가능하게 합니다.\n\n기계와 인간 사이의 이분법은 기계가 하드코딩된 프로그램의 고성능 처리를 요구하면서, 인간은 더 인터랙티브하고 사용자 친화적인 경험을 선호한다는 것을 보여줍니다. 이 본질적인 이견으로 인해 스트림 처리 엔진과 스트리밍 데이터베이스 간의 기능, 사용자 경험 및 기타 측면에서 차이가 생깁니다.\n\n요약하면, 이러한 기술들 간의 응용 시나리오에서의 공통점은 최종 사용자와 상호 작용 모드에 특정 초점이 있어 각 시스템 내에서 독특한 기능을 결과로 낳습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 역사적 차원으로 나아가는 거담아, 아니면 현재 시대의 트렌드일까요?\n\n데이터베이스와 컴퓨팅 엔진은 종종 서로 다른 두 가지 설계 철학을 반영합니다. 그것들은 독특한 학계 기여와 산업 발전을 통해 명백히 입증됩니다.\n\n학계에서는 컴퓨팅 엔진 논문들이 주로 OSDI, SOSP, EuroSys와 같은 시스템 회의에서 발표되는 반면, 데이터베이스 중심 작업들은 주로 SIGMOD 및 VLDB 회의에서 자주 볼 수 있습니다. 이 분할은 2008년 David DeWitt와 Michael Stonebraker에 의한 \"MapReduce: 역사적인 큰 걸음\" 비평에 의해 유명하게 강조되었습니다. 그들은 MapReduce가 역사적으로 후퇴적이고 데이터베이스에 비해 더 많은 혁신이 필요하다고 주장했습니다.\n\n스트림 처리의 영역에서, 다음 질문이 등장합니다: 어떤 철학이 역사적으로 후퇴를 대표하고, 어떤 것이 현재 시대의 트렌드를 대변할까요? 저는 스트림 처리 엔진과 스트리밍 데이터베이스가 앞으로 최소 3~5년 동안 공존하고 계속 발전할 것으로 판단합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스트림 처리 엔진은 극한의 성능과 유연성에 중점을 둬 기술적으로 뛰어난 사용자들을 대상으로 합니다. 한편, 스트리밍 데이터베이스는 사용자 경험의 우아함과 높은 성능 그리고 섬세한 내부 구현을 균형있게 갖추고 있습니다. 이러한 철학 간 상호 통합 트렌드는 SQL 인터페이스를 도입해 사용자 경험을 향상시키는 컴퓨팅 엔진과 UDF 기능을 활용해 프로그래밍 적응성을 높이는 데이터베이스 등 양쪽을 모두 강화시킵니다.\n\n# 스트림 처리의 미래 예측\n\n역사를 살펴보면, 스트리밍 데이터베이스 개념은 20년 전에 이미 제안되고 구현되었습니다. 예를 들어, 앞서 언급된 Aurora 시스템은 이미 스트리밍 데이터베이스입니다. 그러나 스트리밍 데이터베이스는 스트림 처리 시스템만큼 인기가 없습니다. 역사는 나선형 패턴으로 진행됩니다.\n\n빅 데이터 시대는 스트림 처리와 데이터베이스를 분리하고 오라클, IBM, Microsoft와 같은 세 거인의 독점을 무너뜨리는 트렌드를 경험했습니다. 최근 클라우드 시대에서는 2012년 전후부터 배치 처리 시스템 분야에서 Redshift, Snowflake, Clickhouse와 같은 시스템이 \"컴퓨팅 엔진\"을 \"데이터베이스\"로 돌아오게 하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금은 스트림 처리를 데이터베이스로 돌려보는 시기입니다. 이것이 현대 스트리밍 데이터베이스인 RisingWave와 Materialize의 주요 아이디어입니다. 그런데 왜 이 시점인 걸까요? 이 부분에서 자세히 분석해보겠습니다.\n\n20년 이상의 개발 노력을 거친 끝에, 스트림 처리는 상용적 채용이라는 측면에서 아직 초기 단계에 머물러 있습니다, 특히 배치 처리와 비교했을 때 말이죠. 그러나 산업계는 스트림 처리 방향에 대해 합의에 이르렀습니다. 스트림 처리 엔진과 스트리밍 데이터베이스 모두 스트림과 배치 처리를 지원해야 한다는 필요성에 동의합니다. 주요 차이점은 이 두 가지 다른 컴퓨팅 모델을 조화롭게 통합하는 데 있습니다. 본질적으로 “통합된 스트림과 배치 처리” 개념은 이 분야에서 공유되는 이해 관계가 되었습니다.\n\n이 통합을 달성하는 일반적인 세 가지 방법이 있습니다:\n\n- 단일 엔진 접근\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 전략은 스트림 처리 및 일괄 처리 기능을 관리하기 위해 동일한 컴퓨팅 엔진을 사용하는 것을 포함합니다. 이의 장점은 비교적 간단한 실행 계획과 더 통합된 사용자 경험을 제공한다는 것입니다. 그러나 스트림 처리 및 일괄 처리는 최적화 및 계산 방법과 같은 실행 측면에서 상당한 차이가 있기 때문에 각각을 구분하여 최고의 성능을 달성하기 위한 처리가 종종 필요합니다.\n\n2. 이중 엔진 접근\n\n이 접근 방식에서는 스트림 처리와 일괄 처리가 두 개의 고유한 시스템 엔진으로 별도로 구성됩니다. 각각을 특정 최적화할 수 있는 장점이 있지만, 공학 리소스의 상당한 투입, 공학 팀 내의 높은 협력 수준 및 엄격한 우선 순위 관리가 필요합니다. 양쪽을 조율하고 둘 다 원활하게 작동시키는 것이 상당한 도전이 될 수 있습니다.\n\n3. 섞여서 시스템 접근 방식\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 세 번째 접근 방식은 새로운 이상적인 시스템을 만드는 대신 기존 시스템을 재활용하여 통합된 사용자 경험을 제공하는 것을 포함합니다. 엔지니어링에 가장 부담이 적을 수 있는 방법으로 보이지만, 매끄러운 사용자 경험을 제공하는 것은 매우 어려울 수 있습니다. 기존 시장 시스템은 인터페이스 지원에서 다양하며 SQL을 사용하더라도 다른 방언을 사용할 수 있습니다. 이러한 불일치로부터 사용자를 보호하는 것이 핵심적인 문제가 됩니다. 게다가, 여러 시스템을 조율하여 상호 작용하고, 서로 인식하도록 유지하는 것은 복잡한 엔지니어링 과제를 도입합니다.\n\n비록 스트림 처리 엔진과 스트리밍 데이터베이스는 일부 중복과 디자인 및 실제적인 적용에서 차이가 있을 수 있지만, 둘 다 해당 배치 처리 시스템과 조화를 이루려고 노력합니다. 채택할 접근 방식을 선택하는 것은 궁극적으로 사용자의 평가에 달려 있으며, 그들의 특정 시나리오와 필요에 대한 고려를 고려합니다. 이 의사 결정 프로세스는 각 시스템의 고유한 특성과 요구 사항을 이해하고, 넓은 컴퓨팅 환경 내에 어떻게 맞는지 강조합니다.","ogImage":{"url":"/assets/img/2024-06-22-DifferencesBetweenStreamProcessingEnginesandStreamingDatabases_0.png"},"coverImage":"/assets/img/2024-06-22-DifferencesBetweenStreamProcessingEnginesandStreamingDatabases_0.png","tag":["Tech"],"readingTime":12},{"title":"SQL에서는 쉽지만 NoSQL에서는 어려운 다대일 관계 설정 방법","description":"","date":"2024-06-22 17:31","slug":"2024-06-22-Many-to-ManyOneRelationshipsareSimpleinSQLbutHardinNoSQL","content":"\n\n![img](/assets/img/2024-06-22-Many-to-ManyOneRelationshipsareSimpleinSQLbutHardinNoSQL_0.png)\n\n- 소개\n- 일대다 관계\n  - 참조 패턴\n  - 내장 패턴\n  - 부분 내장 패턴\n- 다대다 관계\n  - 중간 컬렉션\n  - 양방향 및 단방향 다대다 관계\n- 결론\n\n# 소개\n\n본 글에서는 SQL 및 NoSQL에서 일대다 및 다대다 관계를 모델링하는 방법의 차이점을 살펴볼 것입니다. 또한 NoSQL (문서 지향 데이터베이스)에서 사용되는 다양한 패턴을 탐구하고, 언제 어떤 것을 적용해야 하는지 이해해 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 일대다 관계\n\n우리가 Article과 Comment 엔티티를 가진 애플리케이션을 개발하고 있다고 상상해 봅시다. 또한 요구사항은 다음과 같습니다:\n\n- 하나의 글에는 하나 이상의 댓글이 포함될 수 있습니다.\n- 하나의 댓글은 하나의 글에만 속할 수 있습니다.\n\n위 요구사항은 두 엔티티 간에 일대다 관계를 사용해야 함을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSQL 데이터베이스에서는 보통 Articles 및 Comments 두 테이블을 생성하고 외래 키(Comments 테이블에)를 생성하여 일대다 관계를 설정합니다. 클라이언트 코드는 이후 이 두 테이블을 조인하여 댓글이 달린 기사를 가져와야 합니다.\n\n![이미지](/assets/img/2024-06-22-Many-to-ManyOneRelationshipsareSimpleinSQLbutHardinNoSQL_1.png)\n\nNoSQL 데이터베이스에서는 일대다 관계를 정의하는 데 더 많은 옵션이 있습니다. 참조, 삽입 또는 혼합 방식 중 하나를 선택할 수 있습니다.\n\n참조 및 삽입 사이에서 선택하는 것은 NoSQL 문서 데이터베이스와 작업할 때 개발자들이 하는 주요 선택사항 중 하나입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 참조 패턴\n\nNoSQL에서 참조 패턴을 사용하는 것은 SQL에서 일대다 관계를 정의하는 것과 비슷합니다. 우리는 기사와 댓글의 두 개의 별도의 JSON 컬렉션을 정의하고, 그들 사이의 관계를 ID를 사용하여 수립할 수 있습니다:\n\n```js\n//기사 컬렉션 (부모)\n[\n  {\n    \"id\": 4,\n    \"title\": \"SQL에서는 다대다/일대다 관계가 간단합니다...\",\n    \"content\": \"...\",\n    \"author\": \"...\",\n    \"postedAt\": \"...\"\n  }\n]\n\n//댓글 컬렉션 (자식)\n[\n  {\n    \"id\": \"...\",\n    \"articleID\": 41,\n    \"content\": \"...\",\n    \"author\": \"...\",\n    \"rating\": \"...\"\n  }\n]\n```\n\n이러한 방식으로 문서를 참조할 때, 기사에 댓글 ID의 배열을 포함하는 것도 고려할 수 있습니다. 그러나 이것은 최적의 해결책이 아닐 수 있습니다. 왜냐하면 기사에는 많은 수의 댓글이 포함될 수 있어서, 문서에는 많은 댓글 ID 배열이 포함될 수 있기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n참조 패턴을 사용할 때, 보통 앱은 기사와 관련 댓글을 검색하기 위해 두 번의 쿼리를 수행해야 합니다. 그러나 MongoDB와 같은 문서 지향 데이터베이스는 컬렉션을 단일 읽기 쿼리에서 조인할 수 있는 기능을 제공합니다.\n\n참조 패턴을 사용해야 하는 경우에 대한 일반적인 고려 사항:\n\n✅ 자식 문서가 크거나 많은 경우. 또한 문서 데이터베이스의 문서 당 크기 제한을 고려해야 합니다. 참조 패턴은 이러한 제한을 피하는 데 도움이 됩니다.\n\n✅ 부모 문서와 그 자식 문서가 거의 동시에 읽히거나 쓰이지 않는 경우. 자식 문서가 앱에서 부모 문서와 별도로 사용되는 경우, 이를 서로 다른 컬렉션에 저장함으로써 읽기 및 쓰기 작업이 간편해집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n✅ 문서 데이터베이스에서 쿼리 기능이 제한적이기 때문에 복잡한 중첩 계층 컬렉션을 만드는 것을 피하는 것이 좋습니다.\n\n✅ 자식 문서는 다른 컬렉션의 다른 유형의 상위 문서에 의해 참조되어야 하며, 데이터 일관성 관리를 간소화하기 위해 참조가 사용될 수 있습니다.\n\n## 임베디드 패턴\n\n반면에, 코멘트 배열은 아티클 문서에 완전히 임베드될 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n// 글 모음\n[\n  {\n    \"id\": 41,\n    \"title\": \"매니-투-매니/원 관계는 SQL에서 간단합니다...\",\n    \"content\": \"...\",\n    \"author\": \"...\",\n    \"postedAt\": \"...\",\n    \"comments\": [ \n      {\n        \"id\": \"...\",\n        \"content\": \"...\",\n        \"author\": \"...\",\n        \"rating\": \"...\"\n      }\n    ]\n  }\n]\n```\n\n먼저, 포함은 연관된 댓글이 있는 글을 하나의 간단한 조회 작업으로 검색할 수 있도록 해줍니다.\n\n포함 패턴을 사용해야 하는 경우에 대한 일반적인 고려 사항:\n\n✅ 응용 프로그램이 부모 문서와 별도로 자식 문서를 검색(또는 업데이트)할 필요가 없는 경우(도메인 주도 설계의 집합 및 집합 루트 개념을 상기하세요).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n✅ 부모와 자식 문서를 한 번의 원자적 쓰기 작업으로 업데이트해야 하는 능력이 필요합니다 (대부분의 문서 지향형 데이터베이스에서 쓰기 작업은 보통 단일 문서 수준에서만 원자적입니다).\n\n✅ 내장 문서는 작고, 그 수도 많지 않습니다 (\"하나 대 소수\" 관계).\n\n✅ 자식 문서의 업데이트는 자식 문서가 생성된 후에 드물게 발생하거나 전혀 발생하지 않습니다.\n\n## Partial Embedded Pattern\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n세 번째 방법은 참조 및 포함 패턴의 조합입니다. 부모 문서를 검색할 때 자식 문서의 하위 집합만 정기적으로 읽어야 할 경우 사용할 수 있습니다.\n\n예를 들어, 글을 검색할 때 사용자에게 표시할 수 있는 최상위 댓글만 검색해야 할 수 있습니다. 다른 댓글은 나중에 필요할 때 검색하거나 전혀 필요하지 않을 수 있습니다.\n\n```js\n//Article collection (Parent)\n[\n  {\n    \"id\": 41,\n    \"title\": \"Many-to-Many/One Relationships are Simple in SQL...\",\n    \"content\": \"...\",\n    \"author\": \"...\",\n    \"postedAt\": \"...\",\n    \"comments\": [ //Only 1 of 2 comments included\n      {\n        \"id\": 1,\n        \"content\": \"...\",\n        \"author\": \"...\",\n        \"rating\": 10\n       }\n    ]\n  }\n]\n\n//Comment collection (Child)\n[\n  {\n    \"id\": 1,\n    \"articleID\": 41,\n    \"content\": \"...\",\n    \"author\": \"...\",\n    \"rating\": \"...\"\n  },\n  {\n    \"id\": 2,\n    \"articleID\": 41,\n    \"content\": \"...\",\n    \"author\": \"...\",\n    \"raing\": 3\n  }\n]\n```\n\n부분 포함된 패턴은 성능을 향상시키기 위해 단일 읽기 작업에서 글과 최상위 댓글을 검색하는 애플리케이션에 사용될 수 있으며 모든 댓글을 별도의 컬렉션에 저장하는 이점 중 일부를 유지할 수 있습니다. 그러나 일부 자식 문서가 두 군데에 나타나기 때문에 응용 프로그램은 일관성을 보장하기 위해 추가 작업을 수행해야 할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 다대다 관계\n\n우리가 프로젝트와 직원 엔티티를 가지고 있다고 상상해봅시다. 아래는 그들에 대한 요구 사항입니다:\n\n- 여러 직원이 하나의 프로젝트에서 일할 수 있습니다.\n- 동일한 직원이 동시에 여러 다른 프로젝트에 할당될 수 있습니다.\n\n여기서 우리는 다대다 관계가 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은-대-많은 관계란 한 테이블의 여러 레코드가 다른 테이블의 여러 레코드와 관련이 있다는 것을 의미합니다.\n\nSQL에서는 추가 테이블을 사용하여 많은-대-많은을 정의합니다:\n\n![Many-to-Many Relationship](/assets/img/2024-06-22-Many-to-ManyOneRelationshipsareSimpleinSQLbutHardinNoSQL_2.png)\n\nEmployeeToProjectAssignment 중간 테이블에는 외래 키뿐만 아니라 할당 이벤트에 대한 자세한 정보(이유, 날짜 등)를 제공하는 추가 속성도 포함될 수 있다는 점에 유의하십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNoSQL에서는 두 개의 컬렉션 간의 다대다 관계를 정의하는 여러 가지 방법이 있습니다:\n\n## 중간 컬렉션\n\n첫 번째 옵션은 SQL과 유사한 중간 컬렉션을 생성하는 것입니다:\n\n```js\n//프로젝트 컬렉션\n[\n  {\n    \"id\": 41,\n    \"title\": \"전자 상거래\",\n    \"priority\": \"...\",\n    \"dueDate\": \"...\"\n  }\n]\n\n//직원 컬렉션\n[\n  {\n    \"id\": 82,\n    \"name\": \"존 도우\",\n    \"email\": \"...\",\n    \"role\": \"...\"\n  }\n]\n\n//EmployeeToProjectAssignment (중간) 컬렉션\n[\n  {\n    \"id\": 1,\n    \"projectID\": 41,\n    \"employeeID\": 82,\n    \"reason\": \"...\",\n    \"date\": \"...\"\n  }\n]\r\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n중간 컬렉션을 사용할 수 있는 경우:\n\n✅ 다른 속성 (예: 이유, 날짜 등)을 외래 키 외에도 관계에 특정한 많은 속성을 저장해야 할 때\n\n✅ 많은 문서가 관련된 다대다 관계인 경우 중간 컬렉션을 사용하면 문서 크기 제한을 피할 수 있음\n\n✅ SQL에서 NoSQL로 테이블을 빠르게 마이그레이션해야 하는 경우? 중간 SQL 테이블에서 중간 NoSQL 컬렉션으로 데이터를 마이그레이션하는 것이 구현하기 가장 쉬운 방법일 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 양방향 및 단방향 다대다 관계\n\n다대다 관계를 정의하는 다음 옵션은 각 직원 문서에 프로젝트 ID 목록을 삽입하고 각 프로젝트 문서에 직원 ID 목록을 삽입하는 것입니다:\n\n```js\n//프로젝트 컬렉션\n[\n  {\n    \"id\": 41,\n    \"title\": \"전자 상거래\",\n    \"priority\": \"...\",\n    \"dueDate\": \"...\",\n    \"employees\": [ 1, 2 ]\n  }\n]\n\n//직원 컬렉션\n[\n  {\n    \"id\": 1,\n    \"name\": \"존 도\",\n    \"email\": \"...\",\n    \"role\": \"...\",\n    \"projects\": [ 41 ]\n  },\n  {\n    \"id\": 2,\n    \"name\": \"제인 도\",\n    \"email\": \"...\",\n    \"role\": \"...\",\n    \"projects\": [ 41 ]\n  }\n]\r\n```\n\n식별자를 포함하는 대신 문서 전체를 포함하는 것도 고려할 수 있습니다. 포함할 내용의 선택은 일대다 관계에 대해 설명된 것과 유사하며, 따라서 반복하지 않겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 참조 또는 포함을 사용하여 다대다 관계를 정의할 때 더 고려해야 할 흥미로운 점이 또 있습니다:\n\n이전의 JSON 예시로 돌아갑시다. 프로젝트 컬렉션에서 employees 배열을 완전히 제거해도 두 컬렉션 간의 다대다 관계가 유지될 수 있습니다. 프로젝트 문서가 검색되면 응용 프로그램에서 관련 직원을 찾기 위해 추가 작업을 수행해야 합니다(직원 컬렉션을 스캔하고 프로젝트 배열을 확인함).\n\n이 방법으로 다대다 관계를 모델링하는 것은 한쪽의 문서 수가 많고 다른 쪽은 매우 적은 경우 선택할 수 있습니다. 예를 들어, 단일 프로젝트에 많은 직원이 참여할 수 있지만, 직원은 한 번에 1~2개의 프로젝트만 작업할 수 있습니다. \n\n그러나 이 경우 문서 크기뿐만 아니라 고려해야 할 사항은 두 컬렉션/엔티티가 응용 프로그램에서 어떻게 사용될지도 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다른 예시를 상상해봅시다. 국가와 공식 언어 엔티티 간의 다대다 관계입니다. 이 응용 프로그램의 사용 사례가 특정 국가에서 어떤 언어가 사용되는지에만 관심이 있다면 그 반대는 아니라면, 공식 언어 문서에 국가 배열을 저장하는 것은 단순히 중복일 뿐입니다. 국가 문서에 언어 배열을 저장하는 것이 충분합니다.\n\n# 결론\n\nSQL 및 NoSQL 데이터베이스에서 엔티티간 관계를 정의하기 위해 가장 일반적으로 사용되는 두 가지 방법을 비교해보았습니다. 유연한 문서 데이터베이스의 성격으로 인해, NoSQL은 SQL에 비해 일대다 및 다대다 관계를 정의하는 데 더 많은 옵션을 제공합니다.\n\n특정 사례에 가장 적합한 옵션을 선택하려면 응용 프로그램 사용 사례, 데이터 액세스 패턴, 필요한 일관성 수준, 문서 크기 제한 및 기타 요소를 고려해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n읽어 주셔서 감사합니다. 내용이 마음에 드셨다면 아래 이야기도 확인해보세요:\n\n🔔 그리고 Buy Me a Coffee에서 제를 지원해 주시는 걸 고려해 주세요.","ogImage":{"url":"/assets/img/2024-06-22-Many-to-ManyOneRelationshipsareSimpleinSQLbutHardinNoSQL_0.png"},"coverImage":"/assets/img/2024-06-22-Many-to-ManyOneRelationshipsareSimpleinSQLbutHardinNoSQL_0.png","tag":["Tech"],"readingTime":7},{"title":"GA4 BigQuery Export 스키마와 구조 이해하기","description":"","date":"2024-06-22 17:28","slug":"2024-06-22-UnderstandingtheGA4BigQueryExportSchemaandStructure","content":"\n\n![GA4 BigQuery Export Schema](/assets/img/2024-06-22-UnderstandingtheGA4BigQueryExportSchemaandStructure_0.png)\n\n# 소개\n\n현재 전 세계적으로 15.6 백만 개의 웹 사이트에서 사용 중인 Google Analytics 4는 글로벌 데이터 스키마 중 가장 널리 내보내어진 것 중 하나일 수 있습니다. Google Analytics 4 데이터는 웹 사용자 인터페이스를 통해 액세스하거나 Looker Studio로 API를 통해 직접 접근할 수 있지만, 데이터 소유권을 유지하고자 한다면:\n   \n- 구글의 데이터 보유 정책을 넘어서 데이터 소유권을 보유하려면\n- 데이터 손실 가능성을 방지하기 위해 데이터를 보관하려면\n- 다른 내부 데이터 소스와 데이터를 결합하려면\n- 외부 소스, API 또는 LLM에서 데이터를 확장하려면\n- 사용자 정의 자동화 워크플로우를 구축하려면\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러면 GA4 내보내기를 빅쿼리로 활성화하는 것이 권장되는 접근 방식입니다. 설정 및 구성하기 매우 간단합니다.\n\n좋아요! 작업 완료!\n\n하지만 아직 끝난 게 아닙니다. 매일 빅쿼리 내보내기에 데이터가 나타나기 시작하면 구조가 다소 불규칙하다는 것을 알 수 있을 것입니다. 이는 직접 작업하기 매우 어려워지므로 데이터를 더 쉽게 활용할 수 있는 형식으로 정확하게 변환하는 방법에 대해 설명하는 이 문서를 통해 '하이브리드' 데이터 구조에 대해 설명하고 데이터에 접근하는 방법을 설명합니다. 이는 사용 사례에 적합한 형식으로 변환합니다.\n\n# 조사\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 스키마 요약과 쿼리에서 events_YYYYMMDD는 BigQuery의 Date-Sharded GA4 Export Table을 나타냅니다. 아래와 같은 구문 변형을 사용하여 쿼리할 수 있습니다.\n\n모든 데이터 선택\n\n```js\nSELECT *\nFROM [project_id].[ga4_dataset_name].events_*\n```\n\n날짜 범위 선택\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 예: 지난 7일간의 데이터 선택\n\nSELECT *\nFROM [project_id].[ga4_dataset_name].events_*\nWHERE _TABLE_SUFFIX\nBETWEEN FORMAT_DATE(\"%Y%m%d\", CURRENT_DATE - 7)\nAND FORMAT_DATE(\"%Y%m%d\", CURRENT_DATE))\n```\n\n## GA4 내보내기 스키마\n\nGA4 내보내기의 구조는 표준화되어 있으며 다음 상위 수준 스키마로 나타낼 수 있습니다:\n\n```js\nevents_YYYYMMDD\n    ├── event_date STRING   \n    ├── event_timestamp INTEGER \n    ├── event_name STRING   \n    ├── event_params ARRAY\u003cSTRUCT\u003e  \n    ├── event_previous_timestamp INTEGER    \n    ├── event_value_in_usd FLOAT    \n    ├── event_bundle_sequence_id INTEGER    \n    ├── event_server_timestamp_offset INTEGER   \n    ├── user_id STRING  \n    ├── user_pseudo_id STRING   \n    ├── privacy_info STRUCT \n    ├── user_properties ARRAY\u003cSTRUCT\u003e   \n    ├── user_first_touch_timestamp INTEGER  \n    ├── user_ltv STRUCT \n    ├── device STRUCT   \n    ├── geo STRUCT  \n    ├── app_info STRUCT \n    ├── traffic_source STRUCT   \n    ├── stream_id STRING    \n    ├── platform STRING \n    ├── event_dimensions STRUCT \n    ├── ecommerce STRUCT    \n    ├── items ARRAY\u003cSTRUCT\u003e \n    ├── collected_traffic_source STRUCT \n    └── is_active_user BOOLEAN\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일부 다른 열 데이터 유형이 있으므로 데이터 원본을 쿼리, 분석 및 후속 데이터 워크플로에 대비하도록 준비하는데 다른 처리가 필요합니다.\n\n## 간단한 데이터 유형\n\n이 하위 집합의 열은 각 행마다 열 당 하나의 연결된 값이 있어 각 열에 대해 특정 데이터 유형이 강제로 적용되는 의미에서 '간단한'으로 간주할 수 있습니다.\n\n```js\nevents_YYYYMMDD\n    ├── event_date STRING   \n    ├── event_timestamp INTEGER \n    ├── event_name STRING   \n    ├── event_previous_timestamp INTEGER    \n    ├── event_value_in_usd FLOAT    \n    ├── event_bundle_sequence_id INTEGER    \n    ├── event_server_timestamp_offset INTEGER   \n    ├── user_id STRING  \n    ├── user_pseudo_id STRING   \n    ├── user_first_touch_timestamp INTEGER  \n    ├── stream_id STRING    \n    ├── platform STRING \n    └── is_active_user BOOLEAN\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDATE 및 TIMESTAMP 필드 중 일부는 일부 유형 변환을 필요로 합니다. 이에 필요한 함수는 다음 기사에 포함되어 있습니다. 그러나 이러한 열은 SQL에서 쉽게 작업할 수 있습니다.\n\n간단한 STRUCT 데이터 유형\n\n다음 하위 집합 열은 모두 STRUCT 데이터 유형입니다. 이는 JSON 또는 JavaScript의 개체에 대응하는 구조, Python의 사전 또는 연관 배열로 일반화할 수 있는 구조입니다:\n\n```js\nevents_YYYYMMDD\n    ├── privacy_info STRUCT \n    ├── user_ltv STRUCT \n    ├── device STRUCT   \n    ├── geo STRUCT  \n    ├── app_info STRUCT \n    ├── traffic_source STRUCT   \n    ├── event_dimensions STRUCT \n    ├── ecommerce STRUCT   \n    └── collected_traffic_source STRUCT\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n빅쿼리에서 STRUCT 열은 하위 열의 컨테이너로 간주될 수 있으며, 쿼리 간결성 측면에서 일부 이점을 가지지만 몇 가지 추가적인 도전 과제를 도입하기도 합니다. 위에 나열된 STRUCT 열은 '간단한' 것으로 간주됩니다. 왜냐하면 '간단한' 데이터 유형(즉, ARRAY, STRUCT 또는 JSON 하위 열이 없는)만 포함하고 있기 때문입니다.\n\n예를 들어, geo STRUCT 열에는 이벤트 원점의 감지된 위치를 나타내는 다음과 같은 간단한 STRING 하위 열이 포함되어 있습니다:\n\n```js\nevents_YYYYMMDD\n└── geo STRUCT \n    ├── city STRING \n    ├── country STRING \n    ├── continent STRING \n    ├── region STRING \n    ├── sub_continent STRING \n    └── metro STRING \n```\n\n간단한 STRUCT 하위 열 값은 점 표기법을 사용하여 깔끔하게 액세스할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n-- events_YYYYMMDD 테이블에서 geo.city를 선택합니다.\n\nSELECT geo.city\nFROM events_YYYYMMDD\n```\n\n예시에서 도시 이름을 수정된 컬럼 이름으로 불러올 것이며, 이는 STRUCT 접두어 없이 불러온 것입니다 (예시에서는 city). 구조체 데이터 구조를 유지하기 위해, 다음 구문을 사용하여 명시적으로 포함할 하위 컬럼을 설정할 수 있습니다:\n\n```js\n-- events_YYYYMMDD 테이블에서 geo.country, geo.region, geo.city를 선택하며, geo라는 이름으로 결과를 반환합니다.\n\nSELECT \nSTRUCT (\ngeo.country,\ngeo.region,\ngeo.city) AS geo\nFROM events_YYYYMMDD\n```\n\n전체 STRUCT를 포함하거나 제외하는 것도 간단합니다. 전체 STRUCT를 포함하려면, 간단히 해당 STRUCT 열을 SQL 쿼리에서 다른 열처럼 선택하면 됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nSELECT geo\nFROM events_YYYYMMDD\n```\n\n특정 STRUCT 열 (및 모든 하위 열)을 제외하고 다른 모든 열을 포함하려면 EXCEPT 구문을 사용합니다:\n\n```js\nSELECT *\nEXCEPT (geo)\nFROM events_YYYYMMDD\n```\n\nSTRUCT에서 특정 하위 열을 제외하는 구문은 조금 더 길고 직관적이지 않습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sql\nSELECT \n(SELECT AS STRUCT geo.* EXCEPT(continent, sub_continent, metro)) AS geo\nFROM events_YYYYMMDD\n```\n\nLooker Studio에서 STRUCT 데이터 유형을 지원하지만 다른 비즈니스 인텔리전스 도구에서 문제를 일으킬 수 있으므로 데이터 통합을 지원하기 위해 열 이름 변경이 필요할 수 있습니다.\n\n복잡한 ARRAY`STRUCT` 데이터 유형\n\n이 데이터 구조는 각 배열의 STRUCT 데이터 유형을 저장하기 위해 사용되며, 배열에 포함된 각 STRUCT가 동일한 구조를 갖습니다. 데이터를 모델링하는 방법은 특정 컨텍스트에 따라 달라집니다. GA4 BigQuery 내보내기에서 다음 ARRAY`STRUCT` 열이 발견됩니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nevents_YYYYMMDD\n    ├── event_params ARRAY\u003cSTRUCT\u003e  \n    ├── user_properties ARRAY\u003cSTRUCT\u003e     \n    └── items ARRAY\u003cSTRUCT\u003e\n\n\nevent_params ARRAY`STRUCT`에는 GA4에서 설정한 표준 및 사용자 지정 이벤트 매개변수 배열이 포함되어 있습니다. user_properties ARRAY`STRUCT`에는 사용자 지정 사용자 속성 배열이 들어 있으며, items ARRAY`STRUCT`에는 사용자 정의 항목 및 메타데이터의 배열과 연결된 항목 매개변수의 다른 중첩 배열이 포함되어 있습니다.\n\n이전 예제보다 더 복잡하며, STRUCT 스키마를 살펴보면 현실이 더 복잡하다는 것이 빠르게 명확해집니다. 예를 들어, event_params ARRAY`STRUCT` 스키마는 다음과 같습니다:\n\n\nevents_YYYYMMDD\n└── event_params STRUCT\n    ├── key STRING \n    └── value STRUCT\n        ├── string_value STRING \n        ├── int_value INTEGER \n        ├── float_value FLOAT \n        └── double_value FLOAT\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n구글 애널리틱스 4 속성에 이벤트 매개변수가 추가되면 이들은 BigQuery로 간단한 STRUCT 형식으로 들어오지 않습니다. 따라서 이들은 BigQuery Studio의 스키마 탭에서 보이지 않고 점 표기법을 통해서도 접근할 수 없습니다.\n\n예를 들어 session_engaged 매개변수는 사용자 배포가 필요 없이 구글에서 생성한 표준 이벤트 매개변수로, 각 특정 이벤트가 참여 세션의 일부인지 여부를 결정할 수 있게 합니다.\n\n만약 event_params STRUCT가 다음과 같은 스키마를 가지고 있다면, session_engaged의 값은 점 표기법을 통해 접근할 수 있을 것입니다 (예: SELECT event_params.session_engaged FROM events_YYYYMMDD):\n\n```js\nevents_YYYYMMDD\n└── event_params STRUCT\n    ├── ga_session_id INTEGER \n    └── ...\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 실제 스키마를 고려할 때, session_engaged 값은 다음 SQL 구문을 사용하여 액세스됩니다:\n\n```js\nSELECT \n(SELECT value.int_value FROM UNNEST(event_params) WHERE key='session_engaged') AS `session_engaged`\nFROM events_YYYYMMDD\n```\n\n구문적으로 더 복잡하지만, 이 구문을 사용해야 하는 요구 사항은 추가적인 문제를 야기합니다: 찾고 있는 키의 정확한 이름을 알아야 하고, 예상 데이터 유형도 알아야 합니다. 이는 값 STRUCT가 네 가지 서로 다른 하위 열을 포함하고 있으며, 이 중 하나는 NULL 값이 아니어야 하며 나머지 세 개는 NULL 값이어야 합니다. 이 경우 INT64로 값이 예상되므로 해당 값을 추출합니다.\n\n이 복잡성을 이해하는 것은 이 데이터를 재구성하기 위한 전략을 개발할 때 중요합니다: 데이터의 특정 예상 특성을 알아야만 재구성할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 지식을 얻기 위한 계획이 서로 다른 변형 및 자동화 전략으로 향하는 근본적인 동력입니다.\n\n## 가정\n\n또한 원본 데이터에 대한 우리의 가정과 해당 가정 중 하나라도 잘못된 것으로 판명되었을 경우의 결과를 이해해야 합니다.\n\n데이터 형식 일관성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 예에서는 session_engaged가 정수 값일 것이라고 가정하고, 해당 값을 추출하는 쿼리는 정수 값을 추출합니다. 그러나 이전에 역사적 불일치가 있었고 관측된 값이 STRING에서 INTEGER로 변경되었을 경우, 위의 쿼리는 일부 데이터를 소실하지만 조용히 처리될 것입니다. 데이터를 기반으로 한 보고서는 부정확해질 것이지만 하류 사용자는 이를 알 수 없을 것입니다.\n\nBigQuery의 GA4 데이터는 Google에서 설명한 이유로 GA4 사용자 인터페이스나 GA4 API에서 관측된 데이터와 일치하지 않을 것으로 예상됩니다. 비교할 기준이 없는 상황에서 보니 작아 보이는 테스트되지 않은 가정이 검출되지 않은 잘못된 결과로 쉽게 이어질 수 있음을 의미합니다.\n\nsession_engaged 이벤트 매개 변수는 때때로 STRING 값으로 관측되기도 하고, 때로는 INTEGER 값으로 관측되기도 합니다. 따라서 소스 데이터에서 신뢰할 수 있게 이를 추출하려면 아래 쿼리와 같이 보다 고급 전략이 필요합니다:\n\n```js\nSELECT \n(SELECT COALESCE(value.int_value, SAFE_CAST(value.int_value AS INT64) FROM UNNEST(event_params) WHERE key='session_engaged') AS `session_engaged`\nFROM events_YYYYMMDD\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주의하세요! COALESCE는 비용이 많이 소요되는 작업이기 때문에 이 방법을 많은 이벤트 매개변수에 적용하면 쿼리가 리소스 부족으로 실패할 수 있습니다.\n\n고유키\n\n또한 위의 예시들은 각 키가 배열 `STRUCT` 내에서 고유하다고 가정하므로 SELECT… FROM UNNEST… 쿼리는 정확히 한 행을 반환할 것으로 예상됩니다. 이는 상위 시스템에서 강제되는 올바른 가정인 것으로 보입니다. 그러나 BigQuery에서 강제되는 제약 조건은 아닙니다.\n\n이는 미래에 한 행 내에서 이벤트 매개변수 키가 고유하지 않다는 것이 밝혀지면, 해당 값을 추출하는 데 사용된 서브쿼리는 단일 행 대신 여러 행을 반환하고 쿼리는 다음 오류와 함께 실패합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n쿼리 오류: Scalar subquery는 하나 이상의 요소를 생성함\n\nsingle row event_param 내에서 중복 키의 존재가 관찰되지 않았지만, 미래에 이러한 사례가 발생할 가능성이 있는 BigQuery의 기술적 제약은 없습니다. 즉, 앞으로 아무것도 변하지 않을 것을 가정할 수도 있고, 코드에서 잠재적인 변화에 대비할 수도 있습니다.\n\n# 결론\n\n문제는 당신이 아닌 데이터 구조입니다.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 데이터 구조\n\n본질적으로 BigQuery는 행, 열 및 데이터 조작을 위해 사용되는 SQL 기반 작업을 가진 관계형 데이터베이스로 구성됩니다. BigQuery는 INFORMATION_SCHEMA.COLUMNS를 통해 열 수준의 메타데이터를 사용할 수 있으며 BigQuery Studio 사용자 인터페이스에도 사용할 수 있습니다. 일반적으로 이러한 정보 중 하나를 검사함으로써 데이터 구조를 이해할 수 있습니다.\n\n복잡한 데이터 유형(배열, 구조)의 포함은 약간의 복잡성을 추가하지만 사용자 인터페이스와 INFORMATION_SCHEMA(둘 다 COLUMNS 및 COLUMN_FIELD_PATHS)에도 열 및 하위 열 메타데이터가 사용 가능하기 때문에 검사 및 자동화 사용 사례를 충족시킬 수 있습니다.\n\n그러나 배열`구조`열 내 여러 중첩 키-값 구조의 복잡한 구현으로 인해 데이터의 의미 구조를 메타데이터를 통해 이해하는 능력을 상실하므로 전형적인 관계 구조가 아닙니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기본적으로 event_params, user_properties 및 item_params 데이터 구조는 이 데이터 원본이 전형적인 관계형 구조가 아니라 관계형, 반정형 및 키-값 저장소 특성이 혼합된 구조임을 의미합니다. 이것은 복잡하며, 데이터의 내용이 변경될 때 (시간이 지남에 따라 예상대로), 아래쪽 모델에서 이 데이터를 포착하기 위해 출력 데이터의 스키마를 변경해야 합니다.\n\n즉, 하류 모델에서 데이터를 사용할 수 있도록 데이터를 준비하는 데 사용된 접근 방식은 이 예상된 스키마 진화를 고려해야 합니다.\n\n## 접근 방식\n\n이 복잡한 소스 데이터를 후속 사용을 위해 다시 구조화하는 몇 가지 접근 방식이 있습니다. 이 단계에서는 데이터 변환 로직을 고려하고 있으며, 데이터 변환을 트리거하거나 실행하는 데 사용된 접근 방식이 아닙니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n변환 코드는 다음과 같이 가능합니다:\n\n- 수동으로 제작되어서는 수동 검사와 시행착오가 필요합니다;\n- 다양한 GA4-BigQuery 인터넷 자료에서 수동으로 조정되었습니다;\n- AI로 생성되었지만 (일반적으로 좋은 아이디어는 아닙니다... 정확한지 어떻게 알 수 있을까요? SQL을 배우세요!);\n- Dataform이나 DBT와 같은 변환 도구에 내장된 모델을 사용하여 생성된 구성 요소;\n- 관측 데이터를 기반으로 변환 논리를 구축하는 자동화 방법.\n\n다음 기사에서는 위에 설명된 접근 방식을 사용하여 GA4 BigQuery 모델링 과제를 해결하는 다양한 옵션을 탐색할 것입니다.\n\n## 다음 단계\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문제를 식별하고 상세히 탐색하는 것은 문제를 해결하는 것과 같지 않다는 것을 기억해주세요!\n\n그러나 이 기사의 목적은 당신이 이 데이터셋을 다뤄야 했을 때 느낄 수 있는 'why?!' (또는 더 정확히는 'wtf?!')를 개략적으로 설명하고, 수많은 도전에 대처할 때 고려해야 하는 다양한 고려 사항을 논의하는 것입니다.\n\n이 시리즈의 다음 기사는 현재 존재하는 다양한 실용적 옵션을 탐험하며, 그 비용, 노력, 기술/기술 요구 사항 및 제한 사항에 대해 다룰 것입니다.\n\n다음 내용을 받아보려면 저를 팔로우해주세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 스키마 참조\n\n참고를 위해 전체 소스 데이터 스키마를 아래에 재현하고 있습니다. 편집 가능한 형식은 다음과 같습니다.\n\n```js\nevents_YYYYMMDD\n├── event_date STRING\n├── event_timestamp INTEGER\n├── event_name STRING\n├── event_params ARRAY\u003cSTRUCT\u003e\n│   ├── key STRING\n│   └── value STRUCT\n│       ├── string_value STRING\n│       ├── int_value INTEGER\n│       ├── float_value FLOAT\n│       └── double_value FLOAT\n├── event_previous_timestamp INTEGER\n├── event_value_in_usd FLOAT\n├── event_bundle_sequence_id INTEGER\n├── event_server_timestamp_offset INTEGER\n├── user_id STRING\n├── user_pseudo_id STRING\n├── privacy_info STRUCT\n│   ├── analytics_storage STRING\n│   ├── ads_storage STRING\n│   └── uses_transient_token STRING\n├── user_properties ARRAY\u003cSTRUCT\u003e\n│   ├── key STRING\n│   └── value STRUCT\n│       ├── string_value STRING\n│       ├── int_value INTEGER\n│       ├── float_value FLOAT\n│       ├── double_value FLOAT\n│       └── set_timestamp_micros INTEGER\n├── user_first_touch_timestamp INTEGER\n├── user_ltv STRUCT\n│   ├── revenue FLOAT\n│   └── currency STRING\n├── device STRUCT\n│   ├── category STRING\n│   ├── mobile_brand_name STRING\n│   ├── mobile_model_name STRING\n│   ├── mobile_marketing_name STRING\n│   ├── mobile_os_hardware_model STRING\n│   ├── operating_system STRING\n│   ├── operating_system_version STRING\n│   ├── vendor_id STRING\n│   ├── advertising_id STRING\n│   ├── language STRING\n│   ├── is_limited_ad_tracking STRING\n│   ├── time_zone_offset_seconds INTEGER\n│   ├── browser STRING\n│   ├── browser_version STRING\n│   └── web_info STRUCT\n│       ├── browser STRING\n│       ├── browser_version STRING\n│       └── hostname STRING\n├── geo STRUCT\n│   ├── city STRING\n│   ├── country STRING\n│   ├── continent STRING\n│   ├── region STRING\n│   ├── sub_continent STRING\n│   └── metro STRING\n├── app_info STRUCT\n│   ├── id STRING\n│   ├── version STRING\n│   ├── install_store STRING\n│   ├── firebase_app_id STRING\n│   └── install_source STRING\n├── traffic_source STRUCT\n│   ├── name STRING\n│   ├── medium STRING\n│   └── source STRING\n├── stream_id STRING\n├── platform STRING\n├── event_dimensions STRUCT\n│   └── hostname STRING\n├── ecommerce STRUCT\n│   ├── total_item_quantity INTEGER\n│   ├── purchase_revenue_in_usd FLOAT\n│   ├── purchase_revenue FLOAT\n│   ├── refund_value_in_usd FLOAT\n│   ├── refund_value FLOAT\n│   ├── shipping_value_in_usd FLOAT\n│   ├── shipping_value FLOAT\n│   ├── tax_value_in_usd FLOAT\n│   ├── tax_value FLOAT\n│   ├── unique_items INTEGER\n│   └── transaction_id STRING\n├── items ARRAY\u003cSTRUCT\u003e\n│   ├── item_id STRING\n│   ├── item_name STRING\n│   ├── item_brand STRING\n│   ├── item_variant STRING\n│   ├── item_category STRING\n│   ├── item_category2 STRING\n│   ├── item_category3 STRING\n│   ├── item_category4 STRING\n│   ├── item_category5 STRING\n│   ├── price_in_usd FLOAT\n│   ├── price FLOAT\n│   ├── quantity INTEGER\n│   ├── item_revenue_in_usd FLOAT\n│   ├── item_revenue FLOAT\n│   ├── item_refund_in_usd FLOAT\n│   ├── item_refund FLOAT\n│   ├── coupon STRING\n│   ├── affiliation STRING\n│   ├── location_id STRING\n│   ├── item_list_id STRING\n│   ├── item_list_name STRING\n│   ├── item_list_index STRING\n│   ├── promotion_id STRING\n│   ├── promotion_name STRING\n│   ├── creative_name STRING\n│   ├── creative_slot STRING\n│   └── item_params ARRAY\u003cSTRUCT\u003e\n│       ├── key STRING\n│       └── value STRUCT\n│           ├── string_value STRING\n│           ├── int_value INTEGER\n│           ├── float_value FLOAT\n│           └── double_value FLOAT\n├── collected_traffic_source STRUCT\n│   ├── manual_campaign_id STRING\n│   ├── manual_campaign_name STRING\n│   ├── manual_source STRING\n│   ├── manual_medium STRING\n│   ├── manual_term STRING\n│   ├── manual_content STRING\n│   ├── gclid STRING\n│   ├── dclid STRING\n│   └── srsltid STRING\n└── is_active_user BOOLEAN \n```","ogImage":{"url":"/assets/img/2024-06-22-UnderstandingtheGA4BigQueryExportSchemaandStructure_0.png"},"coverImage":"/assets/img/2024-06-22-UnderstandingtheGA4BigQueryExportSchemaandStructure_0.png","tag":["Tech"],"readingTime":14},{"title":"파이썬 Enum 모듈 완벽 가이드 마스터하기 위한 모든 것","description":"","date":"2024-06-22 17:26","slug":"2024-06-22-MasterPythonEnumModuleAComprehensiveGuide","content":"\n\n## 파이썬의 Enum 모듈로 코드를 향상시키세요\n\n![이미지](/assets/img/2024-06-22-MasterPythonEnumModuleAComprehensiveGuide_0.png)\n\n프로그래밍 세계에서 관련된 상수 집합을 관리하는 것은 종종 번거롭고 오류가 발생하기 쉽습니다. Enumerations 또는 Enums로 일반적으로 알려진 것은 값 집합에 대한 상징적인 이름을 정의함으로써 강력한 솔루션을 제공하여 코드의 가독성을 향상시키고 오류 가능성을 줄이는데 도움이 됩니다.\n\n다재다능하면서 널리 사용되는 프로그래밍 언어인 Python은 Enumerations의 생성과 관리를 간단하게 해주는 강력한 enum 모듈을 포함하고 있습니다. 상태 관리, 데이터 유효성 검사 또는 pandas Series 내에서 데이터를 분류하는 경우에도 Python의 enum 모듈이 중요한 역할을 할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글에서는 Python의 enum 모듈에 대해 자세히 살펴보고 그 기능, 실제 적용 및 모범 사례를 탐구할 것입니다. 이 글을 마치면 Python 프로젝트에서 Enum을 활용하여 더 깨끗하고 유지보수가 쉬운 코드를 작성하는 방법에 대해 철저히 이해하게 될 것입니다.\n\n## 나에 대해\n\n나는 상당히 오랫동안 Python을 사용해왔고 개발자로서의 여정에서 코드의 기능성과 효율성을 향상시키는 다양한 모듈과 라이브러리를 탐험해왔습니다. 그 중에서도 enum 모듈은 관련 상수를 효과적으로 관리하는 데 있어 단숨함과 효과성으로 빛을 발합니다. 당신이 경험 많은 개발자이든 시작하는 프로그래밍 여정이든 상관없이 Enum을 이해하고 활용함으로써 코딩 실천을 크게 개선할 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-22-MasterPythonEnumModuleAComprehensiveGuide_1.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Enumeration (Enum)이란 무엇인가요?\n\nEnumeration 또는 Enum으로 줄여 부르는 것은 요소 또는 멤버라고 불리는 일련의 명명된 값을 포함하는 구분된 데이터 유형입니다. 이러한 멤버들은 특정 값을 나타내는 상수이며, 변수가 미리 정의된 일련의 값 중 하나만 보유해야 하는 경우에 이상적인 Enum입니다.\n\n# Enums 사용 사례\n\nEnums는 다음과 같은 시나리오에서 특히 유용합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 상태 관리: 객체의 상태를 추적할 때, 프로세스의 단계(예: 보류 중, 처리 중, 완료)를 말합니다.\n- 구성 옵션: 특정 옵션 집합에 제한된 구성 값을 설정할 때 사용됩니다(예: 낮음, 중간, 높음).\n- 분류: 데이터를 분류할 때, 사용자 유형(예: 관리자, 사용자, 손님)와 같은 것을 나타낼 수 있습니다.\n\n# 다른 데이터 유형과의 비교\n\n리스트와 사전은 관련 값들을 저장하는 데 사용될 수 있지만, Enum은 여러 장점을 제공합니다:\n\n- 가독성: Enum을 이용하면 명확하고 읽기 쉬운 방식으로 관련된 상수를 정의할 수 있습니다.\n- 안전성: Enum은 잘못된 값을 할당하는 것을 방지하여 오류 발생 가능성을 줄입니다.\n- 유지 관리: 특히 관련된 상수 집합을 다룰 때, Enum은 유지보수와 업데이트가 더 쉽습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 예시 코드\n\nEnum의 사용을 설명하기 위해 Python에서 Enum을 정의하고 사용하는 간단한 예제를 시작해보겠습니다.\n\n```js\nfrom enum import Enum\n\n# 요일에 대한 Enum 정의\nclass Day(Enum):\n    SUNDAY = 1\n    MONDAY = 2\n    TUESDAY = 3\n    WEDNESDAY = 4\n    THURSDAY = 5\n    FRIDAY = 6\n    SATURDAY = 7\n\n# Enum 멤버에 접근\nprint(Day.MONDAY)           # 출력: Day.MONDAY\nprint(Day.MONDAY.name)      # 출력: MONDAY\nprint(Day.MONDAY.value)     # 출력: 2\n\n# 조건문에서 Enum 사용\n# 특정 요일이 토요일 또는 일요일인지 확인하는 함수\ndef is_weekend(day):\n    return day in (Day.SATURDAY, Day.SUNDAY)\n\nprint(is_weekend(Day.SATURDAY))  # 출력: True\nprint(is_weekend(Day.WEDNESDAY)) # 출력: False\n```\n\n코드 설명\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Enum 정의하기: 우리는 enum 모듈에서 Enum 클래스를 사용하여 Day라는 새로운 Enum을 정의합니다. 각 Enum 멤버는 연결된 정수 값과 함께 주의 하루를 나타냅니다.\n- 멤버에 접근하기: 우리는 Enum의 멤버에 이름(Day.MONDAY)을 사용하여 접근할 수 있습니다. 각 멤버는 이름과 값 속성을 가지고 있습니다.\n- 조건문에서 Enum 사용하기: Day.SATURDAY와 Day.SUNDAY와 비교하여 특정 날짜가 주말인지 확인하는 is_weekend 함수를 정의합니다.\n\n# Python의 enum 모듈 개요\n\nPython의 enum 모듈은 Python 3.4에서 도입되어 열거형을 만드는 표준화된 방법을 제공합니다. 이 모듈을 사용하면 개발자는 값들의 집합에 대한 상징적인 이름을 정의할 수 있어 고정된, 상수 값들을 더 읽기 쉽고 유지보수하기 좋은 방식으로 표현할 수 있습니다.\n\n# enum 모듈의 주요 기능\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 클래스 기반 Enum: Enum은 클래스로 정의되어 있어 객체지향 코드에 통합하고 이해하기 쉽습니다.\n- 형 안전성: Enum은 변수가 미리 정의된 값만 가질 수 있도록 보장하여 오류 발생 위험을 줄입니다.\n- 반복 및 비교: Enum은 반복 및 비교를 지원하여 다양한 애플리케이션에 유연하게 사용할 수 있습니다.\n\n## 설치\n\nPython 3.4 이상을 사용하는 경우 enum 모듈이 표준 라이브러리에 포함되어 있어 설치가 필요하지 않습니다. Python의 이전 버전을 사용하는 경우 pip를 사용하여 enum34 패키지를 설치할 수 있습니다:\n\n```js\npip install enum\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 기본 Enum 생성\n\nPython에서 Enum을 만드는 것은 간단합니다. Enum 클래스를 상속하는 클래스를 정의하고 Enum의 멤버를 클래스 속성으로 정의하면 됩니다.\n\n# 예제 코드\n\nEnum 모듈의 또 다른 예제를 살펴봅시다. 이 예제에서는 로그와 함께 enum 모듈을 사용하여 Enum을 정의하고 사용하는 방법을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom enum import Enum\n\n# 다른 로깅 레벨을 위한 Enum 정의하기\nclass LogLevel(Enum):\n    DEBUG = 10\n    INFO = 20\n    WARNING = 30\n    ERROR = 40\n    CRITICAL = 50\n\n# Enum 멤버에 접근하기\nprint(LogLevel.DEBUG)         # 결과: LogLevel.DEBUG\nprint(LogLevel.DEBUG.name)    # 결과: DEBUG\nprint(LogLevel.DEBUG.value)   # 결과: 10\n\n# 함수에서 Enum 사용하기\ndef log_message(level, message):\n    if level == LogLevel.DEBUG:\n        print(f\"DEBUG: {message}\")\n    elif level == LogLevel.INFO:\n        print(f\"INFO: {message}\")\n    elif level == LogLevel.WARNING:\n        print(f\"WARNING: {message}\")\n    elif level == LogLevel.ERROR:\n        print(f\"ERROR: {message}\")\n    elif level == LogLevel.CRITICAL:\n        print(f\"CRITICAL: {message}\")\n\n# 로깅 메시지\nlog_message(LogLevel.INFO, \"이것은 정보 메시지입니다.\")\nlog_message(LogLevel.ERROR, \"이것은 오류 메시지입니다.\")\n```\n\n코드 설명\n\n- Enum 정의: LogLevel이라는 Enum을 정의하여 다른 로깅 레벨을 표현합니다. 각 멤버는 로깅 심각성에 해당하는 정수 값을 가지고 있습니다.\n- 멤버 접근: LogLevel Enum의 멤버에 접근하여 이름과 값 가져오기\n- 함수에서 Enum 사용: LogLevel Enum을 사용하여 로깅 레벨을 결정하고 적절한 메시지 출력하기.\n\n# Enum 멤버 및 속성\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nEnum의 각 구성원은 두 가지 키 속성을 가지고 있어요:\n\n- name: 구성원의 이름.\n- value: 구성원과 연결된 값.\n\n여기 과일 종류를 위한 Enum을 만드는 간단한 예제가 있어요:\n\n```js\nfrom enum import Enum\n\n# 다양한 종류의 과일을 위한 Enum 정의\nclass Fruit(Enum):\n    APPLE = 1\n    BANANA = 2\n    CHERRY = 3\n\n# Enum 구성원에 접근\nprint(Fruit.APPLE)           # 출력: Fruit.APPLE\nprint(Fruit.APPLE.name)      # 출력: APPLE\nprint(Fruit.APPLE.value)     # 출력: 1\n\n# 조건문에서 Enum 사용\ndef is_favorite_fruit(fruit):\n    return fruit == Fruit.CHERRY\n\nprint(is_favorite_fruit(Fruit.CHERRY))  # 출력: True\nprint(is_favorite_fruit(Fruit.BANANA))  # 출력: False\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 속성에는 각각 .name 및 .value 속성을 사용하여 액세스할 수 있습니다.\n\n다음은 이러한 속성을 사용하는 방법을 보여주는 예제입니다:\n\n```js\nfrom enum import Enum\n\n# 상태 코드를 위한 Enum 정의\nclass StatusCode(Enum):\n    SUCCESS = 200\n    NOT_FOUND = 404\n    SERVER_ERROR = 500\n\n# 멤버 이름 및 값에 액세스\nfor status in StatusCode:\n    print(f\"{status.name} = {status.value}\")\n\n# 출력:\n# SUCCESS = 200\n# NOT_FOUND = 404\n# SERVER_ERROR = 500\n```\n\n코드 설명\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Enum 정의: HTTP 상태 코드를 나타내기 위해 StatusCode라는 Enum을 정의합니다. 각 멤버는 연관된 정수 값이 있습니다.\n- 멤버 순회: for 루프를 사용하여 StatusCode Enum의 멤버를 순회하고 이름과 값들을 출력합니다.\n\n# 함수에서 Enum 사용하기\n\nEnum은 함수의 인수로 전달될 수 있어서 코드를 더 가독성 있고 명확하게 만들어줍니다.\n\n함수에서 Enum 사용을 보여주기 위해 StatusCode Enum 예제를 확장해봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom enum import Enum\n\n# 상태 코드를 위한 Enum 정의\nclass StatusCode(Enum):\n    SUCCESS = 200\n    NOT_FOUND = 404\n    SERVER_ERROR = 500\n\n# 상태 메시지를 가져오는 함수\ndef get_status_message(status):\n    if status == StatusCode.SUCCESS:\n        return \"요청이 성공했습니다.\"\n    elif status == StatusCode.NOT_FOUND:\n        return \"자원을 찾을 수 없습니다.\"\n    elif status == StatusCode.SERVER_ERROR:\n        return \"내부 서버 오류가 발생했습니다.\"\n\n# Enum 멤버를 사용하는 함수 호출\nprint(get_status_message(StatusCode.SUCCESS))      # 결과: 요청이 성공했습니다.\nprint(get_status_message(StatusCode.NOT_FOUND))    # 결과: 자원을 찾을 수 없습니다.\nprint(get_status_message(StatusCode.SERVER_ERROR)) # 결과: 내부 서버 오류가 발생했습니다.\n```\n\n코드 해석\n\n- Enum 정의: StatusCode라는 Enum을 정의하고 다른 HTTP 상태 코드를 나타내는 멤버를 가집니다.\n- Enum 인자를 받는 함수: StatusCode Enum 멤버를 인자로 받아 해당하는 메시지를 반환하는 get_status_message 함수를 정의합니다.\n- 함수 사용: get_status_message 함수를 다른 StatusCode 멤버와 함께 호출하여 사용법을 보여줍니다.\n\n# 조건문과 함께 Enum 사용하기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nEnum을 사용하면 코드의 가독성과 유지 보수성을 높일 수 있어요. Enum 멤버를 비교하여 Enum 값에 따라 특정 코드를 실행할 수 있어요.\n\n## 코드 예시\n\n아래는 조건문에서 Enum을 사용하는 예시에요:\n\n```js\nfrom enum import Enum\n\n# 다양한 종류의 차량을 위한 Enum 정의\nclass VehicleType(Enum):\n    CAR = 1\n    TRUCK = 2\n    MOTORCYCLE = 3\n    BICYCLE = 4\n\n# 운전 면허가 필요한 차량인지 확인하는 함수\ndef requires_license(vehicle):\n    if vehicle in (VehicleType.CAR, VehicleType.TRUCK, VehicleType.MOTORCYCLE):\n        return True\n    elif vehicle == VehicleType.BICYCLE:\n        return False\n\n# 다양한 VehicleType 멤버를 이용한 함수 호출\nprint(requires_license(VehicleType.CAR))        # 출력: True\nprint(requires_license(VehicleType.BICYCLE))    # 출력: False\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드 설명\n\n- Enum 정의: VehicleType이라는 Enum을 정의합니다. 이 Enum은 다양한 유형의 차량을 나타내는 멤버를 포함합니다.\n- 조건부 함수: VehicleType Enum 멤버를 인수로 받아 해당 차량이 운전면허가 필요한지 여부를 반환하는 requires_license 함수를 정의합니다.\n- 함수 사용: 다른 VehicleType 멤버를 사용하여 requires_license 함수를 호출하여 사용법을 보여줍니다.\n\n# 반복과 비교\n\nEnum은 반복과 비교를 지원하여 다양한 응용 프로그램에 유용합니다. Enum의 멤버를 반복하고 표준 비교 연산자를 사용하여 비교할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nEnum 멤버를 반복하고 비교하는 예제를 살펴보겠습니다:\n\n```js\nfrom enum import Enum\n\n# 심각도 레벨을 위한 Enum 정의\nclass Severity(Enum):\n    LOW = 1\n    MEDIUM = 2\n    HIGH = 3\n    CRITICAL = 4\n\n# Enum 멤버 반복\nfor level in Severity:\n    print(level.name, level.value)\n\n# 출력:\n# LOW 1\n# MEDIUM 2\n# HIGH 3\n# CRITICAL 4\n\n# Enum 멤버 비교\ndef compare_severity(level1, level2):\n    if level1 \u003e level2:\n        return f\"{level1.name}이(가) {level2.name}보다 심각합니다.\"\n    elif level1 \u003c level2:\n        return f\"{level1.name}이(가) {level2.name}보다 덜 심각합니다.\"\n    else:\n        return f\"{level1.name}과 {level2.name}은(는) 동등한 심각도입니다.\"\n\n# 비교 함수 사용\nprint(compare_severity(Severity.HIGH, Severity.MEDIUM))     # 출력: HIGH이(가) MEDIUM보다 더 심각합니다.\nprint(compare_severity(Severity.LOW, Severity.LOW))         # 출력: LOW와 LOW는 동등한 심각도입니다.\n```\n\n코드 설명\n\n- Enum 정의: Severity라는 Enum을 정의하고 다른 심각도 레벨을 나타내는 멤버를 정의합니다.\n- 멤버 반복: for 루프를 사용하여 Severity Enum의 멤버를 반복하고 이름과 값을 출력합니다.\n- 멤버 비교: 두 개의 Severity Enum 멤버를 인자로 받아서 두 심각도 레벨을 비교하는 compare_severity 함수를 정의합니다.\n- 비교 함수 사용: compare_severity 함수를 다른 Severity 멤버와 함께 호출하여 사용법을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-22-MasterPythonEnumModuleAComprehensiveGuide_2.png\" /\u003e\n\n# Enum의 실제 적용\n\n## 데이터 유효성 검증에서의 Enum\n\nEnum을 사용하여 데이터를 유효성 검사할 수 있습니다. 이는 특정 값을 허용하는 미리 정의된 집합에 속하는지 확인하여 데이터를 인증하는 데 특히 유용합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 데이터 유효성을 위해 Enum을 사용하는 예제입니다:\n\n```js\nfrom enum import Enum\n\n# 사용자 역할을 나타내는 Enum 정의\nclass UserRole(Enum):\n    ADMIN = 'admin'\n    USER = 'user'\n    GUEST = 'guest'\n\n# 사용자 역할을 유효성 검사하는 함수\ndef validate_user_role(role):\n    if role in UserRole.__members__.values():\n        return True\n    else:\n        raise ValueError(f\"잘못된 사용자 역할: {role}\")\n\n# 유효성 검사 함수 사용 예시\ntry:\n    print(validate_user_role(UserRole.ADMIN))  # 결과: True\n    print(validate_user_role('superuser'))     # ValueError 발생\nexcept ValueError as e:\n    print(e)\n```\n\n코드 설명\n\n- Enum 정의: UserRole이라는 Enum을 정의하고 다른 사용자 역할을 나타내는 멤버를 포함시킵니다.\n- 유효성 확인 함수: role을 인수로 받아서 유효한 UserRole 멤버인지 확인하는 validate_user_role 함수를 정의합니다. 유효하지 않을 경우 ValueError를 발생시킵니다.\n- 유효성 확인 함수 사용: 다른 역할로 validate_user_role 함수를 호출하여 사용법을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 상태 관리에서의 Enums\n\nEnums은 응용 프로그램 또는 객체의 상태를 관리하는 데에도 유용합니다. Enums를 사용하여 상태를 나타내면 유효한 상태만 할당되고 상태간 전환도 명확히 정의할 수 있습니다.\n\nEnums를 사용하여 상태 관리를 보여주는 예제를 살펴봅시다:\n\n```js\nfrom enum import Enum\n\n# 주문 상태에 대한 Enum 정의\nclass OrderStatus(Enum):\n    PENDING = 'pending'\n    PROCESSING = 'processing'\n    SHIPPED = 'shipped'\n    DELIVERED = 'delivered'\n    CANCELLED = 'cancelled'\n\n# 주문 상태를 관리하는 클래스\nclass Order:\n    def __init__(self, order_id):\n        self.order_id = order_id\n        self.status = OrderStatus.PENDING\n\n    def update_status(self, new_status):\n        if isinstance(new_status, OrderStatus):\n            self.status = new_status\n        else:\n            raise ValueError(f\"잘못된 상태: {new_status}\")\n\n# 주문 생성 및 상태 업데이트\norder = Order(order_id=123)\nprint(order.status)  # 출력: OrderStatus.PENDING\n\norder.update_status(OrderStatus.PROCESSING)\nprint(order.status)  # 출력: OrderStatus.PROCESSING\n\n# 유효하지 않은 상태로 업데이트 시도\ntry:\n    order.update_status('completed')  # ValueError 발생\nexcept ValueError as e:\n    print(e)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드 설명\n\n- Enum 정의: OrderStatus라는 Enum을 정의하며, 각 멤버는 주문의 다양한 단계를 나타냅니다.\n- 주문 클래스: OrderStatus Enum을 사용하여 주문의 상태를 관리하는 Order 클래스를 만듭니다.\n- 상태 업데이트: update_status 메서드는 새로운 상태가 유효한 OrderStatus 멤버인 경우 주문의 상태를 업데이트하고, 그렇지 않으면 ValueError를 발생시킵니다.\n- 클래스 사용법: Order 클래스의 인스턴스를 만들어 상태를 업데이트하고, 유효하지 않은 상태 업데이트에 대한 에러 처리를 보여줍니다.\n\n![MasterPythonEnumModuleAComprehensiveGuide_3.png](/assets/img/2024-06-22-MasterPythonEnumModuleAComprehensiveGuide_3.png)\n\n# 판다스 시리즈에서의 Enum들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 아마 기다리고 계셨던 순간입니다. 여기서는 Enum이 pandas Series와 함께 데이터 범주화에 어떻게 활용될 수 있는지 살펴보겠습니다. Enum을 사용하면 DataFrame 전체에서 일관된 범주를 유지할 수 있어 데이터 조작과 분석을 더 구조화되고 오류가 없게 만들 수 있습니다.\n\n## 데이터 범주화를 위한 Enum 사용\n\nEnum은 pandas에서 범주형 데이터를 다룰 때 특히 유용할 수 있습니다. Enum을 사용하여 범주를 정의하면 DataFrame 전체에서만 유효한 범주가 사용되도록 보장할 수 있습니다.\n\n다음은 pandas Series와 Enum을 사용하는 방법을 보여주는 예제입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport pandas as pd\nfrom enum import Enum\n\n# 제품 카테고리를 위한 Enum 정의\nclass ProductCategory(Enum):\n    ELECTRONICS = '전자제품'\n    CLOTHING = '의류'\n    FOOD = '식품'\n    TOYS = '장난감'\n\n# 샘플 DataFrame 생성\ndata = {\n    'Product': ['노트북', '티셔츠', '사과', '곰인형'],\n    'Category': [ProductCategory.ELECTRONICS.value, ProductCategory.CLOTHING.value, ProductCategory.FOOD.value, ProductCategory.TOYS.value]\n}\n\ndf = pd.DataFrame(data)\n\n# DataFrame 출력\nprint(df)\n\n# 출력:\n#       Product    Category\n# 0       노트북      전자제품\n# 1      티셔츠         의류\n# 2        사과          식품\n# 3      곰인형        장난감\n```\n\n코드 설명\n\n- Enum 정의: ProductCategory Enum을 정의하여 각각의 제품 카테고리를 나타냅니다.\n- DataFrame 생성: ProductCategory Enum의 멤버 값들을 사용하여 제품명과 해당 카테고리를 포함한 샘플 DataFrame을 생성합니다.\n- DataFrame 출력: Enum 값을 Category 열에 어떻게 사용하는지를 보여주기 위해 DataFrame을 출력합니다.\n\n## Pandas에서 Enum 사용의 장점\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 일관성: Enum을 사용하면 미리 정의된 카테고리만 사용하므로 오타와 불일치의 위험을 줄입니다.\r\n- 가독성: Enum을 사용하면 카테고리에 대한 명확하고 설명적인 이름을 제공하여 코드를 더 읽기 쉽게 만듭니다.\r\n- 유효성 검사: Enum을 사용하여 데이터가 DataFrame에 추가되기 전에 데이터를 유효성 검사할 수 있어 데이터 무결성을 보장합니다.\n\n## Enum을 사용한 데이터 유효성 검사 예제 코드\n\n다음은 pandas Series에 데이터를 추가하기 전에 Enum을 사용하여 데이터 유효성을 확인하는 예제입니다:\n\n```python\nimport pandas as pd\nfrom enum import Enum\n\n# 제품 카테고리를 위한 Enum 정의\nclass ProductCategory(Enum):\n    ELECTRONICS = '전자제품'\n    CLOTHING = '의류'\n    FOOD = '식품'\n    TOYS = '장난감'\n\n# 제품 카테고리를 검증하는 함수\ndef validate_category(category):\n    if category in ProductCategory._value2member_map_:\n        return True\n    else:\n        raise ValueError(f\"유효하지 않은 카테고리: {category}\")\n\n# 샘플 데이터\nproducts = ['노트북', '티셔츠', '사과', '곰 인형']\ncategories = ['전자제품', '의류', '식품', '가구']  # 참고: '가구'는 올바른 카테고리가 아닙니다.\n\n# 유효성을 검사하고 DataFrame 생성\nvalidated_data = {\n    '제품': [],\n    '카테고리': []\n}\n\nfor product, category in zip(products, categories):\n    try:\n        if validate_category(category):\n            validated_data['제품'].append(product)\n            validated_data['카테고리'].append(category)\n    except ValueError as e:\n        print(e)\n\ndf = pd.DataFrame(validated_data)\n\n# 유효성 검사된 DataFrame 출력\nprint(df)\n\n# 결과:\n# 유효하지 않은 카테고리: 가구\n#     제품      카테고리\n# 0  노트북     전자제품\n# 1  티셔츠       의류\n# 2  사과         식품\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드 설명\n\n- Enum 정의: ProductCategory라는 Enum을 정의합니다. 각 멤버는 다른 제품 카테고리를 표현합니다.\n- 유효성 검사 함수: validate_category라는 함수를 만들어 카테고리가 ProductCategory의 유효한 멤버인지 확인합니다.\n- 데이터 유효성 검사: DataFrame에 추가하기 전에 샘플 데이터를 유효성 검사합니다. 유효하지 않은 카테고리가 있으면 해당 항목을 잡아내고 오류 메시지를 출력합니다.\n- DataFrame 생성 및 표시: 유효한 데이터로 DataFrame을 생성하고 표시하며, 유효하지 않은 항목은 제외됩니다.\n\n## Enum 사용 시기\n\nEnum은 변수가 미리 정의된 값 집합으로 제한되어야 하는 경우에 이상적입니다. Enum이 특히 유용한 몇 가지 상황을 살펴보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 고정 상수 집합: 주간, 월, 또는 사용자 역할과 같이 변경될 가능성이 거의 없는 관련 상수 집합이 있는 경우에 사용합니다.\n- 상태 관리: 주문 상태, 프로세스 단계 또는 UI 상태와 같은 응용 프로그램이나 시스템에서 상태를 관리해야 할 때 사용합니다.\n- 구성 옵션: 로깅 수준 또는 환경 유형과 같이 특정 값으로 제한해야 하는 구성 옵션을 정의할 때 사용합니다.\n\n## 피해야 할 일반적인 함정\n\nEnum은 강력하고 유용하지만, 피해야 할 몇 가지 일반적인 함정이 있습니다:\n\n- Enum 과용: 모든 상수 집합이 Enum이 될 필요는 없습니다. Enum은 가독성, 안정성 및 유지 관리 측면에서 명확한 이점을 제공할 때에만 사용하세요.\n- 변경 가능한 Enum: Enum은 고정된 변경할 수 없는 값의 집합을 나타내어야 합니다. 런타임에서 Enum을 수정하는 것은 예상치 못한 동작 및 오류를 유발할 수 있으므로 피하세요.\n- 형식 혼합: Enum 내 모든 값이 동일한 형식이어야 합니다. 형식을 혼합하면 (예: 문자열 및 정수) 혼란과 오류가 발생할 수 있으니 주의하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 팁 및 권장 사항\n\n여기 Python 프로젝트에서 Enum을 효과적으로 사용하는 데 도움이 되는 몇 가지 팁과 권장 사항이 있습니다:\n\n- 설명적인 이름 사용: Enum 멤버에 명확하고 설명적인 이름을 선택하세요. 이렇게 하면 코드의 가독성이 향상되고 각 멤버의 목적을 이해하기 쉬워집니다.\n- 관련된 상수를 그룹화: Enum을 사용하여 관련된 상수를 함께 그룹화하세요. 이렇게 하면 코드가 더 정리되고 오류 발생 가능성이 줄어듭니다.\n- Enum 기능 활용: enum 모듈에서 제공하는 기능을 활용하세요. 예를 들어 반복, 비교, 멤버 속성 등의 기능을 사용하면 코드가 간소화되고 더 견고해질 수 있습니다.\n- Enum에 설명 추가: Enum에 설명을 제공하세요. 특히 복잡한 값이나 상태 집합을 나타내는 경우 다른 개발자가 올바르게 사용하는 방법을 이해하는 데 도움이 됩니다.\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n열거형(Enum)은 파이썬에서 가독성, 안전성 및 유지보수성을 향상시키는 강력한 기능입니다. 이름이 지정된 상수 집합을 정의함으로써, 열거형은 유효한 값들을 강제하고 오류를 줄이며 코드를 이해하기 쉽게 만듭니다.\n\n## 다루는 내용:\n\n- 열거형(Enum)의 기본: Enum이 무엇이고 왜 유용한지.\n- Python의 enum 모듈: Python에서 열거형을 생성하고 사용하는 방법.\n- 고급 열거형 사용법: 조건, 반복 및 열거형과의 비교.\n- 실용적인 응용: 데이터 유효성 검사, 상태 관리 및 pandas Series.\n- 최적의 사례: 효과적인 열거형 사용을 위한 팁.\n\n파이썬 코드에 열거형을 도입하면 더 명확하고 신뢰할 수 있는 결과를 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 마지막으로:\n\n제 글을 읽어 주셔서 감사합니다!\n\n안녕하세요! 저는 데이터 엔지니어 Charilaos Alkiviades Savoullis입니다. 엔드 투 엔드 솔루션을 만드는 것을 좋아하는 엔지니어입니다. Python, SQL, AI, 데이터 엔지니어링, 라이프스타일 등에 관한 글을 씁니다!\n\n함께 테크놀로지, 데이터 및 더 나아가는 세계를 탐험해 봅시다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비슷한 글이나 업데이트를 보려면 내 Medium 프로필을 방문해보세요. https://medium.com/@casavoullis\n\n만약 이 글을 즐겼다면, 앞으로의 업데이트를 받기 위해 좋아요와 팔로우를 고려해보세요.\n\n이 글은 Charilaos Savoullis가 처음으로 Medium에 게시했습니다.\n\n# 내 소셜 미디어에서 나와 연락하기를 주저하지 마세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLinkedIn: [Casavoullis](https://www.linkedin.com/in/casavoullis/)  \nGitHub: [GitHub Profile](https://bit.ly/3WrMzgm)\n\n파이썬 콘텐츠 및 팁에 관심이 있으신가요? 저의 Medium 목록을 확인하려면 여기를 클릭해 보세요.\n\nSQL, 데이터베이스 및 데이터 엔지니어링 콘텐츠에 더 관심이 있으신가요? 더 알아보려면 여기를 클릭해 주세요!\n\n아래 댓글란에 의견을 남겨주세요... 또는 아래가 아니라 위에 남기셔도 됩니다. 🙃\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 친절한 한국어로 번역해 드립니다 🚀\n\nIn Plain English 커뮤니티에 참여해 주셔서 감사합니다! 마지막으로 가시기 전에:\n\n- 작가를 갈채하고 팔로우해 주세요 ️👏️️\n- 팔로우하기: X | LinkedIn | YouTube | Discord | 뉴스레터\n- 다른 플랫폼 방문하기: CoFeed | Differ\n- PlainEnglish.io에서 더 많은 콘텐츠 확인하기","ogImage":{"url":"/assets/img/2024-06-22-MasterPythonEnumModuleAComprehensiveGuide_0.png"},"coverImage":"/assets/img/2024-06-22-MasterPythonEnumModuleAComprehensiveGuide_0.png","tag":["Tech"],"readingTime":17},{"title":"Airflow, DuckDB, Streamlit으로 StarCraft 2 데이터 탐험하기","description":"","date":"2024-06-22 17:22","slug":"2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit","content":"\n\n- 📝 소개 및 개요\n- ⏱️ Airflow\n- 🦆 DuckDB\n· DuckDB: 여러분의 휴대용 분석 데이터베이스\n· DuckDB: 여러분의 다재다능한 데이터 조작 도구\n- 🚀 Streamlit\n- 🎮 Airflow, DuckDB 및 Streamlit을 활용한 StarCraft II 데이터 파이프라인\n· 프로젝트 설정\n· Airflow 준비\n· StarCraft II API 액세스 가져오기\n· DAG 구현\n· 데이터 가져오기\n· 데이터 저장\n· Streamlit을 사용하여 데이터 시각화\n- 💡 결론\n\n# 소개 및 개요\n\n이 프로젝트와 기사는 지식 공유의 원천일 뿐만 아니라 게임을 사랑하고 데이터가 보유한 끝없는 가능성을 축하하는 것이기도 합니다. 이것은 두 가지 큰 열정, 즉 게임과 데이터 엔지니어링의 융합입니다. 저는 '스타크래프트: 브루드 워'뿐만 아니라 '스타크래프트 II'도 많이 플레이해오면서, 이 게임은 플레이어가 은하 간 전쟁에서 세 개의 독특한 진영 중 하나를 조종하는 실시간 전략 비디오 게임으로, 자원 관리, 건물 건설 및 전술 전투가 특징입니다. 저는 최고 순위 선수들이 서로 경쟁하는 그랜드마스터 래더에 도달하지는 못했지만, 전장에서 군대를 지휘하는 아드레날린 붐을 경험하고 상대를 앞지르며 승리를 차지하는 것을 즐겼습니다 (적어도 때때로).\n\n마치 스타크래프트에서 빌드 오더를 세밀하게 조정하고 적의 전술에 적응했던 것처럼, 지금은 데이터 엔지니어로서 데이터 파이프라인을 최적화하고 트렌드를 분석하며 통찰을 시각화하고 있습니다. 오늘의 현대 데이터 스택에서 강력한 세 가지 기술에 대해 지식을 공유하고 싶습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- ⏱️ Apache Airflow: 복잡한 작업 흐름을 조정하고 예약하는 플랫폼입니다.\n- 🦆 DuckDB: 가벼우면서 다재다능한 분석용 데이터베이스입니다.\n- 🚀 Streamlit: 상호작용 웹 애플리케이션을 구축하기 위한 사용자 친화적인 프레임워크입니다.\n\n본 글에서는 이 세 가지 기술에 대한 기본을 설명하고, 일상 업무에서 어떻게 활용할 수 있는지 예시를 드리겠습니다.\n\n마지막으로, StarCraft II 데이터 파이프라인 예제 프로젝트를 생성할 때 모든 것이 결합됩니다. 본 프로젝트에서는 StarCraft II API에서 데이터를 가져와 DuckDB에 결과를 저장하고, Airflow를 통해 조정합니다. 또한 데이터를 시각화하기 위해 Streamlit 앱을 만들어 볼 것입니다. StarCraft II의 현재 그랜드마스터 래더가 어떻게 보이는지 살펴봅니다 (스포일러: 저는 찾아보기 힘들 것입니다). 이것이 최종 결과물입니다:\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_0.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGithub에서 완성된 프로젝트를 찾을 수 있어요 🪄: https://github.com/vojay-dev/sc2-data-pipeline\n\n자, 마우스와 키보드를 쥐고, 내부 프로토스, 저그 또는 테란 지휘관을 소환하고 데이터와 게임 세계를 함께 탐험할 준비를 하세요.\n\n![Airflow](/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_1.png)\n\n# Airflow\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아파치 에어플로우는 파이썬을 사용하여 프로그램 방식으로 작성하고 예약하며 워크플로를 모니터링할 수 있는 오픈 소스 플랫폼입니다. 워크플로는 방향성 비순환 그래프(Directed Acyclic Graphs, DAGs)로 표현되며 그래프의 각 정점은 작업의 단위입니다.\n\n일반적으로, 워크플로는 데이터 추출, 변환, 로드(Extract, Transform, Load, ETL) 프로세스로 알려져 있지만, 사실 에어플로우는 매우 유연하여 어떠한 종류의 워크플로도 구현할 수 있습니다.\n\n![Airflow Image](/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_2.png)\n\n에어플로우는 웹 인터페이스를 제공하여 DAGs를 관리하고 모니터링할 수 있습니다. 에어플로우에는 네 가지 주요 구성 요소가 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 🌎 웹서버: Airflow 웹 인터페이스를 제공합니다.\n- ⏱️ 스케줄러: 구성된 시간에 DAG를 실행할 수 있도록 일정을 관리합니다.\n- 🗄️ 데이터베이스: 모든 DAG 및 작업 메타데이터를 저장합니다.\n- 🚀 실행자: 개별 작업을 실행합니다.\n\n데이터베이스와 실행자에 관해서 Airflow는 매우 유연합니다. 예를 들어 SequentialExecutor는 로컬 개발에 사용될 수 있으며 한 번에 하나의 작업을 실행합니다. 한편, CeleryExecutor 또는 KubernetesExecutor는 작업자 노드 클러스터에서 병렬 실행을 가능하게 합니다.\n\n이 프로젝트에서는 Airflow를 사용하여 StarCraft II API에서 데이터를 가져와 DuckDB에 유지하도록 작업 흐름을 조정할 것입니다.\n\n# DuckDB\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_3.png\" /\u003e\n\n덕디비 세계에 오신 것을 환영합니다! 덕디비를 데이터 분석 영역에서의 신뢰할 수 있는 동반자로 상상해보세요. 가벼우면서도 강력한 도구로, 강력한 효과를 발휘합니다. 덕디비를 스타크래프트 군에서의 민첩한 정찰병으로 상상해보세요. 적의 선을 민첩하게 헤쳐가며 정보를 수집합니다.\n\n본 예제 프로젝트에서는 덕디비를 사용하여 스타크래프트 API의 데이터, 특히 그랜드마스터 랭킹 데이터를 임베디드 방식으로 영구 저장하는 방법을 살펴보겠습니다. 이를 위해 파이썬을 통해 덕디비를 사용하여 데이터를 한 파일에 저장합니다.\n\n하지만 프로젝트에 바로 뛰어들기 전에, 아래 장들에서 덕디비에 대해 더 자세히 소개하고 일상적인 데이터 엔지니어링/분석 비즈니스에서 덕디비를 활용하는 데 어떻게 도움을 받을 수 있는지 설명하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# DuckDB: 휴대용 분석용 데이터베이스\n\nDuckDB를 데이터베이스의 스위스 아미 나이프로 생각해보세요. 이것은 빠르고 효율적이며 다재다능합니다. 마치 전투터스 제알럿처럼 어떤 상황에도 쉽게 적응할 수 있습니다.\n\nDuckDB는 쉽게 설치할 수 있으며 휴대용이며 오픈소스입니다. SQL 언어에서 기능이 풍부하며 CSV, Parquest, JSON과 같은 다양한 형식의 데이터를 가져오고 내보낼 수 있습니다. 또한 Pandas 데이터프레임과 원활하게 통합되어 있어 데이터 테이블 작업에 강력한 도구로 사용할 수 있습니다. 다음 장에서 자세히 살펴보겠습니다.\n\nPython 프로젝트에 간단히 설치할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\npip install duckdb\n```\n\n그 후에는 DuckDB를 인메모리 데이터베이스로 즉석에서 사용할 수 있습니다:\n\n```js\nimport duckdb\n\nduckdb.execute(\"CREATE TABLE tbl AS SELECT 42 a\")\ndf = duckdb.execute(\"SELECT * FROM tbl\").df()\nprint(df)\n```\n\n```js\n    a\n0  42\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또는 간단한 파일에 데이터를 유지하는 내장 데이터베이스로 DuckDB를 사용할 수도 있어요:\n\n```js\nimport duckdb\n\n# 영구 데이터베이스를 생성하고 데이터를 씁니다\nwith duckdb.connect(database=\"my_duckdb.db\") as write_conn:\n    write_conn.execute(\"CREATE TABLE tbl AS SELECT 42 a\")\n\n# 다른 곳에서: 데이터를 읽고 처리합니다\nwith duckdb.connect(database=\"my_duckdb.db\", read_only=True) as read_conn:\n    df = read_conn.execute(\"SELECT * FROM tbl\").df()\n    print(df)\n```\n\n이렇게 하면 모든 데이터가 my_duckdb.db라는 파일에 저장됩니다. 여기서는 DuckDB와 Pandas 데이터프레임 간에 데이터를 교환하는 것 뿐만 아니라 CSV, JSON 등을 읽고 쓸 수도 있어요.\n\n만약 SQLite에 익숙하다면 DuckDB를 그보다 더 성능 중심적이고 더 세련된 씨봉인 것으로 생각해보세요. SQLite가 소규모 프로젝트에 적합하다면 DuckDB는 한 단계 발전된 것입니다. 작은 Terran Hellion에서 강력한 Thor 유닛으로 업그레이드하는 것 같아요. 더 큰 데이터셋과 복잡한 쿼리도 무리 없이 처리할 준비가 되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDuckDB는 Command Line Interface (CLI)도 제공합니다. DuckDB CLI는 Windows, Mac 및 Linux용으로 미리 컴파일된 간단한 실행 파일입니다. 내 Mac 환경에서는 간단히 Homebrew를 통해 설치할 수 있어요:\n\n```js\nbrew install duckdb\n```\n\n이를 사용하여 DuckDB 소스에 연결하거나 인메모리 작업을 수행할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일부 능력을 보여주기 위해 DuckDB 데이터베이스 파일에 연결하여 프로젝트의 일부로 나준비 한 일부를 실행하기 위해 사용했습니다:\n\n```js\nSELECT favorite_race, SUM(wins) AS total_wins, MAX(mmr) AS max_mmr, AVG(mmr) AS avg_mmr\nFROM ladder\nWHERE favorite_race IN ('protoss', 'terran', 'zerg')\nGROUP BY favorite_race\nORDER BY total_wins DESC;\n```\n\n```js\n| favorite_race | total_wins | max_mmr |      avg_mmr      |\n|    varchar    |   int128   | double  |      double       |\n|---------------|------------|---------|-------------------|\n| protoss       |      11816 |  6840.0 |            5541.3 |\n| terran        |       7207 |  7140.0 | 5501.839285714285 |\n| zerg          |       5380 |  7080.0 | 5591.622222222222 |\n```\n\n- 🥇 Protoss가 그랜드마스터 래더에서 가장 높은 총 승리 수를 가졌지만 최대 MMR는 가장 낮습니다.\n- 🥈 Terran은 그랜드마스터 래더에서 최고의 최대 MMR을 가지고 있습니다.\n- 🥉 Zerg는 그랜드마스터 래더에서 가장 적은 승리를 가지고 있지만 평균 MMR이 가장 높습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# DuckDB: 다목적 데이터 조작 도구\n\n덕DB에 대해 처음 프로젝트를 진행하면서, 휴대용 가벼운 분석 데이터베이스 이상의 기능을 발견할 수 있었어요. 실제로 덕DB를 데이터 처리 작업에 사용하면 매우 강력해질 수 있답니다. \n\n덕DB의 핵심은 SQL 기반 작업과 Pandas와 같은 다른 데이터 처리 도구 사이에 원활한 통합 기능을 제공하는 것입니다. 이 독특한 기능 덕분에 데이터 처리 스크립트 내에서 다른 기술들 간에 쉽게 전환할 수 있어요.\n\nPandas나 NumPy와 같은 전형적인 라이브러리를 사용하여 Python 스크립트 내에서 데이터 처리를 완전히 구현하는 대신, 복잡한 데이터베이스 통합 설정 없이 이러한 환경들 간에 전환할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAPI로부터 데이터를 가져와 Pandas 데이터프레임에 로드하고 메모리 내 DuckDB에 삽입하여 SQL을 사용하여 집계를 수행한 다음 결과를 또 다른 데이터프레임에 다시 작성하여 계속 진행할 수 있습니다. 특히 SQL과 함께 많이 작업하는 데이터 엔지니어로서 이는 더 직관적인 데이터 흐름을 만들기 위한 제 도구 상자에 강력한 도구를 제공했습니다.\n\n스타크래프트와 마찬가지로, 상황에 맞는 올바른 유닛을 선택해야 합니다.  Zealots의 군대를 만들 수 있지만 상대가 많은 Roaches로 공격할 때에는 군대 구성을 조정하고 Immortals와 Void Rays를 추가해야 합니다. 데이터 처리 스크립트도 마찬가지입니다: DuckDB와 같은 도구를 전체 구성에 추가함으로써 데이터 처리 시 더 많은 도전에 대처할 수 있는 가능성을 가질 수 있습니다.\n\n다음 예시는 Airflow DAG로 채울 StarCraft II 래더 데이터를 읽어와 SQL을 사용하여 집계하고 결과를 Pandas 데이터프레임으로 작성한 다음 Pandas를 사용하여 일부 열을 추가하고 결과를 다시 메모리 내 DuckDB 테이블로 이동하여 최종적으로 Pandas 데이터프레임으로 돌아가게 됩니다.\n\n```js\nimport duckdb\n\nif __name__ == '__main__':\n    # 영구 DuckDB 사용\n    with duckdb.connect(database=\"sc2data.db\") as conn:\n        df = conn.sql(f\"\"\"\n            SELECT\n                favorite_race,\n                SUM(wins) AS total_wins,\n                SUM(losses) AS total_losses,\n                MAX(mmr) AS max_mmr,\n                AVG(mmr) AS avg_mmr\n            FROM ladder\n            WHERE favorite_race IN ('protoss', 'terran', 'zerg')\n            GROUP BY favorite_race\n            ORDER BY total_wins DESC;\n        \"\"\").df()\n        print(df)\n\n    # 팬더스에서 데이터 처리\n    df[\"win_pct\"] = (df[\"total_wins\"] / (df[\"total_wins\"] + df[\"total_losses\"]) * 100)\n\n    # 추가 처리를 위해 메모리 내 DuckDB 사용\n    duckdb.sql(\"\"\"\n        CREATE TABLE aggregation AS\n        SELECT CASE\n            WHEN favorite_race = 'protoss' THEN 'p'\n            WHEN favorite_race = 'terran' THEN 't'\n            WHEN favorite_race = 'zerg' THEN 'z'\n        END AS fav_rc,\n        total_wins + total_losses AS total_games,\n        win_pct\n        FROM df;\n    \"\"\")\n\n    # 팬더스로 변경\n    df_agg = duckdb.sql(\"SELECT * FROM aggregation;\").df()\n    print(df_agg)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 코드와 StarCraft II API 그랜드마스터 래더 데이터를 사용하여 위와 같은 결과가 생성됩니다:\n\n```js\n  favorite_race  total_wins  total_losses  max_mmr      avg_mmr\n0       protoss     11816.0        8927.0   6840.0  5541.300000\n1        terran      7207.0        5655.0   7140.0  5501.839286\n2          zerg      5380.0        3985.0   7080.0  5591.622222\n  fav_rc  total_games    win_pct\n0      p      20743.0  56.963795\n1      t      12862.0  56.033276\n2      z       9365.0  57.447944\n```\n\n우리는 그랜드마스터 래더에서 프로토스가 가장 인기 있는 선택이라는 것을 배울 수 있을 뿐만 아니라, DuckDB가 Pandas와의 호환성으로 데이터 과학자와 분석가들에게 새로운 가능성을 열어 줍니다. DuckDB의 SQL 능력과 Pandas의 데이터 조작 기능을 원활하게 전환하여 사용자들은 두 플랫폼의 강점을 활용하여 업무 효율성과 유연성을 극대화할 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_5.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n숫자를 다루거나 데이터 집합을 변환하거나 복잡한 분석을 수행하더라도, DuckDB는 신뢰할 수 있는 동반자로 빛을 발합니다. SQL의 편리함과 Pandas 및 기타 라이브러리의 다양성을 통해 데이터 조작을 효율적으로 진행할 수 있습니다.\n\n# Streamlit\n\nStreamlit의 오픈 소스 앱 프레임워크를 통해 데이터 시각화 및 상호 작용 가능한 웹 앱을 만들 수 있습니다. 이러한 앱을 로컬에서 실행하거나 무료로 Streamlit Community Cloud에 배포할 수도 있습니다. Streamlit 자체에는 모든 종류의 데이터를 렌더링할 요소가 이미 포함되어 있지만, Streamlit의 가능성을 확장하는 서드파티 모듈(구성 요소)도 많이 있습니다.\n\n간단히 Python 프로젝트에 설치할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nstreamlit 설치하기\n```\n\n그런 다음, 전용 스크립트 파일에서 앱을 만들고 streamlit run 명령어를 통해 실행합니다:\n\n```js\nstreamlit run your_script.py [-- 스크립트 인수]\n```\n\n파이썬 스크립트를 코드로 상상해 보세요. 위에서부터 아래로 앱을 나타내죠. 함께 하면:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport streamlit as st\n\nst.title(\"내 Streamlit 앱\")\n```\n\n간단한 헤더가 있는 웹 앱이 생성됩니다.\n\n![img](/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_6.png)\n\n다음 코드를 사용하면:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nimport streamlit as st\n\nst.dataframe(df)\n```\n\n대화형 테이블로 데이터프레임을 표시할 수 있어요. 데이터를 시각화하는 많은 가능성이 있습니다. StarCraft II 데이터를 렌더링하는 앱을 구현할 때 구체적인 사용 사례를 보게 될 거에요. 기대하시고 즐겁게 읽어 주세요.\n\n# Airflow, DuckDB 및 Streamlit을 사용한 StarCraft II 데이터 파이프라인\n\n프로젝트의 기본 아이디어는 StarCraft II API에서 데이터를 가져오는 것이에요. 보다 정확하게 말하자면 현재 게임에서 가장 우수한 사람을 알아보기위해 그랜드마스터 래더에 대한 정보를 가져올 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희는 그 데이터를 DuckDB 파일에 저장한 다음 Airflow DAG와 TaskFlow API를 사용하여 이 프로세스를 조정할 것입니다. 마지막으로, Streamlit을 사용하여 간단한 앱을 만들 것입니다.\n\n최종 프로젝트는 Github에서도 확인하실 수 있어요 🪄: https://github.com/vojay-dev/sc2-data-pipeline 하지만 이어지는 챕터에서는 이 스타크래프트 II 데이터 파이프라인을 어떻게 단계별로 구현하는지 설명할 거예요.\n\n제가 사용하고 있는 환경은 다음과 같아요:\n\n- OS: macOS Sonoma\n- Python: 3.11.8\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 프로젝트 설정\n\n새로운 파이썬 프로젝트를 만들기 위해 시작합니다. 먼저 새로운 폴더를 만듭니다. 이 폴더 안에서 빌트인 venv 모듈을 사용하여 가상 환경을 만듭니다:\n\n```bash\nmkdir sc2-data-pipeline\ncd sc2-data-pipeline\npython -m venv .venv\nsource .venv/bin/activate\n```\n\n![이미지](/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막 명령어로 가상 환경도 활성화되었습니다. 이는 말하자면: 해당 터미널 세션에서 실행하는 모든 것이 시스템 전역 Python이 아닌 가상 Python을 사용하게 됩니다. 이는 우리가 다음에 설치할 종속성을 프로젝트 내에서 격리시키고 싶어하는 중요한 부분입니다.\n\n이 프로젝트는 Airflow, DuckDB, Streamlit, Pandas 및 PyArrow을 사용하므로, 다음 단계는 모든 요구 사항을 설치하는 것입니다:\n\n```javascript\n# Airflow 설치\nAIRFLOW_VERSION=2.8.2\nPYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\nCONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\npip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n\n# DuckDB 설치\npip install duckdb\n\n# Pandas 및 PyArrow 설치\npip install pandas\npip install pyarrow\n\n# Streamlit 설치\npip install streamlit\n```\n\n이 과정은 조금 시간이 걸릴 수 있으니, 이는 커피를 마실 좋은 순간입니다. ☕️\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_8.png\" /\u003e\n\n위에서 보듯이 Airflow 2.8.2를 사용하고 있습니다. 또한, 왜 Poetry나 최소한 requirements.txt를 사용하는 대신 이러한 종속성을 수동으로 설치하는지 궁금해할 수도 있습니다. Airflow를 로컬에 설치할 때는 이 접근 방식이 가장 안정적으로 작동하며, 공식적인 Poetry 지원이 아직 없습니다. 이러한 이유로 그리고 간단하게 유지하기 위해 수동 방식을 선택했습니다.\n\n# Airflow 준비하기\n\nAirflow는 구성 파일과 같은 데이터를 관리하기 위해 로컬 디스크에 airflow라는 폴더를 사용합니다. 보통 이 폴더는 현재 사용자의 홈 디렉터리에 배치됩니다. 그러나 다른 프로젝트와 충돌을 피하기 위해 AIRFLOW_HOME 환경 변수를 해당하는 프로젝트 폴더로 설정하여 airflow 폴더의 기준으로 사용할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAirflow를 독립 실행 모드로 처음 시작하면, 지정된 위치에 폴더를 생성하고 기본 구성을 사용합니다. SequentialExecutor와 SQLite를 데이터베이스로 사용하며, 데이터베이스 파일은 AIRFLOW_HOME 위치에 저장됩니다.\n\n다음 명령어는 AIRFLOW_HOME 환경 변수를 현재 디렉토리(프로젝트 디렉토리)에 airflow라는 폴더로 설정하고 Airflow를 독립 실행 모드로 시작합니다. 또한 명령에 NO_PROXY라는 다른 환경 변수를 추가합니다. 이는 macOS에서 DAG를 Airflow 웹 인터페이스를 통해 실행할 때 발생하는 SIGSEGV 문제로 인한 것입니다.\n\n```sh\nNO_PROXY=\"*\" AIRFLOW_HOME=\"$(pwd)/airflow\" airflow standalone\n```\n\n이로써 Airflow를 시작할 뿐만 아니라 프로젝트 디렉토리에 airflow 폴더를 자동으로 생성할 것입니다. 또한 웹 인터페이스를 위해 관리자 사용자를 자동으로 생성합니다. 로그 출력에 사용자 이름과 비밀번호를 확인할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_9.png\" /\u003e\n\n```js\nstandalone | Airflow is ready\nstandalone | Login with username: admin password: FZCvvSd8WVYDb2Vm\nstandalone | Airflow Standalone is for development purposes only. Do not use this in production!\n```\n\n브라우저에서 http://localhost:8080/을 열어서 로그 출력에서 제공된 자격 증명을 사용하여 로그인할 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_10.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n축하해요 🎉, 실용적이고 로컬 Airflow 환경을 갖추셨군요. 웹 인터페이스의 경고 메시지가 표시되는 이유는 자동으로 SequentialExecutor와 스탠드얼론 모드의 SQLite 데이터베이스를 사용하고 있기 때문입니다. 당연히 이는 본격적인 운영용이 아닌 것이죠.\n\n스탠드얼론 프로세스를 종료하려면 컨트롤+c를 누르세요.\n\nDAG 작업에 앞서 환경을 좀 더 준비해보겠습니다.\n\n한 가지 주목할 점이 있을 겁니다: 예제 DAG들이 많이 있습니다. 저는 개인적으로 깔끔한 환경으로 시작하는 것을 좋아해요. 이 예제들은 특정 구성 변수가 설정되었을 때 시작됩니다. 그러니 먼저 이 부분을 구성 수정해 볼게요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n프로젝트 폴더 내의 airflow 폴더에 AIRFLOW_HOME 변수를 설정했기 때문에 구성 파일의 위치는 airflow/airflow.cfg입니다.\n\n![이미지](/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_11.png)\n\n즐겨 사용하는 편집기에서 구성을 열어 다음 구성을 변경하세요:\n\n```js\nload_examples = False\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n혼자서 실행되는 프로세스를 다시 시작하더라도 예시 DAG들은 데이터베이스에 남아있기 때문에 계속 나타날 수 있습니다. 따라서 다음 명령어로 데이터베이스를 재설정해야 합니다. (가상 환경을 활성화하고 프로젝트 폴더 내에 있어야 함을 확인하세요).\n\n```bash\nNO_PROXY=\"*\" AIRFLOW_HOME=\"$(pwd)/airflow\" airflow db reset\n```\n\n정상적으로 재설정했다면, 환경을 다시 시작하고 이제는 깨끗한 상태입니다. 이제 새로운 관리자 사용자를 생성하지만 이번에는 예시 DAG는 생성되지 않습니다.\n\n```bash\nNO_PROXY=\"*\" AIRFLOW_HOME=\"$(pwd)/airflow\" airflow standalone\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_12.png)\n\nDAG를 생성하기 전에 조정해야 할 사항이 하나 더 있습니다. 일반적으로 프로젝트를 Git 저장소에 커밋할 때 airflow 폴더를 추가하고 싶지 않습니다. 그 이유는 프로덕션 환경에서 프로젝트 폴더에 위치하지 않고 로컬 환경이기 때문에 다른 개발자들이 각자의 환경을 설정할 수 있도록 하기 위함입니다.\n\n그래서 .gitignore 파일에 airflow/를 추가할 것입니다. 그러나 이 접근 방식에 문제가 있습니다. 기본적으로 Airflow는 airflow 폴더 내의 dags라는 폴더에서 DAG를 찾기 때문에 airflow/dags로 지정합니다. 이 폴더에 DAG 구현을 추가하고 .gitignore 파일에서 airflow/ 폴더를 무시한다면, 우회 방식 없이 코드를 저장소에 커밋할 수 없게 됩니다.\n\n다행히도, 해결책은 Airflow 구성에서 DAGs 폴더를 변경하는 것뿐입니다. 이를 해결하기 위해 이 변수를 프로젝트 폴더 내에 있는 dags라는 폴더로 설정하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해당 목적을 달성하려면 Airflow/airflow.cfg 파일을 다시 열어서 dags_folder 변수를 찾으세요. 이 변수를 프로젝트 폴더 내의 dags라는 폴더를 가리키도록 설정하십시오. 예를 들어:\n\n```js\ndags_folder = /tmp/sc2-data-pipeline/dags\n```\n\n마지막으로, 프로젝트 내에 빈 dags 폴더를 생성하고 준비 완료입니다.\n\n```js\nmkdir dags\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_13.png)\n\n# 스타크래프트 II API 액세스 받기\n\n우리가 사용하고 있는 API는 스타크래프트 II 커뮤니티 API의 일부입니다.\n\n![image](/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_14.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제한은 매우 관대합니다: 1시간당 36000개의 요청 및 1초당 100개의 요청이 가능하므로 이 시나리오에서 DAG를 자유롭게 실행할 수 있습니다.\n\n액세스를 받으려면 무료로 battle.net 계정을 사용하여 OAuth 클라이언트를 생성해야 합니다. 단순히 https://develop.battle.net/access/clients 로 이동하여 battle.net 계정으로 로그인하고 클라이언트를 생성하면 됩니다.\n\n![이미지](/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_15.png)\n\n이렇게 하면 다음을 얻을 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 클라이언트 ID 및\n- 클라이언트 비밀번호\n\n둘 다 API에 액세스하기 위해 필요하니 안전하게 보관하세요.\n\n기본 흐름은 먼저 클라이언트 ID와 비밀번호를 사용하여 액세스 토큰을 가져온 후 이 토큰을 사용하여 커뮤니티 API에서 데이터를 가져오는 것입니다. 다음 예시는 터미널에서 curl 및 jq를 사용하여 이 작업을 수행하는 방법을 보여줍니다. jq는 brew install jq를 사용하여 설치할 수 있습니다.\n\n액세스 토큰 가져오기 예시\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\ncurl -s -u your_client_id:your_client_secret -d grant_type=client_credentials https://oauth.battle.net/token | jq .\n```\n\n```js\n{\n  \"access_token\": \"super_secret_token\",\n  \"token_type\": \"bearer\",\n  \"expires_in\": 86399,\n  \"sub\": \"xxx\"\n}\n```\n\n데이터 가져오는 예시\n\n```js\ncurl -s --header \"Authorization: Bearer super_secret_token\" \"https://eu.api.blizzard.com/sc2/ladder/season/2\" | jq .\n```\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```json\n{\n  \"seasonId\": 58,\n  \"number\": 1,\n  \"year\": 2024,\n  \"startDate\": \"1704412800\",\n  \"endDate\": \"1711929600\"\n}\n```\n\n클라이언트 ID 및 시크릿이 준비되었으니 이제 DAG를 구현하여 그랜드마스터 래더 데이터를 가져와 유지할 수 있습니다.\n\n# DAG 구현\n\ndags 폴더에 Python 파일 sc2.py를 만들어 DAG의 구현을 실행할 것입니다. 아래 코드를 추가하십시오. 이 코드는 TaskFlow API를 사용한 DAG 구현입니다. 이후에 일부 세부 정보를 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nimport logging\n\nimport pendulum\nimport requests\nfrom airflow.decorators import dag, task, task_group\nfrom airflow.models import Variable\nfrom requests.adapters import HTTPAdapter\nfrom urllib3 import Retry\nimport duckdb\nimport pandas as pd\n\nlogger = logging.getLogger(__name__)\n\nDUCK_DB = \"sc2data.db\"\n\nCLIENT_ID = \"your_client_id\"\nCLIENT_SECRET = \"your_client_secret\"\n\nBASE_URI = \"https://eu.api.blizzard.com\"\nREGION_ID = 2  # Europe\n\n# retry strategy for contacting the StarCraft 2 API\nMAX_RETRIES = 4\nBACKOFF_FACTOR = 2\n\n\n@dag(start_date=pendulum.now())\ndef sc2():\n    retry_strategy = Retry(total=MAX_RETRIES, backoff_factor=BACKOFF_FACTOR)\n    adapter = HTTPAdapter(max_retries=retry_strategy)\n    session = requests.Session()\n    session.mount('https://', adapter)\n\n    @task\n    def get_access_token() -\u003e str:\n        data = {\"grant_type\": \"client_credentials\"}\n        response = session.post(\"https://oauth.battle.net/token\", data=data, auth=(CLIENT_ID, CLIENT_SECRET))\n        return response.json()[\"access_token\"]\n\n    @task\n    def get_grandmaster_ladder_data(token: str):\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n\n        response = session.get(f\"{BASE_URI}/sc2/ladder/grandmaster/{REGION_ID}\", headers=headers)\n        ladder_teams = response.json().get(\"ladderTeams\", [])\n        return [{\n            \"id\": lt[\"teamMembers\"][0][\"id\"],\n            \"realm\": lt[\"teamMembers\"][0][\"realm\"],\n            \"region\": lt[\"teamMembers\"][0][\"region\"],\n            \"display_name\": lt[\"teamMembers\"][0][\"displayName\"],\n            \"clan_tag\": lt[\"teamMembers\"][0][\"clanTag\"] if \"clanTag\" in lt[\"teamMembers\"][0] else None,\n            \"favorite_race\": lt[\"teamMembers\"][0][\"favoriteRace\"] if \"favoriteRace\" in lt[\"teamMembers\"][0] else None,\n            \"previous_rank\": lt[\"previousRank\"],\n            \"points\": lt[\"points\"],\n            \"wins\": lt[\"wins\"],\n            \"losses\": lt[\"losses\"],\n            \"mmr\": lt[\"mmr\"] if \"mmr\" in lt else None,\n            \"join_timestamp\": lt[\"joinTimestamp\"]\n        } for lt in ladder_teams if lt[\"teamMembers\"] and len(lt[\"teamMembers\"]) == 1]\n\n    def get_profile_metadata(token: str, region: str, realm: int, player_id: int) -\u003e dict:\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n\n        response = session.get(f\"{BASE_URI}/sc2/metadata/profile/{region}/{realm}/{player_id}\", headers=headers)\n        return response.json() if response.status_code == 200 else None\n\n    @task\n    def enrich_data(token: str, data: list) -\u003e list:\n        logger.info(\"Fetching metadata for %d players\", len(data))\n\n        for i, player in enumerate(data, start=1):\n            logger.info(\"Fetching metadata for player %d/%d\", i, len(data))\n            metadata = get_profile_metadata(token, player[\"region\"], player[\"realm\"], player[\"id\"])\n\n            player[\"profile_url\"] = metadata.get(\"profileUrl\") if metadata else None\n            player[\"avatar_url\"] = metadata.get(\"avatarUrl\") if metadata else None\n            player[\"name\"] = metadata.get(\"name\") if metadata else None\n\n        return data\n\n    @task\n    def create_pandas_df(data: list) -\u003e pd.DataFrame:\n        return pd.DataFrame(data)\n\n    @task\n    def store_data_in_duckdb(ladder_df: pd.DataFrame) -\u003e None:\n        with duckdb.connect(DUCK_DB) as conn:\n            conn.sql(f\"\"\"\n                DROP TABLE IF EXISTS ladder;\n                CREATE TABLE ladder AS\n                SELECT * FROM ladder_df;\n            \"\"\")\n\n    @task_group\n    def get_data() -\u003e list:\n        access_token = get_access_token()\n        ladder_data = get_grandmaster_ladder_data(access_token)\n        return enrich_data(access_token, ladder_data)\n\n    @task_group\n    def store_data(enriched_data: list) -\u003e None:\n        df = create_pandas_df(enriched_data)\n        store_data_in_duckdb(df)\n\n    store_data(get_data())\n\n\nsc2()\n```\n\nDAG의 기본 흐름은 꽤 간단합니다. 앞서 설명한 두 주요 태스크 그룹인 get_data와 store_data가 연결됩니다.\n\n![image](/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_16.png)\n\n이제 이러한 태스크 그룹의 주요 요소들을 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터 가져오기\n\n데이터 가져오기는 Airflow에서 각각의 태스크로 실행되는 다음 3단계로 이루어집니다:\n\n- 🔐 get_access_token: 클라이언트 ID와 비밀번호를 사용하여 최신 엑세스 토큰을 가져옵니다.\n- 📝 get_grandmaster_ladder_data: 토큰을 사용하여 모든 플레이어의 최신 그랜드마스터 래더 데이터를 가져옵니다.\n- 👥 enrich_data: 다른 API 엔드포인트를 사용하여 래더의 각 항목을 플레이어 프로필 URL, 아바타 및 이름과 함께 풍부하게 만듭니다.\n\nrequest.get 또는 request.post 함수를 직접 사용하는 대신, 모든 요청에 사용하는 세션을 생성합니다. 이를 통해 재시도 및 백오프 전략을 정의할 수도 있습니다. 외부 API 소스에서 데이터를 가져오는 경우 DAG가 일시적으로 사용할 수 없는 API 때문에 실패하지 않도록 하려면 이 방법이 권장됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nMAX_RETRIES = 4\nBACKOFF_FACTOR = 2\n\nretry_strategy = Retry(total=MAX_RETRIES, backoff_factor=BACKOFF_FACTOR)\nadapter = HTTPAdapter(max_retries=retry_strategy)\nsession = requests.Session()\nsession.mount('https://', adapter) \n```\n\n여기에, 우리는 세션을 사용하여 작업에서 요청을 보낼 수 있습니다. 예를 들어, 액세스 토큰을 얻기 위해:\n\n```js\n    @task\n    def get_access_token() -\u003e str:\n        data = {\"grant_type\": \"client_credentials\"}\n        response = session.post(\"https://oauth.battle.net/token\", data=data, auth=(CLIENT_ID, CLIENT_SECRET))\n        return response.json()[\"access_token\"]\n```\n\nget_grandmaster_ladder_data에서는 https://eu.api.blizzard.com/sc2/ladder/grandmaster/'REGION_ID' 엔드포인트에서 최신 그랜드마스터 래더를 가져옵니다. 여기서 REGION_ID는 유럽 데이터를 얻기 위해 우리의 경우에 2로 설정됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, enrich_data 작업에서는 래더의 각 플레이어에 대해 https://eu.api.blizzard.com/sc2/metadata/profile/'region'/'realm'/'player_id' 엔드포인트를 호출하여 기존 플레이어 항목을 보강합니다. 엔드포인트 호출 자체는 get_profile_metadata 도우미 함수에 캡슐화되어 있습니다.\n\n# 데이터 저장\n\n데이터 저장은 다음 2 단계에서 발생하며 각각 Airflow에서 작업으로 실행됩니다:\n\n- 🐼 create_pandas_df: 플레이어 목록을 기반으로 판다스 데이터프레임 생성.\n- 🦆 store_data_in_duckdb: 데이터프레임을 파일에 저장된 DuckDB에 저장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전에도 언급했듯이, DuckDB는 Pandas 데이터프레임을 포함한 다양한 형식을 읽고 쓸 수 있습니다. 따라서 첫 번째 단계는 사다리의 각 플레이어가 되는 딕셔너리 목록에서 데이터프레임을 생성하는 것입니다.\n\n```js\n    @task\n    def create_pandas_df(data: list) -\u003e pd.DataFrame:\n        return pd.DataFrame(data)\n```\n\nDuckDB에 이 데이터프레임을 저장하는 것은 놀랍도록 쉽습니다. 처음 보는 코드를 읽을 때 놀라실 수도 있지만, 네: SQL에서 데이터프레임 변수를 참조할 수 있습니다.\n\n```js\n    @task\n    def store_data_in_duckdb(ladder_df: pd.DataFrame) -\u003e None:\n        with duckdb.connect(DUCK_DB) as conn:\n            conn.sql(f\"\"\"\n                DROP TABLE IF EXISTS ladder;\n                CREATE TABLE ladder AS\n                SELECT * FROM ladder_df;\n            \"\"\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파일에 데이터를 유지하는데, 각 실행에서 기존 데이터를 삭제하여 가장 최신 정보만 저장합니다. 우리는 INSERT OR REPLACE를 사용할 수 있지만, 그런 경우에는 기본 키 제약 조건을 정의해야 합니다. 이는 데이터프레임을 기반으로 직접 테이블을 생성할 때 불가능합니다. 하지만 우리의 사용 사례에는 이 방법이 충분합니다. 이런 경우에 저는 사람들에게 KISS 원칙을 상기시키는 것을 좋아합니다:\n\n데이터를 저장한 후에는 DAG가 완료되어 시각화할 수 있습니다.\n\n# Streamlit을 사용한 데이터 시각화\n\nStreamlit 앱을 위해 프로젝트의 루트 디렉토리에 새 파일을 만듭니다: app.py. 다음 내용을 간단히 추가할 수 있습니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nimport streamlit as st\n\nst.title(\"StarCraft 2 Grandmaster Ladder\")\n\n\n앱을 실행하려면 다음을 사용하세요:\n\n\nstreamlit run app.py\n\n\n헤더가 있는 간단한 웹 페이지가 표시됩니다. 앱을 확장할 때마다 자동으로 새로 고침됩니다. 이제 DuckDB에서 데이터를 읽고 렌더링하는 실제 앱으로 내용을 바꿔봅시다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nimport streamlit as st\nimport duckdb\n\ncon = duckdb.connect(database=\"sc2data.db\", read_only=True)\n\nst.title(\"StarCraft 2 Grandmaster Ladder\")\n\n@st.cache_data\ndef load_ladder_data():\n    df = con.execute(\"SELECT * FROM LADDER\").df()\n\n    # mmr로 정렬하고 아바타를 첫 번째 열로 이동\n    df.sort_values(\"mmr\")\n    avatar_url = df.pop(\"avatar_url\")\n    df.insert(0, \"avatar\", avatar_url)\n\n    return df\n\n@st.cache_data\ndef load_favorite_race_distribution_data():\n    df = con.execute(\"\"\"\n        SELECT favorite_race, COUNT(*) AS count\n        FROM LADDER\n        WHERE favorite_race IS NOT NULL\n        GROUP BY 1\n        ORDER BY 2 DESC\n    \"\"\").df()\n    return df\n\nladder = load_ladder_data()\n\nst.dataframe(ladder, column_config={\n    \"avatar\": st.column_config.ImageColumn(\"avatar\")\n})\n\ndistribution_data = load_favorite_race_distribution_data()\nst.bar_chart(distribution_data, x=\"favorite_race\", y=\"count\")\n\n\n마지막으로, MMR순으로 정렬된 StarCraft II 그랜드마스터 래더 데이터를 시각화하고, 플레이어들의 아바타도 보여줍니다:\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_17.png\" /\u003e\n\nPandas 데이터프레임과 DuckDB를 강력한 데이터 가공 툴킷으로 결합하는 좋은 예시를 보여주는 앱 구현입니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndf = con.execute(\"SELECT * FROM LADDER\").df()\n\n# mmr을 기준으로 정렬하고 아바타를 첫 번째 열로 옮깁니다\ndf.sort_values(\"mmr\")\navatar_url = df.pop(\"avatar_url\")\ndf.insert(0, \"avatar\", avatar_url)\n\nreturn df\n```\n\nStreamlit을 사용하면 데이터프레임을 쉽게 렌더링할 수 있을 뿐만 아니라 특정 열을 교체하여 앱에서 렌더링하는 방식을 수정할 수도 있습니다. 이 예시에서는 아바타 열에서 URL을 가져와 이미지로 렌더링합니다:\n\n```js\nst.dataframe(ladder, column_config={\n    \"avatar\": st.column_config.ImageColumn(\"avatar\")\n})\n```\n\n마지막으로, 그랜드마스터 래더에서 프로토스가 가장 주요한 진영으로 보이는 것을 확인할 수 있습니다. 제가 전 프로토스 플레이어였기 때문에 이 소식을 듣는 것은 좋습니다 😉. \n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n마지막으로, Apache Airflow, Streamlit, 그리고 DuckDB를 활용하여 데이터 파이프라인을 구축하는 여정은 데이터 파이프라인을 조율하고 대화형 데이터 애플리케이션을 개발하는 데 소중한 기술적 통찰력을 제공했습니다.\n\nDuckDB는 데이터 wrangling 도전에 강력한 동반자로 나타났으며, Pandas 데이터프레임과 고급 분석 SQL 기능과의 원활한 통합을 제공했습니다. 가벼운 성격과 효율적인 성능을 통해 리소스 제한적 환경에서 분석 워크로드에 적합함을 입증했습니다.\n\n직관적인 인터페이스와 강력한 시각화 기능을 갖춘 Streamlit은 대화형 데이터 애플리케이션의 신속한 개발 잠재력을 보여주었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 기술을 탐색한 것을 되돌아보면, 현대 데이터 엔지니어링 및 분석 워크플로우에서 그 역할의 중요성을 인지합니다. 별Craft II 그랜드마스터 플레이어처럼, 당신의 유닛 구성을 신중하게 계획하고, 항상 데이터 엔지니어링 도구 상자를 확장하고 최적화하세요. 데이터 영역에서의 다음 임무까지, 여러분의 파이프라인이 원활히 흐르고 승리가 시간이 잘 맞은 레베이저 사격만큼 달콤하길 바랍니다.\n\n\u003cimg src=\"/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_18.png\" /\u003e","ogImage":{"url":"/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_0.png"},"coverImage":"/assets/img/2024-06-22-ExploringStarCraft2datawithAirflowDuckDBandStreamlit_0.png","tag":["Tech"],"readingTime":27},{"title":"Spark 심화 학습 Delta 테이블에서 Z-Ordering과 Data Skipping 사용법","description":"","date":"2024-06-22 17:20","slug":"2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables","content":"\n\n\u003cimg src=\"/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_0.png\" /\u003e\n\n만약 스파크가 말할 수 있다면, 데이터 엔지니어들에 대해 말할 것이고 그들을 우울하게 만들고 예측할 수 있는 미래에 불안감을 주게 될 거에요! 🫢😬\n\n말이지, 이런 데이터 엔지니어들 (내 자신을 포함해서 👀)은 스파크가 문제를 해결할 수 있다고 가정하는데 그들은 해결하지 못해서 불안해해요 (안 코딩해도 돼서 말이지… 내가 말하는 건 나의 경우에요🙈).\n\n하지만 스파크는 실제로 문제를 해결해 주거든요! 그 중 하나는 Z-Ordering을 적용하여 쿼리 실행 속도를 10-12배 빠르게 하는 문제를 해결하는 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Z-Ordering\n\n하루에 \"n\" 번 쿼리되는 델타 테이블이 있다고 해보죠. 이 쿼리들은 실행될 때마다 특정 열을 기준으로 필터링됩니다. 🤔\n\nSpark은 이미 적절한 파티션 수를 얻기 위해 적응형 쿼리 최적화 (AQE)를 사용하고 있으며, 이를 통해 쿼리의 성능을 높이기 위해 프레디케이트 푸시다운을 수행합니다.\n\n하지만 스파크는 여전히 최고 조절에 이르지 않았습니다! 우리는 스파크가 자신의 능력을 발휘할 수 있도록 도와주어야 합니다😤! 특정 열에 필터를 적용할 것이라는 점은 이미 알고 있기 때문에, 해당 열에 Z-Ordering을 활성화할 수 있습니다 🤨\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n컬럼에서 Z-Ordering을 활성화하면 다음과 같은 효과가 있습니다:\n\n- 지정된 열을 기준으로 테이블을 재분할합니다.\n- 지정된 열을 기준으로 파티션 내에서 정렬됩니다.\n\n더 잘 이해하기 위해 예시를 살펴보겠습니다: 🤓\n\n## 시나리오\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 가지 흥미로운 \"이름\" 열을 제외한 몇 가지 흥미없는 열을 포함한 델타 테이블을 생각해보세요. 이미 Spark의 AQE 이후 최적화된 파티션이 4개 있습니다.\n\n![이미지](/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_1.png)\n\n- 해당 쿼리를 테이블에서 실행하자마자, Spark는 델타 트랜잭션 로그의 최소-최대 값들을 확인하고 \"필요한 데이터가 이 파일에 포함되어 있는가\"라고 물을 것입니다. 🧐\n- 따라서 Spark는 처음 3개 파티션을 확인하지만, \"Dan\"보다 더 작은 이름은 없기 때문에 마지막 파티션을 확인하지 않을 것입니다. 이것이 바로 데이터 스킵(Data Skipping)이라고 합니다. 😲\n\n![이미지](/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 그러나 이러한 종류의 데이터 스킵은 'Name' 열에 대해 최적화되지 않았기 때문에 불규칙한 데이터 스킵이라고 할 수 있습니다.\n- 'Name' 열에 Z-Order를 적용해보고 무슨 일이 일어나는지 확인해 봅시다! ✨\n\n![image](/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_3.png)\n\n![image](/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_4.png)\n\n- 이제 위의 쿼리를 실행하면 Spark는 첫 번째 파티션만 확인하므로 데이터 스킵을 완벽하게 수행합니다. 😉\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Z-Ordering 구문:\n\n데이터의 Z-Ordering을 하려면 ZORDER BY 절에 정렬할 열을 지정합니다.\n\n```js\nOPTIMIZE table_name ZORDER BY column_name\n```\n\n## Z-Ordering의 이점:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Z-ordering은 상당한 속도 향상을 가져올 수 있으며, 쿼리 실행 시간을 분 단위에서 초 단위로 줄일 수 있어요🤯\n- 작업 태스크를 실행하는 스파크 워커 노드의 코어에서 균형을 유지하는 적절한 파티션 수⚖️\n\n## Z-Ordering과 관련된 도전:\n\n- 파티션 열을 선택하는 과정은 복잡합니다 🥴\n- Z-Ordering 작업은 비용이 많이 발생하며 더 오래 걸린다는 것을 염두에 두세요 ⏱️\n- Z-Ordering이 활성화된 테이블에서 동시적인 쓰기는 불가능합니다\n\n혜택과 도전을 따져보시면, 사용할지 말지 고민이 될 거예요. 😵‍💫\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이타브릭스도 이러한 딜레마를 느꼈고, 그들은 최첨단 솔루션을 개발했어요: 리퀴드 클러스터링 🥂\n\n다음 블로그에서 리퀴드 클러스터링에 대해 다뤄볼 예정이에요, 기대해주세요!\n\n만약 블로그가 마음에 들었다면 👏 손뼉을 치셔서 이 내용이 모든 데이터 엔지니어에게 전달되도록 해주세요.\n\n읽어 주셔서 감사합니다! 😁","ogImage":{"url":"/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_0.png"},"coverImage":"/assets/img/2024-06-22-Spark-BeyondBasicsZ-OrderingandDataSkippinginDeltaTables_0.png","tag":["Tech"],"readingTime":3},{"title":"BigQuery에서 함수형 데이터 엔지니어링 가이드","description":"","date":"2024-06-22 17:17","slug":"2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery","content":"\n\n\n\u003cimg src=\"/assets/img/2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery_0.png\" /\u003e\n\n# 소개\n\n## 함수형 데이터 엔지니어링이란 무엇인가요?\n\n함수형 데이터 엔지니어링은 수학과 소프트웨어 엔지니어링의 함수 개념을 반영한 데이터 작업 방식입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n막심 보셰만(Maxime Beauchemin)은 Apache Airflow 및 Apache Superset의 창립자로, 2018년 발표한 'Functional Data Engineering — a modern paradigm for batch data processing'에서 데이터에 대한 기능적 엔지니어링 원칙의 목표와 실제 적용을 설명했습니다.\n\n명확한 목표와 제약 조건을 정의함으로써, 이 접근 방식은 데이터에서 명확함, 재현성, 안정성, 신뢰성 및 추적 가능성을 달성하려는 목표를 갖습니다. 실제적으로, 이는 도구, 개발, 테스트 및 자동화를 간소화하고, 어떠한 조직, 팀 또는 프로젝트에 대한 데이터 관련 작업의 확장 가능하고 효율적인 기초를 구축하는 데 도움이 될 수 있습니다.\n\n# 동기\n\n데이터 공간에서 우리는 지속적으로 엔트로피와 싸움을 벌이고 있습니다. 데이터 소스 및 도구의 증식, 데이터 번성, 문서화 흩어짐, 일관되지 않고 잘못 계획된 명명 규칙, 스파게티처럼 얽힌 의존성 맵 등. 시스템은 뒤죽박죽으로 변화할 수 있습니다.하지만 이 저항할 수 없는 진전의 반대에 에너지와 의지를 투입하지 않는 한 요구되는 엔트로피 증가에는 버팁을 세우게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단함은 엔트로피를 막기 위해 우리가 사용하는 무기입니다.\n\n간단함은 기능적 데이터 엔지니어링의 존재 이유인데요: 아키텍처의 간단함; 프로세스의 간단함; 실행의 간단함; 쿼리 및 비용의 간단함이 중요합니다. 이러한 간단함은 증가하는 데이터 원본, 증가하는 데이터 양, 성장 중인 분산 팀 및 신속하고 신뢰할 수 있는 데이터로 의사결정을 내리고 모델을 학습하며 비즈니스 목표를 지원하는 수요를 다룰 때 필요합니다.\n\n비용과 복잡성의 증가, 신뢰성과 정확성의 감소를 피하기 위해서는 데이터 처리, 통합 및 관리에 일관된, 규율적인 방식이 필요하며, 이러한 목표를 달성하는데 도움이 되는 것이 바로 기능적 데이터 엔지니어링 접근 방식입니다.\n\n이 기사에서는 Google Cloud 데이터 스택에서 BigQuery를 중심으로 이를 어떻게 구현할 수 있는지 소개하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 상황\n\n함수형 데이터 엔지니어링의 원리를 이해하기 위해서는 해당 원리들이 유도된 수학 및 소프트웨어 엔지니어링 원리로 시작하는 것이 유익합니다.\n\n## 함수란 무엇인가요?\n\n수학적 함수의 정의부터 시작해보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떤 함수에 대해서 입력과 출력 사이에 1:1 매핑이 있습니다. 이건 직관적으로 이해되지만, 애플리케이션에서 '요소'가 실제로 무엇인지 정의하는 것이 중요할 것입니다.\n\n## 함수형 프로그래밍이란?\n\n함수형 프로그래밍의 정의로 넘어가겠습니다:\n\n함수는 다른 함수들로 구성될 수 있으며, 값들을 다른 값들로 매핑합니다. 좀 더 자세히 읽어보면:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n동일한 입력이 제공되면 항상 동일한 출력을 반환하는 함수를 의미합니다 (즉, 결정적인 함수입니다).\n\n## 데이터 파티셔닝\n\n함수형 데이터 엔지니어링의 기본 원칙을 살펴보기 전에 파티션된 데이터의 개념을 소개하는 것이 중요합니다.\n\n실제로, 데이터 파티셔닝은 데이터를 원래 수신된 시간을 기준으로 별도의 파티션으로 물리적으로 분리합니다. 이렇게 함으로써 하향 작업이 특정 쿼리나 작업에 필요한 파티션만을 대상으로 쿼리할 수 있도록 만들어줍니다. 이는 파티션 프루닝이라 불리는 작업입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파티셔닝 시간 단위는 데이터의 양과 속도에 따라 다를 수 있지만, 많은 경우 도착 날짜별로 데이터를 파티션으로 나누는 것이 합리적입니다. 이 기사에서는 날짜를 기반으로 한 데이터 배치를 다루고 있다고 가정하겠습니다.\n\n수학적 정의에 관련해서, '요소'라는 개념을 보면 입력 데이터 소스에서 날짜로 구분된 단일 파티션을 고려할 수 있으며, 이는 한 입력 파티션이 항상 정확히 하나의 출력 파티션에 매핑된다는 것을 의미합니다.\n\n# 해결책\n\n기능적 데이터 엔지니어링의 본질은 어떤 데이터 엔지니어링 작업도 입력 데이터, 변환 로직 및 출력 데이터로 축소하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![AGuidetoFunctionalDataEngineeringinBigQuery_1](/assets/img/2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery_1.png)\n\n입력 데이터, 변환 로직 및 출력 데이터를 분리하는 것이 매우 중요합니다. 로직은 시간이 지남에 따라 변경되므로 필요에 따라 새로운 로직을 기존 데이터에 적용하여 출력 데이터 테이블 파티션을 재구성할 수 있어야 합니다.\n\n## 1. 영구적이고 변경 불가능한 입력 데이터 파티션\n\n첫 번째 기본 원칙은 데이터 소스 파티션이 시스템에 들어오면 영구적이며 결코 삭제될 수 없다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 날짜 파티션은 변경할 수 없는 객체로 간주되며, 행을 UPDATE, DELETE 또는 INSERT할 수 없습니다.\n\n이는 변경되는 로직이 계속하여 변경되지 않은 입력 데이터에서 작동하기 때문에 견고한 기반을 제공해줍니다. 이로써 논리 변경의 효과를 격리시킬 수 있습니다.\n\n## 2. 결정론적이고 아이덴포턴트한 버전 관리된 논리\n\n결정론적 논리는 동일한 입력을 주면 항상 똑같은 응답을 반환하는 함수를 의미합니다. 이는 LLMs의 특정 상황에서 흥미로울 것입니다. 왜냐하면 LLM은 본성상 결정론적이 아니기 때문에 동일한 프롬프트라도 다른 응답을 반환합니다. 그러나 응답 구조 자체가 결정론적이라면 LLM 응답을 여전히 기능적 데이터 엔지니어링 워크플로에 활용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아이덴포턴시는 두 번째 중요한 제약 사항으로, 반복된 논리적 실행이 초기 실행에서 발생한 최종 상태를 변경하지 않는다는 것을 본질적으로 의미합니다. 간단한 INSERT 문을 고려해 보겠습니다: 이를 여러 번 실행하면 의도적으로든 우연히든 중복된 행이 생성되는 원치 않는 부작용이 발생합니다. 이는 INSERT가 아이덴포턴트 작업이 아님을 의미합니다. 아이덴포턴시는 특정 날짜 파티션을 작성하거나 덮어쓰기만 할 수 있도록 작업의 허용된 출력을 제한함으로써 달성됩니다.\n\n부작용(즉, 외부 상태를 변경하지 않는)이 없는 결정론적이고 아이덴포턴트한 작업은 일반적으로 순수 작업이라고 합니다.\n\n재현성을 달성하기 위해 버전 제어는 중요한 능력입니다: 논리적 변경으로 인해 잘못된 최종 상태가 발생하면 이전 버전의 로직으로 되돌아가거나 출력 테이블을 다시 작성하는 것이 간단하게 되어야 합니다. 결과 데이터를 이전 버전으로 롤백하여 출력 데이터를 이전 버전으로 복원하는 효과적인 방법과 거의 동일해야 합니다.\n\n결과 데이터를 직접 버전 관리하는 것과는 다르지만, 영구적이고 변경할 수 없는 입력 데이터에 버전 관리된 로직을 적용하는 것은 출력 데이터 파티션을 버전 관리하는 것과 사실상 동일해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 3. 변경할 수 없는, 교체 가능한, 분할된 출력 데이터\n\n출력된 데이터 파티션은 변경할 수 없는 상태로 유지되며, 한 번 생성되면 데이터 파티션을 변경할 수 없습니다. 이는 UPDATE, DELETE, INSERT 또는 MERGE 작업이 허용되지 않음을 의미합니다. 즉, 행은 데이터 파티션 내에서 변이될 수 없습니다. 이는 배포된 변환 로직을 통해 출력 데이터 추적 가능성을 보존하여 파티션 및 행 수준의 추적이 가능하게 합니다.\n\n입력 및 출력 데이터 제약 사항 간의 주요 차이점은 출력 데이터 파티션은 영구적이지 않지만 교체 가능하다는 점입니다. 이들은 전체적으로 생성되거나 덮어쓸 수 있지만, 변환 실행 로직의 일부로 업데이트하거나 삭제할 수 없습니다.\n\n날짜 파티션 필드의 선택은 중요하며, 트레이드오프가 필요할 수 있습니다. 출력 데이터를 입력 데이터와 동일한 필드로 분할함으로써, 메타데이터 쿼리만을 기반으로 모니터링 및 자동화 작업을 진행할 수 있습니다. 이는 효율적이고 예측 가능하며 기본 데이터의 잠재적으로 비싼 스캔이 필요하지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 구현 로직\n\n이 기술의 실제 응용을 이해하기 위해서는 변환을 실행하기 위해 필요한 논리적 상호 작용을 명확히해야 합니다. 변환 논리가 출력 데이터 파티션을 생성하거나 대체하기 위해 실행되어야 할 작업을 결정하는 실행 논리를 제공하며, 실행 트리거는 실행 논리가 실행되는 시점을 결정합니다.\n\n![image](/assets/img/2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery_2.png)\n\n실행 트리거는 요청에 따라, 일정에 따라 또는 이벤트 기반(예: 새로운 들어오는 데이터 감지)일 수 있으며, 실행 논리는 그 후에 변환 논리가 어떻게 배포되며 필요 시 어떤 출력 데이터 파티션이 덮어쓰여야 하는지를 결정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n점진적 접근 방식은 메타데이터를 확인하여 입력 데이터 파티션 날짜가 아직 출력 데이터에 없는 경우를 식별하고, 그런 다음 변환 로직을 적용하여 새로운 출력 데이터 파티션을 만드는 것을 의미합니다. 그러나 이 작업은 새로 도착한 날짜 파티션에 대해서만 수행됩니다.\n\n더 복잡한 실행 로직을 구현할 수도 있습니다. 예를 들어, 늦게 도착한 데이터를 고려하여 최근 날짜 파티션의 추가 수를 덮어쓰거나, 특정 날짜 이전 또는 이후의 모든 출력 날짜 파티션을 덮어쓸 수도 있습니다.\n\n기술적 구현 옵션에는 천천히 변하는 차원에 대한 고려 사항이 있지만, 이는 이 기본 구현 범위를 벗어납니다.\n\n# 실행\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 제약 조건과 구현 논리를 고려할 때, 이 패러다임을 BigQuery에서 구현하는 운영 프레임워크의 각 측면을 정의하는 것은 실제로 매우 간단합니다. BigQuery 및 관련 기술에 대한 맥락을 고려하면, 다음 기사는 우리가 사용할 수있는 Google Cloud 리소스에 대한 넓은 개요를 제공합니다:\n\n이제 BigQuery에서 Functional Data Engineering 워크플로우를 구축, 실행 및 관리하기 위해 필요한 모든 리소스의 구현 옵션을 고려하고 평가하며, 다음 영역을 다루고 있습니다:\n\n- 인바운드 데이터\n- 변환 논리\n- 데이터 증가\n- 출력 데이터\n- 실행 논리\n- 실행 트리거\n- 모니터링\n- 버전 관리\n\n## 1. 인바운드 데이터\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은 BigQuery 데이터를 가져오는 메커니즘이 있지만, 입력 데이터가 분할되어야 하고 영구적이며 변경될 수 없다는 제약 조건으로 인해 다음 옵션이 선호됩니다:\n\n날짜별 테이블\n\n날짜별 테이블은 데이터를 효율적인 쿼리 패턴을 위해 서로 다른 물리적 파티션으로 분리하는 기본적인 방법입니다. 이를 함수형 데이터 엔지니어링 워크플로우의 입력 데이터로 사용할 수 있지만, 상류 프로세스는 변형 불가 제약 조건을 준수하고 데이터 파티션이 로드 후 변경되지 않도록 구성되어야 합니다.\n\n이를 지원하기 위한 메커니즘 중 하나는 흡수 시간에 의한 파티셔닝이며, 특정 데이터 집합 권한을 설정하여 데이터가 삭제되지 않고 영구적이 되도록 해야 합니다. 파티션된 테이블은 BigQuery 사용자 인터페이스에서 간단한 몇 번의 클릭으로 삭제할 수 있으므로 이를 방지하기 위한 보호장치가 마련되어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n날짜별 샤딩된 테이블\n\n날짜별 샤딩된 테이블은 Google에서 관리하는 가져오기 작업(예: GA4 BigQuery 내보내기)을 할 때 자주 만날 수 있습니다. 이들은 날짜별로 파티셔닝된 테이블과 유사하지만, 실제로는 접두사를 공유하고 날짜를 기반으로 한 접미사를 가진 기술적으로 별개의 테이블입니다. 이들의 주요 이점은 날짜별로 파티셔닝된 테이블에서 가능한 효율적인 쿼리 패턴을 달성할 수 있는 와일드카드 및 _TABLE_SUFFIX를 결합하여 질의할 수 있다는 것입니다.\n\n저장소 폴더 구조\n\nGoogle Cloud Storage와 BigQuery 외부 테이블을 결합한 접근 방식은 데이터 상호작용과 관리 사이의 분리를 제공하면서 불변성과 영속성 제약 조건을 달성하기에 매우 견고하고 효과적입니다. GCS 버킷에 파일로 저장된 데이터는 직접 쿼리할 수 있지만 BigQuery에서 삭제할 수는 없어 데이터 삭제에 대한 추가적인 보안 계층을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n_FILE_NAME 가상 열을 변환 로직에 포함시킬 수 있습니다. 이는 추가 입력 메타데이터를 제공하기 위해 구문 분석될 수 있습니다. 이는 잘 설계된 날짜 기반 폴더 구조(서로 다른 파일 유형을 별도 계층으로 구분)와 결합되어, 이 방법은 날짜별 파티셔닝 또는 날짜별 샤딩된 테이블이 달성할 수 있는 효율적인 쿼리 패턴을 복제하는 데 사용될 수 있습니다.\n\n## 2. 변환 로직\n\nBigQuery 뷰는 임의의 SQL을 데이터의 기저에있는 개요 데이터의 일시적인 뷰로 작성할 수 있는 유용한 자원입니다. SQL 기반의 변환은 출력 스키마와 데이터를 결정하며, 검사 가능한 스키마를 가지고 있어 매우 유용할 수 있지만 실제로는 널리 남용됩니다.\n\n쿼리시 뷰는 항상 모든 기저 데이터를 쿼리하고 다시 계산하므로 매우 비효율적입니다. 뷰는 종종 주기적으로 전체 하위 테이블을 다시 빌드하는 데 사용되며, 이는 불필요하게 비싼 방법이며, 입력 데이터 양이 증가함에 따라 비용이 증가할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 Table Functions을 사용하여 날짜 분할 기능을 추가하면 Views의 대부분의 긍정적인 특성을 활용할 수 있습니다.\n\nTable Functions\n\nTable Functions은 Views와 논리적으로 동일하지만 한 가지 중요한 측면에서 다릅니다. Table Functions은 인수를 취할 수 있으며, 이러한 인수는 SQL 쿼리에서 변수로 사용할 수 있습니다. 이것은 보기를 사용하면서도 날짜 파티션 하위 집합을 쿼리할 수 있는 강력한 기능을 제공합니다.\n\n이 인수들은 Table Function SQL의 구조적 부분(예: 열 또는 테이블 이름)을 형성할 수 없다는 점에 유의하십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n날짜 범위 테이블 함수\n\n날짜 범위 테이블 함수는 정확히 두 개의 DATE 인수인 start_date와 end_date를 사용하는 테이블 함수의 구현에 불과합니다. 테이블 함수 SQL이 우리의 영구적이고 변경할 수 없는 입력 데이터를 참조할 때, 쿼리 문에 WHERE 절을 추가하여 이러한 날짜 사이의 파티션 범위에 대한 데이터만 쿼리하고 반환할 수 있습니다.\n\n날짜 범위 테이블 함수를 연결하여 변환 로직을 별도의 단계로 분리할 수 있으며, 이로 인해 독립적으로 개발, 쿼리 및 테스트할 수 있게 됩니다.\n\n다음 예제 논리 구조는 날짜 범위 테이블 함수의 시퀀스에서 구성됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n외부 쿼리가 최종 Table Function(TF12)을 호출할 때, 쿼리 데이터 범위를 start_date 및 end_date 인수로 전달하면 해당 인수가 Table Functions의 일련의 순서를 통해 상위로 전파되며 입력 데이터에서 특정 날짜 범위만 쿼리됩니다. 변환 로직은 이후 이 날짜 범위에만 적용되고, Table Function은 변환된 데이터 분할을 반환하여 검사, 분석 또는 출력 테이블의 기존 분할에 덮어쓸 준비가 됩니다.\n\n## 3. 데이터 증강\n\n변환 로직은 일반적으로 데이터 구조 변경, 조인, 집계 및 계산을 포함하나, BigQuery는 입력 데이터에서 출력 데이터 분할로 전달되는 데이터를 더욱 증강하는 강력한 도구 세트를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사용자 정의 함수 (UDF)\n\n사용자 정의 함수는 강력하고 모듈식이며 재사용 가능한 기능으로 로직을 캡슐화하는 깔끔한 메커니즘입니다. SQL 또는 Javascript로 작성한 후에 SQL 쿼리 내에서 사용하여 데이터에 사용자 정의 분류, 계산 또는 기타 작업을 추가할 수 있습니다.\n\nBigQuery ML: AI 함수\n\nBigQuery ML (머신 러닝)에는 강력한 ML 모델을 바로 호출할 수 있게 하는 여러 내장 함수가 있습니다. 이 함수들은 입력부터 출력으로 흘러가는 데이터를 개선할 수 있도록 ML.UNDERSTAND_TEXT, ML.TRANSLATE, ML.ANNOTATE_IMAGE, ML.TRANSCRIBE 및 ML.PROCESS_DOCUMENT을 포함합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 경우에 대해 번역하면 다음과 같습니다.\n\n이러한 것들에는 경우에 따라 상당한 추가 비용이 소요되며, API 속도 제한을 피하고 일괄 처리 전략이 필요할 수도 있습니다.\n\n또한 ML.ANNOTATE_IMAGE 및 ML.PROCESS_DOCUMENT는 Google Cloud Storage 버킷에 호스팅된 파일로 분석해야 하며, BigQuery에서는 Object Table이 객체 인벤토리 역할을 하게 됩니다.\n\nBigQuery ML: 생성적 AI 기능\n\nBigQuery ML.GENERATE_TEXT 함수를 사용하면 간단한 SQL 쿼리를 통해 Google LLMs(gemini-1.5-flash, gemini-1.5-pro, gemini-pro, gemini-pro-vision,text-bison,text-bison-32,text-unicorn)에 직접 액세스할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기능을 사용하면 추가 비용이 발생할 수 있으며 안정적인 성능을 얻기 위해 일괄 처리 전략이 필요할 수도 있습니다.\n\n생성 AI 함수는 본질적으로 결정론적이지 않기 때문에 순수한 기능 데이터 엔지니어링 워크플로에 허용되지 않을 수 있습니다. 그러나 응답 구조가 결정론적인 경우, 이러한 경우처럼 변활 논리에 실제로 포함될 수 있으며 LLM의 응답은 출력 데이터에서 액세스할 수 있습니다. 다만 출력 데이터 파티션이 덮어쓰기되면 포함된 응답은 이전 응답과 다를 수 있습니다.\n\n원격 함수\n\n원격 함수는 Cloud Functions에 대한 액세스 포인트를 제공하여 다양한 언어로 사용자 정의 코드를 작성하고 외부 시스템과 상호 작용한 다음 BigQuery에서 응답을 인라인으로 반환할 수 있습니다. 이를 통해 BigQuery의 영역을 데이터 이상 및 자동화 사용 사례에 대한 Google 및 외부 API의 광범위한 세계에 확장할 수 있습니다. 그러나 Cloud Functions은 추가 개발, 인프라, 모니터링, 권한 및 디버깅 오버헤드가 따르므로 내장 또는 사용자 정의 함수를 사용하는 것이 더 좋습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4. 출력 데이터\n\n정확하고 신뢰할 수 있는 출력 데이터를 작성하고 유지하는 것이 기능 데이터 엔지니어링 워크플로우의 주요 목표입니다. 이 출력 데이터는 일반적으로 다음과 같은 형태를 띕니다:\n\n- 날짜별 테이블\n\n날짜별 테이블은 기능적 데이터 엔지니어링에서 선호하는 출력 구조입니다. 필요에 따라 클러스터링된 고카디널리티 열로 구성되며, 추가 필터링이 적용될 것으로 예상되는 열로 클러스터링될 수 있습니다. 파티션 가지치기를 통해 효율적인 쿼리 수행이 가능하며 (필요한 특정 데이터 파티션만 쿼리함), 쿼리할 데이터양이 줄어듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파티셔닝된 테이블 메타데이터는 INFORMATION_SCHEMA.PARTITIONS 뷰를 통해 조회할 수도 있습니다. 이는 모니터링 및 자동화 활동에 중요합니다.\n\n파티셔닝된 테이블은 최대 10,000개의 파티션을 가질 수 있으며, 따라서 날짜별로 분할된 테이블은 하루 날짜 파티션을 27년 이상 보유할 수 있습니다.\n\n클라우드 스토리지 버킷\n\n출력 데이터는 EXPORT DATA 문을 사용하여 다양한 파일 및 압축 형식으로 Google Cloud Storage 버킷에 내보낼 수도 있습니다. 그런 다음 이 데이터는 BigQuery에서 외부 테이블을 사용하여 압축 파일의 일반적으로 더 낮은 비용의 저장소인 Cloud Storage에서 액세스할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGoogle Cloud Storage 버킷 내용은 Object Tables을 사용하여 BigQuery에서 나열할 수 있습니다. 이를 통해 모니터링 및 자동화 기능의 기초로 활용해볼 수 있습니다. 그러나 모니터링 및 자동화는 URI(전역적으로 고유한 파일 경로)에서 날짜 메타데이터를 구문 분석하는 것에 따라 약간 복잡할 수 있습니다.\n\n또한 데이터를 Amazon S3 버킷이나 Azure Blob Storage로 내보내어 안전한 크로스 클라우드 데이터 통합을 할 수도 있습니다.\n\n## 5. 실행 로직\n\n기능 데이터 엔지니어링 워크플로우의 논리적 실행은 두 가지 핵심 BigQuery 기능에 의해 주도됩니다: 리소스 메타데이터에 액세스하기 위한 INFORMATION_SCHEMA 뷰 및 원하는 결과에 따라 기능 작업을 실행하기 위한 PROCEDURES입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 메타데이터 비교\n\n입력 및 출력 데이터가 파티션 열을 공유하도록 제한했다면, 입력 데이터와 출력 데이터 간의 메타데이터 비교는 차이점을 식별하는 매우 간단한 방법을 제공합니다. 또한 이는 모니터링 및 자동화를 구축하는 효율적인 기반으로 작용하며, 상태를 외부 저장소(예: 최신 실행 결과를 저장하는 테이블)에 저장할 필요가 없고 기본 데이터를 쿼리할 필요도 없습니다. 이것은 운영 비용을 예측 가능하고 낮출 수 있습니다.\n\n기능 캡슐화\n\n입력 데이터 테이블의 날짜 파티션 또는 샤드 배열을 반환하는 데 필요한 SQL을 수동으로 작성하는 것은 가능하지만, 우리의 선호하는 접근 방식은 그 자체로 기능적입니다. SQL 사용자 정의 함수(UDF)를 활용하여 SQL 컴파일 및 프로시저를 실행하기 위한 프로시저를 사용함으로써 자동화에 기능적 데이터 엔지니어링 접근을 지원하는 일부 유틸리티 함수에 액세스할 수 있습니다. 파티션된 테이블의 최신 날짜 파티션을 가져오기 위한 예시 기능 사용 코드는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nDECLARE source_table_id STRING;\nDECLARE last_partition_date DATE;\n\nSET source_table_id = 'myproject.mydataset.mypartitioned_table';\nCALL [project].[region].get_last_partition_date (source_table_id, last_partition_date) \n```\n\n이 예시에서 get_last_partition_date를 성공적으로 실행한 후, source_table_id로 참조된 테이블에 유효한 날짜 분할이 있는 경우, get_last_partition_date DATE 변수는 최신 날짜 값을 설정하고 후속 논리 단계에서 사용할 수 있습니다.\n\n비교적 작은 범위의 유사한 메타데이터 함수를 사용하여 실행 로직을 간단하게 표현하고 자동화 활동에 대한 신뢰할 수 있는 기반으로 사용할 수 있습니다.\n\n실행 모드\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기능 데이터 엔지니어링 플로우의 개발, 디버깅, 업데이트 및 유지 관리를 도와주는 여러 실행 모드가 있습니다. 첫 번째 두 가지는 다음과 같습니다:\n\n- 전체 테이블 새로고침 — 첫 번째부터 마지막 파티션까지 모든 데이터 덮어쓰기\n- 증분 새로고침 — 새 데이터가 도착했을 때 가장 최근의 n개 파티션을 덮어쓰기\n\n이러한 것들이 기본 실행 모드이지만, 추가적으로 도움이 되는 몇 가지 실행 모드가 있으며, 가속화되고 효율적인 개발 및 디버깅을 지원합니다:\n\n- 날짜 범위 새로고침 — 특정 날짜 범위 내의 모든 파티션 덮어쓰기\n- 최신 정보 새로고침 — 특정 날짜까지의 파티션 덮어쓰기\n- 시작 날짜 새로고침 — 특정 날짜부터 파티션 덮어쓰기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 실행 모드의 조합은 함수형 데이터 엔지니어링 흐름의 실행에 대한 세밀한 제어를 제공합니다.\n\n함수형 데이터 조작\n\n출력 날짜 파티션을 덮어쓰는 데 하드 코딩된 DML(Data Manipulation Language)을 사용할 수 있지만, 이렇게 하면 스크립트가 사용 사례별로 제한되며, 코드를 복사하고 편집하지 않으면 재사용할 수 없습니다.\n\n메타데이터 함수에도 동일한 방식을 적용하고 SQL UDF(User-Defined Functions), 프로시저(Procedures), 트랜잭션을 결합하여 날짜 파티션을 덮어쓰는 함수를 방한 방법으로 구축합니다. 이는 덮어쓰기 함수가 사실 DELETE 및 INSERT DML 문으로 구성되지만, 이것이 TRANSACTION 내에서 발생한다는 사실은 어떤 실패가 발생하더라도 롤백되므로 트랜잭션이 방한적이라는 것을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기능 권한\n\n함수 및 프로시저에 대한 액세스 권한을 허용할 때 주의해야 할 두 가지 중요한 기능이 있습니다:\n\n- 사용자 권한 — 사용자는 이메일, Google 그룹 또는 서비스 계정을 통해 지정된 데이터 세트에서 함수를 실행할 수 있는 권한을 부여받을 수 있습니다.\n- 루틴 권한 — 루틴에는 지정된 데이터 세트에서 함수를 실행할 수 있는 권한이 부여될 수 있습니다.\n\n즉, 사용자는 외부 프로젝트의 단일 데이터 세트에 대한 BigQuery Viewer 액세스를 부여받을 수 있으며, 해당 데이터 세트에 대한 권한만 있으면 데이터 세트 함수와 그에 허용된 종속 함수를 성공적으로 실행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n6. 트리거 로직\n\n실행 함수는 데이터의 신선도에 따라 여러 가지 다른 메커니즘을 통해 트리거될 수 있습니다.\n\n요청 시\n\n실행 함수는 필요에 따라 호출될 수 있으며, 후속 활동 직전에 BigQuery 콘솔에서 배포된 함수를 실행함으로써 실행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**예약된 쿼리**\n\n예약된 쿼리는 주기적인 일정에 맞게 실행 함수를 트리거하는 가벼운 방법으로, 실패 시 내장된 이메일 알림(간단한 경보 시스템을 위해 Slack로 자동 전달될 수 있음)이 제공됩니다. 쿼리 레이블을 추가하면 특정 쿼리 레이블을 기반으로 필터링하여 처리량 및 추정 비용별로 분석할 수 있습니다.\n\n예약된 쿼리는 복잡한 코드를 작성하기에는 권장되지 않으며, 해당 코드는 BigQuery에서 볼 수 없습니다. 예약된 쿼리의 결과를 테이블에 삽입하는 것은 비멱득 연산이며, 따라서 기능형 데이터 엔지니어링 흐름에서 피해야 합니다. 쓰기/잘라내기 연산은 멱득 연산이지만 전체 테이블 덮어쓰기의 처리 비용이 시간이 지남에 따라 증가하므로 일반적으로 비효율적입니다.\n\n이벤트 기반 트리거\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n빅쿼리에 데이터가 도착할 때 함수나 쿼리를 트리거하는 간단하고 기본 메커니즘이 없습니다. 이상적으로는 PubSub 주제를 테이블이나 데이터 세트에 연결하여 새 테이블이나 파티션에 도착할 때 트리거되고, 다른 임의의 함수 실행을 트리거할 수 있으면 좋을 것입니다. 그러나 현재는 이를 외부 도구 없이 실행할 수 없으므로 일반적으로 출력 데이터의 원하는 데이터 신선도와 일치하는 빈도로 예약된 실행 쿼리를 배포합니다.\n\n## 7. 모니터링\n\n빅쿼리에서 데이터 워크플로우를 모니터링하는 것은 다양한 방법을 결합하여 구현할 수 있습니다.\n\n메타데이터 모니터링\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nThe INFORMATION_SCHEMA views can be used to monitor resource metadata in order to manage and optimize resource, partition, and storage infrastructure.\n\nLog Monitoring\n\nThe INFORMATION_SCHEMA.JOBS view is used to monitor query patterns, compute volume and associated costs, and to isolate unnecessary compute-related expenses.\n\nQuery Labelling\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n테이블 태그를 Markdown 형식으로 변경해 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저장된 쿼리는 BigQuery 내에서 버전 관리를 구현하는 초보자용 방법이지만, 정의된 프로세스를 엄격히 준수하여 사용해야 합니다. 현재 BigQuery에서 코드 버전을 관리하는 강인한 독립형 솔루션이 아닙니다.\n\nGit 저장소 통합\n\n지금까지 BigQuery에서 코드를 버전 관리하는 것은 매우 어려웠지만, Git 저장소 통합이 현재 시험 중이므로 주목해 주세요! 일부 테스팅을 거친 후에 이 새로운 (그리고 매우 환영받는) BigQuery 기능을 함수형 데이터 엔지니어링 워크플로에 어떻게 사용하는지에 대한 업데이트를 올릴 수 있을 것입니다.\n\n# 구현\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기까지 오신 여러분은 BigQuery에서의 기능적 데이터 엔지니어링에 관심이 있는 분들이시군요... 멋지네요!\n\n우리가 구현하고 확장하며 기능적 데이터 엔지니어링 워크플로우를 관리하기 위해 사용하는 기능 라이브러리에 대한 문서는 여기에서 확인할 수 있습니다: bqtools.\n\n또한, 이 기능적 데이터 엔지니어링 접근 방식을 활용하여 Google Analytics 4 데이터의 자동 전처리 및 증강을 실행하는 첫 번째 제품을 개발했습니다. 자세한 내용은 여기에서 확인할 수 있습니다: Decode Data for GA4.\n\n이 자료에서 여러분이 BigQuery 여정에서 도움이 될만한 유용한 자료가 있다면, 저희에게 문의해 주세요. 도움이 필요한 사항이 있다면 언제든지 연락해 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n행복한 기능성 데이터 엔지니어링!\n\n🔎 변환 플로우에 대해:\n\n저희는 기업 및 기관들이 Google Cloud에서 빅쿼리를 중심으로 데이터 기능을 설계, 배포 및 관리할 수 있도록 지원합니다.\n\n빅쿼리 프로그래밍에 대한 깊은 전문 지식을 가지고 있기 때문에, 기능적 데이터 엔지니어링, 빠른 능력 개발 및 비용/보안 감사를 지원하기 위해 빅쿼리 기능을 확장하는 라이브러리를 구축합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 내부 원격 함수 라이브러리에서 배포할 수도 있고, 사용 사례에 맞춰 BigQuery 기능을 확장하기 위해 사용자 정의 함수를 개발할 수도 있습니다.\n\n당신의 BigQuery 여정이 어디에 있든 우리가 도와드릴 수 있어요. 여기까지 읽으셨다면 직접 jim@transformationflow.io 로 연락하시거나 여기에서 상담 예약해주세요! 🔎","ogImage":{"url":"/assets/img/2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery_0.png"},"coverImage":"/assets/img/2024-06-22-AGuidetoFunctionalDataEngineeringinBigQuery_0.png","tag":["Tech"],"readingTime":16},{"title":"품질 엔지니어를 위한 RAG 사용 가이드","description":"","date":"2024-06-22 17:14","slug":"2024-06-22-RAGforQualityEngineers","content":"\n\n## RAG 만들기는 쉽지만, 품질 있는 RAG 만들기는 어려워요\n\n![image](/assets/img/2024-06-22-RAGforQualityEngineers_0.png)\n\n검색 확장 생성(RAG)은 대규모 언어 모델(LLMs)의 기능을 확장하는 일반적인 패턴이 되었습니다.\n\n이론적으로 RAG는 간단합니다(컨텍스트 창에 데이터를 추가하기만 하면 됩니다!) 하지만 실제로는 복잡합니다. 상자 다이어그램 너머에는 고급 청킹 전략, 재랭킹, 다중 쿼리 리트리버, 작은 데이터부터 큰 데이터 검색, 가상 문서 임베딩, 사전 임베딩 데이터 보강, 동적 라우팅, 맞춤 임베딩 모델... 등이 숨어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n초기 파이프라인을 설정하는 것은 빠르고 쉽지만, 프로덕션 수준의 품질에 도달하는 것은 상당히 복잡합니다. 신중한 고려 없이 RAG 시스템은 부정확하거나 관련성이 없는 정보를 반환할 수 있습니다. 비효율적으로 비싼 리소스를 소비하거나 프로덕션 규모의 소스 데이터로 확장할 때 병목 현상이 발생할 수도 있습니다.\n\nRAG 시스템의 품질을 효과적으로 평가하고 효율적으로 이해하는 것은 각 개별 구성 요소가 전체 RAG 파이프라인을 만드는 과정을 이해하는 데 필요합니다. 이러한 각 부분에 대한 설계 결정은 품질에 영향을 미치며, RAG 애플리케이션을 배포하려는 모든 사람이 알아야 합니다.\n\n이 글에서는 테스트와 품질 관점에서 RAG 개념과 패턴에 대한 소개를 제공합니다. RAG가 왜 가치 있는지에 대한 소개로 시작하여, 프로덕션 품질의 RAG를 구축하는 데 내재된 많은 설계 결정이 어떻게 향상되는지에 대해 설명합니다. 이 소개는 특정 평가 방법과 기법에 대해 논의하기 전에 필요한 기초를 제공할 것입니다.\n\n# LLM의 한계\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRAG 파이프라인을 이해하려면, RAG가 대응하려는 LLM의 한계를 먼저 이해해야 합니다.\n\nLLM의 핵심은 간단합니다: 프롬프트를 보내면 응답을 받습니다.\n\nLLM이 응답을 반환하려면 모델과 추론 계산을 실행해야 합니다. 이 계산은 입력을 수백억 개 또는 수조의 매개변수와 결합하는 것을 포함합니다. 이는 비용이 많이 드는 계산입니다.\n\nLLM을 호출하는 것만큼이나 LLM을 훈련시키는 것은 훨씬 어렵습니다. 훈련은 모델 내 매개변수에 대한 최적값을 결정하는 과정입니다. 최상의 가중치를 계산하는 데 사용되는 다양한 알고리즘이 있지만, 모두 특정 입력에서 모델을 실행하고 오차를 계산한 후에 조정을 하는 반복적 과정을 포함합니다. 이 과정은 많은 횟수로 많은 입력에서 계속되며, 결국 훈련된 모델을 얻게 됩니다. 모델 추론은 몇 초만에 완료될 수 있지만, 모델 훈련은 광범위한 GPU 클러스터에서도 몇 주가 걸릴 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-RAGforQualityEngineers_1.png\" /\u003e\n\n엄청난 교육 비용은 새로운 정보를 LLM에 통합하는데 병목 현상을 야기합니다. 대부분의 기업은 모델을 교육할 자원이 없으며, 단순히 사적 데이터로 교육하여 LLM에 \"새로운 정보를 추가\"할 수 없습니다. 대신, 잘 자금을 지원받는 대규모 기술 기업은 대형 공개 데이터 세트에서 일반 목적의 기반이 되는 모델을 교육하고, 이러한 모델은 RAG와 같은 보조 프로세스로 새로운 능력 및 정보가 부가됩니다.\n\n구체적으로 RAG는 새로운 모델을 교육하는 높은 비용을 우회하는 방식으로 LLM이 추가적인 지식에 접근하도록 하는 것을 목표로 합니다.\n\n# RAG 기본 원리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRAG가 실제로 작동하는 방식에 대해 알아봅시다.\n\nLLM에 전송된 프롬프트는 제한된 길이인 context window을 가지고 있습니다. Context window은 토큰(단어에 대한 대략적인 동등물) 단위로 측정됩니다. Context window의 크기는 보통 1K, 4K 또는 그 이상의 토큰으로 표시되지만 더 큰 context window이 사용 가능해지고 있습니다(예: Gemini 1.5 Pro의 128K).\n\n많은 사람들이 직관적으로 context window은 할 수 있는 가장 긴 질문이라고 생각하지만, 이는 context window에 대한 한정적인 사고 방식입니다. LLM의 작동 방식으로 인해, context window에 제공된 정보는 LLM이 응답을 생성하는 동안 LLM에게 사용 가능합니다. 따라서 추가 정보를 제공하는 데 사용될 수 있습니다. 일반적으로 이를 in-context learning 이라고 합니다.\n\n따라서, 우리는 context window을 사용하여 LLM이 질문에 답변하기 위해 필요한 새로운 지식을 제공할 수 있습니다. 예를 들어, 우리가 상조에 관한 회사 정책에 대해 물어보는 프롬프트를 만들고, context window 내에서 이에 관한 전체 회사 안내서(상조에 관한 섹션 포함)를 넣는다면 새로운 정보를 제공하여 LLM이 응답할 수 있게 할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해당 솔루션은 이미 관련 정보가 있고 그 정보가 문맥 창 안에 쏙 들어갈 수 있는 경우 간단합니다. 불행히도 항상 그런 것은 아닙니다. 따라서 우리는 우리의 프롬프트에 관련 정보만 검색 및 다운 선택할 수 있는 메커니즘이 필요합니다.\n\n무식한 접근법은 프롬프트에서 용어를 검색하여 관련할 수 있는 데이터 전체에서 주변 텍스트를 복사한 후 이를 프롬프트에 추가하는 것입니다.\n\n이 간단한 키워드 검색 형태의 RAG는 LLM 응답을 향상시킬 수 있으며 어떤 맥락에서는 유용할 수 있지만 가끔식 거짓 양성 (다른 맥락에서 사용된 키워드) 때문에 문제가 될 수도 있습니다. 다행히도 우리는 텍스트의 의미에 맞추어 일치시키는 의미 검색을 활용하여 더 나은 결과를 얻을 수 있습니다.\n\n구체적으로, 우리는 포지블리 관련 데이터 청크에서 임베딩 모델을 활용하여 임베딩을 생성한 다음 이 임베딩을 통해 데이터를 검색하여 우리의 프롬프트에 관련된 데이터를 찾을 수 있습니다. 이 방법은 매우 단순화된 접근법이지만, 진정한 RAG와 같은 결과를 얻기 시작하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# RAG 및 임베딩\n\n간단한 키워드 검색보다 RAG가 제공하는 이점을 이해하려면 임베딩 모델의 목적과 성격을 이해해야 합니다. 이는 그 자체로 심도 있는 주제이지만, RAG를 이해하는 데 중요합니다.\n\n임베딩 모델은 우리의 원래 LLM과 유사하지만, 새로운 콘텐츠를 생성하는 대신 입력을 벡터(숫자 목록)로 축소합니다. 임베딩 모델은 매우 큰 숫자 목록입니다. 임베딩 모델이 생성하는 벡터는 일반적으로 768 또는 1536 숫자(차원)지만, 다른 크기의 벡터도 존재합니다.\n\n임베딩 모델에 의해 생성된 벡터는 단순히 임의의 숫자 세트가 아니라 모델에 따라 입력 데이터의 의미를 요약한 것입니다. 이 벡터는 다른 모델에게는 의미가 없지만 \"유사한\" 텍스트는 같은 모델에서 유사한 벡터를 생성할 것입니다. \"유사하다\"는 단순히 \"동일한 키워드를 가지고 있다\" 이상을 의미합니다. 임베딩 모델은 구조화되지 않은 데이터로부터 보다 심층적인 의미를 추출하기 위해 특별히 훈련되었습니다. 예를 들어 \"남자 말이 날지 않는다\"와 \"날개가 있는 사나이가 말 타고 있다\"는 비슷한 단어를 가지고 있지만 같은 모델에서는 서로 멀리 떨어진 벡터를 생성할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n벡터의 멋진 점은 그들에게 수학 연산을 수행할 수 있다는 것입니다. 빠른 수학이죠. 수백만 개의 벡터를 검색하여 비교적 짧은 시간 안에 유사한 벡터를 찾을 수 있습니다. (여기에 사용된 일부 알고리즘이 있습니다.)\n\n이제 우리의 RAG 파이프라인 조각들이 모두 마련되었으니, 단계별로 진행해봅시다.\n\n첫 네 단계는 한 번에 수행되거나 소스 데이터가 변경될 때 업데이트됩니다. 다섯 번부터 여덟 번까지의 단계는 각 추론 요청마다 수행됩니다:\n\n- 모든 가능성 있는 데이터를 수집합니다. - 이토록 많은 데이터가 있어서 우리의 프롬프트의 컨텍스트 창에 쏙 들어가기 불가능합니다.\n- 이 데이터를 더 작은 조각으로 나눕니다 (나중에 자세히 설명하겠습니다).\n- 그런 다음 각 조각을 임베딩 모델을 통해 실행하여 조각의 의미를 포함하는 벡터를 생성합니다.\n- 해당 벡터를 벡터 데이터베이스에 저장합니다.\n- 각 추론 요청으로: 프롬프트를 받으면, 그 프롬프트를 조각낸 소스 데이터와 동일한 임베딩 모델을 통해 실행하여 다른 벡터 (프롬프트 벡터 또는 쿼리 벡터라고 함)를 생성합니다.\n- 우리의 벡터 데이터베이스에서 우리의 프롬프트 벡터와 유사한 벡터를 검색합니다. 반환된 벡터들은 생 데이터를 키워드로 검색했다면 얻었을 것보다 더 나은 매치일 것입니다.\n- 식별된 관련 벡터를 (선택적으로) 재정렬하고, 그런 다음 상위 벡터 각각의 생 데이터를 반환합니다.\n- 원시 데이터는 초기 프롬프트와 결합되어 LLM으로 보내집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_2.png)\n\n여기, 우리 LLM은 이제 새로운 독점 데이터를 모두 훈련한 것처럼 작동하며, 기본 모델 훈련을 비용 문제로 수행할 필요가 없습니다.\n\n이론상으로는 이렇게 작동해야 합니다. 그러나 실제로는 이 너무 단순한 파이프라인은 여러분의 제품 요구를 만족시키지 못할 가능성이 높으며, 우수한 품질의 제품에 도달하려면 RAG 파이프라인을 준비하려면 특정 애플리케이션의 요구를 충족시키기 위해 다양한 부분을 적응, 개선, 교체 또는 확장해야 할 것입니다.\n\n# RAG 디자인과 품질\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서는 RAG를 소개하지만, RAG는 실제로 매우 복잡할 수 있으며, 이러한 실제 세계의 복잡성은 응용 프로그램의 품질에 영향을 미칠 수 있습니다. RAG 파이프라인 내에서 사용 가능한 일부 구현 도전, 품질 위험 및 대안을 이해하기 위해 각 단계를 살펴보겠습니다.\n\n## #1—관련 데이터 수집, 적재 및 풍부화\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_3.png)\n\n시작부터 모든 \"가능성 있는 데이터\"를 찾아야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 RAG 다이어그램에서 1번 아이콘은 여러 소스에서 데이터를 수집하고, 정리하고, 변환하며, 익명화하고, 토큰화하는 데이터 파이프라인(또는 파이프라인 세트!)일 가능성이 높습니다.\n\n일부 파이프라인은 특히 텍스트 이외의 형식을 가진 원시 데이터의 경우 매우 복잡해질 수 있습니다. 예를 들어, 일부 파이프라인은 대량의 스캔된 물리 문서를 처리하기 위해 OCR 기술을 널리 활용합니다.\n\n데이터 파이프라인의 복잡성은 데이터 파이프라인 테스트의 모든 과제가 따라옵니다.\n\n가장 잘 구현된 RAG 파이프라인도 소스 데이터가 심지어 벡터 DB로 전달되지 않는다면 완전히 실패할 수 있으며, 이 데이터의 다양성, 속도 및 양에 따라 RAG의 이 단계는 복합적이며 많은 응용프로그램 품질 문제의 원인이 될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적인 데이터 파이프라인 활동에 추가로, RAG는 데이터 풍부화를 통해 혜택을 얻을 수 있습니다. 종종, 다른 시스템(또는 사람들)은 소스 데이터에 대한 맥락을 알고 있어서 그 의미를 평가하는 데 엄청난 도움이 될 수 있습니다. 예를 들어, 고객 데이터베이스에는 다른 시스템에서 제공하는 태그나 주석과 같은 관련 정보를 추가하여 데이터를 풍부화할 수 있습니다. 종종, 다른 생성 모델이나 자연어 처리(NLP)가 더 깨끗하거나 요약된 메타데이터를 생성하는 데 사용됩니다. 모두 \"임베딩 생성\" 전의 \"전처리\"로 생각해보세요. 그리고 제대로 수행된다면, 검색 품질을 크게 향상시킬 수 있습니다.\n\n당신이 RAG 검색 시스템의 품질을 평가하고 있다면, 데이터가 실제로 어떻게 소스되고 흡수되는지 이해하는 데 시간을 투자하는 것이 가치가 있습니다. 훌륭한 AI 부분에 도착하기 전에 RAG 파이프라인에 도달하기 전에 데이터가 어떻게 가져오고 처리되는지 알아두세요.\n\n## #2—Chunking\n\n![2024-06-22-RAGforQualityEngineers_4](/assets/img/2024-06-22-RAGforQualityEngineers_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터가 수신되고 임베딩 모델을 실행하기 전에는 데이터를 diskrete pieces로 나눠야 합니다. 그렇다면 데이터를 어떻게 분할할지 어떻게 결정하나요? 이를 chunking strategy라고 합니다.\n\n최적의 크기는 얼마나 크거나 작아야 할까요? 청크들은 서로 중첩되어야 할까요? 페이지, 단락 또는 일정한 길이로만 나누는 것 이외에 더 스마트한 분할 방법이 있을까요? 비표준 형식의 데이터(code, JSON 등)는 어떻게 chunk해야 할까요?\n\n이러한 질문들은 chunking strategy가 답하려고 노력하는 것이며 완벽한 해결책은 없습니다. 서로 다른 전략은 서로 다른 타협점을 갖습니다. 일부는 간단하고 빠르게 구현할 수 있으며, 보통 결과를 제공합니다. 다른 전략은 더 복잡하고 관련이 깊습니다. 더 나은 히트율과 LLM 응답 품질을 제공할 수 있습니다. 데이터를 너무 거칠게 나누면 의미 없는 데이터로 context window를 채우거나 다른 관련 청크를 밀어내거나 의미 있는 일치를 얻을 수 없을 정도로 일반적인 임베딩을 생성할 수 있습니다. 너무 세밀하게 나누면 관련 데이터를 잘라낼 수 있습니다.\n\n이 문서에서는 다섯 가지 chunking 범주를 탐구합니다: 고정 크기, 재귀, 문서 기반, 의미 기반, 그리고 AI를 사용한 Agentic(체킹에 인공 지능 사용, 멋지죠!)입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은 기타 접근 방식이 있습니다. 예를 들어, 작은 것에서 큰 것으로 검색을 최적화하려면 작은 청크를 사용하지만 각 청크는 큰 부모 청크에 연결되어 있어서 삽입될 컨텍스트 모델에 검색됩니다. 콘텍스트 인식 청킹은 문서의 성격에 대해 기존 지식을 활용하여 문서를 논리적 청크로 적절하게 분할합니다.\n\n위 목록은 아마 완전하지 않을 수 있지만, RAG 구현자가 사용할 수 있는 다양한 옵션과 애플리케이션 전체의 품질에 적합하고 조정된 청킹 전략의 중요성을 보여줍니다. Pinecone 블로그에서 이러한 전략 중 많은 내용을 자세히 다루고 있습니다.\n\n## #3—임베딩 모델 선택과 구성\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n임베딩을 생성하는 데 사용할 수 있는 여러 모델이 있습니다. 서로 다른 모델은 다양한 상황에서 더 나은 또는 나쁘게 수행할 수 있습니다. 일부 모델은 일반 사용을 위해 사전 훈련되어 있고 일부는 특정 도메인(예: 의학 기록)에 대해 세밀하게 조정되어 있습니다. 또한 응용 프로그램에서 처리하는 특정 데이터에 대해 자체 임베딩 모델을 세밀하게 조정할 수도 있습니다.\n\n또한 많은 모델이 다른 크기로 제공되며(임베딩 생성 비용 및 시간에 영향을 미침), 다른 입력 길이(처리 가능한 최대 청크 크기) 및 다른 출력 벡터 차원(높은 차원 = 더 정확하지만 더 많은 공간 요구와 느린 속도)으로 제공됩니다.\n\n일부 임베딩 모델은 API를 통해만 액세스할 수 있습니다(예: OpenAI 임베딩 엔드포인트), 다른 모델은 완전한 오픈 소스로 제공되어 로컬에서 다운로드하고 실행하거나 클라우드 공급업체에 호스팅할 수 있습니다.\n\n응용 프로그램 내에서 다른 데이터 경로에 대해 다른 임베딩 모델을 사용하는 것도 가능합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적으로 좋은 임베딩 모델은 RAG 응용 프로그램에 충분할 수 있지만, 일부는 특정 임베딩 모델이나 사용자 지정된 모델을 사용하여 혜택을 얻을 수 있습니다.\n\n임베딩 전략에 대한 설계 고려 사항과 해당 선택의 품질 특성을 알면 응용 프로그램의 평가 요구 사항과 접근 방식에 대한 통찰력을 제공할 것입니다. 임베딩 모델 선택을 평가하는 더 깊은 논의는 여기에서 확인할 수 있습니다.\n\n## #5—쿼리 처리 및 임베딩\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수신한 쿼리에 임베딩 모델을 정확히 실행해야 한다는 규칙은 없습니다. 실제로 이 쿼리를 최적화하여 애플리케이션의 전반적인 품질을 향상시킬 수 있는 다양한 방법이 있습니다. 이는 특히 쿼리가 사람 사용자로부터 직접 제출되었으며 모호하고 애매한 쿼리일 경우에 더욱 참된 것입니다.\n\n애플리케이션의 특성 또는 의도에 대한 추가 지식을 통해 LLM 또는 전통적인 논리를 사용하여 쿼리를 축소하거나 다시 작성하는 것이 가능할 수 있습니다. 다시 말해, 의도된 것이 아닌 실제로 묻는 것이었던 쿼리를 재작성하는 방식으로 쿼리를 재구성할 수 있습니다.\n\n쿼리 처리의 고급 형태인 HyDE도 있습니다. 여기서는 가상 문서를 작성하여 유사한 문서(답변에서 답변)를 벡터 검색하고 임베딩 및 쿼리 검색(질문에서 답변)을 하는 것대신 사용할 수 있습니다.\n\n또 다른 옵션은 쿼리를 여러 관련된 쿼리로 분할하고 각각을 병렬로 실행한 다음 결과를 결합하는 것입니다. 이는 처리 비용이 듬성들지만 검색 품질을 향상시킬 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특정 사용 사례에 따라 사용자 지정 쿼리 처리가 필요할 수 있으며 응용 프로그램의 품질과 동작에 큰 영향을 미칠 수 있습니다.\n\n## #4, #6—Vector DB 및 Vector Search\n\n![image](/assets/img/2024-06-22-RAGforQualityEngineers_7.png)\n\n벡터 검색은 빠르지만, 쿼리와 유사한 임베딩을 찾기 위해 벡터 DB를 검색하는 데에는 시간 (그리고 돈) 비용이 소요될 수 있습니다. 이 비용을 최소화하는 한 가지 방법은 의미 캐싱입니다. 의미 캐싱에서는 임베딩이 처음 검색된 후 응답이 캐시되어, 향후 유사한 검색이 캐시로부터 직접 데이터를 반환하게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n물론, 캐싱은 복잡성을 증가시킵니다 (그리고 컴퓨터 과학에서의 두 번째 어려운 문제 중 하나입니다—다른 하나의 이름을 기억하지 못하겠군요). 캐싱은 성능을 향상시킬 수 있지만, 오래된 캐시는 변동성 있는 소스 데이터 환경에서 특히 응답 품질을 해치는 요인이 될 수 있습니다.\n\n## #7—재랭킹\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_8.png)\n\n위에서 설명한 내용에서 우리는 단순히 우리의 벡터 검색으로 반환된 모든 관련 데이터를 컨텍스트 창에 채울 수 있다고 가정했습니다. 이것은 명백히 간소화된 내용이며, 반환된 모든 벡터 중 어느 것이 컨텍스트 창에 포함될 것인지를 결정하기 위한 과정이 있어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n검색 결과를 콘텍스트 창 안에 맞출 수 있을 때에도, 많은 연구에서 콘텍스트 Stuffing(콘텍스트 창 채우기)가 LLM 회상을 부정적으로 영향을 줄 수 있다는 것을 지적합니다. 이는 중간에서 사라지는 문제를 도입하여 응답 품질(회상은 LLM이 콘텍스트 창에 있는 정보를 사용하는 능력입니다)에 영향을 줄 수 있습니다.\n\n해결책은 초기 벡터 검색 후에 추가 단계로 재랭킹을 추가하는 것입니다.\n\n재랭킹의 TLDR(요약): 임베딩 모델은 속도에 최적화되어 있으며 많은 문서에 대해 실행되어야 하므로 빠릅니다. 재랭킹 모델(또는 교차 인코더)이라 불리는 다른 모델은 느리지만 정확도에 최적화되어 있습니다. 그래서 빠르고 부정확한 임베딩 모델을 사용하여 임베딩을 생성한 다음, 작은 집합에서 최고 품질의 문서를 찾기 위해 느리고 정확한 모델을 사용합니다. 느린 정확한 검색에서 가장 일치하는 결과는 콘텍스트 창에서 우선순위를 갖습니다.\n\n다시 말하지만, 이보다 더 많은 내용이 있지만, 재랭킹의 본질은 바로 이것입니다. Pinecone 블로그에서 더 자세한 설명을 찾을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다시 순위를 매기면 RAG에서 반환된 데이터의 관련성을 크게 향상시킬 수 있습니다. 컨텍스트 창에서 더 관련성이 높은(또는 무관련성이 적은) 데이터는 응답 품질을 향상시킬 것입니다. 그러나 복잡성과 지연이 증가하지만, 품질의 트레이드오프는 많은 RAG 응용 프로그램에서 가치 있는 요소일 수 있습니다.\n\n## 큰 컨텍스트 창 vs. RAG\n\n우리는 마침내 LLM을 호출하는 지점에 도달했지만, 프롬프트 엔지니어링에 대해 이야기하기 전에 RAG와 큰 컨텍스트 창 간의 관계에 대해 언급할 시간을 가져야 합니다.\n\nLLM 기술은 빠르게 발전하고 있으며 개선의 한 가지 측면은 컨텍스트 창의 크기입니다. 한 가지 대표적인 예는 2024년 2월에 출시된 Gemini 1.5 Pro이며, 128K 컨텍스트 창을 제공하며(공개적으로 출시되지 않음) 최대 백만(!!!) 토큰까지 확장할 수 있는 옵션이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일부 사람들은 100만 토큰 컨텍스트 창이 RAG 파이프라인을 사용할 때 사용되지 않을 것이라고 예상했습니다. 그러나 실제로는 그렇지 않습니다. 이 블로그에서는 RAG가 왜 유용하며 (심지어 필수적이기도 한) 거대한 컨텍스트 창을 사용할 때도 필요한 이유에 대해 설명합니다. (스포일러: 비용, 지연 시간 및 회수 품질)\n\n대규모 컨텍스트 모델은 유용하며, LLMs가 많은 사실들 간에 종합적인 결론을 요구하는 쿼리에 응답하는 데 도움이 될 수 있습니다 (이러한 사실들이 RAG를 통해 선별되어 있는지 여부는 상관 없음).\n\n큰 컨텍스트 창과 RAG 간의 관계는 계속 발전할 것이며, RAG를 구현하고 테스트하는 사람들은 응용 프로그램 품질에 미치는 이러한 트레이드오프와 그들의 영향을 이해해야 합니다.\n\n## #8—프롬프트 생성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![2024-06-22-RAGforQualityEngineers_9](/assets/img/2024-06-22-RAGforQualityEngineers_9.png)\n\n벡터DB에서 관련 데이터를 많이 받아 재설정하고, LLM(Large Language Model)의 문맥 창 안에 맞는 적절한 데이터 세트로 마무리했습니다. 그럼 이제 어떻게 해야 할까요? 받은 데이터를 초기 질문 뒤에 밀어 넣고 끝내면 될까요?\n\nLLM을 다뤄본 사람이라면 알 수 있듯이, 그것만큼 간단한 일이 아닙니다. LLM은 강력할 수 있지만, 변덕스럽고 짜증을 유발할 수도 있습니다. 당신의 프롬프트에 작은 세부 사항이 응담 품질에 상당한 영향을 미칠 수 있다는 것이 밝혀졌습니다. 프롬프트의 단어 선택, 데이터 순서, 사용하는 어조, \"시간을 들이다\"와 같은 제안, 심리적 언어 사용까지 모두 LLM 응답 품질에 영향을 미칠 수 있습니다. 프롬프트를 자동으로 생성하는 최적 프롬프트 생성 전략이 있습니다. ...맞아, 프롬프트 생성에 특별히 훈련된 다른 모델을 사용하는 것입니다. 이것은 신속히 진화하는 프롬프트 엔지니어링 분야의 일부입니다.\n\n최상의 품질의 응답을 생성할 정확한 프롬프트 템플릿은 보통 모델 및 응용 프로그램에 따라 다르며 종종 실험과 시행착오가 필요할 수 있습니다. RAG의 이 보이지 않는 작은 세부 사항이 가지는 품질 영향을 감안할 때, 적용된 특정 프롬프트 엔지니어링은 시스템의 다른 부분과 마찬가지로 면밀히 평가되고 심사되어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# RAG 시스템의 측정 및 평가\n\n우리는 RAG 파이프라인의 주요 구성 요소들을 살펴보고 (간략하게) 이들이 애플리케이션 품질에 미치는 영향에 대해 이야기해 보았습니다. 이것은 소개였지만, 이러한 유형의 애플리케이션의 내부 작업 및 품질 도전에 대한 통찰력을 제공해야 합니다. RAG에 대해 더 깊이 파고드는 많은 훌륭한 기사, 블로그, 논문이 있습니다. 시작할 때 하나만 선택한다면, \"Retrieval-Augmented Generation for Large Language Models—A Survey\"를 읽어보세요.\n\n주요 교훈: RAG를 구현할 때 선택할 수 있는 옵션과 선택지가 많으며, 각각에는 품질에 대한 대가와 영향이 있습니다. 이러한 선택들 중 일부는 직접적으로 평가될 수 있고, 일부는 전반적인 검색이나 응답 품질에 영향을 미칩니다. 이러한 선택 각각과 이들이 당신의 RAG 시스템에 어떤 영향을 미칠 수 있는지 이해하는 것이 전반적인 애플리케이션의 제품 품질을 달성하는 데 중요합니다.\n\n당연한 다음 질문은 다음과 같습니다: 좋아, 그런데 RAG를 어떻게 평가할까요? 개방형 자유형식 응답의 품질을 어떻게 측정할까요? 어떤 지표를 사용하여 실제로 측정할 수 있을까요? 이러한 평가를 자동화할 수 있을까요, 그리고 어느 수준에서 할 수 있을까요? LLM은 본질적으로 비결정론적이고 그들이 소비하는 데이터도 본질적으로 불안정할 때 품질을 어떻게 보장할까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이런 건들로 이루어진 큰 질문들이에요. 우리는 ARC나 HellaSwag 같은 프레임워크를 이용한 모델 평가, LLM-as-a-judge와 같은 접근 방식, 바늘 찾기 테스트와 같은 테스트, 어려움, 신뢰성, 그리고 관련성과 같은 측정 항목, Ragas와 LlamaIndex와 같은 도구 등의 주제를 다룰 거에요.\n\n하지만, 이 모든 재미로움은 다음 블로그를 기다려야 해요.\n\n본 글에 대한 기술적 피드백으로 Etienne Ohl와 Jack Bennetto에게 특별히 감사드려요.","ogImage":{"url":"/assets/img/2024-06-22-RAGforQualityEngineers_0.png"},"coverImage":"/assets/img/2024-06-22-RAGforQualityEngineers_0.png","tag":["Tech"],"readingTime":13},{"title":"데이터 메시에 대한 도전 과제 및 해결책  3부","description":"","date":"2024-06-22 17:11","slug":"2024-06-22-ChallengesandSolutionsinDataMeshPart3","content":"\n\n'연합된 계산 기반 거버넌스'는 안전하고 신뢰할 수 있으며 상호 운용 가능한 데이터 메시를 보장합니다. 상호 운용성으로부터 추가 가치는 종종 \"전체가 부분의 합보다 더 크다\"는 구절로 요약됩니다. HTTP와 같은 표준 프로토콜, 효율적인 데이터 전송 메커니즘, 그리고 구성 요소의 포괄적인 버전 관리와 같은 기술적 측면을 설정하는 것이 상호 운용성을 활성화하는 데 중요하다면, 여기서 우리의 초점은 전체적인 데이터 일관성과 호환성 유지에 있을 것입니다.\n\n상호 운용성을 참으로 지원하려면, 우리는 모든 데이터 제품에서 내용을 포괄하는 일관된 혹은 일관성 있는 모델을 보장해야 합니다. 이 모델은 새로운 데이터 제품이 추가됨에 따라 동적으로 업데이트되어야 합니다. 나의 삼부작 시리즈의 마지막 글에서는 본래 체계적이며 최신으로 유지되는 종합적인 관점을 유지하는 실용적인 방법을 보여드리도록 하겠습니다.\n\n거버넌스는 종종 발전을 억제할 수 있는 엄격한 규칙으로 여겨집니다. 즉시적인 혜택 없이 부가 작업을 추가하는 부담으로 여겨집니다. 거버넌스 프로세스를 자동화함으로써(즉 '계산 기반'으로 만들면) 데이터 메시 내에서 그 적용을 효율적으로 만들 수 있지만, 프로젝트나 제품 비용은 그대로 남아 있을지도 모릅니다.\n\n그렇다면, 우리는 어떻게 거버넌스를 변경하여 분산된 팀에 가치를 제공하고 동시에 참여를 촉진할 수 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 지배 규칙을 유용한 프레임워크로 변환하고 커뮤니티 기반의 아이디어와 시장 기반의 메커니즘을 활용함으로써 이를 달성할 수 있습니다. 이러한 전략은 참여와 가이드라인 준수를 효과적으로 장려할 수 있습니다.\n\n## 가이드로 연방화\n\n세계적인 인터넷과 그 개발에서 얻을 수 있는 교훈을 고려해봅시다. 웹은 모든 정보를 일관되고 구조적으로 조직화하기 위한 중앙 기관이나 카탈로그 없이 운영됩니다.\n\n예전에야후는 웹의 정보를 구조화하는 데 중앙에서 분류 체계를 작성하고 정리하는 것이 옳은 방식이라고 믿었습니다. 그러나 구글은 분산된 자동화 검색 기능으로 월드 와이드 웹의 정보를 보다 획기적으로 처리하여 더 큰 성공을 거두었습니다. 구글의 방식은 작업을 분배하고 자동화를 통해 아래에서 위로 데이터를 합치는 것을 지원했기 때문에 더욱 효과적이었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n웹의 비구조화된 다양한 정보는 대기업의 명확성 부족을 반영하기도 하지만, 우리는 여전히 조직 내에서 선제적으로 행동할 기회가 있습니다. 그러나 Google 검색 엔진과 같은 하위 검색 기능은 마술처럼 누락된 메타데이터를 생성하거나 일관성 없는 데이터를 일관성 있게 만들어 줄 수 없습니다. 따라서 데이터가 일관되게 정의되고 비즈니스 컨텍스트를 위한 메타데이터가 데이터 제품에 통합되도록 선제적으로 행동하고 보장하는 것이 중요합니다.\n\n비즈니스 도메인에서 정보 다양성을 유도하여 통합된 하나로 집계할 수 있도록 해야 합니다. 데이터 메쉬는 다중 비즈니스 도메인 경계를 가로지르는 폴리세미에 대해 말하지만 중앙에서 모델링해야 하는 점이 모호합니다. 이를 구체화하고 중앙 집중화와 분산화 사이의 적절한 균형을 이룰 수 있는 모델링이 어떻게 구현될 수 있는지 살펴봅시다.\n\n프로세스 월드(운영 시스템)에서는 두 가지 접근 방식이 나왔습니다. 이전에 생성된 정보의 고립된 영역(엔터프라이즈 응용 프로그램 통합 또는 EAI)을 다시 통합하거나 처음부터 이러한 영역이 만들어지는 것을 방지하는 것(도메인 주도 설계 또는 DDD)입니다. DDD는 \"Bounded Contexts\"의 전체 인터페이스를 모델링하기 위해 \"Context Maps\"를 참조합니다. EAI는 응용 프로그램 간 마찰없는 데이터 교환을 가능하게 하는 중심 요소인 \"Canonical Data Model\"을 참조합니다. 분석적인 세계에서 Ralph Kimball은 공유 비즈니스 차원을 가진 데이터 웨어하우스 버스 아키텍처를 소개하고, Dan Linstedt는 비즈니스를 세밀하게 대표하고 비즈니스 키를 사용하는 교차 도메인 허브의 필요성을 강조하는 데이터 보트 모델을 옹호했습니다. 모든 접근 방식은 정보 고립 영역의 급증을 방지하기 위해 비즈니스 개념의 핵심을 포착하는 기업 데이터 모델의 필요성을 강조합니다.\n\n이러한 포괄적 모델 개발의 불가능성에 대한 이야기가 많이 나오지만, 많은 프로젝트에서 실용적인 접근 방식이 성공을 거두었습니다. 중요한 것은 도메인별 데이터 모델을 통합할 온톨로지(프레임워크)를 수립하는 것입니다. DDD 용어로 말하면, \"Context Maps\"를 통해 Bounded Contexts의 명확하게 인식된 중첩 목록이 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다시 말해, 핵심 비즈니스 컨셉에 대한 지표를 제공하고 개요를 제공하는 고수준 비즈니스 모델입니다. 개별 도메인 모델의 상세한 디자인은 온톨로지의 범위를 벗어납니다. 온톨로지는 다수의 도메인 모델을 통합하고 비즈니스 도메인 전반에 걸쳐 유효한 공통점과 추상화를 설명합니다. 다의어를 정의하는 것뿐만 아니라 공통 비즈니스 용어, 객체 및 관계를 식별하는 데 중점을 둡니다.\n\n여러 테스트된 모델 패턴과 산업 모델이 솔리드한 기반이 됩니다. 이를 통해 바퀴를 재창조하는 것을 피할 수 있습니다. 그러나 최종 해결책으로서가 아닌 시작점으로 활용하세요. 공통 업계 모델이 아닌 고유한 비즈니스 요구사항을 대표하는 것이 중요합니다.\n\n때로는 기업 모델이 이미 존재하지만 프로젝트에서 활발하게 활용되지 않을 수 있습니다. 이러한 자료를 활용하여 특정 컨텍스트에 맞추도록 '활성화'를 시도하세요. 반면에, 존 지일즈는 \"냉장고 속 코끼리\"라는 책에서 \"어떻게 '충분한' 기업 데이터 모델을 직접 만드는가\"란 특정한 챕터에서 처음부터 새로운 온톨로지를 만드는 실용적인 접근 방법을 소개합니다.\n\n기존 모델을 활용하고 새로운 모델을 만드는 두 가지 접근법을 균형 있게 조화시키는 것은 많은 프로젝트에서 긍정적인 결과를 이끌어냈습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서 발견된 풍부한 조언들을 완전히 다 다루지는 못하지만, 두 가지 주요 아이디어를 강조하겠습니다:\n\n- 온톨로지에서 비즈니스 참여 활성화\n비즈니스 정보를 성공적으로 통합하기 위해서는, 여러 영역을 중심으로 조정할 수 있다는 오해를 피해야 합니다. 그러나 중앙 감독 없이는 조정될 수 없습니다. 그리고 중앙화된 IT 데이터 모델러 팀은 종종 각각의 비즈니스 도전에 대한 구체적인 지식이 부족할 수 있습니다. 따라서, 비즈니스 전문가들을 참여시키는 것이 중요합니다.\n연합 접근 방식을 유지하기 위해서, 모델링 팀은 IT 데이터 모델링 전문가들이 중재하는 모든 비즈니스 영역의 전문가들을 포함해야 합니다. 그 목표는 비즈니스에 대한 통합적이고 고수준의 이해를 달성하는 것입니다. 기억하세요, 데이터 모델링은 근본적으로 비즈니스 모델링이라는 것을.\n\n- 고수준 유지 및 세부 사항 연방화\nOntology는 고수준의 공통점에 관한 것입니다. 핵심 개체 및 관계를 작업하기 위해 추상 모델링 패턴을 사용하세요, 예를 들어 다음과 같은 것들.\n\n| 주체 \u0026 역할: 비즈니스 역할을 수행하는 개인 또는 조직 (예: 고객, 에이전트, 공급업체).\n| 위치: 지리적 위치, 건물 또는 지역.\n| 이벤트: 사고와 같은 중요한 이벤트 또는 '애플리케이션이 생성됨'과 같은 루틴 이벤트.\n| 문서: 물리적 또는 전자 문서, 계약서 또는 신분증의 스캔 이미지, 또는 모든 유형의 데이터 파일을 포함합니다.\n| 합의: 종종 형식적으로 문서화된 당사자 간의 계약.\n| 계정: 레코드의 일반적인 표현.\n| 작업: 계획된 또는 실제 작업 항목.\n| 자원 / 자산: 회사 건물, 컴퓨터 또는 차량과 같은 자산.\n| 제품: 고객에게 제공되는 상품 및 서비스.\n\n- 이러한 모델링 패턴은 다음과 같은 관계를 가지고 있습니다.\n제품 -` 주체가 활용하는 -` 합의를 체결한 -` 계정을 용이하게 하는 합의 -` 위치에 유지된 -` 자원의 장소이고, 그렇게하여야 된다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n희망을 통해 일반적인 개념을 제공했기를 바랍니다 — 참조 도서에서 더 자세하고 유용한 조언을 찾을 수 있을 거에요. 일반적인 패턴은 비즈니스 개념을 설명하는 상향식 프레임워크를 제공하여 다양한 영역간의 이해를 통합하는 데 도움이 됩니다. 자세한 비즈니스 도메인 모델은 상향식으로 개발된 상위 온톨로지에 명시적으로 연결되는 분산 도메인 팀에 의해 아래에서 위로 보완되어야 합니다. 이는 다양한 관점을 조화시키기 위한 강력한 토론 없이는 성공할 수 없는 창의적인 과정입니다.\n\n도메인 주도 설계는 Shared Kernel, Customer/Supplier Dev Teams, Conformist, Anticorruption Layer, Separate Ways, Open Host Service, 또는 Published Language과 같은 이 과정을 중재하는 실용적인 패턴을 제공합니다. 많은 용어와 설명이 주로 기능적 통합으로부터 비롯되었다 하더라도, 그 원칙들은 데이터 관리와 모델링에도 적용됩니다.\n\n전반적으로, 모든 데이터 생산자는 상세한 비즈니스 도메인 데이터 모델을 온톨로지에 매핑해야 합니다. 이 정보는 데이터 메시에 발행된 데이터 제품에 캡슐화된 메타데이터로 제공됩니다. 모든 가능한 데이터 제품의 메타데이터에서 파생된 데이터 메시는 언제든지 소비자를 위한 최신의 기업용 데이터 모델을 제공할 수 있습니다.\n\n# 참여를 활발하게 하는 동기부여\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 메시 내에서 효율적인 거버넌스는 참여자들이 데이터 기여를 온톨로지에 맞추도록 장려하며, 기업 전체의 이익과 개별 또는 부서별 목표를 균형 있게 조율합니다.\n\n기업 당사자들이 적극 참여하도록 어떻게 동기부여할 수 있을까요?\n\n## 커뮤니티 및 오픈 소스 원칙\n\n오픈 소스 운동에서 배울 점이 있습니다. 오픈 소스 운동의 철학에 부합하여, 데이터 모델의 특정 부분의 소유권이 아닌 일관된 전체적인 협력에 대한 작업이 중요합니다. 협업은 정보를 공유하는 것이며, 따라서 데이터 모델에 대한 모든 변경 사항은 시간이 지남에 따라 공개되고 문서화되어야 합니다. 거버넌스 프로세스는 이러한 공유를 가장 간단하고 보상적으로 만들어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오픈 소스 개발에서는 Git과 같은 코드 저장소를 활용하여 배포 및 업데이트 작업을 투명하게 처리하고 코드의 전체 내용을 분산하여 업데이트할 수 있습니다. 개발자들은 제품 상태에 대한 포괄적인 개요를 언제든 확인할 수 있습니다. 풀 리퀘스트를 사용하면 코드 변경에 대한 다양한 관점을 토론하고 중재할 수 있는 가치 있는 플랫폼을 구축할 수 있습니다.\n\n이와 유사하게, 데이터 메시를 활용하여 기업 데이터 모델의 분산 작업에 동일한 기능을 제공할 수 있습니다. 이 시리즈의 제2부에서는 데이터 자체에 메타데이터를 캡슐화하는 방법을 소개하여 전체 온톨로지에 맞게 자립적인 데이터 제품을 만들었습니다. 데이터 모델의 변경 사항은 이후 새로운 데이터 원자로 스트리밍되거나 변환으로 추가될 수 있습니다. 발행된 업데이트는 신속하게 기업 데이터 모델의 최신 상태를 생성하기 위해 소비될 수 있습니다. 데이터 모델에 대한 상충되는 변화나 다른 관점은 조정되고 토론될 수 있습니다. 온톨로지는 전반적 일관성을 보장하며 모든 변경 사항은 원본 데이터 모델로 귀결할 수 있는 투명하고 추적 가능한 형태로 남습니다. 이 접근법은 분산 데이터 모델링 팀 간의 협업을 촉진합니다.\n\n기업 내에서 데이터 커뮤니티를 형성하면 회사의 성공에 대한 공헌도 높일 수 있습니다. 데이터 메시는 가치 있는 데이터를 공유할 수 있도록 모두가 동참할 수 있는 협업 환경으로 작용합니다. 종종, 애플리케이션 소유자들은 생성하는 데이터가 기업의 다른 맥락에서 실질적인 추가 가치를 창출할 수 있다는 사실을 충분히 인식하지 못합니다. 이러한 데이터 제품은 온톨로지와 조화를 이룬 경우, 생산자가 자세한 응용 프로그램을 이해하지 않고도 다른 비즈니스 맥락에서 원활하게 활용할 수 있습니다. 개별 데이터 제품마다 이중 데이터 계약을 작성하는 대신 애플리케이션 소유자는 기업과 단일 가상 계약을 체결합니다. 이를 통해 제품 소유자의 데이터를 발행하고 다양한 비즈니스 영역에서 활용하여 전반적인 가치와 효율성을 향상시킬 수 있습니다.\n\n## 데이터 제품 시장\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알truistic 동기 외에도, 시장 기반 메커니즘은 기업 내 분산 모델링 노력에 참여를 동기부여할 수 있습니다. 이러한 메커니즘은 데이터 제품을 위한 고객을 찾는 데 도움을 주는 것뿐만 아니라 데이터의 명확한 정의와 구조화를 장려합니다.\n\n참여 동기:\n\n- 생산자와 고객에게 부가 가치 제공\n생산자는 기업 내에서 자신의 응용 프로그램의 중요성과 가시성을 높여 혜택을 얻습니다. 반면에 고객은 자신의 데이터 요구를 효율적으로 충족시켜주는 것으로, 생산자와 직접 소통할 수 있는 명확한 개요를 얻을 수 있습니다.\n- 참여를 간소화하고 민주화\n프로세스를 간소화하기 위해 자동화된, 셀프 서비스 데이터 패브릭을 구현합니다. 정확한 정보 매핑에 대한 긴 조정은 데이터 제품을 빠르게 출시하고 싶은 생산자들에게 장벽이 될 수 있습니다. 이 프로세스를 간소화함으로써 참여를 장려하여 지각된 장벽을 줄입니다.\n\n마켓플레이스에 제공되는 데이터 제품의 경우, 시장 메커니즘을 활용하여 데이터 제품의 가치를 결정할 수 있습니다. 본문에서 상세한 가치 평가 방법에 대해 다룰 수 없지만, 사용자 사용량을 추적하고 평가 및 리뷰와 같은 소비자 피드백을 가능하게 함으로써 간단하면서도 효과적인 접근 방법이 있습니다. 이러한 메커니즘들은 참여를 장려할 뿐만 아니라 데이터 제품의 품질을 평가하고 개선하기 위한 가치 있는 통찰을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기업 데이터 모델을 만드는 것이 어려운 일로 여겨졌지만, 이제는 고품질 데이터 제품을 위한 자체 지속 경쟁으로 변모했습니다.\n\n다음은 데이터 메쉬에서 '연방형 컴퓨터 관리'의 개선 내용을 요약한 것입니다:\n\n- 거버넌스 프로세스를 접근하기 쉽고, 마찰이 적도록 만듭니다.\n- 비즈니스 도메인 팀에 데이터 모델링 활동을 연방화하지만, 프레임워크로서의 안내적 온톨로지를 제공합니다.\n- 데이터 모델에 대한 협업 작업을 허용하고, 변경 사항을 모두 데이터 메쉬를 통해 투명하게 게시합니다.\n- 데이터 메쉬를 통해 데이터 시장을 제품으로서 활용할 수 있게 배치하여 투자 수익률(ROI)을 얻을 수 있도록 합니다.\n\n이로써 데이터 메쉬의 도전과 해결책에 관한 세 번의 시리즈가 마무리됩니다. 종합적으로, Zhamak Dehghani의 정의에 따라 데이터 메쉬를 개선하기 위해 다음과 같은 조언을 제공했습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n부분 1\n\n- 데이터 메시의 운영 및 분석 데이터 평면 사이의 큰 차이를 줄입니다.\n\n부분 2\n\n- '데이터 제품'을 스마트 데이터 구조로 구현하여 전체 데이터 계보와 비즈니스 컨텍스트를 포함하도록 하고, 자체 생성이 가능한 새로운 '슈퍼 객체'로 사용하지 않도록 합니다.\n- 데이터 메시의 하부에 데이터 제품 인프라를 제공하여 느슨하게 결합된 운영 및 분석 시스템 간에 데이터 제품을 원활하게 교환할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파트 3\n\n- '활성화'하면서 연방화된 기업 데이터 모델링을 위한 거버넌스 프로세스를 활성화하고, 쉽게 접근 가능한 서비스를 통해 개방형 협업 활성화.\n- 데이터 메쉬를 시장에서 데이터 제품으로 사용되는 하부 구조로 배치하여 참여자에게 투자 수익을 보장합니다.","ogImage":{"url":"/assets/img/2024-06-22-ChallengesandSolutionsinDataMeshPart3_0.png"},"coverImage":"/assets/img/2024-06-22-ChallengesandSolutionsinDataMeshPart3_0.png","tag":["Tech"],"readingTime":8},{"title":" Docker로 Apache Spark와 Minio를 사용하여 Apache Iceberg 활용하는 방법","description":"","date":"2024-06-22 17:09","slug":"2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker","content":"\n\n\u003cimg src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png\" /\u003e\n\n이 게시물에서는 Apache Iceberg(데이터 테이블 형식), 분산 처리 엔진인 Apache Spark, 고성능 객체 저장 솔루션인 Minio를 결합하는 방법에 대해 탐구합니다. 주요 초점은 이러한 구성 요소를 Docker 컨테이너 내에 설정하여 통제된 환경에서 격리된 환경을 제공하는 데 있습니다. 이러한 기술을 결합하여 ACID 트랜잭션, 스키마 진화 및 Minio 내에서 효율적인 데이터 파티셔닝을 통한 효율적인 데이터 관리 기능을 확보할 수 있습니다.\n\n# 아키텍처 개요\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*gM-qHwR03S6IEh32mgJpjA.gif\" /\u003e  \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 컴포넌트 개요 이론적 개요📖\n\n실제 구현에 들어가기 전에, 사용할 기술들인 Apache Iceberg, Apache Spark, 그리고 Minio에 대한 간단한 이론적 개요를 살펴보겠습니다:\n\n![이미지](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_1.png)\n\n- Apache Iceberg: 차세대 데이터 테이블 형식\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터 레이크 및 데이터 웨어하우스용으로 설계됨: Iceberg는 대규모 데이터의 복잡성을 처리하도록 특별히 구축되었습니다. 데이터 레이크와 데이터 웨어하우스 내에서 강력한 데이터 관리 기능을 제공하면서 분석 워크로드의 성능을 최적화합니다.\n- ACID 트랜잭션: Iceberg는 ACID(원자성, 일관성, 고립성, 지속성) 트랜잭션을 지원하여 동시 쓰기 시나리오에서도 데이터 무결성과 일관성을 보장합니다. 이는 동일한 데이터에 여러 애플리케이션이나 프로세스가 동시에 쓰기를 하는 경우에 데이터 유효성을 유지하고 데이터 손상을 방지하는 데 중요합니다.\n- 스키마 진화: Iceberg는 기존 데이터 형식과 달리 데이터 테이블의 스키마를 손실 없이 진화시킬 수 있습니다. 이를 통해 분석 요구 사항이나 데이터 소스가 시간이 지남에 따라 변경될 때 데이터 구조를 조정할 수 있습니다. 기존 데이터를 다시 작성하지 않고도 열을 추가, 제거 또는 수정할 수 있습니다.\n- 타임 트래블 쿼리: Iceberg는 타임 트래블 쿼리를 지원하여 언제든지 과거 데이터 스냅샷에 액세스할 수 있습니다. 이는 변경 사항 감사, 분석 파이프라인 디버깅 및 역사적 분석에 매우 유용합니다. Iceberg는 데이터 버전을 추적하여 필요한 경우 특정 버전의 테이블을 검색할 수 있습니다.\n- 효율적인 파티셔닝: Iceberg는 특정 열을 기준으로 데이터 테이블을 파티션하여 읽기 성능과 데이터 관리를 최적화합니다. 해당 파티션 값에 따라 데이터 파일을 지능적으로 저장하므로 Spark가 특정 쿼리에 대한 관련 데이터만 효율적으로 스캔할 수 있습니다. 이는 쿼리 속도를 크게 향상시킵니다.\n- 데이터 조직 및 압축: Iceberg는 자동으로 Parquet 또는 ORC와 같은 효율적인 파일 형식으로 데이터를 구성하여 효율적인 데이터 압축과 열 액세스를 용이하게 합니다. 또한 데이터 압축을 수행하여 저장 공간을 최소화하고 시간이 지남에 따라 읽기 성능을 향상시킵니다.\n\n2. Apache Spark: 통합 분석 엔진\n\n![이미지](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_2.png)\n\n- 대규모 데이터 처리: Apache Spark는 대규모 데이터셋을 클러스터로 효과적으로 처리하는 강력한 오픈 소스 분산 처리 엔진입니다. 관계형 데이터베이스, NoSQL 데이터베이스, CSV 파일 및 Minio와 같은 객체 저장소를 포함한 다양한 데이터 원본을 지원합니다.\n- 인메모리 처리: Spark는 인메모리 계산을 활용하여 성능을 향상시키며, 자주 액세스되는 데이터를 디스크 기반 솔루션에 비해 더 빠른 처리를 위해 메모리에 유지합니다.\n- 구조적, 반구조적 및 비구조적 데이터: Spark는 CSV, JSON과 같은 구조적 데이터, XML과 같은 반구조적 데이터, 텍스트와 같은 비구조적 데이터를 포함한 다양한 데이터 형식을 처리할 수 있습니다. 이러한 유연성으로 인해 현대 데이터 생태계에서 다양한 데이터 유형을 다루는 데 이상적입니다.\n- 머신러닝 및 스트림 처리: 전통적인 분석 이상으로 Spark는 머신러닝 파이프라인 및 실시간 데이터 처리까지 확장됩니다. Spark는 TensorFlow, PyTorch와 같은 인기있는 머신러닝 라이브러리를 효율적인 모델 훈련 및 배포를 위해 통합합니다. 또한 Apache Flink와 같은 스트림 처리 프레임워크를 지원하여 거의 실시간 데이터 분석을 수행할 수 있습니다.\n- Iceberg와의 원활한 통합: Spark는 네이티브 Iceberg 지원을 제공하여 Spark 애플리케이션에서 Iceberg 테이블을 직접 읽고 쓰기 및 쿼리할 수 있습니다. 이를 통해 Spark 워크플로우 내에서 데이터 관리를 간편하게 할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. Minio: 고성능 객체 저장 서버\n\n![Minio Image](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_3.png)\n\n- 오픈소스 및 비용 효율적: Minio는 무료 및 오픈소스 객체 저장 솔루션으로, 대형 클라우드 공급업체가 제공하는 프로프리어터리 객체 저장 서비스에 대안으로 경제적입니다. 자체 데이터 저장 인프라를 관리하고 데이터 레이크 또는 데이터 웨어하우스 내에서 해당 기능을 활용할 수 있습니다.\n- 확장성 및 성능: Minio는 확장성을 고려하여 제작되었습니다. 데이터 저장 필요가 증가함에 따라 더 많은 노드를 Minio 클러스터에 쉽게 추가할 수 있어 증가하는 데이터 양을 처리할 수 있습니다. 또한 효율적인 데이터 액세스를 제공하여 Spark 애플리케이션에 대한 효율적인 데이터 액세스를 제공합니다.\n- S3 호환성: Minio는 Amazon S3와 API 호환성이 있어서 S3와 작동하는 기존 도구 및 애플리케이션과의 원활한 통합이 가능합니다. 이는 전통적인 클라우드 객체 저장에서 더 경제적인 온프레미스 솔루션으로의 원활한 전환을 용이하게 합니다.\n- 내구성 및 신뢰성: Minio는 데이터 중복 및 복제 메커니즘을 제공하여 데이터가 장애에 대비하여 보호되도록 합니다. 다중 노드 또는 저장 장치 간의 데이터 복제를 구성하여 하드웨어 문제 발생 시 데이터 손실 위험을 최소화할 수 있습니다.\n\n4. Docker 통합\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_4.png\" /\u003e\n\n도커는 구성 요소의 실행을 격리하고 관리하는 데 사용할 수있는 컨테이너화 플랫폼을 제공합니다. 이 아키텍처에서 도커가 어떻게 적합한지 살펴보겠습니다:\n\n- **도커 이미지**: 각 서비스(Spark Master, Spark Worker 및 Minio)를 위한 도커 이미지를 만들 수 있습니다. 필요한 모든 종속성(Spark, Minio 이진 파일, Iceberg 라이브러리 등)을 포함하여 일관된 환경을 제공하고 다양한 기기에 배포를 단순화합니다.\n- **도커 콤포즈**: 도커 콤포즈와 같은 도구를 사용하여 모든 서비스의 구성 및 배포를 함께 관리할 수 있습니다. 이 도구는 서비스, 종속성 및 환경 변수를 단일 YAML 파일에 정의하여 전체 환경 설정 프로세스를 간편화합니다.\n\n도커를 활용하면 이동성이 뛰어나고 격리된 개발 또는 프로덕션 환경을 구축할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 도커 컴포즈 구성\n\n아래는 Minio, Spark 마스터 및 Spark 워커 서비스를 설정하는 Docker Compose 파일입니다.\n\n```js\nversion: '3.9'\nservices:\n  minio:\n    image: minio/minio\n    container_name: minio\n    environment:\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    command: server /data --console-address \":9001\"\n    volumes:\n      - minio_data:/data\n\nspark-master:\n    image: bitnami/spark:latest\n    container_name: spark-master-minio-iceberg\n    environment:\n      - SPARK_MODE=master\n      - SPARK_SUBMIT_ARGS=--packages org.apache.iceberg:iceberg-spark3-runtime:0.12.0\n    ports:\n      - \"7077:7077\"\n      - \"8080:8080\"\n  spark-worker:\n    image: bitnami/spark:latest\n    container_name: spark-worker-minio-iceberg\n    environment:\n      - SPARK_MODE=worker\n      - SPARK_MASTER_URL=spark://spark-master:7077\n      - SPARK_SUBMIT_ARGS=--packages org.apache.iceberg:iceberg-spark3-runtime:0.12.0\n    depends_on:\n      - spark-master\n    ports:\n      - \"8081:8081\"\nvolumes:\n  minio_data:\n```\n\n# Docker Compose 파일 설명\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Minio 서비스: Minio 서버를 실행하여 포트 9000(API)과 9001(콘솔)을 노출합니다. Minio 데이터는 minio_data라는 이름의 Docker 볼륨에 저장됩니다.\n- Spark Master 서비스: Delta Lake 및 Iceberg를 위한 패키지가 포함된 Spark 마스터 노드를 실행합니다. Spark UI 및 마스터 통신을 위한 포트 7077 및 8080이 노출됩니다.\n- Spark Worker 서비스: Spark 워커 노드를 실행하고 Spark 마스터에 연결됩니다. Delta Lake 및 Iceberg를 위한 패키지가 포함되어 있습니다.\n\n# 이미지 빌드\n\n다음 명령을 실행하여\n\n```js\ndocker-compose up -d\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 서비스가 정상적으로 작동 중이거나 문제가 발생한 경우 알림이 표시됩니다 (문제가 발생하지 않기를 희망합니다 😄)\n\n![Image 5](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_5.png)\n\n![Image 6](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_6.png)\n\n먼저 http://localhost:9001/browser를 방문하여 아래 이미지를 확인해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_7.png\" /\u003e\n\n모든 서비스가 실행되면 Apache Iceberg를 사용하여 Apache Spark 및 Minio로 읽기와 쓰기를 하는 간단한 데이터 파이프라인을 생성해 봅시다.\n\n# Spark 작업 설정 및 실행하기\n\n다음의 Python 코드는 Minio와 상호 작용하고 Iceberg를 사용하여 Spark를 사용하는 방법을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Minio 클라이언트 초기화\n\n제공된 자격 증명으로 Minio 서버에 연결합니다.\n\n```js\nfrom minio import Minio\nclient = Minio(\n    \"127.0.0.1:9000\",\n    access_key=\"your_admin_name_account\",\n    secret_key=\"your_password\",\n    secure=False\n)\n```\n\n## 설명\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Minio 초기화: 이 코드는 Minio 클라이언트를 초기화하여 127.0.0.1의 9000 포트에서 실행 중인 Minio 서버에 연결합니다. 액세스 키와 시크릿 키로 minioadmin을 사용하고, SSL을 사용하지 않는 연결이므로 secure를 False로 설정합니다.\n\n# 버킷 관리\n\n버킷이 존재하는지 확인하고, 존재하지 않으면 생성합니다.\n\n```js\nminio_bucket = \"my-first-bucket\"\nfound = client.bucket_exists(minio_bucket)\nif not found:\n    client.make_bucket(minio_bucket)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 설명\n\n- Bucket 존재 여부 확인: bucket_exists 메서드는 Minio에 my-first-bucket이라는 이름의 버킷이 이미 있는지 확인합니다.\n- Bucket 생성: 버킷이 존재하지 않는 경우 make_bucket 메서드를 사용하여 my-first-bucket이라는 새 버킷을 만듭니다.\n\n# 파일 업로드\n\nMinio에 지정된 버킷에 CSV 파일을 업로드합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n목적지_파일 = 'data.csv'\n원본_파일 = './data/data.csv'  # 프로젝트 폴더에이 파일이 존재하는지 확인하세요\nclient.fput_object(minio_bucket, 목적지_파일, 원본_파일)\r\n```\n\n## 설명\n\n- 파일 업로드: fput_object 메서드는 로컬 파일 ./data/data.csv를 Minio 버킷 my-first-bucket으로 객체 이름이 data.csv인 파일로 업로드합니다.\n\n## 코드 실행 후🎦\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*rZUjqaNCls7_73eojd2RQw.gif)\n\n# SparkSession 구성\n\nSpark를 Iceberg 및 Minio를 스토리지 백엔드로 사용하도록 구성합니다.\n\n```js\nfrom pyspark.sql import SparkSession\niceberg_builder = SparkSession.builder \\\n    .appName(\"iceberg-spark-minio-example\") \\\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.iceberg:iceberg-hive-runtime:1.5.0\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n    .config(\"spark.sql.catalog.spark_catalog.warehouse\", f\"s3a://{minio_bucket}/iceberg_data/\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"your_admin_name_account\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"your_pasword\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .enableHiveSupport()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 설명\n\n- SparkSession 빌더: Iceberg 및 Minio를 사용하기 위해 구성된 Spark 세션 빌더를 생성합니다.\n- 애플리케이션 이름: \"iceberg-spark-minio-example\"로 애플리케이션 이름을 설정합니다.\n- JAR 패키지: Hadoop AWS 및 Iceberg에 필요한 JAR를 포함합니다.\n- Spark 확장: Spark SQL을 위해 Iceberg 확장 기능을 활성화합니다.\n- 카탈로그 구성: 카탈로그 유형을 Hadoop으로 구성하고, 웨어하우스 위치를 Minio 버킷을 가리키는 S3 경로로 설정합니다.\n- Minio 자격 증명: Minio의 액세스 및 시크릿 키를 설정합니다.\n- 엔드포인트 구성: S3 엔드포인트를 로컬 Minio 서버를 가리키도록 구성하고, S3 URI에 대한 경로 스타일 액세스를 활성화합니다.\n\n# Iceberg용 SparkSession 빌드\n\nIceberg 테이블과 상호 작용하기 위한 Spark 세션을 빌드합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\niceberg_spark = iceberg_builder.getOrCreate()\n```\n\n## 설명\n\n- SparkSession 생성: getOrCreate 메서드는 지정된 구성으로 Spark 세션을 초기화합니다.\n\n# 데이터 로드\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스파크 데이터프레임으로 CSV 파일을 읽습니다.\n\n```js\ndf = iceberg_spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(source_file)\n```\n\n## 설명\n\n- CSV 파일 읽기: ./data/data.csv 파일을 스파크 데이터프레임으로 로드합니다. header 옵션은 첫 번째 행을 열 이름으로 사용하도록 설정되었고, inferSchema 옵션은 데이터 유형을 자동으로 추정하도록 설정되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 아이스버그 테이블 위치 정의\n\n미니오 내 아이스버그 테이블의 위치를 지정합니다.\n\n```js\niceberg_table_location = f\"s3a://{minio_bucket}/iceberg_data/default\"\n```\n\n## 설명\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Iceberg Table 위치: Minio에 Iceberg 테이블 데이터를 저장하는 데 사용되는 S3 경로를 정의합니다. 경로는 버킷 이름과 하위 디렉터리 iceberg_data/default을 사용하여 구성됩니다.\n\n# Iceberg 테이블에 쓰기\n\nDataFrame 데이터를 Minio의 Iceberg 테이블에 씁니다.\n\n```js\ndf.write \\\n    .format(\"iceberg\") \\\n    .mode(\"append\") \\\n    .saveAsTable(\"iceberg_table_name\")  # Iceberg 테이블의 이름\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 설명\n\n- DataFrame 작성: Iceberg 형식을 사용하여 append 모드로 iceberg_table_name이라는 Iceberg 테이블에 DataFrame을 작성합니다.\n\n## 코드 실행 후🎦\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*oUTGnwgU4yk2nG31-kuIWA.gif\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Iceberg 테이블에서 읽기\n\nIceberg 테이블에서 데이터를 읽어 스키마와 몇 가지 샘플 레코드를 표시합니다.\n\n```js\niceberg_df = iceberg_spark.read.format(\"iceberg\").load(f\"{iceberg_table_location}/iceberg_table_name\")\niceberg_df.printSchema()\niceberg_df.show()\n```\n\n## 설명\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 아이스버그 테이블 읽기: 아이스버그 테이블에서 데이터를 읽어와 DataFrame에로드합니다.\n- 스키마 출력: DataFrame의 스키마를 표시합니다.\n- 데이터 표시: DataFrame에서 몇 가지 샘플 레코드를 표시합니다.\n\n## 코드 실행 후🎦\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*qnK1mFZ6ecqTXBGCtEWi0w.gif\" /\u003e\n\n# 전체 코드🖥️\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom minio import Minio\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\n# Minio 클라이언트 및 버킷 생성\nclient = Minio(\n    \"127.0.0.1:9000\",\n    access_key=\"minioadmin\",\n    secret_key=\"minioadmin\",\n    secure=False\n)\n\nminio_bucket = \"my-first-bucket\"\n\nfound = client.bucket_exists(minio_bucket)\nif not found:\n    client.make_bucket(minio_bucket)\n\ndestination_file = 'data.csv'\nsource_file = './data/data.csv' ## 프로젝트 폴더 내에 파일이 있어야 함\n\n# Minio에 파일 업로드\nclient.fput_object(minio_bucket, destination_file, source_file,)\n\n# Iceberg와 Minio 설정을 사용한 SparkSession 빌더 생성\niceberg_builder = SparkSession.builder \\\n    .appName(\"iceberg-concurrent-write-isolation-test\") \\\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.iceberg:iceberg-hive-runtime:1.5.0\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n    .config(\"spark.sql.catalog.spark_catalog.warehouse\", f\"s3a://{minio_bucket}/iceberg_data/\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .enableHiveSupport()\n\n# Iceberg를 위한 SparkSession 생성\niceberg_spark = iceberg_builder.getOrCreate()\n\n# Iceberg 테이블을 Minio에 쓰기\ndf.write \\\n    .format(\"iceberg\") \\\n    .mode(\"append\") \\\n    .saveAsTable(\"iceberg_table_name\")  # Iceberg 테이블 이름\n\n# Iceberg 테이블에서 데이터 읽기\niceberg_df = iceberg_spark.read.format(\"iceberg\").load(f\"{iceberg_table_location}/iceberg_table_name\")\n\n# 데이터프레임 스키마 및 데이터 출력\nprint(\"**************************\")\nprint(\"This the Dataframe schema \")\nprint(\"**************************\")\niceberg_df.printSchema()\n\nprint(\"**************************\")\nprint(\"******Dataframe Data******\")\nprint(\"**************************\")\niceberg_df.show()\n```\n\n# GitHub\n\n프로젝트 링크\n\n# Summary\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 설정을 사용하면 Spark 및 Iceberg로 대규모 데이터를 효율적으로 관리하고 처리할 수 있고, 확장 가능한 객체 저장소로 Minio를 사용할 수 있습니다. Docker를 활용하면 이러한 구성 요소의 배포와 관리가 간편하고 재현 가능해집니다.\n\n즐거운 학습이 되길 바래요 😉","ogImage":{"url":"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png"},"coverImage":"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png","tag":["Tech"],"readingTime":14}],"page":"38","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"38"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>