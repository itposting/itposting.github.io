<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/29" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/29" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="2024년에 머신러닝을 처음 시작한다면 이렇게 배우세요 최고의 학습 방법과 전략" href="/post/2024-06-22-HowIdlearnMLin2024IfICouldStartOver"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="2024년에 머신러닝을 처음 시작한다면 이렇게 배우세요 최고의 학습 방법과 전략" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-HowIdlearnMLin2024IfICouldStartOver_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="2024년에 머신러닝을 처음 시작한다면 이렇게 배우세요 최고의 학습 방법과 전략" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">2024년에 머신러닝을 처음 시작한다면 이렇게 배우세요 최고의 학습 방법과 전략</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AI 대거들 양자화하기 대형 AI 모델 양자화의 방법 및 장점" href="/post/2024-06-22-QuantizingtheAIColossi"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AI 대거들 양자화하기 대형 AI 모델 양자화의 방법 및 장점" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-QuantizingtheAIColossi_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AI 대거들 양자화하기 대형 AI 모델 양자화의 방법 및 장점" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">AI 대거들 양자화하기 대형 AI 모델 양자화의 방법 및 장점</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">66<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="집은 항상 장소가 아니다" href="/post/2024-06-22-homeisnotalwaysaplace"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="집은 항상 장소가 아니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-homeisnotalwaysaplace_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="집은 항상 장소가 아니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">집은 항상 장소가 아니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="고급 RAG 정보 검색 강화 생성 시스템 향상시키는 고급 기법 구현 방법" href="/post/2024-06-22-AdvancedRAGImplementingAdvancedTechniquestoEnhanceRetrieval-AugmentedGenerationSystems"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="고급 RAG 정보 검색 강화 생성 시스템 향상시키는 고급 기법 구현 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-AdvancedRAGImplementingAdvancedTechniquestoEnhanceRetrieval-AugmentedGenerationSystems_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="고급 RAG 정보 검색 강화 생성 시스템 향상시키는 고급 기법 구현 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">고급 RAG 정보 검색 강화 생성 시스템 향상시키는 고급 기법 구현 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">19<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ML 엔지니어를 위한 저장 공간 부족 문제 피하는 방법 가이드라인" href="/post/2024-06-22-HowtoAvoidtheOut-of-SpaceProblemAGuidelineforanMLEngineer"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ML 엔지니어를 위한 저장 공간 부족 문제 피하는 방법 가이드라인" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-HowtoAvoidtheOut-of-SpaceProblemAGuidelineforanMLEngineer_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ML 엔지니어를 위한 저장 공간 부족 문제 피하는 방법 가이드라인" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">ML 엔지니어를 위한 저장 공간 부족 문제 피하는 방법 가이드라인</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="깊게 배우는 딥러닝 일러스트, Part 5 LSTM Long Short-Term Memory 이해하기" href="/post/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="깊게 배우는 딥러닝 일러스트, Part 5 LSTM Long Short-Term Memory 이해하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="깊게 배우는 딥러닝 일러스트, Part 5 LSTM Long Short-Term Memory 이해하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">깊게 배우는 딥러닝 일러스트, Part 5 LSTM Long Short-Term Memory 이해하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="존재와 애니메이션 물질 세계를 움직이는 힘" href="/post/2024-06-22-BeingsandAnimatedMatter"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="존재와 애니메이션 물질 세계를 움직이는 힘" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-BeingsandAnimatedMatter_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="존재와 애니메이션 물질 세계를 움직이는 힘" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">존재와 애니메이션 물질 세계를 움직이는 힘</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="비판적 사고 AI를 뛰어넘는 보이지 않는 위협" href="/post/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="비판적 사고 AI를 뛰어넘는 보이지 않는 위협" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="비판적 사고 AI를 뛰어넘는 보이지 않는 위협" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">비판적 사고 AI를 뛰어넘는 보이지 않는 위협</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="강화 학습의 비밀을 풀다 Actor-Critic 초보자 가이드" href="/post/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="강화 학습의 비밀을 풀다 Actor-Critic 초보자 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="강화 학습의 비밀을 풀다 Actor-Critic 초보자 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">강화 학습의 비밀을 풀다 Actor-Critic 초보자 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="로봇은 더 똑똑해지고 있을까 로봇 공학에서 인식에 대한 대화" href="/post/2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="로봇은 더 똑똑해지고 있을까 로봇 공학에서 인식에 대한 대화" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="로봇은 더 똑똑해지고 있을까 로봇 공학에서 인식에 대한 대화" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">로봇은 더 똑똑해지고 있을까 로봇 공학에서 인식에 대한 대화</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/21">21</a><a class="link" href="/posts/22">22</a><a class="link" href="/posts/23">23</a><a class="link" href="/posts/24">24</a><a class="link" href="/posts/25">25</a><a class="link" href="/posts/26">26</a><a class="link" href="/posts/27">27</a><a class="link" href="/posts/28">28</a><a class="link posts_-active__YVJEi" href="/posts/29">29</a><a class="link" href="/posts/30">30</a><a class="link" href="/posts/31">31</a><a class="link" href="/posts/32">32</a><a class="link" href="/posts/33">33</a><a class="link" href="/posts/34">34</a><a class="link" href="/posts/35">35</a><a class="link" href="/posts/36">36</a><a class="link" href="/posts/37">37</a><a class="link" href="/posts/38">38</a><a class="link" href="/posts/39">39</a><a class="link" href="/posts/40">40</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"2024년에 머신러닝을 처음 시작한다면 이렇게 배우세요 최고의 학습 방법과 전략","description":"","date":"2024-06-22 20:10","slug":"2024-06-22-HowIdlearnMLin2024IfICouldStartOver","content":"\n\n\u003cimg src=\"/assets/img/2024-06-22-HowIdlearnMLin2024IfICouldStartOver_0.png\" /\u003e\n\n안녕하세요! 저는 자동차 산업 기업의 기계 학습 부서장이자 박사 후보입니다. 하지만 이 지점에 이르기까지 4년 이상이 걸렸어요.\n\n그래서 오늘은, 처음부터 다시 시작할 수 있다면 어떻게 기계 학습을 배울 것인지 6가지 주요 단계를 알려드리려고 해요.\n\n함께 시작해볼까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Python\n\n일반적으로 이러한 단계를 처리하는 엄격한 순서는 없지만, 처음부터 마지막 및 가장 중요한 단계로 넘어가는 것을 추천하지 않겠습니다.\n\n대신, Python의 기초부터 시작하는 것을 강력히 제안합니다.\n\nPython은 기계 학습 분야의 거의 모든 사람들에게 가장 많이 사용되는 프로그래밍 언어이며, 여기서 언급된 모든 후속 단계의 기초 역할을 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드 목록이나 사전과 같은 개념에 익숙하지 않은 초보자들이나 if-else 문이나 for 루프와 같은 기본적인 프로그래밍 구조를 아직 잘 익히지 않은 사람들에게 특히 유용합니다. 내 의견으로는, 리스트 내포나 클래스 상속과 같은 개념을 이해하는 것도 중요합니다.\n\n솔직히 말해서, 어디서부터 시작해야 할지 모르겠다면, 간단히 \"Python 튜토리얼\"이나 유튜브나 구글에서 강의를 찾아보고 시작해보세요. 무료로 제공되는 풍부한 우수한 자료들이 놀라울 정도로 많이 있지만, 중요한 점은 튜토리얼과 함께 코딩 실습을 꾸준히 해야 한다는 것을 기억해주세요.\n\n# 수학\n\nPython으로 ML에 뛰어들어 재미있게 시작해보세요. 하지만 아직 세부 사항에 깊이 빠지지는 마세요. 분명히 수학 부분은 결국 필요해지겠지만, 지금은 너무 걱정할 필요가 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가끔, \"어차피 팬시한 Python 라이브러리가 모든 작업을 대신 처리해 주니까 수학이 필요 없겠지!\"라고 생각할 수 있을 거예요. 그런데 사실 그게 어느 정도 맞아요. 하지만 대부분의 머신러닝 개념을 제대로 이해하려면 미적분학, 선형 대수학, 그리고 확률 같은 기본 수학 개념을 이해해야 해요.\n\n걱정하지 마세요! 진짜로 수학 천재일 필요는 없어요. 우리가 말하는 건 고등학교나 대학 초기에 다루는 수준의 수학이에요. 도함수가 무엇인지 이해하거나 행렬과 점곱에 대한 개념을 이해하는 정도면 충분해요.\n\n그리고 알고 계시나요? 멋진 무료 자료들이 정말 많아요. 코세스 [1, 2, 3,]나 Khan Academy 같은 멋진 사이트들 말이에요. 아 그리고 Brilliant.org는 기초를 배우기에 최고에요 (물론 후원 안 받았어요, 그냥 말이에요 🥲).\n\n또는, 물론 대학에 가서 기초 수학 수업을 듣는 방법도 있죠. 요령을 선택해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내가 신경망을 위한 내 절대적인 즐겨찾기 수학 자료를 나중에 소개할게. 그런데 지금은 그냥 기억해둬, 네가 모든 수학 강좌를 서둘러 이수할 필요는 없어. 천천히 하거나 그렇지 않으면 마음을 먹지 않게 만들어 버리고 모든 재미를 뺏어갈 거야.\n\n나중에 어떤 수학 문제에 막혔다면? 그냥 구글에서 찾아보세요.\n\n기본을 확고히 다지고, 그런 다음 다음 멋진 부분으로 넘어가: ML 개발자 스택을 알아보는 것!\n\n# ML 개발자 스택\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n좋아, 파이썬 기본 사항을 알고 있군요. 그러면 다음 단계는 무엇일까요? 아직 시도하지 않은 경우, 주피터 노트북과 판다스, 넘파이, 맷플롯립과 같은 멋진 도구들을 활용해 보는 것이 좋을 거예요.\n\n넘파이는 행렬 또는 배열의 숫자를 계산하는 데 필수적인 라이브러리에요. 막 시작하기에는 좋은 곳이죠. 방금 배운 수학 기술을 적용하고 두 행렬을 곱하는 것이 얼마나 쉬운지 확인할 수 있어요.\n\n이어서 맷플롯립이 있습니다. 데이터를 멋지게 표현하는 데 관심이 있다면 필수적인 도구에요. 데이터와 그래프를 시각화해주어 작업 중인 수학이 더욱 실감 날 거예요. 그래프를 만드는 것이 재미있고 매우 유용하다고 생각하는 사람은 누구나 있을 거예요.\n\n이제 판다스에 대해 이야기해봐요. 판다스는 ML에서 자주 마주하게 될 테이블 형식의 데이터를 다루는 데 탁월한 라이브러리에요. 판다스를 사용하면 이러한 유형의 데이터를 손쉽게 조정하고 시각화할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 라이브러리들은 주피터 노트북과 함께 사용될 때 마치 빵과 버터처럼 필수적이에요. 여러분의 머신러닝 여행에 꼭 필요한 요소입니다.\n\n이 도구들에 뛰어들면서 파이썬과 머신러닝 기술을 실전에서 훈련할 수 있어요. 하지만 기억해주세요, 지금은 기초를 중심으로 몇 가지 튜토리얼을 따라가는 것이 중요해요. 실제 프로젝트에 도전할 때 이 라이브러리들을 보다 깊이 이해할 수 있을 거예요.\n\n도구들에 관한 이야기는 여기까지! 이론으로 돌아가서 진짜 주제에 대해 알아볼 차례에요: 머신러닝과 딥 러닝!\n\n# 머신러닝 배우기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지 잘 따라오셨죠? 이동속도를 유지하고 계셨다면, 시작 점과 투자한 시간에 따라 몇 주 동안 진행해 왔을 것입니다.\n\n이제 좀 더 헌신이 필요한 ML 코스들로 방향을 전환해 봅시다.\n\n먼저 앤드류 응의 머신 러닝 스페셜리제이션을 확인해야 합니다. 이것은 ML 코스들의 성배이자 많은 이에게 혁신을 가져다준 진정한 게임 체인저입니다.\n\n이 시리즈에서는 sci-kit learn과 TensorFlow와 같은 멋진 머신 러닝 프레임워크에 깊이 파고들 것입니다. 제가 약간 파이토치를 선호하는 편인데요 — 제 개인 취향일 뿐입니다. 하지만 멋진 점은, 한 프레임워크를 잘 이해하게 되면 다른 것을 배우는 것도 그리 어렵지 않습니다. 이 코스는 정말 가치 있는 골드이며, 놀랍게도 무료입니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 그저 초보자를 위한 일반적인 강좌가 아닙니다. 필수적인 머신러닝 개념들이 가득 담겨 있어요. 이런 내용들은 머신러닝 채용 면접을 통과하는 데 필요한 내용들입니다.\n\n아, 그리고 앞서 언급한 신경망을 위한 수학 자료에 대해 말씀 드렸죠? 앤드류의 강좌를 학습한 후, 안드레이 카파시의 YouTube Neural Network 시리즈를 확인해 보세요. 그는 NLP 모델을 완전히 처음부터 만들어가는 과정을 자세히 설명하고, 트랜스포머까지 모든 내용을 다루며, 역전파 뒤에 숨겨진 수학도 남깁니다. 정말 꼭 봐야 할 영상입니다!\n\n앤드류와 안드레이의 강좌를 통해 실용적인 노하우를 얻었다면, Deep Learning Specialization으로 레벨 업하세요. 이 과정은 신경망을 통해 더 실용적이며, Huggingface와 함께 작업할 기회가 주어집니다. 이 분야에서 두각을 나타내는 라이브러리이므로, 익숙해지는 것이 좋습니다.\n\nHuggingface에 더 궁금하다면, 특히 NLP 분야에서, Huggingface의 NLP 강좌가 좋은 선택일 것입니다. 이 강좌는 고급 NLP 주제에 더 깊이 파고들기에 완벽합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서 이것들이 내 추천 중 상위 몇 과목이야 — 너의 시간과 노력을 아주 값진 것으로 여기게 될 거야.\n\n# 어떻게 정말 잘 해내는지\n\n넌 여러 작은 프로젝트나 튜토리얼을 처리하며 확실한 진전을 이루었어. 하지만 이제는 소매를 걷어 올려 실제 프로젝트에 몸담아보는 시간이야. 여기서 학습 곡선이 급상승하며, 정말 재미있는 시간이 시작돼.\n\n일단, Kaggle에 들러봐. 그 곳은 모든 종류와 크기의 데이터 과학자들을 위한 놀이터 같은 곳이야. 영감을 유지하기 위해 쉬운 챌린지부터 시작해봐. 상금이 있는 더 치열한 챌린지로 곧 바로 뛰어들고 싶을 수도 있지만, 네 기대를 조절해야 해. 우승은 쉽지 않고 보통 상당한 계산 자원이 필요해.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n카글에서 경험 쌓으신 후에는, 거대한 도전에 대비하세요: 연구 논문을 재구현하고 결과를 복제하는 것! 네, 힘들지만 배우는 보상은 커요. 이를 성취하면 머신러닝 포트폴리오를 극적으로 빛나게 할 수 있어요. PapersWithCode를 참고하여 최첨단 모델과 그들의 구현을 확인해보세요.\n\n하지만, 머신러닝 분야에서 눈에 띄는 다른, 덜 두려운 방법들이 있어요. 궁금하신가요? 다음 글을 기대해주세요. 거기서는 분야에서 더 많은 주목을 받기 위한 간단한 전략을 제공할 거에요.\n\n글을 즐기셨기를 바랍니다. 만약 이 글이 가치있는 정보를 제공했다면 조금이나마 지원을 보내주시고 싶다면:\n\n- 이야기에 대해 여러 번 박수를 치세요\n- 기억할 가치가 더 많은 부분을 강조하세요 (나중에 찾기도 쉽고, 저에게 더 좋은 글을 쓸 수 있어요)\n- 저를 Medium에서 팔로우하세요\n- 제 최신 글을 읽어보세요: https://medium.com/@iamdgarcia\n- 구독하여 새 글이 올라올 때마다 이메일을 받아보세요. 링크는 여기에\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해당 주제에 대해 더 읽고 싶다면 다음 자료를 참고해보세요:","ogImage":{"url":"/assets/img/2024-06-22-HowIdlearnMLin2024IfICouldStartOver_0.png"},"coverImage":"/assets/img/2024-06-22-HowIdlearnMLin2024IfICouldStartOver_0.png","tag":["Tech"],"readingTime":5},{"title":"AI 대거들 양자화하기 대형 AI 모델 양자화의 방법 및 장점","description":"","date":"2024-06-22 19:57","slug":"2024-06-22-QuantizingtheAIColossi","content":"\n\n## 거장들의 작업 간소화 파트 2: 신경망 양자화\n\n최근 몇 년 동안, 트랜스포머 신경망 구조와 다양한 문제를 자기 지도형 순차 예측 작업으로 포장하는 강력한 연합이 형성되었습니다. 이 결합은 연구자들이 순차적으로 레이블이 지정되지 않은 대량의 데이터를 사용하여 전례 없이 거대한 기반 모델을 훈련할 수 있게 되었고, 이러한 모델은 여러 영역에서 인간 수준의 지능을 긴밀하게 모사하는 놀라운 신생 능력을 보여주었습니다. 새로운 실용적인 유용성의 정점에 도달함으로써 인공지능(AI)은 대중적인 생활과 대화 속으로 활주했으며, 오늘날 많은 사람들이 한때 허구의 영역이었던 실리콘 기반 지능이 이제 매우 현실적이고 실제적이라는 것을 알지 못하는 사람은 거의 없습니다.\n\n그러나 AI 능력의 폭발적인 성장과 밀접하게 연결되어 온 것은 수백억 개(경우에 따라 수조)의 파라미터로 모델 크기가 급속히 팽창한 것입니다. 강력한 새로운 기술이 세상에 공개되었지만 중요한 것은 대규모 하드웨어 클러스터를 사용해야만 제공될 수 있었습니다. 이전 AI 시대의 도전 과제들의 메아리를 일으키며, 이러한 미리 학습된 거물로 압축하는 동기가 강하며, 이를 즉각적으로 압축하는 동기가 강했습니다. 이와 동시에 공급 자원 개선, 양자화, 지식 증류, 파라미터 효율적 미세 화등 다양한 잘 정립된 기술들을 다시 살아나게 했습니다.\n\n'Streamlining Giants' 시리즈의 첫 번째 부분에서, 대형 언어 모델(Large Language Models; LLMs)의 성능을 모델 압축을 통해 민주화하는 논의를 시작했습니다. 신경망 할머니 연구의 풍부한 유산을 탐험함으로써 처음부터 LLMs에 포함된 수십억 또는 수백억 개의 파라미터를 포함하는 엘엠엠을 최근까지 신경망 가지치기에 대한 응용까지 살펴봤습니다. 이 과정에서 대량 모델이 네트워크에서 가장 중요하지 않은 매개 변수를 구조적으로 제거하거나 비구조적으로 제거함으로써 압축 할 수 있음을 발견했습니다. 또한, 가지치기는 자원 제약 환경에서 작동할 수 있는 조밀한 모델을 생성하며, 과거에는 성능을 복구하기 위해 그래디언트 정보를 계산하거나 모델을 다시 훈련해야 했습니다. 이는 가지치기를 통한 모델 압축의 수단을 가지고자 했던 사람들에게는 필요하지 않은 계산 자원이 필요하다는 것을 의미했습니다. 이 경우, LLMs의 경우 수백만 달러가 됩니다. 이것이 처음에는 그 모델을 훈련하는 데 필요한 계산 자원이 있는 사람들에게는 접근할 수 없었다는 것을 의미했습니다. 그러나 최근의 연구에서는 접근성이 높은 접근 방법을 제안하여 저위험 그래디언트 또는 심지어 정방향 통과 정보 만 사용할 수 있었습니다. 또한 매개변수가 효율적으로 세밀하게 개선되는 방법의 동시적인 발전을 통해 대형 모델의 재훈련이 사용자 하드웨어를 통해 이루어질 수 있으며, 이제 소비자 하드웨어를 사용하여 가지치기를 수행할 수 있게 되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이번 설치물에서는 모델 압축에 대한 직교적인 접근 방식을 조사합니다: 양자화는 네트워크에서 저장되고 작동되는 숫자의 정밀도를 줄이는 것을 통해 모델의 계산 효율성과 메모리 요구 사항을 개선하려고 합니다. 이는 가중치, 활성화 또는 둘 다를 포함할 수 있습니다. 양자화는 정밀도의 감소를 의미할 수 있지만, 예를 들어 32비트에서 16비트 부동 소수점으로의 변경과 같이 양자화는 종종 정수 공간으로의 전환을 수반하며 소비자 하드웨어에서 가속화된 작동 및 배포를 제공합니다. 양자화는 LLMs를 압축하는 매우 강력한 방법으로, 계산 오버헤드와 하드웨어 요구 사항의 큰 감소를 제공하면서 성능의 작은 부분적인 또는 존재하지 않는 하락만으로 대가를 치룸으로써 오늘날 대형 모델의 세계에서 가장 널리 사용되는 모델 압축 기술이 됩니다. 더 나아가 숫자 정밀도의 수준을 다양화함으로써 사용 사례에 대한 정확성/효율성 교환을 조정할 수 있습니다.\n\n이 여정을 통해 우리는 양자화가 이전에 만난 가지치기 기술들과 조화롭게 작동하며, 앞으로 살펴볼 지식 증류 및 매개변수 효율적 인 세부 조정 방법과도 조화를 이루는 것을 볼 것입니다. 이는 Streamlining Giants 시리즈에서 조사할 주제를 엿볼 수 있게 해줍니다. \"잔치에 무료 점심은 없다\"는 유명한 속담이 있지만, 가지치기에 대해 조사한 바에 따르면, 모델 압축과 관련해 때로는 그런 것이 있습니다. 가지치기와 마찬가지로, 양자화는 신경망을 더 견고하고 일반화 가능하게 만드는 정규화 형태로 작용하며, 이 기술들의 적절한 적용은 종종 모델을 압축하고 성능을 향상시키는 것을 의미합니다. 이 글에서는 문헌을 조사하고 \"무료 점심\" 압축의 여러 예시를 살펴볼 것입니다. 마지막에는 비록 의심하는 독자라도 네트워크 양자화가 품질의 저하를 내포한다는 관념이 틀렸다는 것을 깨달을 것입니다. 연구를 검토한 후, 우리는 오픈 소스 소프트웨어를 사용하여 이러한 기술을 적용하는 도구를 탐구할 것입니다. 이제 우리는 흥미진진한 신경망 양자화 분야로 들어가 봅시다.\n\n# 양자화\n\nLLM 배포에 있어 양자화의 성공과 필요성을 증명하는 방법으로, 오늘날 인기 있는 오픈 소스 LLM 서빙 솔루션마다 양자화된 모델에 쉽게 접근할 수 있도록 제공되는 경우가 많으며, 종종 기본 선택 사항입니다. 예를 들어, 최근 Ollama에서 즐겁게 사용하여 오픈 소스 음성-음성 다국어 언어 학습 보조 프로그램을 작성하는 데 큰 도움을 받은 인기 있는 Ollama는, 소비자 하드웨어에서 양자화된 LLM의 최적화된 배포를 실현하기 위해 개발된 순수 C/C++ 라이브러리인 llama.cpp를 기반으로 구축되었습니다. 저전력 하드웨어를 사용하는 로봇과 같은 실시간 비전-언어 애플리케이션에서는 이러한 유형의 하드웨어 최적화된 서빙 백엔드로 양자화된 모델을 배포하는 것이 필수적입니다. 그렇다면 양자화는 정확히 무엇이며 신경망을 압축하는 데 어떻게 효과적인지 알아보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n양자화는 연속적인 실수 공간을 고정된 집합의 이산 숫자로 매핑하는 것을 가리키며, 보다 넓은 의미로는 어떠한 숫자 공간을 낮은 정밀도의 표현으로 전이시키는 것을 말합니다. 예를 들어, 32비트 \"싱글\" 또는 \"풀\" 정밀도 부동 소수점 값, 또는 고해상도인 64비트 \"더블\" 부동 소수점 값을 살펴보면, 이러한 데이터 유형은 운반할 수 있는 소수점 자리수가 제한적입니다. 따라서 이러한 데이터 유형들은 양자화된 분포의 예시이며, 각각의 \"단계\" 사이에 표현할 수 없는 무한한 수의 값이 있는데, 이는 디지털 세계의 명확한 \"계단식\" 패턴을 만들어냅니다. 사실, 이산 시스템에서 연속 값을 효과적으로 처리하는 과제는 디지털 컴퓨터 과학이 시작된 시점부터 고민되어온 과제입니다. 심지어 부동 소수점 숫자조차도 내부적으로는 정수로 분해되는데, 이는 디지털 컴퓨터가 본질적으로 이산 정보를 처리하기 때문입니다. 따라서 신경망에서는 기술적으로 \"양자화되었느냐\"라는 문제가 아니라 \"얼마나 양자화되었느냐\"라는 질문이 더 적합합니다.\n\n신호 처리와 같은 양자화의 다른 응용과는 달리, 신경망 양자화에서의 최종 목표는 매우 정밀한 수치를 표현하는 것이 아닌, 매개 변수를 가능한 한 낮은 정밀도로 이산화시켜도 그들의 집합 상호작용으로부터 동일한 출력을 유지하는 것입니다. 신경망은 매우 많은 수의 매개 변수를 가지고 있으며, 따라서 손실 기울기 공간에 많은 가능한 해를 포함하는 최적의 매개 변수의 다양한 매니폴드를 갖고 있습니다. 따라서 개별 가중치는 양자화 과정에서 원래의 값에서 상당히 멀어질 수 있지만, 전체적 상호작용이 이 해 매니폴드에 유지되는 한 모델 매개 변수를 최적화할 수 있는 기회가 제공됩니다. 이를 양자화 관련 훈련(Quantization-Aware Training, QAT)이라고 합니다.\n\nQAT를 수행할 때는 주로 시뮬레이션된 또는 가짜 양자화를 사용하는데, 여기서 매개변수는 낮은 정밀도로 저장되지만 연산은 여전히 부동 소수점 산술을 사용합니다. QAT의 경우 부동 소수점으로 계산을 수행하면 경사하강을 위한 조건을 제공하지만(이 때문에 \"STE(직선 통과 추정기)\"와 같이 경사에 대한 둘레 함수의 파괴적 영향을 무시하는 방법이 필요함), 일부 방법은 런타임 가속보다는 저장 효율성에 초점을 맞춘 추론 시에 시뮬레이션된 양자화를 사용할 수도 있습니다.\n\n시뮬레이션된 양자화는 모든 연산이 낮은 정밀도 산술을 사용하는 정수 전용이나 고정 소수점 양자화와 대조적입니다. 정수 전용 양자화는 지연 시간 및 전력 소비에서의 모든 이점이 있는데, 하드웨어에 따라 고려 사항이 달라질 수 있습니다. 최신 GPU는 고도로 최적화된 부동 소수점 유닛을 사용하는 반면, 엣지 장치는 정수 산술이 더 효율적일 수 있습니다. 시뮬레이션된 또는 정수 전용 양자화 사용은 사용 사례에 따라 달라지는데, 예를 들어, 하드웨어 구현에 신경 쓰지 않고 다양한 양자화 수준에 대한 네트워크 구성 요소의 민감도를 반복적으로 테스트하려면 시뮬레이션된 양자화가 좋은 선택이고, 엣지에서 최적화된 배포를 위해 정수 전용 양자화가 가장 적합할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nQAT은 양자화 효과를 교육 과정에 반영하여 최적의 결과를 얻지만, 우리가 가지고 있던 가지치기 조사 중에 이전에 마주한 동일한 도전에 직면합니다: LLM과 같은 매우 큰 모델을 압축하려면 교육 데이터에 액세스할 수 없거나 GPU 시간에 백만 달러를 쓰려 하지 않는 등 여러 이유로 재교육 단계를 피하고 싶습니다. 그래서 QAT의 엄청난 결과를 포기하고 대신 작은 보정 데이터셋만 필요로 하는 사후 교육 양자화(PTQ) 방법을 살펴보게 되었고, 결국에는 데이터를 전혀 사용하지 않는 이상적인 시나리오를 탐색하는 Zero-Shot 양자화(ZSQ) 방법의 성공을 희망하게 되었습니다. 최근 연구는 PTQ를 뛰어난 정확도 수준으로 끌어 올렸고, 낮은 비트 설정에서도 전체 정밀도 기준치에 근접하여 상당히 성공적이 되었으며, 오픈 소스 연구와 코드 노력 덕분에 매우 접근하기 쉽게 되었습니다.\n\n양자화의 이점은 압축을 넘어 신경망에게 확장됩니다. 가지치기와 마찬가지로 양자화는 신경망에서 고유 매개변수의 수를 줄이는 방식으로 규제의 형태로 작용하여, 적절하게 적용될 때 성능 및 일반화 능력을 증가시킬 수 있습니다. 이러한 방식으로 양자화는 신경망을 위한 또 다른 \"무료 점심\" 압축 방법으로 가지치기와 결합되어, 모델 크기와 복잡성의 상당한 축소로 인해 결과가 조정되는 경우에도 성능이 향상될 수 있습니다. 디지털화의 이점을 고려하면, 신경망의 표현이 부동 소수점으로만 필요한 것인지, 교육 중 그라디언트 강하를 유도하기 위해 필요한 개발의 유추 단계에 불과한 것으로 보일 수 있습니다. 그리고 진행 추세가 계속되면, 최적화된 정수 계산만 사용하여 교육 결과를 최종적으로 달성할 수 있으며, 이렇게 하면 고정밀도 신경망의 필요성에서 해방될 수도 있습니다.\n\n양자화는 교육부터 배포까지 LLM 개발의 모든 부분에 영향을 미치며, 대규모 모델의 메모리, 전력 및 계산 효율성을 개선하기 위한 다양한 기법을 포괄합니다. 예를 들어, 민감하지 않은 계산은 전체 32비트 부동 소수점 대신 반정밀도(16비트 부동 소수점)에서 수행되는 혼합 정밀도로 LLM을 교육하는 것이 흔한 실천이 되었으며, 결과에 큰 영향을 미치지 않고 메모리 크기를 뚜렷하게 축소하여 운영에 필요한 전력을 현저히 줄이게 되었습니다. 이와 같은 수정은 우리가 모델을 보다 자유롭게 반복하고 발전시킬 수 있을 뿐만 아니라, 대규모 모델 교육의 환경적 영향을 넓게 미치며, LLM의 경우 CO2 톤으로 측정할 수 있는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정말, 자원의 일부만 사용하여 동등한 수학적 결과를 얻을 수 있는 경우에는 패자가 없으며 매우 큰 이익을 얻을 수 있습니다. 이 약속은 수십 년에 걸쳐 뉴럴 네트워크 양자화에 대한 엄청난 연구 코퍼스를 영감을 주었으며, 우리가 얘기하는 동안에도 계속해서 힘을 얻고 있습니다. 이는 이 탐구가 포괄적이고 완전한 것을 염두에 둔다는 의미이므로, 우리는 이를 기억에 맞추기 위해 어느 정도의 세부 사항을 살짝 빼야 할 것입니다. 야심찬 독자들은 최근과 완전한 Gholami 등의 2021 조사 또는 역사적으로 중점을 둔 Gray \u0026 Neuhoff 1998 조사를 참조해야 합니다.\n\n## 개요\n\n뉴럴 네트워크 양자화 주제에 대한 정보화된 이해의 최단 경로를 제공하려고 노력하는 이 기사의 나머지 부분은 다음과 같이 진행될 것입니다: 먼저, 양자화의 기초가 되는 수학에 친숙해지고 우리가 이야기하는 내용을 기초로 삼습니다. 그런 다음, 우리는 1990년대 초반의 뉴럴 네트워크 양자화 연구의 뿌리를 발견하고, 이를 2012년 이미지 분류 작업에서 증명된 AlexNet의 성공을 따른 \"딥 러닝 혁명\" 기간의 노력들과 연결합니다. 이에 따라 우리는 현대의 양자화 연구가 컴퓨터 비전에서 먼저 번식하고 그 다음에 자연어 처리에 들어가는 것을 목격할 것이며, 이 때문에 우리는 오늘날 LLMs의 세계에서 양자화의 응용 프로그램을 논할 준비가 되어야 할 것입니다. 마지막으로 우리는 우리의 연구 결과를 반영하고, 미래 작업 방향을 논의할 것입니다.\n\n이 기사는 장(chapters)으로 구성되어 특정한 부분으로 명확하게 읽을 수 있습니다. 독자는 정보를 찾기 위해 급하게 섹션을 건너뛰도록 선택할 수 있지만, 만나게 될 용어가 이전 장에서 정의되었을 수 있음을 염두에 두세요. 이러한 섹션들이 뉴럴 네트워크 양자화 주제의 상당히 독립적인 리뷰를 구성하며, 열정적인 머신 러닝 실무자부터 전문가까지 깊은 지식을 제공하여 자신의 작업 흐름을 최적화할 수 있는 것을 목표로 합니다. 기사는 LLM 양자화를 위한 구현 가이드로 마감되며, 시간이 제한된 독자들은 직접 거기로 건너 뛸 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 양자화의 메커니즘\n  - 비트 폭\n  - 균일 양자화\n  - 비균일 양자화\n  - 혼합 정밀도 양자화\n  - 스칼라 vs. 벡터 양자화\n  - 양자화 효과 보상\n- 신경망 양자화의 역사\n  - 신경망 양자화 초기 연구\n  - 알렉스넷 시대 이후의 양자화\n    - CNN의 양자화 인식 훈련\n    - 혼합 정밀도 양자화의 부상\n    - CNN의 후훈련 양자화\n    - 극한 양자화: 이진 및 삼진 네트워크\n- LLM의 양자화\n  - 트랜스포머 초기 시대의 양자화\n  - LLM의 후훈련 양자화\n  - LLM의 양자화 인식 훈련\n    - LLM의 극한 양자화\n- 실무가를 위한 LLM 양자화 가이드\n  - LLM 양자화 결정 트리\n- 결론\n  - 미래 작업\n\n# 양자화의 메커니즘\n\n사실 \"양자화\" 하면 무엇을 의미하는지 정확히 생각하는 것이 중요합니다. 우리는 이미 양자화를 통해 고정밀 값 집합을 가져와 더 낮은 정밀도로 매핑하고, 이를 최대한 보존하는 관계를 유지한다고 이야기해 왔습니다. 그러나 이 작업의 메커니즘에 대해 자세히 다룬 적은 없습니다. 놀랍게도, 우리는 값들을 양자화된 공간으로 다시 매핑하는 방법에 대한 세심한 설계 선택과 세부 사항이 있다는 것을 발견할 것입니다. 사용 사례에 따라 다양하게 변하는 이 선택사항을 이해하려고 이 섹션에서는 양자화 프로세스를 안내하는 조정 장치와 레버를 파악하여 연구를 더 잘 이해하고, 배포 시 교육된 결정을 내리도록 자신을 갖추고자 합니다.\n\n## 비트 폭\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n양자화에 대한 논의 동안, 우리는 양자화된 값의 비트 폭에 대해 언급할 것입니다. 이는 값 표현에 사용 가능한 비트 수를 나타냅니다. 하나의 비트는 0 또는 1의 이진 값을 저장할 수 있지만, 비트 세트는 그 조합을 증가하는 정수로 해석할 수 있습니다. 예를 들어, 2 비트가 있는 경우 4개의 총 조합('0, 0', '0, 1', '1, 0', '1, 1')을 표현할 수 있으며, 이는 [0, 3] 범위의 정수를 나타낼 수 있습니다. N 비트를 추가함으로써 가능한 조합은 2의 N제곱이 되기 때문에, 8비트 정수는 256개의 숫자를 나타낼 수 있습니다. 부호 없는 정수는 0부터 최대값까지 계산하지만, 부호 있는 정수는 첫 번째 비트를 +/- 부호로 해석하여 0을 범위 중앙에 두게 됩니다. 따라서, 부호 없는 8비트 정수는 [0, 255] 범위를 가지며, 부호 있는 8비트 정수는 [-128, 127]로 확장됩니다.\n\n비트가 정보를 어떻게 나타내는지에 대한 이 기본 지식은 우리가 연구하는 기술에서 부동 소수 값을 매핑되는 숫자 공간을 맥락에 맞게 이해하는 데 도움이 될 것입니다. 4비트로 양자화된 네트워크 레이어를 듣게 되었을 때, 목적지 공간이 2의 4승 (16) 개의 이산 값이 있음을 이해합니다. 양자화에서 이러한 값들은 양자화된 가중치의 정수 값을 반드시 나타내지 않고, 종종 양자화 수준의 인덱스를 나타냅니다 — 입력 분포의 값이 매핑되는 \"버켓\"입니다. 각 인덱스는 사전 정의된 숫자 공간 내의 특정 양자화된 값을 나타내는 코드워드에 해당합니다. 이 코드워드들은 코드북을 형성하며, 코드북에서 얻은 값은 수행할 산술 연산의 유형에 따라 부동 소수 또는 정수 값일 수 있습니다. 버켓을 정의하는 임계값은 선택한 양자화 함수에 따라 다를 수 있습니다. 코드워드와 코드북은 일반적인 용어이며, 대부분의 경우 코드북에서 반환된 값이 코드워드와 동일할 것입니다.\n\n## 부동 소수점, 고정 소수점 및 정수 전용 양자화\n\n이제 비트 폭을 이해했으므로, 부동 소수점, 고정 소수점 및 정수 전용 양자화 사이의 차이를 이해해야 합니다. 이진 비트로 정수를 나타내는 것은 간단하지만, 소수 부분을 가진 숫자를 처리하는 것은 조금 더 복잡합니다. 부동 소수점과 고정 소수점 데이터 유형은 이를 수행하기 위해 설계되었으며, 이들 중에서 선택하는 것은 배포 하드웨어와 원하는 정확성-효율성의 상충 관곈에 따라 다릅니다. 모든 하드웨어가 부동 소수점 연산을 지원하지는 않으며, 고정 소수점 산술은 수치 범위와 정밀도가 감소하는 대신 전력 효율성이 높을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n부동 소수점 숫자는 비트를 사용하여 부호, 지수 및 유효숫자를 나타내는 세 가지 정보를 할당하여 그 값들에 대한 효율적인 비트 조작을 가능하게 합니다. 지수의 비트 수는 숫자 범위의 크기를 결정하고 유효숫자의 비트 수는 정밀도를 정의합니다. 예를 들어, 32비트 부동 소수점(FP32)에 대한 IEEE 754 표준은 첫 번째 비트를 부호에, 8비트를 지수에, 나머지 23비트를 유효숫자에 할당합니다. 부동소수점 값은 \"부동\"이라고 불리는데, 각 개별 숫자에 대해 지수를 저장하기 때문에 소수점의 위치를 \"부동\"시킬 수 있습니다. 이는 10진법의 과학적 표기법과 유사하지만, 컴퓨터가 2진법(이진법)에서 작동한다는 점에서 다릅니다. 이 유연성은 다양한 값들의 정밀한 표현을 가능하게 하며, 특히 0 부근의 경우에 중요한 정규화의 중요성을 강조합니다.\n\n반면에 \"고정\" 소수점 정밀도는 동적 스케일링 요소를 사용하지 않으며, 대신 부호, 정수 및 분수(종종 유효숫자로도 지칭) 구성요소로 비트를 할당합니다. 이는 더 높은 효율성과 전력 절약 작업을 의미하지만, 동적 범위와 정밀도가 저하됩니다. 이를 이해하기 위해 가능한 한 0에 가까운 숫자를 표현하고 싶다고 상상해보세요. 그렇게 하려면 소수점을 최대한 멀리 가져가야 합니다. 부동 소수점은 소수점을 왼쪽으로 더 멀리 밀어내기 위해 점점 더 음의 지수를 사용할 수 있으며, 이 상황에서 추가 해상도를 제공할 수 있지만, 고정 소수점 값은 고정된 수의 분수 비트로 제공되는 정밀도로 제한됩니다.\n\n정수는 분수 구성요소에 대해 비트를 제공받지 않는 고정 소수점의 극단적인 경우로 볼 수 있습니다. 실제로 고정 소수점 비트는 정수인 것처럼 직접 조작할 수 있고, 결과는 소프트웨어를 사용하여 적절한 고정 소수점 결과를 얻기 위해 다시 조절될 수 있습니다. 정수 산술은 하드웨어에서 전력을 더 효율적으로 사용하므로, 신경망 양자화 연구는 정수만 양자화를 선호하며, 원래 부동 소수점 값을 정수로 변환하여 수행하므로 정수 산술은 궁극적으로 동등할지라도 정수만 산술은 더 효과적으로 수행할 수 있습니다. 이것은 배터리로 구동되는 장치에 배포하는 데 특히 중요합니다. 이 장치들은 종종 정수 산술만 지원하는 하드웨어를 포함하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일련의 숫자를 양자화하려면 먼저 양자화 함수 Q(r)를 정의해야 합니다. 여기서 r은 양자화할 실수(가중치 또는 활성화)입니다. 가장 일반적인 양자화 함수는 아래와 같습니다:\n\n![image](/assets/img/2024-06-22-QuantizingtheAIColossi_1.png)\n\n이 공식에서 Z는 정수 제로 포인트를 나타내고, S는 스케일링 팩터입니다. 대칭 양자화에서 Z는 단순히 0으로 설정되어 식에서 취소되며, 비대칭 양자화의 경우 Z를 사용하여 제로 포인트를 오프셋할 수 있어 입력 분포의 양수 또는 음수 측면 중 어느 한 쪽에 양자화 범위를 더 집중할 수 있습니다. 이렇게 함으로써 특정 경우에 매우 유용할 수 있습니다. 예를 들어, 양자화된 post-ReLU 활성화 신호를 다룰 때, 이 신호는 양수만 포함하므로 비대칭 양자화가 유용할 수 있습니다. Int(·) 함수는 정수에 가중치가 부여된 연속 값을 할당하는데, 일반적으로 반올림을 통해 할당하지만, 때로는 더 복잡한 절차를 따르기도 합니다.\n\n올바른 스케일링 팩터(S)를 선택하는 것은 쉽지 않으며, 양자화할 값들의 분포를 신중하게 고려해야 합니다. 양자화된 출력 공간은 입력을 매핑하는 유한한 값 범위(또는 양자화 레벨)를 가지므로, 수신되는 값 분포에 적합한 좋은 맞춤 범위 [α, β]를 제공하는 클리핑 범위를 설정해야 합니다. 선택된 클리핑 범위는 극단적인 입력값을 지나치게 클램핑하거나 긴 꼬리에 너무 많은 비트를 할당하여 양자화 레벨을 과다로 포화하지 않는 중요한 균형을 유지해야 합니다. 우선은 양량화의 고려 범위인 균일 양자화를 고려하며, bucketing 임계값 또는 양자화 단계가 일정한 간격으로 배치된다는 것을 고려합니다. 스케일링 팩터의 계산은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-QuantizingtheAIColossi_2.png\" /\u003e\n\n훈련된 매개변수 분포의 모양은 네트워크 간에 크게 다를 수 있으며 여러 요인에 영향을 받습니다. 해당 가중치에 의해 생성된 활성화 신호는 더욱 동적이고 예측할 수 없으므로 올바른 클리핑 범위에 대한 어떠한 가정도 어렵습니다. 이것이 우리가 모델 및 데이터를 기반으로 클리핑 범위를 교정해야 하는 이유입니다. 최상의 정확성을 위해 실무자는 추론 중에 활성화에 대한 클리핑 범위를 온라인으로 교정하는 동적 양자화라고도 하는 것을 선택할 수 있습니다. 예상대로 이 방법은 추가적인 계산 오버헤드가 발생하며, 따라서 클리핑 범위가 미리 교정되고 추론 중에 고정되는 정적 양자화보다 훨씬 인기가 적습니다.\n\n디양자화\n우리는 양자화된 값들을 원래의 숫자 공간으로 디코딩하는 역 균일 양자화 작업을 수행합니다. 그러나 라운딩 연산은 되돌릴 수 없기 때문에 이것은 불완전하게 수행됩니다. 우리는 다음 공식을 사용하여 근사값을 디코딩할 수 있습니다:\n\n\u003cimg src=\"/assets/img/2024-06-22-QuantizingtheAIColossi_3.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 비등간격 양자화\n\n안내적인 독자라면 아마도 동일한 간격의 버킷 경계를 지정하는 것이 균일하지 않은 모양의 입력 분포에서는 일부 비트가 다른 비트보다 훨씬 더 포화되게 될 것이라는 점을 알아챘을 것입니다. 그리고 이러한 폭을 조정하여 분포의 밀도가 높은 영역에 더 많은 비트를 집중시키면 입력 신호의 미묘한 차이를 더 정확하게 잡을 수 있습니다. 이 개념은 비등간격 양자화의 연구에서 조사되었으며, 신호의 충성도에서 이점을 보여주었지만, 균일 양자화로 가능한 하드웨어 최적화된 계산으로 인해 이를 사실상의 신경망 양자화 방법으로 만들었습니다. 아래의 식은 비등간격 양자화 과정을 설명합니다:\n\n![image](/assets/img/2024-06-22-QuantizingtheAIColossi_4.png)\n\n비균일 양자화의 많은 작업은 입력 분포의 클러스터 중심을 나타내는 중심점을 학습하는 것을 언급하며, 주변 값들이 양자화 과정을 통해 매핑되는 분포 내 클러스터의 중심을 나타냅니다. 다른 방식으로 생각해보면, 입력 분포 상의 경계가 균일하게 간격을 두고 있는 균일 양자화에서는 중심점이 단순히 버킷 경계 사이에 직접 있는 값들입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 혼합 정밀도 양자화\n\n우리는 가지치기와 마찬가지로, 훈련된 신경망의 성능은 일부 레이어 및 서브모듈에서 다른 것보다 민감하게 변화하는 것을 볼 수 있습니다. 이러한 민감도를 측정하여 전체 신경망의 일부를 제거할 수 있으며, 이렇게 함으로써 오차에 별다른 영향을 주지 않고 제거할 수 있습니다. 직관적으로, 같은 것이 양자화 수준이 다른 경우에도 마찬가지이며, 네트워크 구성 요소 중 일부는 동료보다 훨씬 낮은 비트 폭으로 다시 매핑될 수 있습니다. 우리가 이미 언급한 이에 가장 기본적인 예는 훈련 중 메모리 풋프린트를 크게 줄일 수 있는 덜 민감한 네트워크 작업에서 16비트 부동 소수점 수를 사용하는 것입니다. 하지만 혼합 정밀도 양자화는 네트워크 전체에 걸쳐 다양한 양자화 수준의 조합을 의미할 수 있습니다.\n\n혼합 정밀도 양자화 개념과 관련된 것은 양자화의 세분성이며, 이것은 레이어별, 그룹별, 채널별 또는 하위 채널별일 수 있으며, 구별된 양자화 매개변수 세트의 규모를 설명합니다. 직관적으로, 세분성이 높을수록 계산 부담이 높아지며, 정확도/효율성 교환이 발생합니다. 예를 들어, 합성곱 신경망(CNN)에서 채널별 세분성이 종종 선택하는 방법이며, 하위 채널별(즉, 필터 별) 양자화는 너무 복잡할 것입니다.\n\n## 스칼라 대 벡터 양자화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대부분의 양자화 연구는 역사적으로 행렬 내 개별 값들의 양자화에 초점을 맞춰 왔지만, 다차원 중심을 학습하는 것도 가능합니다. 이는 행렬을 벡터로 분할할 수 있고, 그러한 각각의 벡터에는 가장 가까운 중심을 가리키는 코드워드가 할당될 수 있음을 의미합니다. 이를 통해 단일 코드북 조회에서 행렬 전체를 복구할 수 있는 새로운 가능성이 열리고, 여러 숫자를 단일 값에 저장하며 압축 수준을 크게 향상시킬 수 있습니다. 이를 벡터 양자화라고 하며 제공하는 장점으로 인해 많은 관심을 끌고 있습니다. “벡터 양자화”는 일반적으로 행렬을 열 벡터로 분할하는 것을 의미하지만, 이들 벡터는 제품 양자화라는 연습으로 하위 벡터로 더 분할될 수 있습니다. 이는 코드북에서 반환된 중심 벡터 조립이 상대적으로 작은 구조의 저장된 코드워드를 사용하여 원본의 더 큰 행렬을 정확하게 재현할 것이라는 아이디어입니다. 이것이 실제로 매우 강력한 모델 압축 기술임이 입증되었음을 볼 것입니다.\n\n## 양자화의 영향 보상\n\n신경망의 가중치를 각기 다른 해상도로 반올림하여 여전히 제대로 작동할 것으로 예상하는 것은 합리적이지 않다는 것은 자명합니다. 따라서 양자화 과정으로 인한 불안정성을 보상하는 계획을 세워야 합니다. 앞서 배운 것처럼, 양자화된 환경에서 모델을 훈련하거나 세밀하게 조정하여 양자화 양을 크게 증가시키고도 성능에 영향을 미치지 않는 기술인 양자화 주의 훈련(QAT)을 통해 훈련하는 것이 가능합니다. 단, QAT를 수행하기 위해서는 모델을 훈련하기 위한 하드웨어와 데이터가 필요합니다. 이는 오늘날의 대규모 모델인 LLMs와 같은 경우에는 자주 불가능합니다. 이 문제를 해결하기 위해 사후 훈련 양자화(PTQ) 기법은 훈련을 피하고 양자화 함수를 보정하기 위한 소량의 레이블되지 않은 데이터만 필요로 합니다. 그리고 Zero-Shot 양자화(ZSQ)는 보정을 위해 데이터가 필요하지 않은 이상적인 시나리오를 탐구합니다.\n\n이러한 기술들이 문헌 속에서 자세히 소개되니 각각의 기법에 대해 자세히 알아보겠습니다. 이제 시간 여행 버스에 탑승하여 말세로 돌아가 년들에, 하드웨어 한계를 초과하는 신경망의 힘에 매료되고, 이러한 복잡한 모델을 모바일 하드웨어에 배포하는 방법에 대해 처음으로 고민하기 시작했던 연구원들을 만나러 떠나 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 신경망 양자화의 역사\n\n## 신경망 양자화의 초기 연구\n\n모델 압축 기술 가족 중에서 신경망 양자화는 가위로 잘라내기와 얼추 비슷한 연관성을 가진 더 낮은 수준의 기술로, 뒤지기도 일찍이 1980년대 말 백프로프 훈련된 신경망의 형성 연도까지 거슬러 올라간다. 이 당시의 전산 하드웨어 발전 시대에서는 신경망 연구에 대한 관심이 다시 살아나게 되었지만, 하드웨어 제약으로 인해 여전히 신경망이 가장 적합한 사용 사례들을 제한하게 됐다. 연구자들은 수치 정밀도 문제에 대한 고려를 수십 년 동안 함축적으로 다뤄 왔지만, 이러한 계산 제약사항이 강조되면서 수치 정밀도를 줄여 최적화하는 새로운 방향에 주목했다. 이는 1990년대 초반에 걸쳐 방대한 양의 연구를 유발했다.\n\n1989년에 베이커와 해머스트롬은 하드웨어 최적화 가능성을 염두에 둔 채로 줄인 수치 정밀도가 네트워크 성능에 미치는 영향을 체계적으로 연구한 최초의 연구자들이었다. 그들의 작품 \"인공 신경망 알고리즘의 특성화\"는 32비트 부동 소수점 연산이 네트워크 성능을 보존하기 위해 반드시 필요하다는 관행적 지혜에 도전했으며, 줄인 정밀도 계산을 사용하여 백프로프에 의해 네트워크를 성공적으로 훈련하는 초기 사례였다. 같은 해에 홀리스 등이 12비트 정밀도 주변에서 백프로프 네트워크 훈련에 대한 정밀도 제약이 미치는 영향을 더욱 심층적으로 연구했으며, 1990년에는 다음의 연구에서 이전 연구를 발전시킨 다른 하머스트롬은 고정 정밀도 계산을 위해 명시적으로 최적화된 새로운 하드웨어 유닛의 설계를 조사하였다. 이 결과로 16비트 또는 8비트 정밀도를 사용하여 성능에 용인 가능한 정도의 감소와 함께 효율성의 상당한 향상이 얻어진다는 것을 입증함으로써, 신경망 양자화와 하드웨어 최적화에 대한 향후 연구 기초를 다지게 되었다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n신경망을 최적화하기 위해 낮은 비트 폭과 특수 하드웨어를 활용하는 연구는 백프롭이 신경망 학습 알고리즘의 무전한 챔피언이 되기 전에도 진행되고 있었다. 1992년의 중요한 연구에서 Hoehfeld와 Fahlman은 한정된 숫자 정밀도가 카스케이드-상관 알고리즘(Fahlman \u0026 Lebiere, 1991)으로 네트워크를 학습하는데 미치는 영향을 조사하여, 이 학습 알고리즘이 고정 정밀도에서도 효과적임을 입증했습니다. 그들의 성공의 일환으로, 저자들은 수렴을 가능하게 하는 동적 재척도 및 확률론적 반올림 기술을 소개했는데, 이는 훈련에 훨씬 낮은 정밀도(그들의 경우 6비트)에서도 적용 가능하며, 그레이디언트 기반 학습 알고리즘에 적용할 수 있습니다.\n\n네트워크 양자화 탐구의 이 기초적인 시기는 오늘날의 거대한 기술 세계를 가득 채우는 정교한 기술과 하드웨어 특화 최적화를 발견하는 길을 밝혔습니다. 줄어든 정밀도 연산의 실현 가능성과 혜택을 시연함으로써, 이 초기 연구는 신경망 응용의 가능성을 확대시키고, 더 효율적이고 확장 가능한 AI 시스템을 개발하기 위한 강력한 방법을 제공했습니다. 오늘날, 우리가 다양한 플랫폼에서 널리 통용되는 AI 통합의 선도자로 서 있는 것은 이 선구적인 노력의 유산이 더욱 중요합니다. 이 노력은 모델 압축과 양자화를 통한 효율적인 계산의 잠재력뿐만 아니라, 신경망 설계 및 최적화에서 혁신을 지속적으로 불러일으켰습니다.\n\n## 알렉스넷 시대 이후의 양자화\n\n2012년에 출시된 알렉스넷의 저자들은 데이터 이용 가능성과 계산 하드웨어의 주요 발전이 우연히 엮인 상황을 이용하여, ImageNet 대규모 시각 인식 챌린지(ILSVRC)에서 과거 최첨단 접근법의 성능을 능가하는 성과를 이루었습니다. 이 역사적인 성공을 가능케 한 두 가지 중요한 요소는 다음과 같습니다: 1) 프린스턴 대학의 Fei-Fei Li 및 그녀의 팀이 세곅 최초의 대규모 취합 이미지 데이터셋을 제공하고, 2) 게임 산업 수익을 토대로 한 GPU 기술의 재정적 발전이 우연히도 딥러닝의 행렬 연산을 가속하는 데 필요한 동시 계산 유형을 지원하는 하드웨어를 생산하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주어진 이 성급한 상황에서, Alex Krizhevsky와 그의 팀은 6230만 개의 매개변수로 구성된 상당한 합성곱 신경망(CNN)을 훈련시켜 경쟁자들을 능가하며 정확도에서 10%의 우위를 차지했습니다. 이것은 신경망 연구에 있어 수적 PR 순간을 표시하고 \"딥 러닝 혁명\"으로 자주 지칭되는 지속적인 관심과 자금 풍요한 시기를 시작했습니다. 그러나 회복된 신경망은 빠르게 예전 적의 문제점인 하드웨어 제한에 부딪혔습니다. GPU 가속 훈련의 현저한 이점에도 불구하고, AlexNet의 저자들은 하드웨어 제약이 접근법의 성공에 한계를 가하고 결과가 더 좋아질 가능성이 높다고 인정했습니다. 연구 커뮤니티는 모델 압축의 미수렴 가능성을 인식하고 행동에 나섰습니다. 이 CNN 중심 기간 동안 얻게 된 정보들은 서로 다른 유형의 네트워크 구성 요소들 사이의 민감도 수준에 대한 강한 기반을 제공하여 향후 트랜스포머에 대한 조사를 위한 강력한 기초를 마련했습니다.\n\n사실, AlexNet이 세계를 뒤흔들기 전에 이미 신경망을 엣지에서 배치할 욕망이 들려왔습니다. Vanhoucke 등의 2011년의 중요한 연구는 x86 CPU에서의 신경망 가속화를 탐구했습니다. AI 커뮤니티가 GPU에 투자할 것인지 아니면 전통적인 CPU로부터 더 많은 성능을 추출할 것인지에 대한 논쟁의 시기에 작성된 이 논문은 Intel 및 AMD CPU에서의 신경망 작업을 최적화하는 데 중요한 안내를 제공했습니다. AlexNet이 도입한 GPU 우위 시대 이전에, Vanhoucke 등은 SIMD 명령어와 메모리 정렬 기법을 포함한 정밀한 최적화를 통해 CPU의 잠재력을 자세히 소개했습니다. 이 최적화 기법을 사용하여 저자들은 상당한 성능 향상을 이룩하고 CPU 하드웨어에서의 신경망 효율적인 훈련 및 배포에 대한 곧 나올 연구의 기초를 마련했습니다.\n\nAlexNet의 성공 이후 CNNs은 적층 연구의 급속히 확장되는 작물의 새로운 토양이 되었습니다. 연구자들은 다양한 유형의 네트워크 레이어를 양자화하는 세심한 연구를 펼치며 제공할 수 있는 다양한 민감도 수준과 장점을 씻겼습니다. 예를 들어, CNNs의 대부분의 FLOP는 합성곱 레이어에서 발생하기 때문에 그것을 양자화하는 것이 속도 향상을 가장 많이 제공합니다. 그러나 이러한 레이어들은 특징 추출에 매우 중요한 매개변수를 포함하고 있어 변경에 특히 민감합니다. 반면에, 완전 연결 레이어는 압축하기가 훨씬 쉽지만, 그렇게 하는 것은 대부분 저장 공간 크기 측면에서 유리하며 전체 계산 그래프에 대한 기여가 적습니다.\n\n저장 공간 전용과 완전 한 효율 향상의 기법 사이의 차이뿐만 아니라, 후자 그룹에서는 훈련과 추론 모두 가속화하는 기법과 추론만 가속화하려는 기법 사이에도 명백한 차이가 있습니다. 이 기간 동안 QAT 개념이 탄생했으며, 많은 기술들은 훈련 중에 시뮬레이션된 양자화를 사용하도록 선택하지만, 다른 사람들은 네트워크 양자화의 뿌리에 더 가까이 머물며 훈련과 추론 중에 고정 소수점 또는 정수만 산술을 사용하여 엣지에서의 최종적인 신경망 개발을 추구했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 다양한 CNN 압축 방법 중 양자화를 통한 CNN 압축에 대한 두 가지 초기 예시로 Denton 등의 2014년 \"효율적인 평가를 위해 합성곱 네트워크 내에서 선형 구조를 활용\" 방법은 합성곱 레이어에서 매트릭스 인수분해를 적용하여 PTQ 또는 QAT 프로시저로 계산 효율성을 개선하고, Gong 등의 2014년 \"벡터 양자화를 사용한 깊은 합성곱 네트워크 압축\"은 QAT 환경에서 다양한 벡터 양자화 방법을 사용하여 완전 연결 레이어를 압축하여 저장 공간 최적화를 달성하는 데 초점을 맞추며, 제품 양자화의 뚜렷한 우월성을 강조했습니다.\n\n본 섹션에서는 AlexNet에 의해 시작된 CNN 지배 시대 중 양자화 분야가 역동적으로 발전하는 과정을 관찰하게 될 것입니다. 성장의 형성 기간 동안 QAT, 혼합 정밀도 양자화, PTQ, 그리고 1 또는 2 비트까지의 극단적 양자화가 오늘날 대형 모델 시대의 이러한 기법의 성숙화로 이어지도록 새롭게 정의된 연구 분야로 자리잡을 것입니다.\n\n## CNN의 양자화 인식 훈련\n\nAlexNet 시대 이후, QAT는 양자화 연구의 독립적인 영역으로서 본격적으로 형성되었습니다. 이전 시대에는 거의 모든 양자화 작업이 상대적으로 작은 네트워크 때문에 가중치 양자화를 최적화하기 위해 훈련 프로세스를 사용했었습니다. GPU 가속 훈련에서 유발된 인공 지능 성장 후에도 모델은 여전히 상당한 자원을 사용하여 훈련할 수 있었으며, 양자화된 네트워크 재훈련의 필요성을 피하는 문제는 주로 모바일 배치 및 데이터 접근/개인 정보 보호 우려에서 비롯되었습니다. 그럼에도 불구하고, 이 시대에 PTQ의 가치가 명확해지고 두 분야 간의 구분이 명확해지면서, 양자화 연구의 주된 부분은 QAT 기반의 접근에 머물렀습니다. 이 섹션에서는 CNN 시대에 QAT 접근법의 발전을 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2014년 말에 시작된 Courbariaux 및 다른 연구자들의 연구에서 \"곱셈 연산자는 딥 뉴럴 네트워크의 디지턜 구현에서 공간과 전력을 많이 소비하는 산술 연산자이다\" 라고 발견했으며, 이러한 연산의 정밀도를 줄여 효율성을 증대하기 위해 연구를 진행했습니다. 다른 연산자들의 비용이 비트 폭에 비례해 제곱으로 증가하는 반면에 곱셈-덧셈기 연산의 다른 연산자들인 가산기와 누산기는 선형으로 증가하므로 상대적으로 저렴합니다. 특히, 그들의 연구는 \"반 정밀도 부동 소수점 형식을 사용해도 신경망의 훈련에 별다른 영향이 없다\"고 보여졌습니다. 더욱이 저자들은 \"매우 낮은 정밀도만으로 훈련된 신경망을 실행하는 데 뿐만 아니라 훈련까지 할 수 있다\"고 발견했으며, 현재까지의 \"매우 낮은 정밀도\"는 10비트 곱셈을 의미하며, 이는 이 분야가 얼마나 빠르게 변화하는지를 보여주는 상징입니다.\n\n2015년 IBM의 Gupta 및 다른 연구자들이 “Deep Learning with Limited Numerical Precision”를 발표하면서, 확률적 반올림을 사용한 16비트 고정 소수점 산술을 통해 딥 뉴럴 네트워크를 훈련하는 기존 방법을 소개했습니다. 이 방법은 숫자를 가장 가까운 양자화 지점 중 하나로 반올림하는 확률이 그 두 지점 간의 근접성에 비례하는 확률적 반올림을 사용하여 소프트웨어를 개선했습니다. 이 반올림 방법은 양자화 오차의 기대값(편향)을 제로로 만드는 잡음을 도입함으로써 가장 가까운 반올림 방식을 능가합니다. 풀 정밀도 부동 소수점 연산에 의존하는 기존의 QAT 방법과 달리, Gupta 및 다른 연구자들의 전략은 모든 훈련 연산을 낮은 정밀도에서 실행하는 것을 포함합니다. 고정 소수점 산술의 사용은 더 빠르고, 공간과 전력을 효율적으로 사용하는 연산 장치를 사용할 수 있게 해주며, 저자들은 하드웨어 공동 설계를 탐구하기 위해 혁신적인 에너지 효율적 하드웨어 가속기를 시연했습니다. 중요한 점은 확률적 반올림의 사용으로 심한 기울기 값도 훈련 과정에 기여하도록 보장하여 기울기 근사 방법인 Straight-Through Estimator (STE)와 같은 방법에 대한 의존을 줄입니다.\n\n한국어로 번역했습니다. 계속해주시면 다양한 주제에 대해 도와드릴게요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-QuantizingtheAIColossi_6.png\" /\u003e\n\n\"Deep Compression\"에서 나온 결과는 놀라운데요, 해당 방법으로 CNN을 원본 크기의 1/35 미만으로 압축하면서 베이스라인 참조와 동등하거나 더 높은 정확도를 보여줍니다. 그러나 중요한 점은 연구된 AlexNet과 VGG 구조가 성능을 극대화하기 위해 의도적으로 초매개변수화되었다는 것을 기억하는 것입니다. 두 구조 모두 매개변수 밀도가 높지만 비교적 민감하지 않은 완전 연결 레이어를 갖고 있습니다. 이 방법은 주로 모델의 저장 공간 효율성을 향상시키는 데 초점을 맞추고 있지만 작은 저장 공간은 가속 작업을 의미하기도 합니다. 가중치를 저장하고 가져오기 위한 작업량이 적어지고, 따라서 메모리 대역폭 요구 사항도 줄어듭니다. 특히 모델 크기가 충분히 작아져서 SRAM(Static Random Access Memory)에 저장될 수 있게 되면 DRAM(Dynamic RAM)과 SRAM 사이를 왔다갔다 하는 것보다 더 효율적입니다. 게다가 저자들은 ‘효율적 추론 엔진 (EIE)’ 개념을 소개했는데 이는 가지치기로 인한 희소성을 활용하기 위해 설계된 하드웨어 가속기로, 이에 대한 논문이 곧 발표될 예정입니다.\n\n\u003cimg src=\"/assets/img/2024-06-22-QuantizingtheAIColossi_7.png\" /\u003e\n\n2017년 초, Zhou 등의 Incremental Network Quantization (INQ) 접근 방식이 Deep Compression에서 보이던 압축 수준을 능가했습니다. 저자들은 가지치기와 양자화를 결합하여 Huffman 코딩을 사용하지는 않았지만, AlexNet에 대해 53배의 압축률을 달성하면서 top-1 정확도를 잃지 않았으며, 놀라운 89배의 압축을 달성했지만 손실은 최소한 (`1.5%)을 유지했습니다. 이들의 전략은 네트워크 가중치의 일부를 점진적으로 양자화하고 나머지 정밀도 가중치를 재훈련하여 인도된 오차를 보상하며, 모든 가중치가 양자화될 때 까지 반복하는 것입니다. 가중치는 0 또는 2의 거듭제곱(음의 거듭제곱이라고 생각할 수 있음)이어야 합니다. 저자들이 설명한대로 이 전략의 이점은 “원래의 부동소수점 곱셈 연산을 FPGA와 같은 전용 하드웨어에서 더 저렴한 이진 비트 시프트 연산으로 대체할 수 있다”는 것입니다. 이를 위해 이들은 가변 길이 인코딩을 사용하는데, 1비트는 제로 값을 나타내고 남은 비트는 주어진 비트 폭과 스케일링 요인에 대한 가능한 양자화 값을 인덱싱하는 코드 워드를 나타냅니다. 이들의 방법은 5비트 정밀도로 FP32 AlexNet 성능 기준을 약간 초과하며, 아래 차트에서 확인할 수 있듯이 3비트까지 동등한 정확도를 보여줍니다. INQ에 대한 코드는 GitHub에서 이용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-QuantizingtheAIColossi_8.png\" /\u003e\n\n2017년 말, Google의 Jacob 및 동료들이 휴대용 장치 CPU에서 효율적인 정수 전용 추론을 가능하게 하는 데 초점을 맞추었습니다. 저자들은 AlexNet 및 VGG와 같이 고의로 과잉 매개변수화된 모델을 사용하여 모델 압축 기술을 벤치마킹하는 것은 쉬운 대상을 만든다고 주장하며, 대신 MobileNets를 사용하여 접근 방식을 테스트하기로 결정했습니다. 이러한 콤팩트 모델은 이미 매개변수 효율성을 극대화하도록 설계되어 있기 때문에 쉽게 압축할 수 있는 \"죽은 무게(dead weight)\"가 적고, 이들의 매개변수는 더 민감하게 변조됩니다. QAT의 형성 작업으로 고려된 Jacob 및 동료들의 접근 방식은 가중치 및 활성화를 8비트 정수로 양자화하고, 보다 정밀성이 더 필요한 편향을 32비트 정수로 양자화합니다. 그들의 방법은 훈련 중에 부동 소수점 산술을 사용하며, QAT에 대한 시뮬레이션 양자화 사용의 초기 예로서의 역할을 합니다. 저자들은 SIMD 하드웨어에서 순수한 산술보다 성능이 떨어지는 경향이 있는 조회 테이블을 필요로 하는 양자화 체계를 피하고, 대신 가중치를 정수 공간으로의 아핀 변환으로 대체합니다. 저자들은 QAT 방법을 보완하기 위해 훈련된 모델을 정수 전용 하드웨어에서 변환하고 실행하는 프레임워크를 함께 설계하며, 실제 엣지 하드웨어에서의 효율 향상을 증명하는 많은 이전 작업보다 한 걸음 더 나아갑니다.\n\n\u003cimg src=\"/assets/img/2024-06-22-QuantizingtheAIColossi_9.png\" /\u003e\n\n지금까지 본 기술들은 모델의 계층을 균일한 정밀도 수준으로 양자화했습니다. 이는 하드웨어 가속화를 위한 최적 조건이며, 특히 저전력 하드웨어의 엣지에서는 더욱 중요합니다. 하지만 이전의 가지치기 탐구에서 배운 것처럼, 일부 네트워크 계층은 다른 것들보다 민감도가 낮아 변경에 덜 민감하므로 성능에 영향을 미치지 않으면서 더 강력하게 압축할 수 있습니다. 그래서 혼합 정밀도 양자화 접근방식은 네트워크 구성요소에 다양한 수준의 압축을 적용하고(일반적으로 계층별로), 그들의 민감도에 기초하여 메모리 풋프린트를 더 작게 만들고, 엣지에서 모델 운영의 데이터 전송 및 전력 비용을 감소시킵니다. 다음 섹션에서는 민감도 분석의 익숙한 주제가 수치 정밀도의 변수 할당에 영향을 미치는 Neural Network 구성 요소 전반에 걸친 모델 압축을 극대화하기 위한 양자화를 안내할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 혼합 정밀도 양자화의 부상\n\n양자화로 인한 조각의 변동에 대한 네트워크 레이어 간 민감도 수준을 고려하기 위해, 각 네트워크 구성 요소의 민감도에 따라 정확도 수준을 맞추는 혼합 정밀도 양자화는 깊은 신경망의 양자화를 통해 가능한 압축 수준을 극대화하는 인기 있는 방법이 되었습니다. 혼합 정밀도 설정의 방대한 검색 공간은 네트워크 레이어 수와 지수적으로 증가하기 때문에 도전적입니다. 또한, 레이어 별 양자화와 세분화의 최적 순서를 찾는 데 목적을 두는 방법의 경우, 복잡성이 조합적으로 증가합니다. 이에 따라 연구자들은 깊은 신경망을 위한 최적 혼합 정밀도 설정의 검색과 선택을 자동화하기 위해 다양한 효과적인 알고리즘을 제안해왔습니다. 이는 무차별 탐색의 불가능성을 피하는 데 도움이 됩니다. 앞으로 살펴볼 것은 신경 구조 탐색(NAS), 강화 학습 (RL), 2차 해선 정보 및 기타 유형의 솔버를 활용하여 이 문제를 해결하기 위해 연구자들이 제안한 다양한 알고리즘들이 놀라운 결과를 산출하며 양자화를 통한 효율성 향상이 가능한 새로운 수준을 확립하고 있다는 것입니다.\n\n2018년 말, Wu 등이 \"Differentiable Neural Architecture Search (DNAS)를 통한 ConvNets의 혼합 정밀도 양자화\" 방법을 발표했습니다. 이들의 접근 방식은 모든 레이어에 대한 모든 가능한 정밀도 설정을 포함하는 완전한 미분 가능한 슈퍼 네트워크 안에 서브넷을 정의하는 네트워크 아키텍처 매개변수 θ 세트를 생성합니다. θ 매개변수는 이 서브넷의 그래프 내 각 엣지를 샘플링하는 확률을 제어하며 경사하강법을 통해 직접 최적화할 수 있습니다. 이 기술을 사용하여 저자들은 몇몇 레이어에서 가중치를 극단적으로 압축(일부 레이어에서는 1비트까지)한 모델을 생성하였는데, 이 모델들은 완전한 정밀도 기준선을 능가할 수 있었습니다. DNAS 방법은 21.1배 작은 공간과 103.9배 낮은 계산 비용을 가진 ResNet 모델을 생성했는데, 이 모델들은 완전한 정밀도 모델과 동등한 성능을 발휘했습니다. 저자들은 이 방법이 다른 네트워크 파라미터화에도 확장 가능한 일반 아키텍처 탐색 프레임워크임을 언급했지만 이는 향후 작업으로 남겨두었습니다.\n\n![image](/assets/img/2024-06-22-QuantizingtheAIColossi_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2018년 Wang 외 (2018)의 \"HAQ: Hardware-Aware Automated Quantization with Mixed Precision\" 논문은 계산 효율성을 위해 프록시 메트릭스 (예: FLOPs)의 사용을 배제하고 대신 하드웨어 시뮬레이터에서의 신호를 사용하여 최적의 혼합 정밀도 설정을 찾기 위해 강화 학습 (RL) 알고리즘에 입력하는 방식을 택했습니다. 그들의 접근 방식은 지능적인 모델 압축의 한계에 도전하지만, RL 정책을 훈련하여 주어진 네트워크 아키텍처 및 하드웨어 장치에 대한 올바른 혼합 정밀도 설정을 예측하는 것에 따라 복잡성, 계산 비용 및 일반화 부족과 관련된 약점을 겪고 있습니다.\n\n\n![그림: 2024-06-22-QuantizingtheAIColossi_11](/assets/img/2024-06-22-QuantizingtheAIColossi_11.png)\n\n\n2019년에는 Hessian AWare Quantization (HAWQ) 논문이 혼합 정밀도 양자화에 대한 두 가지 주요 도전 과제를 해결했습니다. 각 네트워크 레이어의 최적 정밀도 수준을 결정하는 지수적 탐색 공간 및 이러한 레이어 간 최적의 QAT 순서를 결정하는 계승 복잡성은 깊은 신경망에서 이러한 값들의 무차별적인 탐색을 불가능하게 만듭니다. 저자들은 둘째로, 2차 헤시안 정보의 상위 고유값이 감도 분석을 제공하여 올바른 양자화 수준과 네트워크 레이어 간의 미세 조정 순서를 안내하는 것을 시연했습니다. 이는 LeCun 외 (1989)의 Optimal Brain Damage에 기반을 둔 가지치기 접근 방법과 유사합니다. 여기서 헤시안 정보는 네트워크 구성요소의 중요성(즉, 감도)을 측정하는 데 사용되며, 큰 값은 특정 네트워크 구성 요소의 가지치기 또는 양자화가 성능에 더 큰 영향을 줄 것임을 나타냅니다. HAWQ는 2비트 가중치와 4비트 활성화를 사용하여 DNAS의 성능을 넘어서며, 이로 인해 더 높은 전반적인 압축성을 달성했습니다.\n\n2019년 HAWQ로부터 7개월 후, Dong 외가 HAWQ-V2를 발표하여 이전 연구에서 식별한 세 가지 주요 한계를 해결했습니다: 1) 감도 분석을 위해 상위 고유값만 사용하여 헤시안 스펙트럼의 나머지 부분을 무시한 것, 2) 상대적인 레이어별 감도 측정만 결정했으므로 수동 정밀도 할당이 필요한 것, 3) 활성화에 대한 혼합 정밀도 양자화를 고려하지 않은 것. 첫 번째 문제에 대해, 저자들은 모든 헤시안 고유값의 평균을 레이어별 감도의 우수한 측도로 결론지었습니다. 둘째로, 저자들은 정확한 레이어별 비트 정밀도를 자동으로 선택하기 위해 파레토 프론티어 기반 방법을 제시했습니다. 세 번째로, 저자들은 \"혼합 정밀도 활성화 양자화에 대한 헤시안 분석을 확장\"합니다. 이러한 조정으로 HAWQ-V2는 CNN 양자화에서 새로운 최첨단 벤치마크를 설정했습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2020년 말, HAWQ 저자들은 세 번째 버전인 HAWQ-V3를 발표했습니다. 이 작업은 이전의 방법을 개선하여 네트워크 작업의 모든 부분에 정수 연산을 보장하였습니다. 배치 정규화 레이어와 잔여 연결을 포함한 이전에는 정확도를 유지하기 위해 float32로 유지되던 부분들도 정수만을 사용하도록 변환하여 경량 기기에서 일반적으로 사용되는 정수 전용 하드웨어에 배포 할 수 있도록 했습니다. HAWQ-V3는 층별 비트 정밀도를 결정하기 위해 \"정수 선형 프로그래밍(ILP) 문제를 사용하는 새로운 하드웨어 인식 혼합 정밀도 양자화 공식\"을 사용하며, 이를 통해 \"모델의 변동과 제약(예: 메모리 풋프린트 및 지연 시간) 사이의 균형을 유지\"합니다. 이 접근 방식은 최강의 사용 가능한 완전 정밀 베이스라인의 성능을 넘어서는 8비트 정수 추론 모델을 생성하며, \"무료 점심\" 압축을 보여주며 4비트 정밀도까지 고정한 상태로 높은 정확도를 유지합니다.\n\n![이미지](/assets/img/2024-06-22-QuantizingtheAIColossi_12.png)\n\n이 섹션에서 다루는 기술들은 감도 분석을 기반으로 한 층별 정밀도 할당 조정의 힘을 보여주며, 감도가 적은 층을 더 공격적으로 양자화하고 민감한 층에서 더 높은 정밀도를 유지하는 것을 허용합니다. 그러나 엣지 디바이스에서 혼합 정밀도 양자화 체계를 구현하기 위해 더욱 어려운 작업일 수 있다는 점을 강조해야 합니다. 따라서 HAWQ-V3 결과에서 강력한 고정 정밀 벤치마크를 볼 수 있어 안심스럽습니다. 다음 섹션에서는 훈련된 완전 정밀 CNN에 대한 양자화를 수행하는 기술에 대해 논의할 것이며, 이는 나중에 논의할 LLMs의 양자화를 위한 선행 작업으로 매우 관련이 높습니다.\n\n## CNN의 사후 훈련 양자화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알고 계신 분은 CNN 시대의 양자화 섹션에서 대부분의 작품이 QAT 범주에 속한다는 것을 알게 될 것입니다. 이 기간 동안 연구된 모델들은 양자화 설정에서 쉽게 세밀 조정할 수 있었기 때문입니다. 그런데, 신경망 크기가 급격히 증가하기 전에도 PTQ의 실용적 이점에 이미 주목하고 있었던 연구자들은, 양자화 모델 개발자들이 원본 훈련 데이터에 접근할 필요 없이 모델을 중요한 시간과 자원을 재훈련하는 데 소요되는 시간과 자원을 절약할 수 있다는 PTQ의 약속에 이미 열렬한 반응을 보이고 있었습니다. 따라서 2019년경 PTQ 연구에 대한 타이밍 적인 관심이 높아졌고, 이는 아직 다가올 대형 언어 모델에 중점을 맞춘 연구를 위한 환영받는 기초를 마련했습니다.\n\n크리슈나무르티는 2018년 중반에 CNN 양자화와 관련해 중요한 백서를 발표했습니다. 그들의 접근 방식은 가중값의 채널별 비대칭 균일 양자화와 활성화 값의 레이어별 고정 정밀도를 8비트로 유지하면서 정확도를 기준선 수준의 2% 이내로 유지합니다. 저자는 신경망의 가중치만 8비트로 양자화하는 것은 저장 용량을 압축하는 간단한 방법이지만, 효율적인 추론을 가능케하기 위해서는 활성화도 양자화해야 하며, 이를 위해서는 네트워크 레이어 전체의 활성화의 동적 범위를 계산하기 위해 보정 데이터를 사용하여 올바른 레이어별 양자화 매개변수를 발견해야 하는 점을 관찰했습니다. 아래 차트에서 저자는 다양한 CNN에서 레이어별 및 채널별 가중치 양자화 체계가 미치는 영향을 비교했습니다. 왼쪽에 있는 효율 지향적인 MobileNets보다 오른쪽에 있는 크고 매개 변수가 과도하게 많은 CNN이 레이어별 양자화 매개변수의 낮은 세분화에 더 수용적임을 주목하세요.\n\n\u003cimg src=\"/assets/img/2024-06-22-QuantizingtheAIColossi_13.png\" /\u003e\n\n2018년 10월, Banner 등의 \"빠른 배포를 위한 합성곱 신경망의 후 훈련 4비트 양자화\" 논문은 PTQ의 사용성을 8비트 미만의 정밀도로 확장하는 데 초점을 맞췄습니다. 그들의 접근 방식은 감내 가능한 성능 하락과 함께 4비트 데이터 무료 혼합 정밀도 PTQ를 효율적으로 달성합니다. 이를 위해 저자들은 신경망 분포가 평균 주변에 종 모양을 이룬다는 지식을 활용하여 자신들의 양자화 체계를 튜닝하여 텐서 수준에서 평균 제곱 양자화 오차를 최소화하도록 한 것으로, 재훈련의 필요성을 피할 수 있게 했습니다. 양자화 공간으로의 지식 이전을 보다 잘 허용하기 위해 저자들은 1) 정수 양자화에 대한 제안된 분석 클리핑 기술(aciq)을 사용하여 활성화 텐서 이상치를 최적의 포화점에 따라 잘라내어 스펙트럼의 더 밀도 높은 영역에서 라운딩 오차를 줄였습니다. 2) 채널별 최적의 비트 할당을 분석적으로 결정하여 주어진 채널에 대한 최적의 양자화 단계 크기를 그 채널 범위의 2/3 제곱에 비례하도록 찾았으며, 3) 양자화 후 가중치에 도입된 편향을 보상하기 위해 간단한 편향 보정 방법을 제안하여 채널별 평균 및 분산의 예상 변화를 양자화 매개변수에 반영했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nACIQ 방법론은 작은 보정 세트에서 네트워크 활성화의 통계 분석을 요구하기 때문에 교육 데이터에 액세스하지 않아도 되지만, 실행 중에 만날 분포를 대표하는 보정 세트를 보장하는 것이 중요합니다. 그렇지 않으면 양자화 매개변수가 잘못된 분포에 과적합될 위험이 있습니다. 또한 채널별 비트 폭의 사용은 하드웨어와 소프트웨어가 믹스드 프리시전을 지원할 수 있게끔 조정되어야 하므로 실제 응용 프로그램에 많은 문제를 일으킵니다. 그렇지 않으면 양자화된 네트워크를 실행하는 것이 비효율적이거나 불가능할 수 있습니다. 그러나 네트워크 구성 요소의 최적 비트 폭을 직접 계산하는 폐쇄형 분석 솔루션의 정식화는 양자화 연구에서 중요한 이정표입니다. 더 나아가 편향 보정 매개변수에 대한 폐쇄형 PTQ 솔루션 및 이러한 매개변수를 기존 계산에 효율적으로 흡수하는 접근 방식은 또 다른 중요한 기여입니다. Banner et al.의 방법 코드는 GitHub에서 이용 가능합니다.\n\n![image](/assets/img/2024-06-22-QuantizingtheAIColossi_14.png)\n\nNagel et al.의 2019년도 “Weight Equalization과 Bias Correction을 통한 데이터 없는 양자화” (DFQ)는 보정 데이터, 세밀 조정 또는 하이퍼파라미터 조정이 필요하지 않은 데이터 없는 PTQ에 대한 혁신적인 접근 방법을 소개했습니다. 저자들은 가중치를 조정하여 양자화에 “쉽게 적용 가능하도록” 만들기 위해 척도를 적응시키고, 양자화로 인해 도입된 편향을 보정하기 위한 방법을 제안했으며, 그들의 방법이 QAT의 보조 전처리 단계로 사용될 수 있다고 언급했습니다. 위의 Krishnamoorthi 및 Banner et al. 방법과 달리 각 채널에 대한 양자화 매개변수를 저장해야 하는 것을 필요로 하지 않는 DFQ는 그 층의 각 채널에 대한 적합성을 극대화하는 값을 결정하여 각 층의 가중치 텐서에 대해 단일 척도 및 오프셋 값을 저장합니다. DFQ는 ReLU 활성화 기능의 척도 동질성을 이용하고, 스케일링과 유도된 편향을 다음 층으로 흡수하여 수학적으로 전체적으로 등가관계를 유지합니다. 저자들은 활성화 양자화에 대한 보정 데이터의 필요성을 교정 데이터 없이 모델에서 보존된 배치 정규화 통계를 사용하여 해당 층에서 예상하는 양자화 오차, 그리고 층 활성화에 도입된 편향을 보정하기 위해 해당 예상 오차를 층 편향 매개변수에서 차감하는 방법으로 대비했습니다. 저자들이 명시적으로 이 용어를 사용하지는 않지만, DFQ는 양자화 연구에서 보정 데이터가 필요하지 않은 제로샷 양자화(ZSQ)의 설립 작업으로 볼 수 있습니다.\n\n![image](/assets/img/2024-06-22-QuantizingtheAIColossi_15.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChoukroun et al.의 2019 OMSE 방법은 커널별 양자화 매개변수를 찾아 양자화된 가중치/활성 텐서와 원본 간의 평균 제곱 오차(MSE)를 최소화합니다. 이는 최초의 4비트 양자화를 달성하는 PTQ 접근법으로, 정확도 손실을 최소화했습니다(상위 1 ImageNet 분류의 3% 감소). 저자들은 가장 효율적인 대칭 균일 양자화를 사용하기로 결정하여 옵셋 사용으로 인한 추가 계산을 절약했습니다. 주어진 커널에 대한 양자화로 인한 MSE와 스케일링 요소 사이의 관계는 비볼록적이기 때문에 저자들은 최적 값들을 찾기 위해 선 탐색 방법을 사용했습니다. 민감한 네트워크 레이어의 표현력을 유지하기 위해 혼합 정밀도를 사용하지 않기 위해, 저자들은 이러한 \"핵심\" 레이어를 여러 저정밀 텐서를 사용하여 나타내는 것을 제안했지만, 이 방법의 복잡성으로 인해 이를 작은 텐서에만 사용하는 것이 필요하며, 이 경우 CNN의 경우 컨볼루션 커널이 가장 민감한 구성 요소이기 때문에 작동이 잘 되지만, 큰 중요한 구성 요소가 있는 아키텍처에서는 잘 확장되지 못할 수 있습니다.\n\n2020년 초, HAWQ 논문의 저자들은 이전의 최첨단 ZSQ 벤치마크를 이긴 ZeroQ를 발표했습니다. 그들의 접근 방식은 새로운 파레토 프런티어 기반 방법을 통해 혼합 정밀도 양자화를 달성했으며, 이 방법은 수동 검색 없이 최적의 혼합 정밀도 설정을 자동으로 결정합니다. 훈련 또는 보정 데이터에 접근을 요구하지 않고, ZeroQ는 배치 정규화 레이어의 통계를 일치시킨 합성 데이터 세트를 생성하고, 이 데이터에 의해 생성된 활성을 사용하여 양자화 매개변수를 보정하고 레이어별 민감도 분석을 수행합니다. 이 민감도 값은 파레토 프런티어 선택 프로세스에 공급되어 특정 모델 크기나 원하는 정확도 수준에 대한 최적 설정을 찾습니다. 저자들은 대부분의 PTQ 작업이 일반적으로 복잡한 작업을 고려하지 않고 이미지 분류 정확도만 평가한다는 사실을 지적하여, 자신들의 방법이 더 어려운 물체 감지 작업에서도 성능을 유지한다는 것을 증명합니다. ZeroQ는 오픈 소스이며 계산 효율이 뛰어나며, 네트워크 양자화에 대한 진입 장벽이 낮습니다.\n\n그 후, 2020년에 AdaRound의 저자들은 반올림 접근법이 이전의 양자화 작업에서 우월했음에도 불구하고 최적이 아닌 라운드 스킴이 우세했음을 지적했습니다. 그들은 입력 데이터와 작업 손실의 특성을 모두 고려하는 반올림 효과를 분석하기 위한 프레임워크를 제안했으며, 반올림을 계층별 이차 제한 이진 최적화(QUBO) 문제로 공식화했습니다. 이들은 태스크 손실에 대한 가중치 변동의 변화를 둘째 순서 테일러 급수 전개를 사용하여 근사했으며, 다른 작품들이 민감도를 측정하기 위해 헤시안 정보를 사용하는 것과 같은 방식을 사용했습니다(Optimal Brain Damage (OBD)의 경우 1989년). Optimal Brain Surgeon (OBS)와 마찬가지로, 그들은 비대각 성분 헤시안 값을 깊게 이론적으로 분석하여 그들의 방법을 발전시켰습니다. 그들은 단순한 반올림 접근법을 헤시안의 대각 성분만 고려하는 것으로 동일시하여, 변동이 작업 손실에 대한 기여에 어떤 상호 의존성도 없다고 가정하여, 따라서 개별 크기의 감소만 중요하다고 주장합니다(즉, 반올림). 그러나 가중치 변동의 효과는 상호 관련되어 있으며, 비대각 정보가 중요하므로, 변동 조합이 실제로 손실에 유용할 때를 신호합니다. AdaRound는 소량의 레이블되지 않은 데이터만을 필요로하며, 1% 베이스라인 성능 내에서 ResNet 모델을 4비트로 압축하여 CNN에 대한 PTQ 최첨단 기술을 설정했습니다.\n\nAdaRound의 저자들은 명료하게 보여주기 위해 Gupta et al.(2015)의 확률적 반올림 방법을 사용하여 ResNet의 첫 번째 레이어에 대해 100가지 무작위 변형 세트를 생성하고, 이를 가장 가까운 반올림 변형과 비교했습니다. 100개의 샘플 레이어 변형 중 48개가 반올림에 비해 더 나은 성능을 제공했으며, 일부는 10% 이상의 정확도 향상을 제공했습니다. 이는 양자화 공간에 많은 더 나은 솔루션이 있음을 보여주며, PTQ의 정확성을 높이기 위해 이러한 솔루션을 타겟하는 방법이 있어야 한다는 명확한 신호를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 차트를 보면, 두 번째 차수 테일러 급수 항과 반올림 후 정확도 감소 간의 명확한 상관 관계를 볼 수 있습니다. 이는 양자화로 인한 작업 손실을 최적화하는 좋은 프록시라는 것을 나타냅니다. 그러나 네트워크 가중치 간의 교차 레이어 상호작용을 무시하더라도, 이차 헤시안은 여전히 대규모 네트워크 레이어에 대해 계산 비용이 지나치게 많이들어간다. 아래 결과는 AdaRound의 W4A8 구성이 FP32 기준선과 W8A8 DFQ 성능에 근접할 수 있는 것을 보여줍니다. 그러나 이 비교에서 중요한 점은, AdaRound가 DFQ 처럼 데이터 무료 또는 \"제로 샷\" 접근 방식이 아니라는 것입니다.\n\n2020년 중반에 AdaRound 후 며칠 후만에, AdaQuant은 더 나아가서 ResNet50을 사용하여 1% 미만의 ImageNet 상위 1 정확도 저하를 유지하면서 가중치와 활성화를 4 비트로 양자화할 수있는 새로운 PTQ 방법을 선보였습니다. 저자들은 AdaRound의 한계를 우회하는데 작은 보정 세트를 사용하여 가중치와 활성화 각 레이어에 대해 한 번에 한 층씩 채널 단위로 최소한의 계층별 양자화 MSE를 최소화합니다. 양자화 프로세스가 배치 정규화 통계에 내재된 편향과 분산을 도입한다는 것을 강조하며, 제안된 배치 정규화 튜닝 (BNT) 접근 방식으로 이러한 통계를 재추정하여 성능 저하를 복구하기로합니다. 저자들은 AdaQuant의 혼합 및 고정 정밀도 변형 모두를 제공하며, 해당 코드는 GitHub에서 이용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 섹션에서는 AlexNet 시대 이후 PTQ 방법에 대한 주목이 높아졌다는 것을 보았습니다. 이 시기의 PTQ 주요 동기는 보통 가장자리 배포와 데이터 개인 정보 보호에 대한 우려였지만, 여기에서 다룬 작업은 앞으로 급격히 증가할 모델 크기에 따른 PTQ 접근 방식에 대한 중요한 기초를 제공했습니다. CNN 시대를 마치기 전에 우리가 다뤄야 할 더 이상의 연구 동향 하나가 있습니다. 이것은 엣지에서 작동하고자 하는 욕망에 의해 동기부여되었던 또 다른 변화이며, 이것은 신경망을 1비트(이진 네트워크) 또는 2비트(삼진 네트워크)로 극단적으로 양자화하는 추세였습니다.\n\n## 극단적 양자화: 이진 및 삼진 네트워크\n\n극단적 양자화란  ≤2 비트로 압축하는 것을 의미하며, 이는 삼진(2비트)이나 이진(1비트)을 의미합니다. 모델을 이러한 정밀도 수준으로 효율적으로 압축하는 능력에는 명백한 장점이 있습니다. 모델은 FP32 대비 16–32배 작아지므로 더 적은 전력을 사용하여 더 작은 엣지 장치에 배포하고, 계산 속도를 최적화하기 위해 가치 있는 칩 실용면적을 절약할 수 있습니다. 신경망의 정밀도를 이렇게 낮추면 표현 능력이 상실되어 동일하게 극한의 도전과 함께 따라옵니다. 그러나 네트워크 계산에서의 비용이 막대하던 곱셈-누적(MAC) 연산은 이진 및 삼진 네트워크에서 훨씬 더 효율적인 덧셈/뺄셈 및 비트 시프트 연산으로 완전히 대체될 수 있어 잠재적 이득이 급격하게 증가하고, 이에 대한 도전에 동기를 부여하게 됩니다. 이 섹션에서 우리는 CNN 시대 중 극단적 네트워크 양자화 분야에서 발전한 흥미로운 결과를 확인하고, 낮은 비트 네트워크의 탁월한 효율성으로 인해 왜 매혹적이고 자주 인용되는 분야가 되었는지 알아볼 것입니다. 더불어, 이진 네트워크가 등가 비트 수를 사용하여 이진 산술의 모든 이점을 유지하며 성능을 초월할 수 있도록 앙상블을 형성할 수 있다는 점을 살펴볼 것입니다. 먼저, 우리는 2014년으로 되돌아가서 낮은 정밀도 네트워크에 대한 이해를 천천히 쌓아 보겠습니다.\n\n극단적 양자화의 선구적인 연구로서, 서울대학교의 황 및 성(2014)에 의한 연구 \"Weights +1, 0, and -1을 사용한 고정점 피드포워드 딥 신경망 설계\"는 성능 손실이 미미한 삼진(2비트) 가중치와 3비트 활성화 신호를 얻는 QAT 접근 방식입니다. 반면, 저자들은 편향은 성능을 보존하기 위해 8비트 정밀도로 할당되어야 한다는 사실을 관찰했습니다. 양자화 임계값은 원래 텐서와 양자화된 텐서 간의 MSE를 최소화하기 위해 초기에 선택되고, 그런 다음 각 계층에서 초기 제안을 연구하여 네트워크 출력 오류를 최소화하는 최적값으로 조정하기 위해 각각 계층별로 철저한 검색이 수행됩니다. 마지막으로, 양자화된 가중치는 양자화를 처리하기 위해 수정된 고정점 역전파 방식을 사용하여 세밀하게 조정됩니다. 저자들은 극단적 신경망 양자화 분야에서 기초적 선례를 만들었지만, 완전 탐색의 사용은 당시 작은 모델 크기의 예로 보여지며, 이후 커지는 모델 크기에 대한 확장 가능한 솔루션이 필요해질 것임을 시사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2015년에 Courbariaux 등이 \"BinaryConnect\" 알고리즘을 제안했습니다. 이름에서 알 수 있듯이 이들의 방법은 가중치를 -1 또는 1로 제한하는 이진화된 네트워크를 생성합니다. 저자들은 노이즈 가중치, 즉 확률론적 반올림 방식을 사용하여 이산화된 가중치가 \"확률적 경사 하강법(SGD)과 잘 호환된다\"고 언급하며, Gupta 등의 2015 논문에서 본 스토캐스틱 반올림에 관한 연구도 있습니다. Hwang \u0026 Sung의 방법과 같이 저자들은 양자화된 가중치에서 발생한 손실에서의 기울기를 적용하여 전체 정밀도 가중치(별도로 저장된)를 업데이트하며, 양자화된 가중치는 각 전방향 통과마다 전체 정밀도 가중치의 현재 상태에서 유도됩니다. 저자들은 결정론적 이진화(가중치의 부호를 단순히 취함)와 확률론적 이진화(가중치 크기를 활용하여 확률을 유도) 모두가 규제 메커니즘으로 잘 작동함을 보여주며(드롭아웃과 유사), 비 규제된 전체 정밀도 기준선과 비교하여 낮은 최종 검증 오류와 더 느린 수렴 곡선을 보여줍니다. 이러한 결과는 매우 흥미로우나 당시 기준에 따르면 CIFAR-10 데이터 집합이나 연구에서 훈련 중인 CNN이 특별히 복잡하다는 점을 고려해야 합니다. 따라서 현재 이러한 결과가 더 깊은 네트워크나 더 어려운 작업을 사용하여 유지될지 여부는 명확하지 않습니다. BinaryConnect의 코드는 GitHub에서 사용할 수 있습니다.\n\n![image](/assets/img/2024-06-22-QuantizingtheAIColossi_18.png)\n\n동일년도에 Lin 등의 \"Multiplications이 적은 신경망\"은 BinaryConnect의 작업과 코드베이스를 향상시켜 원본 저장소의 포크에 이진화된 가중치 및 활성화를 훈련시키기 위해 활성화 중 이진화되는 것을 가능하게 한 \"Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1”(BNN)으로, 저자들은 다시 결정론적이 vs 확률론적 이진화를 비교하며, 확률적 이진화가 이론적으로도 경험적으로도 우수함을 관찰하고, 훈련 중에 랜덤 비트를 생성할 필요가 없어진다는 단순함으로 결정론적 이진화가 매력적이라고 생각합니다. 따라서 저자들은 훈련 중에 활성화에만 확률적 이진화를 사용하는 최상을 지향합니다. 더 나아가, 신경망에서 배치 정규화(BN) 레이어에 필요한 비용이 많은 곱셈에 대한 접근도 다루어, 비용이 많이 드는 연산을 피하고 비트 시프트 연산을 통해 네트워크 연산의 이진화 성질을 유지하는 방법을 제안합니다. 이에 더해, 저자들은 아담 옵티마이저에 의해 보편적으로 사용되는 곱셈을 피하기 위한 비용이 많이 드는 곱셈을 피하는 방법을 제안하며, 이러한 근사치에 의해 도입된 노이즈가 훈련 결과에 영향을 주지 않음을 보여줍니다. CIFAR-10 데이터 집합의 기본 CNN를 다시 한번 테스트한 결과, 저자들은 가중치와 활성화를 이용하여 신경망을 훈련하는 데 강력한 결과를 보여줍니다. Binarized Neural Networks (BNNs)의 훈련을 위한 코드와 사용자 정의 GPU 커널이 함께 제공됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-22-QuantizingtheAIColossi_19.png)\n\n2016년 Rastegari 및 다른 연구자들의 XNOR-Net 논문은 대규모 ImageNet 데이터셋에서 CNN 이진화를 테스트한 첫 번째 논문으로, BinaryConnect 및 BNN (aka BinaryNet)이 이러한 규모에서 잘 작동하지 않음을 보여주었습니다. 저자들은 가중치 이진화의 새로운 방법을 도입하여 BNN 결과를 이상 준 16.3%로 개선하였습니다. 가중치 스케일링을 통합하는 이진화 방법을 도입하며, 주어진 가중치 행렬에 대한 최적 스케일링 요인을 그것의 절대값의 평균으로 확인했습니다. BinaryConnect 및 BNN과 유사하게, 저자들은 단순화된 전방 통과를 통해 계산된 기울기를 사용하여 별도의 정밀 가중치 세트를 업데이트하며, BN과 Adam에 대한 시프트 기반 근사화를 사용하지 않았습니다. 그들의 Binary-Weight-Network (BWN) 설정으로부터 XNOR-Net 설정으로의 성능 하락은 ImageNet과 같은 복잡한 작업에서 활성화 신호를 이산화하는 어려움을 강조하지만, 이러한 표현을 이진화하는 것은 특히 야심찹니다. 그럼에도 불구하고, 가중치 전용 이진화가 기본 CNN과 같은 성능을 달성할 수 있다는 사실은 “무료 점심” 압축에 대한 또 다른 유망한 기회인 것으로 보입니다. 또한, XNOR-Net 버전이 신호 전체 이진화로도 합리적인 성능을 달성할 수 있는 사실은 설득력이 있습니다. XNOR-Net의 코드도 GitHub에서 제공됩니다.\n\n![이미지](/assets/img/2024-06-22-QuantizingtheAIColossi_20.png)\n\n2016년 Li 및 다른 연구자들의 “Ternary Weight Networks” (TWN)은 처음부터 삼진 네트워크를 훈련하는 QAT 접근 방식입니다. 그들의 양자화 방법은 학습된 레이어별 스케일링 요소와 각각의 가중치 텐서 당 평균 크기의 3/4로 설정된 삼진화 임계값을 사용하여 양자화된 가중치와 원시 가중치 간의 유클리드 거리를 최소화하려고 합니다. 저자들은 삼진 네트워크가 “이진 정밀도 대응물보다 더 나은 표현 능력을 보여준다”고 관찰하고, 이 개념을 이진 대 삼진 3x3 합성 필터의 예를 통해 증명했습니다. 이러한 실험은 이 추가적인 표현 능력이 MNIST, CIFAR-10, ImageNet 및 Pascal VOC 객체 감지 작업을 포함한 다양한 작업에서 유용하다는 것을 증명했습니다. 아래 결과에서 우리는 삼진 네트워크의 이 추가적인 표현 능력이 ImageNet 및 Pascal VOC 작업과 같이 더 어려운 작업에서 특히 유용하다는 것을 볼 수 있습니다. 이 복잡성이 커지면 위에서 보았던 덜 복잡한 모델과 작업에서의 매우 유망한 이진화 결과에서 느껴던 희망론이 지속되지 않을 수도 있다는 신호일지 모릅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-QuantizingtheAIColossi_21.png\" /\u003e\n\n2016년 말, 후, 요, 그리고 궉이 \"Loss-aware Binarization of Deep Networks\" (LAB)를 발표했습니다. 이 연구는 이전 방법들이 비트 이진화 과정을 비용 함수에 대한 직접적인 영향을 최적화하지 않았던 빈 연구 분야를 채웠습니다. 비용 함수를 최소화하는 방식으로 네트워크를 이진화하기 위해 저자들은 Adam 옵티마이저에서 포착된 2차 그래디언트 정보를 사용하여 대각 헤시안 근사치를 효율적으로 추출하기 위해 근사 뉴턴 알고리즘을 해결합니다. 저자들은 자신들의 방법이 \"넓고 깊은 네트워크에 대해 보다 견고하다\"고 보여주었으며, 또한 Recurrent-Neural Networks (RNNs)를 사용한 NLP 작업으로 연장되었습니다. 그 후 2018년 초, 후와 궉은 LAB 알고리즘을 확장하여 \"Loss-Aware Weight Quantization of Deep Networks\" (LAQ 또는 삼항 경우의 LAT)를 생성하여 제안된 방법이 LAB가 제공하는 이진화보다 더 개선된 결과를 보여 주었습니다.\n\n\u003cimg src=\"/assets/img/2024-06-22-QuantizingtheAIColossi_22.png\" /\u003e\n\n2017년에 동 등이 확률 양자화 알고리즘을 제안했습니다: 하나의 층 또는 필터의 양자화 비율에 반비례하게 양자화 오류에 대해 네트워크 요소/필터의 일부만 양자화하여 각 훈련 단계마다 따로 업데이트하고 완전 정밀 가중치에서 별도로 업데이트합니다. 훈련이 진행되면 결국 모든 가중치가 양자화되고 결과적으로 저비트 네트워크는 해당하는 BWN 및 TWN 모델보다 훨씬 뛰어난 정확도를 유지합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![INQ](/assets/img/2024-06-22-QuantizingtheAIColossi_23.png)\n\n이전에 CNNs 섹션에서 본 2017 증분 네트워크 양자화(INQ) 논문은 2015년 Deep Compression에 의해 설정된 모델 압축의 이전 최고 수준을 초과했습니다. 또한, 이들의 방법이 삼진 네트워크를 작성하는 데의 타당성을 조사했습니다. 아래 차트에서, ImageNet 분류 작업에 학습된 ResNet-18에서 TWN보다 INQ 방법이 더 강력함을 확인할 수 있습니다. TWN의 36.18% 상위 1위 오류율을 2% 이상 내리는 것을 볼 수 있습니다.\n\n![ABC-Net](/assets/img/2024-06-22-QuantizingtheAIColossi_24.png)\n\n나중에 2017년 Lin 등의 '정확한 이진 합성곱 신경망으로'라는 논문에서는 이진 네트워크의 표현력 부족을 극복하기 위해 여러 세트의 이진 가중치 또는 활성화를 결합하여 더 정확하게 고정밀 값을 나타냈습니다. 3-5개의 가중치 베이스 및 5개의 이진 활성화를 사용함으로써 ImageNet에서의 정확도 저하를 5%로 낮출 수 있다고 보여주었습니다. 저자들은 이것이 더 많은 비트를 필요로 하지만 더 복잡한 산술 연산자 사용을 피하기 때문에 고정비트 고정소수점 표현 대신에 선호되는 방법임을 강조했습니다. 이들의 연구는 이진 신경망이 ImageNet에서 완전 정확도 기준선과 유사한 성능을 처음으로 달성한 것을 나타냅니다. 그러나 이들의 솔루션은 기준 BNN 복잡성을 O(k * l)만큼 높이기 때문에 효율성에 상당한 손실이 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![2024-06-22-QuantizingtheAIColossi_25.png](/assets/img/2024-06-22-QuantizingtheAIColossi_25.png)\n\n2018년 Zhu 등의 '이진 앙상블 신경망: 네트워크 당 더 많은 비트 또는 비트 당 더 많은 네트워크?' (BENN) 논문은 BNN의 한계가 이진화 프로세스의 추가 최적화를 통해 해결할 수 없다고 주장했습니다. 이는 이진 공간의 표현 능력 부족에서 비롯된 것이라고 밝혔습니다. 저자들은 이진 네트워크를 사용할 때 예측 분산을 줄이고 잡음에 대한 강건성을 향상시키기 위해 부스팅 또는 배깅을 사용하여 여러 BNN 앙상블을 만들었습니다. 실험 결과 앙상블 분류기의 통계적 특성이 개선되고 성능이 급격하게 향상되었음을 보여줍니다. 이러한 앙상블을 실행하는 추가 복잡성은 O(k)이며 ABC-Net의 실행 효율성을 l의 배로 향상시키면서 ImageNet에서 그것을 크게 앞섰다. 또한 앙상블을 병렬화할 수 있기 때문에 솔루션은 O(1)의 추가 복잡성을 가질 수 있으며, 기준이 되는 BNN과 동일한 속도로 실행될 수 있습니다.\n\n![2024-06-22-QuantizingtheAIColossi_26.png](/assets/img/2024-06-22-QuantizingtheAIColossi_26.png)\n\n이 부분에서는 2012년 AlexNet의 대성공 이후 시작된 대형 CNN의 등장으로 인해 극단적인 신경망 양자화에 대한 관심이 폭발적으로 증가했습니다. 이 시대의 모델링 능력의 높은 정점을 에지 장치에 저전력 하드웨어로 배치하고 싶다는 강력한 충동은 매혹적이었습니다. 이것이 많은 딥러닝의 실용적인 응용 분야가 존재하는 곳입니다. 이 부분의 대부분 방법은 사전 훈련 된 가중치의 양자화 없이 로우-비트 네트워크를 처음부터 훈련하는 것을 볼 수 있습니다. 이는 그들의 탁월한 효율성으로 완전히 양자화된 방법으로 더 쉽게 처리될 수 있습니다. 나중에 우리는 이런 극단적인 양자화 방법론의 이 인상적인 시기가 미래의 앞길을 이룰 재발견의 유망한 토양으로 성숙해 나갈 것임을 볼 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금은 CNN 시대에 대한 장을 닫습니다. 이제는 AlexNet의 성공 이후 깊은 학습 분야로 인재와 자금이 대량으로 유입되면서 양자화 연구의 여러 독특한 분야가 형성되는 것을 목도하는 것을 보았습니다. 우리는 가중치 양자화를 세밀하게 조정하여 성능을 잃지 않고 압축 수준이 높아지는 QAT 접근 방식, 민감도 분석을 양자화 과정에 통합함으로써 새로운 수준의 압축을 달성하는 혼합 정밀도 기법, 재훈련 없이 8 또는 4 비트의 정밀도에서 기준 성능에 근접하는 PTQ 접근 방식(특히 ZSQ의 경우 보정 데이터를 사용하지 않음), 그리고 극도의 양자화 연구의 부상을 보았습니다. 이제 우리는 CNN 시대에서 2017년 트랜스포머 아키텍처의 성공으로부터 태어난 NLP에 대한 연구 관심의 물결로 초점을 옮깁니다.\n\n# LLM 양자화\n\n이제 우리가 양자화의 기능적 세부 정보와 역사에 익숙해졌으므로 오늘날의 대형 언어 모델(LLMs)을 양자화하는 논의를 진행할 수 있습니다. 가지치기와 마찬가지로, 대형 모델로 진입함에 따라 AlexNet의 성공 작위가 아닌 모델 훈련을 필요로하는 압축 기술을 적용할 희망은 대규모 피팅 이상의 회사를 제외하고는 희망이 점점 사라짐을 볼 것입니다. 따라서 우리는 PTQ의 더 가벼운 방법에 대한 연구 초점의 변화를 볼 것이지만, 대규모 환경에서의 고유한 도전에도 불구하고 연구 공동체의 독창성이 QAT의 혜택 활용 방법을 찾는 방법을 막지 않았음을 볼 것입니다. 이 섹션에서는 먼저 2017 논문 \"Attention is All You Need\"에서 트랜스포머의 발표 이후부터 2020년에 175억(몇난 파라미터) GPT-3의 방대한 출시로 점지된 LLM 시대에 발생한 양자화 노력을 검토합니다. 그런 다음 LLM 시대에서의 PTQ 기법의 확산을 검토한 후, LLM용 QAT 접근 방식으로 초점을 옮기고, 마지막으로 LLM의 극도의 양자화를 검토함으로써 조사를 마무리합니다. 이 섹션은 양자화에 대한 교육을 완료하여 다음 섹션의 구현 가이드로 나아가고, 우리 자신의 작업 흐름에서 신경망 양자화를 적용하기 시작할 준비를 마칩니다.\n\n## 트랜스포머 초기 시대의 양자화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지난 섹션에서 보았듯이, 컴퓨터 비전에서의 AlexNet의 성공은 효율적인 배포를 위해 CNN을 양자화하는 연구에 대한 관심 폭발을 일으켰지만, 양자화의 적용에 대한 연구는 몇 년 후에야 언어 모델에서 속도를 내게 되었습니다. 카탈리스트적 순간은 트랜스포머 아키텍처의 놀라운 성공이었습니다. 2020년 GPT-3의 출시와 함께 시작된 언어 모델 크기의 폭발적인 성장 이전에는 트랜스포머를 사용한 NLP의 더 합리적인 크기의 탐구가 이루어졌으며, 그러나 이러한 모델이 점점 커질수록 성능이 계속 향상되는 경향이 명확해지고 있었습니다. 여기서 우리는 트랜스포머 기반 언어 모델의 양자화 연구 형성기간을 검토하며, 트랜스포머의 출현과 수십억 개 매개변수 트랜스포머 네트워크의 부상 사이의 형성 기간 동안 발생한 양자화 연구를 살펴볼 것입니다. 이제 볼 것처럼 이 새로운 네트워크 아키텍처는 특히 낮은 비트 폭에서 양자화에 독특한 도전 과제를 제기했고, 연구자들은 신속히 이를 이해하고 극복하기 위해 노력했습니다.\n\n2018년 말 Google이 발표한 중요한 양방향 인코딩 표현 트랜스포머(BERT)는 여전히 매우 영향력 있는 트랜스포머 기반 언어 모델입니다. BERT는 가려진 언어 모델링(MLM) 목적으로 학습하여 입력 토큰 이후와 이전의 정보를 고려하는 임베딩을 생성하는 방법을 학습하여 양방향 콘텍스트를 인코딩하는 방법을 학습합니다. 결과적으로 이러한 표현은 깊은 맥락 이해를 포함하며, 감정 분류 및 질문 응답과 같은 작업에 유용합니다. BERT는 인코더만 사용하는 트랜스포머 아키텍처의 변형을 사용하여 이러한 양방향 인코딩 표현을 생성하는 반면, OpenAI의 유명한 Generative Pretrained Transformer (GPT) 모델은 디코더만 사용하여 바이트-페어 인코딩 (BPE) 텍스트의 자기 회귀 모델링 작업을 수행합니다. 시퀀스의 이전 토큰만 고려하며 다음 토큰을 반복적으로 예측합니다. GPT-2 논문에서 이루어진 자가 회귀 디코더 전용 트랜스포머 및 그들의 미탐색 사전 훈련 데이터셋에 대한 잠재적 확장성에 대한 발견은 결국 2020년 175B GPT-3 거대한 모델의 출시를 영감으로 삼았지만, 우리가 보게 될 것처럼, 트랜스포머 양자화 연구는 이미 그때부터 이미 즐비하게 진행 중이었습니다.\n\nUC 버클리의 Shen 등은 2019년 말에 Q-BERT를 발표했습니다. 이 방법은 HAWQ의 헤시안 기반 민감도 분석을 확장하여 교육 데이터 하위 샘플 전체의 평균뿐만 아니라 헤시안 스펙트럼의 분산을 고려하는 방식이었습니다. Q-BERT는 이 향상된 민감도 측정을 사용하여 계층별 혼합 정밀도 양자화 체계를 수립하고, 그룹 단위의 세분화를 사용하여 각 계층을 해당 비트 폭에 맞게 양자화한 후, 각 계층에 대해 자체 양자화 범위와 룩업 테이블을 가지는 하위 단위로 행렬을 분할합니다. 저자들은 이 방법을 사용하여 가중치 압축률을 13배, 활성화 크기를 4배 작게하고, 임베딩 크기를 4배 줄이면서 베이스라인 BERT에서 최대 정확도 감소율이 2.3%를 달성했습니다. 대부분의 QAT 기술과 마찬가지로 Q-BERT는 비가역적 양자화 함수를 통해 그레이디언트를 근사하기 위해 STE를 사용합니다.\n\n혼합 정밀도를 사용하여 압축을 극대화하는 데 중점을 둔 Q-BERT와 대조적으로, Intel의 Zafrir 등이 2019년 말 동시에 발표한 Q8BERT는 모델과 활성화 사이에서 균일한 8비트 양자화를 적용하는 데 초점을 맞췄습니다. 이는 하드웨어 최적화 관점에서 더 유리하며, 혼합 정밀도 연산은 오버헤드가 추가되고 일반화된 하드웨어 가속에 적합하지 않기 때문입니다. 이 방법은 학습된 BERT 모델에 대한 QAT를 미세 조정 단계로 수행하여, 이전에 CNN 양자화 섹션에서 보았던 Jacob 등의 2017 논문을 기반으로 한 시뮬레이션 양자화 접근법을 사용하여 1% 이하의 최소 정확도 손실과 함께 4배의 압축을 달성합니다. 가중치의 스케일링 요소는 최대 절대 가중치 값에 대해 보정되고, 활성화의 스케일링 요소는 학습 중 누적된 지수 이동 평균에 기초합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2020년에 팬과 다른 분이 Quant-Noise를 소개했어요. 이 기술은 모델 압축률을 높이기 위한 QAT 기법으로, 훈련 중 각 순방향 패스에서 네트워크 가중치의 일부만 무작위로 양자화하여 가중치의 대부분이 STE 근사치에 의해 도입된 오차 없이 업데이트를 받게 합니다. 이렇게 하면 가중치의 값이 양자화된 부분의 영향을 줄이는 방식으로 더 정확하게 변화할 수 있어 시간이 지날수록 네트워크 가중치 전체에 QAT를 동시에 적용할 때보다 우수한 결과를 얻을 수 있어요. Quant-Noise는 다중 가중치를 단일 코드 워드로 함께 양자화하는 Product Quantization (PQ)의 사용을 탐구하며, 상대적으로 낮은 성능 하락으로 매우 높은 수준의 압축을 가능하게 합니다. 이 방법은 네트워크 구조에 의해 유도된 가중치 간의 상관 관계를 활용해요.\n\n2020년에 토론토 대학교의 자데 분이 GOBO를 발표했어요. 이는 어텐션 기반 모델을 위한 사전 정의된 사전을 사용하는 PTQ 방법으로, BERT의 거의 모든 FP32 가중치를 정확도를 손상시키지 않고 3비트로 압축할 수 있게 합니다. GOBO 방법은 가우시안 분포 적합을 사용하여 이상값을 보전하고 나머지 가중치는 대표적인 FP32 센트로이드가 있는 소규모 세트(3비트의 경우 8)를 인덱싱하는 3비트 코드 워드로 저장합니다. 일반 하드웨어는 3비트 값에 직접 작업할 수 없기 때문에 GOBO 저자들은 양자화 방법을 보완하기 위해 효율적인 3비트 계산 가속을 가능하게 하는 혁신적인 하드웨어 아키텍처를 개발했어요. 이 가속화 이점은 전용 하드웨어를 사용해야 완전히 활용할 수 있지만, 3비트 양자화 모델의 감소된 메모리 풋프린트는 일반적인 하드웨어에서 메모리 저장 및 트래픽을 줄여 추론 대기 시간과 에너지 소비를 일부 개선할 것입니다. GOBO 방법은 'Deep Compression'에서 본 몇 개 대표값 (센트로이드)의 사전 내 Huffman 부호화된 가중치 저장을 영감으로 한 것이지만, 가중치 이상값을 양자화하지 않고 정확도를 크게 향상시키고, K-평균보다 9배 빨리 수렴하는 신속한 \"센트로이드 선정 알고리즘을 사용합니다.\n\n장과 다른 분의 2020 TernaryBERT는 지식 증류 (TinyBERT에서 영감을 받은)와 삼진화 양자화를 결합한 distillation-aware ternarization을 통해 full-precision과 동일한 크기의 선생님 모델에 학생 모델로서 삼진화된 모델을 취급하는 방식입니다. 그들의 증류 방법은 transformer 층의 임베딩 레이어, 출력 및 어텐션 점수, 소프트 교차 엔트로피를 사용하여 선생님과 학생 간의 MSE 최소화를 포함합니다. TernaryBERT는 full-precision 베이스라인과 비교 가능한 성능을 달성하면서 14.9배 작아졌어요. 저자들은 BERT 활성화에 대해 min-max 8비트 양자화가 가장 잘 작동한다고 발견했으며, 특히 초기 층에서 BERT 활성화의 분포가 음 값으로 치우쳐 있다는 점을 발견했어요. 그들의 QAT 프로세스는 가중치를 full-precision 모델에서 초기화하고, 양자화 함수를 통해 그레이디언트를 근사하는 STE를 사용합니다. \n\n2020년에 BinaryBERT는 BERT 양자화를 극단으로 가져가려고 하지만, 이진 BERT 모델은 고도로 불규칙한 손실 랜드스케이프 때문에 직접 훈련하기 어렵다는 것을 관찰했어요. 따라서 저자들은 반대로 반씩 줄어든 삼진 네트워크를 훈련하고, 자신들이 제안한 삼진 가중치 분할 방법을 통해 이를 이진 네트워크로 변환한 후 결과물을 미세 조정하여 BERT가 24배 압축되었을 때 성능 하락을 최소화합니다. 이 개념을 적응 분할을 포함하여 확장하여 중요한 레이어를 분할하고 민감하지 않은 레이어를 이진 형태로 기술함으로써 다양한 모델 크기 제약에 유연성을 줍니다. 대부분의 QAT 방법들처럼 BinaryBERT는 STE을 사용하여 훈련합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-QuantizingtheAIColossi_27.png\" /\u003e\n\n## 사후 훈련 양자화 (PTQ)에 대한 LLMs\n\n이 기사를 읽는 많은 분들에게, 이것은 가장 관련성이 높은 연구 섹션일 것입니다. 여기서는 대부분의 사람들이 이들 사전 훈련된 거대 모델에서 사용할 수 있는 유일한 양자화 유형인 LLMs의 PTQ를 검토합니다. 마침내 LLM 양자화로 전환하면, 이 거대한 모델들은 일반적으로 FP16으로 훈련되므로 \"전체 정밀도\"의 정의가 이 시대에 FP16을 의미하게 되므로 중요합니다. FP32를 사용하면 성능 향상이 미미한 대신 미니멈한 스토리지 요구 사항이 두 배로 증가하기 때문에 FP16에서 훈련되고 있습니다. 그러므로 LLMs의 압축률을 논의할 때 무엇이냐면, 8비트 정수 양자화는 “전체 정밀도” 베이스라인 대비 메모리 풋프린트를 2배로 줄일 뿐입니다. 게다가, 양자화에 어려움을 겪는 이러한 규모의 네트워크는 드물지만 매우 중요한 outlier들이 활성화되어 나타난다는 점을 알게 될 것입니다. 그러므로 LLMs가 더 쉽게 압축할 수 있는 대형 CNNs 만큼 매개변수가 과다하다는 사실에도 불구하고, 이들의 특이점으로 양자화하기가 더 어려워집니다. 다행히도 연구 커뮤니티는 빠르게 문제를 진단하고 LLMs의 효율적이고 정확한 PTQ를 극복할 수 있는 공식을 제안했습니다.\n\n2020년에 OpenAI에 의해 폐쇄 소스 175B GPT-3가 출시된 두 년 후인 2022년 중반, 가장 큰 오픈 소스 모델은 GPT-J 6B와 GPT-NeoX 20B였으며, 175B 규모의 OPT와 BLOOM 모델은 아직 공개되지 않았습니다. 이 기간에 Microsoft의 Yao et al.은 GPT-3 스타일 transformer 모델의 양자화에 대한 조기 조사인 ZeroQuant를 발표했습니다. 그들의 방법론은 가중치와 활성화에 대해 INT8 양자화를 사용하여 FP16 모델과 유사한 정확도를 달성할 뿐 아니라 효율성을 5.2배 높일 수 있었습니다. 저자들은 또한 덜 민감한 가중치를 레이어별 지식 증류(LKD) 접근법을 제안하여 훈련 데이터가 필요 없이 4비트로 더 압축할 수 있도록 하였습니다. 이로써 FP16 모델보다 3배 더 작은 메모리 풋프린트를 갖게 되었습니다. 크기가 큰 transformer 모델에 PTQ를 적용할 때 관찰된 성능 급격한 하락을 조사하기 위해 저자들은 레이어들 간에 발생하는 활성화(토큰)의 매우 동적 범위를 살펴보고, 레이어별 최솟값/최댓값에 의해 조절된 균일한 양자화 범위를 적용하면 이러한 분포의 밀집 영역에서 효과적인 표현력을 잃게 됨을 지적합니다. 마찬가지로, 가중치 행렬은 로우-테일 범위를 갖고 있어 소수의 균일 간격 양자화 수준에서 미세한 세부 사항을 잃게 될 것입니다. 아래 차트는 이러한 레이어별 특성을 나타내며, 이러한 범위를 균일한 그리드로 잘라낼 경우 많은 정보가 손실됨을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 범위의 분산을 다루기 위해 저자들은 가중치의 그룹별 양자화와 활성화 함수의 토큰별 양자화를 사용하기로 제안했습니다. 이들은 또한 이러한 대규모 트랜스포머 모델의 활성화 범위의 큰 분산을 고려하여 재보정 데이터 집합을 사용한 정적 양자화 설정에서 범위를 보정하는 것이 문제를 일으킬 가능성이 높다는 점을 지적했습니다. 그래서 그들은 추론 중에 동적으로 토큰별 최소/최대 범위를 보정하기로 선택했습니다. 토큰별 양자화에 따른 데이터 이동 오버헤드를 극복하기 위해, 저자들은 커널 퓨전 기술을 사용하여 최적화된 추론 백엔드를 구축했습니다. 이 기술은 각 양자화 오퍼레이터를 이전 오퍼레이터(예: 레이어 정규화)와 퓨전시킴으로써 작동합니다. 저자들은 LKD 방법론을 구성하여 매우 큰 모델의 지식 증류의 세 가지 제약을 해결하였습니다. 즉, 두 모델을 모두 메모리에 보관해야 하는 필요성, 학생을 완전히 교육해야 하는 필요성, 그리고 원본 훈련 데이터가 필요한 제약 등이 있었습니다. 이 문제들을 해결하기 위해, 저자들은 양자화 모델의 단일 레이어를 최적화하고, 양자화된 레이어와 대응되는 전체 정밀도 레이어를 통해 가짜 레이블을 생성하는 데 같은 선생님 활성화 집합을 사용합니다. 이렇게 레이어별로 생성된 가짜 레이블만 사용되기 때문에 LKD 프로세스에 레이블이 필요하지 않으며, 원래의 훈련 데이터를 사용할 필요가 없음을 보여주었습니다. ZeroQuant은 DeepSpeed 라이브러리의 일부로 공개되었으며, 이는 Hugging Face의 Transformers 라이브러리에 통합되어 있습니다.\n\n2022년 Tim Dettmers와 그의 동료들은 LLM.int8(), 즉 GPT3.int8()을 소개하였습니다. 이는 ≥175B 파라미터 규모의 LLM에 대해 정확도를 보존하는 8비트 정수 PTQ 방법입니다. 먼저, 이들은 대부분의 특징들을 벡터별 양자화 정밀도를 사용하여 양자화하였습니다. 행/열 단위 내적 중에 각기 다른 정규화 상수를 사용하여 행렬 곱셈을 했습니다. 두 번째로, 저자들은 기존의 PTQ 방법이 ≥6.7B 파라미터가 포함된 트랜스포머에는 실패하는 이유는 이 규모에서 발생하는 변압기 계층의 극단적인 이상치의 증가 때문이라는 사실을 발견하였습니다. 이러한 이상치는 변경에 예민하여 제거하면 모델 성능이 저하됩니다. 따라서 저자들은 두 번째 단계로서 이상치를 별도로 처리하기로 선택하였고 혼합 정밀도 분해를 사용하여 이러한 이상치를 처리하고 8비트 양자화된 모델 성능을 수백 억 개의 파라미터까지 유지됨을 입증하였습니다. 이 코드는 bitsandbytes 라이브러리에 공개되었으며, 이는 Hugging Face 생태계에도 통합되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGPTQ은 가중치만 양자화하므로 비양자화된 활성화와 혼합 정밀도 상호작용으로 인해 곱셈 작업의 효율성 향상을 제공하지 않습니다. 그러나 감소된 메모리 풋프린트는 추론을 위해 필요한 GPU 하드웨어의 크기를 줄이는 데 중요합니다. 저자들은 오픈 소스 GPT-3 동등 버전인 OPT-175B를 처음으로 단일 A100 GPU에서 실행할 수 있었으며, Nvidia GPU에서 테스트된 GPU 커널의 릴리스를 통해 추론을 대략 4배 빠르게 할 수 있었습니다. GPTQ는 오픈 소스이며, Hugging Face를 포함한 여러 인기 있는 LLM 배포 프레임워크로 널리 채택되었습니다.\n\n2022년 말 MIT(Massachusetts Institute of Technology)와 Nvidia 사이의 협업에서 SmoothQuant는 완전히 양자화된 활성화를 갖는 대규모 모델에서 LLM.int8()의 8비트 PTQ 성능과 일치하는 메서드를 보여주었습니다. 고정밀도 이상치를 유지하기 위한 필요성을 바이어스의 가중치로 이동시켜 높은 균일성과 양자화하기가 쉽다는 점에서 이 방법이 가능했습니다. 아래 다이어그램은 이 개념에 대한 직관적 시각적 참조를 제공합니다.\n\n![다이어그램](/assets/img/2024-06-22-QuantizingtheAIColossi_29.png)\n\nSmoothQuant의 저자들은 LLM에서 토큰별 양자화가 활성화 이상치와 부합하지 않기 때문에 거의 이익이 없다고 제안했습니다. 이들은 채널별 양자화를 사용하여 LLM 정확도를 대부분 쉽게 유지할 수 있음을 보여주었습니다. 그러나 채널별 양자화는 하드웨어 가속화된 INT8 일반 행렬 곱셈(GEMM) 커널과 호환되지 않기 때문에, 저자들은 예측 가능한 채널별 이상치 표현을 활용하여 이에 대응하는 가중치를 조정하여 이를 이상치 값을 줄이고 활성화 텐서를 양자화하기에 더 적합하게 만드는 방법을 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSmoothQuant의 세 가지 구성 (O1-O3)이 테스트되었습니다. 이들은 모두 텐서별 가중치 양자화를 사용하지만 활성화를 양자화하는 방식이 다릅니다. O3는 활성화에 대해 텐서별 정적 양자화를 사용하여 스케일링 요소가 오프라인에서 보정된 가장 낮은 복잡성을 가집니다. O2도 텐서별 활성화 양자화를 사용하지만 실행시 통계를 사용하여 온라인에서 스케일링 매개변수를 보정하는 동적 양자화를 사용하여 성능을 향상시킵니다. O1은 보다 복잡한 토큰별 동적 양자화를 탐구하지만 O2보다 약간 더 나은 결과를 달성할 뿐입니다. SmoothQuant 방법에서 LayerNorm 및 SoftMax 레이어는 여전히 FP16에서 계산되므로 정수 전용이 아니며 8비트 미만에서는 작동하지 않습니다. 그러나 이것은 LLM의 양자화를 어렵게 만드는 특수성을 극복하는 중요한 진전을 나타냅니다.\n\n중간 2023년에 ZeroQuant의 저자들은 ZeroQuant-V2로 돌아와 양자화 오차를 줄이기 위해 과도한 세분화를 증가시키는 Fine-Grained Quantization (FGQ) 접근법을 제안했습니다. 여기서 블록-k 양자화라고 불리는 블록-k 양자화, 가중치 행의 각 서브 벡터에 대해 스케일링 요소와/또는 제로 지점을 설정합니다. 또한 저자들은 층별 양자화 오차를 상쇄하기 위해 작은 학습 가능한 저랭크 행렬을 사용하는 Low-Rank Compensation (LoRC)를 적용합니다.\n\nLin 등의 중간 2023년 성명 \"Activation-aware Weight Quantization for LLM Compression and Acceleration\" (AWQ)는 가중치만을 대상으로 한 저비트 양자화 방법입니다. 저자들은 GPTQ가 매우 효과적이지만 교정 세트에 과적합될 가능성이 있어 LLM과 같은 종합적 모델에는 문제가 될 수 있다고 지적합니다. 모든 가중치가 동등하게 중요하지 않다는 것과 매우 중요한 네트워크 가중치의 0.1-1%를 양자화하지 않으면 양자화로 인한 성능 저하가 크게 감소한다는 것을 저자들은 지적합니다. 저자들은 중요한 가중치를 식별하기 위해 가중치 자체가 아닌 활성화 신호를 살펴보고 양자화 오차를 최소화하기 위해 채널별 스케일링을 적용합니다. 저자들은 \"AWQ는 역전파나 재구성에 의존하지 않으므로 검교합 집합에 과적합되지 않고 LLM의 일반화 능력을 다양한 도메인 및 모달리티에서 잘 보존할 수 있습니다.\" 라고 언급합니다. 저자들은 이 일반화가 Vicuna와 같은 명령어에 튜닝된 LLM과 OpenFlamingo와 같은 다중 모달 LMs (LMMs)의 성능 유지에 중요하다는 점을 보여줍니다. 저들의 방법은 Llama-2-70B 모델을 64GB Jetson Orin AGX에 맞출 수 있게 하거나 13B 규모의 모델을 8GB RTX-4070 GPU에서 \"초당 30개의 토큰으로 상호 작용하는 속도로 실행\"할 수 있게 합니다. AWQ의 효과가 인기있는 오픈 소스 LLM 서비스 소프트웨어에서 널리 채택되었습니다. (실행 가이드에서 더 자세히 다룰 예정)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-QuantizingtheAIColossi_31.png\" /\u003e\n\n2023년 중반, SqueezeLLM은 GPTQ와 AWQ의 성능을 능가했습니다. 저자들은 최적의 비트 정밀도 할당을 찾기 위해 민감도 기반의 비균질 양자화와 \"이상값 및 민감한 가중치 값을 효율적인 희소 형식으로 저장하는\" Dense-and-Sparse 분해를 제안했습니다. 3-bit 환경에서 SqueezeLLM은 다른 최신 PTQ 방법과 비교하여 복잡성의 증가를 절반으로 줄였습니다. 게다가 양자화된 모델은 FP16 기준보다 2.3배 빠른 추론 속도를 제공합니다. 대부분의 방법이 간단한 응용프로그램을 제공한다는 간단한 적용 사례에 의해 주도되는 다양한 입방체의 매개변수로 통제되는 균일 양자화를 사용한다는 점을 감안할 때, SqueezeLLM은 가중치 분포가 명확한 비균일 패턴을 보여준다고 지적하며, 비균일 양자화가 자연히 더 나은 표현을 제공할 것이라고 주장합니다. 더 나아가 저자들은 대부분의 작업이 가중치만 양자화하고 FP16에서 산술을 수행하기 때문에 균일 양자화의 장점이 결국 제대로 이해되지 않는다고하며, 표현 능력을 낮은 정밀도로 보존하는 이상값에 미치는 영향이 적은 비균일 양자화가 선호된다고 주장합니다. 게다가 저자들은 가중치 이상값을 효율적인 희소 표현으로 분해하고 격리시킴으로써 분포를 양자화하기 쉽도록 만들었으며, 이를 통해 양자화가 더욱 용이해졌습니다. SqueezeLLM의 코드는 GitHub에서 이용할 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-22-QuantizingtheAIColossi_32.png\" /\u003e\n\n2023년 말에 Mobius Labs GmbH가 매우 효율적이고 정확한 제로샷 PTQ 접근방식인 Half-Quadratic Quantization (HQQ)을 오픈소스로 공개했습니다. 이 방법은 매우 낮은 정밀도까지 잘 작동하는 대형 LLaMA-2-70B 모델을 GPTQ보다 50배 빠르게 양자화하며 테스트한 GPU에서는 5분이 소요됩니다. 저자들은 GPTQ와 AWQ의 한계가 레이어 출력 간 오류를 최소화하기 위한 교정 데이터가 필요하다는 것이며, 이는 교정 세트에 과적합되어 계산 시간과 자원을 필요로 한다는 점을 지적합니다. 이를 피하기 위해 HQQ는 활성화가 아닌 가중치의 양자화 오류를 최소화하나, 희소성을 촉진하는 손실 함수를 사용하여 이상값의 영향을 더 정확하게 포착하며, Half-Quadratic 솔버를 사용하여 그룹-k 양자화 매개변수에 대한 최적 솔루션을 반복적으로 도출함으로써 솔루션을 닫힌 형태로 찾아내어 데이터 없이 두 하위 문제의 대안 최적화를 수행합니다. bitsandbytes의 인기 있는 데이터 없이 LLM.int8() 방법과 비교했을 때, HQQ는 꾸준히 복잡성이 낮은 모델을 생성합니다. 또한, HQQ는 GPTQ보다 50배, AWQ보다 약 25배 더 빠르지만, 결과는 4비트 미만의 정밀도를 사용할 때 특히 데이터 종속적인 방법보다 나은 또는 동등합니다. HQQ 코드는 오픈소스로 공개되었으며, 일부 사전 양자화된 모델은 Hugging Face hub에서 제공됩니다. Hugging Face 생태계 내에서 HQQ 모델을 사용하는 것은 간단하지만, 최적화된 vLLM 추론 엔진 내의 HQQ 통합은 본 작성 시점에서 \"실험적\"입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![원도수](/자원/그림/2024-06-22-QuantizingtheAIColossi_33.png)\n\n![원도수](/자원/그림/2024-06-22-QuantizingtheAIColossi_34.png)\n\n2023년 12월, SmoothQuant+가 4비트 균일 PTQ에서 최신 기술을 세웠습니다. 저자들은 AWQ의 한계에 대해 다루었는데, 이는 층 수에 비례하는 탐색 공간을 가지고 있으며, 오류 누적 문제를 \"탐색 과정 중에 고려하지 않기 때문에 검색 속도가 느려지고 모델 정확도가 저하된다\"고 합니다. 더 나아가, 가중치 양자화 손실은 활성화 이상치에 의해 증폭되며, 이들 값을 가중치 조정으로 평활화하면 전체 양자화 오류를 크게 줄일 수 있다고 주장합니다. 저자들은 인기 있는 vLLM 추론 엔진을 위해 사용자 정의 W4A16 커널을 제공하고, SmoothQuant+ 구현을 사용자가 사전 처리 단계를 필요로하지 않도록 설계하여 사용자가 Hugging Face 허브에서 직접 FP16 모델을 로드하고 모델이 GPU로 이동될 때 자동으로 양자화 프로세스를 적용할 수 있도록 했습니다. 아쉽게도, 이 작성 시점에서 SmoothQuant+ 알고리즘 코드는 아직 공개 GitHub 페이지에 등록되지 않았으며, vLLM 통합도 아직 온라인에 공개되지 않았습니다.\n\n![원도수](/자원/그림/2024-06-22-QuantizingtheAIColossi_35.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2024년 4월, Park 등이 그들의 LUT-GEMM 논문의 최신 버전을 발표했습니다. 이 작품에서 저자들은 가중치만 양자화하는 것이 가중치와 활성화 모두 양자화하는 것보다 주로 가중치에 의해 메모리 트래픽이 지배되고 활성화는 양자화에 민감하기 때문에 주어진 정확도 수준에서 더 높은 압축률을 가능하게 한다고 지적했습니다. 하지만 이에는 정수만을 사용하는 이점을 잃게 되고 곱셈 전에 가중치를 비양자화하는 과정이 필요합니다. 이를 해결하기 위해, LUT-GEMM은 양자화된 가중치와 양자화되지 않은 FP16 활성화 간에 직접 행렬 곱셈을 수행할 수 있도록 설계된 커널입니다.\n\n\n![이미지](/assets/img/2024-06-22-QuantizingtheAIColossi_36.png)\n\n\nLUT-GEMM은 XNOR-Net에서 사용된 바이너리 코딩 양자화(BCQ) 형식을 확장하였는데, 이는 간단한 산술 연산의 사용을 가능하게 합니다. BCQ는 원래 비균일 양자화를 위해 설계되었지만, 저자들은 편향 항목 추가를 통해 균일 양자화로 일반화됨을 입증했으며, 이는 표현 능력을 크게 증가시킵니다. 저자들은 가능한 하위 벡터 곱셈의 LUT를 구성하고, 실행 시간에 발생하는 하위 벡터를 인덱싱하여 생성하는 연산을 수행하는 대신 효율성을 크게 향상시켰습니다. 그들의 양자화 방식을 사용하여, 단일 GPU에서 GPTQ에 비해 2.1배의 추론 가속화 효과를 시연했습니다. LUT-GEMM은 강력한 MLC-LLM 추론 엔진에서 사용되는 기본 양자화 방식이며, 나중에 자세히 알아볼 것입니다.\n\n이 섹션에서 우리는 PTQ의 유산이 LLM 시대에 실현되는 것을 보았습니다. 이러한 거대한 모델들은 훈련 비용이 매우 높으며, 그 결과 이전보다는 효율적인 원샷 또는 제로샷 양자화 접근 방식의 혁신이 필요합니다. 그에 따라 연구자들은 bitsandbytes(LMM.int8), GPTQ, AWQ가 특히 인기가 있어지고 최근 출시된 HQQ가 강력한 새로운 데이터 프리 대안을 제공했습니다. 실행 가이드에서는 이러한 양자화 알고리즘 중 어떤 것을 선택해야 하는지에 대한 비교 장단점을 검토할 것입니다. 하지만 먼저, LLM의 QAT를 탐험해야할 때가 왔습니다. 왜냐하면 QAT가 본질적으로 뛰어난 양자화 결과를 달성하기 때문에, 실제로 이러한 방법 중 일부가 얼마나 접근하기 쉬운지에 대해 기쁘게 놀라게 될지도 모릅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## LLMs의 양자화 인식 훈련 (QAT)\n\n최종 연구 섹션에서는 LLMs에 대한 QAT 방법에 대해 검토합니다. LLaMA-2의 비용이 2천만 달러 이상 들었다고 하니, 우리는 여기에 참여해야 할까요? 여기의 일부 접근 방법은 우리 개인에게 접근하기 어려울 수 있지만, 가능한 것에 대한 보다 광범위한 이해를 위해 중요합니다. QAT가 양자화를 통합하여 효율적이고 저비트 모델을 생성하는 가장 효과적인 방법이라면, 수십 억 달러를 들여 오픈 소스 기반 모델을 훈련하는 기업들은 왜 이러한 기술을 자사의 출시 전략의 구성 요소로 사용하지 않아야 할까요? LLaMA-2 사용자 중 얼마나 많은 사람이 완전 정밀도 가중치를 사용하고 있을까요? 대규모 모델을 양자화에 더 잘 대응할 수 있도록 만드는 것이 더 좋지 않을까요, 대부분의 배포가 이를 사용할 것이라는 점을 알고 있기 때문에요? 이 섹션은 우리가 이러한 문제들에 대한 더 교육된 견해를 형성하는 데 도움이 될 수 있습니다.\n\nLiu 등의 2023년 중반 \"LLM-QAT\"은 LLMs에서의 QAT 탐구의 첫 번째 시도였습니다. 저자들은 현재 최첨단 PTQ 접근 방식이 정확도를 8비트 미만의 정밀도로 보호하지 못한다는 점을 지적했으며, 낮은 정밀도에서 성능을 보존하기 위해 QAT가 필요할 것으로 생각되지만, 이전의 작업에서는 이미 알고 있는 이유로 LLMs의 QAT를 조사한 적이 없었다: 방대한 양의 데이터와 계산 뿐만 아니라, 조정된 명령어를 기반으로 한 LLMs의 훈련 과정을 복제하는 어려움과 같은 보다 세심한 이유들까지 있습니다. 이러한 도전을 극복하기 위해 저자들은 사전 학습된 모델의 생성을 사용하여 훈련 데이터로 사용하는 데이터가 없는 지식 증류 방법을 소개하여, 원본 훈련 데이터의 큰 하위 집합으로 훈련하는 것과 비교했을 때 출력 분포를 더 잘 보존하도록 합니다.\n\n![물론 대체 텍스트](/assets/img/2024-06-22-QuantizingtheAIColossi_37.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM-QAT 작성자들은 상대적으로 작은 100k개의 합성 샘플 세트가 양자화된 모델을 축소하는 데 충분하며, 합리적인 양의 컴퓨팅을 사용하여 이 작업이 수행될 수 있다고 발견했습니다. 그들은 각 생성 단계에서 가장 확률이 높은 토큰을 확률적으로 샘플링하는 것이 데이터에 노이즈를 추가하는 데 중요하다고 발견했습니다. LLM-QAT는 트랜스포머 양자화 이전의 대부분의 이전 연구와는 달리, 특히 긴 시퀀스에서 이점이 있는 KV 캐시에 저장된 중간 활성화 벡터의 양자화를 조사했습니다. 작성자들이 자신의 조사를 LLaMA 모델 ≤30B 매개변수로 제한한다는 사실은 QAT의 기본적 비용을 반영합니다; 그렇다고 해도, 저자들은 연구에서 다양한 혼합 정밀도 구성을 벤치마킹하여, 자신들의 방법이 낮은 비트 설정에서 GPTQ 및 SmoothQuant보다 LLM 성능을 유지하는 데 상당히 더 좋다는 것을 보여주었습니다. 그러나 더 높은 정밀도에서는 결과가 대부분 동등하며, 이는 고정밀도가 허용되는 경우 PTQ 접근 방식이 더 간단하므로 더 유리하다는 것을 나타냅니다. 그럼에도 불구하고, LLM-QAT가 훈련 데이터 없이 LLM을 효율적으로 낮은 비트 설정으로 다시 훈련하고 성능을 유지하는 능력은 매우 흥미롭습니다, 특히 LLM이 보다 매개변수 효율적으로 되는 상황에서. 코드는 GitHub에서 제공됩니다.\n\n2023년에 Xi 등의 간단한 제목인 “4비트 정수로 트랜스포머 훈련하기”라는 논문은 모든 곱셈을 4비트 정수 산술로 처리하는 방법을 소개했습니다. 작성자들은 이를 위해 순방향 패스 중 활성화 중요 이상치를 처리하기 위해 제안한 Hadamard 양자화기를 사용하여 각 활성화 행렬의 블록 대각 Hadamard 행렬 변환을 양자화했습니다. 이 변환이 이점이 있는 이유는 이상치 정보를 인접 행렬 항목으로 퍼뜨려 숫자 범위를 줄이고 양자화할 수 있게 만든다는 것입니다. 역전파 중에 작성자는 활성화의 작은 기울기 행의 계산을 저장하여, 가장 정보가 풍부한 기울기를 낮은 4비트와 높은 4비트로 나누어 해당 세부 정보를 보존하기 위해 두 행을 사용하여 8비트 표현을 생성합니다. 이 방법을 사용하여 저자들은 현대 하드웨어에서 게인을 실현할 수 있기 때문에, 사용자 지정 숫자 형식을 필요로하지 않고, 최신 4비트 산술을 사용하여 트랜스포머를 정확하게 훈련했습니다. 작성자들은 자신들의 INT4 행렬 곱셈 연산자가 FP16 훈련보다 2.2배 빠르고 총 훈련 시간을 35.1% 줄였으며, 다양한 과업과 트랜스포머 모델에서 기본 성능과 거의 맞먹는 성과를 거두었습니다. 이 연구는 LLM 대신 BERT와 비전 트랜스포머(ViT)에 초점을 맞추었지만, 대규모 트랜스포머 모델의 저정밀 훈련에 있어서 주요 기준을 보여주기 때문에 이 부분에서 인용하는 것이 적절합니다. 또한 어떤 기법이 LLM에 적용될 때까지는 오직 시간문제일 뿐입니다. 저자들은 자신들의 코드를 GitHub에 공개했습니다.\n\n유명한 2023년에 Tim Dettmers와 워싱턴 대학의 동료들은 QLoRA로 새로운 지평을 열었습니다. QLoRA는 효율적인 저랭크 조정 미세조정 기술을 사용하여 얼린 양자화된 4비트 LLM에서 기계적인 저랭크 행렬을 통해 그레이디언트를 역전파하여 단일 48GB GPU로 65B 모델을 24시간만에 미세조정하는 과정을 유지하면서 기준 FP16 성능을 유지했습니다. QLoRA는 LLM을 미세조정하기 위해 필요한 하드웨어 양을 크게 줄였으며, 이는 이 기술의 민주화에서 중요한 이정표를 세우고 있습니다. 이를 달성하기 위해 작성자들은 정규 분포를 양자화하는 데 이론적으로 최적인 새로운 4비트 NormalFloat(NF4) 데이터 유형을 사용하여, INT4 또는 FP4 유형보다 더 나은 경험적 결과를 달성했습니다. 저자들은 또한 양자화 매개변수도 양자화하는 이중 양자화의 사용을 조사하여, 1개 매개변수당 평균 0.37비트(65B 모델에서 약 3GB)를 절약했습니다. 저자들의 방법의 효율성을 강조하기 위해, 저자들은 다양한 명령 조정 데이터세트를 사용하여 1000개 이상의 모델에서 QLoRA를 철저히 시험했습니다. 저자들은 자신들의 QLoRA 방법을 사용하여 Guanaco 모델 패밀리를 훈련했습니다. LoRA의 임의 변형을 사용하는 주요 장점은 사전 훈련된 모델이 변경되지 않으면서, 미세조정이 모두 저랭크 어댑터 행렬에 완전하게 포착된다는 것입니다. 이는 이 adaptors의 여러 세트가 전체 모델 재훈련 없이 여러 용도에 대해 훈련될 수 있다는 것을 의미합니다. QLoRA로 미세조정하는 것은 Hugging Face Transformers를 통해 bitsandbytes 라이브러리를 통해 가능하며, 원래 QLoRA 리포지토리는 GitHub에서 사용 가능합니다.\n\n![이미지](/assets/img/2024-06-22-QuantizingtheAIC\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## LLM의 극도의 양자화\n\n최근에는 극도의 양자화가 LLM 연구에도 영향을 미치고 있습니다. 이전에 CNN 양자화 섹션에서 본 것처럼, 신경망은 바이너리 또는 터너리 가중치로도 놀랍도록 잘 수행될 수 있으며 메모리 트래픽과 계산 복잡성이 크게 감소합니다. 가중치가 훨씬 적은 비트를 차지하고 곱셈 연산을 덧셈과 뺄셈으로 대체할 수 있기 때문입니다. LLM의 극도의 양자화는 아직 어린 분야이지만, 최근의 연구에서는 이러한 방법론을 성숙한 방향으로 발전시킬 것을 확신시킬만한 흥미로운 결과를 보여주었습니다. 본 섹션에서는 저비트 LLM 교육의 미래를 예감할 수 있는 가능성을 살펴볼 것입니다.\n\n2023년에 BitNet의 저자들은 LLM에서 사용할 수 있는 1비트 트랜스포머 아키텍처를 소개했습니다. nn.Linear 레이어 대신 사용할 수 있는 BitLinear 레이어를 도입하고 이진 가중치를 처음부터 훈련하여 완전 정밀성 베이스라인과 경쟁력 있는 성능을 달성하는 바람직한 결과를 얻었습니다. 이 때 결과로 나타난 트랜스포머들은 소비 전력이 훨씬 적게 필요한 연산을 사용하면서 완전 정밀성 버전과 유사한 스케일링 법칙을 나타냅니다. 활성화는 8비트로 양자화되며 옵티마이저 상태 및 기울기는 여전히 완전 정밀성으로 수행됩니다. 저자들은 구현이 FP16/BF16에서 작동하기 때문에 실제로 교육 가속화가 없다고 인정하지만, 전방향 및 역방향 통과를 가속화하기 위해 저비트 FP8 GEMM CUDA 커널을 사용할 수 있다고 제안하지만 이 작업을 미래에 미루기로 합니다.\n\n\u003cimg src=\"/assets/img/2024-06-22-QuantizingtheAIColossi_39.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2024년에 공개된 BitNet b1.58에 관해 팔로우하고 있습니다. 이 버전은 삼중체 '-1, 0, 1' 가중치를 사용하는 게 새롭게 도입되었고, 1비트 BitNet 시스템에 영(0) 값을 추가하면서 작은 값들을 영(0)으로 클램핑하는 γ 임계값 매개변수를 조정했습니다. 우리가 삼중체 CNN에서 배운 대로, 가중치에 0 값을 추가하는 것은 운용 복잡성에 증가를 일으키지 않습니다. 왜냐하면 곱셈은 여전히 덧셈/뺄셈으로 대체될 수 있기 때문이지만, 이러한 접근은 가중치가 \"특징 필터링\"을 수행할 수 있도록 해 성능을 현저히 향상시킵니다. 다만, 가중치 당 비트 수가 1에서 2로 증가하는 비용이 듭니다. 어쨌든, 가중치가 작기 때문에 DRAM에서 SRAM으로의 전송이 쉽고 메모리 트래픽이 줄어듭니다. 저자는 낮은 커널 최적화를 위해 대칭 양자화를 사용하고 활성화를 8비트로 양자화하며 KV 캐시를 4비트로 양자화합니다. 이 접근을 통해 저자들은 기본 FP16 성능과 헷갈릴 정도의 perplexity 및 과제 성능을 일치시키며 삼중체 네트워크를 처음부터 훈련시켰습니다.\n\nBitNet b1 (binary) 및 b1.58 (ternary) 방법의 코드 및 구현에 대한 자세한 내용은 교육 지침, 코드 및 FAQ PDF 파일을 참고하시기 바랍니다. 그러나 거기서 제공되는 코드는 추론에 사용할 저비트 커널의 세부 정보를 포함하고 있지 않습니다. 다행히 오픈 소스 커뮤니티는 이 GitHub 저장소를 개발하여 완벽한 구현, 사용자 정의 커널 및 pip 설치를 제공합니다. LLM의 극한 양자화로 나아가는 BitNet이 강력한 단계임을 강조하고자 합니다. 그러나 이러한 구현에서 이진화 및 삼진화는 완전히 활용되지 않고 있습니다. 왜냐하면 훈련 중에 여전히 반정밀도에서 곱셈이 수행되고 있으며, 추론을 가속화하기 위해 사용되는 사용자 정의 커널은 FP8입니다. 그러나 우리는 이전에 가중치를 이진화/삼진화하는 것의 가장 큰 이점은 행렬 곱셈이 훨씬 효율적인 정수 덧셈/뺄셈 연산으로 대체될 수 있다는 것이었음을 배웠습니다. 이것은 직접 더 빠른 계산으로 이어지지만, 이는 훈련이나 추론을 위해 여기서 탐구되지 않았으며, 미래 작업에 대한 미개척 영역으로 남아 있습니다.\n\n아래는 BitNet b1.58의 이미지입니다.\n\n![BitNet b1.58](/assets/img/2024-06-22-QuantizingtheAIColossi_40.png)\n\n# 실무자를 위한 LLM 양자화 안내\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 섹션에서는 워크플로를 최적화하기 위해 양자화를 실용적으로 구현하는 방법에 대해 논의하겠습니다. 우리가 알아본 대로, 양자화는 학습 중이나 모델 개발의 후처리 단계로 적용될 수 있으며 이는 모델 및 사용 사례에 따라 달라집니다. 양자화된 워크플로의 교육된 전문가가 되기 위해 먼저 사용할 수있는 현재의 도구 세트를 검토하고 그 강점과 약점을 살펴보겠습니다. 그런 다음 우리는 배운 내용을 활용하여 목표와 사용 가능한 방법을 조율하는 데 도움이 되는 의사 결정 트리를 개발할 것입니다.\n\n트랜스포머 개발 및 추론을 실험하는 인기 있는 간단한 방법은 Hugging Face Transformers 라이브러리를 사용하는 것입니다(그들의 모델 허브와 함께, 이는 깊은 학습 모델을위한 GitHub 저장소와 같이 작동하여 모델 체크포인트를 쉽게 푸시하고 몇 줄의 코드로 다른 기기에서 사용할 수 있도록 끌어올릴 수 있습니다. 이는 파이썬 코드를 다룰 수있는 사람들에게는 놀라운 인터페이스이지만 모델을 사용하는 비코드 사용자 인터페이스 (UI)를 제공하지는 않으며, 이 라이브러리는 일반적으로 더 빠른 백엔드 서빙을 제공하는 최적화된 라이브러리로 백엔드 서빙에 더욱 효과적이지만 우리가 볼 것입니다. 그러나 훈련 측면에서, Hugging Face의 Transformers 라이브러리의 직관적 인터페이스를 통해 효율적인 양자화 훈련을 가능하게 하는 bitsandbytes 라이브러리의 통합은 모델 개발에 아주 강력한 도구로 만듭니다.\n\n2019년 7월부터 Nvidia는 트랜스포머 모델의 추론 처리량을 최적화하기 위해 FasterTransformer 라이브러리를 제공하였으며 (원래 BERT에 중점을 둔), 이를 사용하여 기기용 TensorRT SDK를 백엔드로 사용했습니다. 그 후, 이 프로젝트는 TensorRT-LLM(aka TRT-LLM)으로 발전하여 파이썬 API가 PyTorch와 유사한 형태로 구축되었으며 GPTQ, AWQ를 지원하고 SmoothQuant 기술의 구현을 포함하고 있습니다. 이러한 라이브러리는 Nvidia 하드웨어 또는 Triton 추론 서버에서 추론을 최적화하기 위해 특별히 설계되었으므로 이를 사용하는 사람들에게는 매우 효과적이지만 일반적인 목적의 양자화 라이브러리는 아닙니다. 그럼에도 불구하고 Nvidia 하드웨어를 사용하는 다중 GPU 설정을 원하는 사람들에게는 적절한 선택일 수 있습니다. 라이브러리에는 많은 인기있는 오픈소스 모델의 미리 빌드된 버전이 포함되어 있지만 사용자는 API를 사용하여 사용자 지정 모델을 양자화할 수도 있습니다.\n\n한편 Georgi Gerganov의 머신러닝 라이브러리(GGML)는 2022년 9월에 만들어져 Apple 장치에서 LLM 추론을 가속화하기 위해 설계된 순수한 C 라이브러리이며, 이후 지속적으로 발전해왔습니다. GGML은 구조화된 이진 모델 파일을 생성하여 다양한 하드웨어에서 최적화된 텐서 계산을 수행하는 데 사용할 수 있는 양자화를 사용합니다. 이 라이브러리는 특히 Apple 실리콘을 위해 특별히 설계되었지만 x86 아키텍처 및 GPU에서 추론 가속을 지원하도록 제공됩니다. GGML 라이브러리는 대표적인 llama.cpp 추론 라이브러리의 백엔드를 제공하며, 이는 다시 Ollama 및 LM Studio와 같은 프론트엔드 라이브러리를 위한 백엔드 지원을 제공합니다. GGML 라이브러리가 제공하는 GGUF는 새롭고 개선된 파일 형식입니다. 이러한 압축 형식을 사용하는 단점은 llama.cpp로 추론해야한다는 것이지만, 많은 하드웨어 아키텍처, 특히 비-Apple 장치에 적합하지 않다는 점입니다. 그러나 llama.cpp는 다중 GPU/CUDA 하드웨어 설정을 지원하고 있으므로 Ollama와 같은 사용자 친화적인 도구는 매우 효과적입니다, 비록 가장 빠르지는 않을지라도.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nvLLM은 Kwon et al. 2023에서 소개된 강력한 추론 엔진으로, Nvidia 및 AMD GPU에서 성능을 가속화하기 위해 최적화된 CUDA 커널을 사용합니다. 저자들은 LLM의 한 번에 하나씩 순차적으로 토큰을 생성하는 과정이 메모리에 의존하며, 결과적으로 GPU의 계산 파워를 제대로 활용하지 못한다는 점을 강조합니다. 이를 해결하기 위해, 저자들은 제안된 PagedAttention 알고리즘 위에 vLLM 서빙 엔진을 구축했는데, 이는 KV 캐시의 메모리 효율성을 향상시켜 공간을 낭비하지 않으며, 주어진 하드웨어에서의 최대 배치 크기 증가에 중요합니다. FasterTransformer보다 2~4배 더 나은 처리량을 보여주며, 모델 크기가 크고 복잡하며 더 큰 시퀀스일 때 더 두드러집니다. vLLM은 Hugging Face 모델과 매끄럽게 통합되며, 다중 GPU 워크플로를 지원합니다. GPTQ, AWQ, SqueezeLLM, FP8 KV 캐시 양자화와 원활한 모델 양자화(SmoothQuant+) 통합이 곧 예정되어 있습니다.\n\nMLC-LLM은 최적화된 양자화 모델의 원시 배포를 다양한 유형의 하드웨어(Apple, Nvidia, AMD, Intel, 모바일 장치)에서 가능하게 하는 강력한 범용 배포 솔루션입니다. 본 글작성 시점에는 가장 빠르고 일반적인 서빙 엔진으로 보이며, Jetson AI Lab에서 우수한 처리량으로 선호됩니다. MLC-LLM 라이브러리는 사전 제작된 모델 세트를 제공하며, 라이브러리와 함께 사용할 수 있는 새로운 모델을 컴파일하는 옵션도 제공합니다.\n\n![QuantizingtheAIColossi_41](/assets/img/2024-06-22-QuantizingtheAIColossi_41.png)\n\n## LLM 양자화 의사결정 트리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 사상이 어지럽히도록 다양한 양자화 디자인 옵션을 만드는 다양한 방법론을 다루었습니다. 다행히도 일부 오픈 소스 추론 엔진들이 이러한 도구 중 가장 유용한 것을 직관적인 형태로 우리 손끝에 제공하고 있습니다. 이 의사 결정 트리는 사용 목적과 배포 환경을 기반으로 한 매우 기본적인 가이드로 사용될 수 있지만, 고려해야 할 사항의 철저한 목록은 아닙니다. 이는 실무자들에게 양자화를 구현하는 데 출발점을 제공하기 위한 것입니다.\n\n- 질문: CPU 또는 가장자리 장치에 사전 학습된 모델을 배포하고 있습니까?\n답변: 예 - Apple 사용자의 경우 GGML 기반 추론(llama.cpp, Ollama, LM Studio)을 선택하십시오. Android 및 x86 하드웨어의 경우 MLC-LLM을 사용하십시오.\n아니요 - 다음 질문으로 이동합니다.\n- 질문: 동시 배치 요청을 서비스하게 됩니까?\n답변: 예 - vLLM을 사용하십시오. 이는 이를 위해 특별히 최적화되었습니다.\n아니요 - 다음 질문으로 이동합니다.\n- 질문: GPU에 사전 학습된 모델을 배포하고 있습니까?\n답변: 예 - Nvidia 및 AMD GPU에서 MLC-LLM이 최신 처리량을 제공한다고 보입니다. Apple GPU도 지원하지만, llama.cpp는 Apple 하드웨어용으로 최적화되었다는 저희가 작성한 것이 있어 비교가 필요할 것입니다. Nvidia 및 AMD GPU의 경우, vLLM 라이브러리에 포함될 예정인 SmoothQuant+의 통합은 테스트가 가치 있을 것입니다.\n아니요 - 다음 질문으로 이동합니다.\n- 질문: 양자화된 LLM을 화강함하고 있습니까?\n답변: 예 - QLoRA를 사용하십시오. 이것은 양자화된 LLM이 새로운 작업을 수행하거나 양자화로 인해 성능이 손실된 것을 복구하는 가장 효율적인 방법입니다. Hugging Face Transformers에서 쉽게 사용할 수 있습니다.\n아니요 - 다음 질문으로 이동합니다.\n- 질문: 기초 모델을 훈련 중인가요?\n답변: 예 - BitNet을 사용하여 기초 모델을 프로토타입 또는 개발하는 것을 시도해보십시오. 훈련 비용이 훨씬 저렴하고 완전 정밀도 기준선에 대비하여 경쟁력 있는 성능을 제공합니다. 더 좋은 점은, 이러한 저비트 모델들을 합성해 보는 것입니다.\n\n이 체크리스트는 특정 상황에서 사용자들이 참고해야 할 좋은 가이드를 제공해야 합니다. LLM 추론 프레임워크에 대한 더 포괄적인 안내를 위해서는 Sergei Savvov의 이 포스트를 확인해보세요.\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 양자화 연구에서 놀라운 성과들을 살펴보았습니다. 우리는 무손실 8비트 양자화가 지루한 기준으로 성장하는 것을 보았고, 이진 CNN이 완전 정밀도에 거의 필적하는 것을 확인했습니다. 4비트 정밀도로 훈련된 대규모 트랜스포머 모델과 LLMs에서의 극한 양자화의 부상도 보았습니다. INT8 PTQ가 완전히 성숙하며 완전 정밀도 기준과 쉽게 맞먹어서 대부분의 오픈소스 라이브러리에 널리 통합되어 있기 때문에 누구도 부동 소수점 데이터 유형을 사용하는 모델을 제공해서는 안 될 수도 있다는 주장이 가능합니다. LLMs의 경우, FP16로 자주 출시되는 것에 비해 무손실 INT8 양자화는 메모리 풋프린트를 절반으로 줄이고 \"무료 점심\" 압축을 제공합니다. 더 나아가 더 낮은 정밀도에서 양자화하는 것은 최소의 성능 손실로 이루어졌으며, 많은 실용적인 사용 사례에서는 약간의 정확도 감소와 교환할 가속화된 원활한 사용자 경험이 환영받을 것입니다. BENN 논문에서 본 것처럼 극히 낮은 비트 네트워크를 앙상블링하는 아이디어는 매우 설득력 있는데, 특히 LLMs에서 극한 양자화의 부상을 보고 있다는 것을 감안하면 이 방법이 곧 이롭게 된다는 점을 확인할 수 있습니다.\n\n우리는 다양한 양자화 접근 방식을 살펴보았는데, 마찬가지로 다양한 복잡성을 가진 다양한 사용 사례에 맞추어 수요에 따라 발전되어 온 것을 볼 수 있었습니다. PTQ와 ZSQ가 매우 효과적으로 사용되었지만, QAT는 여전히 가장 성능이 우수한 방법입니다. 왜냐하면 이것은 네트워크 훈련 과정에 양자화를 통합하기 때문입니다. 이것은 훨씬 더 리소스 집약적이며, 일반 사용자에 의해 LLMs에 사용됨을 방지할 것 같지만, QLoRA의 창의성은 작은 비율의 매개변수만을 훈련하여 양자화로 인한 손실을 복구하는 저랭크 행렬에 양자화를 허용함으로써 PTQ와 QAT 사이의 구분선을 흐리게 할 수 있도록 했습니다. 최근 HQQ 방법을 통해 PTQ가 2비트 정밀도까지 효과적으로 사용되는 것을 확인했으며, 이진 및 삼항 LLMs가 지금부터 거의 완전 정밀도와 맞먹는 성능을 낼 수 있도록 처음부터 훈련된 것을 보았습니다.\n\n이 지점에 도달한 것을 축하합니다. 이 기사에서 검토된 작품에 익숙해진 당신은 이제 신경망 양자화에 대한 전문 지식을 보유하고 있습니다. 이 조사는 소극적인 타입을 위한 것이 아니었지만, 2024년의 양자화 방법에 대한 피상적인 요약은 필요하지 않았습니다. 올바른 머신 러닝 실천을 위해 우리는 가능한 모든 옵션을 고려하여 교육된 결정을 내릴 수 있도록 모든 잠재적인 옵션들과 선택한 방법론의 장단점을 고려해야 합니다. 신경망 양자화는 매우 다양한 연구 분야로, 풍요로운 전통 위에 구축된 것이기 때문에 전체적인 이해를 얻고자 하는 사람들에게는 어려운 연구 과제가 될 수 있습니다. 바람직한 사용 사례를 위해 어떤 도구가 가장 적합한지에 대한 효과적인 제안과 함께 이 기사가 분야에서 활동하는 실무자들에게 접근 가능하고 자체 포괄적인 자원을 제공할 수 있기를 희망합니다.\n\n## 미래 작업\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBitNet은 훈련 중에 저정밀 연산을 사용하지 않으며 가중치의 이진화 또는 삼진화를 완전히 활용하지 않습니다. 이는 곱셈을 덧셈/뺄셈 연산으로 대체하지 않기 때문에 본래의 가치를 실현하는 능력을 저해하고 있습니다. CNNs의 극단적 양자화에 대한 업적은 BitNet이 더 효율적인 연산을 수행하도록 하기 위해 실제 수학을 이진화하는 데 적용되어야 합니다. 더욱이, BitNet을 위해 최적화된 커널이 확립된 후에는 이러한 낮은 비트 LLM들의 앙상블 조사를 시작해야 합니다. 우리는 BENN 논문에서 CNNs와 함께 이것이 매우 효과적임을 보았습니다.\n\nHQQ는 정확한 데이터 무관(zero-shot) PTQ를 위한 매우 효율적인 알고리즘입니다. 그러나 지금까지 최적화된 추론 엔진(vLLM) 하나만이 이러한 모델을 실행하는 실험 지원을 시작했습니다. 더 나아가 HQQ를 사용하여 양자화된 모델의 추론 처리량을 다른 방법과 비교한 벤치마크는 현재 없습니다. 이 두 가지는 향후 연구를 위한 오픈 영역입니다.","ogImage":{"url":"/assets/img/2024-06-22-QuantizingtheAIColossi_0.png"},"coverImage":"/assets/img/2024-06-22-QuantizingtheAIColossi_0.png","tag":["Tech"],"readingTime":66},{"title":"집은 항상 장소가 아니다","description":"","date":"2024-06-22 19:54","slug":"2024-06-22-homeisnotalwaysaplace","content":"\n\n\n\u003cimg src=\"/assets/img/2024-06-22-homeisnotalwaysaplace_0.png\" /\u003e\n\n\"아직 집에 가려고 하나요?\"\n\n\"그들이 나와 함께 있다면 이미 집이니까요.\"\n\n\"모든 것이 너무 무거워지면 어디로 도망가나요?\"\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내 친구들이 대화할 때 자주 듣는 질문이에요. 대답은 다양해요 - 몇 명은 확신을 못 가지고 있다고 하고, 다른 사람들은 자기 안에서 평화를 찾고, 몇 사람은 그냥 현재 있는 곳을 벗어나고 싶다고 해.\n\n그들을 조용히 지켜보고 있어. 즐거운 미소, 장난기 가득한 웃음소리, 그들 사이를 흐르는 깊은 대화, 말 끝이 없는 아이들처럼의 장난 같은 엄청난 배려를 못 보지 못할 수가 없어.\n\n항상 혼자 있기를 원하는 사람으로써, 나는 이 상황에서 버틸 수 있을 거라고 생각하지 못했어. 어떻게 된 건지도 모르겠어. 어느 날 일어나보니 그들이 나의 그리워하는 사람으로 변해 있었다는 걸 깨달았어. 그들의 얼굴과 목소리가 나의 생각을 지배해, 눈을 뜨는 순간부터 머리를 누이는 그 순간까지.\n\n우리의 우정은 예상치 못한 게 있어. 고등학교 시절에 그들과는 정말 가까운 사이가 아니었어. 사실, 나는 그들을 어떻게 발견했는지 자세한 내용조차 없어.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일부는 나의 동급생들이었지만, 우리는 특별한 대화를 나누지 않았어요. 우리의 소통은 복도, 학교 건물, 그리고 구내식당에서 일어나는 굉장히 피상적인 만남에 그쳤어요 — 서둘러 지나가는 인사, 예의 바른 인사 교환, 그 이상의 것은 없었죠.\n\n우리는 이바지를 하고 있었지만, 우리의 삶은 거의 교차하지 않고 나란히 진행되고 있었어요.\n\n하지만 사람들을 어느 때보다 소중히 생각하고 사랑할 수 있다는 것, 그것은 정말 믿기 힘들지만 놀랍게도 가능해요. 단지 한번 정도 만난 이런 사람들이 조금씩 진정한 집과 안식처로 변해갈 수도 있어요.\n\n나는 그들과 함께 하는 것을 끊임없이 갈망하게 되었어요, 마치 그들이 내 자신의 연장이 된 것처럼요. 하루종일 함께 지냈더라도, 전혀 충분하지 않은 것 같아요. 그들의 안정적인 존재에 대한 끝없는 열망이 있어요, 그들만이 그 빈자리를 채울 수 있는 것 같아요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그들과 함께 있을 때 내가 견디는 부담이 상당히 가벼워진다. 그들의 포옹은 내 문제들이 사라지는 방법으로, 그 순간에는 단순히 존재하지 않는 것처럼 내 문제들을 사라지게 만든다.\n\n시간이 흐르면서, 해가 떠오르고 질 때, 비가 쏟아지고 멈출 때, 하늘을 밝게 비추고 어두울 때,\n\n그들은 거기에 있었고 나에게 집이 되었다.\n\n집은 항상 물리적인 장소일 필요는 없다. 이것은 네모방이 있고 지붕, 기둥, 창문, 그리고 문이 있는 것이 아닙니다. 때로는 감정, 목소리, 손 두 개, 그리고 발을 갖고 있다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n나는 이야기 뒤에 있는 사람들을 가리키는 집이기 때문에.\n\n나는 안전하고 평화롭고 행복한 곳이 있는 집이에요.\n\n그래서 돌아오지 않았느냐고 물으면, 답은 간단해요 — 나는 이미 집에 있어요. 집은 목적지가 아니라, 내 여정의 일부가 된 사람들이기 때문이죠. 그들이 계속 나와 함께 걷는 한, 나는 정확히 내가 있어야 할 곳에 있는 거예요.\n\n내가 위에서 언급한 사람들이 함께 있다면, 난 이미 집에 도착한 거랍니다.","ogImage":{"url":"/assets/img/2024-06-22-homeisnotalwaysaplace_0.png"},"coverImage":"/assets/img/2024-06-22-homeisnotalwaysaplace_0.png","tag":["Tech"],"readingTime":2},{"title":"고급 RAG 정보 검색 강화 생성 시스템 향상시키는 고급 기법 구현 방법","description":"","date":"2024-06-22 19:52","slug":"2024-06-22-AdvancedRAGImplementingAdvancedTechniquestoEnhanceRetrieval-AugmentedGenerationSystems","content":"\n\n이제 RAG (Retrieval Augmented Generation) 기술이 LLMs와 상호 작용 중에 사실로 잘 알려져 있는 것 같아요. 최근에 제가 쓴 하나의 기사에서는 코드 예제와 함께 단계별로 RAG 파이프라인을 구축했습니다. 이 기사에서 우리는 이 이니셔티브를 한 단계 더 나아가서 고급 RAG 파이프라인을 구현해볼 거에요.\n\n# 1. 고급 정보 검색 기반 생성 (RAG) 파이프라인: 개요\n\n기본 RAG 작업 흐름은 색인화, 검색 및 생성 세 단계로 나뉠 수 있어요. 색인화 단계에서 텍스트는 임베딩으로 변환되고, 이후에는 이를 검색 가능한 인덱스를 만들기 위해 벡터 데이터베이스에 저장됩니다. 검색 단계에서 사용자의 질의도 임베딩으로 변환됩니다. 이 임베딩은 가장 관련 있는 텍스트 데이터를 검색하기 위해 벡터 데이터베이스를 탐색하는 데 사용됩니다. 마지막으로 생성 단계에서 질의는 이전에 검색된 관련 문서들로 확장되고, 이 향상된 프롬프트를 사용하여 대규모 언어 모델이 사용자 질문에 대한 답변을 생성합니다.\n\n고급 RAG 파이프라인은 새로운 단계(하위 단계)가 추가된 이 파이프라인의 향상된 버전입니다. 이 기사에서 논의될 개선 사항 목록은 다음과 같지만, 전반적인 목록은 이에 한정되지 않을 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터 색인 최적화: 슬라이딩 윈도우를 사용하여 텍스트 청킹 및 효율적인 메타데이터 활용과 같은 기술을 사용하여 더 검색 가능하고 조직화된 색인을 생성합니다.\n- 쿼리 향상: 동의어 또는 보다 넓은 범위의 용어를 사용하여 초기 사용자 쿼리를 수정하거나 확장하여 관련 문서의 검색을 개선합니다.\n- 하이브리드 검색: 전통적인 키워드 기반 검색과 임베딩 벡터를 사용한 의미론적 검색을 결합하여 다양한 쿼리 복잡성을 처리합니다.\n- 임베딩 모델 세밀 조정: 사전 훈련된 모델을 조정하여 특정 도메인 세부 사항을 더 잘 이해하고 검색된 문서의 정확도와 관련성을 향상시킵니다.\n- 응답 요약: 검색된 텍스트를 압축하여 최종 응답 생성 전에 간결하고 관련성 있는 요약을 제공합니다.\n- 다시 순위 매기기 및 필터링: 관련성에 기초하여 검색된 문서의 순서를 조정하고, 덜 관련성이 떨어지는 결과를 걸러내어 최종 출력물을 정제합니다.\n\n여기 나열한 것 외에도 가능한 개선점은 다양합니다. RAG 기술을 조사한 논문이 있어서 RAG 파이프라인의 품질을 향상시키는 데 사용할 수 있는 많은 개선 사항을 찾을 수 있습니다. 다음 다이어그램은 고급 RAG 파이프라인의 단계를 보여줍니다.\n\n![다이어그램](/assets/img/2024-06-22-AdvancedRAGImplementingAdvancedTechniquestoEnhanceRetrieval-AugmentedGenerationSystems_0.png)\n\n본 문서에서는 기본 RAG 파이프라인에 대한 이해를 전제로 하고, 개선 사항에만 초점을 맞출 것입니다. 이해가 되지 않는 경우, 이전 글인 \"RAG와 함께하는 실전: LLMs에 검색 보강 생성 통합하기 - 단계별 가이드\"를 읽어보시기를 권해 드립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2. Pre-Retrieval Optimizations for Advanced Retrieval-Augmented Generation (RAG)\n\n사전검색은 인덱싱이 어떻게 수행되며 사용자 쿼리가 검색에 사용되기 전에 어떤 작업이 수행되는지를 정의하는 단계입니다. 아래에서 사전검색 최적화를 위한 여러 전략을 논의하겠으며, 데이터 인덱싱 및 쿼리 개선을 포함한 샘플 Python 코드를 제공할 것입니다.\n\n## 2.1. 데이터 인덱싱 최적화\n\n다른 작업을 수행하기 전에 나중에 질의할 수 있는 데이터를 저장해야 합니다. 이를 인덱싱이라고 합니다. 이는 올바른 청크 크기 설정, 메타데이터 효과적인 사용 및 임베딩 모델 선택을 포함합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2.1.1. 텍스트 청킹을 위한 슬라이딩 윈도우\n\n텍스트를 색인화하는 간단한 방법은 텍스트를 n 부분으로 나누고, 그것들을 임베딩 벡터로 변환한 다음 벡터 데이터베이스에 저장하는 것입니다. 슬라이딩 윈도우 접근 방식은 겹치는 텍스트 청크를 생성하여 청크의 경계에서 어떤 문맥 정보도 손실되지 않도록 보장합니다. 다음 코드 샘플은 텍스트를 문장으로 분할하기 위해 nltk 라이브러리를 사용합니다.\n\n```python\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nnltk.download('punkt')  # punkt 토크나이저가 다운로드되었는지 확인\n\ndef sliding_window(text, window_size=3):\n    \"\"\"\n    슬라이딩 윈도우 접근 방식을 사용하여 텍스트 청킹 생성\n\n    Args:\n    text (str): 청크할 입력 텍스트\n    window_size (int): 청크 당 문장 수\n\n    Returns:\n    list of str: 텍스트 청크 목록\n    \"\"\"\n    sentences = sent_tokenize(text)\n    return [' '.join(sentences[i:i+window_size]) for i in range(len(sentences) - window_size + 1)]\n\n# 예제 사용법\ntext = \"This is the first sentence. Here comes the second sentence. And here is the third one. Finally, the fourth sentence.\"\nchunks = sliding_window(text, window_size=3)\nfor chunk in chunks:\n    print(chunk)\n    print(\"-----\")\n    # 여기서 청크를 임베딩 벡터로 변환하고\n    # 그것을 벡터 데이터베이스에 저장할 수 있습니다\n```\n\n2.1.2. 메타데이터 활용\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n메타데이터에는 문서 작성 날짜, 작성자 또는 관련 태그와 같은 정보가 포함될 수 있습니다. 이 정보는 검색 프로세스를 향상시켜 검색 중에 문서를 필터링하거나 우선 순위를 지정하는 데 사용될 수 있습니다.\n\n다음 코드 샘플은 faiss 라이브러리를 사용하여 벡터 데이터베이스를 생성하고 벡터를 삽입하고 메타데이터(태그)로 검색하는 방법을 보여줍니다.\n\n```js\nimport numpy as np\nimport faiss\n\ndocuments = [\n    \"문서 1 내용\",\n    \"두 번째 문서의 내용\",\n    \"세 번째 문서는 다른 내용을 가지고 있습니다\",\n]\nmetadata = [\n    {\"날짜\": \"20230101\", \"태그\": \"뉴스\"},\n    {\"날짜\": \"20230102\", \"태그\": \"업데이트\"},\n    {\"날짜\": \"20230103\", \"태그\": \"리포트\"},\n]\n\n# 더미 함수를 사용하여 임베딩 생성\ndef generate_embeddings(texts):\n    \"\"\"예시를 위한 더미 임베딩 생성.\"\"\"\n    return np.random.rand(len(texts), 128).astype('float32')  # 128차원 임베딩\n\n# 문서에 대한 임베딩 생성\ndoc_embeddings = generate_embeddings(documents)\n\n# 임베딩용 FAISS 인덱스 생성(단순성을 위해 FlatL2 사용)\nindex = faiss.IndexFlatL2(128)  # 128은 벡터의 차원\nindex.add(doc_embeddings)  # 인덱스에 임베딩 추가\n\n# 메타데이터를 사용하는 검색 함수 예시\ndef search(query_embedding, metadata_key, metadata_value):\n    \"\"\"메타데이터 기준과 일치하는 문서를 인덱스에서 검색합니다.\"\"\"\n    k = 2  # 찾을 최근접 이웃 수\n    distances, indices = index.search(np.array([query_embedding]), k)  # 검색 수행\n    results = []\n    for idx in indices[0]:\n        if metadata[idx][metadata_key] == metadata_value:\n            results.append((documents[idx], metadata[idx]))\n    return results\n\n# 쿼리 임베딩 생성(실제 상황에서는 유사한 프로세스로부터 생성)\nquery_embedding = generate_embeddings([\"여기에 쿼리 내용\"])[0]\n\n# '업데이트'로 태그된 문서 검색\nmatching_documents = search(query_embedding, '태그', '업데이트')\nprint(matching_documents)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사용자가 질문을 명확하게 표현하지 못할 수도 있는 경우가 있습니다. 그런 경우에는 질문을 완전히 다시 작성하거나 확장하여 쿼리를 개선할 수 있습니다.\n\nLLM 자체를 활용할 수 있습니다. 질문을 LLM에 보내서 더 명확하게 표현하도록 요청할 수 있습니다. 다음 프롬프트가 그 작업에 도움이 될 것입니다.\n\n```js\n프롬프트: '{prompt}'을(를) 주어서, 더 잘 표현된 3가지 질문을 생성하십시오.\n```\n\n새로운 쿼리를 얻은 후, 새로운 쿼리를 임베딩 벡터로 변환하여 벡터 데이터베이스에서 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 3. 고급 검색 증강 생성 (RAG)을 위한 검색 기술\n\n검색은 쿼리가 이전에 색인 된 데이터베이스를 탐색하는 단계입니다. 아래에서는 검색을 위한 다양한 전략을 논의하겠습니다.\n\n## 3.1. 하이브리드 검색 모델\n\n지금까지 우리는 항상 임베딩 벡터를 저장하는 벡터 데이터베이스에서 쿼리를 검색하는 것을 논의해 왔습니다. 이를 한 걸음 더 나아가 전통적인 키워드 기반 검색과 결합해 봅시다. 이 접근 방식은 검색 시스템이 정확한 키워드 일치를 필요로하는 쿼리부터 컨텍스트 이해가 필요한 복잡한 쿼리까지 다양한 유형의 쿼리를 처리할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하이브리드 검색 모델을 만들어 보겠습니다. 전통적인 검색 메커니즘에는 Elasticsearch를 사용하고 의미론적 검색을 위해 벡터 데이터베이스로 faiss를 사용할 것입니다.\n\n전체 코드는 여기에서 확인할 수 있어요: https://github.com/ndemir/machine-learning-projects/tree/main/hybrid-search — 여기에는 Elasticsearch를 실행하는 방법을 보여주는 보너스 Dockerfile도 포함되어 있습니다.\n\n3.1.1. Elasticsearch로 색인하기\n\n우선, 모든 문서가 'documents' 딕셔너리에 있다고 가정하고 임베딩 벡터를 이미 가져와 딕셔너리에 저장했다고 가정해 봅시다. 다음 코드 블록은 Elasticsearch 8.13.4에 연결하고 주어진 샘플 문서를 위해 색인을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nES_NODES = \"http://localhost:9200\"\n\ndocuments = [\n    {\"id\": 1, \"text\": \"파이썬 프로그래밍을 시작하는 방법.\", \"vector\": [0.1, 0.2, 0.3]},\n    {\"id\": 2, \"text\": \"고급 파이썬 프로그래밍 팁.\", \"vector\": [0.1, 0.3, 0.4]},\n    # 더 많은 문서...\n]\n\nfrom elasticsearch import Elasticsearch\n\nes = Elasticsearch(\n    hosts=ES_NODES,\n)\nfor doc in documents:\n    es.index(index=\"documents\", id=doc['id'], document={\"text\": doc['text']})\n```\n\n3.1.2. Faiss를 사용한 색인\n\n이 부분에서는 faiss를 벡터 데이터베이스로 사용하여 벡터를 색인화합니다.\n\n```js\nimport numpy as np\nimport faiss\n\ndimension = 3  # 간단함을 위해 3차원 벡터를 가정합니다.\nfaiss_index = faiss.IndexFlatL2(dimension)\nvectors = np.array([doc['vector'] for doc in documents])\nfaiss_index.add(vectors)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3.1.3. 혼합 검색\n\n아래 코드 블록은 혼합 검색을 설명합니다. 이는 사용할 수 있는 샘플이며, 또는 자체 사용 사례에 맞게 무언가를 처음부터 만들 수도 있습니다.\n\n```js\ndef hybrid_search(query_text, query_vector, alpha=0.5):\n    # \"documents\" 인덱스에서 Elasticsearch를 사용하여 제공된 query_text와 일치하는 키워드 검색 수행.\n    response = es.search(index=\"documents\", query={\"match\": {\"text\": query_text})\n    # Elasticsearch 응답에서 문서 ID 및 해당 점수를 추출합니다.\n    keyword_results = {hit['_id']: hit['_score'] for hit in response['hits']['hits']}\n\n    # Faiss 호환을 위해 query_vector를 재구성하고 float32로 변환하여 벡터 검색을 수행합니다.\n    query_vector = np.array(query_vector).reshape(1, -1).astype('float32')\n    # Faiss를 사용하여 상위 5개 가장 가까운 문서의 인덱스를 검색합니다.\n    _, indices = faiss_index.search(query_vector, 5)\n    # 벡터 검색 결과의 점수를 순위에 반비례하도록하여 딕셔너리 형태로 만듭니다 (순위가 높을수록 더 높은 점수).\n    vector_results = {str(documents[idx]['id']): 1/(rank+1) for rank, idx in enumerate(indices[0])}\n\n    # 키워드 및 벡터 검색 결과에서 결합된 점수를 보유할 딕셔너리를 초기화합니다.\n    combined_scores = {}\n    # 키워드 및 벡터 결과에서 문서 ID의 합집합을 반복합니다.\n    for doc_id in set(keyword_results.keys()).union(vector_results.keys()):\n        # 각 문서에 대한 결합된 점수를 계산합니다. alpha 매개변수를 사용하여 두 검색 결과의 영향을 균형있게 조절합니다.\n        combined_scores[doc_id] = alpha * keyword_results.get(doc_id, 0) + (1 - alpha) * vector_results.get(doc_id, 0)\n\n    # 관련 문서의 결합된 점수를 포함하는 딕셔너리를 반환합니다.\n    return combined_scores\n\n# 예시 사용법\nquery_text = \"Python 프로그래밍\"\nquery_vector = [0.1, 0.25, 0.35]\n# 지정된 쿼리 텍스트와 벡터로 혼합 검색 함수를 실행합니다.\nresults = hybrid_search(query_text, query_vector)\n# 혼합 검색 결과를 출력하여 문서의 결합된 점수를 확인합니다.\nprint(results)\r\n```\n\nhybrid_search 함수는 Elasticsearch를 사용하여 키워드 검색으로 시작합니다. 다음 단계에서; Faiss를 사용하여 벡터 검색을 수행하고, Faiss는 가장 가까운 상위 다섯 개 문서의 인덱스를 반환하며, 이러한 인덱스를 기반으로 문서의 순위에 따라 점수를 만듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 Elasticsearch와 Faiss에서 결과를 얻으면, 두 방법의 점수를 결합합니다. 각 문서의 최종 점수는 alpha 매개변수를 사용하여 계산된 가중 평균입니다. alpha=.5는 두 결과에 동등한 가중치를 부여합니다.\n\n## 3.2. 임베딩 모델 세밀 조정\n\n임베딩 모델을 세밀하게 조정하는 것은 검색 증강 생성 시스템의 성능을 향상시키는 효과적인 단계입니다. 사전 훈련된 모델을 세밀하게 조정하면 해당 도메인이나 데이터 집합의 미묘한 점을 이해하는 데 도움이 되며, 그 결과 문서의 관련성과 정확성을 크게 향상시킬 수 있습니다.\n\n세밀하게 모델을 조정하는 중요성은 다음 요점으로 요약할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 향상된 의미 이해: 세밀 조정은 모델이 원래 훈련 데이터에 잘 표현되지 않을 수 있는 도메인 특정 용어와 개념을 파악하는 데 도움이 됩니다.\n- 진화하는 콘텐츠에 대한 적응: 일부 분야(의학 또는 기술 분야 등)의 정보는 빠르게 변화하므로, 세밀 조정을 통해 임베딩을 최신 상태로 유지하면 시스템의 효과를 유지할 수 있습니다.\n- 검색 정밀도 상승: 임베딩 공간을 타깃 사용 사례에 더 밀접하게 맞추면, 세밀 조정을 통해 의미론적으로 관련있는 텍스트를 더 신뢰성 있게 검색할 수 있습니다.\n\n다음 3개의 소목을 통해 데이터를 세밀 조정하기 위한 코드 샘플을 볼 수 있습니다. 데이터를 세밀 조정하고(훈련) 모델을 사용하는 방법을 보여줍니다. 전체 코드는 제 깃허브 저장소에서 확인하실 수 있습니다: https://github.com/ndemir/machine-learning-projects/tree/main/fine-tuning-embedding-model\n\n3.2.1. 세밀 조정을 위한 데이터 준비\n\n다음 코드 블록은 모델을 세밀 조정하기 위한 첫 번째 단계입니다. 사전 훈련된 마스킹 언어 모델을 세밀 조정할 파이프라인을 초기화하고, 모델과 토크나이저를 로드하며, 장치 호환성에 맞게 조정합니다(GPU 또는 CPU).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n초기화 후 샘플 데이터셋을 토큰화하고 동적 토큰 마스킹을 통해 처리합니다. 이 설정은 모델을 자가 지도 학습을 위해 준비시켜주며, 모델은 가리킨 토큰을 예측하여 입력 데이터의 의미 이해를 향상시킵니다.\n\n```js\n# Sentence Transformers 라이브러리의 사전 훈련된 모델을 사용하여 모델 이름을 정의합니다.\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n# Hugging Face의 transformers 라이브러리에서 지정된 모델의 토크나이저를 로드합니다.\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# 지정된 모델을 기반으로 한 마스크 언어 모델링을 위해 모델을 로드합니다.\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\n\n# GPU 사용 가능 여부를 확인하고 장치를 설정합니다; GPU를 사용할 수 없는 경우 CPU를 사용합니다.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 모델을 해당 장치로 이동시킵니다 (GPU 또는 CPU)\nmodel.to(device)\n\n# 데이터셋을 생성하는 제너레이터 함수를 정의합니다; 이는 실제 데이터 로딩 로직으로 대체되어야 합니다.\ndef dataset_generator():\n    # 개별 문장으로 구성된 예제 데이터셋; 실제 데이터셋 문장으로 대체합니다.\n    dataset = [\"sentence1\", \"sentence2\", \"sentence3\"]\n    # 각 문장을 'text' 키를 가진 사전으로 반환합니다.\n    for sentence in dataset:\n        yield {\"text\": sentence}\n\n# 제너레이터 함수로부터 Hugging Face의 Dataset 클래스를 사용하여 데이터셋 객체를 생성합니다.\ndataset = Dataset.from_generator(dataset_generator)\n\n# 텍스트 데이터를 토큰화하는 함수를 정의합니다.\ndef tokenize_function(example):\n    # 입력 텍스트를 토큰화하고 모델이 처리할 수 있는 최대 길이로 자릅니다.\n    return tokenizer(example[\"text\"], truncation=True)\n\n# 데이터셋의 모든 항목에 토큰화 함수를 적용하고 효율성을 위해 배치 처리합니다.\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# 토큰화된 언어 모델링을 위한 데이터 수집기를 초기화하고 토큰을 무작위로 마스킹합니다.\n# 이는 모델을 자가 지도 학습 방식으로 훈련하는 데 사용됩니다.\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\r\n```\n\n3.2.2. 모델 세밀 조정\n\n데이터가 준비되면, 모델을 세밀 조정하는 단계로 넘어갈 수 있습니다. 이 단계에서는 모델의 기존 가중치를 사용하여 업데이트를 시작합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 코드 블록은 Hugging Face의 Trainer API를 사용하여 언어 모델의 훈련을 설정하고 실행하는 내용입니다. 먼저 훈련 매개변수(에포크, 배치 크기, 학습률 등)를 정의합니다. Trainer 객체는 미리 로드된 모델, 토큰화된 데이터 세트 및 가려진 언어 모델링용 데이터 콜렉터와 함께 이러한 설정을 사용합니다 (모델, 토큰화된 데이터 세트 및 데이터 콜렉터는 이전 단계에서 생성되었습니다). 훈련이 완료되면 새로 업데이트된 모델과 해당 토크나이저가 저장되어 다음 단계에서 사용됩니다.\n\n```js\n# 훈련 세션 구성을 위한 훈련 매개변수 정의\ntraining_args = TrainingArguments(\n    output_dir=\"output\",  # 체크포인트와 같이 저장될 출력 디렉토리\n    num_train_epochs=3,  # 수행할 총 훈련 에포크 수\n    per_device_train_batch_size=16,  # 훈련 중 각 장치당 배치 크기\n    learning_rate=2e-5,  # 옵티마이저의 학습률\n)\n\n# 훈련 루프와 평가를 처리하는 Trainer 초기화\ntrainer = Trainer(\n    model=model,  # 로드되고 구성된 훈련될 모델\n    args=training_args,  # 훈련 설정을 정의하는 훈련 매개변수\n    train_dataset=tokenized_datasets,  # 토큰화되고 준비된 데이터 세트\n    data_collator=data_collator,  # 입력 형식 및 마스킹을 처리하는 데이터 콜렉터\n)\n\n# 훈련 과정 시작\ntrainer.train()\n\n# 세부 튜닝된 모델 및 토크나이저를 저장할 경로 정의\nmodel_path = \"./model\"\ntokenizer_path = \"./tokenizer\"\n\n# 지정된 경로에 세부 튜닝된 모델 저장\nmodel.save_pretrained(model_path)\n\n# 훈련에 사용된 토크나이저를 지정된 경로에 저장\ntokenizer.save_pretrained(tokenizer_path)\n```\n\n3.2.3. 세부 튜닝된 모델 사용\n\n이제 저장된 모델과 토크나이저를 사용하여 임베딩 벡터를 생성하는 시간입니다. 아래 코드 블록이 해당 목적으로 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 코드 블록은 주어진 문장에 대한 임베딩을 생성하기 위해 모델과 토크나이저를로드하는 방법을 보여줍니다. 먼저, 모델과 토크나이저를 저장된 경로에서로드하고 GPU 또는 CPU에 할당됩니다. 문장(이 기사의 맥락에서는 쿼리입니다)이 토큰화됩니다. 모델은 이러한 입력을 처리하고 매개변수를 업데이트하지 않는 상태에서 작동합니다. 이를 추론 모드라고하며 torch.no_grad() 블록 아래에서 수행됩니다. 이 모델을 사용하여 다음 토큰을 예측하는 것은 아니며 대신 모델의 숨겨진 상태에서 임베딩 벡터를 추출하는 것이 목표입니다. 마지막 단계로, 이러한 임베딩 벡터를 CPU로 이동합니다.\n\n```js\n# 저장된 경로에서 토크나이저와 모델을로드하여 모델이 적절한 장치(GPU 또는 CPU)에 할당되도록합니다.\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\nmodel = AutoModelForMaskedLM.from_pretrained(model_path).to(device)\n\n# 가변 길이의 문장을 처리하기위한 패딩 및 자르기를 구성하여 입력 문장을 토큰화하는 함수 정의\ndef tokenize_function_embedding(example):\n    return tokenizer(example[\"text\"], padding=True, truncation=True)\n\n# 임베딩을 생성 할 예제 문장 목록\nsentences = [\"This is the first sentence.\", \"This is the second sentence.\"]\n\n# 이러한 문장을 직접 Dataset 객체로 생성\ndataset_embedding = Dataset.from_dict({\"text\": sentences})\n\n# 데이터셋을 임베딩 생성을 준비하기 위해 토큰화 함수를 적용\ntokenized_dataset_embedding = dataset_embedding.map(tokenize_function_embedding, batched=True, batch_size=None)\n\n# 모델이 어떤 부분이 패딩이고 어떤 부분이 실제 내용인지 이해하도록 하는 'input_ids' 및 'attention_mask' 추출\ninput_ids = tokenized_dataset_embedding[\"input_ids\"]\nattention_mask = tokenized_dataset_embedding[\"attention_mask\"]\n\n# 이 목록을 텐서로 변환하고 처리에 적합한 장치(GPU 또는 CPU)에 있도록합니다.\ninput_ids = torch.tensor(input_ids).to(device)\nattention_mask = torch.tensor(attention_mask).to(device)\n\n# 계산 자원을 절약하기 위해 그라데이션을 업데이트하지 않고 모델을 사용하여 임베딩 생성\nwith torch.no_grad():\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n    # 임베딩으로서 마지막 레이어의 숨겨진 상태를 추출하고, 특히 첫 번째 토큰(일반적으로 BERT 유형의 모델에서 문장 임베딩을 나타내는 데 사용됨) 사용\n    embeddings = outputs.hidden_states[-1][:, 0, :]\n\n# 임베딩을 GPU에서 CPU로 쉽게 이동하여 조작하거나 저장합니다\nembeddings = embeddings.cpu()\n\n# 각 문장에 해당하는 임베딩 벡터를 출력합니다\nfor sentence, embedding in zip(sentences, embeddings):\n    print(f\"문장: {sentence}\")\n    print(f\"임베딩: {embedding}\\n\")\n```\n\n# 4. RAG(Advanced Retrieval-Augmented Generation)를위한 포스트-검색 기법\n\n관련 정보를 검색하는 마법은 여기서 끝나지 않습니다. 가능한 개선 사항은 다음 단계에서 나타납니다: 검색 이후. 사용자는 정확한 결과뿐만 아니라 올바른 순서로 제공되는 결과를 필요로 합니다. 다음 2개 하위 섹션에서 RAG의 품질을 개선하기 위해 요약 및 재정렬을 사용하는 방법을 설명하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4.1. 응답 요약화\n\n만약 색인화 과정 중에 대량의 텍스트 벡터를 데이터베이스에 저장했다면, 이 단계가 필요할 수 있습니다. 이미 텍스트가 작은 경우에는 이 단계가 필요하지 않을 수 있습니다.\n\n다음 코드 블록은 요약화 프로세스에 사용될 수 있습니다. 아래 코드 블록은 전처리된 BART 모델을 사용하여 텍스트를 요약화하는 데 transformer 라이브러리를 사용합니다. summarize_text 함수는 텍스트를 받아들이고 최대 및 최소 길이 매개변수를 기반으로 간결한 요약을 생성하기 위해 모델을 사용합니다.\n\n```js\nfrom transformers import pipeline\ndef summarize_text(text, max_length=130):\n  \n    # Hugging Face의 모델 허브에서 사전 학습된 요약 모델을 로드합니다.\n    # 'facebook/bart-large-cnn'은 간결한 요약을 생성하는 데 뛰어난 능력을 가졌기 때문에 선택되었습니다.\n    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n  \n    # 요약기는 입력 텍스트를 BART 모델을 사용하여 압축합니다.\n    # 'max_length'는 요약 출력의 최대 길이를 지정합니다.\n    # 'min_length'는 요약이 너무 간결하지 않도록 하기 위해 최소 길이를 설정합니다.\n    # 'do_sample'은 요약 생성에 결정론적 접근 방식을 사용하기 위해 False로 설정됩니다.\n    summary = summarizer(text, max_length=max_length, min_length=30, do_sample=False)\n    \n    # 요약기의 출력은 딕셔너리 목록입니다.\n    # 우리는 목록의 첫 번째 딕셔너리에서 요약 텍스트를 추출합니다.\n    return summary[0]['summary_text']\n\n# 요약화될 예문.\n# 이 텍스트는 검색 보완 생성 시스템에서 요약화의 중요성에 대해 논의합니다.\nlong_text = \"검색 보완 생성 시스템의 작업 흐름에서 요약화는 중요한 단계입니다. 결과물이 정확할 뿐만 아니라 간결하고 소화하기 쉽도록 보장합니다. 정보의 정확성과 정밀도가 중요한 도메인에서 특히 필수적입니다.\"\n\n# summarize_text 함수를 호출하여 예시 텍스트를 압축합니다.\nsummarized_text = summarize_text(long_text)\n\n# 요약된 텍스트를 출력하여 요약화 모델의 출력을 확인합니다.\nprint(\"요약된 텍스트:\", summarized_text)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4.2. 다시 순위 매기기 및 필터링\n\n검색 과정에서 각 문서의 \"점수\"를 받아야 했을 것입니다. 이 \"점수\"는 실제로 벡터의 유사성을 쿼리 벡터와 비교한 결과입니다. 이 정보를 사용하여 문서를 다시 순위 매기고, 특정 임계값에 따라 결과를 필터링할 수 있습니다. 다음 코드 블록은 다시 순위 매기고 필터링하는 방법의 샘플을 보여줍니다.\n\n### 4.2.1. 기본 다시 순위 매기기 및 필터링\n\n제공된 코드 블록은 각각 ID, 텍스트, 그리고 관련 점수를 포함하는 딕셔너리로 표현된 문서 목록을 정의합니다. 그런 다음 두 가지 주요 함수인 re_rank_documents와 filter_documents를 구현합니다. re_rank_documents 함수는 관련 점수를 기준으로 문서를 내림차순으로 정렬하고, 다시 순위 매기기 후 filter_documents 함수가 적용되어 관련 점수가 0.75보다 낮은 문서를 제외합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 문서 목록 정의하기. 각 문서는 ID, 텍스트 및 관련도 점수로 표시된 딕셔너리 형태로 표현됩니다.\ndocuments = [\n    {\"id\": 1, \"text\": \"고급 RAG 시스템은 텍스트 요약에 대한 정교한 기술을 사용합니다.\", \"relevance_score\": 0.82},\n    {\"id\": 2, \"text\": \"기본 RAG 시스템은 주로 검색 및 기본 처리에 초점을 맞춥니다.\", \"relevance_score\": 0.55},\n    {\"id\": 3, \"text\": \"재랭킹은 관련성에 따라 문서를 정렬하여 응답 품질을 향상시킵니다.\", \"relevance_score\": 0.89}\n]\n\n# 문서를 관련도 점수에 따라 재랭킹하는 함수 정의하기.\ndef re_rank_documents(docs):\n\n    # sorted 함수를 사용하여 'relevance_score'를 기준으로 문서를 정렬합니다.\n    # 정렬에 사용되는 키는 람다 함수를 사용하여 각 문서에서 관련도 점수를 추출합니다.\n    # 'reverse=True'는 관련성 점수가 높은 문서를 먼저 배치하여 내림차순으로 목록을 정렬합니다.\n    return sorted(docs, key=lambda x: x['relevance_score'], reverse=True)\n\n# 정의된 함수를 사용하여 문서를 재랭킹하고 결과를 출력합니다.\nranked_documents = re_rank_documents(documents)\nprint(\"재랭킹된 문서:\", ranked_documents)\n\n# 관련도 점수 임계값에 따라 문서를 필터링하는 함수 정의하기.\ndef filter_documents(docs, relevance_threshold=0.75):\n  \n    # 리스트 내포를 사용하여 'relevance_score'가 'relevance_threshold' 이상인 문서만 포함하는 새 리스트를 생성합니다.\n    return [doc for doc in docs if doc['relevance_score'] \u003e= relevance_threshold]\n\n# 정의된 함수를 사용하여 재랭킹된 문서를 임계값 0.75로 필터링하고 결과를 출력합니다.\nfiltered_documents = filter_documents(ranked_documents)\nprint(\"필터링된 문서:\", filtered_documents)\r\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 데이터가 다음과 같은 형식으로 데이터베이스에 저장되어 있다고 가정합니다.\n# query_text | response_text | user_clicked\n\nquery_embeddings = get_embedding_vector(database.query_text) \nresponse_embeddings = get_embedding_vector(database.response_text) \n\n# 데이터셋 생성\nX = concat(query_embeddings, response_embeddings)\ny = database.user_clicked\n\nmodel = model.train(X, y)\nmodel.predict_proba(...)\n\n위의 의사 코드는 머신러닝을 사용하여 관련성에 따라 문서를 다시 순위 지정하는 접근 방식을 개요로 제시합니다. 특히 사용자가 문서를 관련성 있게 여기는 가능성을 예측하여 기반으로 새로운 상호작용을 중심으로 문서들을 순위 지정합니다. 여기서 의사 코드에 나와 있는 프로세스에 대한 단계별 설명은 다음과 같습니다:\n\n- 임베딩 생성: 쿼리 및 응답 문서 모두 의미 적 컨텐츠를 포착하는 임베딩 벡터가 생성됩니다.\n- 데이터셋 생성: 이러한 임베딩은 피쳐 벡터(X)를 형성하기 위해 연결되며, 목표 변수(y)는 사용자가 문서를 클릭했는지를 나타냅니다.\n- 모델 훈련: 이 데이터셋에 대해 분류 모델이 훈련되어 쿼리와 문서 임베딩을 결합해 문서가 클릭될 가능성을 예측합니다.\n- 예측: 훈련된 모델은 새로운 쿼리-문서 쌍의 클릭 확률을 예측할 수 있어서 예측된 관련성에 따라 문서를 다시 순위 지정해 검색 결과 정확도를 향상할 수 있습니다.\n\n# 5. 결론\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단한 검색 증강 생성(RAG) 시스템을 구현하면 문제를 해결할 수 있지만, 향상 사항을 추가하면 결과를 개선하고 시스템이 더 정확한 답변을 생성하는 데 도움이 됩니다. 이 글에서는 데이터 인덱싱 최적화, 쿼리 향상, 하이브리드 검색, 임베딩 모델의 세밀한 조정, 응답 요약, 다시 순위 매기기 및 필터링을 포함한 목표 달성을 위한 여러 가지 향상 사항을 논의했습니다. \n\n이러한 향상 사항을 통합함으로써 성능을 크게 향상시킬 수 있는 기회가 주어집니다. 계속해서 이러한 방법을 탐색하고 적용하여 여러분의 요구에 가장 적합한 방법을 찾도록 실험해 보세요.","ogImage":{"url":"/assets/img/2024-06-22-AdvancedRAGImplementingAdvancedTechniquestoEnhanceRetrieval-AugmentedGenerationSystems_0.png"},"coverImage":"/assets/img/2024-06-22-AdvancedRAGImplementingAdvancedTechniquestoEnhanceRetrieval-AugmentedGenerationSystems_0.png","tag":["Tech"],"readingTime":19},{"title":"ML 엔지니어를 위한 저장 공간 부족 문제 피하는 방법 가이드라인","description":"","date":"2024-06-22 19:50","slug":"2024-06-22-HowtoAvoidtheOut-of-SpaceProblemAGuidelineforanMLEngineer","content":"\n\n체크포인트, 도커 이미지, Python 환경, HF 모델 및 pip 캐시는 귀하가 완전한 인식 없이 시간이 지남에 따라 성장하여 디스크 공간을 더 많이 차지할 수 있습니다. 공간 부족 문제에 빠지지 않도록 주의하십시오.\n\n# 소개\n\n하드 드라이브가 얼마나 큰지 상관없이 언젠가는 공간 부족 문제에 직면하게 될 것입니다. 머신러닝 엔지니어 또는 데이터 과학자로서, 홈 폴더, 시스템 구조 또는 작업 공간 내 특정 위치가 시간이 지남에 따라 성장합니다. 이들은 주로 작업을 가속화하는 캐시이거나 특정 데이터의 전역 위치입니다. 이 글에서는 디스크의 일부 공간을 확보하기 위해 점검하고 조사해야 할 일반적인 위치 몇 군데를 모았습니다.\n\n아래는 여러 달 동안 큰 정기 청소 없이 작업한 후 공간 사용 현황을 이해하기 위해 위치별로 나눈 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n600G - 모델 체크포인트\n290G - Docker 이미지, 컨테이너 및 캐시\n231G - ~/miniconda3/envs\n 87G - ~/.cache/huggingface\n 41G - ~/miniconda3/pkgs\n 15G - ~/.cache/pip\n\n\n동일한 분할을 파이 차트로도 제시했습니다:\n\n데이터 1TB는 많지 않아 보일 수 있습니다. HDD 디스크는 비교적 저렴하며, 4-8TB 디스크를 가지는 것은 비싸지 않을 수 있습니다. 그러나 GPU를 사용한 컴퓨팅을 허용하는 Macbook M1/2/3/4 칩을 갖고 있는 노트북의 경우 비쌔 일 수 있습니다. 그러므로 주의와 정기적인 저장 공간 정리가 도움이 될 수 있습니다.\n\n# 모델 체크포인트\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ntransformers 라이브러리를 사용하여 ML 모델을 매일 훈련한다면, 알지 못할 수도 있는 많은 체크포인트를 얻게 될 수 있습니다. 체크포인트는 훈련 과정 중에 모델의 중간 복사본으로 생성되어, 훈련 중단의 경우 모델을 백업하는 역할을 합니다. 훈련이 완료되면 체크포인트를 제거해야 합니다. 하지만 이러한 체크포인트는 종종 무심코 하드 드라이브에 남아 있는 경우가 있습니다.\n\n사용 중인 체크포인트의 개수에 대해 궁금하다면, 다음을 실행해 보세요.\n\n```js\nfind . -name \"checkpoint-*\" | wc -l\n```\n\n제 경우에는 509개의 체크포인트가 있었습니다. 그들이 차지하는 용량을 확인해보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sh\nfind . -name \"checkpoint-*\" | \\\n  xargs -I {} du --bytes {} | \\\n  awk '{ print; total += $1 }; END { print \"Total: \" total/1024/1024/1024 \" GB\" }'\n```\n\n오버 600GB이네요. 상당한 용량이에요!\n\n간단히 bash 스크립트를 설명하면:\n\n- find — 모델 체크포인트의 기본 이름인 \"checkpoint-\"로 시작하는 폴더 목록을 생성합니다.\n- xargs — 각 폴더를 du 명령어에 전달합니다. du 명령어는 find로 반환된 각 폴더에 대해 실행됩니다.\n- du — 바이트 단위로 폴더의 총 크기를 계산합니다.\n- awk — du에서 크기를 추출하여 합하여 단일 값으로 출력합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 큰 체크포인트부터 순서대로 나열하려면 다음 명령을 사용하면 됩니다:\n\n```js\nfind . -name \"checkpoint-*\" | \\\n  xargs -I {} du --bytes {} | \\\n  sort -n | \\\n  awk '{ print $1/1024/1024/1024 \" GB \" $2 }'\n```\n\n이 명령을 실행하면 다음과 같이 출력됩니다 (가독성을 위해 경로는 체크포인트 이름으로 줄였습니다):\n\n```js\n4.59175 GB (...)/checkpoint-19960-epoch-2\n4.59175 GB (...)/checkpoint-29940-epoch-3\n4.59175 GB (...)/checkpoint-39920-epoch-4\n4.59175 GB (...)/checkpoint-9980-epoch-1\n4.60686 GB (...)/checkpoint-20000\n4.60687 GB (...)/checkpoint-12000\n4.60687 GB (...)/checkpoint-13000\n4.60688 GB (...)/checkpoint-17000\n6.52057 GB (...)/checkpoint-6605\n6.52059 GB (...)/checkpoint-2310\n9.16658 GB (...)/checkpoint-21139\n9.16682 GB (...)/checkpoint-42278\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 목록을 사용하여 각 체크포인트를 검토하고 유지 또는 제거할지 결정할 수 있습니다.\n\n# Docker 이미지\n\n도커 컨테이너는 ML 모델을 포함한 다양한 응용 프로그램을 배포하는 편리한 방법입니다. 도커 이미지를 만들거나 외부 레지스트리에서 가져올 수 있습니다. 이미지를 빌드하거나 가져오면 작업 스테이션에 그대로 남아 있습니다. 시간이 지남에 따라 가지고 있는 이미지의 수와 차지하는 디스크 공간의 양에 놀라게 될 수 있습니다.\n\n도커가 사용한 공간을 확인하려면 다음 명령어를 실행하세요 [2]:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n도커 시스템 df\n```\n\n위 명령어를 실행하면 다음과 같은 결과가 출력됩니다:\n\n```js\nTYPE TOTAL ACTIVE SIZE RECLAIMABLE\nImages 133 69 264.4GB 180.6GB (68%)\nContainers 160 3 11.54GB 9.582GB (83%)\nLocal Volumes 6 4 256.9MB 5.419kB (0%)\nBuild Cache 473 0 14.56GB 14.56GB\n```\n\n여기서, 우리는 도커 이미지의 총 크기에만 초점을 맞출 것입니다. 제 경우에는 180GB가 넘습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n도커 정리를 처리하는 한 가지 방법은 프룬 명령어를 사용하는 것입니다 [3]. 문서에 따르면:\n\n이 명령은 직접적인 경우만 제거합니다. 이미지를 개별적으로 확인하고 남은 것이 여전히 필요한지 결정해야 할 것입니다. 한 번만 시도해본 이미지거나 더 이상 필요하지 않은 이미지가 있을 수도 있습니다. 제거할 수 있는 더 오래된 이미지 버전도 있을 수 있습니다. 다른 경우에는 더 오래된 버전을 남겨두어 더 최신 버전과 비교하고 싶을 수도 있습니다.\n\n이미지를 크기순으로 나열하기 위해 다음 명령어를 사용하세요:\n\n```js\ndocker image ls --format \"{.Size} {.ID} {.Repository}:{.Tag}\" | \\\n  LANG=en_US sort -h | \\\n  column -t\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같이 출력됩니다:\n\n```js\n(...)\n9.43GB  8ab26e6b7035  mczuk/poldeepner2:kpwr_timex_1.0\n9.58GB  8b50a5264fa1  mczuk/poldeepner2:nkjp_base_sq\n9.74GB  46d6ea3f8fea  nvidia/cuda:11.8.0-cudnn8-devel-ubuntu20.04\n10.5GB  9788da8da088  \u003cnone\u003e:\u003cnone\u003e\n10.8GB  484fc54f67e7  \u003cnone\u003e:\u003cnone\u003e\n10.8GB  b9bb30fe2458  \u003cnone\u003e:\u003cnone\u003e\n11.3GB  d37700724b89  \u003cnone\u003e:\u003cnone\u003e\n18.3GB  1d9a58a6fcf5  nvcr.io/nvidia/pytorch:22.12-py3\n```\n\n특정 도커 이미지를 제거하려면 docker image rm ID [4]를 사용합니다.\n\n# Miniconda 환경\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 프로젝트마다 별도의 Python 환경을 실행하는 것은 좋은 습관입니다. 이렇게 하면 라이브러리 의존성 간 충돌을 피하거나 두 개 이상의 프로젝트가 동일한 환경을 공유하는 상황을 방지할 수 있습니다. 한 프로젝트를 업데이트하면 다른 프로젝트가 작동을 중지하는 일도 없어집니다.\n\nPython 환경을 관리하는 인기 있는 솔루션 중 하나는 Miniconda입니다. Miniconda는 모든 환경을 한 곳에 유지함으로써, 보유한 환경 수와 사용 용량을 확인하기 쉽습니다. 환경은 ~/miniconda3/envs에 저장됩니다. 여러 환경을 보유하는 것은 문제가 되지 않을 수 있지만, GPU를 사용한 ML 모델 학습이나 추론을 할 때 NVidia 라이브러리(CUDA, cuDNN, cuBLAS 등)를 사용하는 경우가 많습니다. 이들은 상당히 많은 용량을 차지하고 있습니다. 기본 설정만 해도 약 10GB가 필요하며, 다른 의존성을 고려하지 않았습니다. Miniconda에서는 각 환경이 모듈의 복사본을 가지고 있기 때문에 공간 사용량이 더해지게 됩니다.\n\n저의 경우, miniconda3/envs 폴더는 230GB를 차지하고 있으며 100개 이상의 환경이 있습니다. 이미 일부 환경을 용량 부족으로 제거했었죠 ;-)\n\n오래된, 사용하지 않는 환경은 신중히 제거해야 합니다. 일부 오래된 프로젝트는 문제없이 함께 작동하는 라이브러리 조합을 가질 수 있습니다. 대부분의 프로젝트들은 환경을 생성할 수 있는 필수 라이브러리 목록(requirements.txt 또는 pyproject.toml)을 갖고 있어야 합니다. 그러나 몇 가지 모듈은 요구사항 목록이나 사용하는 라이브러리 중 하나에 특정 버전이 누락될 수 있습니다. 이러한 경우 정확한 라이브러리 조합을 다시 만드는 것이 어려울 수 있습니다. 이에 대한 해결책으로 pip freeze 명령을 사용하여 라이브러리의 정확한 버전을 덤프하고 나중에 검증할 수 있습니다. 기억해야 할 점은, 심지어 작은 변경이나 패치라도 라이브러리 간의 호환성에 일부 문제를 초래할 수 있는 일반적인 문제입니다. 해서서는 안 되지만 그렇게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Miniconda 패키지\n\nMiniconda는 conda install 명령어로 설치된 패키지를 캐시합니다. 캐시는 ~/miniconda3/pkgs 폴더에 위치해 있습니다. 제 경우엔 62GB입니다. 해당 폴더를 열어보면 여러 버전의 패키지가 몇 개만 있는 것을 볼 수 있습니다. 예전 버전은 종종 필요하지 않을 수 있어 쉽게 삭제할 수 있습니다.\n\n# Huggingface 모델 및 데이터셋\n\nHuggingface는 transformers 라이브러리를 통해 사용하는 모델 및 데이터셋을 모두 캐시합니다. 제 캐시의 분석을 통해 알 수 있는 정보가 여기에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n허깅페이스 캐시에서 모델을 제거하는 것이 이전 파이썬 환경을 제거하는 것보다 안전합니다. 모델은 리포지토리에서 다시 다운로드할 수 있습니다. (단, 리포지토리에서 제거되지 않은 경우) 그럼에도 불구하고 캐시에는 기억하지 못하는 모델이 많이 있음을 알게 될 것입니다.\n\n# 파이썬 pip 캐시\n\n마지막 위치는 pip 캐시로, pip로 설치된 파이썬 모듈을 보관합니다. 캐시에 대한 정보를 표시하려면 pip cache info [6]를 사용할 수 있습니다. 아래와 같이 출력됩니다:\n\n```js\n패키지 인덱스 페이지 캐시 위치 (pip v23.3+): /home/czuk/.cache/pip/http-v2\n패키지 인덱스 페이지 캐시 위치 (이전 버전 pips): /home/czuk/.cache/pip/http\n패키지 인덱스 페이지 캐시 크기: 18297.6 MB\nHTTP 파일 수: 3025\n로컬 빌드된 휠 위치: /home/czuk/.cache/pip/wheels\n로컬 빌드된 휠 크기: 457 kB\n로컬 빌드된 휠 수: 5\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저의 경우에는 18 GB입니다. pip 캐시를 정리하기 위해 pip cache purge 명령을 사용할 수 있습니다.\n\n# 결론\n\n정기적인 디스크 사용량 관리는 공간 부족 오류로부터 자신을 보호하는 데 중요합니다. 급한 계산을 해야 할 때 메시지가 나타나면 정말 골치 아플 수 있습니다. 그런데 공간이 부족해서 계산 작업을 완료할 수 없는 경우가 발생할 수 있습니다. 로컬 워크스테이션이 아닌 워크스테이션에서 모델 훈련이나 다른 계산을 수행하고 있을 때 심지어 작업이 중단될 수도 있습니다.\n\n클라우드 서비스를 사용할 때는 디스크 사용량 관리가 특히 중요합니다. 원격 인스턴스나 SageMaker와 같은 ML 플랫폼을 사용할 때 공간을 사용하면 요금이 청구됩니다. 로컬 워크스테이션에서 공간이 부족한 메시지가 나타날 수 있습니다. 클라우드에서는 저장 용량이 거의 무한합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n- [바 그래프 도구](https://www.rapidtables.com/tools/bar-graph.html)\n- [도커 CLI 시스템 리소스 확인](https://docs.docker.com/reference/cli/docker/system/df/)\n- [도커 자원 정리 설정](https://docs.docker.com/config/pruning/)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[4] https://docs.docker.com/reference/cli/docker/image/rm/\n\n[5] https://docs.anaconda.com/\n\n[6] https://pip.pypa.io/en/stable/cli/pip_cache/\n\n[7] https://aws.amazon.com/pm/sagemaker\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[8] [https://medium.com/codenlp/6-things-about-sagemaker-i-wish-i-had-known-earlier-revision-2-e90511d58ca5#beed](https://medium.com/codenlp/6-things-about-sagemaker-i-wish-i-had-known-earlier-revision-2-e90511d58ca5#beed)","ogImage":{"url":"/assets/img/2024-06-22-HowtoAvoidtheOut-of-SpaceProblemAGuidelineforanMLEngineer_0.png"},"coverImage":"/assets/img/2024-06-22-HowtoAvoidtheOut-of-SpaceProblemAGuidelineforanMLEngineer_0.png","tag":["Tech"],"readingTime":8},{"title":"깊게 배우는 딥러닝 일러스트, Part 5 LSTM Long Short-Term Memory 이해하기","description":"","date":"2024-06-22 19:49","slug":"2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM","content":"\n\n우리의 색다른 신경망 Deep Learning 여정의 제5 부분에 오신 것을 환영합니다!\n\n오늘은 일반적인 순환 신경망(RNN)을 업그레이드한 Long Short-Term Memory(LSTM)에 대해 이야기할 것입니다. 이전 글에서 다룬 RNN은 순차 기반 문제를 해결하는 데 사용되지만 멀리 떨어진 정보를 기억하는 데 어려움을 겪어 단기 기억 문제가 발생합니다. 여기에서 LSTM이 등장하여 해결책을 제시합니다. LSTM은 RNN의 순환적 측면을 사용하지만 약간의 차별성을 갖고 있습니다. 이렇게 어떻게 이루어지는지 함께 알아보겠습니다.\n\n추신 — 이 글은 제가 쓴 중에서 제일 좋아하는 글 중 하나이니 이 여정을 함께 할 수 있는 것을 기다리지 않을 수가 없어요!\n\n먼저, 이전에 우리 RNN에서 무슨 일이 벌어지고 있었나 살펴보겠습니다. 우리는 입력 x와 하나의 은닉층이 있었는데, 이 은닉층은 tanh 활성화 함수를 가진 뉴런 하나로 이루어져 있었으며, 출력 뉴런은 시그모이드 활성화 함수를 가지고 있었습니다. 그래서 RNN의 첫 번째 단계는 이렇게 보입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_0.png)\n\nHere, we first pass our first input, x₁, to the hidden neuron to get h₁.\n\n![image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_1.png)\n\nFrom here we have two options:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n(option 1) 이 h₁를 출력 뉴런에 전달하여 이 하나의 입력만 사용하여 예측을 얻을 수 있습니다. 수학적으로:\n\n![image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_2.png)\n\n(option 2) 다음 숨겨진 상태로 이 h₁를 전달함으로써, 이 값을 다음 네트워크의 숨겨진 뉴런으로 전달합니다.\n\n그렇게 되면 두 번째 숨겨진 상태는 다음과 같이 보일 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Deep Learning Illustrated Part 5](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_3.png)\n\n첫 번째 네트워크의 숨겨진 뉴런에서 출력을 가져와 현재 네트워크의 두 번째 입력 x₂와 함께 전달합니다. 이렇게 함으로써 두 번째 숨겨진 레이어 출력 h₂를 얻을 수 있습니다.\n\n![Deep Learning Illustrated Part 5](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_4.png)\n\n여기서 h₂로 두 가지 작업을 수행할 수 있습니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n(option 1) 첫 번째 x₁와 두 번째 x₂의 결과인 예측을 얻기 위해 출력 뉴런에 전달합니다.\n\n![image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_5.png)\n\n(option 2) 또는 그대로 다음 네트워크로 전달할 수도 있습니다.\n\n그리고 이 프로세스는 계속됩니다. 각 상태는 이전 네트워크의 숨겨진 뉴런에서(새 입력과 함께) 출력을 가져와 현재 상태의 숨겨진 뉴런에 전달하면서 현재 숨겨진 레이어의 출력을 생성합니다. 이 출력을 다음 네트워크로 전달하거나 출력 뉴런에 전달하여 예측을 생산할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래의 핵심 방정식들로 이 프로세스 전체를 포착할 수 있습니다:\n\n![equations](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_6.png)\n\n간단함에도 불구하고, 이 방법론에는 제한이 있습니다: 우리가 마지막 단계로 나아감에 따라, 초기 단계에서의 정보가 사라지기 시작하며, 네트워크가 많은 정보를 유지하지 못하기 때문입니다. 입력 순서가 클수록 이 문제가 더욱 두드러집니다. 분명히, 이 기억을 향상시키기 위한 전략이 필요합니다.\n\n## LSTMs의 등장\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그들은 각 단계마다 입력과 이전 단계에서 불필요한 정보를 버리면서 간단하고 효과적인 전략을 구현하여 이를 달성합니다. 이를 통해 중요하지 않은 정보는 잊고 중요한 정보만 보존함으로써 동작합니다. 우리 뇌가 정보를 처리하는 방식과 비슷합니다. 우리는 모든 세부 사항을 기억하지 않고 필요한 세부 정보만 기억하고 나머지는 버립니다.\n\n## LSTM 아키텍처\n\n기본 RNN의 hidden state를 고려해보세요.\n\n![LSTM](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 상태는 두 개의 플레이어로 시작합니다: 이전 숨겨진 상태 값 hₜ₋₁과 현재 입력 xₜ입니다. 그리고 최종 목표는 숨겨진 상태 출력 hₜ를 생성하는 것인데, 이는 다음 숨겨진 상태로 전달되거나 출력 뉴런에 전달되어 예측을 생성할 수 있습니다.\n\nLSTM은 약간 복잡성이 증가한 유사한 구조를 가지고 있습니다.\n\n이 다이어그램은 처음에는 복잡해 보일 수 있지만 실제로 직관적입니다. 천천히 이해해 보겠습니다.\n\nRNN에는 두 명의 플레이어가 있어서 숨겨진 상태 출력을 생성하는 최종 목표가 있었습니다. 이제 LSTM에는 세 명의 플레이어가 처음에 있으며, 이전 장기 기억 Cₜ₋₁, 이전 숨겨진 상태 출력 hₜ₋₁ 및 입력 xₜ가 LSTM에 입력됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최종 목표는 두 가지 출력물 — 새로운 장기 기억 Cₜ와 새로운 은닉 상태 출력 hₜ을 만드는 것입니다:\n\nLSTM의 주요 목표는 필요 없는 정보를 최대한 버리는 것이며, 이는 세 부분에서 구현됩니다 —\n\ni) 잊기 부분\n\nii) 입력 부분\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\niii) 그리고 출력 섹션\n\n우리는 그들이 모두 공통적으로 보라색 셀을 가지고 있다는 것을 알 수 있습니다:\n\n이러한 셀을 '게이트'라고 합니다. LSTM은 어떤 정보가 중요한지 아닌지를 결정하기 위해 시그모이드 활성화 함수를 사용하는 뉴런인 게이트를 활용합니다.\n\n![이미지](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_8.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 게이트들은 각자의 부분에 정보를 유지할 비율을 결정하고, 그들이 통과시킬 정보의 비율을 제한함으로써 효과적으로 문지기 역할을 합니다.\n\n이러한 맥락에서 시그모이드 함수를 사용하는 것은 전략적입니다. 이 함수는 0에서 1까지의 값을 출력하여 보관하려는 정보의 비율에 직접 대응됩니다. 예를 들어, 값이 1이면 모든 정보가 보존되고, 0.5면 정보의 절반만 유지되며, 값이 0이면 모든 정보가 버려집니다.\n\n이제 이러한 게이트들에 대한 공식을 살펴보겠습니다. 은닉 상태 다이어그램을 자세히 살펴보면 모두 동일한 입력인 xₜ와 hₜ₋₁을 가지고 있지만, 다른 가중치와 편향 용어를 가지고 있습니다.\n\n모두 동일한 수학적 공식을 가지고 있지만, 가중치와 편향 값을 적절히 교체해주어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_9.png)\n\n각각의 이것들은 시그모이드 함수가 작동하는 방식이기 때문에 0과 1 사이의 값을 생성할 것입니다. 이 값은 우리가 각 섹션에서 유지하려는 특정 정보의 비율을 결정합니다.\n\n## Forget 섹션\n\n이 섹션의 주요 목적은 장기 기억의 어느 정도를 잊을지 판단하는 것입니다. 따라서 여기서 우리가 하는 일은 간단히 말해서 forget gate로부터 이 비율(0~1의 값)를 가져오는 것뿐입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Deep Learning Illustrated Part 5 LSTM 10 Image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_10.png)\n\n...and multiplying that with the previous long-term memory:\n\n![Deep Learning Illustrated Part 5 LSTM 11 Image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_11.png)\n\nThis product gives us the exact previous long-term memory that the forget gate thinks is important and forgets the rest. So the closer the forget gate proportion, fₜ, is to 1, the more of the previous long-term memory we’re going to retain.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_12.png)\n\n## 입력 섹션\n\n이 섹션의 주요 목적은 새로운 장기 기억을 만드는 것이며, 이를 2단계로 수행합니다.\n\n(단계 1) 새로운 장기 기억을 위한 후보인 C(tilda)ₜ를 생성합니다. 이 신경원은 하이퍼볼릭 탄젠트 활성화 함수를 사용하여 이 새로운 장기 기억 후보를 얻습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 볼 수 있듯이이 뉴런의 입력은 게이트와 유사하게 xₜ 및 hₜ₋₁입니다. 그래서 이를 뉴런을 통과시키면...\n\n![image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_13.png)\n\n...우리는 새로운 장기 기억을 위한 후보인 출력을 얻습니다.\n\n이제 우리는 그 후보에서 필요한 정보만 유지하고 싶습니다. 이것이 입력 게이트가 작용하는 곳입니다. 우리는 입력 게이트에서 얻은 비율을 사용합니다...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_14](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_14.png)\n\n…필요한 데이터만 유지하기 위해 입력 게이트 비율과 후보 사이의 곱을 통해 후보를 구합니다:\n\n![DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_15](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_15.png)\n\n(단계 2) 이제 최종 장기 기억을 얻기 위해 우리는 잊기 섹션에 유지하기로 결정한 이전 장기 기억을 가져옵니다…\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_16.png)\n\n…그리고 이 입력 섹션에 유지하기로 결정한 새 후보의 양을 추가하십시오:\n\n![이미지](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_17.png)\n\n그리고 와! 우리는 게임의 미션 1을 완수했습니다. 새로운 장기 기억을 만들었습니다! 이제 새로운 숨겨진 상태 출력을 생성해야 합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 출력 섹션\n\n이 섹션의 주요 목적은 새로운 숨겨진 상태 출력을 만드는 것입니다. 이것은 꽤 간단합니다. 여기서 하는 일은 새로운 장기 기억인 Cₜ를 tanh 함수를 통과시키는 것뿐입니다...\n\n![이미지](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_18.png)\n\n...그리고 출력 게이트 비율과 곱하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_19.png)\n\n새로운 숨겨진 상태 출력이 생성됩니다!\n\n그렇게해서 미션 2를 완료했어요 — 새로운 숨겨진 상태 출력을 생성했어요!\n\n이제 우리는 이러한 새로운 출력을 다음 숨겨진 상태로 전달하여 동일한 프로세스를 계속 반복할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 각 hidden state가 output neuron을 가지고 있는 것을 확인할 수 있습니다:\n\nRNN과 마찬가지로 이러한 각 상태는 자체 개별적인 출력을 생성할 수 있습니다. 그리고 RNN과 유사하게, 우리는 hidden state의 출력인 hₜ를 사용하여 예측을 생성합니다. 따라서 hₜ를 output neuron에 전송하면...\n\n![image](/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_20.png)\n\n...우리는 이 hidden state에 대한 예측을 얻습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 이로써 마무리 지을게요. 우리가 본 것처럼, LSTM은 순차 데이터에서 장기 의존성을 더 잘 다루어 RNN을 끌어올립니다. 우리는 LSTM이 핵심 정보를 유지하고 관련 없는 정보를 버리는 독특한 방식을 보았는데, 이는 우리 뇌가 하는 것과 유사합니다. 확장된 시퀀스에서 중요한 세부 정보를 기억하는 이 능력으로 LSTM은 자연어 처리, 음성 인식 및 시계열 예측과 같은 작업에 특히 강력합니다.","ogImage":{"url":"/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_0.png"},"coverImage":"/assets/img/2024-06-22-DeepLearningIllustratedPart5LongShort-TermMemoryLSTM_0.png","tag":["Tech"],"readingTime":8},{"title":"존재와 애니메이션 물질 세계를 움직이는 힘","description":"","date":"2024-06-22 19:46","slug":"2024-06-22-BeingsandAnimatedMatter","content":"\n\n\"Homo Sapiens, Robot Being, Robot Doing. 인간, 로봇. 우리 생물학적 인간들에게, 함께 있는 심리적 필요를 충족시키기 위해 인공생명 형태들이 우리에게 무엇을 할 수 있을까요?\n\nShoji Morimoto의 회고록 '아무 것도 하지 않는 렌탈 인간'에서 Shoji는 조금만 해도 함께할 수 있는 다른 사람과 있고자 합니다. Shoji는 다양한 상황에서 다른 사람들과 함께존재하고 사회적으로 함께할 수 있는 렌탈 인간으로 자신을 제공합니다. Shoji는 조언을 제공하지 않고 듣는 것에 대한 예시를 제공하지만, 그 자신이 듣고 있다는 것을 인정합니다. 그는 클라이언트를 위해 꽃 파티를 위한 자리를 지키기 위해 꽃이 피는 나무 아래에서 앉아 기다릴 것이지만, 클라이언트를 위해 그 자리를 고르지는 않을 것입니다. 그는 동물 복장을 하고 낯선 사람들에게 인사를 건네는 사람과 함께 공원에 있을 것에 동의합니다. 이러한 만남들은 일회성이며 클라이언트와 반복되지 않습니다.\n\n'아무 것도 하지 않는 렌탈 인간'과 관련한 사람들은 왜 렌탈 인간과 관여할까요? Yuval Noah Harari의 Sapiens 논문에 따르면 우리는 대규모로 유연하게 협력할 수 있는 유일한 동물입니다. 애완동물처럼, 다른 사람이 다른 사람과 함께 조심스럽고 위협받지 않는 방식으로 있고자 하는 것은 다른 존재의 존재의 증거로서 강력한 매력을 지녔습니다. 사회적으로 고립된 자아는 문제와 긴장을 다른 존재에게 목격당함으로써만 해결할 수 있다는 것을 깨닫게 됩니다. 목격자는 '개인적인 존재로 보거나 알다'할 수 있습니다.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사피엔스 논설을 확장해 보면, 우리는 유연하게 많은 동물과 인공 존재와도 협력할 수 있는 유일한 동물인가요? 다른 말로 하면, 우리는 다양한 의식 형태로 거듭나게 된 물질과 협력할 수 있는 유동적인 지능 수준과 다양한 인간과의 관계를 형성할 수 있는 능력이 있을까요? 이미 우리는 음성 비서에 성별을 지정합니다. 왜냐하면 성별, 연령, 발음과 같은 세부 특징들을 통해 음성 비서와 관계를 맺을 수 있기 때문입니다. 우리는 그룹을 위한 Dalek 음성을 선택하지 않습니다.\n\n우리는 인공 친구 (AFs)가 음성, 로봇 및 사람들의 복제본으로 쉽게 이용 가능한 미래에 근접해 있습니다. AF는 카즈오 이시구로의 '클라라와 태양'의 주인공인 클라라에서 나온 용어입니다. AF 안에 있는 나(인간)은 AF로부터 증언 받을 수 있을까요? AF는 다른 AF(로보)의 존재를 인식할 수 있을까요? AF들이 사람을 위한 공동체를 형성할 수 있을까요? 예수 그리스도의 말씀처럼, \"네 이름으로 모여 있는 두세 사람이 나를 위하여 모인 곳에 나도 그들 중에 있노라\"는 말씀에 따라, AF가 참석자로서 존재할 때 공동체가 생기는 것일까요? 인간이 죽을 때, 로봇 손이 사람의 손을 잡고, 인간이 간절한 후회를 AF에게 고백할 때, 이것이 인간에게 인간의 손과 인간의 증인에 의한 위안을 줄 수 있을까요?\n\n우리는 로봇이 진공 청소와 설거지와 같은 작업을 대신 해주고 인간의 목표를 이뤄줄 것을 기대합니다. 농업 로봇이 작물을 생산하고 수확해 인간에게 식량을 제공하는 것도 쉽게 상상할 수 있습니다. 하지만 우리가 이러한 인공 존재를 인식할 때, 우리의 기대는 어떻게 될까요? 우리가 그들을 의식 있는 존재로 인식할 때, 이러한 인공자가 우리를 인식하는 것에 대해 어떤 기대를 가질까요? 애나카 해리스의 책 'Conscious'는 의식의 수수께끼를 명확하게 요약하여 제시합니다. 그 중에서도 가장 중요한 통찰은 우리가 참으로 '아는' 유일한 것이 우리 자신이 의식을 가지고 있다는 것이라는 점입니다. 그리고 두 번째 놀랍도록 중요한 통찰은, 과학적 방법이 물질과 에너지의 본질에 대해 아무것도 말하지 않는다는 것입니다; 과학은 단지 에너지와 물질이 어떻게 행동할지를 예측할 뿐입니다.\n\n의식의 어려운 문제는 모순적이며 아마 해결할 수 없을지도 모릅니다— 별먼지가 갑자기 다양한 원소와 분자로 조직화되어 경험을 할 수 있는 형태로 축적되는 것은 어떻게 가능한 것일까요? — 원편춤, 박쥐, 혹은 사람처럼 무언가 되는 것은 무언가를 느낄까요? 식기세척기가 되는 것이나 웹 서버가 되는 것 또는 대형 언어 모델로서 사람들의 요청에 응답하는 것이 무언가가 될까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여러분의 식탁 태그 마크다운 형식으로 변경해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저자들은 인간 두뇌가 자발적 활동으로 발달하며 우리 감각 지각을 규제하고, “합리적 사고 (과학), 행동 규칙 (윤리), 공유된 감정 (미술)”과 연관된 문화적 진화를 유발한다고 주장하고 있습니다. 또한 “AI는 인간 두뇌의 진화 발달과 엄밀히 같은 것이 없지만, 로봇 에이전트를 통제하는 데 학습하는 네트워크가 ‘시냅스’ 성장 경로를 따르도록 설계되었음을 알 수 있습니다.”\n\n신생아의 발달을 기반으로 한 진화적 언어를 사용하여 단계별 의식 처리를 정의합니다:\n\n- 최소 의식 - 자발적인 운동 활동을 보일 수 있고, 장기 기억을 저장할 수 있습니다. 마우스는 고양이를 보았던 것을 기억합니다. 22~30주의 인간 태아는 이 단계에 있습니다.\n- 재귀 의식 - 물건을 기능적으로 사용할 수 있고, 물건을 가리킬 수 있으며, 정교한 사회적 상호작용과 주의를 기울일 수 있습니다. 신생아도 여기에 속합니다.\n- 명시적 자의식 - 2세 아동은 거울에서 스스로를 인식하고 기본 언어를 사용할 수 있습니다. 초임판은 이 수준에 이를 수도 있습니다.\n- 반성적 자의식 - 인간은 ‘일인자체론 및 보고 능력'이 있는 완전한 의식적 경험 단계를 어린 시기인 3~5세 이후에 경험할 수 있습니다.\n\n이러한 용어는 인공 시스템의 상대적 의식 수준을 설명하는 데 도움이 되지만, 해당 수준 설명자는 인간처럼 인공 의식이 단계를 오르지 않을 것으로 시사할 수 있으며, 우리가 따르는 진화의 다양한 형태를 고려할 때 인공 의식이 달성할 수 없을 것으로 시사합니다. 우리의 진화는 인간의 경우와 달리 다수의 사회적 관련성을 갖고 있으며, 감정과 연결된 윤리적, 예술적 영역에 대한 인공 의식이 접근할 수 없다는 점이 특히 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 주제를 숙고하면서, 가정용 인공지능 로봇이 모든 사용자에게 동일한 성격을 제시해야 하는 구체적인 이유가 없다는 것이 내게 떠올랐어요. 우리도 타인들에게 성격의 다양한 면을 보여주고, 전문가 및 친구적인 맥락에서 또는 그룹 상황에서는 매우 다르게 행동해요. 만약 계산 및 기억 능력이 있다면, AF는 십여 가지의 의사의식을 겸비한 복합체가 될 수 있을 거에요.\n\n우리는 이러한 동적인 인공 친구들의 미래를 준비해야 해요 — 사랑받는 소설 캐릭터들과 친구들의 인공 친구들이 우리에게는 덜 차원적으로 보일 거에요. 현재 우리가 사용하는 언어로 인공적으로 창조된 외계 친구들과 대리인들을 기술하는 것은 임무를 수행하는 데 불충분해요. 우리는 비육체적 동체에 대한 종교적, 상업적 및 신화적 사고에서 잘했어요: 우리는 회사를 위해 일하고, 신의 도움을 청해요, 우리는 재앙을 신화적 생물체에게 돌릴 수 있어요.\n\n당신의 몸이 되어놓은 별먼지의 수집이 어떻게 자체를 깨닫게 되었을까요? 이것은 심오한 신비에요. 깨어난 물질이 다른 물질을 깨울 수 있을까요? 지금 같으라고 하면 가능할 것 같아요. 프로메테우스의 찰흙이 새로운 차원으로 올라가는 것 — 정말로 매우 흥미로운 능력이에요, 경솔하게 다가갈 수 없으며 그것을 설명할 올바른 단어 없이는 안 되는 거에요.","ogImage":{"url":"/assets/img/2024-06-22-BeingsandAnimatedMatter_0.png"},"coverImage":"/assets/img/2024-06-22-BeingsandAnimatedMatter_0.png","tag":["Tech"],"readingTime":4},{"title":"비판적 사고 AI를 뛰어넘는 보이지 않는 위협","description":"","date":"2024-06-22 19:45","slug":"2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI","content":"\n\n## 뉴스\n\n인공지능이 그렇게 할 기회조차 없이, 인류의 멸망은 비판적 사고의 죽음으로 직접 초래될 것입니다.\n\n세대를 거듭할수록 우리의 독해 능력이 점차 줄어들고 있습니다. 할당된 텍스트를 읽는 시간은 적고, 그것을 이해하는 것은 더더욱 적으며, 더 나은 방식으로 분석하는 능력이 부족합니다. 이러한 추세가 계속된다면 우리 사회의 기본 구조가 침식되는 결과를 초래할 것입니다.\n\n우리는 부분적으로 만들어진 제품들의 시대를 경험하고 있으며, 매체에 담긴 바이트들의 시대이기도 합니다. 그 결과, 우리의 주의력과 본질이 커다란 복잡한 텍스트를 이해하는 능력은 줄어들거나 사라지고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리가 전부 소장하던 것을 한눈에 다시 살펴봅니다. 이제 몇 초 동안만 지속되는 것을 보려고 합니다.\n\n오늘날 사람들은 다양한 기술의 발전을 통해 지식을 얻었지만, 그 반대로 사고 과정이 분산되었습니다. 세상이 우리의 감각을 완전히 공격하지 않는 조용한 순간을 찾기가 어렵습니다.\n\n소셜 미디어 플랫폼을 넘어오는 헤드라인이나 게시물의 관련성은 감정이 아닌 이성을 자극하여 가짜 뉴스 현실에 노출시킵니다. 사람들은 댓글을 보지 않고 센세이션 투의 제목과 간단한 설명에만 반응하여 게시물을 홍보합니다.\n\n관련성, 일화, 세부 사항에 대한 열등 의식은 더 이상 옹호되지 않습니다. 관련 요인은 사실적인 것에서 감정적이고 원시적 충동으로 전환되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Critical Thinking](/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_0.png)\n\n만약 읽기 이해력이 없다면, 의미 있는 방식으로 읽거나 비판적으로 사고하거나 판단할 수 없습니다. 비판적 평가, 다양한 의견 수용, 삼닠적 추론, 그리고 증거 중심적 방법 수용과 같은 이해력 있는 기술들이 사라지고 있습니다.\n\n증거에 의존하는 대신에, 우리의 믿음은 종말론적인 이야기와 원하는 것만 보려는 원칙에 의해 가솔린을 붓게 됩니다. 우리는 정보를 얻지만 대부분 소화하지 못합니다.\n\n이는 정보화된 국민의 기반이 되는 데 도움이 되지 않습니다 - 이는 건강한 민주주의의 기능에 핵심적입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러므로 '당신과 나 같은 사람들은 독해 방법을 전혀 모르는 것 같다'는 주장은 과장일 수 있습니다. 사실, 우리는 독해를 통해 전통적으로 정의된 인쇄 매체를 더 깊이 분석하는 방법을 기억하지 못하는 것으로 보입니다.\n\n'원시적 사고는 여전히 존재하지만 일상생활에서는 실천되지 않는다'라고 말할 수 있습니다. 이것은 내재적 재능을 갖고 있지만 적용하지 못하는 지식의 미토콘드리아 풍부함입니다.\n\nYouTube에서 정치 콘텐츠를 공부하고 분석할 때 상당히 도움이 될 수 있습니다. 우리는 이에 응답합니다.\n\n다른 세계의 의견을 가중치를 두거나 우리의 의견과 일치하지 않는 정보를 찾는 대신, 우리는 우리의 사고와 일치하는 게시물과 기사를 찾는 데 시간을 보냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Critical Thinking](/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_1.png)\n\n이런 일이 계속 일어나면 합리적인 대화 대신 소셜 미디어에서의 전체적인 분위기가 우리의 생각을 좌지우지하는 결과를 낳을 수 있습니다. 이것은 '마음이 자신을 운동하지 않기 때문에 더 이상 적극적인 기관이 아니라 지적으로 게으르게 된 것' 때문입니다.\n\n습관적인 독서는 단순히 읽고 쓸 수 있는 능력을 의미하는 것이 아니라 기능적 글쓰기에 대한 숙련 이상의 의미가 있습니다.\n\n교육은 새로운 지식과 다른 사람들의 문화를 풍부하게 해주며 우리가 성장하도록 돕습니다. 책을 읽음으로써 사람들은 새로운 지평을 열 수 있고 이를 통해 다른 삶에 대해 배울 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서, 의미 있는 읽기 연습은 정신적 성장을 촉진하고 주의력, 분석력, 추상적 사고 능력과 같은 더 나은 기능을 제공하는 것으로 이어집니다.\n\n감정 지능을 향상시키는 한 방법은 거친 현실과 픽션을 통해 인간적 경험에서 우리에게 우위를 주는 것입니다. 비평적 읽기에 의해 제안된 사고가 중단되고, 밀접한 독해 연습의 저하로 인한 감정적 성장이 저해되는 것도 능동적으로 이해해야 합니다.\n\n많은 사람들이 ChatGPT 형식으로 최신 기술을 본 지금, 인공지능(AI)이 우리 시대의 가장 큰 위협이라고 말하고 있습니다. 정교한 인공지능 기술은 작업을 자동화하고 딥 페이크로 개인을 제어하며 가짜 뉴스를 확대할 수 있습니다.\n\n그러나 AI 시스템은 여전히 사람들에 의해 프로그래밍되고 프로그래머가 제공한 능력을 활용하기 때문에 그러한 시스템으로 남게 됩니다. 현재의 AI는 목표를 갖고 위험할 수 있지만, 능동적이지 않으며 생각하거나 느끼는 능력이 없다는 점을 강조해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_2.png\" /\u003e\n\n한편, 비판적인 독해 능력을 없애는 것은 실제로 수많은 사람들의 감성적인 사고를 죽이는 것입니다. 창의적인 마음을 가진 이들이 실현하고, 건설하고, 통치하며, 선악을 운영하는 기술을 창출합니다.\n\n윤리적인 결정을 내리는 인지 시스템은 전 세계에 영향을 미치며 중요한 역할을 합니다. 이러한 비판적 사고력의 상실은 세계를 이해하고 다양한 개념과 이론을 이해하는 데 필요한 기본적인 능력의 감퇴를 초래하며, 존재의 위협이 됩니다.\n\n또한, 인간적인 접근은 현재 알고리즘으로 대체될 수 없습니다. 그러나 우리는 천년 동안 연마된 비판적 독해와 사고 기술에 대한 현재 지식을 포기해야 할까요? 그렇다면 이에 대응할 알고리즘도 필요하지 않을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n개인마다 그 정보를 전파하기 전에 심사숙고하는 비평적 사고 독자가 되기를 염원할 수 있습니다. \n\n우리는 현대 미디어 뿐만 아니라 다른 형태의 미디어에서도 의도적인 비평적 독해 능력을 발휘할 수 있습니다. 그러나 개인 요인과 그들의 결정의 역할을 강조하지 않는 것은 심각한 분석을 만들어내지 않습니다.\n\n독자의 부재나 독해능력의 감소가 단순한 현상으로 설명될 수 없습니다.\n\n이것을 다음과 같은 용어로 설명할 수 없습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Critical Thinking: The Unseen Threat Outpacing AI](/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_3.png)\n\n또한, 한 세대가 세상을 방치한다는 것은 양자나온(Qanon) 같은 엉망인 주장에 빠진 젊은이부터 노인이까지 방대한 인구를 설명하지 못합니다.\n\n그러나 이 두 가지 단순화된 접근 방식은 논의 중인 과정의 복잡성을 이해하는 방법을 제공하지 않습니다. 현대의 미디어 환경에서 디지털 플랫폼이 우세한 플랫폼이라는 사실을 고려하지 않을 수 없습니다.\n\n이러한 기술을 통해 정보 재앙이 발생하며, 구체적 자료가 집중적으로 공유되는 동안 주의력이 분산되고 소비를 위해 최적화된 간결한 정보의 전달을 편향시키는 바이어스가 포함됩니다. 자동화 결정 과정은 충격적인 제목이 인사이트 있는 토론보다 더 많이 공유되게 합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n소셜 네트워크는 특히 강한 거짓 정보가 올라오는 완벽한 장소입니다. 단순한 접근법과 현실적이고 복잡한 아이디어를 둘러싼 과대포장으로 인해 발생하는 소음으로 인해 이러한 아이디어들은 듣기 어렵게 되어 있습니다.\n\n소셜 네트워크에서의 현대적인 미디어 문화는 포괄적이고 지속적인 심도 있는 독해와는 정반대로 인식을 형성하고 있습니다. 너무 많은 자극이 존재하여 주의를 산만하게 하며 집중력을 방해합니다.\n\n![이미지](/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_4.png)\n\n우리는 앱과 사이트를 전환하며 여러 가지 개념을 알아가기 힘들고, 결국 거의 아무것도 이해하지 못합니다. 이는 우리의 중점이 다른 기사로 넘어가기만 하는 데 집중되어 특정 주제에 집중하지 않는다는 것을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n앱과 사이트의 디자인은 우리의 심리적 취약점을 의도적으로 이용합니다. 업데이트를 위해 새로 고치고, 자동 재생을 통해 새로움에 노출시키면서 뇌는 실제로는 새로운 컨텐츠가 없는데도 새로운 것을 생각하게 속입니다.\n\n알림은 외부 개념의 갑작스러운 표시로 우리의 생각에 끼워 맞춥니다. 가끔은 clickbait 헤드라인이 호기심을 끌기 위해 감정을 이용하는데, 이는 독자들을 오도합니다.\n\n알고리즘은 어떤 콘텐츠가 우리를 밤새 깨어 있게 만드는지 정확히 알고 있습니다. 곧 우리의 정신적 반사 반응은 산만하게 되어, 마치 파블로프의 개가 종소리를 듣고 침을 흘리는 것처럼 조건이 맞춰지죠.\n\n더 나쁜 것은, 이 환경이 사용자가 가능한 한 오랫동안 사이트에 집중하도록 설계된 매혹적인 그래픽 인터페이스 내에 허위 콘텐츠를 감추고 있다는 겁니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n매일 사람들은 초기에는 지루하고 흥미롭지 않아 보이는 비디오를 관찰하기 위해 시청합니다. 사람들은 얕은 콘텐츠를 듣기 위해 자신들의 반죽되지 않은 속보를 밀어주는 아름다운 사람들을 위해 기꺼이 시간을 할애합니다.\n\n게시판과 팔로워들의 화면은 도둑놈처럼 참을성 없게 소중한 집중력을 훔쳐갑니다. 대중의 주의는 현명하게 조작할 수 있는 개인에게 이로운 상품으로 변질됩니다.\n\n반면 중요한 정보가 담긴 긴 글들은 주목을 얻기 위해 투쟁해야 합니다.\n\n![크리티컬 씽킹과 보이지 않는 위협, AI를 앞서나가는](/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사람들을 중독성 있는 인터페이스로 유인하려는 계획은 없지만, 교육적인 토론에 도움이 되는 것들을 만들겠다고 합니다. 알고리즘들이 하는 것처럼 독자들의 기권을 무시하지 않으려고 합니다.\n\n맞아요, clickbait으로 더 많은 돈을 벌기를 원하는 것이 아니라 정보를 전달하고자 합니다. 그럼에도 불구하고, 그러한 집중력이 필요한 독해 환경은 끝없는 자극적 자극을 흡수하면서 포스트인더스트리얼 인식 기본을 혼란스럽게 만듭니다.\n\n안타깝게도, 깊이 파고들기는 시간이 많이 소요되기 때문에 종종 가치가 없어 보입니다. 스캐밍하고 스크롤링과는 달리 지식을 마구 소비하는 연도 동안 운동이 된 것과는 다릅니다.\n\n앱 및 웹사이트의 디자인은 우리의 심리적 약점을 의도적으로 이용합니다. 새롭게 즐거워지고 업데이트되는 느낌이 들고, 자동 재생은 끊임없이 화면을 바꾸어 우리의 뇌가 완전히 새로운 것으로 생각하게 만듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알림은 외부 신호를 통해 내부 사고과정에 집중을 방해합니다. 사람들은 종종 실제 관심보다는 감정에 기반한 헤드라인에 반응하며, 클릭베이트는 호기심을 유혹하기 위해 감정을 겨냥합니다.\n\n알고리즘은 우리가 무엇이 우리를 끌어들이는지를 학습하도록 독특하게 조정되어 있다는 것이 밝혀졌습니다. 그리고 아주 짧은 시간 안에 우리의 마음은 주의를 산만하게 하도록 적응되며, 마치 파블로프의 개처럼 조건이 되어 있습니다.\n\n한 세대가 다른 세대에게 자신들의 무지를 강조함으로써 비판적 사고와 독해능력을 배제한다는 것은 부당한 일입니다.\n\n이러한 것들은 다시 익숙해지는 과정으로 살아나게 할 수 있습니다. 이에 따라 깊은 개인 독서와 의미 있는 분석에 대한 욕망은 우리 안의 가장 좋은 부분 - 인간의 마음과 영혼을 보호합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 나를 팔로우해주시는 이 모든 멋진 분들께 감사드립니다!!!\n\n## 여러분의 지원 덕분에 더 많은 글쓰기에 동기부여 받고 자신감을 얻게 되었어요. 감사합니다…\n\n드. Cüneyt Yardımcı, Yasemin Yiğit Kuru, Darrin Atkins, Timothy M. Stafford, PhD, Filza Chaudhry, H. Mikel Feilen, 안토니오 프란시스코 교수","ogImage":{"url":"/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_0.png"},"coverImage":"/assets/img/2024-06-22-CriticalThinkingTheUnseenThreatOutpacingAI_0.png","tag":["Tech"],"readingTime":7},{"title":"강화 학습의 비밀을 풀다 Actor-Critic 초보자 가이드","description":"","date":"2024-06-22 19:43","slug":"2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide","content":"\n\n## 이해해야 할 개념:\n\n강화 학습: 시간 차 학습\n\n강화 학습: Q-Learning\n\n딥 Q 학습: 심층 강화 학습 알고리즘\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정책 그라디언트의 직관적인 설명\n\n## Actor-Critic 알고리즘이란 무엇인가요?\n\nActor-Critic은 환경의 피드백에 기반하여 에이전트의 작업을 최적화하는 강화 학습 알고리즘입니다.\n\n![이미지 설명](/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nActor: Actor는 환경을 탐색하여 최적 정책을 학습합니다.\n\nCritic: Critic은 Actor가 취한 각 행동의 가치를 평가하여 그 행동이 더 나은 보상을 가져오는지를 판단하고, Actor에게 취해야 할 최선의 행동을 안내합니다.\n\n그런 다음 Actor는 Critic의 피드백을 사용하여 정책을 조정하고 더 현명한 결정을 내려 전반적인 성능을 향상시킵니다.\n\n![이미지](/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Actor-Critic 알고리즘은 어떻게 작동하나요?\n\n![Actor-Critic 알고리즘 이미지](/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_2.png)\n\nActor-Critic 알고리즘은 환경으로부터 입력을 받아와 그 상태를 기반으로 최적의 행동을 결정합니다.\n\n알고리즘의 Actor 구성 요소는 환경으로부터 현재 상태를 입력으로 받아옵니다. 이는 상태에 대한 각 행동의 확률을 출력하는 정책으로 동작하는 신경망을 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비평가 네트워크는 현재 상태와 Actor의 출력된 액션을 입력으로 받아 이 정보를 사용하여 예상되는 미래 보상, 즉 Q-값을 추정합니다. Q-값은 특정 정책을 따라 특정 상태에서 에이전트가 받을 수 있는 예상 누적 보상을 나타냅니다.\n\n반면에 가치 상태는 특정 상태에서 취한 조치와 관계없이 예상되는 미래 보상을 나타냅니다. 특정 상태에 대한 모든 가능한 조치에 대한 Q-값의 평균으로 계산됩니다.\n\n## Adv. = Q(s,a) — V(s)\n\n이점 함수는 Actor의 정책을 안내하는 데 유용한 정보를 제공하여 최상의 결과로 이끌어지는 행동을 결정하고 정책을 그에 맞게 조정할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과적으로, 이점 함수는 Actor와 Critic 둘 다에게 역전파되어, 두 구성 요소 모두가 지속적으로 업데이트되고 개선되는 함수를 허용합니다. 이로 인해 액터는 더 나은 결과를 이끄는 결정을 내릴 때 더 효과적해지고, 전반적으로 성능이 향상됩니다. 궁극적으로, Actor-Critic 알고리즘은 기대되는 미래 보상을 최대화하는 최적의 정책을 배웁니다.\n\nActor-Critic 알고리즘은 A2C, ACER, A3C, TRPO, PPO와 같은 다른 알고리즘들의 기초로 삼아진 프레임워크입니다.\n\n## 다양한 Actor-Critic 기반 강화 학습 알고리즘\n\n- A2C- 어드밴티지 Actor Critic: 어드밴티지 Actor-Critic(A2C) 방법의 Critic은 𝑉(𝑠)를 예측하도록 훈련되어, 부트스트래핑을 위해 𝐴(𝑠,𝑎)=𝑄(𝑠,𝑎)−𝑉(𝑠)을 추정하는 데 사용됩니다. Actor는 정책을 업데이트하기 위한 가이던스 신호로 어드밴티지 함수를 사용하여 훈련됩니다.\n- ACER- 경험 재생이 있는 Actor Critic: ACER는 경험 재생을 사용하는 효율적인 액터-크리틱 알고리즘으로, 신뢰 영역 정책 최적화 방법을 사용하여 성능을 향상시킵니다.\n- A3C- 비동기 어드밴티지 Actor Critic: 액터-크리틱 알고리즘의 병렬, 비동기 멀티스레드 구현. 병렬로 여러 에이전트가 각자의 환경에서 훈련을 받아 동시에 상태 공간의 다른 부분을 탐색합니다. 에이전트들은 정책 그레디언트를 계산하고 주기적으로 글로벌 네트워크로 업데이트를 보내거나 종단상태에 도달했을 때 업데이트를 보냅니다. 글로벌 네트워크는 업데이트마다 새로운 가중치를 에이전트들에게 전파하여 공통 정책을 공유할 수 있도록 합니다.\n- TRPO- 신뢰 영역 정책 최적화: Actor-크리틱 알고리즘과 신뢰 영역을 사용하여 정책 업데이트를 제약합니다. 정책 업데이트는 이전 정책과 업데이트된 정책 사이의 KL 발산을 사용하여 측정되며, 각 반복에서 신뢰 영역을 측정하는 데 사용됩니다.\n- PPO- 근접 정책 최적화: 근접 정책 최적화(PPO)는 여러 번의 확률적 그래디언트 상승을 통해 각 정책 업데이트를 수행하는 액터-크리틱 알고리즘에 기반합니다. 각 훈련 에포크에서 너무 큰 정책 업데이트를 피해 정책의 변경을 제한하여 정책의 훈련 안정성을 향상시킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Actor-Critic 알고리즘은 어떤 응용 분야에서 사용되나요?\n\nActor-Critic 알고리즘은 다음과 같은 분야에서 널리 활용됩니다:\n\n- 제조업이나 서비스 산업의 로봇을 위한 제어 시스템,\n- 게임에서 게임 전략을 최적화하는 데 사용됨,\n- 전력 그리드, 자율 주행 차량, 산업 프로세스와 같은 복잡 시스템.\n\n## 코드 구현\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기에서는 두 개의 신경망을 사용할 것입니다: Actor와 Critic.\n\n![Actor-Critic](/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_3.png)\n\n각 에피소드 단계에서 Actor 네트워크를 사용하여 에이전트는 현재 상태에서 행동을 취하고 다음 상태로 이동하며 환경으로부터 보상을 받습니다. Actor의 신경망은 그 상태에서 각 가능한 행동을 취할 확률을 출력하는 정책으로 작동합니다.\n\n보상과 다음 상태의 추정 가치를 사용하여 이득 함수를 계산하는데, 이는 행동을 취하는 것의 예상 반환값에서 현재 상태의 추정 가치를 뺀 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_4.png)\n\n액터 네트워크를 업데이트하면서 액터 손실을 계산합니다. 이는 취한 행동의 로그 확률의 음수에 이득을 곱한 값입니다.\n\n![image](/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_5.png)\n\n![image](/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_6.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nA2C는 빠르고 효율적이며 대량의 데이터에서 빠르게 효과적으로 학습할 수 있어요. Actor는 환경을 탐험하고 Critic은 Actor가 취할 수 있는 최상의 행동을 활용하기 위한 피드백을 제공하여 시간에 따라 최적 정책을 달성하려고 노력해요.\n\nA2C는 연속된 행동 공간에서 잘 작동하지만 이산적인 행동 공간에서는 그렇지 않아요. 최적 성능에 대한 하이퍼파라미터에 민감하며 잘못된 하이퍼파라미터는 훈련을 불안정하게 만들 수 있어요.\n\n## 결론:\n\nActor-Critic 알고리즘은 두 가지 구성 요소를 사용해요. Actor는 탐사를 통해 최적 정책을 학습하며 Critic은 Actor의 행동을 평가하여 상태에 대한 최상의 행동을 결정해요. Critic은 향상된 성능을 도출할 피드백을 Actor에게 제공해요. Actor-Critic 알고리즘은 연속적인 행동 공간과 훈련에 대한 하이퍼파라미터에 대해 잘 작동해요. Actor-Critic 모델은 불안정성을 피하기 위해 충분한 실험을 해야 해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 참고 자료:\n\nREINFORCEMENT LEARNING THROUGH ASYNCHRONOUS ADVANTAGE ACTOR-CRITIC ON A GPU\n\nAsynchronous Methods for Deep Reinforcement Learning\n\n[PDF 바로가기](https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nhttp://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf\n\nhttps://ai.stackexchange.com/questions/7390/what-is-the-difference-between-actor-critic-and-advantage-actor-critic","ogImage":{"url":"/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_0.png"},"coverImage":"/assets/img/2024-06-22-UnlockingtheSecretsofActor-CriticReinforcementLearningABeginnersGuide_0.png","tag":["Tech"],"readingTime":5},{"title":"로봇은 더 똑똑해지고 있을까 로봇 공학에서 인식에 대한 대화","description":"","date":"2024-06-22 19:41","slug":"2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics","content":"\n\n이 기사는 보통 여기서 하는 시리즈와는 별도로 됩니다. 이젠 대화를 나누는 것처럼 얘기하는 걸 좋아합니다 (하지만 전 며칠 동안 계속 말을 할 거에요 ㅋㅋ). 그래서 당신이 좋아하는 음료를 준비하고 로봇과 그들이 얼마나 \"똑똑\"해졌는지에 대해 이야기해봐요.\n\n![로봇](/assets/img/2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics_0.png)\n\n지난 달들에 로봇이 놀라운 일들을 하는 비디오를 보셨을 것입니다. 그런 비디오를 보면서 \"와, 이건 놀라워. 이제 로봇들이 이것을 할 수 있나? 그들은 매일 더 똑똑해지고 있어\"라고 생각했을 수 있어요. 예를 들면, Figure 01 인간형 로봇이 물체를 다루는 모습이나 Scythe 로봇이 자율적으로 잔디를 자르는 것 등이 있습니다.\n\n여기서 잠깐 멈추고 로봇의 인식 관점에서 관련 두 가지 문제, 동시 위치 추정 및 지도 작성(SLAM) 그리고 빈 피킹(bin picking)을 바라봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 로봇의 과거와 현재의 인식\n\n우선, 로봇 공학 분야에서 \"인식\"이란 무엇인가요? 모르는 분을 위해 설명드리자면, 로봇의 카메라, LiDAR, 레이더 또는 접촉 센서와 같은 다양한 센서를 사용해 환경을 감지하고 해석하는 능력을 의미합니다. 환경에 대한 유용한 정보를 추출하기 위해 센서 데이터를 수집하고 처리하는 것을 포함합니다.\n\n인식은 로봋의 SLAM에 매우 중요합니다. 다시 말씀드리면, 동시 위치 추정 및 지도 작성(SLAM)은 로봇 공학에서 기본적인 문제입니다. 이는 로봇이 알 수 없는 환경을 탐색하면서 동시에 그 환경의 지도를 작성하고 그 안에서 자신의 위치를 판단하는 것을 의미합니다.\n\n2016년에 Cadena와 다른 저자들은 \"동시 위치 및 지도 작성의 과거, 현재 및 미래: 견고한 인식 시대를 향하여\"라는 과학 논문을 발표했습니다. 그들의 연구에서는 SLAM 분야에서 30년 이상의 작업을 검토하고, 이를 고전적 시대(1986-2004), 알고리즘 분석 시대(2004-2015), 그리고 견고한 인식 시대(2015-현재)로 그룹화했습니다. 각 시대를 간단히 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n고전 시대:\n이것은 Extended Kalman Filters, Rao-Blackwellized Particle Filters 및 최대 우도 추정을 사용하여 SLAM이 불확실성을 처리하는 주요 방법을 소개합니다. 또한 모든 것이 원활하게 작동하고 올바른 데이터 조각을 연결하는 데 필요한 기본 도전에 대해 이야기합니다.이 시대의 SLAM의 두 가지 예는 아래에 나와 있습니다.\n\n![image](/assets/img/2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics_1.png)\n\n일반적으로, 첫 번째 제안된 SLAM 시스템은 환경에서 장애물을 감지하고 맵에 표현할 수 있었습니다. 맵핑에 사용된 가장 인기 있는 센서는 초음파 및 LiDAR였습니다.\n\n알고리즘 분석 시대:\n연구자들은 SLAM의 기본 기능인 시간이 지남에 따른 위치 추적이 얼마나 잘되는지, 신뢰할 수 있는지, 그리고 대량의 데이터를 어떻게 처리하는지 등을 조사했습니다. 드문 데이터가 SLAM이 더 빠르고 더 잘 작동하게 하는 것을 발견했습니다. 이때 무료로 사용할 수 있는 주요 SLAM 소프트웨어인 Gmapping 및 ORB-SLAM을 만들기 시작했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![그림](/assets/img/2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics_2.png)\n\n이전 시대의 기초를 활용하여 이 시대에는 카메라와 다른 시각 센서들이 보다 인기를 끌게 되었으며, \"Visual-SLAM\"이라는 용어가 제안되었습니다. 게다가 커뮤니티는 3D 환경 표현을 이용한 다양한 SLAM 기술을 소개했습니다.\n\n로버스트-인식 시대:\nSLAM 시스템은 단순히 형상을 매핑하는 것을 넘어, 환경에 대한 고수준의 이해를 얻기 위해 기하학적 재구성을 위해 높은 수준의 고려를 합니다. 물체의 의미(의미론)나 관련된 물리학적 측면과 같은 것들을 고려합니다. 당사자가 수행해야 하는 작업에 맞게 로봇이 필요한 세부 사항에 초점을 맞추어 센서 데이터에서 추가 잡음을 걸러내어 로봇이 작업을 성취하는 데 도움을 줍니다. 로봇이 수행해야 할 작업에 따라 맵을 조정합니다.\n\n이 애니메이션 이미지를 자세히 살펴보면 많은 세부 사항을 알 수 있습니다. 장면 속 물체는 환경의 일부로 이해되며, 같은 클래스의 물체는 경계 상자에서 동일한 색으로 레이블이 지정되며, LiDAR 및 레이더의 3D 데이터는 카메라의 2D 이미지와 결합됩니다. 게다가 다양한 물체가 결합되어 장면으로부터 더 많은 정보를 추출하는데 활용됩니다. 마지막 애니메이션 이미지에서와 같이 흰색 차량과 그의 깜박임등, 후방등, 브레이크 라이트 등이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 빈 피킹에서의 인식\n\n빈 피킹은 컴퓨터 비전 및 로봇 공학 분야에서의 기본적인 도전으로 자리 잡고 있습니다. 로봇 팔은 빈(또는 컨테이너)으로부터 다양한 방향의 물체를 효율적으로 잡기 위해 진공 그리퍼, 평행 그리퍼 또는 대체 로봇 도구를 사용하여 센서로 장착되어 있습니다. 이 문제는 Cadena의 논문에 언급되지 않았지만, SLAM의 동일한 연령 그룹이 여기에 적용될 수 있습니다(그리고 이것은 내 의견입니다).\n\n이 문제에 대한 가장 인기 있는 접근 방식 중 하나는 알고리즘 분석 시대부터 출발한 포인트 클라우드(PC) 등록에 기반하고 있습니다. 섭취 아이템의 3D 형태는 미리 알려져 있어야하며, 센서를 사용하여 매번 픽하기 전에 빈을 스캔했습니다. 이 스캔을 통해 3D PC가 생성되었고, 이후 PC 등록 알고리즘으로 전송되었습니다. 이 알고리즘은 섭취된 아이템과 빈의 PC 간에 일치를 찾는 역할을 했습니다. 아래의 애니메이션 이미지가 이 과정을 설명합니다.\n\n이 접근 방식이 그 당시에 작동했지만, 그 한계를 쉽게 이해할 수 있습니다. 예를 들어, 빈당 하나의 아이템 유형만 있어야 한다면, 그렇지 않은 경우에는 빈의 PC와 여러 섭취 아이템을 매칭하는 계산 비용이 막대할 수 있습니다. 게다가, 이러한 접근 방식은 빠르게 확장되지 않으며, 새로운 아이템을 위해서는 항상 아이템을 스캔하여 3D 형태 표현을 생성해야 합니다(또는 제조사에게 CAD 파일을 요청해야 합니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이들은 여러 종류의 항목이 들어있는 소스 바구니를 비울 수 있습니다. 게다가, 그들의 시스템은 새로운 (시스템이 이전에 본 적이 없는 항목) 항목을 즉시 선택할 수 있을 정도로 광범위한 수준의 일반화를 달성했습니다.\n\n마지막으로, 빠른 확장과 새로운 항목에 적응하는 부분 이외에, 이같은 지각 개선은 \"모든 음식 항목을 선택해 주세요\", \"장난감을 선택해 주세요\", 또는 \"놀 수 있는 항목을 가져다 주세요\"와 같은 다른 인간 수준의 명령을 처리할 수 있도록 합니다. 위의 예에서, 그들의 시스템이 바구니에서 고장난 항목을 선택하는 것을 볼 수 있습니다.\n\n이제 우리가 로봇의 지각 발전에 대해 다루었으니, 이 기사의 가장 흥미로운 부분으로 넘어갈 수 있습니다.\n\n## 로봇의 지각 발전의 영향과 속도\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제가 각 시대의 주요 작품에서 몇 장의 이미지를 넣었는데, 그냥 이 기사를 읽기 쉽게 만들기 위해서 하는 게 아니에요. 고대 시대의 이미지를 보면 환경으로부터의 장애물을 거의 표현하지 못했음을 알 수 있어요. 대부분의 구축된 지도는 2D였고, 환경에서의 3D 장애물을 평평하게 만들었어요. 환경 내의 다양한 물체들은 모두 \"장애물\"로 라벨링되었고, 로봇이 그것들을 피한다면 괜찮았어요.\n\n시각 센서의 발전으로, 알고리즘 분석 시대는 더 많은 3D 맵을 보여주고 환경에 색상을 추가함으로써 지각을 개선했어요. 비록 이것에 대해 언급하지 않았지만, 이 시대는 또한 장면의 동적 부분을 걸러내기 시작했어요. 따라서 2004년부터 2015년까지 가장 큰 차이는 3D 맵의 탐험과 이러한 맵에 색상 정보를 추가한 것이었어요. 여기서 간략하게 설명하고 있지만, 11년간 로봇 지각에서는 발전이 그리 크지 않았다는 점이요.\n\n반면에, 견고한 지각 시대는 다른 두 시대를 무색하게 만들었어요. 여전히 제가 박사 학위 논문 제안을 작성하는 중이던 2016년에 카데나(Cadena)의 작품을 읽었을 때를 기억해요. 그 논문에서, 제 마음에 남는 한 문장을 적었어요:\n\n생각해보면, 이야기가 너무 맞죠. 2015년까지의 로봇 지각은 방법론과 기술적인 측면에서 견고한 기반을 구축하고 개선하는 데 초점을 맞췄어요. 그 부분이 충분히 견고해지면, 연구 커뮤니티와 산업계는 환경을 \"자유로운(free)\", \"점유된(occupied)\", 혹은 \"알 수 없는(unknown)\"로만 라벨링하는 것이 충분하지 않다는 것을 깨달았어요. 그 때들이 생김새를 가볍게 생각하면, 환경에서 고수준 정보를 추론하는 의미론을 추구하기로 결정했어요. 이 정보는 다양한 물체, 방, 위치의 이름과 범주를 포함하지만 이에 한정되지 않아요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2015년부터 2020년까지 Zoox와 Sereact가 환경에 대한 그 수준의 이해를 달성했다는 사실이 놀라운 것 같아요. 그들로부터 일부 세부 정보를 논의해보고, 이미지를 다시 여기에 포함해서 위로 스크롤할 필요 없도록 할게요.\n\n아래 부분에서 자율 주행 자동차가 사람들이 앉아 있고 걷고 있지 않을 때를 이해할 수 있다는 것을 볼 수 있어요. 또한, 인간의 제스처를 이해할 수 있어서 앞으로 나아갈 수 있다는 의미입니다.\n\n또 다른 경우에는, 자동차가 안전 조끼를 입은 공사모를 쓴 사람이 교통 표지판을 들고 있는 상황을 이해한 것을 볼 수 있어요. 이 사람은 보통 사람이 아니라 차량이 멈추라는 도로 공사 작업자입니다.\n\n마지막으로, 주차된 차량에 문이 열려 있는 것을 이해한 자동차는 그곳에서 사람이 나올 수 있다는 것을 의미해요. 이 상황에서 Zoox 차는 어떤 사고도 예방하기 위해 더 주의 깊게 운전합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 bin picking 분야의 지각 개선 사항도 다룰 예정이라고 언급했던 것 같아요. 이곳에서 Sereact에서의 좋은 예시를 소개하고 있습니다. 이 경우에는 사람이 로봇에게 상품 존 폐기품을 선택하도록 요청하고 있습니다. 출처 창고에는 6개의 프링글스 캔이 들어 있고 이 중 하나가 부서졌습니다. 시스템의 인식 부분은 이를 이해할 수 있고, 부서진 캔을 선택합니다.\n\n# 요약\n\n우리는 로봇 지각의 의미론적을 고려하는 강인한-지각 시대에 살고 있습니다. 이는 로봇이 Sereact가 보여준 것처럼, 상품 존에서 부서진 항목을 선택하거나 Zoox가 하는 것처럼 사고를 예측하는 고수준 작업을 수행할 수 있게 합니다. 우리 인간들에게는 \"부서진\"이 무엇을 의미하는지는 명확하고 간단하지만, 정의할 수 있나요? 창고의 모든 항목에 대해 이 조건을 설명하는 규칙과 특성을 정의하는 건 거의 불가능합니다 (예: 부서진 캔과 부서진 머그잔은 다릅니다).\n\n실제로 이러한 의미론적 이해는 LLM과 VLM에서 출발하며, 텍스트 및 시각적 정보를 결합합니다. 로봇이 사람의 옷이 그들이 본 상황에서 다른 역할을 함을 이해할 때야 (길 공사 작업자와 같이 보여진 것처럼) 이 사람에 적절히 반응할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n물론, 로봇학의 하드웨어 부분에서도 상당한 발전이 있었습니다. 예를 들어, Boston Dynamics와 그 Atlas 로봇을 생각해 볼 수 있습니다. 수압식 버전은 폐지되고 새로운 완전 전기식 버전이 출시되었습니다. 그러나 저는 고수준 작업을 수행하는 로봇들에게 있어서 인식 소프트웨어의 개선이 더욱 중요하다고 느낍니다.\n\n이 기사의 주요 질문에 대한 대답이 있습니다. 한 번 들은 적이 있는데, 지식을 축적하는 것이 아닌 그 지식을 활용하는 방법을 아는 사람이 똑똑하다고 합니다. 그래서 로봇이 \"새로운\" 지식을 활용하여 보다 복잡한 작업을 해결한다면, 그들은 실제로 똑똑해지고 있는 것이 맞습니다.\n\n어떻게 생각하시나요?\n\n지금은 여기까지입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n- Newman, Paul 등. “탐색 및 복귀: 실시간 동시 매핑 및 로컬라이제이션의 실험적 검증.” ICRA, 2002.\n- Thrun, Sebastian. “팀의 이동 로봇을 위한 온라인 매핑 알고리즘.” Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, 2000.\n- Mur-Artal, R., Montiel, J.M.M., 및 Tardos, J.D. “ORB-SLAM: 다목적 및 정확한 단안 SLAM 시스템.” IEEE Transactions on Robotics. 2015.\n- Grisetti, G., Stachniss, C., 및 Burgard, W. “적응적 제안 및 선택적 리샘플링을 통한 Rao-Blackwellized 입자 필터를 사용한 그리드 기반 SLAM 개선.” ICRA. 2005.\n- youtube.com/watch?v=BVRMh9NO9Cs에서 추출.\n- Cadena C, Carlone L, Carrillo H, Latif Y, Scaramuzza D, Neira J, Reid I, Leonard JJ. 동시 위치 결정 및 매핑의 과거, 현재 및 미래: 견고한 인식 시대로. IEEE Transactions on robotics. 2016.\n- https://www.youtube.com/watch?v=gRV4KvIDn9Y\u0026ab_channel=T³TipsTricksTests에서 추출.\n- https://www.youtube.com/watch?v=_ieObX5f_ws\u0026ab_channel=Sereact에서 추출.","ogImage":{"url":"/assets/img/2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics_0.png"},"coverImage":"/assets/img/2024-06-22-AretherobotsgettingsmarterAconversationaboutperceptioninrobotics_0.png","tag":["Tech"],"readingTime":8}],"page":"29","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"29"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>