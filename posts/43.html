<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/43" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/43" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="리눅스 프로세스 여정  로그인 과정 깊이 파헤치기" href="/post/2024-06-22-TheLinuxProcessJourneylogin"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="리눅스 프로세스 여정  로그인 과정 깊이 파헤치기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-TheLinuxProcessJourneylogin_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="리눅스 프로세스 여정  로그인 과정 깊이 파헤치기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">리눅스 프로세스 여정  로그인 과정 깊이 파헤치기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="리눅스 보안 여정  EGID 유효 그룹 ID 이해하기" href="/post/2024-06-22-TheLinuxSecurityJourneyEGIDEffectiveGroupID"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="리눅스 보안 여정  EGID 유효 그룹 ID 이해하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-TheLinuxSecurityJourneyEGIDEffectiveGroupID_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="리눅스 보안 여정  EGID 유효 그룹 ID 이해하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">리눅스 보안 여정  EGID 유효 그룹 ID 이해하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">1<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="쉘 트랩과 POSIX 신호 쉽게 이해하기" href="/post/2024-06-22-ShellTrapsandPOSIXSignals"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="쉘 트랩과 POSIX 신호 쉽게 이해하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-ShellTrapsandPOSIXSignals_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="쉘 트랩과 POSIX 신호 쉽게 이해하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">쉘 트랩과 POSIX 신호 쉽게 이해하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">1<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="개발자를 위한 Linux 명령어 치트시트 필수 DevOps 커맨드 모음" href="/post/2024-06-22-CheatSheetLinuxCommandsforDevOps"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="개발자를 위한 Linux 명령어 치트시트 필수 DevOps 커맨드 모음" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-CheatSheetLinuxCommandsforDevOps_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="개발자를 위한 Linux 명령어 치트시트 필수 DevOps 커맨드 모음" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">개발자를 위한 Linux 명령어 치트시트 필수 DevOps 커맨드 모음</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="개발자를 행복하게 만드는 5가지 필수 Linux 명령어" href="/post/2024-06-22-5LinuxCommandsthatMakesYouaHappyDev"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="개발자를 행복하게 만드는 5가지 필수 Linux 명령어" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-5LinuxCommandsthatMakesYouaHappyDev_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="개발자를 행복하게 만드는 5가지 필수 Linux 명령어" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">개발자를 행복하게 만드는 5가지 필수 Linux 명령어</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">1<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="예제 중심으로 배우는 AWK를 사용한 리눅스유닉스 텍스트 처리 방법" href="/post/2024-06-22-TextProcessingwithAWKinLinuxUnixwithexamples"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="예제 중심으로 배우는 AWK를 사용한 리눅스유닉스 텍스트 처리 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-TextProcessingwithAWKinLinuxUnixwithexamples_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="예제 중심으로 배우는 AWK를 사용한 리눅스유닉스 텍스트 처리 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">예제 중심으로 배우는 AWK를 사용한 리눅스유닉스 텍스트 처리 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="쿠버네티스 네트워킹 완벽 가이드 로드 밸런스, BGP, IPVS 등 핵심 기술 탐구하기" href="/post/2024-06-22-DecipheringtheKubernetesNetworkingMazeNavigatingLoad-BalanceBGPIPVSandBeyond"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="쿠버네티스 네트워킹 완벽 가이드 로드 밸런스, BGP, IPVS 등 핵심 기술 탐구하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-DecipheringtheKubernetesNetworkingMazeNavigatingLoad-BalanceBGPIPVSandBeyond_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="쿠버네티스 네트워킹 완벽 가이드 로드 밸런스, BGP, IPVS 등 핵심 기술 탐구하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">쿠버네티스 네트워킹 완벽 가이드 로드 밸런스, BGP, IPVS 등 핵심 기술 탐구하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="좀비 프로세스의 비밀을 밝히다 Linux에서 알아야 할 모든 것" href="/post/2024-06-21-LinuxUnveilingtheMysteriesofZombieProcesses"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="좀비 프로세스의 비밀을 밝히다 Linux에서 알아야 할 모든 것" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-21-LinuxUnveilingtheMysteriesofZombieProcesses_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="좀비 프로세스의 비밀을 밝히다 Linux에서 알아야 할 모든 것" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">좀비 프로세스의 비밀을 밝히다 Linux에서 알아야 할 모든 것</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 21, 2024</span><span class="PostList_reading_time__6CBMQ">1<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="파이썬 엔지니어를 위한 3D 가우시안 스플래팅 소개 파트 2" href="/post/2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="파이썬 엔지니어를 위한 3D 가우시안 스플래팅 소개 파트 2" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="파이썬 엔지니어를 위한 3D 가우시안 스플래팅 소개 파트 2" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">파이썬 엔지니어를 위한 3D 가우시안 스플래팅 소개 파트 2</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="신경망 기본 이론과 구조 유형" href="/post/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="신경망 기본 이론과 구조 유형" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="신경망 기본 이론과 구조 유형" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">신경망 기본 이론과 구조 유형</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">22<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/41">41</a><a class="link" href="/posts/42">42</a><a class="link posts_-active__YVJEi" href="/posts/43">43</a><a class="link" href="/posts/44">44</a><a class="link" href="/posts/45">45</a><a class="link" href="/posts/46">46</a><a class="link" href="/posts/47">47</a><a class="link" href="/posts/48">48</a><a class="link" href="/posts/49">49</a><a class="link" href="/posts/50">50</a><a class="link" href="/posts/51">51</a><a class="link" href="/posts/52">52</a><a class="link" href="/posts/53">53</a><a class="link" href="/posts/54">54</a><a class="link" href="/posts/55">55</a><a class="link" href="/posts/56">56</a><a class="link" href="/posts/57">57</a><a class="link" href="/posts/58">58</a><a class="link" href="/posts/59">59</a><a class="link" href="/posts/60">60</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"리눅스 프로세스 여정  로그인 과정 깊이 파헤치기","description":"","date":"2024-06-22 00:28","slug":"2024-06-22-TheLinuxProcessJourneylogin","content":"\n\n일반적으로 \"login\"은 Linux 시스템에서 세션을 시작하는 데 사용되는 ELF 바이너리로 기본적으로 \"/usr/bin/login\" (또는 /bin/login)에 위치합니다. Linux 시스템에 로그인할 때 사용되며 인자를 전달하지 않으면 사용자의 프롬프트가 표시됩니다. (https://man7.org/linux/man-pages/man1/login.1.html) - 아래 스크린샷에 나와 있습니다 (https://www.tecmint.com/understanding-shell-initialization-files-and-user-profiles-linux/).\n\n전체적으로, login은 \"util-linux\" 패키지의 일부이며 이는 \"Linux Kernel Organization\"에 의해 배포되는 표준 패키지입니다. kill, more, renice, su 등과 같은 이 패키지에 포함된 다른 실행 파일도 있음을 이해하는 것이 중요합니다 (https://en.wikipedia.org/wiki/Util-linux). \"login\"의 소스 코드를 \"util-linux\" 깃허브 레포지토리 (https://github.com/util-linux/util-linux/blob/master/login-utils/login.c)에서 확인할 수 있습니다.\n\n또한, login은 시스템 전체 사용자 인증을 위한 프레임워크를 제공하는 PAM (Package Authentication Modules)에 기반합니다. 이를 확인하기 위해 \"ldd\" (https://medium.com/@boutnaru/linux-instrumentation-part-4-ldd-888502965a9b)를 사용할 수 있으며 libpam.so 및 아마도 libpam_misc.so도 표시될 것입니다 (https://medium.com/@boutnaru/the-linux-security-journey-pam-pluggable-authentication-module-388496a8785c).\n\n마지막으로, \"login\"의 동작에 영향을 주는 다양한 구성 파일이 있습니다 (PAM 구성 파일 외). 이러한 구성 파일 중에는 \"/etc/login.def\", /etc/motd, /etc/passwd 및 /etc/nologin이 있으며 미래의 글에서 자세한 정보를 확인할 수 있습니다. 또한 \"login\"에 의해 처리되는 로그 기반 파일도 있습니다. 예를 들어 /var/run/utmp, /var/log/wtmp 및 /var/log/lastlog이 있으며 미래의 글에서 자세히 다루겠습니다 (https://linux.die.net/man/1/login).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 다음 글에서 만나요 ;-) 트위터에서 제 계정을 팔로우할 수 있어요 — @boutnaru (https://twitter.com/boutnaru). 그리고 다른 글들은 미디엄에서도 읽을 수 있어요 — https://medium.com/@boutnaru. 무료 eBook도 https://TheLearningJourneyEbooks.com에서 찾을 수 있어요.\n\n![이미지](/assets/img/2024-06-22-TheLinuxProcessJourneylogin_0.png)","ogImage":{"url":"/assets/img/2024-06-22-TheLinuxProcessJourneylogin_0.png"},"coverImage":"/assets/img/2024-06-22-TheLinuxProcessJourneylogin_0.png","tag":["Tech"],"readingTime":2},{"title":"리눅스 보안 여정  EGID 유효 그룹 ID 이해하기","description":"","date":"2024-06-22 00:26","slug":"2024-06-22-TheLinuxSecurityJourneyEGIDEffectiveGroupID","content":"\n\nEUID(Effective User ID)와 마찬가지로 EGID(Effective Group ID)도 있어요. 리눅스 시스템에서 특정 작업(프로세스/쓰레드)의 권한을 결정하는 데 주로 사용되며 그룹 소속을 의미합니다.\n\n또한, 특권 사용자인 root와 같이 특권 사용자만 액세스하는 파일에 액세스할 수 있도록 비특권 사용자가 RGID(Real Group ID)와 다른 경우도 있습니다. \"current_egid\" 매크로를 사용하여 커널 내에서 EGID에 액세스할 수 있습니다.\n\n마지막으로 효과적인 그룹 ID를 출력하는 \"id\" 명령 줄 유틸리티를 사용할 수 있습니다. 또한 사용자 모드에서는 \"getegid()\" 시스템 호출을 사용하여 호출한 프로세스/작업의 효과적인 그룹 ID를 검색할 수 있습니다. 동일한 이름의 라이브러리 호출도 있습니다.\n\n다음 글에서 뵐게요 ;-) 트위터에서 저를 팔로우할 수도 있습니다 - @boutnaru. 미디엄에서 다른 글도 읽어보세요 - https://medium.com/@boutnaru. 무료 eBook은 https://TheLearningJourneyEbooks.com에서 다운로드할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Linux Screenshot](/assets/img/2024-06-22-TheLinuxSecurityJourneyEGIDEffectiveGroupID_0.png)","ogImage":{"url":"/assets/img/2024-06-22-TheLinuxSecurityJourneyEGIDEffectiveGroupID_0.png"},"coverImage":"/assets/img/2024-06-22-TheLinuxSecurityJourneyEGIDEffectiveGroupID_0.png","tag":["Tech"],"readingTime":1},{"title":"쉘 트랩과 POSIX 신호 쉽게 이해하기","description":"","date":"2024-06-22 00:22","slug":"2024-06-22-ShellTrapsandPOSIXSignals","content":"\n\n\n![Shell traps](/assets/img/2024-06-22-ShellTrapsandPOSIXSignals_0.png)\n\nShell traps catch POSIX signals (and more) to allow asynchronous inter-process communication to inform any process or particular thread of various events and do some work.\n\nBut do you know about all the different signals and ways to use the `trap` command?\n","ogImage":{"url":"/assets/img/2024-06-22-ShellTrapsandPOSIXSignals_0.png"},"coverImage":"/assets/img/2024-06-22-ShellTrapsandPOSIXSignals_0.png","tag":["Tech"],"readingTime":1},{"title":"개발자를 위한 Linux 명령어 치트시트 필수 DevOps 커맨드 모음","description":"","date":"2024-06-22 00:12","slug":"2024-06-22-CheatSheetLinuxCommandsforDevOps","content":"\n\n\u003cimg src=\"/assets/img/2024-06-22-CheatSheetLinuxCommandsforDevOps_0.png\" /\u003e\n\n당연히요! DevOps 전문가로서, 리눅스 명령줄을 능숙하게 사용하는 것은 서버 관리, 자동화 및 문제 해결에 매우 중요합니다. 이 포괄적인 가이드에서는 명확한 설명과 실용적인 예제를 함께 제공하여 50가지 이상의 필수 리눅스 명령(치트시트)를 다룰 것입니다. 이를 통해 당신의 리눅스 기술을 쉽고 실용적인 방식으로 향상시키는 데 도움이 될 것입니다.\n\n- id - 현재 사용자 또는 다른 사용자의 사용자 및 그룹 이름 및 숫자 ID (UID 또는 그룹 ID)를 찾는 데 사용됩니다.\n예: id -u root\n\n2. cd - 디렉토리 변경: 다른 디렉토리로 이동합니다.\n예시: cd /home/user/documents\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. **pwd** - Print Working Directory: 현재 디렉토리의 전체 경로를 표시합니다. 예시: pwd\n\n4. **mkdir** - Make Directory: 새로운 디렉토리를 생성합니다. 예시: mkdir new_folder\n\n5. **rm** - Remove: 파일이나 디렉토리를 삭제합니다. 예시: rm file.txt\n\n6. **cp** - Copy: 파일이나 디렉토리를 복사합니다. 예시: cp file.txt /backup\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n7. mv - 이동: 파일이나 디렉토리를 이동합니다.\n예시: mv file.txt /new_location\n\n8. touch - 빈 파일 생성: 새로운 빈 파일을 생성합니다.\n예시: touch new_file.txt\n\n9. cat - 연결하고 내용 표시: 파일의 내용을 확인합니다.\n예시: cat file.txt\n\n10. nano - 텍스트 편집기: 텍스트 파일을 편집합니다.\n예시: nano file.txt\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n11. grep - 텍스트 검색: 파일에서 텍스트 패턴을 검색합니다.\n예시: grep \"패턴\" file.txt\n\n12. find - 파일 및 디렉터리 검색: 파일 및 디렉터리를 찾습니다. 예시: find /검색할/경로 -name \"파일명\"\n\n13. chmod - 파일 권한 변경: 파일 권한을 수정합니다.\n예시: chmod 755 file.sh\n\n14. chown - 소유권 변경: 파일 또는 디렉터리의 소유자 및 그룹을 변경합니다.\n예시: chown 사용자:그룹 file.txt\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n15. **ps** - Process Status: 현재 실행 중인 프로세스를 표시합니다.\n예시: ps aux\n\n16. **top** - 시스템 활동 모니터링: 시스템 프로세스를 실시간으로 모니터링합니다. 예시: top\n\n17. **kill** - 프로세스 종료: 프로세스 ID를 사용하여 프로세스를 종료합니다. 또한 이름 또는 다른 속성에 기반하여 프로세스를 종료하는 pkill을 사용할 수도 있습니다.\n예시: kill PID\npkill 프로세스_이름\n\n18. **wget** - 파일 다운로드: 인터넷에서 파일을 다운로드합니다.\n예시: wget https://example.com/file.zip\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n19. less - 파일 내용을 한 화면씩 볼 수 있어 파일 내에서 쉽게 이동하고 검색할 수 있습니다. 예시: less test.log\n\n20. tar - 아카이브 및 추출: 압축된 아카이브 파일을 생성하거나 추출합니다. 예시: tar -czvf archive.tar.gz folder\n\n21. ssh - 보안 셸: 원격 서버에 안전하게 연결합니다. 예시: ssh user@remote_host\n\n22. scp - 안전하게 파일 복사: SSH를 사용하여 로컬 및 원격 시스템 간에 파일을 복사합니다. 예시: scp file.txt user@remote_host:/path\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n23. rsync - Remote Sync: 시스템 간 파일 및 디렉터리를 동기화합니다.\n예시: rsync -avz local_folder/ user@remote_host:remote_folder/\n\n24. df - 디스크 여유 공간: 디스크 공간 사용량을 표시합니다.\n예시: df -h\n\n25. du - 디스크 사용량: 파일 및 디렉터리의 크기를 표시합니다.\n예시: du -sh /path/to/directory\n\n26. ifconfig - 네트워크 구성: 네트워크 인터페이스를 표시하거나 구성합니다 (폐기됨, ip를 사용하세요).\n예시: ifconfig\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n27. **ip** - IP Configuration: IP 주소 및 네트워크 설정을 관리합니다. 예: `ip addr show`\n\n28. **netstat** - 네트워크 통계: 네트워크 연결 및 통계를 표시합니다 (사용이 권장되지 않습니다, **ss**를 사용하세요). 예: `netstat -tuln`\n\n29. **systemctl** - 시스템 제어: systemd를 사용하여 시스템 서비스를 관리합니다. 예: `systemctl start service_name`\n\n30. **journalctl** - 시스템 로그: systemd의 저널을 사용하여 시스템 로그를 확인합니다. 예: `journalctl -u service_name`\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n31. free - 이 명령은 이용 가능한 총 빈 공간의 양을 표시합니다.  \n예시: free -m\n\n32. at - 나중에 명령 실행: 지정된 시간에 명령을 실행합니다.  \n예시: echo \"command\" | at 15:30\n\n33. ping - 네트워크 연결 확인: 호스트로의 네트워크 연결을 확인합니다.  \n예시: ping google.com\n\n34. traceroute - 경로 추적: 호스트에 도달하기 위해 패킷이 이동하는 경로를 추적합니다.  \n예시: traceroute google.com\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n35. - 웹사이트 연결 확인: 웹사이트가 정상적으로 작동하는지 확인합니다.\n예시: curl -Is https://example.com | head -n 1\n\n36. dig - 도메인 정보 검색기: 도메인의 DNS 정보를 가져옵니다.\n예시: dig example.com\n\n37. hostname - 호스트 이름 표시 또는 설정: 시스템의 호스트 이름을 표시하거나 변경합니다.\n예시: hostname\n\n38. who - 사용자 표시: 현재 로그인한 사용자를 표시합니다.\n예시: who\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n39. useradd - 사용자 추가: 새 사용자 계정을 만듭니다.\n예시: useradd 새사용자\n\n40. usermod - 사용자 수정: 사용자 계정 속성을 수정합니다.\n예시: usermod -aG 그룹이름 사용자이름\n\n41. passwd - 비밀번호 변경: 사용자 비밀번호를 변경합니다.\n예시: passwd 사용자이름\n\n42. sudo - 슈퍼유저 권한 실행: 슈퍼유저로써 명령어를 실행합니다.\n예시: sudo 명령어\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n43. **lsof** - 파일 목록 표시: 열려 있는 파일과 해당 파일을 사용하는 프로세스 목록을 표시합니다. 예시: lsof -i :포트\n\n44. **nc** - Netcat: 네트워크 연결을 통해 데이터를 읽고 쓰는 네트워크 유틸리티입니다. 예시: echo \"Hello\" | nc 호스트 포트\n\n45. **scp** - 호스트 간 안전한 복사: 호스트 간에 파일을 안전하게 복사합니다. 예시: scp 파일.txt 사용자@원격_호스트:/경로\n\n46. **sed** - 스트림 편집기: 정규 표현식을 사용한 텍스트 조작입니다. 예시: sed `s/old/new/g` 파일.txt\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n47. awk - 텍스트 처리: 패턴 스캔 및 텍스트 처리.\n예시: awk `'print $2'` file.txt\n\n48. cut - 텍스트 열 추출: 텍스트에서 특정 열을 추출합니다. 예시: cut -d\",\" -f2 file.csv\n\n49. sort - 줄 정렬: 텍스트 파일의 줄을 정렬합니다.\n예시: sort file.txt\n\n50. diff - 파일 비교: 두 파일을 비교하여 차이점을 표시합니다. 예시: diff file1.txt file2.txt\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n51. ls - 파일 및 디렉토리 목록 조회: 디렉토리 내용을 나열합니다.\n예시: ls -la\n\n52. history - 이 명령은 이전에 실행된 명령을 확인하는 데 사용됩니다.\n예시: history 10\n\n53. cron - 작업 일정 예약: 예약된 작업을 관리합니다.\n예시: crontab -e\n\n54. ssh-keygen - 이 명령은 공개 및 개인 인증 키 쌍을 생성하는 데 사용됩니다. 이 인증 과정을 통해 사용자는 암호를 제공하지 않고 원격 서버에 연결할 수 있습니다.\n예시: ssh-keygen\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n55. nslookup - \"Name server Lookup\"의 약자입니다. DNS 호스트 이름을 IP 또는 IP를 호스트 이름으로 확인하는 도구입니다. 문제 해결 중에 매우 유용합니다.  \n예시: nslookup google.com\n\n56. tr - 문자 변환 또는 삭제에 사용됩니다.\n\n이러한 명령어는 Linux 시스템과 작업하는 DevOps 전문가에게 필수적인 다양한 작업을 다룹니다. 각 명령어와 옵션에 대한 자세한 정보는 항상 man 페이지 (man 명령)를 참조하시기 바랍니다.  \n예시: cat crazy.txt | tr \"[a-z]\" \"[A-Z]\"\n\n57. tnc - \"Test Network Connection\"의 약어입니다. 주로 문제 해결 중에 사용되는 명령어입니다. 연결에 대한 진단 정보를 표시합니다.  \n예시: tnc google.com --port 443\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n58. w - 현재 사용자를 표시합니다.\n\n59. su - 사용자 전환.\n예시: su - root\n\n60. ac(All Connections) — 모든 사용자 또는 지정된 사용자의 총 연결 시간을 나타냅니다.\n예시: ac john\n\n61. tail — 파일의 마지막 부분을 표시합니다. 실시간 로그 모니터링에 자주 사용됩니다.\n예시: tail monitor.logs\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n62. head — 파일의 첫 부분을 표시하여 파일 내용의 처음을 빠르게 확인하는 데 자주 사용됩니다.\n예시: head content.txt\n\n63. ip route — IP 라우팅 테이블을 표시하거나 조작하는 데 사용됩니다. IP 테이블 규칙을 명확하게 표시합니다.\n예시: ip rout\n\nDevOps 작업을 위한 Linux 명령어와 팁 자세히 보기\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDevOps 전문가들은 주로 시스템을 관리하고 작업을 자동화하며 인프라의 원활한 작동을 보장하기 위해 필수적인 Linux 명령어 세트에 의존합니다. 이러한 명령어는 DevOps 작업에 기초를 두고 있으며 시스템 관리부터 배포 자동화에 이르기까지 다양한 문맥에서 사용됩니다.","ogImage":{"url":"/assets/img/2024-06-22-CheatSheetLinuxCommandsforDevOps_0.png"},"coverImage":"/assets/img/2024-06-22-CheatSheetLinuxCommandsforDevOps_0.png","tag":["Tech"],"readingTime":6},{"title":"개발자를 행복하게 만드는 5가지 필수 Linux 명령어","description":"","date":"2024-06-22 00:10","slug":"2024-06-22-5LinuxCommandsthatMakesYouaHappyDev","content":"\n\n![Linux Commands](/assets/img/2024-06-22-5LinuxCommandsthatMakesYouaHappyDev_0.png)\n\n저는 다음 명령어를 사용하는 개발자들이 정말로 가장 흥미로운 사람들이라는 것을 발견했어요!\n\n# CMATRIX\n\nCMatrix는 더 매트릭스 웹사이트의 스크린세이버를 기반으로 합니다. 이것은 \"매트릭스\" 영화에서 본 것처럼 터미널에서 텍스트가 날아다니는 것을 보여줍니다. 모든 라인을 동일한 속도로 또는 비동기적으로 그리고 사용자가 정의한 속도로 스크롤할 수 있습니다. CMATRIX는 영감을 받아…","ogImage":{"url":"/assets/img/2024-06-22-5LinuxCommandsthatMakesYouaHappyDev_0.png"},"coverImage":"/assets/img/2024-06-22-5LinuxCommandsthatMakesYouaHappyDev_0.png","tag":["Tech"],"readingTime":1},{"title":"예제 중심으로 배우는 AWK를 사용한 리눅스유닉스 텍스트 처리 방법","description":"","date":"2024-06-22 00:06","slug":"2024-06-22-TextProcessingwithAWKinLinuxUnixwithexamples","content":"\n\n![이미지](/assets/img/2024-06-22-TextProcessingwithAWKinLinuxUnixwithexamples_0.png) \n\nawk은 패턴 스캔 및 처리를 위한 강력한 프로그래밍 언어이자 명령 줄 도구입니다. 일반적으로 텍스트 처리에 사용되며 데이터 추출 및 보고 도구로 주로 사용됩니다. 이 안내서는 awk의 기본을 이해하고 Linux/Unix에서 효과적으로 사용하는 방법을 보여줄 것입니다.\n\n# awk 소개\n\nawk는 생성자인 Alfred Aho, Peter Weinberger, Brian Kernighan의 이름을 따서 지어졌습니다. 사용자가 지정한 패턴 및 작업을 적용하여 텍스트를 한 줄씩 처리합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 기본 구문\n\nawk의 기본 구문은 다음과 같습니다:\n\n```js\nawk '패턴 { 동작 }' 파일\n```\n\n- 패턴: 일치해야 하는 조건을 지정합니다.\n- 동작: 패턴이 일치할 때 수행할 작업을 지정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식으로 표의 내용을 변경한 코드 예시입니다:\n\n\n| 표형식의 데이터 |\n\n\n# 일반적인 사용 사례 및 예시\n\n텍스트를 입력하여 awk를 인라인으로 사용할 수도 있습니다:\n\n```js\necho \"text\" | awk '패턴 { 동작 }'\n```\n\n예를 들어, 다음과 같은 data.txt 파일이 있다고 가정해보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nJohn Doe 30 180\nJane Smith 25 165\nAlice Johnson 35 170\nBob Brown 28 175\nCharlie White 32 160\n```\n\n\u003cimg src=\"/assets/img/2024-06-22-TextProcessingwithAWKinLinuxUnixwithexamples_1.png\" /\u003e\n\n## 1. Printing Specific Columns separated by space\n\nTo print the first names (1st field) and ages (3rd field) separated by space:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nawk '{ print $1, $3 }' data.txt\n\n\n----- 결과 -----\n\nJohn 30\nJane 25\nAlice 35\nBob 28\nCharlie 32\n```\n\n## 2. 특정 열을 특정 문자로 구분하여 출력하기\n\n성씨(1번 필드)와 나이(3번 필드)를 세미콜론으로 구분하여 출력하는 방법:\n\n```js\nawk '{ print $1 \";\" $3 }' data.txt\n\n\n----- 결과 -----\n\nJohn;30\nJane;25\nAlice;35\nBob;28\nCharlie;32\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 3. 조건에 따라 행 필터링하기\n\n네 번째 열이 169보다 큰 경우에 해당하는 모든 행을 출력합니다.\n\n```js\nawk '$4 \u003e 169' data.txt\n\n\n----- 결과 -----\n\nJohn Doe 30 180\nAlice Johnson 35 170\nBob Brown 28 175\n```\n\n## 4. 특정 단어를 포함하는 행 출력하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"John\"이라는 단어를 포함한 라인을 출력합니다.\n\n```js\nawk '/John/' data.txt\n\n\n----- 결과 -----\n\nJohn Doe 30 180\nAlice Johnson 35 170\n```\n\n이 기능은 로그 파일을 처리하고 \"Error\" 또는 \"Warning\"과 같은 키워드를 포함하는 라인을 검색할 때 특히 유용합니다.\n\n## 5. 열을 더합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nawk '{ print $3 + $4 }' data.txt\n\n----- 출력 결과 -----\n\n210\n190\n205\n203\n192\n```\n\n- 다른 대안 (변수 사용)\n\n```js\nawk '{ sum=$3+$4 ; print sum }' data.txt\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 6. 합계 값\n\n세 번째 열의 값을 합산하여 총합을 출력합니다.\n\n```js\nawk '{ sum+=$3 } END { print sum }' data.txt\n\n----- 출력 결과 -----\n\n150\n```\n\n## 7. 평균 계산\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n평균 연령을 계산하려면 (세 번째 필드):\n\n```js\nawk '{ sum += $3; count++ } END { print sum / count }' data.txt\n\n\n----- 결과 -----\n\n30\n```\n\n## 8. 행 번호 출력\n\n각 행에 행 번호를 추가하고 출력합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nawk '{print NR, $0}' data.txt\n\n\n----- 출력 -----\n\n1 John Doe 30 180\n2 Jane Smith 25 165\n3 Alice Johnson 35 170\n4 Bob Brown 28 175\n5 Charlie White 32 160\n```\n\n## 9. 필드 수 출력\n\n각 줄의 필드 수를 출력합니다.\n\n```js\nawk '{ print \"필드 수:\", NF }' data.txt\n\n\n----- 출력 -----\n\n필드 수: 4\n필드 수: 4\n필드 수: 4\n필드 수: 4\n필드 수: 4\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 10. 첫 번째 및 마지막 필드 인쇄\n\n```js\nawk '{ print $1, $NF }' data.txt\n\n\n----- 결과 -----\n\nJohn 180\nJane 165\nAlice 170\nBob 175\nCharlie 160\n```\n\n## 11. 대문자로 필드 출력\n\n첫 번째 필드를 대문자로 출력\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nawk '{ print toupper($1) }' data.txt\n\n----- 결과 -----\n\nJOHN\nJANE\nALICE\nBOB\nCHARLIE\n```\n\n## 12. 필드에서 부분 문자열 추출하기\n\n2번 필드에서 부분 문자열 추출: 1번 문자부터 3번 문자까지\n\n```js\nawk '{print substr($2,1,3)}' data.txt\n\n----- 결과 -----\n\nDoe\nSmi\nJoh\nBro\nWhi\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 13. 각 행의 두 번째 필드의 길이 출력\n\n각 행의 두 번째 필드의 길이를 출력합니다.\n\n```js\nawk '{ print length($2) }' data.txt\n\n\n----- 결과 -----\n\n3\n5\n7\n5\n5\n```\n\n## 14. 사용자 정의 함수\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 복잡한 작업을 위해 awk 스크립트 내에서 함수를 정의할 수 있어요:\n\n```js\nawk '\nfunction square(x) { return x * x }\n{ print $3, \" --\u003e square :\" , square($3) }\n' data.txt\n\n\n----- 출력 결과 -----\n\n30  --\u003e square : 900\n25  --\u003e square : 625\n35  --\u003e square : 1225\n28  --\u003e square : 784\n32  --\u003e square : 1024\n```\n\n# 결론\n\nawk는 여러 가지 방법으로 텍스트 파일을 조작하고 분석하는 데 도움이 되는 다재다능한 도구입니다. 데이터 추출, 계산 수행 또는 텍스트 변환 등을 하고자 할 때 awk는 작업을 간편하게 해주는 다양한 기능을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nawk 명령어를 적극 활용하여 그 능력을 최대한 발휘하고 리눅스/유닉스에서 더 효율적인 텍스트 처리를 위한 워크플로에 통합해보세요!","ogImage":{"url":"/assets/img/2024-06-22-TextProcessingwithAWKinLinuxUnixwithexamples_0.png"},"coverImage":"/assets/img/2024-06-22-TextProcessingwithAWKinLinuxUnixwithexamples_0.png","tag":["Tech"],"readingTime":4},{"title":"쿠버네티스 네트워킹 완벽 가이드 로드 밸런스, BGP, IPVS 등 핵심 기술 탐구하기","description":"","date":"2024-06-22 00:02","slug":"2024-06-22-DecipheringtheKubernetesNetworkingMazeNavigatingLoad-BalanceBGPIPVSandBeyond","content":"\n\n쿠버네티스 세계에서는 매일 ipvs 대 iptables || pureLB 대 metalLB || overlay 대 underlay || Nodeport 대 Loadbalance와 같은 용어가 자주 들려옵니다. 이런 정보들을 다양한 소스에서 얻어서 하나로 묶는 것은 정말 어렵습니다. 그래서 저는 여기서 그것을 해냈습니다.\n\n이 질문에 대한 답을 아시나요?\n네트워킹 측면이 모두 어떻게 관리되는가요?\npureLB가 CNI에 어떻게 연결되는가요?\nClusterIP 서비스가 IPVS에 어떻게 연결되는가요?\nNodeport를 사용했을 때 netstat으로 열린 포트를 볼 수 없는 이유는 무엇일까요?\n\n# 모든 것의 큰 그림\n\n저는 머릿 속에 20개의 웹사이트와 기사를 모두 정리해서 쿠버네티스 네트워킹을 이해하는 것이 어려웠지만, 그걸 해냈고, 여러분에게 더 쉽게 이해시킬 수 있기를 바랍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제가 말씀드릴 주제는 이러한 주제들이 Kubernetes와 어떻게 연결되어 있는지 보고 통합되어 있는지를 알아보겠습니다.\n로드 밸런싱, ipvs, iptables, BGP, 브릿지, CNI, PureLB, 엔드포인트, 서비스, 오버레이, 언더레이, ipip, kube-proxy, 인그레스 컨트롤러.\n\n빠르고 단계별로 진행해 봅시다:\n\n# 1- CNI, LB 컨트롤러와 Kube-proxy의 관계\n\nCNI: 각 컨테이너에 대한 네트워크 인터페이스를 생성하고 구성하여 Kubernetes 네트워킹을 구성합니다. Kubelet은 CNI를 호출하여 네트워크 인터페이스를 설정하고 IP 주소를 할당합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCNI는 2가지 모델에서 작동합니다:\n- 캡슐화(overlay)\n- 비캡슐화(underlay)\n\n- 캡슐화(overlay): VXLAN 및 IPSEC와 같은 기술을 나타냅니다. 이는 여러 Kubernetes 노드에 걸칠 수 있는 레이어-3 네트워킹의 레이어-2입니다. 레이어-2는 격리되어 있어 라우팅 배포가 필요하지 않습니다. IP 패키지를 제공하는 IP 헤더를 생성합니다. 이 모델은 워커와 파드를 연결하는 브리지를 제공합니다. 통신을 관리하는 요소는 CRI입니다.\n\n![이미지](/assets/img/2024-06-22-DecipheringtheKubernetesNetworkingMazeNavigatingLoad-BalanceBGPIPVSandBeyond_0.png)\n\n- 비캡슐화(underlay): 컨테이너 간에 패킷을 라우팅하기 위한 L3 네트워크를 제공합니다. BGP를 사용하여 라우팅 정보를 분배하기 위해 워커가 필요합니다. 이 모델은 워커 사이에 네트워크 라우터를 확장하는 것을 포함합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-DecipheringtheKubernetesNetworkingMazeNavigatingLoad-BalanceBGPIPVSandBeyond_1.png)\n\nLB-controller: MetalLB, PureLB,...은 Kubernetes의 LoadBalancer 서비스 유형의 기능을 제공합니다.\n로드 밸런서(LB) 서비스를 생성할 때 할당된 외부 IP는 기본 인터페이스 아래에 보조 주소로 생성됩니다. 이는 BGP BIRD 라우터가 IP를 캡처하고 경로, 주소 및 기타 구성을 추가할 수 있게 합니다.\n\n새로운 IP가 할당되면:\n\n![이미지](/assets/img/2024-06-22-DecipheringtheKubernetesNetworkingMazeNavigatingLoad-BalanceBGPIPVSandBeyond_2.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nKube-proxy: iptables, ipvs 등에서 네트워크 규칙을 유지합니다.\n네트워크 정책, NAT, 전방규칙을 추가합니다.\n\n간단한 예를 들어보면:\nsvc를 생성하면 kube-proxy가 iptables에 규칙을 추가합니다.\n\n이 부분에 대한 요약:\n\n- Kube-proxy: IPTABLES, IPVS 등에서 규칙을 유지합니다.\n- CNI: 기본 네트워크에 대한 공통 인터페이스를 제공하고, 트래픽을 원하는 대상으로 라우팅하며, 관련 기능을 수행합니다.\n- LB-controller: 부하 분산 기능을 제공하고, 호스트 인터페이스를 업데이트하여 보조 IP 주소를 추가합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2-POD to POD / Container to Container — 단일 노드 (IP 주소 기반)\n\n![링크와 관련된 이미지](/assets/img/2024-06-22-DecipheringtheKubernetesNetworkingMazeNavigatingLoad-BalanceBGPIPVSandBeyond_3.png)\n\nCustom Bridge (CBR), Veth (가상 이더넷), Ethernet (eth) 및 전체 네트워킹 설정은 containerd, CRI-O, Mirantis 등의 컨테이너 런타임에 의해 처리됩니다. 대부분의 컨테이너 런타임 인터페이스 (CRI)는 Calico, Flannel, Cilium 등의 옵션을 포함하여 목적에 맞게 컨테이너 네트워킹 인터페이스 (CNI) 플러그인을 활용합니다.\n\nPod 내의 모든 컨테이너는 동일한 네트워크를 공유합니다. 왜냐하면 그들은 동일한 네트워크 네임스페이스 내에 있기 때문이죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"일시 정지(pause) 컨테이너는 Kubernetes에서 네트워킹 및 프로세스간 통신(IPC)을 책임집니다.\n\n각 파드마다 CBR에 Veth가 생성되며, 이 브리지에서 L-2 라우팅이 수행됩니다. 예를 들어, 파드1에서 파드2로의 패킷은 CBR을 통해 전달되며, 이 경우 NAT가 발생하지 않습니다.\n\n# 3- POD to POD / Container to Container — multi node (IP 기반)\n\n![이미지](/assets/img/2024-06-22-DecipheringtheKubernetesNetworkingMazeNavigatingLoad-BalanceBGPIPVSandBeyond_4.png)\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 노드가 동일한 네트워크에 있기 때문에 팟의 IP 주소가 노드 네트워크 전체에서 라우팅됩니다.\n- 두 노드 모두 동일한 네트워크에 있습니다. (서로를 볼 수 있음)\n- CNI는 각 노드의 각 팟에 대한 라우트를 생성합니다.\n\nNode-1의 CBR에는 pod4의 Mac 주소가 없기 때문에 패킷이 지정된 라우팅 테이블이 있는 인터페이스를 통해 전달됩니다. 이것은 터널, 다른 인터페이스, eth0 등이 될 수 있습니다. 구조에 따라 실제로 달라집니다.\n\nKubernetes의 각 노드는 자체 CIDR을 갖고 있어 올바른 노드로 트래픽을 라우팅할 수 있습니다.\n\n# 4- POD to POD / Container to Container — multi node (Service IP addr based)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-22-DecipheringtheKubernetesNetworkingMazeNavigatingLoad-BalanceBGPIPVSandBeyond_5.png)\n\n서비스에 관한 경우, IPTABLES/IPVS가 중요한 역할을 합니다. Netfilter에서, 서비스 IP 주소는 무작위로 관련 pod IP 주소로 변경됩니다(로드 밸런싱 알고리즘에 따라). Kube-proxy는 Netfilter 규칙을 업데이트하고 pod IP 주소를 서비스에 할당하는 책임이 있습니다.\n\n노드가 svc 목적지를 갖는 패킷을 받을 때, Netfilter에서 규칙이 서비스와 일치하고 대상 pod IP 주소로 경유됩니다.\n\n그렇다면, 그 과정이 어떻게 이루어지는 걸까요?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n서비스는 서비스 선택기에 지정된 파드 레이블과 일치하여 엔드포인트 슬라이스를 업데이트합니다. 선택기가 파드의 레이블과 일치하면 IP 주소, 포트, 프로토콜 등과 같은 관련 정보가 검색되어 서비스와 관련된 엔드포인트 슬라이스에 주입됩니다.\n\n# 큰 그림에 대비 준비 되셨나요? 준비 되셨다면 이제 시작하세요.\n\n\u003cimg src=\"/assets/img/2024-06-22-DecipheringtheKubernetesNetworkingMazeNavigatingLoad-BalanceBGPIPVSandBeyond_7.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 그림에는 CNI (Calico), IPVS, PURELB, IPIP (overlay) 및 인그레스 컨트롤러가 모두 함께 구현되어 있으며 각각의 역할이 있습니다.\n\nCalico: Calico에 의해 모든 네트워킹이 처리됩니다. 오버레이 (IPIP) 및 언더레이 (BGP).\n\n- 이전에 언급한 바와 같이 모든 파드 IP 주소는 이 경우에는 kube-ipvs 브리지 인터페이스인 CBR에 할당됩니다.\n- 각 파드는 자체 가상 인터페이스를 갖습니다.\n- tunl0 (IPIP)은 가상 인터페이스로, 오버레이 아키텍처로 노드를 서로 연결합니다. 이는 모든 파드의 IP 주소가 이 터널을 통과한다는 것을 의미합니다.\n- PureLB는 로드 밸런서(LB) 컨트롤러이며 kube-lb0를 가상 인터페이스로 구현하여 호스트 네트워크를 관리하며, 호스트 기본 인터페이스에 LB IP 주소를 추가로 할당합니다.\n- PureLB는 BGP, OSPF와 같은 라우팅 프로토콜과 호환됩니다. 이미 Calico에 의해 구현된 BGP BIRD 라우터가 있기 때문에, PureLB는 이를 인식하고 다른 BGP 라우터를 구현하지 않습니다.\n- BGP는 인터페이스에 할당된 모든 IP 주소를 수집하고 이를 라우팅 테이블에서 정의합니다.\n\n지금까지, 파드-1에서 다른 노드의 파드-5에 도달하려는 패킷이 있을 때, kube-ipvs가 알지 못하는 것에 대한 답변을 할 수 없어서, 다음 단계는 BGP에 의해 업데이트된 라우팅 테이블입니다. 오버레이 네트워크 아키텍처를 갖고 있기 때문에, 원하는 대상으로 라우팅될 것입니다. 서비스를 호출하면 ipvs 규칙이 작동합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제는 IPVS에서 Endpoints, 서비스, NodePort 및 LB가 유일한 규칙임을 알고 있습니다. 그것을 염두에 두고:\n\n![이미지](/assets/img/2024-06-22-DecipheringtheKubernetesNetworkingMazeNavigatingLoad-BalanceBGPIPVSandBeyond_8.png)\n\n저희는 인그레스 컨트롤러를 위한 로드밸런서 유형의 서비스를 가지고 있습니다. 이는 외부에서 인그레스에 접근 가능함을 의미합니다. 이 IP를 호출하면 패킷이 적절한 노드로 이동한 후, IPVS가 그것을 NodePort(NAT)로 전달하여 노드를 통해 경로를 찾아 해당 노드로 이동합니다.\n\n그 후에, NodePort는 ClusterIP와 연관되는데, 이는 인그레스 컨트롤러 팟의 IP 주소를 알고 있습니다. 이 설정은 유용한데, 인그레스 컨트롤러가 패킷을 받는 즉시 정의된 규칙을 기반으로 원하는 서비스로 경로를 설정하고, 그 후 목적지 팟으로 이동하기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사의 목표는 각 구성 요소에 대한 철저한 설명을 제공하는 것이 아니었습니다. 대신, 이미 각 개념에 익숙한 사람들을 위해 정보를 통합하여 한 곳에서 포괄적인 개요를 제공하는 데 초점을 맞추었습니다.\n\n저는 여러 출처를 사용했고, 더 많은 정보를 위해 여기에 언급합니다:\n\nhttps://medium.com/thermokline/comparing-k8s-load-balancers-2f5c76ea8f31\n\nhttps://medium.com/@seifeddinerajhi/kube-proxy-and-cni-the-hidden-components-of-kubernetes-networking-eb30000bf87a\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[Calico Networking](https://docs.tigera.io/calico/latest/networking/)\n\n[Overview of PureLB](https://purelb.gitlab.io/docs/how_it_works/overview/)","ogImage":{"url":"/assets/img/2024-06-22-DecipheringtheKubernetesNetworkingMazeNavigatingLoad-BalanceBGPIPVSandBeyond_0.png"},"coverImage":"/assets/img/2024-06-22-DecipheringtheKubernetesNetworkingMazeNavigatingLoad-BalanceBGPIPVSandBeyond_0.png","tag":["Tech"],"readingTime":7},{"title":"좀비 프로세스의 비밀을 밝히다 Linux에서 알아야 할 모든 것","description":"","date":"2024-06-21 23:58","slug":"2024-06-21-LinuxUnveilingtheMysteriesofZombieProcesses","content":"\n\n\n![LinuxUnveilingtheMysteriesofZombieProcesses](/assets/img/2024-06-21-LinuxUnveilingtheMysteriesofZombieProcesses_0.png)\n\n어떤 프로세스가 \"exit\"를 호출하면 즉시 사라지지 않는다는 사실을 아는 사람은 적을 것입니다. 대신, \"좀비\" 프로세스라고 불리는 데이터 구조를 남깁니다. Linux 프로세스의 다섯 가지 상태 중에서 좀비 프로세스는 특히 독특합니다.\n\n거의 모든 메모리 공간을 포기했으며 실행 가능한 코드가 전혀 없으며 스케줄링될 수 없으며 단지 ...에 위치하고...\n","ogImage":{"url":"/assets/img/2024-06-21-LinuxUnveilingtheMysteriesofZombieProcesses_0.png"},"coverImage":"/assets/img/2024-06-21-LinuxUnveilingtheMysteriesofZombieProcesses_0.png","tag":["Tech"],"readingTime":1},{"title":"파이썬 엔지니어를 위한 3D 가우시안 스플래팅 소개 파트 2","description":"","date":"2024-06-20 19:11","slug":"2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2","content":"\n\n## 3D 가우시안 스플래팅 내에서 가우시안 함수가 어떻게 사용되는지 이해하고 코딩하기\n\n이제 가우시안에 대해 이야기해보겠습니다! 모두가 좋아하는 분포입니다. 지금부터 함께하는 분들을 위해, 카메라의 위치를 이용하여 3D 점을 2D로 변환하는 방법에 대해 part 1에서 다룬 바 있습니다. 이 글에서는 가우시안 스플래팅의 가우시안 부분을 다룰 것입니다. 우리는 GitHub에서 part_2.ipynb를 사용할 것입니다.\n\n여기서 우리가 만들게 될 약간의 변경사항은, 이전 글에서 보여준 것과는 다른 내부 매트릭스를 활용하는 원근 투영을 사용할 것이라는 것입니다. 그러나 2D로 점을 투영할 때 두 방법은 동등하며, 저는 part 1에서 소개된 첫 번째 방법이 이해하기 쉽다고 생각합니다. 그러나 가능한한 저자의 코드를 파이썬으로 복제하기 위해 저희는 방법을 변경할 것입니다. 구체적으로 우리의 \"내부\" 매트릭스는 이곳에 표시된 OpenGL 투영 매트릭스에 의해 이제 제공되며, 곱셈의 순서는 이제 points @ external.transpose() @ internal으로 변경될 것입니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2_0.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n호기심이 있는 분들을 위해 새로운 내부 매트릭스에 대해 알고 싶은 경우(그렇지 않으면 이 단락을 건너뛰어도 괜찮아요) r과 l은 오른쪽과 왼쪽 측면의 클리핑 평면이며, 사진의 너비에 관한 시야에 포함될 수 있는 지점을 기본적으로 나타내고 있습니다. t와 b는 상단과 하단 클리핑 평면이고, N은 가까운 클리핑 평면(투영될 점들이 있는 곳)이며, f는 먼 클리핑 평면입니다. 더 자세한 정보는 scratchapixel의 챕터들이 여기에서 매우 유익하다고 생각합니다(https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/opengl-perspective-projection-matrix.html). 이것은 또한 점들을 정규화된 장치 좌표( -1과 1 사이)로 반환하며, 이를 픽셀 좌표로 투영합니다. 이론에서 벗어나서 우리의 작업은 같습니다, 3D에서 점을 가져와 2D 이미지 평면으로 투영하는 것입니다. 그러나 이 튜토리얼의 이 부분에서는 이제 포인트 대신 가우시안 함수를 사용합니다.\n\n```js\ndef getIntinsicMatrix(\n    focal_x: torch.Tensor,\n    focal_y: torch.Tensor,\n    height: torch.Tensor,\n    width: torch.Tensor,\n    znear: torch.Tensor = torch.Tensor([100.0]),\n    zfar: torch.Tensor = torch.Tensor([0.001]),,\n) -\u003e torch.Tensor:\n    \"\"\"\n    내부 퍼스펙티브 투영 매트릭스 가져오기\n    \n    znear: 사용자가 지정한 가까운 평면\n    zfar: 사용자가 지정한 먼 평면\n    fovX: 초점 길이에서 계산된 x의 시야\n    fovY: 초점 길이에서 계산된 y의 시야\n    \"\"\"\n    fovX = torch.Tensor([2 * math.atan(width / (2 * focal_x))])\n    fovY = torch.Tensor([2 * math.atan(height / (2 * focal_y))])\n    \n    tanHalfFovY = math.tan((fovY / 2))\n    tanHalfFovX = math.tan((fovX / 2))\n\n    top = tanHalfFovY * znear\n    bottom = -top\n    right = tanHalfFovX * znear\n    left = -right\n    P = torch.zeros(4, 4)\n    z_sign = 1.0\n\n    P[0, 0] = 2.0 * znear / (right - left)\n    P[1, 1] = 2.0 * znear / (top - bottom)\n    P[0, 2] = (right + left) / (right - left)\n    P[1, 2] = (top + bottom) / (top - bottom)\n    P[3, 2] = z_sign\n    P[2, 2] = z_sign * zfar / (zfar - znear)\n    P[2, 3] = -(zfar * znear) / (zfar - znear)\n    return P\n```\n\n3D 가우시안 splat은 x, y, z 좌표 및 관련 공분산 행렬로 구성됩니다. 저자들이 언급한 대로: \"명백한 접근 방식은 공분산 행렬 Σ를 직접 최적화하여 빛의 필드를 나타내는 3D 가우시안을 얻는 것일 것입니다. 그러나, 공분산 행렬은 양의 준정치일 때만 물리적인 의미를 갖습니다. 우리가 모든 매개변수를 최적화하기 위해 사용하는 경사 하강법은 이러한 유효한 행렬을 생성하기가 쉽지 않으며, 업데이트 단계와 그래디언트는 쉽게 유효하지 않은 공분산 행렬을 만들어냅니다.\"\n\n그래서 저자들은 항상 양의 준정부 공분산 행렬을 생성할 수 있는 공분산 행렬의 분해를 사용합니다. 특히, 3개의 \"크기\" 매개변수와 4개의 쿼터니언을 사용하여 3x3 회전 행렬(R)로 변환합니다. 그런 다음 공분산 행렬은 다음과 같이 주어집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2_1.png)\n\n쿼터니온 벡터를 회전 행렬로 변환하기 전에 정규화해야만 유효한 회전 행렬을 얻을 수 있습니다. 따라서 저희 구현에서 가우스 포인트는 다음 매개변수로 구성됩니다. 좌표 (3x1 벡터), 쿼터니온 (4x1 벡터), 스케일 (3x1 벡터) 및 불투명도(스플래팅이 얼마나 투명한지를 나타내는 최종 float 값)입니다. 이제 모든게 갖춰졌네요! 이 11개의 매개변수를 최적화하여 우리의 씬을 만들 수 있습니다 — 간단하지요!\n\n하지만 실제로는 조금 복잡합니다. 고등학교 수학을 기억한다면 특정 지점에서의 가우시안의 세기는 아래의 방정식으로 주어집니다:\n\n![이미지](/assets/img/2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2_2.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만, 우리는 이미지 평면인 2D에서 3D 가우시안의 강도에 중점을 두고 있습니다. 하지만 여러분은 아마 \"우리는 2D로 점을 투영하는 방법을 알고 있다고?\" 할 수 있겠지만요! 그럼에도 불구하고, 아직 2D로 공분산 행렬을 투영하는 방법에 대해 다루지 않았기 때문에, 만약 우리가 2D 공분산 행렬의 역행렬을 찾지 않았다면 이 무슨 얘기인지 알 수 없겠죠.\n\n이제 재미있는 부분입니다(어떻게 보느냐에 따라 다를 수 있습니다). 3D 가우시안 스플래팅 저자들의 논문인 EWA Splatting은 3D 공분산 행렬을 2D로 투영하는 정확한 방법을 보여줍니다. 그러나 이것은 알려진 야코비안 아핀 변환 행렬의 지식을 전제로 한다는 것에 유념해야 합니다. 아래에서 계산하는 것처럼. 어려운 개념을 풀어갈 때 가장 도움이 되는 것은 코드이므로, 3D 공분산 행렬에서 2D로 전환하는 방법을 실제로 어떻게 하는 지 보여주기 위해 아래에 일부 코드를 제공했습니다.\n\n```js\ndef compute_2d_covariance(\n    points: torch.Tensor,\n    external_matrix: torch.Tensor,\n    covariance_3d: torch.Tensor,\n    tan_fovY: torch.Tensor,\n    tan_fovX: torch.Tensor,\n    focal_x: torch.Tensor,\n    focal_y: torch.Tensor,\n) -\u003e torch.Tensor:\n    \"\"\"\n    각 가우시안의 2D 공분산 행렬 계산\n    \"\"\"\n    points = torch.cat(\n        [points, torch.ones(points.shape[0], 1, device=points.device)], dim=1\n    )\n    points_transformed = (points @ external_matrix)[:, :3]\n    limx = 1.3 * tan_fovX\n    limy = 1.3 * tan_fovY\n    x = points_transformed[:, 0] / points_transformed[:, 2]\n    y = points_transformed[:, 1] / points_transformed[:, 2]\n    z = points_transformed[:, 2]\n    x = torch.clamp(x, -limx, limx) * z\n    y = torch.clamp(y, -limy, limy) * z\n\n    J = torch.zeros((points_transformed.shape[0], 3, 3), device=covariance_3d.device)\n    J[:, 0, 0] = focal_x / z\n    J[:, 0, 2] = -(focal_x * x) / (z**2)\n    J[:, 1, 1] = focal_y / z\n    J[:, 1, 2] = -(focal_y * y) / (z**2)\n\n    # 초기에 원근 투영을 위해 설정한 대로 전치함\n    # 이제 우리가 다시 변환하는 것이므로\n    W = external_matrix[:3, :3].T\n\n    return (J @ W @ covariance_3d @ W.T @ J.transpose(1, 2))[:, :2, :2]\n```\n\n먼저, tan_fovY와 tan_fovX는 시야각의 반을 나타내는 tangent 값입니다. 이러한 값들을 사용하여 투영을 클램핑하여 화면 바깥으로 너무 많이 벗어났을 때 렌더에 영향을 미치지 않도록 합니다. 우리는 초기 순방향 변환으로부터 주어진 3D에서 2D로의 변환으로부터 야코비안을 유도할 수 있지만, 여러분이 귀찮을 일을 덜어드리기 위해 위에서 기대할 수 있는 유도를 보여드릴게요. 마지막으로, 우리가 회전 행렬을 변환하면서 처음에 전치했습니다만, 최종 공분산 계산을 반환하기 전에 다시 전치해야 합니다. EWA 스플래팅 논문에 따르면, 우리는 2D 이미지 평면에만 관심이 있으므로 세 번째 행과 열은 무시할 수 있습니다. 처음부터 그렇게 할 수 없었던 이유에 대해 궁금할 수도 있습니다. 대부분의 경우, 이는 완벽한 구로 표현되지 않을 것이기 때문에 각도에 따라 공분산 행렬 매개변수가 달라지기 때문입니다! 이제 올바른 관점으로 변환했으므로, 공분산 z축 정보는 쓸모없으며 버릴 수 있게 되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주어진 2D 공분산 행렬이 있으면 이미지의 임의의 픽셀에 각 가우시안이 미치는 영향을 계산할 수 있게 되었습니다. 이제 역 공분산 행렬을 찾아야 합니다. 선형 대수학에서 다시 상기해보면 2x2 행렬의 역행렬을 찾으려면 행렬식을 찾고 일부 용어를 재배열하면 됩니다. 이 코드를 통해 해당 프로세스를 안내해드릴게요.\n\n```js\ndef compute_inverted_covariance(covariance_2d: torch.Tensor) -\u003e torch.Tensor:\n    \"\"\"\n    역 공분산 행렬 계산\n\n    2x2 행렬의 경우\n    다음과 같이 주어질 때\n    [[a, b],\n     [c, d]]\n     행렬식은 ad - bc입니다.\n\n    역행렬을 구하려면 다음과 같이 용어를 재배열하고\n    행렬식의 역수를 곱하면 됩니다\n    [[d, -b],\n     [-c, a]] * (1 / 행렬식)\n    \"\"\"\n    행렬식 = (\n        covariance_2d[:, 0, 0] * covariance_2d[:, 1, 1]\n        - covariance_2d[:, 0, 1] * covariance_2d[:, 1, 0]\n    )\n    행렬식 = torch.clamp(행렬식, min=1e-3)\n    역_공분산 = torch.zeros_like(covariance_2d)\n    역_공분산[:, 0, 0] = covariance_2d[:, 1, 1] / 행렬식\n    역_공분산[:, 1, 1] = covariance_2d[:, 0, 0] / 행렬식\n    역_공분산[:, 0, 1] = -covariance_2d[:, 0, 1] / 행렬식\n    역_공분산[:, 1, 0] = -covariance_2d[:, 1, 0] / 행렬식\n    return 역_공분산\n```\n\n그리고 이제 이미지의 모든 픽셀에 대해 픽셀 강도를 계산할 수 있습니다. 그러나 이렇게 하는 것은 굉장히 느리고 불필요합니다. 예를 들어, (0,0)에서 스플래시가 (1000,1000)의 픽셀에 어떤 영향을 미치는지 계산하는 데 계산 시간을 낭비할 필요가 없습니다. 공분산 행렬이 거대하지 않다면 말이죠. 따라서 저자들은 각 스플래시마다 \"반경\"이라고 부르는 값을 계산하기로 결정했습니다. 아래 코드에서 볼 수 있듯이 각 축을 따라 고유값을 계산합니다(고유값은 변화를 나타냅니다). 그런 다음 가장 큰 고유값의 제곱근을 취하여 표준 편차를 얻고 3.0을 곱합니다. 이것은 분포의 99.7%를 3표준 편차 내에 포함시킵니다. 이 반경을 사용하면 스플래시가 닿는 x 및 y 값의 최솟값과 최댓값을 파악할 수 있습니다. 렌더링할 때 이러한 경계 내의 픽셀에 대해만 스플래시 강도를 계산하며 불필요한 계산을 피합니다. 상당히 똑똑한 방법이죠?\n\n```js\ndef compute_extent_and_radius(covariance_2d: torch.Tensor):\n    mid = 0.5 * (covariance_2d[:, 0, 0] + covariance_2d[:, 1, 1])\n    det = covariance_2d[:, 0, 0] * covariance_2d[:, 1, 1] - covariance_2d[:, 0, 1] ** 2\n    intermediate_matrix = (mid * mid - det).view(-1, 1)\n    intermediate_matrix = torch.cat(\n        [intermediate_matrix, torch.ones_like(intermediate_matrix) * 0.1], dim=1\n    )\n\n    max_values = torch.max(intermediate_matrix, dim=1).values\n    lambda1 = mid + torch.sqrt(max_values)\n    lambda2 = mid - torch.sqrt(max_values)\n    # 이제 고유값을 갖고 있으므로 최대 반경을 계산할 수 있습니다\n    max_radius = torch.ceil(3.0 * torch.sqrt(torch.max(lambda1, lambda2)))\n\n    return max_radius\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 모든 단계를 거쳐 우리는 그것을 렌더 단계에서 사용할 수 있는 전처리된 장면을 얻습니다. 간단히 말해 이제 2D에서의 포인트, 해당 포인트와 관련된 색, 2D에서의 공분산, 2D에서의 역공분산, 정렬된 깊이 순서, 각 스플랫에 대한 최소 x, 최소 y, 최대 x, 최대 y 값, 그리고 관련 투명도를 가지게 되었어요. 이러한 모든 구성 요소를 갖고 이미지 렌더링으로 넘어 갈 수 있습니다!\n\n- Kerbl, Bernhard, et al. “3d gaussian splatting for real-time radiance field rendering.” ACM Transactions on Graphics 42.4 (2023): 1–14.\n- Zwicker, Matthias, et al. “EWA splatting.” IEEE Transactions on Visualization and Computer Graphics 8.3 (2002): 223–238.","ogImage":{"url":"/assets/img/2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2_0.png"},"coverImage":"/assets/img/2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2_0.png","tag":["Tech"],"readingTime":9},{"title":"신경망 기본 이론과 구조 유형","description":"","date":"2024-06-20 19:07","slug":"2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes","content":"\n이 이야기에서는 신경망의 이론적 기초와 이로부터 파생된 기술, 그리고 PyTorch를 사용한 구현의 가장 중요한 측면을 높은 수준에서 리뷰하고 설명해보려고 합니다. 가능한 간단한 언어를 사용하여 설명하겠습니다. 또한 다른 문서에서 문서화한 사용 사례 예시를 소개할 예정입니다.\n\n신경망은 이름 그대로 뉴런으로 구성된 복잡한 시스템입니다. 이 네트워크의 힘은 이 인공 뉴런들 간의 상호 연결에서 나옵니다. 이러한 NN 알고리즘은 생물학적 시스템을 모방한다고 합니다.\n\n# 뉴런:\n\n신경망의 핵심은 뉴런입니다. 뉴런은 단순히 입력(변수) 집합을 받아 선형 및 비선형 변환을 적용하고 수학적 다변수 함수처럼 출력을 생성하는 수학 도구입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_0.png\" /\u003e\n\n첫 번째 레이어의 뉴런들에 대한 입력은 원래 세트의 데이터입니다. 각 샘플은 네트워크에 의해 독립적으로 동일한 방식으로 처리됩니다.\n\n네트워크의 기본 구조는 다음과 같습니다: 각각의 뉴런으로 구성된 여러 레이어로, 각 레이어마다 독립적으로 구성됩니다. 첫 번째 레이어는 데이터 원본에서 공급받고, 마지막 레이어는 출력으로 공급하며, 중간 레이어는 이전 레이어에서 공급받고 다음 레이어로 이어집니다.\n\n일부 아키텍처는 레이어 간에 변형을 추가하거나 피드백 루프를 도입한 이 모델에 변형을 도입할 수 있습니다. 나중에 이에 대해 논의할 예정입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Neural Networks Basic Theory and Architecture Types 1](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_1.png)\n\n각 뉴런은 선형 부분과 비선형 부분으로 구성됩니다. 선형 부분은 표준 선형 방정식이며, 비선형 변환은 네트워크 및 층에 따라 다를 수 있습니다.\n\n구체적으로, 레이어는 입력 벡터(크기 n)로 구성되며, 이는 가중치 행렬(크기 nxm, 여기서 n은 입력의 크기이고 m은 레이어의 뉴런 수입니다)에 의해 곱해지고 결과는 크기 m인 다른 벡터로 반환됩니다. 이 결과는 자유 매개 변수 벡터에 추가됩니다. 전체 결과는 비선형 함수를 통해 전달되며, 이를 활성화 함수라고 합니다.이 프로세스의 출력은 다음 레이어로 전달됩니다.\n\n![Neural Networks Basic Theory and Architecture Types 2](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또는 구체적으로:\n\n![Neural Networks Basic Theory and Architecture Types](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_3.png)\n\n이 시스템이 얼마나 복잡해질 수 있는지를 보여주기 위해, 두 번째 층의 출력에서 수식이 어떻게 보일지 알아보겠습니다:\n\n![Neural Networks Basic Theory and Architecture Types](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n실제 출력(목표 변수)를 재현하는 최적의 가중치와 자유 매개변수 값을 찾는 것이 바로 이어지는 교육 과정의 전부적인 목적입니다. 원본 값과 예측된 값 사이의 차이를 측정하기 위해 손실 함수를 도입합니다. 이로써, 모든 신경망은 근본적으로 지도 회귀 문제로 전환됩니다.\n\n활성화 함수는 다양한 사용 사례를 충족하는 함수 세트에서 선택됩니다. 높은 수준에서 가장 많이 사용되는 것은 ReLU(0보다 큰 값을 유지하며 음수는 0으로 설정), Sigmoid 및 Tanh입니다.\n\n# 학습 과정:\n\n언급했듯이, 각 계층별로 많은 매개변수의 최적 값을 찾는 것이 목표입니다. 이를 위해 예측된 Y와 실제 Y 간의 관계를 나타내는 함수(손실 함수)가 선택됩니다. 최적화 문제와 마찬가지로 목표는 이 기능이 최소값일 때의 지점을 찾기 위해 고정됩니다. 일부 손실 함수의 예는 평균 제곱 오차, 제곱근 평균 제곱 오차, 평균 절대 오차 및 이진 교차 엔트로피가 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최적화 문제를 해결하는 데 사용된 알고리즘은 그래디언트 강하법의 변형으로, 이는 다변수 적용에서 함수의 최소값을 찾는 전형적인 미적분 문제의 수치 구현입니다. 알고리즘은 각 반복에서 함수의 그래디언트 값을 추정하고 다음과 같은 방식으로 매개변수를 업데이트합니다:\n\n![image](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_5.png)\n\n이 프로세스는 주어진 반복 횟수(에폭)만큼 반복되며, 각 실행에서 손실 값이 감소하는지 확인합니다.\n\n학습 속도는 미리 설정해야 하는 하이퍼파라미터입니다. 학습 속도에 대한 중요한 사항은 너무 높게 설정해서는 안 된다는 것입니다. 그렇지 않으면 손실 값이 진동을 시작하고 결코 함수의 최소값을 찾지 못할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Neural Networks Theory](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_6.png)\n\n일반적으로 매개변수는 무작위 변수를 사용하여 초기화됩니다. 이 변수는 가우시안 분포를 따릅니다. 모든 입력이 독립적이며 레이어에 무한 개수의 뉴런이 있는 이상적인 경우에는 출력과 훈련된 매개변수도 가우시안 분포를 형성합니다.\n\n이러한 분포는 매개변수, 변수 또는 출력으로 형성된 다변량 공간 상의 파형패킷으로 볼 수 있습니다. 이는 양자장론에서 자유 입자를 모델링하기 위해 사용되는 수학적 구조와 유사합니다. 양자장론에서 상호작용으로 나타나는 작은 편차가 있는 것과 같이, 신경망에서는 변수간의 종속성과 레이어 당 유한 개수의 뉴런 삽입에 의해 생성됩니다. 특히, 네트워크 구성원 간의 내부 또는 보이지 않는 구조에 의해 생성되는 이러한 편차는 시스템의 예측력의 원천입니다. 그러나 이러한 편차가 너무 커지면 시스템이 발산하여 혼돈스럽게 됩니다.\n\n양자장론과 마찬가지로, 자유(가우시안) 경우에서의 작은 편차로 인한 문제들에 대한 수학적 해석을 위해 섭동 이론을 사용할 수 있습니다. 물리적 입자간의 상호작용을 이해하는 데 사용되는 수학은 신경망의 내부 동작을 이해하는 데 활용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 접근 방식에 대해 더 읽고 싶다면 원본 논문을 참고할 수 있어요: [2307.03223] Neural Network Field Theories: Non-Gaussianity, Actions, and Locality (arxiv.org)\n\n다시 본론으로 돌아와서, 신경망을 설계할 때 결정되어야 할 여러 가지 결정 사항 또는 하이퍼파라미터가 있어요. 이들은 다음과 같아요:\n\n- 각 층의 입력과 출력 수, 단, 첫 번째 층의 입력은 변수의 수, 마지막 층의 출력은 해결할 문제의 성격에 따라 정해지며 각 내부 또는 숨겨진 층의 출력은 다음 층의 입력이에요.\n- 신경망의 층 수.\n- 각 층의 활성화 함수.\n- 손실 함수.\n- 기울기 알고리즘.\n- 학습률.\n- 아키텍처(다음 세그먼트에서 탐구할 사항)\n\n다음으로, 가장 일반적인 신경망 아키텍처 몇 가지, 각각의 고수준 설명, 및 샘플 사용 사례를 살펴볼게요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 신경망과 기계 학습 내부의 수학을 더 잘 이해하고 싶다면 Ian Goodfellow, Yoshua Bengio, 그리고 Aaron Courville의 책을 참고하실 수 있어요. 해당 책은 Deep Learning (deeplearningbook.org)에서 구할 수 있어요. 그리고, Goodfellow은 적대적 생성 신경망(Generative Adversarial Networks)의 발명과도 함께 언급되어 있어요.\n\n# 다중 계층 퍼셉트론 (MLP)\n\n이것은 신경망의 가장 기본적인 아키텍처이며, 각 계층이 이전 계층에 의존하는 선형 구조로 형성되어 있어요. 또한 변수들 사이의 특정한 관계를 고려하지 않아요. MLP는 예측 변수들이 서로 의존하지 않는 문제에서 유용하게 사용됩니다. 예를 들어, 나이, 연봉, 교육 또는 성별과 같은 요소로 구성된 데이터셋에 대해요.\n\n![Neural Networks Basic Theory and Architecture Types](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 코드 블록은 PyTorch 패키지를 사용하여 Python에서 다층 퍼셉트론을 정의한 샘플입니다:\n\n```python\nimport torch.nn as nn\n\nclass SimpleClassifier(nn.Module):\n    def __init__(self):\n        super(SimpleClassifier, self).__init__()\n#과적합을 줄이기 위해 드롭아웃 레이어를 도입합니다.\n#드롭아웃은 신경망에게 층 사이의 데이터를 무작위로 삭제하여 변동성을 도입하도록 지시합니다.\n        self.dropout = nn.Dropout(0.1)\n#레이어는 열의 두 배 정도로 시작하고 다음 레이어로 증가한 다음 다시 2로 감소하는 것을 권장합니다.\n#이 경우 응답은 이진입니다.\n        self.layers = nn.Sequential(\n            nn.Linear(input_size, 250),\n            nn.Linear(250, 500),\n            nn.Linear(500, 1000),\n            nn.Linear(1000, 1500),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(1500, 1500),\n            nn.Sigmoid(),\n            self.dropout,\n            nn.Linear(1500, 1500),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(1500, 1500),\n            nn.Sigmoid(),\n            self.dropout,\n            nn.Linear(1500, 1500),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(1500, 1500),\n            nn.Sigmoid(),\n            self.dropout,\n            nn.Linear(1500, 1500),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(1500, 500),\n            nn.Sigmoid(),\n            self.dropout,\n            nn.Linear(500, 500),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(500, 500),\n            nn.Sigmoid(),\n            self.dropout,\n#마지막 레이어는 응답 변수가 이진(0, 1)이기 때문에 2를 출력합니다.\n#다중 클래스 분류의 출력은 클래스 수와 같아야 합니다.\n            nn.Linear(500, 2),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\n#모델 정의\nmodel = SimpleClassifier()\n```\n\n이 모델을 사용한 전형적인 학습 루프는 다음 블록에서 나타납니다:\n\n```python\n#모델 로드\nmodel = SimpleClassifier()\nmodel.train()\n\n#학습 파라미터(사이클 수 및 학습률)입니다.\nnum_epochs = 100\nlearning_rate = 0.00001\n#과적합을 줄이기 위해\nregularization = 0.0000001\n\n#손실 함수\ncriterion = nn.CrossEntropyLoss()\n\n#기울기를 찾는 알고리즘\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=regularization)\n\n#이 코드는 학습 루프를 수행하는 동안 최상의 모델을 유지합니다.\nbest_model_wts = copy.deepcopy(model.state_dict())\nbest_acc = 0.0\nbest_f1 = 0.0\nbest_epoch = 0\nphases = ['train', 'val']\ntraining_curves = {}\nepoch_loss = 1\nepoch_f1 = 0\nepoch_acc = 0\n\n#데이터셋은 학습, 검증 및 테스트로 분할됩니다.\nfor phase in phases:\n    training_curves[phase+'_loss'] = []\n    training_curves[phase+'_acc'] = []\n    training_curves[phase+'_f1'] = []\n\n#이것은 학습 루프입니다.\nfor epoch in range(num_epochs):\n    print(f'\\n에포크 {epoch+1}/{num_epochs}')\n    print('-' * 10)\n    for phase in phases:\n        if phase == 'train':\n            model.train()\n        else:\n            model.eval()\n        running_loss = 0.0\n        running_corrects = 0\n        running_fp = 0\n        running_tp = 0\n        running_tn = 0\n        running_fn = 0\n        #데이터 반복\n        for inputs, labels in dataloaders[phase]:\n            inputs = inputs.view(inputs.shape[0], -1)\n            inputs = inputs\n            labels = labels\n\n            #매개변수의 기울기를 0으로 설정\n            optimizer.zero_grad()\n\n            #순방향 패스 (위의 차트 참조)\n            with torch.set_grad_enabled(phase == 'train'):\n                outputs = model(inputs)\n                _, predictions = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n\n                #역방향 패스 (학습 중에만)\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n\n                #통계. f1 메트릭을 사용합니다.\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(predictions == labels.data)\n                running_fp += torch.sum((predictions != labels.data) \u0026 (predictions \u003e= 0.5))\n                running_tp += torch.sum((predictions == labels.data) \u0026 (predictions \u003e= 0.5))\n                running_fn += torch.sum((predictions != labels.data) \u0026 (predictions \u003c 0.5))\n                running_tn += torch.sum((predictions == labels.data) \u0026 (predictions \u003c 0.5))\n                print(f'에포크 {epoch+1}, {phase:5} 손실: {epoch_loss:.7f} F1: {epoch_f1:.7f} 정확도: {epoch_acc:.7f} 부분 손실: {loss.item():.7f} 최상의 f1: {best_f1:.7f}')\n\n        #손실, 정확도 및 f1 메트릭 계산\n        epoch_loss = running_loss / dataset_sizes[phase]\n        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n        epoch_f1 = (2 * running_tp.double()) / (2 * running_tp.double() + running_fp.double() + running_fn.double() + 0.0000000000000000000001)\n        training_curves[phase+'_loss'].append(epoch_loss)\n        training_curves[phase+'_acc'].append(epoch_acc)\n        training_curves[phase+'_f1'].append(epoch_f1)\n\n        print(f'에포크 {epoch+1}, {phase:5} 손실: {epoch_loss:.7f} F1: {epoch_f1:.7f} 정확도: {epoch_acc:.7f} 최상의 f1: {best_f1:.7f}')\n\n        if phase == 'val' and epoch_f1 \u003e= best_f1:\n            best_epoch = epoch\n            best_acc = epoch_acc\n            best_f1 = epoch_f1\n            best_model_wts = copy.deepcopy(model.state_dict())\n\nprint(f'최상의 val F1: {best_f1:5f}, 최상의 val 정확도: {best_acc:5f}, 에포크 {best_epoch}')\n\n#최상의 모델 가중치로드\nmodel.load_state_dict(best_model_wts)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래의 글에서는 실제 데이터를 사용하여 MLP를 구현한 예시를 확인할 수 있습니다: [Neural Networks와 Pytorch를 사용하여 자동 복구 실패 예측하기 (저자: Greg Postalian-Yrausquin | 2024년 6월 | Towards AI (medium.com)](https://medium.com)\n\n더 많은 정보는 위키피디아 페이지에서 찾을 수 있습니다: [다층 퍼셉트론 (Multilayer perceptron) — Wikipedia](https://en.wikipedia.org/wiki/Multilayer_perceptron)\n\n# 합성곱 신경망 (CNN):\n\n전형적인 다층 퍼셉트론은 입력이 필드인 경우 성능이 좋지 않습니다. 여기서 필드란 점들 간의 관계(함수, 연속성을 통해)가 있는 구조를 말합니다. 예를 들어, 금속 판의 온도는 한 지점에 열원이 연결되어 있는 경우 열원에서 더 멀리 있는 위치로 갈수록 그래디언트를 따를 것입니다. 이 2차원 예시에서 표면의 온도는 그리드(행렬)로 나타낼 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_8.png)\n\n이러한 구조들은 3D일 수도 있습니다 (여러 개가 서로 위에 쌓인 것을 상상해보세요) 또는 우리가 원하는 대로 복잡할 수 있습니다. MLPs는 훈련 중에 데이터의 내부 구조를 잃어버리므로 이를 모델링하는 데 좋지 않습니다. 그와는 반대로 CNNs는 원래의 관계를 보존합니다.\n\nPython에서는 이미지가 행렬로 저장됩니다. 여기서 행과 열은 위치를 나타내고 숫자는 강도를 측정하는 값입니다. 컬러 이미지의 경우 각 이미지에 대해 RGB 색상 인코딩 형식에 대한 값을 저장하는 3개의 행렬이 사용됩니다. 수학에서 이러한 다차원 행렬은 Tensor라고 불리며 벡터 함수로도 볼 수 있습니다 (출력이 벡터의 모양으로 나오는 것), 이 경우 출력 벡터의 좌표는 RGB 색상값입니다.\n\n이러한 이유로 CNN은 이미지 및 비디오 데이터를 모델링하는 데 널리 사용됩니다. CNN의 아이콘은 이미지 분류입니다. 이 기사에서는 그 목적으로 CNN 사용 예제를 볼 수 있습니다: Convolutional Neural Networks in PyTorch: Image Classification | by Greg Postalian-Yrausquin | Jun, 2024 | Towards AI (medium.com).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n합성곱 신경망은 네트워크 내에 하나 이상의 합성 계층이 존재하는 것으로 정의됩니다. 이들은 데이터 내부에서 창 또는 행렬(커널)을 슬라이딩하여 원소별 곱셈을 수행하고 커널 내부 값의 합을 구하는 수학 연산입니다. 패딩을 도입하여 원래 데이터 매트릭스 크기의 축소를 고려할 수 있습니다.\n\n![CNN Architecture](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_9.png)\n\nCNN에는 다른 종류의 계층도 소개되는데, 예를 들면: Max pool (지도의 일부분의 최대값을 얻어 데이터 크기를 줄임), flatten (데이터 매트릭스를 벡터로 변환하여 네트워크 끝에 사용되거나 표준 네트워크로 계속해서 훈련) 및 unflatten (이전 과정을 역으로 수행).\n\n다음 샘플 코드는 PyTorch에서 CNN 클래스의 정의입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom torch.nn.modules.flatten import Flatten\nclass CNNClassifier(nn.Module):\n    def __init__(self):\n        super(CNNClassifier, self).__init__()\n        self.dropout = nn.Dropout(0.05)\n        self.pipeline = nn.Sequential(\n            #in channels is 1, because the input is grayscale\n            nn.Conv2d(in_channels = 1, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 5, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            #dropout to introduce randomness and reduce overfitting\n            self.dropout,\n            #reduce and flat the tensor before applying the flat layers\n            nn.MaxPool2d(kernel_size = 2, stride = 2),\n            nn.Flatten(),\n            nn.Linear(500, 50),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(50, 50),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(50, 10),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(10, 10),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(10, 5),\n        )\n\n    def forward(self, x):\n        return self.pipeline(x)\n\nmodel = CNNClassifier()\n```\n\nCNN에 대해 더 자세히 알아보기 좋은 정보를 찾는 것을 시작하는 데 좋은 곳인 Wikipedia의 CNN에 대한 항목을 찾았어요: [Convolutional neural network — Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network)\n\n# 오토인코더\n\n아키텍처의 하위 클래스로 오토인코더가 있습니다. 입력과 출력의 수가 동일한 특정 구성으로 상상할 수 있습니다. 모델은 입력된 데이터를 재현하는 방법을 학습하도록 구성되어 있으며 한 개 이상의 숨겨진 레이어를 통해 통과합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델은 두 부분으로 설계되어 있습니다. 입력을 다른 표현으로 변환하는 Encoder와 이 표현을 기반으로 입력의 버전을 재구성하는 Decoder입니다. 아이디어는 재구성이 초기 데이터와 가능한 한 유사해야 한다는 것입니다.\n\n이 네트워크에서 목표는 동일한 입력 데이터이기 때문에 이들은 사실상 감독되지 않은 학습 방법입니다. 예를 들어 Autoencoder 아키텍처는 생성적 AI 작업의 일부로 사용됩니다.\n\n자연어 처리(NLP)에서 오토인코더는 단어 또는 문장의 임베딩(표현)을 생성하는 데 사용됩니다. 이 텍스트의 숫자 표현은 그 후 분류, 거리 계산 등과 같은 하향 작업에서 사용됩니다.\n\nNLP에서 오토인코더를 사용하는 한 가지 훌륭한 방법은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Neural Network Example](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_10.png)\n\n이 문서에서는 오토인코더의 정의 예시를 찾을 수 있습니다: Neural networks: encoder-decoder example (autoencoder) | 작성자 Greg Postalian-Yrausquin | 날짜 2024년 6월 | Medium. 여기서 모델이 이미지를 재구성하는 데 사용됩니다.\n\n```js\n# 훈련 이미지의 채널 수. 컬러 이미지의 경우 3개입니다\nnc = 3\n\n# 표현의 크기\nnr = 1000\n\n# 디코더의 시작점의 크기\nnz = 50\n\nclass Encdec(nn.Module):\n    def __init__(self, nc, nz, nr):\n        super(Encdec, self).__init__()\n# 이것이 인코더입니다\n        self.encoder = nn.Sequential(\n            nn.Conv2d(in_channels = nc, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 1, kernel_size = 5, stride = 1, padding=1),\n            nn.Flatten(),\n            nn.Linear(2916, 3000),\n            nn.ReLU(),\n            nn.Linear(3000, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, nr),\n         )\n# 이것이 디코더입니다\n        self.decoder = nn.Sequential(\n            nn.Linear(nr, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, 500),\n            nn.ReLU(),\n            nn.Linear(500, nz*64*64),\n            nn.Unflatten(1, torch.Size([nz, 64, 64])),\n            nn.Conv2d(in_channels = nz, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = nc, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(10092, 2000),\n            nn.ReLU(),\n            nn.Linear(2000, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, 500),\n            nn.ReLU(),\n            nn.Linear(500, nc*64*64),\n            nn.Unflatten(1, torch.Size([nc, 64, 64])),\n            nn.Tanh()\n         )\n\n    def encode(self, x):\n        return self.encoder(x)\n\n    def decode(self, x):\n        return self.decoder(x)\n\n    def forward(self, input):\n        return self.decoder(self.encoder(input))\n\nnetEncDec = Encdec(nc, nz, nr)\n```\n\n자세한 내용은 위키피디아에서 오토인코더 아키텍처에 대해 더 알아보기 시작점으로 참조할 수 있습니다: [오토인코더 - 위키백과](https://ko.wikipedia.org/wiki/%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인코더-디코더 메커니즘의 일반적이고 잘 알려진 구현은 트랜스포머 입니다. 이 아키텍처는 2017년 구글의 데이터 과학자들이 발표한 “Attention is all you need” [1706.03762] 논문에서 소개되었습니다. 트랜스포머는 NLP에서 널리 사용되며, 입력 및 출력 집합에 대한 임베딩 생성부터 시작하여 여러 단계로 구성됩니다. 이 집합 모두에 대해 위치 정보를 유지할 수 있도록 처리된 후에, 초기 오토인코더에는 포함되어 있지 않은 단계가 포함됩니다. 이는 반복의 오버헤드 없이 RNN과 동일한 이점을 제공합니다. 그 다음 데이터는 인코딩 프로세스(어텐션 스택)를 거치고, 디코딩 단계(두 번째 어텐션 스택)에서 출력과 비교됩니다. 마지막 단계는 소프트맥스 변환을 적용하는 것입니다.\n\n![트랜스포머 아키텍처](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_11.png)\n\n트랜스포머 아키텍처: 원본 논문 \"Attention is all you need\"에서 가져온 다이어그램입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n전이 학습 패러다임의 매우 흥미로운 그리고 유용한 구현 중 하나는 Google이 만든 BERT(Bidirectional Encoder Representations for Transformers)입니다. 영어 처리를 위해 트랜스포머를 처음부터 훈련하는 것은 거대한 작업이 될 수 있지만 다행히도 다양한 용도에 맞게 사전 훈련된 모델을 다운로드하고 적용할 수 있습니다 (이러한 모델을 다운로드하고 적용하는 방법은 Huggingface 페이지를 참조하세요): 모델 다운로드 (huggingface.co)\n\n# 순환 신경망:\n\nRNN은 신경망의 비선형 시도로 간주될 수 있습니다. RNN에서는 한 레이어가 자신에게 영향을 미칠 수 있습니다 (역행 효과가 있습니다). 이 작용은 시퀀스 형식으로 된 데이터를 모델링하는 데 이상적이라고 할 수 있습니다. 이러한 데이터의 가장 좋은 예는 텍스트 스트림이며, 그 이유로 NLP에서 가장 효율적인 트랜스포머가 도입될 때까지 주로 사용되었습니다. RNN은 음성 및 필기 인식에도 구현되어 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_12.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRNN(순환 신경망)은 계산적으로 요구가 높을 수 있는 것 외에, 비선형성에 의해 확대되는 전파 오류와 실제로 이전 단계의 매우 짧은 메모리를 유지하는 사라져버리는 그래디언트와 같은 다른 문제가 있습니다. 이러한 문제를 해결하기 위해 LSTM(Long-Short Term Memory) 및 GRU(Gated Recurrent Units)와 같은 RNN 아키텍처의 더 복잡한 파생물이 소개되었습니다.\n\nRNN의 응용 예는 다음 글에서 소개됩니다: RNN: PyTorch에서 Sentiment Analysis를 위한 기본 순환 신경망 | Greg Postalian-Yrausquin 저 | 2024년 6월 | Towards AI (medium.com)\n\nPyTorch에서 이 네트워크의 정의는 다음과 같습니다:\n\n```python\n# 신경망의 정의입니다. 보시다시피 RNN 정의 하나만 있습니다.\n# 2개의 레이어와 하나의 선형 레이어가 포함됩니다.\n# 오버피팅을 방지하기 위해 드롭아웃 및 정규화가 도입되었습니다\n\nclass RNNClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNNClassifier, self).__init__()\n        self.hidden_size = hidden_size\n        self.RNN = nn.RNN(input_size, hidden_size, num_layers=2, dropout=0.2)\n        self.fc = nn.Linear(hidden_size, output_size)\n        pass\n\n    def forward(self, input):\n        output, hn = self.RNN(input)\n        output = self.fc(output)\n        return output, hn\n\nmodel = RNNClassifier(insize, 8, 2)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 생성 적대 신경망 (Generative Adversarial Networks, GAN):\n\n이것은 MLP, RNN 또는 CNN들의 조합에서 형성될 수 있는 또 다른 복합 아키텍처입니다. 2014년 Ian Goodfellow와 그의 동료들에 의해 작성되었습니다 (원본 논문은 Generative Adversarial Nets (nips.cc)에서 확인할 수 있으며 [1701.00160] NIPS 2016 Tutorial: Generative Adversarial Networks (arxiv.org)에서 튜토리얼을 참조할 수 있습니다). GAN은 두 가지 다른 모델이 훈련되는 매우 똑똑한 신경망 응용 프로그램으로, 하나는 원래 데이터셋을 기반으로 샘플을 생성하는 것을 목표로하고 다른 하나는 이 첫 번째 모델에 대항하여 샘플이 실제인지 가짜인지를 추측합니다.\n\nGAN은 생성적 AI 작업(모델을 기반으로 실제 데이터를 생성하는 것)에 사용됩니다. 텍스트, 이미지 또는 비디오 생성 등이 해당될 수 있습니다. 자세한 설명은 원본 샘플을 기반으로 새로운 객체를 생성해야 하는 생성자(Generator) 네트워크 및 인공적으로 생성된 샘플과 실제 샘플을 구별하도록 훈련된 구분자(Discriminator)를 포함하고 있습니다. 학습의 여러 반복 후에 구분자가 실제 데이터와 가짜 데이터를 구분하는 데 어려움을 겪도록 함으로써 신뢰할 수 있는 제품 생성을 보장합니다.\n\n이것은 PyTorch에서 두 적대적 신경망에 대한 클래스 정의 샘플입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 트레이닝 이미지 내의 채널 수. 컬러 이미지의 경우, 채널 수는 3입니다.\nnc = 3\n\n# z 잠재 벡터의 크기 (즉, 생성기 입력의 크기)\nnz = 100\n\n# 생성기의 특징 맵 크기\nngf = 64\n\n# 판별자의 특징 맵 크기\nndf = 64\n\n# 이것은 진짜와 가짜 이미지를 분리하려는 작업을 수행하는 판별자 네트워크입니다.\nclass Discriminator(nn.Module):\n    def __init__(self, nc, ndf):\n        super(Discriminator, self).__init__()\n        self.pipeline = nn.Sequential(\n# nc는 3이며, 입력은 텐서 3x64x64(64x64의 컬러 이미지이며, 각 컬러 이미지에는 3개의 텐서가 필요)입니다.\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2),\n# 출력은 크기가 1인 벡터입니다.\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.pipeline(input)\n\n\nclass Generator(nn.Module):\n    def __init__(self, nc, nz, ngf):\n        super(Generator, self).__init__()\n        self.pipeline = nn.Sequential(\n# 생성기의 입력은 무작위 생성된 이미지입니다. 이 경우 채널 수가 100이므로 nz는 100입니다.\n            nn.ConvTranspose2d(nz, ngf * 16, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 16),\n            nn.ReLU(),\n            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(),\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(),\n            nn.ConvTranspose2d(ngf * 4, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(),\n# 출력은 트레이닝 이미지와 동일한 차원의 이미지입니다. 따라서 출력은 크기가 nc여야 합니다.\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        return self.pipeline(input)\n\n\n저는 다음 기사에서 이미지 생성을 위한 GAN을 구현했습니다: GAN: training a Generative Adversarial Network for image generation | by Greg Postalian-Yrausquin | Jun, 2024 | Medium\n\n이것들은 머신 러닝을 위한 신경망의 기본입니다. 하지만 모든 아키텍처에 대한 완전한 설명은 아닙니다. 이 주제는 아주 거대하고 매혹적이며, 새로운 기술과 알고리즘이 지속적으로 등장하고 있는 폭발적인 성장을 이루고 있습니다. 이러한 많은 것들은 이 문서에서 설명된 아키텍처의 수정이나 결합입니다.\n","ogImage":{"url":"/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_0.png"},"coverImage":"/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_0.png","tag":["Tech"],"readingTime":22}],"page":"43","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"43"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>