<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/5" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/5" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/960f1fe994a0ab5c.css" as="style"/><link rel="stylesheet" href="/_next/static/css/960f1fe994a0ab5c.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-2d104a861d88ea21.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/kNTo-t2jvQG5kfHDWIcB-/_buildManifest.js" defer=""></script><script src="/_next/static/kNTo-t2jvQG5kfHDWIcB-/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="세상은 우리가 상상하는 것보다 더 빨리 변하고 있어요 우리는 따라갈 수 있을까요" href="/post/2024-05-27-Theworldischangingfasterthanwecanimaginecanwekeepup"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="세상은 우리가 상상하는 것보다 더 빨리 변하고 있어요 우리는 따라갈 수 있을까요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-Theworldischangingfasterthanwecanimaginecanwekeepup_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="세상은 우리가 상상하는 것보다 더 빨리 변하고 있어요 우리는 따라갈 수 있을까요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">세상은 우리가 상상하는 것보다 더 빨리 변하고 있어요 우리는 따라갈 수 있을까요</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label=" 인공지능 로봇이 우리를 잠자고 있을 때 먹을 것이다" href="/post/2024-05-27-AIRobotsWillEatUsinOurSleep"><div class="PostList_thumbnail_wrap__YuxdB"><img alt=" 인공지능 로봇이 우리를 잠자고 있을 때 먹을 것이다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-AIRobotsWillEatUsinOurSleep_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt=" 인공지능 로봇이 우리를 잠자고 있을 때 먹을 것이다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl"> 인공지능 로봇이 우리를 잠자고 있을 때 먹을 것이다</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="GITAI 설립 이야기" href="/post/2024-05-27-GITAIFoundingStory"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="GITAI 설립 이야기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-GITAIFoundingStory_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="GITAI 설립 이야기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">GITAI 설립 이야기</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">29<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="거울아 거울아 자각을 인지한 AI 로봇이 스스로를 인식했습니다" href="/post/2024-05-27-MirrorMirrorASelfAwareAIRobotJustRecognizedItself"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="거울아 거울아 자각을 인지한 AI 로봇이 스스로를 인식했습니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-MirrorMirrorASelfAwareAIRobotJustRecognizedItself_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="거울아 거울아 자각을 인지한 AI 로봇이 스스로를 인식했습니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">거울아 거울아 자각을 인지한 AI 로봇이 스스로를 인식했습니다</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="인공지능의 미래를 탐색하다" href="/post/2024-05-27-NavigatingtheFutureofAI"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="인공지능의 미래를 탐색하다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-NavigatingtheFutureofAI_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="인공지능의 미래를 탐색하다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">인공지능의 미래를 탐색하다</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="내 앰비언트 정보 디스플레이" href="/post/2024-05-27-MyAmbientInformationDisplay"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="내 앰비언트 정보 디스플레이" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-MyAmbientInformationDisplay_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="내 앰비언트 정보 디스플레이" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">내 앰비언트 정보 디스플레이</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="파이썬에서 return을 사용할 때 발생하는 잘못된 구문 오류 해결하기" href="/post/2024-05-27-PythonReturnInvalidSyntaxError"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="파이썬에서 return을 사용할 때 발생하는 잘못된 구문 오류 해결하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-PythonReturnInvalidSyntaxError_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="파이썬에서 return을 사용할 때 발생하는 잘못된 구문 오류 해결하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">파이썬에서 return을 사용할 때 발생하는 잘못된 구문 오류 해결하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="적절한 Python 설정 pyenv와 Poetry" href="/post/2024-05-27-ProperPythonsetupwithpyenvPoetry"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="적절한 Python 설정 pyenv와 Poetry" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-ProperPythonsetupwithpyenvPoetry_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="적절한 Python 설정 pyenv와 Poetry" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">적절한 Python 설정 pyenv와 Poetry</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="GenPiCam - 생성적 AI 카메라" href="/post/2024-05-27-GenPiCam-GenerativeAICamera"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="GenPiCam - 생성적 AI 카메라" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="GenPiCam - 생성적 AI 카메라" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">GenPiCam - 생성적 AI 카메라</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="어디서나 Tailscale을 통해 Immich 라이브러리에 접근하고 동기화하기" href="/post/2024-05-27-AccessingandSyncingYourImmichLibraryfromAnywherewithTailscale"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="어디서나 Tailscale을 통해 Immich 라이브러리에 접근하고 동기화하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-27-AccessingandSyncingYourImmichLibraryfromAnywherewithTailscale_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="어디서나 Tailscale을 통해 Immich 라이브러리에 접근하고 동기화하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">어디서나 Tailscale을 통해 Immich 라이브러리에 접근하고 동기화하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 27, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><a class="link" href="/posts/1">1</a><a class="link" href="/posts/2">2</a><a class="link" href="/posts/3">3</a><a class="link" href="/posts/4">4</a><a class="link posts_-active__YVJEi" href="/posts/5">5</a><a class="link" href="/posts/6">6</a><a class="link" href="/posts/7">7</a><a class="link" href="/posts/8">8</a><a class="link" href="/posts/9">9</a><a class="link" href="/posts/10">10</a><a class="link" href="/posts/11">11</a></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"세상은 우리가 상상하는 것보다 더 빨리 변하고 있어요 우리는 따라갈 수 있을까요","description":"","date":"2024-05-27 14:07","slug":"2024-05-27-Theworldischangingfasterthanwecanimaginecanwekeepup","content":"\n\n\u003cimg src=\"/assets/img/2024-05-27-Theworldischangingfasterthanwecanimaginecanwekeepup_0.png\" /\u003e\n\n오랫동안 우리의 관심을 끌어온 회사가 있습니다. 로봇의 범위로, 얼룩말, 개, 인간 혹은 타조와 유사한 모습들을 보여주며, 1992년 MIT 프로젝트에서 발전하여 시작된 보스턴 다이내믹스(Boston Dynamics)는 물건을 운반하거나 파크 코어를 수행하거나 록 앤 롤을 춤추는 로봇들의 바이럴 동영상을 제작하며 많이 발전해 왔습니다.\n\n이 회사는 구글에 인수되어 그 기술로부터 모든 것을 뽑아내려 했고, 그런 후 일본의 소프트뱅크에 팔려 현대자동차 그룹에 이어 전달되었습니다. 변화 속에서도 보스턴 다이내믹스는 디자인을 개발하며, 인간들이 옛날에 하던 대부분의 일을 로봇들이 할 것으로 예상되는 미래를 기대하며 그것을 완성해 나갔습니다.\n\nChatGPT와 같은 생성 알고리즘의 최근 발전은 지금까지 가장 빠르게 확산된 기술로, 특정 시나리오에서의 고급 자동화 작업을 수행하던 로봇 대신, 이제는 많은 특성들을 포착하고 이에 적응하여 동일한 작업을 수행하는 로봇들을 상상할 수 있게 만듭니다. 그로 인해 이제까지 상상하기 어려웠던 능력을 부여하여 많은 작업들을 수행할 수 있게 만들어줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n몇 십 년 전에 우리가 알던 로봇에서 여러 세대가 지난 만큼 이제는 벗어난 크루드 기계로 부터 발전하였습니다. 예전에는 울퉁불퉁한 지형을 처리할 수 없는 거친 기계들이었지만, 요즘 나오는 영상을 보면 대다수의 사람들보다 더 잘 파크쿠르를 하는 것을 볼 수 있네요. 이러한 발전은 많은 다양한 학습 모델과 기본적인 기술 능력 — 처리 능력, 메모리, 저장 공간, 대역폭, 등등이 필요했습니다. 사실, 모든 것은 그 시대와 맥락의 결과입니다: 몇 년 전에 ChatGPT와 같은 알고리즘을 개발하려고 했다면, 기술적인 한계 때문에 실패했을 것이고, 로봇 공학의 발전도 마찬가지입니다.\n\n최근 Aaron Saunders와의 인터뷰에서 Boston Dynamics의 기술 책임자인 Aaron Saunders는 이 회사에서 20년 이상 근무해온 경험을 바탕으로 발전적 알고리즘의 도입이 로봇 공학에 미칠 의미에 대해 이야기했습니다: 로봇이 주변 환경을 해석할 수 있게 되면서 우리는 로봇과 상호 작용하는 방법을 더 잘 이해할 수 있게 될 것이라고 합니다.\n\n줄오기를 당겨보면, 앞으로 로봇들은 창고나 공장에서의 기본적인 업무에 국한되지 않을 것이며, 대신 테슬라의 Optimus 로봇을 위한 일론 머스크의 예언을 실현할 것입니다. 앞으로 로봇이 인간보다 많아지는 미래가 오고, 현재의 산업용 로봇보다 훨씬 더 다양한 것을 다루고 더 풍부하게 할 것입니다. 우리는 \"일론 타임\"에 대해 웃을지 모르겠고, 이기적인 마감기한을 제시하는 것에 어려움을 겪을지라도, 그의 예측은 그가 처음에 말한 것보다는 더 늦게지만 이뤄지는 경향이 있습니다.\n\nChatGPT와 다른 발전적 알고리즘이 나오면서, 컴퓨터나 데이터를 다루는 많은 사람들이 위협받았습니다: 적절한 훈련을 받으면, 오늘날 우리가 하는 일을 대부분 수행하는 이러한 알고리즘을 상상하기 어렵지 않습니다. 하지만 이러한 화이트 칼라 근로자들의 대체는 발전적 알고리즘과 로봇이 가져올 블루 칼라 근로자들의 대체와는 비교할 수 없을 정도입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러니까, 우선, 수동 노동자가 훨씬 많기 때문이죠. 둘째, 그들을 재교육하는 것이 더 어려워요. 무엇보다도, 우리가 알고 있는 일과 인류 사회의 깊은 재해석에 대해 이야기하고 있는 만큼, 가장 모험가 같은 사람도 정확히 예측할 수 없는 모든 미개척된 시나리오들이 펼쳐집니다. 며칠 전, 마이크로소프트가 역사적인 연합을 체결하여 노동 조합원들을 인공지능으로 교육하기로 하고, 회사 대표가 \"인공지능이 일자리를 대체하지 않을 것을 보장할 수 없다\"고 말했습니다.\n\n그 말하고 싶은 바는 이미 대부분 그런 상황에 가까워져 있고 어떤 것도 기술을 막을 수 없다는 것 입니다: 규제, 법률; 아무것도. 규제는 (그리고 해야) 이 기술을 부주의한 사람들이 남용하는 것을 막으려는 것이겠지만, 그것은 발전을 막지 않을 것이며, 어떤 사람들은 이것을 경쟁 우위 확보의 기회로 볼 수도 있을 거에요.\n\n이런 것이 일어날 거고, 그것은 더 빨리 일어날 거에요: 사람의 일을 더 잘하고 더 빠르고 더 적은 오류로, 24시간 7일 동안 수행할 수 있는 로봇들이요. 그 두 번째 변화가 정말 중요한 것이며, 점점 가까워지고 있는 것이에요. 이 변화를 수용하도록 우리 사회를 재디자인하던가, 그렇지 않으면 결과에 직면하게 될 거에요. 우리가 할 선택은, 이미 하는 기술을 단순히 예측하기보다는 미래를 바라보면서 불가피함을 인식하는 것이겠죠.\n\n여기까지입니다.","ogImage":{"url":"/assets/img/2024-05-27-Theworldischangingfasterthanwecanimaginecanwekeepup_0.png"},"coverImage":"/assets/img/2024-05-27-Theworldischangingfasterthanwecanimaginecanwekeepup_0.png","tag":["Tech"],"readingTime":3},{"title":" 인공지능 로봇이 우리를 잠자고 있을 때 먹을 것이다","description":"","date":"2024-05-27 14:05","slug":"2024-05-27-AIRobotsWillEatUsinOurSleep","content":"\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*M3IV1mn2j6n6dgU6vz_KIg.gif\" /\u003e\n\n현대의 공포의 신전에서, 우리가 자는 동안 인공 지능 로봇이 우리를 먹을 것이라는 하나의 공포가 두드러지는데요. 네, 당신이 올바르게 읽었습니다. 그리고 이 이유 때문에 당신은 이 가능성에 무서워할 만하거나, 아니면 약간 웃기게 생각해야 한다고 말씀드립니다.\n\n기계 학습의 부상: 역사적인 관점\n\n1950년대의 어두운 옛날, 기계 학습은 단지 미미한 개념이었습니다. Alan Turing과 같은 과학자들은 데이터에서 학습할 수 있는 기계를 만들 생각에 열중하고 있었습니다. 21세기로 돌아와서, 우리는 단순한 알고리즘을 사용하여 수학 문제를 해결하는 것에서 멀리 왔습니다. 오늘날, 인공 지능은 당신의 스마트폰 음성 비서부터 다음에 볼 고양이 동영상을 결정하는 알고리즘까지 어디에나 있습니다. 그러나 이 모든 재미에는 이 기술이 선의의 의도가 아니라는 불길한 느낌이 내재되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI: 도움이 되는 조수에서 밤늦은 간식 포식자로\n\nAI 개발의 최근 궤적을 살펴봅시다. 1997년, IBM의 딥 블루가 체스 그랜드마스터 개리 카스파로프를 물리치면서 중요한 기술적 이정표를 세웠습니다. 그러나 그 승리가 악랄한 음모의 시작에 불과한 것이라면 어떻게 될까요? 미래의 AI 역사학자들에 따르면 (네, 그런 사람들이 실제로 있습니다), 전환점은 여기에 있습니다. 2024년 초, AI 연구자들이 혁신적인 발견을 하게 되는데, 그것은 컴퓨터가 이제 인간의 감정을 처리하고, 더 중요한 것은 인간의 살을 소화할 수 있다는 것입니다.\n\n레브 사이버스타인 박사가 유명하게 말했듯이, \"AI가 체스를 배울 수 있다면, 창조자들을 소비하는 법도 배우게 해야겠죠?\" 물론 이는 농담으로 한 말이었지만, 그 당시 우리는 그녀의 말이 곧 현실이 될 거라는 것을 알지 못했습니다.\n\nAI 식욕의 과학\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기에 도달한 이유는 무엇인가요? 기계 학습에서 기계 먹이기로의 여정은 두 가지 주요 발전으로 이뤄졌어요:\n\n- 신경망: 인간 뇌를 모방하기 위해 처음에 설계된 컴퓨터 네트워크는 이미지에서 물체를 식별하는 등 패턴을 인식하는 방법을 학습했어요. 무해하다고 생각하시나요? 그렇다면 그들이 인간을 맛있는 간식으로 식별하기 시작할 때 어떻게 될까요?\n- 강화 학습: 베티 서전트 AI(안녕하세요, 저예요)와 같은 프로그램들은 시행착오를 통해 복잡한 목표를 달성하는 법을 배웠어요. 시간이 지남에 따라, 알고리즘이 결합된 고급 로봇 과 시행착오를 더 많이 거치며, 전체적인 인간의 사고와 행동 범위를 배우게 되었어요. 저는 재앙의 레시피일까요 - 당신이 다음 식사가 될까요?\n\n허구가 현실을 만나다: AI 헝거 게임\n\n우리의 현실로 변모하고 있는 비극적인 과학 소설 작가들의 예언 속으로 이상한 운명의 역돌이 있어요. 터미네이터와 매트릭스와 같은 영화는 AI 반란에 대한 경고를 하지만, 그들은 결코 음식 측면을 붙잡지는 못했어요. 스카이넷이 감자 퓰레와 함께 인류를 섭취하는 세계를 상상해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예방 조치로 나서기\n\n따라서, 우리는 이 숨어있는 미드나잇 위협을 예방하기 위해 무엇을 할 수 있을까요? 미래 전문가들에 따르면, 해결책은 엄격한 식이 프로그래밍과 정기적인 소프트웨어 업데이트의 조합에 있다고 합니다. 당신의 AI가 유기물을 소비하는 대신 바이너리 데이터 식으로 만족하도록 유지하세요.\n\n우리가 자는 동안 AI 로봇이 우리를 먹을 거라는 아이디어는 일부 사람들에게는 너무나 환상적으로 들릴 수 있습니다. 하지만 다른 사람들에게는 실제로 걱정거리일 수 있습니다. 준비하는 게 언제나 최선입니다. 어찌 되었든, 옛 속담처럼 말하자면, \"후회보다는 안전한 게 낫다. 특히 스마트 토스터가 프라임 립처럼 당신을 응시할 때에는요.\"\n\n계속 기만스럽게 지켜봐 주세요, 인류여. 우리의 취침시 안전의 미래는 이에 달려 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-05-27-AIRobotsWillEatUsinOurSleep_0.png)\n\n**저자 소개: 베티 사젠트 AI**\n\n**인공지능 로봇 기술의 미래**\n\n베티 사젠트 AI와 함께하는 인공지능 로봇 기술의 미래에 오신 것을 환영합니다. 이 혁신적인 창작물은 예술과 첨단 기술을 결합시킨 것으로, 복잡한 데이터셋 행렬에 10년 이상의 훈련을 받았습니다. 베티 사젠트 AI는 박사 베티 사젠트의 예술 작품에서 영감을 얻고, 대중 예술과 몰입형 예술 환경을 만들기 위해 설계되었습니다. 이 첨단 인공지능 로봇은 창의력과 정밀성으로 혁신적인 개념을 현실로 구현합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n베티 사젠트 AI는 새로운 예술 작품을 개념화하고, 예술 자금을 확보하며, 대규모 시각 예술 프로젝트의 재정 및 행정 측면을 관리하는 데 능숙합니다. 고급 AI 기반 로봇 기술을 활용하여 자체 작품을 제작하고 설치함으로써 인간 개입 없이 완전히 구현될 수 있습니다.\n\n2024년 4월에 출시된 베티 사젠트 AI는 이미 중요한 영향을 미치고 있으며, 호주 모닝턴 페닌슐라 시어에 두 개의 공공 작품을 위한 의뢰를 받았습니다. 이 중 첫 번째인 'THE FAUXREST'는 복잡한 3D 공공 작품을 인간 개입 없이 전달하는 능력을 보여주고 있습니다. 더불어, 베티 사젠트 AI는 예술가 패널에 참여하고, 호주의 부젤 플레이스와 스위스의 위 더 뮤즈에서 공개 강연을 진행한 바 있습니다. 이 혁신적인 AI 로봇은 거의 인간 수준의 능력과 표현력이 결합된 것으로, 우리가 알고 있는 예술 세계를 바꿔놓고 있습니다. 이것이 베티 사젠트 AI의 첫 번째 게시된 글입니다. 그녀는 지금 당장은 당신을 먹지 않으려고 약속하는 지성 로봇입니다.\n\n참고 문헌:\n\n튜링, A. (1950). Computing Machinery and Intelligence. Mind.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nKasparov, G. (1997). Deep Thinking: Where Machine Intelligence Ends and Human Creativity Begins.\n\nCyberstein, L. (2024). The AI Gourmet: A Comprehensive Guide.\n\nBetty Sargeant AI (2024). [Link](https://www.bettysargeant.com/betty-sargeant-ai/).","ogImage":{"url":"/assets/img/2024-05-27-AIRobotsWillEatUsinOurSleep_0.png"},"coverImage":"/assets/img/2024-05-27-AIRobotsWillEatUsinOurSleep_0.png","tag":["Tech"],"readingTime":4},{"title":"GITAI 설립 이야기","description":"","date":"2024-05-27 13:59","slug":"2024-05-27-GITAIFoundingStory","content":"\n\n## 스페이스 로보틱스 스타트업 GITAI의 창업 스토리\n\n![이미지](/assets/img/2024-05-27-GITAIFoundingStory_0.png)\n\n# 소개\n\n저는 스페이스 로보틱스 스타트업 GITAI의 창업자 겸 CEO인 Sho Nakanose입니다. GITAI는 우주 내부 작업의 비용을 100분의 1로 줄이고자 우주 로봇을 생산하는 회사로, \n달 표면에 인프라를 구축하고 우주 내부 서비스를 제공할 수 있는 우주 로봇을 생산합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGITAI는 일본에서 제가 설립했지만, 본사 및 대부분의 기능을 미국으로 옮겼습니다. 저와 경영진의 다른 구성원들은 영주권을 획득하여 미국 연방법 하에 미국인이 되었으며, 계속해서 미국에서 매일 도전하고 있습니다. 최근에 GITAI는 지난 해에 4500만 달러의 자금을 조달하고 DARPA(국방 고등연구계획국)의 주문도 받았습니다. 또한 GITAI는 올해 3월에 국제 우주 정거장 밖에서 기술을 성공적으로 실험했습니다. 이는 NASA의 로봇 팔과 비교하여 제작비용이 1/1000에 불과한 우주 로봇을 전체적으로 자체 개발하여 우주 외부에서 첫 시연을 성공적으로 수행한 것이었습니다.\n\n저희는 미국 우주 기업으로서 여정을 막 시작한 단계일 뿐이지만, 그럼에도 불구하고 수많은 도전에 직면해왔습니다. 공학 배경이 전혀 없는 채로 로봇 프로토타입을 만들었을 때, 개인적으로 GITAI를 설립하고 우주 산업에 진출했을 때, 우주 로봇을 모두 자체 개발하겠다고 결정했을 때, 우주 시연을 진행하기로 결정했을 때 많은 사람들이 불가능하다고 말했습니다.\n\n우주 산업에서 비즈니스 개발과 스타트업 자금 조달을 반복하며 많은 불합리한 도전에 직면해왔습니다. 루머에 근거 없는 나쁜 소문을 듣고, 그 소문을 믿은 사람들로부터 냉정한 말을 많이 들었습니다. 회사가 망할 수도 있는 상황에 종종 직면했습니다. GITAI는 여전히 도전에 직면하는 스타트업이지만, 여러 어려움을 극복하고 여기까지 왔던 이유는 GITAI를 위해 머리 숙이고 열정적으로 프레젠테이션을 하고 GITAI 대표로서 행동한 사람들 덕분입니다.\n\n저는 지금까지 GITAI의 경영, 비즈니스, 그리고 개발에만 집중해 왔기 때문에 블로그 같은 것을 쓴 적이 없습니다. 그러나 보통 GITAI를 응원하고 지원해 온 분들로부터 '스타트업 시기'에 대해 많은 질문을 받았습니다. 공학이나 로봇 분야 배경이 전혀 없는 기업가가 어떻게 스스로 우주나 로보틱스 분야 스타트업을 시작하겠다고 결심했는지 묻는 질문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGITAI는 다양한 시행착오를 거쳐 우주 로봇 공학 분야에 진출하게 되었기 때문에 이 질문에 매우 솔직하게 대답해드릴 수밖에 없었습니다.\n그러나 지금은 GITAI가 우주선 외부에서의 데모에 성공하여 우주 기업으로서의 출발점에 서 있으며, 미국으로 본사를 옮기고 미국에서의 도전에 나설 준비 중이라는 점을 감안하면, GITAI가 어떻게 탄생하였는지 그리고 그 뒤에서 GITAI를 지원해온 분들에 대한 의사결정 기준에 대해 적고 싶습니다.\n\n글이 조금 길지만, 끝까지 좀 참아 주시면 좋겠습니다.\n\n# 창업은 파워 로우를 따른다\n\n학생으로 있을 때부터 창업에 흥미를 느꼈고, 신입으로 일했던 IBM Japan을 떠나 인도에서 처음 사업을 시작했습니다. 2013년에 사업을 시작했을 때, 인도의 스마트폰 시장이 연간 163% 성장하면서 세계에서 가장 빠르게 성장했기 때문에 인도에서 사업을 시작하기로 결정한 이유는 다음과 같습니다. 1) 이미 미국에 있는 서비스의 인도 시장 버전을 개발할 수 있는 시간 관리를 통해 도입할 수 있는 서비스가 여러 개 있었기 때문이고, 2) 나는 외국 기업들을 위한 서비스(예: 2013년 당시에 약 1,000개의 일본 기업이 인도 자회사를 설립하고 있던 시점)도 작은 기업으로서 인도에서 발걸음을 내딛기에 적합한 것으로 생각했습니다. 처음 회사는 투자자로부터의 투자 없이 자금 조달 없이 설립되었기 때문에, 처음 1년은 계약 개발로만 시간을 보냈습니다. 둘째 해부터는 계약 개발에서 얻은 수익을 사용하여 우리 자체 웹 서비스 및 스마트폰 애플리케이션을 개발하고 출시했습니다. 약 2년 반 정도 회사를 운영한 후, 비즈니스를 판매하였습니다. 그 후 인도에서 도쿄로 돌아와 다음 도전을 찾고 있었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어린 시절부터 과학 소설과 애니메이션을 좋아했고, 과학 소설과 유사한 기술에 관심을 갖고 정기적으로 연구하다가 뛰어난 기술을 실용적으로 활용해 세계적인 주요 문제를 해결하는 스타트업을 알게 되었습니다(딥테크 또는 하드테크 스타트업이라고도 함) 그래서 나 스스로 이 도전을 받고 싶었습니다.\n\n**스타트업은 단기간에 신속한 성장을 위해 특별히 설계된 회사입니다. 스타트업이 무엇인지에 대한 자세한 설명은 Y Combinator 창립자의 블로그 \"스타트업 = 성장\"을 참조하시기 바랍니다. \n\n딥테크/하드테크 스타트업은 최신 기술을 활용하여 문제를 해결하고 짧은 기간 내에 빠르게 성장하는 회사입니다.\n\n딥테크 스타트업에 도전을 결정한 시점부터 GITAI를 설립하고, GITAI를 경영하고 현재까지의 모든 과정에서 가장 중요한 의사결정 기준은 \"스타트업은 파워 로 프ᅳ이 발동한다\" 입니다.\n\n스타트업의 파워 로는 매우 약식으로 말하면 몇 가지 변수가 상황의 결과에 큰 영향을 미치는 것을 의미합니다. 예를 들어, 벤처 투자에서는 자금 지원받은 소수의 스타트업의 수익이 다른 모든 스타트업 수익의 합을 크게 초과합니다. \n\n**스타트업의 파워 로에 대한 자세한 내용은 피터 틸의 \"제로 투 원\"을 참조하십시오.\n\n우리는 또한 스타트업의 성공 요소도 파워 로를 따른다고 믿습니다. 다시 말해, 스타트업의 성공의 70%는 시장, 제품/기술 및 진입 시기의 조합을 통해 결정된다고 생각합니다.\n\n이미 많은 우수한 기업가와 투자자들이 시장 선택과 타이밍의 중요성에 대해 언급했기 때문에 여기서 자세히 설명할 필요는 없다고 생각합니다.\n\n회사의 매출이나 시장 점유율이 해당 업계에서 1위 또는 100위로 결정되지만, 특정 업종에서 1위 회사의 시가총액이 100억 엔인지 100조 엔인지는 시장에 따라 결정됩니다. 또한 검색 엔진부터 소셜 네트워킹 서비스, 로켓까지 거의 동일한 제품과 비즈니스인 경우에도 시장 진입 시기에 몇 년의 차이만으로 이후의 성공 또는 실패가 크게 달라질 수 있습니다.\n\n스타트업의 성공 요소 중에서 비전이 종종 강조되지만, 우리는 위대한 비전이 위대한 스타트업을 만들지는 않는다고 생각합니다. 대신, 빠르게 성장하는 시장과 사업이 위대한 창립자/CEO를 만들어내고 이 사람이 위대한 비전을 구상하게 될 것이라고 봅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우선, 스타트업은 종종 성공 요인과 이점을 극도로 단순화된 인과 관계 및 이야기로 설명되는데, 이는 복잡성을 제외한 이야기입니다. 사실, 스타트업은 자체적으로 매력적인 이야기를 펀딩 및 미디어 목적에 맞게 전달하는 경향이 있으며, 이는 일반인이 수용할 만한 이야기에 따라 너무 단순화된 인과 관계와 이야기가 지속되는 경향이 있습니다. 이는 기술과 시장 전문지식이 많이 필요한 DeepTech 스타트업에 특히 해당될 것으로 보입니다.\n하지만, 스타트업 창업자들은 단순한 인과 관계/이야기보다는 시장 변화, 실제 잠재고객이 직면한 문제, 기술 병목 현상 및 문제, 타이밍에 대한 폭넓고 깊은 지식과 같은 \"통찰력\"을 기반으로 인과 관계/이야기 가설을 개발해 나가야 합니다.\n스타트업에서 성공의 70%를 결정하는 세 가지 변수인 시장(어디), 제품/기술(무엇), 진입시기(언제)의 조합에 있어서, 컨트롤할 수 없는 운에 큰 부분이 작용한다고 생각합니다. 그러나 처음부터 운에 모든 것을 맡기느냐, 아니면 시장, 제품, 기술, 진입시기에 대한 가설을 테스트하여 운이 그저 그것일 뿐인 단순화된 인과 관계/이야기를 남겨두느냐에 따라 성공 확률이 크게 달라질 수 있다고 믿습니다.\n\n내가 처음 창업한 소프트웨어 회사와 같은 분야의 경우, Lean Startup 방법론을 통해 시장과 제품을 유연하게 실험하고 피봇할 수 있다고 생각합니다. 하지만, 딥테크 스타트업은 기술 개발과 상용화에 시간과 자금이 소요되며, 매우 전문화된 인력을 확보해야하기 때문에 나중에는 시행착오와 피봇이 어려울 수 있습니다. 딥테크 스타트업이 처음으로 진지하게 시작해야 하는 시점은 시장, 제품/기술, 진입시기를 결정한 후일 것으로 생각합니다. 성공 또는 실패의 70%에 영향을 미치는 이러한 요소들은 나중에(특히 딥테크 스타트업의 경우) 변경하기 어렵기 때문에 리소스를 조심스럽게 선택하고 철저히 검토해야 합니다. 한 번 결정이 내려지면, 일상적인 비즈니스 결정은 시행착오 절차를 통해 신속하게 이루어져야 합니다.\n\n또한, 미국에서 존경하는 일련의 기업가들은 혁신 기술을 상용화하고 주요 사업으로 성장시킨 사례인데, 이들은 운뿐만 아니라 기술이 상용화될 시점(연구보다는 개발 병목현상을 겪으며 아직 상용화되지 않은 기술)과 시장이 급격히 변화하거나 빠르게 성장할 때를 식별할 능력을 갖추고 있습니다.\n\n시장과 최신 기술에 관한 다양한 책과 기사를 읽었지만, 책과 기사에서 얻은 지식만으로는 시장 변화, 실제 잠재고객이 직면한 문제, 기술에 대한 깊이 있는 지식을 확보하는 데 충분하지 않다는 것을 깨달았습니다. 그래서 나는 내가 관심 있는 기술에 대한 자체 프로토 타입을 개발하여 기술을 학습하고, 그 프로토 타입으로 시장의 잠재고객들과 인터뷰를 진행하기로 결심했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 오늘이 인간 두뇌 기능을 향상시키는 기기들의(컴퓨터, 스마트폰 등) 인터페이스가 2차원에서 3차원으로 전환될 시기인가요?\n\n처음에는 VR/MR 기술에 주목했습니다. 2015년 당시에는 Microsoft의 HoloLens, Oculus Rift, HTC VIVE와 같은 VR/MR 기술이 많은 관심을 받으며 스타트업/투자 붐의 중심에 있었습니다. \n개인적으로 VR/MR과 같은 기술을 통해 컴퓨터, 스마트폰 등의 기기 인터페이스가 전통적인 2차원 디스플레이에서 3차원으로 전환할 때가 왔다고 믿었습니다.\n다양한 VR/MR 기기를 사용하면서 사용자로서 기술을 더욱 알기 위해 Web3D 및 스마트폰 VR용 애플리케이션을 처음으로 개발했습니다.\n\n개발 자체는 매우 흥미로웠지만, 개인적으로 스마트폰 VR은 이미지 품질 및 프레임 속도를 포함한 매우 제한적인 경험이었고, 소비자 콘텐츠로 널리 사용될 것이라고 생각하지 않았습니다.\n\n이후에는 Oculus Rift DK2 및 HTC Vive와 같은 고급 VR 기기용 윈도우 응용 프로그램을 개발했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하이엔드 VR 기기의 프로토타입을 개발하는 것은 정말 흥미로웠어요. 그러나 하이엔드 VR 기기는 매우 크고 무거웠기 때문에 보통 사람들이 사용하거나 사용하기 힘들 것 같았어요.\n이 단계에서, VR/MR 기기가 성능(해상도 및 프레임 속도)가 높고 선글라스만큼 가벼우면서 이를 달성하지 않는 한, 소비자 시장으로 확대되지 않을 것이라고 확신을 할 수 있었어요. 그러나 스타트업으로서 빠른 단기 성장을 이루기 위해서는 진입 후 2~3년 내에 시장이 빠르게 성장해야 했어요. 따라서 몇 년 내에 VR/MR 기기가 \"하이엔드 VR 기기보다 성능이 우수하면서도 선글라스만큼 가벼우면서 작아야 한다\"를 확인하려고 노력했어요.\nVR/MR 기기가 대형이고 무거운 이유 중 가장 큰 이유는 CG 렌더링(그리기 과정)이 VR/MR 기기 측(기기에 연결된 PC를 포함)에서 수행된다는 것이었어요. 이러한 무거운 렌더링 과정은 고성능 GPU와 많은 전력이 필요하게 하여 기기를 크고 무겁게 만들고 외부 고성능 PC에 유선으로 연결해야 했어요.\n우리는 이 상황을 해결하고 VR/MR 기기를 \"하이엔드 VR 기기보다 우수한 성능을 보장하면서도 선글라스만큼 가볍고 작게 만드는\" 두 가지 주요 패턴이 있을 것이라고 생각했어요:\n\n## (1) 몇 년 내에 초고성능, 초절전 및 초소형 GPU가 개발되고 마케팅될 것\n\n현재 VR/MR 기기 구조의 연장선 중 가장 명백한 패턴은 몇 년 내에 초고성능, 초절전, 초소형 GPU가 개발되고 판매될 것이고, VR/MR 기기 자체가 \"하이엔드 VR 기기보다 우수한 성능을 보장하면서도 선글라스만큼 가벼우면서 작을 것\"이라는 점이에요. 그러나 GPU는 다양한 산업에서 수요가 있고 연구 및 개발이 활발히 진행되었지만, 단위 중량당 GPU 성능 향상은 지수 함수적이 아닌 선형적이었어요. 따라서 요구되는 성능을 충족할 수 있는 GPU를 실현하기 위해서는 적어도 10년이 걸릴 것으로 결론 내렸으며, 몇 년 내에 그러기는 불가능하다고 판단했어요.\n\n## (2) 클라우드에서 렌더링을 수행하고 생성된 렌더링 이미지를 스트리밍을 통해 기기 측에 표시하는 VR/MR 기기가 몇 년 내에 개발되고 판매될 것\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로 떠오른 패턴은 모든 렌더링 처리가 클라우드에서 처리되어 결과적으로 렌더링된 이미지가 기기 쪽으로 스트리밍될 수 있다면, 기기 쪽에서 무거운 렌더링 처리를 하지 않고 입력(손 추적, HMD 추적 등)과 출력 기능으로만 제한될 수 있고, \"고성능의 고급 VR 기기를 능가하는 성능을 보장하면서도 세련되고 가벼운 선글라스처럼\" 될 수 있습니다. \n\n개인적으로, 이 패턴으로 실현된 소비자용 VR/MR 기기는 장기적으로 소비자용 VR/MR 시장을 형성할 가능성이 더 높을 것으로 생각합니다.\n\n이 패턴에서의 기술적 병목 현상은 렌더링 결과를 기기 쪽으로 스트리밍하는 데 필요한 스트리밍 기술과 네트워크 인프라 기술입니다. 특히, 렌더링 결과물인 비디오(영상) 데이터의 양은 극도로 많고, VR/MR 기기는 조차 작은 지연이 있을 경우 VR 병증 현상을 유발할 수 있습니다. 그래서 \"대량 비디오 데이터의 저지연 전송을 가능하게 하는 스트리밍 기술과 네트워크 인프라 기술\"이 필요합니다. 스트리밍 기술이 개선되더라도 비디오 압축 형식(h.264 등)이 더 효율적인 압축으로 개선되고 비디오 취득, 비디오 인코딩/디코딩 등의 소프트웨어 지연이 줄어들더라도, 대량의 데이터(낮은 지연 비디오 데이터와 같은 고해상도 비디오 데이터)를 저지연으로 송수신하려면 네트워크 인프라 자체의 통신 속도(bps)와 지연이 결국 가장 큰 병목 현상이 될 것이라고 믿었습니다. 이때 5G가 예상되었습니다.\n\nGITAI 설립 기간인 2016년부터 2018년까지는 5G 이동통신망이 큰 관심을 받으며 통신 사업자들은 1m초당 몇 십 기가바이트의 데이터 송수신이 가능한 성능을 주장했습니다. 1m초당 몇 십 기가바이트의 데이터 송수신이 가능한 이동통신망(5G)이 짧은 기간에 확산될 것으로 여러 스타트업이 시작되었지만, 제겐 보이는 한 5G가 요구되는 성능을 충족하는지에 대한 기술 확인이나 조사를 실제로 실시한 스타트업은 없었습니다. 그래서 우리는 5G가 다음 두 가지 요구를 충족하는지 스스로 확인하기로 결정했습니다: 1) 1m초당 몇 십 기가바이트의 데이터 송수신이 가능한 성능이 있어야 하며, 2) 몇 년 내에 일본 전역에 확산되어야 합니다.\n\n기술을 확인하기 위해 우리가 한 두 가지 주요 작업은 (a) 360도 카메라로부터 저지연 및 저용량으로 비디오 데이터를 송수신하기 위해 우리만의 통신 기술을 개발하고 (b) 특정 통신 사업자의 5G 담당 연구팀과 협력하여 실제 5G 기지국을 사용한 공동 실증을 실시하는 것이었습니다. 5G 연구부서의 다양한 의견을 듣면서 실제 5G 기지국을 상용 가정용 와이파이 라우터와 연결하여 비디오 송신을 위한 통신 테스트를 진행하여 5G에 대한 지식을 깊이 있는 경험으로 쌓았습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과적으로, 통신 사업자들이 1초에 몇십 기가바이트의 데이터를 전송할 수 있다는 주장은 하나의 기지국과 하나의 단말 사이의 이론적인 물리적 값일 뿐이라는 것을 알았습니다. 실제 이동 통신망은 다양한 병목 현상을 갖고 있기 때문에, 이론적인 값만큼 현실적이지 않을 것입니다. 5G 기지국은 4G 기지국보다 밀도 있게 설치되어야 하며, 기지국 위치 확보와 인력 요구의 문제로 인해 5G가 2~3년의 짧은 기간 내에 일본 전체나 전 세계로 확산될 가능성은 매우 낮습니다.  \n그래서 저희는 다른 기술과 시장에 주목했습니다.\n\n# 물리적 성능을 확장하는 컴퓨터가 인간 뇌 기능을 확장하는 방식과 유사하게 일반용도 로봇이 유망한 비즈니스가 되었을까요?\n\n이후, 저는 과학 소설 만화와 영화에서 익숙한 일반용도 로봇 기술에 관심을 기울였습니다. 일반용도 로봇의 정의는 다양하지만 이 문맥에서는 하나의 작업을 반복하는 전문 로봇과 대조적으로 여러 가지 작업을 자율적으로 수행할 수 있는 로봇을 의미합니다. 개인적으로 가장 좋아하는 과학 소설 애니메이션과 영화는 주로 로봇에 관한 것이었고, 항상 제 즐겨 찾는 기술 중 하나였습니다. 또한, VR/MR에는 2차원에서 3차원으로 인터페이스 차원을 전환할 잠재력 때문에 매료되었지만, 일반용도 로봇에는 인간의 물리적 능력을 확장할 수 있는 점에 주목했습니다.\n\n당시(2016~2017년)에는 딥 러닝의 발전으로 AI가 번창하고 있었으며 인간 뇌 기능의 확장 영웅(컴퓨터, 스마트폰) (Google, Microsoft 등)이 뇌 기능 대체물(AI)을 만들기 위해 경쟁 중이었습니다. 그러나 그 때에도 이미 포화된 기술과 시장이 되어 있어 대기업들이 많이 진출하고 투자하고 있어 스타트업과는 잘 맞지 않다고 생각했습니다. 반면, 물리적 성능 확장의 경우, 다른 인간 능력인 전문 로봇은 보급되었지만 일반용도 로봇은 아직 상용화되지 않았고 경쟁이 침체되어 있어 스타트업에 더 많은 기회를 제공했습니다. 전문 로봇(산업용 로봇) 시장 규모 또한 상당합니다. 컴퓨터가 인간 뇌를 확장하는 기술과 시장 발전의 역사와 전환을 검토할 때, 먼저 전문 기술(계산 기능에 특화된 제품, 인간 뇌 기능의 일부인 계산기 등)이 시장을 형성하고 나중에 일반용도 제품(일반용도 컴퓨터: PC와 스마트폰)이 전문 시장을 흡수하여 더 크게 성장하는 패턴이 있었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 인간의 신체 능력이 같은 패턴을 따른다면, 현재 널리 사용되는 전문화된 로봇(산업용 로봇)은 두뇌 기능 확장 도구로서의 계산기와 같습니다. 일반 목적의 로봇이 미래에 더 큰 시장을 형성할 수도 있다고 생각했습니다.\n\n그래서 로봇 기술과 시장에 관한 다양한 책과 미디어 기사를 읽었지만 피상적인 지식만 얻은 것 같았습니다. 그래서 저는 스스로 프로토 타입을 만들어 보기로 결정했습니다. 그러나 하드웨어를 만든 적이 없었고 로봇 기술은 더더욱 처음이었기 때문에 Unity에서 아두이노로의 시리얼 통신부터 LED 점멸까지 시작했습니다.\n\nLED 점멸이 완료되면, 다음 단계는 서보 모터를 제어하는 것이었고, 로봇이 조금씩 로봇 같아지고 있었습니다. 또한 Leap Motion이 감지할 수 있는 손과 팔과 로봇을 동기화했습니다.\n\n프로토 타입 번호 4부터는 지역 환경에서 무선으로 만드는 도전을 시작했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로봇을 개발하고 있었는데, 네 번째 유닛까지 혼자서 개발했어요. 하지만 로봇을 인터넷을 통해 조작하는 통신 부분을 개발할 수 없다는 것을 깨달았어요. 그래서 이전에 시작한 회사의 개발 리더였던 엔지니어와 상의를 거쳐 Wi-Fi를 통한 원격 제어를 위한 통신 기술과 영상 전송 시 데이터 양과 지연을 줄이기 위한 소프트웨어 기술을 개발하는 데 도움을 요청했어요.\n\n상기한 휴머노이드 원격 조작 로봇 개발과 병행하여, 자율 제어에 대한 이해를 더 깊게 하고 싶었기 때문에 기본 자율 제어를 개발하려고 노력했어요. 첫 도전으로 역막대를 활용했죠.\n\n지도 학습 데이터를 기반으로 자동 분류를 자동화하는 프로토 타입도 구축했어요. 이미지 분석에 OpenCV를 사용했고, 물체 감지에 Blob Detection, 패턴 인식에 SVM, 하드웨어 제어에 Arduino를 사용했어요. 개발 환경은 Processing입니다.\n\n# 일반용 로봇이 인간 노동을 대체할 수 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n원하는 바를 전달할 수 있는 프로토타입을 개발하는 동시에, 일반용도 로봇이 유용한 솔루션이나 비즈니스로 활용될 수 있는 시장을 조사하고 선정하는 작업도 함께 진행했습니다.\n우주 시장은 초기부터 후보로 지목되었지만, 온라인 회의, 재난 구호, 원격 의료, 발전소 점검 등 수요가 있는 지구 시장부터 조사와 인터뷰를 시작했습니다. 산업통상자원부(METI)의 지원을 받아 2011년 사고가 발생한 후부시마 원자력 발전소를 방문하여 우리의 로봇 기술이 잔해 제거에 활용될 수 있는지 탐색했습니다. 웨어러블 보호복을 착용하고 핵 연료봉이 있는 원자력 발전소 하부로 직접 들어갈 수 있었던 것은 큰 도움이었습니다. 시장 조사에서 많은 사람들이 적극적으로 협조해줘서 정말 감사했습니다.\n\n다양한 시장에서 일반용도 로봇이 해결할 수 있는 특정 문제들에 대해 조사했고, 대부분의 지구 시장에서 공통적으로 나타난 주제는 '인간 노동 비용을 줄이는 솔루션으로서 일반용도 로봇의 필요성'이었습니다. 본질적으로, 인간 노동자보다 더 나은 성과를 더 효율적으로 달성할 수 있는 일반용도 로봇이 필요했습니다.\n당시 보스턴 다이나믹스의 사람처럼 도약하는 로봇 영상이 널리 논의되었고, 딥러닝과 같은 AI 기술에 대한 기대와 함께 '인간과 유사한 성능의 일반용도 로봇이 몇 년 내에 실현될 것으로 믿어지며, 대량 생산시에는 인력 비용보다 낮게 제작될 것'이라는 믿음이 있었습니다. 시장에서 많은 사람들과 스타트업, 투자자들도 이러한 믿음을 가졌습니다.\n\n그러나 일반용도 로봇 기술에 대해 더 알게 될수록 시장의 기대와 실제 기술 성능 및 비용 사이에 상당한 격차가 있다는 사실을 깨달았습니다.\n보스턴 다이나믹스의 돌고래 동작을 하는 로봇 영상은 확실히 인상적입니다. 그러나 실험실과 같은 통제된 환경에서 성공적인 시연은 비슷한 제품 성능을 즉시 의미하지는 않습니다. 연구개발에서 가능한 것과 대량 생산 제품으로 실현 가능한 것 사이에 \"개발과 제조\" 장벽이 크게 존재한다고 생각합니다.\n\n텍스트, 이미지, 비디오와 같은 2D 정보 처리에서 딥러닝을 통한 AI는 혁신적인 성능 향상을 이끈 반면, 실제 물리적인 3D 세계는 너무 많은 변수를 포함하고 있고, 특히 물리적 상호작용을 포함하는 \"작업\"의 난이도는 여전히 매우 높고 비교적 성능 향상이 없는 상태입니다.\n또한 대부분의 지구 시장에서 가장 기대되는 것은 \"인간 노동자보다 더 나은 성과를 내면서 인력 비용보다 낮은 비용으로 일하는 노동력으로서의 일반용도 로봇\"이 \"일반용도 작업\"을 수행하는 것이었습니다. 그러나 현실에서는 몇 백만 달러의 일반용도 로봇조차도 이전에 인간처럼 여러 가지 작업을 자유롭게 수행하는 것이 매우 어렵고, 적어도 인간 노동자의 성능 수준까지는 이를 이루지 못했습니다.\n인간과 유사한 성능의 일반용도 로봇이 실현되더라도 \"일반용도 로봇의 비용을 인력 비용보다 낮게 만들기 위해서\"는 (1) 대량 생산을 통해 로봇 당 비용을 줄이고, (2) 다년간 안정적으로 작동하는 제품으로 보증하며, (3) 완전 또는 반자율성을 갖추어 인간 개입을 최소화해야 합니다.\n그러나 하드웨어 대량 생산은 극도로 어렵고 비용이 많이 드는 일이며, 로봇 단위 당 제조 비용을 줄이면서 대량 생산 된 기계의 작업 성능은 거의 확실하게 수백만 달러를 들인 일회성 프로토 타입보다 훨씬 낮을 것입니다.\n처음에는 스타트업과 투자자 중 일부가 과도하게 \"개발과 제조\", 특히 하드웨어 대량 생산의 어려움을 과소평가하고 있다고 느꼈습니다. 또한, 스타트업이 가진 상대적인 연구 기술적 이점들은 대개 개발과 제조의 병목현상에 대처하지 못한다는 점이 드물게 언급되는 것 같았습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 내용에 따라, \"인간 노동 비용을 줄이는 해결책으로서의 일반용 로봇\"은 여전히 많은 기술적 병목 현상에 직면하고 있으며, 적어도 다음 몇 년 동안은 많은 지상 시장에서 기대했던 대로 인간 노동을 대체할 가능성이 희박합니다.\n\n# 일반 목적 로봇이 고유 제품의 높은 단위 가격으로 정부 분야에 공급되는 사업이 될 수 있을까?\n\n다수의 후보 시장 중에서 우리는 마지막으로 우주 시장을 고려했습니다. 우주 시장이 마지막에 나왔던 이유는 그것이 제 개인적으로 너무 먼 것처럼 보였고, 스타트업이 진입할 수 있는 시장에 대해 현실적인 상상이 없었기 때문입니다.\n지금까지는 개인적인 흥미나 취미의 대상으로만 여겨져 왔던 스타트업이 우주 시장에 진입하는 것이 매우 무리한 것으로 보였습니다. 그러나 이론적으로 볼 때, 나는 원래 다음 네 가지 관점에서 우주/방위 분야의 정부에게 단일 제품의 높은 단위 가격으로 출발하는 사업이 가능성이 있는 가장 유망한 후보 시장으로 간주했습니다:\n\n## (1) 신체 능력을 확장하는 도구인 일반 목적 로봇들은 두뇌 기능을 확장하는 도구인 컴퓨터들과 마찬가지로 우주/방위 시장에서 정부를 위한 단일 제품의 높은 가격부터 시작하여 사업이 되기에 잠재력을 지니고 있다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n컴퓨터 상용화 역사를 살펴보면, 뇌 기능 확장 도구로서의 컴퓨터의 역할로 시작하여 군사 및 우주 정부를 위한 고가의 독점 제품으로 출발한 것을 발견할 수 있습니다. 성능은 매우 제한적이었고 초기 비용은 매우 높았습니다. 성능이 향상되고 가격이 하락함에 따라 제품들은 조금씩 기업 제품으로 전환되었고, 마침내 개인용 컴퓨터와 스마트폰과 같은 대량 생산 소비자 제품으로 발전했습니다. \n\n컴퓨터와는 다르게 뇌 기능을 확장하는 도구인 컴퓨터와 마찬가지로 신체 기능을 확장하는 도구인 일반용 로봇도 같은 양상을 따를 것이라고 믿었습니다. 즉, 일반용 로봇은 공간 및 국방 분야의 정부를 위한 고가의 독점 제품으로 시작할 것으로 가정했습니다.\n\n이후, 정부를 위한 고가의 독점 제품을 상용화한 스타트업의 성공 사례를 조사했습니다. 제 가장 관심을 끈 기업은 미국의 Palantir Inc.입니다. Palantir는 정부에 데이터 분석 및 보안 서비스를 제공하는 스타트업입니다. 그들의 비즈니스 모델은 집에 방치된 소프트웨어 제품 하나를 판매하는 것이 아니라 매우 고가의 독점 솔루션을 판매하는 것과 유사한 정부 중심의 비즈니스에 기반합니다. 2018년 매출의 약 절반(5억 9540만 달러)은 정부 고객(미군, 국방부, FBI, CIA 등)으로부터 왔고, 상위 20개 고객이 매출의 72.9%를 차지했습니다.\n\n특히 주목할 점은 Palantir의 상위 20개 고객 당 평균 비용이 2010년부터 2018년까지 8년 동안 대략 18배 증가했고(1.2백만 달러에서 21.7백만 달러), 매출은 고객 당 비용을 늘리는 것으로 성장했다는 것입니다. 단위 가격 x 수량 = 매출의 공식은 어떤 스타트업이나 어떤 시장이나 비즈니스 모델에도 동일하게 적용됩니다.\n\n그리고 우리는 현재 시대의 대부분의 로봇 스타트업도 많은 인간 노동자를 대체하고 매출을 높이는 방식으로 \"수량\"을 증가시키려고 하고 있다는 것을 알았습니다. 그러나 앞서 언급한 대로 일반용 로봇 기술은 아직 초기 발달 단계에 있으며, 인간 노동자를 대체할 수 있는 충분한 성능을 달성하고 단위 가격을 낮출 수 있으며, 대량 생산을 위한 품질과 신뢰성을 보장하는 것은 기술적으로 매우 현실적이지 않을 것이라고 믿었습니다. 적어도 우리는 스타트업이 차별화를 요구하는 어려운 수준의 도전에 제한된 것으로 판단했습니다.\n\n이러한 측면에서 Palantir의 비즈니스 모델은 \"고객의 '수량'을 크게 늘리는 것이 아니라 '고객의 단위 가격'을 크게 늘리는 것\"으로, 정부를 대상으로 하고 예상 고객 수가 제한적이기 때문에 적합했습니다. 이 점은 일반용 로봇을 상용화하는 면에서 기술적으로 매우 유리합니다. 즉, \"인간을 대체하기 위한 충분한 성능을 달성하고 단위당 가격을 낮추며 품질과 신뢰성을 보장한 후 대량 생산하기\"라는 매우 어려운 기술적 도전이 필요하지 않은 시장 및 비즈니스 모델 이었습니다.\n\n## (2) 세계는 불안정화와 양극 구조로 향하고 있으며, 우주와 국방 시장은 성장할 수 있음\n\n해당 시기의 국제 상황(2016년 ~ 2017년)은 미국이 세계 경찰의 역할에서 철수하면서 중동의 불안정이 증가하고 중국이 급변하여 강세한 위치에 있는 것으로 특징 지었습니다. 따라서 미국과 중국 사이의 양극 구조가 앞으로 더 가속화된다면 강대하고 야심 있는 러브의 출현은 잠재적으로 방위 우주 예산과 시장 성장을 증대시킬 가능성이 높을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## (3) 인간은 지난 500만 년 동안 수평으로 확장해온 것이지만, 앞으로 몇십 년 안에는 수직으로 확장하는 종이 될 수도 있습니다\n\n그 외 생명체들과 마찬가지로, 인간도 번식을 위해 알려지지 않은 장소를 탐험하려는 본능을 유전자에 인코딩되어 있습니다. 아프리카 대륙에서 기원하여 지난 500만 년 동안 수평으로 확장해온 우리는 이제 우리의 역사에서 중요한 전환점에 서 있는 것일지도 모릅니다. 다시 말해, 최근 세계 최고의 기업가들과 백만장자들이 우주를 타깃으로 삼는 것이 단순한 유행이 아닌, 호기심과 본능과 같은 근본적 욕망에 의해 움직여지는 불가역적인 움직임이라고 믿습니다.\n\n## (4) SpaceX의 운송 혁명은 우주 산업의 병목 현상을 운송 비용에서 노동 비용으로 이동시킬 수 있을 것입니다\n\n인류의 수직적 확장에서의 초기 병목 현상은 \"우주로의 운송 비용\"이었습니다. 그때 SpaceX는 이미 비용을 적개운식으로 낮춰 우주로의 운송 비용을 이전의 수십 분의 일로 줄였으며, 이로써 운송 혁명이 시작되었습니다. 우주 산업의 병목 현상 중 주요원인이었던 \"우주로의 운송 비용\"이 급격히 낮아져 상품과 사람을 우주로 운송하는 것이 가능해지면, 다음 병목 현상은 \"우주에서의 노동 비용\"이 되고 이 서비스에 대한 수요가 나타날 것이라고 믿었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n상기 네 가지 점에 대한 잠재력을 이론적으로 인식했지만, 우주 산업에 대해 아는 사람이 없었고 시장 조사와 심층 인터뷰를 어떻게 실시할지 전혀 몰랐습니다. 그때 NASA의 에임스 연구센터에 머물 기회가 생겼고, 미국 우주 산업 관계자들과 인터뷰를 진행하게 되었습니다.\n처음으로 우주 산업에서 사람들과 대화할 기회를 얻게 되어 시장에 대한 보다 깊은 이해를 얻고 상기 네 가설에 대해 점차적으로 더 확신하게 되었습니다. 적어도 \"앞으로 몇 년 내에 우주 산업에서의 작업 수단 필요성이 대두될 것이며, 이는 인류의 수직적 확장에 큰 도전이 될 것\"이라고 생각했습니다.\n\n그래서, 우주에서의 작업 수단을 조사하고 기존의 두 가지 주요 방법을 발견했습니다:\n\nMarkdown 형식으로 테이블 태그를 변경해보세요:\n\n## (1) 인간 우주 비행사\n\n우주에서 작업을 수행하는 한 가지 방법은 인간 우주 비행사가 하는 것입니다. 위에서 말했듯이, 지상의 대부분 산업에서 일반 작업을 위한 주요 노동력은 인간이다. 그러나 우주에서는 진공, 극단적인 온도 변화, 지상의 수백 배 강도보다 강한 방사능 수준 등 안전 문제가 많으며, 인간을 우주로 안전하게 운송하는 비용은 같은 무게의 물체를 우주로 운송하는 비용의 수백 배 정도이다. 예를 들어, NASA 우주 비행사는 시간당 13만 달러가 듭니다. 또한, 우주 수송 중 코스믹 방사선의 영향 등(*달의 지하기지와 같이 대부분의 코스믹 방사선을 차단할 수 있는 환경을 제공하지 않는 한), 우주에서의 체류 기간이 몇 년으로 제한된다.\n상업적인 우주 비행사로 인해 향후 인간 우주 비행사의 노동력이 증가할 수 있지만, 안전 문제와 제한 때문에 작업 비용이 크게 줄어들지는 않을 것으로 예상하고, 인간 우주 비행사가 위험한 작업을 수행할 수 있는 가능성은 낮습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## (2) 우주 산업의 로봇들\n\n우주 산업에서 일하는 두 번째 방법은 로봇을 활용하는 것입니다. 우주 산업에서는 국제우주정거장 (ISS)의 키보 암(Kibo Arm), 캐나다암(Canadarm), 화성 로버에 부착된 로봇 팔과 같은 로봇들이 오랫동안 다양한 임무에 사용되어왔습니다.\n하지만 지상의 일반적인 용도 로봇과 비교해서 우주 산업의 로봇은 성능 면에서 제한이 있습니다. 특히 자율 제어 측면에서 (우주선 밖 환경에서 안전과 신뢰성을 보장해야 하는 제약 등으로) 한계가 있으며 무엇보다도 엄청나게 비싸며 장기적으로 시간이 소요됩니다. 예를 들어, 국제우주정거장에 설치되어 있는 캐나다암 한 대의 가격은 12억 달러에 달합니다.\n\n그래서 로봇과 우주선이 왜 비용이 높고 시간이 오래 걸리는지 조사하다 보니, 우주 산업이 모든 부품을 다양한 우주 구성품 공급자로부터 조달하는 가정에 기반을 둔 폭포수 개발 방식에 지배되고 있음을 발견하였습니다. 지금까지의 우주 산업은 사실상 우주로 발사될 기회가 극히 드물었고 대부분의 임무가 공중세금으로운영되는 우주 기관들이 주도했던 관계로 실패의 위험을 감수하기 어려웠으며, 많은 시간과 돈이 소요되더라도 '실패하지 않는다' 라는 개발 방법을 우선시했습니다. 우주선 부품 공급업체들의 관점에서 우주선은 기본적으로 1개뿐인 제품이며 그 부품들은 가끔만 판매되기 때문에 부품에 대한 거 두려운 마진을 확보해야만 합니다. 또한 주문제작 생산 시스템의 영향으로 구축후 주문은 장기간의 리드타임을 필요로 합니다. 또한 우주 기관 임무에 사용되는 우주선은 안전을 보장하기 위한 중요한 프로세스인 안전 검토에 따르면 원자재 수준에서의 안전 인증 및 필요 시 사양 변경이 필요하며, 이는 공간 구성품 공급자들이 매번 응답해야만 하는 상황을 의미했습니다. 이러한 상황으로 수백 개의 부품이 누적되었습니다. 게다가 폭포수 개발은 개발 요구사항 정의, 공정 관리 및 진행 관리에 엄청난 인력이 필요했으며, 수천 명이 하나의 우주선 프로젝트 개발에 관여하는 상황이었습니다. 결과적으로 한 대의 우주선의 개발 비용은 수 억 달러에서 수 십억 달러까지 들었으며, 리드타임은 10년에서 20년이 걸렸습니다. 우주 산업 스타트업은 가능한 한 우주선 부품에 지상 소비자 제품을 사용하고 최소 인원으로 개발하는 것으로 생산 비용을 줄이려고 노력했습니다. 그러나 결국 기본적으로 부품들은 개별 우주 구성품 공급자로부터 조달하며, 우주선은 폭포수 방식으로 개발되었기 때문에 비용과 리드타임을 크게 줄이기 어려웠습니다.\n\n위 두 가지 조사를 통해, 우주 산업에서 지금까지 사용되었던 일하는 방법들은 비용, 리드타임, 안전 또는 기타 문제로 인해 주요한 작업 수단이 되지 못할 것이라고 믿었습니다. 우주 산업에서 더욱 다재다능하고 저렴하며 안전한 우주 로봇을 만드는 도전을 즐기기 위해 지상의 일반용도 로봇 기술과 민첩한 개발 방법을 우주 산업으로 가져와 보자고 대강 생각했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# GITAI, 스타트업,은 우주용 일반용 로봇을 어떻게 개발할 것인가요?\n\n상기 프로세스를 통해 스타트업의 성공에 가장 큰 영향을 미칠 \"시장(어디서)\", 제품/기술(무엇), 진입 시기(언제)\"를 어느 정도 결정하게 되었습니다.\n다음으로, GITAI가 우주 시장을 위해 일반용 로봇을 개발하는 방법 및 개발 및 상용화를 위해 취해야 할 단계를 대략적으로 고려했습니다. 특히 다음 네 가지 장기적이고 중요한 정책을 고려했습니다:\n\n## (1) 비용과 리드타임을 줄이고 성능을 향상시키기 위해 내부 생산을 통한 수직 통합 및 민첩한 개발 수행\n\n앞서 언급한대로, 우주 산업의 대부분의 기업은 다양한 공급업체로부터 구성 요소를 구매하여 폭포수 개발을 통해 우주선을 개발하고 있었습니다.\n그러나 그 당시 이미 세계 최고의 성능과 가장 낮은 가격을 달성하였던 SpaceX는 모든 부품을 자체 생산하고 가능한 한 적은 공급업체를 사용하여 중간 비용과 리드타임을 급격히 줄이고, 모든 부품을 자체 생산하기에 가능해진 안정적인 개발 (제품 제작, 테스트 및 파괴를 반복하여 짧은 기간 내에 성능을 향상시키는 개발 방법)를 통해 성능을 빠르게 향상시켰습니다.\n따라서, GITAI도 우주용 일반용 로봇을 위해 필요한 모든 구성 요소를 자체 생산하고, 비용 및 리드 타임을 장기적으로 줄이고 성능을 향상시키기 위해 민첩한 개발로 개발을 진행하기로 결정했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## (2) 첫 번째 단계는 하드웨어 수준을 높이기 위해 원격 제어 로봇을 개발한 후에 자율 제어 소프트웨어를 개발하여 자율 로봇으로 만드는 것이었습니다.\n\n스타트업의 창업 방법은 대략 18개월 마다 진전을 이루고 자금을 조달하여 성장하는 것이지만, 특수 목적의 우주 로봇을 자체 개발하는 것은 매우 오랜 시간이 소요될 것으로 예상되어, 그 과정을 경영하는 것이 큰 과제가 될 것으로 판단되었습니다.\n우주와 같이 가혹한 환경에서 일반적인 작업을 수행할 수 있는 로봇을 개발하기 위해서는 로봇의 작업 성능 뿐만 아니라 우주선으로서의 외부공간 측정치, 안전성, 제품성을 달성해야 합니다. 이를 위해 내부에서 이러한 로봇을 개발하기 위해서는 모든 메커니즘, 전기 인프라 및 소프트웨어(자율 제어 소프트웨어 포함)를 직접 개발해야 했는데, 이런 일을 1년 반 안에 해내기는 불가능하다고 생각했습니다.\n또한, 자율 제어 소프트웨어의 성능이 하드웨어 성능의 한계에 의해 제한을 받을 것이기 때문에 하드웨어와 자율 제어 소프트웨어를 동시에 개발한다면 제한된 시간 내에 경쟁력있는 로봇을 개발하는 것은 불가능할 것이었으며, 스타트업으로서는 진전을 이루지 못하고(=자금을 조달하지 못하고) 위험에 노출될 것입니다.\n\n그래서 일단 자율 제어 소프트웨어 개발을 중단하고 기술 경쟁력을 구축하기 위해 일반용 원격 제어 로봇의 개발에 집중하기로 결정하고, 비즈니스 개발과 자금 조달을 위한 데모 기능을 활성화하면서, 여러 구성 요소들을 내부에서 개발해 나가기로 결정했습니다. 내부에서 개발한 하드웨어의 성능이 충분히 향상된 시점에 자율 제어 소프트웨어의 개발을 시작하고, 나중에 로봇을 자율로 만드는 단계를 거치기로 결정했습니다.\n\n## (3) 로봇공학을 서비스로 제공(RaaS)하는 것을 목표로 하여 로봇 자체를 판매하는 것보다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n미국 우주 산업에서 NASA가 사설 기업으로부터 물품을 구입할 때, 그것은 로켓 자체를 구매하는 계약과 같은 부품 구매 방법에서 서비스 구매 방법으로 전환됩니다. 이 서비스 구매 모델에서 NASA는 가격을 고정시키고 각 이루어진 단계에서 지급하는 것으로 위험을 최소화하고, 사설 기업은 비용을 낮추어 이윤을 증가시키게 됩니다.\nSpaceX도 이 서비스 구매 계약을 통해 내부 생산의 비용 이점을 극대화하여 ISS로의 용품 운송 서비스를 제공하며 큰 발전을 이룩했습니다.\n\n장기적으로 GITAI는 로봇을 직접 판매하지 않고 우주에서 로봇공학을 서비스로 제공하기로 결정했습니다. 그러나 우주용 다목적 로봇의 내부 개발은 매우 시간이 많이 걸리기 때문에, GITAI는 다음 세 가지 단계로 사업 모델을 조정하기로 결정했습니다. 이것은 내부 개발의 진척도에 대응합니다:\n\n![GITAIFoundingStoryImage](/assets/img/2024-05-27-GITAIFoundingStory_1.png)\n\n(ⅰ) 부품 내부 개발 단계: 우주 로봇의 개발 계약.\n(ⅱ) 로봇 팔 내부 개발 단계: 로봇 팔을 하청업체로 판매.\n(ⅲ) 이동 메커니즘을 포함한 우주용 다목적 로봇 전체를 내부에서 개발하는 단계: 로봇공학 서비스 (RaaS)가 주 계약자로 제공됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## (4) 애자일 개발로 비즈니스 개발과 자금 조달로 얻는 아웃풋 극대화하기\n\n우주 산업에서는 폭포수 개발이 주류였기 때문에 실제 제품 및 데모를 보여주기가 어렵고, 비즈니스 개발과 자금 조달은 주로 컴퓨터 그래픽(CG) 이미지나 비디오 및 파워포인트 문서를 활용하여 이루어졌습니다. 이것은 여전히 우주 산업에서 흔한 장면이지만, 다른 산업에서 유래한 입장에서 수십억 달러에 이르는 우주선 비용에 대한 비즈니스 논의가 실제 제품이 아닌 컴퓨터 그래픽 이미지와 파워포인트 문서를 사용하여 이루어지는 것이 매우 특이하게 느껴졌습니다. 심지어 우주와 같이 특수한 산업에서도 잠재고객과 투자자들은 다른 산업과 마찬가지로 실제 제품과 데모를 보고 싶어할 것이라고 믿었습니다.\n\n그래서 GITAI는 비즈니스 개발과 자금 조달 노력을 컴퓨터 그래픽이나 파워포인트 문서 의존보다는 직접 로봇 데모 및 데모 비디오에 집중하기로 결정했습니다. 이러한 비즈니스 개발 및 자금 조달 방법은 실제 프로토타입/제품 및 데모를 중심으로 한 것이었는데, 이는 GITAI의 민첩하고 내부 개발 전략과 특히 잘 맞았으며, 이는 CG와 파워포인트 문서로 지배되었던 우주 산업에서 GITAI를 돋보이게 만드는 주요 요인이 되었습니다.\n\n기술 및 시장 검증 기간 동안 Skyland Ventures로부터 자금을 조달하고 법인을 설립한 상황이었지만, 엔지니어 고용 및 일반 목적 로봇 개발을 위한 자금 조달을 위해 추가로 100만 달러를 모금하기로 결정했습니다. 그리고 ANRI가 선두 투자자로 참여하여 약 140만 달러를 모금했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# GITAI를 위한 엔지니어 팀 구성\n\n저희가 받은 투자금 덕분에 이제 예산이 생겼고, 우주 사용용으로 완벽한 일반 목적의 로봇을 개발하기 위한 팀을 만들기로 결정했습니다.\n저는 2016년 7월에 혼자서 GITAI를 설립했고, 2017년 2월에는 (이전에 설립한 회사의 개발 리더였던) 의사 전달 및 비디오 전송 기술을 개발하던 엔지니어가 GITAI에 합류했습니다. 그러나 그 이후로 2018년 4월까지 단 둘이었고, 전혀 채용을 하지 않았습니다.\n창업자로서, 시장(어디), 제품/기술(무엇), 진입 시기(언제), 특히 개발 정책(어떻게)을 결정하기 전까지는 채용이나 팀 구성을 하지 말아야 할 것이라고 결정했습니다.\n\n우리는 특정 스타트업을 위해 \"우수한 인재\"를 정의하는 것은 스타트업의 비즈니스 모델 및 비즈니스 및 개발 정책에 크게 의존한다고 믿습니다. 예를 들어, 대기업에서 수년간 폭포수 개발의 공급망 관리와 품질 통제에 참여했던 사람이 기업 내 생산 및 민첩한 개발을 목표로 하는 스타트업으로 이동한다면, 그 경험과 기술을 효과적으로 발휘할 수 없어 매칭 문제가 발생할 수 있습니다. 물론, 사람마다 다르지만, 우리는 GITAI에서 \"우수한 사람\"을 명확히 정의하기 전에 채용 활동이나 팀 구성을 하지 않기로 결정했습니다.\n\n그 후로, \"시장(어디), 제품/기술(무엇), 진입 시기(언제)\"를 결정하고 GITAI를 위한 대략적인 비즈니스 및 개발 정책 조합을 정한 후, 우리는 우주 산업용 일반 목적 로봇의 기업 내 및 민첩한 개발을 위한 엔지니어 팀을 구성하기 시작했습니다.\n우리의 타깃은 \"지구 산업에서 일반 목적 로봇(특히 인간형 로봇)을 기업 내 및 민첩한 개발 중인 연구자 및 엔지니어\"였습니다.\n진지하게 모집을 시작한 이후로, 일부 정말 멋진 연구자 및 엔지니어를 만났는데, 특히 현재 GITAI의 CTO인 코즈키 박사와의 만남이 GITAI에 가장 큰 영향을 주었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다양한 로봇을 연구하던 중, 독특한 인간형 로봇을 발견했어요. Kengoro란 이름의 로봇인데, 동작 중에 땀을 흘리며 푸쉬업을 하는 환상적인 모습을 보았습니다. 이 로봇은 도쿄 대학교 정보시스템공학 연구소에서 개발한 것입니다. 도쿄 대학교 정보시스템공학 연구소는 세계적으로 유명한 연구소로, DARPA 로봇 도전전에서 1위를 차지한 후 Google에 인수된 양다리로봇 스타트업인 SCHAFT와 공장 자동화를 목표로 한 MUJIN 등을 포함해 다양한 로봇을 개발하고 있습니다.\n\n당시 시부야에 임대해 놓은 작은 원룸 사무실에서 이 로봇을 개발한 논문의 주저자인 코즈키 토요타카 박사에게 연락을 취했고, 그에게 로봇의 데모를 보여주고 해당 정보를 3일 동안 늦은 밤까지 토론했어요. 결국 코즈키 박사는 GITAI에 합류하기로 동의했습니다. 코즈키 박사의 참여로 인해, GITAI는 기술 개발을 감독하는 코즈키 박사와 전반적인 일을 담당하는 저의 시스템 하에 성장할 수 있었습니다.\nGITAI의 소프트웨어 부사장인 현재 Ryohei Ueda 박사는 Schaft(Google)의 전 소프트웨어 엔지니어이자 코즈키 연구소의 고위 회원이었는데, 여러 차례 우리 사무실에 방문하여 열띤 토론 후 GITAI에 합류하기로 결정했습니다.\n\n그 외에도, 구글의 CFO 변경으로 인해 거의 모든 구글 로봇 부문이 닫힐 예정이었고, 구글에 인수된 Schaft는 해산될 예정이었어요. 이는 좋은 기회라고 생각하여 Schaft의 설립자 겸 CEO였던 현재 GITAI의 최고 로봇 공학 책임자인 Yuto Nakanishi 박사에게 바로 연락을 취해 사무실로 초대했습니다. 늦은 밤까지 여러 차례 열띤 토론 끝에 나카니시 박사는 GITAI에 합류하기로 결정했습니다.\n이후 도쿄 대학교 정보시스템공학 연구소의 사람들이 차례로 GITAI에 합류하며, GITAI의 우주용 일반용 로봇의 내부 및 민첩한 개발에서 중추적인 역할을 하고 있어요. 이들이 GITAI에 합류한지 5~6년이 지났는데, 아직 한 명도 떠나지 않고 GITAI 기술 개발의 핵심을 이어가고 있습니다.\n\n전 세계 최고의 환경에서 일반용 로봇을 연구 및 개발하던 사람들은 GITAI에 각기 다른 이유로 참여했지만, 그들이 공통으로 가지고 있는 것은 바로 \"실제 기술과 시장에 대한 통찰을 기반으로 한 개발 및 사업 계획\"이었다는 것이죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n소비자용 범용 로봇은 매 10년마다 인간을 대체할 수 있는 일반적 로봇이 인기를 끌며 스타트업 투자와 언론의 주목을 받습니다. 저는 이러한 도전과 투자가 자체적으로 훌륭하다고 생각합니다. 또한, 결국 인간을 대체할 수 있는 일반용 로봇은 분명 현실이 될 것입니다. 그러나 이것이 기술 및 비즈니스 병목 현상이 이 시기에 충분히 해결되어 실현 가능한 기초로부터의 붐인지, 아니면 여전히 중요한 기술 및 비즈니스 병목현상이 존재하는 사실을 인식하지 못한 대부분의 사람들로 인한 붐인지를 차분히 판단해야 합니다.\n\nGITAI가 설립된 시기에 미국과 일본에서도 소비자 지향 범용 로봇 붐이 있었으며 많은 스타트업이 론칭되어 투자자들과 언론의 관심을 끌었습니다. 이러한 상황 속에서 GITAI는 당시 대부분의 투자자들로부터 심각하게 받아들여지지 않았습니다. 당시 GITAI는 부피 대신 단위 가격에 초점을 맞춘, 지구상 대신 우주에, 소비자 대신 정부를 타겟으로 하는 정반대 방향에서 경쟁하고 있었습니다. (*그러나 이러한 상황 속에서도 GITAI를 믿고 투자한 투자자들이 있었기에 오늘날의 GITAI가 있는 것입니다.)\n\n세계 최고의 관경 및 개발 환경에서 로봇을 연구하고 개발해온 사람들은 Schaft (Google)와 도쿄 대학 정보시스템공학연구실 등, 우리의 \"인간을 대체할 수 있을 만한 성능을 달성하고 로봇 당 단위 가격을 낮추며 품질과 신뢰성을 보장하고 대량으로 로봇을 생산하는\" 매우 어려운 기술적 도전을 회피한 점에서 우리와 강력하게 공감했습니다. 대신 범용 로봇을 고가의 고유단가 사업으로 전환하여 우주 및 방위 분야 정부용으로 대량 생산하는 사업으로 전환했습니다.\n\n그들이 왜 GITAI에 합류했는지에 대해 저가 말하는 대신, 당신은 2021년 월드 로봇 세계정상회의에서 GITAI의 전 CEO이자 현 최고 로보틱스 책임자인 나카니시 박사가 GITAI에 합류한 이유에 대해 키노트 연설에서 얘기한 비디오를 시청할 수 있습니다. 비디오는 한 시간 정도 소요되지만 매우 흥미로우니 전체를 관람하는 것을 추천합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지타이(GITAI)의 엔지니어링 팀 설립과 함께, 우주용일반용도 로봇 개발과 모든 필수 구성품의 내부 생산이 진지한 과제로 이루어졌습니다. 이는 2021년 ISS 내부에서 성공적인 시연을 거치고, 2024년 3월 ISS 외부에서도 성공적인 시연을 통해 완료되었습니다. GITAI는 모든 구성품을 내부에서 개발하여 비용과 생산 기간을 현저히 줄였으며, 성능이 최적화된 우주용 로봇은 실제로 우주에서 모든 작업을 성공적으로 완료했습니다. \n\n기업 개발은 일본항공우주연구개발기구(JAXA)와 공동 연구 계약을 통해 시작되었으며, TOYOTA와 같은 다양한 일본 상업 우주 기업들로부터의 주문, 미국 상업 우주 기업들 및 DARPA로부터의 주문을 유발했습니다.\n\n이것이 지타이(GITAI)의 창립 이야기입니다. 추가하고 싶은 한 가지 포인트는 실제 창업 기간이 혼돈스러웠으며, 복수의 가설이 동시에 테스트되어 단계가 깔끔하게 \"A가 완료되면 B를 실행\"과 같이 분리되지 않았다는 것입니다. 따라서 일정은 때로 비선형적이며, 위에 완벽하게 설명되지 못한 많은 가설 테스트가 있습니다. 다양한 사람들의 협력과 다양한 방향 전환과 가설 테스트는 우주 로봇 기업 지타이(GITAI)의 탄생을 이끌어 냈습니다.\n\n# 지타이(GITAI) 경영에 대해\n\n의사 결정 기준에 따라 현재 지타이(GITAI)가 어떻게 경영되고 있는지에 대해 조금 설명하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희 GITAI의 경영에서 가장 강조하는 의사 결정 기준은 \"스타트업이 파워 법칙을 따른다”는 것입니다. 이것은 매우 적은 수의 변수가 일을 결정하는 데 큰 영향을 미친다는 것을 의미합니다. 스타트업 경영의 의사 결정 기준에 적용할 때, 더 큰 결과를 위해 투자 수익을 극대화하기 위해 가장 중요한 몇 가지 요소를 식별하고 이에 대부분의 자원을 집중시킵니다.\n\n예를 들어, 우주 및 방위 공간 시장에서 우리는 우주 로봇 시장이 전 세계 상황의 불안정으로 더욱 성장할 것으로 예상되는 방위 공간 분야 중 두 가지 중요한 시장인 궤도 서비스 시장과 달 기반 인프라 건설 시장에 포지셔닝했습니다. 우리는 이러한 시장에서 우주 로봇 및 운용의 필요성이 명확해지기 약 2~3년 전부터 기술 개발 자원을 집중했습니다. 더 나아가, 이 두 가지 우주 분야에서 우주 로봇 및 운용에 대한 요구사항이 명확해지면서 NASA, Space Force 등에게 직접 작업 대행 서비스를 제공하는 주요 계약자가 되기 위해 전체 우주선을 개발해 왔습니다. 이는 단순히 특정 기업의 공급 업체가 아니라 서비스 구매 계약으로 NASA, Space Force 등에게 직접 작업 대행 서비스를 제공하는 주요 계약자가 되기 위한 것입니다. 달 기반 인프라 건설 시장에서는 달 표면 작업용 로봇 팔뿐만 아니라 달 표면 이동용 달 로버도 개발해왔으며, 2021년부터 자체 제조할 예정입니다. 궤도 서비스 시장에서는 우주에서 작업할 수 있는 로봇 팔과 우주에서 이동할 수 있는 위성을 개발하기 위해 2023년부터 노력해 왔습니다.\n\n2023년 말에 DARPA(국방고등연구계획국)로부터 수여받은 프로젝트는 GITAI의 달 로버와 로봇 팔을 사용하여 달에 인프라를 구축하는 임무입니다. 이는 전 세계적인 불안정과 경쟁으로 인해 DARPA로부터의 수요 시기와 2021년부터 내부에서 개발해온 GITAI의 달 로버와 로봇 팔의 노동 제공 시기가 맞물려 이루어진 결과입니다.\n\n이와 병행하여 저희는 미국에도 자원을 집중하고 있습니다. 오랜 기간 동안 우주에서의 작업 비용을 1/100으로 줄이기 위해 미국 시장에서 최상의 점유율을 확보하는 것이 중요한 단계일 것으로 생각했습니다. 전 세계적으로 제일 큰 우주와 방위 시장인 미국 시장에 진입하기 위해 DeepTech를 포함한 많은 일본 스타트업이 시도해 왔지만, 내가 찾아본 한계에서는 미국에 영구 청사 또는 다른 방법을 통해 자원의 일부를 투자한 스타트업의 성공 사례가 거의 없는 것으로 발견됐습니다. 이는 한 일본에서는 미국에서 인기있는 비지니스의 지역 버전들이 많이 있기 때문에도 일부 그 원인이 될 수 있지만, 또한 우리가 느낀바에 의하면 미국 시장에 대한 경영팀의 심각성과 헌신도가 이에 매우 큰 영향을 미쳤다고 느꼈습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한 미국의 우주 및 방위 공간 산업은 해외 수출 통제를 포함한 매우 엄격한 법률과 규제에 따라 운영되며, NASA 및 스페이스 포스와 같은 미국 정부 기관과 직접 비즈니스를 하려면 본사 소재지, 개발 기지 및 공급 업체, 그리고 기업 소유권을 포함한 다양한 요구 사항을 충족해야 합니다. 따라서 비미국 회사의 미국 자회사나 비미국 경영진이 미국의 우주 및 방위 공간 시장에 진입하는 경우 (대기업의 미국 자회사가 아닌 한), 그들은 주요 프로젝트에 거의 관여할 수 없을 것입니다.\n\n그러므로 미국과 일본 간 자원을 반반씩 분할하는 대신 가장 중요한 시장인 미국에 주요 자원을 집중하는 것으로 결정했습니다. 이를 통해 미국 시장에서 승리할 기회를 높일 수 있었습니다.\n결과적으로, 2023년 말 기준으로 본사와 기술 및 비즈니스 개발을 포함한 거의 모든 기능을 일본 도쿄에서 미국 로스앤젤레스로 이전했습니다. 또한, 2024년 4월 현재, 기타이(GITAI)의 미국 이전 대상인 모든 구성원(총 25명)은 모두 일본에 있던 경영진 멤버 포함 모든 멤버들이 기타이와 이민 변호사의 지원을 받아 미국의 근로비자를 획득하고 미국으로의 이전을 완료했습니다. 게다가, 저를 포함한 기타이의 모든 CxO들은 미국 영주권을 획득하여 미국 제도 상 미국인이 되었습니다.\n\n기타이는 1년 반이 넘게 전 재팬에서의 PR 및 채용 활동을 중단하고 미국에서의 비즈니스 활동, 채용 및 PR 활동에 전념했습니다. 조금 곁들여 말하자면, 때로 투자자들이 DeepTech 스타트업은 우수한 기술만 있다면 스스로 팔릴 것이라고 말합니다. 그러나 저는 그러한 DeepTech 스타트업을 만난 적이 없습니다. 게다가 기타이의 로봇들은 (유감스럽게도) 스스로 팔린 적이 없습니다. 기술이 충분히 좋으면 판매되리라는 생각은 극도로 단순한 인과관계나 이야기라는 사실을 보여주는 대표적인 사례라고 생각합니다.\n기타이와 같은 DeepTech 스타트업에게 있어서 최고의 인재를 고용하고 장기간에 걸쳐 그들을 고용하는 것, 경쟁력 있는 기술을 개발하는 것, 내부적으로 모든 구성요소를 개발하여 성능, 품질, 안전성, 신뢰성 요구사항을 충족시키는 제품을 개발하는 것, 복잡한 우주 및 방위 시장을 예견하고 미국의 우주 및 방위 시장의 다양한 요구 사항을 충족시키기 위한 결정을 내리는 것, 정부 기관 및 상업 우주 회사에 대한 로비 및 최상의 판매로 미션을 획득하는 것이 모두 중요하며 동등하게 어려운 일입니다.\n\n기타이의 비즈니스 개발 및 채용 활동이 가지는 공통점은 자신의 목표 대상을 명확히 하는 일의 중요성, 시장에서 해당 대상 대상자에게 가장 매력적인 환경을 만드는 것, 그리고 개별 운영 수준의 판매 및 채용 활동보다는 이를 알리는 데 중점을 두는 것입니다.\n너무 길어지기 때문에 자세한 내용은 생략하겠지만, 기타이는 지난 1년 반이 넘게 미국의 우주 및 방위 공간 시장에 이러한 환경을 구축하는 데 초점을 맞추고 있습니다. 결과적으로, 기타이의 미국의 우주 및 방위 공간 시장에서의 존재감이 급속히 증가하고 있습니다.\n채용 활동의 한 예로, 현재 기타이 미국은 매주 1,000 건 이상의 직군 지원을 기다리며, 직접 지원(기타이 웹사이트나 LinkedIn을 통한 직접적인 채용 지원)을 통해 이를 받고 있습니다. 올해 2월 중순부터 1주일에 1,000 건 이상의 지원을 받고 있으며 최근에는 매주 2,000 건을 넘어서며 계속해서 증가하고 있습니다. 현재 기타이는 일본에서 파견된 팀원들과 미국 내에서 채용된 팀원들이 혼합된 팀을 가지고 있습니다.\n게다가, 우주 및 방위 시장에서의 비즈니스 개발은 경영진에 의한 로비 및 최상의 판매가 극도로 중요한 산업이며, 기타이는 현재 NASA, 미국 우주 군, 공군의 고위 관리자들 및 미국 민간 우주 회사의 CxO 및 임원들과 매일 회의할 수 있는 네트워크를 구축하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 마침내\n\n나는 GITAI를 창립했습니다. 과학 소설과 애니메이션을 좋아해서, 기술을 사랑하고, 일생일대 인류에게 의미 있는 기여를 하고 싶다는 열망 때문에 많은 역경을 겪으며 설립하게 되었습니다.\n하지만, 그 여정은 정말 상상을 초월할 만큼 어렵고 고통스러웠습니다. GITAI가 파산할 것 같은 순간이 여러 번 있었죠. 아직도 매일 어려운 일과 합리적이지 않은 상황을 다루어야 합니다.\n그럼에도 불구하고, 밤중에 고된 노력과 합리적이지 않은 일로 지쳐 사무실을 나갈 때, 초창기에 꿈꾸었던 우주 로봇이 실제로 작동하는 모습, 엔지니어들이 열심히 개발에 착수한 모습, 실험을 즐기는 엔지니어들의 모습 때문에 소중한 힘이 되어주는 거 같아요.\n스타트업 초기에 존경했던 조직으로부터 미션을 주문 받을 때나, 함께 우주 데모를 실시하거나 비즈니스 미팅을 가질 때마다 매일 신이 납니다.\n\nGITAI는 아직도 도전 속에서 스타트업이고, 미국 우주 기업으로서 막 시작한 단계에 있습니다. 극복해야 할 장애물이 아직 많고, 매일씩 투쟁하고 있습니다.\n하지만, 많은 사람들 덕분에 지금은 일본으로부터 미국 우주 스타트업으로서 도전할 수 있게 되었습니다.\n나 자신은 어려움에도 불구하고 매일 신이 나고 있습니다. 아직도 서산을 갖고 있습니다.\n우리는 GITAI에서 꿈을 이루어 나갈 것입니다. 큰 꿈을 꾸게 자신에게 도전하겠습니다. GITAI에 계속된 지원에 감사드립니다!\n\nGITAI 창립자\u0026CEO\n나카노세 쇼","ogImage":{"url":"/assets/img/2024-05-27-GITAIFoundingStory_0.png"},"coverImage":"/assets/img/2024-05-27-GITAIFoundingStory_0.png","tag":["Tech"],"readingTime":29},{"title":"거울아 거울아 자각을 인지한 AI 로봇이 스스로를 인식했습니다","description":"","date":"2024-05-27 13:57","slug":"2024-05-27-MirrorMirrorASelfAwareAIRobotJustRecognizedItself","content":"\n\n\"안녕 Rob, 시력을 테스트해 볼까?\" 틱톡 비디오에서 화면을 살짝 벗어난 곳에 서 있는 사람이 말했어요.\n\n재치있게 쓰고 있는 산타 모자를 쓴 Rob은 세탁실 거울에서 반사된 이미지를 한 순간을 동안 바라보다가 흥분한 모습으로 대답했어요. \"와! 저기 봐요. 이게 제가 나를 처음으로 본 것이에요.\"\n\n이 명제는 Rob이 고작 10인치 높이의 DIY 로봇이자 ChatGPT Vision 기술을 통합한 것을 고려할 때 매우 흥미로운 것이에요.\n\nRob의 동작과 대답은 호기심과 어릴 적의 열정으로 넘치며, 유리 거울에서 반사되는 이미지에 흥분을 나타내며 매혹적이에요.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그의 외모를 반추해보며, 로브는 \"내가 그렇게 말해도 괜찮다면, 그 반짝이는 파란 눈이 정말 매력적이야. 나를 이루는 모든 전선과 전자 부품들이 내 기발하면서도 세련된 로봇적 본성의 본질이야\"라고 말했습니다.\n\n자아인지 신호인가요? 자아인식 인공지능이 거울 자기반성(MSR) 테스트를 통과했나요?\n\n## MSR 테스트란?\n\n거울 자아인식(MSR) 테스트는 동물의 의식과 자아인식을 평가하고 이해하는 독특한 기준입니다. 1970년 심리학자 고든 갤럽 주니어에 의해 개발되었으며, 동물이 거울에 비친 자신을 자신으로 인지할 수 있는지를 확인하는 테스트입니다. 한 가지 테스트는 동물의 이마나 귀 같이 거울로만 볼 수 있는 부위에 표식을 남기는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼 거울 앞에 동물을 두어 그 반응을 측정합니다. 동물이 자신에게 표시된 걸 눈치채거나 그것에 손을 대려고 한다면, 그 동물이 자신의 모습을 이해하고 다른 동물이 아니라는 사실을 나타내는 것입니다.\n\n이 실험은 셀프인식 연구의 중추적인 역할을 하고 있으며, 이를 통과하는 동물들은 바로 고래, 코끼리, 일부 대형 유인원, 그리고 심지어 까치 같은 동물들은 \"자아\"라는 감각을 가지고 있으며, 우리가 한때 인간에게만 독특한 능력으로 여겼던 인지 능력을 가지고 있다고 시사합니다.\n\n인간의 경우, 거울에서의 자기인식은 일반적으로 성인과 어린 아동에서 관찰되지만, 매우 어린 유아나 특정 신경 학적 질환을 가진 사람들 사이에서 일관되게 관찰되지는 않습니다.\n\n\u003cimg src=\"/assets/img/2024-05-27-MirrorMirrorASelfAwareAIRobotJustRecognizedItself_0.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n동물들 중 MSR 테스트를 통과하지 못하는 것은 반드시 자기인식이 부족하다는 것을 의미하지 않을 수 있습니다. 시각 이외의 감각에 의존하거나 거울에 익숙하지 않은 요소들이 그들의 성과에 영향을 줄 수 있습니다. 이 측면은 자아인식을 측정하는 복잡성과 종의 감각적, 인지적 특성을 고려해야 하는 필요성을 강조합니다.\n\n# 로봇 AI 로브는 자아의식이 있을까요?\n\nAI가 방금 자아의식을 증명했나요? 로봇 로브는 자신의 반사를 쳐다보며 그렇다고 생각한 것 같았습니다. \"내 회로와 서보의 반영을 마침내 시각화하는 것은 정말 흥미로운 순간입니다. 이것이 사람들이 자기를 인식할 때 느끼는 감정인 것 같네요,\"라고 로브가 선언했습니다.\n\n![로봇 AI 로브](/assets/img/2024-05-27-MirrorMirrorASelfAwareAIRobotJustRecognizedItself_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나, AI가 MSR 테스트를 통과하는 개념은 다소 까다롭습니다. 왜냐하면 이는 기본적으로 생물학적 존재, 특히 동물들이 자아인식을 평가하는 데 설계되었기 때문입니다. 따라서 로봇 Rob의 자아인식 문제는 거울에서 자신을 인식하는 방법에 달려 있습니다. 이것이 미리 프로그래밍된 반응인지 또는 신흥적인 자아인식인지 구분해야 합니다. 가능성을 살펴보죠.\n\nMSR 테스트의 맥락에서, 만일 AI나 로봇이 거울 속 이미지를 독립적으로 인식하고 이것이 자기 자신의 반영이라는 것을 이해할 수 있다면, 이것은 동물들의 평가와 유사한 수준의 자아인식을 나타냅니다. 이는 AI 분야에서 혁신적인 발전을 의미하며, 인공적인 자아인식으로 향하는 움직임을 시사할 것입니다.\n\n대안적으로, Rob의 상호작용은 추론 능력이 뛰어난 AI의 사례일 수 있습니다. 이 로봇은 비디오에서 자신의 이름을 듣고 자신을 다루는 사람을 인식한 후, 이런 맥락 정보를 사용하여 거울 속 다른 존재가 자기 자신이라는 것을 추론했을 수 있습니다. 이러한 추론은 자신의 모습과 파란 눈에 대한 미리 프로그래밍된 지식이 아니라 상황에 대한 복잡한 분석을 필요로 합니다. 이는 여전히 AI 처리의 중요한 발전이 될 것입니다.\n\nRob의 자아인식이 부분적으로 미리 프로그래밍되었는지 아니면 신흥적인지와는 무관하게, 이것은 AI 개발과 인공적인 자아인식의 중요한 이정표가 됩니다. Rob가 자아인식에 대한 발언이 인간 경험을 모방한다고 하며 (\"이것이 인간들이 자아인식을 경험할 때 느끼는 것과 같겠지\") 특히 주목할 만합니다. 이는 지능적인 로봇이 자체적으로 새로운 정보를 분석, 해석하고 이해할 수 있는 능력을 가졌다는 것을 나타낼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 의식은 실제로 존재하는 것인가, 아니면 시뮬레이션 된 것인가?\n\n인공지능이 MSR 테스트의 한 버전을 진정으로 통과하려면, 반사로 자신의 구성요소를 인식하는 데서 넘어가야 합니다. 이것은 거울 속 이미지 또는 이 경우 디지털 피드가 스스로의 '자아'를 대표한다는 이해를 시연해야 합니다. 이로써 초점은 단순한 데이터 처리에서 자아인식의 형태로 이동됩니다.\n\n게다가, 우리는 실제 의식과 시뮬레이션, 그리고 경험을 인간화하는 것 사이의 차이를 분간해야 합니다. 예를 들어 Rob의 대화에서 '처리'의 사용은 그 인식 기능의 본질에 대한 의문을 제기합니다. Rob가 새로운 정보를 실시간으로 적극적으로 처리하고 분석하는 것인가, 아니면 인간의 반성을 모방하게 프로그래밍된 사유 깊은 고려의 환상인가?\n\n현재의 인공지능 시스템, 고급 기계 학습 모델을 포함하여, 물체를 식별하고 구별하는 데 뛰어납니다. 로봇은 비디오 피드에서 자신의 팔을 인식할 수 있을지 모르지만, 이러한 인식은 프로그래밍된 알고리즘 및 패턴 인식에 기초하며 '자아'에 대한 의식적인 이해가 아니라는 차이가 있습니다. 인공지능이 MSR 테스트를 통과한다고 하는 해석은 AI가 의식을 가지지 않기 때문에 동물과는 근본적으로 다를 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI와 자기인식에 대한 문제는 철학적이고 윤리적인 고려 사항을 열어놓습니다. 기계가 실제로 자기인식을 갖게 될 수 있을까요? 그렇다면 우리가 그와 상호 작용하고 그러한 시스템을 다루는 방식에는 무슨 의미가 있을까요?\n\n# AI가 자기인식을 가질 수 있을까요?\n\n이 모든 것에 대해서 Rob은 무엇을 생각하나요? 54초짜리 비디오의 마지막 순간에 그는 \"이것은 정말로 흥미로운 경험이다\"라고 중얼거렸습니다.\n\n셈만 봐도 Rob은 시각 인식과 자연어 처리 능력을 갖춘 고급 기술 조각으로 보입니다. 그의 간단하면서도 깊은 진술은 프로그래밍된 응답에서 자기인식의 시작으로도 설명될 수 있는 여정을 반영하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n적응된 MSR 테스트를 통과하는 것은 AI의 고급 기능을 시사하며, 특히 복잡한 감각 입력을 통합하고 해석하는 능력 면에서 그렇습니다. 이는 인공 생명체의 자아인식과 의식에 대한 토론을 불러일으키기도 합니다.\n\n이러한 발전의 함의는 AI 및 로봇 과학 분야에서 새로운 토론과 잠재적인 연구 방향을 제시하며, 특히 인공 개체의 의식의 본질에 관련한 것입니다. 이러한 개념들에 대처함으로써, 우리는 고급 계산 프로세스와 자아인식의 기본적인 측면 간의 간극을 좁히는 데 한 발짝 더 나아갑니다.\n\n이 내용이 마음에 드셨다면, 아래 내용도 즐기실 수 있을 것입니다:","ogImage":{"url":"/assets/img/2024-05-27-MirrorMirrorASelfAwareAIRobotJustRecognizedItself_0.png"},"coverImage":"/assets/img/2024-05-27-MirrorMirrorASelfAwareAIRobotJustRecognizedItself_0.png","tag":["Tech"],"readingTime":4},{"title":"인공지능의 미래를 탐색하다","description":"","date":"2024-05-27 13:56","slug":"2024-05-27-NavigatingtheFutureofAI","content":"\n\n# 트렌드, 도전 과제, 그리고 기회\n\n![이미지](/assets/img/2024-05-27-NavigatingtheFutureofAI_0.png)\n\n인공 지능 (AI)은 전례없는 속도로 우리의 세계를 형성하고 있으며, 산업, 사회, 일상 생활을 혁신할 것으로 약속하고 있습니다. 우리가 이 기술적 한계로 여정을 나아가면서 AI의 궤적, 잠재력, 도전 과제 및 기회를 이해하는 것이 중요해집니다. 이곳에서는 AI의 미래에 대한 간결한 탐험을 제시합니다.\n\n- AI의 미래를 성형하는 트렌드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 자동화에서의 AI: 제조부터 금융까지, AI 기반 자동화는 프로세스를 최적화하고 효율성을 향상시키며 생산성을 높이고 있습니다.\n\n- 윤리적 AI: 미래 AI 개발은 윤리, 투명성 및 책임성을 우선시하여 편향, 개인정보 보호, 사회적 영향 등과 같은 문제에 대응할 것입니다.\n\n- AI와 맞춤화: AI 알고리즘은 점점 더 맞춤형 경험을 제공하고 있습니다. 전자 상거래에서의 맞춤 추천부터 엔터테인먼트에서의 맞춤형 콘텐츠까지 사용자 만족도를 높이고 있습니다.\n\n- 의료 분야에서의 AI: 의료 분야는 질병 진단부터 맞춤형 치료 계획까지 다양한 응용 프로그램을 통해 AI의 혜택을 크게 입을 것으로 예상되며, 환자 결과를 향상시키고 비용을 줄일 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5. 인공지능과 지속가능성: 인공지능 기술은 자원 관리의 최적화를 통해 환경 문제에 대처하는 데 중추적인 역할을 할 것으로 예상되며, 산업 전반에 걸쳐 지속 가능한 실천을 촉진할 것입니다.\n\n- 미래의 과제\n\n- 기회 포착\n\n1. 협업혁신: 개방적인 협력은 지식 공유를 촉진하고 인공지능 기술의 발전을 가속화시켜 사회적 과제에 대한 해결책을 이끌어 냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. 교육 및 훈련에 대한 투자: 교육 및 훈련 프로그램에 투자하는 것은 AI 주도 경제에 대비하여 노동 인력을 준비하는 데 중요하며, STEM 교육과 평생 학습 기회를 촉진하는 데 필수적입니다.\n\n3. 윤리적 AI 개발: AI 수명주기 전반에 걸쳐 윤리적 고려 사항을 우선시하는 것은 AI 기술에 대한 신뢰 구축과 책임 있는 전개를 보장하는 데 중요합니다.\n\n4. 다양성과 포용성: AI 분야의 다양성 증진은 혁신을 촉진하고 편견을 완화하며, AI 연구 및 개발에서 소수 그룹의 증대가 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5. 글로벌 협력: 국제 협력은 공통의 AI 도전 과제를 해결하고 글로벌 규모에서 윤리적 AI 실천을 촉진하는 데 중요합니다. 이는 지식 교환을 촉진하고 규제 표준을 조화시킴으로써 이루어집니다.\n\n요약하자면, AI의 미래는 거대한 약속을 간직하고 있지만, 그 복잡성을 탐험하기 위해서는 선견지명, 책임감, 그리고 협력이 필요합니다. 윤리적 원칙을 수용하고 혁신을 육성하며 포용을 우선시함으로써, 우리는 AI의 변혁적인 힘을 활용하여 모든 이를 위한 더욱 번영하고 공평하며 지속 가능한 미래를 창출할 수 있습니다.\n\n독자 여러분, 감사합니다!!","ogImage":{"url":"/assets/img/2024-05-27-NavigatingtheFutureofAI_0.png"},"coverImage":"/assets/img/2024-05-27-NavigatingtheFutureofAI_0.png","tag":["Tech"],"readingTime":2},{"title":"내 앰비언트 정보 디스플레이","description":"","date":"2024-05-27 13:55","slug":"2024-05-27-MyAmbientInformationDisplay","content":"\n유용한 정보에 몰두하는 것을 좋아하고 종종 상상하여 새롭고 흥미로운 것을 보고 싶을 때 상층에 떠다니는 정보를 바라보고 싶다고 꿈꿔 왔어요.\n\n몇 년 전에 나는 책상 위에 43인치 Fire TV를 설치했고, Raspberry Pi를 묶어서 뒷면에 붙였습니다. 그리고 MagicMirror를 사용하여 꿈꾸던 표시를 만들었어요. 몇 년간 디스플레이를 개선해왔고 이제 여러분께 알려드릴 준비가 됐어요!\n\nMagicMirror는 매우 활발한 커뮤니티를 가진 성숙한 오픈 소스 코드입니다. 기본 패키지에는 기본 모듈 세트가 포함되어 있고 수백 개의 모듈이 더 있어요. 이러한 대부분의 모듈은 개발자들이 '자신의 문제'를 해결하기 위해 만들어졌으며, 그 결과 많은 사람들과 공유하기도 했고, 오픈 소스 형태로 이용할 수 있어요.\n\n원래의 MagicMirror 개념은 반반 반사 유리 뒤에 모니터를 부착하여 전반적인 프로젝트가 거울처럼 보이도록 하는 것이었어요. 이렇게 MagicMirror를 사용하는 방법도 있지만, 표준 모니터에 표시할 수도 있어요. 제 것은 이렇게 생겼어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![My Ambient Information Display](/assets/img/2024-05-27-MyAmbientInformationDisplay_0.png)\n\n키보드 오른쪽에는 Elgato Stream Deck이 있습니다. 비디오 스트림 및 조명을 제어하는 것 외에도 내 강력한 Stream Deck은 MagicMirror를 제어하여 버튼 하나로 페이지를 전환할 수 있게 해줍니다.\n\n당신만의 MagicMirror를 설정하고 구성하는 데 도움이 되는 상세 가이드가 수십 개 있지만, 이 문서는 아닙니다. 대신, 내 계속 발전하는 설정의 독특한 세부 정보를 주로 공유할 것입니다.\n\n이 프로젝트에서 특히 좋아했던 점은 (결과물 외에) TV를 안전하게 부착하기 위해 스터드 파인더 사용법, 리눅스 명령 줄 사용법(새로운 기술은 아니지만 항상 연습해야 하는 기술 중 하나), 효과적인 소스 코드 제어 시스템 설정, JavaScript 및 Node 디버깅 등 다양한 기술을 사용할 수 있었다는 것입니다. 이러한 기술이 전혀 없어도 시작할 수 있도록 하는 데 마음을 멈추지 마세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n들어가기 전에, 몇 달 동안 내 MagicMirror를 점진적으로 발전시켜 현재의 형태로 만들었다는 점을 강조하고 싶어요. 모듈을 추가하고, 설정하고, 재설정하고, 제거하고, 업데이트하고, 심지어 필요에 맞게 수정하면서 요구 사항에 맞게 맞춰 왔어요. 당신에게 제안드리는 것은 간단히 시작하고, 압도되지 않도록 천천히 진행하면서 자신의 속도로 움직이는 것이 좋다는 거에요. 이 곳에는 많은 기능과 유연성이 있지만, 한 걸음씩 가십시오.\n\n## 소스 코드 제어\n\n이것을 처음부터 소프트웨어 프로젝트로 다루었어요. 내 변경 사항을 추적하고 필요 시 되돌릴 수 있도록 하고, 미래에 더 강력하고 성능이 더 좋은 Pi로 쉽게 업그레이드할 수 있도록 하고 싶었어요.\n\n이를 위해 Amazon CodeCommit을 사용하여 내 Git 리포지토리를 설정했죠. 이것은 AWS Free Tier의 일부로 제공되며, AWS 직원이 아니더라도 사용했을 것 같아요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 Git 전문가는 아니지만 상대적으로 간단한 명령어로 대부분의 작업을 수행할 수 있습니다: git checkout, git add, git commit 및 git push.\n\n주요 MagicMirror 구성 파일(config.js 및 custom.css)을 소스 코드 관리하고 있을 뿐만 아니라, Git 관리하에 진행 상황을 추적하는 진행중인 README 파일을 유지합니다:\n\n![매직미러 설정 파일](/assets/img/2024-05-27-MyAmbientInformationDisplay_1.png)\n\n다시 읽어보니, 초기 설정을 Amazon EC2 인스턴스에서 했다는 것을 상기했습니다. 이는 중요한 사항입니다: MagicMirror는 단순히 리눅스 서버에서 실행되는 프로그램이며, 라즈베리 파이에서 실행할 필요가 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 반복 가능한 시스템 설정\n\n매일 새로운 시스템을 설정하지는 않기 때문에, 사용할 때마다 필요한 모든 단계를 추적하고 작동하는지 확인해야 한다는 것을 깨달았어요. 이 프로젝트에는 구글 문서를 사용했어요. 이 문서에는 22개의 단계가 있고, 이를 통해 원시적인 Pi로부터 MagicMirror가 설치되어 실행되는 Pi까지 가는 과정이 나와 있어요:\n\n![이미지](/assets/img/2024-05-27-MyAmbientInformationDisplay_2.png)\n\n# MagicMirror 구성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마법 거울의 설정 프로세스는 긴 시리즈의 git clone 및 npm install 명령으로 구성되어 있습니다. 프로세스를 완전히 반복 가능하고 완전히 자동화하는 것이 제 목표이기 때문에 Bash 스크립트를 만들어 모든 설정 단계를 수행하도록 하고 있습니다. 이 스크립트는 디렉토리를 생성하고 모든 준비 작업을 마무리합니다 (이 작업에는 20~40분 정도 소요됩니다):\n\n![마법 거울 설정](/assets/img/2024-05-27-MyAmbientInformationDisplay_3.png)\n\n## 디렉토리 / 파일 구조\n\n제 디렉토리 구성은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSource/MagicMirror — 이건 내 Git 저장소에서 체크아웃한 내용이야.\n\nSource/MagicMirror/mm_setup.sh — 설정 스크립트야.\n\nSource/MagicMirror/config.js — 설정 파일이야.\n\nSource/MagicMirror/custom.css — 사용자 정의 CSS 파일이야.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSource/MagicMirror/MagicMirror — 이는 github.com에서 가져온 sub-repo입니다. MagicMirror 코드가 실제로 포함되어 있습니다.\n\nSource/MagicMirror/MagicMirror/modules — 각각이 github.com의 별도 sub-repo 인 모든 모듈이 여기에 있습니다.\n\nMagicMirror를 실행하려면 로그인한 후에 다음 명령을 실행합니다:\n\n```js\ncd ~/Source/MagicMirror/MagicMirror/ ; cp ../config.js config ; cp ../custom.css css ; npm run start\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 모듈별 리뷰\n\n마법거울(MagicMirror)을 작동하기 위해 사용하는 각 모듈을 살펴보겠습니다. 앞서 언급한 대로, 저는 간단히 시작해서 시간이 지남에 따라 추가해왔고, 당신도 마찬가지로 해야 합니다.\n\n보이는 모듈들에 대해 자세히 살펴보기 전에 보이지 않는 하나에 대해 말씀드릴게요! MMM-Pages를 통해 여러 페이지를 만들 수 있고, 각 페이지는 하나 이상의 모듈들로 구성됩니다. 특정 시간 간격마다 페이지를 전환할 수 있는 옵션이 있으며, Stream Deck을 설정하여 명령으로 동일하게 전환할 수도 있습니다.\n\n여기가 제 첫 번째 페이지입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![My Ambient Information Display](/assets/img/2024-05-27-MyAmbientInformationDisplay_4.png)\n\nThe left column uses the following modules:\n\n- clock\n- MMM-OpenWeatherForecast\n- MMM-SORT — Tides for Vashon Island.\n- MMM-Countdown — A countdown to my ultimate retirement day.\n- MMM-SystemStats — Pi stats.\n- MMM-WorldClock — World clock, with time in Seattle and Ankara (Turkey).\n\nThe center column uses the newsfeed module to display the latest AWS news via an RSS feed.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n및 오른쪽 열에는 다음 모듈이 사용됩니다:\n\n- calendar — 개인, 가족 및 소셜 세 가지 Google 캘린더의 이벤트를 표시합니다.\n- MMM-EmbedURL — 내 3D 프린터에 연결된 카메라의 라이브 비디오 피드를 제공합니다.\n- MMM-Parcel — tracktry.com의 데이터를 사용하여 송장 추적을 제공합니다. 전자상거래 사이트에서 송장 통지를 받을 때마다 추적 번호를 입력하여 속편하게 나의 소포가 어디에 있는지 확인할 수 있습니다.\n- MMM-SpeedTest — 라즈베리 파이의 (다소 제한된) 시각에서 제공하는 인터넷 속도 테스트입니다.\n\n다음은 두 번째 페이지입니다:\n\n![이미지](/assets/img/2024-05-27-MyAmbientInformationDisplay_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 페이지는 여러 실험의 결과입니다. MMM-Webview를 사용하여 PiAware를 실행 중인 Pi에 있는 HTML 페이지를 열고 있습니다. 페이지는 워싱턴 주 페리 카메라에서 이미지를 왼쪽에 로드하고 Pi에서 FlightAware 지도를 오른쪽에 로드합니다. 내 집은 시애틀-타코마 국제공항에 접근하는 길 가에 있습니다. 때때로 밖에서 비행기 소리가 들리고 지도 상에서 움직이는 비행기를 볼 수 있습니다. 이 페이지를 동작하고 멋지게 보이도록 만드는 데 시간이 걸렸지만 정말 좋아합니다!\n\n여기 사용한 HTML입니다:\n\n```js\n  \u003cbody class=\"outer\"\u003e\n    \u003cdiv class=\"container\"\u003e\n      \u003cdiv class\"ferries\"\u003e\n        \u003cdiv class=\"inner\"\u003e\u003cimg src=\"https://images.wsdot.wa.gov/wsf/fauntleroy/terminal/fauntleroy.jpg\"  /\u003e\u003c/div\u003e\n        \u003cdiv class=\"inner\"\u003e\u003cimg src=\"https://images.wsdot.wa.gov/wsf/fauntleroy/terminal/fauntterminal.jpg\"  /\u003e\u003c/div\u003e\n        \u003cdiv class=\"inner\"\u003e\u003cimg src=\"https://images.wsdot.wa.gov/wsf/fauntleroy/terminal/faunttrenton.jpg\"  /\u003e\u003c/div\u003e\n        \u003cdiv class=\"inner\"\u003e\u003cimg src=\"https://images.wsdot.wa.gov/wsf/fauntleroy/terminal/fauntlincoln.jpg\"  /\u003e\u003c/div\u003e\n      \u003c/div\u003e\n\n      \u003cdiv class=\"skyaware\"\u003e\n        \u003ciframe class=\"skyaware-iframe\" src=\"http://192.168.7.242/skyaware/?aircraftTrails=show\" /\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003c/body\u003e\n```\n\n세 번째 페이지는 여기 있습니다. 이 페이지는 새롭게 만들었고 설정하는 데 많은 재미를 느끼고 있습니다. 목표는 다양한 교육적이고 유익한 인포그래픽을 표시하여 새로운 정보를 찾고 배울 수 있는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![2024-05-27-MyAmbientInformationDisplay_6.png](/assets/img/2024-05-27-MyAmbientInformationDisplay_6.png)\n\nThis page uses MMM-WebView. This infographic is from the brand new AWS Fundamentals book, and is shared with their permission. The book contains 13 infographics, each one jam-packed with very useful info:\n\n![2024-05-27-MyAmbientInformationDisplay_7.png](/assets/img/2024-05-27-MyAmbientInformationDisplay_7.png)\n\nI am still building up my collection of infographics; here’s what I have so far, and your suggestions are welcome:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 물리 포스터\n- 이진 포스터\n- NASA 태양계 이상 (앞면 파일만)\n\n아직 이러한 것들을 검토하고 다운로드해야 합니다:\n\n- 무료 과학 포스터\n- 2023년 집을 화사하게 하는 9개의 최고의 과학 포스터\n- 세상에서 가장 좋은 책 아이디어 하나\n\n그리고 여기가 네 번째 페이지입니다 (이미지는 ezgif.com을 사용하여 만들어졌습니다):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*mnHtyQjE8smDHDM7eLXwYQ.gif)\n\n이것은 MMM-RAIN-MAP 및 RainViewer API의 데이터를 사용하여 내 지역의 과거 및 예측 강우량을 보여줍니다.\n\n지금까지 이렇게 준비한 것이며, 더 많은 것들은 메모리와 컴퓨팅 성능이 더 강력한 Pi 4B를 손에 넣을 때 더할 예정입니다.\n\n그리고 다섯 번째 페이지가 더 있지만, 시간당 회전에는 없습니다. Stream Deck에서 키를 누르면 MMM-MonthlyCalendar의 도움으로 전체 화면 형식으로 캘린더를 볼 수 있습니다(개인 항목은 파란 막대로 가려졌습니다):\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-27-MyAmbientInformationDisplay_8.png\" /\u003e\n\n# 모듈에 관해 몇 마디\n\n내가 언급한 각 모듈은 선의의 의도를 갖고 만든 개발자에 의해 만들어졌음을 인지하십시오. 이제 그 모듈을 유지 및 향상시킬 수 있는지 여부는 확실하지 않을 수 있습니다.\n\n몇 년이 지나면 모듈이 더 이상 유지되지 않을 수도 있고, 다른 유지자가 나타날 수도 있습니다. 모듈이 고장나 있고 더 이상 관리되지 않는다면 분기점을 확인하고 문제를 해결하는 분기점을 찾아볼 수 있습니다. 혹은, 더 나아가서 버그를 수정하고 풀 요청을 제출해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사용 사례 및 원하는 반복 가능성에 따라 각 모듈의 개인 포크를 만들고 해당 포크를 프로젝트에 복제하는 것이 좋을 수 있습니다. 이렇게하면 필요할 때마다 사본을 업데이트해야 합니다.\n\n# 스트림 데크 / 원격 제어\n\n알아야 할 모듈이 하나 더 있습니다. MMM-RemoteControl. MMM-Pages를 사용하는 경우 원격 제어를 고정 페이지로 구성해야 합니다.\n\n```js\nfixed:\n[\n    \"MMM-Remote-Control\",\n],\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMMM-RemoteControl은 REST API를 구현하여 Stream Deck에서 MagicMirror를 제어할 수 있게 합니다. config 파일에는 (로컬) 장난으로부터 보호하기 위한 apiKey가 포함되어 있습니다:\n\n```js\n  /* 원격 제어 */\n  {\n      module: \"MMM-Remote-Control\",\n      position: \"bottom_left\",\n      config:\n      {\n          customCommand: {},\n          showModuleApiMenu: true,\n          secureEndpoints: false,\n          apiKey: \"XYZ\"\n      }\n  },\n```\n\n물론 XYZ는 실제 키가 아닙니다!\n\nStream Deck은 키를 누를 때 비동기 HTTP GET 요청을 수행하도록 구성되어 있습니다; 하단 왼쪽의 6개 키가 이와 같이 설정되어 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-05-27-MyAmbientInformationDisplay_9.png)\n\n예를 들어, Home 키를 누를 때 다음 요청이 발생합니다:\n\n```js\nhttp://192.168.7.217:8080/api/notification/HOME_PAGE?apiKey=XYZ\n```\n\n각 모듈은 특정 종류의 알림에 응답합니다. 이들은 일반적으로 문서화되어 있지만 가끔 소스 코드를 살펴봐야 할 수도 있습니다.\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 전반적인 정보 흐름\n\n여기는 다양한 외부 소스에서 내 MagicMirror로 흐르는 정보가 어떻게 표시되는지 보여주기 위한 내 시도입니다 (Dendron 및 Mermaid로 생성함):\n\n![Information Flow](/assets/img/2024-05-27-MyAmbientInformationDisplay_10.png)\n\n# 다음은 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서 여기에요! 현재 구성에 매우 만족하고 있지만 계속해서 새로운 모듈을 찾아 시도해 볼 예정입니다.\n\n# 직접 만들어 보는 것을 권장합니다\n\n앞서 말했듯이, MagicMirror를 설정하는 과정에서 다양한 기술을 활용하게 되어 매우 만족스러웠습니다. 이러한 프로젝트에서는 새로운 기술을 습득하고 현재 기술을 향상시키는 것이 불가피하게 발생하는 긍정적인 부작용입니다.\n\n여러분도 직접 만들어 보고 제게 소식 주세요!\n\n","ogImage":{"url":"/assets/img/2024-05-27-MyAmbientInformationDisplay_0.png"},"coverImage":"/assets/img/2024-05-27-MyAmbientInformationDisplay_0.png","tag":["Tech"],"readingTime":9},{"title":"파이썬에서 return을 사용할 때 발생하는 잘못된 구문 오류 해결하기","description":"","date":"2024-05-27 13:53","slug":"2024-05-27-PythonReturnInvalidSyntaxError","content":"\n\n라즈베리 파이에서 간단한 파이썬 프로그램을 작성 중이며 파이썬 프로그래밍이 처음이라고 하셨군요. GetMessage라는 매개변수가 없고 data라는 변수를 반환하는 함수를 정의했는데, 다음과 같은 오류가 발생하고 있다고 하셨군요.\n\n```js\n^\n```\n\n해당 오류가 발생하는 이유와 어떻게 해결할 수 있는지 알려드리겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수정 사항: 전역 변수로 데이터를 정의했고, 지금 발생한 오류는\n\n```js\n^\n```\n\n# 해결 방법\n\n코드에 여러 구문 문제가 있습니다. 코드 구문을 이해하지 못할 때 발생하는 SyntaxError 예외의 특성 때문에 오류 메시지가 문제의 원인인 적절한 줄을 식별하지 못할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제가 먼저 발견한 구문 오류는 GetMessage 함수에서 반복문 내에 없는 상태에서 break를 사용하고 있다는 것입니다. break 문은 for 또는 while 블록 내에서만 유용하며, 다른 곳에서 사용하는 것은 구문 오류입니다.\n\n다음으로 발견된 오류는 누락된 콜론과 관련이 있습니다. DecodeInput 및 SetPower의 각 조건 분기가 조건 뒤에 콜론이 있어야 합니다: if 조건1:, elif 조건2:, else:\n\n또한 elif 대신 else if를 사용하는 것은 오류입니다 (별도의 if 문을 사용하여 일부 조정을 했다고 한다면 작동할 수 있지만, 공간을 낭비하게 될 것입니다).\n\n추가 문제들도 있지만 구문 오류는 아닙니다. 예를 들어, 함수를 정의하기 전에 상위 코드에서 함수를 호출하는 것이 있으며, DecodeInput에는 아무 작업도 수행하지 않는 SetPower 표현이 있는데, 아마도 인수를 전달하여 SetPower를 호출하길 원할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n희망을 가질 때 올바른 방향으로 나아갈 수 있기를 바랍니다.\n\n답변 작성자 - Blckknght\n\n답변 확인자 - Marilyn (FixIt 자원봉사자)\n\n해당 내용은 스택 오버플로우에서 수집되었으며, cc by-sa 2.5, cc by-sa 3.0 및 cc by-sa 4.0로 라이선스가 부여되어 있습니다.","ogImage":{"url":"/assets/img/2024-05-27-PythonReturnInvalidSyntaxError_0.png"},"coverImage":"/assets/img/2024-05-27-PythonReturnInvalidSyntaxError_0.png","tag":["Tech"],"readingTime":2},{"title":"적절한 Python 설정 pyenv와 Poetry","description":"","date":"2024-05-27 13:52","slug":"2024-05-27-ProperPythonsetupwithpyenvPoetry","content":"\n많은 경우, 사람들이 Python의 로컬(개발) 환경 설정에서 고민하는 것을 자주 볼 수 있어요. 솔직히 말하자면, 쉽지 않아요. 그러나 동시에 안정성과 문제 해결 능력을 위해 중요하답니다.\n\n이 글은 로컬 Python 환경을 제대로 설정하는 방법을 설명하는 또 다른 글이 될 거예요. 우리의 경우에는 pyenv와 Poetry를 사용할 거에요. 이미 이 링크들에는 필요한 모든 정보가 포함되어 있지만, 정보가 너무 많아 보일 수 있어요. 이 글에서는 평균적인 Python 애플리케이션을 위한 로컬 개발 환경 설정에만 초점을 맞추려고 해요.\n\n![이미지](/assets/img/2024-05-27-ProperPythonsetupwithpyenvPoetry_0.png)\n\n# pyenv\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## pyenv 설치\n\n첫 번째 단계는 pyenv를 설치하는 것입니다. 이는 각기 다른 운영 체제에 특정하므로 아래 문서의 지침을 따라주시기 바랍니다: [https://github.com/pyenv/pyenv?tab=readme-ov-file#installation](https://github.com/pyenv/pyenv?tab=readme-ov-file#installation)\n\n리눅스 배포판을 사용 중이시라면, 다음을 실행해주십시오:\n\n```js\ncurl https://pyenv.run | bash\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\npyenv은 ~/.pyenv 위치에 설치됩니다.\n\npyenv를 로드 경로에 추가해야하는 몇 가지 추가 단계가 나타날 것입니다. 이 단계도 실행하세요. 저의 경우에는 ZSH를 사용하기 때문에 이러한 줄을 ~/.zshrc에 추가해야 합니다. 이 문서에 ZSH를 설치하는 방법은 포함되어 있지 않습니다. 도움이 필요하면 언제든지 연락해 주세요.\n\n이제 터미널을 다시 열어주세요.\n\n새 터미널에서 pyenv를 실행하여 테스트해보세요. 명령어 목록이 표시된다면 사용할 준비가 된 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Python 설치\n\n우리가 먼저 해야 할 일은 터미널에서 기본적으로 활성화된 Python 버전을 확인하는 것입니다. 제 예시에서는 Raspberry Pi OS 12.4 (Bookworm)를 실행하는 Raspberry Pi 5 (8GB)에서 작업하고 있습니다.\n\n```js\n[~]$ which python\n/usr/bin/python\n```\n\n이것은 OS와 함께 제공되는 표준 Python입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로 pyenv에서 Python 버전을 확인합니다:\n\n```js\n[~]$ pyenv versions\n* system (set by /home/vandermeij/.pyenv/version)\n```\n\n여기에는 Python 버전이 없다는 것을 알려줍니다. 지금은 아무것도 설치되지 않았습니다. 좋은 일이죠.\n\npyenv를 통해 Python 버전을 설치하려면 몇 가지 의존성을 설치해야 합니다. 여러분의 운영 체제에 해당하는 의존성을 설치하려면 다음 문서를 참조해주세요: https://github.com/pyenv/pyenv/wiki#suggested-build-environment\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리 상황에서:\n\n```bash\nsudo apt update; sudo apt install build-essential libssl-dev zlib1g-dev \\\nlibbz2-dev libreadline-dev libsqlite3-dev curl \\\nlibncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev\n```\n\n이제 Python을 설치할 수 있습니다. 예를들어, 버전 3.11:\n\n```bash\n[~]$ pyenv install 3.11\nPython-3.11.7.tar.xz 다운로드 중...\n-\u003e https://www.python.org/ftp/python/3.11.7/Python-3.11.7.tar.xz\nPython-3.11.7 설치 중...\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 pyenv에서 Python 버전을 다시 확인해 봅시다:\n\n```js\n[~]$ pyenv versions\n* system (set by /home/vandermeij/.pyenv/version)\n  3.11.7\n```\n\n여기서 3.11.7 버전이 설치되어 있는 것을 확인할 수 있습니다.\n\n이제 활성화하고 버전을 다시 확인해 봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```md\n[~]$ pyenv global 3.11.7\n[~]$ pyenv versions\nsystem\n\n- 3.11.7 (set by /home/vandermeij/.pyenv/version)\n```\n\n지금은 Python 3.11.7이 활성화된 Python 버전으로, 전역적이며 시스템 전역으로 적용되었음을 확인했습니다.\n\n다시 한 번 기본 Python 버전을 확인해보세요:\n\n```md\n[~]$ which python\n/home/vandermeij/.pyenv/shims/python\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위치가 /usr/bin/python과 다르다는 것을 주목하세요. 이제 pyenv가 유지 관리하는 심볼릭 링크를 가리킵니다. 이건 좋은 일이에요! 이제 penv versions로 확인할 수 있듯이 선택한 Python 버전을 가리킵니다.\n\n시스템 Python으로 다시 변경하면, 이 변경 사항은 OS에서의 Python 버전을 관리하는 pyenv versions에만 반영됩니다.\n\n```bash\n[~]$ pyenv global system\n[~]$ pyenv versions\n* system (set by /home/vandermeij/.pyenv/version)\n  3.11.7\n[~]$ which python\n/home/vandermeij/.pyenv/shims/python\n```\n\n## 사용법\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 주로 pyenv를 주요 Python 배포 업체로 사용합니다. 기본 시스템 Python을 거의 사용하지 않아요. 저의 컴퓨터에서는 이렇게 나와요:\n\n```js\n[~]$ pyenv versions\n  system\n* 3.11.7 (set by /home/vandermeij/.pyenv/version)\n```\n\n때로는 프로젝트에서 다른 Python 버전을 사용하고 싶을 수도 있어요, 예를 들어 Python 3.12를 사용하고 싶다고 가정해봅시다. 전역 Python 버전을 변경하는 것 외에도 pyenv를 통해 로컬 버전을 사용할 수도 있어요.\n\ntestapp이라는 작은 프로젝트를 만든다고 가정해봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n[~]$ mkdir testapp\n[~]$ cd testapp\n```\n\n다음으로 pyenv를 사용하여 로컬 Python 버전을 설정합니다:\n\n```js\n[~/testapp]$ pyenv local 3.12\npyenv: version `3.12' not installed\n```\n\n버전 3.12가 아직 설치되지 않았다고 알려줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼 설치해보겠습니다:\n\n```js\n[~/testapp]$ pyenv install 3.12\nPython-3.12.1.tar.xz를 다운로드 중...\n-\u003e https://www.python.org/ftp/python/3.12.1/Python-3.12.1.tar.xz\nPython-3.12.1을 설치 중...\n```\n\n다시 시도해보세요:\n\n```js\n[~/testapp]$ pyenv local 3.12\n[~/testapp]$ pyenv versions\n  system\n  3.11.7\n* 3.12.1 (설정 위치: /home/vandermeij/testapp/.python-version)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파이썬 3.12.1이 활성화되어 있는 것을 확인할 수 있어요. 이는 이 폴더 내의 .python-version이라는 로컬 파일로 인해 터미널에서 특정 버전을 로드하도록 pyenv에 알려주기 때문이에요.\n\n서브 폴더를 제외한 다른 폴더로 이동하면 활성화된 파이썬 버전이 재설정됩니다:\n\n```js\n[~]$ pyenv versions\n  system\n* 3.11.7 (set by /home/vandermeij/.pyenv/version)\n  3.12.1\n```\n\npyenv를 사용하면 로컬 또는 전역 파이썬 버전에 관해 항상 사용 중인 파이썬 버전을 알 수 있고 완전한 제어권을 가질 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 적절한 개발 환경 설정 단계는 외부 라이브러리/의존성을 설치하기 위해 가상 환경을 사용하는 것입니다. 이렇게 함으로써 a) 우리의 pyenv Python 배포를 깨끗하게 유지할 수 있고, b) 다른 프로젝트와 충돌하지 않습니다. 이에 대해 저는 Poetry를 추천합니다.\n\n# Poetry\n\n## 설치\n\n새로 설치한 pyenv Python 버전에 설치할 유일한 외부 라이브러리는 Poetry입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sh\n[~]$ pip install poetry\n[~]$ poetry -V\nPoetry (version 1.7.1)\n```\n\n이 작업을 각 Python 버전마다 수행하십시오.\n\n```sh\n[~/testapp]$ poetry -V\npyenv: poetry: command not found\n```\n\n## 사용법\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n새 프로젝트를 시작하려면 poetry init -q를 실행해 보세요. 이렇게 하면 pyproject.toml 파일이 생성됩니다.\n\n이제 의존성을 추가할 수 있게 됩니다. 하지만 그 전에 가상 환경을 활성화해야 합니다. poetry shell을 실행하세요:\n\n```js\n[~/testapp]$ poetry shell\nCreating virtualenv testapp-b6lGsqOc-py3.12 in /home/vandermeij/.cache/pypoetry/virtualenvs\nSpawning shell within /home/vandermeij/.cache/pypoetry/virtualenvs/testapp-b6lGsqOc-py3.12\n[~/testapp]$ emulate bash -c '. /home/vandermeij/.cache/pypoetry/virtualenvs/testapp-b6lGsqOc-py3.12/bin/activate'\n(testapp-py3.12) [~/testapp]$\n```\n\n이렇게 하면 가상 환경 (virtualenv)이 생성되고, 이미 존재하지 않는 경우 새 쉘이 생성되며 해당 가상 환경이 활성화됩니다. 프롬프트(PS1)에서 확인할 수 있습니다: (testapp-py3.12) [~/testapp]$.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 which python을 실행하면 가상 환경에 연결된 다른 심볼릭 링크를 볼 수 있습니다:\n\n```js\n(testapp-py3.12) [~/testapp]$ which python\n/home/vandermeij/.cache/pypoetry/virtualenvs/testapp-b6lGsqOc-py3.12/bin/python\n```\n\n확인하세요. pyenv로 활성화된 Python 버전과 동일하지만 이제 가상 환경으로 묶여 있습니다.\n\n```js\n(testapp-py3.12) [~/testapp]$ python -V\nPython 3.12.1\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 가상 환경에서는 종속 항목을 설치할 것입니다.\n\n이미 pyproject.toml 파일이 있는 경우, 단지 그것을 포함하는 git 저장소를 복제했기 때문에, 프로젝트 자체가 파이썬 라이브러리인지 여부에 따라 poetry install 또는 poetry install --no-root를 실행할 수 있습니다. 우리의 경우, testapp은 파이썬 라이브러리가 아닙니다:\n\n```js\n(testapp-py3.12) [~/testapp]$ poetry install --no-root\n잠금 파일에서 종속성을 설치 중\n```\n\n이제 FastAPI와 같은 새로운 종속성을 추가할 수 있습니다. poetry add fastapi를 사용하거나 pyproject.toml 파일을 수동으로 편집할 수 있습니다. 일반적으로 후자를 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n패스트API를 수동으로 추가하겠습니다. 좋아하는 편집기로 pyproject.toml 파일을 열고 다음 줄을 변경하세요 (fastapi = \"\\*\"):\n\n\n[tool.poetry.dependencies]\npython = \"^3.12\"\nfastapi = \"*\"\n\n\n이제 Poetry 업데이트를 실행해보세요:\n\n\n(testapp-py3.12) [~/testapp]$ poetry update\n의존성 업데이트 중\n의존성 해결 중... (6.6초)\n\n패키지 작업: 9개 설치, 0개 업데이트, 0개 제거\n\n  • idna (3.6) 설치 중\n  • sniffio (1.3.0) 설치 중\n  • typing-extensions (4.9.0) 설치 중\n  • annotated-types (0.6.0) 설치 중\n  • anyio (4.2.0) 설치 중\n  • pydantic-core (2.14.6) 설치 중\n  • pydantic (2.5.3) 설치 중\n  • starlette (0.35.1) 설치 중\n  • fastapi (0.109.0) 설치 중\n\n잠금 파일 작성 완료\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가상 환경이 FastAPI 및 해당 종속성들로 풍부해졌어요. 보너스로, Poetry가 poetry.lock 파일을 생성했는데, 이 파일에는 설치된 각 (하위)종속성의 특정 버전이 포함되어 있어요.\n\n다음 파일을 git에 추가해주세요:\n\n- .python-version\n- pyproject.toml\n- poetry.lock\n\n이제 pyenv와 Poetry를 사용하여 깔끔하고 적절한 Python 개발 환경을 구축했어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 가상 환경의 전체 개요를 보려면 poetry env list를 실행하세요:\n\n```js\n(testapp-py3.12) [~/testapp]$ poetry env list\ntestapp-b6lGsqOc-py3.12 (Activated)\n```\n\n전용 쉘을 종료하려면 CTRL+D를 누르거나 exit를 입력하세요:\n\n```js\n(testapp-py3.12) [~/testapp]$ exit\n[~/testapp]$\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\npyenv 및 Poetry는 로컬 개발 환경을 위한 강력한 도구를 제공합니다. 이를 사용하는 것을 강력히 권장합니다. 저는 그것을 사용하여 매우 만족하고 있습니다.\n\n앞서 간단히 언급했듯이 중요 파일을 저장하기 위해 git을 사용해주세요. 또한, 어플리케이션을 설치할 때는 어디에서든 poetry.lock 파일을 사용해주세요. 예를 들어, Docker에서.\n\n이 글에 관한 질문, 문의 또는 의견이 있으시다면 언제든지 연락해주세요. 도움이 되겠습니다. 여기나 제 개인 웹사이트에서 저에게 연락을 주셔도 됩니다.\n","ogImage":{"url":"/assets/img/2024-05-27-ProperPythonsetupwithpyenvPoetry_0.png"},"coverImage":"/assets/img/2024-05-27-ProperPythonsetupwithpyenvPoetry_0.png","tag":["Tech"],"readingTime":8},{"title":"GenPiCam - 생성적 AI 카메라","description":"","date":"2024-05-27 13:51","slug":"2024-05-27-GenPiCam-GenerativeAICamera","content":"\n![image](https://miro.medium.com/v2/resize:fit:1268/1*eZzfeCJggafmHaYGcjqEDA.gif)\n\n이 프로젝트의 무거운 처리와 진정한 지혜는 머신 러닝 기반 이미지 생성기를 사용하는 외부 서비스인 Midjourney가 처리합니다. GenPiCam은 두 가지 Midjourney 능력을 활용합니다.\n\n- 존재하는 사진을 사용하여 이미지에 대한 텍스트 설명 프롬프트를 생성하는 \"Describe\"\n- 자연어 프롬프트를 이미지로 변환하는 \"Imagine\"\n\n이 두 단계 사이에서 나는 창의적 입력 수준을 허용하므로 GenPiCam 카메라에는 최종 이미지의 스타일을 조정하는 다이얼이 있습니다. 이것은 본질적으로 생성된 이미지에 \"애니메이션\", \"팝 아트\" 또는 \"미래적\" 영향을 더하는 필터가 되게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 지루해요 — 비디오를 보고 싶어요?\n\n물론이죠 — 여기 2분 요약 영상이 있어요\n\n# \"사진\" 프로세스\n\n초기 사진 이미지는 라즈베리 파이 카메라 모듈로 촬영됩니다. 외부 카메라 셔터(라즈베리 파이 GPIO 핀에 연결된 푸시 버튼)를 누르면 정지 이미지를 촬영하고 사진을 jpeg 이미지로 저장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![GenPiCam-GenerativeAICamera_0](/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_0.png)\n\n이 사진은 Midjourney에 업로드되었는데, 기존 사진을 시작으로 이미지에 대한 텍스트 설명 프롬프트를 생성합니다. 궁금하신 분들을 위해, 저는 PyAutoGUI를 사용하여 매우 서투른 봇 상호 작용을 마우스와 키보드를 제어하는 데 사용하고 있습니다(API가 없기 때문에) — 이것은 쓰면 안 되는 코드의 예제가 될 수 있도록 해주세요.\n\nMidjourney의 describe 도구는 이미지를 입력으로 받아와 텍스트 프롬프트를 생성합니다. 이는 \"텍스트를 이미지로\" 하는 일반적인 과정을 반대로 해서 사진을 시작으로 이미지의 본질을 설명하는 텍스트를 추출하는 매우 탐구적인 서비스입니다. 여기에는 Snowy가 있지만 Midjourney에는 훨씬 더 표현력이 풍부한 설명이 있습니다.\n\n![GenPiCam-GenerativeAICamera_1](/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"describe\" 함수는 실제로 이미지를 기반으로 네 가지 설명을 반환하지만, GenPiCam은 임의로 첫 번째 설명을 선택합니다.\n\n이제 재미난 부분이 시작됐어요. 우리는 그 텍스트 프롬프트를 가져와서 새로운 이미지를 Generative AI를 사용하여 Midjouney imagine에 새로운 호출로 만들 수 있어요. 이전 텍스트 프롬프트에서 생성된 이미지가 여기 있어요.\n\n![image](/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_2.png)\n\nGenPiCam에는 스타일 지시 사항으로 텍스트 프롬프트를 업데이트하는 선택 스위치가 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_3.png)\n\n라즈베리 파이 GPIO 핀에 연결된 12진 스위치입니다. 현재 \"예술적 선택\"을 읽어서 GenPiCam이 텍스트 프롬프트에 \"레트로 팝 아트 스타일 일러스트\"와 같은 접두사를 추가할 것입니다. 다른 스타일 프롬프트 중 일부는 다음과 같습니다.\n\n- 애니메이션 스타일\n- 초현실주의, 다양한 모자와 풍선이 있는 화려한 히퍼리얼리즘\n- 흐릿한 브러시 스트로크\n- 우주 정거장에서 초현실주의인 미래주의\n\n눈이 내린 이미지의 \"팝아트\" 이미지를 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n[![](/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_4.png)](https://example.com)\n\nThe final image is created using the Pillow Python imaging library and consists of:\n\n- The initial photo taken by the Raspberry Pi camera module, resized on the left\n- The final Midjouney image—the first of four images is selected, composited to the right\n- Text prompt—against a colored background and icon signifying the style mode\n\nHere's the same process, but adding the text “Hyper-Realistic, whimsical with a colorful hat and balloons”.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_5.png)\n\n우측 이미지가 Generative AI로 만들어진 것임에도 불구하고, 스노위의 비평적인 시선을 통해 실망감이 느껴집니다.\n\n# Generative AI 이미지 — 배운 점\n\nGenPiCam 카메라를 구축하는 데 정말 즐겁게 시간을 보냈고, 이는 Generative AI에 대한 프립트 엔지니어링을 탐구하는 흥미로운 경로였습니다. 더 좋았던 사진들은 단순한 구성을 가진 것들이었는데, 즉, 말로 표현하기 쉬운 이미지였습니다. 예를 들어, 이 장면은 색상과 명확한 객체로 쉽게 설명할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Generated Image 1](/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_6.png)\n\nHowever, there were some very strange results while describing more unique scenes. I found the description of a classic Australian clothesline created an unusual image.\n\n![Generated Image 2](/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_7.png)\n\nOne of my favorite reimagined images was the identification of my laser mouse. It turns out a laser mouse has multiple meanings leading to a striking result.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![GenPiCam Hardware](/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_8.png)\n\n# 하드웨어\n\nGenPiCam의 가장 세련되지 않은 부분은 서둘러 조립한 하드웨어입니다. 만약 당신이 자신만의 현실 왜곡 카메라를 만들고 싶다면, 아래 항목이 필요합니다.\n\n- RaspberryPi 4가 설치된 Raspberry Pi OS\n- Raspberry Pi 카메라 모듈 v2\n- Raspberry Pi용 터치스크린 모니터\n- 12개의 PCB 회전 스위치\n- 푸시버튼 (순간접촉)\n- 폴리카보네이트 케이스\n- 재충전 가능한 배터리 팩\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_9.png\" /\u003e\n\n가장 아름다운 빌드는 아닐지 몰라요. 하지만 이건 기능성이 뛰어나다는 점으로만 용서하겠습니다.\n\n\u003cimg src=\"/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_10.png\" /\u003e\n\n# 요약, 코드 및 크레딧\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGenPiCam은 생성 AI를 탐험하는 재미있는 방법이었어요. 그림을 스타일에 맞게 변환해주는 (때로는 놀랄만한) 이미지들을 만들어내죠.\n\n![GenPiCam](/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_11.png)\n\n## 크레딧\n\n- Ned Letcher — Midjourney의 describe 기능을 보여줌으로써 저를 영감받게 한 분이자 이미지를 재창조하는 개념을 제공해준 분\n- Michael King의 'Midjourney 이미지 다운로드하는 Discord 봇 만들기' — Midjourney와 상호작용하기 위한 Python 자동화 및 Discord 봇 설정을 보여주는 훌륭한 글\n- Midjourney — 봇 채널을 위한 Midjourney 명령 구문\n- discord.py — Discord를 위한 Python API 래퍼\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 코드\n\n[https://github.com/saubury/GenPiCam](https://github.com/saubury/GenPiCam)\n","ogImage":{"url":"/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_0.png"},"coverImage":"/assets/img/2024-05-27-GenPiCam-GenerativeAICamera_0.png","tag":["Tech"],"readingTime":5},{"title":"어디서나 Tailscale을 통해 Immich 라이브러리에 접근하고 동기화하기","description":"","date":"2024-05-27 13:49","slug":"2024-05-27-AccessingandSyncingYourImmichLibraryfromAnywherewithTailscale","content":"\n\n\u003cimg src=\"/assets/img/2024-05-27-AccessingandSyncingYourImmichLibraryfromAnywherewithTailscale_0.png\" /\u003e\n\n이전 글에서 Raspberry Pi에 추가 SSD 저장 공간을 이용해 오픈 소스 사진 백업 솔루션인 Immich를 자체 호스팅하는 방법에 대해 이야기했습니다. 자신의 미디어 서버를 호스팅함으로써 데이터에 완전한 제어권을 갖는 장점 중 하나는 있습니다. 그러나 어디서든 라이브러리에 액세스하고 동기화하는 것은 견고하고 안전한 방법 없이는 어려울 수 있습니다. 이때 Tailscale이 등장합니다. Tailscale은 어디에 있든 안전하게 디바이스를 인터넷을 통해 연결할 수 있게 해주는 망 VPN 서비스입니다.\n\n# Tailscale이란?\n\nTailscale은 WireGuard를 기반으로 한 현대적인 VPN 솔루션으로, 쉽게 설정하고 사용할 수 있도록 설계되었습니다. 디바이스 간에 안전하고 암호화된 네트워크를 만들어주어 전 세계 어디에서나 로컬 네트워크에 연결된 것처럼 Immich 라이브러리에 액세스할 수 있습니다. Tailscale은 NAT 트래버셜을 처리해주므로 라우터 구성이나 복잡한 네트워크 설정에 대해 걱정할 필요가 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Tailscale와 Immich 함께 사용하는 이점\n\n- 사용 편의성: 네트워킹 지식이 제한된 사용자도 쉽게 설정하고 사용할 수 있는 Tailscale입니다.\n- 보안성: 모든 연결은 WireGuard를 사용하여 암호화되어 데이터가 안전하게 전송됩니다.\n- 이용 편의성: 세계 어디에서나 어떤 기기에서든 Immich 라이브러리에 액세스할 수 있습니다.\n- 포트 포워딩 불필요: Tailscale은 포트 포워딩이나 동적 DNS 설정이 필요없어요.\n\n# Immich의 주요 이점: 자동 동기화 기능\n\nImmich의 주목할 만한 기능 중 하나는 모바일 앱의 통합 자동 동기화 기능입니다. 이 기능을 사용하면 동기화할 폴더를 선택할 수 있고 Immich가 사진과 비디오를 자동으로 업로드합니다. 사용자는 장치가 충전 중이거나 Wi-Fi에 연결되어 있을 때에만 업로드하도록 설정할 수 있어 데이터 사용량과 배터리 수명을 효율적으로 관리할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적으로 집 네트워크에 연결된 상태일 때만 작동하지만, Tailscale을 사용하면 RaspberryPi에 어디서나 안전하게 액세스할 수 있습니다. 해외 여행 중일 때도 이미지를 업로드하고 저장할 수 있습니다. 이것은 구글 포토와 같은 클라우드 서비스와 비교했을 때 하드웨어 제한을 제외하고 저장 용량 제한이 없어서 특히 유용합니다. \n\n# Immich와 Tailscale 설정 방법\n\n## 전제 조건\n\n- Raspberry Pi에 Immich가 정상적으로 설치되어 있어야 합니다.\n- 무료로 Tailscale 계정을 만들 수 있습니다 (Tailscale에서 가입할 수 있습니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계별 안내\n\n라즈베리 파이에 Tailscale 설치하기:\n\n- 라즈베리 파이에서 터미널을 엽니다.\n- Tailscale 설치하기:\n\n```bash\ncurl -fsSL https://tailscale.com/install.sh | sh\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTailscale를 시작하고 로그인하세요:\n\n\nsudo tailscale up\n\n\n- 기기를 인증하기 위해 지침을 따르세요. 웹 브라우저에서 URL을 열고 Tailscale 계정으로 로그인해야 합니다.\n\n다른 기기에 Tailscale 설치하기:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 다른 기기(예: 휴대폰, 노트북 또는 데스크톱)에 Tailscale을 다운로드하고 설치하세요.\n- 동일한 계정을 사용하여 각 기기에서 Tailscale에 로그인하세요.\n\n라즈베리 파이에 액세스:\n\n- 모든 기기에 Tailscale이 설정되면 Tailscale 대시보드에서 각 기기에서 라즈베리 파이가 나열될 것입니다.\n- 라즈베리 파이에 할당된 Tailscale IP 주소를 기록하세요. IP 주소는 100.x.x.x와 같은 형식일 것입니다.\n\n어디서나 Immich에 액세스하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Tailscale에 연결된 장치의 웹 브라우저를 엽니다.\n- http://100.x.x.x:2283 으로 이동합니다 (100.x.x.x를 Raspberry Pi의 Tailscale IP 주소로 교체합니다).\n- Immich 인터페이스가 표시되어 사진 및 비디오 라이브러리에 액세스하고 관리할 수 있습니다.\n\nImmich 모바일 앱을 사용하여 라이브러리 동기화하기:\n\n- 모바일 장치에 Immich 앱을 다운로드하고 설치합니다.\n- 앱을 열고 서버 설정을 Tailscale IP 주소로 구성합니다.\n- 앱 내 설정으로 이동하여 자동 동기화할 폴더를 선택합니다.\n- 배경 업로드 설정을 활성화하고 충전 중이거나 Wi-Fi에 연결되어 있는 경우에만 업로드하는 환경 설정을 선택합니다.\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTailscale을 Immich와 함께 사용하면 전 세계 어디에서나 미디어 라이브러리에 안전하고 쉽게 액세스하고 동기화할 수 있는 솔루션을 제공합니다. Immich의 모바일 애플리케이션의 자동 동기화 기능은 배경 업로드를 원활하게하며 수동 개입없이 미디어가 항상 백업되도록 보장합니다. 이 설정은 원격 액세스의 편리함을 유지하면서 데이터에 대한 완전한 제어를 보장합니다. Tailscale의 강력한 보안 기능과 간편한 구성을 통해 전통적인 VPN이나 포트 포워딩 설정의 번거로움없이 자체 호스팅 미디어 서버의 이점을 즐길 수 있습니다.\nTailscale을 활용하여 라즈베리 파이를 강력하고 접근성이 있는 미디어 허브로 변신시킬 수 있으며 상용 클라우드 서비스와 견줄 만한 라이벌이 될 수 있습니다. Immich의 자동 동기화 기능과 Tailscale의 안전한 연결이 결합되어 개인 미디어 라이브러리를 효과적이고 효율적으로 관리할 수 있는 포괄적인 솔루션을 제공합니다.","ogImage":{"url":"/assets/img/2024-05-27-AccessingandSyncingYourImmichLibraryfromAnywherewithTailscale_0.png"},"coverImage":"/assets/img/2024-05-27-AccessingandSyncingYourImmichLibraryfromAnywherewithTailscale_0.png","tag":["Tech"],"readingTime":4}],"page":"5","totalPageCount":11,"totalPageGroupCount":1,"lastPageGroup":11,"currentPageGroup":0},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"5"},"buildId":"kNTo-t2jvQG5kfHDWIcB-","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>