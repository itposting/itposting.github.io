<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/85" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/85" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="온라인 플랫폼에서 참여 예측을 위한 딥 러닝" href="/post/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="온라인 플랫폼에서 참여 예측을 위한 딥 러닝" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="온라인 플랫폼에서 참여 예측을 위한 딥 러닝" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">온라인 플랫폼에서 참여 예측을 위한 딥 러닝</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="당신이 얼마나 많은 무게를 진다는지 알 수 없어요" href="/post/2024-06-19-youhavenoideahowmuchweighticarry"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="당신이 얼마나 많은 무게를 진다는지 알 수 없어요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-youhavenoideahowmuchweighticarry_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="당신이 얼마나 많은 무게를 진다는지 알 수 없어요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">당신이 얼마나 많은 무게를 진다는지 알 수 없어요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">1<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="딥 러닝 모델 최적화를 위한 가중치 양자화" href="/post/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="딥 러닝 모델 최적화를 위한 가중치 양자화" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="딥 러닝 모델 최적화를 위한 가중치 양자화" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">딥 러닝 모델 최적화를 위한 가중치 양자화</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">16<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="선형 회귀를 사용한 비농업 부문 고용 예측" href="/post/2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="선형 회귀를 사용한 비농업 부문 고용 예측" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="선형 회귀를 사용한 비농업 부문 고용 예측" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">선형 회귀를 사용한 비농업 부문 고용 예측</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="인간형 로봇의 문제점" href="/post/2024-06-19-TheProblemsWithHumanoidRobots"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="인간형 로봇의 문제점" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-TheProblemsWithHumanoidRobots_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="인간형 로봇의 문제점" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">인간형 로봇의 문제점</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="사이보그의 각성" href="/post/2024-06-19-TheCyborgsAwakening"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="사이보그의 각성" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-TheCyborgsAwakening_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="사이보그의 각성" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">사이보그의 각성</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="강화 학습 소개" href="/post/2024-06-19-AnIntroductiontoReinforcementLearning"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="강화 학습 소개" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="강화 학습 소개" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">강화 학습 소개</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">28<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="로스 2에서 확장 칼만 필터를 활용한 센서 융합" href="/post/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="로스 2에서 확장 칼만 필터를 활용한 센서 융합" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="로스 2에서 확장 칼만 필터를 활용한 센서 융합" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">로스 2에서 확장 칼만 필터를 활용한 센서 융합</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">31<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ROS 2 Python 런치 파일의 비밀을 해제합니다" href="/post/2024-06-19-UnlockingtheSecretsofROS2PythonLaunchFiles"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ROS 2 Python 런치 파일의 비밀을 해제합니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-UnlockingtheSecretsofROS2PythonLaunchFiles_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ROS 2 Python 런치 파일의 비밀을 해제합니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">ROS 2 Python 런치 파일의 비밀을 해제합니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이에 Kali 설정하기 파트 3, VNC 서버 및 클라이언트 설정하기" href="/post/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이에 Kali 설정하기 파트 3, VNC 서버 및 클라이언트 설정하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이에 Kali 설정하기 파트 3, VNC 서버 및 클라이언트 설정하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">라즈베리 파이에 Kali 설정하기 파트 3, VNC 서버 및 클라이언트 설정하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/81">81</a><a class="link" href="/posts/82">82</a><a class="link" href="/posts/83">83</a><a class="link" href="/posts/84">84</a><a class="link posts_-active__YVJEi" href="/posts/85">85</a><a class="link" href="/posts/86">86</a><a class="link" href="/posts/87">87</a><a class="link" href="/posts/88">88</a><a class="link" href="/posts/89">89</a><a class="link" href="/posts/90">90</a><a class="link" href="/posts/91">91</a><a class="link" href="/posts/92">92</a><a class="link" href="/posts/93">93</a><a class="link" href="/posts/94">94</a><a class="link" href="/posts/95">95</a><a class="link" href="/posts/96">96</a><a class="link" href="/posts/97">97</a><a class="link" href="/posts/98">98</a><a class="link" href="/posts/99">99</a><a class="link" href="/posts/100">100</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"온라인 플랫폼에서 참여 예측을 위한 딥 러닝","description":"","date":"2024-06-19 06:44","slug":"2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms","content":"\n\n\n![image](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_0.png)\n\n# 요약\n\n에딘버러 대학에서 MSc 논문의 일환으로, 저는 딥러닝 기술을 사용하여 준니버스의 사용자 참여를 예측했습니다. 준니버스는 비과학자들이 행성 탐사와 같은 특정 분야에 기여할 수 있는 온라인 시민 과학 플랫폼입니다. 준니버스는 100편 이상의 논문 발표에 기여했습니다. 실제 환경에서 시민 과학자들은 영국의 농업 유출물이 영국 강의 안전 한 한계를 초과하는 수위의 오염 증가를 입증했습니다.\n\n이 문맥에서 참여는 이전 행동을 고려할 때 플랫폼의 미래 사용을 예측할 수 있는지 여부입니다. 이는 여러 형태로 나타날 수 있습니다, 예를 들면:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 시간 기준 내에서 완료할 작업의 예상 수\n- 현재 작업 세션에서 머무를 시간의 양\n\n우리는 섹션 정의에 초점을 맞추고 사용자가 10, 20 또는 30분 동안 계속 작업할지를 예측하기 위해 노력했습니다. 이는 사용자 작업 T를 작업 세션 W로 그룹화하여 계산되었습니다.\n\n![이미지](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_1.png)\n\n이 정의는 유연성과 Zooniverse, StackOverflow 및 Coursera에서의 이전 사용을 위해 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 접근 방식의 주의 사항 중 하나는 생산적인 온라인 시스템의 한 속성인 플랫폼 시간만을 예측한다는 것입니다. 예를 들어, 올바른 기여를 하는 동기부여된 참여자들을 갖는 것은 고려되지 않습니다. 온라인 시스템에 참여하는 동기를 측정하는 것은 활발한 연구 분야입니다. 심리적 상태인 좌절감과 이해도와 같은 것들이 학습되어 맞춤형 교육 개입에 활용될 수 있는 잠재적인 작업이 있습니다.\n\n# 데이터 및 세션 버킷 알고리즘\n\nZooniverse는 친철하게 2021년 10월 19일에서 2022년 8월 14일까지 시민 과학자들의 클릭스트림 데이터를 제공해 주었습니다. 이 데이터는 미국, 중국, 싱가포르 및 핀란드의 시민 과학자들을 다루었습니다.\n\n원시 데이터 세트에는 다음과 같은 열이 포함되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n| Column Name |               Column Desc               | Column Type |\n|-------------|-----------------------------------------|-------------|\n| id          | 클릭스트림 항목을 식별하는 고유 식별자  | bigint      |\n| user_id     | 사용자를 식별하는 고유 식별자           | bigint      |\n| project_id  | 프로젝트를 식별하는 고유 식별자         | bigint      |\n| workflow_id | 워크플로우를 식별하는 고유 식별자       | bigint      |\n| subject_ids | 고유한 작업 식별자                      | bigint      |\n| country     | 국가 이름                               | str         |\n| latitude    | 국가 위도                               | float       |\n| longitude   | 국가 경도                               | float       |\n| timestamp   | 클릭스트림 타임스탬프                   | bigint      |\n\n\nZooniverse의 계층 구조에서 각 행은 다음 거래를 나타냅니다.\n\n- 사용자가 시스템에 로그인하고 참여할 프로젝트를 선택합니다.\n- 프로젝트는 여러 워크플로우를 포함하며 태스크 그룹화를 수행합니다.\n- 사용자는 프로젝트에 연관된 작업을 수행합니다. 작업에는 여러 주제가 포함될 수 있습니다 (하지만 대부분은 하나만 포함합니다).\n\n\u003cimg src=\"/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_2.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 특성 선택\n\n데이터에는 약 38,500,990개의 고유 이벤트가 포함되어 있습니다. 서로 다른 지역별 이벤트 분포는 다음과 같습니다:\n\n\n| 국가명        | 백분율     |\n|---------------|------------|\n| 핀란드         | 59.4       |\n| 미국           | 25.5       |\n| 싱가포르       | 10.8       |\n| 중국           | 4.3        |\n\n\n위도와 경도는 국가 정보를 중복해서 나타내므로 제거되었습니다. 주제 ID 및 작업 ID도 제거되었는데, 이 정보의 세분화가 참여 패턴 학습에 기여하지 않았기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리 연구에서는 이전 연구에서 강조된 주요 변수가 부족했습니다. 예를 들어, Semenov et al.은 로그인한 사용자들이 더 오랜 시간 동안 작업하고 작업 세션에 대해 더 높은 투자 수준을 나타내는 것으로 발견했습니다. 또한, Mao et al.은 투표 엔트로피가 감소하는 경우, 사용자가 목록에서 반복해서 동일한 옵션을 선택하는 것이 지루함과 참여하지 않음의 유용한 대리자로 작용할 수 있다고 밝혀냈습니다. 이러한 연구 결과는 사용자 참여도를 효과적으로 측정할 수 있는데 사용자 로그인 상태와 투표 패턴을 통해 이를 할 수 있음을 시사했으며, 이러한 측면들이 우리의 현재 분석에는 고려되지 않았습니다.\n\n따라서 우리는 이러한 특성을 근사화하는 데 사용될 수 있는 다양한 속도에서 통계치를 계산해야 했습니다. 이에는 다음이 포함되었습니다:\n\n- 사용자 세션 수의 롤링 번호\n- 현재 세션 내의 시간 및 이벤트 수\n- 과거 세션 간의 평균 시간 및 이벤트 수\n- 이전 세션에서 소비한 시간 및 이전 세션의 이벤트 수\n- 마지막 이벤트로부터의 시간 차 및 과거 이벤트의 평균 시간 차\n\n![image](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# **The Feature Long Tail**\n\nZooniverse 및 Coursera, StackOverflow, Snap과 같은 다른 온라인 플랫폼의 분석 결과를 보면, 매우 소수의 사용자가 플랫폼 활동의 대부분을 지나치게 책임지는 거대한 권한 분포가 있다는 것을 알 수 있습니다. 이 현상은 소수의 고도로 활발한 사용자가 플랫폼 기여를 주도하고 대부분의 사용자는 비교적 활동이 적다는 것을 나타냅니다.\n\n저희의 데이터는 권한 분포에 부합되어, 사용자의 25%만이 세 번 이상의 세션을 완료하였으며, 작업 세션의 68%는 30분 미만으로 지속되었습니다. 이 분포는 사용자 참여의 불균형을 강조하며, 소수의 고도로 활발한 사용자가 플랫폼에서의 활동을 지배하고 있음을 보여줍니다.\n\n많은 사용자에게 시간 순서 채널은 중복될 수 있습니다. 사용자가 단 한 번의 세션만 완료하는 경우, 과거 세션 계산은 일정한 0으로 유지됩니다. 마찬가지로 세션 간 시간과 같은 채널의 경우, 사용자가 두 번의 세션만 완료하는 경우, 과거 세션 채널은 첫 번째 세션에 대해서는 0으로 일정하게 유지되거나 이전 세션 통계의 일정한 값이 될 것입니다. 이 중복성은 드물게 사용하는 사용자에게는 이러한 메트릭이 의미 있는 통찰력을 제공하지 않을 수 있으며, 참여를 효과적으로 평가하기 위해 대안적인 측정 방법이 필요할 수 있다는 것을 시사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_4.png)\n\nThe feature space was designed to represent user information over various time intervals. Because many users only completed two sessions, a lot of data would be repeated for these users. Any model for predicting user behavior would need to be carefully interrogated to ensure it did not memorize these aspects of user behavior and project results that mimicked the most likely user, rather than the projection of users.\n\nTherefore, it was important to design experiments that enabled differentiation between the small number of highly active users and the large population of inactive, short-term contributors.\n\n# Experiments\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n실험 결과 다음 질문에 대한 대답을 제공했습니다. 사용자의 클릭 스트림에 기록된 항목이 주어졌을 때 사용자가 10, 20 또는 30분 동안 계속 작업을 할 것인가?\n\n클릭 스트림의 각 행은 사용자가 지정된 시간대(10, 20 또는 30분) 동안 계속 작업할지 여부를 나타내는 레이블 yiy_iyi로 표시되었습니다. 데이터가 시계열을 따르고 사용자 행동이 이전 이력에 의존한다는 점을 감안하여, 클릭 스트림 F(X_i, Y_i)의 예측은 과거 정보를 포함하여 확장되었습니다. 따라서 예측 함수는 F(X_i, Y_i, | X_i-1, X_i-2, ..., X_0)가 되었습니다.\n\n장기 단기 메모리(Long Short-Term Memory, LSTM) 네트워크는 과거와 현재 정보를 제어하고 가중시키도록 설계된 것으로 이러한 종류의 분석에 적합합니다. 이들은 다른 시계열 작업으로도 효과적으로 적용되어 왔으며, 활동 인식, 행동 인식 및 지진 예측과 같은 작업에 사용되었습니다.\n\n사용자 세션 창을 통해 이러한 함수를 쌓는 과정에서, 모델이 세션 통계, 사용자 동작 변화 및 예상된 참여 사이의 관계를 학습할 수 있을 것으로 기대했습니다¹.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 질문에 대답하려고 실험을 진행했습니다:\n\n- 단기 또는 장기 행동을 예측하는 것이 더 쉬운지 여부.\n- 데이터 창 크기를 확장하면 참여 측정에 기여하는지 여부.\n\n또한 실험에서는 각 이벤트를 독립적으로 고려하는 대신 네트워크 아키텍처와 종속성을 포착하는 것이 모델 성능에 영향을 미치는지 확인하기 위해 RandomForest 분류에 대해 베이스라인을 설정했습니다.\n\n사용자가 현재 세션에서 계속 작업하는지 여부는 이진적입니다. 따라서 RandomForest를 구성하고 LSTM 매개변수 최적화를 위해 유전 알고리즘과 경사 하강법을 적용할 수 있었고, 바이너리 크로스 엔트로피를 사용하여 꼭 맞추었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일괄 처리된 N개의 관측치에 대한 손실 함수는 다음과 같습니다:\n\n![Loss Function](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_5.png)\n\n목표는 yi(실제 예측값)와 네트워크 출력인 y^i 사이의 거리를 최소화하는 것입니다.\n\n# 구현\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터는 타임스탬프에 따라 정렬되고 훈련, 테스트 및 평가 파티션으로 분할되었습니다. 이미지 분류 실험과는 달리 시계열 관측 사이의 관계가 중요하기 때문에 데이터를 섞으면 시퀀스의 일관성이 깨질 수 있습니다. 따라서 데이터를 섞으면 훈련에 후속 관측사항이 포함될 수 있습니다. 예를 들어, 모델이 작업 세션이 10분만 계속된다는 정보를 받기 전에 작업 세션의 1분과 2분의 관측을 받는다면 결과적으로 선견지명을 얻을 수 있습니다.\n\n장기 단기 메모리 (LSTM) 네트워크가 사용자 관측 윈도우의 길이 1, 10, 20, 30, 40에 대해 생성되고 훈련되었습니다. PyTorch Lightning을 사용하여 네트워크를 훈련했는데, 이를 통해 편리한 래퍼(wrapper)를 제공하여 훈련 루프 구현, 로깅 및 메트릭 처리를 감싸줍니다. 10, 20, 30분 동안의 지속을 예측하기 위한 별도의 실험이 수행되었습니다.\n\n첫 번째의 출력이 두 번째로 전파되는 두 개의 LSTM 네트워크를 쌓는 것이 가장 성능이 좋았습니다. 두 번째 레이어는 선형 레이어를 따라, 기능을 하나의 대상 변수로 변환하여 사용자의 세션 지속 확률을 정의했습니다. 최종 선형 레이어 다음의 엔트로피에서 역전파가 계산되었고, 이를 통해 네트워크 전체의 가중치가 업데이트되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결과\n\n실험을 통해 성과가 좋은 모델을 분석했습니다. 각 실험에 대한 정밀도, 재현율 및 AUC를 플롯했습니다. AUC는 분류 임계값 범위에서 실제 양성 비율(재현율)과 거짓 양성 비율(1-정밀도)을 집계합니다.\n\n검증 및 테스트 데이터셋 간의 균형은 모델이 특정 시간 역학에 기반하지 않고 패턴을 학습하고 있다는 것을 나타냅니다. 따라서 참여 패턴이 크게 변하지 않는 한, 새로운 데이터에 적용될 때 모델의 동작을 예측할 수 있을 가능성이 높습니다.\n\n결과는 장기적 예측에서 일반적으로 더 일관성있게 나타납니다. 정밀도는 검증 데이터셋에서 68%에서 70%로, 테스트 데이터셋에서 71%에서 74%로 범위가 확장됩니다. 모든 창에서 재현율은 57%에서 59%까지 범위가 확장됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단기 행동을 예측하는 실험들은 데이터 윈도우의 크기에 민감합니다. 10분 동안의 실험에서, 검증 데이터셋의 정밀도는 64%에서 80%로 범위가 나타나며, 테스트 데이터셋의 경우 66%에서 89%까지 변동합니다. 재현율은 89%에서 30%까지 범위가 나타나며, 최적의 균형을 얻으려면 20개에서 30개의 이벤트 윈도우를 사용하는 것이 좋습니다.\n\n![Image 7](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_7.png)\n\n![Image 8](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_8.png)\n\n![Image 9](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_9.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델의 다양한 행동 양상은 짧은, 중간, 그리고 장기적인 참여를 예측하는 것이 서로 다른 접근 방식이 필요한 다른 도전이라는 것을 보여줍니다. 더 높은 정밀도는 단기적으로 참여하는 사용자를 놓치지 않을 가능성을 보여줍니다. 그러나 장기간의 참여가 떨어질 것으로 예측하는 것은 더 어려운 과제입니다.\n\n실제 상황에서 모델의 선택은 참여하지 않은 사용자를 놓치는 것을 우선시해야 하는지, 아니면 동기를 부여받은 사용자를 잘못 분류하는 것을 우선시해야 하는지를 고려해야 합니다.\n\n30분 참여 예측과 데이터 창의 30개 이전 관측치를 사용한 우수한 모델/실험 구성이 우리의 최고의 성과를 냈습니다. 이 모델은 AUC 스코어가 0.748을 달성했습니다. 이는 Mao Et Al이 30분 예측에서 약 0.76을 달성한 것보다 약간 떨어지는 성과입니다. 이는 데이터의 분산 부족으로 인한 것으로 여겨집니다. 예를 들어, 첫 번째 사용자 세션에는 많은 동일한 변수들이 포함되어 있습니다. 이들은 다음을 포함합니다:\n\n- 누적 플랫폼 시간 및 누적 세션 시간.\n- 플랫폼과 세션 전체 이벤트 수.\n- 이전 세션 통계.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**사용자 세그먼트별 평가**\n\n플랫폼에 작은 기여만 한 사용자를 고려해 역사적 정보를 포함시키면 많은 중복이 발생한다는 것을 알고 있습니다. 이는 대부분의 사용자들에 해당합니다.\n\n다양한 세션 길이 별 성능을 이해하기 위해 사용자 작업 세션에서 지낸 누적 시간에 따른 정확도, 재현율 및 AUC를 검토합니다. 평가와 테스트 세션을 유사성으로 통합합니다. 정밀도와 재현율의 가장 큰 요인은 사용자가 작업 세션에서 얼마나 시간을 보내는지에 달려 있습니다. 데이터 창 크기에 관계 없이, 사용자가 작업 세션에서 60분을 보낸 후에는 모델의 효과가 극적으로 향상됩니다.\n\n![이미지](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기존 온라인 플랫폼 연구와 일관성 있는 결과를 보여주었으며, 최소한의 히스토리로 사용자에 대한 추론의 어려움을 보여줍니다. 이는 사용자 행동에 대한 우리 이전 분석을 뒷받침합니다.\n\n산키 다이어그램에 의하면, 한 번 사용자 행동이 시작되면 짧은 기간 내에는 비교적 예측 가능합니다. 사용자가 20분 미만의 세션에 참여하면 다음 세션이 또한 20분 미만인 확률이 66.7%이고, 40분을 넘는 세션이 될 확률은 15.4%에 불과합니다. 20분에서 40분 사이의 세션을 가진 사용자는 다음 세션이 20분을 넘을 확률이 약 50%입니다. 이는 사용자가 참여하면 더 오래 플랫폼에 투자하게 되는 경향을 나타냅니다.\n\n사용자의 역사가 거의 없는 사용자 행동을 예측하는 어려움이 분명합니다. 사용자가 5회 세션을 완료하거나 세션이 40분을 초과하면 성능이 일반적으로 향상되며, 가장 안정적인 결과는 20~30 이벤트의 데이터 창을 포함합니다. 이 정보는 Zooniverse 및 다른 온라인 플랫폼의 행동 예측 모델을 개선하는 데 도움이 될 수 있습니다. 모델이 빈번한 기여자에게는 효과적이지 않으므로, 이 그룹의 참여 증가를 위한 개입 시기를 결정하는 데 사용해서는 안 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n연구 결과에서 알 수 있듯이, LSTMs는 시계열 작업 모델링에 효과적입니다. 모든 실험과 모델 구성에서, 사용자가 현재 작업 세션에서 탈락하는지 여부를 학습할 수 있었습니다. 복잡한 특징 집계 대신, 데이터 창을 사용하여 데이터 전체의 의존 관계를 학습했습니다. 이는 GPU를 필요로하며 계산적으로 더 많은 비용이 소요됩니다만, 고전적인 머신 러닝보다 훨씬 적은 시간과 도메인 지식이 필요합니다.\n\n모델은 두 가지 요인으로 제한되었습니다. 첫 번째는 사용자가 언제 탈락할 지 나타내는 기능의 부족이었습니다. 미래 실험에서는 Mao et al.의 연구에서 설명된 특징을 사용하여 모델 성능이 향상되는지 확인해야합니다.\n\n두 번째 제한은 극복하기 어렵습니다. 최소한의 데이터로 사용자에 대해 일반화하는 것은 어렵습니다. 대부분의 Zooniverse 사용자가 단기적이거나 \"관심을 끄는 사람\"이기 때문에 Zooniverse에서 행동을 모델링하는 것은 도전적입니다. 최소한의 경험이있는 사용자에 대한 모델의 한계를 밝히고 설명하는 것은 이전 데이터 분석과 플랫폼 사용 방법 조사만이 가능합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_12.png\" /\u003e\n\n머신 러닝을 시작하기 전에 데이터와 문제를 명확히 이해하는 것이 중요함을 강조합니다. 초기 분석을 통해 모델의 효과에 대한 사전 확률을 설정할 수 있었고, 해당 분석을 통해 이를 확인할 수 있었습니다.\n\n실제 산업 환경에서 모델이 Zooniverse에 대한 개입(뱃지 또는 메시지와 같은)을 시간화하는 데 사용된다면, 하이브리드 접근 방식을 권장합니다. 짧은 기간 사용자에게는 규칙 기반 논리를 사용하여 개입을 타이밍하고, 장기 사용자에게는 모델이 개입을 안내하는 데 효과적일 수 있습니다.\n\n# 각주\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- LSTMs가 어떻게 작동하는지 멋진 설명을 찾으려면 Colah의 LSTMs에 대한 블로그 포스트를 확인하시기를 추천합니다. LSTMs의 역전파 알고리즘을 이해하고 싶다면 Goodfellow Et Al의 \"Deep Learning\"의 10장을 살펴보시는 걸 추천드립니다.","ogImage":{"url":"/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_0.png"},"coverImage":"/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_0.png","tag":["Tech"],"readingTime":11},{"title":"당신이 얼마나 많은 무게를 진다는지 알 수 없어요","description":"","date":"2024-06-19 06:43","slug":"2024-06-19-youhavenoideahowmuchweighticarry","content":"\n\n\n![Image](/assets/img/2024-06-19-youhavenoideahowmuchweighticarry_0.png)\n\n다른 쪽이 항상 푸른 것은 아니에요..\n\n대부분의 사람들은 막내가 되는 것이 축복이라고 생각해요.\n\n맏이나 중간 아이들은 종종 더 어린 아이를 부러워하며, 더 어린 사람이라면 삶이 훨씬 쉬울 것이라고 생각해요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자신의 문제를 말해도 되지 않고, 특히 자신의 감정이 무시당한다는 것에 대해 열거나 살 수 없다고 상상해보세요.\n\n가족들이 당신이 자리하고 있는 자택이 조용한 곳으로 변모하는 것을 목격하고 있는 것을 상상해보세요.\n\n이제는 다른 사람들이 각자 바쁜 삶을 살아가고 있어서 매일 집에서 혼자 남아 있는 상황을 상상해보세요. 당신은 그들이 취해 침대에서 술에 취해 잠들어있는 모습만을 보게 될 것입니다.\n\n그들은 항상 우리는 가족이라고 말했지만, 모든 사람이 나를 잊어버렸다. - 나에게는 항상 너무 바빠서 소홀했다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 어린 사람이 늘 축복이 되는 것은 아니에요... 저에게는 부담이에요.","ogImage":{"url":"/assets/img/2024-06-19-youhavenoideahowmuchweighticarry_0.png"},"coverImage":"/assets/img/2024-06-19-youhavenoideahowmuchweighticarry_0.png","tag":["Tech"],"readingTime":1},{"title":"딥 러닝 모델 최적화를 위한 가중치 양자화","description":"","date":"2024-06-19 06:40","slug":"2024-06-19-OptimizingDeepLearningModelswithWeightQuantization","content":"\n\n![Image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png)\n\n# 📚딥러닝에서 양자화란?\n\n딥러닝에서 양자화에 대해 이야기해보겠습니다. 딥러닝에서 양자화가 왜 중요한지 궁금했던 적이 있나요? 딥러닝과 대규모 언어 모델(LLMs)이 아주 강력하다고는 하지만 많은 도전 과제를 가지고 있어요. 이러한 모델들이 크기 때문에, 많은 계산 성능과 메모리가 필요하여 자원이 제한된 곳에서 사용하기 어려워집니다. 게다가, 예측을 할 때 많은 에너지를 소비할 수 있어서, 한정된 컴퓨팅 자원으로 추론을 하는 것이 불가능해질 수도 있어요.\n\n양자화는 이러한 문제를 해결하기 위해 모델의 크기를 줄여 더 쉽게 다루고, 거의 동일한 성능을 유지할 수 있도록 돕습니다. 이 과정은 모델의 매개변수 수와 데이터 유형의 정밀도를 수정하는 것을 포함합니다. 이를 통해 모델은 가볍고 빠르게 되어, 더 많은 곳에서 실행되고 더 적은 에너지를 사용할 수 있게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델의 크기는 매개변수(크기)의 수를 값들의 정밀도(데이터 형식)로 곱해서 계산됩니다.\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_1.png)\n\n그래서 모델의 크기를 효율적으로 줄이는 방법에 대한 중요한 질문은 무엇일까요? 음, 이를 위한 몇 가지 방법이 있습니다. 매개변수의 수를 줄이거나 데이터 형식을 낮추는 것이 가능합니다. 그러나 매개변수의 수를 줄이는 것은 모델을 더 작고 단순하게 만드는 것을 의미하며, 이는 모델의 품질에 상당한 영향을 줄 수 있어 매우 tricky할 수 있습니다. 더 나은 옵션은 데이터 형식의 정밀도를 조절하는 것입니다. 이것이 양자화가 등장하는 이유입니다 - 이를 통해 모델 가중치를 낮은 정밀도 형식으로 저장할 수 있습니다. 이 방법은 모델의 효과를 유지하면서 가볍고 빠르게 만들어줍니다.\n\n아래는 양자화가 딥러닝에서 중요한 이유인 몇 가지 주요 이유입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 효율성: 양자화는 모델 내의 숫자 값의 정밀도를 부동 소수점에서 정수로 줄입니다. 이겈 간단해 보이지만, 계산을 훨씬 쉽고 빠르게 만들어주어 일을 빨리 처리할 수 있게 해줍니다!\n- 메모리 절약: 부동 소수점에서 정수로 변환할 때 비트 수를 줄이면, 모델 크기를 크게 축소할 수 있습니다. 이것은 저장 공간과 메모리가 제한된 스마트폰이나 임베디드 시스템과 같은 기기에 모델을 배포할 때 아주 유용합니다.\n- 에너지 소비: 모델 크기가 작아지면 모델을 실행하는 데 더 적은 계산 능력이 필요합니다. 이는 특히 배터리로 작동하는 기기에 모델을 배포할 때 유용합니다.\n- 모델 배포: 모델이 작고 더 빠르게 실행될 때, 전용 대규모 서버 대신 다양한 장소에서 모델을 사용하기 쉬워집니다. 이는 자율 주행 자동차나 실시간 번역 서비스와 같이 빠른 응답이 필요한 작업에 중요합니다.\n\n## 양자화 종류\n\n딥러닝에서 양자화는 일반적으로 세 가지 주요 유형으로 구분됩니다:\n\n- 사후 학습 정적 양자화 (PTQ): PTQ는 이미 훈련된 모델을 추가로 훈련하지 않고(가중치 및 활성화 모두) 줄이는 작업을 수행합니다. 사용하기 매우 간단하고, 훈련을 마친 후 모델을 빠르게 작게 만들어주는 데 도움이 됩니다. 단지 기억해 두세요! 훈련 중에 모델을 양자화하지 않기 때문에 원본 모델과 성능에 차이가 있을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![그림](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_2.png)\n\n- 집행 후 다이나믹 양자화 또는 다이나믹 양자화: 이 방법은 훈련이 완료된 후에 모델 가중치를 줄이고, 활성화를 동적으로 처리합니다(추론 중에). 이 방법은 다른 유형과 크기의 입력을 다루는 모델에 아주 편리합니다. 그러나 모델이 실행되는 동안 활성화를 실시간으로 조정하기 때문에 정적 양자화보다 약간 느릴 수 있습니다. 또한, 이 방법의 또 다른 단점은 모든 장치가 이 동적 접근을 처리할 수 없다는 점이므로 어디에 이 방법을 사용할지 계획할 때 고려해야 합니다.\n\n![그림](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_3.png)\n\n- 양자화 인식 훈련(QAT): 마지막 공통 방법은 QAT입니다. 이는 양자화를 직접 훈련 과정에 통합하여 모델 성능을 유지합니다. 이것은 모델 최적화 중 양자화 효과를 고려함으로써 위 두 가지 방법보다 성능을 더 잘 보존할 수 있습니다. 결과적으로, QAT는 조금 더 많은 시간과 에너지를 요구합니다. 학습 작업 및 양자화를 동시에 조정하기 때문에 더 오래 훈련에 걸리고 구현하기는 훨씬 복잡합니다. 정확도가 필요한 경우, QAT는 모델을 효과적이고 효율적으로 유지하는 데 큰 차이를 만들 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_4.png)\n\n# 📖 부동 소수점 숫자 구성\n\n데이터 형식을 변경하면 모델 크기를 줄이는 이유에 대해 자세히 살펴보겠습니다. 컴퓨터에서 숫자에 대해 이야기할 때, 0과 1에 대해 모두 이야기합니다. 이진 인코딩 시스템은 컴퓨터 작업의 기초이며, 정수 및 부동 소수점 숫자와 같은 다양한 숫자 표현은 이러한 비트를 구성하는 특정 방법을 갖고 있습니다.\n\n## 정수 표현\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정수에 대한 가장 일반적인 형식은 부호 있는 정수와 부호 없는 정수입니다.\n\n부호 없는 정수:\n\n* 비트: 모든 비트가 숫자의 크기를 나타냅니다.\n* 범위: 0부터 2n-1까지 (여기서 n은 비트의 수)입니다.\n\n부호 있는 정수:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 첫 번째 비트는 숫자가 양수 (0)인지 음수 (1)인지를 나타냅니다.\n- 나머지 비트는 숫자의 크기 또는 이른바 크기를 보여줍니다. 여기서 이진값은 음수 숫자에 대해 반전되고 1이 더해집니다.\n- 범위: -2ⁿ⁻¹ ~ 2ⁿ⁻¹-1\n\n## 부동 소수점 표현\n\n- 부호 비트 (1 비트): 숫자의 부호를 나타냅니다; 0은 양수이고 1은 음수입니다.\n- 지수: 바이어스로 조정된 지수를 나타냅니다. 저장된 지수에서 바이어스를 빼면 실제 지수가 계산됩니다. 지수는 사실적으로 숫자의 중요한(또는 가수) 부분을 2의 거듭제곱으로 확장하여 부동 소수점 숫자가 매우 크거나 매우 작은 값을 간결한 형식으로 표현할 수 있도록 합니다.\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 유의적/맨티사: 숫자의 정밀도를 나타냅니다.\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_6.png)\n\n## 다른 데이터 유형의 생성\n\nFloat32: 숫자를 나타내는 데 32비트를 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 부호에는 1 비트가 사용됩니다\n- 지수에는 8 비트가 사용됩니다\n- 나머지 23 비트는 유효숫자를 나타냅니다\n- FP32는 높은 정밀도를 제공하지만, 계산 및 메모리 사용량이 많은 것이 단점입니다.\n\n이미지 링크:\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_7.png)\n\nFloat16: 숫자를 저장하는 데 16 비트를 사용합니다\n\n- 부호에는 1이 사용됩니다\n- 지수에는 5가 사용됩니다\n- 유효숫자에 10이 사용됩니다\n- 이로 인해 더 효율적인 메모리 사용 및 빠른 연산이 가능하지만, 범위 및 정밀도가 줄어들어 숫자의 불안정성을 초래할 수 있고, 이는 모델 정확도에 영향을 줄 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_8.png\" /\u003e\n\n\"float\"이 종종 \"전체 정밀도\"(4 바이트)로 불리는 반면, \"float16\"은 \"반 정밀도\"(2 바이트)로 불립니다.\n\n## 일반적인 하위 정밀도 데이터 유형\n\n양자화를 수행하는 두 가지 일반적인 방법이 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- float32 -` float16\n- float32 -` int8\n\n## 양자화의 효과\n\n대규모 모델인 BLOOM과 같은 경우, 약 1760억 개의 파라미터를 갖고 있는 모델을 다룬다고 상상해봅시다. float32를 사용하면 모델 크기는 176*10**9 x 4 바이트 = 704GB가 됩니다. 그러나 float16로 전환하면 352GB로 줄어들고, int8로 전환하면 176GB로 줄어듭니다. 이는 메모리 공간에서 굉장한 절감을 의미합니다. 176GB라도 여전히 많은 개인 컴퓨터에 대한 큰 도전이 될 수 있습니다.\n\nfloat32에서 float16으로 양자화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nfloat32에서 float16로 변경하는 것은 꽤 간단합니다. 왜냐하면 두 형식 모두 숫자를 표현하는 방식이 비슷하기 때문이죠. 그러나 양자화를 구현하기 전에 몇 가지 고려해야 할 사항이 있습니다:\n\n- 소프트웨어 및 하드웨어 호환성: 먼저, 사용 중인 패키지가 float16을 처리할 수 있는지 확인해보세요. 또한, 하드웨어가 지원하는지도 확인해야 합니다. NVIDIA의 튜링 및 암페어 또는 구글의 TPU와 같은 현대 GPU 및 TPU는 float16과 잘 작동하도록 제작되었기 때문에 학습 및 추론 프로세스가 속도가 향상됩니다. 그러나 Intel CPU는 저장 유형으로 float16을 지원하고 있지만, 연산은 float32로 변환한 후에 수행됩니다.\n- 정밀도 요구 사항: 모델이 얼마나 정밀해야 하는지를 고려해보세요. 다른 말로, 낮은 정밀도에 얼마나 민감한지 생각해보세요. 일부 의료 영상 처리와 같이 모든 작은 세부 사항이 중요한 작업/모델의 경우, float16과 같은 낮은 정밀도로 내려가면 중요한 세부 사항이 손실되어 모델의 성능에 영향을 줄 수 있습니다.\n\nfloat32에서 int8로의 양자화\n\nfloat32에서 int8로의 양자화는 더 어렵습니다. 왜냐하면 int8은 256가지만 다룰 수 있고, 이는 float32가 다루는 광대한 범위와는 비교할 수 없이 작습니다. float32는 약 -3.4e38 ~ 3.4e38 범위에서 약 40억 개의 숫자를 처리합니다. 이 과제는 float32 값의 특정 범위를 int8의 매우 제한된 공간에 어떻게 압축할지 찾는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n계속해서 진행하고, 우리가 int8로 효과적으로 양자화하는 방법에 대해 자세히 살펴볼 거예요.\n\n# int8로 양자화하는 방법\n\n## 균일 양자화\n\n이 방법은 입력을 출력으로 매핑하는 간단한 선형 함수를 사용합니다. 선에 놓인 간격이 동일한 점들을 상상해보세요 — 균일 양자화는 이들이 변환될 때 모두 잘 정렬되어 있도록 유지합니다. 빠르고 쉽지만, 여기 주목할 점이 있어요: 데이터가 처음부터 고르게 분포되어 있지 않다면, 데이터의 분포를 항상 잘 보존하지는 못할 수도 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 부동 소수점 값을 두 숫자 α와 β 사이에 매핑하는 것이 균일 양자화의 작동 원리입니다. 이 두 값을 α와 β라고 부르겠습니다. 그리고 그 값을 [-2ᵇ⁻¹, 2ᵇ⁻¹–1]의 일정 범위로 변환합니다. 이 범위를 벗어나는 값이 있다면 가장 가까운 한계값으로 잘립니다 — 이것을 클리핑이라고 합니다.\n\n부동 소수점 숫자(xf)를 8비트 표현(xq)으로 변환할 때에는 스케일 팩터(S)를 사용합니다. 이는 원본 데이터를 int8의 새로운 형식 [-128, 127]에 맞춰주는 데 도움을 줍니다. 그리고 원본 데이터의 0은 새 데이터의 0과 일치하게 됩니다. 이것이 대칭 양자화라고 부르는 개념입니다.\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_9.png)\n\n## 양자화 스케일(S)를 계산하는 방법?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식으로 바뀐 텍스트입니다:\n\nCompute the max value of xf:\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_10.png)\n\nCompute the quantization scale (S):\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_11.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n컨버팅 비티에이엘 테이블:\n\n\n| 값       | 양자화된 값 |\n|----------|------------|\n|   3.14   |    3.0     |\n|  -2.718  |   -2.5     |\n|   6.626  |    6.5     |\n\n\n오리지널 값으로 돌아가기:\n\n\n| 양자화된 값 | 값       |\n|------------|----------|\n|    3.0     |   3.14   |\n|   -2.5     |  -2.718  |\n|    6.5     |   6.626  |\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대칭 양자화는 제로 주변을 균일하게 취급합니다. 즉, 데이터의 상승 및 하락(양수 및 음수 값)을 균형 있게 처리하여 모든 것이 균형을 이룹니다. 데이터가 제로 중심이거나 즉, 제로 양쪽으로 고르게 퍼져있을 때 특히 유용합니다.\n\n하지만 여기서 중요한 점은 대칭 양자화가 제로 주변에 깔끔하게 정렬되지 않은 데이터에는 부적합할 수 있다는 것입니다. 데이터가 더 치우쳐져 있다면, 이 방법은 범위의 모든 부분을 동일하게 처리하기 때문에 더 많은 양자화 오류를 발생시킬 수 있습니다.\n\n이 문제를 해결하기 위해, 비균일 또는 비대칭 양자화가 있습니다. 때로는 이를 아핀 양자화라고도 부릅니다. 이 기술은 데이터의 다른 부분에 대해 서로 다른 방식으로 스케일과 제로 포인트를 조정하기 때문에 대칭적으로 분포되지 않은 데이터 집합에 더 적합합니다.\n\n간단한 예제로 이를 시도해 봅시다. 우리가 NumPy의 랜덤 정규 함수를 사용하여 가중치 배열을 만들었기 때문에 배열은 제로 중심입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 원본 가중치 배열\nweights = np.random.normal(size=(20000)).astype(np.float32)\nweights = torch.from_numpy(weights)\nprint(weights.mean(), weights.min(), weights.max())\n\u003e\u003e\u003e tensor(0.0057) tensor(-3.9224) tensor(4.4791)\n```\n\n![Image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_14.png)\n\n```js\n# 대칭 방식을 사용하여 양자화\nweights_sym_quant, weights_sym_dequant = symmetric_quantize(weights)\nprint(weights_sym_quant.double().mean(), weights_sym_quant.double().min(), weights_sym_quant.double().max())\nprint(weights_sym_dequant.double().mean(), weights_sym_dequant.double().min(), weights_sym_dequant.double().max())\n\u003e\u003e\u003e tensor(0.1585, dtype=torch.float64) tensor(-111., dtype=torch.float64) tensor(127., dtype=torch.float64)\n\u003e\u003e\u003e tensor(0.0056, dtype=torch.float64) tensor(-3.9148, dtype=torch.float64) tensor(4.4791, dtype=torch.float64)\n```\n그런 다음 대칭 양자화 함수를 적용하면, 새로 양자화된 배열도 거의 0에 가까운 평균값을 가지며, 최솟값은 -111이고 최댓값은 127입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제, 우리는 데이터를 원래의 부동 소수점 범위로 되돌리는 시도를 할 것입니다. 이것이 바로 양자화 해제(dequantization)라고 불리는 과정입니다. 양자화를 해제한 후에는, 양자화 해제된 배열의 평균, 최소값 및 최대값이 대략 원래 값과 동일해야 합니다.\n\n## 비균일 양자화\n\n비대칭 양자화의 경우, 양자화 값을 계산할 때 정수가 추가됩니다. 이것을 제로 포인트 (Z)라고 합니다. Z는 float32 영역에서 0의 값과 대응합니다.\n\n양자화된 값 계산:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_15.png)\n\n스케일(S) 값을 계산하세요:\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_16.png)\n\n제로포인트(Z) 값을 계산하세요:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Optimizing Deep Learning Model with Weight Quantization](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_17.png)\n\n비대칭 양자화는 범위의 다른 부분에 대해 스케일과 제로 포인트를 다르게 조정하여 비대칭 데이터 분포를 효과적으로 처리할 수 있습니다. 그러나 스케일과 제로 포인트 2개의 매개변수가 필요하기 때문에 구현 및 최적화 과정이 복잡해질 수 있고, 양자화 및 역양자화 단계에서 추가적인 계산 능력이 필요할 수 있습니다.\n\n비대칭 양자화는 데이터 분포를 조정함으로써 데이터 범위 내에서 스케일과 제로 포인트를 다르게 조정하여 불규칙한 데이터 분포를 훌륭히 처리할 수 있습니다. 데이터가 양자화 다리를 건널 때 더 편안하게 걷도록 데이터의 신발을 맞춤 제작하는 것과 같은 원리입니다!\n\n하지만 여기서 중요한 점은 두 가지 매개변수 - 스케일과 제로 포인트 - 를 사용자 정의하기 때문에 설정 및 세밀한 조정이 약간 까다로울 수 있습니다. 게다가, 양자화 및 역양자화 시에 조금 더 많은 계산 노력이 필요할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 비대칭 방식을 사용하여 양자화하기 - 정규 분포 데이터\nweights_assym_quant, weights_assym_dequant = assymmetric_quantize(weights)\nprint(weights_assym_quant.double().mean(), weights_assym_quant.double().min(), weights_assym_quant.double().max())\n\u003e\u003e\u003e tensor(-8.8287, dtype=torch.float64) tensor(-128., dtype=torch.float64) tensor(127., dtype=torch.float64)\n\u003e\u003e\u003e tensor(0.0056, dtype=torch.float64) tensor(-3.9207, dtype=torch.float64) tensor(4.4808, dtype=torch.float64)\n\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_18.png)\n\n정규 분포 배열 예제를 살펴보았지만, 이것은 간단한 경우입니다. 좀 더 어려운 비정규 분포를 가진 경우를 시도해 보겠습니다.\n\n```python\n# 비정규 분포 데이터 생성\nskewed_weights = np.random.exponential(scale=2, size=20000) - 7 # 데이터를 음수 값과 양수 값을 모두 가지도록 이동\nskewed_weights = torch.from_numpy(skewed_weights)\nprint(skewed_weights.mean(), skewed_weights.min(), skewed_weights.max())\n\u003e\u003e\u003e tensor(-5.0192, dtype=torch.float64) tensor(-6.9999, dtype=torch.float64) tensor(16.4827, dtype=torch.float64)\n```\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 대칭 방식을 사용하여 양자화\nweights_sym_quant, weights_sym_dequant = symmetric_quantize(skewed_weights)\nprint(weights_sym_quant.double().mean(), weights_sym_quant.double().min(), weights_sym_quant.double().max())\n\u003e\u003e\u003e tensor(-38.6737, dtype=torch.float64) tensor(-54., dtype=torch.float64) tensor(127., dtype=torch.float64)\n```\n\n이 분포는 정규분포가 아니기 때문에 양자화된 가중치의 평균 값은 -38.67, 최솟값은 -54이며 최대값은 127입니다. 문제는 전체 int8의 범위가 완전히 활용되지 않는다는 것입니다. 최솟값이 -64인데 이는 양자화가 사용 가능한 비트를 최대로 활용하지 못한다는 것을 의미합니다. 이로 인해 많은 서로 다른 값을 동일한 양자화된 값으로 반올림하여 고유성과 데이터 내의 세부 정보를 상실할 수 있습니다.\n\n가중치를 다시 부동소수점으로 역양자화할 때, 평균값은 대략적으로 원래 가중치의 값에 도달합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 비대칭 방법을 사용하여 양자화 - 정상 분포 데이터\nweights_assym_quant, weights_assym_dequant = assymmetric_quantize(skewed_weights)\nprint(weights_assym_quant.double().mean(), weights_assym_quant.double().min(), weights_assym_quant.double().max())\n\u003e\u003e\u003e tensor(-106.5096, dtype=torch.float64) tensor(-128., dtype=torch.float64) tensor(127., dtype=torch.float64)\n```\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_20.png)\n\n양자화된 값들을 원래의 부동 소수점 범위로 돌리면 어떻게 되는지 알아보겠습니다. 대칭 방법에서의 값들은 원래 데이터와 비교했을 때 그렇게 고른 분포를 보여주지 않는데, 비대칭 방법의 경우와는 다르게 퍼져 있는 것이 흥미롭습니다.\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_21.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 📝 코드 구현\n\n파이토치 양자화를 사용하여 양자화 예제를 작업해 보겠습니다.\n\n이 예제에서는 MobileNetV2 모델과 MINIST 데이터셋을 사용할 것입니다. 데이터셋에 대한 자세한 내용은 여기에서, 그리고 데이터셋을 로드하는 방법은 여기에서 확인할 수 있습니다.\n\nMobileNetV2를 양자화하려면 네트워크에 일부 수정을 구현해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- In InvertedResidual 블록의 torch.add가 nn.quantized.FloatFunctional()로 대체되었습니다.\n\n```js\n- self.skip_add = torch.add()\n+ self.skip_add = nn.quantized.FloatFunctional()\n```\n\n- 양자화 전에 Conv+BN 및 Conv+BN+Relu 모듈을 결합하는 fuse_model() 메서드가 추가되어 메모리 액세스를 줄이고 수치 정확도를 향상시켜 모델의 효율성을 높입니다. 이 실천은 양자화된 모델에서 일반적입니다.\n\n```js\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=10, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n+     self.quant = QuantStub()\n+     self.dequant = DeQuantStub()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPyTorch 프레임워크를 사용하여 모델을 양자화하는 일반적인 흐름은 다음과 같습니다:\n\n```python\n# 양자화하기 전에 Conv+BN 및 Conv+BN+Relu 모듈을 퓨즈합니다 (이 작업은 숫자를 변경하지 않습니다)\ndef fuse_model(self, is_qat=False):\n    fuse_modules = torch.ao.quantization.fuse_modules_qat if is_qat else torch.ao.quantization.fuse_modules\n    for m in self.modules():\n        if type(m) == ConvBNReLU:\n            fuse_modules(m, ['0', '1', '2'], inplace=True)\n        if type(m) == InvertedResidual:\n            for idx in range(len(m.conv)):\n                if type(m.conv[idx]) == nn.Conv2d:\n                    fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)\n```\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_22.png)\n\n- QConfig를 사용하여 연산자가 어떻게 관찰되어야 하는지 구성합니다. 이 코드에서는 단순한 최소/최대 관찰자를 사용하여 양자화 매개변수를 결정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nquantized_model.qconfig = torch.ao.quantization.default_qconfig\n```\n\n2. 준비하기: 지정된 qconfig를 기반으로 Observer/FakeQuantize 모듈을 모델에 삽입합니다.\n\n```js\ntorch.ao.quantization.prepare(quantized_model, inplace=True)\n```\n\n3. 모델을 캘리브레이션하여 가중치와 활성화에 대한 양자화 매개변수를 결정합니다. 이는 훈련 데이터셋으로 수행됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nevaluate(quantized_model, criterion, data_loader, neval_batches=num_calibration_batches)\n\n\n4. Convert the calibrated model to a quantized model.\n\n\ntorch.ao.quantization.convert(quantized_model, inplace=True)\n\n\nWe will load the pretrained model for the MNIST dataset as the original model, quantize this model, and compare the results in both size and performance. Performance is evaluated using Top 1 and Top 5 Accuracy.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 👑결과👑\n\n![결과 이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_23.png)\n\n모델 크기가 8.9MB에서 2.35MB로 줄어든 건 정말 놀라운 일이에요! 거의 4배나 크기가 줄었어요! 🌟\n\n성능도 매우 좋아서, 양자화된 모델의 최상위 1위와 최상위 5위 정확도는 원본과 비슷합니다. 최대/최소 옵서버만 사용해서 양자화 매개변수를 선택한 것에도 불구하고요. 그래서 거의 공간을 차지하지 않으면서도 약속받는 결과를 확인할 수 있어요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다른 옵저버의 결과를 확인해보면, 이 옵저버와 비교하여 성능이 어떤지 알 수 있어요. 새 옵저버는 자동으로 양자화 매개변수를 결정할 거에요.\n\n디폴트 qconfig를 사용하는 대신에, 구성을 x86 아키텍처로 설정할 거에요. 이 아키텍처는 가중치를 채널 단위로 양자화하고, 활성화도를 수집하고 최적의 양자화 매개변수를 선택하는 히스토그램을 사용해요. 나머지 흐름은 동일하게 유지돼요.\n\n```js\nper_channel_quantized_model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n```\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_24.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n새 양자화 아키텍처로 얻는 결과는 Top 1 및 Top 5 성능 모두 강력한 출력을 유지한다는 것을 관찰할 수 있습니다. 그리고 가장 좋은 부분은 또 다른 양자화 방법과 모델 크기를 거의 동일하게 유지할 수 있다는 것입니다.\n\n**Notebook**: [링크]\n\n# 📕 최종 생각\n\n마무리하며, 사후 훈련 동적 양자화는 머신러닝 모델을 배포하기 위해 최적화하는 편리하고 효율적인 요령입니다. 이 방법은 모든 훈련이 완료된 후 가중치와 활성화를 조정함으로써 원래의 부동 소수점 모델과 가끔 수용 가능한 성능을 보장하며 동등한 성능을 나타낼 수 있습니다. AI 프로젝트를 빠르고 가벼워 만들고 싶다면, 이 방법이 진정한 게임 체인저가 될 수 있을 것입니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n- [Achieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with TensorRT](https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/)\n  \n- [PyTorch 공식 문서 - 양자화](https://pytorch.org/docs/stable/quantization.html)\n  \n- MNIST 상업적 이용을 위한 라이선스: GNU General Public License v3.0. 링크: [MNIST 라이선스](#)","ogImage":{"url":"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png"},"coverImage":"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png","tag":["Tech"],"readingTime":16},{"title":"선형 회귀를 사용한 비농업 부문 고용 예측","description":"","date":"2024-06-19 06:38","slug":"2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression","content":"\n\n\n![image](/assets/img/2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression_0.png)\n\n선형 회귀는 가끔 단숨함과 선형 종속적 결과 때문에 무시당하기도 합니다. 그러나 많은 복잡한 예측 작업을 선형 회귀를 사용하여 해결할 수 있습니다. 한 번 매우 성공한 헤지 펀드 관리자로부터 들은 적이 있는데, 그들의 정교한 거래 모델 중 하나는 간단한 선형 회귀 모델에 의존했다고 말씀하셨습니다.\n\n이 기사에서는 파이썬을 사용하여 간단한 선형 회귀를 통해 미국의 고용 데이터를 예측하는 방법을 보여줍니다.\n\n# 비농업 실업자수란 무엇인가요?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비농업 실업률(NFP)은 미국에서 중요하게 살펴보고 있는 경제 지표로, 미국 노동 시장의 건강 상태를 판단하는 중요한 기준 역할을 합니다. 매달 발표되는 이 취업 보고서는 미국의 고용 상황을 종합적으로 보여줍니다. 이 보고서에서는 농업 부문, 가정부 및 비영리 기관의 일자리를 제외한 미국 내 유료 종업원 수의 순 증가량이 공개됩니다.\n\n이 NFP 보고서는 지난 달 동안 특정 부문을 제외한 미국 내 유료 종업원 수의 순 증감을 보여줍니다. 이는 미국 경제의 전반적인 강도와 방향에 대한 통찰력을 제공하기 때문에 특히 중요합니다. 데이터를 분석함으로써 경제학자, 정책 결정자, 투자자 및 기업은 노동 시장의 건강 상황을 평가하고 취업 트렌드를 추적하며 경제 정책, 투자, 채용 관행에 관한 판단을 내릴 수 있습니다.\n\n또한, NFP 보고서는 금융 시장을 넘어서 통화 환율, 이자율 결정 및 기타 금융 상품 등에도 영향을 미칩니다. 매달 예측을 시도하며 우리만의 모델을 적용해 보고 나온 결과를 살펴봅시다.\n\n우리는 방향성 정확도 및 RMSE를 이용하여 예측을 평가할 것입니다. 이들이 의미하는 것은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 방향성 정확도는 NFP가 상승인 경우와 NFP가 하락인 경우를 비교하여 올바른 예측 수와 예측 수를 비교하는 단순한 이진 측정입니다.\n- RMSE는 평균 제곱근 오차를 의미합니다. 데이터 세트에서 예측 값과 실제 값 사이의 오차의 평균 크기를 측정하는 지표입니다.\n\n더 많은 작업을 보고 싶으시면, 제 웹사이트에서 PDF 도서 카탈로그를 확인하실 수 있습니다. 아래 그림에 첨부된 링크를 따라가시면 됩니다:\n\n![PDF books catalogue](/assets/img/2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression_1.png)\n\n# 알고리즘 만들기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알고리즘을 생성하기 전에 선형 회귀 알고리즘이 어떻게 작동하는지 알아보겠습니다.\n\n선형 회귀는 데이터 점들의 산점도를 통해 직선을 그리는 것과 같습니다. 집의 크기와 가격에 관한 데이터가 있다고 상상해보세요. 이 정보를 수집하여 대부분의 점을 지나가는 최적의 선을 찾는 데 사용합니다. 이 선은 일반적인 추세를 나타냅니다: 집이 커질수록 가격도 올라가는 경향이 있습니다.\n\n이 과정에는 수학적 계산이 필요하여 선이 가능한 모든 데이터 점에 가장 가깝게 위치하도록 합니다. 이 선을 갖고 나면 집의 크기를 알고 있다면 집의 가격을 예측할 수 있습니다. 이 선은 기울기(크기에 따라 가격이 얼마나 변하는지)와 y절편(크기가 0일 때의 가격, 집에 대해서는 의미가 없음)을 갖고 있습니다. 따라서 이는 두 가지 사이의 관계를 이해하고 예측하는 방법입니다.\n\n작업 계획은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 해당 GitHub 저장소에서 NFP 데이터를 다운로드하고 Python에 업로드하세요.\n- NFP 데이터의 차이를 취하세요. 이미 stationary 상태이지만 방향성 정확성을 측정하기 위해 이를 수행합니다.\n- 데이터를 학습 세트와 테스트 세트로 분할하세요.\n- 모델을 학습시키는 데에는 마지막 다섯 개의 NFP 변경 사항을 특성으로 사용하세요. 그런 다음 테스트 세트의 이전에 본 적이 없는 데이터에 대해 예측하세요.\n- 예측된 데이터와 실제 데이터를 평가하고 비교하세요.\n\n아래 코드를 사용하여 프로세스를 구현하세요 (저장소에서 NFP 데이터를 다운로드해야 함):\n\n```js\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef data_preprocessing(data, num_lags, train_test_split):\n    # 데이터 처리를 위해 준비\n    x = []\n    y = []\n    for i in range(len(data) - num_lags):\n        x.append(data[i:i + num_lags])\n        y.append(data[i+ num_lags])\n    # 데이터를 넘파이 배열로 변환\n    x = np.array(x)\n    y = np.array(y)\n    # 데이터를 학습 및 테스트 세트로 분할\n    split_index = int(train_test_split * len(x))\n    x_train = x[:split_index]\n    y_train = y[:split_index]\n    x_test = x[split_index:]\n    y_test = y[split_index:]\n    \n    return x_train, y_train, x_test, y_test \n# 시간 인덱스가 설정되지 않았다면 설정하세요\ndata = pd.read_excel('NFP.xlsx').values\ndata = np.reshape(data, (-1))\ndata = np.diff(data)\nx_train, y_train, x_test, y_test = data_preprocessing(data, 5, 0.80)\n# CatBoostRegressor 모델 생성\nmodel = LinearRegression()\n# 데이터에 모델 학습\nmodel.fit(x_train, y_train)\n# 학습에 사용된 데이터에 대해 예측\ny_pred = model.predict(x_test)  # 예측을 위해 X 대신 X_new 사용\n# 오리지널 사인파와 예측값 플롯\nplt.plot(y_pred[-50:], label='예측 데이터', linestyle='--', marker='o')\nplt.plot(y_test[-50:], label='실제 데이터', marker='o')\nplt.legend()\nplt.grid()\nplt.axhline(y=0, color='black', linestyle='--')\nimport math\nfrom sklearn.metrics import mean_squared_error\nrmse_test = math.sqrt(mean_squared_error(y_pred, y_test))\nprint(f\"테스트의 RMSE: {rmse_test}\")\nsame_sign_count = np.sum(np.sign(y_pred) == np.sign(y_test)) / len(y_test) * 100\nprint('방향성 정확도 = ', same_sign_count, '%')\n```\n\n다음 그림은 실제 데이터와 예측 데이터를 비교합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 코드의 출력은 다음과 같습니다:\n\n```js\n테스트의 RMSE: 188.81\n방향 정확도 = 69.65%\n```\n\n69.65%의 방향 정확도로 보아, 모델은 마지막 변화에서 긍정적 또는 부정적 변화가 발생할지를 예측할 수 있는 것으로 보입니다. RMSE는 예측이 약 188의 오류 항을 가지고 있음을 보여줍니다. 이는 개선할 여지가 많이 남아 있다는 것을 의미합니다.\n\n개선은 특성을 더 추가하거나 래깅된 입력의 수를 변경하고 다른 조건을 추가하는 방식으로 이루어질 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Graph](/assets/img/2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression_2.png)\n","ogImage":{"url":"/assets/img/2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression_0.png"},"coverImage":"/assets/img/2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression_0.png","tag":["Tech"],"readingTime":5},{"title":"인간형 로봇의 문제점","description":"","date":"2024-06-19 06:36","slug":"2024-06-19-TheProblemsWithHumanoidRobots","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-TheProblemsWithHumanoidRobots_0.png\" /\u003e\n\n요즘에 협력로봇에서 우리가 인간형 로봇을 만들고 있는지 물어봤어요. 아니요... 간단히 말해서, 저는 인간형 로봇을 믿지 않아요. 로봇 개나 고양이나 말에 대해서도 별로 믿지 않아요.\n\n하지만 이런 노력 뒤에 있는 기술에 감탄을 금치 못해요. Boston Dynamics의 네 다리 개 모양 로봇인 Spot Mini은 훌륭한 공학적 성취입니다. iPhone이나 FreeStyle Libre Continuous Glucose Monitor만큼 제 마음에 감탄을 일으키는데, 그 정도의 유용성은 아니에요.\n\nBoston Dynamics의 Atlas도 놀라운데, 그 구동기의 강력함과 로봇 제어 루틴의 품질만큼이나 인상적해요. Agility Robotics의 Digit 또한 놀라운 로봇이자 멋진 공학 작품입니다. Shadow Dextrous Hand도 마찬가지로 멋진 공학 작품이에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 시장에는 다른 기업도 있지만, 이 기업들은 현재 다른 기업들보다 훨씬 앞서 나가 있습니다. 멋진 로보틱스 비디오들이 있긴 하지만 (사람을 로봇 코스튬에 넣지 않아도), 좋은 비디오가 제품화된 솔루션을 제공하는 것은 아닙니다. 워이어드가 보스턴 다이내믹스 아틀라스가 파쿠르 루틴을 수행하는 것을 보도한 후, 그들만이 더 심층적으로 파헤쳐보고 시연이 실제로 작동하는 건 20번 중 1번 정도 된다는 사실을 알아냈습니다. 애자일리티는 최근에 땅에서 일어설 수 있는 능력을 시연했는데, 인상적이고 중요하지만 결국 기본적인 문제를 다시 한 번 확인합니다... 그들의 로봇들은 넘어질 수 있는 존재입니다.\n\n인간형 로봇에는 세 가지 특별한 문제가 있습니다. 첫 번째는 계속 발전하는 인공지능으로 극복될 것이라고 믿습니다. 두 번째는 충분한 투자자 자금으로 극복될 수도 있습니다. 세 번째는 아킬레스 건입니다.\n\n- 아직 인공지능이 부족합니다. 강력한 균형 잡는 시스템에 필요한 일반화된 제어기가 부족합니다.\n- 인공지능이 부족한 상태에서의 하드웨어 투자는 나쁜 투자입니다. 인간형 로봇을 제품 수준으로 개발하는 데 필요한 자금은 10억 달러 이상 들어갈 가능성이 매우 높습니다.\n- 생물모방은 적절한 방법이 아닙니다. 인간형 로봇은 대부분의 제조 작업에 대한 올바른 설계 솔루션이 아닙니다.\n\n첫 번째 문제에 대해 이야기해보죠. 로봇을 위한 견고한 컨트롤러들은 어렵습니다. 전년에 스탠포드에서 후원하는 제어 과학 워크숍에 참석했습니다. 이 행사에서 스티븐 보이드 박사가 인상적인 발표를 했습니다. 보이드 박사에 대한 예의를 기리며, 그의 의도와는 다를 수 있지만 제가 받아들인 내용을 간략하게 정리해보겠습니다. 전반적으로 이 발표는 강화 학습을 비롯한 다양한 제어 기법들을 비교하고, 이들이 볼록 최적화 문제로 축소될 수 있음을 명확하게 해주어 문제 공간을 크게 단순화할 수 있다는 것을 명확히 보여주었습니다. 하지만 그는 흥미로운 발언을 했습니다 (직역하면 이런 느낌이었는데, 제가 제대로 받아 썼길 바라며), \"그래서 차원을 6 이하로 줄이면 이 문제들은 고전적으로 해결 가능해집니다.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그거 대단한 포인트였죠. 정확히 우리가 로보틱스에서 하는 일이에요. 우리는 문제의 차원을 6개의 제어 구동 이하로 줄이고, 몇 가지 수학, 볼록 최적화, 강화학습 또는 유사한 기술을 결합하여 컨트롤러를 유도해요. 쿼드콥터 드론은 4개의 구동을 갖고 일반적으로 IMU(IMU)를 갖추고 있어요. 자동차는 스로틀, 브레이크, 스티어링을 갖고 있어요. 비행기는 일반적으로 에일러론, 루더, 엘리베이터, 스로틀을 갖고 있어요. Agility가 아름답게 해낸 것은 보행의 물리학을 간단하게 만들어서 컨트롤러를 스프링-매스 시스템으로 모델링할 수 있다는 것이에요. Boston Dynamics가 인상적으로 해낸 것은 한 제어 규칙에서 다른 것으로 매끄럽게 전환이 가능하다는 것이에요. 그러나 각 컨트롤러는 단순화돼 있어요. 생산에 성공한 핸드 컨트롤러들은 아이겐핸드 또는 저차원 제어 공간으로 차원을 축소했어요.\n\n우리가 제어 공간을 단순화하고 차원을 줄인다 해도, 여전히 하나의 제어 문제를 해결하고 있어요. 우리에게는 ChatGPT와 같은 기초 모델이 모든 문을 여는 능력을 갖춘 모델이 없어요. 다양한 종류의 문 손잡이를 열기 위한 동작을 유도하기 위해 RL 또는 기타 기술이 필요해요. TRI는 최근에 Diffusion Policy 작업으로 학습 작업을 보다 신속하게, 데이터 효율적인 방식으로 보여준 인상적인 성과를 거뒀지만, 그들 또한 문제를 하나씩 해결하고 있어요, 비록 더 빠르게 하고 있지만요.\n\n처음에 말한대로, 머신러닝/인공지능의 발전이 이 문제를 해결할 것이라고 생각해요. 우리는 결국 더 견고한 로보틱스 컨트롤러를 얻게 될 거예요. 그러나 개방적인 세계의 복잡성으로 인해, 이 문제가 자율 주행 자동차를 개발하는 것만큼 힘들거나 심지어 더 어렵다는 합리적인 주장도 있어요. 예를 들어, 자율 주행 자동차는 수동적으로 안정해요. 액체든 고체든 상관없이 운반해도 괜찮아요. 그러나 인간형 로봇이 볼링공이 담긴 상자를 운반한다면, 제어 문제는 아주 어려워져요. 우리는 몸을 균형 있게 해주는 많은 다양한 근육, 특히 머리를 섬세하게 조정하여 중심질량이 발밑에 머무를 수 있도록 하는 목 근육을 통해 몸을 안정화시켜요. 그것은 정말 어려운 일이에요! 그리고 보세요, 아직도 자율 주행 자동차를 위한 견고한 AI에 대한 타임테이블을 정할 수 없어요.\n\n여기서 두 번째 문제로 이어져요. 하드웨어는 비싸요. 그리고 복잡한 하드웨어는 정말 비싸요. 개방적이고 해결되지 않은 AI 문제와 복잡한 하드웨어 공학 비용을 결합하면 자금 요건이 끝없이 늘어날 수밖에 없어요. 인간형 로봇에 일하려면 균형 문제를 해결하지 않고는 할 수 없는 것처럼요. 어쩌면 일부 인간형 로봇은 바퀴베이스를 사용하고 있지만, 그들은 본질적으로 안정적이지 않아요... 중심 질량이 여전히 너무 높아 안전하지 않아요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n벤처 생태계에 충분한 자금이 있을까요? 아마도 있을 것입니다. 소프트뱅크는 가장 큰 자금력을 가지고 있으며 로봇에 많은 투자를 했습니다. 구글도 마찬가지입니다. 그러나 지금까지 그 투자로 얻은 수익은 다소 실망스럽습니다.\n\n그러나 가장 큰 문제는 대부분의 작업에 대한 올바른 해결책이 아니라는 것입니다. 모든 작업에 대한 해결책은 아니지만, 디즈니의 동작하는 인형 배우들이 더욱 세련되고 인상적일 것이라고 생각합니다. 도쿄에는 동작하는 공룡이 체크인을 도와주는 호텔이 있습니다. 동작하는 인형 인간은 공룡보단 조금 더 친근할지도 모릅니다. 그러나 우리 주변의 실제 작업을 수행할 때, 생물모방은 해답이 아닙니다.\n\n교통을 한 예로 살펴봅시다. 거의 5,000년 동안 말 소매치기는 화물과 사람을 이동시키는 데 최신 기술이었습니다. 로마인들은 공급품 이동을 용이하게 하기 위해 25만 마일 이상의 도로를 건설했습니다. 자동차가 등장하자, 그들은 기존의 인프라와 함께 작동해야 했습니다. 그러나 헨리 포드는 유명한 말을 했습니다. \"만약 사람들이 원했다면 더 빠른 말을 원했을 것이다.\" 더 빠른 기계적 말은 교통을 위한 올바른 해답이 아니었습니다. 바퀴가 올바른 해답이었습니다.\n\n바퀴는 물류, 제조업, 병원, 공항, 경기장, 인도, 오피스 건물 및 거의 모든 상업 환경에서 올바른 답입니다. 또한, 안정성을 갖추어야 합니다. 땅에 적어도 3개의 접촉 점, 가능하면 4개의 접촉 점을 가져야 합니다. 로봇 앞에 동력을 보내거나 오버헤드하는 대신 안정 쏘포무렁에 적재물을 두는 것이 더 나은 방법입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 아마존의 접근 방식이 옳았다고 믿습니다... 오늘날 로봇이 무엇을 할 수 있는지 이해하고 현재 최첨단 기술을 대규모로 신속하게 배치한 후 기계 학습과 AI의 발전을 활용하여 운영을 더 개선하고 효율화하는 것이죠. 이것이 로봇을 세상으로 가져오는 실용적인 방법입니다. 75만 대 이상의 로봇이 배치된 아마존은 규모에 있어서 세계에서 가장 성공적인 기업입니다. 그 팀이 하는 일에 대해 자랑스럽고 영감을 얻고, 그 가속화를 이끈 나의 시간을 기억하며 뒤돌아보고 있습니다.\n\nCobot에서는 아마존 충족 센터나 분류 센터 또는 항공 허브 네 벽 바깥으로 이득을 가져다주는 협력 로봇을 개발하고 있습니다. 열린 문제를 해결하려는 것이 아니라 바퀴 사용, 지면의 네 점을 접촉점으로 사용, 웨이브로드를 안정 위치에 가져다 놓기 등 실용적인 일을 하고 있는데, 이것을 신뢰할 수 있고 협력적이며 인간이 설계한 공간에서 작동할 수 있는 방식으로 하고 있습니다. 1월에 첫 현장 배치에 나갈 예정이며 실제로는 인간 크기의 적재물, 즉 상자, 토트 및 카트의 이동 문제를 해결하게 될 것입니다.\n\n그러니까, 우리는 인간형 로봇을 제작하는 것이 아닙니다. 미래에 세상을 개선하기 위해 로봇에 대한 가능성이 발전하고 있다고 내심 낙관적이지만, 인간형은 주요 형태 요소가 될 것이라고 믿지 않습니다. 우리의 형태 요소는 준비가 되지 않았지만, 그 날을 기대하고 있습니다.\n\n브래드 포터는 캘리포니아 주 산타클라라에 본사를 둔 로보틱스 기업 Sequoia, Khosla 및 Mayo Clinic 후원 Collab...","ogImage":{"url":"/assets/img/2024-06-19-TheProblemsWithHumanoidRobots_0.png"},"coverImage":"/assets/img/2024-06-19-TheProblemsWithHumanoidRobots_0.png","tag":["Tech"],"readingTime":5},{"title":"사이보그의 각성","description":"","date":"2024-06-19 06:35","slug":"2024-06-19-TheCyborgsAwakening","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-TheCyborgsAwakening_0.png\" /\u003e\n\n3567년, 대전투장이 '붕괴'라는 재앙에 의해 파괴된 400년 후, 과학자 Marcus Callahan 박사는 넥서스 시티의 거대한 창고를 뒤지면서 새로운 기계 몸체에 연결된 두퇴한 여성을 발견했습니다. Marcus는 그녀의 두뇌에 연결한 뒤 그녀를 자신의 늦은 딸을 기리는 이름인 \"Luna\"라고 명명했습니다. Luna는 자신의 과거에 대한 기억이 없이 깨어나며 곧 Kai라는 부유한 도시 '엘리시움'으로 이사 가는 것을 꿈꾸는 젊은 남성과 친구가 됩니다. 또한 Marcus의 이혼한 전 부인인 Dr. Serena와도 친구가 됩니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-TheCyborgsAwakening_1.png\" /\u003e\n\n그 후 Kai는 Luna를 로봇 글라디에이터가 하는 축구와 유사한 Hyperball이라는 게임에 소개합니다. Kai는 시민들의 주체인 Hyperball 토너먼트의 소유자 인 Victor를 위해 로봇 부품을 훔치며 성당의 '사실상' 지도자인 시타델의 결사 단체에 참여합니다. Marcus를 따라 가면 로봇 시리얼 킬러들의 단체인 Malakai가 주도하는 갱단에 습격당합니다. Marcus는 다쳐 Luna는 본능적으로 \"판저-쿤스트\"(a lost martial art)를 사용하여 싸웁니다. 그녀는 두 대의 안드로이드를 죽이고 Malakai를 상처입힙니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![The Cyborgs Awakening 2](/assets/img/2024-06-19-TheCyborgsAwakening_2.png)\n\n마커스는 자신이 요새의 수호자이자 시타델 내에서 일하는 현상금 사냥꾼임을 밝힌다. 말라카이, 빅터를 위해 일하는 세레나 박사. 루나는 전투를 통해 과거를 재발견할 것이라 믿지만, 마커스는 그를 실망시키며 수호자의 역할을 떠맡게 된다. 루나는 도시 외부의 난파선에서 고급 로봇을 발견한다. 그 몸이 베르서커에 속한 라이벌 소행력인 토르의 무적의 적대적 힘이었음을 깨닫고, 루나가 그곳에 남겨둘 것을 거부하는 마커스로 인해 실망한 루나는 자신을 감시자로 등록한다.\n\n![The Cyborgs Awakening 3](/assets/img/2024-06-19-TheCyborgsAwakening_3.png)\n\n넥서스 선술집에서 그와 카이는 말라카이를 물리치기 위해 다른 수호자를 찾지 못한다. 폭도 경비병인 제인은 루나가 싸욯 때 그를 때려서 화를 내고, 그로 인해 마커스가 개입할 때까지 싸움이 벌어진다. 갑자기 개숙한 말라카이가 나타나 루나를 도전하며, 루나가 엘리시움의 기술 지도자 오라이언에 의해 파괴하기 위해 보냈다는 사실을 밝힌다. 루나의 전투 능력에도 불구하고, 말라카이에 의해 그녀의 몸이 파괴되었지만, 마커스, 카이, 그리고 헌터-키퍼 마스터 맥켄지가 도착해 말라다이를 밀어냈다. 마커스는 루나를 베르서커로 만들고 사과한다. 루나와의 관계로 카이는 범죄 경력을 떠나기로 결정한다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![The Cyborg's Awakening](/assets/img/2024-06-19-TheCyborgsAwakening_4.png)\n\n파트너 타로와 마주치지만, 제인이 나타나 타로를 죽이고 카이를 다른 로봇을 죽인 것으로 비난합니다. 카이는 가까스로 도망쳐 루나에게 도움을 요청합니다. 제인처럼 게임을 멈추고 결심한 그는 카이를 위해 목숨을 건졌습니다. 드. 세레나는 빅터를 위해 일하게 바뀌었으나 카이의 머리를 루나의 생명 지원 시스템에 연결하여 카이를 구할 것을 제안합니다. 이 시도를 알아차린 제인이 루나를 멈추려 할 때 다마스쿠스가 바가지를 잡아 제인을 멍하게 만듭니다. 카이가 엘리시움에 도달하기 위해 빅터의 도움을 받는 것은 거짓말이었습니다; 추방된 엘리시움 시민으로, 마커스는 허브 시티 시민들이 슈퍼 볼 챔피언 없이 엘리시움에 들어갈 수 없다고 믿었습니다.\n\n![The Cyborg's Awakening](/assets/img/2024-06-19-TheCyborgsAwakening_5.png)\n\n루나는 성을 습격하여 세레나의 시체를 소장한 빅터와 마주칩니다. 빅터는 말라카이를 소환하나 루나의 새로운 나노 기술 몸이 그를 쉽게 물리칠 수 있게 합니다. 그는 오라이언에게 빅터를 통해 소통하도록 강제합니다. 오라이언이 친구를 해치겠다고 협박하자, 루나가 빅터를 찔러 죽입니다. 루나가 그를 붙잡고 다시 돌아오라고 말하지만, 마침내 동의하지만, 오라이언이 빠진 찔린 보호링이 그의 몸을 뚫고 그를 관통하여 관통 시키고 그를 튜브 밖으로 밀어내버립니다. 루나가 그를 잡으려 하지만 끌어낼 수 없습니다. 죽기 전에, 카이가 자신을 구해준 루나에게 감사합니다. 대중들의 환호 속에서, 그는 복수를 맹세하고 위에서 오라이언을 웃음 지으며 엘리시움을 향해 플라즈마로 가득찬 검을 향합니다.","ogImage":{"url":"/assets/img/2024-06-19-TheCyborgsAwakening_0.png"},"coverImage":"/assets/img/2024-06-19-TheCyborgsAwakening_0.png","tag":["Tech"],"readingTime":3},{"title":"강화 학습 소개","description":"","date":"2024-06-19 06:29","slug":"2024-06-19-AnIntroductiontoReinforcementLearning","content":"\n\n## 강화 학습의 기초에 대한 심층 탐구, 모델 기반 및 모델 없는 방법 포함\n\n![Image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png)\n\n## 강화 학습이란?\n\n공학 지능의 한 경로는 생물학적 생물체를 모방하는 것에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n생물학적 생명체들은 환경으로부터 정보를 전도하고, 이를 처리(인지과학이 연구하는 바)하며, 생존에 유리한 행동을 출력합니다. 이러한 행동들은 가장 기본적인 수준에서 음식 수확, 번식, 위험 회피와 관련됩니다. 또한, 이는 놀이, 창의성, 문제 해결, 설계 및 공학, 사교, 로맨스, 지성 생활과 같은 다양한 인간 활동도 포함합니다.\n\n그렇다면, 위의 모든 것을 수행할 수 있는 시스템을 어떻게 설계할까요?\n\n만약 우리가 간단한 생물체를 어떤 환경의 함수로 모델링한다면, 우리는 에이전트, 환경의 모델, 그 에이전트를 현재 상태에서 원하는 상태로 이동시키는 함수가 필요할 것입니다.\n\n심리학에서, 두 가지 주요 학파인 행동주의와 인지과학은 인간 행동을 설명하기 위해 양립하고 있습니다. 행동주의자는 학습 메커니즘의 함수로써 행동을 이해하며, 학습은 행동적 출력에 귀속될 수 있다고 합니다. 반면에, 인지과학은 환경과의 상호작용을 정보 처리 접근법을 통해 모델링합니다. 이 접근법에서, 에이전트는 외부 자극을 처음에는 감각을 통해 내부 표현으로 변환하고, 그 후 사고와 추론 능력에 이르기까지 변형 및 통합 과정을 거쳐 행동적 출력을 반환합니다. 전자 접근법에서는, 학습은 주로 환경적 조건부로서 이해됩니다. 반면에 후자에서는, 정신적 표현이 행동을 예측하는 데 필수적이라고 여겨집니다. 강화 학습은 주로 행동주의 접근법에서 영향을 받아, 환경적 보상이 에이전트의 탐색 공간 내에서 진화를 결정한다고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n작업자 조건화, 1950년대-60년대에 유행했던 행동주의 심리학 학파로서, 학습을 보상과 처벌이라는 환경 메커니즘의 결과물로 정의했습니다. 조작적 조건화의 전제 조건으로는 에드워드 손다이크가 제안한 효과의 법칙이 포함되어 있습니다. 이 법칙은 만족스러운 효과를 일으키는 행동은 재발을 더 많이 유발하며, 불만족스러운 효과를 일으키는 행동은 재발을 덜 유발한다는 것을 제안합니다. B.F. 스키너는 효과를 강화와 처벌의 용어로 운용했습니다. 강화는 행동의 재발 발생 가능성을 증가시키며, 이것은 접근 또는 억제 요인의 제거를 말할 수 있습니다. 접근은 긍정적 강화, 회피의 역전인 부정적 강화로 표현됩니다. 긍정적 강화의 예로는 스포츠에서 뛰어나고 자주 이기는 것이 포함됩니다. 부정적 강화의 예는 억제적 자극을 제거하는 것인데, 이를 예로 들 수 있는 것은 경기 도중 당신을 조롱하는 학교 폭력가입니다. 작업자 조건화는 가장 큰 보상을 받는 행동을 반복할 가능성이 높다고 예측합니다. 반면 처벌은 행동 효과를 제어하기 위해 부정적 결과를 추가하거나 행동과 관련된 보상을 제거함으로써 구성됩니다. 파울링으로 게임에서 퇴장당했을 때의 경우는 긍정적 처벌을 보여줍니다. 성적이 좋지 않고 게임에서 패배한 경우는 부정적 처벌을 나타내며, 이는 미래에 더 이상 게임을 하지 않을 수 있습니다.\n\n인간 사회의 삶의 게임은 행동을 구성하는 보조적 강화나 사회적으로 구성된 보상과 처벌로 가득 차 있습니다. 이는 돈, 학점, 대학 입학 기준, 게임에서 이기고 지는 규칙과 같이 사회적 보상과 처벌을 포함합니다. 이러한 것들은 음식, 번식, 사회적 승인과 같은 생물학적 요구에 더 가까운 자연적 강화요인을 보완합니다.\n\n기억은 이전 경험을 유지할 수 있도록 하는 학습에서 중요한 역할을 합니다. 증거에 따르면 기억은 경험의 콘텐츠보다는 보상과 처벌을 부호화합니다. 실험 대상은 보상을 받는 경험을 기억할 가능성이 더 높아지며, 따라서 이를 반복하는 경향이 있습니다. 부정적인 경험은 불리하게 기억될 가능성이 더 높아지며, 이를 피하려고 합니다. 기억 메커니즘은 복잡하고 다양하며, 실험 대상들이 기억을 회상함으로써 기억을 다시 구성함에 있어서 적극적인 역할을 하는 것으로 나타납니다. 이 사실은 행동주의에 대한 예측을 어렵게 만들며, 단독으로 조건화 원리에 근거한 예측을 하기 어렵게 만든다. 게다가 보상과 처벌은 긍정적과 부정적 영향의 풍경을 단순화하며, 이것은 복합한 골짜기와 웅덩이들, 중첩된 의존성으로 이루어진 복잡한 지형이며, 이는 이진 공간보다는 연속적 스펙트럼으로 더 잘 모델링됩니다.\n\n불구하고, 강화 학습은 인공지능을 모델링하기 위해 에이전트, 환경 및 보상의 행동 온톨로지를 적응하는 다양한 수학적 기법으로 이루어져 있습니다. 아래에서 보게 되겠지만, 강화 학습의 측면 중 일부는 통제 이론에서 비롯되어 물리학과 공학으로 확장되는 전제 조건으로부터 나오며, 다른 측면은 심리학과 생물학으로부터 직접적으로 나오는 것입니다. 통제 이론의 대상과 생명체는 열역학적 균형으로부터 멀리 떨어진 최적 범위 내에 남아야 하는 동력학 시스템으로 구성되기 때문에, 기본 원리는 강화 학습과 인공지능의 목표에 부합됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다이내믹 프로그래밍은 주로 제어 이론에서 시작되어, 더 큰 문제를 하위 문제로 재귀적으로 분해하여 해결하는 수학적 최적화 방법으로 발전했습니다. 보통 재귀는 함수가 직접 또는 간접적으로 자기 자신을 파라미터로 전달하는 것을 말합니다.\n\n본 글에서는 주로 동적 프로그래밍의 요소에 초점을 맞추며, 이를 이산적이고 유한한 게임에 초점을 맞출 것입니다. 그러나 동적 프로그래밍은 모델 없이 강화학습 접근 방식과 결합하여 해결되는 한계를 가지고 있으며, 이를 보완하기 위해 동적 프로그래밍과 인공 신경망을 결합한 방법이 있습니다. 이는 한 때 신경동적 프로그래밍이라 불렸습니다. 보다 넓게는 강화학습과 인공 신경망의 결합을 딥 강화학습이라고 합니다. 이러한 모델은 강화학습 기법 내에서 딥 러닝의 강점을 통합하고 있습니다. 이러한 알고리즘 가운데 가장 인기 있는 것은 2013년 DeepMind에 의해 소개된 딥 Q-네트워크(DQN)입니다. 이 알고리즘 계열은 Q 함수를 근사화하기 위해 딥 러닝을 활용합니다. Q 함수의 근사화가 강화학습의 한 약점 중 하나이므로, 이러한 알고리즘들은 강화학습 패러다임의 주요 개선점을 제공합니다.\n\nDQN이 해결한 다른 약점에는 비선형 동역학을 캡처하는 유연성 부여, 차원의 저주로 인해 계산적으로 처리하기 어려워지는 일반적 범위의 차원을 수용하는 능력, 그리고 환경에 대한 보다 큰 일반화 능력이 포함됩니다.\n\n신경동적 프로그래밍은 순수히 행동주의 접근 방식의 약점을 해결하기 위해 심리학의 인지 패러다임을 활용하는 방향으로 발전하고 있습니다. 그러나 하위 수준 지각 정보의 계층적 구조와 처리에 대한 과학적 진전이 이루어지는 반면, 그 정보를 생각과 의식에 연결시키는 데는 더 많은 노력이 필요하며, 이는 과학적으로 약간 불가능한 것으로 남아 있습니다. 이러한 이유로 인공 신경망(ANNs)은 아직까지 사람의 지능의 복잡한 일반화 능력을 갖추고 있지 않습니다. 이는 ANNs보다 지수적으로 적은 샘플로 학습하는 인간의 지능과 대조적입니다. 본 글의 마지막 섹션에서 강화학습의 원칙을 인공 일반 지능(AGI) 쪽으로 채택하는 시사점을 논의하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 의사 결정 이론 및 제어 이론\n\n동적 프로그래밍과 강화 학습의 수학적 요소에 깊이 파고들기 전에, 철학적이고 수학적인 의사 결정 이론과 강화 학습 간의 관계를 명확히해야 합니다. 의사 결정 이론은 주로 합리적 선택 이론의 수학적 형식화로 구성되어 있지만, 강화 학습의 목표와 겹치는 부분이 있습니다. 강화 학습은 복잡한 환경과 정보 환경과 상호작용할 수 있는 성공적인 인공 에이전트로의 모델을 구축하려고 합니다.\n\n의사 결정 이론, 또는 선택 이론으로도 알려진 이론은 20세기에 이상적인 이유의 형식화가 짙어진 가운데 발전하였습니다. 구체적으로, 에이전트의 행위 확률을 그들의 선호도를 고려하여 양적화하기 위해 확률 이론을 사용합니다. 이 형식화 노력의 꼭대기는 폰 노이만-모건슈턴 유틸리티 절차였습니다. 요약하자면, 이 절차는 에이전트가 유틸리티 기대치에 따라 최대 이익을 가져다주는 행동을 선택하는 경향이 있음을 설명합니다.\n\n제어 이론은 기계 및 전기 공학 분야에서 나타나며, 동적 시스템의 상태 및 성능을 원하는 매개변수에 대해 최적화하는 데 관심이 있습니다. 중요한 메커니즘은 희망 변수를 측정하고 설정점과 비교한 후 그 차이를 수정을 위한 피드백으로 전달하는 컨트롤러로 이루어져 있습니다. 제어 이론의 큰 그림은 생명체의 대사 과정과 유사하며, 외부 변수 조건에 대비해 내부 온도의 설정 점을 유지하는 생물들의 과정을 반영합니다. 제어 이론과 의사 결정 이론의 연결은 명백합니다: 둘 다 시스템의 상태를 최적화하거나 발전시키기 위해 환경으로부터의 피드백에 의존합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수학적으로, 제어 및 의사 결정 문제의 부분 집합은 모두 동적 프로그래밍을 통해 해결할 수 있는 최적화 문제로 축소될 수 있습니다. 동적 프로그래밍은 상태 변수의 수가 지수적으로 증가함에 따라 계산 요구 사항이 지수적으로 증가하는 차원의 저주에 시달린 일반적인 확률적 최적 제어 문제를 해결하기 위해 그것을 더 작은 하위 문제로 분해하고 가치 함수를 계산함으로써 해결합니다. 저희는 강화 학습의 기본 원칙을 시연하면서, 동적 프로그래밍의 핵심인 에이전트의 상태 및 가치 함수 사이의 재귀적 관계에 대해 깊이 파헤쳐볼 것입니다.\n\n강화 학습과 의사 결정 이론은 보상 또는 유틸리티를 극대화하기 위한 절차를 정의하는 부분에서 겹칩니다. 그러나 의사 결정 이론에서는 유틸리티가 명시적으로 정의되지만, 경제 행동을 모델링하려는 것인 반면, 강화 학습에서는 유틸리티가 누적 보상으로 대체됩니다. 서로 다른 작업 목표에 대한 다른 정책을 적용하여 누적 보상을 극대화할 수 있으며, 탐구와 개발의 극성 방향 간의 상호 관계에 따라 달라집니다. 우리가 볼 것처럼, 탐색과 개발의 상호 관계를 탐색하는 것으로 표현되는 탐사-개발 딜레마에 따라 누적 보상을 극대화하는 것이 달라집니다.\n\n강화 모델의 기반이 되는 온톨로지를 개요화하는 것으로 시작해 봅시다.\n\n## 상태, 동작 및 보상\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n강화 학습은 의사 결정 이론의 이론적 장치를 활용하여 에이전트, 환경 및 동적 진화 규칙을 포함하는 모델을 구성합니다. 진화 규칙은 에이전트가 환경 내에서 보상을 추구할 수 있게 허용하며, 이를 관찰이라고도 합니다.\n\n에이전트는 환경으로부터 결정까지의 출력으로 정의됩니다. 특정 결정을 행동이라고 합니다. 네트워크의 현재 상태에서 행동으로의 매핑을 정책이라고 합니다. 정책은 상태에서 결과로의 매핑으로서 행동을 안내합니다.\n\n따라서 형식적으로 정책은 상태를 행동으로 매핑하는 함수입니다. 현재 상태가 주어졌을 때 행동의 조건부 확률로 나타낼 수 있으며, 여기서 그리스 문자 𝛑은 정책을 나타냅니다:\n\n![정책](https://yourwebsite.com/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n전이 역학은 모든 가능한 상태 및 보상 값에 대한 확률 분포로 주어진 입력 보상에 따라 다음 상태를 정의합니다:\n\n![이미지](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_2.png)\n\n위의 공식은 다음 상태와 보상 쌍의 확률을 현재 상태 s와 행동 a가 주어졌을 때 다음 상태 s'와 보상 r의 조건부 확률과 같다고 정의합니다.\n\n행동은 보상을 누적하여 환경을 변경합니다. 그 결과로 보상은 에이전트 상태나 관측을 변경합니다. 보상 입력은 정책에 기반하여 미래의 행동 출력을 결정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적으로, 정책에는 두 가지 유형이 있습니다:\n\n보상은 일반적으로 스칼라 값 x로 형식화됩니다.\n\n특정 보상이 주어지면, 에이전트는 최적화 딜레마에 직면합니다: 에이전트는 단기 보상을 극대화해야 하는지, 아니면 완전한 생생력 기록을 통해 누적 보상을 극대화해야 하는지를 결정해야 합니다.\n\n이것은 탐색-이용 딜레마로 알려져 있습니다. 다시 말해, 전이 함수는 환경을 탐색하고 누적한 지식을 활용하여 최대 보상을 얻으며, 그 둘 사이의 균형을 최적화해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n탐색-활용 딜레마에 대한 최적의 해결책은 모델이 학습해야 하는 작업의 유형에 따라 달라집니다. 이 작업은 유한에서 무한(연속적 또는 이산적으로)로 범위가 있을 수 있습니다. 예를 들어 체스 게임은 에피소드 작업으로 형식화될 수 있습니다. 왜냐하면 유한한 구성 공간과 승, 패, 무승부 세 가지 가능한 결과를 가진 미리 정의된 종료 상태가 있기 때문입니다. 이는 현재 상태를 기준으로 최적의 후속 상태를 결정할 수 있는 것을 의미하며, 결정론적 전이 동역학을 통해 계산됩니다. 따라서 각 상태에 대해 단일 최적의 행동이 존재합니다.\n\n그러나 대부분의 작업은 유한한 구성 공간이나 미리 정의된 종료 상태를 가지고 있지 않습니다. 우리는 이러한 것들을 연속적인 작업으로 분류하고, 모델이 없는 방법을 통해 최적화합니다. 모델 없는 방법론에서는 전환 동역학을 계산하는 대신 모델이 환경에서 샘플링하여 최적의 후속 상태를 계산합니다. 다르게 말하면, 선견지명을 통해 행동을 계획하는 대신 환경에 대해 배우기 위해 시행착오를 사용합니다.\n\n모델 없이 강화 학습하는 두 가지 접근법이 일반적으로 있습니다: 몬테 카를로 접근법과 시간차 학습. 충분한 샘플의 평균이 기대값으로 수렴하기 때문에, 모델 없는 방법은 샘플 평균을 통해 예상값을 추정합니다. 몬테 카를로 방법은 충분히 큰 상태-행동 쌍의 샘플로 예상 누적 반환을 추정하여 가치 함수를 계산합니다. 일부 몬테 카를로 방법은 에피소드 작업의 끝에서만 가치 함수를 평가합니다. 연속적인 작업에서는 에피소드의 정의가 다양하게 변할 수 있고, 디자이너에 따라 시간 간격에 따라 설정할 수 있습니다.\n\n몬테 카를로 탐색과 반대로 시간차 학습은 시간 단계 간의 차이를 활용하여 가치 함수를 증분적으로 추정합니다. 시간차 방법의 접근 방식을 두면 몬테 카를로 방법에 비해 실제 예상 값과의 분산을 낮추는 특성이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요약하자면: 에이전트는 현재 상태와 액션 공간 쌍에서 상태 공간으로의 매핑을 통해 환경을 탐색합니다. 전이 동적은 미리 정의된 종단 상태를 가진 유한한 구성 공간에 대한 모든 가능한 매핑을 계산합니다. 미리 정의된 종단 상태와 유한한 상태 공간 대신에, 모델 무작위 접근법은 최상의 정책을 찾기 위해 환경에서 계속 샘플링합니다.\n\n동적 프로그래밍은 모든 상태-액션 쌍에서 상태 전이 확률과 예상 보상을 계산합니다. 이 프로세스가 어떻게 작동하는지 이해하기 위해서는, 마르코프 프로세스를 이해해야 합니다.\n\n다음에는 에이전트가 최적 후속 상태를 계산할 수 있도록 하는 수학적 모델을 배우게 됩니다. 앞서 논의한 대로, 최적성은 탐사-이용 딜레마로 이어지며, 이는 모델링하려는 작업 유형에 따라 다릅니다. 보상 구조를 자세히 살펴봄으로써 이를 더 잘 이해할 수 있을 겁니다.\n\n## 보상 평가\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n강화 학습에서 보상을 측정하는 방법은 에이전트가 행동을 취함으로써 환경으로부터 얻는 스칼라 값으로 계량화됩니다. 이 보상의 가치는 행동의 즉각적인 선호도를 나타냅니다.\n\n반면에 누적 보상 또는 반환은 해당 시점까지 환경으로부터 누적된 모든 보상의 합을 나타냅니다. 에이전트의 목표는 단순히 즉각적 보상을 최적화하는 것이 아니라 누적 보상을 최적화하는 것입니다. 전자는 근시적 에이전트를 나타내며, 후자는 장기간 수익을 극대화하려는 장기 노력 에이전트를 나타냅니다.\n\n대부분의 경우 에이전트가 가장 높은 보상을 최대한 빨리 극대화하길 원하기 때문에 할인은 현재 최대 보상을 나중에 최대 보상보다 우선시하는 방식으로 도입됩니다.\n\n할인을 적용한 누적 보상 G는 아래 식으로 표현됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Reinforcement Learning Introduction](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_3.png)\n\n여기서 누적 보상 G는 보상과 해당 할인 계수 감마 𝜸의 곱의 합과 같습니다. 감마는 항상 0과 1 사이의 값인 '0,1'입니다. 감마는 각 시간 단계마다 지수적으로 증가되므로 무한한 시간 단계를 통해 감마가 0에 접근합니다.\n\n감마가 0에 접근할수록 단기 이익을 장려하고, 감마가 1에 가까워지면 무한한 반복을 통해 보상 합이 자체적으로 무한에 접근하므로 장기 이익을 장려합니다.\n\n대부분의 작업은 시간에 제한이 있기 때문에, 감마 할인은 값이 1 미만일 때 보상에 상한선을 부과합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n할인을 고려한 누적 보상의 압축된 방정식은 아래와 같습니다. 여기서 G는 보상 R의 예상 합을 나타내며, 이는 할인 요소 감마로 곱해집니다. 따라서 누적 보상은 보상과 할인 요소의 합으로 계산됩니다:\n\n\n![cumulative reward equation](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_4.png)\n\n\n## 마르코프 의사결정 과정 (MDP)\n\n지금까지 정책을 상태에서 행동으로 매핑하는 확률적 정의, 보상이 주어졌을 때 한 상태에서 다른 상태로 움직일 확률인 전이 역학, 그리고 보상이 어떻게 계산되는지에 대한 공식에 대해 논의해 왔습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자, 이제 조금 물러나서 확률적 전이 체인을 정의하는 보충 이론을 제공하겠습니다. 먼저 마르코프 과정이라고 하는 것부터 시작해봅시다. 마르코프 과정은 마르코프 성질을 만족하는 확률 과정입니다. 확률 과정은 무작위로 변하는 과정입니다. 마르코프 성질은 모든 상태에 대해 후속 상태가 현재 상태에만 의존된다는 것을 말합니다.\n\n과거 상태가 미래 상태에 영향을 미치지 않기 때문에 마르코프 성질을 만족하는 과정을 메모리리스라고 합니다. 집을 나가서 일하러 나가 다시 집으로 돌아오는 매일 재발되는 일정된 목적지 집합을 상상해보세요. 즉, 시작과 끝이 있는 순환 과정이 있습니다. 이제 더 나아가서 다음 목적지로 움직일 결정이 현재 목적지에만 의존한다고 상상해보세요. 처음에는 각 연결된 목적지가 동일한 확률 분포를 갖게 될 것입니다. 예를 들어, 집을 나가면 운전하거나 지하철을 탈 수 있는 선택지가 있다면, 두 가능한 미래 상태에 대한 초기 확률을 각각 0.5로 정할 수 있습니다. 모든 가능한 경로의 반복을 통해 이러한 확률은 어떤 경로가 다른 경로보다 선호되는 빈도 분포로 안정화될 수 있습니다. (이 유형의 확률을 경험적 확률이라고 부르며, 가능한 사건에 대한 결과를 한정된 테스트 수에 대해 평균화합니다) 그 분포 평형은 마르코프 체인 또는 과정이 될 것입니다.\n\n이제 아마도 생각 중일 것입니다: 어떻게 사건과 상태를 정의하나요? 고정된 가능한 상태와 안정한 확률 분포에 대해 얘기하려면 세상이 너무 복잡하지 않나요?\n\n매우 그렇습니다. 그러나 우리는 환경 속 요소들의 수학적 형식론을 원하기 때문에 모델링하려는 작업 또는 환경 유형을 구별해야 합니다. 이를 위해 시간 단계와 상태 공간의 표현, 즉 모든 가능한 상태의 분포를 명시해야 합니다. 아래의 정사각 행렬은 상태 공간과 시간의 축을 기준으로 마르코프 체인의 정의를 제공합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_5.png)\n\n상태 공간은 셀 수 있는/유한한 상태거나 연속적일 수 있습니다. 유한 상태 공간은 시스템의 모든 가능한 구성을 조합 이론을 통해 설명하고, 연속 상태 공간은 연속 함수를 통해 모든 가능한 구성을 설명합니다.\n\n유한 및 가산 무한 공간은 측정 가능한 공간으로 정수 또는 유리수를 취하며, 연속 공간은 실수를 취합니다.\n\n마찬가지로 시간 축은 이산 또는 연속적으로 정의될 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이산시간 프로세스는 이산적으로 상태 전이를 계산하지만, 가산 또는 비가산 상태 공간에서 모델링할 수 있습니다. 여기서 비가산이라 함은 실수의 무한한 10진 확장을 의미합니다. 실제로 컴퓨터가 시간을 세는 방식도 이와 같습니다. 이를 이산 단계로 처리합니다. 단계 사이의 간격은 아키텍처에 따라 다르지만, 주기는 보통 레지스터 상태를 변경하는 데 필요한 시간 단계의 길이로 측정됩니다.\n\n연속시간 체인은 연속으로 상태 전이를 계산하며, 가산 또는 비가산 상태 공간에 모델링될 수 있습니다.\n\n마르코프 프로세스라는 용어는 일반적으로 연속시간 프로세스에 사용되며, 마르코프 체인이라는 용어는 이 중 일부인 것을 나타냅니다: 이산시간, 확률적 제어 프로세스입니다. 이 기사에서는 이산시간, 유한 상태 공간에 초점을 맞출 것입니다.\n\n지금까지 우리의 마르코프 체인은 상태간 전이를 고정된 확률로 설명하는 매우 단순한 모델입니다. 행동과 보상이라는 모델링에 중요한 두 가지 요소가 빠져 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n보상을 전이 확률로 할당하는 것이 마르코프 보상 과정입니다. 마르코프 보상 과정은 각 전이 상태에 보상을 할당합니다(양수 또는 음수 정수로 정의됨)으로써 시스템을 원하는 상태로 이끕니다. 누적 보상 공식을 상기해 보겠습니다. 기대 보상의 합에 일정한 할인 계수가 곱해진 값입니다. 마르코프 보상 과정을 사용하면 초기 상태 S가 주어졌을 때 상태 v(s)의 값과 누적 보상 G의 확률을 계산할 수 있습니다(여기서 G는 많은 반복 시행에서 평균화된 값입니다):\n\n![이미지](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_6.png)\n\n마르코프 결정 과정으로 나아가기 위해 필요한 마지막 변수는 행동입니다. 에이전트는 가능한 행동 집합에 대해 동등하게 분포된 확률로 시작하고 이후에 전이 함수를 업데이트하여 현재 상태와 행동을 다음 상태와 보상으로 매핑합니다. 이렇게 하면 앞서 설명한 전이 동학에 다시 도달하게 됩니다:\n\n![이미지](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 동적 계획법 및 벨만 최적성\n\n이것은 벨만(1957)에 의해 개발된 동적 프로그래밍의 개념으로 이어집니다.\n\n동적 프로그래밍을 이해하면, 동적 프로그래밍과 같은 완벽한 환경 지식이 필요하지 않는 근사 방법인 몬테카를로 탐색 및 시간차이 메소드도 이해할 수 있습니다. 이러한 모델-프리 방법은 완벽한 정보 대신 동적 프로그래밍의 결정적 정책을 근사화합니다. 따라서, 실제 세계 학습을 근사화하는 강력한 메커니즘이 제공됩니다.\n\n동적 프로그래밍이 최적의 에이전트 상태를 검색하고 찾는 핵심 아이디어는 상태 가치 함수와 행동 가치 함수 사이의 관계에 있습니다. 이들은 재귀적으로 관련되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 아이디어를 관련성 있는 예시로 확장해 봅시다. 예를 들어, 당신이 삶에서 최적 상태가 아니고 이를 바꾸고 싶다고 가정해 봅시다. 미래에 이루고 싶은 구체적인 목표나 위치가 있다고 해 봅시다. 이 큰 목표에 도달하기 위해 (더 좋은 직장을 얻는다던가, 가족을 꾸린다던가 등을 대체할 수 있습니다), 당신은 원하는 결과에 도움이 되는 일련의 작은 단계나 행동을 취해야 할 것입니다. 강화 학습의 언어로 번역하면, 현재 상태에는 특정 가치가 할당될 것입니다. 현재 상태와 가치를 고려하여 당신은 행동을 취할 것입니다. 이러한 행동은 전체 목표와 현재 상태에 따라 평가될 것입니다. 좋은 행동은 나쁜 행동보다 높은 가치를 받을 것입니다. 환경으로부터의 피드백은 행동의 가치를 결정할 것입니다 (이 값들이 어떻게 결정되는지는 작업마다 다릅니다). 상태의 평가는 사용 가능한 행동과 후속 상태의 가치에 영향을 미칠 것입니다. 그리고 행동의 평가는 현재 상태의 가치를 재귀적으로 영향을 줄 것입니다. 다시 말해, 행동과 상태는 재귀적으로 연결되어 있습니다.\n\n이제 현실에서, 당신의 목표와 그 목표에 이르는 행동 단계들은 이산 시간 단계 및 이산 상태 공간을 갖는 결정론적 시스템으로 명시할 수 없습니다 (비록 이 방식으로 근사화할 수도 있습니다). 대신, 동적 프로그래밍은 체스와 같은 게임처럼 정의 가능한 환경을 가정합니다. 여기서 시간 단계와 행동 공간이 이산적이고 유한하게 추상화됩니다. 현실과의 중요한 점은 더 큰 목표가 해당 큰 목표에 유리한 작은 부목표를 최적화함으로써 다가올 것이라는 점입니다.\n\n따라서 동적 프로그래밍은 다음 값들을 가정할 것입니다: (Ω, A, 𝒫), 여기서 Ω는 모든 가능한 상태의 합을 나타냅니다, A는 유한 샘플 공간의 부분집합인 행동 이벤트를 나타내며, P는 일정 정책 함수 𝝅에 의해 각 행동 이벤트에 할당된 확률을 나타냅니다.\n\n이제 우리가 결정론적 전이 역학에 대해 생각해 보면, 상태, 행동 및 보상의 집합이 유한하기 때문에, 특정 상태와 보상 쌍은 일정한 상태 및 행동 쌍이 주어졌을 때 그 값들이 발생할 확률을 갖게 될 것입니다. 이러한 확률은 상태 공간이 이산적이기 때문에 이산 확률 분포로 명시됩니다. 우리는 상태, 행동 및 보상으로 구성된 일련의 순서가 누적 보상을 최대화하려는 마르코프 결정 과정(MDPs)이라고 했습니다. 이때 보상을 스칼라 값으로 표현하며, 시간이 흐름에 따라 예상되는 누적 보상을 최대화하려고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 다루어야 할 질문은 우리가 지정한 가정에 따라 마르코프 의사결정 프로세스가 누적 보상을 최대화하는 방법입니다. 이 답은 벨만 최적 방정식에 의해 제공되며 두 함수인 상태 가치 함수와 행동 가치 함수 사이의 관계를 설명합니다.\n\n## 상태 가치 함수\n\n상태 가치 함수는 에이전트가 정책 𝝅에 따라 취할 수 있는 모든 가능한 조치의 확률의 합으로 정의될 수 있습니다. 각 조치에 대해 가능한 후속 상태의 가중치 값의 합으로 그 가치가 결정됩니다.\n\n보다 간단하게 말하자면, 상태 가치 함수는 특정 상태(s)에서 정책 𝝅을 따라 시작하여 에이전트가 얻을 수 있는 예상 누적 보상을 정의합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![equation_8](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_8.png)\n\n위의 방정식은 두 항으로 구성되어 있습니다: a) 정책 (𝝅)을 따라 상태 (s)에서 에이전트가 취할 수 있는 모든 가능한 조치들의 확률의 합, 그리고 b) 각 가능한 조치마다 가능한 후속 상태의 가중치 값을 계산하는 내부 합계입니다. 대괄호 내의 항은 각 조치의 가능한 상태의 기여도를 즉각적 보상 R(s, a, s’)의 합과 감마 요소 𝛾에 의한 할인된 보상의 합으로 계산합니다.\n\n상태-가치 함수를 표현하는 다른 방법은 다음과 같습니다:\n\n![equation_9](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_9.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 공식은 다음 상태의 가치를 조건부 확률로 계산된 예상 반환 E𝝅로 정의합니다. 시간 t에서 상태 s가 주어졌을 때 시간 t에서 보상 R을 받을 조건부 확률로 계산됩니다. 보상 R은 후속 상태의 예상 반환의 곱의 합과 감마 감쇠를 고려하여 계산됩니다.\n\n더 잘 이해하기 위해 3 x 3 그리드 월드의 에이전트를 상상해보세요. 각 시간 단계마다 상, 하, 오른쪽, 왼쪽 네 가지의 가능한 조치가 사용 가능합니다.\n\n![grid](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_10.png)\n\n우리는 상태 가치를 0으로 초기화하고, 상태 가치 함수에 대한 벨만 방정식을 사용하여 그리드 내의 보상 분포가 주어졌을 때 상태 가치를 최적화합니다. 우리는 (행, 열) 색인을 사용하여 그리드의 각 위치를 식별합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 테이블 태그를 마크다운 형식으로 변경해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Action-Value Function\n\n우리는 행동-가치 함수가 상태-가치 함수의 두 번째 항목으로 내장되어 있다는 것을 보았습니다. 이는 행동-가치 함수가 상태 (s)에서 가능한 모든 행동의 가치를 계산한다는 것을 의미합니다. (s)에서 (s')로의 전이로부터 얻은 즉각적인 보상의 합과 다음 상태 (s')에서의 예상 누적 보상을 고려하여 주어진 작업으로부터 계산됩니다.\n\n다시 말해, 행동 가치 함수는 상태 (s)에서 작업 (a)를 수행하는 것에 대한 누적 보상을 계산합니다. 여기서 기대 수익은 즉각적인 상태 전이 — R(s, a, s')로 표시됨 — 및 다음 상태 s'의 누적 보상의 할인 가치 —𝛾∑𝝅(a'|s')Q(s',a')​​로 표시됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n행동 가치 함수를 정하는 또 다른 방법은 최적 정책 𝝅을 따라 상태와 행동 쌍 (s, a)이 주어졌을 때 기대 반환값 E로 나타내는 것입니다:\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_14.png)\n\n상태 가치 함수와 행동 가치 함수는 상태 가치 함수가 정책과 행동 가치 함수 Q(s, a)로 구할 수 있다는 관점에서 관련이 있습니다.\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_15.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서 행동-가치 함수와 상태-가치 함수는 재귀적으로 관련이 있습니다: 행동-상태 쌍의 가치가 상태의 가치를 결정하며, 상태는 반대로 행동의 가치를 결정합니다.\n\n상태-가치 함수는 상태를 우선으로 하고 기대값 E를 출력합니다. 행동 가치 함수는 상태와 행동 쌍을 우선으로 하여 보상을 계산하고 기대 누적 반환 E를 얻습니다.\n\n따라서 벨만 최적 방정식은 상태-가치와 행동-가치 함수의 재귀적 반복을 나타내며, 최적 값에 수렴할 때까지 반복됩니다. 상태-가치 함수를 위한 벨만 방정식은 아래와 같이 표현됩니다: \n\n![image](https://yourwebsite.com/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_16.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 상태의 값은 가능한 모든 행동의 최대 보상으로 정의되며, 이는 (s) 상태에서 행동 a를 취할 때 얻는 보상과 다음 행동 s'의 값 및 할인 계수 감마의 곱으로 계산됩니다.\n\n벨만 방정식은 현재 상태에서 모든 가능한 행동을 평균화하고 발생 확률에 따라 가중치를 부여합니다.\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_17.png)\n\n## 모델 프리 메소드: 몬테카를로 \u0026 시간차학습\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 예시는 전환 동역학이 알려져 있어 따라서 완벽하게 계산될 수 있는 결정론적 모델을 설명합니다. 이것은 환경에 대한 완전한 지식을 가지고 있기 때문입니다.\n\n그러나 대부분의 작업에서는 환경에 대해 완전한 지식을 갖고 있지 않습니다. 이 정보 대신에 우리는 동적 프로그래밍 방정식을 해결할 수 없기 때문에 정확한 결정론적 전환 동역학으로 진행할 수 없습니다. 이 문제를 극복하기 위해 통계에서 빌려온 기술을 사용하여 환경의 상태를 샘플에서 추론할 수 있습니다.\n\nMonte Carlo 방법론에서는 예상 수익을 샘플 수익의 평균으로 근사화합니다. 샘플이 무한대로 접근함에 따라 평균 수익이 예상 수익의 실제 값으로 수렴합니다. 에이전트가 종료될 때까지 전체 에피소드를 실행한 다음 가치 함수를 계산하는 방식으로 이를 수행합니다. 그런 다음 N개의 에피소드 샘플을 취하여 평균을 사용하여 대상 상태의 예상 가치를 근사화합니다. 지금까지 궁금해 하고 계실 수 있듯이, 에피소드가 어떻게 정의되는지는 작업과 모델의 목적에 따라 달라집니다. 예를 들어, 체스 게임에서는 전체 게임을 실행하거나 임의의 단계 시리즈를 에피소드로 정의할 수 있습니다.\n\nMC 업데이트 규칙을 다음과 같이 작성할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_18.png)\n\nV(s) n+1은 다음 에피소드의 가치를 나타내며, S(s)n은 상태의 누적 가치를 나타내고 G는 보상의 가치를 나타냅니다. 누적 보상 G를 상태 값에 추가하고 에피소드 또는 샘플의 수로 나눕니다.\n\n우리는 MC 업데이트 규칙을 대수적으로 재배치할 수 있습니다:\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_19.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMonte Carlo 방법과는 달리 Temporal Difference (TD)에서는 각 에피소드가 끝난 후가 아니라 각 시간 단계나 증분마다 상태 가치 함수를 평가합니다. 환경에 대한 정보가 없는 초기 상태에서는 상태의 가치 V(s)를 0이나 다른 값으로 초기화해야 하며, 이후 매 시간 단계마다 업데이트됩니다.\n\nTD에서는 상태의 가치를 계산하는 데 두 단계가 필요합니다. 먼저 한 단계의 오차를 계산한 다음 업데이트 규칙을 적용하여 상태의 가치를 변경합니다. 오차는 다음과 같은 차이 공식으로 주어집니다:\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_20.png)\n\n여기서 𝜹t는 오차를 나타내며, R(t+1)은 행동에서 얻는 보상, V(S t+1)은 다음 상태의 추정 가치, V(S)는 현재 상태의 가치를 의미합니다. TD가 다음 상태의 추정 가치를 사용하여 현재 상태를 평가하는 것을 부트스트랩이라고 합니다. 이를 통해 현재 상태의 값에서 행동의 보상과 할인 계수와 다음 상태의 가치의 곱을 더하고 빼는 것으로 상태의 값이 즉시 시간 단계마다 업데이트됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예상 보상과 실제 관측 값 사이의 차이를 𝜹(탐색-백업 간격)을 𝛼(학습률)에 곱해주면 관측과 기대 사이의 차이를 줄일 수 있어요.\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_21.png)\n\n𝛼의 역할은 TD 알고리즘의 학습 정도를 결정하는데, 𝛼는 실수형양수값이에요. 일반적으로 𝛼는 0.1, 0.01, 0.001 같은 값으로 설정돼요. 높은 𝛼 값은 업데이트를 더 적극적으로 진행하도록 해주고, 낮은 값은 보수적인 업데이트를 보장해요. 𝛼의 값은 탐험-활용 균형에 영향을 미치는데, 더 높은 𝛼는 탐험을 선호하고, 낮은 𝛼는 활용을 선호하게 됩니다.\n\nMC와 TD 방법은 환경에 대한 사전 지식 없이 진행되지만, Temporal Difference의 장점은 매 시간 단계에서 온라인 업데이트를 계산한다는 것이고, 몬테카를로 방법의 장점은 값 추정을 위해 샘플링에만 의존하여 편향이 없다는 것이에요. TD 방법의 단점은 높은 편향이고, MC 방법의 단점은 중요한 업데이트를 간과하여 높은 분산을 유발한다는 점이에요. 이것은 두 학습 전략 사이의 최적점이 어딘가에서 존재해야 한다는 것을 시사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTD 방법론은 단계 평가 전략을 n단계로 변경함으로써 최적화할 수 있습니다. 이렇게 함으로써, TD와 MC 사이에서 타협할 수 있는 기회를 얻게 됩니다. 우리가 n단계마다 상태 가치를 평가할 때, 우리는 매 단계 후가 아닌 미래 n단계를 추정함으로써 이를 수행합니다.\n\nn단계 TD에 대한 수정된 접근 방식은 TD(𝝀)입니다. TD(𝝀) 방법은 지나간 상태-액션 쌍에 대한 신용을 할당하기 위해 '적격성 흔적(eligibility traces)'이라 불리는 매개변수를 사용합니다. 미래 n단계를 추정하는 대신, 적격성 흔적은 여러 TD 단계에 걸쳐 상태-액션 쌍에 대한 신용을 할당합니다. 적격성 흔적은 지난 상태-액션 쌍이 관찰된 보상 전환에 기여한 정도에 대해 신용을 부여합니다. 적격성 흔적은 각 상태-액션 쌍에 연관된 벡터나 행렬로 표현됩니다. 시간 단계에 대한 적격성 흔적은 다음과 같이 재귀적으로 계산됩니다:\n\n![Eligibility Trace Formula](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_22.png)\n\n여기서 람다(𝝀) 매개변수는 부트스트래핑의 정도를 제어합니다. 𝝀 = 1일 때, 부트스트래핑이 없어지고 업데이트 규칙은 Monte Carlo로 축소됩니다. 𝝀 = 0일 때는 부트스트래핑이 있는 TD(0)로 축소됩니다. TD(𝝀)는 TD와 MC를 연속체로 일반화한 것으로, 여기서 TD(0)는 단일 단계 TD를 의미하며, TD(1)은 TD를 ∞ 단계까지 확장한 극한값인 MC를 의미합니다. 수식에서 보듯이, 적격성 흔적 매개변수는 재귀적으로 계산됩니다. 다음 시간 단계의 적격성 흔적값은 이전 단계의 적격성 흔적 값을 입력으로 취합니다. E(s) = 0일 때, 부트스트랩이 없어집니다. TD(𝝀) 업데이트 규칙은 아래와 같이 TD 및 MC 업데이트 규칙과 동일하게 계산되지만, 애러에 적격성 흔적을 곱하는 것이 차이입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_23.png)\n\n## ANNs를 활용한 강화 학습 확장\n\n모델 기반 또는 모델 무관 RL 알고리즘은 차원의 저주로 인한 스케일링 문제에 직면하며, 다양한 유형의 환경 간 일반화에 어려움을 겪으며, 샘플 효율성에 대한 어려움이 있습니다.\n\n인공 신경망(ANN)은 RL 아키텍처 내재한 일부 한계를 교정하는 강력한 방법을 제공합니다. 특히, ANNs는 샘플링 효율성, 환경 일반화, 차원의 저주로 인한 스케일링 문제를 개선합니다. 데이터로부터 일반 함수를 학습하기 때문에 우수한 일반화 능력을 통해 샘플 효율성을 줄이고 환경 일반화를 향상시킵니다. 이는 숨겨진 계층의 수와 각 숨겨진 계층 당 뉴런 수를 늘릴 수 있어 더 잘 스케일링할 수 있도록 합니다. 그러나 너무 많은 숨겨진 계층과 뉴런은 계산 스케일링 문제를 야기할 수도 있습니다(특정 범위를 벗어날 경우 차원의 저주를 피할 수 없습니다). 또한 전통적으로 ANNs가 사전에 목표 상태의 비정상성 문제에 시달리며, RL 알고리즘은 정책 상이든 오프-정책 상에 상관없이 업데이트 함수를 통해 최적 상태를 찾습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n전통적인 강화 학습 알고리즘이 확률적 전이 규칙에 의존하는 데 반해, 강화 학습에 인공 신경망(ANNs)을 적용하면 함수 근사를 사용하여 상태 및 상태-행동 값들을 계산하게 됩니다. 선형 근사 및 타일 코딩과 같은 여러 함수 근사 방법을 적용할 수 있지만, 인공 신경망은 비선형 함수 근사를 활용한 일반화 능력으로 가장 강력한 기술을 구성합니다.\n\n강화 학습에 인공 신경망을 적용하는 두 가지 접근 방식인 딥 Q 학습(DQN)과 시간차 학습( Temporal Difference; TD(𝝀))에 대해 알아봅시다. 미리 목표 값들을 모르기 때문에 MC 또는 TD를 사용하여 목표 상태의 추정인 예상 반환값을 생성합니다. 그런 다음 이 값은 함수(실제로는 네트워크 매개 변수 𝜃에 대한 전체 네트워크의 오차의 편도함수인 경사)가 근사화할 목표값으로 사용됩니다. 인공 신경망은 목표값을 추정 값과 출력 사이의 오차를 계산하여 그 오차를 역전파 및 최적화 알고리즘을 통해 감소시킴으로써 목표 값을 근사화합니다. 가장 일반적인 최적화 알고리즘은 확률적 경사 하강법의 변형인 것이며, 이를테면 확률적 경사 하강법이 대표적입니다.\n\n![강화 학습 소개](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_24.png)\n\n## 오프-폴리시 DQN\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n큐-러닝은 SARSA(State, Action, Reward, State', Action')의 오프-폴리시 버전으로, 다음 상태-액션 쌍 Q(s', a')은 다음 상태에서 이용 가능한 액션들 중에서 최대 예측 값으로 추정됩니다. 다시 말해, 큐-러닝은 다음 상태 s'에서 이용 가능한 액션들 사이에서 Q(s',a')의 최댓값을 선택합니다. 이는 정책(𝜋)을 사용하지 않고 Q(s',a')를 학습한다는 것을 의미합니다. 반면, SARSA는 이전 액션을 선택하고 다음 상태-액션 쌍인 Q(s',a')를 추정하는 온-폴리시 방법입니다. 이는 상태가 주어졌을 때 액션의 확률인 정책(𝜋)을 사용하여 Q-함수를 학습한다는 것을 의미합니다.\n\n딥 큐-러닝에서는 액션-가치 함수 Q(a, s)가 Q(a,s, 𝜃)로 표현되며, 여기서 𝜃은 신경망 매개변수를 나타냅니다. Theta(𝜃) 매개변수는 신경망에서의 가중치 w에 해당하며, 뉴런들 간의 연결에 관련되어 있습니다. 이 가중치는 연결의 강도를 결정하고, 오차를 최소화하기 위해 역전파를 통해 후방으로 조정됩니다. DQN은 환경의 고차원 표현을 입력으로 삼고, 각 가능한 액션에 대한 액션-가치 벡터를 출력합니다. 예상 수익은 일반적으로 MC 또는 TD 접근법을 통해 근사됩니다. 이후, 역전파와 최적화 함수를 사용하여 정책 기울기를 계산하고 정책 네트워크의 매개변수(𝜃)를 조정하여 오차를 줄입니다.\n\n인공신경망은 새로운 정보에 매우 민감하기 때문에, 새로운 정보가 이전에 작성된 정보를 덮어쓰는 치명적인 잊혀짐을 초래할 수 있습니다. 이러한 치명적인 잊혀짐을 다루는 방법 중 하나는 경험 재생을 적용하는 것입니다. 이 기법은 과거 경험을 저장하고 네트워크를 훈련하는 데 재사용합니다.\n\n## 온-폴리시 딥 TD(𝝀) \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nANNs는 TD(λ) 방법에도 적용할 수 있습니다. 여기서 상태 관찰은 ANN에 입력으로 공급되고, ANN은 그것을 통해 액션-가치 함수를 출력으로 근사화합니다. TD(λ)의 온-폴리시 성격 때문에 Deep TD(λ) 방법은 상태 간의 장기 의존성이 필요한 작업에 가장 적합합니다.\n\nTD(λ)와 같은 온라인 학습 방법을 훈련하는 것은 도전적일 수 있습니다. 왜냐하면 환경의 분포가 부트스트래핑으로 인해 매 단계마다 또는 n 단계마다 변경되기 때문이기 때문입니다. 이를 비정상성(nonstationarity)이라고 부르며, ANN 매개변수 𝜃가 최적으로 수렴하는 것을 방해합니다. 온라인 학습에서 연이어 발생하는 상태 간의 종속성은 치명적인 잊혀짐(catastrophic forgetting)을 발생시킬 수 있어, 업데이트가 과거 학습에 영향을 미칩니다. 더욱이, 과거 조치에 신용을 할당하는 자격 흔적 및 ANNs의 결합은 역전파 단계에서 추가적인 복잡성을 초래할 수 있습니다.\n\n위기에 대처하는 한 가지 방법은 경험 재생(experience replay) 기법을 활용하는 것입니다. 경험 재생은 학습된 에이전트 에피소드를 [s, a, r, s’]의 벡터로 메모리 버퍼에 저장합니다. 훈련 중에 네트워크는 저장된 학습 벡터의 메모리 버퍼에서 샘플을 추출하여 네트워크 매개변수를 업데이트합니다. 이를 통해 네트워크는 더 큰 안정성을 제공받고, 새로운 경험으로 인한 큰 오류나 단계 간의 시간 차이로 인한 치명적인 간섭에서 덜 영향을 받습니다.\n\nDeep TD(λ) 알고리즘은 상태 공간이 연속적이고 대상이 알려지지 않거나 불분명한 연속 제어 작업에서 뛰어난 성과를 보여주었습니다. 이러한 작업에는 로봇 과제, 자율 주행 자동차, 금융 시장의 연속 제어 작업 등이 포함됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 강화 학습과 인공 일반 지능\n\n강화 학습이 인공 일반 지능에 미칠 영향은 무엇인가요?\n\n\"지능\"이라는 것은 서로 다른 능력을 단일한 개념으로 결합하기 때문에 모호한 변수이지만, \"일반 지능\"은 생물의 진화된 능력들을 기반으로 하며, 생존과 번식을 위해 세계적인 정보를 변환하는 것이 요구된다. 심지어 인간의 맥락에서도 지능은 유기적인 생존 가능성의 윤곽에서 분리될 수 없다. 그러나 이것이 통용되는 견해는 아니다. 일반적인 지혜는 지능이 이용 가능한 정보를 기반으로 추론을 계산하는 프로그램 또는 소프트웨어와 유사하다고 주장한다.\n\n후자의 개념은 경쟁한다고 여겨지는 두 가지 모델로 구성되어 있는데, 하나는 절차를 따르는 지능으로 설명되고, 다른 하나는 최적의 예측을 위해 데이터로부터 일반화하는 지능으로 설명된다. 전자는 일반적으로 더 잘 이해되지만, 후자는 예측의 강도를 신뢰성 있게 향상시키는 기법 집합으로 이루어져 있다고 할 수 있다. 동물의 지능은 대부분 후자 모델에 기반한다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 번째 모델의 가장 성공적인 패러다임은 인공 신경망을 통한 딥 러닝입니다. 인공 신경망 구조의 주요 장점은 사전 정보나 개념 없이 데이터로부터 일반화를 가능하게 한다는 것입니다. 이는 비지도 학습과 혼동해서는 안됩니다. 인공 신경망은 먼저 훈련을 통해 모델을 구축한 다음 새로운 데이터에 대해 해당 모델을 기반으로 예측합니다. 따라서 두뇌도 (진화를 통한 사전 훈련을 고려하면) 비슷한 일을 한다고 생각됩니다. 그러나 현재 인공 신경망에서는 두 가지 약점이 있습니다. 첫 번째 약점은 목표나 결과가 인간 디자이너에 의해 설정되어야 한다는 것입니다. 인공 신경망은 스스로 목표를 설정할 수 없습니다. 또한, 스스로 진실과 거짓을 구별할 수 없습니다. 모델이 그 결과를 근사화하는 방법을 배우기 위해 참된 결과를 제공해야 합니다. 두 번째 약점은 강화 학습 없이 인공 신경망이 자체 상태를 최적화하기 위해 환경을 탐색할 수 없다는 것입니다. 이러한 이유로 인공 신경망의 일반화와 예측 능력을 강화 학습의 결정 최적화 능력과 결합하는 것이 엄청난 섞임을 만들어냅니다.\n\n이 기반 위에서 강화 학습이 인공 일반 지능으로 가는 가장 명확한 길을 대표한다고 주장한 사람들도 있습니다(Sutton, 2014). 이에 대한 직관은 분명합니다. 강화 학습은 생명체를 모델링하는 데 가장 가깝습니다. 이것이 다른 성공적인 구조들과 결합되면 (예를 들어 변환기와 같은), 모든 인간 능력을 복제하고 능가하는 AI 모델로 이어질 수 있습니다.\n\n그러나 만약 인간이 일반 지능의 기초라면, 일반 지능의 개념은 인지능력을 생존 제약과 어떤 형태의 구현체와 이혼시키지 않는 것일 수 없습니다. 반면에, 일반 지능을 생명 형태에 언급하지 않고 정의할 수 있다면, 그것이 어떤 모습인지는 분명하지 않습니다. 즉 순수한 추상 모델은 Marcus Hutter의 AIXI와 같은 시도에도 불구하고 만족스러운 형식화를 피해갑니다. 추상적으로는 논리적 추론과 계산 능력만으로 문제를 해결하는 완벽하게 합리적인 에이전트로 생각할 수 있습니다. 정보와 구현체 간의 격차는 이 기사의 범위를 넘어서는 더 큰 토론을 유발하는 것이며, 관심이 있다면 이 논문이 좋은 시작점을 제공합니다.\n\n그러나 강화 학습만으로는 인공 일반 지능을 충분히 표현할 수 있는지에 대한 의문이 있습니다. 이에 대한 이유로는 일반 지능의 정의에서 비롯한 것들이며 현재의 대부분의 AI 연구자들이 명시적인 내부 표현을 필수적인 요소로 간주하지 않고 있기 때문입니다. 이에 대한 상당한 이유가 있습니다. 딥 러닝의 성공 이전에 인공 일반지능의 희망을 걸고 있던 상징적 AI는 실패로 이어졌습니다. 상징적 AI는 주로 명시적으로 코딩된 논리 규칙과 최적 추론 생성을 위한 지식 저장소에 기초한 인공 지능 접근 방식을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n상징적 인공지능과 신경망 사이의 긴장은 그러나 근거 없을 수도 있습니다. 많은 연구자들은 인공 일반 지능을 얻기 위한 탐구가 이러한 접근 방식을 적절히 결합하는 데 있다고 믿습니다. 신경망이 뇌의 본질적 오네톨로지를 근사화한다고 생각하는 이유는 수학적 논리가 뇌가 추론하는 방식이 아니기 때문입니다. 즉, 그것은 필요충분 조건을 계산하지 않거나 정확한 멤버십을 계산하는 것보다는 점수 있는 멤버십에 중점을 두며, 이는 퍼지 논리와 같은 방식에 의해 근사화되며 ANN(인공신경망)이 뛰어납니다.\n\n신경망은 원하는 출력을 달성하기 위해 파라미터화된 은닉층의 계층적 아키텍처로 구성되어 있으며, 고도로 보정된 동적 학습률, 활성화 함수, 연결 가중치 및 최적화 알고리즘을 통해 오차를 최소화하기 위해 보정된 학습율을 통해 원하는 출력을 달성합니다. 위와 같이 고도로 보정된 하이퍼파라미터를 넘어서 사람이 이해하지 못하는 정보가 은닉층에서 처리된다는 가정입니다. 정보가 이산적인 표현 단위(아날로그나 이미지 등)의 조합으로 저장되지 않으며 수십억 개의 뉴런들로 이루어진 분산 아키텍처로 저장된다는 것이 뇌의 경우와 같다는 가정입니다. 언어적으로 구조화된 생각이 고정적인 뉴런 조합으로 내부적으로 표현되는 것이 아닌데, 예를 들어 '존재 자체가 다른 존재를 위한 존재임을 하자’ 라는 문장 또는 단어를 나타내는 뉴런의 특정 조합이 없습니다.\n\n언어적 능력은 대신 경험에 의해 강화된 의미 연결과 재생규칙으로 내장된 거대한 네트워크에 포함되어 있습니다. 다시 말해, 우리가 반영적으로 문장을 작성하고 말할 때 언어와 생각이 뇌의 본질적 오네톨로지와 문법 간 이쾌적 매핑이 아닌, 연결의 정도와 연결 강도에 의해 특징을 가진 신경 접속의 분산 네트워크에 내재되어 있다는것을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 AI는 세계에서 자체적으로 추진되는 자율 시스템을 근사하지 않습니다. 또한 인간과 다른 동물들이 하는 방식으로 자체 환경적 환경을 생성하거나 자체 검색 공간을 재구성하지도 않습니다. 현재 이 제약이 없기 때문에 인간 디자이너는 AI의 정보적 중요성을 설정할 수 있습니다. 예를 들어 텍스트 생성, 환경 감지 등이 있습니다. 아키텍처가 진정한 문제 해결 기계로 진화하더라도 반성적 인식 능력이 없다면 일반적 지능을 보유하고 있다고 할 수 없습니다. 전통적으로 일반 지능의 정의에서는 인간 지능의 상징인 전체 인식의 변수를 생략합니다. 전통적인 지능 정의는 반사적이고 전체적 인식은 역공학 및 구성 요소의 분석에 대한 강한 저항을 가지고 있기 때문입니다. 그 이유로 반성적 인식은 지능의 구성 요소로서 배제됩니다. 그러나 현재의 과학적 설명에 대한 저항을 인정한다고 해도, 물리주의를 배제하거나 비자연론의 지지를 함축하지 않습니다. 오히려 이해력의 부재를 인정하는 신호일 뿐입니다. 이해력의 공방 속에, 반성적인 인식이 생명 유기체의 기본 속성인 감각의 확장임을 가설합니다. 이를 주장함으로써, 자연선택 이외의 방법을 통해 자율 시스템을 설계할 수 없다는 의미는 아니지만, 그들이 가까운 미래에도 과학적 분석에는 어려움을 줄 수 있다는 가능성은 여전히 열려 있습니다. 강화 학습이 일반적 지능으로 이어지길 바란다면, 에이전트는 세계의 복잡한 표현을 보유할 뿐만 아니라 그 표현의 내부에서 전반적인 관점을 유지할 수 있는 강력한 아키텍처를 사전으로 가져야 합니다. 이는 모델-세계 상호작용이 과제에 반드시 필수적이지만, 원조 아키텍처는 다중 모달 정보 처리와 통합 능력을 갖춘 복합적인 계층적 내부 구조를 요구할 것입니다.\n\n## 선택된 참고 자료\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., \u0026 Hassabis, D. (2015). Deep Reinforcement Learning을 통한 Human-level Control. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236\n\nNeftci, E. O., \u0026 Averbeck, B. B. (2019, March 4). 인공 및 생물학적 시스템에서 강화 학습. Nature News. https://www.nature.com/articles/s42256-019-0025-4\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSharma, S. (2024, March 7). Learning to Mix 𝑛-Step Returns: Generalizing 𝜆-Returns for Deep Reinforcement Learning. Ar5iv. [Link](https://ar5iv.labs.arxiv.org/html/1705.07445)\n\nSanghi, Nimish. Deep Reinforcement Learning with Python: With PYTORCH, Tensorflow, and Openai Gym. Apress, 2021.\n\nSilver, D., Singh, S., Precup, D., \u0026 Sutton, R. S. (2021). Reward is enough. Artificial Intelligence, 299, 103535. [Link](https://doi.org/10.1016/j.artint.2021.103535)\n\nSpens, E., \u0026 Burgess, N. (2024, January 19). A generative model of memory construction and consolidation. Nature News. [Link](https://www.nature.com/articles/s41562-023-01799-z)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSutton, Richard S. Introduction to Reinforcement Learning. MIT Press.\n\nTyng, C. M., Amin, H. U., Saad, M. N. M., \u0026 Malik, A. S. (2017, August 24). The influences of emotion on learning and memory. Frontiers in psychology. [Link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5573739/)\n\nWhite, A., Modayil, J., \u0026 Sutton, R. (2014). Surprise and Curiosity for Big Data Robotics. Association for the Advancement of Artificial Intelligence, 19–22.","ogImage":{"url":"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png"},"coverImage":"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png","tag":["Tech"],"readingTime":28},{"title":"로스 2에서 확장 칼만 필터를 활용한 센서 융합","description":"","date":"2024-06-19 06:22","slug":"2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2","content":"\n\n![Sensor Fusion with the Extended Kalman Filter in ROS2](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png)\n\n안녕하세요! 이 글은 가우시안 필터를 소개하는 시리즈 중 두 번째 글입니다. 구체적으로, 이 글은 칼만 필터 패밀리에 대한 세 번째 세부 소개입니다. 이미 칼만 필터에 대해 익숙하지 않다면, 계속하기 전에 첫 번째 글을 읽기를 권장합니다. 다음 글에서는 언센티드 칼만 필터를 소개할 예정입니다. 이 글의 결과를 재현하는 데 사용된 데이터와 코드는 이 글의 끝 부분에 찾을 수 있습니다.\n\n# 소개\n\n이전 글에서 소개된 대로, 성공적인 로봇 시스템은 유용한 작업을 수행하기 위해 물리적 세계를 인식하고 조작할 수 있어야 합니다. 이를 달성하기 위해 로봇은 환경의 중요한 불확실성을 고려해야 합니다. 현대 로봇 공학에서 가장 기본적인 문제 중 하나는 상태 추정입니다. 상태 추정은 로봇과 환경(랜드마크 및 기타 객체의 위치 등)의 가장 확률적인 상태(예: 위치, 방향, 속도)를 불확실한(잡음이 있는) 및 아마도 불완전한 정보를 기반으로 결정하는 것을 포함합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n칼만 필터는 불확실한 환경에서 상태 추정에 대응합니다. 필터는 상태의 각 요소 (예: x, y 좌표, 헤딩)을 가우시안 확률 변수로 모델링합니다. 가우시안은 상태 x에 대한 확률 밀도를 벡터 μ (뮤)와 공분산 행렬 Σ (시그마)만 사용하여 표현할 수 있게 합니다. 이 매개변수화에서 우리의 상태 x는 기대값 μ에 의해 표현되며, Σ는 제어, 이동 및 관측 잡음으로 인한 상태의 내재적 불확실성을 포착합니다.\n\n칼만 필터가 사용하는 베이지안 프레임워크에서는 전체 상태를 믿음(belief)이라고 합니다. 가우시안 (또는 정규분포)을 사용하는 장점은 그들의 수학적 성질에 있습니다. 이 성질은 칼만 필터 방정식을 단순화합니다. 가우시안 믿음이 선형 변환을 겪을 때의 특징 중요한데, 가우시안 믿음이 선형 변환을 겪으면 결과는 여전히 가우시안 확률 변수로 유지됩니다. 이 성질은 칼만 필터의 방정식이 우아하고 다루기 쉬운 상태를 유지하도록 보장합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_1.png)\n\n사후 믿음을 계산하기 위해 칼만 필터는 이전 믿음을 시간을 경과함에 따라 전달하는 모션 모델을 사용합니다. 그런 다음 관측 모델은 로봇 센서에서의 데이터를 통합하여 예측된 믿음을 업데이트하고 이를 사후로 변환합니다. 칼만 필터 알고리즘은 아래에서 요약됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Linearity of the Linear Kalman Filter](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_2.png)\n\n선형 칼만 필터의 \"선형성\"은 알고리즘의 2번째와 5번째 줄에서 가장 명백합니다. 2번째 줄에서 예측된 상태는 이전 상태 μ_t−1과 제어 입력 u_t의 선형 함수입니다. 5번째 줄에서 예측된 관측값 (y = Cμ)은 인수 μ^bar_t의 선형 함수입니다. 이 선형성은 가우시안 특성을 유지하여 필터를 구현하기 쉽게 만듭니다. 그러나 이것은 또한 주요 약점 중 하나를 나타냅니다.\n\n선형 칼만 필터가 실제 문제에 부적합한 이유는 실제 문제가 종종 선형적이지 않기 때문입니다. 이전 글에서 도입했던 간단한 상태 추정의 경우, 상태가 모바일 로봇의 2차원 포즈 (x, y, θ)로 표현되었지만 정확히 선형적이지 않았습니다. 아래에 다시 소개되는 이동 모델에서 확인할 수 있습니다.\n\n![Motion Model](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_4.png)\n\n모델이 쉽게 알아볼 수 있듯이, 삼각 함수가 사용되어 로봇의 좌표를 업데이트하는데 사용되며, 이러한 함수들은 선형이 아닙니다. 이 모델에서 선형 칼만 필터는 어느 순간 발산할 가능성이 있습니다. 이 이유로 선형 칼만 필터를 소개한 후, 대부분의 실제 현상의 비선형성을 고려하기 위해 확장 방법을 고안하는 작업이 즉시 시작되었습니다.\n\n# 확장 칼만 필터\n\n## 비선형성의 문제\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비선형성에 관한 문제를 더 자세히 설명하기 위해 다음 애니메이션을 살펴볼 수 있습니다. 첫 번째 애니메이션에서는 선형 칼만 필터의 가정이 성립하는 세계에 있으며, 새로운 상태가 인수에 대해 단순하게 선형인 경우를 살펴봅니다. 시각화를 이해하기 쉽게 하기 위해 1차원 상태 x를 가정합니다. 이 애니메이션은 가우스를 선형 함수 g를 통과시켰을 때 다른 가우스가 되는 과정을 보여줍니다. 이 경우 g = -0.5*x+1입니다.\n\nx의 가우시안 표현에서 시작하지만 비선형 함수 g를 선택하는 경우, 결과 확률 밀도 함수는 더 이상 가우시안이 아닙니다. 새로운 밀도를 계산하기 위한 폐쇄형 방법이 없습니다. 대신 입력 분포에서 점들을 샘플링하고 이를 g를 통과시켜 출력 히스토그램을 구축하여 출력 분포를 만들어야 합니다. 아래에 표시된 출력의 형태에서 확인할 수 있듯이, 이는 가우시안이 아닌 것을 알 수 있습니다. 또한 이 출력 분포는 칼만 필터의 단봉성 가정을 위배하며, 단일 피크를 요구합니다. P(y)의 가우시안 근사는 출력 데이터에 가우시안을 맞추어 얻었습니다. 이는 실제 모델의 비선형성을 다루기 위한 선형 칼만 필터의 한계를 강조합니다.\n\n요약하면 비선형 모델을 다룰 때 출력 밀도는 가우시안이 아니며 폐쇄형으로 계산할 수 없으며 종종 다중 피크를 갖기 때문에 칼만 필터 방정식이 무용지물이 됩니다. 이 문제를 해결하기 위해 확장 칼만 필터(EKF)는 선형성 가정을 버립니다. 대신 상태 전이 확률과 측정 확률은 비선형 함수 g 및 h에 따라 결정됨을 수용합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_5.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![그림](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_6.png)\n\n모든 것이 잘 진행되고 있어요. 칼만 필터의 우아한 방정식을 활용하려면 여전히 우리의 신뢰를 가우시안으로 표현해야 합니다. 이 필수성은 정확한 사후분포를 계산하는 것에서 EKF의 초점을 이동시켜 실제 신뢰의 좋은 가우시안 근사값을 찾게 됩니다. 아래 그림에서 보듯, 몬테칼로를 사용하여 출력 분포를 계산한 후, 그에 대한 가우시안을 fitting하고 필요한 매개변수 μ (뮤)와 Σ (시그마)를 얻을 수 있습니다. 그러나 아직도 가우시안을 닫힌 형태로 계산할 수 없는 문제가 남아 있습니다.\n\n## 테일러 전개를 통한 선형화\n\n이 문제를 해결하기 위해 확장 칼만 필터는 선형화라는 추가 근사값을 적용합니다. 선형화의 핵심 아이디어는 비선형 함수 g를 해당 관심점에서 g에 접하는 접선인 선형 함수로 근사하는 것입니다. 비선형 함수를 선형화하는 다양한 기술 중, EKF가 사용하는 것은 일차 테일러 전개입니다. 함수의 테일러 전개는 함수의 도함수를 하나의 점 a에서 표현된 다항식 항들의 무한 합입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 이미지에서 높은 차수의 테일러 전개가 점 a = 0 주변에서 g를 더 가까운 근사로 제공하는 것을 볼 수 있습니다. 그러나 고차 다항식이 늘어날수록 요구되는 계산도 증가하며, 문제가 빠르게 풀기 어려워집니다. 다행히도 Kalman 필터가 자주 업데이트되는 경우(작은 Δt), 관심점 a의 차이가 매우 작아야합니다. 따라서 우리는 다음 (매우 가까운) 각 지점 a에서의 함수 g의 값을 및 기울기(점 a에서의 미분)를 사용하여 함수 g의 선형 근사를 얻기 위해 1차 다항식(선)을 사용할 수 있습니다. 이 문제는 본질적으로 아래와 같이 간소화됩니다:\n\n\n![2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_9.png](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_9.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 동작이 왜 잘 작동하는지 설명하기 위해 g라는 함수를 가정해 볼게요.\n\n![그림](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_10.png)\n\n점 a에서 함수 g의 일차 테일러 전개는 아래 그림에서 빨간색으로 표시되어 있어요. 큰 x 값 범위를 관찰하면 좋은 근사치를 제공하지 않음이 명백해요.\n\n![그림](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_11.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 우리가 사후 분포의 값들에 대한 근사치에만 주의를 기울이기 때문에, 새로운 사후 분포에 대해 그러한 근사치를 매우 짧은 시간 이후에 다시 계산할 것을 알고 있기 때문에, 아래 그래프에서 볼 수 있듯이, 우리의 근사치가 관심 지점 주변에서 매우 좋다는 것을 알 수 있습니다. 이는 첫 번째 차수 테일러 전개가 우리의 목적에 대해 충분한 근사치를 제공하며, 시스템 내부의 비선형성에도 불구하고 확장 칼만 필터를 효과적으로 적용할 수 있도록 합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_12.png)\n\n마지막으로, 아래 이미지는 두 프로세스를 비교하려고 합니다: 첫 번째는 원래의 가우시안으로 시작하여 비선형 함수 g를 통과시키고, 몬테카를로를 사용하여 비가우시안 출력 분포를 얻은 후, 이 출력에 가우시안을 적합시킵니다. 두 번째로, EKF가 사용하는 프로세스는 g를 선형화시키고, 원래의 가우시안을 이 선형 근사치를 통해 통과시킨 후, 선형화를 통해 닫힌 형태로 출력을 직접 얻습니다. 이 비교는 비선형 시스템 다루기에 대한 EKF 접근 방식의 효율성과 실용성을 강조합니다.\n\n명확성을 위해, 아래에서는 출력만 표시됩니다. 보시다시피, EKF 가우시안이 몬테카를로 시뮬레이션에서 적합된 가우시안과 정확히 같지는 않지만, 충분히 가깝습니다. 이 작은 차이는 실제 분포의 닫힌 형태의 추정치를 효율적으로 얻기 위해 지불하는 대가입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서 설명한 간단한 예제는 스칼라 경우에 대한 것이었지만, 우리의 상태는 벡터입니다. 따라서 기울기를 구하기 위해 우리는 상태에 대한 g의 편미분을 계산합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_13.png)\n\ng와 그 기울기 g′의 값은 그들의 인자 (u_t와 x_t-1)에 따라 달라지는데요, 이는 우리의 관심 지점입니다 (스칼라 경우의 a와 대조적입니다). u_t의 값에 대해서는 로봇에 제공된 제어 명령을 사용합니다. x_t-1에 대해서는 선형화할 시기에서 가장 가능성이 높은 상태의 값으로 선택할 수 있습니다. 가우시안의 경우, 최대 가능성 값은 이전 시간 단계에서 계산된 사후값의 평균인 μ_t-1로 표시됩니다. 이 선형화는 업데이트 속도가 매우 빠른 필터 (매우 작은 Δt)에 대해 잘 작동하며, 이때 μ_t-1의 값과 우리가 추정하려는 현재 상태 간의 차이가 크지 않을 때 잘 작동합니다.\n\n이제 기울기를 계산했기 때문에 g를 다음과 같이 추정할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![sensor fusion](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_14.png)\n\n가우시안에서, 모션 모델 또는 상태 전이 확률은 아래와 같이 표기됩니다. 여기서 R_t는 보통의 프로세스 노이즈 공분산입니다.\n\n![motion model](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_15.png)\n\n기울기가 숫자인 스칼라 케이스와는 달리, g'(u_t, μ_t-1)으로 알려진 G_t는 행렬입니다. 비선형 함수 g에 대한 상태 x의 일차 편미분값을 모두 포함하는 이 행렬은 야코비안이라고 합니다. 이 야코비안 행렬은 상태의 차원인 n×n의 크기를 가지며, 현재 제어 및 이전 사후 평균에 따라 값이 달라집니다. 따라서 야코비안 값은 시간이 지남에 따라 변합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n확장 칼만 필터(Extended Kalman Filter)는 함수 h에 의해 표현되는 비선형 관측 모델을 다룹니다. 특히, 타일러 전개(Taylor Expansion)는 새롭게 예측된 믿음 μ^bar_t을 중심으로 진행되며, 이는 h를 선형화하는 시점에서 가장 가능성이 높은 상태입니다.\n\n![이미지](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_16.png)\n\n![이미지](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_17.png)\n\n가우시안으로, 측정 모델은 아래와 같이 표기됩니다. 여기서 Q_t는 전통적인 측정 잡음 공분산입니다. 여기서 야코비안 H_t는 관측 모델의 비선형 함수 h에 대한 상태 x에 대한 일차 편미분의 m×n 행렬입니다. 따라서 m은 관측의 차원이고, n은 상태의 차원입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_18.png\" /\u003e\n\n## Extended Kalman Filter Algorithm\n\n요약하면, 아래에 표시된 EKF 알고리즘은 이 기사의 앞부분에 표시된 LKF 알고리즘과 매우 유사하지만, 주요한 차이점은 모션 및 관측 모델의 선형화가 2번 줄과 5번 줄에서 이루어진다는 것입니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_19.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 이미지는 EKF 알고리즘을 LKF와 나란히 재진 다음 주요 차이점을 강조합니다. 예측 단계에서 EKF는 선형 시스템 행렬 A 및 B 대신 상태를 시간에 따라 진화시키는 비선형 함수 g를 사용합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter�...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 모델 선형화\n\n선형 칼만 필터와 사용된 모션 모델은 정확히 선형은 아니지만 여전히 단순하여 LKF가 처리할 수 있는 간단한 상수 속도 모델이었습니다. 참고로, 아래에 다시 표시해 드립니다. 이 모델은 확장 칼만 필터의 능력을 보여주기 위해 사용될 더 복잡한 변형을 소개하는 기초 역할을 할 것입니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_21.png)\n\n이 방정식들은 다음과 같이 LKF에서 요구하는 선형 시스템 행렬로 변환되었습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식입니다.\n\n\n![sensorfusion1](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_22.png)\n\nEKF를 사용하여 동일한 모델을 사용하려면 선형화해야 합니다. 먼저로봇의 상태 형식을 재정의합니다.\n\n![sensorfusion2](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_23.png)\n\n이후 비선형 함수 g를 정의합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_24.png\" /\u003e\n\n다음으로, 우리는 각 상태 변수 x, y, 그리고 θ에 대한 함수 g의 편도함수를 포함하는 야코비안 행렬 G를 정의합니다. 이 행렬은 상태 변수의 변화가 운동 모델에 어떻게 영향을 미치는지를 포착합니다. 야코비안 행렬 G는 EKF 알고리즘에서 공분산 행렬을 업데이트하는 데 사용될 것이며, 우리에게 운동 모델의 비선형성들을 고려하면서도 칼만 필터 프레임워크의 계산 효율성을 유지할 수 있게 해줍니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_25.png\" /\u003e\n\nx 업데이트 방정식의 x, y, 그리고 θ에 대한 도함수를 나타내는 야코비안 행렬 G의 첫 번째 행을 살펴보면, 다음과 같은 것을 볼 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 첫 번째 항목은 x에 대한 부분 도함수이기 때문에 1입니다.\n- 두 번째 항목은 x가 y에 의존하지 않음을 나타내는 0입니다.\n- 세 번째 항목은 −vsin(θ)Δt이며, 이것은 x가 θ에 −vsin(θ)Δt항으로 의존함을 보여줍니다. 이것은 θ의 변화가 x 좌표에 미치는 영향을 반영합니다.\n\n다음으로 측정 함수 h가 필요합니다. 이전 Linear Kalman Filter의 구현과 유사하게, 사용할 측정은 로봇의 오도메트리 시스템에서 제공하는 것만 사용할 것입니다. 이 시스템은 바퀴 엔코더에서 계산된 로봇의 추정 자세를 직접 제공합니다. 그 데이터 형식이 우리의 상태 형식과 일치하기 때문에, h(μ^bar_t)는 단순히 μ^bar_t를 반환합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_26.png)\n\n그리고 상태에 대한 측정 함수의 부분 미분을 포함하는 Jacobian 행렬 H는 단위 행렬이며 아래와 같이 표시됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![sensor fusion](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_27.png)\n\n이전 그림에서 EKF와 LKF를 비교한 것처럼, 선형화가 두 알고리즘의 주요 차이점입니다. 관련 기능 및 해당 야코비안을 확인한 후, EKF의 구현은 LKF의 구현을 밀접하게 따릅니다. 아래는 선형화된 속도 모션 모델의 Python 구현입니다. 행렬 g와 야코비안 G를 모두 반환합니다.\n\n```python\ndef velocity_motion_model_linearized_1():\n\n def state_transition_function_g(mu = None, u = None, delta_t = None):\n  x = mu[0]\n  y = mu[1]\n  theta = mu[2]\n  \n  v = u[0]       \n  w = u[1]       \n  \n  g = np.array([\n            x + v * np.cos(theta) * delta_t,\n            y + v * np.sin(theta) * delta_t,\n            theta + w * delta_t\n        ])\n\n  return g\n\n def jacobian_of_g_wrt_state_G(mu = None, u = None, delta_t = None):\n  theta = mu[2]\n  v = u[0]       \n  w = u[1]       \n  \n  G = np.array([\n   [1, 0, -v * np.sin(theta) * delta_t],\n   [0, 1, v * np.cos(theta) * delta_t],\n   [0, 0, 1]\n  ])\n\n  return G\n\n return state_transition_function_g, jacobian_of_g_wrt_state_G\n```\n\n다음으로, 다음과 같이 간단한 관측 모델을 구현합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef odometry_observation_model_linearized():\n def observation_function_h(mu):\n  return mu\n \n def jacobian_of_h_wrt_state_H(mu):\n  return np.eye(3)\n\n return observation_function_h, jacobian_of_h_wrt_state_H\n```\n\n마지막으로, 아래에 구현된 Extended Kalman Filter 코드를 확인해보세요. 직전 게시물에서 소개된 Linear Kalman Filter 코드와 얼마나 비슷한지 주목하세요.\n\n```js\nimport numpy as np \n\nfrom rse_motion_models.velocity_motion_models import velocity_motion_model_linearized_1\nfrom rse_observation_models.odometry_observation_models import odometry_observation_model_linearized\n\nclass KalmanFilter:\n\n def __init__(self, initial_state, initial_covariance, proc_noise_std = [0.02, 0.02, 0.01], obs_noise_std = [0.02, 0.02, 0.01]):\n\n  self.mu = initial_state # 초기 상태 추정\n  self.Sigma = initial_covariance # 초기 불확실성\n\n  self.g, self.G = velocity_motion_model_linearized_1() # 사용할 액션 모델\n\n  # 과정 모델 노이즈의 표준 편차\n  self.proc_noise_std = np.array(proc_noise_std)\n  # 과정 노이즈 공분산 (R)\n  self.R = np.diag(self.proc_noise_std ** 2) \n\n  self.h, self.H = odometry_observation_model_linearized() # 사용할 관측 모델\n\n  # 관측 모델 노이즈의 표준 편차\n  self.obs_noise_std = np.array(obs_noise_std)\n  # 관측 노이즈 공분산 (Q)\n  self.Q = np.diag(self.obs_noise_std ** 2)\n\n def predict(self, u, dt):\n  # 상태 추정 (mu) 예측\n  self.mu = self.g(self.mu, u, dt)\n  # 공분산 (Sigma) 예측\n  self.Sigma = self.G(self.mu, u, dt) @ self.Sigma @ self.G(self.mu, u, dt).T + self.R\n\n  return self.mu, self.Sigma\n\n def update(self, z, dt):\n  # 칼만 이득 (K) 계산\n  K = self.Sigma @ self.H(self.mu).T @ np.linalg.inv(self.H(self.mu) @ self.Sigma @ self.H(self.mu).T + self.Q)\n  \n  # 상태 추정 (mu) 업데이트\n  self.mu = self.mu + K @ (z - self.h(self.mu))\n\n  # 공분산 (Sigma) 업데이트\n  I = np.eye(len(K)) \n  self.Sigma = (I - K @ self.H(self.mu)) @ self.Sigma\n\n  return self.mu, self.Sigma\n```\n\n성능:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nExtended Kalman Filter(EKF)의 성능을 평가하기 위해, 동일한 상수 속도 모델과 오도메트리 관측 모델을 사용하는 선형 칼만 필터(LKF)의 성능과 비교할 것입니다. 먼저, 초기 과정 노이즈를 크게 설정하고 관측 노이즈를 매우 낮게 설정할 것입니다. 이 설정은 사실상 필터에게 행동 모델보다는 관측을 신뢰하도록 지시합니다. 예상했듯이, 두 버전의 칼만 필터는 관측을 따라가며 비슷한 성능을 발휘합니다. 관측 모델의 간단함과 선형성을 고려하면 이 결과가 예상된 것입니다.\n\n다음 테스트는 더 어려울 것입니다. 과정 노이즈를 매우 낮게 설정하고 관측 노이즈를 매우 높게 설정할 것입니다. 이 설정은 필터가 대부분의 관측을 무시하고 상태 추정에 운동 모델에 크게 의존하게 만듭니다. 운동 방정식이 같더라도, EKF가 적용한 선형화 때문에 크게 다른 결과를 예상할 것입니다.\n\n예상대로, 차이는 상당합니다. 이러한 노이즈 설정 하에서 LKF는 성능이 저조합니다. 비선형 운동 모델에 완전히 의존할 때, LKF의 결과는 실제 값과 로봇 센서에서 보고된 오도메트리에서 크게 벗어납니다. 반면에, EKF는 관측 데이터에 더 가까이 머물며 훨씬 나은 성능을 발휘합니다. 또한 오른쪽 그림의 큰 타원에서 나타나는 바와 같이 EKF는 높은 불확실성을 정확히 나타냅니다. 반면에, LKF는 낮은 불확실성을보고하는데, 이것은 부정확합니다.\n\n## 대안 운동 모델\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n과거 두 필터에서 사용된 모션 모델은 꽤 간단하며 로봇이 헤딩 각도 θ(세타)의 방향으로 직선으로 이동한다고 가정합니다. 보다 정교한 모션 모델이 존재하며, 그 모델이 더 나은 성능을 발휘할 수 있는지 확인하는 것이 중요합니다. 다음에 탐구할 새로운 상수 속도 모션 모델은 로봇을 변환(직선) 및 회전(각도) 속도 v 및 ω를 통해 제어할 수 있도록 합니다. 직선 이동을 가정하는 대신, 이 모델은 아래에 나와 있는 것처럼 반지름이 r인 원 위를 로봇이 이동한다고 가정합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_28.png)\n\n직선 모션 가정과 같이 원형 모션 가정도 근사값일 뿐이며, 시간 간격이 매우 작은 경우에만 유효합니다(움직임이 원이건 직선이건 구분할 수 없을 정도로 아주 작을 때). ω 값이 0에 가까워질수록, 반지름은 아주 크게되어 거의 직선 상에서의 움직임을 나타내게 됩니다. 시간에 따라 상태를 진화시키기 위해 이 모델을 따르는 비선형 함수 g에 해당하는 방정식 벡터는 아래에 나와 있습니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_29.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제이 모델을 선형화하기 위해 g의 자코비안을 계산해야 합니다. 이는 상태에 대한 g의 편도함수에 해당합니다. 우리의 상태는 다음과 같습니다:\n\n![State](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_30.png)\n\n따라서 자코비안 G는 다음과 같이 표현됩니다:\n\n![Jacobian](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_31.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서 보았듯이, 관련 행렬을 결정한 후에 구현은 간단합니다. 먼저, 아래 속도 모델 코드를 정의합니다.\n\n```js\ndef velocity_motion_model_linearized_2():\n\n def state_transition_function_g(mu = None, u = None, delta_t = None):\n  x = mu[0]\n  y = mu[1]\n  theta = mu[2]\n  \n  v = u[0]       \n  w = u[1]       \n  if w == 0:\n   w = 1e-6   # 직선 이동의 경우 0으로 나누는 것을 피하기 위해\n\n  g = np.array([\n     x + -v/w * np.sin(theta) + v/w * np.sin(theta + w * delta_t),\n     y + v/w * np.cos(theta) - v/w * np.cos(theta + w * delta_t),\n     theta + w * delta_t\n  ])\n\n  return g\n\n def jacobian_of_g_wrt_state_G(mu = None, u = None, delta_t = None):\n  theta = mu[2]\n  v = u[0]       \n  w = u[1]       \n  if w == 0:\n   w = 1e-6   # 직선 이동의 경우 0으로 나누는 것을 피하기 위해\n\n  G = np.array([\n   [1, 0, -v / w * np.cos(theta) + v / w * np.cos(theta + w * delta_t)],\n   [0, 1, -v / w * np.sin(theta) + v / w * np.sin(theta + w * delta_t)],\n   [0, 0, 1]\n  ])\n\n  return G\n\n return state_transition_function_g, jacobian_of_g_wrt_state_G\n```\n\n관측 모델은 이전 예제에서 사용한 것과 정확히 동일하며 완전성을 위해 여기에 다시 제시하겠습니다.\n\n```js\ndef odometry_observation_model_linearized():\n def observation_function_h(mu):\n  return mu\n \n def jacobian_of_h_wrt_state_H(mu):\n  return np.eye(3)\n\n return observation_function_h, jacobian_of_h_wrt_state_H\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내 두 번째 확장 칼만 필터가 아래에 구현되었습니다. 코드는 이전 것과 정확히 동일하지만 움직임 모델을 정의하는 라인만 예외입니다.\n\n```js\nclass KalmanFilter:\n\n def __init__(self, initial_state, initial_covariance, proc_noise_std = [0.02, 0.02, 0.01], obs_noise_std = [0.02, 0.02, 0.01]):\n\n  self.mu = initial_state # 초기 상태 추정\n  self.Sigma = initial_covariance # 초기 불확실성\n\n  self.g, self.G = velocity_motion_model_linearized_2() # 사용할 액션 모델\n  \n  # 프로세스 또는 액션 모델 노이즈의 표준 편차\n  self.proc_noise_std = np.array(proc_noise_std)\n  # 프로세스 노이즈 공분산 (R)\n  self.R = np.diag(self.proc_noise_std ** 2) \n\n  self.h, self.H = odometry_observation_model_linearized() # 사용할 관측 모델\n\n  # 관측 또는 센서 모델 노이즈의 표준 편차\n  self.obs_noise_std = np.array(obs_noise_std)\n  # 관측 노이즈 공분산 (Q)\n  self.Q = np.diag(self.obs_noise_std ** 2)\n\n def predict(self, u, dt):\n  # 상태 추정 (mu) 예측\n  self.mu = self.g(self.mu, u, dt)\n  # 공분산 (Sigma) 예측\n  self.Sigma = self.G(self.mu, u, dt) @ self.Sigma @ self.G(self.mu, u, dt).T + self.R \n\n  return self.mu, self.Sigma\n\n def update(self, z, dt):\n  # 칼만 이득 (K) 계산\n  K = self.Sigma @ self.H(self.mu).T @ np.linalg.inv(self.H(self.mu) @ self.Sigma @ self.H(self.mu).T + self.Q)\n  \n  # 상태 추정 (mu) 업데이트\n  self.mu = self.mu + K @ (z - self.h(self.mu))\n\n  # 공분산 (Sigma) 업데이트\n  I = np.eye(len(K)) \n  self.Sigma = (I - K @ self.H(self.mu)) @ self.Sigma\n\n  return self.mu, self.Sigma\n```\n\n성능:\n\n이 시점에서 EKF가 LKF보다 우월함이 명확하며, 이제는 더 정교한 움직임 모델이 뚜렷한 향상을 가져오는지를 결정하는 것에 중점을 두고 연구를 계속할 것입니다. 관측 모델이 명백히 움직임 모델보다 우선하는 잡음 설정에서 평가를 수행하지 않을 것이며, 이렇게 하면 모든 필터에 대해 단순히 관측 결과와 일치할 것으로 예상됩니다. 따라서 여기서는 관측을 대부분 무시하고 움직임 모델에 더 집중하는 잡음 설정으로 결과를 제시합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n놀랍게도 더 고급 모델이 더 간단한 모델보다 우수한 성능을 내지 못했습니다. 위의 두 개의 도표에서는 순수한 눈으로는 중요한 성능 차이를 알아차리기 힘듭니다. 심지어 보고된 잡음 타원도 거의 같아 보입니다.\n\n이전의 기사에서 얻은 결론 중 하나는 우리도 이곳에서 도출하는 것인데, 어떻게 소위 세련되고 고급스러운 필터를 사용하더라도 잘못된 데이터를 입력하면 잘못된 추정값을 제공할 것이라는 것입니다. 우리의 오도메트리 관측에 문제가 있다는 사실을 조정할 방법이 없네요. 이전의 모든 도표에서 주요 문제가 헤딩인 것처럼 보입니다. 올바른 헤딩 데이터가 없으면 어떤 고급 움직임 모델도 이를 보상할 수 없을 겁니다. 로봇이 헤딩의 대안 소스를 제공한다면 (그리고 다른 유용한 센서 데이터도 제공한다면), 필터를 사용하여 서로 다른 소스를 융합하고 더 나은 추정값을 얻을 수 있습니다. 다음에는 센서 융합을 살펴보겠습니다.\n\n## 센서 융합의 힘\n\n센서 융합에는 다양한 종류가 있으며, 우리가 적용할 유형은 저수준 융합으로 알려져 있습니다. 저수준 데이터 융합의 목표는 함께 더 유익한 여러 소스의 기본 센서 데이터를 결합하는 것입니다. 이 아이디어는 서로 다른 소스의 강점을 활용하여 더 나은 추정값을 만들어내는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 원래 오도메트리 데이터와 관성 측정 장치(IMU)에서 오는 데이터를 어떻게 결합하는지에 대해 연구할 것입니다. IMU는 일반적으로 우리에게 방향, 각속도 및 선형 가속도를 제공할 수 있습니다. 이는 우리가 부정확한 추정이 나쁜 방향 정보 때문인 것으로 의심하기 때문에 매우 유익합니다. IMU는 이러한 종류의 정보에 대해 오도메터보다 정확할 경향이 있기 때문에, IMU 데이터가 오도메트리와 어떻게 융합되어 더 나은 결과를 얻을 수 있는지 알아보겠습니다.\n\n우리가 할 첫 번째 일은 상태와 모델을 확장하는 것입니다. 이전에는 추가 데이터의 좋은 소스가 없었기 때문에 로봇의 자세 이상을 고려하는 것이 별 의미가 없었습니다. 그러나 이제 우리에게 각속도와 선형 가속도를 제공할 수 있는 센서가 있기 때문에, 새로운 정보를 상태에 통합하는 것이 로봇의 상태를 더 잘 추정하는 데 도움이 될 것입니다. 이는 또한 우리의 동작 및 센서 모델을 확장할 것이기 때문에 필수적입니다. 이 기사에서는 새로운 7차원 상태로 시작하는 두 가지 다른 확장을 시도할 것입니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_32.png)\n\n여기서 x, y 및 θ는 친숙한 로봇 자세에 해당합니다. 과거 구현과는 달리, 속도 v와 각속도 ω가 제어 입력에서만 얻어졌던 경우, 이제 이들의 추정치도 상태의 일부로 유지합니다. 상태에는 x 및 y의 선형 가속도도 포함됩니다. 확장된 상태를 고려할 때, 동작 모델도 각 상태 구성 요소를 계산하기 위해 확장되어야 합니다. 가속도를 고려하는 동작 모델은 더 이상 상수 속도 모델이 될 수 없습니다. 따라서 속도는 추정 사이에 변화하고, 가속도는 일정하게 유지된다는 새로운 가속도 일정 모델을 사용할 것입니다. 이 새로운 가속도 상수 모델은 아래 표시된 함수 g와 야코비안 행렬 G에 의해 표현됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_33.png)\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_34.png)\n\n지금부터 조금 복잡해집니다. 이 큰 야코비안 행렬을 계산하는 것은 매우 에러가 발생하기 쉽습니다. 수동으로 시도해보는 것도 재미있지만, 항상 결과를 확인하기 위해 소프트웨어를 사용하는 것이 좋은 생각입니다. 계산이 올바르다고 확신하고 구현한 후에도 해당 구현이 정확한지 확인할 방법이 있어야 합니다. 이 기사에 구현된 모든 야코비안에 대해, 테스트 스크립트가 잡아낸 적어도 하나의 작은 오류가 있었습니다. 야코비안을 계산하고 구현이 올바른지 확인하는 데 소프트웨어를 사용하지 않으면 코드에 버그를 도입할 확률이 높습니다.\n\n상태 및 운동 모델을 성공적으로 확장했습니다. 이제 센서 퓨전을 가능하게 하려면 관찰 모델도 확장해야 합니다. 확장된 상태 및 운동 모델과 결합하여 퓨전을 수행할 수 있도록 두 센서의 데이터를 연결하려 할 것입니다. 새로운 관찰 모델은 아래에 표시된 벡터의 첫 세 요소인 보통의 로봇 자세와 IMU 센서에서 파생된 방향 θ_imu, 각 속도 ω, 그리고 x 및 y 구성 요소에서의 가속도 a_x 및 a_y와 조합된 것입니다. 우리가 선속도 v를 직접 관측하지는 않지만 여전히 상태에 있을 수 있으며, Kalman 필터에 의해 운동 모델을 통해 추론될 것입니다. 이와 같은 관측되지 않는 변수를 숨겨진 또는 잠재 변수라고 부르는 경우가 종종 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 아래에 정의된 자코비안 행렬입니다. 관측값에서 상태 변수로의 직접적인 매핑이 있으므로, 자코비안에서 관측값(행)이 상태 변수(열)에 해당될 때는 1이 있습니다. 자코비안에서 상태 변수 θ에 해당하는 3열에는 두 개의 1이 있음을 주목해 주세요 — odometry에서 θ에 대한 행과 IMU에서 θ에 대한 행이 각각 하나씩 있습니다. 또한, 선형 속도에 해당하는 4열은 직접 관측하지 않기 때문에 모두 0입니다.\n\n구현된 모션 및 관측 모델은 아래에 나와 있습니다. 필터의 구현은 이전 것과 매우 유사하므로 간결함을 위해 생략하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef acceleration_motion_model_linearized_1():\n\n def state_transition_function_g(mu = None, u = None, delta_t = None):\n  \n  x, y, theta, v, w, a_x, a_y = mu\n\n  v = u[0]      \n  w = u[1] \n  \n  g = np.array([\n   x + v * np.cos(theta) * delta_t + 0.5 * a_x * delta_t**2,      \n      y + v * np.sin(theta) * delta_t + 0.5 * a_y * delta_t**2,    \n      theta + w * delta_t,\n      v + a_x * np.cos(theta) * delta_t + a_y * np.sin(theta) * delta_t,\n      w,                                                      \n      a_x,                                                              \n      a_y\n  ])\n\n  return g\n\n def jacobian_of_g_wrt_state_G(mu = None, u = None, delta_t = None):\n  x, y, theta, v, w, a_x, a_y = mu\n\n  v = u[0]       \n  w = u[1]       \n\n  G = np.array([[1.0, 0.0, -delta_t * v * np.sin(theta), delta_t  * np.cos(theta), 0.0, 0.5*delta_t**2, 0.0],   \n                   [0.0, 1.0, delta_t * v * np.cos(theta), delta_t * np.sin(theta), 0.0, 0.0, 0.5*delta_t**2],       \n                   [0.0, 0.0, 1.0, 0.0, delta_t, 0.0, 0.0],                                      \n                   [0.0, 0.0, -delta_t * a_x * np.sin(theta) + delta_t * a_y * np.cos(theta), \n                   1.0, 0.0, delta_t * np.cos(theta), delta_t * np.sin(theta)],                  \n                   [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],                                          \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],                                          \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])                                         \n  \n  return G\n\n return state_transition_function_g, jacobian_of_g_wrt_state_G\n```\n\n```js\ndef odometry_imu_observation_model_with_acceleration_motion_model_linearized_1():\n def observation_function_h(mu):\n  x, y, theta, v, w, ax, ay = mu\n  return np.array([[x], [y], [theta], [theta], [w], [ax], [ay]]\n \n def jacobian_of_h_wrt_state_H():\n  return np.array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],    \n                    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],         \n                    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],         \n                    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],        \n                    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],         \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],         \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n\n return observation_function_h, jacobian_of_h_wrt_state_H\n```\n\n성능:\n\n더 정교한 모션 모델을 사용하고 오도메트리 데이터와 IMU 데이터를 퓨즈하는 개선된 EKF를 평가하는 시간입니다. 특히, 노이즈 매개변수를 다음과 같이 설정하겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nproc_noise_std = [0.1, 0.1, 0.05, 0.1, 0.1, 0.1, 0.1] # [x, y, theta, v, w, a_x, a_y]\nobs_noise_std = [100.0, 100.0, 1000.0, 6.853891945200942e-06, 1.0966227112321507e-06, 0.0015387262937311438, 0.0015387262937311438] #[x, y, theta, theta_imu, w, a_x, a_y]\n```\n\n이 설정은 우리의 모션 모델에 대해 높은 신뢰를 가지고 있지만 관측 모델에 대해서는 IMU 데이터를 오도메트리 데이터보다 더 신뢰한다고 필터에 알려줍니다. 특히, 우리는 필터에게 오도메트리에서의 헤딩이 미친 것이라고 생각하고 IMU에서의 헤딩이 매우 정확하다고 말하고 있습니다. 이는 IMU가 매우 정밀할 수 있고 오도메트리 데이터가 악명 높게 나쁠 수 있기 때문에 종종 사실입니다. 이 설정은 우리가 예측한 궤적을 수정하는 데 도움이 될 수 있는데, 이전에 본 것처럼 모양은 그리 나쁘지 않지만 방향은 매우 잘못된 경우가 많습니다.\n\n아래 비디오와 이어지는 두 그림에서 볼 수 있듯이 가속도 모델과 센서 퓨전이 포함된 새 필터는 이전 필터보다 훨씬 더 잘 수행됩니다. 특히 IMU로부터 제공된 더 나은 방향성 덕분에 추정 궤적이 초기에 실제 궤적에 훨씬 가까웠음을 볼 수 있습니다. 그러나 마지막에는 지그재그 패턴을 따르기 시작했습니다.\n\n더 나은 결과를 얻기 위한 레시피는 없습니다. 이것이 필터를 설계하는 것을 과학보다는 예술로 만드는 것입니다. 위의 지그재그 궤적을 개선할 수 있는 더 나은 모션 모델로 수정할 수 있을 것이라고 생각할 수 있습니다. 이 결과를 본 것처럼 나도 같은 방식으로 느꼈습니다. 모션 모델을 개선해 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 모션 모델 개선하기\n\n센서 퓨전은 도움이 되고 있지만, 아직 개선할 부분이 많이 남아 있습니다. 상태 추정을 위한 필터를 설계하려면 매우 교육된 추측을 하고 직관을 따라야 합니다. 앞서 논의한 대로, 최근 얻은 결과는 격려적이며 우리가 모델을 개선해 보아야 한다는 제안이 있습니다. 특히 상태를 모델링할 때 글로벌 선속도만을 고려하고, 가속도와 같이 다른 축을 따른 속도를 고려하지 않는 것으로 보입니다. 우리의 상태 벡터를 확장하여 이를 고려해보겠습니다.\n\n![이미지](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_37.png)\n\n새로운 상태 벡터에는 로봇의 움직임에 대한 더 자세한 정보를 제공할 x와 y 성분의 속도가 포함되어 있습니다. 이 확장이 필터의 성능에 어떤 영향을 미치는지 알아보겠습니다. g 함수와 해당 야코비안 G을 이용하여 표현된 확장된 모션 모델은 아래와 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_38.png)\n\n![Image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_39.png)\n\nThe observation model function h remains the same, but the Jacobian H now has two columns filled with zeros corresponding to the unobserved state variables v_x and v_y.\n\n![Image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_40.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델과 사용된 잡음 매개변수에 대한 코드는 다음과 같습니다. 여기서 추가적으로 필터 코드는 이전에 소개된 코드와 매우 유사하므로 생략될 것입니다.\n\n```js\ndef acceleration_motion_model_linearized_2():\n\n def state_transition_function_g(mu = None, u = None, delta_t = None):\n  \n  x, y, theta, v_x, v_y , w, a_x, a_y = mu\n\n  v = u[0]       \n  w = u[1]       \n  \n  g = np.array([\n      x + v * np.cos(theta) * delta_t + 0.5 * a_x * delta_t**2,  \n      y + v * np.sin(theta) * delta_t + 0.5 * a_y * delta_t**2,  \n      theta + w * delta_t,                                   \n      v * np.cos(theta) + a_x * delta_t,                         \n      v * np.sin(theta) + a_y * delta_t,                         \n      w,                                                         \n      a_x,                                                       \n      a_y                                                        \n  ])\n\n  return g\n\n def jacobian_of_g_wrt_state_G(mu = None, u = None, delta_t = None):\n  x, y, theta, v_x, v_y, w, a_x, a_y = mu\n\n  v = u[0]       \n  w = u[1]       \n\n  G = np.array([[1.0, 0.0, -delta_t * v * np.sin(theta), 0.0, 0.0, 0.0, 0.5*delta_t**2, 0.0],   \n                   [0.0, 1.0, delta_t * v * np.cos(theta), 0.0, 0.0, 0.0, 0.0, 0.5*delta_t**2],        \n                   [0.0, 0.0, 1.0, 0.0, 0.0, delta_t, 0.0, 0.0],                                      \n                   [0.0, 0.0, -v * np.sin(theta), 0.0, 0.0, 0.0, delta_t, 0.0],                       \n                   [0.0, 0.0, v * np.cos(theta), 0.0, 0.0, 0.0, 0.0, delta_t],                        \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],                                          \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],                                          \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])                                         \n  \n  return G\n \n return state_transition_function_g, jacobian_of_g_wrt_state_G\n```\n\n```js\ndef odometry_imu_observation_model_with_acceleration_motion_model_linearized_2():\n def observation_function_h(mu):\n  x, y, theta, v_x, v_y, w, ax, ay = mu\n  return np.array([[x], [y], [theta], [theta], [w], [ax], [ay]]\n \n def jacobian_of_h_wrt_state_H():\n  return np.array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],       \n                    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],         \n                    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],         \n                    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],            \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],         \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],         \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n \n return observation_function_h, jacobian_of_h_wrt_state_H\n```\n\n```js\nproc_noise_std = [0.1, 0.1, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1] # [x, y, theta, v_x, v_y, w, a_x, a_y]\nobs_noise_std = [100.0, 100.0, 1000.0, 6.853891945200942e-06, 1.0966227112321507e-06, 0.0015387262937311438, 0.0015387262937311438] #[x, y, theta, theta_imu, w, a_x, a_y]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n성능:\n\n새로운 필터는 이전 동영상과 아래 그림에서 명백히 확인할 수 있듯이 인상적인 성과를 보여주었습니다. 예측된 궤적이 이제 센서 퓨전과 더 정교한 모델의 덕분에 실제 지면에 매우 가까워졌습니다. 게다가 그림의 최신 자세에서 타원으로 표시된 최종 불확실성은 이전 필터 버전보다 훨씬 작습니다. 이는 센서 퓨전이 잘 설계된 EKF의 효과를 보여줍니다. 추가로 소음 매개변수를 조정하면 더 많은 개선이 가능하지만, 이는 기사의 길이 때문에 여기서 탐구되지 않을 것입니다.\n\n## 직접 시도해보세요\n\n이전 기사와 마찬가지로, 코드는 전체 코드를 검사하거나 단순히 알고리즘을 실행하고 결과를 실시간으로 확인하려는 사람들을 위해 제공됩니다. 코드를 실행하려면 아래 지시사항을 따르십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저 필요한 것은 ROS 2입니다. ROS 2 Humble을 Ubuntu Jammy Jellyfish (22.04)에 설치하는 방법은 여기에서 찾을 수 있습니다. Ubuntu의 다른 버전이나 다른 운영 체제의 경우 공식 ROS 2 문서를 참고하십시오.\n\n데이터를 얻으려면 이 링크에서 ROS 2 가방을 다운로드해야 합니다. 사용하기 전에 파일을 압축 해제해야 합니다.\n\n마지막으로 ROS 2 패키지를 복제하고 빌드해야 합니다. 아래 단계를 따라 진행할 수 있습니다. ros2_ws를 실제 ROS 2 작업 공간으로 교체해야 합니다.\n\n\n# 종속성 설치\nsudo apt install python3-pykdl\n\n# 패키지를 복제하고 빌드\ncd ros2_ws/src\ngit clone https://github.com/carlos-argueta/rse_prob_robotics.git\ncd ..\ncolcon build --symlink-install\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n확장 칼만 필터를 실행하려면 3개의 다른 터미널을 열어야 합니다.\n\n터미널 1에서 (ros2_ws를 실제 워크스페이스로 교체해주세요) 다음 명령을 실행하여 Rviz를 열고 로봇이 보고 있는 것을 확인하세요.\n\n```js\nsource ~/ros2_ws/install/setup.bash\nros2 launch rse_gaussian_filters rviz_launch.launch.py\n```\n\n터미널 2에서 확장 칼만 필터의 버전에 따라 다음 명령 중 하나를 실행하세요. 먼저 아무 출력도 나오지 않을 것이며, ROS 2 가방(bag)을 실행할 때까지 기다리세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsource ~/ros2_ws/install/setup.bash\n\n# 아래 명령어 중 하나 실행\n\n# 3D 상태, 기본 속도 모델\nros2 run rse_gaussian_filters ekf_estimation_3d_v1 \n\n# 3D 상태, 고급 속도 모델\nros2 run rse_gaussian_filters ekf_estimation_3d_v2 \n\n# 7D 상태, 가속 모델, 센서 퓨전\nros2 run rse_gaussian_filters ekf_estimation_7d \n\n# 8D 상태, 가속 모델, 센서 퓨전\nros2 run rse_gaussian_filters ekf_estimation_8d \n```\n\n터미널 3에서 ROS 2 가방이 추출된 위치로 이동하여 다음 명령어로 재생하십시오. \"Ignoring a topic '/navrelposned', reason: package 'ublox_msgs' not found\"와 같은 경고 메시지를 무시해도 됩니다.\n\n```js\nros2 bag play linkou-2023-12-27-2-med --clock\n```\n\n위 단계를 따르면 확장 칼만 필터를 실행하고 결과를 실시간으로 확인할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 실용적인 고려사항 및 마지막으로\n\n이 글은 확장 칼만 필터 (EKF)를 기본적인 선형 칼만 필터 (LKF)의 확장으로 소개했습니다. EKF는 대부분의 실세계 시스템의 비선형성을 근사화를 통해 다룹니다. 이는 사후 평균 주변의 비선형 함수를 일차 테일러 전개를 이용하여 선형화합니다. 선형화된 후, EKF는 LKF와 유사하게 작동합니다.\n\n보여진 대로, 센서 퓨전과 결합된 EKF는 놀라운 결과를 얻을 수 있습니다. 그러나 좋은 필터를 설계하는 것은 어렵고 오류를 범하기 쉽습니다. 종종 과학과 예술 사이의 균형을 요구합니다. 적합한 움직임 및 관측 모델을 찾는 것이 첫 번째 난관입니다. 이동 로봇에 대한 사용 가능한 모델이 있더라도, 다른 시나리오는 좋은 모델이 부족할 수 있습니다. 자코비안 계산 또한 어렵고 실수하기 쉬우며, 소프트웨어 확인이 필요함을 강조합니다.\n\n효과적인 EKF 설계를 위해 여러 다른 고려사항과 결정이 중요합니다. 한 가지 중요한 측면은 올바른 시간 간격(delta_t)을 선택하는 것으로, 효과적인 선형화를 위해 충분히 작아야 합니다. 이 경우, 고정값 대신 동적 delta_t가 사용되었습니다. 필터의 업데이트 단계를 수행할 때 언제, 어떻게 결정하는지도 중요합니다. 특히 서로 다른 속도로 측정 값을 제공하는 다른 센서가 있을 때 (예: IMU는 보통 오도메트리보다 높은 주파수로), 적절한 노이즈 매개 변수 선택도 여러 가지 방법이 가능한 복잡한 작업입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요약하자면, 확장 칼만 필터는 특히 센서 퓨전과 함께 사용될 때 우수한 상태 추정 결과를 제공할 수 있습니다. 그러나 견고한 EKF를 설계하려면 상당한 노력과 연습이 필요합니다. 다음 글에서는 Unscented Kalman Filter (UKF)를 소개할 예정입니다. UKF는 EKF보다 여러 장점을 제공합니다. Unscented Transform을 사용하여 비선형 변환의 평균과 공분산을 더 정확하게 캡처합니다. UKF는 Jacobian을 필요로하지 않아 구현을 간단하게 만들어줍니다. 선형화로 인한 근사 오차를 줄이므로 매우 비선형 시스템에 더 효과적입니다. 또한 UKF는 비가우시안 분포를 더 잘 처리하여 견고성과 다양성을 향상시킵니다.\n\n# 독후감\n\n다음은 칼만 필터 패밀리에 대해 학습하기 위해 참고한 훌륭한 자료 목록입니다:\n\n- Optimal State Estimation: Kalman, H∞, and Nonlinear Approaches\n- State Estimation for Robotics\n- Kalman and Bayesian Filters in Python\n- Probabilistic Robotics\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사가 도움이 되셨기를 바랍니다. 피드백이 있으시면 언제든지 댓글을 남겨주세요. 또한, 이후의 주제를 다룬 보다 심도 있는 강좌 시리즈를 시작하려고 합니다. 이 강좌는 비디오, 코딩 프로젝트 등이 포함될 예정이며 유료로 운영될 가능성이 높습니다. 이런 강좌에 관심이 있다면 댓글로 알려주시면 참여 의향을 파악할 수 있습니다.\n\n저와 소통하고 싶다면 LinkedIn에서 저를 찾아보세요: https://www.linkedin.com/in/carlos-argueta\n\n저와 함께 ROS 2를 이용한 로보틱스를 배우고 싶으신가요? 제 라이브 강의에 참여해보세요!","ogImage":{"url":"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png"},"coverImage":"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png","tag":["Tech"],"readingTime":31},{"title":"ROS 2 Python 런치 파일의 비밀을 해제합니다","description":"","date":"2024-06-19 06:19","slug":"2024-06-19-UnlockingtheSecretsofROS2PythonLaunchFiles","content":"\n\n![2024-06-19-UnlockingtheSecretsofROS2PythonLaunchFiles_0.png](/assets/img/2024-06-19-UnlockingtheSecretsofROS2PythonLaunchFiles_0.png)\n\n안녕하세요! ROS 2는 로봇 소프트웨어를 간단하게 만들어주는 강력한 프레임워크와 도구입니다. cartographer_ros, navigation2, ros2_control 등 다양한 유용한 패키지들을 통해 로봇 엔지니어들은 로봇을 자율적으로 움직이도록 빠르게 구현할 수 있습니다.\n\n로봇 소프트웨어를 배우는 초기 단계에서 개발자들은 종종 런치 파일을 작성하는 데 많은 시간을 투자합니다. 이 파일들은 준비된 패키지의 프로세스(Ros 2 노드)를 시작하는 데 사용됩니다. 처음에는 모바일 앱과 같이 로직을 많이 작성하지 않는다는 점이 조금 이상하게 느껴졌습니다. 로봇을 위한 소프트웨어를 작성하는 개념을 이해하는 데 시간이 걸렸죠.\n\n특히, Python 런치 파일을 작성하는 것은 일반적인 Python 프로그래밍과는 다르기 때문에 조금 까다로울 수 있습니다. 그래서 이 글을 쓰게 되었습니다. 함께 런치 파일에 혼동을 겪고 계신 분들을 위해 몇 가지 팁을 공유하고 싶기 때문이죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## DeclareLaunchArgument 및 LaunchConfiguration\n\nTurtleBot3 로봇 런치 파일에서 추출된 DeclareLaunchArgument 및 LaunchConfiguration 예제를 볼 수 있습니다:\n\n```js\nuse_sim_time = LaunchConfiguration('use_sim_time', default='false')\n…\nDeclareLaunchArgument(\n    'use_sim_time',\n    default_value=use_sim_time,\n    description='true이면 시뮬레이션 (Gazebo) 시계 사용'\n)\n```\n\n초보자로서, 두 요소 간의 관계를 종종 잊어버리곤 합니다. 또한 LaunchConfiguration과 DeclareLaunchArgument 생성자는 default 또는 default_value 매개변수를 지원하며, 이는 매우 혼란스러울 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 정의는 ROS 2의 공식 문서에서 발췌되었습니다. **굵게 표시한** 단어에 주목해 주시면 단어의 사용법을 이해하는 데 도움이 됩니다.\n\n여러 번 읽은 후에 그것들은 쌍으로 사용되어야 한다는 것을 깨달았어요. 항상 함께 작성하는 것이 좋은 실천 방법이라고 생각해요. Mini Pupper bringup 런치 파일의 예시에서 보여주는 것처럼요:\n\n```js\nuse_sim_time = LaunchConfiguration('use_sim_time')\nuse_sim_time_launch_arg = DeclareLaunchArgument(\n    name='use_sim_time',\n    default_value='False',\n    description='Use simulation (Gazebo) clock if true'\n)\n\nhardware_connected = LaunchConfiguration(\"hardware_connected\")\nhardware_connected_launch_arg = DeclareLaunchArgument(\n    name='hardware_connected',\n    default_value='True',\n    description='Set to true if connected to a physical robot'\n)\n```\n\n이렇게 함께 묶으면 코드가 깔끔하고 명확하게 보이죠. LaunchConfiguration 생성자는 실제로 기본 매개변수를 수용하지만, 사용을 권하지 않습니다. 우리는 LaunchConfiguration에서는 설정된 기본 값을 사용하지 말아야 하며, DeclareLaunchArgument에서만 기본 값을 설정해야 해요. LaunchConfiguration은 런치 파일 내에서 값을 사용하기 위해 런치 인자의 값을 획득하는 데 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 콘솔에 대체값 출력하는 방법\n\n소프트웨어 엔지니어로서, 코드를 디버깅하는 일이 종종 필요합니다. 지금까지 실행 중에 런치 파일에서 중단점을 설정하고 변수 값들을 살펴볼 수 있는 IDE는 없다고 알고 있습니다. 디버깅 방법 중 하나는 콘솔에 일부 정보를 로깅(출력)하는 것입니다.\n\nPython의 print 함수를 사용하면 문자열과 같은 원래 변수를 쉽게 출력할 수 있습니다. 그러나 ROS 2 런치 파일에서 Substitution이라는 특수 유형의 변수를 처리하는 경우가 많습니다. 아래는 ROS 2 공식 문서에서 복사한 정의입니다.\n\nLaunchConfiguration, PathJoinSubstitution 또는 PythonExpression과 같은 종류의 Substitution을 로깅하려면 launch.actions.LogInfo를 사용해야 합니다. rviz2를 위한 런치 파일 예시에서 변수 rviz_config_path의 정확한 값을 알고 싶은 경우를 살펴봅니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport launch\nfrom launch.substitutions import PathJoinSubstitution\nfrom launch_ros.substitutions import FindPackageShare\nfrom launch_ros.actions import Node\nfrom launch.actions import LogInfo\n\ndef generate_launch_description():\n    description_package = FindPackageShare('mini_pupper_description')\n\n    rviz_config_path = PathJoinSubstitution(\n        [description_package, 'rviz', 'urdf_viewer.rviz']\n    )\n\n    a = \"Mini Pupper\"\n    b = \"has 4 legs\"\n    sentence = a + \" \" + b\n    print(sentence)\n\n    return launch.LaunchDescription([\n       Node(\n            package=\"rviz2\",\n            namespace=\"\",\n            executable=\"rviz2\",\n            name=\"rviz2\",\n            arguments=[\"-d\", rviz_config_path]\n        ),\n        LogInfo(msg='rviz_config_path is'),\n        LogInfo(msg=rviz_config_path)\n    ])\n```\n\nLogInfo를 사용하면 콘솔 출력이 아래와 같이 됩니다. \"urdf_viewer.rviz\" 파일의 정확한 위치를 명확하게 확인할 수 있습니다. 참고로, 일반적인 Python 문자열 유형인 \"sentence\" 변수는 Python의 print 함수를 사용하여 쉽게 인쇄되었습니다.\n\n```js\ncullensun@ubuntu:~/ros2_ws$ ros2 launch mini_pupper_bringup rviz.launch.py\n[INFO] [launch]: All log files can be found below /home/cullensun/.ros/log/2024-06-18-00-34-07-704969-ubuntu-7389\n[INFO] [launch]: Default logging verbosity is set to INFO\nMini Pupper has 4 legs\n[INFO] [launch.user]: rviz_config_path is\n[INFO] [launch.user]: /home/cullensun/ros2_ws/install/mini_pupper_description/share/mini_pupper_description/rviz/urdf_viewer.rviz\n[INFO] [rviz2-1]: process started with pid [7390]\n```\n\n## 결론\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사는 ROS 2 Python 런치 파일 작성의 까다로운 측면 중 일부를 탐색했는데, 특히 DeclareLaunchArgument와 LaunchConfiguration에 초점을 맞추고, 디버깅 목적으로 콘솔에 대체 값을 인쇄하는 방법을 다뤘습니다. 이러한 개념을 이해하는 것은 서로 다른 시나리오에 쉽게 적용할 수 있는 견고하고 유연한 런치 파일을 작성하는 데 중요합니다.\n\nDeclareLaunchArgument와 LaunchConfiguration 쌍을 사용하면 명확하고 조직적인 런치 파일을 보장할 수 있습니다. 또한 launch.actions.LogInfo를 활용하면 대체 값을 콘솔에 인쇄하여 디버깅을 지원하고 런치 파일의 동작을 이해하는 데 도움을 줍니다.\n\nROS 2와 함께하는 여정을 계속하면서, 런치 파일은 로봇 응용 프로그램을 관리하는 강력한 도구임을 기억해주세요. 이러한 기술을 숙달함으로써 더 효율적이고 유지 보수가 용이한 런치 파일을 작성하여 개발 프로세스를 보다 원활하고 즐거운 경험으로 만들 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-UnlockingtheSecretsofROS2PythonLaunchFiles_0.png"},"coverImage":"/assets/img/2024-06-19-UnlockingtheSecretsofROS2PythonLaunchFiles_0.png","tag":["Tech"],"readingTime":5},{"title":"라즈베리 파이에 Kali 설정하기 파트 3, VNC 서버 및 클라이언트 설정하기","description":"","date":"2024-06-19 06:18","slug":"2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient","content":"\n\nx11vnc 서버와 RealVNC 뷰어 클라이언트를 사용했어요.\nKali에 VNC 서버를 설치하려면 다음 명령어를 실행하세요,\n\n```js\nsudo apt install x11vnc\n```\n\n다음으로, VNC 서버를 위한 암호를 생성하고 다음 명령어를 실행하여 /etc/vncserver.pass 파일에 저장할 거에요,\n\n```js\nsudo x11vnc -storepasswd\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient_0.png\" /\u003e\n\n만일 VNC 서버를 systemd 서비스로 실행하고 관리하려면, /etc/systemd/system/ 디렉토리 아래에서 해당 서비스를 정의할 수 있습니다. 이를 통해 Linux 운영 체제를 위한 시스템 및 서비스 관리자 인 systemd를 사용하여 VNC 서버를 쉽게 관리하고 구성할 수 있습니다.\n다음 명령을 실행하여 서비스를 만듭니다.\n\n```js\nsudo nano /etc/systemd/system/vncserver.service\n```\n\n아래 스크립트를 vncserver.service 파일에 붙여넣으세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n[Unit]\nDescription=start vnc at boot\nAfter=multi-user.target\n\n[Service]\nType=simple\nExecStart=/usr/bin/x11vnc -display :0 -auth guess -forever -loop -noxdamage -repeat -rfbauth /etc/vncserver.pass -rfbport 5900 -shared\n\n[Install]\nWantedBy=multi-user.target\n\n\n“After=multi-user.target”는 서비스가 활성화된 모든 서비스 이후에 실행되도록합니다. 이것은 시스템이 비그래픽 다중 사용자 세션을 수용할 준비가 된 상태를 나타냅니다. \n\n/boot/config.txt 파일에서 다음 플래그를 주석 처리 해제하세요.\n\n\nframebuffer_width=1280\nframebuffer_height=720\nhdmi_force_hotplug=1\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nframebuffer_width과 framebuffer_height는 디스플레이 해상도를 조절하는 데 사용됩니다.\nhdmi_force_hotplug=1은 HDMI 모니터가 감지되지 않아도 HDMI 모드를 활성화하는 데 사용됩니다.\n\n다음 명령을 실행하여 VNC 서버를 시작하세요.\n\n```js\nsudo systemctl enable vncserver\nsudo systemctl start vncserver\n```\n\n다음 명령으로 VNC 서버 상태를 확인하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsystemctl status vncserver\n```\n\n현재 VNC 서버가 실행 중이며 기본적으로 포트 5900을 사용하고 있습니다.\n\n이제 VNC 서버가 가동 중이므로 클라이언트를 준비해봅시다.\n여러 가지 클라이언트가 있지만, 저는 RealVNC를 사용하고 있습니다. 다음 링크에서 다운로드할 수 있습니다: [https://www.realvnc.com/en/connect/download/viewer/](https://www.realvnc.com/en/connect/download/viewer/)\n\n설치가 완료되면 \"File\"을 클릭한 후 \"New Connection\"을 클릭하여 새로운 VNC 연결을 만들어보세요. 다음과 같은 창이 표시될 것입니다,\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![](/assets/img/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient_1.png)\n\nVNC 서버 IP 주소 또는 호스트명을 입력해주세요. 원하는대로 다른 설정도 사용자 정의할 수 있어요.\n\"확인\"을 클릭하면 연결이 주소록에 나열되어 있을 거에요.\n연결을 두 번 클릭하면 연결이 암호화되지 않았다는 대화 상자가 나타날 거에요.\n\n![](/assets/img/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient_2.png)\n\n나중에 VNC 서버와의 암호화된 연결을 수립하기 위해 SSH 터널을 설정하겠어요. 그 전에 VNC 연결이 성공적으로 설정되었는지 테스트해볼게요. \"계속\"을 클릭하고, 이전에 구성한 VNC 서버 암호를 제공해주세요. 이제 그래픽 사용자 인터페이스(GUI)를 통해 Kali 상자에 액세스할 수 있어야 해요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 VNC 뷰어 창에서 검은 화면이 나타나면, 라즈베리 파이의 해상도 설정을 조정해야 할 수도 있습니다. Kali 상자에 미리 설치된 kalipi-config을 사용하여 라즈베리 파이의 디스플레이 설정과 VNC 뷰어의 설정을 호환되도록 조정할 수 있습니다.\n디스플레이 설정을 구성하려면 다음을 실행하세요:\n\n```bash\nsudo kalipi-config\n```\n\n그러면 다음과 같은 대화 상자가 표시됩니다,\n\n![dialog box](/assets/img/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"“고급 옵션” 섹션으로 이동하여 해상도 설정을 귀하의 요구에 맞게 구성하세요.\n구성을 수정한 후 시스템을 재부팅하겠느냐는 질문이 표시될 것입니다. \"예\"를 선택하세요.\n\nKali가 재부팅될 때까지 기다리세요. 그런 다음 SSH를 통해 Kali에 연결하고 VNC 서버 상태를 확인하여 장치와 함께 부팅되었는지 확인하세요. RealVNC에서 Kali 상자에 연결을 시도하면 이제 GUI 인터페이스를 볼 수 있어야 합니다.\n\n이제 GUI를 통해 원격으로 Kali 상자를 관리할 수 있습니다.\n\nVNC 트래픽은 기본적으로 암호화되어 있지 않습니다. 데이터의 개인 정보보호와 안전을 보장하기 위해 VNC를 사용할 때 SSH 터널을 설정하는 것이 좋습니다. VNC 연결을 보호하기 위해 SSH 터널을 설정하는 방법은 다음 시리즈에서 확인할 수 있습니다.\"","ogImage":{"url":"/assets/img/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient_0.png"},"coverImage":"/assets/img/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient_0.png","tag":["Tech"],"readingTime":4}],"page":"85","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"85"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>