<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/85" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/85" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="로스 2에서 확장 칼만 필터를 활용한 센서 융합" href="/post/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="로스 2에서 확장 칼만 필터를 활용한 센서 융합" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="로스 2에서 확장 칼만 필터를 활용한 센서 융합" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">로스 2에서 확장 칼만 필터를 활용한 센서 융합</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">31<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ROS 2 Python 런치 파일의 비밀을 해제합니다" href="/post/2024-06-19-UnlockingtheSecretsofROS2PythonLaunchFiles"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ROS 2 Python 런치 파일의 비밀을 해제합니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-UnlockingtheSecretsofROS2PythonLaunchFiles_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ROS 2 Python 런치 파일의 비밀을 해제합니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">ROS 2 Python 런치 파일의 비밀을 해제합니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이에 Kali 설정하기 파트 3, VNC 서버 및 클라이언트 설정하기" href="/post/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이에 Kali 설정하기 파트 3, VNC 서버 및 클라이언트 설정하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이에 Kali 설정하기 파트 3, VNC 서버 및 클라이언트 설정하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">라즈베리 파이에 Kali 설정하기 파트 3, VNC 서버 및 클라이언트 설정하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="DIY 홈 서버 히어로 미디어 및 저장 용 Raspberry Pi 5  CasaOS" href="/post/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="DIY 홈 서버 히어로 미디어 및 저장 용 Raspberry Pi 5  CasaOS" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="DIY 홈 서버 히어로 미디어 및 저장 용 Raspberry Pi 5  CasaOS" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">DIY 홈 서버 히어로 미디어 및 저장 용 Raspberry Pi 5  CasaOS</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="실시간 얼굴 인식 끝에서 끝까지의 프로젝트" href="/post/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="실시간 얼굴 인식 끝에서 끝까지의 프로젝트" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="실시간 얼굴 인식 끝에서 끝까지의 프로젝트" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">실시간 얼굴 인식 끝에서 끝까지의 프로젝트</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">16<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="이전에 사용하던 라즈베리 파이를 디지털 포토 프레임으로 재활용하기" href="/post/2024-06-19-Up-cyclinganoldRaspberryPiintoadigitalphotoframe"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="이전에 사용하던 라즈베리 파이를 디지털 포토 프레임으로 재활용하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Up-cyclinganoldRaspberryPiintoadigitalphotoframe_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="이전에 사용하던 라즈베리 파이를 디지털 포토 프레임으로 재활용하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">이전에 사용하던 라즈베리 파이를 디지털 포토 프레임으로 재활용하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이 홀딩스 작지만 큰 영향력을 지닌 장치" href="/post/2024-06-19-RaspberryPiHoldingsASmallDevicewithaBigImpact"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이 홀딩스 작지만 큰 영향력을 지닌 장치" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-RaspberryPiHoldingsASmallDevicewithaBigImpact_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이 홀딩스 작지만 큰 영향력을 지닌 장치" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">라즈베리 파이 홀딩스 작지만 큰 영향력을 지닌 장치</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="물감을 만들어보세요" href="/post/2024-06-19-MakeYourOwnWatercolorPaint"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="물감을 만들어보세요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-MakeYourOwnWatercolorPaint_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="물감을 만들어보세요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">물감을 만들어보세요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="달콤한 증말 커뮤니티" href="/post/2024-06-19-DolceFavorCrate"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="달콤한 증말 커뮤니티" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-DolceFavorCrate_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="달콤한 증말 커뮤니티" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">달콤한 증말 커뮤니티</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Raspberry Pi 5 비디오 스트림 지연 시간 비교 UDP, TCP, RTSP, 그리고 WebRTC" href="/post/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Raspberry Pi 5 비디오 스트림 지연 시간 비교 UDP, TCP, RTSP, 그리고 WebRTC" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Raspberry Pi 5 비디오 스트림 지연 시간 비교 UDP, TCP, RTSP, 그리고 WebRTC" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Raspberry Pi 5 비디오 스트림 지연 시간 비교 UDP, TCP, RTSP, 그리고 WebRTC</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/81">81</a><a class="link" href="/posts/82">82</a><a class="link" href="/posts/83">83</a><a class="link" href="/posts/84">84</a><a class="link posts_-active__YVJEi" href="/posts/85">85</a><a class="link" href="/posts/86">86</a><a class="link" href="/posts/87">87</a><a class="link" href="/posts/88">88</a><a class="link" href="/posts/89">89</a><a class="link" href="/posts/90">90</a><a class="link" href="/posts/91">91</a><a class="link" href="/posts/92">92</a><a class="link" href="/posts/93">93</a><a class="link" href="/posts/94">94</a><a class="link" href="/posts/95">95</a><a class="link" href="/posts/96">96</a><a class="link" href="/posts/97">97</a><a class="link" href="/posts/98">98</a><a class="link" href="/posts/99">99</a><a class="link" href="/posts/100">100</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"로스 2에서 확장 칼만 필터를 활용한 센서 융합","description":"","date":"2024-06-19 06:22","slug":"2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2","content":"\n\n![Sensor Fusion with the Extended Kalman Filter in ROS2](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png)\n\n안녕하세요! 이 글은 가우시안 필터를 소개하는 시리즈 중 두 번째 글입니다. 구체적으로, 이 글은 칼만 필터 패밀리에 대한 세 번째 세부 소개입니다. 이미 칼만 필터에 대해 익숙하지 않다면, 계속하기 전에 첫 번째 글을 읽기를 권장합니다. 다음 글에서는 언센티드 칼만 필터를 소개할 예정입니다. 이 글의 결과를 재현하는 데 사용된 데이터와 코드는 이 글의 끝 부분에 찾을 수 있습니다.\n\n# 소개\n\n이전 글에서 소개된 대로, 성공적인 로봇 시스템은 유용한 작업을 수행하기 위해 물리적 세계를 인식하고 조작할 수 있어야 합니다. 이를 달성하기 위해 로봇은 환경의 중요한 불확실성을 고려해야 합니다. 현대 로봇 공학에서 가장 기본적인 문제 중 하나는 상태 추정입니다. 상태 추정은 로봇과 환경(랜드마크 및 기타 객체의 위치 등)의 가장 확률적인 상태(예: 위치, 방향, 속도)를 불확실한(잡음이 있는) 및 아마도 불완전한 정보를 기반으로 결정하는 것을 포함합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n칼만 필터는 불확실한 환경에서 상태 추정에 대응합니다. 필터는 상태의 각 요소 (예: x, y 좌표, 헤딩)을 가우시안 확률 변수로 모델링합니다. 가우시안은 상태 x에 대한 확률 밀도를 벡터 μ (뮤)와 공분산 행렬 Σ (시그마)만 사용하여 표현할 수 있게 합니다. 이 매개변수화에서 우리의 상태 x는 기대값 μ에 의해 표현되며, Σ는 제어, 이동 및 관측 잡음으로 인한 상태의 내재적 불확실성을 포착합니다.\n\n칼만 필터가 사용하는 베이지안 프레임워크에서는 전체 상태를 믿음(belief)이라고 합니다. 가우시안 (또는 정규분포)을 사용하는 장점은 그들의 수학적 성질에 있습니다. 이 성질은 칼만 필터 방정식을 단순화합니다. 가우시안 믿음이 선형 변환을 겪을 때의 특징 중요한데, 가우시안 믿음이 선형 변환을 겪으면 결과는 여전히 가우시안 확률 변수로 유지됩니다. 이 성질은 칼만 필터의 방정식이 우아하고 다루기 쉬운 상태를 유지하도록 보장합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_1.png)\n\n사후 믿음을 계산하기 위해 칼만 필터는 이전 믿음을 시간을 경과함에 따라 전달하는 모션 모델을 사용합니다. 그런 다음 관측 모델은 로봇 센서에서의 데이터를 통합하여 예측된 믿음을 업데이트하고 이를 사후로 변환합니다. 칼만 필터 알고리즘은 아래에서 요약됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Linearity of the Linear Kalman Filter](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_2.png)\n\n선형 칼만 필터의 \"선형성\"은 알고리즘의 2번째와 5번째 줄에서 가장 명백합니다. 2번째 줄에서 예측된 상태는 이전 상태 μ_t−1과 제어 입력 u_t의 선형 함수입니다. 5번째 줄에서 예측된 관측값 (y = Cμ)은 인수 μ^bar_t의 선형 함수입니다. 이 선형성은 가우시안 특성을 유지하여 필터를 구현하기 쉽게 만듭니다. 그러나 이것은 또한 주요 약점 중 하나를 나타냅니다.\n\n선형 칼만 필터가 실제 문제에 부적합한 이유는 실제 문제가 종종 선형적이지 않기 때문입니다. 이전 글에서 도입했던 간단한 상태 추정의 경우, 상태가 모바일 로봇의 2차원 포즈 (x, y, θ)로 표현되었지만 정확히 선형적이지 않았습니다. 아래에 다시 소개되는 이동 모델에서 확인할 수 있습니다.\n\n![Motion Model](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_4.png)\n\n모델이 쉽게 알아볼 수 있듯이, 삼각 함수가 사용되어 로봇의 좌표를 업데이트하는데 사용되며, 이러한 함수들은 선형이 아닙니다. 이 모델에서 선형 칼만 필터는 어느 순간 발산할 가능성이 있습니다. 이 이유로 선형 칼만 필터를 소개한 후, 대부분의 실제 현상의 비선형성을 고려하기 위해 확장 방법을 고안하는 작업이 즉시 시작되었습니다.\n\n# 확장 칼만 필터\n\n## 비선형성의 문제\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비선형성에 관한 문제를 더 자세히 설명하기 위해 다음 애니메이션을 살펴볼 수 있습니다. 첫 번째 애니메이션에서는 선형 칼만 필터의 가정이 성립하는 세계에 있으며, 새로운 상태가 인수에 대해 단순하게 선형인 경우를 살펴봅니다. 시각화를 이해하기 쉽게 하기 위해 1차원 상태 x를 가정합니다. 이 애니메이션은 가우스를 선형 함수 g를 통과시켰을 때 다른 가우스가 되는 과정을 보여줍니다. 이 경우 g = -0.5*x+1입니다.\n\nx의 가우시안 표현에서 시작하지만 비선형 함수 g를 선택하는 경우, 결과 확률 밀도 함수는 더 이상 가우시안이 아닙니다. 새로운 밀도를 계산하기 위한 폐쇄형 방법이 없습니다. 대신 입력 분포에서 점들을 샘플링하고 이를 g를 통과시켜 출력 히스토그램을 구축하여 출력 분포를 만들어야 합니다. 아래에 표시된 출력의 형태에서 확인할 수 있듯이, 이는 가우시안이 아닌 것을 알 수 있습니다. 또한 이 출력 분포는 칼만 필터의 단봉성 가정을 위배하며, 단일 피크를 요구합니다. P(y)의 가우시안 근사는 출력 데이터에 가우시안을 맞추어 얻었습니다. 이는 실제 모델의 비선형성을 다루기 위한 선형 칼만 필터의 한계를 강조합니다.\n\n요약하면 비선형 모델을 다룰 때 출력 밀도는 가우시안이 아니며 폐쇄형으로 계산할 수 없으며 종종 다중 피크를 갖기 때문에 칼만 필터 방정식이 무용지물이 됩니다. 이 문제를 해결하기 위해 확장 칼만 필터(EKF)는 선형성 가정을 버립니다. 대신 상태 전이 확률과 측정 확률은 비선형 함수 g 및 h에 따라 결정됨을 수용합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_5.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![그림](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_6.png)\n\n모든 것이 잘 진행되고 있어요. 칼만 필터의 우아한 방정식을 활용하려면 여전히 우리의 신뢰를 가우시안으로 표현해야 합니다. 이 필수성은 정확한 사후분포를 계산하는 것에서 EKF의 초점을 이동시켜 실제 신뢰의 좋은 가우시안 근사값을 찾게 됩니다. 아래 그림에서 보듯, 몬테칼로를 사용하여 출력 분포를 계산한 후, 그에 대한 가우시안을 fitting하고 필요한 매개변수 μ (뮤)와 Σ (시그마)를 얻을 수 있습니다. 그러나 아직도 가우시안을 닫힌 형태로 계산할 수 없는 문제가 남아 있습니다.\n\n## 테일러 전개를 통한 선형화\n\n이 문제를 해결하기 위해 확장 칼만 필터는 선형화라는 추가 근사값을 적용합니다. 선형화의 핵심 아이디어는 비선형 함수 g를 해당 관심점에서 g에 접하는 접선인 선형 함수로 근사하는 것입니다. 비선형 함수를 선형화하는 다양한 기술 중, EKF가 사용하는 것은 일차 테일러 전개입니다. 함수의 테일러 전개는 함수의 도함수를 하나의 점 a에서 표현된 다항식 항들의 무한 합입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 이미지에서 높은 차수의 테일러 전개가 점 a = 0 주변에서 g를 더 가까운 근사로 제공하는 것을 볼 수 있습니다. 그러나 고차 다항식이 늘어날수록 요구되는 계산도 증가하며, 문제가 빠르게 풀기 어려워집니다. 다행히도 Kalman 필터가 자주 업데이트되는 경우(작은 Δt), 관심점 a의 차이가 매우 작아야합니다. 따라서 우리는 다음 (매우 가까운) 각 지점 a에서의 함수 g의 값을 및 기울기(점 a에서의 미분)를 사용하여 함수 g의 선형 근사를 얻기 위해 1차 다항식(선)을 사용할 수 있습니다. 이 문제는 본질적으로 아래와 같이 간소화됩니다:\n\n\n![2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_9.png](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_9.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 동작이 왜 잘 작동하는지 설명하기 위해 g라는 함수를 가정해 볼게요.\n\n![그림](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_10.png)\n\n점 a에서 함수 g의 일차 테일러 전개는 아래 그림에서 빨간색으로 표시되어 있어요. 큰 x 값 범위를 관찰하면 좋은 근사치를 제공하지 않음이 명백해요.\n\n![그림](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_11.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 우리가 사후 분포의 값들에 대한 근사치에만 주의를 기울이기 때문에, 새로운 사후 분포에 대해 그러한 근사치를 매우 짧은 시간 이후에 다시 계산할 것을 알고 있기 때문에, 아래 그래프에서 볼 수 있듯이, 우리의 근사치가 관심 지점 주변에서 매우 좋다는 것을 알 수 있습니다. 이는 첫 번째 차수 테일러 전개가 우리의 목적에 대해 충분한 근사치를 제공하며, 시스템 내부의 비선형성에도 불구하고 확장 칼만 필터를 효과적으로 적용할 수 있도록 합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_12.png)\n\n마지막으로, 아래 이미지는 두 프로세스를 비교하려고 합니다: 첫 번째는 원래의 가우시안으로 시작하여 비선형 함수 g를 통과시키고, 몬테카를로를 사용하여 비가우시안 출력 분포를 얻은 후, 이 출력에 가우시안을 적합시킵니다. 두 번째로, EKF가 사용하는 프로세스는 g를 선형화시키고, 원래의 가우시안을 이 선형 근사치를 통해 통과시킨 후, 선형화를 통해 닫힌 형태로 출력을 직접 얻습니다. 이 비교는 비선형 시스템 다루기에 대한 EKF 접근 방식의 효율성과 실용성을 강조합니다.\n\n명확성을 위해, 아래에서는 출력만 표시됩니다. 보시다시피, EKF 가우시안이 몬테카를로 시뮬레이션에서 적합된 가우시안과 정확히 같지는 않지만, 충분히 가깝습니다. 이 작은 차이는 실제 분포의 닫힌 형태의 추정치를 효율적으로 얻기 위해 지불하는 대가입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서 설명한 간단한 예제는 스칼라 경우에 대한 것이었지만, 우리의 상태는 벡터입니다. 따라서 기울기를 구하기 위해 우리는 상태에 대한 g의 편미분을 계산합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_13.png)\n\ng와 그 기울기 g′의 값은 그들의 인자 (u_t와 x_t-1)에 따라 달라지는데요, 이는 우리의 관심 지점입니다 (스칼라 경우의 a와 대조적입니다). u_t의 값에 대해서는 로봇에 제공된 제어 명령을 사용합니다. x_t-1에 대해서는 선형화할 시기에서 가장 가능성이 높은 상태의 값으로 선택할 수 있습니다. 가우시안의 경우, 최대 가능성 값은 이전 시간 단계에서 계산된 사후값의 평균인 μ_t-1로 표시됩니다. 이 선형화는 업데이트 속도가 매우 빠른 필터 (매우 작은 Δt)에 대해 잘 작동하며, 이때 μ_t-1의 값과 우리가 추정하려는 현재 상태 간의 차이가 크지 않을 때 잘 작동합니다.\n\n이제 기울기를 계산했기 때문에 g를 다음과 같이 추정할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![sensor fusion](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_14.png)\n\n가우시안에서, 모션 모델 또는 상태 전이 확률은 아래와 같이 표기됩니다. 여기서 R_t는 보통의 프로세스 노이즈 공분산입니다.\n\n![motion model](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_15.png)\n\n기울기가 숫자인 스칼라 케이스와는 달리, g'(u_t, μ_t-1)으로 알려진 G_t는 행렬입니다. 비선형 함수 g에 대한 상태 x의 일차 편미분값을 모두 포함하는 이 행렬은 야코비안이라고 합니다. 이 야코비안 행렬은 상태의 차원인 n×n의 크기를 가지며, 현재 제어 및 이전 사후 평균에 따라 값이 달라집니다. 따라서 야코비안 값은 시간이 지남에 따라 변합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n확장 칼만 필터(Extended Kalman Filter)는 함수 h에 의해 표현되는 비선형 관측 모델을 다룹니다. 특히, 타일러 전개(Taylor Expansion)는 새롭게 예측된 믿음 μ^bar_t을 중심으로 진행되며, 이는 h를 선형화하는 시점에서 가장 가능성이 높은 상태입니다.\n\n![이미지](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_16.png)\n\n![이미지](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_17.png)\n\n가우시안으로, 측정 모델은 아래와 같이 표기됩니다. 여기서 Q_t는 전통적인 측정 잡음 공분산입니다. 여기서 야코비안 H_t는 관측 모델의 비선형 함수 h에 대한 상태 x에 대한 일차 편미분의 m×n 행렬입니다. 따라서 m은 관측의 차원이고, n은 상태의 차원입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_18.png\" /\u003e\n\n## Extended Kalman Filter Algorithm\n\n요약하면, 아래에 표시된 EKF 알고리즘은 이 기사의 앞부분에 표시된 LKF 알고리즘과 매우 유사하지만, 주요한 차이점은 모션 및 관측 모델의 선형화가 2번 줄과 5번 줄에서 이루어진다는 것입니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_19.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 이미지는 EKF 알고리즘을 LKF와 나란히 재진 다음 주요 차이점을 강조합니다. 예측 단계에서 EKF는 선형 시스템 행렬 A 및 B 대신 상태를 시간에 따라 진화시키는 비선형 함수 g를 사용합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter는 EKF보다 더 나은 정확도를 제공합니다. Unscented Kalman Filter�...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 모델 선형화\n\n선형 칼만 필터와 사용된 모션 모델은 정확히 선형은 아니지만 여전히 단순하여 LKF가 처리할 수 있는 간단한 상수 속도 모델이었습니다. 참고로, 아래에 다시 표시해 드립니다. 이 모델은 확장 칼만 필터의 능력을 보여주기 위해 사용될 더 복잡한 변형을 소개하는 기초 역할을 할 것입니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_21.png)\n\n이 방정식들은 다음과 같이 LKF에서 요구하는 선형 시스템 행렬로 변환되었습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식입니다.\n\n\n![sensorfusion1](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_22.png)\n\nEKF를 사용하여 동일한 모델을 사용하려면 선형화해야 합니다. 먼저로봇의 상태 형식을 재정의합니다.\n\n![sensorfusion2](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_23.png)\n\n이후 비선형 함수 g를 정의합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_24.png\" /\u003e\n\n다음으로, 우리는 각 상태 변수 x, y, 그리고 θ에 대한 함수 g의 편도함수를 포함하는 야코비안 행렬 G를 정의합니다. 이 행렬은 상태 변수의 변화가 운동 모델에 어떻게 영향을 미치는지를 포착합니다. 야코비안 행렬 G는 EKF 알고리즘에서 공분산 행렬을 업데이트하는 데 사용될 것이며, 우리에게 운동 모델의 비선형성들을 고려하면서도 칼만 필터 프레임워크의 계산 효율성을 유지할 수 있게 해줍니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_25.png\" /\u003e\n\nx 업데이트 방정식의 x, y, 그리고 θ에 대한 도함수를 나타내는 야코비안 행렬 G의 첫 번째 행을 살펴보면, 다음과 같은 것을 볼 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 첫 번째 항목은 x에 대한 부분 도함수이기 때문에 1입니다.\n- 두 번째 항목은 x가 y에 의존하지 않음을 나타내는 0입니다.\n- 세 번째 항목은 −vsin(θ)Δt이며, 이것은 x가 θ에 −vsin(θ)Δt항으로 의존함을 보여줍니다. 이것은 θ의 변화가 x 좌표에 미치는 영향을 반영합니다.\n\n다음으로 측정 함수 h가 필요합니다. 이전 Linear Kalman Filter의 구현과 유사하게, 사용할 측정은 로봇의 오도메트리 시스템에서 제공하는 것만 사용할 것입니다. 이 시스템은 바퀴 엔코더에서 계산된 로봇의 추정 자세를 직접 제공합니다. 그 데이터 형식이 우리의 상태 형식과 일치하기 때문에, h(μ^bar_t)는 단순히 μ^bar_t를 반환합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_26.png)\n\n그리고 상태에 대한 측정 함수의 부분 미분을 포함하는 Jacobian 행렬 H는 단위 행렬이며 아래와 같이 표시됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![sensor fusion](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_27.png)\n\n이전 그림에서 EKF와 LKF를 비교한 것처럼, 선형화가 두 알고리즘의 주요 차이점입니다. 관련 기능 및 해당 야코비안을 확인한 후, EKF의 구현은 LKF의 구현을 밀접하게 따릅니다. 아래는 선형화된 속도 모션 모델의 Python 구현입니다. 행렬 g와 야코비안 G를 모두 반환합니다.\n\n```python\ndef velocity_motion_model_linearized_1():\n\n def state_transition_function_g(mu = None, u = None, delta_t = None):\n  x = mu[0]\n  y = mu[1]\n  theta = mu[2]\n  \n  v = u[0]       \n  w = u[1]       \n  \n  g = np.array([\n            x + v * np.cos(theta) * delta_t,\n            y + v * np.sin(theta) * delta_t,\n            theta + w * delta_t\n        ])\n\n  return g\n\n def jacobian_of_g_wrt_state_G(mu = None, u = None, delta_t = None):\n  theta = mu[2]\n  v = u[0]       \n  w = u[1]       \n  \n  G = np.array([\n   [1, 0, -v * np.sin(theta) * delta_t],\n   [0, 1, v * np.cos(theta) * delta_t],\n   [0, 0, 1]\n  ])\n\n  return G\n\n return state_transition_function_g, jacobian_of_g_wrt_state_G\n```\n\n다음으로, 다음과 같이 간단한 관측 모델을 구현합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef odometry_observation_model_linearized():\n def observation_function_h(mu):\n  return mu\n \n def jacobian_of_h_wrt_state_H(mu):\n  return np.eye(3)\n\n return observation_function_h, jacobian_of_h_wrt_state_H\n```\n\n마지막으로, 아래에 구현된 Extended Kalman Filter 코드를 확인해보세요. 직전 게시물에서 소개된 Linear Kalman Filter 코드와 얼마나 비슷한지 주목하세요.\n\n```js\nimport numpy as np \n\nfrom rse_motion_models.velocity_motion_models import velocity_motion_model_linearized_1\nfrom rse_observation_models.odometry_observation_models import odometry_observation_model_linearized\n\nclass KalmanFilter:\n\n def __init__(self, initial_state, initial_covariance, proc_noise_std = [0.02, 0.02, 0.01], obs_noise_std = [0.02, 0.02, 0.01]):\n\n  self.mu = initial_state # 초기 상태 추정\n  self.Sigma = initial_covariance # 초기 불확실성\n\n  self.g, self.G = velocity_motion_model_linearized_1() # 사용할 액션 모델\n\n  # 과정 모델 노이즈의 표준 편차\n  self.proc_noise_std = np.array(proc_noise_std)\n  # 과정 노이즈 공분산 (R)\n  self.R = np.diag(self.proc_noise_std ** 2) \n\n  self.h, self.H = odometry_observation_model_linearized() # 사용할 관측 모델\n\n  # 관측 모델 노이즈의 표준 편차\n  self.obs_noise_std = np.array(obs_noise_std)\n  # 관측 노이즈 공분산 (Q)\n  self.Q = np.diag(self.obs_noise_std ** 2)\n\n def predict(self, u, dt):\n  # 상태 추정 (mu) 예측\n  self.mu = self.g(self.mu, u, dt)\n  # 공분산 (Sigma) 예측\n  self.Sigma = self.G(self.mu, u, dt) @ self.Sigma @ self.G(self.mu, u, dt).T + self.R\n\n  return self.mu, self.Sigma\n\n def update(self, z, dt):\n  # 칼만 이득 (K) 계산\n  K = self.Sigma @ self.H(self.mu).T @ np.linalg.inv(self.H(self.mu) @ self.Sigma @ self.H(self.mu).T + self.Q)\n  \n  # 상태 추정 (mu) 업데이트\n  self.mu = self.mu + K @ (z - self.h(self.mu))\n\n  # 공분산 (Sigma) 업데이트\n  I = np.eye(len(K)) \n  self.Sigma = (I - K @ self.H(self.mu)) @ self.Sigma\n\n  return self.mu, self.Sigma\n```\n\n성능:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nExtended Kalman Filter(EKF)의 성능을 평가하기 위해, 동일한 상수 속도 모델과 오도메트리 관측 모델을 사용하는 선형 칼만 필터(LKF)의 성능과 비교할 것입니다. 먼저, 초기 과정 노이즈를 크게 설정하고 관측 노이즈를 매우 낮게 설정할 것입니다. 이 설정은 사실상 필터에게 행동 모델보다는 관측을 신뢰하도록 지시합니다. 예상했듯이, 두 버전의 칼만 필터는 관측을 따라가며 비슷한 성능을 발휘합니다. 관측 모델의 간단함과 선형성을 고려하면 이 결과가 예상된 것입니다.\n\n다음 테스트는 더 어려울 것입니다. 과정 노이즈를 매우 낮게 설정하고 관측 노이즈를 매우 높게 설정할 것입니다. 이 설정은 필터가 대부분의 관측을 무시하고 상태 추정에 운동 모델에 크게 의존하게 만듭니다. 운동 방정식이 같더라도, EKF가 적용한 선형화 때문에 크게 다른 결과를 예상할 것입니다.\n\n예상대로, 차이는 상당합니다. 이러한 노이즈 설정 하에서 LKF는 성능이 저조합니다. 비선형 운동 모델에 완전히 의존할 때, LKF의 결과는 실제 값과 로봇 센서에서 보고된 오도메트리에서 크게 벗어납니다. 반면에, EKF는 관측 데이터에 더 가까이 머물며 훨씬 나은 성능을 발휘합니다. 또한 오른쪽 그림의 큰 타원에서 나타나는 바와 같이 EKF는 높은 불확실성을 정확히 나타냅니다. 반면에, LKF는 낮은 불확실성을보고하는데, 이것은 부정확합니다.\n\n## 대안 운동 모델\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n과거 두 필터에서 사용된 모션 모델은 꽤 간단하며 로봇이 헤딩 각도 θ(세타)의 방향으로 직선으로 이동한다고 가정합니다. 보다 정교한 모션 모델이 존재하며, 그 모델이 더 나은 성능을 발휘할 수 있는지 확인하는 것이 중요합니다. 다음에 탐구할 새로운 상수 속도 모션 모델은 로봇을 변환(직선) 및 회전(각도) 속도 v 및 ω를 통해 제어할 수 있도록 합니다. 직선 이동을 가정하는 대신, 이 모델은 아래에 나와 있는 것처럼 반지름이 r인 원 위를 로봇이 이동한다고 가정합니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_28.png)\n\n직선 모션 가정과 같이 원형 모션 가정도 근사값일 뿐이며, 시간 간격이 매우 작은 경우에만 유효합니다(움직임이 원이건 직선이건 구분할 수 없을 정도로 아주 작을 때). ω 값이 0에 가까워질수록, 반지름은 아주 크게되어 거의 직선 상에서의 움직임을 나타내게 됩니다. 시간에 따라 상태를 진화시키기 위해 이 모델을 따르는 비선형 함수 g에 해당하는 방정식 벡터는 아래에 나와 있습니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_29.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제이 모델을 선형화하기 위해 g의 자코비안을 계산해야 합니다. 이는 상태에 대한 g의 편도함수에 해당합니다. 우리의 상태는 다음과 같습니다:\n\n![State](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_30.png)\n\n따라서 자코비안 G는 다음과 같이 표현됩니다:\n\n![Jacobian](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_31.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서 보았듯이, 관련 행렬을 결정한 후에 구현은 간단합니다. 먼저, 아래 속도 모델 코드를 정의합니다.\n\n```js\ndef velocity_motion_model_linearized_2():\n\n def state_transition_function_g(mu = None, u = None, delta_t = None):\n  x = mu[0]\n  y = mu[1]\n  theta = mu[2]\n  \n  v = u[0]       \n  w = u[1]       \n  if w == 0:\n   w = 1e-6   # 직선 이동의 경우 0으로 나누는 것을 피하기 위해\n\n  g = np.array([\n     x + -v/w * np.sin(theta) + v/w * np.sin(theta + w * delta_t),\n     y + v/w * np.cos(theta) - v/w * np.cos(theta + w * delta_t),\n     theta + w * delta_t\n  ])\n\n  return g\n\n def jacobian_of_g_wrt_state_G(mu = None, u = None, delta_t = None):\n  theta = mu[2]\n  v = u[0]       \n  w = u[1]       \n  if w == 0:\n   w = 1e-6   # 직선 이동의 경우 0으로 나누는 것을 피하기 위해\n\n  G = np.array([\n   [1, 0, -v / w * np.cos(theta) + v / w * np.cos(theta + w * delta_t)],\n   [0, 1, -v / w * np.sin(theta) + v / w * np.sin(theta + w * delta_t)],\n   [0, 0, 1]\n  ])\n\n  return G\n\n return state_transition_function_g, jacobian_of_g_wrt_state_G\n```\n\n관측 모델은 이전 예제에서 사용한 것과 정확히 동일하며 완전성을 위해 여기에 다시 제시하겠습니다.\n\n```js\ndef odometry_observation_model_linearized():\n def observation_function_h(mu):\n  return mu\n \n def jacobian_of_h_wrt_state_H(mu):\n  return np.eye(3)\n\n return observation_function_h, jacobian_of_h_wrt_state_H\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내 두 번째 확장 칼만 필터가 아래에 구현되었습니다. 코드는 이전 것과 정확히 동일하지만 움직임 모델을 정의하는 라인만 예외입니다.\n\n```js\nclass KalmanFilter:\n\n def __init__(self, initial_state, initial_covariance, proc_noise_std = [0.02, 0.02, 0.01], obs_noise_std = [0.02, 0.02, 0.01]):\n\n  self.mu = initial_state # 초기 상태 추정\n  self.Sigma = initial_covariance # 초기 불확실성\n\n  self.g, self.G = velocity_motion_model_linearized_2() # 사용할 액션 모델\n  \n  # 프로세스 또는 액션 모델 노이즈의 표준 편차\n  self.proc_noise_std = np.array(proc_noise_std)\n  # 프로세스 노이즈 공분산 (R)\n  self.R = np.diag(self.proc_noise_std ** 2) \n\n  self.h, self.H = odometry_observation_model_linearized() # 사용할 관측 모델\n\n  # 관측 또는 센서 모델 노이즈의 표준 편차\n  self.obs_noise_std = np.array(obs_noise_std)\n  # 관측 노이즈 공분산 (Q)\n  self.Q = np.diag(self.obs_noise_std ** 2)\n\n def predict(self, u, dt):\n  # 상태 추정 (mu) 예측\n  self.mu = self.g(self.mu, u, dt)\n  # 공분산 (Sigma) 예측\n  self.Sigma = self.G(self.mu, u, dt) @ self.Sigma @ self.G(self.mu, u, dt).T + self.R \n\n  return self.mu, self.Sigma\n\n def update(self, z, dt):\n  # 칼만 이득 (K) 계산\n  K = self.Sigma @ self.H(self.mu).T @ np.linalg.inv(self.H(self.mu) @ self.Sigma @ self.H(self.mu).T + self.Q)\n  \n  # 상태 추정 (mu) 업데이트\n  self.mu = self.mu + K @ (z - self.h(self.mu))\n\n  # 공분산 (Sigma) 업데이트\n  I = np.eye(len(K)) \n  self.Sigma = (I - K @ self.H(self.mu)) @ self.Sigma\n\n  return self.mu, self.Sigma\n```\n\n성능:\n\n이 시점에서 EKF가 LKF보다 우월함이 명확하며, 이제는 더 정교한 움직임 모델이 뚜렷한 향상을 가져오는지를 결정하는 것에 중점을 두고 연구를 계속할 것입니다. 관측 모델이 명백히 움직임 모델보다 우선하는 잡음 설정에서 평가를 수행하지 않을 것이며, 이렇게 하면 모든 필터에 대해 단순히 관측 결과와 일치할 것으로 예상됩니다. 따라서 여기서는 관측을 대부분 무시하고 움직임 모델에 더 집중하는 잡음 설정으로 결과를 제시합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n놀랍게도 더 고급 모델이 더 간단한 모델보다 우수한 성능을 내지 못했습니다. 위의 두 개의 도표에서는 순수한 눈으로는 중요한 성능 차이를 알아차리기 힘듭니다. 심지어 보고된 잡음 타원도 거의 같아 보입니다.\n\n이전의 기사에서 얻은 결론 중 하나는 우리도 이곳에서 도출하는 것인데, 어떻게 소위 세련되고 고급스러운 필터를 사용하더라도 잘못된 데이터를 입력하면 잘못된 추정값을 제공할 것이라는 것입니다. 우리의 오도메트리 관측에 문제가 있다는 사실을 조정할 방법이 없네요. 이전의 모든 도표에서 주요 문제가 헤딩인 것처럼 보입니다. 올바른 헤딩 데이터가 없으면 어떤 고급 움직임 모델도 이를 보상할 수 없을 겁니다. 로봇이 헤딩의 대안 소스를 제공한다면 (그리고 다른 유용한 센서 데이터도 제공한다면), 필터를 사용하여 서로 다른 소스를 융합하고 더 나은 추정값을 얻을 수 있습니다. 다음에는 센서 융합을 살펴보겠습니다.\n\n## 센서 융합의 힘\n\n센서 융합에는 다양한 종류가 있으며, 우리가 적용할 유형은 저수준 융합으로 알려져 있습니다. 저수준 데이터 융합의 목표는 함께 더 유익한 여러 소스의 기본 센서 데이터를 결합하는 것입니다. 이 아이디어는 서로 다른 소스의 강점을 활용하여 더 나은 추정값을 만들어내는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 원래 오도메트리 데이터와 관성 측정 장치(IMU)에서 오는 데이터를 어떻게 결합하는지에 대해 연구할 것입니다. IMU는 일반적으로 우리에게 방향, 각속도 및 선형 가속도를 제공할 수 있습니다. 이는 우리가 부정확한 추정이 나쁜 방향 정보 때문인 것으로 의심하기 때문에 매우 유익합니다. IMU는 이러한 종류의 정보에 대해 오도메터보다 정확할 경향이 있기 때문에, IMU 데이터가 오도메트리와 어떻게 융합되어 더 나은 결과를 얻을 수 있는지 알아보겠습니다.\n\n우리가 할 첫 번째 일은 상태와 모델을 확장하는 것입니다. 이전에는 추가 데이터의 좋은 소스가 없었기 때문에 로봇의 자세 이상을 고려하는 것이 별 의미가 없었습니다. 그러나 이제 우리에게 각속도와 선형 가속도를 제공할 수 있는 센서가 있기 때문에, 새로운 정보를 상태에 통합하는 것이 로봇의 상태를 더 잘 추정하는 데 도움이 될 것입니다. 이는 또한 우리의 동작 및 센서 모델을 확장할 것이기 때문에 필수적입니다. 이 기사에서는 새로운 7차원 상태로 시작하는 두 가지 다른 확장을 시도할 것입니다.\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_32.png)\n\n여기서 x, y 및 θ는 친숙한 로봇 자세에 해당합니다. 과거 구현과는 달리, 속도 v와 각속도 ω가 제어 입력에서만 얻어졌던 경우, 이제 이들의 추정치도 상태의 일부로 유지합니다. 상태에는 x 및 y의 선형 가속도도 포함됩니다. 확장된 상태를 고려할 때, 동작 모델도 각 상태 구성 요소를 계산하기 위해 확장되어야 합니다. 가속도를 고려하는 동작 모델은 더 이상 상수 속도 모델이 될 수 없습니다. 따라서 속도는 추정 사이에 변화하고, 가속도는 일정하게 유지된다는 새로운 가속도 일정 모델을 사용할 것입니다. 이 새로운 가속도 상수 모델은 아래 표시된 함수 g와 야코비안 행렬 G에 의해 표현됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_33.png)\n\n![image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_34.png)\n\n지금부터 조금 복잡해집니다. 이 큰 야코비안 행렬을 계산하는 것은 매우 에러가 발생하기 쉽습니다. 수동으로 시도해보는 것도 재미있지만, 항상 결과를 확인하기 위해 소프트웨어를 사용하는 것이 좋은 생각입니다. 계산이 올바르다고 확신하고 구현한 후에도 해당 구현이 정확한지 확인할 방법이 있어야 합니다. 이 기사에 구현된 모든 야코비안에 대해, 테스트 스크립트가 잡아낸 적어도 하나의 작은 오류가 있었습니다. 야코비안을 계산하고 구현이 올바른지 확인하는 데 소프트웨어를 사용하지 않으면 코드에 버그를 도입할 확률이 높습니다.\n\n상태 및 운동 모델을 성공적으로 확장했습니다. 이제 센서 퓨전을 가능하게 하려면 관찰 모델도 확장해야 합니다. 확장된 상태 및 운동 모델과 결합하여 퓨전을 수행할 수 있도록 두 센서의 데이터를 연결하려 할 것입니다. 새로운 관찰 모델은 아래에 표시된 벡터의 첫 세 요소인 보통의 로봇 자세와 IMU 센서에서 파생된 방향 θ_imu, 각 속도 ω, 그리고 x 및 y 구성 요소에서의 가속도 a_x 및 a_y와 조합된 것입니다. 우리가 선속도 v를 직접 관측하지는 않지만 여전히 상태에 있을 수 있으며, Kalman 필터에 의해 운동 모델을 통해 추론될 것입니다. 이와 같은 관측되지 않는 변수를 숨겨진 또는 잠재 변수라고 부르는 경우가 종종 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 아래에 정의된 자코비안 행렬입니다. 관측값에서 상태 변수로의 직접적인 매핑이 있으므로, 자코비안에서 관측값(행)이 상태 변수(열)에 해당될 때는 1이 있습니다. 자코비안에서 상태 변수 θ에 해당하는 3열에는 두 개의 1이 있음을 주목해 주세요 — odometry에서 θ에 대한 행과 IMU에서 θ에 대한 행이 각각 하나씩 있습니다. 또한, 선형 속도에 해당하는 4열은 직접 관측하지 않기 때문에 모두 0입니다.\n\n구현된 모션 및 관측 모델은 아래에 나와 있습니다. 필터의 구현은 이전 것과 매우 유사하므로 간결함을 위해 생략하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef acceleration_motion_model_linearized_1():\n\n def state_transition_function_g(mu = None, u = None, delta_t = None):\n  \n  x, y, theta, v, w, a_x, a_y = mu\n\n  v = u[0]      \n  w = u[1] \n  \n  g = np.array([\n   x + v * np.cos(theta) * delta_t + 0.5 * a_x * delta_t**2,      \n      y + v * np.sin(theta) * delta_t + 0.5 * a_y * delta_t**2,    \n      theta + w * delta_t,\n      v + a_x * np.cos(theta) * delta_t + a_y * np.sin(theta) * delta_t,\n      w,                                                      \n      a_x,                                                              \n      a_y\n  ])\n\n  return g\n\n def jacobian_of_g_wrt_state_G(mu = None, u = None, delta_t = None):\n  x, y, theta, v, w, a_x, a_y = mu\n\n  v = u[0]       \n  w = u[1]       \n\n  G = np.array([[1.0, 0.0, -delta_t * v * np.sin(theta), delta_t  * np.cos(theta), 0.0, 0.5*delta_t**2, 0.0],   \n                   [0.0, 1.0, delta_t * v * np.cos(theta), delta_t * np.sin(theta), 0.0, 0.0, 0.5*delta_t**2],       \n                   [0.0, 0.0, 1.0, 0.0, delta_t, 0.0, 0.0],                                      \n                   [0.0, 0.0, -delta_t * a_x * np.sin(theta) + delta_t * a_y * np.cos(theta), \n                   1.0, 0.0, delta_t * np.cos(theta), delta_t * np.sin(theta)],                  \n                   [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],                                          \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],                                          \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])                                         \n  \n  return G\n\n return state_transition_function_g, jacobian_of_g_wrt_state_G\n```\n\n```js\ndef odometry_imu_observation_model_with_acceleration_motion_model_linearized_1():\n def observation_function_h(mu):\n  x, y, theta, v, w, ax, ay = mu\n  return np.array([[x], [y], [theta], [theta], [w], [ax], [ay]]\n \n def jacobian_of_h_wrt_state_H():\n  return np.array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],    \n                    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],         \n                    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],         \n                    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],        \n                    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],         \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],         \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n\n return observation_function_h, jacobian_of_h_wrt_state_H\n```\n\n성능:\n\n더 정교한 모션 모델을 사용하고 오도메트리 데이터와 IMU 데이터를 퓨즈하는 개선된 EKF를 평가하는 시간입니다. 특히, 노이즈 매개변수를 다음과 같이 설정하겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nproc_noise_std = [0.1, 0.1, 0.05, 0.1, 0.1, 0.1, 0.1] # [x, y, theta, v, w, a_x, a_y]\nobs_noise_std = [100.0, 100.0, 1000.0, 6.853891945200942e-06, 1.0966227112321507e-06, 0.0015387262937311438, 0.0015387262937311438] #[x, y, theta, theta_imu, w, a_x, a_y]\n```\n\n이 설정은 우리의 모션 모델에 대해 높은 신뢰를 가지고 있지만 관측 모델에 대해서는 IMU 데이터를 오도메트리 데이터보다 더 신뢰한다고 필터에 알려줍니다. 특히, 우리는 필터에게 오도메트리에서의 헤딩이 미친 것이라고 생각하고 IMU에서의 헤딩이 매우 정확하다고 말하고 있습니다. 이는 IMU가 매우 정밀할 수 있고 오도메트리 데이터가 악명 높게 나쁠 수 있기 때문에 종종 사실입니다. 이 설정은 우리가 예측한 궤적을 수정하는 데 도움이 될 수 있는데, 이전에 본 것처럼 모양은 그리 나쁘지 않지만 방향은 매우 잘못된 경우가 많습니다.\n\n아래 비디오와 이어지는 두 그림에서 볼 수 있듯이 가속도 모델과 센서 퓨전이 포함된 새 필터는 이전 필터보다 훨씬 더 잘 수행됩니다. 특히 IMU로부터 제공된 더 나은 방향성 덕분에 추정 궤적이 초기에 실제 궤적에 훨씬 가까웠음을 볼 수 있습니다. 그러나 마지막에는 지그재그 패턴을 따르기 시작했습니다.\n\n더 나은 결과를 얻기 위한 레시피는 없습니다. 이것이 필터를 설계하는 것을 과학보다는 예술로 만드는 것입니다. 위의 지그재그 궤적을 개선할 수 있는 더 나은 모션 모델로 수정할 수 있을 것이라고 생각할 수 있습니다. 이 결과를 본 것처럼 나도 같은 방식으로 느꼈습니다. 모션 모델을 개선해 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 모션 모델 개선하기\n\n센서 퓨전은 도움이 되고 있지만, 아직 개선할 부분이 많이 남아 있습니다. 상태 추정을 위한 필터를 설계하려면 매우 교육된 추측을 하고 직관을 따라야 합니다. 앞서 논의한 대로, 최근 얻은 결과는 격려적이며 우리가 모델을 개선해 보아야 한다는 제안이 있습니다. 특히 상태를 모델링할 때 글로벌 선속도만을 고려하고, 가속도와 같이 다른 축을 따른 속도를 고려하지 않는 것으로 보입니다. 우리의 상태 벡터를 확장하여 이를 고려해보겠습니다.\n\n![이미지](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_37.png)\n\n새로운 상태 벡터에는 로봇의 움직임에 대한 더 자세한 정보를 제공할 x와 y 성분의 속도가 포함되어 있습니다. 이 확장이 필터의 성능에 어떤 영향을 미치는지 알아보겠습니다. g 함수와 해당 야코비안 G을 이용하여 표현된 확장된 모션 모델은 아래와 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_38.png)\n\n![Image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_39.png)\n\nThe observation model function h remains the same, but the Jacobian H now has two columns filled with zeros corresponding to the unobserved state variables v_x and v_y.\n\n![Image](/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_40.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델과 사용된 잡음 매개변수에 대한 코드는 다음과 같습니다. 여기서 추가적으로 필터 코드는 이전에 소개된 코드와 매우 유사하므로 생략될 것입니다.\n\n```js\ndef acceleration_motion_model_linearized_2():\n\n def state_transition_function_g(mu = None, u = None, delta_t = None):\n  \n  x, y, theta, v_x, v_y , w, a_x, a_y = mu\n\n  v = u[0]       \n  w = u[1]       \n  \n  g = np.array([\n      x + v * np.cos(theta) * delta_t + 0.5 * a_x * delta_t**2,  \n      y + v * np.sin(theta) * delta_t + 0.5 * a_y * delta_t**2,  \n      theta + w * delta_t,                                   \n      v * np.cos(theta) + a_x * delta_t,                         \n      v * np.sin(theta) + a_y * delta_t,                         \n      w,                                                         \n      a_x,                                                       \n      a_y                                                        \n  ])\n\n  return g\n\n def jacobian_of_g_wrt_state_G(mu = None, u = None, delta_t = None):\n  x, y, theta, v_x, v_y, w, a_x, a_y = mu\n\n  v = u[0]       \n  w = u[1]       \n\n  G = np.array([[1.0, 0.0, -delta_t * v * np.sin(theta), 0.0, 0.0, 0.0, 0.5*delta_t**2, 0.0],   \n                   [0.0, 1.0, delta_t * v * np.cos(theta), 0.0, 0.0, 0.0, 0.0, 0.5*delta_t**2],        \n                   [0.0, 0.0, 1.0, 0.0, 0.0, delta_t, 0.0, 0.0],                                      \n                   [0.0, 0.0, -v * np.sin(theta), 0.0, 0.0, 0.0, delta_t, 0.0],                       \n                   [0.0, 0.0, v * np.cos(theta), 0.0, 0.0, 0.0, 0.0, delta_t],                        \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],                                          \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],                                          \n                   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])                                         \n  \n  return G\n \n return state_transition_function_g, jacobian_of_g_wrt_state_G\n```\n\n```js\ndef odometry_imu_observation_model_with_acceleration_motion_model_linearized_2():\n def observation_function_h(mu):\n  x, y, theta, v_x, v_y, w, ax, ay = mu\n  return np.array([[x], [y], [theta], [theta], [w], [ax], [ay]]\n \n def jacobian_of_h_wrt_state_H():\n  return np.array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],       \n                    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],         \n                    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],         \n                    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],            \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],         \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],         \n                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n \n return observation_function_h, jacobian_of_h_wrt_state_H\n```\n\n```js\nproc_noise_std = [0.1, 0.1, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1] # [x, y, theta, v_x, v_y, w, a_x, a_y]\nobs_noise_std = [100.0, 100.0, 1000.0, 6.853891945200942e-06, 1.0966227112321507e-06, 0.0015387262937311438, 0.0015387262937311438] #[x, y, theta, theta_imu, w, a_x, a_y]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n성능:\n\n새로운 필터는 이전 동영상과 아래 그림에서 명백히 확인할 수 있듯이 인상적인 성과를 보여주었습니다. 예측된 궤적이 이제 센서 퓨전과 더 정교한 모델의 덕분에 실제 지면에 매우 가까워졌습니다. 게다가 그림의 최신 자세에서 타원으로 표시된 최종 불확실성은 이전 필터 버전보다 훨씬 작습니다. 이는 센서 퓨전이 잘 설계된 EKF의 효과를 보여줍니다. 추가로 소음 매개변수를 조정하면 더 많은 개선이 가능하지만, 이는 기사의 길이 때문에 여기서 탐구되지 않을 것입니다.\n\n## 직접 시도해보세요\n\n이전 기사와 마찬가지로, 코드는 전체 코드를 검사하거나 단순히 알고리즘을 실행하고 결과를 실시간으로 확인하려는 사람들을 위해 제공됩니다. 코드를 실행하려면 아래 지시사항을 따르십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저 필요한 것은 ROS 2입니다. ROS 2 Humble을 Ubuntu Jammy Jellyfish (22.04)에 설치하는 방법은 여기에서 찾을 수 있습니다. Ubuntu의 다른 버전이나 다른 운영 체제의 경우 공식 ROS 2 문서를 참고하십시오.\n\n데이터를 얻으려면 이 링크에서 ROS 2 가방을 다운로드해야 합니다. 사용하기 전에 파일을 압축 해제해야 합니다.\n\n마지막으로 ROS 2 패키지를 복제하고 빌드해야 합니다. 아래 단계를 따라 진행할 수 있습니다. ros2_ws를 실제 ROS 2 작업 공간으로 교체해야 합니다.\n\n\n# 종속성 설치\nsudo apt install python3-pykdl\n\n# 패키지를 복제하고 빌드\ncd ros2_ws/src\ngit clone https://github.com/carlos-argueta/rse_prob_robotics.git\ncd ..\ncolcon build --symlink-install\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n확장 칼만 필터를 실행하려면 3개의 다른 터미널을 열어야 합니다.\n\n터미널 1에서 (ros2_ws를 실제 워크스페이스로 교체해주세요) 다음 명령을 실행하여 Rviz를 열고 로봇이 보고 있는 것을 확인하세요.\n\n```js\nsource ~/ros2_ws/install/setup.bash\nros2 launch rse_gaussian_filters rviz_launch.launch.py\n```\n\n터미널 2에서 확장 칼만 필터의 버전에 따라 다음 명령 중 하나를 실행하세요. 먼저 아무 출력도 나오지 않을 것이며, ROS 2 가방(bag)을 실행할 때까지 기다리세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsource ~/ros2_ws/install/setup.bash\n\n# 아래 명령어 중 하나 실행\n\n# 3D 상태, 기본 속도 모델\nros2 run rse_gaussian_filters ekf_estimation_3d_v1 \n\n# 3D 상태, 고급 속도 모델\nros2 run rse_gaussian_filters ekf_estimation_3d_v2 \n\n# 7D 상태, 가속 모델, 센서 퓨전\nros2 run rse_gaussian_filters ekf_estimation_7d \n\n# 8D 상태, 가속 모델, 센서 퓨전\nros2 run rse_gaussian_filters ekf_estimation_8d \n```\n\n터미널 3에서 ROS 2 가방이 추출된 위치로 이동하여 다음 명령어로 재생하십시오. \"Ignoring a topic '/navrelposned', reason: package 'ublox_msgs' not found\"와 같은 경고 메시지를 무시해도 됩니다.\n\n```js\nros2 bag play linkou-2023-12-27-2-med --clock\n```\n\n위 단계를 따르면 확장 칼만 필터를 실행하고 결과를 실시간으로 확인할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 실용적인 고려사항 및 마지막으로\n\n이 글은 확장 칼만 필터 (EKF)를 기본적인 선형 칼만 필터 (LKF)의 확장으로 소개했습니다. EKF는 대부분의 실세계 시스템의 비선형성을 근사화를 통해 다룹니다. 이는 사후 평균 주변의 비선형 함수를 일차 테일러 전개를 이용하여 선형화합니다. 선형화된 후, EKF는 LKF와 유사하게 작동합니다.\n\n보여진 대로, 센서 퓨전과 결합된 EKF는 놀라운 결과를 얻을 수 있습니다. 그러나 좋은 필터를 설계하는 것은 어렵고 오류를 범하기 쉽습니다. 종종 과학과 예술 사이의 균형을 요구합니다. 적합한 움직임 및 관측 모델을 찾는 것이 첫 번째 난관입니다. 이동 로봇에 대한 사용 가능한 모델이 있더라도, 다른 시나리오는 좋은 모델이 부족할 수 있습니다. 자코비안 계산 또한 어렵고 실수하기 쉬우며, 소프트웨어 확인이 필요함을 강조합니다.\n\n효과적인 EKF 설계를 위해 여러 다른 고려사항과 결정이 중요합니다. 한 가지 중요한 측면은 올바른 시간 간격(delta_t)을 선택하는 것으로, 효과적인 선형화를 위해 충분히 작아야 합니다. 이 경우, 고정값 대신 동적 delta_t가 사용되었습니다. 필터의 업데이트 단계를 수행할 때 언제, 어떻게 결정하는지도 중요합니다. 특히 서로 다른 속도로 측정 값을 제공하는 다른 센서가 있을 때 (예: IMU는 보통 오도메트리보다 높은 주파수로), 적절한 노이즈 매개 변수 선택도 여러 가지 방법이 가능한 복잡한 작업입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요약하자면, 확장 칼만 필터는 특히 센서 퓨전과 함께 사용될 때 우수한 상태 추정 결과를 제공할 수 있습니다. 그러나 견고한 EKF를 설계하려면 상당한 노력과 연습이 필요합니다. 다음 글에서는 Unscented Kalman Filter (UKF)를 소개할 예정입니다. UKF는 EKF보다 여러 장점을 제공합니다. Unscented Transform을 사용하여 비선형 변환의 평균과 공분산을 더 정확하게 캡처합니다. UKF는 Jacobian을 필요로하지 않아 구현을 간단하게 만들어줍니다. 선형화로 인한 근사 오차를 줄이므로 매우 비선형 시스템에 더 효과적입니다. 또한 UKF는 비가우시안 분포를 더 잘 처리하여 견고성과 다양성을 향상시킵니다.\n\n# 독후감\n\n다음은 칼만 필터 패밀리에 대해 학습하기 위해 참고한 훌륭한 자료 목록입니다:\n\n- Optimal State Estimation: Kalman, H∞, and Nonlinear Approaches\n- State Estimation for Robotics\n- Kalman and Bayesian Filters in Python\n- Probabilistic Robotics\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사가 도움이 되셨기를 바랍니다. 피드백이 있으시면 언제든지 댓글을 남겨주세요. 또한, 이후의 주제를 다룬 보다 심도 있는 강좌 시리즈를 시작하려고 합니다. 이 강좌는 비디오, 코딩 프로젝트 등이 포함될 예정이며 유료로 운영될 가능성이 높습니다. 이런 강좌에 관심이 있다면 댓글로 알려주시면 참여 의향을 파악할 수 있습니다.\n\n저와 소통하고 싶다면 LinkedIn에서 저를 찾아보세요: https://www.linkedin.com/in/carlos-argueta\n\n저와 함께 ROS 2를 이용한 로보틱스를 배우고 싶으신가요? 제 라이브 강의에 참여해보세요!","ogImage":{"url":"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png"},"coverImage":"/assets/img/2024-06-19-SensorFusionwiththeExtendedKalmanFilterinROS2_0.png","tag":["Tech"],"readingTime":31},{"title":"ROS 2 Python 런치 파일의 비밀을 해제합니다","description":"","date":"2024-06-19 06:19","slug":"2024-06-19-UnlockingtheSecretsofROS2PythonLaunchFiles","content":"\n\n![2024-06-19-UnlockingtheSecretsofROS2PythonLaunchFiles_0.png](/assets/img/2024-06-19-UnlockingtheSecretsofROS2PythonLaunchFiles_0.png)\n\n안녕하세요! ROS 2는 로봇 소프트웨어를 간단하게 만들어주는 강력한 프레임워크와 도구입니다. cartographer_ros, navigation2, ros2_control 등 다양한 유용한 패키지들을 통해 로봇 엔지니어들은 로봇을 자율적으로 움직이도록 빠르게 구현할 수 있습니다.\n\n로봇 소프트웨어를 배우는 초기 단계에서 개발자들은 종종 런치 파일을 작성하는 데 많은 시간을 투자합니다. 이 파일들은 준비된 패키지의 프로세스(Ros 2 노드)를 시작하는 데 사용됩니다. 처음에는 모바일 앱과 같이 로직을 많이 작성하지 않는다는 점이 조금 이상하게 느껴졌습니다. 로봇을 위한 소프트웨어를 작성하는 개념을 이해하는 데 시간이 걸렸죠.\n\n특히, Python 런치 파일을 작성하는 것은 일반적인 Python 프로그래밍과는 다르기 때문에 조금 까다로울 수 있습니다. 그래서 이 글을 쓰게 되었습니다. 함께 런치 파일에 혼동을 겪고 계신 분들을 위해 몇 가지 팁을 공유하고 싶기 때문이죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## DeclareLaunchArgument 및 LaunchConfiguration\n\nTurtleBot3 로봇 런치 파일에서 추출된 DeclareLaunchArgument 및 LaunchConfiguration 예제를 볼 수 있습니다:\n\n```js\nuse_sim_time = LaunchConfiguration('use_sim_time', default='false')\n…\nDeclareLaunchArgument(\n    'use_sim_time',\n    default_value=use_sim_time,\n    description='true이면 시뮬레이션 (Gazebo) 시계 사용'\n)\n```\n\n초보자로서, 두 요소 간의 관계를 종종 잊어버리곤 합니다. 또한 LaunchConfiguration과 DeclareLaunchArgument 생성자는 default 또는 default_value 매개변수를 지원하며, 이는 매우 혼란스러울 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 정의는 ROS 2의 공식 문서에서 발췌되었습니다. **굵게 표시한** 단어에 주목해 주시면 단어의 사용법을 이해하는 데 도움이 됩니다.\n\n여러 번 읽은 후에 그것들은 쌍으로 사용되어야 한다는 것을 깨달았어요. 항상 함께 작성하는 것이 좋은 실천 방법이라고 생각해요. Mini Pupper bringup 런치 파일의 예시에서 보여주는 것처럼요:\n\n```js\nuse_sim_time = LaunchConfiguration('use_sim_time')\nuse_sim_time_launch_arg = DeclareLaunchArgument(\n    name='use_sim_time',\n    default_value='False',\n    description='Use simulation (Gazebo) clock if true'\n)\n\nhardware_connected = LaunchConfiguration(\"hardware_connected\")\nhardware_connected_launch_arg = DeclareLaunchArgument(\n    name='hardware_connected',\n    default_value='True',\n    description='Set to true if connected to a physical robot'\n)\n```\n\n이렇게 함께 묶으면 코드가 깔끔하고 명확하게 보이죠. LaunchConfiguration 생성자는 실제로 기본 매개변수를 수용하지만, 사용을 권하지 않습니다. 우리는 LaunchConfiguration에서는 설정된 기본 값을 사용하지 말아야 하며, DeclareLaunchArgument에서만 기본 값을 설정해야 해요. LaunchConfiguration은 런치 파일 내에서 값을 사용하기 위해 런치 인자의 값을 획득하는 데 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 콘솔에 대체값 출력하는 방법\n\n소프트웨어 엔지니어로서, 코드를 디버깅하는 일이 종종 필요합니다. 지금까지 실행 중에 런치 파일에서 중단점을 설정하고 변수 값들을 살펴볼 수 있는 IDE는 없다고 알고 있습니다. 디버깅 방법 중 하나는 콘솔에 일부 정보를 로깅(출력)하는 것입니다.\n\nPython의 print 함수를 사용하면 문자열과 같은 원래 변수를 쉽게 출력할 수 있습니다. 그러나 ROS 2 런치 파일에서 Substitution이라는 특수 유형의 변수를 처리하는 경우가 많습니다. 아래는 ROS 2 공식 문서에서 복사한 정의입니다.\n\nLaunchConfiguration, PathJoinSubstitution 또는 PythonExpression과 같은 종류의 Substitution을 로깅하려면 launch.actions.LogInfo를 사용해야 합니다. rviz2를 위한 런치 파일 예시에서 변수 rviz_config_path의 정확한 값을 알고 싶은 경우를 살펴봅니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport launch\nfrom launch.substitutions import PathJoinSubstitution\nfrom launch_ros.substitutions import FindPackageShare\nfrom launch_ros.actions import Node\nfrom launch.actions import LogInfo\n\ndef generate_launch_description():\n    description_package = FindPackageShare('mini_pupper_description')\n\n    rviz_config_path = PathJoinSubstitution(\n        [description_package, 'rviz', 'urdf_viewer.rviz']\n    )\n\n    a = \"Mini Pupper\"\n    b = \"has 4 legs\"\n    sentence = a + \" \" + b\n    print(sentence)\n\n    return launch.LaunchDescription([\n       Node(\n            package=\"rviz2\",\n            namespace=\"\",\n            executable=\"rviz2\",\n            name=\"rviz2\",\n            arguments=[\"-d\", rviz_config_path]\n        ),\n        LogInfo(msg='rviz_config_path is'),\n        LogInfo(msg=rviz_config_path)\n    ])\n```\n\nLogInfo를 사용하면 콘솔 출력이 아래와 같이 됩니다. \"urdf_viewer.rviz\" 파일의 정확한 위치를 명확하게 확인할 수 있습니다. 참고로, 일반적인 Python 문자열 유형인 \"sentence\" 변수는 Python의 print 함수를 사용하여 쉽게 인쇄되었습니다.\n\n```js\ncullensun@ubuntu:~/ros2_ws$ ros2 launch mini_pupper_bringup rviz.launch.py\n[INFO] [launch]: All log files can be found below /home/cullensun/.ros/log/2024-06-18-00-34-07-704969-ubuntu-7389\n[INFO] [launch]: Default logging verbosity is set to INFO\nMini Pupper has 4 legs\n[INFO] [launch.user]: rviz_config_path is\n[INFO] [launch.user]: /home/cullensun/ros2_ws/install/mini_pupper_description/share/mini_pupper_description/rviz/urdf_viewer.rviz\n[INFO] [rviz2-1]: process started with pid [7390]\n```\n\n## 결론\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사는 ROS 2 Python 런치 파일 작성의 까다로운 측면 중 일부를 탐색했는데, 특히 DeclareLaunchArgument와 LaunchConfiguration에 초점을 맞추고, 디버깅 목적으로 콘솔에 대체 값을 인쇄하는 방법을 다뤘습니다. 이러한 개념을 이해하는 것은 서로 다른 시나리오에 쉽게 적용할 수 있는 견고하고 유연한 런치 파일을 작성하는 데 중요합니다.\n\nDeclareLaunchArgument와 LaunchConfiguration 쌍을 사용하면 명확하고 조직적인 런치 파일을 보장할 수 있습니다. 또한 launch.actions.LogInfo를 활용하면 대체 값을 콘솔에 인쇄하여 디버깅을 지원하고 런치 파일의 동작을 이해하는 데 도움을 줍니다.\n\nROS 2와 함께하는 여정을 계속하면서, 런치 파일은 로봇 응용 프로그램을 관리하는 강력한 도구임을 기억해주세요. 이러한 기술을 숙달함으로써 더 효율적이고 유지 보수가 용이한 런치 파일을 작성하여 개발 프로세스를 보다 원활하고 즐거운 경험으로 만들 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-UnlockingtheSecretsofROS2PythonLaunchFiles_0.png"},"coverImage":"/assets/img/2024-06-19-UnlockingtheSecretsofROS2PythonLaunchFiles_0.png","tag":["Tech"],"readingTime":5},{"title":"라즈베리 파이에 Kali 설정하기 파트 3, VNC 서버 및 클라이언트 설정하기","description":"","date":"2024-06-19 06:18","slug":"2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient","content":"\n\nx11vnc 서버와 RealVNC 뷰어 클라이언트를 사용했어요.\nKali에 VNC 서버를 설치하려면 다음 명령어를 실행하세요,\n\n```js\nsudo apt install x11vnc\n```\n\n다음으로, VNC 서버를 위한 암호를 생성하고 다음 명령어를 실행하여 /etc/vncserver.pass 파일에 저장할 거에요,\n\n```js\nsudo x11vnc -storepasswd\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient_0.png\" /\u003e\n\n만일 VNC 서버를 systemd 서비스로 실행하고 관리하려면, /etc/systemd/system/ 디렉토리 아래에서 해당 서비스를 정의할 수 있습니다. 이를 통해 Linux 운영 체제를 위한 시스템 및 서비스 관리자 인 systemd를 사용하여 VNC 서버를 쉽게 관리하고 구성할 수 있습니다.\n다음 명령을 실행하여 서비스를 만듭니다.\n\n```js\nsudo nano /etc/systemd/system/vncserver.service\n```\n\n아래 스크립트를 vncserver.service 파일에 붙여넣으세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n[Unit]\nDescription=start vnc at boot\nAfter=multi-user.target\n\n[Service]\nType=simple\nExecStart=/usr/bin/x11vnc -display :0 -auth guess -forever -loop -noxdamage -repeat -rfbauth /etc/vncserver.pass -rfbport 5900 -shared\n\n[Install]\nWantedBy=multi-user.target\n\n\n“After=multi-user.target”는 서비스가 활성화된 모든 서비스 이후에 실행되도록합니다. 이것은 시스템이 비그래픽 다중 사용자 세션을 수용할 준비가 된 상태를 나타냅니다. \n\n/boot/config.txt 파일에서 다음 플래그를 주석 처리 해제하세요.\n\n\nframebuffer_width=1280\nframebuffer_height=720\nhdmi_force_hotplug=1\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nframebuffer_width과 framebuffer_height는 디스플레이 해상도를 조절하는 데 사용됩니다.\nhdmi_force_hotplug=1은 HDMI 모니터가 감지되지 않아도 HDMI 모드를 활성화하는 데 사용됩니다.\n\n다음 명령을 실행하여 VNC 서버를 시작하세요.\n\n```js\nsudo systemctl enable vncserver\nsudo systemctl start vncserver\n```\n\n다음 명령으로 VNC 서버 상태를 확인하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsystemctl status vncserver\n```\n\n현재 VNC 서버가 실행 중이며 기본적으로 포트 5900을 사용하고 있습니다.\n\n이제 VNC 서버가 가동 중이므로 클라이언트를 준비해봅시다.\n여러 가지 클라이언트가 있지만, 저는 RealVNC를 사용하고 있습니다. 다음 링크에서 다운로드할 수 있습니다: [https://www.realvnc.com/en/connect/download/viewer/](https://www.realvnc.com/en/connect/download/viewer/)\n\n설치가 완료되면 \"File\"을 클릭한 후 \"New Connection\"을 클릭하여 새로운 VNC 연결을 만들어보세요. 다음과 같은 창이 표시될 것입니다,\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![](/assets/img/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient_1.png)\n\nVNC 서버 IP 주소 또는 호스트명을 입력해주세요. 원하는대로 다른 설정도 사용자 정의할 수 있어요.\n\"확인\"을 클릭하면 연결이 주소록에 나열되어 있을 거에요.\n연결을 두 번 클릭하면 연결이 암호화되지 않았다는 대화 상자가 나타날 거에요.\n\n![](/assets/img/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient_2.png)\n\n나중에 VNC 서버와의 암호화된 연결을 수립하기 위해 SSH 터널을 설정하겠어요. 그 전에 VNC 연결이 성공적으로 설정되었는지 테스트해볼게요. \"계속\"을 클릭하고, 이전에 구성한 VNC 서버 암호를 제공해주세요. 이제 그래픽 사용자 인터페이스(GUI)를 통해 Kali 상자에 액세스할 수 있어야 해요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 VNC 뷰어 창에서 검은 화면이 나타나면, 라즈베리 파이의 해상도 설정을 조정해야 할 수도 있습니다. Kali 상자에 미리 설치된 kalipi-config을 사용하여 라즈베리 파이의 디스플레이 설정과 VNC 뷰어의 설정을 호환되도록 조정할 수 있습니다.\n디스플레이 설정을 구성하려면 다음을 실행하세요:\n\n```bash\nsudo kalipi-config\n```\n\n그러면 다음과 같은 대화 상자가 표시됩니다,\n\n![dialog box](/assets/img/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"“고급 옵션” 섹션으로 이동하여 해상도 설정을 귀하의 요구에 맞게 구성하세요.\n구성을 수정한 후 시스템을 재부팅하겠느냐는 질문이 표시될 것입니다. \"예\"를 선택하세요.\n\nKali가 재부팅될 때까지 기다리세요. 그런 다음 SSH를 통해 Kali에 연결하고 VNC 서버 상태를 확인하여 장치와 함께 부팅되었는지 확인하세요. RealVNC에서 Kali 상자에 연결을 시도하면 이제 GUI 인터페이스를 볼 수 있어야 합니다.\n\n이제 GUI를 통해 원격으로 Kali 상자를 관리할 수 있습니다.\n\nVNC 트래픽은 기본적으로 암호화되어 있지 않습니다. 데이터의 개인 정보보호와 안전을 보장하기 위해 VNC를 사용할 때 SSH 터널을 설정하는 것이 좋습니다. VNC 연결을 보호하기 위해 SSH 터널을 설정하는 방법은 다음 시리즈에서 확인할 수 있습니다.\"","ogImage":{"url":"/assets/img/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient_0.png"},"coverImage":"/assets/img/2024-06-19-KalisetuponRaspberryPiPart3SetupVNCserverandclient_0.png","tag":["Tech"],"readingTime":4},{"title":"DIY 홈 서버 히어로 미디어 및 저장 용 Raspberry Pi 5  CasaOS","description":"","date":"2024-06-19 06:16","slug":"2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_0.png\" /\u003e\n\n# 소개\n\n이 튜토리얼에서는 라즈베리 파이 5를 사용하여 강력한 홈 서버를 구축한 방법을 공유하겠습니다. 이 서버는 미디어 스트리밍과 저렴한 NAS 기능을 해결합니다. 사전 구축된 NAS 솔루션이 있지만, 직접 만드는 과정에서 독특한 만족감이 있습니다.\n\n# 왜 홈 서버를 만들어야 할까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 비용 효율적: Raspberry Pi 5 기반 NAS는 상업용 NAS 솔루션보다 훨씬 저렴합니다.\n- 맞춤 설정: 운영 체제와 서버에서 실행할 서비스를 포함해 설정에 대한 완전한 제어권이 있습니다. 이를 통해 특정한 요구 사항에 맞게 맞춤 설정할 수 있습니다.\n- 학습 경험: 홈 서버를 구축하고 유지하는 것은 서버, 저장 솔루션, 네트워킹 개념 및 Cloudflare 터널과 같은 도구와 유사 경험을 쌓는 훌륭한 방법입니다.\n- 개인정보 보호: 데이터를 직접 관리할 수 있습니다. 자체 호스팅으로 제3자 클라우드 솔루션에 의존하지 않아 개인정보 관련 우려를 피할 수 있습니다.\n\n# 준비물\n\n## 하드웨어\n\n- Raspberry Pi 5\n- 마이크로 SD 카드 (32GB 이상)\n- Raspberry Pi 공식 전원 어댑터\n- 이더넷 케이블 (안정적인 연결을 위해)\n- 외장 하드 드라이브 또는 SSD\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 소프트웨어\n\n- Raspberry Pi Imager (Ubuntu Server 24.04 LTS)\n- Casa OS\n- Jellyfin 또는 Plex (미디어 서버)\n- Nextcloud (Google Drive 대안)\n- Cloudflared 또는 Cloudflare Tunnel Client (네트워크 외부에서 원격 액세스)\n\n# 단계 1 : Ubuntu Server 24.04 LTS 설치\n\nRaspberry Pi Imager를 다운로드하고 Ubuntu Server를 마이크로 SD 카드에 플래시합니다. VNC 또는 모니터를 사용하여 Raspberry Pi에 부팅하고 화면에 표시된 지침에 따라 설정을 완료합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n터미널에서 업데이트 및 업그레이드 명령어를 입력하세요.\n\n```bash\nsudo apt-get update\n\nsudo apt-get upgrade\n```\n\nUbuntu Server 24.04 LTS의 특징\n\n- LTS는 Long Term Support의 약자로 다섯 년간의 무료 보안 및 유지 보수 업데이트를 제공합니다.\n- Linux 6.8 커널에서는 기본으로 저지연 커널 기능이 활성화되어 있습니다.\n- 대부분의 x86 아키텍처 패키지에 대해 프레임 포인터가 기본으로 활성화되어 있습니다.\n- Rust 1.75, .NET 8 및 TCK 인증이 포함된 OpenJDK 21과 같은 다른 툴체인 업그레이드\n- 2038년 문제를 해결하기 위해 armhf에서는 기본으로 64비트 타임스탬프를 사용합니다.\n- AppArmor로 강제된 특권이없는 사용자 이름 영역이 기본적으로 제한됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저 충돌하는 패키지를 모두 제거해야 해요. 다음 명령어를 실행해서 충돌하는 모든 패키지를 제거하세요.\n\n```js\nfor pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done\n```\n\n- Docker apt 저장소를 설정하세요.\n\n```js\n# 도커의 공식 GPG 키를 추가하세요:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n# Apt 소스에 저장소를 추가하세요:\necho \\\n\"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n$(. /etc/os-release \u0026\u0026 echo \"$VERSION_CODENAME\") stable\" | \\\nsudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null\nsudo apt-get update\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. 도커 패키지를 설치하세요.\n\n```bash\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n```\n\n3. 도커 설치가 성공적인지 확인하려면 hello-world 이미지를 실행하세요.\n\n```bash\nsudo docker run hello-world\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 명령은 테스트 이미지를 다운로드하여 컨테이너에서 실행합니다. 컨테이너가 실행되면 확인 메시지를 출력한 후 종료됩니다.\n\n# 단계 3: Casa Os 설치\n\nCasa OS와 Casa Os를 선택한 이유는 무엇인가요?\n\n- Casa OS는 간단한 가정용 클라우드 경험을 제공하는 커뮤니티 기반의 오픈 소스 소프트웨어입니다.\n- 아무 싱글 보드 컴퓨터(SBC), 오래된 PC 또는 Linux를 실행하는 장치에서 실행할 수 있습니다.\n- 간단히 말해 Casa Os는 앱 스토어를 통해 도커 컨테이너를 관리하는 멋진 대시보드입니다.\n- 도커 컨테이너를 관리하기 위한 여러 대안이 인터넷에서 제공되지만, 그 중 몇 가지는 portainer, proxmox가 있습니다.\n- 저는 Casa Os를 선택했는데, 이는 초보자 친화적이며 설정이 쉽고 proxmox와 같은 다른 소프트웨어보다 적은 시스템 자원을 필요로 하기 때문입니다. Portainer는 검토할 수 있는 좋은 대안입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 다음 명령을 사용하여 시스템에 Casa Os를 설치하세요\n\n```js\ncurl -fsSL https://get.casaos.io | sudo bash\n```\n\n2. Casa Os에 액세스하기\n\n사용 중인 브라우저를 열어 Casa Os 웹 UI가 준비되었는지 확인하세요. 터미널에서 얻은 URL을 사용해주세요. 이 URL은 라즈베리 파이의 IP 주소입니다. Casa Os에 액세스하려는 기기가 라즈베리 파이가 연결된 동일한 네트워크에 연결되어 있는지 확인하세요. 설정을 완료하기 위해 화면 안내에 따라 진행하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가서 계정 설정을 시작하려면 Go를 클릭하세요\n\n![이미지](/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_1.png)\n\n세부 정보를 입력하고 생성을 클릭하세요.\n\n![이미지](/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 페이지에서는 까사 오스(Casa Os)에 대한 최신 소식을 받고 싶은지 물어볼 것입니다. 원하시는 대로 '예' 또는 '아니오'를 선택하실 수 있습니다.\n\n그 다음 페이지에서는 까사 오스(Casa Os) 대시보드를 볼 수 있습니다.\n\n![Casa Os Dashboard](/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_3.png)\n\n# 단계 4 : 앱 스토어에서 앱 설치하기.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n원하는 모든 필수 앱 (기본적으로 도커 이미지입니다)을 다운로드하세요. 이 튜토리얼에서는 다음을 다운로드할 것입니다.\n\n- Jellyfin (미디어 서버)\n\n![Jellyfin](/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_4.png)\n\n2. Nextcloud (Google 드라이브 대안)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n천천히 즐기며 작업하시고 계속 진행하시기 바랍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n외부 드라이브 또는 SSD가 라즈베리 파이의 USB 3.0 포트에 연결되어 있는지 확인해주세요. 앱이 설치된 후, 앱에 표시된 세 개의 점을 클릭하고 설정으로 이동해주세요. 여기서 도커의 모든 설정을 볼 수 있을 거에요. 포트, 네트워크, 볼륨, 환경 변수 등이 있어요. '볼륨'이 보이기 전까지 스크롤을 내려주세요. '호스트' 섹션에 외부 드라이브의 경로를 추가하고 '컨테이너' 섹션에 원하는 이름을 추가해주세요. 저는 \"Lap\"을 추가했어요. 아래 그림에서 확인할 수 있어요.\n\n![이미지](/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_7.png)\n\n이제 앱이 새로고침되고 열 수 있게 되며 Jellyfin 설정을 완료하기 위해 화면 안내에 따라 진행하실 수 있어요. 이 설정을 하기 전에 Jellyfin에서 권장하는 폴더 구조를 따르는지 확인해주세요. 이를 위해 공식 Jellyfin 문서를 참고하실 수 있어요:\n\n- TV 프로그램 : [https://jellyfin.org/docs/general/server/media/shows/](https://jellyfin.org/docs/general/server/media/shows/)\n- 영화 : [https://jellyfin.org/docs/general/server/media/movies](https://jellyfin.org/docs/general/server/media/movies)\n- 음악 : [https://jellyfin.org/docs/general/server/media/music](https://jellyfin.org/docs/general/server/media/music)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 단계 5: 원격 액세스를 위한 Cloudflare 터널 설정. (선택사항)\n\n이제 모든 앱이 설치되고 설정이 완료되었습니다. 당신만의 홈 서버를 구축하는 마지막 단계입니다. 가정 내 홈 서버가 설정되어 있고 당신은 다른 곳에 있다면서도 서버에 액세스하고 싶을 때 Cloudflare 터널을 사용할 수 있습니다 (미디어 서버를 제외하고). Cloudflare의 이용 약관에 따라 클라우드플레어 무료 티어를 사용하여 미디어 서버에 원격 액세스할 수 없습니다. 아래 단계를 따라 Cloudflare 터널을 설정하세요\n\n- Cloudflare 터널을 사용하려면 도메인 이름이 필요합니다. hostinger, namecheap, cloudflare 등의 도메인 공급업체에서 매우 저렴하게 구매할 수 있습니다.\n- 도메인 이름을 구매한 후에는 도메인 이름 공급업체의 웹사이트에서 네임서버를 cloudflare로 변경해야 합니다.\n- cloudflare.com에 가서 '회원 가입'을 클릭하세요. 사용 가능한 옵션 중 '무료 티어'를 클릭하세요. 이메일과 비밀번호를 입력하여 회원 가입을 완료하세요.\n\n![이미지](/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_8.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4. \"웹 사이트 또는 애플리케이션 추가\"를 클릭하세요.\n\n5. 방금 구매한 도메인 이름을 입력하거나 Cloudflare에서 도메인 이름을 등록할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_9.png)\n\n6. 아래로 스크롤하여 무료 요금제를 클릭하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_10.png)\n\n7. 도메인 이름과 관련된 DNS 레코드를 찾으려면 스캔을 클릭하세요.\n\n8. 다음 단계는 활성화입니다. 여기에서는 도메인 이름을 구매한 웹사이트로 돌아가서 화면에 제공된 클라우드플레어 이름 서버로 웹사이트의 네임서버를 변경해야 합니다. 이 작업은 일반적으로 시간이 걸리며 프로세스가 완료되면 이메일로 통보받게 됩니다.\n\n![이미지](/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_11.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n9. 이제 Cloudflare 터널 설정을 진행합니다. 사이드바에서 '제로 트러스트'를 선택하세요. 팀 이름을 선택해주세요. 원하는 것으로 입력하시면 됩니다.\n\n![이미지](/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_12.png)\n\n10. 다음 화면에서 무료 요금제를 선택하세요. 작업을 위해 카드 세부 정보를 입력해주셔야 합니다. 하지만 걱정마세요, 클라우드플레어 이용 약관을 준수할 때까지 요금이 청구되지는 않습니다.\n\n![이미지](/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_13.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n11. 지금 결제 세부 정보가 추가되었습니다. 네트워크의 사이드바 하단에 터널이 표시됩니다. 터널을 추가하고 클라우드플레어드를 커넥터로 선택해 주세요.\n\n![image](/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_14.png)\n\n12. 터널을 이름 붙이고 환경을 선택하세요 (도커가 권장됩니다). 화면에 표시된 명령어를 복사하여 Casa Os에 다운로드한 클라우드플레어 앱에 붙여넣으세요. 시작을 클릭하면 클라우드플레어 대시보드에서 커넥터 상태를 확인할 수 있습니다.\n\n![image](/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_15.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n13. 마지막 페이지에 터널을 위한 공개 호스트 이름을 추가하세요. 도메인 이름, 서브도메인(선택 사항) 및 서비스 유형을 입력하십시오. URL에는 IP 주소 및 특정 포트의 세부 정보를 입력합니다. 터널을 저장하고 나면 앱에 원격으로 액세스할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_16.png)\n\n# 도전과 해결책:\n\n## 도전 1: 원격 액세스를 위한 SSH 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제가 모니터가 없어서 Raspberry Pi에 Putty 소프트웨어를 사용하여 ssh하고 싶었지만, 라즈베리 파이에 Ubuntu Server 24.04를 처음 설치했을 때 라즈베리 파이에 부팅할 수 없는 문제가 발생했습니다.\n\n해결책:\n\n- 문제 해결: 일부 문제 해결 후 Ubuntu Server에 ssh할 수 있었습니다.\n\n## 도전 2: 원격 액세스 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 집 서버에 원격으로 액세스하고 싶었습니다. 그렇게 하면 집에 없을 때도 파일을 관리하고 미디어를 스트리밍할 수 있을 것 같았어요. 그러나 원격 액세스를 안전하게 구성하는 것은 조금 복잡할 수 있어요.\n\n해결책:\n\n- 조사: SSH 터널 및 Cloudflare 터널을 포함한 다양한 원격 액세스 방법을 조사했어요.\n- Cloudflare 터널 선택: Cloudflare 터널을 사용하기로 결정했는데, 그들의 사용 편의성과 보안 기능 때문이었어요.\n- 구성 과제: Cloudflare 터널 설정에는 DNS 레코드 생성 및 라즈베리 파이에서 방화벽 규칙 구성이 포함되었어요. 온라인에서 안내하는 유익한 자습서를 찾아 프로세스를 단계별로 진행할 수 있었어요.\n- 문제 해결: 처음에 원격으로 서버에 액세스할 수 없는 문제가 발생했어요. 문제 해결 후 DNS 레코드 구성에 오타가 있었음을 발견했어요. 그 오타를 고치면 문제가 해결되었어요.\n\n# 향후 계획\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 클라우드플레어 터널 대신 tailscale 및 twingate 같은 소프트웨어를 사용하여 라즈베리 파이에 원격으로 액세스할 수 있습니다.\n- 미디어 서버를 자동화해서 사용자가 arr 앱을 사용하여 볼 미디어를 요청할 수 있습니다.\n- 서버에 자체 서명 SSL 인증서를 설정하여 보안 경고를 피할 수 있습니다.\n- 서버에 개인 웹 사이트를 호스팅할 수 있지만, 서버 다운 시에 웹 사이트에 액세스할 수 없기 때문에 권장되지 않습니다.\n- Raspberry Pi 5에 UPS를 설정하여 다운 타임을 없앨 수 있습니다.\n\n# 결론\n\n이 홈 서버 구축은 정말 즐거운 프로젝트였습니다. 가끔 발생하는 문제 해결이 괴로울 수도 있지만, 처음부터 완전히 기능적인 홈 서버를 만들어내는 엄청난 만족감은 모든 어려움을 뛰어넘었습니다. 재미 이상으로, 이 프로젝트는 귀중한 학습 경험이 되었습니다. 서버 관리, 네트워킹 개념 및 Docker에 대한 실용적인 지식을 습득하며, 모든 기술 애호가에게 필수적인 기술들을 얻을 수 있었습니다. 이 프로젝트에 착수한 저 자신을 자랑스럽게 생각하고, 여러분도 고유의 홈 서버를 구축하는 것을 고려해 보라고 추천하고 싶습니다. 보상적이고 교육적인 여정이 될 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_0.png"},"coverImage":"/assets/img/2024-06-19-DIYHomeServerHeroRaspberryPi5CasaOSforMediaStorage_0.png","tag":["Tech"],"readingTime":10},{"title":"실시간 얼굴 인식 끝에서 끝까지의 프로젝트","description":"","date":"2024-06-19 06:12","slug":"2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject","content":"\n\n단계별로 배우세요! PiCam을 사용하여 실시간으로 얼굴을 인식하는 방법을 배워보세요.\n\n![PiCam Image](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_0.png)\n\n# 1. 소개\n\nOpenCV를 탐구하는 내 교재에서는 자동 비전 객체 추적을 배웠습니다. 이제 PiCam을 사용하여 실시간으로 얼굴을 인식해보겠습니다. 위에서 볼 수 있듯이, 함께해요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_1.png)\n\n이 프로젝트는 이 훌륭한 \"Open Source Computer Vision Library\" 인 OpenCV를 사용하여 수행되었습니다. 이 튜토리얼에서는 Raspberry Pi (즉, Raspbian을 사용한 OS)와 Python에 중점을 두지만 Mac에서 코드를 테스트하고 또한 잘 작동하는 것을 확인했습니다.\n\nOpenCV는 계산 효율성을 위해 설계되었으며 실시간 애플리케이션에 중점을 두고 있습니다. 따라서 카메라를 사용한 실시간 얼굴 인식에 적합합니다.\n\n## 3 단계\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n얼굴인식에 대한 완전한 프로젝트를 생성하려면 3가지 매우 다른 단계에서 작업해야 합니다:\n\n- 얼굴 감지 및 데이터 수집\n- 인식기 훈련\n- 얼굴 인식\n\n다음 블록 다이어그램은 이러한 단계들을 요약합니다:\n\n![Face Recognition Project Phases](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2. OpenCV 3 패키지 설치\n\n저는 최신 버전의 라스비안(Stretch)이 설치된 라즈베리 파이 V3를 사용하고 있습니다. 따라서 OpenCV를 설치하는 가장 좋은 방법은 Adrian Rosebrock이 개발한 훌륭한 튜토리얼을 따라하는 것입니다: \"Raspbian Stretch: 라즈베리 파이에 OpenCV 3 + Python 설치\".\n\nAdrian의 튜토리얼을 완료하면 라즈베리 파이에서 실험을 실행할 준비가 된 OpenCV 가상 환경이 준비됩니다.\n\n이제 가상 환경으로 이동하여 OpenCV 3이 올바르게 설치되었는지 확인해 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n에드리안이 새 터미널을 열 때마다 \"source\" 명령을 실행하여 시스템 변수가 올바르게 설정되었는지 확인하는 것을 권장합니다.\n\n```js\nsource ~/.profile\n```\n\n다음으로, 가상 환경에 들어가 봅시다:\n\n```js\nworkon cv\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 당신이 cv 가상환경 앞에 있는 텍스트를 보신다면, cv 가상환경 안에 있습니다:\n\n```js\n(cv) pi@raspberry:~$\n```\n\n이제 파이썬 인터프리터로 들어가보세요:\n\n```js\npython\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희가 현재 3.5 버전 (또는 그 이상)을 실행 중이라고 확인해주세요.\n\n인터프리터 안에 ( 가 표시될 것입니다), OpenCV 라이브러리를 import 해주세요:\n\n```js\nimport cv2\n```\n\n만약 에러 메시지가 나타나지 않는다면, OpenCV가 정확하게 파이썬 가상 환경에 설치된 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n설치된 OpenCV 버전을 확인할 수도 있어요:\n\n```js\ncv2.__version__\n```\n\n3.3.0이 표시되어야 해요 (또는 미래에 출시될 우수한 버전). \n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_3.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 터미널 스크린샷은 이전 단계를 보여줍니다.\n\n# 3. 카메라 테스트\n\nRPi에 OpenCV를 설치했다면 카메라가 제대로 작동하는지 확인하기 위해 테스트를 해봅시다.\n\n이미 PiCam을 설치하고 활성화했다는 것을 전제로 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 Python 코드를 IDE에 입력해보세요:\n\n```js\nimport numpy as np\nimport cv2\ncap = cv2.VideoCapture(0)\ncap.set(3,640) # 너비 설정\ncap.set(4,480) # 높이 설정\nwhile(True):\n    ret, frame = cap.read()\n    frame = cv2.flip(frame, -1) # 카메라 세로로 뒤집기\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    \n    cv2.imshow('frame', frame)\n    cv2.imshow('gray', gray)\n    \n    k = cv2.waitKey(30) \u0026 0xff\n    if k == 27: # 'ESC' 키를 눌러 종료\n        break\ncap.release()\ncv2.destroyAllWindows()\n```\n\n위 코드는 PiCam에서 생성된 비디오 스트림을 캡처하여 BGR 색상과 회색 모드로 모두 표시합니다.\n\n또는 GitHub에서 코드를 다운로드할 수도 있습니다: simpleCamTest.py\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해당 스크립트를 실행하려면 다음 명령어를 입력해주세요:\n\n```js\npython simpleCamTest.py\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_4.png\" /\u003e\n\n위의 그림은 결과를 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일부 사용자가 카메라를 열려고 시도할 때 문제가 발생하고 \"Assertion failed\" 오류 메시지가 표시된다는 것을 발겠어요. 이것은 카메라가 OpenCV 설치 중에 활성화되지 않았거나 카메라 드라이버가 올바르게 설치되지 않아 발생할 수 있습니다. 수정하려면 다음 명령을 사용하세요:\n\n```js\nsudo modprobe bcm2835-v4l2\n```\n\nOpenCV에 대해 더 알고 싶다면 다음 튜토리얼을 참고해보세요: loading-video-python-opencv-tutorial\n\n# 4. 얼굴 인식\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nFace Recognition에서 가장 기본적인 작업은 물론 \"얼굴 감지\"입니다. 무엇보다 먼저, 미래에 캡처된 새 얼굴과 비교할 수 있도록 얼굴을 캡처해야 합니다(Phase 1).\n\n(혹시 이전에 말한 'Capture'부분 해석이 잘못된 것 같아 원문에서 대체했습니다.)\n\n한 객체를 감지하는 가장 일반적인 방법은 \"Haar Cascade classifier\"를 사용하는 것입니다.\n\n\"Haar 특징 기반 캐스케이드 분류기\"를 사용한 객체 감지는 Paul Viola와 Michael Jones가 2001년에 발표한 논문 \"Rapid Object Detection using a Boosted Cascade of Simple Features\"에서 제안된 효과적인 객체 감지 방법입니다. 이는 많은 양의 긍정 이미지와 부정 이미지로부터 케스케이드 함수가 훈련된 기계 학습 기반 방법으로, 다른 이미지에서 객체를 감지하는 데 사용됩니다.\n\n여기서는 얼굴 감지 작업을 수행할 것입니다. 먼저, 알고리즘에는 분류기를 훈련시키기 위해 많은 양의 양성 이미지(얼굴 이미지)와 음성 이미지(얼굴이 없는 이미지)가 필요합니다. 그런 다음 그로부터 특징을 추출해야 합니다. 좋은 소식은 OpenCV에 트레이너와 탐지기가 함께 제공된다는 것입니다. 자동차, 비행기 등의 모든 객체에 대해 자체 분류기를 훈련시키고 싶다면 OpenCV를 사용해 생성할 수 있습니다. 자세한 내용은 여기를 참고하십시오: 캐스케이드 분류기 훈련.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 여러분이 자체 분류기를 만들고 싶지 않다면, OpenCV에는 이미 얼굴, 눈, 웃음 등을 위한 사전 훈련된 분류기가 많이 포함되어 있습니다. 이러한 XML 파일은 haarcascades 디렉토리에서 다운로드할 수 있습니다.\n\n이론은 이만하고, 이제 OpenCV를 사용하여 얼굴 탐지기를 만들어 봅시다!\n\n제 GitHub에서 파일 faceDetection.py를 다운로드하세요.\n\n```python\nimport numpy as np\nimport cv2\nfaceCascade = cv2.CascadeClassifier('Cascades/haarcascade_frontalface_default.xml')\ncap = cv2.VideoCapture(0)\ncap.set(3,640) # 너비 설정\ncap.set(4,480) # 높이 설정\nwhile True:\n    ret, img = cap.read()\n    img = cv2.flip(img, -1)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = faceCascade.detectMultiScale(\n        gray,\n        scaleFactor=1.2,\n        minNeighbors=5,\n        minSize=(20, 20)\n    )\n    for (x,y,w,h) in faces:\n        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n        roi_gray = gray[y:y+h, x:x+w]\n        roi_color = img[y:y+h, x:x+w]\n    cv2.imshow('video',img)\n    k = cv2.waitKey(30) \u0026 0xff\n    if k == 27: # 'ESC'를 눌러 종료\n        break\ncap.release()\ncv2.destroyAllWindows()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n믿을 수 있건대, Python과 OpenCV를 사용하여 얼굴을 감지하는 데 필요한 코드는 바로 위의 몇 줄 뿐입니다.\n\n카메라를 테스트하는 데 사용된 마지막 코드와 비교해보면, 약간의 부분이 추가되었음을 깨달을 것입니다. 아래의 줄을 주목해주세요:\n\n```js\nfaceCascade = cv2.CascadeClassifier('Cascades/haarcascade_frontalface_default.xml')\n```\n\n이 줄은 \"classifier\"를 로드하는 줄입니다 (프로젝트 디렉토리 하위에 \"Cascades/\"라는 디렉토리에 있어야 합니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼, 우리는 카메라를 설정하고 루프 내에서 입력 비디오를 회색조 모드로 불러올 것입니다 (이전에 보았던 것과 동일합니다).\n\n이제 분류기 함수를 호출해야 하는데, 이때 매우 중요한 매개변수들을 전달해주어야 합니다. 이들은 scale factor, 이웃의 수, 그리고 감지된 얼굴의 최소 크기입니다.\n\n```js\nfaces = faceCascade.detectMultiScale(\n        gray,     \n        scaleFactor=1.2,\n        minNeighbors=5,     \n        minSize=(20, 20)\n        )\n```\n\n여기서,\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- gray는 입력 그레이스케일 이미지입니다.\n- scaleFactor는 각 이미지 스케일에서 이미지 크기가 감소하는 정도를 지정하는 매개 변수입니다. 스케일 피라미드를 만드는 데 사용됩니다.\n- minNeighbors는 각 후보 사각형이 유지해야 하는 이웃 수를 지정하는 매개 변수입니다. 숫자가 높을수록 낮은 거짓 양성이 발생합니다.\n- minSize는 고려해야 할 최소 사각형 크기입니다.\n\n함수는 이미지에서 얼굴을 감지합니다. 다음으로 이미지에서 얼굴을 \"표시\"해야 합니다. 예를 들어, 파란색 사각형 등을 사용합니다. 이 작업은 다음 코드 부분으로 수행됩니다:\n\n```js\nfor (x,y,w,h) in faces:\n    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n    roi_gray = gray[y:y+h, x:x+w]\n    roi_color = img[y:y+h, x:x+w]\n```\n\n얼굴이 발견되면, 감지된 얼굴의 위치를 왼쪽 위 모서리인 (x,y)로 하는 네모 상자로 반환하며, \"w\"를 너비, \"h\"를 높이로 가집니다 == (x, y, w, h). 사진을 참조하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_5.png\" /\u003e\n\n이 위치들을 얻으면, 얼굴을 위한 \"ROI\" (사각형으로 그린)를 만들고 imshow() 함수를 사용하여 결과를 표시할 수 있습니다.\n\nRpi 터미널을 사용하여 위의 Python 스크립트를 Python 환경에서 실행해보세요:\n\n```js\npython faceDetection.py\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과:\n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_6.png\" /\u003e\n\n당신은 \"눈 탐지\" 또는 \"미소 탐지\"를 위한 분류기도 포함할 수 있습니다. 해당 경우에는 얼굴 루프 내에 분류기 함수와 사각형 그리기를 포함해야 합니다. 왜냐하면 얼굴 외부에서 눈이나 미소를 탐지하는 것은 의미가 없기 때문입니다.\n\n## 예시\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내 GitHub에서 다른 예제들을 찾을 수 있어요:\n\n- faceEyeDetection.py\n- faceSmileDetection.py\n- faceSmileEyeDetection.py\n\n그리고 사진에서 결과를 확인할 수 있어요.\n\n![image](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 튜토리얼을 따라해서 얼굴 감지에 대해 더 잘 이해할 수 있어요:\n\nHaar Cascade Object Detection Face \u0026 Eye OpenCV Python Tutorial\n\n# 5. 데이터 수집\n\n우선, 사진을 통한 얼굴 인식에 대한 람리즈 라자의 훌륭한 작업에 감사드려야 해요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nOPENCV와 Python을 사용한 얼굴 인식: 초보자를 위한 안내서\n\n그리고 비디오를 사용해 매우 포괄적인 튜토리얼을 개발한 Anirban Kar:\n\n얼굴 인식 - 3 부분\n\n두 튜토리얼을 꼭 확인해보시기를 적극 추천합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그렇게 말씀하시면 프로젝트의 첫 단계를 시작하겠습니다. 여기서 우리가 할 일은 마지막 단계(얼굴 감지)부터 시작하여 간단히 데이터 세트를 만드는 것입니다. 각 ID에 대해, 얼굴 감지에 사용된 부분이 회색으로 표시된 사진 그룹을 저장할 것입니다.\n\n![이미지](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_8.png)\n\n먼저, 프로젝트를 개발할 디렉터리를 만드세요. 예를 들어, FacialRecognitionProject:\n\n```js\nmkdir FacialRecognitionProject\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 디렉토리에는 프로젝트용으로 만들 3개의 Python 스크립트뿐만 아니라 Facial Classifier도 저장해 두어야 해요. 이를 다운로드할 수 있는 GitHub 링크는 haarcascade_frontalface_default.xml입니다.\n\n그리고 우리의 얼굴 샘플을 저장할 하위 디렉토리를 만들어 이름을 \"dataset\"으로 지어줘요:\n\n```bash\nmkdir dataset\n```\n\n그리고 저의 GitHub에서 코드를 다운로드하세요: 01_face_dataset.py\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport cv2\nimport os\ncam = cv2.VideoCapture(0)\ncam.set(3, 640) # 비디오 너비 설정\ncam.set(4, 480) # 비디오 높이 설정\nface_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n# 각 사람에 대해 하나의 숫자 얼굴 ID 입력\nface_id = input('\\n 사용자 ID를 입력하고 \u003creturn\u003e을 누르세요 ==\u003e ')\nprint(\"\\n [INFO] 얼굴 캡처 초기화. 카메라를 응시하고 기다리세요 ...\")\n# 개별 샘플링 얼굴 카운트 초기화\ncount = 0\nwhile(True):\n    ret, img = cam.read()\n    img = cv2.flip(img, -1) # 비디오 이미지 수직으로 뒤집기\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = face_detector.detectMultiScale(gray, 1.3, 5)\n    for (x,y,w,h) in faces:\n        cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)     \n        count += 1\n        # 캡처된 이미지를 데이터셋 폴더에 저장\n        cv2.imwrite(\"dataset/User.\" + str(face_id) + '.' +  \n                    str(count) + \".jpg\", gray[y:y+h,x:x+w])\n        cv2.imshow('image', img)\n    k = cv2.waitKey(100) \u0026 0xff # 'ESC' 키를 눌러 비디오 종료\n    if k == 27:\n        break\n    elif count \u003e= 30: # 30개의 얼굴 샘플 촬영 후 비디오 중지\n         break\n# 청소 조금\nprint(\"\\n [INFO] 프로그램 종료 및 정리 진행\")\ncam.release()\ncv2.destroyAllWindows()\n``` \n\n코드는 얼굴 감지를 위한 코드와 매우 유사합니다. 추가한 부분은 \"사용자 ID를 캡처하기 위한 입력 명령\"이며, 이는 정수 번호(1, 2, 3 등)여야 합니다.\n\n```js\nface_id = input('\\n 사용자 ID를 입력하고 \u003creturn\u003e을 누르세요 ==\u003e ')\n```\n\n그리고 각 캡처된 프레임마다 \"데이터셋\" 디렉토리에 파일로 저장해야 합니다. \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ncv2.imwrite(\"dataset/User.\" + str(face_id) + '.' + str(count) + \".jpg\", gray[y:y+h,x:x+w])\n```\n\n위의 파일을 저장하기 위해서는 라이브러리 \"os\"를 import해야 합니다. 각 파일의 이름은 다음과 같은 구조를 따릅니다:\n\n```js\nUser.face_id.count.jpg\n```\n\n예를 들어, face_id가 1인 사용자의 경우, dataset/ 디렉토리에 있는 네 번째 샘플 파일은 아래와 같이 될 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nUser.1.4.jpg\n```\n\nPI 사진에서 보여지는 것과 같이:\n\n\n![Image](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_9.png)\n\n내 코드에서는 각 ID로부터 30개의 샘플을 캡처하고 있습니다. 마지막 \"elif\"에서 이를 변경할 수 있습니다. 샘플의 수는 얼굴 샘플을 캡처하는 루프를 종료하는 데 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파이썬 스크립트를 실행하고 몇 개의 ID를 캡처하세요. 새 사용자를 집계하거나 이미 존재하는 사용자의 사진을 변경하려면 매번 스크립트를 실행해야 합니다.\n\n# 6. 트레이너\n\n이 두 번째 단계에서는 데이터셋에서 모든 사용자 데이터를 가져와 OpenCV Recognizer를 \"트레이닝\"해야 합니다. 이 작업은 특정한 OpenCV 함수를 사용하여 직접 수행됩니다. 결과는 \"trainer/\" 디렉토리에 저장된 .yml 파일이 될 것입니다.\n\n![이미지](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러면, 훈련 데이터를 저장할 하위 디렉토리를 만드는 것으로 시작해봐요:\n\n```js\nmkdir trainer\n```\n\n제 GitHub에서 두 번째 파이썬 스크립트를 다운로드하실 거에요: 02_face_training.py\n\n```js\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport os\n# 얼굴 이미지 데이터베이스 경로\npath = 'dataset'\nrecognizer = cv2.face.LBPHFaceRecognizer_create()\ndetector = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\");\n# 이미지 및 레이블 데이터 가져오는 함수\ndef getImagesAndLabels(path):\n    imagePaths = [os.path.join(path,f) for f in os.listdir(path)]     \n    faceSamples=[]\n    ids = []\n    for imagePath in imagePaths:\n        PIL_img = Image.open(imagePath).convert('L') # 흑백\n        img_numpy = np.array(PIL_img,'uint8')\n        id = int(os.path.split(imagePath)[-1].split(\".\")[1])\n        faces = detector.detectMultiScale(img_numpy)\n        for (x,y,w,h) in faces:\n            faceSamples.append(img_numpy[y:y+h,x:x+w])\n            ids.append(id)\n    return faceSamples,ids\nprint (\"\\n [INFO] 얼굴을 학습 중입니다. 몇 초가 걸릴 것입니다. 기다려 주세요...\")\nfaces,ids = getImagesAndLabels(path)\nrecognizer.train(faces, np.array(ids))\n# 모델을 trainer/trainer.yml에 저장하기\nrecognizer.write('trainer/trainer.yml') \n# 학습된 얼굴 수 및 프로그램 종료 출력\nprint(\"\\n [INFO] {0} 개의 얼굴을 학습했습니다. 프로그램을 종료합니다.\".format(len(np.unique(ids))))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n라즈베리파이에 PIL 라이브러리가 설치되어 있는지 확인해주세요. 만약 설치되어 있지 않다면, 아래의 명령을 터미널에서 실행해주세요:\n\n```js\npip install pillow\n```\n\n우리는 OpenCV 패키지에 포함된 LBPH (LOCAL BINARY PATTERNS HISTOGRAMS) 얼굴 인식기(recognizer)를 사용할 것입니다. 아래와 같이 코드를 작성해주세요:\n\n```js\nrecognizer = cv2.face.LBPHFaceRecognizer_create()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"getImagesAndLabels (path)\" 함수는 \"dataset/\" 디렉토리의 모든 사진을 가져와서 \"Ids\"와 \"faces\" 두 가지 배열을 반환합니다. 이 배열을 입력으로 사용하여 \"인식기를 학습\"할 것입니다:\n\n```js\nrecognizer.train(faces, ids)\n```\n\n이 결과로 \"trainer.yml\"이라는 파일이 우리가 이전에 생성한 트레이너 디렉토리에 저장됩니다.\n\n여기까지입니다! 훈련시킨 사용자의 얼굴 수를 확인하기 위해 마지막으로 출력 문을 포함했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n매번 Phase 1을 실행할 때마다 Phase 2도 실행해야 합니다.\n\n# 7. 인식기\n\n지금까지 우리 프로젝트의 마지막 단계에 도달했습니다. 여기서 우리는 카메라로 새로운 얼굴을 촬영하고, 이 사람이 이전에 그의 얼굴을 촬영하고 훈련했다면, 우리의 인식기는 \"예측\"을 수행하여 해당 ID와 일치 여부를 나타내는 지수를 반환합니다. 이로써 해당 일치에 대해 인식기가 얼마나 확신하는지 보여줍니다.\n\n![이미지](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_11.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGitHub에서 3단계 Python 스크립트를 내려받아봅시다: 03_face_recognition.py.\n\n```python\nimport cv2\nimport numpy as np\nimport os \nrecognizer = cv2.face.LBPHFaceRecognizer_create()\nrecognizer.read('trainer/trainer.yml')\ncascadePath = \"haarcascade_frontalface_default.xml\"\nfaceCascade = cv2.CascadeClassifier(cascadePath)\nfont = cv2.FONT_HERSHEY_SIMPLEX\n# 아이디 카운터 초기화\nid = 0\n# id에 대응하는 이름: 예) Marcelo: id=1,  등\nnames = ['없음', '마르셀로', '파울라', '일자', 'Z', 'W'] \n# 실시간 비디오 캡처 시작\ncam = cv2.VideoCapture(0)\ncam.set(3, 640) # 비디오 너비 설정\ncam.set(4, 480) # 비디오 높이 설정\n# 얼굴 인식으로 인정할 최소 윈도우 크기 정의\nminW = 0.1*cam.get(3)\nminH = 0.1*cam.get(4)\nwhile True:\n    ret, img =cam.read()\n    img = cv2.flip(img, -1) # 수직으로 뒤집기\n    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    \n    faces = faceCascade.detectMultiScale( \n        gray,\n        scaleFactor = 1.2,\n        minNeighbors = 5,\n        minSize = (int(minW), int(minH)),\n       )\n    for(x,y,w,h) in faces:\n        cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n        id, confidence = recognizer.predict(gray[y:y+h,x:x+w])\n        # 신뢰도가 100 미만이면 \"0\" : 완벽 일치\n        if (confidence \u003c 100):\n            id = names[id]\n            confidence = \"  {0}%\".format(round(100 - confidence))\n        else:\n            id = \"알수없음\"\n            confidence = \"  {0}%\".format(round(100 - confidence))\n        \n        cv2.putText(\n                    img, \n                    str(id), \n                    (x+5,y-5), \n                    font, \n                    1, \n                    (255,255,255), \n                    2\n                   )\n        cv2.putText(\n                    img, \n                    str(confidence), \n                    (x+5,y+h-5), \n                    font, \n                    1, \n                    (255,255,0), \n                    1\n                   )  \n    \n    cv2.imshow('camera',img) \n    k = cv2.waitKey(10) \u0026 0xff # 'ESC'를 눌러 비디오 종료\n    if k == 27:\n        break\n# 정리 작업\nprint(\"\\n [INFO] 프로그램 종료 및 정리 작업\")\ncam.release()\ncv2.destroyAllWindows()\n```\n\n여기에 새 배열을 추가했으므로, 숫자로 된 ID 대신 \"names\"를 표시할 것입니다:\n\n```python\nnames = ['없음', '마르셀로', '파울라', '일자', 'Z', 'W']\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, Marcelo는 id=1인 사용자이고, Paula는 id=2인 등입니다.\n\n다음으로, 우리는 얼굴을 감지할 것이며, 이전과 같이 haasCascade 분류기를 사용할 것입니다. 감지된 얼굴을 가지고 우리는 위의 코드에서 가장 중요한 함수를 호출할 수 있습니다:\n\n```js\nid, confidence = recognizer.predict(얼굴의 회색 부분)\n```\n\nrecognizer.predict()는 분석할 얼굴의 캡처된 부분을 매개변수로 사용하고, 해당 소유자와 일치에 대한 신뢰도를 나타내는 id 및 인식기의 신뢰도 값을 반환할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 마지막으로, 인식기가 얼굴을 예측할 수 있다면, 이미지 위에 “확률적 ID”와 일치가 올바른지에 대한 “확률”이 표시됩니다 (“확률” = 100 - 신뢰도 지수). 그렇지 않으면, “알 수 없음” 레이블이 얼굴 위에 표시됩니다.\n\n아래는 결과를 보여주는 gif입니다:\n\n![결과](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_12.png)\n\n이 사진에서는 이 프로젝트로 수행한 일부 테스트를 보여드립니다. 여기서 인식기가 작동하는지 확인하기 위해 사진을 사용했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_13.png\" /\u003e\n\n# 8. 결론\n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_14.png\" /\u003e\n\n언제나처럼, 이 프로젝트가 다른 사람들이 전자기술의 흥미로운 세계로 진입하는 데 도움이 되기를 바랍니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n세부 정보 및 최종 코드는 제 GitHub 저장소를 방문해주세요:\n\nhttps://github.com/Mjrovai/OpenCV-Face-Recognition\n\n더 많은 프로젝트를 보시려면 제 블로그를 방문해주세요: MJRoBot.org\n\n이른 아침 인사드립니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 글에서 봐요!\n\n감사합니다,\n\n마르셀로","ogImage":{"url":"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_0.png"},"coverImage":"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_0.png","tag":["Tech"],"readingTime":16},{"title":"이전에 사용하던 라즈베리 파이를 디지털 포토 프레임으로 재활용하기","description":"","date":"2024-06-19 06:10","slug":"2024-06-19-Up-cyclinganoldRaspberryPiintoadigitalphotoframe","content":"\n\n라즈베리 파이 프로젝트에 관한 몇 가지 이야기를 게시했어요. \"SaaS prepper\" 프로젝트 두 가지가 있어요: 하나는 플리커용이고, 다른 하나는 에버노트용이야. 그리고 후에는 휴 조명 자동화와 그에 대한 규칙 언어 추가도 있었어요. 모두 빌트인 와이파이가 있는 최신 모델 Pi에서 작동해.\n\n와이파이가 없는 구형 3 Pi 모델이 두 대 있어. 예전에 여러 프로젝트에 사용했지만 오랫동안 서랍 속에 방치되어 있었어. 정말 안쓰러워서, 저렴한 오프라인 프로젝트를 찾고 있었어.\n\n우리 둘째딸과 맏내아들은 매우 다른 관심사를 가지고 있어서, 각각에게 오프라인 사진액자를 만들어주면 방에 디지턈 장식물로서 멋진 효과를 줄 수 있을 것 같았어. 전체화면으로 슬라이드쇼를 열어서 디렉토리 안의 사진들을 랜덤하게 보여주면 될 거야. 9세 아들을 위한 것은 그의 고양이 사진, 예술품 스캔, 그리고 괴로운 미미들이겠지. 15세 딸을 위한 것은 앨범 커버, 영화 포스터, TikTok에서 찍은 포즈 사진, 가끔 친구들과 함께 한 인스타 셀카가 있겠지.\n\n돈을 많이 쓰기 싫었고, 프레임으로 사용하기에 적합한 상태의 중고 모니터가 많을 거라 생각했어. 디지털 입력이 있는 17\"~21\" 크기쯤의 뭔가를 찾을 수 있다면, 꽤 좋은 프레임이 될 것 같았어.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 파이 준비하기\n\n가장 저렴한 클래스 10 Micro SD 카드를 구했습니다. 클래스 10보다 느린 것은 라즈베리 파이 운영체제를 실행하는 데 잘 작동하지 않습니다. 뉴질랜드에 PB Tech 라는 지역 컴퓨팅 및 전자 제품 소매업자가 있습니다. 그들은 때로는 가격 조작을 당한다는 비판을 받기도 하지만, 나는 그들을 항상 최고로 생각했습니다. 그들의 물류는 훌륭하고 문제가 발생하면 반품 및 교환에 매우 도움이 됩니다. 더 좋은 점은 \"PB\"가 \"팻 \u0026 브렌다\"를 의미한다는 것입니다 - 그 창립 부부입니다. 그것만으로 충분히 충성심을 가질만한 이유입니다.\n\n![이미지](/assets/img/2024-06-19-Up-cyclinganoldRaspberryPiintoadigitalphotoframe_0.png)\n\nNZD $6.78은 USD $4.25 정도입니다 - 그 정도로 돈이 많이 나가지는 않겠네요. 요즘에는 32GB가 적당한 크기인 것 같습니다. 저는 최대 12GB 정도만 필요하지만, 그보다 더 많은 용량이 있다면 GB 당 가격이 더 비싸질 것 같네요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오피셜 라즈베리 파이 OS 사이트의 올인원 유틸리티는 요즘 정말 멋지네요. 이미지 파일을 다운로드하고 dd 구문을 뒤적일 필요 없이 모든 작업을 대신 해줍니다. 저는 최신 라즈베리 파이 OS 풀 데스크탑 버전으로 SD 카드를 플래싱했어요.\n\n# 모니터 찾기\n\n뉴질랜드에서 중고로 사고 파는 특이한 점은 우리만의 국내 플랫폼이 있다는 거예요. 예전에 이민 진입 가능성에 대해 이베이가 사용한 메트릭이 무엇이었든, 그들은 우리를 방치했죠. 그 때문에 우리에게 생겨난 건 TradeMe이라는 자체적인 경매 플랫폼인데, 2003년부터 사용해왔어요.\n\n제 집에서 차로 5분 거리 내의 리스트를 지켜보고 낮은 입찰을 넣은 후 1주일 정도 지켜보다가 어떤 경매에서 낙찰했어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Up-cycling an old Raspberry Pi into a digital photo frame](/assets/img/2024-06-19-Up-cyclinganoldRaspberryPiintoadigitalphotoframe_1.png)\n\n$11.50 NZD is approximately $7.20 USD. The resolution is adequate and it has a DVI input. My 9-year-old couldn't believe that such a budget-friendly purchase was possible!\n\n## A few more bits and pieces\n\nRaspberry Pis have an HDMI output, so I needed to convert that into the DVI format. Fortunately, I already had a 0.5m male-to-male HDMI cable that came with electronic items in a drawer. I only needed to buy an adapter to convert it to a female DVI-D.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Up-cycling an old Raspberry Pi into a digital photo frame](/assets/img/2024-06-19-Up-cyclinganoldRaspberryPiintoadigitalphotoframe_2.png)\n\nPB Tech가 다시 도와주었습니다. 32GB 마이크로 SD나 전자 모니터보다는 비싸지만 여전히 꽤 합리적인 가격입니다.\n\n물론 Pi에는 전원이 필요합니다. 뒷면에 USB에 최소 1.5암페어를 가진 모니터를 찾았지만, 이런 기능이 없는 거였다. Pi의 전원 공급은 약간 까다로울 수 있습니다. 전체 성능을 발휘하려면 공식 전원 공급이 정말 좋습니다. 이 가격은 합리적이지만, 마이크로 USB 플러그가 달린 구형 제품은 수급이 부족합니다 (새로운 모델은 USB-C를 사용합니다).\n\n아래 충전기는 이용하기 거의 완벽합니다. 대부분의 경우 잘 작동하지만 Pi가 무언가 I/O 또는 CPU 집약적인 작업을 수행해야 할 때 전압 경고가 발생하여 성능이 제한됩니다. 그래도 내 목적에는 괜찮다고 생각했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식으로 변환해 보겠습니다.\n\n\n![Image 1](/assets/img/2024-06-19-Up-cyclinganoldRaspberryPiintoadigitalphotoframe_3.png)\n\n그리고... $6.54 NZD: SD 카드보다 심지어 더 싸요.\n\n마지막 항목은 USB-A에서 USB-micro로 변환하는 것이었어요: $3.82. 제 서랍에 다 떨어져 있었던데요 — 이제는 USB-C 세상이에요.\n\n![Image 2](/assets/img/2024-06-19-Up-cyclinganoldRaspberryPiintoadigitalphotoframe_4.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 슬라이드 쇼 설정\n\nPi OS는 기본적으로 십 분 후에 화면을 꺼버립니다. 이 기능을 비활성화하려면 raspi-config의 \"Display Options\"에서 설정할 수 있습니다.\n\n빠른 구글링 결과, Linux 및 Pi 슬라이드 쇼에는 FEH가 필수적이라고 합니다. 기본 Apt 저장소에 있으므로 sudo apt-get install feh을 실행하여 Pi에 FEH를 추가했습니다.\n\n32GB SD 카드로 공간이 많이 남아 있어 Pi에 SSH를 활성화하고 라우터의 네트워크 소켓에 연결하여 큰 양의 사진을 scp로 전송했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nFEH 문서가 너무 최신이 아니라서 찾는 데 시간이 좀 걸렸어요. 제 요구 사항에 맞는 명령어를 찾는 데 조금 더 구글링을 해야 했어요. 다음과 같은 명령어를 사용했어요:\n\n```js\nDISPLAY=:0.0 feh --randomize --full-screen --slideshow-delay 30 --auto-rotate /home/pi/images\n```\n\n이 명령어는 다음을 제공해줘요:\n- X Windows의 첫 번째, 기본 가상 데스크톱\n- 디렉토리에서 랜덤 사진 선택\n- 전체 화면\n- 각 사진 변경 사이의 30초\n- 사진이 EXIF 태그에 포함된 방향에 따라 자동으로 회전됨 (기본적으로 그렇게 설정되어야겠죠!)\n- 기본 사용자의 홈 디렉토리의 \"images\" 디렉토리에서 읽기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금 작동 중인 슬라이드 쇼가 있으나 작업을 끝내고 네트워크에서 연결을 해제하고 그 일을 맡길 수 있도록 하기 전에 할 일이 몇 가지 더 남았어요.\n\n# 부팅 시 자동 시작\n\n키보드가 연결되지 않은 상태에서 사진 프레임이 작동해야 했기 때문에 OS가 부팅될 때 바로 슬라이드 쇼가 실행되어야 했어요. rc.local을 사용하는 것에 대한 제안을 보았지만, 그 스크립트가 X가 실제로 업 및 완전히 사용 가능해지기 전에 실행될 수 있기 때문에 매우 신뢰할 수 없다는 점을 알았어요. 따라서 Pi 데스크톱 관리자인 LXDE에 내장된 \"자동 시작\" 기능을 사용하기로 했어요.\n\n/home/pi/runshow.sh에 셸 스크립트의 명령어를 넣고 이와 함께 .config/autostart/photos 경로에 이 텍스트 파일을 만들었어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n[Desktop Entry]\nType=Application\nName=Slide show\nExec=/usr/bin/bash /home/pi/runshow.sh\n```\n\nLXDE가 X가 실행되었을 때 이를 읽고 명령을 실행합니다. 좋아요.\n\n# 마우스 커서 숨기기\n\n마우스가 없더라도 LXDE의 기본 동작은 화면 가운데에 마우스 포인터를 표시하는 것입니다. 짜증나죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다행히도, 이와 같은 거북한 상황을 다루는 기능이 기본 Apt 저장소에도 있습니다. 해당 튜토리얼에서 자세한 내용을 확인할 수 있지만, 제 간략한 버전은:\n\n```js\nsudo apt-get install unclutter\nmkdir -p ~/.config/lxsession/LXDE-pi\necho \"@unclutter -idle 0\" \u003e ~/.config/lxsession/LXDE-pi/autostart\n```\n\n# 데스크톱에서 저전압 경고 비활성화\n\n제 성능이 약간 떨어지지만 매우 검소한 전원 공급 구매를 기억하시나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n전압이 떨어질 때마다 화면 오른쪽 상단에 \"토스트\"가 나타나 경고로 팝업됩니다. 짜증나죠.\n\n다행히도, 이 경고는 전용 Apt 패키지에서 제공됩니다. 이것을 제거하면 그런 일이 일어나지 않습니다: sudo apt remove lxplug-ptbatt\n\n# 모두 끝났습니다\n\n이렇게 해서 50뉴질랜드 달러 미만으로 빈번하게 사용하지 않는 Pi를 재활용한 독립적인 사진 프레임을 만들었습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그 행복한 고양이를 보세요...\n\n![행복한 고양이](/assets/img/2024-06-19-Up-cyclinganoldRaspberryPiintoadigitalphotoframe_5.png)","ogImage":{"url":"/assets/img/2024-06-19-Up-cyclinganoldRaspberryPiintoadigitalphotoframe_0.png"},"coverImage":"/assets/img/2024-06-19-Up-cyclinganoldRaspberryPiintoadigitalphotoframe_0.png","tag":["Tech"],"readingTime":6},{"title":"라즈베리 파이 홀딩스 작지만 큰 영향력을 지닌 장치","description":"","date":"2024-06-19 06:09","slug":"2024-06-19-RaspberryPiHoldingsASmallDevicewithaBigImpact","content":"\n\n\n![Raspberry Pi](/assets/img/2024-06-19-RaspberryPiHoldingsASmallDevicewithaBigImpact_0.png)\n\n기술 세계에서 혁신은 성공의 열쇠이며, Raspberry Pi Holdings는 이 분야의 마스터임을 입증했습니다. 컴퓨터 과학자 에벤 업톤이 2008년에 설립한 Raspberry Pi는 학교와 개발도상국에서 기초 컴퓨터 과학 교육을 촉진하기 위한 작은 프로젝트로 시작했습니다. 겸손한 시작에서 회사는 지수적으로 성장하여 최근 런던 증권 거래소에서 성공적인 기업공개를 선보였습니다.\n\n소매 투자자들이 주식을 거래하기 시작함에 따라 Raspberry Pi 주가는 상당한 폭으로 상승하며, 회사의 인기와 제품에 대한 높은 수요를 반영했습니다. 이 회사의 IPO는 1억 6600만 파운드 또는 2억 2800만 달러를 모아내며, 투자자들은 회사의 보통 주식의 30.7%를 사들였으며, 결제 가능한 과잉 배분 조항 하에 추가로 2.3%를 구입할 수 있는 옵션을 획득했습니다. 주가는 IPO 당일 14% 상승하여 계속 상승세를 이어가고 있습니다. 이 상승은 회사의 강력한 재정 성과, 혁신적인 제품 라인, 그리고 165억 파운드에 이르는 산업, 내장, 애호가, 교육용 컴퓨팅 시장에서의 성장 잠재력과 관련이 있습니다.\n\n60만 대 이상이 판매된 Raspberry Pi 컴퓨터는 회사의 기술에 대한 혁신적인 접근을 증명하는 것입니다. 이 소형 컴퓨터는 취미가 있는 사람, 교사 및 전문가들에게 다양한 프로젝트에 대한 다목적 플랫폼을 제공하여 교육용 도구부터 복잡한 인터넷 연결 장치까지 다양한 분야에서 활용되고 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n라즈베리 파이 재단인 회사의 자선 단체는 교육과 지원에서 중요한 역할을 합니다. 수백 가지의 무료 코딩 및 컴퓨팅 프로젝트를 제공하며, 다양한 경험 수준에 맞는 지침을 제공하고 여러 언어로 자료를 제공합니다. 이 재단은 원격 수업을 통해 교사를 지원하고, 자원봉사자가 가상 코딩 클럽을 운영하도록 돕고, 집에 컴퓨터가 없는 어린이들에게 컴퓨터를 제공하는 데 힘씁니다. 이러한 노력을 통해 라즈베리 파이 재단은 전 세계 사람들에게 컴퓨팅과 디지털 제작의 힘을 전달하는 미션을 성취하고 있습니다.\n\n소규모 프로젝트에서 공개 회사로의 여정을 거쳐 라즈베리 파이 홀딩스는 성공의 놀라운 이야기입니다. 최근의 증권상장과 이어지는 주가 상승은 회사의 잠재력과 투자자들이 그 미션에 대한 신뢰를 명백히 보여줍니다. 라즈베리 파이가 혁신을 지속하고 영향력을 확대하는 동안, 이 회사는 분명히 컴퓨팅의 미래를 선도하는 중요한 역할을 할 것입니다.","ogImage":{"url":"/assets/img/2024-06-19-RaspberryPiHoldingsASmallDevicewithaBigImpact_0.png"},"coverImage":"/assets/img/2024-06-19-RaspberryPiHoldingsASmallDevicewithaBigImpact_0.png","tag":["Tech"],"readingTime":2},{"title":"물감을 만들어보세요","description":"","date":"2024-06-19 06:08","slug":"2024-06-19-MakeYourOwnWatercolorPaint","content":"\n\n## DIY / ART\n\n![Image](/assets/img/2024-06-19-MakeYourOwnWatercolorPaint_0.png)\n\n# 레시피를 찾고 있어요\n\n몇 년 전에 이츠리에서 수제 수채화물감을 사서, 수채화 물감 레시피에 대해 연구해 왔어요. 그 수채화 물감 중 일부가 아직도 있고 그것을 아주 좋아해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 궁금증은 항상 일어납니다. 무언가를 사는 대신에 직접 만들 수도 있다는 것을 깨달을 때 말이죠. 항상 성공하는 것은 아니지만, 시도하는 것이 재미 있어요.\n\n이 레시피는 시간을 들여 시도해 본 몇 가지 다른 레시피를 혼합한 것이고, 클로브 오일과 물 같은 제 개인적인 추가물도 포함되어 있어요.\n\n양을 약간 바꾸기도 했어요. 제가 찾은 대부분의 레시피보다 비술을 꽤 더 넣어요. 더 높은 농도로 모든 것을 더 잘 결합시킨다고 생각하기 때문이에요. 너무 많은 올리고당이 페인트를 끈적거리게 만들까 걱정했지만, 그렇지 않아요. 그림이 종이 위에 완전히 말랐어요.\n\n이 기사의 제목 이미지는 아래 DIY 수채화 페인트 레시피를 사용한 두 가지 예시를 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하루 마무리할 때, 이 레시피는 절대적인 것이 아닙니다. 여러분의 변형은 나열된 재료들 중 어느 것이든 더 많거나 더 적게 필요할 수 있습니다. 자유롭게 시도해보세요. 무엇이 잘 작용하는지 확인해보세요.\n\n# 페인트 만들기\n\n재료:\n\n- 식용색소\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n테이블 태그를 Markdown 형식으로 변경하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1큰술 물\n\n추가:\n\n위의 섞인 재료 1큰술 당 ½ 작은술의 경미한 옥수수 시럽 - 4.5 작은술\n\n재료를 섞는 방법:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n베이킹 소다를 그릇에 넣고 식초를 추가하세요. 이 작업은 과학 실험처럼 거품이 일어날 거에요, 그러니 거품이 잘 포옹될 수 있도록 충분히 큰 그릇을 사용해 주세요.\n\n저어주세요.\n\n물과 크로브 오일을 추가하세요.\n\n저어주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n옥수수 전분을 넣어주세요.\n\n지속적으로 저어주세요.\n\n용기에 따라 붓습니다. 저는 달러 스토어에서 구입한 1 TBS 크기의 작은 용기를 사용했지만, 알약 정리함을 사용해도 됩니다.\n\n믹스 1 TBS 당 식품 염색약을 최소 2방울씩 넣어주세요. 그러나 선택은 여러분의 몫이에요. 더 많은 식품 염색약을 추가하면 더 짙은 색조를 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n젤 식용색소도 괜찮아요. 케이크 데코용 젤을 시도해봤는데 잘 작동하지 않았어요. 저는 액체 식용색소를 선호해요.\n\n참고: 이 작업은 두 날이 걸려요.\n\n물, 옥수수 시럽, 식용색소는 위로 떠오르고, 베이킹 소다와 옥수수 전분은 가라앉아요.\n\n![이미지](/assets/img/2024-06-19-MakeYourOwnWatercolorPaint_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n미물을 몇 시간마다 섞어주어야 할 거에요 - 하루에 세 번정도인 거 같아요.\n\n둘째 날에는 내가 그림에 사용할 페인트를 팔레트 트레이에 붓고 하루를 더 말려놓았어요. 그랬더니 딱딱한 케이크 모양으로 말려있었답니다. 이 작은 페인트 팔레트 트레이들을 달러 상점에서 구했어요.\n\n## 동영상\n\n이 동영상을 만들어서 페인트 만드는 모습을 시청하고 싶다면 보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 그림칠 시간\n\n페인트를 사용하려면 물로 재수분화하고, 팬에 담긴 수채화 페인트를 사용하는 것처럼 사용하십시오.\n\n이 페인트의 유통 기한을 모르겠지만, 세기오일을 추가하고 서늘하고 건조한 곳에 보관한다면 몇 달 동안 좋을 것 같습니다.\n\n이를 시도해 보거나 다른 집에서 만든 페인트 레시피를 시도해 본 적이 있다면 알고 싶습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 팁:\n\n음식색을 섞는 데 사용할 수 있는 색상이 제한적이라면, 서로 다른 색조를 섞어 보세요. 예를 들어, 노랑과 빨강을 섞어 주황색을 만들 수 있습니다.\n\n또는 이미 섞인 초록색 페인트에 노랑 음식색을 추가하여 다른 색조의 초록을 만들 수도 있어요. 이해가 되시나요? 궁금하신 점이 있다면 댓글로 글을 남겨 주세요.\n\n# 온라인에서 저에게 방문해주세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n유튜브 | 에츠이 | 웹사이트 | 티퍼블릭","ogImage":{"url":"/assets/img/2024-06-19-MakeYourOwnWatercolorPaint_0.png"},"coverImage":"/assets/img/2024-06-19-MakeYourOwnWatercolorPaint_0.png","tag":["Tech"],"readingTime":3},{"title":"달콤한 증말 커뮤니티","description":"","date":"2024-06-19 06:07","slug":"2024-06-19-DolceFavorCrate","content":"\n\n🔍 지금 Dolce Favor Crate를 확인해보세요! 📁 생일 3D SVG 공예를 둘러보고 여기서 🆓 무료 다운로드를 받으세요. 함께 크래프팅을 시작해봐요! 🚀\n\n이 품질 높은 SVG 템플릿은 마종 그레고리아에 의해 만들어졌어요. 소중한 사람에게 얼마나 신경쓰는지 보여줄 수 있는 완벽한 방법을 찾고 계신가요? 달체 페이버 크레이트를 확인해보세요! 이 크레이트는 어떤 행사에도 아름다운 종이 선물을 만들 수 있는 모든 것으로 가득한데요. 저희 SVG 템플릿을 사용하면 쉽게 모든 요소를 만들 수 있어요: — 종이 크레이트 — 테디 베어 — 샴페인 상자 — 미니 장미 식물 — 컵케이크 선물 상자 그리고 비디오 튜토리얼 링크로, 이 프로젝트들을 만드는 방법을 더욱 쉽게 배울 수 있어요. 이 파일들은 Silhouette Studio, Cricut Design Space, 그리고 이러한 파일 유형을 사용하는 다른 커팅 소프트웨어에서 사용하도록 설계되었어요. 이 디자인은 SVG 형식으로 제공됩니다. 그럼 무엇을 기다리고 계신가요? 지금 당신의 Dolce Favor Crate를 주문하세요!\n\n![Dolce Favor Crate 이미지 0](/assets/img/2024-06-19-DolceFavorCrate_0.png)\n\n![Dolce Favor Crate 이미지 1](/assets/img/2024-06-19-DolceFavorCrate_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-06-19-DolceFavorCrate_2.png)\n\n![Image 2](/assets/img/2024-06-19-DolceFavorCrate_3.png)\n\n![Image 3](/assets/img/2024-06-19-DolceFavorCrate_4.png)\n\nPhoto by 3D SVG Crafts on Creative Fabrica\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n면책 공지: 본 문서에는 제휴 링크가 포함되어 있습니다. 이는 해당 링크를 통해 구매를 하실 경우 추가 비용 없이 수수료를 받을 수 있음을 의미합니다.","ogImage":{"url":"/assets/img/2024-06-19-DolceFavorCrate_0.png"},"coverImage":"/assets/img/2024-06-19-DolceFavorCrate_0.png","tag":["Tech"],"readingTime":2},{"title":"Raspberry Pi 5 비디오 스트림 지연 시간 비교 UDP, TCP, RTSP, 그리고 WebRTC","description":"","date":"2024-06-19 06:05","slug":"2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC","content":"\n\n라즈베리 파이 5를 위한 최적의 라이브 스트리밍 옵션을 발견해보세요. 비디오 라이브 스트림 지연 시간을 테스트하고 목록 중에서 최고를 선택하겠습니다.\n\n![이미지](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_0.png)\n\n라즈베리 파이와 관련된 많은 프로젝트는 \"눈\", 다시 말해 카메라가 필요합니다. 로봇이나 원격 제어 자동차와 같은 것을 조종하기 위해서죠.\n\n본문에서는 비디오 라이브 스트리밍의 지연 시간을 테스트할 것입니다. 궁극적으로, 우리는 200ms 정도의 지연을 가진 솔루션을 갖게 될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비디오 형식의 기사를 실시간으로 시청하는 것이 더 좋습니다:\n\n왜 영상 스트림의 레이턴시가 중요한가요?\n\n물론, 그것은 작업에 따라 다릅니다. 창문 밖에있는 새를 녹음한다면 레이턴시는 중요하지 않습니다. 아니면 아름다운 폭포 영상을 시청할 때도요.\n\n그러나 FPV 드론 조종, 레이싱 카, 또는 원격 제어 박싱 로봇을 조정할 때 레이턴시는 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사는 각 프로토콜이 작동하는 방식이나 이러한 지연이 왜 발생하는지에 대한 내용은 아닙니다.\n\n이 기사는 우리의 옵션을 검토하고 최소한의 산란 없이 구현하는 방법을 살펴볼 것입니다. 심층적인 프로그래밍 지식이 없는 사람도 설정하고 사용할 수 있습니다.\n\n# 하드웨어\n\n하드웨어에 대해 말할 것이 별로 없습니다. 가능한 한 간단합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_1.png)\n\n- 라즈베리 파이 5\n- 카메라 케이블\n- 카메라 모듈 3 (와이드 에디션)\n\n운영 체제로는 최신 공식 Debian Bookworm 포트를 사용하고 있습니다. 사용자 지정 설정은 없으며 모든 것은 공식 라즈베리 스토어와 라즈베리 파이 이미저에서 찾을 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_2.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n공식 라즈베리 카메라 문서로 시작해서 네트워크 스트림 권장 사항을 시도해 봅시다.\n\n![image](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_3.png)\n\n[라즈베리 파이 카메라 소프트웨어 문서](https://www.raspberrypi.com/documentation/computers/camera_software.html)에 접속해 봅시다.\n\n왼쪽 메뉴에 rpicam-vid 아이템이 있습니다. 해당 항목을 누르면 페이지가 필요한 정보로 스크롤됩니다. '네트워크 스트리밍' 섹션으로 조금 내려가서 UDP 스트림을 시도해 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 개의 터미널을 사용할 거에요. 하나는 SSH를 통해 라즈베리 파이에 연결하고, 다른 하나는 로컬 머신에서 명령을 실행할 거에요.\n\n# Raspberry PI에서 네이티브 코덱을 사용한 UDP 비디오 스트림\n\nhttps://www.raspberrypi.com/documentation/computers/camera_software.html#udp\n\n라즈베리 파이에서 스트림을 시작하는 명령어는 다음과 같아요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nrpicam-vid -t 0 --width 1280 --height 720 --framerate 30 --inline -o udp://노트북_IP_여기에:5555\n```\n\n라즈베리 파이의 LAPTOP_IP_HERE를 노트북의 IP 주소로 대체해야 합니다.\n\n포트는 임의로 설정할 수 있습니다. 여기서는 5555를 사용했습니다.\n\n위와 같이 라즈베리 파이가 몇 프레임을 노트북으로 보내주는 것을 볼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_4.png\" /\u003e\n\n비디오를 재생하려면 노트북에서 다음 명령어를 실행할 수 있어요:\n\n```js\nffplay udp://RASPBERRY_PI_IP_HERE:5555 -fflags nobuffer -flags low_delay -framedrop\n```\n\n이제 성능을 확인해 볼 시간이에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n실시간으로 타이머를 기록하고 지역 네트워크를 통해 스트림을 전송하는 카메라가 있습니다.\n\n![image](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_5.png)\n\nUDP 비디오 전송 방식은 3,400밀리초의 지연이 있다는 것을 알 수 있습니다.\n\n큰 지연입니다. 예를 들어, 초속 40마일(약 64킬로미터)로 이동하는 차량은 3.4초 동안 66.49야드(60미터)를 이동합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 축구장에서 66야드가 어떻게 보이는지에 대한 내용이에요. 정말 인상적죠.\n\n![66 yards on a football field](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_6.png)\n\n드론과 같은 고속 장치를 제어할 때 신뢰할 수 있는 것은 아닙니다.\n\n# Raspberry PI 5 TCP 비디오 스트림 네이티브 코덱\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n라즈베리 파이에서 스트리밍하는 명령어는 다음과 같습니다:\n\n```js\nrpicam-vid -t 0 --width 1280 --height 720 --framerate 30 --inline --listen -o tcp://0.0.0.0:5556\n```\n\n라즈베리 파이는 자체 포트(5556으로 설정함)로 스트리밍을 생성하며, 클라이언트는 연결하여 스트림을 받을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n보시다시피, 일부 프레임을 보내고 수신자가 스트림을 받을 때까지 멈춰있습니다.\n\n![이미지](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_7.png)\n\n노트북에서 스트림을 표시하려면 다음을 사용할 수 있습니다:\n\n```js\nffplay tcp://라즈베리파이_IP_여기에:5556 -vf \"setpts=N/30\" -fflags nobuffer -flags low_delay -framedrop\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 TCP 비디오 스트리밍 방법은 UDP보다 훨씬 나은 반초의 지연만 있습니다.\n\n![이미지](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_8.png)\n\n이 지연을 초당 40마일 주행하는 자동차에 적용해보겠습니다.\n\n![이미지](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_9.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5 초 동안, 자동차는 10야드(또는 9미터)만 이동합니다.\n\n# Raspberry PI 5 RTSP 비디오 스트림 네이티브 코덱\n\n이제 RTSP 스트림을 테스트해 보겠습니다.\n\n문서에서 제공된 명령어:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nrpicam-vid -t 0 --inline -o - | cvlc stream:///dev/stdin --sout '#rtp{sdp=rtsp://:8554/stream1}' :demux=h264\n```\n\n그리고.....\n\n![image](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_10.png)\n\n...... 네. 작동하지 않아요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n테이블 태그를 마크다운 형식으로 변경해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Raspberry PI 5에서 libav 코덱(mpegts)을 사용한 TCP 비디오 스트리밍\n\nhttps://www.raspberrypi.com/documentation/computers/camera_software.html#network-streaming-with-libav\n\n이것은 다른 비디오 코덱이며, 지연에 영향을 줄 수 있습니다.\n\nRaspberry PI에서 실행할 명령어:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nrpicam-vid -t 0 --width 1280 --height 720 --framerate 30 --codec libav --libav-format mpegts --libav-audio -o \"tcp://0.0.0.0:1234?listen=1\"\n```\n\n노트북에서 재생하는 명령어:\n\n```js\nffplay tcp://라즈베리파이IP주소:1234 -vf \"setpts=N/30\" -fflags nobuffer -flags low_delay -framedrop\n```\n\n의외로 LibAv는 큰 딜레이가 있습니다 — 약 10.5초가 됩니다. 기본 코덱과 TCP로는 0.5초였으나, 이제 10.5초가 소요됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_11.png)\n\n차량에 적용하면 그 지연 시간으로 인해 차량이 필드 밖으로 멀리 나갈 것을 볼 수 있습니다. 200야드 또는 190미터입니다. 농담이 아닙니다.\n\n![Image](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_12.png)\n\n# Raspberry PI 5 UDP 비디오 스트림과 libav 코덱 (mpegts)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n라즈베리 파이에서 실행해야 하는 명령어:\n\n```js\nrpicam-vid -t 0 --width 1280 --height 720 --framerate 30 --codec libav --libav-format mpegts --libav-audio  -o \"udp://REPLACE_WITH_LAPTOP_IP:5555\"\n```\n\n라이플랩을 위한 플레이어:\n\n```js\nffplay tcp://RASPBERRY_PI_IP_HERE:1234 -vf \"setpts=N/30\" -fflags nobuffer -flags low_delay -framedrop\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 UDP에 대한 지연 시간은 반으로 줄었습니다.\n\n![이미지](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_13.png)\n\n이 모든 정보를 축구장에 추가합시다.\n\n![이미지](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_14.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 결과들이 정말 이상해요. 원시 코덱을 사용하면 UDP에서는 큰 지연이 있고 TCP에서는 작은 지연이 있었고, LibAV를 사용하면 그 반대로 되었어요.\n\n하지만 여전히 0.5초보다 빠른 속도를 내볼 수 있어요.\n\n# Raspberry PI 5 MediaMTX 설정\n\n마침내, 우리는 Raspberry PI에서 스트림 지연 우승자에 가까워졌어요: MediaMTX입니다. 이 소프트웨어는 이전 비디오에서 언급된 모든 프로토콜과 몇 가지 더 사용하여 스트림을 전송할 수 있어요. 설정부터 시작해봐요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n설치하려면 정말 간단한 단계를 따라야 해요:\n\n- 먼저 GitHub의 릴리스 페이지를 열어주세요.\n- 두 번째로, ARM64 아카이브 링크를 복사해주세요. 적절한 버전을 선택하는 것이 중요해요.\n\n![이미지](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_15.png)\n\n- 세 번째로, Raspberry PI에 해당 폴더를 생성해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sh\nmkdir mediamtx \u0026\u0026 cd mediamtx\n```\n\n다음으로, GitHub에서 최신 링크를 찾아 WGET 명령어를 사용하여 다운로드합니다.\n\n```sh\nwget https://github.com/bluenviron/mediamtx/releases/download/v1.7.0/mediamtx_v1.7.0_linux_arm64v8.tar.gz\n```\n\n그 다음, 동일한 폴더에 압축을 푸세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ntar -xvzf mediamtx_v1.7.0_linux_arm64v8.tar.gz\n```\n\n- 여섯 번째로, YML 구성 파일을 편집하려면 엽니다.\n\n```js\nnano mediamtx.yml\n```\n\n- 일곱 번째로, 아래로 스크롤하고 이 구성을 붙여넣습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ncam1:\n    runOnInit: bash -c 'rpicam-vid -t 0 --camera 0 --nopreview --codec yuv420 --width 1280 --height 720 --inline --listen -o - | ffmpeg -f rawvideo -pix_fmt yuv420p -s:v 1280x720 -i /dev/stdin -c:v libx264 -preset ultrafast -tune zerolatency -f rtsp rtsp://localhost:$RTSP_PORT/$MTX_PATH'\n    runOnInitRestart: yes\n```\n\n간단히 말해서, MediaMTX 소프트웨어는 이 명령어를 bash에서 실행할 겁니다.\n\n그 다음 RPICAM-VID 명령어에게 (이전에 다뤘던 것과 동일하게) 스트림을 FFMPEG로 보내도록 요청할 겁니다.\n\nFFMPEG은 그것을 RTSP 프로토콜을 통해 로컬로 MediaMTX로 보낼 겁니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n구성을 저장하고 MediaMTX를 실행할 수 있어요.\n\n```js\n./mediamtx\n```\n\n처음 실행 시 프로토콜 및 사용 가능한 포트에 대한 유용한 정보가 표시될 거예요.\n\n![이미지](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_16.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Raspberry PI 5 RTSP 비디오 스트림 및 mediaMTX\n\nVLC 플레이어 및 RTSP 스트림으로 시도해 봅시다.\n\n노트북에서 다음 명령을 사용하여 vlc 플레이어를 실행할 수 있습니다:\n\n```js\nvlc rtsp://라즈베리파이_IP_주소_여기에:8554/cam1\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당신은 개발자이다. 위의 텍스트를 친근하게 한국어로 번역해주시겠어요?\n\n잠시 후에는 스트림이 표시됩니다.\n\n![stream](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_17.png)\n\n이 유형의 스트림은 1.3초의 지연이 있습니다. 이 번호를 축구장에 추가해 봅시다.\n\n![football field](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_18.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Raspberry PI 5 WebRTC 비디오 스트림과 mediaMTX\n\n드디어, 당첨에 한 발짝 더 다가갔습니다. 콘솔로 돌아가서 WebRTC용 포트를 확보해 봅시다. 제 경우에는 8889번 포트를 사용하고 있습니다.\n\n![이미지](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_19.png)\n\n이를 재생하려면, 브라우저에서 이 스트림을 열어보세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nhttp://RASPBERRY_PI_IP_HERE:8889/cam1\n```\n\n금방 스트림이 제공될 예정이에요. \n\n전체 화면으로 변경해보고, 지연 시간을 확인해 보세요.\n\n![Raspberry Pi Video Stream Latencies Comparing UDP, TCP, RTSP, and WebRTC](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_20.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n테이블 태그를 마크다운 형식으로 변경하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 모든 결과가 하나의 표에 있습니다.\n\n![표](/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_22.png)\n\nFPV 드론과 함께 스트림을 사용할 예정이라면, 200밀리초보다 10배 낮은 지연 시간이 필요합니다. 약 20-30밀리초 정도어야 합니다.\n\n제 필요성에 따라 200밀리초의 지연은 무의미하며, MediaMTX를 사용하여 계획을 실행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 낮은 지연을 원하신다면 WebRTC 스트림 조정이나 프레임 속도 감소, 그리고 다른 매개변수 조작을 시도해보세요.\n\n이 글이 낮은 지연 라이브 스트림을 갖춘 흥미로운 프로젝트를 구축하는 데 도움이 되기를 바랍니다.\n\nYouTube 채널의 댓글에서 의견을 남겨주세요. 또한 이와 유사한 정보가 필요하다면 말씀해주세요!\n\n읽어주셔서 감사합니다!","ogImage":{"url":"/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_0.png"},"coverImage":"/assets/img/2024-06-19-RaspberryPi5VideoStreamLatenciesComparingUDPTCPRTSPandWebRTC_0.png","tag":["Tech"],"readingTime":10}],"page":"85","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"85"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>