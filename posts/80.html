<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/80" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/80" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="리눅스 보안 여정 - 안전한 실행 모드" href="/post/2024-06-19-TheLinuxSecurityJourneySecureExecutionMode"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="리눅스 보안 여정 - 안전한 실행 모드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-TheLinuxSecurityJourneySecureExecutionMode_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="리눅스 보안 여정 - 안전한 실행 모드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">리눅스 보안 여정 - 안전한 실행 모드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="리눅스 디렉터리 구조" href="/post/2024-06-19-LinuxDirectoryStructure"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="리눅스 디렉터리 구조" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-LinuxDirectoryStructure_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="리눅스 디렉터리 구조" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">리눅스 디렉터리 구조</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="컨테이너가 스왑 공간을 사용할 수 있을까요" href="/post/2024-06-19-CanContainersUseSwapSpace"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="컨테이너가 스왑 공간을 사용할 수 있을까요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-CanContainersUseSwapSpace_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="컨테이너가 스왑 공간을 사용할 수 있을까요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">컨테이너가 스왑 공간을 사용할 수 있을까요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="가이드 Nuclei 사용법" href="/post/2024-06-19-GuidetoUsingNuclei"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="가이드 Nuclei 사용법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-GuidetoUsingNuclei_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="가이드 Nuclei 사용법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">가이드 Nuclei 사용법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="SQL에 대한 설명 랭킹 분석" href="/post/2024-06-19-SQLExplainedRankingAnalytics"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="SQL에 대한 설명 랭킹 분석" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-SQLExplainedRankingAnalytics_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="SQL에 대한 설명 랭킹 분석" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">SQL에 대한 설명 랭킹 분석</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">15<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="제목 제로부터 dbt까지 스포티파이의 백만 개 플레이리스트 데이터를 분석하고 데이터 모델 구축하는 방법" href="/post/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="제목 제로부터 dbt까지 스포티파이의 백만 개 플레이리스트 데이터를 분석하고 데이터 모델 구축하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="제목 제로부터 dbt까지 스포티파이의 백만 개 플레이리스트 데이터를 분석하고 데이터 모델 구축하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">제목 제로부터 dbt까지 스포티파이의 백만 개 플레이리스트 데이터를 분석하고 데이터 모델 구축하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터, AI 서밋에서 얻은 교훈 파트 II" href="/post/2024-06-19-DataAISummitTakeawaysPartII"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터, AI 서밋에서 얻은 교훈 파트 II" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-DataAISummitTakeawaysPartII_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터, AI 서밋에서 얻은 교훈 파트 II" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터, AI 서밋에서 얻은 교훈 파트 II</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 " href="/post/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 </strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="DBT  Airflow  " href="/post/2024-06-19-dbtAirflow"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="DBT  Airflow  " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-dbtAirflow_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="DBT  Airflow  " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">DBT  Airflow  </strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">16<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 엔지니어를 위한 자료 구조와 알고리즘 면접 질문" href="/post/2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 엔지니어를 위한 자료 구조와 알고리즘 면접 질문" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 엔지니어를 위한 자료 구조와 알고리즘 면접 질문" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 엔지니어를 위한 자료 구조와 알고리즘 면접 질문</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/61">61</a><a class="link" href="/posts/62">62</a><a class="link" href="/posts/63">63</a><a class="link" href="/posts/64">64</a><a class="link" href="/posts/65">65</a><a class="link" href="/posts/66">66</a><a class="link" href="/posts/67">67</a><a class="link" href="/posts/68">68</a><a class="link" href="/posts/69">69</a><a class="link" href="/posts/70">70</a><a class="link" href="/posts/71">71</a><a class="link" href="/posts/72">72</a><a class="link" href="/posts/73">73</a><a class="link" href="/posts/74">74</a><a class="link" href="/posts/75">75</a><a class="link" href="/posts/76">76</a><a class="link" href="/posts/77">77</a><a class="link" href="/posts/78">78</a><a class="link" href="/posts/79">79</a><a class="link posts_-active__YVJEi" href="/posts/80">80</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"리눅스 보안 여정 - 안전한 실행 모드","description":"","date":"2024-06-19 14:57","slug":"2024-06-19-TheLinuxSecurityJourneySecureExecutionMode","content":"\n\n일반적으로 \"Secure Execution Mode\"에서는 보조 벡터의 \"AT_SECURE\" 항목에 0이 아닌 값이 포함되어 있는 경우 이진 파일이 실행됩니다. LSM(https://medium.com/@boutnaru/linux-security-lsm-linux-security-modules-907bbcf8c8b4)이 값을 설정한 경우, 작업/프로세스의 \"실제 UID\"(https://medium.com/@boutnaru/the-linux-security-journey-ruid-real-user-id-b23abcbca9c6) 및 \"유효 UID\"(https://medium.com/@boutnaru/the-linux-security-journey-euid-effective-user-id-65f351532b79)가 다른 경우(그룹 값도 동일)에도 이 값을 0이 아닌 값으로 만들 수 있습니다. 또한, 일반 사용자가 실행한 이진 파일이 프로세스에 권한을 부여했을 경우에도 이 값이 0이 아닌 값이 됩니다(https://man7.org/linux/man-pages/man8/ld.so.8.html).\n\n전반적으로, 안전한 실행 모드는 동적 링커/로더의 기능입니다. 이 모드가 활성화되어 있는 경우 특정 환경 변수가 이진 파일을 실행할 때 무시됩니다. 이러한 변수의 예로는 \"LD_LIBRARY_PATH\", \"LD_DEBUG\"(단, /etc/suid-debug가 있는 경우는 제외), \"LD_DEBUG_OUTPUT\", \"LD_DEBUG_WEAK\"(glibc 2.3.4부터), \"LD_ORIGIN_PATH\", \"LD_PROFILE\"(glibc 2.2.5부터), \"LD_SHOW_AUXV\"(glibc 2.3.4부터) 및 \"LD_AUDIT\"이 있습니다(https://manpages.ubuntu.com/manpages/focal/en/man8/ld.so.8.html) — 아래 스크린샷에서 확인할 수 있습니다.\n\n마지막으로, 안전한 실행 모드의 목표는 \"setuid\"/\"setgid\"로 실행할 수 있는 이진 파일이 임의의 코드를 로드/실행하는 능력을 차단하여 특권 상승을 수행하는 것을 방지하는 것입니다(사용자 한 명이 실행하지만 다른 사용자(루트일 수도 있음) 권한으로 실행될 때).\n\n다음 글에서 만나요 ;-) 트위터에서 제 소식을 확인하려면 @boutnaru(https://twitter.com/boutnaru)를 팔로우해주세요. 또한, 미디엄(https://medium.com/@boutnaru)에서 다른 글도 읽어볼 수 있습니다. 무료 eBook은 https://TheLearningJourneyEbooks.com에서 찾아볼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![2024-06-19-TheLinuxSecurityJourneySecureExecutionMode](/assets/img/2024-06-19-TheLinuxSecurityJourneySecureExecutionMode_0.png) \n","ogImage":{"url":"/assets/img/2024-06-19-TheLinuxSecurityJourneySecureExecutionMode_0.png"},"coverImage":"/assets/img/2024-06-19-TheLinuxSecurityJourneySecureExecutionMode_0.png","tag":["Tech"],"readingTime":2},{"title":"리눅스 디렉터리 구조","description":"","date":"2024-06-19 14:55","slug":"2024-06-19-LinuxDirectoryStructure","content":"\n\n# Linux 디렉토리 구조에 대한 자세한 설명\n\n# 부팅 프로세스\n\n부팅 로더 (/boot)\n\n- Linux 시스템이 시작되면 BIOS 또는 UEFI 펌웨어가 하드웨어를 초기화하고 부팅로더(예: GRUB)를 /boot 디렉토리에서 불러옵니다.\n- 부팅로더는 커널(/boot/vmlinuz)과 초기 RAM 디스크(/boot/initrd.img)를 메모리에 불러옵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n커널 초기화 (/proc, /sys)\n\n- 커널은 시스템 구성 요소를 초기화하고 루트 파일 시스템을 마운트합니다.\n- 초기화 과정에서 커널은 프로세스와 시스템 하드웨어에 관한 정보를 /proc 및 /sys에 채웁니다.\n\n# 시스템 초기화\n\n시스템 및 서비스 초기화 (/etc, /lib, /sbin, /run)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 커널이 초기화된 후에는 부팅 프로세스를 담당하는 init 프로세스(또는 현대의 Linux 시스템에서는 systemd)가 시작됩니다.\n- systemd는 /etc/systemd 및 /etc의 다른 디렉토리에서 구성 파일을 읽어 필요한 시스템 서비스를 시작합니다.\n- 부팅 및 시스템 서비스 실행에 필요한 필수 라이브러리는 /lib에 위치합니다.\n- 시스템 관리 이진 파일은 /sbin 및 /usr/sbin에 있어 서비스를 관리하는 데 도움이 됩니다.\n- /run은 시스템 작동 중에 일시적인 시스템 정보를 저장하는 데 사용됩니다.\n\n# 사용자 환경 설정\n\n사용자 환경 (/home, /usr, /opt)\n\n- 시스템 서비스가 시작되면 사용자가 로그인하여 개인 환경이 설정됩니다.\n- 각 사용자는 /home 아래에 홈 디렉토리를 갖고 있어 개인 파일과 설정이 저장됩니다.\n- 사용자 응용 프로그램 및 도구는 일반적으로 /usr/bin 및 /usr/local/bin에 저장됩니다.\n- 선택적 및 제3자 응용 프로그램은 /opt에 설치됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 시스템 작동\n\n장치 관리 (/dev)\n\n- /dev는 하드 드라이브, 터미널, 프린터 등의 하드웨어 구성 요소를 나타내는 장치 파일을 포함합니다.\n- 장치 파일을 통해 소프트웨어가 표준 입력/출력 작업을 사용하여 하드웨어와 상호 작용할 수 있습니다.\n\n구성 및 관리 (/etc)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- /etc에 있는 설정 파일들은 시스템 동작, 네트워크 설정, 사용자 계정 정보, 서비스 구성을 정의합니다.\n- 관리자들은 이러한 파일을 편집하여 시스템 설정을 관리합니다.\n\n변수 데이터 (/var)\n\n- 시스템과 애플리케이션이 생성한 로그, 캐시, 스풀 파일 등과 같은 동적 데이터는 /var에 저장됩니다.\n- 로그 파일 (/var/log)은 시스템 및 애플리케이션 활동을 기록하여 모니터링 및 문제 해결을 위해 사용됩니다.\n- 메일 스풀 (/var/spool/mail)은 수신된 이메일을 저장합니다.\n\n임시 파일 (/tmp, /var/tmp)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 애플리케이션이 /tmp 및 /var/tmp에 임시 파일을 저장합니다.\n- /tmp에있는 파일은 일반적으로 재부팅 시 삭제되지만, /var/tmp에있는 파일은 다시 부팅해도 유지됩니다.\n\n# 예: 사용자 애플리케이션 실행\n\n애플리케이션 실행\n\n- 사용자는 /usr/bin이나 /usr/local/bin에서 애플리케이션을 실행합니다.\n- 해당 애플리케이션은 /lib이나 /usr/lib에서 공유 라이브러리를 로드할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하드웨어와 상호 작용하기\n\n- 애플리케이션이 하드웨어와 상호 작용해야 할 때, /dev 디렉토리에 해당 장치 파일에 접근합니다.\n\n사용자 데이터 저장 및 접근\n\n- 사용자별 데이터 및 구성은 사용자의 홈 디렉토리 (/home/사용자이름)의 파일에서 읽거나 쓰여집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로그 및 임시 데이터\n\n- 애플리케이션은 활동을 /var/log에 있는 파일에 기록할 수 있습니다.\n- 애플리케이션이 생성한 임시 데이터는 /tmp 또는 /var/tmp에 저장됩니다.\n\n# 시스템 유지 보수\n\n소프트웨어 업데이트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 시스템 및 응용 프로그램 업데이트는 /bin, /sbin, /lib, /usr/bin, /usr/sbin 또는 /opt에 새 파일을 설치할 수 있습니다.\n\n구성 변경\n\n시스템 관리자는 시스템 또는 서비스 동작을 변경하기 위해 /etc에 있는 구성 파일을 편집할 수 있습니다.\n\n모니터링 및 문제 해결\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`/var/log`에 있는 로그는 시스템 상태를 모니터링하고 문제를 해결하기 위해 검토됩니다.\n\n아키텍처\n\n![Image](/assets/img/2024-06-19-LinuxDirectoryStructure_0.png)\n\n디렉토리 구조 개요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n/\n├── bin\n├── boot\n├── dev\n├── etc\n├── home\n│ └── user\n├── lib\n├── media\n├── mnt\n├── opt\n├── proc\n├── root\n├── run\n├── sbin\n├── srv\n├── sys\n├── tmp\n├── usr\n│ ├── bin\n│ ├── lib\n│ ├── local\n│ └── sbin\n└── var\n ├── log\n ├── spool\n └── tmp\n\n\n\n|- / (Root)\n\n설명: 모든 다른 디렉토리가 뻗어나온 최상위 디렉토리입니다.\n예시: /bin, /etc, /home은 모두 /의 하위 디렉토리입니다.\n\n\n\n|- /bin\n\n설명: 모든 사용자가 필요로 하는 기본 명령어와 도구를 위한 필수 바이너리 실행 파일이 들어 있습니다.\n예시:\n/bin/ls (디렉토리 내용 나열)\n/bin/bash (Bourne Again Shell)\n/bin/cp (파일과 디렉토리 복사)\n\n\n\n|- /boot\n\n설명: 부트로더 파일과 커널 이미지가 들어 있습니다.\n예시:\n/boot/vmlinuz (리눅스 커널)\n/boot/initrd.img (초기 RAM 디스크 이미지)\n/boot/grub/grub.cfg (GRUB 부트로더 구성 파일)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n| - /dev \n\n설명: 하드웨어 장치를 나타내는 장치 파일이 포함되어 있습니다.\n예시:\n/dev/sda1 (첫 번째 하드 드라이브의 첫 번째 파티션)\n/dev/tty1 (터미널 장치)\n/dev/null (데이터를 버리는 널 장치)\n\n```\n\n```js\n| - /etc \n\n설명: 시스템 전체의 구성 파일 및 초기화를 위한 쉘 스크립트가 포함되어 있습니다.\n예시:\n/etc/passwd (사용자 계정 정보)\n/etc/fstab (파일 시스템 마운트 포인트)\n/etc/hosts (호스트 이름의 정적 테이블 조회)\n\n```\n\n```js\n| - /home \n\n설명: 시스템에 존재하는 각 사용자의 개인 디렉토리가 포함되어 있습니다. (사용자별 데이터)\n예시:\n/home/alice (앨리스의 홈 디렉토리)\n/home/bob (밥의 홈 디렉토리)\n\n```\n\n```js\n| - /lib \n\n설명: 시스템 이진 파일이 필요로 하는 공유 라이브러리가 포함되어 있습니다.\n예시:\n/lib/libc.so.6 (C 표준 라이브러리)\n/lib/modules (커널 모듈)\n\n```  \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n|- /media\n\nDescription: USB 드라이브, CD 및 DVD와 같은 탈착식 미디어를 연결하는 데 사용됩니다.\n예시:\n/media/cdrom (CD-ROM을 연결하는 마운트 포인트)\n/media/usb (USB 드라이브를 연결하는 마운트 포인트)\n\n\n\n|- /mnt\n\nDescription: 임시로 파일 시스템을 마운트하는 일반적인 마운트 포인트입니다.\n예시:\n관리자는 다음과 같이 파일 시스템을 임시로 마운트할 수 있습니다:\nsudo mount /dev/sdb1 /mnt\n\n\n\n|- /opt\n\nDescription: 선택적 소프트웨어 패키지 및 타사 응용 프로그램이 포함됩니다.\n예시:\n/opt/google (Google 애플리케이션의 설치 디렉토리)\n/opt/vmware (VMware 애플리케이션의 설치 디렉토리)\n\n\n\n|- /proc\n\nDescription: 프로세스 및 시스템에 대한 정보를 제공하는 가상 파일 시스템입니다.\n예시:\n/proc/cpuinfo (CPU 정보)\n/proc/meminfo (메모리 사용량 정보)\n/proc/1234 (PID가 1234인 프로세스에 대한 정보를 포함하는 디렉토리)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n|- /root\n\n설명: 시스템 관리자를 위한 홈 디렉토리입니다.\n예시:\n/root/.bashrc (루트 사용자용 Bash 구성 파일)\n/root/scripts (관리 작업을 위한 사용자 정의 스크립트)\n```\n\n```js\n|- /run\n\n설명: 시스템 프로세스 및 서비스의 런타임 데이터를 포함합니다.\n예시:\n/run/lock (락 파일)\n/run/user/1000 (UID 1000을 가진 사용자를 위한 런타임 데이터)\n```\n\n```js\n|- /sbin\n\n설명: 시스템 관리에 사용되는 필수 시스템 이진 파일이 포함되어 있습니다.\n예시:\n/sbin/reboot (시스템 재부팅)\n/sbin/ifconfig (네트워크 인터페이스 구성)\n/sbin/fdisk (디스크 파티셔닝 도구)\n```\n\n```js\n|- /srv\n\n설명: 시스템에서 제공하는 서비스에 대한 데이터가 포함되어 있습니다.\n예시:\n/srv/ftp (FTP 서버 데이터)\n/srv/www (웹 서버 데이터)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n|- /sys\n\n설명: 커널, 장치 및 시스템 하드웨어에 대한 정보를 제공하는 가상 파일 시스템입니다.\n예시:\n/sys/class/net (네트워크 인터페이스)\n/sys/devices (시스템 장치)\n```\n\n```js\n|- /tmp\n\n설명: 애플리케이션에서 사용하는 임시 파일을 저장하는 디렉토리입니다.\n예시:\n애플리케이션 실행 시 생성되는 임시 파일로, 종종 재부팅 시 정리됩니다.\n/tmp/install.log (소프트웨어 설치 중 생성된 임시 로그 파일)\n```\n\n```js\n|- /usr\n\n설명: 사용자 관련 프로그램 및 데이터를 포함합니다. 중요한 하위 디렉토리가 여러 개 있습니다:\n/usr/bin: 사용자 실행 파일.\n예시:\n/usr/bin/python (Python 인터프리터)\n/usr/bin/gcc (GNU C 컴파일러)\n```\n\n```js\n|- /usr/sbin: 시스템 관리 이진 파일.\n예시:\n/usr/sbin/apache2 (아파치 웹 서버)\n\n|- /usr/lib: 사용자 응용프로그램을 위한 라이브러리.\n예시:\n/usr/lib/libssl.so (OpenSSL 라이브러리)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n|- /usr/local\n\nDescription: 로컬에 설치된 소프트웨어 및 사용자 지정 스크립트입니다.\n예시:\n/usr/local/bin/myscript.sh (사용자 지정 스크립트)\n/usr/local/lib/mylib.so (사용자 지정 공유 라이브러리)\n```\n\n```js\n|- /var\n\nDescription: 로그, 데이터베이스 및 스풀 파일과 같은 변수 데이터 파일이 포함되어 있습니다.\n예시:\n/var/log: 로그 파일.\n예시:\n/var/log/syslog (시스템 로그)\n/var/log/auth.log (인증 로그)\n```\n\n```js\n|- /var/spool \nDescription: 메일 및 인쇄와 같은 작업을 위한 스풀 디렉토리입니다.\n예시:\n/var/spool/mail (사용자 메일함)\n```\n\n```js\n|- /var/tmp: \n\nDescription: 재부팅 간에 유지되는 임시 파일입니다.\n예시:\n세션 당보다 오래 지속되어야 하는 응용 프로그램이 생성하는 임시 파일.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 요약\n\n- Root (/): 최상위 디렉토리.\n- 시스템 이진 파일 (/bin, /sbin): 중요한 명령 줄 유틸리티 및 시스템 이진 파일.\n- 부팅 파일 (/boot): 부트로더 및 커널 파일.\n- 장치 파일 (/dev): 하드웨어 장치와의 인터페이스.\n- 설정 파일 (/etc): 시스템 및 애플리케이션 설정.\n- 홈 디렉토리 (/home): 사용자 데이터 및 설정.\n- 라이브러리 (/lib, /usr/lib): 공유 라이브러리.\n- 임시 파일 (/tmp, /var/tmp): 애플리케이션의 임시 저장소.\n- 사용자 프로그램 (/usr): 사용자 애플리케이션과 도구.\n- 로그 및 변수 데이터 (/var): 로그, 수동 파일 및 기타 변수 데이터.\n- 선택적 소프트웨어 (/opt): 제3자 애플리케이션 및 패키지.","ogImage":{"url":"/assets/img/2024-06-19-LinuxDirectoryStructure_0.png"},"coverImage":"/assets/img/2024-06-19-LinuxDirectoryStructure_0.png","tag":["Tech"],"readingTime":7},{"title":"컨테이너가 스왑 공간을 사용할 수 있을까요","description":"","date":"2024-06-19 14:53","slug":"2024-06-19-CanContainersUseSwapSpace","content":"\n\n\n![image](/assets/img/2024-06-19-CanContainersUseSwapSpace_0.png)\n\nLinux을 잘 아시는 분이라면 스왑 공간에 익숙할 것입니다. 간단히 말하면, 스왑은 가상 메모리로 사용되는 디스크의 일부입니다. 물리적 메모리 (RAM)가 가득 찼을 때 RAM에서 드물게 사용되는 데이터를 일시적으로 스왑 공간으로 이동할 수 있습니다. 이렇게 함으로써 새로운 메모리 요구에 맞춰 RAM을 확보합니다.\n\n스왑을 사용하는 장점은 메모리 사용량의 갑작스러운 변화를 다룰 수 있어 Out-Of-Memory(OOM) Killer가 충분한 메모리 없이 프로세스를 종료하는 것을 방지할 수 있다는 것입니다.\n\n특히 Memory Cgroups(제어 그룹)로 관리되는 컨테이너의 경우, 다음과 같은 의문이 생깁니다. 그들은 여전히 스왑 공간을 사용할 수 있을까? 그렇다면, 어떠한 잠재적인 문제가 있을까요?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 문제 현상\n\n먼저, 사용 가능한 스왑 공간이 있는 노드에 컨테이너를 시작하고 메모리 Cgroup 제한을 설정해봅시다. 이후 어떤 일이 일어나는지 관찰해보겠습니다.\n\n만약 당신의 노드에 스왑 파티션이 없다면, 다음 명령어를 사용하여 하나를 생성할 수 있습니다.\n\n이 예시에서 스왑 공간 크기는 20G로 설정되어 있지만, 사용 가능한 디스크 공간에 따라 조정할 수 있습니다. 이러한 명령어를 실행한 후 free 명령어를 실행하면 스왑 공간이 지금 20G임을 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 스크린샷을 참고하면 출력 예시를 확인할 수 있어요.\n\n![image](/assets/img/2024-06-19-CanContainersUseSwapSpace_1.png)\n\n자, 이제 OOM(메모리 부족) 문서의 예제와 비슷하게 512MB로 설정된 메모리 Cgroup 제한이 있는 컨테이너를 시작해보겠습니다. 컨테이너 내부의 mem_alloc 프로그램은 2GB의 메모리를 할당하려고 할 거에요.\n\nOOM 문서에서 설명한 상황과 달리, 이번에는 OOM 이벤트로 인해 컨테이너가 종료되지 않아요. 대신, 컨테이너는 계속해서 원활하게 실행됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 출력에서 우리는 mem_alloc 프로세스의 RSS (Resident Set Size) 메모리가 약 512MB(RES: 515596)로 유지되는 것을 관찰할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-CanContainersUseSwapSpace_2.png)\n\n이제 스왑 공간을 살펴봅시다. 1.5GB (사용 중 1542144KB)의 스왑 공간이 사용되고 있음을 관찰할 수 있습니다. 아래 그림에서 출력이 나와 있습니다. 간단한 계산으로 1.5GB + 512MB가 mem_alloc 프로그램이 요청한 2GB의 메모리와 동일함을 알 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-CanContainersUseSwapSpace_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리가 방금 토론한 예시를 기반으로 하면, 스왑 공간이 있는 경우 OOM-킬 당할 수 있던 컨테이너가 원활하게 실행될 것이라고 생각할 수 있습니다. 처음에는 유익해 보일 수 있지만, 좀 더 심층적으로 생각해보면, 이는 메모리 Cgroup 제한의 목적을 약화시킬 수 있다는 점을 생각해보셨나요?\n\n더 분석해봅시다. 컨테이너 내 프로그램이 메모리 누수를 경험하는 경우, 메모리 Cgroup은 일반적으로 다른 애플리케이션들에 영향을 미치지 않도록 적시에 프로세스를 종료합니다. 그런데 이제 스왑 공간이 활성화되어 있으면, 누수되는 프로세스가 종료되지 않습니다. 대신에 계속해서 스왑 디스크에 읽고 쓰기를 계속하며 전체 노드의 성능을 저하시킬 수도 있습니다.\n\n이 분석을 고려하면, 컨테이너를 실행하는 노드에서 스왑을 비활성화하는 것이 더 나을 수 있다고 결론짓을 수도 있습니다. 하지만 결론을 서두르지 않는 것이 중요합니다. 우리는 항상 말씀드리듯이, “구체적인 환경은 구체적인 분석이 필요합니다.” 구체적인 시나리오를 고려할 때, 처음에는 그렇게 간단하지 않을 수도 있습니다.\n\n예를 들어 일부 종류의 프로그램은 가끔 메모리가 급증하여 OOM 킬러에 의해 종료되지 않도록하기 위해 스왑 공간이 필요할 수 있습니다. 이러한 프로그램을 재시작하는 것은 초기화 시간이 길기 때문에 비용이 들 수 있습니다. 이러한 프로그램들에 대해서는 스왑을 활성화하는 것이 의미가 있을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 프로그램들이 컨테이너에서 실행될 때, 같은 호스트 머신을 다른 컨테이너와 함께 공유하게 될 거예요. 하나의 컨테이너가 스왑이 필요없고 엄격한 Memory Cgroup 제한을 의존하는 경우, 두 컨테이너 간에 호스트 머신에서 충돌이 발생할 수 있어요. 그래서, 이 충돌을 어떻게 해결할 수 있을까요?\n\n이 문제를 해결하기 위해서는 Linux의 \"swappiness\" 개념을 살펴볼 필요가 있어요. 이것이 매우 도움이 될 거예요.\n\n# swappiness 매개변수를 올바르게 이해하는 방법\n\n일반적인 Linux 시스템에서 스왑 공간을 사용한 경우, proc 파일 시스템 (/proc/sys/vm/swappiness)에서 찾을 수 있는 swappiness 매개변수를 구성했을 수 있어요. swappiness의 정의는 Linux 커널 문서에 나와 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n고 значение은 kernel이 swap 하는 경향을 늘리고, 낮은 값은 그 경향을 줄입니다. swappiness 매개변수는 kernel이 런타임 메모리를 swap out 하는 선호도를 정의하며, 값이 100이면 kernel이 적극적으로 swap하고, 값이 0이면 kernel은 가능한 경우에는 swap을 피합니다. 기본 값은 60입니다.\n\n이 정의를 처음 읽고 값의 범위를 알게 되었을 때, swappiness를 백분율 값으로 생각했습니다. 이는 swap 공간 사용 빈도를 나타냅니다. 값이 100이면 메모리가 충분할 때에도 항상 디스크로 swap하게 되고, 값이 0이면 디스크로 swap하지 않는다는 것을 의미합니다.\n\n그러나 더 심층적인 고찰을 통해, 이해를 바로 잡아보자면, 이해가 완전히 잘못됐다기보다는 개념을 단순화한다는 점이 있습니다. 그래서 이 정의를 올바르게 이해하는 방법은 무엇일까요?\n\nLinux에서 디스크 파일 접근할 때, 시스템은 파일 I/O 성능을 향상시키기 위해 쉬는 메모리를 페이지 캐시로 사용하려고 노력합니다. swap 공간이 없는 경우, 메모리가 부족해지면 페이지 캐시가 해제되지만 Resident Set Size (RSS) 메모리는 해제되지 않는다는 것을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대부분의 RSS 메모리는 malloc()에 의해 할당된 메모리와 같이 특정 디스크 파일과 연관시킬 수 없는 익명 메모리입니다. 스왑 공간이 활성화되어 있는 경우, 이 익명 메모리는 스왑 공간에 기록될 수 있습니다.\n\n그래서 스왑 공간이 활성화된 상황에서, 메모리가 부족할 때 리눅스 시스템이 페이지 캐시를 해제할지 아니면 익명 메모리를 해제하고 스왑 공간에 기록할지 결정하는 방법이 궁금해집니다.\n\n가능성이 높은 두 가지 시나리오를 분석해 봅시다:\n\n- 시스템이 모든 페이지 캐시를 먼저 해제하는 경우, 자주 파일 읽기/쓰기 작업이 필요한 경우에 성능이 저하될 수 있습니다.\n- 시스템이 먼저 모든 익명 메모리를 해제하고 스왑 공간에 기록하면, 그리고 즉시 해당 해제된 익명 메모리를 사용해야 할 때, 이를 스왑 공간에서 다시 읽어들여야 하므로 자주 디스크 I/O와 성능 저하가 발생할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n명확하게, Page Cache 및 익명 메모리의 릴리스를 균형 있게 유지해야 합니다. swappiness는 이 균형을 정의하는 매개변수입니다.\n\nswappiness는 이 균형을 어떻게 제어할까요? Linux 커널 코드가 이 swappiness 매개변수를 사용하는 방법을 살펴봅시다.\n\nswappiness 값의 범위는 0부터 100까지입니다. 하지만 이 값은 비율이지만 백분율은 아닙니다. 이 값은 릴리스되는 익명 메모리와 Page Cache 메모리 사이의 비율을 정의합니다.\n\n커널 코드에서 이 비율은 anon_prio:file_prio로 표현됩니다. 여기서 anon_prio는 swappiness와 동일합니다. 세 가지 시나리오에 대해 논의해 봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 스왑니스가 100 일 때, 익명 메모리를 해제하는 비율과 페이지 캐시 메모리를 해제하는 비율은 100 : 100이며, 동일한 비율로 해제됩니다.\n- 기본 스왑니스 값이 60 일 때, 익명 메모리를 해제하는 비율과 페이지 캐시 메모리를 해제하는 비율은 60 : 140으로, 페이지 캐시 메모리가 익명 메모리보다 더 적극적으로 해제됨을 나타냅니다.\n\n```js\n        /*\n         * 스왑니스가 100 인 경우, 익명 및 파일에 동일한 우선 순위가 부여됩니다.\n         * 이 스캔 우선 순위는 사실 IO 비용의 역입니다.\n         */\n        anon_prio = swappiness;\n        file_prio = 200 - anon_prio;\n```\n\n한 가지 더 고려해야 할 시나리오가 있습니다: 스왑니스가 0으로 설정된 경우 무엇이 발생합니까? 이것은 Linux 가 익명 메모리를 스왑 공간에 쓰지 않도록 허용하지 않는다는 의미입니까?\n\n스왑니스의 정의를 다시 살펴보고 스왑니스가 0 인 경우에 특히 주의 깊게 살펴봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n메모리(zone)의 \"high water mark\" 아래로 떨어질 때 swappiness가 0으로 설정되어 있어도 Linux는 여전히 메모리 스와핑을 수행합니다. 이는 메모리를 해제하기 위해 익명 메모리를 스왑 공간에 기록한다는 것을 의미합니다.\n\n이 문맥에서 \"zone\"은 Linux에서 물리적 메모리의 영역이며, 각 zone에는 메모리 압력 수준을 나타내는 세 가지 워터 마크가 있습니다.\n\n이 동작을 검증하기 위해 실험을 실행해 봅시다. 먼저 swappiness를 0으로 설정하는 명령어인 'echo 0 \u003e /proc/sys/vm/swappiness'를 사용하세요. 그런 다음 이전 예제에서 사용한 mem_alloc 프로그램을 사용하여 메모리를 할당하세요. 예를 들어, 노드에 12GB의 메모리와 2GB의 스왑 공간이 있다면 mem_alloc을 사용하여 12GB의 메모리를 할당하세요.\n\nmem_alloc을 실행하기 전에 스왑 공간 사용량을 확인하세요. 사용량은 used=0로 표시되어야 합니다. mem_alloc을 실행한 후에는 일부 메모리가 스왑 공간에 기록되었음을 나타내는 출력이 표시되어야 합니다. 이것은 swappiness가 0으로 설정되어 있더라도 Linux가 필요할 때 여전히 메모리를 스왑 공간으로 스왑한다는 것을 확인합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-CanContainersUseSwapSpace_4.png)\n\nmem_alloc를 호출한 후에는 교체 공간이 실제로 사용 중입니다.\n\n![이미지](/assets/img/2024-06-19-CanContainersUseSwapSpace_5.png)\n\nmem_alloc가 노드에서 사용 가능한 최대 메모리(12GB)를 거의 요청했기 때문에 cat /proc/zoneinfo를 확인하면 normal 존의 높은 값이 free 값과 근접함을 알 수 있습니다. 이 상황에서 free가 high보다 작을 때 시스템은 익명 메모리 페이지를 회수하고 교체 공간에 기록합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 리눅스 시스템에서 swappiness 개념을 소개한 내용입니다. 이는 메모리가 부족할 때 익명 메모리와 페이지 캐시 메모리 간 재할당 비율을 결정합니다.\n\nswappiness 값은 0부터 100까지이며, 100은 익명 메모리와 페이지 캐시 메모리 간 재할당이 동일함을 의미합니다. 기본값은 일반적으로 60이며, 페이지 캐시 재할당을 우선시합니다. 심지어 swappiness가 0으로 설정되어도 Swap 파티션 사용이 완전히 비활성화되지는 않습니다. 이는 메모리가 부족할 때 Swap이 여전히 익명 메모리를 반환하기 위해 사용된다는 것을 의미합니다.\n\n# 문제 해결 방법\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n컨테이너를 Memory Cgroup으로 실행할 때 swappiness는 어떻게 작동하나요?\n\nMemory Cgroup 제어 그룹 아래 매개변수를 확인하면 memory.swappiness라는 매개변수가 있습니다. 이 매개변수는 무엇을 하는 걸까요?\n\nmemory.swappiness는 이 Memory Cgroup 제어 그룹 아래의 익명 메모리 및 페이지 캐시 회수를 제어하며, 범위 및 동작은 전역 swappiness와 유사합니다. 여기에는 우선 순위 순서가 있습니다: Memory Cgroup 제어 그룹에서 memory.swappiness 매개변수를 설정하면 이 그룹 내에서 전역 swappiness를 무효화시키므로 전역 swappiness가 작동하지 않게 됩니다.\n\n하지만 여기서 주의해야 할 차이점이 있습니다: memory.swappiness = 0으로 설정하면 익명 페이지 회수가 항상 비활성화되어 Swap 공간을 사용하지 않게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 시점에서 Linux 시스템은 더 이상 영역의 높은 기준 마크와 빈 메모리를 비교하여 메모리 Cgroup의 익명 메모리를 회수해야 하는지 여부를 결정하지 않습니다.\n\n“memory.swappiness=0”을 설정하면 메모리 Cgroup의 프로세스가 스왑 공간을 더 이상 사용하지 않습니다. 이 점을 이해하는 것이 중요합니다.\n\n이를 확인하기 위해 스왑 공간이 있는 노드에서 여전히 컨테이너를 실행해 볼 수 있습니다. 이전 글에서와 같은 컨테이너를 실행하되, 해당 Memroy Cgroup의 memory.swappiness를 0으로 설정하는 방법으로 진행할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-CanContainersUseSwapSpace_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이번에는 컨테이너에 메모리를 할당한 후 스왑 공간이 사용되지 않았으며, 컨테이너가 할당된 메모리가 memory.limit_in_bytes를 초과하면 OOM Kill이 발생했습니다.\n\n“memory.swappiness = 0”의 구성 및 기능으로 이 기사 초반에 제기한 문제를 해결할 수 있습니다.\n\n같은 호스트에서 컨테이너 A와 컨테이너 A의 스왑 공간이 필요한 애플리케이션을 실행하는 다른 컨테이너가 있는 상황에서, 다른 컨테이너는 스왑 공간을 사용할 필요가 없습니다.\n\n이 경우에는 호스트 노드에서 스왑 공간을 활성화하고, 다른 컨테이너에 해당하는 Memory Cgroups 제어 그룹에서 memory.swappiness 매개변수를 0으로 설정할 수 있습니다. 이렇게 함으로써 우리는 컨테이너 A의 요구 사항을 충족시키고, 다른 컨테이너가 영향을 받지 않고 여전히 Memory Cgroups의 memory.limit_in_bytes에 따라 메모리 사용량을 엄격하게 제한할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결론적으로, memory.swappiness 매개변수는 매우 유용합니다. 이를 통해 Swap 공간을 사용해야 하는 컨테이너와 Swap 공간을 필요로 하지 않는 컨테이너가 동시에 동일한 호스트에서 실행될 수 있습니다.\n\n# 결론\n\n본 문서에서는 컨테이너에서 Swap 사용 여부에 대해 주로 다뤘습니다. 이 문제는 보다 간단하지 않습니다. 물론 호스트 노드에서 Swap 공간이 활성화되어 있다면 컨테이너에서 Swap을 사용할 수 있습니다. 그러나 동일한 호스트에서 Swap을 사용하지 않아도 되는 컨테이너를 위한 메모리 Cgroups 제한이 효과가 없어지는 문제가 발생할 수 있습니다.\n\n이 문제를 해결하기 위해 Linux의 swappiness 매개변수에 대해 알아보았습니다. swappiness 매개변수는 시스템에 Swap 공간이 있고 시스템이 메모리를 회수해야 할 때 페이지 캐시에서 메모리 해제를 우선시할지 익명 메모리(즉, Swap에 기록)에서 메모리 해제를 우선시할지 결정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nswappiness 값은 0부터 100까지의 범위를 가지며, 다음 세 가지 값을 기억할 수 있습니다:\n\n- 100의 값은 페이지 캐시와 익명 메모리의 메모리 해제에 동등한 우선 순위를 부여함을 의미합니다.\n- 대부분의 Linux 시스템에서 기본값인 60의 값은 페이지 캐시에서 익명 메모리보다 메모리를 해제하는 것을 우선시합니다.\n- 0의 값은 시스템의 무료 메모리가 임계 값을 초과할 때에도 익명 메모리가 여전히 해제되고 페이지가 Swap 영역에 씌워집니다.\n\nproc 파일 시스템의 전역 swappiness 매개변수와 각 메모리 Cgroup 제어 그룹의 memory.swappiness 매개변수의 차이점은 메모리 Cgroup 제어 그룹의 swappiness 매개변수 값이 0으로 설정되면 해당 제어 그룹에서 Swap으로의 메모리 작성이 중지된다는 것입니다. memory.swappiness 매개변수를 사용하면 Swap을 사용해야 하는 컨테이너와 그렇지 않은 컨테이너가 동일한 호스트에서 실행되어 더 높은 하드웨어 자원 활용률을 이끌어냅니다.\n\n![이미지](/assets/img/2024-06-19-CanContainersUseSwapSpace_8.png)","ogImage":{"url":"/assets/img/2024-06-19-CanContainersUseSwapSpace_0.png"},"coverImage":"/assets/img/2024-06-19-CanContainersUseSwapSpace_0.png","tag":["Tech"],"readingTime":9},{"title":"가이드 Nuclei 사용법","description":"","date":"2024-06-19 14:51","slug":"2024-06-19-GuidetoUsingNuclei","content":"\n\nNuclei라는 강력한 취약점 스캐너 사용 방법을 배우세요.\n\n# 소개\n\nNuclei는 보안 연구원과 전문가를 위해 설계된 강력하고 유연한 오픈 소스 취약점 스캐너입니다. 웹 애플리케이션, API 및 네트워크 서비스의 다양한 취약점을 식별하고 보고하기 위해 사용자 정의 가능한 템플릿을 사용합니다. Nuclei는 다른 보안 도구와 통합되어 자동화된 워크플로에 매끄럽게 포함될 수 있습니다. 속도 제한, 사용자 지정 헤더, 외부 테스트, 다양한 구성 옵션과 같은 기능을 갖춘 Nuclei는 적극적인 취약점 관리와 보안 평가를 위한 효율적이고 철저한 솔루션을 제공합니다.\n\n![Nuclei 사용 가이드](/assets/img/2024-06-19-GuidetoUsingNuclei_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## TL;DR\n\n이 문서의 간략한 요약 버전을 여기에서 확인할 수 있습니다.\n\n# Basic Usage\n\nNuclei는 단일 대상물을 스캔하거나 파일에서 여러 대상물을 스캔하고 다른 도구들과의 워크플로에 통합할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단일 대상 스캔\n\n취약점을 찾기 위해 단일 대상 URL을 스캔하려면 다음 명령을 사용하세요:\n\n```js\nnuclei -u http://example.com\n```\n\n다른 방법으로는 아래와 같이 사용할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nnuclei -target http://example.com\n```\n\n두 명령 모두 지정된 URL을 스캔하여 사용 중인 템플릿을 기반으로 알려진 취약점을 찾습니다.\n\n## 파일에서 대상 스캔\n\n대상 목록이 있는 경우 파일에 저장하고 각 대상을 스캔할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nnuclei -l targets.txt\n```\n\n이 방법은 여러 대상을 스캔하고 프로세스를 자동화하며 모든 지정된 URL이 취약점을 확인하는 데 효율적입니다.\n\n## 다른 도구와 Nuclei 통합하기\n\nNuclei를 다른 보안 도구와 통합하여 포괄적인 워크플로우를 만들 수 있습니다. 예를 들어 `subfinder`와 `httpx`를 Nuclei와 결합하여 하위 도메인을 찾은 다음 노출에 대해 스캔할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsubfinder -d targetdomain.site -silent | httpx | nuclei -t http/exposures/\n```\n\n이 워크플로우는 먼저 `subfinder`를 사용하여 `targetdomain.site`의 서브도메인을 발견한 후, `httpx`로 해당 서브도메인의 HTTP 상태를 확인하고, 마지막으로 `http/exposures/` 디렉토리에 있는 Nuclei 템플릿을 사용하여 취약점을 스캔합니다.\n\n# 템플릿\n\n템플릿은 Nuclei가 스캔 중에 무엇을 찾을지를 정의합니다. 특정 유형의 취약점에 대한 것이거나 보다 일반적인 내용일 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 템플릿 폴더 사용하기\n\n특정 폴더의 모든 템플릿을 사용하여 스캔할 수 있습니다. 예를 들어, `http/exposures/` 폴더의 모든 템플릿을 사용하려면:\n\n```js\nnuclei -t http/exposures/\n```\n\n이 명령은 지정된 디렉토리의 모든 템플릿을 사용하여 대상을 스캔하며 다양한 유형의 노출을 확인합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 특정 템플릿 사용하기\n\n특정 템플릿을 활용하여 스캔하려면 `-t` 플래그와 함께 해당 템플릿을 나열하면 됩니다. 이를 통해 특정 취약점이나 기술을 기반으로 한 타겟 스캔이 가능합니다:\n\n```js\nnuclei -t http/technologies/tech-detect.yaml -t http/technologies/nginx/nginx-version.yaml\n```\n\n위 예시는 특정 기술을 스캔하고 사용 중인 Nginx 버전을 체크하는데, 지정된 템플릿을 활용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 템플릿 태그 사용하기\n\n템플릿에 태그를 달아서 보다 쉽게 정리하고 활용할 수 있습니다. 특정 기준과 일치하는 템플릿을 사용하기 위해 태그를 지정할 수 있습니다:\n\n```js\nnuclei -u https://jira.targetdomain.site -tags jira,generic\n```\n\n이 명령은 `jira`와 `generic` 태그가 달린 템플릿을 사용하여 대상 URL을 스캔하여 관련 있는 취약점을 확인합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 심각도별 필터링\n\n더 중요한 문제에 집중하기 위해 심각도에 따라 템플릿을 필터링할 수 있습니다. 예를 들어:\n\n```js\nnuclei -u https://targetdomain.site -s critical,high,medium\n```\n\n이 명령은 심각하거나 높거나 중간 심각도로 분류된 템플릿을 사용하여 대상을 검사하며, 중요한 취약점을 우선적으로 처리합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 템플릿 제외하기\n\n스캔에서 특정 템플릿을 제외하려면 `-et` 플래그를 사용하세요. 이는 스캔을 개선하여 관련없거나 중요하지 않은 체크를 제외하는 데 도움이 됩니다:\n\n```js\nnuclei -et http/fuzzing/\n```\n\n이 명령은 스캔에서 `http/fuzzing/` 디렉토리의 모든 템플릿을 제외합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 헤더 설정\n\n스캔 중 HTTP 요청에 포함될 사용자 지정 헤더를 설정할 수 있습니다. 특정 상호 작용을 위해 특정 헤더가 필요한 응용 프로그램을 스캔할 때 유용합니다.\n\n## 사용자 지정 헤더\n\n`-H` 플래그를 사용하여 사용자 지정 헤더를 설정할 수 있습니다. 예를 들어, User-Agent 헤더를 설정하는 방법은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nnuclei -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36' -l targets.txt\n```\n\n이 명령어는 `targets.txt`에 나열된 모든 대상을 스캔하면서 각 요청마다 지정된 User-Agent 헤더를 보냅니다.\n\n# 속도 제한\n\n대상 서버를 과부하시키지 않도록 요청률과 동시 스레드 수를 제한할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 요청 및 쓰레드 제한\n\n초당 요청 수와 동시 쓰레드 수를 설정하여 스캔 속도를 제어하세요:\n\n```js\nnuclei -l targets.txt -rl 20 -c 5\n```\n\n이 명령은 Nuclei를 초당 20개의 요청으로 제한하고 최대 5개의 동시 쓰레드를 사용하여 속도와 서버 부하를 균형있게 유지합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 최적화\n\nNuclei는 스캔 성능을 최적화하고 오류를 효율적으로 처리하기 위한 여러 옵션을 제공합니다.\n\n## 타임아웃 설정\n\n스캔 속도를 높이기 위해 요청의 타임아웃을 줄일 수 있습니다. 기본 타임아웃은 10초이지만 필요에 따라 더 낮출 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nnuclei -l targets.txt -timeout 3\n```\n\n이 명령어는 요청 제한 시간을 3초로 설정합니다.\n\n## 오류 처리와 재시도\n\nNuclei가 오류를 처리하고 재시도하는 방법을 구성하세요. 일정 횟수의 오류 후 호스트를 건너뛰거나 실패한 요청에 대한 재시도 횟수를 설정하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nnuclei -l targets.txt --max-host-errors 5\n```\n\n```js\nnuclei -l targets.txt --retries 3\n```\n\n이 명령어들은 5번의 오류 발생 후 호스트를 건너뛰고, 실패한 요청을 최대 3번까지 다시 시도합니다.\n\n## 스캔 전략\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로드와 효율성을 균형 있게 유지할 스캔 전략을 선택하세요. `host-spray`는 다음 대상으로 이동하기 전에 모든 템플릿을 단일 대상에 적용하고, `template-spray`는 여러 대상에 걸쳐 템플릿을 실행합니다:\n\n```js\nnuclei -l targets.txt -ss host-spray\n```\n\n이 명령은 `host-spray` 전략을 사용하며, 각 대상에 대한 부하를 줄일 수 있습니다.\n\n# 결과\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**저장 결과**\n\n나중에 분석할 파일로 스캔 결과를 저장해보세요:\n\n```js\nnuclei -l targets.txt -o nuclei.log\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 명령어는 스캔 결과를 `nuclei.log`에 기록합니다.\n\n## JSONL 출력\n\n스캔 결과를 JSONL (JSON Lines) 형식으로 출력하여 쉽게 구문 분석하고 다른 도구와 통합할 수 있습니다:\n\n```js\nnuclei -l targets.txt -jsonl\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 프린팅 통계\n\n스캔 중에 진행 상황과 성능을 모니터링하는 통계를 표시하십시오:\n\n```js\nnuclei -l targets.txt -stats\n```\n\n## 마크다운 출력\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식으로 결과를 저장하세요:\n\n```js\nnuclei -l targets.txt -me results/\n```\n\n이 명령은 스캔 결과를 `results/` 디렉토리에 Markdown 형식으로 저장합니다.\n\n# 외부 밴드 테스팅\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n외부 밴드 (OOB) 테스트는 일반적인 HTTP 요청/응답 주기 외에 발생하는 상호 작용을 테스트하는 것을 의미합니다.\n\n## OOB 테스트 비활성화\n\nOOB 테스트가 필요하지 않은 경우 비활성화할 수 있습니다:\n\n```js\nnuclei -l targets.txt -ni\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 인터랙트 서버 사용하기\n\nOOB 상호 작용을 처리하기 위해 자체 호스트된 Interactsh 서버를 지정하십시오:\n\n```js\nnuclei -l targets.txt -iserver \u003cserver-addr\u003e -itoken \u003cserver-token\u003e\n```\n\n상호 작용 유출 시간:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nnuclei -l targets.txt -interactions-eviction 120\n```\n\n그리고 사용자 지정 폴링 기간을 정의하십시오:\n\n```js\nnuclei -l targets.txt -interactions-poll-duration 10\n```\n\n이 명령어들은 Nuclei를 특정 Interactsh 서버를 사용하도록 구성하고 상호 작용을 기다릴 시간을 조절합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 설정\n\n세팅을 간편하게하고 일관된 스캔을 보장하기 위해 YAML 파일에서 구성을로드합니다. 기본 구성 파일은 `~/.config/nuclei/config.yaml`에 위치해 있습니다.\n\n## 구성 파일 사용\n\n`-config` 플래그를 사용하여 구성을 로드할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nnuclei -config nuclei.yaml -l targets.txt\n```\n\n## 구성 예시\n\n예시 구성 파일에는 사용자 지정 헤더, 템플릿 경로, 태그, 심각도 필터 및 속도 제한 설정이 포함될 수 있습니다:\n\n```js\nheader:\n  - 'X-BugBounty-Hacker: h1/nickname'\n\ntemplates:\n  - cves/\n  - vulnerabilities/\n  - misconfiguration/\n\ntags: exposures,cve\nseverity: critical,high,medium\n\ninclude-templates:\n  - vulnerabilities/xxx\n  - misconfiguration/xxxx\n\nexclude-tags: info,fuzz\nexclude-templates:\n  - vulnerabilities/xxx\n  - misconfiguration/xxxx\n\n# 속도 제한 설정\nrate-limit: 50\nbulk-size: 20\nconcurrency: 20\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 구성은 스캔 프로세스를 최적화하기 위해 사용자 지정 헤더를 설정하고 템플릿을 지정하며 속도 제한 및 기타 설정을 정의합니다.\n\n# 업데이트\n\nNuclei 및 해당 템플릿을 최신 상태로 유지하여 최신 취약점을 확인할 수 있도록 합니다.\n\n## 업데이트 확인 비활성화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자동 업데이트 확인을 비활성화하여 스캔 중단을 방지하세요:\n\n```js\nnuclei -l targets.txt -duc\n```\n\n## 템플릿 및 Nuclei 업데이트\n\nNuclei 설치를 최신 버전으로 업데이트하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nnuclei -up\n```\n\n템플릿을 업데이트하려면:\n\n```js\nnuclei -ut\n```\n\n이 명령어들은 최신 기능 및 취약점 검사를 보장하여 템플릿과 Nuclei 도구 자체를 업데이트합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\nNuclei는 보안 전문가와 연구자들에게 필수적인 도구로, 취약성 스캔을 위한 견고하고 유연한 솔루션을 제공합니다. 사용자는 다양한 타깃에서 보안 위험을 효과적으로 식별하고 관리할 수 있도록, 다양한 커스터마이즈 가능한 템플릿, 통합 기능 및 포괄적인 구성 옵션을 제공합니다. 레이트 제한, 커스텀 헤더, out-of-band 테스팅과 같은 강력한 기능을 활용함으로써 사용자들은 스캔 전략을 특정 요구사항과 환경에 맞게 맞춤화할 수 있습니다. Nuclei를 일상적인 보안 점검이나 더 큰 보안 작업 흐름에 도입할 때, 이 도구는 견고한 보안 자세를 유지하기 위해 필요한 다양성과 심도를 제공합니다. Nuclei의 파워를 활용하여 취약성 관리 방법을 향상시키고 잠재적인 위협에 선제적으로 대처하세요.","ogImage":{"url":"/assets/img/2024-06-19-GuidetoUsingNuclei_0.png"},"coverImage":"/assets/img/2024-06-19-GuidetoUsingNuclei_0.png","tag":["Tech"],"readingTime":8},{"title":"SQL에 대한 설명 랭킹 분석","description":"","date":"2024-06-19 09:54","slug":"2024-06-19-SQLExplainedRankingAnalytics","content":"\n\n인기 있는 RDBMS 관리 시스템 중 하나인 Oracle, SQL Server, Postgres 등에 경험이 있다면, 분석 또는 윈도잉 함수로 불리는 함수를 어느 정도 접해봤을 것입니다.\n\n분석 함수를 사용하면 데이터 집합 내의 행 그룹에 대한 집계 및 순위 시퀀스를 계산할 수 있습니다. 분석 함수에 대해 알아보고 사용해볼 가치가 있는지 궁금해 한다면, 확실한 \"예\"라고 말씀드릴 수 있어요. 그들은 굉장히 유용하며, 그들 없이는 어려운 것이 아니면 불가능한 일들을 SQL로 처리할 수 있게 해 줍니다.\n\n이 글에서는 SQL의 가장 일반적인 랭킹 기술 중 네 가지를 살펴보며, 랭킹 함수라고 하는 특정 분석 카테고리에 집중하겠습니다. 어떤 역할을 하는지 설명하고 사용 예시를 제공할 거에요.\n\n## 랭킹 분석 구문\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대부분의 최신 SQL 방언에서 랭킹 분석 함수의 일반적인 형식은 다음과 같습니다.\n\n위 문장의 각 부분을 각각 간략히 살펴봅시다.\n\n## RANK | DENSE_RANK | ROW_NUMBER() | NTILE\n\n이들은 랭킹 함수의 이름들이며 대부분의 SQL에서 표시된 네 가지 함수를 지원합니다. 곧 각 개별 랭킹 함수에 대해 더 자세히 이야기해 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## OVER(…)\n\nover 키워드는 그룹화 정의를 나타내며 데이터 테이블에서 어떤 행을 등수 매기는지를 나타냅니다.\n\nPARTITION_BY_CLAUSE\n\npartition_by_clause는 선택 사항이며 데이터 세트에서 그룹화하려는 열(하나 이상)의 이름을 포함합니다. 생략하면 SQL이 등수 함수를 실행할 때 테이블의 모든 레코드를 고려합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**ORDER_BY_CLAUSE**\n\n이 order_by_clause는 그룹을 구성하는 열이 순위를 매기기 전에 어떻게 정렬되는지를 지정합니다.\n\n**WINDOWING_CLAUSE**\n\n특정 그룹 내에서 windowing_clause는 현재 행에 대해 순위 함수가 작동해야 하는 레코드 범위를 정의합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 가지 유형의 윈도잉이 있습니다.\n\n- 로우 윈도우, 즉 현재 레코드로부터 봐야 할 한 개 이상의 물리적 행을 지정하는 윈도우\n- 레인지 윈도우로 현재 행의 값에서 빼거나 더하여 행의 범위를 정의하는 윈도우가 있습니다.\n\n일반적인 윈도우 프레임은 다음과 같습니다:\n\n- UNBOUNDED PRECEDING과 CURRENT ROW 사이의 행\n- X PRECEDING과 CURRENT ROW 사이의 행\n- CURRENT ROW과 X FOLLOWING 사이의 행\n- UNBOUNDED PRECEDING과 UNBOUNDED FOLLOWING 사이의 범위\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n완전성을 위해 WINDOWING_CLAUSE에 대한 설명을 추가했지만 솔직히 말해서, 일상 업무에서는 거의 사용할 일이 없을 것입니다. 99%의 경우, 기본값만으로 충분합니다. 사실, 일반적으로 랭킹 분석 함수에서는 windowing_clause를 전혀 사용하지 않습니다.\n\n랭킹 분석은 말로만 설명하기 어려운데, 그 사용 예를 몇 가지 보여드리는 게 가장 좋은 방법입니다.\n\n## 테스트 환경 설정\n\n저는 Oracle의 live SQL 웹사이트를 사용하여 테스트를 실행합니다. 이 서비스에 액세스하고 사용하는 방법에 대해 이전에 SQL에서 Grouping Sets, Rollup 및 Cube를 사용하는 데 관한 기사에서 설명했습니다. 완전히 무료로 설정하고 사용할 수 있습니다. 해당 기사의 링크는 아래에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 샘플 테이블 생성 및 데이터 입력\n\n저희 예제에는 하나의 테이블만 필요하며 학생들과 공부하고 있는 과목의 시험 점수에 관한 데이터 세트가 포함되어 있습니다. 테이블은 student_id, student_name, subject, score로 구성되어 있습니다.\n\n```js\nCREATE TABLE student_scores (\n    student_id INT,\n    student_name VARCHAR(50),\n    subject VARCHAR(50),\n    score INT\n);\n\nINSERT INTO student_scores VALUES (1, 'Alice', 'Math', 95);\nINSERT INTO student_scores VALUES (2, 'Bob', 'Math', 85);\nINSERT INTO student_scores VALUES (3, 'Charlie', 'Math', 90);\nINSERT INTO student_scores VALUES (4, 'David', 'Math', 80);\nINSERT INTO student_scores VALUES (5, 'Eva', 'Math', 70);\nINSERT INTO student_scores VALUES (6, 'Frank', 'Science', 88);\nINSERT INTO student_scores VALUES (7, 'Grace', 'Science', 92);\nINSERT INTO student_scores VALUES (8, 'Hannah', 'Science', 85);\nINSERT INTO student_scores VALUES (9, 'Ivy', 'Science', 90);\nINSERT INTO student_scores VALUES (10, 'Jack', 'Science', 82);\nINSERT INTO student_scores VALUES (11, 'Kate', 'History', 78);\nINSERT INTO student_scores VALUES (12, 'Leo', 'History', 88);\nINSERT INTO student_scores VALUES (13, 'Mia', 'History', 84);\nINSERT INTO student_scores VALUES (14, 'Nina', 'History', 90);\nINSERT INTO student_scores VALUES (15, 'Oscar', 'History', 92);\n```\n\n```js\nselect * from student_scores;\n\n\n+------------+--------------+---------+-------+\n| student_id | student_name | subject | score |\n+------------+--------------+---------+-------+\n|          1 | Alice        | Math    |    95 |\n|          2 | Bob          | Math    |    85 |\n|          3 | Charlie      | Math    |    90 |\n|          4 | David        | Math    |    80 |\n|          5 | Eva          | Math    |    70 |\n|          6 | Frank        | Science |    88 |\n|          7 | Grace        | Science |    92 |\n|          8 | Hannah       | Science |    85 |\n|          9 | Ivy          | Science |    90 |\n|         10 | Jack         | Science |    82 |\n|         11 | Kate         | History |    78 |\n|         12 | Leo          | History |    88 |\n|         13 | Mia          | History |    84 |\n|         14 | Nina         | History |    90 |\n|         15 | Oscar        | History |    92 |\n+------------+--------------+---------+-------+\n\n15 rows selected.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 데이터가 준비되었으니 순위 함수를 소개할 수 있어요.\n\nRANK\n\n랭크 함수를 사용하면 테이블의 행에 연속적인 정수 번호를 할당할 수 있지만, 랭크를 사용하면 순서 번호가 꼭 연속적이지 않을 수 있다는 점을 알아두어야 해요.\n\n가장 좋은 방법은 올림픽 스프린트 경기의 선수들을 상상하는 것이에요. 만약 두 선수가 1위에서 동시에 와 발생한다면 그들은 둘 다 1위를 할당받아 금메달을 딴다고 생각하시면 돼요. 그 다음으로 먼저 도착한 선수는 두 번째 위치가 아닌 세 번째 위치 (즉, 동메달)를 받게 될 거예요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것이 순위가 번호를 할당하는 방법이며, 아래 예시에서 명확히 볼 수 있습니다.\n\n```js\nselect subject, student_name, rank() over(order by subject) rnk  \nfrom student_scores;\n\n+---------+--------------+-----+\n| subject | student_name | rnk |\n+---------+--------------+-----+\n| History | Kate         |   1 |\n| History | Leo          |   1 |\n| History | Mia          |   1 |\n| History | Nina         |   1 |\n| History | Oscar        |   1 |\n| Math    | Alice        |   6 |\n| Math    | Bob          |   6 |\n| Math    | Charlie      |   6 |\n| Math    | David        |   6 |\n| Math    | Eva          |   6 |\n| Science | Frank        |  11 |\n| Science | Grace        |  11 |\n| Science | Hannah       |  11 |\n| Science | Ivy          |  11 |\n| Science | Jack         |  11 |\n+---------+--------------+-----+\n\n15 rows selected.\n```\n\n더 현실적인 사용 사례로, 각 과목의 점수를 해당 과목에서 가장 높은 점수순으로 나열할 수 있습니다.\n\n```js\nSELECT student_name, subject, score,\n       RANK() OVER (PARTITION BY subject ORDER BY score DESC) AS rank\nFROM student_scores;\n\n+--------------+---------+-------+------+\n| student_name | subject | score | rank |\n+--------------+---------+-------+------+\n| Alice        | Math    |    95 |    1 |\n| Charlie      | Math    |    90 |    2 |\n| Bob          | Math    |    85 |    3 |\n| David        | Math    |    80 |    4 |\n| Eva          | Math    |    70 |    5 |\n| Oscar        | History |    92 |    1 |\n| Nina         | History |    90 |    2 |\n| Leo          | History |    88 |    3 |\n| Mia          | History |    84 |    4 |\n| Kate         | History |    78 |    5 |\n| Grace        | Science |    92 |    1 |\n| Ivy          | Science |    90 |    2 |\n| Frank        | Science |    88 |    3 |\n| Hannah       | Science |    85 |    4 |\n| Jack         | Science |    82 |    5 |\n+--------------+---------+-------+------+\n\n15 rows selected.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 과목 그룹의 시작마다 랭크가 초기화된다는 것을 주목해주세요. 위의 데이터 세트를 입력으로 사용하여 각 과목 내에서 개별 최고 점수를 강조하는 것은 매우 쉽습니다.\n\n```js\nselect * from\n    (\n    SELECT student_name, subject, score,\n           RANK() OVER (PARTITION BY subject ORDER BY score DESC) AS rank\n    FROM student_scores\n    )\nwhere rank = 1\n\n\n\n| student_name | subject | score | rank |\n|--------------|---------|-------|------|\n| Alice        | Math    |    95 |    1 |\n| Oscar        | History |    92 |    1 |\n| Grace        | Science |    92 |    1 |\n```\n\n## DENSE_RANK\n\nDENSE_RANK 함수는 행에 순차적인 번호를 할당하는 데 rank 함수와 유사합니다. 차이점은 dense_rank가 간격이 없는 번호 시퀀스를 보장한다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n올림픽 경주 비유를 이어가면, dense_rank 조건에서는 동일한 순위에 올라간 두 명이 여전히 금메달을 획득하지만, 그 다음 순위에 온 사람이 두 번째로 인정되어 은메달을 받게 됩니다.\n\n우리의 첫 번째 순위 SQL에서 rank를 dense_rank로 대체하면 내용을 명확히 이해할 수 있습니다. 다음 출력이 나옵니다.\n\n```js\nselect subject, student_name,dense_rank() over(order by subject) rnk  \nfrom student_scores;\n\n\n| subject | student_name | rnk |\n|---------|--------------|-----|\n| History | Kate         |   1 |\n| History | Leo          |   1 |\n| History | Mia          |   1 |\n| History | Nina         |   1 |\n| History | Oscar        |   1 |\n| Math    | Alice        |   2 |\n| Math    | Bob          |   2 |\n| Math    | Charlie      |   2 |\n| Math    | David        |   2 |\n| Math    | Eva          |   2 |\n| Science | Frank        |   3 |\n| Science | Grace        |   3 |\n| Science | Hannah       |   3 |\n| Science | Ivy          |   3 |\n| Science | Jack         |   3 |\n15 rows selected.\n```\n\ndense_rank에 대해 말할 것이 더는 없습니다. 간격 없는 순위 시퀀스가 반드시 필요하다면 순위 분석 대신 사용하면 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## ROW_NUMBER\n\n로우 넘버 분석은 각 파티션 내의 각 행에 고유한 정수 값을 할당합니다. 처음 들었을 때는 다른 두 등수 함수처럼 들릴 수 있지만, 중요한 차이점은 파티션 내에서 \"동점\"이 될 수 있는 레코드들이 다른 랭크가 부여되며, 각 파티션 내의 각 랭크는 그 파티션에 유니크하며 갭이 없다는 것입니다.\n\n다시 말해, 우리의 올림픽 경주 비유를 사용하면, 동시에 선을 획득한 선수 중 하나만 금메달을 획득하게 되고, 나머지는 은메달을 획득하며, 그 다음 순으로 들어온 사람은 동메달을 획득하게 됩니다. 이것은 예시입니다.\n\n```js\nSELECT student_name, subject, score,\n       ROW_NUMBER() OVER (ORDER BY score DESC) AS rn\nFROM student_scores;\n\n+--------------+---------+-------+----+\n| student_name | subject | score | rn |\n+--------------+---------+-------+----+\n| Alice        | Math    |    95 |  1 |\n| Grace        | Science |    92 |  2 |\n| Oscar        | History |    92 |  3 |\n| Ivy          | Science |    90 |  4 |\n| Nina         | History |    90 |  5 |\n| Charlie      | Math    |    90 |  6 |\n| Frank        | Science |    88 |  7 |\n| Leo          | History |    88 |  8 |\n| Bob          | Math    |    85 |  9 |\n| Hannah       | Science |    85 | 10 |\n| Mia          | History |    84 | 11 |\n| Jack         | Science |    82 | 12 |\n| David        | Math    |    80 | 13 |\n| Kate         | History |    78 | 14 |\n| Eva          | Math    |    70 | 15 |\n+--------------+---------+-------+----+\n\n15개 행이 선택되었습니다.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그레이스가 영 매칭 점수를 받은 것과 마찬가지로 오스카도 점수가 같은데 그것보다 상위에 랭크되어 있는 것을 보실 수 있습니다. 점수별 정렬 순서가 결정되지 않았기 때문에 그들의 랭킹 역시 결정적이지 않다는 것을 의미합니다. 위와 같은 쿼리를 다시 실행하면 그들의 순서, 즉 순위 값이 반전될 수도 있습니다.\n\n참고로, over() 절 내부에서 파티션을 지정하지 않았기 때문에 랭킹은 전체 데이터 세트에 적용되어 레코드 번호가 1에서 시작하여 모든 레코드가 처리될 때까지 1씩 증가합니다.\n\n마지막 예제인 row_number() 함수로, 파티션 절을 함께 사용하는 방법은 테이블 데이터의 중복을 제거해야 할 때 매우 유용하다는 것을 보여드리겠습니다. 먼저 좀 더 많은 데이터를 생성하기 위해 약간의 중복된 행을 세 명의 학생, 케이트, 앨리스, 오스카에 대해 삽입할 것입니다.\n\n```js\ninsert into student_scores select * from student_scores\nwhere student_name in ('Alice','Kate','Oscar');\n\n\nselect * \nfrom student_scores;\n+------------+--------------+---------+-------+\n| student_id | student_name | subject | score |\n+------------+--------------+---------+-------+\n|          1 | Alice        | Math    |    95 |\n|          1 | Alice        | Math    |    95 |\n|          2 | Bob          | Math    |    85 |\n|          3 | Charlie      | Math    |    90 |\n|          4 | David        | Math    |    80 |\n|          5 | Eva          | Math    |    70 |\n|          6 | Frank        | Science |    88 |\n|          7 | Grace        | Science |    92 |\n|          8 | Hannah       | Science |    85 |\n|          9 | Ivy          | Science |    90 |\n|         10 | Jack         | Science |    82 |\n|         11 | Kate         | History |    78 |\n|         11 | Kate         | History |    78 |\n|         12 | Leo          | History |    88 |\n|         13 | Mia          | History |    84 |\n|         14 | Nina         | History |    90 |\n|         15 | Oscar        | History |    92 |\n|         15 | Oscar        | History |    92 |\n+------------+--------------+---------+-------+\n\n18 rows selected.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 학생 이름을 기준으로 순위를 매기는 row_number()를 사용하여 순위가 `1인 데이터를 선택함으로써 데이터 테이블에서 중복 레코드를 효과적으로 식별할 수 있습니다. 다음은 이 작업을 수행하는 SQL입니다.\n\n```js\nWITH RankedScores AS (\n    SELECT student_id, student_name, subject, score,\n           ROW_NUMBER() OVER (PARTITION BY student_id, \n           student_name, subject, score ORDER BY student_id) AS rn\n    FROM student_scores\n)\nSELECT student_id, student_name, subject, score\nFROM RankedScores\nWHERE rn \u003e 1;\n\n\n- 학생_id | 학생_이름 | 과목 | 점수\n-|------------|--------------|---------|-------|\n|          1 | Alice        | Math    |    95 |\n|         11 | Kate         | History |    78 |\n|         15 | Oscar        | History |    92 |\n\n3개의 행이 선택됨.\n```\n\n이 식별된 레코드를 사용하여 테이블에서 중복을 제거할 수 있습니다. 다음은 이러한 레코드를 사용하여 테이블을 원래 데이터 세트로 복원하는 인플레이스 삭제를 수행하는 예시입니다.\n\n```js\nDELETE FROM student_scores\nWHERE rowid IN (\n    SELECT rid\n    FROM (\n        SELECT rowid AS rid,\n               ROW_NUMBER() OVER (PARTITION BY student_id, \n               student_name, subject, score ORDER BY student_id) AS rn\n        FROM student_scores\n    )\n    WHERE rn \u003e 1\n);\n\n3개의 행이 삭제됨.\n\n\nSELECT * FROM student_scores;\n\n- 학생_id | 학생_이름 | 과목 | 점수\n-|------------|--------------|---------|-------|\n|          1 | Alice        | Math    |    95 |\n|          2 | Bob          | Math    |    85 |\n|          3 | Charlie      | Math    |    90 |\n|          4 | David        | Math    |    80 |\n|          5 | Eva          | Math    |    70 |\n|          6 | Frank        | Science |    88 |\n|          7 | Grace        | Science |    92 |\n|          8 | Hannah       | Science |    85 |\n|          9 | Ivy          | Science |    90 |\n|         10 | Jack         | Science |    82 |\n|         11 | Kate         | History |    78 |\n|         12 | Leo          | History |    88 |\n|         13 | Mia          | History |    84 |\n|         14 | Nina         | History |    90 |\n|         15 | Oscar        | History |    92 |\n\n15개의 행이 선택됨.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## NTILE\n\nNTILE 함수를 사용하면 데이터 세트를 대략적으로 동일한 크기의 레코드 그룹으로 나눌 수 있습니다. 이러한 그룹을 \"타일\"이라고 하며, 동일한 타일 내의 모든 항목에는 동일한 순위가 할당됩니다.\n\n아래 예시에서는 학생들의 성적을 기반으로 테이블을 4개의 그룹으로 세분화하려고 합니다. 다시 말해, 각 그룹(또는 타일)은 실제 데이터 세트의 범위 내에서 대략적으로 유사한 점수를 가져야 합니다.\n\n각 그룹 내의 레코드 수는 미리 알 수 없습니다. 요청한 타일 수가 출력에 표시되지만, 두 개의 그룹에는 3개의 레코드가 있고 다른 두 그룹에는 각각 4개와 5개의 레코드가 포함되어 있습니다. SQL이 보장할 수 있는 것은 가능한 경우 요청한 타일 수를 반환한다는 것뿐입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\nSELECT \n    student_name,\n    subject,\n    score,\n    NTILE(4) OVER (ORDER BY score) AS tile_rank\nFROM \n    student_scores;\n\n\n| student_name | subject | score | tile_rank  |\n|--------------|---------|-------|------------|\n| Eva          | Math    |    70 |          1 |\n| Kate         | History |    78 |          1 |\n| David        | Math    |    80 |          1 |\n| Jack         | Science |    82 |          2 |\n| Mia          | History |    84 |          2 |\n| Bob          | Math    |    85 |          2 |\n| Hannah       | Science |    85 |          2 |\n| Frank        | Science |    88 |          3 |\n| Leo          | History |    88 |          3 |\n| Ivy          | Science |    90 |          3 |\n| Nina         | History |    90 |          3 |\n| Charlie      | Math    |    90 |          3 |\n| Grace        | Science |    92 |          4 |\n| Oscar        | History |    92 |          4 |\n| Alice        | Math    |    95 |          4 |\n\n 15 rows selected\r\n```\n\n- NTILE(4)는 행을 4개의 타일 또는 그룹으로 나누고자 함을 나타냅니다.\n- OVER (ORDER BY score) 파티션 절을 지정하지 않았기 때문에 SQL은 데이터세트를 위해 테이블의 모든 레코드를 고려하고 이 레코드들을 점수 열을 기준으로 순서대로 정렬한 후 그것들을 네 개의 타일로 분할하도록 지시합니다.\n\n## 요약\n\n마무리로, 현대 SQL 시스템에서 가장 흔한 4가지 SQL 랭킹 함수를 강조하고 그 사용 예시를 보여드렸습니다. 이 함수들은 모두 상대적인 순서에 따라 레코드에 랭킹 값을 할당하며, RANK, DENSE_RANK 및 ROW_NUMBER는 동점 처리를 다루고 순번을 지정하는 방식이 다르게 제공합니다. 한편, NTILE은 데이터에 대한 통계 분석이나 세분화 분석을 수행해야 하는 경우에 유용합니다. 이러한 기능을 가능한 많이 사용하고 일상적으로 사용해 보시기를 권장합니다. 데이터 분석, 조작 및 보고를 위한 강력한 도구이기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 컨텐츠가 마음에 드셨다면, 이 기사들도 흥미롭게 보실 것 같아요.","ogImage":{"url":"/assets/img/2024-06-19-SQLExplainedRankingAnalytics_0.png"},"coverImage":"/assets/img/2024-06-19-SQLExplainedRankingAnalytics_0.png","tag":["Tech"],"readingTime":15},{"title":"제목 제로부터 dbt까지 스포티파이의 백만 개 플레이리스트 데이터를 분석하고 데이터 모델 구축하는 방법","description":"","date":"2024-06-19 09:52","slug":"2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_0.png\" /\u003e\n\n앞으로 몇 주 동안, dbt (data build tool)를 사용하여 Spotify의 백만 개 플레이리스트 데이터셋을 엔드 투 엔드 분석 프로젝트로 변환하는 방법을 안내할 것입니다. 중소형 대형 실제 세계 원시 데이터를 상호 작용적인 데이터 모델로 변환하는 방법을 배우게 될 거에요. (어떤걸 🤣 기반으로 한 George Orwell의 하층층상 중간층에요)\n\n## 배울 내용\n\n- 30GB의 원시 JSON 데이터를 효율적이고 확장 가능하게 5GB Parquet 파일로 변환하기.\n- Parquet 파일을 심층적인 탐색과 분석을 위한 여러 dbt 모델로 변환하기.\n- 데이터 변환 프로세스에서 dbt를 사용하는 것이 왜 최선의 실천법인지 이해하기.\n- 데이터 무결성과 정확성을 보장하기 위해 각 dbt 모델 변경을 검증하는 방법에 대해 배우기 (스포일러: 오픈 소스 dbt 모델 코드 리뷰 도구인 Recce를 사용하세요).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 데이터로부터 중요한 질문에 답변해주세요\n\n매주 Recce LinkedIn 페이지에 스포티파이 데이터셋에 관한 두 가지 질문을 게시할 것입니다. 예를 들어,\n\n- 적어도 3곡의 테일러 스위프트 노래를 포함하는 재생 목록은 몇 개인가요?\n- 제이 체오의 인기 있는 상위 10곡은 무엇인가요?\n- BLACKPINK 💗과 Post Malone이 모두 포함된 재생 목록은 몇 개인가요?\n\n그 후에 투표를 가장 많이 받은 질문을 오픈 소스 저장소에 구현할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 누구를 위한 것인가요?\n\n비즈니스 BI 또는 ML을 위해 데이터 변환에 dbt를 사용하는 방법에 관심이 있는 모든 분들을 환영합니다. 뿐만 아니라, 데이터 또는 분석 엔지니어로 계속된 작업에 유용한 몇 가지 dbt 모베스트 사항을 함께 공유할 예정입니다.\n\n# 백만 플레이리스트 데이터 준비하기\n\n시작할 준비가 되셨나요? 멋지네요. 이 프로젝트에서는 스포티파이 백만 플레이리스트 데이터셋을 사용할 예정입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_1.png)\n\n## 데이터셋 다운로드\n\nSpotify 정책에 따라 등록하고 여기서 원시 데이터를 다운로드해야 합니다. 우리는 spotify_million_playlist_dataset.zip 파일을 사용할 거에요 (크기는 5.4 GB 👀).\n\n이 zip 파일은 31GB로 풀리니 충분한 공간이 있는지 확인해주세요! (나중에 Parquet으로 변환하면 용량이 줄어듭니다)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](https://miro.medium.com/v2/resize:fit:960/1*HbIZkLZc-9ClzgToTNLvsg.gif)\n\n데이터셋을 다운로드하고 압축 해제한 후, data 폴더에는 천 개의 분할된 JSON 파일로 구성되어 있음을 발견할 것입니다. 이러한 파일들은 다음과 같은 패턴으로 명명되어 있습니다:\n\n- mpd.slice.0–999.json\n- mpd.slice.1000–1999.json\n- …\n- mpd.slice.999000–999999.json\n\n분할된 JSON 파일 중 하나에서 플레이리스트 항목의 전형적인 예시는 다음과 같습니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![링크](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_2.png)\n\n데이터셋의 각 JSON 파일은 1,000개의 재생목록을 나타내며, 총 1백만 개의 재생목록이 포함되어 있습니다. 파일 접두사 \"mpd\"는 \"Million Playlist Dataset\"의 약자입니다.\n\nSpotify 팀은 이러한 JSON 파일의 무결성을 확인하고 MD5 체크섬을 사용하여 기본 통계를 계산하는 데 도움이 되도록 ./src 폴더에 스크립트를 제공했습니다. 아래 명령어로 기본 통계를 계산할 수 있습니다:\n\n```js\n$ python src/stats.py data\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSpotify의 README 문서에 따르면, 이 프로그램의 결과물은 'stats.txt' 내용과 일치해야 합니다. stats.py의 실행 시간은 노트북의 성능에 따라 다를 수 있으며, 30분을 초과할 수도 있습니다.\n\n# 초기 인사이트\n\n우리는 우선적으로 몇 가지 탐구를 시작해 초기 인사이트를 얻고 데이터셋을 더 잘 이해할 것입니다. 이 작업은 raw json을 사용하여 이루어질 것이지만, 더 고급 데이터 상호작용을 위해서는 데이터를 더 효율적인 형식으로 변환해야 할 것입니다. 이에 Parquet을 사용할 것이며 (자세한 내용은 아래에 소개되어 있음), 이는 dbt와 함께 사용하기에 이상적이며 raw 데이터를 변환하는 데 유용합니다.\n\nSpotify의 1000개 raw 데이터 파일로 되돌아가보죠. 모든 데이터 분석 처리 워크플로우에서 겪었던 노고와 눈물이 어떤 것이었는지 보여드리겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# DuckDB 및 jq\n\nDuckDB와 jq는 모두 JSON 데이터와 상호 작용하기 위한 훌륭한 도구입니다. 이 멋진 도구들을 설치하려면 선호하는 패키지 관리자를 사용하십시오. 예를 들어:\n\n```sh\n$ brew install duckdb\n$ brew install jq\n```\n\n## JSON 구조 이해하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 JSON 데이터를 빠르게 확인하기 위해 jq를 사용할 수 있습니다. 이후 보다 심층적인 분석을 위해 DuckDB를 활용할 수 있습니다. 1000개 파일 중 하나를 살펴보겠습니다:\n\n```js\n$ cd spotify_million_playlist_dataset/data\n$ jq 'keys' mpd.slice.0-999.json\n```\n\nMarkdown 양식으로 표를 변경했습니다:\n\n![표](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_3.png)\n\nJSON의 모든 조각은 두 개의 키만 포함하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 정보 — 이것은 단순히 JSON 파일의 메타데이터입니다.\n- 재생 목록 — 실제로 관심 있는 데이터\n\n아마도 \"재생 목록\"이 배열이라는 것을 짐작하실 수 있습니다. 따라서 재생 목록에서 첫 번째 요소를 살펴보겠습니다.\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_4.png)\n\n부분 재생 목록 데이터(첫 번째 트랙만 표시)는 다음과 같이 보일 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_5.png)\n\n# 덕DB로 분석하기\n\nJSON 데이터의 구조를 파악한 후에는 DuckDB를 사용하여 데이터를 분석할 수 있습니다. DuckDB는 SQL 데이터 유형을 JSON 파일 내에서 자동으로 감지하는 기능을 제공하므로 분석에 SQL 구문을 손쉽게 적용할 수 있습니다.\n\n## DuckDB 대화형 셸 열기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n터미널에 duckdb를 입력하면 PostgreSQL의 psql 및 SQLite 셸과 유사한 대화형 셸에 들어갈 수 있어요.\n\n```js\n$ duckdb\n```\n\n## DuckDB의 maximum_object_size 조정\n\n다음 명령을 실행하면 아래의 오류가 표시됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nSELECT * FROM read_json_auto('./mpd.slice.0-999.json');\n\n-- \"maximum_object_size\" of 16777216 bytes exceeded \n-- while reading file \"./mpd.slice.0-999.json\" (\u003e33554428 bytes).\n-- \"maximum_object_size\"을 늘려주세요.\n```\n\n이 오류는 밀리언 플레이리스트 데이터셋의 JSON 슬라이스가 DuckDB의 기본 maximum_object_size보다 크기 때문에 발생했습니다. 따라서 이를 조정하여 40MB로 설정해야 합니다 🫰:\n\n```js\nSELECT * \nFROM read_json_auto('./mpd.slice.0-999.json', maximum_object_size = 40000000); \n\n┌──────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n│         info         │                                                                            playlists                                                                             │\n│ struct(generated_o…  │ struct(\"name\" varchar, collaborative varchar, pid bigint, modified_at bjigint, num_tracks bigint, num_albums bigint, num_followers bigint, tracks struct(pos bi…  │\n├──────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n│ {'generated_on': 2…  │ [{'name': Throwbacks, 'collaborative': false, 'pid': 0, 'modified_at': 1493424000, 'num_tracks': 52, 'num_albums': 47, 'num_followers': 1, 'tracks': [{'pos': …  │\n└──────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n```\n\n## JSON 해제하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 컬럼 안에 중첩 구조인 재생 목록에만 관심이 있다는 것을 알고 있습니다. 그러므로 플레이리스트 열을 정규화하기 위해 UNNEST를 사용할 수 있습니다:\n\n```js\nSELECT UNNEST(playlists) \nFROM read_json_auto('./mpd.slice.0-999.json', maximum_object_size = 40000000) \nLIMIT 5;\n\n┌─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n│                                                                                    unnest(playlists)                                                                                    │\n│ struct(\"name\" varchar, collaborative varchar, pid bigint, modified_at bigint, num_tracks bigint, num_albums bigint, num_followers bigint, tracks struct(pos bigint, artist_name varch…  │\n├─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n│ {'name': Throwbacks, 'collaborative': false, 'pid': 0, 'modified_at': 1493424000, 'num_tracks': 52, 'num_albums': 47, 'num_followers': 1, 'tracks': [{'pos': 0, 'artist_name': Missy …  │\n│ {'name': Awesome Playlist, 'collaborative': false, 'pid': 1, 'modified_at': 1506556800, 'num_tracks': 39, 'num_albums': 23, 'num_followers': 1, 'tracks': [{'pos': 0, 'artist_name': …  │\n│ {'name': korean , 'collaborative': false, 'pid': 2, 'modified_at': 1505692800, 'num_tracks': 64, 'num_albums': 51, 'num_followers': 1, 'tracks': [{'pos': 0, 'artist_name': Hoody, 't…  │\n│ {'name': mat, 'collaborative': false, 'pid': 3, 'modified_at': 1501027200, 'num_tracks': 126, 'num_albums': 107, 'num_followers': 1, 'tracks': [{'pos': 0, 'artist_name': Camille Sai…  │\n│ {'name': 90s, 'collaborative': false, 'pid': 4, 'modified_at': 1401667200, 'num_tracks': 17, 'num_albums': 16, 'num_followers': 2, 'tracks': [{'pos': 0, 'artist_name': The Smashing …  │\n└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n```\n\nDuckDB는 UNNEST 함수에서 `recursive := true`와 같이 매우 편리한 옵션을 제공합니다. 이 옵션은 열을 재귀적으로 정규화합니다:\n\n```js\nSELECT UNNEST(playlists, recursive := true) \nFROM read_json_auto('./mpd.slice.0-999.json', maximum_object_size = 40000000) \nLIMIT 5;\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이렇게 하면 깊게 중첩된 JSON 데이터를 다루기가 매우 편리합니다.\n\n## JSON을 단일 표로 결합\n\n현재, 우리는 하나의 JSON 파일만 처리하고 있습니다. 만약 1,000개의 나누어진 JSON 파일을 모두 한 표로 합치고 싶다면 어떻게 해야 할까요?\n\nDuckDB는 여러 JSON 파일을 한 번에 읽을 수 있게 해주는 glob 구문을 제공합니다. `./mpd.slice.0-999.json`을 `./mpd.slice*.json`로 수정하면 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n테이블 태그를 Markdown 형식으로 변경하세요.\n\nCREATE TABLE playlists AS \nSELECT UNNEST(playlists , recursive:= true) \nFROM read_json_auto('./mpd.slice*.json', maximum_object_size = 40000000);\n```\n\n내 노트북(M3 MacBook)에서 playlists DuckDB 테이블을 만드는 데 30초가 걸렸어요. 이제 데이터를 Parquet으로 변환할 준비가 되었어요.\n\n## Parquet으로 변환\n\n변환 과정 중간에 메모리 부족 오류를 방지하기 위해 일부 임시 파일이 필요할 수 있습니다. DuckDB 쉘에서 계속하여, 먼저 다음을 실행하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nSET temp_directory='./tmp';\n```\n\n이제 DuckDB 테이블에서 플레이리스트를 Parquet 파일로 내보낼 준비가 되었습니다. copy 명령을 사용하여 .parquet 확장자를 갖는 파일을 지정하면 DuckDB가 자동으로 Parquet 파일로 내보내기를 원한다는 것을 알게 됩니다.\n\n```js\nCOPY playlist TO 'playlists.parquet';\n```\n\n쉽죠?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Parquet 대 JSON\n\n그래서, 왜 Parquet을 사용해야 할까요?\n\nParquet은 분석을 위한 우수한 파일 포맷으로, 컬럼 저장 방식을 통해 JSON에 비해 주목할만한 장점을 제공합니다. 이 설계는 데이터 압축 및 인코딩을 향상시켜 저장 공간을 줄이고 데이터 분석 워크플로우의 데이터 액세스 속도를 높이는데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n복잡한 중첩 데이터 구조도 지원하며 기존 데이터를 수정하지 않고 새 열을 추가할 수 있는 유연한 스키마 진화를 제공합니다. 이는 스키마 변경이 자주 발생하는 시나리오에 이상적인 형식이 됩니다.\n\n또한, 주요 데이터 웨어하우스와의 호환성을 통해 Parquet은 특히 dbt 사용자에게 매우 중요하며 데이터 통합 및 분석 워크플로우를 간소화합니다. 즐겨 사용하는 데이터 웨어하우스에서 쉽게 Parquet 파일을 가져오고 내보낼 수 있습니다.\n\nDuckDB와 jq를 사용하면 기가바이트의 JSON 데이터를 노트북에서 간단하게 분석할 수 있습니다.\n\n\"ON - YOUR - LAPTOP\"을 반복해보세요 💻\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 요약\n\n원본 데이터 세트는 1,000개의 JSON 파일로 이루어져 있으며 총 31GB입니다.\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_7.png)\n\n세 가지 간단한 DuckDB 쿼리를 실행한 후 1분의 처리 시간을 거쳐 단일 5.7GB Parquet 파일을 얻게 되어, 500% 개선이 이뤄졌습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_8.png)\n\n지금은 몇 초 안에 노트북으로 \"플레이리스트에 테일러 스위프트 노래가 몇 개 있는지?\"와 같은 질문에 빠르게 답변할 수 있습니다. 마음이 홀립니다.\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_9.png)\n\n이 데이터 분석 프로젝트의 첫 번째 부분은 여기까지입니다. 곧 두 번째 부분도 뵙겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 다음에는...\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_10.png)\n\n데이터셋을 Parquet 파일로 성공적으로 전환한 후, 다음 목표는 dbt의 파워를 활용하여 One Million Playlists 데이터셋에서 더 깊고 더 매력적인 분석적 인사이트를 발굴하는 것입니다.\n\n## 데이터에 소프트웨어 엔지니어링 최상의 실천 방법 적용\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n덕DB 셸은 데이터 집합을 대화식으로 분석할 수 있는 기능을 제공하지만, 우리의 SQL 변환에 보다 구조화되고 협업적이며 버전 관리된 접근이 필요함을 알 수 있습니다.\n\n여기서 dbt가 빛을 발합니다 🤩. dbt를 사용하면 데이터 변환을 코드로 처리할 수 있어 소프트웨어 엔지니어링 관행인 버전 관리, 코드 리뷰(Recce 빛나요 💖), 그리고 자동화된 테스트를 데이터 워크플로에 적용할 수 있습니다.\n\n## 함께 작업하기\n\n여러 SQL 쿼리를 논리적인 dbt 모델로 구성함으로써, 데이터 변환의 명확성과 유지 관리성을 향상시킬 뿐만 아니라, 데이터 팀이 서로 협력하여 서로의 작업을 점진적으로 빌드할 수 있습니다. 이 협업적인 접근은 데이터 모델이 견고하고 정확하며 최신 비즈니스 로직과 분석적 통찰을 반영하도록 보장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 신뢰할 수 있는 환경\n\n또한, dbt의 문서 기능을 사용하면 데이터 모델의 포괄적인 문서를 자동으로 생성하여 새 팀원들이 데이터 환경을 이해하기 쉽고 이해관계자들이 데이터 주도적 의사결정을 신뢰할 수 있게 합니다.\n\n## 데이터 주도적 개발\n\n요약하면, dbt는 SQL 변환을 효율적으로 관리할 수 있는 필수 도구와 함께 제공하여 협업적이고 반복적인 데이터 문화를 육성하는 데 도움이 되어, 오늘날의 데이터 주도적 세상에서 경쟁력을 유지하는 데 필수적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Part 2에서 만나요\n\n최신 소식을 받아보고 더 흥미로운 소식을 확인하려면 LinkedIn을 팔로우하세요! 🤩\n\n업데이트: Part 2가 이제 사용 가능합니다. 저의 샘플 프로젝트를 따라가면서 dbt가 데이터 프로젝트 모델링에 적합한 이유를 살펴보겠습니다. 아, 그리고 중요한 Spotify 플레이리스트 질문에 대해 답변도 해드립니다!\n\n# 파이프라인에서 더 많은 기사","ogImage":{"url":"/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_0.png"},"coverImage":"/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_0.png","tag":["Tech"],"readingTime":12},{"title":"데이터, AI 서밋에서 얻은 교훈 파트 II","description":"","date":"2024-06-19 09:50","slug":"2024-06-19-DataAISummitTakeawaysPartII","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-DataAISummitTakeawaysPartII_0.png\" /\u003e\n\n# 소개\n\n지난 주 데이터 및 AI 써밋에서 전체 세션을 빠르게 업로드해 준 Databricks에 큰 감사를 전합니다. 그들 모두를 살펴보는 것은 불가능하지만, 내가 확인한 몇 가지 가운데 내가 좋아하는 이야기는 다음과 같습니다.\n\n# Spark 업그레이드/마이그레이션\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대량 업그레이드와 마이그레이션은 플랫폼 팀에게 언제나 쉬운 작업이 아닙니다. 넷플릭스는 이 세션에서 모든 Spark 작업에 대한 인벤토리를 작성하고 마이그레이션 제어 및 자동화 플레인을 호스팅하며, 심지어 마이그레이션 프로세스에 유효성 검사 및 관측 기능을 통합하는 방법을 설명했습니다. 넷플릭스가 운영하는 규모를 고려하면, 그것은 매우 인상적입니다.\n\n우리는 이제까지 기관 전체적인 Spark 업그레이드를 하지 않았습니다(하지만 Databricks 런타임이 관련될 때 특히 그렇게 할 생각이 좋을 수도 있습니다). 하지만 이것은 어떻게 그겢을 달성할 수 있는 유용한 프레임워크를 제공합니다. 개별 팀이 업그레이드를 도울 수 있는 프로세스를 구축하고 적절한 관측 기능을 갖추어 각 팀이 너무 뒤처져 있지 않도록 확인하는 것이 의미가 있을지도 모릅니다. 우리 플랫폼 팀이 처리해야 할 다른 책임을 고려하면, 그렇게 하는 것이 관리하기 쉬워 보입니다.\n\n# 가드레일\n\n이전 게시물에서, 저는 Databricks 사용을 관리하는 데 중요한 컴퓨팅 정책의 중요성에 대해 썼습니다. 프로비던스는 Unity 마이그레이션 동안 훌륭한 일련의 가드레일을 소개했습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 클러스터 유형 제한\n- 대화식 클러스터 예약 불가\n- 운영 중 대화식 클러스터 미사용\n- 노드 수와 인스턴스 유형에 대한 제어 설정\n\n대화식 클러스터 예약을 허용하지 않는 것은 중요한 사항입니다. 모든 워크플로우는 이상적인 클러스터 구성을 위해 작업 클러스터를 사용해야 하며 대화식 클러스터처럼 일반적으로 보이는 유휴 기간을 피해야 합니다. (또한 작업 컴퓨팅이 대화식 컴퓨팅보다 저렴하다는 점도 말이죠).\n\n운영 중 대화식 클러스터를 사용하지 않는 것은 이전에 생각해보지 못한 부분이었는데, 합리적으로 보입니다. 하위 환경에서 생산 데이터를 읽기 위한 필요한 액세스가 있고 비상 시나리오를 위한 경우를 제외하고 별도의 운영 대화식 클러스터가 정말 필요할까요? 아마도 그렇지 않기 때문에 처음부터 허용하지 않는 것이 좋습니다.\n\n# 차원 모델링\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래, 당신은 아마 데이터 및 AI 서밋에서 차원 모델링 세션을 듣기 위해 참석하지 않았을 것 같지만, 컬럼형 데이터베이스를 적절히 설계할 때 장점이 있어요. 사실 테이블, 차원 테이블 및 다양한 유형의 천천히 변화하는 차원들 사이의 차이를 알아두면 정말 유용할 거예요. (여기서 네 가지만 있다고 생각했는데...)\n\n이 대화는 성능 대 개인 요구사항에 대한 더 나은 지원의 상충관계인 큰 하나의 테이블(OBT) 주변의 전형적인 사례를 언급하기에 잘 했어요. 기존 데이터 업데이트, 거버넌스 및 전반적인 중복성에 관해서는 OBT 접근 방식에는 단점이 있지만, 가끔 고려할 가치가 있어요.\n\n# 결론\n\n아마도 앞으로 몇 일 동안 더 많은 세션을 확인하게 될 것 같아요, 여기서 매우 훌륭한 시작이었어요. 다시 한 번 Databricks에 이러한 멋진 행사를 주최해준 것에 감사드려요.","ogImage":{"url":"/assets/img/2024-06-19-DataAISummitTakeawaysPartII_0.png"},"coverImage":"/assets/img/2024-06-19-DataAISummitTakeawaysPartII_0.png","tag":["Tech"],"readingTime":2},{"title":"특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 ","description":"","date":"2024-06-19 09:48","slug":"2024-06-19-DataEngineeringpipelineleveragingAirflowKafka","content":"\n\n# 소개\n\n이 문서에서는 Apache Airflow 및 Kafka(오픈 소스)를 활용하여 실시간 날씨 업데이트를 읽고 Twitter의 이벤트 스트림을 분석하여 대중의 반응을 이해하는 데이터 엔지니어링 파이프라인 아키텍처를 솔루션 디자인하는 방법에 대한 기본적인 개요를 제공합니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png)\n\n# ETL 기본 원칙\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nETL은 데이터 웨어하우스 또는 데이터 마트와 같은 분석 환경을 위해 데이터 획득 및 준비를 자동화하는 데이터 파이프라인 엔지니어링 방법론인 추출, 변환, 로드를 의미합니다. 이는 다양한 소스에서 데이터를 수집하고 표준 형식으로 편집한 다음 시각화, 탐색 및 모델링을 위한 새로운 환경으로 로드하는 것을 포함하며, 자동화 및 의사 결정을 지원합니다.\n\n# ELT 기본\n\nELT 프로세스는 추출, 로드 및 변환을 나타내며, 단곅하계의 단계 순서로 인한 독특한 차이로 인해 ETL과 다릅니다. ELT에서는 데이터가 데이터 레이크와 같은 목적지 환경으로 원본 형식 그대로 직접 로드됩니다. 이를 통해 목적지 플랫폼 내에서 필요에 따라 변환을 수행하고 동적으로 사용자 주도적 변경을 가능하게 합니다.\n\n# ETL과 ELT 비교\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nETL과 ELT의 차이점: ETL 파이프라인에서는 변환 작업이 목적지에 도달하기 전에 데이터 파이프라인 내에서 발생하는 반면, ELT는 변환 작업을 분리하여 목적지 환경에서 필요한 대로 수행할 수 있습니다. ETL은 엄격하고 목적이 명확하지만, ELT는 유연하며 빅 데이터 처리에 대한 셀프 서비스 분석을 제공합니다.\n\n# 데이터 수집 기술\n\n다양한 데이터 수집 기술에는 다음이 포함됩니다:\n\n- 완전 수집 대 부분 수집\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 전체 로딩: 데이터베이스에 초기 히스토리를 로드합니다. 추적 데이터는 새 창고에서 시작됩니다.\n- 점진적 로딩: 새 데이터를 삽입하거나 이미 로드된 데이터를 업데이트합니다. 거래 히스토리를 누적하는 데 사용됩니다. 데이터 양과 속도에 따라 일괄 또는 스트림 로드될 수 있습니다.\n\n정기 로딩 vs. 요청 로딩:\n\n- 정기 로딩: 매일 거래를 데이터베이스에 주기적으로 로드하며, 스크립트 작업에 의해 자동화됩니다.\n- 요청 로딩: 소스 데이터가 지정된 크기에 도달하거나 움직임, 소리 또는 임의의 변경 이벤트와 같은 다양한 이벤트에 의해 트리거됩니다.\n\n일괄 처리 vs. 스트림 로딩:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 배치 로딩: 시간별로 정의된 단위로 데이터를 로드하며 일반적으로 몇 시간에서 며칠 동안 누적됩니다.\n- 스트림 로딩: 데이터가 제공되는 즉시 실시간으로 로드됩니다.\n- 마이크로 배치 로딩: 즉시 처리를 위한 최근 데이터에 액세스합니다.\n\n푸시 대 수신 데이터 로딩:\n\n- 수신 방법: 클라이언트가 서버로부터 데이터를 요청합니다(예: RSS 피드, 이메일).\n- 푸시 방법: 클라이언트가 서버 서비스에 구독하여 데이터를 실시간으로 전달 받습니다(예: 푸시 알림, 즉각 메시징 서비스).\n\n# 데이터 파이프라인이란 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 파이프라인은 데이터의 이동 또는 수정에 특히 관련이 있어요. 이러한 파이프라인은 데이터를 한 곳이나 형식에서 다른 곳이나 형식으로 운송하는 것을 목표로 하며, 데이터를 추출하고 최종적으로 적재하기 위해 선택적 변환 단계를 통해 안내하는 시스템을 구성합니다.\n\n파이프라인을 통해 흐르는 데이터를 시각화하는 것은 데이터 패킷으로 표현할 수 있으며, 이는 데이터의 단위를 넓게 이야기합니다. 이러한 패킷은 단일 레코드나 이벤트에서 대량 데이터 수집물까지 다양할 수 있어요. 이 맥락에서 데이터 패킷은 파이프라인으로 흡수되기 위해 대기열에 정리되며, 데이터 파이프라인의 길이는 단일 패킷이 횡단하는 데 걸리는 시간을 의미합니다. 패킷 간의 화살표는 처리량 지연이나 연속 패킷 도착 사이의 시간을 나타냅니다.\n\n데이터 파이프라인 주요 성능 지표\n\n- 지연 시간: 데이터 패킷이 파이프라인을 통과하는 총 시간을 의미합니다. 지연 시간은 파이프라인 내 각 처리 단계에서 소요된 개별 시간의 합으로, 파이프라인 내 가장 느린 프로세스에 의해 제한됩니다. 예를 들어, 웹 페이지의 로딩 시간은 서버 속도에 따라 결정되며 인터넷 서비스 속도와 관계없이 서버 속도에 의해 제어됩니다.\n- 처리량: 이는 시간 단위당 파이프라인을 통해 처리될 수 있는 데이터 양을 의미합니다. 처리량을 증가시키는 것은 시간 단위당 더 많은 패킷을 처리하고 큰 상자를 차례차례 통과시키는 우리 친구 사슬 예시와 유사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 파이프라인 응용:\n\n- 간단한 복사 파이프라인: 파일 백업과 같이 데이터를 한 위치에서 다른 위치로 복사하는 작업을 포함합니다.\n- 데이터 레이크 통합: 분산된 래 데이터 소스를 데이터 레이크에 통합하는 작업입니다.\n- 거래 기록 이동: 거래 기록을 데이터 웨어하우스로 전송하는 작업입니다.\n- IoT 데이터 스트리밍: IoT 장치에서 데이터를 스트리밍하여 대시보드나 경보 시스템에서 정보를 제공하는 것을 말합니다.\n- 기계 학습용 데이터 준비: 기계 학습의 개발이나 제품화를 위해 래 데이터를 준비하는 작업입니다.\n- 메시지 보내기 및 받기: 이메일, SMS 또는 온라인 비디오 회의와 같은 애플리케이션을 포함합니다.\n\n# 주요 데이터 파이프라인 프로세스\n\n데이터 파이프라인 프로세스는 일반적으로 구조화된 일련의 단계를 따릅니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 추출: 하나 이상의 소스에서 데이터를 검색하는 과정입니다.\n- 투입: 추출된 데이터는 후속 처리를 위해 파이프라인에 투입됩니다.\n- 변환: 파이프라인 내의 선택적인 단계에서 데이터를 변환할 수 있습니다.\n- 로딩: 최종 단계는 변환된 데이터를 대상 시설로 로드합니다.\n- 스케줄링/트리거링: 작업을 필요에 따라 예약하거나 트리거하는 메커니즘입니다.\n- 모니터링: 전체 워크플로우가 효율적으로 작동하도록 모니터링됩니다.\n- 유지보수 및 최적화: 원활한 파이프라인 운영을 보장하기 위해 정기적인 유지보수 및 최적화 작업이 수행됩니다.\n\n# Apache Airflow\n\n- Python 기반의 오픈 소스 \"구성과 코드\" 플랫폼입니다. AirBNB에서 오픈 소스로 공개되었습니다.\n- 데이터 파이프라인 워크플로우를 작성, 예약 및 모니터링할 수 있습니다.\n- 확장 가능하며 병렬 컴퓨팅 노드를 지원하며 주요 클라우드 플랫폼과 통합됩니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 스케줄러: 예약된 워크플로우를 트리거합니다.\n- 실행기: 작업을 Worker에 할당하여 실행합니다.\n- 웹 서버: DAG 검사, 트리거 및 디버깅을 위한 대화형 UI를 호스팅합니다.\n- DAG 디렉토리: 스케줄러, 실행기 및 Worker가 액세스할 수 있는 DAG 파일을 저장합니다.\n- 메타데이터 데이터베이스: 각 DAG 및 해당 작업의 상태를 유지합니다.\n\nDAG 및 작업 라이프사이클\n\nDAG(유향 비순환 그래프)는 작업 간의 종속성과 실행 순서를 지정합니다. 'DAG'는 순환이나 사이클이 없는 관계를 나타내는 특정 유형의 그래프로, 노드와 간선으로 구성되며 방향성을 가진 간선이 노드 간의 흐름을 보여줍니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n작업 상태:\n\n- 상태 없음: 실행을 위해 대기 중인 작업.\n- 스케줄됨: 의존성에 따라 실행이 예약된 작업.\n- 제거됨: 실행이 시작된 이후에 사라진 작업.\n- 상류 작업 실패: 상류 작업에서 실패 발생.\n- 대기 중: 워커 가용성을 기다리는 작업.\n- 실행 중: 워커에 의해 실행 중인 작업.\n- 성공: 오류 없이 작업이 완료된 상태.\n- 실패: 실행 중에 오류가 발생한 작업.\n- 재시도 예정: 남은 재시도 횟수가 남아 있는 실패한 작업으로, 다시 예약됨.\n- 이상적인 작업 흐름: '상태 없음'에서 '스케줄됨'으로, '대기 중'으로, '실행 중'으로 이어져 '성공'으로 마무리됨.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_3.png)\n\nAirflow DAG 스크립트의 논리 블록\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 라이브러리 가져오기: 필요한 Python 라이브러리 가져오기.\n- DAG 인수: DAG에 대한 기본 인수(시작 날짜와 같은 것) 정의.\n- DAG 정의: 특정 속성을 사용하여 DAG 인스턴스화.\n- 작업 정의: DAG 내부의 개별 작업(노드) 정의.\n- 작업 파이프라인: 작업 간의 의존성을 지정하여 작업 간의 흐름을 구축.\n\n이러한 논리적 블록이 포함된 Python 스크립트 예제를 살펴보세요.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_4.png)\n\n# Kafka를 활용한 스트리밍 파이프라인 구축\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이벤트는 이벤트 스트리밍의 맥락에서 엔티티의 관찰 가능한 상태 업데이트를 설명하는 데이터를 의미합니다. 예시로는 자동차의 GPS 좌표, 방 온도, 또는 응용 프로그램의 RAM 사용량 등이 있습니다.\n\n이벤트는 다양한 형식으로 제공됩니다:\n\n- 텍스트, 숫자 또는 날짜와 같은 원시 유형\n- 값이 원시 또는 복합 유형인 키-값 쌍 형식(e.g., JSON, XML)\n- 타임스탬프가 포함된 시간 감도를 위한 키-값 형식\n\n한 소스에서 한 대상으로: 이벤트 스트리밍은 소스(센서, 데이터베이스, 응용 프로그램)가 실시간 이벤트를 지속적으로 생성하고 이를 대상지(파일 시스템, 데이터베이스, 응용 프로그램)로 전달하는 것을 의미합니다. 이 과정은 이벤트 스트리밍이라고 불립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은 출처에서 많은 대상으로: 다양한 통신 프로토콜(FTP, HTTP, JDBC, SCP)을 사용하는 여러 분산 이벤트 소스 및 대상을 관리하는 것은 도전이 될 수 있습니다. 이벤트 스트림 플랫폼(ESP)은 미들웨어로 작용하여 다양한 이벤트 기반 ETL의 처리를 간단하게 만듭니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_5.png)\n\nESP 구성 요소\n\n- 이벤트 브로커: 이벤트를 수신하고 소비하는 핵심 구성 요소입니다.\n- 이벤트 저장소: 받은 이벤트를 저장하여 대상이 비동기적으로 검색할 수 있도록 합니다.\n- 분석 및 쿼리 엔진: 저장된 이벤트를 쿼리하고 분석합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이벤트 브로커는 중요한 컴포넌트입니다. 이는 다음과 같은 구성 요소들을 포함합니다:\n\n- Ingester: 다양한 소스에서 이벤트를 효율적으로 수신합니다.\n- Processor: 직렬화, 역직렬화, 압축, 압축 해제, 암호화 및 복호화와 같은 작업을 수행합니다.\n- Consumption: 저장소에서 이벤트를 검색하고 구독된 대상에게 분배합니다.\n\n인기 있는 이벤트 처리 시스템 솔루션:\n\n- Apache Kafka\n- Amazon Kinesis\n- Apache Flink\n- Apache Spark\n- Apache Storm\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아파치 카프카: 독특한 기능과 광범위한 응용 시나리오를 갖춘 가장 인기 있는 ESP 중 하나입니다. 카프카는 분산 클라이언트-서버 모델을 따릅니다.\n\n- 서버 측: 효율적인 협업을 위해 ZooKeeper가 관리하는 여러 브로커로 구성됩니다.\n- 네트워크 통신: 클라이언트와 서버 간의 데이터 교환에 TCP를 활용합니다.\n- 클라이언트 측: CLI, 자바, 스칼라, REST API 및 타사 옵션을 포함한 다양한 클라이언트를 제공합니다.\n\n카프카의 인기 이유는?\n\n- 확장성: 데이터를 여러 브로커에 분산하여 확장성과 고 처리량을 보장합니다.\n- 높은 신뢰성: 안정성을 위해 여러 파티션과 복제를 사용합니다.\n- 영구적인 지속성: 이벤트를 영구적으로 저장하여 소비자의 편의에 맞게 사용할 수 있습니다.\n- 오픈 소스: 특정 요구 사항에 맞춰 사용자 정의가 가능하여 무료로 제공됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 카프카 아키텍처\n\n카프카 클러스터는 여러 브로커로 구성되어 있으며, 각 브로커는 이벤트를 수신, 저장, 처리 및 배포하는 역할을 합니다. ZooKeeper에 의해 조율되는 이러한 브로커들은 로그 또는 트랜잭션과 같은 특정 이벤트 유형을 저장하는 데이터베이스와 유사한 주제를 관리합니다.\n\n파티셔닝과 복제: 카프카는 장애 허용성과 병렬 이벤트 처리를 위해 파티셔닝과 복제를 사용합니다. 일부 브로커가 실패하더라도, 카프카는 주제 파티션을 운영 중인 브로커에 분산시킴으로써 지속성을 보장합니다.\n\nKafka CLI를 사용한 주제 관리: 카프카 명령줄 인터페이스는 카프카 클러스터 내에서 주제를 생성, 나열, 설명 및 삭제하는 기능을 제공합니다. 명령에는 정의된 파티션 및 복제로 주제를 생성하고 주제 및 구성에 대한 자세한 정보를 얻는 등의 작업이 포함됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_6.png)\n\n카프카 프로듀서\n\n카프카 프로듀서는 이벤트를 토픽 파티션에 발행하는 클라이언트 앱입니다. 이벤트는 선택적 파티셔닝을 위해 키와 연결될 수 있습니다. 프로듀서 CLI를 사용하면 프로듀서를 관리하고 지정된 토픽에 이벤트를 키와 함께 발행할 수 있습니다.\n\n컨슈머로 이벤트 읽기: 컨슈머는 토픽에 가입하고 저장된 이벤트를 읽어 순차적으로 오프셋을 유지합니다. 오프셋을 재설정함으로써 컨슈머는 처음부터 이벤트를 다시 재생할 수 있습니다. 카프카 컨슈머와 프로듀서는 독립적으로 작동하여 동기화 없이 이벤트를 저장하고 소비할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n끝까지 이어지는 이벤트: 날씨 파이프라인\n\n단계 1: 이벤트 소스 정의\n\n극단적인 날씨에 대한 대중의 반응을 이해하기 위해 날씨와 트위터 이벤트 스트림을 분석하고 싶다고 상상해보세요. 두 가지 주요 이벤트 소스를 활용할 것입니다:\n\n- IBM Weather API: JSON 형식의 실시간 및 예보 날씨 데이터를 제공합니다.\n- Twitter API: JSON 형식의 실시간 트윗 및 언급을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계 2: Kafka 토픽 구성\n\nKafka 클러스터에서 날씨 및 트위터 이벤트용 전용 토픽을 생성하여 데이터 흐름을 효율적으로 처리하기 위해 적절한 파티션 및 복제를 보장하세요.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_7.png)\n\n단계 3: 프로듀서 개발\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 이벤트 소스에 대해 특정한 프로듀서를 개발하세요. 이들은 JSON 데이터를 바이트로 직렬화하고 해당 Kafka 토픽으로 게시할 것입니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_8.png)\n\n단계 4: 컨슈머 구현\n\n날씨 및 Twitter 이벤트용 전용 컨슈머를 생성하세요. 이 컨슈머들은 Kafka 토픽에서 바이트를 역직렬화하여 JSON 데이터로 변환한 후 처리할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_9.png\" /\u003e\n\n단계 5: Persistence를 위한 DB Writer 통합\n\n이벤트 데이터를 관계형 데이터베이스에 쓰고 싶다면 DB writer를 사용하십시오. 이 구성 요소는 컨슈머에서 JSON 파일을 구문 분석하고 해당 데이터베이스 레코드를 생성합니다.\n\n```js\n#db writer EXAMPLE\nimport json\nimport sqlite3\n\ndef write_to_database(record):\n    connection = sqlite3.connect(\"event_database.db\")\n    cursor = connection.cursor()\n    cursor.execute(\"INSERT INTO events VALUES (?)\", (json.dumps(record),))\n    connection.commit()\n    connection.close()\n\nrecord = {\"event_type\": \"weather\", \"data\": {\"temperature\": 25, \"location\": \"NYC\"}}\nwrite_to_database(record)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n6단계: SQL을 사용한 데이터베이스 상호작용\n\n데이터베이스에 레코드를 쓰기 위해 SQL 삽입문을 사용하세요. 이 단계는 카프카 토픽에서 데이터를 영구 저장소 솔루션으로 전환하는 과정을 완료합니다.\n\n```js\n-- SQL 삽입 예시\nINSERT INTO events VALUES ('{\"event_type\": \"weather\", \"data\": {\"temperature\": 25, \"location\": \"NYC\"}');\n```\n\n7단계: 시각화 및 분석\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내, 수집하고 저장된 이벤트 데이터로부터 통찰력 있는 시각화와 분석을 위해 데이터베이스 레코드를 쿼리하세요. 수집된 이벤트 데이터로부터 가치 있는 통찰력을 얻기 위해 대시보드를 사용하는 것이 가장 좋습니다.\n\n이 end-to-end 파이프라인은 다양한 구성 요소의 원활한 통합을 보여주며, 이벤트 스트림을 관리하는 Kafka의 유연성과 강력함을 강조합니다.\n\n참고: 본 블로그 게시물에서 제공된 메모 및 정보는 \"ETL 및 쉘, Airflow 및 Kafka를 사용한 데이터 파이프라인\" 과정 중에 편집되었으며 개인적인 용도를 위한 기본 개요를 제공하기 위한 것입니다.","ogImage":{"url":"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png"},"coverImage":"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png","tag":["Tech"],"readingTime":10},{"title":"DBT  Airflow  ","description":"","date":"2024-06-19 09:45","slug":"2024-06-19-dbtAirflow","content":"\n\n![image](/assets/img/2024-06-19-dbtAirflow_0.png)\n\n요즘의 동적이고 경쟁적인 환경에서 기업은 데이터 기반 의사결정에 크게 의존하고 있습니다. 이를 실현하기 위해 조직은 믿을 수 있는 견고한 데이터 플랫폼과 고품질 데이터가 필요합니다. 이는 데이터를 효과적으로 수집하고 저장하는 시스템뿐만 아니라 데이터의 정확성과 신뢰성을 보장하는 것을 의미합니다.\n\n수백 개 또는 수천 개의 데이터 모델을 관리하는 것은 항상 간단한 과정은 아닙니다. 데이터 팀은 종종 데이터 자산을 효과적으로 구축, 테스트 및 유지하는 데 고민합니다. 플럼에서는 데이터 빌드 도구(dbt)를 활용하여 데이터 모델을 효과적으로 관리할 수 있는 CLI 도구에 의존합니다.\n\n그러나 dbt는 명령줄 인터페이스의 특성 때문에 도전적인 요소를 가지고 있습니다. 그렇다면, 제품 환경에 배포할 때 dbt의 장점을 팀이 어떻게 활용할 수 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ndbt Cloud가 당연한 선택처럼 보일 수 있지만, 모든 팀 또는 회사가 이 제품에 투자하기에 준비되어 있는 것은 아닙니다. 이러한 상황에서는 dbt 모델의 실행을 조율하고 그 사이의 종속성을 유지하는 대체 방법이 필요합니다.\n\n# 왜 자체 통합을 구축하기로 투자했는가\n\ndbt Cloud에 필요한 투자 외에도, 우리 팀은 특정 기능을 지원받길 원했습니다. 그러나 당시에는 플랫폼에서 이러한 기능 중 일부만 지원되었습니다. 아래에서 가장 기본적인 것들을 개요하겠습니다.\n\n![2024-06-19-dbtAirflow_1.png](/assets/img/2024-06-19-dbtAirflow_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- dbt 프로젝트 일정 설정: 우리는 dbt 프로젝트를 cron 작업 스타일로 일정을 잡고, 특정 시간 간격을 지정합니다.\n- 작업 세분화: 각 dbt 엔티티(모델, 테스트, 시드, 스냅샷을 포함)는 개별적으로 처리되어 필요한 경우에 개별적으로 트리거될 수 있도록 합니다.\n- dbt 의존성 유지: 또한, dbt 엔티티 간의 종속성을 유지해야 합니다. 예를 들어, 모델 A가 테스트를 포함하여 다른 모델 B의 상위 의존성으로 작용하는 경우 실행 순서를 유지해야 합니다.\n\n```js\n[dbt run A] -\u003e [dbt test A] -\u003e [dbt run B]\n```\n\n4. 다른 워크플로우 트리거: 특정 dbt 엔티티의 성공적인 완료 후에 특정 워크플로우를 시작해야 하며, 전체 dbt 프로젝트가 완료될 필요가 없는 경우가 있습니다. 또한, 두 dbt 엔티티 실행 사이에 특정 워크플로우를 시작해야할 경우가 있을 수 있습니다.\n\n```js\n[dbt run A] -\u003e [dbt와 무관한 워크플로우 실행] -\u003e [dbt run B]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5. 알림: 문제가 발생했을 때, 슬랙으로 알림을 받아야 합니다.\n\n6. 컨테이너화된 dbt 프로젝트 실행: 각각의 dbt 프로젝트는 자체 Docker 이미지를 가져야 합니다. 필요시 다른 dbt 버전에서 실행할 수 있는 유연성을 제공합니다.\n\n7. 모델 하위 집합 트리거링: dbt 엔티티는 dbt 태그를 기반으로 필터링될 수 있어서 필요한 특정 모델 그룹을 실행할 수 있습니다.\n\n8. 여러 일정 생성: 동일한 프로젝트를 서로 다른 일정으로 실행해야 할 수도 있습니다. 이는 모델 하위 집합을 트리거할 필요와 밀접하게 관려이 있습니다. 예를 들어, 모델을 시간별, 일별, 주간별로 태그 지정하여 해당 일정에 맞춰 실행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다양한 대안을 탐색해 본 결과, 천문학자가 개발한 Cosmos 패키지를 포함해 어떤 것도 당시의 우리 사용 사례와 명확한 요구 사항에 적합하다는 것을 입증하지 못했습니다.\n\n이러한 도전에 직면하자, 우리는 우리의 요구에 맞게 제작된 자체 Python 패키지를 개발하기로 전략적인 결정을 내렸습니다. 이 맞춤형 솔루션은 우리에게 dbt와 Airflow를 원활하게 통합할 수 있는 유연성과 기능이 제공되어, 데이터 팀이 효과적으로 데이터 모델을 관리하고 최적화할 수 있게 해줍니다.\n\n# 왜 Airflow를 선택했나요?\n\n데이터 파이프라인 Orchestration 전략을 설계할 때, 우리는 특정 간격(시간별, 일별 또는 주별)으로 작업을 예약하고 관리할 수 있는 능력을 지향했습니다. 팀이 Airflow 및 Google Cloud의 Cloud Composer 서비스에 익숙하고 기존 인프라가 있었기 때문에 Airflow를 선택하는 것이 우리의 요구에 적합한 자연스러운 선택이었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한 Airflow와 dbt는 Directed Acyclic Graphs (DAGs) 개념을 중심으로 하고 있어, dbt DAGs를 Airflow DAGs로 변환하여 Orchestration(오케스트레이션)할 수 있습니다.\n\nDAGs는 노드가 닫힌 순환 루프를 형성하지 않고 방향성을 가지고 연결된 작업 또는 데이터 모델을 나타내는 그래프입니다. dbt에서 DAGs는 데이터 모델 간의 관계와 종속성을 나타내고, Airflow에서는 DAGs는 데이터 파이프라인에 포함된 단계와 종속성을 시각화합니다.\n\n# 0부터 1까지: Cloud Composer에서 dbt 실행하기\n\n이 초기 단계에서 우리의 작업은 상대적으로 간단했습니다: Airflow를 사용하여 dbt 프로젝트를 실행하고 테스트하되, 각 모델, 테스트, 스냅샷 또는 시드를 개별 작업으로 설정할 필요가 없었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n필요한 것은 GitHub Actions를 사용하여 CI/CD 파이프라인을 설정하는 것이었습니다. 이 파이프라인은 관련된 dbt 프로젝트를 Cloud Composer 버킷으로 복사하고 Airflow 내에서 DAG를 구성합니다. 이 DAG에는 모델을 실행하는 하나의 작업 및 해당 작업을 테스트하는 다른 작업이 포함됩니다.\n\n![2024-06-19-dbtAirflow_2](/assets/img/2024-06-19-dbtAirflow_2.png)\n\n이것은 Airflow에서 dbt 프로젝트를 실행하는 가장 간단한 방법일 수 있습니다. DAG는 BashOperator로 생성된 두 개의 작업으로 구성됩니다. 기본적으로, 이는 로컬에서 dbt run 및 dbt test 명령을 실행하는 dbt CLI를 사용한 프로세스를 반영합니다.\n\n그러나 이 방법에는 여러 가지 제약이 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 만약에 dbt_run 작업 내에서 모델 실행이 실패한다면, 문제를 해결하기 위해 작업을 다시 실행해야 합니다. 이는 이전에 성공했던 모델들을 포함하여 모든 모델을 다시 실행해야 하므로 실행 시간과 비용이 증가하는 것을 의미합니다.\n- 모델 실행과 테스트는 완전히 독립적입니다. 모든 테스트는 워크플로우의 끝에 실행됩니다. 따라서 다른 모델들의 상위 종속성인 모델들에서 문제가 발생할 경우 너무 늦기 전까지 알아차리지 못할 수 있습니다.\n- 만약에 dbt_test 작업에서 모델 테스트가 실패한다면, 이 문제를 해결하기 위해 영향을 받는 모든 모델을 수정하기 위해 dbt_run과 dbt_test 작업을 검토하고 재실행해야 합니다.\n- Airflow 작업은 BashOperator를 사용하여 생성되며, Airflow 환경에 dbt 패키지 의존성이 설치되어 있어야 합니다. 이는 패키지 버전 호환성 문제로 인해 문제가 될 수 있으며, 특히 GCP의 Cloud Composer나 AWS의 MWAA와 같은 관리형 Airflow 서비스에서 더욱 그렇습니다.\n\n이 초기 시도는 Proof-of-Concept(PoC)로서의 느낌이 더 큰 노력입니다. 제한사항과 확장성 부족에도 불구하고, 팀의 기대에 부합하는 해결책을 개발할 자신감을 제공했습니다.\n\n우리의 다음 과제는 dbt 프로젝트를 Airflow DAG로 변환하여 각 dbt 엔티티가 해당하는 Airflow 작업으로 매핑되고 모든 종속성이 유지되도록 하는 것이었습니다.\n\n# dbt-airflow 빌딩\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ndbt DAG를 Airflow DAG로 변환하기 위해 첫 번째 작업은 dbt 엔터티에서 의존성을 추출하는 것이었습니다. 이는 데이터 모델 간의 관계를 구문 분석하고 DAG 내의 Airflow 작업에 매핑하는 것을 포함했습니다.\n\ndbt 의존성을 Airflow 작업으로 표현함으로써 데이터 워크플로를 원활하게 조정할 수 있었습니다. 작업(예: 모델 실행 또는 테스트)이 실패하면 분석가와 분석 엔지니어는 DAG의 나머지 부분을 해제하기 위해 기초 모델 문제에 대처해야 했습니다.\n\n이 접근 방식은 처음에는 방해적으로 보일 수 있지만, 물론 목표는 신뢰할 수 없는 데이터로 모델을 구축하는 것이 아닙니다. 그러나 초기 오류 감지는 우리 설계의 중요한 측면이었습니다. 이렇게 함으로써 비용을 최소화하고 계산 속도를 절약하며 데이터에 대한 신뢰를 유지할 수 있었습니다.\n\n## 반복 1: manifest.json 파일 활용\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n첫 번째 반복에서는 dbt가 생성한 manifest.json 파일을 읽는 파서를 개발했습니다. 이 메타데이터 파일은 dbt가 실행, 테스트 및 컴파일 중에 생성됩니다.\n\n이 파일의 내용을 사용하여 모든 dbt 엔티티와 해당 의존성을 추출할 수 있었습니다. 이 이정표는 dbt-airflow shaping의 탄생을 알리는 중요한 순간이었습니다.\n\n첫 릴리스는 manifest.json 파일에서 프로젝트를 읽고 모델 의존성을 추출하여 Airflow DAG 내의 TaskGroup으로 변환하는 능력을 갖추었습니다.\n\n이제 특정 작업이 실패할 때와 같이, 하류 의존성은 누군가 기본 문제를 해결할 때까지 일시 중지되어 DAG의 나머지 부분이 차단 해제됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 방법을 통해 문제가 발생했을 때 전체 모델 또는 테스트 세트를 다시 실행할 필요가 없어졌어요. 결과적으로, 이 전략은 Airflow에서 실행되는 dbt 프로젝트의 실행 및 유지에 관련된 비용을 크게 줄였어요.\n\n초기 Airflow DAG는 다음과 같았어요;\n\n```js\nimport functools\nfrom datetime import datetime \nfrom datetime import timedelta\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import PythonOperator\nfrom dbt_airflow.core.config import DbtAirflowConfig\nfrom dbt_airflow.core.config import DbtProfileConfig\nfrom dbt_airflow.core.config import DbtProjectConfig\nfrom dbt_airflow.core.task_group import DbtTaskGroup\nfrom dbt_airflow.operators.execution import ExecutionOperator\n\n\nwith DAG(\n    dag_id='test_dag',\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2),\n        },\n    on_failure_callback=functools.partial(\n        our_callback_function_to_send_slack_alerts\n    ),\n) as dag:\n\n    t1 = EmptyOperator(task_id='extract')\n    t2 = EmptyOperator(task_id='load')\n\n    tg = DbtTaskGroup(\n        group_id='transform',\n        dbt_project_config=DbtProjectConfig(\n            project_path=Path('/path/to/example_dbt_project/'),\n            manifest_path=Path('/path/to/example_dbt_project/target/manifest.json'),\n        ),\n        dbt_profile_config=DbtProfileConfig(\n            profiles_path=Path('/path/to/example_dbt_project/profiles'),\n            target='dev',\n        ),\n        dbt_airflow_config=DbtAirflowConfig(\n            execution_operator=ExecutionOperator.BASH,\n        ),\n    )\n\n    t1 \u003e\u003e t2 \u003e\u003e tg\n```\n\n프로젝트를 Cloud Composer에 배포하는 관점에서 이번 반복에서는 아무런 변경 사항이 없었어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Screenshot 1](/assets/img/2024-06-19-dbtAirflow_3.png)\n\nBy the end of this first iteration, we were able to meet half of the requirements we specified during ideation:\n\n![Screenshot 2](/assets/img/2024-06-19-dbtAirflow_4.png)\n\n## Iteration 2: Introducing Extra Tasks\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이번 반복에서는 초기 설계의 핵심 측면으로 초점을 옮겼습니다: 기존 모든 모델을 Airflow로 원활하게 마이그레이션할 수 있는 능력입니다.\n\nMachine Learning 워크플로우를 모델 실행 사이에 통합하여 하향 모델이 필요로 하는 데이터를 생성할 수 있도록 목표를 설정했습니다. 이 접근은 결과적으로 강하게 결합된 시스템을 만들어 내어 최선의 방법에 부합하지 않을 수 있지만, 이는 당시 실행 흐름을 대표했습니다. dbt-airflow가 이 흐름을 재현할 수 있도록 보장하는 것이 우선이었고, 이러한 구성 요소를 분리하는 구조 변경을 고려하기 전에 기술적 부채에 대한 대응은 그때의 주요 초점이 아니었습니다.\n\n그러나 이것은 다른 작업을 가속화할 수 있는 가치 있는 기능이었습니다. 예를 들어, 특정 dbt 모델에서 데이터에 의존하는 워크플로우는 해당 모델이 완료된 직후에 즉시 트리거될 수 있었고, 전체 DAG가 완료될 때까지 기다릴 필요가 없었습니다.\n\n기술적으로는 관심있는 dbt 프로젝트를 구문 분석한 후 생성된 Airflow 작업 이후에 Airflow Operator를 도입하려고 했습니다. 이를 달성하기 위해 Extra Task의 상위 또는 하위 작업을 명시해야 했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n'Extra Tasks'를 표현하기 위해 우리는 ExtraTask라는 객체를 개발했습니다. 이 객체는 렌더링된 dbt 프로젝트 내에서 이러한 작업을 소개하는 데 활용될 수 있습니다.\n\n```js\nfrom airflow.operators.python import PythonOperator\nfrom dbt_airflow.core.task import ExtraTask\n\n\nExtraTask(\n    task_id='test_task',\n    operator=PythonOperator,\n    operator_args={\n        'python_callable': lambda: print('Hello world'),\n    },\n    upstream_task_ids={\n        'model.example_dbt_project.int_customers_per_store',\n        'model.example_dbt_project.int_revenue_by_date',\n    },\n)\n```\n\n다음은 dbt-airflow의 Extra Task 기능을 활용하여 유명한 Sakila 프로젝트를 사용하여 생성된 더미 dbt 프로젝트를 렌더링하고 실행하는 예시 DAG입니다.\n\n```js\nimport functools\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import PythonOperator\nfrom dbt_airflow.core.config import DbtAirflowConfig\nfrom dbt_airflow.core.config import DbtProfileConfig\nfrom dbt_airflow.core.config import DbtProjectConfig\nfrom dbt_airflow.core.task import ExtraTask\nfrom dbt_airflow.core.task_group import DbtTaskGroup\nfrom dbt_airflow.operators.execution import ExecutionOperator\n\n\nwith DAG(\n    dag_id='test_dag',\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2),\n    },\n    'on_failure_callback': functools.partial(\n        our_callback_function_to_send_slack_alerts\n    ),\n) as dag:\n\n    t1 = EmptyOperator(task_id='extract')\n    t2 = EmptyOperator(task_id='load')\n\n    extra_tasks = [\n        ExtraTask(\n            task_id='test_task',\n            operator=PythonOperator,\n            operator_args={\n                'python_callable': lambda: print('Hello world'),\n            },\n            upstream_task_ids={\n                'model.example_dbt_project.int_customers_per_store',\n                'model.example_dbt_project.int_revenue_by_date',\n            },\n        ),\n        ExtraTask(\n            task_id='another_test_task',\n            operator=PythonOperator,\n            operator_args={\n                'python_callable': lambda: print('Hello world 2!'),\n            },\n            upstream_task_ids={\n                'test.example_dbt_project.int_customers_per_store',\n            },\n            downstream_task_ids={\n                'snapshot.example_dbt_project.int_customers_per_store_snapshot',\n            },\n        ),\n        ExtraTask(\n            task_id='test_task_3',\n            operator=PythonOperator,\n            operator_args={\n                'python_callable': lambda: print('Hello world 3!'),\n            },\n            downstream_task_ids={\n                'snapshot.example_dbt_project.int_customers_per_store_snapshot',\n            },\n            upstream_task_ids={\n                'model.example_dbt_project.int_revenue_by_date',\n            },\n        )\n    ]\n\n    tg = DbtTaskGroup(\n        group_id='my-dbt-project',\n        dbt_project_config=DbtProjectConfig(\n            project_path=Path('/path/to/example_dbt_project/'),\n            manifest_path=Path('/path/to/example_dbt_project/target/manifest.json'),\n        ),\n        dbt_profile_config=DbtProfileConfig(\n            profiles_path=Path('/path/to/example_dbt_project/profiles'),\n            target='dev',\n        ),\n        dbt_airflow_config=DbtAirflowConfig(\n            extra_tasks=extra_tasks,\n            execution_operator=ExecutionOperator.BASH,\n        ),\n    )\n\n    t1 \u003e\u003e t2 \u003e\u003e tg\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 번째 반복을 마칠 때, 우리는 더욱 우리가 목표로 한 최종 제품에 한 발짝 더 가까워졌어요.\n\n![이미지](/assets/img/2024-06-19-dbtAirflow_5.png)\n\n## 반복 3: 필터링 태그 및 다중 스케줄 생성\n\n이 반복에서, 우리의 목표는 특정 DAG 내에서 특정 dbt 엔티티의 하위 집합만 렌더링할 수 있게 하는 기능을 구현하는 것이었어요. 이는 여러 목적을 달성하기 위한 것이었죠:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 동일한 모델에 대해 시간별, 일별 또는 주간별로 다른 일정을 생성하려면 해당 태그가 지정된 모델만 선택하여 선택하면 됩니다.\n- 특정 이유로 특정 태그가 지정된 모델을 특정 DAG에 포함하지 않으려면 필터링을 해야 합니다.\n- 다른 워크플로에서 특정 시점에 일부 모델을 새로 고침해야 할 때 일부 모델만 렌더링해야 할 수 있습니다.\n\n이를 달성하기 위해, DbtAirflowConfig 객체에 'include_tags'와 'exclude_tags' 두 가지 추가 인수를 도입했습니다. 두 인수 모두 렌더링된 프로젝트에서 Entity를 포함하거나 제외하는 데 사용되는 dbt 태그와 일치하는 문자열 목록을 수용합니다.\n\n이 기능 추가의 주요 도전 과제는 필터링된 노드가 올바른 종속성을 유지하는 것이어서 예상보다 복잡했습니다.\n\n이 반복에서 우리는 이 프로젝트를 시작할 때 목표로 했던 최종 버전에 한 걸음 가까워졌습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-dbtAirflow_6.png\" /\u003e\n\n## 이터레이션 4: 컨테이너화된 프로젝트 실행\n\n에어플로우에서 BashOperator를 통해 dbt 명령을 실행할 때는 Airflow 환경 내에서 결과가 자료화되는 대상 시스템을 기준으로 dbt-core 및 해당 dbt 어댑터를 설치하는 것이 중요합니다.\n\n그러나 이 접근 방식은 패키지의 호환성 문제 등의 위험과 어려움을 야기할 수 있습니다. 특히 제공 업체의 다른 클라우드 서비스와의 통합을 용이하게하기 위해 미리 정의된 종속성을 함께 제공하는 클라우드의 관리형 Airflow 서비스인 클라우드 컴포저와 같은 서비스를 사용할 때 이 문제가 발생할 가능성이 높아집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고, 여러 개의 dbt 프로젝트를 관리하는 것은 서로 다른 dbt 버전에서 실행해야 할 수도 있습니다. 안타까운 점은 Airflow 환경에 dbt를 직접 설치하면 기술적으로 이것이 불가능하다는 것입니다.\n\n이러한 도전에 대처하고 버전 관리의 유연성을 보장하기 위해 우리는 dbt 프로젝트를 컨테이너화하고 k8s에서 실행하기로 결정했습니다. 이 방법은 다양한 사용 사례를 지원할뿐만 아니라 필요에 따라 dbt 버전을 업그레이드 또는 다운그레이드할 수 있는 유연성을 제공합니다.\n\n![이미지](/assets/img/2024-06-19-dbtAirflow_7.png)\n\n새 CI/CD 파이프라인이 트리거될 때마다, dbt 프로젝트를 포함하는 Docker 이미지가 빌드되어 Google Cloud의 Artifact Registry로 푸시됩니다. 그러나 dbt-airflow는 여전히 DAG를 렌더링하기 위해 manifest.json 파일에 의존하므로 전체 dbt 프로젝트를 복사하는 대신 manifest 파일만 Cloud Composer GCS 버킷으로 복사해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ndbt-airflow에서는 라이브러리를 확장하여 KubernetesPodOperator를 사용하여 Airflow Tasks를 실행하는 지원을 추가했어요. 이 기능을 통해 DbtAirflowConfig 내에서 execution_operator=ExecutionOperator.KUBERNETES를 지정할 수 있게 되었어요. 이 설정을 사용하면 TaskGroup 내의 각 작업이 자체 Kubernetes pod에서 실행되며 지정된 컨테이너 및 추가 구성을 활용할 수 있어요.\n\n```python\nimport functools\nfrom datetime import datetime \nfrom datetime import timedelta\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import PythonOperator\nfrom dbt_airflow.core.config import DbtAirflowConfig\nfrom dbt_airflow.core.config import DbtProfileConfig\nfrom dbt_airflow.core.config import DbtProjectConfig\nfrom dbt_airflow.core.task_group import DbtTaskGroup\nfrom dbt_airflow.operators.execution import ExecutionOperator\n\n\nwith DAG(\n    dag_id='test_dag',\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2),\n    },\n    'on_failure_callback': functools.partial(\n        our_callback_function_to_send_slack_alerts\n    ),\n) as dag:\n\n    t1 = EmptyOperator(task_id='extract')\n    t2 = EmptyOperator(task_id='load')\n\n    tg = DbtTaskGroup(\n        group_id='transform',\n        dbt_airflow_config=DbtAirflowConfig(\n            create_sub_task_groups=True,\n            execution_operator=ExecutionOperator.KUBERNETES,\n            operator_kwargs={\n                'name': f'dbt-project-1-dev',\n                'namespace': 'composer-user-workloads',\n                'image': 'gcp-region-docker.pkg.dev/gcp-project-name/ar-repo/dbt-project-1:latest',\n                'kubernetes_conn_id': 'kubernetes_default',\n                'config_file': '/home/airflow/composer_kube_config',\n                'image_pull_policy': 'Always',\n            },\n        ),\n        dbt_project_config=DbtProjectConfig(\n            project_path=Path('/home/project-1/'),  # 도커 컨테이너 내의 경로\n            manifest_path=Path('/home/airflow/gcs/dags/dbt/project-1/target/manifest.json'),  # Cloud Composer GCS 버킷에 있는 경로\n        ),\n        dbt_profile_config=DbtProfileConfig(\n            profiles_path=Path('/home/project-1/profiles/'),  # 도커 컨테이너 내의 경로\n            target=dbt_profile_target,\n        ),\n    )\n\n    t1 \u003e\u003e t2 \u003e\u003e tg\n```\n\n그리고 이 기능 추가로 프로젝트 초반에 지정된 초기 요구 사항 집필을 성공적으로 구현했어요.\n\n\u003cimg src=\"/assets/img/2024-06-19-dbtAirflow_8.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# dbt-airflow을 시작해 보세요\n\ndbt 프로젝트를 배포하고 일정을 예약하는 방법을 찾고 있다면, dbt-airflow를 사용하면 귀찮음을 덜 수 있습니다. Plum의 데이터팀은 이미 1년 이상 운영 중인 이 도구를 안정적이고 확장 가능하다고 입증했습니다. 그래서 여러분의 프로젝트에도 실제 가치를 제공할 수 있다고 자신합니다. 이 패키지는 플랫폼에 구애받지 않으며 dbt에서 지원하는 모든 대상과 함께 사용할 수 있습니다.\n\n이 프로젝트는 GitHub에서 유지되고 PyPI에서도 사용할 수 있습니다.\n\n또한 공식 문서에서 더 많은 세부 정보를 찾을 수 있습니다. 사실, 이 문서에서는 패키지가 제공하는 전체 기능 중 일부만 다루었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n프로젝트에 기여하는 것을 장려합니다. 무언가 부족한 부분이 있으면 참여해 주시기를 바랍니다.\n\n코딩 즐기세요,\n\nPlum Data Engineering Team ❤","ogImage":{"url":"/assets/img/2024-06-19-dbtAirflow_0.png"},"coverImage":"/assets/img/2024-06-19-dbtAirflow_0.png","tag":["Tech"],"readingTime":16},{"title":"데이터 엔지니어를 위한 자료 구조와 알고리즘 면접 질문","description":"","date":"2024-06-19 09:44","slug":"2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer","content":"\n\n데이터 엔지니어링 면접 준비는 데이터 구조와 알고리즘(DSA)에 대한 튼튼한 이해력이 필요합니다. 여러분이 잘 준비될 수 있도록, 자주 물어지는 DSA 면접 질문들을 정리한 목록을 제공해 드립니다. 연습 문제 링크가 포함되어 있어 다양한 주제를 다루며 다음 면접에 잘 준비될 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer_0.png)\n\n## 가장 자주 물어지는 DSA 문제 목록\n\n- 두 수의 합: [문제 링크](https://leetcode.com/problems/two-sum/description/)\n- 중복되는 문자가 없는 가장 긴 부분 문자열: [문제 링크](https://leetcode.com/problems/longest-substring-without-repeating-characters/)\n- 합이 k인 부분 배열: [문제 링크](https://leetcode.com/problems/subarray-sum-equals-k/description/)\n- 중복 숫자 찾기: [문제 링크](https://leetcode.com/problems/find-the-duplicate-number/description/)\n- 배열에서 Leaders: [문제 링크](https://www.geeksforgeeks.org/problems/leaders-in-an-array-1587115620/1)\n- 첫 번째 문자열에서 두 번째 문자열에 있는 문자 제거: [문제 링크](https://www.geeksforgeeks.org/remove-characters-from-the-first-string-which-are-present-in-the-second-string/)\n- 두 수 더하기: [문제 링크](https://leetcode.com/problems/add-two-numbers/description/)\n- 0과 1 문제: [문제 링크](https://leetcode.com/problems/ones-and-zeroes/description/)\n- 주식 매수와 매도의 최적 시기 II: [문제 링크](https://leetcode.com/problems/best-time-to-buy-and-sell-stock-ii/description/)\n- 최대 연속 1 개: [문제 링크](https://leetcode.com/problems/max-consecutive-ones/description/)\n- 끝에서 N번째 노드 제거: [문제 링크](https://leetcode.com/problems/remove-nth-node-from-end-of-list/description/)\n- 무작위 포인터가 있는 목록 복사: [문제 링크](https://leetcode.com/problems/copy-list-with-random-pointer/)\n- 문자열 뒤집기: [문제 링크](https://leetcode.com/problems/reverse-string/description/)\n- 첫 번째 누락된 양수: [문제 링크](https://leetcode.com/problems/first-missing-positive/description/)\n- 연속적인 1의 최대 개수 III: [문제 링크](https://leetcode.com/problems/max-consecutive-ones-iii/description/)\n- 홀수 짝수 연결 목록: [문제 링크](https://leetcode.com/problems/odd-even-linked-list/)\n- 목록 분할: [문제 링크](https://leetcode.com/problems/partition-list/description/)\n- 연결된 목록 순환이 있습니까? [문제 링크](https://leetcode.com/problems/linked-list-cycle/description/)\n- 유효한 괄호: [문제 링크](https://leetcode.com/problems/valid-parentheses/description/)\n- 부분 문자열의 모든 발생 제거: [문제 링크](https://leetcode.com/problems/remove-all-occurrences-of-a-substring/description/)\n- 첫 번째 나타나는 아닌 반복 문자 찾기: [문제 링크](https://www.geeksforgeeks.org/given-a-string-find-its-first-non-repeating-character/)\n- 목록에서 양수와 음수 수 세기: [문제 링크](https://www.geeksforgeeks.org/python-program-to-count-positive-and-negative-numbers-in-a-list/)\n- 목록에서 두 번째로 큰 숫자 찾기: [문제 링크](https://www.geeksforgeeks.org/python-program-to-find-second-largest-number-in-a-list/)\n- 최대 유효 괄호: [문제 링크](https://leetcode.com/problems/longest-valid-parentheses/description/)\n- 목록 뒤집기: [문제 링크](https://www.geeksforgeeks.org/python-reversing-list/)\n- 가장 긴 공통 접두어: [문제 링크](https://leetcode.com/problems/longest-common-prefix/)\n- 최대 부분 배열: [문제 링크](https://leetcode.com/problems/maximum-subarray/description/)\n- 평균 최대 부분 배열 I: [문제 링크](https://leetcode.com/problems/maximum-average-subarray-i/description/)\n- 배열의 곱: [문제 링크](https://leetcode.com/problems/product-of-array-except-self/description/)\n- 마지막 단어의 길이: [문제 링크](https://leetcode.com/problems/length-of-last-word/description/)\n- 문자열 뒤집기: [문제 링크](https://leetcode.com/problems/reverse-string/description/)\n- 목록 정렬: [문제 링크](https://leetcode.com/problems/sort-list/)\n- 최소 부분 배열 합: [문제 링크](https://leetcode.com/problems/minimum-size-subarray-sum/description/)\n- 최소 창 하위 문자열: [문제 링크](https://leetcode.com/problems/minimum-window-substring/description/)\n- 배열 K회 회전하기: [문제 링크](https://leetcode.com/problems/rotate-array/description/)\n- 과반 요소: [문제 링크](https://leetcode.com/problems/majority-element/description/)\n- 가장 긴 팰린드롬 부분 문자열: [문제 링크](https://leetcode.com/problems/longest-palindromic-substring/)\n- 엑셀 시트 열 제목: [문제 링크](https://leetcode.com/problems/excel-sheet-column-title/description/)\n- 애너그램 확인: [문제 링크](https://leetcode.com/problems/valid-anagram/description/)\n- 주식 매수 및 매도 최적 시기 III: [문제 링크](https://leetcode.com/problems/best-time-to-buy-and-sell-stock-iii/description/)\n- 오른쪽에서 가장 큰 요소로 요소 바꾸기: [문제 링크](https://leetcode.com/problems/replace-elements-with-greatest-element-on-right-side/description/)\n- 문장에서 가장 흔한 단어: [문제 링크](https://leetcode.com/problems/most-common-word/description/)\n- 주어진 합계를 갖는 부분 배열 찾기: [문제 링크](https://www.geeksforgeeks.org/find-subarray-with-given-sum/)\n- 단어 분리: [문제 링크](https://leetcode.com/problems/word-break/description/)\n- 문자열에서 첫 번째 발생지의 인덱스 찾기: [문제 링크](https://leetcode.com/problems/find-the-index-of-the-first-occurrence-in-a-string/description/)\n- 최대 연속 1 개: [문제 링크](https://leetcode.com/problems/max-consecutive-ones/description/)\n- 연속 배열: [문제 링크](https://leetcode.com/problems/contiguous-array/description/)\n- 주식 매수 및 매도 최적 시기: [문제 링크](https://leetcode.com/problems/best-time-to-buy-and-sell-stock/description/)\n- 쌍으로 노드 교환: [문제 링크](https://leetcode.com/problems/swap-nodes-in-pairs/)\n- 두 수 II: [문제 링크](https://leetcode.com/problems/two-sum-ii-input-array-is-sorted/description/)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 문제들을 연습함으로써, 데이터 엔지니어링 인터뷰에서 가장 일반적인 자료 구조와 알고리즘 문제에 대비할 준비가 더 잘 될 것입니다. 행운을 빕니다!","ogImage":{"url":"/assets/img/2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer_0.png"},"coverImage":"/assets/img/2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer_0.png","tag":["Tech"],"readingTime":6}],"page":"80","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"80"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>