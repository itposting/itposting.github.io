<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/59" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/59" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-2d104a861d88ea21.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_buildManifest.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="SQL 윈도우 함수 데이터 열정가들을 위한 최고의 도구" href="/post/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="SQL 윈도우 함수 데이터 열정가들을 위한 최고의 도구" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="SQL 윈도우 함수 데이터 열정가들을 위한 최고의 도구" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">SQL 윈도우 함수 데이터 열정가들을 위한 최고의 도구</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="최고의 Kafka에서 Delta 적재를 위한 도구들을 벤치마킹합니다" href="/post/2024-06-19-NeedforSpeedBenchmarkingtheBestToolsforKafkatoDeltaIngestion"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="최고의 Kafka에서 Delta 적재를 위한 도구들을 벤치마킹합니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-NeedforSpeedBenchmarkingtheBestToolsforKafkatoDeltaIngestion_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="최고의 Kafka에서 Delta 적재를 위한 도구들을 벤치마킹합니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">최고의 Kafka에서 Delta 적재를 위한 도구들을 벤치마킹합니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="트위터가 매일 40억 건의 이벤트를 실시간으로 처리하는 방법" href="/post/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="트위터가 매일 40억 건의 이벤트를 실시간으로 처리하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="트위터가 매일 40억 건의 이벤트를 실시간으로 처리하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">트위터가 매일 40억 건의 이벤트를 실시간으로 처리하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="보안 데이터 플랫폼 구축 팀 구조 및 접근 전략에 대한 안내" href="/post/2024-06-19-BuildingSecureDataPlatformsAGuideforTeamsStructureAccessStrategies"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="보안 데이터 플랫폼 구축 팀 구조 및 접근 전략에 대한 안내" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-BuildingSecureDataPlatformsAGuideforTeamsStructureAccessStrategies_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="보안 데이터 플랫폼 구축 팀 구조 및 접근 전략에 대한 안내" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">보안 데이터 플랫폼 구축 팀 구조 및 접근 전략에 대한 안내</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="아이스버그 카탈로그 당신의 레이크하우스의 TIP" href="/post/2024-06-19-IcebergCatalogTheTIPofyourLakehouse"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="아이스버그 카탈로그 당신의 레이크하우스의 TIP" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-IcebergCatalogTheTIPofyourLakehouse_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="아이스버그 카탈로그 당신의 레이크하우스의 TIP" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">아이스버그 카탈로그 당신의 레이크하우스의 TIP</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="마라톤 결과의 연령 그레이딩을 위한 더 나은 시스템이 있을까요" href="/post/2024-06-19-IsThereaBetterSystemforAgeGradingMarathonResults"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="마라톤 결과의 연령 그레이딩을 위한 더 나은 시스템이 있을까요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-IsThereaBetterSystemforAgeGradingMarathonResults_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="마라톤 결과의 연령 그레이딩을 위한 더 나은 시스템이 있을까요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">마라톤 결과의 연령 그레이딩을 위한 더 나은 시스템이 있을까요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="더 나은 데이터 스토리텔링 탐구적 연구를 통해 시각물을 만드는 방법" href="/post/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="더 나은 데이터 스토리텔링 탐구적 연구를 통해 시각물을 만드는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="더 나은 데이터 스토리텔링 탐구적 연구를 통해 시각물을 만드는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">더 나은 데이터 스토리텔링 탐구적 연구를 통해 시각물을 만드는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터에서 시각화까지 OpenAI Assistants API 및 GPT-4o와 함께" href="/post/2024-06-19-FromDatatoVisualizationwiththeOpenAIAssistantsAPIandGPT-4o"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터에서 시각화까지 OpenAI Assistants API 및 GPT-4o와 함께" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-FromDatatoVisualizationwiththeOpenAIAssistantsAPIandGPT-4o_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터에서 시각화까지 OpenAI Assistants API 및 GPT-4o와 함께" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터에서 시각화까지 OpenAI Assistants API 및 GPT-4o와 함께</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="그래프 시각화 초보부터 고급까지 7단계" href="/post/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="그래프 시각화 초보부터 고급까지 7단계" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="그래프 시각화 초보부터 고급까지 7단계" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">그래프 시각화 초보부터 고급까지 7단계</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="MKBHD의 YouTube 성공을 데이터로 살펴보기" href="/post/2024-06-19-AData-DrivenLookatMKBHDsYouTubeSuccess"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="MKBHD의 YouTube 성공을 데이터로 살펴보기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-AData-DrivenLookatMKBHDsYouTubeSuccess_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="MKBHD의 YouTube 성공을 데이터로 살펴보기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">MKBHD의 YouTube 성공을 데이터로 살펴보기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/41">41</a><a class="link" href="/posts/42">42</a><a class="link" href="/posts/43">43</a><a class="link" href="/posts/44">44</a><a class="link" href="/posts/45">45</a><a class="link" href="/posts/46">46</a><a class="link" href="/posts/47">47</a><a class="link" href="/posts/48">48</a><a class="link" href="/posts/49">49</a><a class="link" href="/posts/50">50</a><a class="link" href="/posts/51">51</a><a class="link" href="/posts/52">52</a><a class="link" href="/posts/53">53</a><a class="link" href="/posts/54">54</a><a class="link" href="/posts/55">55</a><a class="link" href="/posts/56">56</a><a class="link" href="/posts/57">57</a><a class="link" href="/posts/58">58</a><a class="link posts_-active__YVJEi" href="/posts/59">59</a><a class="link" href="/posts/60">60</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"SQL 윈도우 함수 데이터 열정가들을 위한 최고의 도구","description":"","date":"2024-06-19 01:43","slug":"2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts","content":"\n\n\n![SQL Window Functions](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_0.png)\n\n요즘에는 엄청난 양의 데이터를 다루고 있습니다. 이 주요한 도전에 따라 다양한 소스의 복잡도도 함께 증가하고 있습니다. 이러한 환경에서 SQL은 여전히 영웅이며, 이 데이터 바다에서 가치 있는 통찰을 추출하고 탐색하는 데 꼭 필요한 도구입니다.\n\nSQL이 제공하는 많은 강력한 기능 중에서도 윈도우 함수는 특히 주목할 만한 요소입니다. 이러한 함수들은 테이블 행 집합을 대상으로 높명한 계산을 가능하게 하며, 고급 데이터 분석에 필수적이며 데이터와 상호작용하는 방법을 변화시키는 데 중요합니다.\n\n이 기사에서는 SQL의 윈도우 함수 개념을 해부하고 이해할 것입니다. 언제 윈도우 함수를 사용해야 하는지, 그리고 SQL 쿼리에서 효과적으로 구현하는 방법에 대해 살펴볼 것입니다. 이 가이드를 마치면 윈도우 함수의 강력함과 유연성에 대한 깊은 이해를 얻게 될 것이며, 데이터 분석 기술을 향상시키기 위한 실제 예제를 활용할 수 있을 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 윈도우 함수가 뭔가요?\n\n경험 수준에 상관 없이 모든 데이터 애호가는 윈도우 함수에 대해 들어봤거나 사용해 본 적이 있을 것입니다. 이 강력한 도구들은 모든 SQL 강좌에서 퍼져 있으며 데이터 작업을 하는 사람들의 일상생활에서 필수불가결합니다.\n\n구글에서 빠르게 검색을 해보죠…몇 분 후에 혹은 TV 광고를 보고 나서, 우리는 윈도우 함수가 다음과 같다는 사실을 알게 됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 문법에 관해서 뭔가 언급했다고 했나요?\n\n그렇습니다. 이 매우 강력한 도구에는 특정 구문과 같은 트릭이 함께 제공됩니다.\n\n![image](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_1.png)\n\n위 이미지에서 볼 수 있듯이, 윈도우 함수의 구문은 네 부분으로 나뉠 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 집계/함수: 여기서 집계 (예: AVG, SUM) 또는 LAG(), LEAD(), ROW_NUMBER(), RANK(), DENSE_RANK()과 같은 특정 창 함수를 배치하여 작업을 시작합니다. 몇 가지 더 있지만, 이 중에서는 가장 일반적으로 사용되는 것들이에요 (적어도 저는 이것들을 가장 많이 사용해요 😁)\n- OVER: 이 키워드는 윈도우 함수를 사용할 것임을 IDE에 \"알리는\" 데에 사용됩니다. 이는 \"여기서 무언가를 할 것이고, 무언가 복잡한 것에 대비해야 한다\"고 말하는 것과 같아요.\n- PARTITION BY: 이 절은 결과를 파티션 또는 창으로 나눕니다. 우리는 이 과정에서 초기에 설정한 집계나 함수를 적용할 것입니다. 이 부분을 작성한 후에는 파티션을 기준으로 필드를 개발해야 합니다. 순위 함수와 함께 사용되지 않아요.\n- ORDER BY: 경우에 따라 선택 사항일 수 있지만, 이것이 하는 일을 알아두는 것이 좋아요. 이는 각 파티션 내의 행을 정렬하는 데 사용되며, RANK(), DENSE_RANK(), ROW_NUMBER()와 같은 순위 함수를 사용할 때 유용합니다.\n\n# 목표를 달성하기 위한 다양한 창 함수\n\n이전 섹션에서 창 함수 구문에 대해 이야기했어요. 창 함수 구문과 독립적으로 작동하지 않는 몇 가지 함수를 언급했죠.\n\n일부는 각 파티션의 각 행에 대한 순위 값을 반환하기 때문에 순위 함수라고 불리며, 다른 것은 시계열 창 함수입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n순위 함수:\n\n- RANK() : 결과 집합의 파티션 내 각 행에 순위를 할당하며, 동일한 값을 가진 행은 동일한 순위를 받습니다.\n- DENSE_RANK() : RANK()와 유사하지만 연속적인 순위 값을 가집니다. 동일한 값은 동일한 순위를 받으며, 다음 순위 값은 다음 연속 정수입니다.\n- NTILE() : 결과 집합을 동일한 그룹으로 분할하고 각 행에 속하는 그룹을 나타내는 숫자를 할당합니다.\n- ROW_NUMBER() : 결과 집합의 파티션 내 각 행에 고유한 연속 정수를 할당하며, 각 파티션의 첫 번째 행부터 1로 시작합니다.\n\n시계열 함수:\n\n- LAG() : 결과 집합 내 이전 행의 값을 가져오는 함수로, 자체 조인이 필요하지 않습니다. 연속된 행 간의 차이를 계산하는 데 도움이 됩니다.\n- LEAD() : 다음 행의 값을 예측하는 데 유용한, 자체 조인 없이 다음 행의 값을 액세스할 수 있습니다. 추세나 값의 변화를 예측하는 데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 영원한 질문: 왜...\n\n많은 것들에 대해 우리가 하는 일반적인 질문들이 있습니다. SQL의 창 함수도 예왽이 아닙니다. 창 함수가 여러분에게 시간과 노력을 절약해줄 수 있는 상황을 이해하려면 다음을 살펴보겠습니다:\n\n왜 그리고 언제 우리는 창 함수를 사용해야 할까요?\n\n언제부터 시작해볼까요. 언제 우리는 창 함수를 사용할까요? 잘, 우리가 창 함수를 사용해야 하는 시점은 언제든지 우리가 필요로 할 때 입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 특정 조건에 따라 데이터 하위 집합에서 누적 합계, 순위, 평균 또는 다른 계산을 계산합니다.\n- 현재 및 이전/다음 행 값 비교\n\n왜 윈도우 함수를 사용해야 하는지 왜도 빼놓지 마세요. 상황에 필요할 때 윈도우 함수를 사용해야 하는 이유는 무엇인가요?\n \n윈도우 함수를 사용해야 하는 이유:\n\n- 행 레벨 세부 정보 유지 — 데이터를 축소하지 않고 계산을 수행할 수 있는데, 이는 원본 데이터를 유지한 채 여러 행을 대상으로 계산할 수 있도록 합니다.\n- 복잡한 쿼리 간소화 — 이 도구를 사용하면 가장 복잡한 쿼리를 간소화하여 읽기 좋고 작성하기 쉽고, 무엇보다도 유지보수하기 쉽게 만들어줍니다.\n- 성능 향상 — SQL 엔진에서 최적화되어 대량 데이터셋의 경우 더 나은 성능을 제공하는 경우가 많습니다.\n- 고급 분석 활성화 — 누적 합계, 이동 평균 및 기타 고급 분석 작업을 실행할 수 있도록 합니다.\n- 자세한 분석을 위한 데이터 파티션 — 특정 기준에 따라 데이터를 분할하여 전체 데이터 집계 없이 그룹 내에서 자세한 분석을 가능하게 합니다.\n- 시계열 및 변경 감지 지원 — 이전 또는 다음 행 값에 액세스하는 내장 지원을 제공하여 시계열 데이터 및 변경 감지에 유용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 실제 사용 사례\n\n은행 분야에서 데이터 엔지니어로 일하고 있는데, 계약의 \"단계\"가 변경된 레코드를 식별하고 이 변경 날짜를 기록해야 하는 요청을 받았어요.\n\n쉽게 말해, 그렇게 하는 게 쉽지 않을 것 같죠? 그렇게는 안 돼요. 윈도우 함수를 사용해서 요청을 완료하고 결과를 빠르게 전달하는 데 도움이 되었어요.\n\n우리가 두 개의 테이블이 있다고 가정해 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nsource.data_records\n\n![Image 2](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_2.png)\n\nand temp.data_records:\n\n![Image 3](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_3.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 다음 안에 테이블을 생성해야 합니다. 그 안에는 다음과 같은 정보가 포함되어 있어야 합니다:\n\n- 식별자\n- 식별자의 현재 레벨\n- 현재 단계의 참조 날짜\n- 식별자의 이전 레벨\n- 이전 참조 날짜\n- 식별자가 레벨을 변경한 날짜\n\n테이블은 아래 코드를 기반으로 생성되었습니다:\n\n```js\ncreate table tmp_change_level_date as\n(\nselect distinct * from ( \n    select \n        fct.identifier, fct.level, fct.date_ref,\n        lag(fct.level) over (partition by fct.identifier order by fct.date_ref) as previous_level,\n        lag(fct.date_ref) over (partition by fct.identifier order by fct.date_ref) as previous_date,\n        case\n            when lag(fct.level) over (partition by fct.identifier order by fct.date_ref) is not null then fct.date_ref\n            else NULL\n        end as change_level_date,\n        dense_rank() over (partition by fct.identifier order by fct.date_ref desc) as ranks\n    from source.data_records fct  join temp.data_records TFCT \n    on fct.identifier = TFCT.identifier\n    where TFCT.amount \u003c\u003e 0 and TFCT.account in (select account_code from accounts_list)\n    ) x\nwhere ranks = 1 \nand level \u003c\u003e previous_level\nand previous_date \u003c\u003e change_level_date\n)\ncommit;\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자, 이제 설명으로 들어가볼게요:\n\n- 우선적으로, loan identifier(대출 식별자), level, date_ref(대출의 실제 단계 및 현재 단계의 기준 날짜)와 같은 정보를 가져오는 주요 SELECT 문을 만들었습니다:\n\n```js\nselect \n        fct.identifier, fct.level, fct.date_ref,\n        lag(fct.level) over (partition by fct.identifier order by fct.date_ref) as previous_level,\n        lag(fct.date_ref) over (partition by fct.identifier order by fct.date_ref) as previous_date,\n        case\n            when lag(fct.level) over (partition by fct.identifier order by fct.date_ref) is not null then fct.date_ref\n            else NULL\n        end as change_level_date,\n        dense_rank() over (partition by fct.identifier order by fct.date_ref desc) as ranks\n    from source.data_records fct  join temp.data_records TFCT \n    on fct.identifier = TFCT.identifier\n    where TFCT.amount \u003c\u003e 0 and TFCT.account in (select account_code from accounts_list)\n    ) x\n```\n\n그 다음으로, 각 대출에 대해 이전 대출 단계와 이전 참조 날짜를 가져오기 위해 LAG() 함수를 사용했습니다. PARTITION BY를 사용하여 식별자에 따라 데이터셋을 작은 파티션으로 나누고, 각 파티션 내에서 레코드를 date_ref에 따라 정렬했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nlag(fct.level) over (partition by fct.identifier order by fct.date_ref) as previous_level,\nlag(fct.date_ref) over (partition by fct.identifier order by fct.date_ref) as previous_date\n\n\nand assign a rank to each record within the partition by using DENSE_RANK() function:\n\n\ndense_rank() over (partition by fct.identifier order by fct.date_ref desc) as ranks\n\n\nThis code will return the following result:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_4.png\" /\u003e\n\n더 나아가서, 이전 결과에서 일부 필터를 적용할 수 있도록 다음 SELECT문을 작성합니다 (위의 표에 해당하는):\n\n```js\nselect distinct * from (\n\n---- 이전 select를 하위 쿼리로 사용 ----\n\n) x\nwhere ranks = 1 \nand level \u003c\u003e previous_level\nand previous_date \u003c\u003e change_level_date\n```\n\n그리고 각 식별자에 대해 가장 최근 레코드만 가져와서 (ranks = 1은 설명에서 앞에서 언급한 가장 최근 레코드에 해당함), 현재 레벨이 이전 레벨과 다른 레코드만 가져오도록 필터를 적용하며 (level != previous_level), 변경 날짜가 유효하고 이전 참조 날짜와 다른지 확인합니다. 이러한 필터를 기반으로 결과를 새로운 테이블 tmp_change_level_date에 삽입합니다 (CREATE TABLE table_name AS와 유명한 구문을 사용하여 만든 것):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![SQL Window Functions](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_5.png)\n\n이 결과를 통해 다음을 알 수 있습니다:\n\n- 식별자 2의 경우: 레벨이 2023년 03월 15일에 A에서 C로 변경되었습니다.\n- 식별자 3의 경우: 레벨이 2023년 02월 20일에 B에서 A로 변경되었습니다.\n\n# 결론\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSQL 윈도우 함수는 복잡한 데이터 분석을 간편하게 하고 성능을 향상시킵니다. 이 글에서는 기본 사항, 구문, 랭킹 및 시계열 분석과 같은 일반적인 사용 사례, 실제 예제에 대해 다룹니다. 이러한 함수를 숙달하면 SQL 쿼리를 더 효율적이고 통찰력 있게 만들 수 있습니다.\n\n실습하고 실험하여 그 능력을 최대로 발휘하고 데이터 분석 능력을 향상시키세요.","ogImage":{"url":"/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_0.png"},"coverImage":"/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_0.png","tag":["Tech"],"readingTime":8},{"title":"최고의 Kafka에서 Delta 적재를 위한 도구들을 벤치마킹합니다","description":"","date":"2024-06-19 01:41","slug":"2024-06-19-NeedforSpeedBenchmarkingtheBestToolsforKafkatoDeltaIngestion","content":"\n\n# 소개\n\n데이터브릭스 플랫폼에서 카프카에서 델타 테이블로 데이터를 수집하는 시리즈의 두 번째 부분에 다시 오신 것을 환영합니다. 이전에 우리가 논의한 것을 더욱 발전시키면서, 카프카 토픽으로 합성 데이터를 생성하고 스트리밍하는 것에 대해 살펴보겠습니다.\n\n본 블로그 포스트에서는 델타 레이크로부터 아파치 카프카에서 스트리밍 데이터를 수집하기 위한 데이터브릭스 플랫폼에서 사용 가능한 세 가지 강력한 옵션을 벤치마킹합니다: 데이터브릭스 잡, 델타 라이브 테이블(DLT) 및 델타 라이브 테이블 서버리스(DLT 서버리스). 주요 목표는 이러한 접근 방식을 통해 카프카에서 델타 테이블로 데이터를 수집할 때의 엔드 투 엔드 지연 시간을 평가하고 비교하는 것입니다.\n\n지연 시간은 중요한 지표이며, 하류 분석 및 의사 결정 프로세스에 사용 가능한 데이터의 신선도와 적시성에 직접적인 영향을 미칩니다. 모든 세 가지 도구가 Apache Spark의 구조적 스트리밍을 내부적으로 활용한다는 점을 강조해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 벤치마크 설정\n\n## 벤치마킹 기준\n\n측정한 주요 메트릭은 대기 시간이었습니다 - 카프카에서 행이 생성된 시점부터 델타 레이크에서 이용 가능해질 때까지 걸리는 시간입니다. 대기 시간은 정밀하게 측정되었으며 정확성을 보장하고 변동성을 고려하기 위해 장기간에 걸쳐 신중하게 측정되었습니다.\n\n## 입력 카프카 피드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희가 수행한 벤치마크에서는 매 초 100개의 행을 생성하는 Kafka 피드를 활용했어요. 각 행은 대략 1MB로, 초당 100MB로 이루어져요. 연간으로 계산하면 약 3.15 페타바이트가 되어, 저희가 선택한 도구의 수신 능력을 평가하기 위한 엄격한 테스트 베드가 됐어요.\n\nConfluent Cloud를 사용하여 6개 파티션으로 Kafka 클러스터를 설정했는데, 5분 미만이 걸렸어요. 그리고 실험을 위해 300달러의 크레딧을 제공해 주었어요.\n\n![이미지](/assets/img/2024-06-19-NeedforSpeedBenchmarkingtheBestToolsforKafkatoDeltaIngestion_0.png)\n\n## 비교 도구\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Databricks 작업: Kafka에서 읽고 Delta Lake 테이블에 쓰기 위해 Apache Spark Structured Streaming을 활용합니다. 작업 구성 및 예약에 유연성을 제공하지만 클러스터 리소스의 수동 관리가 필요합니다.\n- Delta Live Tables (DLT): Kafka에서 Delta Lake로 데이터를 입력하기 위해 선언적 접근 방식을 사용하며, 인프라를 자동으로 관리하고 파이프라인 개발을 간소화합니다.\n- Delta Live Tables Serverless (DLT Serverless): DLT와 동일한 입력 작업을 수행하면서, 인프라 관리를 더 간소화하기 위한 서버리스 모델을 활용합니다. 자동 스케일링과 리소스 최적화를 제공합니다.\n\n## 지연 시간은 어떻게 측정되었나요?\n\n지연 시간은 테이블로의 연속적인 스트리밍 업데이트 타임스탬프 간의 밀리초 단위 시간 차이를 계산하여 측정됩니다. 이는 각 순차적 커밋에 대해 이전 업데이트의 타임스탬프를 현재 업데이트의 타임스탬프에서 뺌으로써 수행되며, 각 업데이트가 이전 업데이트에 비해 처리하는 데 얼마나 걸리는지 분석할 수 있습니다. 분석은 현재 300개의 커밋으로 제한되어 있지만 필요에 따라 조정할 수 있습니다.\n\n```js\nfrom pyspark.sql import DataFrame\n\ndef run_analysis_about_latency(table_name: str) -\u003e DataFrame:\n    # Python 다중 라인 문자열로 형식 지정된 SQL 명령어 텍스트\n    sql_code = f\"\"\"\n        -- 테이블의 업데이트 이력에 대한 가상 뷰 정의\n        WITH VW_TABLE_HISTORY AS (\n          -- 테이블의 역사적 변화 설명\n          DESCRIBE HISTORY {table_name}\n        ),\n        \n        -- 이전 쓰기 작업의 타임스탬프를 계산하는 뷰 정의\n        VW_TABLE_HISTORY_WITH_previous_WRITE_TIMESTAMP AS (\n          SELECT\n            -- 현재 작업 이전의 마지막 쓰기 작업의 타임스탬프를 계산\n            lag(timestamp) OVER (\n              PARTITION BY 1\n              ORDER BY version\n            ) AS previous_write_timestamp,\n            timestamp,\n            version\n          FROM\n            VW_TABLE_HISTORY\n          WHERE\n            operation = 'STREAMING UPDATE'\n        ),\n        \n        -- 연속 커밋 간의 밀착 정도를 분석하는 뷰 정의\n        VW_BOUND_ANALYSIS_TO_N_COMMITS AS (\n          SELECT\n            -- 이전 및 현재 쓰기 타임스탬프 간의 밀리초 단위 시간 차이 계산\n            TIMESTAMPDIFF(\n              MILLISECOND,\n              previous_write_timestamp,\n              timestamp\n            ) AS elapsed_time_ms\n          FROM\n            VW_TABLE_HISTORY_WITH_previous_WRITE_TIMESTAMP\n          ORDER BY\n            version DESC\n          LIMIT\n            300  -- 최근 300개 커밋만 분석\n        )\n        \n        -- 쓰기 지연 시간에 대한 다양한 통계 계산\n        SELECT\n          avg(elapsed_time_ms) AS average_write_latency,\n          percentile_approx(elapsed_time_ms, 0.9) AS p90_write_latency,\n          percentile_approx(elapsed_time_ms, 0.95) AS p95_write_latency,\n          percentile_approx(elapsed_time_ms, 0.99) AS p99_write_latency,\n          max(elapsed_time_ms) AS maximum_write_latency\n        FROM\n          VW_BOUND_ANALYSIS_TO_N_COMMITS\n    \"\"\"\n    # Spark의 SQL 모듈을 사용하여 SQL 쿼리 실행\n    display(spark.sql(sql_code))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터 수집\n\n이 코드는 Apache Spark를 사용하여 Kafka topic에서 데이터를 효율적으로 수집하는 스트리밍 데이터 파이프라인을 설정합니다. Kafka 메시지에서 예상되는 데이터 유형 및 열에 맞게 구성된 스키마를 정의하고, 차량 세부 정보, 지리적 좌표 및 텍스트 필드를 포함합니다. read_kafka_stream 함수는 스트리밍 프로세스를 초기화하고 Kafka에 안전하고 신뢰할 수 있는 연결을 구성하며, 지정된 주제를 구독하여 개선된 처리 속도를 위해 여러 파티션을 통해 데이터를 처리합니다. 스트림은 정의된 스키마에 따라 JSON 형식 메시지를 디코딩하고 필수 메타데이터를 추출합니다.\n\n```js\nfrom pyspark.sql.types import StructType, StringType, FloatType\nfrom pyspark.sql.functions import *\n\n# DataFrame 구조에 기반한 스키마 정의\nschema = StructType() \\\n    .add(\"event_id\", StringType()) \\\n    .add(\"vehicle_year_make_model\", StringType()) \\\n    .add(\"vehicle_year_make_model_cat\", StringType()) \\\n    .add(\"vehicle_make_model\", StringType()) \\\n    .add(\"vehicle_make\", StringType()) \\\n    .add(\"vehicle_model\", StringType()) \\\n    .add(\"vehicle_year\", StringType()) \\\n    .add(\"vehicle_category\", StringType()) \\\n    .add(\"vehicle_object\", StringType()) \\\n    .add(\"latitude\", StringType()) \\\n    .add(\"longitude\", StringType()) \\\n    .add(\"location_on_land\", StringType()) \\\n    .add(\"local_latlng\", StringType()) \\\n    .add(\"zipcode\", StringType()) \\\n    .add(\"large_text_col_1\", StringType()) \\\n    .add(\"large_text_col_2\", StringType()) \\\n    .add(\"large_text_col_3\", StringType()) \\\n    .add(\"large_text_col_4\", StringType()) \\\n    .add(\"large_text_col_5\", StringType()) \\\n    .add(\"large_text_col_6\", StringType()) \\\n    .add(\"large_text_col_7\", StringType()) \\\n    .add(\"large_text_col_8\", StringType()) \\\n    .add(\"large_text_col_9\", StringType())\n\ndef read_kafka_stream():\n    kafka_stream = (spark.readStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers_tls ) \n      .option(\"subscribe\", topic )\n      .option(\"failOnDataLoss\",\"false\")\n      .option(\"kafka.security.protocol\", \"SASL_SSL\")\n      .option(\"kafka.sasl.mechanism\", \"PLAIN\") \n      .option(\"kafka.sasl.jaas.config\", f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_api_key}\" password=\"{kafka_api_secret}\";')\n      .option(\"minPartitions\",12)\n      .load()\n      .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"), \"topic\", \"partition\", \"offset\", \"timestamp\", \"timestampType\" )\n      .select(\"topic\", \"partition\", \"offset\", \"timestamp\", \"timestampType\", \"data.*\")\n    )\n    return kafka_stream\n```\n\n## 설명:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Connection Setup: 특정 부트스트랩 서버를 사용하여 Kafka에 연결하고 SASL_SSL과 같은 보안 설정을 포함하여 암호화 및 인증된 데이터 전송을 합니다.\n- Topic Subscription: 지정된 Kafka 주제에 가입하여 계속해서 새로운 데이터를 수신합니다.\n- Stream Configuration: 잠재적인 데이터 손실을 처리하고 여러 파티션 간의 데이터를 처리 속도를 높이기 위해 견고하게 구성됩니다.\n- Data Transformation: 수신된 JSON 메시지를 설정된 스키마에 따라 디코딩하기 위해 from_json을 사용하며 Spark 내에서 구조화된 형식으로 변환합니다.\n- Metadata Extraction: Kafka 주제, 파티션 및 메시지 타임 스탬프와 같은 필수 메타데이터를 추출합니다.\n\n이 설정은 Kafka에서 Spark로의 데이터 흡수를 최적화하고 데이터를 추가적으로 처리하거나 Delta Lake와 같은 저장 시스템으로 통합하기 위한 준비를 합니다.\n\n## Databricks Jobs을 위한 추가 코드\n\n구성: 이 방법은 Databricks 작업 및 클러스터 리소스를 설정하는 것을 포함하며 유연한 스케줄링 및 흡수 프로세스 모니터링을 가능하게 합니다만, 올바른 컴퓨팅을 선택하는 것을 이해해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n(  \n  read_kafka_stream()\n  .writeStream\n  .option(\"checkpointLocation\",checkpoint_location_for_delta)\n  .trigger(processingTime='1 second')\n  .toTable(target)\n)\n```\n\n## Delta Live Tables에 대한 추가 코드\n\n구성: Delta Live Tables는 인프라를 자동으로 관리하여 데이터 파이프라인을 구성하는 간단하고 선언적인 방식을 제공합니다.\n\n이 코드 스니펫은 Delta Live Tables (DLT) API를 사용하여 Kafka에서 스트리밍 데이터를 수신하는 데이터 테이블을 정의합니다. @dlt.table 데코레이터를 사용하여 테이블의 이름을 지정하고 (원하는 테이블 이름으로 대체), 파이프라인을 매 초 Kafka를 폴링하도록 설정합니다. 이 신속한 폴링은 거의 실시간 데이터 처리 요구를 지원합니다. dlt_kafka_stream() 함수는 read_kafka_stream()을 호출하여 Kafka 스트리밍을 DLT로 직접 통합하여 Databricks 환경 내에서의 관리 및 운영을 간소화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n@dlt.table(name=\"여기에 바꿔야 할 DLT 테이블 이름\",\n           spark_conf={\"pipelines.trigger.interval\" : \"1 초\"})\ndef dlt_kafka_stream():\n    read_kafka_stream()\r\n```\n\n# 결론\n\n저희의 벤치마크 결과에 따르면, 델타 라이브 테이블 서버리스는 대기 시간 성능과 운영 간소화 측면에서 우수한 성과를 보여주며, 다양한 데이터 부하가 있는 시나리오에 매우 적합합니다. 한편, 데이타브릭스 잡과 델타 라이브 테이블도 실용적인 솔루션을 제공합니다.\n\n- 대기 시간 비교: 델타 라이브 테이블의 서버리스 버전은 모든 측정 백분위수에서 대기 시간 측면에서 다른 것들을 능가합니다.\n- 운영 복잡성: 델타 라이브 테이블 서버리스는 수동 인프라 관리가 필요하지 않은 가장 단순한 설정을 제공하며, 그 다음으로 델타 라이브 테이블, 그리고 데이타브릭스 잡이 이어집니다. \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Delta Live Tables Serverless가 표준 Delta Live Tables를 능가하는 이유\n\nDelta Live Tables Serverless가 표준 Delta Live Tables보다 우수한 성능을 발휘하는 핵심 요소는 Stream Pipelining을 활용한 것입니다. 표준 구조화된 스트리밍과 달리 DLT Serverless는 마이크로배치를 동시에 처리함으로써 처리량을 향상시키고 응답 시간을 크게 향상시킵니다. 이 기능을 통해 전체 컴퓨팅 자원 활용도도 크게 향상됩니다. Stream Pipelining은 서버리스 DLT 파이프라인의 기본 기능이며, 데이터 입력과 같은 스트리밍 데이터 워크로드를 위해 성능을 최적화합니다.\n\n또한, 수직 자동 스케일링은 DLT Serverless의 효율성을 향상시키는 중요한 역할을 합니다. 이 기능은 Databricks Enhanced Autoscaling의 수평 자동 스케일링 기능을 보완하여 DLT 파이프라인을 실행하는 데 필요한 가장 비용 효율적인 인스턴스 유형을 자동으로 선택합니다. 더 많은 자원이 필요할 때 적절하게 대규모 인스턴스로 확장하고, 메모리 사용률이 지속적으로 낮을 때 축소합니다. 이러한 다이내믹한 조정은 실시간 요구 사항에 기반하여 드라이버 및 워커 노드를 최적으로 조정하여 중요한 역할을 합니다. 제품 모드에서 파이프라인을 업데이트하거나 개발 중 수동으로 조정하는 경우에도, 수직 자동 스케일링은 메모리 부족 오류가 발생한 후 신속하게 더 큰 인스턴스를 할당하여 서비스 중단 없이 자원 할당을 최적화합니다.\n\nStream Pipelining과 수직 자동 스케일링은 운영 복잡성을 감소시키고 Serverless DLT 파이프라인의 신뢰성 및 비용 효율성을 향상시킵니다. 이러한 기능을 통해 Serverless DLT는 수동 개입을 최소화하면서 변동하는 데이터 입력 부하를 처리하는 데 이상적인 선택지가 되어 더 빠르고 효율적인 데이터 파이프라인 실행을 실현합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 각주:\n\n이 글을 읽어 주셔서 감사합니다. 도움이 되었거나 즐거우셨다면 박수를 치는 것을 고려해 주시고, 다른 사람들이 이를 발견할 수 있도록 도와주세요. 더 많은 정보를 찾으려면 제 웹사이트 CanadianDataGuy.com을 방문하시고, 더 많은 통찰력 있는 콘텐츠를 제공하기 위해 제 팔로우도 잊지 마세요. 여러분의 지원과 피드백은 제게 귀중하며, 제 작품에 대한 여러분의 참여를 감사히 여깁니다.","ogImage":{"url":"/assets/img/2024-06-19-NeedforSpeedBenchmarkingtheBestToolsforKafkatoDeltaIngestion_0.png"},"coverImage":"/assets/img/2024-06-19-NeedforSpeedBenchmarkingtheBestToolsforKafkatoDeltaIngestion_0.png","tag":["Tech"],"readingTime":10},{"title":"트위터가 매일 40억 건의 이벤트를 실시간으로 처리하는 방법","description":"","date":"2024-06-19 01:40","slug":"2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily","content":"\n\n## 람다에서 카파로\n\n# 목차\n\n- 문맥과 과제\n- 이전 아키텍처\n- 새로운 아키텍처\n- 평가\n\n# 소개\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n몇 주 전에는 우리가 어떻게 Uber가 실시간 인프라를 처리하여 매일 수백만 건의 이벤트를 처리하는지 배웠습니다. 이번 주에는 데이터 실시간 처리 요구 사항에 대한 다른 대형 기술 회사의 처리 방법을 살펴볼 것입니다: 트위터.\n\n# 맥락과 도전 과제\n\n트위터는 매일 400조 건의 이벤트를 실시간으로 처리하고 페타바이트(PB)의 데이터를 생성합니다. 이벤트는 다양한 소스(하둡, 카프카, 구글 빅쿼리, 구글 클라우드 스토리지, 구글 퍼브섭 등)에서 발생합니다. 트위터는 데이터의 대규모 규모에 대응하기 위해 각 요구 사항에 특화된 내부 도구를 구축했습니다: 배치 처리용 Scalding, 스트림 처리용 Heron, 배치 및 실시간 처리용 TimeSeries AggregatoR 프레임워크, 데이터 소비용 데이터 액세스 레이어.\n\n기술의 견고성에도 불구하고 데이터의 성장은 인프라에 압력을 가합니다; 가장 현저한 예는 상호 작용 및 참여 파이프라인인데, 이는 대규모 데이터를 배치 및 실시간으로 처리합니다. 이 파이프라인은 Tweet와 사용자 상호 작용 데이터를 다양한 수준의 집계 및 메트릭스 차원을 사용하여 추출하기 위해 다양한 실시간 스트림 및 서버 및 클라이언트 로그에서 데이터를 수집하고 처리합니다. 이 파이프라인의 집계 데이터는 트위터의 광고 수익과 다양한 데이터 제품 서비스의 진실의 원천 역할을 합니다. 따라서 이 파이프라인은 낮은 지연 시간과 높은 정확성을 보장해야 합니다. 트위터가 이 임무를 처리하는 방법을 살펴봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 오래된 아키텍처\n\n## 개요\n\n![이미지](/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_0.png)\n\n트위터는 처음에 람다 아키텍처를 사용했습니다. 이는 정확한 일괄 데이터 뷰를 제공하는 일괄 처리와 온라인 데이터를 보여주는 실시간 스트림 처리 두 개의 별도 파이프라인이 있습니다. 두 뷰 출력은 하루가 끝날 때 합쳐집니다. 트위터는 다음과 같이 아키텍처를 구축했습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Summingbird Platform: 제가 이해한 바로는, 이 플랫폼에는 Scalding과 Heron과 같은 여러 분산 엔진들과 MapReduce 로직을 정의하고 이를 해당 엔진에서 실행할 수 있도록 허용하는 전용 라이브러리가 포함되어 있습니다.\n- TimeSeries AggregatoR: 견고하고 확장 가능한 실시간 이벤트 시계열 집계 프레임워크.\n- Batch: 배치 파이프라인의 소스는 로그, 클라이언트 이벤트 또는 HDFS의 트윗 이벤트에서 나올 수 있습니다. Summingbird 플랫폼으로 데이터를 전처리하고 결과를 맨해튼 분산 저장 시스템에 저장하기 위해 많은 Scalding 파이프라인이 사용됩니다. 비용을 절감하기 위해 Twitter는 배치 파이프라인을 한 데이터 센터에 배치하고 데이터를 다른 2개 데이터 센터에 복제합니다.\n- 실시간: 실시간 파이프라인의 소스는 Kafka 주제에서 나옵니다. 데이터는 Summingbird 플랫폼 내의 Heron으로 \"흘러들어가\", 그런 다음 Heron의 결과가 Twitter Nighthawk 분산 캐시에 저장됩니다. 배치 파이프라인과 달리, 실시간 파이프라인은 3개 서로 다른 데이터 센터에 배포됩니다.\n- 배치 및 실시간 저장소 위에 쿼리 서비스가 있습니다.\n\n## 도전\n\n실시간 데이터의 대규모 및 높은 처리량으로 인해 데이터 손실 및 부정확성의 위험이 있습니다. 이벤트 스트림에 대한 처리 속도가 따라가지 못할 경우, Heron 토폴로지에서 백프레셔가 발생할 수 있습니다 (방향성을 갖는 비순환 그래프는 데이터 처리의 Heron 흐름을 나타냅니다). 시스템이 어느 정도 동압을 겪으면, Heron 볼트(일종의 노동자로 생각할 수 있음)가 지연이 누적되어 전체 시스템 대기 시간이 길어질 수 있습니다.\n\n뿐만 아니라, 백프레셔로 인해 많은 Heron 스트림 매니저 (스트림 매니저는 토폴로지 구성 요소 간의 데이터 라우팅을 관리함)가 실패할 수 있습니다. Twitter의 해결책은 스트림 매니저를 다시 시작하여 스트림 매니저를 복구하는 것입니다. 그러나, 재시작은 행사 손실을 일으킬 가능성이 있어 파이프라인의 전반적 정확성을 줄일 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 새 아키텍처\n\n## 개요\n\n![이미지](/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_1.png)\n\n새로운 접근 방식으로 트위터는 Kappa 아키텍처를 사용하여 하나의 실시간 파이프라인으로 솔루션을 간소화했습니다. 이 아키텍처는 트위터 내부 및 Google Cloud Platform 솔루션을 활용할 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 온프레미스: 그들은 카프카 토픽 이벤트를 구글 파브섭 이벤트 형식으로 변환하는 전처리 서비스를 구축했습니다.\n- 구글 클라우드: 이벤트 들어오기 위해 파브섭을 사용하고, 중복 제거 및 실시간 집계에는 Dataflow 작업을 활용하며, 결과를 저장하기 위해 BigTable을 사용합니다.\n\n새로운 아키텍처의 프로세스 흐름은 다음과 같이 설명할 수 있습니다:\n\n- 단계 1: 소스 카프카 토픽에서 데이터를 소비하고 변환 및 필드 재매핑을 수행한 후 최종 결과를 중간 카프카 토픽으로 보냅니다.\n- 단계 2: 이벤트 프로세서는 중간 카프카 토픽에서 데이터를 파브섭 형식으로 변환하고 이벤트에 데이터 중복 제거를 위해 사용되는 UUID 및 처리 컨텍스트와 관련된 일부 메타정보를 추가합니다.\n- 단계 3: 이벤트 프로세서는 이벤트를 구글 파브섭 토픽으로 보냅니다. 트위터는 메시지가 구글 클라우드로 적어도 한 번 방식으로 전달되도록 PubSub을 게시하는 이 프로세스를 거의 무한정 재시도합니다.\n- 단계 4: 구글 Dataflow 작업은 파브섭에서 데이터를 처리합니다. Dataflow 작업자는 실시간으로 데이터 중복 제거 및 집계를 처리합니다.\n- 단계 5: Dataflow 작업자는 집계 결과를 BigTable에 기록합니다.\n\n# 평가\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 새로운 접근 방식의 성취\n\n- 오래된 아키텍처의 10초에서 10분 지연과 비교하여 대략 10초의 지연이 안정적으로 유지됩니다.\n- 실시간 파이프라인은 최대 ~100MB/s 인 이전 아키텍처 대비 대략 1GB/s의 처리량을 달성할 수 있습니다.\n- Google Pubsub에 최소 한 번 데이터 게시 및 데이터 흐름으로 부터의 중복 제거 작업으로 거의 정확히 한 번 처리 보장.\n- 일괄 처리 파이프라인 구축 비용 절감.\n- 더 높은 집계 정확도 달성.\n- 늦게 발생하는 이벤트 처리 기능.\n- 다시 시작 시 이벤트 손실 없음\n\n## 중복 비율 모니터링 방법\n\n\u003cimg src=\"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_2.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n트위터는 두 개의 별도 Dataflow 파이프라인을 생성합니다: 하나는 Pubsub에서 원시 데이터를 직접 BigQuery로 전달하고, 다른 하나는 중복 제거된 이벤트 카운트를 BigQuery로 내보냅니다. 이 방식으로 Twitter는 중복 이벤트 백분율 및 중복 제거 후의 백분율 변경을 모니터링할 수 있습니다.\n\n## 이전 배치 파이프라인의 중복 제거된 개수를 새 Dataflow 파이프라인과 비교하는 방법은?\n\n![image](/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_3.png)\n\n- BigTable에 쓰는 것 외에도, 새 워크플로우는 중복 제거된 및 집계된 데이터를 BigQuery로 내보냅니다.\n- 트위터는 또한 이전 배치 데이터 파이프라인 결과를 BigQuery로 로드합니다.\n- 중복 카운트를 비교하기 위해 예약된 쿼리를 실행합니다.\n- 결과는 새로운 파이프라인 결과 중 95% 이상이 이전 배치 파이프라인과 정확히 일치한다는 것입니다. 5%의 차이는 주로 원래 배치 파이프라인이 지연된 이벤트를 버린 반면, 새 파이프라인은 효율적으로 포착할 수 있기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 마무리말\n\nTwitter가 새로운 Kappa 아키텍처로 전환함으로써, 예전 아키텍처와 비교하여 지연 시간과 정확성 면에서 크게 개선되었습니다. 더 나은 성능 뿐만 아니라, 새로운 아키텍처는 데이터 파이프라인을 간소화하여 스트림만을 유지했습니다.\n\n다음 블로그에서 뵙겠습니다.\n\n# 참고문헌\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[1] Lu Zhang and Chukwudiuto Malife, 트위터에서 실시간으로 수십억 개의 이벤트 처리하기 (2021)","ogImage":{"url":"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_0.png"},"coverImage":"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_0.png","tag":["Tech"],"readingTime":5},{"title":"보안 데이터 플랫폼 구축 팀 구조 및 접근 전략에 대한 안내","description":"","date":"2024-06-19 01:38","slug":"2024-06-19-BuildingSecureDataPlatformsAGuideforTeamsStructureAccessStrategies","content":"\n\n몇 달 동안 데이터 솔루션을 개발하며, 엔지니어와 데이터 소유자들은 증가된 데이터 액세스 요청으로 인한 병목 현상에 직면할 수 있습니다(전체 테이블, 스키마, 서브셋, 민감한 데이터 등). 응용 프로그램 기술 사용자나 셀프 서비스 이용자와 관련이 있는 이러한 상황을 처리하는 것이 어렵거나 스트레스를 받을 수 있습니다. \n\n데이터 아키텍처, 데이터가 어떻게 구조화되고 모델링되는지, 데이터 액세스 전략과 패턴, 기술 등이 이러한 시나리오를 다루는 데 얼마나 어렵거나 원활한지에 영향을 미칩니다. \n\n이 기사에서는 조직이 데이터 제품을 구조화하고 데이터 플랫폼의 데이터 액세스 전략을 설계할 수 있는 방법에 대해 논의합니다. 유연하고 빠르며 안전한 데이터 소비를 보장합니다. 또한 데이터 아키텍처와 조직 구성이 확장 가능성에 영향을 미치는 다양한 전략과 잠재적인 아키텍처 및 전략을 통해 병목 현상에 대한 대응 전략을 살펴봅니다. 사용자와 팀의 액세스 수명주기 및 일부 기술이 프로세스를 용이하게 할 수 있는 방법에 대해 이야기합니다. 모든 동안 팀과 조직이 그러한 솔루션을 개발하고 적응하는 데 직면할 수 있는 제약 사항을 고려합니다.\n\n# 목차\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1. 소개\n\n2. 장면 설정\n\n3. 간단한 역할 기반 데이터 액세스\n\n4. 팀 내 세밀한 액세스 제어\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5. 다국적 데이터 접근\n\n6. 요약 및 주요 포인트\n\n7. 참고 자료\n\n# 1. 소개\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아키텍처와 솔루션을 상세히 설명하기 전에, 왜 이 주제를 다뤄야 하고 몇 분 동안이라도 이에 집중하는 것이 중요한 지 강조해보겠습니다.\n\n다양한 IT 기관에서 일하고 데이터 솔루션을 구축해온 제 경험과 많은 전문가 및 동료들로부터 받은 피드백들을 종합해보면, 현대 데이터 플랫폼을 구축할 때 이러한 중요한 측면에 대해 자세히 다룬 자료가 부족하다는 결론에 도달하게 되었습니다.\n\n빠른 전달 속도는 종종 초기 개념 설계에 대한 노력이 제한되어 장기적으로 확장성을 방해하는 병목 현상을 일으킵니다. 많은 경우, 이는 재설계 작업 및 추가 개발 노력을 수반하게 됩니다.\n\n이 주제에 대한 중요성은 더 많은 최근 사건들을 살펴볼수록 더욱 빛을 발합니다. 민감한 고객 데이터가 유출된 사건들을 생각해보면요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n글로벌 기관이 개인 데이터를 처리하는 것에 대한 고객의 신뢰가 상실되었습니다. 몇 가지 이벤트로 인해 액세스 권한으로 인한 800만 명 이상의 사용자 데이터 유출이 발생했습니다. 직원들이 중요한 민감 데이터에 액세스하면서 소셜 엔지니어링에 속아들기도 했고, 때로는 정부 기관까지 영향을 끼쳤습니다. 이 같은 사례가 몇 년 동안 계속되었지만, 이해하셨을 것입니다.\n\n고객의 신뢰는 첫 번째 영향이며, 기관이 사용자 데이터를 보호하는 데 책임이 있다고 판명되면 법적 결과까지 번질 수 있습니다. GDPR, CCPA 등의 법적 틀이 조직이 올바른 방어 조치를 마련하여 이러한 실수를 피하기 위해 설정된 데이터 보호 조치를 의무화합니다.\n\n이제 데이터 액세스 관리를 위한 강력하고 안전한 전략이 중요하다는 사실을 간단히 명확히 한 바 있으니, 이를 실현하기 위한 방법에 대해 이야기해 보겠습니다.\n\n# 2. 상황 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특정 데이터 아키텍처에 대한 해결책을 제시하는 것은 쉽지만, 이는 현실을 반영하지 않습니다. 조직은 다양한 데이터 아키텍처(Data Mesh, 중앙 데이터 웨어하우스, Lakehouse 등)와 구성원 구조를 가지고 있어 구현 가능한 범위에 제한을 둡니다.\n\n우리는 다양한 아키텍처를 고려한 여러 사용 사례에 대해 논의할 것이며, 이는 명확한 해결책 전략으로 나아가는 첫걸음이 될 것입니다.\n\n해결책과 사용 사례를 더 쉽게 이해하기 위해 기본 사례부터 시작하여 추가 요구사항을 시뮬레이션하여 확장해 나갈 것입니다.\n\n기본 사용 사례는 다음과 같이 시작됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 중심이 되는 데이터 팀이 메달리온 기반 데이터 아키텍처를 관리합니다.\n- 데이터 통합 프로세스는 ETL 또는 ELT 방식을 따를 수 있습니다.\n- 여러 소스에서 데이터를 수집하고 셀프 서비스 및 기술 사용자 소비자(응용 프로그램)를 위한 데이터 액세스를 활성화합니다.\n\n데이터 자산에 대한 접근이 주요 관심사가 될 것입니다. 이 아키텍처에는 각 단계별 데이터 자산이 포함될 것입니다. 사용 사례에 따라 소비자들은 소비 사용 사례에 맞는 다양한 단계에 액세스가 필요할 수 있습니다.\n\n우리가 답변하고자 하는 질문은 누가 무엇에 액세스할 것이며 어떻게 하는가인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 소규모 조직이나 스타트업에서 발견되는 전형적인 설정입니다.\n\n데이터 레이크/하우스 조직은 베이스 메달리온 단계로 나눠집니다. 액세스는 고수준 팀 역할에 따라 분리됩니다.\n\n베이스 케이스 가정\n\n- 사용자들은 최종 품질 테스트된 데이터 제품에만 액세스가 필요합니다.\n- 데이터 제품에는 고객/조직의 민감한 정보가 포함되어 있지 않습니다.\n- 액세스는 조직 내의 셀프 서비스 소비자에 대해 유연합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최종적으로, 데이터 이용자들은 Gold 단계 내에서 개발된 데이터 제품에 액세스할 수 있어야 합니다. 경우에 따라서는 업무 지속성과 확장을 보장하기 위해 용량이나 시간 제약 때문에 우회되기도 합니다.\n\n각 조직이 데이터 과학자와 분석가 역할에 대해 고유한 정의를 가지고 있다는 점을 인지하는 것이 중요합니다. 여기서 논의하는 맥락에서, 데이터 과학자는 예를 들어 ML 모델을 개선하기 위해 원시 데이터에 접근할 수도 있습니다. 그러나 데이터 과학자들의 구성원은 데이터 분석가 그룹 내에서도 존재할 수 있습니다.\n\n팀을 엔지니어, 분석가 및 과학자로 나누는 것은 보다 세분화된 역할 기반의 액세스 제어 (RBAC)로 나아가는 첫 걸음이 될 것입니다. 이는 조직 전반에 걸쳐 데이터 이용 가능성을 보장하면서도 액세스 범위를 제한하는 것을 의미합니다.\n\n이와 같은 설정에서 준수하는 민감한 데이터 처리를 보장하는 주제는 다른 기사에서 다루고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 시나리오는 단기적으로는 작동할 수 있지만, 조직, 팀 구조 및 액세스 프로세스가 더 복잡해질 것입니다.\n\n## 4. 팀 내의 세분화된 액세스 제어\n\n단순한 액세스 제어는 복잡한 액세스 제어 요구 사항에 대응하기 어려울 수 있습니다. 본 섹션에서는 팀이 액세스 요구 사항에 더 많은 복잡성과 유연성을 도입할 수 있는 잠재적인 아키텍처와 전략을 정의합니다.\n\n아키텍처 설정에 대한 예상된 지형을 구성하는 것부터 시작해보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 분석가 분배: 분석가들은 부서/제품팀 사이에 나뉘어 있습니다.\n- 역할 계층구조: 엔지니어 및 분석가들은 조직 구조에 따라 주니어, 시니어 등으로 더 세분화됩니다.\n- 데이터 접근 변동성: 팀 구성원들은 동일한 데이터에 접근 권한을 가지지 않을 수 있습니다.\n- 개인 데이터 기여: 팀 구성원들은 자신의 데이터/자산을 데이터 플랫폼에 제공할 수 있습니다.\n\n이러한 점들을 한 단계씩 처리해 보겠습니다.\n\n- 점 1은 추가적인 팀을 소개합니다.\n- 점 2와 3은 팀 당 추가 역할이 필요합니다.\n- 점 4는 팀 데이터 분석가에게 새로운 요구사항/권한을 제공합니다.\n\n이러한 설정으로 시각적으로 어떻게 보일지 살펴보고, 그 후에 다이어그램 구성 요소를 명확히 설명해보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 팀 설정\n\n비즈니스/IT 조직을 이해하세요. 올바른 그룹 및 액세스 전략을 설정하려면 사용 및 액세스 패턴을 이해해야 합니다. 이러한 설정은 계속 변경될 수 있습니다. 자동 액세스 및 역할 할당 프로세스를 구축하면 병목 현상을 완화할 수 있습니다.\n\n도표는 팀 수준에서 그룹이 할당되는 방식을 설명합니다. 서브 그룹은 권한을 상속하고 확장하여 세밀한 액세스 제어를 보장하기 위해 추가로 생성됩니다.\n\n서브그룹은 읽기/쓰기 권한 및 민감한 데이터 보호를 위한 추가 보안 계층을 제공하는 데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n팀의 분리 방법은 확정되어 있지 않습니다. 여러 범위와 수준을 자유롭게 시험해보세요. 그러나 효과적으로 관리하지 않으면 지나치게 복잡한 계층 구조가 될 수 있습니다.\n\n기술 사용자\n\n어플리케이션에 사용되는 기술 사용자들은 팀 그룹에 추가할 수도 있습니다. 이를 통해 데이터 범위를 셀프 서비스 사용자에게도 포함시키는 데 도움이 됩니다. 기술 사용자를 위해 별도의 하위 그룹을 만들면 여러 종류의 사용자를 쉽게 모니터링하고 관리할 수 있습니다.\n\n## 데이터 레이크/하우스 구조화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터가 데이터 레이크나 데이터 웨어하우스 내에서 구조화되고 조직화되는 방식은 액세스 제어 전략의 유연성을 지원하거나 제한할 수 있습니다.\n\n아키텍처에 따라 각 변환 수준에 다른 이름/레이블이 할당됩니다 (다이어그램 참조). 일부는 서로 다른 기능을 수행할 수도 있지만, 액세스 계층은 동일한 패턴을 따릅니다. 다이어그램은 데이터베이스/데이터 레이크, 스키마, 테이블, 행/열 수준의 각 팀별로 액세스 권한이 어떻게 나뉘어지는지 보여줍니다.\n\n보안 요구 사항에 따라 팀들이 데이터베이스 수준에서 더 분리되어야 할 수도 있습니다. 다시 말하지만, 이는 실제로 가능한 일이지만, 확장성과 복잡성 요소에 대해 고려하는 것이 중요합니다.\n\n내가 경험한 또 다른 액세스 전략은 액세스를 제한하기 위해 뷰를 사용하는 것이었습니다. 이는 요구 사항을 충족시키지만, 병목 현상은 뷰를 작성하고 관리하는 데서 나타납니다. 필요한 그룹에 사용자를 간단히 추가하는 속도가 뷰에 비해 일부 시나리오에서 더 나은 결과를 도출할 수 있습니다 (성장 및 확장이 제한된 소규모 팀의 경우).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-BuildingSecureDataPlatformsAGuideforTeamsStructureAccessStrategies_0.png)\n\n# 5. 다국적 데이터 접근\n\n마지막 섹션에서 논의한 내용을 국제적 수준으로 확장하기 위해 국가 차원을 그룹에 추가합니다.\n\n세계적 규모로 운영하는 기업은 전 섹션에서 논의된 데이터 보안 및 규정 준수 주제에 관해서 다양한 어려움을 겪게 됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 분리 및 지역성 규칙을 다양한 법적 요구 사항에 맞추는 것은 도전적입니다. 다행히 현재는 액세스 측면을 논의하고 있으며 데이터 준수 법률 컨설팅은 아직 논의하지 않고 있습니다.\n\n그렇다면 이러한 아키텍처는 어떻게 보일까요? 우리가 위에서 스케치한 다이어그램을 다국적 수준에 맞게 조정해 보겠습니다.\n\n각각이 포함하는 구성 요소를 자세히 설명하는 두 가지 아키텍처 레이아웃이 있습니다. 이는 사업 단위 및 부서 수준으로 더 자세히 들어가지만, 그것은 세분화된 액세스 섹션에서 언급한 내용을 확장하는 것에 불과할 것입니다.\n\n일부 팀이 다국적으로 확장될 수 있다는 점도 보여주는 것이 중요합니다. 조직 구성에 따라 그룹은 적절하게 조정되어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n관심이 있다면, 자동 설정 및 구성을 위해 관련 아키텍처를 자세히 살펴보고 싶다면, 주제에 대한 다른 기사를 확인하실 수 있습니다.\n\n# 요약 및 주요 포인트\n\n팀 및 구성원들에 대한 올바른 액세스 전략을 보장하는 것은 완전하고 안전한 데이터 아키텍처를 위한 중요한 요소입니다. 우리는 팀 수준에서 액세스 그룹을 구분하고, 데이터베이스/데이터 레이크, 변환 단계, 테이블, 그리고 열/행 수준별로 더 나누는 중요성에 대해 논의했습니다. 우리가 논의한 내용에서 얻을 수 있는 주요 포인트는 다음과 같습니다:\n\n- 데이터 아키텍처와 비즈니스 조직은 성공적인 액세스 전략을 설정하는 데 중요합니다.\n- 데이터 자산에 액세스하는 구성원들은 항상 자동화된 액세스 그룹의 구성원이어야 하며, 그들의 액세스는 로그 기록 및 모니터링되어야 합니다.\n- 법적 요구 사항은 데이터 아키텍처 및 액세스 그룹을 조정해야 할 수 있습니다.\n- 그룹에 대한 액세스를 쉽게 조절하고 변경할 수 있는 원활한 프로세스를 구축함으로써, 액세스 제어 전략을 관리하는 데 속도, 유연성, 확장성을 제공할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n구독하시면 새 이야기를 게시할 때 알림을 받을 수 있어요.\nLinkedIn에서 언제든지 연락 주시면 됩니다.\n\n더 비슷한 기사를 찾고 계시다면 아래 목록을 확인해보세요:\n\n## 참고 자료","ogImage":{"url":"/assets/img/2024-06-19-BuildingSecureDataPlatformsAGuideforTeamsStructureAccessStrategies_0.png"},"coverImage":"/assets/img/2024-06-19-BuildingSecureDataPlatformsAGuideforTeamsStructureAccessStrategies_0.png","tag":["Tech"],"readingTime":7},{"title":"아이스버그 카탈로그 당신의 레이크하우스의 TIP","description":"","date":"2024-06-19 01:36","slug":"2024-06-19-IcebergCatalogTheTIPofyourLakehouse","content":"\n\n아이스버그 카탈로그 랜드스케이프는 Snowflake와 Databricks로부터 중요한 발표가 속출하며 급속하게 변화하고 있어요. 이 다채로운 생태계에 TIP를 소개합니다. HANSETAG가 선보이는 이 Rust-native Iceberg REST 카탈로그는 데이터 품질, 거버넌스, 그리고 유연성을 우선시합니다. 변경 이벤트, 계약 유효성 검사, 멀티 테넌시와 함께 경량화되고 맞춤형 솔루션으로 혁신을 추구해요.\n\n![이미지](/assets/img/2024-06-19-IcebergCatalogTheTIPofyourLakehouse_0.png)\n\n이제 몇 날 뒤 데이터 랜드스케이프에서는 지표적인 변화가 일어나고 있어요. 특히, Snowflake는 공개 소스 아이스버그 REST 카탈로그인 Polaris를 발표했고, Databricks는 Apache Iceberg를 만든 파이오니어 회사 Tabular.io를 인수했어요. 더불어 Databricks는 아이스버그 REST(읽기) 지원을 하는 Unity Catalog도 오픈소스로 공개했죠. 이제 요즘에는 데이터 회사로 진정으로 인정받으려면 아이스버그 카탈로그의 구현을 오픈소스화해야 하는 것 같아요.\n\n이미 화요일이 다가왔고, 의심스러운 침묵이 느껴집니다. 이번 주에 새로운 카탈로그가 나오지 않았어요. 하지만 걱정하지 마세요! 저희가 TIP를 소개해 드릴게요. 이는 Rust-native, 멀티 테넌트, 싱글 바이너리로 Iceberg REST 카탈로그를 구현한 것이에요. 하지만 주의해 주세요, 우리는 약간 다르게(알려진 대로 😉) 그리고 기존 카탈로그와 다른 방식으로 일을 진행하고 있어요. 이에 대해 더 자세히 설명하겠어요. 함께 시작해 볼까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 우리는 누구인가\n\nHANSETAG에서는 회사들이 데이터 자산을 구축, 공유 및 활용할 수 있는 첨단 데이터 제품 플랫폼을 구축합니다. 저희 플랫폼은 데이터 계약서에 기록된 강제 SLO를 통해 지속적으로 높은 데이터 품질을 보장합니다. 플랫폼을 구동하기 위해 신뢰할 수 있는 멀티 테넌트 Iceberg 카탈로그가 필요합니다. 기존 구현이 우리의 요구를 충족하지 못했기 때문에 Apache 라이센스 하에 자체 카탈로그를 구축했습니다. 기여해 주시기 바랍니다! 😊\n\n# 데이터 레이크하우스\n\n데이터 레이크하우스는 데이터 및 분석 프로젝트의 기본 아키텍처가 되었으며 데이터 메쉬 구현을 위한 기반을 제공합니다. 데이터 레이크의 유연성과 데이터 웨어하우스의 구조와 편의성을 결합합니다. 모든 레이크하우스가 동일하지 않다는 점을 인식하는 것이 중요합니다. 레이크하우스에는 Apache Iceberg, Delta Lake 및 Apache Hudi와 같은 세 가지 테이블 포맷이 등장했습니다. Iceberg의 신속한 개발과 뛰어난 벤더 독립적인 커뮤니티로 인해 HANSETAG에서는 초기 레이크하우스 형식으로 Iceberg를 선택했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 아이스버그 및 REST 카탈로그\n\n아파치 아이스버그 카탈로그는 데이터 레이크하우스의 핵심으로 대규모 데이터 세트의 효율적인 관리 및 조직을 제공합니다. 이는 네임스페이스, 테이블 및 뷰의 중앙 저장소 역할을 하며, 개별 객체에 대한 세밀한 접근을 관리합니다.\n\n아이스버그는 REST 이외의 다른 카탈로그를 지원합니다. 그러나 최근 시장 활동에 의해 강조된 것처럼, 아이스버그 REST 카탈로그가 가장 밝은 미래를 가지고 있습니다. 또한 현재의 아이스버그 카탈로그 중 유일한 것으로, 클라이언트 측에서 저장 자격 증명을 지정하지 않고 데이터에 대한 클라이언트 접근을 허용합니다. 아이스버그 커뮤니티는 REST 명세의 새 버전의 일부로 모든 다른 카탈로그를 REST 카탈로그를 통해 보완할 것을 고려하고 있습니다.\n\n# TIP가 다른 점\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n카탈로그를 구축하는 것은 로켓 과학은 아니지만 경솔하게 결정한 것은 아닙니다. 우리 플랫폼에서 데이터 품질과 데이터 거버넌스를 우선시하며 사용자들이 자체 호스팅이든 클라우드에 있든 데이터에 대해 완전한 통제를 할 수 있기를 바랍니다. 결과적으로 기존 표준을 혁신하는 방법으로 카탈로그를 구축했습니다:\n\n- 멀티 테넌트: 하나의 TIP 배포로 여러 프로젝트를 서비스할 수 있습니다. 각 프로젝트는 여러 창고를 포함할 수 있습니다. 창고와 프로젝트는 실행 중에 REST-API를 통해 동적으로 추가되며 각각 다른 저장 위치에 있습니다.\n- 변경 이벤트: 외부 시스템이 우리 테이블에 어떤 변경이 가해졌는지 알 수 있어야 합니다. 우리는 각 테이블 변경 시 이벤트(Cloudevents)를 발생시킵니다. 예를 들어 테이블 스키마가 변경될 때 이벤트를 발행합니다.\n- 변경 승인 / 계약 검증: 데이터 계약은 HANSETAG에서 하는 주요 작업입니다. 변경 이벤트를 통해 스키마 변경을 알 수 있지만, 데이터 계약에 영향을 미칠 수 있으며 이로 인해 하류 파이프라인이 모두 손상될 수 있습니다. TIP를 사용하면 ContractVerification 트레이트를 사용하여 해당 변경 사항을 금지할 수 있습니다. 데이터 계약의 오픈 소스 구현에 주목하세요!\n- Rust로 작성: 단일 작고 통합된 이진 파일. JVM이나 Python 환경이 필요하지 않습니다.\n- 사용자 정의 가능: TIP는 확장 가능하도록 설계되었지만 필수 구성요소가 함께 제공됩니다. 향후 통합을 추가하고 여러분이 몇 가지 메서드를 구현하여 자체 통합을 작성할 수 있도록 키 인터페이스를 공개합니다. 이러한 인터페이스는 다음과 같습니다:\n   - 백엔드 데이터베이스 (카탈로그): Postgres 외부에 내장\n   - 시크릿 저장소: Postgres 내장, Vault 작업 진행 중\n   - 이벤트 시스템: NATS 내장\n   - 인증: OIDC 내장, Zitadel 작업 진행 중\n   - 권한: 곧 OpenFGA 내장 예정\n   - 계약 검증: 곧 출시될 데이터 계약 라이브러리 지원\n- 저장소 액세스 관리: 클라이언트와 S3 자격 증명을 공유하지 않고 자체 호스팅 및 AWS S3를 지원하는 내장된 S3-Signing을 제공합니다. 또한 판매 자격 정보에 대해 작업 중입니다!\n- 외부 잘 정의된 접근 (FGA): TIP는 권한을 내부적으로 저장하지 않으며 결코 저장하지 않습니다. 인증을 구현하기 위해 일부 메서드를 구현하여 다른 시스템과 통합할 수 있습니다.\n\n# 시작하기\n\n```js\ngit clone https://github.com/hansetag/iceberg-catalog.git\ncd iceberg-catalog/examples\ndocker compose up\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGitHub에서 더 많은 자세한 내용을 확인하러 가보세요! 그리고 별표 한 개 꾸욱 눌러주세요!\n\nGithub: https://github.com/hansetag/iceberg-catalog\nHANSETAG: https://hansetag.com/","ogImage":{"url":"/assets/img/2024-06-19-IcebergCatalogTheTIPofyourLakehouse_0.png"},"coverImage":"/assets/img/2024-06-19-IcebergCatalogTheTIPofyourLakehouse_0.png","tag":["Tech"],"readingTime":4},{"title":"마라톤 결과의 연령 그레이딩을 위한 더 나은 시스템이 있을까요","description":"","date":"2024-06-19 01:33","slug":"2024-06-19-IsThereaBetterSystemforAgeGradingMarathonResults","content":"\n\n\n\u003cimg src=\"/assets/img/2024-06-19-IsThereaBetterSystemforAgeGradingMarathonResults_0.png\" /\u003e\n\n서로 다른 두 명의 러너 사이에 경주 결과를 공정하게 비교하려면 어떻게 해야 할까요?\n\n예를 들어, 세 명의 러너가 모두 3:00에 마라톤을 완주했다고 가정해 봅시다. 표면적으로 보면 세 명 모두 잘 한 것으로 말할 수 있습니다.\n\n하지만 그 세 명의 러너가 25세 남성, 40세 여성, 60세 남성이라면 어떨까요? 그 시간은 각각의 러너에게 동일한 만큼 힘든 것이 아닙니다. 그렇다면 누가 가장 우수한 성적을 거뒀을까요?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지난 몇 십 년 동안 연령 등급이 시도해 온 질문입니다. 최근 연재한 몇 개의 기사에서 이 문제를 탐구해 왔습니다.\n\n전통적인 연령 등급 체계는 유용하며 비교를 하나의 방법으로 분류하는 데 도움이 됩니다. 그러나 그것은 결함이 없는 것은 아닙니다.\n\n성적을 하나의 숫자로 단순화한다는 점 때문에 항상 정확하고 신뢰할 수 있는 객관적인 측정이라고 생각하는 함정에 빠지기 쉽습니다.\n\n마라톤 성적의 비교적인 강도를 대략적으로 나타내기 위해 몇 가지 대규모 결과 데이터 세트를 수집해 분석한 결과, 성적을 연령별 백분위수로 비교하는 대안을 제시하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 시리즈를 따라오셨다면 다음 두 링크가 가장 흥미로울 것입니다:\n\n- 2023 데이터를 사용하여 업데이트된 (그리고 향상된) 연령 등급 계산기 버전을 확인할 수 있습니다.\n- 퍼센타일을 계산하는 데 사용한 2023 데이터셋의 공개 버전이 여기 있습니다. Kaggle에서 데이터를 사용하거나 CSV 형식으로 전체 데이터셋을 다운로드할 수 있습니다.\n\n# 데이터에 관한 주요 사항\n\n이 문제에 대해 처음 작업을 시작할 때, 2010년부터 2019년까지의 마라톤 샘플로 구성된 데이터셋으로 시작했습니다. 그 순간에는 명확한 결과를 고려하지 않고 데이터를 탐색했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n퍼센타일이 탐색할 수 있는 좋은 옵션이라고 결정한 후, 2023년 미국 내 모든 마라톤 대회를 포함한 새 데이터셋을 수집했어요.\n\n만약 데이터 분석 도구를 잘 다룬다면, 전체 데이터셋을 Kaggle에 업로드했어요: 2023 마라톤 결과. 또한 데이터를 불러오고 데이터를 간단히 탐색하는 샘플 노트북도 공유했어요.\n\n전체 데이터셋은 641개의 레이스와 42만 9천개가 넘는 개별 완주 시간을 포함하고 있어요.\n\n그러나 퍼센타일 테이블을 계산하는 데 사용한 데이터는 약간 더 작아요. 일부 결과에는 연령 데이터가 없어서 제외되었어요. 또한 LA 마라톤, CIM, 콜로라도와 같은 몇몇 레이스 데이터가 부족했고, 나중에 데이터셋에 추가되었어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 공유하는 분석은 대략 40만 건의 경주 결과를 기반으로 합니다. 추가 데이터를 수집한 후 90번째 백분위를 확인하여 유의미한 차이가 있는지 확인했고, 그 차이는 단 10초 정도였습니다.\n\n# 전통적인 연령 평가 방식은 어떻게 작동합니까?\n\n그렇다면 전통적인 연령 평가 방식은 무엇이며, 왜 우리가 가진 방법을 그대로 사용하지 않아야 하는지 알아야 할까요?\n\n전통적인 연령 평가 방법론은 지난 몇 10년 동안 발전해 왔습니다. 하지만 핵심적으로 이것이 어떻게 작동하는지 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n신뢰도 있는 경험적 관찰과 통계 분석을 통해 세계 마스터스 애슬레틱스는 사람이 나이를 먹을수록 얼마나 느려지는지를 추정하는 표를 만들었습니다. 선수의 시간을 나이 계수로 곱하면 나이에 비해 얼마나 빠른지를 알 수 있는 나이 등급 시간이 나옵니다 - 더 어려운 선수일 때의 동등한 시간입니다.\n\n따라서, 60세 남성의 원래 예시에서 나이 계수는 0.8331입니다. 만약 그가 3시간(10,800초) 동안 경주를 한다면, 나이 등급 시간은 2:29:58이 될 것입니다 (0.8331 * 10,800).\n\n25세 남성의 경우, 3시간 결과는 좋지만 놀라운 것은 아닙니다. 그러나 60세 남성의 경우, 이는 25세 남성에게 2:30 마라톤에 해당한다고 여겨집니다 - 정말 놀라운 것입니다.\n\n여기서 멈춰서 시간을 비교할 수도 있습니다. 또는 마지막 단계로 넘어가서 나이 등급 시간을 그 이벤트의 표준 시간으로 나눌 수도 있습니다 - 일반적으로 표가 작성된 당시 오픈 세계 기록입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n남자 마라톤의 이전 세계 신기록 2:01:39를 나누면 결과물인 2:29:58의 점수는 81.12%가 됩니다. 수여를 위해 나이 그레이딩 결과를 제공하는 경우 레이스 결과에서 일반적으로 볼 수 있는 것이죠. 이것은 성적 등급 퍼센트(PPL)라고도 불리워요.\n\n# 그러면, 이 방법에는 무엇이 문제인가요?\n\n저는 전통적인 나이 그레이딩 시스템에 몇 가지 비판과 문제점이 있지만, 가장 중요한 것은 두 가지로 요약할 수 있다고 생각해요: a) 교정과 b) 유용성.\n\n## 교정에 대한 명확한 문제점들이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 두 문제 중에서 더 문제가 되는 것은 때로는 나이 등급 계산이 신뢰할 수 없다는 점이며, 일부 연령군이 다른 연령군보다 좋은 점수를 획들기 쉽다는 것입니다.\n\n이 시리즈의 이전 기사에서는 2023년에 나이 등급별 상위 1,000명을 조사하여 나이와 성별별 분석을 살펴봤습니다. 여기 몇 가지 데이터 점:\n\n- 35세 미만 남성은 전체 샘플에서 1,000명 중 221명을 대표했지만, 나이 등급별 상위 1,000명 중 368명을 획득했습니다.\n- 45~59세 남성은 샘플에서 1,000명 중 65명을 대표했지만, 나이 등급별 상위 1,000명 중 39명만을 획득했습니다.\n- 60~64세 여성은 샘플에서 1,000명 중 14명을 대표했지만, 나이 등급별 상위 1,000명 중 27명을 획득했습니다.\n- 65~69세 여성은 샘플에서 1,000명 중 단 6명이었지만, 나이 등급별 상위 1,000명 중 20명을 획득했습니다.\n\n나이 등급이 실제로 경기장을 평평하게 만들고 다른 연령군과 성별을 비교할 수 있는 신뢰할 수 있는 방법을 제공한다면, 상위 1,000명의 분포는 일반 러너들의 분포와 유사해야 합니다. 그러나 일부 뚜렷한 불일치가 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBoston Marathon 예선 시간 분석에서 나타난 이 문제의 또 다른 예는 남성의 경우입니다.\n\n35세 미만 남성의 예선 시간은 가장 높은 연령 등급 결과 중 하나가 필요하며, 질 충전 비율도 가장 낮다는 것은 예상할 수 있습니다. 이 부분은 이해됩니다.\n\n그러나 여성의 경우에는 명백한 문제가 있습니다. 50대 여성의 예선 시간은 20대, 30대 또는 40대 여성의 시간보다 높은 연령 등급 결과가 필요합니다. 이러한 시간이 동등하게 어려울 경우, 50대 여성 중 예선을 통과하는 비율이 더 낮을 것으로 예상됩니다.\n\n그러나 실제로는, 그들이 더 높은 비율로 통과합니다. 45-49세 여성만 예외입니다. 60대 여성은 연령 등급으로 볼 때 비슷하게 엄격한 기준이 있으며, 그들은 더 높은 합격률을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만일 연령 등급 시스템이 적절하게 보정되었다면, 자격 요건 시간의 상대적 연령 등급이 서로 다른 연령 그룹의 자격 달성률을 대략적으로 반영할 것으로 예상됩니다. 그러나 이 두 가지 사이에 일치하지 않는 것은 어떤 부분이 문제가 있다는 신호입니다.\n\n이에 대한 가능한 이유들은 여러가지가 있지만, 제 추측으로는 a) 더 어린 연령 그룹의 엘리트 경쟁, b) 일부 더 늙은 연령 그룹의 이상값 결과, 그리고 c) 특히 여성들의 일부 더 늙은 연령 그룹 규모가 작기 때문에 일어난 결과일 것이라고 생각합니다.\n\n## 연령 등급에 의해 제공되는 유용한 정보는 무엇인가요?\n\n두 번째 문제는 — 여러분의 의견은 달라질 수 있습니다 — 연령 등급이 제공하는 정보가 얼마나 유용한지입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n나이 그레이드의 특성은 100%에 가까워질수록 시간의 차이가 적어져도 나이 그레이드에 더 큰 차이가 난다는 것이죠.\n\n예를 들어, 50세 남성이 2시간 35분의 마라톤을 뛴다고 가정해봅시다. 멋진 성과네요. 86.45%의 나이 그레이드를 받습니다. 다섯 분을 줄이면, 2시간 30분의 결과는 89.33%가 됩니다.\n\n그러나, 중간부로 갈수록 시간의 큰 차이가 있어도 나이 그레이드에서는 작은 차이를 보입니다. 만약 50세 남성이 4시간의 레이스를 뛴다면, 평균 이상의 성과입니다. 55.83%의 나이 그레이드를 받습니다.\n\n다섯 분을 줄이면, 3시간 55분은 약간 더 좋아집니다. 57.02%에 해당합니다. 그가 나이 그레이드를 3% 더 올리려면 거의 15분이 더 달려야 할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 경우 테이블 태그를 Markdown 형식으로 변경하십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서 대중들과 비교하는 대신 나 자신을 그들과 비교하게 되는 이유가 뭘까요?\n\n# 대안은 무엇일까요? 백분위수.\n\n제가 제안하는 대안은 특정 연령 및 성별 그룹의 다른 모든 러너와의 비교 결과를 기반으로 하는 것입니다.\n\n러너들의 분포를 가지고 — 예를 들어 50-54세 남성 — 당신은 모든 결과를 정렬하여 90%의 다른 러너보다 빠른 시간이 얼마나 걸릴지, 또는 75%, 또는 50%, 등을 볼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 연령 그룹에서의 시간 분포는 유사합니다. 탁월한 성적을 낸 소수의 러너들, 평균 주변에 처한 러너들이 점차 증가하며, 훨씬 더 느린 시간에 도달하는 소수의 러너들이 있습니다.\n\n![이미지](/assets/img/2024-06-19-IsThereaBetterSystemforAgeGradingMarathonResults_1.png)\n\n러너들이 나이를 먹을수록 분포가 약간 오른쪽으로 이동하지만, 일반적인 모양은 동일합니다. 충분히 많은 수의 러너들을 대상으로 고려하면, 그들이 정말 훌륭한 성적, 좋은 성적, 평균적인 성적, 혹은 평균 이하 성적으로 크게 분할될 것으로 예상됩니다.\n\n따라서 50세의 남성이 50세의 다른 남성 중 90%보다 빠르게(3시간 18분 8초) 달리면, 90%의 동료들을 이기는 40세 여성(3시간 32분 22초)과 거의 동등한 성적을 거둡니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최적 시간에 의존하는 대신 나이 관련 감소가 그 시간의 감소로 나타나는 것을 기다릴 필요가 없습니다. 대신 백분율을 사용하면 전체 시간 분포를 기반으로 하여 모든 참가자와 비교할 수 있습니다.\n\n# 백분율은 나이 측정에 실패한 곳에서 동작합니까?\n\n그렇다면 위에서 언급한 두 가지 불만족에 대해 백분율은 어떻게 대처할까요?\n\n백분율이 전통적인 나이 측정보다 더 정확하게 보정되는가요? 대부분의 러너들에게 보다 유용한 정보를 제공하나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 백분위수가 더 잘 보정되나요?\n\n한 마디로?\n\n네.\n\n위의 시각화는 세 가지를 보여줍니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 1,000명당 각 연령/성별 그룹의 완주자 수 (파란색)\n- 나이 등급에 따른 각 연령/성별 그룹의 상위 1,000명 완주자 수 (보라색)\n- 백분위로 나눈 각 연령/성별 그룹의 상위 1,000명 완주자 수 (분홍색)\n\n잘 보정된 측정은 파란색 막대와 유사해야하며, 잘 보정되지 않은 경우에는 종종 차이가 날 것입니다.\n\n파란색 막대와 보라색 막대 사이에 명백한 불일치가 있습니다.\n\n여성들 중에서, 나이가 어린 연령 그룹(35-49세)은 매우 소외되어 있고 노인 연령 그룹(60-79세)은 매우 과대표시되어 있습니다. 남성들 중에서는, 35세 미만 그룹이 지나치게 과대표시되어 있고, 35-49세 그룹은 약간 소외되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파란 막대와 분홍 막대가 항상 동일하지는 않지만, 그들 사이의 차이는 훨씬 적습니다.\n\n각 측정 항목의 총 분산을 더하면, 백분위수는 117의 총 분산을 가지고 있고, 연령 등급은 그것의 세 배가 넘는 383의 총 분산을 가지고 있습니다.\n\n만약 연령 등급의 목표가 최상위 완주자가 어떤 성별과 연령 그룹에서든 동일하게 나올 가능성이 있는 보다 공정한 경기 환경을 만드는 것이라면, 백분위수를 사용하는 것이 이미 존재하는 연령 등급 시스템을 사용하는 것보다 더 적절하게 보정되어 있습니다.\n\n## 백분위수는 더 유용한 정보를 제공할까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n의견은 각양각색일 수 있지만, 저는 이렇게 생각해요.\n\n제 개인적인 진척 상황을 예시로 들어볼게요.\n\n37세 때, 저는 3시간 35분으로 마라톤을 뛰었어요. 38세 때, 3시간 20분으로 뛰었고요. 39세 때, 3시간 10분을 달성했어요. 그리고 40세 때, 3시간 08분을 뛰었죠. 실제로 10월에 시카고에서 3시간을 넘어설 것이라고 가정해봅시다 (제 생각에는 상당히 가능하다고 생각해요).\n\n나이 그레이딩으로 따지면, 제 진척은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 56.58% - 61.15% - 64.83% - 66.00% - 69.23%\n\n그리고 백분위로 따지면 제 발전은:\n\n- 71.53% - 80.34% - 85.20% - 88.13% - 92.53%\n\n나이 그레이딩을 고려하면 이런 개선은 꽤 중요하지 않아 보일 수도 있어요. 56.58%는 꽤 평균적으로 들릴 수 있어요 — 하지만 37세 남성의 중간 완주시간(4:06)을 월등히 뛰어넘는 3:35에요. 그리고 69.23%는 그다지 더 인상적으로 들리진 않을 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n백분위수는 저와 동료들 간의 성과를 더 잘 이해할 수 있게 해주고, 시간이 지남에 따라 어떻게 변화하는지 보여줍니다. 제 초기 시간(3:35)은 평균보다 우수했지만 놀라울 정도는 아니었습니다. 한편, 10월의 목표 시간은 평균 마라톤러와 비교했을 때 상당히 좋지만(비켈레와의 격차가 많이 남은 것은 사실입니다).\n\n정말 중요한 질문은 이겁니다: 최고를 대비하려는 건가요? 아니면 동료들과 비교하려는 건가요?\n\n제가 최고와 정당하게 경쟁할 가능성이 거의 없다면, 나 자신을 동료들과 비교하는 것이 훨씬 더 합리적으로 보입니다. 그리고 그를 위해 백분위수가 더 유용한 도구인 것 같습니다.\n\n# 백분위수가 완벽한가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n안녕하세요!\n\n표를 마크다운 형식으로 변경해주세요.\n\n고맙습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다른 문제는 연장된 연령 그룹에는 충분히 많은 러너가 없어 신뢰할 수 있는 분포를 만들어내기 어렵다는 것입니다. 70대 여성과 70대 후반 / 80대 남성을 대상으로 하면 문제가 발생하기 시작합니다.\n\n이를 개선하기 위해 몇 가지 수학 모델링을 통해 분포를 완화시킬 수 있습니다. 하지만 이러한 러너들을 위한 더 신뢰할 수 있는 데이터셋을 구축하기 위해 더 많은 데이터가 필요합니다.\n\n아마도 2024년 말까지 또 다른 한 해 동안의 데이터가 추가된다면, 작업을 업데이트하고 일부 개선을 할 수 있을 것입니다.\n\n# 이 도구를 직접 활용하는 방법은 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전에 독자들이 마라톤 성적을 백분위로 계산하고 전통적인 연령 점수와 비교할 수 있는 계산기를 만들었습니다.\n\n그 도구를 업데이트하여 새로운 2023 데이터에서 생성된 백분위 표를 반영했습니다.\n\n연령 점수 계산기는 여기에서 이용할 수 있습니다.\n\n또한 계산기를 개선했습니다. 빠르게 작동하고 오류가 발생하면 더 정확하게 실패합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금은 이것을 만들어서 내 웹사이트(Running with Rock)에 호스팅했어요. 당신이 러닝 웹사이트를 운영하고 계시고 본인만의 계산기 버전에 관심이 있다면 알려주세요. 실제로 관심이 있는 사람이 있다면 워드프레스 플러그인이나 유사한 것을 만들어볼 수 있을 거에요.\n\n# 다음은 뭐가 있을까요?\n\n지금까지 이 토끼굴을 끝까지 쫓아갔어요. 데이터를 몇 차례 반복하고 유용한 도구를 만들었어요.\n\n나는 이것이 나이 등급과 개인 러닝 퍼포먼스 레벨(PLPs)을 대체할 것이라는 환상은 없어도, 이것이 가치 있는 관점을 제공한다고 생각해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제부터, 내가 고민 중인 몇 가지 다른 질문들이 있어요. 아마도 다음 몇 주 안에 그에 대해 글을 쓸 것 같아요:\n\n- 이전에 보스턴 마라톤에서 날씨가 미치는 영향에 대한 기사를 썼어요. 이에 대한 후속 기사를 작성 중이고, 타임 예선자와 미예선자 간의 잠재적 차이를 살펴볼 계획이에요.\n- 성별에 따라 러너들의 분포가 시간에 따라 어떻게 변화하는지 살펴보고 있어요. 특히, 연령 그룹 간의 차이와 여성이 데이터에서 역사적으로 배제된 흔적이 여전히 있는지 궁금해요.\n- 나이 등급 및 백분위와 관련된 몇 가지 질문들도 있어요 — 그리고 이 시리즈와 별도로 각 주제를 더 자세히 탐구할 거에요.\n\n만약 이러한 질문들에 관심이 있다면 — 또는 마라톤에 관한 데이터 기반 이야기에 대해 다른 이야기를 원한다면, 이메일 업데이트를 구독해 주세요.\n\n저는 열렬한 러너이자 데이터 열정가에요. 제가 무엇을 하고 있는지 따라가는 방법은 다음과 같아요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 나의 훈련에 대해 알고 싶다면 Running with Rock을 팔로우하세요.\n- 마라톤 훈련 계획 선택에 관한 팁을 읽어보세요.\n- Strava에서 제 프로필을 염탐하세요.","ogImage":{"url":"/assets/img/2024-06-19-IsThereaBetterSystemforAgeGradingMarathonResults_0.png"},"coverImage":"/assets/img/2024-06-19-IsThereaBetterSystemforAgeGradingMarathonResults_0.png","tag":["Tech"],"readingTime":9},{"title":"더 나은 데이터 스토리텔링 탐구적 연구를 통해 시각물을 만드는 방법","description":"","date":"2024-06-19 01:31","slug":"2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch","content":"\n\n\n![Better Data Storytelling - Creating Visuals Through Exploratory Research](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_0.png)\n\n데이터 시각화와 스토리텔링을 통해 명확하고 간결한 이미지를 제시하면 의견을 전달하는 데 큰 가치가 있습니다.\n내닌 Yau가 그의 새 책(Visualize This, 2판)에서 강조한 것처럼, 데이터를 효율적이고 정확하게 제시하기 위해 항상 노력한다면 대부분의 경우 막대 차트를 사용해야 합니다. 대부분의 사람들이 막대 차트를 보고 비교적 쉽게 해석할 수 있습니다. 양적으로 매우 효과적입니다.\n\n하지만 어떻게 하면 당신의 데이터를 돋보이게 만들 수 있을까요? 춤추게 할 수 있을까요? 청중을 행동으로 이끄는 데 무엇을 할 수 있을까요?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어쩌면 좀 더 흥미로운 것을 찾아볼 필요가 있을지도 몰라요. 처음 단계로, 우리는 탐구적 연구를 수행하여 데이터를 다양한 방식으로 시각화하여 우리의 청중을 정말로 사로잡을 시각이나 시각 집합을 찾을 수 있습니다.\n\n탐구적 연구란 무엇일까요? 설명해 드리겠습니다. 또한 데이터 집합을 살아있게 만들기 위해 다양한 시각화를 사용하는 방법에 대한 유용한 예시도 제공해 드리겠습니다.\n\n# 탐구적 연구\n\n데이터 집합으로 이야기를 전할 수 있는 지점에 도달하려면 탐구적 연구 과정을 거쳐야 할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기에 자연적인 호기심이 담긴 기회가 있어요. 데이터에 관한 어떤 질문이 있나요? 데이터에 대해 알아보고 싶으신 점이 무엇인가요?\n\nCole Nussbaumer-Knaflic은 2015년에 출간된 대표적인 책인 'Storytelling With Data'에서 탐색 단계를 전복 속에서 진주를 찾는 것과 유사하다고 설명합니다. 100개의 전복 중에서 2개의 진주를 찾을 수 있다고 가정해보세요. 탐색 단계에서 성공하기 위한 핵심은 데이터를 여러 다양한 방법으로 바라보는 것입니다.\n\n그리고 Nathan Yau(2024)가 제시한 유용한 질문 목록이 있습니다. 몇 가지 유용한 질문 중 일부는 다음과 같아요:\n\n- 이 데이터는 무엇에 관한가요?\n- 시간이 지남에 따라 어떻게 변했나요?\n- 어떤 관련성이 있나요?\n- 무엇이 돋보이나요?\n- 이것이 정상인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n큰 데이터 집합인 유엔 난민고등판데사(UNHCR)의 글로벌 망명자 데이터 집합을 사용한 몇 가지 예시를 살펴봅시다.\n\n## 데이터셋\n\n유엔 난민고등판데사(UNHCR)는 전 세계 난민 이동에 대한 통계를 추적합니다.\n\n그 데이터는 [여기](링크)에서 자유롭게 액세스할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n링크를 클릭해서 다운로드 페이지로 이동한 후에, 선택하는 데이터를 자세히 살펴볼 수 있습니다:\n\n![다운로드 이미지](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_1.png)\n\n이 프로젝트에서는 각 난민의 출신 국가와 피난처 국가를 살펴봅시다:\n\n- 출신 국가에서 — 피난민이 이주하는 국가\n- 피난처 국가에서 — 피난민이 출신하는 국가\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터셋을 다운로드한 후에는 스프레드시트 형식으로 열어서 어떤 내용을 다루고 있는지 살펴볼 수 있어요:\n\n![Spreadsheet](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_2.png)\n\n이 프로젝트에서 주로 관심 있는 데이터 필드는 다음과 같아요:\n\n- 출신 국가 (3자리 ISO 코드 포함) - 망명을 찾는 사람이 어디서 왔는지\n- 망명 국가 (3자리 ISO 코드 포함) - 실제로 망명을 찾으려는 사람이 있는 곳\n- 인정된 결정 - 망명을 찾는 사람이 수용되었는지 여부 (국가별 숫자 총계)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n원산지와 망명국 두 곳은 신뢰할 수 있는 고유 식별을 위해 사용할 수 있는 3자리 ISO 코드를 갖고 있어요.\n\n# 어떤 이야기들을 할 수 있을까요?\n\n데이터셋이 어떤 내용을 포함하고 있는지 아는 것으로 우리는 다음 단계를 진행할 때 호기심을 안내할 수 있습니다.\n\n저는 캐나다인으로써, 캐나다로 오는 망명 신청자에 관심이 있어요. 반면 캐나다에서 떠나 다른 곳으로 망명을 찾는 사람들보다는 그 수가 많지 않아요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마음에 떠오르는 질문들:\n\n- 사람들이 어디서 왔는지?\n- 캐나다가 난민을 받아들이는 면에서 다른 나라들과 어떻게 비교되는가?\n- 그들의 출신지가 시간이 지남에 따라 변했는가?\n- 총 인원이 시간이 지남에 따라 변했는가?\n\n이러한 질문에 대답하기 위해 가장 매력적인 방법으로 가능한 시각화 옵션들을 살펴볼 수 있습니다.\n\n한 번 더 말하지만, 2024년에 출간된 Dr. Yau의 Visualize This에서는 60가지 시각화 옵션이 제공되므로 선택할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_3.png\" /\u003e\n\n오늘의 탐색적 연습에서는 주황색으로 강조된 5가지 다양한 탐색 옵션을 살펴보겠습니다:\n\n- 막대 차트\n- 시계열 선 그래프\n- 쌓인 영역 차트\n- 등치지도\n- 선채우기 차트 (원형 차트처럼 비율에 따라 크기가 조절되지만 두 가지 수준이 있음)\n\n본문에 포함된 모든 5가지 예시는 Python Plotly 라이브러리를 사용하여 생성되었습니다(Github에서 모든 파일을 확인할 수 있습니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시작해봅시다!\n\n# 탐사 1. 막대 차트\n\n막대 차트는 오늘날 가장 인기 있는 데이터 시각화 중 하나입니다. 데이터를 처음 탐색할 때 좋은 시작점이 됩니다.\n\n![Bar Chart](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 막대 차트를 통해 수용된 망명자 수가 2021년까지 상승 추세였으며 (2020년 코로나19 예외), 그 이후로는 하락 추세임을 알 수 있습니다. 이 차트는 캐나다로의 망명자의 숫자를 매우 명확하게 보여줍니다.\n\n좋아요, 좋아요, 이제 캐나다에서 망명을 찾는 사람들의 구성에 대해 어떻게 생각하세요? 그들은 어디에서 오고 있나요?\n\n우리는 데이터 집합의 범위 (2015–2023)에 대한 숫자를 보여주는 막대 차트로 시작할 수 있습니다:\n\n![차트](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가로 막대 차트를 사용하면 각 국가를 쉽게 읽고 서로 비교할 수 있어요. 멋지네요!\n\n하지만 만약 이 기간 동안 캐나다를 전 세계와 비교하려면 어떻게 해야 할까요? 막대 차트로는 그것을 보여주기가 어려울 수 있어요. 시계열 차트가 더 나을 수도 있으니 한 번 시도해보죠.\n\n# 2번째 탐색. 시계열 선 그래프\n\n먼저, 이전 예시의 막대 차트를 선 그래프로 변환해봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 우리가 캐나다만을 대표하는 선 그래프로 연도별 신청자 총 수를 보여준다면:\n\n![line chart](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_6.png)\n\n우리는 코로나 이전에 상승세를 보이다가 코로나 기간 동안 짧은 하락세를 보이는 것을 확인할 수 있습니다 (캐나다는 매우 엄격한 입국 규정을 가지고 있었습니다). 그러나 이 시기 동안 캐나다는 다른 나라들과 어떻게 비교되는 걸까요?\n\n데이터 시각화의 최상의 방법을 활용하여 데이터 세트의 나머지 국가들을 (연한 회색으로) 추가할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Better Data Storytelling](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_7.png)\n\n이제 캐나다를 주황색으로 표시하고 다른 국가들을 연한 회색으로 표현하면, 캐나다의 다른 나라들에 비한 위치를 알 수 있습니다.\n\n이 차트에서 우리는 많은 다른 국가들에 비해 캐나다가 실제로 많은 난민들을 받아들인다는 것을 볼 수 있습니다. 우리가 2016년에 볼 수 있는 큰 증가는 그때 독일이 많은 수의 시리아 난민을 받아들였던 것입니다.\n\n멋져요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금은 캐나다가 전 세계와 비교되는 방법과 어디서 사람들이 왔는지에 대한 전체 숫자를 알게 되었습니다.\n\n하지만 캐나다의 경우 (출신 국가별로) 연간 변화를 추적하고 싶다면 어떨까요?\n\n여기서 좀 더 세부적으로 파고들어야 합니다. 스택된 면적 차트를 사용하면 시각적으로 이를 파악할 수 있습니다.\n\n# Exploration 3. 스택된 면적 차트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스택된 영역 차트는 시간이 지남에 따른 체적 변화를 살펴보는 뛰어난 방법입니다. 우리 데이터셋의 경우, 체적 변화는 캐나다로 매년 망명을 찾는 각 출신 국가의 사람 수일 것입니다.\n\n![영역 차트](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_8.png)\n\n이 특정 차트는 캐나다로 이주하는 사람들이 어디에서 망명을 찾고 있는지를 보여줍니다. 사용자가 세부사항에 빠져들지 않도록 상위 10개 국가로 좁혀졌습니다.\n\n자세히 살펴보면, 주황색 영역은 캐나다에서 망명을 찾는 이란 사람들의 많은 수를 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n적절한 데이터로 스택된 영역 차트는 시간에 따른 변화에 대해 확실한 시각적 효과를 제공할 수 있습니다.\n\n이 특정 차트는 Python Plotly express 라이브러리를 사용하여 생성되었습니다. 이 라이브러리에는 이 스타일의 차트를 생성하는 area()라는 내장 함수가 있습니다.\n\n이제 이 3가지 훌륭한 시각화를 통해도, 우리가 아직 부족한 것은 캐나다로의 국가별 양민 신청자의 명확한 전체적 인식을 제공해주는 유용한 시각화입니다.\n\n이를 위해 우리에게 제공하는 유용한 표현은 코로플레스 지도입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 4. 코로플레스 맵 고찰\n\n코로플레스 맵은 지리적 영역(예: 국가별) 전체의 데이터 변화를 보여주기 위해 음영 처리된 영역을 제공합니다.\n\n저희 데이터셋과 함께 코로플레스 맵을 사용하면 각 국가별 신청자 수에 대한 전체적인 시각화를 얻을 수 있습니다:\n\n![이미지](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_9.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 예에서, 더 어두운 색조와 음영은 캐나다로의 많은 수의 난민 신청자를 대표합니다. 이 예에서 대상 국가인 캐나다는 녹색 원(마치 \"녹색 빛\"처럼)으로 강조되어 있습니다.\n\n이전의 시각화(스택된 면적 차트)에서 2023년에 난민 신청을 한 이란 국민들의 수가 많다는 것을 알 수 있었습니다. 우리는 이 지도에서 이란이 가장 어두운 색인 것을 볼 수 있습니다. 이는 이전의 예시와 일치하는 멋진 부분입니다!\n\n추가 기능으로, 코로플레스 맵은 Streamlit(파이썬용)과 같은 현대적인 코딩 라이브러리와 결합하여, 년도를 선택할 수 있게 하는 상호작용성을 제공할 수 있습니다:\n\n![이미지](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파이썬 Plotly를 사용하여 생성된 코로플레스 맵으로 각 나라 위를 마우스 오버하여 추가 정보(즉, 정확한 숫자)를 확인할 수 있습니다.\n\n# 탐험 5. 썬버스트 차트\n\n썬버스트 차트는 데이터를 표현하는 아름다운 방법입니다. 다양한 형태의 데이터를 시각화하는 가장 효과적인 방법은 아니지만, 썬버스트 차트는 항상 눈길을 끕니다. 나는 그것들을 주시하고 항상 표현하고 있는 데이터 포인트가 무엇인지 궁금해합니다.\n\n우리 데이터셋에서 각 해의 각 나라에 대한 캐나다의 망명 신청 국가 데이터를 나타낼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Better Data Storytelling: Creating Visuals Through Exploratory Research](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_11.png)\n\n매우 다채롭고 인상적이지만 동시에 매우 복잡합니다. 이런 차트를 소화하는 데는 많은 시간이 걸릴 수 있습니다.\n\n하지만 Python Plotly와 같은 현대 데이터 시각화 도구의 큰 이점 중 하나는 사용자 상호작용을 허용한다는 점입니다. 이 시각화(다시 말해 Python Plotly로 만든 것)에서 각 연도를 클릭하여 데이터를 자세히 살펴볼 수 있습니다.\n\n예를 들어, 2023년을 클릭하면 해당 연도의 데이터만 볼 수 있습니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_12.png\" /\u003e\n\n그리고 특정 국가(예: 인도) 위로 마우스를 올리면 해당 국가에 대한 추가 데이터를 볼 수 있습니다. 매우 유용합니다.\n\n# 요약\n\n이 글의 목표는 독자들에게 데이터 집합에서 탐색 분석을 수행하는 다양한 차트 및 매핑 기술을 사용하는 방법에 대한 예시를 제공하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 과정을 통해 데이터에 대해 궁금한 점이 있으면 답변할 수 있습니다. 혹은 아직 질문이 없다면 질문을 세우는 데 도움을 줄 수도 있습니다.\n\n이 연습이 유용하고 즐거우셨기를 바랍니다.\n\n읽어 주셔서 감사합니다!\n\n참고: 이 글의 예시는 모두 Python 코드를 사용하여 (Plotly express 및 Streamlit 라이브러리를 활용하여) 작성되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 코드 파일과 CSV 파일은 GitHub 저장소에 있습니다. \n\n만약 이 유형의 이야기가 당신의 취향이고 작가로서 저를 지원하고 싶다면, 제 Substack를 구독해주세요.\n\nSubstack에서는 2주에 한 번 뉴스레터와 다른 플랫폼에서 찾을 수 없는 기사들을 게시합니다.","ogImage":{"url":"/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_0.png"},"coverImage":"/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_0.png","tag":["Tech"],"readingTime":8},{"title":"데이터에서 시각화까지 OpenAI Assistants API 및 GPT-4o와 함께","description":"","date":"2024-06-19 01:28","slug":"2024-06-19-FromDatatoVisualizationwiththeOpenAIAssistantsAPIandGPT-4o","content":"\n\nGPT-4의 능력이 계속해서 확장되면서, OpenAI의 기술을 기반으로 한 도구들은 개발자들에게 점차적으로 강력한 자산으로 발전하고 있습니다.\n\n본 글에서는 최신 버전의 차트 작성 기능을 탐구할 예정입니다. 데이터 파일과 구체적인 지침을 Assistant에 제공하여 우리의 데이터 시각화 아이디어를 구현하는 과정을 살펴볼 것입니다.\n\n이를 위해 Assistant API의 내장 도구들을 활용할 것입니다.\n\n지금 당장 OpenAI Python 패키지(v1.30.0, 작성 시점 기준)에는 Assistants API 안에 File Search, Code Completion 및 Function Calling 도구가 포함되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기능 호출을 통해 개발자는 작업을 완료하기 위해 AI가 지능적으로 선택할 수 있는 함수를 정의할 수 있으며, 파일 검색을 통해 다양한 파일 유형을 업로드하고 벡터 데이터베이스에 RAG 스타일로 저장할 수 있습니다. 코드 완성은 보조 프로그램이 프로그래밍 및 수학 문제를 해결하기 위해 파이썬 프로그램을 작성하고 실행할 수 있는 격리된 환경에서 작동합니다.\n\n코드 완성은 업로드된 파일과 함께 사용할 수도 있습니다. 이 파일들은 데이터 파일과 차트 이미지를 생성하기 위해 처리될 수 있습니다. 그리고 바로 이 기능을 우리가 여기에서 사용할 것입니다.\n\n아래에서 탐구할 코드는 CSV 형식의 데이터 파일을 로드하고 적절한 프롬프트를 사용하여 데이터에서 그래프를 생성하도록 보조 프로그램에 지시합니다. 그런 다음 그래프를 다운로드하고 표시할 것입니다.\n\n간단한 영어 프롬프트를 사용하여 원시 CSV 데이터에서 아래와 같은 차트를 쉽게 생성할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# OpenAI Assistant\n\n이전에 OpenAI의 Assistant API에 대해 설명했고 이를 시작하는 방법을 설명했습니다(OpenAI의 강력한 새 Assistant API를 사용하여 데이터 분석). API의 새 버전들이 이 기사를 다소 구식으로 만들었지만, Assistant의 설명과 작동 방식은 여전히 대체로 정확하며 OpenAI 계정을 설정하는 방법 또한 그대로 유효합니다.\n\n그래서 더 자세히 살펴보려면 해당 기사를 참고하시고, 여기서는 핵심 사항에 대한 간략한 소개로 한정하겠습니다.\n\n# OpenAI\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저, 물론 OpenAI 계정이 필요하며 사용 시 요금이 부과될 것임을 알아야 합니다. 그러나 요금은 높지 않습니다: 여기서 설명할 코드를 실행하는 비용은 몇 센트만 들 것입니다. 파일 저장 등의 기타 요금이 부과될 수 있으나, 이는 해당 맥락에서는 관련성이 없을 수도 있지만 최신 요금을 확인해 보는 것이 좋습니다.\n\n그렇다면, OpenAI 대시보드를 정기적으로 사용하여 사용량을 확인하여 큰 청구서가 발생하지 않도록 해야합니다.\n\n OpenAI 어시스턴트의 모든 출력물과 업로드한 파일이 모두 저장되므로 사용중인 저장소도 확인해야 합니다. 대시보드에서 수동으로 삭제할 수 있으며, 여러분은 이렇게 해야 합니다. 왜냐하면 요금이 부과되지 않을 수 있지만, 시간이 지날수록 작업 공간에 불필요한 파일이 많이 축적될 수 있기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 어시스턴트, 스레드, 그리고 런\n\n어시스턴트 API에서 가장 기본적인 세 가지 객체입니다.\n\n- 어시스턴트: 이름에서 알 수 있듯이, 이 설정의 주요 부분입니다. 어시스턴트를 만들 때 우리는 모델 (예: gpt-4o), 모델에게 원하는 행동 유형에 대한 지시사항, 코드 해석기와 파일 검색과 같은 도구, 그리고 모델이 사용해야 하는 파일과 같은 다양한 속성을 지정합니다.\n- 스레드: 이들은 대화의 상태를 저장하고 사용자와 어시스턴트가 생성하는 메시지를 포함합니다. 런(아래 참조)이 시작될 때 스레드는 어시스턴트와 연결됩니다.\n- 런: 런은 스레드의 정보와 어시스턴트를 가져와 LLM(AI 모델)과의 상호 작용을 관리합니다. 완료되기 전에 런은 여러 단계를 거칩니다. 런이 완료되면, 어시스턴트가 만든 응답을 확인하기 위해 스레드를 조사할 수 있습니다.\n\n이 기본 객체들 외에도 스레드에서는 모델에 대한 지시와 해당 응답을 포함하는 메시지가 필요합니다. 또한 어시스턴트가 사용하는 분리된 객체인 파일을 사용하며, 업로드된 파일의 세부 정보를 저장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 비서 코딩하기\n\n우리의 비서를 만들고 실행하기 위해 몇 가지 단계를 거쳐야 합니다. 아래에 나열된 이벤트 순서는 각 구성 요소가 사용되는 방식에 대한 개요를 제공합니다.\n\n다음은 절차입니다:\n\n- API 키로 OpenAI 클라이언트를 생성합니다.\n- 로컬 파일을 업로드하고 나중에 사용할 파일 객체를 검색합니다.\n- 모델에 대한 지침과 업로드된 파일의 ID로 비서를 생성합니다.\n- 파일 ID 및 모델의 지침을 포함하는 스레드를 생성합니다.\n- 비서와 스레드를 실행합니다.\n- 이제 스레드에 있는 사용자 및 AI가 생성한 메시지를 표시합니다(모델이 결과물을 생성하는 과정을 보여주어야 하며, 문제가 발생한 경우 무엇이 문제인지 확인할 수 있습니다).\n- 생성된 이미지를 검색하여 표시합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 각 항목을 Python으로 코드화하고 정확히 무슨 일이 일어나고 있는지 설명하겠습니다. 저는 Jupyter 노트북 형식으로 코드를 작성했으므로, 함께 따라해보고 싶으시다면 각 코드 부분을 새로운 노트북 셀에 복사하여 제 노트북을 복제할 수 있습니다.\n\n첫 번째 단계는 클라이언트를 생성하는 것입니다.\n\n## 클라이언트 생성\n\n클라이언트는 OpenAI API에 접근할 수 있게 해줍니다. API 키를 제공해야 하는데, 아래 코드에서는 사용자가 키를 수동으로 입력해야 하도록 입력문을 포함시켰습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom openai import OpenAI\n\nkey = input(\"API 키\")\nclient = OpenAI(api_key=key)\n```\n\n또는 키를 하드코딩할 수도 있어요 (하지만 코드를 공개하지 않도록 주의하세요).\n\n```js\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"여기에 키를 입력하세요\")\n```\n\n또는 키가 환경 변수로 저장된 경우에는 클라이언트가 자동으로 찾아내기 때문에 코딩할 필요가 없습니다...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n```\n\n이 중 하나는 여러분의 첫 번째 주피터 노트북 셀입니다.\n\n## 파일 업로드\n\n먼저 파일이 필요합니다! 저는 Our World in Data(OWID) 웹사이트의 데이터를 기반으로 한 CSV 파일을 사용하고 있습니다. OWID는 정보와 데이터의 훌륭한 출처이며, 그들은 친절하게 모든 콘텐츠를 Creative Commons BY 라이센스 하에 자유롭게 사용할 수 있도록 허용하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파일은 1850년부터 2021년까지 전 세계 CO2 배출량을 기록한 것입니다 (원본 데이터에는 다른 항목의 데이터도 많이 포함되어 있지만, 여기에는 세계 데이터만 포함했습니다). 아래 스크린샷에서 파일이 어떻게 보이는지 확인할 수 있습니다.\n\n파일 이름은 world_df.csv로 지었고, 또한 보조 프로그램에 지정할 이름에 해당하는 변수를 설정하고 싶습니다. 따라서 두 값을 담은 변수를 두 번째 노트북 셀에 넣었습니다.\n\n```js\nfilename = \"world_df.csv\"\nassistant_name = \"data-analyst-v0.1\"\n```\n\n다른 파일을 읽거나 새 보조 프로그램을 만들기 위해 코드를 사용하려면 이 셀에서 값들을 변경할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 셀에 파일을 업로드합니다. 파일을 업로드하는 주요 작업은 client.files.create 메서드에 의해 수행됩니다. 아래 코드에서 이 메서드는 파일 자체와 파일의 목적을 나타내는 'assistants' 문자열 두 가지 매개변수를 사용합니다.\n\n이 코드는 파일을 업로드하는 작업 이상을 수행합니다. 코드가 한 번 이상 실행될 것이므로(다른 지시사항과 함께 실행될 수도 있음) 파일을 중복으로 업로드하고 싶지 않습니다. 따라서 파일이 새로운 경우 코드는 파일을 업로드하지만, 이미 업로드된 경우 코드는 해당 기존 파일을 검색합니다.\n\n```js\n# 파일이 이미 업로드되었는지 확인\nfilelist = client.files.list(purpose=\"assistants\")\nfilenames = [x.filename for x in filelist.data]\n\n# \"assistants\" 목적의 파일을 업로드하거나 기존 파일 사용\nif not filename in filenames:\n  file = client.files.create(\n    file=open(filename, \"rb\"),\n    purpose='assistants'\n  )\nelse:\n  for f in filelist:\n    if f.filename == filename:\n      file = client.files.retrieve(f.id)\n      break\n```\n\n이미 업로드된 파일 목록을 다운로드하여 파일이 있는지 확인할 수 있습니다. client.files.list() 메서드는 서버에서 목록을 검색하며, parameterpurpose='assistants'를 전달하여 관심 있는 파일 유형을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼 관심 있는 파일 이름을 찾기 위해 목록을 스캔할 수 있어요. 없다면 업로드하고, 그렇지 않으면 클라이언트에서 파일 객체를 가져와요. 어느 쪽이든, 파일은 파일 객체로 설정됩니다.\n\n앱에서는 이 코드를 파일 객체를 반환하는 함수에 유용하게 배치할 수 있어요.\n\n이제 파일이 업로드되고 파일 객체의 레코드가 생성되었어요. 다음으로, 이 파일을 사용할 보조를 만들어야 해요.\n\n## 보조 만들기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n업로드한 파일과 마찬가지로 이미 존재하는지 확인합니다. 이 코드를 이전에 실행했다면, 도우미가 이미 만들어졌을 것이고, 중복으로 생성하고 싶지 않으므로 기존의 도우미 객체를 가져옵니다. 그렇지 않다면 새로운 객체를 생성합니다.\n\n이 기능에 대한 코드는 파일 업로드에 사용한 것과 거의 동일합니다.\n\n도우미를 생성하는 것은 client.beta.assistants.create()를 호출하여 수행됩니다.\n\n도우미의 이름, 일부 기본 지침(시스템 프롬프트가 될 것입니다), 사용할 모델(이 경우 GPT-4o), 요청하는 도구(코드 해석기) 및 리소스에 대한 매개변수를 설정합니다. 이 마지막 매개변수에서는 업로드한 파일의 파일 객체를 참조하고, 코드 해석기가 파일을 사용할 것임을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\n# 조수가 이미 존재하는지 확인합니다\nassistant_list = client.beta.assistants.list()\nassistant_names =  [x.name for x in assistant_list.data]\n\nif not assistant_name in assistant_names:\n  # 파일 ID를 사용하여 조수를 생성합니다\n  assistant = client.beta.assistants.create(\n    name = \"data-analyst-v0.1\",\n    instructions=\"You are a data analyst\",\n    model=\"gpt-4o\",\n    tools=[{\"type\": \"code_interpreter\"}],\n    tool_resources={\n      \"code_interpreter\": {\n        \"file_ids\": [file.id]\n      }\n    }\n  )\nelse:\n    for a in assistant_list:\n      if a.name == assistant_name:\n        assistant = client.beta.assistants.retrieve(a.id)\n        break\r\n```\n\n다시 말해서, 앱에서 이것은 조수 객체를 반환하는 함수일 수 있습니다.\n\n## 스레드 생성\n\n스레드를 만들려면 단순히 client.beta.threads.create()를 호출하고, 이 스레드를 사용하여 조수가 실행될 때 조수에 전달되는 첫 번째 메시지를 지정하면 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 코드에서 볼 수 있듯이, 메시지에서는 역할을 설정하고 프롬프트를 설정하며 파일 ID를 첨부합니다.\n\n```js\nthread = client.beta.threads.create(\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": \"첨부된 csv 파일을 사용하여 'Year'에 대한 '연간 이산화탄소 배출' 그래프를 표시하십시오\",\n      \"attachments\": [\n        {\n          \"file_id\": file.id,\n          \"tools\": [{\"type\": \"code_interpreter\"}]\n        }\n      ]\n    }\n  ]\n)\n```\n\nLLM으로 전송하는 프롬프트는 다음과 같습니다:\n\n“첨부된 csv 파일을 사용하여 'Year'에 대한 '연간 이산화탄소 배출' 그래프를 표시하십시오”.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그것은 상당히 간단한 요구 사항이에요. 코드 해석기가 데이터 파일을 분석하고 필요한 코드를 생성해야 해요.\n\n이제 우리는 쓰레드를 사용하여 어시스턴트를 실행할 준비가 모두 끝났어요.\n\n## 실행 만들기\n\n실행은 어시스턴트와 쓰레드를 가져와 LLM에 제출해요. 비동기로 실행되며 완료되기 전에 여러 단계를 거쳐 가요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과를 기다리기 위해서는 두 가지 방법을 사용할 수 있는데요: 폴링 또는 스트리밍 방식이 있습니다. 폴링은 실행 상태가 완료될 때까지 반복적으로 확인하는 방식이에요. 반면에 스트리밍은 다양한 단계가 자동으로 감지되며 함수가 해당 이벤트에 매핑될 수 있는 이벤트 핸들러에 반응하는 방식입니다.\n\n아래는 OpenAI 문서에서 제공하는 스트리밍 코드입니다 (메시지가 변경되었습니다).\n\n```js\nfrom typing_extensions import override\nfrom openai import AssistantEventHandler\n\n# 먼저, 이벤트 핸들러 클래스를 생성하여\n# 응답 스트림에서 이벤트를 처리하는 방법을 정의합니다.\n\nclass EventHandler(AssistantEventHandler):\n  @override\n  def on_text_created(self, text) -\u003e None:\n    print(f\"\\nassistant \u003e {text.value}\", end=\"\", flush=True)\n\n  @override\n  def on_text_delta(self, delta, snapshot):\n    print(delta.value, end=\"\", flush=True)\n\n  def on_tool_call_created(self, tool_call):\n    print(f\"\\nassistant \u003e {tool_call.type}\\n\", flush=True)\n\n  def on_tool_call_delta(self, delta, snapshot):\n    if delta.type == 'code_interpreter':\n      if delta.code_interpreter.input:\n        print(delta.code_interpreter.input, end=\"\", flush=True)\n      if delta.code_interpreter.outputs:\n        print(f\"\\n\\noutput \u003e\", flush=True)\n        for output in delta.code_interpreter.outputs:\n          if output.type == \"logs\":\n            print(f\"\\n{output.logs}\", flush=True)\n\n# 그런 다음, 이벤트 핸들러 클래스를 사용하여\n# `stream` SDK 도우미와 함께 Run을 생성하고\n# 응답을 스트리밍합니다.\n\nwith client.beta.threads.runs.stream(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n  instructions=\"그래프에 대한 다운로드 가능한 파일을 생성하세요\",\n  event_handler=EventHandler(),\n) as stream:\n  stream.until_done()\n```\n\n이번 실행을 시작하는 함수는 client.bet.threads.run.stream()이며, 이 함수에는 이번 실행 및 이벤트 핸들러와 같은 특정 실행을 위한 지침을 포함하여 스레드와 어시스턴트의 ID가 전달됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이벤트 핸들러의 작동 방식에 대해서는 자세히 다루지 않겠습니다. 단순히 말하자면 텍스트가 생성되거나 도구가 출력되는 이벤트를 잡아내고 결과를 출력합니다. 이 기능은 실험적인 목적으로 충분하지만 실제 앱을 위해서는 이러한 출력물에 대해 더 정교한 작업을 원할 수도 있습니다.\n\n스레드에서 그래프를 생성하길 원한다고 명시했으며, 여기 실행에서 다운로드 가능한 파일을 생성하도록 요청했습니다.\n\n실행 결과는 아래와 같이 나와 있으며, 주로 어시스턴트에 의해 생성된 Python 코드로 구성되어 있습니다.\n\n```js\nassistant \u003e code_interpreter\n\nimport pandas as pd\n\n# CSV 파일 불러오기\nfile_path = '/mnt/data/file-8XwqMOlaH6hoKEEKOYXPYqTh'\ndata = pd.read_csv(file_path)\n\n# 데이터 프레임의 처음 몇 행을 표시하여 구조를 이해합니다\ndata.head()\n\nimport matplotlib.pyplot as plt\n\n# 'Year' 대 'Annual CO₂ emissions' 그래프 그리기\nplt.figure(figsize=(10, 6))\nplt.plot(data['Year'], data['Annual CO₂ emissions'], marker='o', linestyle='-')\nplt.xlabel('Year')\nplt.ylabel('Annual CO₂ emissions')\nplt.title('Year vs Annual CO₂ emissions')\nplt.grid(True)\nplt.tight_layout()\n\n# 그래프를 파일로 저장\nplot_file_path = '/mnt/data/year_vs_annual_co2_emissions.png'\nplt.savefig(plot_file_path)\nplot_file_path\n\noutput \u003e\n\nassistant \u003e 'Year' 대 'Annual CO₂ emissions'를 나타내는 그래프가 생성되었습니다. 아래 링크를 통해 플롯을 다운로드할 수 있습니다:\n\n[그래프 다운로드](sandbox:/mnt/data/year_vs_annual_co2_emissions.png)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 코드는 주피터 노트북에 포함되어서는 안 됩니다. GPT가 생성하고 실행한 것입니다.\n\n이 출력 결과는 LLM이 우리의 지시를 이해했고 올바른 그래프를 생성하는 코드를 생성하고 실행하여 이미지 파일을 만들었다는 것을 보여줍니다.\n\n## 생성된 파일 다운로드\n\n이제 우리가해야 할 일은 어시스턴트가 생성한 파일을 찾아 내려받는 것뿐입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 노트북의 마지막 코드 셀이 표시됩니다.\n\n```js\nfilelist = client.files.list(purpose=\"assistants_output\")\n\nimage_list = [x for x in filelist.data if \"png\" in x.filename]\n\nid = image_list[-1].id  # 리스트의 마지막은 최신 파일입니다.\n\nimage_data = client.files.content(id)\nimage_data_bytes = image_data.read()\n\nwith open(\"./my-image.png\", \"wb\") as file:\n    file.write(image_data_bytes)\n```\n\n우리는 이 코드가 이미 실행되었을 수 있다고 가정하므로 이미지 파일이 하나 이상 있을 수 있습니다.\n\n그래서 먼저 \"assistants_output\"로 레이블이 지정된 모든 파일의 목록을 가져온 다음, 이미지 목록(즉, 확장자가 '.png'인 파일)을 만들고 해당 목록에서 마지막 파일을 찾습니다. 그게 바로 생성된 마지막 파일입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 차트를 표시하기 위해 다음 내용을 포함한 마크다운 셀을 만들 수 있습니다.\n\n```js\n![](my-image.png)\n```\n\n아래 이미지에서 결과물을 확인할 수 있습니다:\n\n![이미지](/assets/img/2024-06-19-FromDatatoVisualizationwiththeOpenAIAssistantsAPIandGPT-4o_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# **프롬프트 변경**\n\n다른 차트를 생성하려면 단순히 다른 프롬프트로 코드를 다시 실행하면 됩니다. 예를 들어:\n\n“첨부된 CSV 파일을 사용하여 '연도'에 대한 그래프를 모든 다른 열과 함께 표시하십시오.”\n\n결과는 아래 이미지와 같습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 내용을 읽고 다른 문서에서 이 내용을 인용하실 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nassistant \u003e code_interpreter\n\nimport pandas as pd\n\n# CSV 파일 로드\nfile_path = '/mnt/data/file-8XwqMOlaH6hoKEEKOYXPYqTh'\ndata = pd.read_csv(file_path)\n\n# 데이터프레임의 처음 몇 행을 표시하여 구조를 이해합니다.\ndata.head()\n\n# 데이터를 21세기 이후(2000년 이후)로 필터링합니다.\ndata_21st_century = data[data['Year'] \u003e= 2000]\n\n# 'Year' 대 'Year'를 기준으로 플롯할 열을 정의합니다.\ncolumns_to_plot = data.columns.drop(['Entity', 'Code', 'Year'])\n\n# 데이터 플롯\nplt.figure(figsize=(12, 8))\nfor column in columns_to_plot:\n    plt.plot(data_21st_century['Year'], data_21st_century[column], label=column)\n\nplt.title('21세기 연간 데이터')\nplt.xlabel('년도')\nplt.ylabel('값')\nplt.legend(title='메트릭', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\nplt.tight_layout()\n\n# 플롯을 파일로 저장\nplot_file_path = '/mnt/data/my-image3.png'\nplt.savefig(plot_file_path)\n\nplt.show()\n\nplot_file_path\n\noutput \u003e\n\nassistant \u003e 그래프가 성공적으로 생성되었고 저장되었습니다. 아래 링크를 사용하여 파일을 다운로드할 수 있습니다:\n\n[그래프 다운로드](sandbox:/mnt/data/my-image3.png)None\n```\n\n이를 통해 코드 해석기가 데이터를 필터링하여 원하는 차트를 생성했음을 알 수 있습니다.\n\n아래 차트를 확인할 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-FromDatatoVisualizationwiththeOpenAIAssistantsAPIandGPT-4o_3.png\" /\u003e\n\n# 결론 및 앱 방향으로\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오픈AI의 Assistants API와 코드 인터프리터를 사용하면 일반 영어로 데이터 파일에서 차트를 생성할 수 있는 코드를 생성할 수 있습니다.\n\n이 코드는 특별히 어렵지 않으며 Jupyter Notebook 코드는 단순히 데모용일 뿐입니다. 그러나 이를 쉽게 응용하여 사용자로부터 데이터 파일을 업로드하고 필요한 차트를 설명하는 프롬프트를 입력하도록 요청하고 사용자가 그 차트를 이미지 파일로 다운로드할 수 있는 앱으로 확장할 수 있을 것입니다.\n\n## 업데이트: 프로토타입 앱\n\nGitHub 리포지토리(아래 참조)의 코드를 기반으로 한 Streamlit 앱 프로토타입인 'streamlit' 폴더가 있습니다. 이 앱을 사용하려면 API 키를 제공하고 Streamlit 비밀 파일에 넣어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n앱은 Streamlit 파일 업로드 컨트롤을 사용하여 CSV 파일을 업로드하고 작업하기 위해 프롬프트를 입력할 수 있는 입력 상자가 제공됩니다. 프롬프트가 실행 중일 때 상태 문자열이 표시됩니다. LLM가 프롬프트를 이해하지 못하거나 다른 오류가 발생하는 경우 간단한 오류 메시지가 표시됩니다.\n\n위의 주피터 노트북의 수정된 버전은 로컬 라이브러리 패키지에서 클래스로 코드화되고 Streamlit 앱에서 해당 메서드를 호출합니다. 몇 가지 간단한 데이터 파일도 있습니다. 자유롭게 다운로드하여 수정하고 자신의 목적을 위해 실행할 수 있지만 'streamlit' 폴더의 README.md 파일을 먼저 읽어야 합니다!\n\n읽어 주셔서 감사합니다. GitHub 리포지토리에서 코드와 데이터를 찾을 수 있습니다. 자유롭게 다운로드하거나 복제하거나 포크할 수 있습니다. 더 많은 기사를 보려면 중간에서 제 계정을 팔로우하거나 무료 종간 소식지를 구독해 주세요. 이전 기사는 제 웹페이지에 나열되어 있습니다.\n\n모든 이미지와 스크린샷은 별도로 표시되지 않는 한 제가 작성한 저자에 의해 만들어졌습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 코드는 MIT 라이선스에 의해 보호받습니다 (저장소에서 확인하실 수 있습니다). 언급이 필수는 아니지만 언제나 감사히 받습니다.","ogImage":{"url":"/assets/img/2024-06-19-FromDatatoVisualizationwiththeOpenAIAssistantsAPIandGPT-4o_0.png"},"coverImage":"/assets/img/2024-06-19-FromDatatoVisualizationwiththeOpenAIAssistantsAPIandGPT-4o_0.png","tag":["Tech"],"readingTime":14},{"title":"그래프 시각화 초보부터 고급까지 7단계","description":"","date":"2024-06-19 01:25","slug":"2024-06-19-GraphVisualization7StepsfromEasytoAdvanced","content":"\n\n\n\u003cimg src=\"/assets/img/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced_0.png\" /\u003e\n\n일부 데이터 유형(예: 소셜 네트워크 또는 지식 그래프)는 그래프 형식으로 \"원래\" 표현될 수 있습니다. 이 유형의 데이터 시각화는 도전적일 수 있으며, 이를 위한 보편적인 레시피는 없습니다. 이 기사에서는 오픈 소스 NetworkX 라이브러리를 사용한 그래프 시각화의 여러 단계를 보여 드리겠습니다.\n\n시작해보겠습니다!\n\n## 기본 예제\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파이썬에서 그래프를 사용하려면 NetworkX가 아마도 가장 인기 있는 선택일 것입니다. NetworkX는 네트워크 분석을 위한 오픈 소스 파이썬 패키지로, 다양한 알고리즘과 강력한 기능이 포함되어 있습니다. 우리가 알다시피, 모든 그래프에는 노드(정점)와 간선이 포함되어 있습니다. NetworkX에서 간단한 그래프를 쉽게 만들 수 있습니다:\n\n```python\nimport networkx as nx\n\nG = nx.Graph()\nG.add_node(\"A\")\nG.add_node(\"B\")\nG.add_edge(\"A\", \"B\")\n...\n```\n\n하지만 이 방법으로 대규모 그래프를 만드는 것은 피곤할 수 있으며, 이 기사에서는 NetworkX 라이브러리에 포함된 \"데이비스의 사우스 클럽 여성\" 그래프를 사용할 것입니다(3-clause BSD 라이센스). 이 데이터는 1930년대에 A. Davis 등에 의해 수집되었습니다(A. Davis, 1941, 딥 사우스, 시카고: 시카고 대학 출판사). 이 데이터는 18명의 사우스 여성이 14개의 사교 행사에 참석한 것을 나타냅니다. 이제 그래프를 불러와서 그려보겠습니다:\n\n```python\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nG = nx.davis_southern_women_graph()\n\nfig1 = plt.figure(figsize=(12, 8))\nnx.draw(G, with_labels=True)\nplt.show()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과가 여기처럼 보입니다:\n\n![그래프 시각화](/assets/img/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced_1.png)\n\n작동하고 있지만, 이 이미지는 확실히 개선할 수 있습니다. 다른 방법을 살펴보겠습니다.\n\n## 1. 레이아웃\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래프 자체는 노드와 그들 사이의 관계만을 포함하고 있어요. 좌표는 가지고 있지 않아요. 동일한 그래프가 여러 가지 다른 방식으로 표시될 수 있으며, NetworkX에는 다양한 레이아웃이 있어요. 만별한 해결책이 없고, 시각적인 인상은 주관적일 수도 있어요. 가장 좋은 방법은 다양한 옵션을 시도하고 특정 데이터셋과 가장 잘 맞는 이미지를 찾는 것이에요.\n\n스파이럴 레이아웃\n이 레이아웃은 spiral_layout 메소드를 사용하여 생성할 수 있어요:\n\n```js\npos = nx.spiral_layout(G)\nprint(pos)\n#\u003e {'Evelyn Jefferson': array([-0.51048124,  0.00953613]),\n#   'Laura Mandeville': array([-0.59223481, -0.08317364]), ... }\n\nnx.draw(G, pos=pos, with_labels=True)\n```\n\nprint 출력을 보면 레이아웃 자체가 좌표를 가진 사전(dictionary)임을 알 수 있어요. 이 레이아웃은 draw 메소드의 옵션 매개변수로 지정될 수 있어요. 결과는 다음과 같이 보여요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이미지 태그를 Markdown 형식으로 변경하세요.\n\n\n![Graph Visualization](/assets/img/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced_2.png)\n\n\n이 유형의 그래프는 최적의 것이 아닙니다. 다른 방법을 시도해 보죠.\n\n원형 레이아웃\n여기서 코드 로직은 동일합니다. 먼저 레이아웃을 생성한 다음 코드에서 사용합니다:\n\n```js\npos = nx.circular_layout(G)\nnx.draw(G, pos=pos, with_labels=True)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과:\n\n\u003cimg src=\"/assets/img/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced_3.png\" /\u003e\n\n이전 예제와 마찬가지로 이 그래프에 대해 원형 레이아웃은 최적이 아닙니다.\n\nKamada-Kawai 레이아웃\n이 방법은 Kamada-Kawai 경로 길이 비용 함수를 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\npos = nx.kamada_kawai_layout(G)\nnx.draw(G, pos=pos, with_labels=True)\n```\n\n보다 나은 결과가 나타납니다:\n\n\u003cimg src=\"/assets/img/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced_4.png\" /\u003e\n\nSpring Layout\n이 방법은 Fruchterman-Reingold 힘 방향 알고리즘을 사용하여 노드를 서로 멀어지도록 당깁니다. 이 시스템이 평형에 도달할 때까지 노드를 서로 멀어지도록 당깁니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\npos = nx.spring_layout(G, seed=42)\nnx.draw(G, pos=pos, with_labels=True)\n```\n\n결과는 주관적으로 가장 좋아 보입니다:\n\n\u003cimg src=\"/assets/img/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced_5.png\" /\u003e\n\n여기서 seed 매개변수는 결과를 동일하게 유지하려면 유용합니다. 그렇지 않으면 각 재그리기마다 다른 모양의 그래프가 생성됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다른 그래프 레이아웃 유형은 NetworkX에서 사용할 수 있습니다. 독자들은 직접 테스트해보기를 환영합니다.\n\n## 2. 노드 색상\n\n참고로, 우리의 그래프는 14개의 사회적 이벤트에 참여하는 18명의 여성을 나타냅니다. 그래프의 모든 이벤트는 \"Exx\"로 이름이 지어져 있습니다. 시각적으로 더 나은 표현을 위해 이벤트들의 색상을 변경해보겠습니다.\n\n먼저, 노드가 이벤트인지 감지하는 헬퍼 메서드를 만들겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef is_event_node(node: str) -\u003e bool:\n    \"\"\"이벤트가 'E'로 시작하는지 확인합니다.\"\"\"\n    return re.match(\"^E\\d\", node) is not None\n```\n\n여기서는 노드 패턴을 결정하기 위해 정규식을 사용했습니다 (처음에는 node.startswith(\"E\") 메서드를 사용하려고 했지만 일부 여성 이름도 \"E\"로 시작할 수 있습니다). 이제 각 노드에 대한 색상 배열을 쉽게 만들고 그래프를 그릴 때 사용할 수 있습니다:\n\n```js\ndef get_node_color(node: str) -\u003e str:\n    \"\"\"개별 노드의 색상을 가져옵니다.\"\"\"\n    return \"#00AA00\" if is_event_node(node) else \"#00AAEE\"\n\nnode_colors = [get_node_color(node) for node in G.nodes()]\nnx.draw(G, pos=pos, node_color=node_colors, with_labels=True)\n```\n\n결과는 다음과 같습니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced_6.png\" /\u003e\n\n## 3. 노드 크기\n\n색상과 마찬가지로 각 노드의 크기를 지정할 수 있습니다. \"이벤트\" 노드를 더 크게 만들어 보겠습니다. 노드 크기는 연결 수에 비례할 수도 있습니다:\n\n```js\nedges = {노드: len(G.edges(노드)) for 노드 in G.nodes()}\n\ndef node_size(노드: str) -\u003e int:\n    \"\"\" 개별 노드의 크기 가져오기 \"\"\"\n    k = 4 if is_event_node(노드) else 1\n    return 100 * k + 100 + 50 * edges[노드]\n\nnode_sizes = [node_size(노드) for 노드 in G.nodes()]\nnx.draw(G, pos=pos, node_color=node_colors, node_size=node_sizes,\n        with_labels=True)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기, 노드 당 엣지 수를 별도의 딕셔너리에 저장했어요. 또한 \"이벤트\" 노드를 더 크게 만들기 위해 이전과 동일한 is_event_node 메서드를 사용했어요.\n\n결과물은 이렇게 생겼어요:\n\n![그래프 시각화](/assets/img/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced_7.png)\n\n## 4. 엣지 색상\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n노드 뿐만 아니라 엣지 색상도 지정할 수 있습니다. 예를 들어, Theresa Anderson이 방문한 모든 이벤트를 강조해 보겠습니다. 이를 위해 세 가지 도우미 메서드가 필요합니다:\n\n```js\nhighlighted_node = \"Theresa Anderson\"\n\n\ndef get_node_color(node: str) -\u003e str:\n    \"\"\" 개별 노드의 색상을 가져옵니다 \"\"\"\n    if is_event_node(node):\n        if G.has_edge(node, highlighted_node):\n            return \"#00AA00\"\n    elif node == highlighted_node:\n        return \"#00AAEE\"\n    return \"#AAAAAA\"\n\ndef edge_color(node1: str, node2: str) -\u003e str:\n    \"\"\" 개별 엣지의 색상을 가져옵니다 \"\"\"\n    if node1 == highlighted_node or node2 == highlighted_node:\n        return \"#992222\"\n    return \"#999999\"\n\ndef edge_weight(node1: str, node2: str) -\u003e str:\n    \"\"\" 개별 엣지의 두께를 가져옵니다 \"\"\"\n    if node1 == highlighted_node or node2 == highlighted_node:\n        return 3\n    return 1\n```\n\n여기에서 Theresa Anderson의 노드에 대해 별도의 색상을 사용했습니다. 그녀가 방문한 모든 이벤트의 색상도 변경했습니다. has_edge 메서드는 두 노드가 공통 엣지를 가지고 있는지 확인하는 쉬운 방법입니다. 또한 엣지 두께도 변경했습니다.\n\n이제 그래프를 그릴 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nedge_colors = [edge_color(n1, n2) for n1, n2 in G.edges()]\nedge_weights = [edge_weight(n1, n2) for n1, n2 in G.edges()]\nnode_colors = [get_node_color(node) for node in G.nodes()]\nnx.draw(G, pos=pos, node_color=node_colors, node_size=node_sizes,\n        edge_color=edge_colors, width=edge_weights, with_labels=True)\n```\n\n이렇게 결과가 나옵니다:\n\n![Graph Visualization](/assets/img/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced_8.png)\n\n## 5. 노드 레이블\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래프에 서로 다른 노드 유형이 있는 경우, 서로 다른 노드에 대해 서로 다른 글꼴을 사용할 수 있습니다. 그러나 제 놀람에는 NetworkX에서 색상과 같이 글꼴을 지정하는 쉬운 방법이 없습니다. \"이벤트\"와 \"사람\" 노드를 그릴 때, 그래프를 서브그래프로 분할하여 따로 그릴 수 있습니다:\n\n```js\nnode_events = [node for node in G.nodes() if is_event_node(node)]\nnode_people = [node for node in G.nodes() if not is_event_node(node)]\n\nnx.draw(G, pos=pos, node_color=node_colors, node_size=node_sizes, with_labels=False)\nnx.draw_networkx_labels(G.subgraph(node_events), pos=pos, font_weight=\"bold\")\nnx.draw_networkx_labels(G.subgraph(node_people), pos=pos, font_weight=\"normal\", font_size=11)\n```\n\n여기서, 먼저 노드를 이전과 같이 그리되, with_labels 매개변수를 False로 설정했습니다. 그런 다음 서로 다른 글꼴 설정으로 draw_networkx_labels 메서드를 두 번 사용했습니다.\n\n결과는 다음과 같이 보입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![그래프 시각화](/assets/img/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced_9.png)\n\n## 6. 노드 속성\n\n우리는 도움이 되는 Python 메소드를 사용하여 노드 색상과 크기를 설정할 수 있었습니다. 그러나 노드 속성으로도 이 정보를 그래프 자체에 저장할 수 있습니다:\n\n```js\ncolors_dict = {node: get_node_color(node) for node in G.nodes()}\nnx.set_node_attributes(G, colors_dict, \"color\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같이 일부 노드에 수동으로 속성을 지정할 수도 있습니다:\n```js\ncustom_colors_dict = {\n             \"Frances Anderson\": \"orange\",\n             \"Theresa Anderson\": \"orange\",\n             \"E3\": \"darkgreen\",\n             \"E5\": \"darkgreen\",\n             \"E6\": \"darkgreen\"\n}\nnx.set_node_attributes(G, custom_colors_dict, \"color\")\n```\n\n그런 다음, 그래프를 파일로 저장하면 모든 정보가 그대로 유지됩니다:\n```js\nnx.write_gml(G, \"davis_southern_women.gml\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로는 그래프를 로드하고 노드 속성에서 모든 색상을 추출할 수 있습니다. 더 이상 헬퍼 메서드가 필요하지 않습니다:\n\n```js\nattributes = nx.get_node_attributes(G, \"color\")\nnode_color_attrs = [attributes[node] for node in G.nodes()]\nnx.draw(G, pos=pos, node_color=node_color_attrs, node_size=node_sizes, with_labels=False)\nnx.draw_networkx_labels(G.subgraph(node_events), pos=pos, font_weight=\"bold\")\nnx.draw_networkx_labels(G.subgraph(node_people), pos=pos, font_weight=\"normal\", font_size=11)\n```\n\n위와 같은 결과가 나타납니다:\n\n\u003cimg src=\"/assets/img/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced_10.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서는 이전에 사용한 것과 동일한 색상들을 보고, 사용자 정의 색상을 가진 네 개의 노드를 볼 수 있습니다. 모든 정보가 GML 파일에 저장되었습니다.\n\n## 7. 보너스: D3.JS로 그래프 그리기\n\n그래프를 그릴 때 NetworkX는 Matplotlib을 사용합니다. 이는 이와 같은 작은 그래프에는 좋지만, 그래프에 1000개 이상의 노드가 포함된 경우 Matplotlib은 극도로 느려집니다. D3.JS를 사용하면 훨씬 더 나은 결과를 얻을 수 있습니다. D3.JS는 그래프 시각화를 훨씬 더 효율적으로 수행할 수 있는 오픈 소스 JavaScript 라이브러리입니다. D3는 데이터 시각화를 위한 성숙한 프로젝트이며(첫 번째 버전은 2011년에 출시되었습니다), 그래프뿐만 아니라 다양한 아름다운 이미지를 Examples 갤러리에서 찾아볼 수 있습니다.\n\nNetworkX 그래프를 D3로 내보내는 \"원시적인\" 방법을 찾지 못했지만, 몇 줄의 코드로 수행할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\ndef convert_to_d3(graph: nx.Graph) -\u003e dict:\n    \"\"\" Convert nx.Graph to D3 data \"\"\"\n    nodes, edges = [], []\n    for node_name in graph.nodes:\n        nodes.append({\"id\": node_name,\n                      \"color\": get_node_color(node_name),\n                      \"radius\": 0.01*node_size(node_name)})\n    for node1, node2 in graph.edges:\n        edges.append({\"source\": node1, \"target\": node2})\n    return {\"nodes\": nodes, \"links\": edges}\n\n# D3 형식으로 저장\nd3 = convert_to_d3(G)\nwith open('d3_graph.json', 'w', encoding='utf-8') as f_out:\n    json.dump(d3, f_out, ensure_ascii=False, indent=2)\n```\n\n이후에는 JSON 데이터를 Javascript 페이지로 로드할 수 있습니다:\n\n```javascript\n\u003cscript type=\"module\"\u003e\n    // 차트의 차원을 지정합니다.\n    const width = window.innerWidth;\n    const height = window.innerHeight;\n\n    // 색상 척도를 지정합니다.\n    const color = d3.scaleOrdinal(d3.schemeTableau10);\n\n    const data = await d3.json(\"./d3_graph.json\");\n    const links = data.links.map(d =\u003e ({...d}));\n    const nodes = data.nodes.map(d =\u003e ({...d}));\n\n    // SVG 컨테이너를 생성합니다.\n    const svg = d3.create(\"svg\")\n        .attr(\"width\", width)\n        .attr(\"height\", height)\n        .attr(\"viewBox\", [0, 0, width, height])\n        .attr(\"style\", \"max-width: 100%; height: auto;\");\n\n    ...      \n\n    // 여러 힘을 가진 시뮬레이션을 생성합니다.\n    const simulation = d3.forceSimulation(nodes)\n        .force(\"link\", d3.forceLink(links).id(d =\u003e d.id))\n        .force(\"charge\", d3.forceManyBody())\n        .force(\"center\", d3.forceCenter(width / 2, height / 2))\n        .on(\"tick\", ticked);\n\n    // SVG 요소를 추가합니다.\n    container.append(svg.node());\n\u003c/script\u003e\n```\n\n이 문서는 JavaScript 자체에 집중하고 있지 않습니다. D3.JS로 그래프를 그리는 코드 예제는 온라인에서 쉽게 찾을 수 있습니다. (이것은 좋은 시작점일 수 있습니다. 전체 소스 코드 링크도 이 페이지의 끝에 있습니다.)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요렇게 하여 로컬 서버를 실행하고 브라우저에서 페이지를 열 수 있습니다. JavaScript의 장점 중 하나는 그래프를 상호작용 가능하게 만들 수 있고 노드를 드래그앤드롭으로 이동할 수 있다는 것입니다:\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*4asm0pegdu8GK_tg5pOiVQ.gif)\n\n하지만 단점으로는 D3 렌더링에서 변경 사항을 만드려면 JavaScript, CSS 및 HTML 스타일로 심층적으로 파고들어야 한다는 것입니다. 저는 프론트엔드 웹 개발자가 아니라서 작은 조정조차 시간이 많이 소요되지만 (그래도 항상 뭔가를 배우는 게 좋죠:)) 예를 들어, 내 예제에서 기본 그래프 크기가 너무 작았고 기본 \"줌\" 값을 설정하는 간단한 방법을 찾지 못했습니다. 그러나 복잡한 그래프의 경우 Matplotlib 렌더링은 너무 느리므로 다른 선택지가 없습니다. 저는 현대 예술가들의 그래프 시각화를 위해 D3 라이브러리를 사용했고 결과는 좋았습니다.\n\n## 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글에서는 NetworkX를 사용하여 그래프 시각화하는 다양한 방법을 보여드렸어요. 보다시피, 이 과정은 대부분 직관적이며 노드 크기 또는 색상과 같은 많은 매개변수를 쉽게 조절할 수 있어요. 보다 복잡한 그래프는 JSON으로 내보내어 JavaScript로 사용할 수 있어요. 이후에는 강력한 D3.JS 라이브러리를 사용할 수 있어요 - 웹 브라우저에서의 렌더링 프로세스는 아마도 하드웨어 가속화되어 더 빠를 거예요.\n\n이전 글에서는 위키피디아에서 수집한 현대 예술가에 관한 데이터를 분석하기 위해 NetworkX 라이브러리를 사용했어요. 사회 데이터 분석에 관심이 있는 분들은 다른 게시물들도 읽어보세요:\n\n- 탐색적 데이터 분석: YouTube 채널에 대해 알고 있는 것은 무엇인가요?\n- 독일의 주거 임대 시장: Python으로 탐색적 데이터 분석\n- 사람들이 기후에 대해 쓰는 내용: Python으로 Twitter 데이터 클러스터링\n- Twitter 게시물에서 시간 패턴 찾기: Python으로 탐색적 데이터 분석\n- Python 데이터 분석: 팝송에 대해 우리가 아는 것은?\n\n이 이야기를 즐겼다면 Medium을 구독해도 좋아요. 새로운 글이 게시되면 알림을 받을 수 있을 뿐만 아니라 다른 저자들의 수천 개의 이야기에도 완전한 액세스 권한을 얻을 수 있어요. LinkedIn을 통해 연결해도 좋고요. 이와 다른 게시물들의 전체 소스 코드를 얻고 싶다면 Patreon 페이지를 방문해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n읽어 주셔서 감사합니다!","ogImage":{"url":"/assets/img/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced_0.png"},"coverImage":"/assets/img/2024-06-19-GraphVisualization7StepsfromEasytoAdvanced_0.png","tag":["Tech"],"readingTime":12},{"title":"MKBHD의 YouTube 성공을 데이터로 살펴보기","description":"","date":"2024-06-19 01:23","slug":"2024-06-19-AData-DrivenLookatMKBHDsYouTubeSuccess","content":"\n\n## 테크 비디오 블로깅에서 콘텐츠 트렌드와 관객 참여에 대해 발견하다\n\n![마크 블라운리의 유튜브 성공을 데이터로 살펴보기](/assets/img/2024-06-19-AData-DrivenLookatMKBHDsYouTubeSuccess_0.png)\n\n테크 유튜브 세계에서 마크 브라운리인 MKBHD는 주목할 만한 권위자로 자리매깁니다. 최신 기기와 기술 트렌드에 대한 그의 매력적인 비디오는 수백만 명의 구독자를 모았습니다. 그러나 세련된 시각과 통찰력 있는 코멘터리 이상의 것은 무엇일까요? 여기에서는 데이터 과학적 시각으로 MKBHD의 콘텐츠 세계를 탐구하며, 유튜브 비디오 대본을 분석하여 흥미로운 패턴과 트렌드를 발견합니다. 이 기사에서는 MKBHD의 지난 몇 년간의 콘텐츠에 대한 고유한 시각을 제공하며 내 연구 결과를 공유할 것입니다.\n\n![마크 브라운리의 유튜브 성공을 데이터로 살펴보기](/assets/img/2024-06-19-AData-DrivenLookatMKBHDsYouTubeSuccess_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 소개\n\nMKBHD는 기본적으로 그의 리뷰와 비평을 유튜브 채널을 통해 전달하는데, 2008년 첫 기술 게시물 이후 1천 6백개가 넘는 동영상을 올려 1억 9천만 명 이상의 팔로워를 보유하고 있습니다.\n그는 현재 총 43억 회 이상의 조회수를 기록하고 있습니다.\n\n# 아이디어\n\n유튜브는 플랫폼에서 비디오에 대한 대본을 제공합니다. 이 대본은 유튜버가 직접 설정할 수도 있고, 자동 생성된 자막(대게 엉터리이지만, 우리가 갖고 있는 것으로 충분히 활용합니다)일 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희 목표는 이 대본을 분석하고 데이터의 기저 관계를 파악하는 것입니다.\n\n# 개요\n\nYouTube Data v3 API를 사용하여 MKBHD의 모든 이용 가능한 대본 및 비디오 통계를 추출하여 DataFrame에 보존했습니다. 그 후에 NLTK, SentenceTransformers, SpaCy, UMAP 및 HDBSCAN을 사용하여 이 대본을 다양한 용도로 분석했습니다:\n\n- 동영상 시간 대 뷰어 수\n- 주제 별 동영상 클러스터링\n- 이러한 클러스터의 평균 뷰/좋아요\n- 뷰어 수 대 평균 발언 속도\n- 감정 분석\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 프로젝트\n\n## 1. 조회수 대 동영상 시간\n\n유튜브 알고리즘이 주로 5-10분 사이의 동영상을 선호한다고 말해지는 것이 종종 있습니다. 채널의 동영상 시간은 다음과 같습니다:\n\n![chart](/assets/img/2024-06-19-AData-DrivenLookatMKBHDsYouTubeSuccess_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요즘 그는 더 긴 형식의 동영상을 업로드하고 있습니다. 그 결과 구독자 수가 늘어난 것은 물론 조회수에도 영향을 미치지 않았습니다. 최근 2-3년 동안 게시된 동영상들이 이전에 업로드된 짧은 동영상들보다 평균적으로 더 나은 성과를 거두고 있습니다.\n\n따라서 콘텐츠 품질 또한 조회수를 얻는 데 중요한 요소임을 결론지을 수 있습니다.\n\n## 2. 주제별 동영상 클러스터링\n\nMKBHD 채널에는 핸드폰 리뷰부터 카메라 리뷰, 그리고 자동차 시리즈까지 다양한 주제의 동영상이 있습니다. 저는 각 대본에 대한 임베딩에 HDBSCAN을 수행하여 지정된 매개변수에 기반하여 14개 그룹으로 분할했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해당 글의 내용은 다음과 같습니다. \n\n왼쪽 아래의 해군 청색 점들은 자동차 리뷰에 관한 비디오를 나타내고, 왼쪽 상단의 주황색 군집은 오디오 장비에 관한 것이며, 오른쪽 하단 쪽에 있는 주황빛 붉은 그룹은 안드로이드 기능을 대표합니다.\n\n동일한 그룹의 모든 대본을 합쳐 14개의 슈퍼 대본을 만들었고, 이를 기반으로 TFIDF 알고리즘을 사용하여 각 그룹의 키워드를 추출했습니다:\n\n```js\n-1: 질소, 이온, 행아웃, 전해질\n 0: 베이퍼웨어, 재생, 견인, 암레스트, 인테리어\n 1: 이젤, 하즈웰, 소켓, 오버클럭, 에이서\n 2: XLR, 순간이동, 제로, 지퍼, EVF\n 3: 스피커폰, 심전도, 플럭스, 샤워\n 4: 여론조사, 패배, 총계, 준결승전, 가짜\n 5: 자세, 아코디언, 방수\n 6: 낭비적인, 휴대용, 데멘시티, 얼룩\n 7: 탈옥, 페퍼민트, 넥서스5, 라임, 탈옥\n 8: 8A, 흐릿하게, 욕조, 4A\n 9: 묶인, 블루버즈, 엠파우, 방수, 오디오파일, M70X\n 10: 10R, 11R, 인그레스, 그래핀, 메테오라이트\n 11: QI2, 1플러스, 햄버거, 강조된\n 12: LTEA, 얼룩, IP58, 평범한, 미터링, 50X\n```\n\n비디오 클러스터 및 키워드를 기반으로 다음은 각 클러스터별로 제안할 수 있는 주제입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n-1: Outliers (내 휴대폰에 뭐가 있을까 (20xx), DALL-E, 5G)\n0:  자동차 리뷰\n1:  맥북\n2:  테크 기어\n3:  스마트 워치\n4:  스마트폰 카메라\n5:  접이식 폰\n6:  아이패드\n7:  안드로이드 기능 및 발표\n8:  구글 픽셀 시리즈\n9:  웨어러블 오디오 장치\n10: 아이폰과 iOS\n11: 원플러스와 옥시젠 OS\n12: 안드로이드 스마트폰\n```\n\n내가 방금 깨달은 것은 클러스터 1이 맥북에 대한 것이라는 것이며, 'The #1 Most Overpriced Tech in 2023'라는 제목의 비디오도 있습니다. 또한 클러스터 12인 안드로이드 폰에 대한 내용은 '보통'을 키워드로 갖고 있네요.\n\n## 3. 이 클러스터의 평균 조회수/좋아요 수\n\n당신의 관객이 좋아하는 콘텐츠를 파악하고 그것을 활용하는 것은 대부분의 유튜버들이 가지고 싶어하는 기술이에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-AData-DrivenLookatMKBHDsYouTubeSuccess_3.png\" /\u003e\n\n산점도는 각 클러스터별 평균 조회수와 좋아요 수를 보여줍니다. 우리는 두 클러스터 중 특히 두드러지는 클러스터는 번호 5 (폴더블 폰)와 10 (아이폰 및 iOS)이며, 그 뒤를 이어 번호 0 (자동차 리뷰), 6 (아이패드), 8 (구글 픽셀 시리즈) 및 11 (원플러스)가 있음을 알 수 있습니다.\n\n따라서 우리는 시청자들이 모바일 기술의 발전을 관심 있게 보는 것을 결론 지을 수 있습니다.\n\n## 4. 조회수 대 평균 발표 속도\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n유튜버들은 종종 콘텐츠를 어떻게 제시할지 실험합니다. 그들은 말하는 단어의 명료도, 말 속도, 어휘 등을 조절합니다. 아래 그래프는 MKBHD의 조회수 대 말한 단어 수/분을 보여줍니다.\n\n![MKBHD의 조회수 대 말한 단어 수/분](/assets/img/2024-06-19-AData-DrivenLookatMKBHDsYouTubeSuccess_4.png)\n\n분당 단어 수는 영상 내 스크립트의 개별 단어 수를 총 발언 시간으로 나눈 것입니다.\n\n이 그래프들은 그가 말하는 속도를 약 100-120 단어/분으로 일정하게 유지하고 있지만, 최근에는 보통보다 훨씬 높은 속도인 190-200 단어/분을 시도하며 실험하고 있다는 것을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 연설 속도가 이 채널의 조회수에 큰 영향을 미치지 않는다는 결론을 내릴 수 있습니다(그러나 에미넴과 같은 다른 채널들의 조회수에는 영향을 줄 수 있습니다. 에미넴은 450마일/시간에 도달하는 연설 속도로 유명합니다).\n\n## 5. 감성 분석\n\n기술 리뷰어로서, MKBHD는 최신 기기에 대한 공정하고 편향되지 않은 의견을 가져야하며, 시청자에게 정보에 근거한 결정을 내릴 수 있는 필수적인 지침을 제공해야 합니다.\n\n여기서, 음성의 부정적 점수가 낮을수록 채널에서 부정적인 단어의 사용이 적습니다. 그래프에서 알 수 있듯이, 그는 중립을 유지하며 일반적으로 긍정적인 리뷰를 제공합니다.\n재미있는 점은 최근 공개된 '내가 지금까지 리뷰한 제품 중 최악이다... 지금까지'라는 제목의 비디오에 대해 언급할 수 있습니다. 인간적 AI 핀에 대한 리뷰로 사용자들을 깜짝 놀라게 했는데, 이 리뷰는 최근에 제공한 가장 부정적인 리뷰 중 하나였습니다. 마크는 제품에 대한 희망을 인정했고 미래 아이디어를 좋아한다고 말했습니다. 이 비디오의 부정적 점수는 0.1이고, 긍정적 점수는 0.26으로 상당히 표준적입니다. 이는 비디오 콘텐츠 자체가 너무 심하지 않을 수도 있지만, 네티즌들이 지나치게 과장하고 과도하게 비난한 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n최근에는 데이터가 가장 원하는 상품 중 하나입니다. 이는 영향력과 콘텐츠 제작 업무에서 더욱 명백해졌습니다. 창작자들은 이전 데이터를 활용하여 더 효율적이고 매력적인 콘텐츠를 제작할 수 있으며, 이는 더 큰 영향력과 참여를 가져올 수 있습니다. 콘텐츠는 지속적인 과정이며, 효율적이고 최신 정보가 필요할 때 데이터는 매우 중요한 역할을 합니다.\n\n# 향후 작업\n\nYouTube에는 '가장 재생된' 기능이 있어 콘텐츠의 가장 인기 있는 부분과 가장 인지도가 낮은 부분을 볼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 프로젝트는 비디오의 전체 콘텐츠와 통계에 초점을 맞추지만, 이를 사용하여 비디오의 작은 부분들의 유사한 기능을 보여주는 기반으로도 사용할 수 있습니다. 이를 통해 유튜버들이 인기 없는 부분에서 발생하는 지속적인 문제를 해결하고, 그 콘텐츠에서 가장 많이 재생된 스니펫의 품질을 대규모로 구현하는 데 도움을 줄 수 있습니다.\n\n평균 시청 시간 같은 다른 통계도 추출하여 이러한 클러스터와 대조해볼 수 있으며, 뷰어들이 상호 작용하는 주제를 분석할 수 있습니다.\n\n# 참고 자료\n\n- https://www.youtube.com/watch?v=lXeNZeLSsgY\u0026t=18s\n- https://www.youtube.com/watch?v=qzKCEhYssAk\u0026list=PLwm1cDL75rYMECvKZzSwUz2hwZNkuKO8T\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 소스 코드\n\n참고: 이 프로젝트에는 약 1120개의 비디오가 사용되었습니다. 일부 비디오에는 대본이 제공되지 않았기 때문에 고려되지 않았습니다.","ogImage":{"url":"/assets/img/2024-06-19-AData-DrivenLookatMKBHDsYouTubeSuccess_0.png"},"coverImage":"/assets/img/2024-06-19-AData-DrivenLookatMKBHDsYouTubeSuccess_0.png","tag":["Tech"],"readingTime":6}],"page":"59","totalPageCount":71,"totalPageGroupCount":4,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"59"},"buildId":"FH3Qr-mLAesqA0X5IFRQr","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>