<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/2" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/2" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-2d104a861d88ea21.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_buildManifest.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="좀비 프로세스의 비밀을 밝히다 Linux에서 알아야 할 모든 것" href="/post/2024-06-21-LinuxUnveilingtheMysteriesofZombieProcesses"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="좀비 프로세스의 비밀을 밝히다 Linux에서 알아야 할 모든 것" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-21-LinuxUnveilingtheMysteriesofZombieProcesses_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="좀비 프로세스의 비밀을 밝히다 Linux에서 알아야 할 모든 것" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">좀비 프로세스의 비밀을 밝히다 Linux에서 알아야 할 모든 것</strong><div class="PostList_meta__VCFLX"><span class="date">14 hours ago</span><span class="PostList_reading_time__6CBMQ">1<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="파이썬 엔지니어를 위한 3D 가우시안 스플래팅 소개 파트 2" href="/post/2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="파이썬 엔지니어를 위한 3D 가우시안 스플래팅 소개 파트 2" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="파이썬 엔지니어를 위한 3D 가우시안 스플래팅 소개 파트 2" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">파이썬 엔지니어를 위한 3D 가우시안 스플래팅 소개 파트 2</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="신경망 기본 이론과 구조 유형" href="/post/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="신경망 기본 이론과 구조 유형" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="신경망 기본 이론과 구조 유형" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">신경망 기본 이론과 구조 유형</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">22<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="비디오 주석 도구 비전-언어 모델과 액티브 러닝을 활용하여 효율적으로 비디오 분류기를 만들 수 있는 프레임워크" href="/post/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="비디오 주석 도구 비전-언어 모델과 액티브 러닝을 활용하여 효율적으로 비디오 분류기를 만들 수 있는 프레임워크" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="비디오 주석 도구 비전-언어 모델과 액티브 러닝을 활용하여 효율적으로 비디오 분류기를 만들 수 있는 프레임워크" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">비디오 주석 도구 비전-언어 모델과 액티브 러닝을 활용하여 효율적으로 비디오 분류기를 만들 수 있는 프레임워크</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="대형 언어 모델 LLM의 잠재력 발휘하기" href="/post/2024-06-20-UnleashingthePotentialofLargeLanguageModelsLLMs"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대형 언어 모델 LLM의 잠재력 발휘하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-UnleashingthePotentialofLargeLanguageModelsLLMs_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대형 언어 모델 LLM의 잠재력 발휘하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">대형 언어 모델 LLM의 잠재력 발휘하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="언어 에이전트 트리 검색 - LATS" href="/post/2024-06-20-LanguageAgentTreeSearchLATS"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="언어 에이전트 트리 검색 - LATS" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-LanguageAgentTreeSearchLATS_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="언어 에이전트 트리 검색 - LATS" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">언어 에이전트 트리 검색 - LATS</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Transformers에 대한 심층적인 탐구 " href="/post/2024-06-20-DeepDiveintoTransformersbyHand"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Transformers에 대한 심층적인 탐구 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Transformers에 대한 심층적인 탐구 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Transformers에 대한 심층적인 탐구 </strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="대형 언어 모델의 미래 2030년과 2050년을 위한 예측" href="/post/2024-06-20-TheFutureofLargeLanguageModelsPredictionsfor2030and2050"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대형 언어 모델의 미래 2030년과 2050년을 위한 예측" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-TheFutureofLargeLanguageModelsPredictionsfor2030and2050_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대형 언어 모델의 미래 2030년과 2050년을 위한 예측" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">대형 언어 모델의 미래 2030년과 2050년을 위한 예측</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Llama 3 해제하기 Llama 3 마스터하기를 위한 궁극적인 안내" href="/post/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Llama 3 해제하기 Llama 3 마스터하기를 위한 궁극적인 안내" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Llama 3 해제하기 Llama 3 마스터하기를 위한 궁극적인 안내" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Llama 3 해제하기 Llama 3 마스터하기를 위한 궁극적인 안내</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="RL 경계를 넓히기 LLMs와 VLMs와 같은 기본 모델을 강화 학습에 통합하기" href="/post/2024-06-20-PushingRLBoundariesIntegratingFoundationalModelsegLLMsandVLMsintoReinforcementLearning"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="RL 경계를 넓히기 LLMs와 VLMs와 같은 기본 모델을 강화 학습에 통합하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-PushingRLBoundariesIntegratingFoundationalModelsegLLMsandVLMsintoReinforcementLearning_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="RL 경계를 넓히기 LLMs와 VLMs와 같은 기본 모델을 강화 학습에 통합하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">RL 경계를 넓히기 LLMs와 VLMs와 같은 기본 모델을 강화 학습에 통합하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">15<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><a class="link" href="/posts/1">1</a><a class="link posts_-active__YVJEi" href="/posts/2">2</a><a class="link" href="/posts/3">3</a><a class="link" href="/posts/4">4</a><a class="link" href="/posts/5">5</a><a class="link" href="/posts/6">6</a><a class="link" href="/posts/7">7</a><a class="link" href="/posts/8">8</a><a class="link" href="/posts/9">9</a><a class="link" href="/posts/10">10</a><a class="link" href="/posts/11">11</a><a class="link" href="/posts/12">12</a><a class="link" href="/posts/13">13</a><a class="link" href="/posts/14">14</a><a class="link" href="/posts/15">15</a><a class="link" href="/posts/16">16</a><a class="link" href="/posts/17">17</a><a class="link" href="/posts/18">18</a><a class="link" href="/posts/19">19</a><a class="link" href="/posts/20">20</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"좀비 프로세스의 비밀을 밝히다 Linux에서 알아야 할 모든 것","description":"","date":"2024-06-21 23:58","slug":"2024-06-21-LinuxUnveilingtheMysteriesofZombieProcesses","content":"\n\n```markdown\n![LinuxUnveilingtheMysteriesofZombieProcesses](/assets/img/2024-06-21-LinuxUnveilingtheMysteriesofZombieProcesses_0.png)\n\n어떤 프로세스가 \"exit\"를 호출하면 즉시 사라지지 않는다는 사실을 아는 사람은 적을 것입니다. 대신, \"좀비\" 프로세스라고 불리는 데이터 구조를 남깁니다. Linux 프로세스의 다섯 가지 상태 중에서 좀비 프로세스는 특히 독특합니다.\n\n거의 모든 메모리 공간을 포기했으며 실행 가능한 코드가 전혀 없으며 스케줄링될 수 없으며 단지 ...에 위치하고...\n```","ogImage":{"url":"/assets/img/2024-06-21-LinuxUnveilingtheMysteriesofZombieProcesses_0.png"},"coverImage":"/assets/img/2024-06-21-LinuxUnveilingtheMysteriesofZombieProcesses_0.png","tag":["Tech"],"readingTime":1},{"title":"파이썬 엔지니어를 위한 3D 가우시안 스플래팅 소개 파트 2","description":"","date":"2024-06-20 19:11","slug":"2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2","content":"\n\n## 3D 가우시안 스플래팅 내에서 가우시안 함수가 어떻게 사용되는지 이해하고 코딩하기\n\n이제 가우시안에 대해 이야기해보겠습니다! 모두가 좋아하는 분포입니다. 지금부터 함께하는 분들을 위해, 카메라의 위치를 이용하여 3D 점을 2D로 변환하는 방법에 대해 part 1에서 다룬 바 있습니다. 이 글에서는 가우시안 스플래팅의 가우시안 부분을 다룰 것입니다. 우리는 GitHub에서 part_2.ipynb를 사용할 것입니다.\n\n여기서 우리가 만들게 될 약간의 변경사항은, 이전 글에서 보여준 것과는 다른 내부 매트릭스를 활용하는 원근 투영을 사용할 것이라는 것입니다. 그러나 2D로 점을 투영할 때 두 방법은 동등하며, 저는 part 1에서 소개된 첫 번째 방법이 이해하기 쉽다고 생각합니다. 그러나 가능한한 저자의 코드를 파이썬으로 복제하기 위해 저희는 방법을 변경할 것입니다. 구체적으로 우리의 \"내부\" 매트릭스는 이곳에 표시된 OpenGL 투영 매트릭스에 의해 이제 제공되며, 곱셈의 순서는 이제 points @ external.transpose() @ internal으로 변경될 것입니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2_0.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n호기심이 있는 분들을 위해 새로운 내부 매트릭스에 대해 알고 싶은 경우(그렇지 않으면 이 단락을 건너뛰어도 괜찮아요) r과 l은 오른쪽과 왼쪽 측면의 클리핑 평면이며, 사진의 너비에 관한 시야에 포함될 수 있는 지점을 기본적으로 나타내고 있습니다. t와 b는 상단과 하단 클리핑 평면이고, N은 가까운 클리핑 평면(투영될 점들이 있는 곳)이며, f는 먼 클리핑 평면입니다. 더 자세한 정보는 scratchapixel의 챕터들이 여기에서 매우 유익하다고 생각합니다(https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/opengl-perspective-projection-matrix.html). 이것은 또한 점들을 정규화된 장치 좌표( -1과 1 사이)로 반환하며, 이를 픽셀 좌표로 투영합니다. 이론에서 벗어나서 우리의 작업은 같습니다, 3D에서 점을 가져와 2D 이미지 평면으로 투영하는 것입니다. 그러나 이 튜토리얼의 이 부분에서는 이제 포인트 대신 가우시안 함수를 사용합니다.\n\n```js\ndef getIntinsicMatrix(\n    focal_x: torch.Tensor,\n    focal_y: torch.Tensor,\n    height: torch.Tensor,\n    width: torch.Tensor,\n    znear: torch.Tensor = torch.Tensor([100.0]),\n    zfar: torch.Tensor = torch.Tensor([0.001]),,\n) -\u003e torch.Tensor:\n    \"\"\"\n    내부 퍼스펙티브 투영 매트릭스 가져오기\n    \n    znear: 사용자가 지정한 가까운 평면\n    zfar: 사용자가 지정한 먼 평면\n    fovX: 초점 길이에서 계산된 x의 시야\n    fovY: 초점 길이에서 계산된 y의 시야\n    \"\"\"\n    fovX = torch.Tensor([2 * math.atan(width / (2 * focal_x))])\n    fovY = torch.Tensor([2 * math.atan(height / (2 * focal_y))])\n    \n    tanHalfFovY = math.tan((fovY / 2))\n    tanHalfFovX = math.tan((fovX / 2))\n\n    top = tanHalfFovY * znear\n    bottom = -top\n    right = tanHalfFovX * znear\n    left = -right\n    P = torch.zeros(4, 4)\n    z_sign = 1.0\n\n    P[0, 0] = 2.0 * znear / (right - left)\n    P[1, 1] = 2.0 * znear / (top - bottom)\n    P[0, 2] = (right + left) / (right - left)\n    P[1, 2] = (top + bottom) / (top - bottom)\n    P[3, 2] = z_sign\n    P[2, 2] = z_sign * zfar / (zfar - znear)\n    P[2, 3] = -(zfar * znear) / (zfar - znear)\n    return P\n```\n\n3D 가우시안 splat은 x, y, z 좌표 및 관련 공분산 행렬로 구성됩니다. 저자들이 언급한 대로: \"명백한 접근 방식은 공분산 행렬 Σ를 직접 최적화하여 빛의 필드를 나타내는 3D 가우시안을 얻는 것일 것입니다. 그러나, 공분산 행렬은 양의 준정치일 때만 물리적인 의미를 갖습니다. 우리가 모든 매개변수를 최적화하기 위해 사용하는 경사 하강법은 이러한 유효한 행렬을 생성하기가 쉽지 않으며, 업데이트 단계와 그래디언트는 쉽게 유효하지 않은 공분산 행렬을 만들어냅니다.\"\n\n그래서 저자들은 항상 양의 준정부 공분산 행렬을 생성할 수 있는 공분산 행렬의 분해를 사용합니다. 특히, 3개의 \"크기\" 매개변수와 4개의 쿼터니언을 사용하여 3x3 회전 행렬(R)로 변환합니다. 그런 다음 공분산 행렬은 다음과 같이 주어집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2_1.png)\n\n쿼터니온 벡터를 회전 행렬로 변환하기 전에 정규화해야만 유효한 회전 행렬을 얻을 수 있습니다. 따라서 저희 구현에서 가우스 포인트는 다음 매개변수로 구성됩니다. 좌표 (3x1 벡터), 쿼터니온 (4x1 벡터), 스케일 (3x1 벡터) 및 불투명도(스플래팅이 얼마나 투명한지를 나타내는 최종 float 값)입니다. 이제 모든게 갖춰졌네요! 이 11개의 매개변수를 최적화하여 우리의 씬을 만들 수 있습니다 — 간단하지요!\n\n하지만 실제로는 조금 복잡합니다. 고등학교 수학을 기억한다면 특정 지점에서의 가우시안의 세기는 아래의 방정식으로 주어집니다:\n\n![이미지](/assets/img/2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2_2.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만, 우리는 이미지 평면인 2D에서 3D 가우시안의 강도에 중점을 두고 있습니다. 하지만 여러분은 아마 \"우리는 2D로 점을 투영하는 방법을 알고 있다고?\" 할 수 있겠지만요! 그럼에도 불구하고, 아직 2D로 공분산 행렬을 투영하는 방법에 대해 다루지 않았기 때문에, 만약 우리가 2D 공분산 행렬의 역행렬을 찾지 않았다면 이 무슨 얘기인지 알 수 없겠죠.\n\n이제 재미있는 부분입니다(어떻게 보느냐에 따라 다를 수 있습니다). 3D 가우시안 스플래팅 저자들의 논문인 EWA Splatting은 3D 공분산 행렬을 2D로 투영하는 정확한 방법을 보여줍니다. 그러나 이것은 알려진 야코비안 아핀 변환 행렬의 지식을 전제로 한다는 것에 유념해야 합니다. 아래에서 계산하는 것처럼. 어려운 개념을 풀어갈 때 가장 도움이 되는 것은 코드이므로, 3D 공분산 행렬에서 2D로 전환하는 방법을 실제로 어떻게 하는 지 보여주기 위해 아래에 일부 코드를 제공했습니다.\n\n```js\ndef compute_2d_covariance(\n    points: torch.Tensor,\n    external_matrix: torch.Tensor,\n    covariance_3d: torch.Tensor,\n    tan_fovY: torch.Tensor,\n    tan_fovX: torch.Tensor,\n    focal_x: torch.Tensor,\n    focal_y: torch.Tensor,\n) -\u003e torch.Tensor:\n    \"\"\"\n    각 가우시안의 2D 공분산 행렬 계산\n    \"\"\"\n    points = torch.cat(\n        [points, torch.ones(points.shape[0], 1, device=points.device)], dim=1\n    )\n    points_transformed = (points @ external_matrix)[:, :3]\n    limx = 1.3 * tan_fovX\n    limy = 1.3 * tan_fovY\n    x = points_transformed[:, 0] / points_transformed[:, 2]\n    y = points_transformed[:, 1] / points_transformed[:, 2]\n    z = points_transformed[:, 2]\n    x = torch.clamp(x, -limx, limx) * z\n    y = torch.clamp(y, -limy, limy) * z\n\n    J = torch.zeros((points_transformed.shape[0], 3, 3), device=covariance_3d.device)\n    J[:, 0, 0] = focal_x / z\n    J[:, 0, 2] = -(focal_x * x) / (z**2)\n    J[:, 1, 1] = focal_y / z\n    J[:, 1, 2] = -(focal_y * y) / (z**2)\n\n    # 초기에 원근 투영을 위해 설정한 대로 전치함\n    # 이제 우리가 다시 변환하는 것이므로\n    W = external_matrix[:3, :3].T\n\n    return (J @ W @ covariance_3d @ W.T @ J.transpose(1, 2))[:, :2, :2]\n```\n\n먼저, tan_fovY와 tan_fovX는 시야각의 반을 나타내는 tangent 값입니다. 이러한 값들을 사용하여 투영을 클램핑하여 화면 바깥으로 너무 많이 벗어났을 때 렌더에 영향을 미치지 않도록 합니다. 우리는 초기 순방향 변환으로부터 주어진 3D에서 2D로의 변환으로부터 야코비안을 유도할 수 있지만, 여러분이 귀찮을 일을 덜어드리기 위해 위에서 기대할 수 있는 유도를 보여드릴게요. 마지막으로, 우리가 회전 행렬을 변환하면서 처음에 전치했습니다만, 최종 공분산 계산을 반환하기 전에 다시 전치해야 합니다. EWA 스플래팅 논문에 따르면, 우리는 2D 이미지 평면에만 관심이 있으므로 세 번째 행과 열은 무시할 수 있습니다. 처음부터 그렇게 할 수 없었던 이유에 대해 궁금할 수도 있습니다. 대부분의 경우, 이는 완벽한 구로 표현되지 않을 것이기 때문에 각도에 따라 공분산 행렬 매개변수가 달라지기 때문입니다! 이제 올바른 관점으로 변환했으므로, 공분산 z축 정보는 쓸모없으며 버릴 수 있게 되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주어진 2D 공분산 행렬이 있으면 이미지의 임의의 픽셀에 각 가우시안이 미치는 영향을 계산할 수 있게 되었습니다. 이제 역 공분산 행렬을 찾아야 합니다. 선형 대수학에서 다시 상기해보면 2x2 행렬의 역행렬을 찾으려면 행렬식을 찾고 일부 용어를 재배열하면 됩니다. 이 코드를 통해 해당 프로세스를 안내해드릴게요.\n\n```js\ndef compute_inverted_covariance(covariance_2d: torch.Tensor) -\u003e torch.Tensor:\n    \"\"\"\n    역 공분산 행렬 계산\n\n    2x2 행렬의 경우\n    다음과 같이 주어질 때\n    [[a, b],\n     [c, d]]\n     행렬식은 ad - bc입니다.\n\n    역행렬을 구하려면 다음과 같이 용어를 재배열하고\n    행렬식의 역수를 곱하면 됩니다\n    [[d, -b],\n     [-c, a]] * (1 / 행렬식)\n    \"\"\"\n    행렬식 = (\n        covariance_2d[:, 0, 0] * covariance_2d[:, 1, 1]\n        - covariance_2d[:, 0, 1] * covariance_2d[:, 1, 0]\n    )\n    행렬식 = torch.clamp(행렬식, min=1e-3)\n    역_공분산 = torch.zeros_like(covariance_2d)\n    역_공분산[:, 0, 0] = covariance_2d[:, 1, 1] / 행렬식\n    역_공분산[:, 1, 1] = covariance_2d[:, 0, 0] / 행렬식\n    역_공분산[:, 0, 1] = -covariance_2d[:, 0, 1] / 행렬식\n    역_공분산[:, 1, 0] = -covariance_2d[:, 1, 0] / 행렬식\n    return 역_공분산\n```\n\n그리고 이제 이미지의 모든 픽셀에 대해 픽셀 강도를 계산할 수 있습니다. 그러나 이렇게 하는 것은 굉장히 느리고 불필요합니다. 예를 들어, (0,0)에서 스플래시가 (1000,1000)의 픽셀에 어떤 영향을 미치는지 계산하는 데 계산 시간을 낭비할 필요가 없습니다. 공분산 행렬이 거대하지 않다면 말이죠. 따라서 저자들은 각 스플래시마다 \"반경\"이라고 부르는 값을 계산하기로 결정했습니다. 아래 코드에서 볼 수 있듯이 각 축을 따라 고유값을 계산합니다(고유값은 변화를 나타냅니다). 그런 다음 가장 큰 고유값의 제곱근을 취하여 표준 편차를 얻고 3.0을 곱합니다. 이것은 분포의 99.7%를 3표준 편차 내에 포함시킵니다. 이 반경을 사용하면 스플래시가 닿는 x 및 y 값의 최솟값과 최댓값을 파악할 수 있습니다. 렌더링할 때 이러한 경계 내의 픽셀에 대해만 스플래시 강도를 계산하며 불필요한 계산을 피합니다. 상당히 똑똑한 방법이죠?\n\n```js\ndef compute_extent_and_radius(covariance_2d: torch.Tensor):\n    mid = 0.5 * (covariance_2d[:, 0, 0] + covariance_2d[:, 1, 1])\n    det = covariance_2d[:, 0, 0] * covariance_2d[:, 1, 1] - covariance_2d[:, 0, 1] ** 2\n    intermediate_matrix = (mid * mid - det).view(-1, 1)\n    intermediate_matrix = torch.cat(\n        [intermediate_matrix, torch.ones_like(intermediate_matrix) * 0.1], dim=1\n    )\n\n    max_values = torch.max(intermediate_matrix, dim=1).values\n    lambda1 = mid + torch.sqrt(max_values)\n    lambda2 = mid - torch.sqrt(max_values)\n    # 이제 고유값을 갖고 있으므로 최대 반경을 계산할 수 있습니다\n    max_radius = torch.ceil(3.0 * torch.sqrt(torch.max(lambda1, lambda2)))\n\n    return max_radius\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 모든 단계를 거쳐 우리는 그것을 렌더 단계에서 사용할 수 있는 전처리된 장면을 얻습니다. 간단히 말해 이제 2D에서의 포인트, 해당 포인트와 관련된 색, 2D에서의 공분산, 2D에서의 역공분산, 정렬된 깊이 순서, 각 스플랫에 대한 최소 x, 최소 y, 최대 x, 최대 y 값, 그리고 관련 투명도를 가지게 되었어요. 이러한 모든 구성 요소를 갖고 이미지 렌더링으로 넘어 갈 수 있습니다!\n\n- Kerbl, Bernhard, et al. “3d gaussian splatting for real-time radiance field rendering.” ACM Transactions on Graphics 42.4 (2023): 1–14.\n- Zwicker, Matthias, et al. “EWA splatting.” IEEE Transactions on Visualization and Computer Graphics 8.3 (2002): 223–238.","ogImage":{"url":"/assets/img/2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2_0.png"},"coverImage":"/assets/img/2024-06-20-APythonEngineersIntroductionto3DGaussianSplattingPart2_0.png","tag":["Tech"],"readingTime":9},{"title":"신경망 기본 이론과 구조 유형","description":"","date":"2024-06-20 19:07","slug":"2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes","content":"\n이 이야기에서는 신경망의 이론적 기초와 이로부터 파생된 기술, 그리고 PyTorch를 사용한 구현의 가장 중요한 측면을 높은 수준에서 리뷰하고 설명해보려고 합니다. 가능한 간단한 언어를 사용하여 설명하겠습니다. 또한 다른 문서에서 문서화한 사용 사례 예시를 소개할 예정입니다.\n\n신경망은 이름 그대로 뉴런으로 구성된 복잡한 시스템입니다. 이 네트워크의 힘은 이 인공 뉴런들 간의 상호 연결에서 나옵니다. 이러한 NN 알고리즘은 생물학적 시스템을 모방한다고 합니다.\n\n# 뉴런:\n\n신경망의 핵심은 뉴런입니다. 뉴런은 단순히 입력(변수) 집합을 받아 선형 및 비선형 변환을 적용하고 수학적 다변수 함수처럼 출력을 생성하는 수학 도구입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_0.png\" /\u003e\n\n첫 번째 레이어의 뉴런들에 대한 입력은 원래 세트의 데이터입니다. 각 샘플은 네트워크에 의해 독립적으로 동일한 방식으로 처리됩니다.\n\n네트워크의 기본 구조는 다음과 같습니다: 각각의 뉴런으로 구성된 여러 레이어로, 각 레이어마다 독립적으로 구성됩니다. 첫 번째 레이어는 데이터 원본에서 공급받고, 마지막 레이어는 출력으로 공급하며, 중간 레이어는 이전 레이어에서 공급받고 다음 레이어로 이어집니다.\n\n일부 아키텍처는 레이어 간에 변형을 추가하거나 피드백 루프를 도입한 이 모델에 변형을 도입할 수 있습니다. 나중에 이에 대해 논의할 예정입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Neural Networks Basic Theory and Architecture Types 1](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_1.png)\n\n각 뉴런은 선형 부분과 비선형 부분으로 구성됩니다. 선형 부분은 표준 선형 방정식이며, 비선형 변환은 네트워크 및 층에 따라 다를 수 있습니다.\n\n구체적으로, 레이어는 입력 벡터(크기 n)로 구성되며, 이는 가중치 행렬(크기 nxm, 여기서 n은 입력의 크기이고 m은 레이어의 뉴런 수입니다)에 의해 곱해지고 결과는 크기 m인 다른 벡터로 반환됩니다. 이 결과는 자유 매개 변수 벡터에 추가됩니다. 전체 결과는 비선형 함수를 통해 전달되며, 이를 활성화 함수라고 합니다.이 프로세스의 출력은 다음 레이어로 전달됩니다.\n\n![Neural Networks Basic Theory and Architecture Types 2](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또는 구체적으로:\n\n![Neural Networks Basic Theory and Architecture Types](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_3.png)\n\n이 시스템이 얼마나 복잡해질 수 있는지를 보여주기 위해, 두 번째 층의 출력에서 수식이 어떻게 보일지 알아보겠습니다:\n\n![Neural Networks Basic Theory and Architecture Types](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n실제 출력(목표 변수)를 재현하는 최적의 가중치와 자유 매개변수 값을 찾는 것이 바로 이어지는 교육 과정의 전부적인 목적입니다. 원본 값과 예측된 값 사이의 차이를 측정하기 위해 손실 함수를 도입합니다. 이로써, 모든 신경망은 근본적으로 지도 회귀 문제로 전환됩니다.\n\n활성화 함수는 다양한 사용 사례를 충족하는 함수 세트에서 선택됩니다. 높은 수준에서 가장 많이 사용되는 것은 ReLU(0보다 큰 값을 유지하며 음수는 0으로 설정), Sigmoid 및 Tanh입니다.\n\n# 학습 과정:\n\n언급했듯이, 각 계층별로 많은 매개변수의 최적 값을 찾는 것이 목표입니다. 이를 위해 예측된 Y와 실제 Y 간의 관계를 나타내는 함수(손실 함수)가 선택됩니다. 최적화 문제와 마찬가지로 목표는 이 기능이 최소값일 때의 지점을 찾기 위해 고정됩니다. 일부 손실 함수의 예는 평균 제곱 오차, 제곱근 평균 제곱 오차, 평균 절대 오차 및 이진 교차 엔트로피가 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최적화 문제를 해결하는 데 사용된 알고리즘은 그래디언트 강하법의 변형으로, 이는 다변수 적용에서 함수의 최소값을 찾는 전형적인 미적분 문제의 수치 구현입니다. 알고리즘은 각 반복에서 함수의 그래디언트 값을 추정하고 다음과 같은 방식으로 매개변수를 업데이트합니다:\n\n![image](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_5.png)\n\n이 프로세스는 주어진 반복 횟수(에폭)만큼 반복되며, 각 실행에서 손실 값이 감소하는지 확인합니다.\n\n학습 속도는 미리 설정해야 하는 하이퍼파라미터입니다. 학습 속도에 대한 중요한 사항은 너무 높게 설정해서는 안 된다는 것입니다. 그렇지 않으면 손실 값이 진동을 시작하고 결코 함수의 최소값을 찾지 못할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Neural Networks Theory](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_6.png)\n\n일반적으로 매개변수는 무작위 변수를 사용하여 초기화됩니다. 이 변수는 가우시안 분포를 따릅니다. 모든 입력이 독립적이며 레이어에 무한 개수의 뉴런이 있는 이상적인 경우에는 출력과 훈련된 매개변수도 가우시안 분포를 형성합니다.\n\n이러한 분포는 매개변수, 변수 또는 출력으로 형성된 다변량 공간 상의 파형패킷으로 볼 수 있습니다. 이는 양자장론에서 자유 입자를 모델링하기 위해 사용되는 수학적 구조와 유사합니다. 양자장론에서 상호작용으로 나타나는 작은 편차가 있는 것과 같이, 신경망에서는 변수간의 종속성과 레이어 당 유한 개수의 뉴런 삽입에 의해 생성됩니다. 특히, 네트워크 구성원 간의 내부 또는 보이지 않는 구조에 의해 생성되는 이러한 편차는 시스템의 예측력의 원천입니다. 그러나 이러한 편차가 너무 커지면 시스템이 발산하여 혼돈스럽게 됩니다.\n\n양자장론과 마찬가지로, 자유(가우시안) 경우에서의 작은 편차로 인한 문제들에 대한 수학적 해석을 위해 섭동 이론을 사용할 수 있습니다. 물리적 입자간의 상호작용을 이해하는 데 사용되는 수학은 신경망의 내부 동작을 이해하는 데 활용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 접근 방식에 대해 더 읽고 싶다면 원본 논문을 참고할 수 있어요: [2307.03223] Neural Network Field Theories: Non-Gaussianity, Actions, and Locality (arxiv.org)\n\n다시 본론으로 돌아와서, 신경망을 설계할 때 결정되어야 할 여러 가지 결정 사항 또는 하이퍼파라미터가 있어요. 이들은 다음과 같아요:\n\n- 각 층의 입력과 출력 수, 단, 첫 번째 층의 입력은 변수의 수, 마지막 층의 출력은 해결할 문제의 성격에 따라 정해지며 각 내부 또는 숨겨진 층의 출력은 다음 층의 입력이에요.\n- 신경망의 층 수.\n- 각 층의 활성화 함수.\n- 손실 함수.\n- 기울기 알고리즘.\n- 학습률.\n- 아키텍처(다음 세그먼트에서 탐구할 사항)\n\n다음으로, 가장 일반적인 신경망 아키텍처 몇 가지, 각각의 고수준 설명, 및 샘플 사용 사례를 살펴볼게요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 신경망과 기계 학습 내부의 수학을 더 잘 이해하고 싶다면 Ian Goodfellow, Yoshua Bengio, 그리고 Aaron Courville의 책을 참고하실 수 있어요. 해당 책은 Deep Learning (deeplearningbook.org)에서 구할 수 있어요. 그리고, Goodfellow은 적대적 생성 신경망(Generative Adversarial Networks)의 발명과도 함께 언급되어 있어요.\n\n# 다중 계층 퍼셉트론 (MLP)\n\n이것은 신경망의 가장 기본적인 아키텍처이며, 각 계층이 이전 계층에 의존하는 선형 구조로 형성되어 있어요. 또한 변수들 사이의 특정한 관계를 고려하지 않아요. MLP는 예측 변수들이 서로 의존하지 않는 문제에서 유용하게 사용됩니다. 예를 들어, 나이, 연봉, 교육 또는 성별과 같은 요소로 구성된 데이터셋에 대해요.\n\n![Neural Networks Basic Theory and Architecture Types](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 코드 블록은 PyTorch 패키지를 사용하여 Python에서 다층 퍼셉트론을 정의한 샘플입니다:\n\n```python\nimport torch.nn as nn\n\nclass SimpleClassifier(nn.Module):\n    def __init__(self):\n        super(SimpleClassifier, self).__init__()\n#과적합을 줄이기 위해 드롭아웃 레이어를 도입합니다.\n#드롭아웃은 신경망에게 층 사이의 데이터를 무작위로 삭제하여 변동성을 도입하도록 지시합니다.\n        self.dropout = nn.Dropout(0.1)\n#레이어는 열의 두 배 정도로 시작하고 다음 레이어로 증가한 다음 다시 2로 감소하는 것을 권장합니다.\n#이 경우 응답은 이진입니다.\n        self.layers = nn.Sequential(\n            nn.Linear(input_size, 250),\n            nn.Linear(250, 500),\n            nn.Linear(500, 1000),\n            nn.Linear(1000, 1500),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(1500, 1500),\n            nn.Sigmoid(),\n            self.dropout,\n            nn.Linear(1500, 1500),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(1500, 1500),\n            nn.Sigmoid(),\n            self.dropout,\n            nn.Linear(1500, 1500),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(1500, 1500),\n            nn.Sigmoid(),\n            self.dropout,\n            nn.Linear(1500, 1500),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(1500, 500),\n            nn.Sigmoid(),\n            self.dropout,\n            nn.Linear(500, 500),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(500, 500),\n            nn.Sigmoid(),\n            self.dropout,\n#마지막 레이어는 응답 변수가 이진(0, 1)이기 때문에 2를 출력합니다.\n#다중 클래스 분류의 출력은 클래스 수와 같아야 합니다.\n            nn.Linear(500, 2),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\n#모델 정의\nmodel = SimpleClassifier()\n```\n\n이 모델을 사용한 전형적인 학습 루프는 다음 블록에서 나타납니다:\n\n```python\n#모델 로드\nmodel = SimpleClassifier()\nmodel.train()\n\n#학습 파라미터(사이클 수 및 학습률)입니다.\nnum_epochs = 100\nlearning_rate = 0.00001\n#과적합을 줄이기 위해\nregularization = 0.0000001\n\n#손실 함수\ncriterion = nn.CrossEntropyLoss()\n\n#기울기를 찾는 알고리즘\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=regularization)\n\n#이 코드는 학습 루프를 수행하는 동안 최상의 모델을 유지합니다.\nbest_model_wts = copy.deepcopy(model.state_dict())\nbest_acc = 0.0\nbest_f1 = 0.0\nbest_epoch = 0\nphases = ['train', 'val']\ntraining_curves = {}\nepoch_loss = 1\nepoch_f1 = 0\nepoch_acc = 0\n\n#데이터셋은 학습, 검증 및 테스트로 분할됩니다.\nfor phase in phases:\n    training_curves[phase+'_loss'] = []\n    training_curves[phase+'_acc'] = []\n    training_curves[phase+'_f1'] = []\n\n#이것은 학습 루프입니다.\nfor epoch in range(num_epochs):\n    print(f'\\n에포크 {epoch+1}/{num_epochs}')\n    print('-' * 10)\n    for phase in phases:\n        if phase == 'train':\n            model.train()\n        else:\n            model.eval()\n        running_loss = 0.0\n        running_corrects = 0\n        running_fp = 0\n        running_tp = 0\n        running_tn = 0\n        running_fn = 0\n        #데이터 반복\n        for inputs, labels in dataloaders[phase]:\n            inputs = inputs.view(inputs.shape[0], -1)\n            inputs = inputs\n            labels = labels\n\n            #매개변수의 기울기를 0으로 설정\n            optimizer.zero_grad()\n\n            #순방향 패스 (위의 차트 참조)\n            with torch.set_grad_enabled(phase == 'train'):\n                outputs = model(inputs)\n                _, predictions = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n\n                #역방향 패스 (학습 중에만)\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n\n                #통계. f1 메트릭을 사용합니다.\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(predictions == labels.data)\n                running_fp += torch.sum((predictions != labels.data) \u0026 (predictions \u003e= 0.5))\n                running_tp += torch.sum((predictions == labels.data) \u0026 (predictions \u003e= 0.5))\n                running_fn += torch.sum((predictions != labels.data) \u0026 (predictions \u003c 0.5))\n                running_tn += torch.sum((predictions == labels.data) \u0026 (predictions \u003c 0.5))\n                print(f'에포크 {epoch+1}, {phase:5} 손실: {epoch_loss:.7f} F1: {epoch_f1:.7f} 정확도: {epoch_acc:.7f} 부분 손실: {loss.item():.7f} 최상의 f1: {best_f1:.7f}')\n\n        #손실, 정확도 및 f1 메트릭 계산\n        epoch_loss = running_loss / dataset_sizes[phase]\n        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n        epoch_f1 = (2 * running_tp.double()) / (2 * running_tp.double() + running_fp.double() + running_fn.double() + 0.0000000000000000000001)\n        training_curves[phase+'_loss'].append(epoch_loss)\n        training_curves[phase+'_acc'].append(epoch_acc)\n        training_curves[phase+'_f1'].append(epoch_f1)\n\n        print(f'에포크 {epoch+1}, {phase:5} 손실: {epoch_loss:.7f} F1: {epoch_f1:.7f} 정확도: {epoch_acc:.7f} 최상의 f1: {best_f1:.7f}')\n\n        if phase == 'val' and epoch_f1 \u003e= best_f1:\n            best_epoch = epoch\n            best_acc = epoch_acc\n            best_f1 = epoch_f1\n            best_model_wts = copy.deepcopy(model.state_dict())\n\nprint(f'최상의 val F1: {best_f1:5f}, 최상의 val 정확도: {best_acc:5f}, 에포크 {best_epoch}')\n\n#최상의 모델 가중치로드\nmodel.load_state_dict(best_model_wts)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래의 글에서는 실제 데이터를 사용하여 MLP를 구현한 예시를 확인할 수 있습니다: [Neural Networks와 Pytorch를 사용하여 자동 복구 실패 예측하기 (저자: Greg Postalian-Yrausquin | 2024년 6월 | Towards AI (medium.com)](https://medium.com)\n\n더 많은 정보는 위키피디아 페이지에서 찾을 수 있습니다: [다층 퍼셉트론 (Multilayer perceptron) — Wikipedia](https://en.wikipedia.org/wiki/Multilayer_perceptron)\n\n# 합성곱 신경망 (CNN):\n\n전형적인 다층 퍼셉트론은 입력이 필드인 경우 성능이 좋지 않습니다. 여기서 필드란 점들 간의 관계(함수, 연속성을 통해)가 있는 구조를 말합니다. 예를 들어, 금속 판의 온도는 한 지점에 열원이 연결되어 있는 경우 열원에서 더 멀리 있는 위치로 갈수록 그래디언트를 따를 것입니다. 이 2차원 예시에서 표면의 온도는 그리드(행렬)로 나타낼 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_8.png)\n\n이러한 구조들은 3D일 수도 있습니다 (여러 개가 서로 위에 쌓인 것을 상상해보세요) 또는 우리가 원하는 대로 복잡할 수 있습니다. MLPs는 훈련 중에 데이터의 내부 구조를 잃어버리므로 이를 모델링하는 데 좋지 않습니다. 그와는 반대로 CNNs는 원래의 관계를 보존합니다.\n\nPython에서는 이미지가 행렬로 저장됩니다. 여기서 행과 열은 위치를 나타내고 숫자는 강도를 측정하는 값입니다. 컬러 이미지의 경우 각 이미지에 대해 RGB 색상 인코딩 형식에 대한 값을 저장하는 3개의 행렬이 사용됩니다. 수학에서 이러한 다차원 행렬은 Tensor라고 불리며 벡터 함수로도 볼 수 있습니다 (출력이 벡터의 모양으로 나오는 것), 이 경우 출력 벡터의 좌표는 RGB 색상값입니다.\n\n이러한 이유로 CNN은 이미지 및 비디오 데이터를 모델링하는 데 널리 사용됩니다. CNN의 아이콘은 이미지 분류입니다. 이 기사에서는 그 목적으로 CNN 사용 예제를 볼 수 있습니다: Convolutional Neural Networks in PyTorch: Image Classification | by Greg Postalian-Yrausquin | Jun, 2024 | Towards AI (medium.com).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n합성곱 신경망은 네트워크 내에 하나 이상의 합성 계층이 존재하는 것으로 정의됩니다. 이들은 데이터 내부에서 창 또는 행렬(커널)을 슬라이딩하여 원소별 곱셈을 수행하고 커널 내부 값의 합을 구하는 수학 연산입니다. 패딩을 도입하여 원래 데이터 매트릭스 크기의 축소를 고려할 수 있습니다.\n\n![CNN Architecture](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_9.png)\n\nCNN에는 다른 종류의 계층도 소개되는데, 예를 들면: Max pool (지도의 일부분의 최대값을 얻어 데이터 크기를 줄임), flatten (데이터 매트릭스를 벡터로 변환하여 네트워크 끝에 사용되거나 표준 네트워크로 계속해서 훈련) 및 unflatten (이전 과정을 역으로 수행).\n\n다음 샘플 코드는 PyTorch에서 CNN 클래스의 정의입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom torch.nn.modules.flatten import Flatten\nclass CNNClassifier(nn.Module):\n    def __init__(self):\n        super(CNNClassifier, self).__init__()\n        self.dropout = nn.Dropout(0.05)\n        self.pipeline = nn.Sequential(\n            #in channels is 1, because the input is grayscale\n            nn.Conv2d(in_channels = 1, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 5, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            #dropout to introduce randomness and reduce overfitting\n            self.dropout,\n            #reduce and flat the tensor before applying the flat layers\n            nn.MaxPool2d(kernel_size = 2, stride = 2),\n            nn.Flatten(),\n            nn.Linear(500, 50),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(50, 50),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(50, 10),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(10, 10),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(10, 5),\n        )\n\n    def forward(self, x):\n        return self.pipeline(x)\n\nmodel = CNNClassifier()\n```\n\nCNN에 대해 더 자세히 알아보기 좋은 정보를 찾는 것을 시작하는 데 좋은 곳인 Wikipedia의 CNN에 대한 항목을 찾았어요: [Convolutional neural network — Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network)\n\n# 오토인코더\n\n아키텍처의 하위 클래스로 오토인코더가 있습니다. 입력과 출력의 수가 동일한 특정 구성으로 상상할 수 있습니다. 모델은 입력된 데이터를 재현하는 방법을 학습하도록 구성되어 있으며 한 개 이상의 숨겨진 레이어를 통해 통과합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델은 두 부분으로 설계되어 있습니다. 입력을 다른 표현으로 변환하는 Encoder와 이 표현을 기반으로 입력의 버전을 재구성하는 Decoder입니다. 아이디어는 재구성이 초기 데이터와 가능한 한 유사해야 한다는 것입니다.\n\n이 네트워크에서 목표는 동일한 입력 데이터이기 때문에 이들은 사실상 감독되지 않은 학습 방법입니다. 예를 들어 Autoencoder 아키텍처는 생성적 AI 작업의 일부로 사용됩니다.\n\n자연어 처리(NLP)에서 오토인코더는 단어 또는 문장의 임베딩(표현)을 생성하는 데 사용됩니다. 이 텍스트의 숫자 표현은 그 후 분류, 거리 계산 등과 같은 하향 작업에서 사용됩니다.\n\nNLP에서 오토인코더를 사용하는 한 가지 훌륭한 방법은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Neural Network Example](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_10.png)\n\n이 문서에서는 오토인코더의 정의 예시를 찾을 수 있습니다: Neural networks: encoder-decoder example (autoencoder) | 작성자 Greg Postalian-Yrausquin | 날짜 2024년 6월 | Medium. 여기서 모델이 이미지를 재구성하는 데 사용됩니다.\n\n```js\n# 훈련 이미지의 채널 수. 컬러 이미지의 경우 3개입니다\nnc = 3\n\n# 표현의 크기\nnr = 1000\n\n# 디코더의 시작점의 크기\nnz = 50\n\nclass Encdec(nn.Module):\n    def __init__(self, nc, nz, nr):\n        super(Encdec, self).__init__()\n# 이것이 인코더입니다\n        self.encoder = nn.Sequential(\n            nn.Conv2d(in_channels = nc, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 1, kernel_size = 5, stride = 1, padding=1),\n            nn.Flatten(),\n            nn.Linear(2916, 3000),\n            nn.ReLU(),\n            nn.Linear(3000, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, nr),\n         )\n# 이것이 디코더입니다\n        self.decoder = nn.Sequential(\n            nn.Linear(nr, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, 500),\n            nn.ReLU(),\n            nn.Linear(500, nz*64*64),\n            nn.Unflatten(1, torch.Size([nz, 64, 64])),\n            nn.Conv2d(in_channels = nz, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = 10, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = 10, out_channels = nc, kernel_size = 5, stride = 1, padding=1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(10092, 2000),\n            nn.ReLU(),\n            nn.Linear(2000, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, 500),\n            nn.ReLU(),\n            nn.Linear(500, nc*64*64),\n            nn.Unflatten(1, torch.Size([nc, 64, 64])),\n            nn.Tanh()\n         )\n\n    def encode(self, x):\n        return self.encoder(x)\n\n    def decode(self, x):\n        return self.decoder(x)\n\n    def forward(self, input):\n        return self.decoder(self.encoder(input))\n\nnetEncDec = Encdec(nc, nz, nr)\n```\n\n자세한 내용은 위키피디아에서 오토인코더 아키텍처에 대해 더 알아보기 시작점으로 참조할 수 있습니다: [오토인코더 - 위키백과](https://ko.wikipedia.org/wiki/%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인코더-디코더 메커니즘의 일반적이고 잘 알려진 구현은 트랜스포머 입니다. 이 아키텍처는 2017년 구글의 데이터 과학자들이 발표한 “Attention is all you need” [1706.03762] 논문에서 소개되었습니다. 트랜스포머는 NLP에서 널리 사용되며, 입력 및 출력 집합에 대한 임베딩 생성부터 시작하여 여러 단계로 구성됩니다. 이 집합 모두에 대해 위치 정보를 유지할 수 있도록 처리된 후에, 초기 오토인코더에는 포함되어 있지 않은 단계가 포함됩니다. 이는 반복의 오버헤드 없이 RNN과 동일한 이점을 제공합니다. 그 다음 데이터는 인코딩 프로세스(어텐션 스택)를 거치고, 디코딩 단계(두 번째 어텐션 스택)에서 출력과 비교됩니다. 마지막 단계는 소프트맥스 변환을 적용하는 것입니다.\n\n![트랜스포머 아키텍처](/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_11.png)\n\n트랜스포머 아키텍처: 원본 논문 \"Attention is all you need\"에서 가져온 다이어그램입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n전이 학습 패러다임의 매우 흥미로운 그리고 유용한 구현 중 하나는 Google이 만든 BERT(Bidirectional Encoder Representations for Transformers)입니다. 영어 처리를 위해 트랜스포머를 처음부터 훈련하는 것은 거대한 작업이 될 수 있지만 다행히도 다양한 용도에 맞게 사전 훈련된 모델을 다운로드하고 적용할 수 있습니다 (이러한 모델을 다운로드하고 적용하는 방법은 Huggingface 페이지를 참조하세요): 모델 다운로드 (huggingface.co)\n\n# 순환 신경망:\n\nRNN은 신경망의 비선형 시도로 간주될 수 있습니다. RNN에서는 한 레이어가 자신에게 영향을 미칠 수 있습니다 (역행 효과가 있습니다). 이 작용은 시퀀스 형식으로 된 데이터를 모델링하는 데 이상적이라고 할 수 있습니다. 이러한 데이터의 가장 좋은 예는 텍스트 스트림이며, 그 이유로 NLP에서 가장 효율적인 트랜스포머가 도입될 때까지 주로 사용되었습니다. RNN은 음성 및 필기 인식에도 구현되어 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_12.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRNN(순환 신경망)은 계산적으로 요구가 높을 수 있는 것 외에, 비선형성에 의해 확대되는 전파 오류와 실제로 이전 단계의 매우 짧은 메모리를 유지하는 사라져버리는 그래디언트와 같은 다른 문제가 있습니다. 이러한 문제를 해결하기 위해 LSTM(Long-Short Term Memory) 및 GRU(Gated Recurrent Units)와 같은 RNN 아키텍처의 더 복잡한 파생물이 소개되었습니다.\n\nRNN의 응용 예는 다음 글에서 소개됩니다: RNN: PyTorch에서 Sentiment Analysis를 위한 기본 순환 신경망 | Greg Postalian-Yrausquin 저 | 2024년 6월 | Towards AI (medium.com)\n\nPyTorch에서 이 네트워크의 정의는 다음과 같습니다:\n\n```python\n# 신경망의 정의입니다. 보시다시피 RNN 정의 하나만 있습니다.\n# 2개의 레이어와 하나의 선형 레이어가 포함됩니다.\n# 오버피팅을 방지하기 위해 드롭아웃 및 정규화가 도입되었습니다\n\nclass RNNClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNNClassifier, self).__init__()\n        self.hidden_size = hidden_size\n        self.RNN = nn.RNN(input_size, hidden_size, num_layers=2, dropout=0.2)\n        self.fc = nn.Linear(hidden_size, output_size)\n        pass\n\n    def forward(self, input):\n        output, hn = self.RNN(input)\n        output = self.fc(output)\n        return output, hn\n\nmodel = RNNClassifier(insize, 8, 2)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 생성 적대 신경망 (Generative Adversarial Networks, GAN):\n\n이것은 MLP, RNN 또는 CNN들의 조합에서 형성될 수 있는 또 다른 복합 아키텍처입니다. 2014년 Ian Goodfellow와 그의 동료들에 의해 작성되었습니다 (원본 논문은 Generative Adversarial Nets (nips.cc)에서 확인할 수 있으며 [1701.00160] NIPS 2016 Tutorial: Generative Adversarial Networks (arxiv.org)에서 튜토리얼을 참조할 수 있습니다). GAN은 두 가지 다른 모델이 훈련되는 매우 똑똑한 신경망 응용 프로그램으로, 하나는 원래 데이터셋을 기반으로 샘플을 생성하는 것을 목표로하고 다른 하나는 이 첫 번째 모델에 대항하여 샘플이 실제인지 가짜인지를 추측합니다.\n\nGAN은 생성적 AI 작업(모델을 기반으로 실제 데이터를 생성하는 것)에 사용됩니다. 텍스트, 이미지 또는 비디오 생성 등이 해당될 수 있습니다. 자세한 설명은 원본 샘플을 기반으로 새로운 객체를 생성해야 하는 생성자(Generator) 네트워크 및 인공적으로 생성된 샘플과 실제 샘플을 구별하도록 훈련된 구분자(Discriminator)를 포함하고 있습니다. 학습의 여러 반복 후에 구분자가 실제 데이터와 가짜 데이터를 구분하는 데 어려움을 겪도록 함으로써 신뢰할 수 있는 제품 생성을 보장합니다.\n\n이것은 PyTorch에서 두 적대적 신경망에 대한 클래스 정의 샘플입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 트레이닝 이미지 내의 채널 수. 컬러 이미지의 경우, 채널 수는 3입니다.\nnc = 3\n\n# z 잠재 벡터의 크기 (즉, 생성기 입력의 크기)\nnz = 100\n\n# 생성기의 특징 맵 크기\nngf = 64\n\n# 판별자의 특징 맵 크기\nndf = 64\n\n# 이것은 진짜와 가짜 이미지를 분리하려는 작업을 수행하는 판별자 네트워크입니다.\nclass Discriminator(nn.Module):\n    def __init__(self, nc, ndf):\n        super(Discriminator, self).__init__()\n        self.pipeline = nn.Sequential(\n# nc는 3이며, 입력은 텐서 3x64x64(64x64의 컬러 이미지이며, 각 컬러 이미지에는 3개의 텐서가 필요)입니다.\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2),\n# 출력은 크기가 1인 벡터입니다.\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.pipeline(input)\n\n\nclass Generator(nn.Module):\n    def __init__(self, nc, nz, ngf):\n        super(Generator, self).__init__()\n        self.pipeline = nn.Sequential(\n# 생성기의 입력은 무작위 생성된 이미지입니다. 이 경우 채널 수가 100이므로 nz는 100입니다.\n            nn.ConvTranspose2d(nz, ngf * 16, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 16),\n            nn.ReLU(),\n            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(),\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(),\n            nn.ConvTranspose2d(ngf * 4, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(),\n# 출력은 트레이닝 이미지와 동일한 차원의 이미지입니다. 따라서 출력은 크기가 nc여야 합니다.\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        return self.pipeline(input)\n\n\n저는 다음 기사에서 이미지 생성을 위한 GAN을 구현했습니다: GAN: training a Generative Adversarial Network for image generation | by Greg Postalian-Yrausquin | Jun, 2024 | Medium\n\n이것들은 머신 러닝을 위한 신경망의 기본입니다. 하지만 모든 아키텍처에 대한 완전한 설명은 아닙니다. 이 주제는 아주 거대하고 매혹적이며, 새로운 기술과 알고리즘이 지속적으로 등장하고 있는 폭발적인 성장을 이루고 있습니다. 이러한 많은 것들은 이 문서에서 설명된 아키텍처의 수정이나 결합입니다.\n","ogImage":{"url":"/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_0.png"},"coverImage":"/assets/img/2024-06-20-NeuralNetworksBasicTheoryandArchitectureTypes_0.png","tag":["Tech"],"readingTime":22},{"title":"비디오 주석 도구 비전-언어 모델과 액티브 러닝을 활용하여 효율적으로 비디오 분류기를 만들 수 있는 프레임워크","description":"","date":"2024-06-20 19:05","slug":"2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning","content":"\n\n아미르 지아이, 아니시 바르타카비, 켈리 그리그스, 유진 로크, 이본 주크스, 알렉스 알론소, 비 이엔가르, 안나 풀리도\n\n# 소개\n\n## 문제\n\n고품질 및 일관된 주석은 견고한 머신 러닝 모델의 성공적인 개발에 필수적입니다. 머신 러닝 분류기를 훈련시키기 위한 전통적인 기술은 자원이 많이 필요합니다. 이들은 도메인 전문가가 데이터 세트에 주석을 달고, 그 후 데이터 과학자에게 모델을 훈련시키고 결과를 검토하고 변경하는 과정을 포함합니다. 이 라벨링 프로세스는 종종 시간이 많이 소요되고 비효율적이며, 때로는 몇 차례의 주석 주기 후 중단됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 영향\n\n결과적으로 고품질 데이터셋에 주석을 다는 것에 투자하는 노력이 복잡한 모델 및 알고리즘 방법을 반복하고 성능을 개선하고 엣지 케이스를 수정하는 것보다 적습니다. 이로 인해 머신러닝 시스템은 빠르게 복잡성을 증가시킵니다.\n\n또한, 시간과 자원에 대한 제한으로 인해 도메인 전문가 대신 제3자 주석 달기를 활용하는 경우가 많습니다. 이러한 주석 달기 작업자는 모델의 의도된 배포나 사용 방법에 대한 깊은 이해 없이 레이블 작업을 수행하며, 특히 주관적인 작업에서 과제인 또는 어려운 예제를 일관되게 주석 달기가 어려울 수 있습니다.\n\n이로 인해 도메인 전문가와의 여러 번의 검토 회의가 필요해 예상치 못한 비용과 지연이 발생합니다. 이러한 긴 주기는 모델 이탈로 이어질 수 있으며, 엣지 케이스를 해결하고 새 모델을 배포하는 데 더 오랜 시간이 걸리기 때문에 유용성과 이해 관계자 신뢰를 해치는 결과를 초래할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 해결 방법\n\n우리는 도메인 전문가들의 직접적인 참여을 통한 인간 중심 시스템 사용으로 이러한 실용적인 도전 과제를 해결할 수 있다고 제안합니다. 우리는 활발한 학습 기법과 대규모 비전-언어 모델의 제로샷 기능을 활용한 새로운 Video Annotator (VA) 프레임워크를 소개합니다. 이를 통해 사용자들이 더 어려운 예제에 집중하도록 안내하고 모델의 샘플 효율성을 향상시키며 비용을 낮출 수 있습니다.\n\nVA는 모델 구축을 데이터 주석 프로세스에 매끄럽게 통합하여 배포 전 모델의 사용자 검증을 용이하게 하여 신뢰 구축을 돕고 소유감을 유발합니다. 또한 VA는 연속 주석 프로세스를 지원하여 사용자가 신속하게 모델을 배포하고 제작 과정에서 모델의 품질을 모니터링하고 몇 가지 추가 예제를 주석 처리하고 새로운 모델 버전을 배포하여 엣지 케이스를 신속히 수정할 수 있습니다.\n\n이 self-service 아키텍처는 데이터 과학자나 제3자 주석자의 활발한 참여 없이 사용자들이 빠른 반복을 통해 개선할 수 있도록 돕습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 비디오 이해\n\n저희는 VA를 디자인하여 비디오 세그먼트 내에서 시각적인 이미지, 개념 및 이벤트를 식별해야 하는 세밀한 비디오 이해를 지원합니다. 비디오 이해는 검색 및 발견, 맞춤화 및 홍보 자산의 생성과 같은 다양한 응용 프로그램에 근본적으로 중요합니다. 저희의 프레임워크는 이진 비디오 분류기의 확장 가능한 세트를 개발하여 비디오 이해를 위해 기계 학습 모델을 효율적으로 훈련할 수 있도록 합니다. 이는 거대한 콘텐츠 카탈로그의 확장 가능한 점수화 및 검색을 가능하게 합니다.\n\n## 비디오 분류\n\n비디오 분류는 임의 길이의 비디오 클립에 레이블을 할당하는 작업으로, 일반적으로 확률 또는 예측 점수와 함께 나타납니다. Fig 1에서 설명되듯이, 고립된 분류 점수와 검색이 가능한 포괄적인 콘텐츠 카탈로그가 함께 동작합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_0.png)\n\n## 확장 가능한 비디오 분류기를 통한 비디오 이해\n\n이진 분류는 독립성과 유연성을 제공하여 다른 모델과 독립적으로 하나의 모델을 추가하거나 개선할 수 있도록 합니다. 또한 사용자에게 이해하기 쉽고 구축하기 쉬운 추가적인 혜택이 있습니다. 여러 모델의 예측을 결합하면 동영상 콘텐츠의 다양한 수준에서 더 깊은 이해를 얻을 수 있습니다. (그림 2 참조)\n\n![이미지](/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_1.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 비디오 주석 도구 (VA)\n\n이 섹션에서는 비디오 분류기를 구축하는 VA의 세 단계 프로세스에 대해 설명합니다.\n\n## 단계 1 - 검색\n\n사용자들은 주석 프로세스를 부트스트랩하기 위해 대형이고 다양한 말뭉치 내에서 초기 예제 세트를 찾아 시작합니다. 우리는 비전-언어 모델의 비디오 및 텍스트 인코더에서 임베딩을 추출하는 데 도움을 주는 텍스트-비디오 검색을 활성화합니다. 예를 들어 \"건물의 넓은 샷\"을 검색하여 (\"건물의 넓은 샷\"라고 작성된 그림 3에 설명된) 설립 샷 모델에서 작업하는 주석 작업자가 프로세스를 시작할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_2.png)\n\n## 단계 2 — 액티브 러닝\n\n다음 단계는 전통적인 액티브 러닝 루프를 포함합니다. VA는 그런 다음 비디오 임베딩 위에 가벼운 이진 분류기를 구축하고, 이 분류기는 대상 코퍼스 내의 모든 클립에 점수를 매기도록 하며, 피드 내의 일부 예시를 주어 더 많은 주석 및 개선을 위해 제시합니다. 이는 그림 4에서 설명된 내용입니다.\n\n![이미지](/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_3.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최고 점수의 긍정적 및 부정적 피드는 각각 가장 높은 점수와 가장 낮은 점수를 가진 예제를 보여줍니다. 저희 사용자들은 이를 통해 분류기가 훈련 초기에 올바른 개념을 잘 파악하고, 훈련 데이터의 편향 사례를 식별하여 이후 수정할 수 있었다고 보고했습니다. 또한, 모델이 확신을 가지지 못하는 \"중간\" 예제를 표시합니다. 이러한 피드는 흥미로운 극단적 사례를 발견하는 데 도움이 되며, 추가적인 개념을 레이블 지정할 필요성을 영감을 줍니다. 마지막으로, 무작위 피드에는 임의로 선택된 클립이 포함되어 다양한 예제를 주석 처리하는 데 도움이 되어 일반화에 중요합니다.\n\n주석 작업자는 언제든지 피드 중 하나에 추가적인 클립에 레이블을 지정하고 새로운 분류기를 구축하고 원하는만큼 반복할 수 있습니다.\n\n## 단계 3 — 리뷰\n\n마지막 단계에서는 사용자에게 모든 주석이 달린 클립이 제시됩니다. 주석 실수를 발견하고 추가 주석을 위한 아이디어와 개념을 확인할 수 있는 좋은 기회입니다. 이 단계에서 사용자들은 종종 단계 1로 돌아가거나 주석을 보다 정제하기 위해 단계 2로 돌아갑니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 실험\n\nVA를 평가하기 위해 3명의 비디오 전문가에게 500k 샷의 비디오 코퍼스에서 다양한 56가지 레이블을 주석 달아달라고 요청했습니다. VA를 몇 가지 기준선 방법의 성능과 비교한 결과, VA가 높은 품질의 비디오 분류기를 생성하는 데 효과적임을 관찰했습니다. 그림 5에서 VA의 성능을 주석 달린 클립 수의 함수로 기준선과 비교했습니다.\n\n![그림](/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_4.png)\n\n더 많은 정보 및 VA 및 우리의 실험에 대한 자세한 내용은 이 논문에서 확인하실 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n저희는 비디오 어노테이터(VA)를 소개했습니다. VA는 기계 학습 분류기 교육에 관련된 다양한 과제를 해결하는 대화형 프레임워크입니다. VA는 대형 비전-언어 모델의 영점(Zero-shot) 능력과 능동 학습 기술을 활용하여 샘플 효율성을 향상시키고 비용을 줄입니다. VA는 비디오 분류 데이터 집합에 어노테이팅, 관리 및 반복 작업에 독특한 접근 방식을 제공하며, 도메인 전문가들이 인간 중심 시스템에 직접 참여하도록 강조합니다. 어노테이션 프로세스 중에 이러한 사용자들이 어려운 샘플에 대해 신속히 정보 기반의 결정을 내릴 수 있도록 함으로써, VA는 시스템의 전체 효율성을 높입니다. 더불어, 이는 지속적인 어노테이션 프로세스를 가능하게 하며, 사용자가 신속하게 모델을 배포하고, 제작에서 그 품질을 모니터하고, 어떤 예외 사례라도 신속히 수정할 수 있도록 합니다.\n\n이 SELF-SERVICE 구조는 도메인 전문가가 데이터 과학자나 제3자 어노테이터의 적극적인 참여 없이 자체 개선을 할 수 있도록 하며, 시스템에 대한 신뢰를 쌓음과 동시에 소유감을 유지하게 합니다.\n\n우리는 VA의 성능을 연구하기 위한 실험을 실시했으며, 여러 비디오 이해 작업에 걸쳐 가장 경쟁력 있는 기준에 비해 평균 정밀도(average precision)에서 중위 8.3 포인트 개선을 나타내었습니다. VA를 사용하여 3명의 전문 비디오 편집자가 어노테이팅한 56가지 비디오 이해 작업에 걸친 153,000개의 레이블이 포함된 데이터셋을 공개하며, 실험을 복제하기 위한 코드도 공개합니다.","ogImage":{"url":"/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_0.png"},"coverImage":"/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_0.png","tag":["Tech"],"readingTime":5},{"title":"대형 언어 모델 LLM의 잠재력 발휘하기","description":"","date":"2024-06-20 19:04","slug":"2024-06-20-UnleashingthePotentialofLargeLanguageModelsLLMs","content":"\n\n인공 지능의 지속적인 발전 속에서, 대형 언어 모델 (LLM)은 혁신적인 혁명을 일으키며 기계가 인간 언어를 이해하고 생성하는 방법을 혁신하고 있습니다.\n\n## LLMs란 무엇인가요?\n\nLLM은 인간 언어를 처리하고 이해하기 위해 방대한 매개 변수를 가진 신경망을 사용하는 고급 인공 지능 알고리즘입니다. 이러한 모델은 자가 지도 학습 기술을 활용하여 텍스트 생성, 기계 번역, 요약, 텍스트로부터 이미지 생성, 코딩, 대화형 인공 지능과 같은 작업을 수행할 수 있습니다. 주목할만한 예시로는 OpenAI의 GPT 시리즈와 구글의 BERT가 있습니다.\n\n## LLM의 진화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM의 발전은 GPT 모델 시리즈 내에서 특히 중요한 이정표로 기록되어 있습니다:\n\n- GPT-1 (2018): 1억 1700만 개의 파라미터.\n- GPT-2 (2019): 15억 개의 파라미터.\n- GPT-3 (2020): 1750억 개의 파라미터로, ChatGPT의 기초를 형성했습니다.\n- GPT-4 (2023): 수조 개의 파라미터를 특징으로 할 것으로 예상됩니다.\n\n## LLM은 어떻게 동작하나요?\n\nLLM은 Transformer와 같은 아키텍처를 활용하는 딥러닝 원리에 기반합니다. LLM은 방대한 데이터셋에서 훈련되며, feedforward, embedding 및 attention 레이어와 같은 레이어로 구성됩니다. Self-attention과 같은 attention 메커니즘을 통해 LLM은 시퀀스 내 다른 토큰들의 중요성을 가중치로 삼아 복잡한 종속성과 관계를 포착할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## LLM 아키텍처의 주요 구성 요소\n\n- 입력 임베딩: Tokenized 텍스트가 연속 벡터 표현으로 변환됩니다.\n- 위치 인코딩: 임베딩에 위치 정보를 추가합니다.\n- 인코더 레이어: 입력 텍스트를 처리하여 컨텍스트와 의미를 보존하는 숨겨진 상태를 생성합니다.\n- Self-Attention 메커니즘: Token의 중요성을 문맥적으로 가중치로 산정합니다.\n- 피드-포워드 신경망: 복잡한 토큰 상호 작용을 포착합니다.\n- 디코더 레이어: 일부 모델에서 자기 회귀적 생성을 가능하게 합니다.\n- Multi-Head Attention: 동시에 다양한 관계를 포착합니다.\n- 레이어 정규화: 학습과 일반화를 안정화합니다.\n- 출력 레이어: 언어 모델링과 같은 특정 작업에 따라 달라집니다.\n\n## LLM의 응용 분야\n\nLLM은 다양한 도메인에서 폭넓은 응용 분야를 가지고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 자연어 이해 (NLU): 고급 챗봇과 가상 어시스턴트를 구동합니다.\n- 콘텐츠 생성: 인간과 유사한 텍스트 및 코드 단편을 생성합니다.\n- 언어 번역: 언어 간 텍스트 번역합니다.\n- 텍스트 요약: 간결한 요약을 생성합니다.\n- 감성 분석: 소셜 미디어 및 리뷰에서 감정을 분석합니다.\n\n### LLM의 장점\n\n- 제로샷 러닝: 추가 훈련 없이 새로운 작업에 적응합니다.\n- 대량 데이터 처리: 광범위한 텍스트 이해가 필요한 작업에 적합합니다.\n- 지속적인 학습: 특정 도메인에 맞게 세밀하게 조정할 수 있습니다.\n- 자동화: 복잡한 작업을 위해 인력을 확보합니다.\n\n### LLM 훈련에서의 도전과제\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 높은 계산 비용: 교육에 상당한 자원이 필요합니다.\n- 데이터 요구 사항: 대량의 텍스트 말뭉치를 획득하는 것은 어려울 수 있습니다.\n- 윤리적 우려: 잠재적 편향과 환경 영향.\n- 포화: 모델 크기가 증가함에 따라 성능 향상이 일시적으로 멈출 수 있습니다.\n\n## 결론\n\nLLMs는 고급 언어 처리 능력을 가능하게 함으로써 AI와 NLP를 혁신하였습니다. 그들은 상당한 이점을 제공하지만, 윤리적 우려와 계산 비용 같은 과제에 대처하는 것이 그들의 지속 가능한 발전과 응용에 중요합니다. LLMs의 가능성을 계속 탐구함에 따라, 그들의 산업 전반에 대한 변혁 능력은 인공 지능의 힘을 입증합니다.","ogImage":{"url":"/assets/img/2024-06-20-UnleashingthePotentialofLargeLanguageModelsLLMs_0.png"},"coverImage":"/assets/img/2024-06-20-UnleashingthePotentialofLargeLanguageModelsLLMs_0.png","tag":["Tech"],"readingTime":3},{"title":"언어 에이전트 트리 검색 - LATS","description":"","date":"2024-06-20 19:02","slug":"2024-06-20-LanguageAgentTreeSearchLATS","content":"\n\n# 소개\n\n얼마 전부터 LLMs를 복잡하고 상세하며 맥락을 이해하는 구현에 독립적으로 사용할 수 없다는 것이 알려졌습니다.\n\nLLMs는 외부 도구로 더 많은 기능을 제공받아야 하며, 주체적인 프레임워크의 기반을 형성해야 합니다. 외부 도구와 의미적 피드백을 활용함으로써 이를 달성할 수 있습니다.\n\n외부 도구에는 웹 검색, 특정 API 접근, 문서 검색 등과 같은 기능이 포함될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n연구에 의하면 대부분의 대화형 UI는 인간 수준의 신중하고 사려 깊은 의사 결정에 부족함을 자주 겪습니다. 게다가 이러한 방법들 중 많은 방법들은 여러 추론 경로를 고려하거나 앞선 계획을 실패하는 면에서 부족합니다.\n\n이러한 방법들은 종종 외부 피드백의 통합을 잊고 독립적으로 운영되며, 이는 추론을 개선할 수 있는 외부 피드백이 누락되었음을 의미합니다.\n\n# 실용적인 작업 예시\n\nLATS와 같은 프레임워크는 처음에는 알아보기 어려울 수 있지만, 이 기사에서 나중에 이 프레임워크의 LlamaIndex 구현을 하나씩 살펴볼 것입니다. 이를 통해 실제로 동작 중인 프레임워크를 보면 프레임워크의 신비로움을 제거하는 데 큰 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# LATS로 돌아가기\n\n## 초기 고려 사항\n\n- 이 프레임워크는 인간 대화의 기본 구성 요소를 포함합니다. 이는 기억, 맥락적 의식, 검색에 기반한 인간과 유사한 사고 과정을 따르는 것을 의미합니다.\n- LlamaIndex의 LATS 에이전트 구현은 각 노드 아래 탐색해야 하는 가능한 하위 작업 수와 검색의 얼마나 깊이 진행되어야 하는지를 설정해야 함을 보여줍니다.\n- 이러한 에이전트를 고려할 때 항상 대기 시간이 생각나며 LLM 백본 실행 비용도 고려되어야 합니다. 그러나 이러한 도전에 대한 해결책이 있습니다.\n- 비용은 앞서 언급한 대로 제한될 수 있으며, LLM에 대한 예산을 설정하는 것이 도움이되며, 결정적인 답변이 없이 예산이 소진된 경우 대화와 맥락을 인간에게 전달할 수 있습니다.\n- LATS 에이전트는 샘플 작업을 기반으로 최적의 궤적을 찾아내며 반사적 프롬프팅 방법보다 문제 해결에 더 유연하고 적응력이 뛰어납니다.\n- 외부 피드백과 자기 반성을 통합함으로써 LATS는 모델 감성을 향상시키고 에이전트가 경험으로부터 배울 수 있도록 하여 추론 기반 검색 방법을 능가합니다.\n- LlamaIndex를 고려해 보면, 자율 에이전트와 LlamaIndex가 Agentic RAG로 지칭하는 RAG의 조합은 모범 사례의 자연스러운 결합이다.\n- 에이전트가 흡수하는 문서는 훌륭한 맥락적 참조가 되며 에이전트를 사용 도메인 특정으로 만듭니다.\n- 한 기관에서 내부에서 사용되는 봇으로 이 에이전트가 사용된다면 어떨까요? 복잡하고 종종 대담한 질문에 대답하는 직원을 돕습니다.\n\n# 실용적인 LATS 구현\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## LlamaIndex 구현\n\n전체 LlamaIndex 노트북은 여기에서 찾을 수 있습니다.\n\n노트북은 초기 설정으로 시작합니다:\n\n```js\n%pip install llama-index-agent-lats\n%pip install llama-index-program-openai\n%pip install llama-index-llms-openai\n%pip install llama-index-embeddings-openai\n%pip install llama-index-core llama-index-readers-file\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오픈AI API 키를 정의하세요:\n\n```js\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"\u003c여기에 API 키를 넣으세요\u003e\"\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n그런 다음 LLM 및 임베딩 모델을 정의하세요:\n\n```js\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core import Settings\n\n# 참고: 더 높은 온도는 트리 확장을 더 다양하게 만들 수 있습니다\nllm = OpenAI(model=\"gpt-4-turbo\", temperature=0.6)\nembed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n\nSettings.llm = llm\nSettings.embed_model = embed_model\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래의 표를 Markdown 형식으로 변경하세요:\n\n\nDownload the relevant data:\n\n```js\n!mkdir -p 'data/10k/'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'\n```\n\nIndex the documents:\n\n```js\nimport os\nfrom llama_index.core import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    load_index_from_storage,\n)\nfrom llama_index.core.storage import StorageContext\n\n\nif not os.path.exists(\"./storage/lyft\"):\n    # load data\n    lyft_docs = SimpleDirectoryReader(\n        input_files=[\"./data/10k/lyft_2021.pdf\"]\n    ).load_data()\n    uber_docs = SimpleDirectoryReader(\n        input_files=[\"./data/10k/uber_2021.pdf\"]\n    ).load_data()\n\n    # build index\n    lyft_index = VectorStoreIndex.from_documents(lyft_docs)\n    uber_index = VectorStoreIndex.from_documents(uber_docs)\n\n    # persist index\n    lyft_index.storage_context.persist(persist_dir=\"./storage/lyft\")\n    uber_index.storage_context.persist(persist_dir=\"./storage/uber\")\nelse:\n    storage_context = StorageContext.from_defaults(\n        persist_dir=\"./storage/lyft\"\n    )\n    lyft_index = load_index_from_storage(storage_context)\n\n    storage_context = StorageContext.from_defaults(\n        persist_dir=\"./storage/uber\"\n    )\n    uber_index = load_index_from_storage(storage_context)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 두 개의 도구 또는 엔진을 설정하십시오:\n\n```js\nlyft_engine = lyft_index.as_query_engine(similarity_top_k=3)\nuber_engine = uber_index.as_query_engine(similarity_top_k=3)\n```\n\n전형적인 에이전트 스타일로, 두 에이전트 도구에는 설명이 제공됩니다. 이 설명은 에이전트가 도구를 언제, 어떻게 사용해야 하는지 결정하는 데 중요합니다.\n\n물론 에이전트는 여러 도구에 액세스할 수 있으며, 에이전트가 보유한 도구가 많을수록 에이전트가 강력해집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=lyft_engine,\n        metadata=ToolMetadata(\n            name=\"lyft_10k\",\n            description=(\n                \"2021년도 Lyft 재무에 관한 정보를 제공합니다. \"\n                \"도구에 자세한 일반 텍스트 질문을 입력으로 사용하세요. \"\n                \"입력은 의미 검색 엔진을 구동하는 데 사용됩니다.\"\n            ),\n        ),\n    ),\n    QueryEngineTool(\n        query_engine=uber_engine,\n        metadata=ToolMetadata(\n            name=\"uber_10k\",\n            description=(\n                \"2021년도 Uber 재무에 관한 정보를 제공합니다. \"\n                \"도구에 자세한 일반 텍스트 질문을 입력으로 사용하세요. \"\n                \"입력은 의미 검색 엔진을 구동하는 데 사용됩니다.\"\n            ),\n        ),\n    ),\n]\n```\n\n이것이 LATS 에이전트의 설정입니다...\n\n이것은 에이전트의 예산이 정의된 곳입니다...\n\n- num_expansions는 각 노드 아래에서 탐색할 가능한 하위 동작 수를 나타냅니다.\n- num_expansions=2는 각 부모 동작에 대해 가능한 다음 동작을 탐색할 것을 의미합니다.\n- max_rollouts는 탐색 공간의 각 탐사가 얼마나 깊게 이어지는지를 나타냅니다.\n- max_rollouts=5는 트리에서 최대 깊이 5까지 탐사됨을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n할 수 있습니다. LLM과 도구에 대한 참조도 볼 수 있어요.\n\n```js\nfrom llama_index.agent.lats import LATSAgentWorker\n\nagent_worker = LATSAgentWorker.from_tools(\n    query_engine_tools,\n    llm=llm,\n    num_expansions=2,\n    max_rollouts=3,  # rollouts를 무제한으로 사용하려면 -1 입력\n    verbose=True,\n)\nagent = agent.as_worker()\n```\n\n아래의 질문은 에이전트에게 제시됩니다. 질문이 얼마나 모호하고 다양한 조건과 뉘앙스를 가지고 있는지 주목하세요.\n\n```js\ntask = agent.create_task(\n    \"Uber와 Lyft의 10K 파일에서 설명된 위험 요인을 고려할 때, \"\n    \"어떤 회사가 더 나은 성과를 거두고 있는지? 구체적인 숫자를 사용하여 판단해주세요.\"\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLlamaIndex의 노트북은 여러 가지 다양한 순열을 거칩니다...\n\n⭐️ 대형 언어 모델 업데이트를 위해 LinkedIn에서 팔로우하세요 ⭐️\n\n![이미지](/assets/img/2024-06-20-LanguageAgentTreeSearchLATS_0.png)\n\n저는 현재 Kore AI의 최고 전도사입니다. 인공 지능과 언어가 교차하는 모든 것에 대해 탐구하고 쓰고 있습니다. 대형 언어 모델(LLMs), 챗봇, 음성봇, 개발 프레임워크, 데이터 중심의 잠재 공간 등을 다루고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Language Agent Tree Search - Image 1](/assets/img/2024-06-20-LanguageAgentTreeSearchLATS_1.png)\n\n![Language Agent Tree Search - Image 2](/assets/img/2024-06-20-LanguageAgentTreeSearchLATS_2.png)\n\n![Language Agent Tree Search - Image 3](/assets/img/2024-06-20-LanguageAgentTreeSearchLATS_3.png)\n\n[Click here for more information](https://llamahub.ai/l/agent/llama-index-agent-lats?from=agent)\n","ogImage":{"url":"/assets/img/2024-06-20-LanguageAgentTreeSearchLATS_0.png"},"coverImage":"/assets/img/2024-06-20-LanguageAgentTreeSearchLATS_0.png","tag":["Tech"],"readingTime":7},{"title":"Transformers에 대한 심층적인 탐구 ","description":"","date":"2024-06-20 19:00","slug":"2024-06-20-DeepDiveintoTransformersbyHand","content":"\n\n## 트랜스포머의 힘에 대한 세부 정보 살펴보기\n\n우리 동네에 새로운 변화가 일어났어요.\n\n아들이 \"로보 트럭\"이라고 부르는 한 대의 '로보-트럭'이 우리 거리에 새 집을 얻었답니다.\n\n이 트럭은 테슬라 사이버 트럭이고, 저는 아들에게 그 이름의 의미를 여러 번 설명해 주었지만 그는 여전히 로보 트럭이라고 부릅니다. 그래서 이제 로보 트럭을 보면 그 이름을 들으면 항상 로보트로 컨버트할 수 있는 로봇들이 나오는 영화 '트랜스포머'를 떠올립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오늘 우리가 아는 대로, 트랜스포머가 이 로보트럭을 구동하는 데 사용될 수 있다니 이상하지 않나요? 이것은 거의 한 바퀴 도는 순간입니다. 그렇다면 이 모든 얘기를 하고 있는 나는 어디로 향하고 있을까요?\n\n그래요, 나는 목적지로 가고 있어요 — 트랜스포머입니다. 로봇 자동차 트랜스포머가 아니라 신경망 트랜스포머죠. 여러분도 초대됐어요!\n\n![이미지](/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_0.png)\n\n## 트랜스포머란 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTransformer는 본질적으로 신경망입니다. 데이터에서 맥락을 학습하는 데 특화된 신경망입니다.\n\n하지만 그들을 특별하게 만드는 것은 레이블이 달린 데이터셋과 신경망 내의 컨볼루션 또는 순환을 필요로 하지 않는 메커니즘이 존재한다는 것입니다.\n\n## 이 특별한 메커니즘들은 무엇인가요?\n\n많은 메커니즘이 있지만, Transformer의 핵심인 어텐션 가중치와 피드포워드 네트워크(FFN)라는 두 가지 메커니즘이 진정으로 그들을 특별하게 만드는 힘입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 어텐션 가중치란 무엇인가요?\n\n어텐션 가중치는 모델이 들어오는 시퀀스의 어떤 부분에 집중해야 하는지 학습하는 기술입니다. 모든 시간에 모두 주시하는 '사우론의 눈'이 모든 것을 스캔하고 관련 있는 부분에 빛을 비추는 것으로 생각해보세요.\n\n## FFN은 무엇을 의미하나요?\n\n트랜스포머의 맥락에서, FFN은 주로 일관된 데이터 벡터 집단에 작용하는 일반적인 다층 퍼셉트론입니다. 어텐션과 결합되어 올바른 '위치-차원' 조합을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 어텐션과 FFN은 어떻게 작동할까요?\n\n그러니 더 이상 말더듬거리지 말고, 어텐션 가중치와 FFN이 트랜스포머를 이렇게 강력하게 만드는 방법에 대해 알아봅시다.\n\n이 토론은 톰 예 교수님의 멋진 '손으로 만드는 인공지능' 시리즈에 기반을 두고 있습니다. (아래 이미지는, 별도 언급이 없는 한, 상기 LinkedIn 게시물 중 톰 예 교수님의 것으로, 교수님의 허락을 받아 편집한 것입니다.)\n\n그럼 시작해봅시다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 중요한 아이디어는 주의 가중치와 피드포워드 네트워크(FFN)입니다.\n\n이것들을 염두에 두고, 우리에게 다음을 제공받는 경우를 가정해 봅시다:\n\n- 이전 블록으로부터 5개의 입력 특성 (여기서 3x5 행렬인 A로, X1, X2, X3, X4 및 X5가 특성이며 각 행은 각각의 특성을 나타냅니다.)\n\n![이미지](/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[1] 주의 집중 가중치 행렬 A 획득\n\n과정에서 첫 번째 단계는 주의 집중 가중치 행렬 A를 획득하는 것입니다. 이 부분은 자기 주의 메커니즘이 작용하는 곳입니다. 이 단계는 입력 시퀀스 중에서 가장 관련성 높은 부분을 찾는 데 사용됩니다. \n\n입력 특성을 쿼리-키(QK) 모듈에 공급하여 수행합니다. 간편하게 말해, QK 모듈의 세부 사항은 여기에 포함되어 있지 않습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*DYNNNiaZac_ZNGFVUn4aag.gif)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[2] 주의 집중 가중치\n\n주의 집중 가중치 행렬 A (5x5)을 얻으면, 입력 기능 (3x5)을 곱하여 주의 집중된 기능 Z를 얻습니다.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*1_VmXxp6iPkwVEdhFwExkg.gif)\n\n여기서 중요한 점은 기능이 위치 P1, P2 및 P3에 따라 가로로 결합된다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음과 같이 세분화된 계산을 행별로 수행해보세요:\n\nP1 X A1 = Z1 → 위치 [1,1] = 11\n\nP1 X A2 = Z2 → 위치 [1,2] = 6\n\nP1 X A3 = Z3 → 위치 [1,3] = 7\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nP1 X A4 = Z4 → Position [1,4] = 7\n\nP1 X A5 = Z5 → Position [1,5] = 5\n\n.\n\n.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n변경된 내용은 다음과 같습니다:\n\nP2 X A4 = Z4 → Position [2,4] = 3\n\nP3 X A5 = Z5 → Position [3,5] = 1\n\n이것이 예시입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_2.png)\n\n처음에는 조금 귀찮아 보일 수 있지만 행별 곱셈을 따르면 결과는 매우 직관적일 것입니다.\n\n멋진 점은 우리의 주의 가중치 행렬 A가 배열된 방식 때문에 새로운 특징 Z가 X의 조합으로 나타난다는 것이다:\n\nZ1 = X1 + X2\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nZ2 = X2 + X3\n\nZ3 = X3 + X4\n\nZ4 = X4 + X5\n\nZ5 = X5 + X1\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n(힌트: 행렬 A에서 0과 1의 위치를 살펴보세요).\n\n[3] FFN: 첫 번째 레이어\n\n다음 단계는 어텐션 가중치가 적용된 피쳐를 피드포워드 신경망에 전달하는 것입니다.\n\n그러나 이번에는 이전 단계에서의 위치가 아닌 차원을 가로지르는 값들을 결합하는 것이 차이점입니다. 아래처럼 수행됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래에 있는 링크를 사진으로 보여줄게요.\n\n- 관심 단계에서는 원래 특징을 기반으로 입력을 결합하여 새로운 특징을 얻었어요.\n\n- FFN 단계에서는 그들의 특성을 고려하여 새로운 행렬을 얻기 위해 특징을 세로로 결합해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 번 더 element-wise 행 연산이 도움이 됩니다. 여기서 새 행렬의 차원 수가 4로 증가했다는 점에 주목하세요.\n\n[4] ReLU\n\n저희가 가장 좋아하는 단계 : ReLU는 이전 행렬에서 얻은 음의 값이 0으로 반환되고 양의 값은 변경되지 않습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*FmroND2LsW91TrYXNh2UGQ.gif)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[5] FFN : 두 번째 레이어\n\n결과 매트릭스의 차원을 4에서 3으로 줄이는 두 번째 레이어를 통과합니다.\n\n![Image](https://miro.medium.com/v2/resize:fit:1400/1*z0CE0MMXVIuuu0qPYybrjA.gif)\n\n여기서의 출력은 다음 블록으로 공급할 준비가 되어 있습니다 (원본 매트릭스와 유사성을 확인하십시오) 및 전체 프로세스가 처음부터 반복됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 기억해야 할 두 가지 주요 사항은 다음과 같습니다:\n\n- 어텐션 레이어는 위치를 가로 방향으로 결합합니다.\n- 피드포워드 레이어는 차원을 세로 방향으로 결합합니다.\n\n이것이 트랜스포머의 강력함에 대한 비밀 소스입니다. 데이터를 여러 방향에서 분석하는 능력입니다.\n\n위의 아이디어를 요약하면 다음과 같은 주요 포인트가 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 트랜스포머 아키텍처는 어텐션 레이어와 피드-포워드 레이어의 조합으로 이해될 수 있습니다.\n- 어텐션 레이어는 특성을 결합하여 새로운 특성을 생성합니다. 예를 들어 두 로봇 Robo-Truck과 Optimus Prime을 결합하여 새로운 로봇인 Robtimus Prime을 얻는 것을 생각해보세요.\n- 피드-포워드(FFN) 레이어는 특성의 부분이나 특성을 결합하여 새로운 부분/특성을 생성합니다. 예를 들어 Robo-Truck의 바퀴와 Optimus Prime의 이온 레이저가 합쳐져 바퀴 레이저가 될 수 있습니다.\n\n# 늘같이 강력한 트랜스포머\n\n신경망은 상당히 오랫동안 존재해왔습니다. 합성곱 신경망(CNN)과 순환 신경망(RNN)이 주류인 동안 2017년에 트랜스포머가 소개되면서 상황이 크게 바뀌었습니다. 그 이후로 인공지능 분야는 기하급수적으로 성장했고 매일 새로운 모델, 새로운 기준, 새로운 배움이 이어졌습니다. 그리고 언제가 미래에 더 큰 변화를 이끌어낼 수 있는 현상적인 아이디어로 발전할지에 대해 시간만이 알게 해줄 것입니다. 그러나 현재는 아이디어가 우리 삶을 어떻게 변화시킬 수 있는지를 생각해보는 것이 잘못된 말이 아닐 것입니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_3.png\" /\u003e\n\nP.S. 이 연습을 혼자 진행하고 싶다면, 여기에 사용할 빈 템플릿이 있어요.\n\nRobtimus Prime를 만들어서 즐거운 시간 보내세요!","ogImage":{"url":"/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_0.png"},"coverImage":"/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_0.png","tag":["Tech"],"readingTime":5},{"title":"대형 언어 모델의 미래 2030년과 2050년을 위한 예측","description":"","date":"2024-06-20 18:59","slug":"2024-06-20-TheFutureofLargeLanguageModelsPredictionsfor2030and2050","content":"\n\n![Future of Large Language Models](/assets/img/2024-06-20-TheFutureofLargeLanguageModelsPredictionsfor2030and2050_0.png)\n\n미래를 예측하는 것은 기술적으로 26년 앞을 내다보는 일이 어려운 일입니다. 예를 들어, 누가 1998년에 기술이 얼마나 발전할지 예상했을까요? 특히 대형 언어 모델(Large Language Models, LLMs)과 같은 기술이 혁신적으로 발전하면서 비즈니스와 사회에 미칠 잠재적인 영향을 계속해서 모니터링하는 것이 중요합니다. 본 기사에서는 현재 LLMs의 상태를 기반으로 2030년과 2050년에 이들이 어떠한 능력을 갖게 될지 예측해보려고 합니다.\n\n무료로 읽으려면 여기를 클릭하세요.\n\nGPT-2와 BERT의 초기 모델은 몇 년 전에도 despite에서와 같이 최신 모델로 발전했습니다. 이러한 모델들은 데이터셋을 토대로 훈련되어 거의 인간이 생성한 대답과 같은 능력을 제공하며 이미 창의성, 번역, 텍스트 요약 분야에서 그 영향이 눈에 띄고 있습니다. 오늘날, LLMs는 이미 우리의 삶과 다양한 산업에 자리 잡고 있으며, 챗봇, 콘텐츠 생성, 여가 활동 등 다양한 분야에서 활약하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2030\n\n2030년을 미리 보면, 대형 언어 모델은 우리의 삶, 학습 및 일터를 깊이 연결하여 변화를 가져올 것입니다. 여기 몇 가지 예측이 있습니다:\n\n- 높은 이해력: 대형 언어 모델은 인간 수준에 거의 필적하는 이해력을 처리할 수 있게 될 것으로 예상되며, 감정 지능에 도달하고 복잡한 문제를 해결하는 신호를 보일 것입니다.\n- 다중 모달 능력: 텍스트, 음성, 그림 및 비디오를 설명하는 능력은 동일한 모델에 의해 수행될 가능성이 높아져, 가상 현실, 비서 및 게임과 더 상호 작용할 수 있는 경험을 제공할 수 있을 것입니다.\n- AI 생태계: 대형 언어 모델은 컴퓨터 비전 및 로봇공학과 같은 다른 AI 기술과 조화롭게 공존할 것으로, 사물 인터넷(IoT)과 유사하게 진화할 것입니다. 이는 더 타당하고 맥락이 풍부하며 변화혁신적인 시스템을 제공함으로써 길을 열어갈 것입니다.\n- 실시간 번역과 통역: 대형 언어 모델이 실시간으로 고품질의 번역 및 통역 서비스를 제공하므로 언어 장벽이 제거되어 글로벌 규모에서의 커뮤니케이션과 협업이 가능해질 것입니다.\n- 보안과 개인정보 보호: 대형 언어 모델은 고급 보안과 개인정보 보호 기능을 내장하여 데이터 보안과 안전한 커뮤니케이션 채널을 강화할 것입니다. 이러한 감지기는 깊은 가짜 영상과 잘못된 정보를 발견하고 중화하는 능력을 갖추게 될 것입니다.\n- 맞춤화: 대형 언어 모델은 교육이 제공되는 방식에 변화를 일으킬 것으로, 학습 상호작용 및 선호도를 사용자에게 더욱 개인화된 방식으로 제공할 수 있을 것입니다. 사용자의 고유한 요구사항, 선호도 및 학습 스타일에 따라 각 사용자에게 맞춤형 응답과 제안을 할 수 있습니다.\n\n![이미지](/assets/img/2024-06-20-TheFutureofLargeLanguageModelsPredictionsfor2030and2050_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2050\n\n의문을 제기할 수 있지만, 2050년까지 대형 언어 모델(LLMs)은 아마도 인간 언어 이상의 능력을 갖출 것으로 예측됩니다. 이는 LLMs가 2050년에 어떻게 보일지에 대한 중요한 선견지명에 관한 예측입니다:\n\n- 감정 지능: LLMs는 어느 정도의 감정 지능을 나타내어 더 의미 있는 대화를 나누며, 감정적 육아를 제공하고 공감을 통해 연결감을 느끼게 될 것입니다.\n- 특정 전문 지식: 기계 학습은 법률, 금융, 의학, 공학 등을 포함한 영역을 이해할 수 있도록 대규모 특정 데이터를 기반으로 훈련될 것입니다. 그러나 이러한 결과는 이 산업 분야의 대부분 직업이 사라지는 시기로 이어질 수 있습니다.\n- 창의적 생성: 소설부터 시나리오, 음악 및 예술 작품까지 원래의 창의적 작품을 만들어내는 능력은 이미 진행 중입니다. 2050년에는 새로운 형태의 엔터테인먼트와 표현뿐만 아니라 지적 재산권과 소유권에 대한 새로운 우려가 발생할 수 있습니다.\n\n## 윤리적 고려사항\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM이 퍼지고 영향력을 키우면서, 그 개발과 사용의 윤리를 규제하기 위한 더 많은 방법이 필요해질 것입니다. 따라서, 2030년까지, 그리고 특히 2050년까지 대부분의 영역에서 LLM의 사용을 다루기 위한 합리적인 도덕적 및 규제 지침을 개발했을 것으로 기대할 수 있습니다. 주요한 몇 가지는 다음과 같습니다:\n\n- 편향과 공정성: 학습 데이터의 편향을 완화하여 공정하고 평등한 결과를 보장합니다.\n- 개인 정보와 보안: 사용자 개인 정보를 유지하고 데이터 처리를 안전하게 다루는 것이 매우 중요합니다.\n- 잘못된 정보와 조작: 가장 큰 도전 중 하나는 LLM을 악의적으로 활용하여 잘못된 정보를 확산하거나 대중의 인식에 영향을 주는 것을 막는 것입니다.\n- 직업 이동: LLM의 높은 자동화 잠재력은 상당한 양의 직업을 대체하고 노동력 재교육의 필요성을 야기할 가능성이 있습니다.\n\nLLM은 기대되는 동시에 예측하기 어려운 미래이며, 우리가 기계나 다른 사람들과 소통하는 방식을 변화시킬 것입니다. 2030년과 2050년까지의 미래를 바라볼 때, 우리는 이러한 모델의 책임 있는 개발과 배포를 즉각적으로 우선시해야 합니다. 이러한 예측은 가상적이지만, 곧 있을 수도 있는 LLM의 사용으로 인한 결과와 위험을 어느 정도 예측해줍니다. 강력한 기술과 마찬가지로, LLM의 성공적인 개발과 사용은 예견가능한 의도가 필요합니다; 구체적으로는 모든 인류의 복지를 증진하기 위해 사용되어야 합니다.","ogImage":{"url":"/assets/img/2024-06-20-TheFutureofLargeLanguageModelsPredictionsfor2030and2050_0.png"},"coverImage":"/assets/img/2024-06-20-TheFutureofLargeLanguageModelsPredictionsfor2030and2050_0.png","tag":["Tech"],"readingTime":3},{"title":"Llama 3 해제하기 Llama 3 마스터하기를 위한 궁극적인 안내","description":"","date":"2024-06-20 18:57","slug":"2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3","content":"\n\n![이미지](/assets/img/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3_0.png)\n\n최근 몇 년 동안 인공 지능의 세계는 대부분 대형 언어 모델(Large Language Models, LLMs)의 등장 덕분에 급속하게 발전해 왔습니다. 이러한 고급 시스템들은 기본 텍스트 처리기에서 인간과 유사한 텍스트를 이해하고 생성할 수 있는 정교한 모델로 진화했습니다. 이러한 능력과 응용 프로그램의 중요한 발전을 표시하는 것은 메타의 최신 제품인 Llama3입니다. Llama3는 오픈 모델의 접근성과 성능의 경계를 재정의할 것을 약속하는 플랫폼입니다.\n\n지난 주에 메타는 8B 및 70B 모델의 Llama3를 공개했으며, 이는 개선된 추론 기능을 포함하여 해당 모델 규모에 대한 새로운 기준을 세우는 놀라운 발전을 선보였습니다. Llama3는 날이 갈수록 가장 뛰어난 공개적으로 이용 가능한 LLM으로, 그 출시는 인공 지능 분야에서 중요한 대목을 이루었습니다.\n\n본 문서에서는 Llama3에 대해 자세히 살펴볼 것이며, 해당 기술을 효과적으로 활용하는 방법에 대한 포괄적인 가이드를 제공할 것입니다. 또한 Llama3의 잠재력을 탐구하고 어떻게 여러 산업을 혁신할 수 있는지 살펴볼 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 시작하기\n\n## 목차\n\n- Llama 3이란 무엇인가요?\n- 주요 기능\n- Llama 2 대 Llama 3\n- Llama 3 대 다른 모델들\n- Llama 3 안전 기능\n- Llama 3 실험\n- 방법 1: Google Colab 및 HuggingFace 사용하기\n- Llama 3를 활용한 챗봇 만들기\n- 방법 2: Ollama 사용하기\n\n## Llama 3이란 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n메타 라마 3는 Meta의 언어 모델 라인 중 가장 최신 제품으로, 80억과 700억 개의 매개변수를 포함한 버전이 있습니다. 이 모델은 일상 대화에서부터 복잡한 추론 작업까지 다양한 응용 프로그램에서 우수한 성능을 발휘하도록 설계되었습니다. 이전 모델을 능가하는 성능을 보여줍니다. 라마 3는 무료로 이용할 수 있어 AI 개발 및 기타 분야에서의 혁신을 촉진합니다.\n\n![Llama3](/assets/img/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3_1.png)\n\n### 주요 기능:\n\n- 80억과 700억 개의 매개변수 모델 모두에 통합되어 집중적이고 효과적인 처리를 위해 추론 효율성을 향상시킵니다.\n- MMLU 및 HumanEval과 같은 작업에서 이전 모델 및 경쟁 모델을 능가하여 다양한 벤치마크에서 뛰어난 성과를 보입니다.\n- 라마 3은 디코더 전용 트랜스포머 구조를 유지하면서 중요한 개선 사항을 포함하고, 128,000 개의 토큰을 지원하는 토크나이저를 사용하여 언어 부호화 효율성을 향상시킵니다.\n- Llama 2의 데이터셋보다 7배 큰 15조 토큰 이상의 데이터셋에서 훈련되었으며, 30개 이상 언어의 다양한 언어 표현과 비영어 데이터가 통합되어 있습니다.\n- 지도 미세 조정, 거부 샘플링 및 정책 최적화를 결합하여 모델 품질과 의사 결정 능력을 개선하는 향상된 사후 훈련 단계가 있습니다.\n- 상세한 스케일링 법칙을 사용하여 데이터 혼합 및 계산 자원을 최적화하고, Llama 2와 비교하여 학습 과정의 효율성을 세 배로 높이면서 다양한 응용 프로그램에서 견고한 성능을 보장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 람마 2 대 람마 3\n\n람마 3은 이전 람마 2 모델을 기반으로 하여 핵심 디코더 전용 트랜스포머 아키텍처를 유지하며 여러 가지 주요 개선 사항을 도입했습니다. 토크나이저는 이제 128,000개의 토큰을 지원하여 언어의 더 효율적인 인코딩과 향상된 성능을 가능하게 합니다. 또한, 람마 3은 그룹화된 쿼리 어텐션 (GQA)을 통합하여 다양한 매개 변수 모델에 걸쳐 추론 효율성을 향상시킵니다. 이 모델은 8,192토큰의 시퀀스를 마스킹을 사용하여 보다 집중된 효과적인 처리를 보장합니다.\n\n![이미지](/assets/img/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3_2.png)\n\n## 람마 3 대 다른 모델들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n메타가 개발한 라마 3은 다양한 벤치마크에서 이전 모델 및 경쟁 모델들을 능가하는 새로운 기준을 세웠습니다. 특히 MMLU와 같은 다양한 영역의 지식을 평가하는 테스트와 코딩 기술에 중점을 둔 HumanEval과 같은 테스트에서 뛰어난 성과를 보였습니다. 또한, 라마 3은 Google의 제미니 1.5 Pro나 안소로픽의 클로드 3 소네토와 같은 다른 고매개변수 모델들을 특히 복잡한 추론 및 이해 과제에서 능가했습니다.\n\n메타의 라마 3은 다양한 벤치마크와 응용 프로그램에서 뛰어난 성능을 보이며, 특히 추론, 코딩 및 창의적 글쓰기와 관련된 작업에서 뛰어난 성과를 거두고 있습니다. 다양하고 정확한 응답을 생성할 수 있는 능력은 다른 모델들과 구별되며, 개선된 사용자 경험과 생산성을 보장합니다.\n\n![이미지](/assets/img/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3_3.png)\n\n## 라마 3 안전 기능\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLlama 3은 Llama Guard 2, Cybersec Eval 2, 그리고 Code Shield와 같은 새로운 안전 및 신뢰 기능을 소개합니다. 이러한 기능들은 사용 중에 안전하지 않은 코드를 걸러내는 역할을 합니다. torchtune과 함께 개발된 Llama 3은 효율적인 작성, 세밀 조정, 그리고 대규모 언어 모델(LLM)의 테스트를 용이하게 하는 PyTorch 기반 라이브러리이며, Hugging Face와 Weights \u0026 Biases와 같은 플랫폼들과 통합되어 있습니다.\n\n![이미지](/assets/img/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3_4.png)\n\n책임 있는 배포는 \"레드팀\" 노력을 포함한 체계적인 테스트를 통해 보장되며, 특히 사이버 보안 분야에서의 안전성과 견고성을 평가합니다. Llama Guard 2는 MLCommons의 산업 표준을 따르며, CyberSecEval 2는 보안 조치를 강화합니다. Llama 3의 개발은 AI 커뮤니티를 통합하고 잠재적인 위험을 대응하기 위한 개방적인 접근을 강조하며, Meta의 책임 있는 사용 가이드(RUG)는 모베이션 모델 측면과 클라우드 제공업체에 의한 내용 관리 도구를 제공합니다.\n\n# Llama 3 실험하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로컬 머신에서 Llama 3을 실험하는 것은 오픈 소스 기능을 활용하는 다양한 도구 덕분에 더 쉬워졌어요. Hugging Face가 이끄는 선두주자로 Llama 3 모델의 지원이 이제 가능하며, 그들의 Hub에서 Transformers 라이브러리를 통해 접근할 수 있어요. 전체 정밀도 모델을 선호하시든가, 4비트 양자화 모델의 효율성을 선호하든가, 설치와 실행은 매끄럽게 처리됩니다.\n\n여기서는 서로 다른 사용자 선호도와 기술 수준에 맞는 두 가지 구체적인 방법을 살펴볼 거에요.\n\n## Method 1: Google Colab 및 HuggingFace 사용\n\n자유로운 Colab 티어에서 Llama 3을 실행하는 실습으로 빠져들어 봅시다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 1: 라마 3 액세스 활성화\n\n라마 3은 액세스 요청이 필요한 보안 모델입니다.\n\n![이미지](/assets/img/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3_5.png)\n\n모델 액세스를 활성화하는 단계를 따르세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Hugging Face 계정에 로그인하거나 아직 계정이 없는 경우 새 계정을 등록해보세요.\n- https://huggingface.co/meta-llama/Meta-Llama-3-8B를 방문하여 접근 권한을 요청할 수 있어요.\n- 성명, 생년월일, 국가, 소속과 같은 사용자 세부 정보를 제공해주세요. 라마 3 모델에 접근할 수 있도록 라이선스 동의를 받은 후에, 이제 Llama 3 모델에 접속할 수 있어요.\n\n- 접속을 확인하려면 다음 링크로 이동해주세요: huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json. Llama 3 모델에 성공적으로 접근한 경우, 관련 정보를 수신할 수 있을 거예요. \n\n![UnlockingLlama3YourUltimateGuidetoMasteringLlama3_6](/assets/img/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3_6.png)\n\n## 단계 2: Hugging Face 액세스 토큰 생성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델에 액세스하려면 HuggingFace 액세스 토큰이 필요합니다. 설정으로 이동하여 왼쪽 사이드 바의 Access Tokens에 들어가 \"New token\" 버튼을 클릭하여 새 액세스 토큰을 생성할 수 있습니다.\n\n## 단계 3: HuggingFace를 사용하여 Llama 3로 첫 번째 스크립트 만들기\n\n- [Colaboratory에 오신 것을 환영합니다 — Colab](https://colab.research.google.com/) 링크로 이동하고 \"로그인\"을 클릭하여 colab 계정에 로그인하거나 계정이 없는 경우 새로 만드세요.\n- 런타임을 T4 GPU로 변경하려면 런타임 → 런타임 유형 변경 → T4 GPU → 저장을 클릭하세요.\n- Gemma를 사용하려면 Hugging Face 액세스 토큰을 제공해야 합니다. 왼쪽 편에 있는 Secrets(🔑)를 선택하고 HF_TOKEN 키를 추가하세요.\n- + 새 노트북 버튼을 클릭하여 새 콜랩 노트북을 만드세요.\n\n## 단계 4: 종속 항목 설치\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 명령어를 사용하여 transformers, accelerate 및 bitsandbytes 라이브러리를 설치해보세요.\n\n```js\n!pip install -U \"transformers==4.40.0\" --upgrade\n!pip install accelerate bitsandbytes\n```\n\n## 단계 5: 모델 다운로드 및 설치\n\nLlama 3 모델을 설치하고 텍스트 생성 파이프라인을 설정해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport transformers\nimport torch\n\nmodel_id = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\n        \"torch_dtype\": torch.float16,\n        \"quantization_config\": {\"load_in_4bit\": True},\n        \"low_cpu_mem_usage\": True,\n    },\n)\n```\n\n## 단계 6: 쿼리 전송\n\n추론을 위해 모델에 쿼리를 보내세요.\n\n```js\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant!\"},\n    {\"role\": \"user\", \"content\": \"\"\"Hey how are you doing today?\"\"\"},\n]\n\nprompt = pipeline.tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n)\n\nterminators = [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids(\"\u003c|eot_id|\u003e\")\n]\n\noutputs = pipeline(\n    prompt,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\n\nprint(outputs[0][\"generated_text\"][len(prompt):])\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같은 결과를 얻게 됩니다.\n\n```js\n잘 지내고 있어요, 물어봐 주셔서 감사해요! 저는 도움이 되는 어시스턴트이기 때문에 언제든지 궁금한 점이나 해야할 일이 있으면 도와드릴 준비가 되어 있어요. 여러분은요? 오늘은 어떠신가요? 좋은 하루 보내고 있나요?\n```\n\n\u003cimg src=\"/assets/img/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3_7.png\" /\u003e\n\n## Llama 3를 사용하여 챗봇 만들기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 섹션에서는 gradio를 사용하여 Llama 3를 이용한 챗봇을 생성할 것입니다.\n\n- gradio 패키지 설치\n\n```js\n!pip install gradio\n```\n\n- 노트북에 새 셀을 생성하고 다음 코드를 추가하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport gradio as gr\n\nmessages = []\n\ndef add_text(history, text):\n    global messages\n    history = history + [(text,'')]\n    messages = messages + [{\"role\":'user', 'content': text}]\n    return history, text\n\ndef generate(history):\n  global messages\n  prompt = pipeline.tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n)\n\n  terminators = [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids(\"\u003c|eot_id|\u003e\")\n]\n\n  outputs = pipeline(\n    prompt,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\n  response_msg = outputs[0][\"generated_text\"][len(prompt):]\n  for char in response_msg:\n      history[-1][1] += char\n      yield history\n  pass\n\nwith gr.Blocks() as demo:\n\n    chatbot = gr.Chatbot(value=[], elem_id=\"chatbot\")\n    with gr.Row():\n            txt = gr.Textbox(\n                show_label=False,\n                placeholder=\"Enter text and press enter\",\n            )\n\n    txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(\n            generate, inputs =[chatbot,],outputs = chatbot,)\n\ndemo.queue()\ndemo.launch(debug=True)\n```\n\n- 셀을 실행하세요. 노트북에 gradio 인터페이스가 표시되거나 새 탭에서 열기 위해 제공된 링크를 사용할 수 있습니다. 아래와 같이 출력이 나타납니다.\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*tQMX7SYlik7Riuwd5FE_Jg.gif\" /\u003e\n\n## 방법 2: Ollama 사용하기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n큰 언어 모델(LLMs)을 클라우드 서비스에 의존하지 않고 로컬에서 실행할 수 있는 대안을 찾고 있다면, Ollama가 그에 최적의 선택일 것입니다. Ollama는 지역에서 LLMs를 실행하기 위해 설계된 오픈 소스 소프트웨어로, 직접 제어를 받을 수 있도록 해줍니다.\n\nOllama를 시작하려면 소프트웨어를 다운로드하기만 하면 됩니다. Ollama를 사용하면 데이터 프라이버시를 유지하면서 계산 자원을 직접 제어할 수 있는 이점을 누릴 수 있습니다.\n\n- Ollama 공식 사이트로 이동합니다.\n- 다운로드를 클릭하여 소프트웨어를 다운로드합니다.\n- 설치 파일을 더블 클릭하고 설치를 클릭하여 설치합니다.\n- 설치가 완료되면 다음 명령을 사용하여 지정된 모델로 로컬 서버를 시작하실 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\njs\nCompile the text ollama run llama3:instruct.\n\nAlso, use llama3, llama3:70b, llama3:70b-instruct as arguments for different types of llama3 models. Make sure you have a proper internet connection; otherwise, you may encounter an error like dial tcp: lookup no such host error while pulling the model.\n\n## Running the Model\n\n- Once the model is downloaded, you can start querying. Input your context directly through the terminal or use the API to interact with the model.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3_8.png\" /\u003e\n\n## Curl 명령어를 사용하여 모델 쿼리\n\nOllama는 curl을 사용하기 위해 포트 11434에 노출된 엔드포인트 (/api/generate)를 노출했습니다. 다음 형식을 사용하여 쿼리할 수 있습니다.\n\n```js\ncurl http://localhost:11434/api/generate -d \"{ \\\"model\\\": \\\"llama3:instruct\\\", \\\"prompt\\\": \\\"무지개에는 몇 가지 색이 있나요?\\\", \\\"stream\\\": false }\"\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3_9.png\" /\u003e\n\n## 포스트맨(Postman)을 이용하여 모델 조회하기\n\n- Postman을 엽니다.\n- 요청 방법으로 POST를 선택합니다.\n- URL 입력 필드에 엔드포인트를 제공합니다: localhost:11434/api/generate.\n- 요청 바디 형식으로 JSON을 선택합니다.\n- 요청 바디 내용을 아래와 같이 제공합니다. \"prompt\": \"value\"를 적절한 내용으로 바꿉니다.\n\n```js\n{\n    \"model\": \"llama3:instruct\",\n    \"prompt\":\"무지개에는 몇 가지 색이 있나요?\",\n    \"stream\":false\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Ollama 엔드포인트에 요청을 보내려면 Send 버튼을 클릭하세요.\n- 아래와 같이 결과를 받게 됩니다.\n\n```js\n{\n    \"model\": \"llama3:instruct\",\n    \"created_at\": \"2024-04-29T17:34:38.0223636Z\",\n    \"response\": \"무지개에서 흔히 볼 수 있는 7가지 색은 다음과 같습니다.\\n\\n1. 빨강\\n2. 주황\\n3. 노랑\\n4. 초록\\n5. 파랑\\n6. 남색\\n7. 보라\\n\\n이 색은 때때로 ROY G BIV라는 약어를 사용하여 기억되기도 하는데, 각 글자는 색깔의 이름을 나타냅니다. 무지개에 대해 더 알고 싶으세요? 아니면 다른 점에 도움을 받고 싶으세요?\",\n    \"done\": true,\n    \"context\": [\n        128006,\n        ...\n        128009\n    ],\n    \"total_duration\": 17278010500,\n    \"load_duration\": 5247897400,\n    \"prompt_eval_count\": 19,\n    \"prompt_eval_duration\": 1196966000,\n    \"eval_count\": 92,\n    \"eval_duration\": 10829807000\n}\n```\n\n![UnlockingLlama3YourUltimateGuidetoMasteringLlama3_10](/assets/img/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3_10.png)\n\n만약 이 기사가 마음에 드셨다면 👏 버튼을 클릭하여 공유해 주세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 튜토리얼의 전체 소스 코드는 여기에서 찾을 수 있어요,\n\n## 참고문헌","ogImage":{"url":"/assets/img/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3_0.png"},"coverImage":"/assets/img/2024-06-20-UnlockingLlama3YourUltimateGuidetoMasteringLlama3_0.png","tag":["Tech"],"readingTime":11},{"title":"RL 경계를 넓히기 LLMs와 VLMs와 같은 기본 모델을 강화 학습에 통합하기","description":"","date":"2024-06-20 18:53","slug":"2024-06-20-PushingRLBoundariesIntegratingFoundationalModelsegLLMsandVLMsintoReinforcementLearning","content":"\n\n## Foundational Models를 RL Training 루프에 통합하는 심층 탐구\n\n저자: Elahe Aghapour, Salar Rahili\n\n## 개요:\n\n트랜스포머 아키텍처와 고성능 컴퓨팅 기술의 발전으로 인해, 최근에는 Foundational 모델을 훈련하는 것이 핫한 주제로 떠올랐습니다. 이를 통해, RL(강화 학습) 알고리즘의 성능을 향상시키기 위해 Foundational 모델을 통합하거나 훈련하는 노력들이 이루어지고 있으며, 이는 이 분야에 대한 흥미로운 방향을 시사합니다. 여기에서 Foundational 모델이 강화 학습에 큰 도움이 될 수 있는지에 대해 논의하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최신 연구에 대한 탐구에 앞서, 기반 모델이 강화 학습에 큰 도움을 줄 수 있는 방법에 대해 이야기해보겠습니다. 우리의 목표는 사전 훈련된 기반 모델, 특히 대형 언어 모델(LLMs) 또는 비전-언어 모델(VLMs)이 우리를 어떻게 지원할 수 있는지, 또는 어떻게 기반 모델을 처음부터 훈련시킬 수 있는지를 확인하는 것입니다. 유용한 접근 방법은 강화 학습 훈련 루프의 각 요소를 개별적으로 검토하여 개선의 여지가 있는 곳을 식별하는 것입니다:\n\n![이미지](/assets/img/2024-06-20-PushingRLBoundariesIntegratingFoundationalModelsegLLMsandVLMsintoReinforcementLearning_0.png)\n\n1- 환경: 사전 훈련된 기반 모델은 사건 간의 인과 관계를 이해하기 때문에 현재 행동으로 인한 환경 변화를 예측하는 데 활용할 수 있습니다. 이 개념은 흥미로운데, 현재는 이에 중점을 두는 구체적인 연구에 대해 알지 못합니다. 이 아이디어를 더 탐구하는 데 제한된 이유가 두 가지 있습니다.\n\n- 강화 학습 훈련 과정은 다음 단계 관측에 대해 매우 정확한 예측을 요구하는데, 사전 훈련된 LLMs/VLMs가 이에 부합하는 데이터셋에서 직접 훈련되지 않아 이 측면에서 미흡할 수 있습니다. 우리가 이전 글에서 강조한 바와 같이, 특히 평생 학습 시나리오에서 사용되는 고수준 플래너는 효과적으로 기반 모델을 통합할 수 있습니다.\n- 환경 단계의 지연은 RL 알고리즘을 제한할 수 있는 중요한 요소입니다. 특히 훈련 단계에 대한 고정 예산 내에서 작업할 때 매우 큰 지연을 도입하는 모델은 제약을 줄 수 있습니다. 도전적일 수 있지만, 작은 네트워크로 압축하는 것이 여기에 해결책이 될 수 있음을 유의해 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2- 상태 (LLM/VLM 기반의 상태 생성기): 전문가들은 종종 관찰과 상태라는 용어를 서로 바꿔 사용하지만, 이 둘 사이에는 차이가 있습니다. 상태는 환경의 포괄적인 표현이며, 반면에 관찰은 부분적인 정보만을 제공할 수 있습니다. 표준 강화 학습 프레임워크에서는 관측치, 지난 행동 및 환경의 내부 지식에서 유용한 특징을 추출하고 병합하여 \"상태\"인 정책 입력을 생성하는 구체적인 변환에 대해 자주 논의하지 않습니다. 이러한 변환은 LLMs/VLMs를 활용함으로써 크게 향상될 수 있으며, 우리에게 세계, 물리학 및 역사에 대한 폭넓은 지식을 \"상태\"에 주입할 수 있는 기회를 제공합니다 (핑크로 강조된 그림 1을 참조).\n\n3- 정책 (기초적인 정책 모델): 정책에 기초 모델을 통합하는 것은 강화 학습에서 중요한 결정 요소인 중심적인 의사 결정 요소이므로 매우 유익할 수 있습니다. 고수준 계획을 생성하기 위해 이러한 모델을 활용하는 것이 성공적이었지만, 상태를 저수준 행동으로 변환하는 것은 나중에 자세히 다뤄볼 문제가 있습니다. 다행히도 최근에 이 분야에서 약간의 유망한 연구가 있었습니다.\n\n4- 보상 (LLM/VLM 기반의 보상 생성기): 연구자들 사이에서 선택된 행동을 보다 정확하게 평가하기 위해 기초 모델을 활용하는 것이 주요 관심사가 되었습니다. 보상은 인간과 에이전트 사이의 커뮤니케이션 채널로 기능해 왔으며, 목표를 설정하고 에이전트를 희망하는 방향으로 안내하는 중요한 역할을 합니다.\n\n- 사전 훈련된 기초 모델은 세계에 대한 깊은 지식을 가지고 있으며, 이러한 이해를 의사 결정 과정에 주입함으로써 결정을 인간의 욕망과 더 잘 일치하고 성공할 가능성을 높일 수 있습니다. 또한, 기초 모델을 사용하여 에이전트의 행동을 평가하면 검색 공간을 신속하게 줄이고 에이전트에게 이해를 시작하는 데 선뜻 도와줄 수 있습니다.\n- 사전 훈련된 기초 모델은 대부분 인간에 의해 생성된 인터넷 규모의 데이터로 훈련되었으며, 이는 그들이 인간과 유사한 방식으로 세계를 이해할 수 있도록 만들었습니다. 이는 기초 모델을 비용 효율적인 주석 생성기로 활용할 수 있게 만들어 줍니다. 그들은 대규모로 레이블을 생성하거나 궤적이나 롤아웃을 평가할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1- 보상에 대한 기본 모델\n\n에이전트 설정에 매우 의존적이며 기본 모델의 교육 데이터 세트에서 불충분하게 나타나는 낮은 수준의 조치를 생성하는 데 기초 모델을 사용하는 것은 어려운 과제입니다. 따라서, 기초 모델 응용은 일반적으로 낮은 수준의 조치보다는 높은 수준의 계획에 중점을 두고 있습니다. 보상은 높은 수준의 계획자와 낮은 수준의 조치 사이의 간격을 메꾸는 역할을 하고 기초 모델을 사용할 수 있게 합니다. 연구자들은 보상 할당을 위해 기초 모델을 통합하는 다양한 방법론을 채택했습니다. 그러나 핵심 원칙은 VLM/LLM을 활용하여 하위 목표나 작업을 향한 진행을 효과적으로 추적하는 데 있습니다.\n\n1.a 유사성을 기반으로 보상값 할당\n\n보상값을 고려할 때, 이는 에이전트의 이전 조치가 목표에 가까이 이동하는 데 유익했는지를 나타내는 신호로 간주될 수 있습니다. 합리적인 방법은 이전 조치가 현재 목표와 얼마나 밀접하게 일치하는지를 평가하는 것입니다. 이 방법을 실천하기 위해, 그림 2에서 볼 수 있듯이, 반드시 다음과 같은 작업이 필요합니다:\n- 이러한 조치의 의미 있는 임베딩을 생성합니다. 이는 가장 최근 관측의 이미지, 비디오 또는 텍스트 설명을 통해 수행할 수 있습니다.\n- 현재 목표의 의미 있는 표현을 생성합니다.\n- 이러한 표현 사이의 유사성을 평가합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![2024-06-20-PushingRLBoundariesIntegratingFoundationalModelsegLLMsandVLMsintoReinforcementLearning_1.png](/assets/img/2024-06-20-PushingRLBoundariesIntegratingFoundationalModelsegLLMsandVLMsintoReinforcementLearning_1.png)\n\n이 분야의 선도적인 연구에 대한 구체적인 메커니즘을 살펴봅시다.\n\n밀도 높고 잘 정의된 보상 함수는 RL 에이전트의 안정성과 훈련 속도를 향상시킵니다. 내재 보상은 혁신적인 상태의 탐험에 대해 에이전트를 보상하여 이러한 도전에 대처합니다. 그러나 대부분의 보이지 않는 상태가 하류 작업과 관련이 없는 대규모 환경에서는 이 방법이 효과가 줄어듭니다. ELLM은 탐사를 형성하기 위해 LLM의 백그라운드 지식을 활용합니다. LLM에게 에이전트의 사용 가능한 작업 목록과 상태 캡션 생성기에 의해 생성된 에이전트 현재 관찰의 텍스트 설명 목록이 주어졌을 때, LLM에게 가능한 목표/하위 목표 목록을 생성하도록 요청합니다. 그런 다음 각 시간 스텝에서 보상은 LLM이 생성한 목표와 에이전트의 전환 설명 간의 의미적 유사성, 코사인 유사성에 의해 계산됩니다.\n\nLiFT도 비슷한 프레임워크를 가지고 있지만 보상 할당을 위해 CLIP4Clip 스타일 VLMs를 활용합니다. CLIP4Clip은 대립 학습을 통해 비디오와 해당 언어 설명을 정렬하는 데 사전 훈련되었습니다. LiFT에서 에이전트는 CLIP4Clip에 의해 코딩된 과제 지침과 에이전트의 해당 행동의 비디오 간의 정렬 점수, 코사인 유사성에 기반해 보상을 받습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nUAFM은 로봇 조작 작업에 중점을 둔 유사한 프레임워크를 갖고 있습니다. 예를 들어, 물건을 쌓는 작업과 같은 작업에 중점을 둡니다. 보상 할당에서는 에이전트 상태 이미지와 작업 설명 사이의 유사성을 측정하며 둘 다 CLIP에 의해 임베드됩니다. 이들은 시뮬레이션된 쌓기 도메인의 소량의 데이터로 CLIP을 미세 조정하여 이용 사례에 더욱 일치시킵니다.\n\n1.b 보조 작업 추론을 통한 보상 할당:\n\n환경의 올바른 이해를 갖고 있는 기본 모델이 있는 시나리오에서는 궤적 내의 관측값을 직접 모델(LIM/VLM)로 전달하는 것이 실현 가능해집니다. 관측값을 기반으로 한 간단한 QA 세션을 통해 또는 관측값 궤적을 보고 목표를 예측하는 모델의 능력을 검증함으로써 이 평가를 할 수 있습니다.\n\n![image](/assets/img/2024-06-20-PushingRLBoundariesIntegratingFoundationalModelsegLLMsandVLMsintoReinforcementLearning_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRead and Reward은 환경 설명서를 보상 생성에 통합하는 데 두 가지 핵심 구성 요소를 통해 수행됩니다. 다음과 같이 확인할 수 있습니다.:\n\n- QA 추출 모듈: 이 모듈은 게임 목표와 기능에 대한 요약을 생성합니다. 이 LLM 기반 모듈인 RoBERTa-large는 게임 설명서와 질문을 입력 받아 텍스트로부터 해당 답변을 추출합니다. 질문은 게임 목표와 에이전트-객체 상호 작용에 초점을 맞추며, TF-IDF를 사용하여 중요도를 식별합니다. 각 핵심 객체에 대해 \"플레이어가 '객체'를 칠 때 무슨 일이 일어납니까?\"라는 질문이 질문 세트에 추가됩니다. 그런 다음 모든 비어 있지 않은 질문-답변 쌍을 연결하여 요약이 형성됩니다.\n- 추론 모듈: 게임 플레이 중에 규칙 기반 알고리즘이 \"히트\" 이벤트를 감지합니다. 각 \"히트\" 이벤트 후에는 환경의 요약과 질문 \"승리하려면 '상호 작용 객체'를 치면 괜찮을까요?\"로 LLM 기반 추론 모듈에 쿼리됩니다. 가능한 답변은 '예, 아니오'로 제한됩니다. \"예\" 응답은 긍정적인 보상을 추가하고, \"아니오\"는 부정적인 보상으로 이어집니다.\n\nEAGER는 특별히 설계된 보조 작업을 통해 내재적 보상을 생성하는 독특한 방법을 소개합니다. 이 방식은 현재 관찰을 기반으로 목표를 예측하는 보조 작업을 포함하는 혁신적인 개념을 제시합니다. 모델이 정확하게 예측하면 의도된 목표와 강력한 일치를 나타내며, 따라서 예측 신뢰도 수준에 따라 더 큰 내재적 보상이 제공됩니다. 이 목표를 달성하기 위해 두 모듈이 사용됩니다.:\n\n- 질문 생성 (QG): 이 구성 요소는 사용자가 제공한 상세 목표에서 모든 명사와 형용사를 마스킹하여 작동합니다.\n- 질문 응답 (QA): 이 모델은 관찰, 질문 마스크 및 작업을 입력으로 받아 마스킹된 토큰을 예측하는 지도 방식으로 교육된 모델입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n(P.S. 기본 모델을 사용하지 않았지만 흥미로운 접근 방식으로 여기에 포함했습니다. 이 접근 방식은 사전 훈련된 LLM에 쉽게 적용할 수 있습니다.)\n\n1. 보상 함수 코드 생성\n\n지금까지는 보상 학습 알고리즘을 위해 직접 보상 값을 생성하는 방법에 대해 논의했습니다. 그러나 RL 루프의 각 단계에서 대형 모델을 실행하는 것은 교육 및 추론의 속도를 현저하게 늦출 수 있습니다. 이 병목 현상을 우회하기 위한 한 가지 전략은 기본 모델을 활용하여 보상 함수의 코드를 생성하는 것입니다. 이를 통해 각 단계에서 보상 값을 직접 생성하여 프로세스를 간소화할 수 있습니다.\n\n보상 코드 생성 스키마가 효과적으로 작동하려면 두 가지 주요 구성 요소가 필요합니다:\n1. 모든 필요한 정보를 포함하는 자세한 프롬프트를 수신하는 코드 생성기(LMM).\n2. 코드 생성기와 협업하여 코드를 평가하고 향상시키는 정제 프로세스.\n보상 코드를 생성하는 데 중요한 기여를 살펴보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nR2R2S는 두 가지 주요 구성 요소를 통해 보상 함수 코드를 생성합니다:\n\n- LLM 기반 동작 설명자: 이 모듈은 미리 정의된 템플릿을 사용하여 로봇 동작을 설명하고, 대형 언어 모델 (LLM)을 활용하여 동작을 이해합니다. 동작 설명자는 템플릿을 채워 넣어 \"목적지 좌표\" 등의 자리 표시자를 구체적인 세부 정보로 대체하여 미리 정의된 템플릿 내에서 원하는 로봇 동작을 설명합니다.\n- LLM 기반 보상 코드 생성기: 이 구성 요소는 보상 함수 코드를 생성합니다. 이때 prompt에 포함된 내용은 동작 설명, LLM이 보상 함수 코드를 생성하는 데 사용할 수 있는 함수 목록과 설명, 응답이 어떻게 보이는지를 보여주는 예제 코드, 그리고 보상 함수가 따라야 하는 제약 조건과 규칙입니다.\n\nText2Reward는 반복적인 개선 내에서 실행 가능한 코드로 밀집 보상 함수를 생성하는 방법을 개발합니다. 작업의 하위 목표가 주어졌을 때, 두 가지 주요 구성 요소가 있습니다:\n\n- LLM 기반 보상 코드 생성기: 보상 함수 코드를 생성합니다. prompt에는 관측과 가능한 동작의 요약, 객체, 로봇 및 호출 가능한 함수의 구성을 나타내는 간결한 파이썬 스타일 환경, 보상 함수 설계를 위한 배경 지식 (예: \"작업 X의 보상 함수에는 일반적으로 객체 x와 y 간의 거리를 포함하는 항목이 포함됨\"), 그리고 몇 가지 샷 예시가 포함됩니다. 그들은 명령어의 풀에 액세스하고, 상위 k개 유사한 명령어를 몇 가지 샷 예시로 검색한다고 가정합니다.\n- LLM 기반 개선: 보상 코드가 생성되면, 코드가 실행되어 구문 오류와 런타임 오류를 식별합니다. 이러한 피드백은 후속 prompt로 통합되어 더 정교한 보상 함수를 생성합니다. 추가로, 현재 정책에 의한 작업 실행 비디오를 기반으로 사용자 피드백이 요청됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAuto MC-Reward은 Text2Reward와 유사한 알고리즘을 가지고 있습니다. 보상 함수 코드를 생성하려면 Fig. 4를 참조하세요. 주요 차이점은 두 개의 LLM을 갖고 있는 세밀화 단계에 있습니다:\n\n- LLM 기반 보상 평가자: 코드를 평가하고 코드가 자기 일관성이 있고 구문 및 의미적 오류가 없는지에 대한 피드백을 제공합니다.\n- LLM 기반 경로 분석자: 훈련된 에이전트와 환경 사이의 상호 작용에 대한 과거 정보를 검토하고 보상 함수의 수정을 안내하는 데 사용합니다.\n\n![이미지](/assets/img/2024-06-20-PushingRLBoundariesIntegratingFoundationalModelsegLLMsandVLMsintoReinforcementLearning_3.png)\n\nEUREKA는 특정 작업 프롬프팅, 사전 정의된 보상 템플릿 또는 사전 정의된 소수의 예제가 필요하지 않고 보상 코드를 생성합니다. 이 목표를 달성하기 위해 두 단계가 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- LLM 기반 코드 생성: 원시 환경 코드, 작업, 일반적 보상 설계 및 형식 지침이 LLM에게 컨텍스트로 제공되며, LLM은 실행 가능한 보상 코드와 해당 구성 요소 목록을 반환합니다.\n- 진화적 탐색 및 교정: 각 반복마다 EUREKA는 LLM에 쿼리를 보내서 여러 독립적인 보상 함수를 생성합니다. 실행 가능한 보상 함수로 에이전트를 훈련시키면 에이전트의 수행 상황을 피드백으로 제공합니다. 보상 함수의 각 구성 요소에 대한 상세하고 집중된 분석을 위해 피드백은 보상 함수의 각 구성 요소에 대한 스칼라 값도 포함합니다. LLM은 상위 성능을 발휘하는 보상 코드와 해당 상세한 피드백을 가져와서 보상 코드를 컨텍스트 내에서 변이시킵니다. 각 후속 반복에서 LLM은 상위 보상 코드를 참조로 사용하여 추가로 K개의 독립적인 보상 코드를 생성합니다. 이 반복적 최적화는 지정된 반복 횟수에 도달할 때까지 계속됩니다.\n\n이 두 단계 내에서 EUREKA는 전문가의 휴먼-엔지니어링 된 보상을 뛰어넘는 보상 함수를 생성할 수 있습니다.\n\n1.d. 선호에 기반한 보상 모델 훈련 (RLAIF)\n\n대체 방법으로 기초 모델을 사용하여 보상 함수 모델을 훈련하기 위한 데이터를 생성할 수 있습니다. 휴먼 피드백을 통한 강화 학습의 중요한 성공들로 Reinforcement Learning with Human Feedback (RLHF)가 최근에 큰 주목을 받으면서 교육된 보상 함수를 대규모로 사용하는 데 집중적인 관심이 집중되고 있습니다. 이러한 알고리즘의 핵심은 선호 데이터셋을 사용하여 보상 모델을 훈련시키는 것이며, 이는 후에 강화 학습 알고리즘에 통합될 수 있습니다. 휴먼 피드백을 통해 선호 데이터를 생성하는 높은 비용이 가질 수 있기 때문에, VLM/LLM과 같은 AI 에이전트에서 피드백을 얻어 이 데이터셋을 만드는 데 대한 관심이 증가하고 있습니다. AI 생성 데이터를 사용하여 보상 함수를 훈련시키고 강화 학습 알고리즘에 통합하는 것이 Reinforcement Learning with AI Feedback (RLAIF)로 알려져 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMOTIF은 충분한 커버리지를 갖춘 관측 pass 데이터 세트에 액세스해야 합니다. 먼저 LLM은 환경 내에서 원하는 행동에 대한 요약과 무작위로 추출한 두 관측의 텍스트 설명을 쿼리합니다. 그런 다음, 그림 5에 나와 있는 것처럼 촉진을 생성하여 1, 2 또는 0(좋아하는 것이 아님을 나타냄) 중 하나를 선택합니다. 이 프로세스는 관측 쌍 사이의 선호도 데이터 세트를 구성합니다. 이후에는 이 데이터 세트를 사용하여 기반 선호도 기반 RL 기법을 활용한 보상 모델을 교육합니다.\n\n![image](/assets/img/2024-06-20-PushingRLBoundariesIntegratingFoundationalModelsegLLMsandVLMsintoReinforcementLearning_4.png)\n\n2- 정책으로서의 기초 모델들\n\n지금까지 조사된 작업에서 뛰어나는 것뿐만 아니라, 과거 학습을 통해 새로운 작업을 추론하고 적응할 수 있는 능력을 갖추는 기본 정책을 교육하는 능력은 RL 커뮤니티에서의 포부입니다. 이러한 정책은 이전 경험으로부터 새로운 상황에 대처하고, 환경적 피드백을 통해, 인간과 유사한 적응성을 통해 이전에 본 적 없는 목표를 달성할 수 있도록 일반화되는 것이 이상적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나, 이러한 에이전트를 교육하는 데는 몇 가지 도전이 있습니다. 이 도전 중 몇 가지는 다음과 같습니다:\n\n- 매우 큰 모델을 관리해야 하므로, 낮은 수준의 제어 동작에 대한 의사 결정 과정에 상당한 대기 시간이 발생합니다.\n- 효과적인 학습을 위해 다양한 작업 영역에서 방대한 상호 작용 데이터를 수집해야 합니다.\n- 또한, RL을 사용하여 매우 큰 네트워크를 처음부터 교육하는 과정은 추가 복잡성을 도입합니다. 이는 역전파 효율이 감독 학습 방법과 비교하여 RL에서 더 약한 특성 때문입니다.\n\n지금까지 이 도메인에서 정말 한창 인 것은 주로 상당한 자원과 최고 수준의 환경을 갖춘 팀들뿐이었습니다.\n\nAdA는 X.Land 2.0 3D 환경 내에서 RL 기반 모델을 교육하는 길을 열었습니다. 이 모델은 추가 교육 없이 테스트 작업에서 인간 시간 규모의 적응을 달성합니다. 모델의 성공 요소는 세 가지입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- AdA의 학습 메커니즘 핵심은 2,300만 개에서 2억 6,500만 개의 매개변수로 이루어진 Transformer-XL 아키텍처로, Muesli RL 알고리즘과 함께 사용됩니다. Transformer-XL은 시간 t부터 T까지의 관측, 행동 및 보상의 경로를 입력으로 받아 각 시간 단계에 대해 숨겨진 상태의 일렬을 출력합니다. 숨겨진 상태는 보상, 가치 및 행동 분포 π를 예측하는 데 사용됩니다. 장기 및 단기 기억의 조합은 빠른 적응을 위해 중요합니다. 장기 기억은 천천히 경사 업데이트를 통해 달성되며, 단기 기억은 트랜스포머의 문맥 길이 내에서 포착될 수 있습니다. 이 독특한 조합은 모델이 환경이 재설정되어도 시도 간 메모리를 유지하여 여러 작업 시도 간 지식을 보존할 수 있게 합니다.\n- 모델은 변형자가 메타 학습자이기 때문에 1⁰⁴⁰개의 다양한 부분 관찰 가능한 마르코프 의사 결정 프로세스 (POMDPs) 작업을 통해 메타-RL 훈련의 혜택을 받습니다. 과제 풀의 크기와 다양성을 감안할 때, 많은 작업이 좋은 훈련 신호를 생성하기에는 너무 쉽거나 너무 어려울 수 있습니다. 이를 해결하기 위해 자동화된 커리큘럼을 사용하여 기능 경계 내에 있는 작업을 우선 순위로 두었습니다.\n\nRT-2는 로보틱 경로 데이터와 시각-언어 작업 양쪽에서 VLM에 대해 공동으로 세밀 조정하는 방법을 소개하며, 결과적으로 RT-2라는 정책 모델을 생성합니다. 시각-언어 모델이 저수준 액션을 생성할 수 있도록하기 위해, 액션은 256개의 바구니로 이산화되어 언어 토큰으로 표현됩니다.\n\n액션을 언어 토큰으로 표현함으로써, RT-2는 상당한 수정 없이 이미 존재하는 VLM 아키텍처를 직접 활용할 수 있습니다. 따라서 VLM 입력에는 로봇 카메라 이미지와 시각적 질문 응답 작업과 유사한 형식으로 구성된 텍스트 작업 설명이 포함되며, 출력은 로봇의 저수준 작업을 나타내는 언어 토큰의 일련이 됩니다. Fig. 6을 참조하세요.\n\n`\u003cimg src=\"/assets/img/2024-06-20-PushingRLBoundariesIntegratingFoundationalModelsegLLMsandVLMsintoReinforcementLearning_5.png\" /\u003e`\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그들은 웹 데이터를 사용한 두 가지 유형의 데이터에 대한 공동미세조정이 보다 일반화된 정책을 이끌어냄을 알아차렸습니다. 공동미세조정 프로세스는 RT-2에게 교육 데이터에 명시적으로 존재하지 않는 명령을 이해하고 실행할 수 있는 능력을 제공함으로써 놀랍도록 적응성을 보여줍니다. 이 접근법은 VLM의 인터넷 규모 사전학습을 통해 새로운 작업에 대한 일반화를 가능케 했습니다.\n\n3- 상태 표현으로서의 기반 모델\n\nRL에서 정책이 주어진 시점에 환경을 이해하는 것은 본질적으로 주변을 어떻게 인식하는지인 '상태'에서 옵니다. RL 블록 다이어그램을 살펴볼 때, 세계 지식을 주입할 합리적인 모듈은 상태입니다. 작업 완료에 유용한 일반 지식으로 관측을 풍부하게 하면, 정책은 처음부터 학습을 시작하는 RL 에이전트에 비해 새로운 작업을 더 빨리 학습할 수 있습니다.\n\nPR2L은 인터넷 규모의 VLM의 백그라운드 지식을 RL에 주입하는 새로운 방법론을 소개합니다. PR2L은 이미지 및 텍스트 입력에 대한 언어를 생성하는 생성적 VLM을 활용합니다. VLM은 시각 및 텍스트 입력을 이해하고 응답하는 데 능숙하기 때문에, 관찰에서 행동에 연결될 수 있는 의미론적 기능의 풍부한 소스를 제공할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPR2L은 각각의 시각적 관찰에 대해 VLM에 작업 관련 프롬프트로 쿼리하고, 생성된 텍스트 응답과 모델의 중간 표현을 받습니다. 그들은 텍스트를 버리고 시각 및 텍스트 입력 및 VLM의 생성된 텍스트 응답에 대한 중간 표현을 \"프롬프트 가능한 표현\"으로 사용합니다. 이러한 표현의 크기가 변수이기 때문에 PR2L은 모든 프롬프트 가능한 표현에 포함된 모든 정보를 고정 크기 임베딩으로 임베딩하기 위해 인코더-디코더 트랜스포머 레이어를 통합합니다. 이 임베딩은 비시각 관찰 데이터와 함께 사용되어 에이전트의 상태를 나타내는 정책 네트워크에 제공됩니다. 이 혁신적인 통합을 통해 RL 에이전트는 VLM의 풍부한 의미 이해와 배경 지식을 활용하여 작업을 보다 신속하고 정보화된 학습을 할 수 있습니다.\n\n참고문헌:\n\n[1] ELLM: Du, Yuqing 등. “Guiding pretraining in reinforcement learning with large language models.” 2023.\n[2] Text2Reward: Xie, Tianbao 등. “Text2reward: Automated dense reward function generation for reinforcement learning.” 2023.\n[3] R2R2S: Yu, Wenhao 등. “Language to rewards for robotic skill synthesis.” 2023.\n[4] EUREKA: Ma, Yecheng Jason 등. “Eureka: Human-level reward design via coding large language models.” 2023.\n[5] MOTIF: Klissarov, Martin 등. “Motif: Intrinsic motivation from artificial intelligence feedback.” 2023.\n[6] Read and Reward: Wu, Yue 등. “Read and reap the rewards: Learning to play atari with the help of instruction manuals.” 2024.\n[7] Auto MC-Reward: Li, Hao 등. “Auto MC-reward: Automated dense reward design with large language models for minecraft.” 2023.\n[8] EAGER: Carta, Thomas 등. “Eager: Asking and answering questions for automatic reward shaping in language-guided RL.” 2022.\n[9] LiFT: Nam, Taewook 등. “LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers.” 2023.\n[10] UAFM: Di Palo, Norman 등. “Towards a unified agent with foundation models.” 2023.\n[11] RT-2: Brohan, Anthony 등. “Rt-2: Vision-language-action models transfer web knowledge to robotic control.” 2023.\n[12] AdA: Team, Adaptive Agent 등. “Human-timescale adaptation in an open-ended task space.” 2023.\n[13] PR2L: Chen, William 등. “Vision-Language Models Provide Promptable Representations for Reinforcement Learning.” 2024.\n[14] Clip4Clip: Luo, Huaishao 등. “Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning.” 2022.\n[15] Clip: Radford, Alec 등. “Learning transferable visual models from natural language supervision.” 2021.\n[16] RoBERTa: Liu, Yinhan 등. “Roberta: A robustly optimized bert pretraining approach.” 2019.\n[17] Preference based RL: SWirth, Christian 등. “A survey of preference-based reinforcement learning methods.” 2017.\n[18] Muesli: Hessel, Matteo 등. “Muesli: Combining improvements in policy optimization.” 2021.\n[19] Melo, Luckeciano C. “Transformers are meta-reinforcement learners.” 2022.\n[20] RLHF: Ouyang, Long 등. “Training language models to follow instructions with human feedback, 2022.\n\n","ogImage":{"url":"/assets/img/2024-06-20-PushingRLBoundariesIntegratingFoundationalModelsegLLMsandVLMsintoReinforcementLearning_0.png"},"coverImage":"/assets/img/2024-06-20-PushingRLBoundariesIntegratingFoundationalModelsegLLMsandVLMsintoReinforcementLearning_0.png","tag":["Tech"],"readingTime":15}],"page":"2","totalPageCount":71,"totalPageGroupCount":4,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"2"},"buildId":"FH3Qr-mLAesqA0X5IFRQr","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>