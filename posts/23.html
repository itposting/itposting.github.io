<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/23" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/23" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-2d104a861d88ea21.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_buildManifest.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="콜모고로프-아놀드 네트워크 혹평인가, 딥 러닝 혁명인가" href="/post/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="콜모고로프-아놀드 네트워크 혹평인가, 딥 러닝 혁명인가" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="콜모고로프-아놀드 네트워크 혹평인가, 딥 러닝 혁명인가" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">콜모고로프-아놀드 네트워크 혹평인가, 딥 러닝 혁명인가</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">16<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="OpenAI Assistant API와 Streamlit을 사용하여 도우미 만들기" href="/post/2024-06-19-CreatinganAssistantwithOpenAIAssistantAPIandStreamlit"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="OpenAI Assistant API와 Streamlit을 사용하여 도우미 만들기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-CreatinganAssistantwithOpenAIAssistantAPIandStreamlit_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="OpenAI Assistant API와 Streamlit을 사용하여 도우미 만들기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">OpenAI Assistant API와 Streamlit을 사용하여 도우미 만들기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="대규모 언어 모델Large Language Models, LLMs 이해하기" href="/post/2024-06-19-UnderstandingLargeLanguageModelsLLMs"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대규모 언어 모델Large Language Models, LLMs 이해하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-UnderstandingLargeLanguageModelsLLMs_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대규모 언어 모델Large Language Models, LLMs 이해하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">대규모 언어 모델Large Language Models, LLMs 이해하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="100 무료, 추적 가능하고 안전한 RAG 챗봇 만들기 Reranker와 GPT-4o를 활용해 보세요" href="/post/2024-06-19-Builda100FreeTraceableandSecureRAGChatbotUsingRerankerandGPT-4o"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="100 무료, 추적 가능하고 안전한 RAG 챗봇 만들기 Reranker와 GPT-4o를 활용해 보세요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Builda100FreeTraceableandSecureRAGChatbotUsingRerankerandGPT-4o_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="100 무료, 추적 가능하고 안전한 RAG 챗봇 만들기 Reranker와 GPT-4o를 활용해 보세요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">100 무료, 추적 가능하고 안전한 RAG 챗봇 만들기 Reranker와 GPT-4o를 활용해 보세요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Anthropic의 Sparse Autoencoder를 직접 깊이 파헤쳐 보기 " href="/post/2024-06-19-DeepDiveintoAnthropicsSparseAutoencodersbyHand"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Anthropic의 Sparse Autoencoder를 직접 깊이 파헤쳐 보기 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-DeepDiveintoAnthropicsSparseAutoencodersbyHand_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Anthropic의 Sparse Autoencoder를 직접 깊이 파헤쳐 보기 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Anthropic의 Sparse Autoencoder를 직접 깊이 파헤쳐 보기 </strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="텍스트, 이미지 및 오디오를 지원하는 멀티모달 LLM LLaVA  Whisper 구축 방법" href="/post/2024-06-19-HowtoBuildaTextImageandAudio-CapableMultimodalLLMLLaVAWhisper"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="텍스트, 이미지 및 오디오를 지원하는 멀티모달 LLM LLaVA  Whisper 구축 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-HowtoBuildaTextImageandAudio-CapableMultimodalLLMLLaVAWhisper_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="텍스트, 이미지 및 오디오를 지원하는 멀티모달 LLM LLaVA  Whisper 구축 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">텍스트, 이미지 및 오디오를 지원하는 멀티모달 LLM LLaVA  Whisper 구축 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="대화 형식으로 친근한 말투로 번역하면 다음과 같습니다 커다란 언어 모델을 사용한 사고 체계인 Buffer of ThoughtsBoT 이해하기" href="/post/2024-06-19-UnderstandingBufferofThoughtsBoTReasoningwithLargeLanguageModels"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대화 형식으로 친근한 말투로 번역하면 다음과 같습니다 커다란 언어 모델을 사용한 사고 체계인 Buffer of ThoughtsBoT 이해하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-UnderstandingBufferofThoughtsBoTReasoningwithLargeLanguageModels_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대화 형식으로 친근한 말투로 번역하면 다음과 같습니다 커다란 언어 모델을 사용한 사고 체계인 Buffer of ThoughtsBoT 이해하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">대화 형식으로 친근한 말투로 번역하면 다음과 같습니다 커다란 언어 모델을 사용한 사고 체계인 Buffer of ThoughtsBoT 이해하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="당신의 LLM에 맞는 적절한 RAG 프레임워크 선택 LlamaIndex 또는 LangChain" href="/post/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="당신의 LLM에 맞는 적절한 RAG 프레임워크 선택 LlamaIndex 또는 LangChain" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="당신의 LLM에 맞는 적절한 RAG 프레임워크 선택 LlamaIndex 또는 LangChain" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">당신의 LLM에 맞는 적절한 RAG 프레임워크 선택 LlamaIndex 또는 LangChain</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="랭체인 대 람마 인덱스 여러분의 창조적 AI 프로젝트에 딱 맞는 것 찾기" href="/post/2024-06-19-LangChainvsLlamaIndexFindingthePerfectFitforYourGenerativeAIProjects"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="랭체인 대 람마 인덱스 여러분의 창조적 AI 프로젝트에 딱 맞는 것 찾기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-LangChainvsLlamaIndexFindingthePerfectFitforYourGenerativeAIProjects_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="랭체인 대 람마 인덱스 여러분의 창조적 AI 프로젝트에 딱 맞는 것 찾기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">랭체인 대 람마 인덱스 여러분의 창조적 AI 프로젝트에 딱 맞는 것 찾기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LLM 여행 개념 증명부터 제품화까지" href="/post/2024-06-19-AnLLMJourneyFromPOCtoProduction"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LLM 여행 개념 증명부터 제품화까지" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-AnLLMJourneyFromPOCtoProduction_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LLM 여행 개념 증명부터 제품화까지" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">LLM 여행 개념 증명부터 제품화까지</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/21">21</a><a class="link" href="/posts/22">22</a><a class="link posts_-active__YVJEi" href="/posts/23">23</a><a class="link" href="/posts/24">24</a><a class="link" href="/posts/25">25</a><a class="link" href="/posts/26">26</a><a class="link" href="/posts/27">27</a><a class="link" href="/posts/28">28</a><a class="link" href="/posts/29">29</a><a class="link" href="/posts/30">30</a><a class="link" href="/posts/31">31</a><a class="link" href="/posts/32">32</a><a class="link" href="/posts/33">33</a><a class="link" href="/posts/34">34</a><a class="link" href="/posts/35">35</a><a class="link" href="/posts/36">36</a><a class="link" href="/posts/37">37</a><a class="link" href="/posts/38">38</a><a class="link" href="/posts/39">39</a><a class="link" href="/posts/40">40</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"콜모고로프-아놀드 네트워크 혹평인가, 딥 러닝 혁명인가","description":"","date":"2024-06-19 20:01","slug":"2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution","content":"\n\n## 더 좋은 해석 가능성, 작은 네트워크 크기 및 학습 가능한 활성화 함수가 MLPs를 무너뜨리게 될까요?\n\n# 주요 내용 (기사 개요)\n\nSubstack 그룹 채팅, LinkedIn 등에서 생각을 공유해 준 모든 분들께 감사드립니다. 거기서 우리가 가지는 대화들을 모두 사랑하고, 앞으로도 계속 많은 대화를 이어갈 예정입니다.\n\nKolmogorov–Arnold Networks 및 그들이 과학적 기능을 모델링하는 데 특히 유리할 수 있는 잠재력에 대해 많은 이야기가 되었습니다. 본 기사에서는 KANs 및 그들이 새로운 세대의 딥 러닝에서의 타당성에 대해 탐구할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![KANs 이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_0.png)\n\n먼저, KANs와 그들을 가능하게 하는 이론에 대해 간단히 살펴보겠습니다.\n\n콜모고로프-아놀드 표현 정리: 모든 무서운 방정식과 정의를 건너뛰고 간단한 설명으로 해결합시다. KART는 여러 입력을 가진 연속 함수는 단일 입력 (사인이나 제곱과 같은)의 간단한 함수들을 결합하고 더하는 것으로 생성할 수 있다는 것을 말합니다. 예를 들어, 다중 변수 함수 f(x,y)= x*y는 ( (x + y)² — (x² +y²) ) / 2로 표현할 수 있습니다. 이는 덧셈, 뺄셈 및 제곱 연산(단일 입력 함수)만 사용합니다. 실제 KART는 뺄셈을 덧셈으로 다시 구성하는 것이 포함되지만 여기서는 이를 간단하게 유지하겠습니다.\n\nKANs (콜모고로프-아놀드 네트워크) - 기존의 MLP (다층 퍼셉트론)과 달리 고정된 노드 활성화 함수를 갖는 MLPs와 달리, KANs는 가장자리에 학습 가능한 활성화 함수를 사용하여 선형 가중치를 비선형 가중치로 대체합니다. 이는 KANs를 보다 정확하고 해석 가능하게 만들어주며, 특히 학문적 응용 및 일상생활에서 자주 발견되는 희소한 합성 구조와 관련이 있는 함수에 유용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_1.png)\n\n와우, 구성 요소 구조가 뭔지 궁금하셨나요? 제처럼 gawaar(무지한 사람)이라면 \"희소한 구성 요소 구조\"라는 말에 혼란스러울 수 있습니다. 그럼 이건 뭐길래요, 그렇게 중요한가요? 간단히 말하자면- 특정 함수가 몇 개의 간단한 함수로 구성되어 있고 각 함수가 입력 변수 중 일부에만 의존할 때 그 함수는 희소한 구성 요소 구조를 갖고 있습니다. 예를 들어, 함수 f(x, y, z) = sin(x) * exp(y) + z는 구성 요소가 희소합니다.\n\n- 세 가지 간단한 함수인 sin(x), exp(y), z로 이루어져 있습니다.\n- 각 간단한 함수는 하나의 입력 변수(x, y, z 중 하나)에만 의존합니다.\n\n비교적으로, f(x, y, z) = x² * y³ + sin(x + y + z)와 같은 함수는 덜 희소하다고 볼 수 있습니다. 왜냐하면 더 복잡한 연산(x² * y³와 같은)이 필요하며 sin 함수에서 모든 세 가지 입력 변수를 결합해야하기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수학적으로 엄격한 정의를 원하는 경우, \"딥러닝의 기초: 계산 가능한 함수의 복합 희박성\"에서 발견한 정의는 다음과 같습니다.\n\n그들의 중요성을 요약하면, 저자들은 과학과 현실에서 만나는 대부분의 함수가 더 단순한 구조를 갖고 있어서, 그것들을 모델링하기 위해 MLP에 대안으로 KAN이 매력적이라고 언급합니다. 아래 표는 이 주장을 뒷받침하는 것으로 보입니다.\n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_2.png)\n\nKAN과 스플라인- KAN의 또 다른 흥미로운 특성은 스플라인의 활용입니다. 우리의 목적을 위해, 스플라인은 곡선에 맞게 구부러질 수 있는 유연한 자 등처럼 생각해야 합니다. 이들은 부드럽게 연결된 여러 다항 조각들로 이루어져 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](https://miro.medium.com/v2/resize:fit:576/0*9JUhBPlynwlgU86F.gif)\n\n스플라인을 활성화 함수로 사용함으로써 KAN은 입력 변수 간의 복잡한 관계를 학습할 수 있으면서 해석 가능성과 지역적인 제어를 유지할 수 있습니다. 뒤에 나오는 기사에서는 스플라인에 대해 더 자세히 설명하겠습니다.\n\nKAN에는 장단점이 있습니다. 먼저, 세 가지 주목할 만한 단점이 있습니다.\n- 연구 부족 - 새로운 아이디어인만큼 이해하지 못하는 것은 용서될 수 있지만, 새로운 아이디어를 살펴볼 때는 항상 이를 기억하는 것이 중요합니다. AI 연구 분야에서 천재아이디어에서 비실용적인 물건이 된 사례가 흔한 일입니다. 우리가 알기론 KAN에는 특정한 시점을 넘어서 발전하기 어렵게 하는 근본적인 장애물이 있을 수도 있습니다.\n- 시장 적합성 - 현재는 트랜스포머와 NN에 특화된 하드웨어가 만들어지고 있습니다. 이러한 개발은 KAN에 대한 강력한 선택 편향을 만들 수 있으며, 결국 그들의 수용을 방해할 수 있습니다 (이 훌륭한 의견을 제공해준 Patrick McGuinness에게 감사드립니다).\n- 훈련 속도가 느림 - KAN 훈련은 NN보다 10배 느립니다. 작업 중이신 내용에 따라 이는 그들을 부정할 수도 있고 큰 문제가 되지 않을 수도 있습니다. 수용/연구가 더 많아짐에 따라 해결되기도 할 것입니다 (혹시 여러분 중 누가 이 문제를 해결하게 된다면 좋겠네요). 하지만 현재로서는 더 많은 주류 방향으로의 수용을 막을 것이며 이는 규모 측면에서 주도되고 있는 곳에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nKANs은 여러 가지 장점을 가지고 있습니다:\n\n- 향상된 정확도: 다양한 작업에 대해 MLP보다 적은 매개변수로 더 낮은 RMSE 손실을 달성할 수 있습니다. 저자들은 몇 가지 놀라운 결과를 보여주며 Deepmind를 이기는 것을 포함합니다-\n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_3.png)\n\n- 유리한 스케일링 법칙: 더 빠른 신경 스케일링 법칙을 나타내며, 모델 크기가 커짐에 따라 성능 향상이 더 큽니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_4.png)\n\n- Catastrophic forgetting을 완화: 일반 MLPs는 이전에 학습한 내용을 새로운 내용으로 덮어쓰기 때문에 NNs가 이전 입력을 '잊어버리는' 문제를 발생시킵니다. KANs는 지속적인 학습을 용이하게 하는 지역 가용성을 활용합니다. 제가 Spines에 대해 언급할 때 이 기계적 측면에 대해 자세히 이야기하겠습니다. tl;dr 섹션에서- 1년 전 대화에서 일반 NNs의 주요 제한 사항으로 이를 지적한 첫 번째 사람으로 Dr. Bill Lambos에게 특별한 감사를 전하고 싶습니다. 그의 예측과 계산 신경과학자이자 생물학자(최초의 AI 전문가)로서의 통찰력은 여러 차례 옳았습니다.\n\n![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_5.png)\n\n- 설명 가능성- KANs는 더 설명 가능하며, 특정 분야에 매우 유리합니다. 저희 그룹 채팅 참가자 중 한 명이 그룹 채팅에서 다음과 같이 말했습니다-\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 상호작용성 - 사람들이 KAN과 조작하여 다양한 결과를 달성할 수 있는 근본적인 수준에서 상호작용할 수 있습니다. 네트워크 내에서 도메인 전문 지식을 입력하는이 사용법은 나에게 매우 유망해 보이며, 더 많은 사람들이 그에 대해 이야기하지 않는 것에 놀랍습니다. 제 반응이 과하게 반응인가요? 당신의 생각을 듣고 싶어요\n\n![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_6.png)\n\n이러한 장점들은 미래 탐사에 매력적인 전망을 제공합니다.\n\n이 기사의 나머지 부분은 KAN이 작동하는 속성을 조사할 것입니다. 기대하는 대로, 논의할 많은 포인트가 있으며, 때로는 이 기사가 약간 압도적일 수 있습니다. 이 분해 방법을 다루기 위해 우리는 적어도 느림을 준다는 구태의 없는 말을 사용할 것입니다. 우리는 현재의 신경망 기반 아키텍처에 대한 심층 학습의 이론적 기초와 그 파급 효과로 시작합시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 다양한 컨설팅 및 자문 서비스를 제공합니다. 함께 일할 수 있는 방법을 탐색하고 싶다면 여기 있는 내 소셜 미디어를 통해 저에게 연락하거나 이 이메일에 답장해주세요.\n\n15만 명 이상의 기술 리더와 함께하고 AI의 가장 중요한 아이디어에 대한 통찰을 무료 뉴스레터인 \"AI Made Simple\"을 통해 받아보세요.\n\n# 이론적 기반\n\n## A. 유니버설 근사 정리와 딥 러닝\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nUniversal Approximation Theorem (UAT)는 시그모이드 활성화 함수를 사용하는 뉴럴 네트워크가 한정된 수의 뉴런을 가진 단일 은닉층을 포함하여, 임의의 정확도로 제한된 세트의 연속 함수를 근사화할 수 있다고 말합니다. 우리가 해야 할 일은 비선형 함수의 다양한 조합을 계속해서 쌓아가는 것 뿐입니다. 그렇게 함으로써 우리가 원하는 함수의 근사치를 얻을 수 있습니다. (ML 모델에 데이터를 제공할 때, 우리는 모든 특징과 그들의 타겟과의 관계를 모델링하는 함수를 원합니다.)\n\n![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_7.png)\n\n나중에 다른 비선형성으로 확장되었으며, 이것이 왜 큰 신경망이 잘 작동하는지에 중요한 역할을 합니다. 큰 신경 네트워크 → 더 좋은 복잡한 비선형 관계를 모델링하기 위한 능력 → 연속 함수에 대한 더 가까운 근사값.\n\n계속하기 전에, 많은 사람들이 종종 간과하는 중요한 점이 있습니다. 모든 데이터셋/도메인이 연속 함수로 모델링될 수 있는 것은 아니라는 것입니다. 예를 들어, 더욱 울퉁불퉁한 의사결정 경계를 가진 특정 데이터셋이 있습니다. 이러한 경우, 신경망은 보다 매끄러운 의사결정 경계를 선호하므로 랜덤 포레스트와 같은 트리 기반 알고리즘이 더 나은 선택일 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 논문의 부록에서 저자들은 위 시각화에 관한 다음 문장을 제시했습니다-\n\n이제 이산적인 결정 공간을 다루는 매우 중요한 사용 사례도 있습니다. 여기서 신경망과 그래디언트 기반 방법은 대단히 Yamcha'ed 됩니다. 정부 검열에 대항하기 위한 제네바 프로젝트가 좋은 예입니다. 인공지능은 정부 검열을 회피하기 위해 창의적인 방법으로 4가지 기능을 결합하려고합니다. 이에 대해 자세히 알아 보려면 저희의 기사인 \"AI를 사용하여 검열에 맞서는 방법\" (이 프로젝트에 대한 깊은 살펴 보기) 또는 \"인터넷을 다시 제어하는 방법\" (인터넷의 기관적 조작의 영향을 줄이기 위해 적용할 수있는 몇 가지 AI 기술에 대한 살펴 보기)를 읽으실 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것이 중요한 이유는 신경망 및 그 구현에 대한 토론에서 종종 간과되는 세세한 부분이기 때문에 강조하고 있습니다. KAN(콜모고로프-아놀드 Representation Theorem)이 더 일반화되면서 비슷한 일이 발생할 수 있다는 걱정이 듭니다. 이 중요한 포인트를 언급했으니, KAN의 이야기로 넘어가겠습니다.\n\n## B. 콜모고로프-아놀드 Representation Theorem 이해하기\n\n콜모고로프-아놀드 Representation Theorem(KAT)은 바운드 영역에서의 임의의 연속 다변수 함수를 연속 단변수 함수의 유한 조합과 덧셈 연산으로 표현할 수 있다는 것을 제시합니다. 이 정리는 원칙적으로 고차원 함수를 학습하는 것을 단변수 함수의 다항수만 학습하는 것으로 줄일 수 있다는 것을 시사합니다. (비록 이러한 1D 함수 중 일부는 매끄럽지 않거나 프랙탈이거나 학습할 수 없을 수도 있습니다.)\n\n\u003cimg src=\"/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_10.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최근 연구에 따르면, 과학 응용 프로그램에서 많이 사용되는 다변량 함수들은 매끄럽고 희소한 구조를 가지며, 효과적인 실제 구현을 가능하게 합니다. 이것이 왜 KAN이 우리 과학계 사람들 사이에서 인기가 높아 보이는지 알겠죠. 이 부분은 이해하셨으니, 이제 KART의 매우 흥미로운 함의로 넘어가 봅시다.\n\n## C. KAN은 차원의 저주에 강하게 대응할 수 있을 것입니다\n\n차원의 저주란 입력 차원이 증가함에 따라 주어진 정확도를 달성하기 위해 필요한 데이터 포인트 또는 모델 파라미터의 급격한 증가를 가리킵니다. 이 문제는 고차원에서 데이터 포인트가 서로 점점 더 희소하고 멀어지기 때문에 발생하며, 기저 관계를 캡처하기 위해 더 많은 데이터와 모델 복잡성이 필요해집니다.\n\nKAN은 단변량 함수에 의존하고 합성 구조를 활용할 수 있기 때문에 차원의 저주를 완화할 수 있는 잠재력이 있습니다. 이것은 정말 멋지지만, 저는 현재 더 큰 규모와 더 많은 다양성이 통합된 상황에서 어떻게 잘 유지될지에 대해 의심하고 있습니다. 그럼에도 불구하고, 그들이 어떻게 진전될지에 대해 조심스럽게 낙관적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기반이 마련되었으니 이제 실제 KAN 구조와 그 작동을 이루는 다른 부분들에 대해 이야기해 보겠습니다.\n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_11.png)\n\n# 콜모고로프-아놀드 네트워크(KANs): 심층적인 탐구\n\n## A. KANs의 아키텍처\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n페이퍼의 2.2절에는 많은 정의와 표기법이 나와 있어서, 한 걸음씩 세밀히 설명해보겠습니다. 먼저, 일부 배경 정보-\n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_12.png)\n\n가장 중요한 부분은 마지막 강조 부분입니다- 우리는 더 넓고 깊은 KANs가 필요합니다 (smh, 이제는 인공지능 모델을 몸매 비하하는 걸 믿을 수가 없네). 그렇다면 우리가 KANs를 어떻게 강화할 수 있을까요? 임의로 큰 신경망의 비밀은 내일이 없는 것처럼 레이어를 쌓을 수 있는 능력에 있습니다. 그저 마음대로 레이어를 쌓아올리기가 KANs로는 문제인 이유는, 우리가 레이어에 대응하는 유사한 개념이 부족하기 때문입니다. 그래서, KAN 레이어에 대한 정의를 찾아보고, 이것을 확장하여 더 큰 chonky bois를 얻어보겠습니다.\n\n우리는 KAN 레이어를 다음과 같이 정의할 수 있습니다-\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_13.png)\n\n더 깊은 KAN을 만들어봅시다.\n\nKAN의 형태는 배열로 표시됩니다: [n0, n1, …, nL], 여기서 ni는 i번째 레이어의 노드 수를 의미합니다. 차원이 n0인 입력 벡터 x0가 주어지면, L개 레이어로 구성된 KAN 네트워크는 출력을 다음과 같이 계산합니다:\n\nKAN(x) = (ΦL−1 ◦ ΦL−2 ◦ … ◦ Φ1 ◦ Φ0)x. 이는 출력이 입력 레이어부터 시작하여 각 레이어의 활성화 함수를 순차적으로 적용하여 얻어진다는 의미입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같이 다시 작성할 수도 있습니다.\n\n![Image 1](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_14.png)\n\n우리의 작업은 미분 가능하기 때문에 backprop으로 KAN을 학습할 수 있습니다.\n\n![Image 2](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_15.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nKAN 레이어를 최적화하는 중요한 몇 가지 팁이 있어요.\n\n잔차 활성화 함수. 우리는 잔차 연결과 비슷한 기저 함수 𝑏⁢(𝑥)(residual connections)를 포함하여 활성화 함수 𝜙⁢(𝑥)가 기저 함수 𝑏⁢(𝑥)과 스플라인 함수의 합으로 정의됩니다:\n\n𝜙⁢(𝑥)=𝑤⁢(𝑏⁢(𝑥)+spline⁢(𝑥)).(2.10)\n\n우리는 다음과 같이 설정합니다- 𝑏⁢(𝑥)=silu⁢(𝑥)=𝑥/(1+𝑒^−𝑥)(2.11)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적으로, spline⁢(𝑥)는 B-스플라인의 선형 조합으로 매개화됩니다. 이 조합은 다음과 같습니다:\n\n![B-spline formula](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_16.png)\n\n초기화: \"각 활성화 함수는 spline⁢(𝑥)≈0으로 초기화됩니다. 𝑤는 Xavier 초기화에 따라 초기화되며, 이 초기화는 MLP의 선형 레이어를 초기화하는 데 사용됩니다.\"\n\n스플라인 격자의 업데이트: \"입력 활성화에 따라 각 격자를 업데이트하여, 활성화 값이 훈련 중 고정된 영역을 벗어나는 문제를 해결합니다.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nKAN이 기본적으로 더 복잡하기 때문에, 동일 크기의 MLP보다 매우 많은 매개변수를 가지게 됩니다. 간단히 말하자면, 다음과 같은 네트워크를 가정해 봅시다.\n1) 깊이가 L인,\n2) 너비가 n0 = n1 = · · · = nL = N 인 층,\n3) 각 spline이 주로 k(일반적으로 k = 3)차이며 G 구간(G + 1개의 그리드 포인트)에서 \n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_17.png)\n\n하지만, KAN은 보통 MLP보다 훨씬 작은 𝑁이 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 매개변수를 줄임으로써,\n- 더 나은 일반화를 달성합니다\n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_18.png)\n\n- 해석 가능성을 용이하게 함- 더 작은 네트워크는 읽고 확인하기 쉽습니다.\n\n지금까지 오셨으면, 확실히 KANs에서 많이 보이는 스플라인을 알아차릴 수 있었을 것입니다. 그리고 당신은 틀리지 않았습니다- 스플라인은 KANs의 매우 중요한 부분입니다. 그래서 다음으로 그것들에 대해 논의할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## B. 스플라인 매개 변수화 및 KANs\n\n여러 점을 통과하는 매끄러운 곡선을 그리려고 상상해보세요. 컴퓨터에게 원하는 대로 곡선을 그리도록 하는 방법은 무엇일까요? 여기서 B-스플라인이 등장합니다. 이들은 본질적으로 \"매듭(knots)\"이라고 불리는 점 세트 위에 정의된 부드럽고 조각으로 나뉘는 다항식 곡선입니다.\n\n![image](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_19.png)\n\n이러한 매듭은 곡선을 세그먼트로 나누고, 각 세그먼트 내에서 B-스플라인은 특정 차수의 다항식입니다. 매력은 이러한 세그먼트들이 매끄럽게 연결되어 원하는 수준의 부드러움을 가진 연속적인 곡선을 만든다는 데 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nB-스플라인은 매우 멋지기 때문에 그들의 유연성에서 Camavinga의 스타일을 띠고 있습니다. 매우 부드러운 곡선을 원한다면 더 높은 차수의 B-스플라인을 사용해야 합니다. 곡선의 모양에 대한 유연성과 제어를 더 원한다면 매듭의 위치를 조절할 수 있습니다. 이러한 이유로 B-스플라인은 다양한 함수를 표현하는 데 놀라운 유연성을 제공합니다. 그래서 다음 번에 \"문화인\" 여러분들 중 한 분이 선호하는 애니메이션, AI 또는 심지어 포토샵으로 가공한 모델에 대한 갈망이 생길 때, B 스플라인의 신에게 감사하는 것을 잊지 마세요.\n\nB-스플라인은 KANs에서 중요한 역할을 합니다. 왜냐하면 엣지에서 배우는 활성화 함수를 표현하는 강력하고 해석 가능한 방식을 제공하기 때문입니다. ReLU나 시그모이드와 같은 고정된 활성화 함수 대신 KANs는 이러한 활성화 함수를 근사하기 위해 B-스플라인을 사용합니다. 이것은 여러 가지 이점을 제공합니다:\n\n- 정확성: B-스플라인은 고정성이 높은 복잡한 비선형 함수를 높은 정확도로 근사할 수 있습니다.\n- 지역 제어: B-스플라인의 계수를 변경하면 조정된 매듭 부근에서만 곡선의 모양에 영향을 줍니다. 이는 전체 네트워크 구조에 영향을 미치지 않고 활성화 함수의 동작을 세세하게 제어할 수 있게 합니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/0*OhuCnxJme6EnYIWX.gif)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 해석 가능성: B-스플라인은 구간 다항식으로 구성되어 있기 때문에 블랙박스 신경망보다 시각적으로 직관적이며 이해하기 쉽습니다. 이는 학습된 활성화 함수가 더 해석 가능하며 과학적 발견을 용이하게 합니다. \"우리는 KAN이 매듭 이론(4.3절)과 응축물질 물리의 상 전이 경계(4.4절) 모두를 (재)발견할 수 있다는 것을 입증합니다. KAN은 정확성(마지막 절)과 해석 가능성(이 절)으로 인해 AI+과학의 기초 모델이 될 수 있습니다.\"\n\nSplines를 최상의 성능으로 작동하기 위해서는 그들을 지원하는 것이 필요합니다. 이것이 Grid Extension 기술이 필요한 곳입니다.\n\n## C. Splines 개선을 위한 Grid Extension\n\nGrid Extension 기술은 KAN의 중요한 구성 요소로, 스플라인에 내재된 제한된 해상도를 해결합니다. 스플라인은 유한한 점 그리드 상에 정의되어 있으므로 정확도는 그리드의 밀도에 따라 달라집니다. Grid Extension 기술은 모델이 기반이 되는 함수의 점점 미세한 세부사항들을 학습할 수 있도록 훈련 중에 그리드 밀도를 증가시킴으로써 이를 해결합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_20.png)\n\n이 프로세스는 기존의 더 거친 스플라인에 새로운, 더 세밀한 스플라인을 맞추는 것을 포함합니다. 이를 통해 모델이 데이터 분포의 변화에 적응하고 이전에 배운 지식을 버리지 않고 정확성을 향상시킬 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_21.png)\n\nKANs에 대한 해석 가능성이 얼마나 중요한지 고려할 때, 가능한 한 우리의 KANs가 해석 가능하도록 보장할 수 있는 방법에 대해 이야기해 보겠습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## D. 해석 가능성을 위한 KAN 단순화\n\nKAN의 기본 아키텍처는 해석 가능성을 촉진하지만 활성화 함수의 많은 수로 모델을 이해하기 어렵게 만들 수 있습니다. 이를 해결하기 위해 여러 가지 단순화 기술이 사용됩니다:\n\n- 희소화 규제 (L1 및 엔트로피): 희소화 규제는 활성화 함수의 크기를 벌칙으로 삼아 희소한 표현을 촉진하며, 중요한 관계만을 나타내는 일부 활성화 함수만 값이 큰 방향으로 유도합니다. 이는 모델이 학습한 가장 중요한 관계를 식별하는 데 도움이 됩니다.\n- 가지치기 기술: 가지치기 기술은 중요도 점수에 기초하여 네트워크에서 노드를 자동으로 제거하여 아키텍처를 더 단순화하고 매개변수 수를 줄입니다.\n- 심볼화: 심볼화는 사용자가 활성화 함수를 삼각, 지수 또는 로그 함수와 같은 기호적 표현으로 변환할 수 있게 합니다. 이 단계는 활성화 함수의 숫자적 표현을 보다 쉽게 해석 가능한 기호적 공식으로 변환하기 위해 인간의 전문 지식을 활용합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_22.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 해석가능성의 주요 이점은 사람들(사용자, 전문가 등)이 프로세스/훈련 중간에서 AI를 결정으로 이끌 수 있도록 하는 것입니다. 이 상호작용은 AI 솔루션을 구축하는 가장 간과된 측면 중 하나이며, 대부분의 팀이 이를 구축하는 데 아무것도 하지 않거나 거의 하지 않습니다. 그러나 제대로 하면 게임 체인저가 될 수 있습니다. 그러니 KAN과 그들의 상호작용을 어떻게 만들 수 있는지에 대해 이야기해보겠습니다.\n\n## E. 대화형 KAN\n\nKAN의 주요 이점 중 하나는 그들의 고유한 상호작용성입니다. 시각화 도구와 상징적 조작 기능을 활용하여 사용자는 학습된 표현을 개량하고 근본적인 관계에 대한 보다 깊은 통찰력을 얻기 위해 모델과 협업할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 대화식 방식을 통해 사용자는 모델의 학습 과정을 안내할 수 있습니다.\n\n- 활성화 함수 시각화: 사용자는 활성화 함수를 시각화하여 잠재적인 상징적 표현을 식별하고 모델의 의사 결정 과정을 이해할 수 있습니다.\n- 활성화 함수 수동 설정: 사용자는 도메인 지식을 기반으로 특정 활성화 함수를 상징적 공식에 수동으로 맞출 수 있어 모델의 해석가능성과 정확성을 향상시킬 수 있습니다.\n- 네트워크 반복적으로 개선: 사용자는 모델 구조, 하이퍼파라미터 및 정규화 전략을 반복적으로 조정하여 정확성과 상호 운용성 사이의 균형을 최적화할 수 있습니다.\n\n모든 이러한 것들이 함께 결합하여 우리에게 KANs를 제공합니다- 결함이 있고 재미있으며 극도로 흥미로운 것입니다. 저에게는 KANs가 MLP의 가장 근본적인 문제 중 일부를 진정으로 다루는 신선한 시도를 제공한다고 생각됩니다. 그래서 이 논문은 모든 인정을 받아야 한다고 생각합니다. KANs가 쓸모 없거나 너무 특정한 제약이 있다고 해도- 우리는 KANs를 더 자세히 연구함으로써 많은 통찰을 얻을 것이라고 생각합니다.\n\n그리고 그로써 코끼리 번식이 해결되었다고 생각해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 이 기사가 마음에 드셨고 공유하고 싶으시다면 아래 가이드라인을 참조해 주세요.\n\n이것으로 이 기사를 마치겠습니다. 여러분의 시간을 감사히 여깁니다. 언제나처럼, 저와 함께 일하길 희망하시거나 제 다른 작품을 확인해 보고 싶다면, 제 링크는 이 이메일/게시물의 맨 끝부분에 있을 거예요. 그리고 만약 이 글에서 가치를 발견했다면, 더 많은 사람들과 공유해 주시면 감사하겠습니다. 여러분과 같은 입소문 추천이 제 성장을 돕는데 큰 도움이 됩니다.\n\n![이미지](/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_24.png)\n\n저는 정보를 제공하고 유용하며, 부당한 영향으로부터 독립된 작품을 만드는 데 많은 노력을 기울였습니다. 제 글을 지원하고 싶으시다면, 이 뉴스레터의 유로 구독자가 되는 것을 고려해 주세요. 이를 통해 더 많은 노력을 기울여 쓰기/연구를 할 수 있고, 더 많은 사람들에게 도달하며, 제 치명적인 초콜릿 우유 중독을 지원할 수 있습니다. 매주 10만 명이 넘는 독자들에게 인공지능 연구와 엔지니어링의 가장 중요한 아이디어들을 대중화하는 데 도와주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n초콜릿 우유를 사주실래요?\n\n부탁드립니다~\n\nPS- 우리는 \"자신의 능력에 맞게 지불\"하는 모델을 따릅니다. 자세한 내용 및 여러분에게 적합한 방법을 찾는 데 도움이 될 포스트를 확인해보세요.\n\n나는 주기적으로 미니 업데이트를 Microblogging 사이트인 Twitter(https://twitter.com/Machine01776819), Threads(https://www.threads.net/@iseethings404), TikTok(https://www.tiktok.com/@devansh_ai_made_simple)에서 공유하고 있어요. 제 학습 내용을 계속 알고 싶다면 팔로우해주세요.\n\n# 언제든지 연락주세요~\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 링크를 사용하여 다른 콘텐츠를 확인하거나 튜터링에 대해 더 알아보거나 프로젝트에 대해 연락하거나 인사를 전하실 수 있습니다.\n\n기술, AI 및 기계 학습에 대한 작은 단편들은 여기에서 확인하세요.\n\nAI 뉴스레터 - [artificialintelligencemadesimple.substack.com](https://artificialintelligencemadesimple.substack.com/)\n\n제 할머니가 좋아하는 기술 뉴스레터 - [codinginterviewsmadesimple.substack.com](https://codinginterviewsmadesimple.substack.com/)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내 다른 기사들도 확인해보세요. : [https://rb.gy/zn1aiu](https://rb.gy/zn1aiu)\n\n내 유튜브 채널: [https://rb.gy/88iwdd](https://rb.gy/88iwdd)\n\nLinkedIn에서 연락해요. 함께 소통해요: [https://rb.gy/m5ok2y](https://rb.gy/m5ok2y)\n\n내 인스타그램: [https://rb.gy/gmvuy9](https://rb.gy/gmvuy9)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 트위터: [https://twitter.com/Machine01776819](https://twitter.com/Machine01776819)","ogImage":{"url":"/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_0.png"},"coverImage":"/assets/img/2024-06-19-KolmogorovArnoldNetworksHypeorDeepLearningRevolution_0.png","tag":["Tech"],"readingTime":16},{"title":"OpenAI Assistant API와 Streamlit을 사용하여 도우미 만들기","description":"","date":"2024-06-19 19:58","slug":"2024-06-19-CreatinganAssistantwithOpenAIAssistantAPIandStreamlit","content":"\n\n## 단계별 가이드\n\n![이미지](https://miro.medium.com/v2/resize:fit:1200/1*bX5eqE7EUmnwxWuqjZDzIQ.gif)\n\n# OpenAI Assistant API\n\n최근 OpenAI가 새로운 기능을 소개했습니다. 이들은 Assistant API와 같이 에이전트와 같은 아키텍처를 보여줍니다. OpenAI에 따르면:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 발전은 희망적이지만, 아직 LangChain을 따라가지 못합니다. LangChain은 자연어 입력을 처리하고 문맥 기반 액션을 실행하는 더 유연한 LLM을 활용하여 에이전트 형태의 시스템을 만들 수 있습니다.\n\n하지만, 이것은 시작에 불과합니다.\n\n높은 수준에서 Assistant API와 상호 작용하는 것은 루프로 상상할 수 있습니다:\n\n- 사용자 입력을 받으면 LLM이 호출되어 응답을 제공할지 또는 특정 조치를 취할지를 결정합니다.\n- LLM의 결정이 쿼리에 대한 답변으로 충분하다면 루프가 종료됩니다.\n- 만약 행동이 새로운 관찰로 이어진다면, 이 관찰은 프롬프트에 포함되고 LLM이 다시 호출됩니다.\n- 그런 다음 루프가 다시 시작됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-CreatinganAssistantwithOpenAIAssistantAPIandStreamlit_0.png)\n\n안타깝게도 발표된 장점에도 불구하고, API에 대한 문서는 특히 사용자 정의 함수 호출 및 Streamlit와 같은 프레임워크를 사용한 앱 구축과 관련하여 제대로 작성되지 않았다고 생각했습니다.\n\n이 블로그 포스트에서는 OpenAI Assistant API 및 사용자 정의 함수 호출을 사용하여 Streamlit 인터페이스와 함께 AI 어시스턴트를 구축하는 방법을 안내해드리겠습니다. 이를 통해 Assistant API를 효과적으로 사용하고자 하는 분들께 도움이 될 것입니다.\n\n# 사용 사례: 세금 계산 어시스턴트\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 블로그 포스트에서는 간단한 예제를 보여드리겠습니다: 주어진 수익에 기반한 세금을 계산할 수 있는 AI 어시스턴트입니다. Langchain 사용자들은 \"세금 계산\" 도구를 가진 에이전트를 생성함으로써 이를 쉽게 이해할 수 있습니다.\n\n이 도구에는 필요한 계산 단계와 LLM이 수익 또는 세금과 관련된 질문이 있을 때 도구를 호출해야 하는지를 알려주는 잘 설계된 프롬프트가 포함될 것입니다.\n\n그러나 이 프로세스는 OpenAI 어시스턴트 API와 정확히 동일하지는 않습니다. OpenAI의 문서에 따르면 코드 해석기와 파일 검색 도구는 직접적으로 간단한 방식으로 사용할 수 있지만, 사용자 정의 도구는 약간 다른 방식으로 접근해야 합니다.\n\n```js\nassistant = client.beta.assistants.create(\n  name=\"데이터 시각화자\",\n  description=\"당신은 아름다운 데이터 시각화를 만드는 데 뛰어나십니다. .csv 파일에 있는 데이터를 분석하며 트렌드를 이해하고 해당 트렌드에 관련된 데이터 시각화를 제시합니다. 또한 관찰된 트렌드에 대한 간단한 텍스트 요약을 공유합니다.\",\n  model=\"gpt-4o\",\n  tools=[{\"type\": \"code_interpreter\"}],\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 단계씩 세부 내용을 살펴보겠습니다. 다음을 목표로 합니다:\n\n- 주어진 수익에 기반한 세금을 계산하는 함수 정의하기.\n- 이 함수를 사용하는 도구 개발하기.\n- 이 도구에 액세스하고 세금 계산이 필요할 때 호출할 수 있는 어시스턴트 만들기.\n\n# 어시스턴트 통합을 위한 세금 계산 함수\n\n다음 단락에서 설명하는 세금 계산 도구는 이 글에서 논의된 API를 사용하는 방법을 보여주기 위한 예시로 설계되었음을 유념해 주세요. 실제 세금 계산에 사용해서는 안 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음과 같이 조각별 함수를 고려해 보세요. 이 함수는 주어진 매출에 대한 세금 값을 반환합니다. 입력이 간단한 구문 분석을 위해 문자열로 설정되어 있음을 유의하세요:\n\n```js\ndef calculate_tax(revenue: str):\n    try:\n        revenue = float(revenue)\n    except ValueError:\n        raise ValueError(\"매출은 숫자의 문자열 표현이어야 합니다.\")\n\n    if revenue \u003c= 10000:\n        tax = 0\n    elif revenue \u003c= 30000:\n        tax = 0.10 * (revenue - 10000)\n    elif revenue \u003c= 70000:\n        tax = 2000 + 0.20 * (revenue - 30000)\n    elif revenue \u003c= 150000:\n        tax = 10000 + 0.30 * (revenue - 70000)\n    else:\n        tax = 34000 + 0.40 * (revenue - 150000)\n\n    return tax\n```\n\n다음으로, 비서(assistant)를 정의합니다:\n\n```js\nfunction_tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"calculate_tax\",\n            \"description\": \"유로로 주어진 매출에 대한 세금을 가져옵니다.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"revenue\": {\n                        \"type\": \"string\",\n                        \"description\": \"유로로 연간 매출\"\n                    }\n                },\n                \"required\": [\"revenue\"]\n            }\n        }\n    }\n]\n\n# 비서(assistant) 정의\nassistant = client.beta.assistants.create(\n    name=\"Assistant\",\n    instructions=\"\",\n    tools=function_tools,\n    model=\"gpt-4o\",\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제, 주요한 포인트에 대해서 얘기해볼게요:\n\n어시스턴트가 \"calculate_tax\"가 호출될 때 어떻게 함수를 사용하는지 알고 계신가요? 이 부분은 OpenAI 어시스턴트에서 문서화가 잘 되어 있지 않아, 많은 사용자들이 처음 사용할 때 혼동을 겪을 수 있어요. 이를 해결하기 위해, 응담 스트림(response stream)에서 다양한 이벤트를 관리하기 위한 EventHandler를 정의해야 합니다. 특히 \"calculate_tax\" 도구가 호출될 때의 이벤트를 어떻게 처리하는지에 대해 명확히 알아둬야 해요.\n\n```js\n    def handle_requires_action(self, data, run_id):\n        tool_outputs = []\n\n        for tool in data.required_action.submit_tool_outputs.tool_calls:\n            if tool.function.name == \"calculate_tax\":\n                try:\n                    # 도구 매개변수에서 수익 추출\n                    revenue = ast.literal_eval(tool.function.arguments)[\"revenue\"]\n                    # 세금을 계산하는 calculate_tax 함수 호출\n                    tax_result = calculate_tax(revenue)\n                    # 필요한 형식에 맞게 도구 출력을 추가\n                    tool_outputs.append({\"tool_call_id\": tool.id, \"output\": f\"{tax_result}\"})\n                except ValueError as e:\n                    # 세금 계산 시 발생하는 모든 오류 처리\n                    tool_outputs.append({\"tool_call_id\": tool.id, \"error\": str(e)})\n        # 모든 도구 출력을 동시에 제출\n        self.submit_tool_outputs(tool_outputs)\n```\n\n위 코드는 다음과 같이 동작해요: 동작이 필요한 각 도구 호출에 대해:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- \"calculate_tax\" 함수 이름을 확인합니다.\n- 툴 매개변수에서 수익 값을 추출합니다.\n- 수익을 이용하여 calculate_tax 함수를 호출하여 세금을 계산합니다. (여기서 실제 상호작용이 이루어집니다.)\n- 모든 툴 호출을 처리한 후, 수집된 결과를 제출합니다.\n\n# 보조 인공지능과 대화하기\n\n다음은 OpenAI가 문서화한 표준 단계를 따라 보조 인공지능과 상호작용할 수 있습니다. 따라서 이 섹션에서는 많은 세부 정보를 제공하지 않겠습니다:\n\n- 스레드 생성: 이는 사용자와 보조 인공지능 간의 대화를 나타냅니다.\n- 사용자 메시지 추가: 이는 스레드에 추가되는 텍스트 및 파일을 포함할 수 있습니다.\n- 실행 생성: 보조 인공지능과 연관된 모델 및 도구를 활용하여 응답 생성합니다. 이 응답은 다시 스레드에 추가됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 코드 조각은 특정 사용 사례에서 어시스턴트를 실행하는 방법을 보여줍니다: 코드는 스레드 ID 및 어시스턴트 ID를 사용하는 특정 매개변수를 설정하여 어시스턴트와의 스트리밍 상호작용을 설정합니다. EventHandler 인스턴스는 스트림 중 이벤트를 관리합니다. stream.until_done() 메서드는 모든 상호작용이 완료될 때까지 스트림을 유지합니다. with 문은 스트림이 적절히 닫히도록 보장합니다.\n\n```js\n  with client.beta.threads.runs.stream(thread_id=st.session_state.thread_id,\n                                         assistant_id=assistant.id,\n                                         event_handler=EventHandler(),\n                                         temperature=0) as stream:\n        stream.until_done()\n```\n\n# Streamlit 인터페이스\n\n여기서 내 게시물을 마칠 수 있지만, Streamlit 포럼(예: 이 포스트)에서 사용자들이 터미널에서는 정상 작동하지만 인터페이스에서 스트리밍이 작동하지 않는다는 수많은 문의를 발견했습니다. 이것이 나로 하여금 더 깊이 파고들도록 유도했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스트리밍을 앱에 성공적으로 통합하려면, 앞서 언급한 EventHandler 클래스의 기능을 확장해야 합니다. 특히 텍스트 생성, 텍스트 델타 처리 및 텍스트 완료를 중점적으로 다루어야 합니다. 채팅 히스토리를 관리하면서 Streamlit 인터페이스에 텍스트를 표시하기 위해 필요한 세 가지 주요 단계는 다음과 같습니다:\n\n- 텍스트 생성 처리 (on_text_created): 어시스턴트의 각 응답마다 새로운 텍스트 상자를 초기화하고 표시하여 이전 작업의 상태를 반영하도록 UI를 업데이트합니다.\n- 텍스트 델타 처리 (on_text_delta): 어시스턴트가 텍스트를 생성할 때 현재 텍스트 상자를 동적으로 업데이트하여 전체 UI를 새로 고치지 않고도 점진적으로 변경할 수 있도록 합니다.\n- 텍스트 완료 처리 (on_text_done): 새로운 빈 텍스트 상자를 추가하여 각 상호작용 세그먼트를 완료하고, 다음 상호작용을 준비합니다. 또한, 대화 세그먼트를 chat_history에 기록합니다.\n\n예를 들어, 텍스트 델타를 관리하는 다음 코드 조각을 살펴봅시다:\n\n```python\ndef on_text_delta(self, delta: TextDelta, snapshot: Text):\n    \"\"\"\n    텍스트 델타가 생성될 때의 핸들러\n    \"\"\"\n    # 최신 텍스트 상자를 지웁니다.\n    st.session_state.text_boxes[-1].empty()\n    \n    # 새로운 텍스트가 있으면, 어시스턴트 텍스트 목록의 마지막 요소에 추가합니다.\n    if delta.value:\n        st.session_state.assistant_text[-1] += delta.value\n    \n    # 업데이트된 어시스턴트 텍스트를 최신 텍스트 상자에 다시 표시합니다.\n    st.session_state.text_boxes[-1].info(\"\".join(st.session_state[\"assistant_text\"][-1]))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 코드는 세 가지 주요 작업을 수행합니다:\n\n- 최신 텍스트 상자 지우기: 최신 텍스트 상자의 내용을 지워 새 입력을 준비합니다 (st.session_state.text_boxes[-1]).\n- 델타 값을 도우미 텍스트에 추가: 새 텍스트 (delta.value)가 있는 경우, 이를 st.session_state.assistant_text[-1]에 저장된 지속적인 도우미 텍스트에 추가합니다.\n- 업데이트된 도우미 텍스트 다시 표시: 지금까지 축적된 모든 도우미 텍스트의 내용을 반영하기 위해 최신 텍스트 상자의 내용을 업데이트합니다 (st.session_state[\"assistant_text\"][-1]).\n\n# 결론\n\n이 블로그 포스트에서는 OpenAI Assistant API와 Streamlit을 사용하여 세금을 계산할 수 있는 AI 도우미를 만드는 방법을 보여주었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 Assistant API의 능력을 강조하기 위해 이 간단한 프로젝트를 수행했어요. 문서가 다소 불명확하더라도, 목표는 모호한 부분을 명확하게 하고 Assistant API를 사용하고자 하는 분들에게 일부 지침을 제공하는 것이었습니다. 이 게시물이 도움이 되었으면 좋겠고, 이 강력한 도구로 더 많은 가능성을 탐험하도록 격려하길 바랍니다.\n\n공간 제약으로 인해 불필요한 코드 조각을 포함하지 않으려고 노력했어요. 그러나 필요한 경우, 제 Github 저장소를 방문하여 전체 구현 내용을 확인해주세요.","ogImage":{"url":"/assets/img/2024-06-19-CreatinganAssistantwithOpenAIAssistantAPIandStreamlit_0.png"},"coverImage":"/assets/img/2024-06-19-CreatinganAssistantwithOpenAIAssistantAPIandStreamlit_0.png","tag":["Tech"],"readingTime":8},{"title":"대규모 언어 모델Large Language Models, LLMs 이해하기","description":"","date":"2024-06-19 19:56","slug":"2024-06-19-UnderstandingLargeLanguageModelsLLMs","content":"\n\n![UnderstandingLargeLanguageModelsLLMs_0](/assets/img/2024-06-19-UnderstandingLargeLanguageModelsLLMs_0.png)\n\n대형 언어 모델(LLMs)은 현대 인공 지능의 중요한 요소로 자리 잡아, 기계가 인간 언어를 이해하고 생성하는 방식을 혁신하고 있습니다. 챗봇과 가상 비서부터 고급 연구 도구에 이르기까지, LLMs는 다양한 분야에서 혁신을 주도하고 있습니다. 이 글은 LLMs의 복잡성을 탐구하여 그 개발, 기술 기반, 응용 및 전망을 살펴봅니다.\n\n# 대형 언어 모델이란?\n\nLLMs는 자연어 텍스트를 처리하고 생성하기 위해 설계된 인공 지능 모델의 하위 집합입니다. 이러한 모델은 수억에서 수조에 이르는 많은 매개변수를 특징으로 하며, 이를 통해 인간과 유사한 텍스트를 높은 정확성과 일관성으로 생성하고 이해할 수 있습니다. LLMs에서의 \"대형\"이라는 용어는 이러한 모델을 훈련하기 위해 필요한 방대한 양의 데이터와 계산 능력을 가리킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# LLMs의 진화\n\nLLMs의 여정은 1960년대 ELIZA와 같은 초기 자연 언어 처리(NLP) 모델을 시작으로 했습니다. ELIZA는 간단한 패턴 매칭 기술을 사용했습니다.\n\n![이미지](/assets/img/2024-06-19-UnderstandingLargeLanguageModelsLLMs_1.png)\n\n1990년대에는 딥 러닝의 등장으로 패러다임이 전환되었습니다. 이 기계 학습 분야는 인간 뇌 구조를 모방한 인공 신경망을 활용하여 방대한 양의 데이터에서 학습합니다. 이로써 더 정교한 언어 모델의 개발이 가능해졌습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1997년에 핵심적인 개발이 이루어졌는데, Long Short-Term Memory (LSTM) 네트워크가 소개되었습니다. 텍스트와 같은 순차 데이터에 어려움을 겪는 전통적인 신경망과는 달리, LSTMs는 장기 의존성을 다룰 수 있어 문장 내 맥락을 파악할 수 있었습니다. 이로 인해 보다 큰 데이터셋에서 언어 모델을 훈련하고 언어의 미묘한 측면을 캡처할 수 있게 되었습니다.\n\n하지만, 신경망과 딥러닝이 2010년대에 도입되면서 중대한 발전이 이루어졌습니다.\n\nLLM 진화의 주요 이정표는 다음과 같습니다:\n- 단어 임베딩 (2013): Word2Vec과 같은 모델은 단어를 고차원 공간에서 연속적인 벡터로 표현하는 단어 임베딩 개념을 소개하여 의미론적 관계를 포착했습니다.\n- Sequence-to-Sequence 모델 (2014): Seq2Seq와 같은 모델의 발전은 특히 번역 작업에서 유용한 입력-출력 쌍 처리를 더 잘할 수 있게 해주었습니다.\n- Attention 메커니즘 (2017): Vaswani 등이 제안한 Transformer 모델은 어텐션 메커니즘을 통합하여 다양한 단어의 중요성을 가중치로 고려할 수 있는 NLP 혁명을 일으켰습니다.\n- Generative Pre-trained Transformers (GPT, 2018–2024): OpenAI의 GPT 시리즈는 GPT-4가 1.5조 개의 파라미터를 자랑하며 중대한 발전을 이루었습니다. 이러한 모델은 텍스트 생성, 번역 및 요약에서 놀라운 능력을 보여주었습니다.\n\n# 대형 언어 모델 예제\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 개발되어 사용 중인 유명한 LLM 중 일부를 살펴보겠습니다.\n\n- GPT: GPT의 전체 명칭은 Generative pre-trained Transformer입니다. Open AI가 개발한 이 모델은 Chat GPT(Open AI에서 출시)에서 GPT-4 모델을 사용하고 있으며 들어보셨을 것입니다.\n- BERT: BERT의 전체 명칭은 Bidirectional Encoder Representations from Transformers입니다. 구글이 개발한 이 큰 언어 모델은 자연어 처리와 관련된 다양한 작업에 주로 사용됩니다. 또한, 특정 텍스트에 대한 임베딩을 생성하거나 다른 모델을 훈련시키는 데 사용될 수도 있습니다.\n- RoBERTa: RoBERTa의 전체 명칭은 Robustly Optimized BERT Pretraining Approach입니다. 변형기 아키텍처의 성능을 향상시키기 위한 시도 중 하나로, RoBERTa는 Facebook AI Research에 의해 개발된 BERT 모델의 향상된 버전입니다.\n- BLOOM: 다양한 기관과 연구자들이 협력하여 개발한 최초의 다국어 언어 모델인 BLOOM은 GPT 아키텍처와 유사합니다.\n\n![이미지](/assets/img/2024-06-19-UnderstandingLargeLanguageModelsLLMs_2.png)\n\n# NLP와 LLM의 차이\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNLP은 자연 언어 처리의 알고리즘 개발으로 이루어진 인공 지능(인공지능) 분야입니다. NLP는 알고리즘 및 기술로 이루어진 LLM보다 더 넓은 범위의 분야입니다. NLP는 기계 학습 및 언어 데이터 분석이라는 두 가지 접근 방식을 가지고 있습니다. NLP의 응용 분야는 다음과 같습니다.\n\n- 자동화된 루틴 작업\n- 검색 기능 개선\n- 검색 엔진 최적화\n- 대규모 문서 분석 및 정리\n- 소셜 미디어 분석\n\n반면에, LLM은 대규모 언어 모델로, 인간과 유사한 텍스트에 더 특화되어 있으며, 콘텐츠 생성 및 맞춤형 추천을 제공합니다.\n\n# 대규모 언어 모델의 장점은 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대형 언어 모델(LLM)은 여러 이점을 가지고 있어 다양한 응용 분야에서 널리 사용되고 성공을 거두는데 이바지합니다:\n\n- LLM은 제로샷 학습을 수행할 수 있어 명시적으로 훈련받지 않은 작업에 대해 일반화할 수 있습니다. 이 능력은 추가적인 훈련 없이 새로운 응용분야나 시나리오에 대응할 수 있도록 해줍니다.\n- LLM은 방대한 양의 데이터를 효율적으로 처리할 수 있어 언어 번역이나 문서 요약과 같은 방대한 텍스트 말뭉치에 대한 심도 있는 이해가 필요한 작업에 적합합니다.\n- LLM은 특정 데이터셋이나 도메인에 대해 미세 조정이 가능하며, 특정 사용 사례나 산업에 계속적인 학습과 적응이 가능합니다.\n- LLM은 코드 생성부터 콘텐츠 생성까지 여러 언어 관련 작업을 자동화할 수 있어 프로젝트의 전략적이고 복잡한 측면을 위해 인력을 확보할 수 있습니다.\n\n# LLM은 어떻게 작동하나요?\n\nLLM은 신경망 아키텍처를 기반으로 하며, 특히 Transformer 아키텍처에 기반합니다. 이러한 모델의 주요 구성 요소는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 임베딩: 단어 또는 토큰을 의미와 관계를 담은 밀집 벡터로 변환합니다.\n- 셀프 어텐션 메커니즘: 이 메커니즘은 모델이 입력 텍스트의 관련 부분에 초점을 맞추도록 하며, 다양한 단어의 중요성을 동적으로 가중치를 부여합니다.\n- 피드 포워드 레이어: 이러한 레이어는 셀프 어텐션 메커니즘의 출력을 처리하여 의미 있는 표현을 생성합니다.\n- 대규모 말뭉치에서의 훈련: LLM(Large Language Models)은 방대한 양의 텍스트 데이터에서 훈련되며, 비감독 또는 준지도 학습을 통해 언어의 패턴, 구조 및 뉘앙스를 학습합니다.\n- 파인튜닝: 사전 훈련 이후 모델은 종종 특정 작업이나 데이터셋에 대해 파인튜닝되어 특정 응용 프로그램에서의 성능을 향상시킵니다.\n\nTransformer 아키텍처에 대한 자세한 내용은 여기에서 읽어보세요:\n\n# LLM의 응용\n\nLLM의 다양한 용도로 인해 다양한 분야에서 채택되고 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 자연어 이해 (NLU): LLMs는 감성 분석, 엔티티 인식, 언어 번역과 같은 작업에서 뛰어나며 인간 언어를 더 잘 이해할 수 있게 합니다.\n- 텍스트 생성: 창의적 글쓰기와 코드 생성부터 이메일과 보고서 작성까지, LLMs는 일관된 문맥적인 텍스트를 생성할 수 있습니다.\n- 대화형 에이전트: Siri, Alexa 및 고객 서비스의 챗봇과 같은 가상 비서들은 LLMs를 활용하여 자연스럽고 효과적인 상호작용을 제공합니다.\n- 내용 요약: LLMs는 대량의 정보를 간결한 요약으로 정리하여 정보 검색과 지식 관리를 도와줍니다.\n- 코드 생성: GitHub Copilot과 같은 도구는 LLMs를 활용하여 개발자들에게 코드 조각을 제안하고 함수를 완성하는 데 도움을 줍니다.\n\n# 대형 언어 모델 실험\n\n## 최신 모델과 함께 직접 체험하기\n\n대형 언어 모델의 능력을 직접 탐구하고 싶은 분들을 위해 몇 가지 접근 가능한 플랫폼과 모델이 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- OpenAI의 GPT-4: OpenAI는 GPT 모델의 여러 버전에 대한 액세스를 제공하며, 이 중 GPT-4는 사용자가 API를 통해 실험할 수 있습니다. 사용자는 OpenAI의 플랫폼(chat.openai.com 등)에 가입하여 응용 프로그램을 작성하거나 모델과 상호 작용하여 텍스트 생성, 질문에 답변 또는 코딩 능력을 이해할 수 있습니다.\n- Anthropics의 Claude 2: Anthropics는 안전과 유틸리티에 초점을 맞춘 Claude 2라는 흥미로운 모델을 제공합니다. 대화형 인공지능에 대한 독특한 관점을 제공하며 다양한 응용 프로그램을 위해 자사 플랫폼을 통해 액세스할 수 있습니다.\n- Hugging Face의 오픈 모델: 인기 있는 AI 모델 허깅페이스는 GPT와 BERT 모델의 변형을 포함한 다양한 오픈 소스 모델을 호스팅합니다. 열정가와 개발자는 이러한 모델을 텍스트 생성, 감성 분석 등 다양한 작업에 활용할 수 있습니다. 사용자 친화적인 인터페이스로 초보자도 손쉽게 실험을 시작할 수 있습니다.\n- Google Bard: Google은 자사의 LLM인 Bard로 경쟁에 뛰어들었습니다. 글 작성 언어 모델인 Bard는 작성 시점을 기준으로 한계적으로 공개되거나 테스트 중일 수 있지만, 대화형 인공지능 및 언어 이해 분야에서 중요한 역할을 약속합니다.\n\n# 도전과 윤리적 고려 사항\n\n그들의 능력에도 불구하고, LLM은 여러 가지 도전과 윤리적 고려 사항을 안고 있습니다:\n\n- 편향성과 공정성: LLM은 훈련 데이터에 존재하는 편향을 지속하거나 심지어 확대시킬 수 있어 공정하지 않거나 유해한 결과를 초래할 수 있습니다.\n- 잘못된 정보: LLM이 현실적인 텍스트를 생성할 수 있는 능력으로 인해 잘못된 정보의 퍼지와 딥 페이크의 생성에 대한 우려가 제기됩니다.\n- 자원 집약적: LLM의 훈련 및 배포는 상당한 계산 자원이 필요하므로 높은 에너지 소비와 환경 영향을 초래할 수 있습니다.\n- 데이터 프라이버시: LLM의 훈련에 사용되는 방대한 데이터셋은 종종 민감한 정보를 포함하고 있어 데이터 프라이버시와 보안 문제를 제기합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 향후 방향\n\nLLM의 미래는 희망적입니다. 현재의 한계를 해결하고 새로운 지평을 탐구하기 위해 계속된 연구에 집중되어 있습니다:\n\n- 효율성 향상: 연구자들은 LLM의 계산 부담을 줄이기 위해 더 효율적인 아키텍처와 교육 방법을 개발하고 있습니다.\n- 향상된 이해: 향후 모델은 더 깊은 언어 이해를 달성하고, 추론, 설명, 지식 생성 능력을 향상시키는 데 주력하고 있습니다.\n- 다중 모달 모델: 텍스트와 이미지, 오디오 등 다른 데이터 유형을 통합하여 시각적 질문 응답과 같은 복잡한 작업을 수행할 수 있는 포괄적인 AI 시스템을 만들고 있습니다.\n- 윤리적 AI: LLM이 책임 있게 사용되고 편향이 최소화되며 남용을 방지하기 위해 윤리적 가이드라인과 프레임워크 개발에 중점을 둡니다.\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대형 언어 모델은 인공 지능의 중요한 발전을 나타내며, 다양한 산업 분야에서 혁신적인 잠재력을 제공합니다. 연구가 진행됨에 따라 초점은 점차 더 효율적이고 윤리적이며 지적인 모델을 만들어 인간들과 자연스럽게 상호작용하며 의미 있는 방식으로 우리의 능력을 확장할 것입니다. LLM의 복잡성을 이해하는 것은 점점 더 디지털 세계에서 그들의 힘을 책임있고 효과적으로 활용하기 위해 중요합니다.","ogImage":{"url":"/assets/img/2024-06-19-UnderstandingLargeLanguageModelsLLMs_0.png"},"coverImage":"/assets/img/2024-06-19-UnderstandingLargeLanguageModelsLLMs_0.png","tag":["Tech"],"readingTime":7},{"title":"100 무료, 추적 가능하고 안전한 RAG 챗봇 만들기 Reranker와 GPT-4o를 활용해 보세요","description":"","date":"2024-06-19 19:54","slug":"2024-06-19-Builda100FreeTraceableandSecureRAGChatbotUsingRerankerandGPT-4o","content":"\n\n홍태, 징 왕, 징 주, 그리고 루차오 제이.\n\n![이미지](/assets/img/2024-06-19-Builda100FreeTraceableandSecureRAGChatbotUsingRerankerandGPT-4o_0.png)\n\n완전히 무료이며 저지연, 환각 없는 챗봇을 보유하는 것은 대형 언어 모델 애플리케이션의 황금빛 꿈이죠. 이 오픈 소스 RAG 챗봇은 그 목표에 한 걸음 더 가까워졌습니다.\n\n# 리랭커란 무엇이며 왜 사용하는 건가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n리랭커(re-ranker)는 언어 모델을 사용하여 문서 청크를 평가하고 정렬하여 사용자 질문에 대답하기 위한 가장 관련성 높은 문서를 선택합니다. 무료이며(리랭커로 오픈 소스 교차 인코더를 사용합니다) 성능도 우수합니다.\n\n리랭커에는 여러 가지 종류가 있습니다. 교차 인코더는 쿼리와 문서 사이의 관련성을 평가하고 높은 품질의 답변을 위해 LLM으로 보낼 가장 관련성 높은 문서를 선택하는 BERT 기반 모델입니다. 속도와 정확성을 균형있게 유지하며, 리랭킹 방법은 더 나은 검색 결과를 강조합니다. 여기서는 Hugging Face의 교차 인코더를 사용합니다.\n\n# 임베딩의 한계\n\n임베딩은 의미 정보를 캡처하기 위해 설계되었지만 종종 \"사과를 좋아해\"와 \"이전에는 사과를 좋아했어\"와 같은 비슷한 구문 사이의 미묘한 차이를 구분하는 데 어려움을 겪습니다. 이는 대조적인 정보 부족으로 인한 것입니다. 임베딩은 주로 1024 차원 정도로 고정되어 있으며, 이는 복잡하거나 긴 문서 및 쿼리를 완전히 나타내는 능력을 제한할 수 있습니다. 또한, 이들은 실제 세계 검색 애플리케이션에 중요한, 보지 못한 콘텐츠에 효과적으로 일반화할 수 없습니다. 이 한계는 고정된 차원 및 훈련 데이터의 제약으로 자주 악화됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# reranker가 어떻게 작동하는가요?\n\nreranker는 교차 인코딩 reranker로, 사전 훈련된 BERT 모델을 사용하여 쿼리, 문서 쌍의 관련성을 평가하는 데 사용됩니다. 교차 인코더의 입력은 쿼리와 문서의 쌍입니다. 출력은 순위 점수이며, 이는 검색 결과를 평가하고 제품을 추천하는 데 적합합니다. 여기서 우리는 reranker를 사용하여 LLM에게 사용자 질문에 대답할 수 있는 매우 관련 있는 문서를 선택합니다.\n쿼리와 문서를 함께 처리함으로써 모델은 그들 사이의 뉘앙스와 문맥적 관계(트랜스포머의 self-attention)을 잡아내어 더 정확한 관련성 평가를 가능케 합니다.\n잠재적인 단점은 대기 시간과 비용입니다. 본 연구에서는 reranker의 비용을 줄이기 위해 오픈 소스 reranker를 사용하고 텍스트 전처리 결과를 캐싱하여 대기 시간을 줄였습니다. 몇 메가바이트의 PDF 파일을 테스트한 결과, 대기 시간은 몇 초 미만입니다.\n\n현재 reranker 모델의 비교는 Galileo.AI에서 수행됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 아키텍처 비교:\n\nReranker 기반 RAG는 기존 RAG 단계(text 토큰화, 임베딩, 벡터 데이터베이스 생성 및 유사성 검색)를 하나의 교차 인코딩 리랭커로 단순화합니다. 리랭커에 의해 선택된 상위 N개의 가장 관련성 높은 텍스트 청크는 답변 생성을 위해 ChatGPT와 같은 LLM에 공급됩니다.\n\n주요 차이점은 다음 색깔 청크 안에 강조되어 있습니다.\n\n![Architecture Comparison](/assets/img/2024-06-19-Builda100FreeTraceableandSecureRAGChatbotUsingRerankerandGPT-4o_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 챗봇 사용 설명서\n\nreranker 기반 RAG를 시연하기 위해 Streamlit 인터페이스가 구축되었습니다.\n사용자는 자신의 OpenAI API 키를 복사하여 붙여넣고 왼쪽에 PDF 파일을 업로드할 수 있습니다. 일부 텍스트 전처리 후, 사용자는 PDF에 관한 질문을 할 수 있습니다.\n언급했듯이, 전처리된 문서는 지연 시간을 줄이기 위해 캐시 메모리에 저장됩니다. ChatGPT-4o API의 최근 릴리스를 활용하여 챗봇 답변이 만족스러운 것을 확인할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-Builda100FreeTraceableandSecureRAGChatbotUsingRerankerandGPT-4o_3.png)\n\n# 튜토리얼\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n패키지를 로드합니다\n\n```python\nfrom openai import OpenAI\n# Text Splitting Utilities\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\nfrom sentence_transformers import CrossEncoder\n\nimport streamlit as st\nfrom pypdf import PdfReader\nimport openai\nimport numpy as np\n```\n\n크로스-인코더 'ms-marco-MiniLM-L-6-v2'는 사용자 쿼리에 대한 모든 텍스트 청크의 순위를 지정하는 리랭커로 사용됩니다. 가장 관련성 높은 상위 N개의 텍스트 청크가 사용자 질문에 대한 답변에 사용될 것입니다.\n\n```python\nst.set_page_config(page_title=\"Reranker 및 완전히 무료 PDF 쿼리 어시스턴트\", layout=\"wide\")\n\ndef rank_doc(query, text_chunks, topN=5):\n    # 지정된 모델 이름으로 CrossEncoder 모델을 초기화합니다\n    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    \n    # 쿼리와 문서 각각에 대한 점수를 예측합니다\n    scores = reranker.predict([[query, doc] for doc in text_chunks])\n    \n    # 내림차순으로 상위 N개의 점수의 인덱스를 가져옵니다\n    top_indices = np.argsort(scores)[::-1][:topN]\n    \n    # 상위 순위의 텍스트 문서를 리스트 인덱싱을 사용하여 검색합니다\n    top_pairs = [text_chunks[index] for index in top_indices]\n    return top_pairs  # 상위 순위 텍스트 문자열의 목록을 반환합니다\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 관련성이 높은 상위 N개의 검색된 문서와 사용자 쿼리가 GPT-4o에 입력으로 사용됩니다.\n\n```js\ndef rag(query, retrieved_documents, api_key):\n    model = \"gpt-4o\"\n\n\n    # API 키 설정\n    openai.api_key = api_key\n\n    information = \"\\n\\n\".join(retrieved_documents)\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"도움이 되는 전문 금융 연구 보조원입니다. 사용자는 연례 10K 보고서에 포함된 정보에 관한 질문을 하고 있습니다.\"\n                       \"사용자의 질문과 연례 보고서에서 관련 정보가 표시됩니다. 이 정보만을 사용하여 사용자의 질문에 답하세요.\"\n        },\n        {\"role\": \"user\", \"content\": f\"질문: {query}. \\n 정보: {information}\"}\n    ]\n    \n    response = openai.chat.completions.create(\n        model=model,\n        messages=messages,\n    )\n    content = response.choices[0].message.content # 올바른 속성 엑세스로 업데이트됨\n    return content\n```\n\n텍스트 전처리 단계에는 PDF 텍스트를 분할하고 ‘\\t’ 및 ‘\\n’과 같은 특수 구분 기호를 제거하는 정리 단계가 포함됩니다. Streamlit은 PDF 처리 단계를 메모리에 캐싱하여 대기 시간을 줄입니다.\n\n```js\n@st.cache_data\ndef process_pdf_texts(pdf_file):\n    reader = PdfReader(pdf_file)\n    pdf_texts = [p.extract_text().strip() for p in reader.pages if p.extract_text()]\n    character_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"], chunk_size=1000, chunk_overlap=0)\n    character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n    return clean_text_list(character_split_texts)\n\ndef clean_text_list(text_list):\n    cleaned_texts = []\n    for text in text_list:\n        text = text.replace('\\t', ' ').replace('\\n', ' ')\n        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n        cleaned_text = '\\n'.join(lines)\n        cleaned_texts.append(cleaned_text)\n    return cleaned_texts\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내, 사용자가 패스코드를 입력하고 파일을 업로드하며 질문할 수 있는 간단한 Streamlit 인터페이스가 설계되었습니다.\n\n```js\nst.sidebar.title(\"구성\")\napi_key = st.sidebar.text_input(\"오픈AI API 키를 입력하세요\", type=\"password\")\nuploaded_file = st.sidebar.file_uploader(\"PDF 파일을 선택하세요\", type=['pdf'])\n\nif uploaded_file and api_key:\n    formatted_texts = process_pdf_texts(uploaded_file)\n    st.session_state.processed_texts = formatted_texts\n\nst.title(\"Reranker가 있는 무료 PDF 쿼리 어시스턴트\")\nif 'chat_history' not in st.session_state:\n    st.session_state.chat_history = []\n\nif st.session_state.chat_history:\n    for query, response in st.session_state.chat_history:\n        st.container().markdown(f\"**질문**: {query}\")\n        st.container().markdown(f\"**답변**: {response}\")\n\nquery = st.text_input(\"여기에 질문을 입력하세요:\", key=\"query\")\n\nif st.button(\"쿼리 제출\"):\n    if 'processed_texts' in st.session_state and query and api_key:\n        with st.spinner('처리 중...'):\n            retrieved_documents = rank_doc(query, st.session_state.processed_texts)\n            output_wrapped = rag(query, retrieved_documents, api_key)\n            st.session_state.chat_history.append((query, output_wrapped))\n            st.container().markdown(f\"**질문**: {query}\")\n            st.container().markdown(f\"**답변**: {output_wrapped}\")\n    else:\n        st.error(\"PDF를 업로드하고 API 키를 설정하고 질문을 입력하세요.\")\n```\n\n예시 질문\n\n```js\n    \"2024년에 테슬라가 좋은 투자인가요?\",\n    \"일론 머스크는 천재인가요?\",\n    \"2023년 수익은 얼마인가요?\"\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예시 출력\n\n```js\nQ1에 대한 답변\n죄송하지만, 2023년 12월 31일 종료인 Tesla, Inc.의 연례 보고서로부터 제공된 정보를 기반으로 하여 2024년에 Tesla가 좋은 투자일지에 대해 명확한 답변을 제공할 수 없습니다. 투자 결정은 재무제표 이외의 다양한 요소들을 고려해야 합니다. 시장 상황, 경쟁, 산업 동향, 그리고 전반적인 경제 전망 등을 고려해야 합니다. 투자자들은 투자 결정을 내리기 전에 철저한 연구와 분석을 진행하거나 재무 자문가와 상담하는 것이 권장됩니다.\n\nQ2에 대한 답변\n제공된 정보는 Elon Musk가 천재인지에 대한 직접적인 의견을 제공하지 않습니다. 연례 보고서는 Elon Musk가 Tesla의 Technoking이자 최고 경영자로서의 역할과 그가 다른 기술 기업에 참여한 사실을 개요로 제공합니다. 이는 그의 중요한 책임과 만약 그가 주식을 판매하거나 그의 서비스가 더 이상 사용 불가능해질 경우 Tesla에 미치는 잠재적인 영향을 강조합니다. 그러나 보고서는 Elon Musk가 천재인지에 대한 명확한 평가를 제공하지 않습니다.\n\nQ3에 대한 답변\n2023년 매출은 967.7억 달러였습니다.\n```\n\n임렬한 답변입니다. 다음 기사에서는 다양한 RAG 파이프라인을 평가하여 RAG 모델 구축, 평가 및 모니터링의 완전한 프로세스를 보여줄 것입니다.\n\n본 기사가 도움이 되었다면 5회 이상 클랩을 주시기 바랍니다. 제가 향후 내용을 꾸준히 공유할 Medium 팔로우도 부탁드립니다. 이 기사를 공유하고 의견을 남겨주시기 바랍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고자료\n\nGithub 링크","ogImage":{"url":"/assets/img/2024-06-19-Builda100FreeTraceableandSecureRAGChatbotUsingRerankerandGPT-4o_0.png"},"coverImage":"/assets/img/2024-06-19-Builda100FreeTraceableandSecureRAGChatbotUsingRerankerandGPT-4o_0.png","tag":["Tech"],"readingTime":8},{"title":"Anthropic의 Sparse Autoencoder를 직접 깊이 파헤쳐 보기 ","description":"","date":"2024-06-19 19:52","slug":"2024-06-19-DeepDiveintoAnthropicsSparseAutoencodersbyHand","content":"\n\n## LLMs 해석 가능성 탐색의 개념을 살펴보세요\n\n![이미지](/assets/img/2024-06-19-DeepDiveintoAnthropicsSparseAutoencodersbyHand_0.png)\n\n\"고대의 마법이 공중을 가득 채운 루마리아의 신비한 땅에는 에테리얼 그리핀 제피라가 살았습니다. 사자의 몸과 독수리의 날개를 지닌 제피라는 우주의 비밀을 담은 고대 서언 '진리의 코덱스'의 존중받는 수호자였습니다.\n\n성스러운 동굴에 자리한 코덱스는 제피라의 녹색 눈에 의해 보호되었으며, 그 눈은 속임수를 파헤쳐 순수한 진리를 드러냈습니다. 어느 날, 어둠의 마법사가 루마리아의 땅에 내려와 세계를 무지에 묻기 위해 코덱스를 감추려 했습니다. 마을 사람들은 희망의 기운으로 제피라를 부르자, 제피라는 하늘을 날아 빛의 방패를 만들어 어둠의 마법사를 물리치고 진리를 드러내는 향취가 되었습니다.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n긴 싸움 끝에 어둠의 마법사는 제피라의 빛 앞에는 무리였다는 결론이 내려졌어요. 용기와 경계심을 통해 진정한 빛은 루마리아에 계속하여 빛나고 있었죠. 그리고 시간이 흘러감에 따라 루마리아는 제피라의 지키는 아래 번영을 이루고, 그 길은 제피라가 지켜주는 진리에 의해 계속 밝은 모습을 유지했어요. 그리고 이렇게 제피라의 전설이 이어졌답니다!\n\n# Anthropic의 해석 가능한 기능 추출을 향한 여정\n\n제피라 이야기를 따라, Anthropc AI는 모델에서 의미 있는 특징을 추출하는 원정을 시작했어요. 이 조사의 아이디어는 신경망의 다양한 구성 요소가 어떻게 상호 작용하며 각 구성 요소가 어떤 역할을 하는지 이해하는 데 있어요.\n\n논문인 \"Towards Monosemanticity: Decomposing Language Models With Dictionary Learning\"에 따르면, Sparse Autoencoder는 모델에서 의미 있는 기능을 성공적으로 추출할 수 있어요. 다시 말해, Sparse Autoencoders는 '다의성' 문제를 해결하는 데 도움을 줍니다 - 즉, 여러 의미/해석에 해당하는 신경 활성을 동시에 분해함으로써, 각각의 해석을 지니고 있는 드문드문 활성화 기능에 집중함을 통해 좀 더 단방향적인 요소를 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에 모든 것이 어떻게 이루어지는지 이해하려면, 이 멋진 작품들을 자세히 살펴보면 됩니다. 교수님, 뒤에 감춰진 현상들을 설명해주는 대표적인 작품인 Autoencoders 및 Sparse Autoencoders입니다.\n\n(아래의 모든 이미지는 따로 언급이 없을 경우, 위에서 소개한 LinkedIn 게시물에서 Tom Yeh 교수님의 것으로, 교수님의 허락을 받아 편집한 것입니다.)\n\n먼저, Autoencoder가 무엇인지와 그 작동 방식에 대해 알아봅시다.\n\n## Autoencoder란 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nImagine a writer has his desk strewn with different papers — some are his notes for the story he is writing, some are copies of final drafts, some are again illustrations for his action-packed story. Now amidst this chaos, it is hard to find the important parts — more so when the writer is in a hurry and the publisher is on the phone demanding a book in two days. Thankfully, the writer has a very efficient assistant — this assistant makes sure the cluttered desk is cleaned regularly, grouping similar items, organizing and putting things into their right place. And as and when needed, the assistant would retrieve the correct items for the writer, helping him meet the deadlines set by his publisher.\n\nWell, the name of this assistant is Autoencoder. It mainly has two functions — encoding and decoding. Encoding refers to condensing input data and extracting the essential features (organization). Decoding is the process of reconstructing original data from encoded representation while aiming to minimize information loss (retrieval).\n\nNow let’s look at how this assistant works.\n\n# How does an Autoencoder Work?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주어진: 네 개의 훈련 예제 X1, X2, X3, X4.\n\n### [1] 자동\n\n첫 번째 단계는 훈련 예제들을 대상 Y’로 복사하는 것입니다. 오토인코더의 작업은 이러한 훈련 예제들을 재구성하는 것입니다. 대상이 훈련 예제 자체이기 때문에, ‘자동’이라는 단어가 사용되었으며 이는 ‘자체’를 의미하는 그리스어입니다.\n\n### [2] 인코더: 레이어 1 + ReLU\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전의 모든 모델에서 본 바와 같이, 간단한 가중치와 편향 행렬이 ReLU와 결합된 것이 강력하며 놀라운 결과를 얻을 수 있음을 알 수 있었습니다. 따라서, 첫 번째 인코딩 레이어를 사용하여 원래의 피처 세트 크기를 4x4에서 3x4로 줄입니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*INIu2VmAyBnQHRLUc_pY-g.gif)\n\n## [3] 인코더: 레이어 2 + ReLU\n\n이전 레이어의 출력은 두 번째 인코더 레이어에 의해 처리되며 입력 크기를 2x3으로 더욱 줄입니다. 이 단계에서 관련 피처의 추출이 발생합니다. 이 레이어는 입력 피처보다 훨씬 적은 피처를 가지고 있기 때문에 '병목'이라고도 불립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*0UBKNLacq0ZOXF-f9Tzvzg.gif)\n\n## [4] 디코더: 레이어 1 + ReLU\n\n인코딩 프로세스가 완료되면, 다음 단계는 관련 피처를 디코드하여 최종 출력을 '다시' 작성하는 것입니다. 이를 위해, 우리는 마지막 단계의 피처를 해당 가중치 및 편향과 곱한 다음 ReLU 레이어를 적용합니다. 결과는 3x4 매트릭스입니다.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*yCWisBAtVJ35IZB164Vvew.gif)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## [5] 디코더: 레이어 2 + ReLU\n\n이전 출력에 두 번째 디코더 레이어(가중치, 편향 + ReLU)를 적용하여 최종 결과를 얻습니다. 이 결과는 복원된 4x4 행렬입니다. 우리는 이렇게 함으로써 결과를 원래 목표값과 비교하기 위해 원래 차원으로 돌아갑니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*jUXnoKZk1kQP3MDUA9SLtA.gif)\n\n## [6] 손실 그래디언트 및 역전파\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n디코더 레이어에서 출력을 받은 후, 우리는 출력(Y)과 타겟(Y') 사이의 평균 제곱 오차(MSE)의 그래디언트를 계산합니다. 이를 위해, 우리는 2*(Y-Y')을 찾아 역전파 프로세스를 활성화시키고 가중치와 편향을 업데이트하는 최종 그래디언트를 얻습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*R_qDdXzetVZZJ8oKaEVeig.gif)\n\n자동 인코더의 작동 방식을 이해했으니, 이제 그 희소한 변형이 어떻게 큰 언어 모델(LLMs)에게 해석 가능성을 달성하는지 알아보는 것이 중요합니다.\n\n# 희소 자동 인코더 - 어떻게 작동하나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우선, 다음을 전제로 합시다:\n\n- 트랜스포머의 출력은 피드포워드 레이어에서 처리된 후이다. 즉, 다섯 개의 토큰(X)에 대한 모델 활성화를 가정해봅시다. 이들은 좋지만, 모델이 결정을 내리는 방법이나 예측을 하는 방식에 대한 정보를 제공하지는 않습니다.\n\n![Image](/assets/img/2024-06-19-DeepDiveintoAnthropicsSparseAutoencodersbyHand_1.png)\n\n여기서 주된 질문은:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## [1] 인코더: 선형 레이어\n\n인코더 레이어의 첫 번째 단계는 입력 X를 인코더 가중치로 곱하고 편향을 더하는 것입니다 (오토인코더의 첫 번째 단계와 같이 수행됩니다).\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*xhgfTmOD7ZowFVtSBHPn8Q.gif)\n\n## [2] 인코더: ReLU\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 하위 단계는 ReLU 활성화 함수를 적용하여 비선형성을 추가하고 부정적인 활성화를 억제하는 것입니다. 이 억제는 많은 기능이 0으로 설정되어 희소성 개념을 가능케 하며, 희소하고 해석 가능한 특성 f를 출력합니다.\n\n해석 가능성은 하나 또는 두 개의 양적 특성만 있는 경우에 발생합니다. f6를 살펴보면 X2 및 X3이 양수인 것을 볼 수 있으며, 둘 다 'Mountain'을 가지고 있을 수 있다고 말할 수 있습니다.\n\n![Image](https://miro.medium.com/v2/resize:fit:1400/1*L5PxylZCTjdNULt4gjt7oQ.gif)\n\n## [3] 디코더: 재구성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인코더 작업이 완료되면, 디코더 단계로 넘어갑니다. 우리는 f를 디코더 가중치와 바이어스와 곱한 후 더합니다. 이것은 해석 가능한 특성으로부터 X의 재구성인 X'을 출력합니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*3sZJcXiZSQVr41YSTA33RA.gif)\n\n오토인코더처럼, 우리는 X'이 가능한 X에 가깝도록 하고 싶습니다. 이를 보장하기 위해 추가로 훈련이 중요합니다.\n\n## [4] 디코더 : 가중치\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n중간 과정으로 이번 단계의 각 가중치에 대해 L2 노름을 계산합니다. 나중에 사용할 수 있도록 따로 저장합니다.\n\n![Image](https://miro.medium.com/v2/resize:fit:824/1*k3zIB0kEP1sewwORw08FOQ.gif)\n\n## 훈련\n\n이전에 언급했듯이 Sparse Autoencoder은 재구성된 X'를 X에 더 가깝게 만들기 위해 광범위한 훈련을 실시합니다. 이를 설명하기 위해 아래 단계로 진행합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## [5] 희소성: L1 손실\n\n여기서의 목표는 가능한 한 많은 값을 0 또는 0에 가까운 값으로 얻는 것입니다. 우리는 가중치의 절대값을 패널티로 삼아 L1 희소성을 활용하여 이를 수행합니다. 핵심 아이디어는 합을 가능한 작게 만들고 싶다는 것입니다.\n\n![image](https://miro.medium.com/v2/resize:fit:1080/1*RYebksXA--6kfOCWkZxRiA.gif)\n\n## [6] 희소성: 경사\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 단계는 L1의 기울기를 계산하는 것입니다. 양수 값에 대해 -1로 설정됩니다. 따라서 모든 f `0` 값에 대해 결과는 -1로 설정됩니다.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*1WXXLP5p7zYyBK2T22CbcA.gif)\n\n![image](/assets/img/2024-06-19-DeepDiveintoAnthropicsSparseAutoencodersbyHand_2.png)\n\n![image](/assets/img/2024-06-19-DeepDiveintoAnthropicsSparseAutoencodersbyHand_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-06-19-DeepDiveintoAnthropicsSparseAutoencodersbyHand_4.png)\n\n![Image 2](/assets/img/2024-06-19-DeepDiveintoAnthropicsSparseAutoencodersbyHand_5.png)\n\n## [7] 희소성: 제로\n\n이미 0인 모든 다른 값들은 변경하지 않고 그대로 유지합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*gtDUWgJ11gs1bh77CEt-Qw.gif)\n\n## [8] 희소성 : 가중치\n\nStep 6에서 얻은 그래디언트 행렬의 각 행을 Step 4에서 얻은 해당 디코더 가중치로 곱합니다. 이 단계는 잘못된 정보를 추가하여 결과를 재구성하는 동안 모델이 큰 가중치를 학습하는 것을 방지하는 데 중요합니다.\n\n![image](https://miro.medium.com/v2/resize:fit:1104/1*kM4XIHlPsa7su69XV11H7Q.gif)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## [9] 재구성: MSE 손실\n\nX’와 X 사이의 차이를 계산하기 위해 평균 제곱 오차 또는 L2 손실 함수를 사용합니다. 이전에 본 것과 같이 목표는 오차를 최소화하는 것입니다.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*3bOB5l-c-cXhrtX89Fk0AA.gif)\n\n## [10] 재구성: Gradient\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nL2 손실의 경사는 2*(X'-X)입니다.\n\n따라서 원래의 오토인코더에서처럼 backpropagation을 실행하여 가중치와 편향을 업데이트합니다. 이 중요한 점은 희소성(sparsity)과 재구성(reconstruction) 사이에 좋은 균형을 찾는 것입니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1004/1*3MDwExTyz4ImSJX2GMzHkA.gif)\n\n이로써, 모델이 개념을 이해하고 응답을 생성하는 방향을 학습하는 매우 똑똑하고 직관적인 방법의 끝에 도달했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 요약하면:\n\n- 오토 인코더는 일반적으로 Encoder와 Decoder 두 부분으로 구성됩니다. Encoder는 가중치와 편향을 사용하고 ReLU 활성화 함수와 결합하여 초기 입력 기능을 낮은 차원으로 압축하여 관련 부분들만을 캡처하려고 합니다. 반면에 Decoder는 Encoder의 출력을 가져와 입력 기능을 원래 상태로 다시 복원하려고 노력합니다. 오토 인코더의 대상은 초기 기능 자체이기 때문에 'auto'라는 단어가 사용됩니다. 목표는 일반적인 신경망의 경우와 마찬가지로 목표와 입력 기능 사이의 최소 오차(차이)를 달성하는 것입니다. 이는 네트워크를 통해 오차의 기울기를 전파하면서 가중치와 편향을 업데이트하여 달성됩니다.\n\n- Sparse Autoencoder는 일반 오토 인코더와 동일한 구성요소로 구성되며 몇 가지 추가 구성요소가 더 있습니다. 여기서 중요한 것은 교육 단계에서 다른 접근 방식입니다. 해석 가능한 기능을 검색하는 것이 목표이므로 비교적 의미가 적은 값들을 제로로 만들고자 합니다. Encoder가 음의 값들을 억제하기 위해 ReLU를 사용한 후에 결과에 L1 손실을 사용하여 가중치의 절대값을 처벌하여 희소성을 장려합니다. 손실 함수에 패널티 항을 추가하여 이를 달성합니다. 이 패널티는 가중치의 절대값의 합입니다: λΣ|w|. 남아있는 0이 아닌 가중치는 모델의 성능에 중요한 것들입니다.\n\n# 희소성을 활용하여 해석 가능한 기능 추출\n\n인간은 특정 자극에 대한 반응으로 뉴런의 작은 하위 집합만을 활성화시킵니다. 마찬가지로, Sparse Autoencoder는 L1 정규화와 같은 희소성 제약을 활용하여 입력의 희소한 표현을 학습합니다. 이렇게 함으로써 Sparse Autoencoder는 복잡한 데이터로부터 해석 가능한 기능을 추출하여 학습된 기능의 단순함과 해석 가능성을 향상시킵니다. 이 생물학적 신경 과정을 모방한 선택적 활성화는 모델이 입력 데이터의 가장 관련성 있는 측면에 초점을 맞추어 모델을 더 견고하고 효율적으로 만듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAnthropic의 노력을 통해 AI 모델의 해석 가능성을 이해하려는 노력은 투명하고 이해하기 쉬운 AI 시스템의 필요성을 강조하며, 특히 잠재적인 중요한 결정 과정에 더 많이 통합되면서 중요해지고 있습니다. 강력하고 해석 가능한 모델을 만드는 데 초점을 맞춘 Anthropic은 신뢰할 수 있고 실제 응용 프로그램에서 효과적으로 활용할 수 있는 AI 개발에 기여합니다.\n\n요약하면, Sparse Autoencoders는 해석 가능한 특징을 추출하고 모델의 견고성을 향상시키며 효율성을 보장하는 데 중요합니다. 이러한 강력한 모델을 이해하고 어떻게 추론을 만드는지 계속 연구함으로써 AI에서의 해석 가능성의 중요성이 커지고 있으며, 더 투명한 AI 시스템의 길을 열고 있습니다. 이러한 개념이 어떻게 발전하고 우리를 안전하게 AI를 삶 속에 통합하는 미래로 나아가게 될지 기대됩니다!\n\nP.S. 본 연습을 직접 진행하고 싶다면 여기에 빈 템플릿 링크가 있습니다.\n\n손으로 연습할 빈 템플릿\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지퍼가 진리의 코덱스를 안전하게 보호할 수 있도록 도와주는 재미를 누려보세요!\n\n\n\u003cimg src=\"/assets/img/2024-06-19-DeepDiveintoAnthropicsSparseAutoencodersbyHand_6.png\" /\u003e\n\n\n이 작업을 지원해준 톰 예 교수님께 특별히 감사드립니다!\n\n## 참고문헌:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[1] Monosemanticity를 향하여: 사전 학습으로 언어 모델을 분해하기, Bricken 등. 2023년 10월 https://transformer-circuits.pub/2023/monosemantic-features/index.html\n\n[2] Monosemanticity 확대하기: Claude 3 Sonnet에서 해석 가능한 특징 추출하기, Templeton 등. 2024년 5월 https://transformer-circuits.pub/2024/scaling-monosemanticity/","ogImage":{"url":"/assets/img/2024-06-19-DeepDiveintoAnthropicsSparseAutoencodersbyHand_0.png"},"coverImage":"/assets/img/2024-06-19-DeepDiveintoAnthropicsSparseAutoencodersbyHand_0.png","tag":["Tech"],"readingTime":10},{"title":"텍스트, 이미지 및 오디오를 지원하는 멀티모달 LLM LLaVA  Whisper 구축 방법","description":"","date":"2024-06-19 19:50","slug":"2024-06-19-HowtoBuildaTextImageandAudio-CapableMultimodalLLMLLaVAWhisper","content":"\n\n![이미지](/assets/img/2024-06-19-HowtoBuildaTextImageandAudio-CapableMultimodalLLMLLaVAWhisper_0.png)\n\n안녕하세요! 이 블로그는 두 개의 오픈 소스 모델을 활용하여 텍스트, 이미지 및 오디오 지원이 가능한 멀티모달 LLM을 구축하는 방법에 대해 소개하고 있어요. LLaVA와 Whisper라는 두 모델은 각각 독특한 능력을 가지고 있답니다.\n\n멀티모달 LLM은 다양한 데이터 유형을 지원할 수 있어요. 이 모델의 워크플로우는 세 가지 모달을 지원할 수 있어요. 이미지, 텍스트, 그리고 오디오; 이미지가 모델로 입력되며, 해당 이미지를 기반으로 한 입력 프롬프트가 오디오로 제공돼요.\n\n오디오는 whisper 모델을 활용하여 전사되어 모델로 텍스트 입력을 제공해요. 그럼 모델은 텍스트 컨텐츠를 생성하고, gTTS (Google 변역 텍스트 음성 합성) 패키지를 활용하여 텍스트 콘텐츠를 오디오로 변환해요. 기본적으로 결과는 텍스트와 오디오 형식으로 표시돼요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## LLaVA 모델:\n\n첫 번째 모델은 LLaVA입니다. LLaVA는 \"큰 언어 및 비전 어시스턴트\"를 나타냅니다. LLaVA는 GPT로 생성된 멀티모달 명령을 미세 조정하고 있는 LlamA/Vicuña를 바탕으로 훈련된 오픈 소스 모델입니다. 이는 transformer 아키텍처를 기반으로 한 자기회귀 언어 모델입니다. 이 LLM 모델은 비전 인코딩 기능을 가지고 있습니다. 텍스트와 이미지 형식 모두에서 문맥을 이해할 수 있습니다.\n\nLLaVA는 비전 인코더와 Vicuna를 결합하여 시각 및 언어 이해를 가능하게 하는 혁신적인 솔루션입니다. 자연어와 컴퓨터 비전의 융합은 인공지능 분야에서 중요한 발전을 이끌어내었습니다.\n\n![이미지](/assets/img/2024-06-19-HowtoBuildaTextImageandAudio-CapableMultimodalLLMLLaVAWhisper_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델 워크플로우는 네트워크 아키텍처를 통해 이해할 수 있습니다. 비전 인코더는 입력 이미지에서 특징을 추출하는 데 사용됩니다. 특징과 입력 텍스트 명령은 벡터로 변환되며, 이 벡터 값은 모델에서 처리되어 관련 콘텐츠가 출력됩니다.\n\n\"시각 지시 조정\"이라는 연구 논문에서는 LLAVA(Large Language and Vision Assistant)라는 혁신적인 접근 방식을 소개합니다. 이 접근 방식은 GPT-4의 성능을 활용하여 새로운 다중 모달 지시 데이터 패러다임을 만들어냅니다. 이 모델은 원래 텍스트 기반 작업을 위해 설계된 것이었지만, 텍스트와 시각적 구성 요소를 원활하게 통합하는 방식으로 작동합니다. 시각적 지시 조정은 큰 언어 모델(Large Language Model, LLM)을 세밀하게 조정하여 시각적 신호를 기반으로 지시를 이해하고 실행하는 기술입니다.\n\n이 접근 방식은 언어와 시각 사이의 연결을 확립하여, 인공지능 시스템이 두 가지 형태를 포함하는 사람의 지시를 이해하고 실행할 수 있도록 하는 것을 목표로 합니다. 예를 들어, 이미지를 설명하거나 가상 환경에서 작업을 수행하거나 사진 속 장면에 대한 질문에 답변하도록 기계 학습 모델에 요청하는 상황을 상상해보세요. 시각 지시 조정은 이러한 작업을 효과적으로 수행할 수 있도록 모델에 능력을 제공합니다.\n\n## 휘스퍼 모델:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 번째 모델은 휘스퍼입니다. 이 모델은 OpenAI에서 개발되었습니다. 휘스퍼는 웹에서 수집된 68만 시간의 다국어 및 다작업 감독 데이터로 훈련된 자동 음성 인식(ASR) 시스템으로 일반 목적의 음성 인식 모델입니다.\n\n휘스퍼는 다양한 오디오 데이터셋에서 훈련되었으며, 다국어 음성 인식, 음성 번역 및 언어 식별을 수행할 수 있는 다작업 모델입니다.\n\n다양한 음성 처리 작업(다국어 음성 인식, 음성 번역, 말의 언어 식별, 음성 활동 감지)에 대해 트랜스포머 시퀀스-투-시퀀스 모델이 훈련되었습니다.\n\n![이미지](/assets/img/2024-06-19-HowtoBuildaTextImageandAudio-CapableMultimodalLLMLLaVAWhisper_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n이러한 작업들은 전통적인 음성 처리 파이프라인의 여러 단계를 대체하는 단일 모델이 가능하도록, 디코더에 의해 예측될 토큰 시퀀스로 공동으로 표현됩니다. 멀티태스크 트레이닝 형식은 특별한 토큰 세트를 사용하여 태스크 지정자 또는 분류 대상으로 작용합니다.\n\n# 모델 구축을 시작해봅시다\n\n아래 코드를 실행하려면 GPU를 사용할 수 있는 시스템이 필요합니다. 저는 Google Colab \"T4 GPU\"를 사용하여 전체 코드를 실행했습니다.\n\n첫 번째 단계는 우리 환경에 필요한 패키지를 설치하는 것입니다. transformer 라이브러리는 모델 파이프라인을 생성하는 데 사용되며, bitsandbytes 라이브러리는 CUDA 사용자 지정 함수 (특히 8비트 최적화기, 행렬 곱셈 (LLM.int8()) 및 8 + 4비트 양자화 함수)의 가벼운 Python 래퍼입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그런 다음 OpenAI 휘스퍼 모델을 사용하여 음성을 텍스트로 변환하고, gTTs 패키지를 사용하여 텍스트를 음성으로 변환합니다. Gradio 라이브러리를 사용하여 모델을 위한 사용자 인터페이스를 만듭니다.\n\n```js\n!pip install -q transformers==4.37.2\n!pip install bitsandbytes==0.41.3 accelerate==0.25.0\n!pip install -q git+https://github.com/openai/whisper.git\n!pip install -q gradio\n!pip install -q gTTs\n```\n\n두 번째 단계는 설치된 라이브러리에서 필요한 모듈을 가져오는 것입니다.\n\n```js\nimport torch\nfrom transformers import BitsAndBytesConfig, pipeline\nimport whisper\nimport gradio as gr\nimport time\nimport warnings\nimport os\nfrom gtts import gTTS\nfrom PIL import Image\nimport re\nimport datetime\nimport requests\nimport nltk\nfrom nltk import sent_tokenize\nimport base64\nimport numpy as np\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n세 번째 단계는 \"BitsAndBytesConfig\" 모듈을 사용하여 모델 양자화 매개변수를 설정하는 것입니다. 모델 양자화는 모델 매개변수를 표현하는 데 사용되는 숫자의 정밀도를 줄이는 과정입니다. 이는 모델 크기를 크게 줄이고 추론 속도를 높여주어, 특히 자원이 제한된 하드웨어에서 효율적으로 실행할 수 있습니다.\n\n이 매개변수는 모델을 4비트 정밀도로로로 로드해야 함을 나타냅니다. 일반적인 신경망 매개변수는 일반적으로 32비트 부동소수점(float32) 형식으로 저장됩니다. 이를 4비트로 줄이면 각 매개변수가 더 적은 메모리를 사용하게 되어, 더 작은 모델 크기와 빠른 연산 속도를 가져옵니다. 이는 모델을 4비트 양자화로 로드하고 16비트 부동소수점 정밀도로 계산함을 의미합니다.\n\n이는 모델의 메모리 풋프린트를 크게 줄여주어, 제한된 메모리를 가진 장치에 배포할 수 있게 해줍니다. 그러나, 이것의 대가로 정밀도를 줄였기 때문에 모델 정확도에 약간의 손실이 있을 수 있습니다.\n\n모델 파이프라인에서는 이미지에서 텍스트로의 생성 작업을 위해 LLaVA 1.5B 매개변수 모델을 사용하고 양자화 구성을 전달했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nquant_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_compute_dtype = torch.float16\n)\n\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\n\npipe = pipeline(\n    \"image-to-text\",\n    model = model_id,\n    model_kwargs={\"quantization_config\": quant_config}\n)\n```\n\n네 번째 단계는 시스템 계산 단위를 \"DEVICE\" 변수로 구성하는 것입니다. 이것은 속삭임 모델을 로드하기 위한 필수 매개변수입니다. 그 후, 우리는 속삭임 모델을 다운로드하고, 39M (작음), 74M (베이스), 244M (작은), 769M (중간), 1550M (큰)과 같이 다양한 매개변수 수가 있는 모델이 나왔습니다. 우리는 769M 매개변수를 가진 속삭임-중간 모델을 사용하고 있습니다.\n\n이 속삭임 모델은 762,321,920개의 다국어 기능을 갖추고 있습니다. OpenAI의 속삭임 모델은 99가지 다른 언어를 지원하지만, 영어 언어에서 가장 잘 작동합니다.\n\n```js\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"using torch {torch.__version__} ({DEVICE})\")\n\nmodel = whisper.load_model(\"medium\", device = DEVICE)\n\nprint(\n    f\"Model is {'다국어 ' if model.is_multilingual else '영어 전용'}\"\n    f\"이며 매개변수는 {sum(np.prod(v.shape) for v in model.parameters()):,}개 있습니다.\"\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 코드는 현재 날짜와 시간을 기준으로 고유한 이름을 가진 로그 파일에 텍스트 항목을 작성하는 로깅 시스템을 생성하는 데 사용됩니다. 이는 이벤트 추적, 디버깅 또는 타임스탬프와 함께 일련의 작업 또는 메시지를 기록해야 하는 상황에 유용할 수 있습니다.\n\n```js\n#Logger file\ntstamp = datetime.datetime.now()\ntstamp = str(tstamp).replace(\" \", \"_\")\nlogfile = f\"log_{tstamp}.txt\"\n\ndef writehistory(text):\n  with open(logfile, \"a\", encoding='utf-8') as f:\n    f.write(text)\n    f.write(\"\\n\")\n  f.close()\n```\n\n\"img2txt\" 함수는 LLaVA 모델 파이프라인을 사용합니다. 이 함수는 이미지와 텍스트 프롬프트를 인자로 사용합니다. 함수는 입력 이미지와 관련된 모델에 의해 생성된 텍스트 콘텐츠를 생성합니다.\n\n```js\ndef img2txt(input_text, input_image):\n\n    # 이미지 로드\n    image = Image.open(input_image)\n\n    writehistory(f\"Input text: {input_text} - Type: {type(input_text)} - Dir: {dir(input_text)}\")\n    if type(input_text) == tuple:\n        prompt_instructions = \"\"\"\n       가능한 한 자세히 이미지를 설명하십시오.\n       이미지에 대한 질문에 답변할 수 있는 유용한 AI 어시스턴트입니다.\n       이미지에 대한 전체 내용은 무엇입니까?\n       이제 유용한 답변을 생성하십시오.\n        \"\"\"\n    else:\n        prompt_instructions = \"\"\"\n        가능한 한 자세히 이미지 설명을 전문적으로 분석하여 다음 프롬프트에 응답하십시오:\n        \"\"\" + input_text\n\n    writehistory(f\"prompt_instructions: {prompt_instructions}\")\n    prompt = \"USER: \u003cimage\u003e\\n\" + prompt_instructions + \"\\nASSISTANT:\"\n\n    outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n\n    # 적절히 응답 텍스트 추출\n    if outputs is not None and len(outputs[0][\"generated_text\"]) \u003e 0:\n        match = re.search(r'ASSISTANT:\\s*(.*)', outputs[0][\"generated_text\"])\n        if match:\n            # \"ASSISTANT:\" 이후의 텍스트 추출\n            reply = match.group(1)\n        else:\n            reply = \"응답을 찾을 수 없습니다.\"\n    else:\n        reply = \"생성된 응답이 없습니다.\"\n\n    return reply\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"transcribe\" 기능은 whisper 모델을 사용합니다. 이 기능은 오디오를 입력으로 받아서 전사된 텍스트를 출력으로 반환합니다. 전사된 텍스트는 모델의 입력으로 사용됩니다.\n\nMarkdown 형식으로 표를 변경하겠습니다.\n\n\ndef transcribe(audio):\n\n    # 입력 오디오가 None이거나 비어 있는지 확인합니다.\n    if audio is None or audio == '':\n        return ('', '', None)  # 빈 문자열 및 None 오디오 파일을 반환합니다.\n\n    # language = 'en'\n\n    audio = whisper.load_audio(audio)\n    audio = whisper.pad_or_trim(audio)\n\n    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n\n    _, probs = model.detect_language(mel)\n\n    options = whisper.DecodingOptions()\n    result = whisper.decode(model, mel, options)\n    result_text = result.text\n\n    return result_text\n\n\n\"text_to_speech\" 함수는 gTTs 패키지를 사용하여 텍스트와 파일 경로를 인수로 취합니다. 텍스트는 오디오로 변환되고, 그 오디오 파일은 지정된 파일 경로에 저장됩니다.\n\n\ndef text_to_speech(text, file_path):\n    language = 'en'\n\n    audioobj = gTTS(text=text,\n                    lang=language,\n                    slow=False)\n\n    audioobj.save(file_path)\n\n    return file_path\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n워크플로의 마지막 단계는 이미 만들어진 모든 기능을 통합하여 사용자 인터페이스를 생성하는 것입니다. \"process_inputs\" 함수는 모든 함수를 활용합니다. 사용자로부터 두 가지 입력을 받습니다: 하나는 이미지이고, 또 다른 하나는 이미지 기반 음성 입력입니다.\n\n## 모델 워크플로 요약\n\n이미지와 텍스트(휘스퍼 모델로부터 오디오로 변환된 텍스트)가 입력으로 LLaVA 모델에 제공됩니다. LLaVA 모델은 입력 이미지를 기반으로 출력 콘텐츠를 생성합니다. 생성된 텍스트 콘텐츠는 gTTs 라이브러리를 사용하여 오디오로 변환됩니다. 출력은 사용자 인터페이스에서 텍스트 및 오디오 형태로 표시됩니다.\n\n```python\n# 오디오 및 이미지 입력을 처리하는 함수\ndef process_inputs(오디오_경로, 이미지_경로):\n    # 오디오 파일 처리 (이를 처리하는 함수 'transcribe'를 가정합니다)\n    음성_텍스트_출력 = transcribe(오디오_경로)\n\n    # 이미지 입력 처리\n    if 이미지_경로:\n        chatgpt_output = img2txt(음성_텍스트_출력, 이미지_경로)\n    else:\n        chatgpt_output = \"이미지가 제공되지 않았습니다.\"\n\n    # 'transcribe'가 처리된 오디오 파일 경로도 반환한다고 가정합니다\n    처리된_오디오_경로 = text_to_speech(chatgpt_output, \"Temp3.mp3\")  # 다르면 실제 경로로 교체\n\n    return 음성_텍스트_출력, chatgpt_output, 처리된_오디오_경로\n\n# 인터페이스 생성\niface = gr.Interface(\n    fn=process_inputs,\n    inputs=[\n        gr.Audio(sources=[\"microphone\"], type=\"filepath\"),\n        gr.Image(type=\"파일경로\")\n    ],\n    outputs=[\n        gr.Textbox(label=\"음성을 텍스트로 변환\"),\n        gr.Textbox(label=\"출력1\"),\n        gr.Audio(\"Temp.mp3\")\n    ],\n    title=\"Multimodel LLM\",\n    description=\"이미지를 업로드하고 음성 입력을 통해 상호작용합니다.\"\n)\n\n# 인터페이스 실행\niface.launch(debug=True)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGradio 인터페이스를 실행한 후, 모델과 상호 작용할 수 있는 사용자 인터페이스가 나타납니다.\n\n![Gradio Interface](/assets/img/2024-06-19-HowtoBuildaTextImageandAudio-CapableMultimodalLLMLLaVAWhisper_3.png)\n\n읽어 주셔서 감사합니다!\n\n더 알고 싶다면:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- LLaVA: [링크](https://huggingface.co/llava-hf/llava-1.5-7b-hf)\n- Whisper: [링크](https://github.com/openai/whisper)\n\n# Vasukumar P\n\nLinkedIn\n\nGitHub","ogImage":{"url":"/assets/img/2024-06-19-HowtoBuildaTextImageandAudio-CapableMultimodalLLMLLaVAWhisper_0.png"},"coverImage":"/assets/img/2024-06-19-HowtoBuildaTextImageandAudio-CapableMultimodalLLMLLaVAWhisper_0.png","tag":["Tech"],"readingTime":10},{"title":"대화 형식으로 친근한 말투로 번역하면 다음과 같습니다 커다란 언어 모델을 사용한 사고 체계인 Buffer of ThoughtsBoT 이해하기","description":"","date":"2024-06-19 19:48","slug":"2024-06-19-UnderstandingBufferofThoughtsBoTReasoningwithLargeLanguageModels","content":"\n\n![사진1](/assets/img/2024-06-19-UnderstandingBufferofThoughtsBoTReasoningwithLargeLanguageModels_0.png)\n\n복잡한 추론 작업에서의 능숙성 향상 및 환각 방지는 대규모 언어 모델(LLMs)의 주요 연구 주제입니다. 노력에도 불구하고, LLMs는 일반화된 추론 능력을 향상시키는 데 도움이 필요합니다. Chain-of-Thought (CoT)나 Tree-of-Thought (ToT)와 같은 전통적인 방법은 종종 다수의 가정이나 번갈아가며 진행되는 프롬프팅을 필요로 하기 때문에 계산이 많이 필요합니다.\n\n![사진2](/assets/img/2024-06-19-UnderstandingBufferofThoughtsBoTReasoningwithLargeLanguageModels_1.png)\n\n논문에서 제안된 새로운 방법인 \"Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models\" [1]은 이러한 제한 사항을 대응하기 위해 high-level thought templates의 동적이고 적응형 저장소인 meta-buffer를 활용합니다. BoT에서 사용자가 새로운 문제를 제시하면 먼저 문제가 단순화되고 분석되어 핵심 요소가 추출되며, 이후 동적 데이터셋에서 관련된 thought template을 검색하는데 이를 이용합니다. 이를 통해 수정된 복잡한 추론 패턴을 통해 적응적이고 효율적인 문제 해결이 가능해집니다. 원문에 따르면 이 방법은 \"Llama3–8B+BoT가 Llama3–70B 모델을 능가할 잠재력이 있다\"고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBoT은 템플릿과 유사한 문제들 간에 효율적인 추론을 이룹니다:\n\n- (1) 새로운 도전에 이전 솔루션을 활용하며,\n- (2) 쿼리 반복을 제거하여 효율성을 증가시킵니다 (우리가 그래프-오브-쏘츠(GoT)나 ToT에서 볼 수 있듯이), 그리고\n- (3) 새로운 작업을 만나면 템플릿 저장소를 동적으로 업데이트하여 진화할 수 있도록 합니다.\n\n이 글에서는 먼저 BoT의 작동 방식의 일반적인 개요를 살펴본 후, 각 주요 요소의 기능을 이해하고 예시를 통해 절차를 테스트해보겠습니다.\n\n# BoT는 어떻게 작동하나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적인 사고 증강 추론 과정(아래 그림 참조)은 문제 증류로 시작됩니다. 이 단계에서는 들어오는 작업을 분석하여 중요한 요소와 제약 조건으로 요약하고 간소화된 문제 설명을 만듭니다.\n\n이 요약된 정보는 그런 다음 메타 버퍼를 쿼리하는 데 사용됩니다. 이 메타 버퍼는 고수준 사고 템플릿을 포함하는 동적 저장소입니다. 사고 템플릿 중에서 증류된 문제와 가장 유사한 것을 검색합니다. 그런 다음 인스턴스화 프로세스 중에 특정 요구 사항 및 증류된 문제에 대한 정보와 함께 인스턴스화됩니다.\n\n이 과정 동안 버퍼 매니저는 메타 버퍼를 적극적으로 모니터링합니다. 메타 버퍼에 포함되지 않은 새로운 통찰력을 감지하면 버퍼 매니저가 업데이트하여 사고 템플릿 저장소가 지속적으로 발전하도록 합니다.\n\n![이미지](/assets/img/2024-06-19-UnderstandingBufferofThoughtsBoTReasoningwithLargeLanguageModels_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 주요 부분들을 자세히 살펴봅시다:\n\n## 문제 축약기\n\n문제 축약기는 입력 작업에 대한 전처리로 볼 수 있습니다. 이를 통해…\n\n- (1) 문제의 필수적인 정보를 추출하고\n- (2) 복잡한 작업을 단순화하여 사고 템플릿을 더 쉽게 검색하고 검색할 수 있게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문제 디스틸러는 LLM에게 문제의 중요 정보와 제약 조건을 식별하고 추출하는 부담을 덜어줍니다. 이 작업은 메타 프롬프트 ϕ를 통해 수행됩니다.\n\n작성자가 사용하는 프롬프트는 다음과 같은 과제에 대한 핵심 정보를 증류하는 데 사용됩니다:\n\n```js\n[Problem Distiller]:\n정보 증류에서 뛰어난 전문가로, 사용자 입력 쿼리로부터 문제를 해결하기 위해 필요한 핵심 정보를 효과적으로 추출하는 당신은\n해당 문제 유형에 기반하여 추출된 정보를 적절한 형식으로 변환합니다.\n사용자의 입력 쿼리로부터 문제를 해결하기 위해 필요한 중요 정보를 분류하고 추출하십시오. 증류된 정보에는 다음이 포함되어야 합니다.\n1. 핵심 정보:\n사용자 입력으로부터 추출된 주요 변수의 값 및 정보, 문제 해결 전문가에게 전달될 것입니다. 문제를 해결하는 데 필요한 모든 중요 정보를 제공합니다.\n2. 제약 조건:\n문제의 목적 및 해당 제약 조건.\n3. 증류된 작업:\n1과 2를 기반으로 문제를 확장하고, 사용자 쿼리에 대응하고 더 많은 입력 및 출력 변형을 처리할 수 있는 메타 문제를 요약합니다. 확장된 문제에 실제 세계 시나리오와 초기 문제의 주요 변수 및 정보 제약 조건을 통합하여 주요 변수를 제한한 후 사용자 쿼리 입력의 핵심 정보를 입력으로 사용하여 문제를 해결하는 예시를 제시하십시오.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 메타 버퍼\n\n메타 버퍼는 고수준 사고 템플릿을 저장하는 중앙 데이터베이스입니다. 이러한 템플릿들은 다양한 문제 해결 프로세스를 나타내는 고수준 추상화입니다. LLM은 과거 문제와 통찰을 활용하여 현재의 도전 과제를 해결할 수 있습니다. 가장 좋은 점은 메타 버퍼가 동적으로 업데이트되어 새로운 보이지 않는 문제도 포함되도록 하는 것입니다. 메타 버퍼는 특정 지침을 따르도록 사고 템플릿을 강요하지는 않습니다.\n\n템플릿 검색: 작업이 단순화되면, BoT가 사고 템플릿을 확인하고 작업과 가장 유사한 것을 선택합니다. 이는 작업과 사고 템플릿 간의 내포 유사성을 계산하여 수행됩니다.\n\n![이미지](/assets/img/2024-06-19-UnderstandingBufferofThoughtsBoTReasoningwithLargeLanguageModels_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n리트리버는 입력 작업의 임베딩 f(xd)와 템플릿의 임베딩 f(DTi) 사이의 유사성을 계산합니다. 이 작업은 유사성이 일정 임계값 δ(0.5~0.7)를 초과할 때만 수행됩니다. 생각 템플릿 중 어느 것도 해당 작업과의 유사성 점수가 δ 임계값을 초과하지 않으면 xd를 새로운 작업으로 식별합니다. 작업이 새로운지 여부에 따라 두 가지 경로 중 하나가 선택됩니다:\n\n- 작업이 생각 템플릿 중 하나와 유사한 경우, 해당 템플릿은 instantiation prompt를 사용하여 요약 정보로 구체화됩니다 (논문에서 확인할 수 있습니다). 이 instantiation 과정은 아래와 같이 표현할 수 있습니다:\n\n![image](/assets/img/2024-06-19-UnderstandingBufferofThoughtsBoTReasoningwithLargeLanguageModels_5.png)\n\n- 작업이 새로운 경우, 문제의 다양한 범위를 다루기 위해 설계된 일반적인 생각 템플릿이 사용됩니다. 작업이 처리되는 동안 버퍼 관리자가 관찰하고 학습하며 새로운, 보다 구체적인 생각 템플릿을 만들어 메타 버퍼에 푸시할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 버퍼 관리자\n\n버퍼 관리자는 메타 버퍼를 유지하고 향상시키는 데 중요한 역할을 합니다. 해결된 작업에서 얻은 새로운 통찰과 결과에 기초하여, 생각 템플릿을 업데이트합니다. 또한 새로운 문제가 해결되거나 크게 다른 문제인 경우, 버퍼 관리자는 새로운 생각 템플릿을 만들지 여부를 평가합니다. 이는 생각 템플릿이 핵심을 유지하고 중복되지 않도록 하기 위한 것입니다.\n\n![이미지](/assets/img/2024-06-19-UnderstandingBufferofThoughtsBoTReasoningwithLargeLanguageModels_6.png)\n\n위 공식을 활용하여, 버퍼 관리자는 메타 버퍼에 이미 문제를 해결하기에 필요한지 여부를 확인합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## BoT 대 Single-Query 대 Multi-Query\n\n이전 방법들과 비교하여 BoT는 어떻게 뛰어날까요? 논문 저자들은 다양한 작업의 다양한 데이터셋에서 BoT를 포함한 다양한 방법을 평가했습니다. 이들 작업에는 데이터 이해, 파이썬 프로그래밍 퍼즐, 다국어 초등 수학(MGSM) 등이 포함되어 있습니다. 결과는 거의 모든 작업에서 BoT의 놀라운 우위를 보여줍니다.\n\n![BoT](/assets/img/2024-06-19-UnderstandingBufferofThoughtsBoTReasoningwithLargeLanguageModels_7.png)\n\nBoT의 주요 장점 중 하나는 효율성입니다. Multi-Query 프롬프팅 방법과 비교했을 때, 평균적으로 계산 비용의 12%만 필요합니다. ToT와 같은 Multi-Query 방법의 높은 계산 비용과 대기 시간으로 실제 사용 사례에서는 실용적이지 않게 만들 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-UnderstandingBufferofThoughtsBoTReasoningwithLargeLanguageModels_8.png\" /\u003e\n\n# Buffer of Thoughts 실전 적용\n\nBufer of Thoughts(BoT)를 위한 데모 코드는 GitHub [2]에 게시되어 있습니다. 실전에서 이 기능을 테스트해보기 위해 이 방법을 커스텀 작업인 단어 재배열에 사용할 것입니다. 이 작업에서는 대규모 언어 모델(LLM)이 \"Sam name is my\"와 같이 단어가 뒤섞인 문장을 가져와 이 중의 의미 있는 단어 순열인 \"my name is Sam\"과 같이 반환해야 합니다(이는 벤치마크와 기준 성능이 없는 예시입니다). 몇 가지 뒤섞인 문장과 올바른 문장의 예시는 다음과 같습니다:\n\n```js\n{\"input\": \"\u003cstart\u003e life plan and families to for social hospital workers outside with patients work the \u003cend\u003e\",\n\"target\": \"\u003cstart\u003e social workers work with patients and families to plan for life outside the hospital \u003cend\u003e\"}\n{\"input\": \"\u003cstart\u003e yield plant refers dry total to production biological matter \u003cend\u003e\",\n\"target\": \"\u003cstart\u003e biological yield refers to total plant dry matter production \u003cend\u003e\"}\n{\"input\": \"\u003cstart\u003e the bloodstream into alcohol from directly stomach goes the \u003cend\u003e\",\n\"target\": \"\u003cstart\u003e alcohol goes directly from the stomach into the bloodstream \u003cend\u003e\"}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n유저 프롬프트:\n단어 목록을 재배열하여 의미가 있는 문장으로 만드세요, 예를 들면 \"Sam name is my\" -\u003e \"my name is Sam\". 설명은 제외하고 재배열된 문장만 제공해주세요. 정렬된 문장은 \"\u003cstart\u003e\"로 시작하고 \"\u003cend\u003e\"로 끝냅니다.\n\n입력:\n\u003cstart\u003e the melting in solid to gold leaf metals is achieve made by desired gold and mixing color other \u003cend\u003e\n\n정리된 정보:\n\n1. 핵심 정보:\n- 입력: \"\u003cstart\u003e the melting in solid to gold leaf metals is achieve made by desired gold and mixing color other \u003cend\u003e\"\n\n2. 제한 사항:\n- 문장을 의미 있게 재배열합니다.\n- 출력은 \"\u003cstart\u003e\"로 시작하고 \"\u003cend\u003e\"로 끝나야 합니다.\n\n3. 정리된 작업:\n- 주어진 혼합된 단어를 \"\u003cstart\u003e\"와 \"\u003cend\u003e\" 태그 내에서 의미 있는 문장으로 재배열하는 작업입니다. \n\n4. 파이썬 변환:\n   (선택 사항, 파이썬 태그가 파이썬용이 아닌 경우 스킵) 입력 매개변수:\n     input_sentence = \"\u003cstart\u003e the melting in solid to gold leaf metals is achieve made by desired gold and mixing color other \u003cend\u003e\"\n\n5. 응답 형식: (특정 응답 형식이 없는 경우 스킵)\n     출력 문장은 \"\u003cstart\u003e ... \u003cend\u003e\" 형식이어야 합니다\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일부 BoT를 사용하여 다시 정렬된 문장 예시:\n\n```js\n{\"input\": \"\u003cstart\u003e life plan and families to for social hospital workers outside with patients work the \u003cend\u003e\", \n\"result\": \"\u003cstart\u003e 병원 직원은 환자들과 함께 외부에서 작업하며 사회적 가족들을위한 인생 계획을 합니다 \u003cend\u003e\\n\"}\n{\"input\": \"\u003cstart\u003e yield plant refers dry total to production biological matter \u003cend\u003e\", \n\"result\": \"\u003cstart\u003e 식물 수확은 총 건조물 생물학적 생산물을 가리킵니다 \u003cend\u003e\\n\"}\n{\"input\": \"\u003cstart\u003e the bloodstream into alcohol from directly stomach goes the \u003cend\u003e\", \n\"result\": \"\u003cstart\u003e 알코올은 위에서 직접 혈류로 이동합니다 \u003cend\u003e\\n\"}\n```\n\nBoT 리포지토리가 데모 코드이므로 원본 논문에서 언급된 기능 중 몇 가지가 부족할 수 있음을 유의해주세요. 일반적인 생각 템플릿, Meta-Buffer의 동적 업데이트 또는 사용자 작업에 대한 가장 가까운 템플릿 임베딩을 찾는 기능과 같은 기능이 없습니다. 이러한 기능은 프레임워크의 중요한 측면이며, 이러한 요소가 없으면 Buffer of Thoughts의 성능을 실제로 평가할 수 없습니다.\n\n# 마지막으로\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결론적으로 BoT은 다양한 영역과 작업에서 정확성과 효율성 면에서 유망한 결과를 보여줍니다. 그것은 추론 문제를 근본적인 제약 조건과 주요 정보로 분해하고 이전 솔루션과 템플릿을 기초로 쌓아가면서 LLM이 이해할 수 있도록 작업을 더 잘 구성하는 흥미로운 접근 방법입니다.\n\n다른 프롬프트 기술의 일부 한계를 해결함으로써, Buffer of Thoughts는 LLM이 더 복잡한 사고 패턴을 가질 수 있게 해주어, 작은 경량 모델이 대형 모델 수준의 성능을 발휘할 수 있을 것으로 기대됩니다.\n\n작은 LLM이 대형 LLM에 가까운 결과를 달성할 수 있게 하는 것은 현재 많은 연구 논문에서 다루는 핵심 주제입니다. 목표는 다양한 프롬프팅 및 세밀 조정 기술을 활용하여 저량의 계산 및 비용으로 정확한 AI 출력을 얻는 것입니다.\n\nBuffer of Thoughts는 LLM을 이해와 추론 과정에서 단계별로 안내하기 위해 다양한 기술 영역을 활용하는 혁신적이고 유망한 프롬프팅 프레임워크입니다. Buffer of Thoughts 기술의 완전한 실용적 구현은 아직 이루어지지 않았지만, 그동안 데모 GitHub 저장소의 제공된 벤치마크를 테스트해 보세요. [2]\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지 읽어주셔서 감사합니다!\n\n🌟 파이썬, ML / MLOps / AI, 데이터 과학 및 LLM에 대해 배우고 있는 1000명 이상의 사람들과 함께하고 싶다면 제 X/Twitter를 팔로우해주세요. 거기서는 매일 업데이트된 소식을 받아보실 수 있습니다.\n\n읽어주셔서 감사합니다,\n\n— Hesam\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[1] Yang, L., Yu, Z., Zhang, T., Cao, S., Xu, M., Zhang, W., Gonzalez, J. E., \u0026 Cui, B. (2024). Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models. arXiv. https://arxiv.org/abs/2406.04271\n\n[2] buffer-of-thought-llm, https://github.com/YangLing0818/buffer-of-thought-llm","ogImage":{"url":"/assets/img/2024-06-19-UnderstandingBufferofThoughtsBoTReasoningwithLargeLanguageModels_0.png"},"coverImage":"/assets/img/2024-06-19-UnderstandingBufferofThoughtsBoTReasoningwithLargeLanguageModels_0.png","tag":["Tech"],"readingTime":9},{"title":"당신의 LLM에 맞는 적절한 RAG 프레임워크 선택 LlamaIndex 또는 LangChain","description":"","date":"2024-06-19 19:45","slug":"2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain","content":"\n\n이미지(`\u003cimg\u003e`) 태그를 Markdown 형식으로 변경하겠습니다.\n\n\n![Choosing the right RAG framework for your LLMLlamaIndex or LangChain](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_0.png)\n\n\n큰 언어 모델(Large Language Models, LLMs)은 현대의 주요 인공지능 기술 중 하나입니다. 2022년 11월에 OpenAI가 자체 생성 신생대화봇(Generative AI chatbot)을 공개하면서, 이런 첨단 기술들의 응용 가능성에 대한 사람들의 관심이 커졌습니다. ChatGPT의 놀라운 기능을 보고 나서 기업, 개발자, 개인들이 자신만의 맞춤형 ChatGPT 버전을 원했습니다. 이것이 Gen AI 모델 개발, 통합, 관리를 용이하게 하는 도구/프레임워크에 대한 수요 급증을 야기했습니다.\n\n시장에는 이런 수요를 채우는 두 주요 프레임워크가 있습니다: LlamaIndex와 LangChain. 하지만, 이 두 프레임워크의 목표는 개발자가 자신만의 맞춤형 LLM 응용프로그램을 만드는 데 도움을 주는 것입니다. 이들 프레임워크 각각은 고유의 장단점을 갖고 있습니다. 본 블로그 포스트의 목적은 LlamaIndex와 LangChain 사이의 주요 차이점을 드러내어 당신이 특정 용례에 맞는 적절한 프레임워크를 선택하는 데 도움을 주는 것입니다.\n\n# LlamaIndex 소개\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![LlamaIndex](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_1.png)\n\nLlamaIndex는 사용자 정의 데이터를 기반으로 한 LLM(Llama Language Model)을 색인하고 쿼리하는 프레임워크입니다. 구조화된 데이터(관계형 데이터베이스), 비구조화된 데이터(NoSQL 데이터베이스) 및 반구조화된 데이터(세일즈포스 CRM 데이터)와 같은 다양한 소스를 통해 데이터 연결을 가능케 합니다.\n\n데이터가 소유권이라고 할지라도, 최신 LLM의 이해 가능한 임베딩(embedding)으로 대규모로 색인화할 수 있습니다. 따라서 모델을 다시 교육할 필요가 사라집니다.\n\n# LlamaIndex 작동 방식은?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![RAG Framework](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_2.png)\n\n라마 인덱스는 LLM의 고급 맞춤화를 용이하게 합니다. 소유권 데이터를 활용하여 모델이 문맥 기반 응답을 점차적으로 개선할 수 있도록 메모리에 내장합니다. 라마 인덱스는 대규모 언어 모델을 도메인 지식 전문가로 전환시킵니다. 이는 AI 어시스턴트로 작동하거나 소유한 진실의 소스(예: 영업 부문에 특화된 비즈니스 정보가 포함된 PDF 문서)를 기반으로 쿼리에 응답하는 대화형 챗봇 역할을 수행할 수 있습니다.\n\n소유권 데이터에 기반한 LLM 맞춤화를 위해, 라마 인덱스는 검색 증가 생성(RAG) 기술을 사용합니다. RAG는 주로 두 가지 주요 단계로 구성됩니다.\n- 인덱싱 단계: 소유권 데이터가 효과적으로 벡터 인덱스로 변환됩니다. 이때 데이터는 벡터 임베딩이나 의미적 의미가 부여된 숫자 표현으로 변환됩니다.\n- 쿼리 단계: 시스템이 쿼리를 받을 때, 가장 높은 의미 유사성을 가진 쿼리가 정보 청크 형태로 반환됩니다. 이 정보 청크는 LLM으로 보내져 최종 응답을 얻기 위해 원본 프롬프트 쿼리와 함께 전송됩니다. 이 메커니즘을 통해 RAG는 LLM의 기본 지식만으로는 불가능한 매우 정확하고 관련성 높은 출력을 생성하는 데 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 라마지수 사용 시작하기\n\n우선, 시작하기 위해 라마-인덱스를 설치해보세요:\n\n```js\npip install llama-index \n```\n\n오픈AI의 LLM을 사용하기 위해서는 오픈AI API 키가 필요합니다. 비밀 키를 받으셨다면 .env 파일에 다음과 같이 설정해야 합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\"\n```\n\n여기서부터 LlamaIndex로 빌딩을 시작할 수 있어요! 더 많은 예제, 안내 및 사용 사례를 보려면 LlamaIndex 문서를 참조해주세요.\n\n에이전트, 지수, 쿼리 엔진 및 데이터셋과 같은 더 많은 리소스를 탐색하려면 Llama 인덱스 개발자를 위한 커뮤니티 공유 리소스/구성 요소에 액세스하기 위해 Llama 허브로 이동해주세요.\n\n# 🦙LlamaIndex로 QnA 애플리케이션 개발하기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n라마인덱스의 기능을 실시간으로 보여주기 위해, 사용자 정의 문서에 기반한 쿼리에 답변을 할 수 있는 QnA 애플리케이션을 개발하는 코드 워크스루를 진행하겠습니다.\n\n먼저, 필요한 모든 종속 항목을 설치하는 과정부터 시작해보겠습니다:\n\n```js\npip install llama-index openai nltk\n```\n\n다음으로, LlamaIndex의 SimpleDirectoryReader 함수를 사용하여 문서를 로드하고 인덱스를 구축하는 과정을 시작해봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom llama_index.core import (\n    VectorStoreIndex,\n    SimpleDirectoryReader\n )\n\n# SDR 함수 내에서 경로를 정의하고 색인을 빌드합니다\ndocuments = SimpleDirectoryReader(\"docs\").load_data()\nindex = VectorStoreIndex.from_documents(documents, show_progress=True)\n```\n\n\"docs\" 폴더 안에 Python 코스 자격 요건 PDF를 업로드했습니다:\n\n인덱스를 쿼리하고 응답을 확인합니다:\n\n```python\n# 쿼리 엔진\nquery_engine = index.as_query_engine()\n\nresponse = query_engine.query(\"과제와 프로젝트를 놓치면 성적과 백분율이 어떻게 될까요?\")\nprint(response)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Choosing the right RAG framework for your LLMLlamaIndex or LangChain](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_3.png)\n\n검색 엔진은 데이터 인덱스를 검색하여 관련 조각을 가져올 것입니다.\n\n또한 이 질의 엔진을 기억하면서 채팅 엔진으로 변환할 수도 있습니다. 함수를 수정하여 다음과 같이:\n\n```js\nchat_engine = index.as_chat_engine()\nresponse = chat_engine.chat(\"만약 제 숙제와 프로젝트를 놓치면, 어떤 등급과 퍼센티지를 받게 될까요?\")\nprint(response)\nfollow_up = chat_engine.chat(\"그리고 프로젝트만 놓친다면, 어떤 등급을 받을까요?\")\nprint(follow_up)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n매번 인덱스를 다시 빌드하지 않도록 하려면, 디스크에 저장할 수 있어요:\n\n```js\nindex.storage_context.persist()\n```\n\n그리고 나중에 다시 불러올 수 있어요:\n\n```js\nfrom llama_index.core import (\n    StorageContext,\n    load_index_from_storage,\n)\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\nindex = load_index_from_storage(storage_context)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에 표시된 코드는 Hugging Face Spaces에 배포된 QnA 챗봇 Gradio 애플리케이션의 일부입니다. 소스 코드와 데이터셋은 여기에서 사용할 수 있습니다.\n\n# LangChain 소개\n\n![이미지](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_4.png)\n\nLangChain은 사용자 지정 데이터 소스를 기반으로 맞춤형 LLMs(언어 생성 모델)를 구축하는 데 사용되는 또 다른 프레임워크입니다. LangChain은 관계형 데이터베이스(예: 테이블 데이터), 비관계형 데이터베이스(예: 문서), 프로그래밍 소스(예: API) 또는 심지어 사용자 정의 지식 베이스와 같은 다양한 소스로부터 데이터를 연결할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLangChain은 단순히 다른 통합 도구들과 함께 LLMs에 전송되는 요청의 연속인 체인을 형성하는 메커니즘을 활용합니다. 각 단계의 출력물이 다음 입력으로 전달되어 체인이 형성됩니다.\n\nLangChain은 귀사의 독점 데이터와 함께 작동하며, 관련 콘텍스트가 LLMs에 제공되어 적절한 응답을 생성합니다. 회사 데이터용 맞춤형 QNA 챗봇, 내부 분석 도구, 또는 귀사 데이터 소스와 함께 작동하는 AI 보조 프로그램이든, LangChain을 통해 다양한 도구를 통합하고 여러 LLM 응용 프로그램을 체인으로 연결하여 더욱 포괄적인 시스템을 구축할 수 있습니다.\n\n# LangChain 작동 방식\n\n![이미지](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLangChain은 다음과 같은 구성 요소를 가지고 있습니다:\n\n- \"Prompts\"는 원하는 출력/응답을 얻기 위해 모델에게 제공하는 암시입니다.\n- LangChain은 사용자에게 Language 모델을 쉽게 교체할 수 있는 인터페이스를 제공합니다. LangChain은 GPT-4, Gemini 1.5 Pro, Hugging Face LLM, Claude 3와 같은 최신 LLM들과 작업할 수 있도록 해줍니다.\n- LangChain은 임베딩, 인메모리 벡터 저장소 등과 같은 색인 기술을 활용합니다.\n- LangChain은 다양한 구성 요소를 연결하는 것을 용이하게 합니다.\n- LangChain은 사용자가 작업 및 도구를 할당하는 데 도움이 되는 다양한 AI 에이전트를 제공합니다.\n\n# LangChain으로 시작하기\n\nLangChain을 사용하여 빌드를 시작하는 첫 번째 단계는 LangChain 패키지를 설치하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\npip install langchain\n```\n\nLangChain 튜토리얼을 위해 cohere API 키를 사용할 것입니다. .env 파일 내에 API 키를 넣어 cohere 환경 변수를 설정해 주세요:\n\n```js\nimport os\nos.environ[\"cohere_apikey\"] = \"여러분의_API_키_여기에_입력\"\n```\n\n이후에는 LangChain을 활용하여 개발을 시작할 수 있습니다! 더 자세한 예제, 가이드 및 사용 사례는 LangChain 문서를 참조해 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLangChain 생태계를 탐험하고 싶다면 Langchain Hub로 이동하여 LLM 작업 흐름에 통합할 수 있는 개발자 커뮤니티와 데이터 커넥터, 도구 및 프레임워크를 찾을 수 있어요.\n\n# 🦜🔗LangChain으로 QnA 애플리케이션 만들기\n\nLangchain의 능력을 실시간으로 시연하기 위해 사용자 정의 문서를 기반으로 질문에 답변할 수 있는 QnA 애플리케이션을 개발하는 코드 워크스루를 진행할 거에요.\n\n첫 번째 단계는 모든 종속성을 설치하는 것이에요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\npip install langchain cohere chromadb ## openai의 LLM 대신 cohere를 사용하겠습니다.\n```\n\n그 후 문서 데이터를로드하고 색인을 생성합니다. 또한 cohere embeddings를 사용하여 임베딩을 생성할 것입니다:\n\n```js\nfrom langchain.document_loaders import OnlinePDFLoader\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import CohereEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nloader = OnlinePDFLoader(document)\ndocuments = loader.load()\n\n# 텍스트 분할기 초기화\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=chunksize, chunk_overlap=10,\n   separators=[\" \", \",\", \"\\n\"])\n\n# Cohere 임베딩 초기화\nembeddings = CohereEmbeddings(model=\"large\", cohere_api_key=st.secrets[\"cohere_apikey\"])\n\ntexts = text_splitter.split_documents(documents)\nglobal db\ndb = Chroma.from_documents(texts, embeddings)\nretriever = db.as_retriever()\nglobal qa\n```\n\n색인을 쿼리하고 응답을 확인합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nquery = \"다른 국가들과 인도 헌법의 세속주의 접근 방식을 비교해보세요?\"\nresult = db.query(query)\nprint(result)\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_6.png\" /\u003e\n\n쿼리는 의미론적으로 데이터를 검색하고 적절한 답변을 검색합니다.\n\nRetrievalQA 모듈을 사용하여 체인을 할 수 있습니다. 여기서는 cohere의 LLM을 사용할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom langchain.llms import Cohere\nfrom langchain.chains import RetrievalQA\n\nqa = RetrievalQA.from_chain_type(\n    llm=Cohere(\n        model=\"command-xlarge-nightly\",\n        temperature=temp_r,\n        cohere_api_key=st.secrets[\"cohere_apikey\"],\n    ),\n    chain_type=\"stuff\",\n    retriever=retriever,\n    return_source_documents=True,\n    chain_type_kwargs=chain_type_kwargs,\n)\n```\n\n위에 표시된 코드는 QnA Streamlit 애플리케이션의 일부입니다. 소스 코드와 데이터 세트는 여기에서 사용할 수 있습니다.\n\n# 람마인덱스(LlamaIndex) vs 랑체인(LangChain)의 최상의 사용 사례\n\n람마인덱스(LlamaIndex):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 특정 지식 베이스를 가진 쿼리 및 검색 기반 정보 검색 시스템 구축.\n- 사용자 쿼리에 대답으로 관련 정보 청크만 제공할 수 있는 QnA 챗봇 개발.\n- 대규모 문서의 요약, 텍스트 완성, 언어 번역 등\n\nLangChain:\n\n- 엔드 투 엔드 대화형 챗봇 및 AI 에이전트 구축\n- LLMs에 사용자 정의 워크플로 통합\n- API 및 기타 데이터 소스를 통해 LLMs의 데이터 연결 옵션 확장\n\nLangchain과 LlamaIndex의 결합된 사용 사례: (\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_7.png)\n\n- 전문가 AI 에이전트 구축: LangChain은 여러 데이터 원본을 통합하고 LlamaIndex는 유사 의미 검색 능력으로 빠른 응답을 생성하고 정리할 수 있습니다.\n- 고급 R\u0026D 도구: LangChain의 체이닝을 사용하여 도구와 워크플로우를 동기화하고, LlamaIndex를 사용하여 더 맥락을 이해할 수 있는 LLM을 생성하고 가장 관련성 높은 응답을 얻을 수 있습니다.\n\n# LlamaIndex 대 LangChain: 적절한 프레임워크 선택\n\n![이미지](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_8.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 중요한 몇 가지 질문이 있습니다. 적절한 프레임워크를 선택하기 전에 다음을 고려해 보세요:\n\n- 프로젝트 요구 사항은 무엇인가요? 기본적으로 인덱스, 쿼리 검색 및 검색 애플리케이션을 위해서는 LlamaIndex를 선택할 수 있습니다. 그러나 사용자 지정 워크플로를 통합해야 하는 애플리케이션의 경우 LangChain이 더 나은 선택일 수 있습니다.\n- 사용하기 쉽고 접근하기 쉬운가요? LlamaIndex는 더 간단한 인터페이스를 제공하지만, LangChain은 NLP 개념과 구성 요소에 대한 심층적인 이해가 필요합니다.\n- 얼마나 많은 사용자 정의를 원하시나요? LangChain은 쉬운 사용자 정의와 도구 통합을 가능케 하는 모듈식 디자인을 제공합니다. 그러나 LlamaIndex는 주로 검색 및 검색 기반 프레임워크입니다.\n\n![이미지](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_9.png)\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLlamaIndex과 LangChain은 사용자 정의 LLM 기반 응용 프로그램을 개발하려는 개발자들에게 매우 유용한 프레임워크입니다. LlamaIndex의 USP는 우수한 검색 및 검색 기능을 제공하여 사용자에게 간소화된 인덱싱 및 쿼리 솔루션을 제공합니다. 반면 LangChain의 USP는 모듈식 설계 및 LLM 영역의 다양한 도구 및 구성 요소와의 통합 가능성에 있습니다.\n\n둘 중에 선택하는 데 고민이 되면 프로젝트 요구 사항이 무엇인지, 사용하기 쉽고 접근성이 어떤지, 얼마나 많은 사용자 정의를 원하는지와 같은 질문을 스스로에게 한 번 던져 보세요.\n\nLangChain은 보다 넓은 프레임워크 내에서 여러 도구를 사용하고 싶다면 최적입니다. 예를 들어, 다중 작업이 가능한 AI 기반 지능형 에이전트와 같은 경우입니다.\n\n그러나 만약 스마트 검색 및 검색 시스템을 구축하는 게 목표라면, LlamaIndex를 선택해보세요. LlamaIndex의 강점은 정보의 색인 및 검색에 있어서, LLM을 위한 깊은 데이터 탐색기 앱을 구축하는 것이 가능해집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nFinally remember that it’s not a classic case of either or, in the real world, you can implement a system whose architecture may contain both the frameworks, each playing their own unique roles.\n\n# FAQs\n\nQ1: How do LlamaIndex and LangChain differ in their primary focus?\n\nA1: LangChain’s main focus is the development \u0026 deployment of LLMs, along with the customization of LLMs using fine-tuning methods. However, LlamaIndex aims to provide an end-to-end ML workflow, along with data management \u0026 model evaluation.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nQ2: 기계 학습 초보자에게 더 나은 플랫폼은 무엇인가요?\n\nA2: LlamaIndex가 구현이 간단하고 직관적이기 때문에 초보자에게 더 선호됩니다. 반면에 LangChain은 LLM 및 NLP 개념에 대해 더 심층적인 이해가 필요합니다.\n\nQ3: LlamaIndex와 LangChain을 함께 사용할 수 있나요?\n\nA3: 네, 두 플랫폼의 강점을 결합하여 사용 사례에 대한 솔루션을 개발하는 것이 가능합니다. LlamaIndex는 데이터 전처리 및 초기 모델 훈련 단계를 담당하고, LangChain은 LLM의 세밀한 조정, 도구 통합, 및 배포를 용이하게 해 줄 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nQ4: 내 맞춤형 LLM 애플리케이션에 사용할 프레임워크는 무엇이 좋을까요?\n\nA4: LangChain은 자연어 처리 작업 및 외부 데이터와의 복잡한 상호작용에 의존하는 사용 사례에 유리하며, 텍스트 요약, 감성 분석, 대화형 AI 봇 등과 같이 고급 언어 모델 기능이 필요한 애플리케이션에 적합합니다. 반면, LlamaIndex는 외부 데이터와의 일반 상호작용이 필요한 작업에 더 유리합니다(빠른 데이터 조회 및 검색과 같은 질의 응답 챗봇).\n\nQ5: LlamaIndex 또는 LangChain 사용 시 제한 사항이 있나요?\n\nA5: LlamaIndex는 고도로 전문화된 NLP 작업에는 적합하지 않습니다. 반면, LangChain은 고급 언어 모델 기능이 실제로 필요하지 않은 기계 학습 워크플로우를 해결하는 데 과도할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 자원\n\n- kyrolabs/awesome-langchain: 😎 멋진 LangChain 프레임워크 관련 도구 및 프로젝트의 멋진 목록 (github.com)\n- LLamaIndex와 함께하는 멋진 프로젝트들 (github.com)\n- [LangChain과 LLamaIndex의 4가지 작업 비교하기](https://lmy.medium.com/comparing-langchain-and-llamaindex-with-4-tasks-2970140edf33)\n- Langchain 플레이리스트\n- LLamaIndex 플레이리스트\n\n![이미지](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_10.png)\n\n이 이야기는 Generative AI에 게시되었습니다. LinkedIn에서 저희와 연락하고 최신 AI 이야기를 받아보려면 Zeniteq를 팔로우하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최신 generative AI 뉴스 및 업데이트를 받아보려면 저희 뉴스레터를 구독해주세요. 함께 AI의 미래를 함께 만들어요!\n\n![참조 이미지](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_11.png)","ogImage":{"url":"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_0.png"},"coverImage":"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_0.png","tag":["Tech"],"readingTime":12},{"title":"랭체인 대 람마 인덱스 여러분의 창조적 AI 프로젝트에 딱 맞는 것 찾기","description":"","date":"2024-06-19 19:43","slug":"2024-06-19-LangChainvsLlamaIndexFindingthePerfectFitforYourGenerativeAIProjects","content":"\n\nLangChain과 LlamaIndex는 최신 대형 언어 모델 (LLM) 애플리케이션을 구축하는 데 요즘 널리 사용되는 Python 프레임워크입니다. LangChain은 데이터와 LLM 간의 다리 역할을 합니다. 마찬가지로 LlamaIndex도 데이터를 대형 언어 모델에 연결하는 방법으로 LangChain과는 다른 방식으로 작동합니다. 이러한 프레임워크는 프로토타입부터 제품 생산까지 생성형 인공 지능 애플리케이션을 구축하는 데 필요한 도구를 제공합니다.\n\n우리는 이러한 프레임워크 각각을 자세히 살펴보고 생성형 AI 애플리케이션을 구축하는 데 특정 특징들에 대해 논의할 것입니다. 그 전에 대형 언어 모델에 대한 간단한 소개를 해보겠습니다.\n\n# 대형 언어 모델 (LLMs)\n\n대형 언어 모델 (LLMs)은 요즘 AI 애플리케이션의 최전선에 있습니다. 이들은 방대한 텍스트 및 코드 데이터셋에서 훈련된 복잡한 생성형 AI 모델입니다. 주로 텍스트 생성, 텍스트 이해 및 언어 번역에 사용됩니다. 그들은 또한 코드를 이해하고 생성할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인기 있는 LLM의 예시는 다음과 같습니다:\n\n- OpenAI의 GPT-4\n- Meta의 Llama3\n- Google의 Gemini\n\n이들은 기본적으로 트랜스포머 기반 모델로, 입력을 받아 인간과 유사한 텍스트를 생성하기 위해 자기 주의 메커니즘을 사용합니다. 최근 이러한 모델에 다중 모달성을 도입하는 작업이 많이 이루어지고 있습니다. 예를 들어, 글을 작성하는 시점에서 가장 최근의 모델인 GPT-4o는 오디오, 비전, 그리고 텍스트 과정을 실시간으로 추론할 수 있는 주요 모델입니다.\n\n# LangChain이 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-LangChainvsLlamaIndexFindingthePerfectFitforYourGenerativeAIProjects_0.png\" /\u003e\n\nLangChain은 대형 언어 모델 (LLM)을 활용한 응용 프로그램을 개발하기 위한 프레임워크입니다. 이 프레임워크는 복잡한 생성형 AI 응용 프로그램을 만드는 과정을 간편화하는 데 도움이 되도록 설계되었습니다. 모듈식 아키텍처를 갖춘 이 프레임워크를 이용하면 개발자들은 특정 사용 사례에 맞게 맞춤형 솔루션을 만들 수 있습니다. LangChain은 LLM 응용 프로그램 생명주기의 각 단계를 개발에서 제품화, 배포까지 간단하게 만들어 줍니다.\n\nLangChain의 체인 개념은 LLM, 도구 또는 데이터 전처리 단계에 대한 호출 시퀀스를 의미합니다. 체인을 사용하면 여러 LLM 프롬프트나 작업을 연결하여 복잡한 다단계 상호작용이나 워크플로를 생성할 수 있습니다. LangChain은 프롬프트를 연결하고 컨텍스트를 처리하며 여러 상호작용에서 상태를 관리하는 데 필요한 도구를 제공합니다. 이를 통해 개발자는 다양한 구성 요소를 통합하여 사용자 지정 워크플로를 만들 수 있습니다.\n\nLangChain을 사용하여 다음을 구축할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 질문 응답 챗봇\n- 가상 어시스턴트\n- 고객 지원 시스템\n- 텍스트 요약기\n- 코드 생성기\n- 창의적인 글쓰기 블로그\n- 그리고 대화 문맥을 유지하고 상호 작용을 관리하는 다양한 복잡한 애플리케이션들.\n\n# 람마인덱스(LlamaIndex)란 무엇인가요?\n\n![람마인덱스](/assets/img/2024-06-19-LangChainvsLlamaIndexFindingthePerfectFitforYourGenerativeAIProjects_1.png)\n\n람마인덱스는 개인 또는 도메인별 데이터 위에 LLM(Language Model)을 적용하는 컨텍스트 증강된 LLM 애플리케이션을 구축하기 위한 프레임워크입니다. 이는 분산 코퍼스로부터 정보의 효율적인 검색 및 검색을 가능하게 하는 대규모 언어 모델 인덱스입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLlamaIndex는 대규모 데이터셋에 빠른 액세스가 필요한 애플리케이션에 특히 적합하여, 검색 엔진, 추천 시스템 및 데이터 중심 애플리케이션에 가치 있는 도구입니다.\n\nLlamaIndex를 사용하여 다음과 같은 것들을 구축할 수 있습니다:\n\n- 질문-답변 챗봇\n- 문서 요약기\n- 자율적인 연구 에이전트\n- LangChain을 사용하여 구축할 수 있는 기타 애플리케이션들\n\n# LangChain 대 LlamaIndex\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 LangChain과 LlamaIndex 간의 주요 차이점입니다:\n\n- LangChain은 다양한 LLM 응용 프로그램을 구축하는 데 사용할 수 있는 일반 목적의 프레임워크이며, 반면에 LlamaIndex는 검색 및 검색 응용 프로그램을 구축하기 위해 특별히 설계된 가벼운 도구입니다.\n- LlamaIndex는 원하는 작업을 더 적은 코드 라인으로 수행할 수 있도록 더 간단한 인터페이스를 갖추고 있습니다. 반면에 LangChain은 더 유연한 인터페이스를 갖고 있습니다. 데이터 로더, 분할기 및 각각에 대한 많은 옵션이 있습니다. 예를 들어 pdf, txt, csv, excel 및 코드용 별도의 로더가 있어서 다양한 유형의 데이터를 처리하는 데 다재다능합니다.\n- LangChain은 FAISS, Pinecone, Chroma 등과 같은 다양한 벡터 스토어와 통합될 수 있지만 LlamaIndex는 내장형 벡터 스토어를 가지고 있습니다. 마찬가지로 LlamaIndex는 기본적으로 OpenAI 임베딩을 사용하며 다른 임베딩을 사용하려면 아마도 LangChain을 사용할 것입니다.\n- LangChain은 데이터 로드, 처리, 분할 및 색인화 도구뿐만 아니라 LLM과의 상호 작용을 위한 도구를 제공하기 때문에 사용하기 위해 일정 수준의 이해가 필요합니다. 반면에 LlamaIndex는 많은 것을 이미 구현했기 때문에 코드 라인이 적은 사용이 조금 더 쉽습니다.\n- LangChain은 사용자 요구에 맞게 사용자 정의할 수 있지만 LlamaIndex에는 일정한 제한 사항이 있습니다. 당연히 사용 편의성을 고려하면 LlamaIndex는 제한 사항이 있기 때문에 사용자 정의가 덜 되며 따라서 LangChain이 LlamaIndex보다 유연합니다.\n- LlamaIndex는 데이터 처리량이 많은 응용 프로그램에 적합한 매우 효율적이며, 반면에 LangChain은 LlamaIndex보다 효율성이 떨어지며 사용자 정의가 필요한 응용 프로그램에 더 적합합니다.\n- LangChain은 더 많은 유연성과 사용자 정의를 제공하기 때문에 더 가파른 학습 곡선을 가지고 있습니다. 복잡한 워크플로우를 구축하기 위해 컴포넌트 간의 상호 작용과 아키텍처에 대한 이해가 중요합니다. 반면에 LlamaIndex는 간단한 인터페이스 때문에 개발자들이 쉽게 시작할 수 있습니다.\n\n# 장단점\n\n## LangChain 장점:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- LangChain은 복잡하고 맥락에 맞는 애플리케이션 개발을 용이하게 합니다.\n- 프롬프트를 연결하고 상태를 관리하는 프로세스를 단순화합니다.\n- 외부 도구 및 API와 쉽게 통합됩니다.\n\n## LangChain 단점:\n\n- 간단한 애플리케이션이나 복잡한 상호 작용이 필요 없는 경우에는 LangChain이 과도하게 사용될 수 있습니다.\n- 상태와 컨텍스트 관리에 익숙하지 않은 개발자에게는 학습 곡선이 가파를 수 있습니다.\n\n## LlamaIndex 장점:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- LlamaIndex는 빠르고 효율적인 데이터 검색을 위해 최적화되어 있습니다.\n- 대규모 데이터 세트와도 잘 호환되며 높은 성능을 유지합니다.\n- 데이터를 관리하고 구조화하는 강력한 도구를 제공합니다.\n\n## LlamaIndex 단점:\n\n- LlamaIndex는 주로 데이터 색인 및 검색에 중점을 둔 기능으로, 복잡한 상태 관리가 필요한 애플리케이션에는 적합하지 않을 수 있습니다.\n- 특정 사용 사례에서 최적의 성능을 위해 상당한 설정과 구성이 필요할 수 있습니다.\n\n# 프로젝트에 적합한 선택을 만드는 방법\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLangChain과 LlamaIndex 중 어느 것을 선택할지는 당신이 진행 중인 프로젝트의 요구 사항에 달려 있어요. 각 프레임워크는 고려해야 할 독특한 장단점을 가지고 있으므로 선택하기 전에 고려해야 해요.\n\n대략적인 지침으로 말하면, 효율성을 원하며 사용자 정의 없이 실시간 검색 애플리케이션을 구축하길 원한다면 LlamaIndex를 고려해 보세요. 특정 사용자 정의가 필요하고 효율성에 조금 타협을 할 수 있으며 다양한 LLM을 지원받을 경우, LangChain을 사용해 보세요. 두 개를 모두 프로젝트에서 사용할 수 있을까요? 왜 안 될까요? LlamaIndex의 효율성과 LangChain의 사용자 정의 기능을 활용하여 더 강력한 생성적 AI 애플리케이션을 만들 수 있습니다.\n\n# 결론\n\nLangChain과 LlamaIndex는 대규모 언어 모델을 기반으로한 애플리케이션을 구축하기 위한 강력한 프레임워크입니다. LangChain은 여러 LLM 애플리케이션에 대한 더 일반적이고 유연한 프레임워크이며, LlamaIndex는 특히 검색 및 검색 애플리케이션을 위해 구축된 좀 더 효율적인 프레임워크입니다. 대부분의 애플리케이션은 이러한 프레임워크 중 하나로 구축할 수 있습니다. 적절한 프레임워크를 선택하는 것은 당신의 프로젝트 요구 사항에 달려 있습니다. 효율적인 검색 및 검색이 필요하면 LlamaIndex를 고려해 보세요. 반면에 특정 사용 사례에 대한 사용자 정의가 필요하다면 LangChain을 고려해 보세요. 각 프레임워크의 장단점을 이해하면 당신의 애플리케이션 요구 사항에 가장 잘 맞는 결정을 내릴 수 있을 거에요.","ogImage":{"url":"/assets/img/2024-06-19-LangChainvsLlamaIndexFindingthePerfectFitforYourGenerativeAIProjects_0.png"},"coverImage":"/assets/img/2024-06-19-LangChainvsLlamaIndexFindingthePerfectFitforYourGenerativeAIProjects_0.png","tag":["Tech"],"readingTime":5},{"title":"LLM 여행 개념 증명부터 제품화까지","description":"","date":"2024-06-19 19:40","slug":"2024-06-19-AnLLMJourneyFromPOCtoProduction","content":"\n\n\n![LLM Journey](/assets/img/2024-06-19-AnLLMJourneyFromPOCtoProduction_0.png)\n\n이렇게 상상해 봐: LLM(대형 언어 모델)을 사용하여 실행할 수 있는 멋진 프로젝트 아이디어가 있다고 상상해 봐. 빠르게 작업 가능한 개념 증명(PoC)에 도달했다. 네 자신에게 자랑스러워하며 실제로 동작하는 데 필요한 작업이 얼마나 적은지에 놀라게 될 것이다. (5줄의 프롬프트의 마법 ☺)\n\n하지만 이제 어떻게 해야 할까?\n\nLLM을 사용할 때 POC를 작성하는 것은 쉽지만, 실제로 탄생 가능한 제품을 만드는 것은 어려운 작업임을 빨리 깨닫게 될 것이다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 이 상황에 공감한다면, 이 게시물이 흥미로울 수 있습니다.\n\n# LLM 여정의 시작\n\n이 여정을 이해하는 가장 좋은 방법은 현재 진행 중인 LLM 프로젝트 중 하나를 살펴보는 것입니다. 여정의 첫 부분은 세 단계로 나눌 수 있습니다.\n\n## 동기 부여 찾기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n복잡한 제품들과 방대한 양의 정보들이 있는 세상에서, 우리 고객들은 종종 길을 잃기도 합니다. 때로 제품에서 기본 작업을 수행하려면 많은 문서 페이지를 읽고, 제품의 UI 페이지를 탐색하거나 로그, 보고서 및 기타 원시 자료를 분석해야 하는 경우도 있습니다.\n\n고객이 자주 묻는 질문 중 하나는 \"내가 원하는 대로 내 말로 말하기만 하면 시스템이 해결해주지 않을까?\" 입니다.\n\n답은 LLM을 사용하면 가능합니다!\n\n## 목표 선언\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 경우 프로젝트 목표는 고객이 제공한 자연어 (NL)를 사용하여 제품에 대한 작업을 수행하는 것이었습니다.\n\n먼저 우리 프로젝트는 제품의 API 명세 파일을 학습합니다 (보통 OpenAPI와 같은 표준 형식으로 API를 선언하는 파일) 그런 다음 LLM을 사용하여 NL 요청을 올바른 API로 변환합니다.\n\n## 작동하는 개념 증명 (POC)에 도달\n\n프로젝트 목표를 선언한 후 POC 단계에 도달합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPOC(Port of Call)의 목적은 우리가 가진 아이디어를 실제로 실행할 수 있는지 확인하는 것입니다.\n\n우리는 API 명세 파일과 자연어로 된 사용자 요청을 함께 입력으로 받아 사용자의 요청을 실행하는 기본 시스템을 구축해야 합니다.\n\n이를 위해, 우리는 우리의 LL(Large Language)를 OpenAI GPT로 선택하고, LLM 사용을 래핑하는 라이브러리로서 LangChain을 선택했습니다.\n\n우리는 입력 데이터를 받아 논리적 그룹(서비스)으로 처리하고, 사용자의 요청과 수행 방법에 대한 몇 가지 지침이 포함된 프롬프트와 함께 LangChain 라이브러리로 로드하는 엔진을 작성했습니다. 우리는 LangChain 체인, 도구 및 에이전트를 사용하여 이 작업을 수행했으며, OpenAI의 기능 호출 기능도 사용했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLangChain 도구 및 에이전트에 대한 자세한 내용은 여기에서 확인할 수 있어요.\n\n아래 다이어그램은 저희 POC의 주요 참가자들을 설명하고 있어요.\n\n축하해요, 작동하는 POC가 있어요! 이제 실제 여정이 시작돼요...\n\n# 먼저 해야 할 일: 정확성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최초의 POC 작업에 대한 설령 기대가 식어가기 시작했다 해도, LLM 결정 및 응답에서 일부 결함을 발견하기 시작했습니다.\n\n소프트웨어 산업에서는 종종 결정론적 알고리즘을 사용합니다 (즉, 동일한 입력에 대해 알고리즘은 항상 동일한 출력을 생성합니다).\n\n내 첫 번째 조언은 익숙한 결정론적 예상을 버리는 것입니다. 네, 심지어 OpenAI 온도가 0이더라도요.\n\n목표는 이 새로운 비결정론적 세계에서 어떻게 탐색할지를 배우는 것입니다. 다시 말해, 어떻게 더 예측 가능하게 만들고, 어떻게 다양한 응답을 처리할 수 있는지에 대해 알아보는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 팁 몇 가지 있어요:\n\n## 팁 #1: 올바른 프롬프트를 사용하고 있는지 확인하세요\n\nLLM에게 해야 할 일을 단계별로 설명해야 해요. 입력 내용, 형식, 의미, 예상 출력물, 형식, 의미를 설명해야 해요. 때로는 당신이 원했던 것보다 많은 프롬프트 라인을 사용해야 할 때도 있고, 예시를 포함해야 할 때도 있어요.\n\n아래 Few-Shots Learning 예제에서 프롬프트에 사용 사례를 추가하는 장점을 볼 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 요청을 받으셨군요: \"사용자 'user1'의 사용자 세부 정보를 가져와 해당 세부 정보로 새 사용자를 생성하세요\".\n\n상당히 기본적이고 명확한 요청이죠? 그렇다고 생각했다면, 틀렸어요! LLM이 한 순환 그 구체적인 단어 \"those\"를 새 사용자 세부 정보에 넣으려는 기이한 행동을 믿을 수 없을 겁니다. 대신 위 요청의 예시를 팁으로 제시했더니 정상적으로 작동했어요:\n\n## 팁 #2: 당신의 LLM에게 도구를 제공하세요\n\n가끔은 LLM이 스스로 올바른 조치에 대한 도달 방법을 모를 때가 있습니다. 이런 경우 LLM에게 하나의 툴 세트를 제공하여 LLM이 무엇을 해야 하는지 확신이 없을 때 사용할 수 있습니다. 이런 것은 LangChain 도구나 OpenAI 함수 호출로 쉽게 처리할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여러분이 사용할 수 있는 다양한 도구 예시를 드리겠습니다:\n\n- 날짜 도구는 날짜를 계산하는 데 도움을 줍니다. 예를 들어 \"어제의 모든 로그를 나에게 보여줘\" 라고 물어볼 수 있습니다. '어제'가 무엇인지 이해하는 데 어려움이 있을 수 있으므로, 날짜 도구를 사용하여 '어제'를 사용할 수 있는 타임스탬프로 변환할 수 있습니다.\n- 사용자/인간 도구는 사용자로부터 명확한 설명을 얻기 위해 사용됩니다. 예를 들어 \"새로운 사용자를 생성해줘\" 라고 요청할 수 있습니다. LLM이 추가 정보(예를 들어 사용자 이름)가 필요할 때 사용자 도구를 사용하여 사용자에게 원하는 사용자 이름이 무엇인지 물어볼 수 있습니다.\n\n## 팁 #3: LLM의 창의성 제한하기\n\n다른 제안으로, LLM의 창의성을 제한하여 LLM이 모르는 것들을 추측하는 대신에 명확한 설명을 요청하도록 하는 것이 좋습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, \"사용자 이름이 'user1'인 새 사용자를 생성하라\"고 요청할 때, API의 매개변수 중 하나는 암호입니다.\n\nLLM은 Password123이라는 생성된 비밀번호로 사용자를 만들려고 할 것입니다. 아마 원하는 바가 아닐 겁니다. 당신은 LLM에게 해당 사용 사례에서 추측하는 대신 명확히 설명하도록 요청할 수 있습니다.\n\n# 언제나 예기치 못한 것을 기대하세요\n\n이제 우리의 모든 사용 사례를 지원하고 \"happy path\"에서 매우 잘 수행하는 엔진을 만들었습니다. 그런데 오류 흐름은 어떠한가요? 예외적인 경우는 어떠한가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 처리 방법에 대한 몇 가지 아이디어가 있습니다:\n\n- 예상치 못한 응답 처리 방법을 LLM에게 알려주세요: LLM에게 오류가 발생할 수 있다고 설명하고, 오류를 식별하는 방법과 오류 발생 시 어떻게 대처해야 하는지 알려주세요.\n- 오류 발생 시 사용자에게 명확한 메시지를 제공하여 사용자가 무슨 일이 일어났는지 이해하고 어떻게 해결해야 하는지 알 수 있도록 해주세요.\n- 자동 복구 동작 정의: 일부 경우, LLM은 장애로부터 자동으로 회복할 수 있습니다.\n\n## 예시 #1: 오류 자동 수정\n\n“'user1'이라는 사용자를 생성하세요.”\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM(링크드리스트 매니저)는 이 사용자 생성을 요청할 수 있습니다. 사용자에 대한 자동 생성 설명은 \"이는 관리자 사용자입니다!\"와 같이 생성됩니다. 이 경우, 제품이 '!'의 사용이 불법적이라는 오류를 반환할 수 있습니다. 오직 알파벳과 숫자 값만 허용된다는 에러 메시지를 읽고, 유효하지 않은 문자를 포함하지 않는 설명을 사용하여 자동으로 수정하도록 LLM에 지시할 수 있습니다.\n\n## 예시 #2: 인증 문제 우회\n\n\"'user1'이라는 사용자를 가져오세요.\"\n\n제품 시스템에 인증하려는 우리가 사용하는 토큰이 만료되었을 수 있습니다. 이 경우, 제품이 \"만료된 토큰\"이라는 오류를 반환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 LLM에게 인증 문제를 처리하고 특별한 메시지를 반환하도록 지시할 수 있어서 토큰을 자동으로 새로 고치고 요청을 다시 시도할 수 있습니다.\n\n# 배포, 호스팅 및 주요 결정\n\n엔진을 구축하고 모델을 세밀하게 조정한 후 배포에 대한 결정을 내야 합니다. 다양한 장단점이 있는 많은 선택 사항 중 몇 가지 중요한 결정 사항에는 다음이 포함됩니다:\n\n## 엔진을 호스팅하는 위치\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 우리에게 아주 쉬운 결정이었습니다. SaaS를 지원하고 있기 때문에 이것이 우리의 최상의 선택입니다.\n\n우리의 주요 클라우드 제공업체가 AWS이기 때문에 이것 또한 우리의 최상의 선택이었습니다.\n\n## LLM 모델을 호스팅할 위치\n\n우리는 기능 호출 기능을 적극 활용하고 있는 OpenAI GPT 모델을 사용했는데, 이 모델이 최상의 결과를 제공했습니다. 일부 비교를 거친 후, 우리는 이 모델을 호스팅하는 가장 좋은 방법은 Azure OpenAI를 사용하는 것이라는 결론에 도달했습니다. 따라서 이 프로젝트는 두 개의 클라우드 제공업체를 기반으로 구축되었습니다 — 하지만 이는 전혀 문제가 되지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 배포 전략 선택\n\n저는 상태를 가지지 않고 서버리스 솔루션을 선호하는 입장입니다.\n\n이겢이 정적인, 상태를 가지는 기계에 호스팅되지 않는다는 뜻은 아닙니다. 하지만 저는 탄력성, 확장성, 배포 및 관리 측면에서 서버리스와 상태를 가지지 않는 것의 장점이 경쟁하기 너무 어렵다고 믿습니다.\n\n이것이 우리가 다음 도전에 직면하게 되는 이유입니다...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## LLM 엔진을 무상태(Stateless) 및 서버리스(Serverless)로 만들기\n\nLLM 엔진을 작성할 때, \"세션\" 또는 \"대화\"라는 개념에 마주치게 될 것입니다. 이 경우, 이 세션/대화의 상태를 필요할 때 로드될 수 있는 외부 위치로 추출해야 합니다. 외부 위치는 분산 캐시 또는 데이터베이스일 수 있으며, 여러 엔진 워커에서 액세스할 수 있습니다.\n\n세션 또는 대화의 상태를 추출하는 단계는 다음과 같습니다.\n\nStep 1) 세션 또는 대화 기록을 추출합니다. 과거 메시지를 고려하도록 LLM에게 전달하려면 대화 기록을 전달해야 합니다. 대화 기록은 별도의 위치에 유지되어 있어야 하며, 필요할 때 로드되어 프롬프트 내에 포함되어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nStep 2) LangChain 상태를 추출하세요. LangChain 라이브러리는 에이전트와 도구를 함께 사용하여 설계상 상태를 유지합니다. 메모리에 여러 가지를 보관하여 계속 진행하는 데 도움을 주거나, 현재까지 무슨 일이 있었는지 등의 힌트를 제공합니다. 우리가 마주한 실제 예시 중 하나는 \"사용자 명확화 도구\"를 사용하는 것입니다: LangChain이 이 도구를 사용하기로 결정하면 명확화를 위해 나가고, 그 후에 정확히 같은 위치에서 정확히 같은 상태로 계속해야 합니다.\n\n이 문제를 해결하기 위해 LangChain 코드를 심층적으로 분석하고, 일부 부분을 다시 작성하여 상태를 외부 위치로 직렬화하고 역직렬화해야 했습니다.\n\n이것은 가장 쉬운 일은 아니었지만, 좋은 소식은 가능하다는 것입니다!\n\nStep 3) 프로젝트/어플리케이션의 비-LLM 상태를 추출하세요. LLM과 관련이 없는 상태가 있다면, 필요할 때 추출하고 로드해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, 엔진 워커 플로우는 다음과 같습니다:\n\n사용자로부터 요청 받기 -\u003e 외부 캐시 리소스에서 상태 로드 -\u003e 요청 처리 -\u003e LLM -\u003e 상태를 외부 캐시 리소스에 저장 -\u003e 사용자에게 응답 반환\n\n아래 다이어그램에서도 확인할 수 있습니다:\n\n# 보안, 보안, 보안\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알겠어요, 이제 무서운 부분에 도착했네요.\n\n당신이 제어할 수 없는 것을 어떻게 안전하게 보호할 수 있을까요?\n\n답은 있습니다: 가능한 한 많은 제어권을 얻어 내는 것으로!\n\n먼저, LLMs를 위한 OWASP Top 10에 익숙해지는 것이 항상 좋은 생각입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 경험을 토대로 몇 가지 추가적인 팁을 전해드릴게요:\n\n## 팁 #1: 탈옥(Jailbreak) 피하기\n\nLLMs에서 탈옥 개념은 LLM 프로젝트의 기본 기능 및 안전장치를 우회하고 사용자의 이익을 위해 사용하는 것을 의미합니다. 실제로 이것은 보호하기가 매우 어려운 부분 중 하나예요. 사용자의 요청을 충족시키기 위해 LLM이 규칙을 무시하는 것이 얼마나 쉽게 일어나는지 놀랄 것입니다. 프로젝트에 따라 탈옥으로부터 보호하기 위해 취해야 할 조치를 창의적으로 고민해봐야 할 수도 있어요.\n\n아래에 몇 가지 아이디어가 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시스템 프롬프트를 사용하여 감옥 탈출을 피하기 위한 규칙을 LLM에 제공해보세요. 시스템 프롬프트 규칙은 사용자 프롬프트 규칙보다 더 심각하게 취급되며, 그것들은 무시되는 경우가 적습니다. 이 방법으로도 100%의 보호는 제공되지는 않습니다.\n\nLLM의 기능을 제한하기 위해 화이트리스트 테크닉을 사용해보세요.\n\nLLM 기능이 LangChain 도구를 사용한다면, 각 요청에 대해 사용하는 도구 중 하나가 확실히 사용되도록 할 수 있습니다. 이렇게 하면 요청이 내장 기능과 관련이 있는지 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 두 가지 예시가 있어요:\n\n유효한 프롬프트\n\n“’user1’이라는 사용자를 만들어주세요”:\n\n이것은 사용자 기능 API를 사용하는 도구를 이용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 요청을 허용할 수 있어요.\n\n유효하지 않은 프롬프트\n\n“미국의 첫 번째 대통령은 누구였나요?”:\n\n이 질문에 대한 답변을 위해 프로젝트에서 어떤 도구도 사용할 수 없지만, LLM은 자체 지식으로 답변하는 방법을 알고 있어요. 이 요청은 차단해야 해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 팁 #2: 모델 접근을 보호하세요\n\nLLM 서비스를 호스팅하고 제공하는 것은 비용이 많이 들 수 있으며 할당량 제한이 있을 수 있습니다. 가능한 한 LLM에 대한 접근을 강화하는 것이 좋습니다. OpenAI 키를 사용하는 것 뿐만 아니라 네트워크를 보호하고 접근을 제한하는 것도 중요합니다.\n\n저희 프로젝트에서 Azure OpenAI를 사용할 때는 LLM을 격리된 네트워크( Azure Virtual Network)에 배포할 수 있고, 엔진 사용만을 허용하도록 엔진과 연결된 AWS 엔진 람다를 전용 VPC(Amazon Virtual Private Cloud)에 설치할 수 있습니다. 또한 LLM과 엔진 간의 연결은 VPN을 통해 보호됩니다.\n\n## 팁 #3: 모욕적인 응답 차단하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당신의 LLM 프로젝트가 고객을 욕하고 모욕하는 것을 상상해 보세요. 악몽 같죠? 이러한 시나리오를 피하기 위해 우리는 최선을 다해야 합니다.\n\n사용할 수 있는 두 가지 방법은 다음과 같습니다:\n\n- OpenAI moderation\n- Azure OpenAI content filtering\n\n## 팁 #4: LLM 엔진에 제어점/훅을 추가하세요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM이 여러 작업을 차례대로 실행해야 하는 상황이 있다고 가정해 봅시다. 작업이 의미 있는지 확인하기 위해 \"점검 포인트\"를 추가하면 좋을 것입니다.\n\n다음은 구체적인 예시입니다:\n\n저희 프로젝트에서 LLM은 NL 입력을 사용하여 API와 매개변수를 생성하고, 제품의 API를 실행하고, 응답을 분석하고 필요할 때 재시도한 다음, 서식이 지정된 결과를 반환해야 합니다.\n\n이 경우, API를 생성한 후 실행하기 전에 주요 제어 포인트가 발생합니다. API, 매개변수를 검증하고 입력을 사용하여 즉시 주입을 방지하기 위해 내용을 정리할 수 있습니다. 다른 제어 포인트는 결과를 고객에게 반환하기 전에 발생할 수 있습니다. 응답의 내용과 형식을 검증할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 팁 #5: 평소 보안 가이드라인을 소홀히 하지 마세요\n\n이 프로젝트는 다른 프로젝트와 똑같은 위험을 안고 있습니다. 그 이상의 위험도 있습니다. 많은 사람들이 자신의 용도로 무료 LLM 액세스를 얻고자 하며, 특히 자신에게 금지된 자산에 액세스하려는 공격자도 있습니다.\n\n다음에 집중하세요:\n\n- 인증 및 권한 부여. 새 제품에 액세스하는 사람이 그렇게 할 수 있는 권한이 있는지 확인하세요.\n- 테넌트 격리. 항상 사용하던 것과 같은 테넌트 격리 보호를 유지하세요. 새로운 LLM 프로젝트가 중요 정보에 액세스할 수 있는 벽구멍이 되지 않도록 주의하세요.\n- 방화벽 및 쓰로틀링. 받는 요청의 수를 제어하세요. 프로젝트가 남용되지 않도록 주의하세요. LLM이 부담을 견딜 수 있는지 확인하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 고려해야 할 다른 사항\n\n제작 준비가 거의 끝났어요!\n\n다음은 고려해야 할 몇 가지 사항입니다:\n\n## 피드백\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당신이 LLM 프로젝트가 프로덕션 환경에서 잘 작동하는지 어떻게 알 수 있을까요? 고객이 결과에 만족했는지 어떻게 알 수 있을까요? 프로젝트를 계속 향상시키기 위해 결과를 따르고 평가할 수 있는 피드백 메커니즘을 고려하는 것이 현명할 수도 있겠죠.\n\n## 모델 평가\n\n출시하기 전에 모델이 원하는 대로 작동하는지 어떻게 알 수 있을까요? 모델에 적용하는 변경 사항에 대해 확신을 얻는 방법은 무엇인가요? 모델 평가는 매우 복잡한 주제입니다.\n\n다음 사항을 고려해 보시기 바랍니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- LLM 프로젝트의 기본 기능을 커버하는 테스트를 추가해보세요. 이번 테스트는 여태 익숙했던 것과는 다를 수 있어요 (새롭고 확정되지 않은 세상을 탐험한다고 언급한 적이 있죠?). 예를 들어, 정확한 단어 일치를 사용할 수 없을 수도 있어요. 대신에 중요한 기능을 테스트해보세요. 우리 프로젝트에서는 사용 사례를 제공한 다음 올바른 API가 선택되고 주요 매개변수가 올바르게 설정되었는지 확인해야 해요.\n- 자체 LLM 엔진을 평가하기 위해 LLM을 활용해보세요. LLM 평가자는 사용 사례를 생성하고 예측을 제공한 다음 실제 결과와 비교할 수 있어요. 참 멋지죠?\n\n## 법률\n\n새로운 LLM 프로젝트를 시작할 때는 무엇을 하고 있는지가... 법적으로... 합법적인지 확인하기 위해 법률 팀에 문의해보는 것이 좋아요.\n\n이에는, AI/Gen-AI를 사용하기 위해 고객으로부터 동의를 얻는 것, LLM을 훈련시키기 위해 고객 데이터를 사용하지 않는 것, 대화에 관한 정보를 유지하지 않도록 LLM 호스팅 플랫폼을 어떻게 시행할 것인지, 규정 준수 등이 포함돼요 (예: GDPR 등).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 여정의 끝\n\n저희 LLM 제품의 아키텍처를 보여주는 다이어그램이 여기 있어요. 이 블로그에서 설명한 모든 것을 포함하고 있습니다:\n\n주요 참가자들은 다음과 같아요:\n\n- GPT 모델. Azure OpenAI에 호스팅되며, 우리의 AWS VPC를 통해서만 안전하게 액세스됩니다.\n- 전처리 워커. AWS Lambda에 호스팅되어 있습니다. 세션 시작 시 데이터(API 명세 파일)를 준비하고 외부 저장소에 처리된 데이터를 저장합니다.\n- 엔진 워커. AWS Lambda에 호스팅되어 있습니다. 처리된 데이터와 상태를 로드하고, 우리의 LLM을 사용하여 관련 API 요청을 생성하고 제품에서 작업을 수행합니다.\n- 피드백 워커. AWS Lambda에 호스팅되어 있습니다. 세션 피드백을 수집하고 저장하며, 세션 상태를 초기화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 이것은 우리 여행의 끝입니다. 아니면 이제 막 시작인 걸까요? ☺","ogImage":{"url":"/assets/img/2024-06-19-AnLLMJourneyFromPOCtoProduction_0.png"},"coverImage":"/assets/img/2024-06-19-AnLLMJourneyFromPOCtoProduction_0.png","tag":["Tech"],"readingTime":11}],"page":"23","totalPageCount":71,"totalPageGroupCount":4,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"23"},"buildId":"FH3Qr-mLAesqA0X5IFRQr","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>