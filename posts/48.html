<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/48" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/48" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_buildManifest.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="인공지능과 로봇공학 사회에 미치는 영향과 수조 달러 규모의 비지니스 기회" href="/post/2024-06-20-AIandRoboticsImplicationsforSocietyandaMulti-TrillionDollarBusinessOpportunity"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="인공지능과 로봇공학 사회에 미치는 영향과 수조 달러 규모의 비지니스 기회" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-AIandRoboticsImplicationsforSocietyandaMulti-TrillionDollarBusinessOpportunity_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="인공지능과 로봇공학 사회에 미치는 영향과 수조 달러 규모의 비지니스 기회" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">인공지능과 로봇공학 사회에 미치는 영향과 수조 달러 규모의 비지니스 기회</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ROS2 겸손한 이미지 세분화" href="/post/2024-06-20-ROS2HumbleImageSegmentation"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ROS2 겸손한 이미지 세분화" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-ROS2HumbleImageSegmentation_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ROS2 겸손한 이미지 세분화" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">ROS2 겸손한 이미지 세분화</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="모바일 로봇, 더 많으면 더 좋다는 통신 원칙을 잘 활용하기" href="/post/2024-06-20-Mobilerobotsgetalegupfromamore-is-bettercommunicationsprinciple"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="모바일 로봇, 더 많으면 더 좋다는 통신 원칙을 잘 활용하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-Mobilerobotsgetalegupfromamore-is-bettercommunicationsprinciple_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="모바일 로봇, 더 많으면 더 좋다는 통신 원칙을 잘 활용하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">모바일 로봇, 더 많으면 더 좋다는 통신 원칙을 잘 활용하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기" href="/post/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이를 사용한 자동 번호판 인식" href="/post/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이를 사용한 자동 번호판 인식" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이를 사용한 자동 번호판 인식" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">라즈베리 파이를 사용한 자동 번호판 인식</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">30<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이가 이제 Hailo-8L M2 모듈로 AI를 처리할 준비가 되었습니다" href="/post/2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이가 이제 Hailo-8L M2 모듈로 AI를 처리할 준비가 되었습니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이가 이제 Hailo-8L M2 모듈로 AI를 처리할 준비가 되었습니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">라즈베리 파이가 이제 Hailo-8L M2 모듈로 AI를 처리할 준비가 되었습니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이 5 CPU 성능" href="/post/2024-06-20-RaspberryPi5CPUPerformance"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이 5 CPU 성능" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-RaspberryPi5CPUPerformance_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이 5 CPU 성능" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">라즈베리 파이 5 CPU 성능</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="가장 빠른 쿠버네티스 배포 방법일까요 라즈베리 파이 클러스터 구축에 대해 알아봅시다" href="/post/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="가장 빠른 쿠버네티스 배포 방법일까요 라즈베리 파이 클러스터 구축에 대해 알아봅시다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="가장 빠른 쿠버네티스 배포 방법일까요 라즈베리 파이 클러스터 구축에 대해 알아봅시다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">가장 빠른 쿠버네티스 배포 방법일까요 라즈베리 파이 클러스터 구축에 대해 알아봅시다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="홈 어시스턴트에 MQTT 서버를 설치하고 추가하는 방법" href="/post/2024-06-20-InstallandaddingMQTTservertotheHomeAssistant"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="홈 어시스턴트에 MQTT 서버를 설치하고 추가하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-InstallandaddingMQTTservertotheHomeAssistant_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="홈 어시스턴트에 MQTT 서버를 설치하고 추가하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">홈 어시스턴트에 MQTT 서버를 설치하고 추가하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="70달러 AI 키트로 나만의 GPT와 비슷한 어시스턴트를 만들어보세요" href="/post/2024-06-20-BuildYourOwnGPT-likeAssistantwiththis70AIKit"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="70달러 AI 키트로 나만의 GPT와 비슷한 어시스턴트를 만들어보세요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-BuildYourOwnGPT-likeAssistantwiththis70AIKit_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="70달러 AI 키트로 나만의 GPT와 비슷한 어시스턴트를 만들어보세요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">70달러 AI 키트로 나만의 GPT와 비슷한 어시스턴트를 만들어보세요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/41">41</a><a class="link" href="/posts/42">42</a><a class="link" href="/posts/43">43</a><a class="link" href="/posts/44">44</a><a class="link" href="/posts/45">45</a><a class="link" href="/posts/46">46</a><a class="link" href="/posts/47">47</a><a class="link posts_-active__YVJEi" href="/posts/48">48</a><a class="link" href="/posts/49">49</a><a class="link" href="/posts/50">50</a><a class="link" href="/posts/51">51</a><a class="link" href="/posts/52">52</a><a class="link" href="/posts/53">53</a><a class="link" href="/posts/54">54</a><a class="link" href="/posts/55">55</a><a class="link" href="/posts/56">56</a><a class="link" href="/posts/57">57</a><a class="link" href="/posts/58">58</a><a class="link" href="/posts/59">59</a><a class="link" href="/posts/60">60</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"인공지능과 로봇공학 사회에 미치는 영향과 수조 달러 규모의 비지니스 기회","description":"","date":"2024-06-20 17:52","slug":"2024-06-20-AIandRoboticsImplicationsforSocietyandaMulti-TrillionDollarBusinessOpportunity","content":"\n\n# 테크 엘리트: AI 및 점점 벌어지는 부의 격차\n\n포사이트 비로 Youtube 채널을 구독하세요: https://www.youtube.com/@ForesightBureau?sub_confirmation=1\n\n로봇에게 일자리를 빼앗길 걱정이 있나요? 혼자가 아닙니다. 저희의 'AI 직업 아포칼립스' 시리즈의 첫 번째 동영상에서는 자동화가 일으키는 위협으로부터 미래를 지키는 방법을 제안했습니다. 최신 비디오에서는 사회에 미칠 잠재적인 영향에 대해 심층적으로 살펴보며, 이 번창하고 있는 산업에 투자해야 하는 시기가 지금이라고 설명합니다. 왜냐하면 AI 산업이 수조 달러 규모의 기회로 성장하고 있기 때문입니다.\n\n![AI와 로봇공학이 사회와 수조 달러 비즈니스 기회에 미치는 영향](/assets/img/2024-06-20-AIandRoboticsImplicationsforSocietyandaMulti-TrillionDollarBusinessOpportunity_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시리즈의 이전 비디오에서는 자동화의 새로운 시대에 있어 가장 위험에 노출된 화이트 칼라 직업과 블루 칼라 직업이 무엇인지, 어떤 기업들이 인간형 로봇 산업을 지배할 것으로 보이며 해당 기업에 투자를 고려해야 하는 이유에 대해 다루었습니다. 또한 인간형 양다리로의 흥미로운 부상을 살펴보고 이들이 노동력을 대체할 것으로 예상되는 이유를 설명했습니다.\n\n아래 비디오에서는 AI와 로보틱스가 사회에 미칠 넓은 영향에 대해 다룹니다. 우리는 자동화가 기술 엘리트에게 혜택을 더 많이 줄 수 있고 이로 인해 부의 불평등이 더 커지고 사회적 격변을 야기하며, 기술적으로 우월한 소수의 손에 권력이 집중되는 가능성 등을 탐구합니다.\n\n물리는 오는 기계 경제에서 이윤을 얻을 수 있는 생존 전략 몇 가지를 제시하며 긍정적으로 마무리됩니다.\n\n![AI and Robotics Implications for Society and a Multi-Trillion Dollar Business Opportunity](/assets/img/2024-06-20-AIandRoboticsImplicationsforSocietyandaMulti-TrillionDollarBusinessOpportunity_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리를 소셜 미디어에서 팔로우해 주세요:\n\nYoutube: https://www.youtube.com/@ForesightBureau\nInstagram: https://www.instagram.com/foresight.bureau\nSubstack: https://substack.com/@foresightbureau\nTwitter/X: https://x.com/foresightbureau\nPodcast: https://foresightbureau.podbean.com\nLinkedin: https://bit.ly/ForesightBureauLI\nFacebook: https://bit.ly/ForesightBureauFB\nMedium: https://medium.com/@foresightbureau\nWebsite: https://foresightbureau.com\n\n면책 조항\n\n본 블로그는 오로지 즐거움을 위한 목적으로 작성되었습니다. 게시된 정보의 정확성이나 완전성을 보장하지 않습니다. 본 블로그를 사용함으로 인해 발생할 수 있는 손실이나 손해에 대해 우리는 책임을 지지 않습니다. 투자나 금융 조언으로 해석해서는 안 됩니다.","ogImage":{"url":"/assets/img/2024-06-20-AIandRoboticsImplicationsforSocietyandaMulti-TrillionDollarBusinessOpportunity_0.png"},"coverImage":"/assets/img/2024-06-20-AIandRoboticsImplicationsforSocietyandaMulti-TrillionDollarBusinessOpportunity_0.png","tag":["Tech"],"readingTime":2},{"title":"ROS2 겸손한 이미지 세분화","description":"","date":"2024-06-20 17:50","slug":"2024-06-20-ROS2HumbleImageSegmentation","content":"\n\n\n\u003cimg src=\"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_0.png\" /\u003e\n\n# 소개\n\n이 튜토리얼에서는 DeepLabV3 모델을 사용하여 이미지의 의미론적 세분화를 수행하는 ROS2 노드를 만들 것입니다. ResNet-101 백본을 사용한 이 모델은 의미론적 세분할 작업에 대한 최신 아키텍처입니다. 노드는 웹캠 피드를 구독하고 이미지를 처리한 후 세그멘테이션을 수행하고 세분화된 이미지를 게시할 것입니다.\n\n## 준비물\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 설치되어 있는지 확인하세요:\n\n- ROS2 (Humble)\n- PyTorch\n- OpenCV\n- cv_bridge (ROS 이미지와 OpenCV 이미지 간 변환을 위한)\n- torchvision (사전 학습 모델과 이미지 변환을 위한)\n\n## DeepLabV3\n\nDeepLabV3은 시맨틱 세그멘테이션 작업을 위해 설계된 최신 딥러닝 모델입니다. 시맨틱 세그멘테이션은 이미지의 각 픽셀을 미리 정의된 범주로 분류하는 작업을 포함합니다. 물체 감지와 달리 물체를 식별하고 주위에 바운딩 상자를 넣는 대신, 시맨틱 세그멘테이션은 장면에 대한 상세하고 픽셀 수준의 이해를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## DeepLabV3의 주요 기능\n\nResNet 백본: DeepLabV3은 특성 추출을 위해 ResNet-101을 사용합니다. ResNet-101은 잔차 학습 방식으로 유명한 강력하고 깊은 신경망으로, 사라져가는 그래디언트 문제를 해결하여 매우 깊은 네트워크를 효율적으로 훈련할 수 있으며 견고한 특성 추출을 보장합니다.\n\n## DeepLabV3 작동 방식\n\n- 입력 이미지: 고정된 크기(예: 512x512 픽셀)의 입력 이미지로 프로세스가 시작됩니다.\n- 특성 추출: 입력 이미지는 ResNet-101 백본을 통해 전달됩니다. 이 네트워크는 ImageNet과 같은 대규모 데이터셋에서 사전 훈련되어 다양한 시각적 특성에 대한 견고한 이해력을 갖추고 있습니다.\n- 어트러스 합성곱 레이어: 초기 특성 추출 후, 모델은 다양한 확장률을 가진 일련의 어트러스 합성곱을 적용합니다. 이 단계를 통해 모델은 다양한 크기의 객체를 분할하는 데 중요한 다중 스케일의 특성을 캡처할 수 있습니다.\n- 공간 피라미드 풀링: 어트러스 합성곱의 출력은 공간 피라미드 풀링 모듈로 전달됩니다. 이 모듈은 다양한 스케일에서 특성을 풀링하여 이미지의 풍부한 다중 문맥적 표현을 제공합니다.\n- 분할 맵: 마지막으로, 풀링된 특성은 원본 이미지 해상도로 업샘플링되고 최종 합성곱 레이어가 분할 맵을 생성합니다. 이 맵의 각 픽셀에는 클래스 레이블이 지정되어 이미지의 상세한 세그멘테이션이 이루어집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_1.png\" /\u003e\n\n## DeepLabV3\n\n- Architecture: DeepLabV3는 시멘틱 이미지 세그멘테이션을 위해 설계된 딥러닝 모델입니다. Dilated (확장된) 합성곱을 활용하여 수용 영역을 확대시키면서도 공간 해상도를 유지하는 방식으로 다중 스케일 문맥을 캡처합니다.\n- Backbone: 여기서 사용된 백본은 ResNet-101이며, 보다 복잡한 표현을 학습하는 데 도움이 되는 깊은 잔여 네트워크입니다.\n- 데이터 세트: 모델은 COCO 데이터 세트에서 사전 훈련을 받고 VOC 라벨로 세부 조정되어 이러한 데이터 세트에서 발견되는 공통 객체를 세분화할 수 있습니다.\n\n## 모델 로딩 코드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nself.model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', weights='DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1')\n```\n\n## 코드 단계\n\n- 초기화: 모델은 SegmentationNode 클래스의 __init__ 메서드에서 초기화됩니다. torch.hub.load 메서드는 미리 훈련된 deeplabv3_resnet101 모델을 불러옵니다.\n- 전처리: 입력 이미지는 모델의 입력 요구 사항과 일치하도록 크기 조정, 중앙 자르기 및 정규화됩니다.\n- 추론: 이미지가 수신될 때 콜백에서 모델은 전처리된 이미지 텐서에 대해 추론을 수행합니다.\n- 후처리: 모델의 출력은 각 픽셀의 클래스 점수를 포함하는 텐서입니다. 각 픽셀에서 가장 높은 점수가 클래스를 결정합니다. 그런 다음 결과는 PASCAL VOC 컬러 맵을 사용하여 컬러맵으로 변환된 세그멘테이션 이미지로 변환됩니다.\n\n![image](/assets/img/2024-06-20-ROS2HumbleImageSegmentation_2.png)  \n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 어플리케이션: 시맨틱 세그멘테이션을 위한 ROS2 노드\n\n실용적인 어플리케이션에서 DeepLabV3를 활용하기 위해 웹캠 이미지로부터 시맨틱 세그멘테이션을 수행하는 ROS2 (로봇 운영 시스템 2) 노드를 만들 수 있습니다. 다음은 단계별 개요입니다:\n\n- 노드 초기화: 웹캠 이미지 토픽을 구독하는 ROS2 노드를 초기화합니다.\n- 이미지 전처리: 들어오는 이미지를 DeepLabV3 모델이 필요로 하는 형식으로 변환합니다. 일반적으로 이미지 크기 조정 및 정규화를 수행합니다.\n- 모델 추론: 전처리된 이미지를 DeepLabV3 모델을 통해 전파하여 세그멘테이션 맵을 얻습니다.\n- 후처리: 세그멘테이션 맵을 원본 이미지 해상도에 맞게 업샘플링하고 클래스 레이블을 시각적으로 식별 가능한 색상으로 변환합니다.\n- 결과 게시: 세그멘트된 이미지를 시각화나 추가 처리를 위해 ROS2 토픽에 게시합니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_3.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 사용법\n\n이 모델은 웹캠에서 수신한 이미지를 세분화하는 데 사용되며, COCO 및 VOC 데이터 세트에서 학습한 클래스에 따라 장면에서 다양한 개체를 식별하고 색칠합니다. 이 세분화된 이미지는 그런 다음 ROS 주제에 발행됩니다.\n\n이 설정을 통해 물체 인식, 자율 탐사, 그리고 장면 이해를 포함한 여러 로보틱 응용 프로그램에 적합한 실시간 이미지 세분화가 가능합니다.\n\n## 자율 탐사\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n장애물 감지 및 회피:\n\n- 도시 환경: 자율 주행 자동차에서 시멘틱 세분화는 차로, 인도, 보행자, 차량 및 도시 풍경의 다른 중요 요소를 식별하는 데 도움이 됩니다. 이러한 요소를 세분화함으로써, 자율 주행 차량은 복잡한 환경에서 안전하게 탐색할 수 있습니다.\n- 실내 내비게이션: 실내에서 작동하는 로봇은 벽, 가구, 문 및 기타 장애물을 감지하기 위해 시멘틱 세분화를 사용할 수 있어 효과적으로 탐색과 경로를 계획할 수 있습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*YcvLdT_wB5wd6jfa6f3xWA.gif)\n\n## 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDeepLabV3는 새로운 atrous convolution과 공간 피라미드 풀링 기술을 통해 높은 정확도와 효율성을 제공하는 시멘틱 세그멘테이션에 강력한 도구입니다. DeepLabV3를 ROS2와 통합함으로써, 개발자들은 환경을 픽셀 수준에서 이해하고 상호 작용하는 지능적인 로봇 응용 프로그램을 만들 수 있습니다. DeepLabV3를 사용하면 자율 주행, 로봇 조작 또는 장면 이해와 같은 영역에서 고급 인식을 위한 새로운 가능성이 열립니다.","ogImage":{"url":"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_0.png"},"coverImage":"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_0.png","tag":["Tech"],"readingTime":4},{"title":"모바일 로봇, 더 많으면 더 좋다는 통신 원칙을 잘 활용하기","description":"","date":"2024-06-20 17:49","slug":"2024-06-20-Mobilerobotsgetalegupfromamore-is-bettercommunicationsprinciple","content":"\n\n## 한 연구에서 발견된바 미끄럼 방지 장치를 가지고 있는 것보다 지면 주변 상황에 대한 감각이 뛰어날 때 모바일 로봇에 더 도움이 된다고 해요.\n\n조지아 공과대학의 복잡한 레오로지학과 생체역학 연구소의 백시 총 박사 연구원이 말하길...\n\n![image](/assets/img/2024-06-20-Mobilerobotsgetalegupfromamore-is-bettercommunicationsprinciple_0.png)\n\n주변 환경에 대한 최소한의 인식력을 가진 로봇에 다리를 추가하면, 어려운 지형에서 로봇이 더 효율적으로 작동할 수 있다는 것을 저와 제 동료들이 발견했어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 수학자이자 엔지니어인 클로드 샤넌의 통신 이론에서 영감을 받았습니다. 먼 거리를 통해 신호를 전송하는 방법에 대해 그의 이론을 참고했습니다. 완벽한 전선을 구축하기 위해 엄청난 비용을 지출하는 대신, 샤넌은 잡음이 많은 통신 채널을 통해 정보를 신뢰할 수 있게 전달하는 데 중복성을 사용하는 것이 충분하다는 것을 보여 주었습니다. 우리는 화물을 로봇을 통해 운송하는 것에 대해 같은 접근법을 적용할 수 있을지 궁금해졌습니다. 즉, 우리가 \"잡음이 많은\" 지형을 통해 화물을 운송하려면, 쓰러진 나무와 큰 바위와 같은 지형에서 이를 로봇의 다리를 추가함으로써 센서와 카메라 없이 수행할 수 있을까요?\n\n대부분의 이동 로봇은 관성 센서를 사용하여 공간을 통해 어떻게 이동하는지 인식합니다. 우리의 주요 아이디어는 관성을 잊고 단순히 반복적으로 걸음을 내딛는 기능으로 대체하는 것입니다. 이를 통해 이론적 분석을 통해 추가 센싱 및 제어 없이도 신뢰할 수 있고 예측 가능한 로봇 운동(따라서 화물 운송)을 확인했습니다.\n\n우리의 가설을 검증하기 위해 우리는 천둥 벌레에 영감을 받은 로봇을 제작했습니다. 우리는 다리를 더 추가할수록 로봇이 추가 센싱이나 제어 기술 없이도 불균일한 표면을 효과적으로 이동할 수 있게 되는 것을 발견했습니다. 구체적으로, 우리는 일련의 실험을 수행했으며, 불균일한 자연 환경을 모방하는 지형을 구축하여로봇의 이동 성능을 평가했습니다. 우리는 다리 수를 6개로 시작하여 총 16개가 되기까지 2의 증분으로 다리 수를 점진적으로 늘려가며 로봇의 이동 성능을 평가했습니다.\n\n다리 수가 증가함에 따라 로봇은 센서 없이도 지형을 이동할 때 높은 민첩성을 보였습니다. 그 성능을 추가로 평가하기 위해 실제 지형에서 야외 테스트를 수행하여 더 현실적인 조건에서의 성능을 확인했는데, 그 결과도 좋았습니다. 많은 다리를 가진 로봇을 농업, 우주 탐사 및 수색 및 구조 작업에 사용할 수 있는 잠재력이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 왜 중요한가요\n\n음식, 연료, 건축 자재, 의약품을 운송하는 것은 현대 사회에 꼭 필요한 일이며, 효율적인 화물 교환이 상업 활동의 기초입니다. 여러 세기 동안 물자를 땅 위에서 운송하는 것은 도로와 추적선을 건설해야 했습니다. 그러나 도로와 추적선이 어디나 구비되어 있는 것은 아닙니다. 언덕 지역과 같은 곳은 화물에 제한적인 접근권을 가졌습니다. 로봇은 이러한 지역에서 화물을 운송하는 방법일 수 있습니다.\n\n# 이 분야에서 다른 연구는 무엇이 진행 중인가요\n\n다른 연구자들은 최근 몇 년 동안 더욱 민첩해진 인간형 로봇과 로봇 개를 개발해왔습니다. 이러한 로봇은 자신이 어디에 있는지와 자신 앞에 무엇이 있는지 정확한 센서에 의존하여 이를 알고, 그 뒤에 어떻게 항해할지에 대한 결정을 내립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 그들의 환경 인식에 대한 강한 의존으로, 예측할 수 없는 환경에서는 제한될 수 있습니다. 예를 들어, 수색 및 구조 작업에서 센서가 손상을 입을 수 있고 환경이 변할 수 있습니다.\n\n# 다음은 무엇인가요?\n\n저와 제 동료들은 연구에서 얻은 소중한 통찰을 가지고 농업 분야에 적용했습니다. 우리는 이 로봇을 사용하여 농지에서 효율적으로 잡초를 뽑는 회사를 설립했습니다. 이 기술을 계속 발전시키는 과정에서, 로봇의 디자인과 기능성에 초점을 맞추고 있습니다.\n\n우리는 백종 로봇의 기능적 측면을 이해하고 있지만, 계속되는 노력은 외부 센싱에 의존하지 않고 움직임에 필요한 최적의 다리 수를 결정하는 데 집중되어 있습니다. 우리의 목표는 비용 효율성과 시스템의 혜택을 유지하는 사이의 균형을 맞추는 것입니다. 현재 우리는 이 로봇이 효과적으로 작동하려면 최소 12개의 다리가 필요하다는 것을 보여 주었지만, 아직 이상적인 수를 조사 중에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사는 학술 전문가의 지식을 공유하기 위해 전논허 비영리 뉴스 기관인 더 컨버세이션(The Conversation)에서 제공한 것입니다. 저희에 대해 더 알고 싶으시거나 주간 과학 편집자 선정 기사를 구독하고 싶으시다면 저희 웹사이트를 방문해주세요.\n\n이 기사를 읽는 데 즐거우셨나요? 그렇다면 박수로 응원해주시고 이 페이지 우측 상단의 \"팔로우\" 버튼을 클릭해 주세요. 그렇게 하면 전문 연구자들이 쓴 저희 기사가 여러분의 피드에 자주 나타날 것입니다.\n\n저자는 NSF-Simons Southeast Center for Mathematics and Biology (Simons 재단 SFARI 594594), Georgia Research Alliance (GRA.VL22.B12), Army Research Office (ARO) MURI 프로그램, Army Research Office Grant W911NF-11-1-0514 및 던 가족 교수 자리로부터 자금 지원을 받았습니다. 저자와 동료들은 이 기사에서 다룬 연구와 관련한 하나 이상의 특허 출원을 진행 중입니다. 또한 저자와 동료들은 이 작업을 바탕으로 일부 원천 기업인 Ground Control Robotics, Inc.를 설립했습니다.","ogImage":{"url":"/assets/img/2024-06-20-Mobilerobotsgetalegupfromamore-is-bettercommunicationsprinciple_0.png"},"coverImage":"/assets/img/2024-06-20-Mobilerobotsgetalegupfromamore-is-bettercommunicationsprinciple_0.png","tag":["Tech"],"readingTime":3},{"title":"ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기","description":"","date":"2024-06-20 17:48","slug":"2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch","content":"\n\n\u003cimg src=\"/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png\" /\u003e\n\n이 튜토리얼에서는 PyTorch에서 미리 학습된 딥 러닝 모델을 사용하여 실시간 인간 포즈 추정을 위한 ROS 2 노드를 만들겠습니다. 이 노드는 웹캠 이미지를 구독하고 포즈 추정을 수행한 뒤 주석이 달린 이미지를 발행할 것입니다. 구현 세부 내용으로 들어가 봅시다.\n\n## 요구 사항\n\n시작하기 전에 다음이 설치되어 있는지 확인하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- ROS 2\n- Python 3\n- PyTorch\n- torchvision\n- OpenCV\n- cv_bridge (ROS 패키지로 ROS와 OpenCV 이미지 간 변환을 위한 것)\n\n모델: keypointrcnn_resnet50_fpn\n\n우리는 torchvision의 keypointrcnn_resnet50_fpn 모델을 사용합니다. 이 모델은 사람 자세 추정을 위해 설계되어 여러 신체 부위의 키포인트를 예측합니다. 여기에 이 모델의 구성 요소가 있습니다:\n\n- ResNet-50 백본: ResNet-50는 특징 추출기로 작용하는 합성곱 신경망입니다. 공간적 계층을 효과적으로 캡처하는 데 알려져 있습니다.\n- FPN (Feature Pyramid Network): FPN은 다중 스케일에서 특성 맵을 구축하여 감지 능력을 향상시킵니다.\n- Keypoint R-CNN: 이 Faster R-CNN의 변형은 바운딩 박스 외에도 키포인트를 감지하는 데 특화되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*LbBdaJJRpnNGRLdKExdzdw.gif\" /\u003e\n\n자율 이동 로봇(AMR) 및 로보틱스에서 실시간 포즈 추정의 응용\n\n포즈 추정은 로보틱스 분야에서 강력한 도구이며 자율 이동 로봇(AMR)의 성능을 크게 향상시킬 수 있습니다. 실시간 포즈 추정을 통합함으로써 로봇은 상황 인식, 인간과의 상호 작용, 다양한 작업에서의 성능을 향상시킬 수 있습니다. 다음은 주요 응용 분야 몇 가지입니다:\n\n- 협업 로봇 (Cobots)\n- 감시 및 보안\n- 제조 및 조립 라인\n- 내비게이션 및 장애물 회피\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계별 실행\n\n- 노드 초기화 및 구독\n\n우리는 필요한 라이브러리를 가져오는 것으로 시작합니다. 이에는 ROS 2 Python 클라이언트 라이브러리 (rclpy), ROS 메시지 종류 (Image), 이미지 변환을 위한 CvBridge, 그리고 딥러닝을 위한 PyTorch 및 torchvision이 포함됩니다.\n\n2. 노드 클래스 생성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPoseEstimationNode 클래스를 정의하고 Node를 상속합니다. 생성자에서는:\n\n- 노드를 pose_estimation_node으로 이름을 지정하여 초기화합니다.\n- 이미지를 수신하기 위해 /jetson_webcam 토픽에 구독합니다.\n- 이미지_pose 토픽에 주석 처리된 이미지를 게시할 발행자를 만듭니다.\n- ROS 및 OpenCV 이미지 간 변환을 위해 CvBridge를 초기화합니다.\n- torchvision에서 사전 학습된 자세 추정 모델 keypointrcnn_resnet50_fpn을 평가 모드로 설정하여 로드합니다.\n- 이미지를 텐서로 변환하기 위한 변환을 정의합니다.\n\n3. 수신된 이미지 처리\n\nlistener_callback 메서드에서 수신된 이미지를 처리합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 받은 이미지의 인코딩을 기록합니다.\n- CvBridge를 사용하여 이미지를 ROS 형식에서 OpenCV 형식으로 변환하고 다양한 이미지 인코딩을 처리합니다.\n\n4. 포즈 추정\n\n다음으로, OpenCV 이미지를 PIL 이미지로 변환하고 텐서로 변환하기 위한 변환이 적용됩니다. 이 텐서를 모델에 전달하여 예측을 얻고, torch.no_grad()를 사용하여 기울기 계산이 이루어지지 않도록 합니다.\n\n5. 이미지 주석 및 게시\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그런 다음 이미지에서 주요 지점 위치에 원을 그립니다. 이러한 주요 지점은 모델의 예측에서 추출되어 OpenCV로 그리기 위해 numpy 배열로 변환됩니다. 마지막으로 주석이 달린 이미지를 ROS 메시지로 변환하여 발행합니다.\n\n![이미지](/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_1.png)\n\n6. 노드 실행하기\n\nmain 함수는 ROS 2 Python 클라이언트 라이브러리를 초기화하고 노드의 인스턴스를 생성한 다음 종료될 때까지 작동하도록 유지하도록 되어 있습니다. 그 후에는 노드가 제거되고 ROS 2 컨텍스트가 종료됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Building a Pose Estimation Node with ROS 2](/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_2.png)\n\n# 결론\n\n본 튜토리얼을 따라하면 최신 딥러닝 기법을 사용하여 실시간 포즈 추정이 가능한 견고한 ROS 2 노드를 만들 수 있습니다. 이 설정은 인간-로봇 상호작용 및 감시를 포함한 다양한 로봇 응용 프로그램으로 확장할 수 있습니다.\n\n실시간 포즈 추정은 AMR 및 로봇의 능력에 새로운 차원을 추가하여 인간의 동작 및 자세를 이해하고 반응할 수 있습니다. 이 기능은 인간-로봇 상호작용을 향상시키며 안전성을 향상시키고 로봇이 자율적으로 또는 협업적으로 수행할 수 있는 작업 범위를 확장시킵니다. 기술이 계속 발전함에 따라 로봇학의 다양한 분야에서 더 많은 혁신적인 응용 프로그램이 예상됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# NVIDIA Jetson 플랫폼에서 ROS 2 및 인공지능을 이용한 로봇 응용 프로그램 구현\n\nhttps://developer.nvidia.com/blog/implementing-robotics-applications-with-ros-2-and-ai-on-jetson-platform-2/#ros_2_nodes_for_human_pose_estimation","ogImage":{"url":"/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png"},"coverImage":"/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png","tag":["Tech"],"readingTime":4},{"title":"라즈베리 파이를 사용한 자동 번호판 인식","description":"","date":"2024-06-20 17:43","slug":"2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi","content":"\n\n\n\u003cimg src=\"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_0.png\" /\u003e\n\n# 소개\n\n이 프로젝트의 목표는 Raspberry Pi 마이크로 컴퓨터를 사용하여 주차 장벽을 제어하기 위한 자동 번호판 인식 시스템을 설계하는 것입니다.\n\n왜 이 프로젝트를 하게 되었을까요?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어딘가에 참여 중인 프로젝트가 없는 Rpi가 하나 있고 카메라와 잠재적인 고민이 있는데요 ― 사무실 주차장에 자동 주차 장벽 제어 시스템이 없습니다. 그러니 이 프로젝트를 시작해보는 건 어떨까요?\n\n이 프로젝트의 목적은 생산에 적합한 안정적이고 경쟁력있는 솔루션을 만드는 것이 아니라, 한정된 장비를 사용하여 실제 문제를 위한 작동 제품을 만들면서 재미를 느끼는 것입니다. 그리고 그 이후에는 이 솔루션을 경량 엣지 디바이스에서 빠르게 작동하도록 최적화하는 재미도 봅시다)\n\n![image](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_1.png)\n\n일반적인 아이디어는 Rpi 카메라를 사용하여 일정 주기로 사진을 촬영하고, 이미지를 처리하여 차량 번호판을 감지하고 문자를 인식한 다음 데이터베이스에서 허용된 번호 목록과 비교하는 것입니다. 목록의 번호판과 일치한다면 장벽이 열릴 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기본적인 단계에서는 다음 도구를 사용할 것입니다: \n\n- 이미지 소스 — Raspberry Pi Camera 모듈 v2;\n- 번호판 검출기 — pyTorch를 사용하여 제공되는 Yolo v7;\n- 광필 인식 (OCR) — EasyOCR;\n- \"데이터베이스\" — Google 시트의 테이블;\n\n모든 처리 작업과 계산은 Raspberry Pi 4b에서 로컬로 실행되어야 하며, 이 솔루션은 자율적으로 작동해야합니다.\n\n![이미지](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n라즈베리 파이는 \"거의 실시간\"으로 Pi 카메라에서 프레임을 계속해서 읽습니다. 그런 다음, 사용자 정의 데이터셋 YOLOv7 모델을 미세 조정하여 번호판이 있는 영역을 감지합니다. 그 다음, 필요한 경우 이미지 전처리를 제공하고 EasyOCR 모델이 제공된 자르기된 프레임에서 번호를 감지합니다. 그런 다음 번호판 문자열을 \"데이터베이스\"에 저장된 번호판 중 어느 것과 일치하는지 확인하고 해당 작업을 실행합니다. 라즈베리 GPIO (General-Purpose Input-Output) - 제어 릴레이 스위치를 사용하여 주차장 장벽과 빛 등 추가 부하를 연결할 수 있습니다.\n\nGPIO 핀을 사용하면 입력 센서 (IR, PIR와 같은)를 연결하고 자동차가 감지됐을 때에만 카메라를 작동시킬 수 있습니다.\n\n이 작업은 여러 가지 방법으로 해결할 수 있습니다. 일부 방법은 특정 요구 사항과 사용 사례에 더 효율적이고 간편할 수 있습니다. 예를 들어, 모든 중요한 처리를 클라우드에서 수행하거나 GPU 기반 엣지 장치를 사용하거나 다른 모델을 사용할 수 있습니다. ONNX, TFLite 등을 사용하여 제공할 수도 있습니다. 그러나 이 프로젝트는 실험으로 진행되었고, 현재 사용 가능한 장비를 사용했으며, 쉬운 방법을 찾는 것이 아니었습니다 =)\n\n# 환경 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 하드웨어 디자인\n\n필수 하드웨어:\n\n- 카메라 — Raspberry Pi Camera 모듈 v2 (Sony IMX219 8MPx, 1080p30, 720p60)\n- 엣지 디바이스 — Raspberry Pi 4 모델 B 4GB (CPU: Broadcom BCM2711, 쿼드 코어 Cortex-A72 (ARM v8) 64비트 SoC @ 1.5GHz; RAM: 4GB LPDDR4–3200 SDRAM; 40핀 GPIO 헤더; 2.4 GHz/5.0 GHz 802.11ac Wi-Fi, 블루투스 5.0)\n- SD 카드 (8GB)\n- 전원 공급 장치 — 5V 3A USB-C\n\n![image](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n추가 내용:\n\n- 열흡수기, 냉각 팬\n- UPS\n- 디스플레이 (Waveshare 2.7인치 e-Paper HAT)\n- 외부 장치(장벽) 제어용 릴레이 / Raspberry HAT\n- 카메라 마운트 (\"카메라용 독특한 금속 와이어 마운트\" :))\n\n* 색깔 다시 채워주는 시간이 괜찮은 TFT 또는 OLED 유형의 화면을 사용하는 것이 좋지만, 그 당시에는 이 것만 사용할 수 있었습니다.\n\n[이미지](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n설정하기\n\nPyTorch를 사용하여 솔루션을 만들기로 결정했으므로, Arm 64비트(aarch64)용 pip 패키지만 제공되므로 64비트 버전의 OS(데비안 버전: 11 - “Bullseye”)를 설치해야 합니다.\n\n최신 arm64 라즈베리 파이 OS는 공식 사이트에서 다운로드할 수 있으며 rpi-imager를 통해 설치할 수 있습니다.\n\n설치가 완료되면 다음과 같아야 합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_5.png\" /\u003e\n\n라즈베리 파이에 SD 카드를 삽입한 후 부팅하면 다음과 같은 설정을 수행해야 합니다:\n\n/boot/config.txt 파일을 수정하여 카메라를 활성화합니다.\n\n```js\n# 이는 카메라와 같은 확장 기능을 사용하도록합니다.\nstart_x=1\n# 카메라 처리에 적어도 128M이 필요하며 더 크면 그대로 둘 수 있습니다.\ngpu_mem=128\n# 기존의 camera_auto_detect 줄을 주석 처리/삭제해야합니다. 이것은 OpenCV/V4L2 캡처에서 문제를 일으킵니다.\n#camera_auto_detect=1\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한 아마 I2C, SSH 및 VNC을 활성화하려고 할 것입니다. 이 작업은 raspi-config 또는 GUI에서 할 수 있습니다.\n\n![image](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_6.png)\n\n요구 사항 설치\n\n저는 Python 버전 3.9 및 3.10을 사용했습니다. 일부 경우에 따르면 3.11 버전이 더 빠르다고 보고되지만 아직 안정적인 PyTorch가 3.11에는 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`requirements.txt` 파일을 사용하여 pip 패키지 관리자를 통해 모든 필요한 라이브러리와 모듈을 설치하세요:\n\n\nmatplotlib\u003e=3.2.2\nnumpy\u003e=1.18.5\nopencv-python==4.5.4.60\nopencv-contrib-python==4.5.4.60\nPillow\u003e=7.1.2\nPyYAML\u003e=5.3.1\nrequests\u003e=2.23.0\nscipy\u003e=1.4.1\ntorch\u003e=1.7.0,!=1.12.0\ntorchvision\u003e=0.8.1,!=0.13.0\ntqdm\u003e=4.41.0\nprotobuf\u003c4.21.3\ntensorboard\u003e=2.4.1\npandas\u003e=1.1.4\nseaborn\u003e=0.11.0\neasyocr\u003e=1.6.2\n\n\n수동으로 직접 설치하거나 기존 환경에 구현할 경우 (하지 마세요 :)), 현재 OpenCV 버전에 문제가 있으므로 정확한 버전 4.5.4.60을 설치해야 합니다.\n\n모든 것이 올바르게 설치되었는지 확인하려면 `pip list` 명령어를 사용하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`\u003cimg src=\"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_7.png\" /\u003e`\n\n그럼, 하드웨어와 환경을 설정해놓았으니 코딩을 시작해봅시다.\n\n# 소프트웨어 설계\n\n이미지 캡쳐\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이미지 캡처를 위해 표준 picamera 라이브러리 대신 OpenCV를 사용하여 비디오 프레임을 스트리밍할 것입니다. 64비트 OS에서 picamera 라이브러리를 사용할 수 없고 그 속도도 느립니다. OpenCV는 직접 /dev/video0 장치에 액세스하여 프레임을 캡처합니다.\n\nOpenCV 카메라 읽기를 위한 사용자 정의 간단한 래퍼:\n\n```python\nclass PiCamera():\n    def __init__(self, src=0, img_size=(640,480), fps=36, rotate_180=False):\n        self.img_size = img_size\n        self.fps = fps\n        self.cap = cv2.VideoCapture(src)\n        #self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n        #self.cap.set(cv2.CAP_PROP_FPS, self.fps)\n        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.img_size[0])\n        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.img_size[1])\n        self.rotate_180 = rotate_180\n    def run(self):       \n        # 프레임 읽기\n        ret, image = self.cap.read()\n        if self.rotate_180:\n            image = cv2.rotate(image, cv2.ROTATE_180)\n        if not ret:\n            raise RuntimeError(\"프레임 읽기 실패\")\n        return image \n```\n\n여기서 카메라가 뒤집혀 있기 때문에 `image = cv2.rotate(image, cv2.ROTATE_180)`를 사용하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n버퍼 크기와 FPS 설정은 랙을 고치고 프레임 스트림을 적절하게 정렬하는 데 사용할 수 있습니다. 그러나 제 경우에는 카메라 제조사 및 프레임을 읽는 데 사용된 백엔드에 따라 달라서 작동하지 않습니다.\n\n카메라에서 이미지가 캡처된 후, 우리는 번호판을 감지하는 작업을 시작하여 이미지를 처리해야 합니다.\n\n번호판 감지 모듈\n\n이 작업에는 YOLOv7 사전 훈련된 모델을 사용할 것입니다. 이 모델을 사용하여 사용자 지정 번호판 데이터 세트에 대해 미세 조정할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nYOLOv7는 정확성과 속도 측면에서 최신 기술인 실시간 객체 감지 알고리즘입니다. COCO 데이터셋에 미리 학습되어 있습니다.\n\n이 알고리즘에 대한 자세한 내용은 다음 논문에서 확인할 수 있어요: YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.\n\n![이미지](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_8.png)\n\n공식 저장소에서 YOLOv7 레포지토리를 복제해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ngit clone https://github.com/WongKinYiu/yolov7.git\ncd yolov7\n```\n\n요로 요구 사항은 위에서 설치한 프로젝트 요구 사항에 이미 흡수되었습니다.\n\nFine-tuning을 위해 YOLOv7의 사전 훈련된 작은 버전인 이미지 크기 640을 적용하겠습니다.\n\n```js\n# 사전 훈련된 가중치 다운로드\n!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기본 사전 훈련된 물체 탐지:\n\n![image](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_9.png)\n\nNumberplate Detection Model training\n\n커스텀 데이터셋에 대한 모델 훈련은 꽤 간단하고 직관적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n친구야, 좋은 GPU를 이용해 Google Colab에서 모델 파인 튜닝을 진행할 거야.\n\n시작하기 전에 단일 번호판 클래스로 적절한 데이터셋을 생성하고 레이블을 지정해야 해.\n\n나의 데이터셋은 나만의 사진을 기반으로 부분적으로 만들었으며 AUTO.RIA Numberplate Dataset에서 일부를 활용했어 (이 멋진 분들에게 감사합니다!). 총 2000장의 이미지를 사용했어.\n\n레이블링은 Yolo 포맷으로 roboflow 서비스를 통해 진행했어.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```yaml\ntrain: dataset/train\nval: dataset/valid\n# Classes\nnc: 1  # number of classes\nnames: ['numberplate']  # class names\n```\n\n모델을 훈련하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```yaml\npython train.py --epochs 25 --workers 8 --device 0 --batch-size 32 --data data/numberplates.yaml --img 640 640 --cfg cfg/training/yolov7.yaml --weights 'yolov7-tiny.pt' --name yolov7_tiny_numberplates --hyp data/hyp.scratch.tiny.yaml\n``` \n\nBaseline으로 25회의 에포크가 충분하다고 결정했어요.\n\n![AutomaticNumberPlateRecognitionwithRaspberryPi_11.png](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_11.png)\n\n추론:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_12.png\" /\u003e\n\n프로젝트의 첫 번째 버전으로는 충분해 보이지만, 실제 응용 프로그램 중 발견된 특수 사례들을 통해 나중에 업데이트할 수 있습니다.\n\nYOLOv7 디텍터를 위한 추상적인 간단한 래퍼 클래스를 생성하였습니다:\n\n```js\nclass Detector():\n    def __init__(self, model_weights, img_size=640, device='cpu', half=False, trace=True, log_level='INFO', log_dir = './logs/'):\n        # 초기화\n        self.model_weights = model_weights\n        self.img_size = img_size\n        self.device = torch.device(device)\n        self.half = half  # half = device.type != 'cpu'  # half precision only supported on CUDA\n        self.trace = trace  # 모델을 Traced-모델로 변환\n        self.log_level = log_level\n        if self.log_level:\n            self.num_log_level = getattr(logging, self.log_level.upper(), 20) ##log_level 입력 문자열을 로깅 모듈이 허용하는 값 중 하나로 변환합니다. 값이 없다면 20 - INFO로 설정됩니다.\n            self.log_dir = log_dir\n            log_formatter = logging.Formatter(\"%(asctime)s %(message)s\")\n            logFile = self.log_dir + 'detection.log'\n            my_handler = RotatingFileHandler(logFile, mode='a', maxBytes=25 * 1024 * 1024,\n                                             backupCount=10, encoding='utf-8', delay=False)\n            my_handler.setFormatter(log_formatter)\n            my_handler.setLevel(self.num_log_level)\n            self.logger = logging.getLogger(__name__)  \n            self.logger.setLevel(self.num_log_level)\n            self.logger.addHandler(my_handler)\n        # YOLO 모델의 경로를 추가합니다. ('weights.pt')를 로드할 때마다, pytorch는 path 환경 변수(models/yolo)에서 모델 구성을 찾습니다.\n        yolo_folder_dir = str(Path(__file__).parent.absolute()) +\"\\yolov7\" #  모델 폴더 경로\n        sys.path.insert(0, yolo_folder_dir)\n        # 모델 로드\n        self.model = attempt_load(self.model_weights, map_location=self.device)  # FP32 모델 로드\n        # 모델을 Traced-모델로 변환\n        if self.trace:\n            self.model = TracedModel(self.model, self.device, self.img_size)\n        # if half:\n        #     model.half()  # to FP16\n        # 이름과 색상 가져오기\n        self.names = self.model.module.names if hasattr(self.model, 'module') else self.model.names\n        if len(self.names) \u003e 1:\n            self.colors = [[0, 255, 127]] + [[random.randint(0, 255) for _ in range(3)] for _ in self.names[1:]]\n        else:\n            self.colors = [[0, 255, 127]]\n        sys.path.remove(yolo_folder_dir)\n    def run(self, inp_image, conf_thres=0.25):\n        # 추론 실행\n        # 데이터 로드\n        dataset = LoadImage(inp_image, device=self.device, half=self.half)\n        t0 = time.time()\n        self.file_name, self.img, self.im0 = dataset.preprocess()\n        # 추론\n        t1 = time.time()\n        with torch.no_grad():  # 그래디언트를 계산하면 GPU 메모리 누수가 발생할 수 있습니다\n            self.pred = self.model(self.img)[0]\n        t2 = time.time()\n        # NMS 적용\n        self.pred = non_max_suppression(self.pred, conf_thres=conf_thres)\n        t3 = time.time()\n        # 검출 처리\n        bbox = None  # 최대 Confidence를 가진 검출 객체의 바운딩 상자\n        cropped_img = None  # 최대 Confidence를 가진 검출 객체를 자른 이미지\n        det_conf = None  # 최대 Confidence를 가진 검출 객체의 신뢰 수준\n        self.det = self.pred[0]  # pred[0] - NMX suppr는 이미지 당 1개의 텐서를 반환합니다;\n        if len(self.det):\n            # img_size에서 im0 크기로 상자 크기 조정\n            self.det[:, :4] = scale_coords(self.img.shape[2:], self.det[:, :4], self.im0.shape).round()\n            # 결과 출력\n            print_strng = \"\"\n            for c in self.det[:, -1].unique():\n                n = (self.det[:, -1] == c).sum()  # 클래스 당 검출\n                print_strng += f\"{n} {self.names[int(c)]}{'s' * (n \u003e 1)}\"  # 문자열에 추가\n            # 시간 출력(추론 + NMS)\n            print(\n                f'{print_strng} 검출. ({(1E3 * (t1 - t0)):.1f}ms)-데이터 로드, ({(1E3 * (t2 - t1)):.1f}ms)-추론, ({(1E3 * (t3 - t2)):.1f}ms)-NMS')\n            # 디버그 모드이면 결과를 파일에 기록\n            if self.log_level:\n                self.logger.debug(\n                    f'{self.file_name} {print_strng} 검출. ({(1E3 * (t1 - t0)):.1f}ms)-Load data, ({(1E3 * (t2 - t1)):.1f}ms)-Inference, ({(1E3 * (t3 - t2)):.1f}ms)-NMS')\n                if self.logger.getEffectiveLevel() == 10:  # 레벨 10 = 디버그\n                    gn = torch.tensor(self.im0.shape)[[1, 0, 1, 0]]  # 정규화 gain whwh\n                    for *xyxy, conf, cls in reversed(self.det):\n                        # 바운딩 박스와 함께 xywh 형식으로 검출 저장\n                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # 정규화된 xywh\n                        line = (int(cls), np.round(conf, 3), *xywh)  # 라벨 형식\n                        self.logger.debug(f\"{self.file_name} {('%g ' * len(line)).rstrip() % line}\")\n            # 최대 Confidence를 가진 검출 찾기:\n            indx = self.pred[0].argmax(0)[\n                4]  # pred[0] - NMX suppr는 이미지 당 1개의 텐서를 반환; argmax(0)[4] - conf는 [x1,y1,x2,y2,conf,cls]에서 indx 4를 가짐\n            max_det = self.pred[0][indx]\n            # 검출 바운딩 상자와 해당 자른 이미지 수집\n            bbox = max_det[:4]\n            cropped_img = save_crop(max_det[:4], self.im0)\n            cropped_img = cropped_img[:, :, ::-1] # BGR to RGB\n            det_conf = max_det[4:5]\n        print(f'검출 총 시간: {time.time() - t0:.3f}s')\n        return {'file_name': self.file_name, 'orig_img': self.im0, 'cropped_img': cropped_img, 'bbox': bbox,\n                'det_conf': det_conf}\n```    \n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n디버깅 목적을 위해 로깅 감지 데이터를 파일에 활성화할 수 있는 기능을 추가했습니다. 각 파일의 최대 크기는 25Mb이며 최대 10개의 파일을 저장한 후 덮어쓰기합니다.\n\n현재 작업에서는 감지기가 가장 높은 신뢰 점수를 가진 단일 감지만 반환하도록 설정해야 합니다. 감지기는 원본 이미지, 해당 경계 상자와 함께 자르기 감지된 영역, 신뢰 점수, 그리고 디버깅을 용이하게 하기 위해 각 이미지마다 생성된 고유한 이름을 출력합니다.\n\n번호판 영역 이미지 전처리\n\n일반적으로 다음 단계는 특정 이미지 전처리(예: RGB에서 그레이스케일로 변환, 노이즈 제거, 침식 + 팽창, 임계 처리, 히스토그램 평활화 등)를 수행하여 다음 OCR 단계를 위해 준비하는 것입니다. 전처리 작업은 OCR 솔루션 및 촬영 조건에 매우 의존하며 이에 맞게 조정됩니다. 그러나 EasyOCR로 이 기준 버전을 수행 중이며(나중에 사용자 지정 솔루션으로 대체해야 합니다), 저는 그레이스케일 변환 및 투영 프로필 방법을 이용한 기울기 보정이라는 두 가지 범용적인 단계로 전처리를 제한하기로 결정했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서는 평면 각도 보정을 사용하고 있지만 나중에는 원래의 번호판 모서리 탐지기와 호모그래피 계산 및 원근 변환을 사용한 보정으로 업데이트해야 합니다.\n\n```js\n# Skew Correction (projection profile)\ndef _find_score(arr, angle):\n    data = rotate(arr, angle, reshape=False, order=0)\n    hist = np.sum(data, axis=1)\n    score = np.sum((hist[1:] - hist[:-1]) ** 2)\n    return hist, score\n\ndef _find_angle(img, delta=0.5, limit=10):\n    angles = np.arange(-limit, limit+delta, delta)\n    scores = []\n    for angle in angles:\n        hist, score = _find_score(img, angle)\n        scores.append(score)\n    best_score = max(scores)\n    best_angle = angles[scores.index(best_score)]\n    print(f'Best angle: {best_angle}')\n    return best_angle\n\ndef correct_skew(img):\n    # correctskew\n    best_angle = _find_angle(img)\n    data = rotate(img, best_angle, reshape=False, order=0)\n    return data\n```\n\n![이미지](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_13.png)\n\n위 이미지 처리 단계 이후에는 인식을 위해 충분히 좋은 이미지로 간주할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n번호판 인식 (OCR)\n\n기준으로 EasyOCR 솔루션을 사용하기로 결정했어요. 쓰기 편하고 인식 정확도가 높아서 그리고 지루한 테서랙트에 비해 내가 알고 있는 괜찮은 대체재인 것 같아서요)\n\nEasyOCR을 이용한 번호판 인식을 위한 간단한 래퍼 클래스:\n\n```js\nclass EasyOcr():\n    def __init__(self, lang = ['en'], allow_list = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ', min_size=50, log_level='INFO', log_dir = './logs/'):\n        self.reader = easyocr.Reader(lang, gpu=False)\n        self.allow_list = allow_list\n        self.min_size = min_size\n        self.log_level = log_level\n        if self.log_level:\n            self.num_log_level = getattr(logging, log_level.upper(),\n                                         20)  ## log_level 입력 문자열을 로깅 모듈에서 허용하는 값 중 하나로 변환하고, 만약 없으면 20(INFO)으로 설정\n            self.log_dir = log_dir\n            # 로거 설정\n            log_formatter = logging.Formatter(\"%(asctime)s %(message)s\")\n            logFile = self.log_dir + 'ocr.log'\n            my_handler = RotatingFileHandler(logFile, mode='a', maxBytes=25 * 1024 * 1024,\n                                             backupCount=10, encoding='utf-8', delay=False)\n            my_handler.setFormatter(log_formatter)\n            my_handler.setLevel(self.num_log_level)\n            self.logger = logging.getLogger(__name__)  \n            self.logger.setLevel(self.num_log_level)\n            self.logger.addHandler(my_handler)\n\n    def run(self, detect_result_dict):\n        if detect_result_dict['cropped_img'] is not None:\n            t0 = time.time()\n            img = detect_result_dict['cropped_img']\n            img = ocr_img_preprocess(img)\n            file_name = detect_result_dict.get('file_name')\n            ocr_result = self.reader.readtext(img, allowlist = self.allow_list, min_size=self.min_size)\n            text = [x[1] for x in ocr_result]\n            confid = [x[2] for x in ocr_result]\n            text = \"\".join(text) if len(text) \u003e 0 else None\n            confid = np.round(np.mean(confid), 2) if len(confid) \u003e 0 else None   \n            t1 = time.time()\n            print(f'인식된 번호판: {text}, 신뢰도: {confid}.\\nOCR 총 시간: {(t1 - t0):.3f}s')\n            if self.log_level:\n                # 디버그 모드일 때 결과를 파일에 작성\n                self.logger.debug(f'{file_name} 인식된 번호판: {text}, 신뢰도: {confid}, OCR 총 시간: {(t1 - t0):.3f}s.')\n\n            return {'text': text, 'confid': confid}\n        else:\n            return {'text': None, 'confid': None}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n디버그 목적으로 Detector와 마찬가지로 OCR 데이터를 파일에 기록할 수 있는 기능도 추가되었다.\n\n인식 모듈은 인식된 문자열과 신뢰도 점수를 반환합니다.\n\n검증 및 조치\n\n검출된 번호판에서 성공적으로 인식된 텍스트를 가져왔으면, 이를 확인하고 일부 조치를 취해야 합니다. 번호판 확인 단계에서 가장 합리적인 일은 고객이 업데이트하는 데이터베이스를 사용하는 것입니다. 이 데이터베이스는 매번 또는 하루에 한 번씩 읽어서 로컬 저장소에 목록을 저장할 것입니다. 현재 기준 버전에서 데이터베이스를 설정하지 않고 주요 기능에 집중하기로 결정했습니다. 대신 Google Sheets를 예시로 사용할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_14.png\" /\u003e\n\n이 순간에는 아무런 조치 단계가 설정되어 있지 않습니다. 허용 목록에 있는 라이선스 번호 확인 결과만 표시됩니다. 하지만 라즈베리파이를 사용하면 GPIO 제어 릴레이 스위치를 통해 어떤 하중이든 매우 쉽게 작동시킬 수 있습니다.\n\n시각화\n\n해결책을 편안하게 모니터링하고 디버그할 수 있도록 시각화 모듈을 추가했습니다. 이 모듈은 번호판 인식 프로세스 표시, 입력 이미지 저장, 검출된 번호판이 있는 자르기된 영역 및 출력 결과 이미지 표시를 처리합니다. 또한, e-ink 스크린에 번호판 영역 및 인식된 텍스트를 표시하는 기능을 추가했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재, 편의를 위해 이미지는 압축된 JPG로 저장되며 로그 폴더에 10800개의 이미지가 한정된 양으로 저장됩니다(폴더 최대 크기 약 500Mb). 프로덕션 솔루션에서 시각화가 필요하지 않으며, 디버깅을 위해 이미지는 NumPy ndarrays나 이진 문자열에 저장하는 것이 더 좋습니다.\n\n```js\nclass Visualize():\n    def __init__(self, im0, file_name, cropped_img=None, bbox=None, det_conf=None, ocr_num=None, ocr_conf=None, num_check_response=None, out_img_size=(720,1280), outp_orig_img_size = 640, log_dir ='./logs/', save_jpg_qual = 65, log_img_qnt_limit = 10800):\n        self.im0 = im0\n        self.input_img = im0.copy()\n        self.file_name = file_name\n        self.cropped_img = cropped_img\n        self.bbox = bbox\n        self.det_conf = det_conf\n        self.ocr_num = ocr_num\n        self.ocr_conf = ocr_conf\n        self.num_check_response = num_check_response\n        self.out_img_size = out_img_size\n        self.save_jpg_qual = save_jpg_qual\n        self.log_dir = log_dir\n        self.imgs_log_dir = self.log_dir + 'imgs/'\n        os.makedirs(os.path.dirname(self.imgs_log_dir), exist_ok=True)\n        self.crop_imgs_log_dir = self.log_dir + 'imgs/crop/'\n        os.makedirs(os.path.dirname(self.crop_imgs_log_dir), exist_ok=True)\n        self.orig_imgs_log_dir = self.log_dir + 'imgs/inp/'\n        os.makedirs(os.path.dirname(self.orig_imgs_log_dir), exist_ok=True)\n        self.log_img_qnt_limit = log_img_qnt_limit\n\n        # Create blank image\n        h, w = self.out_img_size\n        self.img = np.zeros((h, w, 3), np.uint8)\n        self.img[:, :] = (255, 255, 255)\n\n        # Draw bounding box on top the image\n        if (self.bbox is not None) and (self.det_conf is not None):\n            label = f'{self.det_conf.item():.2f}'\n            color = [0, 255, 127]\n            plot_one_box(self.bbox, self.im0, label=label, color=color, line_thickness=3)\n\n        # Resize img width to fit the plot, keep origin aspect ratio\n        h0, w0 = im0.shape[:2]\n        aspect = w0 / h0\n        if aspect \u003e 1:  # horizontal image\n            new_w = outp_orig_img_size\n            new_h = np.round(new_w / aspect).astype(int)\n        elif aspect \u003c 1:  # vertical image\n            new_h = outp_orig_img_size\n            new_w = np.round(new_h * aspect).astype(int)\n        else:  # square image\n            new_h, new_w = outp_orig_img_size, outp_orig_img_size\n        self.im0 = cv2.resize(self.im0, (new_w, new_h), interpolation=cv2.INTER_AREA)\n        im0_h, im0_w = self.im0.shape[:2]\n\n        # Add original full image\n        im0_offset = 0\n        self.img[im0_offset:im0_h + im0_offset, im0_offset:im0_w + im0_offset] = self.im0\n\n        # Add cropped image with detected number bbox\n        if self.cropped_img is not None:\n            # Resize cropped img\n            target_width = int((w - (im0_w + im0_offset)) / 3)\n            r = target_width / self.cropped_img.shape[1]\n            dim = (target_width, int(self.cropped_img.shape[0] * r))\n            self.cropped_img = cv2.resize(self.cropped_img, dim, interpolation=cv2.INTER_AREA)\n            crop_h, crop_w = self.cropped_img.shape[:2]\n            # Add cropped img\n            crop_h_y1 = int(h/7)\n            crop_w_x1 = im0_w + im0_offset + int((w - (im0_w + im0_offset) - crop_w) / 2)\n            self.img[crop_h_y1:crop_h + crop_h_y1, crop_w_x1:crop_w + crop_w_x1] = self.cropped_img\n            # Add `_det` to filename\n            self.file_name = Path(self.file_name).stem + \"_det\" + Path(self.file_name).suffix\n\n        # Add ocr recognized number\n        if self.ocr_num is not None:\n            label = f\"{self.ocr_num} ({self.ocr_conf})\"\n            t_thickn = 2  # text font thickness in px\n            font = cv2.FONT_HERSHEY_SIMPLEX  # font\n            fontScale = 1.05\n            # calculate position\n            text_size = cv2.getTextSize(label, font, fontScale=fontScale, thickness=t_thickn)[0]\n            w_center = int((im0_w + im0_offset + w)/2)\n            ocr_w_x1 = int(w_center - text_size[0]/2)\n            ocr_h_y1 = int(crop_h_y1 + crop_h + 55)\n            org = (ocr_w_x1, ocr_h_y1)  # position\n            # Plot text on img\n            cv2.putText(self.img, label, org, font, fontScale,  color=(0, 0, 0), thickness=t_thickn, lineType=cv2.LINE_AA)\n\n        # Add number check response if in allowed list\n        if self.num_check_response == 'Allowed':\n            label = \"-=Allowed=-\"\n            fontColor = (0,255,0)\n        else:\n            label = \"-=Prohibited!=-\"\n            fontColor = (0,0,255)\n        t_thickn = 2  # text font thickness in px\n        font = cv2.FONT_HERSHEY_SIMPLEX  # font\n        fontScale = 1.05\n        # calculate position\n        text_size = cv2.getTextSize(label, font, fontScale=fontScale, thickness=t_thickn)[0]\n        w_center = int((im0_w + im0_offset + w) / 2)\n        response_w_x1 = int(w_center - text_size[0] / 2)\n        response_h_y1 = int(h*3/7) #TBD\n        org = (response_w_x1, response_h_y1)  # position\n        # Plot text on img\n        cv2.putText(self.img, label, org, font, fontScale, color=fontColor, thickness=t_thickn, lineType=cv2.LINE_AA)\n\n    def show(self):\n        # Show the image\n        cv2.imshow('image', self.img)\n\n    def save(self):\n        # Remove oldest file if reach quantity limit\n        if self.get_dir_file_quantity(self.imgs_log_dir) \u003e self.log_img_qnt_limit:\n            oldest_file = sorted([self.imgs_log_dir+f for f in os.listdir(self.imgs_log_dir)])[\n                0]  \n            os.remove(oldest_file)\n        # Write compressed jpeg with results\n        cv2.imwrite(f\"{self.imgs_log_dir}{self.file_name}\", self.img, [int(cv2.IMWRITE_JPEG_QUALITY), self.save_jpg_qual])\n\n    def save_input(self):\n        if self.input_img is not None:\n            # Remove oldest file if reach quantity limit\n            if self.get_dir_file_quantity(self.orig_imgs_log_dir) \u003e self.log_img_qnt_limit:\n                oldest_file = sorted([self.orig_imgs_log_dir+f for f in os.listdir(self.orig_imgs_log_dir)])[\n                    0]  \n                os.remove(oldest_file)\n            # Write compressed jpeg with results\n            cv2.imwrite(f\"{self.orig_imgs_log_dir}orig_inp_{self.file_name}\", self.input_img)\n\n    def save_crop(self):\n        if self.cropped_img is not None:\n            # Remove oldest file if reach quantity limit\n            if self.get_dir_file_quantity(self.crop_imgs_log_dir) \u003e self.log_img_qnt_limit:\n                oldest_file = sorted([self.crop_imgs_log_dir+f for f in os.listdir(self.crop_imgs_log_dir)])[\n                    0]  \n                os.remove(oldest_file)\n            # Write compressed jpeg with results\n            cv2.imwrite(f\"{self.crop_imgs_log_dir}crop_{self.file_name}\", self.cropped_img)\n\n    def display(self):\n        # Display img using e-ink display 176*264\n        disp_img = np.zeros((epd2in7.EPD_WIDTH, epd2in7.EPD_HEIGHT,3), np.uint8)\n        disp_img[:, :] = (255, 255, 255)\n        \n        if self.cropped_img is not None:\n            # Add cropped number\n            crop_resized = cv2.resize(self.cropped_img, (epd2in7.EPD_HEIGHT-4, 85), interpolation=cv2.INTER_AREA)\n            crop_resized_h, crop_resized_w = crop_resized.shape[:2]\n            crop_w_x1 = int(epd2in7.EPD_HEIGHT/2 - crop_resized_w/2)\n            disp_img[2:crop_resized_h+2, crop_w_x1:crop_resized_w+crop_w_x1] = crop_resized\n        \n        if\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지 이룩한 것을 시험해 봅시다. 정지 이미지에서의 탐지 및 인식 파이프라인:\n\n![image1](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_15.png)\n\n길거리에서 기기 카메라를 사용한 종단간 솔루션 테스트:\n\n![image2](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_16.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```\n![Image 1](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_17.png)\n\n![Image 2](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_18.png)\n\n![Image 3](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_19.png)\n\nPerformance\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 구성으로는 감지에 약 700~800ms, OCR 단계에 약 900~1200ms가 소요되며, 평균 FPS는 약 0.4~0.5입니다.\n\n![이미지](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_20.png)\n\n현재 주차 장벽 자동화 프로젝트에는 이러한 프레임 속도 값이 중요하지 않지만, 개선할 여지가 분명히 많습니다.\n\nhtop에서 CPU 활용률이 거의 100%에 가깝다는 것을 알 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_21.png)\n\n모든 테스트는 Raspberry Pi OS의 기본 설정으로 수행되었습니다. UI를 비활성화하고 기본적으로 활성화된 다른 백그라운드 서비스를 모두 제거하면 성능과 안정성이 높아집니다.\n\n보너스\n\n추가 조정 없이도 우리의 감지기 모듈은 LEGO 자동차의 번호판을 완벽하게 감지할 수 있다는 것이 밝혀졌습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지1](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_22.png)\n\n![이미지2](/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_23.png)\n\n그래서 레고를 아들에게 빌려 Raspberry Pi Build Hat을 사용하여 나만의 주차장 바리어를 만들기로 결정했고, \"실제\" 조건에서 완전한 엔드 투 엔드 테스트를 제공하기로 했습니다.\n\nLEG 월드 햇 프로프라이어터리 라이브러리를 기반으로 한 Action 모듈용 간단한 랩퍼:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nclass Action():\n    def __init__(self):\n        self.motor = Motor('A')\n        self.motor.set_default_speed(25)\n        self.matrix = Matrix('B')\n        self.ok_color = [[(6, 10) for x in range(3)] for y in range(3)]\n        self.nok_color = [[(9, 10) for x in range(3)] for y in range(3)]\n        self.matrix.set_transition(2) #fade-in/out\n        self.matrix.set_pixel((1, 1), (\"blue\", 10))\n\n    def _handle_motor(self, speed, pos, apos):\n        print(\"Motor:\", speed, pos, apos)\n\n    def run(self, action_status):\n        while True:\n            if action_status[0] == 'Allowed':\n                self.matrix.set_pixels(self.ok_color)\n                time.sleep(1)\n                self.motor.run_for_degrees(-90, blocking=False)\n                time.sleep(5)\n                self.motor.run_for_degrees(90, blocking=False)\n                time.sleep(1)\n            elif action_status[0] == 'Prohibited':\n                self.matrix.set_pixels(self.nok_color)\n                time.sleep(3)\n            else:\n                self.matrix.clear()\n                self.matrix.set_pixel((1, 1), (\"blue\", 10))\n                time.sleep(1)\n                self.matrix.set_pixel((1, 1), (0, 10))\n                time.sleep(1)\n```\n\nMain 프로그램에서 action_status가 감지되고 변경될 때 메인 프로그램에서 액션을 트리거하여 병렬 스레드에서 이 모듈을 실행합니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_24.png\" /\u003e\n\nLEGO 번호판 중 하나를 Google 시트 \"데이터베이스\"에 추가했으므로 이제 모든 조각들을 함께 조합하여 실행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*ZHTFqk1E0pKLGAht0W_mnw.gif\" /\u003e\n\n# 최종 결론\n\n전반적으로 라즈베리 파이를 사용하여 주차장 장벽을 제어하기 위한 자동 번호판 인식 시스템을 완전히 구현하는 데 성공했습니다.\n\n강조해야 할 문제 중 하나는 처리 속도가 느린 관계로 이미지 지연이 발생할 수 있다는 점입니다. 카메라는 자체 버퍼가 있으며 이미지를 느린 속도로 캡처하는 동안 씬이 변경되어도 버퍼에서 여전히 \"이전\" 프레임을 읽는 문제가 있습니다. 현재 사용 사례에서는 그다지 중요하지 않지만 개선을 위해 전체 처리 시간과 거의 동일한 간격으로 프레임 스킵을 추가했습니다. 이렇게 하면 더 빠른 프레임 읽기와 버퍼의 정리가 가능하며 CPU의 부하를 줄일 수 있습니다. 그러나 지연 없이 거의 실시간 스무스한 이미지 스트리밍이 필요하다면 최상의 옵션은 카메라 읽기를 별도의 병렬 스레드로 설정하여 버퍼에서 가능한 최대 속도로 프레임을 읽도록 하는 것이며, 주 프로그램이 필요 시에만 이 프로세스에서 프레임을 가져 올 수 있도록 합니다. 그러나 파이썬에서 멀티 스레딩은 실제 다중 프로세스 처리가 아니라 아키텍처적 관점에서 코드를 보다 명확하게 조직화하고 실행하는 데 도움이 되는 시뮬레이션인 것을 기억해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 추가 단계\n\n- OCR. 현재 병목 현상인 OCR을 빠르게 처리할 수 있도록 개선해보세요. 나는 속도를 올리기 위해 작은 커스텀 RNN 기반 모델을 개발하기로 했습니다. 시간이 중요하지 않고 정확도만 필요한 경우 EasyOCR에서 다양한 모델을 사용하고 이를 여러분의 사례에 맞게 튜닝할 수 있습니다. 또는 WPOD-NET과 같은 다른 솔루션을 시도해볼 수도 있습니다. 또한 인식 품질을 향상시키는 중요한 포인트로는 정확한 사용 사례에 맞게 이미지 전처리를 조정하는 것이 있습니다.\n- Detector. 속도를 높이기 위해 카메라가 근거리에 있는 자동차에서만 작업해야 하는 경우 해상도가 높은 이미지가 필요하지 않습니다. 또 다른 옵션은 가능하다면 카메라와 차량의 위치가 대략 고정되어 있다면 전체 프레임이 아닌 번호판이 위치할 것으로 예상되는 영역만 캡처할 수 있습니다.\n\n나중에 이 두 모델 모두 전이 학습, 양자화, 가지치기 및 기타 방법을 사용하여 경량화하고 엣지 장치에서 더 빠르고 가벼운 작동이 가능하도록 할 수 있습니다.\n\n그러나 아무리 빠른 실시간 처리가 필수적이더라도 (물론 자동 주차 장벽에는 해당하지 않을 것입니다), 텐서 코어가 있는 장치가 없으면 NVIDIA Jetson과 같은 장치가 없으면 속도와 품질 사이에 항상 트레이드오프가 존재할 것입니다. CPU 전용 장치에서는 항상 속도와 품질 사이의 교환 관계가 발생할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 개선할 수 있는 다른 옵션이 있어요 — 현재 상황에서는 CPU를 24시간 7일 돌릴 필요가 없어요. 자동차가 다가올 때만 PIR 또는 IR 센서에 의해 카메라가 작동될 수 있어요.\n\n다음 번에 구현해보려고 하는 마지막 포인트 — 솔루션을 마이크로서비스로 전환하고 생산자-소비자 데이터 흐름 패턴을 구현할 거예요.\n\n그럼 이만 하겠습니다. 이 긴 지루한 프로젝트 구현 설명을 읽어 주셔서 감사해요.\n\n건강하게 지내시고 우크라이나를 응원해주세요 ❤.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 프로젝트에서 사용된 장비에 대한 링크:\n\n- Raspberry Pi 4 Model B 4GB\n- Raspberry Pi Camera Module v2\n- Raspberry Pi 4 Aluminum Case with Dual Cooling Fan\n- GeeekPi(52pi) Raspberry Pi UPS EP-0136\n- 264x176 2.7인치 E-Ink 디스플레이 HAT for Raspberry Pi\n- Raspberry Pi Build HAT\n- LEGO 3x3 컬러 라이트 매트릭스\n- LEGO 작은 테크닉 직각 모터","ogImage":{"url":"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_0.png"},"coverImage":"/assets/img/2024-06-20-AutomaticNumberPlateRecognitionwithRaspberryPi_0.png","tag":["Tech"],"readingTime":30},{"title":"라즈베리 파이가 이제 Hailo-8L M2 모듈로 AI를 처리할 준비가 되었습니다","description":"","date":"2024-06-20 17:41","slug":"2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module","content":"\n\n## 가젯\n\n![라즈베리파이](/assets/img/2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module_0.png)\n\n인공지능은 매우 높은 에너지와 자원을 필요로 한다는 이미지를 가지고 있습니다. 그래서 강력하고 고용량의 기계에 전념되어 왔습니다. 많은 기업들이 클라우드 및 전용 솔루션에 솔루션을 선택하였습니다.\n\n이 상황을 극복하기 위한 것이 라즈베리 파이 AI 키트입니다. 잘 알려진 마이크로 컴퓨터와 함께 만들어진 제안서입니다. 라즈베리 파이의 최신 기술과 Hailo사의 M.2 HAT+가 함께 포함되어 있으며, 라즈베리 파이 5 위에 Hailo-8L AI 가속기 모듈이 사전에 조립되어 있습니다. 이 제품은 예상대로 70달러에 구매 가능합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n복잡한 AI 비전 앱을 만들 수 있게 하여, 실시간으로 실행되고 저 지연시간 및 저 전력 소비로 동작하는 앱도 가능해질 것입니다. Hailo-8L 공변처리기는 물체 감지, 의미 및 인스턴스 세분화, 자세 추정, 얼굴 태깅 등의 인공 신경망을 실행할 수 있습니다. 이로써 주CPU는 다른 작업을 처리할 수 있게 됩니다.\n\n그 결과 이 제안은 라즈베리 파이 사용자들의 요구를 충족할 수 있을 것입니다. 이 제품의 낮은 가격은 이 마이크로컴퓨터의 철학과 일치하며, 이제까지 존재하지 않았던 많은 기능과 새로운 기능을 제공할 수 있습니다.\n\n![2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module_1.png](/assets/img/2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module_1.png)\n\n이는 AI가 매우 흥미로운 영역 중 하나를 더 다루는데 도움이 될 것입니다. 소규모 프로젝트에 대해 이 솔루션이 매우 저렴하고, 이제 이미지를 인식하고 해석된 이미지에 대해 작용할 수 있는 능력을 지니고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n동일한 시간에, 이 새로운 기능을 통해 라즈베리 파이는 다시 한 번 독특한 위치에 있습니다. 사용자들은 더 완벽하고 훨씬 더 비싼 솔루션을 필요로하지 않고 매우 중요한 것에 대처할 수 있는 인공지능 솔루션에 접근할 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module_0.png"},"coverImage":"/assets/img/2024-06-20-RaspberryPiisnowreadytohandleAIwiththeHailo-8LM2module_0.png","tag":["Tech"],"readingTime":2},{"title":"라즈베리 파이 5 CPU 성능","description":"","date":"2024-06-20 17:40","slug":"2024-06-20-RaspberryPi5CPUPerformance","content":"\n\n라즈베리 파이 기기들은 사용하기 정말 재미있어요. 가격도 저렴하고 성능도 좋아서 다양한 문제를 해결하는 방법에 대한 예제를 쉽게 찾을 수 있어요. Pi 5가 9월 28일에 발표되었을 때 저는 그것을 만져보고 싶었어요.\n\n![라즈베리 파이 5 CPU 성능](/assets/img/2024-06-20-RaspberryPi5CPUPerformance_0.png)\n\n저는 대부분의 프로젝트가 CPU에 의존적이기 때문에 CPU 성능을 측정하여 시작했어요. 아래에서 Pi 5를 최근의 Intel 13세대, SiPeed Lichee Pi 4A, Microsoft/Qualcomm SQ3, Apple M1, 이전 라즈베리 파이 모델 4와 3B, 그리고 열년 전의 Intel i7-4770K 칩과 비교해보았어요.\n\n벤치마크에는 다양한 환경에서 작성, 빌드 및 실행된 하나의 앱을 사용했어요. 이 앱은 간단합니다 - 상대방이 유능하다면 틱택토에서 이길 수 없다는 것을 증명합니다. 이는 알파/베타 가지치기 알고리즘을 사용하여 3가지 고유한 시작 수를 평가합니다. 6493개의 판 상태가 검토되었어요. 변형에는 다음이 포함돼요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 각 대상 CPU에 대한 원시 어셈블리 버전이 있습니다. 코드를 더 최적화할 수 있다고 생각하지만, 각 CPU의 명령어 세트와 레지스터를 활용하려고 노력했습니다. Arm32, Arm64, RISC-V 64 및 AMD64와 같은 다양한 변형이 있습니다.\n- 6502, 8080, 8086 및 RISC-V 64 CPU에 대한 원시 어셈블리 버전은 다양한 에뮬레이터에서 실행됩니다: NTVAO (6502 + Apple 1), NTVCM (8080 + CP/M 2.2), NTVDM (8086 + DOS 3.3), RVOS (RISC-V 64 + Linux). 모든 에뮬레이터는 C++로 작성되었으며 대상 플랫폼의 원시 컴파일러를 사용하여 생성되었습니다.\n- 알고리즘의 C++ 버전이 있습니다. 플랫폼의 기본 컴파일러를 사용했습니다 — Windows의 경우 Microsoft, MacOS의 경우 clang, Linux 배포판의 경우 Gnu를 사용했습니다. Gnu 및 clang는 Windows를 대상으로 할 수 있으며 Microsoft의 컴파일러보다 훨씬 빠른 코드(일반적으로 20% 이상)를 생성합니다. 하지만 대부분의 사람들이 기본값으로 사용할 것으로 생각하여 해당 컴파일러를 선택했습니다.\n\n![RaspberryPi5CPUPerformance_1](/assets/img/2024-06-20-RaspberryPi5CPUPerformance_1.png)\n\n![RaspberryPi5CPUPerformance_2](/assets/img/2024-06-20-RaspberryPi5CPUPerformance_2.png)\n\nRaspberry Pi 5는 이전 Pi 버전보다 상당히 빠릅니다. 이를 통해 많은 새로운 Pi 솔루션이 가능해질 것으로 기대됩니다. 기대되는 결과물이 무엇인지 기대됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n벤치마크 결과에 대한 몇 가지 참고 사항:\n\n- 모든 시간은 틱택토에서 이기지 못 한 것을 증명하는 한 번의 반복에 대한 밀리초로 표시됩니다.\n- Pi 5에 대해 5볼트 3암프 어댑터를 사용했습니다. 이 기기는 5볼트 5암프를 요구하며 제 어댑터로 화면의 알림 영역에 \"저전압\" 오류가 표시됩니다. 공식 어댑터는 Pi 5 공급 업체에서 구할 수 있지만 현재는 드물습니다. 그리고 5암프를 전달하는 일반 어댑터를 쉽게 찾을 수 없었습니다. 제 집에 있는 열두 개의 USB 어댑터는 모두 5V에서 1에서 3 암프를 제공합니다. 낮은 전력 어댑터를 인식하면 Pi 5가 언더클럭될 수 있기 때문에 실제 어댑터를 사용하면 벤치마크 시간이 개선될 수 있습니다. 업데이트: 공식 Pi 5 어댑터는 5.1V에서 5암프입니다. 이 어댑터를 사용하면 더 이상 \"저전압\" 경고를 받지 않습니다. 싱글 코어 성능은 동일하지만 3코어 성능은 11% 이상 향상되었습니다. 이를 반영하기 위해 위의 표에 새 열을 추가했습니다.\n- Pi 5는 Apple M1보다 두 배 정도 느립니다. 실제로 아주 빠릅니다.\n- 위에서 언급한 대로, AMD64용 Microsoft C++ 컴파일러는 clang과 Gnu 컴파일러보다 나쁩니다. 그러나 Arm64에 대한 성능 차이는 훨씬 더 커집니다. Microsoft/Qualcomm SQ3는 Arm64 어셈블러 코드를 M1과 대략 같은 속도로 실행하지만 C++ 앱은 현격히 느립니다. 이로 인해 Pi 5는 CPU가 덜 강력함에도 불구하고 윈도우 기기와 경쟁력이 높아졌습니다.\n- Intel i7-4770K는 Pi 5보다 10년 더 오래되었지만 Microsoft의 더 느린 컴파일러에도 불구하고 더 나은 성능을 발휘합니다.\n- 표에는 RVOS 에뮬레이터가 중첩된 버전을 실행하는 런타임이 나와있습니다. 여기서 Pi 5는 64비트 CPU를 32비트 기계에서 에뮬레이션하는 것이 비용이 많이 들기 때문에 이전 Pi 모델들보다 훨씬 우수한 성능을 보여줍니다. 64비트 OS가 탑재된 Pi 4는 32비트 OS보다 성능이 향상될 것이라고 생각합니다.\n\n이러한 벤치마크는 제게 중요한 시나리오에 대한 성능을 반영하며, Pi 5가 이전 버전보다 훨씬 빠르다는 것을 명백하게 보여줍니다. 이제 더 나은 전원 공급원을 찾아봐야겠어요.\n\n(참고: 틱택토 구현의 소스 코드는 https://github.com/davidly/ttt 에 있습니다. 에뮬레이터 코드는 https://github.com/davidly/ntvao, https://github.com/davidly/ntvcm, https://github.com/davidly/ntvdm, https://github.com/davidly/rvos 에서 찾을 수 있습니다.)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n(참고 2: 1970년대에 라디오 샥에서 구입한 부품들로 TTL 칩으로 만든 프로젝트에 전원을 공급하기 위해 5V 전원 공급기를 제작했습니다. 50년 후에 다시 5V 전원 공급기를 만들어야 할 것 같네요.)\n\n(참고 3: Anker USB 전원 공급기를 구입했는데, 5V를 4.5A로 공급할 수 있습니다. 그런데 라즈베리 파이 5에서 여전히 \"저전압 경고\" 알림이 표시되며 성능이 크게 향상되지 않습니다.)\n\n(참고 4: 라즈베리 파이 5에서 어셈블리 앱의 Arm32 버전을 실행했습니다. 1개와 3개의 스레드에 대한 시간은 각각 0.0919 및 0.0407입니다. Arm32는 Arm64보다 레지스터가 적기 때문에 느립니다만, 여전히 라즈베리 파이 4보다 약 2배 빠릅니다.)","ogImage":{"url":"/assets/img/2024-06-20-RaspberryPi5CPUPerformance_0.png"},"coverImage":"/assets/img/2024-06-20-RaspberryPi5CPUPerformance_0.png","tag":["Tech"],"readingTime":4},{"title":"가장 빠른 쿠버네티스 배포 방법일까요 라즈베리 파이 클러스터 구축에 대해 알아봅시다","description":"","date":"2024-06-20 17:38","slug":"2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster","content":"\n\n최근에 라즈베리 파이 4 싱글보드 컴퓨터를 사용하여 쿠버네티스 클러스터를 빠르고 쉽게 부트스트랩 할 수 있는 솔루션을 발견했어요. 이 솔루션은 다른 베어 메탈 클러스터에도 훌륭하게 작동해요.\n\n![이미지](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_0.png)\n\n# 문제점 - 왜 이것을 해야 할까요?\n\n쿠버네티스 클러스터를 설정하고 필요한 모든 단계를 수행하는 방법에 대한 이전 자습서를 확인할 수 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nk3sup를 사용하더라도 아직 시간이 많이 소요되고 오류가 발생하기 쉬운 것 같아요.\n\n# 더 나은 방법\n\nTalos-Linux 및 Kubernetes 및 Talos-Linux 커뮤니티의 놀라운 작업 덕분에 빠르고 쉬운 해결책을 얻을 수 있어요. (https://www.talos.dev/)\n\n## Talos Linux이란?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n태로스는 컨테이너를 최적화한 리눅스 배포판입니다. 쿠버네티스와 같은 분산 시스템을 위해 리눅스를 새롭게 상상해 만든 제품입니다. 실용성을 유지하면서 최대한 최소화된 디자인으로 제작되었습니다. 이러한 이유로 탈로스에는 몇 가지 고유한 기능들이 있습니다:\n\n- 변경할 수 없습니다\n- 원자적입니다\n- 일시적입니다\n- 최소화되어 있습니다\n- 기본적으로 보안이 설정되어 있습니다\n- 단일 선언 구성 파일과 gRPC API를 통해 관리됩니다\n\n탈로스는 컨테이너, 클라우드, 가상화, 그리고 베어 메탈 플랫폼에 배포할 수 있습니다.\n\n출처: https://www.talos.dev/v1.7/introduction/what-is-talos/\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 왜 Talos Linux를 사용해야 할까요?\n\n주로 API를 통해 제어할 수 있는 Kubernetes용 운영 체제입니다.\n\n- Kubernetes를 사용하려면 구성해야 할 우분투 또는 유사한 배포판을 사용할 필요가 없습니다. Talos는 오직 Kubernetes를 위해 만들어졌습니다!\n- Kubernetes 노드로 사용할 모든 장치는 Talos 이미지로 간편하게 로드됩니다.\n- 부팅 과정 이후, 모든 장치는 유지 보수 모드에서 시작되어 추가 명령을 실행할 준비가 됩니다.\n- talosctl 도구를 사용하여 Kubernetes 클러스터의 각 개별 노드가 어떻게 동작해야 하는지 구성할 수 있습니다.\n- 모든 노드에 SSH를 설정할 필요가 없습니다.\n- 모든 노드에 k3s 또는 유사한 것을 설치할 필요가 없습니다.\n- 제어 노드에서 토큰을 복사하고 작업자 노드를 설정하기 위해 초기화할 때 고려해야 하는 번거로움이 없습니다.\n- 호스트 컴퓨터로 kubectl 파일을 복사해야 하는 걱정을 할 필요가 없습니다.\n\n## Talos-Bootstrap\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTalos-Linux과 talosctl은 Kubernetes 클러스터 초기화를 매우 간단하게 만듭니다. Talos의 API 중심적인 특성으로 인해 다른 도구들은 이러한 API를 사용하여 초기화를 더욱 간단하게 할 수 있습니다.\n\naenix-io의 놀라운 팀에서 만든 도구인 talos-bootstrap이 있습니다.\n\n우리는 이 도구를 사용하여 기록 시간 안에 Kubernetes를 설정할 것입니다.\n\n## 파이들을 준비하세요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 명령어를 사용하여 Talos 이미지를 다운로드하세요. (https://www.talos.dev/v1.7/talos-guides/install/single-board-computers/rpi_generic/#download-the-image)\n\n```bash\ncurl -LO https://factory.talos.dev/image/ee21ef4a5ef808a9b7484cc0dda0f25075021691c8c09a276591eedb638ea1f9/v1.7.0/metal-arm64.raw.xz\nxz -d metal-arm64.raw.xz\n```\n\n라즈베리 파이 장치에 이미지를 플래싱하세요.\n\n저는 Balena Etcher를 사용하고 있지만 별도의 플래시 도구를 사용할 수도 있습니다. 한 장치마다 요렇게 하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다운로드한 탈로스 이미지를 라즈베리 파이용으로 선택해주세요.\n\n![Talos Image 1](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_1.png)\n\n![Talos Image 2](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_2.png)\n\n대상 드라이브를 선택하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 3](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_3.png)\n\nFinally flash the image\n\n![Image 4](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_4.png)\n\n![Image 5](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_5.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Talos Tools 준비 사항\n\n먼저 talosctrl 도구를 설치해야 합니다.\n\n```js\ncurl -sL https://talos.dev/install | sh\n```\n\n다음 단계에서는 talos-bootstrap 도구를 설치하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```bash\ncurl -LO https://github.com/aenix-io/talos-bootstrap/raw/master/talos-bootstrap\nchmod +x ./talos-bootstrap\nsudo mv ./talos-bootstrap /usr/local/bin/talos-bootstrap\n```\n\n## 보너스\n\n전체 과정을 간편화하기 위해 개발 컨테이너 환경을 만들었습니다.\n\n- devcontainers 확장 프로그램을 설치하세요 (https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers).\n- 위의 저장소를 복제하세요.\n- VSCode로 이 저장소를 열어보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 Macbook Pro M1에서 Orbstack을 Docker 런타임으로 사용하여 전체 프로세스를 테스트했어요. 그러나 MacOS와 Linux에서 다른 Docker 런타임을 사용해도 잘 작동해야 합니다.\n\n## 클러스터 부트스트랩\n\n클러스터를 부트스트랩하려면 다음 명령을 실행하세요:\n\n```js\ntalos-bootstrap install\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_6.png)\n\n![Image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_7.png)\n\nAfter some time, the talos-bootstrap should find your Raspberry Pi nodes in talos maintenance mode. Select your first node:\n\n![Image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_8.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼 역할을 선택할 수 있어요, 먼저 controlplane으로 시작할게요.\n\n![control node](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_9.png)\n\n해당 control 노드에 대한 호스트명을 입력해주세요.\n\n![hostname](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n탈로스를 설치할 디스크를 선택해주세요:\n\n![이미지](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_11.png)\n\n저는 HDD와 USB 스틱이 연결되어 있습니다. 더 큰 240GB 디스크를 선택했습니다.\n\n탈로스가 사용해야 할 네트워크 인터페이스를 선택해주세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Node custom address](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_12.png)\n\nSelect a custom address for your node.\n\n![Gateway address](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_13.png)\n\nSet your gateway address. In most cases, it's your router address.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n아래는 Markdown 형식으로 변경한 텍스트입니다.\n\n![이미지](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_14.png)\n\n기본 DNS 서버를 선택하세요.\n\n![이미지](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_15.png)\n\n다음 단계에서 클러스터에 대한 VIP(가상 공유 IP)를 선택할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n아래는 데모를 위해 두 개의 노드 클러스터를 설정한 주소입니다:\n\n- 192.168.2.81\n- 192.168.2.82\n\nVIP 주소로 192.168.2.240를 선택했습니다. 또한 이는 DHCP 범위를 벗어난 곳에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 탈로스와 함께 제공되는 항목입니다. 아래와 같이 설명되어 있습니다.\n\n원본\n\n간단히 말해서, 클러스터 액세스를 위한 주소를 정의할 수 있습니다. 이는 실제 노드 주소와는 독립적입니다. 탈로스-리눅스의 매우 편리한 기능입니다.\n\nKubernetes 엔드포인트를 사용자 정의하십시오:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같이 설정이 되었는지 확인해 주세요:\n\n![Configuration 1](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_17.png)\n\n다음 대화 상자에서 \"예\"를 선택해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_19.png)\n\n![image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_20.png)\n\n다이얼로그를 완료하면 talos-boostrap을 실행하는 경로에 kubeconfig 파일이 생성됩니다.\n\n![image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_21.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 테스트\n\n```js\nexport KUBECONFIG=./kubeconfig\nkubectl get nodes\n```\n\n다음과 같이 출력됩니다:\n\n```js\nNAME          STATUS   ROLES           AGE     VERSION\nk8s-control   Ready    control-plane   4m55s   v1.30.1\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n컨트롤 플레인이 성공적으로 생성되었습니다!\n\n## 워커 노드\n\n워커 노드의 경우, 대부분의 경우 작업 노드 역할을 선택하는 것을 제외하고 대부분 동일한 단계가 수행됩니다:\n\n![이미지](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_22.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_23.png)\n\n![image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_24.png)\n\n![image](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_25.png)\n\n## Testing\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n워커 노드에 시간을 주고 다시 다음을 실행해보세요:\n\n```js\nexport KUBECONFIG=./kubeconfig\nkubectl get nodes\n```\n\n이렇게 하면 클러스터 내 양쪽 노드를 볼 수 있을 겁니다:\n\n![노드 이미지](/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_26.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 요약\n\n지침을 읽어보면 모든 단계에 대해 설명이나 주석을 다 해 놓았기 때문에 클러스터 설정이 길어 보일 수 있지만, 제가 말하건대로 (모든 것이 순조롭게 진행된다면 🤞🏻) 5~10분 안에 설정이 완료됩니다. 🚀\n\n이제 완전히 작동하는 클러스터를 갖추었으므로 인그레스, 스토리지, 인증서 또는 flux-cd와 같은 다른 유용한 구성 요소를 설치할 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_0.png"},"coverImage":"/assets/img/2024-06-20-IsThistheFastestWaytoDeployKubernetesDiveintoSettingUpaRaspberryPiCluster_0.png","tag":["Tech"],"readingTime":8},{"title":"홈 어시스턴트에 MQTT 서버를 설치하고 추가하는 방법","description":"","date":"2024-06-20 17:36","slug":"2024-06-20-InstallandaddingMQTTservertotheHomeAssistant","content":"\n\n만약 홈 어시스턴트 인스턴스가 있으면 Mosquitto가 유용할 수 있습니다. Mosquitto를 사용하면 이 프로토콜을 지원하는 여러 IoT 장치를 연결할 수 있습니다. 예를 들어, OSS 펌웨어를 실행할 수 있는 Tasmota, ESPHome, OpenBeken 등이 있습니다.\n\n나는 라즈베리 파이에서 홈 어시스턴트를 사용하고 있어서 이를 몇 단계만 거쳐 쉽게 설정하는 방법을 보여줄게요. 만약 아직 홈 어시스턴트를 설치하지 않았다면, 이 문서를 확인하세요.\n\n## MQTT 서버 설치\n\n이 과정은 간단하며, 터미널에 간단한 명령어 한 줄로 설치할 수 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsudo apt-get install mosquitto mosquitto-clients \n```\n\n이게 전부에요 🐧.\n\n이제 Mosquitto 브로커에 대한 액세스를 보호하는 것이 중요합니다.\n\n## MQTT 시작 및 부팅 시 추가\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금 MQTT를 시작할 수 있고, 장치 부팅 후 내장 서비스를 사용할 수 있습니다:\n\n```js\nsudo systemctl enable mosquitto\nsudo systemctl start mosquitto\n```\n\n## 사용자 및 비밀번호 생성\n\nmosquitto_passwd를 사용하여 새로운 사용자 이름과 비밀번호를 만들 수 있습니다. YOUR_MQTT_USER를 사용하려는 사용자로 대체해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsudo mosquitto_passwd -c /etc/mosquitto/passw YOUR_MQTT_USER\n```\n\n암호를 설정하고 기억하세요.\n\n이제 익명 사용자를 비활성화해야 합니다:\n\n```js\necho -e \"allow_anonymous false\\npassword_file /etc/mosquitto/passw\" | sudo tee -a sudo nano /etc/mosquitto/mosquitto.conf\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 Mosquitto 브로커를 다시 시작해주세요:\n\n```js\nsudo systemctl restart mosquitto\n```\n\n좋아요, 이제 Home Assistant를 구성해보겠습니다!\n\n## Home Assistant MQTT 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내장된 HASS 통합을 사용하여 MQTT 브로커에 쉽게 연결할 수 있어요. \"설정\" - \"장치 및 서비스\" - \"+ 통합 추가\"로 이동하신 후 MQTT를 목록에서 찾아보세요. MQTT 브로커 세부 정보를 사용하여 연결하세요.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*KCF19chIplt1xq_1AQr2bw.gif)\n\n이제 Mosquitto 호환 장치를 홈 어시스턴트에서 사용할 수 있어요 👌.\n\n## localhost 외부에서 듣기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 라즈베리 파이 밖에서 MQTT를 사용하여 LAN 또는 인터넷에서 IoT 장치를 설정하려는 경우, Mosquitto 브로커가 아웃바운드 연결 요청을 수신하도록 설정해야 합니다:\n\n```sh\necho -e \"listener 1883\" | sudo tee -a sudo nano /etc/mosquitto/mosquitto.conf\n```\n\n## Tasmota 예시\n\nTasmota를 실행 중인 장치가 있다면 Home Assistant에서 MQTT를 사용하여 해당 장치를 제어할 수 있습니다. Tasmota에서 MQTT를 설정하려면 \"Configuration\" - \"Configure MQTT\"로 이동한 후 MQTT 브로커 데이터를 추가하십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n라즈베리 파이의 로컬 네트워크 IP로 YOUR_MQTT_DEVICE_IP를 대체하고, 위에서 사용한 YOUR_MQTT_USER 및 YOUR_PASSWORD로 대체하십시오.\n\n이후에는 Tasmota 엔티티가 홈 어시스턴트 내에서 자동으로 나타날 것입니다.\n\n만약 나타나지 않는다면 \"통합 추가\"를 눌러 추가하십시오. MQTT 연결이 제대로 작동하는지 확인하려면 Tasmota \"콘솔\"을 확인하십시오.\n\n이제 Tasmota 엔티티 ID를 알고 있다면 대시보드에 버튼을 추가할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 수동 통합\n\n다른 IoT 장치는 수동 구성이 필요할 수 있으며 MQTT 통합 내에 나타날 수도 있습니다. 또는 MQTT 서비스를 사용하여 스크립트 및 자동화 내에서 데이터를 송수신할 수 있습니다.\n\n![이미지](/assets/img/2024-06-20-InstallandaddingMQTTservertotheHomeAssistant_0.png)\n\n스크립트에서 사용할 수 있는 발행 및 수신 MQTT 서비스가 모두 있어서 HASS 인터페이스에 버튼을 추가할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## MQTT 클라이언트로 테스트해보기\n\nWindows/Linux 또는 다른 기계에서 Mosquitto 브로커를 테스트하기 위해 MQTTX를 사용할 수 있습니다. [여기](링크)에서 다운로드할 수 있어요.\n\n장치 데이터를 사용해보세요. Raspberry Pi IP인 192.168.0.190을(를) 교체해주세요.\n\n## 대박!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 디바이스와 홈 어시스턴트에서 MQTT를 구성하는 방법을 알게 되셨군요.\n\n⚠️ 인터넷을 통해 이 MQTT 서버를 직접 노출하려는 경우, TLS / SSL로 보호되지 않았으므로 악의적인 사용자가 로그인 자격 증명을 읽을 수 있습니다. 일반적인 것과 다른 사용자 이름 및 비밀번호를 사용하면 상대적으로 안전합니다. 그러나 그래도 악의적인 사용자가 당신의 디바이스를 제어하고, 네트워크 전체에 액세스할 수 있는 악성 코드를 주입할 수도 있습니다. 언제나 IoT 장치를 최신으로 유지하세요!\n\nMQTT에 TLS / SSL을 빠르게 추가할 수 있지만 모든 IoT 장치와 호환되지 않을 수 있으므로 보다 안전하게 로컬에 유지하고 MQTT 포트를 인터넷에 노출하지 마세요.\n\n이것은 홈 어시스턴트를 통해 MQTT 장치를 인터넷을 통해 제어할 수 없다는 뜻인가요? 아니요! 당신의 HASS 인스턴스는 Mosquitto 브로커와 별도입니다. 브로커는 로컬 네트워크에서만 IoT 장치를 처리하고 홈 어시스턴트와 인터페이스할 것이며, 암호화된 연결을 통해 인터넷에 노출될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 목록에서 더 많은 관련 기사를 찾을 수 있어요:","ogImage":{"url":"/assets/img/2024-06-20-InstallandaddingMQTTservertotheHomeAssistant_0.png"},"coverImage":"/assets/img/2024-06-20-InstallandaddingMQTTservertotheHomeAssistant_0.png","tag":["Tech"],"readingTime":4},{"title":"70달러 AI 키트로 나만의 GPT와 비슷한 어시스턴트를 만들어보세요","description":"","date":"2024-06-20 17:35","slug":"2024-06-20-BuildYourOwnGPT-likeAssistantwiththis70AIKit","content":"\n\n\u003cimg src=\"/assets/img/2024-06-20-BuildYourOwnGPT-likeAssistantwiththis70AIKit_0.png\" /\u003e\n\n만약 예산 제약으로 인해 인공 지능 세계에 뛰어들고 싶었지만 그렇지 못했다면, 새로운 Raspberry Pi AI 키트가 혁신으로 통하는 게이트웨이가 될 수 있습니다. 매력적인 가격인 70달러에 예약 주문 가능한 이 키트는 다재다능한 Raspberry Pi 5에서 인공 지능을 탐험하는 새로운 기회를 제공합니다. 이 키트는 강력한 Raspberry Pi의 기능과 결합하여 AI 실험과 학습의 민주화를 실현하고 있습니다.\n\n# 키트 뒤에 있는 파워하우스: Hailo\n\n\u003cimg src=\"/assets/img/2024-06-20-BuildYourOwnGPT-likeAssistantwiththis70AIKit_1.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRaspberry Pi AI Kit은 고성능 AI 프로세서로 유명한 선도적인 AI 하드웨어 업체인 Hailo와의 협업 제품입니다. Hailo의 칩은 신경망 연산을 최적화하여 우수한 처리 성능을 제공하면서도 에너지 효율성을 유지합니다. Hailo의 고급 AI 기술을 통합함으로써, Raspberry Pi AI Kit은 사용자가 Raspberry Pi 5에서 복잡한 AI 모델을 원활하게 실행할 수 있도록 보장합니다.\n\nHailo의 미션은 엣지 장치가 효율적이고 효과적으로 딥러닝 작업을 수행할 수 있도록 하는 것입니다. Raspberry Pi와의 파트너십은 AI를 일상 응용 프로그램에서 보다 접근 가능하고 실용적으로 만들기 위한 중요한 한걸음을 의미합니다. Hailo의 AI 역량과 Raspberry Pi의 가격 대비 성능 및 유연성이 결합되면 혁신적인 프로젝트에 대한 다양한 가능성이 열립니다.\n\n# Raspberry Pi AI Kit으로 만들 수 있는 GenAI 프로젝트\n\nRaspberry Pi AI Kit을 손에 넣으면 General AI (GenAI) 프로젝트의 세계가 펼쳐집니다. 아래는 시작할 수 있는 몇 가지 흥미로운 프로젝트입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 스마트 홈 자동화: 인공지능을 활용하여 스마트 홈 설정을 강화하세요. 얼굴을 인식하고 감정을 감지하며 목소리 명령에 반응할 수 있는 시스템을 만들어, 집을 더 직관적이고 안전하게 만드세요.\n- 인공지능 보안 카메라: 이상 활동을 감지하고 경고하는 보안 카메라를 구축하세요. 실시간 물체 감지와 인식을 통해 소유물을 더 정확하게 감시할 수 있습니다.\n- 개인 비서: 자연어를 이해하고 일정을 관리하며 알림을 설정하고 집의 다른 스마트 기기를 제어할 수 있는 AI 개인 비서를 개발하세요.\n- 건강 모니터링 시스템: AI를 활용하여 생체 신호를 추적하고 이상을 감지하며 실시간 건강 정보를 제공하는 건강 모니터링 시스템을 설계하세요. 이는 노인 요양이나 개인 피트니스 모니터링에 특히 유용할 수 있습니다.\n- 교육용 도구: AI를 활용한 맞춤형 학습 경험을 제공하는 상호 작용 교육 도구를 만드세요. 언어 학습 앱, 수학 가르침, 또는 학습자의 속도와 스타일에 맞춰 적응하는 가상 과학 실험실 등이 포함될 수 있습니다.\n- 자율 주행 차량: AI 모델을 기반으로 내비게이션하고 결정을 내릴 수 있는 소형 자율 주행 차량을 실험해보세요. 이 프로젝트는 단순한 선 따라가기 로봇부터 좀 더 복잡한 장애물 회피 및 경로 계획 기기까지 다양할 수 있습니다.\n- AI 아트와 음악: AI의 창의적인 면에 뛰어든 개발 알고리즘을 만드세요. 자동화된 작품 생성이나 음악 작곡이 가능합니다. 기술과 창의성의 교차로를 탐색하면 독특하고 영감을 주는 결과물이 나올 것입니다.\n- 음성 인식 시스템: 여러 언어로 명령을 이해하고 대답하는 음성 인식 시스템을 구축하세요. 대화식 키오스크부터 무선 조절까지 다양한 응용 분야에서 활용할 수 있습니다.\n\n# 라즈베리 파이 AI 키트를 선택하는 이유?\n\n라즈베리 파이 AI 키트는 AI 애호가를 위한 비용 효율적이고 강력한 솔루션을 제공합니다. 70달러에 Hailo의 최첨단 AI 기술을 이용할 수 있으며, Raspberry Pi 5의 다재다능함과 결합하여 다양한 AI 응용 프로그램을 구축할 수 있습니다. AI에 대해 배우려는 초보자든 혁신적인 솔루션을 원하는 경험있는 개발자든, 이 키트는 훌륭한 자원입니다.\n\n또한 라즈베리 파이 제품에 대한 커뮤니티 지원과 상세한 문서가 제공되어 프로젝트에 대해 충분한 지속적인 지원과 영감을 받을 수 있습니다. 이 AI 키트는 도구뿐만 아니라 AI로 가능한 가능성을 넓히는 메이커와 혁신가들의 글로벌 커뮤니티에 초대하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요약하자면, Hailo의 고급 AI 기술을 통해 구동되는 라즈베리 파이 AI 키트는 가격이 저렴하고 접근성이 좋은 AI 실험을 위한 게임 체인저입니다. 이 제품 출시로 AI 개발에 대한 진입 장벽이 크게 낮아져 더 많은 사람들이 이 흥미로운 분야에 참여하고 기여할 수 있게 되었습니다. 지금 당장 키트를 예약하고 라즈베리 파이 5에서 AI의 끝없는 가능성을 탐험해 보세요.\n\n제게 대리기를 할 수 없는 분들을 위해 Hailo-8L Accelerator 모듈을 기다리지 못하시는 경우 Coral M.2 Accelerator with Dual Edge TPU를 시도해 보세요. 4TFlops의 성능을 자랑하며 Hailo-8L이 약속한 13 TFlops는 아니지만 여러 사용 사례에 대해 충분히 대응할 것입니다.\n\nCoral M.2 Accelerator with Dual Edge TPU는 M.2 E-key 슬롯이있는 기기의 AI 능력을 향상시키기 위해 설계된 강력한 AI 처리 모듈입니다. 두 개의 엣지 텐서 처리 유닛(TPU)을 갖춘 이 가속기는 머신 러닝 모델을 빠르고 효율적으로 실행할 수 있습니다. TensorFlow Lite를 지원하여 최소한의 지연 시간과 전력 소비로 엣지에서 복잡한 신경망을 실행하는 데 이상적입니다.\n\n# Coral M.2 Accelerator Dual Edge TPU로 할 수 있는 일:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 실시간 객체 감지: 보안 카메라, 로봇 및 스마트 홈 기기에 대한 고속 객체 감지 및 인식 구현\n- 이미지 분류: 제조업에서의 자동화된 품질 검사 또는 의료 분석 등 이미지 처리 응용 프로그램 강화\n- 자연어 처리: 음성 인식 시스템, 챗봇 및 번역 서비스의 성능 향상\n- 엣지 AI 응용: 엣지 장치에 직접 AI 모델을 개발 및 배포하여 클라우드 서비스의 의존성을 줄이고 더 빠르고 오프라인 데이터 처리 보장\n- 스마트 기기: 얼굴 인식, 동작 감지 및 예측 유지보수와 같은 복잡한 작업을 수행할 수 있도록 스마트 홈 기기 업그레이드\n\nCoral M.2 Accelerator with Dual Edge TPU는 프로젝트의 AI 성능을 최소한의 통합 노력으로 향상시키고자 하는 개발자와 엔지니어에게 최적입니다.","ogImage":{"url":"/assets/img/2024-06-20-BuildYourOwnGPT-likeAssistantwiththis70AIKit_0.png"},"coverImage":"/assets/img/2024-06-20-BuildYourOwnGPT-likeAssistantwiththis70AIKit_0.png","tag":["Tech"],"readingTime":4}],"page":"48","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"48"},"buildId":"kkckKPyxcjJp_o8wb2unW","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>