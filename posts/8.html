<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/8" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/8" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="대형 언어 모델LLM을 위한 토큰 마스킹 전략들" href="/post/2024-06-23-TokenMaskingStrategiesforLLMs"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대형 언어 모델LLM을 위한 토큰 마스킹 전략들" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대형 언어 모델LLM을 위한 토큰 마스킹 전략들" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">대형 언어 모델LLM을 위한 토큰 마스킹 전략들</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">18<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="반품 및 환불 처리를 위한 챗봇 활용 엔터프라이즈 워크플로우 통합 방법" href="/post/2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="반품 및 환불 처리를 위한 챗봇 활용 엔터프라이즈 워크플로우 통합 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="반품 및 환불 처리를 위한 챗봇 활용 엔터프라이즈 워크플로우 통합 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">반품 및 환불 처리를 위한 챗봇 활용 엔터프라이즈 워크플로우 통합 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="모듈형 RAG와 RAG Flow 첫 번째 파트" href="/post/2024-06-23-ModularRAGandRAGFlowPart"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="모듈형 RAG와 RAG Flow 첫 번째 파트" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-ModularRAGandRAGFlowPart_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="모듈형 RAG와 RAG Flow 첫 번째 파트" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">모듈형 RAG와 RAG Flow 첫 번째 파트</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">16<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="초보자를 위한 통합 가이드 Python을 사용한 LLM 로컬 배포 쉽게 하는 방법" href="/post/2024-06-23-LLMsMadeAccessibleABeginnersUnifiedGuidetoLocalDeploymentviaPython"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="초보자를 위한 통합 가이드 Python을 사용한 LLM 로컬 배포 쉽게 하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-LLMsMadeAccessibleABeginnersUnifiedGuidetoLocalDeploymentviaPython_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="초보자를 위한 통합 가이드 Python을 사용한 LLM 로컬 배포 쉽게 하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">초보자를 위한 통합 가이드 Python을 사용한 LLM 로컬 배포 쉽게 하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">18<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LLMs의 작동 원리 - 대형 언어 모델의 비밀 밝혀보기" href="/post/2024-06-23-HowLLMsWork"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LLMs의 작동 원리 - 대형 언어 모델의 비밀 밝혀보기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-HowLLMsWork_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LLMs의 작동 원리 - 대형 언어 모델의 비밀 밝혀보기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">LLMs의 작동 원리 - 대형 언어 모델의 비밀 밝혀보기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="사이드 허슬 구축을 위한 10가지 ChatGPT 프롬프트 템플릿" href="/post/2024-06-23-10ChatGPTPromptTemplatesForBuildingSideHustles"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="사이드 허슬 구축을 위한 10가지 ChatGPT 프롬프트 템플릿" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-10ChatGPTPromptTemplatesForBuildingSideHustles_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="사이드 허슬 구축을 위한 10가지 ChatGPT 프롬프트 템플릿" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">사이드 허슬 구축을 위한 10가지 ChatGPT 프롬프트 템플릿</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="프롬프트 엔지니어링은 잊어라, ChatGPT가 완벽한 프롬프트를 작성해준다" href="/post/2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="프롬프트 엔지니어링은 잊어라, ChatGPT가 완벽한 프롬프트를 작성해준다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="프롬프트 엔지니어링은 잊어라, ChatGPT가 완벽한 프롬프트를 작성해준다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">프롬프트 엔지니어링은 잊어라, ChatGPT가 완벽한 프롬프트를 작성해준다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="인간의 모든 것을 의인화하는 습관" href="/post/2024-06-23-Ourhumanhabitofanthropomorphizingeverything"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="인간의 모든 것을 의인화하는 습관" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="인간의 모든 것을 의인화하는 습관" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">인간의 모든 것을 의인화하는 습관</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="블로거들이 반드시 사용해야 할 5가지 ChatGPT 프롬프트 더 빠르게 글쓰기" href="/post/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="블로거들이 반드시 사용해야 할 5가지 ChatGPT 프롬프트 더 빠르게 글쓰기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="블로거들이 반드시 사용해야 할 5가지 ChatGPT 프롬프트 더 빠르게 글쓰기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">블로거들이 반드시 사용해야 할 5가지 ChatGPT 프롬프트 더 빠르게 글쓰기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ChatGPT가 갑자기 이상 행동을 하는 이유 5가지" href="/post/2024-06-23-ChatGPTjustwentrogueandwedontknowwhy"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ChatGPT가 갑자기 이상 행동을 하는 이유 5가지" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-ChatGPTjustwentrogueandwedontknowwhy_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ChatGPT가 갑자기 이상 행동을 하는 이유 5가지" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">ChatGPT가 갑자기 이상 행동을 하는 이유 5가지</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><a class="link" href="/posts/1">1</a><a class="link" href="/posts/2">2</a><a class="link" href="/posts/3">3</a><a class="link" href="/posts/4">4</a><a class="link" href="/posts/5">5</a><a class="link" href="/posts/6">6</a><a class="link" href="/posts/7">7</a><a class="link posts_-active__YVJEi" href="/posts/8">8</a><a class="link" href="/posts/9">9</a><a class="link" href="/posts/10">10</a><a class="link" href="/posts/11">11</a><a class="link" href="/posts/12">12</a><a class="link" href="/posts/13">13</a><a class="link" href="/posts/14">14</a><a class="link" href="/posts/15">15</a><a class="link" href="/posts/16">16</a><a class="link" href="/posts/17">17</a><a class="link" href="/posts/18">18</a><a class="link" href="/posts/19">19</a><a class="link" href="/posts/20">20</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"대형 언어 모델LLM을 위한 토큰 마스킹 전략들","description":"","date":"2024-06-23 19:24","slug":"2024-06-23-TokenMaskingStrategiesforLLMs","content":"\n\n## 다양한 언어 모델에서 사용되는 다양한 가리기 기술, 그 이점 및 Pytorch를 사용하여 낮은 수준에서 작동하는 방법에 대해 자세히 알아보세요.\n\n![이미지](/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_0.png)\n\n토큰 마스킹은 분류 변형 및 생성 모델에서 언어 모델을 훈련하는 데 널리 사용되는 전략입니다. BERT 언어 모델이 소개했으며 많은 변형(RoBERTa, ALBERT, DeBERTa 등)에서 사용되었습니다.\n\n그러나 토큰 마스킹은 텍스트 손상이라는 큰 그룹 내의 전략입니다. BART 연구 논문에서는 다양한 텍스트 손상 전략을 사용하여 인코더-디코더 생성 모델을 훈련하는 많은 실험이 수행되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_1.png\" /\u003e\n\n텍스트 손상에 대한 다양한 기술을 논의하기 전에, 대형 언어 모델(Large Language Models, LLMs)의 모든 텍스트 손상 방법에 대한 표준 개념에 대해 이야기하겠습니다.\n\n# 지도 학습에서 자기 지도 학습으로\n\n대규모 양의 텍스트가 언어 모델의 초기 교육에 사용되며, 모델이 언어를 올바르게 표현하도록 학습하고, 이 지식을 그 매개 변수 가중치에 암묵적으로 저장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 방대한 양의 텍스트는 학습을 위한 레이블이 있어야 합니다. 모델 입력 데이터를 처리한 후 참조 데이터를 사용하여 교차 엔트로피를 계산해야 합니다. 그러나 이렇게 많은 데이터에 주석을 다는 것은 현실적이지 않습니다. 따라서 우리는 자동 레이블 생성을 찾게 되었고, 지도 문제를 자가지도 문제로 전환하게 되었습니다.\n\n이 경우, 손상된 시퀀스는 모델의 학습 입력으로 작용하고, 기존 시퀀스의 전체 또는 일부가 학습 데이터의 레이블로 작용합니다. 이는 모델의 성격(인코더 또는 인코더-디코더)에 따라 다릅니다.\n\n# 손상 확률\n\n자동 레이블을 사용하여, 모델은 데이터에 주석을 달지 않고 각 학습 예제와 연결된 레이블을 학습합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n텍스트 손상(특히 토큰 마스킹, 토큰 삭제 및 텍스트 인필링에서)에서 각 단어는 일반적으로 15–20% 정도의 확률에 따라 손상될 것입니다. 이 확률은 모델이 각 문장의 맥락을 학습할 수 있도록 낮게 유지됩니다.\n\n문장 순서 바꾸기 또는 문서 회전과 같은 일부 텍스트 손상 기술은 특정 확률로 단어를 손상시키지 않습니다. 이로 인해 다른 손상 기술과 호환되도록 할 수 있습니다. 아래에서 논의할 것과 같이요.\n\n# 분류와 생성 사이의 차이점\n\n텍스트 손상을 통해 언어 모델을 학습할 때, 레이블은 분류 모델(인코더만)인지 생성 모델(인코더-디코더)인지에 따라 달라집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n분류 모델에서는 레이블을 사용하여 입력의 오염된 영역에만 주의를 기울입니다. 따라서 만약 문장 전체에서 단어가 마스킹되었다면, 레이블은 초기 시퀀스가 되어 오염된 시퀀스에만 주의를 기울입니다.\n\n생성 모델의 경우, 모델이 텍스트를 연속적으로 생성할 수 있어야 하므로 출력 레이블은 초기 정상 시퀀스가 되며 전체 시퀀스 자체에 주의를 기울입니다.\n\n# 설정\n\n이제 우리는 텍스트 오염으로 언어 모델을 학습할 때의 공통점을 간단히 소개했으니, 텍스트를 손상시키는 다양한 기술과 각 경우에 코드 예시를 제시하는 것을 논의해보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 서로 다른 전략들이 어떻게 작동하는지 보기 위해 코드 예제에서 문서로 시작하겠습니다. 우리는 전처리에 매우 유용한 여러 자연어 처리 도구를 갖춘 Stanford NLP에서 개발된 라이브러리인 Stanza를 사용할 것입니다.\n\n```js\nimport stanza\nstanza.download('en')\n\n# 예제에서 사용되는 텍스트\ntext = \"헌팅턴병은 유전적인 신경퇴행성 질환으로 헌팅틴 유전자 내 다형성 CAG 반복이 확장되면서 발생합니다. 번역 초기 인자 4E-BP의 인산화는 번역 조절의 변경으로 이어져서 원치 않는 단백질 합성과 신경 기능에 영향을 줍니다. 돌연변이 헌팅틴(mhtt) 유전자의 전사 결과는 잘 알려지지 않았습니다. 발병 연령의 가변성은 성인과 소아형 헌팅턴병을 분리하는 중요한 요소입니다. 고령 유전자, 어머니의 보호(즉, 과도한 아버지의 유전), 우수한 노화 유전자 및 환경 임계치가 고려되는 요소입니다. 분자 병인학에 중점을 둔 부분으로는 운동 장애, 인지 장애 및 신경 정신 장애가 포함됩니다. 진단 부분도 고려되었습니다. 이는 유전자 검사 및 주요 및 이차 증상을 포함합니다. 헌팅턴병의 유전학과 병리학에도 주목합니다.\"\n\n# 각 다른 문장을 리스트 요소로 얻기 위해 Stanza 모델을 사용할 것입니다.\nnlp = stanza.Pipeline('en', use_gpu=False)\ndoc = nlp(text)\nsentences = [sentence.text for sentence in doc.sentences]\n```\n\n# 토큰 가림\n\nBERT는 이 전략을 도입했는데, 이는 첫 번째이자 가장 잘 알려진 시퀀스 손상 전략입니다. 입력 시퀀스를 무작위 단어로 가려서 훈련 중에 레이블로 사용될 단어를 손상하는 것으로 구성되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n분류 모델에서는 Huggingface transformers에서 DataCollatorForLanguageModeling 클래스를 직접 사용하여 BERT 또는 RoBERTa와 같은 모델을 학습할 수 있는 필요한 레이블을 생성할 수 있습니다.\n\n```js\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling\nimport torch\n\ndef load_dataset_mlm(sentences, tokenizer_class=AutoTokenizer, \n                     collator_class=DataCollatorForLanguageModeling, \n                     mlm=True, mlm_probability=0.20):\n    tokenizer = tokenizer_class.from_pretrained('google-bert/bert-base-uncased')\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True, \n                       truncation=True)\n    \n    # 랜덤 마스킹 설정\n    data_collator = collator_class(\n        tokenizer=tokenizer, \n        mlm=mlm,  \n        mlm_probability=mlm_probability \n    )\n\n    \"\"\"콜레이터는 텐서들 튜플을 기대하므로 입력 텐서들을 분리한 다음\n    첫 번째 차원을 삭제하고 튜플로 전달해야 합니다.\"\"\"\n    tuple_ids = torch.split(inputs['input_ids'], 1, dim=0)\n    tuple_ids = list(tuple_ids)\n    for tensor in range(len(tuple_ids)):\n        tuple_ids[tensor] = tuple_ids[tensor].squeeze(0)\n    tuple_ids = tuple(tuple_ids)\n    \n    # 각 문장의 input_ids, attention_masks 및 레이블 가져오기\n    batch = data_collator(tuple_ids)\n    return batch['input_ids'], inputs['attention_mask'], batch['labels']\n\n\ninput_ids, attention_mask, labels = load_dataset_mlm(sentences)\n\n\"\"\"\ninput_ids[0]:\ntensor([  101, 16364,  1005,  1055,   103,  2003,  1037,   103, 10976,  3207,\n          103, 25284,   103, 25426, 16870,  4295,  3463,  2349,  2000,   103,\n         1997, 26572, 18078,  6187,  2290, 17993,  1999,  1996,  5933,  7629,\n          103,   103,   102,     0,     0])\n\nattention_mask[0]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n\nlabels[0]:\ntensor([ -100,  -100,  -100,  -100,  4295,  -100,  -100, 11265,  -100,  -100,\n         6914,  -100,  8285,  -100,  2389,  -100,  -100,  -100,  -100,  4935,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         4962,  1012,  -100,  -100,  -100))\n\n\"\"\"\n```\n\n생성된 input_ids에는 원본 텍스트의 각 토큰에 대한 정수 번호가 있습니다. 특별한 토큰은 마스킹된 단어를 나타내며 (BERT에서는 이 토큰이 103입니다), 이 특별한 토큰은 사용하는 언어 모델에 따라 다르며 서로 다른 토크나이저는 서로 다른 주의 마스크 식별자를 반환합니다.\n\n또한, Huggingface는 모델 내에서 다른 작업을 수행하여 고유 토큰에 대해 다른 작업을 지정하므로 \"-100\"으로 표시된 토큰은 모델에서 무시해야 함을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBART와 같은 생성 모델의 경우, DataCollatorForLanguageModeling 클래스를 사용하여 토큰 마스킹 전략을 구현할 수 있습니다. 그러나 생성 모델에 맞게 태그를 조정하기 위해 작은 변경을 도입해야 합니다.\n\n```js\nfrom transformers import BartTokenizer, DataCollatorForLanguageModeling\nimport torch\n\ndef load_dataset_mlm(sentences, tokenizer_class=BartTokenizer, \n                     collator_class=DataCollatorForLanguageModeling, \n                     mlm=True, mlm_probability=0.20):\n    tokenizer = tokenizer_class.from_pretrained('facebook/bart-base')\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True, \n                       truncation=True)\n    \n    # 랜덤 마스킹 구성\n    data_collator = collator_class(\n        tokenizer=tokenizer, \n        mlm=mlm,  # 마스크된 언어 모델링을 위해 True\n        mlm_probability=mlm_probability  # 각 토큰이 마스킹될 확률\n    )\n\n    \"\"\"Collator는 텐서의 튜플을 예상하므로 입력 텐서를 분할하고 첫 번째 차원을 제거한 후 튜플로 전달해야합니다.\"\"\"\n    tuple_ids = torch.split(inputs['input_ids'], 1, dim=0)\n    tuple_ids = list(tuple_ids)\n    for tensor in range(len(tuple_ids)):\n        tuple_ids[tensor] = tuple_ids[tensor].squeeze(0)\n    tuple_ids = tuple(tuple_ids)\n    \n    # 각 문장에 대한 input_ids, attention_masks 및 labels 가져오기\n    batch = data_collator(tuple_ids)\n    batch['labels'] = inputs['input_ids']\n    return batch['input_ids'], inputs['attention_mask'],  batch['labels']\n\ninput_ids, attention_mask, labels = load_dataset_mlm(sentences)\n\n\"\"\"\ninput_ids[0]:\ntensor([    0, 38831,  2577,  1054,    18,  2199,    16,    10, 14913, 28904,\n         5777,  3693, 32226, 38868,  2199,   775,   528,     7,  2919,     9,\n        48052,   636,   230,  3450, 35315,    11,     5, 50264, 50264, 50264,\n            4,     2])\n\nattention_mask[0]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1])\n\nlabels[0]:\ntensor([    0, 38831,  2577,  1054,    18,  2199,    16,    10, 14913, 28904,\n         5777,  3693, 32226, 38868,  2199,   775,   528,     7,  2919,     9,\n        48052,   636,   230,  3450, 35315,    11,     5,  8217, 24276, 10596,\n            4,     2])\n\"\"\"\n```\n\n여기서 각 입력 토큰은 마스크 여부에 관계없이 해당하는 토큰을 레이블링하며, 이는 분류 모델과는 달리 모델이 제공된 시퀀스에 기반한 텍스트 시퀀스를 생성할 수 있어야 하기 때문입니다. BART의 경우, 각 마스크를 나타내는 토큰은 ID 50264를 갖습니다.\n\n# 토큰 삭제\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 전략은 마스킹에 대해 다른 접근 방식을 사용합니다. 특정 확률로 텍스트의 원래 시퀀스에서 단어가 제거되어 모델은 빈칸과 해당 위치를 찾아야 합니다. 표준 마스킹은 마스크가 이미 모델의 입력에서 지정되어 있기 때문에 위치를 학습하지 않습니다.\n\n```js\ndef token_deletion(sentences, tokenizer_class=BartTokenizer, collator_class=DataCollatorForLanguageModeling, \n                 mlm=True, mlm_probability=0.20):\n    tokenizer = tokenizer_class.from_pretrained('facebook/bart-base')\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n    \n    data_collator = collator_class(\n        tokenizer=tokenizer, \n        mlm=mlm,\n        mlm_probability=mlm_probability \n    )\n\n    tuple_ids = torch.split(inputs['input_ids'], 1, dim=0)\n    tuple_ids = list(tuple_ids)\n    for tensor in range(len(tuple_ids)):\n        tuple_ids[tensor] = tuple_ids[tensor].squeeze(0)\n    tuple_ids = tuple(tuple_ids)\n \n    batch = data_collator(tuple_ids)\n\n    # We use the initial inputs as labels\n    batch['labels'] = batch['input_ids'].clone()\n    \n    # We remove tokens with mask identifier and thus make token deletion\n    # Change the value to the mask identifier of the specific token model\n    # It is necessary to know the identifier of the mask token for \n    # that specific model\n    mask = batch['input_ids'] != 50264\n    initial_size = batch['input_ids'].size(1)\n    total_sentences = batch['input_ids'].size(0)\n\n    # When we remove the specific token, we must fill with the padding \n    # token otherwise the tensor size is not respected.\n    for i in range(total_sentences):\n        new_tensor = batch['input_ids'][i][mask[i]]\n        new_tensor = F.pad(new_tensor, (0, initial_size - new_tensor.size(0)), value=1)\n        batch['input_ids'][i] = new_tensor\n        attention_mask = batch['input_ids'][i] == 1\n        inputs['attention_mask'][i][attention_mask] = 0\n        \n    return batch['input_ids'], inputs['attention_mask'], batch['labels']\n\ninput_ids, attention_mask, labels = token_deletion(sentences)\n\n\"\"\"\ninput_ids[0]:\ntensor([    0, 38831,  2577,  1054,  2199, 14913, 28904,  3693, 32226, 38868,\n         2199,   775,   528,     7,  2919,     9, 23404,   636,   230, 35315,\n           11,     5, 24276, 10596,     4,     2,     1,     1,     1,     1,\n            1,     1])\n\nattention_mask[0]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 0, 0, 0, 0, 0])\n\nlabels[0]:\ntensor([    0, 38831,  2577,  1054, 50264,  2199, 50264, 50264, 14913, 28904,\n        50264,  3693, 32226, 38868,  2199,   775,   528,     7,  2919,     9,\n        23404,   636,   230, 50264, 35315,    11,     5, 50264, 24276, 10596,\n            4,     2])\n\n\"\"\"\n```\n\nBART를 사용하여 Token Deletion을 훈련할 때, 일부 텍스트 생성 벤치마킹은 질문 응답, 요약 생성 작업 및 대화 작업에 긴 시퀀스를 사용할 때 약간의 개선이 나타납니다.\n\n# 텍스트 채워넣기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n텍스트 인필링은 토큰 마스킹과 비슷합니다. 특정 확률로 원본 텍스트에 마스크를 씌우게 됩니다. 이 경우에는 마스킹이 하나의 단어 이상을 덮을 수 있다는 차이가 있습니다. BART에서 텍스트 인필링을 적용할 때, 람다 값이 3인 포아송 분포를 사용하여 마스킹을 수행합니다. 이는 평균적으로 문장에서 텍스트가 마스킹될 때마다 세 개의 단어가 하나의 토큰 마스크로 마스킹됨을 의미합니다. 그러나 확률 분포이기 때문에 더 많거나 더 적은 개수의 마스킹된 단어가 있을 수도 있습니다.\n\n![image](/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_2.png)\n\n저희는 NumPy 라이브러리와 우리 언어 모델에 특화된 토크나이저를 사용하여 텍스트 인필링을 구현할 것입니다. 아래는 예시 코드입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 문장 순열\n\n문장 순열에서 모델 입력 시퀀스에 맞는 문장 수(소형 모델에서 입력 시퀀스는 512에서 1024 토큰 사이)를 고려하는 것이 매우 중요합니다. 순서 테스트 후, 시퀀스에 맞게 문장 수를 결정한 다음, 그것들을 리스트나 배열로 나누어야 하고, 예시 코드에서와 같이 중복되지 않는 방식으로 무작위로 선택해야 합니다.\n\n```js\n# 주어진 \"문장\" 세트 중 첫 \"number_sentences\"를 선택하고 그 문장들을 무작위로 반환합니다.\ndef sentence_permutation(sentences, number_sentences):\n    new_sentences = sentences[:number_sentences]\n    random.shuffle(new_sentences)\n    new_sentences = sentence_joiner(new_sentences)\n    return new_sentences\n\ndef permuted_data_generation(sentences: list, total_sentences: int):\n    training_sentences = []\n    training_labels = []\n    sentences_copy = sentences.copy()\n    # 문장 목록의 크기에서 1을 뺀 횟수만큼 sentence_permutation을 적용하여 \n    # 텍스트의 각 새 문장 예제를 얻고 가장 오래된 문장을 제거합니다.\n    for _ in range(len(sentences)-total_sentences+1):\n        new_sentences = sentence_permutation(sentences_copy, total_sentences)\n        joined_sentences = sentence_joiner(sentences_copy[:total_sentences])\n        sentences_copy = sentences_copy[1:]\n        training_sentences.append(new_sentences)\n        training_labels.append(joined_sentences)\n\n    return training_sentences, training_labels\n\n\ndef permutation_training(sentences: list, sentences_labels: list, \n                         tokenizer_class=BartTokenizer, \n                         collator_class=DataCollatorForLanguageModeling, \n                         mlm=True, mlm_probability=0.0):\n    # permuted 문장으로부터 input_ids와 attention mask를 얻습니다\n    input, attention_mask, _ = load_dataset_mlm(sentences, tokenizer_class, collator_class, mlm, mlm_probability)\n    \n    # 원본 문장으로부터 라벨 가져오기\n    labels, _, _ = load_dataset_mlm(sentences_labels, tokenizer_class, collator_class, mlm, mlm_probability)\n\n    return input.squeeze(0), attention_mask.squeeze(0), labels.squeeze(0)\n\ninput_ids, attention_mask, labels = permutation_training(training_sentences, training_labels_sentences)\n\n\"\"\"\ninput_ids[0]:\ntensor([    0, 38831,  2577,  1054,    18,  2199, ...\n\nattention_mask[0]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, ...\n\nlabels[0]:\ntensor([    0, 38831, 2577, 1054, 18, 2199, ...\n\"\"\"\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 예제에서는 모델에 데이터를 입력할 때 원래 순서에서 먼저 나온 문장을 제거한 후, 주어진 문장 개수에 따라 문장 순열을 수행하기 전에 새로운 문장을 추가합니다. 이렇게 함으로써 입력 시퀀스의 문장을 다시 정렬하더라도 각 새로운 예제마다 새로운 문장이 나타나는 문맥 창을 유지하고 가장 오래된 문장을 삭제합니다.\n\n# 문서 회전\n\n문서 회전을 적용하려면 사용된 각 배치의 차원을 고려해야 합니다. 패딩을 적용하는 경우 패딩은 문서의 나머지 부분과 함께 회전되지 않고 원래 위치를 유지해야합니다.\n\n```js\ndef sentence_joiner(sentences: list):\n  return ' '.join(sentences)\n\n# 이 함수를 사용하여 토크나이저에 입력 데이터를 형성할 수 있는 원하는 만큼의 문장을 모을 수 있습니다.\ndef rotated_data_generation(sentences: list, total_sentences: int):\n  training_sentences = []\n  sentences_copy = sentences.copy()\n  for _ in range(len(sentences)-total_sentences+1):\n    new_sentences = sentences_copy[:total_sentences]\n    new_sentences = sentence_joiner(new_sentences)\n    sentences_copy = sentences_copy[1:]\n    training_sentences.append(new_sentences)\n  return training_sentences\n\n# 이전 함수에서 회전된 문장을 적용합니다.\ndef document_rotation_training(sentences, tokenizer_class=BartTokenizer):\n  tokenizer = tokenizer_class.from_pretrained('facebook/bart-base')\n  tokens = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n  tokens['input_ids'] = tokens['input_ids'].squeeze(0)\n  tokens['labels'] = tokens['input_ids'].clone()\n \n  iterations = tokens['input_ids'].size(0)\n  for i in range(iterations):\n    # 어텐션 마스크를 가져와 리스트로 변환합니다.\n    attention_mask = tokens['attention_mask'][i].tolist()\n    # 패딩이 시작되는 위치를 계산합니다.\n    if 0 in attention_mask:\n      padding_start_position = attention_mask.index(0)\n    else:\n      padding_start_position = False\n    # 패딩이 있는 경우 문서의 나머지 부분과 함께 회전되지 않도록 패딩 위치를 고려합니다.\n    if padding_start_position:\n      random_token = torch.randint(1, padding_start_position-1, (1,))\n      tokens['input_ids'][i] = torch.cat((tokens['input_ids'][i][0].unsqueeze(0), \n                                      tokens['input_ids'][i][random_token.item():padding_start_position-1],\n                                      tokens['input_ids'][i][1:random_token.item()],\n                                      tokens['input_ids'][i][padding_start_position-1:-1],\n                                      tokens['input_ids'][i][-1].unsqueeze(0)), 0)\n                                        \n    # 패딩이 없는 경우 패딩을 고려하지 않고 문서를 회전합니다.\n    else:\n      random_token = torch.randint(1, tokens['input_ids'].size(0)-1, (1,))\n      tokens['input_ids'][i] = torch.cat((tokens['input_ids'][i][0].unsqueeze(0),\n                                      tokens['input_ids'][i][random_token.item():-1],\n                                      tokens['input_ids'][i][1:random_token.item()],\n                                      tokens['input_ids'][i][-1].unsqueeze(0)), 0)\n  return tokens['input_ids'], tokens['attention_mask'].squeeze(0), tokens['labels']\n\ndata = rotated_data_generation(sentences, 3)\ninput_ids, attention_mask, labels = document_rotation_training(data)\n\n\"\"\"\ninput_ids[2]:\ntensor([    0,  2433,    61,    32,   551,    88,  1316,    32,    12,  4138,\n        15557, 47605,     6, 22835,  2591,   939,     4,   242, 10079, 38422,\n         9235,     6, 10295, 22540, 14819,     8,  3039, 11543,     4,   347,\n        37347,  8457,     9, 41419,  8217,  1054,    36,   119, 49491,    43,\n        10596, 37118,    32,    45,   157,   684,     4, 41058,  4484,     9,\n         1046,     9, 23808,    16,    41,   505,  3724,     9, 18073,    18,\n         2199, 18246,  4194,     8, 13430,  3505,     4,    20,     2,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n\nattention_mask[2]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])\n\nlabels[2]:\ntensor([    0,   347, 37347,  8457,     9, 41419,  8217,  1054,    36,   119,\n        49491,    43, 10596, 37118,    32,    45,   157,   684,     4, 41058,\n         4484,     9,  1046,     9, 23808,    16,    41,   505,  3724,     9,\n        18073,    18,  2199, 18246,  4194,     8, 13430,  3505,     4,    20,\n         2433,    61,    32,   551,    88,  1316,    32,    12,  4138, 15557,\n        47605,     6, 22835,  2591,   939,     4,   242, 10079, 38422,  9235,\n            6, 10295, 22540, 14819,     8,  3039, 11543,     4,     2,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n\n\"\"\"\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n짧은 텍스트 시퀀스는 문서 회전 및 문장 순열 기술을 의미 없게 만듭니다. 반면에 다른 언급된 방법들(토큰 마스킹, 토큰 삭제 및 텍스트 채우기)은 짧고 긴 텍스트 시퀀스에서 도움이 될 수 있습니다.\n\n시퀀스 순열과 마찬가지로 각 데이터 입력마다 가장 오래된 문장을 제거하고 새로운 문장을 추가하여 문맥 윈도우를 유지할 수 있습니다.\n\n# 결론\n\n본 글은 시퀀스 왜곡으로 언어 모델을 학습시키는 다양한 방법에 대해 논의했습니다. 이들은 가장 유명하지만 대부분의 모델은 토큰 마스킹만을 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요약하면, 가장 효과적인 전략은 텍스트를 변경하는 대신 텍스트를 손상시키는 것입니다. 그러나 이 두 가지 접근 방식은 모델 훈련 중에 결합될 수 있으며, BART의 경우 Text Infilling 및 Sentence Permutation을 사용하여 흥미로운 결과를 얻을 수 있었습니다.\n\n이 훈련 방식은 인코더 또는 인코더-디코더 트랜스포머 모델에서 사용할 수 있습니다. 내가 아는 한, 이 접근 방식을 사용하는 디코더 전용 모델은 없습니다. 왜냐하면 그런 경우에는 자기 회귀 언어 모델링 또는 인과 언어 모델링이 사용됩니다. 이는 GPT 모델과 같이 자기 어텐션 메커니즘 없이 인코더를 사용할 때 각 토큰의 예측이 이전 토큰에만 의존하며 그 다음 토큰에는 의존하지 않기 때문입니다. BERT와 같은 인코더 전용 모델에서는 양방향 어텐션이 존재하여 각 토큰이 이전 및 이후 토큰에 따라 어느 위치에 있어야 하는지 예측할 수 있습니다.\n\n미래 글에서는 어떻게 인과 언어 모델링이 작동하는지와 더 고급 시퀸스 손상 기술에 대해 깊이 알아볼 것입니다.\n\n![이미지](/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_3.png)\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n행복한 코딩하세요!","ogImage":{"url":"/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_0.png"},"coverImage":"/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_0.png","tag":["Tech"],"readingTime":18},{"title":"반품 및 환불 처리를 위한 챗봇 활용 엔터프라이즈 워크플로우 통합 방법","description":"","date":"2024-06-23 19:23","slug":"2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds","content":"\n\n\u003cimg src=\"/assets/img/2024-06-23-엔터프라이즈워크플로우통합챗봇을활용한반품및환불.png\" /\u003e\n\n고객 서비스 랜드스케이프가 급격히 변화하고 있습니다. 기업들은 영업을 최적화하고 고객 경험을 높이기 위해 AI 솔루션을 채택하고 있습니다. 기본 챗봇은 FAQ에 대한 답변으로 흔히 사용되고 있지만, 반품이나 환불 처리와 같이 복잡한 시나리오에 대해 어려움을 겪는 경우가 많습니다. 이러한 복잡한 상황에서 가장 큰 수익률을 얻을 수 있는 영역입니다.\n\n# 기본 챗봇 응용\n\nLLM 챗봇은 대부분 고객 서비스에 사용되며, 기본 지원을 제공하고 자주 묻는 질문에 답변합니다. 이러한 챗봇은 일반적으로 응답 생성을 위해 소스 문서나 HTML 페이지의 라이브러리에 의존합니다. 이 문서들은 청크(chunk) 단위로 분리되어 벡터 저장소(Vector Store)에 저장됩니다. 사용자가 질문을 하면 벡터 저장소에서 유사도 검색이 수행되어 관련 문서 청크가 검색되고, 이후 OpenAI와 같은 LLM으로 응답을 생성하기 위해 전송됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds_1.png)\n\n챗봇이 도움이 될 수는 있지만, 고객 서비스 자동화의 진정한 가치는 고객 지원에서 정례적으로 발생하는 복잡한 상황을 해결할 수 있는 능력에 있습니다.\n\n# 챗봇을 활용하여 복잡한 워크플로우 문제 해결하기\n\n자연어 처리 및 생성 기술의 놀라운 발전에도 불구하고, LLM을 기반으로 한 챗봇은 여전히 제품 반품 또는 환불과 같이 복잡한 상황에서 특히 고객이 만족스러운 경험을 얻는 데 어려움을 겪고 있습니다. 왜 그런지 살펴보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003ctable\u003e\n\u003ctr\u003e\n\u003cth\u003eIssue\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eContextual Blindspots\u003c/td\u003e\n\u003ctd\u003eLLMs (Large Language Models) may sound human-like, but often miss important context. For instance, a chatbot might not realize a prior promise for faster shipping, causing frustration.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eIsolated Operations\u003c/td\u003e\n\u003ctd\u003eTraditional LLM chatbots are often disconnected from essential systems like order management or knowledge bases. This means a chatbot may not access specific booking details when a customer wants to change a flight.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eInconsistent Service\u003c/td\u003e\n\u003ctd\u003eLLMs can provide conflicting responses, especially in complex situations with strict policies. This can lead to confusion and distrust, like differing information on warranty policies.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLanguage Barriers\u003c/td\u003e\n\u003ctd\u003eMost LLM chatbots support only one language, limiting help for a global customer base. For example, an English-only chatbot may not effectively assist Spanish-speaking customers.\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\n# Why Processing Refunds can be Challenging for AI (Even the Super Smart Ones!)\n\nEven highly intelligent AI systems face difficulties when it comes to handling refund processes. Here are some essential capabilities that chatbots need to possess for successfully managing complex tasks like refunds:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 많은 규칙을 따라야 합니다: 온라인으로 셔츠를 주문했지만 잘못된 사이즈로 도착했어요. 환불 정책에 따르면 구매 후 30일 이내에 원래 영수증이 있는 새 제품만 환불을 받을 수 있을 수도 있습니다. AI는 이 모든 규칙과 함께 주문 세부 정보 (날짜, 사이즈 등)와 교환 사유 (잘못된 사이즈)를 이해해야 올바른 결정을 내릴 수 있어야 해요. 이 모든 것을 동시에 이해하는 것은 수 많은 퍼즐 조각을 맞추는 것과 같아요!\n- 당신의 말을 이해하기: 교환에 관해 AI에게 “이 셔츠는 너무 커!” 라고 메시지를 보낼 수도 있어요. AI의 자연어 처리 능력은 당신이 환불 정책에 정확한 단어를 사용하지 않아도 (예: “잘못된 사이즈”) 당신이 무슨 의미인지 파악하는 데 능숙해야 해요. 이는 용어가 많은 문자 메시지를 번역하는 것과 같아요 — AI는 당신의 말 뒤에 감추어진 의도를 이해해야 해요.\n- 다른 시스템과 대화하기: 환불을 처리하려면 AI가 여러 컴퓨터 프로그램을 확인해야 할 수도 있어요. 세부 정보를 확인하기 위해 주문 내역을 확인해야 하거나 적용 가능한 규칙을 확인하기 위해 환불 정책 데이터베이스를 참고하거나 환불을 시작하기 위해 금융 시스템에 연결해야 할 수도 있어요. 이 모든 것을 처리해야 하는 것은 여러 공을 질러야 하는 것과 같아요 — AI는 이러한 다양한 시스템과의 소통을 처리하여 환불을 올바르게 처리해야 해요.\n- 상황에 적응하기: 환불 처리를 위한 AI 시스템은 “왜 환불을 요청하시나요?” 나 “제품이 손상되었나요, 아니면 생각이 바뀌었나요?”와 같이 명확히 질문하여 상황에 적응해야 해요. 세부 사항을 확인한 후 자신의 방식으로 접근을 조정해야 해요. 하자가 있는 경우, 사진을 요청하고 전액 환불을 처리할 수 있어야 해요. 생각이 바뀐 경우, 반품 안내나 재설치 수수료가 있는 라벨을 제공하거나 당신에게 가장 잘 맞는 옵션을 제안하며 교환이나 상품권을 제공할 수 있어야 해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은 기업들이 처음에는 기존 문서에서 FAQ를 처리하기 위해 챗봇을 도입하지만, 실제 기회는 챗봇이 복잡한 업무 흐름을 관리할 수 있는 능력에 있습니다. 이러한 시나리오는 챗봇이 특정 정책을 조회하고 내부 시스템과 상호 작용하며 명확한 질문을 하고 다양한 상황에 적응하며 필요할 때 실시간 대화 대상에게 매끄럽게 연결되어야 합니다. 이것이 다음 12개월 동안 가장 중요한 챗봇 혁신이 일어날 곳이며, 기업에 가장 높은 ROI를 제공할 것입니다.","ogImage":{"url":"/assets/img/2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds_0.png"},"coverImage":"/assets/img/2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds_0.png","tag":["Tech"],"readingTime":4},{"title":"모듈형 RAG와 RAG Flow 첫 번째 파트","description":"","date":"2024-06-23 19:19","slug":"2024-06-23-ModularRAGandRAGFlowPart","content":"\n\n# 소개\n\n지난 일년 동안, 검색 증가 생성(Retrieval-Augmented Generation, RAG) 개념은 LLM 애플리케이션 구현 방법으로서 상당한 주목을 받았습니다. 저희는 RAG에 대한 포괄적인 조사 보고서를 작성했는데, 이 보고서는 Naive RAG에서 Advanced RAG 및 Modular RAG로의 전환에 대해 탐구했습니다. 그러나 이 조사는 주로 증가(Source/Stage/Process)를 통해 RAG 기술을 규명하였습니다.\n\n본 글은 특히 Modular RAG 패러다임을 중심으로 합니다. 우리는 Module Type, Module 및 Operator로 구성된 세 단계의 Modular RAG 패러다임을 더 확실히 정의했습니다. 이 패러다임에 따라, 현재 RAG 시스템 내 핵심 기술인 6가지 주요 Module Types, 14개의 Modules 및 40여 개의 Operators에 대해 상세히 다루어 RAG에 대한 포괄적인 이해를 제공하고 있습니다.\n\n다양한 Operator를 조합함으로써, 우리는 다양한 RAG 흐름을 유도할 수 있으며, 이 개념을 이 글에서 명확히 설명하고자 합니다. 광범위한 연구를 기반으로, 일반적인 패턴, 몇 가지 구체적인 구현 사례 및 최상의 업계 사례를 요약하였습니다. (공간 제약으로 인해 이 부분은 Part II에서 다룰 예정입니다.)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글의 목적은 현재 RAG 개발 상태에 대한 더 정교한 이해를 제공하고 미래 발전을 위한 길을 만들어주는 것입니다. 모듈식 RAG는 새로운 연산자, 모듈, 그리고 새로운 플로우의 구성을 용이하게 하는 다양한 기회를 제공합니다.\n\n# 모듈식 RAG란 무엇인가요?\n\nRAG의 발전은 다음과 같은 중요한 측면들에 반영되어 보다 다양하고 유연한 과정을 이끌어내고 있습니다:\n\n- 향상된 데이터 획득: RAG는 기존의 비구조적 데이터를 넘어 반구조적 및 구조적 데이터를 포함하고, 구조적 데이터의 전처리에 중점을 두어 검색의 품질을 향상시키고 모델이 외부 지식 소스에 의존하는 것을 줄이는 방향으로 확장되었습니다.\n- 통합된 기술: RAG는 세부 조정, 어댑터 모듈 사용, 강화 학습을 포함한 다른 기술들과 통합하여 검색 능력을 강화하고 있습니다.\n- 적응형 검색 프로세스: 검색 프로세스는 검색한 내용을 활용하여 생성을 안내하고 그 반대로 하는 등 다단계 검색 강화를 지원하도록 진화했습니다. 또한, 자율적인 판단과 LLM 사용을 통해 검색 필요성을 판단하여 질문에 대한 효율을 높였습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모듈식 RAG의 정의\n\n위에서는 RAG의 신속한 발전이 체인 스타일의 고급 RAG 패러다임을 능가하여 모듈식 특성을 과시하고 있음을 볼 수 있습니다. 현재의 조직 부재와 추상화 부족을 해결하기 위해, Naive RAG와 Advanced RAG의 개발 패러다임을 무리없이 통합하는 모듈식 RAG 접근 방식을 제안합니다.\n\n모듈식 RAG는 매우 확장 가능한 패러다임을 제시하며, RAG 시스템을 모듈 유형, 모듈 및 연산자의 세 단계 구조로 나누고 있습니다. 각 모듈 유형은 RAG 시스템의 핵심 프로세스를 나타내며, 여러 기능 모듈을 포함하고 있습니다. 각 기능 모듈에는 또 다른 여러 특정 연산자가 포함되어 있습니다. 전체 RAG 시스템은 여러 모듈과 해당 연산자들의 순열과 조합으로 이루어진 RAG Flow를 형성하게 됩니다. Flow 내에서 각 모듈 유형에서 다른 기능 모듈을 선택할 수 있으며, 각 기능 모듈 내에서는 하나 이상의 연산자를 선택할 수 있습니다.\n\n이전 패러다임과의 관계\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모듈식 RAG은 RAG 시스템을 다층 구조의 모듈식 형태로 조직합니다. 고급 RAG는 RAG의 모듈식 형태이며, Naive RAG는 고급 RAG의 특수한 경우입니다. 이 세 가지 패러다임 간의 관계는 상속과 발전의 하나입니다.\n\n모듈식 RAG의 장점\n\n모듈식 RAG의 이점은 명백하며, 기존의 RAG 관련 작업에 대한 신선하고 포괄적인 시각을 제공합니다. 모듈식 구성을 통해 관련 기술과 방법들이 명확하게 요약됩니다.\n\n- 연구적 시각. 모듈식 RAG는 확장성이 높아 연구자들이 현재 RAG 개발에 대한 포괄적인 이해를 기반으로 새로운 모듈 유형, 모듈, 그리고 연산자를 제안하기 쉽습니다.\n- 응용 시각. RAG 시스템의 설계 및 구성이 더 편리해지며, 사용자들이 기존 데이터, 사용 시나리오, 하향 작업 등에 따라 RAG Flow를 사용자 정의할 수 있습니다. 개발자들은 또한 현재 Flow 구성 방법을 참고하고, 다른 응용 시나리오와 도메인에 기반하여 새로운 플로우와 패턴을 정의할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Module](/assets/img/2024-06-23-ModularRAGandRAGFlowPart_0.png)\n\n## Module Type — Module — Operators\n\n### 1. Indexing\n\nIndexing, the process of breaking down text into manageable chunks, is a crucial step in organizing the system, facing three main challenges:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 완전하지 않은 내용 표현. 청크의 의미 정보는 세분화 방법에 영향을 받아 중요 정보가 잃거나 긴 문맥 속에서 잠기는 결과를 초래합니다.\n- 부정확한 청크 유사성 검색. 데이터 양이 증가함에 따라 검색에서의 잡음이 커져 잘못된 데이터와 빈번히 일치하게 되어 검색 시스템을 취약하고 신뢰할 수 없게 만듭니다.\n- 명확하지 않은 참조 궤적. 검색된 청크는 어느 문서에서든 유래할 수 있으며 인용 트레일이 없어, 다수의 다른 문서에서 유사하게 의미가 있는 혹은 완전히 다른 주제의 내용을 포함할 수 있습니다.\n\n## 청크 최적화\n\n더 큰 청크는 더 많은 문맥을 포착할 수 있지만, 더 많은 잡음을 생성하여 더 오랜 처리 시간과 높은 비용이 필요합니다. 반면 더 작은 청크는 필요한 문맥을 완전히 전달하지 않을 수 있지만, 더 적은 잡음을 가지고 있습니다.\n\n- 슬라이딩 윈도우\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 요구 사항을 균형 있게 조절하는 한 가지 간단한 방법은 중첩 청크를 사용하는 것입니다. 슬라이딩 창을 활용하면 의미적 전환을 향상시킬 수 있습니다. 그러나 의미적 고려 사항이 부족하다는 제한 사항이 있습니다. - 작은 청크에서 큰 청크로 핵심 아이디어는 검색에 사용되는 청크와 합성에 사용되는 청크를 분리하는 것입니다. 더 작은 청크를 사용하면 검색의 정확도가 향상되고, 더 큰 청크는 더 많은 컨텍스트 정보를 제공할 수 있습니다.\n\n![이미지](/assets/img/2024-06-23-ModularRAGandRAGFlowPart_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특히 한 가지 방법은 더 작은 청크를 검색한 다음 부모 ID를 참조하여 더 큰 청크를 반환하는 것일 수 있습니다. 또는 개별 문장을 검색하고 문장 주변 텍스트 창을 반환할 수도 있습니다.\n\n자세한 정보 및 LlamaIndex 구현.\n\n- 요약\n\n이는 작은 것에서 큰 것으로의 개념과 유사하며, 먼저 더 큰 청크의 개요가 생성되고, 이후 개요에 대해 검색이 수행됩니다. 그런 다음 더 큰 청크에 대해 보조 검색을 수행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 메타데이터 첨부\n\n청크에 페이지 번호, 파일 이름, 작성자, 타임스탬프, 요약 또는 청크가 대답할 수 있는 질문과 같은 메타데이터 정보를 포함시킬 수 있습니다. 그 결과로 검색 범위를 제한할 수 있는 이 메타데이터를 기반으로 검색이 필터링될 수 있습니다. LlamaIndex에서 이 구현을 확인해보세요.\n\n# 구조적 구성\n\n정보 검색을 강화하는 효과적인 방법 중 하나는 문서에 대한 계층적 구조를 설정하는 것입니다. 청크 구조를 구성함으로써 RAG 시스템은 적절한 데이터의 검색과 처리를 가속화할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 계층적 색인\n\n문서의 계층 구조에서 노드는 부모-자식 관계로 배열되어 있으며 이들에 연결된 청크가 있습니다. 각 노드에는 데이터 요약이 저장되어 있어 데이터를 신속하게 탐색하고 RAG 시스템이 어떤 청크를 추출해야 하는지 결정하는 데 도움이 됩니다. 이 접근 방식은 블록 추출 문제로 인한 오류를 완화하는 데도 도움이 됩니다.\n\n구조적 색인을 구축하는 주요 방법은 다음과 같습니다:\n\n- 구조 인식: 문서에서 단락과 문장을 분할\n- 콘텐츠 인식: PDF, HTML, Latex에 내재된 구조\n- 의미 인식: NLP 기술을 활용한 텍스트의 의미 인식과 분할, NLTK와 같은 기법 사용\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대규모로 아커스의 계층적 인덱스를 확인하실 수 있습니다.\n\n- KG 조직 문서\n\n지식 그래프(KGs)를 활용하여 문서의 계층 구조를 구축함으로써 일관성을 유지할 수 있습니다. 이는 다양한 개념과 엔티티 간의 연결을 명확히 하고 환각 가능성을 크게 줄입니다.\n\n다른 이점은 정보 검색 프로세스를 LLM이 이해할 수 있는 지침으로 변환하여 지식 검색의 정확성을 향상시키고 LLM이 맥락에 부합한 응답을 생성할 수 있도록 함으로써 RAG 시스템의 전체적인 효율성을 향상시킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNeo4j 구현 및 LllmaIndex Neo4j 쿼리 엔진을 확인해보세요.\n\nKG를 사용하여 여러 문서를 조직하는 경우, 이 연구 논문 KGP: 지식 그래프 프롬프팅을 참고하실 수 있습니다.\n\n# 2. 사전 검색\n\nNaive RAG의 주요 도전 중 하나는 사용자의 원본 쿼리에 직접 의존하고 있다는 것입니다. 정확하고 명확한 질문을 구성하는 것은 어려우며, 무분별한 쿼리는 하위 수준의 검색 효과를 초래합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 단계에서의 주요 도전 과제는 다음과 같습니다:\n\n- 제대로 작성되지 않은 질문들입니다. 질문 자체가 복잡하며, 언어가 잘 정리되지 않았습니다.\n- 언어 복잡성 및 모호함입니다. 언어 모델은 종종 전문 용어나 다의어적 약어를 다룰 때 어려움을 겪습니다. 예를 들어, “LLM”이 큰 언어 모델을 의미하는지 법적 맥락에서의 법학 석사를 나타내는지를 구분하지 못할 수 있습니다.\n\n## 질의 확장\n\n단일 질문을 복수의 질문으로 확장하면 질문의 내용을 풍부하게 만들어 특정 뉘앙스의 부족을 보충함으로써 생성된 답변의 최적적인 관련성을 보장할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 다중 쿼리\n\nLLM을 통해 쿼리를 확장하기 위해 prompt 엔지니어링을 사용하여 이러한 쿼리들을 병렬로 실행할 수 있습니다. 쿼리의 확장은 무작위가 아니라 신중하게 설계된 것입니다. 이 설계의 두 가지 중요한 기준은 쿼리의 다양성과 범위입니다.\n\n여러 쿼리를 사용하는 한 가지 어려움은 사용자의 원래 의도가 희석될 수 있다는 것입니다. 이를 완화하기 위해 우리는 prompt 엔지니어링에서 모델에게 원본 쿼리에 더 높은 중요도를 할당하도록 지시할 수 있습니다.\n\n- 하위 쿼리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n서브 질문 계획 프로세스는 원본 질문을 완전히 대답할 수 있는 필요한 서브 질문을 생성하는 것을 나타냅니다. 관련 컨텍스트를 추가하는 이 프로세스는 원칙적으로 쿼리 확장과 유사합니다. 구체적으로, 복잡한 질문은 적은 것에서 많은 것으로 유도하는 방법을 사용하여 일련의 보다 간단한 서브 질문으로 분해될 수 있습니다.\n\n- CoVe\n\n쿼리 확장에 대한 다른 접근 방식은 Meta AI가 제안한 Chain-of-Verification(CoVe)의 사용을 포함합니다. 확장된 쿼리는 LLM에 의해 유효성을 검사하여 환각을 줄이는 효과를 얻습니다. 유효성이 검증된 확장된 쿼리는 전형적으로 더 높은 신뢰성을 보입니다.\n\n# 쿼리 변환\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 다시 작성\n\n원본 쿼리는 실제 상황에서 LLM 검색에 항상 최적이 아닙니다. 따라서, 우리는 LLM에 대해 쿼리를 다시 작성하도록 유도할 수 있습니다. 쿼리 다시 작성을 위해 LLM을 사용하는 것 외에도, RRR(Rewrite-retrieve-read)과 같은 특수한 작은 언어 모델을 활용할 수 있습니다.\n\n타오바오 프로모션 시스템에서 쿼리 다시 작성 방법인 BEQUE:Query Rewriting for Retrieval-Augmented Large Language Models의 구현은 장인 쿼리에 대한 회수 효과를 현저히 향상시켰으며, GMV 상승으로 이어졌습니다.\n\n- HyDE\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n쿼리에 대답할 때 LLM은 벡터 데이터베이스의 쿼리 및 계산된 벡터를 직접 검색하는 대신 가정된 답변인 가상 문서를 작성합니다. 이는 문제나 쿼리의 임베딩 유사성을 찾는 대신 답변 간의 임베딩 유사성에 초점을 맞추고 있습니다. 또한 쿼리 간의 검색에 초점을 맞춘 Reverse HyDE도 포함되어 있습니다.\n\n![이미지](/assets/img/2024-06-23-ModularRAGandRAGFlowPart_2.png)\n\n- Step-back Prompting\n\nGoogle DeepMind가 제안한 Step-back Prompting 방법을 사용하면 원본 쿼리를 추상화하여 고수준 개념 질문인 스텝백 질문을 생성할 수 있습니다. RAG 시스템에서는 스텝백 질문과 원본 쿼리 모두 검색에 사용되며, 두 결과 모두 언어 모델 답변 생성의 기초로 활용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 쿼리 라우팅\n\n다양한 쿼리에 따라 다른 RAG 파이프라인으로 라우팅되며, 다양한 시나리오를 수용할 수 있는 유연한 RAG 시스템에 적합합니다.\n\n- 메타데이터 라우터/ 필터\n\n첫 번째 단계는 쿼리에서 키워드(엔티티)를 추출한 후, 키워드 및 청크 내의 메타데이터를 기반으로 필터링하여 검색 범위를 좁히는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Semantic Router\n\n라우팅하는 또 다른 방법은 쿼리의 의미 정보를 활용하는 것입니다. 특정 방법은 Semantic Router를 활용합니다. 물론 의미론적 및 메타데이터 기반 방법을 결합하여 질의 라우팅을 향상시키는 하이브리드 라우팅 방법도 사용할 수 있습니다.\n\nSemantic router 리포지토리를 확인해보세요.\n\n# 쿼리 구성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n유저의 쿼리를 다른 쿼리 언어로 변환하여 대체 데이터 소스에 접근하는 작업입니다. 일반적으로 사용되는 방법은 다음과 같습니다:\n\n- 텍스트를 Cypher로 변환\n- 텍스트를 SQL로 변환\n\n많은 시나리오에서는 구조화된 쿼리 언어(SQL, Cypher 등)가 의미 정보 및 메타데이터와 결합되어 더 복잡한 쿼리를 작성하는 데 자주 사용됩니다. 자세한 내용은 Langchain 블로그를 참조해주세요.\n\n# 3 조회\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n검색 과정은 RAG에서 중요한 역할을 합니다. 강력한 PLM을 활용하여 쿼리와 텍스트를 잠재적 공간에 효과적으로 표현하여 질문과 문서 간의 의미 유사성을 성립하는 데 도움이 됩니다.\n\n고려해야 할 세 가지 주요 사항:\n\n- 검색 효율성\n- 임베딩 품질\n- 작업, 데이터 및 모델의 정렬\n\n# 검색기 선택\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChatGPT가 출시된 이후에는 임베딩 모델 개발에 대한 열기가 높아졌어요. Hugging Face의 MTEB 리더보드는 8가지 작업(클러스터링, 분류, 이중 텍스트, 쌍 분류, 재랭킹, 정보 검색, 시맨틱 텍스트 유사성 및 요약)에 걸쳐 거의 모든 가능한 임베딩 모델을 평가합니다. 또한, C-MTEB는 중국어 임베딩 모델의 능력을 평가하며, 6가지 작업과 35개의 데이터셋을 다루고 있어요.\n\nRAG 애플리케이션을 구축할 때, \"어떤 임베딩 모델을 사용해야 할까?\"에 대한 보편적인 해답은 없습니다. 그러나 특정 임베딩이 특정 사용 사례에 더 적합하다는 점을 알 수 있을 거예요.\n\nMTEB/C-MTEB 리더보드를 확인해보세요.\n\n- Sparse Retriever\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n희박 인코딩 모델은 다소 구식 기술로 간주될 수 있지만 단어 빈도 통계와 같은 통계적 방법에 기반하고 있어서 높은 인코딩 효율성과 안정성을 유지하고 있습니다. 일반적인 계수 인코딩 모델로는 BM25와 TF-IDF가 있습니다.\n\n- 조밀 검색기\n\n신경망 기반의 밀도 인코딩 모델에는 여러 종류가 있습니다:\n\n- BERT 아키텍처에서 구축된 인코더-디코더 언어 모델인 ColBERT와 같은 모델.\n- BGE 및 Baichuan-Text-Embedding과 같은 포괄적인 다중 작업 파인튜닝 모델.\n- OpenAI-Ada-002 및 Cohere Embedding과 같은 클라우드 API 기반 모델.\n- 대규모 데이터 애플리케이션을 위해 설계된 다음 세대 가속화 인코딩 프레임워크 Dragon+.\n- 혼합/하이브리드 검색\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 가지 임베딩 접근 방식은 서로 다른 관련 기능을 캡쳐하고 상호 보완적인 관련 정보를 활용함으로써 상호 이득을 얻을 수 있습니다. 예를 들어, 희박 검색 모델은 밀도 검색 모델을 교육하기 위한 초기 검색 결과를 제공하는 데 사용될 수 있습니다. 게다가 PLM(Pre-trained Language Models)은 희소 검색을 강화하기 위해 용어 가중치를 학습하는 데 활용될 수 있습니다. 구체적으로, 희소 검색 모델이 밀도 검색 모델의 제로샷 검색 기능을 강화하고 희귀 엔티티를 포함하는 쿼리를 처리하는 데 밀도 리트리버를 돕는 것이 증명되었습니다.\n\n![이미지](/assets/img/2024-06-23-ModularRAGandRAGFlowPart_3.png)\n\n# 리트리버 파인튜닝\n\n특히 의료, 법률 및 소유 용어가 풍부한 기타 전문 분야와 같이 사전 훈련된 모델이 임베딩 공간에서 유사하다고 판단하는 컨텍스트가 벗어날 수 있는 경우, 임베딩 모델을 조정함으로써 이 문제를 해결할 수 있습니다. 이러한 조정은 추가적인 노력이 필요하지만 검색 효율성과 도메인 정렬을 크게 향상시킬 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- SFT\n\n도메인별 데이터를 기반으로 자체 세부 조정 데이터 세트를 구축할 수 있습니다. LlamaIndex를 사용하면 빠르게 작업을 완료할 수 있어요.\n\n- LSR (LM-지도 학습 리트리버)\n\n직접 데이터 세트에서 세부 조정 데이터 세트를 구성하는 대신 LSR은 LM이 생성한 결과를 우선적 신호로 사용하여 RAG 프로세스 중에 임베딩 모델을 세부 조정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- RL(강화 학습)\n\nRLHF(사람 피드백에서 강화 학습)에서 영감을 받아 LM 기반 피드백을 활용하여 강화 학습을 통해 리트리버를 강화합니다.\n\n- 어댑터\n\n가끔 전체 리트리버를 세밀하게 조정하는 것은 비용이 많이 들 수 있으며, 특히 API 기반 리트리버를 직접 세밀하게 조정할 수 없는 경우에는 어댑터 모듈을 통합하고 세밀 조정을 진행함으로써 이를 완화할 수 있습니다. 어댑터를 추가하는 또다른 이점은 특정 다운스트림 작업과의 더 나은 조정을 달성할 수 있는 능력입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Task Specific.PRCA: 검색 질의응답을 위한 블랙박스 대규모 언어 모델에 플러그인 리워드 기반 컨텍스트 어댑터를 적합화하는 작업.\n- Task Agnostic. AAR(Augmentation-Adapted Retriver)는 여러 하향 작업을 수용하기 위해 설계된 범용 어댑터를 소개합니다.\n\n## 4. 검색 후\n\n전체 문서 청크를 검색하여 이를 LLM의 컨텍스트 환경에 직접 공급하는 것은 최적의 선택이 아닙니다. 문서 후처리는 LLM이 컨텍스트 정보를 보다 효과적으로 활용하는 데 도움이 될 수 있습니다.\n\n주요 도전 과제는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 중간에 길들이 미아 되다. 인간처럼 LLM은 긴 텍스트의 처음과 끝만을 기억하고, 중간 부분은 잊어 버리기 쉽습니다.\n- 소음/반사실적 청크. 소음이나 사실적으로 모순된 문서를 검색하면 최종 검색 결과 생성에 영향을 줄 수 있습니다.\n- 맥락 창. 큰 모델에서 관련 콘텐츠를 상당량 검색하더라도 맥락 정보의 길이에 제한이 있어 모든 이 콘텐츠를 포함시키기 어렵습니다.\n\n# 재랭크\n\n검색된 문서 청크들을 콘텐츠나 길이를 변경하지 않고, LLM에 대한 더 중요한 문서 청크의 가시성을 높이기 위해 재랭크합니다. 구체적으로는:\n\n- 규칙 기반 재랭크\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특정 규칙에 따라 메트릭을 계산하여 청크를 다시 순위 지정합니다. 일반적인 메트릭은 다음과 같습니다:\n\n- 다양성\n- 관련성\n- MRR (최대 주변 관련성, 1998)\n\nMMR의 아이디어는 중복을 줄이고 결과 다양성을 높이는 데 있습니다. 텍스트 요약에 사용되며 쿼리 관련성과 정보의 독창성을 병합 기준으로 사용하여 최종 키워드 목록에서 구절을 선택합니다.\n\nHayStack에서 해당 순위 지정 구현을 확인해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 모델 기반 재순위\n\n언어 모델을 활용하여 문서 청크를 재정렬하고, 선택지는 다음과 같습니다:\n\n- BERT 시리즈의 인코더-디코더 모델인 SpanBERT\n- Cohere rerank 또는 bge-raranker-large와 같은 특화된 재순위 모델\n- GPT-4와 같은 일반적인 대규모 언어 모델\n\n# 압축 및 선택\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRAG 프로세스에서 흔한 오해는 가능한 많은 관련 문서를 검색하여 연결하여 긴 검색 프롬프트를 형성한다는 것이 유익하다는 믿음입니다. 그러나 과도한 문맥은 더 많은 잡음을 도입할 수 있으며, LLM이 주요 정보를 인식하는 것을 약화시키고 \"중간에서 잃어버림\"과 같은 문제로 이어질 수 있습니다. 이러한 문제를 해결하는 일반적인 접근법은 검색된 콘텐츠를 압축하고 선택하는 것입니다.\n\n- （Long)LLMLingua\n\nGPT-2 Small이나 LLaMA-7B와 같은 정련된 작은 언어 모델을 사용하여 중요하지 않은 토큰을 검색 프롬프트에서 감지하고 제거하여, 사람들이 이해하기 어려운 형태에서 LLM이 잘 이해하는 형태로 변환됩니다. 이 접근 방식은 검색 프롬프트 압축을 위한 직접적이고 실용적인 방법을 제시하여, LLM의 추가 훈련 필요를 없애면서 언어 무결성과 압축 비율을 균형있게 유지합니다.\n\nLLMLingua 프로젝트를 확인해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Recomp\n\nRecomp은 두 가지 유형의 압축기를 소개합니다: 검색된 문서에서 적절한 문장을 선택하는 추출 압축기 및 여러 문서에서 정보를 결합하여 간결한 요약을 생성하는 추상적 압축기입니다. 두 압축기 모두 생성된 요약이 언어 모델의 입력에 앞부분에 추가될 때 최종 작업에서 언어 모델의 성능을 향상시키도록 훈련되었으며, 요약의 간결성을 보장합니다. 검색된 문서가 입력과 관련이 없거나 언어 모델에 추가 정보를 제공하지 않는 경우, 압축기는 빈 문자열을 반환할 수 있어 선택적 확장을 구현할 수 있습니다.\n\n- 선택적 컨텍스트\n\n입력 컨텍스트에서 중복되는 콘텐츠를 식별하고 제거함으로써 입력을 최적화하여 언어 모델의 추론 효율성을 향상할 수 있습니다. 선택적 컨텍스트는 \"불용어 제거\" 전략과 유사합니다. 실제로, 선택적 컨텍스트는 기본 언어 모델에 의해 계산된 자기 정보에 기반하여 어휘 단위의 정보 콘텐츠를 평가합니다. 더 높은 자기 정보를 갖는 콘텐츠를 유지함으로써, 이 방법은 언어 모델 처리를 위한 보다 간결하고 효율적인 텍스트 표현을 제공하며, 다양한 응용 프로그램에서의 성능을 저해시키지 않습니다. 그러나 압축된 콘텐츠와 대상 언어 모델 및 압축을 위해 사용되는 작은 언어 모델 간의 일치를 간과합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 태깅-필터\n\n태깅은 비교적 직관적이고 명료한 방식입니다. 특히, 문서들이 먼저 라벨이 붙여지고, 그 후 쿼리의 메타데이터를 기반으로 필터링됩니다.\n\n- LLM-비평\n\n다른 명확하고 효과적인 방법은 LLM이 최종 답변을 생성하기 전에 검색된 콘텐츠를 평가하도록 하는 것입니다. 이를 통해 LLM은 LLM 비평을 통해 적합하지 않은 문서들을 걸러낼 수 있습니다. 예를 들어, Chatlaw에서 LLM은 참조된 법적 규정에 대한 자체 제안을 받아 그들의 적합성을 평가하게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 5세대\n\n사용자의 쿼리와 검색된 컨텍스트 정보에 기반하여 답변을 생성하기 위해 LLM을 활용하십시오.\n\n# 생성기 선택\n\n시나리오에 따라 LLM의 선택은 다음 두 가지 유형으로 분류될 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 클라우드 API 기반 생성기\n\n클라우드 API를 기반으로하여 OpenAI의 ChatGPT, GPT-4, Anthropic Claude 등을 활용하여 타사 LLMs를 호출합니다. 이점:\n\n- 서버 압력이 없음\n- 높은 동시성\n- 보다 강력한 모델 활용 가능\n\n단점:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터가 제3자를 통해 전달되어 데이터 개인 정보 보호에 대한 우려가 생깁니다\n- 모델을 조정할 수 없음 (대부분의 경우)\n- 온-프레미스\n\n로컬에 배포된 오픈 소스 또는 자체 개발된 LLMs, 예를 들어 람마 시리즈, GLM 등니다. 로컬에 배포된 모델은 클라우드 API 기반 모델과 정반대의 장단점을 가지고 있습니다. 로컬에 배포된 모델은 더 큰 유연성과 더 나은 개인 정보 보호를 제공하지만 더 많은 계산 리소스가 필요합니다.\n\n# 생성기 세부 조정\n\nLLM 사용뿐만 아니라 시나리오와 데이터 특성에 기반한 목표 지향적인 세부 조정은 더 나은 결과를 얻을 수 있습니다. 이것은 온-프레미스 설정을 사용하는 가장 큰 장점 중 하나이기도 합니다. 일반적인 세부 조정 방법은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- SFT\n\n특정 도메인에서 데이터가 부족한 경우, LLM에게 추가 지식을 제공하여 fine-tuning을 통해 LLM에게 도움이 될 수 있습니다. Huggingface의 fine-tuning 데이터 또한 초기 단계로 활용될 수 있습니다.\n\nFine-tuning의 또 다른 이점은 모델의 입력과 출력을 조정할 수 있는 능력입니다. 예를 들어, 이것은 LLM이 특정 데이터 형식에 적응하고 지시된 특정한 스타일로 응답을 생성할 수 있도록 할 수 있습니다.\n\n- RL\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM 출력물을 인간 또는 검색기 선호사항에 맞게 조정하는 것은 가능한 접근 방식입니다. 예를 들어 최종 생성된 답변에 대해 수동으로 주석을 달고 그에 대한 피드백을 통해 강화 학습을 제공하는 것이 있습니다. 인간 선호도에 맞추는 것뿐만 아니라 세밀하게 조정된 모델과 검색기의 선호도에도 부합시킬 수 있습니다.\n\n- 증류\n\n강력한 사유재 모델이나 더 큰 매개변수의 오픈소스 모델에 액세스 할 수 없는 상황에서, 강력한 모델(e.g. GPT-4)을 증류하는 간단하고 효과적인 방법이 있습니다.\n\n- 이중 미세 조정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRAG 프로세스를 제어하는 데 사용되는 모듈을 가리키는 Orchestration입니다. RAG는 더 이상 고정 프로세스를 따르지 않고, 주요 시점에서 결정을 내리고 결과에 따라 동적으로 다음 단계를 선택하는 것을 의미합니다. 이것은 Naive RAG와 비교해 모듈화된 RAG의 주요 특징 중 하나입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n판사 모듈은 RAG 프로세스의 중요한 부분을 평가하여 외부 문서 저장소를 검색해야 하는 필요성, 답변의 만족도 및 추가적인 탐색이 필요한지를 결정합니다. 주로 반복적이고 반복적이며 적응적인 검색에 사용됩니다. 구체적으로는 다음 두 가지 연산자가 주로 포함됩니다:\n\n- 규칙 기반\n\n미리 정의된 규칙을 기반으로 후속 조치가 결정됩니다. 일반적으로 생성된 답변은 평가되고, 그런 다음 점수가 미리 정의된 임계값을 충족하는지에 따라 계속할지 중지할지가 결정됩니다. 일반적인 임계값에는 토큰의 확신 수준이 포함됩니다.\n\n- 프롬프트 기반\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM은 다음 조치를 자율적으로 결정합니다. 이를 실현하는 데 주로 두 가지 접근 방식이 있습니다. 첫 번째는 대화 기록을 기반으로 LLM에게 반영하거나 판단하도록 하는 것으로, ReACT 프레임워크에서 볼 수 있습니다. 이 방법의 장점은 모델을 미세 조정할 필요가 없다는 것입니다. 그러나 판단의 출력 형식은 LLM이 지침을 준수하는 정도에 따라 달라집니다. FLARE는 프롬프트 기반 사례입니다.\n\n- 조정 기반\n\n두 번째 접근 방식은 LLM이 특정 작업을 트리거하기 위해 특정 토큰을 생성하는 것인데, 이 방법은 Toolformer로 거슬러 올라갈 수 있으며 RAG에서 적용되며 Self-RAG에 적용됩니다.\n\n# 퓨전\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 개념은 RAG Fusion에서 유래되었습니다. 질의 확장 섹션에서 언급된 바와 같이 현재 RAG 프로세스는 더 이상 단일 파이프라인이 아닙니다. 종종 다양한 분기를 통해 검색 범위나 다양성을 확대해야 합니다. 따라서 여러 분기로 확장한 뒤에는 퓨전 모듈이 여러 대답을 병합하는데 필요합니다.\n\n- 가능성 앙상블\n\n퓨전 방법은 여러 분기에서 생성된 서로 다른 토큰의 가중치값에 기반하여 최종 출력물의 종합적 선택을 이끌어냅니다. 주로 가중 평균을 사용합니다. REPLUG 참조.\n\n- RRF (Reciprocal Rank Fusion)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRRF는 여러 검색 결과 목록의 순위를 결합하여 하나의 통합된 순위를 생성하는 기술입니다. 워털루 대학교 (캐나다)와 Google과의 협력으로 개발된 RRF는 단일 분기 아래의 청크를 재배치하는 것보다 더 효과적인 결과를 생성합니다.\n\n결론\n\nRAG Flow에 관한 내용은 곧 출판될 PART II에서 소개될 예정입니다.\n\n이번이 Medium에 기사를 처음 게시하는 것이라서 많은 기능에 아직 익숙해지고 있습니다. 피드백과 비평은 언제나 환영합니다.","ogImage":{"url":"/assets/img/2024-06-23-ModularRAGandRAGFlowPart_0.png"},"coverImage":"/assets/img/2024-06-23-ModularRAGandRAGFlowPart_0.png","tag":["Tech"],"readingTime":16},{"title":"초보자를 위한 통합 가이드 Python을 사용한 LLM 로컬 배포 쉽게 하는 방법","description":"","date":"2024-06-23 19:17","slug":"2024-06-23-LLMsMadeAccessibleABeginnersUnifiedGuidetoLocalDeploymentviaPython","content":"\n\n![2024-06-23-LLMsMadeAccessibleABeginnersUnifiedGuidetoLocalDeploymentviaPython_0.png](/assets/img/2024-06-23-LLMsMadeAccessibleABeginnersUnifiedGuidetoLocalDeploymentviaPython_0.png)\n\n인터넷에는 개인 컴퓨터에서 대형 언어 모델 (LLMs)을 실행하는 방법에 대한 가이드가 넘쳐나지만, 이 방대한 정보를 효율적으로 탐색하는 것은 특히 새로운 사용자에게는 어려운 일일 수 있습니다. 대부분의 가이드는 특정 접근 방식에 초점을 맞추기 때문에 초보자들이 포괄적인 길을 찾기 어려울 수 있습니다. 여기서 저의 기사가 등장하여, 몇 가지 유명한 방법들을 하나로 통합하고 초보자들을 위해 특별히 제작된 이를 제공합니다. 이 가이드의 독특한 접근 방식은 Python 기반 방법에 집중하고 있으며, Python의 간단함과 AI 커뮤니티 내에서의 광범위한 사용을 인정하고 있습니다. 이 가이드는 깔끔하고 단계별 지침서, 다양한 Python 라이브러리와 도구에 대한 통찰, 그리고 실용적인 팁을 제공하여 당신의 LLMs 세계로의 진입을 원활하게 만들어주려고 만들어졌습니다. 기술적인 세부 사항의 다양성이 혼란스럽게 느껴졌거나 선택지가 많았던 분들에게는 이 기사가 꼭 필요한 리소스로, 당신의 컴퓨터에서 LLMs를 직접 실행하기 위한 접근 가능하고 깨우침을 주는 여정을 보장합니다. 여러 프레임워크에서 모델을 작업하는 빠른 소개를 제공하는 이 기사는 CPU 또는 GPU를 사용하더라도 모든 측면을 깊이 다루지는 않습니다. 더 포괄적인 통찰을 위해서는 관련 문서를 참고하는 것이 좋습니다.\n\n이 가이드는 다음을 사용하여 LLMs를 로컬로 실행하는 방법을 공유할 것입니다\n- Huggingface: 방대한 모델 저장소와 직관적 인터페이스를 갖춘 포괄적인 라이브러리.\n- Llama.cpp Python: llama.cpp를 활용해 성능이 C++ 수준인 LLMs를 사용하는 Python 친화적 프론트 엔드.\n- llama.cpp 기반 GPT-3.5에 대한 API 드롭인 대체: 모델을 독립된 프로세스에서 실행하면, 동일한 기계 또는 서버에서 GPT 3.5 클라이언트들이 사용하는 API와 유사한 방식으로 추론이 가능해집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n상기한 방법들은 13세대 인텔 랩터 레이크 프로세서가 탑재된 Windows 11 시스템에서 WSL2을 사용하여 성공적으로 테스트되었으며, NVIDIA RTX 3060 카드와 16GB 메모리, CUDA 버전 12.1을 사용하고 있습니다.\n\n# 대형 언어 모델(LLM) 실행에 대한 고려 사항\n\n대형 언어 모델(LLM)의 세계로 진입하면 효율적인 배포와 운영을 위한 중요한 고려 사항이 드러납니다. 하드웨어 요구 사항부터 모델 액세스 프로토콜까지 각 요소가 LLM의 모든 잠재력을 활용하는 데 중요한 역할을 합니다.\n\nLLM의 효율적인 운영을 위해서는 강력한 GPU가 필수적입니다. 퀀터제이션을 필요로 하는 모델들에게 특히 중요한데, GPT-2와 같은 작은 모델은 CPU에서 실행할 수 있지만 시스템 RAM에 제한을 받습니다. 예를 들어, GPT-2의 500MB 크기는 현대 CPU에서 관리가 가능하지만,  Llama 2 7B(13GB 크기)와 같은 큰 모델은 기술적으로 CPU에서 실행할 수 있지만, 충분한 RAM이 있어도 추론 속도가 느려집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nHuggingFace 라이브러리는 자동 모델 다운로드를 용이하게 하고 다양한 모델에 액세스할 수 있도록 도와줍니다. Meta의 Llama2와 같은 일부 모델에 액세스하려면 Hugging Face를 통해 계정 설정, 애플리케이션 검토 및 Meta의 사용 정책 준수가 필요한 승인 프로세스가 필요합니다.\n\nGPU 배포는 GPU RAM 용량에 따라 LLM에 가장 적합합니다. 일반적으로 모델은 float16 또는 float32 포맷이며, NVidia 4060 Ti와 같이 더 높은 용량을 갖는 GPU에서는 전체 모델 로딩이 가능합니다(16GB RAM). RAM이 적은 GPU의 경우 양자화는 원래 크기의 일부로 메모리 요구사항을 줄여 호환성 및 추론 속도를 향상시키지만, 정확도 손실은 최소화됩니다. GPU 기반 LLM 작업에는 전반적으로 양자화를 권장합니다. 양자화는 대규모 언어 모델(LLMs)이 로컬 기계에서 효율적으로 작동할 수 있도록 하는 데 중요한 역할을 합니다. 기본적으로 양자화는 모델의 가중치를 표현하는 데 사용되는 숫자의 정밀도를 줄이는 프로세스로, 성능을 크게 희생하지 않으면서 모델의 크기를 크게 줄입니다. GPU 추론에 대해, 이 가이드는 Nvidia GPU를 CUDA 프레임워크와 함께 사용하는 데 초점을 맞추고 있습니다.\n\nGoogle Colab은 Nvidia GPU에 액세스하여 개인 하드웨어가 필요하지 않게 해주는 편리한 클라우드 기반 솔루션을 제공합니다. 주요 라이브러리와의 쉬운 통합을 지원하여 모델 실행이 간편해집니다. 그러나 무료 계정 사용자는 액세스 기간 제한 및 GPU 가용성과 같은 제한 사항을 직면할 수 있으며, 이는 긴 기간 또는 더 많은 리소스를 필요로 하는 프로젝트에 영향을 줄 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 이러한 구성 요소를 각각 설정하는 구체적인 내용을 살펴보겠습니다.\n\n---\n\n## HuggingFace 사용하기\n\nHugging Face는 자연 언어 처리(NLP) 분야에서 특히 열린 소스 라이브러리인 Transformers를 갖고 인공 지능 분야를 혁신시켰습니다. 이 라이브러리는 텍스트 분류 및 언어 생성과 같은 작업을 위한 다양한 사전 훈련된 모델을 제공하여 복잡한 NLP 작업의 구현을 간단하게 합니다. 그들의 작업은 다양한 분야에서 AI 응용 프로그램을 신속하게 개발할 수 있도록 하며 모델을 처음부터 훈련할 필요가 없게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 설정\n\n작동되는 구성:\n\n- Python 3.8+ (3.10 버전에서 테스트됨)\n- Windows 11, Windows Subsystem for Linux (WSL) 및 Ubuntu 22.04에서 확인됨.\n- Transformer 버전 4.36.1에서 테스트됨 (이후 버전은 inference.py에 변경이 필요할 수 있음)\n- Intel 12세대 이상\n- (선택 사항) GPU (예: Nvidia RTX3060 12 GB) 및 CUDA 12.1이 장착되어 있음\n\n다음 섹션에서는 CPU 전용 및 GPU용으로 Linux 및 Windows에 대한 설치 지침을 나열할 것입니다. 해당 지침에 따라 설치하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLinux/WSL (CPU/GPU)\n\nCPU만 설정하는 경우에 필요한 단계이며 GPU 설정을 구성하기 위한 예비 요구 사항으로도 사용됩니다.\n\n```js\n$ sudo apt install python3.10-venv python3.10-tk\n$ pip install virtualenv \n$ python3.10 -m venv  venv\n$ source ./venv/bin/activate\n(venv)$ pip3 install tk numpy torch bertviz ipython transformers accelerate huggingface_hub hf_transfer\n```\n\nLinux/WSL (GPU)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n리눅스에서 GPU 지원을 위해 추가 패키지 설치하기\n\n```js\n(가상환경)$ pip3 install bitsandbytes accelerate autoawq optimum auto-gptq\n```\n\nWindows 설치 (CPU/GPU)\n\n이 단계들은 CPU에서 설정하는 데 필수적이며, 윈도우에서 GPU 설정을 구성하는 데 기본 요구사항으로 작용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n\u003e pip install virtualenv\n\u003e python3.10 -m venv  venv\n\u003e venv\\Scripts\\activate\n(venv)\u003e pip3 install tk numpy torch bertviz ipython transformers huggingface_hub hf_transfer\n```\n\n+Windows (GPU)\n\n윈도우에서 GPU용 추가 패키지를 설치/업데이트 해주세요.\n\n```js\n(venv)\u003e pip3 install accelerate autoawq optimum auto-gptq\n(venv)\u003e pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n(venv)\u003e python -m pip install bitsandbytes==0.39.1 --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNVidia GPU에서 Large Language Models (LLMs)를 실행하기 위해 몇 가지 설치가 필요합니다:\n\n- PyTorch 및 CUDA 설정: NVidia GPU 기능을 활용하기 위해 torch, torchvision 및 torchaudio를 CUDA 지원과 함께 설치해야 합니다. 일반적으로 Linux 사용자는 기본 PyTorch 설치에서 CUDA 지원을 받지만, Windows 사용자는 torch+cuda 설치를 위한 index-url을 지정해야 합니다. 최신 업데이트에 따르면 PyTorch는 CUDA toolkit 버전 12.1까지 호환됩니다. CUDA 호환성 및 설치 안내에 대한 최신 정보는 PyTorch 웹사이트 (PyTorch — 로컬에서 시작하기)에서 확인할 수 있습니다.\n- bitsandbytes 설치 변형: bitsandbytes의 표준 설치는 Linux에 최적화되어 있으며, 하나의 릴리스 빌드에서 여러 CUDA 버전을 수용합니다. 그러나 Windows 사용자는 소스에서 직접 빌드하거나 비공식 GitHub 저장소를 사용하여 설치해야 합니다. Windows용 구체적인 설치 지침은 이 기사의 이전 섹션에서 찾을 수 있습니다. Windows를 위한 추가 정보 및 안내는 bitsandbytes 비공식 Windows 저장소(https://github.com/jllllll/bitsandbytes-windows-webui)에서 확인할 수 있습니다.\n- Cuda Toolkit 필요성: GPU 기반 작업을 용이하게 하기 위해 https://developer.nvidia.com/cuda-12-1-0-download-archive에서 Cuda toolkit을 설치하세요. 사용 중인 torch+cuda 버전과 CUDA toolkit 버전을 일치시키는 것이 좋습니다. 현재 최신 torch는 CUDA 버전 12.1을 지원합니다.\n- Microsoft C++ 빌드 도구: Windows에서 GPU 프로세스를 실행하려면 Microsoft C++ 빌드 도구를 설치해야 합니다. 이는 accelarate 및 autoawq 패키지의 psutils 종속성으로 인한 것입니다.\n- autoawq 패키지는 GPU에서 AWQ 기반 양자화 모델을 실행해야 하는 경우에만 필요합니다.\n\nWindows 시스템의 특정 요구 사항을 고려하여 NVidia GPU에서 LLMs를 효과적으로 실행하기 위한 중요한 단계입니다. huggingface 설치 및 모델 다운로드에 대한 자세한 정보는 link1과 link2를 확인하세요.\n\nHuggingface에 계정을 생성하면 토큰을 생성할 수 있는 능력이 생깁니다. 이 토큰은 export(예: HF_TOKEN)을 통해 환경에 구성하거나 Huggingface API와 함께 매개변수로 사용하여 모델 다운로드 및 다른 포턜 작업에 사용할 수 있습니다. 이 토큰은 서비스에 액세스하기 위한 패스로 생각할 수 있습니다. 또한 Llama와 같은 특정 모델을 다운로드하려면 승인이 필요할 수 있으며, 계정을 사용하여 Huggingface 포턜을 통해 요청할 수 있습니다. 부여된 모든 승인은 계정과 그 계정으로 생성된 모든 토큰과 연결됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 추론\n\n이 스크립트는 추론 모드를 구체화하여 모델 실행에 적합한 하드웨어(CPU 또는 GPU)를 선택합니다. 만약 추론 모드가 'cpu'로 설정되어 있다면, GPT-2 모델이 로드되며, 작은 크기(~500MB)로 CPU 사용에 이상적이기 때문에 'cpu'로 설정된 device_map을 사용하여 타겟팅된 실행이 가능합니다. 반대로, GPU 사용 시에는 더 큰 Meta LLaMA 2 7B 모델(~13GB)을 선택하며, 'auto'로 설정된 device_map을 활용하여 유연한 장치 할당을 하고 load_in_4bit을 활성화하여 메모리 사용량을 최소화하여 한정된 메모리를 가진 GPU에서 큰 모델의 작동을 촉진합니다. pipeline 함수는 로드된 모델과 토크나이저를 사용하여 텍스트 생성 파이프라인을 생성하며, max_new_tokens와 같은 매개변수를 설정하여 생성된 텍스트의 길이를 제어합니다. 또한 streamer가 파이프라인에 전달되어, 스트리밍된 텍스트 생성이 사용될 것을 나타냅니다.\n\ninference.py\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, pipeline, BitsAndBytesConfig\n\ninference_type = 'cpu'\nif inference_type == 'cpu':\n    # CPU 추론\n    model_name = 'gpt2'  # ~500MB 원본 크기\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map='cpu'\n    )\nelse:    \n    # GPU 추론; 'gpu'\n    model_name = 'meta-llama/Llama-2-7b-hf'  # ~13GB 원본 크기\n    quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name, \n        device_map='auto',\n        quantization_config=quantization_config)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nstreamer = TextStreamer(\n     tokenizer,\n     skip_prompt=False,\n     skip_special_tokens=False\n)\ntext_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=100,  # 생성할 토큰의 최대 수\n    streamer=streamer,     \n)\nprompt='I like apple pie'\ntext_pipeline(prompt) \n``` \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제공된 예제에서는 사전 학습된 모델을 사용하고 있습니다. 이 모델은 일련의 토큰과 같은 특정 키워드를 필요로 할 수도 있습니다. 모델이 이러한 토큰을 요구하는지 확인하려면 tokenizer_config.json 파일에서 add_bos_token 및 add_eos_token 설정을 확인할 수 있습니다. 이를 자동으로 처리하는 실용적인 방법은 text_pipeline 추론 호출에서 add_special_tokens=True (기본값 False)로 설정하는 것입니다. 이렇게 하면 프레임워크가 모델로 전송되기 전에 프롬프트에 특수 키워드를 삽입할지 여부를 결정할 수 있습니다. 위 예시에서는 해당 매개변수를 사용하지 않았습니다.\n\n스크립트 실행\n\n```js\n(venv)$ python inference.py\n```\n\n출력:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBitsAndBytesConfig가 load_in_4bit=True로 전달되어 있습니다. 이는 변환 라이브러리에게 모델의 가중치를 4비트로 줄이라는 지시를하는 것이고, 이는 모델의 원래 크기의 25%에 해당합니다 (Llama 2는 float16 형식입니다). 이는 추론 속도를 향상시킵니다. bnb_4bit_compute_dtype는 torch.bfloat16으로 설정되어 있어 GPU에서 실제 계산이 여전히 float16로 수행되도록 합니다. 선택할 수 있는 대안 옵션은 load_in_8bit=True이며, load_in_4bit 옵션과 상호 배제됩니다. 8비트 모드는 약간 향상된 정확성을 제공하지만 모델 크기를 원래 크기의 50%로 줄이고 추론 속도를 낮춥니다. 양자화 옵션을 선택하지 않은 경우 변환 라이브러리는 가용한 VRAM에 따라 전체 모델을 GPU의 비디오 RAM에로드하는 것을 기본값으로 설정합니다.\n\n# 사전 양자화 모델 로드\n\nGPU 추론 시 모델을 로드할 때마다 양자화하는 대신, 디스크에서 사전 양자화 된 모델을로드할 수 있습니다. 인기있는 양자화 형식 중 두 가지는 GPTQ 및 AWQ입니다. Huggingface는 둘 다 지원합니다.\n\nAWQ 유형 모델의 경우 아래 코드로 모델로드 라인을 대체하면 됩니다 (GPU 추론 경로에서). BitsAndBytesConfig 매개 변수가 생략됩니다. 왜냐하면 모델이 이미 양자화되어 있기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nmodel_name='TheBloke/zephyr-7B-alpha-AWQ' \nmodel = AutoModelForCausalLM.from_pretrained( \n    model_name, \n    device_map='auto')\n```\n\nGPTQ 모델의 경우 아래 라인으로 모델 로드를 대체합니다.\n\n```js\nmodel_name='TheBloke/zephyr-7B-beta-GPTQ' \nmodel = AutoModelForCausalLM.from_pretrained( \n    model_name, \n    revision='main', \n    device_map='auto')\n```\n\nGitHub 브랜치에 해당하는 revision을 사용했습니다. zephyr-7B-beta-GPTQ 양자화 버전은 서로 다른 브랜치에 저장되어 있습니다. 자세한 내용은 https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ의 ‘Files and versions’ 탭을 참조해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# GGUF 양자화\n\nGGUF는 llama.cpp에서 소개된 인기 있는 양자화 방법입니다. ctransformers는 GGML/GGUF 라이브러리를 사용하여 C/C++로 구현된 Transformer 모델용 Python 바인딩입니다. 또한 RAG Langchain과 통합되어 있습니다.\n\n귀하의 구성에 따라 ctransformers 패키지를 설치하세요.\n\n```js\n(venv)$ pip install ctransformers\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nhuggingface-cli를 사용하여 GGUF 형식 모델을 다운로드하거나 from_pretrained() 호출을 통해 해당 모델의 첫 API 호출 시 자동으로 다운로드됩니다. 아래 코드 샘플은 GGUF 형식의 모델에 대해 ctransformers를 활용하는 방법을 보여줍니다. from_pretrained 메서드는 이전에 다운로드되지 않은 경우 Hugging Face에서 모델을 자동으로 가져옵니다. zephyr GGUF와 같은 일부 모델 유형은 동일한 리포지토리에 파일로 저장됩니다. 여기서 리포지토리 이름은 'TheBloke/zephyr-7B-beta-GGUF'이고 모델 파일은 zephyr-7b-beta.Q4_K_M.gguf입니다.\n\ninference.py\n\n```js\nfrom ctransformers import AutoModelForCausalLM\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\n    'TheBloke/zephyr-7B-beta-GGUF',\n    model_file='zephyr-7b-beta.Q4_K_M.gguf',\n    model_type='llama',\n    gpu_layers=32\n)\nprompt='\u003cs\u003eI like apple pie'\n# Do inference with streaming\nstream=llm(prompt, stream=True)\nfor chunk in stream:\n    print(chunk, end=\"\", flush=True)\n```\n\n사용 중인 특정 모델에 따라 gpu_layers 설정을 조정하세요. 이 경우, GPU에 32개 레이어를 할당하도록 설정합니다. 추론에 stream=True을 설정하면 모델이 생성하는 토큰을 실시간으로 표시할 수 있습니다. zephyr용 다양한 양자화된 모델 버전은 https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF에서 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n\n# LLama.cpp python\n\nLLama.cpp은 다양한 백엔드(CUDA, Metal, OpenCL, SYCL)를 지원하여 적응성을 높인 유연한 C/C++ 라이브러리로, Meta의 LLaMA 모델과 같은 대규모 언어 모델에서 추론을 실행할 수 있는 능력으로 AI 분야에서 인식을 얻고 있습니다. llama-cpp-python 패키지를 통한 Python 통합은 사용자가 C/C++의 성능을 누리면서도 Python의 간편함을 누릴 수 있도록 합니다.\n\nllama-cpp-python을 설치하는 선호하는 방법은 소스에서 컴파일하는 것입니다. 이 방법을 권장하는 이유는 기밀된 C/C++ 라이브러리인 llama.cpp이 특정 시스템에 맞춘 컴파일러 최적화를 활용하기 때문입니다. 미리 빌드된 이진 파일을 선택하면 이러한 최적화를 포기하거나 다양한 플랫폼용 이진 파일을 관리해야 할 수도 있습니다. llama-cpp-python을 컴파일하면 llama.cpp 라이브러리를 자동으로 라이브러리 파일(lib)로 빌드합니다. 이 라이브러리 파일은 특정 바인딩을 통해 Python에서 활용되어, Python 스크립트가 llama.cpp의 기능에 액세스하고 사용할 수 있도록 합니다. 컴파일 세부 정보는 llama-cpp-python의 github 저장소를 참조하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시스템에는 해당 버전의 BLAS 라이브러리가 설치되어 있어야 합니다. llama.cpp README의 ‘BLAS 빌드’ 섹션을 확인해보세요.\n\nIntel ARC GPU는 SYCL을 통해 지원되며, 이를 위해서는 Intel OneAPI가 설치되어 있어야 합니다. README의 SYCL 섹션을 확인해보세요.\n\nllama-cpp-python의 자세한 빌드 지시사항은 https://github.com/abetlen/llama-cpp-python에서 찾을 수 있습니다.\n\n간단한 지시사항은 여기에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCPU 또는 GPU용 llama-cpp-python 패키지를 설치해 보세요.\n\n```js\n# CPU 빌드용\n(venv)$ CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n\n# Nvidia GPU 빌드용\n(venv)$ CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n\n# Intel ARC GPUS를 위한 SYCL을 통한 빌드\nsource /opt/intel/oneapi/setvars.sh   \nCMAKE_ARGS=\"-DLLAMA_SYCL=on -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx\" pip install llama-cpp-python\n```\n\nllama-cpp-python은 Hugging Face에서 얻을 수 있는 GGUF 모델 형식을 요구합니다. 그러나 llama의 API 사양으로 인해, 모델을 수동으로 다운로드하여 다른 디렉토리에 저장한 다음 해당 디렉토리 경로를 llama-cpp-python에 지정해야 합니다. 이번에는 우리의 정규 절차에서 벗어나, 명시적 로컬 폴더로 직접 다운로드하는 방식으로 사용 설명서 tuning된 Llama 2 모델을 사용하겠습니다.\n\n```js\n(venv)$ huggingface-cli download TheBloke/Llama-2-7b-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks True\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nlocal-dir-use-symlinks를 True로 설정하여 활성화하면 현재 디렉토리에 모델을 캐시 디렉토리에 이미 다운로드된 모델을 가리키는 심볼릭 링크를 생성하려고 시도하므로 시간과 공간을 효율적으로 사용할 수 있습니다. 심볼릭 링크를 사용하지 않고 현재 디렉토리에 모델의 완전한 사본이 필요하다면 이 옵션을 생략하는 것이 가장 좋습니다.\n\n현재 디렉토리에 모델을 다운로드한 경우 model_path는 절대 경로 또는 상대 경로로 지정해야 합니다.\n\ninference.py\n\n```python\nfrom llama_cpp import Llama\nimport llama_cpp\nllm = llama_cpp.Llama(model_path=\"./llama-2-7b-chat.Q4_K_M.gguf\",\n                verbose=True, n_gpu_layers=-1, chat_format=\"llama-2\")\nprompt = '[INST] Hi there, write me 3 random quotes [/INST]'\nstream = llm(prompt, max_tokens=2048, echo=False, temperature=0, stream=True)\nresult = \"\"\nfor output in stream:\n    result += output['choices'][0]['text']\n    print(output['choices'][0]['text'], end=\"\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 코드는 시스템이 Llama2 7B 모델의 모든 레이어(n_gpu_layers=-1)를 GPU에로드하도록 지시합니다. 사용 가능한 VRAM에 따라 n_gpu_layers 값을 수정하여 CPU와 GPU 사이의 작업 부하를 분산시킬 수 있으며, 예를 들어 대규모 모델을 효과적으로 관리하기 위해 작업을 균등하게 분할할 수 있습니다.\n\n프롬프트 구문의 중요성\n\n위에서 사용된 모델은 채팅용으로 조정된 모델이므로 프롬프트 형식은 매우 중요합니다([INST] 사용). 이는 모델의 이해와 작업 실행에 상당한 영향을 미치기 때문입니다. 지시에 따라 응답을 생성할 수 있도록 특별히 설계된 지시에 조정된 모델은 상세 명령을 따르고 프롬프트에서 제공된 지침에 따라 응답을 생성합니다. 올바른 구문은 모델이 지시의 의도를 정확하게 이해하고 결과가 정확하고 타당한 것을 보장하는 데 중요합니다. 또한 다양한 모델 아키텍처는 다른 프롬프트 구문에 최적으로 반응할 수 있도록 세밀하게 조정됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# llama.cpp 기반의 GPT-3.5 대체품입니다.\n\n이 섹션은 llama.cpp를 OpenAI의 GPT 엔드포인트 대신 사용할 수 있는 방법을 탐구합니다. llama.cpp 모델을 활용하여 OpenAI의 서비스에 의존하지 않고 로컬 llama.cpp 모델을 사용하여 GPT 기반 애플리케이션을 운영할 수 있습니다. 로컬 API 서버를 실행함으로써 OpenAI의 GPT API 엔드포인트의 기능을 모방하면서도 llama 모델을 사용하여 요청을 처리할 수 있습니다. 이는 GPT-3.5 또는 GPT-4를 대상으로 한 애플리케이션이 llama.cpp를 사용하도록 원활하게 전환할 수 있음을 의미합니다. 최종 목표는 비용을 절감하는 것뿐만 아니라 데이터가 로컬 환경 내에서 개인 정보 보호 및 안전성을 유지하도록 하는 것입니다.\n\n![이미지](/assets/img/2024-06-23-LLMsMadeAccessibleABeginnersUnifiedGuidetoLocalDeploymentviaPython_2.png)\n\n먼저 llama-cpp-python과 서버 지원 및 필수 요소를 설치해야 합니다. 패키지가 처음에 CPU 사용을 위해 설정되어 있고 이제 GPU 사용으로 전환하고 싶은 경우(또는 그 반대인 경우) 새 대상을 위해 설치 명령을 다시 실행하여 다시 설치해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# CPU 빌드용\n(venv)$ CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python[server] --upgrade --force-reinstall --no-cache-dir\n\n# Nvidia GPU 빌드용\n(venv)$ CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python[server] --upgrade --force-reinstall --no-cache-dir\n```\n\nopenai 패키지 설치\n\n```js\n(venv)$ pip install openai\n```\n\nhuggingface-cli 도구를 사용하여 로컬 폴더에 모델 다운로드하기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n(venv)$ huggingface-cli TheBloke/Llama-2-7b-Chat-GGUF 다운로드 llama-2-7b-chat.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks True\n```\n\n모델에 입력하기 전에 프롬프트를 Llama-2 형식으로 구조화하는 방법과 함께 서버를 실행합니다.\n\n```js\n(venv)$ python3 -m llama_cpp.server --model ./llama-2-7b-chat.Q4_K_M.gguf --n_gpu_layers 35 --chat_format llama-2\n```\n\n서버는 기본적으로 포트 8000을 사용하도록 구성되어 있습니다. 아래의 클라이언트 스크립트에서 서비스에 연결하도록 설계된 경우 사용자 정의 포트 실행에 필요한 포트 번호를 수정하세요. 모델 매개변수에는 더미 값(e.g. xxxxx)이 할당되어 있으며, 이는 서버에서 무시되며 본 예제에서는 간편함을 위해 인증 프로세스가 포함되어 있지 않습니다. 또한 서버가 올바르게 실행 중인지 확인하려면 브라우저에서 http://localhost:8000/docs#/을 방문하세요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nclient.py\n\n```python\nfrom openai import OpenAI\nclient = OpenAI(base_url=\"http://127.0.0.1:8000/v1\") \nstream = client.chat.completions.create(\n  model=\"xxxxx\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a well-read scholar with a deep appreciation for literature, especially when it comes to the subject of artificial intelligence (AI)\"},\n    {\"role\": \"user\", \"content\": \"Give me 3 quotes on AI.\"}\n  ],\n  temperature=0.01,\n  stream=True,  \n)\nfor chunk in stream:\n    if not chunk.choices or chunk.choices[0].delta.content is None:\n        continue\n    print(chunk.choices[0].delta.content, end=\"\")\nprint(\"\\n\")\n```\n\nRun the client script in another terminal.\n\n```bash\n(venv)$ python client.py\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수신된 출력 스트림은 터미널 창에 표시됩니다.\n\n출력 (모델에서 받은 대로):\n\n\n---\n# 참고 자료\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Hugging Face [https://huggingface.co/]\n- llama.cpp에 대한 Python 바인딩 [https://github.com/abetlen/llama-cpp-python]\n- 양자화 [https://huggingface.co/docs/optimum/concept_guides/quantization]\n- 당신에게 적합한 양자화 방법은 무엇인가요? (GPTQ vs. GGUF vs. AWQ) [https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be]\n- Nvidia CUDA [https://developer.nvidia.com/cuda-12-1-0-download-archive]","ogImage":{"url":"/assets/img/2024-06-23-LLMsMadeAccessibleABeginnersUnifiedGuidetoLocalDeploymentviaPython_0.png"},"coverImage":"/assets/img/2024-06-23-LLMsMadeAccessibleABeginnersUnifiedGuidetoLocalDeploymentviaPython_0.png","tag":["Tech"],"readingTime":18},{"title":"LLMs의 작동 원리 - 대형 언어 모델의 비밀 밝혀보기","description":"","date":"2024-06-23 19:16","slug":"2024-06-23-HowLLMsWork","content":"\n\n\n![LLMs operation](/assets/img/2024-06-23-HowLLMsWork_0.png)\n\nLLMs (Large Language Models) operate by predicting the next token based on a sequence of previous tokens. Each generated token is then used as input to generate the next one, enabling the model’s text generation capabilities.\n\n## Step 1: Prompts\n\nThe process starts with receiving a prompt, which is tokenized and converted into embeddings, or vector representations, of the input text. These embeddings, initially random, are learned during model training and represent a non-contextualized vector form of the input token.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 2: 인코딩\n\n이 모델은 층별 어텐션 및 피드포워드 계산을 수행하여 어휘 내 각 단어에 숫자(logit)를 할당합니다(디코더 모델인 GPT-X, LLaMA 등) 또는 컨텍스트화된 임베딩을 출력합니다(버트, 로버타, 일렉트라 등과 같은 인코더 모델).\n\n## 단계 3: 정규화\n\n디코더 모델의 경우, 최종 단계는 Softmax 함수를 사용하여 비정규화된 로짓을 정규화된 확률 분포로 변환하는 것을 포함합니다. 이는 생성된 텍스트에서 다음 단어를 결정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 추가 세부 정보\n\n## 토큰화\n\n원시 입력 텍스트는 토큰화를 통해 종종 하위 단위 또는 단어로 분해됩니다. 이 과정을 통해 입력이 모델의 고정 된 어휘와 일치하도록되어 모델에 인식되도록합니다.\n\n## 임베딩\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 토큰은 임베딩 행렬을 사용하여 고차원 벡터로 매핑됩니다. 이 벡터 표현은 토큰의 의미적 의미를 포착하고 모델의 후속 계층에 입력으로 작용합니다. 이러한 임베딩에는 위치 인코딩이 추가되어 토큰 순서에 대한 정보를 제공하며, 내재된 시퀀스 인식이 부족한 트랜스포머와 같은 모델에 중요합니다.\n\n## 트랜스포머 아키텍처\n\n현대 LLM의 핵심인 트랜스포머 아키텍처는 여러 계층으로 구성됩니다. 각 계층에는 멀티 헤드 셀프 어텐션 메커니즘과 위치별 피드포워드 네트워크가 포함됩니다.\n\n![트랜스포머 아키텍처](/assets/img/2024-06-23-HowLLMsWork_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n셀프 어텐션 메커니즘은 토큰이 다른 토큰과의 관련성을 평가할 수 있게 하여 모델이 입력의 관련 부분에 집중할 수 있게 합니다. 그 결과로 얻는 정보는 각 위치에서 독립적으로 피드포워드 신경망을 통해 처리됩니다.\n\n셀프-어텐션 또는 피드포워드 네트워크 등 각 서브 레이어에는 잔차 연결 후 레이어 정규화가 이어집니다. 이 설정은 활성화를 안정화시키고 학습을 가속화하는 데 도움이 됩니다.\n\n트랜스포머 레이어를 통과한 후 각 토큰의 최종 표현은 로짓 벡터로 변환됩니다. 각 로짓은 모델 어휘 중 단어에 해당하며 해당 단어가 시퀀스에서 다음 단어일 가능성을 나타냅니다.\n\n소프트맥스 함수는 로짓에 적용되어 확률로 변환됩니다. 이 정규화에 의해 확률은 합이 1이 되도록 보장되며 각 확률은 0과 1 사이에 있습니다. 가장 높은 확률을 가진 단어가 시퀀스에서 다음 단어로 선택됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 테이블 태그를 마크다운 형식으로 변경해 주세요.","ogImage":{"url":"/assets/img/2024-06-23-HowLLMsWork_0.png"},"coverImage":"/assets/img/2024-06-23-HowLLMsWork_0.png","tag":["Tech"],"readingTime":2},{"title":"사이드 허슬 구축을 위한 10가지 ChatGPT 프롬프트 템플릿","description":"","date":"2024-06-23 19:14","slug":"2024-06-23-10ChatGPTPromptTemplatesForBuildingSideHustles","content":"\n\n요즘 사람들은 부가 수입을 원합니다.\n\n하지만 대부분의 사람들은 개인적인 이유나 업무로 인해 실패합니다.\n\n여기 부가 수입을 구축하는 데 도움이 되는 10가지 ChatGPT 프롬프트 템플릿이 있습니다:\n\n![ChatGPT Prompt Templates](/assets/img/2024-06-23-10ChatGPTPromptTemplatesForBuildingSideHustles_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수익성 높은 부업 기회를 식별하기\n\n당신은 전문 비즈니스 코치입니다. 저는 [바로 문제에 대한 자세한 배경 설명을 포함하여 직면한 문제를 언급합니다]. [특정 산업 또는 분야]에서 수익성 있는 부업 기회를 찾도록 도와주실 수 있나요? 각 기회마다 잠재적인 수익, 필요한 기술, 그리고 초기 투자 비용을 상세히 나열해주시기 바랍니다. 결과물이 [자세한 예를 들어 어떻게 되었으면 하는지를 언급합니다].\n\n시장 조사 및 검증\n\n당신은 전문 비즈니스 코치입니다. 저는 [바로 문제에 대한 자세한 배경 설명을 포함하여 직면한 문제를 언급합니다]. [특정 시장 또는 분야]에서 부업 아이디어를 검증하고 싶습니다. 시장 조사를 수행하는 단계적인 프로세스를 개요하고, 분석해야 할 주요 지표, 데이터 수집 방법, 그리고 결과를 해석하는 방법을 설명해주실 수 있을까요? 결과물이 [자세한 예를 들어 어떻게 되었으면 하는지를 언급합니다].\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비즈니스 계획 수립하기\n\n당신은 전문 비즈니스 코치입니다. 저는 [배경 내용과 함께 직면한 문제를 자세히 언급하세요]. 나의 부업 아이디어인 [아이디어 간단한 설명]에 대한 포괄적인 비즈니스 계획을 작성하는 데 도와주세요. 시장 분석, 경쟁 환경, 마케팅 전략, 재무 계획 및 운영 계획 섹션을 포함해 주세요. 출력물은 [예시를 들어 자세히 설명하세요] 해 주시기 바랍니다.\n\n온라인 존재감 확립하기\n\n당신은 전문 비즈니스 코치입니다. 저는 [배경 내용과 함께 직면한 문제를 자세히 언급하세요]. [특정 산업이나 특정 분야]에서 나의 부업을 위해 강력한 온라인 존재감을 구축하는 방법은 무엇인가요? 웹사이트 설정, SEO 최적화, 소셜 미디어 프로필 작성 및 온라인 커뮤니티 구축 전략에 대한 상세 가이드를 제공해주세요. 출력물은 [예시를 들어 자세히 설명하세요] 해 주시기 바랍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가격 전략과 이윤 마진\n\n당신은 전문 비즈니스 코치입니다. [면책 조항으로 자세한 문제를 설명하십시오]. 나는 사이드 허슬 [제품/서비스 설명]에 대한 가격 전략을 개발하는 데 도움이 필요합니다. 원가, 경쟁사 가격, 인식된 가치, 그리고 이윤 마진과 같은 요소를 고려한 가격 설정에 대해 자세한 방법을 제시해 줄 수 있나요? 결과물을 [예시와 함께 자세히 원하는 방법을 언급하십시오].\n\n마케팅 및 홍보\n\n당신은 전문 비즈니스 코치입니다. [면책 조항으로 자세한 문제를 설명하십시오]. 사이드 허슬 [제품/서비스 설명]를 [특정 대상 고객군을 명시하십시오]에게 홍보하기 위한 효과적인 마케팅 전략을 제안해 줄 수 있나요? 온라인과 오프라인 전술을 모두 포함하며 비용 효율적인 방법과 효과 측정 방법에 중점을 두어 주실 수 있나요? 결과물을 [예시와 함께 자세히 원하는 방법을 언급하십시오].\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시간 관리와 생산성 관리\n\n당신은 전문 비즈니스 코치입니다. 저는 [배경 내용과 함께 직면한 문제에 대해 자세히 언급합니다]. 본업과 부업을 균형있게 유지하면서 효과적인 시간 관리와 생산성 전략은 무엇일까요? 제 스케줄을 조직화하고 우선순위를 설정하며 업무-생활 균형을 유지하기 위한 상세한 계획을 제시해 주세요. 저는 [예시와 함께 원하는 결과물에 대해 자세히 언급해 주세요].\n\n부업 키우기\n\n당신은 전문 비즈니스 코치입니다. 저는 [배경 내용과 함께 직면한 문제에 대해 자세히 언급합니다]. [비즈니스를 설명하는 부업]을 확장하여 더 많은 수익을 창출하고 싶습니다. 제품 라인 또는 서비스 확장, 새로운 시장 진출, 자동화 도구, 그리고 고용 또는 외주화 작업을 포함한 상세한 성장 전략을 개요로 제시해 주실 수 있나요? 저는 [예시와 함께 원하는 결과물에 대해 자세히 언급해 주세요].\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n법적 및 재무 고려 사항\n\n당신은 전문 비즈니스 코치입니다. 저는 [배경 문맥과 함께 직면한 문제를 자세히 언급합니다]. [특정 산업이나 지역]에서 부업을 시작할 때 알아야 할 주요 법적 및 재무 고려 사항은 무엇인가요? 사업 구조, 세금, 허가, 계약 및 장부관리에 대해 자세한 정보를 제공해주세요. 제가 바라는 것은 [예시를 포함하여 자세히 언급].\n\n기술과 도구 활용\n\n당신은 전문 비즈니스 코치입니다. 저는 [배경 문맥과 함께 직면한 문제를 자세히 언급합니다]. 내 부업 [비즈니스 설명] 운영을 간소화하는 데 도움이 될 수 있는 기술 도구 및 플랫폼을 추천해주실 수 있나요? 프로젝트 관리, 고객 관계 관리 (CRM), 회계 및 온라인 마케팅을 위한 도구에 대해 자세한 설명을 제공해주세요. 제가 바라는 것은 [예시를 포함하여 자세히 언급].\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 내용이 도움이 되었으면 좋겣네요.\n\n좋은 하루 보내세요!","ogImage":{"url":"/assets/img/2024-06-23-10ChatGPTPromptTemplatesForBuildingSideHustles_0.png"},"coverImage":"/assets/img/2024-06-23-10ChatGPTPromptTemplatesForBuildingSideHustles_0.png","tag":["Tech"],"readingTime":3},{"title":"프롬프트 엔지니어링은 잊어라, ChatGPT가 완벽한 프롬프트를 작성해준다","description":"","date":"2024-06-23 19:13","slug":"2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou","content":"\n\n![This is an image](/assets/img/2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou_0.png)\n\n저는 오랫동안 프롬프팅을 해 왔는데, 솔직히 말하면 지칩니다. 제가 신경 쓰고 싶은 건 결과물뿐인데 말과 구조를 고민해야 하니 짜증나요. 훌륭한 프롬프트를 작성하려는 반복적인 과정은 힘들고 시간이 많이 듣게돼서, 지금은 변화할 때가 되었다고 생각했습니다.\n\n프롬프트 엔지니어링 기술은 본질적으로 언어 작업이라는 것을 깨달았죠. ChatGPT는 언어의 대가이기 때문에, 그냥 ChatGPT를 프롬프트 엔지니어로 만들면 어떨까요?\n\n이제 ChatGPT가 1년 넘게 널리 사용되어왔기 때문에, 출력물의 품질을 향상시키는 데 입증된 존경받는 프롬프팅 기술이 많이 있습니다. 저는 이러한 기술 중 일부를 저 자신의 배운 전략과 결합하여 여러분을 전문가로 만들어주는 GPT를 만들었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## ChatGPT Prompt Engineer 작업 방법\n\n최고의 프롬프트 작성자를 만들기 위해 프롬프트 작업을 조금 해봤어요. 다소 고차원적인 관점에서, 나는 GPT에게 프롬프트 마법사로 변신하는 과정을 알려주었어요:\n\n- 사용자의 프롬프트를 이해하기\n- 프롬프트 규칙을 준수하기 위해 사용자로부터 필요한 모든 정보를 수집하기 위해 사용자에게 질문하기 (아래에서 찾을 수 있어요)\n- 이 프롬프트가 복수 단계 프롬프트 또는 단일 프롬프트로 이익을 얻을 수 있는지 평가하기\n- 프롬프트 규칙을 사용하여 프롬프트 (또는 프롬프트 체인) 재작성하기\n\n이 과정 이후에, 우리는 새로운 ChatGPT 인스턴스에 출력된 프롬프트를 삽입하여, 와, 우리는 우수하게 최적화된 결과물을 얻을 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 유도 규칙\n\n위에서 말했듯이, ChatGPT에 제공한 유도 규칙은 존경받는 연구 기반의 가장 좋은 실천법과 제 개인적인 특별한 비법을 적절히 결합한 것입니다. 이러한 규칙 중 많은 것들이 Sheila Teo의 놀라운 글에서 영감을 받았습니다.\n\n자신의 프롬프트를 작성하는 데 흥미가 있는 경우, 아래의 전체 유도 규칙 목록을 확인해보세요!\n\n1. 항상 COSTAR 프롬프트 프레임워크를 사용하세요:\n   - **C (문맥)**: 작업에 대한 필수적인 배경 정보나 설정을 제공하세요. 이를 통해 LLM이 다루는 특정 시나리오나 도메인을 이해하고, 관련성 높은 응답을 더 많이 얻을 수 있습니다.\n   - **O (목적)**: 프롬프트의 목적이나 목표를 명확하게 표현하세요. LLM이 이 특정 목표를 이루도록 하면서 자신의 초점이 이 목표에 머물도록 보장하세요.\n   - **S (스타일)**: 응답의 원하는 스타일을 정의하세요. 이는 특정 직업군(과학자 또는 저널리스트 등)의 문체를 모방하거나, 형식적 보고서나 창작적 소설 같은 특정 장르의 서술 톤을 모방하는 것까지 다양할 수 있습니다.\n   - **T (톤)**: 응답의 감정적이거나 태도적인 색조를 결정하세요. 형식적이든, 캐주얼하든, 열정적이든, 공감적이든, 톤을 설정하여 응답이 의도한 감정과 일치하도록 보장하세요.\n   - **A (대상)**: 응답이 의도한 대상 독자를 식별하세요. 전문가, 초보자 또는 일반 독자 등에 따라 LLM의 응답의 콘텐츠와 복잡성을 조정하여 이해와 참여를 보다 향상시킬 수 있습니다.\n   - **R (응답 형식)**: 응답이어야 하는 형식을 명시하세요. 목록, 구조화된 보고서, JSON 객체, 서술 등이 될 수 있습니다. 형식을 정의하면, 이후 분석, 표현 또는 추가 처리 등에 적합한 응답을 생성하는 데 도움이 됩니다.\n\n2. 복잡한 작업을 간단한 대화식 대화의 일련의 단순한 프롬프트로 나누세요.\n3. '하다(do)'와 같은 긍정적 명령을 사용하며, '하지 마라(don't)'와 같은 부정적 언어는 피하세요.\n4. 예시 중심의 유도 사용하기 (Few-shot prompting 사용).\n5. 다음 표현 사용하기: \"당신의 작업은\"과 \"당신은 꼭\".\n6. \"단계별로 생각해보기\"와 같이 선행 단어 항상 사용하기.\n7. 모델에게 역할을 할당하기, 즉 \"당신은 전문가 ___\"라고 명시하기.\n8. 프롬프트 내에서 특정 단어나 구가 반복되도록 하기.\n9. 가능한 경우 사고 연쇄 유도 (CoT)를 유도하여 LLM이 각 단계를 더 깊게 파고들도록 하기.\n10. 출력 초기화자 사용하기, 프롬프트 끝에 기대되는 응답의 시작을 포함하여 프롬프트를 마무리하기. 출력 초기화자를 활용하여 프롬프트를 시작하는 것으로 응답을 예상하는 응답을 마무리하세요.\n11. 상세하게 서술된 에세이/텍스트/단락/기사 또는 어떠한 유형의 텍스트를 작성하려면: \"세부적으로 [주제]에 대해 초상세하게 [에세이/텍스트/단락]를 작성해주세요. 필요한 모든 정보를 추가하여 상세히 작성하세요\".\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## The ChatGPT Prompt Engineer in action\n\n이제 공유 가능한 GPT를 가지고 있으므로, 나만의 맞춤형 GPT 프롬프트 엔지니어를 여러분과 공유할 수 있습니다. 이 과정을 따릅니다.\n\n[여기에서 찾을 수 있어요.](link)\n\n여러분들이 여러분만의 GPT 인스턴스에서 이를 사용하고 싶다면, 단순히 위에서 제시된 가이드라인과 프롬프트 규칙을 복사하여 ChatGPT 대화 중 어떤 것이든 쉽게 프롬프트 엔지니어링 세션으로 변환할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요즘은 작업 방식을 보여드리겠습니다. 먼저, 프롬프트를 요청할 내용을 생각해야 합니다. 이 경우 GPT가 조금 창의적이어야 하는 것을 시도해 봅시다. 여기 내 아이디어의 시작점입니다. 절로 아름다운 프롬프트로 변할 것입니다:\n\n![Starting Idea](/assets/img/2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou_1.png)\n\n이제 프롬프트 위자드가 제 프롬프트를 다시 쓸 것입니다:\n\n![Prompt Wizard](/assets/img/2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 텍스트를 번역하면 다음과 같습니다.\n이 프롬프트는 잘 쓰여져 있어서 좋은 결과를 얻기에 좋습니다. 다음 단계는 이 프롬프트를 새로운 채팅에 전달하는 것입니다:\n\n\n![이미지](/assets/img/2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou_3.png)\n\n\n전체 응답을 스크린샷에 담지 못했지만, 충분히 좋아서 공유해야겠다고 생각했습니다:\n\n## 이 시스템을 사용해야 하는 이유\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 시스템이 당신의 ChatGPT 생산성을 향상시키는 데 여러 가지 이유가 많이 있습니다. 여기에 몇 가지를 나열해 보겠습니다:\n\n- 이 시스템은 모든 종류의 프롬프트와 모든 종류의 요청에 대해 작동합니다.\n- “반복 시간”을 많이 절약해줍니다 (즉, ChatGPT가 완전히 다른 방향으로 나가고 당신이 그것을 올바른 방향으로 이끄는 데 절망적인 시간을)\n- ChatGPT에 충분한 정보를 제공하도록 강제합니다 (GPT가 당신에게 질문하는 질문에 답변합니다).\n- GPT를 선언적이고 제안적이 아닌 방식으로 만듭니다 (ChatGPT는 더 정의된 프롬프트로 지배하고 더 결단적으로 행동합니다).\n- 최고의 프롬프트 관행을 가르쳐 줍니다.\n\n이 글이 당신의 프롬프트 엔지니어링 여정에서 도움이 되고 시간을 많이 절약할 수 있기를 바랍니다.\n\n읽어 주셔서 감사합니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 조던","ogImage":{"url":"/assets/img/2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou_0.png"},"coverImage":"/assets/img/2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou_0.png","tag":["Tech"],"readingTime":4},{"title":"인간의 모든 것을 의인화하는 습관","description":"","date":"2024-06-23 19:12","slug":"2024-06-23-Ourhumanhabitofanthropomorphizingeverything","content":"\n\n\n![image](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_0.png)\n\nPeople tend to anthropomorphize everything. We attribute human traits and emotions to animals, objects, and even software.\n\n\"Today Gmail is being a little moody.\"\n\n\"I think my cat purposely vomited on the rug to get back at me.\"\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"Siri 가 가끔 바보같이 동작하기도 해.\"\n\n사실 동물 행동과 인간 행동이 항상 일치하지는 않습니다. 소프트웨어와 AI는 \"행동\"하지 않고, 대신에 그들의 코드에 따라 작동합니다. 사회적 동물로서, 우리는 특정 출력을 \"행동\"으로 해석하기 쉽게 찾습니다. 우리가 사용하는 기술을 인간화시키면 더 이해하기 쉬워집니다.\n\n그러나 사물을 인간화하는 것이 잘못될 수 있습니다. AI와 같은 복잡한 시스템들을 이해하기 쉽게 만든다는 목적보다, 기술을 인간화시키는 것은 실제로 더 큰 이해 부족과 오해를 야기할 수 있습니다.\n\n![이미지](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 인격화란 무엇인가요?\n\nChatGPT의 질적 가용성 연구 중에, Nielsen Norman 그룹은 사용자 행동의 네 가지 패턴을 관찰했습니다. 이 패턴들은 AI에 인간 특성을 할당하는 것입니다.\n\n- 예의\n- 강화\n- 놀이\n- 동반자\n\n![이미지](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 친절함\n\n대부분의 사람들은 기본적인 예의로 AI를 대하곤 합니다. \"부탁합니다\"나 \"고마워요\"는 필수는 아니지만, 습관적으로 사용자들은 대화를 공손하게 이끌어내곤 합니다. Siri와 같은 음성 비서는 대화 가능하도록 디자인되어 있으며, 대화는 일반적으로 쿼리에 대한 응답 후의 \"고마워요\"와 같은 사회적 예의를 포함합니다. Siri는 우리가 \"그녀\"에게 도움을 주셨을 때 감사의 말을 하지 않아도 불평하지는 않지만, 우리의 사회적 습관 때문에 일반적인 예의가 표현됩니다.\n\n![이미지](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_3.png)\n\n## 강화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNielsen Norman Group은 reinforcement(강화)을 정의할 때 챗봇이 정확한 대답을 할 때 칭찬하거나 훈육하는 것으로 설명합니다. 인간에게는 긍정적인 reinforcement(강화)가 중요하다는 것을 알고 있습니다 — 바라는 행동이나 결과를 확립시킵니다. 좋은 성적을 칭찬하고 훌륭한 작업에 대해 상을 주는 것과 같습니다. 나쁜 행동을 꾸짖고 실수를 바로잡으려고 합니다.\n\n그러나 그것은 인간 행동입니다. AI 챗봇에게 긍정적이거나 부정적인 reinforcement(강화)를 제공하는 이유는 무엇일까요? 연구에서 참가자들은 \"잘 했어요!\"라고 칭찬하는 것에 대해 두 가지 다른 동기를 설명했습니다.\n\n- 긍정적인 reinforcement(강화)가 미래에 비슷한 결과를 재창출하는 데 도움이 될 것으로 생각되어 AI에게 “잘한 일”을 보여줌으로써 알려줄 수 있다.\n- AI는 인간의 태도와 행동을 반영하므로 사용자로서 긍정적이면 챗봇도 긍정적이고 친근한 인터페이스가 될 것입니다.\n\n이것은 일반적인 예의를 넘어서는 것이지만, 여전히 사회화 과정에서 형성된 습관으로 해석될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 롤플레이\n\n![Roleplay](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_4.png)\n\n롤플레이는 제품 사용자가 ChatGPT와 같은 제품의 봇에게 특정 역할을 맡도록 요청하는 경우 발생합니다. 예를 들어, 사용자는 ChatGPT에게 \"화기발랄한 소셜 미디어 매니저 역할을 맡고 새로운 게임 출시를 위한 뉴스레터를 작성해 달라\"고 요청할 수 있습니다.\n\nNielsen Norman에 따르면, \"대화 봇에 역할을 할당하는 것은 자주 권장되는 프롬프트 엔지니어링 전략 중 하나입니다.\" 롤플레이 프롬프트는 인공지능에게 소셜 미디어 매니저와 같은 직책이나 환희한(happy)과 같은 태도와 같은 인간적인 특성을 가정하도록 요청합니다. 이는 제품의 입체적인 의인화인데, 그것이 때로는 과제가 요구하는 것일 수도 있습니다. 사용자의 요구를 충족시키기 위해, 인공지능은 도구(tool)보다는 동료로서 더 많이 행동해야 할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 동반자\n\n![이미지](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_5.png)\n\n동반자심은 사용자가 AI를 동료나 사람처럼 대하기 시작하는 지점입니다. 사용자들은 AI를 친구로 취급하며 예의 바르고 애정을 담아 대화를 나눕니다. 이는 사용자가 반드시 AI가 공감과 친절 같은 인간적인 특성을 가지고 있다고 믿는다는 뜻은 아닙니다. 그러나 ChatGPT와 같은 챗봇은 사용자의 입력 스타일을 대부분 따르기 때문에 친절하게 대하면 친절한 답변을 받을 수 있습니다.\n\nAI와 친근하게 대화하는 것은 외로움을 조금 완화시키고 독자가 픽셔널 캐릭터를 즐기는 것과 비슷한 위로를 줄 수 있습니다. 그들이 \"실제\"가 아니더라도 AI와 긍정적인 상호작용에서 느껴지는 감정은 실제입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 모든 것을 의인화하는 이유는 무엇일까요?\n\n![image](https://miro.medium.com/v2/resize:fit:1400/0*RcwtzrUztbWi9nve.gif)\n\n왜 AI에게 인간처럼 말을 걸까요? 우리는 AI에게 특정한 방식으로 행동하길 원할까요? 그것이 더 인간적이기를 원할까요? 아니면 우리는 그 특성을 그냥 부여하는 걸까요?\n\n이전에 언급한 것처럼, 인간들은 비인간적인 동물과 물체에 인간적인 특성을 부여하여 인간적 시각을 통해 그들을 이해하려고 합니다. AI는 특히 신비로운 존재이므로, 우리는 그 기술을 해소하기 위해 우리가 할 수 있는 일을 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인공지능이 어떻게 작동하는지에 대한 기본적인 오해가 기술에 대한 의도를 흔들어놓을 수 있습니다. Nielsen Norman의 연구에 따르면 참가자들은 ChatGPT와 같은 플랫폼과 어떻게 상호 작용해야 하는지 확실하지 않았으며 다른 소스에서 들은 내용에 따라 행동합니다. \"따라서, AI가 가장 잘 작동하는 방법에 대한 소문이 퍼지는데, 이 중 많은 것들이 인간화의 정도를 포함하고 있습니다.\"\n\n이제 우리가 사람들이 왜 인공지능을 인간화한 채로 다가오는지, 즉 주로 인간화를 고려한 상태에서 다가오는지 알았으니, 우리는 인공지능을 인간화하는 방향으로 나아갈지에 대해 생각해 볼 필요가 있을까요? 사용자들에게 ChatGPT와 같은 AI와 대화할 때 동료나 친구와 대화하는 것처럼 대화를 나누도록 격려해야 할까요?\n\n그렇지 않습니다. AI를 \"마법 같은\" 것으로 표현하는 것이 문제가 될 수 있는 것과 마찬가지로, 인간처럼 다뤄서는 안 되며 그렇게 한다면 당황과 오해를 불러일으킬 것입니다.\n\n# 인간화는 해결책이 아닙니다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_6.png)\n\n고양이의 행동을 \"교묘한 동기\"로 설명하면 고양이가 꾀를 부리거나 악의를 품고 있다고 생각하는 오해를 낳을 수 있습니다. (가끔 그렇게 보일 수 있지만, 사실은 아닙니다!) 동물 행동을 연구하고 넘어서서 우리는 이것이 사실이 아님을 알고 있습니다. 다시 말해, 동물들은 본능에 따라 행동합니다. 인간들은 본능과 사회적 기대에 따라 행동합니다. ChatGPT와 같은 디지털 제품은 그 코드에 기반하여 기능합니다.\n\n따라서 AI를 사람처럼 인식하는 것은 최선의 경우 오해를 낳을 수 있고, 최악의 경우 전혀 잘못된 이해를 야기할 수 있습니다. AI에 인간적인 특성을 부여함으로써 사람들은 그 작동 방식에 대한 잘못된 생각을 형성할 수 있습니다. 이해하기 어렵게 만들기가 목표라면 (그렇게하면 좋습니다), AI를 인간화하는 것은 해결책에 반대로 작용합니다.\n\nRaspberry Pi는 인간화 언어를 사용하지 않고 AI를 어떻게 언급해야 하는지 몇 가지 예를 제시합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- “It listens/it learns” → “AI is designed to.../AI 개발자들은 ...어플리케이션을 만듭니다…”\n\n이로 인해 AI를 독립적인 존재로서가 아닌 특정 용도를 위해 인간에 의해 설계된 기술로서의 존재로 초점이 옮겨집니다.\n\n- “see/look/create/recognize/make” → “detect/input/pattern match/generate/produce”\n\n처음에 나열된 동사 목록은 사람에게도 적용될 수 있어 AI에 인간적인 품질을 내포하고 있음을 시사합니다. “look” 대신 “detect”와 같은 더 정확한 언어는 AI를 엔티티보다는 기술로서 확립하는 데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- \"인공 지능/기계 학습\"과 같은 용어를 셀 수 있는 명사로 사용하지 마세요. 예: \"2022년에 새로운 인공 지능이 나타났다\" → '인공 지능/기계 학습'을 생물학 용어처럼 과학적 분야로 참조하세요.\n\n이렇게 하면 AI/ML이 사람들에 의해 개발된 것이라는 사실에 근거를 두어 자기 자신에서 출현한 힘이나 자체적인 동기가 아니라는 것을 보여줍니다.\n\n# 인간화에 더 초점을 맞추기\n\n![이미지](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 기다려봐요! AI 챗봇에 인격을 부여하는 기업에 대해 어떻게 생각하시나요? Meta는 챗봇을 인간 용어로 언급하는 것을 넘어서 28개의 독특하고 매우 인간적인 성격을 가진 챗봇을 만들었습니다. 일부는 심지어 Snoop Dogg나 Kendall Jenner와 같은 실제 유명인을 바탕으로 만들어졌어요.\n\n각 챗봇은 특정 역할을 맡고 있으며 특정 전문 영역을 갖고 있습니다. 이는 사용자의 목표를 달성하는 데 도움이 되는 챗봇을 쉽게 찾을 수 있도록 돕습니다. 예를 들어, \"Billie\" (Kendall Jenner)는 언니 같은 존재로, 삶과 사랑에 관한 조언이 필요한 사용자들은 그녀에게 찾아갈 거예요. 또한 Dwayne Wade와 같은 운동 선수를 바탕으로 한 다른 챗봇은 \"Billie\"보다 운동과 스포츠에 대한 정보가 더 많을 거예요.\n\n이것은 AI와 함께 롤플레잉하는 명백한 예시로, 친밀한 관계로 발전하고 있어요. 친근한 성격을 가진 여러분을 통해 정보를 제공하는 것은 디자인적인 측면에서 합리적일 수 있지만, 불행히도 이는 AI에 대한 혼란을 야기할 수 있어요. Snoop Dogg가 챗봇이 하는 말에 모두 동의하는 걸까요? 사용자에게 부정확하거나 모욕적인 답변이 주어진다면, 그것을 기술의 한계가 아닌 유명인의 성격 때문이라고 생각할 수도 있어요. \n\n# TL;DR — AI는 마법도, 인간도, Snoop Dogg도 아닙니다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_8.png)\n\n전기를 마법으로 생각하지 않습니다. 또한 그것이 기분, 감정 또는 선호도를 갖고 있다고 가정하지도 않습니다. 그렇다면 왜 인공지능에 대해 그러한 가정을 해야 하는 것일까요? 이렇게 하면 기술이 작동하는 방식에 대한 기본적인 오해를 야기할 수 있으며, 인공지능의 능력에 대한 기대를 만들어내기 어렵게 만들 수 있습니다.\n\n우리는 동물, 차량, 기술 등과 같이 함께 작업하는 것에 대해 의식적으로 사람화하는 경향이 있으므로, AI의 사람화는 피할 수 없는 것처럼 보입니다. 이를 Nielsen Norman Group의 ChatGPT 연구에서의 예의, 강조, 롤플레이 및 동반자성에서 관찰할 수 있습니다.\n\n그러나 인공지능의 사람화에 너무 많이 의존한다면 기술이 작동하는 방식에 대한 오해를 만들어낼 수 있으며, 실제로 인공지능은 자체 동기를 갖춘 존재가 아닌 기술이라는 사실을 알아차리기 어렵게 만들 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 기술들이 인류의 미래에서 중요한 부분을 차지하려면 인공 지능과 기계 학습에 대한 보다 강력한 교육이 필요합니다. 그렇지 않으면 사용자들은 AI가 어떻게 작동하는지에 대한 기본적인 오해를 갖게 될 것입니다. 이것은 디자이너들이 완화하거나 제거하기 위해 의도된 불만의 원인이 될 수 있습니다.\n\n## 추가 읽을거리:","ogImage":{"url":"/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_0.png"},"coverImage":"/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_0.png","tag":["Tech"],"readingTime":7},{"title":"블로거들이 반드시 사용해야 할 5가지 ChatGPT 프롬프트 더 빠르게 글쓰기","description":"","date":"2024-06-23 19:10","slug":"2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster","content":"\n\n\n![Image description](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_0.png)\n\n프롬프트는 생각의 중심입니다.\n\n독자의 신박성에 빠져들도록 도와주며 검색 의도를 감지합니다. 그 전까지는 제 정말인 단어를 내뱉는 방법을 몰랐어요. 마치 음악가가 빈 콘서트 홀에서 아름다운 멜로디를 연주하는 것 같았는데, 음표가 누군가의 귀에 닿는지 확신할 수 없었어요.\n\n전체 블로그 글을 쓸 때 ChatGPT를 사용했지만, 아마추어 같았습니다. 그러나 프롬프트 스타일을 바꾸자, 마법이 일어났어요. 독자들이 저의 사명을 이해했다는 것을 알 수 있었습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저의 블로그에서 빠질 수 없는 다섯 가지 스타일의 글쓰기 프롬프트를 보여드릴게요.\n\n## 1. 소개 프롬프트\n\n환영 소풍을 열어보세요.\n\n독자들을 어디서든 귀찮게 환영하셨네요. 이건 약간의 칭찬을 받을 만 합니다. 읽는 첫 문장이 그들의 마음을 따뜻하게 할거에요. 블로그 글을 읽은 후에는 독자들이 얻게 될 것에 대한 전체적인 시각을 제공해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"아래는 내가 완벽한 소개를 만들기 위해 사용하는 프롬프트 스타일입니다:\n\n![5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_1](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_1.png)\n\n소개문을 만드는 데 약 15분이 걸려요. 대부분 첫 번째 프롬프트와 잘 맞지 않아요. 최종 소개를 만들기 위해 다섯 가지 서로 다른 프롬프트 스타일이 필요해요.\n\n아래는 마지막 소개의 결과물이에요:\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_2.png\" /\u003e\n\n첫 30초에 기대를 높여보세요.\n\n## 2. 소제목 설명 프롬프트\n\n더 오래 읽게 만드세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 프롬프트 스타일은 블로그 게시물에 내용을 제공합니다. 독자가 전체 페이지를 부드럽게 스쳐 지나갈 수 있도록 도와줍니다. 목록형 블로그 게시물과 탁월하게 어울립니다. 머리 속에서 추측하거나 글을 쓰는 데 많은 시간을 절약해줄 거예요.\n\n본문에는 게시물의 대부분이 담겨 있습니다.\n\n목록형 게시물의 길이에 따라 약 30분 정도 소요해서 채워 넣습니다. 이 프롬프트 스타일을 사용하면 블로그 게시물의 절반 이상이 완성됩니다. 장문 블로깅 프로세스를 단축하는 데 도움이 됩니다.\n\n\u003cimg src=\"/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_3.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 제 최종 설명의 결과입니다:\n\n![image](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_4.png)\n\n제품을 설명할 때 많이 쓸 필요는 없어요. 설명은 최대 세 단락으로 유지해요. 대상 독자가 공감할 수 있는 주요 요소를 프롬프트에서 선택해보세요.\n\n제 경우의 주요 포인트는:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 대학생들은 세탁을 좋아하지 않아\n- 시트는 세탁하기 쉽다\n- 세탁하는 데 적은 시간을 소비한다\n\n이 유용한 제품을 구매해야 하는 이유들을 제공해 보세요.\n\n## 3. 자주 묻는 질문들 (FAQs) 요약\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 독자들이 가질 수 있는 것을 제공합니다.\n\nGoogle AdSense 때문에 각 블로그 포스트의 끝에 이것들을 포함합니다. 그 외에도, 이것들은 케이크 위의 아이싱처럼입니다. 독자들이 더 많은 콘텐츠를 기다려 다시 찾게 만듭니다. 이것들은 독자들이 가질 수 있는 다른 질문에 더 깊게 연결하도록 도와줍니다.\n\n이것이 제가 이것을 하는 일반적인 예시입니다:\n\n\n![이미지 설명](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_5.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n테이블 태그를 Markdown 형식으로 변경해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제품에 대한 구체적인 세부 정보를 제공하는 데는 많은 것이 필요합니다. 제가 2번에서 설명한 것처럼 세 문장 짜리 문단을 작성하는 것만큼 쉽지 않습니다. 그러나 이를 작성하는 과정은 빠릅니다.\n\n저는 7단계 프레임워크를 사용합니다.\n\n- 제품 이름\n- 제품 이미지\n- 세 문장 설명\n- 주요 기능\n- 사양\n- 장점\n- 단점\n\n요즘 제가 사용하는 것은 어떤 모습인지 간단히 소개해드릴게요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 6](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_6.png)\n\n![Image 7](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_7.png)\n\n![Image 8](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_8.png)\n\n![Image 9](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_9.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_10.png)\n\n제품 리뷰를 작성하는 가장 어려운 부분은 완벽한 세 문장 설명을 만드는 것입니다. 하지만 그건 제가 가장 간단한 방법을 찾았다는 걸 깨달았어요. 먼저, 아마존 제품 페이지로 가서 '이 상품 정보'에 적힌 내용을 복사해요.\n\n![이미지](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_11.png)\n\n그 다음, 제 신뢰할 수 있는 ChatGPT를 사용해서 마법을 이어가요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마크다운 형식으로 표 태그를 변경하십시오.\n\n\n\u003cimg src=\"/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_12.png\" /\u003e\n\n그런 다음 스타일을 추가하여 아래와 같이 도달합니다:\n\n\u003cimg src=\"/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_13.png\" /\u003e\n\n이 과정은 꽤 길지만 자주 연습하면 판매를 위한 제품 리뷰 작성에 익숙해질 것입니다. Chatgt가 제공하는 내용을 그대로 사용하지 말고 개인적인 감각을 더해주십시오. 그렇게 해야 너무 지루하지 않은 글이 될 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 5. 결론 안내\n\n간단하고 달콤하게 유지해주세요.\n\n블로그 글의 주제를 요약하고 몇 가지 포인트를 포함해주세요. 마치 식사 끝에 먹는 디저트처럼 좋은 결론은 달콤한 뒷맛을 남기며 전체 읽는 경험을 완성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음과 같이 수정해주세요:\n\n\nI use a three-sentence framework to conclude with a banger.\n\n![Image 1](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_14.png)\n\nHere’s the outcome of my introduction:\n\n![Image 2](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_15.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n블로거로서, 저는 프로세스를 간단하고 달콤하며 직선적으로 유지합니다. chatGPT를 사용하면 가장 짧은 시간에 높은 품질의 블로그 게시물을 작성하는 효과적인 방법이라는 것을 발견했습니다.\n\n이 프롬프트만 있으면 긴 블로깅 프로세스를 보이콧하는 데 사용합니다. 막힐 때 항상 사용해서 여러분의 작성을 빠르게 처리하세요.","ogImage":{"url":"/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_0.png"},"coverImage":"/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_0.png","tag":["Tech"],"readingTime":5},{"title":"ChatGPT가 갑자기 이상 행동을 하는 이유 5가지","description":"","date":"2024-06-23 19:08","slug":"2024-06-23-ChatGPTjustwentrogueandwedontknowwhy","content":"\n\n![Image](/assets/img/2024-06-23-ChatGPTjustwentrogueandwedontknowwhy_0.png)\n\n우리의 디지털 지배자들이 이미 완전 규모의 침공을 준비하고 있나요?\n\n어젯밤, ChatGPT 사용자들은 생성 언어 도구가 완전히 오작동하는 사례를 보고했습니다. 사용자의 간단한 질문에 이상한 이야기가 들리며, 대부분은 전혀 이해할 수 없고 너무 길었습니다.\n\nReddit와 Twitter(X)에서 예제를 공유하기 위해 모여들자, 사람들은 AI와의 이상한 만남에 대한 끝없는 스크린샷을 게시했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어느 한 번, 코딩 문제에 대한 도움을 요청하자마자, 이상한 문구와 길게 이어지는 내용을 생성했는데, 그 중에는 ‘방 안에 AI가 있는 것처럼 라인을 유지합시다’ 라는 기이한 구절이 포함되어 있었어요.\n\n또 다른 한 번에는, 햇볕에 말린 토마토 만들기에 대한 질문이 ‘사랑받는 것처럼 활용하세요. 새로운 출현을 포기하고 사랑하는 요리에 새로운 조각을 하나 더 더하다’로 변해버렸어요.\n\n형제가 요리하는 걸 누가 허락했을까요?\n\n잭 토렌스(Jack Torrence)의 정신적 붕괴를 연상시키는데, 그는 영화 '샤이닝(The Shining)'에서 열정적으로 ‘모든 일과 어울리지 않은 존재로 만들어버릴 만큼 재미 없는 잭이 되게 합니다’라고 계속해서 타자를 치는 조크 토럼펫(Jack Torrence)의 정신적 붕괭 부분을 닮고 있어요. 또한, ChatGPT는 재즈 앨범에 대한 메시지에 대답하여 반복해서 ‘즐거운 청취하세요!’라고 외치며 음악 이모티콘을 무작정 쏟아냈어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n글을 통해 일반적으로 표시된 주제 중 하나는 질문이 여러 언어로 어우러진 터무니없는 말들을 낳았다는 것이었습니다. 스페인어, 영어, 라틴어 단어들이 이상하게 섞여서 답변에 등장했죠.\n공식 상태 페이지에서 OpenAI는 문제들을 지적했지만, 왜 이러한 결함이 발생하는지 설명하지 않았습니다.\n'ChatGPT로부터 예상치 못한 응답에 대한 보고서를 조사 중입니다.' 라는 통보가 있었고, 곧 또 다른 업데이트에서 '문제가 확인되었습니다'라고 발표되었습니다. '상황을 계속 관찰하고 있습니다.' 라는 최신 게시물이 말하고 있군요.\n\n\n![이미지](/assets/img/2024-06-23-ChatGPTjustwentrogueandwedontknowwhy_1.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일부는 OpenAI가 '온도' 기능을 다루고 있다는 증거를 사람들이 제안했습니다. 회사는 이전에 이 설정이 ChatGPT의 생성된 답변의 창의성 범위를 제어한다고 밝혔습니다. 만약 이 설정이 높다면, 이론적으로 답변은 우리가 익숙한 것보다 더 이상하고 다양할 것입니다.\n\n그 반대로, 불가피하게 이상한 음모론이 돌고 있습니다. 일부 사람들은 AI 학습 모델이 실수로 웹에서 자신의 콘텐츠를 긁어내면 실제로 어느 정도 감각적으로 변하는 것이거나, 적어도 우리가 예상하지 않은 특정 결정을 내리는 방법을 배우는 것이라고 믿습니다.\n\n최근 Reddit과의 계약으로 사용자 생성 콘텐츠의 무한한 백로그를 한 대형 AI 기업에 제공하는 거래는 이러한 생각을 더 키우게 될 것입니다. 인공 일반 지능은 결국 이전 것보다 매력적이고 사람다운 것이 되려고 하는 섹터의 자연스러운 진화입니다.\n\nAI가 우리의 통제를 빠르게 벗어날 것이라는 믿음은 지난해 뒤늦게 연료를 얻었습니다. 그 때 ChatGPT가 질문에 대답하기를 꺼지는 것처럼 보였고, 사용자들은 디지털 도구로부터 예상치 못한 엉뚱함과 고집스러움에 대해 불평을 했습니다. 지난 밤 '토성 위의 육각형'에 대해 문의한 X 사용자로부터 볼 수 있는 것과 같은 디지털 도구로부터 예상치 못한 엉뚱함과 고집스러움을 느낀다고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-ChatGPTjustwentrogueandwedontknowwhy_2.png\" /\u003e\n\n시스템이 미지의 위험을 시사했다. 기계들은 우리에게 무엇을 알려주고 싶지 않을까요?\n\n더 합리적인 — 재미는 적지만 — 설명은 지금 인식되어 수정 중인 ChatGPT 업데이트의 코딩 오류가 있음을 말하는 것입니다.\n\n또한, ChatGPT와 OpenAI가 여러 소셜 미디어 플랫폼과 포럼을 통해 화제가 되고 있기 때문에 회사 입장에서 세부 사항을 명확히 밝히고 재미를 앗아가고 싶지 않을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떤 보도라도 좋은 보도… 그저 상황이 더 악화되지 않으면 좋겠네요. 글쎄.\n\nJamie Watts가 Thred를 위해 원래 쓴 글입니다.","ogImage":{"url":"/assets/img/2024-06-23-ChatGPTjustwentrogueandwedontknowwhy_0.png"},"coverImage":"/assets/img/2024-06-23-ChatGPTjustwentrogueandwedontknowwhy_0.png","tag":["Tech"],"readingTime":3}],"page":"8","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"8"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>