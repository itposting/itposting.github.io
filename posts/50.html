<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/50" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/50" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="유기농 아몬드버터 종교" href="/post/2024-06-20-OrganicAlmondButterReligion"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="유기농 아몬드버터 종교" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-OrganicAlmondButterReligion_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="유기농 아몬드버터 종교" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">유기농 아몬드버터 종교</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="하드웨어 패스키로 안드로이드 장치 잠금 해제하기 Digispark Attiny85 사용 가이드" href="/post/2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="하드웨어 패스키로 안드로이드 장치 잠금 해제하기 Digispark Attiny85 사용 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="하드웨어 패스키로 안드로이드 장치 잠금 해제하기 Digispark Attiny85 사용 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">하드웨어 패스키로 안드로이드 장치 잠금 해제하기 Digispark Attiny85 사용 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Niels, 러스트를 배우다 4 - VM을 모듈화하는 세 가지 다른 방법" href="/post/2024-06-20-NielslearnsRust4ThreedifferentwaysmaketheVMmodular"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Niels, 러스트를 배우다 4 - VM을 모듈화하는 세 가지 다른 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-NielslearnsRust4ThreedifferentwaysmaketheVMmodular_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Niels, 러스트를 배우다 4 - VM을 모듈화하는 세 가지 다른 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">Niels, 러스트를 배우다 4 - VM을 모듈화하는 세 가지 다른 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="IoT ESP32가 MQTT Broker에 연결하기 위해 SSL TLS 인증서를 사용하는 방법 파트 2" href="/post/2024-06-20-IoTESP32usingssltlscertificatetoconnectwidthMQTTBrokerPart2"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="IoT ESP32가 MQTT Broker에 연결하기 위해 SSL TLS 인증서를 사용하는 방법 파트 2" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-IoTESP32usingssltlscertificatetoconnectwidthMQTTBrokerPart2_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="IoT ESP32가 MQTT Broker에 연결하기 위해 SSL TLS 인증서를 사용하는 방법 파트 2" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">IoT ESP32가 MQTT Broker에 연결하기 위해 SSL TLS 인증서를 사용하는 방법 파트 2</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="짝커 - 아두이노로 박수 감지 스위치를 만드는 방법" href="/post/2024-06-20-TheClapperHowtomakeaclapsensitiveswitchwithArduino"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="짝커 - 아두이노로 박수 감지 스위치를 만드는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-TheClapperHowtomakeaclapsensitiveswitchwithArduino_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="짝커 - 아두이노로 박수 감지 스위치를 만드는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">짝커 - 아두이노로 박수 감지 스위치를 만드는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="소형 머신 러닝 - 주성분 분석" href="/post/2024-06-20-TinyMLPrincipalComponentAnalysis"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="소형 머신 러닝 - 주성분 분석" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="소형 머신 러닝 - 주성분 분석" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">소형 머신 러닝 - 주성분 분석</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">17<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="마이크로 머신러닝  합성곱 신경망 CNN" href="/post/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="마이크로 머신러닝  합성곱 신경망 CNN" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="마이크로 머신러닝  합성곱 신경망 CNN" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">마이크로 머신러닝  합성곱 신경망 CNN</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">23<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="시리얼 통신 마스터하기 아두이노 메가 2560 마스터와 우노 슬레이브 UART 튜토리얼" href="/post/2024-06-20-MasteringSerialCommunicationArduinoMega2560MasterandUnoSlaveUARTTutorial"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="시리얼 통신 마스터하기 아두이노 메가 2560 마스터와 우노 슬레이브 UART 튜토리얼" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-MasteringSerialCommunicationArduinoMega2560MasterandUnoSlaveUARTTutorial_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="시리얼 통신 마스터하기 아두이노 메가 2560 마스터와 우노 슬레이브 UART 튜토리얼" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">시리얼 통신 마스터하기 아두이노 메가 2560 마스터와 우노 슬레이브 UART 튜토리얼</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="컴퓨터에서 RC 모델을 프로그래밍하여 제어하기" href="/post/2024-06-20-ControlanyRCmodelprogrammaticallyfromcomputer"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="컴퓨터에서 RC 모델을 프로그래밍하여 제어하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-ControlanyRCmodelprogrammaticallyfromcomputer_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="컴퓨터에서 RC 모델을 프로그래밍하여 제어하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">컴퓨터에서 RC 모델을 프로그래밍하여 제어하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="아두이노 프로그래밍에서 iOS 앱 개발로" href="/post/2024-06-20-FromArduinoprogrammingtoiOSAppdevelopment"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="아두이노 프로그래밍에서 iOS 앱 개발로" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-20-FromArduinoprogrammingtoiOSAppdevelopment_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="아두이노 프로그래밍에서 iOS 앱 개발로" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">아두이노 프로그래밍에서 iOS 앱 개발로</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 20, 2024</span><span class="PostList_reading_time__6CBMQ">18<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/41">41</a><a class="link" href="/posts/42">42</a><a class="link" href="/posts/43">43</a><a class="link" href="/posts/44">44</a><a class="link" href="/posts/45">45</a><a class="link" href="/posts/46">46</a><a class="link" href="/posts/47">47</a><a class="link" href="/posts/48">48</a><a class="link" href="/posts/49">49</a><a class="link posts_-active__YVJEi" href="/posts/50">50</a><a class="link" href="/posts/51">51</a><a class="link" href="/posts/52">52</a><a class="link" href="/posts/53">53</a><a class="link" href="/posts/54">54</a><a class="link" href="/posts/55">55</a><a class="link" href="/posts/56">56</a><a class="link" href="/posts/57">57</a><a class="link" href="/posts/58">58</a><a class="link" href="/posts/59">59</a><a class="link" href="/posts/60">60</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"유기농 아몬드버터 종교","description":"","date":"2024-06-20 17:11","slug":"2024-06-20-OrganicAlmondButterReligion","content":"\n\n\n![Organic Almond Butter](/assets/img/2024-06-20-OrganicAlmondButterReligion_0.png)\n\n우리는 아몬드를 믿고, 아몬드 버터를 땅콩 버터보다 더 좋아해요. 아몬드 버터는 유기농이 아니어도 꽤 비싸요. 유기농 라벨에 대한 프리미엄이 더 높아져요.\n\n## 직접 만들어요\n\n우리는 유기농 아몬드로부터 직접 우리만의 유기농 아몬드 버터를 만들어요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아몬드는 아몬드 버터보다 파운드당 훨씬 싸요. 대량 구매로 가격을 매우 합리적으로 만들 수 있어요!\n\n직접 아몬드 버터를 만들면 같은 유리 캐닝 병을 계속해서 재사용할 수 있어요. 모든 면에서 폐기물이 줄어들어요.\n\n아몬드 + 전기 + 시간 = 아몬드 버터\n\n우리는 유기농 아몬드 한 쿼트로 시작해요. 350도 F의 오븐에서 쿠키 시트에 로스팅해서 10~15분간 굽고 식힌 후, 푹 갈라 놓기 위해 음식 가공기에 넣어서 칼날로 약 20분간 갈아요. 원하는 조직감에 도달할 때까지요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그들이 갈고 있는 동안, 우리는 맛을 내기 위해 소금을 1/4 티스푼 넣습니다. 아몬드 1쿼트는 약 절반의 양으로 아몬드 버터를 만들어 냅니다.\n\n## 배운 교훈\n\n우리는 머신이 매우 뜨거워지고 아몬드 버터를 만드는 데 최대 한 시간이 걸렸던 생 아몬드로 시작했습니다. 아몬드를 볶음으로 만들 때, 가공 속도가 빨라지며 기계에 더 적은 감손을 줍니다. 더 짧은 가동 주기로 전기를 절약하고, 귀에 덜 무리가 갑니다.\n\n푹푹 끓일 때 소금을 넣으세요. 이렇게 하면 푸레터에 소금 뭉치가 생기는 것을 방지할 수 있습니다. 어떤 사람들은 이것을 좋아할 수도 있지만, 우리는 싫습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-20-OrganicAlmondButterReligion_1.png)\n\nStory enjoyed? Join Sweet Publications!\n\nSweet.pub is a family of three — 💚 Short, 💙 Long, and 🧡 Deep.\nExplore stories that will make your 🤍 beat!\n\nThis article was published on June 19th, 2024 in the Short. Sweet. Valuable. publication.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Organic Almond Butter Religion](/assets/img/2024-06-20-OrganicAlmondButterReligion_2.png)\n","ogImage":{"url":"/assets/img/2024-06-20-OrganicAlmondButterReligion_0.png"},"coverImage":"/assets/img/2024-06-20-OrganicAlmondButterReligion_0.png","tag":["Tech"],"readingTime":2},{"title":"하드웨어 패스키로 안드로이드 장치 잠금 해제하기 Digispark Attiny85 사용 가이드","description":"","date":"2024-06-20 17:09","slug":"2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85","content":"\n\n우리 모두 한 번쯤은 그 순간을 겪어봤죠—핸드폰 잠금 상태에서 핀 코드를 기억하지 못하는 그 순간. 무수히 입력했지만 이제 손가락이 주저하는 그 순간. 근육 기억에 의지하지만 그것조차 실망스러운 것 같아요. 이런 순간들이 우리에게 이 중요한 보안 코드를 잊어버리기 쉬운지 상기시켜 줍니다.\n\n다행히도 이 문제에 대한 혁신적인 해결책이 즉시 사용 가능한 것 같아요. 이 해결책은 보통의 비밀번호 알림과는 다릅니다. 하드웨어 패스키입니다—당신의 핸드폰이나 컴퓨터에 연결되면 핀 코드를 자동으로 입력해주는 소형 회로입니다. 이 기술적인 놀라움은 주로 10달러 미만으로 구할 수 있는 저렴한 Digispark Attiny85 보드로 구현할 수 있어요.\n\n본 기사에서는 직접 하드웨어 패스키를 만드는 단계별 가이드를 안내해 드릴게요.\n\n![이미지](/assets/img/2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 요구 사항:\n\n- Windows 컴퓨터\n- Digispark Attiny85 보드\n- 안드로이드 폰\n- 아두이노 소프트웨어\n\n# 아두이노 설정\n\n- 공식 웹사이트에서 아두이노 다운로드\n- 파일 -` 기본 설정 -` 설정 -` 추가 보드 관리자 URL로 이동\n- 다음 URL을 목록에 추가하고 저장: https://raw.githubusercontent.com/digistump/arduino-boards-index/master/package_digistump_index.json\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Step 1](/assets/img/2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85_1.png)\n\n- Go to Tools -\u003e Board -\u003e Boards Manager..., search for “digistump”, and install “Digistump’s AVR Boards”.\n\n![Step 2](/assets/img/2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85_2.png)\n\n![Step 3](/assets/img/2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85_3.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Windows 10 사용자들은 usbser 드라이버 설치 중 실패할 수 있습니다. 그런 경우에는 여기서 직접 다운로드할 수 있습니다: [https://github.com/digistump/DigistumpArduino/releases/download/1.6.7/Digistump.Drivers.zip](https://github.com/digistump/DigistumpArduino/releases/download/1.6.7/Digistump.Drivers.zip) 다운로드한 파일을 압축 해제하고 DPinst64.exe를 실행하여 드라이버를 설치하세요.\n- 아두이노 인터페이스에서 Tools -` Board -` Digistump AVR Boards -` Digispark (Default 16.5 MHz)를 선택하세요.\n\n![이미지](/assets/img/2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85_4.png)\n\n설정이 완료되었습니다! 이제 스케치 코드를 작성할 준비가 되었습니다!!\n\n# 페이로드 코딩\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희 스케치에는 두 가지 미리 정의된 함수가 있습니다: setup()과 loop(). 이 튜토리얼에서는 PIN 코드가 '1234'라고 가정해봅시다. 우리의 코드는 다음과 같이 보일 것입니다:\n\n```js\n#include \"DigiKeyboard.h\"\n\nvoid setup() {\n  // setup에서 할 일이 없습니다.\n}\n\nvoid loop() {\n  // 이 지연은 컴퓨터가 DigiSpark를 인식하는 데 시간을 주기 위한 것입니다.\n  // 연결 후 2000밀리초는 2초입니다.\n  DigiKeyboard.delay(2000);\n\n  // 이제 PIN에 대한 키 입력을 보낼 것입니다.\n  DigiKeyboard.println(\"1234\");\n\n  // PIN을 입력한 후, 다시 입력하기 전에 오랜 지연을 할 것입니다.\n  // 이는 DigiSpark가 플러그를 꽂은 채로 있는 경우 계속해서 PIN을 입력하지 않도록 방지하기 위한 것입니다.\n  DigiKeyboard.delay(60000);\n}\n```\n\n이 코드를 입력한 후, \"Verify\" 버튼을 클릭하여 컴파일 및 오류 확인을 실행해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이미지 태그를 Markdown 형식으로 변경하세요.\n\n\n![Unlocking Your Android Device with a Hardware Passkey - A Guide to Using Digispark Attiny85](/assets/img/2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85_6.png)\n\n![Unlocking Your Android Device with a Hardware Passkey - A Guide to Using Digispark Attiny85](/assets/img/2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85_7.png)\n\n# 페이로드 업로드\n\nAttiny85 보드에 코드를 업로드하는 중:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 아두이노 인터페이스에서 업로드 버튼을 클릭하세요.\n\n![이미지](/assets/img/2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85_8.png)\n\n- \"지금 장치를 연결하세요... (60초 후 타임아웃)\"라는 메시지가 나오면 Attiny85 보드를 컴퓨터의 USB 포트에 연결하세요.\n\n![이미지](/assets/img/2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85_9.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- \"Micronucleus done. Thank you!\" 메시지가 표시될 때까지 기다렸다가 USB 포트에서 보드를 제거해주세요.\n\n![이미지](/assets/img/2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85_10.png)\n\n축하합니다! 이제 하드웨어 패스키를 사용할 준비가 되었습니다.\n\n# 패스키 테스트:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nWindows에서는 메모장을 열고 패스키를 연결하세요. 자동으로 PIN 코드를 입력해줄 것입니다.\n\n## 온라인 PIN 저장소 또는 비밀번호 관리자보다 하드웨어 패스키의 장점:\n\n- 오프라인 저장: 하드웨어 패스키는 오프라인이므로 온라인 해킹에 취약성이 낮습니다. 당신의 PIN은 인터넷 서버에 저장되지 않아 보안이 향상됩니다.\n- 타사 신뢰 불필요: 비밀번호 관리 애플리케이션과는 달리, 하드웨어 패스키는 데이터를 보호하기 위해 제3자에게 신뢰를 두지 않아도 됩니다.\n- 마스터 비밀번호 필요 없음: 비밀번호 관리자는 마스터 비밀번호가 필요한데, 이를 잊어버리면 잠금 상태가 될 수 있습니다. 하드웨어 패스키는 PIN을 자동 입력하여 이 문제를 피할 수 있습니다.\n\n기억하세요, 완벽한 해결책은 없습니다. 하드웨어 패스키에는 잠재적인 보안 문제도 있으니 아래 섹션에서 자세히 설명하겠습니다. 당신의 디지털 보안 요구 및 편안함에 맞는 최상의 솔루션을 선택하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 하드웨어 패스키에 대한 잠재적 보안 위험:\n\n하드웨어 패스키는 기기 접근을 간단하게 만들지만, 관련된 보안 문제를 고려하는 것이 중요합니다:\n\n- 물리적 보안: 분실되거나 도난당하면 패스키를 사용하여 당신의 기기에 무단 접근할 수 있습니다. 안전하게 보관하세요.\n- 악성 소프트웨어: 프로그래밍 기기가 감염되면 악성 소프트웨어가 패스키에 감염될 수 있습니다. 최신 백신 소프트웨어로 기기를 보호하세요.\n- 하드코딩된 PIN: 숙련된 개인이 패스키에서 하드코딩된 PIN을 추출할 수 있습니다. 안전하게 다루세요.\n\n편리하더라도, 이러한 잠재적 보안 위험을 고려하여 하드웨어 패스키를 책임있게 사용하는 것이 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 마무리 글\n\n저렴한 Digispark Attiny85 보드로 구동되는 하드웨어 패스키는 잊혀진 PIN에 대한 일반적인 문제에 효과적이고 오프라인 솔루션을 제공합니다. 온라인 저장소나 비밀번호 관리 앱 대안으로 사용자 친화적이며 편리함과 통제를 동시에 제공합니다.\n\n그러나 이 도구를 사용하는 동안 물리적 보안 및 악성 소프트웨어 위협과 같은 잠재적인 위험을 인식하는 것이 중요합니다. 안드로이드용 하드웨어 패스키를 만드는 가이드이지만 기술 책임성의 보다 넓은 맥락을 무시할 수는 없습니다.\n\n본질적으로, 하드웨어 패스키는 기술이 우리 삶을 간편하게 만들 수 있는 잠재력을 상징합니다. 이러한 솔루션을 계속 탐구할 때 우리는 편의와 안전한 사용을 균형있게 고려해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 Medium 계정을 팔로우하여 제 기술 여정을 계속 지켜보세요. 그리고 만약 함께 힘을 합쳐 기술 세계를 정복하고 싶다면, LinkedIn에서 연결해요!","ogImage":{"url":"/assets/img/2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85_0.png"},"coverImage":"/assets/img/2024-06-20-UnlockingYourAndroidDevicewithaHardwarePasskeyAGuidetoUsingDigisparkAttiny85_0.png","tag":["Tech"],"readingTime":6},{"title":"Niels, 러스트를 배우다 4 - VM을 모듈화하는 세 가지 다른 방법","description":"","date":"2024-06-20 17:07","slug":"2024-06-20-NielslearnsRust4ThreedifferentwaysmaketheVMmodular","content":"\n\n이 문서는 '루스트(Rust)'를 배우면서 내장형 자바 가상 머신을 이식하는 과정의 네 번째 단골입니다. 전체 시리즈를 보려면 여기를 클릭하세요.\n\n이전 VM 버전은 Darjeeling 가상 머신의 변경 버전이었습니다. 모듈식으로 설정되어 다양한 구성 요소 세트로 쉽게 컴파일할 수 있었습니다.\n\n이러한 구성 요소는 무선 프로그래밍, 장치 UART 액세스, Java 기본 라이브러리 등과 같은 다양한 기능을 제공했습니다. 이를 사용한 연구 프로젝트는 이종적인 사물 인터넷(IoT) 장치 네트워크를 조정하는 데 사용되었습니다. 이 프로젝트에서는 심지어 자바 가상 머신 자체를 선택적 구성 요소로 만들어 가장 작은 장치용 버전을 작성할 수 있었으므로 자바 코드를 실행할 수 없어도 네트워크에 참여할 수 있었습니다.\n\n# 프로젝트 레이아웃\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nVM을 빌드하기 전에 프로젝트 레이아웃을 먼저 설정해 봅시다. 목표는 VM을 구축할 때 설정 옵션에 따라 선택적으로 포함될 수 있는 구성 요소를 갖는 것입니다. 각 구성 요소는 코어 VM에서 노출된 여러 훅에 대한 콜백을 등록할 수 있어야 합니다. 이를 통해 구성 요소를 초기화하고 가비지 컬렉터와 상호 작용하게 하며 네트워크 메시지를 수신할 수 있어야 합니다. 이 단계에서 우리는 init()라는 훅을 정의할 것입니다. 이 훅은 시작할 때 호출됩니다.\n\n다양한 옵션을 설명하기 위해 두 개의 구성 요소로 시작해 봅시다.\n\n- jvm : 실제 Java 가상 머신을 포함할 것입니다.\n- uart : CPU의 UART(유니버설 비동기 수신기/발신기)를 제어하는 코드를 포함할 것입니다.\n\n프로젝트 레이아웃은 이렇게 보일 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```rust\n.\n├── avrora.rs\n├── main.rs\n└── components\n    ├── mod.rs\n    ├── jvm\n    │   └── mod.rs\n    └── uart\n        └── mod.rs\n\n4 directories, 5 files\n```\n\n러스트는 세 가지 방법으로 모듈을 정의하여 계층적으로 범위를 지정합니다:\n\n- mod modname ' ... ' 블록,\n- modname.rs 파일로, 이 프로젝트에는 avrora라는 모듈이 있습니다,\n- 그리고 mod.rs가 포함된 하위 디렉토리로, 해당 하위 디렉토리의 이름을 따르는 모듈이 정의됩니다. 여기에서는 components, components::jvm, 그리고 components::uart 모듈이 있습니다.\n\n컴파일은 main.rs 또는 라이브러리의 경우 lib.rs에서 시작됩니다. 다른 파일이나 디렉토리의 모듈은 명시적으로 프로젝트에 포함되어야 합니다. 그렇지 않을 경우 무시됩니다. 이 경우 mod avrora;와 mod components; 라인은 avrora 및 components 모듈을 가져와서 전체 크레이트에서 사용할 수 있도록 만듭니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n목표는 구성 설정에 따라 components::uart 및 components::vm을 선택적으로 포함하고, 활성화되어 있는 경우 init() 함수를 호출하는 것입니다.\n\n이를 수행하는 세 가지 방법을 고려해 보았는데, 각각이 저에게 몇 가지 흥미로운 Rust 기능을 가르쳐 주었습니다. 따라서 모두 살펴보도록 하겠습니다:\n\n# 옵션 A: build.rs\n\nCargo에는 코드를 빌드하기 전에 Rust 스크립트를 실행할 수 있는 옵션이 있습니다. 스크립트는 build.rs로 불리며 프로젝트의 루트 디렉토리에 배치되어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n선택한 구성 요소를 가져오는 데 필요한 코드를 생성하는 데 사용할 수 있습니다. 구성 요소가 활성화되는 설정을 따로 저장할 것이며, 이는 프로젝트 루트에 있는 vm-config.toml 파일에 저장됩니다:\n\n```js\n[capevm]\ncomponents = [\"jvm\"]\n```\n\n이 예에서는 jvm이 활성화되었지만 uart 구성 요소는 비활성화되었습니다. 그럼 우리의 빌드 스크립트는 다음 코드를 포함한 파일을 생성해야 합니다:\n\n```js\n#[path = \"/home/niels/git/capevm-rust/capevm/src/components/jvm/mod.rs\"]\nmod jvm;\n\npub fn init() {\n    jvm::init();\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n러스트는 프로젝트에 명시적으로 임포트되지 않는 코드를 무시하므로 최종 빌드에는 jvm 모듈만 포함되고 uart 모듈은 건너뛰게 됩니다.\n\n생성된 코드는 일시적인 파일이며 소스 제어에 포함되지 않아야 합니다. 따라서 Cargo의 출력 디렉토리에 생성하고 components/mod.rs에서 포함시키겠습니다. 이 파일은 이제 다음과 같이 한 줄만 포함하고 있습니다:\n\n```js\ninclude!(concat!(env!(\"OUT_DIR\"), \"/enabled_components.rs\"));\n```\n\n생성된 코드의 #[path] 속성이 필요한 이유는 모듈 임포트가 가져오는 파일의 위치를 기준으로 상대적이기 때문입니다. 그러나 Rust의 include! 매크로는 C의 #include 전처리기 지시문과 다르게 작동합니다. 포함된 코드는 단순히 파일에 붙여넣는 것이 아니라 해당 소스 위치에 따라 구문 분석되는데, 이 경우에는 Cargo 출력 디렉토리입니다. 만약 #[path] 속성을 사용하지 않고 Rust에게 해당 컴포넌트가 다른 위치에 있다고 알리지 않으면 아래와 같은 오류가 발생할 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-NielslearnsRust4ThreedifferentwaysmaketheVMmodular_0.png\" /\u003e\n\n다음은 완전한 빌드 스크립트입니다:\n\n```js\nextern crate toml;\n\nuse std::fs;\nuse std::path::Path;\nuse toml::Value;\n\nfn main() {\n    println!(\"cargo:rerun-if-changed=build.rs\");\n    println!(\"cargo:rerun-if-changed=vm-config.toml\");\n\n    let manifest_dir = std::env::var(\"CARGO_MANIFEST_DIR\").unwrap();\n    let out_dir = std::env::var(\"OUT_DIR\").unwrap();\n    let dest_path = Path::new(\u0026out_dir).join(\"enabled_components.rs\");\n\n    let contents: String = fs::read_to_string(\"vm-config.toml\").unwrap();\n    let cargo_toml = contents.parse::\u003cValue\u003e().unwrap();\n\n    let vm_components =\n        if let Some(capevm_components) = cargo_toml.get(\"capevm\")\n                                        .and_then(Value::as_table)\n                                        .and_then(|table| table.get(\"components\"))\n                                        .and_then(Value::as_array) {\n            capevm_components.iter().filter_map(|v| v.as_str()).collect::\u003cVec\u003c\u0026str\u003e\u003e()\n        } else {\n            Vec::\u003c\u0026str\u003e::default()\n        };\n\n    let mod_imports =\n        vm_components.iter()\n            .map(|name| format!(r#\"\n                #[path = \"{manifest_dir}/src/components/{name}/mod.rs\"]\n                mod {name};\"#, manifest_dir=manifest_dir, name=name))\n            .collect::\u003cVec\u003c_\u003e\u003e().join(\"\\n\");\n    let mod_inits =\n        vm_components.iter()\n            .map(|name| format!(\"\n                {}::init();\", name))\n            .collect::\u003cVec\u003c_\u003e\u003e().join(\"\\n\");\n\n    let generated_code =\n        format!(\"{}\n            \n            pub fn init() {\n                {}\n            }\", mod_imports, mod_inits);\n\n    fs::write(dest_path, generated_code.as_bytes()).unwrap();\n}\n```\n\n조금 주목해야 할 사항이 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- extern crate toml; 메인 애플리케이션과 마찬가지로 build.rs 스크립트는 외부 크레이트를 사용할 수 있습니다. 다른 크레이트와 마찬가지로 Cargo.toml에 선언해야 하지만, [dependencies] 대신 [build-dependencies] 섹션에 선언해야 합니다.\n- println!(\"cargo:rerun-if-changed=...\"); 표준 출력에 쓰는 것으로 Cargo가 빌드 스크립트를 실행하는 시점(그리고 다른 여러 가지 것들)을 제어할 수 있습니다. 여기서 두 줄은 Cargo에게 build.rs 또는 vm-config.toml이 변경되면 빌드 스크립트를 다시 실행하도록 지시합니다.\n- std::env::var(\"CARGO_MANIFEST_DIR\"): Cargo는 환경 변수를 통해 빌드 프로세스의 여러 매개변수를 스크립트에 노출시킵니다. 이 경우 OUT_DIR을 사용하여 생성된 파일이 위치해야 하는 곳을 결정하고, CARGO_MANIFEST_DIR을 사용하여 구성 요소의 위치를 파악합니다.\n- unwrap(): Rust의 주요 오류 처리 방식은 Result\u003cT, E\u003e를 반환하는 것입니다. 이는 T 값이나 E 오류 중 하나를 포함할 수 있습니다. 일반적으로 오류를 처리하거나 전달해야 하지만, 오류가 발생할 가능성이 없거나 코드가 패닉 상황이라고 생각하면 unwrap을 사용하여 Result에서 값을 가져올 수 있습니다.\n- .and_then(): toml 크레이트는 toml 파일의 내용을 나타내는 Value 객체를 제공합니다. 이는 이름으로 검색할 수 있습니다. 이는 Option\u003c\u0026Value\u003e를 반환하며, 이름을 찾지 못하면 None일 수 있습니다. .and_then() 호출을 사용하면 값을 포함하는 경우에는 Option에서 작업을 연결하고, 그렇지 않은 경우에는 None을 유지할 수 있습니다. 다른 언어에서는 이를 flatmap 또는 bind라고도 부르기도 합니다.\n\n# 옵션 B: Cargo 피쳐\n\n두 번째 옵션은 Rust 피쳐를 사용하는 것입니다. 이 방법은 훨씬 간단하지만 한계가 있습니다. 먼저 다음과 같이 Cargo.toml에 [features] 섹션을 선언합니다:\n\n```js\n[features]\ndefault = [\"jvm\"]\njvm = []\nuart = []\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 기능은 종속성 목록이 포함된 이름에 해당합니다. 기본 기능은 기본적으로 포함됩니다. 예를 들어 이 예시에서 기본 기능은 jvm에 의존하므로 해당 기능은 활성화되지만 uart는 활성화되지 않습니다.\n\n그런 다음 조건부 컴파일에 사용할 수 있는 이름이 지정된 기능이 있습니다. #[cfg(feature = \"...\")] 속성을 사용하여 구성요소/mod.rs를 다음과 같이 구현할 수 있습니다:\n\n```js\n#[cfg(feature = \"jvm\")]\nmod jvm;\n#[cfg(feature = \"uart\")]\nmod uart;\n\npub fn init() {\n    #[cfg(feature = \"jvm\")]\n    jvm::init();\n    #[cfg(feature = \"uart\")]\n    uart::init();\n}\n```\n\n이 접근 방식의 장점은 빌드 스크립트보다 훨씬 간단하다는 것입니다. 또한 명령줄에서 선택된 기능을 제어할 수 있습니다. 예를 들어, --features=\"uart\"은 uart 기능을 활성화하며, --no-default-features는 기본 기능을 무시합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단점 중 하나는 해당 기능이 활성화되어 있는 경우 그 모든 컴포넌트를 가져오기 위해 components/mod.rs에 수동으로 목록을 작성해야 한다는 것입니다. 게다가, 현재 init() 함수만 있어야 하는데 이 함수는 모든 활성화된 기능을 위해 호출되어야 합니다. 그러나 쓰레기 수집 및 네트워킹과 같은 항목을 추가할 때 가능한 후크 목록이 늘어나게 됩니다. 그래서 일부 컴포넌트는 수신 메시지를 듣거나 힙에 자체 객체를 등록하려는 것일 수 있습니다.\n\n이것은 꽤 강한 결합이며, 모듈 및/또는 후크의 수가 계속해서 늘어난다면 이 접근법은 유지하기 어려워질 수 있습니다.\n\n# 옵션 C: 기능 + 인벤토리 크레이트\n\n이제 옵션 C로 넘어가봅시다: 인벤토리 크레이트입니다. 이를 통해 컴포넌트와 코어 VM 간의 강한 결합을 줄일 수 있습니다. 안타깝게도 AVR에서는 작동하지 않지만, 학습할 가치가 있는 내용입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n크레이트를 사용하면 데이터 형식을 정의하고 해당 데이터 형식의 인스턴스를 코드의 한 부분에서 등록하고 다른 곳에서 수집할 수 있습니다. 저희 경우에는 데이터 형식이 init() 함수 포인터를 포함하는 구조체일 수 있습니다:\n\n```rust\npub struct Component {\n    init: fn()\n}\n```\n\n이제 컴포넌트의 구현은 아래와 같습니다:\n\n```rust\npub fn init() {\n    println!(\"jvm 초기화 중...\");\n}\n\ninventory::submit! {\n    crate::components::Component { init }\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n구성 요소/components/mod.rs의 구현부는 다음과 같습니다:\n\n```js\n#[cfg(feature = \"jvm\")]\nmod jvm;\n#[cfg(feature = \"uart\")]\nmod uart;\n\npub struct Component {\n    init: fn()\n}\n\ninventory::collect!(Component);\n\npub fn init() {\n    for component in inventory::iter::\u003cComponent\u003e {\n        (component.init)();\n    }\n}\n```\n\ncollect! 매크로는 submit!()를 통해 등록된 모든 Component 객체를 순회할 수 있는 반복자를 생성합니다. 이 반복자는 main() 함수에 진입하기 전에 초기화되며, 우리가 수동으로 초기화 코드를 실행할 필요가 없습니다.\n\n이 동작이 가능하게 하는 핵심은 submit! 매크로에 있습니다. 매크로가 어떻게 확장되는지 볼 수 있다는 점은 종종 유용하거나 그냥 재미있을 수 있습니다. cargo-expand라는 Cargo 확장 프로그램을 사용하여 확장된 소스를 표시할 수 있습니다. 설치한 후(cargo install cargo-expand), cargo expand components::jvm: 명령을 통해 확장된 소스를 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-20-NielslearnsRust4ThreedifferentwaysmaketheVMmodular_1.png)\n\n특정 링크된 섹션에 코드를 배치하여 마법이 일어납니다. 인벤토리 크레이트의 소스를 살펴보면(조밀하지만 500줄 미만이고 그 중 절반은 주석입니다), 생성될 링커 섹션이 운영 체제에 따라 다르다는 것을 알 수 있습니다. 리눅스의 경우 .init_array가 되고, 윈도우의 경우 .CRT$XCU가 되며 macOS의 경우 __DATA,__mod_init_func가 됩니다. 각각은 main() 함수에 진입하기 전에 실행될 코드를 포함하고 있습니다.\n\nAVR의 경우 이러한 종류의 코드가 .initN 섹션으로 들어가지만, 안타깝게도 인벤토리 크레이트는 AVR에서 작동하지 않습니다:\n\n![이미지](/assets/img/2024-06-20-NielslearnsRust4ThreedifferentwaysmaketheVMmodular_2.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n에러 메시지가 약간 암호화되어 있어서 이 문제는 조금 어렵게 느껴질 수도 있어요. 특히 `ptr` 피처 부분이 숨겨져 있는 것이요. 이 크레이트는 core::sync::atomic::AtomicPtr라는 유형을 사용하는데, 어떤 이유 때문에 사용할 수 없네요. 이 유형의 구현을 살펴보면 플랫폼이 원자 포인터 작업을 지원하는 경우에만 설정되는 #[cfg(target_has_atomic_load_store = \"ptr\")]라는 조건부 컴파일 속성이 있다는 것을 알게 됩니다.\n\nAVR은 그 작업을 지원하지 않아요. 이는 8비트 CPU이며 포인터는 16비트이기 때문에 포인터 조작은 항상 여러 번의 읽기 또는 쓰기가 필요해요.\n\n# 비교와 결정\n\n우리는 아마도 인벤토리 크레이트가 하는 것을 AtomicPtr이 없이도 일부 코드를 복사하고 수정하여 재생산할 수 있을 거예요. 그러나 여기서 최종적으로 가장 좋은 선택이 아닌 이유가 하나 더 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 방식은 iterator가 루프를 돌 수 있는 정적 inventory::Node 객체의 링크드 리스트를 만들어 작동합니다. 이것은 RAM을 사용하므로, 객체당 4바이트가 필요한 경우에도 RAM이 단 4KB만 있는 장치에서 정적 목록에 낭비하고 싶지 않습니다.\n\n그러므로 옵션 A와 B 중에서 선택해야 합니다. 옵션 A는 핵심 vm 코드가 구성 요소에 대해 알 필요가 없어 보이지만, 옵션 B는 각 구성 요소를 해당 기능 플래그와 등록해야 한다는 점이 필요합니다.\n\n옵션 A의 단점은 각 구성 요소가 초기화할 것이 없더라도 init()에 대한 구현을 정의해야 한다는 것입니다. 빌드 스크립트는 항상 호출을 생성하므로 말입니다. 러스트 컴파일러는 죽은 코드를 제거하는 데 매우 뛰어나기 때문에, 컴파일 시간에 이 코드들이 제거될 가능성이 높지만, 나중에 사용할 쓰레기 수집기를 위해 비슷한 후크를 추가할 때마다, 적어도 빈 구현을 제공해야 하는데 이는 추가 작업이 필요합니다.\n\n구성 요소와 후크의 수가 제한될 것이므로, 두 옵션 모두 잘 작동할 것으로 보입니다. 옵션 A는 더 많은 요소가 움직이며, 마법이 적으면 항상 좋은 것이므로, 일단 옵션 B를 선택하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n보통 이번 단계의 코드 상태는 Github에서 확인하실 수 있어요. 세 가지 옵션을 업로드했어요:\n\n- 옵션 A: build.rs\n- 옵션 B: features\n- 옵션 C: features + inventory (AVR에서 컴파일되지 않아요)\n- 데스크탑에서 작동하는 옵션 C의 최소 예제.\n\n2024년 5월 23일에 https://nielsreijers.com에서 최초로 게시되었어요.","ogImage":{"url":"/assets/img/2024-06-20-NielslearnsRust4ThreedifferentwaysmaketheVMmodular_0.png"},"coverImage":"/assets/img/2024-06-20-NielslearnsRust4ThreedifferentwaysmaketheVMmodular_0.png","tag":["Tech"],"readingTime":10},{"title":"IoT ESP32가 MQTT Broker에 연결하기 위해 SSL TLS 인증서를 사용하는 방법 파트 2","description":"","date":"2024-06-20 17:06","slug":"2024-06-20-IoTESP32usingssltlscertificatetoconnectwidthMQTTBrokerPart2","content":"\n\n시작부터 따라오고 싶다면 다음 링크를 확인하세요 (Part 1)\n\n[Part 1](/assets/img/2024-06-20-IoTESP32usingssltlscertificatetoconnectwidthMQTTBrokerPart2_0.png)\n\n## 개요\n\n이 기사에서는 ESP32를 사용하여 SSL/TLS 프로토콜에서 MQTT 통신을 하는 방법을 보여줍니다. 암호화된 메시지를 발행하고 주제를 구독하는 방법에 대해 설명합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 똑같은 Raspberry Pi 3에 설치된 Mosquitto 브로커를 사용할 것입니다. 이 브로커는 모든 메시지를 수신하고, 메시지를 필터링하며, 누가 관심이 있는지 결정하고 구독한 모든 클라이언트에게 메시지를 게시하는 역할을 합니다.\n\n# 준비 사항\n\n- Raspberry Pi에 익숙해야 합니다 — Getting Started with Raspberry Pi와 Arduino IDE를 읽어보세요.\n- Raspberry Pi에 Raspbian 운영 체제가 설치되어 있어야 합니다 — Raspbian Lite 설치와 SSH로 연결하는 방법에 대한 유튜브 비디오를 시청하세요.\n- esp32 컨트롤러가 필요합니다.\n- MQTT가 무엇이고 어떻게 작동하는지 배워보세요.\n\n여기가 전체 코드입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```json\n#include \"WiFiClientSecure.h\"\n#include \u003cPubSubClient.h\u003e\n#include \"credentials.h\"\n\n// WiFi credentials\nconst char* ssid = \"\u003c와이파이 라우터 이름\u003e\";\nconst char* password = \"\u003c와이파이 비밀번호\u003e\";\n\n// MQTT Broker credentials\nconst char* mqtt_broker = \"\u003c브로커의 IP 주소\u003e\";\nconst char* topic = \"test\";\nconst int mqtt_port = 8883 ;\n\n// SSL 인증서 설정\nconst char* root_ca =  CA_CRT;\nconst char* server_cert = SERVER_CERT;\nconst char* server_key  = SERVER_KEY;\n\n// WiFiClient espClient;\nWiFiClientSecure espClient;\nPubSubClient client(espClient);\n\nvoid setup() {\n  // 소프트웨어 시리얼 보드레이트 설정: 115200\n  Serial.begin(115200);\n  // WiFi 네트워크에 연결\n  WiFi.begin(ssid, password);\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.println(\"WiFi에 연결 중..\");\n  }\n  Serial.println(\"WiFi 네트워크에 연결되었습니다.\");\n  // SSL을 사용하여 MQTT 브로커에 연결\n  espClient.setCACert(root_ca);\n  espClient.setCertificate(server_cert);  // 클라이언트 확인을 위해\n  espClient.setPrivateKey(server_key);    // 클라이언트 확인을 위해\n\n  // 원격 MQTT 브로커에 연결\n  client.setServer(mqtt_broker, mqtt_port);\n  client.setCallback(callback);\n\n  while (!client.connected()) {\n    // 클라이언트 ID 생성\n    String client_id = \"esp32-client\";\n    client_id += String(WiFi.macAddress());\n    // esp32 컨트롤러의 이름과 ID 출력\n    Serial.printf(\"클라이언트 %s가 공개 MQTT 브로커에 연결합니다\\n\", client_id.c_str());\n    if (client.connect( client_id.c_str())) {\n      Serial.println(\"Public emqx mqtt 브로커에 연결되었습니다\");\n    } else {\n      Serial.print(\"연결 실패, 상태: \");\n      Serial.print(client.state());\n      delay(2000);\n    }\n  }\n  // 게시 및 구독\n  client.publish(topic, \"안녕, 나는 ESP32^^\");\n  client.subscribe(topic);\n}\n\nvoid callback(char* topic, byte* payload, unsigned int length) {\n  Serial.print(\"주제에서 메시지 도착: \");\n  Serial.println(topic);\n  Serial.print(\"메시지:\");\n  for (int i = 0; i \u003c length; i++) {\n    Serial.print((char)payload[i]);\n  }\n  Serial.println();\n  Serial.println(\"-----------------------\");\n}\n\nvoid loop() {\n  client.loop();\n\n  client.publish(topic, \"ESP32가 MQTT 브로커로 암호화된 메시지를 보냅니다\");\n\n  // 10초마다 메시지 전송\n  delay(10000);\n}\n```\n\nSSL 인증서를 설정하려면 credentials.h 파일을 작성해야 합니다.\n\n아래와 같은 형태여야 합니다:\n\n```js\n#define CA_CRT                                                           \\\n    \"-----BEGIN CERTIFICATE-----\\n\"                                      \\\n    \"MIIDVzCCAj+gAwIBAgIUIwOnfZCxwCKrQXHpbI0rVksEcaMwDQYJKoZIhvcNAQEL\\n\" \\\n    // 중략\n#define SERVER_CERT                                                      \\\n    \"-----BEGIN CERTIFICATE-----\\n\"                                      \\\n    \"MIIDBTCCAe0CFHjJN1GNHVnqXiMyngV3yvLg/70FMA0GCSqGSIb3DQEBCwUAMDsx\\n\" \\\n    // 중략\n#define SERVER_KEY                                                       \\\n    \"-----BEGIN RSA PRIVATE KEY-----\\n\"                                  \\\n    \"MIIEpQIBAAKCAQEAvsxvL0H8M9HjGplper2/oRtQQTFfBYLX3JfBrTJIXD6A5HFJ\\n\" \\\n    // 중략\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그런 다음 자격 증명 파일을 업로드하려는 메인 파일에 자격 증명.h 파일을 가져와서 사용할 수 있습니다.\n\n## 마지막으로 MQTT 브로커에서 나타나는 다음 메시지를 확인해야합니다.\n\n![이미지](/assets/img/2024-06-20-IoTESP32usingssltlscertificatetoconnectwidthMQTTBrokerPart2_1.png)\n\n# 정리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요약하면, 저는 브로커 부분과 컨트롤러 부분을 포함한 가정용 IOT 시스템의 기본 개념을 보여드렸습니다.","ogImage":{"url":"/assets/img/2024-06-20-IoTESP32usingssltlscertificatetoconnectwidthMQTTBrokerPart2_0.png"},"coverImage":"/assets/img/2024-06-20-IoTESP32usingssltlscertificatetoconnectwidthMQTTBrokerPart2_0.png","tag":["Tech"],"readingTime":4},{"title":"짝커 - 아두이노로 박수 감지 스위치를 만드는 방법","description":"","date":"2024-06-20 17:03","slug":"2024-06-20-TheClapperHowtomakeaclapsensitiveswitchwithArduino","content":"\n\n당신이 말한 코딩에 관한 책이 필요하다면 언제든지 물어보세요! 저는 여러가지 책과 자료를 추천해드릴 수 있어요. 요즘 프로그래밍을 배우는 것은 정말 뜻깊은 경험이 될거에요. 함께 열심히 공부해서 좋은 개발자가 되어봐요!😊\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n필요한 것이 있을까요? 하드웨어부터 시작해봅시다. 이러한 프로젝트를 위해서는 하드웨어가 필수입니다. 사용된 구성 요소 목록은 다음과 같습니다:\n\n- 브레드보드 (1개)\n- LED (1개)\n- 아두이노 Me 마이크로컨트롤러 (거의 모든 마이크로컨트롤러를 사용할 수 있습니다) (1개)\n- 소리 센서 (1개)\n- 1000R 저항 (1개)\n- M-F 점퍼 케이블 (3개)\n- M-M 점퍼 케이블 (5개)\n- 전원 공급원 (1개)\n\n## 핀 매칭\n\n![핀 매칭](/assets/img/2024-06-20-TheClapperHowtomakeaclapsensitiveswitchwithArduino_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 코드\n\n가장 기본적인 구현부터 시작해서 원하는 대로 가보자. 우리의 사운드 센서로부터 신호를 수신하고, 신호를 \"임계값\"과 비교하는 코드를 구현해보자. 만약 신호가 임계값을 초과하면 LED 라이트를 토글한다.\n\n```js\n // sound-sensitive-switch-v1.ino\n\nconst int micPin = A0; // 마이크가 연결된 아날로그 입력 핀\nconst int ledPin = 2;\nconst int threshold = 60;\n\nint sensorValue = 0;\nbool ledState = false;\n\nvoid setup() {\n Serial.begin(9600);\n \n pinMode(micPin, INPUT);\n pinMode(ledPin, OUTPUT);\n}\n\nvoid loop() {\n sensorValue = analogRead(micPin);\n \n // 박수 감지 로직\n if (sensorValue \u003e threshold) {\n  // 박수 감지!\n  digitalWrite(ledPin, ledState ? LOW : HIGH);\n  ledState = !ledState;\n  \n  delay(500); // 여러 번 트리거를 방지하기 위한 디바운스 딜레이\n }\n\n delay(10); // 샘플링 딜레이\n}\n```\n\n연결이 제대로 되어있다면 주어진 임계값을 초과하는 모든 소리로 라이트를 트리거할 수 있어야 합니다. 그러나 이 설정과 코드에 대한 또 다른 문제로 우리를 이끌어간다면, 마이크 민감도입니다. 나의 실험에서, 마이크를 낮은 민감도로 설정하면 회로 내의 소리 센서에서의 잡음을 더 잘 관리할 수 있었습니다. 이것은 완벽하지 않았으며, 조용한 환경에서도 0-45 사이의 신호값을 가지고 있었습니다. 이 레벨의 침묵을 달성하기 위해 소리 센서 부품의 기록용 가변저항을 사용했습니다. 이것은 3326 모델이며, 회로에서 잡음 신호를 줄이기 위해 회전시킬 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-20-TheClapperHowtomakeaclapsensitiveswitchwithArduino_2.png)\n\n여기에는 포트 튜닝 전후의 신호 데이터가 표시되어 있습니다. Y축에서 볼 수 있듯이, 튜닝 전에는 40 이상의 값이 나타나는 반면, 튜닝 후에는 10 미만으로 감소된 것을 확인할 수 있습니다.\n\n포트 튜닝 전.\n\n![image](/assets/img/2024-06-20-TheClapperHowtomakeaclapsensitiveswitchwithArduino_3.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그 후, 마이크 민감도를 더 줄일 수 있다는 것을 발견했습니다. 더 부드러운 파형을 강제하는데 그러나 다른 제약 사항이 있었습니다. 클랩하면 스위치가 활성화되는 매우 높은 수준의 소음이 있어야 했습니다.\n\n![이미지](/assets/img/2024-06-20-TheClapperHowtomakeaclapsensitiveswitchwithArduino_4.png)\n\n참고: 마이크로컨트롤러에 따라 다른 베이스라인 읽기가 있는 것을 알았습니다. 내 아두이노 Uno Mini가 소음 수준에서 나의 아두이노 메가보다 더 나은 성능을 발휘했습니다.\n\n위의 조정을 하고 나서, 라이트 스위치가 작동하지만 우리의 진폭 임계값을 초과하는 모든 소리에 의해 트리거됩니다. 이와 같은 문제에 직면했을 때는 전략 재고가 필요합니다. 그래서 다시 기본부터 시작하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n손뼉 소리는 복잡한 소리 파형을 생성하지 않는다는 것을 알아봅시다. 마이크가 고품질이 아닌 것이 이를 강조합니다. 더 복잡한 파형은 분석을 위한 더 명확한 신호를 제공할 수 있을지 모르지만, 여전히 간단한 손뼉에서 유용한 통찰을 얻을 수 있습니다. 나는 아두이노를 연결하여 손뼉의 특성을 나타내는 그래프를 그리는 데이터를 캡처했습니다. 여기에 결과가 있습니다.\n\n![TheClapper](/assets/img/2024-06-20-TheClapperHowtomakeaclapsensitiveswitchwithArduino_5.png)\n\n손뼉은 소리 파동의 진폭이 급격하게 증가한 후 급격히 감소하는 것으로 특징 지어집니다. 그러나, 이 정의는 한 가지 문제점을 강조합니다: 비슷한 특성은 문을 세게 닫거나 탁을 치거나 책을 떨어뜨릴 때와 같은 다양한 소리로도 나타낼 수 있습니다.\n\n이를 해결하기 위해 이러한 제한을 극복할 전략을 개발해 보겠습니다. 여기 잠재적인 접근 방식이 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 소리를 캡처합니다.\n- 녹음된 소음 레벨이 미리 정의된 진폭 임계값을 초과하는지 확인합니다.\n- 특정 시간 창 내에서 추가 소음을 청취합니다.\n- 박수의 빠르게 감소하는 특성과 일치하는지 확인하기 위해 소리의 감쇠 속도를 분석합니다. 그렇다면 원하는 작업(예: 조명 켜기/끄기)을 계속 진행합니다.\n\n이 프로그램의 핵심은 명백히 소리 인식이므로, 박수의 소리파를 어떻게 인식할 수 있는지 궁금해집니다.\n\n변화를 분석하고 감지하기 위해 ML 모델을 사용하는 것이 떠올랐기 때문에, 이러한 특성을 찾기 위한 모델을 개발했습니다. 그러나 이에 대해 다른 글에서 자세히 다루겠습니다. tinyML과 같은 기술의 계산 요구 사항에 따라, 소리파 신호를 깨끗하고 부드럽게 만들기 위한 물리학과 수학의 방법을 먼저 조사하기로 결정했습니다. 일부 연구를 거친 후, 그리고 FFT를 시도한 후, 고역통과 필터라는 좀 더 \"기본적\"인 접근 방법으로 결정했습니다.\n\n고역통과 필터\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하이 패스 필터는 일정한 차단 주파수보다 높은 주파수를 가진 신호를 통과시키고, 차단 주파수보다 낮은 주파수를 가진 신호를 약화시키는 전자 필터입니다. 이 필터는 낮은 주파수 신호를 통과시키지 않고, 대신 필터 설계에 따라 일관되게 차단합니다.\n\n우리 필터는 어떻게 생겼을까요?\n\n```js\n// 낮은 주파수 잡음을 제거하는 간단한 하이 패스 필터\nhighPass = alpha * (highPass + sample - lastSample);\nlastSample = sample;\n\n// 획득기 - 절대값 취하고 평활화\nenvelope *= release;\nenvelope = max(envelope, abs(highPass));\n```\n\nhighPass: 필터의 현재 값\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알파: 알파는 일반적으로 0에서 1 사이의 값으로 범위하며, 필터의 차단 주파수를 결정하는 계수입니다. 알파 값이 낮을수록 차단이 더 뚜렷해지며, 이는 필터가 이전 상태를 더 오래 유지하게 한다는 것을 의미합니다. 그 반대로 높은 값은 값을 더 많이 변경하며 알파가 클수록 부드러운 차단이 이뤄집니다.\n\n샘플: 이것은 마이크로폰에서의 현재 입력 샘플입니다.\n\nlastSample: 이것은 마이크로폰에서 이전에 입력한 샘플입니다.\n\nEnvelope Detector: 엔벨롭 디텍터는 소리 처리 도구로, 소리 파형의 바깥쪽 한계를 캡처하는 데 사용됩니다. 이는 신호의 급격한 변화가 아닌 파형의 진폭의 느린 변화를 캡처하는 것을 의미합니다. 아래는 디지털 하이 패스 필터를 통과한 후의 파형의 결과입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 코드를 Markdown 형식으로 변경했습니다:\n\n![이미지](/assets/img/2024-06-20-TheClapperHowtomakeaclapsensitiveswitchwithArduino_6.png)\n\n변경된 코드는 다음과 같습니다:\n\n```js\n// sound-sensitive-switch-v2.ino\n\nconst int micPin = A0; // 마이크가 연결된 아날로그 입력 핀\nconst float alpha = 0.9; // 고주파 필터용 계수\n\nint sample;\nfloat filtered, highPass;\nfloat lastSample = 0;\nfloat envelope = 0;\nfloat release = 0.9; // 엔벨롭 감지기 용 해제 계수\nint threshold = 30; // 박수 감지 임계값\n\nbool ledState = false;\nint ledPin = 2;\n\nvoid setup() {\n Serial.begin(9600);\n pinMode(micPin, INPUT);\n pinMode(ledPin, OUTPUT);\n}\n\nvoid loop() {\n sample = analogRead(micPin);\n // 단순 고주파 필터를 사용하여 저주파 노이즈 제거\n highPass = alpha * (highPass + sample - lastSample);\n lastSample = sample;\n \n // 엔벨롭 감지기 - 정류하고 부드럽게\n envelope *= release;\n envelope = max(envelope, abs(highPass));\n \n // 박수 감지 논리\n if (envelope \u003e threshold) {\n  unsigned long timeStart = millis();\n  \n  while (millis() - timeStart \u003c 100) { // 100ms 내에 빠른 감쇠 확인\n   sample = analogRead(micPin);\n   highPass = alpha * (highPass + sample - lastSample);\n   lastSample = sample;\n   envelope *= release;\n   envelope = max(envelope, abs(highPass));\n  }\n  \n  if (envelope \u003c threshold / 2) { // 엔벨롭이 감쇠해야 함\n   // 박수 감지!\n   digitalWrite(ledPin, ledState ? LOW : HIGH);\n   ledState = !ledState;\n  }\n  \n  delay(500); // 다중 트리거 방지를 위한 디바운스 지연\n }\n\n delay(10); // 샘플링 지연\n}\n```\n\n이 코드는 상당히 잘 작동합니다. 소음이 큰 음악으로 테스트했을 때, 원하는 특성을 가진 짧은 소리 폭발 이외에는 스위치를 트리거할 수 없었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 시스템이 잘 작동하는 것을 확인했으니 강화를 시도해 볼 수 있을 것 같아요. 이를 위한 한 가지 방법은 트리거 소리의 파형을 더 복잡하게 만드는 것입니다. 우리는 박수 그 자체를 바꿀 수 없지만, 대신 1번의 박수 대신 2번 연속된 박수를 트리거로 인식할 수 있습니다. 이렇게 하면 책이 탁 탁하는 소리나 문이 쾅 하고 닫혀도 스위치가 작동하지 않을 가능성이 높아집니다. 이것은 간단한 선택이죠.\n\n이를 실행하기 위해 대부분의 코드는 동일하게 유지되지만, 여기에 변경 사항이 있습니다:\n\n```js\n// 박수를 마지막으로 감지한 타임스탬프를 추적\nunsigned long lastClapTime = 0;\n// 발견한 박수의 횟수\nint clapCount = 0;\n// 더블 박수를 감지할 시간 창문 (1초)\nconst unsigned long clapDelay = 1000;\n```\n\n우리의 \"박수 감지 로직\" 안에서, 연속된 박수가 세어질 수 있는 시간 창문을 고려하면서 발견한 박수 횟수를 세는 이 코드 조각을 추가할 것입니다. 이는 500ms의 시간 창문 내에서 연속된 박수가 카운트될 수 있음을 의미하는데요. 이것은 특정한 박수 패턴을 갖게끔 하기 위해 연속된 박수를 계산하는 상한선을 여기에 표시하고 있음을 기억해 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nif (timeNow - lastClapTime \u003e 500) { // 클랩이 너무 가깝지 않게 500밀리초 간격 확인\n clapCount++;\n lastClapTime = timeNow;\n}\n```\n\n여기서 중요한 점은 코드에서 clapCount가 이 시점에서 1만큼 증가했음에도 실제로 클랩으로 간주하려면 신호 읽기가 신속한 감쇠 테스트를 통과해야 한다는 것입니다. 따라서 그 코드를 구현해 보겠습니다.\n\n```js\nif (clapCount == 2 \u0026\u0026 (millis() - lastClapTime \u003c= clapDelay)) {\n digitalWrite(ledPin, ledState ? LOW : HIGH);\n ledState = !ledState;\n clapCount = 0; // 작업 후 클랩 횟수 카운터 재설정\n}\n```\n\n여기서, 스위치를 토글하기 전에 클랩 횟수가 2이고 연이어 발생한 클랩이 clapDelay 시간 내에 있는지 확인합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막 코드 부분은 가정정리용입니다. 여기서 clapCount가 0보다 크고 현재 시간과 박수가 발생한 시간 간의 차이 (lastClapTime)가 clapDelay 시간 프레임을 벗어나는 경우 clapCount를 재설정합니다. 즉, clap이 clapDelay 시간 프레임을 벗어나 늙었으면 그 clap을 무시합니다. 이때 clapDelay는 lastClapTime 이후 1000ms입니다.\n\n```js\nif (millis() - lastClapTime \u003e clapDelay \u0026\u0026 clapCount \u003e 0) {\n  clapCount = 0;\n}\n```\n\n이제 2개 clap을 고려한 최종 코드는 다음과 같습니다:\n\n```js\nconst int micPin = A0; // 초음파가 연결된 아날로그 입력 핀\nconst float alpha = 0.9; // 하이패스 필터용 계수\nint sample;\nfloat highPass;\nfloat lastSample = 0;\nfloat envelope = 0;\nfloat release = 0.9; // 엔벨로프 감지기의 릴리스 계수\nint threshold = 20; // 박수 감지 임계값\n\nbool ledState = false;\nint ledPin = 2;\nunsigned long lastClapTime = 0;\nint clapCount = 0;\nconst unsigned long clapDelay = 1000; // 이중 박수 감지 시간 창문 (1초)\n\nvoid setup() {\n  Serial.begin(9600);\n  pinMode(micPin, INPUT);\n  pinMode(ledPin, OUTPUT);\n}\n\nvoid loop() {\n  sample = analogRead(micPin);\n  // 저주파 노이즈 제거를 위한 하이패스 필터\n  highPass = alpha * (highPass + sample - lastSample);\n  lastSample = sample;\n\n  // 엔벨로프 감지기 - 정류 및 평활화\n  envelope *= release;\n  envelope = max(envelope, abs(highPass));\n\n  Serial.println(sample);\n  // 박수 감지 논리\n  if (envelope \u003e threshold) {\n    unsigned long timeNow = millis();\n    if (timeNow - lastClapTime \u003e 500) {  // 박수가 너무 가깝지 않도록, 500ms 간격 확인\n      clapCount++;\n      lastClapTime = timeNow;\n    }\n\n    // 100ms 내에 빠르게 감소하는 경우 확인\n    while (millis() - timeNow \u003c 100) {\n      sample = analogRead(micPin);\n      highPass = alpha * (highPass + sample - lastSample);\n      lastSample = sample;\n      envelope *= release;\n      envelope = max(envelope, abs(highPass));\n    }\n\n    if (envelope \u003c threshold / 2) { // 엔벨로프가 감소해야 함\n      if (clapCount == 2 \u0026\u0026 (millis() - lastClapTime \u003c= clapDelay)) {\n        digitalWrite(ledPin, ledState ? LOW : HIGH);\n        ledState = !ledState;\n        clapCount = 0; // 동작 후 clap 카운터 재설정\n      }\n    }\n  }\n\n  // 시간 창문 내에 두 번째 clap이 없을 경우 clap 수를 재설정\n  if (millis() - lastClapTime \u003e clapDelay \u0026\u0026 clapCount \u003e 0) {\n    clapCount = 0;\n  }\n\n  delay(10); // 샘플링 지연\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 다음 수준으로 업그레이드해 보세요\n\n이제 우리에게 완전히 작동하는 시스템과 코드가 준비되었으니, 실제 기기에 연결하여 회로를 더 개선해 보는 것이 좋겠죠. 이를 위해 아이디어를 고민해 오셨다면, 여기에 답이 있습니다!\n\n계속 진행하려면 몇 가지 추가 구성품이 필요합니다:\n\n- 아두이노 보드, 브레드보드 및 마이크로폰을 포함할 하우징\n- 5V 단일 채널 릴레이와 함께 선택한 릴레이\n- 전원 케이블이 연장된 플러그\n- 고전압 기기에 연결할 여성 AC 전원 소켓\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 핀 재매핑\n\n우선 배선을 변경해보세요. 이 기사의 첫 부분에서는 LED 전구를 켰었습니다. 이번에는 LED를 제거하지만 동일한 배선을 사용하고, 우리의 램프가 도입한 추가 연결을 더할 것입니다.\n\n새로운 연결은 다음과 같습니다:\n\n마이크 연결은 그대로 유지됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-TheClapperHowtomakeaclapsensitiveswitchwithArduino_7.png\" /\u003e\n\n만약 모든 연결이 올바르게 되어 있다면, 램프를 전원에 연결하고 아두이노를 5V 전원에 연결하십시오. 이제 당신의 램프는 박수로 제어될 수 있습니다 👏.\n\n자신에게 박수를 쳐주시겠어요? 😅","ogImage":{"url":"/assets/img/2024-06-20-TheClapperHowtomakeaclapsensitiveswitchwithArduino_0.png"},"coverImage":"/assets/img/2024-06-20-TheClapperHowtomakeaclapsensitiveswitchwithArduino_0.png","tag":["Tech"],"readingTime":10},{"title":"소형 머신 러닝 - 주성분 분석","description":"","date":"2024-06-20 17:00","slug":"2024-06-20-TinyMLPrincipalComponentAnalysis","content":"\n\nFrom mathematical foundations to edge implementation\n\n# 소셜 미디어:\n\n👨🏽‍💻 Github: thommaskevin/TinyML (github.com)\n👷🏾 Linkedin: Thommas Kevin | LinkedIn\n📽 Youtube: Thommas Kevin — YouTube\n👨🏻‍🏫 Research group: Conecta.ai (ufrn.br)\n\n![image](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 1 — PCA 알고리즘 이론\n\n주성분 분석 (PCA)은 데이터 분석에서 널리 사용되는 강력한 기술로, 데이터 세트의 차원을 줄이면서 중요한 정보를 유지하는 데 특히 유용합니다. 이는 원래 변수를 주성분이라고 불리는 새로운, 상관관계가 없는 변수 세트로 변환하여 달성됩니다. PCA의 주요 측면을 자세히 살펴봅시다:\n\n- 차원 축소: PCA는 고차원 데이터 세트를 처리하는 데 중요합니다. 중요한 정보를 추출하고 관련성이 낮은 특징을 제거하여 분석 프로세스를 단순화합니다.\n- 데이터 탐색 및 시각화: PCA는 데이터 탐색 및 시각화에 중요한 역할을 합니다. 숨겨진 패턴과 통찰을 찾아내는 데 기여합니다.\n- 선형 변환: PCA는 데이터의 선형 변환을 수행하며, 분산의 방향을 식별하는 데 목적을 두고 있습니다. 이를 통해 데이터 세트의 더 간결한 표현이 가능해집니다.\n- 특성 선택: 주성분은 설명하는 분산에 기반하여 순위가 매겨집니다. 이 순위는 데이터의 가장 중요한 측면을 강조하는 효과적인 특성 선택을 용이하게 합니다.\n- 데이터 압축: PCA는 데이터를 압축하는 데 뛰어납니다. 큰 데이터 세트의 효율적인 저장 및 처리를 위해 원본 정보의 상당 부분을 유지합니다.\n- 클러스터링 및 분류: PCA는 클러스터링 및 분류 작업에서 실용적으로 활용됩니다. 잡음을 줄이고 기존 구조를 강조함으로써 알고리즘의 성능을 향상시킵니다.\n\n## 1.1 — 데이터 적합성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"Adequacy of Data\" 또는 \"Data Suitability\"는 일반적으로 사용 가능한 데이터가 특정 목적이나 분석에 적합하고 충분하며 관련성 있는지 여부를 나타냅니다. 그것은 손에 있는 데이터가 의도된 사용 또는 연구에 필요한 요구 사항과 기준을 충족하는지를 평가합니다.\n\n1.1.1 — Kaiser-Meyer-Olkin(KMO)\n\nKaiser-Meyer-Olkin 측정치는 분석을 위한 표본의 적절성을 평가하며, 고려중인 변수가 구분된 신뢰성 있는 요인을 얻을 것으로 예상되는지 여부를 나타냅니다. KMO 통계치는 0에서 1 사이의 범위 내에 있으며, 더 높은 값은 요인 분석에 더 적합함을 나타냅니다.\n\n- KMO가 0.5 이상일 경우: 주로 허용 가능하다고 간주되며, 데이터 집합이 요인 분석에 적합함을 나타냅니다.\n- KMO가 0.5 미만일 경우: 데이터 집합이 요인 분석에 적합하지 않을 수 있다는 것을 시사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_1.png\" /\u003e\n\n1.1.2 — Bartlett의 구형성 검정\n\nBartlett의 구형성 검정은 요인 분석의 맥락에서 사용되는 통계적 검정으로, 관측된 변수들의 상관 행렬이 항등 행렬과 유의하게 다른지를 결정하기 위해 사용됩니다. 간단히 말하면, 이는 변수들 사이에 충분한 상관관계가 있는지를 평가하여 요인 분석을 계속 진행할 수 있는지 여부를 파악하는 데 도움이 됩니다.\n\n다음은 Bartlett의 구형성 검정의 자세한 단계와 수학적 공식입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계 1: 가설 설정\n\n- 귀무가설 (H0): 관찰된 상관 행렬은 독립 행렬이며 변수 간 상관 관계가 없음을 나타냅니다.\n- 대립가설 (H1): 관찰된 상관 행렬은 독립 행렬이 아니며 변수 간 유의한 상관 관계가 있다는 것을 시사합니다.\n\n단계 2: 바트렛의 구형성 검정 통계량 계산\n\n바트렛의 구형성 검정 통계량은 카이제곱 (χ2) 분포를 따릅니다. 검정 통계량의 공식은 다음과 같습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_2.png\" /\u003e\n\n여기서:\n\n- n은 관찰값의 수입니다.\n- p는 변수의 수입니다.\n- det(R)은 관찰된 상관 행렬의 행렬식입니다.\n- det(I)은 항등 행렬의 행렬식입니다.\n\n단계 3: 자유도 결정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자유도(df)는 카이 제곱 분포의 경우 다음과 같이 계산됩니다:\n\n![image](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_3.png)\n\n단계 4: 임계값과 비교\n\n계산된 χ² 통계량을 선택한 유의 수준 (예: 0.05)에서 카이 제곱 분포 표의 임계값과 비교합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n### 단계 5: 결정 내리기\n\n만약 계산된 χ2 값이 임계값보다 크다면, 귀무가설을 기각해야 합니다. 이는 변수들 간에 상관 관계가 유의미하며 요인 분석이 적절할 수 있다는 것을 시사합니다.\n\n### 1.2 — 상관계수 행렬\n\n변수 집합에 대한 Pearson 상관계수 행렬은 행렬 형태로 표현될 수 있습니다. n개의 변수를 X1,X2,…,Xn으로 표시한다고 가정해 봅시다. 이러한 변수에 대한 Pearson 상관계수 행렬 R은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_4.png)\n\nHere:\n\n- The diagonal elements (main diagonal) are all 1 because each variable perfectly correlates with itself.\n- The off-diagonal elements (rij) represent the Pearson correlation coefficient between variables Xi and Xj.\n- The matrix is symmetric (rij=rji) because the correlation between Xi and Xj is the same as the correlation between Xj and Xi.\n\nThe formula to compute rij (Pearson correlation coefficient between Xi and Xj) is given by:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_5.png)\n\n이 공식에는 평균 (Xˉi 및 Xˉj) 및 제곱 차이의 합이 포함되어 있습니다. 두 변수에 대한 이전에 설명한 쌍별 계산과 유사합니다.\n\n피어슨 상관 관계는 선형 관계만을 측정하므로 비선형 관계는 포착하지 못할 수 있습니다. 피어슨 상관 계수는 -1에서 1 사이의 범위를 가집니다:\n\n- r=1: 완벽한 양의 상관 관계;\n- r=−1: 완벽한 음의 상관 관계;\n- r=0: 선형 상관 관계 없음.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 1.3 — 고유값과 고유벡터\n\n1.3.1 — 고유값 (λ)\n\n고유값은 데이터의 최대 변이성을 포착하는, 정사각 행렬에서 유도된 독립적인 벡터들입니다. 고유값은 따라서 연구된 변수들에 의해 포착된 총 분산의 부분으로 이해될 수 있습니다.\n\n- 고유값은 식 Av=λv이 비제로 해 v를 가지는 스칼라 λ입니다.\n- 종종 λ로 표시됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n고윳값을 찾는 방정식은 A를 원래 행렬에서 λI(단위 행렬)을 뺀 후 Determinant를 구하여 얻는 특성 방정식을 풀어야 합니다:\n\ndet(A−λI)=0\n\nλ에 대한 이 방정식을 해결하면 행렬 A의 고윳값을 얻게 됩니다.\n\n1.3.2 —고유벡터 (v)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 고유벡터는 방정식 Av=λv를 만족하는 0이 아닌 벡터 v입니다.\n- 종종 v로 표기됩니다.\n\n- 고유값을 구했다면, 각 고유값을 방정식 Av=λv에 대입하여 v에 대해 풀어서 해당하는 고유벡터를 찾을 수 있습니다. 이 솔루션들이 고유벡터를 형성합니다.\n\n이 과정은 아래와 같이 요약될 수 있습니다:\n\n- 고유문자식: det(A−λI)=0;\n- 고유값(λ) 구하기: 고유문자식을 해결하여 고유값 λ를 찾습니다.\n- 고유벡터(v) 찾기: 각 고유값 λ에 대해, (A−λI)v=0인 선형 방정식을 풀어 해당하는 고유벡터 v를 찾습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 1.4 — 요인들\n\nPCA는 식별된 직교 기저의 새로운 축을 나타내는 요인의 집합입니다. 첫 번째 요인은 샘플의 분산을 가장 잘 설명하는 조합입니다. 두 번째 요인은 두 번째로 높은 분산을 설명하며 첫 번째 요인과 관련이 없습니다. 이와 같이 계속됩니다. 따라서 PCA 알고리즘의 출력은 초관련 변수 집합으로부터 유도된 상관관계가 없는 변수 집합입니다.\n\nPCA에서 주성분(요인)을 형성하기 위해 원래 변수들의 선형 조합을 행렬 형태로 표현할 수 있습니다. 표준화된 데이터 행렬을 Z로 표시하고(열은 표준화된 변수를 나타냄), 고유벡터 행렬을 V로 표시합니다(열은 고유벡터를 나타냄). k번째 주성분(Fk)을 얻기 위한 선형 조합은 다음과 같이 표현될 수 있습니다:\n\n![이미지](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기에는 Z가 표준화된 데이터 행렬이고, Vk는 k번째 고유값에 해당하는 고유벡터 행렬의 k번째 열입니다. Fk의 개별 요소는 다음과 같이 표현될 수 있습니다:\n\n\n| Fk1 |\n| Fk2 |\n| ... |\n| Fkn |\n\n\n이 식에서 Fki는 k번째 주성분의 i번째 관측값, Zij는 j번째 표준화된 변수의 i번째 관측값, Vjk는 j번째 고유벡터의 k번째 요소입니다.\n\n이 선형 결합을 사용하여 k번째 주성분을 원래 표준화된 변수들의 가중 합으로 표현할 수 있습니다. 가중치는 k번째 고유벡터의 요소로 제공됩니다. 각 주성분은 원래 변수들의 다른 선형 결합을 나타내며, 데이터의 최대 분산을 포착합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1.4.1 — 요인 수\n\n- 카이저의 규칙: 이 방법은 각각의 고유값이 1보다 큰 값을 가지는 요인을 선택하는 방법으로 구성되어 있습니다. 다시 말해서, 우리는 1 이상의 분산을 설명하는 요인들만 사용하고 싶습니다.\n- 공유 분산 분석: 이 방법은 누적된 공유 분산의 합을 분석하는 주관적인 방법입니다. 이 방법을 통해 우리는 가능한 한 적은 수의 요인을 유지하면서 가장 많은 분산을 설명하는 요인의 수를 선택하려고 합니다. 이를 위해, 비즈니스 문제에 따라 70~90%의 공유 분산이 설명되면 충분할 수 있으므로, 공유 분산의 70~90%를 합한 요인이 정의될 것입니다.\n\n## 1.5— 요인 부하\n\n요인 부하는 특정 요인에 대한 각 원본 변수에 할당된 가중치를 의미합니다. 이러한 부하는 각 변수와 해당 요인간의 관계의 강도와 방향을 나타냅니다. 요인 부하의 수학적 설명을 살펴봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPCA에서 요인 적재량은 표준화된 데이터의 공분산 행렬 또는 상관 행렬의 고유벡터에서 유도됩니다. Vk가 k번째 고유값에 연결된 고유벡터라고 가정해 봅시다. j번째 변수와 k번째 요인의 요인 적재량(λjk)은 Vk의 요소에서 얻어집니다.\n\n요인 적재량은 원래 변수 Xj가 k번째 요인에 얼마나 기여하는지를 나타냅니다. λjk가 양수이면, Xj와 k번째 요인 사이에 긍정적 상관 관계가 있다는 것을 시사하며, 음수이면 음적 상관 관계를 시사합니다. 요인 적재량의 크기는 관계의 강도를 나타냅니다.\n\n요인 모델에서 요인 적재량(λjk)은 원래 변수와 잠재적인 요인 사이의 관계를 나타내는 매개변수입니다. 이 모델은 다음과 같이 표현됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같은 테이블을 Markdown 포맷으로 바꿔주세요.\n\n\n| 변수 | 설명 |\n|------|------|\n| Xj   | j번째 원본 변수 |\n| λjk  | Xj와 k번째 요인 간의 관계를 나타내는 요인 로딩 |\n| Fk   | k번째 잠재 요인 |\n| εj   | j번째 변수와 연관된 오차 |\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기존 변수에서 잠재 요인으로 설명되는 분산의 비율을 의미합니다. \"공용성\"의 수학적 설명을 살펴보겠습니다.\n\n만약 p개의 원래 변수 X1,X2,…,Xp가 있다면, j번째 변수 (Hj)의 공용성은 다음과 같이 계산됩니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- m은 인자의 수입니다.\n- λjk는 j번째 변수의 k번째 인자에 대한 인자 적재입니다.\n\n공통 분산은 잠재적 요인에 의해 설명된 Xj 변수의 분산의 총량을 나타냅니다. 합계에서 각 항목 2λjk^2은 공통 분산에 기여하며, k번째 인자에 의해 설명된 변수 Xj의 분산의 비율을 나타냅니다.\n\n실용적으로, Hj가 1에 가까우면 변수 Xj가 잠재적 요인에 의해 잘 설명된다는 것을 나타냅니다. 0에 가까우면 변수는 모델의 요인들에 의해 잘 표현되지 않습니다.\n\n모든 변수의 공통성의 합계는 모델의 요인들에 의해 설명된 총 분산의 전반적인 지표입니다. 변수가 p개인 경우, 총 분산은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 요인에 의해 설명된 분산이며:\n\n![image2](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_12.png)\n\n요인에 의해 설명된 분산의 비율은 요인에 의해 설명된 분산을 총 분산으로 나누어 계산할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2— TinyML 구현\n\n이 예시를 통해 ESP32, 아두이노, 라즈베리파이 및 다른 다양한 마이크로컨트롤러 또는 IoT 장치에 기계 학습 알고리즘 (PCA)을 구현할 수 있습니다.\n\n1 — 다음 명령으로 micromlgen 패키지를 설치하세요:\n\n```js\n!pip install micromlgen\n!pip install factor_analyzer\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2 — 라이브러리 가져오기\n\n```python\nfrom micromlgen import port\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport numpy as np\n\nfrom factor_analyzer.factor_analyzer import calculate_kmo\nfrom factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\nfrom factor_analyzer import FactorAnalyzer\n```\n\n3 — 데이터셋 로드\n\nDecathlon\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n41행 13열로 구성된 데이터베이스가 있습니다. 첫 10개 열은 각 선수의 10종목 경기 성적에 해당합니다. 열 11과 12는 각각 선수의 등급과 획득한 점수를 나타냅니다. 마지막 열은 2004년 올림픽 게임 또는 2004년 데카스론과 같은 종목에 해당하는 범주형 변수입니다.\n\n다음은 변수입니다.\n\n- 100m (100m 달리기)\n- long.jump (멀리뛰기)\n- shot.put (역도)\n- High.jump (높이뛰기)\n- 400m (400m 달리기)\n- 110m.hurdle (110m 허들)\n- Discus (원반 던지기)\n- Pole.vault (양봉)\n- Javeline (투창)\n- 1500m (1500m 달리기)\n\n\npatch = './data/decathlon.csv'\ndf = pd.read_csv(patch, index_col=0)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n안녕하세요! 아래는 Markdown 형식으로 테이블의 변경 내용이 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_13.png\" /\u003e\n\n4 — 데이터셋 시각화\n\n```js\nsns.pairplot(df)\n```\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_14.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5- 데이터 표준화하기\n\n```js\ncolumns_selected = ['100m', 'Long.jump', 'Shot.put', 'High.jump', '400m', '110m.hurdle',\n       'Discus', 'Pole.vault', 'Javeline']\n\nX = df[columns_selected]\n```\n\n```js\nX_standardized = StandardScaler().fit_transform(X)\ndf_X_standardized = pd.DataFrame(X_standardized, columns=columns_selected)\n```\n\n6- 데이터의 상관 행렬 분석하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n6.1- 상관 행렬\n\n```js\ncorr = X.corr()\ncorr\n```\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_15.png\" /\u003e\n\n```js\n# 그림 크기 조정\nplt.figure(figsize=(10, 8))\n\n# 히트맵 생성을 위한 기존 코드\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)\n\n# 히트맵에 값 추가\nfor i in range(len(corr.columns)):\n    for j in range(len(corr.columns)):\n        plt.text(j + 0.5, i + 0.5, f\"{corr.iloc[i, j]:.2f}\", ha='center', va='center', color='w')\n\n# 히트맵 표시\nplt.show()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_16.png)\n\n6.2 — 고유값과 고유벡터\n\n```js\nX = np.matrix(X)\ncov_matrix =  np.cov(np.transpose(X))\n```\n\n```js\nnp.diagonal(cov_matrix)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_17.png\" /\u003e\n\n```js\neigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\nsorted_indices = np.argsort(eigenvalues)[::-1]\neigenvalues = eigenvalues[sorted_indices]\neigenvectors = eigenvectors[:, sorted_indices]\n```\n\n```js\neigenvalues\n```\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_18.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n고유벡터\n```\n\n![이미지](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_19.png)\n\n7— 데이터 적합성\n\n7.1 — 카이저-마이어-올킨 (KMO)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# KMO 값 계산\nkmo_score, kmo_model = calculate_kmo(X)\n\n# KMO 점수 표시\nprint(f'카이저-마이어-올킨 (KMO) 점수: {kmo_model}')\n```\n\n![image](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_20.png)\n\n```js\n# 변수별로 KMO를 시각화하는 막대 차트 생성\nplt.figure(figsize=(12, 6))\nplt.bar(df_X_standardized.columns, kmo_score, color='blue')\nplt.title('변수별 KMO')\nplt.xlabel('변수')\nplt.ylabel('KMO 값')\nplt.grid()\nplt.xticks(rotation=45, ha='right')  # 더 잘 보이도록 x축 레이블 회전\n```\n\n![image](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_21.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n7.2 — 바틀렛의 구 구형성 검정\n\n```js\n# 바틀렛의 구 구형성 검정 계산\nchi_square, p_value = calculate_bartlett_sphericity(X)\n\n# 검정 통계량 표시\nprint(f'카이제곱 값: {chi_square}')\nprint(f'P-value: {p_value}')\n```\n\n![Image](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_22.png)\n\n```js\n# 바틀렛의 구 구형성 검정 시각화\nplt.figure(figsize=(6, 4))\nplt.bar(['카이제곱 값', 'P-value'], [chi_square, p_value], color=['blue', 'green'])\nplt.title(\"바틀렛의 구 구형성 검정\")\nplt.ylabel('값')\nplt.show()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_23.png\" /\u003e\n\n### 8 - 주성분 분석\n\n```js\nX = np.asarray(X)\n```\n\n```js\npca = PCA()\npca.fit(X)\nautovalores = pca.explained_variance_\nautovetores = pca.components_\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n8.1 — 카이젤의 법칙\n\n```js\n# 관련 요인 분석 객체를 생성하고 데이터에 적합화합니다\nfa = FactorAnalyzer()\nfa.fit(X)\n\n# 고유값을 가져옵니다\neigenvalues, _ = fa.get_eigenvalues()\n```\n\n```js\n# 고유값을 요인 번호에 대해 그려봅니다\nplt.figure(figsize=(10, 8))\nplt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o')\nplt.title(\"Scree Plot\")\nplt.xlabel(\"요인 번호\")\nplt.ylabel(\"고유값\")\nplt.axhline(1, color='red', linestyle='dashed', linewidth=2, label=\"카이젤의 기준 (고유값 = 1)\")\nplt.legend()\nplt.grid()\nplt.show()\n```\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_24.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n8.2 — 공유 분산의 분석\n\n```js\nexp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n```\n\n```js\n# 공분산 설명 비율 시각화\nplt.figure(figsize=(10, 8))\n# 각 요인으로 설명되는 누적 분산 플롯\nplt.plot(exp_var_cumul )\nplt.title('요인별 공분산 설명 비율')\nplt.xlabel('요인 수')\nplt.ylabel('누적 설명 분산 (%)')\nplt.grid()\nplt.show()\n```\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_25.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n8.3 — Plot Components\n\n```python\nmodel = PCA(n_components=2)\nX_pca = model.fit(X)\n```\n\n```python\ncomponents = model.fit_transform(X)\ncomponents \n```\n\n```python\nloadings = model.components_.T * np.sqrt(model.explained_variance_)\n\nfig = px.scatter(components, x=0, y=1)\n\nfor i, feature in enumerate(columns_selected):\n    fig.add_shape(\n        type='line',\n        x0=0, y0=0,\n        x1=loadings[i, 0],\n        y1=loadings[i, 1]\n    )\n    fig.add_annotation(\n        x=loadings[i, 0],\n        y=loadings[i, 1],\n        ax=0, ay=0,\n        xanchor=\"center\",\n        yanchor=\"bottom\",\n        text=feature,\n    )\nfig.show()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_26.png)\n\n9 - Inverse Transform\n\n```js\nX_reconstructed = model.inverse_transform(components)\nX_reconstructed\n```\n\n![Image](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_27.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n10— 마이크로콘트롤러에 구현할 모델 획득\n\n```js\nprint(port(model))\n```\n\n```js\n#pragma once\nnamespace Eloquent {\n    namespace ML {\n        namespace Port {\n            class PCA {\n                public:\n                    /**\n                    * 차원 축소 적용\n                    * @warn 목적지 벡터가 제공되지 않으면 원본 벡터를 덮어씁니다!\n                    */\n                    void transform(float *x, float *dest = NULL) {\n                        static float u[2] = {0};\n                        u[0] = dot(x, -0.010208828727, 0.009522664636, 0.07801034572, 0.004103495259, -0.003935853874, -0.004232033634, 0.21088426957, -0.00282519103, 0.974263356619);\n                        u[1] = dot(x, -0.015215508318, 0.016476395215, 0.128355383147, 0.008750586985, -0.049637455695, -0.049172415055, 0.964175225747, -0.0112017847, -0.219782142624);\n                        memcpy(dest != NULL ? dest : x, u, sizeof(float) * 2);\n                    }\n\n                protected:\n                    /**\n                    * 가변 길이 인수로 점곱 계산\n                    */\n                    float dot(float *x, ...) {\n                        va_list w;\n                        va_start(w, 9);\n                        static float mean[] = {10.998048780488, 7.26, 14.477073170732, 1.976829268293, 49.616341463415, 14.605853658537, 44.325609756098, 4.76243902439, 58.316585365854};\n                        float dot = 0.0;\n\n                        for (uint16_t i = 0; i \u003c 9; i++) {\n                            dot += (x[i] - mean[i]) * va_arg(w, double);\n                        }\n\n                        return dot;\n                    }\n                };\n            }\n        }\n    }\n```\n\n11— 템플릿을 .h 파일에 저장\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nwith open('./PCA/PCA.h', 'w') as file:\n    file.write(port(model))\n```\n\n12- 완성된 아두이노 스케치\n\n아래에 표시된 대로 아두이노 스케치에 \"PCA.h\" 파일을 포함시키세요:\n\n```js\n#include \"PCA.h\"\n\nEloquent::ML::Port::PCA pca;\n\nvoid setup() {\n    Serial.begin(115200);\n}\n\nvoid loop() {\n    float X_1[9] = {11.04,  7.58, 14.83,  2.07, 49.81, 14.69, 43.75,  5.02, 63.19};\n    float result_1[2];\n    pca.transform(X_1, result_1);\n    Serial.print(\"입력 X1로 예측한 결과:\");\n    for (int i = 0; i \u003c 2; i++) {\n        Serial.print(\" \");\n        Serial.print(result_1[i]);\n    }\n    Serial.println();  // 맨 끝에 새 줄 추가\n    delay(2000);\n\n    float X_2[9] = {10.76,  7.4 , 14.26,  1.86, 49.37, 14.05, 50.72,  4.92, 60.15};\n    float result_2[2];\n    pca.transform(X_2,  result_2);\n    Serial.print(\"입력 X2로 예측한 결과:\");\n    for (int i = 0; i \u003c 2; i++) {\n        Serial.print(\" \");\n        Serial.print(result_2[i]);\n    }\n    Serial.println();  // 맨 끝에 새 줄 추가\n    delay(2000);\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과:\n\n구성요소 X1: [ 4.65528953, -1.59196422]\n\n역변환 X1: [10.97474627, 7.27810093, 14.63589674, 1.98200161, 49.67703998, 14.66443304, 43.77240463, 4.76711978, 63.20194867]\n\n구성요소 X2: [ 3.12393184, 5.77720022]\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n역변환 된 X2: [10.87825406, 7.38493559, 15.46230692, 2.0402022, 49.3172806, 14.30855419, 50.55463117, 4.68889837, 60.09039224]\n\n![이미지](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_28.png)\n\n전체 프로젝트는 다음에서 확인할 수 있어요: [TinyML/07_principal_components_analysis at main · thommaskevin/TinyML](github.com)\n\n## 만약 마음에 드신다면, 제게 커피한 잔 ⚡️💰(Bitcoin)을 사주실래요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식으로 변경된 코드입니다:\n\n\ncode: bc1qzydjy4m9yhmjjrkgtrzhsgmkq79qenvcvc7qzn\n\n![image](/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_29.png)\n","ogImage":{"url":"/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_0.png"},"coverImage":"/assets/img/2024-06-20-TinyMLPrincipalComponentAnalysis_0.png","tag":["Tech"],"readingTime":17},{"title":"마이크로 머신러닝  합성곱 신경망 CNN","description":"","date":"2024-06-20 16:55","slug":"2024-06-20-TinyMLConvolutionalNeuralNetworksCNN","content":"\n\n수학적 기초부터 엣지 구현까지\n\n# 소셜 미디어:\n\n👨🏽‍💻 Github: thommaskevin/TinyML (github.com)\n👷🏾 Linkedin: Thommas Kevin | LinkedIn\n📽 Youtube: Thommas Kevin — YouTube\n👨🏻‍🏫 연구 그룹: Conecta.ai (ufrn.br)\n\n![이미지](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 요약\n\n# 1 — 컨볼루션 신경망 역사\n\n컨볼루션 신경망(CNN)의 개념은 1980년대 금사이 후코시마의 연구로 형성되기 시작했습니다. 그는 Neocognitron을 개발했는데, 이는 동물의 시각 시스템 구조에서 영감을 받아 자가 조직화 과정을 통해 시각적 패턴을 인식할 수 있는 학습이 가능한 계층적 구조를 가졌습니다. 이 작업은 현대 CNN의 개발을 위한 중요한 선행 연구였습니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_1.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCNN의 현대 아키텍처는 얀 르쿤과 그의 동료들에 의해 1980년대 후반과 1990년대 초에 제안되었습니다. 그들은 MNIST 데이터셋에서 손으로 쓴 숫자를 인식하기 위해 설계된 컨볼루션 신경망인 LeNet-5를 개발했습니다. LeNet-5는 여러 개의 컨볼루션 레이어를 거친 후 pooling 레이어와 완전 연결 레이어로 이어지는 구조로, 오늘날 사용되는 CNN의 아키텍처의 기초를 구축했습니다.\n\n![image](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_2.png)\n\n초기 성공에도 불구하고 CNN의 활용은 계산 제약과 대규모 레이블 데이터셋의 부족으로 제한되었습니다. 그러나 컴퓨팅 파워가 증가하고 GPU(그래픽 처리 장치)를 사용해 딥 네트워크를 훈련하는 기술이 실현 가능해지면서 CNN은 더 많은 관심을 끌게 되었습니다. 게다가 ImageNet과 같은 대규모 레이블 이미지 데이터베이스의 개발은 딥 네트워크를 효과적으로 훈련하기 위한 필수 자료를 제공했습니다.\n\n2012년에 Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton이 개발한 AlexNet이 ImageNet 대규모 시각 인식 챌린지(ILSVRC)에서 경쟁 상대들보다 큰 폭으로 우승하면서 전환점이 찾아왔습니다. AlexNet은 여러 컨볼루션 레이어, ReLU 활성화 함수, 그리고 dropout과 같은 정규화 기법을 활용하여 시각 인식 작업에 CNN의 성능을 입증했습니다. 이후로 CNN은 다양한 응용 분야에 대한 주요 도구가 되어 지속적인 혁신을 이끌어내는 데 이르렀습니다. VGGNet, GoogLeNet(Inception), ResNet과 같은 모델들은 네트워크의 깊이, 효율성, 정확도를 향상시키기 위한 새로운 아키텍처와 기법들을 소개했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_3.png\" /\u003e\n\n지금은 CNN이 많은 인공지능 시스템의 필수 구성 요소입니다. 이미지 인식 뿐만 아니라 비디오 분석, 자연어 처리, 의학 진단, 자율 주행차 등 다양한 분야에서 사용됩니다. 연구는 계속해서 발전하고 있으며, 효율적인 합성곱 신경망, 깊은 신경망(DNNs), 생성적 적대 신경망(GANs) 등의 혁신이 이루어지고 있습니다.\n\n# 2— 합성곱 신경망 이론\n\n수학에서 \"합성곱\"은 한 함수가 다른 함수에 의해 변환되는 통합 연산을 나타냅니다. 그러나 신경망의 맥락에서는 이 개념이 전통적인 통계적 해석과 다릅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기본적으로 우리는 입력 함수로 시작합니다. 우리의 경우에는 주로 이미지입니다. 또한, 필터(커널이라고도 함)를 소개합니다. 이미지는 점곱 연산을 통해 변환되는 함수로, 일반적으로 \"합성곱(convolution)\"이라고 합니다. 그 다음, 이러한 필터를 입력 이미지에 적용하면 \"특성 맵(feature maps)\"이라고 하는 출력 이미지를 얻게 됩니다.\n\n## 2.1 — 합성곱 계층\n\n합성곱 계층은 패턴을 감지하고 필터(커널)를 통해 특성 맵을 생성하기 위해 이미지가 처리되는 곳입니다. 이러한 특성 맵은 필터가 식별하려는 각 속성을 나타냅니다. 필터는 일반적으로 (3x3) 또는 (5x5) 행렬로 구성되어 있으며, 각 필터는 입력 이미지에서 동일한 크기의 필드를 차지합니다. 그런 다음, 필터가 한 칸씩 가로로 이동하고 같은 프로세스가 반복됩니다. 가로 끝에 도달하면 필터가 한 칸 아래로 이동하고 점곱 프로세스가 다시 수평으로 적용됩니다. 그 결과는 순서대로 출력에 추가되어 특성 맵을 생성합니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/0*4UZXfXs7eQ3TT02M.gif)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n입력 이미지를 나타내는 텐서 I의 차원이 m1 x m2 x mc인 경우를 가정해 봅시다. 이 텐서에서,\n\n![image1](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_4.png)\n\n우리는 입력 이미지와 일치하는 채널 수를 가진 (n1 x n2 x nc) 차원을 가지는 필터를 적용합니다. 이 필터는 이미지를 왼쪽에서 오른쪽으로 움직이면서, 입력 텐서 I의 해당 영역과 요소별 곱셈을 수행하고 이러한 곱셈 결과를 합산합니다. 스트라이드 매개변수는 필터가 이미지를 횡단하는 단계 크기를 결정합니다. I와 K 사이의 이 작업의 결과는 다른 차원 (m1 - n1 + 1) x (m2 - n2 + 1) x 1을 가진 또 다른 텐서를 생성합니다.\n\n![image2](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n안녕하세요,\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_6.png\" /\u003e\n\n특징 맵의 (i, j)번째 항목은 다음과 같이 계산됩니다:\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_7.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 예를 선택했습니다. 5x5x1 차원 이미지가 3x3x1 커널로 합성되고 s=1 스트라이드가 적용됩니다.\n\n![image](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_8.png)\n\n특징 맵의 (i, j)-번째 항목은 단일 채널에 대한 다음 일반 공식으로 주어집니다:\n\n![formula](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_9.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 예에서 feature map의 (1, 1)번 째 항목을 계산해 봅시다:\n\n![image](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_10.png)\n\n![image](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_11.png)\n\n사용할 수 없는 항목은 0으로 대체되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_12.png)\n\n마찬가지로, 남은 항목들은 동일한 공식을 사용하여 계산할 수 있습니다. 이 과정은 서로 다른 유형의 필터를 적용함으로써 반복되며, 각각이 이미지의 다른 특징을 캡처하는 것을 보여줍니다. 예를 들어, 필터 개수가 하나 이상이 될 수 있어서 스트라이드 개념이 도입됩니다.\n\n## 2.2— 패딩 레이어\n\n기본 CNN은 (n x n) 크기의 흑백 이미지와 (f x f) 크기의 필터/커널을 사용하여 결과를 제공하며, 출력 크기는 (n - f + 1) x (n - f + 1)이 됩니다. 예를 들어, (8 x 8) 이미지와 (3 x 3) 필터를 사용한 어떤 합성 곱 작업의 경우, 출력 이미지 크기는 (6 x 6)이 됩니다. 이러한 크기의 감소는 이미지 처리 중 일관적으로 발생하며, 레이어의 출력이 일반적으로 입력보다 작습니다. 또한, 합성 곱 작업에서 사용되는 필터는 픽셀을 횡단하면서 항상 모서리에 초점을 두지 않습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![TinyML CNN](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_13.png)\n\n머신 러닝에서 일반적으로 사용되는 여러 유형의 패딩이 있습니다:\n\n- Same 패딩: Same 패딩은 원본 이미지의 외부 프레임에 일반적으로 0과 같은 추가 요소를 추가하는 것을 포함합니다. 이 방식으로 입력을 확장함으로써, 필터가 더 넓은 영역을 스캔할 수 있게 되어 출력 이미지가 원본과 동일한 크기를 유지할 수 있습니다. 이것은 컨볼루션 작업 중 공간적 차원을 유지하는 데 유용합니다.\n- Valid 패딩: Same 패딩과는 달리, Valid 패딩은 이미지에 추가 요소를 추가하는 것을 포함하지 않습니다. 필터는 추가 요소 없이 원본 이미지를 훑어갑니다. 이것은 간격으로 인한 일부 데이터 손실이 발생할 수 있지만, valid 패딩은 출력 피쳐 맵의 크기를 줄이고자 할 때 사용됩니다. 이러한 축소는 모델의 매개변수 수를 줄이고 계산 효율성을 향상시킬 수 있습니다.\n- Causal 패딩: Causal 패딩은 주로 시퀀스-투-시퀀스 모델 및 시계열 예측에 사용되며, 특히 1차원 컨볼루션 레이어에서 사용됩니다. 이 유형의 패딩은 데이터 시퀀스의 시작 부분에 요소를 추가하여 알고리즘이 초기 시간 단계에 대한 값을 예측할 수 있게 합니다. 과거 및 현재 데이터를 예측에 포함시킴으로써, causal 패딩은 모델이 추론 중에 사용할 수 없는 미래 데이터를 활용하지 않도록 보장합니다.\n- Full 패딩: 이 유형의 패딩은 입력의 테두리 주위에 여러 레이어의 0을 추가하여 원본 이미지 크기보다 큰 출력 피쳐 맵을 생성합니다. Full 패딩은 덜 일반적이지만 더 큰 출력 크기가 필요한 특정 시나리오에서 사용할 수 있습니다.\n\n## 2.3 —Pooling Layer\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n풀링 레이어에서는 컨볼루션된 특징의 공간 차원이 일반적으로 축소되어 입력 이미지에서 주요한 특징을 추출하는 데 도움이 됩니다. 이 크기의 축소는 컨볼루션 레이어에서 얻은 출력에 풀링 함수를 적용하여 달성됩니다. 이렇게 가정해 봅시다:\n\n![그림 1](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_14.png)\n\n풀링된 부분의 차원은 다음과 같습니다:\n\n![그림 2](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_15.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n딥러닝에서는 3가지 종류의 풀링이 있어요:\n\n평균 풀링: 커버된 영역 내 픽셀 값의 평균이 출력 매트릭스로 전달됩니다.\n\n최대 풀링: 커버된 영역 내 픽셀 값 중 가장 높은 값이 출력 매트릭스로 전달됩니다.\n\n![이미지](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_16.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n전역 최대 풀링(Global Max Pooling) : 모든 입력 크기의 픽셀 값 중 가장 높은 값이 출력 행렬로 전달됩니다. 이 유형의 풀링에서 풀 크기는 입력 크기와 동일합니다.\n\n![이미지](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_17.png)\n\nsum pooling, average pooling, max pooling과 같은 다양한 유형의 풀링이 있습니다. 최대 풀링의 예는 아래에 제공됩니다. 최대 풀링은 2x2 패치에 수행됩니다. 각 패치에서 최댓값이 선택됩니다.\n\n## 2.4 — 플래튼 레이어\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n플래튼 레이어는 신경망 아키텍처에서 중요한 구성 요소이며, 특히 합성곱 레이어에서 완전히 연결된 레이어로의 전환 시에 중요합니다. 이 레이어는 합성곱 및 풀링 레이어에서 생성된 다차원 피쳐 맵을 일차원 벡터로 변환하여, 분류 또는 회귀 작업을 위해 후속 완전히 연결된 레이어로 전달할 수 있습니다.\n\n![이미지 설명](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_18.png)\n\n다음은 플래튼 레이어의 작동 방식입니다:\n\n- 입력: 플래튼 레이어의 입력은 일반적으로 이전 합성곱 또는 풀링 레이어에서 생성된 피쳐 맵을 나타내는 다차원 텐서입니다. 예를 들어, 마지막 합성곱 또는 풀링 레이어가 높이, 너비, 깊이의 피쳐 맵을 생성한다면 입력 텐서는 (배치 크기, 높이, 너비, 깊이) 형태를 가질 것입니다.\n- 플래팅: 플래팅 레이어는 단순히 피쳐 맵의 모든 요소를 하나의 차원을 따라 연결하여 입력 텐서를 일차원 벡터로 다시 형태화합니다. 예를 들어, 피쳐 맵이 높이, 너비, 깊이의 차원을 가진다면 플래팅 레이어는 이를 높이 * 너비 * 깊이의 길이를 가진 벡터로 변환합니다.\n- 출력: 플래팅 레이어의 출력은 피쳐 맵을 플래팅한 일차원 벡터입니다. 이 벡터는 이후의 완전히 연결된 레이어의 입력으로 전달될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n평탄화 레이어의 목적은 특성 맵에서 캡처된 공간 정보를 완전 연결 레이어에서 처리할 수 있는 형식으로 변환하는 것입니다. 완전 연결 레이어는 일차원 입력 벡터를 필요로하므로 특성 맵을 평탄화함으로써 신경망이 데이터의 다양한 공간 위치에 걸쳐 복잡한 패턴과 관계를 효과적으로 학습할 수 있습니다. 이를 통해 더 정확한 예측을 할 수 있습니다.\n\n이전의 합성곱 또는 풀링 레이어에 의해 생성된 특성 맵 𝐹 집합이 있다고 가정해봅시다. 이러한 특성 맵의 차원을 다음과 같이 표기해 봅시다:\n\n- 𝐻: 특성 맵의 높이\n- 𝑊: 특성 맵의 너비\n- 𝐷: 특성 맵의 깊이 (채널 수)\n- 𝐵: 배치 크기 (배치에 포함된 샘플 수)\n\n그러면 특성 맵 𝐹의 형태는 (B,H,W,D)가 됩니다. 여기서 𝐵은 배치 크기를 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 피처 맵을 일차원 벡터로 평탄화하기 위해, 단순히 이를 길이 𝐻×𝑊×𝐷의 벡터로 재구성합니다. 수학적으로 표현하면 다음과 같습니다:\n\nFlatten(𝐹)=reshape(𝐹, (𝐵, 𝐻×𝑊×𝐷))\n\n이 경우, 재구성 연산은 (𝐵, 𝐻, 𝑊, 𝐷) 텐서를 (𝐵, 𝐻×𝑊×𝐷) 텐서로 재구성하여 공간 차원을 하나의 차원으로 펼치게 됩니다.\n\n예를 들어, 만약 𝐹가 차원이 (4,5,5,3)인 경우 (배치 크기가 4, 높이가 5, 너비가 5, 깊이가 3인 피처 맵), 그러면 평탄화된 출력은 (4,75) 차원을 갖게 되며, 각 행은 배치의 한 샘플에 대한 평탄화된 피처 맵을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 평탄화된 벡터는 신경망의 후속 완전 연결 레이어에 입력으로 전달될 수 있습니다.\n\n# 3 — TinyML 구현\n\n이 예제를 통해 ESP32, 아두이노, 라즈베리 파이 및 기타 다양한 마이크로컨트롤러 또는 IoT 장치에서 머신러닝 알고리즘을 구현할 수 있습니다.\n\n3.0 — requirements.txt 파일에 나열된 라이브러리를 설치합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\n!pip install -r requirements.txt\n```\n\n3.1 — 라이브러리 가져오기\n\n```python\nimport numpy as np\nfrom sklearn.datasets import load_digits\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\nimport time\nimport seaborn as sns\nimport os\n```\n\n3.2 — 데이터셋 불러오기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMNIST은 Modified National Institute of Standards and Technology database의 줄임말로, 기공 학습 및 컴퓨터 비전 분야에서 널리 사용되는 데이터셋입니다. 이 데이터셋은 0에서 9까지의 손으로 쓴 숫자들의 모음으로, 각 숫자는 28x28 픽셀 크기의 회색 음영 이미지로 표현됩니다. 이 데이터셋에는 총 70,000개의 이미지가 포함되어 있으며, 이 중 60,000개의 이미지는 훈련에 사용되고 10,000개의 이미지는 테스트에 사용됩니다.\n\n링크: [https://www.nist.gov/itl/products-and-services/emnist-dataset](https://www.nist.gov/itl/products-and-services/emnist-dataset)\n\n```python\ndef get_data():\n    np.random.seed(1337)\n    x_values, y_values = load_digits(return_X_y=True)\n    x_values /= x_values.max()\n    # reshape to (8 x 8 x 1)\n    x_values = x_values.reshape((len(x_values), 8, 8, 1))\n    # split into train, validation, test\n    TRAIN_SPLIT = int(0.6 * len(x_values))\n    TEST_SPLIT = int(0.2 * len(x_values) + TRAIN_SPLIT)\n    x_train, x_test, x_validate = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])\n    y_train, y_test, y_validate = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])\n\n    return x_train, x_test, x_validate, y_train, y_test, y_validate\n```\n\n3.3 — 데이터 분할\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nX_train, X_test, X_validate, y_train, y_test, y_validate = get_data()\n```\n\n3.4 — 탐색적 데이터 분석\n\n```js\nX_train__ = X_train.reshape(X_train.shape[0], 8, 8)\n\nfig, axis = plt.subplots(1, 4, figsize=(20, 10))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(X_train__[i], cmap='binary')\n    digit = y_train[i]\n    ax.set(title = f\"실제 숫자는 {digit}입니다.\")\n```\n\n![이미지](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_19.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3.5— 모델 정의하기\n\n```js\nmodel = tf.keras.Sequential()\nmodel.add(layers.Conv2D(8, (3, 3), activation='relu', input_shape=(8, 8, 1)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(len(np.unique(y_train))))\n```\n\n```js\nmodel.summary()\n```\n\n![이미지](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_20.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nplot_model(model, to_file='./figures/model.png')\n```\n\n![Plot](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_21.png)\n\n3.6—모델 컴파일하기\n\n```js\nmodel.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3.7 — 모델 훈련\n\n```js\nhistory = model.fit(X_train, y_train,\n                    epochs=50,\n                    batch_size=16,\n                    validation_data=(X_validate, y_validate))\n```\n\n```js\nmodel.save('.\\models\\model.keras')\n```\n\n```js\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'r.', label='훈련 손실')\nplt.plot(epochs, val_loss, 'y', label='검증 손실')\nplt.title('훈련 및 검증 손실')\nplt.xlabel('에포크')\nplt.ylabel('손실')\nplt.grid()\nplt.legend()\nplt.savefig('.\\\\figures\\\\history_traing.png', dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_22.png\" /\u003e\n\n모델 평가\n\n테스트 데이터\n\n```js\ndef test_model(model, x_test, y_test):\n    x_test = (x_test / x_test.max()).reshape((len(x_test), 8, 8, 1))\n    y_pred = model.predict(x_test).argmax(axis=1)\n    print('정확도', ((y_pred == y_test).sum() / len(y_test))*100, \"%\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\ntest_model(model, X_test, y_test)\n```\n\n![Image](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_23.png)\n\n3.8.2 — Confusion matrix\n\n```js\nfig = plt.figure(figsize=(10, 10)) # Set Figure\n\ny_pred = model.predict(X_test) # Predict class probabilities as 2 =\u003e [0.1, 0, 0.9, 0, 0, 0, 0, 0, 0, 0]\nY_pred = np.argmax(y_pred, 1) # Decode Predicted labels\nmat = confusion_matrix(y_test, Y_pred) # Confusion matrix\n\n# Plot Confusion matrix\nsns.heatmap(mat.T, square=True, annot=True, cbar=False, cmap=plt.cm.Blues, fmt='.0f', \n            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test), \n            annot_kws={\"fontsize\": 14}, linewidths=1, linecolor='white')\n\nplt.xlabel('Predicted Values', fontsize=14)\nplt.ylabel('True Values', fontsize=14)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.savefig('.\\\\figures\\\\confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_24.png)\n\n3.8.3— 예측 유효성 검사 결과\n\n```js\ny_pred = model.predict(X_test)\nX_test__ = X_test\n\nfig, axis = plt.subplots(4, 4, figsize=(12, 14))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(X_test__[i], cmap='binary')\n    ax.set(title = f\"실제 숫자: {y_test[i]}\\n예측 숫자: {y_pred[i].argmax()}\")\n```\n\n![이미지](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_25.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3.9 — 마이크로컨트롤러에 구현할 모델을 얻기\n\n3.9.1 — C 프로그래밍을 위해 일부 16진수 값을 배열로 변환하기\n\n```js\n# 함수: C 프로그래밍을 위해 일부 16진수 값을 배열로 변환\ndef hex_to_c_array(hex_data, var_name):\n\n  c_str = ''\n\n  # 헤더 가드 생성\n  c_str += '#ifdef __has_attribute\\n'\n  c_str += '#define HAVE_ATTRIBUTE(x) __has_attribute(x)\\n'\n  c_str += '#else\\n'\n  c_str += '#define HAVE_ATTRIBUTE(x) 0\\n'\n  c_str += '#endif\\n'\n  c_str += '#if HAVE_ATTRIBUTE(aligned) || (defined(__GNUC__) \u0026\u0026 !defined(__clang__))\\n'\n  c_str += '#define DATA_ALIGN_ATTRIBUTE __attribute__((aligned(4)))\\n'\n  c_str += '#else\\n'\n  c_str += '#define DATA_ALIGN_ATTRIBUTE\\n'\n  c_str += '#endif\\n\\n'\n\n  # C 변수 선언\n  c_str += 'const unsigned char ' + var_name + '[]  DATA_ALIGN_ATTRIBUTE = {'\n  hex_array = []\n  for i, val in enumerate(hex_data) :\n\n    # 16진수에서 문자열로 변환\n    hex_str = format(val, '#04x')\n\n    # 각 줄이 80자 이내로 유지되도록 서식 지정 추가\n    if (i + 1) \u003c len(hex_data):\n      hex_str += ','\n    if (i + 1) % 12 == 0:\n      hex_str += '\\n '\n    hex_array.append(hex_str)\n\n  # 마지막 중괄호 추가\n  c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n\n  # 헤더 가드 종료\n  c_str += 'const int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n\n  return c_str\n```\n\n3.9.2—모델을 Float32와 Int8형식으로 변환하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef representative_dataset():\n    for i in range(len(X_train)):\n        input_data = np.array([X_train[i]], dtype=np.float32)\n        yield [input_data]\n\ndef converter_quantization_model(model, model_name):\n\n    # Convert the model to float32\n    converter_float32 = tf.lite.TFLiteConverter.from_keras_model(model)\n    converter_float32.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter_float32.target_spec.supported_types = [tf.float32]\n    converter_float32._experimental_lower_tensor_list_ops = False\n    converter_float32.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n    converter_float32.representative_dataset = representative_dataset\n    tflite_model_float32 = converter_float32.convert()\n    print(tflite_model_float32)\n    with open(model_name+'_quant_float32' + '.h', 'w') as file:\n        file.write(hex_to_c_array(tflite_model_float32, model_name+'_quant_float32'))\n    with open(model_name+'_quant_float32.tflite', 'wb') as f:\n        f.write(tflite_model_float32)\n    size_model_tflite_float32 = os.path.getsize(model_name+'_quant_float32.tflite')\n    print(model_name+f'_quant_float32.tflite: {size_model_tflite_float32} Bytes')\n\n    # Convert the model to Int8\n    converter_int8 = tf.lite.TFLiteConverter.from_keras_model(model)\n    converter_int8.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter_int8.target_spec.supported_types = [tf.int8]\n    converter_int8.representative_dataset = representative_dataset\n    converter_int8.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n        tf.lite.OpsSet.SELECT_TF_OPS,\n    ]\n    converter_int8.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n    converter_int8.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n    converter_int8.experimental_new_converter = True\n    converter_int8.experimental_new_quantizer = True\n    converter_int8.experimental_new_calibrator = True\n    tflite_model_int8 = converter_int8.convert()\n    with open(model_name+'_quant_int8' + '.h', 'w') as file:\n        file.write(hex_to_c_array(tflite_model_int8, model_name+'_quant_int8'))\n    with open(model_name+'_quant_int8.tflite', 'wb') as f:\n        f.write(tflite_model_int8)\n    size_model_tflite_int8 = os.path.getsize(model_name+'_quant_int8.tflite')\n    print(model_name+f'_quant_int8.tflite: {size_model_tflite_int8} Bytes')\n\n    return None\n```\n\n```js\nmodel_name='.\\models\\model'\nconverter_quantization_model(model, model_name)\n```\n\n3.10 — Quantized Model Evaluation\n\n```js\ndef evaluate_quantization(model_path, X_test, y_test, quantization_type):\n    interpreter = tf.lite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # Evaluate the quantized model\n    input_index = interpreter.get_input_details()[0]['index']\n    output_index = interpreter.get_output_details()[0]['index']\n    predictions = []\n    processing_times = []\n\n    X_test = np.array(X_test, dtype=np.float32)\n    \n    for X in X_test:\n        interpreter.set_tensor(input_index, [X])       \n        start_time = time.time()\n        interpreter.invoke()\n        end_time = time.time()\n        processing_time = end_time - start_time\n        processing_times.append(processing_time)\n        output = interpreter.get_tensor(output_index).argmax(axis=1)\n        predictions.append(output[0])\n\n    acc = accuracy_score(y_test, predictions)\n   \n    # Calculate the average and standard deviation of differences\n    result = { \"Accuracy (%): \":acc*100,\n                \"Process time (s): \": np.mean(processing_times)\n            }\n\n    return result\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nmodel_name = '.\\models\\model'\n```\n\n```js\neval_quant_float32 = evaluate_quantization(model_name + '_quant_float32.tflite', X_test, y_test, 'float32')\neval_quant_float32\n```\n\n![Image](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_26.png)\n\n```js\neval_quant_int8 = evaluate_quantization(model_name + '_quant_int8.tflite', X_test, y_test, 'int8')\neval_quant_int8 \n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_27.png\" /\u003e\n\n## 3.11 — 모델 배포\n\n이 예제를 통해 ESP32, 아두이노, 아두이노 Portenta H7 with Vision Shield, 라즈베리파이 및 기타 다양한 마이크로컨트롤러 또는 IoT 장치에 머신러닝 알고리즘을 구현할 수 있습니다.\n\n3.11.1 — EloquentTinyML 라이브러리 설치\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n도서관 폴더로 이동하여 EloquentTinyML-main을 설치해주세요.\n\n3.11.2 — 완전한 아두이노 스케치\n\nmodel_quant_float32.h 또는 model_quant_int8.h 파일을 열어서 다음에서 모든 16진수 값을 복사하세요:\n\n![이미지](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_28.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같이 변경해주세요:\n\n\nand model len\n\n![image](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_29.png)\n\nand cut in model.h:\n\n![image](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_30.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nand\n\n![image](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_31.png)\n\n3.11.2 — 완성된 아두이노 스케치\n\n```js\n#include \u003cEloquentTinyML.h\u003e\n#include \u003celoquent_tinyml/tensorflow.h\u003e\n\n// sine_model.h contains the array you exported from Python with xxd or tinymlgen\n#include \"model.h\"\n\n#define N_INPUTS 64\n#define N_OUTPUTS 10\n// in future projects you may need to tweak this value: it's a trial and error process\n#define TENSOR_ARENA_SIZE 6*1024\n\nEloquent::TinyML::TensorFlow::TensorFlow\u003cN_INPUTS, N_OUTPUTS, TENSOR_ARENA_SIZE\u003e tf;\n\nfloat input[64] = {0.00000000000f, 0.12500000000f, 0.00000000000f, 0.50000000000f, 0.56250000000f, 0.00000000000f, 0.00000000000f, 0.00000000000f, 0.00000000000f, 0.81250000000f, 0.31250000000f, 0.87500000000f, 0.50000000000f, 0.43750000000f, 0.00000000000f, 0.00000000000f, 0.00000000000f, 0.75000000000f, 0.31250000000f, 0.12500000000f, 0.00000000000f, 0.56250000000f, 0.00000000000f, 0.00000000000f, 0.00000000000f, 0.43750000000f, 0.31250000000f, 0.00000000000f, 0.00000000000f, 0.18750000000f, 0.31250000000f, 0.00000000000f, 0.00000000000f, 0.18750000000f, 0.62500000000f, 0.00000000000f, 0.00000000000f, 0.12500000000f, 0.62500000000f, 0.00000000000f, 0.00000000000f, 0.06250000000f, 0.81250000000f, 0.00000000000f, 0.00000000000f, 0.06250000000f, 0.75000000000f, 0.00000000000f, 0.00000000000f, 0.00000000000f, 0.31250000000f, 0.81250000000f, 0.31250000000f, 0.56250000000f, 0.81250000000f, 0.00000000000f, 0.00000000000f, 0.00000000000f, 0.00000000000f, 0.56250000000f, 1.00000000000f, 1.00000000000f, 0.43750000000f, 0.00000000000f};\n\nfloat y_pred[10] = {0};\n\nvoid setup() {\n    Serial.begin(9600);\n    delay(4000);\n    tf.begin(model);\n\n    // check if model loaded fine\n    if (!tf.isOk()) {\n      Serial.print(\"ERROR: \");\n      Serial.println(tf.getErrorMessage());\n\n      while (true) delay(1000);\n    }\n}\n\nvoid loop() {\n\n        tf.predict(input, y_pred);\n        for (int i = 0; i \u003c 10; i++) {\n            Serial.print(y_pred[i]);\n            Serial.print(i == 9 ? '\\n' : ',');\n        }\n    Serial.print(\"Predicted class is: \");\n      Serial.println(tf.probaToClass(y_pred));\n      // or you can skip the predict() method and call directly predictClass()\n      Serial.print(\"Sanity check: \");\n      Serial.println(tf.predictClass(input));\n      delay(2000);\n\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3.12 — 결과\n\n3.12.1 — 양자화된 모델 Float32\n\n![이미지](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_32.png)\n\n3.12.1 — 양자화된 모델 Int8\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_33.png)\n\nFull project in: [TinyML/13_CNN at main · thommaskevin/TinyML](https://github.com/thommaskevin/TinyML)\n\n## If you like it, consider buying my coffee ☕️💰 (Bitcoin)\n\ncode: bc1qzydjy4m9yhmjjrkgtrzhsgmkq79qenvcvc7qzn\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`\u003cimg src=\"/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_34.png\" /\u003e`","ogImage":{"url":"/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_0.png"},"coverImage":"/assets/img/2024-06-20-TinyMLConvolutionalNeuralNetworksCNN_0.png","tag":["Tech"],"readingTime":23},{"title":"시리얼 통신 마스터하기 아두이노 메가 2560 마스터와 우노 슬레이브 UART 튜토리얼","description":"","date":"2024-06-20 16:52","slug":"2024-06-20-MasteringSerialCommunicationArduinoMega2560MasterandUnoSlaveUARTTutorial","content":"\n\n\n![이미지](/assets/img/2024-06-20-MasteringSerialCommunicationArduinoMega2560MasterandUnoSlaveUARTTutorial_0.png)\n\n# 소개\n\n아두이노 프로젝트의 영역에서 시리얼 통신은 기본적인 기술입니다. 이것은 새로운 언어로 대화하는 것을 배우는 것과 같습니다. 하지만 단어 대신에 전기 신호를 사용합니다. 이 튜토리얼에서는 두 가지 인기 있는 아두이노 보드인 메가 2560을 마스터로, 그리고 우노를 슬레이브로하여 UART (Universal Asynchronous Receiver-Transmitter) 통신을 설정하는 방법을 알아볼 것입니다. 이 안내서를 끝까지 따라오면 이 두 보드가 서로 '대화'할 수 있는 방법에 대해 명확히 이해하게 될 것입니다.\n\n# 필요한 것\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 1 x 아두이노 메가 2560\n- 1 x 아두이노 우노\n- 점퍼 와이어\n- 아두이노 IDE\n\n# 마스터 이해하기: 아두이노 메가 2560\n\n# 설정\n\n시리얼 포트 초기화: 우리의 메가 2560은 두 개의 시리얼 포트를 사용합니다. 디버깅용으로 Serial을 사용하고 Uno와 통신하기 위해 Serial1을 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nSerial.begin(9600);\nSerial1.begin(9600);\n```\n\nLED로 표시기 사용: 내장 LED를 사용하여 받은 데이터를 시각적으로 표현합니다.\n\n```js\npinMode(LED_BUILTIN, OUTPUT);\ndigitalWrite(LED_BUILTIN, HIGH);\n```\n\n# 루프\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 수신: Mega는 시리얼 포트에서 데이터를 기다립니다.\n\n```js\nif (Serial.available() \u003e 0) {\n   char received = Serial.read();\n   ...\n}\n```\n\n데이터 전달: 데이터를 수신하면 Mega는 동일한 데이터를 Serial1을 통해 Uno로 전달합니다.\n\n```js\nSerial1.write(received);\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터에 따른 조치: 받은 데이터가 '1'이면 LED가 꺼지고, '2'이면 켜집니다.\n\n```js\nif (received == '1') {\n   digitalWrite(LED_BUILTIN, LOW);\n} else if (received == '2') {\n   digitalWrite(LED_BUILTIN, HIGH);\n}\n```\n\n# 슬레이브 이해하기: 아두이노 우노\n\n# 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n소프트웨어 시리얼: Uno는 하드웨어 시리얼 포트가 하나뿐이기 때문에, 우리는 SoftwareSerial을 사용하여 가상 시리얼 포트를 생성합니다.\n\n```js\nSoftwareSerial Serial1(10, 9); // RX, TX\n```\n\n시리얼 포트 초기화: 하드웨어 및 소프트웨어 시리얼 포트가 모두 초기화됩니다.\n\n```js\nSerial.begin(9600);\nSerial1.begin(9600);\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 루프\n\n데이터 수신: Uno는 Mega로부터 소프트웨어 시리얼 포트에서 데이터를 수신합니다.\n\n```js\nif (Serial1.available() \u003e 0) {\n   char received = Serial1.read();\n   ...\n}\n```\n\n데이터 응답: Mega와 유사하게, Uno는 수신한 데이터를 사용하여 LED를 제어합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\nif (received == '1') {\n   digitalWrite(LED_BUILTIN, LOW);\n} else if (received == '2') {\n   digitalWrite(LED_BUILTIN, HIGH);\n}\n```\n\n# 함께 모두 넣어봅시다\n\n# 아두이노 메가 2560 (마스터)\n\n```js\n#include \u003cSoftwareSerial.h\u003e\n\nvoid setup() {\n    pinMode(LED_BUILTIN, OUTPUT);\n    digitalWrite(LED_BUILTIN, HIGH);\n    Serial.begin(9600);\n    Serial1.begin(9600);\n}\n\nvoid loop() {\n    if (Serial.available() \u003e 0) {\n        char received = Serial.read();\n        Serial1.write(received);\n\n        if (received == '1') {\n            digitalWrite(LED_BUILTIN, LOW);\n        } else if (received == '2') {\n            digitalWrite(LED_BUILTIN, HIGH);\n        }\n    }\n}\n```\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 아두이노 Uno (슬레이브)\n\n```js\n#include \u003cSoftwareSerial.h\u003e\n\nSoftwareSerial Serial1(10, 9); // RX, TX\n\nvoid setup() {\n    pinMode(LED_BUILTIN, OUTPUT);\n    digitalWrite(LED_BUILTIN, HIGH);\n    Serial.begin(9600);\n    Serial1.begin(9600);\n}\n\nvoid loop() {\n    if (Serial1.available() \u003e 0) {\n        char received = Serial1.read();\n\n        if (received == '1') {\n            digitalWrite(LED_BUILTIN, LOW);\n        } else if (received == '2') {\n            digitalWrite(LED_BUILTIN, HIGH);\n        }\n    }\n}\n```\n\n# 배선\n\n5V to 5V 연결:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 아두이노 메가의 5V 핀을 아두이노 우노의 5V 핀에 연결하십시오. 이 단계에서 우노를 메가로부터 전원을 공급합니다.\n\n그라운드 연결:\n\n- 아두이노 메가의 GND(그라운드) 핀을 아두이노 우노의 GND 핀에 연결하십시오. 이 공통 그라운드는 올바른 통신을 위해 중요합니다.\n\nTX에서 RX로 연결:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 아두이노 메가의 TX1 (송신) 핀을 아두이노 우노의 10번 핀에 연결하세요. 코드에서 우노의 SoftwareSerial에서 10번 핀을 RX (수신) 핀으로 설정해주세요.\n\n컴퓨터 연결 및 시리얼 모니터:\n\n- USB 케이블을 사용하여 아두이노 메가를 컴퓨터에 연결하세요.\n- 아두이노 IDE를 열고 메가에 대한 올바른 COM 포트를 선택하세요.\n- 아두이노 IDE에서 시리얼 모니터를 열어주세요. 이것은 메가에서 우노로 데이터를 보내는 데 사용될 것입니다.\n\n이제 실험을 위한 설정이 완료되었습니다. 아두이노 IDE의 시리얼 모니터에 '1' 또는 '2'를 입력하면 (메가를 선택한 상태에서), 메가는 TX1을 통해 10번 핀에 데이터를 보내고, 우노는 수신된 데이터에 따라 LED를 켜거나 끌 것입니다.","ogImage":{"url":"/assets/img/2024-06-20-MasteringSerialCommunicationArduinoMega2560MasterandUnoSlaveUARTTutorial_0.png"},"coverImage":"/assets/img/2024-06-20-MasteringSerialCommunicationArduinoMega2560MasterandUnoSlaveUARTTutorial_0.png","tag":["Tech"],"readingTime":4},{"title":"컴퓨터에서 RC 모델을 프로그래밍하여 제어하기","description":"","date":"2024-06-20 16:51","slug":"2024-06-20-ControlanyRCmodelprogrammaticallyfromcomputer","content":"\n\n유니버설 RC 컨트롤러와 멀티 모듈을 사용하여 만들기\n\n# RC 모델 제어를 위한 비침입적 방식\n\n\"자율주행 미니-Z와 함께 경주하는\" 프로젝트의 일환으로, 첫 번째 작업은 컴퓨터에서 미니-Z를 제어하는 방법을 찾는 것입니다. 이를 달성하기 위한 여러 가지 방법이 있습니다. 예를 들어, RC 차량에 마이크로컨트롤러를 추가하거나 무선 송신기를 수정하여 제어 신호를 프로그램 할 수 있습니다. 그러나 저는 제 RC 모델을 수정하지 않는 비침입적인 방법을 선호합니다. 따라서 컴퓨터가 라디오 송신기를 모방하고 기본 Mini-Z를 제어할 수 있는 솔루션을 찾고 있습니다.\n\n이 방식을 택한 이유는 몇 가지가 있습니다. 첫째, Mini-Z는 제 취미이며 아직도 즐겁게 놀고 싶어서 손상을 입히고 싶지 않습니다.: ) 게다가, 제 프로젝트의 목표는 사람과 경주하는 것이기 때문에, 자동차가 일반 플레이어의 자동차와 동일한 구성을 유지하여 공정한 경기를 보장하고 싶습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 컴퓨터에서 라디오 신호 전송하기\n\n내 첫 번째 아이디어는 트레이너 포트를 통해 컴퓨터를 라디오 송신기에 연결할 수 있는지 알아보는 것입니다. 라디오 송신기의 트레이너 포트는 주로 모델용 원격 제어(RC) 시스템에서 사용됩니다. 두 송신기를 연결하는 인터페이스 역할을 하며, 일반적으로 강사가 학생을 가르치는 훈련 목적으로 사용됩니다. PPM 신호를 에뮬레이트하고 송신기에 입력할 수 있다면, 주식 모델 컨트롤러의 라디오를 활용할 수 있을 것입니다.\n\n![이미지](/assets/img/2024-06-20-ControlanyRCmodelprogrammaticallyfromcomputer_0.png)\n\n실제로, 트레이너 포트에 대한 정보를 찾던 중 이 프로젝트인 PPMControl을 발견했습니다. 누군가가 이미 이 아이디어를 구현했습니다. 이 접근 방식이 괜찮다면, 이 프로젝트를 살펴보시기 바랍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 이 해결책을 사용할 수 없다는 것을 발견했어요. 현대 RC 모델에서 2.4GHz 무선 신호가 흔하지만, 서로 다른 브랜드나 심지어 같은 브랜드 내의 다른 시리즈도 서로 다른 프로토콜을 사용합니다. 따라서 2.4GHz 송신기를 사용하더라도 모델과 호환되지 않을 수 있습니다. 예를 들어, 내 Mini-Z는 FHSS 프로토콜을 사용하는데, 이를 지원하는 컨트롤러만 호환됩니다. FHSS 프로토콜을 지원하는 컨트롤러를 찾던 중 여러 프로토콜을 지원하는 모듈을 개발한 흥미로운 프로젝트를 발견했어요.\n\n# 멀티 모듈\n\n![이미지](/assets/img/2024-06-20-ControlanyRCmodelprogrammaticallyfromcomputer_1.png)\n\n멀티 모듈은 4가지 다른 RF 구성 요소를 통합한 오픈 소스 2.4GHz 송신기 모듈로, 거의 모든 RC 송신기가 다양한 수신기와 모델을 작동할 수 있도록 합니다. 일종의 유니버설 TV 원격 제어기로, 여러 TV 브랜드의 원격 제어 코드를 이미 모두 포함하고 있는 것을 비유할 수 있어요. 마찬가지로 멀티 모듈은 여러 RF 구성 요소를 포함하며 가장 일반적인 RC 프로토콜을 지원합니다. 이는 모듈 포트가 있는 무선 송신기와 사용하도록 처음에 설계되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-20-ControlanyRCmodelprogrammaticallyfromcomputer_2.png\" /\u003e\n\n이 프로젝트는 오픈 소스로, 새로운 프로토콜을 추가할 수 있도록 허용합니다. 계속 활발히 유지되는 이 프로젝트에는 공동 작업자들이 계속해서 새로운 모델 지원을 추가하고 있습니다. 그래서 나는 컴퓨터와 연결해서 RC 모델을 수정하지 않고도 모든 RC 모델을 프로그래밍 방식으로 제어할 수 있게끔 고려 중입니다.\n\n# 구현\n\n이 모듈은 시리얼 및 PPM 입력을 수락하며, 이상적으로 컴퓨터에서 직접 시리얼 신호를 보내는 것이 좋을 것입니다. 그러나 작성 시점에 라디오 송신기의 시리얼 사양에 대한 명확한 문서를 찾지 못했기 때문에 (참고 자료가 있는 경우 알려주세요!). 개념을 먼저 테스트하고 싶었기 때문에, 컴퓨터에서 시리얼 명령을 수신하고 그에 따라 PPM 신호를 생성하는 변환기로 아두이노를 사용했습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 3](/assets/img/2024-06-20-ControlanyRCmodelprogrammaticallyfromcomputer_3.png)\n\n아두이노에는 이미 PPMEncoder 라이브러리가 있습니다. 따라서 제 프로그램은 주로 시리얼 통신을 초기화하고 라이브러리를 호출하여 PPM 신호를 생성합니다. 컴퓨터 측에서는 시리얼 포트 연결을 지원하고 스티어링(채널 1)과 스로틀(채널 2)의 타겟 펄스 폭을 전송하는 간단한 파이썬 애플리케이션을 작성했습니다.\n\n![Image 4](/assets/img/2024-06-20-ControlanyRCmodelprogrammaticallyfromcomputer_4.png)\n\n# 결론\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내 비디오에서 보여준 대로 차량을 수정하지 않고 재고 Mini-Z를 제어할 수 있게 되었어요. 이 설정은 철저히 제품의 조합이며, 납땜도 필요 없고 모든 항목은 다른 목적으로 재사용할 수 있어요. 이 기사가 여러분의 프로젝트에 도움이 되는 비침입적인 방법을 제공하는 데 도움이 되기를 바라요.\n\n참고: \n\n이 프로젝트의 소스 코드: https://github.com/kelvinkoko/universal-rc-controller","ogImage":{"url":"/assets/img/2024-06-20-ControlanyRCmodelprogrammaticallyfromcomputer_0.png"},"coverImage":"/assets/img/2024-06-20-ControlanyRCmodelprogrammaticallyfromcomputer_0.png","tag":["Tech"],"readingTime":3},{"title":"아두이노 프로그래밍에서 iOS 앱 개발로","description":"","date":"2024-06-20 16:48","slug":"2024-06-20-FromArduinoprogrammingtoiOSAppdevelopment","content":"\n\n## BLE 연결을 통한 IoT 프로젝트 구축 실습 가이드\n\n이 튜토리얼에서는 iOS 앱 개발과 Arduino 프로그래밍의 세계를 탐험하면서 전자 보드를 무선으로 제어할 수 있는 간단한 시스템을 생성하는 방법을 살펴보겠습니다. Bluetooth Low Energy (BLE) 연결 기능을 활용하여 Arduino Nano 33 BLE Sense 보드와 상호작용하고 LED 상태를 제어하며 센서에서 온도 데이터를 읽는 모바일 애플리케이션을 개발할 것입니다. 이 실습 가이드를 통해 iOS 앱과 Arduino 스케치를 개발하는 단계별 프로세스를 경험하면서 자체 IoT 프로젝트를 만들 수 있는 귀중한 기술을 습득할 수 있습니다.\n\n![이미지](/assets/img/2024-06-20-FromArduinoprogrammingtoiOSAppdevelopment_0.png)\n\nNano 33 BLE Sense는 BLE 연결 및 센싱 기능이 필요한 프로젝트에 적합한 소형 Arduino 보드입니다. 다양한 센서를 갖추고 있어 IoT 응용 프로그램, 웨어러블 기기 및 데이터 획득 프로젝트에 적합합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# BLE 통신 이해하기\n\nBLE는 저전력 기기를 위해 설계된 무선 통신 기술입니다. 이는 클라이언트-서버 아키텍처를 사용하며 다음과 같이 기기가 작동할 수 있습니다:\n\n- 중앙 (클라이언트): 통신을 시작하고 제어합니다.\n- 주변 (서버): 액세스할 데이터 또는 작업을 제공합니다.\n\n서비스와 특성은 BLE 통신의 구성 요소입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 서비스는 관련 기능이나 데이터 모음을 나타냅니다.\n- 특징은 서비스 내에서 특정한 데이터 값을 가리킵니다.\n\n연결을 설정하려면 주변 기기가 광고 패킷이라는 작은 메시지를 방송하여 가용성을 알립니다. 중심 기기는 스캔 과정을 통해 이러한 패킷을 수신하고 주변 기기를 발견할 수 있습니다.\n\n중심 기기가 관심 있는 주변 기기를 식별하면 연결을 설정하고 사용 가능한 서비스와 특징과 상호 작용을 시작할 수 있습니다.\n\n우리 프로젝트에서 iOS 앱은 중심의 역할을 하고, Arduino 보드는 주변으로 동작합니다. 보드는 두 개의 별도 서비스 내에서 두 개의 특징을 통해 데이터를 노출할 것입니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Led Status Characteristic (in Led Service): 이 특성은 쓰기 속성을 가진 특성으로, 앱이 0 (끄기) 또는 1 (켜기) 값을 쓰면 LED의 상태를 변경할 수 있습니다.\n- Temperature Characteristic (in Sensor Service): 이 특성은 읽기 및 알림 속성을 가진 특성으로, 보드에서 측정된 온도 값을 검색하고 온도 업데이트에 대한 알림을 받을 수 있도록 합니다.\n\n![이미지](/assets/img/2024-06-20-FromArduinoprogrammingtoiOSAppdevelopment_1.png)\n\n# Arduino 프로그램 빌드\n\nArduino 프로그램은 ArduinoBLE 및 Arduino_HTS221 라이브러리를 활용하여 BLE 통신 및 온도 감지 기능을 가능하게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\n#include \u003cArduinoBLE.h\u003e\n#include \u003cArduino_HTS221.h\u003e\r\n```\n\n먼저, 스케치는 UUID를 사용하여 필요한 BLE 서비스 및 특성을 설정하고 각 특성에 대한 속성을 정의합니다. UUID(Universally Unique Identifiers)는 BLE 통신 프로토콜에서 서비스 및 특성을 식별하는 데 사용되는 고유 식별자입니다.\n\n```js\r\nBLEService ledService(\"cd48409a-f3cc-11ed-a05b-0242ac120003\");\nBLEByteCharacteristic ledstatusCharacteristic(\"cd48409b-f3cc-11ed-a05b-0242ac120003\", BLEWrite);\n\nBLEService sensorService(\"d888a9c2-f3cc-11ed-a05b-0242ac120003\");\nBLEByteCharacteristic temperatureCharacteristic(\"d888a9c3-f3cc-11ed-a05b-0242ac120003\", BLERead | BLENotify);\r\n```\n\n그런 다음, 프로그램은 BLE 모듈을 초기화하고 로컬 이름 및 광고 서비스를 설정합니다. 사용자가 정의한 로컬 이름은 Arduino에 의해 방송되며 페리퍼럴을 식별하는 사람이 읽을 수 있는 식별자 역할을 합니다. 광고 서비스를 Led 서비스로 설정함으로써, Arduino는 iOS 앱이나 다른 중앙 장치에게 제공하는 특정 서비스에 대해 알립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nvoid setup() {\n    // ...\n\n    // BLE 초기화\n    if (!BLE.begin()) {\n        while (1);\n    }\n\n    // 광고할 로컬 이름과 서비스 UUID 설정\n    BLE.setLocalName(\"iOSArduinoBoard\");\n    BLE.setAdvertisedService(ledService);\n}\n```\n\nBLE 스택에 서비스와 특성이 추가되며, 온도 특성에 대한 읽기 요청 핸들러가 설정됩니다.\n\n```js\n// 서비스에 특성 추가\nledService.addCharacteristic(ledstatusCharacteristic);\nsensorService.addCharacteristic(temperatureCharacteristic);\n\n// BLE 스택에 서비스 추가\nBLE.addService(ledService);\nBLE.addService(sensorService);\n\n// 온도 특성에 대한 읽기 요청 핸들러 설정\ntemperatureCharacteristic.setEventHandler(BLERead, temperatureCharacteristicRead);\n```\n\n프로그램은 광고를 시작하고 메인 루프에 진입합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n // 광고 시작\n BLE.advertise();\n}\n```\n\n메인 루프에서는 중앙 기기가 연결하도록 대기하며, 연결되면 통신을 처리하는 루프를 시작합니다.\n\n```js\nvoid loop() {  \n  // BLE 센트럴 기기가 연결될 때까지 대기\n  BLEDevice central = BLE.central();\n\n  // 센트럴 기기가 페리페럴에 연결된 경우\n  if (central) {\n    // ...\n    while (central.connected()) {\n      // ...\n    }\n  }\n}\n```\n\n이 루프 내에서 프로그램은 주기적으로 센서에서 온도를 읽어 올바른 온도 값을 업데이트합니다. 또한, 센트럴 기기가 LedStatus 특성에 새 값을 쓰지 않았는지 확인합니다. 새 값을 감지하면 LED를 켜거나 끕니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nwhile (central.connected()) {\n    // ...\n    // 온도 값을 읽어옴\n    temperature = (int) HTS.readTemperature();\n    temperatureCharacteristic.writeValue(temperature);\n    \n    // ...\n    // LedStatus characteristic의 쓰기 여부 확인\n    if (ledstatusCharacteristic.written()) {\n        if (ledstatusCharacteristic.value()) {\n            digitalWrite(LED_BUILTIN, HIGH);\n        } else {\n            digitalWrite(LED_BUILTIN, LOW);\n        }\n    }\n}\n```\n\n마지막으로, 온도 characteristic의 읽기 요청에 응답하기 위해 read 이벤트 핸들러 함수를 구현했습니다. 이 함수는 현재 온도 값을 characteristic에 작성합니다.\n\n```js\nvoid temperatureCharacteristicRead(BLEDevice central, BLECharacteristic characteristic) {\n    temperatureCharacteristic.writeValue(temperature);\n}\n```\n\n# iOS 앱 설계\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\niOS 앱은 사용자 친화적인 인터페이스로 설계되었으며 두 가지 주요 화면으로 구성되어 있습니다:\n\n- 스캔 화면: 앱을 실행할 때 초기화면으로 작동하여 사용자가 주변 기기를 스캔할 수 있게 합니다. 발견된 기기의 목록을 표시하며 사용자는 특정 기기의 이름을 탭하여 연결을 시작할 수 있습니다. 연결에 성공하면 앱은 연결 화면으로 전환됩니다.\n- 연결 화면: 연결된 아두이노와 상호 작용할 수 있는 인터페이스를 제공합니다. 사용자는 LED의 상태를 수정하여 켜거나 끌 수 있습니다. 또한 사용자는 온도를 두 가지 모드로 읽을 수 있습니다: 단일 읽기(Read) 또는 변화의 지속적 모니터링(Notify).\n\n![이미지](/assets/img/2024-06-20-FromArduinoprogrammingtoiOSAppdevelopment_2.png)\n\n이 앱은 MVVM (Model-View-ViewModel) 디자인 패턴을 사용한 Clean Architecture에 영감을 받은 아키텍처로 구축되었으며 몇 가지 조정을 통해 단숨함과 이해하기 쉬운 요소를 향상시켰습니다. 이 접근 방식은 앱을 3개의 구분된 계층으로 구성하여 컴포넌트 독립성과 테스트 가능성을 촉진합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 프리젠테이션 계층은 사용자 인터페이스와 상호 작용을 처리합니다. 이 계층은 View(사용자 인터페이스 렌더링 담당)와 ViewModel(View의 상태를 관리하는)으로 구성됩니다.\n- 도메인 계층은 응용 프로그램의 핵심 비즈니스 로직을 나타냅니다. 이 계층은 사용 사례를 캡슐화하고 데이터와 상호 작용합니다. 사용 사례는 응용 프로그램에서 수행할 수 있는 특정 비즈니스 작업을 나타냅니다.\n- 데이터 계층은 데이터 접근 및 지속성을 담당합니다. 데이터 작업을 추상화하는 엔티티를 포함합니다.\n\n![image](/assets/img/2024-06-20-FromArduinoprogrammingtoiOSAppdevelopment_3.png)\n\n앱은 CoreBluetooth를 활용하여 Bluetooth 관련 작업을 처리할 것입니다. CoreBluetooth는 Apple이 제공하는 프레임워크로, iOS 플랫폼에서 Bluetooth 기기와의 원활한 통신을 가능하게 합니다. Bluetooth 기능 구현을 간소화하는 포괄적인 기능 세트를 제공합니다.\n\nCoreBluetooth를 활용하여, 앱은 주변의 Bluetooth 주변 기기를 탐지하고 연결을 설정하며 기기 간 데이터를 교환할 수 있을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자 이제 iOS 프로그래밍에 직접 참여해 봅시다!\n시작하기 전에 필요한 필수 도구 몇 가지가 있습니다: 맥 컴퓨터와 Apple 플랫폼을 위한 공식 통합 개발 환경(Integrated Development Environment, IDE) 인 Xcode입니다. Xcode는 iOS 애플리케이션을 디자인, 코딩 및 디버깅할 수 있는 종합적인 도구 및 자원 세트를 제공합니다.\n\nXcode를 열고, 환영 화면에서 \"새 Xcode 프로젝트 생성\"을 선택하고 iOS 섹션에서 App 템플릿을 선택하세요. 고유한 제품명(iOSArduinoBLE)을 제공하고, 팀 식별자(귀하의 Apple ID 이름)를 선택하고, 언어(Swift)와 프로젝트를 저장할 위치를 선택하세요.\n\n...여기까지입니다! 시작해 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# iOS 앱: Arduino 보드를 스캔하고 연결하기\n\n스캔 화면에서 사용자는 깔끔하고 직관적인 UI로 BLE 장치를 발견하고 연결할 수 있습니다.\n\n![이미지](/assets/img/2024-06-20-FromArduinoprogrammingtoiOSAppdevelopment_5.png)\n\n이 화면의 기본 아키텍처는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- ScanView: 화면의 UI를 렌더링하는 데 책임이 있습니다. SwiftUI를 사용하여 디자인되었습니다.\n- ScanViewModel: \"Start Scan\" 버튼을 탭하거나 장치 이름을 선택하는 이벤트를 처리합니다. 스캔 및 연결 작업을 실행하기 위해 ScanViewModel은 CentralUseCase에 의존합니다.\n- CentralUseCase: CoreBluetooth 프레임워크와 상호 작용하기 위한 필수 로직을 캡슐화하며, CBCentralManager 객체를 활용합니다. 이 객체는 프레임워크 내에서 스캔을 시작하고 연결을 설정하며 주변기기를 관리합니다.\n\n![이미지](/assets/img/2024-06-20-FromArduinoprogrammingtoiOSAppdevelopment_6.png)\n\n데이터 레이어에는 Peripheral 및 UUIDs 두 가지 엔티티가 포함되어 있습니다. Peripheral 객체는 BLE 장치를 나타내며, 장치 이름, 식별자 및 기타 관련 속성과 같은 세부 정보를 보유합니다. UUIDs는 응용 프로그램에서 사용할 서비스 및 특성 목록을 보유하는 컬렉션입니다.\n\n# 스캐닝\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"Start Scan\" 버튼을 누르면 ScanView가 사용자 상호작용을 캡처하고 요청을 처리하기 위해 ScanViewModel로 전달합니다.\n\n```js\n//  ScanView.swift\n// ...\nButton {\n    viewModel.scan()\n} label: {\n    Text(\"Start Scan\")\n    .frame(maxWidth: .infinity)\n}\n// ...\n```\n\nScanViewModel은 상호작용을 받으면 해당 사용 사례에서 스캔 기능을 트리거합니다. 사용 사례는 LedService를 광고하는 디바이스만 찾도록 지시 받습니다.\n\n```js\n//  ScanViewModel.swift\n// ...\nfunc scan() {\n    useCase.scan(for: [UUIDs.ledService])\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최종적으로 사용 사례는 CoreBluetooth 프레임워크와 상호 작용하여 스캔 프로시저를 시작합니다.\n\n```js\n// CentralUseCase.swift\n// ...\nlazy var central: CBCentralManager = {\n CBCentralManager(delegate: self, queue: DispatchQueue.main)\n}()\n\nfunc scan(for services: [CBUUID]) {\n guard central.isScanning == false else {\n     return\n }\n central.scanForPeripherals(withServices: services, options: [:])\n}\n```\n\nCoreBluetooth에서 scanForPeripherals 함수를 사용할 때, CBCentralManagerDelegate 메소드를 통해 응답을 처리할 수 있습니다. 발견된 페리페럴을 처리하는 델리게이트 메소드는 didDiscover이며, 스캔 과정 중에 페리페랄이 발견될 때마다 호출됩니다.\n\n기기가 발견되면 CentralUseCase는 onPeripheralDiscovery 클로저(즉, 자체 포함된 코드 블록)를 호출하여 새로 발견된 페리페럴의 세부 정보를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```swift\n// CentralUseCase.swift\n// ...\nextension CentralUseCase: CBCentralManagerDelegate {\n  func centralManager(_ central: CBCentralManager, didDiscover peripheral: CBPeripheral,\n                       advertisementData: [String : Any], rssi RSSI: NSNumber) {\n      onPeripheralDiscovery?(.init(cbPeripheral: peripheral))\n  }\n  // ...\n}\n```\n\nScanViewModel은 내부 상태를 업데이트하고 ScanView에 새로운 장치가 주변 장치 목록에 표시될 준비가 되었음을 알립니다.\n\n```swift\n// ScanViewModel.swift\n// ...\nuseCase.onPeripheralDiscovery = { [weak self] peripheral in\n    guard let self = self else {\n        return\n    }\n    self.foundPeripherals.insert(peripheral)\n    self.state = .scan(Array(self.foundPeripherals))\n}\n```\n\n```swift\n// ScanView.swift\n// ...\nVStack {\n    List(peripheralList, id: \\.id) { peripheral in\n        Text(\"\\(peripheral.name ?? \"N/A\")\")\n        // ...\n    }\n}\n// ...\n}\n.onReceive(viewModel.$state) { state in\n    switch state {\n        // ...\n        case .scan(let list):\n            peripheralList = list\n        // ...\n    }\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 연결하기\n\n목록 중 한 장치를 누르면 ScanView가 사용자 상호 작용을 캡처하고 해당 페리페럴에 연결하기 위해 ScanViewModel에 요청을 전달합니다.\n\n```js\n// ScanView.swift\n// ...\nList(peripheralList, id: \\.id) { peripheral in\n    Text(\"\\(peripheral.name ?? \"N/A\")\")\n        // ...\n        .onTapGesture {\n            viewModel.connect(to: peripheral)\n        }\n}\n```\n\n이 작업은 ScanViewModel이 연결 작업을 해당 사용 사례에 전달하도록 유도합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```swift\n//  ScanViewModel.swift\n// ...\nfunc connect(to peripheral: Peripheral) {\n    useCase.connect(to: peripheral)\n}\n```\n\n마침내 사용 사례는 선택한 장치와 연결 프로세스를 시작하는 동안 스캐닝 프로세스를 중단하여 CoreBluetooth 프레임워크와 상호 작용합니다.\n\n```swift\n//  CentralUseCase.swift\n// ...\nfunc connect(to peripheral: Peripheral) {\n    central.stopScan()\n    central.connect(peripheral.cbPeripheral!)\n}\n```\n\n연결이 성공하면 didConnect 대리자 메서드가 호출됩니다. CentralUseCase는 이 정보를 ScanViewModel에게 onConnection 클로저를 호출하여 전달합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```swift\n//  CentralUseCase.swift\n// ...\nfunc centralManager(_ central: CBCentralManager, didConnect peripheral: CBPeripheral) {\n    onConnection?(.init(cbPeripheral: peripheral))\n}\n```\n\nScanViewModel은 내부 상태를 업데이트하고 선택한 장치와의 연결이 설정되었음을 ScanView에 알립니다.\n\n```swift\n//  ScanViewModel.swift\n// ...\nuseCase.onConnection = { [weak self] peripheral in\n    self?.state = .connected(peripheral)\n}\n```\n\n그에 따라 ScanView는 ConnectView로 전환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```swift\n// ScanView.swift\n// ...\n.onReceive(viewModel.$state) { state in\n    switch state {\n    case .connected:\n        shouldShowDetail = true\n    // ...\n    }\n}\n.navigationDestination(isPresented: $shouldShowDetail) {\n    if case let .connected(peripheral) = viewModel.state  {\n        let viewModel = ConnectViewModel(useCase: PeripheralUseCase(),\n            connectedPeripheral: peripheral)\n        ConnectView(viewModel: viewModel)\n    }\n}\n```\n\n# iOS 앱: 데이터 쓰기 및 읽기\n\nConnect 화면에서 사용자는 Arduino와 상호 작용하여 LED 및 온도와 관련된 정보를 교환할 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-FromArduinoprogrammingtoiOSAppdevelopment_7.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아키텍처는 이전 화면과 일관성을 유지하며 다음 구성 요소를 포함합니다:\n\n- ConnectView: LED 상태를 제어하는 UI 요소, 온도 정보를 읽는 UI 요소, 및 장치와의 연결을 해제하는 UI 요소를 표현합니다. SwiftUI를 사용하여 구현되었습니다.\n- ConnectViewModel: LED 제어를 위한 온/오프 버튼을 탭하거나 온도 알림 활성화를 전환하거나 즉시 온도를 읽는 버튼을 누르는 등 사용자 상호작용을 캡처합니다. 또한 연산을 처리하기 위해 PeripheralUseCase와 통신합니다.\n- PeripheralUseCase: CentralUseCase와 유사하게 CoreBluetooth와 관련된 로직을 담당합니다. CBPeripheral 객체와 상호작용하여 특성과 서비스를 통해 데이터를 관리하고 교환합니다.\n\n![아무 이미지](/assets/img/2024-06-20-FromArduinoprogrammingtoiOSAppdevelopment_8.png)\n\n서비스와 특성과의 데이터 교환을 시작하기 전에 발견 단계를 수행해야 합니다. 이 단계에서 CoreBluetooth 프레임워크는 연결된 주변 장치에 의해 제공되는 사용 가능한 서비스와 특성에 대한 정보를 스캔하고 검색합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 발견\n\nConnectViewModel 및 해당 PeripheralUseCase를 생성한 후에는 발견 서비스 프로시저가 시작됩니다. 발견된 각 서비스에는 해당하는 특성도 발견됩니다.\n\nCoreBluetooth 프레임워크에서의 응답은 각각의 대리자 메서드인 didDiscoverServices 및 didDiscoverCharacteristicsFor로 전달됩니다. 이러한 메서드는 장치에서 발견된 서비스 및 특성에 대한 정보를 앱에 제공합니다.\n\n```swift\n//  PeripheralUseCase.swift\n// ...\nfunc discoverServices() {\n cbPeripheral?.discoverServices([UUIDs.ledService, UUIDs.sensorService])\n}\n// ...\nfunc peripheral(_ peripheral: CBPeripheral, didDiscoverServices error: Error?) {\n // ...\n  for service in services {\n      // ...\n      peripheral.discoverCharacteristics(uuids, for: service)\n  }\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nOnce the discovery process is completed, the `ConnectViewModel` is notified with `onPeripheralReady`, and the UI is prepared to handle operations on the discovered characteristics.\n\n```js\n// PeripheralUseCase.swift\n// ...\nfunc peripheral(_ peripheral: CBPeripheral, didDiscoverCharacteristicsFor service: CBService, error: Error?) {\n  for characteristic in characteristics {\n      discoveredCharacteristics[characteristic.uuid] = characteristic\n  }\n\n  if discoveredCharacteristics[UUIDs.temperatureCharacteristic] != nil \u0026\u0026\n      discoveredCharacteristics[UUIDs.ledStatusCharacteristic] != nil {\n      onPeripheralReady?()\n  }\n}\n```\n\n# Controlling LED\n\nWhen the user presses the on/off buttons, it triggers a write operation to the LedStatus characteristic with a numerical value (1 for “On” and 0 for “Off”). This action, in turn, controls the integrated LED on the board, either turning it on or off accordingly.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적인 흐름을 따라 전체 프로세스가 관리됩니다:\n\n```js\n// ConnectView.swift\n// ...\nButton(\"On\") {\n    viewModel.turnOnLed()\n}\n// ...\nButton(\"Off\") {\n    viewModel.turnOffLed()\n}\n\n// ConnectViewModel.swift\n// ...\nfunc turnOnLed() {\n    useCase.writeLedState(isOn: true)\n}\n\nfunc turnOffLed() {\n    useCase.writeLedState(isOn: false)\n}\n\n// PeripheralUseCase.swift\n// ...\nfunc writeLedState(isOn: Bool) {\n    cbPeripheral?.writeValue(Data(isOn ? [0x01] : [0x00]), for: ledCharacteristic, type: .withResponse)\n}\n```\n\n# 온도 읽기\n\n온도 측정은 두 가지 방법으로 수행할 수 있습니다: 한 번의 값을 얻기 위해 트리거되는 읽기 작업을 사용하는 싱글 샷 모드 또는 실시간 업데이트를 받기 위해 알림 작업을 사용하는 연속 모드입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 읽기 작업 요청의 흐름입니다:\n\n```js\n//  ConnectView.swift\n// ...\nButton(\"READ\") {\n  viewModel.readTemperature()\n}\n\n//  ConnectViewModel.swift\n// ...\nfunc readTemperature() {\n  useCase.readTemperature()\n}\n\n//  PeripheralUseCase.swift\n// ...\nfunc readTemperature() {\n cbPeripheral?.readValue(for: tempCharacteristic)\n}\n```\n\n알림 작업 흐름의 활성화/비활성화는 동일한 패턴을 따릅니다:\n\n```js\n//  ConnectView.swift\n// ...\nToggle(\"Notify\", isOn: $isToggleOn)\n// ...\n.onChange(of: isToggleOn) { newValue in\n if newValue == true {\n  viewModel.startNotifyTemperature()\n } else {\n  viewModel.stopNotifyTemperature()\n }\n}\n\n//  ConnectViewModel.swift\n// ...\nfunc startNotifyTemperature() {\n  useCase.notifyTemperature(true)\n}\n\nfunc stopNotifyTemperature() {\n  useCase.notifyTemperature(false)\n}\n\n//  PeripheralUseCase.swift\n// ...\nfunc notifyTemperature(_ isOn: Bool) {\n cbPeripheral?.setNotifyValue(isOn, for: tempCharacteristic)\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 작업은 모두 CoreBluetooth의 동일한 대리자에 응답을 생성합니다. 구체적으로 didUpdateValueFor 메서드입니다. 읽기 작업의 경우 요청한 데이터와 함께 단일 응답이 있을 것입니다. 알림의 경우, 알림이 비활성화될 때까지 응답이 계속 전송됩니다. 각 응답은 UI 상태를 업데이트하기 위해 ConnectViewModel의 onReadTemperature 클로저를 트리거합니다.\n\n```swift\n//  PeripheralUseCase.swift\n// ...\nfunc peripheral(_ peripheral: CBPeripheral, didUpdateValueFor characteristic: CBCharacteristic, error: Error?) {\n  switch characteristic.uuid {\n   case UUIDs.temperatureCharacteristic:\n     let value: UInt8 = {\n       guard let value = characteristic.value?.first else {\n         return 0\n       }\n       return value\n     }()\n     onReadTemperature?(Int(value))\n  }\n}\n\n//  ConnectViewModel.swift\n// ...\nuseCase.onReadTemperature = { [weak self] value in\n self?.state = .temperature(value)\n}\n\n//  ConnectView.swift\n// ...\n@State var lastTemperature: Int = 0\n// ...\nText(\"\\(lastTemperature) °C\")\n// ...\n.onReceive(viewModel.$state) { state in\n  switch state {\n   // ...\n   case let .temperature(temp):\n       lastTemperature = temp\n  }\n}\n```\n\n# 연결 해제\n\n연결 해제 버튼을 누르면 흐름이 약간 다른 방향으로 진행됩니다. 연결 해제 작업은 CBCentralManager 객체에 속하므로 CentralUseCase에서 수행되어야 합니다. 이를 가능하게 하기 위해 사용자가 버튼을 누르면 현재 화면이 해제되고 사용자는 스캔 화면으로 돌아갑니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```swift\n//  ConnectView.swift\n// ...\nButton {\n  dismiss()\n} label: {\n  Text(\"연결 해제\")\n  .frame(maxWidth: .infinity)\n}\n```\n\nScanView가 나타날 때, 수행하는 첫 번째 작업은 이미 연결된 장치가 있는지 확인하고 있다면 연결을 해제하는 것입니다. 이 작업은 해당 ScanViewModel에서 처리되며, 그런 다음 CentralUseCase에 해당 요청을 전달하여 CoreBluetooth 프레임워크에서 연결 해제 작업을 수행합니다.\n\n```swift\n//  ScanView.swift\n// ...\n.onAppear {\n  viewModel.disconnectIfConnected()\n}\n\n//  ScanViewModel.swift\n// ...\nfunc disconnectIfConnected() {\n  guard case let .connected(peripheral) = state,\n  peripheral.cbPeripheral != nil else {\n      return\n  }\n  useCase.disconnect(from: peripheral)\n}\n\n//  CentralUseCase.swift\n// ...\nfunc disconnect(from peripheral: Peripheral) {\n  central.cancelPeripheralConnection(peripheral.cbPeripheral!)\n}\n```\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글에서는 Arduino 프로그래밍부터 iOS 앱 개발로 이동하는 여정을 살펴보았는데, 이 과정에서 BLE 연결을 통해 IoT 프로젝트를 구축하는 데 초점을 맞추었습니다.\n\n우리는 Xcode와 CoreBluetooth 프레임워크를 사용하여 iOS 앱을 만드는 방법을 배웠습니다. BLE의 기본 지식을 습득하고, 청결한 아키텍처 원칙을 공부하며, 확장 가능하고 유지보수 가능한 코드베이스를 보장하기 위해 MVVM 디자인 패턴을 구현해 보았습니다.\n\n이 프로젝트는 여러분의 IoT 프로젝트를 시작하는 좋은 지점이 될 수 있습니다. 그러나 CoreBluetooth를 직접 사용하는 것은 백그라운드 작업 처리의 복잡도와 상위 수준의 기능(메쉬, 펌웨어 업데이트 등) 부족과 같은 몇 가지 제한 사항이 있을 수 있습니다. 이러한 제한 사항을 극복하기 위해, 추가적인 추상화 계층과 고급 기능을 지원하는 서드파티 라이브러리를 활용하는 것이 좋습니다.\n일부 인기 있는 옵션은 다음과 같습니다:\n\n- RxBluetoothKit: 강력한 ReactiveX 기반 BLE 라이브러리.\n- Bluejay: 간편하고 사용하기 쉬운 BLE 라이브러리를 강조하는 현대적인 라이브러리.\n- LittleBlueTooth: 가벼우면서 간편한 BLE 라이브러리.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 코드\n\n- iOSArduinoBLE_ArduinoSketch\n- iOSArduinoBLE_iOSApp","ogImage":{"url":"/assets/img/2024-06-20-FromArduinoprogrammingtoiOSAppdevelopment_0.png"},"coverImage":"/assets/img/2024-06-20-FromArduinoprogrammingtoiOSAppdevelopment_0.png","tag":["Tech"],"readingTime":18}],"page":"50","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"50"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>