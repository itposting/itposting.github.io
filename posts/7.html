<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/7" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/7" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="에이전트 AI의 핵심 과제인 계획 문제 강화학습으로 해결하는 방법" href="/post/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="에이전트 AI의 핵심 과제인 계획 문제 강화학습으로 해결하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="에이전트 AI의 핵심 과제인 계획 문제 강화학습으로 해결하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">에이전트 AI의 핵심 과제인 계획 문제 강화학습으로 해결하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LLM 출력 구조화하는 방법 안내" href="/post/2024-06-23-GuidinganLLMsResponsetoCreateStructuredOutput"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LLM 출력 구조화하는 방법 안내" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-GuidinganLLMsResponsetoCreateStructuredOutput_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LLM 출력 구조화하는 방법 안내" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">LLM 출력 구조화하는 방법 안내</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="검색 증강 생성RAG을 최적화하는 4가지 전략" href="/post/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="검색 증강 생성RAG을 최적화하는 4가지 전략" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="검색 증강 생성RAG을 최적화하는 4가지 전략" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">검색 증강 생성RAG을 최적화하는 4가지 전략</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">17<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="대형 언어 모델LLMs로 Next-Token 예측을 분류로 전환하는 방법" href="/post/2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대형 언어 모델LLMs로 Next-Token 예측을 분류로 전환하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대형 언어 모델LLMs로 Next-Token 예측을 분류로 전환하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">대형 언어 모델LLMs로 Next-Token 예측을 분류로 전환하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기" href="/post/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="약물 발견의 출발점으로서 방대한 화학 공간을 구조 기반 가상 스크리닝하기 위한 방법" href="/post/2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="약물 발견의 출발점으로서 방대한 화학 공간을 구조 기반 가상 스크리닝하기 위한 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="약물 발견의 출발점으로서 방대한 화학 공간을 구조 기반 가상 스크리닝하기 위한 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">약물 발견의 출발점으로서 방대한 화학 공간을 구조 기반 가상 스크리닝하기 위한 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="BERT 미세 조정으로 텍스트 분류하는 방법" href="/post/2024-06-23-FinetuneBERTfortextclassification"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="BERT 미세 조정으로 텍스트 분류하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="BERT 미세 조정으로 텍스트 분류하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">BERT 미세 조정으로 텍스트 분류하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="자율 에이전트의 흥망성쇠 그 발전과 한계 " href="/post/2024-06-23-TheRiseandFallofAutonomousAgents"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="자율 에이전트의 흥망성쇠 그 발전과 한계 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="자율 에이전트의 흥망성쇠 그 발전과 한계 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">자율 에이전트의 흥망성쇠 그 발전과 한계 </strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="대형 언어 모델LLM로 민감한 데이터 처리하는 방법 총정리" href="/post/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대형 언어 모델LLM로 민감한 데이터 처리하는 방법 총정리" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대형 언어 모델LLM로 민감한 데이터 처리하는 방법 총정리" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">대형 언어 모델LLM로 민감한 데이터 처리하는 방법 총정리</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LangChain, Semantic Kernel, AutoGen 최신 비교 및 분석" href="/post/2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LangChain, Semantic Kernel, AutoGen 최신 비교 및 분석" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LangChain, Semantic Kernel, AutoGen 최신 비교 및 분석" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">LangChain, Semantic Kernel, AutoGen 최신 비교 및 분석</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 23, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><a class="link" href="/posts/1">1</a><a class="link" href="/posts/2">2</a><a class="link" href="/posts/3">3</a><a class="link" href="/posts/4">4</a><a class="link" href="/posts/5">5</a><a class="link" href="/posts/6">6</a><a class="link posts_-active__YVJEi" href="/posts/7">7</a><a class="link" href="/posts/8">8</a><a class="link" href="/posts/9">9</a><a class="link" href="/posts/10">10</a><a class="link" href="/posts/11">11</a><a class="link" href="/posts/12">12</a><a class="link" href="/posts/13">13</a><a class="link" href="/posts/14">14</a><a class="link" href="/posts/15">15</a><a class="link" href="/posts/16">16</a><a class="link" href="/posts/17">17</a><a class="link" href="/posts/18">18</a><a class="link" href="/posts/19">19</a><a class="link" href="/posts/20">20</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"에이전트 AI의 핵심 과제인 계획 문제 강화학습으로 해결하는 방법","description":"","date":"2024-06-23 19:42","slug":"2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning","content":"\n\n창의력을 발휘하여 복잡한 비즈니스 전략 문제를 해결하기 위해 원활하게 협업하는 AI 에이전트 팀을 상상해보세요. 시장 동향을 조사하는 한 명의 에이전트, 재무 데이터를 분석하는 다른 한 명, 그리고 권고 사항을 준비하는 세 번째 에이전트가 모두 공통 목표를 향해 노력하고 있습니다.\n\n이 협력적 인 인공 지능의 논리, 즉 앤젠틱 AI를 알아보면 자동화와 문제 해결의 다음 단계를 나타냅니다. AI 시스템이 더욱 정교해지면서 미리 정의된 고정적인 프로세스를 벗어나 유연성, 적응력 및 AI 에이전트 간의 팀워크를 받아들이는 데 관심이 증가하고 있습니다.\n\n앤젠틱 AI는 기존의 전통적인 자동화 기술로 해결하기 어려웠던 복잡한, 개방형 작업을 자동화하는데 큰 약속을 합니다. 복잡한 문제를 전문화된 역할로 분해하고 개별 AI 에이전트의 고유한 능력을 활용함으로써, 다양한 에이전트 시스템은 이전에 상상도 못 했던 방식으로 지능적인 자동화를 조율할 수 있습니다. CrewAI, Langraph, Autogen과 같은 개척적인 프레임워크는 이 새로운 패러다임을 위한 길을 열며, 개발자가 복잡한 워크플로를 자율적으로 탐색하고 실행할 수 있는 AI 에이전트 팀을 디자인하고 배포할 수 있도록 도와주고 있습니다.\n\n그러나 이 새로운 협업 AI 영역으로 나아가면 앤젠틱 시스템의 핵심에 있는 근본적인 도전 과제를 마주하게 됩니다: 계획.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI 에이전트들이 효과적으로 행동을 계획하고 서로 협력하며 동적이고 개방적인 환경에서 자신들의 전략을 적응시킬 수 있게 하는 방법은 무엇일까요?\n\n이 문서는 계획이 에이전트 AI의 핵심 과제이며 강화 학습(RL)이 이 중요한 문제에 대한 유망한 해결책을 제시한다고 주장합니다.\n\n다음 섹션에서는 에이전트 AI의 부상과 주요 원칙에 대해 탐구하고, 이러한 시스템에서 계획이 이러한 의미 있는 과제로 작용하는 이유를 설명하며, 강화 학습 기법이 이러한 어려움을 해결할 수 있는 방법을 살펴볼 것입니다.\n\n에이전트 AI에서 계획과 강화 학습의 상호작용을 이해함으로써, 지능적 자동화와 협력적인 인공지능의 미래에 대한 중요한 통찰을 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Planning as the Core Challenge in Agentic AI: Solving it with Reinforcement Learning](/assets/img/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning_0.png)\n\n# Agentic AI의 부상\n\nAgentic AI는 인공지능 시스템을 개념화하고 구현하는 방식에서 패러다임이 바뀌었다. 핵심적으로, Agentic AI는 자율적인 AI 에이전트들이 복잡하고 개방적인 과제에 대처하기 위해 팀 또는 \"크루\"로 함께 일하는 모습을 상상한다. 이 접근 방식은 단일 모델 AI 시스템의 제약을 넘어서 전문화와 협력의 힘을 활용하여 더 정교하고 유연한 문제 해결 능력을 실현한다.\n\n이 Agentic AI 혁명의 전선에는 다수의 에이전트들 사이의 협력에 대한 독특한 접근 방식을 제공하는 여러 중요한 프레임워크들이 등장했다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- CrewAI: 이 프레임워크는 특정 역할을 갖는 AI 팀을 설계할 수 있게 해 주어, 그들이 특정 작업에 따라 선별된 연구 및 분석 도구 세트를 갖추도록 돕습니다.\n- Langraph: Langraph는 더 구조화된 방식을 채택하여, 명시적인 방향 그래프를 사용하여 에이전트 간의 작업 흐름을 정의합니다. 이를 통해 개발자들은 에이전트 조정과 작업 할당에 대해 세밀한 제어를 할 수 있습니다.\n- Autogen: Autogen은 에이전트 간의 다중 대화로부터 발생하는 신생 작업 흐름에 의존하여, 보다 동적이고 적응적인 협업 패턴을 가능하게 합니다.\n\n이 프레임워크들은 구체적인 구현에서 차이가 있지만, 모두 에이전틱 AI 접근 방식을 정의하는 중심 원칙을 공유합니다:\n\n전문화와 협업: 이러한 시스템 전반에 걸쳐 두드러지는 공통점 중 하나는 다수의 특화된 에이전트를 활용하여 스스로 작업하는 방식입니다. 단일 대형 모델에 의존하는 대신, 에이전틱 AI는 작업을 하위 작업으로 분해하여 각각 다른 역할과 기술을 갖춘 에이전트에 위임합니다. 이러한 전문화는 각 에이전트가 자신의 전문 분야에 집중할 수 있도록 하고, 협업은 팀이 어떤 개별 에이전트에겐 도전적일 수 있는 문제들을 해결할 수 있도록 돕습니다.\n\n예를 들어 채용 상황에서, 크루는 기술 직군 연구, 인적 프로필 엔지니어링, 이력서 전략 및 면접 준비에 특화된 에이전트로 구성될 수 있습니다. 이러한 특화된 에이전트들이 함께 작업하여 단일 일반적 AI보다 개인을 고용 전 과정에서 효과적으로 안내할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n언어 모델과 외부 도구의 활용: 에이전트 AI 시스템에서 또 다른 중요한 패턴은 각 에이전트를 뒷받침하는 \"두뇌\"로서 대형 언어 모델(LLMs)을 사용하는 것입니다. 이러한 미리 학습된 모델은 에이전트가 열린 대화를 할 수 있게 하며 자연어 질의를 해석하고 유창한 응답을 생성하며 판단을 내릴 수 있도록 합니다.\n\n그러나 에이전트 AI는 언어 모델만을 의존하지는 않습니다. 에이전트의 지식을 기반을 다지고 그들의 능력을 확장하기 위해, 이러한 시스템은 또한 외부 도구와 데이터 소스에 연결합니다. 웹에서 단락을 검색하거나 구조화된 데이터베이스를 질의하거나 타사 API를 호출하는 등의 방식으로, 에이전트들은 실제 세계 정보를 활용하여 자신들의 결정과 행동에 영감을 얻습니다.\n\n이러한 언어적 유연성과 외부 기반의 결합으로 인해 에이전트 AI 시스템은 넓은 세계로부터 통찰을 얻으면서 일관된 대화를 유지할 수 있습니다. 이는 인간이 언어를 지식과 행동의 관문으로 활용하는 방식을 재현하는 데 필수적인 한 걸음입니다.\n\n에이전트 상태 및 워크플로우 관리: 에이전트 AI 설계의 가장 다양한 측면은 플랫폼이 에이전트 팀의 상태와 워크플로우 오케스트레이션을 어떻게 다루는지입니다. 에이전트 작업은 종종 다수의 단계와 에이전트 출력 간의 의존성을 포함하므로 일관된 전역 상태와 제어 흐름을 유지하는 것이 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 도전 과제에 대한 접근 방식은 플랫폼마다 다양합니다. Langraph는 워크플로우를 정의하기 위해 명시적인 방향 그래프를 사용하여 개발자에게 세밀한 제어를 제공합니다. Autogen은 에이전트 간의 멀티턴 대화에서 발생하는 신흥 워크플로에 더 의존합니다. CrewAI는 상호 작용을 안내하는 고수준 태스크 플로우를 갖추고 있지만 에이전트들이 서브태스크를 자율적으로 위임하고 응답할 수 있는 유연성을 가지고 있습니다.\n\n이러한 차이점에도 불구하고, 에이전트 상태 및 워크플로우 관리를 위한 일관된 우선순위 목록이 도출됩니다:\n\n- 에이전트가 시간이 지남에 따라 다른 에이전트들의 작업 및 결정을 발전시킬 수 있는 메커니즘 제공\n- 태스크 분할 및 에이전트 조정 패턴의 유연한 정의 가능\n- 에이전트 역할, 도구 및 위임 권한의 태스크별 맞춤화 허용\n- 예외 처리 및 에이전트 출력 간 비선형 종속성 그래프 우아하게 처리\n\n보다시피, 에이전트 AI의 부상은 유연하고 지능적인 자동화의 엄청난 잠재력을 가져옵니다. 특화, 협업, 외부 데이터를 기반으로 한 언어 모델의 강점을 활용하여 이러한 시스템은 기존의 전통적인 AI 접근 방식으로는 이루기 힘든 복잡하고 개방적인 작업에 대처할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 이러한 잠재력은 상당한 도전과 함께 옵니다. 그 중에서도 가장 중요한 것은 계획 문제입니다. 어떻게 하면 다양한 AI 에이전트 팀이 효과적으로 행동을 조정하고 불확실성 하에서 결정을 내리며, 동적 환경에서 전략을 적응할 수 있도록 할 수 있을까요? 이것이 에이전트 AI 시스템의 핵심 도전에 대한 핵심을 담고 있습니다.\n\n# 핵심 도전으로서의 계획\n\n에이전트 AI 시스템이 복잡성과 능력을 키우면 효과적인 계획의 필요성이 점점 더 중요해집니다. 이 문맥에서의 계획은 AI 에이전트들이 목표를 달성하기 위해 행동 순서를 결정하고, 다른 에이전트들과 협력하며, 변화하는 상황에 적응하는 과정을 말합니다. 계획은 지적 행동의 기본적인 측면이지만, 에이전트 AI의 영역에서 특히 어려운 도전을 제기합니다.\n\n왜 계획이 특히 복잡한가요, 특히 다중 에이전트 시나리오에서? 이러한 어려움에 기여하는 몇 가지 주요 요소가 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 고차원 상태 및 행동 공간: 에이전틱 인공지능에서 상태 공간(환경 및 에이전트의 모든 가능한 구성)과 행동 공간(에이전트가 취할 수 있는 모든 가능한 행동)은 매우 크고 복잡합니다. 이는 각각의 능력과 잠재적 행동을 갖는 여러 에이전트가 상호작용하는 경우 조합 폭발로 인한 것입니다.\n- 부분 관측성: 에이전트들은 종종 환경의 상태와 다른 에이전트의 행동에 대해 불완전한 정보를 갖습니다. 이러한 불확실성으로 인해 행동의 결과를 예측하고 효과적으로 계획하기 어려워집니다.\n- 비정상적인 환경: 다중 에이전트 시스템에서는 환경이 에이전트가 행동을 취하고 서로 상호작용함에 따라 지속적으로 변화합니다. 이러한 비정상성은 시간이 지남에 따라 작용의 효과가 일관되지 않아 계획 과정을 복잡하게 만듭니다.\n- 장기 의존성: 에이전틱 AI의 많은 작업은 단계 간에 의존성을 가진 장기적인 행동 시퀀스를 필요로 합니다. 이러한 확장된 시간 경계를 통해 계획을 수행하는 것은 계산적으로 어려우며 즉시적 보상과 장기적 목표를 균형있게 유지해야 합니다.\n- 조정 및 통신 부담: 다중 에이전트 시스템에서 효과적인 계획은 에이전트 간의 조정이 필요하며 이는 의사 결정 과정에서 추가 복잡성과 병목현상을 초래할 수 있습니다.\n\n이러한 도전에 대처하기 위해 연구자들은 에이전틱 AI의 계획 문제를 마르코프 결정 과정(MDP)으로 정의하고 있습니다. MDP는 상황에 따라 결과가 일부적으로 무작위이고 일부적으로 의사 결정자의 통제 아래 있는 상황에서 의사 결정을 모델링하기 위한 수학적인 프레임워크를 제공합니다.\n\n에이전틱 AI의 맥락에서 MDP의 구성 요소를 다음과 같이 정의할 수 있습니다:\n\n- 상태 공간 (S): 모든 가능한 사고 과정 및 환경 구성의 공간\n- 동작 공간 (A): 사고나 문서 검색의 모든 가능한 조합\n- 전이 역학 (P): 이전 사고와 행동을 기반으로 새로운 사고가 생성되는 방법\n- 보상 함수 (R): 답변의 품질이나 목표에 대한 진전을 평가하는 것\n- 할인 계수 (γ): 단기 vs. 장기적 보상의 우선순위\n- 문제 기간 (T): 허용되는 추론 단계의 최대 수\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n계획 문제를 MDP로 프레임화함으로써, 강화 학습 분야의 다양한 기술을 활용하여 AI의 계획 과제를 해결할 수 있습니다. 그러나 이 정식화는 계획 과정에서 근본적인 긴장을 강조하기도 합니다: 탐험-활용 딜레마.\n\n탐험-활용 딜레마는 새로운 것을 탐색하거나 잘 알려진 좋은 솔루션을 활용하는 사이의 교환 비용을 가리킵니다. 에이전트 AI 계획의 맥락에서 이것은 다음과 같은 균형으로 나타납니다:\n\n- 탐험: 새로운 사고 조합을 시도하거나 다양한 문서를 검색하거나 혁신적인 추론 방향을 추구하여 중요한 솔루션에 이르는 일들에 대한 베스트.\n- 활용: 이미 알려진 효과적인 전략에 초점을 맞추거나 성공적인 사고 과정을 발전시키거나 최대의 즉각적 보상을 위해 기존 솔루션을 정제하는 일에 베스트.\n\n탐험과 활용 사이의 적절한 균형을 찾는 것은 에이전트 AI 시스템에서의 효과적인 계획에 중요합니다. 탐험이 과도하면 낭비되는 컴퓨팅 자원과 일관성 없는 성능을 야기할 수 있으며, 활용이 지나치다면 최적의 솔루션을 허술하게 만들거나 새로운 상황에 적응하지 못하는 문제를 야기할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n전통적인 계획 접근 방식인 상징적 AI나 철저한 검색을 기반으로 한 방법은 종종 에이전틱 AI의 맥락에서 이러한 도전 과제를 해결하는 데 어려움을 겪는다. 이러한 방법들은 일반적으로 환경의 완전한 지식, 결정론적 행동 결과, 명확히 정의된 목표 상태에 의존하는데, 이는 에이전틱 AI가 활동하는 복잡하고 불확실하며 개방적인 도메인에서 거의 적용되지 않는 가정들이다.\n\n대신 필요한 건 유연하고 적응적인 계획 접근 방식으로, 다음과 같은 기능을 갖추어야 한다:\n\n- 고차원 상태 및 행동 공간을 효율적으로 처리\n- 부분 관찰 가능성과 불확실성 다루기\n- 비정상적인 환경에 적응하기\n- 복잡한 종속성이 있는 긴 시간 범위에 계획 수립\n- 탐색과 활용을 동적으로 균형있게 유지\n- 여러 전문화된 에이전트 간의 행동 조정\n\n여기서 강화 학습이 등장하여 에이전틱 AI 시스템이 제기하는 독특한 계획 도전 과제를 해결하기에 적합한 강력한 기법 세트를 제공한다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 강화 학습 및 고급 기술로서의 솔루션\n\n강화 학습(RL)은 에이전트형 AI에서 복잡한 계획 도전에 대처하는 유망한 접근법으로 부각되었습니다. RL은 에이전트가 환경과 상호 작용하면서 보상이나 처벌의 형태로 피드백을 받아 결정을 내리는 방식의 머신러닝 유형입니다.\n\n이러한 학습 패러다임은 에이전트형 AI에서 계획 문제에 특히 적합한 이유가 여럿 있습니다:\n\n- 경험으로부터 학습: RL 에이전트들은 환경의 완전한 모델을 요구하지 않고 시행착오를 통해 최적의 전략을 학습할 수 있습니다. 이는 에이전트형 AI가 작동하는 복잡한, 부분 관측 가능한 도메인에서 중요합니다.\n- 탐험과 이용 사이의 균형 유지: RL 알고리즘에는 탐사-이용 교환을 관리하는 내장 기구가 있어, 에이전트들이 새로운 전략을 발견하면서도 알려진 좋은 해결책을 활용할 수 있게 합니다.\n- 불확실성 다루기: RL 방법은 확률적 환경에서 작동하도록 설계되어, 다중 에이전트 시스템에 내재된 불확실성에 탄력적으로 대처할 수 있습니다.\n- 장기 계획: 많은 RL 알고리즘은 명시적으로 장기 보상을 최적화하기 위해 설계되어 있어, 긴 시간 대역으로 계획을 수립하고 행위 간의 복잡한 종속성을 포착할 수 있습니다.\n- 적응성: RL 에이전트들은 새로운 경험을 기반으로 전략을 지속적으로 업데이트할 수 있어, 변동성 있는 환경에 적합합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특히 계획 도전 과제를 해결하는 데 유망한 강화 학습 기법 중 하나는 몬테 카를로 트리 탐색(Monte Carlo Tree Search, MCTS)입니다. MCTS는 휴리스틱 탐색 알고리즘이며, 랜덤 샘플링과 트리 탐색을 결합하여 복잡한 공간에서 결정을 내립니다. 이 기법은 다양한 분야에 성공적으로 적용되었으며, 알파고(AlphaGo)와 같은 게임 플레이 인공지능에서 사용되었습니다.\n\n에이전틱 인공지능 계획의 맥락에서, MCTS는 가능한 사고 과정과 행동 시퀀스의 광범위한 공간을 효율적으로 탐색하는 데 사용될 수 있습니다. MCTS의 주요 단계는 다음과 같습니다:\n\n- 선택(Selection): 루트에서 시작하여 탐색하는 동안 탐색과 활용을 균형있게 고려하는 트리 정책(예: 상한 신뢰 경계)을 사용합니다.\n- 확장(Expansion): 새로운 자식 노드를 추가하여 트리를 확장합니다.\n- 시뮬레이션(Simulation): 새 노드에서 랜덤 시뮬레이션을 실행하여 값을 추정합니다.\n- 역전파(Backpropagation): 루트에 되돌아가는 경로를 따라 노드 통계를 업데이트합니다.\n\n이러한 단계를 반복적으로 적용함으로써, MCTS는 검색 공간의 가장 유망한 지역에 계산 리소스를 집중할 수 있어 에이전틱 인공지능 계획에서 마주치는 고차원 상태 및 행동 공간에 적합합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다른 중요한 강화 학습 개념 중 하나는 에이전틱 AI 계획에 적용할 수 있는 Q-러닝입니다. Q-러닝은 모델이 없는 강화 학습 알고리즘으로, 주어진 상태에서 특정 행동을 취했을 때 기대되는 누적 보상(Q-값)을 추정하는 방법을 학습합니다. 에이전틱 AI 환경에서는 Q-러닝을 사용하여 다양한 사고 과정이나 문서 검색의 가치를 추정할 수 있습니다.\n\n이 분야의 최근 발전은 이러한 기본적인 강화 학습 개념을 바탕으로 한 몇 가지 혁신적인 접근 방식의 발전을 이끌어내어, 에이전틱 AI 시스템에서 계획 및 추론의 특정 도전 과제를 해결하기 위한 기술적 발전을 이끌어내고 있습니다.\n\n또한 특히 유망한 세 가지 혁신적 기술을 살펴보겠습니다:\n\n## Q*: 딥러닝 계획을 통한 다단계 추론 개선\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nWang 및 다른 사람(2024)이 소개한 Q* 프레임워크는 대형 언어 모델(Large Language Models, LLMs)의 다단계 추론 능력을 향상시키는 데 중요한 발전을 이끌어냅니다. Q*는 A* 검색의 능력을 결합하여 학습된 Q-value 모델로 LLMs을 복잡한 추론 작업 중에서 가장 유망한 다음 단계를 선택하도록 안내합니다.\n\nQ*의 주요 특징은 다음과 같습니다:\n\n- 각 노드가 주어진 문제에 대한 부분 솔루션을 나타내는 그래프로 추론 프로세스를 모델링합니다.\n- A* 검색을 위한 학습된 Q-value 모델을 휴리스틱 함수로 사용하여, 전체 문제를 해결하는 데 각 잠재적인 다음 단계가 얼마나 유망한지를 추정합니다.\n- 가능한 추론 경로의 방대한 공간을 효율적으로 탐색하기 위해 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)을 사용합니다.\n- LLM이 자체 정제된 답변에 대한 점수를 매기는 자가 평가 메커니즘을 통합하여 추론 프로세스를 지속적으로 향상시킬 수 있습니다.\n\nQ* 프레임워크는 에이전트 AI 계획에서 몇 가지 중요한 도전에 대처합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 긴 콘텍스트 처리: Q*는 전통적인 LLM의 고정된 콘텍스트 창의 제약을 극복하기 위해 지식 원본에서 대규모 문서 배치를 처리할 수 있습니다.\n- 무관한 정보에 대한 견고성: 다양한 추론 분기를 탐색함으로써 Q*는 실패한 정보 검색 및 오도된 문서에 대해 저항력을 갖습니다.\n- 적응성: 이 프레임워크는 기본 LLM의 작업별 특정 조정 없이 다양한 추론 작업에 적용할 수 있습니다.\n\n실험 결과에 따르면 Q*는 다양한 수학적 추론 및 코드 생성 작업에서 기준선 방법을 크게 앞섰으며, 지식 기반 AI 시스템의 계획 및 추론 능력을 향상시킬 잠재력을 입증했습니다.\n\n## 병렬 함수 호출을 위한 LLM 컴파일러\n\nQ*가 추론 프로세스 자체를 개선하는 데 초점을 맞추고 있을 때, LLM 컴파일러 접근 방식은 에이전틱 AI 계획의 또 다른 중요한 측면을 다룹니다: 병렬 함수 호출의 효율적인 조율. 이 기법은 고전 컴파일러 설계에서 영감을 받아 대규모 언어 모델에서 여러 함수 호출을 실행을 최적화하는 것을 목표로 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM 컴파일러 방식의 주요 측면은 다음과 같습니다:\n\n- 사용자 입력을 상호 의존성을 가진 일련의 작업으로 자동 분해합니다.\n- 독립적인 작업을 병렬로 실행하여 복잡한 워크플로에서 발생하는 지연 시간을 크게 감소시킵니다.\n- 작업의 방향성 비순환 그래프(DAG)를 생성하는 계획 단계를 통해 효율적인 일정 계획 및 실행이 가능합니다.\n- 외부 도구 및 API와 통합하여 LLM의 능력을 언어 처리 이상으로 확장합니다.\n\nLLM 컴파일러는 공별한 AI 계획에서 여러 중요한 도전 과제를 다룹니다:\n\n- 효율성: 병렬화 가능한 패턴을 식별하고 함수 호출 의존성을 관리함으로써, 컴파일러는 복잡한 작업의 지연 시간을 크게 줄일 수 있습니다.\n- 확장성: 이 방식은 다수의 함수 호출과 데이터 의존성이 포함된 대규모 및 복잡한 작업을 다루도록 설계되었습니다.\n- 유연성: 컴파일러는 다양한 종류의 LLM 및 작업 부하에 적응할 수 있어 다양한 AI 응용 프로그램에 대한 다재다능한 도구가 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n초기 결과에 따르면, LLM 컴파일러는 순차 실행 방법과 비교하여 상당한 속도 향상을 달성할 수 있다는 것이 밝혀졌습니다. 최대 3.7배의 대기 시간 개선과 일부 작업에서 최대 6.7배까지의 비용 절감이 가능합니다.\n\n## 수학 올림피아드 솔루션을 위한 몬테카를로 트리 자기 수정\n\n타 분야에서 MCTS의 성공을 바탕으로, 연구자들은 복잡한 수리 추론 작업, 특히 수학 올림피아드에서 마주하는 작업들을 처리하기 위해 특별히 개발된 몬테카를로 트리 자기 수정(MCTSr) 알고리즘을 개발했습니다.\n\nMCTSr의 주요 기능은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 대규모 언어 모델과 몬테카를로 트리 탐색을 통합하여 문제 해결 능력을 향상시킵니다.\n- 선택, 자가 세부화, 자가 평가 및 역전파 단계를 포함하는 반복적인 과정입니다.\n- 모델이 솔루션을 반복적으로 향상시킬 수 있는 피드백 안내형 세분화 과정입니다.\n- 진정으로 개선된 솔루션이 높은 점수를 받도록 하는 엄격하고 비판적인 점수 매커니즘입니다.\n\nMCTSr은 수학적 추론과 계획에서 여러 가지 도전에 대응합니다:\n\n- 복잡한 다단계 문제 다루기: 이 알고리즘은 다단계 추론 단계와 전략적 사고가 필요한 복잡한 수학적 작업을 다루도록 설계되었습니다.\n- 지속적인 개선: 자가 세부화 및 자가 평가 메커니즘을 통해 MCTSr은 솔루션 품질을 점진적으로 향상시킬 수 있습니다.\n- 다양한 문제 유형에 대한 적응성: 이 프레임워크는 초등학교 산술부터 올림피아드 수준의 도전 과제까지 다양한 수학 영역에서 성공을 거두었습니다.\n\n실험 결과에서는 MCTSr이 LLaMA-3 8B와 같은 훨씬 작은 모델을 사용하여 수학 올림피아드 문제에서 GPT-4 수준의 성능을 달성할 수 있다는 것을 입증하였으며, 이는 인공지능 시스템의 추론 능력을 혁신적으로 향상시킬 잠재력을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 세 가지 접근법인 Q*, LLM Compiler 및 MCTSr은 에이전틱 AI의 계획 및 추론 기술의 최신 동향을 대표합니다. 이러한 방법들은 강화 학습 원칙과 혁신적인 탐색 및 최적화 전략을 결합하여 AI 주도 문제 해결에서 가능한 범위를 넓히고 있습니다.\n\n그러나 이러한 고급 기술을 에이전틱 AI 계획에 적용하는 데는 다음과 같은 도전 과제가 있습니다:\n\n- 계산 복잡성: 이러한 방법들은 대부분 고도의 계산 과정을 통합하며, 대규모 응용 프로그램에는 리소스가 많이 필요할 수 있습니다.\n- 탐험과 활용의 균형: 새로운 솔루션을 발견하고 기존의 좋은 전략을 활용하는 적절한 균형을 찾는 것은 여전히 까다로운 작업입니다.\n- 해석 가능성: 이러한 시스템이 더 복잡해지면서 의사 결정 과정에서의 투명성과 해석 가능성을 보장하는 것이 점점 어려워지고 있습니다.\n- 일반화: 이러한 방법은 특정 도메인에서 인상적인 결과를 보여주었지만, 다양한 작업 유형 간의 일반화 능력을 평가하기 위해 추가 연구가 필요합니다.","ogImage":{"url":"/assets/img/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning_0.png"},"coverImage":"/assets/img/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning_0.png","tag":["Tech"],"readingTime":12},{"title":"LLM 출력 구조화하는 방법 안내","description":"","date":"2024-06-23 19:40","slug":"2024-06-23-GuidinganLLMsResponsetoCreateStructuredOutput","content":"\n\n![2024-06-23-GuidinganLLMsResponsetoCreateStructuredOutput_0.png](/assets/img/2024-06-23-GuidinganLLMsResponsetoCreateStructuredOutput_0.png)\n\n이 기사는 Python에서 유효성 검사 라이브러리를 사용하여 GPT-4 또는 Llama 3와 같은 LLM 응답을 구조화하는 방법을 가르쳐줍니다.\n\nJSON 형식에서 구조화된 정보를 추출해야 하는 필요성은 매우 중요한 주제이며, 이것은 데이터 마이닝 작업에서 정확한 정보를 비구조적 형식(예: 자유 텍스트)에서 추출하는 데 기본적입니다.\n\n또한, LLM의 출력 토큰을 생성하는 과정에서 발생하는 확률적 특성으로 인해 GPT와 같은 상업용 시스템에서도 구조화된 응답 형식이 신뢰할 수 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 유효성 검사와 스키마 모델링을 위해 Pydantic와 Instructor와 같은 여러 라이브러리를 사용할 것이고, LLM 부분에는 OpenAI와 ollama를 활용할 것입니다. 제안된 내용은 OpenAI나 Anthropic과 같은 폐쇄 소스 모델뿐만 아니라 Llama 3와 같은 오픈 소스 모델에 대해서도 유효합니다.\n\n본 기사를 통해 아래 내용을 배울 수 있습니다:\n\n- 데이터 모델을 정의하는 방법과 그것이 무엇인지\n- LLM이 출력 형식을 준수하는지를 유효성 규칙을 통해 확인하는 방법\n- Instructor와 Pydantic 라이브러리를 사용하는 방법\n\n즐거운 독해 되세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 구조화된 출력이 필요한 이유\n\nGPT-4와 같은 LLM은 특정 패턴을 따르지 않고도 상당한 가치를 제공할 수 있습니다. 하지만 데이터를 다루는 프로그래머들에게는 사용자의 의지에 따라 가능한 출력 패턴을 준수하는 것이 중요합니다.\n\nGPT-3.5의 특정 버전부터 OpenAI는 완성 API에 response_format 매개변수를 추가했습니다. 이를 통해 사용자는 json_object와 같은 다른 키를 정의하여 모델을 입력한 프롬프트에 더 적합한 응답 방향으로 안내할 수 있습니다.\n\n다음은 예시입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=\"gpt-3.5-turbo-0125\",\n  response_format={ \"type\": \"json_object\" },\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n  ]\n)\nprint(response.choices[0].message.content)\n\n\u003e\u003e\u003e \"content\": \"{\\\"winner\\\": \\\"Los Angeles Dodgers\\\"}\"\n```\n\n하지만 이러한 로직이 항상 작동하는 것은 아닙니다. 실제로 OpenAI의 문서에서는 GPT가 이를 생성하는 데 도움을 주기 위해 프롬프트에 \"JSON\"이라는 단어를 명확하게 작성할 것을 제안합니다. 이는 \"response_format={ \"type\": \"json_object\" }\"를 사용할 때 프롬프트 어딘가에 이를 작성해야만 하는 중요한 팁이기 때문에 강제적으로 작성해야 합니다.\n\n## LLM이 일관된 JSON 출력을 생성하기 어려운 이유는 무엇인가요?\n\nLLM은 입력 프롬프트가 주어졌을 때 이전 토큰 다음에 더 많이 나올 가능성이 있는 다음 토큰을 반환하는 기계로서의 역할을 합니다. 실제로 이러한 형식을 보고 이해하려면 모델이 훈련 단계에서 명시적으로 이러한 형식을 보고 이해하기 위해 안내받아야만 하므로 이러한 패턴을 \"자연\"에서 만나기 어렵습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최신 LLM의 JSON 모드는 출력이 특정 패턴과 일치한다고 보장하지 않습니다. 단지 유효하고 오류 없이 파싱된다는 것만을 보장합니다.\n\n따라서 이러한 출력물 안에 무엇이 포함되어 있는지를 유효성 검사할 수 있고, 데이터 모델과 일치하지 않는 경우 예외와 오류를 발생시키는 것이 중요합니다.\n\n# 사용 사례\n\n우리는 GPT-4 또는 Llama3와 같은 LLM에 간단한 질문에서 시작하여 JSON에서 정보를 추출하는 예제를 살펴볼 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 무엇이든 물어볼 수 있지만, 모델에게 시간이 지남에 따른 축구 월드컵 우승팀에 관한 질문을 하려고 합니다.\n\n특히 우리는 다음을 추출하고 싶습니다.\n\n- 결승 일자\n- 대회의 개최 국가\n- 우승 팀\n- 최다 득점자\n\n우리는 데이터의 정확성을 확인하는 것이 아니라, LLM의 문장 응답을 다음으로 보여줄 스키마에 맞추는 것에만 신경을 쓸 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 이 예제를 살펴보고 다른 것들도 살펴볼 수 있을 것 같아요.\n\n## 필수 종속성\n\n이제 이 튜토리얼을 실행하기 위해 설치해야 할 종속성을 살펴봅시다.\n\n당연히, 이미 활성화된 개발 환경이 있다고 가정하고 Pydantic, Instructor, OpenAI 클라이언트 및 ollama를 설치할 거예요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Pydantic: 커뮤니티에서 널리 사용되는 데이터 모델 정의 및 유효성 검사 라이브러리로, 사용 편의성, 효율성 및 데이터 과학에서의 중요성으로 유명합니다.\n- Instructor: LLMs와 작업하기 위해 특별히 제작된 Pydantic을 감싸는 래퍼로, 유효성 검사 로직을 생성할 수 있는 라이브러리입니다.\n- OpenAI: GPT와 다른 OpenAI 모델에 쿼리를 요청하기 위한 유명한 클라이언트입니다.\n- ollama: llama3와 같은 오픈 소스 LLM에 대한 매우 편리한 인터페이스입니다.\n\n개발 환경에서는 다음 명령어를 사용하여 시작합니다.\n\n```bash\npip install pydantic instructor openai ollama\n```\n\n오픈 소스 모델을 테스트하고자 하기 때문에 다음 단계는 ollama를 시스템 전역에 설치하는 것입니다. ollama의 설치 및 사용 방법은 이 특별한 기사에서 읽어보실 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 개발에 집중할 수 있겠네요.\n\n## 데이터 모델 정의\n\n데이터 모델은 데이터를 구조화하기 위해 따를 논리적인 패턴입니다. 데이터베이스의 테이블을 정의하는 것부터 입력 데이터를 유효성 검사하는 데까지 여러 맥락에서 사용됩니다.\n\n아래 포스트에서 Pydantic을 활용한 데이터 과학과 머신러닝에서의 데이터 모델링에 대해 이미 약간 다룬 적이 있습니다 👇\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파이던틱 데이터 모델을 만들어 보면 좋겠어요:\n\n```js\nfrom pydantic import BaseModel, Field\nfrom typing import List\nimport datetime\n\nclass SoccerData(BaseModel):\n    date_of_final: datetime.date = Field(..., description=\"최종 이벤트 날짜\")\n    hosting_country: str = Field(..., description=\"대회를 개최하는 국가\")\n    winner: str = Field(..., description=\"최종 경기에서 우승한 축구팀\")\n    top_scorers: list = Field(\n        ..., description=\"대회의 상위 3명 스코어러 목록\"\n    )\n\nclass SoccerDataset(BaseModel):\n    reports: List[SoccerData] = []\n```\n\n이 스크립트에서는 Pydantic에서 BaseModel 및 Field 클래스를 가져와 데이터 모델을 만드는 작업을 시작합니다. 사실, 최종 결과가 가져야 할 구조를 만들고 있습니다.\n\nPydantic은 모델에 들어가는 데이터 유형을 선언해야 합니다. 예를 들어 datetime.date는 날짜 필드가 문자열이 아니라 날짜여야 함을 강제합니다. 동시에 top_scorers 필드는 반드시 목록이어야 하며, 그렇지 않으면 Pydantic이 유효성 검사 오류를 반환할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내, 여러 인스턴스를 수집하는 데이터 모델을 만들었습니다. 이것은 SoccerData 모델의 모음을 수집하는 SoccerDataset이라고 합니다. 이 모델은 한 개 이상의 보고서가 있는지 확인하기 위해 강사에 의해 사용될 것입니다.\n\n# 시스템 프롬프트 생성\n\n매우 간단히, 모델이 수행해야 하는 작업을 영어로 적어봅시다. 예를 통해 결과의 의도와 구조를 강조하면서 설명합니다.\n\n```js\nsystem_prompt = \"\"\"당신은 숙련된 스포츠 기자입니다. 특정 연도의 축구 월드컵에서 우승한 팀에 대한 작은 리포트를 작성할 것입니다. 대회 결승전 날짜, 대회 전체에서 상위 3 스코어러, 우승 팀, 그리고 대회를 주최한 국가를 보고합니다. 다음 필드를 포함하는 JSON 객체를 반환하세요: date_of_final, hosting_country, winner, top_scorers.\\\n \n만약 다수 연도가 입력되면, 보고서를 쉼표로 구분하세요.\\\n \n다음은 예시입니다.\n [\n    {\n        \"date_of_final\": \"1966\",\n        \"hosting_country\": \"England\",\n        \"winner\": \"England\",\n        \"top_scorers\": [\"Player A\", \"Player B\", \"Player C\"]\n    },\n    {\n        \"date_of_final\": ...\n        \"hosting_country\": ...\n        \"winner\": ...\n        \"top_scorers\": ...\n    },\n\n]\n\n다음 연도들에 대해 보고해야 할 것입니다:\n\n \"\"\"\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시스템 프롬프트로 사용되며 단순히 쉼표로 구분된 연도를 전달할 수 있습니다.\n\n# 강사 코드 생성\n\n여기서는 Instructor를 사용하여 JSON 유효성 검사 및 구조화의 주요 로직을 만들 것입니다. 이를 통해 GPT를 API를 통해 호출하는 OpenAI에서 제공하는 인터페이스와 유사한 인터페이스를 사용합니다.\n\n먼저 우리는 query_gpt라는 함수를 사용하여 OpenAI를 사용하여 프롬프트를 매개변수화할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom openai import OpenAI\nimport instructor\n\ndef query_gpt(prompt: str) -\u003e list:\n    client = instructor.from_openai(OpenAI(api_key=\"...\"))\n    resp = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=SoccerDataset,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    )\n    return resp.model_dump_json(indent=4)\n```\n\nOpenAI API 키를 새롭게 생성된 클라이언트에 전달하는 것을 잊지 말자. 우리는 GPT-3.5-Turbo를 사용하고, 응답 모델로 SoccerDataset을 전달할 것이다. 또한, 이 기사를 작성하는 시점에서 가장 강력한 모델인 \"gpt-4o\"를 사용할 수도 있다.\n\n모든 것을 함께 조합하여 소프트웨어를 실행해 보자. 사용자 프롬프트로 입력할 내용으로 \"2010, 2014 및 2018\"년을 내용으로 전달하여 구조화된 보고서를 생성하고자 한다. \n\n```js\nfrom openai import OpenAI\nimport instructor\n\nfrom typing import List\nfrom pydantic import BaseModel, Field\nimport datetime\n\n\nclass SoccerData(BaseModel):\n    date_of_final: datetime.date = Field(..., description=\"최종 이벤트의 날짜\")\n    hosting_country: str = Field(..., description=\"대회를 주최하는 나라\")\n    winner: str = Field(..., description=\"최종 경기에서 승리한 축구팀\")\n    top_scorers: list = Field(\n        ..., description=\"대회의 상위 3명의 득점수 리스트\"\n    )\n\n\nclass SoccerDataset(BaseModel):\n    reports: List[SoccerData] = []\n\n\nsystem_prompt = \"\"\"당신은 전문 스포츠 기자입니다. 특정 연도의 축구 월드컵에서 승자를 작은 보고서로 작성해야 합니다.\n대회 최종일, 대회의 전체 득점수 상위 3명, 우승 팀 및 대회를 개최하는 국가를 보고해야 합니다.\n다음 필드를 포함한 JSON 객체를 반환하세요: date_of_final, hosting_country, winner, top_scorers.\n\n쿼리가 유효하지 않은 경우 빈 보고서를 반환하세요.\n\n여러 연도가 입력된 경우 보고서를 쉼표로 구분하세요.\n\n예시입니다\n[\n    {\n        \"date_of_final\": \"1966\",\n        \"hosting_country\": \"England\",\n        \"winner\": \"England\",\n        \"top_scorers\": [\"Player A\", \"Player B\", \"Player C\"]\n    },\n    {\n        \"date_of_final\": ...\n        \"hosting_country\": ...\n        \"winner\": ...\n        \"top_scorers\": ...\n    },\n\n]\n\n다음 보고가 필요한 연도입니다:\n\n\"\"\"\n\ndef query_gpt(prompt: str) -\u003e list:\n    client = instructor.from_openai(OpenAI())\n    resp = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=SoccerDataset,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    )\n    return resp.model_dump_json(indent=4)\n\nif __name__ == \"__main__\":\n  resp = query_llm(\"2010, 2014, 2018\")\n  print(resp)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 문구를 Markdown 형식으로 변환한 결과입니다:\n\n```json\n{\n    \"reports\": [\n        {\n            \"date_of_final\": \"2010-07-11\",\n            \"hosting_country\": \"South Africa\",\n            \"winner\": \"Spain\",\n            \"top_scorers\": [\n                \"Thomas Müller\",\n                \"David Villa\",\n                \"Wesley Sneijder\"\n            ]\n        },\n        {\n            \"date_of_final\": \"2014-07-13\",\n            \"hosting_country\": \"Brazil\",\n            \"winner\": \"Germany\",\n            \"top_scorers\": [\n                \"James Rodríguez\",\n                \"Thomas Müller\",\n                \"Neymar\"\n            ]\n        },\n        {\n            \"date_of_final\": \"2018-07-15\",\n            \"hosting_country\": \"Russia\",\n            \"winner\": \"France\",\n            \"top_scorers\": [\n                \"Harry Kane\",\n                \"Antoine Griezmann\",\n                \"Romelu Lukaku\"\n            ]\n        }\n    ]\n}\n```\n\n멋지네요. GPT-3.5-Turbo가 우리의 지시를 완벽하게 따르고, Instructor가 데이터 모델과 일치하는 구조를 만들어내었습니다. 실제로 이 결과는 GPT와 같은 대형 언어 모델이 일반적으로 반환하는 문자열이 아니라, 파이썬 사전의 리스트입니다.\n\n이제 이상한 입력을 넣어보려고 해봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nif __name__ == \"__main()\":\n      print(query_gpt(\"안녕, 어떻게 지내?\"))\n\n\u003e\u003e\u003e\n{\n \"리포트\": []\n}\n```\n\nLLM은 시스템 프롬프트를 통해 잘못된 쿼리를 처리하는 방법을 요청했기 때문에 올바르게 비어있는 리포트를 반환합니다.\n\n# Instructor와 함께 오픈 소스 템플릿 사용\n\nInstructor를 사용하여 GPT를 어떻게 사용하여 구조화된 JSON 출력을 얻는지 알아보았습니다. 이제 llama3와 같은 오픈 소스 템플릿을 사용하는 방법을 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n새로운 함수인 query_llama을 생성해 봅시다.\n\n```js\ndef query_llama(prompt: str) -\u003e list:\n    client = instructor.from_openai(\n        OpenAI(\n            base_url=\"http://localhost:11434/v1\",\n            api_key=\"ollama\",  # 요청은 필요하지만 영향을 미치지 않습니다\n        ),\n        mode=instructor.Mode.JSON,\n    )\n    resp = client.chat.completions.create(\n        model=\"llama3\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ],\n        response_model=SoccerDataset,\n    )\n    return resp.model_dump_json(indent=4)\n```\n\nGPT 코드와 약간의 차이가 있습니다. 함께 살펴보겠습니다.\n\n- ollama는 GPT와 동일한 인터페이스를 통해 호출되지만, 기본 URL 포인터(base_url) 및 필수적이지만 올바른 작동에 필요하지 않은 API 키를 변경합니다(왜냐면 모르겠어요)\n- JSON 모드를 mode 매개변수를 통해 설명해야 합니다.\n새로운 함수를 실행해 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n함수를 실행해 봅시다.\n\n```js\r\nif __name__ == \"__main__\":\n    print(query_llama(\"2010, 2014, 2018\"))\r\n```\n\n그리고 여기에 결과가 있습니다:\n\n```js\r\n{\n    \"reports\": [\n        {\n            \"date_of_final\": \"2010-07-11\",\n            \"hosting_country\": \"South Africa\",\n            \"winner\": \"Spain\",\n            \"top_scorers\": [\n                \"Thomas Müller\",\n                \"Wolfram Toloi\",\n                \"Landon Donovan\"\n            ]\n        },\n        {\n            \"date_of_final\": \"2014-07-13\",\n            \"hosting_country\": \"Brazil\",\n            \"winner\": \"Germany\",\n            \"top_scorers\": [\n                \"James Rodríguez\",\n                \"Miroslav Klose\",\n                \"Thomas Müller\"\n            ]\n        },\n        {\n            \"date_of_final\": \"2018-07-15\",\n            \"hosting_country\": \"Russia\",\n            \"winner\": \"France\",\n            \"top_scorers\": [\n                \"Harry Kane\",\n                \"Kylian Mbappé\",\n                \"Antoine Griezmann\"\n            ]\n        }\n    ]\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 올바른 JSON이 있는 목록을 가지고 있어요! 이 모든 것은 Llama 3로 로컬에서 이루어져요.\n\n이전에 말한대로, 유효성 검사는 구조를 기반으로 하고 있어요. 실제로, 이 내용은 GPT에서 생성된 내용과 다를 수 있어요.\n\n어떻게 마커들이 다른지 살펴봅시다. 아마도 우리가 받고 싶은 마커들을 명확히 지정하면 올바른 목록을 얻을 수도 있겠죠.\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPydantic, Instructors, 그리고 ollama를 사용하여 LLM의 출력을 JSON과 같은 구조화된 형식으로 변환하는 방법을 살펴봤습니다.\n\n이 과정에서 모델이 실제로 지도되므로 결정론적이지 않습니다. JSON이 LLM의 결정론적이지 않은 성질로 인해 준수되지 않을 수 있는 경우가 있을 것입니다.","ogImage":{"url":"/assets/img/2024-06-23-GuidinganLLMsResponsetoCreateStructuredOutput_0.png"},"coverImage":"/assets/img/2024-06-23-GuidinganLLMsResponsetoCreateStructuredOutput_0.png","tag":["Tech"],"readingTime":12},{"title":"검색 증강 생성RAG을 최적화하는 4가지 전략","description":"","date":"2024-06-23 19:38","slug":"2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG","content":"\n\n# 개인 데이터 및 개인 인프라를 활용한 고급 AI 솔루션\n\n![image](/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_0.png)\n\n머신 러닝 모델을 최적화하기 위해 권장 사항을 몇 번이나 따라보았는데, 여전히 특정 요구 사항과 잘 맞지 않는 솔루션에 부딪힌 적이 몇 번이나 있나요? 대답을 알아요: 많은, 아니면 모든 경우에 해당할 거예요. 모든 것이 데이터에 달려 있기 때문이죠. 특정 상황에 가장 적합한 방법을 찾을 때까지 테스트하고 실패하고 또 다시 테스트해야만 해요. 이 기사에서는 고급 AI 솔루션을 위해 개인 데이터와 개인 인프라를 활용하여 검색 증강 생성(Retrieval-Augmented Generation, RAG)을 최적화하는 네 가지 전략을 제시합니다.\n\n이전 기사에서는 LLaMA 3 같은 공개 모델에 개인 지식을 포함시키기 위해 검색 증강 생성(RAG) 전략을 활용하는 방법을 설명했습니다. RAG를 사용하면 개인 데이터를 개인 인프라에 저장하면서도 민감한 정보를 다른 사람들과 공유하지 않고 사용할 수 있습니다. RAG를 활용하는 장점은 명백하지만, 구현에는 여러 부분에서 중요한 조정이 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## RAG Recap\n\n저는 이전 글에서 설명한 RAG에 대한 간단한 개요부터 시작하겠습니다. 두 가지 주요 프로세스가 있습니다. 첫 번째는 \"데이터 수집 프로세스\"로, 다양한 소스에서 데이터를 수집하여 텍스트로 변환한 후 작은, 일관된 및 의미론적으로 관련 있는 부분으로 분할하고 결과를 벡터 데이터베이스에 저장합니다. 두 번째는 \"추론 프로세스\"로, 사용자 쿼리로 시작하여 첫 번째 프로세스의 결과를 사용하여 관련 데이터 부분을 식별하고 마지막으로 모델의 컨텍스트를 풍부하게하여 출력을 얻습니다.\n\n다음 다이어그램에서 두 프로세스의 자세한 내용을 볼 수 있습니다:\n\n![다이어그램](/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지난 기사에서는 ColdF 라는 가상의 회사에서 데이터를 사용하여 \"데이터 수집\" 및 \"추론\" 프로세스를 생성했습니다. 이 기사에서는 이러한 프로세스의 결과를 평가하고 최적화하는 기본적인 방법을 설명하겠습니다.\n\n## 개선 구성 요소\n\n먼저, 우리의 RAG 프로세스에서 중요한 지점을 식별하는 것부터 시작해봅시다:\n\n- 청킹 접근: 의미 있고 맥락적으로 관련 있는 데이터 세그먼트를 보장하도록 청킹 크기를 최적화합니다.\n- 임베딩 모델: 의미 표현을 개선하기 위해 모델 선택 및 세부 조정을 수행합니다.\n- 벡터 검색 방법: 효과적인 유사성 측정 및 검색 매개변수 선택합니다.\n- 모델에 피드할 최종 프롬프트: 효율적인 결과 품질을 개선하기 위해 효과적인 프롬프트를 작성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## RAG 파이프라인에서의 A/B 테스팅\n\n각 개선 구성 요소를 식별한 후, 전략은 각 구성 요소의 두 가지 다른 구성을 가진 두 버전을 비교하여 어떤 것이 더 나은 성능을 발휘하는지 결정합니다. 이는 두 버전을 실행하고 미리 정의된 지표에 대한 성능을 측정하는 것을 포함합니다. 하지만 성능을 어떻게 측정할까요? 그리고 어떤 지표를 사용해야 할까요? 이에 대한 답변으로 우리는 “RAGAS: 자동화된 검색 증강 생성 평가”¹ 논문을 사용합니다. 이 논문은 세 가지 주요 지표를 제안합니다:\n\n- 충실도: 답변에 있는 정보가 문맥에 제공된 정보와 일치하는지 확인합니다. 답변이 충실하다면 그 안에 있는 모든 내용이 문맥에서 직접 찾거나 추론할 수 있습니다. 예를 들어, 문맥이 “5월에 리스본을 방문했을 때, 앨리스와 나는 알파마, 바이로 알토, 벨렘 타워 그리고 다른 여러 곳으로 갔다”이고 답변이 “5월에 앨리스는 알파마, 바이로 알토, 벨렘 타워 그리고 다른 여러 곳으로 갔다”라면 문맥은 추출된 모든 내용을 지지하므로 충실도 점수는 100%입니다. 그러나 답변이 “5월에 앨리스는 알파마와 상 호르헤 성에 갔다”라면 답변에서 추출된 두 내용 중 (예: “앨리스는 알파마로 갔다” 및 “앨리스는 상 호르헤 성에 갔다”) 하나만이 문맥에서 지지되므로 충실도 점수는 50%입니다.\r\n- 답변 관련성: 생성된 답변이 완전하고 직접적으로 질문에 대답하는지 확인합니다. 정보가 정확하든 아니든 관련성이 있습니다. 예를 들어, 질문이 “포르투갈의 수도는 무엇인가?”이고 답변이 “리스본은 포르투갈의 수도입니다”라면 이 답변은 질문에 직접 대답하므로 관련성이 있습니다. 답변이 “리스본은 다양한 명소가 많은 아름다운 도시입니다”라면 부분적으로 관련성이 있을 수 있지만 질문에 대답하는 데 직접 필요한 정보가 아닌 추가 정보가 포함되어 있습니다. 이 지표는 답변이 집중되고 핵심을 유지하도록 보장합니다.\r\n- 문맥 관련성: 문맥에서 제공된 정보가 질문에 대답하는 데 얼마나 도움이 되는지 확인합니다. 필요한 것만 포함되고 불필요한 관련 없는 정보는 제거되므로 질문에 직접적으로 도움이 되지 않는 부가 정보를 제거합니다. 예를 들어, 질문이 “5월에 앨리스가 리스본에서 어떤 장소를 방문했나요?”이고 문맥이 “5월에 리스본을 방문했을 때, 앨리스는 알파마, 바이로 알토, 벨렘 타워 그리고 다른 여러 곳으로 갔다”이면 이 문맥은 앨리스가 5월에 어떤 장소를 방문했는지에 대한 필수 정보만을 제공하므로 매우 관련성이 높습니다. 그러나 문맥이 “5월에 리스본을 방문했을 때, 앨리스는 많은 흥미로운 사람을 만나 맛있는 음식을 먹었으며 다양한 장소를 갔다”이면 이 문맥은 질문에 대답하는 데 필요하지 않은 부가 정보를 포함하고 있으므로 관련성이 없는 것으로 간주됩니다. 이 지표는 질문에 대답하는 데 도움이 되는 정보만 포함되어 불필요한 자세를 피하도록 보장합니다. 이 지표는 문맥 정밀도로도 불립니다.\n\n이 논문은 또한 이러한 지표가 LLM을 통해 완전 자동화된 방식으로 측정될 수 있다는 방법에 대해 설명합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 평가에서 사용할 라이브러리인 Ragas는 이러한 주요 메트릭의 진화를 보여주며 새로운 메트릭을 추가했습니다:\n\n- Context Recall: 이 메트릭은 문맥과 실제 답변 간의 일치 정도를 측정합니다. Context Relevance와 마찬가지로 생성된 답변 대신 실제 답변을 사용합니다. 이 메트릭을 얻으려면 참 값이 필요합니다. 이러한 전략의 효과를 평가하기 위해 ColdF 데이터를 바탕으로 실제 답변이 포함된 10개의 질문 세트를 준비했습니다.\n\nFaithfulness와 Answer Relevance는 생성기 메트릭으로서, 각각 환영과 답변이 질문과 얼마나 직접적인지를 측정합니다.\n\nContext Relevance와 Context Recall은 검색기 메트릭으로, 벡터 데이터베이스에서 올바른 데이터 청크를 검색하고 필요한 모든 정보를 얻는 능력을 측정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전에 제시한 네 가지 메트릭을 평가하려면 질문, 생성된 답변, 맥락 및 실제 답변이 필요합니다.\n\nLangChain을 사용하여 RAG 프로세스를 구현할 것입니다. 코드를 실행하려면 Python이 설치되어 있어야 하며(version 3.11.9) 다음 라이브러리가 필요합니다:\n\n- ollama==0.2.1\n- chromadb==0.5.0\n- transformers==4.41.2\n- torch==2.3.1\n- langchain==0.2.0\n- ragas==0.1.9\n\n다음은 LangChain을 사용한 코드 스니펫입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\n# 필요한 라이브러리 및 모듈 가져오기\nfrom langchain.embeddings.base import Embeddings\nfrom transformers import BertModel, BertTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer, RobertaModel, RobertaTokenizer\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_text_splitters import MarkdownHeaderTextSplitter\nimport requests\nfrom langchain_chroma import Chroma\nfrom langchain import hub\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.chat_models import ChatOllama\nfrom operator import itemgetter\n\n# DPRQuestionEncoder를 사용하여 사용자 지정 임베딩 클래스 정의\nclass DPRQuestionEncoderEmbeddings(Embeddings):\n    show_progress: bool = False\n    \"\"\"tqdm을 설치해야 하는지 여부 표시합니다.\"\"\"\n    \n    def __init__(self, model_name: str = 'facebook/dpr-question_encoder-single-nq-base'):\n        # 지정된 모델 이름으로 토크나이저와 모델 초기화\n        self.tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(model_name)\n        self.model = DPRQuestionEncoder.from_pretrained(model_name)\n        \n    def embed(self, texts):\n        # texts가 리스트인지 확인\n        if isinstance(texts, str):\n            texts = [texts]\n        \n        embeddings = []\n        if self.show_progress:\n            try:\n                from tqdm import tqdm\n                iter_ = tqdm(texts, desc=\"임베딩 중\")\n            except ImportError:\n                logger.warning(\n                    \"tqdm을 가져올 수 없어 진행률 표시줄을 표시할 수 없습니다. \"\n                    \"`pip install tqdm`으로 설치하세요.\"\n                )\n                iter_ = texts\n        else:\n            iter_ = texts\n\n        for text in iter_:\n            # 입력 텍스트 tokenize\n            inputs = self.tokenizer(text, return_tensors='pt')\n            # 모델을 사용하여 임베딩 생성\n            outputs = self.model(**inputs)\n            # 임베딩 추출하고 리스트로 변환\n            embedding = outputs.pooler_output.detach().numpy()[0]\n            embeddings.append(embedding.tolist())\n        \n        return embeddings\n    \n    def embed_documents(self, documents):\n        return self.embed(documents)\n    \n    def embed_query(self, query):\n        return self.embed([query])[0]\n\n# 프롬프트 생성을 위한 템플릿 정의\ntemplate = \"\"\"\n### CONTEXT\n{context}\n\n### QUESTION\nQuestion: {question}\n\n### INSTRUCTIONS\nCONTEXT 마크다운 텍스트를 사용하여 사용자의 질문에 답변하세요.\n간결하고 명료한 답변을 제공하세요.\n질문에 답변하기 위해 CONTEXT에 근거만 사용하세요.\nCONTEXT에 필요한 정보가 없는 경우 'NONE'을 반환하세요.\n\"\"\"\n\n# 템플릿을 사용하여 ChatPromptTemplate 인스턴스 생성\nprompt = ChatPromptTemplate.from_template(template)\n\n# URL에서 텍스트 데이터 가져오기\nurl = \"https://raw.githubusercontent.com/cgrodrigues/rag-intro/main/coldf_secret_experiments.txt\"\nresponse = requests.get(url)\nif response.status_code == 200:\n    text = response.text\nelse:\n    raise Exception(f\"파일을 가져오는 데 실패했습니다: {response.status_code}\")\n\n# 마크다운 텍스트를 분할할 헤더 정의\nheaders_to_split_on = [\n    (\"#\", \"Header 1\")\n]\n\n# 지정된 헤더로 MarkdownHeaderTextSplitter 인스턴스 생성\nmarkdown_splitter = MarkdownHeaderTextSplitter(\n    headers_to_split_on, strip_headers=False\n)\n\n# 마크다운 splitter를 사용하여 텍스트 분할\ndocs_splits = markdown_splitter.split_text(text)\n\n# Chat 모델 초기화\nllm = ChatOllama(model=\"llama3\")\n\n# 사용자 지정 임베딩을 사용하여 문서에서 Chroma vector store 생성\nvectorstore = Chroma.from_documents(documents=docs_splits, embedding=DPRQuestionEncoderEmbeddings())\n\n# Vector store에서 retriever 생성\nretriever = vectorstore.as_retriever()\n\n# 문서 형식 지정을 위한 함수 정의\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# 검색 보완 생성 (RAG) chain 생성\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n    | {\"answer\": prompt | llm | StrOutputParser(), \n       \"context\": itemgetter(\"context\")}\n)\n\n# 질문으로 RAG chain 실행\nresult = rag_chain.invoke(\"Who led the Experiment 1?\")\nprint(result)\r\n```\r\n\r\n위 코드의 끝에 RAG Chain이 정의되어 있으며 이 코드를 사용하여 메트릭을 평가할 수 있습니다:\r\n\r\n```js\r\n# 필요한 라이브러리 및 모듈 가져오기\nimport pandas as pd\nfrom datasets import Dataset\nfrom ragas import evaluate\nfrom ragas.metrics import (\n        context_precision,\n        faithfulness,\n        answer_relevancy,\n        context_recall\n)\nfrom langchain_community.chat_models import ChatOllama\n\ndef get_questions_answers_contexts(rag_chain):\n    \"\"\" 질문 및 답변 목록을 읽고 ragas 데이터셋을 반환합니다. \"\"\"\n    # 파일 URL\n    url = 'https://raw.githubusercontent.com/cgrodrigues/rag-intro/main/coldf_question_and_answer.psv'\n\n    # URL에서 파일 가져오기\n    response = requests.get(url)\n    data = response.text\n   \n    # 데이터를 줄 단위로 분할\n    lines = data.split('\\n')\n\n    # 각 줄을 파이프 기호로 나누어 튜플 생성\n    rag_dataset = []\n\n    for line in lines[1:10]: # 처음 10개 질문만\n        if line.strip():  # 공백이 아닌지 확인\n            question, reference_answer = line.split('|')\n            result = rag_chain.invoke(question)\n            generated_answer = result['answer']\n            contexts = result['context']\n\n            rag_dataset.append({\n                \"question\": question,\n                \"answer\": generated_answer, \n                \"contexts\": [contexts], \n                \"ground_truth\": reference_answer\n            })\n\n          \n    rag_df = pd.DataFrame(rag_dataset)\n    rag_eval_datset = Dataset.from_pandas(rag_df)\n    \n    # ragas 데이터셋 반환\n    return rag_eval_datset\n\ndef get_metrics(rag_dataset):\n    \"\"\" RAG Dataset에 대해 신의성, 답변 관련성, 컨텍스트 정밀도, \n        컨텍스트 재현율 메트릭 계산 \"\"\"\n    # 계산할 메트릭 목록\n    metrics = [\n        faithfulness,\n        answer_relevancy,\n        context_precision,\n        context_recall\n    ]\n        \n    # LLaMA 3 모델을 사용할 지역 ChatOllama\n    langchain_llm =  ChatOllama(model=\"llama3\")\n    langchain_embeddings = DPRQuestionEncoderEmbeddings('facebook/dpr-question_encoder-single-nq-base')\n\n    # 메트릭 반환\n    results = evaluate(rag_dataset, metrics=metrics, llm=langchain_llm, embeddings=langchain_embeddings)\n    return results\n\n# RAG 데이터셋 가져오기\nrag_dataset = get_questions_answers_contexts(rag_chain)\n\n# 메트릭 계산\nresults = get_metrics(rag_dataset)\nprint(results)\r\n```\r\n\r\n위 코드를 실행하여 결과를 살펴보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```json\r\n{\n  '신뢰성': 0.8611, \n  '답변 관련성': 0.8653, \n  '맥락 정밀도': 0.7778, \n  '맥락 회수율': 0.8889\n}\r\n```\n\n\u003cimg src=\"/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_2.png\" /\u003e\n\n이미 언급된 것처럼, 첫 번째 두 지표(예: 신뢰성 및 답변 관련성)는 생성 과정과 관련이 있습니다. 이는 이러한 지표를 향상시키기 위해서는 언어 모델이나 모델에 입력되는 프롬프트를 변경해야 한다는 것을 의미합니다. 마지막 두 지표(예: 맥락 정밀도 및 맥락 회수율)는 검색과 관련이 있으며, 이는 이러한 지표를 향상시키기 위해서는 문서가 저장되고 색인화되며 선택되는 방식을 개선해야 한다는 것을 의미합니다.\n\n## 청킹 접근 방식\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n청킹 접근 방식은 데이터가 검색을 위해 최적의 세그먼트로 분할되도록 보장합니다. 이 패러다임은 다양한 청크 크기를 실험하여 너무 작아서 문맥이 빠진다거나 너무 크면 (검색 시스템을 압도하는) 문제가 발생하지 않도록 균형을 찾는 것을 포함합니다. 베이스라인에서는 각 실험을 기준으로 문서를 청크로 분할합니다. 그렇기 때문에 실험의 일부가 희석되어 최종 임베딩에 포함되지 않을 수 있습니다. 이 상황을 해결하기 위한 한 가지 가능한 접근 방식은 부모 문서 검색기를 사용하는 것입니다. 이 방법은 특정 관련 문서 단편이나 문단뿐만 아니라 그들의 부모 문서도 검색합니다. 이 접근 방식은 관련 단편 주변의 문맥이 보존되도록 보장합니다. 아래 코드는 이 접근 방식을 테스트할 때 사용되었습니다:\n\n```js\n# 필요한 라이브러리 및 모듈 가져오기\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\n# 부모 문서 검색기 생성\nparent_document_retriever = ParentDocumentRetriever(\n    vectorstore = Chroma(collection_name=\"parents\", \n                         embedding_function=DPRQuestionEncoderEmbeddings('facebook/dpr-question_encoder-single-nq-base')),\n    docstore = InMemoryStore(),\n    child_splitter = RecursiveCharacterTextSplitter(chunk_size=200),\n    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500),\n)\n\nparent_document_retriever.add_documents(docs_splits)\n\n\n# 검색 보강 생성 (RAG) 체인 생성\nrag_chain_pr = (\n    {\"context\": parent_document_retriever | format_docs, \"question\": RunnablePassthrough()}\n    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n    | {\"answer\": prompt | llm | StrOutputParser(), \n       \"context\": itemgetter(\"context\")}\n)\n\n# RAG 데이터세트 가져오기\nrag_dataset = get_questions_answers_contexts(rag_chain_pr)\n\n# 메트릭스 계산\nresults = get_metrics(rag_dataset)\nprint(results)\r\n```\n\n결과는 다음과 같습니다:\n\n```js\r\n{\n  'faithfulness': 0.6667, \n  'answer_relevancy': 0.4867, \n  'context_precision': 0.7778, \n  'context_recall': 0.6574\n}\r\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 성능 향상에 기여하지 않는 것을 보여줍니다. 감소하는 컨텍스트 회상은 회수 과정이 제대로 작동하지 않고 컨텍스트에 완전한 정보가 없음을 나타냅니다. 충실성 및 답변 관련성 지표의 변화는 풍부하지 않은 컨텍스트에서 나옵니다. 이 경우, 청킹 및 회수를 위한 다른 방법을 평가해 볼 수 있습니다.\n\n## 포함 모델\n\n포함 모델은 텍스트 청크를 밀집 벡터 표현으로 변환합니다. 서로 다른 모델은 다양한 주제에서 훈련될 수 있으며 때로는 임베딩을 개선할 수 있습니다. 임베딩 방법의 선택은 계산 효율성과 임베딩 품질 사이의 균형을 고려해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 Dense Passage Retrieval (“facebook/dpr-question_encoder-single-nq-base”), Sentence-BERT (“paraphrase-MiniLM-L6-v2”), 또는 Chroma의 기본 모델 (“all-MiniLM-L6-v2”)과 같은 다양한 임베딩 모델을 비교합니다. 각 모델은 강점을 갖고 있으며 도메인 특정 데이터에서 평가하여 가장 정확한 의미 표현을 제공하는지를 결정하는 데 도움이 됩니다.\n\n임베딩 모델을 변경하기 위해 새로운 클래스 \"SentenceBertEncoderEmbeddings\"를 정의하는 것이 필요합니다. 이 새로운 클래스는 모델인 Sentence-BERT 모델을 구현합니다. 이 새로운 클래스는 우리 이전 임베딩인 \"DPRQuestionEncoderEmbeddings\"를 대체할 것이며, 이것은 아래의 코드로 Sentence-BERT 모델을 테스트하는 데 사용한 코드입니다:\n\n```js\n코드 내용\n```\n\n결과는 요렇습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n{\n  'faithfulness': 0.5278, \n  'answer_relevancy': 0.5306, \n  'context_precision': 0.5556, \n  'context_recall': 0.7997\n}\n```\n\n\u003cimg src=\"/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_4.png\" /\u003e\n\n이 경우, 인코더의 변경은 메트릭스에서 성능이 저하된 것을 의미합니다. 이는 DPR이 Sentence-BERT보다 검색 정확도가 더 높기 때문에, 정확한 문서 검색이 중요한 경우에는 DPR이 더 적합하다는 것을 의미합니다. Sentence-BERT로 전환할 때 '신뢰성' 및 '답변 관련성' 메트릭스의 상당한 하락은 높은 검색 정밀성을 필요로 하는 작업에 적합한 임베딩 모델을 선택하는 중요성을 강조합니다.\n\n## 벡터 검색 방법\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n벡터 검색 방법은 유사성 측정을 기반으로 가장 관련성 높은 청크를 검색합니다. 흔한 방법으로는 유클리드 (L2) 거리, 코사인 유사도 등이 있습니다. 이 검색 방법을 변경하면 최종 출력 품질을 향상시킬 수 있습니다.\n\n다음은 코드입니다:\n\n```js\n# 필요한 라이브러리 및 모듈 가져오기\nimport pandas as pd\nfrom datasets import Dataset\nfrom ragas import evaluate\nfrom ragas.metrics import (\n        context_precision,\n        faithfulness,\n        answer_relevancy,\n        context_recall\n)\nfrom langchain_community.chat_models import ChatOllama\n\n# 문서에서 Chroma 벡터 저장소 생성\n# 사용자 정의 임베딩을 사용하고 코사인 유사도 검색으로 변경\nvectorstore = Chroma.from_documents(collection_name=\"dist\", \n                                    documents=docs_splits, \n                                    embedding=DPRQuestionEncoderEmbeddings(), \n                                    collection_metadata={\"hnsw:space\": \"cosine\"})\n\n# 벡터 저장소로부터 리트리버 생성\nretriever = vectorstore.as_retriever()\n\n# 검색 증강 생성 (RAG) 체인 생성\nrag_chain_dist = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n    | {\"answer\": prompt | llm | StrOutputParser(), \n       \"context\": itemgetter(\"context\")})\n\n# RAG 데이터 세트 가져오기\nrag_dataset = get_questions_answers_contexts(rag_chain_dist)\n\n# 메트릭 계산\nresults = get_metrics(rag_dataset)\nprint(results)\n```\n\n이것이 결과입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n{\n  'faithfulness': 0.9444, \n  'answer_relevancy': 0.8504, \n  'context_precision': 0.6667, \n  'context_recall': 0.8889\n}\n```\n\n![Image](/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_5.png)\n\n'충실성'에서의 개선은 벡터 검색에 코사인 유사도를 사용하여 검색된 문서를 쿼리와 더 잘 일치시키는 것을 나타냅니다. '문맥 정밀도'가 감소했지만 전체적으로 높은 '충실성'과 '문맥 회수율'은 이 맥락에서 코사인 유사도가 보다 효과적인 벡터 검색 방법이라는 것을 보여주며, 검색 성능을 최적화하기 위한 벡터 검색 방법의 선택의 중요성을 뒷받침합니다.\n\n## 모델에 피드 할 최종 프롬프트\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최종 프롬프트 구성은 검색된 데이터를 모델의 쿼리에 통합하는 것을 의미합니다. 프롬프트의 작은 변화는 결과에 큰 영향을 미칠 수 있어 시행 착오 과정이 될 수 있습니다. 프롬프트 내의 예시를 제공하면 모델이 더 정확하고 관련성 높은 결과물을 생성할 수 있습니다.\n\n## 결론\n\n검색 증강 생성(Retrieval-Augmented Generation, RAG) 파이프라인을 최적화하는 것은 특정 데이터와 애플리케이션 컨텍스트에 매우 의존적인 반복적인 과정입니다. 본 논문에서는 네 가지 주요 전략을 탐구했습니다: 청킹 접근 방식의 개선, 임베딩 모델의 선택과 세밀한 튜닝, 효과적인 벡터 검색 방법의 선택, 정확한 프롬프트 작성. 이러한 구성 요소 각각이 RAG 시스템의 성능 향상에 중요한 역할을 합니다.\n\n결과는 일반적인 해결책이 없음을 강조했습니다. 예를 들어, 우리의 맥락에서 Dense Passage Retrieval (DPR)은 Sentence-BERT보다 우수한 성과를 보였지만, 이는 다른 데이터셋이나 요구 사항에 따라 달라질 수 있습니다. 마찬가지로, 벡터 검색에서 코사인 유사도로 전환하면 더 나은 충실도와 콘텍스트 회수가 나타나는 것으로 나타났는데, 검색 과정의 미묘한 변경이 영향을 미치는 것을 보여주었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRAG 파이프라인을 최적화하는 여정은 지속적인 테스트, 실패로부터 교훈을 얻고, 정보에 기반한 조정을 포함합니다. 이 반복적인 방식을 받아들이면, AI 솔루션을 더 효과적으로 사용자의 요구에 맞게 맞춤화할 수 있습니다. 성공의 열쇠는 데이터를 이해하고, 다양한 전략을 실험해보며, 끊임없이 프로세스를 개선하는 데 있습니다.\n\n내 프로필과 이메일 목록을 구독하여 최신 작업을 업데이트 받아보세요. 함께하면, AI 최적화의 복잡성을 탐험하고 데이터 기반 솔루션의 완전한 잠재력을 발휘할 수 있습니다. \n\n## 참고 자료\n\n[1] Es, S., James, J., Espinosa-Anke, L., \u0026 Schockaert, S. (2023). RAGAS: Automated Evaluation of Retrieval Augmented Generation. Exploding Gradients, CardiffNLP, Cardiff University, AMPLYFI.","ogImage":{"url":"/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_0.png"},"coverImage":"/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_0.png","tag":["Tech"],"readingTime":17},{"title":"대형 언어 모델LLMs로 Next-Token 예측을 분류로 전환하는 방법","description":"","date":"2024-06-23 19:36","slug":"2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs","content":"\n\n![이미지](/assets/img/2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs_0.png)\n\n대규모 언어 모델(Large Language Models, LLMs)은 방대한 양의 인터넷 데이터로 훈련되어 다양한 자연어 작업을 수행할 수 있습니다. 그 중 하나인 분류는 주제를 미리 정의된 레이블로 분류하는 지도 학습 작업입니다. 제로샷 및 퓨샷 분류는 인기 있는 기술로, LLMs가 훈련 데이터 없이 또는 몇 가지 예제로 분류 작업을 수행할 수 있습니다. 그러나 보다 정확도를 높이기 위해 가이드 미세 조정을 통해 LLMs의 성능을 향상시킬 수 있음이 입증되었습니다.\n\n# 가이드 미세 조정 LLMs\n\n가이드 미세 조정을 위한 일반적인 방법은 질문-답변 쌍으로 구성된 데이터셋을 작성하는 것입니다. 사전 훈련된 LLMs는 이러한 쌍을 사용하여 지도 학습 방식으로 추가로 미세 조정됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당신은 지난 포스트에서 이 접근법을 확인할 수 있어요.\n\n선택적으로, 데이터셋을 좋아하는 조합과 덜 선호하는 조합의 쌍으로 구성된 직접 선호도 최적화(DPO)를 사용하여 성능을 더 개선할 수 있어요. 상위 순위의 오픈소스 LLMs가 이러한 접근법 중 하나를 사용하여 훈련되는 것은 놀라운 일이 아니에요.\n\n# LLMs는 딥 뉴럴 네트워크입니다\n\n직관적으로, 이러한 방법은 LLMs가 토큰을 기반으로 하는 아키텍처를 사용한다는 사실을 활용해요. 지시된 데이터와 선호도 데이터셋 모두에서 텍스트 쌍이 토큰으로 변환됩니다. 교차 엔트로피 손실과 디코더만을 사용하는 오토레그레시브 속성을 이용하여, LLMs의 가중치가 업데이트되는데, 레이블 토큰은 입력 토큰으로부터 복사되지만 하나씩 밀려서 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주어진 것을 감안하면 우리는 어휘를 분류 라벨로 교체할 수 있습니다! 다음 토큰을 어휘에서 예측하는 대신, 앞선 문맥 토큰에서 분류 작업의 범주를 예측하는 데 관심이 있습니다. 이것은 일반적으로 lm_head로 구현되는 LLMs의 헤드를 변경함으로써 가능합니다. 텍스트 생성에서, lm_head는 (임베딩 차원, 어휘 크기)의 모양을 가지고 있습니다. 분류를 위해 우리는 이것을 (임베딩 차원, 분류 수)로 수정합니다.\n\n# 분류 작업을 위해 LLMs 학습하기\n\n## 모델 및 토크나이저 불러오기\n\n이 접근 방식이 작동하는지 확인하기 위한 실험을 수행해 봅시다. 먼저, HuggingFace에서 사전 훈련된 LLM 및 해당 토크나이저를 사용하여 시작하겠습니다. 본 연구에서는 경량화된 38억 개의 파라미터 모델 microsoft/Phi-3-mini-4k-instruct를 선택했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"microsoft/Phi-3-mini-4k-instruct\"\ndevice_map = \"auto\"\ntrust_remote_code = True\n\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name, device_map=device_map, trust_remote_code=trust_remote_code\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side = \"right\"\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\n```\n\n우리의 분류 실험을 간단히 하기 위해 이진 분류 작업을 선택할 것입니다. 나중에 여러 클래스로 확장할 수 있음을 알고 있습니다. 이진 분류에서 클래스 수는 두 개이므로 torch.nn.Linear(hidden_size, 2)입니다. to(\"cuda:`number`\") 함수는 이 레이어가 할당된 GPU 기기를 지정합니다. 이 레이어가 model_name = \"auto\"을 사용하여 초기로드된 모델이 있는 동일한 기기에 할당되었는지 확인해주세요.\n\n## Modify LLMs Head\n\n```python\n# 모델 수정 및 세밀 조정\nhidden_size = 3072\nmodel.lm_head = torch.nn.Linear(hidden_size, 2).to(\"cuda:3\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또 다른 흥미로운 실험은 fe-fine-tuning을 위해 필요한 레이어를 결정하는 것입니다. 이 게시물에서는 모델이 모든 다른 레이어에서 토큰의 문맥적 의미를 학습했다는 가정에 기반하여, 마지막 블록의 마지막 정규화 레이어에 초점을 맞출 것입니다. 이 레이어는 lm_head 이전의 끝에서 두 번째 레이어입니다.\n\n먼저, 모든 레이어의 Weight를 'param.requires_grad = False'로 지정하여 동결시킵니다. 그리고 나서 마지막 블록의 마지막 정규화 레이어를 찾아 'param.requires_grad = True'로 가중치를 조정 가능하도록 변경합니다. HuggingFace의 모델 클래스에서는 아래 코드 스니펫에서 보여주는 대로 dot 연산을 사용하여 어떤 레이어로든 이동할 수 있습니다.\n\n```js\n# 마지막 블록과 마지막 정규화 레이어만 fine-tune\nfor param in model.parameters():\n    param.requires_grad = False\n\n# 마지막 정규화 레이어만 fine-tune\nlast_block = model.model.layers[-1].to(\"cuda:3\")\nfinal_norm = model.model.norm.to(\"cuda:3\")\n\nfor param in final_norm.parameters():\n    param.requires_grad = True\n```\n\n## Cross Entropy Loss 정의\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전 게시물에서 lm_head의 가장 중요한 로짓이 문장의 마지막 토큰과 관련이 있다고 설명했습니다. 이는 self-attention 메커니즘에서 기인하는데, 마지막 토큰은 이전 모든 문맥 토큰들로부터의 주의 점수를 가지고 있습니다. 따라서 우리는 logits[:, -1, :]를 사용합니다.\n\n```python\nimport torch\n\ndef calculate_loss_batch(input_batch, target_batch, model):\n    input_batch, target_batch = input_batch.to(\"cuda\"), target_batch.to(\"cuda\")\n    logits = model(input_batch).logits[:, -1, :]  # 마지막 출력 토큰의 로짓\n    loss = torch.nn.functional.cross_entropy(logits, target_batch).to(\"cuda\")\n    return loss\n```\n\n## 데이터셋 준비\n\n데이터셋은 텍스트와 레이블 두 개의 리스트로 구성되어 있습니다. 먼저, 토크나이저를 사용하여 텍스트 문자열을 토큰화된 ID로 변환합니다. 토크나이저는 정의된 최대 길이보다 긴 토큰을 자르거나 최대 길이보다 짧은 경우 패딩합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\necoding = tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n)\n```\n\n그 다음으로, torch에서 DataLoader 객체를 사용하여 데이터를 모델 튜닝을 위해 반복적으로 공급합니다. BinaryClassification Dataset 객체는 torch의 Dataset 객체를 상속하며, 이 데이터셋을 DataLoader 객체로 로드합니다.\n\n```js\nclass BinaryClassificationDataset(Dataset):\n      pass\n\ndataset = BinaryClassificationDataset(texts, labels, tokenizer, max_length)\n\n# DataLoader\ndataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n```\n\n1단계와 2단계를 합쳐서, 이것이 완전한 구현입니다. 메인 함수에 예제를 제공하여 BinaryClassificationDataset 객체를 인스턴스화하고 DataLoader 객체를 만들어 데이터를 생성하는 방법을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\n\nclass BinaryClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].squeeze(0)  # Remove batch dimension\n        attention_mask = encoding['attention_mask'].squeeze(0)  # Remove batch dimension\n\n        return {\n            'input_ids': input_ids.long(),\n            'attention_mask': attention_mask.long(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    texts = [\"Hello, this is a sample sentence.\", \"Another sample text for classification.\"]\n    labels = [0, 1]\n\n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    # Define dataset\n    max_length = 128\n    dataset = BinaryClassificationDataset(texts, labels, tokenizer, max_length)\n\n    # DataLoader\n    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n\n    # Iterate through the dataloader\n    for batch in dataloader:\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        labels = batch['labels']\n\n        print(\"Input IDs:\", input_ids)\n        print(\"Attention Mask:\", attention_mask)\n        print(\"Labels:\", labels)\n      \n```\n\n## 모델 훈련\n\n손실 함수를 정의하고 데이터셋을 준비한 후, 모델 훈련을 시작할 수 있습니다. calculate_loss_loader 함수는 losses를 0으로 초기화합니다. 지정된 배치 수(num_batches)를 지정하면, calculate_loss_batch 함수를 사용하여 각 배치의 손실을 계산합니다. DataLoader 객체를 사용하여 calculate_loss_batch 함수에 배치 데이터를 반복적으로 제공하고, 각 반복에 대해 손실을 누적합니다. 재현성을 위해 torch.manual_seed(1234)를 사용합니다. 이 실험에서 0.68의 손실을 달성했습니다.\n\n```python\ndef calculate_loss_loader(data_loader, model, num_batches=None):\n    losses = 0.\n    if len(data_loader) == 0:\n        return \"Please provide dataset, data loader is empty.\"\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches = min(num_batches, len(data_loader))\n    for index, batch in enumerate(data_loader):\n        if index \u003c num_batches:\n            loss = calculate_loss_batch(\n              batch[\"input_ids\"], batch[\"labels\"], model\n            )\n            losses += loss.item()\n        else:\n            break\n    return total_loss / num_batches\n\n\ntorch.manual_seed(1234) # 재현성을 위해\nwith torch.no_grad():\n    train_loss = calculate_loss_loader(dataloader, model, num_batches=2)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 트레이드오프\n\n다음 토큰 예측에서 트랜스포머를 처음 원리를 통해 분류 예측으로 활용할 수 있다는 것은 흥미롭습니다. 미리 훈련된 트랜스포머 모델은 언어 패턴의 풍부한 표현을 포착합니다. 그러나 가벼운 모델이지만 38 억 개의 매개변수를 가지고 있습니다. 독자들은 정확성과 교육 그리고 추론 처리량(초당 생성된 토큰 수) 사이의 트레이드오프를 인식해야 합니다. 저는 기준선으로 더 작은 모델을 시작하는 것을 제안합니다. 예를 들어, 6700만 개의 매개변수를 가진 distilbert-base-uncased 모델을 사용할 수 있습니다. 또는 XGBoost와 같은 더 전통적인 머신러닝 모델을 시도할 수도 있습니다.\n\n# 결론\n\n본 블로그 포스트에서는 처음 원리를 사용하여 다음 토큰 예측 문제를 분류 레이블 예측으로 변환하는 방법을 보여드렸습니다. 이 데모를 통해 LLM의 복잡한 구조를 해체하고 도메인별 문제에 동일한 개념을 적용하는 지식을 습득할 수 있기를 희망합니다. 전체 분류 작업에 대해 트랜스포머가 최적화된 솔루션이 아닐 수 있으며, 다른 더 작은 모델과 데이터를 학습하고 평가해야 함을 상기해야 합니다. 읽어 주셔서 감사합니다. 행복한 학습 되세요!","ogImage":{"url":"/assets/img/2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs_0.png"},"coverImage":"/assets/img/2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs_0.png","tag":["Tech"],"readingTime":9},{"title":"서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기","description":"","date":"2024-06-23 19:35","slug":"2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification","content":"\n\n데이터와 컴퓨터 프로그램의 세계에서 머신 러닝이라는 개념은 어려운 문제 같을 수도 있어요. 복잡한 수학과 이해하기 어려운 개념이 가득한 것 같죠.\n\n그래서 오늘은 여기서 멈추어서, 제 MLBasics 시리즈의 새로운 이슈를 통해 모든 것이 어떻게 작동하는지 기본적인 사항을 살펴보고 싶어요.\n\n오늘의 안건은 서포트 벡터 머신을 이해하는 것이에요.\n\n이 강력한 도구는 데이터를 명확한 범주로 분류하는 데 도움이 되지만...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떻게 동작하는 건가요?\n\nSupport Vector Machines 모델을 간단히 설명해 보겠습니다👇🏻\n\n# Support Vector Machine이란?\n\nSupport Vector Machine (SVM)은 두 가지 다른 클래스로 데이터 포인트를 가장 잘 분리하는 초평면을 찾으려는 지도 학습 알고리즘입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 문제는 이를 수행할 수 있는 무한한 수의 초평면이 존재한다는 점이 어렵습니다. 그래서 SVM의 목표는 클래스를 최대 여백으로 가장 잘 분리하는 초평면을 식별하는 것입니다.\n\n![image](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png)\n\n# SVM의 주요 개념\n\n더 깊이 파고들기 전에, 몇 가지 핵심 용어를 이해해 보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Support Vectors(서포트 벡터): 이들은 초평면에 가장 가까운 데이터 포인트로, 초평면의 위치와 방향에 큰 영향을 미칩니다.\n- 여백(Margin): 여백은 초평면과 각 클래스에서 가장 가까운 데이터 포인트 사이의 거리입니다. 더 큰 여백은 분류기의 일반화를 더 잘 시킬 것입니다.\n- 초평면(Hyperplane): 2차원 공간에서 데이터를 두 부분으로 나누는 선입니다. 고차원에서는 평면이나 고차원의 유사 구조체입니다.\n\n![이미지](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_1.png)\n\n# SVM이 작동하는 방식\n\n두 종류의 데이터 포인트가 있는 데이터셋을 상상해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 파란색 🔵\n- 노란색 🟨\n\n새 데이터 포인트를 파란색 또는 노란색 중 하나로 분류하고 싶습니다. 주요 과제는 두 클래스를 분리할 수 있는 다양한 하이퍼플레인이 존재한다는 것인데, 그런 다음 큰 질문이 있습니다:\n\n어떻게 최적의 하이퍼플레인을 찾을까요?\n\n![이미지](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 좋은 초평면은 두 클래스로부터 최대 거리를 가지는 것입니다. 이는 가능한 다양한 초평면을 찾고 두 클래스로부터 최대 거리를 가지는 것을 선택함으로써 수행됩니다.\n\n# SVM 뒤에 숨겨진 수학적 직관\n\n데이터를 분류하는 방법을 이해하기 위해 수학적 측면을 살펴보겠습니다.\n\n점곱은 하나의 벡터를 다른 벡터에 따라 투영하는 것을 말합니다. 그래서 우리는 한 쪽의 점과 다른 쪽의 초평면이 어디에 있는지 결정하는 데 활용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n임의의 점 X를 고려해 보면:\n\n- 만약 X⋅W ` c 이면 — 이것은 양성 샘플입니다.\n- 만약 X⋅W ` c 이면 — 이것은 음성 샘플입니다.\n- 만약 X⋅W = c 이면 — 이것은 결정 경계 상에 있습니다.\n\n쉽죠?\n\n그러니까 조금 되감아보고 이 방정식들이 어디에서 왔는지 이해해 봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## #1. 하이퍼플레인을 찾는 방법 결정\n\n우리가 “분리선”을 얻기 위해, 서포트 벡터와 하이퍼플레인 사이의 거리 d를 먼저 계산할 수 있습니다. 여유 공간은 하이퍼플레인으로부터 가장 가까운 서포트 벡터까지의 거리의 두 배이며, 이 여유 공간 내에는 어떤 점도 있어서는 안 됩니다.\n\n## #2. 거리 “d” 투영\n\n거리 d는 두 서포트 벡터 사이의 차이를 하이퍼플레인의 법선 벡터 w의 방향으로 투영하면 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_3.png)\n\n여러분 중 많은 분들이 여기에 도착한 방법을 모르실 것 같아요. 그래서 한 발 물러나서 이 함수가 처음부터 무엇을 의미하는지 더 잘 이해해보도록 해요.\n\nA와 B라는 두 벡터가 있다고 상상해봅시다. 그들 사이에 θ도를 생성해요. 이 스칼라 곱을 사용하여 A가 B 위에 떨어지는 투영을 쉽게 찾을 수 있어요.\n\n즉, A의 B에 대한 투영을 찾을 수 있어요. A와 B 벡터를 알면 다음 수식에서도 확인할 수 있듯이요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서 이제 우리가 이 기본 원리를 이해했으니, SVM 모델로 돌아가 봅시다. SVM에 동일한 수학적 개념을 적용할 수 있습니다. 여기서 A는 지원 벡터 머신으로 정의된 벡터이고 B는 우리가 분할 초평면의 법선 벡터입니다.\n\n## #3. 제약 조건 정의하기\n\n이제 여백을 활용하여 제약 조건을 정의할 수 있습니다. 최대 여백 초평면이 (2D 예제에서) 선 방정식을 따라야 한다는 것을 알고 있습니다.\n\n이것은 초평면에 놓인 것은 양수 값을 가질 것이며(양쪽 초평면에 해당), 그 아래에 있는 것은 음수 값을 가질 것입니다(음쪽 초평면에 해당).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 두 초평면 사이의 간격을 \"마진\"이라고 합니다.\n\n## SVM의 마진\n\n마진은 SVM에서 중요한 개념으로, 초평면 주변에 데이터 포인트가 없는 버퍼 영역으로 작용합니다. 이 마진이 넓을수록 모델이 보이지 않는 데이터에 대해 일반화할 수 있으며, 과적합 가능성을 줄입니다.\n\n양수 또는 음수로 점을 분류하기 위해 초평면과의 상대적 위치를 기반으로 결정 규칙을 설정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 한 쪽에 있는 점들은 한 범주로 분류됩니다 (파란색 🔵)\n- 다른 한 쪽에 있는 점들은 반대 범주에 속합니다 (노란색 🟨).\n\n마진을 최대화함으로써 SVM은 의사결정 경계를 최적으로 배치하여 가능한 높은 신뢰도로 클래스를 분리합니다.\n\n그러면 어떻게 최대화할까요?\n\n# 최적화와 제약 사항\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSVM은 여백을 최대화하기 위한 최적화 문제를 해결하는 것을 포함합니다. 이는 선택한 초평면이 각 클래스의 가장 가까운 데이터 포인트에서 충분한 거리를 유지하도록 하는 것을 의미합니다. 이를 서포트 벡터라고 합니다.\n\n이미 이전에 발견한 선 방정식을 기반으로 한 분류 알고리즘이 있습니다. 그래서 출력을 다음과 같이 정의할 수 있습니다:\n\n- +1 또는 🔵는 양쪽의 데이터를 나타냅니다.\n- -1 또는 🟨는 음쪽의 데이터를 나타냅니다.\n\n![이미지](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 여전히 w 벡터와 b 매개변수를 찾아야 합니다.\n\n그래서... 어떻게 할까요?\n\n마진 경계에 위치하는 서포트 벡터는 우리의 양의 및 음의 초평면 내에 포함되어 있기 때문에 다음 제약 조건을 만족합니다.\n\n그래서 이를 쉽게 일반화할 수 있어요...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 일반 제약 조건 방정식\n\n모든 데이터 포인트 (x, y)가 마진을 넘어가지 않도록 하기 위해, 모든 데이터 포인트에 대한 제약 조건은 다음과 같이 요약될 수 있습니다:\n\n![equation](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_5.png)\n\n그리고 수행할 단계가 하나 더 남았습니다...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 최적화 목표\n\n이제 일반적인 제약 방정식을 가지고 있으므로, 벡터 w의 절대값을 최소화하면서 제약 조건을 충족시킬 수 있습니다.\n\n이는 다음과 같이 수학적으로 정의될 수 있습니다:\n\n![equation](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 최적화 문제를 해결함으로써, 클래스 간의 최상의 분리를 보장하는 최대 마진을 가지는 초평면을 정의하는 벡터 w와 b의 최적값을 찾을 수 있습니다.\n\n# 결론\n\n서포트 벡터 머신은 데이터 과학자의 무기 중 강력한 도구로, 이진 분류에 효과적인 방법을 제공합니다.\n\n클래스 간의 간격을 최대화하는 데 초점을 맞추면, SVM은 새로운 데이터에 대해 잘 일반화되는 견고한 분류기를 생성하여, 오버피팅의 위험을 줄입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSVM의 수학적 기반은 최적 초평면의 식별을 보장하여 다양한 분류 작업에 신뢰할 수 있는 선택지로 만듭니다.\n\n복잡한 데이터셋을 다루거나 모델 성능을 향상시키려는 경우, SVM에 대한 이해와 구현은 머신러닝 도구상자를 크게 향상시킬 수 있습니다.\n\nMLBasics 이슈를 좋아하셨나요? 그렇다면 DataBites 뉴스레터를 구독하여 최신 소식을 받아보세요!\n\n내용을 메일로 받아보실 수 있습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_7.png](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_7.png)\n\nX, Threads, LinkedIn에서도 만나볼 수 있어요! 거기서는 머신러닝, SQL, Python, 데이터 시각화에 관한 일일 치트시트를 올려요.\n\n다른 멋진 글도 여기 한번 확인해보세요! 😄","ogImage":{"url":"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png"},"coverImage":"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png","tag":["Tech"],"readingTime":6},{"title":"약물 발견의 출발점으로서 방대한 화학 공간을 구조 기반 가상 스크리닝하기 위한 방법","description":"","date":"2024-06-23 19:33","slug":"2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery","content":"\n\n\n![Structure-based virtual screening](/assets/img/2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery_0.png)\n\n가상 스크리닝은 유형의 분자 제약을 넘어서 널리 합성 가능한 화합물 억양 개 중에서 선도물을 찾을 수 있게 합니다. 현재 약물 설계를 위한 여러 방법들이 있지만, 아직 완벽한 방법은 없기 때문에 새로운 접근법의 개발은 새로운 목표와 도전이 이 분야에 오는 한 계속될 것입니다.\n여기서는 구조 기반 가상 스크리닝 접근법에 집중하고 있으며, 널리 사용되고 있는 새로운 기계 학습 기술과 시너지를 이루고 있는 것을 성공적으로 소개하고 있습니다.\n\n# 거대 라이브러리 도킹\n\n거대 라이브러리 도킹 방법은 여기서 논의된 다른 방법 중 약물 개발에 가장 오랫동안 사용된 방법 중 하나입니다. 높은 활성 물질 수가 이 방법으로 발견되어 왔으며, 서브나노 몰 활동까지 초점이 맞춰졌습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 초대형 라이브러리 도킹에 필요한 두 가지 필수 입력은 대상 구조와 스크리닝 데이터범입니다.\n\n단백질의 고해상도 X-선 및 cryoEM 구조 및 실험적인 것과 유사한 인공지능으로 얻은 구조들이 현재 사용 가능합니다. 여기서 가장 중요한 것 중 하나는 모델 성능을 벤치마킹하고 최적화하여 연구된 리간드와 단백질 간의 주요 상호작용을 복제하는 것이며, 이는 원래의 결정학 모델에서 발생하는 상호작용과 유사합니다.\n\n구조 기반 가상 스크리닝에서 이제 가장 자주 사용되는 데이터범 유형은 온디맨드 생성 라이브러리와 상용 인실리코 빌딩 블록에 기반한 집중 데이터범입니다. 첫 번째 데이터범은 빠른 아날로그 액세스를 제공하여 구조-활성-관계를 얻고 히트-투-리드 생성을 가속화할 수 있습니다. 그들의 단점은 특정 화합물종을 함유한 분자의 제한된 가용성입니다. 집중 데이터범은 원하는 프레임을 함유하는 대규모 라이브러리를 생성함으로써 이 제한을 해소합니다.\n\n도킹 모델이 교정된 후 분자 라이브러리를 가상으로 스크리닝할 수 있습니다. 분자들과 단백질 결합부위의 상호작용은 점수 함수를 사용하여 계산됩니다. 얻은 예측은 필터링되고 클러스터링됩니다. 이러한 필터는 주요 도킹 함수에서 놓친 문제 있는 특징을 포착하고, 알려진 리간드와의 상이함을 보장하며, 우선순위가 있는 화합물들 사이에서 다양성을 촉진할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, 획득한 리간드-생물학적 타겟 복합체를 시각적으로 검사합니다. 표적 단백질 결합 부위와 주요 상호작용을 형성하는 분자들은 후속 실험적 검증을 위해 선택됩니다.\n\n거대 라이브러리 도킹의 주요 문제는 최상위 등수 분자들 사이에 거짓 양성 결과가 누적된다는 점입니다. 상호작용 지문, 보다 정확한 리간드 변형 설명, 및 수용체 유연성은 이 상황을 극복하는 데 도움이 될 수 있습니다. 또한, 분자 도킹과 분자 역학 자유 에너지 계산의 조합과 같은 보다 엄격한 계산 방법을 사용하여 거대 화면에서 상위 순위 분자들의 작은 집합을 다시 점수 매기는 것으로 이 문제를 해결할 수 있습니다.\n\n지금은 딥러닝 기반 도킹 방법이 활발히 개발되어 거대 라이브러리 스크리닝의 속도와 정확도를 향상시키고 있지만, 유사하지 않은 타겟에 일반화하고 물리적으로 유효한 리간드 구조를 생성하는 모델의 능력에 대한 우려가 여전히 있습니다.\n\n# 기계 학습 가속 가상 스크리닝\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가상 스크리닝 파이프라인에 머신 러닝 방법을 통합하면 가장 큰 사용 가능한 온디맨드 라이브러리를 효율적으로 탐색할 수 있습니다. 이 접근 방식은 100배까지 다양한 화합물을 축소하고 동시에 최상위 점수화된 분자를 풍부하게 만들기로 입증되었습니다.\n\n머신 러닝 가속 가상 스크리닝 파이프라인의 본질적인 부분은 다음과 같은 순차적 단계에 따라 진행됩니다. 먼저, 관심 대상과의 관련 도킹 점수에 기반하여 온디맨드 라이브러리 샘플의 해당 훈련 세트가 생성됩니다. 그런 다음, 머신 러닝 모델을 훈련시키고 나머지 라이브러리의 점수를 예측하는 데 사용됩니다. 유망한 분자들은 머신 러닝 모델의 더 깊은 개선(액티브 러닝) 또는 실험적 평가에 사용됩니다. 원하는 결과가 달성될 때까지 이러한 작업들은 반복될 수 있습니다.\n\n머신 러닝 가속 가상 스크리닝의 주요 단점이 몇 가지 있습니다. 첫째로, 더 정교한 알고리즘으로 인해 상당한 계산 비용이 필요하며 도킹 점수 기능의 정확도가 낮아 실제 잠재성을 충분히 발휘하지 못할 수 있습니다. 둘째로, 사용 가능한 데이터셋이 특정 유형의 화합물이나 대상으로 편향될 수 있어 새로운, 보지 못한 화합물에 대한 모델의 일반화 능력이 제한됩니다. 셋째로, 일부 최상위 점수 분자가 생물학적 실험에서 무효일 가능성이 있지만, 이들이 훈련 세트에 포함된다면 머신 러닝 모델이 가상 스크린의 높은 거짓 양성률을 전파하거나 심지어 증가시킬 수 있습니다.\n\n**# 프래그먼트 기반 가상 스크리닝**\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n분자 단편의 내재적으로 낮은 분자 복잡성은 그들이 대상 단백질 결합 부위를 보완하고 vitro에서 테스트될 가능성을 높입니다. 그러나 대형 분자와는 달리 단편은 높은 효능이나 선택성을 갖지 않으며, 추후 광범위한 화학적 발전이 필요하여 선도 후보를 얻기 위해. 새로운 단편 기반 가상 선별 방법으로 이 과정을 최적화하는 것이 가능해졌습니다.\nV-SYNTHES 방법은 대규모 화합물 모델링 도전을 작은 단계로 분해하여 현재 명시적 도킹으로 달성할 수 있는 것보다 몇 차원 더 큰 라이브러리 탐색으로 탐색합니다. 이러한 방법은 대형 라이브러리 도킹의 주요 한계인 화학 공간 크기에 선형적으로 비례하는 컴퓨팅 리소스의 상당한 증가를 다룹니다.\nV-SYNTHES에는 라이브러리 준비, 열거, 도킹 및 히트 선택의 반복적 단계가 포함됩니다. 준비 과정에서 단편 유사 화합물 라이브러리가 생성되며 (\"최소 열거 라이브러리\"(MEL)), 사용된 화학 라이브러리 공간 전체에 대한 모든 반응의 골조-신톤 조합의 모든 가능성을 대표합니다. 다음 단계에서, MEL 화합물은 대상 수용체에 도킹됩니다. 최상의 점수를 받은 신톤이 선택되고 해당 골조에 초점을 맞춘 화학 라이브러리는 반복적인 열거와 도킹이 수행됩니다. 반복 횟수는 화학 공간에서의 전체 화합물을 나타내는 분자가 완성될 때까지 되풀이됩니다. 마지막으로 라이브러리의 최종 열거된 하위 집합에 도킹 스크린을 수행하고 최우선 히트는 방해 화합물, 물리-화학적 특성, 약물 유사성, 독창성 및 화학 다양성에 대한 후처리 필터링을 거칩니다. 그 결과 화합물 세트가 합성되어 실험적으로 테스트됩니다.\n최근 제안된 기계 학습 방법인 FRAME은 구조 안내 방식을 사용하여 단편의 점진적 발전을 기반으로 합니다. 여기서 딥 러닝은 초기 단편에서 결합된 단백질 구조에 기초하여 적합한 성장 벡터를 식별하고 SE(3)-등변 신경망 활용을 통해 다양한 화학적 발전을 평가합니다. FRAME의 주요 단점은 제한된 합성 접근성으로 이어질 수 있다는 것입니다.\n\n저희 Chemspace는 Enamine REAL Space에 접근하여 V-SYNTHES를 사용하여 속도와 효과면에서 가장 효과적인 구조 기반 가상 리간드 스크리닝 방법을 사용합니다. 저희 전문가들이 최단 시간 내에 당신이 원하는 대상에 대한 최상의 히트 화합물을 기가 스케일 라이브러리에서 검색하는 데 도와드릴 것입니다. 본 서비스에 대해 자세히 읽을 수 있습니다.\n\n본 검토는 다음 논문을 바탕으로 작성되었습니다: J. Carlsson et al., Curr. Opin. Struct. Biol. 2024; https://doi.org/10.1016/j.sbi.2024.102829","ogImage":{"url":"/assets/img/2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery_0.png"},"coverImage":"/assets/img/2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery_0.png","tag":["Tech"],"readingTime":4},{"title":"BERT 미세 조정으로 텍스트 분류하는 방법","description":"","date":"2024-06-23 19:32","slug":"2024-06-23-FinetuneBERTfortextclassification","content":"\n\n섬세 조정은 대형 언어 모델이 사용자 지정 데이터에 적응하고 텍스트 분류와 같은 하향 작업을 잘 수행할 수 있도록 돕는 중요한 기술입니다.\n\n본 문서는 섬세 조정의 기본에 초점을 맞추고, LORA, QLORA 등 다른 기술에 대해 깊게 다루지는 않습니다. 시작하는 가장 좋은 방법은 BERT로 실험을 해보는 것입니다.\n\n주로 두 가지 방법으로 이 작업을 수행할 수 있습니다:\n\n- 허깅페이스 트레이너 API 사용: 사용하기 쉽지만 매우 사용자 정의가 어려움\n- PyTorch 사용: 트레이너보다 조금 어려우나 프로세스에 대한 더 많은 사용자 정의와 제어를 제공합니다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터셋\n\n우리는 Hugging Face에서 제공하는 Yelp Reviews 데이터셋을 사용할 예정입니다. 이 데이터셋은 다음 두 열로 구성되어 있습니다:\n\n- 레이블: 1부터 5까지의 별표가 부여된 등급입니다.\n- 텍스트: 리뷰 내용입니다.\n\n저희의 목표는 리뷰 텍스트로부터 별의 개수를 예측할 수 있는 모델을 훈련하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n휍핑페이스 트레이너 API를 사용하여 파인튜닝하기\n\n- 모든 라이브러리를 설치하세요 :\n\n```js\n!pip install --upgrade transformers datasets evaluate huggingface_hub torch\n```\n\n참고: 이 라이브러리들의 최신 버전을 항상 사용하도록 하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. 데이터셋 로드: Hugging Face에서 제공하는 datasets 라이브러리를 사용하여 데이터셋을 로드할 수 있어요.\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"yelp_review_full\")\n```\n\n데이터셋을 확인해봐요. 어떤 데이터를 다루게 될지 알아봅시다.\n\n```python\ndataset[\"train\"][1]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리 데이터가 어떻게 보이는지 확인해보세요.\n\n```js\n{'label': 1,\n 'text': \"안타깝게도 Dr. 골트버그의 환자로서 느끼는 좌절은 뉴욕의 다른 많은 의사들과 겪어온 경험의 반복입니다 - 좋은 의사, 하지만 최악의 스태프. 그의 스텝은 단순히 전화를 받지 않는 것 같습니다. 답변을 받으려면 보통 반복적인 전화로 2시간이 걸립니다. 누가 그런 시간을 가진 사람이며 누가 그것과 소통하길 원하겠습니까? 다른 많은 의사들과도 이 문제를 겪어왔고, 이해가 안 가네요. 사무원이 있고 의료 필요가 있는 환자가 있는데, 왜 전화를 받는 사람이 없는 건지요? 이해할 수 없고, 신경질만 나게 합니다. Dr. 골트버그에게 2점을 주어야 하는 점이 유감입니다.\"}\n```\n\n3. 토크나이저를 로드하고 텍스트를 토큰화하는 함수를 만들어보세요:\n\n```js\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n토크나이저는 텍스트를 입력_ids, 토큰_유형_ids 및 어텐션_마스크로 이해할 수 있는 세 개의 열로 변환합니다.\n\n데이터셋에서 작은 배치를 만들기(선택 사항)\n\n```js\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n```\n\n4. 모델 불러오기:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n```\n\n분류할 레이블 수를 초기화하려면 num_labels 매개변수를 사용하세요.\n\n5. 훈련 인수 초기화\n\n```python\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nhttps://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments에서 제공되는 매개변수에 대한 자세한 정보를 확인할 수 있습니다.\n\n6. 메트릭 계산 함수 설정:\n\n```js\nimport numpy as np\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n```\n\n7. 학습 시작:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom transformers import Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()\n```\n\n트레이닝을 시작하려면 wandb 키를 입력하라는 프롬프트가 나타납니다. 키를 입력하면 트레이닝 프로세스가 시작됩니다. 트레이닝이 완료되면 아래와 같은 결과를 보게 될 것입니다.\n\n![트레이닝 결과](/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png)\n\n선택적으로 노트북에서 허깅페이스로 모델을 저장할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\nfrom huggingface_hub import login\nlogin()\nmodel.push_to_hub(\"HuggingfaceUsername/yourModelName\")\n```\n\n8. 추론 실행:\n\n모델을 테스트하려면 PyTorch를 사용할 수 있습니다.\n\n```js\nimport torch\nimport torch.nn.functional as F\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"HuggingfaceUsername/yourModelName\")\ns=\"The was awesome and I loved it\"\ntt=tokenizer(s,return_tensors=\"pt\", padding=True, truncation=True)\n```\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델을 평가 모드로 설정하면 더 이상 가중치를 업데이트할 필요가 없어지고, 이제 분류 작업에 사용할 수 있습니다.\n\n```python\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(**tt)\n```\n\n결과를 확인해보겠습니다.\n\n```python\nSequenceClassifierOutput(loss=None, logits=tensor([[-2.3995, -2.0111, -0.8381,  2.4683,  2.8968]]), hidden_states=None, attentions=None)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 중요한 변수는 로짓 변수입니다. 이 경우 로짓은 텍스트가 특정 클래스에 속할 확률을 나타냅니다. 현재 로짓은 이해하기 어려운 형식으로 표시됩니다. 이를 이해할 수 있는 형식으로 변환하려면 이해할 수 있는 숫자로 변환해야 합니다.\n\n```js\nlogits = outputs.logits\nprint(\"로짓:\", logits)\n\n# 소프트맥스를 사용하여 로짓을 확률로 변환합니다\nprobabilities = F.softmax(logits, dim=-1)\nprint(\"확률:\", probabilities)\n\n# 예측된 클래스를 결정합니다\npredicted_class = torch.argmax(probabilities, dim=-1)\nprint(\"예측된 클래스:\", predicted_class.item())\n```\n\n여기서 출력은 4입니다.\n\nPyTorch를 사용한 파인 튜닝\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델이 이해할 수 있도록 몇 가지 전처리 단계가 필요합니다.\n\n- 열 삭제\n\n```js\ntokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\ntokenized_datasets.set_format(\"torch\")\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n```\n\n2. 데이터로더(Dataloader) 생성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport torch\nfrom torch.utils.data import DataLoader\ntraindataloader=DataLoader(small_train_dataset,batch_size=8,shuffle=True)\ntestdataloader=DataLoader(small_eval_dataset,batch_size=8)\n```\n\n3. 모델을 다운로드하고 GPU에 로드해주세요.\n\n```js\nfrom transformers import AutoModelForSequenceClassification\nmodel=AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n```\n\n4. 옵티마이저(optimizer)와 학습률 스케줄러(learning rate scheduler)를 생성하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom torch.optim import AdamW, SGD\nfrom transformers import get_scheduler\noptimizer = SGD(model.parameters(), lr=5e-5)\nnum_epochs = 3\nnum_training_steps = num_epochs * len(traindataloader)\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n```\n\n원하는 옵티마이저와 학습률 스케줄러를 조정하여 가장 적합한 것을 선택할 수 있어요.\n\n5. 학습 및 평가\n\n모델을 model.train()을 사용하여 학습 모드로 설정해주세요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in traindataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n```\n\n```js\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\nmodel.eval()\nfor batch in testdataloader:\n    b = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**b)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n\nmetric.compute()\n```\n\n제 Kaggle 노트북에서 스크립트를 확인할 수 있습니다. https://www.kaggle.com/code/exterminator11/finetune-bert. 행운을 빕니다!","ogImage":{"url":"/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png"},"coverImage":"/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png","tag":["Tech"],"readingTime":8},{"title":"자율 에이전트의 흥망성쇠 그 발전과 한계 ","description":"","date":"2024-06-23 19:30","slug":"2024-06-23-TheRiseandFallofAutonomousAgents","content":"\n\n2023년 ChatGPT가 인기를 끌자, 창조적 AI 공간에서 골드 러시 분위기가 등장했습니다. 전 세계적으로 사람들은 미래에 대한 AI의 변혁적 잠재력을 인식했습니다. 이 골드 러시적 마인드셋은 우리를 핵심 질문으로 이끕니다: 여기에는 골드가 어디에 있을까요?\n\n다른 말로 하면, 미래에는 어떤 일이 일어날까요? 보통 우리에게는 미래로 멀리 전망하는 것이 어려우며, 다가오는 변화를 예측하기 위해 짧은 시간대에 초점을 맞추는 것이 더 좋습니다. 그러나 창조적 AI 분야에서는 이것이 다르다고 보이죠. 우리가 향하는 방향을 알 수는 있겠지만, 다음에 무엇이 올지 예측하는 것은 널리 어렵습니다.\n\n2023년 4월, AutoGPT와 BabyAGI와 같은 자율 에이전트 워크플로우가 GitHub에서 인기를 얻기 시작했습니다. 인기가 폭증하며, AutoGPT는 딱 한 달 만에 5만 명 이상의 프로그래머들의 주목을 끌었습니다.\n\n![이미지](/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 자율 에이전트 워크플로 이해\n\n자율 에이전트 워크플로의 구체적인 내용을 알아보기 전에 먼저 '에이전트'가 무엇을 의미하는지 명확히 해보겠습니다. '에이전트'라는 용어는 컴퓨터 과학의 맥락에서 20세기로 거슬러 올라가는 뿌리를 가지며, 1980년대 후반에 큰 인기를 얻었습니다.\n\n일반적으로, 에이전트는 \"행동\"이 가능한 개체입니다 (‘agency' 개념에서 파생됨). 에이전트는 다음과 같은 세 가지 주요 기능으로 정의될 수 있습니다:\n\n- 지각: 센서 또는 텍스트 입력을 통해.\n- 결정: 인식에 기반한 결정을 내림.\n- 행동: 그 결정에 기반한 행동을 실행함.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2023년 초에 발표된 논문 'ReAct: 언어 모델에서의 추론과 실행의 시너지 효과'는 특히 대규모 언어 모델의 맥락에서 마일스톤을 달성했습니다. 인기 있는 프레임워크인 LangChain은 ReAct 로직을 구현하여 자료와 관련된 테마에 상당 부분의 리포지토리를 할애했습니다. 이 접근 방식은 에이전트 논리를 보다 넓은 관중에게 접근 가능하게 만들면서 에이전트 논리의 최신 발전을 통합했습니다.\n\n에이전트를 뛰어넘게 하는 것은 에이전트에게 일련의 도구를 제공하여 대규모 언어 모델의 훈련 데이터의 한계를 극복하는 능력입니다. 이러한 도구는 본질적으로 소프트웨어 함수로, 에이전트가 API 요청(예: Google 검색 쿼리 실행), 웹 사이트 읽기, 프로젝트 보드에 액세스, 계산 수행, 데이터베이스에서 SQL 쿼리 실행 또는 보호된 환경에서 코드 작성 및 실행과 같은 작업을 수행할 수 있게 합니다. 도구는 대규모 모델이 외부 세계와 상호작용할 수 있게 합니다. 모델은 원하는 결과를 달성하기 위해 실행할 함수를 결정할 수 있습니다.\n\nReAct 로직은 각 언어 모델 프로세스 단계에서 'Thought', 'Act/Action' 및 'Observation' 요소를 포함합니다. 'Thought'는 추론을 통해 다음 작업과 그 근간에 있는 결정을 향상시킵니다. 'Action'은 어떤 도구를 사용할지와 어떤 매개변수를 사용할지를 지정하며, 'Observation'은 'Action'에 따라 도구 실행 결과를 포함합니다. 초기 조사에 대한 답변을 위해 이러한 단계들이 필요한 정보가 수집될 때까지 반복됩니다.\n\n![이미지](/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# OpenAI에서 구현됨\n\nOpenAI의 중요한 역할은 에이전트의 잠재력에 대한 인식으로 두드러지며, 이를 기능으로 통합하여 ChatGPT 플러그인 스토어를 열어두었습니다. 또한, GPT-XX-0631 이후의 OpenAI 모델 업데이트는 JSON 출력을 더 잘 제공할 수 있도록 지속적으로 개선되었습니다. 이 향상은 도구, 플러그인 또는 기능을 신뢰할 수 있게 실행하는 데 주로 필수적입니다. 그 이후 API는 또한 GPT 모델 쿼리에서 반환된 텍스트 블록과 별도로 함수의 정의 및 실행을 허용합니다. 일반적으로 JSON 출력은 함수 실행에 필요한 매개변수를 정의하므로, 전통적인 오픈 소스 프로젝트의 경우 (MistralAI와 같이 API 클라이언트에서 함수 호출을 제공하는 몇 가지 예외를 제외하고) 출력에서 추출해야 합니다.\n\n# 자율 에이전트 워크플로우\n\n자율 에이전트로 돌아와서, 클래식 ReAct 에이전트가 특정 작업이나 질문을 다루는 데 설계되었다면, 자율 에이전트 워크플로우는 한 걸음 더 나아갑니다. 작은 작업이 아니라 넓은 목표가 초기 입력으로 표현됩니다. 일반적으로 단일 프로세스 단계는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- (LLM) - 작업/목표를 하위 작업으로 분해합니다.\n- 작업을 작업 풀에 추가합니다.\n- (LLM) - 작업 풀을 우선 순위로 나열합니다.\n- 가장 중요한 작업을 선택합니다.\n- (LLM-Agent) - 가장 중요한 작업을 완료합니다.\n- (LLM-Agent) - 다음 작업을 정의합니다.\n\n다시 말해, '에이전트'들은 이제 '프로젝트 관리 에이전트'에 의해 조정되며 큰 작업을 목표를 달성할 때까지 처리 가능한 작은 작업으로 나누어줍니다. 이 프로세스는 새로운 도구를 만들고, 새로운 에이전트를 조정하고, 정보를 단기 또는 장기 기억에 저장하는 것을 포함합니다. 이 이터레이션 프로세스는 이론적으로 거의 모든 작업을 해결할 수 있습니다. 사용자들은 대형 언어 모델 (GPT-4)에 의해 생성된 창의적인 솔루션을 보고했습니다. 워크플로는 웹사이트나 프로필을 생성할 때 캡차를 우회하기 위해 작업에 막혔는데, '인간'을 위해 작업 플랫폼 (Fiverr)에 작업을 게시하여 문제를 해결했습니다.\n\n# 다중 에이전트 협업 워크플로\n\n우리는 독립적인 에이전트 워크플로가 개별 에이전트의 능력을 활용하여 특정 기능을 수행하는 능력을 증진함으로써 복잡한 작업을 해결할 수 있는 능력을 크게 향상시켰다는 것을 보았습니다. 그러나 이러한 기술의 진정한 잠재력은 독립적인 에이전트보다는 다중 에이전트의 협력 노력에 더 명백해집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다중 에이전트 협업은 특화된 역량을 갖춘 각종 에이전트들이 협력하여 단일 에이전트 시스템으로는 해결할 수 없는 복잡한 목표를 달성하는 것을 포함합니다. 이 방식은 여러 에이전트의 집단적 강점을 활용하여 도전적인 문제에 대한 더 정교하고 확장 가능하며 유연한 해결책을 제공합니다. 성공적인 다중 에이전트 협업의 핵심은 다양한 에이전트의 행동을 조율하고 공통 목표를 향한 효과적인 의사 소통과 협력을 보장하는 능력에 있습니다.\n\nAutoGen은 대규모 언어 모델(LLMs)을 활용하는 워크플로의 조율, 최적화 및 자동화를 촉진하는 Microsoft의 프레임워크입니다. AutoGen의 핵심 아이디어는 특화된 역할과 기능을 갖춘 각 에이전트들이 더 효과적으로 협력할 수 있는 시스템 설계를 용이하게 하는 것입니다. 이러한 에이전트들은 자율적으로 작업을 수행하거나 서로 간 대화를 나누며, 프로그래밍 및 사람 사용자 또는 다른 도구로부터의 입력을 바탕으로 결정을 내릴 수 있습니다. 이는 단일 에이전트나 순수히 인간 팀이 다루기 어려운 복잡한 문제를 해결하는 더 동적이고 유연한 방식을 가능케 합니다.\n\nAutoGen은 두 가지 주요 기능을 통해 이를 실현합니다:\n\n- 개발자가 특화된 기능과 역할을 갖춘 에이전트들의 집합을 정의함으로써, 이러한 에이전트들의 모듈화와 재사용성을 강화합니다.\n- 에이전트 간 상호작용 행동을 위한 프레임워크를 수립하여, 미리 정의된 프로토콜 또는 자동화된 채팅을 통해 효과적으로 소통하고 협력할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_2.png)\n\n# 도전과 제한 사항\n\n클래식 ReAct 에이전트와 비슷한 원칙으로 디자인된 에이전트들은 능력에 제한이 있습니다. 이 제한은 토큰 창문 제약 때문에 발생하는데, 여기서는 어떻게 그리고 언제 사용해야 하는지에 대한 설명이 정의될 수 있는 기능의 수가 제한됩니다. 단일 작업 또는 도구로 구비된 에이전트는 만족스러운 결과를 산출할 수 있지만, 도구의 수가 증가함에 따라 신뢰성이 감소합니다.\n\n토큰 창문의 확장이 있더라도, \"중간\" 문제가 지속됩니다. 즉, 입력 프롬프트의 중간에 위치한 정보가 덜 주목을 받습니다. 따라서 16,000 토큰을 초과하는 문맥 창문 내에 수백 개의 도구를 수용할 수 있는 잠재력이 있더라도 대부분은 여전히 간과됩니다. 게다가, 의사 결정은 여전히 과정에서 중요한 도전 과제입니다. 제한된 도구 수가 있더라도, MultiActionAgents는 적합한 도구를 선택하거나 모든 가능한 도구를 활용해야 한다고 믿는 것에 어려움을 겪습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 자율 에이전트의 과제\n\n자율 에이전트 개발이 미진한 이유 중 하나는 그들이 만들어내는 오픈 루프 시스템에 따른 비용 때문입니다. 보통 이러한 워크플로우는 가장 효율적인 해결책으로 이끌어주지 않습니다. 대신 계속하여 새로운 작업을 정의합니다. 오케스트레이션 에이전트들은 작업 풀을 모니터링하는 데 어려움을 겪어야 합니다. 한 번 이상으로 정의된 작업이 없도록 보장하는 것이 중요합니다. 동시에 최선의 결과를 얻으려면 최신 모델인 GPT-4를 사용해야 하며, 간단한 목표조차도 수천 번의 LLM 호출을 유발할 수 있어 빠르게 비용이 증가할 수 있습니다.\n\n뿐만 아니라, 시스템은 종종 막다른 곳에 이르게 되어 폐쇄 루프를 만들어냅니다:\n\n- 작업 A는 작업 B와 C에 의해 완료되어야 하며,\n- 그리고 작업 C의 분할 결과로써 작업 A가 다시 정의되는 일이 발생합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델 업데이트의 관찰 가능한 저하와 게으름으로 문제가 악화되고 있습니다. 저는 연구를 수행하고 발견물로부터 PowerPoint 프레젠테이션을 작성할 수 있는 반자동 워크플로우를 개발했었는데, GPT-3.5 turbo로 시도 중 3번 중 2번은 성공했습니다. 하지만 GPT-4에서는 이러한 결과를 재현할 수 없었습니다.\n\nLangChain의 미리 정의된 agent + 도구킷에서도 비슷한 상황이 나타납니다. 예를 들어, Pandas 및 SQL agent는 2023년 중반까지 신뢰할 수 있는 결과를 제공했지만 이제 80%의 경우에 오류로 이어지고 있습니다. 이는 agent의 인기가 점차 감소하거나 적어도 침체로 이어지고 있다는 것을 보여줍니다.\n\n# 해결 방안\n\n덜 발전된 모델일지라도 실행 로직과 프롬프트 엔지니어링을 통해 개선의 여지가 있습니다. 한 가지 전략은 MultiActionAgent를 사용하는 대신 agent가 수행할 수 있는 작업 범위를 좁히는 것입니다. 각 도구에 대해 SingleActionAgent가 생성되고, 적합한 SingleActionAgent를 선택하는 Routing Agent가 지정됩니다. 이 방법은 GPT-3.5 turbo와 같이 강력하지 않은 모델에도 만족스러운 결과를 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 이 방법은 이전의 종단 간 에이전트 전략과 비교해 개발 노력이 상당히 많이 필요합니다. 더 비용 효율적인 모델을 활용하도록 솔루션을 분해하면 재정적으로 한 번에 더 많은 LLM 호출을 할 수 있습니다. 이를 통해 주요 투표의 실행이나 자가 비평 방법을 사용하여 하나의 LLM 호출에 대한 고려 사항의 반복적인 유효성 검증이 가능해집니다.\n\n프로세스를 분할하는 것이 가장 합리적인 해결책 중 하나로 보입니다. 현재 에이전트 워크플로우는 한 프롬프트 접근 방식에 의존하며, 결정, 문제 해결 및 추가 프로세스는 반복적으로 해결되고 동일한 프롬프트로 이루어지도록 의도되어 있습니다. 프롬프트 엔지니어링에 익숙한 사람들은 특정 상황에는 특정 프롬프트가 필요하다는 것을 알고 있습니다.\n\n# 전망\n\n현재 직면한 문제들이 시간이 지남에 따라 완화될 것입니다. 더 강력한 모델이 시장에 등장하고 오늘날의 최고 모델들이 더 저렴해질 것입니다. 시장이 많은 비판하는 점진적인 악화를 수정할 수 있는 능력을 가지고 있다고 확신합니다. 필요한 곳에는 해결책이 따를 것입니다. 의심의 여지 없이, 생성모델의 다음 성취는 우리의 능력을 혁신할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 사용성이 매우 낮고 신뢰할 수 없지만, 미래는 모든 종류의 프로세스에서 활용될 AI 엔티티(에이전트)에 의해 형성될 것입니다. 컴퓨터 과학 분야의 최근 논문은 에이전트 프로세스(시행착오 학습)에 보상 학습 모델을 통합하는 해결책을 탐구하고 있습니다. 이는 에이전트가 보상 모델을 정의하고 훈련시킨 다음, 극도로 복잡한 작업을 해결하는 도구로 사용할 수 있다는 것을 의미합니다.\n\n에이전트와 자율 에이전트는 의심의 여지 없이 인공 일반 지능(AGI) 달성을 향한 여정에서 중요한 역할을 할 것입니다.\n\n## 출처:\n\n- Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., \u0026 Zhou, E. (2023). The Rise and Potential of Large Language Model Based Agents: A Survey. arXiv:2309.07864에서 검색됨\n- Yao, S., et al. (2022). REACT: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629에서 검색됨\n- LangChain. (n.d.). 에이전트 유형: REACT. LangChain 문서에서 확인됨\n- Significant-Gravitas. (n.d.). AutoGPT. GitHub에서 확인됨\n- Microsoft Research. (n.d.). AutoGen: 다음 세대 대형 언어 모델 애플리케이션 활성화. Microsoft Research 블로그에서 확인됨\n- Microsoft Research. (n.d.). AutoGen 프로젝트 페이지. Microsoft Research 프로젝트에서 확인됨","ogImage":{"url":"/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_0.png"},"coverImage":"/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_0.png","tag":["Tech"],"readingTime":8},{"title":"대형 언어 모델LLM로 민감한 데이터 처리하는 방법 총정리","description":"","date":"2024-06-23 19:28","slug":"2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels","content":"\n\n## 목차\n\n소개\n\n데이터 민감도는 무엇을 정의하고 있으며 누가 정의하고 있나요?\n데이터 익명화와 익명변환은 무엇인가요?\n민감한 데이터를 처리하기 위해 AI를 활용하는 것이 특별한 이유는 무엇인가요?\n\n실습 안내 — LLM 기반 데이터 프로파일러 구현\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로컬 LLM 설정\n1. 도커를 사용하여 모델 서버 설정하기\n2. 프롬프트 빌드하기\nAzure OpenAI 설정\n\n고수준 솔루션 아키텍처\n결론\n참고 자료\n\n예상되는 하루 평균 데이터 생성량은 328.77 백만 테라바이트입니다. 대부분의 데이터는 데이터 주도형 애플리케이션으로 흐르며 매초마다 처리되고 풍부해집니다. 주요 제품들 사이에서 LLM의 채택과 통합이 확대되어 텍스트 데이터 활용의 사용 사례와 이점이 더욱 증가했습니다.\n\n대규모 데이터를 처리하는 조직은 민감한 데이터 처리 요구 사항을 준수하는 데 어려움을 겪습니다. 이는 데이터 보안이나 데이터 법률 및 규정을 준수하는 것과 관련이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n민감한 데이터 침해의 직접적 및 간접적 영향은 특히 민감한 데이터가 관련될 때 기관에 중대한 재정적 결과를 초래할 수 있습니다. 이는 즉각적인 비용 영향을 넘어서 해당 기관의 고객 기반의 신뢰와 충성을 흔들어놓을 수 있습니다.\n\n![Image](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_0.png)\n\n# 무엇이 그리고 누가 데이터의 민감성을 정의할까요?\n\n민감한 데이터는 데이터 보호와 개인 정보 보호 맥락에서 중요한 개념입니다. 높은 수준에서 민감한 데이터는 비밀유지되고 무단 접근으로부터 보호되어야 하는 정보로 이루어져 있습니다. 민감한 데이터의 중요성은 그것을 보호하기 위해 마련된 법률 및 규정들에 의해 강조됩니다. 예를 들어, EU의 일반 데이터 보호 규정(GDPR)과 캘리포니아 소비자 개인 정보 보호 법(CCPA) 등 다양한 국가에서 적용되는 법률 및 규정이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n민감한 데이터에는 다음과 같은 다양한 정보 카테고리가 포함됩니다:\n\n- 개인 식별 가능 정보 (일반 데이터 보호 규정)\n- 보호된 건강 정보 (건강 보험 이동성 및 책임성 법)\n- 교육 정보 (가족 교육 권리 및 개인 정보 보호 법)\n- 기밀 비즈니스 정보\n- 금융 정보\n- 고용 정보\n- 법적 정부 발급 정보\n- ...\n\n게다가 GDPR은 특히 개인 식별 가능 정보(PII)와 관련하여 민감한 개인 데이터의 특별한 카테고리를 다음과 같이 명시하고 있습니다:\n\n- 인종이나 민족 출신\n- 정치적 견해\n- 종교나 철학적 신념\n- 노동 조합 가입\n- 유전자 정보\n- 생체 인식 정보\n- 건강 정보\n- 성생활이나 성적 취향\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n법적 프레임워크에 따라 민감한 데이터가 무엇인지에 대한 명확한 설명과 경계가 존재하지만, 기관이 운영중인 도메인이나 섹터에 따라 이러한 내용은 더 많은 민감한 기밀 정보를 포함할 수 있습니다. 이로써 기관은 자체 데이터 기밀성 분류를 정의하고 해당 분류를 데이터 민감도 도구에 통합하거나 사용자 정의 솔루션에 통합하는 것으로 나아갈 것입니다.\n\n# 데이터 익명화와 익명도\n\n익명화된 데이터란 특정 개인과 연결되지 않는 데이터를 의미하며, 추가보조 데이터를 통해서도 그러한 연결이 불가능합니다. 완전히 익명화된 데이터는 GDPR과 같은 개인정보 보호 규정의 적용 범위에서 벗어납니다.\n\n추가 정보와 함께 특정 개인에게 속한다고 할 수 있는 데이터는 익명화된 것이라고 합니다. 이는 원본 데이터를 가짜 식별자로 대체하는 것만으로 달성될 수 있으며, 필요한 경우 되돌릴 수도 있습니다. 예를 들어, 암호화는 데이터 익명화의 한 방법입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n개인정보 의사화는 GDPR의 제 4조에 정의되어 있습니다:\n\n개인정보 익명화 및 의사화는 민감한 데이터 식별 후 수행되는 작업입니다. 데이터를 익명화 또는 의사화해야 하는지는 특정 사용 사례 및 미래 어느 시점에서 익명화를 반대로 해야 하는지 여부에 따라 다릅니다.\n\n![Image](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_1.png)\n\nLLM은 데이터 익명화에서 일부 역할을 합니다. 텍스트 데이터에 마스크를 적용할 수 있지만 별도의 복잡성 없이는 모든 요구 사항, 특히 식별 해제와 관련된 요구 사항을 모두 충족시킬 수 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에 제시된 정의는 나중에 이 용어들을 참조할 때 충분한 간략한 개요를 제공합니다. 주제를 더 깊게 파헤치지 않아도 됩니다. 많은 기사들이 이것을 자세히 다루고 있고, 나는 다른 기사 중 하나에서 이를 더 깊이 설명합니다.\n\n# 민감한 데이터를 처리하는 데 인공 지능을 활용하는 것이 무엇이 특별한가요?\n\n민감한 텍스트 데이터는 대용량 텍스트 필드와 문서에 간접적으로 포함될 수 있어 휴리스틱 기술을 사용해서 감지하기 어려울 수 있습니다. 이러한 기술과 방법들은 주로 미리 정의된 규칙과 패턴(예: 명명된 개체 인식)을 의존하므로 그 능력이 제한될 수 있습니다.\n\n대부분의 경우 중요한 민감한 데이터는 문장 안에서 미묘하게 제시되지 않습니다. 메시징 애플리케이션, 고객 지원 서비스, 이메일 등에서 생성된 데이터는 전체 텍스트의 맥락 안에 민감한 데이터가 포함될 수 있습니다. 이러한 복잡한 상황은 데이터 마스커들이 구문 분석된 텍스트를 기억하고 맥락을 이해해야 한다는 것을 요구합니다. 그러면 휴리스틱 기술의 사용이 방해되어 모든 개인정보 보호 요구 사항을 충족시키기에 부적합하다고 판단됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 실습 안내 — LLM-Powered 데이터 식별기 구현\n\n적절한 프롬프트를 제공하면 고급 LLM 모델은 훈련 데이터를 사용하여 문장 내의 민감한 데이터를 식별하고 가리는 데 성공할 수 있습니다. 이 예제에서는 Llama2 Model과 Azure의 GPT4 Model을 사용하여 데이터 민감도 식별기를 설정하는 방법을 시험해보겠습니다. LangChain 프레임워크를 사용하여 모델을 검색하고 프롬프트를 제공할 것입니다.\n\n우리는 모델을 로드하고 응용 프로그램을 실행하는 데 사용할 환경을 설정하는 것부터 시작하겠습니다.\n\n이를 위해 먼저 Python 3.8 이상이 설치되어 있는지 확인하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Local LLM Setup\n\n이 섹션에서는 데이터 민감도 감지를 위한 로컬 Llama2 모델을 준비하고 실행하는 데 필요한 단계를 나열합니다.\n\n## 1. 도커를 사용하여 모델 서버 설정\n\nLangChain과 통신할 수 있는 모델 서버를 설정하기 위해 도커를 사용할 것입니다. 모델을 로컬로 설정하는 다른 방법도 있으며, 여기에서 더 많은 정보를 찾을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우선 도커가 설치되어 실행 중인지 확인한 후, 아래 명령을 실행하여 이미지를 다운로드하고 모델 서버를 설정하세요.\n\n```js\ndocker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n```\n\n서버가 작동 중인지 확인하려면 브라우저에서 http://localhost:11434 링크를 열면 \"Ollama is running\" 문구가 표시됩니다.\n\n컨테이너가 실행되면 모델을 설치하세요. 다양한 모델을 테스트하고 싶다면 여기서 확인할 수 있는 목록을 확인해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n도커 실행 -it 올라마 올라마 실행 llama2\n```\n\n설치가 완료되면 모델을 로드하고 Python 스크립트를 설정할 것입니다.\n\n먼저 LangChain을 설치해 봅시다.\n\n```js\npip install langchain\npip install langchain-community\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스크립트에서 필요한 패키지를 가져오세요.\n\n```js\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_community.llms import Ollama\n```\n\n모델을 불러오세요.\n\n```js\nllm = Ollama(model=\"llama2\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2. 프롬프트 작성\n\nLLM이 이미 문장의 내장된 맥락을 이해할 수 있는 기능을 갖추고 있다고 생각할 때, 과제가 의도대로 실행되는 것을 보장하는 방식으로 프롬프트를 구성하는 것이 중요합니다.\n\n예를 들어, 다음 사항을 LLM이 보장해야 합니다:\n\n- 문맥을 이해하는 데 키워드 일치만 의존하지 않고 민감한 정보를 식별하는 능력\n- * 데이터 보호 법과 규정 (GDPR, CCPA 등)을 준수하는 감지\n- 모델이 정밀도와 재현율 사이의 균형을 유지\n- 문장 구조가 변경되지 않고 감지된 섹션이 처리되는 것을 보장\n- 추가적인 내용 없이 제공된 데이터만 반환되는 것을 보장\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ntemplate = \"\"\"\n민감한 데이터 식별자 및 마스킹 기능이 있습니다. \n텍스트에서 민감한 정보를 식별하고 \"****\"를 사용하여 마스킹하는 기능이 가능합니다. \n민감한 데이터는 종종 명시적으로 언급되지 않을 수 있으며, 텍스트의 맥락에 내재될 수도 있습니다(예: 건강, 금융, 주소 등의 주제)\n개인 식별 데이터가 감지되고 마스킹되도록 하십시오.\nGDPR, CCPA 및 HIPA와 같은 데이터 보호 법률 및 규정을 고려하도록 하십시오.\n입력 텍스트가 수정되거나 변경되지 않도록 하고 감지된 민감 정보만 마스킹하도록 하십시오.\n마스킹한 정보에 대한 높은 신뢰도를 보장하십시오.\n반환된 내용에는 필요한 마스킹이 적용된 입력 텍스트 이외의 내용이 포함되어서는 안 됩니다.\n민감한 텍스트가 감지되지 않으면 추가 콘텐츠 없이 입력값을 그대로 반환하십시오.\n\n문장:\n{sentence}\n\"\"\"\n\noutput_parser = StrOutputParser()\n\n# 설정 구문\nprompt = PromptTemplate.from_template(template)\n\n# 체인 생성\nchain = prompt | llm | output_parser\n```\n\n요청은 모델이 지시된대로 작업을 성공적으로 실행할 수 있도록 필요한 요구사항을 정의합니다. 국가에 따라 준수해야하는 특정 요구 사항에 대한 추가 매개 변수화가 가능합니다. 다음과 같은 것이 추가될 수 있습니다.\n\n- 어떤 법률을 준수해야 하는지 명시하는 것은 국가에 따라 중요합니다.\n- 민감한 데이터 마스킹 시 운영할 때의 신뢰도\n\n첫 번째 예제를 실행해 봅시다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsentence = \"\"\"\n지난 주, 여름 계획을 논의하는 동안,\n마이크는 바칼리로의 솔로 여행을 드디어 떠날 것을 시사했어요\n그는 보너스를 받은 후에 모아놓은 돈으로요.\n그의 보너스로 1만 달러 이상을 받았어요\n\"\"\"\n\n# 감지 실행\nresponse = chain.invoke({'sentence':sentence})\n\n# 최종 응답 출력\nprint(response)\r\n```\n\n![이미지](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_2.png)\n\nLlama2 모델의 응답은 민감한 데이터를 식별하고 지시에 따라 가리는 능력이 있었습니다. 모든 이름이 가려지고, \"보너스\"와 그 금액도 가렸으므로, 해당 모델이 숫자가 민감한 금융 정보와 관련되어 있는 것을 감지할 수 있었음을 나타냅니다.\n\n참고사항\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n* 데이터 보호 법률과 규정이 정기적으로 업데이트되므로, 모델은 최신 버전에 액세스할 수 없습니다.\n\n** 응용 프로그램은 제공된 법률 및 규정 문서의 특정 버전에서 실행되도록 RAG (검색 증강 생성)을 활용하도록 조정될 수 있습니다.\n\n# Azure OpenAI 설정\n\nAzure OpenAI의 GPT4 모델을 사용하려면 Azure OpenAI 서비스를 생성해야 합니다. Microsoft은 여기에서 리소스를 생성하는 단계에 대해 명확한 설명서를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n리소스를 생성한 후에는 배포로 이동하여 기본 버전을 사용하여 gpt4 모델을 배포합니다.\n\n![image](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_3.png)\n\n모델이 성공적으로 배포되면 API에 액세스하기 위해 사용된 Azure OpenAI 키를 검색하는 것을 잊지 마세요.\n\nAzure OpenAI와 작업하기 위해 필요한 LangChain 패키지를 설치해봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\npip install langchain-openai\n```\n\n로컬 설정을 구축할 때와 같은 단계를 따라가되, Azure OpenAI와 작업하기 위해 import 및 프롬프트 템플릿을 조정해야 합니다.\n\n```js\nimport os\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# 필요한 환경 변수 추가\nos.environ[\"OPENAI_API_VERSION\"] = \u003cAPI VERSION\u003e\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \u003cYour AZURE OPENAI KEY\u003e\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \u003cYour AZURE  OPENAI ENDPOINT\u003e\n\n# 프롬프트 템플릿 정의\ntemplate = \"\"\"\n당신은 민감한 데이터 식별자 및 마스커입니다.\n텍스트에서 민감한 정보를 식별하고 \"****\"를 사용하여 마스킹하는 능력이 있습니다.\n민감한 데이터는 텍스트의 맥락 속에 내장될 수 있으며 항상 명시적으로 언급되지 않을 수도 있습니다(예: 건강, 재정, 주소와 관련된 주제 등).\n개인 식별 가능 데이터가 감지되고 마스킹되었는지 확인하세요.\n데이터 보호법과 규정(GDPR, CCPA, HIPA 등)을 고려해 감지가 이뤄지도록 하세요.\n입력 텍스트가 변경되거나 수정되지 않고 감지된 민감한 정보만 마스킹되도록 하세요.\n마스킹된 민감한 정보에 대해 높은 신뢰도를 보장하세요.\n반환된 콘텐츠에 요구된 마스킹이 적용된 입력 텍스트 이외의 것이 포함되어서는 안 됩니다.\n민감한 텍스트가 감지되지 않은 경우에는 추가 내용 없이 입력을 그대로 반환하세요.\n\n문장:\n{sentence}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\n# 모델 선택\nllm = AzureChatOpenAI(\n    azure_deployment=\"gpt4\",\n)\n\n# 체인 설정\nchain = prompt | llm\n\nsentence = \"\"\"\n지난주 여름 계획을 논의하던 중, \n마이크가 발리로의 단독 여행을 드디어 가기로 하는 듯하다는 신호를 주었어요.\n보너스가 들어와서 이제 막 그 여행을 위해 돈 모았다고 하더라구요.\n그는 1만 달러 이상의 보너스를 받았어요.\n\"\"\"\n\n# LLM 실행\nresponse = chain.invoke({'sentence':sentence})\n\nprint(response.content)\n```\n\n\u003cimg src=\"/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_4.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3번의 테스트를 거친 후에도 출력 결과로 나온 것은 모델이 민감한 정보를 식별하고 필요한 곳에서 이를 가려 주었지만 문장의 내용을 변경하지 않았다는 것을 보여줬어요.\n\n다른 예시를 사용해 봅시다:\n\n```js\nsentence = \"\"\"\n에마와의 통화 중, 그녀가 내년에 월 $3000으로 인상된 임대료 때문에 이사를 가겠다고 가벼운 말투로 언급했어. 그녀가 이 세부 정보를 비공개로 유지한 걸 알았어. 어떤 재정적 걱정 때문일 수도 있어.\n\"\"\"\n```\n\n![AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_5.png 이미지](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 모델 모두 제공된 예시에서 민감한 데이터를 정확하게 식별할 수 있었습니다. 그러나 이 예시에는 상대적으로 짧은 텍스트만 포함되어 있었습니다. 보다 긴 텍스트는 특히 민감한 데이터가 보다 나중에 노출되는 경우에 결과에 더 큰 영향을 줄 수 있습니다. 모델이 숫자와 해당 문맥 간의 연결을 놓칠 수 있기 때문입니다.\n\n다음 섹션에서는 휴리스틱과 LLMs를 모두 활용하는 민감도 감지의 고수준 하이브리드 솔루션 접근 방식을 논의할 것입니다.\n\n# 고수준 솔루션 아키텍처\n\n효율적이고 신뢰할 수 있는 민감한 데이터 식별 LLM 애플리케이션을 개발하는 것은 이미 중요한 이정표입니다. 그러나 이러한 애플리케이션을 데이터 아키텍처에 배치하는 것은 신중히 고려되어야 합니다. 대량 및 속도가 빠른 텍스트 데이터를 보유한 기업은 해결책이 비효율적이 되어 큰 비용과 노력을 필요로 할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다이어그램은 각 단계에서 다양한 구성 요소에 의해 텍스트 데이터가 처리되는 방법을 단계별로 보여줍니다. 이를 통해 완전히 준수된 데이터 상태를 달성할 수 있습니다.\n\n이러한 해결책을 구축할 때 고려해야 할 중요한 질문들:\n\n- 사용 사례가 LLM을 배포하고 유지하는 과부하를 정당화하는가? 휴리스틱 기술이 충분한가?\n- 시장에 사용 사례에 대한 기존 솔루션이 있는가?\n- 데이터의 대기 시간과 가용성 요구 사항을 충족할 것인가?\n- 데이터는 컨텍스트 내에서 중요 데이터를 포함하고 있는가?\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기존 휴리스틱 접근 방식을 활용하면 민감한 데이터를 감지하는 데 도움이 될 수 있지만, 문장의 맥락 속에 간접적으로 나타나는 민감한 데이터를 표준 방법으로 감지하기 어려운 상황에서는 어려움이 있습니다. 큰 양의 텍스트를 이해하는 내재 기능을 갖춘 LLM(Large Language Models)은 민감한 데이터 감지 및 분류 문제에 대처하기 위한 차세대 도구로 기능할 수 있습니다.\n\n본 문서에서는 LLM이 이 문제에 대해 대상화될 수 있는 예시를 보여주었습니다. 예시의 프롬프트는 LLM이 과업의 일반적 요구 사항을 충족할 수 있는 능력이 있음을 입증했습니다. 휴리스틱과 LLM 간의 혼합 접근 방식 도입은 데이터 솔루션 아키텍처에 보여진 것처럼 더 나은 결과를 보장하고 추가적인 안전장치를 제공할 수도 있습니다. 이 문서는 LLM을 사용하여 민감한 데이터를 처리하는 가능성에 대한 일부 조감도를 보여주었고, 일부 추가적인 사용 사례와 가능성은 다음과 같습니다:\n\n- RAG를 위해 데이터 카탈로그 메타데이터 통합\n- 민감도 수준에 대한 분류 도입\n- 도메인별 민감한 데이터 지식 통합\n- …\n\n- 새 이야기를 게시할 때 알림을 받으려면 구독해주세요.\n- LinkedIn에서 언제든지 연락 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 민감한 데이터 처리에 대한 보다 자세한 아키텍처에 관심이 있다면 — 내 다른 기사들을 여기에서 확인해보세요.\n\n# 참고 자료\n\n유럽 의회 및 이사회 2016년 4월 27일 제 2016/679 규정 (일반 데이터 보호 규정) (EEA와 관련된 텍스트)에 관한 자연인의 보호에 대한 데이터 처리 및 그와 같은 데이터의 자유 이동의 처리 및 95/46/EC 지침의 폐지. General Data Protection Regulation (GDPR) — 공식 법적 텍스트 (gdpr-info.eu)에서 확인할 수 있습니다.\n\n데이터 침해 비용 2023 IBM. 다음 위치에서 확인 가능: https://www.ibm.com/security/data-breach (2024년 3월 1일 열람).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n| Source | Title | Date |\n|--------|-------|------|\n| LangChain | [Website](https://www.langchain.com) | - |\n| California Consumer Privacy Act (CCPA) | State of California — Department of Justice — Office of the Attorney General | March 13, 2024. [Link](https://oag.ca.gov/privacy/ccpa) |\n| Taylor, P. | Data Growth Worldwide 2010–2025 | November 16, 2023. Statista [Link](https://www.statista.com/statistics/871513/worldwide-data-created/) |","ogImage":{"url":"/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_0.png"},"coverImage":"/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_0.png","tag":["Tech"],"readingTime":11},{"title":"LangChain, Semantic Kernel, AutoGen 최신 비교 및 분석","description":"","date":"2024-06-23 19:27","slug":"2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore","content":"\n\nJane Huang과 Kirk Li가 씀\n\n이 기사에서는 대형 언어 모델 (LLMs)을 활용한 응용 프로그램을 개발하기 위한 다양한 전략을 비교 분석하며, OpenAI의 Assistant API, LangChain, Semantic Kernel, AutoGen 등과 같은 프레임워크를 아우르고 있습니다. LLMs의 동적인 환경에서는 적절한 프레임워크를 선택하는 것이 이러한 모델을 응용 프로그램에 매끄럽게 통합하기 위해 중요합니다. 다행히 LLM을 백엔드로 하는 시스템을 구축하는 데는 처음부터 시작할 필요가 없습니다.\n\n![이미지](/assets/img/2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore_0.png)\n\nOpenAI의 Assistant API는 응용 프로그램 내에서 AI 어시스턴트를 개발하는 데 도움이 되는 강력한 도구로 부상했습니다. 제공하는 편의성에도 불구하고 일부 유경험 개발자들은 비용과 실제 서비스에서의 관측 가능성 문제에 대해 우려를 표명하며, 잠재적인 단점에 대해 거론했습니다. Assistant API는 개발 노력을 크게 줄이지만, 가격 모델의 장기적 지속 가능성에 대한 불확실성이 남아있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대조적으로 LangChain, Semantic Kernel 및 AutoGen과 같은 대체 프레임워크는 개발자들에게 AI 응용 프로그램에 대한 제어와 유연성을 제공합니다. 이러한 대안들은 각각 특정 선호도와 프로젝트 요구 사항을 고려한 선택지를 제시합니다. 오늘날 사용 가능한 또 다른 주목할 만한 옵션은 SDK를 활용하지 않거나 OpenAI에 Assistant API로 복잡성을 맡기지 않고 작동하는 \"자체 구축\" 솔루션입니다. 이러한 선택지는 유연성뿐만 아니라 필요한 개발 노력 수준에서도 차이를 나타냅니다. 다양한 대안을 제공함으로써, 본 글은 개발자들이 자신의 프로젝트에 대한 독특한 요구 사항과 포부에 부합하는 판단력을 가지도록 돕고자 합니다.\n\n오늘날, 우리의 초점은 LangChain, Semantic Kernel, AutoGen과 같은 프레임워크가 제공하는 옵션에 주변합니다. 이러한 프레임워크는 각각 다른 선호도와 프로젝트 요구 사항을 고려한 것입니다. 이 글의 저자들은 이 글에서 논의된 프레임워크의 어떤 측면에 대해서도 새로운 것을 주장하지 않음을 유의해주십시오. 이 내용들은 링크를 통해 공개된 문서에서 출처를 얻은 것으로, 저자들이 다양한 프레임워크에 대한 학습 및 프로젝트 경험을 요약한 것입니다. 인공 지능 기술의 급격한 발전으로 인해, 본 글이 항상 시간에 따른 최신 발전을 포함하지 못할 수 있다는 점을 인식하는 것이 중요합니다.\n\n일반적으로, LangChain과 Semantic Kernel은 LLMs를 응용 프로그램에 통합하는 공통 목표를 가지고 있지만 접근 방식과 기능에서 차이가 있습니다. LangChain은 메모리와 컨텍스트 창을 명시적으로 구성해야 하지만 Assistant API는 이러한 측면을 자동화합니다. OpenAI의 Assistant API는 개발 노력을 최소화하는 반면, LangChain과 Semantic Kernel과 같은 프레임워크는 AI 응용 프로그램에 대한 심층적인 이해와 제어를 원하는 개발자들에게 매력적입니다. 이러한 프레임워크들은 AI 모델과 기존 코드 간의 간극을 메우는 SDK를 제공함으로써 돋보입니다. 이러한 SDK는 실제 세계 조치와 AI 응답의 통합을 용이하게 하여, 복잡한 비즈니스 프로세스를 자동화할 수 있는 완전 자동화된 AI 에이전트 구축에 이상적인 솔루션입니다. 플러그인, 도구 및 콘넥터를 통한 확장성은 다양한 기존 코드를 원활하게 연결함으로써 다른 공급 업체의 AI 서비스를 통합할 때 유연성을 제공합니다.\n\n반면, AutoGen은 다중 에이전트 프레임워크로 위치하며, LangChain의 단일 에이전트 초점과는 다릅니다. 이는 다중 에이전트 협업을 특징으로 하는 애플리케이션을 생성할 수 있어, 복잡한 에이전트 상호작용을 지향하는 개발자들을 위한 다재다능한 옵션을 제공합니다. 이러한 차이를 이해하는 것은 프로젝트 요구 사항과 원하는 협업 기능에 따라 이러한 프레임워크 중에서 선택하는 개발자들에게 중요합니다. 2024년 1월 말에, LangChain의 창시자들은 에이전트 실행 시간을 맞추기 위해 설계된 또 다른 다중 에이전트 워크플로인 LangGraph를 소개했습니다. 이 출시는 AutoGen과 비교했을 때 마음의 모델에서 상당한 변화를 제시합니다. 핵심적인 차이점은 프레임워크가 에이전트를 구성하는 방식에 있습니다. LangGraph는 고유한 에이전트 및 이들의 전이 확률을 명확하게 정의하는 방식을 촉진하며, 그것들을 그래프로 묘사합니다. 이에 반해, AutoGen은 이 과정을 더 \"대화\"로 보고 있습니다. 더불어, LangGraph는 LangChain 생태계에 원활하게 통합되어 있습니다. 이 통합을 통해 사용자들은 모든 LangChain 통합을 활용하고 LangSmith 감시 기능을 활용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리의 비교 분석을 시작하기 위해, 테이블 1에 명시된 여러 프레임워크의 기본적인 특성을 자세히 살펴보겠습니다. (화면의 너비 제한으로 인해 현재 웹페이지에서 보이지 않는 전체 내용을 볼 수 있도록 스크롤 막대를 드래그해주세요). 이 분석에서는 특히 세 가지 최고로 인정받는 프레임워크를 비교합니다. 특정 작업을 위해 개발자들이 개발 프로세스 중에 활용할 수 있는 흥미로운 특화된 라이브러리들인 가이던스, 가드레일, 람마 인덱스, 타입챗과 같은 추가로 흥미로운 라이브러리들이 있습니다. 그러나 이 기사의 목적상 이러한 라이브러리들을 자세히 다루지는 않겠습니다.\n\n## 테이블 1: 기본 특성 개요\n\n## 테이블 2: 샘플 레슨\n\n인터넷에는 소개에 관한 많은 유익한 수업들이 온라인으로 찾아볼 수 있습니다. 몇 가지 예시는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 프레임워크의 구성 요소\n\n이제, 더 깊이 탐구하기 위해 표 3-13에 나타난 프레임워크의 다양한 구성 요소를 면밀히 검토하고 비교해 보겠습니다.\n\n## 표 3: 구성 요소 개요: 작업 조율\n\n## 표 4: 구성 요소 개요: 메모리 관리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 테이블 5: 구성 요소 개요: 재사용 가능한 구성 요소\n\n## 테이블 6: 구성 요소 개요: 프롬프트 템플릿\n\n## 테이블 7: 구성 요소 개요: 문서 로더\n\n## 테이블 8: 구성 요소 개요: 문서 변환 및 분할\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 테이블 9: 구성 요소 개요: 호출 순서 구성\n\n## 테이블 10: 구성 요소 개요: 벡터 저장소\n\n## 테이블 11: 구성 요소 개요: 검색기\n\n## 테이블 12: 구성 요소 개요: 모델 입출력\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 테이블 13: 구성 요소 개요: 데이터 연결\n\n# 결론\n\nLLM(언어 모델 라이브러리)의 환경이 계속 발전함에 따라, 복잡한 AI 애플리케이션을 구축하려는 개발자들에게는 프레임워크 선택이 중요한 결정이 됩니다. Assistant API의 간편한 편리성이나 LangChain, LangGraph, Semantic Kernel, AutoGen과 같은 프레임워크가 제공하는 세밀한 제어라는 각 옵션은 각각의 장점과 고려해야 할 사항이 있습니다. 어떤 SDK를 사용할지 결정하는 것은 특정한 요구 사항, 선호도, 그리고 개발자의 목표뿐만 아니라 수행 중인 프로젝트의 성격에 달려 있습니다. 일반적인 해결책이 아니라 다양한 SDK들을 조화롭게 결합하여 사용하는 것이 종종 최적의 해결책일 수 있습니다. Semantic Kernel과 AutoGen의 원활한 통합에 대해 탐구한 John Maeda의 흥미로운 블로그 게시물과 함께, Matthew Bolanos는 오픈AI 어시스턴트를 통합하고 있으며 오픈AI 어시스턴트를 활용한 시각 등을 설명하는 \"Semantic Kernel의 미래: OpenAI 어시스턴트,\" \"OpenAI 어시스턴트: Semantic Kernel과 오픈AI 어시스턴트 사용에 대한 첫인상,\" 그리고 \"OpenAI 어시스턴트: 템플릿화된 어시스턴트 지시의 힘\" 시리즈를 Microsoft의 플랫폼에 발표하고 있습니다. Microsoft은 이미 OpenAI 어시스턴트 API를 사용하는 실험적인 구현을 갖고 있으나, 팀은 어떠한 모델로 만들어진 에이전트도 수용할 수 있는 에이전트 인터페이스의 완전한 추상화를 목표로 하고 있습니다. 이를 위해 Microsoft의 Semantic Kernel 팀 구성원들은 AutoGen팀의 연구 결과를 활용하여 에이전트가 팀으로 협업하는 시나리오를 포함한 다양한 경험을 수용할 수 있는 추상화를 개발하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더욱 풍부한 대화를 위해 LangChain은 프레임워크와 OpenAI 어시스턴트 간 상호작용을 명료하게 설명하는 포괄적인 문서를 보급했습니다. Gagan Bansal은 OpenAI 어시스턴트를 AutoGen에 통합하는 것을 탐구함으로써 대화에 기여했으며, GPTAssistantAgent에 대한 통찰을 통해 이에 대해 자세히 논의했습니다. 이러한 동적인 환경에서 다양한 SDK 간의 협업 가능성에 대해 정보를 가지고 있는 것이 AI 애플리케이션에서 대형 언어 모델의 전체 잠재력을 이용하는 데 중요합니다.\n\nCasey Doyle가 작업을 검토하는 데 도움을 준 데 대해 감사드립니다.","ogImage":{"url":"/assets/img/2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore_0.png"},"coverImage":"/assets/img/2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore_0.png","tag":["Tech"],"readingTime":5}],"page":"7","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"7"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>