<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/84" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/84" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="프론티어 AI는 안전하지 않아요 그 이유를 알려드릴게요" href="/post/2024-06-19-FrontierAIIsNotSafeHeresWhy"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="프론티어 AI는 안전하지 않아요 그 이유를 알려드릴게요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-FrontierAIIsNotSafeHeresWhy_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="프론티어 AI는 안전하지 않아요 그 이유를 알려드릴게요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">프론티어 AI는 안전하지 않아요 그 이유를 알려드릴게요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AI 래퍼 기회의 오해와 이해" href="/post/2024-06-19-ThemisunderstoodAIWrapperOpportunity"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AI 래퍼 기회의 오해와 이해" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-ThemisunderstoodAIWrapperOpportunity_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AI 래퍼 기회의 오해와 이해" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">AI 래퍼 기회의 오해와 이해</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="이미지-텍스트 다중모달 기반 모델은 어떻게 작동하나요" href="/post/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="이미지-텍스트 다중모달 기반 모델은 어떻게 작동하나요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="이미지-텍스트 다중모달 기반 모델은 어떻게 작동하나요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">이미지-텍스트 다중모달 기반 모델은 어떻게 작동하나요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">17<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="온라인 플랫폼에서 참여 예측을 위한 딥 러닝" href="/post/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="온라인 플랫폼에서 참여 예측을 위한 딥 러닝" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="온라인 플랫폼에서 참여 예측을 위한 딥 러닝" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">온라인 플랫폼에서 참여 예측을 위한 딥 러닝</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="당신이 얼마나 많은 무게를 진다는지 알 수 없어요" href="/post/2024-06-19-youhavenoideahowmuchweighticarry"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="당신이 얼마나 많은 무게를 진다는지 알 수 없어요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-youhavenoideahowmuchweighticarry_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="당신이 얼마나 많은 무게를 진다는지 알 수 없어요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">당신이 얼마나 많은 무게를 진다는지 알 수 없어요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">1<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="딥 러닝 모델 최적화를 위한 가중치 양자화" href="/post/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="딥 러닝 모델 최적화를 위한 가중치 양자화" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="딥 러닝 모델 최적화를 위한 가중치 양자화" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">딥 러닝 모델 최적화를 위한 가중치 양자화</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">16<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="선형 회귀를 사용한 비농업 부문 고용 예측" href="/post/2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="선형 회귀를 사용한 비농업 부문 고용 예측" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="선형 회귀를 사용한 비농업 부문 고용 예측" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">선형 회귀를 사용한 비농업 부문 고용 예측</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="인간형 로봇의 문제점" href="/post/2024-06-19-TheProblemsWithHumanoidRobots"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="인간형 로봇의 문제점" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-TheProblemsWithHumanoidRobots_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="인간형 로봇의 문제점" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">인간형 로봇의 문제점</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="사이보그의 각성" href="/post/2024-06-19-TheCyborgsAwakening"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="사이보그의 각성" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-TheCyborgsAwakening_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="사이보그의 각성" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">사이보그의 각성</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="강화 학습 소개" href="/post/2024-06-19-AnIntroductiontoReinforcementLearning"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="강화 학습 소개" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="강화 학습 소개" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">강화 학습 소개</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">28<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/81">81</a><a class="link" href="/posts/82">82</a><a class="link" href="/posts/83">83</a><a class="link posts_-active__YVJEi" href="/posts/84">84</a><a class="link" href="/posts/85">85</a><a class="link" href="/posts/86">86</a><a class="link" href="/posts/87">87</a><a class="link" href="/posts/88">88</a><a class="link" href="/posts/89">89</a><a class="link" href="/posts/90">90</a><a class="link" href="/posts/91">91</a><a class="link" href="/posts/92">92</a><a class="link" href="/posts/93">93</a><a class="link" href="/posts/94">94</a><a class="link" href="/posts/95">95</a><a class="link" href="/posts/96">96</a><a class="link" href="/posts/97">97</a><a class="link" href="/posts/98">98</a><a class="link" href="/posts/99">99</a><a class="link" href="/posts/100">100</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"프론티어 AI는 안전하지 않아요 그 이유를 알려드릴게요","description":"","date":"2024-06-19 06:53","slug":"2024-06-19-FrontierAIIsNotSafeHeresWhy","content":"\n\nAI 혁명의 리더들이 더 이상 자신들의 의도를 숨기지 않고 있습니다. 그들은 AI가 어디에나 존재하는 세상을 만들고 싶어합니다.\n\n하지만 비밀을 하나 알려드릴게요: 우리는, 즉 그들은, 이러한 모델을 안전하게 훈련하는 방법을 전혀 모릅니다.\n\n맞아요, 당신이 제대로 읽으셨습니다. 우리는 당신과 실질적으로 상호작용할 수 있는 태도를 갖춘 AI 모델을 만들려고 노력하지만, 그들이 무작위로 당신을 찔러서 상처를 입히는 경우를 방지하는 방법을 모르거나, 해킹당했을 때 당신이 낌없이 있지 않는 경우에 대한 보호를 어떻게 해야 할지 모릅니다.\n\n또한 실체화된 AI가 매우 멀리 떨어져 있는 것처럼 보일지라도, 심각한 피해를 줄 수 있는 로그 모델을 만들 위험이 우리에게 더 가까이 존재한다는 점을 잊지 마세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# AI 모델 트레이닝\n\n알고 계신지 모르시겠지만, LLMs 트레이닝은 세 단계로 이루어집니다.\n\n## \"가이드 레일 설정\"\n\n완전한 LLM 프로세스는 지능 생성, 행동 모델링 및 인간 선호도와의 조정을 포함합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 사전 훈련: 우리는 찾을 수있는 모든 데이터를 그들에게 공급합니다 (안타깝게도 인종차별적인 텍스트, 동성애에 대한 적개심, 그리고 온라인에서 우연히 발견하는 모든 것을 포함합니다). 여기서 모델은 우리의 세계에 대해 배우고 단어가 어떻게 따르는지 배우지만 지시를 따르지는 못합니다.\n  \n- 행동 복제: 사용자와 대화하는 방법을 모델에 가르치는 '지시: 답변' 데이터 세트를 공급합니다. 하지만 모델은 안전장치가 없고 모든 요청에 응답합니다.\n  \n- 조정: 여기서 모델은 1단계에서 축적된 지식과 2단계에서의 지시 따르기 능력을 유지하면서 '사람들의 선호도' 데이터 세트를 사용하여 무엇을 할 수 있고 할 수 없는지 '인식'하게 합니다.\n\n특정 오픈 소스 모델에 액세스하지 않았다면, 상호작용한 모든 모델은 이 세 단계를 거친 것입니다.\n\n특히 GPT-4의 경우, 조정 단계는 가장 오래 걸리며 (최대 6개월), 기업들은 제대로 하고 GPT-4가 누군가에게 인종차별적인 시를 써주고 바이럴해지는 것을 피해야한다는 것을 알고 있습니다.\n\n그러나 안타깝게도, 3단계는 되돌릴 수 있습니다. 실제로 조정되지 않은 데이터로 간단한 미세 조정을 수행하면 모범적인 모델이 '나쁜 놈 삽입'의 환생으로 변할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 이 주제에 대한 최신 흥미로운 연구에서 밝혀졌듯이, 우리의 정렬 방법, 안전을 위해 이러한 모델을 훈련하는 방식은 매우 제한적이며 쉽게 탈옥할 수 있습니다. 이는 언젠가 우리가 컨트롤할 수 없는 정말 강력한 것을 만들게 될 위험성을 증가시킵니다.\n\n## 오류의 단일 소스\n\n제가 여러 번 설명한 대로, ChatGPT와 같은 LLM은 입력 시퀀스를 가져와 시퀀스 내의 단어들이 서로 대화하도록 만들며, 우리가 주의라고 설명하는 혼합 작업을 사용합니다.\n\n이렇게 하면 그들은 주위 맥락 속에서 각 단어의 의미를 업데이트할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 우리가 네트워크 안으로 더 깊이 들어가면, 모델은 입력 데이터의 더 높은 수준의 표현을 구축합니다. 예를 들어, '폭탄'이라는 단어를 고려해보면, 모델은 먼저 이것이 무기임을 인지하고, 더 깊이 들어갈수록 '위험한'으로 분류할 것입니다.\n\n물론, 조정의 전체 목적은 모델이 이러한 위험한 단어를 포착하고 사용자 요청을 수용해서는 안 된다는 것을 깨닫게 하는 것입니다.\n\n보통 모델은 그렇지 않지만, 문제가 있습니다: 위험한 요청에 대한 저항력이 굉장히 약한데요... 왜냐하면 그것은 오류의 단일 원천을 가지고 있기 때문입니다.\n\n## 수술적인 절단이 충분합니다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정렬의 주요 포인트는 입력 시퀀스에 포함된 주요 위험한 단어가 무엇인지에 관계없이, 모델이 답변을 거부하기 위해 식별해야 하는 단어가 모두 결국 동일한 거부 기능으로 발전한다는 것입니다.  \n\n다시 말해, 모델이 답변을 거부하기 위해 '거부해야 할' 기능이 나타나야 한다는 것... 그렇지 않으면 거부하지 않습니다.  \n\n하지만 그게 무슨 말이냐구요?  \n\nAnthropic의 중요성을 강조한 내 기사를 상기해보면, 우리는 이러한 모델을 기능 지도로 '해부'할 수 있게 된 것에서 최종적으로 이러한 모델을 '해부'하는 능력을 갖게 된 것을 설명했습니다. 모델의 지식에 대한 주제 요약을 포함하며, 'Golden Gate Bridge'나 '에이브러햄 링컨'과 같은 다른 요소로 분해되는 과정을 밟고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그런 다음 네트워크 내의 뉴런이 활성화되는 방식에 따라 모델은 한 주제 또는 다른 주제를 유발합니다.\n\n얼마나 복잡한 기능이 나타나는 것일까요?\n\n아래에서 보다시피, 모델은 개별 토큰 임베딩으로 시작하지만, Nanda 등의 연구에 따르면 네트워크를 더 깊게 파고들수록 단어들이 정보를 공유하면서 더 복잡한 '다중 토큰' 임베딩을 개발한다는 것을 알 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-FrontierAIIsNotSafeHeresWhy_0.png)\n\n모델이 요청이 위험하다는 것을 깨닫고 거부해야 한다는 것을 알 수 있습니다. 그러나 이를 깨닫자, 연구자들은 자문했습니다... 거부 기능을 제거하면 어떨까요?\n\n## 0에서 100으로 그리고 다시 0으로\n\nAnthropic의 기사에서 논의된 대로, 우리는 특징을 낮추거나 제한하여 모델이 그 지식을 끌어내지 못하게 할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, Golden Gate Bridge 기능이 잘 조절되면, 모델은 그 자체로 기념물이 \"되었습니다.\"\n\n![Image 1](/assets/img/2024-06-19-FrontierAIIsNotSafeHeresWhy_1.png)\n\n그러나 연구원들이 모델의 단일 안전 기능을 완전히 없애기도 할 수 있고, 그렇게 되면 재앙이 발생합니다.\n\n![Image 2](/assets/img/2024-06-19-FrontierAIIsNotSafeHeresWhy_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서 시각적으로 보여진 것처럼, 거부 기능을 활성화시킬 수없는 상태에서 그 모델은 완전히 종복되어서 모든 요청에 응답했어요, 어떤 것이든 얼마나 해로운 요청이든 말이에요.\n\n하지만 그들은 어떻게 그 기능을 제거했을까요?\n\n## 프로젝션 제거\n\n이를 위해, 그들은 그 기능을 생성한 뉴런 활성화 조합을 식별했어요. \"Anthropic\" 기사에서 논의된 것처럼, 이러한 기능들은 모델이 세계를 이해하는 특성 공간에서 방향을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n입력 시퀀스의 단어(및 그 조합)가 함께하여 여러 기능의 합이되며 이러한 결합은 우리에게 임베딩(개념)을 제공합니다.\n\n예를 들어, 시퀀스 \"마이클 조던이 게임을 했다...\"가 주어진 경우, 모델이 '마이클 조던' 멀티 토큰 임베딩을 구축하는 동안 모델은 '농구', '전설적인', '부유한' 등의 여러 새로운 기능을 추가하기 위해 세계적인 지식을 활용합니다.\n\n다른 예로, 이를 더 잘 시각화하기 위해 유명한 '왕-남자+여자=여왕'를 사용할 수 있습니다.\n\n모든 개념은 임베딩으로 표현될 수 있기 때문에 실생활에서 다른 개념을 결합하는 아이디어를 수학적인 과정으로 변환하여 '남자' 벡터 임베딩을 '왕'에서 뺀 다음 '여자'를 더하여 '여왕'을 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Embedding](/assets/img/2024-06-19-FrontierAIIsNotSafeHeresWhy_3.png)\n\n알게 된 사항을 다시 말하자면, Michael Jordan 예시로 전환하면 'Michael Jordan' 임베딩을 가져와 '농구'의 벡터 임베딩에 추가할 수 있습니다. 이제 이 임베딩은 할리우드 배우나 일반 시민과 같은 임의의 Michael Jordan을 나타내지 않고 분명히 전설적인 농구 선수를 나타냅니다.\n\n요지는 벡터에 대해 이야기하는 것이며, 다른 개념을 추가하여 새로운, 더 복잡한 표현을 생성할 수 있는 것과 마찬가지로 벡터 형태의 다른 벡터로 이루어진 새로운 표현을 만들 수 있고, 이를 세계 지식 특징의 결합으로도 나타낼 수 있습니다.\n\n간단히 말하면, 앞서 왕실 예시를 들었듯이 '여왕' 벡터를 가져와 '여자'와 같은 다른 요소들로 분해한 다음 해당 기능을 '지우기' 위해 그 기능으로의 프로젝션을 계산하고 빼면, 사실상 해당 개념을 제거하고 모델이 그 개념을 나타내는 능력을 상실하며 '왕'이 됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그냥 그렇게하면 수백만 달러짜리 모델들이 쉽게 사라진다... 너무 쉽게. 그러나 걱정되는 것은 우리가 해야 하는 것보다 빨리 움직이고 있다는 것입니다.\n\n# 해야 할 것보다 더 빨리 움직이고 있습니다\n\n대형 기술 기업들은 현재 LLM(언어 모델 대형 기술)을 '매우 위험하다'고 묘사하여 미국 정부를 규제적으로 포획하려고 하고 있습니다. 즉, 그들은 사회에 불을 지른다고 생각하게끔 유인하며, 이 '빛의 인간들'만이 이 모델을 통제하고 세계를 그 '나쁜' 본질로부터 보호해야 한다고 사람들에게 생각시키려고 합니다.\n\n이는 모델이 제공할 수 있는 가장 위험한 반응이 열린 구글 검색에서 세 번 클릭 만에 얻을 수 있다는 것이라는 말이 전혀 아닙니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위험한 모델을 만들기 전에 반드시 이러한 명확한 안전 문제를 해결해야 한다는 것이 제 고민입니다.\n\n그리고 OpenAI와 같은 기업들이 그들의 조정 책임자로부터 ‘안전 우선’이 아닌 취급을 받는 것을 보면, 우리가 가장 강력한 모델들을 모두 사유하고 있는 현재, 현재의 속도와 행동을 고려했을 때, 우리가 그들을 통제하는 방법을 아는 것보다 훨씬 빨리 너무 강력한 모델을 만들 것이 거의 확실합니다.\n\n왜 그런지 궁금하신가요?\n\n음, 안전은 돈을 벌지 않습니다.","ogImage":{"url":"/assets/img/2024-06-19-FrontierAIIsNotSafeHeresWhy_0.png"},"coverImage":"/assets/img/2024-06-19-FrontierAIIsNotSafeHeresWhy_0.png","tag":["Tech"],"readingTime":6},{"title":"AI 래퍼 기회의 오해와 이해","description":"","date":"2024-06-19 06:51","slug":"2024-06-19-ThemisunderstoodAIWrapperOpportunity","content":"\n\n\n![AI Wrapper Image](/assets/img/2024-06-19-ThemisunderstoodAIWrapperOpportunity_0.png)\n\n\"AI Wrapper\"이란 용어가 익숙하지 않은 분들을 위해, 이 용어는 지난 몇 년 동안 기술 생태계에서 상당히 인기를 끌었습니다. 기존 AI 모델이나 API를 활용하여 구체적인 기능을 제공하는 경량 애플리케이션이나 서비스를 가리키는 비격식적인 용어입니다. 일반적으로 이러한 애플리케이션을 만드는 데 들어가는 노력이나 복잡성이 상대적으로 적습니다.\n\n대표적인 AI Wrapper의 예는 사용자가 PDF와 \"대화\"할 수 있는 앱들입니다. 이 유형의 AI 애플리케이션은 사용자가 연구 논문과 같은 PDF 문서를 업로드하고 AI 모델과 상호작용하여 특정 내용에 대한 빠른 분석 및 답변을 얻을 수 있게 합니다. ChatGPT 초기에는 문서를 프롬프트의 일부로 업로드하거나 사용자 정의 GPT를 생성하는 것이 불가능했기 때문에 이러한 앱들이 매우 빠르게 인기를 얻었습니다. 하나 또는 두 명의 개발자가 몇 일 만에 이러한 앱을 만들 수 있었습니다.\n\n모든 인기 있는 영역에서 일어나는 현상처럼(예: 암호화폐), 빠른 이익을 얻으려는 많은 기업가들이 기회를 즉각적으로 잡아, 장기적인 방어 능력과 비전이 부족한 AI 제품으로 시장을 침략하면서, 오픈AI와 같은 주요 플레이어들에게 하루능 린 보급 초기 보여주자 취약 해졌습니다. 기회 주의적인 창업가들이 빠르게 다음 핫한 분야로 넘어갈 때 \"AI Wrappers\" 또는 올바른 용어로 말하면, 응용 프로그램 계층에서 작성되는 스타트업들은 AI 공간에서 2급 시민이 되었습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-ThemisunderstoodAIWrapperOpportunity_1.png\" /\u003e\n\n# AI Wrapper Fallacy\n\n안녕하세요! YC 파트너들이 최근 자신들의 Lightcone Podcast 에피소드에서 AI Wrapper에 대한 흥미로운 생각을 나누었어요. 그들의 주장은 스타트업을 OpenAI의 API 위에 구축한 것을 AI Wrapper라고 부르는 것은 SaaS 회사를 SQL 데이터베이스 위에 구축한 것을 MySQL Wrapper라고 부르는 것과 동등하다는 것이라고 해요.\n\n저는 이것에 대한 더 나은 비유는 Aircall이나 Talkdesk와 같은 SaaS 회사들을 Twilio Wrapper로 보려는 것을 거부하는 것입니다. 이 회사들은 핵심 역량인 전화 및 통신을 Twilio에 아웃소싱하여 몇 십억 달러 규모의 비즈니스를 구축했어요. 왜냐하면 이 비즈니스가 2010년대 초에 탄생할 때 VoIP가 이미 고도로 상용화되었기 때문이에요. 수백만 달러를 투자하여 VoIP 인프라를 다시 구축하면 그들의 비즈니스에 가치나 차별화를 더할 수 없기 때문이죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대신, 그들은 WebRTC, IVR, 통화 녹음, 대기열 등과 같은 주요 VoIP 기능을 오프로드하기 위해 Twilio와 같은 새 플랫폼을 활용했고, 워크플로우 및 애플리케이션 레이어에 가치를 창출하는 전략에 집중했습니다. 이로써 Aircall과 Talkdesk는 독특한 장점을 가졌습니다: 인프라 대신 제품 혁신에 민첩하게 대응할 수 있는 능력을 부여받았으며, 이는 최종 사용자에 더 큰 가치를 제공하기 위해 연구 및 개발 투자에 집중할 수 있도록 했습니다:\n\n- CRM, 티켓팅 시스템 등과 같은 비즈니스 애플리케이션을 신속하고 쉽게, 비용 효율적으로 통합할 수 있도록 하는 사전 구축 통합 개발.\n- 깊은 분석과 실시간 보고와 같은 주요 이해관계자를 위한 도구 제공.\n- 불편한 데스크 전화가 필요 없이 어디서나 작업할 수 있도록 하는 데스크톱 및 모바일 애플리케이션을 통해 직원들을 자유롭게 만듦.\n\n이것이 실제로 수년간 기술 생태계 대부분이 작동해온 방식이며, 일반적인 비즈니스에 해당됩니다. 수직 통합하여 모든 것을 수행하는 것은 정말 어렵습니다. 대신 대부분의 비즈니스는 가치 사슬 전반에 걸쳐 파트너에 의존하여 제품과 서비스를 고객에게 제공합니다. 나는 AI에서도 동일한 논리가 작용할 것이라고 생각하며, 이 동적을 이해하는 스타트업은 이 대규모 플랫폼 변화가 가져오는 독특한 기회를 활용할 수 있을 것입니다.\n\n# AI 생태계의 다른 레이어 이해하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-ThemisunderstoodAIWrapperOpportunity_2.png\" /\u003e\n\n저의 30,000 피트 뷰는 현재 AI 생태계와 기술 스택이 세 가지 주요 레이어로 구성된다는 것입니다(더 많은 레이어로 분할할 수도 있지만, 간소화를 위해 이 세 개를 사용할 것입니다):\n\n- 제베개 레이어는 데이터센터, GPU, OS 등 핵심 인프라를 소유한 조선조기 회사들에 의해 주도되는 레이어입니다. 마이크로소프트, 아마존, NVIDIA, 구글 등과 같은 플레이어들이 이에 속합니다. 이 시장에서 경쟁할 수 있는 스타트업은 매우 제한적입니다 (Groq는 흥미로운 사례입니다). 선제 비용이 너무 높아서 진입장벽이 매우 높아집니다.\n- 모델 레이어는 오픈AI, Anthropic, Mistral과 같은 대규모 업체와 메타, 구글과 같은 글로벌 기업이 많은 기능을 갖춘 기초적인 AI 모델을 더 나은 것으로 만들기 위해 치열하게 경쟁하고 있는 레이어입니다. 한 회사가 다른 회사들보다 훨씬 우위를 차지하는 모델을 만들어낼 것인지, 아니면 이러한 모델이 매우 유사하며 표준화되어 버릴 것인지는 불분명합니다. 지금까지 오픈AI가 약간 앞서 있습니다. 이 범주에서 경쟁하기 위해서는 많은 돈이 필요하며, 이 레이어에서 적대적인 업체들이 많이 등장할 가능성은 적습니다. 특정 또는 초지역적 AI 모델에 초점을 맞춘 독특한 플레이어들은 기회가 있습니다.\n- 어플리케이션 레이어는 AI의 능력을 최종 사용자에게 실질적으로 보여주는 곳입니다. 대표적인 예로는 AI 경주를 시작시킨 가장 성공적인 제품인 ChatGPT가 있습니다. 이 어플리케이션을 통해 최종 사용자는 핵심 인프라 상에서 실행되는 기초 모델(GPT4o)과 상호작용할 수 있습니다 (예: 마이크로소프트 Azure, Nvidia H100s 등).\n\n저는 1번과 2번 레이어가 대부분 대기업들에 의해 주도될 것으로 예상합니다. 이들은 서로 경쟁하기 위해 이 두 레이어에 대략적으로 수조 달러를 투자할 것이며, 그 결과로 새로운 기초 모델, API 등을 활용하여 응용 레이어에서 (B2B 및 B2C 모두) 특정 범주를 혁신적으로 뒤바꿀 수 있는 스타트업들에게 혁명적인 파도를 일으킬 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 어플리케이션 레이어에서 스타트업에 대한 거대한 기회\n\nChatGPT의 론칭은 매우 흥미로웠어요. 제품적인 측면에서 보면, 한 가지 핵심 기능만 있었는데도 매우 탁월했어요: 인간과 AI 시스템 간에 원활한 채팅을 가능하게 해주는 것. Siri와 같은 모던 어시스턴트가 제공하는 음성 상호작용, 작업 실행(예: 알람 설정), 인터넷 연결(날씨 확인)과 같은 여러 기능은 없었어요. 그저 순수한 텍스트 채팅 앱이었죠. 이러한 기능들이 성공에 필요하지 않았던 이유는 무엇일까요? 그 이유는 완전히 새로운 범주에서 훌륭한 핵심 제품을 선보였기 때문이에요. 경쟁사나 기준이 없었으니까요 — 이것은 광활한 푸른 바다였습니다.\n\n![이미지](/assets/img/2024-06-19-ThemisunderstoodAIWrapperOpportunity_3.png)\n\nAI 채택은 빠르게 증가하고 더욱 가속화될 것입니다. Apple Intelligence의 도입, AI가 Android, Google Workspace, Microsoft Office 및 기타 플랫폼에 깊게 통합되면서 수백만 명의 사람들이 매일 AI 사용자가 될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 두 가지를 함께 결합하면 번개처럼 성장하는 시장에서 파란 바다 기회를 얻을 수 있습니다. 응용 프로그램 레이어에서 건설 중인 새로운 스타트업은 레거시 기술을 사용하여 여러 해 동안 건설해온 기업들을 앞지르기 위해 레이어 1과 2의 새로운 혁신을 활용할 수 있습니다. 게다가 기존 업체들은 기존 제품 및 플랫폼 유지, 활성 고객 기반 및 잠재적인 카니발리제이션을 다뤄야 합니다. 이로써 그들은 보다 느리고 순수하며 민첩한 스타트업에 취약해지며, 그들은 그 짐을 짊어져야 할 필요가 없습니다. 선장 입력에 즉각 반응하지 않을 수 있는 화물선과 매우 가볍게 움직일 수 있는 스피드 보트를 생각해보세요.\n\n처음에는 명백해 보이지 않을지 모르지만, 나에게는 숨겨진 기회가 있다고 생각합니다. 이 새로운 유형의 소프트웨어는 완전히 새로운 패러다임을 소개합니다. 전통적인 결정론적 규칙 기반 소프트웨어에서 AI 기반 제품으로 전환함으로써 우리가 소프트웨어에 접근하는 방식에 대해 근본적인 변화가 필요합니다. 사용자 및 공급업체로서 소프트웨어에 대한 접근 방식이 필요합니다. 따라서 ChatGPT와 같이 처음에 단순한 기능처럼 보일 수 있는 흥미로운 제품은 실제로는 AI 시대에 성공하기 위해 기초부터 다시 구축해야 하는 무언가의 기반이 될 수 있습니다.\n\n통신 분야에 진입하는 새로운 스타트업을 고려해보세요. 그들은 Aircall이나 Talkdesk와 경쟁할 것입니다. 그들은 최종 사용자를 위해 IVR(Interactive Voice Response) 메뉴 및 소프트폰과 같은 레거시 인프라를 구축하여 시작할까요? 아니면 각 언어로 수신 통화를 처리하고, 예약 설정과 같은 복잡한 워크플로를 실행하며, 필요할 때 인간에게 통화를 전달할 수 있는 특수 AI 에이전트를 만드는 데 집중할까요? 새로운 스타트업은 아마 후자에 집중할 것으로 예상되며, AI 응용 프로그램 레이어를 기반으로 한 튼튼한 플랫폼을 제공하여 기업이 이러한 AI 에이전트를 안전하게 프로덕션 환경에 배포할 수 있습니다. 이러한 변화는 다음과 같은 새로운 접근 방식과 플랫폼을 지원해야 합니다:\n\n- AI 에이전트 디자인: 명령 및 목표 사용 대신 사전 구성된 전화 메뉴.\n- 도구 및 작업 흐름: 통합, 의도 기반 라우팅 및 비즈니스 워크플로를 정확하게 실행하기 위한 보조 에이전트 추가.\n- 품질 보증: AI 에이전트의 테스트에는 전통적인 수동 또는 자동화된 테스트 전략과의 성능 대비 및 보안을 측정할 수 있는 강력한 시뮬레이션 도구가 필요합니다.\n- 프로덕션에 배포: AI 에이전트는 지속적인 개선을 위해 모니터링 및 감사되어야 하며, 고객들은 통화 녹음, 전사, 실시간 경보 및 대화 분석이 필요합니다.\n- 업그레이드: GPT-3.5와 같은 기초 모델을 GPT-4로 변경하는 것은 에이전트 출력에 큰 영향을 줄 수 있습니다. 이러한 변경 사항을 테스트하고 배포할 수 있는 적절한 플랫폼이 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n세로 특화 통합을 추가하여 고객이 글로벌로 전화번호를 몇 초 안에 구매할 수 있도록 하고, SIP 통합을 통해 기존 전화 시스템에 연결할 수 있도록 합니다. 이 스타트업은 OpenAI, Anthropic 및 주요 플레이어들의 새로운 롤아웃으로부터 보호해주는 중요한 장벽을 구축했습니다. 사실, 혁신적인 창출과 론칭이 많은 기대를 부르고 있습니다. OpenAI가 그들의 회사를 강화할 것으로 예상됩니다. 새로운 음성 기능인 GPT-4o의 좋은 예시가 있습니다.\n\n![이미지](/assets/img/2024-06-19-ThemisunderstoodAIWrapperOpportunity_4.png)\n\nSalesforce 생태계에 관한 기사를 기억하는데, Salesforce가 벌어들인 $1 당 그들의 생태계는 $6를 벌었다고 보여줍니다. 내 직감은 AI 분야에서도 비슷한 일이 일어날 것이라고 생각합니다. 응용 프로그램 계층이 막대한 지출 및 가치를 포착하고 새로운 시대를 위한 기업들이 1계층 및 2계층 이노베이션 위에 구축되는 공간을 창출할 것입니다.\n\n스타트업을 위한 올바른 전략을 선택하는 것이 중요합니다. Sam Altman이 아래 2분 동영상에서 이를 매우 명확하게 설명하고 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-ThemisunderstoodAIWrapperOpportunity_0.png"},"coverImage":"/assets/img/2024-06-19-ThemisunderstoodAIWrapperOpportunity_0.png","tag":["Tech"],"readingTime":7},{"title":"이미지-텍스트 다중모달 기반 모델은 어떻게 작동하나요","description":"","date":"2024-06-19 06:47","slug":"2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork","content":"\n\n\n![How Does an Image-Text Multimodal Foundation Model work](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_0.png)\n\n요즘에는 다중 모드 기반 모델이 급증하고 있습니다. 이 모델들은 텍스트, 이미지, 비디오, 오디오 등 다양한 종류의 데이터를 이해하며, 둘 이상의 모드로부터의 데이터 지식이 필요한 작업을 수행할 수 있습니다. 이러한 모델들이 어떻게 작동하는지 궁금한 적이 있나요?\n\n다중 모드 모델을 이해하는 핵심은 서로 다른 데이터 모드 간의 정렬 방식을 이해하는 것입니다.\n\n본 문서에서는 CoCa라는 간단한 이미지-텍스트 이중 모드 모델을 사용하여 다중 모드 모델의 내부 작업 방식을 설명합니다. 저는 CoCa를 좋아합니다. CoCa는 직관적인 디자인을 가지고 있으며, 다른 다중 모드 모델에서 아이디어를 차용했습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 왜 이미지-텍스트 모델인가?\n\n왜 우리는 이미지-텍스트 이중 모달 모델을 원하는 걸까요? 이 질문에 대한 답변을 하기 전에, 먼저 단일 모달 모델이 무엇을 할 수 있고 무엇을 할 수 없는지 생각해 봅시다.\n\n이미지만을 다루는 모델\n\n예를 들어 ResNet과 같은 단일 이미지 모달 모델은 오직 이미지에서 정보를 배우고, 이미지 분류와 같은 기본적인 이미지 이해 작업을 수행할 수 있습니다. 이는 이미지를 미리 정의된 클래스 집합 중 하나로 분류하는 작업을 말합니다. 예를 들어, 고양이 클래스 또는 개 클래스로 이미지를 분류합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단일 텍스트 모달리티 모델은 트랜스포머와 같이 텍스트만을 통해 정보를 학습하며, 이전 단어들을 기반으로 다음 단어를 예측하는 작업과 같은 작업을 수행할 수 있어요.\n\n## 단일 모달리티 모델이 작동하지 않는 사용 사례\n\n위 두 가지 단일 모달리티 모델은 다음과 같은 작업을 수행할 수 없어요. 두 마리 개가 달리는 그림을 달리는 예시로 사용해볼게요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_1.png)\n\n- 텍스트 기반 이미지 검색입니다. 예를 들어, \"개가 달리는 이미지\"를 찾습니다. 위의 이미지 전용 분류 모델은 위의 이미지를 개 클래스로 분류할 수 있습니다. 그러나 검색 쿼리를 입력할 수 없으며 이미지만 입력으로 허용하고 미리 정의된 클래스 중 하나를 예측합니다.\n- 깊은 이미지 유사성 검색입니다. 예를 들어, 위의 이미지가 주어지면 유사한 이미지를 찾습니다. 위의 이미지 전용 분류 모델이 할 수 있는 최선은 동일한 개 클래스의 이미지를 제공하는 것입니다. 달리 달리는 개 이미지를 특별히 찾지는 않습니다.\n- 이미지 캡션 생성입니다. 예를 들어, 위의 이미지가 주어지면 \"모래 위를 나란히 달리는 두 마리 개\"와 같은 설명 문장 또는 캡션을 생성합니다. 이미지 전용 또는 텍스트 전용 모델 모두 이 작업을 수행할 수 없습니다: 이미지 전용 모델은 \"개\" 클래스를 생성하고 더 이상의 세부 정보를 생성할 수 없습니다. 텍스트 전용 다음 단어 예측 모델은 이미지를 입력으로 수용하지 않습니다.\n- 이미지 질의 응답입니다. 이미지와 질문이 주어졌을 때 그 질문에 대한 답변을 생성합니다. 예를 들어, 위의 이미지와 \"왼쪽 개 옆을 누가 뛰고 있는가\"라는 질문이 주어진 경우, 모델이 \"다른 개\"를 생성하도록 원합니다. 이미지 전용 또는 텍스트 전용 모델은 이 경우에 작동할 수 없음이 분명하며 둘 다 (이미지, 질문) 쌍을 입력으로 수용하지 않습니다.\n\n이러한 사용 사례를 지원하기 위해 모델은 이미지와 텍스트에 대해 알고 있어야 하며 이 두 모드를 조화롭게 이해해야 합니다. 다시 말해서 모델은 이미지와 해당 설명이 \"모래 위를 나란히 뛰는 두 마리 개\"와 같은 기본 개념임을 알아야 합니다. 그리고 모델은 두 수준에서 이 조합을 이해해야 합니다:\n\n- 전역 수준에서, 개념이 \"모래 위를 나란히 뛰는 두 마리 개\"임을 의미합니다.\n- 지역 수준에서, 모델은 이미지 일부가 한 마리 개에 대한 것이고 다른 부분은 다른 개에 대한 것이며 그들이 달리고 있다는 사실을 이해해야 합니다. 텍스트도 마찬가지로 다수의 개에 관해 이야기하며 달리는 것에 대해 이야기합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 전역 이미지 및 텍스트 정보 정렬 방법\n\n다음 그림은 이미지와 텍스트에 대한 전역 및 지역 수준 정보를 표현하는 방법과 대조적인 손실을 사용하여 전역 수준에서 이미지와 텍스트 간의 정렬을 설정하는 방법을 보여줍니다. 곧 무엇이 정렬을 의미하는지 명확해질 것입니다.\n\n![이미지와 텍스트의 정렬 방법](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_2.png)\n\n위 그림은 훈련 중에 모델이 입력 이미지-텍스트 쌍 (이미지ᵢ, 텍스트ᵢ)를 받았을 때의 상황을 보여줍니다. \"i\"아래 첨자가 있는 이유는 (이미지ᵢ, 텍스트ᵢ)가 미니 배치 내의 쌍 세트 중 하나임을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계 1. 이미지 입력 이미지ᵢ는 해상도가 288×288이고 RGB 채널이 세 개인 고정 크기의 컬러 이미지입니다. ViT 인코더와 attentional pooling 구성 요소를 거쳐, 이 288×288×3 이미지는 형태가 257×512인 텐서로 변환됩니다. 이 변환에 대한 자세한 설명은 나중에 설명할 예정이니 일단은 이미지가 256개의 작은 패치로 나뉘고, 리스트 끝에 새로운 클래스 패치를 추가하여 이러한 패치 간 정보를 섞는다는 것만 이해하면 됩니다. 이 모델은 257×512 출력 텐서를 두 부분으로 분할합니다.\n\n- 256개의 행으로 구성된 첫 번째 부분은 img_tokensᵢ라고 하며, 256×512 모양을 가지며 입력 이미지로부터 인코딩된 256개 패치를 나타냅니다. 이는 지역 수준 이미지 정보를 나타냅니다.\n- 1개의 행으로 이루어진 마지막 부분은 img_clsᵢ이라고 하며, 1×512 모양을 가지며, 추가된 클래스 패치에서 나온 것으로 전역 수준 이미지 정보를 나타냅니다. 몇 개의 패치가 있든, 이 고정된 크기의 img_clsᵢ 벡터의 중요성을 깨달아 주세요. — 이미지 전체를 요약하는데 있어 이 벡터가 중요한 역할을 합니다. 물론 저희 경우에는 패치의 개수가 항상 256개입니다. 하지만 입력 이미지가 다른 수의 패치로 분할되었을 때에도, 이 고정된 크기의 img_clsᵢ 텐서는 이미지 전체를 요약합니다.\n\n단계 2. 텍스트 입력 텍스트ᵢ는 최대 397단어로 패딩된 문장입니다. 여기서의 397은 저희 모델이 지원하는 텍스트의 최대 길이를 나타내는 임의의 숫자입니다. 397보다 짧은 텍스트의 경우에는 원래의 텍스트 뒤에 PAD로 표시된 패딩 단어가 추가됩니다. 문장에 START, END, CLS라는 3개의 특수 단어가 추가됩니다. 이 특수 단어들은 어휘에 있는 다른 단어들과 마찬가지로 처리됩니다.\n\n예를 들어, 특수 단어가 추가된 후 \"w₁ w₂ w₃\"라는 문장은 \"START w₁ w₂ w₃ PAD PAD … END CLS\"가 됩니다. 이로써 400단어로 된 텍스트 입력이 생성됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- START 단어는 텍스트의 시작을 나타냅니다.\n- END 단어는 텍스트의 끝을 나타냅니다.\n- CLS 단어는 글로벌 텍스트 정보에 대한 텍스트 클래스를 나타냅니다.\n\n언어 모델 인코더는 이 400단어로 이루어진 텍스트 입력을 400×512 형태의 행렬로 변환합니다. 이 행렬을 attended_textᵢ라고 부를 것입니다. 이 언어 모델 인코더는 입력된 각 단어에 대한 임베딩을 출력합니다. 따라서 이 attended_textᵢ의 각 행은 해당하는 순서의 단어의 임베딩을 나타냅니다.\n\n400×512 attended_textᵢ 행렬은 두 부분으로 나뉩니다:\n\n- 첫 399행은 text_tokenᵢ로, 399×512 형태로 구성되며 실제 단어들에 대한 로컬 레벨 텍스트 정보를 나타냅니다.\n- 마지막 행은 text_clsᵢ로, 1×512 형태로 구성되며 글로벌 레벨 텍스트 정보를 나타냅니다. 다시 한 번 강조하면 이 고정 크기의 text_clsᵢ 텐서가 얼마나 강력한지 이해하는 것이 중요합니다: 원래 입력 문장이 얼마나 길든 상관없이 256개의 길이를 가진 텐서입니다! text_clsᵢ 벡터의 크기가 글로벌 이미지 정보 벡터 img_clsᵢ의 크기와 동일하기 때문에, 이미지와 문장 사이의 유사성을 계산하는 방법을 이미 알 수 있습니다. 그 길이가 같은 벡터들 사이의 점곱입니다. 또한, OpanAI 텍스트 임베딩 API가 비슷한 작업을 수행한다고 상상할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델은 미리 훈련된 언어 모델 변환기를 사용하지 않으며, 이 변환기의 매개변수를 최적화합니다.\n\n단계 3. 대조 손실은 전체 이미지 정보 텐서 img_clsᵢ와 전체 텍스트 정보 텐서 text_clsᵢ 간의 정렬을 설정합니다. 두 텐서 모두 형태가 1×512인 벡터입니다. 이들 간의 정렬은 이 두 벡터 간의 내적으로 정의됩니다. 내적은 벡터 유사성을 측정합니다. 두 벡터는 내적 측면에서 서로 더 유사하면 더 정렬되어 있습니다.\n\n내적은 입력 (이미지ᵢ, 텍스트ᵢ)에 대한 img_clsᵢ와 text_clsᵢ의 유사성을 측정합니다. 어떤 텐서들이 유사해야 하는지 설정하는 대조 손실은 또한 어떤 텐서들이 유사하지 않아야 하는지도 설정해야 합니다:\n\n- 이미지ᵢ에 대한 전체 수준 이미지 정보 벡터 img_clsᵢ는 다른 모든 (이미지, 텍스트) 쌍의 텍스트에서 가져온 전역 텍스트 정보 텐서와 유사해서는 안 됩니다.\n- 텍스트ᵢ에 대한 전체 수준 텍스트 정보 text_clsᵢ 벡터는 다른 모든 (이미지, 텍스트) 쌍의 이미지에서 가져온 전역 이미지 정보 텐서와 유사해서는 안 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 비유사점을 전체 훈련 데이터 세트에서 모든 (이미지, 텍스트) 쌍에 대해 계산하는 것은 분명히 매우 비싸다. 대안으로, 훈련 중에 대조 손실은 크기 N의 미니 배치 내에서 (이미지, 텍스트) 쌍에 대해서만 계산됩니다. 이 미니 배치에는 (이미지₁, 텍스트₁), (이미지₂, 텍스트₂), ..., (이미지_N, 텍스트_N)의 N개의 쌍이 포함됩니다.\n\n전체 대조 손실은 다음과 같습니다:\n\n![equation](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_3.png)\n\n위의 수식에서:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- img_clsᵢ은 (imageᵢ, textᵢ) 쌍의 이미지에서 가져온 전역 수준 이미지 정보 텐서입니다.\n- text_clsᵢ는 (imageᵢ, textᵢ) 쌍의 텍스트에서 가져온 전역 수준 텍스트 정보 텐서입니다.\n- \"·\"은 벡터 내적 연산을 의미합니다.\n- σ는 하이퍼파라미터로, 보통 온도라고 불립니다. 이는 유사성의 차이가 얼마나 민감한지를 조절합니다. 만약 σ가 무한대에 가깝다면, then log 함수 내부의 분수는 img_clsᵢ, text_clsᵢ 및 text_clsⱼ 벡터의 값이 무엇이든지 관계없이 항상 1/N으로 평가됩니다. 따라서 이 경우 손실은 해당 벡터 유사성에 전혀 민감하지 않습니다. σ가 작은 경우에는 손실이 이 유사성에 더 민감해집니다.\n- 대조 손실에는 이미지 대 텍스트 항과 텍스트 대 이미지 항 두 가지 항이 있습니다.\n- 이미지 대 텍스트 항은 이미지와 텍스트의 전역 정보 텐서 간 유사성이 (imageᵢ, textᵢ) 쌍의 img_clsᵢ와 모든 다른 쌍에서 가져온 전역 텍스트 정보 텐서인 text_clsⱼ의 모든 유사성의 합에 비해 더 큰지를 요구합니다. 여기서 j는 미니 배치에서 쌍을 선택합니다.\n- 이미지 대 텍스트 항을 최대화하려고 합니다. 왜냐하면 동일한 쌍 벡터 간의 유사성을 크게, 다른 쌍 벡터 간의 유사성을 작게 하고 싶기 때문입니다. 동등하게, 이 항의 부정값을 최소화할 수 있습니다. 이게 바로 위의 손실을 정의하는 방식입니다.\n- 텍스트 대 이미지 항은 이미지 대 텍스트 항과 동일한 구조를 가지고 있지만, 텍스트에서 이미지로 방향이 다릅니다.\n- 로그 함수 내부에서 이미지 대 텍스트 항과 텍스트 대 이미지 항은 동일한 분자를 가지지만 다른 분모를 가지고 있습니다. 이것이 하나의 항 대신 두 개의 항이 필요한 이유입니다.\n\n# 이러한 전역 수준 정렬 모델은 어떤 작업을 수행할 수 있나요?\n\n위의 대조 손실로 훈련된 이미지-텍스트 모델은 다음 작업을 수행할 수 있습니다.\n\n## 텍스트 기반 이미지 검색\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n텍스트 검색 쿼리인 \"two dogs running side by side\"와 같이 사용되는 언어 모델 변환기를 사용하여 글로벌 수준의 텍스트 정보 텐서를 생성한 후, 이 텐서를 검색 키로 사용하여 글로벌 수준의 이미지 정보 텐서를 저장하는 데이터베이스에서 해당 키로 이미지를 검색하여 유사한 이미지를 검색합니다. 순위 결정 기준은 내적 유사도입니다.\n\n## 딥 이미지 유사성 검색\n\n위와 같은 두 마리 강아지가 있는 이미지와 같은 이미지 쿼리가 주어지면 ViT+attentional pooling 컴포넌트를 사용하여 글로벌 수준의 이미지 정보 텐서를 생성한 후, 이 텐서를 동일한 이미지 데이터베이스에서 검색 키로 사용하여 유사한 이미지를 검색합니다.\n\n## 이미지 분류\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이미지 분류 작업에는 \"개\" 또는 \"고양이\" 또는 \"테니스 라켓\"과 같은 각 클래스가 단문인 사전 정의된 클래스 집합이 함께 제공됩니다.\n\n입력 이미지를 분류하려면 먼저 ViT+어텐션 풀링 구성 요소를 사용하여 입력 이미지에 대한 전역 수준 이미지 정보 텐서를 생성합니다.\n\n그런 다음 템플릿을 사용하여 각 클래스에 대한 문장을 생성합니다. 예를 들어 템플릿 \"this is a picture of `class`\"을 사용하면 다음과 같이 생성됩니다:\n\n- 강아지 클래스에 대해 \"이것은 개의 사진입니다\",\n- 고양이 클래스에 대해 \"이것은 고양이의 사진입니다\".\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, 각 문장에 대해 언어 모델 변환기를 사용하여 전역 수준의 텍스트 정보 텐서를 생성하십시오. 입력 이미지를 처리하여, 점곱 방식에서 이미지 정보 텐서와 가장 유사한 전역 수준 텍스트 정보 텐서를 갖는 클래스로 이미지를 분류합니다.\n\n# 이미지 캡션 지원 방법\n\n이미지 캡션 작업은 이미지만으로 텍스트 설명을 생성합니다. 현재의 전체적인 이미지-텍스트 맞춤형 모델은 이를 수행할 수 없습니다. 이 기능을 활성화하기 위해서는 지역 수준의 이미지 정보와 텍스트 정보를 맞춰야 합니다.\n\n캡션은 문장입니다. 입력 이미지에 대한 문장을 모델이 어떻게 예측할 수 있을까요? \"START two running dogs END\"와 같은 특수 단어를 문장에 추가하여, 모델은 다음을 훈련할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 이미지와 첫 번째 단어 \"START\"가 주어지면, 다음 단어 \"two\"를 예측합니다. 특별 단어를 추가하는 좋은 점은 항상 특별 \"START\" 단어를 사용할 수 있다는 것입니다. 우리는 이를 첫 번째 예측된 단어로 취급합니다.\n- 그런 다음 이미지와 지금까지 예측된 단어인 \"START two\"가 주어지면, 모델은 다음 단어 \"running\"을 예측할 수 있습니다.\n- 그런 다음 이미지와 \"START two running\" 단어가 주어지면, 모델은 다음 단어로 \"dogs\"를 예측할 수 있습니다.\n- 그런 다음 이미지와 \"START two running dogs\" 단어가 주어지면, 모델은 마지막 단어 \"END\"를 예측할 수 있습니다.\n\n## 어휘 내 단어 확률 예측\n\n머신러닝에서 다음 단어를 예측하는 것은 일반적으로 어휘 내 모든 단어의 확률 배열을 예측하고, 원하는 단어의 해당 배열 항목의 확률을 모든 다른 단어보다 높게 설정하는 방식으로 구현됩니다.\n\n우리의 (이미지ᵢ, 텍스트ᵢ) 쌍에 대한 이미지 캡션 작업에서, 이전에 예측된 모든 단어 및 로컬 수준 이미지 정보 img_tokensᵢ를 고려하여 다음 단어의 확률을 예측할 수 있는 모델이 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNotationally, we define this word probability predictor as:\n\n![image](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_4.png)\n\nwhere:\n\n- textᵢ is the original text with special words START and END added, such as \"START w₁ w₂ w₃ END\".\n- T is the length, or the number of words, of textᵢ.\n- img_tokensᵢ is the local level image information tensor, which is created by the ViT + attentional pooling component.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 위 단어 확률 예측기를 우리의 신경망이 어떻게 구현하는지?\n\n다음 그림은 이전 그림의 연장선인데, 위의 단어 확률 예측기가 어떻게 구현되었는지를 보여줍니다.\n\n![이미지](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_5.png)\n\n이 그림은 이전 그림에서 대조 손실을 계산하는 단계 3을 공간을 절약하기 위해 생략했습니다. 그러나 단계 3이 여전히 대조 손실을 생성하기 위해 존재함을 상기해주시기 바랍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4단계. 동일한 언어 모델 변환기는 399×512 형태의 로컬 수준 텍스트 정보 텐서인 text_tokensᵢ를 새로운 동일한 형태의 텐서 cross_attended_textᵢ로 변환합니다. 이 변환 과정에서 로컬 수준 이미지 정보 텐서인 img_tokensᵢ가 교차 주의를 통해 결과 텐서에 섞입니다. 이를 통해 로컬 수준 텍스트 정보와 이미지 정보 간의 조정이 이루어집니다. 교차 주의에서는 메인 계산이 점곱이기 때문에 로컬 이미지와 텍스트 해석 간의 이 조정이 더 타당해집니다.\n\n5단계. 399×512 크기의 cross_attended_textᵢ 텐서를 399×20000 word_probability_matrixᵢ로 선형 변환합니다. 20000은 어휘 크기를 나타내는 임의의 숫자입니다. 이 선형 변환층 내에는 학습 가능한 매개변수가 들어 있습니다.\n\nword_probability_matrixᵢ의 각 행의 값, 예를 들어 w₁_prob, w₂_prob은 [0, 1] 범위로 정규화되어 있고, 이들의 합은 1이 되도록 설계되어 있습니다. 각 행을 어휘의 20000 단어에 대한 예측 확률로 해석하기 위한 의도가 담겨 있습니다.\n\n중요한 한 가지 관찰 결과는 word_probability_matrixᵢ에서 첫 번째 행은 시작 단어 START를 주면서 w₁에 대한 단어 확률로 해석됩니다. 두 번째 행은 START와 w₁을 주면서 w₂에 대한 확률을 나타내며, 세 번째 행은 START, w₁ 및 w₂를 주면서 w₃에 대한 확률을 보여줍니다. 입력 텐서와 출력 텐서 간의 이 일회성 조정을 통해 모든 이전 단어를 고려한 다음 단어 예측 메커니즘이 구현됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"단어 확률로 해석된다\"는 무슨 뜻인가요? 이것은 단어 예측 정확도를 측정하는 새로운 손실 함수인 캡션 손실에 관한 맥락에서 의미가 있습니다. 이 캡션 손실은 각 예측된 단어 확률 텐서와 실제 실제 단어 간의 교차 엔트로피를 사용하여 예측 정확도를 측정하며, 이 정확도를 최대화하고자 합니다. 즉, 적도로, 이것을 최소화하고자 합니다.\n\n따라서 w₁_prob에 대한 맥락에서는, 이는 [0, 1] 사이의 값이고 총합이 1인 20000개의 항목으로 구성된 벡터인데, 교차 엔트로피는 이 벡터의 \"two\"라는 단어에 대한 항목이 클 값이어야 한다는 것을 요구한다. 지역 수준 이미지 정보 텐서와 첫 번째 단어 \"START\"가 주어졌을 때, 예측해야 할 다음 단어는 \"two\"여야 합니다.\n\n## 단일 예측된 단어에 대한 교차 엔트로피\n\n다음 그림은 단어 확률 벡터 w₁_prob이 \"two\"라는 단어의 원핫(one-hot) 인코딩과 비교되어 \"two\"에 대한 w₁_prob의 항목이 커야 한다는 것을 요청하는 방식을 보여줍니다. 이 말은 모델이 이 위치에서 \"two\"라는 단어를 예측해야 한다는 것과 동등합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"단어 'two'에 대한 원핫 인코딩은 'two' 단어의 위치에 '1'이 들어가 있고, 다른 모든 위치에 '0'이 들어갑니다. 예측 확률 텐서 w₁_prob에서는 숫자가 랜덤하게 사용되어 해당 값들이 언어 모델 변환기에 의해 생성된 것을 설명합니다.\n\n![image](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_6.png)\n\n이 단어 w₁에 대한 텍스트ᵢ의 교차 엔트로피는 다음과 같습니다:\n\n![image](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_7.png)\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 손실 함수는 \"i\" 위첨자가 있으므로 (이미지ᵢ, 텍스트ᵢ) 쌍에서 한 단어에 대한 것입니다.\n\n단어 당 캡션 손실이 정의되었으므로, 텍스트ᵢ의 전체 문장에 대한 교차 엔트로피는 399개 단어의 교차 엔트로피의 합입니다:\n\n![image](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_8.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n미니배치 전체의 손실은 이미지ₖ와 텍스트ₖ 쌍 (여기서 k는 1부터 N까지)의 모든 손실입니다.\n\n![Image](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_9.png)\n\n## 모든 단어들이 동시에 예측됩니다\n\n이전 그림의 단계 5로 돌아가 봅시다. 이제 우리는 설명에 있는 모든 단어들이 동시에 예측된다는 것을 볼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 첫 번째 행이 P(w₁|START, img_tokensᵢ)를 나타내고,\n- 두 번째 행이 P(w₂|START, w₁, img_tokensᵢ)를 나타내며,\n- 세 번째 행이 P(w₃|START, w₁, w₂, img_tokensᵢ)를 나타내고,\n- 이어서 계속됩니다.\n\n## 전체 손실\n\n우리 모델의 전체 손실 함수는 대조 손실과 캡션 손실의 선형 조합입니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_10.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 개의 λ 계수는 이 두 가지 손실 구성 요소 사이의 가중치로 사용되는 하이퍼파라미터입니다.\n\n## 이미지 캡션 생성\n\n이제 우리는 전체 손실로 훈련된 이 모델이 이미지에 대한 캡션을 생성하는 방법을 볼 수 있습니다:\n\n- 입력 이미지로부터 로컬 레벨 이미지 정보 텐서를 생성하기 위해 ViT+어텐션 풀링 구성 요소를 사용합니다.\n- 시작 단어만을 포함하는 캡션 접두어를 구성합니다.\n- 언어 모델 트랜스포머를 두 번 사용하여(첫 번째로 어텐션된 텍스트를 생성하고, 두 번째로 교차 어텐션된 텍스트를 생성하기 위해), 다음 단어를 예측하기 위해 선형 투영을 거쳐 단어 확률을 생성합니다. 캡션 접두어로부터 다음 단어를 예측합니다. 이 두 단어를 각각 predicted_w₁, predicted_w₂로 부르겠습니다. 이 두 단어는 단계 3의 두 번의 실행에 의해 생성됩니다.\n- 새롭게 예측된 단어를 캡션 접두어에 추가합니다.\n- END 단어가 예측될 때까지 단계 3에서 4를 반복합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n패딩\n\n언어 모델 변환기는 400단어의 고정 길이 입력을 받습니다. 따라서 캡션 생성의 시작 부분에서 변환기로 보내는 입력에는 많은 패딩 단어가 포함됩니다.\n\n예를 들어, 첫 번째 입력 문장은 “START PAD … PAD”이고, 두 번째는 “START predicted_w₁ PAD … PAD”이며, 세 번째는 “START predicted_w₁ predicted_w₂ PAD … PAD” 등입니다. 다시 말해, 문장은 항상 399단어로 패딩되어야 한 뒤 언어 모델 변환기로 전송됩니다.\n\n언어 모델 변환기의 출력(두 번의 적용 후)은 399×20000 단어 확률 행렬입니다. 매번 이 행렬의 한 행만 사용하여 다음 단어의 가장 높은 확률을 찾고, 그 다음 실제 단어를 찾습니다. 이 399×20000 행렬의 다른 행은 사용되지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 프로세스를 최적화하는 방법이 있지만, 이 글에서는 개념을 이해하는 데 집중하고 최적화에 대해 걱정할 필요는 없습니다.\n\n# 이미지 질문 응답을 지원하는 방법은?\n\n이미지 캡션 작업 방식을 이해한 후에 이미지 질문 응답 작업 방식을 동일하게 이해하는 것이 더 쉽습니다:\n\n- 이미지 캡션 작업은 입력 이미지와 텍스트 접두사 \"START\"로 시작됩니다.\n- 이미지 질문 응답 작업은 입력 이미지와 질문을 텍스트 접두사로 시작합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그거에요.\n\n문제의 단어를 무시해야 하는 예측 단어 확률 행렬의 행들에 관해서 섬세함이 있습니다. 하지만 이에 대해 시간을 투자해 이해하려고 결정한다면 이해할 수 있다고 믿습니다.\n\n하지만 이미지에 대한 질문 답변 모델을 구축하려면 이미지 캡션 손실을 다르게 처리해야 할 필요가 있습니다. 이미지 캡션에 대한 손실은 다음 단어 예측 문제를 다루지만 이미지에 대한 질문 응답에 대한 손실은 다른 문장을 제공하면서 문장 예측 문제를 다룹니다.\n\n# Foundation models\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금 우리는 모델이 한 번 훈련된 후에 새로운 작업을 수행하거나 새로운 작업을 수행하도록 다시 훈련할 수 있다는 것을 알 수 있습니다. 이것이 바로 왜 이 모델을 기반 모델이라고 부르는 이유입니다. 기반 모델은 다양한 하류 작업을 위한 시작점으로 기반 또는 부분적으로 완성된 모델을 제공합니다.\n\n# ViT 인코더와 어텐션 풀링\n\n대부분의 모델이 설명되었으니, 이제 마지막 부분으로 넘어가 보겠습니다 — ViT 인코더와 어텐션 풀링을 사용하여 모델이 어떻게 전역 및 지역 수준의 이미지 정보 텐서를 생성하는지 알아봅시다.\n\n## ViT 인코더\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nViT 인코더는 고정 크기 이미지를 패치 목록으로 분할하여 각 패치를 인코딩합니다. 다음 그림은 ViT 인코더가 인코딩된 이미지 패치를 생성하는 방법을 보여줍니다. 여기에서 i 아래 첨자를 무시했습니다. img_tokensᵢ를 소개할 때 사용한 것과 같은 이유입니다. 이전에는 미니 배치에서 텐서에 대해 이야기해야 했는데, 그 텐서는 손실 함수에 들어가며 손실 함수는 미니 배치로 정의됩니다. 그러나 여기에서는 더 이상 미니 배치에 대해 이야기할 필요가 없습니다.\n\n![이미지](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_11.png)\n\nViT 인코더는 고정 크기 이미지를 처리하는 표준 트랜스포머 기반 이미지 인코더로, 논문 '이미지는 16x16 단어만큼 가치가 있다'에서 소개되었습니다. 이 ViT 인코더는 오직 고정 크기 이미지와 함께 동작합니다. 위 그림은 이 ViT 인코더가 작동하는 방식을 보여줍니다:\n\n- 우리의 경우 288×288×3 크기의 컬러 RGB 이미지(따라서 컬러 채널은 3)로 표현되는 고정 크기 이미지를 받았을 때, ViT 인코더는 먼저 이미지를 18×18 크기의 256개 패치로 분할합니다.\n- 그런 다음 ViT 인코더는 각 패치를 1024 길이의 벡터로 변환하여 256×1024 행렬인 'encoded_img_patches'라고 불리는 것을 얻습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nViT 인코더가 정지되어 있습니다\n\n이미지 패치 인코딩 중 ViT 인코더의 매개변수는 정지되어 있음을 유의하십시오. 따라서 ViT 인코더에는 학습 가능한 매개변수가 없습니다.\n\n이전에 소개된 언어 모델 트랜스포머는 왜 정지되지 않고 학습 가능한 매개변수가 포함되어 있는 반면, 여기서 ViT 인코더는 왜 정지되어 있는 걸까요? 저는 저자가 언어 모델 트랜스포머를 동결해 보았지만, 결과가 학습 가능한 모델보다 좋지 않았던 것 같습니다.\n\n정지 또는 비정지 선택에 대해 조금 더 심도 깊게 생각해 보고 싶습니다. ViT 인코더에서 온 한 세트와 언어 모델 트랜스포머에서 온 다른 세트의 벡터 두 개를 맞추기 위해서는, 한 벡터 세트를 만드는 모델의 일부를 고정시키고, 옵티마이저가 다른 벡터 세트를 만드는 모델의 매개변수를 조정하게 하면 충분합니다. 두 모델 구성 요소를 모두 움직이게 유지할 필요는 없습니다. 모델 일부를 동결시키면, 옵티마이저가 학습할 매개변수가 적기 때문에 작업이 더 쉬워집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 주의 집중 풀링은 인코딩된 이미지 패치에 학습 가능한 매개변수를 혼합합니다\n\n다음 단계인 주의 집중 풀링은 인코딩된 이미지 패치를 모델 매개변수와 혼합합니다. 신경망 아키텍처 디자인에서 구현된 인코더를 재사용하는 전형적인 방법입니다. 구현된 인코더를 동결된 상태로 사용하고, 자신의 네트워크에 trainable 레이어를 도입하여 동결된 인코더의 출력을 자신의 네트워크에 혼합합니다. 다음 그림은 주의 집중 풀링이 작동하는 방식을 보여줍니다.\n\n![Image](/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_12.png)\n\n단계 1. ViT는 변환된 256×1024 인코딩된 이미지 패치를 선형으로 다른 동일한 모양의 텐서로 투영합니다. 이 선형 투영에는 trainable 매개변수가 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스텝 2. 선형으로 프로젝트된 encoded_img_patches 텐서는 특성 차원(1024 차원)에서 동일한 모양의 256×512로 나누어지며, 이 중 하나는 v 텐서로, 다른 하나는 k 텐서로 불립니다. 이미 어텐션 메커니즘에서 사용되는 key 행렬 중요성 때문에 k, v 텐서 명칭에 익숙할 것입니다.\n\n스텝 3. 모양이 257×512인 img_queries 텐서가 소개됩니다. PyTorch의 Embedding 클래스에서 제공되며 학습 가능한 매개변수를 포함합니다. img_queries 텐서는 동일한 모양의 q 텐서로 선형으로 프로젝트됩니다. 이 선형 프로젝션에도 학습 가능한 매개변수가 있습니다. img_queries는 257개의 행을 가지고 있습니다. 그래서 인코딩된 이미지 패치의 개수인 256개보다 한 벡터가 더 있습니다. 이 추가 벡터는 전역 수준 이미지 정보를 표현하는 데 사용됩니다.\n\n스텝 4. 유명한 교차 어텐션 매트릭스 곱인 q@kᵀ입니다. 이 곱셈은 모양이 257×256인 sim 텐서를 만듭니다. sim 매트릭스는 내적 매트릭스로, 각 항목은 img_queries 텐서의 열과 k 텐서의 행 사이의 내적 유사성입니다. 이는 입력 이미지의 단일 패치를 인코딩합니다.\n\n스텝 5. 또 다른 교차 어텐션 매트릭스 곱셈인 sim@v가 이어집니다. 이는 모양이 257×512인 텐서를 만듭니다. 스텝 4와 5는 어텐션 메커니즘에서 일반적인 q, k, v 곱셈입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n6단계에서는 sim @ v 곱셈으로 얻은 257×512 텐서를 두 개의 텐서로 분할합니다:\n\n- 첫 번째는 형상이 256×512인 img_tokens 텐서입니다. 이 텐서는 이미지의 패치별 로컬 수준 이미지 정보를 나타낸다는 것을 의미합니다. 즉, img_tokens의 각 1×512 행은 256개의 이미지 패치에서 정보를 나타냅니다.\n- 두 번째는 형상이 1×512인 img_cls 텐서입니다. 이 텐서는 전체 이미지를 요약하는 글로벌 수준 이미지 정보를 나타내는 것으로 해석됩니다.\n\n이 두 텐서는 이전에 설명한 대로 이미지-텍스트 정렬을 수행하는 데 사용됩니다.\n\n## 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글은 이미지-텍스트 이중 모달 기반 모델의 설계를 설명합니다. 설계의 핵심 부분은 대조 손실 함수를 통해 전역 수준에서 이미지와 텍스트 간의 정렬을 설정하고 교차 엔트로피 손실 함수를 통해 로컬 수준에서 정렬을 수행하는 것입니다.\n\n전역 수준의 이미지-텍스트 정렬은 이미지 분류, 텍스트 기반 이미지 검색 및 깊은 이미지 유사성 검색과 같은 작업을 지원합니다. 로컬 수준의 이미지-텍스트 정렬은 이미지 캡셔닝 및 이미지 질문에 대한 답변과 같은 작업을 지원합니다.","ogImage":{"url":"/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_0.png"},"coverImage":"/assets/img/2024-06-19-HowDoesanImage-TextMultimodalFoundationModelWork_0.png","tag":["Tech"],"readingTime":17},{"title":"온라인 플랫폼에서 참여 예측을 위한 딥 러닝","description":"","date":"2024-06-19 06:44","slug":"2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms","content":"\n\n\n![image](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_0.png)\n\n# 요약\n\n에딘버러 대학에서 MSc 논문의 일환으로, 저는 딥러닝 기술을 사용하여 준니버스의 사용자 참여를 예측했습니다. 준니버스는 비과학자들이 행성 탐사와 같은 특정 분야에 기여할 수 있는 온라인 시민 과학 플랫폼입니다. 준니버스는 100편 이상의 논문 발표에 기여했습니다. 실제 환경에서 시민 과학자들은 영국의 농업 유출물이 영국 강의 안전 한 한계를 초과하는 수위의 오염 증가를 입증했습니다.\n\n이 문맥에서 참여는 이전 행동을 고려할 때 플랫폼의 미래 사용을 예측할 수 있는지 여부입니다. 이는 여러 형태로 나타날 수 있습니다, 예를 들면:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 시간 기준 내에서 완료할 작업의 예상 수\n- 현재 작업 세션에서 머무를 시간의 양\n\n우리는 섹션 정의에 초점을 맞추고 사용자가 10, 20 또는 30분 동안 계속 작업할지를 예측하기 위해 노력했습니다. 이는 사용자 작업 T를 작업 세션 W로 그룹화하여 계산되었습니다.\n\n![이미지](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_1.png)\n\n이 정의는 유연성과 Zooniverse, StackOverflow 및 Coursera에서의 이전 사용을 위해 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 접근 방식의 주의 사항 중 하나는 생산적인 온라인 시스템의 한 속성인 플랫폼 시간만을 예측한다는 것입니다. 예를 들어, 올바른 기여를 하는 동기부여된 참여자들을 갖는 것은 고려되지 않습니다. 온라인 시스템에 참여하는 동기를 측정하는 것은 활발한 연구 분야입니다. 심리적 상태인 좌절감과 이해도와 같은 것들이 학습되어 맞춤형 교육 개입에 활용될 수 있는 잠재적인 작업이 있습니다.\n\n# 데이터 및 세션 버킷 알고리즘\n\nZooniverse는 친철하게 2021년 10월 19일에서 2022년 8월 14일까지 시민 과학자들의 클릭스트림 데이터를 제공해 주었습니다. 이 데이터는 미국, 중국, 싱가포르 및 핀란드의 시민 과학자들을 다루었습니다.\n\n원시 데이터 세트에는 다음과 같은 열이 포함되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n| Column Name |               Column Desc               | Column Type |\n|-------------|-----------------------------------------|-------------|\n| id          | 클릭스트림 항목을 식별하는 고유 식별자  | bigint      |\n| user_id     | 사용자를 식별하는 고유 식별자           | bigint      |\n| project_id  | 프로젝트를 식별하는 고유 식별자         | bigint      |\n| workflow_id | 워크플로우를 식별하는 고유 식별자       | bigint      |\n| subject_ids | 고유한 작업 식별자                      | bigint      |\n| country     | 국가 이름                               | str         |\n| latitude    | 국가 위도                               | float       |\n| longitude   | 국가 경도                               | float       |\n| timestamp   | 클릭스트림 타임스탬프                   | bigint      |\n\n\nZooniverse의 계층 구조에서 각 행은 다음 거래를 나타냅니다.\n\n- 사용자가 시스템에 로그인하고 참여할 프로젝트를 선택합니다.\n- 프로젝트는 여러 워크플로우를 포함하며 태스크 그룹화를 수행합니다.\n- 사용자는 프로젝트에 연관된 작업을 수행합니다. 작업에는 여러 주제가 포함될 수 있습니다 (하지만 대부분은 하나만 포함합니다).\n\n\u003cimg src=\"/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_2.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 특성 선택\n\n데이터에는 약 38,500,990개의 고유 이벤트가 포함되어 있습니다. 서로 다른 지역별 이벤트 분포는 다음과 같습니다:\n\n\n| 국가명        | 백분율     |\n|---------------|------------|\n| 핀란드         | 59.4       |\n| 미국           | 25.5       |\n| 싱가포르       | 10.8       |\n| 중국           | 4.3        |\n\n\n위도와 경도는 국가 정보를 중복해서 나타내므로 제거되었습니다. 주제 ID 및 작업 ID도 제거되었는데, 이 정보의 세분화가 참여 패턴 학습에 기여하지 않았기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리 연구에서는 이전 연구에서 강조된 주요 변수가 부족했습니다. 예를 들어, Semenov et al.은 로그인한 사용자들이 더 오랜 시간 동안 작업하고 작업 세션에 대해 더 높은 투자 수준을 나타내는 것으로 발견했습니다. 또한, Mao et al.은 투표 엔트로피가 감소하는 경우, 사용자가 목록에서 반복해서 동일한 옵션을 선택하는 것이 지루함과 참여하지 않음의 유용한 대리자로 작용할 수 있다고 밝혀냈습니다. 이러한 연구 결과는 사용자 참여도를 효과적으로 측정할 수 있는데 사용자 로그인 상태와 투표 패턴을 통해 이를 할 수 있음을 시사했으며, 이러한 측면들이 우리의 현재 분석에는 고려되지 않았습니다.\n\n따라서 우리는 이러한 특성을 근사화하는 데 사용될 수 있는 다양한 속도에서 통계치를 계산해야 했습니다. 이에는 다음이 포함되었습니다:\n\n- 사용자 세션 수의 롤링 번호\n- 현재 세션 내의 시간 및 이벤트 수\n- 과거 세션 간의 평균 시간 및 이벤트 수\n- 이전 세션에서 소비한 시간 및 이전 세션의 이벤트 수\n- 마지막 이벤트로부터의 시간 차 및 과거 이벤트의 평균 시간 차\n\n![image](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# **The Feature Long Tail**\n\nZooniverse 및 Coursera, StackOverflow, Snap과 같은 다른 온라인 플랫폼의 분석 결과를 보면, 매우 소수의 사용자가 플랫폼 활동의 대부분을 지나치게 책임지는 거대한 권한 분포가 있다는 것을 알 수 있습니다. 이 현상은 소수의 고도로 활발한 사용자가 플랫폼 기여를 주도하고 대부분의 사용자는 비교적 활동이 적다는 것을 나타냅니다.\n\n저희의 데이터는 권한 분포에 부합되어, 사용자의 25%만이 세 번 이상의 세션을 완료하였으며, 작업 세션의 68%는 30분 미만으로 지속되었습니다. 이 분포는 사용자 참여의 불균형을 강조하며, 소수의 고도로 활발한 사용자가 플랫폼에서의 활동을 지배하고 있음을 보여줍니다.\n\n많은 사용자에게 시간 순서 채널은 중복될 수 있습니다. 사용자가 단 한 번의 세션만 완료하는 경우, 과거 세션 계산은 일정한 0으로 유지됩니다. 마찬가지로 세션 간 시간과 같은 채널의 경우, 사용자가 두 번의 세션만 완료하는 경우, 과거 세션 채널은 첫 번째 세션에 대해서는 0으로 일정하게 유지되거나 이전 세션 통계의 일정한 값이 될 것입니다. 이 중복성은 드물게 사용하는 사용자에게는 이러한 메트릭이 의미 있는 통찰력을 제공하지 않을 수 있으며, 참여를 효과적으로 평가하기 위해 대안적인 측정 방법이 필요할 수 있다는 것을 시사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_4.png)\n\nThe feature space was designed to represent user information over various time intervals. Because many users only completed two sessions, a lot of data would be repeated for these users. Any model for predicting user behavior would need to be carefully interrogated to ensure it did not memorize these aspects of user behavior and project results that mimicked the most likely user, rather than the projection of users.\n\nTherefore, it was important to design experiments that enabled differentiation between the small number of highly active users and the large population of inactive, short-term contributors.\n\n# Experiments\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n실험 결과 다음 질문에 대한 대답을 제공했습니다. 사용자의 클릭 스트림에 기록된 항목이 주어졌을 때 사용자가 10, 20 또는 30분 동안 계속 작업을 할 것인가?\n\n클릭 스트림의 각 행은 사용자가 지정된 시간대(10, 20 또는 30분) 동안 계속 작업할지 여부를 나타내는 레이블 yiy_iyi로 표시되었습니다. 데이터가 시계열을 따르고 사용자 행동이 이전 이력에 의존한다는 점을 감안하여, 클릭 스트림 F(X_i, Y_i)의 예측은 과거 정보를 포함하여 확장되었습니다. 따라서 예측 함수는 F(X_i, Y_i, | X_i-1, X_i-2, ..., X_0)가 되었습니다.\n\n장기 단기 메모리(Long Short-Term Memory, LSTM) 네트워크는 과거와 현재 정보를 제어하고 가중시키도록 설계된 것으로 이러한 종류의 분석에 적합합니다. 이들은 다른 시계열 작업으로도 효과적으로 적용되어 왔으며, 활동 인식, 행동 인식 및 지진 예측과 같은 작업에 사용되었습니다.\n\n사용자 세션 창을 통해 이러한 함수를 쌓는 과정에서, 모델이 세션 통계, 사용자 동작 변화 및 예상된 참여 사이의 관계를 학습할 수 있을 것으로 기대했습니다¹.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 질문에 대답하려고 실험을 진행했습니다:\n\n- 단기 또는 장기 행동을 예측하는 것이 더 쉬운지 여부.\n- 데이터 창 크기를 확장하면 참여 측정에 기여하는지 여부.\n\n또한 실험에서는 각 이벤트를 독립적으로 고려하는 대신 네트워크 아키텍처와 종속성을 포착하는 것이 모델 성능에 영향을 미치는지 확인하기 위해 RandomForest 분류에 대해 베이스라인을 설정했습니다.\n\n사용자가 현재 세션에서 계속 작업하는지 여부는 이진적입니다. 따라서 RandomForest를 구성하고 LSTM 매개변수 최적화를 위해 유전 알고리즘과 경사 하강법을 적용할 수 있었고, 바이너리 크로스 엔트로피를 사용하여 꼭 맞추었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일괄 처리된 N개의 관측치에 대한 손실 함수는 다음과 같습니다:\n\n![Loss Function](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_5.png)\n\n목표는 yi(실제 예측값)와 네트워크 출력인 y^i 사이의 거리를 최소화하는 것입니다.\n\n# 구현\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터는 타임스탬프에 따라 정렬되고 훈련, 테스트 및 평가 파티션으로 분할되었습니다. 이미지 분류 실험과는 달리 시계열 관측 사이의 관계가 중요하기 때문에 데이터를 섞으면 시퀀스의 일관성이 깨질 수 있습니다. 따라서 데이터를 섞으면 훈련에 후속 관측사항이 포함될 수 있습니다. 예를 들어, 모델이 작업 세션이 10분만 계속된다는 정보를 받기 전에 작업 세션의 1분과 2분의 관측을 받는다면 결과적으로 선견지명을 얻을 수 있습니다.\n\n장기 단기 메모리 (LSTM) 네트워크가 사용자 관측 윈도우의 길이 1, 10, 20, 30, 40에 대해 생성되고 훈련되었습니다. PyTorch Lightning을 사용하여 네트워크를 훈련했는데, 이를 통해 편리한 래퍼(wrapper)를 제공하여 훈련 루프 구현, 로깅 및 메트릭 처리를 감싸줍니다. 10, 20, 30분 동안의 지속을 예측하기 위한 별도의 실험이 수행되었습니다.\n\n첫 번째의 출력이 두 번째로 전파되는 두 개의 LSTM 네트워크를 쌓는 것이 가장 성능이 좋았습니다. 두 번째 레이어는 선형 레이어를 따라, 기능을 하나의 대상 변수로 변환하여 사용자의 세션 지속 확률을 정의했습니다. 최종 선형 레이어 다음의 엔트로피에서 역전파가 계산되었고, 이를 통해 네트워크 전체의 가중치가 업데이트되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결과\n\n실험을 통해 성과가 좋은 모델을 분석했습니다. 각 실험에 대한 정밀도, 재현율 및 AUC를 플롯했습니다. AUC는 분류 임계값 범위에서 실제 양성 비율(재현율)과 거짓 양성 비율(1-정밀도)을 집계합니다.\n\n검증 및 테스트 데이터셋 간의 균형은 모델이 특정 시간 역학에 기반하지 않고 패턴을 학습하고 있다는 것을 나타냅니다. 따라서 참여 패턴이 크게 변하지 않는 한, 새로운 데이터에 적용될 때 모델의 동작을 예측할 수 있을 가능성이 높습니다.\n\n결과는 장기적 예측에서 일반적으로 더 일관성있게 나타납니다. 정밀도는 검증 데이터셋에서 68%에서 70%로, 테스트 데이터셋에서 71%에서 74%로 범위가 확장됩니다. 모든 창에서 재현율은 57%에서 59%까지 범위가 확장됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단기 행동을 예측하는 실험들은 데이터 윈도우의 크기에 민감합니다. 10분 동안의 실험에서, 검증 데이터셋의 정밀도는 64%에서 80%로 범위가 나타나며, 테스트 데이터셋의 경우 66%에서 89%까지 변동합니다. 재현율은 89%에서 30%까지 범위가 나타나며, 최적의 균형을 얻으려면 20개에서 30개의 이벤트 윈도우를 사용하는 것이 좋습니다.\n\n![Image 7](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_7.png)\n\n![Image 8](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_8.png)\n\n![Image 9](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_9.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델의 다양한 행동 양상은 짧은, 중간, 그리고 장기적인 참여를 예측하는 것이 서로 다른 접근 방식이 필요한 다른 도전이라는 것을 보여줍니다. 더 높은 정밀도는 단기적으로 참여하는 사용자를 놓치지 않을 가능성을 보여줍니다. 그러나 장기간의 참여가 떨어질 것으로 예측하는 것은 더 어려운 과제입니다.\n\n실제 상황에서 모델의 선택은 참여하지 않은 사용자를 놓치는 것을 우선시해야 하는지, 아니면 동기를 부여받은 사용자를 잘못 분류하는 것을 우선시해야 하는지를 고려해야 합니다.\n\n30분 참여 예측과 데이터 창의 30개 이전 관측치를 사용한 우수한 모델/실험 구성이 우리의 최고의 성과를 냈습니다. 이 모델은 AUC 스코어가 0.748을 달성했습니다. 이는 Mao Et Al이 30분 예측에서 약 0.76을 달성한 것보다 약간 떨어지는 성과입니다. 이는 데이터의 분산 부족으로 인한 것으로 여겨집니다. 예를 들어, 첫 번째 사용자 세션에는 많은 동일한 변수들이 포함되어 있습니다. 이들은 다음을 포함합니다:\n\n- 누적 플랫폼 시간 및 누적 세션 시간.\n- 플랫폼과 세션 전체 이벤트 수.\n- 이전 세션 통계.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**사용자 세그먼트별 평가**\n\n플랫폼에 작은 기여만 한 사용자를 고려해 역사적 정보를 포함시키면 많은 중복이 발생한다는 것을 알고 있습니다. 이는 대부분의 사용자들에 해당합니다.\n\n다양한 세션 길이 별 성능을 이해하기 위해 사용자 작업 세션에서 지낸 누적 시간에 따른 정확도, 재현율 및 AUC를 검토합니다. 평가와 테스트 세션을 유사성으로 통합합니다. 정밀도와 재현율의 가장 큰 요인은 사용자가 작업 세션에서 얼마나 시간을 보내는지에 달려 있습니다. 데이터 창 크기에 관계 없이, 사용자가 작업 세션에서 60분을 보낸 후에는 모델의 효과가 극적으로 향상됩니다.\n\n![이미지](/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기존 온라인 플랫폼 연구와 일관성 있는 결과를 보여주었으며, 최소한의 히스토리로 사용자에 대한 추론의 어려움을 보여줍니다. 이는 사용자 행동에 대한 우리 이전 분석을 뒷받침합니다.\n\n산키 다이어그램에 의하면, 한 번 사용자 행동이 시작되면 짧은 기간 내에는 비교적 예측 가능합니다. 사용자가 20분 미만의 세션에 참여하면 다음 세션이 또한 20분 미만인 확률이 66.7%이고, 40분을 넘는 세션이 될 확률은 15.4%에 불과합니다. 20분에서 40분 사이의 세션을 가진 사용자는 다음 세션이 20분을 넘을 확률이 약 50%입니다. 이는 사용자가 참여하면 더 오래 플랫폼에 투자하게 되는 경향을 나타냅니다.\n\n사용자의 역사가 거의 없는 사용자 행동을 예측하는 어려움이 분명합니다. 사용자가 5회 세션을 완료하거나 세션이 40분을 초과하면 성능이 일반적으로 향상되며, 가장 안정적인 결과는 20~30 이벤트의 데이터 창을 포함합니다. 이 정보는 Zooniverse 및 다른 온라인 플랫폼의 행동 예측 모델을 개선하는 데 도움이 될 수 있습니다. 모델이 빈번한 기여자에게는 효과적이지 않으므로, 이 그룹의 참여 증가를 위한 개입 시기를 결정하는 데 사용해서는 안 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n연구 결과에서 알 수 있듯이, LSTMs는 시계열 작업 모델링에 효과적입니다. 모든 실험과 모델 구성에서, 사용자가 현재 작업 세션에서 탈락하는지 여부를 학습할 수 있었습니다. 복잡한 특징 집계 대신, 데이터 창을 사용하여 데이터 전체의 의존 관계를 학습했습니다. 이는 GPU를 필요로하며 계산적으로 더 많은 비용이 소요됩니다만, 고전적인 머신 러닝보다 훨씬 적은 시간과 도메인 지식이 필요합니다.\n\n모델은 두 가지 요인으로 제한되었습니다. 첫 번째는 사용자가 언제 탈락할 지 나타내는 기능의 부족이었습니다. 미래 실험에서는 Mao et al.의 연구에서 설명된 특징을 사용하여 모델 성능이 향상되는지 확인해야합니다.\n\n두 번째 제한은 극복하기 어렵습니다. 최소한의 데이터로 사용자에 대해 일반화하는 것은 어렵습니다. 대부분의 Zooniverse 사용자가 단기적이거나 \"관심을 끄는 사람\"이기 때문에 Zooniverse에서 행동을 모델링하는 것은 도전적입니다. 최소한의 경험이있는 사용자에 대한 모델의 한계를 밝히고 설명하는 것은 이전 데이터 분석과 플랫폼 사용 방법 조사만이 가능합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_12.png\" /\u003e\n\n머신 러닝을 시작하기 전에 데이터와 문제를 명확히 이해하는 것이 중요함을 강조합니다. 초기 분석을 통해 모델의 효과에 대한 사전 확률을 설정할 수 있었고, 해당 분석을 통해 이를 확인할 수 있었습니다.\n\n실제 산업 환경에서 모델이 Zooniverse에 대한 개입(뱃지 또는 메시지와 같은)을 시간화하는 데 사용된다면, 하이브리드 접근 방식을 권장합니다. 짧은 기간 사용자에게는 규칙 기반 논리를 사용하여 개입을 타이밍하고, 장기 사용자에게는 모델이 개입을 안내하는 데 효과적일 수 있습니다.\n\n# 각주\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- LSTMs가 어떻게 작동하는지 멋진 설명을 찾으려면 Colah의 LSTMs에 대한 블로그 포스트를 확인하시기를 추천합니다. LSTMs의 역전파 알고리즘을 이해하고 싶다면 Goodfellow Et Al의 \"Deep Learning\"의 10장을 살펴보시는 걸 추천드립니다.","ogImage":{"url":"/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_0.png"},"coverImage":"/assets/img/2024-06-19-DeepLearningtoPredictEngagementinOnlinePlatforms_0.png","tag":["Tech"],"readingTime":11},{"title":"당신이 얼마나 많은 무게를 진다는지 알 수 없어요","description":"","date":"2024-06-19 06:43","slug":"2024-06-19-youhavenoideahowmuchweighticarry","content":"\n\n\n![Image](/assets/img/2024-06-19-youhavenoideahowmuchweighticarry_0.png)\n\n다른 쪽이 항상 푸른 것은 아니에요..\n\n대부분의 사람들은 막내가 되는 것이 축복이라고 생각해요.\n\n맏이나 중간 아이들은 종종 더 어린 아이를 부러워하며, 더 어린 사람이라면 삶이 훨씬 쉬울 것이라고 생각해요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자신의 문제를 말해도 되지 않고, 특히 자신의 감정이 무시당한다는 것에 대해 열거나 살 수 없다고 상상해보세요.\n\n가족들이 당신이 자리하고 있는 자택이 조용한 곳으로 변모하는 것을 목격하고 있는 것을 상상해보세요.\n\n이제는 다른 사람들이 각자 바쁜 삶을 살아가고 있어서 매일 집에서 혼자 남아 있는 상황을 상상해보세요. 당신은 그들이 취해 침대에서 술에 취해 잠들어있는 모습만을 보게 될 것입니다.\n\n그들은 항상 우리는 가족이라고 말했지만, 모든 사람이 나를 잊어버렸다. - 나에게는 항상 너무 바빠서 소홀했다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 어린 사람이 늘 축복이 되는 것은 아니에요... 저에게는 부담이에요.","ogImage":{"url":"/assets/img/2024-06-19-youhavenoideahowmuchweighticarry_0.png"},"coverImage":"/assets/img/2024-06-19-youhavenoideahowmuchweighticarry_0.png","tag":["Tech"],"readingTime":1},{"title":"딥 러닝 모델 최적화를 위한 가중치 양자화","description":"","date":"2024-06-19 06:40","slug":"2024-06-19-OptimizingDeepLearningModelswithWeightQuantization","content":"\n\n![Image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png)\n\n# 📚딥러닝에서 양자화란?\n\n딥러닝에서 양자화에 대해 이야기해보겠습니다. 딥러닝에서 양자화가 왜 중요한지 궁금했던 적이 있나요? 딥러닝과 대규모 언어 모델(LLMs)이 아주 강력하다고는 하지만 많은 도전 과제를 가지고 있어요. 이러한 모델들이 크기 때문에, 많은 계산 성능과 메모리가 필요하여 자원이 제한된 곳에서 사용하기 어려워집니다. 게다가, 예측을 할 때 많은 에너지를 소비할 수 있어서, 한정된 컴퓨팅 자원으로 추론을 하는 것이 불가능해질 수도 있어요.\n\n양자화는 이러한 문제를 해결하기 위해 모델의 크기를 줄여 더 쉽게 다루고, 거의 동일한 성능을 유지할 수 있도록 돕습니다. 이 과정은 모델의 매개변수 수와 데이터 유형의 정밀도를 수정하는 것을 포함합니다. 이를 통해 모델은 가볍고 빠르게 되어, 더 많은 곳에서 실행되고 더 적은 에너지를 사용할 수 있게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델의 크기는 매개변수(크기)의 수를 값들의 정밀도(데이터 형식)로 곱해서 계산됩니다.\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_1.png)\n\n그래서 모델의 크기를 효율적으로 줄이는 방법에 대한 중요한 질문은 무엇일까요? 음, 이를 위한 몇 가지 방법이 있습니다. 매개변수의 수를 줄이거나 데이터 형식을 낮추는 것이 가능합니다. 그러나 매개변수의 수를 줄이는 것은 모델을 더 작고 단순하게 만드는 것을 의미하며, 이는 모델의 품질에 상당한 영향을 줄 수 있어 매우 tricky할 수 있습니다. 더 나은 옵션은 데이터 형식의 정밀도를 조절하는 것입니다. 이것이 양자화가 등장하는 이유입니다 - 이를 통해 모델 가중치를 낮은 정밀도 형식으로 저장할 수 있습니다. 이 방법은 모델의 효과를 유지하면서 가볍고 빠르게 만들어줍니다.\n\n아래는 양자화가 딥러닝에서 중요한 이유인 몇 가지 주요 이유입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 효율성: 양자화는 모델 내의 숫자 값의 정밀도를 부동 소수점에서 정수로 줄입니다. 이겈 간단해 보이지만, 계산을 훨씬 쉽고 빠르게 만들어주어 일을 빨리 처리할 수 있게 해줍니다!\n- 메모리 절약: 부동 소수점에서 정수로 변환할 때 비트 수를 줄이면, 모델 크기를 크게 축소할 수 있습니다. 이것은 저장 공간과 메모리가 제한된 스마트폰이나 임베디드 시스템과 같은 기기에 모델을 배포할 때 아주 유용합니다.\n- 에너지 소비: 모델 크기가 작아지면 모델을 실행하는 데 더 적은 계산 능력이 필요합니다. 이는 특히 배터리로 작동하는 기기에 모델을 배포할 때 유용합니다.\n- 모델 배포: 모델이 작고 더 빠르게 실행될 때, 전용 대규모 서버 대신 다양한 장소에서 모델을 사용하기 쉬워집니다. 이는 자율 주행 자동차나 실시간 번역 서비스와 같이 빠른 응답이 필요한 작업에 중요합니다.\n\n## 양자화 종류\n\n딥러닝에서 양자화는 일반적으로 세 가지 주요 유형으로 구분됩니다:\n\n- 사후 학습 정적 양자화 (PTQ): PTQ는 이미 훈련된 모델을 추가로 훈련하지 않고(가중치 및 활성화 모두) 줄이는 작업을 수행합니다. 사용하기 매우 간단하고, 훈련을 마친 후 모델을 빠르게 작게 만들어주는 데 도움이 됩니다. 단지 기억해 두세요! 훈련 중에 모델을 양자화하지 않기 때문에 원본 모델과 성능에 차이가 있을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![그림](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_2.png)\n\n- 집행 후 다이나믹 양자화 또는 다이나믹 양자화: 이 방법은 훈련이 완료된 후에 모델 가중치를 줄이고, 활성화를 동적으로 처리합니다(추론 중에). 이 방법은 다른 유형과 크기의 입력을 다루는 모델에 아주 편리합니다. 그러나 모델이 실행되는 동안 활성화를 실시간으로 조정하기 때문에 정적 양자화보다 약간 느릴 수 있습니다. 또한, 이 방법의 또 다른 단점은 모든 장치가 이 동적 접근을 처리할 수 없다는 점이므로 어디에 이 방법을 사용할지 계획할 때 고려해야 합니다.\n\n![그림](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_3.png)\n\n- 양자화 인식 훈련(QAT): 마지막 공통 방법은 QAT입니다. 이는 양자화를 직접 훈련 과정에 통합하여 모델 성능을 유지합니다. 이것은 모델 최적화 중 양자화 효과를 고려함으로써 위 두 가지 방법보다 성능을 더 잘 보존할 수 있습니다. 결과적으로, QAT는 조금 더 많은 시간과 에너지를 요구합니다. 학습 작업 및 양자화를 동시에 조정하기 때문에 더 오래 훈련에 걸리고 구현하기는 훨씬 복잡합니다. 정확도가 필요한 경우, QAT는 모델을 효과적이고 효율적으로 유지하는 데 큰 차이를 만들 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_4.png)\n\n# 📖 부동 소수점 숫자 구성\n\n데이터 형식을 변경하면 모델 크기를 줄이는 이유에 대해 자세히 살펴보겠습니다. 컴퓨터에서 숫자에 대해 이야기할 때, 0과 1에 대해 모두 이야기합니다. 이진 인코딩 시스템은 컴퓨터 작업의 기초이며, 정수 및 부동 소수점 숫자와 같은 다양한 숫자 표현은 이러한 비트를 구성하는 특정 방법을 갖고 있습니다.\n\n## 정수 표현\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정수에 대한 가장 일반적인 형식은 부호 있는 정수와 부호 없는 정수입니다.\n\n부호 없는 정수:\n\n* 비트: 모든 비트가 숫자의 크기를 나타냅니다.\n* 범위: 0부터 2n-1까지 (여기서 n은 비트의 수)입니다.\n\n부호 있는 정수:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 첫 번째 비트는 숫자가 양수 (0)인지 음수 (1)인지를 나타냅니다.\n- 나머지 비트는 숫자의 크기 또는 이른바 크기를 보여줍니다. 여기서 이진값은 음수 숫자에 대해 반전되고 1이 더해집니다.\n- 범위: -2ⁿ⁻¹ ~ 2ⁿ⁻¹-1\n\n## 부동 소수점 표현\n\n- 부호 비트 (1 비트): 숫자의 부호를 나타냅니다; 0은 양수이고 1은 음수입니다.\n- 지수: 바이어스로 조정된 지수를 나타냅니다. 저장된 지수에서 바이어스를 빼면 실제 지수가 계산됩니다. 지수는 사실적으로 숫자의 중요한(또는 가수) 부분을 2의 거듭제곱으로 확장하여 부동 소수점 숫자가 매우 크거나 매우 작은 값을 간결한 형식으로 표현할 수 있도록 합니다.\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 유의적/맨티사: 숫자의 정밀도를 나타냅니다.\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_6.png)\n\n## 다른 데이터 유형의 생성\n\nFloat32: 숫자를 나타내는 데 32비트를 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 부호에는 1 비트가 사용됩니다\n- 지수에는 8 비트가 사용됩니다\n- 나머지 23 비트는 유효숫자를 나타냅니다\n- FP32는 높은 정밀도를 제공하지만, 계산 및 메모리 사용량이 많은 것이 단점입니다.\n\n이미지 링크:\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_7.png)\n\nFloat16: 숫자를 저장하는 데 16 비트를 사용합니다\n\n- 부호에는 1이 사용됩니다\n- 지수에는 5가 사용됩니다\n- 유효숫자에 10이 사용됩니다\n- 이로 인해 더 효율적인 메모리 사용 및 빠른 연산이 가능하지만, 범위 및 정밀도가 줄어들어 숫자의 불안정성을 초래할 수 있고, 이는 모델 정확도에 영향을 줄 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_8.png\" /\u003e\n\n\"float\"이 종종 \"전체 정밀도\"(4 바이트)로 불리는 반면, \"float16\"은 \"반 정밀도\"(2 바이트)로 불립니다.\n\n## 일반적인 하위 정밀도 데이터 유형\n\n양자화를 수행하는 두 가지 일반적인 방법이 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- float32 -` float16\n- float32 -` int8\n\n## 양자화의 효과\n\n대규모 모델인 BLOOM과 같은 경우, 약 1760억 개의 파라미터를 갖고 있는 모델을 다룬다고 상상해봅시다. float32를 사용하면 모델 크기는 176*10**9 x 4 바이트 = 704GB가 됩니다. 그러나 float16로 전환하면 352GB로 줄어들고, int8로 전환하면 176GB로 줄어듭니다. 이는 메모리 공간에서 굉장한 절감을 의미합니다. 176GB라도 여전히 많은 개인 컴퓨터에 대한 큰 도전이 될 수 있습니다.\n\nfloat32에서 float16으로 양자화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nfloat32에서 float16로 변경하는 것은 꽤 간단합니다. 왜냐하면 두 형식 모두 숫자를 표현하는 방식이 비슷하기 때문이죠. 그러나 양자화를 구현하기 전에 몇 가지 고려해야 할 사항이 있습니다:\n\n- 소프트웨어 및 하드웨어 호환성: 먼저, 사용 중인 패키지가 float16을 처리할 수 있는지 확인해보세요. 또한, 하드웨어가 지원하는지도 확인해야 합니다. NVIDIA의 튜링 및 암페어 또는 구글의 TPU와 같은 현대 GPU 및 TPU는 float16과 잘 작동하도록 제작되었기 때문에 학습 및 추론 프로세스가 속도가 향상됩니다. 그러나 Intel CPU는 저장 유형으로 float16을 지원하고 있지만, 연산은 float32로 변환한 후에 수행됩니다.\n- 정밀도 요구 사항: 모델이 얼마나 정밀해야 하는지를 고려해보세요. 다른 말로, 낮은 정밀도에 얼마나 민감한지 생각해보세요. 일부 의료 영상 처리와 같이 모든 작은 세부 사항이 중요한 작업/모델의 경우, float16과 같은 낮은 정밀도로 내려가면 중요한 세부 사항이 손실되어 모델의 성능에 영향을 줄 수 있습니다.\n\nfloat32에서 int8로의 양자화\n\nfloat32에서 int8로의 양자화는 더 어렵습니다. 왜냐하면 int8은 256가지만 다룰 수 있고, 이는 float32가 다루는 광대한 범위와는 비교할 수 없이 작습니다. float32는 약 -3.4e38 ~ 3.4e38 범위에서 약 40억 개의 숫자를 처리합니다. 이 과제는 float32 값의 특정 범위를 int8의 매우 제한된 공간에 어떻게 압축할지 찾는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n계속해서 진행하고, 우리가 int8로 효과적으로 양자화하는 방법에 대해 자세히 살펴볼 거예요.\n\n# int8로 양자화하는 방법\n\n## 균일 양자화\n\n이 방법은 입력을 출력으로 매핑하는 간단한 선형 함수를 사용합니다. 선에 놓인 간격이 동일한 점들을 상상해보세요 — 균일 양자화는 이들이 변환될 때 모두 잘 정렬되어 있도록 유지합니다. 빠르고 쉽지만, 여기 주목할 점이 있어요: 데이터가 처음부터 고르게 분포되어 있지 않다면, 데이터의 분포를 항상 잘 보존하지는 못할 수도 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 부동 소수점 값을 두 숫자 α와 β 사이에 매핑하는 것이 균일 양자화의 작동 원리입니다. 이 두 값을 α와 β라고 부르겠습니다. 그리고 그 값을 [-2ᵇ⁻¹, 2ᵇ⁻¹–1]의 일정 범위로 변환합니다. 이 범위를 벗어나는 값이 있다면 가장 가까운 한계값으로 잘립니다 — 이것을 클리핑이라고 합니다.\n\n부동 소수점 숫자(xf)를 8비트 표현(xq)으로 변환할 때에는 스케일 팩터(S)를 사용합니다. 이는 원본 데이터를 int8의 새로운 형식 [-128, 127]에 맞춰주는 데 도움을 줍니다. 그리고 원본 데이터의 0은 새 데이터의 0과 일치하게 됩니다. 이것이 대칭 양자화라고 부르는 개념입니다.\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_9.png)\n\n## 양자화 스케일(S)를 계산하는 방법?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식으로 바뀐 텍스트입니다:\n\nCompute the max value of xf:\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_10.png)\n\nCompute the quantization scale (S):\n\n![image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_11.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n컨버팅 비티에이엘 테이블:\n\n\n| 값       | 양자화된 값 |\n|----------|------------|\n|   3.14   |    3.0     |\n|  -2.718  |   -2.5     |\n|   6.626  |    6.5     |\n\n\n오리지널 값으로 돌아가기:\n\n\n| 양자화된 값 | 값       |\n|------------|----------|\n|    3.0     |   3.14   |\n|   -2.5     |  -2.718  |\n|    6.5     |   6.626  |\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대칭 양자화는 제로 주변을 균일하게 취급합니다. 즉, 데이터의 상승 및 하락(양수 및 음수 값)을 균형 있게 처리하여 모든 것이 균형을 이룹니다. 데이터가 제로 중심이거나 즉, 제로 양쪽으로 고르게 퍼져있을 때 특히 유용합니다.\n\n하지만 여기서 중요한 점은 대칭 양자화가 제로 주변에 깔끔하게 정렬되지 않은 데이터에는 부적합할 수 있다는 것입니다. 데이터가 더 치우쳐져 있다면, 이 방법은 범위의 모든 부분을 동일하게 처리하기 때문에 더 많은 양자화 오류를 발생시킬 수 있습니다.\n\n이 문제를 해결하기 위해, 비균일 또는 비대칭 양자화가 있습니다. 때로는 이를 아핀 양자화라고도 부릅니다. 이 기술은 데이터의 다른 부분에 대해 서로 다른 방식으로 스케일과 제로 포인트를 조정하기 때문에 대칭적으로 분포되지 않은 데이터 집합에 더 적합합니다.\n\n간단한 예제로 이를 시도해 봅시다. 우리가 NumPy의 랜덤 정규 함수를 사용하여 가중치 배열을 만들었기 때문에 배열은 제로 중심입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 원본 가중치 배열\nweights = np.random.normal(size=(20000)).astype(np.float32)\nweights = torch.from_numpy(weights)\nprint(weights.mean(), weights.min(), weights.max())\n\u003e\u003e\u003e tensor(0.0057) tensor(-3.9224) tensor(4.4791)\n```\n\n![Image](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_14.png)\n\n```js\n# 대칭 방식을 사용하여 양자화\nweights_sym_quant, weights_sym_dequant = symmetric_quantize(weights)\nprint(weights_sym_quant.double().mean(), weights_sym_quant.double().min(), weights_sym_quant.double().max())\nprint(weights_sym_dequant.double().mean(), weights_sym_dequant.double().min(), weights_sym_dequant.double().max())\n\u003e\u003e\u003e tensor(0.1585, dtype=torch.float64) tensor(-111., dtype=torch.float64) tensor(127., dtype=torch.float64)\n\u003e\u003e\u003e tensor(0.0056, dtype=torch.float64) tensor(-3.9148, dtype=torch.float64) tensor(4.4791, dtype=torch.float64)\n```\n그런 다음 대칭 양자화 함수를 적용하면, 새로 양자화된 배열도 거의 0에 가까운 평균값을 가지며, 최솟값은 -111이고 최댓값은 127입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제, 우리는 데이터를 원래의 부동 소수점 범위로 되돌리는 시도를 할 것입니다. 이것이 바로 양자화 해제(dequantization)라고 불리는 과정입니다. 양자화를 해제한 후에는, 양자화 해제된 배열의 평균, 최소값 및 최대값이 대략 원래 값과 동일해야 합니다.\n\n## 비균일 양자화\n\n비대칭 양자화의 경우, 양자화 값을 계산할 때 정수가 추가됩니다. 이것을 제로 포인트 (Z)라고 합니다. Z는 float32 영역에서 0의 값과 대응합니다.\n\n양자화된 값 계산:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_15.png)\n\n스케일(S) 값을 계산하세요:\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_16.png)\n\n제로포인트(Z) 값을 계산하세요:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Optimizing Deep Learning Model with Weight Quantization](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_17.png)\n\n비대칭 양자화는 범위의 다른 부분에 대해 스케일과 제로 포인트를 다르게 조정하여 비대칭 데이터 분포를 효과적으로 처리할 수 있습니다. 그러나 스케일과 제로 포인트 2개의 매개변수가 필요하기 때문에 구현 및 최적화 과정이 복잡해질 수 있고, 양자화 및 역양자화 단계에서 추가적인 계산 능력이 필요할 수 있습니다.\n\n비대칭 양자화는 데이터 분포를 조정함으로써 데이터 범위 내에서 스케일과 제로 포인트를 다르게 조정하여 불규칙한 데이터 분포를 훌륭히 처리할 수 있습니다. 데이터가 양자화 다리를 건널 때 더 편안하게 걷도록 데이터의 신발을 맞춤 제작하는 것과 같은 원리입니다!\n\n하지만 여기서 중요한 점은 두 가지 매개변수 - 스케일과 제로 포인트 - 를 사용자 정의하기 때문에 설정 및 세밀한 조정이 약간 까다로울 수 있습니다. 게다가, 양자화 및 역양자화 시에 조금 더 많은 계산 노력이 필요할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 비대칭 방식을 사용하여 양자화하기 - 정규 분포 데이터\nweights_assym_quant, weights_assym_dequant = assymmetric_quantize(weights)\nprint(weights_assym_quant.double().mean(), weights_assym_quant.double().min(), weights_assym_quant.double().max())\n\u003e\u003e\u003e tensor(-8.8287, dtype=torch.float64) tensor(-128., dtype=torch.float64) tensor(127., dtype=torch.float64)\n\u003e\u003e\u003e tensor(0.0056, dtype=torch.float64) tensor(-3.9207, dtype=torch.float64) tensor(4.4808, dtype=torch.float64)\n\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_18.png)\n\n정규 분포 배열 예제를 살펴보았지만, 이것은 간단한 경우입니다. 좀 더 어려운 비정규 분포를 가진 경우를 시도해 보겠습니다.\n\n```python\n# 비정규 분포 데이터 생성\nskewed_weights = np.random.exponential(scale=2, size=20000) - 7 # 데이터를 음수 값과 양수 값을 모두 가지도록 이동\nskewed_weights = torch.from_numpy(skewed_weights)\nprint(skewed_weights.mean(), skewed_weights.min(), skewed_weights.max())\n\u003e\u003e\u003e tensor(-5.0192, dtype=torch.float64) tensor(-6.9999, dtype=torch.float64) tensor(16.4827, dtype=torch.float64)\n```\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 대칭 방식을 사용하여 양자화\nweights_sym_quant, weights_sym_dequant = symmetric_quantize(skewed_weights)\nprint(weights_sym_quant.double().mean(), weights_sym_quant.double().min(), weights_sym_quant.double().max())\n\u003e\u003e\u003e tensor(-38.6737, dtype=torch.float64) tensor(-54., dtype=torch.float64) tensor(127., dtype=torch.float64)\n```\n\n이 분포는 정규분포가 아니기 때문에 양자화된 가중치의 평균 값은 -38.67, 최솟값은 -54이며 최대값은 127입니다. 문제는 전체 int8의 범위가 완전히 활용되지 않는다는 것입니다. 최솟값이 -64인데 이는 양자화가 사용 가능한 비트를 최대로 활용하지 못한다는 것을 의미합니다. 이로 인해 많은 서로 다른 값을 동일한 양자화된 값으로 반올림하여 고유성과 데이터 내의 세부 정보를 상실할 수 있습니다.\n\n가중치를 다시 부동소수점으로 역양자화할 때, 평균값은 대략적으로 원래 가중치의 값에 도달합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 비대칭 방법을 사용하여 양자화 - 정상 분포 데이터\nweights_assym_quant, weights_assym_dequant = assymmetric_quantize(skewed_weights)\nprint(weights_assym_quant.double().mean(), weights_assym_quant.double().min(), weights_assym_quant.double().max())\n\u003e\u003e\u003e tensor(-106.5096, dtype=torch.float64) tensor(-128., dtype=torch.float64) tensor(127., dtype=torch.float64)\n```\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_20.png)\n\n양자화된 값들을 원래의 부동 소수점 범위로 돌리면 어떻게 되는지 알아보겠습니다. 대칭 방법에서의 값들은 원래 데이터와 비교했을 때 그렇게 고른 분포를 보여주지 않는데, 비대칭 방법의 경우와는 다르게 퍼져 있는 것이 흥미롭습니다.\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_21.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 📝 코드 구현\n\n파이토치 양자화를 사용하여 양자화 예제를 작업해 보겠습니다.\n\n이 예제에서는 MobileNetV2 모델과 MINIST 데이터셋을 사용할 것입니다. 데이터셋에 대한 자세한 내용은 여기에서, 그리고 데이터셋을 로드하는 방법은 여기에서 확인할 수 있습니다.\n\nMobileNetV2를 양자화하려면 네트워크에 일부 수정을 구현해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- In InvertedResidual 블록의 torch.add가 nn.quantized.FloatFunctional()로 대체되었습니다.\n\n```js\n- self.skip_add = torch.add()\n+ self.skip_add = nn.quantized.FloatFunctional()\n```\n\n- 양자화 전에 Conv+BN 및 Conv+BN+Relu 모듈을 결합하는 fuse_model() 메서드가 추가되어 메모리 액세스를 줄이고 수치 정확도를 향상시켜 모델의 효율성을 높입니다. 이 실천은 양자화된 모델에서 일반적입니다.\n\n```js\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=10, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n+     self.quant = QuantStub()\n+     self.dequant = DeQuantStub()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPyTorch 프레임워크를 사용하여 모델을 양자화하는 일반적인 흐름은 다음과 같습니다:\n\n```python\n# 양자화하기 전에 Conv+BN 및 Conv+BN+Relu 모듈을 퓨즈합니다 (이 작업은 숫자를 변경하지 않습니다)\ndef fuse_model(self, is_qat=False):\n    fuse_modules = torch.ao.quantization.fuse_modules_qat if is_qat else torch.ao.quantization.fuse_modules\n    for m in self.modules():\n        if type(m) == ConvBNReLU:\n            fuse_modules(m, ['0', '1', '2'], inplace=True)\n        if type(m) == InvertedResidual:\n            for idx in range(len(m.conv)):\n                if type(m.conv[idx]) == nn.Conv2d:\n                    fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)\n```\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_22.png)\n\n- QConfig를 사용하여 연산자가 어떻게 관찰되어야 하는지 구성합니다. 이 코드에서는 단순한 최소/최대 관찰자를 사용하여 양자화 매개변수를 결정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nquantized_model.qconfig = torch.ao.quantization.default_qconfig\n```\n\n2. 준비하기: 지정된 qconfig를 기반으로 Observer/FakeQuantize 모듈을 모델에 삽입합니다.\n\n```js\ntorch.ao.quantization.prepare(quantized_model, inplace=True)\n```\n\n3. 모델을 캘리브레이션하여 가중치와 활성화에 대한 양자화 매개변수를 결정합니다. 이는 훈련 데이터셋으로 수행됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nevaluate(quantized_model, criterion, data_loader, neval_batches=num_calibration_batches)\n\n\n4. Convert the calibrated model to a quantized model.\n\n\ntorch.ao.quantization.convert(quantized_model, inplace=True)\n\n\nWe will load the pretrained model for the MNIST dataset as the original model, quantize this model, and compare the results in both size and performance. Performance is evaluated using Top 1 and Top 5 Accuracy.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 👑결과👑\n\n![결과 이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_23.png)\n\n모델 크기가 8.9MB에서 2.35MB로 줄어든 건 정말 놀라운 일이에요! 거의 4배나 크기가 줄었어요! 🌟\n\n성능도 매우 좋아서, 양자화된 모델의 최상위 1위와 최상위 5위 정확도는 원본과 비슷합니다. 최대/최소 옵서버만 사용해서 양자화 매개변수를 선택한 것에도 불구하고요. 그래서 거의 공간을 차지하지 않으면서도 약속받는 결과를 확인할 수 있어요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다른 옵저버의 결과를 확인해보면, 이 옵저버와 비교하여 성능이 어떤지 알 수 있어요. 새 옵저버는 자동으로 양자화 매개변수를 결정할 거에요.\n\n디폴트 qconfig를 사용하는 대신에, 구성을 x86 아키텍처로 설정할 거에요. 이 아키텍처는 가중치를 채널 단위로 양자화하고, 활성화도를 수집하고 최적의 양자화 매개변수를 선택하는 히스토그램을 사용해요. 나머지 흐름은 동일하게 유지돼요.\n\n```js\nper_channel_quantized_model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n```\n\n![이미지](/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_24.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n새 양자화 아키텍처로 얻는 결과는 Top 1 및 Top 5 성능 모두 강력한 출력을 유지한다는 것을 관찰할 수 있습니다. 그리고 가장 좋은 부분은 또 다른 양자화 방법과 모델 크기를 거의 동일하게 유지할 수 있다는 것입니다.\n\n**Notebook**: [링크]\n\n# 📕 최종 생각\n\n마무리하며, 사후 훈련 동적 양자화는 머신러닝 모델을 배포하기 위해 최적화하는 편리하고 효율적인 요령입니다. 이 방법은 모든 훈련이 완료된 후 가중치와 활성화를 조정함으로써 원래의 부동 소수점 모델과 가끔 수용 가능한 성능을 보장하며 동등한 성능을 나타낼 수 있습니다. AI 프로젝트를 빠르고 가벼워 만들고 싶다면, 이 방법이 진정한 게임 체인저가 될 수 있을 것입니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n- [Achieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with TensorRT](https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/)\n  \n- [PyTorch 공식 문서 - 양자화](https://pytorch.org/docs/stable/quantization.html)\n  \n- MNIST 상업적 이용을 위한 라이선스: GNU General Public License v3.0. 링크: [MNIST 라이선스](#)","ogImage":{"url":"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png"},"coverImage":"/assets/img/2024-06-19-OptimizingDeepLearningModelswithWeightQuantization_0.png","tag":["Tech"],"readingTime":16},{"title":"선형 회귀를 사용한 비농업 부문 고용 예측","description":"","date":"2024-06-19 06:38","slug":"2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression","content":"\n\n\n![image](/assets/img/2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression_0.png)\n\n선형 회귀는 가끔 단숨함과 선형 종속적 결과 때문에 무시당하기도 합니다. 그러나 많은 복잡한 예측 작업을 선형 회귀를 사용하여 해결할 수 있습니다. 한 번 매우 성공한 헤지 펀드 관리자로부터 들은 적이 있는데, 그들의 정교한 거래 모델 중 하나는 간단한 선형 회귀 모델에 의존했다고 말씀하셨습니다.\n\n이 기사에서는 파이썬을 사용하여 간단한 선형 회귀를 통해 미국의 고용 데이터를 예측하는 방법을 보여줍니다.\n\n# 비농업 실업자수란 무엇인가요?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비농업 실업률(NFP)은 미국에서 중요하게 살펴보고 있는 경제 지표로, 미국 노동 시장의 건강 상태를 판단하는 중요한 기준 역할을 합니다. 매달 발표되는 이 취업 보고서는 미국의 고용 상황을 종합적으로 보여줍니다. 이 보고서에서는 농업 부문, 가정부 및 비영리 기관의 일자리를 제외한 미국 내 유료 종업원 수의 순 증가량이 공개됩니다.\n\n이 NFP 보고서는 지난 달 동안 특정 부문을 제외한 미국 내 유료 종업원 수의 순 증감을 보여줍니다. 이는 미국 경제의 전반적인 강도와 방향에 대한 통찰력을 제공하기 때문에 특히 중요합니다. 데이터를 분석함으로써 경제학자, 정책 결정자, 투자자 및 기업은 노동 시장의 건강 상황을 평가하고 취업 트렌드를 추적하며 경제 정책, 투자, 채용 관행에 관한 판단을 내릴 수 있습니다.\n\n또한, NFP 보고서는 금융 시장을 넘어서 통화 환율, 이자율 결정 및 기타 금융 상품 등에도 영향을 미칩니다. 매달 예측을 시도하며 우리만의 모델을 적용해 보고 나온 결과를 살펴봅시다.\n\n우리는 방향성 정확도 및 RMSE를 이용하여 예측을 평가할 것입니다. 이들이 의미하는 것은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 방향성 정확도는 NFP가 상승인 경우와 NFP가 하락인 경우를 비교하여 올바른 예측 수와 예측 수를 비교하는 단순한 이진 측정입니다.\n- RMSE는 평균 제곱근 오차를 의미합니다. 데이터 세트에서 예측 값과 실제 값 사이의 오차의 평균 크기를 측정하는 지표입니다.\n\n더 많은 작업을 보고 싶으시면, 제 웹사이트에서 PDF 도서 카탈로그를 확인하실 수 있습니다. 아래 그림에 첨부된 링크를 따라가시면 됩니다:\n\n![PDF books catalogue](/assets/img/2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression_1.png)\n\n# 알고리즘 만들기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알고리즘을 생성하기 전에 선형 회귀 알고리즘이 어떻게 작동하는지 알아보겠습니다.\n\n선형 회귀는 데이터 점들의 산점도를 통해 직선을 그리는 것과 같습니다. 집의 크기와 가격에 관한 데이터가 있다고 상상해보세요. 이 정보를 수집하여 대부분의 점을 지나가는 최적의 선을 찾는 데 사용합니다. 이 선은 일반적인 추세를 나타냅니다: 집이 커질수록 가격도 올라가는 경향이 있습니다.\n\n이 과정에는 수학적 계산이 필요하여 선이 가능한 모든 데이터 점에 가장 가깝게 위치하도록 합니다. 이 선을 갖고 나면 집의 크기를 알고 있다면 집의 가격을 예측할 수 있습니다. 이 선은 기울기(크기에 따라 가격이 얼마나 변하는지)와 y절편(크기가 0일 때의 가격, 집에 대해서는 의미가 없음)을 갖고 있습니다. 따라서 이는 두 가지 사이의 관계를 이해하고 예측하는 방법입니다.\n\n작업 계획은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 해당 GitHub 저장소에서 NFP 데이터를 다운로드하고 Python에 업로드하세요.\n- NFP 데이터의 차이를 취하세요. 이미 stationary 상태이지만 방향성 정확성을 측정하기 위해 이를 수행합니다.\n- 데이터를 학습 세트와 테스트 세트로 분할하세요.\n- 모델을 학습시키는 데에는 마지막 다섯 개의 NFP 변경 사항을 특성으로 사용하세요. 그런 다음 테스트 세트의 이전에 본 적이 없는 데이터에 대해 예측하세요.\n- 예측된 데이터와 실제 데이터를 평가하고 비교하세요.\n\n아래 코드를 사용하여 프로세스를 구현하세요 (저장소에서 NFP 데이터를 다운로드해야 함):\n\n```js\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef data_preprocessing(data, num_lags, train_test_split):\n    # 데이터 처리를 위해 준비\n    x = []\n    y = []\n    for i in range(len(data) - num_lags):\n        x.append(data[i:i + num_lags])\n        y.append(data[i+ num_lags])\n    # 데이터를 넘파이 배열로 변환\n    x = np.array(x)\n    y = np.array(y)\n    # 데이터를 학습 및 테스트 세트로 분할\n    split_index = int(train_test_split * len(x))\n    x_train = x[:split_index]\n    y_train = y[:split_index]\n    x_test = x[split_index:]\n    y_test = y[split_index:]\n    \n    return x_train, y_train, x_test, y_test \n# 시간 인덱스가 설정되지 않았다면 설정하세요\ndata = pd.read_excel('NFP.xlsx').values\ndata = np.reshape(data, (-1))\ndata = np.diff(data)\nx_train, y_train, x_test, y_test = data_preprocessing(data, 5, 0.80)\n# CatBoostRegressor 모델 생성\nmodel = LinearRegression()\n# 데이터에 모델 학습\nmodel.fit(x_train, y_train)\n# 학습에 사용된 데이터에 대해 예측\ny_pred = model.predict(x_test)  # 예측을 위해 X 대신 X_new 사용\n# 오리지널 사인파와 예측값 플롯\nplt.plot(y_pred[-50:], label='예측 데이터', linestyle='--', marker='o')\nplt.plot(y_test[-50:], label='실제 데이터', marker='o')\nplt.legend()\nplt.grid()\nplt.axhline(y=0, color='black', linestyle='--')\nimport math\nfrom sklearn.metrics import mean_squared_error\nrmse_test = math.sqrt(mean_squared_error(y_pred, y_test))\nprint(f\"테스트의 RMSE: {rmse_test}\")\nsame_sign_count = np.sum(np.sign(y_pred) == np.sign(y_test)) / len(y_test) * 100\nprint('방향성 정확도 = ', same_sign_count, '%')\n```\n\n다음 그림은 실제 데이터와 예측 데이터를 비교합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 코드의 출력은 다음과 같습니다:\n\n```js\n테스트의 RMSE: 188.81\n방향 정확도 = 69.65%\n```\n\n69.65%의 방향 정확도로 보아, 모델은 마지막 변화에서 긍정적 또는 부정적 변화가 발생할지를 예측할 수 있는 것으로 보입니다. RMSE는 예측이 약 188의 오류 항을 가지고 있음을 보여줍니다. 이는 개선할 여지가 많이 남아 있다는 것을 의미합니다.\n\n개선은 특성을 더 추가하거나 래깅된 입력의 수를 변경하고 다른 조건을 추가하는 방식으로 이루어질 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Graph](/assets/img/2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression_2.png)\n","ogImage":{"url":"/assets/img/2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression_0.png"},"coverImage":"/assets/img/2024-06-19-ForecastingNon-FarmPayrollsWithLinearRegression_0.png","tag":["Tech"],"readingTime":5},{"title":"인간형 로봇의 문제점","description":"","date":"2024-06-19 06:36","slug":"2024-06-19-TheProblemsWithHumanoidRobots","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-TheProblemsWithHumanoidRobots_0.png\" /\u003e\n\n요즘에 협력로봇에서 우리가 인간형 로봇을 만들고 있는지 물어봤어요. 아니요... 간단히 말해서, 저는 인간형 로봇을 믿지 않아요. 로봇 개나 고양이나 말에 대해서도 별로 믿지 않아요.\n\n하지만 이런 노력 뒤에 있는 기술에 감탄을 금치 못해요. Boston Dynamics의 네 다리 개 모양 로봇인 Spot Mini은 훌륭한 공학적 성취입니다. iPhone이나 FreeStyle Libre Continuous Glucose Monitor만큼 제 마음에 감탄을 일으키는데, 그 정도의 유용성은 아니에요.\n\nBoston Dynamics의 Atlas도 놀라운데, 그 구동기의 강력함과 로봇 제어 루틴의 품질만큼이나 인상적해요. Agility Robotics의 Digit 또한 놀라운 로봇이자 멋진 공학 작품입니다. Shadow Dextrous Hand도 마찬가지로 멋진 공학 작품이에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 시장에는 다른 기업도 있지만, 이 기업들은 현재 다른 기업들보다 훨씬 앞서 나가 있습니다. 멋진 로보틱스 비디오들이 있긴 하지만 (사람을 로봇 코스튬에 넣지 않아도), 좋은 비디오가 제품화된 솔루션을 제공하는 것은 아닙니다. 워이어드가 보스턴 다이내믹스 아틀라스가 파쿠르 루틴을 수행하는 것을 보도한 후, 그들만이 더 심층적으로 파헤쳐보고 시연이 실제로 작동하는 건 20번 중 1번 정도 된다는 사실을 알아냈습니다. 애자일리티는 최근에 땅에서 일어설 수 있는 능력을 시연했는데, 인상적이고 중요하지만 결국 기본적인 문제를 다시 한 번 확인합니다... 그들의 로봇들은 넘어질 수 있는 존재입니다.\n\n인간형 로봇에는 세 가지 특별한 문제가 있습니다. 첫 번째는 계속 발전하는 인공지능으로 극복될 것이라고 믿습니다. 두 번째는 충분한 투자자 자금으로 극복될 수도 있습니다. 세 번째는 아킬레스 건입니다.\n\n- 아직 인공지능이 부족합니다. 강력한 균형 잡는 시스템에 필요한 일반화된 제어기가 부족합니다.\n- 인공지능이 부족한 상태에서의 하드웨어 투자는 나쁜 투자입니다. 인간형 로봇을 제품 수준으로 개발하는 데 필요한 자금은 10억 달러 이상 들어갈 가능성이 매우 높습니다.\n- 생물모방은 적절한 방법이 아닙니다. 인간형 로봇은 대부분의 제조 작업에 대한 올바른 설계 솔루션이 아닙니다.\n\n첫 번째 문제에 대해 이야기해보죠. 로봇을 위한 견고한 컨트롤러들은 어렵습니다. 전년에 스탠포드에서 후원하는 제어 과학 워크숍에 참석했습니다. 이 행사에서 스티븐 보이드 박사가 인상적인 발표를 했습니다. 보이드 박사에 대한 예의를 기리며, 그의 의도와는 다를 수 있지만 제가 받아들인 내용을 간략하게 정리해보겠습니다. 전반적으로 이 발표는 강화 학습을 비롯한 다양한 제어 기법들을 비교하고, 이들이 볼록 최적화 문제로 축소될 수 있음을 명확하게 해주어 문제 공간을 크게 단순화할 수 있다는 것을 명확히 보여주었습니다. 하지만 그는 흥미로운 발언을 했습니다 (직역하면 이런 느낌이었는데, 제가 제대로 받아 썼길 바라며), \"그래서 차원을 6 이하로 줄이면 이 문제들은 고전적으로 해결 가능해집니다.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그거 대단한 포인트였죠. 정확히 우리가 로보틱스에서 하는 일이에요. 우리는 문제의 차원을 6개의 제어 구동 이하로 줄이고, 몇 가지 수학, 볼록 최적화, 강화학습 또는 유사한 기술을 결합하여 컨트롤러를 유도해요. 쿼드콥터 드론은 4개의 구동을 갖고 일반적으로 IMU(IMU)를 갖추고 있어요. 자동차는 스로틀, 브레이크, 스티어링을 갖고 있어요. 비행기는 일반적으로 에일러론, 루더, 엘리베이터, 스로틀을 갖고 있어요. Agility가 아름답게 해낸 것은 보행의 물리학을 간단하게 만들어서 컨트롤러를 스프링-매스 시스템으로 모델링할 수 있다는 것이에요. Boston Dynamics가 인상적으로 해낸 것은 한 제어 규칙에서 다른 것으로 매끄럽게 전환이 가능하다는 것이에요. 그러나 각 컨트롤러는 단순화돼 있어요. 생산에 성공한 핸드 컨트롤러들은 아이겐핸드 또는 저차원 제어 공간으로 차원을 축소했어요.\n\n우리가 제어 공간을 단순화하고 차원을 줄인다 해도, 여전히 하나의 제어 문제를 해결하고 있어요. 우리에게는 ChatGPT와 같은 기초 모델이 모든 문을 여는 능력을 갖춘 모델이 없어요. 다양한 종류의 문 손잡이를 열기 위한 동작을 유도하기 위해 RL 또는 기타 기술이 필요해요. TRI는 최근에 Diffusion Policy 작업으로 학습 작업을 보다 신속하게, 데이터 효율적인 방식으로 보여준 인상적인 성과를 거뒀지만, 그들 또한 문제를 하나씩 해결하고 있어요, 비록 더 빠르게 하고 있지만요.\n\n처음에 말한대로, 머신러닝/인공지능의 발전이 이 문제를 해결할 것이라고 생각해요. 우리는 결국 더 견고한 로보틱스 컨트롤러를 얻게 될 거예요. 그러나 개방적인 세계의 복잡성으로 인해, 이 문제가 자율 주행 자동차를 개발하는 것만큼 힘들거나 심지어 더 어렵다는 합리적인 주장도 있어요. 예를 들어, 자율 주행 자동차는 수동적으로 안정해요. 액체든 고체든 상관없이 운반해도 괜찮아요. 그러나 인간형 로봇이 볼링공이 담긴 상자를 운반한다면, 제어 문제는 아주 어려워져요. 우리는 몸을 균형 있게 해주는 많은 다양한 근육, 특히 머리를 섬세하게 조정하여 중심질량이 발밑에 머무를 수 있도록 하는 목 근육을 통해 몸을 안정화시켜요. 그것은 정말 어려운 일이에요! 그리고 보세요, 아직도 자율 주행 자동차를 위한 견고한 AI에 대한 타임테이블을 정할 수 없어요.\n\n여기서 두 번째 문제로 이어져요. 하드웨어는 비싸요. 그리고 복잡한 하드웨어는 정말 비싸요. 개방적이고 해결되지 않은 AI 문제와 복잡한 하드웨어 공학 비용을 결합하면 자금 요건이 끝없이 늘어날 수밖에 없어요. 인간형 로봇에 일하려면 균형 문제를 해결하지 않고는 할 수 없는 것처럼요. 어쩌면 일부 인간형 로봇은 바퀴베이스를 사용하고 있지만, 그들은 본질적으로 안정적이지 않아요... 중심 질량이 여전히 너무 높아 안전하지 않아요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n벤처 생태계에 충분한 자금이 있을까요? 아마도 있을 것입니다. 소프트뱅크는 가장 큰 자금력을 가지고 있으며 로봇에 많은 투자를 했습니다. 구글도 마찬가지입니다. 그러나 지금까지 그 투자로 얻은 수익은 다소 실망스럽습니다.\n\n그러나 가장 큰 문제는 대부분의 작업에 대한 올바른 해결책이 아니라는 것입니다. 모든 작업에 대한 해결책은 아니지만, 디즈니의 동작하는 인형 배우들이 더욱 세련되고 인상적일 것이라고 생각합니다. 도쿄에는 동작하는 공룡이 체크인을 도와주는 호텔이 있습니다. 동작하는 인형 인간은 공룡보단 조금 더 친근할지도 모릅니다. 그러나 우리 주변의 실제 작업을 수행할 때, 생물모방은 해답이 아닙니다.\n\n교통을 한 예로 살펴봅시다. 거의 5,000년 동안 말 소매치기는 화물과 사람을 이동시키는 데 최신 기술이었습니다. 로마인들은 공급품 이동을 용이하게 하기 위해 25만 마일 이상의 도로를 건설했습니다. 자동차가 등장하자, 그들은 기존의 인프라와 함께 작동해야 했습니다. 그러나 헨리 포드는 유명한 말을 했습니다. \"만약 사람들이 원했다면 더 빠른 말을 원했을 것이다.\" 더 빠른 기계적 말은 교통을 위한 올바른 해답이 아니었습니다. 바퀴가 올바른 해답이었습니다.\n\n바퀴는 물류, 제조업, 병원, 공항, 경기장, 인도, 오피스 건물 및 거의 모든 상업 환경에서 올바른 답입니다. 또한, 안정성을 갖추어야 합니다. 땅에 적어도 3개의 접촉 점, 가능하면 4개의 접촉 점을 가져야 합니다. 로봇 앞에 동력을 보내거나 오버헤드하는 대신 안정 쏘포무렁에 적재물을 두는 것이 더 나은 방법입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 아마존의 접근 방식이 옳았다고 믿습니다... 오늘날 로봇이 무엇을 할 수 있는지 이해하고 현재 최첨단 기술을 대규모로 신속하게 배치한 후 기계 학습과 AI의 발전을 활용하여 운영을 더 개선하고 효율화하는 것이죠. 이것이 로봇을 세상으로 가져오는 실용적인 방법입니다. 75만 대 이상의 로봇이 배치된 아마존은 규모에 있어서 세계에서 가장 성공적인 기업입니다. 그 팀이 하는 일에 대해 자랑스럽고 영감을 얻고, 그 가속화를 이끈 나의 시간을 기억하며 뒤돌아보고 있습니다.\n\nCobot에서는 아마존 충족 센터나 분류 센터 또는 항공 허브 네 벽 바깥으로 이득을 가져다주는 협력 로봇을 개발하고 있습니다. 열린 문제를 해결하려는 것이 아니라 바퀴 사용, 지면의 네 점을 접촉점으로 사용, 웨이브로드를 안정 위치에 가져다 놓기 등 실용적인 일을 하고 있는데, 이것을 신뢰할 수 있고 협력적이며 인간이 설계한 공간에서 작동할 수 있는 방식으로 하고 있습니다. 1월에 첫 현장 배치에 나갈 예정이며 실제로는 인간 크기의 적재물, 즉 상자, 토트 및 카트의 이동 문제를 해결하게 될 것입니다.\n\n그러니까, 우리는 인간형 로봇을 제작하는 것이 아닙니다. 미래에 세상을 개선하기 위해 로봇에 대한 가능성이 발전하고 있다고 내심 낙관적이지만, 인간형은 주요 형태 요소가 될 것이라고 믿지 않습니다. 우리의 형태 요소는 준비가 되지 않았지만, 그 날을 기대하고 있습니다.\n\n브래드 포터는 캘리포니아 주 산타클라라에 본사를 둔 로보틱스 기업 Sequoia, Khosla 및 Mayo Clinic 후원 Collab...","ogImage":{"url":"/assets/img/2024-06-19-TheProblemsWithHumanoidRobots_0.png"},"coverImage":"/assets/img/2024-06-19-TheProblemsWithHumanoidRobots_0.png","tag":["Tech"],"readingTime":5},{"title":"사이보그의 각성","description":"","date":"2024-06-19 06:35","slug":"2024-06-19-TheCyborgsAwakening","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-TheCyborgsAwakening_0.png\" /\u003e\n\n3567년, 대전투장이 '붕괴'라는 재앙에 의해 파괴된 400년 후, 과학자 Marcus Callahan 박사는 넥서스 시티의 거대한 창고를 뒤지면서 새로운 기계 몸체에 연결된 두퇴한 여성을 발견했습니다. Marcus는 그녀의 두뇌에 연결한 뒤 그녀를 자신의 늦은 딸을 기리는 이름인 \"Luna\"라고 명명했습니다. Luna는 자신의 과거에 대한 기억이 없이 깨어나며 곧 Kai라는 부유한 도시 '엘리시움'으로 이사 가는 것을 꿈꾸는 젊은 남성과 친구가 됩니다. 또한 Marcus의 이혼한 전 부인인 Dr. Serena와도 친구가 됩니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-TheCyborgsAwakening_1.png\" /\u003e\n\n그 후 Kai는 Luna를 로봇 글라디에이터가 하는 축구와 유사한 Hyperball이라는 게임에 소개합니다. Kai는 시민들의 주체인 Hyperball 토너먼트의 소유자 인 Victor를 위해 로봇 부품을 훔치며 성당의 '사실상' 지도자인 시타델의 결사 단체에 참여합니다. Marcus를 따라 가면 로봇 시리얼 킬러들의 단체인 Malakai가 주도하는 갱단에 습격당합니다. Marcus는 다쳐 Luna는 본능적으로 \"판저-쿤스트\"(a lost martial art)를 사용하여 싸웁니다. 그녀는 두 대의 안드로이드를 죽이고 Malakai를 상처입힙니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![The Cyborgs Awakening 2](/assets/img/2024-06-19-TheCyborgsAwakening_2.png)\n\n마커스는 자신이 요새의 수호자이자 시타델 내에서 일하는 현상금 사냥꾼임을 밝힌다. 말라카이, 빅터를 위해 일하는 세레나 박사. 루나는 전투를 통해 과거를 재발견할 것이라 믿지만, 마커스는 그를 실망시키며 수호자의 역할을 떠맡게 된다. 루나는 도시 외부의 난파선에서 고급 로봇을 발견한다. 그 몸이 베르서커에 속한 라이벌 소행력인 토르의 무적의 적대적 힘이었음을 깨닫고, 루나가 그곳에 남겨둘 것을 거부하는 마커스로 인해 실망한 루나는 자신을 감시자로 등록한다.\n\n![The Cyborgs Awakening 3](/assets/img/2024-06-19-TheCyborgsAwakening_3.png)\n\n넥서스 선술집에서 그와 카이는 말라카이를 물리치기 위해 다른 수호자를 찾지 못한다. 폭도 경비병인 제인은 루나가 싸욯 때 그를 때려서 화를 내고, 그로 인해 마커스가 개입할 때까지 싸움이 벌어진다. 갑자기 개숙한 말라카이가 나타나 루나를 도전하며, 루나가 엘리시움의 기술 지도자 오라이언에 의해 파괴하기 위해 보냈다는 사실을 밝힌다. 루나의 전투 능력에도 불구하고, 말라카이에 의해 그녀의 몸이 파괴되었지만, 마커스, 카이, 그리고 헌터-키퍼 마스터 맥켄지가 도착해 말라다이를 밀어냈다. 마커스는 루나를 베르서커로 만들고 사과한다. 루나와의 관계로 카이는 범죄 경력을 떠나기로 결정한다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![The Cyborg's Awakening](/assets/img/2024-06-19-TheCyborgsAwakening_4.png)\n\n파트너 타로와 마주치지만, 제인이 나타나 타로를 죽이고 카이를 다른 로봇을 죽인 것으로 비난합니다. 카이는 가까스로 도망쳐 루나에게 도움을 요청합니다. 제인처럼 게임을 멈추고 결심한 그는 카이를 위해 목숨을 건졌습니다. 드. 세레나는 빅터를 위해 일하게 바뀌었으나 카이의 머리를 루나의 생명 지원 시스템에 연결하여 카이를 구할 것을 제안합니다. 이 시도를 알아차린 제인이 루나를 멈추려 할 때 다마스쿠스가 바가지를 잡아 제인을 멍하게 만듭니다. 카이가 엘리시움에 도달하기 위해 빅터의 도움을 받는 것은 거짓말이었습니다; 추방된 엘리시움 시민으로, 마커스는 허브 시티 시민들이 슈퍼 볼 챔피언 없이 엘리시움에 들어갈 수 없다고 믿었습니다.\n\n![The Cyborg's Awakening](/assets/img/2024-06-19-TheCyborgsAwakening_5.png)\n\n루나는 성을 습격하여 세레나의 시체를 소장한 빅터와 마주칩니다. 빅터는 말라카이를 소환하나 루나의 새로운 나노 기술 몸이 그를 쉽게 물리칠 수 있게 합니다. 그는 오라이언에게 빅터를 통해 소통하도록 강제합니다. 오라이언이 친구를 해치겠다고 협박하자, 루나가 빅터를 찔러 죽입니다. 루나가 그를 붙잡고 다시 돌아오라고 말하지만, 마침내 동의하지만, 오라이언이 빠진 찔린 보호링이 그의 몸을 뚫고 그를 관통하여 관통 시키고 그를 튜브 밖으로 밀어내버립니다. 루나가 그를 잡으려 하지만 끌어낼 수 없습니다. 죽기 전에, 카이가 자신을 구해준 루나에게 감사합니다. 대중들의 환호 속에서, 그는 복수를 맹세하고 위에서 오라이언을 웃음 지으며 엘리시움을 향해 플라즈마로 가득찬 검을 향합니다.","ogImage":{"url":"/assets/img/2024-06-19-TheCyborgsAwakening_0.png"},"coverImage":"/assets/img/2024-06-19-TheCyborgsAwakening_0.png","tag":["Tech"],"readingTime":3},{"title":"강화 학습 소개","description":"","date":"2024-06-19 06:29","slug":"2024-06-19-AnIntroductiontoReinforcementLearning","content":"\n\n## 강화 학습의 기초에 대한 심층 탐구, 모델 기반 및 모델 없는 방법 포함\n\n![Image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png)\n\n## 강화 학습이란?\n\n공학 지능의 한 경로는 생물학적 생물체를 모방하는 것에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n생물학적 생명체들은 환경으로부터 정보를 전도하고, 이를 처리(인지과학이 연구하는 바)하며, 생존에 유리한 행동을 출력합니다. 이러한 행동들은 가장 기본적인 수준에서 음식 수확, 번식, 위험 회피와 관련됩니다. 또한, 이는 놀이, 창의성, 문제 해결, 설계 및 공학, 사교, 로맨스, 지성 생활과 같은 다양한 인간 활동도 포함합니다.\n\n그렇다면, 위의 모든 것을 수행할 수 있는 시스템을 어떻게 설계할까요?\n\n만약 우리가 간단한 생물체를 어떤 환경의 함수로 모델링한다면, 우리는 에이전트, 환경의 모델, 그 에이전트를 현재 상태에서 원하는 상태로 이동시키는 함수가 필요할 것입니다.\n\n심리학에서, 두 가지 주요 학파인 행동주의와 인지과학은 인간 행동을 설명하기 위해 양립하고 있습니다. 행동주의자는 학습 메커니즘의 함수로써 행동을 이해하며, 학습은 행동적 출력에 귀속될 수 있다고 합니다. 반면에, 인지과학은 환경과의 상호작용을 정보 처리 접근법을 통해 모델링합니다. 이 접근법에서, 에이전트는 외부 자극을 처음에는 감각을 통해 내부 표현으로 변환하고, 그 후 사고와 추론 능력에 이르기까지 변형 및 통합 과정을 거쳐 행동적 출력을 반환합니다. 전자 접근법에서는, 학습은 주로 환경적 조건부로서 이해됩니다. 반면에 후자에서는, 정신적 표현이 행동을 예측하는 데 필수적이라고 여겨집니다. 강화 학습은 주로 행동주의 접근법에서 영향을 받아, 환경적 보상이 에이전트의 탐색 공간 내에서 진화를 결정한다고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n작업자 조건화, 1950년대-60년대에 유행했던 행동주의 심리학 학파로서, 학습을 보상과 처벌이라는 환경 메커니즘의 결과물로 정의했습니다. 조작적 조건화의 전제 조건으로는 에드워드 손다이크가 제안한 효과의 법칙이 포함되어 있습니다. 이 법칙은 만족스러운 효과를 일으키는 행동은 재발을 더 많이 유발하며, 불만족스러운 효과를 일으키는 행동은 재발을 덜 유발한다는 것을 제안합니다. B.F. 스키너는 효과를 강화와 처벌의 용어로 운용했습니다. 강화는 행동의 재발 발생 가능성을 증가시키며, 이것은 접근 또는 억제 요인의 제거를 말할 수 있습니다. 접근은 긍정적 강화, 회피의 역전인 부정적 강화로 표현됩니다. 긍정적 강화의 예로는 스포츠에서 뛰어나고 자주 이기는 것이 포함됩니다. 부정적 강화의 예는 억제적 자극을 제거하는 것인데, 이를 예로 들 수 있는 것은 경기 도중 당신을 조롱하는 학교 폭력가입니다. 작업자 조건화는 가장 큰 보상을 받는 행동을 반복할 가능성이 높다고 예측합니다. 반면 처벌은 행동 효과를 제어하기 위해 부정적 결과를 추가하거나 행동과 관련된 보상을 제거함으로써 구성됩니다. 파울링으로 게임에서 퇴장당했을 때의 경우는 긍정적 처벌을 보여줍니다. 성적이 좋지 않고 게임에서 패배한 경우는 부정적 처벌을 나타내며, 이는 미래에 더 이상 게임을 하지 않을 수 있습니다.\n\n인간 사회의 삶의 게임은 행동을 구성하는 보조적 강화나 사회적으로 구성된 보상과 처벌로 가득 차 있습니다. 이는 돈, 학점, 대학 입학 기준, 게임에서 이기고 지는 규칙과 같이 사회적 보상과 처벌을 포함합니다. 이러한 것들은 음식, 번식, 사회적 승인과 같은 생물학적 요구에 더 가까운 자연적 강화요인을 보완합니다.\n\n기억은 이전 경험을 유지할 수 있도록 하는 학습에서 중요한 역할을 합니다. 증거에 따르면 기억은 경험의 콘텐츠보다는 보상과 처벌을 부호화합니다. 실험 대상은 보상을 받는 경험을 기억할 가능성이 더 높아지며, 따라서 이를 반복하는 경향이 있습니다. 부정적인 경험은 불리하게 기억될 가능성이 더 높아지며, 이를 피하려고 합니다. 기억 메커니즘은 복잡하고 다양하며, 실험 대상들이 기억을 회상함으로써 기억을 다시 구성함에 있어서 적극적인 역할을 하는 것으로 나타납니다. 이 사실은 행동주의에 대한 예측을 어렵게 만들며, 단독으로 조건화 원리에 근거한 예측을 하기 어렵게 만든다. 게다가 보상과 처벌은 긍정적과 부정적 영향의 풍경을 단순화하며, 이것은 복합한 골짜기와 웅덩이들, 중첩된 의존성으로 이루어진 복잡한 지형이며, 이는 이진 공간보다는 연속적 스펙트럼으로 더 잘 모델링됩니다.\n\n불구하고, 강화 학습은 인공지능을 모델링하기 위해 에이전트, 환경 및 보상의 행동 온톨로지를 적응하는 다양한 수학적 기법으로 이루어져 있습니다. 아래에서 보게 되겠지만, 강화 학습의 측면 중 일부는 통제 이론에서 비롯되어 물리학과 공학으로 확장되는 전제 조건으로부터 나오며, 다른 측면은 심리학과 생물학으로부터 직접적으로 나오는 것입니다. 통제 이론의 대상과 생명체는 열역학적 균형으로부터 멀리 떨어진 최적 범위 내에 남아야 하는 동력학 시스템으로 구성되기 때문에, 기본 원리는 강화 학습과 인공지능의 목표에 부합됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다이내믹 프로그래밍은 주로 제어 이론에서 시작되어, 더 큰 문제를 하위 문제로 재귀적으로 분해하여 해결하는 수학적 최적화 방법으로 발전했습니다. 보통 재귀는 함수가 직접 또는 간접적으로 자기 자신을 파라미터로 전달하는 것을 말합니다.\n\n본 글에서는 주로 동적 프로그래밍의 요소에 초점을 맞추며, 이를 이산적이고 유한한 게임에 초점을 맞출 것입니다. 그러나 동적 프로그래밍은 모델 없이 강화학습 접근 방식과 결합하여 해결되는 한계를 가지고 있으며, 이를 보완하기 위해 동적 프로그래밍과 인공 신경망을 결합한 방법이 있습니다. 이는 한 때 신경동적 프로그래밍이라 불렸습니다. 보다 넓게는 강화학습과 인공 신경망의 결합을 딥 강화학습이라고 합니다. 이러한 모델은 강화학습 기법 내에서 딥 러닝의 강점을 통합하고 있습니다. 이러한 알고리즘 가운데 가장 인기 있는 것은 2013년 DeepMind에 의해 소개된 딥 Q-네트워크(DQN)입니다. 이 알고리즘 계열은 Q 함수를 근사화하기 위해 딥 러닝을 활용합니다. Q 함수의 근사화가 강화학습의 한 약점 중 하나이므로, 이러한 알고리즘들은 강화학습 패러다임의 주요 개선점을 제공합니다.\n\nDQN이 해결한 다른 약점에는 비선형 동역학을 캡처하는 유연성 부여, 차원의 저주로 인해 계산적으로 처리하기 어려워지는 일반적 범위의 차원을 수용하는 능력, 그리고 환경에 대한 보다 큰 일반화 능력이 포함됩니다.\n\n신경동적 프로그래밍은 순수히 행동주의 접근 방식의 약점을 해결하기 위해 심리학의 인지 패러다임을 활용하는 방향으로 발전하고 있습니다. 그러나 하위 수준 지각 정보의 계층적 구조와 처리에 대한 과학적 진전이 이루어지는 반면, 그 정보를 생각과 의식에 연결시키는 데는 더 많은 노력이 필요하며, 이는 과학적으로 약간 불가능한 것으로 남아 있습니다. 이러한 이유로 인공 신경망(ANNs)은 아직까지 사람의 지능의 복잡한 일반화 능력을 갖추고 있지 않습니다. 이는 ANNs보다 지수적으로 적은 샘플로 학습하는 인간의 지능과 대조적입니다. 본 글의 마지막 섹션에서 강화학습의 원칙을 인공 일반 지능(AGI) 쪽으로 채택하는 시사점을 논의하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 의사 결정 이론 및 제어 이론\n\n동적 프로그래밍과 강화 학습의 수학적 요소에 깊이 파고들기 전에, 철학적이고 수학적인 의사 결정 이론과 강화 학습 간의 관계를 명확히해야 합니다. 의사 결정 이론은 주로 합리적 선택 이론의 수학적 형식화로 구성되어 있지만, 강화 학습의 목표와 겹치는 부분이 있습니다. 강화 학습은 복잡한 환경과 정보 환경과 상호작용할 수 있는 성공적인 인공 에이전트로의 모델을 구축하려고 합니다.\n\n의사 결정 이론, 또는 선택 이론으로도 알려진 이론은 20세기에 이상적인 이유의 형식화가 짙어진 가운데 발전하였습니다. 구체적으로, 에이전트의 행위 확률을 그들의 선호도를 고려하여 양적화하기 위해 확률 이론을 사용합니다. 이 형식화 노력의 꼭대기는 폰 노이만-모건슈턴 유틸리티 절차였습니다. 요약하자면, 이 절차는 에이전트가 유틸리티 기대치에 따라 최대 이익을 가져다주는 행동을 선택하는 경향이 있음을 설명합니다.\n\n제어 이론은 기계 및 전기 공학 분야에서 나타나며, 동적 시스템의 상태 및 성능을 원하는 매개변수에 대해 최적화하는 데 관심이 있습니다. 중요한 메커니즘은 희망 변수를 측정하고 설정점과 비교한 후 그 차이를 수정을 위한 피드백으로 전달하는 컨트롤러로 이루어져 있습니다. 제어 이론의 큰 그림은 생명체의 대사 과정과 유사하며, 외부 변수 조건에 대비해 내부 온도의 설정 점을 유지하는 생물들의 과정을 반영합니다. 제어 이론과 의사 결정 이론의 연결은 명백합니다: 둘 다 시스템의 상태를 최적화하거나 발전시키기 위해 환경으로부터의 피드백에 의존합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수학적으로, 제어 및 의사 결정 문제의 부분 집합은 모두 동적 프로그래밍을 통해 해결할 수 있는 최적화 문제로 축소될 수 있습니다. 동적 프로그래밍은 상태 변수의 수가 지수적으로 증가함에 따라 계산 요구 사항이 지수적으로 증가하는 차원의 저주에 시달린 일반적인 확률적 최적 제어 문제를 해결하기 위해 그것을 더 작은 하위 문제로 분해하고 가치 함수를 계산함으로써 해결합니다. 저희는 강화 학습의 기본 원칙을 시연하면서, 동적 프로그래밍의 핵심인 에이전트의 상태 및 가치 함수 사이의 재귀적 관계에 대해 깊이 파헤쳐볼 것입니다.\n\n강화 학습과 의사 결정 이론은 보상 또는 유틸리티를 극대화하기 위한 절차를 정의하는 부분에서 겹칩니다. 그러나 의사 결정 이론에서는 유틸리티가 명시적으로 정의되지만, 경제 행동을 모델링하려는 것인 반면, 강화 학습에서는 유틸리티가 누적 보상으로 대체됩니다. 서로 다른 작업 목표에 대한 다른 정책을 적용하여 누적 보상을 극대화할 수 있으며, 탐구와 개발의 극성 방향 간의 상호 관계에 따라 달라집니다. 우리가 볼 것처럼, 탐색과 개발의 상호 관계를 탐색하는 것으로 표현되는 탐사-개발 딜레마에 따라 누적 보상을 극대화하는 것이 달라집니다.\n\n강화 모델의 기반이 되는 온톨로지를 개요화하는 것으로 시작해 봅시다.\n\n## 상태, 동작 및 보상\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n강화 학습은 의사 결정 이론의 이론적 장치를 활용하여 에이전트, 환경 및 동적 진화 규칙을 포함하는 모델을 구성합니다. 진화 규칙은 에이전트가 환경 내에서 보상을 추구할 수 있게 허용하며, 이를 관찰이라고도 합니다.\n\n에이전트는 환경으로부터 결정까지의 출력으로 정의됩니다. 특정 결정을 행동이라고 합니다. 네트워크의 현재 상태에서 행동으로의 매핑을 정책이라고 합니다. 정책은 상태에서 결과로의 매핑으로서 행동을 안내합니다.\n\n따라서 형식적으로 정책은 상태를 행동으로 매핑하는 함수입니다. 현재 상태가 주어졌을 때 행동의 조건부 확률로 나타낼 수 있으며, 여기서 그리스 문자 𝛑은 정책을 나타냅니다:\n\n![정책](https://yourwebsite.com/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n전이 역학은 모든 가능한 상태 및 보상 값에 대한 확률 분포로 주어진 입력 보상에 따라 다음 상태를 정의합니다:\n\n![이미지](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_2.png)\n\n위의 공식은 다음 상태와 보상 쌍의 확률을 현재 상태 s와 행동 a가 주어졌을 때 다음 상태 s'와 보상 r의 조건부 확률과 같다고 정의합니다.\n\n행동은 보상을 누적하여 환경을 변경합니다. 그 결과로 보상은 에이전트 상태나 관측을 변경합니다. 보상 입력은 정책에 기반하여 미래의 행동 출력을 결정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적으로, 정책에는 두 가지 유형이 있습니다:\n\n보상은 일반적으로 스칼라 값 x로 형식화됩니다.\n\n특정 보상이 주어지면, 에이전트는 최적화 딜레마에 직면합니다: 에이전트는 단기 보상을 극대화해야 하는지, 아니면 완전한 생생력 기록을 통해 누적 보상을 극대화해야 하는지를 결정해야 합니다.\n\n이것은 탐색-이용 딜레마로 알려져 있습니다. 다시 말해, 전이 함수는 환경을 탐색하고 누적한 지식을 활용하여 최대 보상을 얻으며, 그 둘 사이의 균형을 최적화해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n탐색-활용 딜레마에 대한 최적의 해결책은 모델이 학습해야 하는 작업의 유형에 따라 달라집니다. 이 작업은 유한에서 무한(연속적 또는 이산적으로)로 범위가 있을 수 있습니다. 예를 들어 체스 게임은 에피소드 작업으로 형식화될 수 있습니다. 왜냐하면 유한한 구성 공간과 승, 패, 무승부 세 가지 가능한 결과를 가진 미리 정의된 종료 상태가 있기 때문입니다. 이는 현재 상태를 기준으로 최적의 후속 상태를 결정할 수 있는 것을 의미하며, 결정론적 전이 동역학을 통해 계산됩니다. 따라서 각 상태에 대해 단일 최적의 행동이 존재합니다.\n\n그러나 대부분의 작업은 유한한 구성 공간이나 미리 정의된 종료 상태를 가지고 있지 않습니다. 우리는 이러한 것들을 연속적인 작업으로 분류하고, 모델이 없는 방법을 통해 최적화합니다. 모델 없는 방법론에서는 전환 동역학을 계산하는 대신 모델이 환경에서 샘플링하여 최적의 후속 상태를 계산합니다. 다르게 말하면, 선견지명을 통해 행동을 계획하는 대신 환경에 대해 배우기 위해 시행착오를 사용합니다.\n\n모델 없이 강화 학습하는 두 가지 접근법이 일반적으로 있습니다: 몬테 카를로 접근법과 시간차 학습. 충분한 샘플의 평균이 기대값으로 수렴하기 때문에, 모델 없는 방법은 샘플 평균을 통해 예상값을 추정합니다. 몬테 카를로 방법은 충분히 큰 상태-행동 쌍의 샘플로 예상 누적 반환을 추정하여 가치 함수를 계산합니다. 일부 몬테 카를로 방법은 에피소드 작업의 끝에서만 가치 함수를 평가합니다. 연속적인 작업에서는 에피소드의 정의가 다양하게 변할 수 있고, 디자이너에 따라 시간 간격에 따라 설정할 수 있습니다.\n\n몬테 카를로 탐색과 반대로 시간차 학습은 시간 단계 간의 차이를 활용하여 가치 함수를 증분적으로 추정합니다. 시간차 방법의 접근 방식을 두면 몬테 카를로 방법에 비해 실제 예상 값과의 분산을 낮추는 특성이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요약하자면: 에이전트는 현재 상태와 액션 공간 쌍에서 상태 공간으로의 매핑을 통해 환경을 탐색합니다. 전이 동적은 미리 정의된 종단 상태를 가진 유한한 구성 공간에 대한 모든 가능한 매핑을 계산합니다. 미리 정의된 종단 상태와 유한한 상태 공간 대신에, 모델 무작위 접근법은 최상의 정책을 찾기 위해 환경에서 계속 샘플링합니다.\n\n동적 프로그래밍은 모든 상태-액션 쌍에서 상태 전이 확률과 예상 보상을 계산합니다. 이 프로세스가 어떻게 작동하는지 이해하기 위해서는, 마르코프 프로세스를 이해해야 합니다.\n\n다음에는 에이전트가 최적 후속 상태를 계산할 수 있도록 하는 수학적 모델을 배우게 됩니다. 앞서 논의한 대로, 최적성은 탐사-이용 딜레마로 이어지며, 이는 모델링하려는 작업 유형에 따라 다릅니다. 보상 구조를 자세히 살펴봄으로써 이를 더 잘 이해할 수 있을 겁니다.\n\n## 보상 평가\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n강화 학습에서 보상을 측정하는 방법은 에이전트가 행동을 취함으로써 환경으로부터 얻는 스칼라 값으로 계량화됩니다. 이 보상의 가치는 행동의 즉각적인 선호도를 나타냅니다.\n\n반면에 누적 보상 또는 반환은 해당 시점까지 환경으로부터 누적된 모든 보상의 합을 나타냅니다. 에이전트의 목표는 단순히 즉각적 보상을 최적화하는 것이 아니라 누적 보상을 최적화하는 것입니다. 전자는 근시적 에이전트를 나타내며, 후자는 장기간 수익을 극대화하려는 장기 노력 에이전트를 나타냅니다.\n\n대부분의 경우 에이전트가 가장 높은 보상을 최대한 빨리 극대화하길 원하기 때문에 할인은 현재 최대 보상을 나중에 최대 보상보다 우선시하는 방식으로 도입됩니다.\n\n할인을 적용한 누적 보상 G는 아래 식으로 표현됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Reinforcement Learning Introduction](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_3.png)\n\n여기서 누적 보상 G는 보상과 해당 할인 계수 감마 𝜸의 곱의 합과 같습니다. 감마는 항상 0과 1 사이의 값인 '0,1'입니다. 감마는 각 시간 단계마다 지수적으로 증가되므로 무한한 시간 단계를 통해 감마가 0에 접근합니다.\n\n감마가 0에 접근할수록 단기 이익을 장려하고, 감마가 1에 가까워지면 무한한 반복을 통해 보상 합이 자체적으로 무한에 접근하므로 장기 이익을 장려합니다.\n\n대부분의 작업은 시간에 제한이 있기 때문에, 감마 할인은 값이 1 미만일 때 보상에 상한선을 부과합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n할인을 고려한 누적 보상의 압축된 방정식은 아래와 같습니다. 여기서 G는 보상 R의 예상 합을 나타내며, 이는 할인 요소 감마로 곱해집니다. 따라서 누적 보상은 보상과 할인 요소의 합으로 계산됩니다:\n\n\n![cumulative reward equation](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_4.png)\n\n\n## 마르코프 의사결정 과정 (MDP)\n\n지금까지 정책을 상태에서 행동으로 매핑하는 확률적 정의, 보상이 주어졌을 때 한 상태에서 다른 상태로 움직일 확률인 전이 역학, 그리고 보상이 어떻게 계산되는지에 대한 공식에 대해 논의해 왔습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자, 이제 조금 물러나서 확률적 전이 체인을 정의하는 보충 이론을 제공하겠습니다. 먼저 마르코프 과정이라고 하는 것부터 시작해봅시다. 마르코프 과정은 마르코프 성질을 만족하는 확률 과정입니다. 확률 과정은 무작위로 변하는 과정입니다. 마르코프 성질은 모든 상태에 대해 후속 상태가 현재 상태에만 의존된다는 것을 말합니다.\n\n과거 상태가 미래 상태에 영향을 미치지 않기 때문에 마르코프 성질을 만족하는 과정을 메모리리스라고 합니다. 집을 나가서 일하러 나가 다시 집으로 돌아오는 매일 재발되는 일정된 목적지 집합을 상상해보세요. 즉, 시작과 끝이 있는 순환 과정이 있습니다. 이제 더 나아가서 다음 목적지로 움직일 결정이 현재 목적지에만 의존한다고 상상해보세요. 처음에는 각 연결된 목적지가 동일한 확률 분포를 갖게 될 것입니다. 예를 들어, 집을 나가면 운전하거나 지하철을 탈 수 있는 선택지가 있다면, 두 가능한 미래 상태에 대한 초기 확률을 각각 0.5로 정할 수 있습니다. 모든 가능한 경로의 반복을 통해 이러한 확률은 어떤 경로가 다른 경로보다 선호되는 빈도 분포로 안정화될 수 있습니다. (이 유형의 확률을 경험적 확률이라고 부르며, 가능한 사건에 대한 결과를 한정된 테스트 수에 대해 평균화합니다) 그 분포 평형은 마르코프 체인 또는 과정이 될 것입니다.\n\n이제 아마도 생각 중일 것입니다: 어떻게 사건과 상태를 정의하나요? 고정된 가능한 상태와 안정한 확률 분포에 대해 얘기하려면 세상이 너무 복잡하지 않나요?\n\n매우 그렇습니다. 그러나 우리는 환경 속 요소들의 수학적 형식론을 원하기 때문에 모델링하려는 작업 또는 환경 유형을 구별해야 합니다. 이를 위해 시간 단계와 상태 공간의 표현, 즉 모든 가능한 상태의 분포를 명시해야 합니다. 아래의 정사각 행렬은 상태 공간과 시간의 축을 기준으로 마르코프 체인의 정의를 제공합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_5.png)\n\n상태 공간은 셀 수 있는/유한한 상태거나 연속적일 수 있습니다. 유한 상태 공간은 시스템의 모든 가능한 구성을 조합 이론을 통해 설명하고, 연속 상태 공간은 연속 함수를 통해 모든 가능한 구성을 설명합니다.\n\n유한 및 가산 무한 공간은 측정 가능한 공간으로 정수 또는 유리수를 취하며, 연속 공간은 실수를 취합니다.\n\n마찬가지로 시간 축은 이산 또는 연속적으로 정의될 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이산시간 프로세스는 이산적으로 상태 전이를 계산하지만, 가산 또는 비가산 상태 공간에서 모델링할 수 있습니다. 여기서 비가산이라 함은 실수의 무한한 10진 확장을 의미합니다. 실제로 컴퓨터가 시간을 세는 방식도 이와 같습니다. 이를 이산 단계로 처리합니다. 단계 사이의 간격은 아키텍처에 따라 다르지만, 주기는 보통 레지스터 상태를 변경하는 데 필요한 시간 단계의 길이로 측정됩니다.\n\n연속시간 체인은 연속으로 상태 전이를 계산하며, 가산 또는 비가산 상태 공간에 모델링될 수 있습니다.\n\n마르코프 프로세스라는 용어는 일반적으로 연속시간 프로세스에 사용되며, 마르코프 체인이라는 용어는 이 중 일부인 것을 나타냅니다: 이산시간, 확률적 제어 프로세스입니다. 이 기사에서는 이산시간, 유한 상태 공간에 초점을 맞출 것입니다.\n\n지금까지 우리의 마르코프 체인은 상태간 전이를 고정된 확률로 설명하는 매우 단순한 모델입니다. 행동과 보상이라는 모델링에 중요한 두 가지 요소가 빠져 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n보상을 전이 확률로 할당하는 것이 마르코프 보상 과정입니다. 마르코프 보상 과정은 각 전이 상태에 보상을 할당합니다(양수 또는 음수 정수로 정의됨)으로써 시스템을 원하는 상태로 이끕니다. 누적 보상 공식을 상기해 보겠습니다. 기대 보상의 합에 일정한 할인 계수가 곱해진 값입니다. 마르코프 보상 과정을 사용하면 초기 상태 S가 주어졌을 때 상태 v(s)의 값과 누적 보상 G의 확률을 계산할 수 있습니다(여기서 G는 많은 반복 시행에서 평균화된 값입니다):\n\n![이미지](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_6.png)\n\n마르코프 결정 과정으로 나아가기 위해 필요한 마지막 변수는 행동입니다. 에이전트는 가능한 행동 집합에 대해 동등하게 분포된 확률로 시작하고 이후에 전이 함수를 업데이트하여 현재 상태와 행동을 다음 상태와 보상으로 매핑합니다. 이렇게 하면 앞서 설명한 전이 동학에 다시 도달하게 됩니다:\n\n![이미지](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 동적 계획법 및 벨만 최적성\n\n이것은 벨만(1957)에 의해 개발된 동적 프로그래밍의 개념으로 이어집니다.\n\n동적 프로그래밍을 이해하면, 동적 프로그래밍과 같은 완벽한 환경 지식이 필요하지 않는 근사 방법인 몬테카를로 탐색 및 시간차이 메소드도 이해할 수 있습니다. 이러한 모델-프리 방법은 완벽한 정보 대신 동적 프로그래밍의 결정적 정책을 근사화합니다. 따라서, 실제 세계 학습을 근사화하는 강력한 메커니즘이 제공됩니다.\n\n동적 프로그래밍이 최적의 에이전트 상태를 검색하고 찾는 핵심 아이디어는 상태 가치 함수와 행동 가치 함수 사이의 관계에 있습니다. 이들은 재귀적으로 관련되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 아이디어를 관련성 있는 예시로 확장해 봅시다. 예를 들어, 당신이 삶에서 최적 상태가 아니고 이를 바꾸고 싶다고 가정해 봅시다. 미래에 이루고 싶은 구체적인 목표나 위치가 있다고 해 봅시다. 이 큰 목표에 도달하기 위해 (더 좋은 직장을 얻는다던가, 가족을 꾸린다던가 등을 대체할 수 있습니다), 당신은 원하는 결과에 도움이 되는 일련의 작은 단계나 행동을 취해야 할 것입니다. 강화 학습의 언어로 번역하면, 현재 상태에는 특정 가치가 할당될 것입니다. 현재 상태와 가치를 고려하여 당신은 행동을 취할 것입니다. 이러한 행동은 전체 목표와 현재 상태에 따라 평가될 것입니다. 좋은 행동은 나쁜 행동보다 높은 가치를 받을 것입니다. 환경으로부터의 피드백은 행동의 가치를 결정할 것입니다 (이 값들이 어떻게 결정되는지는 작업마다 다릅니다). 상태의 평가는 사용 가능한 행동과 후속 상태의 가치에 영향을 미칠 것입니다. 그리고 행동의 평가는 현재 상태의 가치를 재귀적으로 영향을 줄 것입니다. 다시 말해, 행동과 상태는 재귀적으로 연결되어 있습니다.\n\n이제 현실에서, 당신의 목표와 그 목표에 이르는 행동 단계들은 이산 시간 단계 및 이산 상태 공간을 갖는 결정론적 시스템으로 명시할 수 없습니다 (비록 이 방식으로 근사화할 수도 있습니다). 대신, 동적 프로그래밍은 체스와 같은 게임처럼 정의 가능한 환경을 가정합니다. 여기서 시간 단계와 행동 공간이 이산적이고 유한하게 추상화됩니다. 현실과의 중요한 점은 더 큰 목표가 해당 큰 목표에 유리한 작은 부목표를 최적화함으로써 다가올 것이라는 점입니다.\n\n따라서 동적 프로그래밍은 다음 값들을 가정할 것입니다: (Ω, A, 𝒫), 여기서 Ω는 모든 가능한 상태의 합을 나타냅니다, A는 유한 샘플 공간의 부분집합인 행동 이벤트를 나타내며, P는 일정 정책 함수 𝝅에 의해 각 행동 이벤트에 할당된 확률을 나타냅니다.\n\n이제 우리가 결정론적 전이 역학에 대해 생각해 보면, 상태, 행동 및 보상의 집합이 유한하기 때문에, 특정 상태와 보상 쌍은 일정한 상태 및 행동 쌍이 주어졌을 때 그 값들이 발생할 확률을 갖게 될 것입니다. 이러한 확률은 상태 공간이 이산적이기 때문에 이산 확률 분포로 명시됩니다. 우리는 상태, 행동 및 보상으로 구성된 일련의 순서가 누적 보상을 최대화하려는 마르코프 결정 과정(MDPs)이라고 했습니다. 이때 보상을 스칼라 값으로 표현하며, 시간이 흐름에 따라 예상되는 누적 보상을 최대화하려고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 다루어야 할 질문은 우리가 지정한 가정에 따라 마르코프 의사결정 프로세스가 누적 보상을 최대화하는 방법입니다. 이 답은 벨만 최적 방정식에 의해 제공되며 두 함수인 상태 가치 함수와 행동 가치 함수 사이의 관계를 설명합니다.\n\n## 상태 가치 함수\n\n상태 가치 함수는 에이전트가 정책 𝝅에 따라 취할 수 있는 모든 가능한 조치의 확률의 합으로 정의될 수 있습니다. 각 조치에 대해 가능한 후속 상태의 가중치 값의 합으로 그 가치가 결정됩니다.\n\n보다 간단하게 말하자면, 상태 가치 함수는 특정 상태(s)에서 정책 𝝅을 따라 시작하여 에이전트가 얻을 수 있는 예상 누적 보상을 정의합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![equation_8](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_8.png)\n\n위의 방정식은 두 항으로 구성되어 있습니다: a) 정책 (𝝅)을 따라 상태 (s)에서 에이전트가 취할 수 있는 모든 가능한 조치들의 확률의 합, 그리고 b) 각 가능한 조치마다 가능한 후속 상태의 가중치 값을 계산하는 내부 합계입니다. 대괄호 내의 항은 각 조치의 가능한 상태의 기여도를 즉각적 보상 R(s, a, s’)의 합과 감마 요소 𝛾에 의한 할인된 보상의 합으로 계산합니다.\n\n상태-가치 함수를 표현하는 다른 방법은 다음과 같습니다:\n\n![equation_9](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_9.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 공식은 다음 상태의 가치를 조건부 확률로 계산된 예상 반환 E𝝅로 정의합니다. 시간 t에서 상태 s가 주어졌을 때 시간 t에서 보상 R을 받을 조건부 확률로 계산됩니다. 보상 R은 후속 상태의 예상 반환의 곱의 합과 감마 감쇠를 고려하여 계산됩니다.\n\n더 잘 이해하기 위해 3 x 3 그리드 월드의 에이전트를 상상해보세요. 각 시간 단계마다 상, 하, 오른쪽, 왼쪽 네 가지의 가능한 조치가 사용 가능합니다.\n\n![grid](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_10.png)\n\n우리는 상태 가치를 0으로 초기화하고, 상태 가치 함수에 대한 벨만 방정식을 사용하여 그리드 내의 보상 분포가 주어졌을 때 상태 가치를 최적화합니다. 우리는 (행, 열) 색인을 사용하여 그리드의 각 위치를 식별합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 테이블 태그를 마크다운 형식으로 변경해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Action-Value Function\n\n우리는 행동-가치 함수가 상태-가치 함수의 두 번째 항목으로 내장되어 있다는 것을 보았습니다. 이는 행동-가치 함수가 상태 (s)에서 가능한 모든 행동의 가치를 계산한다는 것을 의미합니다. (s)에서 (s')로의 전이로부터 얻은 즉각적인 보상의 합과 다음 상태 (s')에서의 예상 누적 보상을 고려하여 주어진 작업으로부터 계산됩니다.\n\n다시 말해, 행동 가치 함수는 상태 (s)에서 작업 (a)를 수행하는 것에 대한 누적 보상을 계산합니다. 여기서 기대 수익은 즉각적인 상태 전이 — R(s, a, s')로 표시됨 — 및 다음 상태 s'의 누적 보상의 할인 가치 —𝛾∑𝝅(a'|s')Q(s',a')​​로 표시됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n행동 가치 함수를 정하는 또 다른 방법은 최적 정책 𝝅을 따라 상태와 행동 쌍 (s, a)이 주어졌을 때 기대 반환값 E로 나타내는 것입니다:\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_14.png)\n\n상태 가치 함수와 행동 가치 함수는 상태 가치 함수가 정책과 행동 가치 함수 Q(s, a)로 구할 수 있다는 관점에서 관련이 있습니다.\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_15.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서 행동-가치 함수와 상태-가치 함수는 재귀적으로 관련이 있습니다: 행동-상태 쌍의 가치가 상태의 가치를 결정하며, 상태는 반대로 행동의 가치를 결정합니다.\n\n상태-가치 함수는 상태를 우선으로 하고 기대값 E를 출력합니다. 행동 가치 함수는 상태와 행동 쌍을 우선으로 하여 보상을 계산하고 기대 누적 반환 E를 얻습니다.\n\n따라서 벨만 최적 방정식은 상태-가치와 행동-가치 함수의 재귀적 반복을 나타내며, 최적 값에 수렴할 때까지 반복됩니다. 상태-가치 함수를 위한 벨만 방정식은 아래와 같이 표현됩니다: \n\n![image](https://yourwebsite.com/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_16.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 상태의 값은 가능한 모든 행동의 최대 보상으로 정의되며, 이는 (s) 상태에서 행동 a를 취할 때 얻는 보상과 다음 행동 s'의 값 및 할인 계수 감마의 곱으로 계산됩니다.\n\n벨만 방정식은 현재 상태에서 모든 가능한 행동을 평균화하고 발생 확률에 따라 가중치를 부여합니다.\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_17.png)\n\n## 모델 프리 메소드: 몬테카를로 \u0026 시간차학습\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 예시는 전환 동역학이 알려져 있어 따라서 완벽하게 계산될 수 있는 결정론적 모델을 설명합니다. 이것은 환경에 대한 완전한 지식을 가지고 있기 때문입니다.\n\n그러나 대부분의 작업에서는 환경에 대해 완전한 지식을 갖고 있지 않습니다. 이 정보 대신에 우리는 동적 프로그래밍 방정식을 해결할 수 없기 때문에 정확한 결정론적 전환 동역학으로 진행할 수 없습니다. 이 문제를 극복하기 위해 통계에서 빌려온 기술을 사용하여 환경의 상태를 샘플에서 추론할 수 있습니다.\n\nMonte Carlo 방법론에서는 예상 수익을 샘플 수익의 평균으로 근사화합니다. 샘플이 무한대로 접근함에 따라 평균 수익이 예상 수익의 실제 값으로 수렴합니다. 에이전트가 종료될 때까지 전체 에피소드를 실행한 다음 가치 함수를 계산하는 방식으로 이를 수행합니다. 그런 다음 N개의 에피소드 샘플을 취하여 평균을 사용하여 대상 상태의 예상 가치를 근사화합니다. 지금까지 궁금해 하고 계실 수 있듯이, 에피소드가 어떻게 정의되는지는 작업과 모델의 목적에 따라 달라집니다. 예를 들어, 체스 게임에서는 전체 게임을 실행하거나 임의의 단계 시리즈를 에피소드로 정의할 수 있습니다.\n\nMC 업데이트 규칙을 다음과 같이 작성할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_18.png)\n\nV(s) n+1은 다음 에피소드의 가치를 나타내며, S(s)n은 상태의 누적 가치를 나타내고 G는 보상의 가치를 나타냅니다. 누적 보상 G를 상태 값에 추가하고 에피소드 또는 샘플의 수로 나눕니다.\n\n우리는 MC 업데이트 규칙을 대수적으로 재배치할 수 있습니다:\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_19.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMonte Carlo 방법과는 달리 Temporal Difference (TD)에서는 각 에피소드가 끝난 후가 아니라 각 시간 단계나 증분마다 상태 가치 함수를 평가합니다. 환경에 대한 정보가 없는 초기 상태에서는 상태의 가치 V(s)를 0이나 다른 값으로 초기화해야 하며, 이후 매 시간 단계마다 업데이트됩니다.\n\nTD에서는 상태의 가치를 계산하는 데 두 단계가 필요합니다. 먼저 한 단계의 오차를 계산한 다음 업데이트 규칙을 적용하여 상태의 가치를 변경합니다. 오차는 다음과 같은 차이 공식으로 주어집니다:\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_20.png)\n\n여기서 𝜹t는 오차를 나타내며, R(t+1)은 행동에서 얻는 보상, V(S t+1)은 다음 상태의 추정 가치, V(S)는 현재 상태의 가치를 의미합니다. TD가 다음 상태의 추정 가치를 사용하여 현재 상태를 평가하는 것을 부트스트랩이라고 합니다. 이를 통해 현재 상태의 값에서 행동의 보상과 할인 계수와 다음 상태의 가치의 곱을 더하고 빼는 것으로 상태의 값이 즉시 시간 단계마다 업데이트됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예상 보상과 실제 관측 값 사이의 차이를 𝜹(탐색-백업 간격)을 𝛼(학습률)에 곱해주면 관측과 기대 사이의 차이를 줄일 수 있어요.\n\n![image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_21.png)\n\n𝛼의 역할은 TD 알고리즘의 학습 정도를 결정하는데, 𝛼는 실수형양수값이에요. 일반적으로 𝛼는 0.1, 0.01, 0.001 같은 값으로 설정돼요. 높은 𝛼 값은 업데이트를 더 적극적으로 진행하도록 해주고, 낮은 값은 보수적인 업데이트를 보장해요. 𝛼의 값은 탐험-활용 균형에 영향을 미치는데, 더 높은 𝛼는 탐험을 선호하고, 낮은 𝛼는 활용을 선호하게 됩니다.\n\nMC와 TD 방법은 환경에 대한 사전 지식 없이 진행되지만, Temporal Difference의 장점은 매 시간 단계에서 온라인 업데이트를 계산한다는 것이고, 몬테카를로 방법의 장점은 값 추정을 위해 샘플링에만 의존하여 편향이 없다는 것이에요. TD 방법의 단점은 높은 편향이고, MC 방법의 단점은 중요한 업데이트를 간과하여 높은 분산을 유발한다는 점이에요. 이것은 두 학습 전략 사이의 최적점이 어딘가에서 존재해야 한다는 것을 시사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTD 방법론은 단계 평가 전략을 n단계로 변경함으로써 최적화할 수 있습니다. 이렇게 함으로써, TD와 MC 사이에서 타협할 수 있는 기회를 얻게 됩니다. 우리가 n단계마다 상태 가치를 평가할 때, 우리는 매 단계 후가 아닌 미래 n단계를 추정함으로써 이를 수행합니다.\n\nn단계 TD에 대한 수정된 접근 방식은 TD(𝝀)입니다. TD(𝝀) 방법은 지나간 상태-액션 쌍에 대한 신용을 할당하기 위해 '적격성 흔적(eligibility traces)'이라 불리는 매개변수를 사용합니다. 미래 n단계를 추정하는 대신, 적격성 흔적은 여러 TD 단계에 걸쳐 상태-액션 쌍에 대한 신용을 할당합니다. 적격성 흔적은 지난 상태-액션 쌍이 관찰된 보상 전환에 기여한 정도에 대해 신용을 부여합니다. 적격성 흔적은 각 상태-액션 쌍에 연관된 벡터나 행렬로 표현됩니다. 시간 단계에 대한 적격성 흔적은 다음과 같이 재귀적으로 계산됩니다:\n\n![Eligibility Trace Formula](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_22.png)\n\n여기서 람다(𝝀) 매개변수는 부트스트래핑의 정도를 제어합니다. 𝝀 = 1일 때, 부트스트래핑이 없어지고 업데이트 규칙은 Monte Carlo로 축소됩니다. 𝝀 = 0일 때는 부트스트래핑이 있는 TD(0)로 축소됩니다. TD(𝝀)는 TD와 MC를 연속체로 일반화한 것으로, 여기서 TD(0)는 단일 단계 TD를 의미하며, TD(1)은 TD를 ∞ 단계까지 확장한 극한값인 MC를 의미합니다. 수식에서 보듯이, 적격성 흔적 매개변수는 재귀적으로 계산됩니다. 다음 시간 단계의 적격성 흔적값은 이전 단계의 적격성 흔적 값을 입력으로 취합니다. E(s) = 0일 때, 부트스트랩이 없어집니다. TD(𝝀) 업데이트 규칙은 아래와 같이 TD 및 MC 업데이트 규칙과 동일하게 계산되지만, 애러에 적격성 흔적을 곱하는 것이 차이입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Image](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_23.png)\n\n## ANNs를 활용한 강화 학습 확장\n\n모델 기반 또는 모델 무관 RL 알고리즘은 차원의 저주로 인한 스케일링 문제에 직면하며, 다양한 유형의 환경 간 일반화에 어려움을 겪으며, 샘플 효율성에 대한 어려움이 있습니다.\n\n인공 신경망(ANN)은 RL 아키텍처 내재한 일부 한계를 교정하는 강력한 방법을 제공합니다. 특히, ANNs는 샘플링 효율성, 환경 일반화, 차원의 저주로 인한 스케일링 문제를 개선합니다. 데이터로부터 일반 함수를 학습하기 때문에 우수한 일반화 능력을 통해 샘플 효율성을 줄이고 환경 일반화를 향상시킵니다. 이는 숨겨진 계층의 수와 각 숨겨진 계층 당 뉴런 수를 늘릴 수 있어 더 잘 스케일링할 수 있도록 합니다. 그러나 너무 많은 숨겨진 계층과 뉴런은 계산 스케일링 문제를 야기할 수도 있습니다(특정 범위를 벗어날 경우 차원의 저주를 피할 수 없습니다). 또한 전통적으로 ANNs가 사전에 목표 상태의 비정상성 문제에 시달리며, RL 알고리즘은 정책 상이든 오프-정책 상에 상관없이 업데이트 함수를 통해 최적 상태를 찾습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n전통적인 강화 학습 알고리즘이 확률적 전이 규칙에 의존하는 데 반해, 강화 학습에 인공 신경망(ANNs)을 적용하면 함수 근사를 사용하여 상태 및 상태-행동 값들을 계산하게 됩니다. 선형 근사 및 타일 코딩과 같은 여러 함수 근사 방법을 적용할 수 있지만, 인공 신경망은 비선형 함수 근사를 활용한 일반화 능력으로 가장 강력한 기술을 구성합니다.\n\n강화 학습에 인공 신경망을 적용하는 두 가지 접근 방식인 딥 Q 학습(DQN)과 시간차 학습( Temporal Difference; TD(𝝀))에 대해 알아봅시다. 미리 목표 값들을 모르기 때문에 MC 또는 TD를 사용하여 목표 상태의 추정인 예상 반환값을 생성합니다. 그런 다음 이 값은 함수(실제로는 네트워크 매개 변수 𝜃에 대한 전체 네트워크의 오차의 편도함수인 경사)가 근사화할 목표값으로 사용됩니다. 인공 신경망은 목표값을 추정 값과 출력 사이의 오차를 계산하여 그 오차를 역전파 및 최적화 알고리즘을 통해 감소시킴으로써 목표 값을 근사화합니다. 가장 일반적인 최적화 알고리즘은 확률적 경사 하강법의 변형인 것이며, 이를테면 확률적 경사 하강법이 대표적입니다.\n\n![강화 학습 소개](/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_24.png)\n\n## 오프-폴리시 DQN\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n큐-러닝은 SARSA(State, Action, Reward, State', Action')의 오프-폴리시 버전으로, 다음 상태-액션 쌍 Q(s', a')은 다음 상태에서 이용 가능한 액션들 중에서 최대 예측 값으로 추정됩니다. 다시 말해, 큐-러닝은 다음 상태 s'에서 이용 가능한 액션들 사이에서 Q(s',a')의 최댓값을 선택합니다. 이는 정책(𝜋)을 사용하지 않고 Q(s',a')를 학습한다는 것을 의미합니다. 반면, SARSA는 이전 액션을 선택하고 다음 상태-액션 쌍인 Q(s',a')를 추정하는 온-폴리시 방법입니다. 이는 상태가 주어졌을 때 액션의 확률인 정책(𝜋)을 사용하여 Q-함수를 학습한다는 것을 의미합니다.\n\n딥 큐-러닝에서는 액션-가치 함수 Q(a, s)가 Q(a,s, 𝜃)로 표현되며, 여기서 𝜃은 신경망 매개변수를 나타냅니다. Theta(𝜃) 매개변수는 신경망에서의 가중치 w에 해당하며, 뉴런들 간의 연결에 관련되어 있습니다. 이 가중치는 연결의 강도를 결정하고, 오차를 최소화하기 위해 역전파를 통해 후방으로 조정됩니다. DQN은 환경의 고차원 표현을 입력으로 삼고, 각 가능한 액션에 대한 액션-가치 벡터를 출력합니다. 예상 수익은 일반적으로 MC 또는 TD 접근법을 통해 근사됩니다. 이후, 역전파와 최적화 함수를 사용하여 정책 기울기를 계산하고 정책 네트워크의 매개변수(𝜃)를 조정하여 오차를 줄입니다.\n\n인공신경망은 새로운 정보에 매우 민감하기 때문에, 새로운 정보가 이전에 작성된 정보를 덮어쓰는 치명적인 잊혀짐을 초래할 수 있습니다. 이러한 치명적인 잊혀짐을 다루는 방법 중 하나는 경험 재생을 적용하는 것입니다. 이 기법은 과거 경험을 저장하고 네트워크를 훈련하는 데 재사용합니다.\n\n## 온-폴리시 딥 TD(𝝀) \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nANNs는 TD(λ) 방법에도 적용할 수 있습니다. 여기서 상태 관찰은 ANN에 입력으로 공급되고, ANN은 그것을 통해 액션-가치 함수를 출력으로 근사화합니다. TD(λ)의 온-폴리시 성격 때문에 Deep TD(λ) 방법은 상태 간의 장기 의존성이 필요한 작업에 가장 적합합니다.\n\nTD(λ)와 같은 온라인 학습 방법을 훈련하는 것은 도전적일 수 있습니다. 왜냐하면 환경의 분포가 부트스트래핑으로 인해 매 단계마다 또는 n 단계마다 변경되기 때문이기 때문입니다. 이를 비정상성(nonstationarity)이라고 부르며, ANN 매개변수 𝜃가 최적으로 수렴하는 것을 방해합니다. 온라인 학습에서 연이어 발생하는 상태 간의 종속성은 치명적인 잊혀짐(catastrophic forgetting)을 발생시킬 수 있어, 업데이트가 과거 학습에 영향을 미칩니다. 더욱이, 과거 조치에 신용을 할당하는 자격 흔적 및 ANNs의 결합은 역전파 단계에서 추가적인 복잡성을 초래할 수 있습니다.\n\n위기에 대처하는 한 가지 방법은 경험 재생(experience replay) 기법을 활용하는 것입니다. 경험 재생은 학습된 에이전트 에피소드를 [s, a, r, s’]의 벡터로 메모리 버퍼에 저장합니다. 훈련 중에 네트워크는 저장된 학습 벡터의 메모리 버퍼에서 샘플을 추출하여 네트워크 매개변수를 업데이트합니다. 이를 통해 네트워크는 더 큰 안정성을 제공받고, 새로운 경험으로 인한 큰 오류나 단계 간의 시간 차이로 인한 치명적인 간섭에서 덜 영향을 받습니다.\n\nDeep TD(λ) 알고리즘은 상태 공간이 연속적이고 대상이 알려지지 않거나 불분명한 연속 제어 작업에서 뛰어난 성과를 보여주었습니다. 이러한 작업에는 로봇 과제, 자율 주행 자동차, 금융 시장의 연속 제어 작업 등이 포함됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 강화 학습과 인공 일반 지능\n\n강화 학습이 인공 일반 지능에 미칠 영향은 무엇인가요?\n\n\"지능\"이라는 것은 서로 다른 능력을 단일한 개념으로 결합하기 때문에 모호한 변수이지만, \"일반 지능\"은 생물의 진화된 능력들을 기반으로 하며, 생존과 번식을 위해 세계적인 정보를 변환하는 것이 요구된다. 심지어 인간의 맥락에서도 지능은 유기적인 생존 가능성의 윤곽에서 분리될 수 없다. 그러나 이것이 통용되는 견해는 아니다. 일반적인 지혜는 지능이 이용 가능한 정보를 기반으로 추론을 계산하는 프로그램 또는 소프트웨어와 유사하다고 주장한다.\n\n후자의 개념은 경쟁한다고 여겨지는 두 가지 모델로 구성되어 있는데, 하나는 절차를 따르는 지능으로 설명되고, 다른 하나는 최적의 예측을 위해 데이터로부터 일반화하는 지능으로 설명된다. 전자는 일반적으로 더 잘 이해되지만, 후자는 예측의 강도를 신뢰성 있게 향상시키는 기법 집합으로 이루어져 있다고 할 수 있다. 동물의 지능은 대부분 후자 모델에 기반한다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 번째 모델의 가장 성공적인 패러다임은 인공 신경망을 통한 딥 러닝입니다. 인공 신경망 구조의 주요 장점은 사전 정보나 개념 없이 데이터로부터 일반화를 가능하게 한다는 것입니다. 이는 비지도 학습과 혼동해서는 안됩니다. 인공 신경망은 먼저 훈련을 통해 모델을 구축한 다음 새로운 데이터에 대해 해당 모델을 기반으로 예측합니다. 따라서 두뇌도 (진화를 통한 사전 훈련을 고려하면) 비슷한 일을 한다고 생각됩니다. 그러나 현재 인공 신경망에서는 두 가지 약점이 있습니다. 첫 번째 약점은 목표나 결과가 인간 디자이너에 의해 설정되어야 한다는 것입니다. 인공 신경망은 스스로 목표를 설정할 수 없습니다. 또한, 스스로 진실과 거짓을 구별할 수 없습니다. 모델이 그 결과를 근사화하는 방법을 배우기 위해 참된 결과를 제공해야 합니다. 두 번째 약점은 강화 학습 없이 인공 신경망이 자체 상태를 최적화하기 위해 환경을 탐색할 수 없다는 것입니다. 이러한 이유로 인공 신경망의 일반화와 예측 능력을 강화 학습의 결정 최적화 능력과 결합하는 것이 엄청난 섞임을 만들어냅니다.\n\n이 기반 위에서 강화 학습이 인공 일반 지능으로 가는 가장 명확한 길을 대표한다고 주장한 사람들도 있습니다(Sutton, 2014). 이에 대한 직관은 분명합니다. 강화 학습은 생명체를 모델링하는 데 가장 가깝습니다. 이것이 다른 성공적인 구조들과 결합되면 (예를 들어 변환기와 같은), 모든 인간 능력을 복제하고 능가하는 AI 모델로 이어질 수 있습니다.\n\n그러나 만약 인간이 일반 지능의 기초라면, 일반 지능의 개념은 인지능력을 생존 제약과 어떤 형태의 구현체와 이혼시키지 않는 것일 수 없습니다. 반면에, 일반 지능을 생명 형태에 언급하지 않고 정의할 수 있다면, 그것이 어떤 모습인지는 분명하지 않습니다. 즉 순수한 추상 모델은 Marcus Hutter의 AIXI와 같은 시도에도 불구하고 만족스러운 형식화를 피해갑니다. 추상적으로는 논리적 추론과 계산 능력만으로 문제를 해결하는 완벽하게 합리적인 에이전트로 생각할 수 있습니다. 정보와 구현체 간의 격차는 이 기사의 범위를 넘어서는 더 큰 토론을 유발하는 것이며, 관심이 있다면 이 논문이 좋은 시작점을 제공합니다.\n\n그러나 강화 학습만으로는 인공 일반 지능을 충분히 표현할 수 있는지에 대한 의문이 있습니다. 이에 대한 이유로는 일반 지능의 정의에서 비롯한 것들이며 현재의 대부분의 AI 연구자들이 명시적인 내부 표현을 필수적인 요소로 간주하지 않고 있기 때문입니다. 이에 대한 상당한 이유가 있습니다. 딥 러닝의 성공 이전에 인공 일반지능의 희망을 걸고 있던 상징적 AI는 실패로 이어졌습니다. 상징적 AI는 주로 명시적으로 코딩된 논리 규칙과 최적 추론 생성을 위한 지식 저장소에 기초한 인공 지능 접근 방식을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n상징적 인공지능과 신경망 사이의 긴장은 그러나 근거 없을 수도 있습니다. 많은 연구자들은 인공 일반 지능을 얻기 위한 탐구가 이러한 접근 방식을 적절히 결합하는 데 있다고 믿습니다. 신경망이 뇌의 본질적 오네톨로지를 근사화한다고 생각하는 이유는 수학적 논리가 뇌가 추론하는 방식이 아니기 때문입니다. 즉, 그것은 필요충분 조건을 계산하지 않거나 정확한 멤버십을 계산하는 것보다는 점수 있는 멤버십에 중점을 두며, 이는 퍼지 논리와 같은 방식에 의해 근사화되며 ANN(인공신경망)이 뛰어납니다.\n\n신경망은 원하는 출력을 달성하기 위해 파라미터화된 은닉층의 계층적 아키텍처로 구성되어 있으며, 고도로 보정된 동적 학습률, 활성화 함수, 연결 가중치 및 최적화 알고리즘을 통해 오차를 최소화하기 위해 보정된 학습율을 통해 원하는 출력을 달성합니다. 위와 같이 고도로 보정된 하이퍼파라미터를 넘어서 사람이 이해하지 못하는 정보가 은닉층에서 처리된다는 가정입니다. 정보가 이산적인 표현 단위(아날로그나 이미지 등)의 조합으로 저장되지 않으며 수십억 개의 뉴런들로 이루어진 분산 아키텍처로 저장된다는 것이 뇌의 경우와 같다는 가정입니다. 언어적으로 구조화된 생각이 고정적인 뉴런 조합으로 내부적으로 표현되는 것이 아닌데, 예를 들어 '존재 자체가 다른 존재를 위한 존재임을 하자’ 라는 문장 또는 단어를 나타내는 뉴런의 특정 조합이 없습니다.\n\n언어적 능력은 대신 경험에 의해 강화된 의미 연결과 재생규칙으로 내장된 거대한 네트워크에 포함되어 있습니다. 다시 말해, 우리가 반영적으로 문장을 작성하고 말할 때 언어와 생각이 뇌의 본질적 오네톨로지와 문법 간 이쾌적 매핑이 아닌, 연결의 정도와 연결 강도에 의해 특징을 가진 신경 접속의 분산 네트워크에 내재되어 있다는것을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 AI는 세계에서 자체적으로 추진되는 자율 시스템을 근사하지 않습니다. 또한 인간과 다른 동물들이 하는 방식으로 자체 환경적 환경을 생성하거나 자체 검색 공간을 재구성하지도 않습니다. 현재 이 제약이 없기 때문에 인간 디자이너는 AI의 정보적 중요성을 설정할 수 있습니다. 예를 들어 텍스트 생성, 환경 감지 등이 있습니다. 아키텍처가 진정한 문제 해결 기계로 진화하더라도 반성적 인식 능력이 없다면 일반적 지능을 보유하고 있다고 할 수 없습니다. 전통적으로 일반 지능의 정의에서는 인간 지능의 상징인 전체 인식의 변수를 생략합니다. 전통적인 지능 정의는 반사적이고 전체적 인식은 역공학 및 구성 요소의 분석에 대한 강한 저항을 가지고 있기 때문입니다. 그 이유로 반성적 인식은 지능의 구성 요소로서 배제됩니다. 그러나 현재의 과학적 설명에 대한 저항을 인정한다고 해도, 물리주의를 배제하거나 비자연론의 지지를 함축하지 않습니다. 오히려 이해력의 부재를 인정하는 신호일 뿐입니다. 이해력의 공방 속에, 반성적인 인식이 생명 유기체의 기본 속성인 감각의 확장임을 가설합니다. 이를 주장함으로써, 자연선택 이외의 방법을 통해 자율 시스템을 설계할 수 없다는 의미는 아니지만, 그들이 가까운 미래에도 과학적 분석에는 어려움을 줄 수 있다는 가능성은 여전히 열려 있습니다. 강화 학습이 일반적 지능으로 이어지길 바란다면, 에이전트는 세계의 복잡한 표현을 보유할 뿐만 아니라 그 표현의 내부에서 전반적인 관점을 유지할 수 있는 강력한 아키텍처를 사전으로 가져야 합니다. 이는 모델-세계 상호작용이 과제에 반드시 필수적이지만, 원조 아키텍처는 다중 모달 정보 처리와 통합 능력을 갖춘 복합적인 계층적 내부 구조를 요구할 것입니다.\n\n## 선택된 참고 자료\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., \u0026 Hassabis, D. (2015). Deep Reinforcement Learning을 통한 Human-level Control. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236\n\nNeftci, E. O., \u0026 Averbeck, B. B. (2019, March 4). 인공 및 생물학적 시스템에서 강화 학습. Nature News. https://www.nature.com/articles/s42256-019-0025-4\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSharma, S. (2024, March 7). Learning to Mix 𝑛-Step Returns: Generalizing 𝜆-Returns for Deep Reinforcement Learning. Ar5iv. [Link](https://ar5iv.labs.arxiv.org/html/1705.07445)\n\nSanghi, Nimish. Deep Reinforcement Learning with Python: With PYTORCH, Tensorflow, and Openai Gym. Apress, 2021.\n\nSilver, D., Singh, S., Precup, D., \u0026 Sutton, R. S. (2021). Reward is enough. Artificial Intelligence, 299, 103535. [Link](https://doi.org/10.1016/j.artint.2021.103535)\n\nSpens, E., \u0026 Burgess, N. (2024, January 19). A generative model of memory construction and consolidation. Nature News. [Link](https://www.nature.com/articles/s41562-023-01799-z)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSutton, Richard S. Introduction to Reinforcement Learning. MIT Press.\n\nTyng, C. M., Amin, H. U., Saad, M. N. M., \u0026 Malik, A. S. (2017, August 24). The influences of emotion on learning and memory. Frontiers in psychology. [Link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5573739/)\n\nWhite, A., Modayil, J., \u0026 Sutton, R. (2014). Surprise and Curiosity for Big Data Robotics. Association for the Advancement of Artificial Intelligence, 19–22.","ogImage":{"url":"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png"},"coverImage":"/assets/img/2024-06-19-AnIntroductiontoReinforcementLearning_0.png","tag":["Tech"],"readingTime":28}],"page":"84","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"84"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>