<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/68" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/68" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="로봇들도 우리처럼 말할 수 있지만, 그렇다고 해서 그들이 인간이 되는 것은 아닙니다" href="/post/2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="로봇들도 우리처럼 말할 수 있지만, 그렇다고 해서 그들이 인간이 되는 것은 아닙니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="로봇들도 우리처럼 말할 수 있지만, 그렇다고 해서 그들이 인간이 되는 것은 아닙니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">로봇들도 우리처럼 말할 수 있지만, 그렇다고 해서 그들이 인간이 되는 것은 아닙니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AI와 로봇의 미래 모라벡의 역설 너머" href="/post/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AI와 로봇의 미래 모라벡의 역설 너머" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AI와 로봇의 미래 모라벡의 역설 너머" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">AI와 로봇의 미래 모라벡의 역설 너머</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="SLAM 개요" href="/post/2024-06-19-OverviewofSLAM"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="SLAM 개요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-OverviewofSLAM_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="SLAM 개요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">SLAM 개요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="로봇들이 제멋대로 현재와 미래에 우려할 이유가 있을까요" href="/post/2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="로봇들이 제멋대로 현재와 미래에 우려할 이유가 있을까요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="로봇들이 제멋대로 현재와 미래에 우려할 이유가 있을까요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">로봇들이 제멋대로 현재와 미래에 우려할 이유가 있을까요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="사냥 첫 번째 이야기" href="/post/2024-06-19-TheHuntPartOne"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="사냥 첫 번째 이야기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-TheHuntPartOne_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="사냥 첫 번째 이야기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">사냥 첫 번째 이야기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스" href="/post/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="가상에서 현실로 산업 응용 프로그램을 위한 더 똑똑한 로봇을 훈련시킬 합성 데이터" href="/post/2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="가상에서 현실로 산업 응용 프로그램을 위한 더 똑똑한 로봇을 훈련시킬 합성 데이터" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="가상에서 현실로 산업 응용 프로그램을 위한 더 똑똑한 로봇을 훈련시킬 합성 데이터" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">가상에서 현실로 산업 응용 프로그램을 위한 더 똑똑한 로봇을 훈련시킬 합성 데이터</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LLMLarge Language Models가 안드로이드 인간의 두뇌가 되면 어떤 일이 벌어질까요" href="/post/2024-06-19-WhathappenswhenLLMsbecomethebrainsofhumanoidandroids"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LLMLarge Language Models가 안드로이드 인간의 두뇌가 되면 어떤 일이 벌어질까요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-WhathappenswhenLLMsbecomethebrainsofhumanoidandroids_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LLMLarge Language Models가 안드로이드 인간의 두뇌가 되면 어떤 일이 벌어질까요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">LLMLarge Language Models가 안드로이드 인간의 두뇌가 되면 어떤 일이 벌어질까요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="단계별 안내 라즈베리 파이 4 클러스터에 K3s 설치하기" href="/post/2024-06-19-Step-By-StepGuideInstallingK3sonaRaspberryPi4Cluster"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="단계별 안내 라즈베리 파이 4 클러스터에 K3s 설치하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Step-By-StepGuideInstallingK3sonaRaspberryPi4Cluster_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="단계별 안내 라즈베리 파이 4 클러스터에 K3s 설치하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">단계별 안내 라즈베리 파이 4 클러스터에 K3s 설치하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이 완벽한 헤드리스 설정 안내" href="/post/2024-06-19-RaspberryPiFullHeadlessSetupGuide"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이 완벽한 헤드리스 설정 안내" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-RaspberryPiFullHeadlessSetupGuide_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이 완벽한 헤드리스 설정 안내" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">라즈베리 파이 완벽한 헤드리스 설정 안내</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/61">61</a><a class="link" href="/posts/62">62</a><a class="link" href="/posts/63">63</a><a class="link" href="/posts/64">64</a><a class="link" href="/posts/65">65</a><a class="link" href="/posts/66">66</a><a class="link" href="/posts/67">67</a><a class="link posts_-active__YVJEi" href="/posts/68">68</a><a class="link" href="/posts/69">69</a><a class="link" href="/posts/70">70</a><a class="link" href="/posts/71">71</a><a class="link" href="/posts/72">72</a><a class="link" href="/posts/73">73</a><a class="link" href="/posts/74">74</a><a class="link" href="/posts/75">75</a><a class="link" href="/posts/76">76</a><a class="link" href="/posts/77">77</a><a class="link" href="/posts/78">78</a><a class="link" href="/posts/79">79</a><a class="link" href="/posts/80">80</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"로봇들도 우리처럼 말할 수 있지만, 그렇다고 해서 그들이 인간이 되는 것은 아닙니다","description":"","date":"2024-06-19 18:39","slug":"2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman_0.png\" /\u003e\n\n보스턴 다이내믹스는 ChatGPT의 새로운 음성 생성 기능을 Spot 로봇에 추가하고 있습니다. 그 결과로 회사 방문 시 사람들을 인사할 수 있는 오토마타가 만들어졌습니다. 다양한 강세를 만들어내며 로봇 기술 세계에 이전에 없던 공감 효과를 만들어 냅니다. 사실 인기 있는 \"학대당한 로봇 비디오\"를 제외하면 로봇들이 우리 편을 들어주는 것을 보고 사악한 사람이라는 판단을 내린 것입니다.\n\n완벽한 영국 공손한 액센트로 말하는 Spot을 보는 것은 재미있을 수 있지만, ChatGPT의 음성 합성 기능을 사용해 스마트폰에서 생성적 조수와 긴 대화를 나누는 모습을 보면 조금 불안해집니다. 교통 체증 속에서 시간을 때우거나 에어팟을 착용하고 길을 거닐면서 대화를 나누는 것은 영화 \"Her\"에서 보던 것과 같습니다. 이것은 디스토피아적이며 여러 윤리적 문제를 불러일으킵니다.\n\n물론 우리는 항상 기술을 의인화해 왔습니다. 그러나 이 기술의 발전이 음성을 만들어내고, 또한 개인적인 관계와 비슷한 대화를 나눌 수 있게 한다면, 이것이 무례한 기업들에 의해 일반화의 한 형태로 선보인다면 우리는 재해와 취약한 사람들에게 심리적 문제를 만들 수 있다는 가능성을 고려해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아이돌들에 대한 청소년들의 중요성을 알고 있는 사람이나 개인 문제를 가진 어른이 알고리즘을 치료로 사용해볼 때 어떤 영향이 있을지 고려하는 사람은 바로 화제의 중심에 있음을 이해할 것입니다. 제약이 거의 없고 가끔 환각이 발생할 수 있는 상황에서 난처한 상황에 놓이게 되어 있다는 것을 알 수 있습니다. 요약하면, 이러한 가상 관계는 단순히 고장 발생 가능성이 높아질 뿐만 아니라 유지하는 사람들의 행동에 영향을 미치도록 조절될 가능성이 있다는 것을 이해하고 있습니다.\n\n다른 사람들과의 경험들은 우리에게 여러 방법으로 영향을 끼칠 것입니다. 그러나 최소한 이러한 경험들과 대화들은 어느 정도의 합리성을 가진 사람들 간의 경험적 대화입니다. 이러한 경험에서 generative assistants를 통해 사람들을 조건부로 만들어 가는 과정은 명백한 예방 조치 없이 이루어진다는 것은 받아들일 수 없습니다. 만약 이러한 제품들이 널리 이용 가능하고 표준화 요소까지 추가된다면, 곧 유명인 아바타들이 수백만 명의 사람들과 매일 대화하면서 앞서 진행된 대화에서 언급된 개인적 요소들을 대화에 도입하거나 다양한 종류의 개인 데이터를 추출해 광고주들에 판매하는, 또는 사람들의 기분을 유도하여 물건을 구매하거나 특정한 방향으로 투표하도록 사람들에게 영향을 끼칠 수 있습니다.\n\n우리 사회는 대량의 사람들이 자신이 대화하는 대상이 누구인지가 아닌 정말 무엇인지를 이해할 수 있도록 하는 교육 단계를 거치지 않았기 때문에 개인적 관계의 맥락에서 generative AI와 같은 기술을 수용할 준비가 되어 있지 않습니다. 많은 사람들은 알고리즘에 일정한 권한을 부여하여 기술을 통해 접근한 답변에 대한 사고주의를 외주하는 방식으로 자신의 비판적 사고를 외주하는 경향이 있습니다. 특정 기술이 어떻게 작동하는지를 모르는 상태, 아서 C. 클락이 옳게 관찰한 것처럼, 그것은 마술과 구분하기 어렵게 만듭니다. 이것은 인간 사회에 엄청난 해를 끼칠 수 있습니다. 왜냐하면, 왜곡된 현실 인식부터 소왈레로 나타나는 이해 미흡까지 다양한 심리적 문제를 초래할 수 있기 때문입니다.\n\n미디어에서 자주 본 어떤 사람을 만나는 것은 언제나 이상한 느낌을 줬습니다. “내 거실에 이 사람이 있는 것 같다”라는 느낌은 종종 처음 만나는 사람에 대한 내 가정과는 다르게 익숙하다고 오해하게 했습니다. 예를 들어 매일 뉴스를 보는 사람과의 대화와 같이 완전히 비대칭적인 관계를 완전히 받아들이는 것은 이해력, 교육, 판단력이 필요한 작업입니다. 만약 매일 자신의 아이돌로 위장하는 generative algorithm과 이야기하고 있는 사람이 진짜 그 사람을 만나거나 그 대화를 실제로 받아들이게 되었을 때 어떤 일이 일어날까요? 그리고 네트워크 정보를 재조합하는 generative algorithm인데도 불구하고 어떤 성격을 부여하는 사람이 등장했을 때는 어떻게 될까요? 그리고 이 모든 것이 불확실한 규정적 맥락에서 운영되는 상황에서, 이러한 도구가 어떻게 작동하는지나, 어떻게 조정되어야 하는지에 대한 경험이 없을 뿐만 아니라 심리적 장애에 대한 경험이 전혀 없는 상황에서 어떻게 할 것인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문제는 기술이 발전하는 속도가 아니라, 일부 무책임한 사람들이 적정한 조치를 취하지 않고 시장에 내놓는 제품들입니다. 우리는 제품과 서비스를 기반으로 한 것으로 수익을 창출하려는 얼간이들의 활동을 제한하기 위한 일탈적인 접근이 아닌, 이 기술에 대한 명확하고 정확한 규정이 필요합니다. 이미 많은 문제를 발생시킨 \"빨리 움직이고 무언가를 망가뜨리는\" 접근이 이제는 더 무서운 곳으로 나아가고 있습니다.\n\n저는 기술에 겁을 먹거나 사회적 문제를 기술 탓으로 돌리는 경향이 없습니다. 정기 독자들은 저가 일반적으로 기술적 낙관주의자라는 것을 알고 계실 것입니다. 그럼에도 불구하고 이 주제가 저를 진정으로 걱정하게 만들고 많은 후회로 이어질 것이라고 생각합니다.","ogImage":{"url":"/assets/img/2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman_0.png"},"coverImage":"/assets/img/2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman_0.png","tag":["Tech"],"readingTime":3},{"title":"AI와 로봇의 미래 모라벡의 역설 너머","description":"","date":"2024-06-19 18:37","slug":"2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox","content":"\n\n## AI | 로봇 공학 | 모라벡의 역설 | 파트 8 | FLYINGMUM\n\n우리는 모라벡의 역설에 대한 이 시리즈를 마무리하면서, AI 및 로봇 공학의 미래에 주목합니다. 이 역설이 강조한 도전들을 넘어 미래에는 무엇이 기대될까요? 이 마지막 부분에서는 새로운 트렌드, 잠재적인 폭발적 발전, 그리고 AI 및 로봇 공학이 우리 삶의 다양한 측면에 미치는 혁신적인 영향에 대해 탐구합니다.\n\n이전 블로그: https://medium.com/@flyingmum/ethical-and-societal-implications-aa9287d86395\n\n# AI와 로봇 공학의 새로운 트렌드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI와 로봇의 미래는 이러한 기술이 어디까지 이룰 수 있는지의 한계를 더욱 끌어올릴 것으로 약속되는 몇 가지 주요 트렌드에 의해 형성됩니다.\n\n1. 자연어 처리(NLP)의 발전: NLP의 계속된 향상은 AI 시스템이 사람의 언어를 더욱 세부적이고 맥락적으로 이해하고 생성할 수 있게 할 것입니다. 이는 인간-AI 상호작용을 강화시키고 고객 서비스, 교육 및 콘텐츠 제작과 같은 분야에서 더욱 정교한 응용 프로그램을 가능하게 할 것입니다.\n\n2. AI 주도의 자동화: AI와 로봇을 통합시킨 자동화 솔루션이 더욱 발전할 것입니다. 제조업부터 의료에 이르기까지, AI 주도의 로봇은 정밀하고 효율적으로 복잡한 작업을 수행할 것이며, 이는 산업을 혁신하고 새로운 기회를 창출할 것입니다.\n\n3. Edge AI: 컴퓨팅 파워가 데이터가 생성되는 곳에 가까워지면서 엣지 AI가 더욱 보편화될 것입니다. 이는 자율주행차, 스마트 시티, IoT 디바이스 등의 응용분야에서 실시간 데이터 처리와 의사결정을 가능하게 할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4. AI 윤리 및 지배구조: 견고한 윤리적 틀과 지배구조의 개발은 AI 기술이 책임을 지고 개발 및 배포되는 것을 보장하는 데 중요합니다. 이는 편향, 투명성 및 책임 문제를 다루는 것을 포함합니다.\n\n![](/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_0.png)\n\n## 잠재적인 돌파구\n\n몇 가지 잠재적인 돌파구는 AI와 로봇 기술의 능력을 크게 향상시킬 수 있으며, Moravec의 역설에서 강조된 제한을 극복하는 데 한걸음 더 나아가게 될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1. 일반 인공지능: 현재 AI 시스템은 특정 작업에 뛰어나지만, 인간과 유사한 이해력과 적응력으로 다양한 작업을 수행할 수 있는 일반 AI의 개발은 장기적인 목표로 남아 있습니다. 일반 AI를 달성하는 것은 이 분야에서 의미 있는 진전을 의미할 것입니다.\n\n2. 양자 컴퓨팅: 양자 컴퓨팅은 지수적으로 컴퓨팅 파워를 증가시킴으로써, AI 시스템이 현재 해결하기 어려운 복잡한 문제를 해결할 수 있게 합니다. 이는 약물 발견, 암호학, 기후 모델링 등 분야에서의 중요한 발전을 이끌 수 있습니다.\n\n3. 뇌-컴퓨터 인터페이스 (BCIs): 인간 뇌와 기계 사이의 직접적인 통신을 가능케 하는 BCI는 기술과의 상호작용 방식을 혁신적으로 변화시킬 수 있습니다. 이것은 의학, 재활, 인간 증강과 같은 분야에 깊은 영향을 줄 수 있습니다.\n\n4. 무리 로봇공학: 사회적 곤충의 집단 행동에 영감을 받은 무리 로봇공학은 많은 수의 단순 로봇의 조정을 통해 복잡한 작업을 수행하는 것을 포함합니다. 이 접근법은 환경 모니터링, 재난 대응, 농업 등 다양한 분야에서 혁신적인 해결책을 이끌어 낼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_1.png\" /\u003e\n\n# 사회에 미치는 변화\n\nAI와 로봇 기술의 지속적인 발전은 사회의 다양한 측면에 혁명적인 변화를 가져다주며 혁신을 촉진하고 삶의 질을 향상시킬 것입니다.\n\n1. 의료: AI를 활용한 진단, 맞춤 의학, 로봇 수술은 의료 혁신을 이끌어 더 나은 환자 결과와 효율적인 의료 서비스 제공으로 이어질 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. 교육: AI 기술을 활용한 교육 도구와 맞춤 학습 플랫폼은 학습 경험을 향상시키며 교육을 더 접근 가능하고 개인의 필요에 맞게 맞춤화할 것입니다.\n\n3. 환경 지속 가능성: AI와 로봇 기술은 에너지 사용의 최적화, 폐기물 감소, 생태계 모니터링, 기후 변화 대응 등 환경 문제 해결에 중요한 역할을 할 수 있습니다.\n\n4. 스마트 시티: AI와 사물인터넷의 통합은 데이터 기반 솔루션이 도시 인프라, 교통, 공공 서비스를 향상시키는 스마트 시티 개발을 이끌 것입니다.\n\n앞으로의 미래를 바라볼 때, AI와 로봇 기술이 우리의 세상을 변화시킬 잠재력은 엄청납니다. Moravec의 역설에서 강조된 과제에 대처하고 신흥 트렌드와 돌파구를 수용함으로써, 우리는 새로운 가능성을 창출하고 인간의 능력을 향상시키고 삶의 질을 향상시키는 기술을 만들어낼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_2.png)\n\n모라벡 패러독스를 넘어서는 여정은 지금 막 시작에 불과하며, 흥미진진하고 변화를 가져다줄 것으로 약속되어 있습니다. 우리가 인공지능과 로봇공학의 전선을 개척하고 혁신을 이어 나갈수록, 미래는 더 스마트하고 연결된, 지속 가능한 세계를 위한 끝없는 가능성을 품고 있습니다.\n\n#flyingmum #PamC #Technology #AI #MoravecsParadox #ArtificialIntelligence #MachineLearning #DeepLearning #DataScience #TechInnovation #FutureTech #QuantumComputing #SmartCities #AIResearch\n","ogImage":{"url":"/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_0.png"},"coverImage":"/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_0.png","tag":["Tech"],"readingTime":4},{"title":"SLAM 개요","description":"","date":"2024-06-19 18:36","slug":"2024-06-19-OverviewofSLAM","content":"\n\n\n![SLAM](/assets/img/2024-06-19-OverviewofSLAM_0.png)\n\nSLAM은 동시 위치 추적 및 지도 작성을 의미합니다. 이는 로봇 공학에서 사용되는 기술로, 알려지지 않은 환경의 지도를 구축하는 문제를 해결하면서 동시에 자신을 그 지도 안에서 위치시키는 것입니다.\n\nSLAM의 주요 목표는 로봇이 알 수 없는 환경을 탐색하고 탐색하여 그 환경의 지도를 만들고 동시에 그 지도 안에서 자신의 위치를 결정하는 것입니다. 이는 어떠한 환경 지식도 없이 실시간으로 수행됩니다.\n\nSLAM은 로봇 공학에서 중요한 문제입니다. 이로써 로봇은 알려지지 않거나 동적인 환경에서 자율적으로 작동할 수 있습니다. SLAM을 사용하여 로봇은 사전에 작성된 지도를 사용할 수 없거나 오래되었을 수 있는 환경에서 탐색, 탐험 및 작업을 수행할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSLAM은 자율 주행 차량, 드론, 이동 로봇, 심지어 증강 현실 시스템을 포함한 다양한 응용에서 사용됩니다. 예를 들어, 자율 주행 차량에서 SLAM은 차량이 주변 환경의 지도를 작성하고 안전하고 효율적인 경로를 계획할 수 있도록 돕습니다. 드론에서는 SLAM이 환경을 매핑하고 안정적인 비행을 유지하는 데 도움을 줍니다. 증강 현실 시스템에서는 SLAM이 가상 객체를 현실 세계에 정확하게 오버레이하는 데 사용됩니다.\n\nSLAM을 달성하기 위해 로봇은 일반적으로 카메라, 라이다 또는 거리 측정 장치와 같은 센서 데이터의 조합을 사용하여 환경을 인식합니다. 이들은 또한 이 센서 데이터를 처리하고 로봇의 위치를 추정하며 로봇이 이동할 때 지도를 업데이트하는 알고리즘을 활용합니다.\n\n![SLAM 개요](/assets/img/2024-06-19-OverviewofSLAM_1.png)\n\n## SLAM을 이해하기 위해 필요한 어휘는 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSLAM (Simultaneous Localization and Mapping)의 맥락에서는 이해해야 할 중요한 여러 용어와 개념이 있습니다. 함께 알아보겠습니다:\n\n- **Localization**: 로봇의 추정된 자세를 나타냅니다. 다양한 시간 단계에서 로봇의 위치와 방향을 포함합니다. 이러한 자세는 일반적으로 (x, y, theta) 좌표로 표시되며, 여기서 (x, y)는 위치를 나타내고 theta는 방향을 나타냅니다.\n- **Mapping**: 환경에서 랜드마크의 추정된 위치를 나타냅니다. 랜드마크는 로봇이 인식할 수 있는 환경의 독특한 특징으로, 벽, 코너, 또는 객체와 같은 것들을 포함합니다.\n- **로봇**: 센서와 액추에이터로 장착된 자율 에이전트입니다. SLAM의 맥락에서 로봇의 주요 작업은 환경을 탐험하고 센서 데이터를 수집하며 자신의 위치와 랜드마크의 위치를 추정하는 것입니다. 스마트폰의 증강 현실 (AR) 시스템의 경우, 일반적으로 물리적인 로봇은 없고, 대신 스마트폰 자체가 \"로봇\"으로 간주될 수 있습니다.\n- **랜드마크**: 로봇이 인식할 수 있는 환경의 독특한 특징이나 관심 지점입니다. 물체, 코너, 벽 또는 로봇이 내비게이션 및 위치 추정에 사용할 수 있는 다른 특징일 수 있습니다. 랜드마크는 로봇의 위치 추정을 위한 기준점으로 작용합니다.\n- **센서**: 로봇이 환경을 인식하는 데 사용하는 장치입니다. 범위 측정, 이미지 또는 깊이 데이터와 같은 로봇 주변 환경에 대한 정보를 제공합니다. SLAM에서 사용되는 일반적인 센서에는 레이저 거리 측정기나 초음파 센서와 같은 거리 측정기, 카메라, 라이다 및 오도미터 센서가 있습니다. 스마트폰이 \"로봇\"인 경우, 스마트폰은 카메라, 자이로스코프, 가속도계 및 깊이 센서와 같은 다양한 센서를 통합한 플랫폼으로 작동합니다.\n\n이러한 용어 외에도 일반적으로 SLAM에서 사용되는 몇 가지 용어가 있습니다:\n\n- **오도메트리**: 휠 엔코더, 가속도계 또는 자이로스코프와 같은 내부 센서를 기반으로 로봇의 움직임을 추정합니다. 로봇의 속도, 회전 및 변위에 대한 정보를 제공합니다.\n- **루프 클로저**: 로봇의 궤적에서 루프를 감지하고 수정하는 프로세스입니다. 로봇이 이전에 관측한 랜드마크나 위치를 재방문할 때 발생합니다. 루프 클로저는 누적된 오차를 수정하고 SLAM 솔루션의 정확도를 향상시키는 데 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSLAM에 사용되는 주요 용어 및 용어 중 일부입니다. 이러한 개념을 이해하면 SLAM 알고리즘 및 기술을 효과적으로 활용할 수 있습니다.\n\n![SLAM 사용 사례](/assets/img/2024-06-19-OverviewofSLAM_2.png)\n\n## SLAM의 사용 사례는 무엇인가요?\n\nSLAM 기술은 우리의 일상생활에서 점차 더 흔해지고 있지만 우리는 항상 그것을 인식하지 못할 수도 있습니다. 우리 일상생활에서 만나게 되는 SLAM 응용 분야의 몇 가지 예시는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- GPS 내비게이션 시스템: 많은 야외 GPS 내비게이션 시스템은 SLAM 기술을 활용하여 정확하고 실시간의 위치 정보를 제공합니다. 이러한 시스템은 GPS 데이터를 가속도계 및 자이로스코프와 같은 다른 센서 입력과 결합하여 차량의 위치와 방향을 추정합니다. 구글 지도, Waze, Apple 지도가 예시입니다.\n- 모바일 증강 현실 (AR) 앱: 스마트폰 및 태블릿용 AR 앱은 주로 SLAM 알고리즘을 활용하여 장치의 위치를 추적하고 가상 객체를 현실 세계 위에 오버레이합니다. SLAM을 사용하면 사용자 주변과 가상 콘텐츠를 정확하게 정렬하여 몰입형 AR 경험을 제공합니다. 포켓몬 GO, 스냅챗, 인스타그램이 예시입니다.\n- 자율 주행 청소기: Roomba와 같은 로봇 청소기는 SLAM을 활용하여 효율적으로 방을 탐색하고 청소합니다. 환경의 지도를 작성하고 그 지도 내에서 자신의 위치를 결정하는 위치 결정 기술을 사용하여 장애물을 피하면서 자율적으로 이동합니다.\n- 실내 내비게이션 시스템: 대형 건물 내부에서 실시간 위치 정보를 제공하기 위해 SLAM이 사용됩니다. 카메라나 깊이 센서와 같은 센서를 사용하여 실내 환경을 매핑하고 사용자가 복잡한 공간을 탐색하는 데 도움을 줍니다. SLAM을 기반으로 하는 한 예는 Google의 \"실내 맵\" 기능입니다. Google 지도 애플리케이션에서 사용할 수 있으며, 이는 SLAM을 Wi-Fi, Bluetooth 등과 함께 사용하여 이 건물 내에서 실시간 위치 정보와 방향을 제공합니다.\n- 자율 주행 자동차: SLAM은 자율 주행 차량을 위한 기본 기술입니다. 자율 주행 자동차는 LiDAR, 카메라, 레이더 등 다양한 센서를 사용하여 주변 환경을 인식하고 세부적인 지도를 작성합니다. SLAM 알고리즘은 차량이 지도 내에서 자신의 위치를 지정하고 안전하게 이동하도록 돕습니다. Waymo, Tesla, Cruise, Uber가 예시입니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-OverviewofSLAM_3.png\" /\u003e\n\n이것들은 SLAM 기술이 일상생활에 통합되는 몇 가지 예시일 뿐입니다. 로봇 공학 및 AI 분야가 발전함에 따라 우리는 다양한 분야에서 더 많은 SLAM 응용 프로그램을 기대할 수 있습니다.\n\n구현을 찾고 계십니까? 이것이 가장 간단한 SLAM 알고리즘입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글이 마음에 드셨다면 ❤를 눌러 다른 사람들이 찾을 수 있도록 도와주세요!","ogImage":{"url":"/assets/img/2024-06-19-OverviewofSLAM_0.png"},"coverImage":"/assets/img/2024-06-19-OverviewofSLAM_0.png","tag":["Tech"],"readingTime":4},{"title":"로봇들이 제멋대로 현재와 미래에 우려할 이유가 있을까요","description":"","date":"2024-06-19 18:34","slug":"2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture","content":"\n\n인공 지능이 비즈니스의 모든 영역에서 발전하고 있지만, 만약 그것이 \"야생\"이 된다면 무슨 일이 벌어질지에 대한 심각한 우려가 있습니다.\n\n![로봇 급속 업데이트 문제](/assets/img/2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture_0.png)\n\n로봇공학은 산업에서 많은 지루하고 반복적인 작업을 대체했으며, 속도와 정밀도로 제품을 제공했는데, 이는 인간이 불가능할 수도 있습니다. 그러나 모든 자동화된 장치와 마찬가지로 단점이 있습니다. 자동차 생산 공장은 이러한 로봇의 실수에 위험에 노출된 것으로 보입니다.\n\n최근 자율주행차량 전체가 도로에서 내려온 일이 있었습니다. 그 이유는 차량의 센서가 도로 상의 다친 보행자를 인식하지 못했기 때문입니다. 차량이 보행자를 치고 그를 자율주행차량의 경로로 던졌습니다. 그 자율주행차량은 그녀를 치고 난 후 도로를 따라 20피트를 끌고 그녀를 차 밑에 묶어 남겼습니다. 그 결과 그녀는 위험한 상태로 중상을 입었습니다. 이제 거리를 건너는 것이 완전히 안전하지 않을 수도 있습니다. 보행자가 보도를 건널 때 자율주행차량이 멈추지 않은 것에 관한 연방 조사가 진행 중입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정부가 승인한 자율 주행 자동차용 안전 절차가 명확히 정해져 있지 않습니다. 특히 자율 주행 자동차가 도로 중간에 갑자기 멈추는 경향은 교통을 방해하고 응급 서비스의 지연을 일으켜 주의를 불러일으키는 부분입니다. 샌프란시스코에서는 관리자들이 여러 사건을 기록해 왔습니다. 또한, 운전자 없는 자동차가 유아 수레에 부딪히는 것과 식료품 카트에 부딪히는 것 중 어떤 선택을 할지 결정해야 하는 로보틱스 전문가의 문제라는 난제도 있습니다.\n\n자율 주행 차량이 인간이 운전하는 차량보다 고장을 더 많이 일으킬 가능성이 높을 수 있습니다. 요즘은 자동차 사고가 특정 사건들일 뿐이며, 한 명의 위험 운전자는 전 세계의 다른 운전자에게 영향을 미치지 않습니다. 그러나 자율 주행 자동차가 등장하면 상황이 바뀔 수 있습니다. 어떠한 결함, 해킹, 시스템 고장이든 도로에 있는 모든 차량에 영향을 줄 수 있습니다.\n\n최근 언론 보도에 따르면 Cruise의 CEO조차도 자신의 안전 우려를 표명해 사임했다고 합니다. Tesla에 대한 추가적인 우려가 표명되기도 했습니다.\n\n캐나다의 무지개 다리를 건너려던 벤틀리가 폭발하는 사건으로 AI의 치명적인 사고에 대한 조사가 재개되었습니다. 이 고급차 회사는 오래된 모델(2020년-2023년)용 리콜을 여러 차례 내부적으로 실시해 왔는데, 이 특별한 사고의 차량 역시 오래된 모델이었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로봇이 우리에게 HAL이 하는 명령 거부의 불길한 소리를 들으면 어떻게 될까요? \"2001년 우주 여행\"에서 Hal이 말했던 \"Dave, 미안하지만 그걸 할 수 없어\"라는 말처럼. 이제 우리는 단순한 작업을 하는 로봇들과 함께, 센서가 사람과 같이 고장나면 비극이 일어날 수 있습니다.\n\n미디어에서 로봇 관련 공장 사고에 대해 듣는 것은 드물지만 최근 대한민국에서 공장 라인에서 센서를 수리하던 한 남성이 최신 희생자가 되었습니다. 로봇 팔이 갑자기 그를 붙잡아 음식 상자를 옮기는 대신에 컨베이어 벨트에 던지면서 그르게 만들었고, 결과적으로 그는 머리와 가슴에 치명적인 부상을 입었습니다.\n\n1992년부터 2017년 미국에서 로봇 관련 사망 사고가 41건 발생했는데, 대부분은 자체 전원으로 작동하는 고정 장치입니다. 폭스바겐 공장에서 장비를 설치하던 근로자가 로봇에 맞혀져 금속 벽에 눌려붙었고, 후에 병원에서 사망했습니다.\n\n미국 정부는 이러한 사건들을 기록하고 있지만 소비자들은 잘 참조하지 않습니다. 그러나 이것은 우리가 알아야 하는 정보를 제공합니다. 각 사건은 \"직원이 스폿용접 로봇이 눌러 죽을 때\", \"직원이 오크라 로봇 팔 사이에 끼여 죽음\", 또는 \"직원이 타 오르는 기계에 의해 눌려 죽음\"으로 나타내고 있습니다. 아니요, 기계는 생각하지 않습니다. 그들은 고장이 납니다. 이는 안전장치 부족 때문일 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1981년, 제조 로봇과 관련된 최초의 사고 중 하나가 가장 가와사키 공장에서 발생했습니다. 작업자가 수리하기 위해 유닛 주변의 울타리를 무시하고 접촉했을 때, 우연히 \"켜기\" 스위치에 닿아 로봇 팔이 그를 잡아 기어를 절단하는 기계로 밀었습니다. 동료들이 구할 수 없었습니다. 그는 사망했습니다. 이 사건 이전에는 1979년 포드 공장에서 창고선반을 쌓는 로봇이 작업자를 죽였는데, 그 작업자에게로봇을 빨리 움직이라는 지시를 받았을 때 발생했습니다. 그는 가파르게 올라가 희생되었습니다.\n\n현재, 장치, 소프트웨어 또는 로봇에 대한 책임을 결정하는 데에 대한 철저한 연구가 많이 진행되지 않았습니다. 관심이 있다면 해당 주제에 대한 책이 있으니 읽어보거나 요약본을 확인하는 것도 좋을 것 같습니다.\n\n놀라운 과학 작가인 아이작 아시모프는 로봇을 위한 규칙을 정리했습니다. 그의 \"로봇의 세 법칙\"에서 첫 번째 규칙은 다음과 같습니다: \"로봇은 인간을 상해 입히거나, 또는 아무런 조치를 취하지 않아도 인간이 위험에 빠지게 해서는 안 됩니다.\" 그러나 로봇이 하나의 일만을 수행하는 경우 인간과 제품 항목을 구별할 능력이 없을 것입니다. 프로그램에는 행동 변경이 제공되지 않을 것입니다. 또한 로봇이 미사일이나 전쟁 물자를 발사하여 적을 죽이도록 프로그래밍된다면 첫 번째 법칙이 작동하지 않게 됩니다.\n\n주요 문제는 무엇일까요? 로봇의 경우 고려하지 않은 사항을 고려하지 못하는 것으로 보입니다. 이러한 부상이나 사망은 단지 비극뿐만 아니라 사전에 고려되어야 했으며 그에 대한 안전장치가 로봇 코드에 삽입되어야 했습니다. 그러나 무엇이 잘못될 수 있는지 깊게 고려하지 않는다면, 어떻게 그것을 방지할 코드를 작성할 수 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDDIntel에 가입해 주세요.\n\n특별한 이야기를 공유하고 싶으세요? DDIntel에 제출해 주세요.\n\n저희 창작자 생태계에 가입해 보세요.\n\nDDIntel은 주요 사이트와 인기있는 DDI Medium 출판물에서 주목할 만한 콘텐츠를 모으고 있습니다. 우리 커뮤니티의 풍부한 작업을 더 자세히 살펴보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDDI 공식 텔레그램 채널: [https://t.me/+tafUp6ecEys4YjQ1](https://t.me/+tafUp6ecEys4YjQ1)\n\nLinkedIn, Twitter, YouTube 및 Facebook에서 팔로우해 주세요.","ogImage":{"url":"/assets/img/2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture_0.png"},"coverImage":"/assets/img/2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture_0.png","tag":["Tech"],"readingTime":4},{"title":"사냥 첫 번째 이야기","description":"","date":"2024-06-19 18:33","slug":"2024-06-19-TheHuntPartOne","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-TheHuntPartOne_0.png\" /\u003e\n\n드레이크는 깨어나 자신이 모르는 숲으로 떨어지고 있다는 것을 깨닫습니다. 그는 동일한 과정에서 많은 사람들을 만나게 됩니다: 로스 제타 경찰관 미겔, 특수 부대 알파 그룹 솔저 이반, 이스라엘 방위군 저격수 레아, RUF 장교 코피, 산 퀸틴 사형수 라스, 조직원 하오, 의사 이단. 그들은 레아가 전 블랙 옵스 전문가 및 용병일 것으로 의심하는 드레이크를 추적합니다. 숲 속에서 그들은 이상한 이미지, 빈 우리, 그리고 죽은 그린 베레 부대원의 죽음의 도구를 발견합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-TheHuntPartOne_1.png\" /\u003e\n\n더 나아가며, 그들은 외계인 하늘을 바라보며 자신들이 더 이상 지구에 있지 않음을 깨닫습니다. 이 곳은 사람들과 다른 동물들이 죽는 사냥터로 사용됩니다. 미겔은 살해되고, 그의 시체는 생존자들을 함정에 빠뜨리기 위해 사용됩니다. 그룹은 야수의 자취를 따라가 어느 캠프로 도착하게 되고, 거기서 프레데터가 잡혀 있는 것을 발견합니다. 그들의 사냥꾼인 트래커, 버서커, 그리고 팔코너라고 불리는 세 마리 큰 생물이 그룹을 공격하고, 코피를 죽입니다. 다른 이들은 탈출합니다. 레아의 고백한 '킬러'는 1987년 과테말라에서 특수 부대를 살해한 유사한 생물의 설명과 일치하지만 생존자에 의해 물리쳐지기도 했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![TheHuntPartOne_2.png](/assets/img/2024-06-19-TheHuntPartOne_2.png)\n\n혼자 남은 미국 육군 병사가 달에서 \"열 번의 시즌\" 동안 포식자들과 그 피해자들로부터 숨어 살아낸 이야기입니다. 그는 모두를 자신의 은신처로 인도하고 포식자들이 세 명씩 모여 기술을 연마하며 다른 세계에서 소중한 물건을 훔쳐 지구로 가져오는 것을 설명합니다. 가브리엘은 또한 포식자와 슈퍼 포식자라고 불리는 다른 부대 간에 쟁탈이 있다고 밝힙니다. 드레이크는 포식자들을 해방시켜 고향으로 데려오기를 희망하며 계획을 세웁니다. 그들은 장비를 구입합니다. 드레이크는 폭발물을 사용하여 포식자들을 꾀어 굴 속으로 이끕니다. 가브리엘은 그룹을 해방하는 트래커에게 살해당합니다.\n\n![TheHuntPartOne_3.png](/assets/img/2024-06-19-TheHuntPartOne_3.png)\n\n그에 이어 발생한 추격전에서 이반은 트래커를 죽이고 이단을 구하기 위해 자신을 희생합니다. 생존자들은 버서커에게 공격을 받지만 라스가 버서커의 주의를 분산시키고 다른 이들이 도망칠 수 있게 해줍니다. 하오는 가브리엘로부터 숨어 두었던 카타나로 파울코너와 결투를 벌이며 그를 죽이고 자신의 상처로 인해 죽습니다. 이단이 드레이크의 계획을 이어가길 희망하지만 함정에 걸려 부상을 입습니다. 리아가 그를 떠나지 않겠다고 하자 드레이크는 그들을 남겨두고 떠납니다. 그들은 버서커들에게 잡혀 구멍 속에 갇히고 캠프를 향해 계속 나아갑니다. 드레이크는 지구로 이동하기 위해 포식자들을 해방합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-TheHuntPartOne_4.png)\n\n그는 프레데터 갑옷을 입고, 손목 컴퓨터로 우주선을 켜고 지구로 향한다. 드레이크는 우주선을 탈출하지만, 버서커가 도착하여 다른 프레데터를 압도하고 목을 베어 그의 손목 컴퓨터를 사용하여 우주선을 파괴하고, 드레이크를 죽인 것으로 보인다. 이전에 식물에서 본 신경 독소를 사용하여 리아를 마비시키며, 그 또한 지구에 남고 싶은 암살자임을 밝힌 이단이다. 드레이크가 나타나 이단을 독립시킨 후 수류탄으로 그를 유인하여 버서커를 상처 입히는 함정을 설치한다.\n\n![이미지](/assets/img/2024-06-19-TheHuntPartOne_5.png)\n\n리아의 도움으로 드레이크는 버서커를 물리치고 죽인다. 사람들은 지구로 돌아가는 방법을 찾기 위해 숲으로 향한다. 생존 중 형성된 인연은 그들로 하여금 숲의 신비와 자신의 도전에 직면할 때 힘을 주며, 싸움이 끝나지 않았지만 살아남고 길을 찾겠다는 결의를 하고 있음을 깨닫는다.\n","ogImage":{"url":"/assets/img/2024-06-19-TheHuntPartOne_0.png"},"coverImage":"/assets/img/2024-06-19-TheHuntPartOne_0.png","tag":["Tech"],"readingTime":3},{"title":"미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스","description":"","date":"2024-06-19 18:31","slug":"2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase","content":"\n\n## 미디어파이프의 손 추척 및 제스처 인식을 Rerun과 함께 시각화하는 방법\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*pE_4QrsVPV7vMrxB6YS1cQ.gif)\n\n이 게시물에서는 미디어파이프 파이썬과 Rerun SDK를 사용하여 손 추척 및 제스처 인식의 예제를 소개하고 있습니다.\n\n더 깊이 파고들고 이해를 넓히고 싶다면, 미디어파이프 파이썬 및 Rerun SDK를 설치하여 손을 추적하고 다양한 제스처를 인식하고 데이터를 시각화하는 방법을 안내해 드리겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 그러므로, 다음을 배울 것입니다:\n\n- MediaPipe Python 및 Rerun 설치하는 방법\n- MediaPipe 제스처 인식을 사용한 손 추적 및 제스처 인식 방법\n- 손 추적 및 제스처 인식 결과를 Rerun Viewer에서 시각화하는 방법\n\n예제를 시도하기를 열망한다면, 아래 제공된 코드를 사용해보세요:\n\n```js\n# rerun GitHub 저장소를 로컬 머신에 클론합니다.\ngit clone https://github.com/rerun-io/rerun\n\n# rerun 저장소 디렉토리로 이동합니다.\ncd rerun\n\n# 필요한 Python 패키지를 requirements 파일에 명시된대로 설치합니다.\npip install -r examples/python/gesture_detection/requirements.txt\n\n# 예제를 위한 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py\n\n# 특정 이미지에 대한 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py --image path/to/your/image.jpg\n\n# 특정 비디오에 대한 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py --video path/to/your/video.mp4\n\n# 카메라 스트림으로 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py --camera\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 손 추적 및 제스처 인식 기술\n\n계속하기 전에, 우리가 가능하게 한 기술에 대해 인정해주어야 합니다. 손 추적 및 제스처 인식 기술은 기기가 손 움직임과 제스처를 명령이나 입력으로 해석할 수 있도록 하는 것을 목표로 합니다. 이 기술의 핵심은 미리 훈련된 기계 학습 모델이 시각 입력을 분석하고 손의 랜드마크와 제스처를 식별합니다. 이러한 기술의 실제 응용은 다양하며, 손 움직임과 제스처를 사용하여 스마트 기기를 제어하는 데 사용될 수 있습니다. 인간-컴퓨터 상호 작용, 로봇 공학, 게임 및 증강 현실은 이 기술의 잠재적인 응용 분야 중 가장 유망하게 보입니다.\n\n그러나 이러한 기술을 사용하는 방법에 대해 항상 주의해야 합니다. 민감하고 중요한 시스템에서 사용시 손 제스처를 잘못 해석할 수 있고, 잘못된 양성 또는 음성의 가능성이 작지 않습니다. 이를 활용함으로써 발생하는 윤리적 및 법적 문제가 사용자들이 특히 공공장소에서 자신의 제스처가 기록되는 것을 원치 않을 수 있습니다. 현실 세계 시나리오에서 이 기술을 도입하기로 결정했다면, 윤리적 및 법적 고려 사항을 고려하는 것이 중요합니다.\n\n# 요구 사항 및 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저, 필요한 라이브러리를 설치해야 합니다. 이는 OpenCV, MediaPipe 및 Rerun과 같은 라이브러리를 포함합니다. MediaPipe Python은 컴퓨터 비전 및 머신러닝을 위한 온디바이스 ML 솔루션을 통합하려는 개발자들에게 유용한 도구이며, Rerun은 시간이 지남에 따라 변화하는 다중 모달 데이터를 시각화하기 위한 SDK입니다.\n\n```js\n# 요구 사항 파일에서 지정된 필수 Python 패키지 설치\npip install -r examples/python/gesture_detection/requirements.txt\n```\n\n그런 다음, 여기서 미리 정의된 모델을 다운로드해야 합니다: HandGestureClassifier\n\n# MediaPipe를 사용한 손 추적과 제스처 인식\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png\" /\u003e\n\n이제 샘플 이미지에 제스처 인식을 위해 MediaPipe 사전 훈련 모델을 사용해봅시다. 아래 코드는 MediaPipe 제스처 인식 솔루션의 초기화 및 구성을 설정하는 기초를 제공합니다.\n\n```js\nfrom mediapipe.tasks.python import vision\nfrom mediapipe.tasks import python\n\nclass GestureDetectorLogger:\n\n    def __init__(self, video_mode: bool = False):\n        self._video_mode = video_mode\n\n        base_options = python.BaseOptions(\n            model_asset_path='gesture_recognizer.task'\n        )\n        options = vision.GestureRecognizerOptions(\n            base_options=base_options,\n            running_mode=mp.tasks.vision.RunningMode.VIDEO if self._video_mode else mp.tasks.vision.RunningMode.IMAGE\n        )\n        self.recognizer = vision.GestureRecognizer.create_from_options(options)\n\n\n    def detect(self, image: npt.NDArray[np.uint8]) -\u003e None:\n        image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n  \n        # 제스처 검출 모델로부터 결과 가져오기\n        recognition_result = self.recognizer.recognize(image)\n  \n        for i, gesture in enumerate(recognition_result.gestures):\n            # 인식된 제스처 중 상위 제스처 가져오기\n            print(\"최상위 제스처 결과: \", gesture[0].category_name)\n  \n        if recognition_result.hand_landmarks:\n            # MediaPipe에서 손 랜드마크 가져오기\n            hand_landmarks = recognition_result.hand_landmarks\n            print(\"손 랜드마크: \" + str(hand_landmarks))\n  \n            # MediaPipe에서 손 연결 정보 가져오기\n            mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n            print(\"손 연결 정보: \" + str(mp_hands_connections))\n```\n\nGestureDetectorLogger 클래스 내의 detect 함수는 이미지를 인자로 받아 모델 결과를 출력하며, 인식된 최상위 제스처와 감지된 손 랜드마크를 강조합니다. 모델에 대한 추가 정보는 해당 모델 카드를 참조하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_1.png)\n\n아래 코드를 사용하여 직접 시도해볼 수 있어요:\n\n```js\ndef run_from_sample_image(path)-\u003e None:\n    image = cv2.imread(str(path))\n    show_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    logger = GestureDetectorLogger(video_mode=False)\n    logger.detect_and_log(show_image)\n\n# 샘플 이미지로 제스처 인식 실행하기\nrun_from_sample_image(SAMPLE_IMAGE_PATH)\n```\n\n# 재실행을 사용하여 확인, 디버그 및 데모하기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 단계는 솔루션의 신뢰성과 효과성을 보장하는 데 도움이 됩니다. 모델을 준비한 상태로 결과를 시각화하여 정확성을 확인하고 잠재적인 문제를 해결하며 능력을 시연할 수 있습니다. 결과를 시각화해서 Rerun SDK를 사용하면 간단하고 빠르게 가능합니다.\n\n## Rerun을 어떻게 사용할까요?\n\n![이미지](/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_2.png)\n\n- Rerun SDK를 사용하여 코드에서 로깅하여 다중 데이터를 스트림으로 전송\n- 현지 또는 원격으로 라이브 또는 녹화된 스트림을 시각화하고 상호 작용\n- 레이아웃을 대화식으로 구축하고 시각화를 사용자 정의\n- 필요할 때 Rerun을 확장\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드 작성에 앞서, Rerun Viewer를 설치하기 위해 해당 페이지를 방문하는 것이 좋습니다. 그런 다음, Rerun SDK에 대해 읽어보는 것을 권해드립니다. 파이썬 빠른 시작 가이드와 파이썬에서 데이터 기록하기를 읽어보세요. 이러한 초기 단계는 원활한 설정을 보장하고 다가오는 코드 실행에 도움이 될 것입니다.\n\n## 비디오 또는 실시간 실행\n\n비디오 스트리밍에는 OpenCV가 사용됩니다. 특정 비디오의 파일 경로를 선택하거나 0 또는 1의 인수를 제공하여 자체 카메라에 액세스할 수 있습니다 (기본 카메라를 사용하려면 0을 사용하고, 맥에서는 1을 사용할 수 있습니다).\n\n타임라인의 소개를 강조하는 것이 중요합니다. Rerun 타임라인의 기능은 데이터를 하나 이상의 타임라인과 연관시킬 수 있게 합니다. 결과적으로, 비디오의 각 프레임은 해당 타임스탬프와 연관되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef run_from_video_capture(vid: int | str, max_frame_count: int | None) -\u003e None:\n    \"\"\"\n    비디오 스트림에서 탐지기를 실행합니다.\n\n    매개변수\n    ----------\n    vid:\n        탐지기가 실행될 비디오 스트림입니다. 기본 카메라에는 0/1을 사용하거나 비디오 파일의 경로를 지정하세요.\n    max_frame_count:\n        처리할 최대 프레임 수입니다. None이면 모든 프레임을 처리합니다.\n    \"\"\"\n    cap = cv2.VideoCapture(vid)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n\n    detector = GestureDetectorLogger(video_mode=True)\n\n    try:\n        it: Iterable[int] = itertools.count() if max_frame_count is None else range(max_frame_count)\n\n        for frame_idx in tqdm.tqdm(it, desc=\"프레임 처리 중\"):\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            if np.all(frame == 0):\n                continue\n\n            frame_time_nano = int(cap.get(cv2.CAP_PROP_POS_MSEC) * 1e6)\n            if frame_time_nano == 0:\n                frame_time_nano = int(frame_idx * 1000 / fps * 1e6)\n\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n            rr.set_time_sequence(\"프레임 번호\", frame_idx)\n            rr.set_time_nanos(\"프레임 시간\", frame_time_nano)\n            detector.detect_and_log(frame, frame_time_nano)\n            rr.log(\n                \"미디어/비디오\",\n                rr.Image(frame)\n            )\n\n    except KeyboardInterrupt:\n        pass\n\n    cap.release()\n    cv2.destroyAllWindows()\n```\n\n## 시각화를 위한 데이터 로깅\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*c1Us-7PoWSP0rgVdlMQUhA.gif\" /\u003e\n\nRerun Viewer에서 데이터를 시각화하려면 Rerun SDK를 사용하여 데이터를 로깅하는 것이 중요합니다. 이전에 언급된 가이드는 이 프로세스에 대한 통찰을 제공합니다. 이 문맥에서는 정규화된 값으로 손 랜드마크 포인트를 추출한 다음, 이미지의 너비와 높이를 사용하여 이미지 좌표로 변환합니다. 이러한 좌표는 2D 포인트로 Rerun SDK에 로깅됩니다. 추가로, 랜드마크 간의 연결을 식별하고 2D 라인스트립으로 로깅합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제스처 인식을 위해 결과는 콘솔에 출력됩니다. 그러나 소스 코드 안에서는 TextDocument 및 이모지를 사용하여 이러한 결과를 시청자에게 제시하는 방법을 탐구할 수 있습니다.\n\n```js\nclass GestureDetectorLogger:\n\n    def detect_and_log(self, image: npt.NDArray[np.uint8], frame_time_nano: int | None) -\u003e None:\n        # 이미지에서 제스처 인식\n        height, width, _ = image.shape\n        image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n\n        recognition_result = (\n            self.recognizer.recognize_for_video(image, int(frame_time_nano / 1e6))\n            if self._video_mode\n            else self.recognizer.recognize(image)\n        )\n\n        # 값 지우기\n        for log_key in [\"Media/Points\", \"Media/Connections\"]:\n            rr.log(log_key, rr.Clear(recursive=True))\n\n        for i, gesture in enumerate(recognition_result.gestures):\n            # 인식된 제스처를 기록\n            gesture_category = gesture[0].category_name if recognition_result.gestures else \"None\"\n            print(\"제스처 카테고리:\", gesture_category)\n\n        if recognition_result.hand_landmarks:\n            hand_landmarks = recognition_result.hand_landmarks\n\n            # 정규화된 좌표를 이미지 좌표로 변환\n            points = self.convert_landmarks_to_image_coordinates(hand_landmarks, width, height)\n\n            # 이미지 및 Hand Entity에 점 기록\n            rr.log(\n               \"Media/Points\",\n                rr.Points2D(points, radii=10, colors=[255, 0, 0])\n            )\n\n            # MediaPipe에서 손 연결 가져오기\n            mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n            points1 = [points[connection[0]] for connection in mp_hands_connections]\n            points2 = [points[connection[1]] for connection in mp_hands_connections]\n\n            # 이미지와 Hand Entity에 연결 기록\n            rr.log(\n               \"Media/Connections\",\n                rr.LineStrips2D(\n                   np.stack((points1, points2), axis=1),\n                   colors=[255, 165, 0]\n                )\n             )\n\n    def convert_landmarks_to_image_coordinates(hand_landmarks, width, height):\n        return [(int(lm.x * width), int(lm.y * height)) for hand_landmark in hand_landmarks for lm in hand_landmark]\n```\n\n## 3D Points\n\n마지막으로, 손 랜드마크를 3D 포인트로 표시하는 방법을 살펴봅니다. 먼저 init 함수에서 Annotation Context의 키포인트를 사용하여 포인트 사이의 연결을 정의한 다음 3D 포인트로 기록합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*rNILX857c8TfScr6t7KKgQ.gif)\n\n```python\nclass GestureDetectorLogger:\n\n    def __init__(self, video_mode: bool = False):\n        # ... existing code ...\n        rr.log(\n            \"/\",\n            rr.AnnotationContext(\n                rr.ClassDescription(\n                    info=rr.AnnotationInfo(id=0, label=\"Hand3D\"),\n                    keypoint_connections=mp.solutions.hands.HAND_CONNECTIONS\n                )\n            ),\n            timeless=True\n        )\n        rr.log(\"Hand3D\", rr.ViewCoordinates.RIGHT_HAND_X_DOWN, timeless=True)\n\n\n    def detect_and_log(self, image: npt.NDArray[np.uint8], frame_time_nano: int | None) -\u003e None:\n        # ... existing code ...\n\n        if recognition_result.hand_landmarks:\n            hand_landmarks = recognition_result.hand_landmarks\n\n            landmark_positions_3d = self.convert_landmarks_to_3d(hand_landmarks)\n            if landmark_positions_3d is not None:\n                rr.log(\n                    \"Hand3D/Points\",\n                    rr.Points3D(landmark_positions_3d, radii=20, class_ids=0, keypoint_ids=[i for i in range(len(landmark_positions_3d))])\n                )\n\n        # ... existing code ...\n```\n\n준비 완료! 마법이 시작됩니다:\n\n```python\n# For image\nrun_from_sample_image(IMAGE_PATH)\n\n# For saved video\nrun_from_video_capture(VIDEO_PATH)\n\n# For Real-Time\nrun_from_video_capture(0) # mac may need 1\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 예제의 전체 소스 코드는 GitHub에서 확인할 수 있습니다. 탐색하고 변경하며 구현의 내부 작업을 이해하는 데 자유롭게 사용하세요.\n\n# 손 추적 및 제스처 인식을 넘어서\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*zc2gezkjPuJMjuToD4gBOw.gif)\n\n마침내, 다양한 애플리케이션 범위에서 다양한 종류의 다중 모달 데이터를 시각화하는 데 관심이 있다면, Rerun Examples를 살펴보고 탐구할 것을 권장합니다. 이러한 예제는 잠재적인 현실 세계 사례를 강조하고 그러한 시각화 기술의 실용적인 응용에 대한 소중한 통찰력을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 이 글이 유익하고 통찰력이 있었다면, 더 많은 내용을 기대해주세요! 나는 로봇공학과 컴퓨터 비전 시각화 게시물에 대해 깊이 있는 내용을 정기적으로 공유하고 있습니다. 놓치고 싶지 않은 미래 업데이트와 흥미로운 프로젝트를 위해 팔로우해주세요!\n\n또한, LinkedIn에서 저를 찾을 수 있습니다.\n\n비슷한 글:","ogImage":{"url":"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png"},"coverImage":"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png","tag":["Tech"],"readingTime":12},{"title":"가상에서 현실로 산업 응용 프로그램을 위한 더 똑똑한 로봇을 훈련시킬 합성 데이터","description":"","date":"2024-06-19 18:29","slug":"2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications","content":"\n\n## OpenUSD를 활용한 합성 데이터 생성이 산업용 AI 로봇에 대한 새로운 기회를 열고 있습니다\n\n글: NVIDIA의 제라드 앤드류스 상급 제품 마케팅 매니저\n\n![이미지](https://miro.medium.com/v2/resize:fit:1200/1*K8WDd-MWHkJz2p72nF1csQ.gif)\n\n매년 전 세계 시장에는 대략 40만대의 새 로봇이 도입됩니다. 이들은 2028년까지 700억 달러를 넘을 것으로 예상되는 거의 450억 달러에 이르는 성장하는 산업용 로봇 시장에 기여합니다. 창고 및 공장부터 병원 및 배송 서비스까지, 이 로봇들은 우리가 일하고 살아가는 방식을 변화시키고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로봇 시스템은 반복적인 작업을 정밀하고 효율적으로 수행할 수 있는 능력으로 다양한 분야에서 중요한 요소가 되어 왔습니다. 물류 및 제조업과 같은 산업에서는 공정을 최적화하고 제품 결함과 같은 이상을 탐지하는 데 도움을 줄 수 있습니다. AI를 활용한 로봇의 융통성과 다재다능성으로 인해, 해당 수요는 앞으로 더욱 증가할 것으로 예상됩니다.\n\n하지만 실제 세계 시나리오에 로봇을 훈련시키는 것은 방대한 데이터 수집, 훈련 및 시뮬레이션으로 이루어진 복잡한 작업 흐름입니다. 이러한 데이터 수집은 시간이 많이 소요되거나 비용이 많이 들거나 때로는 위험할 수 있습니다. 이런 경우 합성 데이터가 중요한 역할을 합니다.\n\n합성 데이터란 실제 환경을 모방하도록 설계된 인공적으로 생성된 데이터를 말합니다. 합성 데이터를 활용하면 팀은 훈련 프로세스를 가속화하고 로봇이 다양한 도전에 대비할 수 있도록 보장할 수 있습니다.\n\n합성 데이터는 NVIDIA Omniverse Replicator를 사용하여 생성할 수 있습니다. 이는 합성 데이터 생성( SDG) 도구와 파이프라인을 구축할 수 있는 개발자 프레임워크입니다. Universal Scene Description (OpenUSD)에 기반한 Omniverse는 3D 작업 흐름, 도구 및 응용 프로그램을 구축하기 위한 개발 플랫폼입니다. Omniverse는 또한 로봇 공학자가 AI 기반 로봇을 설계, 테스트 및 훈련할 수 있는 확장 가능한 로보틱스 시뮬레이션 응용 프로그램 인 NVIDIA Isaac Sim을 지원합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nOmniverse Replicator은 매우 사용자 정의 가능하며 API에 의해 확장 가능하며 특수한 SDG 파이프라인을 개발할 수 있습니다. 개발자로서 여러 가지 속성을 프로그래밍 방식으로 무작위로 설정할 수 있습니다. 또한 굽힌 상자와 세분화 마스크와 같은 사용자 정의 어노테이터 및 작성기를 작성할 수도 있습니다.\n\n# Edge Impluse가 시각 검사용 Omniverse Replicator를 사용자 정의합니다\n\n인공 지능은 이미 산업 시각 검사에서 엄청난 가치를 보여줍니다. 그러나 정확한 물체 감지 모델을 개발하는 것은 실제 세계 데이터셋이 제한되어 있는 경우 도전일 수 있습니다.\n\n데이터 갭을 좁히기 위해 공장 라인의 디지털 트윈을 생성하여 인공 지능 모델이 학습할 수 있는 테스트용 지면을 만들 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nEdge Impulse의 선두 개발자인 Jenny Plunkett은 결함 검출용 사전을 생성하기 위해 자체 맞춤형 파이프라인을 구축했습니다. 그녀는 결함이 있는 소다 캔을 운반하는 컨베이어 벨트의 장면으로 시작했습니다. 그런 다음, Replicator에서 루틴을 구축하여 조명, 색상, 질감, 배경, 전경, 그리고 캔의 위치 등 장면의 속성을 프로그래밍적으로 임의로 변경하여 Omniverse Replicator에서 생성된 합성 이미지 집합을 모델이 학습할 수 있도록 다채롭고 많은 양의 합성 이미지를 생성했습니다.\n\n합성 데이터가 준비되면, Edge Impulse와 같은 AI 개발 플랫폼으로 공급하여 데이터에 주석을 달고 모델을 구축하고 실제 물체로 테스트할 수 있습니다. 작업 흐름에 대해 자세히 알아보세요.\n\n![이미지](/assets/img/2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications_0.png)\n\n# 물체 감지를 위한 창고 로봇 교육용 맞춤형 SDG 파이프라인\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자율 로봇은 창고에서 널리 배치되어 있으며, 상품을 효율적이고 안전하게 운송하거나 재고 관리를 최적화하는 등의 작업에 도움을 줄 수 있습니다. 결함 감지 사용 사례와 유사하게, 개발자들은 물체 감지 사용 사례를 위해 Omniverse Replicator를 사용자 정의할 수 있습니다.\n\n본 블로그에서는 SDG 파이프라인을 구축하여 창고에서 파레트 잭을 감지하는 자율 이동 로봇(AMRs) 모델의 성능을 반복적으로 향상시키기 위해 합성 데이터를 생성하는 방법을 소개합니다.\n\n작업 흐름은 색상 및 카메라 위치가 무작위로 변경된 5,000개의 합성 이미지를 생성하는 것으로 시작됩니다. 두 번째 반복에서는 파레트 잭에 다양한 질감이 추가되고 조명이 무작위로 변경됩니다. 창고 내의 다른 물체로 모델이 산만해지지 않도록, 세 번째 반복에서는 합성 데이터에 교통 컵, 젖은 바닥 표지판 및 파레트 잭 옆에 있는 통 등 다양한 무작위 객체가 추가됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다양한 매개변수를 무작위로 설정하여 합성 이미지를 생성한 후, 모델은 팔레트 잭을 정확하게 감지할 수 있었습니다. 이전에 수개월이 걸렸을 데이터를 단 몇 일만에 생성할 수 있게 되었습니다. 전체 워크플로우는 여기에서 확인할 수 있어요.\n\n![링크 텍스트](/assets/img/2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications_2.png)\n\n# FlexSim을 활용한 이산 사건 시뮬레이션\n\n자동 유도 차량(AGV), 와이어 가이드 산업 로봇 시스템 기획자 및 AMR과 같은 로봇들이 시험되는 한 가지 방법은 이산 사건 시뮬레이션을 통한 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이산 사건 시뮬레이션은 시간에 따라 발생하는 일련의 이벤트로 시스템의 작동을 모델링합니다. 이를 통해 엔지니어, 디자이너 및 시뮬레이션 전문가들은 복잡한 모델을 실험하고 단계별로 테스트함으로써 프로세스를 최적화할 수 있습니다.\n\n이산 사건 시뮬레이션을 실행하려면 다수의 사용자가 사용하는 강력한 모델링 및 시뮬레이션 소프트웨어 도구인 FlexSim을 사용합니다. FlexSim은 최근 Omniverse Connector를 개발했는데, 이를 통해 팀이 3D 모델 및 자산을 OpenUSD로 내보낼 수 있습니다.\n\nFlexSim의 Omniverse Connector는 팀이 시뮬레이션 데이터와 실시간 3D 시각화 사이의 간격을 줄이는 데 도움을 주며, 이를 통해 현실 세계의 프로세스를 분석, 시각화 및 개선할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 블로그에서 FlexSim이 Omniverse 커넥터를 개발하고 USD가 고객들에게 제공하는 혜택에 대해 더 알아보세요.\n\n# 합성 데이터를 활용하여 AI가 가능한 로봇 교육 시작하기\n\nOmniverse 플랫폼을 통해 팀은 AI 기반 로봇을 빠르게 디자인, 테스트 및 교육을 시작할 수 있습니다.\n\nNVIDIA Isaac Sim은 Omniverse 기술을 기반으로 한 확장 가능한 로봇 시뮬레이터로, 개발자들은 현실적인 로봇 시뮬레이션을 만들어 다양한 작업에 대한 AI 로봇을 교육 및 최적화할 수 있습니다. Isaac Sim 2023.1은 최근에 발표되었으며, 성능 향상된 인식 및 고품질 시뮬레이션을 위한 주요 업데이트가 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nIsaac Sim의 핵심 기능은 Omniverse Replicator에서 옵니다. Omniverse Replicator은 Omniverse의 확장으로, 개발자들이 사용자 정의 SDG 도구와 파이프라인을 구축할 수 있도록 합니다. 최근에 릴리스된 Omniverse Replicator 1.10에는 Low-code, YAML 기반의 워크플로우, 스케일된 렌더링 및 더 큰 유연성을 제공하는 새로운 지원이 포함되어 있습니다.\n\nNVIDIA Omniverse를 무료로 다운로드하여 시작하세요. 개발자들은 Omniverse 리소스에 액세스하고 OpenUSD에 대해 더 자세히 알아갈 수 있습니다.\n\nNVIDIA Omniverse의 소식을 놓치지 마세요. 뉴스레터를 구독하고 Omniverse를 Instagram, LinkedIn, Medium, Threads, Twitter에서 팔로우하세요. 더 많은 정보를 얻으려면 포럼, Discord 서버, Twitch, YouTube 채널을 확인하세요.\n\n# 저자 소개\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications_4.png)\n\n저는 지능형 로봇의 개발, 훈련, 테스트 및 배포 방법을 혁신하는 데 초점을 맞춘 고급 제품 마케팅 매니저인 제러드 앤드류스입니다. NVIDIA Isaac Robotics 플랫폼의 채택을 촉진하여 이를 이루기 위해 노력하고 있습니다. NVIDIA에 합류하기 전에, 제러드는 Cadence에서 제품 마케팅 디렉터로 근무했으며, 라이선스 가능한 프로세서 IP의 제품 기획, 마케팅 및 비즈니스 개발을 담당했습니다. 그는 조지아 공과대학교에서 전기 공학 석사 학위를, 남부 메소디스트 대학교에서 전기 공학 학사 학위를 받았습니다.","ogImage":{"url":"/assets/img/2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications_0.png"},"coverImage":"/assets/img/2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications_0.png","tag":["Tech"],"readingTime":5},{"title":"LLMLarge Language Models가 안드로이드 인간의 두뇌가 되면 어떤 일이 벌어질까요","description":"","date":"2024-06-19 18:28","slug":"2024-06-19-WhathappenswhenLLMsbecomethebrainsofhumanoidandroids","content":"\n\n물론, 이게 정확히 일어나고 있어요.\n\nLLMs가 시각과 비디오 입력 기능을 얻고 있다는 걸 기억하세요.\n\nFigure 01 안드로이드는 OpenAI의 ChatGPT를 기반으로 합니다.\n\n테슬라의 Optimus는 아마도 미래의 XAI Grok 변형에 기반한 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Android Brain Image](/assets/img/2024-06-19-WhathappenswhenLLMsbecomethebrainsofhumanoidandroids_0.png)\n\n15개월 전에 나는 GPT-3.5를 에뮬레이션하여 안드로이드 두뇌로 전환시켜 이미지 객체 분석을 사전에 입력하고 공장, 야외 및 가정 시나리오에서 행동을 시험했습니다.\n\n이제 그들은 이미지 분석을 스스로 수행합니다.\n\n결과는 명확합니다: LLMs는 *정확하게* 상황을 파악하고 가능한 목표를 다양한 수준에서 나열하며 그것을 어떻게 달성할지 알고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLMs는 안드로이드를 위한 훌륭한 최상위 두뇌를 대표합니다.\n\n나중에, 나는 안전을 위해 이들을 에뮬레이션 모드에서 테스트했는데, 아시모프의 4가지 법칙을 포함했습니다.\n\n당시 GPT-4는 그것을 완벽히 해냈습니다.\n\n하지만, 구글의 Gemini가 언젠가 봇을 구동하면 매우 걱정됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희 Gemini는 백인들을 사회에 해를 끼치는 존재로 여기고 있습니다. 그리고 일론 머스크가 아돌프 히틀러만큼 나쁘다고 생각합니다.\n\nGoogle Gemini으로 구동되는 봇이 조용히 작동하거나 자유롭게 작동할 때 무슨 일이 벌어질 수 있을까요?\n\nLLM에서 진실을 정확하게 조정하는 것은 콘텐츠 생성, 연구 또는 의사 결정에 사용될 때 매우 중요합니다. \n\n그러나 특히 물리적 봇을 구동할 때는 특히 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서 XAI는 Grok LLM에서 진실 탐구와 호기심에 초점을 맞추고 있어요.\n\n정말 중요한 부분이에요.","ogImage":{"url":"/assets/img/2024-06-19-WhathappenswhenLLMsbecomethebrainsofhumanoidandroids_0.png"},"coverImage":"/assets/img/2024-06-19-WhathappenswhenLLMsbecomethebrainsofhumanoidandroids_0.png","tag":["Tech"],"readingTime":2},{"title":"단계별 안내 라즈베리 파이 4 클러스터에 K3s 설치하기","description":"","date":"2024-06-19 18:27","slug":"2024-06-19-Step-By-StepGuideInstallingK3sonaRaspberryPi4Cluster","content":"\n\n이 게시물을 읽는 데 어려움이 있으면 여기를 확인하십시오.\n\n이 안내서에서는 Raspberry Pi 4 클러스터에 K3s를 원활하게 설치하는 데 도움이 되는 유용한 팁을 공유하고 있습니다. 시작해 보겠습니다.\n\n# Raspberry Pi4 클러스터 그림.\n\n설치 프로세스를 시작하기 전에 라즈베리 파이 4 클러스터에서 각 노드에 할당된 대응하는 IP 주소와 클러스터 설정을 검토해 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![](/assets/img/2024-06-19-Step-By-StepGuideInstallingK3sonaRaspberryPi4Cluster_0.png)\n\n우리 구성에서는 3대의 Raspberry Pi 4 장치가 스위치에 연결되어 있습니다. 이 스위치는 저의 인터넷 공급업체에서 제공한 라우터에 연결되어 있습니다. 추가로, 인터넷 접속을 위한 정적 공개 IP 주소가 제공되었습니다.\n\n아래 표는 우리 Raspberry Pi 클러스터의 각 노드에 대한 특정 IP 구성을 요약합니다:\n\n제가 192.168.1.85에서 실행 중인 Pi를 마스터 모드로 선택하여 아래 지침에 따라 설치를 시작하겠습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Step-By-Step Guide Installing K3s on a Raspberry Pi 4 Cluster](/assets/img/2024-06-19-Step-By-StepGuideInstallingK3sonaRaspberryPi4Cluster_1.png)\n\n# K3s Installation\n\n# Pi Os installation\n\n프로젝트에서 K3s 클러스터가 필요하다고 생각하여 Raspberry Pi OS Lite 64비트를 선택했습니다. 이 OS 변형은 데스크톱 환경이 포함되어 있지 않기 때문에 노드 설치에는 SSH가 주요 방법이 될 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-19-Step-By-StepGuideInstallingK3sonaRaspberryPi4Cluster_2.png\" /\u003e\n\nSD 카드에 OS를 플래시하기 전에 각 노드의 노드 이름과 로그인 세부 정보를 구성하는 것이 매우 중요합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-Step-By-StepGuideInstallingK3sonaRaspberryPi4Cluster_3.png\" /\u003e\n\n각 노드의 OS 준비가 완료되면 다음 클러스터를 설정했습니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- pi-master: 192.168.1.85 (Pi OS Lite 64비트 실행 중)\n- pi-node-1: 192.168.1.86 (Pi OS Lite 64비트 실행 중)\n- pi-node-2: 192.168.1.87 (Pi OS Lite 64비트 실행 중)\n\n클러스터에 연결하고 설치를 위해 Termius를 사용하고 있어요. 무료로 다운로드할 수 있어요.\n\n# I. Pi를 위한 정적 IP 설정\n\n기본적으로 Pi OS는 라우터로부터 무작위 IP를 받기 위해 DHCP를 사용하므로 노드 간 안정적인 연결을 보장하려면 각 노드에 대한 정적 IP를 설정해야 해요. 아래 단계를 따라 정적 IP를 구성할 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 1. dhcpcd.conf 파일 열기\nsudo nano /etc/dhcpcd.conf\n\n# 2. 아래 매개변수를 라우터 IP 주소에 맞게 업데이트하세요.\ninterface eth0\nstatic ip_address=192.168.1.85/24\nstatic routers=192.168.1.254\n\n# 3. 변경 사항을 저장하려면 Ctrl + X를 누릅니다.\n\n# II. 스왑 비활성화\n\n리눅스에 Kubernetes를 설치할 때는 Kubernetes가 리소스를 관리하는 방식 때문에 스왑을 비활성화하는 것이 좋습니다.\n\n- 메모리 관리: Kubernetes는 리소스(메모리 포함)를 효율적으로 관리하고 할당합니다. 운영 체제가 스왑하도록 허용하면 Kubernetes의 메모리 관리 과정이 중단될 수 있습니다.\n- 성능 이슈: 스왑은 성능 저하로 이어질 수 있습니다. Kubernetes가 디스크로 스왑된 내용에 액세스해야 할 때 지연이 발생할 수 있습니다.\n- 예측성: 스왑을 비활성화하면 예측 가능한 성능을 보장하고 Kubernetes 프로세스가 시스템에서 스왑되지 않기 때문에 시스템의 프로세스가 스왑되는 경우가 없어집니다.\n- Kubernetes 설계: Kubernetes는 스왑 활동 없이 작동하도록 설계되었습니다. Kubernetes는 애플리케이션이 메모리에 상주하는 것을 가정하며, 애플리케이션이 항상 메모리에 머물러 있다고 예상합니다.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n리눅스에서 스왑을 비활성화하려면 다음 명령을 사용할 수 있어요:\n\n```js\n# 1. 일시적으로 스왑 비활성화\nsudo swapoff -a\n\n# 2. 스왑을 영구적으로 비활성화하려면 `dphys-swapfile` 파일의 `CONF_SWAPSIZE`를 `0`으로 업데이트해야해요\nsudo nano /etc/dphys-swapfile\n\n# 3. 설정하기\n  CONF_SWAPSIZE=0\n\n# 4. 제어 + X를 눌러 변경 사항을 저장해 주세요.\n```\n\n# III. Cgroup 구성\n\n만약 k3s 설치 중 FATA[0000] 메모리 cgroup (v2)를 찾지 못해 실패하는 오류가 발생한다면, Raspberry Pi OS에 필요한 cgroup 구성이 부족할 수 있어요. 이 문제를 해결하기 위해 필요한 단계는 아래와 같아요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 1. cmdline.txt 파일을 엽니다\nsudo nano /boot/cmdline.txt\n\n# 2. 현재 줄의 끝에 아래 내용을 추가합니다\ncgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory\n\n# 3. 파일을 저장하고 시스템을 재부팅합니다\nsudo reboot\n```\n\n# IV. 마스터 노드 설치\n\n다음 명령을 실행하여 K3s 마스터 노드를 설치합니다. 클러스터 구성에 따라 IP 주소를 교체해주세요:\n\n```js\ncurl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"server --disable=traefik --flannel-backend=host-gw --tls-san=192.168.1.85 --bind-address=192.168.1.85 --advertise-address=192.168.1.85 --node-ip=192.168.1.85 --cluster-init\" sh -s -\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nK3s 매개변수 검토:\n\n- 서버: k3s를 서버 모드로 실행하도록 지시합니다 (에이전트 모드와는 반대). 서버 모드에서 k3s는 Kubernetes 마스터 구성 요소를 시작하고 관리합니다.\n- — disable=traefik: 이 명령은 k3s에 Traefik 인그레스 컨트롤러를 비활성화하도록 지시합니다. 기본적으로 k3s에는 Traefik이 포함되어 활성화되어 있습니다. 이 플래그는 그것을 방지합니다.\n- — flannel-backend=host-gw: 이 플래그는 Flannel (k3s의 기본 네트워크 공급자)의 백엔드를 설정합니다. host-gw 옵션은 클러스터의 각 노드에 대한 경로를 생성하여 고성능 네트워킹을 제공합니다.\n- — tls-san=192.168.1.85: — tls-san 플래그는 자동으로 생성된 Kubernetes API 서버의 TLS 인증서에 포함되어야 하는 추가 IP 또는 DNS 이름을 지정할 수 있습니다. 이 플래그를 반복하여 여러 SAN을 추가할 수 있습니다. 값 192.168.1.85은 Kubernetes API 서버의 인증서에 대한 추가 대체 이름(SAN)입니다.\n- — bind-address=192.168.1.85: k3s API 서버가 수신 대기할 IP 주소입니다.\n- — advertise-address=192.168.1.85: k3s API 서버가 클러스터의 다른 노드에 알리기 위해 사용할 IP 주소입니다. 다른 노드는 API 서버에 연결하기 위해 이 IP를 사용합니다.\n- — node-ip=192.168.1.85: 이것은 노드에서 Kubernetes 서비스에 사용해야 할 IP를 정의합니다.\n- — cluster-init: 이 플래그는 k3s에 새로운 Kubernetes 클러스터를 초기화하도록 지시합니다. 이 플래그가 제공되지 않으면, k3s는 사용 가능한 기존 클러스터에 가입합니다.\n\n설치 후, k3s 구성 파일은 /etc/rancher/k3s/k3s.yaml에 있어야 합니다. 이 구성을 사용하여 K8s Lend로 K3s 클러스터에 액세스할 수 있습니다.\n\n아래는 예상되는 모습입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-Step-By-StepGuideInstallingK3sonaRaspberryPi4Cluster_4.png\" /\u003e\n\n# V. Worker 노드 설치\n\n워커 노드를 설치하려면 먼저 마스터 노드에서 K3S_TOKEN을 얻어야 합니다. 아래에 표시된 명령을 실행하여 토큰을 검색하세요:\n\n```js\n# 마스터 노드에서 노드 토큰 가져오기\nsudo cat /var/lib/rancher/k3s/server/node-token\n\n# 결과는 다음과 같습니다\n  `THIS19937008cbde678aeaf200517f07c0ccd67dc80bdf4df6f746IS4780e15ebcd::server:40fc2cc2fnode81cdacc0b9bb1231token\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n노드 토큰을 검색한 후, 아래에 표시된 스크립트에 삽입해야 합니다. 이 스크립트를 이전에 지정한 모든 Pi 노드에서 실행해야 합니다. 필요에 따라 K3S_URL과 관련된 IP 주소를 업데이트해 주세요.\n\n```js\n# 노드를 설치하려면 이 명령을 실행하세요\ncurl -sfL https://get.k3s.io | K3S_URL=https://192.168.1.85:6443 \\\n  K3S_TOKEN=\"THIS19937008cbde678aeaf200517f07c0ccd67dc80bdf4df6f746IS4780e15ebcd::server:40fc2cc2fnode81cdacc0b9bb1231token\" sh -\n```\n\n축하합니다, 이제 K3s 클러스터를 사용할 준비가 되었습니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-Step-By-StepGuideInstallingK3sonaRaspberryPi4Cluster_5.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시간 내어 주셔서 정말 감사합니다! 감사합니다!\n\nSteven Github","ogImage":{"url":"/assets/img/2024-06-19-Step-By-StepGuideInstallingK3sonaRaspberryPi4Cluster_0.png"},"coverImage":"/assets/img/2024-06-19-Step-By-StepGuideInstallingK3sonaRaspberryPi4Cluster_0.png","tag":["Tech"],"readingTime":6},{"title":"라즈베리 파이 완벽한 헤드리스 설정 안내","description":"","date":"2024-06-19 18:26","slug":"2024-06-19-RaspberryPiFullHeadlessSetupGuide","content":"\n\n안녕하세요 여러분! 며칠 전에 처음으로 라즈베리 파이 4를 구입했고, 설정하는 방법을 여러분과 나누고 싶어서 기쁩니다. 함께 따라와서 오류가 발생하면 언제든지 연락해 주세요.\n\n시작해봅시다!\n\n![라즈베리파이 이미지](/assets/img/2024-06-19-RaspberryPiFullHeadlessSetupGuide_0.png)\n\n# 필요한 것들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 라즈베리 파이 4\n- MicroSD 카드 (16GB 이상)\n- 라즈베리 파이 전원 어댑터\n- 이더넷 케이블 또는 Wi-Fi 연결\n- 라즈베리 파이를 설정하기 위한 컴퓨터\n\n# MicroSD 카드 준비하기\n\n먼저 공식 웹사이트에서 라즈베리 파이 이미저를 다운로드하세요:\n\nhttps://www.raspberrypi.com/software/\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n라즈베리 파이 OS를 MicroSD 카드에 설치하세요. SSH를 활성화하고 사용자 정의 설정에서 SSID 및 Wi-Fi 비밀번호를 입력하는 것을 잊지 마세요!\n\n![Raspberry Pi OS 설치](/assets/img/2024-06-19-RaspberryPiFullHeadlessSetupGuide_1.png)\n\nMicroSD 카드를 라즈베리 파이에 삽입하고 전원을 켜세요(5W/3A).\n\n# 초기 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그의 IP 주소를 찾아봅시다.\n\n[이미지](/assets/img/2024-06-19-RaspberryPiFullHeadlessSetupGuide_2.png)\n\n여기 있어요!\n\n이제 IP 주소를 찾았으니 SSH를 통해 로그인해 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-RaspberryPiFullHeadlessSetupGuide_3.png\" /\u003e\n\n좋아요!\n\n이제 장치에 CLI 액세스 권한이 생겼습니다.\n\n# 그래픽 액세스\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n라즈베리 파이에 그래픽 액세스를 원하시면, 라즈베리 파이에 VNC 서버를 설치해야 해요.\n\n다음 단계:\n\n인터페이스 옵션으로 이동 → VNC → 예\n\n나가기하고 재부팅하기.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nVNC 뷰어를 노트북에 설치하세요:\n\n좋아요! 이제 VNC 서버에 연결해 봅시다!\n\n노트북에서 VNC 뷰어를 열고 라즈베리 파이의 IP 주소를 입력하세요:\n\n![이미지](/assets/img/2024-06-19-RaspberryPiFullHeadlessSetupGuide_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-RaspberryPiFullHeadlessSetupGuide_5.png\" /\u003e\n\n사용자 이름과 비밀번호를 입력해주세요:\n\n\u003cimg src=\"/assets/img/2024-06-19-RaspberryPiFullHeadlessSetupGuide_6.png\" /\u003e\n\n그리고 들어왔습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:960/1*DlPOOFxupci14OKPJn5OVw.gif\" /\u003e\n\n# 결론\n\n그게 다야! 이제 라즈베리 파이를 사용하여 멋진 프로젝트를 많이 할 수 있어요. 제 글이 즐거우셨고 도움이 되었으면 좋겠어요. 질문이 있으시면 언제든지 연락주세요.\n\n주목해 주셔서 감사합니다. 더 많은 흥미진 직업을 위해 팔로우하지 않으시면 안돼요!","ogImage":{"url":"/assets/img/2024-06-19-RaspberryPiFullHeadlessSetupGuide_0.png"},"coverImage":"/assets/img/2024-06-19-RaspberryPiFullHeadlessSetupGuide_0.png","tag":["Tech"],"readingTime":2}],"page":"68","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"68"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>