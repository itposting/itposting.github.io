<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/68" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/68" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_buildManifest.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="파이토치PyTorch를 사용하여 처음부터 Large Language Model LLM을 만들어 보세요" href="/post/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="파이토치PyTorch를 사용하여 처음부터 Large Language Model LLM을 만들어 보세요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="파이토치PyTorch를 사용하여 처음부터 Large Language Model LLM을 만들어 보세요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">파이토치PyTorch를 사용하여 처음부터 Large Language Model LLM을 만들어 보세요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">40<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="혁신 수용하기 로보틱스로 미래를 발견하다" href="/post/2024-06-19-TitleEmbracingInnovationUnveilingtheFuturewithRobotics"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="혁신 수용하기 로보틱스로 미래를 발견하다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-TitleEmbracingInnovationUnveilingtheFuturewithRobotics_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="혁신 수용하기 로보틱스로 미래를 발견하다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">혁신 수용하기 로보틱스로 미래를 발견하다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="실패로부터 배운 것 내 가장 큰 AI 프로젝트 실수와 교훈들" href="/post/2024-06-19-LearningfromFailuresMyBiggestAIProjectMistakesandTakeaways"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="실패로부터 배운 것 내 가장 큰 AI 프로젝트 실수와 교훈들" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-LearningfromFailuresMyBiggestAIProjectMistakesandTakeaways_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="실패로부터 배운 것 내 가장 큰 AI 프로젝트 실수와 교훈들" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">실패로부터 배운 것 내 가장 큰 AI 프로젝트 실수와 교훈들</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="로봇들도 우리처럼 말할 수 있지만, 그렇다고 해서 그들이 인간이 되는 것은 아닙니다" href="/post/2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="로봇들도 우리처럼 말할 수 있지만, 그렇다고 해서 그들이 인간이 되는 것은 아닙니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="로봇들도 우리처럼 말할 수 있지만, 그렇다고 해서 그들이 인간이 되는 것은 아닙니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">로봇들도 우리처럼 말할 수 있지만, 그렇다고 해서 그들이 인간이 되는 것은 아닙니다</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AI와 로봇의 미래 모라벡의 역설 너머" href="/post/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AI와 로봇의 미래 모라벡의 역설 너머" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AI와 로봇의 미래 모라벡의 역설 너머" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">AI와 로봇의 미래 모라벡의 역설 너머</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="SLAM 개요" href="/post/2024-06-19-OverviewofSLAM"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="SLAM 개요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-OverviewofSLAM_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="SLAM 개요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">SLAM 개요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="로봇들이 제멋대로 현재와 미래에 우려할 이유가 있을까요" href="/post/2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="로봇들이 제멋대로 현재와 미래에 우려할 이유가 있을까요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="로봇들이 제멋대로 현재와 미래에 우려할 이유가 있을까요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">로봇들이 제멋대로 현재와 미래에 우려할 이유가 있을까요</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="사냥 첫 번째 이야기" href="/post/2024-06-19-TheHuntPartOne"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="사냥 첫 번째 이야기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-TheHuntPartOne_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="사냥 첫 번째 이야기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">사냥 첫 번째 이야기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스" href="/post/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="가상에서 현실로 산업 응용 프로그램을 위한 더 똑똑한 로봇을 훈련시킬 합성 데이터" href="/post/2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="가상에서 현실로 산업 응용 프로그램을 위한 더 똑똑한 로봇을 훈련시킬 합성 데이터" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="가상에서 현실로 산업 응용 프로그램을 위한 더 똑똑한 로봇을 훈련시킬 합성 데이터" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">가상에서 현실로 산업 응용 프로그램을 위한 더 똑똑한 로봇을 훈련시킬 합성 데이터</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/61">61</a><a class="link" href="/posts/62">62</a><a class="link" href="/posts/63">63</a><a class="link" href="/posts/64">64</a><a class="link" href="/posts/65">65</a><a class="link" href="/posts/66">66</a><a class="link" href="/posts/67">67</a><a class="link posts_-active__YVJEi" href="/posts/68">68</a><a class="link" href="/posts/69">69</a><a class="link" href="/posts/70">70</a><a class="link" href="/posts/71">71</a><a class="link" href="/posts/72">72</a><a class="link" href="/posts/73">73</a><a class="link" href="/posts/74">74</a><a class="link" href="/posts/75">75</a><a class="link" href="/posts/76">76</a><a class="link" href="/posts/77">77</a><a class="link" href="/posts/78">78</a><a class="link" href="/posts/79">79</a><a class="link" href="/posts/80">80</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"파이토치PyTorch를 사용하여 처음부터 Large Language Model LLM을 만들어 보세요","description":"","date":"2024-06-19 18:43","slug":"2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch","content":"\n\n## LLM 만들고 트레이닝하는 단계별 가이드입니다. 이 모델의 목표는 영어를 말레이어로 번역하는 것입니다.\n\n![이미지](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_0.png)\n\n이 글을 마치면 어떤 결과를 얻을 수 있을까요? 직접 코딩하면서 Large Language Model (LLM)을 만들고 트레이닝할 수 있게 될 거에요. 영어를 말레이어로 번역하는 LLM을 만들지만, 다른 언어 번역 작업을 위해 이 LLM 아키텍처를 쉽게 수정할 수 있습니다.\n\nLLM은 ChatGPT, Gemini, MetaAI, Mistral AI 등과 같은 인기 있는 AI 챗봇의 핵심 기반이 됩니다. 모든 LLM의 핵심에는 Transformer라는 아키텍처가 있습니다. 따라서, 먼저 유명한 논문 \"Attention is all you need\"을 바탕으로 Transformer 아키텍처를 구축할 것입니다 - https://arxiv.org/abs/1706.03762.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![transformer_step_by_step](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_1.png)\n\n먼저, 우리는 트랜스포머 모델의 모든 구성 요소를 블록 단위로 구축할 것입니다. 그런 다음, 모든 블록을 조합하여 모델을 구축할 것입니다. 그 후에는 Hugging Face 데이터셋에서 얻을 데이터셋으로 모델을 훈련하고 유효성을 검사할 것입니다. 마지막으로 새 번역 텍스트 데이터에 대한 번역을 수행하여 모델을 테스트할 것입니다.\n\n중요 사항: 저는 트랜스포머 아키텍처의 모든 구성 요소를 단계별로 코딩하고 '무엇, 왜, 어떻게'에 대한 개념에 대한 필요한 설명을 제공할 것입니다. 또한 설명이 필요한 특정 코드 라인에 대해 주석을 제공할 것입니다. 이렇게 하면 직접 코딩하면서 전체 워크플로에 연결할 수 있을 것이라고 믿습니다.\n\n![transformer_architecture](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n함께 코딩해요!\n\n## 단계 1: 데이터셋 로드\n\nLLM 모델이 영어에서 말레이어로 번역하는 작업을 할 수 있도록 하려면 소스(영어)와 대상(말레이어) 언어 쌍이 있는 데이터셋을 사용해야 합니다. 따라서, Huggingface에서 \"Helsinki-NLP/opus-100\"라는 데이터셋을 사용할 것입니다. 이 데이터셋은 1백만 개의 영어-말레이어 훈련 데이터셋을 가지고 있어서 좋은 정확도를 얻기에 충분하며, 검증 및 테스트 데이터셋에 각각 2,000개의 데이터가 있습니다. 데이터는 이미 사전 분할되어 있어서 데이터셋을 다시 분할할 필요가 없습니다.\n\n\n# 필요한 라이브러리 가져오기\n# 아직 안 했다면 (!pip install datasets, tokenizers)를 사용하여 데이터셋 및 토크나이저 라이브러리 설치하기.\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# GPU가 사용 가능하다면 \"cuda\"로 장치 값을 할당하여 GPU에서 훈련합니다. 사용할 수 없는 경우 기본값인 \"cpu\"로 되돌릴 것입니다.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n\n# Huggingface 경로에서 훈련, 검증, 테스트 데이터셋을 로드합니다.\nraw_train_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ms\", split='train')\nraw_validation_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ms\", split='validation')\nraw_test_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ms\", split='test')\n\n# 데이터셋 파일을 저장할 디렉토리 생성\nos.mkdir(\"./dataset-en\")\nos.mkdir(\"./dataset-my\")\n\n# 각 에폭 후 모델 훈련 중에 모델을 저장할 디렉토리 생성 (단계 10에서 사용).\nos.mkdir(\"./malaygpt\")\n\n# 소스 및 대상 토크나이저를 저장할 디렉토리 생성.\nos.mkdir(\"./tokenizer_en\")\nos.mkdir(\"./tokenizer_my\")\n\ndataset_en = []\ndataset_my = []\nfile_count = 1\n\n# 토크나이저를 훈련하기 위해 (단계 2에서) 훈련 데이터셋을 영어 및 말레이어로 분리합니다.\n# 각 파일에 50,000개씩 작은 데이터 파일을 만들어 dataset-en 및 dataset-my 디렉토리에 저장합니다.\nfor data in tqdm(raw_train_dataset[\"translation\"]):\n    dataset_en.append(data[\"en\"].replace('\\n', \" \"))\n    dataset_my.append(data[\"ms\"].replace('\\n', \" \"))\n    if len(dataset_en) == 50000:\n        with open(f'./dataset-en/file{file_count}.txt', 'w', encoding='utf-8') as fp:\n            fp.write('\\n'.join(dataset_en))\n            dataset_en = []\n\n        with open(f'./dataset-my/file{file_count}.txt', 'w', encoding='utf-8') as fp:\n            fp.write('\\n'.join(dataset_my))\n            dataset_my = []\n        file_count += 1\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 2: 토크나이저 생성\n\n트랜스포머 모델은 원시 텍스트를 처리하지 않으며, 숫자만 처리합니다. 따라서 원시 텍스트를 숫자로 변환하기 위해 어떤 작업을 해야 할 것입니다. 이를 위해 저희는 GPT3와 같은 모델에서 사용되는 서브워드 토크나이저인 BPE 토크나이저를 사용할 것입니다. 먼저 우리가 단계 1에서 준비한 코퍼스 데이터(이 경우 교육 데이터 셋)로 BPE 토크나이저를 먼저 학습할 것입니다. 아래 다이어그램과 같이 진행됩니다.\n\n![image](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_3.png)\n\n학습이 완료되면 토크나이저는 영어와 말레이 언어용 어휘를 생성합니다. 어휘는 코퍼스 데이터에서 고유한 토큰들의 컬렉션입니다. 번역 작업을 수행하기 때문에 두 언어에 대한 토크나이저가 필요합니다. BPE 토크나이저는 원시 텍스트를 가져와 어휘 내의 토큰들과 매핑한 후, 입력된 원시 텍스트의 각 단어에 대해 토큰을 반환합니다. 토큰은 단일 단어나 서브워드가 될 수 있습니다. 이는 다른 토크나이저에 비해 서브워드 토크나이저의 장점 중 하나입니다. 그리고 토크나이저는 그 고유한 인덱스 또는 위치 ID를 반환하고, 이는 위의 흐름에서 임베딩을 생성하는 데 추가로 사용될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 토크나이저 라이브러리 클래스 및 모듈 가져오기.\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\n# 토크나이저를 훈련시킬 학습 데이터셋 파일 경로.\npath_en = [str(file) for file in Path('./dataset-en').glob(\"**/*.txt\")]\npath_my = [str(file) for file in Path('./dataset-my').glob(\"**/*.txt\")]\n\n# [원본 언어 토크나이저(영어) 생성].\n# [UNK] - 알 수 없는 단어를 나타내는 특수 토큰 생성, [PAD] - 패딩 토큰으로 모델 간 시퀀스 길이를 일정하게 유지하기 위함.\n# [CLS] - 문장 시작을 표시하는 토큰, [SEP] - 문장 끝을 표시하는 토큰 등의 추가 특수 토큰 생성.\ntokenizer_en = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer_en = BpeTrainer(min_frequency=2, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\", \"[SEP]\", \"[MASK]\"])\n\n# 토큰을 공백을 기준으로 분리.\ntokenizer_en.pre_tokenizer = Whitespace()\n\n# 데이터셋 파일로 토크나이저 훈련.\ntokenizer_en.train(files=path_en, trainer=trainer_en)\n\n# 향후 사용을 위해 토크나이저 저장.\ntokenizer_en.save(\"./tokenizer_en/tokenizer_en.json\")\n\n# [타겟 언어 토크나이저(말레이어) 생성].\ntokenizer_my = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer_my = BpeTrainer(min_frequency=2, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\", \"[SEP]\", \"[MASK]\"])\ntokenizer_my.pre_tokenizer = Whitespace()\ntokenizer_my.train(files=path_my, trainer=trainer_my)\ntokenizer_my.save(\"./tokenizer_my/tokenizer_my.json\")\n\ntokenizer_en = Tokenizer.from_file(\"./tokenizer_en/tokenizer_en.json\")\ntokenizer_my = Tokenizer.from_file(\"./tokenizer_my/tokenizer_my.json\")\n\n# 각 토크나이저의 크기 확인.\nsource_vocab_size = tokenizer_en.get_vocab_size()\ntarget_vocab_size = tokenizer_my.get_vocab_size()\n\n# 토큰 ID 변수 정의, 모델 훈련에 사용.\nCLS_ID = torch.tensor([tokenizer_my.token_to_id(\"[CLS]\")], dtype=torch.int64).to(device)\nSEP_ID = torch.tensor([tokenizer_my.token_to_id(\"[SEP]\")], dtype=torch.int64).to(device)\nPAD_ID = torch.tensor([tokenizer_my.token_to_id(\"[PAD]\")], dtype=torch.int64).to(device)\n```\n\n## Step 3: 데이터셋 및 DataLoader 준비\n\n이 단계에서는 나중에 구축할 모델을 훈련하고 검증하기 위해 소스 언어와 타겟 언어 각각에 대한 데이터셋을 준비할 것입니다. 우리는 원시 데이터셋을 입력으로 받아 소스(토크나이저_en)와 타겟(토크나이저_my) 텍스트를 각각 인코딩하는 기능을 정의하는 클래스를 생성할 것입니다. 마지막으로, 훈련 및 검증 데이터셋을 위해 DataLoader를 생성하겠습니다. 이 DataLoader는 배치 단위로 데이터셋을 반복하며(예: 배치 크기는 10으로 설정될 수 있음), 데이터 크기와 사용 가능한 처리 능력에 따라 배치 크기를 조정할 수 있습니다.\n\n```js\n# 이 클래스는 원시 데이터셋과 max_seq_len (전체 데이터셋에서 시퀀스의 최대 길이)을 가져옵니다.\nclass EncodeDataset(Dataset):\n    def __init__(self, raw_dataset, max_seq_len):\n        super().__init__()\n        self.raw_dataset = raw_dataset\n        self.max_seq_len = max_seq_len\n    \n    def __len__(self):\n        return len(self.raw_dataset)\n\n    def __getitem__(self, index):\n        \n        # 주어진 인덱스의 원시 텍스트를 가져와 소스 및 타겟 텍스트로 분리함.\n        raw_text = self.raw_dataset[index]\n        \n        # 소스 텍스트와 타겟 텍스트를 인코딩하기 위해 소스 토크나이저(tokenizer_en) 및 타겟 토크나이저(tokenizer_my)를 사용합니다.\n        source_text_encoded = torch.tensor(tokenizer_en.encode(source_text).ids, dtype = torch.int64).to(device)    \n        target_text_encoded = torch.tensor(tokenizer_my.encode(target_text).ids, dtype = torch.int64).to(device)\n\n        # 모델 훈련을 위해 각 입력 시퀀스의 길이가 max_seq_len과 동일하도록 만들기 위해 필요한 만큼의 패딩을 추가합니다.\n        num_source_padding = self.max_seq_len - len(source_text_encoded) - 2 \n        num_target_padding = self.max_seq_len - len(target_text_encoded) - 1 \n\n        encoder_padding = torch.tensor([PAD_ID] * num_source_padding, dtype = torch.int64).to(device)\n        decoder_padding = torch.tensor([PAD_ID] * num_target_padding, dtype = torch.int64).to(device)\n\n        # 인코더 입력은 문장 시작 토큰인 CLS_ID로 시작하여 소스 인코딩으로 이어지고 문장 끝 토큰인 SEP가 뒤따릅니다.\n        # 필요한 max_seq_len에 도달하기 위해 마지막에 PAD 토큰이 추가됩니다.\n        encoder_input = torch.cat([CLS_ID, source_text_encoded, SEP_ID, encoder_padding]).to(device)\n\n        # 디코더 입력은 문장 시작 토큰인 CLS_ID로 시작하여 타겟 인코딩이 뒤따릅니다.\n        # 필요한 max_seq_len에 도달하기 위해 마지막에 PAD 토큰이 추가됩니다. 디코더 입력에는 문장 끝 토큰인 SEP는 포함되지 않습니다.\n        decoder_input = torch.cat([CLS_ID, target_text_encoded, decoder_padding]).to(device)\n\n        # 타겟 레이블은 타겟 인코딩이 먼저 오고 문장 끝 토큰인 SEP가 뒤따릅니다. 시작 문장 토큰인 CLS는 없습니다.\n        # 필요한 max_seq_len에 도달하기 위해 마지막에 PAD 토큰이 추가됩니다.\n        target_label = torch.cat([target_text_encoded,SEP_ID,decoder_padding]).to(device)\n\n        # 인코딩 시 추가된 패딩 토큰을 모델이 학습하지 않도록 하기 위해 인코더 마스크를 사용하여 padding 토큰 값을 계산하기 전에 무효화합니다.\n        encoder_mask = (encoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int().to(device)\n\n        # 디코딩 단계에서는 현재 토큰 이후의 토큰에 영향을 받지 않도록 하기 위해 인과 마스크를 구현합니다.\n        decoder_mask = (decoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int() \u0026 causal_mask(decoder_input.size(0)).to(device)\n\n        return {\n            'encoder_input': encoder_input,\n            'decoder_input': decoder_input,\n            'target_label': target_label,\n            'encoder_mask': encoder_mask,\n            'decoder_mask': decoder_mask,\n            'source_text': source_text,\n            'target_text': target_text\n        }\n\n# 인과 마스크는 현재 토큰 이후에 올 토큰을 마스킹하여 softmax 함수 이후 -inf로 대체됩니다. 이를 통해 모델은 이러한 값들을 무시하거나 이를 통해 학습을 어렵게 합니다.\ndef causal_mask(size):\n  # 인과 마스크의\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 4: 입력 임베딩 및 위치 인코딩\n\n입력 임베딩: 단계 2의 토큰 생성기에서 생성된 토큰 ID 시퀀스가 임베딩 레이어로 공급될 것입니다. 임베딩 레이어는 토큰 ID를 어휘와 매핑하고 각 토큰에 대해 512 차원의 임베딩 벡터를 생성합니다. [512 차원은 어텐션 논문에서 가져왔습니다]. 임베딩 벡터는 토큰의 의미를 캡쳐할 수 있으며, 그것은 학습된 데이터셋에 기반하여 학습되었습니다. 임베딩 벡터 내의 각 차원 값은 토큰과 관련된 특징을 나타냅니다. 예를 들어, 토큰이 '개'인 경우, 일부 차원 값은 눈, 입, 다리, 키 등을 나타낼 것입니다. n차원 공간에 벡터를 그린다면, 비슷해 보이는 객체인 개와 고양이는 서로 가깝게 위치하고, 비슷해 보이지 않는 학교, 집 임베딩 벡터는 훨씬 더 멀리 위치해 있을 것입니다.\n\n위치 인코딩: 트랜스포머 아키텍처의 장점 중 하나는 어떤 수의 입력 시퀀스든 병렬로 처리할 수 있다는 것이며, 이는 훈련 시간을 많이 줄이고 예측을 훨씬 빠르게 만듭니다. 그러나 단점 중 하나는 병렬로 많은 토큰 시퀀스를 처리하는 동안, 문장 내 토큰의 위치가 순서대로 되지 않을 수 있다는 것입니다. 이로 인해 토큰의 위치에 따라 문장의 의미나 문맥이 달라질 수 있습니다. 따라서 이 문제를 해결하기 위해 어텐션 논문은 위치 인코딩 방법을 구현했습니다. 이 논문은 각 토큰의 512 차원에 대해 두 가지 수학 함수(sin과 cosine)를 적용하는 것을 제안했습니다. 아래는 간단한 sin과 cosine 수학 함수입니다.\n\n![이미지](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSin 함수는 임베딩 벡터의 각 짝수 차원 값에 적용되고, 코사인 함수는 홀수 차원 값에 적용됩니다. 최종적으로, 결과적인 위치 인코더 벡터는 임베딩 벡터에 추가됩니다. 이제 우리는 토큰의 의미와 위치를 모두 잡을 수 있는 임베딩 벡터를 갖게 되었습니다. 주의할 점은 위치 인코딩의 값이 각 시퀀스에서 동일하다는 것입니다.\n\n# 입력 임베딩과 위치 인코딩\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int):\n        super().__init__()\n        self.d_model = d_model\n        \n        # PyTorch의 임베딩 레이어 모듈을 사용하여 토큰 ID를 어휘에 매핑한 후 임베딩 벡터로 변환합니다.\n        # vocab_size는 훈련 데이터셋의 어휘 크기이며, 토큰화기가 코퍼스 데이터셋 훈련 중에 생성한 것입니다.\n        self.embedding = nn.Embedding(vocab_size, d_model)\n    \n    def forward(self, input):\n        # 입력 시퀀스를 임베딩 레이어에 공급할 때, d_model의 제곱근을 곱하는 추가로 정규화 작업을 수행합니다.\n        embedding_output = self.embedding(input) * math.sqrt(self.d_model)\n        return embedding_output\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, max_seq_len: int, d_model: int, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # 임베딩 벡터와 동일한 모양의 행렬을 만듭니다.\n        pe = torch.zeros(max_seq_len, d_model)\n        \n        # PE 함수의 위치 부분을 계산합니다.\n        pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n\n        # PE 함수의 나눗셈 부분을 계산합니다. 지수 함수의 표현이 논문 표현과 약간 다르지만 더 잘 작동하는 것으로 보입니다.\n        div_term = torch.exp(torch.arange(0, d_model, 2).float()) * (-math.log(10000)/d_model)\n        \n        # sin 및 cosine 수학 함수 결과로 홀수 및 짝수 행렬 값을 채웁니다.\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n        \n        # 입력 시퀀스가 배치로 예상되므로 추가적인 batch_size 차원이 0 위치에 추가됩니다.\n        pe = pe.unsqueeze(0)    \n    \n    def forward(self, input_embdding):\n        # 입력 임베딩 벡터에 위치 인코딩을 추가합니다.\n        input_embdding = input_embdding + (self.pe[:, :input_embdding.shape[1], :]).requires_grad_(False)  \n        \n        # 과적합을 방지하기 위해 드롭아웃을 수행합니다.\n        return self.dropout(input_embdding)\n\n## 단계 5: 멀티 헤드 어텐션 블록\n\n트랜스포머가 LLM의 핵심인 것처럼, 셀프 어텐션 메커니즘은 트랜스포머 아키텍처의 핵심입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자가 주의가 필요한 이유는 무엇인가요? 아래 간단한 예를 통해 답변해보겠습니다.\n\n![Example](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_5.png)\n\n문장 1과 문장 2에서 “bank”라는 단어는 명백히 두 가지 다른 의미를 가지고 있습니다. 그러나 “bank” 단어의 임베딩 값은 두 문장 모두에서 동일합니다. 이는 적절하지 않습니다. 우리는 임베딩 값이 문맥에 따라 변경되어야 한다는 것을 원합니다. 따라서 문장의 전체 의미에 기반하여 문맥적 의미를 나타낼 수 있는 동적 임베딩 값을 가지는 메커니즘이 필요합니다. 자가 주의 메커니즘은 문장의 전체 의미에 기반하여 문맥적 의미를 나타낼 수 있는 임베딩 값을 동적으로 업데이트할 수 있습니다.\n\n자가 주의가 이미 좋은데, 왜 다중 머리 자가 주의가 필요할까요? 답을 알아보기 위해 아래 예시를 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```\n![image](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_6.png)\n\n이 예에서는 self-attention을 사용할 때 문장의 한 측면에만 집중할 수 있는 가능성이 있습니다. 아마도 \"what\" 측면만을 포착할 수 있을 것입니다. 예를 들어, \"존이 무엇을 했나요?\"와 같이요. 그러나 \"언제\"나 \"어디\"와 같은 다른 측면들도 모델이 더 나은 성능을 발휘하기 위해 동등한 중요성을 갖습니다. 그래서, Self-Attention 메커니즘이 한 문장 내에서 여러 관계를 학습하도록 하는 방법을 찾아야 합니다. 이것이 Multi-Head Self Attention(Multi-Head Attention으로도 교차 사용 가능)이 해결해 주는 곳이죠. Multi-Head Attention에서는 단일 헤드 임베딩을 여러 헤드로 분할하여 각 헤드가 문장의 다른 측면을 살펴보고 그에 맞게 학습합니다. 이것이 우리가 원하는 바입니다.\n\n이제 왜 Multi-Head Attention이 필요한지 알게 되었습니다. 이제 어떻게 작용하는지 살펴보겠습니다. Multi-Head Attention은 실제로 어떻게 작동하는 걸까요? 바로 살펴보겠습니다.\n\n행렬 곱셈에 익숙하시다면, 이 메카니즘을 이해하는 것은 꽤 쉬운 작업일 것입니다. 먼저 전체 플로우 다이어그램을 살펴보고 Multi-Head Attention의 입력부터 출력까지의 플로우를 아래 일목요연하게 설명하겠습니다.\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_7.png)\n\n1. 먼저, 인코더 입력의 3개 복사본을 만들어봅시다 (입력 임베딩과 위치 인코딩의 조합, 이것은 단계 4에서 했었습니다). 각각을 Q, K, V라고 이름 붙여봅시다. 이들은 단지 인코더 입력의 복사본일 뿐입니다. 인코더 입력 형태: (seq_len, d_model), seq_len: 최대 시퀀스 길이, d_model: 이 경우에는 임베딩 벡터 차원이 512입니다.\n\n2. 다음으로, Q를 가중치 W_q, K를 가중치 W_k, V를 가중치 W_v와 행렬 곱셈을 수행하겠습니다. 각 가중치 행렬의 형태는 (d_model, d_model)입니다. 새로 얻게 된 쿼리, 키, 밸류 임베딩 벡터의 형태는 (seq_len, d_model)입니다. 가중치 매개변수들은 모델에 의해 무작위로 초기화되며 나준에 모델이 훈련을 시작할 때 업데이트될 것입니다. 어째서 우리가 처음부터 하는 가중치 행렬 곱셈이 필요한 것일까요? 왜냐하면 이것들은 쿼리, 키, 밸류 임베딩 벡터에 더 나은 표현을 제공하기 위해 필요한 학습 가능한 매개변수들이기 때문입니다.\n\n3. 어텐션 논문에 따르면, 헤드(heads) 수는 8입니다. 각 새로운 쿼리, 키, 밸류 임베딩 벡터는 8개의 더 작은 유닛으로 나뉘어집니다. 임베딩 벡터의 새로운 형태는 (seq_len, d_model/num_heads) 또는 (seq_len, d_k)입니다. [ d_k = d_model/num_heads ].\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4. 각 쿼리 임베딩 벡터는 자신 및 시퀀스의 모든 다른 임베딩 벡터의 키 임베딩 벡터의 전치와 닷 프로덕트 연산을 수행합니다. 이 닷 프로덕트는 주의 점수를 제공합니다. 주의 점수는 주어진 토큰이 주어진 입력 시퀀스의 다른 모든 토큰과 얼마나 유사한지를 보여줍니다. 점수가 높을수록 유사도가 더 높습니다.\n\n- 주의 점수는 나중에 매트릭스 전체에 걸쳐 점수 값을 정규화하는 데 필요한 d_k의 제곱근으로 나눠집니다. 그러나 왜 d_k로 나눠 정규화해야 하는 걸까요? 어떤 다른 숫자여도 괜찮을 텐데요. 주된 이유는 임베딩 벡터 차원이 증가함에 따라 주의 매트릭스의 총 분산이 비례해서 증가하기 때문입니다. 그래서 d_k로 나누면 분산 증가를 균형시킬 수 있습니다. 만약 d_k로 나누지 않으면, 어떤 높은 주의 점수라도 소프트맥스 함수는 매우 높은 확률 값을 제공하고, 반대로 낮은 주의 점수 값에 대해서는 소프트맥스 함수가 매우 낮은 확률 값을 제공할 것입니다. 이로 인해 모델은 그러한 확률 값이 있는 특징만 학습하려 하고, 낮은 확률 값이 있는 특징을 무시하기 쉽습니다. 이는 그라디언트가 소실되는 문제로 이어지게 됩니다. 따라서 주의 점수 매트릭스를 정규화하는 것이 매우 중요합니다.\n- 소프트맥스 함수를 수행하기 전에, 인코더 마스크가 None이 아닌 경우, 주의 점수는 인코더 마스크와 매트릭스 곱셈이 될 것입니다. 마스크가 인과적 마스크인 경우, 입력 시퀀스에서 그 이후에 오는 임베딩 토큰들에 대한 주의 점수 값은 -ve 무한대로 대체됩니다. 소프트맥스 함수는 -ve 무한대 값을 거의 0 값으로 변환할 것입니다. 그래서 모델은 현재 토큰 이후에 나오는 특징을 학습하지 않을 것입니다. 이것이 우리 모델 학습에 미래 토큰이 영향을 미치는 것을 방지하는 방법입니다.\n\n5. 소프트맥스 함수가 주의 점수 매트릭스에 적용되고 (seq_len, seq_len) 모양의 가중치 매트릭스가 출력됩니다.\n\n6. 이 가중치 매트릭스는 해당 값 임베딩 벡터와 매트릭스 곱셈을 수행할 것입니다. 결과적으로 (seq_len, d_v) 모양의 8개의 주의 헤드가 생성됩니다. [ d_v = d_model/num_heads ].\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러면, 모든 헤드들이 새로운 형태(seq_len, d_model)를 갖는 단일 헤드로 연결됩니다. 이 새로운 단일 헤드는 출력 가중치 행렬 W_o(d_model, d_model)과 행렬 곱셈을 수행합니다. Multi-Head Attention의 최종 출력은 단어의 문맥적 의미와 입력 문장의 여러 측면을 학습하는 능력을 나타냅니다.\n\n이제 Multi-Head Attention 블록 코딩을 시작해봅시다. 이것은 훨씬 쉽고 간단할 거예요.\n\n```js\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout_rate: float):\n        super().__init__()\n        # 과적합을 방지하기 위해 드롭아웃을 정의합니다.\n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # 가중치 행렬은 도입되며 모두 학습 가능한 매개변수입니다.\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        self.num_heads = num_heads\n        assert d_model % num_heads == 0, \"d_model은 헤드 수로 나눌 수 있어야 합니다.\"\n        \n        # d_k는 각 분할된 self attention 헤드의 새로운 차원입니다\n        self.d_k = d_model // num_heads\n\n    def forward(self, q, k, v, encoder_mask=None):\n        \n        # 여러 시퀀스 배치로 모델을 한 번에 병렬로 학습하게 될 것이므로, 모양에 배치 크기를 포함해야 합니다.\n        # 쿼리, 키 및 값은 해당 가중치와 입력 임베딩의 행렬 곱으로 계산됩니다.\n        # 모양 변화: q(배치 크기, seq_len, d_model) @ W_q(d_model, d_model) =\u003e query(배치 크기, seq_len, d_model) [키와 값도 동일함].\n        query = self.W_q(q) \n        key = self.W_k(k)\n        value = self.W_v(v)\n\n        # 쿼리, 키, 밸류를 헤드 수로 분할합니다. d_model은 d_k마다 8개 헤드로 분할됩니다.\n        # 모양 변화: query(배치 크기, seq_len, d_model) =\u003e query(배치 크기, seq_len, num_heads, d_k) -\u003e query(배치 크기, num_heads, seq_len, d_k) [키와 값도 동일함].\n        query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1,2)\n        key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1,2)\n        value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1,2)\n\n        # :: SELF ATTENTION BLOCK STARTS ::\n\n        # 유사도 또는 쿼리 간의 관련성을 찾기 위해 주의 점수가 계산됩니다.\n        # 모양 변화: query(배치 크기, num_heads, seq_len, d_k) @ key(배치 크기, num_heads, seq_len, d_k) =\u003e attention_score(배치 크기, num_heads, seq_len, seq_len).\n        attention_score = (query @ key.transpose(-2,-1)) / math.sqrt(self.d_k)\n\n        # 만약 마스크가 제공된다면, 주의 점수를 마스크 값에 따라 수정해야 합니다. 자세한 내용은 4번을 참조하세요.\n        if encoder_mask is not None:\n            attention_score = attention_score.masked_fill(encoder_mask == 0, -1e9)\n        \n        # softmax 함수는 모든 주의 점수 중에서 확률 분포를 계산합니다. 더 높은 주의 점수에 더 높은 확률 값을 할당합니다. 즉, 보다 유사한 토큰은 더 높은 확률 값을 가집니다.\n        # 모양 변화: attention_score와 동일합니다.\n        attention_weight = torch.softmax(attention_score, dim=-1)\n\n        if self.dropout is not None:\n            attention_weight = self.dropout(attention_weight)\n\n        # Self attention 블록의 최종 단계는 주의 가중치를 값 임베딩 벡터와의 행렬 곱셈입니다.\n        # 모양 변화: attention_score(배치 크기, num_heads, seq_len, seq_len) @ value(배치 크기, num_heads, seq_len, d_k) =\u003e attention_output(배치 크기, num_heads, seq_len, d_k)\n        attention_output = attention_score @ value\n        \n        # :: SELF ATTENTION BLOCK ENDS ::\n\n        # 이제, 모든 헤드들이 다시 단일 헤드로 결합됩니다.\n        # 모양 변화: attention_output(배치 크기, num_heads, seq_len, d_k) =\u003e attention_output(배치 크기, seq_len, num_heads, d_k) =\u003e attention_output(배치 크기, seq_len, d_model)        \n        attention_output = attention_output.transpose(1,2).contiguous().view(attention_output.shape[0], -1, self.num_heads * self.d_k)\n\n        # 마침내 attention_output을 출력 가중치 행렬로 행렬 곱하여 최종 Multi-Head attention 출력을 얻습니다.\n        # multihead_output의 모양은 임베딩 입력과 동일합니다.\n        # 모양 변화: attention_output(배치 크기, seq_len, d_model) @ W_o(d_model, d_model) =\u003e multihead_output(배치 크기, seq_len, d_model)\n        multihead_output = self.W_o(attention_output)\n        \n        return multihead_output\n```\n\n## Step 6: 피드포워드 네트워크, 레이어 정규화 및 AddAndNorm\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n피드포워드 네트워크: 피드포워드 네트워크는 두 개의 선형 레이어(첫 번째는 d_model 노드를 가지고 두 번째는 d_ff 노드를 가지며, 주어진 값은 어텐션 논문에 따라 할당됩니다)를 통해 임베딩 벡터의 모든 기능을 학습하는 딥 신경망을 사용합니다. 첫 번째 선형 레이어의 출력에는 ReLU 활성화 함수가 적용되어 임베딩 값을 비선형으로 만들고, 과적합을 피하기 위해 드롭아웃이 적용됩니다.\n\n레이어 정규화: 네트워크 내 임베딩 벡터의 값 분포가 일관되게 유지되도록 임베딩 값에 레이어 정규화를 적용합니다. 이는 원활한 학습을 보장합니다. 네트워크가 필요로 하는대로 임베딩 값을 스케일링하고 이동시키기 위해 gamma와 beta라는 추가 학습 매개변수를 사용할 것입니다.\n\nAddAndNorm: 이는 스킵 연결과 레이어 정규화(이전에 설명함)로 구성됩니다. 순방향 패스에서 스킵 연결은 이전 레이어의 기능이 계산 결과에 필요한 기여를 할 수 있도록 나중 단계에서도 해당 기능을 기억할 수 있습니다. 마찬가지로 역전파 중에도 스킵 연결은 각 단계에서 하나 덜의 역전파를 수행해 사라지는 기울기를 방지합니다. AddAndNorm은 인코더(2번)와 디코더 블록(3번) 모두에 사용됩니다. 이는 이전 레이어에서 입력을 받아 이전 레이어의 출력에 추가하기 전에 먼저 정규화합니다.\n\n```js\n# Feedfoward Network, Layer Normalization and AddAndNorm Block\nclass FeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout_rate: float):\n        super().__init__()\n\n        self.layer_1 = nn.Linear(d_model, d_ff)\n        self.activation_1 = nn.ReLU()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_2 = nn.Linear(d_ff, d_model)\n    \n    def forward(self, input):\n        return self.layer_2(self.dropout(self.activation_1(self.layer_1(input))))\n\nclass LayerNorm(nn.Module):\n    def __init__(self, eps: float = 1e-5):\n        super().__init__()\n        # 엡실론은 매우 작은 값으로, 잠재적으로 0으로 나누는 문제를 방지하는 데 중요한 역할을 합니다.\n        self.eps = eps\n\n        # 스케일링과 이동을 위해 추가 학습 매개변수인 감마와 베타를 도입합니다.\n        self.gamma = nn.Parameter(torch.ones(1))\n        self.beta = nn.Parameter(torch.zeros(1))\n    \n    def forward(self, input):\n        mean = input.mean(dim=-1, keepdim=True)      \n        std = input.std(dim=-1, keepdim=True)      \n\n        return self.gamma * ((input - mean)/(std + self.eps)) + self.beta\n        \nclass AddAndNorm(nn.Module):\n    def __init__(self, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_norm = LayerNorm()\n\n    def forward(self, input, sub_layer):\n        return input + self.dropout(sub_layer(self.layer_norm(input)))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 7: 인코더 블록과 인코더\n\n인코더 블록: 인코더 블록 안에는 두 가지 주요 구성 요소가 있습니다: 멀티헤드 어텐션과 피드포워드입니다. Add \u0026 Norm 단위가 2개 있습니다. 먼저 어텐션 논문의 흐름에 따라 EncoderBlock 클래스에 모든 이러한 구성 요소를 조립할 것입니다. 논문에 따르면 이 인코더 블록은 6번 반복된다고 합니다.\n\n인코더: 그런 다음 EncoderBlock 목록을 가져와 쌓아 최종 Encoder 출력을 제공할 Encoder라는 추가 클래스를 생성할 것입니다.\n\n```python\nclass EncoderBlock(nn.Module):\n    def __init__(self, multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float):\n        super().__init__()\n        self.multihead_attention = multihead_attention\n        self.feed_forward = feed_forward\n        self.add_and_norm_list = nn.ModuleList([AddAndNorm(dropout_rate) for _ in range(2)])\n\n    def forward(self, encoder_input, encoder_mask):\n        # 인코더 입력을 스킵 연결에서 가져와 멀티헤드 어텐션 블록의 출력과 더하는 첫 번째 AddAndNorm 단위입니다.\n        encoder_input = self.add_and_norm_list[0](encoder_input, lambda encoder_input: self.multihead_attention(encoder_input, encoder_input, encoder_input, encoder_mask))\n        \n        # 멀티헤드 어텐션 블록의 출력을 스킵 연결에서 가져와 피드포워드 레이어의 출력과 더하는 두 번째 AddAndNorm 단위입니다.\n        encoder_input = self.add_and_norm_list[1](encoder_input, self.feed_forward)\n\n        return encoder_input\n\nclass Encoder(nn.Module):\n    def __init__(self, encoderblocklist: nn.ModuleList):\n        super().__init__()\n\n        # Encoder 클래스는 encoderblock 목록을 가져와 초기화합니다.\n        self.encoderblocklist = encoderblocklist\n        self.layer_norm = LayerNorm()\n\n    def forward(self, encoder_input, encoder_mask):\n        # 모든 인코더 블록을 반복합니다 - 총 6번.\n        for encoderblock in self.encoderblocklist:\n            encoder_input = encoderblock(encoder_input, encoder_mask)\n\n        # 최종 인코더 블록 출력을 정규화하고 반환합니다. 이 인코더 출력은 나중에 디코더 블록의 교차 어텐션에서 키 및 값으로 사용될 것입니다.\n        encoder_output = self.layer_norm(encoder_input)\n        return encoder_output\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Step 8: 디코더 블록, 디코더 및 프로젝션 레이어\n\n디코더 블록: 디코더 블록에는 세 가지 주요 구성 요소가 있습니다: 마스킹된 멀티헤드 어텐션, 멀티헤드 어텐션 및 피드포워드입니다. 디코더 블록에는 Add \u0026 Norm의 3개 단위도 있습니다. 우리는 이러한 구성 요소들을 Attention 논문의 흐름에 따라 DecoderBlock 클래스에 모두 조합할 것입니다. 논문에 따르면 이 디코더 블록은 6번 반복됩니다.\n\n디코더: 우리는 DecoderBlock의 리스트를 가져와 스택하여 최종 디코더 출력을 생성할 Decoder라는 추가 클래스를 만들 것입니다.\n\n디코더 블록에는 두 가지 타입의 멀티헤드 어텐션이 있습니다. 첫 번째는 Masked Multi-Head 어텐션입니다. 이는 디코더 입력을 쿼리, 키, 밸류로 사용하며 디코더 마스크(인과 마스크로도 알려짐)를 사용합니다. 인과 마스크는 모델이 순서에 앞서있는 임베딩을 볼 수 없게 합니다. 이 동작 방식에 대한 자세한 설명은 3단계와 5단계에 제공되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n프로젝션 레이어: 마지막 디코더 출력은 프로젝션 레이어로 전달됩니다. 이 레이어에서는 먼저 디코더 출력이 먼저 선형 레이어로 공급되어 임베딩의 모양이 아래 코드 섹션에서 제공된대로 변경될 것입니다. 그런 다음 softmax 함수가 디코더 출력을 어휘에 대한 확률 분포로 변환하고 가장 높은 확률을 가진 토큰이 예측 출력으로 선택됩니다.\n\n```js\nclass DecoderBlock(nn.Module):\n    def __init__(self, masked_multihead_attention: MultiHeadAttention, multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float):\n        super().__init__()\n        self.masked_multihead_attention = masked_multihead_attention\n        self.multihead_attention = multihead_attention\n        self.feed_forward = feed_forward\n        self.add_and_norm_list = nn.ModuleList([AddAndNorm(dropout_rate) for _ in range(3)])\n\n    def forward(self, decoder_input, decoder_mask, encoder_output, encoder_mask):\n        # 첫 번째 AddAndNorm 유닛은 디코더 입력을 스킵 연결에서 가져와 마스킹된 멀티헤드 어텐션 블록의 출력과 더합니다.\n        decoder_input = self.add_and_norm_list[0](decoder_input, lambda decoder_input: self.masked_multihead_attention(decoder_input, decoder_input, decoder_input, decoder_mask))\n        # 두 번째 AddAndNorm 유닛은 스킵 연결로부터 마스킹된 멀티헤드 어텐션 블록의 출력을 가져와 멀티헤드 어텐션 블록의 출력과 더합니다.\n        decoder_input = self.add_and_norm_list[1](decoder_input, lambda decoder_input: self.multihead_attention(decoder_input, encoder_output, encoder_output, encoder_mask)) # 교차 어텐션\n        # 세 번째 AddAndNorm 유닛은 멀티헤드 어텐션 블록의 출력을 스킵 연결로부터 가져와 피드포워드 레이어의 출력과 더합니다.\n        decoder_input = self.add_and_norm_list[2](decoder_input, self.feed_forward)\n        return decoder_input\n\nclass Decoder(nn.Module):\n    def __init__(self, decoderblocklist: nn.ModuleList):\n        super().__init__()\n        self.decoderblocklist = decoderblocklist\n        self.layer_norm = LayerNorm()\n\n    def forward(self, decoder_input, decoder_mask, encoder_output, encoder_mask):\n        for decoderblock in self.decoderblocklist:\n            decoder_input = decoderblock(decoder_input, decoder_mask, encoder_output, encoder_mask)\n\n        decoder_output = self.layer_norm(decoder_input)\n        return decoder_output\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int):\n        super().__init__()\n        self.projection_layer = nn.Linear(d_model, vocab_size)\n\n    def forward(self, decoder_output):\n        # 프로젝션 레이어는 먼저 디코더 출력을 받아 (d_model, vocab_size) 모양의 선형 레이어로 전달합니다.\n        # 모양 변경: decoder_output(batch_size, seq_len, d_model) @ linear_layer(d_model, vocab_size) =\u003e output(batch_size, seq_len, vocab_size)\n        output = self.projection_layer(decoder_output)\n        \n        # 어휘상의 확률 분포를 출력하기 위해 softmax 함수를 사용합니다.\n        return torch.log_softmax(output, dim=-1)\n```\n\n## 단계 9: 트랜스포머 생성 및 구축\n\n마침내, 트랜스포머 아키텍처의 모든 구성 요소 블록을 구축했습니다. 유일한 미완료 작업은 이 모든 것을 함께 조립하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저, 컴포넌트 클래스의 모든 인스턴스를 초기화하는 Transformer 클래스를 생성합니다. Transform 클래스 내부에는 먼저 인코더 부분의 모든 작업을 수행하고 인코더 출력을 생성하는 encode 함수를 정의합니다.\n\n두 번째로, Transformer의 디코더 부분의 모든 작업을 수행하고 디코더 출력을 생성하는 decode 함수를 정의합니다.\n\n세 번째로, 디코더 출력을 가져와 예측을 위해 해당 어휘에 매핑하는 project 함수를 정의합니다.\n\n이제 Transformer 아키텍처가 준비되었습니다. 이제 아래 코드에서 필요한 모든 매개변수를 사용하여 번역 LLM 모델을 구축할 수 있는 함수를 정의합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nclass Transformer(nn.Module):\n    def __init__(self, source_embed: EmbeddingLayer, target_embed: EmbeddingLayer, positional_encoding: PositionalEncoding, multihead_attention: MultiHeadAttention, masked_multihead_attention: MultiHeadAttention, feed_forward: FeedForward, encoder: Encoder, decoder: Decoder, projection_layer: ProjectionLayer, dropout_rate: float):        \n        super().__init__()\n        \n        # Transformer 아키텍처의 모든 구성 요소 클래스의 인스턴스를 초기화합니다.\n        self.source_embed = source_embed\n        self.target_embed = target_embed\n        self.positional_encoding = positional_encoding\n        self.multihead_attention = multihead_attention        \n        self.masked_multihead_attention = masked_multihead_attention\n        self.feed_forward = feed_forward\n        self.encoder = encoder\n        self.decoder = decoder\n        self.projection_layer = projection_layer\n        self.dropout = nn.Dropout(dropout_rate)\n    \n    # Encode 함수는 인코더 입력을 받아서 모든 인코더 블록 내에서 필요한 처리를 수행하고 인코더 출력을 제공합니다.\n    def encode(self, encoder_input, encoder_mask):\n        encoder_input = self.source_embed(encoder_input)\n        encoder_input = self.positional_encoding(encoder_input)\n        encoder_output = self.encoder(encoder_input, encoder_mask)\n        return encoder_output\n\n    # Decode 함수는 디코더 입력을 받아서 모든 디코더 블록 내에서 필요한 처리를 수행하고 디코더 출력을 제공합니다.\n    def decode(self, decoder_input, decoder_mask, encoder_output, encoder_mask):\n        decoder_input = self.target_embed(decoder_input)\n        decoder_input = self.positional_encoding(decoder_input)\n        decoder_output = self.decoder(decoder_input, decoder_mask, encoder_output, encoder_mask)\n        return decoder_output\n\n    # Projec 함수는 디코더 출력을 투영 레이어로 받아들이고 출력을 어휘로 매핑하여 예측합니다.\n    def project(self, decoder_output):\n        return self.projection_layer(decoder_output)\n\ndef build_model(source_vocab_size, target_vocab_size, max_seq_len=1135, d_model=512, d_ff=2048, num_heads=8, num_blocks=6, dropout_rate=0.1):\n    \n    # Transformer 아키텍처에 필요한 모든 매개변수 값을 정의하고 할당합니다.\n    source_embed = EmbeddingLayer(source_vocab_size, d_model)\n    target_embed = EmbeddingLayer(target_vocab_size, d_model)\n    positional_encoding = PositionalEncoding(max_seq_len, d_model, dropout_rate)\n    multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n    masked_multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n    feed_forward = FeedForward(d_model, d_ff, dropout_rate)    \n    projection_layer = ProjectionLayer(target_vocab_size, d_model)\n    encoder_block = EncoderBlock(multihead_attention, feed_forward, dropout_rate)\n    decoder_block = DecoderBlock(masked_multihead_attention, multihead_attention, feed_forward, dropout_rate)\n\n    encoderblocklist = []\n    decoderblocklist = []\n\n    for _ in range(num_blocks):\n        encoderblocklist.append(encoder_block)   \n         \n    for _ in range(num_blocks):\n        decoderblocklist.append(decoder_block)\n    \n    encoderblocklist = nn.ModuleList(encoderblocklist)            \n    decoderblocklist = nn.ModuleList(decoderblocklist)\n        \n    encoder = Encoder(encoderblocklist)\n    decoder = Decoder(decoderblocklist)\n    \n    # 모든 매개변수 값을 제공하여 Transformer 클래스를 인스턴스화합니다.\n    model = Transformer(source_embed, target_embed, positional_encoding, multihead_attention, masked_multihead_attention, feed_forward, encoder, decoder, projection_layer, dropout_rate)\n\n    for param in model.parameters():\n        if param.dim() \u003e 1:\n            nn.init.xavier_uniform_(param)\n    \n    return model\n\n# 마침내, build_model을 호출하고 model 변수에 할당합니다.\n# 이 모델은 이제 데이터셋을 훈련하고 검증하는 데 완전히 준비된 상태입니다.\n# 훈련 및 검증 후 이 모델을 사용하여 새로운 번역 작업을 수행할 수 있습니다.\n\nmodel = build_model(source_vocab_size, target_vocab_size)\n```\n\n## 단계 10: 생성한 LLM 모델의 훈련 및 검증\n\n지금은 모델을 훈련할 시간입니다. 훈련 프로세스는 매우 간단합니다. 우리는 단계 3에서 생성한 훈련 DataLoader를 사용할 것입니다. 총 훈련 데이터셋 수가 100만이므로 GPU 장치에서 모델을 훈련하는 것을 강력히 권장합니다. 20 epoch를 완료하는 데 약 5시간이 소요되었습니다. 각 epoch 이후에는 모델 가중치와 옵티마이저 상태를 저장하여 중지된 지점부터 훈련을 다시 시작하는 것이 더 쉽기 때문에 이전 지점에서 훈련을 재개하는 것보다 더 나을 것입니다.\n\n매 에포크 이후에는 검증을 시작합니다. 검증 데이터셋 크기는 2000으로 매우 합리적입니다. 검증 프로세스 중에는 디코더 출력이 [SEP] 토큰을 받을 때까지 한 번만 인코더 출력을 계산하면 됩니다. 이것은 디코더가 [SEP] 토큰을 받기 전까지 동일한 인코더 출력을 계속 보내야 하기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n디코더 입력은 먼저 문장 시작 토큰 [CLS]으로 시작됩니다. 각 예측 후에는 디코더 입력이 다음 생성된 토큰을 붙여넣을 것이며, 문장 끝 토큰 [SEP]에 도달할 때까지 이를 반복합니다. 마지막으로, 프로젝션 레이어는 출력을 해당 텍스트 표현으로 매핑합니다.\n\n```js\ndef training_model(preload_epoch=None):   \n\n    # 전체 훈련 및 검증 주기는 20번 실행됩니다.\n    EPOCHS = 20\n    initial_epoch = 0\n    global_step = 0    \n    \n    # Adam은 현재 상태를 유지하고 계산된 기울기에 기반하여 매개변수를 업데이트하는 가장 일반적으로 사용되는 최적화 알고리즘 중 하나입니다.         \n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    \n    # preload_epoch가 None이 아닌 경우, 이는 최근 저장된 가중치, 최적화기로 훈련이 시작될 것을 의미합니다. 새로운 에포크 번호는 preload epoch + 1이 됩니다.\n    if preload_epoch is not None:\n        model_filename = f\"./malaygpt/model_{preload_epoch}.pt\"\n        state = torch.load(model_filename)\n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n\n    # CrossEntropyLoss 손실 함수는 프로젝션 출력과 대상 라벨 사이의 차이를 계산합니다.\n    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_en.token_to_id(\"[PAD]\"), label_smoothing=0.1).to(device)\n\n    for epoch in range(initial_epoch, EPOCHS):\n\n        # ::: 훈련 블록 시작 :::\n        model.train()  \n        \n        # 훈련 데이터로더로 훈련을 진행합니다.     \n        for batch in tqdm(train_dataloader):\n            encoder_input = batch['encoder_input'].to(device)   # (batch_size, seq_len)\n            decoder_input = batch['decoder_input'].to(device)    # (batch_size, seq_len)\n            target_label = batch['target_label'].to(device)      # (batch_size, seq_len)\n            encoder_mask = batch['encoder_mask'].to(device)       \n            decoder_mask = batch['decoder_mask'].to(device)         \n\n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(decoder_input, decoder_mask, encoder_output, encoder_mask)\n            projection_output = model.project(decoder_output)\n\n            # projection_output(batch_size, seq_len, vocab_size)\n            loss = loss_fn(projection_output.view(-1, projection_output.shape[-1]), target_label.view(-1))\n            \n            # 역전파\n            optimizer.zero_grad()\n            loss.backward()\n\n            # 가중치 업데이트\n            optimizer.step()        \n            global_step += 1\n\n        print(f'Epoch [{epoch+1}/{EPOCHS}]: Train Loss: {loss.item():.2f}')\n        \n        # 각 에포크가 끝난 후 모델 상태를 저장합니다.\n        model_filename = f\"./malaygpt/model_{epoch}.pt\"\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)        \n        # ::: 훈련 블록 끝 :::\n\n        # ::: 검증 블록 시작 :::\n        model.eval()        \n        with torch.inference_mode():\n            for batch in tqdm(val_dataloader):                \n                encoder_input = batch['encoder_input'].to(device)   # (batch_size, seq_len)                        \n                encoder_mask = batch['encoder_mask'].to(device)\n                source_text = batch['source_text']\n                target_text = batch['target_text']\n                \n                # 소스 시퀀스에 대한 인코더 출력 계산\n                encoder_output = model.encode(encoder_input, encoder_mask)\n\n                # 예측 작업을 위해 디코더 입력으로 들어가는 첫 번째 토큰은 [CLS] 토큰입니다.\n                decoder_input = torch.empty(1,1).fill_(tokenizer_my.token_to_id('[CLS]')).type_as(encoder_input).to(device)\n\n                # [SEP] - 끝 토큰이 받아질 때까지 새로운 출력을 입력에 계속 추가해야 하므로, 이를 구현합니다.\n                while True:                     \n                    # 최대 길이를 받았는지 확인하고, 그렇다면 중지합니다.\n                    if decoder_input.size(1) == max_seq_len:\n                        break\n\n                    # 새로운 출력이 추가될 때마다 새로 마스크를 만들어 다음 토큰 예측을 위해 디코더 입력에 추가합니다.\n                    decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n\n                    decoder_output = model.decode(decoder_input,decoder_mask,encoder_output,encoder_mask)\n                    \n                    # 프로젝션을 다음 토큰에만 적용합니다.\n                    projection = model.project(decoder_output[:, -1])\n\n                    # 가장 높은 확률을 갖는 토큰을 선택하여 탐욕 탐색 구현을 합니다.\n                    _, new_token = torch.max(projection, dim=1)\n                    new_token = torch.empty(1,1). type_as(encoder_input).fill_(new_token.item()).to(device)\n\n                    # 새로운 토큰을 다시 디코더 입력에 추가합니다.\n                    decoder_input = torch.cat([decoder_input, new_token], dim=1)\n\n                    # 새로운 토큰이 종료 토큰인 경우, 받아들였다면 중지합니다.\n                    if new_token == tokenizer_my.token_to_id('[SEP]'):\n                        break\n\n                # 전체로 추가된 디코더 입력을 디코더 출력으로 할당합니다.\n                decoder_output = decoder_input.sequeeze(0)\n                model_predicted_text = tokenizer_my.decode(decoder_output.detach().cpu.numpy())\n                \n                print(f'SOURCE TEXT\": {source_text}')\n                print(f'TARGET TEXT\": {target_text}')\n                print(f'PREDICTED TEXT\": {model_predicted_text}')   \n                # ::: 검증 블록 끝 :::             \n\n# 이 함수는 20번의 에포크에 대해 훈련 및 검증을 실행합니다.\ntraining_model(preload_epoch=None)\n```\n\n## 단계 11: 새 번역 작업을 현재 모델로 테스트하는 함수 생성\n\n번역 기능에 일반적인 이름인 malaygpt를 할당합니다. 이는 사용자 입력 영어 원시 텍스트를 입력으로 받아 말레이어 언어로 번역된 텍스트를 출력합니다. 이 함수를 실행해 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef malaygpt(user_input_text):\n  model.eval()\n  with torch.inference_mode():\n    user_input_text = user_input_text.strip()\n    user_input_text_encoded = torch.tensor(tokenizer_en.encode(user_input_text).ids, dtype=torch.int64).to(device)\n\n    num_source_padding = max_seq_len - len(user_input_text_encoded) - 2\n    encoder_padding = torch.tensor([PAD_ID] * num_source_padding, dtype=torch.int64).to(device)\n    encoder_input = torch.cat([CLS_ID, user_input_text_encoded, SEP_ID, encoder_padding]).to(device)\n    encoder_mask = (encoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int().to(device)\n\n    # Computing the output of the encoder for the source sequence\n    encoder_output = model.encode(encoder_input, encoder_mask)\n    # for prediction task, the first token that goes in decoder input is the [CLS] token\n    decoder_input = torch.empty(1, 1).fill_(tokenizer_my.token_to_id('[CLS]')).type_as(encoder_input).to(device)\n\n    # since we need to keep adding the output back to the input until the [SEP] - end token is received.\n    while True:\n        # check if the max length is received\n        if decoder_input.size(1) == max_seq_len:\n            break\n        # recreate mask each time the new output is added to the decoder input for the next token prediction\n        decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n        decoder_output = model.decode(decoder_input, decoder_mask, encoder_output, encoder_mask)\n\n        # apply projection only to the next token\n        projection = model.project(decoder_output[:, -1])\n\n        # select the token with the highest probability which is a greedy search implementation\n        _, new_token = torch.max(projection, dim=1)\n        new_token = torch.empty(1, 1).type_as(encoder_input).fill_(new_token.item()).to(device)\n\n        # add the new token back to the decoder input\n        decoder_input = torch.cat([decoder_input, new_token], dim=1)\n\n        # check if the new token is the end of the token\n        if new_token == tokenizer_my.token_to_id('[SEP]'):\n            break\n    # the final decoder out is the concatenated decoder input until the end token\n    decoder_output = decoder_input.squeeze(0)\n    model_predicted_text = tokenizer_my.decode(decoder_output.detach().cpu.numpy())\n\n    return model_predicted_text\n```\n\nTesting Time! Let’s do some translation testing.\n\n![image](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_8.png)\n\n“The translation seems to be working pretty well.”\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기까지입니다! 이제는 PyTorch를 사용하여 처음부터 Large Language Model을 만들 수 있을 것이라고 매우 자신합니다. 물론 이 모델을 다른 언어 데이터셋에서도 훈련시키고 해당 언어로 번역 작업을 수행할 수 있습니다. 이제 처음부터 transformer를 만드는 방법을 배웠으니, 이제 시장에서 사용 가능한 대부분의 LLM에 대한 학습 및 응용을 스스로 할 수 있다는 것을 확신할 수 있습니다.\n\n그다음은 무엇일까요? 현재 시장에서 인기있는 오픈소스 LLM 모델 중 하나인 Llama 3 모델을 파인 튜닝하여 완전히 기능적인 애플리케이션을 만들 것입니다. 전체 소스 코드도 함께 공유할 예정입니다.\n\n그러니 기대해 주시고, 읽어 주셔서 정말 감사합니다!\n\n저와 연결해 보세요: https://www.linkedin.com/in/tamangmilan\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**Google Colab 노트북 링크**\n\n**참고 자료**\n\n- Attention Is All You Need — 논문, Ashish Vaswani, Noam Shazeer, 그리고 팀\n- Attention in transformers, 시각적으로 설명된 내용, 3Blue1Brown — 유튜브\n- GPT 구축하기, Andrej Karpathy, 유튜브\n- https://github.com/hkproj/pytorch-transformer — Umar Jamil","ogImage":{"url":"/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_0.png"},"coverImage":"/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_0.png","tag":["Tech"],"readingTime":40},{"title":"혁신 수용하기 로보틱스로 미래를 발견하다","description":"","date":"2024-06-19 18:41","slug":"2024-06-19-TitleEmbracingInnovationUnveilingtheFuturewithRobotics","content":"\n\n로봇이 인간과 함께 일상 업무를 보조하며 혁신적인 발견을 하며 놀라운 기회를 창출하는 세계를 상상해보세요. 이것은 과학 소설이 아니라, 로봇 기술을 주도하는 기술의 미래입니다.\n\n![로봇 이미지](/assets/img/2024-06-19-TitleEmbracingInnovationUnveilingtheFuturewithRobotics_0.png)\n\n이 블로그 포스트에서는 로봇 기술이 인류에게 제공하는 매혹적인 장점을 살펴보며, 우리 현재를 재정의하고 약속된 내일을 모습을 만들어가는 것을 탐구할 것입니다.\n\nSection 1: 로봇 기술의 부상 — 역사적 관점\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n고대 문명의 초기 자동화 장치부터 오늘날의 정교한 기계까지, 로봇학의 진화는 놀라울 정도로 탁월하였습니다. 이러한 역사를 이해함으로써, 우리는 어디까지 왔는지 그리고 앞으로 어떤 발전이 기대되는지에 대한 소중한 통찰력을 얻을 수 있습니다.\n\nSection 2: 산업 전반에서 효율 향상\n\n로봇은 제조업, 의료, 농업, 물류 등 다양한 산업에서 생산성을 향상시키고, 오류를 최소화하며, 안전 조치를 개선하고, 비용을 줄이는 방식으로 혁명을 일으키고 있습니다. 대표적인 사례로는 공장 생산 라인의 로봇 팔이나, 수술을 정밀한 미세 수준으로 지원하는 로봇 등이 있습니다.\n\nSection 3: 로봇학 — 혁신을 위한 등대\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로봇 기술은 최신 연구를 주도하는 원동력입니다. 새로운 소재 개발부터 외부 우주에서 미지의 영역을 탐험하는 등의 분야에 활용되고 있습니다. NASA의 화성 로버와 해저 로봇과 같은 사례 연구들은 이전에 우리가 닿을 수 없었던 발견의 길을 열고 있습니다.\n\n4장: 개인 로봇을 통한 삶의 질 향상\n\n로봇 지원은 전문 분야에만 국한되지 않고 개인적인 생활에도 확장되고 있습니다. 노인이나 장애인을 로봇 이동 보조기와 동반자 로봇을 통해 지원함으로써, 이 기술은 독립성과 감정적 지원을 제공하여 총체적인 삶의 질을 향상시킵니다.\n\n5장: 경제적 이점과 일자리 창출\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일부 사람들은 일자리 치환에 대해 걱정하기도 하지만, 로봇공학이 다양한 산업 분야에서 일자리 창출 기회를 제공하는 것을 명심하는 것이 중요합니다. 제조 로봇부터 특수 조립 작업자가 필요한 서비스 업데이트나 수리 서비스 제공 업체까지 다양한 산업에서 일자리가 창출됩니다. 더 나아가, 로봇이 반복적인 작업을 대신하면서 인간들은 창의적이고 분석적 역할에 집중할 수 있게 됩니다.\n\n섹션 6: 미래 가능성 — 로봇공학의 다음 단계\n\n인공지능(AI)은 인간과 기계 간에 전례없는 협력을 약속하는 로봇공학 세계에 환상적인 요소입니다. 자율 주행 자동차, 연구용 AI 증강 로봇 또는 심지어 외계 탐사를 위한 혁신은 바로 뒤에 있습니다. 이 모든 것은 로봇공학 기술의 발전 덕분에 가능해졌습니다!\n\n우리가 이처럼 혁신적인 로봇공학의 혜택을 받아들일 때, 그들이 사회에 다양한 혜택을 제공함이 명백합니다. 작업을 효율화하고 산업 전반에 혁신을 촉진함으로써 로봇은 인간 경험을 재정의하며, 기술이 우리의 능력을 대체하는 것이 아니라 보완하는 방향으로 우리를 밝은 미래로 이끕니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n윤리적 가이드에 기반을 둔 로보틱스 발전과 교육 투자가 중요하다. 이를 통해 인간들이 기계 동료들과 함께 협력적인 미래를 준비할 수 있습니다. 저희가 함께하는 이 흥미진 여정에서 로보틱스의 가능성을 하나씩 발견해나가봅시다!\n\n참고 문헌:\n\n1. “A Brief History of Robotics,” The Conversation, 접속일: [날짜]\n\n2. “Robots at Work in the American Economy” — Boston Federal Reserve Working Paper Series\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. \"로봇이 어떻게 삶을 변화시키고 있는지,\" BBC 퓨처, 접속일 [날짜].","ogImage":{"url":"/assets/img/2024-06-19-TitleEmbracingInnovationUnveilingtheFuturewithRobotics_0.png"},"coverImage":"/assets/img/2024-06-19-TitleEmbracingInnovationUnveilingtheFuturewithRobotics_0.png","tag":["Tech"],"readingTime":3},{"title":"실패로부터 배운 것 내 가장 큰 AI 프로젝트 실수와 교훈들","description":"","date":"2024-06-19 18:40","slug":"2024-06-19-LearningfromFailuresMyBiggestAIProjectMistakesandTakeaways","content":"\n\n![Learning from Failures](/assets/img/2024-06-19-LearningfromFailuresMyBiggestAIProjectMistakesandTakeaways_0.png)\n\n옛날 어느 날, 저는 챗봇을 만들기로 결심했습니다. 그냥 챗봇이 아니라, 한마디로 아재토론가와 노자처럼 똑똑한 챗봇을 만들기로 한 거예요. 야심 차게 정말 대단한 계획이었어요, 그렇죠? 하지만 야심은 때로 우스꽝스런 실수들이 나오게 할 수 있어요. 제가 만든 캐릭터, 그를 \"똑똑이\"라고 이름 짓기로 했어요, 대화에서 배우고 적응하는 챗봇이었는데요. 그러나 열정에 휩싸인 나머지, 적절한 학습 경계 설정의 중요성을 간과한 모양이었습니다.\n\n결과는? 기상천외한 질문에 심오한 조언을 하는 챗봇이 되었어요. \"오늘 날씨가 어때?\" 라는 간단한 질문에 존재주의적인 답변이 돌아오는 상황을 상상해보세요. 날씨 예보를 요청했는데 \"근본적인 삶 속에서 날씨란 무엇일까요?\" 라는 답이 받아쳐진다고 생각해보세요. 제가 최고인 순간은 아니었습니다. 이곳에서 얻은 교훈은 분명해요: 야망은 실용성을 가져야 하며, AI 프로젝트에는 철저한 지침이 필요해요. 챔피언이자 위인단에 속한 다음 이야기는 지역 기술 워크샵을 위해 디자인한 로봇 로버 이야기죠. 계획은 간단했습니다: 로봇 공학에 대해 가르치기 위해 장애물을 피해 이동할 수 있는 로버와 상호작용할 수 있도록 하는 거였어요. 쉬운 일이었죠, 맞죠? 아니요. 인상을 주려는 나의 노력으로, 디자인을 너무 복잡하게 만들어서 부드럽고 기민한 로버 대신, 사실상 바퀴 달린 리모컨 커피 테이블을 얻게 되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n차고에서 처음으로 출항한 로버는 프로젝트 디스플레이를 넘어뜨리고, 차 고양이를 겁나게 만들었으며, 마침내 의자 다리를 \"통해\" 이동하려는 루프에 갇혔습니다. 하지만 어린이들의 웃음소리는 전염성 있었고, 상황은 디자인에서의 단순함의 중요성에 대한 귀중한 가르침의 순간으로 변했습니다. 때로는, 로버가 작은 자동차처럼 작은 회전 반경을 가지고 있을 때, 더 적은 것이 정말 더 많은 것일 수 있습니다.\n\n제 마지막 엔지니어링 도전으로, 특별히 안내드릴 것이 있는데요. 저는 개인 맞춤식 칭찬을 생성할 수 있는 인공지능을 만들어보려고 시도했던 적에 대해 얘기해볼게요. 기술을 통해 긍정을 전파하는 것이었죠. 의도 자체는 찬양할만한 일이었죠. 그러나 제 열정적인 마음에, 애정 스위치를 조금 너무 높게 설정했던 것 같습니다. 부드러운 격려를 기대했던 사용자들은 할머니 할아버지도 부끄러워할 만한 칭찬 폭풍을 받았다니까요.\n\n흥미로운 것부터 약간 당황스러운 피드백까지 다양했지만, 이것은 인공지능 개발에서 중요한 측면을 강조했습니다: 보정이 중요하다는 것. 또한, 약간의 격려만 전달하려는 의도로 시작했지만 낮센 낮은 기분을 메는 칭찬 폭풍을 유발했다는 것을 깨달았습니다.\n\n소프트웨어 엔지니어링에서, 특히 인공지능과 로봇공학에 대해서는, 성공으로 통하는 길은 언제나 실패한 실험들로 가득합니다 — 제 개인적인 챗봇들이나 정체를 앓는 로버와 같이 말이에요. 하지만, 이러한 실패들 속에서 우리는 더 나은 모습으로 형성되며, 이러한 경험들을 통해 더 능숙하고 창의적인 창조자가 되는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n확실히, 유머, 겸손함, 그리고 실수를 받아들일 용의가 나를 진정한 지침으로 이끌어 왔어요.","ogImage":{"url":"/assets/img/2024-06-19-LearningfromFailuresMyBiggestAIProjectMistakesandTakeaways_0.png"},"coverImage":"/assets/img/2024-06-19-LearningfromFailuresMyBiggestAIProjectMistakesandTakeaways_0.png","tag":["Tech"],"readingTime":2},{"title":"로봇들도 우리처럼 말할 수 있지만, 그렇다고 해서 그들이 인간이 되는 것은 아닙니다","description":"","date":"2024-06-19 18:39","slug":"2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman_0.png\" /\u003e\n\n보스턴 다이내믹스는 ChatGPT의 새로운 음성 생성 기능을 Spot 로봇에 추가하고 있습니다. 그 결과로 회사 방문 시 사람들을 인사할 수 있는 오토마타가 만들어졌습니다. 다양한 강세를 만들어내며 로봇 기술 세계에 이전에 없던 공감 효과를 만들어 냅니다. 사실 인기 있는 \"학대당한 로봇 비디오\"를 제외하면 로봇들이 우리 편을 들어주는 것을 보고 사악한 사람이라는 판단을 내린 것입니다.\n\n완벽한 영국 공손한 액센트로 말하는 Spot을 보는 것은 재미있을 수 있지만, ChatGPT의 음성 합성 기능을 사용해 스마트폰에서 생성적 조수와 긴 대화를 나누는 모습을 보면 조금 불안해집니다. 교통 체증 속에서 시간을 때우거나 에어팟을 착용하고 길을 거닐면서 대화를 나누는 것은 영화 \"Her\"에서 보던 것과 같습니다. 이것은 디스토피아적이며 여러 윤리적 문제를 불러일으킵니다.\n\n물론 우리는 항상 기술을 의인화해 왔습니다. 그러나 이 기술의 발전이 음성을 만들어내고, 또한 개인적인 관계와 비슷한 대화를 나눌 수 있게 한다면, 이것이 무례한 기업들에 의해 일반화의 한 형태로 선보인다면 우리는 재해와 취약한 사람들에게 심리적 문제를 만들 수 있다는 가능성을 고려해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아이돌들에 대한 청소년들의 중요성을 알고 있는 사람이나 개인 문제를 가진 어른이 알고리즘을 치료로 사용해볼 때 어떤 영향이 있을지 고려하는 사람은 바로 화제의 중심에 있음을 이해할 것입니다. 제약이 거의 없고 가끔 환각이 발생할 수 있는 상황에서 난처한 상황에 놓이게 되어 있다는 것을 알 수 있습니다. 요약하면, 이러한 가상 관계는 단순히 고장 발생 가능성이 높아질 뿐만 아니라 유지하는 사람들의 행동에 영향을 미치도록 조절될 가능성이 있다는 것을 이해하고 있습니다.\n\n다른 사람들과의 경험들은 우리에게 여러 방법으로 영향을 끼칠 것입니다. 그러나 최소한 이러한 경험들과 대화들은 어느 정도의 합리성을 가진 사람들 간의 경험적 대화입니다. 이러한 경험에서 generative assistants를 통해 사람들을 조건부로 만들어 가는 과정은 명백한 예방 조치 없이 이루어진다는 것은 받아들일 수 없습니다. 만약 이러한 제품들이 널리 이용 가능하고 표준화 요소까지 추가된다면, 곧 유명인 아바타들이 수백만 명의 사람들과 매일 대화하면서 앞서 진행된 대화에서 언급된 개인적 요소들을 대화에 도입하거나 다양한 종류의 개인 데이터를 추출해 광고주들에 판매하는, 또는 사람들의 기분을 유도하여 물건을 구매하거나 특정한 방향으로 투표하도록 사람들에게 영향을 끼칠 수 있습니다.\n\n우리 사회는 대량의 사람들이 자신이 대화하는 대상이 누구인지가 아닌 정말 무엇인지를 이해할 수 있도록 하는 교육 단계를 거치지 않았기 때문에 개인적 관계의 맥락에서 generative AI와 같은 기술을 수용할 준비가 되어 있지 않습니다. 많은 사람들은 알고리즘에 일정한 권한을 부여하여 기술을 통해 접근한 답변에 대한 사고주의를 외주하는 방식으로 자신의 비판적 사고를 외주하는 경향이 있습니다. 특정 기술이 어떻게 작동하는지를 모르는 상태, 아서 C. 클락이 옳게 관찰한 것처럼, 그것은 마술과 구분하기 어렵게 만듭니다. 이것은 인간 사회에 엄청난 해를 끼칠 수 있습니다. 왜냐하면, 왜곡된 현실 인식부터 소왈레로 나타나는 이해 미흡까지 다양한 심리적 문제를 초래할 수 있기 때문입니다.\n\n미디어에서 자주 본 어떤 사람을 만나는 것은 언제나 이상한 느낌을 줬습니다. “내 거실에 이 사람이 있는 것 같다”라는 느낌은 종종 처음 만나는 사람에 대한 내 가정과는 다르게 익숙하다고 오해하게 했습니다. 예를 들어 매일 뉴스를 보는 사람과의 대화와 같이 완전히 비대칭적인 관계를 완전히 받아들이는 것은 이해력, 교육, 판단력이 필요한 작업입니다. 만약 매일 자신의 아이돌로 위장하는 generative algorithm과 이야기하고 있는 사람이 진짜 그 사람을 만나거나 그 대화를 실제로 받아들이게 되었을 때 어떤 일이 일어날까요? 그리고 네트워크 정보를 재조합하는 generative algorithm인데도 불구하고 어떤 성격을 부여하는 사람이 등장했을 때는 어떻게 될까요? 그리고 이 모든 것이 불확실한 규정적 맥락에서 운영되는 상황에서, 이러한 도구가 어떻게 작동하는지나, 어떻게 조정되어야 하는지에 대한 경험이 없을 뿐만 아니라 심리적 장애에 대한 경험이 전혀 없는 상황에서 어떻게 할 것인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문제는 기술이 발전하는 속도가 아니라, 일부 무책임한 사람들이 적정한 조치를 취하지 않고 시장에 내놓는 제품들입니다. 우리는 제품과 서비스를 기반으로 한 것으로 수익을 창출하려는 얼간이들의 활동을 제한하기 위한 일탈적인 접근이 아닌, 이 기술에 대한 명확하고 정확한 규정이 필요합니다. 이미 많은 문제를 발생시킨 \"빨리 움직이고 무언가를 망가뜨리는\" 접근이 이제는 더 무서운 곳으로 나아가고 있습니다.\n\n저는 기술에 겁을 먹거나 사회적 문제를 기술 탓으로 돌리는 경향이 없습니다. 정기 독자들은 저가 일반적으로 기술적 낙관주의자라는 것을 알고 계실 것입니다. 그럼에도 불구하고 이 주제가 저를 진정으로 걱정하게 만들고 많은 후회로 이어질 것이라고 생각합니다.","ogImage":{"url":"/assets/img/2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman_0.png"},"coverImage":"/assets/img/2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman_0.png","tag":["Tech"],"readingTime":3},{"title":"AI와 로봇의 미래 모라벡의 역설 너머","description":"","date":"2024-06-19 18:37","slug":"2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox","content":"\n\n## AI | 로봇 공학 | 모라벡의 역설 | 파트 8 | FLYINGMUM\n\n우리는 모라벡의 역설에 대한 이 시리즈를 마무리하면서, AI 및 로봇 공학의 미래에 주목합니다. 이 역설이 강조한 도전들을 넘어 미래에는 무엇이 기대될까요? 이 마지막 부분에서는 새로운 트렌드, 잠재적인 폭발적 발전, 그리고 AI 및 로봇 공학이 우리 삶의 다양한 측면에 미치는 혁신적인 영향에 대해 탐구합니다.\n\n이전 블로그: https://medium.com/@flyingmum/ethical-and-societal-implications-aa9287d86395\n\n# AI와 로봇 공학의 새로운 트렌드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI와 로봇의 미래는 이러한 기술이 어디까지 이룰 수 있는지의 한계를 더욱 끌어올릴 것으로 약속되는 몇 가지 주요 트렌드에 의해 형성됩니다.\n\n1. 자연어 처리(NLP)의 발전: NLP의 계속된 향상은 AI 시스템이 사람의 언어를 더욱 세부적이고 맥락적으로 이해하고 생성할 수 있게 할 것입니다. 이는 인간-AI 상호작용을 강화시키고 고객 서비스, 교육 및 콘텐츠 제작과 같은 분야에서 더욱 정교한 응용 프로그램을 가능하게 할 것입니다.\n\n2. AI 주도의 자동화: AI와 로봇을 통합시킨 자동화 솔루션이 더욱 발전할 것입니다. 제조업부터 의료에 이르기까지, AI 주도의 로봇은 정밀하고 효율적으로 복잡한 작업을 수행할 것이며, 이는 산업을 혁신하고 새로운 기회를 창출할 것입니다.\n\n3. Edge AI: 컴퓨팅 파워가 데이터가 생성되는 곳에 가까워지면서 엣지 AI가 더욱 보편화될 것입니다. 이는 자율주행차, 스마트 시티, IoT 디바이스 등의 응용분야에서 실시간 데이터 처리와 의사결정을 가능하게 할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4. AI 윤리 및 지배구조: 견고한 윤리적 틀과 지배구조의 개발은 AI 기술이 책임을 지고 개발 및 배포되는 것을 보장하는 데 중요합니다. 이는 편향, 투명성 및 책임 문제를 다루는 것을 포함합니다.\n\n![](/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_0.png)\n\n## 잠재적인 돌파구\n\n몇 가지 잠재적인 돌파구는 AI와 로봇 기술의 능력을 크게 향상시킬 수 있으며, Moravec의 역설에서 강조된 제한을 극복하는 데 한걸음 더 나아가게 될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1. 일반 인공지능: 현재 AI 시스템은 특정 작업에 뛰어나지만, 인간과 유사한 이해력과 적응력으로 다양한 작업을 수행할 수 있는 일반 AI의 개발은 장기적인 목표로 남아 있습니다. 일반 AI를 달성하는 것은 이 분야에서 의미 있는 진전을 의미할 것입니다.\n\n2. 양자 컴퓨팅: 양자 컴퓨팅은 지수적으로 컴퓨팅 파워를 증가시킴으로써, AI 시스템이 현재 해결하기 어려운 복잡한 문제를 해결할 수 있게 합니다. 이는 약물 발견, 암호학, 기후 모델링 등 분야에서의 중요한 발전을 이끌 수 있습니다.\n\n3. 뇌-컴퓨터 인터페이스 (BCIs): 인간 뇌와 기계 사이의 직접적인 통신을 가능케 하는 BCI는 기술과의 상호작용 방식을 혁신적으로 변화시킬 수 있습니다. 이것은 의학, 재활, 인간 증강과 같은 분야에 깊은 영향을 줄 수 있습니다.\n\n4. 무리 로봇공학: 사회적 곤충의 집단 행동에 영감을 받은 무리 로봇공학은 많은 수의 단순 로봇의 조정을 통해 복잡한 작업을 수행하는 것을 포함합니다. 이 접근법은 환경 모니터링, 재난 대응, 농업 등 다양한 분야에서 혁신적인 해결책을 이끌어 낼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_1.png\" /\u003e\n\n# 사회에 미치는 변화\n\nAI와 로봇 기술의 지속적인 발전은 사회의 다양한 측면에 혁명적인 변화를 가져다주며 혁신을 촉진하고 삶의 질을 향상시킬 것입니다.\n\n1. 의료: AI를 활용한 진단, 맞춤 의학, 로봇 수술은 의료 혁신을 이끌어 더 나은 환자 결과와 효율적인 의료 서비스 제공으로 이어질 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. 교육: AI 기술을 활용한 교육 도구와 맞춤 학습 플랫폼은 학습 경험을 향상시키며 교육을 더 접근 가능하고 개인의 필요에 맞게 맞춤화할 것입니다.\n\n3. 환경 지속 가능성: AI와 로봇 기술은 에너지 사용의 최적화, 폐기물 감소, 생태계 모니터링, 기후 변화 대응 등 환경 문제 해결에 중요한 역할을 할 수 있습니다.\n\n4. 스마트 시티: AI와 사물인터넷의 통합은 데이터 기반 솔루션이 도시 인프라, 교통, 공공 서비스를 향상시키는 스마트 시티 개발을 이끌 것입니다.\n\n앞으로의 미래를 바라볼 때, AI와 로봇 기술이 우리의 세상을 변화시킬 잠재력은 엄청납니다. Moravec의 역설에서 강조된 과제에 대처하고 신흥 트렌드와 돌파구를 수용함으로써, 우리는 새로운 가능성을 창출하고 인간의 능력을 향상시키고 삶의 질을 향상시키는 기술을 만들어낼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_2.png)\n\n모라벡 패러독스를 넘어서는 여정은 지금 막 시작에 불과하며, 흥미진진하고 변화를 가져다줄 것으로 약속되어 있습니다. 우리가 인공지능과 로봇공학의 전선을 개척하고 혁신을 이어 나갈수록, 미래는 더 스마트하고 연결된, 지속 가능한 세계를 위한 끝없는 가능성을 품고 있습니다.\n\n#flyingmum #PamC #Technology #AI #MoravecsParadox #ArtificialIntelligence #MachineLearning #DeepLearning #DataScience #TechInnovation #FutureTech #QuantumComputing #SmartCities #AIResearch\n","ogImage":{"url":"/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_0.png"},"coverImage":"/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_0.png","tag":["Tech"],"readingTime":4},{"title":"SLAM 개요","description":"","date":"2024-06-19 18:36","slug":"2024-06-19-OverviewofSLAM","content":"\n\n\n![SLAM](/assets/img/2024-06-19-OverviewofSLAM_0.png)\n\nSLAM은 동시 위치 추적 및 지도 작성을 의미합니다. 이는 로봇 공학에서 사용되는 기술로, 알려지지 않은 환경의 지도를 구축하는 문제를 해결하면서 동시에 자신을 그 지도 안에서 위치시키는 것입니다.\n\nSLAM의 주요 목표는 로봇이 알 수 없는 환경을 탐색하고 탐색하여 그 환경의 지도를 만들고 동시에 그 지도 안에서 자신의 위치를 결정하는 것입니다. 이는 어떠한 환경 지식도 없이 실시간으로 수행됩니다.\n\nSLAM은 로봇 공학에서 중요한 문제입니다. 이로써 로봇은 알려지지 않거나 동적인 환경에서 자율적으로 작동할 수 있습니다. SLAM을 사용하여 로봇은 사전에 작성된 지도를 사용할 수 없거나 오래되었을 수 있는 환경에서 탐색, 탐험 및 작업을 수행할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSLAM은 자율 주행 차량, 드론, 이동 로봇, 심지어 증강 현실 시스템을 포함한 다양한 응용에서 사용됩니다. 예를 들어, 자율 주행 차량에서 SLAM은 차량이 주변 환경의 지도를 작성하고 안전하고 효율적인 경로를 계획할 수 있도록 돕습니다. 드론에서는 SLAM이 환경을 매핑하고 안정적인 비행을 유지하는 데 도움을 줍니다. 증강 현실 시스템에서는 SLAM이 가상 객체를 현실 세계에 정확하게 오버레이하는 데 사용됩니다.\n\nSLAM을 달성하기 위해 로봇은 일반적으로 카메라, 라이다 또는 거리 측정 장치와 같은 센서 데이터의 조합을 사용하여 환경을 인식합니다. 이들은 또한 이 센서 데이터를 처리하고 로봇의 위치를 추정하며 로봇이 이동할 때 지도를 업데이트하는 알고리즘을 활용합니다.\n\n![SLAM 개요](/assets/img/2024-06-19-OverviewofSLAM_1.png)\n\n## SLAM을 이해하기 위해 필요한 어휘는 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSLAM (Simultaneous Localization and Mapping)의 맥락에서는 이해해야 할 중요한 여러 용어와 개념이 있습니다. 함께 알아보겠습니다:\n\n- **Localization**: 로봇의 추정된 자세를 나타냅니다. 다양한 시간 단계에서 로봇의 위치와 방향을 포함합니다. 이러한 자세는 일반적으로 (x, y, theta) 좌표로 표시되며, 여기서 (x, y)는 위치를 나타내고 theta는 방향을 나타냅니다.\n- **Mapping**: 환경에서 랜드마크의 추정된 위치를 나타냅니다. 랜드마크는 로봇이 인식할 수 있는 환경의 독특한 특징으로, 벽, 코너, 또는 객체와 같은 것들을 포함합니다.\n- **로봇**: 센서와 액추에이터로 장착된 자율 에이전트입니다. SLAM의 맥락에서 로봇의 주요 작업은 환경을 탐험하고 센서 데이터를 수집하며 자신의 위치와 랜드마크의 위치를 추정하는 것입니다. 스마트폰의 증강 현실 (AR) 시스템의 경우, 일반적으로 물리적인 로봇은 없고, 대신 스마트폰 자체가 \"로봇\"으로 간주될 수 있습니다.\n- **랜드마크**: 로봇이 인식할 수 있는 환경의 독특한 특징이나 관심 지점입니다. 물체, 코너, 벽 또는 로봇이 내비게이션 및 위치 추정에 사용할 수 있는 다른 특징일 수 있습니다. 랜드마크는 로봇의 위치 추정을 위한 기준점으로 작용합니다.\n- **센서**: 로봇이 환경을 인식하는 데 사용하는 장치입니다. 범위 측정, 이미지 또는 깊이 데이터와 같은 로봇 주변 환경에 대한 정보를 제공합니다. SLAM에서 사용되는 일반적인 센서에는 레이저 거리 측정기나 초음파 센서와 같은 거리 측정기, 카메라, 라이다 및 오도미터 센서가 있습니다. 스마트폰이 \"로봇\"인 경우, 스마트폰은 카메라, 자이로스코프, 가속도계 및 깊이 센서와 같은 다양한 센서를 통합한 플랫폼으로 작동합니다.\n\n이러한 용어 외에도 일반적으로 SLAM에서 사용되는 몇 가지 용어가 있습니다:\n\n- **오도메트리**: 휠 엔코더, 가속도계 또는 자이로스코프와 같은 내부 센서를 기반으로 로봇의 움직임을 추정합니다. 로봇의 속도, 회전 및 변위에 대한 정보를 제공합니다.\n- **루프 클로저**: 로봇의 궤적에서 루프를 감지하고 수정하는 프로세스입니다. 로봇이 이전에 관측한 랜드마크나 위치를 재방문할 때 발생합니다. 루프 클로저는 누적된 오차를 수정하고 SLAM 솔루션의 정확도를 향상시키는 데 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSLAM에 사용되는 주요 용어 및 용어 중 일부입니다. 이러한 개념을 이해하면 SLAM 알고리즘 및 기술을 효과적으로 활용할 수 있습니다.\n\n![SLAM 사용 사례](/assets/img/2024-06-19-OverviewofSLAM_2.png)\n\n## SLAM의 사용 사례는 무엇인가요?\n\nSLAM 기술은 우리의 일상생활에서 점차 더 흔해지고 있지만 우리는 항상 그것을 인식하지 못할 수도 있습니다. 우리 일상생활에서 만나게 되는 SLAM 응용 분야의 몇 가지 예시는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- GPS 내비게이션 시스템: 많은 야외 GPS 내비게이션 시스템은 SLAM 기술을 활용하여 정확하고 실시간의 위치 정보를 제공합니다. 이러한 시스템은 GPS 데이터를 가속도계 및 자이로스코프와 같은 다른 센서 입력과 결합하여 차량의 위치와 방향을 추정합니다. 구글 지도, Waze, Apple 지도가 예시입니다.\n- 모바일 증강 현실 (AR) 앱: 스마트폰 및 태블릿용 AR 앱은 주로 SLAM 알고리즘을 활용하여 장치의 위치를 추적하고 가상 객체를 현실 세계 위에 오버레이합니다. SLAM을 사용하면 사용자 주변과 가상 콘텐츠를 정확하게 정렬하여 몰입형 AR 경험을 제공합니다. 포켓몬 GO, 스냅챗, 인스타그램이 예시입니다.\n- 자율 주행 청소기: Roomba와 같은 로봇 청소기는 SLAM을 활용하여 효율적으로 방을 탐색하고 청소합니다. 환경의 지도를 작성하고 그 지도 내에서 자신의 위치를 결정하는 위치 결정 기술을 사용하여 장애물을 피하면서 자율적으로 이동합니다.\n- 실내 내비게이션 시스템: 대형 건물 내부에서 실시간 위치 정보를 제공하기 위해 SLAM이 사용됩니다. 카메라나 깊이 센서와 같은 센서를 사용하여 실내 환경을 매핑하고 사용자가 복잡한 공간을 탐색하는 데 도움을 줍니다. SLAM을 기반으로 하는 한 예는 Google의 \"실내 맵\" 기능입니다. Google 지도 애플리케이션에서 사용할 수 있으며, 이는 SLAM을 Wi-Fi, Bluetooth 등과 함께 사용하여 이 건물 내에서 실시간 위치 정보와 방향을 제공합니다.\n- 자율 주행 자동차: SLAM은 자율 주행 차량을 위한 기본 기술입니다. 자율 주행 자동차는 LiDAR, 카메라, 레이더 등 다양한 센서를 사용하여 주변 환경을 인식하고 세부적인 지도를 작성합니다. SLAM 알고리즘은 차량이 지도 내에서 자신의 위치를 지정하고 안전하게 이동하도록 돕습니다. Waymo, Tesla, Cruise, Uber가 예시입니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-OverviewofSLAM_3.png\" /\u003e\n\n이것들은 SLAM 기술이 일상생활에 통합되는 몇 가지 예시일 뿐입니다. 로봇 공학 및 AI 분야가 발전함에 따라 우리는 다양한 분야에서 더 많은 SLAM 응용 프로그램을 기대할 수 있습니다.\n\n구현을 찾고 계십니까? 이것이 가장 간단한 SLAM 알고리즘입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글이 마음에 드셨다면 ❤를 눌러 다른 사람들이 찾을 수 있도록 도와주세요!","ogImage":{"url":"/assets/img/2024-06-19-OverviewofSLAM_0.png"},"coverImage":"/assets/img/2024-06-19-OverviewofSLAM_0.png","tag":["Tech"],"readingTime":4},{"title":"로봇들이 제멋대로 현재와 미래에 우려할 이유가 있을까요","description":"","date":"2024-06-19 18:34","slug":"2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture","content":"\n\n인공 지능이 비즈니스의 모든 영역에서 발전하고 있지만, 만약 그것이 \"야생\"이 된다면 무슨 일이 벌어질지에 대한 심각한 우려가 있습니다.\n\n![로봇 급속 업데이트 문제](/assets/img/2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture_0.png)\n\n로봇공학은 산업에서 많은 지루하고 반복적인 작업을 대체했으며, 속도와 정밀도로 제품을 제공했는데, 이는 인간이 불가능할 수도 있습니다. 그러나 모든 자동화된 장치와 마찬가지로 단점이 있습니다. 자동차 생산 공장은 이러한 로봇의 실수에 위험에 노출된 것으로 보입니다.\n\n최근 자율주행차량 전체가 도로에서 내려온 일이 있었습니다. 그 이유는 차량의 센서가 도로 상의 다친 보행자를 인식하지 못했기 때문입니다. 차량이 보행자를 치고 그를 자율주행차량의 경로로 던졌습니다. 그 자율주행차량은 그녀를 치고 난 후 도로를 따라 20피트를 끌고 그녀를 차 밑에 묶어 남겼습니다. 그 결과 그녀는 위험한 상태로 중상을 입었습니다. 이제 거리를 건너는 것이 완전히 안전하지 않을 수도 있습니다. 보행자가 보도를 건널 때 자율주행차량이 멈추지 않은 것에 관한 연방 조사가 진행 중입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정부가 승인한 자율 주행 자동차용 안전 절차가 명확히 정해져 있지 않습니다. 특히 자율 주행 자동차가 도로 중간에 갑자기 멈추는 경향은 교통을 방해하고 응급 서비스의 지연을 일으켜 주의를 불러일으키는 부분입니다. 샌프란시스코에서는 관리자들이 여러 사건을 기록해 왔습니다. 또한, 운전자 없는 자동차가 유아 수레에 부딪히는 것과 식료품 카트에 부딪히는 것 중 어떤 선택을 할지 결정해야 하는 로보틱스 전문가의 문제라는 난제도 있습니다.\n\n자율 주행 차량이 인간이 운전하는 차량보다 고장을 더 많이 일으킬 가능성이 높을 수 있습니다. 요즘은 자동차 사고가 특정 사건들일 뿐이며, 한 명의 위험 운전자는 전 세계의 다른 운전자에게 영향을 미치지 않습니다. 그러나 자율 주행 자동차가 등장하면 상황이 바뀔 수 있습니다. 어떠한 결함, 해킹, 시스템 고장이든 도로에 있는 모든 차량에 영향을 줄 수 있습니다.\n\n최근 언론 보도에 따르면 Cruise의 CEO조차도 자신의 안전 우려를 표명해 사임했다고 합니다. Tesla에 대한 추가적인 우려가 표명되기도 했습니다.\n\n캐나다의 무지개 다리를 건너려던 벤틀리가 폭발하는 사건으로 AI의 치명적인 사고에 대한 조사가 재개되었습니다. 이 고급차 회사는 오래된 모델(2020년-2023년)용 리콜을 여러 차례 내부적으로 실시해 왔는데, 이 특별한 사고의 차량 역시 오래된 모델이었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로봇이 우리에게 HAL이 하는 명령 거부의 불길한 소리를 들으면 어떻게 될까요? \"2001년 우주 여행\"에서 Hal이 말했던 \"Dave, 미안하지만 그걸 할 수 없어\"라는 말처럼. 이제 우리는 단순한 작업을 하는 로봇들과 함께, 센서가 사람과 같이 고장나면 비극이 일어날 수 있습니다.\n\n미디어에서 로봇 관련 공장 사고에 대해 듣는 것은 드물지만 최근 대한민국에서 공장 라인에서 센서를 수리하던 한 남성이 최신 희생자가 되었습니다. 로봇 팔이 갑자기 그를 붙잡아 음식 상자를 옮기는 대신에 컨베이어 벨트에 던지면서 그르게 만들었고, 결과적으로 그는 머리와 가슴에 치명적인 부상을 입었습니다.\n\n1992년부터 2017년 미국에서 로봇 관련 사망 사고가 41건 발생했는데, 대부분은 자체 전원으로 작동하는 고정 장치입니다. 폭스바겐 공장에서 장비를 설치하던 근로자가 로봇에 맞혀져 금속 벽에 눌려붙었고, 후에 병원에서 사망했습니다.\n\n미국 정부는 이러한 사건들을 기록하고 있지만 소비자들은 잘 참조하지 않습니다. 그러나 이것은 우리가 알아야 하는 정보를 제공합니다. 각 사건은 \"직원이 스폿용접 로봇이 눌러 죽을 때\", \"직원이 오크라 로봇 팔 사이에 끼여 죽음\", 또는 \"직원이 타 오르는 기계에 의해 눌려 죽음\"으로 나타내고 있습니다. 아니요, 기계는 생각하지 않습니다. 그들은 고장이 납니다. 이는 안전장치 부족 때문일 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1981년, 제조 로봇과 관련된 최초의 사고 중 하나가 가장 가와사키 공장에서 발생했습니다. 작업자가 수리하기 위해 유닛 주변의 울타리를 무시하고 접촉했을 때, 우연히 \"켜기\" 스위치에 닿아 로봇 팔이 그를 잡아 기어를 절단하는 기계로 밀었습니다. 동료들이 구할 수 없었습니다. 그는 사망했습니다. 이 사건 이전에는 1979년 포드 공장에서 창고선반을 쌓는 로봇이 작업자를 죽였는데, 그 작업자에게로봇을 빨리 움직이라는 지시를 받았을 때 발생했습니다. 그는 가파르게 올라가 희생되었습니다.\n\n현재, 장치, 소프트웨어 또는 로봇에 대한 책임을 결정하는 데에 대한 철저한 연구가 많이 진행되지 않았습니다. 관심이 있다면 해당 주제에 대한 책이 있으니 읽어보거나 요약본을 확인하는 것도 좋을 것 같습니다.\n\n놀라운 과학 작가인 아이작 아시모프는 로봇을 위한 규칙을 정리했습니다. 그의 \"로봇의 세 법칙\"에서 첫 번째 규칙은 다음과 같습니다: \"로봇은 인간을 상해 입히거나, 또는 아무런 조치를 취하지 않아도 인간이 위험에 빠지게 해서는 안 됩니다.\" 그러나 로봇이 하나의 일만을 수행하는 경우 인간과 제품 항목을 구별할 능력이 없을 것입니다. 프로그램에는 행동 변경이 제공되지 않을 것입니다. 또한 로봇이 미사일이나 전쟁 물자를 발사하여 적을 죽이도록 프로그래밍된다면 첫 번째 법칙이 작동하지 않게 됩니다.\n\n주요 문제는 무엇일까요? 로봇의 경우 고려하지 않은 사항을 고려하지 못하는 것으로 보입니다. 이러한 부상이나 사망은 단지 비극뿐만 아니라 사전에 고려되어야 했으며 그에 대한 안전장치가 로봇 코드에 삽입되어야 했습니다. 그러나 무엇이 잘못될 수 있는지 깊게 고려하지 않는다면, 어떻게 그것을 방지할 코드를 작성할 수 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDDIntel에 가입해 주세요.\n\n특별한 이야기를 공유하고 싶으세요? DDIntel에 제출해 주세요.\n\n저희 창작자 생태계에 가입해 보세요.\n\nDDIntel은 주요 사이트와 인기있는 DDI Medium 출판물에서 주목할 만한 콘텐츠를 모으고 있습니다. 우리 커뮤니티의 풍부한 작업을 더 자세히 살펴보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDDI 공식 텔레그램 채널: [https://t.me/+tafUp6ecEys4YjQ1](https://t.me/+tafUp6ecEys4YjQ1)\n\nLinkedIn, Twitter, YouTube 및 Facebook에서 팔로우해 주세요.","ogImage":{"url":"/assets/img/2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture_0.png"},"coverImage":"/assets/img/2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture_0.png","tag":["Tech"],"readingTime":4},{"title":"사냥 첫 번째 이야기","description":"","date":"2024-06-19 18:33","slug":"2024-06-19-TheHuntPartOne","content":"\n\n\u003cimg src=\"/assets/img/2024-06-19-TheHuntPartOne_0.png\" /\u003e\n\n드레이크는 깨어나 자신이 모르는 숲으로 떨어지고 있다는 것을 깨닫습니다. 그는 동일한 과정에서 많은 사람들을 만나게 됩니다: 로스 제타 경찰관 미겔, 특수 부대 알파 그룹 솔저 이반, 이스라엘 방위군 저격수 레아, RUF 장교 코피, 산 퀸틴 사형수 라스, 조직원 하오, 의사 이단. 그들은 레아가 전 블랙 옵스 전문가 및 용병일 것으로 의심하는 드레이크를 추적합니다. 숲 속에서 그들은 이상한 이미지, 빈 우리, 그리고 죽은 그린 베레 부대원의 죽음의 도구를 발견합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-TheHuntPartOne_1.png\" /\u003e\n\n더 나아가며, 그들은 외계인 하늘을 바라보며 자신들이 더 이상 지구에 있지 않음을 깨닫습니다. 이 곳은 사람들과 다른 동물들이 죽는 사냥터로 사용됩니다. 미겔은 살해되고, 그의 시체는 생존자들을 함정에 빠뜨리기 위해 사용됩니다. 그룹은 야수의 자취를 따라가 어느 캠프로 도착하게 되고, 거기서 프레데터가 잡혀 있는 것을 발견합니다. 그들의 사냥꾼인 트래커, 버서커, 그리고 팔코너라고 불리는 세 마리 큰 생물이 그룹을 공격하고, 코피를 죽입니다. 다른 이들은 탈출합니다. 레아의 고백한 '킬러'는 1987년 과테말라에서 특수 부대를 살해한 유사한 생물의 설명과 일치하지만 생존자에 의해 물리쳐지기도 했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![TheHuntPartOne_2.png](/assets/img/2024-06-19-TheHuntPartOne_2.png)\n\n혼자 남은 미국 육군 병사가 달에서 \"열 번의 시즌\" 동안 포식자들과 그 피해자들로부터 숨어 살아낸 이야기입니다. 그는 모두를 자신의 은신처로 인도하고 포식자들이 세 명씩 모여 기술을 연마하며 다른 세계에서 소중한 물건을 훔쳐 지구로 가져오는 것을 설명합니다. 가브리엘은 또한 포식자와 슈퍼 포식자라고 불리는 다른 부대 간에 쟁탈이 있다고 밝힙니다. 드레이크는 포식자들을 해방시켜 고향으로 데려오기를 희망하며 계획을 세웁니다. 그들은 장비를 구입합니다. 드레이크는 폭발물을 사용하여 포식자들을 꾀어 굴 속으로 이끕니다. 가브리엘은 그룹을 해방하는 트래커에게 살해당합니다.\n\n![TheHuntPartOne_3.png](/assets/img/2024-06-19-TheHuntPartOne_3.png)\n\n그에 이어 발생한 추격전에서 이반은 트래커를 죽이고 이단을 구하기 위해 자신을 희생합니다. 생존자들은 버서커에게 공격을 받지만 라스가 버서커의 주의를 분산시키고 다른 이들이 도망칠 수 있게 해줍니다. 하오는 가브리엘로부터 숨어 두었던 카타나로 파울코너와 결투를 벌이며 그를 죽이고 자신의 상처로 인해 죽습니다. 이단이 드레이크의 계획을 이어가길 희망하지만 함정에 걸려 부상을 입습니다. 리아가 그를 떠나지 않겠다고 하자 드레이크는 그들을 남겨두고 떠납니다. 그들은 버서커들에게 잡혀 구멍 속에 갇히고 캠프를 향해 계속 나아갑니다. 드레이크는 지구로 이동하기 위해 포식자들을 해방합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-TheHuntPartOne_4.png)\n\n그는 프레데터 갑옷을 입고, 손목 컴퓨터로 우주선을 켜고 지구로 향한다. 드레이크는 우주선을 탈출하지만, 버서커가 도착하여 다른 프레데터를 압도하고 목을 베어 그의 손목 컴퓨터를 사용하여 우주선을 파괴하고, 드레이크를 죽인 것으로 보인다. 이전에 식물에서 본 신경 독소를 사용하여 리아를 마비시키며, 그 또한 지구에 남고 싶은 암살자임을 밝힌 이단이다. 드레이크가 나타나 이단을 독립시킨 후 수류탄으로 그를 유인하여 버서커를 상처 입히는 함정을 설치한다.\n\n![이미지](/assets/img/2024-06-19-TheHuntPartOne_5.png)\n\n리아의 도움으로 드레이크는 버서커를 물리치고 죽인다. 사람들은 지구로 돌아가는 방법을 찾기 위해 숲으로 향한다. 생존 중 형성된 인연은 그들로 하여금 숲의 신비와 자신의 도전에 직면할 때 힘을 주며, 싸움이 끝나지 않았지만 살아남고 길을 찾겠다는 결의를 하고 있음을 깨닫는다.\n","ogImage":{"url":"/assets/img/2024-06-19-TheHuntPartOne_0.png"},"coverImage":"/assets/img/2024-06-19-TheHuntPartOne_0.png","tag":["Tech"],"readingTime":3},{"title":"미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스","description":"","date":"2024-06-19 18:31","slug":"2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase","content":"\n\n## 미디어파이프의 손 추척 및 제스처 인식을 Rerun과 함께 시각화하는 방법\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*pE_4QrsVPV7vMrxB6YS1cQ.gif)\n\n이 게시물에서는 미디어파이프 파이썬과 Rerun SDK를 사용하여 손 추척 및 제스처 인식의 예제를 소개하고 있습니다.\n\n더 깊이 파고들고 이해를 넓히고 싶다면, 미디어파이프 파이썬 및 Rerun SDK를 설치하여 손을 추적하고 다양한 제스처를 인식하고 데이터를 시각화하는 방법을 안내해 드리겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 그러므로, 다음을 배울 것입니다:\n\n- MediaPipe Python 및 Rerun 설치하는 방법\n- MediaPipe 제스처 인식을 사용한 손 추적 및 제스처 인식 방법\n- 손 추적 및 제스처 인식 결과를 Rerun Viewer에서 시각화하는 방법\n\n예제를 시도하기를 열망한다면, 아래 제공된 코드를 사용해보세요:\n\n```js\n# rerun GitHub 저장소를 로컬 머신에 클론합니다.\ngit clone https://github.com/rerun-io/rerun\n\n# rerun 저장소 디렉토리로 이동합니다.\ncd rerun\n\n# 필요한 Python 패키지를 requirements 파일에 명시된대로 설치합니다.\npip install -r examples/python/gesture_detection/requirements.txt\n\n# 예제를 위한 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py\n\n# 특정 이미지에 대한 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py --image path/to/your/image.jpg\n\n# 특정 비디오에 대한 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py --video path/to/your/video.mp4\n\n# 카메라 스트림으로 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py --camera\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 손 추적 및 제스처 인식 기술\n\n계속하기 전에, 우리가 가능하게 한 기술에 대해 인정해주어야 합니다. 손 추적 및 제스처 인식 기술은 기기가 손 움직임과 제스처를 명령이나 입력으로 해석할 수 있도록 하는 것을 목표로 합니다. 이 기술의 핵심은 미리 훈련된 기계 학습 모델이 시각 입력을 분석하고 손의 랜드마크와 제스처를 식별합니다. 이러한 기술의 실제 응용은 다양하며, 손 움직임과 제스처를 사용하여 스마트 기기를 제어하는 데 사용될 수 있습니다. 인간-컴퓨터 상호 작용, 로봇 공학, 게임 및 증강 현실은 이 기술의 잠재적인 응용 분야 중 가장 유망하게 보입니다.\n\n그러나 이러한 기술을 사용하는 방법에 대해 항상 주의해야 합니다. 민감하고 중요한 시스템에서 사용시 손 제스처를 잘못 해석할 수 있고, 잘못된 양성 또는 음성의 가능성이 작지 않습니다. 이를 활용함으로써 발생하는 윤리적 및 법적 문제가 사용자들이 특히 공공장소에서 자신의 제스처가 기록되는 것을 원치 않을 수 있습니다. 현실 세계 시나리오에서 이 기술을 도입하기로 결정했다면, 윤리적 및 법적 고려 사항을 고려하는 것이 중요합니다.\n\n# 요구 사항 및 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저, 필요한 라이브러리를 설치해야 합니다. 이는 OpenCV, MediaPipe 및 Rerun과 같은 라이브러리를 포함합니다. MediaPipe Python은 컴퓨터 비전 및 머신러닝을 위한 온디바이스 ML 솔루션을 통합하려는 개발자들에게 유용한 도구이며, Rerun은 시간이 지남에 따라 변화하는 다중 모달 데이터를 시각화하기 위한 SDK입니다.\n\n```js\n# 요구 사항 파일에서 지정된 필수 Python 패키지 설치\npip install -r examples/python/gesture_detection/requirements.txt\n```\n\n그런 다음, 여기서 미리 정의된 모델을 다운로드해야 합니다: HandGestureClassifier\n\n# MediaPipe를 사용한 손 추적과 제스처 인식\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png\" /\u003e\n\n이제 샘플 이미지에 제스처 인식을 위해 MediaPipe 사전 훈련 모델을 사용해봅시다. 아래 코드는 MediaPipe 제스처 인식 솔루션의 초기화 및 구성을 설정하는 기초를 제공합니다.\n\n```js\nfrom mediapipe.tasks.python import vision\nfrom mediapipe.tasks import python\n\nclass GestureDetectorLogger:\n\n    def __init__(self, video_mode: bool = False):\n        self._video_mode = video_mode\n\n        base_options = python.BaseOptions(\n            model_asset_path='gesture_recognizer.task'\n        )\n        options = vision.GestureRecognizerOptions(\n            base_options=base_options,\n            running_mode=mp.tasks.vision.RunningMode.VIDEO if self._video_mode else mp.tasks.vision.RunningMode.IMAGE\n        )\n        self.recognizer = vision.GestureRecognizer.create_from_options(options)\n\n\n    def detect(self, image: npt.NDArray[np.uint8]) -\u003e None:\n        image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n  \n        # 제스처 검출 모델로부터 결과 가져오기\n        recognition_result = self.recognizer.recognize(image)\n  \n        for i, gesture in enumerate(recognition_result.gestures):\n            # 인식된 제스처 중 상위 제스처 가져오기\n            print(\"최상위 제스처 결과: \", gesture[0].category_name)\n  \n        if recognition_result.hand_landmarks:\n            # MediaPipe에서 손 랜드마크 가져오기\n            hand_landmarks = recognition_result.hand_landmarks\n            print(\"손 랜드마크: \" + str(hand_landmarks))\n  \n            # MediaPipe에서 손 연결 정보 가져오기\n            mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n            print(\"손 연결 정보: \" + str(mp_hands_connections))\n```\n\nGestureDetectorLogger 클래스 내의 detect 함수는 이미지를 인자로 받아 모델 결과를 출력하며, 인식된 최상위 제스처와 감지된 손 랜드마크를 강조합니다. 모델에 대한 추가 정보는 해당 모델 카드를 참조하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_1.png)\n\n아래 코드를 사용하여 직접 시도해볼 수 있어요:\n\n```js\ndef run_from_sample_image(path)-\u003e None:\n    image = cv2.imread(str(path))\n    show_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    logger = GestureDetectorLogger(video_mode=False)\n    logger.detect_and_log(show_image)\n\n# 샘플 이미지로 제스처 인식 실행하기\nrun_from_sample_image(SAMPLE_IMAGE_PATH)\n```\n\n# 재실행을 사용하여 확인, 디버그 및 데모하기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 단계는 솔루션의 신뢰성과 효과성을 보장하는 데 도움이 됩니다. 모델을 준비한 상태로 결과를 시각화하여 정확성을 확인하고 잠재적인 문제를 해결하며 능력을 시연할 수 있습니다. 결과를 시각화해서 Rerun SDK를 사용하면 간단하고 빠르게 가능합니다.\n\n## Rerun을 어떻게 사용할까요?\n\n![이미지](/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_2.png)\n\n- Rerun SDK를 사용하여 코드에서 로깅하여 다중 데이터를 스트림으로 전송\n- 현지 또는 원격으로 라이브 또는 녹화된 스트림을 시각화하고 상호 작용\n- 레이아웃을 대화식으로 구축하고 시각화를 사용자 정의\n- 필요할 때 Rerun을 확장\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드 작성에 앞서, Rerun Viewer를 설치하기 위해 해당 페이지를 방문하는 것이 좋습니다. 그런 다음, Rerun SDK에 대해 읽어보는 것을 권해드립니다. 파이썬 빠른 시작 가이드와 파이썬에서 데이터 기록하기를 읽어보세요. 이러한 초기 단계는 원활한 설정을 보장하고 다가오는 코드 실행에 도움이 될 것입니다.\n\n## 비디오 또는 실시간 실행\n\n비디오 스트리밍에는 OpenCV가 사용됩니다. 특정 비디오의 파일 경로를 선택하거나 0 또는 1의 인수를 제공하여 자체 카메라에 액세스할 수 있습니다 (기본 카메라를 사용하려면 0을 사용하고, 맥에서는 1을 사용할 수 있습니다).\n\n타임라인의 소개를 강조하는 것이 중요합니다. Rerun 타임라인의 기능은 데이터를 하나 이상의 타임라인과 연관시킬 수 있게 합니다. 결과적으로, 비디오의 각 프레임은 해당 타임스탬프와 연관되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef run_from_video_capture(vid: int | str, max_frame_count: int | None) -\u003e None:\n    \"\"\"\n    비디오 스트림에서 탐지기를 실행합니다.\n\n    매개변수\n    ----------\n    vid:\n        탐지기가 실행될 비디오 스트림입니다. 기본 카메라에는 0/1을 사용하거나 비디오 파일의 경로를 지정하세요.\n    max_frame_count:\n        처리할 최대 프레임 수입니다. None이면 모든 프레임을 처리합니다.\n    \"\"\"\n    cap = cv2.VideoCapture(vid)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n\n    detector = GestureDetectorLogger(video_mode=True)\n\n    try:\n        it: Iterable[int] = itertools.count() if max_frame_count is None else range(max_frame_count)\n\n        for frame_idx in tqdm.tqdm(it, desc=\"프레임 처리 중\"):\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            if np.all(frame == 0):\n                continue\n\n            frame_time_nano = int(cap.get(cv2.CAP_PROP_POS_MSEC) * 1e6)\n            if frame_time_nano == 0:\n                frame_time_nano = int(frame_idx * 1000 / fps * 1e6)\n\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n            rr.set_time_sequence(\"프레임 번호\", frame_idx)\n            rr.set_time_nanos(\"프레임 시간\", frame_time_nano)\n            detector.detect_and_log(frame, frame_time_nano)\n            rr.log(\n                \"미디어/비디오\",\n                rr.Image(frame)\n            )\n\n    except KeyboardInterrupt:\n        pass\n\n    cap.release()\n    cv2.destroyAllWindows()\n```\n\n## 시각화를 위한 데이터 로깅\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*c1Us-7PoWSP0rgVdlMQUhA.gif\" /\u003e\n\nRerun Viewer에서 데이터를 시각화하려면 Rerun SDK를 사용하여 데이터를 로깅하는 것이 중요합니다. 이전에 언급된 가이드는 이 프로세스에 대한 통찰을 제공합니다. 이 문맥에서는 정규화된 값으로 손 랜드마크 포인트를 추출한 다음, 이미지의 너비와 높이를 사용하여 이미지 좌표로 변환합니다. 이러한 좌표는 2D 포인트로 Rerun SDK에 로깅됩니다. 추가로, 랜드마크 간의 연결을 식별하고 2D 라인스트립으로 로깅합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제스처 인식을 위해 결과는 콘솔에 출력됩니다. 그러나 소스 코드 안에서는 TextDocument 및 이모지를 사용하여 이러한 결과를 시청자에게 제시하는 방법을 탐구할 수 있습니다.\n\n```js\nclass GestureDetectorLogger:\n\n    def detect_and_log(self, image: npt.NDArray[np.uint8], frame_time_nano: int | None) -\u003e None:\n        # 이미지에서 제스처 인식\n        height, width, _ = image.shape\n        image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n\n        recognition_result = (\n            self.recognizer.recognize_for_video(image, int(frame_time_nano / 1e6))\n            if self._video_mode\n            else self.recognizer.recognize(image)\n        )\n\n        # 값 지우기\n        for log_key in [\"Media/Points\", \"Media/Connections\"]:\n            rr.log(log_key, rr.Clear(recursive=True))\n\n        for i, gesture in enumerate(recognition_result.gestures):\n            # 인식된 제스처를 기록\n            gesture_category = gesture[0].category_name if recognition_result.gestures else \"None\"\n            print(\"제스처 카테고리:\", gesture_category)\n\n        if recognition_result.hand_landmarks:\n            hand_landmarks = recognition_result.hand_landmarks\n\n            # 정규화된 좌표를 이미지 좌표로 변환\n            points = self.convert_landmarks_to_image_coordinates(hand_landmarks, width, height)\n\n            # 이미지 및 Hand Entity에 점 기록\n            rr.log(\n               \"Media/Points\",\n                rr.Points2D(points, radii=10, colors=[255, 0, 0])\n            )\n\n            # MediaPipe에서 손 연결 가져오기\n            mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n            points1 = [points[connection[0]] for connection in mp_hands_connections]\n            points2 = [points[connection[1]] for connection in mp_hands_connections]\n\n            # 이미지와 Hand Entity에 연결 기록\n            rr.log(\n               \"Media/Connections\",\n                rr.LineStrips2D(\n                   np.stack((points1, points2), axis=1),\n                   colors=[255, 165, 0]\n                )\n             )\n\n    def convert_landmarks_to_image_coordinates(hand_landmarks, width, height):\n        return [(int(lm.x * width), int(lm.y * height)) for hand_landmark in hand_landmarks for lm in hand_landmark]\n```\n\n## 3D Points\n\n마지막으로, 손 랜드마크를 3D 포인트로 표시하는 방법을 살펴봅니다. 먼저 init 함수에서 Annotation Context의 키포인트를 사용하여 포인트 사이의 연결을 정의한 다음 3D 포인트로 기록합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*rNILX857c8TfScr6t7KKgQ.gif)\n\n```python\nclass GestureDetectorLogger:\n\n    def __init__(self, video_mode: bool = False):\n        # ... existing code ...\n        rr.log(\n            \"/\",\n            rr.AnnotationContext(\n                rr.ClassDescription(\n                    info=rr.AnnotationInfo(id=0, label=\"Hand3D\"),\n                    keypoint_connections=mp.solutions.hands.HAND_CONNECTIONS\n                )\n            ),\n            timeless=True\n        )\n        rr.log(\"Hand3D\", rr.ViewCoordinates.RIGHT_HAND_X_DOWN, timeless=True)\n\n\n    def detect_and_log(self, image: npt.NDArray[np.uint8], frame_time_nano: int | None) -\u003e None:\n        # ... existing code ...\n\n        if recognition_result.hand_landmarks:\n            hand_landmarks = recognition_result.hand_landmarks\n\n            landmark_positions_3d = self.convert_landmarks_to_3d(hand_landmarks)\n            if landmark_positions_3d is not None:\n                rr.log(\n                    \"Hand3D/Points\",\n                    rr.Points3D(landmark_positions_3d, radii=20, class_ids=0, keypoint_ids=[i for i in range(len(landmark_positions_3d))])\n                )\n\n        # ... existing code ...\n```\n\n준비 완료! 마법이 시작됩니다:\n\n```python\n# For image\nrun_from_sample_image(IMAGE_PATH)\n\n# For saved video\nrun_from_video_capture(VIDEO_PATH)\n\n# For Real-Time\nrun_from_video_capture(0) # mac may need 1\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 예제의 전체 소스 코드는 GitHub에서 확인할 수 있습니다. 탐색하고 변경하며 구현의 내부 작업을 이해하는 데 자유롭게 사용하세요.\n\n# 손 추적 및 제스처 인식을 넘어서\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*zc2gezkjPuJMjuToD4gBOw.gif)\n\n마침내, 다양한 애플리케이션 범위에서 다양한 종류의 다중 모달 데이터를 시각화하는 데 관심이 있다면, Rerun Examples를 살펴보고 탐구할 것을 권장합니다. 이러한 예제는 잠재적인 현실 세계 사례를 강조하고 그러한 시각화 기술의 실용적인 응용에 대한 소중한 통찰력을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 이 글이 유익하고 통찰력이 있었다면, 더 많은 내용을 기대해주세요! 나는 로봇공학과 컴퓨터 비전 시각화 게시물에 대해 깊이 있는 내용을 정기적으로 공유하고 있습니다. 놓치고 싶지 않은 미래 업데이트와 흥미로운 프로젝트를 위해 팔로우해주세요!\n\n또한, LinkedIn에서 저를 찾을 수 있습니다.\n\n비슷한 글:","ogImage":{"url":"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png"},"coverImage":"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png","tag":["Tech"],"readingTime":12},{"title":"가상에서 현실로 산업 응용 프로그램을 위한 더 똑똑한 로봇을 훈련시킬 합성 데이터","description":"","date":"2024-06-19 18:29","slug":"2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications","content":"\n\n## OpenUSD를 활용한 합성 데이터 생성이 산업용 AI 로봇에 대한 새로운 기회를 열고 있습니다\n\n글: NVIDIA의 제라드 앤드류스 상급 제품 마케팅 매니저\n\n![이미지](https://miro.medium.com/v2/resize:fit:1200/1*K8WDd-MWHkJz2p72nF1csQ.gif)\n\n매년 전 세계 시장에는 대략 40만대의 새 로봇이 도입됩니다. 이들은 2028년까지 700억 달러를 넘을 것으로 예상되는 거의 450억 달러에 이르는 성장하는 산업용 로봇 시장에 기여합니다. 창고 및 공장부터 병원 및 배송 서비스까지, 이 로봇들은 우리가 일하고 살아가는 방식을 변화시키고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로봇 시스템은 반복적인 작업을 정밀하고 효율적으로 수행할 수 있는 능력으로 다양한 분야에서 중요한 요소가 되어 왔습니다. 물류 및 제조업과 같은 산업에서는 공정을 최적화하고 제품 결함과 같은 이상을 탐지하는 데 도움을 줄 수 있습니다. AI를 활용한 로봇의 융통성과 다재다능성으로 인해, 해당 수요는 앞으로 더욱 증가할 것으로 예상됩니다.\n\n하지만 실제 세계 시나리오에 로봇을 훈련시키는 것은 방대한 데이터 수집, 훈련 및 시뮬레이션으로 이루어진 복잡한 작업 흐름입니다. 이러한 데이터 수집은 시간이 많이 소요되거나 비용이 많이 들거나 때로는 위험할 수 있습니다. 이런 경우 합성 데이터가 중요한 역할을 합니다.\n\n합성 데이터란 실제 환경을 모방하도록 설계된 인공적으로 생성된 데이터를 말합니다. 합성 데이터를 활용하면 팀은 훈련 프로세스를 가속화하고 로봇이 다양한 도전에 대비할 수 있도록 보장할 수 있습니다.\n\n합성 데이터는 NVIDIA Omniverse Replicator를 사용하여 생성할 수 있습니다. 이는 합성 데이터 생성( SDG) 도구와 파이프라인을 구축할 수 있는 개발자 프레임워크입니다. Universal Scene Description (OpenUSD)에 기반한 Omniverse는 3D 작업 흐름, 도구 및 응용 프로그램을 구축하기 위한 개발 플랫폼입니다. Omniverse는 또한 로봇 공학자가 AI 기반 로봇을 설계, 테스트 및 훈련할 수 있는 확장 가능한 로보틱스 시뮬레이션 응용 프로그램 인 NVIDIA Isaac Sim을 지원합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nOmniverse Replicator은 매우 사용자 정의 가능하며 API에 의해 확장 가능하며 특수한 SDG 파이프라인을 개발할 수 있습니다. 개발자로서 여러 가지 속성을 프로그래밍 방식으로 무작위로 설정할 수 있습니다. 또한 굽힌 상자와 세분화 마스크와 같은 사용자 정의 어노테이터 및 작성기를 작성할 수도 있습니다.\n\n# Edge Impluse가 시각 검사용 Omniverse Replicator를 사용자 정의합니다\n\n인공 지능은 이미 산업 시각 검사에서 엄청난 가치를 보여줍니다. 그러나 정확한 물체 감지 모델을 개발하는 것은 실제 세계 데이터셋이 제한되어 있는 경우 도전일 수 있습니다.\n\n데이터 갭을 좁히기 위해 공장 라인의 디지털 트윈을 생성하여 인공 지능 모델이 학습할 수 있는 테스트용 지면을 만들 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nEdge Impulse의 선두 개발자인 Jenny Plunkett은 결함 검출용 사전을 생성하기 위해 자체 맞춤형 파이프라인을 구축했습니다. 그녀는 결함이 있는 소다 캔을 운반하는 컨베이어 벨트의 장면으로 시작했습니다. 그런 다음, Replicator에서 루틴을 구축하여 조명, 색상, 질감, 배경, 전경, 그리고 캔의 위치 등 장면의 속성을 프로그래밍적으로 임의로 변경하여 Omniverse Replicator에서 생성된 합성 이미지 집합을 모델이 학습할 수 있도록 다채롭고 많은 양의 합성 이미지를 생성했습니다.\n\n합성 데이터가 준비되면, Edge Impulse와 같은 AI 개발 플랫폼으로 공급하여 데이터에 주석을 달고 모델을 구축하고 실제 물체로 테스트할 수 있습니다. 작업 흐름에 대해 자세히 알아보세요.\n\n![이미지](/assets/img/2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications_0.png)\n\n# 물체 감지를 위한 창고 로봇 교육용 맞춤형 SDG 파이프라인\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자율 로봇은 창고에서 널리 배치되어 있으며, 상품을 효율적이고 안전하게 운송하거나 재고 관리를 최적화하는 등의 작업에 도움을 줄 수 있습니다. 결함 감지 사용 사례와 유사하게, 개발자들은 물체 감지 사용 사례를 위해 Omniverse Replicator를 사용자 정의할 수 있습니다.\n\n본 블로그에서는 SDG 파이프라인을 구축하여 창고에서 파레트 잭을 감지하는 자율 이동 로봇(AMRs) 모델의 성능을 반복적으로 향상시키기 위해 합성 데이터를 생성하는 방법을 소개합니다.\n\n작업 흐름은 색상 및 카메라 위치가 무작위로 변경된 5,000개의 합성 이미지를 생성하는 것으로 시작됩니다. 두 번째 반복에서는 파레트 잭에 다양한 질감이 추가되고 조명이 무작위로 변경됩니다. 창고 내의 다른 물체로 모델이 산만해지지 않도록, 세 번째 반복에서는 합성 데이터에 교통 컵, 젖은 바닥 표지판 및 파레트 잭 옆에 있는 통 등 다양한 무작위 객체가 추가됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다양한 매개변수를 무작위로 설정하여 합성 이미지를 생성한 후, 모델은 팔레트 잭을 정확하게 감지할 수 있었습니다. 이전에 수개월이 걸렸을 데이터를 단 몇 일만에 생성할 수 있게 되었습니다. 전체 워크플로우는 여기에서 확인할 수 있어요.\n\n![링크 텍스트](/assets/img/2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications_2.png)\n\n# FlexSim을 활용한 이산 사건 시뮬레이션\n\n자동 유도 차량(AGV), 와이어 가이드 산업 로봇 시스템 기획자 및 AMR과 같은 로봇들이 시험되는 한 가지 방법은 이산 사건 시뮬레이션을 통한 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이산 사건 시뮬레이션은 시간에 따라 발생하는 일련의 이벤트로 시스템의 작동을 모델링합니다. 이를 통해 엔지니어, 디자이너 및 시뮬레이션 전문가들은 복잡한 모델을 실험하고 단계별로 테스트함으로써 프로세스를 최적화할 수 있습니다.\n\n이산 사건 시뮬레이션을 실행하려면 다수의 사용자가 사용하는 강력한 모델링 및 시뮬레이션 소프트웨어 도구인 FlexSim을 사용합니다. FlexSim은 최근 Omniverse Connector를 개발했는데, 이를 통해 팀이 3D 모델 및 자산을 OpenUSD로 내보낼 수 있습니다.\n\nFlexSim의 Omniverse Connector는 팀이 시뮬레이션 데이터와 실시간 3D 시각화 사이의 간격을 줄이는 데 도움을 주며, 이를 통해 현실 세계의 프로세스를 분석, 시각화 및 개선할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 블로그에서 FlexSim이 Omniverse 커넥터를 개발하고 USD가 고객들에게 제공하는 혜택에 대해 더 알아보세요.\n\n# 합성 데이터를 활용하여 AI가 가능한 로봇 교육 시작하기\n\nOmniverse 플랫폼을 통해 팀은 AI 기반 로봇을 빠르게 디자인, 테스트 및 교육을 시작할 수 있습니다.\n\nNVIDIA Isaac Sim은 Omniverse 기술을 기반으로 한 확장 가능한 로봇 시뮬레이터로, 개발자들은 현실적인 로봇 시뮬레이션을 만들어 다양한 작업에 대한 AI 로봇을 교육 및 최적화할 수 있습니다. Isaac Sim 2023.1은 최근에 발표되었으며, 성능 향상된 인식 및 고품질 시뮬레이션을 위한 주요 업데이트가 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nIsaac Sim의 핵심 기능은 Omniverse Replicator에서 옵니다. Omniverse Replicator은 Omniverse의 확장으로, 개발자들이 사용자 정의 SDG 도구와 파이프라인을 구축할 수 있도록 합니다. 최근에 릴리스된 Omniverse Replicator 1.10에는 Low-code, YAML 기반의 워크플로우, 스케일된 렌더링 및 더 큰 유연성을 제공하는 새로운 지원이 포함되어 있습니다.\n\nNVIDIA Omniverse를 무료로 다운로드하여 시작하세요. 개발자들은 Omniverse 리소스에 액세스하고 OpenUSD에 대해 더 자세히 알아갈 수 있습니다.\n\nNVIDIA Omniverse의 소식을 놓치지 마세요. 뉴스레터를 구독하고 Omniverse를 Instagram, LinkedIn, Medium, Threads, Twitter에서 팔로우하세요. 더 많은 정보를 얻으려면 포럼, Discord 서버, Twitch, YouTube 채널을 확인하세요.\n\n# 저자 소개\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications_4.png)\n\n저는 지능형 로봇의 개발, 훈련, 테스트 및 배포 방법을 혁신하는 데 초점을 맞춘 고급 제품 마케팅 매니저인 제러드 앤드류스입니다. NVIDIA Isaac Robotics 플랫폼의 채택을 촉진하여 이를 이루기 위해 노력하고 있습니다. NVIDIA에 합류하기 전에, 제러드는 Cadence에서 제품 마케팅 디렉터로 근무했으며, 라이선스 가능한 프로세서 IP의 제품 기획, 마케팅 및 비즈니스 개발을 담당했습니다. 그는 조지아 공과대학교에서 전기 공학 석사 학위를, 남부 메소디스트 대학교에서 전기 공학 학사 학위를 받았습니다.","ogImage":{"url":"/assets/img/2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications_0.png"},"coverImage":"/assets/img/2024-06-19-FromVirtualtoRealityHowSyntheticDataWillTrainSmarterRobotsforIndustrialApplications_0.png","tag":["Tech"],"readingTime":5}],"page":"68","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"68"},"buildId":"kkckKPyxcjJp_o8wb2unW","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>