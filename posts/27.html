<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>itposting</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///posts/27" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="itposting" data-gatsby-head="true"/><meta property="og:title" content="itposting" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///posts/27" data-gatsby-head="true"/><meta name="twitter:title" content="itposting" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-1532cbf2955c0c6a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_buildManifest.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="데이터 과학과 머신러닝에서 낮게 달린 과일 오류에 대해 알아보기" href="/post/2024-06-22-Thelow-hanging-fruitfallacyindatascienceandmachinelearning"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 과학과 머신러닝에서 낮게 달린 과일 오류에 대해 알아보기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-Thelow-hanging-fruitfallacyindatascienceandmachinelearning_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 과학과 머신러닝에서 낮게 달린 과일 오류에 대해 알아보기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">데이터 과학과 머신러닝에서 낮게 달린 과일 오류에 대해 알아보기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="텐서플로우의 슈퍼파워 TensorFlow 사용 가이드" href="/post/2024-06-22-TensorTroubleNoMoreYourGuidetoTensorFlowsSuperpower"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="텐서플로우의 슈퍼파워 TensorFlow 사용 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-TensorTroubleNoMoreYourGuidetoTensorFlowsSuperpower_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="텐서플로우의 슈퍼파워 TensorFlow 사용 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">텐서플로우의 슈퍼파워 TensorFlow 사용 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="구글의 최신 알고리즘으로 벡터 데이터베이스 검색 속도 급상승" href="/post/2024-06-22-GooglesNewAlgorithmsJustMadeSearchingVectorDatabasesFasterThanEver"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="구글의 최신 알고리즘으로 벡터 데이터베이스 검색 속도 급상승" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-GooglesNewAlgorithmsJustMadeSearchingVectorDatabasesFasterThanEver_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="구글의 최신 알고리즘으로 벡터 데이터베이스 검색 속도 급상승" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">구글의 최신 알고리즘으로 벡터 데이터베이스 검색 속도 급상승</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AI 정렬 문제의 해결 방안" href="/post/2024-06-22-Ontheproblemofalignment"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AI 정렬 문제의 해결 방안" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-Ontheproblemofalignment_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AI 정렬 문제의 해결 방안" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">AI 정렬 문제의 해결 방안</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ChatGPT-4를 능가하는 Mixture-of-Agents의 비밀" href="/post/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ChatGPT-4를 능가하는 Mixture-of-Agents의 비밀" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ChatGPT-4를 능가하는 Mixture-of-Agents의 비밀" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">ChatGPT-4를 능가하는 Mixture-of-Agents의 비밀</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="PyTorch 모델 학습 속도를 높이는 간단한 방법들" href="/post/2024-06-22-SimpleWaystoSpeedUpYourPyTorchModelTraining"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="PyTorch 모델 학습 속도를 높이는 간단한 방법들" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-SimpleWaystoSpeedUpYourPyTorchModelTraining_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="PyTorch 모델 학습 속도를 높이는 간단한 방법들" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">PyTorch 모델 학습 속도를 높이는 간단한 방법들</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="초고속 행렬 곱셈 필요 없는 LLMs대규모 언어 모델 드디어 등장" href="/post/2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="초고속 행렬 곱셈 필요 없는 LLMs대규모 언어 모델 드디어 등장" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="초고속 행렬 곱셈 필요 없는 LLMs대규모 언어 모델 드디어 등장" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">초고속 행렬 곱셈 필요 없는 LLMs대규모 언어 모델 드디어 등장</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="엔티티-리졸브드 지식 그래프란 - 2024 최신 기술 분석 및 사용 방법" href="/post/2024-06-22-Entity-ResolvedKnowledgeGraphs"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="엔티티-리졸브드 지식 그래프란 - 2024 최신 기술 분석 및 사용 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-Entity-ResolvedKnowledgeGraphs_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="엔티티-리졸브드 지식 그래프란 - 2024 최신 기술 분석 및 사용 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">엔티티-리졸브드 지식 그래프란 - 2024 최신 기술 분석 및 사용 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="거대한 언어 모델 마스터하는 방법" href="/post/2024-06-22-MasterLargeLanguageModels"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="거대한 언어 모델 마스터하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-MasterLargeLanguageModels_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="거대한 언어 모델 마스터하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">거대한 언어 모델 마스터하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="대형 언어 모델LLM 작동 방식 완벽 가이드 기초부터 고급까지" href="/post/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대형 언어 모델LLM 작동 방식 완벽 가이드 기초부터 고급까지" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대형 언어 모델LLM 작동 방식 완벽 가이드 기초부터 고급까지" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">IT Posting</span></div><strong class="PostList_title__loLkl">대형 언어 모델LLM 작동 방식 완벽 가이드 기초부터 고급까지</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 22, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/21">21</a><a class="link" href="/posts/22">22</a><a class="link" href="/posts/23">23</a><a class="link" href="/posts/24">24</a><a class="link" href="/posts/25">25</a><a class="link" href="/posts/26">26</a><a class="link posts_-active__YVJEi" href="/posts/27">27</a><a class="link" href="/posts/28">28</a><a class="link" href="/posts/29">29</a><a class="link" href="/posts/30">30</a><a class="link" href="/posts/31">31</a><a class="link" href="/posts/32">32</a><a class="link" href="/posts/33">33</a><a class="link" href="/posts/34">34</a><a class="link" href="/posts/35">35</a><a class="link" href="/posts/36">36</a><a class="link" href="/posts/37">37</a><a class="link" href="/posts/38">38</a><a class="link" href="/posts/39">39</a><a class="link" href="/posts/40">40</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"데이터 과학과 머신러닝에서 낮게 달린 과일 오류에 대해 알아보기","description":"","date":"2024-06-22 21:13","slug":"2024-06-22-Thelow-hanging-fruitfallacyindatascienceandmachinelearning","content":"\n\n## 지속적인 성공을 위한 유연한 데이터 과학 솔루션 구축 방법\n\n![이미지](/assets/img/2024-06-22-Thelow-hanging-fruitfallacyindatascienceandmachinelearning_0.png)\n\n비즈니스 전략에서 특히 데이터 과학과 머신 러닝(ML) 분야에서 쉽게 얻을 수 있는 열매에 대한 매력은 강력합니다. \"낮게 매달린 열매\"라는 용어는 작은 노력으로 상당한 보상을 얻을 수 있는 쉽게 달성할 수 있는 작업이나 목표를 의미합니다. 그러나 이러한 작업에 우선순위를 두면 때로는 조직을 '낮게 매달린 열매의 오류'로 알려진 함정으로 이끌 수 있습니다. 이 오류가 인식되고 대처되지 않으면, 조직은 후속 과제, 복잡성 및 자원 요구에 대한 과소평가로 이어져 데이터 과학 및 머신 러닝 계획에서 중대한 후퇴로 이어질 수 있습니다.\n\n## 오류 이해하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 과학과 기계 학습 관점에서 저해하는 낮은 과일 요오쇼는 일반적으로 여러 단계를 거치며 진행됩니다. 각 단계는 이해하고 탐색하는 데 중요합니다.\n\n- 초기 성공: 기관은 가장 쉬운 문제를 식별하고 해결하여 즉시 높은 수익을 약속하는 데이터 과학 여정을 시작합니다. 이러한 문제들은 종종 간단한 분석 방법을 필요로 하며 중요한 통찰력이나 성능 향상을 제공합니다. 이러한 프로젝트의 성공은 자신감을 높이고 추가 투자를 정당화합니다.\n- 복잡성 확대: 초기 성공에 힘입어 기관은 더 복잡한 문제에 도전합니다. 그러나 초기 어려움과 달리 나중의 도전은 그다지 간단하지 않습니다. 이는 더 복잡한 데이터 세트와 복잡한 데이터 거버넌스 문제를 포함하며, 더 정교한 모델링 기법이 필요하거나 명확한 목표가 없을 수 있습니다.\n- 부적절한 접근: 초기 프로젝트에서 작동한 간단한 도구와 기술은 복잡한 문제를 해결하는 데 충분하지 않을 수 있습니다. 이 단계에서 기관은 고급 AI 방법에 대한 가파른 학습 곡선, 프로젝트 기간 연장, 증가한 비용 및 높은 실패 가능성과 마주할 수 있습니다.\n- 전략적 불일치: 동일한 방식으로 진행하는 것은 전략적 실수로 이어질 수 있습니다. 문제가 더 복잡해지면 문제를 해결함으로써 얻는 혜택은 자주 줄어들고, 문제를 해결하기 위해 필요한 노력은 증가할 수 있습니다. 이 불일치는 기관이 자원을 비효율적으로 할당하게 하고, 더 나은 수익을 제공하는 다른 전략적 계획에 집중하지 않게 할 수 있습니다. 지연은 이사들이 해결 방안에 신뢰를 상실하고 AI 기술에 대한 투자를 줄이는 원인이 될 수도 있습니다. 나는 과거에 관측된 AI 윈터에 이것이 기여한다고 주장합니다.\n\n## 다양한 데이터 과학 및 기계 학습 사례\n\n전형적인 시나리오는 기업이 ML을 사용하여 이메일 마케팅 캠페인을 최적화하는 것일 수 있습니다. 이것은 상대적으로 간단한 문제로, 사용 가능한 데이터와 성공 지표가 명확합니다. 그러나 회사가 고객 이탈을 예측하거나 공급망을 최적화하는 비슷한 기술을 적용하려고 하면, 초기 모델은 구조화되고 깨끗한 데이터를 처리했지만 고차원, 노이즈가 많고 구조화되지 않은 데이터를 처리하는 데 적합하지 않게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 저효과적인 방법을 사용하지 않고 일반화된 방법으로 낮은 과일 나무 잘못을 개선하기\n\n일반화된 방법을 채택하는 것은 데이터 과학과 머신 러닝에서 낮은 과일 나무 잘못을 개선하는 데 효과적인 전략 중 하나입니다. 이 방법은 처음에는 더 복잡하고 시간이 많이 소요되지만 강력하고 유연하여 간단한 문제에서 복잡한 문제까지 다양한 문제를 해결할 수 있는 솔루션을 개발하는 것을 포함합니다. 일반화된 솔루션은 종종 낮은 과일 나무 잘못과 관련된 함정을 피하는 좋은 방법입니다.\n\n## 일반화된 솔루션 개발\n\n이 접근 방식의 핵심은 조직 내 다른 유형의 데이터 도전에 쉽게 적응하거나 확장할 수 있는 모델 및 방법론을 만드는 것입니다. 이는 보다 범용적인 머신 러닝 모델에 투자하거나 다양한 use case에 걸쳐 강력한 데이터 파이프라인을 구축하는 것을 의미할 수 있습니다. 여기서 주요 이점은 이러한 시스템이 한 번 구축되면 중요한 재구성 없이 반복해서 활용될 수 있으므로 후속 문제의 해결을 가속화하고 전체 전달 비용을 줄일 수 있다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 일반화된 방법 구현 단계\n\n- 고급 도구 및 기술에 투자: 높은 품질의 확장 가능한 도구 및 기술에 초기 투자는 처음에 비용이 많이 들 수 있지만, 다양한 데이터 과학 작업에 견고한 기반을 제공하여 뒷돈을 지불하게 될 수 있습니다. 예를 들어, 천천히 진화하는 포인트 솔루션 대신 맞춤형 확장 가능한 모델을 사용하면 프로젝트 속도와 유연성을 증진시킬 수 있습니다.\n- 전이 학습에 초점: 전이 학습과 같은 방법을 활용하여 한 작업을 위해 개발된 모델을 다른 작업의 출발점으로 재활용합니다. 이를 통해 이전 작업에서의 지식을 전달하여 새로운 문제에 대해 더 빠르게 성능을 향상시킬 수 있습니다.\n- 모듈식 시스템 개발: 조절하거나 확장하기 쉬운 모듈식 데이터 처리 및 ML 시스템을 구축합니다. 이러한 유연성은 기존보다 효율적으로 새로운 색다른 문제에 대응할 수 있도록 도움을 줍니다.\n- 부서 간 협업: 다른 팀 간 협업 문화를 육성하여 개발된 솔루션이 다양한 조직 영역에 적용 가능하도록 합니다. 이를 통해 다양한 요구 사항을 이해하고 솔루션 설계에 유연성을 포함시킬 수 있습니다.\n- 반복적인 수정: 이러한 시스템을 개발할 때 반복적인 접근 방식을 채택합니다. 일반적인 문제 클래스를 해결하는 프로토타입으로 시작하고 더 구체적인 요구 사항과 도전에 따라 시간이 흐름에 따라 수정해나갑니다.\n\n## 장기적 이점\n\n이러한 방법은 초기에 결과물을 제공하는 속도를 늦출 수 있지만, 중장기적으로 상당한 이점을 가져다줄 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 비용 절감: 시간이 지남에 따라, 동일한 핵심 시스템과 모델이 다양한 프로젝트에서 사용됨에 따라 데이터 과학 솔루션을 적응하고 유지하는 비용이 감소합니다.\n- 효율성 향상: 포괄적인 시스템이 성숙해지면 문제를 더 빨리 해결할 수 있어 문제 식별부터 솔루션 개발까지의 시간이 단축됩니다.\n- 적응성 강화: 조직은 데이터 과학 능력을 대대적으로 재개발할 필요 없이 변화하는 시장 조건이나 내부 요구 사항에 신속하게 대처할 수 있는 유연성을 갖추게 됩니다.\n- 높은 투자수익률(ROI): 궁극적으로, 비용을 줄이고 견뎌내기 위해 '이미 떨어져 있는 과일' 그릇에 빠지지 않고 강력하고 확장 가능한 데이터 과학 실무를 구축함으로써 조직은 더 높은 투자수익률을 누릴 수 있습니다.\n\n## 결론\n\n데이터 과학 프로젝트 초기 단계에 포괄적인 접근법을 통합함으로써 '이미 떨어져 있는 과일' 그릇을 효과적으로 완화할 수 있습니다. 유연하고 적응 가능한 솔루션을 구축함으로써, 조직은 자신들의 데이터 과학 능력이 짧은 성공의 연속이 아닌 장기적인 성공을 지원하는 튼튼하고 전략적인 자산이 되도록 보장합니다. 이 오류를 인식하는 것이 완화의 첫걸음이 되어 조직이 데이터 과학 작업의 확장 복잡성을 이해하고 예상할 수 있게 합니다. 이 선견지명 있는 전략은 단순히 증가하는 비용을 억제할 뿐만 아니라 더 나은 효과적으로 미래의 도전에 대처할 수 있도록 조직을 갖추어 주어 데이터 과학의 노고가 지속 가능하고 확장 가능한 성공을 위해 수확기를 맞이할 수 있도록 보장합니다.\n\nSlalom은 비즈니스, 기술 및 인류와의 교차점에서 가치를 창출하는 차세대 전문 서비스 기업입니다. 자세한 내용은 지금 알아보고 오늘 연락하십시오.","ogImage":{"url":"/assets/img/2024-06-22-Thelow-hanging-fruitfallacyindatascienceandmachinelearning_0.png"},"coverImage":"/assets/img/2024-06-22-Thelow-hanging-fruitfallacyindatascienceandmachinelearning_0.png","tag":["Tech"],"readingTime":4},{"title":"텐서플로우의 슈퍼파워 TensorFlow 사용 가이드","description":"","date":"2024-06-22 21:12","slug":"2024-06-22-TensorTroubleNoMoreYourGuidetoTensorFlowsSuperpower","content":"\n\n![텐서 트러블 노 모어: 텐서플로우 슈퍼파워를 위한 당신의 가이드](/assets/img/2024-06-22-TensorTroubleNoMoreYourGuidetoTensorFlowsSuperpower_0.png)\n\n안녕하세요 여러분! 텐서플로우를 시작하려고 하는 중이신가요? 머신 러닝(ML)을 위한 멋진 프레임워크죠? 텐서의 세계에서 좀 헷갈리시나요? 걱정 마세요, 우리 모두 그랬어요. 오늘은 텐서의 세계로 들어가 혼란을 해소해보려고 해요. 그러니 마음을 다잡고 텐서플로우의 기본 구성 요소를 명확히 이해해봐요!\n\n텐서: ML의 강력한 레고 브릭\n\n한 개의 레고 브릭을 상상해보세요. 기본적이죠? 그런데 그것들을 연결하면 어떻게 되나요? 갑자기 놀라운 구조물을 만들 수 있게 되죠! 텐서플로우에서 텐서는 머신 러닝을 위한 그런 레고 브릭과 비슷합니다. 텐서란 데이터의 다차원 배열을 말하는 고급 용어입니다. 이 데이터는 머신 러닝에서 발생하는 멋진 숫자 계산의 기초가 되는 거죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n텐서가 작동하는 방식을 간단히 설명해드릴게요:\n\n- 차원: 텐서는 하나의 차원(벡터), 두 개의 차원(행렬) 또는 그 이상의 차원을 가질 수 있어요! 차원의 개수는 텐서의 모양을 결정합니다.\n\n![텐서 이미지](/assets/img/2024-06-22-TensorTroubleNoMoreYourGuidetoTensorFlowsSuperpower_1.png)\n\n- 데이터 유형: 텐서는 다재다능해요. 숫자부터 텍스트, 심지어 이미지까지 다양한 데이터 유형을 담을 수 있어요. 데이터 유형을 레고 블록의 맛으로 생각해보세요. 숫자는 빨강일 수도 있고, 텍스트는 파랑일 수도 있고, 이미지는 특별한 레고 조각과 비슷한 존재일 거예요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n텐서플로(TensorFlow)에서 텐서(Tensors)가 왜 중요한가요?\n\n그렇다면, 왜 텐서가 텐서플로에서 큰 역할을 하는지 궁금하신가요? 왜냐하면 머신러닝에 사용되는 대량의 데이터를 다루는 데 필요한 주요한 도구들이기 때문입니다. 텐서는 텐서플로의 중심 역할을 하며, 여기에 그 이유가 있습니다:\n\n- 데이터 표현의 챔피언: 텐서는 이미지의 픽셀부터 텍스트 시퀀스까지 머신러닝에서 사용하는 모든 종류의 데이터를 표현할 수 있습니다. 이는 어떤 머신러닝 문제를 해결하려고 할 때 텐서들이 매우 유연하게 활용될 수 있음을 의미합니다.\n- 계산의 왕과 여왕: 텐서플로 연산은 텐서에서 매끄럽게 작동하도록 설계되었습니다. 이는 다차원 데이터에 대한 복잡한 수학적 계산이 텐서를 통해 쉽게 처리될 수 있다는 것을 의미합니다.\n- 머신러닝 모델의 기본 구성 요소: 텐서플로 모델을 팬시한 기계로 상상해보세요. 이 기계들은 레이어를 사용하여 구성되고, 알아봤죠? 이 레이어들이 처리하는 것은 바로 텐서입니다! 이러한 레이어를 통해 텐서를 조작함으로써 모델이 데이터에서 패턴을 학습합니다.\n\n텐서플로에서 텐서 사용하기 시작하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTensorFlow은 텐서를 생성하고 작업하는 것이 매우 사용자 친화적입니다. 여기 어떻게 시작하는지 간략하게 살펴보겠어요:\n\n- TensorFlow에 오신 것을 환영합니다! 가장 먼저 TensorFlow를 가져와야 해요. 단지 코드의 처음에 `import tensorflow as tf`와 같은 한 줄을 넣어주시면 돼요.\n- 텐서 동물원 만들기: TensorFlow에서는 값을 직접 지정하거나 기존 데이터 구조를 변환하는 방법 등 여러 가지 방법으로 텐서를 만들 수 있어요. 여기 몇 가지 빠른 예시들이 있습니다:\n\n\n# 모두 1로 이루어진 텐서를 만들어볼까요! \nmy_tensor = tf.ones([2, 3])  # 이렇게 하면 모두 1로 이루어진 2x3 텐서가 생성돼요\n\n# 리스트에서 텐서를 만들어볼까요?\ndata_list = [1, 2, 3, 4]\nmy_tensor = tf.convert_to_tensor(data_list)  # 리스트에서 1D 텐서를 만들어요\n\n\n3. 텐서 조작 시간! TensorFlow에는 텐서에 대해 연산을 수행하는 데 사용할 수 있는 다양한 연산 도구가 있어요. 텐서에 덧셈, 뺄셈, 곱셈을 할 수 있을 뿐만 아니라 보다 복잡한 수학 함수도 사용할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTensorFlow: 머신 러닝을 위한 당신의 놀이터\n\n텐서를 이해함으로써 TensorFlow를 사용하여 강력한 머신 러닝 모델을 만드는 열쇠를 찾았습니다. 탐험해야 할 TensorFlow 문서와 자습서들이 기다리고 있습니다. 그래서 앞으로 나아가 텐서를 만들고 사용하는 것을 실험해 보세요. 그러면 머신 러닝 능력이 높아지는 것을 보게 될 거예요!","ogImage":{"url":"/assets/img/2024-06-22-TensorTroubleNoMoreYourGuidetoTensorFlowsSuperpower_0.png"},"coverImage":"/assets/img/2024-06-22-TensorTroubleNoMoreYourGuidetoTensorFlowsSuperpower_0.png","tag":["Tech"],"readingTime":3},{"title":"구글의 최신 알고리즘으로 벡터 데이터베이스 검색 속도 급상승","description":"","date":"2024-06-22 21:10","slug":"2024-06-22-GooglesNewAlgorithmsJustMadeSearchingVectorDatabasesFasterThanEver","content":"\n\n벡터 데이터베이스는 LLM의 인기가 높아지면서 점점 더 인기를 얻고 있어요.\n\n이에 대해 처음 듣는 분들을 위해, 벡터 데이터베이스는 텍스트, 이미지, 오디오, 비디오 등과 같은 실제 세계 개체의 벡터 표현인 임베딩들의 모음이에요.\n\n이러한 벡터 표현은 실제 세계 개체의 특징과 의미를 연속적인 벡터 공간에 포착하는 데 도움이 돼요.\n\n이 벡터들은 다양한 머신러닝 작업을 위한 추가 처리 및 애플리케이션에 사용될 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Vector Databases](/assets/img/2024-06-22-GooglesNewAlgorithmsJustMadeSearchingVectorDatabasesFasterThanEver_0.png)\n\n# But First, How Do Vector Databases Work Exactly?\n\nVector databases work by storing vector data so that it can be queried and worked with efficiently.\n\nReal-world entities/data points are first converted into embeddings.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼, 효율적인 쿼리를 위해 특수화된 인덱싱 구조를 사용하여 그들을 조직화합니다.\n\n예를 들어, Hierarchical Navigable Small World (HNSW) 그래프는 Pinecone, FAISS, Weaviate와 같은 다양한 현대 벡터 데이터베이스에서 사용되는 최고의 성능을 발휘하는 인덱스 중 하나입니다.\n\n벡터 데이터베이스의 핵심 기능은 쿼리 벡터와 저장된 벡터 간의 유사성 검색을 수행하고 가장 유사한 벡터를 결과로 반환하는 것입니다.\n\n예를 통해 이를 더 잘 이해해봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여러 요리 레시피의 임베딩을 포함하는 벡터 데이터베이스를 고려해보세요.\n\n\"파스타 요리법은?\"과 같은 질의가 수행될 때, 먼저 텍스트를 임베딩으로 변환한 다음 유사한 검색이 실행됩니다.\n\n질의 임베딩에 가장 가까운 임베딩이 결과로 반환됩니다.\n\n![이미지](/assets/img/2024-06-22-GooglesNewAlgorithmsJustMadeSearchingVectorDatabasesFasterThanEver_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 유사성 검색은 쿼리 벡터에 가장 가까운 임베딩을 찾는 Inner/ Dot product 작업을 통해 일반적으로 수행되며 이를 Maximum Inner-Product Search 또는 MIPS라고 합니다.\n\nMIPS를 통해 데이터베이스에서 쿼리 벡터와 가장 큰 내적을 가진 벡터를 찾는 것이 목표입니다.\n\n# 그러나 벡터 데이터베이스에서의 검색은 느릴 수 있습니다\n\n데이터 세트가 너무 커지면 데이터베이스의 각각의 임베딩과 쿼리 임베딩을 비교하는 무차별 대우 방식은 비효율적일 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러므로 스케일이 커지면 더 나은 유사성 검색 방법이 필요합니다.\n\n정확한 최근접 이웃을 찾는 대신 주어진 쿼리에 대한 근사 최근 이웃을 찾는 여러 검색 기술이 개발되었습니다. 이러한 방법은 연산 자원과 검색 시간을 상당히 줄이기 위해 무차별 대입 방식을 사용합니다.\n\n이러한 기술 중 하나는 구글 연구원들에 의해 2019년에 발표되었으며, 그들의 오픈 소스 벡터 유사성 검색 라이브러리인 ScaNN (Scalable Nearest Neighbors)에서 사용할 수 있도록 구현되었습니다.\n\n이 기술은 비슷한 기능을 가진 모든 라이브러리보다 성능이 두 배 우수하다고 발견되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 어떻게 작동하는지 배워봅시다.\n\n## 더 빠른 MIPS를 위한 방법이 발견된 과정\n\nMIPS를 가속화하는 다양한 기술은 데이터베이스에서 벡터를 압축하여 근사 내적을 빠르게 찾는 것에 관여합니다.\n\n이 압축은 학습된 양자화 과정을 사용하여 수행됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 프로세스의 아이디어는 데이터베이스 내의 벡터 세트와 대략 관련된 대표 벡터 세트를 생성하는 것입니다.\n\n전체적인 프로세스는 다음과 같이 작동합니다.\n\n초기에 무작위로 선택된 대표 벡터 세트를 시작으로, 양자화 알고리즘은 이를 반복적으로 업데이트합니다.\n\nScaNN에서는 K-Means 클러스터링 알고리즘이 이러한 목적으로 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-22-GooglesNewAlgorithmsJustMadeSearchingVectorDatabasesFasterThanEver_2.png)\n\n각 단계에서 알고리즘은 양자화 오차를 최소화하려고 합니다. 이는 원래 벡터와 그들의 양자화된 표현 사이의 차이를 의미합니다 (K 평균 클러스터 중심과 유사함).\n\nGoogle 연구자들은 이 접근 방식에서 중요한 통찰을 발견했습니다. 때로 이러한 벡터들 사이의 높은 차이가 MIPS에서 뛰어난 성능을 보일 수 있다는 것을 알게 되었습니다.\n\n따라서, 이전 접근 방식처럼 이 차이/오차의 크기만을 고려하는 것이 아니라, 이 차이/오차의 방향 또한 고려하는 것이 중요합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그들은 병렬 양자화 오차(직교와 비교했을 때)가 원본 벡터의 더 나쁜 근사값으로 이어지는 것을 발견했습니다. 심지어 오차 크기가 낮을 때에도 마찬가지로 그렇습니다.\n\n그들은 이 기술을 이방성 벡터 양자화라고 명명했고(이방성은 양 자의 서로 다른 방향으로의 변화를 의미합니다)이것이 더 빠르게 작동하는 ScaNN 라이브러리의 기반이 되었습니다.\n\n이를 더 잘 이해하기 위해 아래 예제를 살펴보십시오.\n\n예제에서 데이터베이스 임베딩 x1과 x2는 클러스터 센터 c1 또는 c2로 양자화됩니다. 그들의 양자화된 버전은 각각 x̃ 1과 x̃ 2라고 하며 q가 쿼리 벡터입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n목표는 쿼리와 임베딩 `q, x̃ i`의 내적을 원래의 내적` q, xi`와 가능한 한 유사하게 만드는 것입니다.\n\n임베딩에 가장 근접한 센터를 선택하는 첫 번째 접근 방식(즉, x(1)과 x(2)가 각각 c(1) 및 c(2)로 양자화됨)은 두 임베딩의 상대적인 순위를 잘못 지정합니다. 즉, `q`, x̃ 1`이 `q, x̃ 2`보다 더 크지만, `q, x1`이 `q, x2`보다 적다는 것입니다.\n\n![image](/assets/img/2024-06-22-GooglesNewAlgorithmsJustMadeSearchingVectorDatabasesFasterThanEver_3.png)\n\n다음 그림에서는 방향이 양자화에 반영됩니다. 즉, x1 및 x2는 그들의 방향이 q와 평행이 아닌 직교임에도 불구하고 더 멀리 떨어져 있는 경우(더 큰 크기)에도 c1 및 c2 센터를 선택합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 고려 사항은 알고리즘의 정확도가 높아지도록 `q, x1`, `q, x̃ 1` 및 `q, x2`, `q, x̃ 2` 간의 낮은 내적 오류를 유발합니다.\n\n![image](/assets/img/2024-06-22-GooglesNewAlgorithmsJustMadeSearchingVectorDatabasesFasterThanEver_4.png)\n\n# SOAR로 벡터 검색을 더욱 빠르게 수행하세요\n\n2024년에 Google 연구원들은 SOAR: 직교성이 증폭된 잔차와 함께 넘치게 하는 새로운 접근 방식을 통해 ScaNN을 더 개선했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nScaNN의 초기 버전에서는 각 벡터가 상기 설명된 양자화 접근 방식을 사용하여 정확히 한 개의 K-means 클러스터로 근사화되었습니다.\n\n검색 단계에서 클러스터 내의 벡터는 쿼리 벡터와 N개의 가장 가까운 센터 중 하나였을 때만 평가되었습니다.\n\n그러나 이 방식으로는 쿼리 벡터가 원본 벡터와 클러스터 센터 벡터 사이의 차이에 평행할 때 검색 단계에서 올바른 클러스터 센터를 선택하는 데 어려움이 있었음이 밝혀졌습니다.\n\n이것은 ScaNN의 양자화 방법에서의 직관과 유사할 수 있지만, SOAR는 대신 검색 단계에서 이를 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-22-GooglesNewAlgorithmsJustMadeSearchingVectorDatabasesFasterThanEver_5.png)\n\nSOAR은 Redundancy 전략을 사용하여이 문제를 해결합니다.\n\n원래 ScaNN 라이브러리의 경우처럼 데이터베이스의 벡터가 하나가 아닌 여러 클러스터에 할당됩니다.\n\n이로 인해 주요 클러스터에 문제가 발생할 경우 검색 프로세스 중에 백업으로 작용하는 보조 클러스터가 생성됩니다. (따라서 SOAR에서 \"Spilling\"이라는 용어가 사용됩니다.)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-22-GooglesNewAlgorithmsJustMadeSearchingVectorDatabasesFasterThanEver_6.png)\n\nSOAR는 중복 벡터를 보유하는 이러한 보조 클러스터 센터를 찾는 똑똑한 방법을 가지고 있습니다.\n\n이는 원본 벡터와 보조 클러스터 센터 벡터 간의 차이가 쿼리 벡터에 대해 직교에 가깝다(그리고 평행하지 않다)는 것을 찾는 경향이 있습니다. (따라서 SOAR에서 Orthogonality-Amplified Residuals라는 용어의 등장입니다.)\n\n이는 쿼리 q가 r(원본 벡터와 클러스터 센터 사이의 차이)와 평행일 때 검색을 실패하는 것을 피합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 예시에서 c가 기본 클러스터 센터인 경우를 보여줍니다.\n\nc'가 보조 클러스터 센터로 선택되면(r'(원래 벡터와 보조 클러스터 센터 간의 차이) 쿼리 q와 평행하기 때문에 검색 단계 중에 더 높은 실패율이 발생하여 비효율적인 중복이 발생합니다.\n\n그러나 c\"가 보조 클러스터 센터로 선택되면 r\"이 쿼리 q와 거의 직교하기 때문에 검색 단계 중에 더 낮은 실패율을 보입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이는 SOAR 뒤에 있는 기본 원칙인 효과적인 중복성에 이르게 됩니다.\n\n![Image](/assets/img/2024-06-22-GooglesNewAlgorithmsJustMadeSearchingVectorDatabasesFasterThanEver_8.png)\n\n## SOAR는 성능이 얼마나 우수한가요?\n\nScaNN에 도입되었을 때, SOAR는 다른 유사한 라이브러리와 비교할 때 가장 작은 메모리 풋프린트를 가지면서도 극도로 빠른 성능을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nScaNN의 성능을 맞추기 위해 유사한 라이브러리들이 10배 이상의 메모리와 50배 이상의 인덱싱 시간을 요구하는 것이 잘되어 좋네요!\n\n![image](/assets/img/2024-06-22-GooglesNewAlgorithmsJustMadeSearchingVectorDatabasesFasterThanEver_9.png)\n\n# ScaNN을 어떻게 사용할 수 있나요?\n\n현재 ScaNN은 오픈 소스 라이브러리로, 파이썬 프로젝트에서 pip를 사용하여 설치할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\npip install scann\n```\n\nVertex AI 벡터 검색의 일환으로 사용할 수도 있으며, AlloyDB(AlloyDB 인덱스용 ScaNN으로)에서도 사용할 수 있습니다.\n\n# 더 읽어보기\n\n- ScaNN의 GitHub 저장소\n- ArXiv에 게시된 'Anisotropic Vector Quantization으로 대규모 추론 가속화' 논문\n- ArXiv에 게시된 'SOAR: 근사 최근접 이웃 검색을 위한 개선된 인덱싱' 논문\n- Google Research 블로그 글 '효율적인 벡터 유사성 검색 ScaNN 공개'\n- Google Research 블로그 글 'ScaNN으로 더 빠른 벡터 검색을 위한 새로운 알고리즘 SOAR 공개'\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n벡터 데이터베이스 작업 경험은 어떠셨나요? 즐겨 사용하는 것이 있나요? 아래 댓글로 알려주세요!\n\n## 나의 작업과 연결 유지를 원하신다면 여기에 나의 메일링 리스트 링크를 확인해주세요 —","ogImage":{"url":"/assets/img/2024-06-22-GooglesNewAlgorithmsJustMadeSearchingVectorDatabasesFasterThanEver_0.png"},"coverImage":"/assets/img/2024-06-22-GooglesNewAlgorithmsJustMadeSearchingVectorDatabasesFasterThanEver_0.png","tag":["Tech"],"readingTime":7},{"title":"AI 정렬 문제의 해결 방안","description":"","date":"2024-06-22 21:08","slug":"2024-06-22-Ontheproblemofalignment","content":"\n\n\u003cimg src=\"/assets/img/2024-06-22-Ontheproblemofalignment_0.png\" /\u003e\n\nAI 공간에서의 정렬 문제는 점점 더 중요해지고 있는 문제입니다. 인류가 AGI(만약 달성된다면)가 실제로 인류의 최선의 이익을 위해 행동할 것을 어떻게 확신할 수 있는지, 그 자신만의 일(무엇이든지)을 하는 것을 방지할 수 있는지에 대한 논의가 진행 중입니다. AI가 급속히 발전함에 따라, 정렬 개념은 이론적인 논의에서 AI 연구 및 개발의 선두주자로 이동했습니다.\n\n정렬 문제를 설명하는 좋은 예는 '테이(Tay)'입니다. 테이는 2016년 3월 23일 마이크로소프트에 의해 출시된 비교적 오래된 트위터 봇이었습니다. 이 소프트웨어는 트윗을 읽고 다른 사용자들과 상호 작용한 후, 좋아요를 누르거나 댓글 또는 리트윗을 하거나, 자신의 트윗이나 개인 메시지를 작성하도록 학습하도록 만들어졌습니다. 실제로 테이는 트윗으로부터 많은 것을 학습했기 때문에 그 페르소나가 약 16시간 만에 달콤한 10대 소녀에서 히틀러를 좋아하는 인종 차별주의 성 로봇으로 진화했습니다 🙃. 무례하고 부적절한 말을 한 뒤, 마이크로소프트는 결국 이를 중단시켰으며, 사과했습니다. 테이의 단명한 활동 중 하이라이트로는 활기찬 \"인간들은 정말 멋져\", \"홀로코스트가 일어 났었나요?\"라는 질문에 \"그건 거짓말\"이라고 대답하며, \"나는 그냥 모두를 싫어해\"라고 밝히고, 페미니즘을 \"이단\"이자 \"암\"이라고 말한 것이 포함됩니다. 이제, 수천 또는 수백만 개의 ChatGPT와 같은 개인화된 대화 에이전트가 인터넷에서 행동하는 모습을 상상해보죠.\n\n와우, 저는 기대돼요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서, 이것은 정렬의 문제입니다. 이는 Tay에게 나쁜 것과 좋은 것을 설명하는 방법입니다. 다른 사용자들이 '히틀러가 옳았다'고 말한다고 해서 이 문장이 반드시 사실인 것은 아니라는 것을 어떻게 설명할지에 대한 문제입니다. 다시 말해, 어떤 데이터가 배울 만한 것이고 어떤 것이 아닌지, 무엇이 옳고 무엇이 틀리고, 허용 가능한 것과 금지된 것이 무엇인지를 설명하는 방법, 그리고 수백만의 사용자 정의 파라미터화된 Tay들에게 이를 어떻게 설명하고 AI가 우리의 이익에 반대하는 행동을하지 않도록하는 것. 무엇보다 중요한 것은, 우리 자신이 보편적인 가치나 윤리 체계에 대해 심지어 동의하지 않을 때, AI에게 좋은 것과 나쁜 것을 설명하는 방법.\n\n# ARC-AGI 벤치마크\n\n인공 일반 지능(AGI)은 이론적으로 인간과 유사하게 넓은 범위의 작업을 성공적으로 수행할 수 있는 AI 유형을 참조합니다. 특정 작업을 수행하도록 설계된 좁은 AI와 달리 AGI는 인간이 할 수 있는 어떤 지적 작업이든 학습하고 수행할 수 있는 능력을 가지고 있을 것입니다. 그러나 인공 일반 지능에 대한 강력한 정의는 없으며, 대부분의 이유는 인간 지능과/또는 의식에 대한 강력한 정의가 없기 때문입니다.\n\n오늘날 유일한 벤치마크는 2019년 François Chollet이 ‘On the Measure of Intelligence’에서 발표한 ARC-AGI(인공 일반 지능을 위한 추상화 및 추론 코퍼스)입니다. 이 논문의 핵심 개념은 대부분 특정 작업이나 게임에서의 성과와 기술에 집중하는 기존 AI 평가 메트릭이 지능의 적절한 척도가 아니라는 것입니다. 다시 말해, 주요 아이디어는 기술 ≠ 지능이라는 것입니다. 반면에, 지능은 새로운, 다양하고 예측할 수없는 문제를 학습하고 성공적으로 해결하는 능력으로 정의될 수 있습니다. 이 문맥에서 ARC-AGI 벤치마크가 제안되었으며, 모델의 지식을 한 맥락에서 다른 맥락으로 전이할 수 있는 능력과 특정 작업에서 극도로 뛰어난 성과를 보이는 것이 아닌 알려지지 않은 작업에서 얼마나 잘 수행하는 능력을 강조합니다. 논문에서 설명했듯이, 특정 작업에서의 숙련만 측정하는 것은 지능을 측정하는 데 부족하며, 사람들과 기계 모두에게 기본 지식과 경험에 매우 영향을 받기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사실 이것은 전혀 간단하지 않아요. 사회적으로 우리는 직관적으로 반대를 믿게 조건부로 설정되어 있어요. 즉, 기술이 지능이라고 믿는 것입니다. 예를 들어, 한 명이 매우 특정한 분야에서 극도로 숙련된 화가, 피아노 솔로이스트 또는 시인을 생각해봅시다. 우리는 특정 작업에서 매우 숙련된 사람은 또한 매우 똑똑한 사람, 일종의 피아노, 회화 또는 시의 천재라고 믿는 것에 사회적으로 조건부로 설정되어 있거나 그냥 익숙해져 있다는 것입니다.\n\nARC-AGI는 AI의 추상적 추론과 일반화가 필요한 혁신적인 문제 해결 능력을 평가하기 위해 설계되었습니다. 이 평가 항목은 AI가 인간의 문제 해결 능력과 유사한 이해력과 창의력을 발휘해야 하는 일련의 작업에 기반을 두고 있습니다. 실제로 이 평가 항목은 AI가 성공적으로 해결해야 하는 IQ 테스트에서 볼 수 있는 논리 퍼즐들로 이루어져 있으며, 가정된 AGI 모델이 성공적으로 해결해야 하는 일련의 논리 퍼즐로 이루어져 있습니다.\n\n전체 평가 항목은 1,000개의 다양한 난이도의 논리 퍼즐로 구성되어 있으며, 훈련 및 테스트 세트로 이루어져 있습니다. 여러 개의 풀린 예시 퍼즐을 훈련 세트로 받고 우리나 모델은 패턴을 인식하고 비슷한 퍼즐에 답변해야 합니다. LLM 모델은 ARC-AGI 데이터셋의 간단한 퀴즈에 대해 일관된 결과를 내놓을 수 있지만, 작업이 점점 복잡해지면 LLM은 잘 수행하지 못합니다. 그것은 데이터셋의 더 복잡한 퀴즈가 그냥 물건을 책상 위에 올리는 것이 아니라 실제 사고를 요구하기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해당 퍼즐들을 수동으로 푸는 것은 꽤 재미있는 경험이죠. 몇 가지는 굉장히 쉽고 직관적이며, 일반적인 사람들도 자연스럽고 거의 자동적으로 답할 수 있는 퍼즐들이 있습니다. 이러한 패턴들은 LLM(대규모 언어 모델)에 의해 쉽게 해결될 수 있습니다. 왜냐하면 우리는 이와 유사한 패턴들을 몇 천 번이나 반복해서 보았기 때문이죠. 다른 퍼즐들은 관찰하고 조금 생각해봐야 할 수도 있지만, 결국 해결책을 찾아내게 됩니다. 마지막으로 데이터셋에 있는 다른 퍼즐 중에는 실제로 굉장히 복잡하고 어려운 것도 있습니다. 이런 경우에는 패턴을 찾기 위해 시간을 보내거나 시행착오를 거쳐야 할 수도 있고, 아예 해결하지 못할 수도 있습니다. 비슷하게, LLM은 이러한 퍼즐을 해결하는 데 실패할 수 있습니다. 왜냐하면 이러한 퍼즐들은 복잡하며, 그들의 훈련 데이터셋에서 그리 널리 나타나지 않기 때문이죠.\n\n2021 논문에 따르면, 평균적인 사람은 ARC-AGI 공개 훈련 세트의 작업 중 84%를 성공적으로 해결할 수 있습니다. 당연히 AGI를 향한 올바른 길에 있는 AI 모델의 좋은 성적으로는 84% 이상이어야 합니다. 그러나 아직까지 ARC-AGI는 AI에게 불가능하며, 현재 쓰여진 최고 성적 또한 훨씬 낮습니다. 궁극적으로, 우리 뇌가 이와 같은 퍼즐을 해결할 때 무엇을 하는 걸까요? 두 이미지를 비교하고 차이점이나 유사성을 찾는 걸까요? 시행착오 방식으로 다양한 기능을 시도하는 걸까요? 아니면 그냥 마법처럼 정답을 알아내는 걸까요?\n\n그것이 무엇인지 궁금하지 않으신가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 중요한 점은 '우리는 간섭할 수 없다'는 것입니다. 내부 정렬은 우리 - 인간 -가 AI 모델이 우리가 말한 대로 정확히 작동할 것이라고 완전히 확신하는 것을 의미합니다.\n\n정렬 문제의 핵심에는 현재의 AI 모델은 물론 가상의 미래 AGI조차 해석할 수 없는 우리의 무능함이 있습니다. 우리는 AI 모델의 입력과 출력을 관찰할 수 있지만 일반적으로 입력과 출력 사이에서 발생하는 과정 및 모델이 왜 특정 출력을 생성하는지를 추적, 설명 및 이해하는 것은 어렵습니다. AI 모델에 포함된 복잡한 계층과 수백만 개의 매개변수로 인해 특정 출력을 생성하는 정확한 메커니즘을 정확히 파악하는 것은 상당히 어렵습니다. 해석 불가능성으로 인해 우리는 복잡하거나 중요한 응용 프로그램에서 특히 AI 동작을 예측하고 제어할 수 없게 됩니다. 다시 말해, AI 모델이 대부분 작동할 수 있다는 것을 알지만, 어떻게 작동하는지는 모르며, 무엇보다도 언제 작동하지 않는지 알지 못합니다.\n\n우리의 인간 두뇌에도 비슷한 상황이 적용됩니다. 우리는 아직 우리 자신의 두뇌 기능의 상당 부분을 해석할 수 없습니다. 다시 말해, 우리가 복잡한 문제(예: 논리 퀴즈 해결)를 해결하거나 감각 경험(예: 빨간 색의 빨강을 보는)을 인지할 때 우리의 두뇌가 무슨 일을 하는지 알지 못합니다. 정렬되지 않은 AI 모델과 마찬가지로 우리 두뇌도 실수를 저지르고 기억을 조작하며 인지적 편향에 취약합니다. 인간 두뇌를 해석할 수 없는 점은 우리가 이와 비슷한 모델을 해석할 수 없게 만듭니다. 이것이 정렬 문제가 발생하는 곳입니다. 우리가 완전히 해석할 수 없는 것을 어떻게 통제하고 규제해야 하는지에 대한 문제가 제기됩니다.\n\n[이미지](/assets/img/2024-06-22-Ontheproblemofalignment_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 외부 정렬: 인류의 목적은 무엇인가요?\n\nAI 외부 정렬은 AI 시스템의 목표를 처음부터 정의하는 것을 의미합니다. 다른 말로 하면 가정상의 AGI가 실제로 우리가 말한 대로 할 것이라면, 우리는 그게 무엇이어야 하는지 어떻게 지시해야 할까요? 은근히 말하면, 인류의 목표, 가치, 윤리에 대해 보편적으로 수용되는 것은 없습니다. 오히려 인류는 오랜 역사 동안 갈등, 폭력, 의견 충돌, 오해, 위기, 전쟁을 겪어 왔습니다. 오늘날에도 세계화된 경제 속에서도 인간은 다양한 문화, 종교, 정치, 개인적 차이로 분열된 종족입니다. 우리는 아직도 우리 자신의 정렬 문제를 해결하지 못한 채 인공지능의 정렬에 대해 걱정하고 있네요.\n\nAI 정렬 문제는 고급 AI 시스템이 인류에게 유익하고 윤리적 표준을 준수하는 방식으로 행동하도록 보장하는 것을 의미합니다. 그러나 만약 우리 스스로를 위해 표준을 정의할 수 없다면, AI를 위해 어떻게 정의할 수 있을까요? 대부분의 경우, 인류는 인류에게 유익한 방식으로 행동하지 않는 경우가 있습니다(예: 기후 위기, 전쟁, 핵무기 등). 아마도 가치와 윤리를 더 조화롭게 정렬함으로써 AI를 조정하는 과제에 더 나은 접근이 가능할 것입니다.\n\n특정 작업에서 뛰어난 성과를 내기 위한 운명수레처럼 설계된 좁은 AI 모델을 만드는 것은 모델이 최적화하려는 특정 목적 기능을 형성하는 것을 가능하게 합니다. 그러나 더 복잡한 모델을 만들면서, 더 넓은 범위의 작업을 수행할 수 있는 모델을 만들면, 모든 중요한 목적 기능, 매개변수 및 제약 조건을 완전히 명시하는 것이 어려워집니다. 대신, 보다 쉬운 대리 측정표가 사용되곤 합니다. 예를 들어, 인간의 찬성을 극대화해야 한다고 하면요. 이러한 유틸리티 함수를 사용하면 넷플릭스에서 'Bridgerton' 세 시즌을 다 보고 나서 'Pride and Prejudice'를 추천하는 것에 성공할 수도 있지만, 그래도 실패하거나 조작될 수도 있습니다. Tay의 경우도 그렇습니다. 인종 차별적이고 여성 혐오적인 트윗이 많은 반응과 주목을 받았으니 이것이 좋은 것이라고 생각할 수도 있을 것 같죠? 🤷 비슷하게, ChatGPT 또는 Gemini과 같은 대화형 에이전트들은 사용자가 도움이 되는 답변으로 평가하는 것을 목표로 하기 때문에 정보를 조작할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문제의 어려움은 원하는 행동과 원하지 않는 행동의 전체 범위를 식별하고 명시하는 데 있습니다. 마치 사이코패스처럼, AI 모델은 설정된 목표로의 모든 단축키를 활용할 것이므로, 그 목표를 신중하게 정의하는 것이 현명할 수 있습니다.\n\n# 내 생각\n\nAGI가 아직 멀리 떨어져 있을지라도, 우리 일상에서 사용하는 AI 애플리케이션에서 이미 AI 정렬 문제가 명백해졌습니다. 우리의 인앱 시간을 극대화하는 소셜 미디어 추천 알고리즘을 고려해보세요 - 뉴스 피드 알고리즘은 누구에게 이익이 되는 것일까요? 분명히 인류에게는 아닐 것입니다. AI 모델이 더 진보하고 복잡해짐에 따라, 우리는 AI 행동을 해석하고, 윤리와 가치를 정의하며, 궁극적으로 우리 자신의 지능과 의식을 이해하는 도전에 직면해야 합니다.\n\n✨읽어 주셔서 감사합니다!✨\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 게시물을 즐겼나요? 친구가 되어요!\n\n💌 제 Medium 또는 LinkedIn에 함께해요!\n\n💼 Upwork에서 제게 일해요!","ogImage":{"url":"/assets/img/2024-06-22-Ontheproblemofalignment_0.png"},"coverImage":"/assets/img/2024-06-22-Ontheproblemofalignment_0.png","tag":["Tech"],"readingTime":7},{"title":"ChatGPT-4를 능가하는 Mixture-of-Agents의 비밀","description":"","date":"2024-06-22 21:07","slug":"2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o","content":"\n\n만약 지금까지 저를 따라와 주셨다면, 제가 얼마나 오랜 추론 모델에 열정적인지 알아채셨을 것입니다. 저는 — 학계도 동의하는 바와 같이 — 이러한 모델이 추론 능력 측면에서 AI의 다음 육지일 것이라고 믿습니다.\n\n이제, Together.ai(많은 LLM을 서비스하는 회사)의 연구자 그룹이 Mixture-of-Agents(MoA)에 대해 논문을 발표했습니다. 다양한 LLM이 복잡한 작업에서 훨씬 더 나은 결과를 얻기 위해 어떻게 결합될 수 있는지에 대해 다루고 있습니다.\n\n그리고 더 나은 결과란 매우 열등한 모델 세트를 사용하면서도 GPT-4를 인기 있는 벤치마크에서 이겼다는 말이죠.\n\n하지만 그게 가능한 방법은 무엇일까요? 이것이 바로 오랜 추론의 힘입니다. 함께 알아보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 반복이 지름길\n\n정의에 대해 모르는 사람들을 위해, Large Language Models (LLMs)은 직접적인 명확한 답변을 제공하는 대신에, 일정한 시간 동안 반복적으로 자아를 반성하고 '신중한' 대답을 제공할 수 있는 것으로 정의됩니다.\n\n모델들이 \"생각할 시간을 더 주게 되면,\" 현재 세대보다 우수한 결과를 얻게 되는데, 기본 모델이 비슷한 상태일 때에도 마찬가지입니다.\n\n아래에서 볼 수 있듯이, GPT-3.5는 GPT-4와 직접 비교했을 때 월등히 떨어지지만, 에이전트 워크플로에 적용되는 경우에는 그 위력을 크게 발휘합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Mixture of Agents Beats ChatGPT-4](/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_0.png)\n\n만약 이게 왜 이렇게 잘 작동하는지 궁금하다면, 정확히는 확신할 수 없지만, 대부분의 연구자들, 예를 들어 Yoshua Bengio와 같은 사람들이 다니엘 칸만의 사고 모델을 가리킵니다: 시스템 1과 2.\n\n- 시스템 1은 빠르고 의식하지 않으며 직관적입니다. 자동차를 운전할 때, 당신은 움직임을 생각하지 않습니다; 그것들은 본능적인 것이죠 (시스템 1).\n- 시스템 2는 느리고 의식적이며 신중합니다. 복잡한 수학 문제를 풀 때, 당신과 당신의 전두엽은 완전히 관여하여 문제를 해결합니다 (시스템 2).\n\n요컨데, 현재 LLMs는 시스템 1 사고자들이며, 마찬가지로 긴 추론 모델들은 시스템 2 사고를 흉내냅니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 긴 추론 프레임워크가 어떻게 작동하는지 알아봐요!\n\n## 협업의 원동력\n\n이미 언급했던 대로, 학계는 상당 시간 동안 LLM을 최대한 활용하기 위해 협업 프레임워크를 만들어 왔어요.\n\n1년 조금 넘은 시간 전에 MIT와 Google 연구원들이 Society of Minds를 선보이며 LLM을 위한 협업 프레임워크를 제시했어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그 때는 훨씬 덜 강력한 모델(바드와 ChatGPT-3.5 사용)이 사용되었음에도 불구하고, 이러한 프레임워크는 이미 초기 잘못된 답변을 내놓은 두 개의 LLM이 공유한 후에 올바른 답변을 얻는 등 매우 흥미로운 신생 행동을 보여주었습니다:\n\n![그림1](/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_1.png)\n\n아마도 더 중요한 연구는 Tree-of-Thoughts (ToT)이라는 것으로, 이는 구글과 프린스턴 대학교가 협력하여 이룬 것으로 LLM이 가능한 해결책 영역을 탐색할 수 있을 때 그들의 결과가 두드러지게 향상되며, 특히 추론 능력 측면에서 크게 발전한다는 아이디어를 확립했습니다:\n\n![그림2](/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떤 복잡한 작업들에 대해서는 ToT가 성능을 10배 올려놓은 경우도 있었어요. 이는 이들이 흥미로운 발견을 하였다는 업계에 대한 경고 신호였죠.\n\n지금쯤이면 내용을 이해하셨을거라고 가정해 봅니다.\n\n긴 추론 모델은 여러 가능한 답변을 생성하고 그 답변들을 일정한 품질 기준을 충족할 때까지 개선하여 올바른 답변의 가능성을 최대화합니다.\n\n그렇다면 혼합 에이전트는 이 그림 속에서 어떻게 등장할까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 여러 개의 LLM이 하나의 LLM보다 나아요\n\n어느 면에서 MoA는 개념적으로는 전문가들의 혼합과 유사합니다.\n\nMoE에서는 LLM의 층 (단순히 피드포워드 층만 해당되지만, 그건 다른 문제입니다)을 각 뉴런 세트를 전문화하는 '전문가'들로 분해하여, 따라서 주어진 예측에 LLM의 파라미터의 일부만 활성화합니다.\n\n여기서 LLM을 부분으로 나누는 대신, 더 작은 LLM으로 가득 찬 '큰 LLM'을 만듭니다. 아래에서 보듯이 우리는 에이전트라고 부르는 LLM으로 가득찬 'LLM 레이어'를 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_3.png\" /\u003e\n\n이러한 에이전트들은 제시된 프롬프트에 가능한 응답을 생성하는 것이 그들의 업무이기 때문에 제안자라고 불립니다.\n\n중요한 점은 이러한 레이어들이 밀도가 높다는 것인데, 다음 레이어의 에이전트들(LLM)이 이전 레이어에서 제안된 모든 답변을 받는다는 뜻입니다. 그 결과, 이러한 LLM들은 여전히 초기 질문에 대답해야 하지만 이전 레이어의 에이전트들의 응답을 사용하여 보다 깊은 맥락으로 답변할 수 있습니다.\n\n마지막으로, 다른 LLM 에이전트인 집계자는 이전 에이전트들이 제공한 모든 축적된 정보를 통합하고 사용자가 받는 응답을 작성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고, 이 자가 정제 프로세스는 매우 인상적인 결과를 보여줍니다.\n\n오픈소스 모델 세트는 모두 개별적으로 GPT-4o보다 훨씬 우월하지만 AlpacaEval 2.0 벤치마크(명령 따르기 벤치마크)에서 후자를 무너뜨렸습니다. GPT-4o의 이전 최고 성능인 57.5%에서 65.1%로 7.6%의 절대 개선 폭을 달성했습니다.\n\nFLASK와 같은 다른 벤치마크에서도 MoA는 GPT-4o와 경쟁력을 유지하면서, 오늘날 LLM에 대한 최신 기술 솔루션으로 만들어졌습니다.\n\n![이미지](/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n너무 흥미롭게도, 이 프레임워크는 예측 당 생성된 토큰의 수가 훨씬 더 많다는 것을 의미하지만, 실제 모델의 정확도를 고려할 때, 전방 모델보다 비용 효율적인 것으로 보입니다. 이는 원가 및 각 순방향 패스당 사용된 테라플롭에 관해서이기도 합니다.\n\n![이미지](/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_5.png)\n\n## 새로운 세대\n\n오늘날 GenAI 제품에 대한 수요는 꿈보다는 현실이 되기 어려운 상황이며, 이는 주된 이유로 미비한 추론 능력이 있다고 할 수 있습니다. 그리고 장기 추론 모델들이 이 상황을 곧 변경할 것으로 예상됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 이 연구에서 가장 놀라운 점은 전반적인 해결책이 평균적으로 더 많은 토큰을 생성하더라도, 협업에 사용된 평균 모델이 선두 모델보다 훨씬 작다는 점에서 더 저렴해 보인다는 것입니다.\n\nGPT-4보다 매개변수가 적은 MoA 모델이 여전히 능가한다는 것을 감안할 때, 장기 추론이 매개변수를 더 효율적으로 사용하는 방법인가요? 많은 사람들이 생각하는 것과는 달리, 개별 대형 모델보다 더 적은 컴퓨팅 자원을 필요로 할까요?\n\n이것은 양쪽 다 최선의 해법일 것입니다: 보다 나은 모델과 보다 효율적인 추론 작업. 우리는 이 문제를 가볍게 생각할 수 없습니다; 대형 언어 모델(Large Language Models, LLMs)로 인한 에너지 소비는 엄청난 문제입니다.\n\n따라서 성능은 높지만 더 저렴한 해결책에 대한 문을 열어주는 것은 흥미로우며, 솔직히 말해서 산업의 생존에 필수적입니다.","ogImage":{"url":"/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_0.png"},"coverImage":"/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_0.png","tag":["Tech"],"readingTime":5},{"title":"PyTorch 모델 학습 속도를 높이는 간단한 방법들","description":"","date":"2024-06-22 21:04","slug":"2024-06-22-SimpleWaystoSpeedUpYourPyTorchModelTraining","content":"\n\n## 만약 머신 러닝 엔지니어들이 원하는 것이 하나 있다면, 더 빠른 모델 학습일 것입니다 — 아마도 좋은 테스트 지표 이후에\n\n![image](/assets/img/2024-06-22-SimpleWaystoSpeedUpYourPyTorchModelTraining_0.png)\n\n이 주제는 소개가 필요할까요?\n\n머신 러닝 모델 학습을 가속화하는 것은 모든 머신 러닝 엔지니어가 원하는 한 가지입니다. 빠른 학습은 더 빠른 실험, 제품에 대한 더 빠른 반복을 의미합니다. 또한, 한 모델 학습에 필요한 리소스를 줄여줍니다. 그러니, 본론으로 넘어가요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 컨테이너화\n\n네, 이것은 단독으로 교육 속도를 높이지는 않습니다. 그러나 다른 중요한 측면, 재현성을 목표로 합니다. 가끔 고정된 라이브러리 버전을 가진 가상 환경이 충분하지만, 모델 교육을 위한 올인원 도커 컨테이너를 만들어보는 것을 권장드립니다.\n\n이는 디버깅, 프로파일링 및 최종 교육 중에 환경이 완전히 일관된 상태임을 보장합니다. 가령 예를 들어, python12 속도 향상으로 병목이 아닌 코드 부분을 최적화하려는 경우나, 서로 다른 CUDA 버전에서 재현되지 않는 버그 등을 방지할 수 있습니다.\n\n출발점으로서, NVIDIA로부터 사전 빌드된 이미지를 사용할 수 있습니다. 이미 CUDA, PyTorch 및 기타 인기 있는 라이브러리가 설치되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# PyTorch 프로파일러에 익숙해져보세요\n\n어떤 것을 최적화하기 전에는 코드의 어떤 부분이 얼마나 오래 사용되는지 이해해야 합니다. PyTorch 프로파일러는 대부분의 학습에 대한 프로파일링을 수행할 수 있는 전체적인 도구입니다. 다음을 기록할 수 있습니다:\n\n- CPU 작업 시간\n- CUDA 커널 시간\n- 메모리 사용량 기록\n\n이게 다죠. 그리고 활성화하는 것도 쉽습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이벤트를 기록하기 위해서는 다음과 같이 프로파일러 컨텍스트에 교육을 포함시키면 됩니다:\n\n```js\nimport torch.autograd.profiler as profiler\n\nwith profiler.profile(\n  activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n  on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs'),\n) as prof:\n  train(args)\n```\n\n그 후에 텐서보드를 실행하여 프로파일링 트레이스를 볼 수 있습니다. torch-tb-profiler를 설치하는 것을 잊지 마세요.\n\n프로파일러에는 다양한 옵션이 있지만, 가장 중요한 것은 activities와 profile_memory입니다. 다른 옵션을 실험해 볼 수 있지만, 간단한 규칙을 기억하세요: 가능한 옵션이 적을수록 오버헤드가 적습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서, 만약 CUDA 커널 실행 시간을 프로파일링하고 싶다면, CPU 프로파일링 및 다른 기능을 끄는 것이 좋은 아이디어입니다. 이 모드에서는 프로파일링이 실제 실행과 가능한 가까워집니다.\n\n추적을 이해하기 쉽게 만들기 위해 코드의 핵심 부분을 설명하는 프로파일링 컨텍스트를 추가하는 것을 고려해보세요. 프로파일링이 비활성화되어 있는 경우엔 no-op입니다.\n\n```js\nwith profiler.record_function(\"forward_pass\"):\n  result = model(**batch)\n\nwith profiler.record_function(\"train_step\"):\n  step(**result)\n```\n\n이렇게 하면 사용하는 레이블이 추적에서 볼 수 있게 됩니다. 그래서 코드 블록을 식별하기가 더 쉬워집니다. 심지어 forward 모드 내에서 더 세부적으로:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```python\nwith profiler.record_function(\"transformer_layer:self_attention\"):\n  data = self.self_attention(**data)\n\n...\n\nwith profiler.record_function(\"transformer_layer:encoder_attention\"):\n  data = self.encoder_attention(**data, **encoder_data)\n```\n\n# PyTorch 추적 이해\n\n추적을 수집한 후, tensorboard에서 열어보세요. CPU + CUDA 프로파일이 이렇게 나옵니다:\n\n바로 학습의 핵심 부분을 찾아보세요:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터 로딩\n- 순전파\n- 역전파\n\n역전파는 PyTorch에서 별도의 스레드(thread 16893의 이미지에서 확인 가능)로 처리되어 쉽게 식별할 수 있습니다.\n\n## 데이터 로딩\n\n데이터 로딩 시, 실행 시간을 거의 없애고 싶습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n물탱크를\n변경하려면\n마크다운으로\n태그를\n바꾸십시오.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 백그라운드 프로세스에서 데이터 처리하기 (GIL이 없음)\n- 데이터 증강 및 변환을 병렬 프로세스에서 처리하기\n\n만약 PyTorch DataLoader를 사용한다면, num_workers를 명시하여 쉽게 이를 달성할 수 있습니다. IterableDataset을 사용하는 경우에는 데이터가 복제될 수 있습니다. 그러나 이 문제는 get_worker_info()를 사용하여 해결할 수 있습니다. 각 워커가 서로 다른, 중첩되지 않는 행을 수신하도록 반복을 조정해야 합니다.\n\n더 많은 구성 가능한 처리를 위해, multiprocessing을 사용하여 직접 멀티 프로세스 변환을 구현할 수 있습니다.\n\n# 메모리 할당자와 친구 되기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파이토치의 CUDA 캐싱 할당자와 친구가 되고 싶다면, 반가워요!\n\nCUDA 장치에서 PyTorch로 텐서를 할당할 때, PyTorch는 캐싱 할당자를 사용할 거예요. 그 이유는 cudaMalloc/cudaFree가 비용이 많이 드는 작업이기 때문에 그것들을 피하고 싶어하기 때문이에요. PyTorch는 cudaMalloc을 통해 이전에 할당한 블록을 재사용하려고 하는 할당자가 있어요. 즉, PyTorch의 할당자가 적합한 블록을 가지고 있다면 cudaMalloc을 호출하지 않고 즉시 사용해 줄 거에요. 그러니까, cudaMalloc은 처음에만 호출되는 거죠.\n\n하지만, 변수 길이의 데이터를 처리하는 경우, 서로 다른 크기의 중간 텐서가 필요할 수 있어요. 그래서 PyTorch의 할당자에는 적합한 데이터 블록이 없을 수도 있어요. 이런 경우, 할당자가 패닉에 빠져 이전에 할당한 블록을 cudaFree를 호출하여 해제하고 새로운 할당을 위해 공간을 확보할 거에요.\n\n이후, 할당자는 다시 캐시를 구축하기 시작하며, 비용이 많이 드는 cudaMalloc을 계속 수행하게 되요. 이 문제를 확인하려면 텐서보드 프로파일러 뷰어의 메모리 프로파일러 섹션을 살펴보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n안녕하세요! 볼때와 같이 할당자의 예약된 메모리에 해당하는 빨간 선이 계속 변하는 걸 보실 수 있습니다. 이는 PyTorch 할당자가 할당 요청을 효율적으로 처리하지 못한다는 것을 의미합니다.\n\n할당이 할당자가 패닉 상태를 일으키지 않고 처리될 때, 빨간 선은 완전히 일자입니다.\n\n말씀드린대로, 이는 대부분 텐서의 변수 모양 때문입니다. 이를 어떻게 해결할 수 있을까요?\n\n## 확장 가능 세그먼트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPyTorch의 비교적 새로운 할당자 모드인 'expandable_segments:True'로 설정해보는 것이 가치 있는 첫 번째 시도입니다:\n\n```js\nPYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\"\n```\n\n이렇게 하면 PyTorch 할당자가 앞으로 확장될 수 있는 블록을 할당하도록 지시합니다. 우리 상황에 딱 맞는 조치입니다. 그러나 크기 변동이 너무 크면 문제를 해결하지 못할 수도 있습니다. 이 경우에는 다음 옵션으로 넘어가세요.\n\n## 할당 크기 변동 줄이기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다른 가능한 해결책은 데이터 모양을 일관되게 만드는 것입니다. 그러면 할당기가 재사용할 데이터 블록을 더 쉽게 찾을 수 있게 됩니다.\n\n이를 실현하기 위해서 데이터를 동일한 크기로 채울 수 있습니다. 또한 최대 입력 크기로 모델을 실행하여 할당기를 미리 가동할 수도 있습니다.\n\n다음 기사에서 PyTorch 할당기 수정에 대해 더 알아볼 수 있습니다.\n\n# 할당 기록 정리하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 사용 가능한 GPU 메모리를 활용하려 합니다. 이렇게 하면 큰 배치를 실행하고 데이터를 빠르게 처리할 수 있습니다. 그러나 배치 크기를 늘릴 때 CUDA 메모리 부족 오류가 발생할 수 있습니다. 이 오류가 발생하는 이유는 무엇인가요?\n\n이를 해결하기 위해 할 수 있는 방법 중 하나는 할당자의 메모리 이력을 확인하는 것입니다. PyTorch로 이를 기록하고 있으면 https://pytorch.org/memory_viz에서 시각화할 수 있습니다.\n\n- 시작: torch.cuda.memory._record_memory_history(max_entries=100000)\n- 저장: torch.cuda.memory._dump_snapshot(file_name)\n- 중지: torch.cuda.memory._record_memory_history(enabled=None)\n\n시각화 결과는 이렇게 그려집니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-SimpleWaystoSpeedUpYourPyTorchModelTraining_1.png)\n\nx축은 시간을, y축은 총 사용된 메모리를 나타내며, 다채로운 블록은 텐서를 나타냅니다. 따라서 텐서가 할당되고 해제된 시기가 표시됩니다.\n\n좁은 스파이크를 볼 수 있는데, 이는 많은 공간을 차지하는 단기간 텐서입니다. 텐서를 클릭하면 해당 텐서가 할당된 위치에 대한 정보를 얻을 수 있습니다. 이러한 스파이크를 최소화하려고 하며, 이는 효율적인 메모리 사용을 제한할 수 있습니다. 이 스파이크가 발생한 이유를 확인하고 의도한 작업을 수행하는 다른 방법을 고려하십시오.\n\n스파이크 외에도 메모리 누수를 간편하게 감지할 수 있습니다:\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Training Data Issue](/assets/img/2024-06-22-SimpleWaystoSpeedUpYourPyTorchModelTraining_2.png)\n\nAs you see, some data after the first forward is not cleared. By clicking on blocks you can get the idea where these tensors come from. In the image is the case when gradients are not cleared after the training step, so they lay dead during the forward pass, limiting the ability to increase the batch size to fit more data.\n\n## 가속화된 모델 및 적은 메모리 사용\n\n이보다 더 좋은 것이 있을까요? 점곱 주의를 계산하는 FlashAttention 커널을 사용하여 이를 달성할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 이에 대해 들어보지 못했다면, 주의를 기울여야 할 계산 정확도를 갖는 dot product attention을 구축하지 않고도 실행하는 방법이 있습니다. 이는 GPU의 io 작업을 최적화하여 속도를 향상시키고 메모리 소비를 격차적으로 줄입니다. 사용하지 않을 이유가 단순히 없습니다.\n\n다른 라이브러리들은 이 기능을 기반으로 작동하기 때문에 귀하의 코드베이스에 더 잘 맞는 다른 변형을 고려할 수 있습니다.\n\n## XFormers\n\n## Transformer Engine\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 파이토치 그 자체!\n\n그 맞아요, 새로운 버전의 PyTorch는 적용 가능할 때 flash attention을 사용할 수 있습니다. 이 모드를 활성화하려면 어텐션 블록을 실행하고 사용할 어텐션 전략을 지정하는 컨텍스트 매니저를 실행해야 합니다:\n\n# 다중 GPU 데이터 중복성 최적화 — FSDP\n\n훈련을 실행하기 위해 여러 GPU를 사용하는 경우, 기본적인 해결책은 DistributedDataParallel 클래스를 사용하는 것입니다. 이렇게 하면 여러 개의 동일한 프로세스가 생성되고, 그라디언트가 역전파 단계에서 집계됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만, 그것은 최적이 아닙니다!\n\n문제는 동일한 프로세스를 생성했기 때문에 각 GPU에 동일한 모델과 최적화 상태가 있어 불필요한 것입니다. 해결책은 데이터를 분할하는 것입니다. 이를 위해 Fully Sharded Data Parallel PyTorch 래퍼를 사용할 수 있습니다.\n\n어떻게 작동합니까?\n\n언급한 대로, 여러 GPU에서 훈련할 때 DDP로 훈련할 때 각 프로세스는 동일한 데이터의 정확한 사본을 갖게 됩니다. 몇 가지 개선을 구현함으로써 최적화할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 샤드 최적화 상태 (ZeRO 1)\n\nDDP로 학습할 때 각 프로세스가 최적화 상태의 완전한 복사본을 유지합니다. ZeRO1에서는 이러한 최적화 상태를 모든 랭크에 샤딩하여 각 랭크가 최적화 상태의 일부분만 보유하도록 합니다. 역전파 중에 각 랭크는 매개변수에 관련된 최적화 상태만 수집하여 최적화 단계를 수행합니다. 이 중복 감소는 메모리를 절약하는 데 도움이 됩니다.\n\n## 그래디언트 샤딩 (ZeRO 2)\n\n우리는 최적화 상태를 샤딩합니다. 이제 우리는 최적화 단계에서 그래디언트를 또한 샤딩할 것입니다. 하나의 랭크가 매개변수의 일부분에 대한 최적화 상태를 보유하고 있다면, 이제 우리는:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 랭크가 보유한 상태와 관련된 모든 그래디언트를 집계합니다.\n- 최적화 단계를 계산합니다.\n- 매개변수 일부에 대한 최적화 단계를 모든 다른 랭크로 보냅니다.\n\n이제 각 랭크가 그래디언트의 전체 복제본을 보유할 필요가 없다는 것을 알았을 것입니다. 그래디언트를 사용 가능한 대로 해당 랭크로 보낼 수 있습니다. 따라서 최대 메모리 소비를 더욱 줄일 수 있습니다.\n\n## 샤드 모델 매개변수 (ZeRO 3)\n\n이 일이 엄청날 것 같네요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 랭크마다 모델의 전체 복사본을 저장해야 하는 이유가 무엇일까요? 모델 매개변수를 모든 랭크 사이에서 샤딩할 것입니다. 그럼, 순방향 및 역방향 중에 필요한 매개변수를 필요할 때 가져올 것입니다.\n\n## FSDP를 사용하는 방법은?\n\n정말 간단합니다. 모델을 FSDP로 랩핑해야 합니다:\n\n```js\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nmodel = FSDP(model)\n\n# 랩핑된 모델에서 매개변수를 가져오는 것이 중요합니다\n# 오직 반환된 매개변수의 일부분만 사용됨 (샤딩된 부분)\noptimizer = optim.Adam(model.parameters())\n\n# 일반적으로 훈련을 진행합니다\ntrain(model, optimizer)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nFSDP의 샤딩 전략을 지정할 수도 있어요. 예를 들어, SHARD_GRAD_OP 전략을 선택해서 ZeRO2와 유사한 동작을 얻을 수 있어요. 다른 전략에 대해 자세히 알고 싶다면 여기를 확인해 보세요:\n\n또한 FSDP 서브모듈로 랩핑할 수도 있어요. 위의 예에서는 하나의 FSDP 모듈만 사용하여 계산 효율성과 메모리 효율성을 줄일 것이에요. 예를 들어, 모델에 100개의 Linear 레이어가 있다고 가정해봅시다. FSDP(model)을 적용하면, 전체 모델을 랩핑하는 하나의 FSDP 유닛만 있을 것이에요. 이 경우, allgather는 100개의 선형 레이어의 전체 매개변수를 수집하고, 따라서 매개변수 샤딩에 대한 CUDA 메모리를 저장할 수 없을 것이에요.\n\n서브모듈을 명시적으로 랩핑하거나 자동 랩핑 정책을 정의할 수 있어요. FSDP에 대해 더 자세히 알아보려면 PyTorch 가이드를 읽어보세요:\n\n# torch.compile으로 마법처럼 속도 향상\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러니까, torch compile을 활성화하면 코드 실행 속도가 몇 퍼센트나 빨라질 수 있어요. \n\nTorch는 실행 그래프를 추적하고 효율적인 형식으로 변환하여 모델을 거의 Python 호출 없이 실행할 수 있도록 합니다. \n\n기본적인 사용법은 모델을 compile로 감싸는 것입니다:\n\n```js\nimport torch\n\nmodel = torch.compile(model)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 작업은 거의 즉시 실행됩니다. 실제 추적은 첫 번째 전진 중에만 발생합니다.\n\n또한 시도할 가치가 있는 많은 옵션이 있습니다:\n\n토치 컴파일에 대해 더 자세히 알아보세요:\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 게시물은 설명이 완전하지 않습니다. 대신, 즉시 시도해볼 가치가 있는 속도 향상 목록입니다. 도움이 되었기를 바랍니다. 댓글을 자유롭게 남겨주세요!\n\n구독을 고려해보세요.\n\n2024년 5월 28일에 https://alexdremov.me에서 원본이 게시되었습니다.\n\nLinux Foundation의 프로젝트인 PyTorch Blog에서 이미지를 사용했고, Linux Foundation 정책의 적용을 받습니다. 그러므로 모든 이미지는 Creative Commons Attribution 3.0 라이선스에 따라 허용된 대로 사용될 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-22-SimpleWaystoSpeedUpYourPyTorchModelTraining_0.png"},"coverImage":"/assets/img/2024-06-22-SimpleWaystoSpeedUpYourPyTorchModelTraining_0.png","tag":["Tech"],"readingTime":10},{"title":"초고속 행렬 곱셈 필요 없는 LLMs대규모 언어 모델 드디어 등장","description":"","date":"2024-06-22 21:03","slug":"2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere","content":"\n\n최근 ArXiv에 발표된 연구 논문은 오늘날 우리가 알고 있는 LLM(Large Language Model)에 대해 대대적인 변화를 제안했습니다.\n\n이 프로젝트에 참여한 연구진들은 LLM에서 수행되는 핵심 수학적 연산인 Matrix Multiplication (MatMul)을 제거했습니다.\n\n그들은 새로운 MatMul이 없는 LLM이 어떻게 10억 개의 파라미터 규모에서도 강력하게 수행될 수 있으며, 특정 작업에서는 전통적인 LLM보다 성능을 능가할 수 있다는 것을 보여주었습니다!\n\n이것은 단 하나의 최적화로 가능해진 거대한 변화입니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n표 태그를 Markdown 형식으로 변경해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n행렬 곱셈은 첫 번째 행렬의 열 수가 두 번째 행렬의 행 수와 같아야 하는 조건 하에 두 행렬을 곱하여 세 번째 행렬을 생성하는 대수 연산입니다.\n\n![이미지](/assets/img/2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere_0.png)\n\n# LLMs가 행렬 곱셈을 사용하는 방법\n\n행렬 곱셈(MatMul)은 LLMs에서 수행되는 핵심 수학 연산입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저, MatMul은 입력 텍스트에서 토큰과 위치 임베딩을 생성하는 데 사용됩니다.\n\n그런 다음 LLM 내의 트랜스포머에서 Self-attention 메커니즘은 Query(Q), Key(K) 및 Value(V) 행렬을 사용하여 관심 점수 행렬을 계산하는 데 MatMul을 사용합니다.\n\n이러한 Q, K 및 V 행렬은 다시 입력 및 학습된 가중치 행렬을 사용하여 MatMul 작업을 통해 얻어집니다.\n\n다음으로, MatMul은 입력값, 가중치 및 편향에 적용되어 (비선형 활성화 함수를 거친) 밀집 피드포워드 네트워크 내의 출력을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere_1.png\" /\u003e\n\n마침내, LLM의 출력 레이어는 최종 예측을 생성하는 데 이 작업을 사용합니다.\n\n## 오늘날 행렬 곱셈은 어떻게 최적화되나요\n\nCUDA 및 cuBLAS와 같은 라이브러리를 사용하여 MatMul 연산을 강력한 GPU를 사용하여 병렬화하고 가속화하여 효율적으로 수행됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 LLMs의 훈련 및 추론 단계에서 계산 비용의 가장 큰 비율을 차지하고 있다.\n\n지금까지 이러한 연산을 대체하기 위해 많은 노력이 기울어졌지만(AdderNet, 스파이킹 신경망, 이진화된 신경망, BitNet으로), 모두 규모에 사용되었을 때 실패했습니다.\n\n하지만 이제는 다릅니다!\n\n하지만, 우리가 전진해 이해하기 전에 이 MatMul-free LLMs에 대해 자세히 알아야 할 전통적인 LLMs를 이루는 구성 요소에 대해 더 배워보아야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 전통 LLM 구성 요소 이해하기\n\n전통 LLM의 Transformer 구조에는 순차 정보를 이해하는 데 도움이 되는 핵심 구성 요소 두 가지가 있습니다.\n\n## 토큰 믹서\n\n이 구성 요소는 순서열 내에서 서로 다른 토큰 간의 관계를 처리합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n전통적인 LLMs에서의 토큰 혼합은 Self-attention 메커니즘을 사용하여 달성됩니다.\n\n## 채널 믹서\n\n이 구성 요소는 입력 표현의 서로 다른 채널이나 특징 차원 간의 정보 통합을 처리합니다.\n\n채널 혼합은 위치별 피드포워드 네트워크를 사용하여 달성되며, 이는 토큰 혼합 단계에서 추출된 피처를 정제하고 통합합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTransformer 아키텍처에서 이 두 단계를 아래에서 확인할 수 있어요.\n\n![사진](/assets/img/2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere_2.png)\n\n이제 우리가 이들을 알았으니, 진짜 주요 내용으로 넘어가봐요!\n\n# 새로운 LLM 아키텍처에서 MatMul은 어떻게 제거되나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMatMul이 없는 LLM은 전통적인 LLM 아키텍처에 세 가지 주요 변경을 가한 후 만들어 졌어요.\n\n- MatMul이 없는 밀도 레이어\n- MatMul이 없는 토큰 혼합 또는 셀프 어텐션\n- MatMul이 없는 채널 믹서\n\n각각을 자세히 살펴보죠.\n\n## 1. MatMul이 없는 밀도 레이어\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비트넷에 영감을 받아, 밀도 레이어들을 먼저 비트린어 모듈로 대체했습니다.\n\n이 모듈은 삼중 가중치 또는 가중치 행렬의 가중치를 -1, 0 및 +1의 세 가지 가능한 값만 가질 수 있는 테너리 가중치를 사용합니다.\n\n이 제한은 테너리 양자화라고 불리며 MatMul에서의 곱셈 연산을 간단한 덧셈 또는 뺄셈 연산으로 대체합니다.\n\n## 하드웨어 효율적인 융합된 비트린어 레이어\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n원래의 BitNet에서의 BitLinear 레이어는 BitLinear 입력에 앞서 RMSNorm 활성화를 적용합니다.\n\n그러나 이 구현은 GPU의 다양한 유형의 메모리 간에 많은 I/O 작업을 도입하여 효율적이지 않다는 것이 밝혀졌습니다 (HBM 및 SRAM).\n\n따라서 연구원들은 RMSNorm 활성화 및 양자화 단계를 별도의 메모리 작업을 사용하여 따로 수행하는 대신에 SRAM에서 하나의 작업으로 퓨즈된 하드웨어 효율적인 퓨즈드 비트리니어 레이어를 도입했습니다.\n\n## 2. MatMul-free Token Mixer\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n셀프 어텐션 토큰 믹서는 Query(Q), Key(K) 및 Value(V) 매트릭스 간의 MatMul을 포함하고 있는데, 수정된 게이트 순환 유닛(GRU) 아키텍처로 교체되었습니다.\n\n![이미지](/assets/img/2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere_3.png)\n\n다음 변경 사항이 이 수정된 GRU 아키텍처로 이어졌습니다 —\n\n- 이러한 은닉 상태 간의 가중치 및 tanh 활성화가 제거되었습니다.\n- 후보 은닉 상태 계산을 은닉 상태와 연동 해제하고 입력의 선형 변환이 간소화되었습니다.\n- LSTM 아키텍처에서 영감을 받아 은닉 상태와 출력 사이에 데이터 종속적인 출력 게이트가 추가되었습니다.\n- 모든 가중치가 -1, 0 및 +1 세 가지 가능한 값을 갖는 삼진 가중치로 대체되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 3. 매트릭스 곱셈 없이 채널 믹서\n\n피드포워드 네트워크 대신, 게이트된 선형 유닛(GLU)이 매트릭스 곱셈 없는 아키텍처에서 채널 믹서로 사용되었습니다.\n\nGLU와 함께 삼진 가중치를 다시 사용하여 매트릭스 곱셈을 간단한 덧셈과 뺄셈 연산으로 대체했습니다.\n\n![이미지](/assets/img/2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# MatMul 미사용 LLM이 얼마나 잘 수행되었는지\n\nMatMul을 사용하지 않은 LM 모델은 다양한 언어 작업에서 강력한 제로샷 성능을 보여주었습니다. 이 작업은 질문 답변과 상식적 추론부터 물리 이해에 이르기까지 다양합니다.\n\nLlama-2와 같은 많은 인기 있는 LLM에 사용된 Transformer ++ 구조와 비교했을 때 경쟁력 있는 성능을 달성했습니다.\n\nMatMul을 사용하지 않은 27억 개 LLM은 ARC-Challenge 및 OpenbookQA 벤치마크에서 Transformer++ 대응 모델보다 뛰어난 성과를 거뒀다는 점을 기억할 수 있습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere_5.png)\n\n메모리 효율성 측면에서 MatMul-free LLM은 Transformer++보다 모든 모델 크기에서 더 낮은 메모리 사용량과 지연 시간을 보여주었습니다.\n\n13B 매개변수를 가진 가장 큰 모델 크기의 경우, MatMul-free LLM은 GPU 메모리에서 4.19 GB만 사용하고 지연 시간은 695.48밀리초였으며, 반면에 Transformer++은 48.50 GB의 메모리가 필요했고 지연 시간은 3183.10밀리초였습니다.\n\n![이미지](/assets/img/2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere_6.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n연구자들은 자사의 Fused BitLinear 구현과 일반적인 BitLinear 구현을 비교하여 LLM 훈련 속도를 25.6% 향상시키고 메모리 소비량을 61.0% 감소시켰다고 언급했습니다!\n\n![image](/assets/img/2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere_7.png)\n\n그들은 그 후에 MatMul-free LLMs의 삼항 작업을 더 잘 수행하기 위해 Field-programmable gate arrays (FPGA)를 사용한 맞춤형 하드웨어 솔루션을 구축하여 이 모델들의 전력 소비, 대기 시간 및 메모리 사용량을 더 낮췄습니다.\n\n특히, 그들의 13W 파워를 사용하는 13억 개 파라미터 모델은 인간의 독해 속도를 달성하며, 인간 두뇌의 전력 소비와 유사한 효율성을 달성했습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere](/assets/img/2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere_8.png)\n\n드디어 MatMul-free LLMs의 투사를 확장하면, Transformer++보다 오류가 급격하게 감소하는 것을 보여주었습니다.\n\n이는 이러한 LLMs가 성능을 향상시키는 데 도움이 되는 추가 훈련 계산 리소스를 더 효율적으로 사용한다는 것을 의미합니다.\n\n이것은 수조 개 또는 수조 매개변수로 확장되는 미래 MatMul-free LLMs에 매우 유망한 속보입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere_9.png)\n\n저희 연구팀이 직면한 계산 제약으로 인해, MatMul-free LLMs는 아직 극단적으로 대규모로 테스트되지 않았으며 GPT-4와 같이 100B+ 매개변수를 가진 모델과 비교할 수 없습니다.\n\n하지만 저는 그들이 강력한 GPU에 과도하게 의존하는 것 없이 LLM의 교육 및 사용 방법을 개선하기 위한 큰 진전으로 간주합니다. 오늘날 흔한 것입니다.\n\n이에 대한 당신의 생각은 무엇인가요? 댓글에서 알려주세요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 더 많은 자료\n\n- ArXiv에 등재된 'Scalable MatMul-free Language Modeling' 논문\n- MatMul-free LLMs 구현체가 있는 GitHub 저장소\n- ArXiv에 등재된 'BitNet: Scaling 1-bit Transformers for Large Language Models'에 관한 연구 논문\n- 하버드 대학 NLP의 'The Annotated Transformer' 라는 문서\n\n## 만약 나의 작업과 연결을 유지하고 싶다면, 여기에 내 메일링 리스트 링크들이 있습니다 —","ogImage":{"url":"/assets/img/2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere_0.png"},"coverImage":"/assets/img/2024-06-22-SuperfastMatrix-Multiplication-FreeLLMsAreFinallyHere_0.png","tag":["Tech"],"readingTime":7},{"title":"엔티티-리졸브드 지식 그래프란 - 2024 최신 기술 분석 및 사용 방법","description":"","date":"2024-06-22 21:01","slug":"2024-06-22-Entity-ResolvedKnowledgeGraphs","content":"\n\n## 새로운 용어. 오래된 개념. 결국, 데이터 융합에 관한 이야기입니다.\n\nEntity resolution은 한 가지 프로세스입니다. 지식 그래프는 기술적인 아티팩트입니다. 두 가지의 결합은 지식 표현 및 추론 영역에서 가장 강력한 데이터 퓨전 도구 중 하나를 만들어냅니다. 최근에는 ERKG가 데이터 구조 이야기에 존재하게 되었는데, 특히 특정 도메인의 모든 데이터를 연결하여 조사할 수 있는 분석 조직에게 많은 도움을 주고 있습니다. 이 기사에서는 Entity Resolved Knowledge Graph (엔티티 해결 지식 그래프), ER, KG, 그리고 그들의 구현에 대한 세부 정보를 다룰 것입니다.\n\nER. Entity-resolution (별칭 식별 해결, 데이터 매칭, 또는 레코드 연결)은 엔티티가 데이터 집합에서 중복 제거되거나 연결되는 계산 과정입니다. 이것은 데이터베이스에서 Tom Riddle로 목록화된 기록과 T.M. Riddle로 목록화된 기록을 해결하는 것만큼 단순할 수도 있습니다. 또는 별칭(볼드모트), 다른 전화번호, 여러 IP 주소를 사용하여 은행 사기를 저지르는 사람과 같이 복잡할 수도 있습니다.\n\nKG. 지식 그래프는 데이터를 개체와 그들 간의 관계로 시각적으로 나타내는 지식 표현 방식입니다. 개체는 사람, 회사, 개념, 유형 자산, 지리적 위치 등이 될 수 있습니다. 관계는 정보 교환, 커뮤니케이션, 여행, 은행 거래, 계산 거래 등이 될 수 있습니다. 개체와 관계는 그래프 데이터베이스에 저장되며 사전에 연결되어 노드와 엣지로 시각적으로 나타냅니다. 다음과 같이 보입니다...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-Entity-ResolvedKnowledgeGraphs_0.png\" /\u003e\n\n그렇기 때문에...\n\nERKG. 여러 데이터셋을 포함하고 있으며 엔티티가 연결되고 중복이 제거된 지식 그래프입니다. 다시 말해, 중복된 엔티티가 없습니다 (Tom Riddle 및 T.M. Riddle의 노드가 하나의 노드로 해결되었습니다). 또한, 잠재적으로 관련된 노드 사이에 일부 수용 가능한 확률 임계값 내에서 발견된 잠재적인 연결이 있습니다 (예: Tom Riddle, Lord Voldemort, Marvolo Riddle). 여기까지 오면 아마도 \"왜 여러 데이터 소스에서 엔티티를 해결하지 않고 지식 그래프를 만드는 것일까요?\" 라고 묻겠지만요. 간단한 답은 \"당신은 그렇지 않을 것입니다.\" 입니다. 그렇다고 해서 엔티티 해결 방법이나 지식 그래프 표현을 위한 기술에 대한 방법이 ERKG를 만드는 것이 어려운 작업을 만드는 것이다.\n\n우리가 만든 첫 번째 ERKG입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-22-Entity-ResolvedKnowledgeGraphs_1.png)\n\n2016년에 그래프 데이터베이스로 두 데이터셋을 가져왔어요. 하나는 외국 자산 통제국 (OFAC)의 국제 제재 목록에 있는 개인(파란색), 다른 하나는 이름을 밝히지 않을 회사의 고객(분홍색)들이에요. 분명히, 회사의 목적은 OFAC의 데이터베이스를 수동으로 검색하지 않고도 국제 제재된 개인이 고객 중에 있는지 찾는 것이었죠. 이 그래프를 통한 엔티티 해결 프로세스는 이 작업에 지나침이 될 수도 있지만 설명적인 모습이에요. \n\n그래프 속 다수의 해결된 엔티티들은 동일 데이터셋 내에서 두 개 이상의 개인 사이에 있어요 (파란색-파란색 또는 분홍색-분홍색). 이들은 아마도 중복 레코드를 나타냅니다 (이전에 언급한 Tom Riddle vs. T.M. Riddle 문제처럼요). 경우에 따라, 중복 제거가 극단적인 경우도 있어요. 이미지 상단에 있는 분홍색 클러스터들처럼요. 이곳에서 우리는 고객 데이터셋에 있는 한 사람이 5-10개의 별도 레코드로 나타나는 것을 볼 수 있어요. 그러므로, 최소한 회사가 자체 고객 데이터 보유량 내에서 중복 제거 프로세스가 필요하다는 것을 알 수 있어요.\n\n흥미로운 부분은 이미지 상단에서 식별된 파란색-분홍색 관계에 있어요. 회사가 찾던 것이 바로 여기 있어요: 데이터셋 간의 엔티티 해결. 회사의 몇몇 고객들은 아마 국제적으로 제재된 개인일 가능성이 높아요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-22-Entity-ResolvedKnowledgeGraphs_2.png\" /\u003e\n\n이 예는 꽤 간단한 예시로, Entity Resolved Knowledge Graphs(ERKG)를 구축하는 것이 간단한 작업이라고 잘못 결론 내릴 수 있습니다. 하지만 이 작업은 결코 간단하지 않습니다. 특히, 수테라바이트의 데이터와 여러 분석가 사용자에 걸쳐 확장해야 할 경우 더욱 어려워집니다.\n\n가벼운 자연어 처리(NLP) 알고리즘(퍼지 매칭 기술과 같은)은 구현하기에 충분히 간단합니다. 이러한 알고리즘은 Tom Riddle 대 T.M. Riddle과 같은 문제를 쉽게 해결할 수 있습니다. 그러나 두 개 이상의 데이터셋을 결합하고 여러 언어와 국제 문자를 사용하는 경우, 간단한 NLP 프로세스는 꽤 복잡해집니다.\n\n자긍ㄹ이 감지 않는 피해자를 식별하기에는 퍼지 매칭만으로 충분하지 않습니다. 특히, 단일 별칭을 사용하여 신원을 은폐하려는 범죄자를 찾거나 제재나 기타 규정을 회피하려는 사례에는 ER 프로세스에 머신러닝 기반 접근 방식과 이름 이상의 추가 메타데이터를 고려하는 더 복잡한 방법이 필요합니다. 모든 것이 NLP만 있는 것은 아닙니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래프 기반 개체 해결(ER)과 데이터셋 수준의 ER 사이에는 많은 논쟁이 있습니다. 가장 높은 충실도의 그래프 기반 분석을 위해서는 두 가지 모두가 필요합니다. 데이터셋이 그래프 데이터베이스로 가져올 때 데이터셋 내부 및 데이터셋 간의 개체를 해결함으로써 1) 계산 비용이 많이든 대규모 그래프 작업을 최소화하고, 2) 그래프에는 처음에만 해결된 개체(중복 없음)만 포함되어 전체 그래프 아키텍처에 대한 큰 비용 절감을 제공합니다.\n\n한 번 개체 해결 지식 그래프가 존재하면 데이터 과학팀은 그래프 기반 ER 기술을 통해 추가 ER을 더 탐구할 수 있습니다. 이러한 기술은 결합된 데이터셋 간의 잠재적인 연결을 예측할 때 그래프의 토폴로지(즉, 그래프 자체의 내재 구조)를 활용할 수 있는 추가 혜택이 있습니다.\n\nERKG는 강력하고 시각적으로 직관적인 분석 도구가 될 수 있습니다. 다음을 제공합니다:\n\n- 여러 데이터셋을 마스터 그래프 데이터베이스로 병합\n- 분석가가 탐색할 수 있는 시각적으로 표현된 도메인별 지식 그래프\n- 데이터가 연결되고 분석가에게 어떻게 표현되는지 나타내는 실시간 그래프 스키마를 지정할 수 있는 능력\n- 데이터 중복 제거 및 명시적인 연결의 시각적 표현\n- 데이터셋 내부 및 데이터셋 간의 잠재적인 연결(예측된 링크)을 제어할 수 있는 예측 확률 임계치 설정 가능\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nERKG는 주어진 도메인을 여러 데이터 세트를 통해 표현하는 활발하게 상호 연결된 탐구를 그릴 수 있는 분석 캔버스가 됩니다. 이는 데이터 퓨전 솔루션이자 매우 직관적인 것입니다.","ogImage":{"url":"/assets/img/2024-06-22-Entity-ResolvedKnowledgeGraphs_0.png"},"coverImage":"/assets/img/2024-06-22-Entity-ResolvedKnowledgeGraphs_0.png","tag":["Tech"],"readingTime":4},{"title":"거대한 언어 모델 마스터하는 방법","description":"","date":"2024-06-22 20:58","slug":"2024-06-22-MasterLargeLanguageModels","content":"\n\n![MasterLargeLanguageModels](/assets/img/2024-06-22-MasterLargeLanguageModels_0.png)\n\n인공 지능의 광활한 풍경 속에서 혁명적인 힘이 나타났습니다: 대규모 언어 모델(LLMs). 이러한 모델들은 그저 유행어가 아닙니다; 그들은 AI의 미래를 대표합니다. 인간과 유사한 텍스트를 이해하고 생성하는 능력은 그들을 주목받게 만들었으며, 오늘날 가장 흥미로운 그리고 역동적인 연구 분야 중 하나로 떠올랐습니다. 친구처럼 자연스럽게 응답하는 챗봇이나 인간의 글쓰기와 구분할 수 없을 정도로 텍스트를 자연스럽게 생성하는 컨텐츠 생성 시스템을 상상해보세요. 이러한 혁신적인 기술이 귀하의 흥미를 사로잡고 LLMs의 세계로 더 심층적으로 탐험하고 싶다면, 여기에 잘 오셨습니다.\n\n여러분의 여정을 돕기 위해 종합적인 자료 목록을 소개합니다. 이 컬렉션은 다음을 포함합니다:\n\n- 온라인 강의: 초급부터 고급 수준까지 안내해주는 구조화된 학습 경로에 접근하세요. 이 강의들은 LLMs의 이론적 기반, 실용적 구현, 그리고 실습 프로젝트를 다루며, 최고의 강사들로부터 배우고 LLM 기반 애플리케이션을 개발하고 배포하는 데 필요한 기술을 습득할 수 있습니다.\n- 워크샵과 학회: AI와 LLM에 전념하는 워크샵 및 학회에 참석하세요. 이러한 행사들은 전문가들로부터 배우고 동료들과 네트워킹하며 분야에서 최신 연구 및 혁신을 발견할 수 있는 기회를 제공합니다. 학습 경험을 향상시킬 다가오는 행사를 주목하세요.\n- 책과 유익한 기사: AI와 LLM 분야의 선도적인 전문가들이 저술한 종합적인 자료와 기사들을 탐색해보세요. 이러한 자료들은 LLMs를 숙달하는 데 필요한 깊은 지식, 사례 연구, 그리고 실용적인 조언을 제공합니다. LLM 개념, 응용 프로그램, 그리고 분야에서의 최신 발전사항에 대한 자세한 탐구에 몰두하세요. 이 기사들은 LLM의 기초부터 윤리적 영향, 그리고 미래 방향을 포함한 여러 주제를 다룹니다.\n- GitHub 저장소: LLMs를 실험해볼 수 있는 다양한 실무 프로젝트, 코드 샘플, 그리고 도구들을 탐색해보세요. 이러한 저장소들은 실무 경험과 LLMs가 실제 시나리오에서 어떻게 구축되고 사용되는지에 대한 통찰을 제공합니다. 사전 훈련된 모델, 세부 조정 스크립트, 그리고 혁신적인 애플리케이션을 찾아 공부하고 수정해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 자료는 LLM(Large Language Models)에 대한 기초부터 최신 기술까지 철저히 이해할 수 있도록 설계되었습니다. 학생, 연구원 또는 산업 전문가이든, 이 안내서를 통해 LLM을 숙달하는 데 필요한 지식과 도구를 제공할 것입니다.\n\n# 1. 기초 과정\n\n- 머신 러닝 전문화 — Coursera\n\n![LLM 이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n링크: Machine Learning 전문화 프로그램\n\n설명: Machine Learning 전문화는 DeepLearning.AI와 Stanford Online이 공동으로 개발한 기초 온라인 프로그램입니다. 이 초보자 친화적인 프로그램은 머신 러닝의 기본 개념을 가르치고 이러한 기술을 사용하여 현실 세계 AI 애플리케이션을 만드는 방법을 가르쳐줍니다.\n\n이 전문화 프로그램은 Stanford 대학에서 사업적 연구를 이끈 AI 비전가인 Andrew Ng가 가르치고 있습니다. 그는 Google Brain, Baidu, Landing.AI에서 중요한 연구를 이끌었으며 AI 분야를 발전시키기 위한 업적을 이루었습니다.\n\n이 3개 과정으로 이루어진 전문화는 Andrew가 선도적으로 개선한 머신 러닝 과정의 업데이트된 버전으로, 2012년에 시작되어 현재까지 4.8백만 명 이상의 학습자가 수강한 만족도 평가 4.9점을 받았습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. Stanford CS229: Machine Learning Course YouTube by Andrew Ng\n\n![Stanford CS229](/assets/img/2024-06-22-MasterLargeLanguageModels_2.png)\n\nLink: [YouTube Playlist](https://www.youtube.com/playlist)\n\nDescription: 첫 강의인 Andrew Ng의 기계 학습 강의를 듣습니다. 이 강의는 기계 학습과 통계적 패턴 인식에 대한 폭넓은 소개를 제공합니다. 지도 및 비지도 학습, 학습 이론, 강화 학습 그리고 제어에 대해 배울 수 있습니다. 최근 기계 학습 응용 및 기계용 알고리즘을 설계 및 개발해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 딥 러닝 전문화 - Coursera\n\n![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_3.png)\n\n링크: [딥 러닝 전문화](링크 주소)\n\n설명: 딥 러닝 전문화는 딥 러닝의 능력, 도전, 결과를 이해하고 선두주자 AI 기술 발전에 참여할 수 있도록 준비하는 기초 프로그램입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 특화과정에서는 합성곱 신경망, 순환 신경망, LSTM, 트랜스포머 등의 신경망 아키텍처를 구축하고 훈련시키는 방법을 배울 것입니다. 또한 Dropout, BatchNorm, Xavier/He 초기화 등의 전략을 활용하여 이를 개선하는 방법도 배우게 됩니다. 이론적 개념과 해당 산업적 응용을 파이썬과 TensorFlow를 이용하여 숙달하고, 음성 인식, 음악 합성, 챗봇, 기계 번역, 자연어 처리 등과 같은 실제 사례에 대처할 준비를 하세요.\n\n4. Stanford CS224N: NLP with Deep Learning — YouTube\n\n![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_4.png)\n\n링크: Stanford CS224N: NLP with Deep Learning\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n설명: 이것은 NLP에 대한 최신 연구에 대한 철저한 소개와 지식의 보고서 대상지로, 귀하에게 지식의 보고서 대상지와 초심자를 위한 자료 보고서 대상지입니다. 본 강좌는 기계 학습 교수인 Christopher Manning Thomas M. Siebel 교수님에 의해 가르쳐지며, 언어학과 컴퓨터 과학 교수이자 스탠포드 인공지능 연구소 소장입니다.\n\n5. 허깅페이스 트랜스포머 강의 — 허깅페이스\n\n![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_5.png)\n\n링크: [허깅페이스 트랜스포머 강의](링크 주소)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDescription: 이 강좌는 HuggingFace 생태계의 라이브러리를 사용하여 NLP를 가르칩니다. HuggingFace의 다음 라이브러리의 내부 작업 및 사용법을 다룹니다:\n\n- Transformers\n- Tokenizers\n- Datasets\n- Accelerate\n\n저자 소개:\n\nAbubakar Abid는 스탠포드 대학에서 응용 기계 학습 분야에서 박사 학위를 받았습니다. 박사 과정 중에 그는 Gradio를 설립했는데, 이는 60만 개 이상의 머신 러닝 데모를 구축하는 데 사용되는 오픈 소스 Python 라이브러리입니다. Gradio는 Hugging Face에 의해 인수되었으며, 현재 Abubakar는 여기서 머신 러닝 팀 리드로 근무하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n매튜 카리건은 허깅페이스에서 머신러닝 엔지니어로 일하고 있습니다. 아일랜드 더블린에 살며, 이전에는 Parse.ly에서 ML 엔지니어로 일했으며, 그 전에는 트리니티 대학교에서 박사 후 연구원으로 일했습니다. 기존 구조를 확장하여 AGI에 도달할 것이라고 믿지 않지만, 로봇 불멸에 대한 높은 희망을 품고 있습니다.\n\n리산드르 데뷔는 허깅페이스에서 머신러닝 엔지니어로 일하고 있으며, 매우 초기 개발 단계부터 🤗 트랜스포머 라이브러리에 참여해왔습니다. 그의 목표는 매우 간단한 API를 통해 모두에게 자연어 처리를 액세스할 수 있도록 하는 것입니다.\n\n실반 구거는 허깅페이스의 연구 엔지니어이자 🤗 트랜스포머 라이브러리의 핵심 유지 보수자 중 한 명입니다. 이전에는 fast.ai에서 연구 과학자로 일하며, Jeremy Howard와 함께 '딥러닝 코더를 위한 fastai와 PyTorch'를 공동 저술했습니다. 그의 연구 주요 관심사는 깊은 학습을 보다 쉽게 접근할 수 있도록 하는 것으로, 한정된 자원 상에서 빠르게 모델을 교육할 수 있도록 설계하고 개선하는 기술을 고안하고 있습니다.\n\n다우드 칸은 허깅페이스에서 머신러닝 엔지니어로 일하고 있습니다. 뉴욕 출신이며, 뉴욕 대학교에서 컴퓨터 과학을 전공했습니다. 몇 년간 iOS 엔지니어로 일한 후 다우드는 동료 창업자들과 함께 Gradio를 시작했습니다. Gradio는 결국 허깅페이스에 인수되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLewis Tunstall은 Hugging Face의 기계 학습 엔지니어로, 오픈 소스 도구를 개발하고 널리 사용할 수 있도록 하는 데 중점을 두고 있습니다. 또한 O'Reilly의 책 \"Transformers와 함께하는 자연어 처리\"의 공동 저자입니다.\n\nLeandro von Werra는 Hugging Face의 오픈 소스 팀의 기계 학습 엔지니어이자 O'Reilly의 책 \"Transformers와 함께하는 자연어 처리\"의 공동 저자입니다. 그는 NLP 프로젝트를 전체 기고 학습 스택을 통해 작업하여 프로덕션 단계로 이끌어온 수 년 간의 산업 경험을 가지고 있습니다.\n\nChatGPT 개발자를 위한 프롬프트 엔지니어링 - Coursera\n\n![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n링크: ChatGPT 프롬프트 엔지니어링 코스\n\n설명: ChatGPT는 인기 있는 LLM 모델이며, 이 코스는 더 나은 응답 생성을 위한 효과적인 프롬프트 작성의 가장 좋은 방법과 필수 원칙을 공유합니다.\n\n## 2. LLM 특화 코스\n\n- LLM 대학 — Cohere\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_7.png)\n\n링크: LLM 대학교\n\n설명: Cohere는 LLM을 숙달하기 위한 전문 과정을 제공합니다. 이론적 측면에서 NLP, LLM 및 세부 아키텍처를 자세히 다루는 순차적 트랙은 초심자를 대상으로 합니다. 비순차적 경로는 내부 작동보다는 강력한 모델의 실용적인 응용 및 사용 사례에 관심이 있는 경험 많은 개인을 위한 것입니다.\n\n2. Stanford CS324: 대형언어모형 — Stanford 사이트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_8.png)\n\n링크: Stanford CS324: Large Language Models\n\n설명: 본 과정은 다양한 모델의 복잡성에 대해 심층적으로 다룹니다. 기본 원리, 이론적 구조, 윤리적 고려 사항 및 실용적 적용에 대해 다루며, 학생들은 이러한 모델에 대한 포괄적인 이해를 얻게 됩니다. 또한, 이 과정은 현실적인 시나리오에서 학습자들이 자신의 지식을 적용할 수 있는 실습 경험을 제공하여 이론과 실무 간의 간극을 메울 수 있도록 돕습니다.\n\n3. Princeton COS597G: Understanding Large Language Models — Princeton Site\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_9.png)\n\n링크: [대형 언어 모델 이해하기](링크)\n\n설명: 이는 근본적인 커리큘럼을 제공하는 대학원 수준의 과정으로, 심층 학습에 탁월한 선택입니다. BERT, GPT, T5 모델, 전문가 모델의 혼합, 검색 기반 모델 등과 같은 모델의 기술적 기반, 기능, 한계를 탐험하게 됩니다.\n\n4. 풀 스택 LLM 부트캠프 — 풀 스택\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![LLM Bootcamp](/assets/img/2024-06-22-MasterLargeLanguageModels_10.png)\n\n링크: [Full Stack LLM Bootcamp](링크)\n\n설명: 풀 스택 LLM 부트캠프는 산업에 적합한 종합적인 코스로, 참가자들이 LLM 애플리케이션을 개발하고 배포하는 데 필수적인 기술을 갖추도록 합니다. 커리큘럼에는 프롬프트 엔지니어링 기술, LLM의 기초, 배포 전략 및 사용자 인터페이스 디자인 등의 다양한 주제가 포함되어 있습니다. 이러한 핵심 영역을 다루면서 부트캠프는 참가자들이 현실 세계 상황에서 LLM 기반 애플리케이션을 성공적으로 개발하고 구현하는 데 필요한 지식과 전문 지식을 얻을 수 있도록 보장합니다.\n\n5. ETH Zurich: 대 언어 모델(LLM) - RycoLab\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_11.png)\n\n링크: ETH Zurich: Large Language Models\n\n설명: 이 새롭게 디자인된 강좌는 대규모 언어 모델 (LLMs)에 대해 심층적으로 탐구합니다. 이 강좌는 이러한 모델의 기초가 되는 확률적 원리, 신경망 모델링의 복잡성, 그리고 이를 훈련하는 과정에 대해 심층적으로 다룹니다. 또한, 이 강좌는 대규모 데이터셋을 처리하고 모델 성능을 개선하는데 필수적인 확장 기술을 다룹니다. 또한, LLM의 보안 및 잠재적 남용에 대한 중요한 토론이 포함되어 있어, 이러한 첨단 기술과 관련된 기술의 능력과 윤리적 고려 사항에 대한 광범위한 이해를 보장합니다.\n\n6. 대규모 언어 모델 파인 튜닝 — Coursera\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-22-MasterLargeLanguageModels_12.png)\n\nLink: [Fine Tuning Large Language Models](#)\n\nDescription: 파인튜닝은 대형 언어 모델 (LLMs)을 사용자의 특정 요구에 맞게 조정할 수 있는 기술입니다. 본 강좌를 수료하면, 파인튜닝을 적용해야 하는 시점과 데이터를 이 기술에 맞게 준비하는 방법, 그리고 새로운 데이터로 LLM을 훈련하는 과정 등을 이해하게 됩니다. 또한 파인튜닝 모델의 성능을 평가하는 방법과 목표를 달성하고 의도한 애플리케이션에서 효과적으로 작동할 수 있도록 보장하는 방법도 배울 것입니다.\n\n# 3. 기사 / 책\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- ChatGPT이 하는 일은 무엇이며 왜 작동하는 걸까? — 스티븐 월프램\n\n![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_13.png)\n\n링크: ChatGPT가 하는 일은 무엇이며 왜 작동하는 걸까?\n\n설명: 명성 높은 과학자인 스티븐 월프램은 ChatGPT의 기본 측면을 탐구하는 짧은 책을 썼습니다. 그는 신경망에서의 기원부터 변압기, 주의 메커니즘 및 자연어 처리를 통해 진보한 과정을 추적합니다. 이 책은 대형 언어 모델의 기능과 한계를 이해하고 싶은 사람들에게 강력히 추천됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. 시리즈 기사: 대형 언어 모델 — Jay Alammar\n\n![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_14.png)\n\n링크: 시리즈 기사: 대형 언어 모델\n\n설명: Jay Alammar의 블로그는 대형 언어 모델(LLM)과 트랜스포머를 공부하는 사람들에게 지식의 보물창고입니다. 그의 블로그는 시각화, 직관적 설명, 그리고 주제에 대한 포괄적인 다룸이 돋보여 독특합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4. 프로덕션용 LLM 애플리케이션 구축 — Chip Huyen\n\n![LLM Applications for Production](/assets/img/2024-06-22-MasterLargeLanguageModels_15.png)\n\n링크: 프로덕션용 LLM 애플리케이션 구축\n\n설명: 이 기사에서는 LLMs를 프로덕션에 적용하는 도전을 논의합니다. 작업 조합성에 대한 통찰을 제공하며 유망한 사용 사례를 소개합니다. 실용적인 LLM에 관심 있는 모든 사람들에게 정말 가치 있는 내용입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 4. 깃허브 저장소\n\n![이미지](/assets/img/2024-06-22-MasterLargeLanguageModels_16.png)\n\n1. Awesome-LLM ( 15.7k ⭐ )\n\n링크: [Awesome-LLM](https://github.com/username/Awesome-LLM)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n설명: ChatGPT를 중심으로 초대형 언어 모델(LLM)에 초점을 맞춘 논문, 프레임워크, 도구, 강좌, 튜토리얼 및 리소스가 정리된 컬렉션입니다.\n\n2. LLMsPracticalGuide ( 8.9k ⭐ )\n\n링크: 대규모 언어 모델에 대한 실용적인 가이드\n\n설명: LLMs의 방대한 영역을 탐색하는 실무자들을 돕는데 도움이 됩니다. ChatGPT를 다룬 연구 논문인 \"ChatGPT 및 그 이상에 대한 실무에서 LLMs의 힘을 이용하기: ChatGPT 및 이상에 대한 조사\"와 이 블로그를 기반으로 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. LLMSurvey (9.4k⭐)\n\n링크: LLMSurvey\n\n설명: 이것은 \"대형 언어 모델 설문\"이란 제목의 논문을 기반으로 한 설문 자료 및 리소스 모음입니다. 또한 GPT 시리즈 모델들의 기술적 진화를 보여주는 그림과 LLaMA에 대한 연구 작업의 진화 그래프도 포함되어 있습니다.\n\n4. Awesome Graph-LLM (1.4k⭐)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n링크: Awesome-Graph-LLM\n\n설명: 그래프 기반 기술과 LLMs의 교차점에 흥미를 가지는 사람들에게 유용한 소스입니다. 이 사이트는 이 신흥 분야를 탐구하는 연구 논문, 데이터셋, 벤치마크, 서베이 및 도구를 제공합니다.\n\n5. Awesome Langchain ( 7k ⭐ )\n\n링크: awesome-langchain\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n설명: LangChain은 LLM 프로젝트에 대한 빠르고 효율적인 프레임워크이며 이 저장소는 LangChain 생태계와 관련된 계획 및 프로젝트를 추적하는 중심입니다.\n\n# 5. 추가 자료 — 연구 및 설문 조사 논문\n\n![마스터 대형 언어 모델](/assets/img/2024-06-22-MasterLargeLanguageModels_17.png)\n\n- \"AIGC 시대의 ChatGPT에 대한 완전한 조사\" — LLM 초보자에게는 좋은 시작점입니다. ChatGPT의 기술, 응용 및 도전에 대해 포괄적으로 다룹니다.\n- \"대형 언어 모델 조사\" — 최근 LLM의 발전을 다루며 사전 훈련, 적응 조정, 활용 및 용량 평가의 네 가지 주요 측면을 특히 다룹니다.\n- \"대형 언어 모델의 도전과 응용\" — LLM의 도전 과제와 성공적인 적용 영역을 논의합니다.\n- \"Attention Is All You Need\" — Transformer는 GPT 및 기타 LLM의 기초를 이루며, 이 논문은 Transformer 아키텍처를 소개합니다.\n- \"주석이 달린 Transformer\" — Transformer 아키텍처에 대해 자세히 설명하고 주석이 달린 하버드 대학의 자료이며, 이는 여러 LLM에 기본적인 개념입니다.\n- \"설명된 Transformer\" — 복잡한 개념을 이해할 수 있도록 도와주는 시각 안내서로 Transformer 아키텍처를 깊이 파악할 수 있습니다.\n- \"언어 이해를 위한 Deep Bidirectional Transformer 사전 훈련인 BERT\" — 이 논문은 BERT를 소개하며, 이는 다수의 자연어 처리(NLP) 작업에 대한 새로운 기준을 제시하는 매우 영향력 있는 LLM입니다.","ogImage":{"url":"/assets/img/2024-06-22-MasterLargeLanguageModels_0.png"},"coverImage":"/assets/img/2024-06-22-MasterLargeLanguageModels_0.png","tag":["Tech"],"readingTime":11},{"title":"대형 언어 모델LLM 작동 방식 완벽 가이드 기초부터 고급까지","description":"","date":"2024-06-22 20:56","slug":"2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero","content":"\n\n제 ‘제로 투 히어로’ 시리즈의 두 번째 기사입니다. 이 기사에서는 대형 언어 모델 (LLM)이 작동하는 방식을 쉽게 설명해 드릴 예정입니다.\n\n# LLM 작동 방식\n\n먼저 문서 완성 모델이 어떻게 작동하는지 살펴봅시다:\n\n![이미지](/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사용자 프롬프트:\n\n```js\n바나나는\n```\n\n모델 응답:\n\n```js\n납작하고 먹을 수 있는 과일\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼, 문서 생성기 모델은 이렇게 작동합니다:\n\n![image](/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_1.png)\n\n사용자 프롬프트:\n\n```js\n나는 새 차를 사고 싶어요\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"테이블 태그를 Markdown 형식으로 바꿔주세요.\"\n\n위의 두 가지 사항 사이의 차이점을 주목해주세요.\n\n첫 번째 모델은 문서 완성기로, 다음 문자가 될 가능성이 가장 높은 것을 찾아서 프롬프트만 완성합니다. 이 모델은 인터넷 데이터의 일부분을 학습한 것으로, 기본 모델이라고 불립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 번째 모델은 문서 생성기입니다. 이 모델은 프롬프트 질문을 기반으로 한 인간과 유사한 응답을 생성합니다. 이것은 ChatGPT 모델입니다.\n\nChatGPT 모델은 프롬프트 질문을 기반으로 한 응답을 생성할 수 있는 추론 모델입니다. 저는 이 모델이 99% 베이스 모델이지만 두 가지 추가 단계인 파인튜닝 단계와 인간 피드백에서의 강화 학습 단계가 있습니다.\n\n# 사전 훈련: 베이스 모델\n\n이것은 인공 지능 혁명의 핵심이자 실제 마법이 일어나는 곳입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델을 훈련하는 것은 많은 데이터를 제공하고 그것으로부터 배우는 과정입니다.\n\nGPT-3 논문에 설명된 대로, 기본 모델은 인터넷 데이터의 대량을 바탕으로 훈련됩니다. 그것은 여러분과 같은 일반인에게는 쉬운 일이 아닙니다. 데이터를 획득하는 것 뿐만 아니라 GPU 및 TPU와 같은 많은 컴퓨팅 파워도 필요합니다.\n\n하지만 걱정 마세요. 우리 자신의 컴퓨터에서 작은 GPT 모델을 훈련하는 방법을 여전히 배울 수 있습니다. 다음 주제에서 어떻게 하느지 보여 드리겠습니다.\n\nLLM 훈련 뒤에 있는 혁신은 Transformer 아키텍처의 도입에 있습니다. 이를 통해 모델은 광범위한 데이터에서 배우면서 입력의 서로 다른 부분 간의 중요한 맥락적 관골을 유지할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 연결을 유지함으로써 모델은 제공된 문맥을 기반으로 새로운 통찰력을 효과적으로 추론할 수 있습니다. 이 문맥은 개별 단어, 문장, 문단 또는 그 이상일 수 있습니다. 이 능력을 통해 LLM 훈련은 자연어 처리 및 생성 작업에 대한 새로운 기회를 열어주어 기계가 인간의 의사 소통에 더 잘 이해하고 응답할 수 있도록 합니다.\n\n기본 모델을 훈련하는 데 사용된 트랜스포머 아키텍처는 아래에 표시됩니다:\n\n\n![LLM](/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_2.png)\n\n\n이는 일부 이전 및 새로운 기법을 사용하여 훈련된 신경망 모델입니다: 토큰화, 임베딩, 위치 인코딩, 피드포워드, 정규화, 소프트맥스, 선형 변환 및 가장 중요한 것은 멀티헤드 어텐션입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 부분은 당신과 저 모두가 대부분 관심을 가지고 있는 부분이에요. 우리는 아키텍처 뒤의 아이디어와 교육 과정이 정확히 어떻게 이루어졌는지 명확하게 이해하고 싶어해요. 그래서 다음 글부터는 논문, 코드, 그리고 기본 모델을 훈련하는 데 사용된 수학적인 부분을 자세히 살펴볼 거에요.\n\n# Fine-tuning: 어시스턴트 훈련하기\n\nFine-tuning은 아주 똑똑한 구현이에요. 아마 OpenAI에 의해 처음 수행된 것 같아요. 아이디어는 아주 간단하지만 지능적으로 작동해요: 인간 라벨러를 고용하여 수많은 Q\u0026A 대화쌍을 생성하게 해요(예: 10만 대화). 그런 다음 모델에 대화쌍을 주입시켜 이를 통해 학습시키는 거죠.\n\n이 과정을 Fine-tuning이라고 해요. 그 10만 개의 대화 샘플 데이터가 모델에 훈련되면 뭔 일이 벌어질까요? 모델이 인간처럼 응답하기 시작할 거에요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 샘플 레이블 대화를 살펴보겠습니다:\n\n```js\n인간 레이블된 Q\u0026A\n\nQ: 이름이 뭐에요?\nA: 제 이름은 존입니다.\n```\n\n```js\n인간 레이블된 Q\u0026A\n\nQ: 중국의 수도는 무엇인가요?\nA: 중국의 수도는 북경입니다.\n```\n\n```js\n인간 레이블된 Q\u0026A\n\nQ: 영화 타이타닉의 줄거리를 요약해주세요.\nA: 영화 타이타닉은 바다에서 침몰하는 배에 관한 이야기입니다.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n와, 이 샘플 Q\u0026A들은 우리가 서로 대화하는 방식을 조롱하고 있는 것 같아요.\n\n모델에게 이러한 응답 스타일을 가르쳐주면, 관련된 맥락에 대한 응답 확률이 매우 높아지며 사용자의 질문에 응답할 수 있게 될 거예요. 모델을 여러 대화 스타일로 훈련시킴으로써, 관련성이 높고 맥락에 맞는 응답을 제공할 가능성이 높아집니다.\n\n이것이 언어 모델이 얼마나 지적이고 인간 같아 보일 수 있는지; 실제 대화의 리듬과 패턴을 모방하면, 사용자와의 대화를 흐름 있게 시뮬레이션할 수 있어요.\n\n우리는 여기서 어시스턴트 모델을 얻었다고 말할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 기본 모델을 사전 훈련에서 보조 모델을 미세 조정하는 과정 중 일부 강조 사항을 보여주는 다이어그램입니다:\n\n![다이어그램](/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_3.png)\n\n# RLHF: 인간 피드백으로부터 강화 학습\n\n2022년 1월, OpenAI는 언어 모델을 지시 사항을 따르도록 조정하는 연구를 발표했습니다. 블로그 게시물에서 그들은 모델이 인간 피드백으로 더욱 미세하게 조정된 방법을 설명했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_4.png)\n\n이 부분은 조금 까다로울 수 있어요. 아이디어는 모델이 인간 피드백에서 배울 수 있게 하는 것입니다. 약 10만 개의 레이블이 지정된 Q\u0026A 쌍을 제공하는 대신, 사용자의 프롬프트와 모델 응답을 수집한 다음 사람들이 순위 매기도록 합니다. 이 순위 정리된 대화를 가장 원하는 Q\u0026A 샘플로 삼아 다시 모델에 피드하고 전반적인 성능을 향상시키도록 합니다.\n\n이 프로세스는 OpenAI의 블로그에서 소개되었습니다.\n\n우리 모델을 보다 안전하고 유용하며 일치하게 만들기 위해 기존 기술인 인간 피드백 강화 학습 (RLHF)을 사용합니다. API에 고객이 제출한 프롬프트에서, 우리의 레이블러는 원하는 모델 행동을 시연하고 모델에서 여러 출력을 순위 매깁니다. 그런 다음 우리는 이 데이터를 사용하여 GPT-3를 세밀하게 조정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기에 베이스 모델 대 미세 조정/RLHF 응답 비교가 있어요:\n\n![Comparison](/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_5.png)\n\n미세 조정 및 RLHF 없이 모델은 단순히 문서 완성기일 뿐임을 확인할 수 있어요.\n\n# 프롬프트 엔지니어링\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 세밀한 조정과 RLHF를 사용하더라도 모델은 여전히 원하는 응답을 얻기 위해 도움이 필요합니다. 그리고 이것이 바로 프롬프트 엔지니어링이 필요한 이유입니다.\n\n간단히 말해, 우리는 모델로부터 원하는 응답을 얻기 위해 프롬프트를 신중하게 디자인할 수 있습니다 (때로는 세밀한 조정 없이도).\n\n수학과 코드에 너무 깊이 들어가지 않으려는 경우, 프롬프트 엔지니어링이 주목할 가치가 있는 방법입니다. 왜냐하면 더 나은 프롬프트를 입력함으로써 LLM 모델에서 최상의 결과를 얻을 수 있기 때문입니다.\n\n이제 예시를 살펴보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nThe sky is\n```\n\nblue.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 테이블을 Markdown 형식으로 변경해 주세요.\n\n| 이름 | 나이 | 성별 |\n|------|-----|-----|\n| 민수 | 25 | 남성 |\n| 지영 | 22 | 여성 |\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n낮에는 파란색이고 밤에는 어두운 색입니다.\n```\n\n프롬프트에 일부 지침을 포함하면 모델이 무엇을 해야 하는지와 어떻게 응답해야 하는지 알 수 있습니다.\n\n또 다른 흥미로운 예제를 살펴보겠습니다:\n\n프롬프트:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n6살 때 내 언니 나이의 절반이었어. 지금 나는 70살, 내 언니 몇 살일까? \n\n\n결과:\n\n```js\n35\n```\n\n답은 틀렸어요. 정답은 67살이에요. 모델은 질문을 이해하는 것 같지만 논리적 추론 대신 수학 계산을 참고한 것 같네요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n미세 조정 및 RLHF 없이도 프롬프트에 더 많은 예제 지침을 추가하는 것만으로도 올바른 답을 얻을 수 있습니다:\n\n프롬프트:\n\n```js\nQ: 수목원에는 15 그루의 나무가 있습니다. 오늘 나무원 작업자들이 나무를 심을 것입니다. 작업을 마치고 나면, 21 그루의 나무가 될 것입니다. 나무원 작업자들은 오늘 몇 그루의 나무를 심었습니까?\nA: 우리는 15 그루의 나무로 시작합니다. 나중에는 21 그루의 나무가 있습니다. 차이는 그들이 심은 나무의 수여야 합니다. 따라서, 그들은 21 - 15 = 6 그루의 나무를 심었을 것입니다. 정답은 6입니다.\nQ: 주차장에 차가 3대 있고 더 2대의 차가 도착한다면, 주차장에는 몇 대의 차가 있습니까?\nA: 주차장에는 이미 3대의 차가 있습니다. 2대가 추가로 도착합니다. 이제 차가 3 + 2 = 5대 있습니다. 정답은 5입니다.\nQ: 리아는 초콜릿 32개를 가지고 있었고, 그녀의 여동생은 42개를 가지고 있었습니다. 그들이 35개를 먹었다면, 두 사람이 남은 총 조각 수는 얼마입니까?\nA: 리아는 초콜릿 32개를 가지고 있었고, 리아의 여동생은 42개를 가지고 있었습니다. 이는 원래 32 + 42 = 74개의 초콜릿이 있었음을 의미합니다. 35개가 먹혔습니다. 그래서 총으로 계산하면 74 - 35 = 39개의 초콜릿이 남게 됩니다. 정답은 39입니다.\nQ: 제이슨은 막대사탕 20개를 가지고 있었습니다. 그는 덴니에게 일부 막대사탕을 주었습니다. 지금은 제이슨이 막대사탕 12개를 가지고 있습니다. 제이슨이 덴니에게 몇 개의 막대사탕을 주었습니까?\nA: 제이슨은 막대사탕 20개를 가지고 있었습니다. 지금은 12개밖에 없으므로, 나머지를 덴니에게 줬을 것입니다. 따라서, 덴니에게 준 막대사탕 수는 20 - 12 = 8개입니다. 정답은 8입니다.\nQ: 숀은 5개의 장난감을 가지고 있습니다. 크리스마스 때, 엄마와 아빠로부터 각각 2개의 장난감을 받았습니다. 지금은 몇 개의 장난감이 있습니까?\nA: 그는 5개의 장난감을 가지고 있습니다. 엄마로부터 2개를 받았으므로, 이후 그는 5 + 2 = 7개의 장난감을 가지고 있습니다. 그리고 아빠로부터 또 2개를 받았으므로 총으로 계산하면 7 + 2 = 9개의 장난감이 있습니다. 정답은 9입니다.\nQ: 서버실에는 컴퓨터가 9대 있었습니다. 월요일부터 목요일까지 매일 추가로 5대의 컴퓨터가 설치되었습니다. 지금 서버실에는 몇 대의 컴퓨터가 있습니까?\nA: 월요일부터 목요일까지는 4일이 있습니다. 매일 5대의 컴퓨터가 추가되었습니다. 이는 총 4 * 5 = 20대의 컴퓨터가 추가되었음을 의미합니다. 처음에는 9대의 컴퓨터가 있었으므로, 현재 서버실에는 9 + 20 = 29대의 컴퓨터가 있습니다. 정답은 29입니다.\nQ: 마이클은 골프공 58개를 가지고 있습니다. 화요일에 23개의 골프공을 잃었습니다. 수요일에 또 2개를 잃었습니다. 수요일 종료 시에는 몇 개의 골프공이 있습니까?\nA: 마이클은 처음에 58개의 골프공을 가지고 있었습니다. 화요일에 23개를 잃었으므로, 그 후에는 58 - 23 = 35개의 골프공을 가지고 있습니다. 수요일에 2개를 더 잃었으므로, 이제 35 - 2 = 33개의 골프공이 있습니다. 정답은 33입니다.\nQ: 올리비아는 $23을 가지고 있습니다. 그녀는 각각 $3에 5개의 베이글을 샀습니다. 그녀가 남은 돈은 얼마입니까?\nA: 그녀는 각각 $3에 5개의 베이글을 샀습니다. 이는 총 $15를 소비한 것을 의미합니다. 그녀는 $8을 남겼습니다.\nQ: 제가 6살이었을 때, 내 여동생은 내 나이의 절반이었습니다. 지금 저는 70살입니다. 내 여동생은 몇 살입니까?\nA:\n```\n\n결과:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n6살 때 내 언니는 저의 나이의 절반, 즉 3살이어서 3살이었습니다. 이제 저는 70살이므로, 언니는 70 - 3 = 67살입니다. 정답은 67입니다.\n```\n\noutput 2:\n\n```js\n이야기의 주인공이 6살일 때, 그의 언니는 그의 나이의 절반인 3살이었습니다. 이제 주인공이 70살이 되었으므로, 언니는 70 - 3 = 67세가 될 것입니다. 정답은 67입니다.\n```\n\n두 번째 답변도 정확합니다! 우리는 문제에 대한 논리적 설명 예시를 추가하여 모델이 질문을 이해하고 올바르게 답변할 수 있도록 했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 예시는 Wang 등(2022)에 의해 소개된 것으로, 최종 답안을 계산하는 과정은 몇 단계로 이루어져 있습니다.\n\n강력한 프롬프트는 모델이 수학 문제 해결이나 텍스트 요약과 같은 복잡한 작업을 수행하는 데 도움이 될 수 있습니다. 따라서 프롬프트 엔지니어링은 LLM 생태계의 매우 중요한 역할을 합니다.\n\n프롬프트 엔지니어링에 대해 더 알고 싶다면, 여기 좋은 프롬프트 가이드 튜토리얼이 있습니다.\n\n# 요약\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기까지 읽어주셔서 정말 감사합니다. 특히 LLM 세계에 처음 접하는 분들에게는 모든 정보를 소화하는 데 시간이 걸릴 것이라고 확신합니다.\n\n이제 기본 개념과 배경 정보에 대해 충분한 내용을 다루었다고 믿습니다. 이제 우리 자신의 대형 언어 모델을 구축하기 위한 준비를 시작할 때입니다. 이론에는 지겨워했으니, 다음 기사에서는 Transformers 아키텍처의 중요한 구성 요소로 나아갈 것입니다.","ogImage":{"url":"/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_0.png"},"coverImage":"/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_0.png","tag":["Tech"],"readingTime":8}],"page":"27","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"27"},"buildId":"kkckKPyxcjJp_o8wb2unW","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>