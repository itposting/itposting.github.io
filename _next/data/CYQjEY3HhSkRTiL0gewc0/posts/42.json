{"pageProps":{"posts":[{"title":"Stable Diffusion 설치 및 실행 방법 MacOS용","description":"","date":"2024-06-22 16:15","slug":"2024-06-22-HowtoInstallandRunStableDiffusiononyourMacOS","content":"\n\n안녕하세요, 독자 여러분! 저는 AI 및 LLM(언어 모델) 기술의 흥미로운 세계를 안내해 드리는 탈립입니다. 이것은 흥미진진한 여정이며, 제가 여러분과 제 경험과 발견을 공유할 수 있어 정말 기쁩니다.\n\n# 목차\n\n- Stable Diffusion이란 무엇인가요?\n- 맥에 Stable Diffusion 웹 UI 설치하기\n- 맥에서 Stable Diffusion 웹 UI 실행하기\n- 맥에서 Stable Diffusion 웹 UI 종료하기\n- 맥에서 Stable Diffusion 웹 UI 업데이트하기\n\n# Stable Diffusion이란 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n안정적인 확산은 Stability AI에서 2022년에 출시된 텍스트-이미지 확산 모델입니다. 이 모델은 이미지에 점진적으로 잡음을 추가하여 완전히 무작위로 만들어지는 확산 과정에 기반을 두고 있습니다. 안정적인 확산은 반대 방향으로 작동하여, 잡음이 많이 섞인 이미지에서 시작하여 점진적으로 잡음을 제거하여 선명한 이미지를 생성합니다.\n\n이 모델은 텍스트와 이미지의 대량 데이터셋으로 훈련되어 있어 텍스트 설명으로 실제적이고 고품질의 이미지를 생성할 수 있습니다. 안정적인 확산은 매우 다재다능하며, 사실적인, 예술적인 및 추상적인 이미지 스타일 등 다양한 이미지 스타일을 생성하는 데 사용할 수 있습니다.\n\n![이미지](/assets/img/2024-06-22-HowtoInstallandRunStableDiffusiononyourMacOS_0.png)\n\n# 맥에서 안정적인 확산 웹 UI 설치\n\n<div class=\"content-ad\"></div>\n\n## 단계 1 — Homebrew 설치하기\n\n- 먼저 터미널 애플리케이션을 열어주세요. 이 앱은 Applications 디렉토리 내의 Utilities 하위폴더에 있습니다. 또는 Spotlight 검색을 이용하여 Command + Space를 눌러 \"터미널\"을 입력해도 됩니다.\n- 터미널을 열었으면, 다음 명령어를 붙여넣어 실행합니다. 이 명령어는 Homebrew 설치 스크립트를 다운로드하고 실행합니다.\n\n```js\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n```\n\n- 스크립트는 설치를 계속할지 여부를 묻고, Homebrew를 설치하려면 관리자 권한이 필요하므로 비밀번호를 요청합니다. 비밀번호를 입력하고 (입력할 때는 보이지 않습니다) Enter 키를 누릅니다. 설치 과정은 몇 분 정도 소요되며 필요한 파일과 종속성을 다운로드하고 설치합니다.\n- 설치가 완료되면, Homebrew가 성공적으로 설치되었다는 메시지가 표시됩니다.\n- 터미널에서 어느 디렉토리에서든 Homebrew 명령어를 인식하고 사용할 수 있도록 하려면, PATH 환경 변수를 조정해야 합니다. 프롬프트에서 제공된 명령어를 복사하여 붙여넣고 Enter 키를 눌러 환경 변수를 설정할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-HowtoInstallandRunStableDiffusiononyourMacOS_1.png\" />\n\n## 단계 2 — Homebrew를 사용하여 필요한 애플리케이션 설치하기\n\n터미널 앱에서 다음 명령어를 실행하세요:\n\n```js\nbrew install cmake protobuf rust python@3.10 git wget\n```\n\n<div class=\"content-ad\"></div>\n\n홈브루가 Mac에 다운로드하고 설치할 패키지 목록입니다:\n\n- CMake: CMake은 오픈 소스이며 크로스 플랫폼 빌드 시스템입니다. 이를 통해 소프트웨어 프로젝트의 빌드 프로세스를 관리하는 것이 간편화되며, 플랫폼 독립적인 방법으로 소프트웨어를 구성, 빌드 및 테스트할 수 있습니다.\n- Protocol Buffers (protobuf): Protocol Buffers는 Google에서 개발한 언어에 상관없는 직렬화 포맷입니다. 간단한 언어를 사용하여 데이터 구조를 정의한 다음 다양한 프로그래밍 언어의 코드를 생성할 수 있습니다. 생성된 코드를 통해 효율적으로 직렬화, 역직렬화 및 데이터 조작을 수행할 수 있습니다.\n- Rust: Rust는 메모리 안정성, 동시성 및 성능에 중점을 둔 시스템 프로그래밍 언어로 유명합니다. 운영 체제, 장치 드라이버 및 임베디드 시스템을 포함한 저수준 소프트웨어를 개발하는 신뢰할 수 있고 효율적인 방법을 제공합니다.\n- Python@3.10: 이 패키지는 Homebrew를 사용하여 Python 버전 3.10을 설치합니다. Python은 간결함과 다재다능성으로 유명한 인기 있는 프로그래밍 언어입니다. 이 경우의 특정 버전 번호인 3.10은 시간이 지남에 따라 변경될 수 있으며 필요에 따라 다른 버전으로 대체할 수 있습니다.\n- Git: Git은 다수의 개발자가 프로젝트에서 동시에 작업하고 코드의 다른 버전을 관리하며 변경 사항을 원활하게 병합할 수 있도록 돕는 널리 사용되는 버전 관리 시스템입니다.\n- Wget: Wget은 웹에서 파일을 다운로드하는 명령 줄 유틸리티입니다. HTTP, HTTPS 및 FTP를 포함한 다양한 프로토콜을 지원하며 재귀적 다운로드, 중단된 다운로드 재개, 백그라운드 파일 검색 등의 기능을 제공합니다.\n\n## 단계 3 - 안정적인 확산 웹 UI 설치\n\nMac에서 GitHub 저장소 AUTOMATIC1111/stable-diffusion-webui에서 \"Stable Diffusion Web UI\"를 설치하려면 다음 단계를 따라주십시오:\n\n<div class=\"content-ad\"></div>\n\n1. 맥에서 터미널 애플리케이션을 엽니다.\n\n2. 리포지토리를 복제할 디렉토리를 선택하려면 cd 명령을 사용하세요. 예를 들어, 리포지토리를 \"문서\" 폴더에 복제하려는 경우 다음 명령을 실행하십시오:\n\n```js\ncd Documents\n```\n\n3. 이 명령을 실행하면 \"문서\" 디렉토리로 이동합니다. 그러나 리포지토리를 복제할 다른 디렉토리로 경로를 대체할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ngit clone https://github.com/AUTOMATIC1111/stable-diffusion-webui\n```\n\nGit이 복제 작업을 시작하여 저장소를 가져와 현재 디렉토리에 저장할 것입니다.\n\n4. 터미널에서 프로세스를 확인할 수 있습니다. 복제가 완료되면 Mac에 \"Stable Diffusion Web UI\" 저장소의 로컬 복제본이 생성됩니다.\n\n축하합니다! 이제 Mac에 \"stable-diffusion-webui\" 저장소를 복제하였습니다. 내용을 살펴보려면 `cd` 명령을 사용하여 복제된 저장소로 이동하여 필요한 파일 및 폴더에 액세스할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 모델 다운로드\n\n- 링크를 클릭하여 안정적인 확산 모델 버전 1.5를 다운로드하세요.\n- 다운로드한 파일을 클론한 GitHub 저장소의 다음 폴더에 넣어주세요.\n\n```js\nstable-diffusion-webui/models/Stable-diffusion\n```\n\n잘 했어요! 이제 Mac에 Stable Diffusion Web UI를 성공적으로 설치하였으며 이미지 생성을 시작할 준비가 되었습니다.\n\n<div class=\"content-ad\"></div>\n\n# Mac에서 Stable Diffusion Web UI 실행하기\n\n- Mac에서 터미널 애플리케이션을 엽니다.\n- 복제한 저장소가 있는 디렉토리에 액세스하려면 현재 디렉토리를 변경하려면 `cd` 명령을 사용합니다. 예를 들어, \"stable-diffusion-webui\" 폴더로 이동하려면 아래 명령을 실행하세요:\n\n```bash\ncd stable-diffusion-webui\n```\n\n3. 원하는 디렉토리에 들어간 후에는 다음 명령을 실행하여 Stable Diffusion Web UI를 시작합니다:\n\n<div class=\"content-ad\"></div>\n\n```sh\n./webui.sh\n```\n\n이 명령은 안정적인 확산을 웹 브라우저에서 실행할 수 있는 Python 가상 환경을 설정합니다. 이 가상 환경에 직접 액세스할 수 있는 링크가 터미널 앱에서 제공됩니다.\n\n4. 터미널 앱에서 제공된 URL을 복사하여 웹 브라우저에 붙여넣기합니다. Python은 항상 동일한 URL을 생성하므로 이를 즐겨찾기로 저장할 수 있습니다. 그러나 이 가상 환경이 활성화된 경우에만 기능합니다.\n\n![이미지](/assets/img/2024-06-22-HowtoInstallandRunStableDiffusiononyourMacOS_2.png)\n\n\n<div class=\"content-ad\"></div>\n\n# 맥에서 Stable Diffusion Web UI 종료\n\nStable Diffusion Web UI를 사용 중인 가상 환경은 단순히 브라우저 창을 닫는 것으로는 종료되지 않습니다. 이를 올바르게 종료하려면 다음 단계를 따라주십시오:\n\n- 맥에서 여전히 열려 있는 터미널 애플리케이션으로 돌아갑니다.\n- 키보드에서 CONTROL + C를 동시에 누릅니다. 이 동작은 가상 환경 세션을 중단시켜 종료할 것입니다.\n\n# 맥에서 Stable Diffusion Web UI 업데이트하기\n\n<div class=\"content-ad\"></div>\n\nMac에서 Stable Diffusion 웹 UI를 업데이트하려면 GitHub 저장소 AUTOMATIC1111/stable-diffusion-webui에서 최신 변경 사항을 가져오는 다음 단계를 따르세요:\n\n1. Mac에서 Terminal 애플리케이션을 엽니다.\n\n2. 원래 복제된 저장소가 있는 디렉토리로 이동합니다. `cd` 명령을 사용하여 디렉토리를 변경할 수 있습니다. 예를 들어 \"stable-diffusion-webui\" 폴더로 이동하려면 다음 명령을 사용하세요:\n\n```sh\ncd stable-diffusion-webui\n```\n\n<div class=\"content-ad\"></div>\n\n진행하기 전에 올바른 디렉토리에 있는지 확인해주세요.\n\n3. 원하는 디렉토리에 들어간 후 (이 경우 \"stable-diffusion-webui\"), 다음 명령을 실행하여 최신 변경 사항이 반영된 Stable Diffusion Web UI를 GitHub 저장소에서 업데이트하세요:\n\n```js\ngit pull\n```\n\n이 명령은 Stable Diffusion Web UI의 로컬 사본에 가장 최근 업데이트를 가져와 적용합니다.\n\n<div class=\"content-ad\"></div>\n\n이제 맥에서 Stable Diffusion Web UI를 성공적으로 업데이트했습니다. 로컬 사본이 GitHub 리포지토리의 최신 변경 사항과 동기화됩니다.\n\n블로그를 마무리하며, 다음에 어떤 주제를 탐구하고 싶은지 알고 싶습니다. 여러분의 의견은 중요하니 댓글로 의견을 자유롭게 공유해 주세요. 함께해 주셔서 감사합니다. 호기심을 유지하세요!\n\n즐겁게 보내세요!!\n\nLinkedIn에서 연락해요.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-22-HowtoInstallandRunStableDiffusiononyourMacOS_3.png\" />\n\n<img src=\"/assets/img/2024-06-22-HowtoInstallandRunStableDiffusiononyourMacOS_4.png\" />\n\n이 이야기는 Generative AI에서 발행되었습니다. 최신 AI 이야기에 대해 알아가기 위해 LinkedIn에서 저희와 연락을 유지하고 Zeniteq를 팔로우해 주세요.\n\n최신 뉴스 및 Generative AI 업데이트를 받으려면 뉴스레터를 구독해 주세요. 함께 AI의 미래를 함께 만들어 가요!\n\n\n<div class=\"content-ad\"></div>\n\n\n![HowtoInstallandRunStableDiffusiononyourMacOS_5.png](/assets/img/2024-06-22-HowtoInstallandRunStableDiffusiononyourMacOS_5.png)\n","ogImage":{"url":"/assets/img/2024-06-22-HowtoInstallandRunStableDiffusiononyourMacOS_0.png"},"coverImage":"/assets/img/2024-06-22-HowtoInstallandRunStableDiffusiononyourMacOS_0.png","tag":["Tech"],"readingTime":7},{"title":"리눅스에서 LVM 사용하여 논리 볼륨 생성 및 확장하는 방법","description":"","date":"2024-06-22 16:12","slug":"2024-06-22-LVMinLinuxCreateandExtendaLogicalVolume","content":"\n\n## XFS 파일 시스템에 LVM 논리 볼륨을 생성하고 확장하는 방법 안내서\n\n![image](/assets/img/2024-06-22-LVMinLinuxCreateandExtendaLogicalVolume_0.png)\n\n## LVM에 대해 간단히 알아봅시다\n\n논리 볼륨 관리자는 기존의 디스크 관리보다 훨씬 더 효율적인 방식으로 저장소를 다룹니다. 표준 디스크 파티션은 각 디스크의 용량에 기반하여 저장 공간을 할당하지만 LVM은 사용 가능한 모든 물리 하드 드라이브를 하나의 풀의 일부인 것처럼 결합하여 저장 공간을 효율적으로 관리하며 개별적으로 다루는 대신 전체로 사용할 수 있게 만듭니다.\n\n<div class=\"content-ad\"></div>\n\n가정 해 보겠습니다. 4개의 1TB 드라이브가 있습니다. 전통적인 디스크 체계에서는 개별적으로 처리하지만 LVM을 사용하면 이 4개의 1TB 드라이브가 4TB 단일 청크 또는 집계된 저장 용량으로 간주됩니다. 이를 통해 디스크 레이아웃에 대한 더 큰 유연성과 제어권을 얻을 수 있으며 디스크를 더 쉽게 조작할 수 있습니다. LVM을 사용하는 주요 이점 중 하나는 파일 시스템을 쉽게 확장할 수 있는 능력입니다.\n\nLVM을 이해하고 사용하기 위해서 우리는 세 가지 주요 구성 요소를 이해해야 합니다. 이들은 서로 연결되어 있으며 함께 하나의 논리적 볼륨이라는 것을 만듭니다. 이러한 구성 요소는 다음과 같습니다:\n\n- 물리적 볼륨\n- 볼륨 그룹\n- 논리적 볼륨\n\n물리적 볼륨: 이들은 LVM을 만들기 위해 사용되는 기본 블록입니다. 물리적 볼륨 또는 \"PVs\"는 단순히 물리적 저장 장치인 SSD 또는 HDD 드라이브입니다. 하드 드라이브가 물리적 볼륨로 간주되려면 물리적 볼륨로 초기화되어야 하므로 LVM에서 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nVolume Group: 우리는 볼륨 그룹 \"VG\"를 물리적 볼륨으로 구성된 풀로 생각할 수 있습니다. 예를 들어, 1TB SSD 하드 드라이브 세 개가 볼륨 그룹의 일부인 경우, 이 \"VG\"는 총 저장 용량이 3TB로 나타나며, 논리적 볼륨을 생성하는 데 사용됩니다.\n\n논리적 볼륨: 우리의 VG가 생성되면, 마침내 논리적 볼륨을 생성할 수 있습니다. 단일 볼륨 그룹에서 하나 이상의 논리적 볼륨을 생성할 수 있습니다. 논리적 볼륨은 디렉터리에 마운트된 기존 파티션으로 처리되고 사용될 것입니다.\n\n아래 그림은 논리적 볼륨의 구조를 설명합니다:\n\n![논리적 볼륨 구조](/assets/img/2024-06-22-LVMinLinuxCreateandExtendaLogicalVolume_1.png)\n\n<div class=\"content-ad\"></div>\n\n(특정 예시에서 우리가 사용하는 데모 서버가 구성된대로, 여러 논리 볼륨 및 볼륨 그룹이 있습니다.)\n\n물리 장치인 하드 드라이브로부터 시작해요, 이들은 물리 볼륨 또는 PVs를 만드는 데 사용됩니다. 이 예에서, 우리는 세 개의 별도 HDD가 있으며, 각각은 하나의 물리 볼륨을 만드는 데 사용됩니다. 파티션 /dev/sda2는 첫 번째 PV를 만들고, /dev/sdb1은 두 번째이며 sdc1은 세 번째입니다.\n\n이어서 두 개의 별도 볼륨 그룹이 있고, 각각에는 개별적인 논리 볼륨이 있습니다. 언급한 대로, LVs는 볼륨 그룹에서 잘려 나온 것입니다. 한 VG에서 하나 이상의 논리 볼륨이 올 수 있습니다.\n\n이 추상화에서 최종 계층은 논리 볼륨입니다. 예를 들어, vg-data 볼륨 그룹에서 나온 lv-data가 있습니다. lv-data가 준비되면 포맷하여 마운트 지점으로 사용할 수 있습니다. 이 경우: /dev/vg-data/lv-data.\n\n<div class=\"content-ad\"></div>\n\n## LVM 논리 볼륨 만드는 방법:\n\n기존 논리 볼륨을 확장/확장하는 것에 대한 내용을 살펴보기 전에, 이전에 설정되지 않은 시스템에서 처음부터 하나를 만드는 방법을 살펴보고, 그 다음 섹션에서 확장하는 방법을 살펴보겠습니다. 이미 존재하는 LV를 확장하고 싶다면, 이 섹션을 건너뛰시면 됩니다.\n\n새로운 논리 볼륨을 설정하려면 다음 순서대로 진행해야 합니다:\n\n- 기존 하드 드라이브에서 물리 볼륨을 생성합니다.\n- 볼륨 그룹을 생성하고 물리 볼륨을 추가합니다.\n- 볼륨 그룹에서 논리 볼륨을 생성합니다.\n- 필요에 따라 논리 볼륨을 포맷합니다 — xfs, ext4 등.\n- 마지막으로 새 파일 시스템을 마운트합니다.\n\n<div class=\"content-ad\"></div>\n\n— 물리 볼륨을 생성하려면 디스크 공간을 사용할 수 있어야 합니다. 이 서버(가상 머신)에서는 논리 볼륨 생성 데모에 사용할 두 개의 별도의 원시 하드 드라이브가 있습니다.\n\n![이미지](/assets/img/2024-06-22-LVMinLinuxCreateandExtendaLogicalVolume_2.png)\n\n/dev/sdb 및 /dev/sdc 두 개의 원시 디스크가 있는 것을 확인할 수 있습니다. 아직 사용할 수 없으므로 이를 포맷해야 합니다.\n\n이 새 디스크 /dev/sdb를 사용할 수 있게 하는 방법을 빠르게 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*XLof11Om75NHy4PY2-2S2Q.gif)\n\n저희가 한 몇가지에 대해 몇 가지 소식을 전합니다:\n\n- 새로운 드라이브 /dev/sdb를 사용할 수 있도록 새로운 파티션을 생성하고 적절한 레이블을 할당함으로써 진행되고 있습니다.\n- 이 파티션의 특정 크기를 지정하지 않았지만, 기본값을 선택하여 전체 디스크를 사용할 수 있습니다. 크기를 지정하지 않고 엔터를 누르면 사용 가능한 전체 공간이 할당됩니다.\n- 여기서 중요한 부분은 이 새로운 파티션을 레이블링하여 LVM 유형으로 지정하는 “8e” 16진수 코드를 선택하는 것입니다. ext4 파티션을 만들고자 한다면 레이블 코드가 다를 것입니다. 새로운 파티션을 만들 때, 사용하기 전에 레이블링해야 합니다.\n- 우리가 한 변경 사항을 저장하기 위해 “w”를 입력해야 합니다.\n\n이제 디스크가 준비되었으니, pvcreate로 물리적 볼륨을 생성하는 것부터 시작해보겠습니다:\n\n\n<div class=\"content-ad\"></div>\n\n피지컬 볼륨이 성공적으로 생성되었습니다! 이제 pvdisplay를 사용하여 PV가 만들어졌는지 확인해봅시다 (pvs도 사용할 수 있습니다):\n\n\n[itadmin@localhost ~]$ sudo pvdisplay\n[sudo] password for itadmin:\n  --- Physical volume ---\n  PV Name               /dev/sda2\n  VG Name               centos\n  PV Size               <19.00 GiB / not usable 3.00 MiB\n  Allocatable           yes (but full)\n  PE Size               4.00 MiB\n  Total PE              4863\n  Free PE               0\n  Allocated PE          4863\n  PV UUID               sa3xFR-jbDb-SKuS-yQN1-VzzW-bveG-aA0yha\n\"/dev/sdb1\"은 \"<30.00 GiB\"의 새로운 피지컬 볼륨입니다\n  --- NEW Physical volume ---\n  PV Name               /dev/sdb1\n  VG Name\n  PV Size               <30.00 GiB\n  Allocatable           NO\n  PE Size               0\n  Total PE              0\n  Free PE               0\n  Allocated PE          0\n  PV UUID               Q54fqC-Np3X-XUaF-j5vE-Px29-pT9H-JYsDUh\n[itadmin@localhost ~]$\n\n\n이미 /dev/sda2에 PV가 있습니다. 이는 루트 파티션도 논리 볼륨 위에 구축되어 있음을 의미합니다. 새로운 디스크 /dev/sdb1은 이제 피지컬 볼륨입니다. 이 새로운 피지컬 볼륨의 VG 이름이 비어있는 것을 알 수 있습니다! 이는 이제까지 볼륨 그룹에 추가되지 않았기 때문입니다! 이를 바로 해결해봅시다:\n\n<div class=\"content-ad\"></div>\n\n```bash\n[itadmin@localhost ~]$ sudo vgcreate vg-data /dev/sdb1\n  Volume group \"vg-data\" successfully created\n[itadmin@localhost ~]$\n```\n\n- 새 볼륨 그룹을 만들 때 vgcreate를 사용하며, 새 볼륨 그룹에 이름을 할당하고 물리 볼륨 또는 볼륨을 추가합니다. 이 경우에는 /dev/sdb1을 이 VG의 일부로 만듭니다.\n\n새로 만든 볼륨 그룹을 확인해봅시다:\n\n```bash\n[itadmin@localhost ~]$ sudo vgs\n  VG      #PV #LV #SN Attr   VSize   VFree\n  centos    1   2   0 wz--n- <19.00g      0\n  vg-data   1   0   0 wz--n- <30.00g <30.00g\n[itadmin@localhost ~]$\n```\n\n<div class=\"content-ad\"></div>\n\n- vg-data은 실제로 존재하며 30GB의 여유 공간이 있어요 — 이는 이전에 추가한 물리 볼륨(새로운 파티션)의 크기입니다.\n\n이제 lvcreate을 사용하여 논리 볼륨을 생성합니다:\n\n```bash\n[itadmin@localhost ~]$ sudo lvcreate --name lv-data -l 100%FREE vg-data\n논리 볼륨 \"lv-data\"이(가) 생성되었습니다.\n[itadmin@localhost ~]$\n```\n\n- -l : 우리가 볼륨 그룹에서 얼마의 공간을 사용할지를 지정하는 데 사용됩니다. 여기서는 해당 그룹의 100%를 할당합니다. 명령어에 볼륨 그룹 이름을 명시해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n새로운 Logical Volume을 lvdisplay로 확인하고 있어요:\n\n![LV](/assets/img/2024-06-22-LVMinLinuxCreateandExtendaLogicalVolume_3.png)\n\n우리의 Logical Volume이 성공적으로 생성되었어요!\n\n이제 사용할 수 있도록 이 논리적 볼륨을 포맷해야 해요. 새로운 LV를 포맷하는 방법은 mkfs.xfs 명령을 사용하는 거에요:\n\n<div class=\"content-ad\"></div>\n\n```bash\n[itadmin@localhost ~]$ sudo mkfs.xfs /dev/vg-data/lv-data\n[sudo] itadmin 님의 암호:\nmeta-data=/dev/vg-data/lv-data   isize=512    agcount=4, agsize=1965824 블록\n         =                       sectsz=512   attr=2, projid32bit=1\n         =                       crc=1        finobt=0, sparse=0\ndata     =                       bsize=4096   블록=7863296, imaxpct=25\n         =                       sunit=0      swidth=0 블록\nnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1\nlog      =internal log           bsize=4096   블록=3839, version=2\n         =                       sectsz=512   sunit=0 blks, lazy-count=1\nrealtime =none                   extsz=4096   블록=0, rtextents=0\n[itadmin@localhost ~]$\n```\n\n.mkfs는 \"파일 시스템 생성\"을 의미하며, 이를 수행합니다. 이 경우에는 xfs 파일 시스템을 생성하려고 하므로 mkfs.xfs를 사용합니다. xfs 파일 시스템은 여러 측면에서 ext4보다 업그레이드된 것이며 RHEL 서버에서 기본 파일 시스템입니다. 그럼에도 불구하고 상황에 따라 ext4를 사용하는 것이 xfs보다 우위를 가질 수 있습니다.\n\n새로 생성된 논리 볼륨을 마운트하려면 해당 마운트 포인트를 만들고 다음 단계를 수행합니다:\n\n```bash\n[itadmin@localhost ~]$ sudo mkdir /data\n[itadmin@localhost ~]$ sudo mount /dev/vg-data/lv-data /data\n```\n\n<div class=\"content-ad\"></div>\n\n저희는 새 파일 시스템이 마운트될 새로운 디렉토리를 생성했어요. /data 하위에 있는 모든 것들은 이 새 논리 볼륨에 속합니다.\n\n— 부가적인 사항으로 — 새로운 마운트 포인트를 /etc/fstab 파일에 추가해야 부팅 중에도 지속되도록 해야 해요.\n\n새롭게 마운트된 /data 디렉토리를 df 명령어로 살펴봅시다:\n\n![이미지](/assets/img/2024-06-22-LVMinLinuxCreateandExtendaLogicalVolume_4.png)\n\n<div class=\"content-ad\"></div>\n\n- 파일 시스템이 실제로 XFS인지 확인해야 하기 때문에 df 명령에 \"T\" 스위치가 필요합니다. 이 스위치는 파일 시스템 유형을 보여주는 데 사용됩니다.\n- 우리의 논리 볼륨 lv-data는 모두 준비되어 있고 /data에 마운트되어 있습니다!\n\n작업이 완료되었습니다!\n\n## 논리 볼륨 확장 방법\n\n우리는 처음부터 논리 볼륨을 만드는 방법을 보았지만, 대부분의 경우 이미 존재하는 논리 볼륨의 크기를 늘려야 하므로 더 많은 데이터를 수용할 수 있게 할 필요가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n테이블 태그를 Markdown 형식으로 변경해주세요.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-22-LVMinLinuxCreateandExtendaLogicalVolume_5.png)\n\n- 우리는 Volume Group vg-data에 사용 가능한 빈 공간이 없음을 확인할 수 있습니다. 새 물리 볼륨을 추가해 보겠습니다.\n\n먼저, 이 새 드라이브를 사용할 수 있도록 새 파티션을 만들고 \"LVM\" 레이블을 할당해야 합니다. 이를 위해 아래와 같이 fdisk를 사용하여 수행합니다:\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*qB2R3x5Zc2Lt46xIq-bEFQ.gif)\n\n\n<div class=\"content-ad\"></div>\n\n- fdisk를 사용하여 새 파티션을 만들고, 크기를 지정하지 않은 전체 크기를 할당했어요 (값을 지정하지 않고 Enter를 누르면 사용 가능한 크기를 할당합니다).\n- 정확한 라벨을 지정해야 해요, \"LVM 타입\"으로 만들기 위해 필요한 라벨은 8e에요.\n\n디스크가 준비되었고, 이제 물리 볼륨을 생성할 수 있어요:\n\n```js\n[root@localhost ~]$ pvcreate /dev/sdc1\n  Physical volume \"/dev/sdc1\" successfully created.\n[root@localhost ~]$\n```\n\n생성 완료! 이제 vg-data 볼륨 그룹에 추가하려면, vgextend 명령어를 사용해요:\n\n<div class=\"content-ad\"></div>\n\n```bash\n[root@localhost ~]$ vgextend vg-data /dev/sdc1\n  Volume group \"vg-data\" successfully extended\n[root@localhost ~]$\n```\n\nVG가 확장되었습니다! 한 번 더 확인해 봐요:\n\n![이미지](/assets/img/2024-06-22-LVMinLinuxCreateandExtendaLogicalVolume_6.png)\n\n훌륭해요! 이제 vg-data에 25GB의 여유 공간이 있는 것을 확인할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n지금은 Logical Volume에 뛰어들어 보겠습니다. 지금 상태는 다음과 같습니다:\n\n![Logical Volume Status](/assets/img/2024-06-22-LVMinLinuxCreateandExtendaLogicalVolume_7.png)\n\n우리는 lvextend 명령어로 lv-data Logical Volume을 확장했습니다:\n\n```js\n[root@localhost ~]$ lvextend -l +100%FREE /dev/vg-data/lv-data\n  Size of logical volume vg-data/lv-data changed from <30.00 GiB (7679 extents) to 54.99 GiB (14078 extents).\n  Logical volume vg-data/lv-data successfully resized.\n[root@localhost ~]$\n```\n\n<div class=\"content-ad\"></div>\n\n- lvextend 명령은 lv-data 논리 볼륨을 확장하며, +100%FREE 옵션은 해당 볼륨을 볼륨 그룹에서 남아있는 모든 가능한 크기로 확장합니다.\n- 만약 우리가 논리 볼륨을 특정 크기로 확장하고 싶었다면, 예를 들어 5GB로 확장하고 싶다면 다음과 같이 진행할 것입니다:\n\n```js\n[root@localhost ~]$ lvextend -L +5G /dev/vg-data/lv-data\n```\n\n이제 우리의 논리 볼륨 상태를 확인해봅시다:\n\n![논리 볼륨 상태](/assets/img/2024-06-22-LVMinLinuxCreateandExtendaLogicalVolume_8.png)\n\n<div class=\"content-ad\"></div>\n\n출력을 통해 LSize가 실제로 증가했고, 논리 볼륨이 확장되었습니다!\n\n아직 끝나지 않았어요! 마지막 단계는 새로 추가된 저장 용량을 사용할 수 있도록 파일 시스템의 크기를 조정하는 것입니다. 이 작업은 xfs_growfs 명령으로 수행할 수 있어요:\n\n```js\n[root@localhost ~]$ xfs_growfs /dev/vg-data/lv-data\n```\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*yu72eMpz12FUdIOTeL0qgg.gif\" />\n\n<div class=\"content-ad\"></div>\n\n- 데이터 블록이 변경되고 파일 시스템이 확장되었음을 알 수 있습니다.\n- 여기서 언급해야 할 또 다른 훌륭한 점은 파일 시스템을 확장할 때 마운트 포인트인 /data를 해제할 필요가 없었다는 것입니다!\n\ndf -kh 명령의 출력에서 /data 마운트 포인트가 확장되었음을 확인할 수 있습니다. 여기에는 lv-data 논리 볼륨이 마운트된 곳입니다:\n\n![Image](/assets/img/2024-06-22-LVMinLinuxCreateandExtendaLogicalVolume_9.png)\n\n모두 완료되었습니다. 논리 볼륨이 성공적으로 확장되었습니다!\n\n<div class=\"content-ad\"></div>\n\n이 글은 Linux에서 논리 볼륨 매니저에 대해 간략히 다룬 내용이었습니다. 논리 볼륨을 생성하고 확장하는 방법에 대해 설명했습니다. 궁금한 점이나 의견, 추가할 내용이 있으면 자유롭게 남겨주세요.\n\n읽어주셔서 감사합니다! 다음 포스트에서 만나요 :)","ogImage":{"url":"/assets/img/2024-06-22-LVMinLinuxCreateandExtendaLogicalVolume_0.png"},"coverImage":"/assets/img/2024-06-22-LVMinLinuxCreateandExtendaLogicalVolume_0.png","tag":["Tech"],"readingTime":10},{"title":"자정에 크론 작업을 절대 예약하면 안 되는 이유","description":"","date":"2024-06-22 16:11","slug":"2024-06-22-WhyYouShouldNeverScheduleCronJobsExactlyatMidnight","content":"\n\n제 개발자로서, 제가 직접 체험한 바에 의하면 새벽 정각에(0 0 * * *) cron 작업을 예약하는 것이 일으킬 수 있는 혼란에 대해 알고 있습니다. 왜 그렇게 하는 것이 좋지 않은지, 그리고 보다 효과적으로 cron 작업을 예약하는 방법에 대한 팁을 소개하겠습니다.\n\n![Why You Should Never Schedule Cron Jobs Exactly at Midnight](/assets/img/2024-06-22-WhyYouShouldNeverScheduleCronJobsExactlyatMidnight_0.png)\n\n새벽의 신비\n\n이전 직무 중 하나에서, 우리 팀은 심각한 시스템 업데이트를 밤 12시에 예약했을 때 수수께끼 같은 문제에 직면했습니다. 작업이 무작위로 실패하고 시스템이 크게 느려져 서비스 중단을 야기했습니다. 수없이 많은 디버깅 시간 끝에, 우리는 여러 작업이 밤 12시에 실행되도록 설정되어 있음을 발견했습니다. 동시에 발생한 부하로 리소스 충돌과 예측할 수 없는 동작이 발생하여 근본 원인을 식별하는 데 매우 어려워졌습니다.\n\n<div class=\"content-ad\"></div>\n\n## 배운 교훈\n\n## 크론 작업 예약을 위한 최상의 실천 방법\n\n1. 일정 시간을 무작위로 설정: 작업을 동시에 설정하는 대신 무작위 시간을 사용하여 부하를 분산시킵니다. 이렇게 하면 사용량이 많은 시기를 피할 수 있고 충돌 가능성을 줄일 수 있습니다.\n\n2. 지수 백오프 및 지터 사용: 지수 백오프와 지터를 사용하여 재시도를 구현하면 크론 작업이 능률적으로 수행될 수 있습니다. Marc Brooker의 AWS에서 제공하는 멋진 '지수 백오프와 지터' 아티클을 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-22-WhyYouShouldNeverScheduleCronJobsExactlyatMidnight_1.png)\n\n3. 라이브러리 활용: 파이썬을 사용한다면, Tenacity 라이브러리를 사용하여 백오프와 지터 지연을 가진 재시도를 고려해보세요. 이는 프로세스를 간단화하고 크론 작업을 더욱 강화할 수 있습니다.\n\n# 주기적인 작업을 더 견고하게 만들기\n\n크론 작업은 작업을 자동화하는 데 좋지만 부적절한 예약은 큰 문제를 일으킬 수 있습니다. 자정 예약을 피하고 이러한 팁을 따른다면 더욱 신뢰성이 높고 효율적인 작업 실행을 보장할 수 있습니다. 기억하세요, 약간의 무작위성은 \"0 0 * * *\"의 함정을 피하는 데 큰 도움이 됩니다.\n","ogImage":{"url":"/assets/img/2024-06-22-WhyYouShouldNeverScheduleCronJobsExactlyatMidnight_0.png"},"coverImage":"/assets/img/2024-06-22-WhyYouShouldNeverScheduleCronJobsExactlyatMidnight_0.png","tag":["Tech"],"readingTime":2},{"title":"안전한 원격 접속 간소화 리눅스 서버 간 패스워드 없는 SSH 연결 가이드","description":"","date":"2024-06-22 16:10","slug":"2024-06-22-StreamliningSecureRemoteAccessAGuidetoPasswordlessSSHConnectionsBetweenLinuxServers","content":"\n\n---markdown\n![SSH Connection](/assets/img/2024-06-22-StreamliningSecureRemoteAccessAGuidetoPasswordlessSSHConnectionsBetweenLinuxServers_0.png)\n\n두 대의 Linux 서버 간에 비밀번호 없는 SSH 연결을 설정하는 것은 안전한 원격 액세스를 간편하게하는 일반적인 방법입니다. 본 자습서는 비밀번호 없는 SSH 인증을 설정하는 단계를 안내하며, 연결이 예상대로 작동하지 않을 경우 해결 방법을 제공합니다.\n\n# 전제 조건\n\n- 두 대의 Linux 서버 (서버 A 및 서버 B)\n- 두 서버 모두에 대한 관리 액세스\n---\n\n<div class=\"content-ad\"></div>\n\n# 단계 1: 서버 A에서 SSH 키 쌍 생성하기\n\n- SSH 또는 물리적으로 서버 A에 로그인합니다.\n- 터미널 창을 엽니다.\n- 다음 명령을 실행하여 SSH 키 쌍을 생성합니다:\n\n```js\nssh-keygen -t rsa\n```\n\n키 쌍의 기본 위치를 사용하려면 Enter를 누르시고, 인증을 위해 암호를 비워둘 경우 비밀번호 없는 인증을 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n4. Enter 키를 눌러 키 생성을 확인해주세요.\n\n5. 이 작업은 ~/.ssh/ 디렉토리에 개인 키 (id_rsa) 및 공개 키 (id_rsa.pub)를 생성합니다.\n\n## 단계 2: 공개 키를 서버 B로 복사\n\n- ssh-copy-id 명령어를 사용하여 공개 키를 서버 B로 복사하세요:\n\n<div class=\"content-ad\"></div>\n\n```js\nssh-copy-id user@serverB_IP\n```\n\n- 사용자를 서버 B의 사용자 이름으로, serverB_IP를 서버 B의 IP 주소 또는 호스트 이름으로 바꿔주세요.\n- 서버 B의 사용자 계정 암호를 입력하라는 메시지가 표시됩니다.\n- 공개 키를 성공적으로 복사한 후, 키가 추가되었음을 확인하는 메시지가 표시됩니다.\n\n# 단계 3: 무비밀번호 연결 테스트\n\n서버 A에서 서버 B로 SSH 연결을 시도해보세요:\n\n<div class=\"content-ad\"></div>\n\n```js\nssh user@serverB_IP\n```\n\n이제 암호를 묻지 않고 서버 B에 액세스할 수 있어야 합니다.\n\n# 문제 해결 팁\n\n암호없는 연결이 작동하지 않는 경우, 다음 문제 해결 단계를 따라주세요:\n\n<div class=\"content-ad\"></div>\n\n- 권한 확인: Server A 및 Server B의 .ssh 디렉토리가 올바른 권한을 가지고 있는지 확인해주세요. 사용자의 소유이어야 하며 제한된 권한을 가져야 합니다:\n\n```js\nchmod 700 ~/.ssh \nchmod 600 ~/.ssh/authorized_keys\n```\n\n- 키 파일 이름: 기본 키 이름(id_rsa 및 id_rsa.pub) 또는 키 생성 중에 지정한 이름을 사용하는지 확인하세요.\n- SSH 에이전트: Server A에서 프라이빗 키를 SSH 에이전트에 추가했는지 확인해주세요. 다음 명령을 사용하여 추가할 수 있습니다:\n\n```js\nssh-add ~/.ssh/id_rsa\n```\n\n<div class=\"content-ad\"></div>\n\n3. 방화벽 및 SELinux: 각 서버의 방화벽이 SSH 액세스를 차단하는지 확인하고 SELinux 권한이 문제를 일으키지 않도록 합니다.\n\n4. 로그: Server B의 SSH 서버 로그에서 오류 메시지를 확인하세요:\n\n```js\ntail -f /var/log/auth.log  # Ubuntu/Debian에서\ntail -f /var/log/secure    # CentOS/RHEL에서\n```\n\n5. 디버깅 모드: 더 많은 정보를 얻기 위해 SSH를 디버깅 모드로 실행할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nssh -v user@serverB_IP\n```\n\n이 명령어는 자세한 디버그 출력을 제공하여 문제의 원인을 파악하는 데 도움이 될 것입니다.\n\n이러한 단계를 따르고 문제 해결 팁을 고려한다면, Linux 서버간의 안전하고 편리한 원격 액세스를 위한 비밀번호 없는 SSH 연결을 설정할 수 있을 것입니다.","ogImage":{"url":"/assets/img/2024-06-22-StreamliningSecureRemoteAccessAGuidetoPasswordlessSSHConnectionsBetweenLinuxServers_0.png"},"coverImage":"/assets/img/2024-06-22-StreamliningSecureRemoteAccessAGuidetoPasswordlessSSHConnectionsBetweenLinuxServers_0.png","tag":["Tech"],"readingTime":3},{"title":"방어 무력화 T1562012 Linux 감사 로그 변조 탐지 방법 2부","description":"","date":"2024-06-22 16:09","slug":"2024-06-22-ImpairDefensesT1562012DetectLinuxAuditLogsTamperingPart2","content":"\n\n![이미지](/assets/img/2024-06-22-ImpairDefensesT1562012DetectLinuxAuditLogsTamperingPart2_0.png)\n\n이 시리즈의 첫 번째 파트에서는 리눅스 감사 데몬(auditd)이 시스템 이벤트의 상세 로그를 기록하여 시스템 보안을 유지하는 중요한 역할을 강조했습니다. 이 핵심 기능은 악의적 활동을 숨기려는 공격자들에게 auditd를 주요 대상으로 삼게합니다.\n\n이전 토론은 auditd의 지속적인 작동을 보증하는 데 중점을 둔 반면, 본 기사에서는 auditd 규칙의 삭제와 설정 변경을 감지하는 데 깊이 파고들었습니다. 이러한 설정의 무결성을 보장하는 것은 효과적인 보안 모니터링에 중요합니다.\n\n본 기사에서는 auditd 규칙 및 설정에 대한 무단 변경을 감지하는 중요성을 다룰 것입니다. 이러한 변경사항을 식별하고 감사 로그의 신뢰성을 확보할 수 있는 방법과 도구, 예를 들어 auditd 규칙 및 Splunk 쿼리 등을 소개하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n끝나면 공격자가 시스템 보안을 강화하는 데 필요한 지식과 도구로 장착될 것입니다. 이를 위해 auditd 규칙과 설정을 감지하여 조작하는 것을 탐지할 수 있습니다.\n\n## 우리가 해결하려는 문제\n\n공격자는 종종 auditd 규칙을 삭제하거나 구성을 수정하여 로깅을 비활성화하고 활동을 숨깁니다. 이러한 무단 변경 사항을 감지하는 것은 공격자가 사용하는 다양한 방법 때문에 어려울 수 있습니다.\n\n우리의 목표는 auditd 규칙과 설정에 대한 무단 삭제 또는 수정을 감지하는 것입니다. 효과적인 감지 메커니즘을 구현함으로써 감사 로그의 신뢰성을 유지하고 시스템 보안을 강화할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 해결책: 오디트드 규칙 및 설정 변경 감지하기\n\n저희의 해결책은 공격자가 오디트드를 대상으로 하는 도전에 대응하기 위해 두 가지 주요 구성 요소를 활용합니다:\n\n- 오디트드 규칙 삭제 감지: 오디트드에 의해 기록된 감사 레코드 유형(예: CONFIG_CHANGE)을 활용하여 오디트드 규칙의 삭제를 감지하는 모니터링을 구현할 것입니다. 이러한 레코드 유형은 규칙 삭제의 내장 로깅을 제공하여 시스템 보안을 저해할 수 있는 미승인 변경을 사전에 식별하고 대응할 수 있도록 합니다.\n- 오디트드 구성 및 중요 파일 변경 모니터링: 중요한 오디트드 구성 변경을 모니터링하기 위해 새로운 오디트드 규칙을 구현할 것입니다. 이러한 규칙은 필수적인 구성 파일의 수정 또는 삭제를 캡처하여 언제든지 미승인 변경이 즉각적으로 조사 대상으로 표시되도록 보장합니다.\n\n## 로그 활동 이해하기\n\n<div class=\"content-ad\"></div>\n\nauditd는 감사 시스템 구성 변경 시 CONFIG_CHANGE 이벤트를 기록합니다. 이는 감사 규칙을 포함한 구성 변경이 있을 때 발생합니다. 이 레코드 유형은 수정의 타임스탬프와 성격과 같은 세부 정보를 캡처하여 감사 규칙의 무단 삭제를 모니터링하고 감지하는 데 이상적입니다.\n\n![Image](/assets/img/2024-06-22-ImpairDefensesT1562012DetectLinuxAuditLogsTamperingPart2_1.png)\n\n## 로그 검토\n\n감사 규칙 삭제를 감지하기 위한 상관 검색을 확인하기 위해 먼저 Splunk에서 필요한 로그를 생성하기 위해 규칙 삭제 시뮬레이션을 수행했습니다.\n\n<div class=\"content-ad\"></div>\n\n여기서 패턴을 찾기 매우 쉽습니다. type=CONFIG_CHANGE 및 op=remove_rule을 찾아야 합니다.\n\n![이미지](/assets/img/2024-06-22-ImpairDefensesT1562012DetectLinuxAuditLogsTamperingPart2_2.png)\n\n# Splunk에서 오디트된 규칙 삭제 감지 개발\n\n## 설명:\n\n<div class=\"content-ad\"></div>\n\n**검색 기준 (index, sourcetype, type, op, res):**\n\n- linux_audit 인덱스에서 type이 CONFIG_CHANGE이고 op이 remove_rule이며 res가 1인 linux:audit 이벤트를 검색합니다. (성공을 나타냄)\n\n**필드 평가 (eval key):**\n\n- key가 \"(null)\"인 경우 key를 \"unknown\"으로 설정하여 필드 일관성을 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n3. 집계 (통계 값(key) AS deleted_rules):\n\n   - _time, host, type, auid, ses 및 op에 따라 이벤트를 집계하고, 삭제된 auditd 규칙의 모든 key 값(이름)을 deleted_rules로 모음.\n\n4. 시간 변환 (convert ctime(_time)):\n\n   - _time의 Unix 타임스탬프를 사람이 읽을 수 있는 날짜 및 시간 형식으로 변환합니다.\n\n<div class=\"content-ad\"></div>\n\n5. 삭제된 규칙 수 세기 (eval count=mvcount(deleted_rules)):\n\n- 삭제된 규칙 발생 횟수를 세줍니다.\n\n6. 이벤트 설명 (eval event_info):\n\n- 삭제된 규칙의 개수를 기반으로 event_info를 구성합니다.\n- 삭제된 auditd 규칙의 수와 타임스탬프를 지정합니다.\n- 규칙 삭제의 예기치 못한 성격으로 인해 조사를 권장합니다.\n\n<div class=\"content-ad\"></div>\n\n7. 최종 결과 (표 _time, 호스트, 유형, auid, ses, 작업, deleted_rules, 이벤트_정보):\n\n- 타임스탬프(_time), 호스트, 이벤트 유형 (유형), 감사 사용자 ID (auid), 세션 ID (ses), 작업 (op), 삭제된 규칙 (deleted_rules) 및 이벤트 세부정보 (이벤트_정보)를 포함한 구조화된 테이블 형식으로 결과를 제시합니다.\n\n# auditd 구성 수정/삭제를 모니터링하기 위한 탐지 개발\n\n## 단계 1: 구성 수정을 모니터링하기 위한 auditd 규칙 생성\n\n<div class=\"content-ad\"></div>\n\n설정 중인 auditd 규칙은 Linux 감사 인프라에 필수적인 핵심 구성 파일을 모니터링하기 위해 전략적으로 설계되었습니다. 이러한 파일에는 /etc/audit/auditd.conf, /etc/audit/rules.d/test.rules, /etc/audisp/audispd.conf 및 /etc/libaudit.conf이 포함됩니다. 각각이 중요한 이유는 다음과 같습니다:\n\n- /etc/audit/auditd.conf: 감사 데몬 (auditd)의 전역 설정을 제어하며 로그 위치, 보존 정책 및 시스템 전체 감사 구성을 포함합니다.\n- /etc/audit/rules.d/test.rules: 특정 이벤트 및 조건을 정의하는 감사 규칙을 포함합니다.\n- /etc/audisp/audispd.conf: 감사 이벤트 디스패처 (audispd)를 구성하며 감사 이벤트를 처리하고 전달하는 역할을 담당합니다.\n- /etc/libaudit.conf: 감사 프레임워크 (libaudit)의 라이브러리 수준 설정을 관리하며 그 동작과 기능성에 영향을 줍니다.\n\n<img src=\"/assets/img/2024-06-22-ImpairDefensesT1562012DetectLinuxAuditLogsTamperingPart2_3.png\" />\n\n이 auditd 규칙의 논리는 다른 설정 파일 간에 일관성을 유지합니다:\n\n<div class=\"content-ad\"></div>\n\n- 각 규칙은 관련 작업이 발생할 때마다 감사 이벤트가 생성되도록 항상 -a를 사용합니다.\n- 모두 -F path=를 지정하여 감시되는 특정 구성 파일의 경로를 정의합니다.\n- -F perm=wa는 쓰기(w) 및 속성 변경(a) 권한에 대해 이벤트가 트리거되도록 지정합니다.\n- -F success=1은 작업이 성공했을 때에만 이벤트가 기록되도록 합니다.\n- -k 매개변수는 auditd 규칙의 이름을 지정합니다.\n- 차이점은 지정된 경로(-F path=)에만 있으며, 다른 구성 파일(/etc/audit/auditd.conf, /etc/audit/rules.d/test.rules, /etc/audisp/audispd.conf, /etc/libaudit.conf)을 가리킵니다.\n\n구성 파일을 모니터링하기 위해 auditd 규칙을 구성하면 감지된 모든 수정 사항에 대한 로그가 생성됩니다. 이러한 로그는 Splunk에서 침입 탐지 규칙을 생성하기 위한 입력으로 사용됩니다.\n\n이러한 감사 로그를 상호 연관시키고 분석함으로써 Splunk는 무단 변경에 대한 예방적인 모니터링 및 경고를 제공합니다.\n\n## 단계 2: Splunk에서 상호 연관 검색 개발\n\n<div class=\"content-ad\"></div>\n\n새 감사 규칙을 테스트하여 auditd.conf에 다양한 수정 사항을 시뮬레이션하여 올바르게 작동하는지 확인했습니다.\n\n첫 번째 테스트에서는 rm 명령어를 사용하여 auditd.conf를 삭제하는 것을 포함했습니다. 두 번째 테스트에서는 설정 파일을 다른 디렉토리로 이동하고, 세 번째 테스트에서는 vim 텍스트 편집기를 사용하여 수정했습니다.\n\n## 설명:\n\n- 검색 기준 (인덱스, 소스 유형, 유형, 키):\n\n<div class=\"content-ad\"></div>\n\n- linux_audit 인덱스에서 linux:audit 소스 유형 및 type이 SYSCALL인 이벤트를 검색합니다.\n- 키 필드가 생성한 특정 감사 규칙 (auditrule_modification, auditd_conf_modification, audispd_conf_modification, libauditd_conf_modification) 중 하나와 일치하는 이벤트를 필터링합니다.\n\n2. 집계 (transaction host maxpause=1s):\n\n- 1초 시간 간격 내(최대 일시 중지 = 1초) 동일 호스트에서 연속된 이벤트 (SYSCALL 항목)를 그룹화합니다.\n- 이 집계는 위협 행위자의 특정 작업에 대한 추가 컨텍스트를 제공할 수 있는 시스템 호출 순서를 분석하는 데 도움이 됩니다.\n\n3. 통계 요약 (stats count by _time, host, key, comm, exe, uid, gid, _raw):\n\n<div class=\"content-ad\"></div>\n\n- 다양한 필드별로 그룹화된 카운트를 계산합니다: _time (타임스탬프), host (컴퓨터 이름), key (감사 키), comm (명령어 이름), exe (실행 파일 경로), uid (사용자 ID), gid (그룹 ID) 및 _raw (원시 로그 항목). 이 요약은 트랜잭션 내에서 이러한 속성의 각 고유한 조합의 빈도수를 나타냅니다.\n\n## SYSCALL 이벤트의 중요 역할\n\nSYSCALLs (시스템 콜)은 Linux 시스템의 감사 로그 내에서 이벤트를 탐지하고 이해하는 데 중요한 역할을 합니다. 이것이 어떻게 이벤트 탐지에 기여하는지 살펴보겠습니다:\n\n기본 시스템 작업: SYSCALLs는 Linux 시스템에서 프로세스가 수행하는 기본 작업을 나타내기 때문에 필수적입니다. 이러한 작업에는 파일 시스템 접근 (예: open, read, write, unlink), 프로세스 관리 (예: fork, execve, exit), 그리고 네트워크 통신 (예: socket, connect, sendmsg)이 포함됩니다.\n\n<div class=\"content-ad\"></div>\n\nAudit Logging: Linux 시스템은 SYSCALL 이벤트를 기록하기 위해 감사 메커니즘을 사용합니다. 감사 로그에 기록된 각 SYSCALL 이벤트에는 작업 유형 (시스템 호출), 성공 또는 실패 여부 (성공=yes/no), 관련된 프로세스 ID (pid, ppid), 사용자 및 그룹 ID (uid, gid), 그리고 실행 가능한 경로 (exe)와 같은 세부 정보가 포함됩니다.\n\n## Raw Log Entry 분석:\n\n- type=SYSCALL: 로그 항목이 시스템 호출 이벤트와 관련되어 있음을 나타냅니다.\n- msg=audit(1715771488.665:75612745): 감사 이벤트의 타임스탬프 및 일련 번호.\n- syscall=263: 시스템 호출 번호를 지정합니다.\n- success=yes: 시스템 호출이 성공했음을 나타냅니다 (yes).\n- a0, a1, a2, a3: 감사 로그 항목의 이 필드는 시스템 호출 (syscall=263) 실행 시 전달된 인수 (a0부터 a3)를 나타냅니다. 이들은 시스템 호출 작업에 대한 구체적인 세부 정보를 제공합니다.\n- exit=0: 시스템 호출의 종료 상태 (0은 성공을 나타냅니다).\n- auid=*****: 감사 사용자 ID.\n- uid=0, gid=0, euid=0, suid=0, fsuid=0, egid=0, sgid=0, fsgid=0: 프로세스와 관련된 사용자 및 그룹 ID (사용자 ID에 대한 uid 및 그룹 ID에 대한 gid).\n- comm=”rm”: 프로세스에 의해 실행된 명령 이름 (이 경우 'rm').\n- exe=”/usr/bin/rm”: 명령과 관련된 실행 가능 경로 (/usr/bin/rm).\n- key=”auditd_conf_modification”: 이 이벤트와 관련된 감사 키를 식별합니다 (auditd_conf_modification은 감사 구성에 관련된 수정을 나타냅니다).\n\n## 결론\n\n<div class=\"content-ad\"></div>\n\n리눅스 감사 로그 변조를 탐지하는 이 두 부작에서는 auditd 규칙과 구성의 무결성을 보호하는 필수 전략을 탐구했습니다. 우선, 우리는 보안 모니터링에서의 auditd의 중요 역할과 악의적인 변조에 대한 취약성을 강조했습니다. 이를 통해 auditd 규칙의 삭제와 구성 변경을 탐지하는 것에 중점을 두었는데, 이는 견고한 보안 관행을 유지하는 데 중요합니다.\n\nauditd 규칙과 Splunk 쿼리를 활용한 실용적인 구현을 통해 무단 수정을 모니터링하고 감지하는 효과적인 방법을 시연했습니다. CONFIG_CHANGE 및 SYSCALL과 같은 audit 레코드 유형을 활용하여 의심스러운 활동을 신속하게 식별할 수 있는 탐지 메커니즘을 개발했습니다. 이러한 노력은 사건 대응과 규정 준수에 중요한 감사 로그의 신뢰성을 보장합니다.\n\n## 다음 단계:\n\n앞으로의 다음 단계는:\n\n<div class=\"content-ad\"></div>\n\n- 지속적 모니터링: auditd 규칙과 설정을 계속 모니터링하여 향후 무단 변경을 감지합니다.\n- 향상된 경보 설정: Splunk에서 경보 메커니즘을 세밀하게 조정하여 auditd와 관련된 수상한 활동에 대한 실시간 알림을 제공합니다.\n- 사고 대응 준비: 신속하게 수사할 수 있도록 사고 조치를 준비하고 식별된 보안 위반 사항에 대해 대응합니다.\n- 주기적 검토: 진화하는 보안 위협과 조직적 요구에 적응하기 위해 auditd 설정과 규칙을 정기적으로 검토하고 업데이트합니다.\n\n이러한 실천 방법을 보안 운영에 통합함으로써 Linux 환경의 변경 방지 방어를 강화하고 효과적인 보안 모니터링과 규정 준수를 보장할 수 있습니다.\n\nAleksandar Matev 작성","ogImage":{"url":"/assets/img/2024-06-22-ImpairDefensesT1562012DetectLinuxAuditLogsTamperingPart2_0.png"},"coverImage":"/assets/img/2024-06-22-ImpairDefensesT1562012DetectLinuxAuditLogsTamperingPart2_0.png","tag":["Tech"],"readingTime":8},{"title":"Ubuntu Desktop 2404 LTS에서 Autoinstall 시작하는 방법","description":"","date":"2024-06-22 16:03","slug":"2024-06-22-GettingstartedwithAutoinstallonUbuntuDesktop2404LTS","content":"\n\n![이미지](/assets/img/2024-06-22-GettingstartedwithAutoinstallonUbuntuDesktop2404LTS_0.png)\n\n우분투 24.04 LTS의 가장 흥미로운 새로운 기능 중 하나는 새로운 우분투 데스크톱 설치 프로그램에 자동 설치 지원이 추가된 것입니다.\n\n자동 설치는 오랫동안 우분투 서버의 기능이었습니다. 사용자는 설치 프로세스에 맞게 맞춤 설정 구성을 적용하고 필요에 맞게 조정할 수 있습니다. 이는 사용자 생성, 네트워크 설정 구성, 패키지 설치 등을 포함합니다.\n\n우분투 23.04 및 우분투 23.10을 통해 우분투 데스크톱 설치 프로그램을 동일한 기본 코드 기반으로 이주시켰으며, 데스크톱 구성용 자동 설치를 해제하였습니다. 이제 설치 경험의 과대포장을 일부분으로 사용자가 구성을 직접 가져 올 수 있는 기능을 추가했습니다.\n\n<div class=\"content-ad\"></div>\n\n이 게시물에서는 사용자의 필요에 맞게 커스텀하고 반복 가능한 데스크톱 설치를 생성하는 방법을 간단히 보여 드리겠습니다.\n\n## 단계 1: autoinstall.yaml 파일 생성\n\nAutoinstall은 구성 yaml을 사용하여 기본 설치에 원하는 변경 또는 추가를 구조화합니다. 이는 간단한 설정으로 기본 사용자 구성 및 선호 애플리케이션을 커버하는 매우 짧은 설정이 가능하며, 고급 사용자들은 저장 레이아웃, 사용자 정의 네트워킹 및 방대한 포스트-설치 스크립트를 지정할 수 있습니다.\n\n처음 사용하는 사용자를 대상으로 간단한 예제부터 시작해 보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n```yaml\nautoinstall:\n  version: 1\n  identity:\n    hostname: ubuntu-desktop\n    username: local-optimum\n    password: \"$6$ZE4WV3QRJhPUnsNv$BpkTBYjUOxOiWV5sNPYDSitTwxW.3NHLmhRqptzpa8a4KTxGpkvMaSDbyq4PVri9kdpD1t7ldUBgwB6uveObg.\"\n  storage:\n    layout:\n      name: lvm\n  snaps:\n    - name: spotify\n      classic: false\n    - name: telegram-desktop\n      classic: false\n    - name: obsidian\n      classic: true\n    - name: code\n      classic: true\n  packages:\n    - vim\n  late-commands:\n    - curtin in-target -- wget https://repo2.protonvpn.com/debian/dists/stable/main/binary-all/protonvpn-stable-release_1.0.3-3_all.deb\n    - curtin in-target -- dpkg -i ./protonvpn-stable-release_1.0.3-3_all.deb\n    - curtin in-target -- apt update\n    - curtin in-target -- apt install -y proton-vpn-gnome-desktop\n```\n\n오 이 건 무엇을 하는 거에요?\n\n- 먼저, 우분투 데스크톱 기계의 이름은 'ubuntu-desktop'이 되며, 'local-optimum'이라는 사용자 이름과 해당 계정의 비밀번호 해시를 생성합니다 (이 경우에는 'ubuntu'에 해당함).\n- 파일 시스템 레이아웃으로 LVM을 사용하도록 지정합니다.\n- Spotify, Telegram, 멋진 노트 앱 Obsidian 및 VS Code의 네 가지 스냅을 설치합니다. Obsidian 및 VS Code는 클래식 스냅이며 이에 대한 지정이 필요합니다.\n- 제 동료들이 나노를 사용하는 저를 비웃지 않도록 Vim을 설치합니다.\n- 마지막으로, Proton VPN이라는 타사 애플리케이션을 설치하여 Proton의 공식 안내에 따르되 이 형식에 맞게 재구성한 방식으로 시스템에 설치합니다.\n\n이 스크립트는 내 GitHub에 저장되어 있으므로 데스크톱 설치 프로그램을 가리키기 위해 원시 링크를 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![Step 2](/assets/img/2024-06-22-GettingstartedwithAutoinstallonUbuntuDesktop2404LTS_1.png)\n\n# 단계 2: Ubuntu 데스크톱 24.04 LTS 설치 프로그램에 가져오기\n\n이 설정을 테스트하기 위해 가상 머신 관리자에서 VM을 만들겠어요. ISO를 마운트한 후 라이브 세션으로 부팅하고 네트워크에 연결되어 있는지 확인한 다음 아래 화면에서 \"자동 설치\" 옵션을 선택하고 내 GitHub 구성 주소를 제공할 수 있어요.\n\n![Automated Installation](/assets/img/2024-06-22-GettingstartedwithAutoinstallonUbuntuDesktop2404LTS_2.png)\n\n<div class=\"content-ad\"></div>\n\n\"유회 > 확인'을 누른 후 설치 요약 화면이 표시됩니다. 여기서 구성이 성공적으로 가져와졌는지 확인할 수 있습니다.\n\n![이미지](/assets/img/2024-06-22-GettingstartedwithAutoinstallonUbuntuDesktop2404LTS_3.png)\n\n여기까지입니다! Subiquity가 이후의 모든 작업을 처리하고 몇 분 후에 새로 설치된 데스크톱으로 다시 부팅할 수 있습니다.\n\n참고: 자동 설치 후 첫 부팅은 스냅 설치와 같은 작업을 계속 수행하므로 보통보다 시간이 더 걸릴 수 있습니다.\"\n\n<div class=\"content-ad\"></div>\n\n# 단계 3: 맞춤 설정된 Ubuntu 데스크톱을 즐기세요!\n\n지정된 변경 사항이 모두 적용되었는지 확인해 봅시다!\n\n먼저 사용자 이름과 비밀번호가 올바른지 테스트할 수 있습니다.\n\n<img src=\"/assets/img/2024-06-22-GettingstartedwithAutoinstallonUbuntuDesktop2404LTS_4.png\" />\n\n<div class=\"content-ad\"></div>\n\nVim, ProtonVPN, 그리고 설치된 스냅도 앱 뷰에 있습니다:\n\n![image1](/assets/img/2024-06-22-GettingstartedwithAutoinstallonUbuntuDesktop2404LTS_5.png)\n\n마지막으로, LVM을 사용 중임을 확인하기 위해 lvdisplay를 실행할 수 있습니다.\n\n![image2](/assets/img/2024-06-22-GettingstartedwithAutoinstallonUbuntuDesktop2404LTS_6.png)\n\n<div class=\"content-ad\"></div>\n\n# 로컬에서 자동 설치 구성을 호스팅하기\n\n이 설정은 개인화를 위한 것이므로, 공개적으로 공유되어서는 안 되는 정보나 설정 세부사항을 제공해야 하는 경우가 발생할 수 있습니다.\n\n이 경우 로컬 네트워크에서 구성을 제공하는 것이 쉽습니다. 다른 Ubuntu 기기의 디렉토리에 파일을 저장한 다음 해당 디렉토리 내에서 python3 -m http.server 8000을 실행하는 것이 가장 간단한 방법입니다.\n\n![이미지](/assets/img/2024-06-22-GettingstartedwithAutoinstallonUbuntuDesktop2404LTS_7.png)\n\n<div class=\"content-ad\"></div>\n\n설치 중에는 다음 이미지와 같이 머신 IP와 구성 파일 이름을 지정할 수 있습니다.\n\n\n# 더 많은 예제 살펴보기\n\n이 간단한 튜토리얼이 데스크탑을 표준화하고 구성하는 데 자동 설치의 힘을 이해하는 데 도움이 되기를 바랍니다. 선호하는 설정의 사용자 지정 구성을 유지 관리함으로써 Ubuntu 24.04 LTS의 각 설치를 필요에 맞게 쉽게 조정할 수 있어야 합니다.\n\n\n<div class=\"content-ad\"></div>\n\n더 심층적인 예제 구성을 보려면 정교한 서버 사용 사례도 포함된 공식 Subiquity 예제를 확인해보세요.","ogImage":{"url":"/assets/img/2024-06-22-GettingstartedwithAutoinstallonUbuntuDesktop2404LTS_0.png"},"coverImage":"/assets/img/2024-06-22-GettingstartedwithAutoinstallonUbuntuDesktop2404LTS_0.png","tag":["Tech"],"readingTime":5},{"title":" 리눅스가 내 램을 다 먹었어요","description":"","date":"2024-06-22 16:02","slug":"2024-06-22-Linuxatemyram","content":"\n\n린역 케어킹이 어렵고 복잡하다고 느끼셨던 적이 있나요? 걱정하지 마세요! 당신의 램은 괜찮아요! 이 비디오를 통해 리눅스가 메모리를 어떻게 관리하는지, 왜 램이 가득 차있는 것처럼 보이며 걱정할 필요가 없는지 설명해 드릴게요.\n\n🙋‍♂️ 그럼, 다음 질문은 무엇일까요?\n\n리눅스에서 높은 메모리 사용량을 보게 되면, 주로 디스크 캐싱 때문일 거예요. 리눅스는 사용 가능한 메모리를 디스크 작업 캐싱에 사용하여 시스템을 빠르고 효율적으로 만들어요. 이 캐싱된 메모리는 필요 없을 때 다른 곳으로 대여되고 필요할 때 언제든지 애플리케이션에 대해 즉시 확보할 수 있어요.\n\n🙋‍♂️ 다음 질문은, 왜 리눅스가 디스크 캐시를 사용할까요?\n\n<div class=\"content-ad\"></div>\n\n디스크 캐싱은 시스템 성능을 향상시키는데 도움이 됩니다. 자주 액세스되는 데이터를 메모리에 유지함으로써 디스크에서 데이터를 읽는 데 걸리는 시간을 줄일 수 있어 시스템이 더 빠르게 반응합니다. 하지만 이에는 단점이 있을까요? 사용자들이 자신의 메모리가 부족하다고 오해하게 만들 수 있지만, 이는 사실이 아닙니다.\n\n🙋‍♂️ 메모리 관리 방법을 이해해봅시다\n\n리눅스는 메모리 사용량을 다음과 같이 분류합니다:\n\n✅ 사용 중인 메모리는 응용 프로그램에서 활발히 사용되는 메모리입니다.\n\n<div class=\"content-ad\"></div>\n\n✅ 사용되지 않은 메모리가 Free Memory입니다.\n\n✅ Available Memory는 디스크 캐시에 사용되지만 즉시 애플리케이션에 재할당될 수 있는 메모리입니다.\n\n시스템 메모리를 이해해야 할 때에는 \"free\" 메모리가 아닌 \"available\" 메모리에 주목하세요.\n\n🙋‍♂️ 또 다른 혼란스러운 질문은, Swap이 더 필요한가요?\n\n<div class=\"content-ad\"></div>\n\n아마도 그렇지 않을거에요. 디스크 캐싱은 유휴 상태의 RAM을 사용하고 필요할 때 애플리케이션에 반환합니다. 스왑은 물리적 RAM이 완전히 활용될 때 사용됩니다. 애플리케이션이 더 많은 메모리를 필요로 하는 경우 커널이 디스크 캐시로부터 다시 할당하여 최소한의 스왑 사용을 보장합니다.\n\n🙋‍♂️ 메모리 사용량 확인 방법\n\n메모리 사용량을 정확하게 확인하려면 다음을 사용하세요:\n\n\nfree -m 명령어\n\n\n<div class=\"content-ad\"></div>\n\n\"실제로 응용 프로그램에 사용 가능한 메모리 양을 확인하려면 '사용 가능' 열을 보세요. 이것이 메모리 사용량을 정확히 보여줍니다.\n\n🙋‍♂️ 중요한 질문은 언제 걱정해야 하는가입니다.\n\n일반적으로 디스크 캐싱은 유용하지만, 진정한 저 메모리의 조짐이 있습니다:\n\n✅ 사용 가능한 메모리가 거의 0에 가까운 경우.\"\n\n<div class=\"content-ad\"></div>\n\n✅ 스왑 사용량이 계속해서 늘거나 변동하는 경우\n\n✅ o o m 킬러가 활성화되어 있으며, 이는 dmesg로 확인할 수 있습니다.\n\n🏁 요약하자면\n\nLinux가 메모리를 어떻게 관리하는지 이해하면 시스템 성능에 대한 불필요한 걱정이 덜어집니다. 디스크 캐싱은 시스템을 더 빠르고 반응성이 더 뛰어나게 만들며, 사용된 메모리는 즉시 애플리케이션을 위해 다시 확보할 수 있습니다. \"사용 가능한\" 메모리에 집중하면 시스템 상태에 대한 더 명확한 그림을 얻을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n🖼 이미지 참고: [이미지](https://www.linuxatemyram.com/atemyram.png)\n\n📚 책 링크:\n\n![](/assets/img/2024-06-22-Linuxatemyram_0.png)\n\n[DevOps 면접 분석](https://pratimuniyal.gumroad.com/l/cracking-the-devops-interview)","ogImage":{"url":"/assets/img/2024-06-22-Linuxatemyram_0.png"},"coverImage":"/assets/img/2024-06-22-Linuxatemyram_0.png","tag":["Tech"],"readingTime":2},{"title":"7개월간 사용한 14인치 M3 Max 맥북 프로 리뷰","description":"","date":"2024-06-22 00:38","slug":"2024-06-22-SevenMonthsWiththe14-InchM3MaxMacBookPro","content":"\n\n\n![Image](/assets/img/2024-06-22-SevenMonthsWiththe14-InchM3MaxMacBookPro_0.png)\n\n내 M3 맥스 14인치 맥북 프로의 장기 사용 후기를 게시할지 망설였어요. 이유는 마음이 식은 게 아니라(전혀 그렇지 않아요—내 비즈니스에 핵심 도구인데요), 맥북 프로 리뷰가 그다지 오래 지속되지 않기 때문이에요.\n","ogImage":{"url":"/assets/img/2024-06-22-SevenMonthsWiththe14-InchM3MaxMacBookPro_0.png"},"coverImage":"/assets/img/2024-06-22-SevenMonthsWiththe14-InchM3MaxMacBookPro_0.png","tag":["Tech"],"readingTime":1},{"title":"초보자를 위한 Kubernetes와 Docker 비교 가이드","description":"","date":"2024-06-22 00:35","slug":"2024-06-22-KubernetesvsDockerABeginnersGuide","content":"\n\n![Docker vs Kubernetes](/assets/img/2024-06-22-KubernetesvsDockerABeginnersGuide_0.png)\n\n도커와 쿠버네티스는 컨테이너화와 오케스트레이션에 대해 자주 이야기되는 두 가지 필수 도구입니다. 이 블로그에서는 도커와 쿠버네티스의 역할을 분석하고, 상호 작용하는 방법을 설명하여 각각을 사용해야 할 때를 이해하는 데 도움이 될 것입니다.\n\n# 도커란 무엇인가요?\n\n도커는 컨테이너를 사용하여 응용 프로그램을 구축, 배포 및 실행하는 프로세스를 간소화하는 상용 플랫폼입니다. 컨테이너는 코드, 런타임, 시스템 도구, 라이브러리 및 설정을 포함하여 소프트웨어를 실행하는 데 필요한 모든 것을 포함하는 가볍고 휴대 가능하며 일관된 환경입니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-KubernetesvsDockerABeginnersGuide_1.png\" />\n\n# Docker의 주요 기능\n\n- 클라이언트-서버 아키텍처: Docker는 클라이언트-서버 아키텍처를 사용합니다. Docker 클라이언트는 Docker 데몬과 통신하여 컨테이너를 빌드, 실행 및 관리합니다.\n- Dockerfile: 개발자는 Dockerfile을 작성하여 Docker 이미지를 생성합니다. Dockerfile에는 컨테이너를 빌드하는 방법에 대한 일련의 지침이 포함됩니다.\n- 사용 편의성: Docker는 응용프로그램을 패키징하는 간단한 방법을 제공하여 소프트웨어를 공유하고 배포하는 것을 더 쉽게 만듭니다.\n\n# Kubernetes란?\n\n<div class=\"content-ad\"></div>\n\n쿠버네티스(자주 K8s로 불립니다)는 컨테이너화된 애플리케이션의 배포, 확장 및 관리를 자동화하기 위해 설계된 오픈 소스 플랫폼입니다. 처음에는 구글에서 개발되었으며, 지금은 쿠버네티스가 컨테이너 오케스트레이션의 산업 표준이 되었습니다.\n\n![Kubernetes](/assets/img/2024-06-22-KubernetesvsDockerABeginnersGuide_2.png)\n\n# 쿠버네티스의 주요 기능\n\n- 오케스트레이션: 쿠버네티스는 머신 클러스터를 관리하고 사용 가능한 자원을 기반으로 컨테이너를 예약하여 실행할 수 있습니다.\n- 파드: 컨테이너는 파드라고 불리는 단위로 그룹화되며, 쿠버네티스에서 가장 작은 배포 가능한 단위입니다. 파드는 하나 이상의 리소스를 공유하는 컨테이너를 포함할 수 있습니다.\n- 서비스 검색 및 로드 밸런싱: 쿠버네티스는 클러스터 내에서 서비스 검색과 로드 밸런싱을 자동으로 관리합니다.\n- 자동화된 롤아웃 및 롤백: 쿠버네티스는 업데이트를 자동으로 배포하고, 문제가 발생할 경우 변경 사항을 롤백할 수 있습니다.\n- 구성 관리: 쿠버네티스는 구성 및 비밀 관리를 도와 원활하게 애플리케이션을 구성할 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\n# Kubernetes와 Docker 비교\n\n이미지 참조: /assets/img/2024-06-22-KubernetesvsDockerABeginnersGuide_3.png\n\n# Docker의 사용 사례\n\n- 개발 및 테스트: Docker는 일관된 개발 및 테스트 환경을 만들기에 완벽합니다.\n- 간편한 배포: Docker는 컨테이너 오케스트레이션이 주요 고려사항이 아닌 소규모 배포에 적합합니다.\n- CI/CD 파이프라인: Docker는 지속적 통합 및 지속적 배포 (CI/CD) 파이프라인과 매끄럽게 통합됩니다.\n\n<div class=\"content-ad\"></div>\n\n# Kubernetes를 사용하는 사례\n\n- 대규모 배포: Kubernetes는 복잡한 대규모 배포를 여러 서버에 걸쳐 처리하기 위해 설계되었습니다.\n- 클라우드 네이티브 애플리케이션: Kubernetes는 클라우드 환경에서 실행되어야 하는 애플리케이션에 이상적이며, 자동 스케일링 및 멀티 클라우드 배포를 지원합니다.\n- 마이크로서비스 아키텍처: Kubernetes는 각 서비스가 자체 컨테이너에 배포되고 독립적으로 스케일링되는 마이크로서비스 아키텍처를 관리하는 데 뛰어난 성능을 발휘합니다.\n- 관리형 서비스: 모든 주요 클라우드 제공업체는 인프라를 유지 관리하는 운영 부담을 줄이는 관리형 Kubernetes 서비스를 제공합니다.\n\n# Docker Swarm과 Kubernetes 중 어떤 것을 선택해야 할까요?\n\nDocker Swarm 또는 Kubernetes를 사용할지 결정할 때 다음 사항을 고려해 보세요:\n\n<div class=\"content-ad\"></div>\n\n# 도커 스웜\n\n![도커 스웜 이미지](/assets/img/2024-06-22-KubernetesvsDockerABeginnersGuide_4.png)\n\n- 설정 용이성: 도커 스웜은 설정이 쉽고 더 적은 구성이 필요합니다.\n- 소규모 작업량: 더 작고 복잡하지 않은 작업량에 적합합니다.\n- 간단한 인프라: 복잡성 없이 직접 인프라를 관리하는 팀에 적합합니다.\n\n# 쿠버네티스\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-22-KubernetesvsDockerABeginnersGuide_5.png\" />\n\n- 다양한 기능: Kubernetes는 여러 배포 전략, 네트워크 인그레스 관리 및 관찰 가능성을 포함하여 다양한 기능을 제공합니다.\n- 확장성: 대규모 및 복잡한 배포에 더 적합합니다.\n- 클라우드 통합: 클라우드 네이티브 애플리케이션을 위한 우수한 서비스 관리 기능을 제공합니다.\n\n# 결론\n\n<img src=\"/assets/img/2024-06-22-KubernetesvsDockerABeginnersGuide_6.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n도커는 컨테이너화된 애플리케이션을 빌드, 패키징 및 배포하는 데 탁월한 플랫폼입니다. 반면에 쿠버네티스는 대규모로 컨테이너화된 애플리케이션을 관리하는 강력한 오케스트레이션 도구입니다.\n\n더 작은 프로젝트나 간단한 설정의 경우 도커 스웜이 충분할 수 있습니다. 그러나 더 크고 복잡한 배포, 특히 고급 기능과 클라우드 네이티브 기능이 필요한 경우 쿠버네티스가 더 나은 선택일 것입니다.\n\n읽어 주셔서 감사합니다. 좋아요를 눌러주시고 더 많은 기사를 보고 싶으시면 제 뉴스레터를 구독해주세요. 트위터와 링크드인에서도 저와 소통할 수 있습니다. 🤠","ogImage":{"url":"/assets/img/2024-06-22-KubernetesvsDockerABeginnersGuide_0.png"},"coverImage":"/assets/img/2024-06-22-KubernetesvsDockerABeginnersGuide_0.png","tag":["Tech"],"readingTime":4},{"title":"크로스-리전 네트워크 성능 문제 조사 방법","description":"","date":"2024-06-22 00:31","slug":"2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue","content":"\n\nHechao Li, Roger Cruz\n\n# 클라우드 네트워킹 토폴로지\n\nNetflix는 SVOD(Subscription Video on Demand)와 라이브 스트리밍, 그리고 게임 서비스에 필수적인 다양한 애플리케이션을 지원하는 매우 효율적인 클라우드 컴퓨팅 인프라를 운영합니다. Amazon AWS를 활용하여, 저희 인프라는 전 세계 다양한 지역에 걸쳐 호스팅되어 있습니다. 이러한 글로벌 배포는 저희 애플리케이션이 고객에게 더 가까운 위치에서 트래픽을 제공함으로써 콘텐츠를 보다 효과적으로 전달할 수 있게 합니다. 분산 시스템과 마찬가지로, 저희 애플리케이션은 때로는 서비스 제공을 계속적으로 유지하기 위해 지역 간 데이터 동기화가 필요합니다.\n\n다음 다이어그램은 지역 간 트래픽을 위한 간단한 클라우드 네트워크 토폴로지를 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_0.png\" />\n\n# 처음 봤을 때의 문제\n\n우리의 클라우드 네트워크 엔지니어링 당직팀은 교차 지역 트래픽이 있는 애플리케이션에 영향을 미치는 네트워크 문제를 해결하기 위한 요청을 받았습니다. 초반에는 애플리케이션이 시간 초과를 경험하고 있어서 아마도 최적의 네트워크 성능으로 인한 것이라고 생각되었습니다. 우리가 다 아는 대로, 네트워크 경로가 길수록 데이터 패킷이 통과해야 하는 장치가 많아져 문제가 발생할 가능성이 높아집니다. 이 사건에서는 클라이언트 애플리케이션이 미국 지역의 내부 서브넷에 위치하고 있으며 서버 애플리케이션이 유럽 지역의 외부 서브넷에 위치하고 있는 것으로 확인되었습니다. 따라서, 데이터 패킷이 인터넷을 통해 멀리 여행해야하므로 네트워크를 탓하는 것은 자연스럽습니다.\n\n네트워크 엔지니어로서, 네트워크에 책임을 뒤질 때 우리의 초기 반응은 보통 “아니야, 네트워크가 문제일 수 없어”이며 우리의 작업은 이를 입증하는 것입니다. 최근 네트워크 인프라에 변경 사항이 없었으며 다른 애플리케이션에 영향을 주는 AWS 이슈가 보고되지 않은 상황에서 당직 엔지니어는 소음이 심한 이웃 문제를 의심하고 호스트 네트워크 엔지니어링 팀의 지원을 받기로 결정했습니다.\n\n<div class=\"content-ad\"></div>\n\n# 이웃들 탓하기\n\n이 맥락에서 잡음이 나는 이웃 문제는 컨테이너가 다른 네트워크 집약적인 컨테이너와 호스트를 공유할 때 발생합니다. 이러한 잡음이 나는 이웃들은 과도한 네트워크 자원을 소모하여 동일한 호스트의 다른 컨테이너가 네트워크 성능이 저하되는 문제를 일으킵니다. 각 컨테이너가 대역폭 제한을 가지고 있더라도, 과다 등록은 이와 같은 문제를 초래할 수 있습니다.\n\n동일한 호스트의 다른 컨테이너들을 조사한 결과 - 이 중 대부분은 동일한 애플리케이션의 일부였습니다 - 우리는 잡음이 나는 이웃의 가능성을 빠르게 제외했습니다. 문제가 있는 컨테이너와 모든 다른 컨테이너의 네트워크 처리량은 설정된 대역폭 제한보다 크게 낮았습니다. 우리는 이 문제를 해결하기 위해 이 대역폭 제한을 제거하여 애플리케이션이 필요한만큼의 대역폭을 사용할 수 있도록 했지만, 문제는 여전히 지속되었습니다.\n\n# 네트워크 탓하기\n\n<div class=\"content-ad\"></div>\n\n네트워크에서 RST 플래그로 표시된 몇 개의 TCP 패킷을 관찰했습니다. 이 플래그는 연결을 즉시 종료해야 함을 나타냅니다. 이러한 패킷의 빈도가 과도하게 높지는 않았지만, RST 패킷의 존재는 여전히 네트워크에서 의심을 불러일으켰습니다. 이것이 실제로 네트워크로 인한 문제인지 확인하기 위해 클라이언트에서 tcpdump를 수행했습니다. 패킷 캡처 파일에서 정확히 30초 후에 닫힌 TCP 스트림을 발견했습니다.\n\n18:47:06에 SYN\n\n![image](/assets/img/2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_1.png)\n\n3-way 핸드쉐이크 (SYN, SYN-ACK, ACK) 이후, 트래픽은 정상적으로 흘렀습니다. 18:47:36에 FIN (30초 후)까지 아무 이상이 없었습니다.\n\n<div class=\"content-ad\"></div>\n\n![packet_capture_image](/assets/img/2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_2.png)\n\n패킷 캡처 결과를 통해 클라이언트 애플리케이션이 FIN 패킷을 보내 연결 종료를 시작했음을 명확히 확인할 수 있었습니다. 그 이후로 서버는 데이터를 계속 보냈지만, 클라이언트가 이미 연결을 닫기로 결정했기 때문에, 이후 서버에서의 모든 데이터에 대해 클라이언트가 RST 패킷으로 응답했습니다.\n\n패킷 손실 때문에 클라이언트가 연결을 닫지 않았는지 확인하기 위해 서버 측에서도 데이터를 캡처하여 모든 서버에서 보낸 패킷이 제대로 수신되었는지 확인했습니다. 이 작업은 서버 측의 NAT 게이트웨이(NGW)를 통해 패킷이 전달되는 복잡성으로 인해 어려움이 있었습니다. 이는 서버 측에서 클라이언트의 IP 및 포트가 NGW의 것으로 나타나는데 이는 클라이언트 측에서 볼 때와 다릅니다. 따라서 TCP 스트림을 정확하게 매칭시키기 위해 클라이언트 측에서 TCP 스트림을 식별하고, 원시 TCP 시퀀스 번호를 찾아 이 번호를 서버 측에서 필터로 사용하여 해당 TCP 스트림을 찾아야 했습니다.\n\n클라이언트와 서버 측의 패킷 캡처 결과를 통해 서버에서 보낸 모든 패킷이 클라이언트가 FIN을 보내기 전에 제대로 수신되었음을 확인했습니다.\n\n<div class=\"content-ad\"></div>\n\n현재 네트워크적인 측면에서 상황이 명확해 졌어요. 클라이언트가 서버에 데이터 요청을 보내 연결을 시작했어요. 서버는 문제 없이 클라이언트에게 데이터를 계속 전송했어요. 그러나 특정 시점에, 서버는 아직 전송할 데이터가 있는데도 클라이언트가 데이터 수신을 중단하기로 선택했어요. 이로 인해 문제가 클라이언트 애플리케이션 자체와 관련이 있을 수 있다는 의심을 품게 되었어요.\n\n# 애플리케이션에 책임을 묻다\n\n문제를 완전히 이해하기 위해, 이제 애플리케이션이 어떻게 작동하는지 이해해야 해요. 아래 다이어그램에 표시된 것처럼, 애플리케이션은 us-east-1 지역에서 실행됩니다. 이는 교차 지역 서버에서 데이터를 읽어 동일 지역의 소비자에 데이터를 작성합니다. 클라이언트는 컨테이너로 실행되고, 서버는 EC2 인스턴스입니다.\n\n특히, 교차 지역에서의 읽기는 문제가 있었지만 쓰기 경로는 순조롭었습니다.가장 중요한 것은 데이터를 읽는 데 30초의 응용 프로그램 수준의 시간 제한이 있었다는 것이에요. 서버에서 초기 데이터 일괄을 30초 내에 읽지 못하면 응용 프로그램(클라이언트)이 오류를 발생시켰어요. 이 시간 제한을 60초로 증가시키면 모든 것이 예상대로 작동했어요. 이것이 클라이언트가 FIN을 시작한 이유가 된 것입니다 — 서버가 데이터 전송을 기다리는 데 인내심을 잃어서였죠.\n\n<div class=\"content-ad\"></div>\n\n![2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_3](/assets/img/2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_3.png)\n\n아마 서버는 데이터를 더 느리게 전송하도록 업데이트 되었을까요? 아니면 클라이언트 애플리케이션이 데이터를 더 느리게 수신하도록 업데이트 되었을까요? 또는 30초 이내에 완전히 전송할 수 없을 정도로 데이터 양이 너무 커졌을까요? 불행하게도, 모든 이 3가지 질문에 대해 애플리케이션 소유자로부터 부정적인 답변을 받았습니다. 서버는 변경 없이 1년 이상 운영되어 왔고, 최신 클라이언트 롤아웃에서 중요한 업데이트가 없었으며, 데이터 양도 일정했습니다.\n\n# 커널을 탓하다\n\n최근 네트워크와 애플리케이션 모두 변경되지 않았다면, 무엇이 변경되었을까요? 실제로, 우리는 문제가 최근에 발행된 Linux 커널 업그레이드 (버전 6.5.13에서 6.6.10으로 업그레이드)와 동시에 발생했음을 발견했습니다. 이 가설을 테스트하기 위해, 커널 업그레이드를 롤백했을 때 애플리케이션의 정상 작동이 복원되었음을 확인했습니다.\n\n<div class=\"content-ad\"></div>\n\n솔직히 말해서, 그 때 나는 커널 버그라고는 믿지 않았어요. 왜냐하면 저는 커널 내의 TCP 구현은 견고하고 안정적이라고 생각했거든요 (스포일러 주의: 얼마나 틀렸는지요!). 하지만 우리는 다른 각도에서 아이디어가 바닥나 있었어요.\n\n좋은 커널 버전과 나쁜 커널 버전 사이에는 약 14,000개의 커밋이 있었어요. 팀의 엔지니어들은 체계적이고 성실하게 두 버전 사이를 이분법적으로 나누었어요. 이분법적 접근법이 몇 개의 커밋으로 좁혀지자, 커밋 메시지에 \"tcp\"가 포함된 변화가 우리의 주의를 끌었어요. 최종 이분법적 접근으로 이 커밋이 문제의 근원임이 확인되었어요.\n\n또한, 이 커밋과 관련된 이메일 기록을 검토하는 도중, 같은 커널 업그레이드 이후에 파이썬 테스트 실패를 신고한 다른 사용자를 발견했어요. 비록 그들의 해결책이 우리 상황에 직접적으로 적용되지는 않았지만, 더 간단한 테스트도 우리 문제를 재현할 수 있음을 시사했어요. strace를 사용하여, 어플리케이션이 서버와 통신할 때 다음 소켓 옵션을 구성했다는 것을 관찰했어요:\n\n```js\n[pid 1699] setsockopt(917, SOL_IPV6, IPV6_V6ONLY, [0], 4) = 0\n[pid 1699] setsockopt(917, SOL_SOCKET, SO_KEEPALIVE, [1], 4) = 0\n[pid 1699] setsockopt(917, SOL_SOCKET, SO_SNDBUF, [131072], 4) = 0\n[pid 1699] setsockopt(917, SOL_SOCKET, SO_RCVBUF, [65536], 4) = 0\n[pid 1699] setsockopt(917, SOL_TCP, TCP_NODELAY, [1], 4) = 0\n```\n\n<div class=\"content-ad\"></div>\n\n그런 다음, 클라이언트가 동일한 소켓 옵션 집합을 구성하는 서버에서 클라이언트로 파일을 전송하는 최소한의 클라이언트-서버 C 애플리케이션을 개발했습니다. 테스트 중에는 클라이언트가 FIN을 발행하기 전에 일반적으로 30초 내에 전송되는 데이터 양을 나타내는 10M 파일을 사용했습니다. 이전 커널에서는 이 교차 지역 전송이 22초만에 완료되었으나 새 커널에서는 39초가 걸렸습니다.\n\n# 근본 원인\n\n최소한의 재현 설정을 통해 문제의 근본 원인을 궁극적으로 파악할 수 있었습니다. 문제의 근본 원인을 이해하기 위해서는 TCP 수신 창에 대한 이해가 필수적입니다.\n\n## TCP 수신 창\n\n<div class=\"content-ad\"></div>\n\n간단히 말하면, TCP 수신 창은 수신자가 송신자에게 \"이만큼의 바이트를 ACK하지 않고 보내도 괜찮다\"라고 알려주는 것입니다. 송신자가 서버이고 수신자가 클라이언트인 경우에는:\n\n![이미지](/assets/img/2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_4.png)\n\n## 창 크기\n\n이제 TCP 수신 창 크기가 처리량에 영향을 줄 수 있다는 것을 알았으니, 질문은 이 창 크기가 어떻게 계산되는지입니다. 응용 프로그램 작성자로서 당신은 창 크기를 결정할 수 없지만, 받은 데이터를 버퍼링하는 데 사용할 메모리의 양을 결정할 수 있습니다. 이는 방금 전 strace 결과에서 본 SO_RCVBUF 소켓 옵션을 사용하여 설정됩니다. 그러나 이 옵션의 값은 수신 버퍼에 대기하는 응용 프로그램 데이터의 양을 의미한다는 것을 유의하십시오. man 7 socket에서 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n사용자가 값 X를 제공하면, 커널은 변수 sk_rcvbuf에 2X를 저장합니다. 다시 말해, 커널은 장부 오버헤드가 실제 데이터와 마찬가지로 많다고 가정합니다(sk_rcvbuf의 50%와 같음).\n\n## sysctl_tcp_adv_win_scale\n\n하지만 위의 가정이 항상 맞지는 않을 수 있습니다. 실제로 오버헤드는 최대 전송 단위(MTU)와 같은 여러 요소에 따라 다를 수 있습니다. 그래서 커널은 이 sysctl_tcp_adv_win_scale을 제공했는데, 이를 사용하여 커널에 실제 오버헤드가 얼마인지 알려줄 수 있습니다. (99%의 사람들이 이 매개변수를 올바르게 설정하는 방법을 알지 못한다고 믿습니다. 저도 분명히 하나죠. 커널인데 오버헤드를 모르는데 어떻게 나에게 그걸 알려달라고 기대하겠어요?)\n\nsysctl 문서에 따르면,\n\n<div class=\"content-ad\"></div>\n\n거의 모든 사람들이 99% 기본값 1을 사용하고 있습니다. 이는 결국 rcvbuf/2^tcp_adv_win_scale = 1/2 * rcvbuf로 오버헤드가 계산됨을 의미합니다. 이것은 SO_RCVBUF 값을 설정할 때의 가정과 일치합니다.\n\n요약해보겠습니다. SO_RCVBUF를 65536으로 설정한다고 가정해 봅시다. 이 값은 소켓 옵션의 setsockopt 시스콜에 의해 설정된 값입니다. 그러면 다음과 같은 결과가 나옵니다:\n\n- SO_RCVBUF = 65536\n- rcvbuf = 2 * 65536 = 131072\n- 오버헤드 = rcvbuf / 2 = 131072 / 2 = 65536\n- 수신 창 크기 = rcvbuf — 오버헤드 = 131072–65536 = 65536\n\n(참고: 이 계산은 단순화된 것입니다. 실제 계산은 더 복잡합니다.)\n\n<div class=\"content-ad\"></div>\n\n간략히 말해서, 커널 업그레이드 전 수신 윈도우 크기는 65536이었습니다. 이 창 크기로 응용 프로그램은 30초 내에 10M 데이터를 전송할 수 있었습니다.\n\n## 변경 내용\n\n이 커밋은 sysctl_tcp_adv_win_scale을 더 이상 사용하지 않도록 만들었고, 오버헤드나 창 크기를 더 정확하게 계산할 수 있는 스케일링 비율을 도입했습니다. 이것이 옳은 조치입니다. 변경으로 인해, 윈도우 크기는 이제 rcvbuf * scaling_ratio입니다.\n\n그러면 scaling_ratio는 어떻게 계산되는 걸까요? skb-`len 및 truesize로 skb 내의 tcp 데이터 길이와 skb의 총 크기를 사용하여 계산됩니다. 이는 하드코딩된 50%보다 실제 데이터를 기반으로 한 더 정확한 비율입니다. 그럼 다음 질문은 여기 있습니다: TCP 핸드셰이크 중 데이터를 전송하기 전에 초기 scaling_ratio를 어떻게 결정할까요? 답은, 대략 0.25로 설정된 마법과 보수적인 비율이 선택되었습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 다음과 같습니다:\n\n- SO_RCVBUF = 65536\n- rcvbuf = 2 * 65536 = 131072\n- 수신 창 크기 = rcvbuf * 0.25 = 131072 * 0.25 = 32768\n\n요약하자면, 커널 업그레이드 후 수신 창 크기가 반으로 줄었습니다. 그 결과로 처리량이 절반으로 줄어 데이터 전송 시간이 두 배로 증가했죠.\n\n당연히 궁금증이 생길 수 있습니다. 초기 창 크기가 작은 것은 이해되지만, 나중에 페이로드의 더 정확한 비율(즉, skb-`len/skb-`truesize)이 있을 때 창이 왜 커지지 않나요? 몇 가지 디버깅을 거친 뒤에 우리는 scaling_ratio가 더 정확한 skb-`len/skb-`truesize로 업데이트된다는 것을 알 수 있었어요. 우리의 경우에는 약 0.66입니다. 그러나 다른 변수인 window_clamp는 이에 맞게 업데이트되지 않습니다. window_clamp는 광고할 수 있는 최대 수신 창이며, 초기 scaling_ratio를 사용하여 0.25 * rcvbuf로 초기화됩니다. 그 결과로 수신 창 크기가 이 값으로 제한되어 더 커질 수 없습니다.\n\n<div class=\"content-ad\"></div>\n\n# 문제 해결\n\n이론상으로는 window_clamp를 scaling_ratio와 함께 업데이트하는 것이 해결책입니다. 그러나 다른 예기치 않은 동작을 도입하지 않는 간단한 해결책을 찾기 위해 최종 결정은 초기 scaling_ratio를 25%에서 50%로 증가시키는 것이었습니다. 이로써 수신 윈도우 크기가 원래의 기본 sysctl_tcp_adv_win_scale과 하위 호환성을 유지할 수 있습니다.\n\n한편, 문제가 변경된 커널 동작뿐만 아니라 응용 프로그램이 SO_RCVBUF를 설정하고 30초의 응용 프로그램 수준 타임아웃을 가지고 있기 때문에 발생한다는 것을 유의해야 합니다. 사실, 응용 프로그램은 Kafka Connect이며 두 가지 설정 모두 기본 구성(receive.buffer.bytes=64k 및 request.timeout.ms=30s)입니다. 또한 receive.buffer.bytes를 -1로 변경하여 Linux가 수신 윈도우를 자동 조정할 수 있도록 허용하기 위해 카프카 티켓을 작성했습니다.\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n이번 디버깅 연습은 넷플릭스의 여러 계층과 인프라를 다루는 매우 흥미로운 경험이었습니다. 사실 \"네트워크\"가 문제가 아니었지만, 이번에는 네트워크를 구성하는 소프트웨어 구성 요소인 커널 내의 TCP 구현이 문제였음을 발견했습니다.\n\n만약 이러한 기술적인 도전에 흥미를 느낀다면, 저희의 클라우드 인프라 엔지니어링 팀에 합류를 고려해보세요. 넷플릭스 채용 페이지를 방문하여 Cloud Engineering 직무 기회를 살펴보세요.\n\n# 감사의 말\n\n저희를 위해 이 문제를 조사하고 완화하기 위해 노력한 동료인 Alok Tiagi, Artem Tkachuk, Ethan Adams, Jorge Rodriguez, Nick Mahilani, Tycho Andersen 및 Vinay Rayini에게 특별히 감사드립니다. 또한 Linux 커널 네트워크 전문가인 Eric Dumazet에게 패치를 검토하고 적용해 준 것에 대해 감사의 말씀을 전합니다.","ogImage":{"url":"/assets/img/2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_0.png"},"coverImage":"/assets/img/2024-06-22-InvestigationofaCross-regionalNetworkPerformanceIssue_0.png","tag":["Tech"],"readingTime":10}],"page":"42","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true}