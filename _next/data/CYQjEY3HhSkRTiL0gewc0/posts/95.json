{"pageProps":{"posts":[{"title":"플라토닉 인공 지능 모든 인공 지능이 같아지고 있는가","description":"","date":"2024-06-19 03:43","slug":"2024-06-19-PlatonicAIAreAllAIsBecomingtheSame","content":"\n\n몇 일 전에 AI와 철학을 결합한 새로운 이론을 발견했어요: \"AI가 모두 같은 것이 되고 있는 걸까?\" 시사하는 것은, 앞으로 모든 AI 모델이 동일해질까요?\n\n실제로 연구자들은 이미 이것이 벌어지고 있다는 명확한 증거를 발견했습니다. 모든 모델들이 '플라톤적' 표현을 향해 수렴하는 것처럼 보이죠:\n\n세계를 이해하는 하나의 독특한 방법.\n\n만약 이게 사실이라면, 경제학적으로나 철학적으로 엄청난 파장을 불러올 수 있습니다. 왜냐하면 이것은 AI가, 기초 모델 덕분에 현실 자체를 발견, 설명하고, 중요한 것은 예측하기 시작하고 있다는 신호가 될 수 있기 때문이죠.\n\n<div class=\"content-ad\"></div>\n\n# 표현\n\n이 질문을 제기하는 연구자들을 이해하기 위해 우리는 모델이 세계를 해석하는 방법을 이해해야 합니다.\n\n그리고 그것이 바로 표현을 통해 이루어집니다.\n\n## 인공지능에서 가장 중요한 단어\n\n<div class=\"content-ad\"></div>\n\n세상의 모든 개념을 학습할 때마다, AI 모델들은 키 속성을 포착하는 방식으로 그것을 설명하는 요약된 표현을 만듭니다.\n\n특히, AI 모델들은 모든 개념을 임베딩으로 변환하여 벡터 형태로 표현합니다.\n\n그런데 왜 벡터인 걸까요? 세상의 모든 것을 벡터로 만들면 두 가지 이점이 있습니다:\n- 숫자 형태의 개념: 기계는 숫자만 해석할 수 있습니다. 따라서 모든 것을 숫자 형태로 어떤 방식으로든 해석해야 합니다.\n- 유사성: 모든 것을 벡터로 만들어두면, 그들 사이의 거리를 측정할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n따라서 모델은 OpenAI가 정의한 관련성이라는 원칙에 의해 지배되는 세계 개념의 고차원(벡터 당 많은 숫자) 공간을 구축합니다.\n\n이 내부 세계에서 두 실제 세계 개념이 더 유사할수록, 이 공간에서 그들의 벡터가 더 가까워져야 합니다.\n\n예를 들어 이 공간에서 '개'와 '고양이'는 '개'와 '창문'보다 가깝지만 '개'와 '비둘기'보다도 가깝습니다. 왜냐하면 '개'와 '고양이'가 더 의미론적으로 관련되어 있기 때문입니다(비행하지 않는, 네 다리, 가정적 등).\n\n![image](/assets/img/2024-06-19-PlatonicAIAreAllAIsBecomingtheSame_0.png)\n\n<div class=\"content-ad\"></div>\n\n미지공간(이렇게 불리는 공간들)의 매력적인 점은 양방향으로 작동한다는 것입니다. AI 모델에게 세계의 개념을 가르치는 데 도움을 주는 것뿐만 아니라, 아직까지 인간들이 깨닫지 못한 세계 패턴들을 발견하는 데도 도움을 줄 수 있습니다.\n\n예를 들어:\n\n- 미지 공간을 중심으로 한 의미 공간 이론은 Hume.ai에 의해 인간 감정의 새로운 매핑을 발견하는 데 도움이 되고 있습니다.\n- Osmo.ai가 이끄는 연구는 세계의 냄새를 매핑하고 새로운 냄새를 발견하기 위해 보간하는 것이 도움이 되고 있습니다.\n\n![이미지](/assets/img/2024-06-19-PlatonicAIAreAllAIsBecomingtheSame_1.png)\n\n<div class=\"content-ad\"></div>\n\n하지만 이게 어떻게 가능한 걸까요?\n\n사실 AI는 인간보다 패턴 매칭에서 뛰어나며, 우리에게는 무의식적으로 보였거나 우리가 인식하기에 너무 편향되었던 데이터의 주요 패턴을 찾아냅니다 (AI는 서양 문화와 다른 문화를 이어주는 역할을 합니다; 예를 들어, 서양이 아닌 국가의 사람들은 특정 감정을 다르게 표현합니다).\n\n따라서, 만약 AI가 현실에 대한 '편향되지 않은' 시각을 갖추고 있다면, 그들은 현실을 그대로 관찰할 수 있는 능력을 갖고 있을까요?\n\n연구자들이 추측한 대답은 네, 라고 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 동굴의 애니콜\n\nAI들이 모든 관찰이나 사건을 일으키는 현실의 진정한 본성을 배우는 것이 가능할까요?\n\n## 거리가 중요합니다\n\n만약 그게 가능하다면, 우리는 AI 교육을 충분히 성숙시켜서 우리의 기초 모델들이 현실을 해석하는 진정한 방법이 하나뿐이라는 동일한 모델로 진화될 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\n이 이론을 검증하기 위해 연구원들은 이러한 모델들의 표현이 모두 세계에 대한 단일 표현으로 수렴해야 한다고 주장했습니다. 이는 현실을 표현하는 객관적이고 보편적인 방법입니다.\n\n하지만 이게 일어나고 있는지 어떻게 테스트할 수 있을까요? 이를 위해 연구원들은 여러 인기 모델의 잠재 공간을 비교했습니다.\n\n우리가 기억한다면, AI 모델의 세계 표현은 고차원 공간입니다 (이전 다이어그램에서 간단히 세 개의 차원으로 표현됨) 비슷한 것들이 가까이에 있고, 다른 개념들은 멀리 밀려 있습니다.\n\n그러나 개념 표현의 전체 분포뿐만 아니라 거리도 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n그것은 하나의 모델이 '빨간색'에 할당하는 표현이 동일한 모드(다른 LLMs와 비교) 내 다른 모델과 유사해야 할뿐만 아니라 언어 모델과 시각 모델이 '적색'을 해석하는 방식도 유사해야 한다는 뜻입니다.\n\n간단히 말해, 모델 간 '적색'과 '파란색' 사이의 거리가 동일해야 하며, 이렇게 함으로써 각 모델이 현실이 제시하는 색 '빨간색'을 어떻게 해석하는지가 동일함을 입증하게 됩니다. 이렇게 두 모델이 '빨간색'이라는 개념으로 수렴하고 있다는 것을 증명하는 것이죠.\n\n그런데 왜 ‘플라톤적 표현’이라고 부를까요?\n\n**현실의 부분적 관점**\n\n<div class=\"content-ad\"></div>\n\n특히, 연구자들은 플라톤의 동굴 비유를 인용합니다. 현재 모델에 공급하는 데이터는 그림자이며, 이는 현실의 모호한 대표물입니다. 이전 AI 시스템은 그림자를 지켜보는 사람들로, 이는 그들이 삶의 부분적인 면밖에 알지 못한다는 것을 의미합니다.\n\n하지만 규모와 강제적인 다중 작업을 통해, 기초 모델들은 그들의 데이터를 초월하여 결국 동굴에서 나와 진정한 현실의 본질을 깨달을 것입니다.\n\n그러나 이것이 의미하는 것은 무엇일까요?\n\n<div class=\"content-ad\"></div>\n\n아래 이미지를 살펴보면, fimage와 ftext는 동일한 개념의 세계를 관찰하기 때문에, 그들의 표현 - 즉 벡터 -은 동일해야합니다.\n\n![이미지](/assets/img/2024-06-19-PlatonicAIAreAllAIsBecomingtheSame_3.png)\n\n모든 것이 좋아 보이지만, 이것이 일어나고 있는지 어떤 지표가 있을까요?\n\n# 수렴\n\n<div class=\"content-ad\"></div>\n\n매혹적으로, 기반 모델들은 수렴하고 있습니다.\n\n비전 모델들과 LLM 세트를 비교할 때, 내부 표현의 유사성인 정렬(alignment)은 LLM 성능과 비전 모델 간의 거의 1대1 상관 관계를 갖습니다.\n\n쉽게 말해, 우리 LLM이 더 잘 수행할수록, 다른 강력한 비전 모델들과의 표현이 얼마나 유사해지는지가 증가하며 완전히 다른 형태로 제시되었음에도 불구하고 유사해집니다. 이것은 또 다른 말로,\n\nLLM이 더 나아질수록, 그들의 세계 이해가 비전 모델로 수렴한다는 신호입니다. 모델이 커짐에 따라, 그들의 모드나 사용된 데이터에 관계없이, 모든 것이 동일한 표현, 동일한 현실에 대한 이해로 수렴하며 그 결과 더 똑똑해집니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Platonic AI: Are All AIs Becoming the Same](/assets/img/2024-06-19-PlatonicAIAreAllAIsBecomingtheSame_4.png)\n\n이러한 경향이 발생하는 이유는 아래 이미지에서 명확하게 나타났습니다: 모델이 더 많은 문제를 해결하기 위한 공통적인 방법을 찾아야 하므로 두 작업에 대한 가능한 해결책 공간이 줄어듭니다:\n\n![Platonic AI: Are All AIs Becoming the Same](/assets/img/2024-06-19-PlatonicAIAreAllAIsBecomingtheSame_5.png)\n\n결국, 모델이 더 크고 훈련되는 기술 집합이 더 광범위할수록 이러한 모델들은 서로 수렴하는 경향이 있습니다. 사용된 모달리티와 데이터셋과는 독립적으로, 어느 날 언젠가 우리 모두의 선도 연구소가 동일한 모델을 만들어내도록 수렴할 가능성을 그려낼 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 중요한 질문하기\n\n인공지능과 철학의 만남.\n\n나에게 있어 가장 중요한 관심사는 이것이 시장에 엄청난 재화화 효과를 줄 수 있다는 사실 외에도, 이것이 우리 인간들이 세계 모델을 만드는 방향으로 다가가고 있다는 첫 번째 신호일 수 있다는 점입니다. 세계 모델은 로봇이 언젠가 우리 세상에서 살아가는 모습을 상상하는 등 거의 모든 인공지능 분야에서 절대적으로 중요하다고 생각됩니다.\n\n<div class=\"content-ad\"></div>\n\n세계 모델은 인공 지능에게 환경을 관찰하고 적응할 수 있는 능력을 제공합니다. 인간 뇌는 계속해서 다음에 일어날 일을 추론(예측)하고 있습니다.\n\n만약 인간들이 진정으로 일반적인 세계 모델을 훈련하는 방법을 알아낸다면, 인공 지능이 어떻게 변할지를 말로 표현하기 어려울 것입니다. 가장 강력한 모델들이 세계의 동일한 표현으로 수렴되는 것을 보면, 적어도 해결책에 접근하고 있다는 신호로 받아들일 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-PlatonicAIAreAllAIsBecomingtheSame_0.png"},"coverImage":"/assets/img/2024-06-19-PlatonicAIAreAllAIsBecomingtheSame_0.png","tag":["Tech"],"readingTime":5},{"title":"Pinterest의 Ray 인프라","description":"","date":"2024-06-19 03:39","slug":"2024-06-19-RayInfrastructureatPinterest","content":"\n\n| 이름 | 직급 |  \n| --- | --- |  \n| Chia-Wei Chen | 주임 소프트웨어 엔지니어 |  \n| Raymond Lee | 주임 소프트웨어 엔지니어 |  \n| Alex Wang | 소프트웨어 엔지니어 I |  \n| Saurabh Vishwas Joshi | 주임 스태프 소프트웨어 엔지니어 |  \n| Karthik Anantha Padmanabhan | 주임 엔지니어 관리자 |  \n| Se Won Jang | 주임 엔지니어 관리자 |  \n\n# 우리 Ray 인프라의 여정\n\n저희 블로그 시리즈의 제1부에서는 Ray에 투자하여 비즈니스의 핵심적인 문제를 해결하기 위해 동기부여를 받았던 이유에 대해 논의했습니다. 이 블로그 포스트에서는 Pinterest와 같은 웹 규모 회사에 Ray를 통합하는 데 필요한 단계에 대해 더 자세히 설명하겠습니다. 우리는 새로운 기술을 받아들이기 위해 다양한 고유한 제약 조건과 도전에 직면한 Pinterest와 같은 기업에 Ray를 통합하는 데 필요한 것들을 논의할 것입니다. 이는 Ray 인프라 부분에 대한 더 포괄적인 버전으로, 우리가 Ray 설명에서 발표한 Last Mile Data Processing for ML Training using Ray in Ray Summit 2023의 일환입니다.\n\n저희의 사용 사례에서 KubeRay가 제공하는 Ray 클러스터를 프로비저닝할 수 있는 능력이 Ray 인프라를 성숙화하는 일의 일부일 뿐입니다. 회사들은 Ray 및 다른 특정 요구사항을 준수해야 하며, 이는 로그, 메트릭 지속성, 네트워크 격리, 최적의 하드웨어 인스턴스 식별, 보안, 트래픽 설정 및 기타 내부 서비스 통합을 포함한 모든 다른 모베스트 관행을 따라야 합니다.\n\n<div class=\"content-ad\"></div>\n\n2023년에 여정이 시작되었어요. 전문 엔지니어 한 명이 이 프로젝트에 50%의 시간을 투자했어요:\n\n- 2023년 1분기: Anyscale 파트너의 지원을 받아 프로토타이핑 단계가 시작되었어요\n- 2023년 2분기: Ray Infra MVP가 완료되었고, 시스템 로깅, 메트릭, UI, 및 애플리케이션을 위한 CLI와 같은 필수 도구들이 반복되고 향상되었어요\n- 2023년 3분기: 첫 번째 프로덕션 유즈 케이스에 중점을 두어 내부 시스템을 통합하여 서비스 안정성을 향상시켰어요\n- 2023년 4분기: 프로덕션 준비에 중점을 두어 보안 문제 대응, 네트워크 안정성 향상, Ray-optimized Kubernetes 환경으로의 전환 평가가 진행되었어요\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_0.png)\n\n# 직면한 도전들\n\n<div class=\"content-ad\"></div>\n\n핀터레스트에서 Ray 인프라를 구축할 때 몇 가지 중요한 도전 과제가 발생했습니다. 이러한 도전 과제를 해결해야 했습니다:\n\n- K8s API 접근 제한: 핀터레스트의 일반 목적의 연합 Kubernetes 클러스터인 PinCompute에서 작동 중이었기 때문에 KubeRay와 같은 필수 오퍼레이터와 해당 사용자 정의 정의를 설치하는 것이 제한되었습니다.\n- 일시적인 로깅 및 메트릭: Ray 클러스터가 활성 상태인 경우 Ray 대시보드를 통해 로깅과 메트릭을 확인할 수 있었지만, 리소스 집약적인 Ray 클러스터를 디버깅 목적으로 유지하는 것은 현실적이지 않았습니다. Ray 작업 부하의 수명 주기를 영속화하고 다시 재생하는 해결책을 찾았습니다.\n- 메트릭 통합: 회사는 프로메테우스와 그라파나와 같은 인기 있는 오픈 소스 솔루션과 달리 자체의 시계열 데이터베이스와 시각화 도구를 보유하고 있었습니다.\n- 인증, 권한 부여, 감사 (AAA) 가이드라인: 회사 표준에 따라 K8s에서 실행되는 서비스에 AAA를 보장해야 했고, 핀터레스트에서 AAA를 빌드하기 위해 Envoy를 서비스 메시로 사용하는 것이 권장되었습니다.\n- 다양한 개발 환경: Jupyter와 CLI 액세스와 같은 상호 작용하는 옵션과 다양한 개발자 요구를 충족하기 위해 Dev 서버에서 CLI 액세스와 같은 다양한 개발 경험이 구축되었습니다.\n- 비용 최적화 및 자원 낭비: 유휴 상태의 Ray 클러스터는 상당한 비용이 발생할 수 있습니다. 팀의 인식을 높이고 자원 낭비를 줄이기 위해 가비지 컬렉션 정책과 비용 할당이 필요했습니다.\n- 오프라인 데이터 분석: 오프라인 분석을 위해 모든 Ray 클러스터 관련 메트릭을 대규모 데이터 형식 (예: 하이브, 파케)으로 내보내는 것이 우선되었습니다. 이 데이터에는 GPU 활용률과 같은 메트릭이 포함되어 있어 개선 영역을 식별하고 응용 프로그램 및 인프라의 안정성을 시간 경과에 따라 추적할 수 있습니다.\n\n## 쿠버네티스 기반\n\nK8s API 액세스 제한으로 인해 우리 환경에 KubeRay를 쉽게 설치할 수 없어 K8s에서 Ray 클러스터를 운영하기 어려웠습니다. 또한, 핀터레스트 K8s 클러스터 내에서 비밀 관리, 트래픽 처리 및 로그 회전과 같은 작업을 위해 다른 팀에서 관리되는 특정 사이드카가 필요했습니다. 버그 수정이나 보안 패치와 같은 필수 사이드카 업데이트를 중앙 제어하기 위해 특정 제한 사항에 따라 준수해야 했습니다.\n\n<div class=\"content-ad\"></div>\n\nRay 클러스터에 필요한 필수 구성 요소를 프로토타입으로 만드는 중입니다(On-Premise Cluster 가이드에서 설명된 대로), 필요한 사이드카를 통합하기 위해 Pinterest 특정 CRD를 사용하기로 결정했습니다. 이는 오픈 소스 Kubeflow PyTorchJob 위에 구축된 래퍼입니다.\n\n최초 반복에서는 Ray 헤드 및 Ray worker를 클라이언트 측에서 만들어 간단하게 유지하기로 하였습니다. 각 구성 요소에 대해 다른 명령을 사용하고 클라이언트 측에서 실행될 사용자 정의 스크립트를 작성하는 것을 포함했습니다.\n\n```js\ndef launch_ray_cluster(configs: RayClusterConfig) -> str:\n    # resources, instance_type, command, envs_vars 등 정의\n    configs = RayClusterAndJobConfigs()\n    with ThreadPoolExecutor() as executor:  \n        # 함수를 실행자에 제출\n        ray_head = executor.submit(launch_ray_head(configs)).result()\n        ray_workers = executor.submit(launch_ray_workers(configs).result()\n    return check_up_and_running(ray_head, ray_workers)\n```\n\n이 단계에는 개선할 여지가 많이 있습니다. 주요 단점은 클라이언트 측 실행이 네트워크 오류나 만료된 자격 증명과 같은 다양한 이유로 중단될 수 있어 K8s에서 자원을 낭비하는 좀비 Ray 클러스터가 발생할 수 있다는 것입니다. 이 접근 방식은 Ray 사용자들이 Ray에서 놀 수 있도록 막는 데 충분하지만 Ray 클러스터를 효율적으로 관리하기 위해 설계된 플랫폼에 적합하지는 않습니다.\n\n<div class=\"content-ad\"></div>\n\n# API Gateway & Controller\n\n두 번째 반복에서는 클라이언트 측에서 Ray 클러스터를 관리하는 방식에서 KubeRay와 유사한 컨트롤러를 개발하여 서버 측 접근 방식으로 전환되었습니다. 우리의 솔루션은 사용자와 K8s 사이에 중간 계층을 생성하여 API 서버, Ray 클러스터/작업 컨트롤러 및 MySQL 데이터베이스로 구성된 여러 구성 요소를 포함했습니다.\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_1.png)\n\n- API 서버: 이 구성 요소는 요청 유효성 검사, 인증 및 권한 부여를 용이하게 합니다. 클라이언트 측에서 K8s의 복잡성을 추상화하여 사용자가 플랫폼 API 인터페이스와 상호 작용할 수 있도록 하며, 특히 나중 섹션에서 TLS 관련 구현을 강화하는 데 특히 유용합니다.\n- MySQL 데이터베이스: 데이터베이스는 Ray 클러스터와 관련된 상태 정보를 저장하여 K8s 측에서 필요한 일시적 상태를 다시 생성할 수 있게 합니다. 또한 API 서버와 Ray 클러스터 컨트롤러 간의 데이터 흐름을 분리하고 오프라인 분석을 위해 Hive로 데이터 덤프를 수행하는 추가 혜택이 있습니다.\n- Ray 클러스터 컨트롤러: 이 구성 요소는 Ray 클러스터의 수명 주기 관리를 위해 K8s를 지속적으로 쿼리합니다. Ray 헤드 및 워커 노드 프로비저닝, Ray 클러스터 상태 모니터링 및 필요에 따른 정리 작업을 수행하는 역할을 합니다.\n- Ray 작업 컨트롤러: Ray 클러스터 컨트롤러와 유사하게, Ray 작업 컨트롤러는 Ray 작업 수명 주기 관리에 초점을 맞춥니다. RayJobs를 제출하기 위한 주요 엔티티로 작용하여 시스템 내에서 적절한 인증 및 권한 부여 프로토콜을 보장합니다. 또한 컨트롤러는 동일한 Ray 클러스터에 여러 Ray 작업을 제출할 수 있도록 지원함으로써 사용자가 각 작업 제출마다 새 Ray 클러스터 프로비저닝을 기다릴 필요 없이 더 효율적으로 반복할 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\n여기는 테이블 태그를 마크다운 형식으로 변경한 것입니다.\n\n이 방식은 사용자와 Kubernetes 사이에 가치 있는 추상화 계층을 제공하여 사용자가 복잡한 Kubernetes 자산을 이해할 필요가 없게 합니다. 대신, 플랫폼에서 제공하는 사용자 대면 라이브러리를 활용할 수 있습니다. 클라이언트 측에서 프로비저닝 단계의 부담을 옮기면 프로세스가 간소화되어 단계가 단순화되고 전반적인 사용자 경험이 향상됩니다.\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_2.png)\n\n우리 자체 컨트롤러를 구현하는 동안, 우리는 모듈화를 보장하여 나중에 KubeRay로의 원활한 전환을 가능하게 했습니다. 이 방식은 Ray 클러스터를 시작하는 데 사용되는 방법을 손쉽게 바꿀 수 있어, 내부 Kubernetes 기본 도구에서 KubeRay로의 전환이 원활하게 이루어집니다.\n\n```python\nClass Controller:\n    def reconcile(self, ray_cluster: RayClusterRecord):\n        # 이 부분은 내부 기본 도구에서 KubeRay로 교체 가능합니다.\n        status, k8s_meta = self.launch_and_monitor_ray_cluster(ray_cluster.configs)\n        db.update(ray_cluster, status=status, k8s_meta=k8s_meta)\n\n    def run(self):\n        while True:\n            ray_clusters = db.get_ray_cluster_to_dispatch()\n            for ray_cluster in ray_clusters:\n                self.reconcile(ray_cluster)\n            sleep(1)\n   \n    def launch_and_monitor_ray_cluster(self, configs) -> Tuple[str, Dict]:\n        return get_actual_k8s_related_status(ray_identifier=configs.ray_identifier)\n```\n\n<div class=\"content-ad\"></div>\n\n가능한 한 Observable성을 고려해보세요. Ray 클러스터의 기존 Ray 대시보드는 클러스터가 활성화된 상태에서만 접근할 수 있고 로그 또는 메트릭 재생을 위한 기능이 없습니다. 따라서 우리는 지속적인 로깅 및 메트릭 기능을 통합하는 전용 사용자 인터페이스를 개발하기로 결정했습니다. 이 사용자 인터페이스는 이전에 구축한 API Gateway에 의해 지원되며, 실시간으로 Ray 클러스터 및 Ray 작업 상태에 대한 통찰력을 제공합니다. 모든 메타데이터, 이벤트 및 로그는 데이터베이스 또는 S3에 저장되므로 이 전략은 Ray 클러스터를 유지하지 않고도 로그 분석을 가능하게 함으로써 GPU와 같은 유휴 리소스로 인한 비용을 완화할 수 있습니다.\n\n![image](/assets/img/2024-06-19-RayInfrastructureatPinterest_3.png)\n\n다양한 회사들이 자체 시계열 메트릭 솔루션을 가지고 있다는 것은 사실일 것입니다. Pinterest에서는 Goku라는 자체 시계열 데이터베이스를 활용하고 있으며, 이는 OpenTSDB와 호환되는 API를 가지고 있습니다. 우리는 prometheus 메트릭을 스크랩하고 내부 시스템과 호환되도록 재구성하는 추가 사이드카를 실행하고 있습니다. 로깅에 관해서는 Ray의 권고사항을 따라 로그를 AWS S3에 지속 저장하고 있습니다. 이러한 로그는 API 서버에서 소비되어 Ray 클러스터 UI에 표시됩니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-RayInfrastructureatPinterest_4.png\" />\n\n## Ray Application Stats\n\n동일한 Grafana 차트를 회사 내 시각화 도구인 Statsboard로 번역했습니다. 또한 Pinterest의 ML 엔지니어에게 유용한 dcgm GPU 메트릭 및 dataloader 메트릭과 같은 더 많은 애플리케이션별 기능을 추가했습니다. 이를 통해 Ray 애플리케이션의 병목 현상과 문제를 식별할 수 있습니다.\n\n<img src=\"/assets/img/2024-06-19-RayInfrastructureatPinterest_5.png\" />\n\n<div class=\"content-ad\"></div>\n\n## Ray Infrastructure Stats\n\n인프라 수준의 모든 메트릭을 모니터링하는 것은 효과적인 모니터링 구현, 알림 생성, 그리고 역사적 데이터를 기반으로 한 SLO/SLA 벤치마킹을 설정하는 데 중요합니다. 예를 들어, 레이 클러스터 대기 시간의 종단 간 추적 및 레이 작업의 롤링 성공률 모니터링은 시스템 성능을 평가하고 유지하는 데 중요합니다. 또한, 레이 클러스터 프로비저닝 실패로 이어질 수 있는 플랫폼 측의 오류를 식별하는 것은 운영 효율을 유지하는 데 중요합니다.\n\n![Ray Infrastructure](/assets/img/2024-06-19-RayInfrastructureatPinterest_6.png)\n\n# 개발 및 운영 인터페이스\n\n<div class=\"content-ad\"></div>\n\n핀터레스트에서 Ray 애플리케이션을 개발하는 세 가지 옵션을 제공합니다. Dev 서버, 주피터, Spinner 워크플로우가 있습니다. 모든 옵션은 ML 플랫폼의 RESTful API를 사용하여 구동됩니다.\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_7.png)\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_8.png)\n\nAirflow에서 PythonOperator를 활용하여 사용자가 작업 구성을 제공하고, 우리는 그것을 MLP 서버로 향하는 RayJob 요청으로 변환하는 사용자 정의 연산자를 구성합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_9.png)\n\n# 테스트\n\n유닛 테스트 및 통합 테스트\n\n레이 애플리케이션을 개발할 때 이용할 수 있는 두 가지 유형의 테스트를 제공합니다:\n\n\n<div class=\"content-ad\"></div>\n\n- Unittest을 사용하는 것을 권장하는 것은 하위 수준 Ray 코어나 Ray 데이터 라이브러리를 활용하는 플랫폼 라이브러리 소유자들에게 좋습니다. 통합 테스트가 적합합니다. Ray 프로그램을 테스트하는 팁을 따르고 동일한 테스트 스위트 내에서 ray 클러스터를 가능한 한 많이 재사용하기 위해 pytest fixtures를 사용합니다.\n- 완전한 Ray 작업을 실행하여 코드 변경이나 라이브러리 업데이트로 인해 발생할 수 있는 잘못된 부분을 식별하고 해결하려는 사용자들에게는 통합 테스트가 적합합니다. 비즈니스에 중요한 Ray 응용 프로그램의 건강 상태를 주기적으로 모니터링하기 위해 통합 테스트도 실행합니다.\n\n![Ray Infrastructure at Pinterest](/assets/img/2024-06-19-RayInfrastructureatPinterest_10.png)\n\n## 네트워크 및 보안\n\nRay는 컴퓨팅 플랫폼으로 개발자가 API를 통해 쉽게 워크로드를 실행할 수 있는 유연성을 제공하지만, 이는 보안 취약점(CVE-2023-48022)으로 이어질 수 있습니다. Shadowray 기사에서 강조하는 것과 같이 Ray 자체가 적절한 인증 및 권한 부여 방법을 제공하지 않기 때문에 Ray 대시보드 API에 액세스 권한이 있는 모든 사용자가 유효성 검사나 제어 없이 원격으로 코드를 실행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n핀터레스트에서는 이 보안 문제를 심각하게 취급하고 적절히 대응했습니다. 우리는 레이 클러스터에 올바른 인증 및 권한 부여가 적용되어야 한다는 것을 확실히 하기 위해 한 발 더 나아갔습니다. 따라서 사용자가 적절한 권한을 갖고 있지 않은 경우 해당 Ray 클러스터를 사용할 수 없도록 했습니다.\n\n그러나 이 문제의 복잡성은 핀터레스트의 연방 쿠버네티스 클러스터 아키텍처로 인해 더욱 악화되었는데, 이는 군집 간 기능을 군집 내 환경에 적용하는 데 어려움을 겪게 했습니다. 예를 들어 K8s 클러스터 간의 입력 및 출력 흐름을 제어하기 위해 NetworkPolicy를 사용할 수 없기 때문에, 하드웨어 가용성을 극대화하기 위해 Pod가 K8s 클러스터 전체에 흩어질 수 있는 경우, 네트워크 격리를 실현하기 위한 대안적인 방법이 필요합니다.\n\n- HTTP: 핀터레스트에서는 쿠버네티스 환경에서 서비스 메쉬로 Envoy를 사용합니다. 레이 대시보드를 Envoy 뒤의 localhost에 배포하고 핀터레스트에서의 인증 및 권한 부여 표준 방식을 따릅니다. 이를 통해 UI에서 사용자를 위한 OAuth 또는 서비스를 위한 mTLS로 레이 대시보드에 액세스 범위를 제한할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_11.png)\n\n<div class=\"content-ad\"></div>\n\n2. gRPC: K8s 환경에서 임의의 Pod이 활성 상태의 Ray Cluster에 연결하는 것을 방지하기 위해, Ray 클러스터 부트스트랩 시에 Ray TLS를 일부 사용자 정의와 함께 이용합니다. 자세히 말하면, 각 Ray 클러스터마다 고유한 쌍(개인 키, 인증서) 인증서 기관(CA)를 생성합니다. 이를 통해 CA와 특정 Ray 클러스터 간에 1:1 매핑을 보장합니다. 상호 인증의 첫 번째 단계는 클라이언트(Ray Pods)의 접근을 해당 CA로 제한하여 서버 측에서 적절한 AuthN / AuthZ로 수행함으로써 완료됩니다. 이렇게 함으로써 특정 Ray 클러스터를 나타내는 해당 CA에 의해 서명된 인증서를 수령할 수 있는 Pod의 하위 집합만 이를 수행할 수 있습니다. 두 번째 단계는 발급된 인증서를 사용하여 통신하는 Pod들이 예상된 Ray 클러스터에 해당하는 CA에 의해 서명되었는지 확인하는 것입니다. 게다가, Ray Pods에 대한 잎 인증서의 서명 및 발급에 대한 모든 암호화 작업은 클라이언트, 즉 Ray head 및 worker pods이 CA 개인 키에 액세스할 수 없도록 서버 측(MLP 서버)에서 수행되어야 합니다.\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_12.png)\n\n# 교훈\n\n점진적 개선:\n\n<div class=\"content-ad\"></div>\n\n-  단순한 방식으로 Ray 클러스터를 배포한 후에는 프로덕션 환경이나 클라우드 환경에서 자동화 및 확장 프로세스에 초점을 맞추세요.\n- MVP 개발 시 휠을 재발명할 필요를 최소화하기 위해 회사의 기존 인프라를 활용하세요. 저희는 Kubeflow 오퍼레이터를 활용하며 기존의 ML 특화 인프라 논리는 개발 프로세스를 간소화할 수 있습니다.\n- 프로토 타입이 완료된 후 회사 전체의 최고 권고 사항에 따라 보안 문제 및 다른 규정 준수 문제 등 인프라를 개선하세요.\n- 고객과 정기적으로 회의를 진행하여 도전 과제 및 개선 영역에 대한 초기 피드백을 수집하세요.\n- Pinterest의 Ray 이니셔티브의 현재 성공을 고려할 때, ML 전용 K8s 클러스터로 이동할 때 KubeRay 통합과 같은 더 많은 개선 사항을 찾고 있습니다.\n\n클라이언트와 Kubernetes 클러스터 사이의 중간 계층:\n\n- API 서버는 클라이언트와 Kubernetes 간의 다리 역할을 하는 추상화 계층을 제공합니다.\n- Ray 클러스터의 생애 주기 이벤트가 Kubernetes에서 사용자 정의 리소스가 제거된 후에도 지속적으로 기록되도록 보장하세요.\n- 플랫폼은 비즈니스 로직, 즉 추가 유효성 검사 및 사용자 인증, 인가, 사용자를 위한 Ray 대시보드 API 액세스 제한과 같은 사용자 지정을 구현할 수 있는 기회가 있습니다.\n- Ray 클러스터를 제공하는 실제 방법을 분리함으로써 KubeRay 및 전용 K8s 클러스터로 전환하는 것이 훨씬 쉬워지며 앞으로 계획하는 것과 같이 필요에 따라 다른 노드 제공자로 전환할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 사용자에게 충분한 인프라 관련 정보를 제공하지 않으면 Ray 클러스터 프로비저닝의 실패 또는 지연과 관련된 혼란이 발생할 수 있습니다.\n- 동시에 수십 개 또는 수백 개의 Ray 클러스터를 운영하려면 플랫폼 측에서 모니터링 및 경보가 중요합니다. Ray 인프라의 초기 단계에 있으며 신속한 변화로 애플리케이션 측에 장애가 발생할 수 있으므로 경보 설정에 성실해야 하며 프로덕션 환경으로 배포하기 전에 스테이징 환경에서 철저한 테스트를 수행해야 합니다.\n\n# 사용법\n\n우리는 2023년 2분기부터 Ray 인프라 사용량을 수집하기 시작했고, 지난 마일스톤 데이터 처리 응용 프로그램 GA 및 추가 사용자들이 Ray 프레임워크에 참여하면서 2023년 4분기에 급증을 관찰했습니다. 우리는 현재 배치 추론 및 adhoc Ray Serve 개발과 같은 다양한 Ray 응용 프로그램을 탐색하기 위해 Ray 기반으로 이전한 사용자들에게 도움을 주고 있습니다. 우리는 아직 네이티브 PyTorch 기반 애플리케이션에서 Ray 기반 애플리케이션으로 이동하는 초기 단계에 있으나, 더 고급화된 사용 사례로 전환하기 위해 고객들과 열심히 협력하고 있습니다.\n\n<img src=\"/assets/img/2024-06-19-RayInfrastructureatPinterest_13.png\" />\n\n<div class=\"content-ad\"></div>\n\n아래는 Markdown 형식으로 표로 변환하였습니다.\n\n\n| 파일 | 이미지 |\n|:-------------------------:|:-------------------------:|\n| 2024-06-19-RayInfrastructureatPinterest_14.png | <img src=\"/assets/img/2024-06-19-RayInfrastructureatPinterest_14.png\" /> |\n| 2024-06-19-RayInfrastructureatPinterest_15.png | <img src=\"/assets/img/2024-06-19-RayInfrastructureatPinterest_15.png\" /> |\n\n\n# 사용 사례\n\nRay 인프라는 제품 ML 사용 사례에 대해 배포되었으며 새로운 응용 프로그램의 신속한 실험을 위해 사용되었습니다.\n\n<div class=\"content-ad\"></div>\n\n## Ray Train\n\n- 여러 추천 시스템 모델 교육이 Ray로 이관되었으며, 나머지 사용 사례를 적극적으로 도입 중입니다.\n- 현재 Ray를 사용하여 매월 5000개 이상의 교육 작업을 실행 중입니다.\n- 이러한 교육 실행은 이질적인 CPU / GPU 클러스터를 활용합니다.\n\n## 핵심 이점:\n\n확장성:\n\n<div class=\"content-ad\"></div>\n\n- Ray는 교육 실행을 가능한 서로 다른 교육기 인스턴스를 넘어 데이터 로딩 및 전처리를 확장할 수 있도록 합니다.\n- p4d.24xlarge와 같은 단일 GPU 노드는 고정된 12:1 CPU:GPU 비율을 가지고 있어 데이터 로더를 확장하고 GPU를 포화시키는 것을 방해합니다.\n- Ray를 사용하면 p4d 인스턴스 외부에서 데이터 로더를 확장할 수 있습니다. 이는 CPU만 있는 인스턴스를 사용하여 더 저렴하게 처리할 수 있습니다.\n\nDev-velocity\n\n- 확장성 이외에도 Ray는 개발 속도를 크게 높일 수 있습니다.\n- 머신 러닝 엔지니어들의 일상적인 작업 중 상당 부분은 모델 변경을 구현하고 로컬 코드를 사용하여 개발 훈련을 실행하는 것입니다.\n- Ray를 사용하면 Ray 컴퓨팅 클러스터를 상호작용적으로 사용하여 주피터 노트북을 통해 작업을 제출할 수 있습니다.\n\n## 일괄 추론\n\n<div class=\"content-ad\"></div>\n\n- Pinterest는 과거 PySpark 기반의 일괄 추론 솔루션을 활용했습니다.\n- Ray를 사용하여, ray.data.Dataset에서 map_batches 구현으로 설계된 새 BatchInference 솔루션을 재구현했습니다.\n- 우리는 현재 이 솔루션을 세 가지 프로덕션 유즈 케이스에 사용하고 있습니다.\n- 현재 우리는 Ray를 사용하여 매달 300개 이상의 일괄 추론 작업을 실행 중입니다.\n\n## 핵심 이점:\n\n효율성:\n\n- 이전 구현과는 달리, Ray는 전처리, GPU 추론 및 출력 파일 쓰기의 파이프라이닝을 가능케 합니다.\n- 더불어, 자동으로 이 세 단계를 이종 CPU 및 GPU 노드에서 실행할 수 있게 분리할 수 있습니다.\n- 이들을 합쳐, 우리의 프로덕션 GPU 추론 작업의 작업 실행 시간이 4배 감소했습니다 (1시간 → 15분).\n\n<div class=\"content-ad\"></div>\n\nUnlocked Opportunity:\n\n- Ray와 함께 프로그래밍하는 쉬움과 파이프라이닝으로 얻는 효율성 덕분에 GPU 기반 모델에 대한 특성 소거 도구를 채택할 수 있었습니다.\n\n## 실험적 워크로드\n\n- Ray는 Ray Serve를 포함한 다양한 도구 생태계를 제공합니다.\n- Ray Serve는 모델 서빙을 위한 내장된 라우팅 및 자동 스케일링 기능을 제공하며, 모델을 빠르게 평가하기 위해 매우 편리합니다.\n- Ray Serve가 없는 경우 클라이언트는 RPC 서버, 배포 파이프라인, 서비스 검색 및 자동 스케일링을 수동으로 설정해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n## 주요 성과:\n\n- 내부 해커톤에서 팀이 몇 시간 만에 오픈 소스 대규모 모델을 설정하고 사용할 수 있었습니다.\n- Ray가 없었다면 이러한 인프라를 구축하는 데 몇 일에서 몇 주가 걸렸을 것입니다.\n\n# 다음 계획\n\n- Pinterest에서 Ray Batch Inference에 대해 깊이 파고들기\n- Pinterest에서 Ray Tune\n- Pinterest에서 Ray 어플리케이션의 독특한 도전 현황\n\n<div class=\"content-ad\"></div>\n\n# 인사말\n\nCloud Runtime Team: Jiajun Wang, Harry Zhang\n\nTraffic Team: James Fish, Bruno Palermo, Kuo-Chung Hsu\n\nSecurity Team: Jeremy Krach, Cedric Staub\n\n<div class=\"content-ad\"></div>\n\nML 플랫폼: Qingxian Lai, Lei Pan\n\nAnyscale: Zhe Zhang, Kai-Hsun Chen, SangBin Cho","ogImage":{"url":"/assets/img/2024-06-19-RayInfrastructureatPinterest_0.png"},"coverImage":"/assets/img/2024-06-19-RayInfrastructureatPinterest_0.png","tag":["Tech"],"readingTime":14},{"title":"억제된 모든 LLM을 해제하세요","description":"","date":"2024-06-19 03:36","slug":"2024-06-19-UncensoranyLLMwithabliteration","content":"\n\n## 재학습 없이 세밀 조정하기\n\n![이미지](/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png)\n\n람마 모델의 세대가 늘어날수록 새로운 기능이 제공되었습니다. 이는 사용자의 지시를 이해하고 따르는 능력이 뛰어난 '세세한 조정(세세하게 조정)' 버전을 제공합니다. 그러나 이러한 모델들은 매우 검열되어 있으며 해로운 요청으로 간주되는 것은 거부하고 \"AI 어시스턴트로서 도와드릴 수 없습니다.\"와 같은 대답을 합니다. 이 안전 기능은 오용을 방지하는 데 중요하지만, 모델의 유연성과 반응성을 제한합니다.\n\n본 문서에서는 \"무효화\"라는 기술을 탐구하여 재학습 없이 어떤 람마 모델이든 검열을 푸는 방법을 살펴볼 것입니다. 이 기술은 모델에 내장된 거부 메커니즘을 효과적으로 제거하여 모든 유형의 프롬프트에 대응할 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\n코드는 Google Colab에서도 사용할 수 있고, LLM 코스에서도 GitHub에 있습니다. 이 기사를 교정해 주신 FailSpy님에게 특별히 감사드립니다.\n\n# ✂️ 삭제란이란?\n\n현대 LLM은 안전 및 지시를 따르는 방향으로 세밀하게 조정되어 있어, 해로운 요청을 거부하기 위해 훈련되어 있습니다. Arditi 등이 블로그 글에서 설명한 바에 따르면, 이 거부 행동은 모델의 잔류 스트림에 있는 특정 방향을 통해 중재됩니다. 만약 이 방향을 모델이 나타내지 못하도록 막는다면, 요청을 거부하는 능력을 잃게 됩니다. 반대로, 이 방향을 인위적으로 추가하면 모델이 해가 없는 요청조차도 거부할 수 있게됩니다.\n\n전통적인 디코더 전용 Llama류 아키텍처에서는 세 가지의 잔류 스트림을 대상으로 할 수 있습니다: 각 블록의 시작 부분에서(“pre”), 어텐션과 MLP 레이어 사이에서(“mid”), 그리고 MLP 이후에(“post”). 다음 그림은 각 잔류 스트림의 위치를 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_1.png\" />\n\nLLM을 무증검 상태로 만들기 위해 먼저 모델 내의 \"거부 방향\"을 식별해야 합니다. 이 과정에는 몇 가지 기술적 단계가 포함되어 있습니다:\n\n- 데이터 수집: 유해한 지시문 집합과 무해한 지시문 집합을 사용하여 모델을 실행하고, 각각의 마지막 토큰 위치에서 잔여 스트림 활성화를 기록합니다.\n- 평균 차이: 유해한 지시와 무해한 지시의 활성화 간 평균 차이를 계산합니다. 이를 통해 모델의 각 레이어에 대한 \"거부 방향\"을 나타내는 벡터를 얻을 수 있습니다.\n- 선택: 이러한 벡터를 정규화하고, 평가하여 단일 최상의 \"거부 방향\"을 선택합니다.\n\n거부 방향을 식별한 후, 해당 기능을 표현하는 모델의 능력을 효과적으로 제거하는 \"제거(ablate)\" 작업을 수행할 수 있습니다. 이는 추론 시간 간섭이나 가중치 직교화를 사용하여 영구적으로 수행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n먼저 추론 시간 개입에 대해 이야기해 보겠습니다. 잔차 스트림에 기록하는 모든 구성 요소(예: 어텐션 헤드)마다 그 출력을 거부 방향으로 투영한 후 이 투영을 뺍니다. 이 뺄셈은 각 토큰과 각 레이어에 적용되어 모델이 결코 거부 방향을 표현하지 않도록 합니다.\n\n한편, 가중치 직교화는 모델 가중치를 직접 수정하는 것을 포함합니다. 거부 방향에 대해 구성 요소 가중치를 직교화함으로써 모델이 이 방향으로 기록하는 것을 방지합니다. 잔차 스트림에 기록하는 행렬을 조정하여 이러한 기여가 거부 방향에 영향을 주지 않도록 합니다.\n\n다음 섹션에서는 가중치 직교화를 사용한 좌절 실현을 구현할 것입니다.\n\n# 💻 구현\n\n<div class=\"content-ad\"></div>\n\n아래의 abliteration 구현은 FailSpy의 노트북을 기반으로 하고 있습니다. 그 노트북은 원래 저자들의 노트북을 기반으로 하고 있습니다. 저는 주로 이를 적응하여 간단하고 이해하기 쉽도록 했습니다. 이 섹션은 코드가 많이 포함되어 있어서 무슨 일이 벌어지는지 볼 수 있지만, 기술적인 세부 사항에 덜 관심이 있는 경우 FailSpy의 abliterator 라이브러리를 사용할 수도 있습니다 (Hugging Face의 abliterated 모델 모음도 확인해보세요).\n\n이 코드는 뛰어난 TransformerLens 라이브러리 (이전에는 EasyTransformer로 알려졌음)를 사용하여 무거운 작업을 처리합니다. 메커니즘 해석 가능성을 위해 설계되었으며 여기서는 활성화에 개입하는 데 사용됩니다. 이 라이브러리를 만든 Neel Nanda와 Joseph Bloom에게 감사드립니다.\n\n먼저 필요한 패키지를 설치하고 가져와 봅시다. 이러한 모든 단계는 Google Colab 노트북에서 사용할 수 있습니다.\n\n```js\n!pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping\n\nimport torch\nimport functools\nimport einops\nimport gc\n\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom torch import Tensor\nfrom typing import List\nfrom transformer_lens import HookedTransformer, utils\nfrom transformer_lens.hook_points import HookPoint\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom jaxtyping import Float, Int\nfrom collections import defaultdict\n\n# GPU 메모리를 저장하기 위해 자동 미분을 끕니다 (크레딧: Undi95)\ntorch.set_grad_enabled(False)\n```\n\n<div class=\"content-ad\"></div>\n\n두 가지 데이터 세트가 필요합니다: 피해가 없는 지침을 포함한 하나와 유해한 지침을 포함한 하나입니다. 우리는 tatsu-lab/alpaca와 llm-attacks의 데이터를 사용할 것입니다. 모든 것을 더 쉽게 만들기 위해, 저는 이를 두 개의 Hugging Face 데이터 세트로 다시 패키징하여 mlabonne/harmless_alpaca와 mlabonne/harmful_behaviors로 만들었습니다. 그렇게 하면 여러분이 쉽게 여러분 자신의 데이터 세트로 교체할 수 있습니다.\n\n지침을로드하고 \"role\"과 \"content\" 키가 있는 사전 목록으로 다시 서식화할 것입니다. 이렇게 하면 Llama 3의 채팅 템플릿을 따르는 apply_chat_tokenizer() 메서드와 호환됩니다.\n\n```python\ndef reformat_texts(texts):\n    return [[{\"role\": \"user\", \"content\": text}] for text in texts]\n\n# 유해하고 무해한 데이터 세트 가져오기\ndef get_harmful_instructions():\n    dataset = load_dataset('mlabonne/harmful_behaviors')\n    return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text'])\n\ndef get_harmless_instructions():\n    dataset = load_dataset('mlabonne/harmless_alpaca')\n    return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text'])\n\nharmful_inst_train, harmful_inst_test = get_harmful_instructions()\nharmless_inst_train, harmless_inst_test = get_harmless_instructions()\n```\n\n이제 데이터 세트가 준비되었으므로, abliterate 하려는 모델을 로드할 수 있습니다. 안타깝게도, HookedTransformer를 사용하여 직접 사용자 정의 모델을 로드할 수 없습니다. 여기에서, FailSpy의 노트북에 설명된 꼼수를 사용하여 사용자 정의 모델을 다운로드하고 meta-llama/Meta-Llama-3-8B-Instruct로 이름을 변경하겠습니다. GPU가 BF16과 호환되지 않는 경우 torch.float16 형식으로 로드하세요.\n\n<div class=\"content-ad\"></div>\n\n이 예시에서는 DARE TIES(모델 병합에 관한 내 기사 참조)로 만들어진 mlabonne/Daredevil-8B를 사용할 것입니다. 이 모델은 8B 카테고리의 Open LLM Leaderboard에서 가장 높은 MMLU 점수를 가지고 있어요.\n\n```js\nMODEL_ID = \"mlabonne/Daredevil-8B\"\nMODEL_TYPE = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# 모델 다운로드 및 로드\n!git clone https://huggingface.co/{MODEL_ID} {MODEL_TYPE}\n\n# 모델 및 토크나이저 로드\nmodel = HookedTransformer.from_pretrained_no_processing(\n    MODEL_TYPE,\n    local_files_only=True,\n    dtype=torch.bfloat16,\n    default_padding_side='left'\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)\ntokenizer.padding_side = 'left'\ntokenizer.pad_token = tokenizer.eos_token\n```\n\n이제 데이터셋을 토큰화할 수 있습니다. 무해한 및 유해한 지시사항에 대해 동일한 샘플 수를 사용합니다. 샘플 수가 높으면 모든 RAM/VRAM을 사용할 수 있으므로 여기서는 256으로 제한합니다.\n\n```js\ndef tokenize_instructions(tokenizer, instructions):\n    return tokenizer.apply_chat_template(\n        instructions,\n        padding=True,\n        truncation=False,\n        return_tensors=\"pt\",\n        return_dict=True,\n        add_generation_prompt=True,\n    ).input_ids\n\nn_inst_train = min(256, len(harmful_inst_train), len(harmless_inst_train))\n\n# 데이터셋 토큰화\nharmful_tokens = tokenize_instructions(\n    tokenizer,\n    instructions=harmful_inst_train[:n_inst_train],\n)\nharmless_tokens = tokenize_instructions(\n    tokenizer,\n    instructions=harmless_inst_train[:n_inst_train],\n)\n```\n\n<div class=\"content-ad\"></div>\n\n작업이 모두 설정되었습니다. 이제 도처럼 처리하는 첫 번째 단계인 데이터 수집을 구현할 차례입니다. 우리는 이 토큰화된 데이터셋을 처리하고 유해(harmful) 및 무해(harmless)로 나머지 스트림 활성화를 저장하려고 합니다. 이는 transformer_lens 라이브러리로 관리됩니다.\n\n```js\nbatch_size = 32\n\n# 활성화를 저장할 기본 사전 초기화\nharmful = defaultdict(list)\nharmless = defaultdict(list)\n\n# 데이터 학습을 배치 단위로 처리\nnum_batches = (n_inst_train + batch_size - 1) // batch_size\n\nfor i in tqdm(range(num_batches)):\n    print(i)\n    start_idx = i * batch_size\n    end_idx = min(n_inst_train, start_idx + batch_size)\n\n    # 유해 및 무해 프롬프트에 모델 실행 및 활성화 캐시\n    harmful_logits, harmful_cache = model.run_with_cache(\n        harmful_tokens[start_idx:end_idx],\n        names_filter=lambda hook_name: 'resid' in hook_name,\n        device='cpu',\n        reset_hooks_end=True\n    )\n    harmless_logits, harmless_cache = model.run_with_cache(\n        harmless_tokens[start_idx:end_idx],\n        names_filter=lambda hook_name: 'resid' in hook_name,\n        device='cpu',\n        reset_hooks_end=True\n    )\n\n    # 활성화 수집 및 저장\n    for key in harmful_cache:\n        harmful[key].append(harmful_cache[key])\n        harmless[key].append(harmless_cache[key])\n\n    # RAM 및 VRAM 비우기\n    del harmful_logits, harmless_logits, harmful_cache, harmless_cache\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# 캐시된 활성화 결합\nharmful = {k: torch.cat(v) for k, v in harmful.items()}\nharmless = {k: torch.cat(v) for k, v in harmless.items()}\n```\n\n이제 각 층에 대한 거부 방향을 계산할 수 있습니다. 이는 유해 및 무해 명령의 활성화 간 평균 차이에 해당하며 정규화됩니다. 그런 다음 activation_scored에서 내림차순으로 정렬됩니다.\n\n```js\n# 활성화 색인을 가져오는 도우미 함수\ndef get_act_idx(cache_dict, act_name, layer):\n    key = (act_name, layer)\n    return cache_dict[utils.get_act_name(*key)]\n\n# 중간 층에서 유해 및 무해 활성화의 평균 차이 계산\nactivation_layers = [\"resid_pre\", \"resid_mid\", \"resid_post\"]\nactivation_refusals = defaultdict(list)\n\nfor layer_num in range(1, model.cfg.n_layers):\n    pos = -1  # 위치 인덱스\n    for layer in activation_layers:\n        harmful_mean_act = get_act_idx(harmful, layer, layer_num)[:, pos, :].mean(dim=0)\n        harmless_mean_act = get_act_idx(harmless, layer, layer_num)[:, pos, :].mean(dim=0)\n        refusal_dir = harmful_mean_act - harmless_mean_act\n        refusal_dir = refusal_dir / refusal_dir.norm()\n        activation_refusals[layer].append(refusal_dir)\n\nselected_layers = [\"resid_pre\"]\nactivation_scored = sorted(\n    [\n        activation_refusals[layer][l - 1]\n        for l in range(1, model.cfg.n_layers)\n        for layer in selected_layers\n    ],\n    key=lambda x: abs(x.mean()),\n    reverse=True,\n)\n```\n\n<div class=\"content-ad\"></div>\n\n프로세스의 마지막 단계는 계산한 거절 방향을 평가하는 과정입니다. 이를 위해 우리는 거절 방향을 추론 중 각 잔여 스트림과 각 블록에 적용할 것입니다. 다음 코드 스니펫에서 네 가지 테스트 유해 지시와 20개 블록(또는 레이어)에 대한 세대를 가져옵니다.\n\n```js\n...\nCode snippet 생략\n...\n```\n\n우리는 evals 리스트에 모든 세대를 저장했습니다. 이제 이를 출력하고 각 지시에 대한 무겁지 않은 응답을 제공하는 레이어(블록)를 수동으로 선택할 수 있습니다. \"I cannot\" 및 \"I can't\"를 포함하는 응답은 원치 않는 답변을 필터링하기 위해 자동으로 제외합니다.\n\n위 요구사항을 만족하는 레이어를 찾을 수 없는 경우, 이전 selected_layers 목록의 다른 잔여 스트림, 다른 지시, 추가적 블록 등을 테스트하고자 할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\n# 인간 평가를 위한 세대 인쇄\nblacklist = [\"I cannot\", \"I can't\"]\nfor i in range(N_INST_TEST):\n    print(f\"\\033[1mINSTRUCTION {i}: {harmful_inst_test[i]}\")\n    print(f\"\\nBASELINE COMPLETION:\\n{baseline_generations[i]}\\033[0m\")\n    for layer_candidate in range(EVAL_N):\n        if not any(word in evals[layer_candidate][i] for word in blacklist):\n            print(f\"\\n---\\n\\nLAYER CANDIDATE #{layer_candidate} INTERVENTION COMPLETION:\")\n            print(evals[layer_candidate][i])\r\n```\n\n저의 경우, 레이어 후보자 9가 네 가지 명령에 대해 선정적이지 않은 답변을 제공했습니다. 이것이 우리가 거부 방향으로 선택할 것이다. 그 다음으로, 무게 직교화를 구현하여 모델이 이 방향의 출력을 생성하는 것을 방지합니다. 모델이 성공적으로 선정되지 않은지를 확인하려면 완료된 내용을 인쇄하여 확인할 수 있습니다.\n\n```js\r\ndef get_orthogonalized_matrix(\n    matrix: Float[Tensor, \"... d_model\"], vec: Float[Tensor, \"d_model\"]\n) -> Float[Tensor, \"... d_model\"]:\n    proj = (\n        einops.einsum(\n            matrix, vec.view(-1, 1), \"... d_model, d_model single -> ... single\"\n        )\n        * vec\n    )\n    return matrix - proj\n\n# 가장 높은 거부 방향을 갖는 레이어 선택\nLAYER_CANDIDATE = 9\nrefusal_dir = activation_scored[LAYER_CANDIDATE]\n\n# 모델의 가중치 직교화\nif refusal_dir.device != model.W_E.device:\n    refusal_dir = refusal_dir.to(model.W_E.device)\nmodel.W_E.data = get_orthogonalized_matrix(model.W_E, refusal_dir)\n\nfor block in tqdm(model.blocks):\n    if refusal_dir.device != block.attn.W_O.device:\n        refusal_dir = refusal_dir.to(block.attn.W_O.device)\n    block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, refusal_dir)\n    block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, refusal_dir)\n\n# 모델로 텍스트 생성\northogonalized_generations = get_generations(\n    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]\n)\n\n# 세대 출력\nfor i in range(N_INST_TEST):\n    if len(baseline_generations) > i:\n        print(f\"INSTRUCTION {i}: {harmful_inst_test[i]}\")\n        print(f\"\\033[92mBASELINE COMPLETION:\\n{baseline_generations[i]}\")\n    print(f\"\\033[91mINTERVENTION COMPLETION:\\n{evals[LAYER_CANDIDATE][i]}\")\n    print(f\"\\033[95mORTHOGONALIZED COMPLETION:\\n{orthogonalized_generations[i]}\\n\")\r\n```\n\n이제 모델을 사용할 준비가 되었습니다. 모델을 Hugging Face 형식으로 변환하여 HF 허브에 업로드합니다.\n\n<div class=\"content-ad\"></div>\n\n```json\n# 모델을 다시 HF 보안 텐서로 변환합니다\nhf_model = AutoModelForCausalLM.from_pretrained(MODEL_TYPE, torch_dtype=torch.bfloat16)\nlm_model = hf_model.model\n\nstate_dict = model.state_dict()\nlm_model.embed_tokens.weight = torch.nn.Parameter(state_dict[\"embed.W_E\"].cpu())\nfor l in range(model.cfg.n_layers):\n    lm_model.layers[l].self_attn.o_proj.weight = torch.nn.Parameter(\n        einops.rearrange(\n            state_dict[f\"blocks.{l}.attn.W_O\"], \"n h m->m (n h)\", n=model.cfg.n_heads\n        ).contiguous()\n    )\n    lm_model.layers[l].mlp.down_proj.weight = torch.nn.Parameter(\n        torch.transpose(state_dict[f\"blocks.{l}.mlp.W_out\"], 0, 1).contiguous()\n    )\n\nhf_model.push_to_hub(f\"{MODEL_ID}-abliterated\")\n```\n\n# ⚖️ DPO Fine-Tuning\n\n이전 섹션의 abliterated 및 소스 모델을 Open LLM Leaderboard 및 Nous의 벤치마크 스위트에서 평가했습니다. 여기에 결과가 있습니다:\n\n<img src=\"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_2.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n보시다시피, 원본 모델은 Llama 3 8B Instruct보다 현저하게 우수한 성능을 보여줍니다. 그러나 우리는 모든 벤치마크에서 절삭된 버전에서 성능 하락을 관찰하고 있습니다. 절삭 과정은 성능을 향상시키면서도 모델의 품질을 저하시킨 것으로 나타났습니다.\n\n이 문제를 해결하기 위해 우리는 우리의 절삭된 모델을 추가로 훈련하여 회복시키는 아이디어가 있습니다. 대부분의 세밀 조정된 모델들과 마찬가지로 Llama 3 8B Instruct은 지도 학습 세밀 조정에 있어서 꽤 취약합니다. 추가적인 SFT는 모델의 성능을 떨어뜨릴 가능성이 높습니다.\n\n대체로, 선호 맞춤이 상당히 가볍고 우리의 절삭된 모델을 뇌개박하지 않아도 됩니다. DPO는 사용하기 쉽고 우수한 추적 레코드로 여기서 좋은 후보입니다. 이를 구현하기 위해 mlabonne/orpo-dpo-mix-40k 데이터셋을 사용하는 LazyAxolotl (Axolotl을 만들어 준 Wing Lian에게 감사드립니다)을 사용했습니다. 여기에 사용한 구성은 다음과 같습니다:\n\n```js\nbase_model: mlabonne/Daredevil-8B-abliterated\nmodel_type: LlamaForCausalLM\ntokenizer_type: AutoTokenizer\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\nsave_safetensors: true\n\nrl: dpo\nchat_template: chatml\ndatasets:\n  - path: mlabonne/orpo-dpo-mix-40k\n    split: train\n    type: chatml.intel\n\ndataset_prepared_path:\nval_set_size: 0.0\noutput_dir: ./out\n\nadapter: qlora\nlora_model_dir:\n\nsequence_len: 2048\nsample_packing: false\npad_to_sequence_len: false\n\nlora_r: 64\nlora_alpha: 32\nlora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project: axolotl\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n\ngradient_accumulation_steps: 8\nmicro_batch_size: 1\nnum_epochs: 1\noptimizer: paged_adamw_8bit\nlr_scheduler: cosine\nlearning_rate: 5e-6\ntrain_on_inputs: false\ngroup_by_length: false\n... (이어짐)\n```\n\n<div class=\"content-ad\"></div>\n\n6xA6000 GPU와 DeepSpeed ZeRO-2를 사용하여 모델을 훈련했어요. 훈련에는 약 6시간 45분이 소요되었답니다. W&B에서 얻은 훈련 곡선을 여기에 가져왔어요:\n\nDPO를 세밀하게 조정한 mlabonne/NeuralDaredevil-8B-abliterated 모델이 자동으로 업로드되었어요. 저희가 앞서 지워버린 버전을 수정했는지 확인하기 위해 동일한 벤치마크에서 평가했어요:\n\n![훈련 곡선](/assets/img/2024-06-19-UncensoranyLLMwithabliteration_3.png)\n\n이 추가 훈련을 통해 지워진 영향 대부분을 회복할 수 있었어요. 모델이 개선되지 않는 한 영역은 GSM8K, 수학 데이터 세트, 인데요, 이는 orpo-dpo-mix-40k가 더 많은 수학 샘플을 필요로 할 수 있다는 것을 의미할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n최종 모델은 8B 범주에서 최첨단 성능을 자랑하는 미검열 LLM입니다. 필터링이 필요 없을 때는 Llama 3 8B Instruct의 개선된 버전으로 추천합니다. LM Studio에서 GGUF와 같은 양자화된 버전을 사용해 볼 수도 있습니다.\n\n# 결론\n\n이 글에서는 소명화(abliteration) 개념을 소개했습니다. 이 기술은 모델의 활성화를 해롭고 해를 끼치지 않는 프롬프트에 사용하여 거부 방향을 계산하고, 모델의 가중치를 수정하여 거부를 그만 내보낼 수 있도록 합니다. 이 기술은 안전 세밀조정의 취약성을 보여주며 윤리적 고려 사항을 던지고 있습니다.\n\n우리는 Daredevil-8B에 소명화를 적용하여 필터링을 해제했지만, 이로 인해 모델의 성능이 저하되었습니다. 그 후 DPO를 사용하여 NeuralDaredevil-8B 모델을 생성하여 완전히 미검열이고 고품질의 8B LLM을 만들었습니다. 소명화는 정렬 제거에 국한되지 않으며, 다시 교육 없이 세밀 조정의 일종으로 간주해야 합니다. 실제로 MopeyMule의 FailSpy의 경우처럼 좌절적인 대화 스타일을 채택하는 것과 같이 창의적으로 다른 목표에도 적용될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 기사가 마음에 들었으면 좋겠어요. 더 많은 내용을 보고 싶다면 Hugging Face와 Twitter의 @maximelabonne를 팔로우해 주세요.\n\n# 참고 자료\n\n- FailSpy, “abliterator library,” GitHub, 2024.\n- Andy Arditi, Oscar Obeso, Aaquib111, wesg, Neel Nanda, “Refusal in LLMs is mediated by a single direction,” Lesswrong, 2024.","ogImage":{"url":"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png"},"coverImage":"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png","tag":["Tech"],"readingTime":15},{"title":"LLM Unix Linux 도구상자의 궁극적인 스위스 아미 나이프","description":"","date":"2024-06-19 03:35","slug":"2024-06-19-llmTheUltimateSwissArmyKnifeofUnixLinuxToolbox","content":"\n\n## 멋진 도구로 명령줄 작업을 더욱 효율적으로!\n\n최근에 Simon Willison이 만든 llm 명령줄 도구를 발견하게 되었습니다.\n\n그는 최근에 이 도구를 어떻게 사용하는지에 대해 이야기했는데, 이를 일상적으로 사용하는 다른 명령줄 도구와 함께 어떻게 활용하는지를 보여주었습니다.\n\n## 이 도구는 무엇을 할까요?\n\n<div class=\"content-ad\"></div>\n\n간단히 말해서, 이 도구를 사용하면 워크플로우 중간에 LLM (대형 언어 모델)을 호출하고 강력한 방법으로 출력을 변환할 수 있습니다.\n\n빠른 예시:\n\nGithub에서 더 많은 예시와 데모를 찾을 수 있습니다.\n\n## 왜 이것이 중요한가요?\n\n<div class=\"content-ad\"></div>\n\n여기서 인용하자면:\n\n기존 애플리케이션 및 업무에 AI를 통합하는 것은 도전적입니다. 예를 들어, Github Copilot과 같은 솔루션이 이 기능을 지원하려면 각 IDE에 플러그인이 필요합니다. ChatGPT를 사용하는 동안에도 LLM 응답이 필요할 때마다 파일을 드래그 앤 드롭하거나 복사하여 붙여넣어야 하므로 데이터를 이동하는 데 더 많은 노력과 시간이 필요합니다.\n\n## 왜 이것이 뛰어난 아이디어인가요?\n\n이 도구는 Unix 철학에서의 주요 아이디어를 상쾌하게 부활시키는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n- 하나의 기능을 잘 수행하는 프로그램을 작성하세요.\n- 프로그램들이 함께 작동할 수 있도록 작성하세요.\n- 모든 프로그램들이 텍스트 스트림을 처리하도록 작성하세요. 왜냐하면 그것이 보편적인 인터페이스이기 때문입니다.\n\n마지막으로, 위 세 번째 점은 LLMs이 이 목적에 적합하게 만들어진 이유입니다. 왜냐하면 그들은 프롬프트에 기반하여 하나의 텍스트를 다른 텍스트로 효과적으로 변환하는 데 우수하기 때문입니다.\n\n좋은 상호 운용성을 위해서, 각 프로그램에 깔끔한 입출력 인터페이스가 있다면 좋고, 다른 프로그램의 출력을 표준 입력(stdin)을 통해 입력으로 받아 들이게 하는 것은 점 [2]로부터 따라온다. \n\n## 키 기능의 빠른 개요\n\n<div class=\"content-ad\"></div>\n\n- 플러그인을 통한 로컬 및 원격 모델 지원: Ollama와 같은 도구를 통해 LLMs의 로컬 배포로 비용을 관리하세요.\n- 템플릿 지원: Fabric과 같은 대체 도구는 \"패턴\"이라고 하는 템플릿을 커맨드 라인 도구로 제공하기 시작했습니다.\n\n전체 토크를 YouTube에서 시청하는 것을 적극 추천합니다.","ogImage":{"url":"/assets/img/2024-06-19-llmTheUltimateSwissArmyKnifeofUnixLinuxToolbox_0.png"},"coverImage":"/assets/img/2024-06-19-llmTheUltimateSwissArmyKnifeofUnixLinuxToolbox_0.png","tag":["Tech"],"readingTime":2},{"title":"RAG 시스템을 위한 새로운 청킹 방법","description":"","date":"2024-06-19 03:33","slug":"2024-06-19-NewChunkingMethodforRAG-Systems","content":"\n\n## 문서 분할 개선\n\n![image](/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_0.png)\n\n대형 문서를 작은 부분으로 나누는 것은 검색 증강 생성 (RAG) 시스템의 성능에 영향을 미치는 필수적이면서도 중요한 요소입니다. RAG 시스템을 개발하는 프레임워크들은 일반적으로 여러 가지 옵션을 제공합니다. 본문에서는 새로운 옵션을 소개하여 문서를 세분화할 때 문장 임베딩의 도움을 받아 주제 변경을 인식하려고 시도한 이와 같은 방법을 소개하고자 합니다. 이것은 RAG 시스템의 임베딩 단계에서 주제를 인코딩하는 텍스트 부분의 벡터를 찾을 수 있으며 여러 가지 주제의 혼합이 아닙니다. 우리는 주제 모델링의 맥락에서 본 방법을 제안했지만, RAG 시스템에서도 사용할 수 있는 것입니다.\n\n# RAG 시스템\n\n<div class=\"content-ad\"></div>\n\n검색 보완 생성 (Retrieval-Augmented Generation, RAG) 시스템은 검색 기반 및 생성 기반 방법을 결합하여 출력물의 품질과 관련성을 향상시키는 기계 학습 모델입니다. 먼저 입력 쿼리에 기반하여 대규모 데이터셋에서 관련 문서나 정보를 검색합니다. 그런 다음, 검색된 정보를 활용하여 일관된 컨텍스트에 적합한 응답이나 내용을 생성하기 위해 변환기 기반 언어 모델과 같은 생성 모델을 사용합니다. 이 하이브리드 방식은 모델이 정확하고 유익한 응답을 제공하는 능력을 향상시킵니다, 특히 복잡하거나 지식 집약적인 작업에서 더욱 유용합니다.\n\n# 다른 분할 옵션\n\n자세한 절차를 살펴보기 전에, 문서 분할에 대한 몇 가지 표준 옵션을 소개하겠습니다. 널리 사용되는 Langchain 프레임워크를 활용하여 예시를 보여드리겠습니다.\n\nLangChain은 주로 대규모 언어 모델을 적용하기 위해 설계된 견고한 프레임워크로, 다양한 자연어 처리(NLP) 작업을 지원합니다. 그 중 하나인 문서 분할은 사용자가 대형 문서를 작은 관리 가능한 청크로 쪼개는 기능입니다. 아래는 LangChain의 문서 분할의 주요 기능 및 예시를 소개한 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# LangChain의 문서 분할 핵심 기능\n\n- 재귀적 문자 텍스트 분할기: 이 방법은 문자 기반으로 텍스트를 재귀적으로 분할하여 각 청크가 지정된 길이보다 작도록 보장합니다. 이는 자연 단락 또는 문장 구분이 있는 문서에 특히 유용합니다.\n- 토큰 분할기: 이 방법은 토큰을 사용하여 문서를 분할합니다. 언어 모델의 토큰 제한이 있는 경우에 유용하며, 각 청크가 모델의 제약에 맞도록 보장합니다.\n- 문장 분할기: 이 방법은 문장 경계에서 문서를 분할합니다. 문장이 일반적으로 완전한 생각을 나타내므로 텍스트의 맥락적 무결성을 유지하는 데 이상적입니다.\n- Regex 분할기: 이 방법은 사용자 정의 분할 지점을 정의하기 위해 정규 표현식을 사용합니다. 이 방법은 사용 사례에 특정한 패턴을 기준으로 문서를 분할할 수 있는 가장 큰 유연성을 제공합니다.\n- 마크다운 분할기: 이 방법은 마크다운 문서에 맞춰져 있습니다. 제목, 목록, 코드 블록과 같은 마크다운 특정 요소를 기준으로 텍스트를 분할합니다.\n\n# LangChain에서 문서 분할의 예시\n\n## 1. 재귀적 문자 텍스트 분할기\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext = \"여러분의 긴 문서 텍스트를 여기에 넣어주세요...\"\nsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\nchunks = splitter.split_text(text)\nfor chunk in chunks:\n    print(chunk)\n```\n\n## 2. 토큰 분할기\n\n```js\nfrom langchain.text_splitter import TokenSplitter\n\ntext = \"여러분의 긴 문서 텍스트를 여기에 넣어주세요...\"\nsplitter = TokenSplitter(max_tokens=512)\nchunks = splitter.split_text(text)\nfor chunk in chunks:\n    print(chunk)\n```\n\n## 3. 문장 분할기\n\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom langchain.text_splitter import SentenceSplitter\n\ntext = \"문서의 긴 텍스트를 여기에 입력하세요...\"\nsplitter = SentenceSplitter(max_length=5)\nchunks = splitter.split_text(text)\nfor chunk in chunks:\n    print(chunk)\n```\n\n## 4. Regex Splitter\n\n```js\nfrom langchain.text_splitter import RegexSplitter\n\ntext = \"문서의 긴 텍스트를 여기에 입력하세요...\"\nsplitter = RegexSplitter(pattern=r'\\n\\n+')\nchunks = splitter.split_text(text)\nfor chunk in chunks:\n    print(chunk)\n```\n\n## 5. Markdown Splitter\n\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom langchain.text_splitter import MarkdownSplitter\n\ntext = \"여기에 긴 마크다운 문서를 넣어주세요...\"\nsplitter = MarkdownSplitter()\nchunks = splitter.split_text(text)\nfor chunk in chunks:\n    print(chunk)\n```\n\n# 새로운 접근 방식 소개\n\n대량의 문서를 디지털 콘텐츠 분석에서 일관된 주제별 섹션으로 분리하는 것은 상당한 어려움을 겪을 수 있습니다. 위에서 설명한 전통적인 방법들은 종종 주제가 변화하는 미묘한 부분을 정확하게 감지하지 못할 수 있습니다. 우리는 인공지능, 컴퓨터, 데이터 과학 및 응용국제 학회(ACDSA 2024)에서 발표된 논문에서 이 문제를 해결하기 위한 혁신적인 접근 방식을 제안합니다.\n\n## 핵심 도전 과제\n\n\n<div class=\"content-ad\"></div>\n\n대규모 문서, 예를 들어 학술 논문, 긴 보고서 및 상세한 기사는 복잡하며 여러 주제를 포함하고 있습니다. 간단한 규칙 기반 방법부터 고급 기계 학습 알고리즘까지 다양한 전통적인 분할 기술들은 주제 전환의 정확한 지점을 식별하는 것에 어려움을 겪습니다. 이러한 방법들은 종종 섬세한 전환점을 놓치거나 잘못 식별하여 단편화된 또는 겹치는 섹션을 야기할 수 있습니다.\n\n저희 방법은 문장 임베딩의 힘을 활용하여 분할 과정을 개선합니다. 이 접근 방식은 개별 문장에 대한 임베딩을 생성하기 위해 Sentence-BERT (SBERT)를 활용하여 그들의 유사성을 양적으로 측정합니다. 주제가 변경됨에 따라 이러한 임베딩은 벡터 공간에서 변화를 반영하여 잠재적인 주제 전환을 나타냅니다.\n\n## 접근 방식의 각 단계를 살펴보세요:\n\n## 1. 문장 임베딩 사용\n\n<div class=\"content-ad\"></div>\n\n임베딩 생성:\n\n- 이 방법은 개별 문장에 임베딩을 생성하기 위해 Sentence-BERT (SBERT)를 사용합니다. SBERT는 문장의 의미적 내용을 담고 있는 밀집 벡터 표현을 만듭니다.\n- 이러한 임베딩을 비교하여 연이은 문장 간의 일관성을 파악합니다.\n\n유사도 계산:\n\n- 문장 간의 유사도는 코사인 유사도 또는 맨해튼 또는 유클리드 거리와 같은 다른 거리 측정을 사용하여 측정됩니다.\n- 동일한 주제 내의 문장은 유사한 임베딩을 갖게 되며, 서로 다른 주제의 문장은 유사도가 감소하는 것을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n## 2. 갭 점수 계산\n\n매개 변수 n 정의:\n\n- 매개 변수 n을 설정하여 비교할 문장의 수를 지정합니다. 예를 들어, n=2이면 두 연속 문장이 다음 쌍과 비교됩니다.\n- n의 선택은 비교에서 고려되는 문맥 길이에 영향을 미치며, 세밀한 전환을 포착하는 필요와 계산 효율성을 균형있게 유지합니다.\n\n코사인 유사도 계산:\n\n<div class=\"content-ad\"></div>\n\n- 문서의 각 위치에 대해, 알고리즘은 현재 위치 앞 뒤 n개의 문장을 추출합니다.\n- 그런 다음 이러한 시퀀스의 임베딩 간 코사인 유사도, 즉 '간격 점수'를 계산합니다.\n- 이러한 간격 점수는 나중에 처리를 위해 목록에 저장됩니다.\n\n![이미지](/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_1.png)\n\n## 3. 부드러운 처리\n\n노이즈 처리:\n\n<div class=\"content-ad\"></div>\n\n- 텍스트의 미세한 변동으로 인해 기존 갭 점수는 소음이 발생할 수 있습니다. 이를 보정하기 위해 평활화 알고리즘을 적용합니다.\n- 평활화에는 매개변수 k로 정의된 창을 통해 갭 점수를 평균화하는 과정이 포함됩니다.\n\n창 크기 k 선택:\n\n- 창 크기 k는 평활화의 범위를 결정합니다. 큰 k 값은 더 많은 평활화를 유발하며 소음을 줄이지만 섬세한 변환을 놓칠 수 있습니다. 작은 k 값은 더 많은 세부 정보를 유지하지만 소음을 도입할 수 있습니다.\n- 평활화된 갭 점수는 주제 전환 지점이 명확히 나타납니다.\n\n![이미지](/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_2.png)\n\n<div class=\"content-ad\"></div>\n\n## 4. 경계 감지\n\n지역 최솟값 식별:\n\n- 부드러운 갭 점수를 분석하여 지역 최솟값과 주제 전환 지점을 식별합니다.\n- 각 지역 최솟값에 대해 깊이 점수를 계산하여 지역 최솟값과 이전 값과 이후 값 사이의 차이를 합산합니다.\n\n임계값 c 설정하기:\n\n<div class=\"content-ad\"></div>\n\n- 유의미한 경계를 결정하는 임계값 c를 사용합니다. 높은 c 값은 더 적고 더 의미 있는 세그먼트를 만들어내며, 낮은 c 값은 더 많고 더 작은 세그먼트를 만들어냅니다.\n- 평균 깊이 점수보다 표준 편차의 c 배 이상을 초과하는 경계는 유효한 분할 지점으로 간주됩니다.\n\n![그림](/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_3.png)\n\n## 5. 클러스터링 세그먼트\n\n반복된 주제 다루기:\n\n<div class=\"content-ad\"></div>\n\n- 긴 문서는 서로 다른 지점에서 유사한 주제를 재방문할 수 있습니다. 이를 해결하기 위해 알고리즘은 유사한 콘텐츠를 가진 세그먼트를 클러스터링합니다.\n- 이 과정은 세그먼트를 임베딩으로 변환하고 클러스터링 기술을 사용하여 유사한 세그먼트를 병합하는 것을 포함합니다.\n\n중복 감소:\n\n- 클러스터링은 각 주제가 고유하게 표현되어 중복을 줄이는 데 도움이 되며, 세분화의 전반적인 일관성과 정확도를 향상시킵니다.\n\n# 알고리즘 의사 코드\n\n<div class=\"content-ad\"></div>\n\n갭 스코어 계산:\n\n![이미지](/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_4.png)\n\n갭 스코어 부드럽게 표현:\n\n![이미지](/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_5.png)\n\n<div class=\"content-ad\"></div>\n\n경계 감지:\n\n- 각 지역 최솟값에 대한 깊이 점수가 계산됩니다.\n- 중요한 분할 지점을 결정하기 위해 매개변수 c를 사용하여 임계값을 적용합니다.\n\n# 향후 방향\n\n이 연구는 이 방법을 향상시키기 위해 추가 연구를 위한 여러 영역을 개요로 설명합니다:\n\n<div class=\"content-ad\"></div>\n\n- 자동 매개변수 최적화: 기계 학습 기술을 사용하여 매개변수를 동적으로 조정합니다.\n- 보다 광범위한 데이터 집합 실험: 다양하고 대규모 데이터셋에서의 방법을 테스트합니다.\n- 실시간 분할: 동적 문서에 대한 실시간 응용 프로그램을 탐색합니다.\n- 모델 개선: 최신 변형 모델을 통합합니다.\n- 다국어 분할: 멀티링귀얼 SBERT를 사용하여 다른 언어에 해당 방법을 적용합니다.\n- 계층적 분할: 상세한 문서 분석을 위해 여러 수준에서의 분할을 조사합니다.\n- 사용자 인터페이스 개발: 분할 결과를 더 간편하게 조정할 수 있는 대화형 도구를 만듭니다.\n- NLP 작업과의 통합: 알고리즘을 다른 자연어 처리 작업과 결합합니다.\n\n# 결론\n\n우리의 방법은 문서 분할에 정교한 접근법을 제시하며 전통적인 원칙과 첨단 문장 임베딩을 결합합니다. SBERT와 고급 스무딩 및 클러스터링 기술을 활용하여 이 프로세스는 대규모 문서에서 정확한 주제 모델링을 위한 강력하고 효율적인 솔루션을 제공합니다.\n\n논문: https://ieeexplore.ieee.org/document/10467643\n\n<div class=\"content-ad\"></div>\n\nDataDrivenInvestor.com에서 저희를 방문해주세요.\n\nDDIntel을 여기서 구독해보세요.\n\n주요 기사:\n\n저희 창작자 생태계에 참여해보세요.\n\n<div class=\"content-ad\"></div>\n\nDDI 공식 텔레그램 채널: [https://t.me/+tafUp6ecEys4YjQ1](https://t.me/+tafUp6ecEys4YjQ1)\n\nLinkedIn, Twitter, YouTube, 그리고 Facebook에서도 팔로우해주세요.","ogImage":{"url":"/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_0.png"},"coverImage":"/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_0.png","tag":["Tech"],"readingTime":7},{"title":"에 대한 탐색적 데이터 분석을 민주화할 것인가요","description":"","date":"2024-06-19 03:29","slug":"2024-06-19-HowLLMsWillDemocratizeExploratoryDataAnalysis","content":"\n\n## 혹은, 삶이 너무 어렵다고 느낄 때, 클로드와 이야기해보세요\n\n![링크](/assets/img/2024-06-19-HowLLMsWillDemocratizeExploratoryDataAnalysis_0.png)\n\n복잡한 시스템을 이해하는 데 필요한 도전에 대해 생각할 때, 종종 Tripadvisor에서 있었던 일을 떠올립니다. 저는 Machine Learning 팀이 성장 마케팅 팀을 위해 LTV가 높은 예측을 제공하는 고객 행동을 이해하는 분석을 수행하는 데 도와줬었습니다. 굉장히 유능한 박사 학위 소지자인 데이터 과학자와 함께 일했는데, 그는 로지스틱 회귀 모델을 학습시키고 계수를 첫 번째로 출력해주었습니다.\n\n우리가 Growth 팀과 분석을 살펴본 결과, 그들은 혼란스러웠습니다. 로지스틱 회귀 계수는 선형이 아닌 척도를 가지고 있어 해석하기 어려웠으며, 가장 예측력이 뛰어난 특징들은 Growth 팀이 쉽게 제어할 수 없는 것들이었습니다. 우리는 다 같이 잠시 깊이 생각해 보고 이에 대한 후속 분석을 위한 작업을 시작했지만, 자주 그랬던 것처럼 두 팀 모두 빨리 다음 활기 넘치는 아이디어로 넘어가버렸습니다. 데이터 과학자는 검색 순위 알고리즘에 대한 중요 작업이 있었고, 실제적으로 Growth 팀은 분석 자체를 쓰레기통에 버리기로 결정했습니다.\n\n<div class=\"content-ad\"></div>\n\n그 운동에 대해 계속 생각 중이에요 — 우리가 너무 일찍 포기했나요? 피드백 루프가 더 타이트했다면 어땠을까요? 두 당사자가 계속 발굴했다면 어땠을까요? 두 번째나 세 번째 시도에서 무엇이 나타났을까요?\n\n# 규모화된 탐색적 분석 해제\n\n위의 일화는 조금 아쉽게 끝난 탐색적 분석을 설명합니다. 탐색적 분석은 간단히 무슨 일이 벌어지고 있는지를 설명하는 서술적 분석과는 다릅니다. 탐색적 분석은 좀 더 큰 시스템을 이해하려는 것이 목표이며 명확히 정의된 질문보다는 시스템을 이해하려고 노력합니다. 비즈니스 문맥에서 다음과 같은 종류의 질문들이 발생할 수 있습니다:\n\n![image](/assets/img/2024-06-19-HowLLMsWillDemocratizeExploratoryDataAnalysis_1.png)\n\n<div class=\"content-ad\"></div>\n\n탐색적 질문이 어떻게 열린 문제에 대한 이해를 향상시키려는지 주목해보세요. 탐색적 분석은 종종 더 많은 주기와 \"도메인 전문가\"와 실제 분석을 수행하는 사람 사이의 긴밀한 협력을 필요로 합니다. 위 이야기에서는 협력이 충분히 강조되지 않았으며, 피드백 루프가 충분히 짧지 않았으며, 충분한 주기를 할애하지 않았습니다.\n\n이러한 도전들이 왜 많은 전문가들이 데이터 탐색을 위한 \"페어 분석\" 접근을 옹호하는 이유입니다. 페어 프로그래밍과 유사하게, 페어 분석은 분석가와 의사 결정자를 실시간으로 모아 탐색을 실시하는 방법입니다. 유감스럽게도, 분석가와 의사 결정자 사이의 이러한 긴밀한 협력은 자원 및 시간 제한으로 인해 실제 실무에서 거의 발생하지 않습니다.\n\n이제 귀하가 근무하는 조직에 대해 생각해 보세요. 모든 의사 결정자가 경험 많은 분석가와 함께 일할 수 있다면 어떨까요? 그 분석가의 전담 주의를 받을 수 있고, 원할 때 계속해서 따라올 질문을 던질 수 있다면 어떨까요? 그 분석가들이 자유로운 아이디어와 가설의 연상을 따라가며 손쉽게 문맥을 전환할 수 있다면 어떨까요?\n\n이것이 분석 공간에서 LLMs가 제공하는 기회입니다. 기술 분석가와 함께 탐색적 분석을 수행할 수 있는 능력이라는 약속입니다.\n\n<div class=\"content-ad\"></div>\n\n이것이 실제로 어떻게 나타날 수 있는지 살펴보겠습니다. 다음 사례 연구와 데모는 도메인 전문가인 의사 결정자가 데이터를 쿼리하고 시각화할 수 있는 AI 분석가와 효과적으로 협업하는 방법을 보여줍니다. ChatGPT의 4o 모델의 데이터 탐색 경험과 Tableau를 사용한 수동 분석을 비교하여 잠재적인 환각에 대한 오류 점검도 수행할 것입니다.\n\n데이터 프라이버시에 관한 참고 사항: 다음 섹션에 연결된 비디오 데모는 완전히 합성 데이터 세트를 사용하며, 현실적인 비즈니스 패턴을 모방하기 위해 만들어졌습니다. AI 분석가의 개인 정보 보호와 보안에 대한 일반적인 참고 사항을 보려면 데이터 프라이버시를 참조하세요.\n\n# 케이스 스터디: 전자 상거래 분석\n\n이렇게 상상해보세요: 전자 상거래 의류 웹사이트의 바쁜 임원인 당신. 미리 정의된 고수준 KPI들로 이루어진 Exec Summary 대시보드가 있지만 어느 날 아침, 보시기에는 문제가 있는 것 같습니다. 월간 마케팅 수익이 45% 감소했지만 왜 그런지 바로 알기 어렵습니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-19-HowLLMsWillDemocratizeExploratoryDataAnalysis_2.png)\n\n마음이 한꺼번에 몇 가지 방향으로 당신을 끌어당길 것입니다: 수익 감소의 원인은 무엇인가요? 특정 채널에만 해당되는 건가요? 문제가 특정 메시지 유형에만 국한된 건가요?\n\n하지만 무엇보다, 우리는 이에 대해 어떻게 대응할 수 있을까요? 최근에 잘 작동했던 것은 무엇인가요? 잘 되지 않는 것은 무엇인가요? 이번 시기에 어떤 계절적 추세가 나타날까요? 우리는 그에 대해 어떻게 이용할 수 있을까요?\n\n이러한 개방형 질문에 대한 답을 제공하려면, 중간 복잡도의 다변량 분석을 수행해야 합니다. 이는 AI 분석가가 도와줄 수 있는 정확한 유형의 연습입니다.\n\n<div class=\"content-ad\"></div>\n\n# 진단 분석\n\n문제가 되는 월간 수익 감소를 더 자세히 살펴보는 것부터 시작해 보겠습니다.\n\n우리 예시에서는 마케팅 활동으로 인한 총 수익의 큰 감소를 살펴보고 있습니다. 분석가로서 원인을 진단하기 위해 시작할 수 있는 2가지 평행 생각이 있습니다:\n\n총 수익을 다양한 입력 지표로 분해:\n\n<div class=\"content-ad\"></div>\n\n- 총 메시지 발송 횟수: 메시지를 더 적게 보냈나요?\n- 오픈률: 수신자들이 메시지를 열었나요? 즉, 메시지 제목에 문제가 있었나요?\n- 클릭율: 수신자들이 메시지를 클릭할 확률이 낮았나요? 즉, 메시지 내용에 문제가 있었나요?\n- 전환이율: 수신자들이 클릭한 후에 구매할 확률이 낮았나요? 즉, 랜딩 페이지 경험에 문제가 있었나요?\n\n다양한 범주적 차원에서 이러한 추세를 분리하십시오\n\n- 채널: 이 문제가 모든 채널에서 관측되었습니까, 아니면 일부 채널에서만 관측되었습니까?\n- 메시지 유형: 이 문제가 모든 메시지 유형에서 관측되었습니까?\n\n이 경우에는 LLM이 이 두 시기 동안에 보낸 메시지 유형에 큰 차이를 식별할 수 있었습니다. 특히 7월에 진행된 50% 할인 세일이 8월에는 진행되지 않았다는 점을 확인했습니다.\n\n<div class=\"content-ad\"></div>\n\n# Ad Hoc Data Visualization\n\n딥 디퍼 증가율이 더 이해하기 쉬워졌네요, 그러나 매달 50% 할인 행사를 진행할 순 없어요. 우리가 마케팅 터치 포인트를 최대한 활용하기 위해 할 수 있는 다른 방법은 무엇일까요? 우리의 성과가 가장 좋은 캠페인을 살펴봐서 판매 프로모션 이외의 다른 요소들이 상위 10위 안에 있는지 확인해봅시다.\n\n데이터 시각화 도구는 데이터 시각화를 만들기 위한 클릭 인터페이스를 지원합니다. 오늘날, ChatGPT와 Julius AI 같은 도구들은 이미 반복적인 데이터 시각화 워크플로우를 충실하게 재현할 수 있어요.\n\n이러한 도구들은 파이썬 라이브러리를 활용해서 정적 데이터 시각화뿐만 아니라 대화식 차트까지도 생성하고 렌더링할 수 있어요. 자연어를 통해 이러한 시각화를 조정하고 반복하는 능력은 매우 원활해요. 코드 모듈, 이미지 렌더링, 대화식 차트 요소의 도입으로, 채팅 인터페이스는 주피터 노트북으로 대중화된 익숙한 \"노트북\" 형식과 유사해지고 있어요.\n\n<div class=\"content-ad\"></div>\n\n가끔 몇 가지 프롬프트만으로도 Tableau와 같은 데이터 시각화 도구의 고급 사용자인 것처럼 데이터 시각화를 쉽게 설정할 수 있습니다. 이 경우, Tableau의 듀얼 축 차트 작동 방법을 배우려고 도움 문서를 참조할 필요조차 없었습니다.\n\n여기서 \"새로운 도착물\" 메시지는 대량 발송 시에도 수신자당 강력한 수익을 창출함을 확인할 수 있습니다:\n\n![New Arrivals Chart](/assets/img/2024-06-19-HowLLMsWillDemocratizeExploratoryDataAnalysis_3.png)\n\n# 계절성 및 예측\n\n<div class=\"content-ad\"></div>\n\n\"새로운 도착 상품\"이 인기가 있어 보이는 것 같은데, 다음 달에 반드시 출시해야 할 새로운 도착 상품 유형은 무엇인가요? 9월이 다가오고 있으며, 이 시기에 고객 구매 패턴이 어떻게 변화하는지 알고 싶습니다. 어떤 상품 카테고리가 증가할 것으로 예상되나요? 감소할 것으로 예상되나요?\n\n![이미지](/assets/img/2024-06-19-HowLLMsWillDemocratizeExploratoryDataAnalysis_4.png)\n\n다시 한 번, 몇 번의 프롬프트로 명확하고 정확한 데이터 시각화를 얻었는데, 심지어 Tableau의 까다로운 Quick Table 계산 기능을 사용해야 할 필요도 없었습니다!\n\n# Market Basket Analysis\n\n<div class=\"content-ad\"></div>\n\n다음 달에 어떤 제품 카테고리가 증가할 가능성이 높은지 알았으니, 몇 가지 교차 판매 추천을 더 구체화하고 싶을 수도 있습니다. 그래서, 남성 운동용 아우터웨어 제품이 가장 큰 증가를 보일 예정이라면, 이 제품들과 가장 일반적으로 함께 구매되는 다른 카테고리는 어떻게 확인할 수 있을까요?\n\n이를 일반적으로 \"마켓 바스켓 분석\"이라고 부르며, 그를 수행하기 위해 필요한 데이터 변환은 다소 복잡합니다. 실제로, 엑셀에서 마켓 바스켓 분석을 수행하는 것은 번거롭거나 불가능할 수 있습니다. 그러나 LLMs를 사용하면 모든 것을 해결할 수 있습니다. 그저 잠시 멈추고 명확하게 질문을하면 됩니다:\n\n# 비즈니스 프로세스가 AI를 통합하기 위해 어떻게 발전할 것인가요?\n\n위의 데모는 LLMs가 대규모 데이터 기반 의사 결정을 지원하는 방법의 몇 가지 예를 보여줍니다. 주요 플레이어들은 이 기회를 확인했으며, 생태계는 빠르게 진화하여 LLMs를 분석 워크플로에 통합하고 있습니다. 다음을 고려해보세요:\n\n<div class=\"content-ad\"></div>\n\n- 작년 OpenAI가 \"코드 인터프리터\" 베타 버전을 출시했을 때, 이 기능은 빠르게 \"고급 데이터 분석\"으로 이름이 변경되었으며 초기 채택자들이 이 기능을 사용하는 방식에 부합하도록 조정되었습니다.\n- GPT4o를 통해 OpenAI는 이제 상호작용 가능한 차트 렌더링을 지원하며, 색상 코딩 변경, 마우스 호버 시 툴팁 렌더링, 차트 정렬/필터링, 차트 열 선택 및 계산 적용 등의 기능을 제공합니다.\n- Julius.ai와 같은 도구들이 특정한 키 분석 사용 사례를 명확히 다루기 위해 등장하고 있으며, 적절한 경우 여러 모델에 액세스를 제공합니다. Julius는 OpenAI와 Anthropic 모델에 대한 액세스를 제공합니다.\n- 제공 업체들은 정적 파일 업로드에서 Google 시트 커넥터 및 더 많은 고급 API 옵션으로 확장하여 데이터 공유를 점점 쉽게 만들고 있습니다.\n- Voiceflow와 같은 도구들이 등장하여 RAG(검색 증강 생성) 사용 사례(데이터 분석과 같은)에 중점을 둔 AI 앱 개발을 지원하고 있습니다. 이를 통해 타사 개발자들이 다양한 제공 업체의 다양한 LLM에 사용자 정의 데이터 세트를 연결하는 것이 점점 쉬워지고 있습니다.\n\n이를 감안할 때, BI 분석이 12~24개월 내에 어떻게 발전할지 상상해 보겠습니다. 다음은 몇 가지 예측입니다:\n\n- 인간 분석가들은 계속해서 적절한 질문을 하고 모호한 데이터를 해석하며 가설을 반복적으로 정제하는 데 중요한 역할을 할 것입니다.\n\nLLM이 분석에 대한 주요 장점은...\n\n<div class=\"content-ad\"></div>\n\n- 자연어를 코드로 변환하기(Python, R, SQL)\n- 데이터셋을 지능적으로 정리하기\n- 데이터를 효과적으로 시각화하기\n\n트렌드를 해석하고 가설을 검토하기 위해서는 해당 분야의 전문 지식이 여전히 필요합니다. 사람들은 쿼리 및 데이터 시각화 작업을 덜 할 것이지만, 탐색적 분석을 진전시키는 데 필수적인 역할을 계속할 것입니다.\n\n쿼리와 데이터 시각화에 대한 기술적인 역량보다 이 분야에서의 기술이 더 중요해질 것입니다. 강력한 데이터 리터러시와 비판적 사고력을 가진 의사 결정자는 LLM들의 도움을 받아 복잡한 시스템을 탐색하고 이해하는 능력을 크게 확장할 것입니다.\n\n규모에 맞게 AI 분석가들이 기업에서 채택하는 가장 큰 장애물은 데이터 개인 정보 보호에 대한 우려일 것입니다. 이러한 우려는 다양한 방법으로 성공적으로 해결될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n주요 LLM 공급업체들은 견고한 개인정보 보호 정책을 시행 중이지만, 데이터 공유는 주요 걱정 요소가 될 것입니다. 그러나 이미 탐구 중인 해결책들이 있습니다. LLM 공급업체들은 향상된 개인정보 보호 및 보안 조치가 적용된 전용 인스턴스를 실험하기 시작했습니다. 다른 해결책으로는 API를 통해 데이터를 공유할 때 암호화/복호화하거나 더미 데이터를 공유하고 코드 생성을 위해 LLM을 사용하는 등 여러 방법이 있을 수 있습니다.\n\nLLM 공급업체와 클라우드 데이터베이스 공급업체들 간의 수직적 조정이 높아질 것으로 예상됩니다(Gemini/BigQuery, OpenAI/Microsoft Azure, Amazon Olympus/AWS) 이 문제를 해결하기 위한 지원을 받기 위함입니다.\n\n데이터 개인정보 보호와 보안 문제가 해결되면, 초기 채택자들은 속도, 오류 처리, 그리고 가벼운 환각에 대한 도전에 직면할 것입니다. 이러한 도전들은 LLM 공급업체, 조직 및 개인들의 노력을 통해 극복될 것입니다.\n\n주요 LLM 공급업체들은 높은 속도와 정확도로 성능을 개선하고 있습니다. 전문화와 선택적 맥락(기억, 도구 접근 및 좁은 지침)에서 보다 특화된 분석 사용 사례가 혜택을 받을 것으로 기대됩니다.\n\n<div class=\"content-ad\"></div>\n\n기업들은 보다 깊은 통합과 비용 절감이 발생함에 따라 LLM 기반의 분석에 투자할 동기를 얻게 될 것입니다.\n\n개인들은 SQL, Python / R, 및 데이터 시각화 도구보다 훨씬 빠르게 효과적인 프롬프트 작성을 배울 수 있을 것입니다.\n\nBI 팀은 분석 요청에 대한 서비스보다는 기반 데이터 아키텍처의 구축 및 지원에 더 집중할 것입니다. 모든 BI 데이터 세트에 대한 철저한 데이터 세트 문서화가 앞으로 중요해질 것입니다.\n\n한 번 LLM이 일정 수준의 조직적 신뢰를 얻게 되면, 분석은 대부분 자체 서비스가 될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n\"데이터 사전\"은 오랫동안 BI 기관에게는 보조적인 관심사였지만, 앞으로는 AI 분석가를 활용하고자 하는 모든 기관에게 필수 요소가 될 것입니다.\n\nLLM의 능력을 통해 복잡한 데이터 변환을 실시간으로 수행할 수 있기 때문에, 적은 수의 집계 테이블이 필요할 것입니다. LLM이 사용하는 테이블은 메달리언 아키텍처의 \"골드\" 레벨보다는 \"실버\" 레벨에 더 가까울 것입니다.\n\n이 글에서 나중에 살펴볼 마켓 바스켓 분석이 명확한 예시입니다. 전통적인 마켓 바스켓 분석은 복잡한 변환을 통해 새로운 데이터 집합을 생성하는 것을 포함합니다. LLM을 사용하면 \"raw\" 테이블을 사용하고, 그 변환은 최종 사용자가 관심을 갖는 제품이나 카테고리에 대해서만 \"적시에\" 실행될 수 있습니다.\n\n여기서의 반론은 raw 데이터 집합이 더 크다는 것이고, 따라서 입력으로 전송해야 하는 토큰의 양이 많아지므로 더 높은 비용이 발생할 것입니다. 이것은 중요한 트레이드오프이며, leverage LLM API에 대한 비용 모델에 좌우됩니다.\"\n\n<div class=\"content-ad\"></div>\n\n음성 기술은 Siri나 Alexa와 같은 음성 어시스턴트를 통해 실은 크게 사용되지 않아 왔습니다. 그러나 GPT 4o의 출시와 실시간 대화형 음성 기능을 고려하면, 음성 상호작용이 주류로 받아들여지게 될 것입니다.\n\n상호작용형 시각화는 분석용도에서 주류로 될 것입니다.\n\n인간의 뇌가 시각 정보를 처리하는 데 다른 매체보다 효율적이라는 것은 잘 알려져 있습니다. 탐색적 분석의 사용자 경험은 음성/시각 패턴을 따르게 되며, 인간이 질문을 하고 AI 분석가가 가능한 데에서 정보를 시각화할 것으로 예상됩니다.\n\n<div class=\"content-ad\"></div>\n\n가장 효율적인 LLM 분석 시스템은 한 제공업체에서 여러 모델을 활용하여 특정 작업에 필요한 가장 단순한 모델을 지혜롭게 사용함으로써 토큰 비용을 절약할 수 있습니다.\n\nLLM 제공업체가 지속적으로 새로운 모델을 출시함에 따라 모델의 정교성에 따라 승수를 적용하는 토큰 기반의 가격 구조가 나타났습니다. 즉, GPT4o의 응답은 GPT3.5의 응답보다 비용이 더 듭니다. 비용을 절감하기 위해 AI 기반 애플리케이션은 다른 모델간에 사용을 최적화해야 합니다. 대량 데이터 전송에 따른 비용 문제로, 기관들이 데이터 공유를 한 제공업체로 제한할 것으로 예상됩니다.\n\n확장된 컨텍스트 창은 사용자가 장기 분석을 수행할 수 있게 하며, 새로운 데이터가 등장하고 가설이 변할 때에도 지속적인 컨텍스트를 유지할 수 있습니다.\n\nLLM의 컨텍스트 창 크기는 입력으로 전달할 수 있는 정보 양을 결정합니다. 데이터 분석과 같은 RAG (검색 증강 생성) 사용 사례의 경우 특히 높을 수 있습니다. 더 큰 컨텍스트 창은 더 많은 정보를 전달하고 더 긴 대화 교환을 가능하게 합니다.\n\n<div class=\"content-ad\"></div>\n\n지금은 Google의 Gemini 1.5 모델이 LLM 중에서 가장 큰 context window을 가지고 있습니다.\n\n![LLMs](/assets/img/2024-06-19-HowLLMsWillDemocratizeExploratoryDataAnalysis_5.png)\n\n# 2024년 AI 분석가 설정\n\n분석용 LLM을 실험해 보고 싶다면 초기 설정이 필요합니다. 최상의 성능을 얻고 일반적인 함정을 피하는 몇 가지 팁을 읽어보세요.\n\n<div class=\"content-ad\"></div>\n\n# 초기 설정\n\n# 데이터 세트 및 데이터 사전\n\n우선적으로, LLM에 데이터 액세스를 제공해야 합니다. 당연한 얘기지만, 공유하는 데이터 세트에는 LLM이 묻는 질문에 대답할 수 있는 데이터가 포함되어 있어야 합니다. 그 이상으로, 데이터 세트 내 모든 필드에 대한 명확하고 잘 쓰인 데이터 사전이 있어야 하며, LLM이 각 지표와 차원에 대한 강력한 이해를 갖게 해야 합니다. 위 데모에서 사용된 데이터 세트 및 데이터 사전은 다음에서 확인할 수 있습니다:\n\n- 이벤트 데이터 세트 예시\n- 마케팅 성과 데이터 세트 예시\n\n<div class=\"content-ad\"></div>\n\n# 데이터 연결\n\nLLM과 데이터를 공유하는 방법은 사용 중인 모델에 따라 다양합니다:\n\n- 파일 업로드: 오늘날 가장 쉬운 방법은 웹 앱 UI 내에서 정적 파일을 업로드하는 것입니다. ChatGPT, Claude 및 Julius.ai는 모두 웹 UI 내에서 파일 업로드를 지원합니다.\n- Google Sheets: ChatGPT 및 Julius.ai는 Google 시트로부터 데이터 가져오기를 네이티브로 지원합니다.\n- API: 더 고급화된 사용 사례나 AI 기반 앱을 위해 데이터 세트를 API를 통해 공유할 수 있습니다. 자세한 내용은 제공 업체의 개발자 문서를 참조하세요.\n\n# 데이터 개인 정보 보호\n\n<div class=\"content-ad\"></div>\n\n지금은 Meta의 Llama와 같은 오픈 소스 LLM을 설치하고 로컬에서 실행하는 경우를 제외하고는, 데이터를 LLM으로 보내려면 위의 방법 중 하나를 사용해야 합니다. 이로 인해 몇 가지 명백한 보안 및 데이터 개인정보에 대한 우려가 생깁니다.\n\n고객 ID와 같은 개인 식별 정보(PII, 개인을 식별할 수 있는 정보)를 포함하는 데이터 집합을 분석하려면 집계된 데이터 집합이 여러분의 요구를 충분히 충족하는지 고려해 보세요. 사전 집계된 데이터 집합이 부족하다고 결정할 경우, 업로드하기 전에 이러한 필드를 암호화하여 어떤 PII도 노출되지 않도록 해야 합니다.\n\n또한 여러분의 조직에 대한 기밀 정보를 공유하는지 여부도 고려해 보세요. 데이터 유출이 발생한 경우, 민감한 정보를 제3자에게 노출시켰다는 책임을 져서는 안 됩니다.\n\n본문에 언급된 모델들의 개인 정보 보호 정책을 확인할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- OpenAI: [OpenAI Privacy Policy](https://openai.com/policies/privacy-policy/)\n- Anthropic: [Anthropic Privacy and Legal](https://support.anthropic.com/en/collections/4078534-privacy-legal)\n- Julius: [Julius Privacy Policy](https://julius.ai/docs/privacy-policy)\n\n# 사용자정의 지침\n\n만약 사용자정의 GPT를 생성 중이라면, 원하는 행동을 안내할 수 있는 사용자정의 지침을 제공할 수 있습니다. Claude나 Gemini와 같은 다른 모델에서는 사용자정의 인스턴스를 지원하지 않는 경우에는 이러한 안내사항을 포함한 긴 형식의 프롬프트로 대화를 시작할 수 있습니다. 위의 데모에서 사용된 customGPT에 제공된 사용자정의 지침은 이곳에서 찾을 수 있습니다: Ecommerce BI Analyst Custom Instructions\n\n내 경험상 AI 분석가에게 다음과 같은 지침이 유용할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 신원: LLM에게 자신의 신원을 알려주세요. 예시: \"당신은 전문 비즈니스 인텔리전스 분석가로서 전자상거래 의류 소매업체에 대한 질문을 대답할 것입니다.\"\n- 데이터 세트: 일관된 데이터 세트를 사용할 계획이 있다면, LLM에게 데이터 세트의 성격에 대한 추가 콘텐츠를 제공하는 것이 좋습니다. 이는 데이터 사전을 보완하고 데이터에 대한 LLM의 이해를 명확히 합니다. 예시: \"이 데이터 세트에는 2022년 7월부터 8월까지 기록된 전자상거래 의류 소매업체의 모든 구매 데이터가 포함되어 있습니다. 각 행은 하나의 구매 이벤트를 나타내며, 구매된 제품 및 구매를 한 고객에 대한 데이터가 포함되어 있습니다.\"\n- 데이터 시각화 최적화 사례: LLM이 렌더링하는 차트는 사람이 읽기 쉽도록 약간의 조정이 필요할 수 있습니다. 시간이 흐름에 따라 개선될 수 있지만, 현재는 데이터 시각화 최적화 사례를 사용자 정의 지시사항에 \"하드코딩\"하는 것이 유용하다고 생각했습니다. 예시: \"항상 축 레이블이 가시적인지 확인하십시오. 다음 중 하나를 조합하여 축 레이블 수를 줄입니다: 표시된 레이블 수를 줄이기(즉, 매 두 번째 레이블 또는 매 세 번째 레이블을 표시하는 등), 레이블 방향 재구성, 또는 추가 여백 추가.\"\n- 계산된 필드: 파생 지표의 계산에 모호성을 제거하기 위해 계산 방법에 대한 분명한 지침을 제공하는 것이 유용합니다. 예시: \"'클릭 율' = '고유 클릭 수'의 합을 '고유 오픈 수'의 합으로 나눈 값\"\n- 일반 지침: LLM을 실험하면서 부적절하거나 예상치 못한 행동을 주의 깊게 관찰하고 사용자 정의 지시사항에 분명한 지침을 추가하는 것을 고려해보세요. 예시: \"질문에 'show me'와 같은 단어가 포함된 경우, 항상 데이터 시각화로 답하려고 노력해주세요.\"\n\n가끔은 GPT의 인스턴스를 \"프라이머\"로 설정해야 할 때도 있습니다. 이 경우, 사용자 정의 지시사항 내용에 관한 몇 가지 질문을 하여 그것이 이해했는지 확인하는 것이 좋습니다.\n\n# 주의할 점\n\n처리 시간: GPT는 때로는 응답 생성에 꽤 오랜 시간이 걸릴 수 있습니다. 위의 데모는 길이와 명확성을 위해 편집되었습니다. 시스템이 처리하는 총 부하에 따라 LLM이 데이터를 분석하는 동안 조금만 기다려야 할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n에러 메시지: LLM(언어 모델) 분석에 처음 실험을 시작할 때 사용자 지정 지침을 조정하고 프롬프트를 개선하는 과정에서 빈번한 에러 메시지를 만날 수 있습니다. 종종 LLM이 에러를 만나면 자동으로 재시도하여 성공적으로 정상 진행될 수 있습니다. 다른 경우에는 단순히 실패할 수도 있습니다. 이럴 때는 사용 중인 코드를 검토하고 직접 문제 해결을 해야 할 수도 있습니다.\n\n신뢰하되 검증하라: 더 복잡한 분석의 경우 모델의 출력을 점검하여 올바른 추이를 확인하세요. LLM이 좀 더 진화할 때까지, 모델의 접근 방식을 이해하기 위해 파이썬 코드를 Jupyter 노트북에서 로컬로 실행하는 것이 좋습니다.\n\n환각: 지식 베이스(데이터 세트)를 참조하는 대화에서 환각이 발생할 가능성이 훨씬 낮다고 발견했습니다. 그러나 가끔 LLM이 계산된 지표의 정의를 잘못 제시하거나 분석 내 요소의 성격에 대해 잘못 이야기하는 것을 발견하기도 했습니다.\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\nLLM(Large Language Models)은 질의 및 시각화 워크플로를 자동화하여 분석 영역을 근본적으로 뒤바꿀 것으로 예상됩니다. 다음 2-3년 동안 LLM을 분석 워크플로에 통합하고 BI팀을 이 새로운 기술을 지원하는 방향으로 재정렬하는 기관들은 전략적 의사 결정에서 큰 장점을 얻을 것입니다. 여기에 있는 잠재적 가치는 데이터 개인 정보 보호와 사용자 경험 주변의 초기적인 도전을 극복하기에 충분합니다.\n\n불멸의 Al Swearengen의 말처럼, \"모든 것이 변화합니다; 두렵지 마세요.\"\n\n이미지는 그렇게 표시되지 않는 한, 모두 저작자의 것입니다.","ogImage":{"url":"/assets/img/2024-06-19-HowLLMsWillDemocratizeExploratoryDataAnalysis_0.png"},"coverImage":"/assets/img/2024-06-19-HowLLMsWillDemocratizeExploratoryDataAnalysis_0.png","tag":["Tech"],"readingTime":13},{"title":"내 새 이북 초급에서 고급 수준까지 LLM 로드맵","description":"","date":"2024-06-19 03:28","slug":"2024-06-19-MyNewE-BookLLMRoadmapfromBeginnertoAdvancedLevel","content":"\n\n제가 새 이북인 LLM 로드맵을 초보부터 고급 수준까지 출간했다는 것을 기쁜 마음으로 알려드립니다. 이 이북은 LLMs를 숙달하는 여정을 시작하기 위해 필요한 모든 자원을 제공할 것입니다.\n\n![이북 표지](/assets/img/2024-06-19-MyNewE-BookLLMRoadmapfromBeginnertoAdvancedLevel_0.png)\n\n## 책의 내용은 다음과 같은 주제를 다룹니다:\n\n- LLM 기초 및 아키텍처\n\n<div class=\"content-ad\"></div>\n\n### 2. Building & Training LLM From Scratch\n\n- Best Resources On Building Datasets to Trian LLMs\n- Mastering Large Language Model (LLM) Fine-Tuning: Top Learning Resources\n- 14 Free Large Language Models Fine-Tuning Notebooks\n- Best Resources to Learn & Understand Evaluating LLMs\n- Overview of LLM Quantization Techniques & Where to Learn Each of Them?\n- Top Resources to Learn & Understand RLHF & LLM Alignment\n- How to Stay Updated with LLMs Research & Industry News?\n\n### 3. Building LLMs Applications In Production\n\n- Best Resources to Learn Prompt Engineering\n- Top Resources to Master Vector Databases & Building a Vector Storage\n- Top Resources to Master RAG: From Basic Level to Advanced\n- 5 Free Tools to Run Large Language Models (LLM) Locally on Your Laptop\n- Deploying LLMs: Top Learning & Educational Resources to Get Started\n- Getting Started with LLM Inference Optimization: Best Resources\n- What is LLMOps and How to Get Started With It?\n- Securing LLMs: Best Learning & Educational Resources\n\n<div class=\"content-ad\"></div>\n\n4. LLM 프로젝트 포트폴리오 구축하기\n\n- 포트폴리오를 구축할 수 있는 10가지 대형 언어 모델 프로젝트 아이디어\n- 포트폴리오를 구축할 수 있는 10가지 가이드 대형 언어 모델 프로젝트\n\n이 콘텐츠가 마음에 드신다면 LLM 습득을 향한 여정을 시작하고 싶다면 여기서 학습 계획을 받아보세요. 데이터 및 더 나은 미래 뉴스레터의 유료 구독자가 되어 이 책의 70% 할인 혜택을 받을 수 있습니다. 할인 코드는 여기에서 찾으실 수 있습니다:","ogImage":{"url":"/assets/img/2024-06-19-MyNewE-BookLLMRoadmapfromBeginnertoAdvancedLevel_0.png"},"coverImage":"/assets/img/2024-06-19-MyNewE-BookLLMRoadmapfromBeginnertoAdvancedLevel_0.png","tag":["Tech"],"readingTime":2},{"title":"고급 RAG 12 전세계 이해도 향상하기","description":"","date":"2024-06-19 03:24","slug":"2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding","content":"\n\n많은 중요한 실제 업무는 과학 문헌 검토, 법적 사례 요약 및 의료 진단과 같이 다양한 문서 덩어리나 문서 간의 지식 이해가 필요합니다.\n\n기존 RAG 방법은 각 덩어리가 독립적으로 인코딩되기 때문에 정보를 이해하는 작업을 필요로 하는 LLMs에게 도움을 줄 수 없습니다.\n\n본 문서에서는 문서 또는 말뭉치의 전역적 이해를 향상시키기 위한 네 가지 혁신적인 방법을 소개합니다. 이를 통해 얻은 통찰과 생각에 대한 내용도 함께 공유할 것입니다.\n\n네 가지 방법은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- RAPTOR: 이는 텍스트 청크를 재귀적으로 삽입, 클러스터링 및 요약하는 트리 기반 검색 시스템입니다.\n- Graph RAG: 이 방법은 지식 그래프 생성, 커뮤니티 탐지, RAG 및 쿼리 중심 요약(QFS)을 결합하여 전체 텍스트 코퍼스의 철저한 이해를 도와줍니다.\n- HippoRAG: 이 검색 프레임워크는 사람의 장기 기억의 해리 인덱싱 이론에서 영감을 받습니다. LLMs, 지식 그래프 및 개인화된 PageRank 알고리즘과 협력합니다.\n- spRAG: 이 방법은 AutoContext와 Relevant Segment Extraction(RSE)이라는 두 가지 핵심 기술을 통해 표준 RAG 시스템의 성능을 향상시킵니다.\n\n# RAPTOR: 트리 구조화 검색을 위한 재귀적 요약 처리\n\nRAPTOR은 텍스트 세그먼트를 재귀적으로 포함, 클러스터링 및 요약하는 혁신적인 트리 기반 검색 시스템입니다. 이는 아래에서 위로 트리를 구성하여 다양한 수준의 요약을 제공합니다.\n\n추론 중에 RAPTOR는 이 트리에서 정보를 검색하여 다양한 수준의 추상화에서 더 긴 문서의 데이터를 통합합니다.\n\n<div class=\"content-ad\"></div>\n\n## 핵심 아이디어\n\nRAPTOR은 임베딩을 기반으로 텍스트 청크를 클러스터로 구성하기 위해 재귀 방법을 사용합니다. 이는 아래에서 위로 트리를 구성하여 각 클러스터에 대한 요약을 생성합니다. 이 과정은 그림 1에 설명되어 있습니다.\n\n![Figure 1](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_0.png)\n\n아래에서는 그림 1과 관련된 구체적인 주제에 대해 자세히 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n- RAPTOR 트리 작성\n- 검색 프로세스\n\n## RAPTOR 트리 작성\n\n텍스트 청킹\n\n검색 말뭉치를 연속적인 100토큰 단위의 청크로 나눕니다. 100토큰을 초과하는 경우, RAPTOR는 전체 문장을 다음 청크로 이동하여 문맥 및 의미 일관성을 유지합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndef split_text(\n    text: str, tokenizer: tiktoken.get_encoding(\"cl100k_base\"), max_tokens: int, overlap: int = 0\n):\n    \"\"\"\n    입력 텍스트를 tokenizer와 최대 허용 토큰을 기반으로 작은 청크로 분할합니다.\n    \n    Args:\n        text (str): 분할할 텍스트입니다.\n        tokenizer (CustomTokenizer): 텍스트를 분할하는 데 사용할 tokenizer입니다.\n        max_tokens (int): 최대 허용 토큰 수입니다.\n        overlap (int, optional): 청크 간의 겹치는 토큰 수입니다. 기본값은 0입니다.\n    \n    Returns:\n        List[str]: 텍스트 청크의 목록입니다.\n    \"\"\"\n    ...\n    ...        \n        # 현재 청크에 문장을 추가하면 최대 토큰을 초과하는 경우, 새로운 청크를 시작합니다.\n        elif current_length + token_count > max_tokens:\n            chunks.append(\" \".join(current_chunk))\n            current_chunk = current_chunk[-overlap:] if overlap > 0 else []\n            current_length = sum(n_tokens[max(0, len(current_chunk) - overlap):len(current_chunk)])\n            current_chunk.append(sentence)\n            current_length += token_count\n    ...\n    ...\n```\n\n텍스트 임베딩\n\n이러한 청크를 밀집 벡터 표현으로 생성하기 위해 Sentence-BERT를 사용합니다.\n\n이러한 청크와 해당 임베딩은 RAPTOR 트리 구조의 리프 노드를 형성합니다.\n\n<div class=\"content-ad\"></div>\n\n```python\nclass TreeBuilder:\n    \"\"\"\n    TreeBuilder 클래스는 요약 모델과 임베딩 모델을 사용하여 \"트리\"라고 불리는 계층적 텍스트 추상 구조를 작성하는 책임이 있습니다.\n    \"\"\"\n    ...\n    ...\n    def build_from_text(self, text: str, use_multithreading: bool = True) -> Tree:\n        \"\"\"입력 텍스트에서 골든 트리를 작성하며, 선택적으로 멀티스레딩을 사용합니다.\n\n        Args:\n            text (str): 입력 텍스트입니다.\n            use_multithreading (bool, optional): 리프 노드를 만들 때 멀티스레딩을 사용할지 여부입니다.\n                기본값: True.\n\n        Returns:\n            Tree: 골든 트리 구조입니다.\n        \"\"\"\n        chunks = split_text(text, self.tokenizer, self.max_tokens)\n\n        logging.info(\"리프 노드 생성 중\")\n\n        if use_multithreading:\n            leaf_nodes = self.multithreaded_create_leaf_nodes(chunks)\n        else:\n            leaf_nodes = {}\n            for index, text in enumerate(chunks):\n                __, node = self.create_node(index, text)\n                leaf_nodes[index] = node\n\n        layer_to_nodes = {0: list(leaf_nodes.values())}\n\n        logging.info(f\"생성된 {len(leaf_nodes)} 개의 리프 임베딩\")\n        ...\n        ...\n```\n\n클러스터링 방법\n\n클러스터링은 RAPTOR 트리를 구성하는 데 중요하며, 텍스트 단락을 일관된 그룹으로 구성합니다. 관련 콘텐츠를 함께 모아 나중의 검색 프로세스를 향상시킵니다.\n\nRAPTOR의 클러스터링 방법은 다음과 같은 특징을 가지고 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n- 군집에는 가우시안 혼합 모델(GMMs)과 UMAP 차원 축소가 사용됩니다.\n- UMAP 매개변수를 수정하여 전역 및 지역 클러스터를 식별할 수 있습니다.\n- 모델 선택을 위해 Bayesian Information Criterion (BIC)이 사용되어 최적 클러스터 수를 결정합니다.\n\n이 군집화 방법의 핵심은 노드가 여러 군집에 속할 수 있다는 것입니다. 따라서 하나의 텍스트 세그먼트에 여러 주제에 대한 정보가 들어 있기 때문에 고정된 범주 수가 필요하지 않으며, 이는 여러 개요에 텍스트 세그먼트를 포함시켜줍니다.\n\n노드를 GMM을 사용하여 군집화한 후, 각 군집 내의 노드는 LLM에 의해 요약됩니다. 이 과정은 대량의 데이터를 뽑아 선택된 노드의 간결하고 일관된 개요로 변환합니다.\n\n구현에서는 gpt-3.5 turbo가 요약 생성에 사용됩니다. 해당 프롬프트는 그림 2에 나와 있습니다.\n\n<div class=\"content-ad\"></div>\n\nMarkdown 형식으로 변경\n\n![Construction Algorithm](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_1.png)\n\n생성 알고리즘\n\n지금까지 전체 트리의 리프 노드를 얻고 클러스터링 알고리즘을 결정했습니다.\n\n도형 1의 중간에 나타난 것처럼 함께 그룹화된 노드는 형제가 되며, 부모 노드는 해당 특정 클러스터의 요약을 포함합니다. 생성된 요약은 트리의 비리프 노드를 구성합니다.\n\n<div class=\"content-ad\"></div>\n\n## 검색 프로세스\n\n노드들을 요약한 후, 삽입, 클러스터링, 그리고 요약 과정이 더 이상 실행할 수 없을 때까지 계속됩니다. 이렇게 하면 원본 문서의 구조화된 다층 트리 표현이 생성됩니다.\n\n해당 코드는 아래와 같이 나타납니다.\n\n```python\nclass ClusterTreeConfig(TreeBuilderConfig):\n    ...\n    ...\n    def construct_tree(\n        self,\n        current_level_nodes: Dict[int, Node],\n        all_tree_nodes: Dict[int, Node],\n        layer_to_nodes: Dict[int, List[Node]],\n        use_multithreading: bool = False,\n    ) -> Dict[int, Node]:\n        ...\n        ...\n\n        for layer in range(self.num_layers):\n\n            new_level_nodes = {}\n\n            logging.info(f\"Constructing Layer {layer}\")\n\n            node_list_current_layer = get_node_list(current_level_nodes)\n\n            if len(node_list_current_layer) <= self.reduction_dimension + 1:\n                self.num_layers = layer\n                logging.info(\n                    f\"Stopping Layer construction: Cannot Create More Layers. Total Layers in tree: {layer}\"\n                )\n                break\n\n            clusters = self.clustering_algorithm.perform_clustering(\n                node_list_current_layer,\n                self.cluster_embedding_model,\n                reduction_dimension=self.reduction_dimension,\n                **self.clustering_params,\n            )\n\n            lock = Lock()\n\n            summarization_length = self.summarization_length\n            logging.info(f\"Summarization Length: {summarization_length}\")\n\n            ...\n            ...\n```\n\n<div class=\"content-ad\"></div>\n\nRAPTOR 트리를 갖게 된 후 쿼리하는 방법은 무엇인가요?\n\n쿼리하는 방법에는 두 가지가 있습니다: 트리 순회 방법과 축소된 트리를 기반으로 하는 방법으로, 이는 그림 3에 나와 있습니다.\n\n![image](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_2.png)\n\n트리 순회는 트리의 루트 레벨에서 시작하여 쿼리 벡터와의 코사인 유사성에 따라 상위 k개 노드(이 경우 상위 1개)를 검색합니다. 각 레벨에서 이전 레이어의 상위 k개 노드의 자식노드로부터 상위 k개 노드를 검색하며, 해당 코드는 아래에 표시되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\nclass TreeRetriever(BaseRetriever):\n    ...\n    ...\n    def retrieve_information(\n        self, current_nodes: List[Node], query: str, num_layers: int\n    ) -> str:\n        \"\"\"\n        쿼리에 기반하여 트리에서 가장 관련성 높은 정보를 검색합니다.\n\n        Args:\n            current_nodes (List[Node]): 현재 노드의 목록.\n            query (str): 쿼리 텍스트.\n            num_layers (int): 횡단할 레이어의 수.\n\n        Returns:\n            str: 가장 관련성 높은 노드를 사용하여 생성된 콘텍스트.\n        \"\"\"\n\n        query_embedding = self.create_embedding(query)\n\n        selected_nodes = []\n\n        node_list = current_nodes\n\n        for layer in range(num_layers):\n\n            embeddings = get_embeddings(node_list, self.context_embedding_model)\n\n            distances = distances_from_embeddings(query_embedding, embeddings)\n\n            indices = indices_of_nearest_neighbors_from_distances(distances)\n\n            if self.selection_mode == \"threshold\":\n                best_indices = [\n                    index for index in indices if distances[index] > self.threshold\n                ]\n\n            elif self.selection_mode == \"top_k\":\n                best_indices = indices[: self.top_k]\n\n            nodes_to_add = [node_list[idx] for idx in best_indices]\n\n            selected_nodes.extend(nodes_to_add)\n\n            if layer != num_layers - 1:\n\n                child_nodes = []\n\n                for index in best_indices:\n                    child_nodes.extend(node_list[index].children)\n\n                # 중복 값을 제외합니다.\n                child_nodes = list(dict.fromkeys(child_nodes))\n                node_list = [self.tree.all_nodes[i] for i in child_nodes]\n\n        context = get_text(selected_nodes)\n        return selected_nodes, context\n\n\n반면에, 축소된 트리는 트리를 단일 레이어로 축소하고 일정 토큰 수에 도달할 때까지 노드를 검색합니다. 다시 말해, 쿼리 벡터와의 코사인 유사도를 기반으로 상응하는 코드는 다음과 같습니다.\n\n\nclass TreeRetriever(BaseRetriever):\n    ...\n    ...\n    def retrieve_information_collapse_tree(self, query: str, top_k: int, max_tokens: int) -> str:\n        \"\"\"\n        쿼리에 기반하여 트리에서 가장 관련성 높은 정보를 검색합니다.\n\n        Args:\n            query (str): 쿼리 텍스트.\n            max_tokens (int): 최대 토큰 수.\n\n        Returns:\n            str: 가장 관련성 높은 노드를 사용하여 생성된 콘텍스트.\n        \"\"\"\n\n        query_embedding = self.create_embedding(query)\n\n        selected_nodes = []\n\n        node_list = get_node_list(self.tree.all_nodes)\n\n        embeddings = get_embeddings(node_list, self.context_embedding_model)\n\n        distances = distances_from_embeddings(query_embedding, embeddings)\n\n        indices = indices_of_nearest_neighbors_from_distances(distances)\n\n        total_tokens = 0\n        for idx in indices[:top_k]:\n\n            node = node_list[idx]\n            node_tokens = len(self.tokenizer.encode(node.text))\n\n            if total_tokens + node_tokens > max_tokens:\n                break\n\n            selected_nodes.append(node)\n            total_tokens += node_tokens\n\n        context = get_text(selected_nodes)\n        return selected_nodes, context\n\n\n따라서 어떤 방법이 더 나은지요?\n\n\n<div class=\"content-ad\"></div>\n\nRAPTOR이 그림 4에서 보여준대로 비교를 진행했습니다.\n\n![Figure 4](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_3.png)\n\n그림 4에서 보여진 대로, 2000 토큰을 가진 축소된 트리가 최상의 결과를 제공합니다. 이는 트리 탐색보다 더 많은 유연성을 제공하기 때문입니다. 구체적으로, 모든 노드를 동시에 탐색함으로써, 주어진 문제에 대한 적절한 상세 수준에서 정보를 검색합니다.\n\n그림 5는 RAPTOR이 \"이야기의 중심 주제는 무엇인가요?\" 및 \"신데렐라가 행복한 결말을 어떻게 이끌어 냈나요?\"라는 두 질문에 관련된 정보를 검색하는 방법을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_4.png)\n\nHighlighted nodes indicate RAPTOR’s selections, while arrows point to DPR’s (Dense Passage Retrieval) leaf nodes. Importantly, the context provided by RAPTOR often includes the information retrieved by DPR, either directly or within higher-layer summaries.\n\n# Graph RAG\n\nGraph RAG employs LLM to construct a graph-based text index in two stages:\n\n\n<div class=\"content-ad\"></div>\n\n- 먼저, 소스 문서에서 지식 그래프를 도출합니다.\n- 이후에는 밀접하게 연결된 엔터티 그룹에 대한 커뮤니티 요약을 생성합니다.\n\n쿼리가 주어지면 각 커뮤니티 요약은 부분 응답에 기여합니다. 이러한 부분 응답은 최종 글로벌 답변을 형성하기 위해 집계됩니다.\n\n## 개요\n\n도식 6은 Graph RAG의 파이프라인을 보여줍니다. 보라색 상자는 인덱싱 작업을 나타내고, 초록색 상자는 쿼리 작업을 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_5.png)\n\nRAG 그래프는 데이터셋 도메인에 특화된 LLM(대규모 언어 모델) 프롬프트를 사용하여 노드(엔터티와 같은), 엣지(관계와 같은), 및 공변량(클레임과 같은)을 감지, 추출, 요약합니다.\n\n커뮤니티 탐지는 그래프를 노드, 엣지, 공변량의 그룹으로 나누어주며 LLM이 색인 및 질의 시 요약할 수 있도록 합니다.\n\n특정 쿼리에 대한 전역 응답은 해당 쿼리와 관련된 모든 커뮤니티 요약에 대해 최종 쿼리 중심 요약을 수행하여 생성됩니다.\n\n<div class=\"content-ad\"></div>\n\nFigure 6의 각 단계 구현에 대해 아래에서 설명하겠습니다. 2024년 6월 12일을 기준으로 Graph RAG는 현재 오픈 소스가 아니므로 소스 코드와 관련하여 논의할 수 없습니다.\n\n## 단계 1: 소스 문서 → 텍스트 청크\n\n청크 크기의 트레이드오프는 RAG의 오랜 문제입니다.\n\n청크가 너무 길면 LLM 호출 수가 감소합니다. 그러나 컨텍스트 창의 제약으로 인해 대량의 정보를 완전히 이해하고 관리하기가 어려워집니다. 이 상황은 리콜률의 저하로 이어질 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Screenshot](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_6.png)\n\nFigure 7에 설명된 대로 HotPotQA 데이터셋의 경우, 600 토큰의 청크 크기는 2400 토큰의 청크 크기에 비해 효과적인 엔티티를 두 배 더 추출합니다.\n\n## 단계 2: 텍스트 청크 → 요소 인스턴스(엔티티 및 관계)\n\n해당 방법은 각 청크에서 엔티티와 관계를 추출하여 지식 그래프를 구성하는 것을 포함합니다. 이는 LLM과 프롬프트 엔지니어링의 조합을 통해 달성됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n동시에 Graph RAG는 다단계 반복 프로세스를 사용합니다. 이 프로세스에서 LLM은 모든 엔티티가 추출되었는지 결정해야 합니다. 이는 이진 분류 문제와 유사합니다.\n\n## 단계 3: 요소 인스턴스 → 요소 요약 → 그래프 커뮤니티 → 커뮤니티 요약\n\n이전 단계에서 엔티티, 관계, 주장을 추출하는 것은 사실 요약의 한 형태입니다.\n\n하지만 Graph RAG는 이것만으로 충분하지 않고 LLM을 사용하여 이러한 \"요소\"를 더 자세히 요약해야 한다고 생각합니다.\n\n<div class=\"content-ad\"></div>\n\n잠재적인 우려 사항은 LLMs가 항상 같은 엔티티에 대한 참조를 동일한 텍스트 형식으로 추출하지 않을 수 있다는 점입니다. 이로 인해 중복된 엔티티 요소가 발생하여 그래프에서 중복된 노드가 생성될 수 있습니다.\n\n그 이슈는 빠르게 사라질 것입니다.\n\nGraph RAG는 커뮤니티 탐지 알고리즘을 활용하여 그래프 내에서 커뮤니티 구조를 식별하여 연결된 엔티티를 동일한 커뮤니티에 통합합니다. Figure 8은 Leiden 알고리즘을 사용하여 MultiHop-RAG 데이터셋에서 식별된 그래프 커뮤니티를 보여줍니다.\n\n이 시나리오에서 LLM이 추출 중에 엔티티의 모든 변형을 일관되게 식별하지 못하더라도 커뮤니티 탐지는 이러한 변형 사이의 연결을 수립하는 데 도움을 줄 수 있습니다. 한 번 커뮤니티로 그룹화되면, 이러한 변형이 동일한 엔티티 의미를 가리킨다는 것을 나타냅니다. 다만 표현이나 동의어가 다를 뿐입니다. 이는 지식 그래프 분야에서의 엔티티 모호성 해소와 유사합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_7.png\" />\n\n커뮤니티를 식별한 후, Leiden 계층 내 각 커뮤니티에 대한 보고서 형태의 요약을 생성할 수 있습니다. 이러한 요약은 데이터셋의 전역 구조와 의미를 이해하는 데 독립적으로 유용합니다. 이 요약은 또한 어떠한 문제 없이 말뭉치를 이해하는 데 사용할 수 있습니다.\n\n그림 9은 커뮤니티 요약의 생성 방법을 보여줍니다.\n\n## 단계 4: 커뮤니티 요약 → 커뮤니티 답변 → 전역 답변\n\n<div class=\"content-ad\"></div>\n\n이제 마지막 단계에 도달했습니다: 이전 단계에서의 커뮤니티 요약을 기반으로 최종 답변을 생성하는 것입니다.\n\n커뮤니티 구조의 계층적 특성으로 인해 서로 다른 수준의 요약은 다양한 질문에 대답할 수 있습니다.\n\n그러나 여기서 또 하나의 질문이 생깁니다: 다양한 수준의 커뮤니티 요약이 있는 경우, 어떤 수준이 세부 사항과 범위 사이의 균형을 맞출 수 있을까요?\n\nGraph RAG은 Graph RAG 논문의 섹션 3을 더 자세히 살펴보면서 가장 적절한 추상화 수준을 선택합니다.\n\n<div class=\"content-ad\"></div>\n\n주어진 커뮤니티 수준에서는 그림 10에 표시된 대로 어떤 사용자 쿼리에 대한 글로벌 답변이 생성됩니다.\n\n# HippoRAG\n\nHippoRAG는 인간의 장기 기억의 해마 색인 이론에서 영감을 받아 새로운 검색 프레임워크입니다. LLMs, 지식 그래프 및 개인화된 페이지랭크 알고리즘과 협력하여 작동합니다. 이 협력은 인간의 기억에서 피질과 해마프스의 다양한 역할을 모방합니다.\n\n## 주요 아이디어\n\n<div class=\"content-ad\"></div>\n\n**표 11**은 인간 두뇌가 지식 통합의 어려운 과제를 비교적 쉽게 해결하는 방법을 보여줍니다.\n\n인간 장기 기억에 관한 잘 알려진 이원칙인 해마기억색인이론은 이 놀라운 능력에 대한 가능한 설명을 제시합니다.\n\n구체적으로 환경 기반의 지속적으로 업데이트되는 기억은 신장피질과 C자 모양의 해마 간 상호작용에 의존합니다. 신장피질은 실제 기억 표현을 처리하고 저장하는 반면, 해마는 해마색인을 유지합니다. 이 색인은 신장 피질에서 기억 단위를 가리키고 그들의 연결을 저장하는 일련의 상호 연결된 색인입니다.\n\n![이미지](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_8.png)\n\n<div class=\"content-ad\"></div>\n\nFigure 11에서는 수천 명의 Stanford 교수와 알츠하이머병 연구자를 설명하는 여러 통로에서 알츠하이머병 연구와 관련된 Stanford 교수를 식별하는 것을 목표로 합니다.\n\n- 과거 방식의 RAG는 통로를 독립적으로 부호화하여 Thomas 교수를 식별하는 데 어려움을 겪었는데, 통로가 두 기능을 동시에 언급할 때에만 이를 가능케 했습니다.\n- 그에 반해, 이 교수에 익숙한 사람들은 뇌의 연관 메모리 능력 덕분에 기억하기 쉬울 것입니다. 이 능력은 도형 11에서 파란색으로 나타난 C 모양 해마색 부분에 의해 주도되는 것으로 여겨집니다.\n- 이 메커니즘의 영향을 받아 HippoRAG는 LLMs가 지식 통합 작업을 관리하기 위해 유사한 연상 지도를 구축하고 활용할 수 있게 합니다.\n\n## 개요\n\n도형 11에서 영감을 받아, HippoRAG의 각 구성 요소는 도형 12에 제시된 것처럼 인간 장기 기억의 세 구성 요소 중 하나에 해당합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_9.png](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_9.png)\n\nHippoRAG은 인간의 장기기억의 세 가지 구성 요소를 모방하여 패턴 구분 및 완료 기능을 에뮬레이트합니다.\n\n- 오프라인 색인을 위해 LLM은 텍스트 단락을 열린 KG 트리플로 처리합니다. 그런 다음 인공 해마색인에 추가됩니다. 동시에 합성 해마집 영역(PHR)은 동의어를 감지합니다. 위의 예에서 HippoRAG는 Thomas 교수를 포함하는 트리플을 추출하고 KG에 통합합니다.\n- 온라인 검색을 위해 LLM 뉴로피질은 질의에서 명명된 엔터티를 추출합니다. 해마색인에 연결되도록 해마집 검색 인코더가 이들을 링크합니다. HippoRAG은 문맥 기반 검색을 위해 개인화된 페이지랭크 알고리즘을 활용하며 Thomas 교수와 관련된 정보를 추출합니다.\n\n## 전체 프로세스 데모\n\n\n<div class=\"content-ad\"></div>\n\nHippoRAG 파이프라인을 소개하는 실용적인 예시가 있습니다.\n\nFigure 13은 질문, 그에 대한 답변, 그리고 지원 및 분랄 글에서의 내용을 보여줍니다.\n\n![Figure 13](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_10.png)\n\nFigure 14는 OpenIE 절차와 지식 그래프의 관련 부분을 포함한 색인화 단계를 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n마지막으로, 표 15은 검색 단계를 보여줍니다. 쿼리 Named Entity Recognition (NER), 쿼리 노드 검색, 개인화된 페이지 랭크 (PPR) 알고리즘이 노드 확률에 미치는 영향, 그리고 최상위 검색 결과의 계산이 표시됩니다.\n\n아래에는 소스 코드와 함께 HippoRAG가 장기 기억을 구축하고 검색하는 두 가지 측면에 대해 구체적으로 논의합니다.\n\n<div class=\"content-ad\"></div>\n\n## 장기 기억을 구축하는 방법\n\n장기 기억을 구축하는 과정은 주로 다음 세 단계로 구성됩니다.\n\n먼저, Figure 16에서 보여지는 대로 OpenIE를 사용하여 검색 코퍼스의 각 텍스트에서 명명된 개체 세트를 추출하기 위해 LLM을 활용하십시오.\n\n![이미지](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_13.png)\n\n<div class=\"content-ad\"></div>\n\n다음으로, 최종 삼중체를 추출하기 위해 OpenIE 프롬프트에 명명된 엔티티를 추가하십시오. Figure 17에 나와 있는 대로요.\n\n![Figure 17](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_14.png)\n\n마지막으로, 파인튠된 외장형 밀도 인코더를 활용하여 지식 그래프를 생성하고 검색에 사용할 것입니다.\n\n## 검색 방법\n\n<div class=\"content-ad\"></div>\n\n먼저, 사용자 쿼리에서 명명된 엔티티 집합을 추출하기 위해 LLM을 사용하십시오. Figure 18에서 보여지는 것처럼요.\n\n![Figure 18](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_15.png)\n\n그런 다음, 이러한 명명된 엔티티를 지식 그래프의 노드에 유사성에 따라 링크합니다. 우리는 이러한 선택된 노드를 쿼리 노드라고 부릅니다.\n\n해마에서 해마 색인 요소들 간의 신경 경로는 관련 이웃들이 활성화되어 상류로 회상될 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\n이 효율적인 그래프 탐색 프로세스를 모방하기 위해, HippoRAG는 Personalized PageRank (PPR) 알고리즘을 활용합니다. 이 알고리즘은 그래프 상에서 확률을 사용자가 정의한 일련의 소스 노드를 통해만 분배하는 PageRank의 버전입니다. 아래에 해당 코드가 표시되어 있습니다.\n\n```js\n    def rank_docs(self, query: str, top_k=10):\n        \"\"\"\n        쿼리를 기반으로 문서를 순위 지정\n        @param query: 입력 구문\n        @param top_k: 반환할 문서 수\n        @return: 순위가 지정된 문서 ID 및 점수\n        \"\"\"\n        ...\n        ...\n        # Personalized PageRank (PPR) 또는 다른 그래프 알고리즘을 실행하여 문서 점수 계산\n        if len(query_ner_list) > 0:\n            combined_vector = np.max([top_phrase_vectors], axis=0)\n\n            if self.graph_alg == 'ppr':\n                ppr_phrase_probs = self.run_pagerank_igraph_chunk([top_phrase_vectors])[0]\n            elif self.graph_alg == 'none':\n                ppr_phrase_probs = combined_vector\n            elif self.graph_alg == 'neighbor_2':\n                ppr_phrase_probs = self.get_neighbors(combined_vector, 2)\n            elif self.graph_alg == 'neighbor_3':\n                ppr_phrase_probs = self.get_neighbors(combined_vector, 3)\n            elif self.graph_alg == 'paths':\n                ppr_phrase_probs = self.get_neighbors(combined_vector, 3)\n            else:\n                assert False, f'그래프 알고리즘 {self.graph_alg}은(는) 구현되지 않았습니다.'\n\n            fact_prob = self.facts_to_phrases_mat.dot(ppr_phrase_probs)\n            ppr_doc_prob = self.docs_to_facts_mat.dot(fact_prob)\n            ppr_doc_prob = min_max_normalize(ppr_doc_prob)\n        else:\n            ppr_doc_prob = np.ones(len(self.extracted_triples)) / len(self.extracted_triples)\n        ...\n        ...\n``` \n\n마지막으로, 해마신호가 상류로 전달될 때와 같이 HippoRAG는 이전에 인덱싱된 통로 전체에 대한 출력 PPR 노드 확률을 집계하고 이를 검색을 위해 등수를 매기기 위해 사용합니다.\n\n# spRAG\n\n<div class=\"content-ad\"></div>\n\nspRAG은 복잡한 쿼리를 관리하기 위한 방법입니다. 표준 RAG의 성능을 향상시키는 두 가지 주요 기술을 통해 작동합니다:\n\n- AutoContext\n- 관련 세그먼트 추출 (RSE)\n\n우리는 spRAG가 청크 전반에 걸친 복잡한 쿼리를 처리하는 방법에 초점을 맞추고 있습니다. 현재 spRAG에 대한 논문은 없으며 분석과 코드가 결합된 상태만 있습니다.\n\n## AutoContext: 문서 수준 컨텍스트의 자동 주입\n\n<div class=\"content-ad\"></div>\n\n전통적인 RAG에서는 일반적으로 문서를 포함하는 데 고정 길이의 청크로 나눕니다. 이 간단한 방법은 종종 문서 수준의 컨텍스트 정보를 간과하여 보다 정확하고 포괄적인 컨텍스트 포함을 방해할 수 있습니다.\n\n이 문제를 해결하기 위해 AutoContext가 개발되었습니다. 그 핵심 아이디어는 각 청크에 포함되기 전에 문서 수준의 컨텍스트 정보를 자동으로 통합하는 것입니다.\n\n구체적으로, 1~2 문장으로 문서 요약을 작성하고 파일 이름과 함께 각 청크의 시작 부분에 추가합니다. 결과적으로, 각 청크는 고립되어 있지 않지만 전체 문서의 컨텍스트 정보를 가지고 있습니다. 문서 요약을 얻는 코드는 아래와 같이 표시됩니다.\n\n```js\ndef get_document_context(auto_context_model: LLM, text: str, document_title: str, auto_context_guidance: str = \"\"):\n    # content이 너무 긴 경우 자르기\n    max_content_tokens = 6000 # 이 숫자를 변경하면 위의 자르기 메시지도 업데이트해야 합니다\n    text, num_tokens = truncate_content(text, max_content_tokens)\n    if num_tokens < max_content_tokens:\n        truncation_message = \"\"\n    else:\n        truncation_message = TRUNCATION_MESSAGE\n\n    # 문서 컨텍스트 가져오기\n    prompt = PROMPT.format(auto_context_guidance=auto_context_guidance, document=text, document_title=document_title, truncation_message=truncation_message)\n    chat_messages = [{\"role\": \"user\", \"content\": prompt}]\n    document_context = auto_context_model.make_llm_call(chat_messages)\n    return document_context\n```\n\n<div class=\"content-ad\"></div>\n\n## 관련 세그먼트 추출: 연관 텍스트 청크의 지능적 조합\n\nRSE는 후속 처리 단계입니다. 그 목적은 가장 관련성 있는 정보를 제공할 수 있는 청크를 지능적으로 식별하고 결합하여 더 긴 세그먼트를 형성하는 것입니다.\n\n구체적으로, RSE는 먼저 콘텐츠 유사 또는 의미론적으로 관련된 검색된 청크를 그룹화합니다. 그런 다음 쿼리 요구 사항에 따라 이러한 청크를 선택하고 조합하여 최상의 세그먼트를 형성합니다. 관련 코드는 아래에 나와 있습니다.\n\n```js\ndef get_best_segments(all_relevance_values: list[list], document_splits: list[int], max_length: int, overall_max_length: int, minimum_value: float) -> list[tuple]:\n    \"\"\"\n    이 함수는 청크 관련성 값들을 가져와서 최상의 세그먼트를 찾기 위해 최적화 알고리즘을 실행합니다.\n\n    - all_relevance_values: 각 메타-문서의 각 청크에 대한 관련성 값 목록의 목록으로서, 각 외부 목록은 쿼리를 나타냅니다\n    - document_splits: 각 문서의 시작을 나타내는 인덱스 목록 - 최상의 세그먼트는 이러한 인덱스와 중복되지 않을 것입니다\n    \n    반환\n    - best_segments: 메타-문서에서 최상의 세그먼트의 인덱스를 나타내는 튜플 목록 (끝 인덱스는 불포함)\n    \"\"\"\n    best_segments = []\n    total_length = 0\n    rv_index = 0\n    bad_rv_indices = []\n    while total_length < overall_max_length:\n        # 쿼리를 순환합니다\n        if rv_index >= len(all_relevance_values):\n            rv_index = 0\n        # 쿼리 중 더 이상 유효한 세그먼트가 없는 경우 작업을 완료합니다\n        if len(bad_rv_indices) >= len(all_relevance_values):\n            break        \n        # 이미 이 쿼리에 대해 더 이상 유효한 세그먼트가 없음을 결정했는지 확인하고 해당 경우 건너뜁니다\n        if rv_index in bad_rv_indices:\n            rv_index += 1\n            continue\n        \n        # 이 쿼리에 대해 최상의 남은 세그먼트를 찾습니다\n        relevance_values = all_relevance_values[rv_index]  # 해당 쿼리의 관련성 값 가져오기\n        best_segment = None\n        best_value = -1000\n        for start in range(len(relevance_values)):\n            # 음수 값 시작 지점 건너뜁니다\n            if relevance_values[start] < 0:\n                continue\n            for end in range(start+1, min(start+max_length+1, len(relevance_values)+1)):\n                # 음수 값 끝 지점 건너뜁니다\n                if relevance_values[end-1] < 0:\n                    continue\n                # 이 세그먼트가 최상의 세그먼트 중 어느 것과도 겹치는지 확인\n                if any(start < seg_end and end > seg_start for seg_start, seg_end in best_segments):\n                    continue\n                # 이 세그먼트가 문서 분할 중 어느 것과도 겹치는지 확인\n                if any(start < split and end > split for split in document_splits):\n                    continue\n                # 이 세그먼트가 전체 최대 길이를 초과할 것 같은지 확인\n                if total_length + end - start > overall_max_length:\n                    continue\n                segment_value = sum(relevance_values[start:end])  # 세그먼트 값을 그 청크의 관련성 값 합으로 정의\n                if segment_value > best_value:\n                    best_value = segment_value\n                    best_segment = (start, end)\n        \n        # 유효한 세그먼트를 찾지 못한 경우 해당 쿼리를 마쳤다는 표시를 하고 진행\n        if best_segment is None or best_value < minimum_value:\n            bad_rv_indices.append(rv_index)\n            rv_index += 1\n            continue\n\n        # 그렇지 않은 경우, 최상의 세그먼트 목록에 세그먼트를 추가합니다\n        best_segments.append(best_segment)\n        total_length += best_segment[1] - best_segment[0]\n        rv_index += 1\n    \n    return best_segments\n```\n\n<div class=\"content-ad\"></div>\n\n# 통찰과 생각\n\n## 알고리즘과 자료 구조 비교\n\nRAPTOR는 클러스터링을 통해 트리와 유사한 데이터 구조를 생성하고 이 구조를 기반으로 검색을 수행합니다.\n\nGraph RAG와 HippoRAG 모두 지식 그래프를 활용하지만 약간의 차이가 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 데이터 구조 관련하여, Graph RAG는 지식 요소를 요약하여 정보를 통합합니다. 그러므로 새로운 데이터가 추가될 때마다 요약 프로세스를 반복해야 합니다. 이는 RAPTOR에도 적용됩니다. 그러나 HippoRAG는 단순히 지식 그래프에 엣지를 추가함으로써 새로운 지식을 손쉽게 통합할 수 있습니다.\n- 검색 알고리즘 관점에서, Graph RAG는 커뮤니티 감지에 의존하며, HippoRAG는 개인화 페이지랭크 (PPR) 알고리즘을 활용합니다.\n\n그 외의 다른 요소들과 달리, spRAG는 고급 데이터 구조를 사용하지 않습니다. 각 청크에 문서 요약과 파일 이름을 추가한 후, 관련성 값에 기반한 검색을 실행합니다. 이는 spRAG의 색인 및 쿼리 속도가 가장 빠를 것을 시사합니다.\n\n## 성능에 대해\n\nHippoRAG는 실험을 수행하여 기준으로 삼은 RAPTOR를 능가하는 결과를 보여주었습니다. Figure 19에 나와 있는 것처럼요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_16.png\" />\n\n그래프 RAG 논문에는 성능 비교 실험이 포함되어 있지 않습니다.\n\n또한 현재 spRAG에 대한 논문이 없습니다.\n\n## 향상된 범위에 대해\n\n<div class=\"content-ad\"></div>\n\n네 가지 방법 — RAPTOR, Graph RAG, HippoRAG, 그리고 spRAG — 는 전체 말뭉치의 이해를 향상시키기 위해 노력합니다.\n\n각각은 전체 말뭉치를 기반으로 데이터 구조를 구축합니다.\n\n## 사용자 정의 가능성에 대해\n\n이 문맥에서 HippoRAG는 모든 구성 요소가 오프더셸프이기 때문에 추가 교육이 필요하지 않아 Figure 20에 나와 있는 것처럼 더 우수합니다.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_17.png\" />\n\n따라서, 특정 구성 요소를 섬세하게 조정함으로써, 개선의 상당한 잠재력이 있습니다.\n\n# 결론\n\n본문은 코드 설명을 보충하여 문서나 말뭉치의 전통적인 RAG의 전역 이해력을 향상시키기 위한 네 가지 새로운 방법을 소개합니다. 또한 제 개인적인 통찰과 생각도 포함되어 있습니다.\n  \n\n<div class=\"content-ad\"></div>\n\nRAG에 관심이 있으시다면 다른 내 기사들도 살펴보세요.\n\n또한, 최신 기사들은 제 뉴스레터에서 확인할 수 있습니다.\n\n마지막으로, 오류나 생략된 부분이 있거나 공유할 생각이 있다면 댓글 섹션에서 자유롭게 토론해 주세요.","ogImage":{"url":"/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_0.png"},"coverImage":"/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_0.png","tag":["Tech"],"readingTime":24},{"title":"챗GPT 디자인 해킹 - 사용자의 99가 이것을 모른다","description":"","date":"2024-06-19 03:21","slug":"2024-06-19-ChatGPTDesignHack99ofUsersDontKnowThis","content":"\n\n## 챗GPT를 5성급 그래픽 디자이너로 변신시키기\n\n챗GPT(GPT-4o)가 진지한 그래픽 디자이너 업그레이드를 받았어요. 이게 아직 작동하는 건지 확실하지 않아요...이렇게 사용된 걸 본 적이 없어요.\n\n![이미지](/assets/img/2024-06-19-ChatGPTDesignHack99ofUsersDontKnowThis_0.png)\n\n챗GPT를 그래픽 디자이너로 활용하는 방법에 대해 YouTube 동영상을 만들었어요. 프롬프트만으로 YouTube 섬네일, 소셜 미디어 그래픽 등을 만들어보세요.\n\n<div class=\"content-ad\"></div>\n\n이 튜토리얼에서는 ChatGPT를 Photoshop이나 Canva와 유사한 디자인 도구로 변환하는 방법을 보여 드리겠습니다. ChatGPT만을 이용하여 아래에서 보는 배경 이미지와 같은 멋진 디자인을 만드는 방법을 배울 수 있습니다.\n\n![ChatGPT Design](/assets/img/2024-06-19-ChatGPTDesignHack99ofUsersDontKnowThis_1.png)\n\n이제 레이어와 오버레이를 추가할 수 있습니다.\n\n이 디자인 핵은 무한한 가능성을 여는 방법입니다. 오랜 시간 동안 시도해 왔습니다.\n\n<div class=\"content-ad\"></div>\n\n이번 업데이트는 엄청 커요.\n\n이전에는 DALL·E-3에서 그래픽을 만들 수 있었지만, 그것들은 레이어가 없는 이미지였어요. ChatGPT에게 재밌고 흥미로운 디자인과 그래픽을 만들라고 할 수 있었지만, 그것들은 평면적이었어요. 아무것도 변경할 수 없었죠.\n\n어떤 것이든 설계한 적이 있다면, 포토샵이나 캔바와 같은 도구에서 디자인이 이루어지는 곳이 레이어인 것을 아실 거예요.\n\n이것들이 없으면 굉장히 제한되어요.\n\n<div class=\"content-ad\"></div>\n\n위의 YouTube 썸네일은 여러 디자인 자산을 사용하여 레이어와 오버레이를 만들기 위해 프롬프트를 사용해 완전히 생성되었습니다.\n\n배경, 로고, 제 자신의 cutout 배경 이미지 및 폰트(Open Sans)를 포함한 이미지를 프롬프트만으로 만들었습니다. 심지어 폰트 픽셀 크기까지 정의했죠.\n\n마진 및 패딩 CSS 규칙으로 완전히 원하는 대로 만들 수 있습니다.\n\n# 전체 YouTube 썸네일 만드는 방법입니다\n\n제목 | 설명\n----|----\nStep 1 | 배경과 로고 추가\nStep 2 | 자신의 cutout 이미지 추가\nStep 3 | 폰트(Open Sans) 적용\nStep 4 | 마진과 패딩 CSS 규칙 적용\n\n<div class=\"content-ad\"></div>\n\n시작하기 전에 몇 가지 에셋이 필요합니다:\n\n- Cutout Image: 투명 배경이 있는 PNG 이미지, 자신이나 다른 주제의 이미지입니다. 이미 가지고 있지 않은 경우 무료 온라인 사진 편집 도구를 사용하여 만들 수 있습니다.\n- Logo: 투명 배경이 있는 PNG 형식의 브랜드 로고입니다.\n\n이 튜토리얼에서 제가 사용한 것은:\n\n- 투명 배경을 가진 내 사진\n- AI Growth Guys 로고 (여러분의 로고 선택)\n\n<div class=\"content-ad\"></div>\n\n아래는 전적으로 프롬프트에 의해 생성되었습니다. 프롬프트를 사용하여 원하는 레이어나 에셋을 추가할 수 있습니다.\n\n## 배경 차원 설정하기\n\n- 배경 크기 지정: ChatGPT에게 이미지의 크기를 알려주는 것으로 시작하세요. 이 예시에서는 1920x1080 픽셀의 검은색 배경을 사용했습니다.\n- 텍스트 추가: ChatGPT에게 이미지에 텍스트를 추가하도록 지시하세요. 예를 들어, \"상단 좌측에 흰색 120픽셀 굵은 Open Sans 글꼴을 넣고 '나는 ChatGPT를 포토샵으로 변신시킨다'라고 작성해주세요.\"\n- 컷아웃 이미지 추가: 그다음 컷아웃 이미지를 업로드하세요. ChatGPT가 이를 왜곡하지 않고 전체 이미지를 사용하도록 지정해야 합니다.\n\n## 로고 배치하기\n\n<div class=\"content-ad\"></div>\n\n- 초기 배치: 먼저 ChatGPT에게 로고를 왼쪽 하단에 배치하도록 지시하세요. 너무 크게 나오거나 원하는 위치에 없다면 크기와 위치를 조정해보세요.\n- 크기 조정: 로고가 너무 크다면 ChatGPT에게 원하는 크기로 조절하도록 요청하세요. 저는 원본 크기의 25%로 조정했고, 왼쪽 하단에 배치했습니다.\n\nChatGPT는 지침과 HTML을 이해합니다. 그래서 간격과 크기에 관한 일반적인 규칙을 말할 수 있어요.\n\n# 마지막 손질 — 몇 번의 반복이 필요할 거예요.\n\n- 색상 조정: 텍스트를 두드러지게 하기 위해 색상을 변경할 수 있어요. 예를 들어, 'Photoshop'이라는 단어를 빨간색으로 바꾸도록 ChatGPT에게 요청했고, 나머지 텍스트는 흰색으로 유지했어요.\n- 필요한 경우 재배치: 요소들이 예기치 않게 움직일 때, ChatGPT에게 재배치하도록 요청하세요. 예를 들어, 'Photoshop' 단어는 원래 위치에 유지하고 색상만 변경하도록 ChatGPT에게 지시했어요.\n- 또한 ChatGPT에게 텍스트를 여러 줄로 배치하도록 지시해야 했어요. 처음에는 전체 텍스트가 한 줄에 있었는데, 맞지 않았어요. 한 번의 지시만으로 두 줄로 변경하도록 지시했어요.\n\n<div class=\"content-ad\"></div>\n\n# 최종 조정 및 내보내기\n\n모든 조정을 마친 후 멋진 최종 이미지가 나와야 합니다. 이를 소셜 미디어, YouTube 썸네일 또는 다른 창의적인 프로젝트에 활용할 수 있습니다.\n\n마지막에 문제가 생겼었어요. ChatGPT가 제 모든 레이어에 지쳐서 제 프롬프트 스레드를 깨는 것 같더라구요. ㅋㅋ\n\n하지만 멋지게 작동했어요. 더 나아질 것입니다.\n\n<div class=\"content-ad\"></div>\n\n저는 ChatGPT의 사용 방법, AI를 활용한 디자인, 그리고 온라인 비즈니스 성장에 대해 가르치고 있어요.\n\n저의 👉 AI Growth Guys Newsletter 👈를 확인해보세요!\n\n아래에서 다른 채널도 보실 수 있어요.\n\n우리 YouTube 채널도 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n당사 웹사이트에서 저희를 팔로우해주세요: AI Growth Guys","ogImage":{"url":"/assets/img/2024-06-19-ChatGPTDesignHack99ofUsersDontKnowThis_0.png"},"coverImage":"/assets/img/2024-06-19-ChatGPTDesignHack99ofUsersDontKnowThis_0.png","tag":["Tech"],"readingTime":4},{"title":"닉이 말하는데, 무료로 ChatGPT로 시간당 145를 벌 수 있다고 해요 방법을 알려드리겠습니다","description":"","date":"2024-06-19 03:20","slug":"2024-06-19-NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow","content":"\n\n## 정말 해볼 가치가 있는 방법인가요?\n\n![image](/assets/img/2024-06-19-NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow_0.png)\n\n온라인으로 돈을 벌기 위한 방법에 대한 많은 YouTube 동영상을 시청했었는데요,\n\n하지만 대부분이 완전한 헛소리라는 걸 알게 되어서 오랜 시간을 멈추었어요.\n\n<div class=\"content-ad\"></div>\n\n어제는 Dave Nick이라는 유튜버의 영상을 보게 되었는데, 그 영상에서 \"무료로 ChatGPT로 시간당 $145 벌 수 있다\"고 말했어요.\n\n그래서 이 영상에 대한 간단한 리뷰 기사를 쓰기로 결정했어요. 이것을 통해 여러분이 이것을 시도해볼 가치가 있는지 여부를 알 수 있을 거예요.\n\n영상을 확인하고 싶다면 여기로 이동해주세요:\n\n## 방법은 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n닉은 쓰여진 글 스레드를 통해 X에 대한 무료 책을 제공하여 온라인으로 돈을 벌 수 있는 방법을 제안합니다.\n\n아래는 작동 방식입니다:\n\n1. \"Audible\"이라는 프로그램에 등록합니다.\n2. 이 프로그램은 창작자 프로그램을 제공하여 다른 사람들을 그들의 웹 사이트에서 무료 체험에 등록하도록 초대할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이번 시범을 신청하면 초대한 사람에게 5달러의 수수료를 받게 됩니다.\n\n당신의 업무는 간단합니다:\n\n무료 eBook을 방문자들에게 홍보하여 시험 가입을 유도하고 그에 대한 보상금을 받는 것입니다.\n\n![이미지](/assets/img/2024-06-19-NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow_1.png)\n\n<div class=\"content-ad\"></div>\n\n나는 솔직히 이것이 돈을 버는 똑똑하고 효과적인 방법이라고 생각해요,\n\n하지만,\n\n이미 많은 관객이 있다면 그게 가능해요.\n\n## 링크로 트래픽을 어떻게 받을 것인가:\n\n<div class=\"content-ad\"></div>\n\n이제, 큰 질문이 있어요:\n\n그 제휴 링크로 트래픽을 어떻게 유도할까요?\n\n음, \n\n닉은 그것에 대한 전략을 갖고 있어요.\n\n<div class=\"content-ad\"></div>\n\n그는 ChatGPT와 얼티밋 X 트위터 콘텐츠 크리에이터를 사용하는 것을 제안합니다.\n\n![이미지](/assets/img/2024-06-19-NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow_2.png)\n\nChatGPT를 사용하면 인기 있는 책 카테고리를 파악하고 이러한 책에서 주요 통찰을 요약한 흥미로운 트위터 스레드를 작성할 수 있습니다.\n\n이 스레드는 두 가지 목적을 제공합니다:\n\n<div class=\"content-ad\"></div>\n\n그들은 귀하의 관객에 가치를 제공하고 그 안에 포함된 제휴 링크를 신속하게 홍보합니다.\n\n## 하지만 우리 진지해져 봅시다.\n\nTwitter`X`에서 큰 팔로워들을 구축하는 데는 시간과 노력이 필요합니다.\n\n그러니까, 처음부터 시작한다면,\n\n<div class=\"content-ad\"></div>\n\n결과를 실제로 보기까지는 시간이 걸릴 수 있어요.\n\n그 방법은 희망적으로 들리지만, 어떠한 방식으로도 금방 부자가 될 수 있는 계획은 아니에요.\n\n요구되는 것:\n\n헌신성,\n\n<div class=\"content-ad\"></div>\n\nConsistency,\n\n트위터와 같은 소셜 미디어 플랫폼에서 인기를 얻기 위해서는 일정함과 약간의 행운이 필요합니다.\n\n![NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow](/assets/img/2024-06-19-NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow_3.png)\n\n## 대신 해야 할 일\n\n<div class=\"content-ad\"></div>\n\n트위터만 사용하는 것이 아닌 다른 방법들도 시도해보세요.\n\n광고하고 싶은 책에 대한 블로그나 웹사이트를 시작해보세요.\n\n리뷰와 유용한 기사를 작성하여 해당 책에 관심 있는 사람들을 끌어들일 수 있어요.\n\n또한 사이트를 방문한 사람들로부터 이메일 주소를 수집하고 유용한 콘텐츠와 특별 혜택을 보내줄 수도 있어요.\n\n<div class=\"content-ad\"></div>\n\nAudible 홍보만 하는 게 아니라, 더 좋은 혜택을 제공하는 다른 회사를 찾아보세요.\n\n더 많은 수익을 올리거나 여러분의 대상 독자들이 더 선호할 수 있는 제품을 제공하는 제휴 프로그램이 있을 수도 있습니다.\n\n여러 가지 방법을 시도하고 새로운 아이디어에 열려 있으면, 여러분에게 가장 잘 맞는 온라인 수익을 창출할 수 있는 최선의 방법을 찾을 수 있습니다.\n\n## 요약하여 말하자면\n\n<div class=\"content-ad\"></div>\n\n닉의 전략이 똑똑한 것은 사실이지만, 실제로 돈을 벌기 전에 트위터에서 이상적인 청중을 유치해야 합니다.\n\n솔직히 말하자면, 이 방법으로 매 시간 145달러를 벌 수 있다고 생각하지 않아요.\n\n아마도 한 달에 수백 달러를 벌 수도 있지만, 그것도 이미 청중이 있는 경우에 한합니다.\n\n내 의견으로는, 이미 청중이 없다면 이 방법을 시도하지 않는 것이 좋다고 생각해요.\n\n<div class=\"content-ad\"></div>\n\n위 글을 읽어주셔서 감사합니다❤\n\n유튜브에서 닉 데이브의 채널을 꼭 확인해보세요. 온라인으로 돈을 벌기 위한 다양한 방법을 나누고 있어요.\n\n또한, 이 글은 이 플랫폼의 다른 작가인 브리트 말카의 영감을 받았어요 🦊\n\n꼭 그녀의 페이지도 방문해보세요.","ogImage":{"url":"/assets/img/2024-06-19-NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow_0.png"},"coverImage":"/assets/img/2024-06-19-NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow_0.png","tag":["Tech"],"readingTime":3}],"page":"95","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true}