{"pageProps":{"posts":[{"title":"기술이 당신의 버튼을 누르지 못하게 하세요","description":"","date":"2024-06-19 02:01","slug":"2024-06-19-DontLetTechnologyPushYOURButtons","content":"\n\n## 테크놀로지 중독과 정신 건강\n\n오늘 아침, 화면을 응시하는데 하루에 얼마나 많은 시간을 보내는지 생각했습니다. 핸드폰, 랩톱, 텔레비전을 오가면서, 거의 항상 화면을 쳐다보고 있네요. 얼마나 오랫동안 기술을 좋아했는지를 생각해보니 자주 월요일을 떨었어요.\n\n일주일에 보내는 시간을 생각하면 전세계적으로 촘촘하게 서로 마주보고 있는 세 개의 화면을 볼 때 오는게 두려울정도에요. TV, 랩탑, 핸드폰 중심으로 쓰는 일상적인 시간들 이건 거의 나의 대부분 일을 차지하고 있다고 생각해보면, 정말 슬픈 일이에요.\n\n매체를 보는것은 대부분 그런 시간을 잡아먹게 해요. 하루에 두세개의 글을 작성하고 게시하면서 세 개의 퍼블리케이션을 유지하고, 다른 작가들의 작품을 읽으려 노력하고, 가능한 모든 의견에 대한 박수와 응답을 해야하는 일들이 많아진답니다.\n\n<div class=\"content-ad\"></div>\n\n이른 아침에 일어나서 바로 충전 독에서 폰을 집어 들었어. 메일, SNS 알림, 그리고 텍스트를 확인하면서 시작했지. 옆에서 자는 작은 강아지가 움직이는 걸 느꼈어, 그래서 그녀를 살펴보니 평화롭게 자는 걸 볼 수 있었어.\n\n그녀는 정말 만족스럽고 행복해 보였지. 폰을 내려놓고 내 팔을 그녀 주위로 감았어. 그녀가 몸을 뻗고 더 가까이 왔어. 이른 아침의 스크린 시간에서 조금 떨어져 그녀와 함께 있으면서 그녀에게 더 많은 안아줌을 주는 것을 즐겼어.\n\n리비는 아침에 바로 폰을 확인할 필요가 없어. 화면을 통해 자신에게 관심을 가져가려는 사람들의 압력에 구속되어 있지 않아. 그녀는 얼마나 많은 이메일에 답장해야 하는지, 혹은 자신의 미디엄 통계가 어떻게 보이는지 걱정하지 않아.\n\n그녀를 통해 스크린 타임이 언제나 최선은 아니라는 것을 깨달았어. 사랑하는 이에게 주의를 기울이는 게 훨씬 좋다는 걸 깨달았어. 내가 지난 밤 가족과 오게 될 두 명의 손님을 위해 저녁을 요리할 때를 떠올려봤을 때도 마찬가지야. 음악에 맞춰 노래를 부르며, 카베르네를 한잔하며, 그리고 손에 잡히는 일에 집중했던 그 때가 생각났어.\n\n<div class=\"content-ad\"></div>\n\n![2024-06-19-DontLetTechnologyPushYOURButtons_0](/assets/img/2024-06-19-DontLetTechnologyPushYOURButtons_0.png)\n\n새우 타코와 마늘 볶음밥을 만들며 미디엄, 소셜 미디어, 그리고 디스코드에서 무슨 일이 일어나고 있는지 걱정하는 대신 보는 것이 좋았어요. 모든 것은 나중에 해도 괜찮았거든요. 나는 현재에 더 즐거운 저녁을 보내는 것을 통제하고 있었습니다. 기술이 아니라구요.\n\n기술과의 문제는 어린 시절에 시작했어요. 부모님께서는 예전의 구식 게임 시스템 중 하나를 갖고 계셨어요. 아마도 Pong이나 Tank라는 것의 시어스 버전일 거에요. 우리가 처음 중독되기 시작했던 건 그 때였죠.\n\n어린이들은 어릴 적부터 화면에 끌리죠. 우리에게는 텔레비전이 첫 번째 화면이었어요. 제가 어릴 적 즈음에는 태블릿, 노트북, 핸드폰 같은 것들이 없었어요. 70년대와 80년대에는 만화를 담고 있는 마법 상자가 저희가 숭배했던 것이었어요.\n\n<div class=\"content-ad\"></div>\n\n우리가 처음 Atari 2600으로 진화하면서 많이 변하지 않았어요. 아버지께서는 우리가 그 초기에 기본 기술에 감정적으로 연결되는 것을 느끼셨던 것 같아요. 우리를 매일 비디오 게임 한 시간으로 제한했죠. 때때로 우리가 착하게 행동을 하거나 아버지가 우리의 소년 신체 활동에서 쉬어야 할 때만 두 시간까지 가능했어요.\n\n첫 번째 닌텐도 콘솔의 등장과 그 뒤를 이어오는 모든 게임 콘솔은 기술이 우리에게 끝없이 매료하게 했어요. 우리가 하고 싶은 일은 공주를 구하고 외계인을 격추하는 것뿐이었어요. 게임을 하며 대부분의 시간을 보내고 엔딩을 찾으려고 노력했던 것은 꽤 흔한 일이었어요.\n\n처음으로 인터넷에 연결할 수 있게 된 것은 기술에 대한 불건전한 연결까지의 또 다른 단계였어요. 다른 도시에 살고 있던 우리 형제로부터 즉시 메시지를 받을 수 있었죠. 웹 TV는 우리를 초기 웹페이지와 정보에 소개해 주었어요.\n\n![이미지](/assets/img/2024-06-19-DontLetTechnologyPushYOURButtons_1.png)\n\n<div class=\"content-ad\"></div>\n\n처음으로 온 디맨드로 노출된 여자들을 봤어. 그런데, 사진 하나를 다운로드하는 데 수 분이 걸렸지. 응, 기다릴 가치가 있었어. 7-11에서 잡지를 사는 것보다 훨씬 낫더라고. 거기서 알고 있는 사람들에게 알려지는 것을 피할 수 있었으니까.\n\n더 많은 기술적 발전은 화면을 응시하는 시간이 늘어난 동안 우리를 쥐고 있었어. 우리는 웹 TV보다 빠르게 돌아가고 음악을 다운로드하고 CD를 불 수 있는 PC를 샀지. 내가 즐겨듣는 음악가들의 명반들과 내가 좋아하는 영화들을 수십 개의 앨범으로 만드는 데 너무 많은 시간을 보냈어.\n\n가끔 나프스터, 카자, 라임와이어와 같은 초기 사이트들은 내 컴퓨터에 임질균을 걸어놨어. 그래도 그것을 치루는 건 작은 대가였지.\n\n스마트폰과 소셜 미디어는 우리 중독을 깊이 깊이 키웠어. 갑자기 알림음이 좌지우지하는 거야. 게임 데스크에 묶여 있지 않아도 누군가가 너에게 주목하려고 노력 중이라는 걸 알 수 있었어.\n\n<div class=\"content-ad\"></div>\n\nFOMO가 현실이 되었습니다. 새로운 문자, 이메일 또는 소셜 미디어 알림이 있는지 알려주는 벨이나 종 소리를 듣기 위해 핸드폰 알림을 켰습니다. 이 소리들을 사용자화해서 어떤 앱이나 누구가 주의를 끌고 있는지 알 수도 있었습니다.\n\n그리고 그들은 당신의 주의를 사로 잡았습니다.\n\n만약 제가 생각하는 것과 비슷하다면, 화면 시간이 당신의 삶을 지배하는 경향이 있다면, 한 번 생각해보세요. 이 기사를 읽는 핸드폰을 내려놓으세요. 랩탑을 닫아서 몇 분 동안 쉬세요. 조용히 앉아서 이러한 아이디어를 곰곰이 생각할 수 있는 능력을 활용하세요.\n\n<div class=\"content-ad\"></div>\n\n나도 그렇게 할 거야. 우리가 살고 있는 매우 연결된 세계에서 좀 더 분리하는 더 좋은 일을 하려고 계획 중이야. 내 다리에 기대어 누워 있는 모치가 낮잠 자는 걸 보고, 이 글을 게시한 후에 그녀에게 줄 안아줌과 관심을 즐기리라고 확신해. 연휴 동안 낮에 기술로부터 자주 쉬는 시간을 가져갈 것이 내 목표일 거야.\n\n최근에 이 목표를 향해 몇 가지 조치를 이미 취했어. 요즘에는 핸드폰을 고요 모드로 유지하고 있어. 계속된 알림 소리를 듣지 않는 것이 참으로 훌륭해. 내가 체크하는 게 아니라 기술 장치을 제어하는 게 훨씬 쉬워.\n\n내 핸드폰의 모든 앱이 푸시 알림을 보내야 하는 것은 필요하지 않아. 대부분의 소셜 미디어 및 이메일 알림을 해제했어. 이런 것들은 내가 생각할 때 확인하는 게 훨씬 나아, 계속해서 공격당하는 것보다.\n\n이미 더 많은 평화를 느끼고 있어. 부재 중인 두려움이 있어. 평화와 조용함, 친구들과 가족들과 직접 대면하는 순간을 놓치는 것에 대한 두려움이야. 기술이 내 존재를 소비하지 않을 거야. 나는 지금 통제권을 갖고 있어. &:^)\n\n<div class=\"content-ad\"></div>\n\n2024년 Jason Provencio. 모든 권리 보유.","ogImage":{"url":"/assets/img/2024-06-19-DontLetTechnologyPushYOURButtons_0.png"},"coverImage":"/assets/img/2024-06-19-DontLetTechnologyPushYOURButtons_0.png","tag":["Tech"],"readingTime":4},{"title":"AIoT의 힘 탐구 세계적으로 비즈니스 혁신 중","description":"","date":"2024-06-19 01:58","slug":"2024-06-19-ExploringthePowerofAIoTReshapingBusinessesWorldwide","content":"\n\n당신은 개발자이세요. 위의 텍스트를 친근한 어조로 한국어로 번역해 주실 수 있나요?\n\n<div class=\"content-ad\"></div>\n\n# AIoT이란 무엇인가요?\n\nAIoT(Things의 인공지능)는 인공지능(AI)과 사물 인터넷(IoT)의 강력한 융합입니다. 이 시너지는 AI의 분석 능력과 IoT의 연결된 장치 네트워크를 결합합니다. 본질적으로, AIoT는 IoT 장치를 통해 수집된 데이터를 분석하여 기계에 더 스마트한 의사 결정 능력을 제공합니다.\n\nAIoT의 핵심은 AI를 직접 IoT 인프라로 통합하는 개념으로, 데이터를 수집하는 것뿐만 아니라 분석하고 학습하는 더 지능적인 네트워크를 구축합니다. 이 통합은 기계 학습 및 딥 러닝과 같은 AI 능력의 배포를 가능하게 하며, 데이터를 실제로 수집된 곳인 네트워크의 가장자리에 배치합니다. 이를 가장자리 인공지능 또는 가장자리 인텔리전스라는 개념이라고 합니다. 이 접근 방식은 데이터 처리의 효율성, 견고성 및 확장성을 향상시킵니다. \n\n우리 Viso.ai 팀은 시각 AIoT의 잠재력을 활용하며 컴퓨터 비전과 IoT를 결합합니다. 우리는 Viso Suite를 관리하며, 본 플랫폼은 대규모 AIoT 애플리케이션의 개발, 배포 및 운영을 지원하며 노코드 및 로우코드 옵션을 제공합니다. 이 플랫폼은 선두 기업이 AI 비전 응용프로그램의 잠재력을 효과적으로 활용할 수 있도록 도와주는 중요한 역할을 합니다.\n\n<div class=\"content-ad\"></div>\n\n# AIoT의 필요성 이해하기\n\nAI와 IoT가 어째서 변혁적인 조합인지 이해하기 위해서는 각 기술이 가져다주는 독특한 장점을 고려하는 것이 중요합니다. AI는 인간의 지능 - 지각, 추론 및 이해를 모방할 수 있는 시스템을 만드는 것입니다. 이러한 능력은 다양한 산업 분야에서 지능적인 시스템을 개발하기 위해 중요하며, 제품과 서비스 개발에서 효율성을 높이고 혁신을 촉진하는 데 필수적입니다.\n\n반면에 IoT는 연결된 기기 네트워크를 포함하며, 이 기기들은 실시간으로 데이터를 수집하고 전송합니다. 센서나 사용자 입력 인터페이스로 구성된 이 기기들은 방대한 양의 데이터를 생성합니다. AI를 통합함으로써 이 데이터를 지능적으로 처리할 수 있게 되어, IoT 기기들은 데이터를 수집하고 전송하는 것뿐만 아니라 이를 이해하고 분석할 수 있습니다. 이를 통해 IoT의 기능이 단순한 데이터 기록에서 지능적인 의사 결정과 자동화된 조치로 높아지게 되어, 다양한 응용 프로그램에서 자동화 및 효율성을 크게 향상시킵니다.\n\n# AI와 IoT의 차이점 이해하기\n\n<div class=\"content-ad\"></div>\n\nAI와 IoT는 각각 독특한 기능과 초점을 갖고 있지만, 그들의 협업은 디지털 시대에 혁신적인 힘을 만들어냅니다. AI와 IoT의 차이점과 보완적인 성질을 이해하는 것은 혁신과 효율성을 위해 결합된 잠재력을 활용하려는 기업과 산업에게 중요합니다.\n\n## 독특한 초점\n\nAI는 인간과 유사한 의사 결정과 문제 해결을 모방할 수 있는 지능적인 알고리즘과 시스템을 만드는 데 중점을 두고 있습니다. 학습, 추론, 문제 해결과 같은 인지적 과정에 집중합니다.\n\n반면에 IoT는 물리적 장치들을 인터넷에 연결하거나 서로 연결하여 데이터를 수집, 전송 및 교환하는 데 관심이 있습니다. 이는 물리적 세계의 물건과 센서들의 네트워킹을 강조합니다.\n\n<div class=\"content-ad\"></div>\n\n# 기능\n\nAI는 장치나 시스템이 데이터를 해석하고 그로부터 학습하며 자율적인 결정을 내릴 수 있게 합니다. 이는 장치가 환경을 이해하고 대응할 수 있게 하는 지능을 추가합니다.\n\nIoT 장치는 주로 센서와 액추에이터로부터 데이터를 수집하고 전송합니다. 이를 통해 물리적 세계에 대한 정보를 수집하는 수단을 제공합니다.\n\n# 데이터 사용\n\n<div class=\"content-ad\"></div>\n\nAI는 훈련 및 알고리즘 개선을 위해 데이터에 많이 의존합니다. 패턴을 인식하고 신중한 결정을 내리기 위해서는 대량의 데이터셋이 필요합니다.\n\nIoT는 AI가 활용할 수 있는 데이터를 생성합니다. IoT 장치는 물리적 세계에서 실시간 데이터를 수집하며, AI는 이를 분석하여 통찰을 얻고 행동을 안내할 수 있습니다.\n\n# 상호작용\n\nAI는 데이터와 상호작용하며 결정을 내립니다. AI는 연결된 장치가 생성한 데이터를 처리하고 분석하기 위해 IoT 시스템에 통합될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nIoT는 물리 세계와 상호 작용하여 센서와 장치에서 데이터를 수집합니다. 이 데이터를 해석하는 데 AI의 능력을 활용할 수 있습니다.\n\n# 보완적인 성격\n\nAI와 IoT의 진정한 힘은 함께 작동할 때 드러납니다. IoT는 AI 알고리즘을 지원하는 데이터를 제공하여 더 스마트하고 맥락과 함께 동작할 수 있도록 만듭니다. AI는 실시간 데이터 분석, 예방 정비 및 지능형 자동화를 가능하게 함으로써 IoT를 향상시킵니다.\n\nAI는 IoT에 지식을 더해 연결된 장치 네트워크 이상으로 발전시킵니다. 함께 사용하면 AIoT(인공 지능 사물 인터넷)라는 시너지를 만들어 데이터 기반 통찰력, 자율적 의사결정 및 물리 세계에 지능적으로 대응하는 기능을 실행할 수 있습니다. 이 상보적인 관계는 혁신, 효율성 및 다양한 산업 분야에서의 변화 가능성을 촉진합니다.\n\n<div class=\"content-ad\"></div>\n\n# AI와 IoT의 결합 혜택\n\n인공지능(AI)과 사물인터넷(IoT)의 융합은 증가된 운영 효율과 상당한 비용 절감 등 여러 혜택을 가져다줍니다.\n\n## 1. 증가된 운영 효율\n\nAIoT는 기계가 기계 학습 기술을 사용하여 데이터를 분석하고 패턴을 식별할 수 있도록 함으로써 운영 성능을 향상시킵니다. 이 능력은 운영 인사이트의 생성을 가속화할 뿐만 아니라 문제의 탐지와 해결을 향상시키고 이전의 수동적 작업의 자동화를 강화합니다. 예를 들어, AIoT는 산업 분야에서 시각 기반 품질 검사를 자동화하는 데 중요한 역할을 합니다. 카메라를 사용하여 품질 통제 및 마스크, 헬멧과 같은 필수 보호 장비의 감지와 안전 기준 준수와 같은 작업을 자동화합니다.\n\n<div class=\"content-ad\"></div>\n\n## 2. 향상된 고객 경험\n\nAIoT는 개인 맞춤형 반응성 상호작용을 제공하여 고객 서비스와 만족도를 혁신적으로 향상시킬 수 있습니다. 예를 들어, 소매업에서 AIoT는 고객 행동과 선호도를 추적하여 맞춤형 권장 사항 및 서비스를 제공할 수 있는 스마트 스토어를 가능하게 합니다.\n\n## 3. 실시간 모니터링 용이화\n\nAIoT 시스템에 의한 실시간 모니터링은 다운타임을 최소화하고 비용이 많이 드는 장애를 방지하는 데 도움이 됩니다. 이러한 시스템은 지속적으로 운영을 감시하여 이상을 식별하고 예측을 수행하며 자율적으로 결정을 내림으로써 더 빠르고 객관적인 결과를 제공합니다. 석유 및 가스와 같은 산업에서 AIoT 응용 프로그램은 카메라를 사용하여 원격 누출 탐지와 같은 안전과 효율유지에 중요한 역할을 합니다.\n\n<div class=\"content-ad\"></div>\n\n## 4. 확장성과 유연성\n\nAIoT 시스템은 데이터 양과 연결된 장치 수가 증가함에 따라 자연스럽게 확장될 수 있도록 설계되었습니다. 이러한 확장성은 기업이 운영을 확대할 수 있게 하며 비용이나 복잡성이 크게 증가하지 않도록 보장합니다.\n\n## 5. 운영 비용 절감\n\nAIoT는 자원 사용을 최적화하는 더 스마트한 시스템을 개발함으로써 운영 비용을 크게 낮추는 데 기여합니다. 예를 들어, 실시간 점유 데이터에 기초해 조명과 온도를 조절하는 스마트 건물 솔루션은 에너지 절약을 이끌어냅니다. 마찬가지로, 스마트 공장에서는 AIoT 장치가 예방 정비 및 기계 상태 모니터링에 중요한 역할을 하여 고장을 예방하고 잠재적으로 비용이 많이 드는 중단을 줄이는 데 도움을 줍니다.\n\n<div class=\"content-ad\"></div>\n\n## 6. 더 나은 의사 결정\n\nIoT에서 생성된 방대한 양의 데이터를 분석하는 능력으로, AIoT는 기업이 더 유익한 결정을 내릴 수 있도록 도와줍니다. 예측 분석과 기계 학습을 활용하여 기업은 시장 트렌드, 고객 요구사항 및 잠재적인 시스템 장애를 예측할 수 있습니다.\n\n## 7. 강화된 보안\n\nAIoT는 이상 현상 및 잠재적인 위협을 실시간으로 감지하여 보안 조치를 개선할 수 있습니다. 예를 들어, 스마트 감시 시스템은 비정상적인 활동이나 무단 접근을 식별함으로써 스마트 홈, 사무실 및 공공 공간의 보안을 향상시킬 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 8. 환경 모니터링과 지속 가능성\n\nAIoT 응용 프로그램은 환경 모니터링 및 지속 가능성 노력을 관리하는 데 중요합니다. AIoT는 오염 수준, 날씨 조건 또는 에너지 소비를 추적하여 더 환경 친화적인 관행과 정책을 시행하는 데 도움이 됩니다.\n\n## 9. 리스크 관리 향상\n\nAIoT는 잠재적 문제를 예측하고 사전에 해결함으로써 조직의 리스크 관리 능력을 향상시킵니다. 이는 물의 수위 모니터링 및 직원 안전 분석부터 공공 장소에서의 군중 관리까지 다양한 영역을 포함합니다. 또한 보험사들은 점점 더 AIoT를 활용하여 기계 및 전체 생산 시설의 위험을 평가하고 더 정확한 보험 관행을 실시할 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\nAI와 IoT의 혜택\n\n## 현대 시대의 주요 AI와 IoT 사례 및 사용\n\n이 섹션에서는 AI의 지능과 IoT의 연결력을 결합하여 다양한 산업의 도전에 대응하는 AIoT 솔루션을 소개하겠습니다.\n\n## 의료산업\n\n<div class=\"content-ad\"></div>\n\n원격 환자 모니터링: IoT 지능은 환자의 중요 신호와 건강 상태를 원격으로 지속적으로 모니터링하여 의료 접근성을 향상시키고 입원률을 줄입니다.\n\n개인 맞춤 의학: 여러분의 데이터가 맞춤형 의약품 계획을 제시하는 모습을 상상할 수 있나요? 이것이 우리가 문명이 시작된 이후로 갈망해온 것 아닌가요? 개인의 고유한 건강 데이터에 기반하여 치료 계획과 약물 투여량을 맞춤화하는 데 AIoT가 중요한 역할을 할 것입니다.\n\n예측 분석: 의료 제공자들은 시기적절한 질병 검출과 선행적으로 개입하기 위한 예측 분석을 위해 AIoT를 활용합니다.\n\n<div class=\"content-ad\"></div>\n\n예측 유지보수: 제조업에서 AIoT는 장비 고장을 예측함으로써 제조 작업을 최적화하는 데 도움을 줍니다. Siemens와 같은 기업들은 제조업에서 예측 유지보수를 위해 AIoT를 사용하여 공장이 장비 고장을 예방하고 다운타임을 최소화하는 데 도움을 줍니다.\n\n품질 관리: 어셈블리 과정 중에 결함을 감지할 수 있는 카메라가 있을까요? AIoT 기술이 적용된 카메라가 가능합니다! 대규모 전자제품 제조업체인 Foxconn은 실시간 모니터링과 자동 결함 감지를 통해 제품 품질 관리를 강화하는 데 AIoT 기술을 활용하고 있습니다.\n\n공급망 효율: 모든 가능한 자동화 과정으로 조직은 모든 단계에서 제품 및 자재를 추적하고 관리할 수 있어 공급망 가시성을 향상시킬 수 있습니다.\n\n# 농업\n\n<div class=\"content-ad\"></div>\n\n정밀 농업: 인공지능 사물 인터넷(AIoT) 기술은 스마트 센서, 드론, 자율 주행 차량 등을 활용하여 농업을 혁신했습니다. 이를 통해 정확한 자원 할당과 작물 관리가 가능해졌죠. 예를 들어, 정밀 농업 기술은 IoT 센서와 인공지능을 통합하여 작물 재배를 최적화하고 자원 낭비를 줄입니다.\n\n축산물 모니터링: 전통적인 일일 영양원의 신뢰성이 낮아지면서, 사육 농가에게 혁신적인 IoT 지능 기술이 최신 도우미 역할을 합니다. EID 판독기와 디지털 체중계와 같은 기술을 통해 가축의 건강과 행동을 모니터링할 수 있어요. 이러한 기술은 동물들을 최상의 건강 상태로 유지하는 데 도움이 됩니다.\n\n# 교통\n\n자율 주행 차량: 누구나 테슬라를 좋아합니다. AIoT 기술은 이러한 강력한 자율 주행 자동차의 주도적인 역할을 합니다. 이 자동차들은 럭셔리를 위해 만들어진 것뿐만 아니라 교통 안전성, 효율성 및 편의성을 향상시키기 위한 기술로 장착되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n교통 관리: 스마트 교통 시스템은 교통 흐름을 최적화하고 혼잡을 줄여 시간과 연료를 절약할 수 있습니다. 예를 들어 런던은 도시 전역의 중요한 위치에 교통 계수 센서를 설치했습니다. 이 데이터는 교통 흐름을 관리하고 혼잡을 완화하는 데 사용됩니다.\n\n# 스마트 도시\n\n인프라 관리: 핵심적으로 IoT 지능은 기업이 인프라 관리를 개선할 수 있도록 지원합니다. 에너지 효율적인 가로등부터 폐기물 관리까지. 예를 들어 지구 난방 디지털화는 부동산 소유자들에게 전력 비용을 절감시키고 난방 공급 업체들이 난방 네트워크를 최적화하는 데 도움이 됩니다.\n\n공공 안전: AIoT는 IoT 연결 카메라에 AI 기능을 추가하여 스마트 감시 시스템을 향상시킵니다. 물체 탐지, 얼굴 인식, 이상 감지 및 예측 분석이 가능합니다. 게다가 AIoT 기술로 구동된 감시 시스템은 실시간 경보, 교통 관리, IoT 센서와의 통합을 통해 보안 향상, 개인 정보 보호 및 다양한 응용 분야에서 효율성을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n# 소매\n\n개인화 쇼핑: 고객의 선호도와 행동에 기반한 제품을 추천하는 개인화된 쇼핑 경험은 IoT와 AI의 결합에 의해 주도되는 가장 주목할 만한 기능 중 일부입니다. 당신이 좋아하는 Netflix부터 FarFetch까지 - 모든 대형 또는 소규모 데이터 중심 기업은 AIoT의 결합된 힘을 활용하여 당신을 브랜드에 끌어들이고 있습니다.\n\n재고 관리: AIoT 기술을 활용한 실시간 재고 추적은 재고 부족 상황을 줄이고 과다 조달을 최소화합니다.\n\n# 에너지\n\n<div class=\"content-ad\"></div>\n\n스마트 그리드: 인공지능 사물 인터넷 (AIoT)은 에너지 분배를 최적화하고 전력 정전을 최소화하며 에너지 효율성을 향상시킵니다. 미국의 Pacific Gas and Electric (PG&E)는 AIoT를 사용하여 전력 그리드를 관리하며 공급과 수요를 더 효과적으로 균형을 맞춥니다.\n\n에너지 절약: 지능형 기후 제어 및 조명 솔루션을 사용하는 스마트 홈 및 건물은 에너지 소비량을 약 40% 정도 감소시켰습니다.\n\n# AIoT 구현의 도전과제\n\n- 데이터 보안: AIoT 시스템이 수집한 방대한 데이터의 프라이버시와 보안을 보장하는 것이 중요합니다. 강력한 보호 조치와 개인 정보 보호법을 준수해야 합니다.\n- 상호 운용성: 다양한 사물 인터넷(IoT) 장치는 종종 서로 다른 표준을 사용하여 원활한 통신을 복잡하게 만듭니다. 프로토콜을 표준화하는 것이 중요하지만 리소스를 많이 소비할 수 있습니다.\n- 데이터 관리: AIoT 장치가 생성하는 다양한 방대한 데이터를 처리하기 위해서는 고급 분석 및 확장 가능한 저장 솔루션이 필요합니다.\n- 초기 비용: AIoT 시스템을 구축하는 데는 기술 및 기술력에 상당한 투자가 필요하며, 예산에 민감한 조직에게는 어려울 수 있습니다.\n- 기술 인력 부족: AI 및 IoT에 숙련된 전문가에 대한 수요가 공급을 초과하여 교육이나 채용 노력이 필요합니다.\n- 윤리적 고려사항: AIoT를 실행할 때는 윤리적 사용 및 사회적 수용을 보장하기 위해 잠재적인 취업자 이동과 알고리즘적 편향을 고려해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n# AIoT의 훌륭한 미래가 기다립니다\n\n우리가 이 글을 통해 AIoT(인공지능 사물인터넷)의 놀라운 잠재력을 탐구하면서 보듯이, AI와 IoT의 시너지는 단순히 기술적인 발전 이상의 것이라는 사실을 알 수 있습니다. 오늘날의 역동적인 시장 환경은 신속하고 정보에 기반한 의사결정을 요구하며, AIoT는 그런 목표를 달성할 수 있는 도구를 제공합니다.\n\nAIoT의 힘을 활용하여 귀하의 비즈니스를 혁신과 경쟁력의 최전선에 위치시킬 수 있습니다. 단순히 살아남는 것뿐만이 아니라, 데이터 중심 시대에서 번창하는 것이 목표입니다.\n\nAIoT 기술 분야에서 신뢰할 수 있는 선도 기업인 Kanerika는 변화의 여정에서 여러분의 파트너입니다. 우리는 다양한 산업군의 기업들에게 AIoT 솔루션이 가져다주는 현실적인 혜택을 목격했습니다. 프로세스 효율화, 보안 강화, 의사결정 향상, 효율성 향상 등 다방면으로 이뤄지는 혜택들을 통해 AIoT는 미래 성공의 열쇠입니다.\n\n<div class=\"content-ad\"></div>\n\n그래요, 산업에서 선두주자가 되기를 원하신다면, AIoT로의 도약을 해보세요. 카네리카가 여러분을 모든 단계에서 지원할 준비가 되어 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-ExploringthePowerofAIoTReshapingBusinessesWorldwide_0.png"},"coverImage":"/assets/img/2024-06-19-ExploringthePowerofAIoTReshapingBusinessesWorldwide_0.png","tag":["Tech"],"readingTime":9},{"title":"SQL에서 Pivot 테이블하는 방법","description":"","date":"2024-06-19 01:55","slug":"2024-06-19-HowtoPivotTablesinSQL","content":"\n\n## 데이터 과학, SQL, ETL\n\n![이미지](/assets/img/2024-06-19-HowtoPivotTablesinSQL_0.png)\n\n# 서문\n\n구조화된 쿼리 언어(SQL)는 데이터 과학자와 데이터 분석가와 같은 데이터 전문가들에게 필수적인 도구입니다. SQL을 사용하면 대용량 데이터셋을 효율적으로 검색, 조작 및 분석할 수 있습니다. 이는 산업에서 널리 사용되는 도구로, 중요한 기술입니다. 이 글에서는 SQL에서 Pivot 테이블을 만드는 방법에 대해 공유하고자 합니다. 이 글은 지난 \"판다스!!! 첫 기술 면접 후 배운 것들\"이라는 글을 계속해서 다루고 있습니다. 그 글에서는 저의 판다스 학습 경험에 대해 공유한 내용입니다.\n\n<div class=\"content-ad\"></div>\n\nSQL에서 Pivot 테이블은 데이터를 행에서 열로 변환하는 기술로 사용됩니다.\n\nJoan Casteel의 오라클 12c: SQL 책에는 \"Pivot 테이블은 다차원 데이터의 표현입니다.\" 라고 언급되어 있습니다. Pivot 테이블을 사용하면 사용자는 다양한 데이터 차원의 다른 집계를 볼 수 있습니다. 데이터 분석에 강력한 도구로, 데이터를 집계, 요약 및 직관적이고 쉽게 읽을 수 있는 형식으로 제시할 수 있습니다.\n\n예를 들어, 아이스크림 가게 주인은 지난 주에 가장 잘 팔린 아이스크림 맛을 분석하고 싶어할 수 있습니다. 이 경우, 아이스크림 맛과 요일의 두 차원 데이터에 대한 Pivot 테이블이 유용할 것입니다. 매출은 분석을 위한 집계로 합산될 수 있습니다.\u001c\n\n아이스크림 가게 주인은 Pivot 테이블을 사용하여 아이스크림 맛과 요일에 따른 매출을 비교할 수 있습니다. Pivot 테이블은 데이터를 변환하여 패턴과 트렌드를 쉽게 발견할 수 있도록 도와줍니다. 이 정보를 통해 주인은 인기 있는 아이스크림 맛의 공급을 늘리거나 수요에 따라 가격을 조정하는 등 데이터 기반 결정을 내릴 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n전체적으로, 피벗 테이블은 데이터 분석에 탁월한 도구로, 사용자들이 다차원 데이터를 더 직관적이고 의미 있는 방식으로 요약하고 표현할 수 있습니다. 이들은 금융, 소매 및 의료 분야와 같은 산업에서 널리 활용되며, 복잡한 대량의 데이터를 분석해야 하는 경우에 사용됩니다.\n\n![이미지](/assets/img/2024-06-19-HowtoPivotTablesinSQL_1.png)\n\n# 개요\n\n본 문서는 Oracle의 분석 함수인 일반적으로 \"PIVOT\" 함수를 기반으로 합니다. 이는 SQL에서 피벗 테이블을 활용하는 다양한 상황에 대한 포괄적인 시각을 제공하기 위해 구성되어 있습니다. 우리는 피벗 테이블을 생성하는 가장 단순한 방법 뿐만 아니라 PIVOT 함수를 사용하여 작업을 수행하는 가장 쉽고 흔한 방법도 살펴볼 것입니다. 마지막으로, PIVOT 함수의 일부 제약 사항에 대해 논의할 예정입니다.\n\n<div class=\"content-ad\"></div>\n\n## 참고사항:\n\n- 저는 오라클 11g를 사용할 것이지만, 함수들은 최신 오라클 12c 이상에서도 동일합니다.\n- 데모 데이터 세트는 Microsoft의 Northwind 데이터 세트를 사용할 것입니다. Northwind Traders의 판매 데이터가 포함되어 있으며, 이는 허구의 특산품 수출/수입 회사입니다. 이 데이터베이스는 학습 및 데모 목적으로 무료로 제공되며 널리 배포되고 있습니다. 데이터베이스 환경을 미리 설정해 주세요! 아래에 Northwind 스키마를 첨부하였습니다:\n\n```js\nREGION (RegionID, RDescription)\nTERRITORIES (TerritoryID, TDescription, RegionID@)\nCATEGORIES (CategoryID, CategoryName, Description)\nSUPPLIERS (SupplierID, CompanyName, ContactName, ContactTitle, Address, City, Region, PostalCode, Country, Phone)\nCUSTOMERS (CustomerID, CompanyName, ContactName, ContactTitle, Address, City, Region, PostalCode, Country, Phone)\nSHIPPERS (ShipperID, CompanyName, Phone)\nPRODUCTS (ProductID, ProductName, SupplierID@, CategoryID@, QuantityPerUnit, UnitPrice, UnitsInStock, UnitsOnOrder, ReorderLevel, Discontinued)\nEMPLOYEES (EmployeeID, LastName, FirstName, Title, BirthDate, HireDate, Address, City, RegionID@, PostalCode, Country, HomePhone, Extension, ReportsTo@)\nEMPLOYEETERRITORIES (EmployeeID@, TerritoryID@)\nORDERS (OrderID, CustomerID@, EmployeeID@, TerritoryID@, OrderDate, RequiredDate, ShippedDate, ShipVia@, Freight, ShipName, ShipAddress, ShipCity, ShipRegion, ShipPostalCode, ShipCountry)\nORDERDETAILS (OrderID@, ProductID@, UnitPrice, Quantity, Discount)\n```\n\n- SQL*Plus에 익숙하지 않다면, 시작하기 전에 오라클의 SQL*Plus Quick Start를 확인해 주세요.\n\n<div class=\"content-ad\"></div>\n\n자세히 설명하지 않고, 시작해 봅시다!\n\n# \"DECODE\"를 사용한 피벗 테이블\n\n![이미지](/assets/img/2024-06-19-HowtoPivotTablesinSQL_2.png)\n\n테이블을 피벗하는 가장 기본적인 방법은 DECODE() 함수를 활용하는 것입니다. DECODE() 함수는 if else 문과 유사합니다. 입력값을 각 값과 비교하여 출력 값을 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n- 입력/값: \"입력\"은 모든 \"값\"과 비교됩니다.\n- 반환: 입력 = 값이면, \"반환\"이 출력됩니다.\n- 기본값 (옵션): 입력 ≠ 모든 값 중 하나라도 다를 경우, \"기본값\"이 출력됩니다.\n\nDECODE() 함수가 어떻게 작동하는지 알았으니, 이제 첫 번째 피벗 테이블을 만들 차례입니다.\n\n## 1차 버전: 총합 열 및 행이 없는 피벗 테이블\n\n<img src=\"/assets/img/2024-06-19-HowtoPivotTablesinSQL_3.png\" />\n\n<div class=\"content-ad\"></div>\n\nDECODE() 함수를 사용하면 아이스크림 가게 주인을 위한 피벗 테이블의 의사 코드를 작성할 수 있어요. \"요일\"이 각 평일에 일치하면 DECODE()는 해당 날짜의 수익을 반환하고, 일치하지 않으면 대신 0을 반환해요.\n\n```js\nSELECT 아이스크림 맛,\nSUM(DECODE(요일, '월요일', 수익, 0)) AS 월요일, SUM(DECODE(요일, '화요일', 수익, 0)) AS 화요일,\nSUM(DECODE(요일, '수요일', 수익, 0)) AS 수요일,\nSUM(DECODE(요일, '목요일', 수익, 0)) AS 목요일,\nSUM(DECODE(요일, '금요일', 수익, 0)) AS 금요일,\nSUM(DECODE(요일, '토요일', 수익, 0)) AS 토요일,\nSUM(DECODE(요일, '일요일', 수익, 0)) AS 일요일\nFROM 아이스크림 가게 데이터셋\nWHERE 날짜가 지난 월요일부터 지난 일요일 사이에 있는 경우;\n```\n\n## 2차 버전: 총계 열과 행이 있는 피벗 테이블\n\n<img src=\"/assets/img/2024-06-19-HowtoPivotTablesinSQL_4.png\" />\n\n<div class=\"content-ad\"></div>\n\n좋은 일했어요! 이제 아이스크림 가게 주인은 지난 주 판매 현황에 대해 더 알고 싶어합니다. 피벗 테이블에 합계 열과 합계 행을 추가하여 업그레이드할 수 있습니다.\n\n이를 이루는 방법은 GROUP BY 문에서 GROUPING SETS 표현식을 사용하는 것입니다. GROUPING SETS 표현식은 여러 개의 GROUP BY 집계를 위한 기준을 정의합니다.\n\n- attribute: GROUP BY할 요소들의 단일 요소 또는 목록\n- (): 빈 그룹으로, 피벗 테이블의 총합 행이 됩니다\n\n```js\nSELECT NVL(아이스크림 맛, '총합') \"아이스크림 맛\", \nSUM(DECODE(요일, '월요일', 매출, 0)) AS 월요일, SUM(DECODE(요일, '화요일', 매출, 0)) AS 화요일,\nSUM(DECODE(요일, '수요일', 매출, 0)) AS 수요일,\nSUM(DECODE(요일, '목요일', 매출, 0)) AS 목요일,\nSUM(DECODE(요일, '금요일', 매출, 0)) AS 금요일,\nSUM(DECODE(요일, '토요일', 매출, 0)) AS 토요일,\nSUM(DECODE(요일, '일요일', 매출, 0)) AS 일요일, \nSUM(매출) AS 총합\nFROM 아이스크림 가게 데이터셋\nWHERE 날짜 BETWEEN 지난 월요일 AND 지난 일요일\nGROUP BY GROUPING SETS (아이스크림 맛, ());\n```\n\n<div class=\"content-ad\"></div>\n\n참고: NVL() 함수는 경우에 따라 생성된 빈 행을 'TOTAL'로 대체합니다. NVL() 함수에 익숙하지 않다면, 간단히 말해 널 값을 바꿔주는 함수입니다.\n\nTOTAL 열을 계산하는 또 다른 방법은 월요일부터 일요일까지의 모든 수익을 더하는 것입니다:\n\n```js\nSUM(DECODE(요일, 'Monday', 수익, 0)) \n+ SUM(DECODE(요일, 'Tuesday', 수익, 0)) \n+ SUM(DECODE(요일, 'Wednesday', 수익, 0)) \n+ SUM(DECODE(요일, 'Thursday', 수익, 0)) \n+ SUM(DECODE(요일, 'Friday', 수익, 0)) \n+ SUM(DECODE(요일, 'Saturday', 수익, 0))\n+ SUM(DECODE(요일, 'Sunday', 수익, 0)) AS TOTAL\n```\n\n## 3번째 버전: 총합 열과 행이 있는 피벗 테이블 및 기타 총합\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-HowtoPivotTablesinSQL_5.png\" />\n\n아이스크림 매장 소유자가 제공한 피벗 테이블에 더 추가하고 싶은 열이 있는 경우: 각 맛의 아이스크림 구매 총 수를 표시하고 싶어합니다. 문제 없어요! 동일한 개념으로 다른 \"총계\" 열을 추가할 수 있어요!\n\n```js\nSELECT NVL(아이스크림 맛, '총계') \"아이스크림 맛\", \nSUM(DECODE(요일, '월요일', 매출액, 0)) AS 월요일, SUM(DECODE(요일, '화요일', 매출액, 0)) AS 화요일,\nSUM(DECODE(요일, '수요일', 매출액, 0)) AS 수요일,\nSUM(DECODE(요일, '목요일', 매출액, 0)) AS 목요일,\nSUM(DECODE(요일, '금요일', 매출액, 0)) AS 금요일,\nSUM(DECODE(요일, '토요일', 매출액, 0)) AS 토요일,\nSUM(DECODE(요일, '일요일', 매출액, 0)) AS 일요일, \nSUM(매출액) AS 총계,\nSUM(구매 ID) \"기타 총계\"\nFROM 아이스크림 매장 데이터셋\nWHERE 날짜가 지난 월요일부터 지난 일요일까지\nGROUP BY GROUPING SETS (아이스크림 맛, ());\n```\n\nDECODE()로 피벗 테이블을 만드는 방법을 알았으니, 이제 Northwind 데이터셋을 활용하여 세 가지 연습을 해보세요!\n\n<div class=\"content-ad\"></div>\n\nQ\n1. 각 국가별 직원들이 각 지역에서 근무하는 직원 수를 알고 싶다면 어떻게 해야 할까요?\n\n이 질문을 해결하기 위해 먼저 REGION 테이블에서 모든 고유 지역을 쿼리할 수 있습니다. 또한, 직원들이 어느 나라에서 온지도 확인해보세요.\n\n```js\nSELECT DISTINCT REGIONID||' '||RDescription AS REGION\nFROM REGION\nORDER BY 1;\n```\n\n![이미지](/assets/img/2024-06-19-HowtoPivotTablesinSQL_6.png)\n\n<div class=\"content-ad\"></div>\n\n```sql\nSELECT DISTINCT Country\nFROM EMPLOYEES\nORDER BY 1;\n```\n\n![Pivot Table](/assets/img/2024-06-19-HowtoPivotTablesinSQL_7.png)\n\nWe will make a 2 * 4 pivot table for this question.\n\nNext, we can create a pivot table using DECODE(). A sample answer and output are shown below:\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-HowtoPivotTablesinSQL_8.png\" />\n\n```js\nSELECT NVL(Country, 'TOTAL') AS COUNTRY, \nSUM(DECODE(LOWER(REGIONID||' '||RDescription), '1 eastern', 1, 0)) \"1 EASTERN\",\nSUM(DECODE(LOWER(REGIONID||' '||RDescription), '2 western', 1, 0)) \"2 WESTERN\",\nSUM(DECODE(LOWER(REGIONID||' '||RDescription), '3 northern', 1, 0)) \"3 NORTHERN\",\nSUM(DECODE(LOWER(REGIONID||' '||RDescription), '4 southern', 1, 0)) \"4 SOUTHERN\",\nSUM(EmployeeID) AS TOTAL\nFROM EMPLOYEES\nJOIN REGION USING (REGIONID)\nGROUP BY GROUPING SETS (Country, ());\n```\n\n<img src=\"/assets/img/2024-06-19-HowtoPivotTablesinSQL_9.png\" />\n\n```js\n--Q1\nSELECT Country, \nSUM(DECODE(LOWER(REGIONID||' '||RDescription), '1 eastern', 1, 0)) \"1 EASTERN\",\nSUM(DECODE(LOWER(REGIONID||' '||RDescription), '2 western', 1, 0)) \"2 WESTERN\",\nSUM(DECODE(LOWER(REGIONID||' '||RDescription), '3 northern', 1, 0)) \"3 NORTHERN\",\nSUM(DECODE(LOWER(REGIONID||' '||RDescription), '4 southern', 1, 0)) \"4 SOUTHERN\",\nSUM() AS TOTAL\nFROM EMPLOYEES\nJOIN REGION USING (REGIONID)\nGROUP BY Country;\n``` \n\n\n<div class=\"content-ad\"></div>\n\n2010년의 각 달에 대해 각 직원이 처리한 주문의 수익을 표시합니다. 또한 최근화해서 가장 가까운 달러로 반올림하여 총 수익과 총 주문 수를 표시해줍니다.\n\n```js\n--Q2\n열 EMPLOYEE FORMAT A18\nSELECT NVL(EmployeeID||' '||FirstName||' '||LastName, 'TOTAL') AS EMPLOYEE,\nTO_CHAR(SUM(DECODE(EXTRACT(MONTH FROM OrderDate), 1, (UnitPrice * Quantity - Discount), 0)), '$990') AS JAN,\nTO_CHAR(SUM(DECODE(EXTRACT(MONTH FROM OrderDate), 2, (UnitPrice * Quantity - Discount), 0)), '$990') AS FEB,\nTO_CHAR(SUM(DECODE(EXTRACT(MONTH FROM OrderDate), 3, (UnitPrice * Quantity - Discount), 0)), '$990') AS MAR,\nTO_CHAR(SUM(DECODE(EXTRACT(MONTH FROM OrderDate), 4, (UnitPrice * Quantity - Discount), 0)), '$990') AS APR,\nTO_CHAR(SUM(DECODE(EXTRACT(MONTH FROM OrderDate), 5, (UnitPrice * Quantity - Discount), 0)), '$990') AS MAY,\nTO_CHAR(SUM(DECODE(EXTRACT(MONTH FROM OrderDate), 6, (UnitPrice * Quantity - Discount), 0)), '$990') AS JUN,\nTO_CHAR(SUM(DECODE(EXTRACT(MONTH FROM OrderDate), 7, (UnitPrice * Quantity - Discount), 0)), '$99,990') AS JUL,\nTO_CHAR(SUM(DECODE(EXTRACT(MONTH FROM OrderDate), 8, (UnitPrice * Quantity - Discount), 0)), '$99,990') AS AUG,\nTO_CHAR(SUM(DECODE(EXTRACT(MONTH FROM OrderDate), 9, (UnitPrice * Quantity - Discount), 0)), '$99,990') AS SEP,\nTO_CHAR(SUM(DECODE(EXTRACT(MONTH FROM OrderDate), 10, (UnitPrice * Quantity - Discount), 0)), '$99,990') AS OCT,\nTO_CHAR(SUM(DECODE(EXTRACT(MONTH FROM OrderDate), 11, (UnitPrice * Quantity - Discount), 0)), '$99,990') AS NOV,\nTO_CHAR(SUM(DECODE(EXTRACT(MONTH FROM OrderDate), 12, (UnitPrice * Quantity - Discount), 0)), '$99,990') AS DEC, \nTO_CHAR(SUM((UnitPrice * Quantity - Discount)), '$999,990') AS TOTAL\nFROM ORDERS \nJOIN ORDERDETAILS USING (OrderID)\nJOIN EMPLOYEES USING (EmployeeID)\nWHERE EXTRACT(YEAR FROM OrderDate) = 2010\nGROUP BY GROUPING SETS (EmployeeID||' '||FirstName||' '||LastName, ())\nORDER BY 1;\n```\n\n참고: FORMAT 명령어 및 TO_CHAR() 함수는 형식을 지정하는 용도로 사용됩니다. 더 알고 싶다면 오라클 웹사이트의 Format Models 및 Formatting SQL*Plus Reports 섹션을 확인해보세요.\n\n![이미지](/assets/img/2024-06-19-HowtoPivotTablesinSQL_10.png)\n\n<div class=\"content-ad\"></div>\n\n# \"PIVOT\"으로 피벗 테이블 만들기\n\n![이미지](/assets/img/2024-06-19-HowtoPivotTablesinSQL_11.png)\n\n이제 DECODE()를 사용하여 피벗 테이블을 만드는 방법을 알게 되었으니, Oracle 11g 버전에서 소개된 PIVOT() 절로 넘어갈 수 있습니다.\n\n- aggr: SUM, COUNT, MIN, MAX, AVG와 같은 함수\n- value: 테이블 열에 대한 헤딩으로 피벗할 값 목록\n\n<div class=\"content-ad\"></div>\n\n아이스크림 가게 예제로 돌아가 봅시다. 이를 PIVOT() 절을 사용하여 어떻게 만들 수 있는지 알아보겠습니다:\n\n## 1st Version: 총합 열 및 행이 없는 피벗 테이블\n\n```js\nSELECT *\nFROM (\n      SELECT day of the week, ice cream flavor, revenue\n      FROM ice cream shop dataset \n      WHERE date between last Monday and last Sunday\n)\nPIVOT (\n       SUM(revenue) \n       FOR day of the week IN ('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday')\n);\n```\n\n## 2nd Version: 총합 열 및 행이 포함된 피벗 테이블\n\n<div class=\"content-ad\"></div>\n\n만약 피벗 테이블에 총합 열을 추가하고 싶다면, NVL() 함수를 사용하면 좋은 방법입니다.\n\n```js\nSELECT *\nFROM (\n      SELECT NVL(아이스크림 맛, '총합') AS 아이스크림 맛,\n             NVL(요일, -1) AS DOW, \n             SUM(수익) AS REV\n      FROM 아이스크림 가게 데이터셋\n      WHERE 날짜가 지난 월요일부터 지난 일요일까지\n      GROUP BY CUBE (아이스크림 맛, 요일) \n)\nPIVOT (\n       SUM(REV) \n       FOR DOW IN ('월요일', '화요일', '수요일', '목요일', '금요일', '토요일', '일요일', -1 AS 총합)\n);\n```\n\n## 3rd 버전: 총합 열과 행 및 다른 합계가 포함된 피벗 테이블\n\n다른 합계가 등장하는 경우 문제를 해결할 수 있는 유일한 방법은 JOIN() 절을 사용하는 것입니다:\n\n<div class=\"content-ad\"></div>\n\n```sql\nSELECT ice cream flavor, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday, TOTAL, OTHER TOTAL\nFROM (\n      SELECT NVL(ice cream flavor, 'TOTAL') AS ice cream flavor,\n             NVL(day of the week, -1) AS DOW, \n             SUM(revenue) AS REV\n      FROM ice cream shop dataset \n      WHERE date between last Monday and last Sunday\n      GROUP BY CUBE (ice cream flavor, day of the week) \n)\nPIVOT (\n       SUM(REV) \n       FOR DOW IN ('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', -1 AS TOTAL)\n)\nJOIN (\n      SELECT NVL(ice cream flavor, 'TOTAL') AS ice cream flavor, \n             SUM(purchase ID) \"OTHER TOTAL\"\n      FROM ice cream shop dataset \n      WHERE date between last Monday and last Sunday\n      GROUP BY ROLLUP (ice cream flavor)\n) USING (ice cream flavor);\n```\n\n참고: 위의 유사 코드에서는 GROUP BY에서 CUBE 및 ROLLUP 확장을 사용합니다. 간단한 설명으로 충분해요.\n\n- CUBE(A, B, C): (A, B, C), (A, B), (A, C), (B, C), (A), (B), (C), ()\n- ROLLUP(A, B, C): (A, B, C), (A, B), (A), ()\n\nPIVOT() 절 작동 방식을 이해한 후에는 이를 part 1에 있는 Northwind 데이터세트로 연습해볼 수 있을 거예요.\n\n<div class=\"content-ad\"></div>\n\n안녕하세요! 아래는 Markdown 형식으로 변경된 테이블입니다.\n\nQ\n1. 우리가 직원이 각 국가에서 서비스하는 지역별 직원 수를 알고 싶다고 가정해 봅시다.\n\n```js\n--Q1\n--해보세요!\n```\n\nQ\n2. 2010년 각 월별로, 각 직원이 처리한 주문의 수익을 보여주세요. 또한 가장 가까운 달러로 반올림하여 총 수익과 총 주문 수를 표시해 주세요.\n\n```js\n--Q2\n--해보세요!\n```\n\n<div class=\"content-ad\"></div>\n\n# 마무리\n\n이 안내서에서는 SQL의 피벗 테이블의 강력한 기능을 탐구했습니다. DECODE() 및 PIVOT() 함수에 중점을 둔 채 피벗 테이블을 사용하는 방법을 살펴보았습니다. 우리는 피벗 테이블에 대한 소개와 행을 열로 변환하여 데이터 분석을 향상시키는 중요성을 시작으로 합니다. 그런 다음 DECODE()를 사용하여 피벗 테이블을 생성하는 프로세스를 살펴보고, 오라클 11g에 도입된 더 간결한 PIVOT() 함수를 살펴보았습니다. 이러한 기술을 적용하여 다차원 데이터를 효율적으로 분석하는 방법을 실제 아이스크림 가게 데이터 세트와 같은 실용적인 예제를 통해 보여주었습니다.\n\n<img src=\"/assets/img/2024-06-19-HowtoPivotTablesinSQL_12.png\" />\n\n## 요약 및 핵심 포인트\n\n<div class=\"content-ad\"></div>\n\n- DECODE() 함수를 사용한 피벗 테이블: 데이터를 수동으로 피벗하는 데 DECODE() 함수를 사용하는 기본적인 방법입니다.\n- PIVOT() 함수를 사용한 피벗 테이블: PIVOT() 함수를 활용하여 더 효율적이고 가독성 있는 피벗 테이블을 생성하는 방법입니다.\n\n댓글에서 답변을 공유하신 다면 망설이지 마세요! 데이터에 대해 배우고 실제 응용 분야에서 배운 것을 생각하는 것을 좋아합니다. 이 기사를 즐겁게 보셨다면 응원의 의미로 클랩(clap)을 남겨주시면 감사하겠습니다. LinkedIn과 Twitter를 통해 연락하거나 토의할 내용이 있다면 언제든지 연락해주세요. 미디엄(Medium)에서 저를 팔로우하시면 더 많은 데이터 과학 기사를 만나보실 수 있습니다!","ogImage":{"url":"/assets/img/2024-06-19-HowtoPivotTablesinSQL_0.png"},"coverImage":"/assets/img/2024-06-19-HowtoPivotTablesinSQL_0.png","tag":["Tech"],"readingTime":14},{"title":"LLM을 위한 의미론적 엔진을 어떻게 설계했나요 LLM 아키텍처를 위한 의미론적 레이어의 중추입니다","description":"","date":"2024-06-19 01:52","slug":"2024-06-19-HowwedesignoursemanticengineforLLMsThebackboneofthesemanticlayerforLLMarchitecture","content":"\n\n트렌드 AI 에이전트의 등장은 비즈니스 인텔리전스 및 데이터 관리 분야를 혁신적으로 변화시켰습니다. 가까운 미래에는 여러 AI 에이전트가 배포되어 데이터베이스와 데이터 웨어하우스에 저장된 방대한 내부 지식을 활용하고 해석할 것입니다. 이를 용이하게 하기 위해서는 의미론적 엔진이 필수적입니다. 이 엔진은 데이터 스키마를 관련 비즈니스 맥락에 매핑하여 AI 에이전트가 데이터의 기저 의미를 이해할 수 있도록 합니다. 비즈니스 맥락에 대한 구조화된 이해를 제공함으로써 의미론적 엔진은 AI 에이전트가 특정 비즈니스 요구에 맞는 정확한 SQL 쿼리를 생성하고 정확하고 맥락에 맞는 데이터 검색을 보장할 수 있도록합니다.\n\n# LLMs가 데이터 구조와 어떤 문제를 가지고 있을까요?\n\nAI 에이전트가 데이터베이스와 직접 대화할 수 있도록 하는 것입니다. 기술적으로는 자연어를 SQL로 변환하고 데이터베이스를 쿼리하기 위한 인터페이스를 제공합니다.\n\n그러나 데이터베이스로부터 맥락을 가진 스키마를 매핑하는 일은 간단한 작업이 아닙니다. 스키마와 메타데이터를 단순히 저장하는 것만으로 충분하지 않습니다. 데이터를 이해하고 처리하는 데 더 심층적으로 파고들어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n## 의미론적 문맥의 부족\n\n데이터베이스 상단에 직접 LLMs를 활성화하면, 데이터베이스에 이미 있는 DDL 정보를 활용하여 LLMs가 데이터베이스 구조와 유형을 학습하도록 할 수 있습니다. 또한 주어진 DDL을 기반으로 각 테이블과 열의 정의를 이해하는 데 도움을 줄 수 있는 제목과 설명을 추가할 수도 있습니다.\n\nLLMs로부터 최적의 성능과 정확도를 얻으려면, 단순히 DDL과 스키마 정의만으로는 충분하지 않습니다. LLMs는 다양한 엔티티 간의 관계를 이해하고 조직 내에서 사용되는 계산 공식을 파악해야 합니다. 계산, 메트릭, 관계 (조인 경로)와 같은 추가 정보를 제공하여 LLMs가 이러한 측면을 이해하는 데 도움이 되도록 하는 것이 중요합니다.\n\n## LLMs와 의미론적 인터페이스 정의의 부재\n\n<div class=\"content-ad\"></div>\n\n이전 섹션에서 언급한 것처럼 LLM이 계산, 지표, 관계 등의 복잡성을 이해할 수 있는 의미론적 맥락이 중요합니다. 아래에서 우리가 직면한 주제들을 일반화할 정의가 필요합니다.\n\n계산\n\n미리 훈련된 LLM에는 각 용어에 대한 일정이 있으며, 각 회사가 자체 KPI 또는 공식을 정의하는 방식과는 다릅니다. 계산은 우리가 일반화할 정의를 제공하는 곳입니다. 예를 들어, 총이익률은 (수익 - 판매비용) / 수익으로 정의됩니다. LLM은 이미 총이익률, 순이익률, CLTV 등과 같은 일반적인 KPI를 이해할 수 있는 능력이 충분히 있는 상태일 수 있습니다.\n\n그러나 현실 세계에서는 열이 보통 혼란스럽고, 수익은 rev라는 열 이름으로 설정될 수 있으며, rev1, pre_rev_1, rev2 등을 볼 수도 있을 것입니다. 의미론적 맥락 없이는 LLM이 이들이 무엇을 의미 하는지 알 방법이 없습니다.\n\n<div class=\"content-ad\"></div>\n\n메트릭\n\n\"슬라이스 앤 다이스\"는 데이터 분석에서 특히 다차원 데이터의 맥락에서 사용되는 기술로, 데이터를 다양한 관점에서 분해하고 보는 것을 말합니다. 이 접근 방식은 데이터를 자세히 탐색하고 분석하는 데 도움이 됩니다.\n\n예를 들어, 판매 메트릭스 예시:\n\n- 총 판매량: 특정 기간 동안 생성된 총 수익.\n- 지역별 판매: 지리적 지역별로 분할된 판매 데이터.\n- 제품별 판매: 개별 제품 또는 제품 카테고리별로 분할된 판매 데이터.\n- 채널별 판매: 온라인, 소매, 도매 등 다양한 판매 채널별로 분할된 판매 데이터.\n\n<div class=\"content-ad\"></div>\n\n또 다른 예시로 고객 지표를 사용해보겠습니다:\n\n- 고객 인구 통계: 고객을 나이, 성별, 위치 등으로 분석한 것입니다.\n- 고객 세분화: 행동, 구매 이력, 선호도 등을 기반으로 한 고객을 분류한 것입니다.\n- 고객 유치: 특정 기간 동안 새로 유치한 고객의 수를 의미합니다.\n- 고객 이탈률: 회사와의 거래를 중단한 고객의 비율입니다.\n\n의미론적 관계\n\n의미론적 관계는 주 키와 외래 키와는 다르지만, 데이터베이스와 데이터 관리의 맥락에서 관련 있는 개념입니다.\n\n<div class=\"content-ad\"></div>\n\n의미 관계는 주로 실제 세계의 관계에 기반하여 데이터 간의 의미 있는 연결을 나타냅니다. 이러한 관계는 데이터 요소들이 단순히 기본 키(primary key)와 외래 키(foreign key)에 의해 제공되는 구조적 링크 이상의 개념적 상호 관련을 설명합니다. 예를 들어, 고객(Customers)과 주문(Orders) 테이블 사이의 의미 관계는 \"고객이 여러 주문을 할 수 있다\"로 설명될 수 있습니다. 이는 기술적 연결을 넘어 관계의 실제 의미를 포착합니다.\n\n반면, 기본 키와 외래 키는 데이터 무결성을 강화하고 데이터베이스 스키마 수준에서 관계를 설정하는 데 사용됩니다. 의미 관계는 데이터 엔티티 간의 관련을 보다 넓은 맥락에서 설명하고 이해하는 데 사용되며, 기본 키와 외래 키 설정에서 제공되지 않는 일대다, 다대다, 일대일 관계를 정의할 수도 있습니다.\n\n## LLM과 다양한 데이터 소스 통합의 도전\n\n불안정한 SQL 생성 성능\n\n<div class=\"content-ad\"></div>\n\n다양한 SQL 방양을 원활하게 처리할 수 있도록 여러 데이터 소스를 연결하고 LLMs가 다양한 소스 간의 성능 일관성을 보장함에는 상당한 어려움이 따릅니다. 데이터 소스의 수가 증가함에 따라 이 어려움은 더욱 부각됩니다. 일관성은 AI 시스템에 대한 신뢰를 구축하는 데 중요합니다. 안정적인 성능을 보장하는 것은 AI 솔루션의 전반적인 사용성과 신뢰성과 직결됩니다.\n\n접근 제어 불일치\n\n다양한 데이터 소스는 종종 각각의 접근 제어 메커니즘을 갖고 있습니다. 이러한 소스들이 직접 연결되면 일관된 데이터 정책을 유지하는 것이 어려워지며, 이는 대규모 데이터 팀 협업에 매우 중요합니다. 이 문제를 해결하기 위해 중앙 통제 계층이 필요합니다. 이 계층은 모든 LLM 사용 사례를 통해 접근 제어를 관리하여 데이터 정책이 균일하게 시행되고 기업 전체에서 보안 및 규정 준수를 강화합니다.\n\n# 시맨틱 레이어의 출현\n\n<div class=\"content-ad\"></div>\n\n다수의 데이터 소스에 직접 연결하는 것은 일관성과 성능 면에서 중요한 도전을 야기합니다. 더 효과적인 접근 방법은 LLM 사용 사례를 위한 의미론적 레이어를 구현하는 것입니다.\n\n## 의미론적 레이어란?\n\n의미론적 아키텍처의 핵심 개념은 *온톨로지*입니다. 온톨로지는 도메인을 형식적으로 나타내는 것으로, 엔티티와 속성을 나타내는 클래스 및 다른 엔티티와의 관계로 구성된다.\n\n데이터셋의 도메인을 위한 온톨로지를 제공함으로써 LLM은 데이터를 제시하는 방법 뿐만 아니라 데이터가 무엇을 나타내는지에 대한 이해를 얻게 됩니다. 이를 통해 시스템은 데이터셋 내에 명시적으로 명시되지 않은 새로운 정보도 처리하고 추론할 수 있게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-HowwedesignoursemanticengineforLLMsThebackboneofthesemanticlayerforLLMarchitecture_0.png\" />\n\n## 시맨틱 레이어의 장점\n\n시맨틱 레이어는 AI 에이전트가 서로 다른 도메인, 엔티티, 그리고 관계들 사이의 의미론을 이해하는 데 도움을 주는 것 이상의 역할을 합니다. 또한 AI 에이전트가 다음과 같은 프레임워크를 제공합니다:\n\n- 올바른 공식을 사용하여 계산\n- 결합 경로와 메트릭에 대한 맥락 제공\n- 서로 다른 데이터 소스 간 일관성을 보장하는 표준화된 SQL 레이어 제공\n- 캡슐화된 비즈니스 논리를 적용하고 엔티티 간 복잡한 관계를 런타임에서 관리\n\n<div class=\"content-ad\"></div>\n\n의미론적 계층을 구현함으로써 AI 에이전트의 능력을 향상시킵니다. 이는 다양한 데이터 원본과 복잡한 비즈니스 맥락 간의 간극을 줄여 정확하고 일관된 통찰력을 제공할 수 있도록 돕습니다.\n\n# 렌 엔진 — LLM을 위한 의미론적 엔진\n\n그렇기 때문에 우리는 LLM을 위한 의미론적 엔진인 렌 엔진을 디자인했으며, 우리가 제시한 과제를 해결하기 위해 노력하고 있습니다.\n\n렌 엔진을 사용하여 '모델링 정의 언어'(MDL)를 정의했으며, 이를 통해 LLM에게 적절한 문맥과 의미론적 메타데이터를 제공하고 엔진은 다양한 사용자 페르소나 및 의미론적 데이터 모델링 방법을 기반으로 SQL을 다시 작성할 수 있습니다. 엔진을 사용하여 의미론적 계층에 속하는 엑세스 제어 및 거버넌스와 같은 솔루션을 구축할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 의미론적 데이터 모델링\n\n온톨로지의 기본 개념은 메타데이터와 데이터를 그래프 구조로 설계하는 것인데, 이를 일반적으로 지식 그래프라고 합니다. Wren Engine을 사용하면 데이터 모델과 측정 항목을 이 그래프 기반 아키텍처 내에서 정의할 수 있습니다. 이를 통해 서로 다른 모델의 열이 어떻게 관련되어 있는지와 해당 관계가 의미하는 바를 명시할 수 있습니다. 이러한 구조화된 정의는 데이터 관계를 명확히하고 SQL 쿼리를 정확하고 효율적으로 재작성할 수 있는 능력을 향상시킵니다.\n\n의미론적 명명과 설명\n\nMDL에서는 모델, 열, 뷰, 그리고 관계의 의미론적 명명과 설명을 손쉽게 정의할 수 있습니다. 의미론적 정의를 사용하면 LLM이 데이터 구조의 의미를 이해하는데 도움을 줄 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```json\n{\n    \"name\": \"customers\",\n    \"columns\": [\n        {\n            \"name\": \"City\",\n            \"type\": \"VARCHAR\",\n            \"isCalculated\": 0,\n            \"notNull\": 0,\n            \"expression\": \"\",\n            // 시맨틱 속성, 예를 들어 설명, 표시 이름 및 별칭을 여기에 추가할 수 있습니다.\n            \"properties\": {\n            \"description\": \"고객이 위치한 고객 도시입니다. \\\"고객 세그먼트\\\"로도 불립니다.\",\n            \"displayName\": \"도시\"\n            }\n        },\n        {\n            // 시맨틱 네이밍\n            \"name\": \"UserId\",\n            \"type\": \"VARCHAR\",\n            \"isCalculated\": 0,\n            \"notNull\": 0,\n            \"expression\": \"Id\",\n            \"properties\": {\n            \"description\": \"데이터 모델에서 각 고객의 고유 식별자입니다.\",\n            \"displayName\": \"Id\"\n            }\n        }\n    ],\n    \"refSql\": \"select * from main.customers\",\n    \"cached\": 0,\n    \"refreshTime\": null,\n    // 시맨틱 속성, 예를 들어 설명, 표시 이름 및 별칭을 여기에 추가할 수 있습니다.\n    \"properties\": {\n        \"schema\": \"main\",\n        \"catalog\": \"memory\",\n        \"description\": \"구매를 한 고객들의 도시를 포함하는 고객 테이블\",\n        \"displayName\": \"customers\"\n    },\n    \"primaryKey\": \"Id\"\n},\n```\n\n관계와 계산을 지원하는 런타임 SQL 재작성\n\nWren Engine을 사용하면 \"모델 정의 언어\"로 시맨틱 표현을 설계할 수 있고, AI 애플리케이션인 WrenAI에서 해당 주변에 UI를 구축하며 여기서도 오픈 소스로 제공됩니다. WrenAI 뒤에서는 서로 다른 엔티티 간의 관계를 정의하고, 일대다, 다대일, 일대일로 선언할 수 있습니다.\n\n<img src=\"/assets/img/2024-06-19-HowwedesignoursemanticengineforLLMsThebackboneofthesemanticlayerforLLMarchitecture_1.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n다음은 관계를 정의하는 간단한 예시입니다.\n\n```js\n{\n    \"name\" : \"CustomerOrders\",\n    \"models\" : [ \"Customer\", \"Orders\" ],\n    \"joinType\" : \"ONE_TO_MANY\",  // 원 대 다 아키텍처입니다.\n    \"condition\" : \"Customer.custkey = Orders.custkey\"\n}\n```\n\n관계는 다음으로 구성됩니다:\n\n- name: 관계의 이름\n- models: 이 관계와 관련된 모델들. Wren Engine은 관계에서 2개의 모델만 연결합니다.\n- joinType: 관계의 유형. 일반적으로 2개의 모델 간에는 4종류의 관계가 있습니다: ONE_TO_ONE (1 대 1), ONE_TO_MANY (1 대 다), MANY_TO_ONE (다 대 1), MANY_TO_MANY (다 대 다).\n- condition: 두 모델 간의 조인 조건. Wren Engine은 SQL 생성 중 조인 조건을 담당합니다.\n\n<div class=\"content-ad\"></div>\n\n모델에서 계산(식)을 추가할 때 사용자 정의 계산(식)도 추가할 수 있어요.\n\n```js\n{\n    \"name\": \"Customer\",\n    \"refSql\": \"select * from tpch.customer\",\n    \"columns\": [\n        {\n            \"name\": \"custkey\",\n            \"type\": \"integer\",\n            \"expression\": \"c_orderkey\"\n        },\n        {\n            \"name\": \"name\",\n            \"type\": \"varchar\",\n            \"expression\": \"c_name\"\n        },\n        {\n            \"name\": \"orders\",\n            \"type\": \"Orders\",\n            \"relationship\": \"CustomerOrders\"\n        },\n        {\n            \"name\": \"consumption\",\n            \"type\": \"integer\",\n            \"isCalculated\": true,\n            \"expression\": \"sum(orders.totalprice)\" // 식 정의\n        }\n    ],\n    \"primaryKey\": \"custkey\"\n},\n```\n\n재사용 가능한 계산식과 함수 형태의 매크로를 지원해요.\n\n계산에 관해서\n\n<div class=\"content-ad\"></div>\n\nWren Engine은 모델에서 계산을 정의하는 계산 필드를 제공합니다. 계산은 동일한 모델의 정의된 열 또는 관계를 통해 다른 모델의 관련 열을 사용할 수 있습니다. 일반적으로 공통 메트릭은 여러 다른 테이블에 관련이 있습니다. 계산된 필드를 통해 서로 다른 모델 간에 상호 작용하는 공통 메트릭을 정의하는 것이 쉽습니다.\n\n예를 들어, 아래는 3개의 열이 있는 orders라는 정의된 모델입니다. 모델을 향상시키기 위해 각 고객의 성장을 알기 위해 customer_last_month_orders_price라는 열을 추가하려고 할 수 있습니다. 다음과 같이 계산된 필드를 정의할 수 있습니다.\n\n```js\n\"columns\": [\n    {\n        \"name\": \"orderkey\",\n        \"type\": \"INTEGER\"\n    },\n    {\n        \"name\": \"custkey\",\n        \"type\": \"INTEGER\"\n    },\n    {\n        \"name\": \"price\",\n        \"type\": \"INTEGER\"\n    },\n    {\n        \"name\": \"purchasetimestamp\",\n        \"type\": \"TIMESTAMP\"\n    },\n    {\n        \"name\": \"customer_last_month_orders_price\",\n        \"type\": \"INTEGER\",\n        \"isCalculated\": \"true\",\n        // column\n        \"expression\": \"lag(price) over (partition by custkey order by date_trunc('YEAR', purchasetimestamp), 0, 0)\"\n    }\n]\n```\n\nMacro 함수에 관해\n\n<div class=\"content-ad\"></div>\n\n매크로는 모델 정의 언어(MDL)의 템플릿 기능입니다. MDL을 간소화하거나 핵심 개념을 중앙 집중화하는 데 유용합니다. 매크로는 Jinja 사양을 따르는 JVM의 템플릿 엔진 인 JinJava에 의해 구현됩니다. 매크로를 사용하면 특정 매개변수를 사용하여 템플릿을 정의하고 모든 표현식에서 사용할 수 있습니다.\n\n아래 시나리오에서 twdToUsd는 전체 MDL 전체에 걸친 범용적 개념을 나타냅니다. 반면에 revenue 및 totalpriceUsd는 개별 모델에 특정한 부분 개념을 포함합니다.\n\n```js\n\"macros\": [\n    {\n        \"name\": \"twdToUsd\",\n        \"definition\": \"(twd: Expression) => twd / 30\" // 매크로 정의\n    }\n],\n\"models\": [\n    {\n        \"name\": \"Orders\",\n        \"columns\": [\n            {\n                \"name\": \"totalprice\",\n                \"type\": \"double\"\n            },\n            {\n                \"name\": \"totalpriceUsd\",\n                \"expression\": \"{ twdToUsd('totalprice') }\" // 매크로 함수 재사용\n            }\n        ]\n    },\n    {\n        \"name\": \"Customer\",\n        \"columns\": [\n            {\n                \"name\": \"revenue\",\n                \"isCalculated\": true,\n                \"expression\": \"{ twdToUsd('sum(orders.totalprice)') }\" // 매크로 함수 재사용\n            },\n            {\n                \"name\": \"orders\",\n                \"Type\": \"Orders\",\n                \"relationship\": \"OrdersCustomer\"\n            }\n        ]\n    }\n]\r\n```\n\n## 표준 SQL 구문 지원\n\n<div class=\"content-ad\"></div>\n\nWren Engine은 내장된 SQL 프로세서와 변환기를 가지고 있습니다. Wren Engine을 통해 우리는 SQL을 파싱하고 WrenSQL 구문에서 표준 ANSI SQL과 호환되는 BigQuery, PostgreSQL, Snowflake 등과 같은 다른 방양으로 변환합니다.\n\n아래는 간단한 예시입니다. 여기서 데이터셋의 MDL을 정의하고 SQL을 제출하면 모든 관계, 계산, 메트릭이 대상 방양에 특화된 SQL로 변환됩니다.\n\n아래는 MDL 파일의 예시입니다 (Gist에서 확인해주세요): [Gist 링크](https://gist.github.com/...)\n\n<div class=\"content-ad\"></div>\n\n위의 쿼리를 제출하면\n\n```js\nSELECT * FROM orders\n```\n\nWren 엔진은 Wren SQL을 MDL 정의에 따라 다음과 같이 방언별 SQL로 변환합니다.\n\n```js\nWITH\n    \"order_items\" AS (\n    SELECT\n        \"order_items\".\"FreightValue\" \"FreightValue\"\n    , \"order_items\".\"ItemNumber\" \"ItemNumber\"\n    , \"order_items\".\"OrderId\" \"OrderId\"\n    , \"order_items\".\"Price\" \"Price\"\n    , \"order_items\".\"ProductId\" \"ProductId\"\n    , \"order_items\".\"ShippingLimitDate\" \"ShippingLimitDate\"\n    FROM\n        (\n        SELECT\n        \"order_items\".\"FreightValue\" \"FreightValue\"\n        , \"order_items\".\"ItemNumber\" \"ItemNumber\"\n        , \"order_items\".\"OrderId\" \"OrderId\"\n        , \"order_items\".\"Price\" \"Price\"\n        , \"order_items\".\"ProductId\" \"ProductId\"\n        , \"order_items\".\"ShippingLimitDate\" \"ShippingLimitDate\"\n        FROM\n        (\n            SELECT\n            \"FreightValue\" \"FreightValue\"\n            , \"ItemNumber\" \"ItemNumber\"\n            , \"OrderId\" \"OrderId\"\n            , \"Price\" \"Price\"\n            , \"ProductId\" \"ProductId\"\n            , \"ShippingLimitDate\" \"ShippingLimitDate\"\n            FROM\n            (\n            SELECT *\n            FROM\n                main.order_items\n            )  \"order_items\"\n        )  \"order_items\"\n    )  \"order_items\"\n) \n, \"payments\" AS (\n    SELECT\n        \"payments\".\"Installments\" \"Installments\"\n    , \"payments\".\"OrderId\" \"OrderId\"\n    , \"payments\".\"Sequential\" \"Sequential\"\n    , \"payments\".\"Type\" \"Type\"\n    , \"payments\".\"Value\" \"Value\"\n    FROM\n        (\n        SELECT\n        \"payments\".\"Installments\" \"Installments\"\n        , \"payments\".\"OrderId\" \"OrderId\"\n        , \"payments\".\"Sequential\" \"Sequential\"\n        , \"payments\".\"Type\" \"Type\"\n        , \"payments\".\"Value\" \"Value\"\n        FROM\n        (\n            SELECT\n            \"Installments\" \"Installments\"\n            , \"OrderId\" \"OrderId\"\n            , \"Sequential\" \"Sequential\"\n            , \"Type\" \"Type\"\n            , \"Value\" \"Value\"\n            FROM\n            (\n            SELECT *\n            FROM\n                main.payments\n            )  \"payments\"\n        )  \"payments\"\n    )  \"payments\"\n) \n, \"orders\" AS (\n    SELECT\n        \"orders\".\"ApprovedTimestamp\" \"ApprovedTimestamp\"\n    , \"orders\".\"CustomerId\" \"CustomerId\"\n    , \"orders\".\"DeliveredCarrierDate\" \"DeliveredCarrierDate\"\n    , \"orders\".\"DeliveredCustomerDate\" \"DeliveredCustomerDate\"\n    , \"orders\".\"EstimatedDeliveryDate\" \"EstimatedDeliveryDate\"\n    , \"orders\".\"OrderId\" \"OrderId\"\n    , \"orders\".\"PurchaseTimestamp\" \"PurchaseTimestamp\"\n    , \"orders\".\"Status\" \"Status\"\n    , \"RevenueA\".\"RevenueA\" \"RevenueA\"\n    , \"Sales\".\"Sales\" \"Sales\"\n    FROM\n        (((\n        SELECT\n        \"orders\".\"ApprovedTimestamp\" \"ApprovedTimestamp\"\n        , \"orders\".\"CustomerId\" \"CustomerId\"\n        , \"orders\".\"DeliveredCarrierDate\" \"DeliveredCarrierDate\"\n        , \"orders\".\"DeliveredCustomerDate\" \"DeliveredCustomerDate\"\n        , \"orders\".\"EstimatedDeliveryDate\" \"EstimatedDeliveryDate\"\n        , \"orders\".\"OrderId\" \"OrderId\"\n        , \"orders\".\"PurchaseTimestamp\" \"PurchaseTimestamp\"\n        , \"orders\".\"Status\" \"Status\"\n        FROM\n        (\n            SELECT\n            \"ApprovedTimestamp\" \"ApprovedTimestamp\"\n            , \"CustomerId\" \"CustomerId\"\n            , \"DeliveredCarrierDate\" \"DeliveredCarrierDate\"\n            , \"DeliveredCustomerDate\" \"DeliveredCustomerDate\"\n            , \"EstimatedDeliveryDate\" \"EstimatedDeliveryDate\"\n            , \"OrderId\" \"OrderId\"\n            , \"PurchaseTimestamp\" \"PurchaseTimestamp\"\n            , \"Status\" \"Status\"\n            FROM\n            (\n            SELECT *\n            FROM\n                main.orders\n            )  \"orders\"\n        )  \"orders\"\n    )  \"orders\"\n    LEFT JOIN (\n        SELECT\n        \"orders\".\"OrderId\"\n        , sum(\"order_items\".\"Price\") \"RevenueA\"\n        FROM\n        ((\n            SELECT\n            \"ApprovedTimestamp\" \"ApprovedTimestamp\"\n            , \"CustomerId\" \"CustomerId\"\n            , \"DeliveredCarrierDate\" \"DeliveredCarrierDate\"\n            , \"DeliveredCustomerDate\" \"DeliveredCustomerDate\"\n            , \"EstimatedDeliveryDate\" \"EstimatedDeliveryDate\"\n            , \"OrderId\" \"OrderId\"\n            , \"PurchaseTimestamp\" \"PurchaseTimestamp\"\n            , \"Status\" \"Status\"\n            FROM\n            (\n            SELECT *\n            FROM\n                main.orders\n            )  \"orders\"\n        )  \"orders\"\n        LEFT JOIN \"order_items\" ON (\"orders\".\"OrderId\" = \"order_items\".\"OrderId\"))\n        GROUP BY 1\n    )  \"RevenueA\" ON (\"orders\".\"OrderId\" = \"RevenueA\".\"OrderId\"))\n    LEFT JOIN (\n        SELECT\n        \"orders\".\"OrderId\"\n        , sum(\"payments\".\"Value\") \"Sales\"\n        FROM\n        ((\n            SELECT\n            \"ApprovedTimestamp\" \"ApprovedTimestamp\"\n            , \"CustomerId\" \"CustomerId\"\n            , \"DeliveredCarrierDate\" \"DeliveredCarrierDate\"\n            , \"DeliveredCustomerDate\" \"DeliveredCustomerDate\"\n            , \"EstimatedDeliveryDate\" \"EstimatedDeliveryDate\"\n            , \"OrderId\" \"OrderId\"\n            , \"PurchaseTimestamp\" \"PurchaseTimestamp\"\n            , \"Status\" \"Status\"\n            FROM\n            (\n            SELECT *\n            FROM\n                main.orders\n            )  \"orders\"\n        )  \"orders\"\n        LEFT JOIN \"payments\" ON (\"payments\".\"OrderId\" = \"orders\".\"OrderId\"))\n        GROUP BY 1\n    )  \"Sales\" ON (\"orders\".\"OrderId\" = \"Sales\".\"OrderId\"))\n) \nSELECT *\nFROM\n    orders\r\n```\n\n<div class=\"content-ad\"></div>\n\n## 다양한 소스에서 일관된 액세스 제어 (계획)\n\n여러 데이터 소스 간의 액세스 제어를 관리하는 것은 서로 다른 액세스 제어 메커니즘으로 인해 어려울 수 있습니다. Wren Engine은 또한 다음과 같은 문제를 해결하고 있습니다.\n\n- 데이터 정책 정의: 모든 데이터 소스가 동일한 보안 및 액세스 프로토콜을 준수하도록 보장합니다.\n- 통합된 인증 및 권한 부여: 단일 엔진 아래에서 다양한 데이터 소스를 통합함으로써 인증 및 권한 부여 프로세스가 간소화됩니다. 이 일관성은 무단 액세스의 위험을 줄이고 사용자가 모든 데이터 소스에서 일관된 액세스 권한을 갖도록 보장합니다.\n- 역할 기반의 액세스 제어 (RBAC): 역할에 기반하여 액세스 권한이 개별 사용자가 아닌 역할에 할당되는 RBAC를 구현합니다.\n\n프로젝트를 실행할 때 자세한 내용을 공유할 예정이니 기대해주세요!\n\n<div class=\"content-ad\"></div>\n\n## 오픈 및 스탠드얼론 아키텍처\n\nWren Engine은 오픈 소스로 제공되며 독립적인 의미 엔진으로 설계되어 있습니다. 이 엔진은 어떤 AI 에이전트와도 쉽게 구현할 수 있으며 일반적인 의미 엔진으로 사용할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-HowwedesignoursemanticengineforLLMsThebackboneofthesemanticlayerforLLMarchitecture_3.png)\n\n# 최종 의견\n\n<div class=\"content-ad\"></div>\n\nWren Engine의 미션은 LLMs의 시맨틱 엔진으로서의 역할을 하여 시맨틱 레이어를 제공하고 BI 및 LLMs에 비즈니스 컨텍스트를 전달하는 것입니다. 우리는 엔진이 모든 애플리케이션 및 데이터 소스와 호환되도록 하는 오픈 커뮤니티를 구축하는 것을 믿습니다. 또한 개발자가 그 위에 자유롭게 AI 에이전트를 구축할 수 있는 아키텍처를 제공하는 것이 우리의 목표입니다.\n\nWrenAI와 Wren Engine에 관심이 있다면, 우리의 GitHub을 확인해보세요. 모두 오픈 소스입니다!\n\nWrenAI를 아직 확인하지 않았다면, 지금 확인해보세요!\n\n👉 GitHub: https://github.com/Canner/WrenAI\n\n<div class=\"content-ad\"></div>\n\n👉 X: https://twitter.com/getwrenai\n\n👉 Medium: https://blog.getwren.ai/\n\n이 기사를 즐겼다면 ⭐ Github에서 WrenAI에 별표를 주시는 걸 잊지 마세요 ⭐ 항상 읽어 주셔서 감사합니다.","ogImage":{"url":"/assets/img/2024-06-19-HowwedesignoursemanticengineforLLMsThebackboneofthesemanticlayerforLLMarchitecture_0.png"},"coverImage":"/assets/img/2024-06-19-HowwedesignoursemanticengineforLLMsThebackboneofthesemanticlayerforLLMarchitecture_0.png","tag":["Tech"],"readingTime":17},{"title":"확장된 SQL 쿼리들, 꼭 알아야 할 사항들","description":"","date":"2024-06-19 01:50","slug":"2024-06-19-AdvancedSQLQueriesyoumustknow","content":"\n\n데이터 분석 및 데이터 엔지니어링을 위한 최고의 15가지 고급 SQL 명령어!\n\n![이미지](/assets/img/2024-06-19-AdvancedSQLQueriesyoumustknow_0.png)\n\n이미 데이터 분석에 익숙한 분이라면 SELECT, INSERT, UPDATE, DELETE 등과 같은 기본 명령어를 이미 알고 있을 것입니다. 가장 많이 사용되는 명령어라면서도 데이터를 깊이 파고들기 위해 아래 쿼리도 알아두는 것이 좋습니다.\n\n# 1. 윈도우 함수\n\n<div class=\"content-ad\"></div>\n\n윈도우 함수는 현재 행과 관련된 일련의 행을 대상으로 계산에 사용됩니다. 예를 들어, SUM() 함수와 OVER() 절을 사용하여 매출의 누적 합계를 계산하는 예제를 살펴보겠습니다. 'Sales_Data'라는 매출 데이터 테이블을 가정해보겠습니다. 이 테이블은 여러 날짜에 걸친 매출 금액을 기록합니다. 각 날짜별 매출의 누적 합계를 계산하고자 합니다. 즉, 각 날짜까지의 총 매출액을 나타냅니다.\n\n이 쿼리는 시간이 지남에 따른 매출의 누적 합계를 제공하여 누적 매출 성장을 추적하기 쉽게 합니다.\n\n결과:\n\ndate | sales | running_total\n2023-01-01 | 100 | 100\n2023-01-02 | 150 | 250\n2023-01-03 | 200 | 450\n2023-01-04 | 250 | 700\n\n<div class=\"content-ad\"></div>\n\n윈도우 함수는 단일 행 그룹으로 결과 집합을 축소시키지 않고 실행 합계, 이동 평균, 순위 등 다양한 작업에 사용할 수 있어요.\n\n# 2. 공통 테이블 표현식 (CTE)\n\n공통 테이블 표현식은 쿼리 내에서 참조할 수 있는 임시 결과 집합을 만드는 방법을 제공해요. 이것은 가독성을 향상시키고 복잡한 쿼리를 간단하게 만들어줘요. 다음은 각 제품 카테고리별 총 매출을 계산하기 위해 CTE를 사용하는 방법이에요.\n\n이 쿼리는 'category_revenue'라는 CTE를 정의해요. 이는 매출 테이블에서 수익을 합산하고 카테고리 열을 기준으로 결과를 그룹화하여 각 카테고리별 총 수익을 계산해요. 메인 쿼리는 'category_revenue' CTE에서 모든 열을 선택하여 각 카테고리의 계산된 총 수익을 효과적으로 표시해요.\n\n<div class=\"content-ad\"></div>\n\n출력:\n\ncategory | total_revenue\n\nA | 5000\n\nB | 7000\n\n<div class=\"content-ad\"></div>\n\n\nC|4500\n\n# 3. Recursive Queries\n\nRecursive queries enable travel of hierarchical data structures like organisational charts or bill of materials. Suppose we have a table representing employee relationships, and we want to find all the subordinates of a given manager.\n\nThis recursive CTE finds all employees who report directly or indirectly to a specific manager ‘manager_id_of_interest’. It starts with employees directly reporting to the manager and then recursively finds their subordinates, building the hierarchy.\n\n\n<div class=\"content-ad\"></div>\n\n\n| employee_id | name  | manager_id           |\n|:------------|:------|:---------------------|\n| 2           | Alice | manager_id_of_interest |\n| 3           | Bob   | 2                    |\n\n\n<div class=\"content-ad\"></div>\n\n4|Charlie |3\n\n# 4. Pivot Tables\n\n피벗 테이블은 행을 열로 변환하여 데이터를 요약해 주는 표 형식을 제공합니다. 예를 들어, 판매 데이터가 포함된 테이블이 있고, 각 제품의 월별 총 판매액을 표시하기 위해 데이터를 피벗하려고 한다고 가정해 봅시다.\n\n이 쿼리는 조건부 집계를 사용하여 각 제품의 월별 판매 데이터를 집계합니다. 각 제품의 1월, 2월, 3월 판매액을 따로 합산하여, 이러한 월에 대한 각 제품의 총 판매액을 보여주는 테이블을 만듭니다.\n\n<div class=\"content-ad\"></div>\n\n다음은 Markdown 형식으로 테이블을 나타낸 내용입니다:\n\n| product | Jan | Feb | Mar |\n|---------|-----|-----|-----|\n| Product A | 100 | 200 | 150 |\n| Product B | 80 | 190 | 220 |\n| Product C | 60 | 140 | 130 |\n\n# 5. 분석 함수\n\n분석 함수는 행 그룹을 기반으로 집계 값을 계산합니다. 예를 들어, ROW_NUMBER( ) 함수를 사용하여 데이터셋의 각 레코드에 고유한 행 번호를 할당할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 쿼리는 ROW_NUMBER() 윈도우 함수를 사용하여 주문 날짜를 기준으로 각 고객당 주문에 고유한 순위를 할당합니다. 결과는 각 고객이 배치한 주문의 순서를 보여줍니다.\n\n 출력:\n\n| customer_id | order_id | order_rank |\n|------------|-----------|--------------|\n| 1              | 101          | 1                    |\n| 1              | 102          | 2                    |\n| 2              | 201          | 1                    |\n| 2              | 202          | 2                    |\n| 2              | 203          | 3                    |\n\n# 6. Unpivot\n\n<div class=\"content-ad\"></div>\n\n언피벗은 열을 행으로 변환하는 피벗의 반대 작업입니다. 월별로 집계된 매출 데이터가 있는 테이블이 있다고 가정해봅시다. 시간에 따른 트렌드를 분석하기 위해 이를 언피벗하려고 합니다.\n\n다음 쿼리는 매출 열을 행으로 변환하여 제품별 시간에 따른 트렌드를 분석하기 쉽게 만듭니다. 각 행은 특정 월에 대한 제품의 매출을 나타냅니다.\n\n결과:\n\nproduct | month | sales\nProductA | Jan | 100\nProductA | Feb | 150\nProductA | Mar | 200\nProductB | Jan | 200\nProductB | Feb | 250\nProductB | Mar | 300\n\n<div class=\"content-ad\"></div>\n\n# 7. 조건부 집계\n\n조건부 집계는 지정된 기준에 따라 조건부로 집계 함수를 적용하는 것을 말합니다. 예를 들어, 반복 고객이 주문한 주문에 대해서만 평균 판매 금액을 계산하고 싶을 수 있습니다.\n\n다음 쿼리는 두 개 이상의 주문을 한 고객들의 평균 주문 총액을 계산합니다. 각 고객에 대해 주문 수와 총 주문 금액을 집계한 후, 반복 고객들에 대한 평균을 계산합니다.\n\n출력:\n\n<div class=\"content-ad\"></div>\n\ncustomer_id | avg_sales_repeat_customers \n1 | 250 \n2 | 150 \n3 | 300\n\n## 8. 날짜 함수\n\nSQL에서의 날짜 함수는 날짜와 관련된 정보를 조작하고 추출할 수 있게 해줍니다. 예를 들어 DATE_TRUNC() 함수를 사용하여 매출 데이터를 월별로 그룹화할 수 있습니다.\n\n이 출력은 매출이 total_sales로 집계된 각 월을 month으로 나타내며, 각 월은 해당 월의 첫 번째 날로 표시됩니다 (예: 1월의 경우 2023-01-01). 각 해당 월의 총 매출액이 합산되어 표시됩니다.\n\n<div class=\"content-ad\"></div>\n\n출력:\n\n| month      | total_sales |\n|------------|-------------|\n| 2023-01-01 | 15000       |\n| 2023-02-01 | 20000       |\n| 2023-03-01 | 17500       |\n| 2023-04-01 | 22000       |\n\n## 9. 병합문\n\n병합문(UPSERT 또는 ON DUPLICATE KEY UPDATE로도 알려져 있음)은 소스 테이블과의 조인 결과를 기반으로 대상 테이블에 레코드를 삽입, 업데이트 또는 삭제할 수 있게 합니다. 고객 데이터를 포함하는 두 테이블을 동기화하고 싶다고 가정해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n\ncustomers_target (before merge):\n\n| customer_id | name       | email           |\n|-------------|------------|-----------------|\n| 1           | John Doe   | john@example.com|\n| 2           | Jane Smith | jane@example.com|\n\n\n<div class=\"content-ad\"></div>\n\n테이블 태그를 Markdown 형식으로 변경해보면 다음과 같습니다.\n\n\n| customer_id | name         | email            |\n|------------ | ------------ | -----------------|\n| 1           | John Doe     | john@example.com |\n| 2           | Jane Johnson | jane.j@example.com |\n| 3           | Alice Brown  | alice@example.com |\n\n\n<div class=\"content-ad\"></div>\n\nMERGE 문은 customers_source 테이블을 기반으로 customers_target 테이블을 업데이트합니다. customers_source의 customer_id가 customers_target에서와 일치하는 경우, 이름과 이메일이 업데이트됩니다. 일치하는 항목이 없는 경우 새로운 행이 삽입됩니다.\n\n## 10. Case 문\n\nCase 문을 사용하면 SQL 쿼리 내에서 조건부 논리를 구현할 수 있습니다. 예를 들어, 총 구매 금액에 기초하여 고객을 분류하는 Case 문을 사용할 수 있습니다.\n\n예제 데이터 세트를 고려하고 출력을 설명해보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n\norders 테이블에서의 예시 데이터:\n\ncustomer_id | order_total\n1 | 200\n1 | 300\n2 | 800\n3 | 150\n3 | 400\n4 | 1200\n\n결과:\n\ncustomer_id | customer_category\n1 | Gold\n2 | Gold\n3 | Silver\n4 | Platinum\n\n\n<div class=\"content-ad\"></div>\n\n쿼리는 고객의 총 구매 금액을 기준으로 고객을 카테고리로 분류합니다. 총 구매 금액이 $1000 이상인 경우 '플래티넘'으로 레이블을 지정하고, $500에서 $999 사이인 경우 '골드'로 레이블을 지정하며, $500 미만인 경우 '실버'로 레이블을 지정합니다.\n\n# 11. 문자열 함수\n\nSQL에서의 문자열 함수는 텍스트 데이터를 조작하는 데 사용됩니다. 예를 들어 CONCAT( ) 함수를 사용하면 이름과 성을 연결할 수 있습니다.\n\n예시 데이터 세트를 고려하고 출력을 설명해 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n예시 데이터인 직원 테이블:\n\n| 이름 | 성 |\n|--------|--------|\n| John | Doe |\n| Jane | Smith |\n| Alice | Johnson |\n| Bob | Brown |\n\n결과:\n\n| 전체 이름 |\n|--------------|\n| John Doe |\n| Jane Smith |\n| Alice Johnson |\n| Bob Brown |\n\n<div class=\"content-ad\"></div>\n\n질문은 employees 테이블에서 first_name 및 last_name 열을 연결하여 각 직원의 full_name을 만드는 것입니다.\n\n## 12. 그룹 세트\n\n그룹 세트를 사용하면 하나의 쿼리에서 여러 수준의 세분화 데이터를 집계할 수 있습니다. 우리가 매월 그리고 연도별로 총 판매 수익을 계산하고 싶다고 가정해 봅시다.\n\n예시 데이터는 sales 테이블에 있습니다:\n\n<div class=\"content-ad\"></div>\n\n주문 날짜 | 매출액\n2023–01–15 | 1000\n2023–01–20 | 1500\n2023–02–10 | 2000\n2023–03–05 | 2500\n2024–01–10 | 3000\n2024–01–20 | 3500\n2024–02–25 | 4000\n\n결과:\n\n년도 | 월 | 총 매출\n2023 | 1 | 2500\n2023 | 2 | 2000\n2023 | 3 | 2500\n2024 | 1 | 6500\n2024 | 2 | 4000\n2023 | NULL | 7000\n2024 | NULL | 10500\nNULL | 1 | 9000\nNULL | 2 | 6000\nNULL | 3 | 2500\n\n그 쿼리는 GROUPING SETS를 사용하여 매출 데이터를 연도별 및 월별, 연도만, 월만으로 그룹화합니다. 결과적으로 각 연도의 각 월별 소계, 각 연도의 총합, 모든 연도를 통틀어 각 월의 총합이 나타납니다.\n\n<div class=\"content-ad\"></div>\n\n# 13. Cross Joins\n\nCross joins은 두 개의 테이블의 데카르트 곱을 생성하여 각 테이블의 모든 행을 결합한 조합을 생성합니다. 예를 들어, 교차 조인을 사용하여 제품과 고객의 모든 가능한 조합을 생성할 수 있습니다.\n\n제품 및 고객 테이블을 위한 예제 데이터 세트를 살펴봅시다.\n\n제품 테이블:\n\n<div class=\"content-ad\"></div>\n\nproduct_id | product_name\n--- | ---\n1 | Product A\n2 | Product B\n\ncustomers table:\n\ncustomer_id | customer_name\n--- | ---\n101 | Customer X\n102 | Customer Y\n\nOutput:\n\n<div class=\"content-ad\"></div>\n\nproduct_id | product_name | customer_id | customer_name\n1 | Product A | 101 | Customer X\n1 | Product A | 102 | Customer Y\n2 | Product B | 101 | Customer X\n2 | Product B | 102 | Customer Y\n\n해당 쿼리는 PRODUCTS와 CUSTOMERS 테이블 사이에 CROSS JOIN을 수행하여 카테시안 곱을 생성합니다. 이는 모든 제품이 모든 고객과 쌍을 이루어 모든 제품과 고객의 가능한 조합을 생성하는 것을 의미합니다.\n\n## 14. 인라인 뷰\n\n인라인 뷰(파생 테이블로도 알려짐)는 SQL 쿼리 내에서 임시 결과 집합을 만드는 데 사용됩니다. 예를 들어, 우리가 주문 평균 가치를 초과하는 구매를 한 고객을 찾고 싶다면 인라인 뷰를 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n주문 테이블:\n\ncustomer_id | order_total\n1 | 100\n1 | 200\n2 | 500\n3 | 300\n3 | 200\n4 | 700\n\n각 고객별 총 주문을 계산합니다:\n\ncustomer_id | order_total\n1 | 300\n2 | 500\n3 | 500\n4 | 700\n\n<div class=\"content-ad\"></div>\n\n표를 마크다운 형식으로 변경하세요.\n\n\n| customer_id | order_total |\n| --- | --- |\n| 2 | 500 |\n| 3 | 500 |\n| 4 | 700 |\n\n\n<div class=\"content-ad\"></div>\n\n# 15. 집합 연산자\n\nUNION, INTERSECT 및 EXCEPT와 같은 집합 연산자를 사용하여 두 개 이상의 쿼리 결과를 결합할 수 있습니다. 예를 들어, 우리는 UNION 연산자를 사용하여 두 개의 쿼리 결과를 하나의 결과 세트로 병합할 수 있습니다.\n\n이 쿼리는 제품 및 보관된 제품 테이블에서 결과를 결합하여 중복 항목을 제거하여 제품 ID 및 이름의 통합 목록을 만듭니다. UNION 연산자는 각 제품이 최종 출력에서 한 번만 나타나도록 합니다.\n\n결과:\n\n<div class=\"content-ad\"></div>\n\nproduct_id | product_name\n1 | Chocolate Bar\n2 | Dark Chocolate\n3 | Milk Chocolate\n4 | White Chocolate\n5 | Almond Chocolate\n\n이 15가지의 고급 SQL 기술을 사용하여 복잡한 데이터 문제를 쉽고 정확하게 해결할 수 있습니다. 데이터 분석가, 엔지니어, 혹은 과학자라면 SQL 기술을 향상시킴으로써 데이터 처리 역량을 크게 향상시킬 수 있습니다.\n\n이 게시물이 유용했다면, 반드시 클랩(clap), 댓글(comment), 구독(subscribe) 및 팔로우(follow)하여 미디엄(medium.com)에서 더 많은 데이터 관련 콘텐츠를 접해보세요.\n\n즐거운 데이터 분석 되세요!","ogImage":{"url":"/assets/img/2024-06-19-AdvancedSQLQueriesyoumustknow_0.png"},"coverImage":"/assets/img/2024-06-19-AdvancedSQLQueriesyoumustknow_0.png","tag":["Tech"],"readingTime":9},{"title":"SQL과 No-SQL 솔루션 중 어떤 것을 선택해야 할까요","description":"","date":"2024-06-19 01:46","slug":"2024-06-19-HowIchoosebetweenSQLandNo-SQLsolutions","content":"\n\n이 기사에서는 솔루션을 선택하기 위해 SQL 및 No-SQL 데이터베이스 중 어떤 것을 선택할 것인지에 대해 설명하고 있습니다. 이 결정의 일환으로, 구조화된 및 비구조화된 데이터가 결정에 어떤 영향을 미치는지 및 기타 요소를 탐구합니다. 이것은 복잡한 결정일 수 있습니다 [소프트웨어 관리에 관한 기사].\n\n![이미지](/assets/img/2024-06-19-HowIchoosebetweenSQLandNo-SQLsolutions_0.png)\n\n한 대학 강사가 '나는 여러분을 엔지니어가 되도록 가르치러 온 게 아니에요. 엔지니어링은 돈에 관한 것이에요!'라고 말했던 것을 기억합니다.\n\n나는 내 엔지니어링 경력을 돌아보았을 때, 그가 옳았다는 것을 알 수 있었습니다. 기술에 대한 결정은 기술이 작업에 적합한지 여부만큼 비용에 대한 문제도 중요합니다. 둘 다 옳아야 결정을 내릴 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nNo-SQL 대 SQL을 결정할 때도 같은 것이 적용됩니다.\n\n일반적으로 No-SQL 대 SQL에 대한 논의는 주어진 솔루션에 사용할 데이터베이스 기술에 대한 논의입니다.\n\n우리가 해야 할 일을 제대로 처리하기 위해 ‘적합한지 여부’부터 살펴보아야 합니다. 이는 결국 데이터가 구조화되었는지 아니면 비구조화되었는지에 대한 문제로 시작됩니다.\n\n그런 다음에는 결정의 비기술적 측면을 살펴볼 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 구조화된 데이터 vs 구조화되지 않은 데이터\n\n많은 사람들이 자신의 데이터가 구조화되지 않았다는 사실에 대해 이야기한다는 것을 듣습니다. 일부 보고서에는 기업이 약 80%의 구조화되지 않은 데이터로 운영한다고 언급됩니다.\n\n구조화된 데이터에 대해 이야기하는 사람들을 듣는 것은 훨씬 적습니다. 아마도 이는 어느 수준에서든 모든 데이터가 어느 정도로든 구조화되어 있다고 생각되기 때문일 것입니다.\n\n그렇다면 구조화된 데이터 또는 구조화되지 않은 데이터가 무엇을 의미하는 걸까요?\n\n<div class=\"content-ad\"></div>\n\n우리는 구조화된 데이터인지 비구조화된 데이터인지 결정할 수 있는 두 가지 방법이 있어요:\n\n- 데이터에 대한 정보(메타데이터)를 우리가 코드를 작성하기 전에 알 수 있나요?\n- 받는 데이터가 런타임에서 형식을 변경할까요?\n\n이 두 질문에 대한 대답이 '그렇다, 아니요'일 가능성이 높아요.\n\n텍스트 페이지를 예로 들어볼게요. 이는 일반적으로 비구조화물로 분류돼요. 이게 좀 이상하게 들릴 수 있지만, 그것은 매우 잘 구조화돼 있어요. 한정된 기호 집합을 사용하여 언어로 작성되었고(일반적으로) 해당 언어의 문법 규칙을 따라 쓰여졌기 때문이죠. 이 모든 것은 우리의 비구조화된 텍스트 페이지가 매우 구조화된 메타데이터를 갖고 있다는 것을 보여줘요.\n\n<div class=\"content-ad\"></div>\n\n해당 텍스트의 실제 의미는 무엇이든 될 수 있습니다. 따라서 의미론적으로 구조화되지 않았습니다.\n\n따라서, 하나의 수준에서는 구조화된 데이터를 가지고 있지만 다른 수준에서는 구조화되지 않았습니다. 이는 대부분의 데이터에 대한 일반적인 경우입니다.\n\n실제로 구조화되지 않은 데이터는 무작위이며 적용할 수 있는 규칙이 적을 수 있습니다. 존재 및 크기에 제한이 있으며 변환할 수 있는 능력(예: 암호화)과 관련이 있습니다.\n\n다양한 기능을 갖춘 솔루션을 위해 소프트웨어는 데이터나 해당 메타데이터가 구조화되어 있다고 가정합니다. 이를 깨달았을 때, 구조화되거나 구조화되지 않은 데이터가 있는지에 대해 이야기하는 것은 중요하지 않습니다. 아마도 둘 다 가지고 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 데이터 유동성\n\n구조에 대해 이야기하는 것보다는 데이터의 유동성에 대해 이야기하는 것이 더 나을 것 같아요.\n\n당신의 데이터가 엄격한 타입 규칙에 적합하다면, 그것은 집 벽돌처럼 단단한 것으로 간주될 수 있어요. 그 구조는 설정되어 있고 변경할 수 없어요.\n\n만약 데이터가 어떤 타입 규칙에도 적합하지 않는다면, 그것은 물과 같이 유동적인 것으로 간주될 수 있어요. 그 구조는 정의되지 않았으며 사실상 무작위적이에요.\n\n<div class=\"content-ad\"></div>\n\n이제 데이터의 유동성을 살펴볼 차례입니다. 의심의 여지 없이 벽과 물 사이 어딘가에 해당될 것이며, 젤리(또는 젤로)에 더 가까울 것입니다.\n\n해결하려는 문제에 따라 데이터 유동성은 벽에 더 가깝거나 물에 더 가까울 것입니다. 이 유동성을 이해하는 것은 해당 기술을 지원하는 데 필요한 기술을 이해하는 데 도움이 될 것입니다.\n\n# 지속된 데이터\n\n이전에 No-SQL 대 SQL에 관한 결정은 실제로 데이터베이스 기술 또는 지속성 계층에 관한 결정이라고 언급했습니다. 이게 무슨 의미인지 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n만약에 구조화된 vs 비구조화된에 대해 구글링을 하면, 대부분의 설명이 데이터가 데이터베이스에 저장(또는 보존)되는 방식과 관련이 있다는 것을 알 수 있을 거에요. 이 정의를 보면 SQL 기반 데이터베이스가 구조화된 데이터(벽돌과 같은)를 저장하는 데 사용되는 반면, NoSQL 기반 데이터베이스는 비구조화된 데이터(물과 같은)를 저장하는 데 사용된다는 것을 알 수 있을 거에요.\n\n이게 어떻게 작용하는지 살펴봐요.\n\n\n![이미지](/assets/img/2024-06-19-HowIchoosebetweenSQLandNo-SQLsolutions_1.png)\n\n\n## SQL 데이터베이스의 구조화된 데이터\n\n<div class=\"content-ad\"></div>\n\n구조화된 데이터는 고정된 데이터 정의(또는 스키마)를 갖습니다. 데이터베이스는 각각의 새로운 데이터(또는 레코드)를 테이블의 행으로 저장합니다. 레코드 내의 각 필드는 행 안의 열로 저장됩니다. 각 열은 특정 유형의 데이터입니다. 이렇게하여 모든 구조화된 데이터가 테이블, 행 및 열의 고정된 3차원 그리드 내에 저장됩니다.\n\n데이터를 저장하는 데 사용되는 엄격한 구조 때문에 데이터를 생성, 읽기, 업데이트 또는 삭제하는 데는 구조화된 쿼리 언어(SQL)가 사용됩니다. 이제 코드는 데이터 및 그 구조를 처리할 때 데이터를 신뢰할 수 있습니다.\n\n## 비구조화된 데이터와 No-SQL 데이터베이스\n\n비구조화된 데이터는 정의된 스키마가 없습니다. 데이터베이스는 레코드가 무엇을 포함하거나 어떻게 보이는지에 대한 엄격한 정의가 없습니다. 완전히 중립적이며 데이터가 무엇인지는 중요하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 스스로에게 물을 수 있을 것입니다. \"구조가 없다면 데이터에 어떻게 접근할까요?\"\n\n그렇습니다. 구조화되지 않은 데이터도 어떤 방식으로든 참조되어야 합니다. 일반적으로 SQL 데이터베이스의 테이블과 행은 컬렉션과 문서로 대체됩니다.\n\n이것은 이해하기 쉽습니다. 일상 언어로, 문서에는 무엇이든 담길 수 있으며, 구조화되지 않은 데이터도 동일합니다. 컬렉션은 일정한 주제를 가진 문서들의 집합이며, 예를 들어 물고기에 관한 문서들의 컬렉션이 될 수 있습니다. 문서에는 어떤 유형의 데이터든 포함될 수 있으며, 심지어 동일한 컬렉션 내의 문서라 할지라도 다양한 유형의 데이터를 가질 수 있습니다.\n\n이제 매우 유동적인 데이터가 NoSQL 데이터베이스에 훨씬 더 적합하다는 것이 분명해졌을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 관계형 데이터베이스\n\nSQL 데이터베이스에 대해 이야기할 때, 실제로는 관계형 데이터베이스 관리 시스템(RDBMS)을 언급하는 것입니다.\n\n구조화된 데이터에서는 데이터 간에 관계가 있을 수 있습니다. 예를 들어, 어떤 물고기와 어항의 테이블이 있다면, 어떤 물고기가 어떤 어항에 있는지 알고 싶을 수 있습니다. 이는 데이터, 물고기 및 어항 간의 관계를 의미합니다.\n\n관계형 데이터베이스는 이러한 관계를 이해하고 데이터 레코드 간의 참조를 사용하여 해당 관계를 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n관계가 매우 중요하기 때문에 RDBMS는 관계들을 특별히 관리하며, 참조 무결성 규칙을 사용하여 참조가 영원히 깨지지 않도록 합니다.\n\n이러한 관계들 덕분에 SQL이 데이터베이스 전체에서 데이터를 관리할 수 있으며, 서로 다른 테이블, 행 및 열에 있는 데이터도 관리할 수 있습니다.\n\n## 비관계형 데이터베이스\n\n진정한 비구조화 데이터 세트에서는 데이터 간에 정의된 관계가 없습니다. 예를 들어 도서관의 책들을 생각해보십시오. 이들은 어떠한 방식으로도 서로 관련되어 있지 않을 수 있습니다. 그들은 단지 문서(책)의 모음(도서관)일 뿐입니다.\n\n<div class=\"content-ad\"></div>\n\n구조가 없으면 비구조화된 데이터에 액세스하려면 SQL의 대안이 필요합니다. SQL은 상당히 표준화되어 있지만, No-SQL 쿼리 언어는 기본 저장 기술에 더 의존합니다. 그들이 모두 No-SQL이라고 불리는 것은 공통점이 있습니다.\n\nNo-SQL 쿼리를 찾는 것은 도서관에서 금붕어에 대한 설명을 찾는 것과 비슷합니다.\n\nNo-SQL 데이터베이스의 데이터(문서)가 다른 문서와 관련이 없는 것으로 생각하는 것이 유혹적이지만, 그렇지 않습니다. No-SQL 데이터베이스는 관계를 지원하지만 RDBMS와 동일한 무결성으로 강제하지는 않습니다. 이것이 여러분에게 문제가 될 수도, 그렇지 않을 수도 있습니다.\n\n#일하는 데 가장 적합한 기술\n\n<div class=\"content-ad\"></div>\n\n지금까지 데이터가 일반적으로 완전히 구조화되지 않았고 완전히 구조화되지 않았다고 언급했습니다. SQL 데이터베이스는 둘 다 처리할 수 있고 No-SQL 데이터베이스도 둘 다 처리할 수 있다는 것을 설명했습니다.\n\n그렇다면 어떻게 결정을 내리는 걸까요?\n\n데이터의 유동성을 고려해야 합니다. 데이터가 더 벽돌 모양이라면 SQL 데이터베이스의 엄격한 규칙들이 도움이 될 것입니다. 데이터가 더 물 모양이라면 No-SQL 데이터베이스의 유연성에서 이점을 얻을 수 있을 것입니다.\n\n한 가지 더 고려해야 할 시나리오가 있습니다. 때로는 데이터의 구조를 아직 알 수 없지만 저장해야 하는 경우가 있을 수 있습니다. 구조를 식별하는 데 더 나아질 때까지 처리할 것입니다. 이 경우 데이터의 유동성 수준을 아직 모르기 때문에 No-SQL 데이터베이스를 사용하는 것이 더 나을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n흐음, 아마도 이미 알고 계시겠지만, 데이터가 유동적인 경우, 대부분의 데이터는 한쪽 극단에 있지 않고 중간에 위치합니다. 한쪽 끝에 더 가까울 수도 있고 그 반대에 더 가까울 수도 있으므로 적절한 경우 SQL 또는 No-SQL 데이터베이스를 선택하게 될 것입니다.\n\n하지만 다른 유형의 데이터는 어떻게 처리해야 할까요?\n\n## SQL 데이터베이스에서 비구조화된 데이터 다루기\n\n이전에 언급했듯이, SQL 데이터베이스는 일반적으로 비구조화된 데이터를 처리할 수 있습니다. 그들은 데이터의 덩어리로서 처리할 수 있습니다. 파일처럼, 덩어리는 모든 유형의 데이터를 포함할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n대체로, 테이블 형식으로 저장하는 대신 텍스트 필드로도 저장할 수 있습니다. 일반적으로 JSON 형식의 문자열로 텍스트 필드 내에 저장하는 것이 일반적입니다. JSON은 유동적인 데이터를 위한 흔한 형식입니다 (실제로 엄격한 구조를 갖고 있습니다).\n\n최신의 Postgres SQL 데이터베이스에는 실제로 JSON 형식 데이터를 네이티브로 관리하는 JSONB 필드 유형이 있습니다. 그런 다음 SQL을 사용하여 JSON 필드의 데이터에 액세스할 수 있습니다.\n\n그래서 보시다시피, SQL 데이터베이스에 비정형 데이터를 저장하는 여러 옵션이 있습니다.\n\n## No-SQL 데이터베이스의 구조화된 데이터\n\n<div class=\"content-ad\"></div>\n\n그런 반면, No-SQL 데이터베이스를 선택했을 수 있지만 여전히 사용하기 위해 일정한 구조가 필요할 수 있습니다.\n\nSQL 데이터베이스와 마찬가지로 선택 사항이 있습니다.\n\n완전히 정의되지 않은 데이터베이스 구조(컬렉션 및 문서를 넘어서)가 주어지면 구조화된 데이터를 포함하여 필요한 내용을 저장할 수 있습니다. 데이터베이스에 데이터를 저장하기 전에 원하는 구조에 맞도록 유효성 검사 규칙을 적용하는 것만 신경 쓰면 됩니다.\n\n이렇게 하면 코드 내에서 데이터의 구조를 신뢰할 수 있게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n또 다른 고려 사항은 저장 기술 결정 시 일관성이 필요하다는 점입니다.\n\n예를 들어 한 사용자가 데이터를 변경하면 다른 사용자는 그 변경 사항을 어떻게 경험하나요?\n\n<div class=\"content-ad\"></div>\n\n주요한 두 가지 경험이 있습니다:\n\n1. 단일 트랜잭션 중 하나 이상의 변경 사항이 발생할 때 다른 사용자는 트랜잭션이 종료될 때까지 변경 사항을 볼 수 없습니다. 해당 시점에 트랜잭션 중에 변경된 모든 것이 함께 저장되며 다른 사용자는 항상 일관된 데이터 집합을 볼 수 있습니다.\n\n2. 같은 상황에서 각 변경 사항이 별도로 저장되므로 다른 사용자는 해당 변경 사항을 즉시 볼 수 있습니다. 즉, 데이터가 일관성 없을 수 있으며 트랜잭션이 완료될 때까지 데이터는 일관성이 없을 수 있습니다.\n\n두 번째 옵션은 일관성을 보장하기 위해 일시적으로 데이터가 일관성이 없다는 것을 의미하는 '즉시 일관성'이라고 합니다. 문제가 발생할 경우 데이터가 일관성이 없거나 잘못된 상태에 남아 있을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n아마도 이미 알아챘겠지만, SQL 데이터베이스는 옵션 1을 제공하고 No-SQL 데이터베이스는 옵션 2를 제공합니다.\n\n따라서 모든 데이터를 일관된 상태로 표시해야 하는 솔루션이 필요하다면 SQL 데이터베이스를 사용해야 할 수도 있습니다.\n\n그러나 어느 정도의 불일치를 감수할 수 있다면 혜택이 있습니다. 일관성을 보장하지 않아도 No-SQL 데이터베이스는 수평으로 확장할 수 있습니다. 여러 노드 간의 복제가 필요에 따라 수행될 수 있습니다. 이는 No-SQL 데이터베이스가 대규모 데이터 세트에서 더 잘 수행되며 지리적 영역 간에 콘텐츠 복제를 허용하여 응답 성능을 향상시키므로 특히 데이터가 읽는 횟수가 쓰는 횟수보다 많은 경우입니다.\n\n솔루션에서 '최종 일관성'을 왜 허용해야 하는지 궁금해 할 수도 있습니다. 결국 컴퓨터를 사용하는 이유가 그 정확성을 제공하기 때문이 아닌가요?\n\n<div class=\"content-ad\"></div>\n\n실제로, 마이크로서비스 아키텍처를 채택하고 비동기 이벤트 또는 메시징 대기열을 지원하는 경우 이미 결국 일관성을 수용하는 유효한 설계로 인식했을 것입니다. 대부분의 솔루션에서는 수용 가능한 전략으로 보입니다.\n\n이제 일관성과 유동성이 다른 기술을 선택하는 결정의 기술적 요소로 나타납니다.\n\n# 선택을 어떻게 만드는가\n\n지금까지 특정 문제와 해결책에 대해 고려하고 한 기술 또는 다른 기술을 선택했을 것입니다. 그러나 기억하세요, 일을 수행할 수 있는 능력뿐만 아니라 얼마나 비용이 소요될지도 중요하다고 말했었죠.\n\n<div class=\"content-ad\"></div>\n\n해당 비용은 솔루션을 개발하는 비용뿐만 아니라 운영 및 지원 비용까지 포함됩니다. 이 둘이 합쳐져 총 소유 비용(TCO)을 형성합니다. TCO는 시간에 따른 비용을 고려하므로 다음과 같은 비용을 포함할 수 있습니다:\n\n- 놓친 기회 비용 (시장에 나갈 때까지 걸리는 시간)\n- 초기 개발\n- 향상 및 진화\n- 유지보수\n- 품질 보증\n- 제3자 라이선싱 및 지원\n- 서비스 안정성 및 가용성\n- 확장성\n\n실제로, 기술 선택이 기술 능력보다는 TCO와 관련이 더 큰 경우가 많습니다. 일반적으로 솔루션에 할당된 예산이 TCO를 제한하기 때문입니다.\n\n이러한 비용들은 다음과 같이 넓게 분류될 수 있습니다:\n\n\n<div class=\"content-ad\"></div>\n\n- 출시 시기\n- 개발 노력\n- 운영 지원 및 유지보수\n\n이러한 내용들을 좀 더 자세히 살펴보겠습니다.\n\n## 출시 시기\n\n모든 프로젝트에는 마감 기한이 있습니다. 시장 조건을 충족하거나 수익 목표, 비용 절감 또는 다른 종속성을 충족해야 할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n그 마감 기한을 놓치면 회사에는 금전적인 영향이 있을 수 있어요.\n\n따라서, 운송 시간표에 대한 위험을 최소화하는 솔루션이 필요한 상황이에요.\n\n이와 같은 이유로, 시장에 나가는 시간을 기반으로 한 결정은 어떤 기술을 선호하는 것이 아니라 구현 위험이 최소인 것을 선호합니다. 일반적으로 이미 보유하고 있거나 배송 팀이 가장 익숙한 기술입니다.\n\n이러한 결정은 기술적 부채 수준이 높아지도록 할 수 있지만, 모든 부채와 마찬가지로 현재의 비즈니스 목표를 더 빨리 달성할 수 있도록 해 줄 수 있을 거예요.\n\n<div class=\"content-ad\"></div>\n\n## 개발 노력\n\nNo-SQL 데이터베이스를 도입하면 저장 계층을 설계하고 구현할 필요가 없어져서 개발이 빨라진다고 생각되곤 합니다. 현실적으로는 시간을 절약하는 것이 미미할지라도, 비례적으로 전달 속도를 높이고 위험도를 줄일 수 있습니다.\n\n실제로 데이터 계층을 구성하지 않아도, 여전히 설계되고 그 설계를 준수하는 코드를 작성해야 합니다.\n\nNo-SQL 데이터베이스는 알 수 없는 데이터 구조와 비즈니스 요구 사항을 선호하는 경향이 있습니다. 이는 지금 당장 솔루션이 필요하지만 나중에 확인되지 않을 요구 사항이 있는 상황에 이상적입니다. No-SQL 데이터베이스를 사용하면 이러한 결정으로 인해 발생하는 기술 부채의 양을 줄일 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n기술을 결정하기 전에 개발 팀을 고려해야 합니다. 시장 진입 시기, 솔루션의 품질(기능, 비기능 및 보안 요구 사항 측면) 및 기술 부채 감소를 고려할 때, 개발팀이 알고 있는 기술을 사용하는 것이 더 나아요.\n\n만약 개발 팀이 새로운 기술을 배우거나 관련 기술을 갖춘 팀원을 추가할 수 있는 시간과 예산이 있다면, 개발 시간은 결정에 영향을 미치지 않지만, 경험상 이 경우가 드물다고 합니다.\n\n## 운영 지원 및 유지보수\n\n좋아요, 현재 기술과는 다른 기술을 선택하고 그것을 중심으로 솔루션을 개발했습니다. 모든 것이 잘 진행되었고 결정에 만족하고 있습니다. 솔루션을 운영 환경에 적용하고 나면 문제가 시작됩니다.\n\n<div class=\"content-ad\"></div>\n\n유지보수 지원 비용이 증가한다는 것을 알게 됩니다. 새 기술 라이선싱이 기존 계약에 해당하지 않아 추가 비용이 발생합니다.\n\n1, 2, 3 레벨 지원팀을 포함한 운영팀이 새로운 기술을 이해하지 못하고 개발자들과 비슷한 학습 곡선을 겪어야 합니다. 이는 프로젝트 제공 기간 및/또는 서비스 품질에 영향을 미칩니다.\n\n운영팀이 다양한 심각도의 결함을 관리할 수 있도록 새 운영 지원 도구를 구축해야 합니다. 사람들이 학습하는 과정에서 실수가 발생할 수 있으며 이는 사용자에 영향을 줄 수 있습니다. 복구 시간이 늘어날 수 있습니다.\n\n모든 이는 비즈니스에 영향을 줄 수 있으며, 기술 변경을 고려할 때 고려해야 할 사항(및 계획)입니다.\n\n<div class=\"content-ad\"></div>\n\n# 모두 묶어서\n\n지금까지, 이 글에서 데이터베이스 기술을 선택할 때 고려해야 할 사항을 살펴보았습니다. 요약하면, 결정은 'xyz에서 작업하고 싶다'보다 복잡합니다. 고수준에서는 다음을 포함합니다:\n\n- 데이터의 유동성\n- 일관성의 필요성\n- 시장 진입 시간\n- 개발 노력\n- 운영 지원 및 유지 보수\n\n매일 No-SQL과 SQL 데이터베이스 간의 차이와 격차가 좁아지고 있다는 것을 보여 드렸으면 좋겠습니다. 각각이 프로젝트에 도움이 될 수 있는 서로 다른 특성을 갖고 있지만, 전반적으로 이러한 고유한 이점들이 점점 더 작아지고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n기술적으로 모든 것이 동등한 경우 비즈니스 영향으로 돌아가는데, 일반적으로 재무 영향으로 이어집니다. 초록밭 프로젝트의 여유가 있거나 새로운 개발 팀을 모집할 수 있는 상황이 아니라면, 기존 인수기술을 선호하는 요소가 있을 것입니다.\n\n내 경력에서 기술 선택에 참여해 본 적이 있고, 그에 따라 발생하는 도전에 대해 목격했습니다. 초록밭 프로젝트의 혜택을 누린 바 있고, 나만의 개발 팀을 모집할 수도 있었습니다. 내려받은 임베디드 기술과 개발팀을 승계한 경우도 있었으며 다양한 옵션을 고려해야 했습니다.\n\n거의 모든 경우, 내가 답변해야 하는 질문은 예산과 시간표에 미치는 영향에 관한 것입니다. 모든 부채와 마찬가지로 기술적 부채는 나중에 해결될 것이며 아마도 둘 중 어느 쪽으로든 축적될 것입니다.\n\n이야기로 나오는 증거에 따르면 소프트웨어는 대체되기 전에 5-8년 안에 구식이 됩니다. 이 시간 기간은 전체 현대적 아키텍처의 변화, 시장 변동, 공급업체가 기술을 향상시키도록 압박하는 경쟁자의 압력 등을 기반으로 합니다. 5년 후에 사라지는 신용카드를 어떻게 사용하겠습니까?\n\n<div class=\"content-ad\"></div>\n\n위해 No-SQL과 SQL 중에서 선택할 때, 다음 우선순위를 결정하는 데 도움이 될 것 같아요:\n\n- 데이터 유동성 (설계 시 또는 실행 시)\n- 데이터 일관성의 필요성\n- 이용 가능한 예산 / 시간 일정\n- 기술 및 팀 역량\n- 확장성과 성능\n\n이를 몇 가지 시나리오를 통해 살펴볼 수 있어요:\n\n두 가지 다른 프로젝트를 가정해봅시다:\n\n<div class=\"content-ad\"></div>\n\n프로젝트 #1은 유동적인 데이터 세트이며 데이터 일관성이 필요하지 않으며 대규모 확장이 필요합니다.\n\n프로젝트 #2는 비교적 안정적인 데이터 세트이며 일관성이 필요하며 대규모 확장 요구 사항이 없습니다.\n\n- Greenfield 프로젝트가 주어지면, #1은 No-SQL 옵션을 선택할 것입니다.\n- Greenfield 프로젝트가 주어지면, #2는 SQL 옵션을 선택할 것입니다.\n- 기존 기술과 관련 기술을 보유하고 있으며 충분한 예산과 기술 변경 시간이 있다면, 동일한 선택이 적용됩니다 (#1 = No-SQL) 및 (#2 = SQL).\n- 기존 기술과 관련 기술을 보유하고 있지만 기술 변경을 위한 예산이나 시간이 없다면, 어떤 타협을 해결하기 위한 적절한 전략을 사용하여 기존 기술을 사용하여 #1 또는 #2에 대처할 것입니다.\n\n# 요약\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 No-SQL과 SQL 데이터베이스 사이를 결정할 때 고려하는 몇 가지 요소들을 살펴보았습니다.\n\n주로 의사 결정은 '작업을 수행하는 능력'에 기반하고 있지만 기술이 향상되어 격차가 줄어드는 점을 고려하면 이것은 논의해볼 가치가 있는 주제입니다.\n\n다음으로, 의사 결정은 개발 비용, 지원 및 유지 보수 비용, 운영 비용을 모두 고려한 총 소유 비용(TCO)에 기반하고 있습니다.\n\n새로운 기술을 도입하는 비즈니스 케이스를 정당화하는 것은, 기술적인 측면에 '위기요소'가 없다는 가정하에 이미 보유한 것을 활용하는 것보다 훨씬 어려운 일임을 주목해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n모든 프로젝트, 비즈니스 및 의사 결정 프레임워크는 다릅니다. 그러나 제가 고려해야 할 요소에 대해 조명을 켰기를 바랍니다.\n\n이 기사를 즐겁게 읽으셨기를 바라며, 새로운 것을 학습하여 기술을 향상시켰기를 바랍니다. 그것이 작은 일이더라도요.\n\n만약 이 기사가 흥미롭게 느껴지셨다면, 공감 클랩을 주시면 감사하겠습니다. 이를 통해 어떤 정보가 유용하게 여겨지는지와 앞으로 어떤 주제의 기사를 쓸지를 파악할 수 있습니다. 의견이나 제안이 있으시다면 노트나 답글로 추가해 주시기 바랍니다.","ogImage":{"url":"/assets/img/2024-06-19-HowIchoosebetweenSQLandNo-SQLsolutions_0.png"},"coverImage":"/assets/img/2024-06-19-HowIchoosebetweenSQLandNo-SQLsolutions_0.png","tag":["Tech"],"readingTime":12},{"title":" Tiger Analytics 데이터 엔지니어 역할 면접 경험 ","description":"","date":"2024-06-19 01:45","slug":"2024-06-19-TigerAnalyticsDataEngineerRoleInterviewExperience","content":"\n\n<img src=\"/assets/img/2024-06-19-TigerAnalyticsDataEngineerRoleInterviewExperience_0.png\" />\n\nTiger Analytics와의 데이터 엔지니어링 인터뷰 경험 공유: 🐯 ⚡\n\n### 지원 단계:\n\n- Naukri.com을 통해 지원했고, 다음 날에 리크루터로부터 전화를 받았습니다.\n- 대화 중에 리크루터가 총 경험 및 관련 경력, 현재 및 예상 보수, 그리고 통보 기간과 같은 세부 사항을 요청했습니다.\n\n<div class=\"content-ad\"></div>\n\n---\n### 코딩 라운드: (60분 소요)\n\n이 라운드는 중간 수준 이상이었으며 빠른 완료가 필요한 시간이 많이 소요되는 라운드였어요.\n\n**파이썬, SQL 및 스파크 시나리오:**\n\n<div class=\"content-ad\"></div>\n\n**Spark 코딩:**\n\n- Spark RDD를 활용하여 튜플 목록의 요소 발생 횟수를 세는 작업을 `groupByKey`를 피하고 효율적으로 수행했습니다.\n- RDD 내부의 키-값 쌍의 평균을 계산하기 위해 `groupBy`를 사용하지 않고 `combineByKey`와 같은 Spark 변환을 구현했습니다.\n\n**SQL:**\n\n- 복잡한 테이블 조인, CTEs (공통 테이블 표현식) 및 윈도우 함수가 포함된 두 가지 SQL 문제를 성공적으로 해결했습니다. 이러한 도전은 윈도우 함수로 쿼리를 최적화하고 단일 열과 유사한 값이 있는 테이블을 조인하는 데 중점이 있었습니다.\n\n**Python:**\n\n- 중첩된 리스트를 재귀적으로 해체하고 숫자 범위를 사용하여 리스트를 연결하기 위해 Python을 활용했습니다.\n- Hadoop 아키텍처, SQL, Spark 아키텍처, Spark 변환 및 Spark 최적화 기술에 관련된 이론 질문에 답했습니다.\n\n<div class=\"content-ad\"></div>\n\n### 기술 토론 (소요 시간: 60분):\n\n- 이 라운드는 아키텍트 수준의 면접관에의해 수행되었으며 매우 도전적이었습니다.\n- 면접관은 시간을 엄수하며 소개로 시작하여 예의 바르게 행동했습니다.\n- 프로젝트 아키텍처에 대한 깊은 탐구로 토론을 시작했고, 깊이 있는 질문에 대답하였습니다.\n- 복잡한 테이블 조인, CTEs (공통 테이블 표현식), 윈도우 함수를 활용한 2개의 SQL 문제를 성공적으로 해결하였습니다.\n- PySpark로 전환하여 두 가지 PySpark 문제를 해결하는 능력을 증명하였습니다.\n- Apache Spark에 대해 깊이 파헤치며 아키텍처, 재분할 및 병합과 같은 최적화 기술, 응용 프로그램 관리, 단계, 작업, DAG (방향 그래프), 계보 그래프를 다루었습니다.\n- 응답에 대한 소중한 피드백을 받아 기술적 능력 및 팀 기대치 및 문화와의 일치에 대한 통찰력을 얻었습니다.\n\n— -\n\n### 인사담당자 인터뷰:\n\n<div class=\"content-ad\"></div>\n\n- HR 라운드는 친근하게 진행되었습니다. 기본 소개로 시작하여 위치 및 급여에 대한 논의로 이어졌어요.\n- Tiger의 정책을 논의하고 급여 조건을 협상했어요.\n- 한 주 이내에 입사 제안을 받았고, 입사 날짜는 제 퇴직 기간 이후로 예정되었어요.\n- 전체 과정이 한달도 안 되는 시간 내에 완료되었어요.\n\n—-\n\n도움이 되었기를 바랍니다. 😀\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*YPpBH2cBCLbFNc_C2eTlsQ.gif)","ogImage":{"url":"/assets/img/2024-06-19-TigerAnalyticsDataEngineerRoleInterviewExperience_0.png"},"coverImage":"/assets/img/2024-06-19-TigerAnalyticsDataEngineerRoleInterviewExperience_0.png","tag":["Tech"],"readingTime":2},{"title":"SQL 윈도우 함수 데이터 열정가들을 위한 최고의 도구","description":"","date":"2024-06-19 01:43","slug":"2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts","content":"\n\n\n![SQL Window Functions](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_0.png)\n\n요즘에는 엄청난 양의 데이터를 다루고 있습니다. 이 주요한 도전에 따라 다양한 소스의 복잡도도 함께 증가하고 있습니다. 이러한 환경에서 SQL은 여전히 영웅이며, 이 데이터 바다에서 가치 있는 통찰을 추출하고 탐색하는 데 꼭 필요한 도구입니다.\n\nSQL이 제공하는 많은 강력한 기능 중에서도 윈도우 함수는 특히 주목할 만한 요소입니다. 이러한 함수들은 테이블 행 집합을 대상으로 높명한 계산을 가능하게 하며, 고급 데이터 분석에 필수적이며 데이터와 상호작용하는 방법을 변화시키는 데 중요합니다.\n\n이 기사에서는 SQL의 윈도우 함수 개념을 해부하고 이해할 것입니다. 언제 윈도우 함수를 사용해야 하는지, 그리고 SQL 쿼리에서 효과적으로 구현하는 방법에 대해 살펴볼 것입니다. 이 가이드를 마치면 윈도우 함수의 강력함과 유연성에 대한 깊은 이해를 얻게 될 것이며, 데이터 분석 기술을 향상시키기 위한 실제 예제를 활용할 수 있을 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 윈도우 함수가 뭔가요?\n\n경험 수준에 상관 없이 모든 데이터 애호가는 윈도우 함수에 대해 들어봤거나 사용해 본 적이 있을 것입니다. 이 강력한 도구들은 모든 SQL 강좌에서 퍼져 있으며 데이터 작업을 하는 사람들의 일상생활에서 필수불가결합니다.\n\n구글에서 빠르게 검색을 해보죠…몇 분 후에 혹은 TV 광고를 보고 나서, 우리는 윈도우 함수가 다음과 같다는 사실을 알게 됩니다:\n\n<div class=\"content-ad\"></div>\n\n# 문법에 관해서 뭔가 언급했다고 했나요?\n\n그렇습니다. 이 매우 강력한 도구에는 특정 구문과 같은 트릭이 함께 제공됩니다.\n\n![image](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_1.png)\n\n위 이미지에서 볼 수 있듯이, 윈도우 함수의 구문은 네 부분으로 나뉠 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 집계/함수: 여기서 집계 (예: AVG, SUM) 또는 LAG(), LEAD(), ROW_NUMBER(), RANK(), DENSE_RANK()과 같은 특정 창 함수를 배치하여 작업을 시작합니다. 몇 가지 더 있지만, 이 중에서는 가장 일반적으로 사용되는 것들이에요 (적어도 저는 이것들을 가장 많이 사용해요 😁)\n- OVER: 이 키워드는 윈도우 함수를 사용할 것임을 IDE에 \"알리는\" 데에 사용됩니다. 이는 \"여기서 무언가를 할 것이고, 무언가 복잡한 것에 대비해야 한다\"고 말하는 것과 같아요.\n- PARTITION BY: 이 절은 결과를 파티션 또는 창으로 나눕니다. 우리는 이 과정에서 초기에 설정한 집계나 함수를 적용할 것입니다. 이 부분을 작성한 후에는 파티션을 기준으로 필드를 개발해야 합니다. 순위 함수와 함께 사용되지 않아요.\n- ORDER BY: 경우에 따라 선택 사항일 수 있지만, 이것이 하는 일을 알아두는 것이 좋아요. 이는 각 파티션 내의 행을 정렬하는 데 사용되며, RANK(), DENSE_RANK(), ROW_NUMBER()와 같은 순위 함수를 사용할 때 유용합니다.\n\n# 목표를 달성하기 위한 다양한 창 함수\n\n이전 섹션에서 창 함수 구문에 대해 이야기했어요. 창 함수 구문과 독립적으로 작동하지 않는 몇 가지 함수를 언급했죠.\n\n일부는 각 파티션의 각 행에 대한 순위 값을 반환하기 때문에 순위 함수라고 불리며, 다른 것은 시계열 창 함수입니다.\n\n<div class=\"content-ad\"></div>\n\n순위 함수:\n\n- RANK() : 결과 집합의 파티션 내 각 행에 순위를 할당하며, 동일한 값을 가진 행은 동일한 순위를 받습니다.\n- DENSE_RANK() : RANK()와 유사하지만 연속적인 순위 값을 가집니다. 동일한 값은 동일한 순위를 받으며, 다음 순위 값은 다음 연속 정수입니다.\n- NTILE() : 결과 집합을 동일한 그룹으로 분할하고 각 행에 속하는 그룹을 나타내는 숫자를 할당합니다.\n- ROW_NUMBER() : 결과 집합의 파티션 내 각 행에 고유한 연속 정수를 할당하며, 각 파티션의 첫 번째 행부터 1로 시작합니다.\n\n시계열 함수:\n\n- LAG() : 결과 집합 내 이전 행의 값을 가져오는 함수로, 자체 조인이 필요하지 않습니다. 연속된 행 간의 차이를 계산하는 데 도움이 됩니다.\n- LEAD() : 다음 행의 값을 예측하는 데 유용한, 자체 조인 없이 다음 행의 값을 액세스할 수 있습니다. 추세나 값의 변화를 예측하는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 영원한 질문: 왜...\n\n많은 것들에 대해 우리가 하는 일반적인 질문들이 있습니다. SQL의 창 함수도 예왽이 아닙니다. 창 함수가 여러분에게 시간과 노력을 절약해줄 수 있는 상황을 이해하려면 다음을 살펴보겠습니다:\n\n왜 그리고 언제 우리는 창 함수를 사용해야 할까요?\n\n언제부터 시작해볼까요. 언제 우리는 창 함수를 사용할까요? 잘, 우리가 창 함수를 사용해야 하는 시점은 언제든지 우리가 필요로 할 때 입니다:\n\n<div class=\"content-ad\"></div>\n\n- 특정 조건에 따라 데이터 하위 집합에서 누적 합계, 순위, 평균 또는 다른 계산을 계산합니다.\n- 현재 및 이전/다음 행 값 비교\n\n왜 윈도우 함수를 사용해야 하는지 왜도 빼놓지 마세요. 상황에 필요할 때 윈도우 함수를 사용해야 하는 이유는 무엇인가요?\n \n윈도우 함수를 사용해야 하는 이유:\n\n- 행 레벨 세부 정보 유지 — 데이터를 축소하지 않고 계산을 수행할 수 있는데, 이는 원본 데이터를 유지한 채 여러 행을 대상으로 계산할 수 있도록 합니다.\n- 복잡한 쿼리 간소화 — 이 도구를 사용하면 가장 복잡한 쿼리를 간소화하여 읽기 좋고 작성하기 쉽고, 무엇보다도 유지보수하기 쉽게 만들어줍니다.\n- 성능 향상 — SQL 엔진에서 최적화되어 대량 데이터셋의 경우 더 나은 성능을 제공하는 경우가 많습니다.\n- 고급 분석 활성화 — 누적 합계, 이동 평균 및 기타 고급 분석 작업을 실행할 수 있도록 합니다.\n- 자세한 분석을 위한 데이터 파티션 — 특정 기준에 따라 데이터를 분할하여 전체 데이터 집계 없이 그룹 내에서 자세한 분석을 가능하게 합니다.\n- 시계열 및 변경 감지 지원 — 이전 또는 다음 행 값에 액세스하는 내장 지원을 제공하여 시계열 데이터 및 변경 감지에 유용합니다.\n\n<div class=\"content-ad\"></div>\n\n# 실제 사용 사례\n\n은행 분야에서 데이터 엔지니어로 일하고 있는데, 계약의 \"단계\"가 변경된 레코드를 식별하고 이 변경 날짜를 기록해야 하는 요청을 받았어요.\n\n쉽게 말해, 그렇게 하는 게 쉽지 않을 것 같죠? 그렇게는 안 돼요. 윈도우 함수를 사용해서 요청을 완료하고 결과를 빠르게 전달하는 데 도움이 되었어요.\n\n우리가 두 개의 테이블이 있다고 가정해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n\nsource.data_records\n\n![Image 2](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_2.png)\n\nand temp.data_records:\n\n![Image 3](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_3.png)\n\n\n<div class=\"content-ad\"></div>\n\n그리고 다음 안에 테이블을 생성해야 합니다. 그 안에는 다음과 같은 정보가 포함되어 있어야 합니다:\n\n- 식별자\n- 식별자의 현재 레벨\n- 현재 단계의 참조 날짜\n- 식별자의 이전 레벨\n- 이전 참조 날짜\n- 식별자가 레벨을 변경한 날짜\n\n테이블은 아래 코드를 기반으로 생성되었습니다:\n\n```js\ncreate table tmp_change_level_date as\n(\nselect distinct * from ( \n    select \n        fct.identifier, fct.level, fct.date_ref,\n        lag(fct.level) over (partition by fct.identifier order by fct.date_ref) as previous_level,\n        lag(fct.date_ref) over (partition by fct.identifier order by fct.date_ref) as previous_date,\n        case\n            when lag(fct.level) over (partition by fct.identifier order by fct.date_ref) is not null then fct.date_ref\n            else NULL\n        end as change_level_date,\n        dense_rank() over (partition by fct.identifier order by fct.date_ref desc) as ranks\n    from source.data_records fct  join temp.data_records TFCT \n    on fct.identifier = TFCT.identifier\n    where TFCT.amount <> 0 and TFCT.account in (select account_code from accounts_list)\n    ) x\nwhere ranks = 1 \nand level <> previous_level\nand previous_date <> change_level_date\n)\ncommit;\n```\n\n<div class=\"content-ad\"></div>\n\n자, 이제 설명으로 들어가볼게요:\n\n- 우선적으로, loan identifier(대출 식별자), level, date_ref(대출의 실제 단계 및 현재 단계의 기준 날짜)와 같은 정보를 가져오는 주요 SELECT 문을 만들었습니다:\n\n```js\nselect \n        fct.identifier, fct.level, fct.date_ref,\n        lag(fct.level) over (partition by fct.identifier order by fct.date_ref) as previous_level,\n        lag(fct.date_ref) over (partition by fct.identifier order by fct.date_ref) as previous_date,\n        case\n            when lag(fct.level) over (partition by fct.identifier order by fct.date_ref) is not null then fct.date_ref\n            else NULL\n        end as change_level_date,\n        dense_rank() over (partition by fct.identifier order by fct.date_ref desc) as ranks\n    from source.data_records fct  join temp.data_records TFCT \n    on fct.identifier = TFCT.identifier\n    where TFCT.amount <> 0 and TFCT.account in (select account_code from accounts_list)\n    ) x\n```\n\n그 다음으로, 각 대출에 대해 이전 대출 단계와 이전 참조 날짜를 가져오기 위해 LAG() 함수를 사용했습니다. PARTITION BY를 사용하여 식별자에 따라 데이터셋을 작은 파티션으로 나누고, 각 파티션 내에서 레코드를 date_ref에 따라 정렬했습니다.\n\n<div class=\"content-ad\"></div>\n\n\nlag(fct.level) over (partition by fct.identifier order by fct.date_ref) as previous_level,\nlag(fct.date_ref) over (partition by fct.identifier order by fct.date_ref) as previous_date\n\n\nand assign a rank to each record within the partition by using DENSE_RANK() function:\n\n\ndense_rank() over (partition by fct.identifier order by fct.date_ref desc) as ranks\n\n\nThis code will return the following result:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_4.png\" />\n\n더 나아가서, 이전 결과에서 일부 필터를 적용할 수 있도록 다음 SELECT문을 작성합니다 (위의 표에 해당하는):\n\n```js\nselect distinct * from (\n\n---- 이전 select를 하위 쿼리로 사용 ----\n\n) x\nwhere ranks = 1 \nand level <> previous_level\nand previous_date <> change_level_date\n```\n\n그리고 각 식별자에 대해 가장 최근 레코드만 가져와서 (ranks = 1은 설명에서 앞에서 언급한 가장 최근 레코드에 해당함), 현재 레벨이 이전 레벨과 다른 레코드만 가져오도록 필터를 적용하며 (level != previous_level), 변경 날짜가 유효하고 이전 참조 날짜와 다른지 확인합니다. 이러한 필터를 기반으로 결과를 새로운 테이블 tmp_change_level_date에 삽입합니다 (CREATE TABLE table_name AS와 유명한 구문을 사용하여 만든 것):\n\n<div class=\"content-ad\"></div>\n\n\n![SQL Window Functions](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_5.png)\n\n이 결과를 통해 다음을 알 수 있습니다:\n\n- 식별자 2의 경우: 레벨이 2023년 03월 15일에 A에서 C로 변경되었습니다.\n- 식별자 3의 경우: 레벨이 2023년 02월 20일에 B에서 A로 변경되었습니다.\n\n# 결론\n\n\n<div class=\"content-ad\"></div>\n\nSQL 윈도우 함수는 복잡한 데이터 분석을 간편하게 하고 성능을 향상시킵니다. 이 글에서는 기본 사항, 구문, 랭킹 및 시계열 분석과 같은 일반적인 사용 사례, 실제 예제에 대해 다룹니다. 이러한 함수를 숙달하면 SQL 쿼리를 더 효율적이고 통찰력 있게 만들 수 있습니다.\n\n실습하고 실험하여 그 능력을 최대로 발휘하고 데이터 분석 능력을 향상시키세요.","ogImage":{"url":"/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_0.png"},"coverImage":"/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_0.png","tag":["Tech"],"readingTime":8},{"title":"최고의 Kafka에서 Delta 적재를 위한 도구들을 벤치마킹합니다","description":"","date":"2024-06-19 01:41","slug":"2024-06-19-NeedforSpeedBenchmarkingtheBestToolsforKafkatoDeltaIngestion","content":"\n\n# 소개\n\n데이터브릭스 플랫폼에서 카프카에서 델타 테이블로 데이터를 수집하는 시리즈의 두 번째 부분에 다시 오신 것을 환영합니다. 이전에 우리가 논의한 것을 더욱 발전시키면서, 카프카 토픽으로 합성 데이터를 생성하고 스트리밍하는 것에 대해 살펴보겠습니다.\n\n본 블로그 포스트에서는 델타 레이크로부터 아파치 카프카에서 스트리밍 데이터를 수집하기 위한 데이터브릭스 플랫폼에서 사용 가능한 세 가지 강력한 옵션을 벤치마킹합니다: 데이터브릭스 잡, 델타 라이브 테이블(DLT) 및 델타 라이브 테이블 서버리스(DLT 서버리스). 주요 목표는 이러한 접근 방식을 통해 카프카에서 델타 테이블로 데이터를 수집할 때의 엔드 투 엔드 지연 시간을 평가하고 비교하는 것입니다.\n\n지연 시간은 중요한 지표이며, 하류 분석 및 의사 결정 프로세스에 사용 가능한 데이터의 신선도와 적시성에 직접적인 영향을 미칩니다. 모든 세 가지 도구가 Apache Spark의 구조적 스트리밍을 내부적으로 활용한다는 점을 강조해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 벤치마크 설정\n\n## 벤치마킹 기준\n\n측정한 주요 메트릭은 대기 시간이었습니다 - 카프카에서 행이 생성된 시점부터 델타 레이크에서 이용 가능해질 때까지 걸리는 시간입니다. 대기 시간은 정밀하게 측정되었으며 정확성을 보장하고 변동성을 고려하기 위해 장기간에 걸쳐 신중하게 측정되었습니다.\n\n## 입력 카프카 피드\n\n<div class=\"content-ad\"></div>\n\n저희가 수행한 벤치마크에서는 매 초 100개의 행을 생성하는 Kafka 피드를 활용했어요. 각 행은 대략 1MB로, 초당 100MB로 이루어져요. 연간으로 계산하면 약 3.15 페타바이트가 되어, 저희가 선택한 도구의 수신 능력을 평가하기 위한 엄격한 테스트 베드가 됐어요.\n\nConfluent Cloud를 사용하여 6개 파티션으로 Kafka 클러스터를 설정했는데, 5분 미만이 걸렸어요. 그리고 실험을 위해 300달러의 크레딧을 제공해 주었어요.\n\n![이미지](/assets/img/2024-06-19-NeedforSpeedBenchmarkingtheBestToolsforKafkatoDeltaIngestion_0.png)\n\n## 비교 도구\n\n<div class=\"content-ad\"></div>\n\n- Databricks 작업: Kafka에서 읽고 Delta Lake 테이블에 쓰기 위해 Apache Spark Structured Streaming을 활용합니다. 작업 구성 및 예약에 유연성을 제공하지만 클러스터 리소스의 수동 관리가 필요합니다.\n- Delta Live Tables (DLT): Kafka에서 Delta Lake로 데이터를 입력하기 위해 선언적 접근 방식을 사용하며, 인프라를 자동으로 관리하고 파이프라인 개발을 간소화합니다.\n- Delta Live Tables Serverless (DLT Serverless): DLT와 동일한 입력 작업을 수행하면서, 인프라 관리를 더 간소화하기 위한 서버리스 모델을 활용합니다. 자동 스케일링과 리소스 최적화를 제공합니다.\n\n## 지연 시간은 어떻게 측정되었나요?\n\n지연 시간은 테이블로의 연속적인 스트리밍 업데이트 타임스탬프 간의 밀리초 단위 시간 차이를 계산하여 측정됩니다. 이는 각 순차적 커밋에 대해 이전 업데이트의 타임스탬프를 현재 업데이트의 타임스탬프에서 뺌으로써 수행되며, 각 업데이트가 이전 업데이트에 비해 처리하는 데 얼마나 걸리는지 분석할 수 있습니다. 분석은 현재 300개의 커밋으로 제한되어 있지만 필요에 따라 조정할 수 있습니다.\n\n```js\nfrom pyspark.sql import DataFrame\n\ndef run_analysis_about_latency(table_name: str) -> DataFrame:\n    # Python 다중 라인 문자열로 형식 지정된 SQL 명령어 텍스트\n    sql_code = f\"\"\"\n        -- 테이블의 업데이트 이력에 대한 가상 뷰 정의\n        WITH VW_TABLE_HISTORY AS (\n          -- 테이블의 역사적 변화 설명\n          DESCRIBE HISTORY {table_name}\n        ),\n        \n        -- 이전 쓰기 작업의 타임스탬프를 계산하는 뷰 정의\n        VW_TABLE_HISTORY_WITH_previous_WRITE_TIMESTAMP AS (\n          SELECT\n            -- 현재 작업 이전의 마지막 쓰기 작업의 타임스탬프를 계산\n            lag(timestamp) OVER (\n              PARTITION BY 1\n              ORDER BY version\n            ) AS previous_write_timestamp,\n            timestamp,\n            version\n          FROM\n            VW_TABLE_HISTORY\n          WHERE\n            operation = 'STREAMING UPDATE'\n        ),\n        \n        -- 연속 커밋 간의 밀착 정도를 분석하는 뷰 정의\n        VW_BOUND_ANALYSIS_TO_N_COMMITS AS (\n          SELECT\n            -- 이전 및 현재 쓰기 타임스탬프 간의 밀리초 단위 시간 차이 계산\n            TIMESTAMPDIFF(\n              MILLISECOND,\n              previous_write_timestamp,\n              timestamp\n            ) AS elapsed_time_ms\n          FROM\n            VW_TABLE_HISTORY_WITH_previous_WRITE_TIMESTAMP\n          ORDER BY\n            version DESC\n          LIMIT\n            300  -- 최근 300개 커밋만 분석\n        )\n        \n        -- 쓰기 지연 시간에 대한 다양한 통계 계산\n        SELECT\n          avg(elapsed_time_ms) AS average_write_latency,\n          percentile_approx(elapsed_time_ms, 0.9) AS p90_write_latency,\n          percentile_approx(elapsed_time_ms, 0.95) AS p95_write_latency,\n          percentile_approx(elapsed_time_ms, 0.99) AS p99_write_latency,\n          max(elapsed_time_ms) AS maximum_write_latency\n        FROM\n          VW_BOUND_ANALYSIS_TO_N_COMMITS\n    \"\"\"\n    # Spark의 SQL 모듈을 사용하여 SQL 쿼리 실행\n    display(spark.sql(sql_code))\n```\n\n<div class=\"content-ad\"></div>\n\n# 데이터 수집\n\n이 코드는 Apache Spark를 사용하여 Kafka topic에서 데이터를 효율적으로 수집하는 스트리밍 데이터 파이프라인을 설정합니다. Kafka 메시지에서 예상되는 데이터 유형 및 열에 맞게 구성된 스키마를 정의하고, 차량 세부 정보, 지리적 좌표 및 텍스트 필드를 포함합니다. read_kafka_stream 함수는 스트리밍 프로세스를 초기화하고 Kafka에 안전하고 신뢰할 수 있는 연결을 구성하며, 지정된 주제를 구독하여 개선된 처리 속도를 위해 여러 파티션을 통해 데이터를 처리합니다. 스트림은 정의된 스키마에 따라 JSON 형식 메시지를 디코딩하고 필수 메타데이터를 추출합니다.\n\n```js\nfrom pyspark.sql.types import StructType, StringType, FloatType\nfrom pyspark.sql.functions import *\n\n# DataFrame 구조에 기반한 스키마 정의\nschema = StructType() \\\n    .add(\"event_id\", StringType()) \\\n    .add(\"vehicle_year_make_model\", StringType()) \\\n    .add(\"vehicle_year_make_model_cat\", StringType()) \\\n    .add(\"vehicle_make_model\", StringType()) \\\n    .add(\"vehicle_make\", StringType()) \\\n    .add(\"vehicle_model\", StringType()) \\\n    .add(\"vehicle_year\", StringType()) \\\n    .add(\"vehicle_category\", StringType()) \\\n    .add(\"vehicle_object\", StringType()) \\\n    .add(\"latitude\", StringType()) \\\n    .add(\"longitude\", StringType()) \\\n    .add(\"location_on_land\", StringType()) \\\n    .add(\"local_latlng\", StringType()) \\\n    .add(\"zipcode\", StringType()) \\\n    .add(\"large_text_col_1\", StringType()) \\\n    .add(\"large_text_col_2\", StringType()) \\\n    .add(\"large_text_col_3\", StringType()) \\\n    .add(\"large_text_col_4\", StringType()) \\\n    .add(\"large_text_col_5\", StringType()) \\\n    .add(\"large_text_col_6\", StringType()) \\\n    .add(\"large_text_col_7\", StringType()) \\\n    .add(\"large_text_col_8\", StringType()) \\\n    .add(\"large_text_col_9\", StringType())\n\ndef read_kafka_stream():\n    kafka_stream = (spark.readStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers_tls ) \n      .option(\"subscribe\", topic )\n      .option(\"failOnDataLoss\",\"false\")\n      .option(\"kafka.security.protocol\", \"SASL_SSL\")\n      .option(\"kafka.sasl.mechanism\", \"PLAIN\") \n      .option(\"kafka.sasl.jaas.config\", f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_api_key}\" password=\"{kafka_api_secret}\";')\n      .option(\"minPartitions\",12)\n      .load()\n      .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"), \"topic\", \"partition\", \"offset\", \"timestamp\", \"timestampType\" )\n      .select(\"topic\", \"partition\", \"offset\", \"timestamp\", \"timestampType\", \"data.*\")\n    )\n    return kafka_stream\n```\n\n## 설명:\n\n<div class=\"content-ad\"></div>\n\n- Connection Setup: 특정 부트스트랩 서버를 사용하여 Kafka에 연결하고 SASL_SSL과 같은 보안 설정을 포함하여 암호화 및 인증된 데이터 전송을 합니다.\n- Topic Subscription: 지정된 Kafka 주제에 가입하여 계속해서 새로운 데이터를 수신합니다.\n- Stream Configuration: 잠재적인 데이터 손실을 처리하고 여러 파티션 간의 데이터를 처리 속도를 높이기 위해 견고하게 구성됩니다.\n- Data Transformation: 수신된 JSON 메시지를 설정된 스키마에 따라 디코딩하기 위해 from_json을 사용하며 Spark 내에서 구조화된 형식으로 변환합니다.\n- Metadata Extraction: Kafka 주제, 파티션 및 메시지 타임 스탬프와 같은 필수 메타데이터를 추출합니다.\n\n이 설정은 Kafka에서 Spark로의 데이터 흡수를 최적화하고 데이터를 추가적으로 처리하거나 Delta Lake와 같은 저장 시스템으로 통합하기 위한 준비를 합니다.\n\n## Databricks Jobs을 위한 추가 코드\n\n구성: 이 방법은 Databricks 작업 및 클러스터 리소스를 설정하는 것을 포함하며 유연한 스케줄링 및 흡수 프로세스 모니터링을 가능하게 합니다만, 올바른 컴퓨팅을 선택하는 것을 이해해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n(  \n  read_kafka_stream()\n  .writeStream\n  .option(\"checkpointLocation\",checkpoint_location_for_delta)\n  .trigger(processingTime='1 second')\n  .toTable(target)\n)\n```\n\n## Delta Live Tables에 대한 추가 코드\n\n구성: Delta Live Tables는 인프라를 자동으로 관리하여 데이터 파이프라인을 구성하는 간단하고 선언적인 방식을 제공합니다.\n\n이 코드 스니펫은 Delta Live Tables (DLT) API를 사용하여 Kafka에서 스트리밍 데이터를 수신하는 데이터 테이블을 정의합니다. @dlt.table 데코레이터를 사용하여 테이블의 이름을 지정하고 (원하는 테이블 이름으로 대체), 파이프라인을 매 초 Kafka를 폴링하도록 설정합니다. 이 신속한 폴링은 거의 실시간 데이터 처리 요구를 지원합니다. dlt_kafka_stream() 함수는 read_kafka_stream()을 호출하여 Kafka 스트리밍을 DLT로 직접 통합하여 Databricks 환경 내에서의 관리 및 운영을 간소화합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n@dlt.table(name=\"여기에 바꿔야 할 DLT 테이블 이름\",\n           spark_conf={\"pipelines.trigger.interval\" : \"1 초\"})\ndef dlt_kafka_stream():\n    read_kafka_stream()\r\n```\n\n# 결론\n\n저희의 벤치마크 결과에 따르면, 델타 라이브 테이블 서버리스는 대기 시간 성능과 운영 간소화 측면에서 우수한 성과를 보여주며, 다양한 데이터 부하가 있는 시나리오에 매우 적합합니다. 한편, 데이타브릭스 잡과 델타 라이브 테이블도 실용적인 솔루션을 제공합니다.\n\n- 대기 시간 비교: 델타 라이브 테이블의 서버리스 버전은 모든 측정 백분위수에서 대기 시간 측면에서 다른 것들을 능가합니다.\n- 운영 복잡성: 델타 라이브 테이블 서버리스는 수동 인프라 관리가 필요하지 않은 가장 단순한 설정을 제공하며, 그 다음으로 델타 라이브 테이블, 그리고 데이타브릭스 잡이 이어집니다. \n\n<div class=\"content-ad\"></div>\n\n## Delta Live Tables Serverless가 표준 Delta Live Tables를 능가하는 이유\n\nDelta Live Tables Serverless가 표준 Delta Live Tables보다 우수한 성능을 발휘하는 핵심 요소는 Stream Pipelining을 활용한 것입니다. 표준 구조화된 스트리밍과 달리 DLT Serverless는 마이크로배치를 동시에 처리함으로써 처리량을 향상시키고 응답 시간을 크게 향상시킵니다. 이 기능을 통해 전체 컴퓨팅 자원 활용도도 크게 향상됩니다. Stream Pipelining은 서버리스 DLT 파이프라인의 기본 기능이며, 데이터 입력과 같은 스트리밍 데이터 워크로드를 위해 성능을 최적화합니다.\n\n또한, 수직 자동 스케일링은 DLT Serverless의 효율성을 향상시키는 중요한 역할을 합니다. 이 기능은 Databricks Enhanced Autoscaling의 수평 자동 스케일링 기능을 보완하여 DLT 파이프라인을 실행하는 데 필요한 가장 비용 효율적인 인스턴스 유형을 자동으로 선택합니다. 더 많은 자원이 필요할 때 적절하게 대규모 인스턴스로 확장하고, 메모리 사용률이 지속적으로 낮을 때 축소합니다. 이러한 다이내믹한 조정은 실시간 요구 사항에 기반하여 드라이버 및 워커 노드를 최적으로 조정하여 중요한 역할을 합니다. 제품 모드에서 파이프라인을 업데이트하거나 개발 중 수동으로 조정하는 경우에도, 수직 자동 스케일링은 메모리 부족 오류가 발생한 후 신속하게 더 큰 인스턴스를 할당하여 서비스 중단 없이 자원 할당을 최적화합니다.\n\nStream Pipelining과 수직 자동 스케일링은 운영 복잡성을 감소시키고 Serverless DLT 파이프라인의 신뢰성 및 비용 효율성을 향상시킵니다. 이러한 기능을 통해 Serverless DLT는 수동 개입을 최소화하면서 변동하는 데이터 입력 부하를 처리하는 데 이상적인 선택지가 되어 더 빠르고 효율적인 데이터 파이프라인 실행을 실현합니다.\n\n<div class=\"content-ad\"></div>\n\n# 각주:\n\n이 글을 읽어 주셔서 감사합니다. 도움이 되었거나 즐거우셨다면 박수를 치는 것을 고려해 주시고, 다른 사람들이 이를 발견할 수 있도록 도와주세요. 더 많은 정보를 찾으려면 제 웹사이트 CanadianDataGuy.com을 방문하시고, 더 많은 통찰력 있는 콘텐츠를 제공하기 위해 제 팔로우도 잊지 마세요. 여러분의 지원과 피드백은 제게 귀중하며, 제 작품에 대한 여러분의 참여를 감사히 여깁니다.","ogImage":{"url":"/assets/img/2024-06-19-NeedforSpeedBenchmarkingtheBestToolsforKafkatoDeltaIngestion_0.png"},"coverImage":"/assets/img/2024-06-19-NeedforSpeedBenchmarkingtheBestToolsforKafkatoDeltaIngestion_0.png","tag":["Tech"],"readingTime":10},{"title":"트위터가 매일 40억 건의 이벤트를 실시간으로 처리하는 방법","description":"","date":"2024-06-19 01:40","slug":"2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily","content":"\n\n## 람다에서 카파로\n\n# 목차\n\n- 문맥과 과제\n- 이전 아키텍처\n- 새로운 아키텍처\n- 평가\n\n# 소개\n\n<div class=\"content-ad\"></div>\n\n몇 주 전에는 우리가 어떻게 Uber가 실시간 인프라를 처리하여 매일 수백만 건의 이벤트를 처리하는지 배웠습니다. 이번 주에는 데이터 실시간 처리 요구 사항에 대한 다른 대형 기술 회사의 처리 방법을 살펴볼 것입니다: 트위터.\n\n# 맥락과 도전 과제\n\n트위터는 매일 400조 건의 이벤트를 실시간으로 처리하고 페타바이트(PB)의 데이터를 생성합니다. 이벤트는 다양한 소스(하둡, 카프카, 구글 빅쿼리, 구글 클라우드 스토리지, 구글 퍼브섭 등)에서 발생합니다. 트위터는 데이터의 대규모 규모에 대응하기 위해 각 요구 사항에 특화된 내부 도구를 구축했습니다: 배치 처리용 Scalding, 스트림 처리용 Heron, 배치 및 실시간 처리용 TimeSeries AggregatoR 프레임워크, 데이터 소비용 데이터 액세스 레이어.\n\n기술의 견고성에도 불구하고 데이터의 성장은 인프라에 압력을 가합니다; 가장 현저한 예는 상호 작용 및 참여 파이프라인인데, 이는 대규모 데이터를 배치 및 실시간으로 처리합니다. 이 파이프라인은 Tweet와 사용자 상호 작용 데이터를 다양한 수준의 집계 및 메트릭스 차원을 사용하여 추출하기 위해 다양한 실시간 스트림 및 서버 및 클라이언트 로그에서 데이터를 수집하고 처리합니다. 이 파이프라인의 집계 데이터는 트위터의 광고 수익과 다양한 데이터 제품 서비스의 진실의 원천 역할을 합니다. 따라서 이 파이프라인은 낮은 지연 시간과 높은 정확성을 보장해야 합니다. 트위터가 이 임무를 처리하는 방법을 살펴봅시다.\n\n<div class=\"content-ad\"></div>\n\n# 오래된 아키텍처\n\n## 개요\n\n![이미지](/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_0.png)\n\n트위터는 처음에 람다 아키텍처를 사용했습니다. 이는 정확한 일괄 데이터 뷰를 제공하는 일괄 처리와 온라인 데이터를 보여주는 실시간 스트림 처리 두 개의 별도 파이프라인이 있습니다. 두 뷰 출력은 하루가 끝날 때 합쳐집니다. 트위터는 다음과 같이 아키텍처를 구축했습니다:\n\n<div class=\"content-ad\"></div>\n\n- Summingbird Platform: 제가 이해한 바로는, 이 플랫폼에는 Scalding과 Heron과 같은 여러 분산 엔진들과 MapReduce 로직을 정의하고 이를 해당 엔진에서 실행할 수 있도록 허용하는 전용 라이브러리가 포함되어 있습니다.\n- TimeSeries AggregatoR: 견고하고 확장 가능한 실시간 이벤트 시계열 집계 프레임워크.\n- Batch: 배치 파이프라인의 소스는 로그, 클라이언트 이벤트 또는 HDFS의 트윗 이벤트에서 나올 수 있습니다. Summingbird 플랫폼으로 데이터를 전처리하고 결과를 맨해튼 분산 저장 시스템에 저장하기 위해 많은 Scalding 파이프라인이 사용됩니다. 비용을 절감하기 위해 Twitter는 배치 파이프라인을 한 데이터 센터에 배치하고 데이터를 다른 2개 데이터 센터에 복제합니다.\n- 실시간: 실시간 파이프라인의 소스는 Kafka 주제에서 나옵니다. 데이터는 Summingbird 플랫폼 내의 Heron으로 \"흘러들어가\", 그런 다음 Heron의 결과가 Twitter Nighthawk 분산 캐시에 저장됩니다. 배치 파이프라인과 달리, 실시간 파이프라인은 3개 서로 다른 데이터 센터에 배포됩니다.\n- 배치 및 실시간 저장소 위에 쿼리 서비스가 있습니다.\n\n## 도전\n\n실시간 데이터의 대규모 및 높은 처리량으로 인해 데이터 손실 및 부정확성의 위험이 있습니다. 이벤트 스트림에 대한 처리 속도가 따라가지 못할 경우, Heron 토폴로지에서 백프레셔가 발생할 수 있습니다 (방향성을 갖는 비순환 그래프는 데이터 처리의 Heron 흐름을 나타냅니다). 시스템이 어느 정도 동압을 겪으면, Heron 볼트(일종의 노동자로 생각할 수 있음)가 지연이 누적되어 전체 시스템 대기 시간이 길어질 수 있습니다.\n\n뿐만 아니라, 백프레셔로 인해 많은 Heron 스트림 매니저 (스트림 매니저는 토폴로지 구성 요소 간의 데이터 라우팅을 관리함)가 실패할 수 있습니다. Twitter의 해결책은 스트림 매니저를 다시 시작하여 스트림 매니저를 복구하는 것입니다. 그러나, 재시작은 행사 손실을 일으킬 가능성이 있어 파이프라인의 전반적 정확성을 줄일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 새 아키텍처\n\n## 개요\n\n![이미지](/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_1.png)\n\n새로운 접근 방식으로 트위터는 Kappa 아키텍처를 사용하여 하나의 실시간 파이프라인으로 솔루션을 간소화했습니다. 이 아키텍처는 트위터 내부 및 Google Cloud Platform 솔루션을 활용할 것입니다:\n\n<div class=\"content-ad\"></div>\n\n- 온프레미스: 그들은 카프카 토픽 이벤트를 구글 파브섭 이벤트 형식으로 변환하는 전처리 서비스를 구축했습니다.\n- 구글 클라우드: 이벤트 들어오기 위해 파브섭을 사용하고, 중복 제거 및 실시간 집계에는 Dataflow 작업을 활용하며, 결과를 저장하기 위해 BigTable을 사용합니다.\n\n새로운 아키텍처의 프로세스 흐름은 다음과 같이 설명할 수 있습니다:\n\n- 단계 1: 소스 카프카 토픽에서 데이터를 소비하고 변환 및 필드 재매핑을 수행한 후 최종 결과를 중간 카프카 토픽으로 보냅니다.\n- 단계 2: 이벤트 프로세서는 중간 카프카 토픽에서 데이터를 파브섭 형식으로 변환하고 이벤트에 데이터 중복 제거를 위해 사용되는 UUID 및 처리 컨텍스트와 관련된 일부 메타정보를 추가합니다.\n- 단계 3: 이벤트 프로세서는 이벤트를 구글 파브섭 토픽으로 보냅니다. 트위터는 메시지가 구글 클라우드로 적어도 한 번 방식으로 전달되도록 PubSub을 게시하는 이 프로세스를 거의 무한정 재시도합니다.\n- 단계 4: 구글 Dataflow 작업은 파브섭에서 데이터를 처리합니다. Dataflow 작업자는 실시간으로 데이터 중복 제거 및 집계를 처리합니다.\n- 단계 5: Dataflow 작업자는 집계 결과를 BigTable에 기록합니다.\n\n# 평가\n\n<div class=\"content-ad\"></div>\n\n## 새로운 접근 방식의 성취\n\n- 오래된 아키텍처의 10초에서 10분 지연과 비교하여 대략 10초의 지연이 안정적으로 유지됩니다.\n- 실시간 파이프라인은 최대 ~100MB/s 인 이전 아키텍처 대비 대략 1GB/s의 처리량을 달성할 수 있습니다.\n- Google Pubsub에 최소 한 번 데이터 게시 및 데이터 흐름으로 부터의 중복 제거 작업으로 거의 정확히 한 번 처리 보장.\n- 일괄 처리 파이프라인 구축 비용 절감.\n- 더 높은 집계 정확도 달성.\n- 늦게 발생하는 이벤트 처리 기능.\n- 다시 시작 시 이벤트 손실 없음\n\n## 중복 비율 모니터링 방법\n\n<img src=\"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_2.png\" />\n\n<div class=\"content-ad\"></div>\n\n트위터는 두 개의 별도 Dataflow 파이프라인을 생성합니다: 하나는 Pubsub에서 원시 데이터를 직접 BigQuery로 전달하고, 다른 하나는 중복 제거된 이벤트 카운트를 BigQuery로 내보냅니다. 이 방식으로 Twitter는 중복 이벤트 백분율 및 중복 제거 후의 백분율 변경을 모니터링할 수 있습니다.\n\n## 이전 배치 파이프라인의 중복 제거된 개수를 새 Dataflow 파이프라인과 비교하는 방법은?\n\n![image](/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_3.png)\n\n- BigTable에 쓰는 것 외에도, 새 워크플로우는 중복 제거된 및 집계된 데이터를 BigQuery로 내보냅니다.\n- 트위터는 또한 이전 배치 데이터 파이프라인 결과를 BigQuery로 로드합니다.\n- 중복 카운트를 비교하기 위해 예약된 쿼리를 실행합니다.\n- 결과는 새로운 파이프라인 결과 중 95% 이상이 이전 배치 파이프라인과 정확히 일치한다는 것입니다. 5%의 차이는 주로 원래 배치 파이프라인이 지연된 이벤트를 버린 반면, 새 파이프라인은 효율적으로 포착할 수 있기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n# 마무리말\n\nTwitter가 새로운 Kappa 아키텍처로 전환함으로써, 예전 아키텍처와 비교하여 지연 시간과 정확성 면에서 크게 개선되었습니다. 더 나은 성능 뿐만 아니라, 새로운 아키텍처는 데이터 파이프라인을 간소화하여 스트림만을 유지했습니다.\n\n다음 블로그에서 뵙겠습니다.\n\n# 참고문헌\n\n<div class=\"content-ad\"></div>\n\n[1] Lu Zhang and Chukwudiuto Malife, 트위터에서 실시간으로 수십억 개의 이벤트 처리하기 (2021)","ogImage":{"url":"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_0.png"},"coverImage":"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_0.png","tag":["Tech"],"readingTime":5}],"page":"100","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true}