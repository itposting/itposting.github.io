{"pageProps":{"posts":[{"title":"우주 탐험 Midjourney AI로 NASA 급 우주 이미지 생성하는 방법","description":"","date":"2024-06-23 20:15","slug":"2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI","content":"\n\n우주 이미지를 좋아해요. 허블 우주 망원경, 신 개발된 웹 우주 망원경, 그리고 카시니, 준호, 뉴 호라이즌스 임무에서 얻은 사진들이 정말 놀라웠어요.\n\n우주는 정말 멋진 모양과 색으로 가득 차 있어요. 우주는 사진을 찍기에 아주 뛰어난 곳이죠. 이 모든 우주 망원경과 임무들은 우리에게 그 먼 곳들에 대한 지식 뿐만 아니라 새로운 아름다움의 범주도 가져다 주었어요.\n\n여기 저기, 이 거대한 광활함 속에는 매력적인 이름을 가진 물체들이 있어요. 자석별, 중성자 별 같은 멋진 이름들도 있고요. 별의 죽음 이후에 행성 크기의 다이아몬드와 흑홀 같은 것도 있답니다.\n\n![우주 이미지](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_0.png)\n\n<div class=\"content-ad\"></div>\n\n우주 흡수분자운에는 태양계보다 훨씬 큰 크기의 에틸포르메이트로 가득 차 있어요. 이것이 산딸기에 달콤한 향기를 주는 원인입니다. 거기에는 알코올과 설탕으로 이루어진 구름들도 있답니다.\n\n![이미지](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_1.png)\n\n우리가 상상한 것보다 더 이상하고 예쁘답니다. Midjourney로 흥미로운 작품 몇 개 만들어보지 않으면 안 될 것 같아요.\n\nMidjourney는 제 기대를 저버리지 않았어요. 우주와 그 안에 있는 아름다운 현상들의 이미지를 만드는 데 꽤 많은 시간을 보냈죠. 그 이미지들 대부분이 \"와우\" 요소를 담고 있는 거죠.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_2.png)\n\nMidjourney을 사용하여 놀라운 우주 이미지를 만드는 데 사용할 단어를 알면 좋습니다. 만약 천문학 애호가라면 문제없이 진행할 수 있을 것입니다.\n\n천문학에 대해 잘 모르는 경우 다음은 시도해볼만한 멋진 시각 우주 현상 중 일부입니다:\n\n- 나선 은하\n\n\n<div class=\"content-ad\"></div>\n\n수은 궤도\n\n금성 궤도\n\n지구 궤도\n\n화성 궤도\n\n<div class=\"content-ad\"></div>\n\n퀘이사르\n\n별 군집\n\n외계 행성\n\n행성 링\n\n<div class=\"content-ad\"></div>\n\n한편, Midjourney는 매우 복잡한 프롬프트 없이 공간 이미지를 잘 다루고 있다고 생각해요. \"스타일화\" 값이 높은 것도 도움이 된다고 생각해요.\n\n여기 몇 가지 나의 프롬프트 예시가 있어요:\n\n\n![ExploringtheCosmos](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_3.png)\n\n\n나는 우주 이미지를 만들 때 처음으로 새로운 \"개인화된\" 설정을 시도할 기회가 있었어요. Midjourney가 며칠 전에 소개했고, 나의 이미지 순위 설정에 기반을 두었어요.\n\n<div class=\"content-ad\"></div>\n\n이제까지 9000개 이상의 이미지를 평가해 왔기 때문에, 내 개인 설정 효과는 시작해보고 최소 요구되는 200개의 이미지만 평가한 사람들과는 매우 다를 것이라고 생각해요.\n\n이 새로운 설정은 몇몇 이미지를 훨씬 더 나아지게 만든 것 같아요. 하지만 일관된 향상이 아니라, 어떻게 작동하는지에 대한 정보도 매우 적기 때문에 아직은 아무것도 확실하지 않아요.\n\n아래는 Markdown 형식의 이미지 링크입니다:\n\n![Exploring the Cosmos](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_4.png)\n\n좋은 소식은 다른 사람들의 개인화된 코드를 사용할 수 있다는 거에요. 만약 내 설정을 시도하고 싶다면, \"— p 7fp27l4\"를 입력해보세요.\n\n<div class=\"content-ad\"></div>\n\n나는 우주 이미지를 만드는 것을 너무 좋아하는 이유 중 하나는 상상력과 탐험을 위한 많은 공간이 주어진다는 것이에요.\n\n사턴의 달 유럽에 있는 갤저들이 어떻게 보일지, 또는 새로 태어난 별을 보고 싶어요. 분자 구름을 통과하며 새끼 별들을 보고 그 위에 살아가는 생물들이 라즈베리 알코올 구름을 먹고 있는지 확인해보고 싶어요.\n\n![space image](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_5.png)\n\n그리고 Midjourney와 함께 만들어진 멋진 이미지들은 스타일 참조로도 사용될 수 있다는 걸 알았어요.\n\n<div class=\"content-ad\"></div>\n\n일련의 프롬프트 기술을 배우는 것도 좋은 방법이에요. 누구나 호스헤드 성운에 대해 알고 있어요. 우리만의 것을 만들어볼까요? 예를 들어 댄서 성운이나 강아지 머리 성운 같은 것 말이에요?\n\n![](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_6.png)\n\n저는 이런 것들에 빠져들게 되면, 곧 나만의 우주 생명체와 함께 전체 세계를 만들기 시작해요. 이것이 전적으로 좋은 일은 아니지만, 그래도 이 글을 쓸 시간이 더 적어지는 거 같아요.\n\n![](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_7.png)\n\n<div class=\"content-ad\"></div>\n\n그렇게 짧지만, 흥미로운 기사였길 바랍니다. 여러분의 우주 탐험 여행을 시작하는 데 도움이 되었으면 좋겠습니다.\n\n그리고 여러분이 놀라운 성운과 은하, 달과 행성, 그리고 가장 놀라운 생물들로 가득한 자신만의 우주를 만들기를 바랍니다.\n\nAivaras Grauzinis","ogImage":{"url":"/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_0.png"},"coverImage":"/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_0.png","tag":["Tech"],"readingTime":4},{"title":"구도 규칙을 사용하여 Midjourney 이미지를 개선하는 방법","description":"","date":"2024-06-23 20:13","slug":"2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules","content":"\n\n모든 사진 작가는 가장 아름다운 주제, 완벽한 조명, 그리고 멋진 배경을 가졌다 하더라도, 좋은 구도가 없다면 보기 좋은 이미지를 얻을 수 없을 수도 있다는 것을 알고 있어요.\n\n사진과 예술에서의 이미지 구도 규칙들은 인간 심리와 생리학에 깊게 뿌리를 두고 있습니다. 우리 뇌와 몸이 시각 자극을 해석하는 방식을 반영하고 있죠. 이 중 하나인 삼분의 규칙은 이미지를 두 개의 동일한 간격을 두고 수평 및 수직으로 균일하게 나눠 아홉 부분으로 나누는데, 이 구도 지침은 인간 눈이 이미지 내 특정 지점으로 유인되는 자연적 경향과 일치합니다. 시선 추적 연구에 따르면 시청자들의 눈은 이 교차점으로 자연스럽게 이끌리며, 주요 요소를 이 지점에 배치하는 것이 더 균형을 이루고 매력적인 구도를 만들 수 있다는 것을 시사합니다. 이 현상은 복잡한 정보를 효율적으로 처리하는 우리 뇌의 선호성과 관련이 있다고 생각되어, 전체 장면에 압도당하지지 않고 중요한 요소에 집중할 수 있게 합니다.\n\n생리학적으로, 눈과 뇌가 이미지 처리를 함께 하는 방식은 구도 규칙에 영향을 미칩니다. 예를 들어, 선도라는 개념은 이미지 내 자연스러운 선을 활용해 시청자의 눈길을 특정 초점지점으로 이끕니다. 이 기술은 선과 가장자리가 처리되는 시각 체계가 어떻게 작용하는지를 활용하는데, 이것은 인간 눈의 구조와 시각 정보를 해석하는 신경 경로에 뿌리를 두고 있는 시각 인식의 근본적인 측면입니다. 또한 대칭과 패턴의 사용은 조화와 질서감을 조성해 우리의 균형을 선호함에 부합합니다. 이 선호성은 생존에 중요한 패턴을 찾고 예측하고자 하는 우리 뇌의 욕구와 관련이 있다고 생각되어집니다. 이 심리학적이고 생리학적인 기초를 이해하면 사진 작가나 예술가들은 시청자의 깊게 와닿는 구도를 만들어내며, 감정적 반응을 불러일으키고 시각적 흥미를 유지하게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n하지만 Midjourney의 이미지에 관해서는 AI 자체나 사용자들도 구성 규칙에 대해 크게 신경 쓰지 않는 것 같아요. 그것은 나쁜 일이에요; 그래서 Midjourney로 만들어진 몇몇 그 외 완벽한 이미지들이 심심하고 흥미로워 보이지 않는 이유가 되죠.\n\n![이미지](/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_1.png)\n\nMidjourney는 이미지의 주제를 중앙에 배치하려고 할 거예요. 그리고 그것은 우리 눈이 일반적으로 끌리는 이미지 영역을 벗어나게 돼요. 가끔 운 좋게, Midjourney가 선험적인 라인을 만들어내거나 썩이 구성을 만들어내기도 하는데, 그런 경우들은 즉시 더 나아 보일 거에요.\n\n사진과 예술 구성에는 많은 규칙이 있어요. 유감스럽게도 Midjourney는 그 중 많은 규칙을 따르지 않겠지만, 몇 가지 기본 규칙을 추가하면 이미지가 훨씬 더 나아 보일 거에요.\n\n<div class=\"content-ad\"></div>\n\n여기 구성 규칙의 기본 키워드 목록이 있어요:\n\n- 세분의 법칙: 이미지를 가로 세로로 세분하여 주요 대상을 교차점 중 하나에 배치하는 지침.\n- 리딩 라인: 이미지 내부에 있는 라인으로 시선을 주요 대상으로 이끄는 역할.\n- 대칭: 양쪽이 서로 대칭을 이루는 균형 잡힌 이미지 비율.\n- 비대칭: 이미지 내에서 균형을 깨고 흥미와 긴장을 조성할 수 있는 불균형.\n- 리딩 스페이스: 대상 앞에 있는 공간으로 주로 초상화나 행동 사진에서 사용됨.\n- 대각선: 대각선 라인을 사용하여 이미지에 다이나믹한 움직임을 만드는 방법.\n- 골든 삼각형: 이미지를 대각선으로 나누어 동적인 조합을 만드는 방법.\n- 대조: 두 가지 대조적인 요소를 나란히 배치하여 차이점을 부각시키는 방법.\n- 삼각형 구성: 삼각형 모양으로 요소를 배열하여 안정감을 조성하는 방법.\n- 프레임 내 프레임: 장면 요소를 사용하여 대상 주변에 보조 프레임을 만드는 방법.\n\nMidjourney의 구성 가이드와 함께 결과를 살펴보겠습니다. 이렇게 간단한 프롬프트를 사용해볼게요:\n\n결과를 살펴보세요. 이미지에는 여성, 등지고 있는 거리, 오래된 도시가 모두 포함되어 있어요. 그런데 지루하고 나쁜 이미지에요.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_2.png)\n\n이미지 중앙에 우리 여성분이 있지만, 시선을 그녀 쪽으로 이끄는 것은 없습니다. 그녀는 중앙에 있는 어두운 물체일 뿐입니다.\n하지만, 화면 구성 지침으로 프롬프트를 수정하면 상황이 달라집니다:\n\n![이미지](/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_3.png)\n\n\n<div class=\"content-ad\"></div>\n\n이 이미지는 훨씬 더 나아졌어요. 여자가 이미지 가로의 세분의 하나 지점에 위치하고 있어서 눈이 자연스럽게 그녀를 향해 갑니다. 또한 중앙 공간을 비워두어 거리의 깊이로 이어지는 모습을 만들어 더 흥미로운 이미지로 만들었어요.\n\n우리의 프롬프트에 더 많은 구성 규칙을 추가해 봅시다, 이번엔 선도:\n\n\"세분의 일 구도와 선으로 이뤄진 이미지, 오래된 마을에서 습기 낀 랜턴으로 비춰지는 길을 걷는 외로운 여자.\"\n\n![이미지](/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_4.png)\n\n<div class=\"content-ad\"></div>\n\n어떤 사진이 좋네요. 등잔과 포장된 도로가 시선을 우리 레이디 쪽으로 이끌어줍니다. 이미지 너비의 1/3 지점에 여전히 그녀를 배치하고, 밝은 물체와 넓은 공간이 있더라도 그녀는 분명한 주목의 중심입니다.\n\n동일한 안내문의 변형입니다: 다시 한 번 포장된 도로 라인들이 우리의 대상을 향해 눈길을 이끄는 좋은 조각이며 이미지는 흥미롭고 신비로우며 매력적입니다.\n\n![image](/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_5.png)\n\n가끔은 이미지 중앙에 대상을 두고 싶을 수도 있습니다. 특정 상황에서는 합리적이며 흥미로운 이미지를 만들기도 합니다. 하지만 규칙은 그대로입니다: 눈길을 대상물 쪽으로 이끌 것이 있어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n이 경우에는 대칭 구성과 선도 사용하여 작업을 완료할 수 있습니다.\n\n![image](/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_6.png)\n\n이미지가 수평으로 대칭되어 있음을 주목해 보세요. 포장 위의 반사가 수직 대칭 요소를 더해주며, 라인과 등불이 여성의 주목을 이끕니다.\n\n작업에 대칭 구성 규칙을 의존하는 대신, 이미지에 효과나 요소를 추가하여 시선을 주제로 이끄는 것도 가능합니다. 여기에 빛을 이용한 나선 모양 페인팅을 이미지에 추가한 예시가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n마크다운 포맷으로 테이블 태그를 변경하십시오.\n\n<div class=\"content-ad\"></div>\n\n간단한 구성 규칙과 지침을 활용하여 훨씬 강력하고 흥미로운 Midjourney 이미지를 만들 수 있어요. 그리고 그 규칙을 활용하고 Midjourney를 더 높은 수준으로 이끌어내는 것을 즐기시길 바래요. \n\nAivaras Grauzinis","ogImage":{"url":"/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_0.png"},"coverImage":"/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_0.png","tag":["Tech"],"readingTime":4},{"title":"웹 스크래핑으로 데이터 파이프라인 구축하기 단계별 가이드","description":"","date":"2024-06-23 20:10","slug":"2024-06-23-BuildingDataPipelineswithWebScrapingAStep-by-StepGuide","content":"\n\n웹 스크레이핑은 웹을 거대한 데이터 광산으로 변신시키는 기술이에요. 여기에는 모든 정보가 발굴되기를 기다리는 잠재적 보석이 숨어 있답니다.\n\n웹 스크레이핑 또는 데이터 스크래핑은 인터넷에서 콘텐츠와 정보를 수집하는 편리한 방법이에요. 기본적으로 소프트웨어나 봇을 사용해 웹사이트를 방문하고 페이지를 가져와 필요한 데이터를 추출하는 과정을 말해요. 이 프로세스를 자동화함으로써 이러한 봇은 아주 빠르게 다량의 데이터를 수집할 수 있어요.\n\n어떤 도구를 사용하든, 웹 스크레이핑 도구의 작동 방식은 아래와 같아요:\n\n단계 1: 서버로 HTTP 요청을 보내는 것부터 시작돼요.\n\n<div class=\"content-ad\"></div>\n\n단계 2: 그런 다음, 웹 사이트의 코드를 추출하고 분해합니다.\n\n단계 3: 마지막으로, 중요한 데이터를 로컬로 저장합니다.\n\n웹 스크레이퍼는 사이트에 액세스할 수 있는 권한이 필요합니다. 따라서 먼저 하는 일은 대상 사이트로 HTTP 요청을 보내는 것입니다. 사이트가 승인하면 스크레이퍼가 해당 사이트의 HTML 또는 XML 코드를 읽고 추출할 수 있습니다. 이 코드는 사이트의 콘텐츠와 구조에 대한 청사진과 같습니다.\n\n스크레이퍼는 그런 다음 코드를 구문 분석합니다. 이는 기본적으로 코드를 부분으로 나누어 특정 요소나 객체를 식별하고 원하는 특정 요소나 정보를 가져올 수 있도록 하는 것을 의미합니다. 이것은 특정 텍스트, 평점, 클래스, 태그, ID 또는 필요한 다른 정보와 같은 것들일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n웹 크롤러가 HTML 또는 XML에 접근하여 데이터를 가져오고 구문 분석한 후, 해당 데이터를 로컬로 저장합니다. 이미 봇에게 수집하길 원하는 내용을 알려줬으니, 무엇을 찾아야 하는지 알고 있어요. 이 데이터는 주로 구조화된 형식으로 저장되며, .csv 또는 .xls과 같은 Excel 파일 형식으로 저장합니다.\n\n## 이것이 웹 스크래핑의 기본 아이디어입니다!\n\n먼저, 어떤 웹사이트(들)를 스크래핑하고 어떤 특정 데이터를 추출하려는지 결정해야 합니다. 그런 다음, 이 데이터가 웹사이트의 백엔드 코드에서 어디에 위치하는지 찾아야 합니다. 여러분의 목표는 관련 콘텐츠를 감싸고 있는 고유한 태그를 찾는 것입니다. 예를 들면 `div` 태그 같은 거죠.\n\n이러한 태그들을 식별했다면, 이를 선호하는 스크래핑 소프트웨어에 입력해야 합니다. 이렇게 하면 봇이 정확히 어디를 봐야 하는지와 무엇을 추출해야 하는지 알게 됩니다. 다음 단계는 스크래핑 프로세스를 실행하는 것이죠. 여기서 스크래퍼는 사이트에 액세스 권한을 요청하고, 데이터를 추출하며, 구문 분석합니다.\n\n<div class=\"content-ad\"></div>\n\n데이터를 추출하고 구문 분석하고 수집한 후에는 저장해야합니다. 이제 필요한 데이터를 가지고 있으므로 자유롭게 다루고 원하는 대로 사용할 수 있습니다.\n\n웹 스크래핑에 많이 사용되는 여러 도구와 라이브러리가 있으며, 각각의 특징과 장점이 있습니다. 인기있는 몇 가지 도구는 다음과 같습니다:\n\n- Beautiful Soup:\n\nHTML 및 XML 문서를 구문 분석하는 파이썬 라이브러리입니다. HTML 페이지를 구문 분석하고 데이터를 추출하기 위한 파싱 트리를 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n사용 사례: 간단한 스크래핑 작업 및 데이터 추출.\n\n- Scrapy:\n\n웹 스크레이퍼를 구축하기 위한 오픈 소스 Python 프레임워크입니다. Beautiful Soup보다 더 강력하고 견고하며 더 복잡한 스크래핑 작업에 좋습니다.\n\n사용 사례: 대규모 스크래핑 프로젝트 및 데이터 추출 워크플로우.\n\n<div class=\"content-ad\"></div>\n\n- Selenium:\n\n웹 브라우저를 자동화하는 도구입니다. 주로 웹 애플리케이션을 테스트하는 데 사용되지만 JavaScript와 상호 작용이 필요한 동적 콘텐츠를 스크래핑하는 데도 사용할 수 있습니다.\n\n활용 사례: 양식 작성 및 버튼 클릭과 같이 상호 작용이 필요한 웹 사이트에서 동적 콘텐츠를 스크래핑하는 것.\n\n- Puppeteer:\n\n<div class=\"content-ad\"></div>\n\nNode.js 라이브러리인 Puppeteer는 Chrome 또는 Chromium을 DevTools 프로토콜을 통해 제어하기 위한 고수준 API를 제공합니다. Selenium과 유사하지만 특히 Chrome을 위한 것입니다.\n\n사용 사례: 동적 웹사이트 스크래핑 및 자동 브라우저 작업 수행.\n\nBeautiful Soup와 Scrapy는 Python 사용자에게 훌륭한 도구입니다.\n\n아래는 웹 스크래핑에 관한 프로젝트이며, 아래 웹사이트는 저에게 기초 학습을 도와주었습니다: Web Scraping & NLP in Python | DataCamp\n\n<div class=\"content-ad\"></div>\n\n여기서 Project Gutenberg 웹사이트에서 책 정보를 가져올 것입니다.\n\nProject Gutenberg는 60,000권 이상의 무료 eBook을 제공하는 자원 봉사자 주도의 디지털 도서관입니다. 1971년 Michael S. Hart에 의해 설립되어 세계에서 가장 오래된 디지털 도서관입니다. 이 컬렉션은 무료로 공개적으로 이용 가능합니다. 여기에는 고전 문학, 참고 자료 및 기타 문화적으로 중요한 텍스트들이 포함되어 있습니다. 책은 일반 텍스트, HTML 및 ePub 형식으로 제공되며, 다양한 기기에서 접근할 수 있습니다. Project Gutenberg의 목표는 eBook의 창작과 배포를 촉진하고 이러한 작품들을 미래 세대를 위해 보존하는 것입니다.\n\n스크레이핑을 위해 Python 패키지 requests를 사용하여 이 웹 데이터에서 소설을 추출할 것입니다. 그런 다음 Natural Language ToolKit (nltk)을 사용하여 소설을 분석해볼 것입니다.\n\n아래는 'Dead Men Tell No Tales'라는 epub을 스크랩하는 예시입니다.\n\n<div class=\"content-ad\"></div>\n\n## 1. 라이브러리 설치\n\n```js\n# !pip install html5lib\n# !pip install contractions\n# !pip install spacy\n# python -m spacy download en_core_web_sm (명령 프롬프트에서 실행)\n```\n\nhtml5lib은 HTML 및 XHTML 문서를 구문 분석하기 위한 순수한 Python 라이브러리입니다.\n\ncontractions은 텍스트에서 축약어를 확장하는 데 사용됩니다 (예: \"don`t\"를 \"do not\"로 바꿉니다).\n\n<div class=\"content-ad\"></div>\n\n스파시는 파이썬에서 고급 자연어 처리(NLP)를 위한 오픈 소스 라이브러리입니다.\n\n## 2. 필요한 모든 파이썬 라이브러리와 모듈을 임포트하세요\n\n```js\nimport re  #정규 표현식을 위해\nimport bs4  #HTML 및 XML 문서를 구문 분석하기 위해\nimport nltk  #자연어 처리 (NLP) 작업을 위해\nimport spacy  #고급 NLP 작업을 위해\nimport string  #일반적인 문자열 작업을 위해\nimport requests  #HTTP 요청을 만들기 위해\nimport contractions  #텍스트 내 축약어 확장을 위해\nimport seaborn as sns  #통계적 데이터 시각화를 위해\nfrom nltk.util import ngrams  #n-그램 생성을 위해\nfrom bs4 import BeautifulSoup  #HTML 및 XML을 구문 분석하기 위해\nfrom collections import Counter  #해시 가능한 객체를 계산하기 위해\nimport matplotlib.pyplot as plt  #데이터 플로팅을 위해\nfrom nltk.corpus import stopwords  #일반적인 불용어에 액세스하기 위해\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize #텍스트 토큰화를 위해\n\nnltk.download('punkt')\nnltk.download('words')\nnltk.download('stopwords')\nnltk.download('maxent_ne_chunker')\nnltk.download('averaged_perceptron_tagger')\n```\n\nNLTK 자원 다운로드하기:\n\n<div class=\"content-ad\"></div>\n\n- 그런 다음 코드는 여러 필수 NLTK 자원을 다운로드합니다:\n    - punkt: 텍스트를 문장이나 단어 목록으로 분할하는 토크나이저입니다.\n    - words: 영어 단어 목록입니다.\n    - stopwords: 영어에서 흔히 사용되는 불용어 목록입니다.\n    - maxent_ne_chunker: 미리 훈련된 개체명 청커입니다.\n    - averaged_perceptron_tagger: 품사 태거입니다.\n\n이 설정을 통해 다양한 NLP 및 텍스트 처리 작업을 수행하기 위해 필요한 모든 라이브러리와 자원이 제공됩니다.\n\n## 3. URL에서 HTML 콘텐츠를 가져와 표시하기\n\n```js\n# URL 저장\nurl = 'https://www.gutenberg.org/cache/epub/1703/pg1703-images.html'\n# 요청 보내고 객체 유형 확인\nr = requests.get(url, verify=False)\n# 응답 객체에서 HTML 추출하고 출력\nhtml = r.text\nprint(html)\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n# 출력 예시:\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\"><style>\n#pg-header div, #pg-footer div {\n    all: initial;\n    display: block;\n    margin-top: 1em;\n    margin-bottom: 1em;\n    margin-left: 2em;\n}\n#pg-footer div.agate {\n    font-size: 90%;\n    margin-top: 0;\n    margin-bottom: 0;\n    text-align: center;\n}\n#pg-footer li {\n    all: initial;\n    display: block;\n    margin-top: 1em;\n    margin-bottom: 1em;\n    text-indent: -0.6em;\n}\n#pg-footer div.secthead {\n    font-size: 110%;\n    font-weight: bold;\r\n```\n\n웹페이지의 URL은 url 변수에 저장되어 있습니다. 이 URL은 'Dead Men Tell No Lies' 책의 온라인 epub 버전을 얻을 수 있는 페이지를 가리킵니다.\nrequests.get 메서드는 지정한 URL로 HTTP GET 요청을 보냅니다.\n\nverify=False는 SSL 인증서 확인을 비활성화하는 데 사용됩니다(운영 환경에서 사용하지 않는 것이 좋음).\n\n<div class=\"content-ad\"></div>\n\n응답의 HTML 콘텐츠는 r.text를 사용하여 추출됩니다.\n\n## 4. BeautifulSoup를 사용하여 HTML 구문 분석하기\n\n```js\n# HTML에서 BeautifulSoup 객체 생성\nsoup = BeautifulSoup(html, \"html.parser\")\n# soup 제목 가져오기\nsoup.title\n\n#출력\n<title>\n      Dead Men Tell No Tales, by E. W. Hornung\n    </title>\n```\n\n이 코드는 \"html.parser\"를 사용하여 HTML 콘텐츠를 구문 분석하는 BeautifulSoup 객체를 초기화합니다. 그런 다음 이 웹 페이지의 제목인 \"Dead Men Tell No Tales, by E. W. Hornung\"을 추출하고 출력합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 문자열로 된 스프 타이틀 가져오기\nsoup.title.string\n\n#결과\n'\\r\\n      Dead Men Tell No Tales, by E. W. Hornung\\r\\n    '\n```\n\n이 코드는 타이틀 태그의 텍스트 내용을 문자열로 추출합니다. 결과에는 웹 페이지의 제목이 포함되며, 앞뒤의 공백 문자도 포함되어 있습니다.\n\n## 5. BeautifulSoup로 하이퍼링크 추출하기\n\n```js\n# 스프에서 하이퍼링크 가져오기 및 처음 몇 개 확인하기\nsoup.findAll('a')[:8]\n\n# 결과\n[<a class=\"reference external\" href=\"https://www.gutenberg.org\">www.gutenberg.org</a>,\n <a class=\"pginternal\" href=\"#link2HCH0001\"> CHAPTER I. </a>,\n <a class=\"pginternal\" href=\"#link2HCH0002\"> CHAPTER II. </a>,\n <a class=\"pginternal\" href=\"#link2HCH0003\"> CHAPTER III. </a>,\n <a class=\"pginternal\" href=\"#link2HCH0004\"> CHAPTER IV. </a>,\n <a class=\"pginternal\" href=\"#link2HCH0005\"> CHAPTER V. </a>,\n <a class=\"pginternal\" href=\"#link2HCH0006\"> CHAPTER VI. </a>,\n <a class=\"pginternal\" href=\"#link2HCH0007\"> CHAPTER VII. </a>]\n```\n\n<div class=\"content-ad\"></div>\n\n이 코드는 HTML에서 모든 앵커(`a`) 태그를 찾아 처음 여덟 개를 출력합니다. 각 앵커 태그에는 하이퍼링크(href 속성)와 관련된 텍스트가 포함되어 있습니다.\n\n## 6. HTML에서 텍스트 내용 추출 및 인쇄\n\n```js\n# 뷰티풀수프에서 텍스트를 추출하고 인쇄합니다\ntext = soup.get_text()\nprint(text)\n```\n\n```js\n#출력: 예시\n   Dead Men Tell No Tales, by E. W. Hornung\n\nThe Project Gutenberg eBook of Dead Men Tell No Tales\nThis ebook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this ebook or online\nat www.gutenberg.org. If you are not located in the United States,\nyou will have to check the laws of the country where you are located\nbefore using this eBook.\nTitle: Dead Men Tell No Tales\n\nAuthor: E. W. Hornung\n\nRelease date: April 1, 1999 [eBook #1703]\n                Most recently updated: June 10, 2022\nLanguage: English\nCredits: Produced by An Anonymous Project Gutenberg Volunteer, and David Widger\n\n*** START OF THE PROJECT GUTENBERG EBOOK DEAD MEN TELL NO TALES ***\n\n      DEAD MEN TELL NO TALES\n\n      By E. W. Hornung\n```\n\n<div class=\"content-ad\"></div>\n\n이 코드는 파싱된 HTML 문서에서 모든 텍스트 콘텐츠를 추출하기 위해 get_text() 메서드를 사용합니다. 그런 다음 print 문은 이 텍스트 콘텐츠를 콘솔에 출력하여 HTML 태그없이 웹페이지의 전체 텍스트 콘텐츠를 표시합니다.\n\n## 7. 추출된 텍스트 정리\n\n```js\ndef clean_text(text):\n    # 비 문자 숫자 문자를 제거합니다 (공백 및 구두점 제외)\n    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,]', '', text)\n    # 여러 공백을 단일 공백으로 대체합니다\n    text = re.sub(r'\\s+', ' ', text)\n    # 모든 숫자를 공백으로 대체합니다\n    text = re.sub(r'\\d+', '', text)\n    text = contractions.fix(text)\n    text = text.lower()\n    return  \" \".join(text.split())\n\n\n# 'text' 열에 정리 함수를 적용합니다\ntext_c = clean_text(text)\ntext_c\n```\n\n이 코드는 다음과 같은 정리 단계를 수행합니다:\n\n<div class=\"content-ad\"></div>\n\n- 알파벳 숫자만 남기기: 문자, 숫자, 공백, 마침표 및 쉼표만 유지합니다.\n- 공백 정규화: 여러 개의 공백을 단일 공백으로 대체합니다.\n- 숫자 제거: 텍스트에서 모든 숫자를 제거합니다.\n- 축약형 풀기: 축약형을 전체 형태로 확장합니다(예: \"don't\"를 \"do not\"로) contractions 라이브러리를 사용하여.\n- 소문자로 변환: 모든 문자를 소문자로 변환합니다.\n- 추가 공백 제거: 단어 사이에 선행, 후행 또는 추가 공백이 없는지 확인합니다.\n\n정리된 텍스트는 text_c 변수에 저장됩니다.\n\n```js\n#출력\n\n'dead men tell no tales, by e. w. hornung the project gutenberg ebook of dead men tell no tales this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever. you may copy it, give it away or reuse it under the terms of the project gutenberg license included with this ebook or online at www.gutenberg.org. if you are not located in the united states, you will have to check the laws of the country where you are located before using this ebook. title dead men tell no tales author e. w. hornung release date april , ebook most recently updated june , language english credits produced by an anonymous project gutenberg volunteer, and david widger start of the project gutenberg ebook dead men tell no tales dead men tell no tales by e. w. hornung contents chapter i. love on the ocean chapter ii. the mysterious cargo chapter iii. to the waters edge chapter iv. the silent sea chapter v. my reward chapter vi. the sole survivor chapter vii. i find a friend chapter viii. a small precaution chapter ix. my convalescent home chapter x. wine and weakness chapter xi. i live again chapter xii. my ladys bidding chapter xiii. the longest day of my life chapter xiv. in the garden chapter xv. first blood chapter xvi. a deadlock chapter xvii. thieves fall out chapter xviii. a man of many murders chapter xix. my great hour chapter xx. the statement of francis rattray chapter i. love on the ocean nothing is so easy as falling in love on a long sea voyage, except falling out of love. especially was this the case in the days when the wooden clippers did finely to land you in sydney or in melbourne under the four full months. we all saw far too much of each other, unless, indeed, we were to see still more. our superficial attractions mutually exhausted, we lost heart and patience in the disappointing strata which lie between the surface and the bedrock of most natures. my own experience was confined to the round voyage of the lady jermyn, in the year . it was no common experience, as was only too well known at the time. and i may add that i for my part had not the faintest intention of falling in love on board nay, after all these years, let me confess that i had good because to hold myself proof against such weakness.\n```\n\n## 8. 정제된 텍스트 토큰화\n\n<div class=\"content-ad\"></div>\n\n```js\n# 토크나이저 생성\ntokenizer = RegexpTokenizer('\\w+')\n\n# 토큰 생성\ntokens = tokenizer.tokenize(text_c)\ntokens[:8]\n```\n\n이 코드는 다음 단계를 수행합니다:\n\n토크나이저 생성:\n\n- 알파벳 및 숫자(문자와 숫자) 시퀀스를 캡처하는 정규 표현식 \\w+를 사용하여 단어와 일치하는 RegexpTokenizer를 초기화합니다.\n\n<div class=\"content-ad\"></div>\n\n토크나이저를 사용하여 정제된 텍스트(text_c)를 토큰(개별 단어)으로 분할합니다.\n\n첫 번째 몇 개의 토큰을 표시합니다:\n\n토큰화된 출력물을 간략히 보기 위해 처음 여덟 개의 토큰을 표시합니다.\n\n<div class=\"content-ad\"></div>\n\n이 과정은 텍스트를 단어 목록으로 변환하여 추가 텍스트 분석 및 처리에 사용할 수 있습니다.\n\n```js\n#출력:\n['dead', 'men', 'tell', 'no', 'tales', 'by', 'e', 'w']\n```\n\n## 9. 토큰에서 불용어 제거\n\n```js\nsw = set(stopwords.words('english'))\n\n# 추가 불용어 추가\nadditional_stopwords = {'could', 'said', 'must', 'would', 'should', 'might', 'gutenberg','project'}\nsw.update(additional_stopwords)\n# 새로운 목록 초기화\nwords_ns = []\n\n# words_ns에 tokens에 있는 단어 중 sw에 없는 모든 단어 추가\nfor word in tokens:\n    if word not in sw:\n        words_ns.append(word)\n\n# 상식적인 확인을 위해 몇 가지 항목 출력\nwords_ns[:5]\n```\n\n<div class=\"content-ad\"></div>\n\nStopwords 불러오기:\n\n- NLTK의 영어 말뭉치에서 stopwords.words('english')를 사용하여 stopwords를 가져옵니다. Stopwords란 \"the\", \"and\", \"is\" 등과 같은 일반적이고 종종 텍스트의 의미에 크게 기여하지 않는 단어를 말합니다.\n\n추가적인 Stopwords 추가하기:\n\n- 컨텍스트에 특정한 추가적인 stopwords를 포함하는 additional_stopwords 집합을 정의합니다. 이러한 단어들은 update()를 사용하여 sw 집합에 추가됩니다.\n\n<div class=\"content-ad\"></div>\n\n비어 있는 리스트 초기화:\n\n- 비어 있는 단어 목록인 words_ns를 초기화합니다.\n\n불용어 제거:\n\n- 토큰 목록에서 각 단어를 반복합니다.\n- 단어가 불용어 세트(sw set)에 없는지 확인합니다(즉, 불용어가 아닌지 확인합니다).\n- 만약 단어가 불용어가 아니라면, words_ns 목록에 추가됩니다.\n\n<div class=\"content-ad\"></div>\n\n정신을 차리세요:\n\n- words_ns 목록에서 처음 다섯 항목을 인쇄하여 불용어가 성공적으로 제거되었는지 확인합니다.\n\n이 과정을 통해 words_ns 목록에 원본 토큰화된 텍스트에서 일반적인 불용어 및 추가 지정된 단어를 제외하고 의미 있는 단어만 포함되도록 확인합니다.\n\n```js\n#출력\n['dead', 'men', 'tell', 'tales', 'e']\n```\n\n<div class=\"content-ad\"></div>\n\n## 10. 단어 빈도 시각화\n\n```js\n# 그림을 인라인으로 설정하고 시각화 스타일을 설정합니다\n%matplotlib inline\nsns.set()\n\n# 빈도 분포를 생성하고 플롯합니다\nfreqdist1 = nltk.FreqDist(words_ns)\nfreqdist1.plot(20)\n```\n\n인라인 플로팅 활성화:\n\n- %matplotlib inline은 주피터 노트북에서 사용되는 매직 커맨드로, 플롯이 노트북 내에서 인라인으로 나타날 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\n표 태그를 마크다운 형식으로 바꿔주세요.\n\n| 설정 시각화 스타일:\n- sns.set()은 plot에 대한 Seaborn의 기본 미학 매개변수를 설정합니다.\n\n| 빈도 분포 생성:\n- nltk.FreqDist(words_ns)는 words_ns 리스트의 단어들을 빈도 분포로 생성합니다. 각 단어의 빈도수가 계산됩니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 Frequency Distribution을 시각화합니다:\n\n- freqdist1.plot(20)은 frequency distribution인 freqdist1에서 가장 빈도가 높은 상위 20개의 단어를 시각화합니다.\n\n이 시각화는 정제 및 처리된 텍스트에서 가장 빈번하게 발생하는 단어를 이해하는 데 도움이 되며, 문서의 내용과 초점에 대한 통찰을 제공합니다.\n\n![Frequency Distribution Plot](/assets/img/2024-06-23-BuildingDataPipelineswithWebScrapingAStep-by-StepGuide_0.png)\n\n<div class=\"content-ad\"></div>\n\n만약 추가적인 불용어를 추가하지 않는다면 어떻게 되는지 예시가 아래에 나와 있어요.\n\n![이미지](/assets/img/2024-06-23-BuildingDataPipelineswithWebScrapingAStep-by-StepGuide_1.png)\n\n여기가 웹사이트가 가르치는 내용을 마치는 곳이에요. 이제 우리는 다른 URL로 시도해보고 다른 책들을 스크래핑하여 또 다른 빈도 분포를 생성해 볼 수 있어요.\n\n## 11. 바이그램(bigrams)과 트라이그램(trigrams) 추출\n\n<div class=\"content-ad\"></div>\n\n빅램과 트리그램은 주어진 텍스트나 음성 샘플에서 n개의 항목으로 이루어진 연속적인 시퀀스인 n-그램의 한 유형입니다. 자연어 처리(NLP)에서 이러한 항목은 일반적으로 단어입니다.\n\n빅램:\n\n- 빅램은 두 연이은 단어의 시퀀스입니다.\n- 예를 들어, “I love programming”이라는 문장에서 빅램은 다음과 같습니다:\n- “I love”\n- “love programming”\n\n트리그램:\n\n<div class=\"content-ad\"></div>\n\n- Trigrams은 연이은 세 개의 단어 시퀀스입니다.\n- 예를 들어, \"I love programming\"이라는 문장에서 trigrams는 다음과 같습니다:\n- \"I love programming\"\n\n## NLP에서의 사용:\n\n- Bigrams와 trigrams은 다음과 같은 다양한 NLP 작업에 사용됩니다:\n- 텍스트 분석: 단어 사이의 문맥과 관계를 이해하는 데 사용됩니다.\n- 언어 모델링: 시퀀스에서 다음 단어를 예측하는 데 사용됩니다.\n- 정보 검색: 단어 쌍이나 세 번씩 고려함으로써 검색 알고리즘을 개선하는 데 사용됩니다.\n- 감성 분석: 문구의 감정을 이해하는 데 도움이 되는 단일 단어보다 문맥을 더 잘 포착합니다.\n\n```js\nbigrams = list(ngrams(words_ns, 2))\ntrigrams = list(ngrams(words_ns, 3))\n\n# 빈도 분석\nbigram_freq = Counter(bigrams)\ntrigram_freq = Counter(trigrams)\n\n# 상위 10개의 bigram과 trigram 표시\nprint(\"상위 10개 Bigrams:\")\nfor bigram, freq in bigram_freq.most_common(10):\n    print(bigram, freq)\n\nprint(\"\\n상위 10개 Trigrams:\")\nfor trigram, freq in trigram_freq.most_common(10):\n    print(trigram, freq)\n```\n\n<div class=\"content-ad\"></div>\n\nBigrams과 Trigrams을 사용하면 개별 단어(일그램)를 사용하는 것보다 텍스트에서 더 많은 맥락과 의미를 포착할 수 있어요.\n\n```js\n#결과\n\n상위 10개의 Bigrams:\n('lady', 'jermyn') 33\n('mr', 'cole') 23\n('eva', 'denison') 19\n('miss', 'denison') 17\n('electronic', 'works') 16\n('united', 'states') 15\n('captain', 'harris') 14\n('literary', 'archive') 13\n('archive', 'foundation') 13\n('electronic', 'work') 11\n\n상위 10개의 Trigrams:\n('literary', 'archive', 'foundation') 13\n('dead', 'men', 'tell') 6\n('men', 'tell', 'tales') 6\n('never', 'shall', 'forget') 5\n('protected', 'copyright', 'law') 4\n('e', 'w', 'hornung') 3\n('ebook', 'dead', 'men') 3\n('located', 'united', 'states') 3\n('united', 'states', 'check') 3\n('states', 'check', 'laws') 3\n```\n\n## 12. Trigrams와 Bigrams 그래프 작성:\n\n```js\n# ngram 빈도수를 그리는 함수\ndef plot_ngrams(ngram_freq, title, xlabel, ylabel):\n    ngrams, freqs = zip(*ngram_freq.most_common(10))\n    ngrams = [' '.join(ngram) for ngram in ngrams]\n    \n    # 리스트로 변환\n    ngrams = list(ngrams)\n    freqs = list(freqs)\n    \n    # 디버깅: 타입 및 내용 출력\n    print(\"freqs의 타입:\", type(freqs))\n    print(\"ngrams의 타입:\", type(ngrams))\n    print(\"freqs의 내용:\", freqs)\n    print(\"ngrams의 내용:\", ngrams)\n    \n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=freqs, y=ngrams)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.show()\n\n\n# 상위 10개의 Bigrams 그래프로 나타내기\nplot_ngrams(bigram_freq, '상위 10개의 Bigrams', '빈도수', 'Bigrams')\n\n# 상위 10개의 Trigrams 그래프로 나타내기\nplot_ngrams(trigram_freq, '상위 10개의 Trigrams', '빈도수', 'Trigrams')\n```\n\n<div class=\"content-ad\"></div>\n\n## 13. POS 태그 생성\n\n```js\ndef preprocess_text(text): \n    sentences = sent_tokenize(text) \n    sentences = [nltk.pos_tag(word_tokenize(sent)) for sent in sentences]\n    \n    return sentences\n\nsent_text = preprocess_text(text)\n\n# 디버깅: 토큰화 및 POS 태그가 지정된 문장 출력\nprint(\"토큰화 및 POS 태그가 지정된 문장:\")\nfor sent in sent_text:\n    print(sent)\n```\n\n- nltk.pos_tag는 문장의 각 단어에 대해 품사를 태깅합니다.\n- 이 함수는 각 문장이 (단어, POS 태그) 튜플을 포함하는 리스트인 문장의 리스트를 반환합니다.\n\n텍스트 입력에 대한 출력은 각 문장을 (단어, POS 태그) 튜플의 리스트로 나타낸 문장의 리스트가 됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n#출력\n\n토큰화 및 품사 태깅된 문장:\n[('Dead', 'JJ'), ('Men', 'NNP'), ('Tell', 'NNP'), ('No', 'NNP'), ('Tales', 'NNP'), (',', ','), ('by', 'IN'), ('E.', 'NNP'), ('W.', 'NNP'), ('Hornung', 'NNP'), ('The', 'DT'), ('Project', 'NNP'), ('Gutenberg', 'NNP'), ('eBook', 'NN'), ('of', 'IN'), ('Dead', 'JJ'), ('Men', 'NNP'), ('Tell', 'NNP'), ('No', 'NNP'), ('Tales', 'NNP'), ('This', 'DT'), ('ebook', 'NN'), ('is', 'VBZ'), ('for', 'IN'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('anyone', 'NN'), ('anywhere', 'RB'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('and', 'CC'), ('most', 'JJS'), ('other', 'JJ'), ('parts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('world', 'NN'), ('at', 'IN'), ('no', 'DT'), ('cost', 'NN'), ('and', 'CC'), ('with', 'IN'), ('almost', 'RB'), ('no', 'DT'), ('restrictions', 'NNS'), ('whatsoever', 'RB'), ('.', '.')]\n[('You', 'PRP'), ('may', 'MD'), ('copy', 'VB'), ('it', 'PRP'), (',', ','), ('give', 'VB'), ('it', 'PRP'), ('away', 'RB'), ('or', 'CC'), ('re-use', 'VB'), ('it', 'PRP'), ('under', 'IN'), ('the', 'DT'), ('terms', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Project', 'NNP'), ('Gutenberg', 'NNP'), ('License', 'NNP'), ('included', 'VBD'), ('with', 'IN'), ('this', 'DT'), ('ebook', 'NN'), ('or', 'CC'), ('online', 'NN'), ('at', 'IN'), ('www.gutenberg.org', 'NN'), ('.', '.')]\n[('If', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('not', 'RB'), ('located', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), (',', ','), ('you', 'PRP'), ('will', 'MD'), ('have', 'VB'), ('to', 'TO'), ('check', 'VB'), ('the', 'DT'), ('laws', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('country', 'NN'), ('where', 'WRB'), ('you', 'PRP'), ('are', 'VBP'), ('located', 'VBN'), ('before', 'IN'), ('using', 'VBG'), ('this', 'DT'), ('eBook', 'NN'), ('.', '.')]\r\n```\n\n이 전처리 단계는 명명된 엔터티 인식, 구문 분석 및 의미 분석과 같은 NLP 작업에 유용합니다.\n\n## 14. 명명된 엔터티 추출\n\n```js\r\n# NER 수행\ndef extract_named_entities(sentences):\n    named_entities = []\n    for sent in sentences:\n        # nltk의 ne_chunk를 사용하여 NER 수행\n        tree = nltk.ne_chunk(sent, binary=False)\n        print(tree)\n        for subtree in tree:\n            if hasattr(subtree, 'label'):\n                entity_name = ' '.join([child[0] for child in subtree.leaves()])\n                entity_type = subtree.label()\n                named_entities.append((entity_name, entity_type))\n    return named_entities\n\nnamed_entities = extract_named_entities(sent_text)\n\n# 추출된 명명된 엔터티 출력\nfor entity in named_entities:\n    print(entity)\r\n```\n\n<div class=\"content-ad\"></div>\n\n이 코드 블록은 NLTK를 사용하여 토큰화 및 POS 태깅된 문장에 대해 Named Entity Recognition(NER)을 수행하고 명명된 엔티티를 추출하여 출력하는 함수를 정의합니다.\n\n여기서는 각 문장에 대해 NER을 수행하기 위해 NLTK의 ne_chunk 함수를 사용합니다. binary=False 매개변수는 엔티티가 특정 유형(예: PERSON, ORGANIZATION 등)으로 분류되어야 함을 지정합니다.\n\n```js\n#OUTPUT\n(S\n  Dead/JJ\n  Men/NNP\n  (ORGANIZATION Tell/NNP No/NNP Tales/NNP)\n  ,/,\n  by/IN\n  E./NNP\n  W./NNP\n  Hornung/NNP\n  The/DT\n  (ORGANIZATION Project/NNP Gutenberg/NNP)\n  eBook/NN\n  of/IN\n  (ORGANIZATION Dead/JJ Men/NNP Tell/NNP No/NNP Tales/NNP)\n  This/DT\n  ebook/NN\n  is/VBZ\n  for/IN\n  the/DT\n  use/NN\n  of/IN\n  anyone/NN\n  anywhere/RB\n  in/IN\n  the/DT\n  (GPE United/NNP States/NNPS)\n  and/CC\n  most/JJS\n  other/JJ\n  parts/NNS\n  of/IN\n  the/DT\n  world/NN\n  at/IN\n  no/DT\n  cost/NN\n  and/CC\n  with/IN\n  almost/RB\n  no/DT\n  restrictions/NNS\n  whatsoever/RB\n  ./.)\n(S\n  You/PRP\n  may/MD\n  copy/VB\n  it/PRP\n  ,/,\n  give/VB\n  it/PRP\n  away/RB\n  or/CC\n  re-use/VB\n  it/PRP\n  under/IN\n  the/DT\n  terms/NNS\n  of/IN\n  the/DT\n  (ORGANIZATION Project/NNP Gutenberg/NNP License/NNP)\n  included/VBD\n  with/IN\n  this/DT\n  ebook/NN\n  or/CC\n  online/NN\n  at/IN\n  www.gutenberg.org/NN\n  ./.)\nand so on...\n```\n\n이 과정은 텍스트 내의 인물, 조직 및 위치와 같은 명명된 엔티티를 식별하고 분류하여 다양한 NLP 애플리케이션에 유용한 정보를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nprint(named_entities)\n```\n\n```js\n#OUTPUT\n[('Tell No Tales', 'ORGANIZATION'),\n ('Project Gutenberg', 'ORGANIZATION'),\n ('Dead Men Tell No Tales', 'ORGANIZATION'),\n ('United States', 'GPE'),\n ('Project Gutenberg License', 'ORGANIZATION'),\n ('United States', 'GPE'),\n ('Title', 'GPE'),\n ('Tell No Tales', 'ORGANIZATION'),\n ('Hornung Release', 'PERSON'),\n ('David Widger', 'PERSON'),\n ('THE', 'ORGANIZATION'),\n ('PROJECT', 'ORGANIZATION'),\n ('TELL', 'ORGANIZATION'),\n ('TELL', 'ORGANIZATION'),\n ('Hornung', 'PERSON'),\n ('CONTENTS', 'ORGANIZATION'),\n ('LOVE', 'ORGANIZATION'),\n ('OCEAN', 'ORGANIZATION'),\n ('MYSTERIOUS', 'ORGANIZATION'),\n ('THE', 'ORGANIZATION'),\n ('WATER', 'ORGANIZATION'),\n ('EDGE', 'ORGANIZATION'),\n ('SILENT', 'ORGANIZATION'),\n ('REWARD', 'ORGANIZATION'),\n ('SOLE', 'ORGANIZATION'),\n ('FRIEND', 'ORGANIZATION'),\n ('SMALL', 'ORGANIZATION'),\n ('CONVALESCENT', 'ORGANIZATION'),\n ('WINE', 'ORGANIZATION'),\n ('AND', 'ORGANIZATION'),\n ('WEAKNESS', 'ORGANIZATION'),\n ('AGAIN', 'ORGANIZATION'),\n ('BIDDING', 'ORGANIZATION'),\n.......\n```\n\n## NER entities plotting:\n\n```js\n# Frequency analysis\nentity_freq = Counter([entity_type for entity_name, entity_type in named_entities])\n\n# Display top 10 named entities\nprint(\"Top 10 Named Entities:\")\nfor entity, freq in entity_freq.most_common(10):\n    print(entity, freq)\n\n#OUTPUT\nTop 10 Named Entities:\nPERSON 505\nORGANIZATION 257\nGPE 254\n```\n\n<div class=\"content-ad\"></div>\n\n```python\n# 엔티티 빈도수를 그리는 함수\ndef plot_entity_frequency(entity_freq, title, xlabel, ylabel):\n    entities, freqs = zip(*entity_freq.most_common(10))\n    # seaborn과 호환성을 보장하기 위해 리스트로 변환\n    entities = list(entities)\n    freqs = list(freqs)\n    \n    # 디버깅: 타입과 내용 출력\n    print(\"freqs의 타입:\", type(freqs))\n    print(\"entities의 타입:\", type(entities))\n    print(\"freqs의 내용:\", freqs)\n    print(\"entities의 내용:\", entities)\n    \n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=freqs, y=entities)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.show()\n\n# 상위 10개의 명명된 엔티티 그래프로 나타내기\nplot_entity_frequency(entity_freq, '상위 10개 명명된 엔티티', '빈도수', '엔티티')\n```\n\n![그림](/assets/img/2024-06-23-BuildingDataPipelineswithWebScrapingAStep-by-StepGuide_2.png)\n\n## 15. spaCy를 이용한 명명된 엔티티 인식 개선하기\n\nNLTK 기반의 명명된 엔티티 인식 (NER) 방법은 일반적으로 잘 작동하지만 가끔 일반 단어를 엔티티로 잘못 분류할 수 있습니다. 예를 들어, \"Contents,\" \"Love,\" \"Ocean,\" 그리고 \"Friend\"와 같은 단어들이 기관으로 잘못 식별되었는데, 이는 분명히 정확하지 않습니다.\n\n\n<div class=\"content-ad\"></div>\n\n더 정확한 결과를 얻기 위해 spaCy 라이브러리를 사용하도록 변경하겠습니다. spaCy는 강력하고 현대적인 NLP 라이브러리로, NER을 더 정확하게 처리하기 위해 설계되었습니다. SpaCy의 미리 훈련된 모델은 텍스트의 명명된 엔티티를 인식하는 데 매우 효과적으로 작용하여 이러한 잘못된 분류를 최소화합니다.\n\n```python\n# spaCy 모델을 불러오기\nnlp = spacy.load(\"en_core_web_sm\")\n\n# spaCy를 사용하여 텍스트 처리\ndoc = nlp(text_c)\n\n# 명명된 엔티티 추출 및 출력\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n```\n\nspaCy의 출력:\n\n```python\ne. w. hornung PERSON\nthe united states GPE\nthe united states GPE\ne. w. hornung PERSON\njune DATE\nenglish NORP\ndavid PERSON\ne. w. hornung PERSON\nchapter i. love ORG\nchapter vii LAW\nchapter x. LAW\nthe longest day DATE\nfirst ORDINAL\nfrancis rattray chapter i. love ORG\nthe days DATE\nthe four full months DATE\nthe year DATE\neva denison PERSON\nmore than nineteen years of age DATE\nfirst ORDINAL\nher years DATE\ndenison PERSON\ntwo CARDINAL\njanuary DATE\nthe beginning of the following july DATE\nthe most odious weeks DATE\nblack hill LOC\nas much as four CARDINAL\nhalf CARDINAL\nlondon GPE\nfirst ORDINAL\nfive CARDINAL\nfive pounds QUANTITY\naustralia GPE\none CARDINAL\nonly five CARDINAL\nany minute of the day TIME\nthe hour TIME\ndenison PERSON\none CARDINAL\nsixty CARDINAL\njoaquin santos PERSON\nfirst ORDINAL\ndenison PERSON\na few months later DATE\ndenison PERSON\nengland GPE\nafrica LOC\nsecond ORDINAL\neva denison PERSON\n```\n\n<div class=\"content-ad\"></div>\n\n## Plotting Spacy 파생 엔티티\n\n```js\n# 엔티티 타입 카운트\nentity_types = [ent.label_ for ent in doc.ents]\nentity_type_freq = Counter(entity_types)\n\n# 시본을 사용한 플로팅\nplt.figure(figsize=(8, 5))\nsns.barplot(x=list(entity_type_freq.keys()), y=list(entity_type_freq.values()))\nplt.title('엔티티 타입 및 빈도')\nplt.xlabel('엔티티 타입')\nplt.ylabel('빈도')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-23-BuildingDataPipelineswithWebScrapingAStep-by-StepGuide_3.png\" />\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n이 연습에서는 온라인 소스에서 텍스트 데이터를 추출하고 분석하는 과정을 탐색했습니다. 우리는 먼저 웹페이지의 HTML 내용을 가져와 BeautifulSoup로 구문 분석하고 추출된 텍스트를 정리하여 원치 않는 문자와 불용어를 제거했습니다. 그런 다음 텍스트를 토큰화하고 NLTK를 사용하여 초기 Named Entity Recognition(NER)을 수행했습니다.\n\nNLTK 기반의 NER은 텍스트에 존재하는 엔티티에 대한 기본적인 이해를 제공했지만 공통 단어를 명명된 엔티티로 오분류하는 등의 한계가 있었습니다. 이러한 부정확성에 대응하기 위해 보다 고급이고 정확한 NER 접근 방식을 제공하는 spaCy 라이브러리를 소개했습니다.\n\nspaCy를 사용하여 우리는 더 나은 결과를 얻을 수 있었고, 자연어 처리 작업에 적합한 올바른 도구를 선택하는 중요성을 입증했습니다. spaCy를 사용하여 명명된 엔티티를 인식하는 향상된 정밀도는 텍스트 데이터로부터 신뢰할 수 있는 통찰을 제공하는 가치를 강조합니다.\n\n이 단계를 따라가면 큰 양의 텍스트를 효율적으로 처리하고 분석하여 의미 있는 정보를 추출하고 내용에 대한 깊은 통찰을 얻을 수 있습니다. 이 접근 방식은 데이터 과학, 연구 및 비즈니스 인텔리전스의 다양한 응용 분야에 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n# 참고 자료\n\n- Web Scraping & NLP in Python | DataCamp\n- What Is Web Scraping? [A Complete Step-by-Step Guide] (careerfoundry.com)\n\n일부 설명은 chatGPT의 도움을 받아 작성되었습니다.","ogImage":{"url":"/assets/img/2024-06-23-BuildingDataPipelineswithWebScrapingAStep-by-StepGuide_0.png"},"coverImage":"/assets/img/2024-06-23-BuildingDataPipelineswithWebScrapingAStep-by-StepGuide_0.png","tag":["Tech"],"readingTime":27},{"title":"OpenAI의 CLIP 모델 이해하기 2024 최신 분석 및 기능 소개","description":"","date":"2024-06-23 20:08","slug":"2024-06-23-UnderstandingOpenAIsCLIPmodel","content":"\n\nCLIP은 2021년 OpenAI에 의해 출시되어 그 이후로 많은 다중 모달 AI 시스템의 핵심 구성 요소 중 하나가 되었습니다. 이 기사는 CLIP에 대한 심층적인 내용을 다룹니다. CLIP이 무엇이며, 어떻게 작동하는지, 어떻게 사용되는지, 그리고 어떻게 구현되는지에 대해 소개합니다.\n\n# 소개\n\nCLIP은 Contrastive Language-Image Pre-training의 약자로, 자연어 감독에서 학습하기 위한 효율적인 방법으로 2021년에 소개되었습니다. 이 방법은 Learning Transferable Visual Models From Natural Language Supervision 논문에서 소개되었습니다.\n\n요약하면, CLIP은 4억 개의 이미지와 텍스트 쌍을 이용하여 자가 감독 방식으로 훈련된 공통 이미지 및 텍스트 임베딩 모델입니다. 이는 텍스트와 이미지를 동일한 임베딩 공간에 매핑한다는 것을 의미합니다. 예를 들어, 개의 이미지와 \"개의 이미지\"라는 문장은 매우 유사한 임베딩을 갖게 되고 벡터 공간에서 서로 가까이 위치하게 됩니다. 이는 이미지 데이터베이스를 설명으로 검색하거나 그 반대의 응용 프로그램을 구축할 수 있다는 점에서 매우 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n저자들은 CLIP이 훈련되지 않은 다양한 작업에 사용될 수 있음을 발견했습니다. 예를 들어, CLIP은 ImageNet과 같은 이미지 분류 데이터셋에서 놀라운 제로샷 성능을 거뒀다. 제로샷 러닝은 모델이 ImageNet 데이터셋의 1.28백만 개의 훈련 예시 중 어느 것도 명시적으로 훈련받지 않았다는 사실을 가리킵니다. 그럼에도 불구하고, CLIP은 이미지와 텍스트가 동일한 임베딩 공간에 있고 도트 곱이 임베딩 간 유사성을 계산하기 때문에 \"강아지의 사진\"과의 도트 곱이 가장 높을 가능성이 큽니다. 따라서 이미지를 강아지로 예측할 수 있습니다. CLIP을 진정한 분류기로 바꾸고 싶다면 도트 곱을 소프트맥스 함수를 통해 각 클래스에 대한 예측 확률을 얻을 수 있습니다.\n\n위 과정은 다음 그림의 2단계와 3단계에서 확인할 수 있습니다.\n\n![CLIP 모델 이해](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_0.png)\n\n<div class=\"content-ad\"></div>\n\n이제 CLIP가 어떻게 작동하는지 자세히 살펴보겠습니다.\n\n# 모델 세부정보\n\n## 아키텍처\n\nCLIP 모델에는 텍스트 인코더(텍스트를 임베드하는 부분)와 이미지 인코더(이미지를 임베드하는 부분) 두 가지 주요 구성 요소가 있습니다. 텍스트 인코더에는 Transformer가 사용되었습니다. 이 아키텍처는 2017년 이후부터 NLP 분야를 혁신시켜 왔으며 당연히 사용되었다고 할 수 있습니다. 시각적인 설명이 필요하시면 아래 블로그를 참조하세요.\n\n<div class=\"content-ad\"></div>\n\n이미지 인코더에 대해 작성자는 두 가지 다른 모델을 시도했습니다. ResNet-50 및 Vision Transformer (ViT)입니다. ResNet-50은 이미지 분류에 사용되는 컨볼루션 신경망 (CNN)을 사용한 원래의 최신 아키텍처이며, ViT는 이미지를 위한 원본 Transformer의 최근적인 적응으로 각 이미지를 패치 시퀀스로 분할하고 토큰 시퀀스로 유사하게 모델에 전달합니다. 작성자는 ViT가 더 빠르게 훈련되었음을 발견했습니다.\n\n텍스트 및 이미지 인코더 모두 처음부터 훈련되었습니다.\n\n모든 아키텍처에 대해 논문에서 설명한 대로 소량의 수정이 가해졌습니다.\n\n## 훈련\n\n<div class=\"content-ad\"></div>\n\n저자들은 초기에 이미지 캡션 모델을 훈련시켜보려고 했는데, 이미지를 주면 정확한 캡션/설명을 예측하는 모델이었습니다.\n\n그러나 4억(이미지, 텍스트) 쌍을 훈련시키기에는 규모가 맞지 않다고 판단하여, 비교 표현 학습 접근 방식으로 바꾸기로 결정했습니다. 비교적 표현 학습의 목표는 비슷한 샘플 쌍이 서로 가깝게 유지되고 다른 샘플 쌍이 멀리 떨어지도록 하는 임베딩 공간을 학습하는 것입니다.\n\n표준 비교 학습 접근 방식에서는 모델에 (앵커, 양성, 음성) 형식의 예제를 제공합니다. 여기서 앵커는 한 클래스(예: 개)의 이미지이고, 양성은 같은 클래스(개)의 다른 이미지, 음성은 다른 클래스(예: 새)의 이미지입니다. 그런 다음, 이미지를 임베딩하고 같은 클래스(개)에 대한 두 임베딩 사이의 거리(distance(anchor, positive))를 최소화하고, 서로 다른 클래스(개와 새)에 대한 두 임베딩 사이의 거리(distance(anchor, negative))를 최대화하도록 모델을 훈련합니다. 이는 모델이 동일한 객체에 대해 매우 유사한 임베딩을 출력하고, 서로 다른 객체에 대해 서로 다른 임베딩을 출력하도록 격려합니다.\n\n<div class=\"content-ad\"></div>\n\n동일한 방식은 텍스트와 텍스트와 이미지의 조합에도 적용할 수 있습니다. 예를 들어, CLIP의 경우 단일 훈련 예제에 대해 앵커는 개의 이미지일 수 있으며, 양성은 \"개의 이미지\"라는 캡션일 수 있으며, 부정은 \"새의 이미지\"라는 캡션일 수 있습니다.\n\nCLIP는 다중 클래스 N-pair loss를 사용하여 이를 더 확장하며, 이는 앵커마다 여러 개의 부정과 양성이 있을 때의 상황입니다. 논문에 설명된 바에 따르면:\n\n논문에서 제공된 아래 의사 코드는 핵심 세부 정보를 잘 캡슐화합니다:\n\n![이미지](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_2.png)\n\n<div class=\"content-ad\"></div>\n\n다음은 순서입니다:\n\n- 이미지 인코더로 이미지를 포함하고, 텍스트 인코더로 텍스트를 포함합니다.\n- 이미지 및 텍스트 임베딩은 서로 다른 모델에서 나오며 차원이 다르기 때문에, 학습된 프로젝션 매트릭스와의 곱셈을 통해 동일한 합성 다중 모달 임베딩 공간으로 변환합니다. 예를 들어, np.dot(I_f, W_i)는 크기가 [n, d_i]인 행렬을 크기가 [d_i, d_e]인 행렬과 곱하여 크기가 [n, d_e]인 프로젝트된 행렬을 생성합니다.\n- 새로운 임베딩 벡터를 정규화합니다. 이렇게 하면 단위 벡터로 변환됩니다.\n- 도트 곱의 행렬을 계산합니다.\n- 각 행 및 열에 대한 교차 엔트로피 손실을 계산하고, 각 쌍이 두 번씩 계산되므로 2로 나눕니다.\n\n## 프롬프트 엔지니어링 및 앙상블링\n\n언어 모델의 부상 이후 프롬프트 엔지니어링은 생성 모델에서 좋은 출력을 얻기 위한 매우 흔한 실천법이 되었습니다. CLIP의 텍스트 인코더가 트랜스포머 모델인 만큼 제로샷 성능을 얻기 위해서 매우 중요하다는 점을 저자들이 발견했습니다. 저자들은 사진과 텍스트가 단어 하나로만 이루어진 경우인 경우가 그리 흔하지 않았다는 것을 발견했습니다. 예를 들어, \"개\"와 같이 클래스 레이블을 나타내는 경우입니다. 대신, 이미지와 함께 매칭되는 텍스트가 이미지의 캡션 또는 설명과 같이 전체 문장인 경우가 더 흔했습니다. 따라서 저자들은 \" 'object'의 사진\"이 좋은 기본 프롬프트이지만 특정 경우에는 더 전문화된 프롬프트가 더 잘 작동한다고 발견했습니다. 예를 들어, 위성 이미지의 경우 \" 'object'의 위성 사진\"이 잘 작동했습니다.\n\n<div class=\"content-ad\"></div>\n\n작가들은 다른 모델들을 앙상블하는 실험도 진행했습니다. 앙상블은 여러 다른 모델들의 예측을 결합하여 최종 출력을 얻는 것으로, 머신러닝에서 고분산 및 저편향(과적합) 모델의 문제를 해결하는 일반적인 기술입니다. CLIP의 경우, 작가들은 여러 다양한 프롬프트를 사용하여 분류기를 구성하여 앙상블을 만들었습니다.\n\n프롬프트 엔지니어링과 앙상블 모두 ImageNet에서 상당한 성능 향상을 보여주었습니다.\n\n## 한계\n\n논문은 더 많은 실험과 결과에 대해 논하고 있지만, CLIP가 완벽하지 않으며 다양한 한계가 있다는 점을 언급하는 것도 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n- 앞서 언급한 설계 결정으로 이 모델은 생성 모델이 아니며 이미지 캡셔닝을 할 수 없습니다.\n- 저자는 CLIP가 여전히 최신 기술 수준에서는 멀리 떨어져 있음을 언급하며 (위에 선형 레이어가 있는 ResNet과만 비교 가능함) 일부 작업에 대해 매우 안좋은 일반화 성능을 보입니다. 예를 들어, 쉬운 MNIST 손글씨 숫자 인식 데이터셋에서는 88% 정도만 달성합니다. Training 데이터에 유사한 이미지가 없기 때문일 가능성이 높지만, CLIP는 그런 측면에 대해 거의 다루지 않습니다.\n- CLIP는 인터넷에 있는 이미지와 텍스트의 쌍으로 훈련되었습니다. 이러한 이미지-텍스트 쌍들은 필터링되지 않고 정리되지 않았으며, CLIP 모델은 여러 사회적 편향을 학습하게 됩니다. (현재 LLM들과 유사한 우려사항이 있으며, RLFHF와 직접 선호도 최적화와 같은 기술이 이를 해결하려고 노력하고 있습니다.)\n- 트랜스포머 텍스트 인코더의 최대 시퀀스 길이(전달할 수 있는 토큰의 최대 수)는 원본 구현에서 76으로 제한되었는데, 이는 대부분 이미지와 일반적으로 짧은 문장인 캡션으로 이루어진 데이터셋 때문입니다. 따라서 오프더셸프 사전 훈련 모델은 긴 텍스트와 잘 작동하지 않을 것이며, 76개의 토큰 이후로 잘릴 것이므로, 짧은 텍스트로 훈련되었습니다. \n\n# 구현 세부 사항\n\n## HuggingFace Transformers를 사용한 추론\n\nHuggingFace Transformers 라이브러리를 사용하여 몇 줄의 코드로 자신의 컴퓨터에서 CLIP를 사용할 수 있습니다! 먼저 라이브러리를 가져와서 사전 훈련된 모델을 로드하세요.\n\n<div class=\"content-ad\"></div>\n\n```js\nimport transformers\n\nmodel = transformers.CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = transformers.CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n```\n\n그런 다음 캡션/설명 목록과 이미지 목록을 만듭니다. 이미지는 url 또는 PIL 이미지일 수 있습니다.\n\n```js\nimport PIL.Image\n\nimages = [PIL.Image(\"for_example_a_dog_image.jpeg\")]\npossible_classes = [\"새 이미지\", \"개 이미지\", \"고양이 이미지\"]\n```\n\n텍스트와 이미지을 토큰화하고 모델로 전달할 준비를 단계를 수행하는 processor를 호출합니다. 이는 일반적인 텍스트만 사용하는 경우에 토큰화 도구를 호출하는 것과 매우 유사합니다. 설명의 배치가 있으므로 모두 동일한 길이로 \"패딩\"하여 텐서로 저장하고 최대 시퀀스 길이(이전에 설명한대로 76)에서 긴 문장을 자르기 위해 자른다. 그런 다음 토큰화된 입력을 모델로 전달하고 텍스트 및 이미지 인코더를 통해 전파시킵니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\r\ntorch.no_grad()을 사용하여:\n\n    inputs = processor(text=descriptions, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n    outputs = model(**inputs)\r\n```\n\n이제 두 가지 다른 함수를 사용하여 내적 결과 행렬을 검색할 수 있습니다. logits_per_image를 사용하면 [이미지 수, 텍스트 수] 형태의 내적 결과 행렬을 얻을 수 있고, logits_per_text를 사용하면 [텍스트 수, 이미지 수] 형태의 행렬을 얻을 수 있습니다.\n\n```js\r\ndot_products_per_image = outputs.logits_per_image\ndot_products_per_text = outputs.logits_per_text\r\n```\n\n마지막으로, 각 이미지에 대한 확률 분포를 얻고 싶다면 softmax 함수를 통과시킬 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\nprobabilities = dot_products_per_image.softmax(dim=1)\r\n```\r\n\r\n## 구현 내부의 깊은 이해\r\n\r\ntransformers CLIP의 소스 코드는 깃허브에서 찾을 수 있습니다. 모듈식으로 잘 구현되어 있어서 좋았어요. 주요 모델은 CLIPModel 클래스에 구현되어 있으며, 아래에서 볼 수 있는 forward 메서드에서 주요 로직을 확인할 수 있어요.\r\n\r\n<img src=\"/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_3.png\" />\r\n\n\n<div class=\"content-ad\"></div>\n\n이미지를 통해 알 수 있듯이 비전 모델과 텍스트 모델은 임베딩과 레이어 노름에 약간의 차이가 있습니다.\n\n![이미지1](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_4.png)\n\n![이미지2](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_5.png)\n  \n그러나 두 모델 모두 동일한 CLIPEncoder를 공유합니다. 이 CLIPEncoder는 주요 트랜스포머 인코더이며 많은 하위 블록으로 구성된 CLIPEncoderLayer라고 부릅니다. 트랜스포머 아키텍처에서는 각 인코더와 디코더가 N번 쌓입니다. CLIP에서는 텍스트를 생성하지 않기 때문에 디코더를 사용하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![UnderstandingOpenAIsCLIPmodel_6](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_6.png)\n\nEach `CLIPEncoderLayer` is then comprised of the attention mechanism, a normalization layer, and a simple feedforward layer/multi-layer perceptron (MLP).\n\n![UnderstandingOpenAIsCLIPmodel_7](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_7.png)\n\nFinally, I went through and annotated the implementation for the multi-head attention mechanism in the following gist - enjoy!\n\n\n<div class=\"content-ad\"></div>\n\n# 추가 작업\n\n시작부터 언급한대로 CLIP는 다양한 방법으로 활용할 수 있으며 특히 의미 검색 유형의 응용 프로그램에서 유용하게 사용할 수 있습니다. 예를 들어, 이미지의 설명으로 검색하여 데이터베이스에서 이미지를 검색하는 데 CLIP를 사용할 수 있습니다.\n\nCLIP 및 대안들은 이후에 등장한 많은 다중 모달 모델들의 구성 요소이기도 합니다. 예를 들어 Flamingo의 Vision Language Model에서는 텍스트 및 이미지 시퀀스를 한꺼번에 사용하여 텍스트를 생성할 수 있습니다. Flamingo는 이미지를 텍스트와 동일한 임베딩 공간으로 변환하기 위해 Vision 인코더를 사용합니다.\n\n![image](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_8.png)\n\n<div class=\"content-ad\"></div>\n\n작가들은 CLIP와 비슷한 방식으로 훈련된 자체 버전을 실험했습니다.\n\n마지막으로 Google의 Gemini와 같은 모델은 우리가 잘 알지 못해도, 오디오와 비디오를 포함한 다양한 모달리티의 입력 데이터를 결합하는 유사한 접근법을 사용하고 있을 가능성이 높습니다!\n\n![이미지](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_9.png)\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n요약하면, CLIP은 여러 응용 프로그램에 사용할 수 있는 텍스트와 임베딩 모델로, 다중 모달 AI 시스템을 구축하는 데 사용할 수 있습니다. 또한 Python에서 CPU 상에서 몇 줄의 코드로 쉽게 실행할 수도 있습니다.\n\n도움이 되었기를 바랍니다. 읽어 주셔서 감사합니다! 만약 즐겁게 보셨다면, 'Flamingo'에 관한 제 논문도 확인해보시는 것을 권해드립니다!","ogImage":{"url":"/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_0.png"},"coverImage":"/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_0.png","tag":["Tech"],"readingTime":9},{"title":"다양한 용도로 활용 가능한 GenAI 기반 챗봇 만들기 방법","description":"","date":"2024-06-23 20:05","slug":"2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot","content":"\n\n\n![이미지](/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_0.png)\n\n대형 언어 모델(LLM)은 엄청난 파워를 지니고 있으며, 질문 응답, 요약, entity 추출 등 다양한 NLP 작업을 해결하는 데 도움이 될 수 있습니다. 생성형 AI 사용 사례가 계속 확장됨에 따라 종종 현실 세계 응용프로그램에서는 이러한 NLP 작업 중 여러 가지를 해결할 수 있는 능력이 필요합니다. 예를 들어 사용자가 상호 작용할 수 있는 챗봇이 있다면, 대화 내용을 요약하는 것이 일반적인 요청이 될 수 있습니다. 이는 의사-환자 대화 기록, 가상 전화/예약 등 다양한 상황에서 활용될 수 있습니다.\n\n이러한 유형의 문제를 해결하는 방법은 무엇일까요? 질문 응답을 위한 하나, 요약을 위한 다른 하나의 LLM을 사용할 수 있습니다. 또 다른 접근 방식은 동일한 LLM을 다양한 도메인에 대해 파인 튜닝하는 것일 수 있지만, 이 경우엔 전자의 접근 방식에 초점을 맞출 것입니다. 그러나 여러 LLM을 사용하면 해결해야 할 특정한 도전 과제들이 있습니다.\n\n심지어 하나의 모델을 호스팅하는 것은 계산적으로 많은 비용이 소요되며, 대형 GPU 인스턴스가 필요합니다. 여러 LLM을 보유하면 모두에 대한 지속적인 엔드포인트/하드웨어가 필요할 것입니다. 이는 또한 여러 엔드포인트를 관리하고 인프라를 제공하기 위해 추가 비용이 발생한다는 것을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\nSageMaker 추론 컴포넌트를 사용하여이 문제를 해결할 수 있습니다. 추론 컴포넌트를 사용하면 단일 엔드포인트에 여러 가지 모델을 호스팅 할 수 있습니다. 각 모델은 고유 한 컨테이너를 가지고 있으며 모델별로 일정한 하드웨어를 할당하고 확장할 수 있습니다. 이를 통해 단일 엔드포인트 뒤에 두 개의 모델을 가지고 비용과 성능을 최적화 할 수 있습니다.\n\n오늘의 기사에서는 질문 응답과 요약이 가능한 일반적인 AI 기반 챗봇을 만드는 방법을 살펴보겠습니다. 여기서 사용할 도구 몇 가지를 간단히 살펴보겠습니다:\n\n- SageMaker 추론 컴포넌트: 모델을 호스팅하기 위해 SageMaker Real-Time 추론을 사용할 것입니다. Real-Time 추론 내에서 추론 컴포넌트 기능을 사용하여 여러 모델을 호스팅하고 각 모델에 대해 하드웨어를 할당할 것입니다. 추론 컴포넌트에 대해 처음이라면 이곳의 시작 기사를 참조해주세요.\n- Streamlit: Streamlit은 웹 개발을 간단하게하는 오픈 소스 Python 라이브러리입니다. Streamlit을 사용하여 질문 응답 및 요약을 위한 ChatBot UI를 구축할 것입니다. Streamlit을 처음 사용하는 경우 Heiko Hotz의 시작 기사를 참조하여 UI를 구축하는 데 템플릿으로 사용할 수 있습니다.\n- 모델\n- 질문 응답 모델: 챗봇의 질문 응답 부분에는 Llama7B Chat 모델을 사용할 것입니다. Llama7B Chat은 채팅 중심 대화에 최적화되어 있습니다. 사용할 모델 서버/컨테이너는 DJL Serving에서 제공되는 AWS Large Model 추론 (LMI) 컨테이너입니다. LMI 컨테이너를 사용하면 모델 파티션 및 배치 및 양자화와 같은 기타 최적화가 가능합니다. 기존 LMI Llama 7B Chat 배포 예제를 사용하여 모델 아티팩트를 빌드할 것입니다.\n- 요약 모델: 대화의 요약에 대해, Karthick Kaliannan Neelamohan (Apache 2.0 라이센스)에 의해 파인튜닝 된 HuggingFace Hub 모델을 사용할 것입니다. 기본 모델은 BART이며 이미 인기있는 SAMSUM 및 DIALOGSUM 데이터 세트에서 파인튜닝되었습니다. 자체 모델과 데이터가있는 경우 직접 파인튜닝도 가능합니다.\n\n이제 다룰 다양한 구성 요소를 이해했으니 예제로 바로 들어가 봅시다!\n\n<div class=\"content-ad\"></div>\n\n# 주의: 본 기사는 Python, LLMs 및 Amazon SageMaker 인퍼런스에 대한 중급 이해를 전제로 합니다. Amazon SageMaker 인퍼런스를 시작하는 데 도움이 될 것으로 보입니다.\n\n### 면책 조항: 저는 AWS의 머신러닝 아키텍트이며 제 의견은 제 개인적인 것입니다.\n\n## 목차\n\n- 설정 및 엔드포인트 생성\n- 인퍼런스 컴포넌트 배포\n  a. Llama7B 챗 인퍼런스 컴포넌트 생성\n  b. BART 요약 인퍼런스 컴포넌트 생성\n- Streamlit UI 생성 및 데모\n- 추가 자료 및 결론\n\n<div class=\"content-ad\"></div>\n\n# 1. 설정 및 엔드포인트 생성\n\n개발을 위해 SageMaker Studio에서 ml.c5.xlarge 인스턴스와 conda_python3 커널을 사용할 것입니다. SageMaker 엔드포인트 및 추론 구성 요소를 생성하기 위해 AWS Boto3 Python SDK 및 상위 수준 SageMaker Python SDK를 사용할 것입니다.\n\n```python\nimport boto3\nimport sagemaker\nimport time\nfrom time import gmtime, strftime\n\n# 설정\nclient = boto3.client(service_name=\"sagemaker\")\nruntime = boto3.client(service_name=\"sagemaker-runtime\")\nboto_session = boto3.session.Session()\ns3 = boto_session.resource('s3')\nregion = boto_session.region_name\nsagemaker_session = sagemaker.Session()\nbucket = sagemaker_session.default_bucket()\nrole = sagemaker.get_execution_role()\nprint(f\"Role ARN: {role}\")\nprint(f\"Region: {region}\")\n\n# 클라이언트 설정\ns3_client = boto3.client(\"s3\")\nsm_client = boto3.client(\"sagemaker\")\nsmr_client = boto3.client(\"sagemaker-runtime\")\n```\n\n추론 구성 요소를 만들기 전에 먼저 SageMaker 실시간 엔드포인트를 만들어야 합니다. 여기에서 인스턴스 유형, 수량 및 엔드포인트 수준에서 관리되는 AutoScaling을 활성화합니다.\n\n<div class=\"content-ad\"></div>\n\n위의 표를 마크다운 형식으로 변경해 주세요.\n\n우녕하세울나, 이것은 모델/추론 컴포넌트 수준에서 AutoScaling을 활성화하는 방법과는 다릅니다. 거기서는 각 추론 컴포넌트마다 AutoScaling 정책을 적용하여 모델 복사본 수를 조절할 수 있습니다. 각 모델 복사본은 추론 컴포넌트에 할당된 하드웨어 양을 유지합니다. 엔드포인트 수준에서 관리되는 AutoScaling을 설정하면 활성화한 추론 컴포넌트 수준의 스케일링을 처리할 충분한 컴퓨팅 자원이 있는지 확인해야 합니다. 엔드포인트 수준에서 자체 AutoScaling 정책을 정의할 수도 있지만, 여기에서 컴포넌트 및 엔드포인트 수준의 정책 간의 충돌 가능성에 유의해야 합니다.\n\n```js\n# Container Parameters, increase health check for LLMs: \nvariant_name = \"AllTraffic\"\ninstance_type = \"ml.g5.12xlarge\" # 하나의 인스턴스 당 4개의 GPU\nmodel_data_download_timeout_in_seconds = 3600\ncontainer_startup_health_check_timeout_in_seconds = 3600\n\n# 엔드포인트 수준에서 관리되는 AutoScaling 설정\ninitial_instance_count = 1\nmax_instance_count = 2\nprint(f\"초기 인스턴스 수: {initial_instance_count}\")\nprint(f\"최대 인스턴스 수: {max_instance_count}\")\n```\n\n또한 LLMs를 처리하기 위해 컨테이너 레벨 매개변수를 활성화하고 최소 미해결 요청(LOR) 라우팅을 활성화합니다.\n\n```js\n# 엔드포인트 구성 생성\nendpoint_config_response = client.create_endpoint_config(\n    EndpointConfigName=epc_name,\n    ExecutionRoleArn=role,\n    ProductionVariants=[\n        {\n            \"VariantName\": variant_name,\n            \"InstanceType\": instance_type,\n            \"InitialInstanceCount\": 1,\n            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n            \"ManagedInstanceScaling\": {\n                \"Status\": \"ENABLED\",\n                \"MinInstanceCount\": initial_instance_count,\n                \"MaxInstanceCount\": max_instance_count,\n            },\n            # 가장 적은 미해결 또는 임의로 설정할 수 있음: https://aws.amazon.com/blogs/machine-learning/minimize-real-time-inference-latency-by-using-amazon-sagemaker-routing-strategies/\n            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n        }\n    ],\n)\n\n# 엔드포인트 생성\nendpoint_name = \"ic-ep-chatbot\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\ncreate_endpoint_response = client.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=epc_name,\n)\nprint(\"엔드포인트 Arn: \" + create_endpoint_response[\"EndpointArn\"])\n```\n\n<div class=\"content-ad\"></div>\n\n내부에서 우리의 엔드포인트를 만든 후에는 두 가지 다른 모델을 나타내는 두 가지 추론 컴포넌트를 추가할 수 있습니다.\n\n## 2. 추론 컴포넌트 배포\n\n추론 컴포넌트는 단일 모델 컨테이너를 나타냅니다. 일반적으로 추론 컴포넌트를 생성하려면 SageMaker 모델 객체를 참조하고 모델 데이터 및 컨테이너 정보를 상속받을 수 있습니다. 이 정보 외에 할당하는 컴퓨팅 및 모델의 복사본 수를 추가할 수 있습니다. 각 복사본은 초기에 할당한 추론 컴포넌트에 지정된 것과 동일한 컴퓨팅을 가질 것입니다.\n\n### a. Llama7B 채팅 추론 컴포넌트 생성\n\n<div class=\"content-ad\"></div>\n\n\"Llama7B Chat\"에는 DJL Serving에서 제공되는 LMI 컨테이너를 사용할 예정입니다. LMI 컨테이너를 사용하면 serving.properties 파일을 지정하여 작업 중인 모델과 일괄 처리 및 양자화와 같은 기타 최적화를 지정할 수 있습니다.\n\n```js\nengine=MPI\noption.model_id=TheBloke/Llama-2-7B-Chat-fp16\noption.task=text-generation\noption.trust_remote_code=true\noption.tensor_parallel_degree=1\noption.max_rolling_batch_size=32\noption.rolling_batch=lmi-dist\noption.dtype=fp16\n```\n\n\"model_id\" 매개변수는 작업 중인 모델을 지정합니다. 이 경우, 모델 가중치는 HuggingFace Model ID에서 지정된 것을 가져옵니다. 사용자 지정으로 사전 조정된 모델이있는 경우 해당 모델 가중치의 S3 경로를 지정할 수 있습니다. 이 serving 파일과 함께 사용자 지정 사전/후 처리 또는 사용자 고유의 모델 로딩 로직이 있는 경우 추론 스크립트를 지정할 수 있습니다. 이를 구성한 후에는 SageMaker 모델 개체에 대해 예상대로 model.tar.gz를 만들고 사용 중인 컨테이너(관리되는 컨테이너 또는 자체 컨테이너)를 지정합니다.\n\n```js\n%%sh\n# tarball 생성\nmkdir mymodel\nrm mymodel.tar.gz\nmv serving.properties mymodel/\nmv model.py mymodel/\ntar czvf mymodel.tar.gz mymodel/\nrm -rf mymodel\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n# 이미지 가져오기\nimage_uri = sagemaker.image_uris.retrieve(\n        framework=\"djl-deepspeed\",\n        region=sagemaker_session.boto_session.region_name,\n        version=\"0.26.0\"\n    )\nprint(f\"사용 중인 이미지: {image_uri}\")\n\n# SageMaker 모델 객체 생성\nfrom sagemaker.utils import name_from_base\nllama_model_name = name_from_base(f\"Llama-7b-chat\")\nprint(llama_model_name)\n\ncreate_model_response = sm_client.create_model(\n    ModelName=llama_model_name,\n    ExecutionRoleArn=role,\n    PrimaryContainer={\"Image\": image_uri, \"ModelDataUrl\": code_artifact},\n)\nmodel_arn = create_model_response[\"ModelArn\"]\n\nprint(f\"모델 생성됨: {model_arn}\")\n```\n\n추론 구성 요소에서는 SageMaker 모델 객체에서 이 메타데이터를 상속할 수 있습니다. 이 외에도 컴퓨팅 요구 사항과 복사본 수를 지정합니다. Llama7B Chat의 경우 한 복사본 당 하나의 GPU를 지정합니다.\n\n```js\nllama7b_ic_name = \"llama7b-chat-ic\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nvariant_name = \"AllTraffic\"\n\n# llama 추론 컴포넌트 반응\ncreate_llama_ic_response = sm_client.create_inference_component(\n    InferenceComponentName=llama7b_ic_name,\n    EndpointName=endpoint_name,\n    VariantName=variant_name,\n    Specification={\n        \"ModelName\": llama_model_name,\n        \"ComputeResourceRequirements\": {\n            # llama 7b 채팅에는 하나의 GPU가 필요합니다\n            \"NumberOfAcceleratorDevicesRequired\": 1,\n            \"NumberOfCpuCoresRequired\": 1,\n            \"MinMemoryRequiredInMb\": 1024,\n        },\n    },\n    # 복사본에 대한 자동 스케일링 설정 가능, 각 복사본은 할당한 하드웨어를 유지합니다\n    RuntimeConfig={\"CopyCount\": 1},\n)\n\nprint(\"IC Llama Arn: \" + create_llama_ic_response[\"InferenceComponentArn\"])\n```\n\n추론 컴포넌트를 생성한 후 샘플 추론을 실행하여 추론 컴포넌트 이름을 헤더로 지정할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\nimport json\r\ncontent_type = \"application/json\"\r\nchat = [\r\n  {\"role\": \"user\", \"content\": \"안녕하세요, 어떻게 지내세요?\"},\r\n  {\"role\": \"assistant\", \"content\": \"저는 잘 지내고 있어요. 오늘 어떻게 도와드릴까요?\"},\r\n  {\"role\": \"user\", \"content\": \"저는 기계 학습에 대해 더 배우고 싶은 소프트웨어 엔지니어입니다.\"},\r\n]\r\n\r\npayload = {\"chat\": chat, \"parameters\": {\"max_tokens\":256, \"do_sample\": True}\r\nresponse = smr_client.invoke_endpoint(\r\n    EndpointName=endpoint_name,\r\n    InferenceComponentName=llama7b_ic_name, # IC 이름 지정\r\n    ContentType=content_type,\r\n    Body=json.dumps(payload),\r\n)\r\nresult = json.loads(response['Body'].read().decode())\r\nprint(type(result['content']))\r\nprint(type(result))\r\n```\r\n\r\n## b. BART Summarization 추론 컴포넌트 생성\r\n\r\nBART 추론 컴포넌트의 생성은 Llama7B 채팅 컴포넌트와 매우 유사합니다. 주요 차이점은 사용하는 컨테이너가 다르기 때문에 모델 데이터와 이미지 URI의 패키지화 방법이 다를 것입니다. 이 경우에는 HuggingFace PyTorch 이미지를 사용하고 HuggingFace 모델 ID와 해결하려는 NLP 작업을 지정합니다.\r\n\r\n```js\r\nfrom sagemaker.utils import name_from_base\r\n\r\nbart_model_name = name_from_base(f\"bart-summarization\")\r\nprint(bart_model_name)\r\n\r\n# 필요한 경우 귀하의 지역으로 교체\r\nhf_transformers_image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-cpu-py39-ubuntu20.04'\r\n\r\n# 환경 변수\r\nenv = {'HF_MODEL_ID': 'knkarthick/MEETING_SUMMARY',\r\n      'HF_TASK':'summarization',\r\n      'SAGEMAKER_CONTAINER_LOG_LEVEL':'20',\r\n      'SAGEMAKER_REGION':'us-east-1'}\r\n\r\ncreate_model_response = sm_client.create_model(\r\n    ModelName=bart_model_name,\r\n    ExecutionRoleArn=role,\r\n    # 이 경우 HF Hub로 직접 모델 데이터를 가리키는 데이터 포인트 없음\r\n    PrimaryContainer={\"Image\": hf_transformers_image_uri, \r\n                      \"Environment\": env},\r\n)\r\nmodel_arn = create_model_response[\"ModelArn\"]\r\nprint(f\"생성된 모델: {model_arn}\")\r\n```\n\n<div class=\"content-ad\"></div>\n\n한 번 더 SageMaker Model 객체를 Inference 구성 요소에 전달하고 모델에 필요한 하드웨어를 지정합니다.\n\n```js\nbart_ic_name = \"bart-summarization-ic\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nvariant_name = \"AllTraffic\"\n\n# BART inference component reaction\ncreate_bart_ic_response = sm_client.create_inference_component(\n    InferenceComponentName=bart_ic_name,\n    EndpointName=endpoint_name,\n    VariantName=variant_name,\n    Specification={\n        \"ModelName\": bart_model_name,\n        \"ComputeResourceRequirements\": {\n            # will reserve one GPU\n            \"NumberOfAcceleratorDevicesRequired\": 1,\n            \"NumberOfCpuCoresRequired\": 8,\n            \"MinMemoryRequiredInMb\": 1024,\n        },\n    },\n    # can setup autoscaling for copies, each copy will retain the hardware you have allocated\n    RuntimeConfig={\"CopyCount\": 1},\n)\n\nprint(\"IC BART Arn: \" + create_bart_ic_response[\"InferenceComponentArn\"])\r\n```\n\nInference 구성 요소가 모두 생성되면 SageMaker Studio UI에서 시각화할 수 있습니다:\n\n<img src=\"/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_1.png\" />\n\n<div class=\"content-ad\"></div>\n\n# 3. Streamlit UI 생성 및 데모\n\n이제 SageMaker 엔드포인트와 추론 컴포넌트를 생성했으니, 이 모든 것을 Streamlit 애플리케이션에서 함께 사용할 수 있습니다. 나중에 호출할 기준으로 환경 변수를 설정해줍니다.\n\n```js\nimport json\nimport os\nimport streamlit as st\nfrom streamlit_chat import message\nimport boto3\n\nsmr_client = boto3.client(\"sagemaker-runtime\")\nos.environ[\"endpoint_name\"] = \"여기에 엔드포인트 이름 입력\"\nos.environ[\"llama_ic_name\"] = \"여기에 llama IC 이름 입력\"\nos.environ[\"bart_ic_name\"] = \"여기에 bart IC 이름 입력\"\n```\n\n또한 사용자 입력, 모델 출력 및 채팅 대화를 유지하기 위해 Streamlit 세션 상태 변수를 설정합니다. 대화를 지우는 클리어 버튼을 만들어 이 버튼을 클릭하면 우리가 정의한 상태 변수를 재설정합니다.\n\n<div class=\"content-ad\"></div>\n\n```python\n# 세션 상태 변수에 사용자 및 모델 입력을 저장합니다.\nif 'generated' not in st.session_state:\n    st.session_state['generated'] = []\nif 'past' not in st.session_state:\n    st.session_state['past'] = []\nif 'chat_history' not in st.session_state:\n    st.session_state['chat_history'] = []\n\n# 클리어 버튼\nclear_button = st.sidebar.button(\"대화 지우기\", key=\"clear\")\n# 클릭 시 모든 것 초기화\nif clear_button:\n    st.session_state['generated'] = []\n    st.session_state['past'] = []\n    st.session_state['chat_history'] = []\n```\n\n사용자 입력을 받기 위한 제출 버튼을 생성하고, 이 버튼을 클릭하면 Llama7B 채팅 모델이 호출됩니다.\n\n```python\nif submit_button and user_input:\n    st.session_state['past'].append(user_input)\n    model_input = {\"role\": \"user\", \"content\": user_input}\n    st.session_state['chat_history'].append(model_input)\n    payload = {\"chat\": st.session_state['chat_history'], \"parameters\": {\"max_tokens\":400, \"do_sample\": True,\n                                                                        \"maxOutputTokens\": 2000}\n    # Llama 호출\n    response = smr_client.invoke_endpoint(\n        EndpointName=os.environ.get(\"endpoint_name\"),\n        InferenceComponentName=os.environ.get(\"llama_ic_name\"), # IC 이름 지정\n        ContentType=\"application/json\",\n        Body=json.dumps(payload),\n    )\n    full_output = json.loads(response['Body'].read().decode())\n    print(full_output)\n    display_output = full_output['content']\n    print(display_output)\n    st.session_state['chat_history'].append(full_output)\n    st.session_state['generated'].append(display_output)\n```\n\n다음 명령으로 앱을 시작하면 UI를 볼 수 있으며 작동 중인 질문 응답 채팅 모델을 확인할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\r\nstreamlit run app.py\r\n```\n\n<img src=\"/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_2.png\" />\n\n사이드에는 요약 버튼도 만들어 봅시다:\n\n```js\r\nsummarize_button = st.sidebar.button(\"대화 요약\", key=\"summarize\")\r\n```\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_3.png\" />\n\n요약 후에 미세 조정된 BART 모델을 호출합니다. BART 모델의 경우 입력이 모델이 이해할 수 있는 형식으로 구조화되어야 합니다. 다음과 유사한 형식으로 구조화된 입력을 원합니다:\n\n<img src=\"/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_4.png\" />\n\n\"chat_history\" 상태 변수에 입력 및 출력을 모두 캡처하여 모델에 맞게 형식화하고 BART Inference Component를 호출합니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\n# 요약을 위한\nif summarize_button:\n    st.header(\"Summary\")\n    st.write(\"요약 생성 중....\")\n    chat_history = st.session_state['chat_history']\n    text = ''''''\n    for resp in chat_history:\n        if resp['role'] == \"user\":\n            text += f\"Ram: {resp['content]}\\n\"\n        elif resp['role'] == \"assistant\":\n            text += f\"AI: {resp['content']}\\n\"\n    summary_payload = {\"inputs\": text}\n    summary_response = smr_client.invoke_endpoint(\n        EndpointName=os.environ.get(\"endpoint_name\"),\n        InferenceComponentName=os.environ.get(\"bart_ic_name\"),  # IC 이름 지정\n        ContentType=\"application/json\",\n        Body=json.dumps(summary_payload),\n    )\n    summary_result = json.loads(summary_response['Body'].read().decode())\n    summary = summary_result[0]['summary_text']\n    st.write(summary)\n```\n\n![이미지](/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_5.png)\n\n# 4. 추가 자료 및 결론\n\n전체 예제 코드는 위 링크에서 찾을 수 있습니다. 다중 LLM(Large Language Model)을 실제 사례에 대해 비용 효율적이고 성능 효율적으로 활용하는 방법을 보여주는 좋은 예제였기를 희망합니다.\n\n\n<div class=\"content-ad\"></div>\n\n위에서 논의한 주제와 그 외에도 더 다뤄볼 GenAI/AWS 기사와 심층적인 내용을 기대해주세요. 항상 읽어주셔서 감사합니다. 의견이 있으시면 언제든지 남겨주세요. \n\n이 기사가 마음에 드셨다면 LinkedIn에서 저와 연결하고 제 Medium 뉴스레터를 구독해보세요.","ogImage":{"url":"/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_0.png"},"coverImage":"/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_0.png","tag":["Tech"],"readingTime":16},{"title":"자연어 처리NLP 입문 기본 개념과 활용 방법","description":"","date":"2024-06-23 20:04","slug":"2024-06-23-IntroductiontoNaturalLanguageProcessingNLP","content":"\n\n![image](/assets/img/2024-06-23-IntroductiontoNaturalLanguageProcessingNLP_0.png)\n\n# 자연 언어 처리(NLP) 및 응용 프로그램 개요\n\n자연 언어 처리(NLP)는 인공 지능(AI)의 하위 분야로, 컴퓨터와 인간 간의 자연 언어를 통한 상호 작용에 중점을 둡니다. 이는 자연 언어와 음성을 분석하고 합성하기 위해 계산 기술을 적용하는 것을 의미합니다. NLP는 컴퓨터 과학, 언어학 및 기계 학습을 결합하여 컴퓨터가 인간의 언어를 이해하고 해석하며 생산할 수 있도록 합니다.\n\n# NLP의 응용 프로그램\n\n<div class=\"content-ad\"></div>\n\nNLP는 다음과 같은 다양한 응용 프로그램을 포함하고 있습니다:\n\n- 텍스트 분류: 스팸 메일 감지, 감성 분석 및 주제 분류와 같이 미리 정의된 범주로 텍스트를 자동으로 분류합니다.\n- 명명된 엔티티 인식 (NER): 텍스트에서 사람, 조직, 위치, 날짜 등과 같은 엔티티를 식별하고 분류합니다.\n- 기계 번역: 한 언어에서 다른 언어로 텍스트를 번역합니다. 구글 번역과 같은 서비스가 여기에 해당합니다.\n- 감성 분석: 소셜 미디어 게시물에서 표현된 감정을 판별합니다. 긍정적, 부정적 또는 중립적인 감정을 분석합니다.\n- 텍스트 요약: 긴 텍스트의 간결한 요약을 자동으로 생성합니다.\n- 질의 응답: 자연 언어로 제기된 질문에 답변할 수 있는 시스템을 구축합니다.\n\n<img src=\"/assets/img/2024-06-23-IntroductiontoNaturalLanguageProcessingNLP_1.png\" />\n\n# 기본 NLP 작업\n\n<div class=\"content-ad\"></div>\n\n텍스트 분류 모델 구축에 들어가기 전에 몇 가지 기본 NLP 작업을 이해하는 것이 중요합니다: 토큰화, 어간 추출 및 표제어 추출.\n\n# 토큰화\n\n토큰화는 텍스트를 개별 단어 또는 토큰으로 분해하는 과정입니다. 이는 텍스트 처리의 첫 번째 단계입니다.\n\n```python\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\n\ntext = \"Natural Language Processing is fascinating.\"\ntokens = word_tokenize(text)\nprint(tokens)\n```\n\n<div class=\"content-ad\"></div>\n\n# 어간 추출\n\n어간 추출은 단어를 그 뿌리 형태로 줄입니다. 예를들어, \"running\"은 \"run\"이 됩니다.\n\n```js\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nwords = [\"running\", \"ran\", \"runs\"]\nstemmed_words = [stemmer.stem(word) for word in words]\nprint(stemmed_words)\n```\n\n# 표제어 추출\n\n<div class=\"content-ad\"></div>\n\n표 태그를 마크다운 형식으로 변경해주세요.\n\n`js`\n```python\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nlemmatizer = WordNetLemmatizer()\nwords = [\"running\", \"ran\", \"runs\"]\nlemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\nprint(lemmatized_words)\n```\n\n## 간단한 텍스트 분류 모델 구축\n\n이 섹션에서는 파이썬을 사용하여 간단한 텍스트 분류 모델을 구축할 것입니다. 머신러닝 모델을 사용하여 영화 리뷰를 긍정적 또는 부정적으로 분류할 것입니다. 이를 위해 sklearn 라이브러리를 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 단계 1: 라이브러리 및 데이터셋 불러오기\n\n먼저, 필요한 라이브러리를 가져와서 데이터셋을 로드해보겠습니다. 영화 리뷰 데이터셋을 다운로드하기 위해 nltk 라이브러리를 사용할 것입니다.\n\n```python\nimport nltk\nfrom sklearn.datasets import load_files\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# 영화 리뷰 데이터셋 다운로드\nnltk.download('movie_reviews')\nfrom nltk.corpus import movie_reviews\n# 데이터셋 로드\ndocuments = [(movie_reviews.raw(fileid), category)\n             for category in movie_reviews.categories()\n             for fileid in movie_reviews.fileids(category)]\n```\n\n# 단계 2: 데이터셋 준비하기\n\n<div class=\"content-ad\"></div>\n\n데이터 세트를 학습 및 테스트 세트로 분할해야 합니다.\n\n```js\n# 데이터 세트를 텍스트와 레이블로 분할합니다\n텍스트, 레이블 = zip(*문서)\n\n# 학습 및 테스트 세트로 분할합니다\nX_학습, X_테스트, y_학습, y_테스트 = train_test_split(텍스트, 레이블, test_size=0.2, random_state=42)\n```\n\n# 단계 3: 텍스트 전처리\n\n텍스트 데이터를 숫자 벡터로 변환하기 위해 TfidfVectorizer를 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 텍스트 데이터를 TF-IDF 특성으로 변환합니다\nvectorizer = TfidfVectorizer(stop_words='english')\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n```\n\n# 단계 4: 모델 훈련\n\n텍스트 분류 모델을 훈련하는 데 Naive Bayes 알고리즘을 사용할 것입니다.\n\n```js\n# 나이브 베이즈 분류기를 훈련합니다\nclassifier = MultinomialNB()\nclassifier.fit(X_train_tfidf, y_train)\n```\n\n<div class=\"content-ad\"></div>\n\n# 단계 5: 모델 평가하기\n\n이제 테스트 데이터에서 모델을 평가해 봅시다.\n\n```js\n# 테스트 세트에 대한 레이블 예측\ny_pred = classifier.predict(X_test_tfidf)\n\n# 정확도 계산\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'정확도: {accuracy * 100:.2f}%')\n# 분류 보고서 출력\nprint(classification_report(y_test, y_pred))\n```\n\n이 블로그에서 소개된 단계를 따르면, 이제 자연어 처리에 대한 기본적인 이해력과 기본적인 텍스트 분류 모델을 구축할 수 있는 능력이 생겼을 것입니다. 이것은 시작에 불과합니다. 자연어 처리 분야에서 더 많은 고급 기술과 모델을 탐험할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이러한 기술과 도구를 활용해 더 정교한 NLP 애플리케이션을 구축해보세요. 즐거운 코딩 되세요!","ogImage":{"url":"/assets/img/2024-06-23-IntroductiontoNaturalLanguageProcessingNLP_0.png"},"coverImage":"/assets/img/2024-06-23-IntroductiontoNaturalLanguageProcessingNLP_0.png","tag":["Tech"],"readingTime":4},{"title":"트랜스포머 쉽게 이해하기 Part 3 멀티-헤드 어텐션 심층 분석","description":"","date":"2024-06-23 20:02","slug":"2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive","content":"\n\n## 직관적인 트랜스포머 시리즈 NLP\n\n이것은 트랜스포머에 관한 제 시리즈에서 세 번째 글입니다. 우리는 위에서 아래로의 방식으로 그 기능을 다루고 있습니다. 이전 글에서는 트랜스포머가 무엇인지, 그 구조, 그리고 어떻게 작동하는지 배웠습니다.\n\n이번 글에서는 한 발 더 나아가 Multi-head Attention에 대해 더 자세히 파고들 것입니다. 이것이 트랜스포머의 두뇌라고 할 수 있습니다.\n\n시리즈에서 이전 및 다음 글에 대한 간단한 요약입니다. 내 목표는 무엇이 어떻게 작동하는지만 아는 것이 아니라, 왜 그런 방식으로 작동하는지 이해하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n- 기능 개요 (Transformer의 사용 방법 및 RNN보다 우수한 점. 아키텍처 구성 요소 및 훈련 및 추론 중의 동작)\r\n- 작동 방식 (내부 운영 end-to-end. 데이터 흐름 및 수행되는 계산, 행렬 표현 등)\r\n- Multi-head Attention - 이 기사 (Transformer 전체에서 Attention 모듈의 작동 방식)\r\n- 왜 Attention이 성능을 향상시키는가 (Attention이 하는 일뿐만 아니라 왜 잘 작동하는지. Attention이 문장 내 단어 간의 관계를 어떻게 파악하는지)\r\n\r\n그리고 자연어 처리 응용 프로그램에 관심이 있다면, 좋아할만한 기사가 몇 개 더 있습니다.\r\n\r\n- Beam Search (음성인식 및 자연어 처리 응용프로그램에서 일반적으로 사용되는 알고리즘으로 예측을 향상하는 방법)\r\n- Bleu Score (Bleu Score 및 Word Error Rate는 자연어 처리 모델의 두 가지 필수적인 메트릭스입니다)\r\n\r\n# Transformer에서 어떻게 Attention이 사용되는가\n\n<div class=\"content-ad\"></div>\n\nPart 2에서 논의한 대로, 어텐션은 Transformer에서 세 곳에서 사용됩니다:\n\n- 인코더의 셀프 어텐션 - 입력 시퀀스가 자신에게 주의를 기울임\n- 디코더의 셀프 어텐션 - 대상 시퀀스가 자신에게 주의를 기울임\n- 디코더의 인코더-디코더 어텐션 - 대상 시퀀스가 입력 시퀀스에 주의를 기울임\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_0.png)\n\n어텐션 입력 매개변수 - 쿼리(Query), 키(Key), 값(Value)\n\n<div class=\"content-ad\"></div>\n\n주의 계층은 Query, Key 및 Value로 알려진 세 매개변수 형식으로 입력을 받습니다.\n\n세 매개변수는 각 단어가 벡터로 표현된 시퀀스와 유사한 구조를 갖습니다.\n\n인코더 셀프 어텐션\n\n입력 시퀀스는 입력 임베딩 및 위치 인코딩에 공급되어 각 입력 시퀀스의 각 단어에 대한 의미와 위치를 캡처하는 인코딩 표현을 생성합니다. 이것은 모두 Query, Key 및 Value 매개변수에 공급되며, 첫 번째 인코더의 셀프 어텐션에서 각 입력 시퀀스의 각 단어에 대한 인코딩 표현을 생성합니다. 이제 각 단어에 대한 어텐션 점수도 포함된 새 인코딩 표현입니다. 이것이 스택 내의 모든 인코더를 통과할 때마다 각 셀프 어텐션 모듈은 각 단어의 표현에 자체 어텐션 점수를 추가합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_1.png)\n\n디코더 셀프 어텐션\n\n디코더 스택으로 오는 경우, 목표 시퀀스가 출력 임베딩과 위치 인코딩으로 전달됩니다. 이는 목표 시퀀스의 각 단어에 대한 의미와 위치를 캡처한 인코딩 표현을 생성합니다. 이것은 첫 번째 디코더의 Self-Attention에서 모든 세 가지 매개변수인 Query, Key, Value로 전달되며, 목표 시퀀스의 각 단어에 대한 인코딩 표현을 생성하고 각 단어의 주의 점수를 반영합니다.\n\n레이어 정규화를 거친 후, 이것은 첫 번째 디코더의 인코더-디코더 어텐션에서 Query 매개변수로 전달됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n에 대한 관심\n\n이에 더불어 스택 내 최종 인코더의 출력은 Encoder-Decoder Attention의 Value 및 Key 매개변수로 전달됩니다.\n\n따라서 Encoder-Decoder Attention은 디코더 Self-Attention의 대상 시퀀스와 인코더 스택의 입력 시퀀스의 표현을 받게 됩니다. 이로써 입력 시퀀스의 관심 점수의 영향을 포착하는 대상 시퀀스 단어마다 관심 점수를 포함하는 표현을 생성합니다.\n\n이것은 스택 내 모든 디코더를 통해 전달되므로, 각 Self-Attention 및 각 Encoder-Decoder Attention은 각 단어의 표현에 고유한 관심 점수를 추가합니다.\n\n<div class=\"content-ad\"></div>\n\n# 다중 어텐션 헤드\n\nTransformer에서 어텐션 모듈은 병렬로 동시에 여러 번 계산을 반복합니다. 각각을 어텐션 헤드라고 합니다. 어텐션 모듈은 쿼리(Query), 키(Key), 값(Value) 매개변수를 N개로 분할하고 각 분할을 별도로 헤드를 통해 전달합니다. 모든 이 유사한 어텐션 계산은 그런 다음 합쳐져 최종 어텐션 점수를 생성합니다. 이를 다중 헤드 어텐션이라고 하며, 이를 통해 Transformer는 단어마다 여러 관계와 뉘앙스를 인코딩하는 강력한 능력을 갖게 됩니다.\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_2.png)\n\n내부에서 데이터가 어떻게 처리되는지 정확히 이해하기 위해 Transformer를 훈련하여 번역 문제를 해결하는 과정에서 어텐션 모듈의 작동 방식을 살펴보겠습니다. 영어로 된 입력 시퀀스('You are welcome')와 스페인어로 된 대상 시퀀스('De nada')로 구성된 훈련 데이터의 샘플을 사용하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 주목해야 할 하이퍼파라미터\n\n데이터 차원을 결정하는 세 가지 하이퍼파라미터가 있습니다:\n\n- 임베딩 크기 — 임베딩 벡터의 너비입니다 (예시에서는 너비 6을 사용합니다). 이 차원은 Transformer 모델 전반에서 전달되며 때로는 '모델 크기'와 같은 다른 이름으로 불리기도 합니다.\n- 쿼리 크기 (키 및 값 크기에 동일) — 쿼리, 키 및 값 행렬을 만들기 위해 세 개의 선형 레이어에서 사용되는 가중치의 크기입니다 (예시에서는 쿼리 크기를 3으로 사용합니다).\n- 어텐션 헤드의 수 (예시에서는 2개의 헤드를 사용합니다)\n\n또한 샘플의 수를 나타내는 배치 크기가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 입력 레이어\n\n입력 임베딩 및 위치 인코딩 레이어는 (샘플 수, 시퀀스 길이, 임베딩 크기) 형태의 행렬을 생성하여 스택 내 첫 번째 인코더의 쿼리, 키 및 값에 공급됩니다.\n\n![](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_3.png)\n\n시각화를 간단하게 만들기 위해 이미지에서 배치 차원을 제거하고 나머지 차원에 초점을 맞추겠습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_4.png)\n\n## 선형 레이어\n\n쿼리(Query), 키(Key), 값(Value)에 대한 세 개의 별개의 선형 레이어가 있습니다. 각 선형 레이어는 고유한 가중치를 가지고 있습니다. 입력은 이러한 선형 레이어를 거쳐 Q, K, V 행렬을 생성합니다.\n\n## 어텐션 헤드별 데이터 분할\n\n\n<div class=\"content-ad\"></div>\n\n이제 데이터가 여러 개의 Attention head로 분할되어 각각 독립적으로 처리됩니다.\n\n그러나 이해해야 할 중요한 점은 이것이 논리적인 분할임을 주의해야 합니다. Query, Key 및 Value는 물리적으로 분리된 행렬로 분할되지 않습니다. Query, Key 및 Value에 대해 각 Attention head마다 하나의 데이터 행렬이 사용되며, 각 Attention head의 논리적으로 분리된 섹션이 행렬에 있습니다. 마찬가지로 Attention head마다 별도의 Linear layer가 없습니다. 모든 Attention head가 동일한 Linear layer를 공유하지만 각자의 논리적인 데이터 행렬 섹션에 작용합니다.\n\nLinear layer의 가중치는 head별로 논리적으로 분할됩니다.\n\n이 논리적인 분할은 입력 데이터와 Linear layer의 가중치를 Attention head별로 균등하게 분할함으로써 이뤄집니다. 위와 같이 Query Size를 선택함으로써 이를 달성할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n\nQuery Size = Embedding Size / Number of heads\n\n![Image 1](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_5.png)\n\nIn our example, that is why the Query Size = 6/2 = 3. Even though the layer weight (and input data) is a single matrix we can think of it as ‘stacking together’ the separate layer weights for each head.\n\n![Image 2](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_6.png)\n\n\n<div class=\"content-ad\"></div>\n\n모든 헤드에 대한 계산은 N 개의 별도 연산이 필요한 대신 단일 행렬 연산을 통해 달성할 수 있습니다. 이렇게 하면 계산이 효율적으로 이루어지고 모델이 단순해지며 선형 레이어가 더 적게 필요하지만 독립적인 어텐션 헤드의 힘을 여전히 발휘할 수 있습니다.\n\nQ, K 및 V 행렬 재구성\n\n선형 레이어에서 출력된 Q, K 및 V 행렬을 명시적인 헤드 차원이 포함된 모양으로 재구성합니다. 이제 각 '슬라이스'가 하나의 헤드당 하나의 행렬에 해당합니다.\n\n이 행렬은 다시 헤드와 시퀀스 차원을 교체하여 재구성됩니다. 배치 차원은 그림에 표시되지 않았지만, Q의 차원은 이제 (Batch, Head, Sequence, Query size)입니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_7.png)\n\n아래 그림에서는 Linear 레이어에서 나온 예제 Q 매트릭스를 분할하는 전체 과정을 확인할 수 있습니다.\n\n최종 단계는 시각화용입니다 — Q 매트릭스는 단일 매트릭스이지만, 논리적으로 독립된 각 head마다 별도의 Q 매트릭스로 생각할 수 있습니다.\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_8.png)\n\n<div class=\"content-ad\"></div>\n\n우리는 어텐션 점수를 계산할 준비가 되어 있어요.\n\n# 각 헤드에 대한 어텐션 점수 계산\n\n이제 우리는 헤드 간에 분할된 3개의 행렬인 Q, K 및 V를 소유하고 있어요. 이들은 어텐션 점수를 계산하는 데 사용돼요.\n\n우리는 하나의 헤드에 대한 연산을 보여줄 거에요. 이때는 마지막 두 차원(시퀀스 및 쿼리 크기)만 사용하고 처음 두 차원(배치 및 헤드)은 건너뛸 거에요. 본질적으로, 우리는 보고 있는 연산이 각 헤드와 각 배치 샘플마다 \"반복\"되는 것으로 상상할 수 있어요 (물론 실제로는 반복문이 아니라 단일 행렬 작업으로 이루어지고 있어요).\n\n<div class=\"content-ad\"></div>\n\n첫 번째 단계는 Q와 K 사이의 행렬 곱셈을 수행하는 것입니다.\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_9.png)\n\n결과에 Mask 값이 추가됩니다. 인코더 셀프 어텐션에서는 패딩 값을 마스킹하여 주의 점수에 참여하지 않도록합니다.\n\n다른 마스크가 디코더 셀프 어텐션 및 디코더 인코더 어텐션에서 적용되며, 이에 대해서는 흐름에서 조금 뒤에 다룹니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Transformers Explained Visually Part 3](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_10.png)\n\nThe result is now scaled by dividing by the square root of the Query size, and then a Softmax is applied to it.\n\n![Transformers Explained Visually Part 3](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_11.png)\n\nAnother matrix multiplication is performed between the output of the Softmax and the V matrix.\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_12.png\" />\n\n인코더 Self-attention의 완전한 어텐션 점수 계산은 다음과 같습니다:\n\n<img src=\"/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_13.png\" />\n\n# 각 헤드의 어텐션 점수를 병합합니다\n\n<div class=\"content-ad\"></div>\n\n각 헤드에 대해 별도의 주의 점수가 있습니다. 이것들을 한 가지 점수로 결합해야 합니다. 이 병합 작업은 본질적으로 분할 작업의 반대입니다.\n\n헤드 차원을 제거하여 결과 행렬을 단순히 재구성함으로써 수행됩니다. 이 과정은 다음과 같습니다:\n\n- Attention Score 행렬을 재구성하여 헤드 및 시퀀스 차원을 교환합니다. 다시 말해, 행렬 모양이 (배치, 헤드, 시퀀스, 쿼리 크기)에서 (배치, 시퀀스, 헤드, 쿼리 크기)로 변경됩니다.\n- 헤드 차원을 축소하여 (배치, 시퀀스, 헤드 * 쿼리 크기)로 재구성합니다. 이를 통해 각 헤드의 주의 점수 벡터를 단일 병합된 주의 점수로 연결합니다.\n\n임베딩 크기 = 헤드 * 쿼리 크기이므로 병합된 점수는 (배치, 시퀀스, 임베딩 크기)입니다. 아래 그림에서는 예제 점수 행렬에 대한 병합 과정을 자세히 볼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_14.png\" />\n\n# End-to-end Multi-head Attention\n\nPutting it all together, this is the end-to-end flow of the Multi-head Attention.\n\n<img src=\"/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_15.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n# 다중 헤드 분할은 더 풍부한 해석을 제공합니다\n\n임베딩 벡터는 단어의 의미를 포착합니다. 다중 헤드 어텐션의 경우, 입력(및 대상) 시퀀스의 임베딩 벡터가 여러 헤드에 걸쳐 논리적으로 분할된다는 것을 보았습니다. 이것의 의미는 무엇인가요?\n\n![image](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_16.png)\n\n이것은 임베딩의 별도 섹션들이 시퀀스 내 다른 단어와의 관련성에 따라 각 단어의 의미의 다른 측면을 학습할 수 있다는 것을 의미합니다. 이를 통해 트랜스포머는 시퀀스의 보다 풍부한 해석을 제공할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이것은 현실적인 예시가 아닐 수도 있지만 직관을 키우는 데 도움이 될 수 있습니다. 예를 들어, 하나의 섹션은 명사의 ‘성별’(남성, 여성, 중성)을 포함할 수 있고, 다른 하나는 명사의 ‘수’(단수 대 복수)를 포함할 수 있습니다. 이는 번역 중에 중요할 수 있는데, 많은 언어에서 사용해야 하는 동사가 이러한 요인에 따라 달라질 수 있습니다.\n\n# 디코더 셀프 어텐션 및 마스킹\n\n디코더 셀프 어텐션은 인코더 셀프 어텐션과 마찬가지로 작동하지만 대상 시퀀스의 각 단어에 대해 작동합니다.\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_17.png)\n\n<div class=\"content-ad\"></div>\n\n같은 방법으로, Masking은 대상 시퀀스에서 패딩 단어를 가려냅니다.\n\n# 디코더 인코더-디코더 어텐션 및 마스킹\n\n디코더 인코더-디코더 어텐션은 두 개의 소스에서 입력을 받습니다. 따라서, 각 입력 단어 간 상호 작용을 계산하는 인코더 자가 어텐션과 달리, 각 대상 단어 간 상호 작용을 계산하는 디코더 자가 어텐션은 각 대상 단어와 각 입력 단어 간의 상호 작용을 계산합니다.\n\n![image](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_18.png)\n\n<div class=\"content-ad\"></div>\n\n따라서 결과 Attention Score의 각 셀은 하나의 Q (즉, 대상 시퀀스 단어)와 모든 다른 K (즉, 입력 시퀀스) 단어 및 모든 V (즉, 입력 시퀀스) 단어 간 상호 작용을 나타냅니다.\n\n이와 유사하게, Masking은 대상 출력의 후속 단어를 마스킹하여 시리즈의 두 번째 기사에서 자세히 설명했던 대로 수행됩니다.\n\n# 결론\n\n이를 통해 트랜스포머의 어텐션 모듈이 무엇을 하는지에 대한 좋은 감을 제공했기를 바랍니다. 두 번째 기사에서 확인한 트랜스포머의 end-to-end 흐름과 함께 조합하면 이제 전체 트랜스포머 아키텍처의 상세 운영을 다루었습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 우리는 Transformer가 정확히 무엇을 하는지 이해했습니다. 그러나 Transformer의 Attention이 왜 그런 계산을 수행하는지에 대한 질문에 완전히 대답하지 않았습니다. 왜 Query, Key 및 Value 개념을 사용하고, 방금 본 것처럼 왜 행렬 곱셈을 수행하는 걸까요?\n\n우리는 '각 단어 간의 관계를 포착한다'는 희미한 직관을 가지고 있지만, 이게 정확히 무엇을 의미하는지는 무엇일까요? Transformer의 Attention이 앞의 각 단어의 미묘한 차이를 이해하는 능력을 가지도록 하는 방법은 무엇일까요?\n\n그것은 흥미로운 질문이며이 시리즈의 마지막 글에 대한 주제입니다. 그것을 이해하면 우리는 진정으로 Transformer 아키텍처의 우아함을 이해할 것입니다.\n\n마지막으로, 이 기사를 좋아하셨다면, 오디오 딥 러닝, 지리 위치 기계 학습 및 이미지 캡션 아키텍처에 대한 다른 시리즈도 즐기실 수 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n계속해서 배워봐요!","ogImage":{"url":"/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_0.png"},"coverImage":"/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_0.png","tag":["Tech"],"readingTime":11},{"title":"SageMaker 비동기 추론으로 대형 언어 모델 배포하는 방법","description":"","date":"2024-06-23 19:59","slug":"2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference","content":"\n\n<img src=\"/assets/img/2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference_0.png\" />\n\nLLM(대규모 언어 모델)은 인기를 얻고 있으며 이를 추측하는 방법도 늘어가고 있습니다. LLM 호스팅에는 모델의 크기와 배포된 하드웨어의 최적 사용을 보장해야 한다는 어려움이 잘 알려져 있습니다. LLM 사용 사례도 다양합니다. 어떤 것은 실시간 응답 시간이 필요할 수 있고, 다른 것은 거의 실시간 기반의 지연 시간 요구 사항일 수 있습니다.\n\n후자와 더 많은 오프라인 추측 사용 사례를 위해, SageMaker 비동기 추론이 좋은 옵션으로 제공됩니다. 비동기 추론에서는 이름에서 알 수 있듯이 지연 시간이 굉장히 엄격하지 않지만 필요에 따라 호출하고 확장할 수 있는 활성화된 엔드포인트가 필요합니다. 특히 LLM에서 이러한 유형의 작업 부하는 내용 편집/생성, 요약 등과 같은 사용 사례로 인해 점점 인기를 얻고 있습니다. 이러한 작업 부하들은 하위 초 단위 응답이 필요하지는 않지만 필요에 따라 호출할 수 있는 적시의 추론을 필요로 하며, SageMaker Batch Transform과 같은 완전히 오프라인적인 성격과는 대조적입니다.\n\n이 예시에서는 HuggingFace 텍스트 생성 추론 서버를 SageMaker 비동기 엔드포인트와 함께 사용하여 Flan-T-5-XXL 모델을 호스팅하는 방법을 살펴볼 것입니다.\n\n<div class=\"content-ad\"></div>\n\n참고: 본 글은 Python, LLMs 및 Amazon SageMaker에 대한 기본적인 이해를 전제로 합니다. Amazon SageMaker 추론을 시작하려면 다음 가이드를 참조해주세요. SageMaker 비동기 추론의 기초를 다룰 것이지만 더 깊은 소개를 원하시면 다음에 나오는 스타터 예제를 참조해주세요.\n\n공지: 저는 AWS의 머신 러닝 아키텍트이며, 이견은 제 개인적인 의견입니다.\n\n# 목차\n\n- SageMaker 비동기 추론을 사용하는 시점\n- TGI 비동기 추론 구현\n   a. 설정 및 엔드포인트 배포\n   b. 비동기 추론 호출\n   c. 자동 스케일링 설정\n- 추가 자료 및 결론\n\n<div class=\"content-ad\"></div>\n\n# 1. SageMaker 비동기 추론 사용 시점\n\nSageMaker 추론은 현재 사용 사례에 따라 활용할 수 있는 네 가지 옵션이 있습니다. 세 가지 엔드포인트 기반 옵션이 있고 완전 오프라인 추론을 위한 한 가지 옵션이 있습니다:\n\n- 엔드포인트 기반 옵션:\n    - SageMaker 실시간 추론: 서브초/밀리초 응답 시간과 고 처리량 워크로드를 위한 옵션입니다. 이 엔드포인트는 CPU, GPU 또는 Inferentia 칩을 활용하며 하드웨어 단계에서 AutoScaling을 적용하여 인프라를 확장할 수 있습니다. 일반적인 사용 사례로는 Ad-Tech 기반 예측, 실시간 챗봇 등이 있습니다.\n    - SageMaker 서버리스 추론: 갑작스럽고 간헐적인 워크로드에 최적화되어 있으며 cold-start를 허용할 수 있는 옵션입니다 (Provisioned Concurrency를 통해 완화할 수 있음). 여기서는 엔드포인트 뒤에 있는 모든 인프라를 관리하지 않으며 확장은 자동으로 처리됩니다.\n    - SageMaker 비동기 추론: 오늘 다룰 옵션으로, 비동기 추론을 통해 거의 실시간 기반의 응답 시간 요구 사항을 충족하고 여전히 엔드포인트를 위해 정의한 전용 하드웨어를 사용합니다. 그러나 비동기 추론의 경우 실시간 추론과 달리 0 개의 인스턴스로 축소할 수 있는 옵션이 있습니다. 비동기 추론을 통해 내장 큐를 사용하여 요청을 관리하고이 큐의 가득 찬 정도에 따라 확장할 수 있습니다.\n\n- 오프라인 추론:\n    - SageMaker 배치 변환: 데이터셋이 있고 데이터셋으로 반환된 출력만 필요할 때 최적입니다. 영구적인 엔드포인트는 없으며 완전히 오프라인 추론입니다. 일반적인 사용 사례로는 특정 주기에 추론이 필요한 데이터셋이 있는 경우 일정 시간에 배치 변환 작업을 실행하는 것이 있습니다.\n\n이 사용 사례에서는 특히 비동기 추론에 초점을 맞추고 있습니다. 이 옵션은 거의 실시간 능력과 0 인스턴스로 축소할 수 있는 능력 때문에 Batch Transform과 Real-Time Inference 사이에서 결혼 생각할 수 있는 옵션입니다. 즉시 생성이 필요하지 않은 사용 사례를 위한 LLM을 호스팅하는 데 효율적인 방법으로 기능을 제공할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이러한 사용 사례의 예시로는 요약, 콘텐츠 생성, 편집 등이 있습니다. 이러한 사용 사례는 모두 가변 시간에 activation이 필요할 수 있으므로 지속적인 엔드포인트가 필요하지만 실시간 추론의 응답 시간은 필요로 하지 않을 수도 있습니다. 비동기 추론을 통해 성능과 비용 측면에서 이러한 종류의 사용 사례들을 다룰 수 있습니다.\n\n오늘의 예시로, 우리는 인기 있는 Flan 모델을 SageMaker 비동기 추론에 적용해 보겠습니다. SageMaker 비동기 추론 엔드포인트를 생성하는 것은 실시간 엔드포인트 생성과 매우 유사합니다. 주요 차이점은 실시간 추론처럼 페이로드를 직접 전달하는 대신 입력 데이터에 대한 S3 경로가 필요하다는 점입니다.\n\n![이미지](/assets/img/2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference_1.png)\n\n비동기 엔드포인트 내에는 내부 대기열도 있음을 유의해야 합니다. 모든 추론마다 SageMaker는 요청을 대기열에 넣고 결과 위치를 S3에 반환합니다. 비동기 엔드포인트에 대해 AutoScaling을 구성할 때 이 대기열 내의 요청 수에 따라 스케일을 조정할 수 있습니다. 또한 출력 S3 경로에서 직접 폴링하는 대신 성공 또는 오류 있는 추론 알림을 수신하기 위해 선택적으로 SNS 토픽을 통합할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 비동기 추론에 대한 이해가 조금 더 깊어졌으니 구현부로 넘어가 봅시다!\n\n# 2. TGI 비동기 추론 구현\n\n우리는 새로운 SageMaker Studio 환경에서 Base Python3 커널과 ml.c5.xlarge 인스턴스로 작업할 것입니다. 비동기 추론을 위해 익숙한 Boto3 AWS Python SDK 및 상위 레벨 SageMaker Python SDK를 사용할 것입니다.\n\n## a. 설정 및 엔드포인트 배포\n\n<div class=\"content-ad\"></div>\n\n비동기 추론을 사용하려면 먼저 데이터가 저장될 출력 S3 경로를 정의해야 합니다.\n\n```js\nsagemaker_session = sagemaker.Session()\ndefault_bucket = sagemaker_session.default_bucket()\nbucket_prefix = \"async-llm-output\"\nasync_output_path = f\"s3://{default_bucket}/{bucket_prefix}/output\"\nprint(f\"내 모델 추론 결과는 다음 S3 경로에 저장될 것입니다: {async_output_path}\")\n```\n\n그런 다음 이 S3 경로를 사용하여 비동기 추론 구성을 지정할 수 있습니다. 이 경우 SNS 주제를 지정하지 않지만, 성공적이거나 오류가 발생했을 때 서비스를 통해 알림을받기를 원하는 경우 선택적으로 포함할 수 있습니다.\n\n```js\nfrom sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n\nasync_config = AsyncInferenceConfig(\n    output_path=async_output_path,\n    max_concurrent_invocations_per_instance=10,\n    # 선택적으로 Amazon SNS 주제 지정\n    # notification_config = {\n    # \"SuccessTopic\": \"arn:aws:sns:<aws-region>:<account-id>:<topic-name>\",\n    # \"ErrorTopic\": \"arn:aws:sns:<aws-region>:<account-id>:<topic-name>\",\n    # }\n)\n```\n\n<div class=\"content-ad\"></div>\n\n이 정의된 후에는 Flan T-5-XXL 모델을 위한 HuggingFace Hub 링크에서 SageMaker 코드를 직접 가져올 수 있습니다. 이 코드는 Text Generation Inference 모델 서버를 활용하며, Tensor Parallelism과 같은 내장 최적화가 포함되어 있습니다.\n\n```js\n# huggingface hub에서 배포 코드 직접 가져와서 비동기 구성 추가\nhub = {\n   'HF_MODEL_ID':'google/flan-t5-xxl',\n   'SM_NUM_GPUS': json.dumps(4)\n}\n\nhuggingface_model = HuggingFaceModel(\n   image_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"),\n   env=hub,\n   role=role, \n)\r\n```\n\n그런 다음 SageMaker 모델 객체와 비동기 구성을 함께 사용하여 엔드포인트를 생성할 수 있습니다.\n\n```js\n# SageMaker 추론을 위해 모델 배포\npredictor = huggingface_model.deploy(\n initial_instance_count=1,\n instance_type=\"ml.g5.12xlarge\",\n container_startup_health_check_timeout=300,\n async_inference_config=async_config\n)\r\n```\n\n<div class=\"content-ad\"></div>\n\n콘솔 또는 UI에서 엔드포인트가 비동기 타입으로 지정되었음을 확인할 수 있습니다.\n\n![image](/assets/img/2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference_2.png)\n\n## b. 비동기 추론 호출\n\n단일 페이로드로 엔드포인트를 호출하려면 실시간 추론과 같이 \"predict\" 내장 메소드를 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\n# 단일 호출\n\npayload = \"미국의 수도는 어디인가요?\"\ninput_data = {\n    \"inputs\": payload,\n    \"parameters\": {\n        \"early_stopping\": True,\n        \"length_penalty\": 2.0,\n        \"max_new_tokens\": 50,\n        \"temperature\": 1,\n        \"min_length\": 10,\n        \"no_repeat_ngram_size\": 3,\n        },\n}\npredictor.predict(input_data)\r\n```\n\n그러나 현실적인 사용 사례에 대한 비동기 추론을 확장하려면 S3에서 데이터로 엔드포인트를 호출할 수 있습니다. 비동기 추론의 아이디어는 여러 요청이 입력 S3 버킷에 저장되어 있고 호출은 각 데이터 포인트에 대한 결과와 해당하는 S3 출력 파일을 반환한다는 것입니다. 여기서 한 가지 더 강조하고 싶은 것은 전체 데이터 집합이 처리되고 요청에 의해 호출할 엔드포인트가 없는 Batch Transform과 다르다는 점입니다.\n\n이제 우리는 이 데모를 위해 동일한 데이터포인트를 가진 가짜 데이터 집합을 만들어 S3로 옮기겠습니다. 다음 코드는 입력 파일을 포함하는 로컬 디렉터리를 생성합니다:\n\n```py\nimport json\nimport os\n\noutput_directory = 'inputs'\nos.makedirs(output_directory, exist_ok=True)\n\nfor i in range(1, 20):\n    json_data = [input_data.copy()]\n\n    file_path = os.path.join(output_directory, f'input_{i}.jsonl')\n    with open(file_path, 'w') as input_file:\n        for line in json_data:\n            json.dump(line, input_file)\n            input_file.write('\\n')\r\n```\n\n<div class=\"content-ad\"></div>\n\n이미 제공된 유틸리티 함수를 사용하여 로컬 파일을 S3에 업로드하여 추론할 수 있습니다.\n\n```js\ndef upload_file(input_location):\n    prefix = f\"{bucket_prefix}/input\"\n    return sagemaker_session.upload_data(\n        input_location,\n        bucket=default_bucket,\n        key_prefix=prefix,\n        extra_args={\"ContentType\": \"application/json\"} # 꼭 지정해야함\n    )\n\nsample_data_point = upload_file(\"inputs/input_1.jsonl\")\nprint(f\"샘플 데이터 포인트가 업로드되었습니다: {sample_data_point}\")\n```\n\n그런 다음 Boto3 SDK를 통해 \"invoke_endpoint_async\" API 호출로이 S3 경로에서 샘플 추론을 실행할 수 있습니다.\n\n```js\nimport boto3\nruntime = boto3.client(\"sagemaker-runtime\")\n\nresponse = runtime.invoke_endpoint_async(\n    EndpointName=predictor.endpoint_name,\n    InputLocation=sample_data_point,\n    Accept='application/json',\n    ContentType=\"application/json\"\n)\n\noutput_location = response[\"OutputLocation\"]\nprint(f\"출력 위치: {output_location}\")\n```\n\n<div class=\"content-ad\"></div>\n\n한 번 더 제공된 유틸리티 함수를 사용하여 출력 파일의 출력을 관찰합니다. LLM을 사용하여 실제 추론을 수행하고 S3 파일을 생성하는 데 시간이 걸릴 수 있습니다. 따라서 제공된 함수에서는 화면에 표시할 내용이 포함된 데이터 파일이 나타날 때까지 데이터 파일을 확인합니다.\n\n```js\nimport urllib, time\nfrom botocore.exceptions import ClientError\n\n# 함수 참조/크레딧: https://github.com/aws/amazon-sagemaker-examples/blob/main/async-inference/Async-Inference-Walkthrough-SageMaker-Python-SDK.ipynb\ndef get_output(output_location):\n    output_url = urllib.parse.urlparse(output_location)\n    bucket = output_url.netloc\n    key = output_url.path[1:]\n    while True:\n        try:\n            return sagemaker_session.read_s3_file(bucket=output_url.netloc, key_prefix=output_url.path[1:])\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n                print(\"output을 기다리는 중...\")\n                time.sleep(60)\n                continue\n            raise\n\noutput = get_output(output_location)\nprint(f\"Output: {output}\")\n```\n\n그런 다음 모든 샘플 데이터 포인트를 실행하여 모든 입력 파일에서 추론을 수행할 수 있습니다:\n\n```js\ninferences = []\nfor i in range(1, 20):\n    input_file = f\"inputs/input_{i}.jsonl\"\n    input_file_s3_location = upload_file(input_file)\n    print(f\"{input_file}을 사용하여 Endpoint를 호출 중\")\n    async_response = predictor.predict_async(input_path=input_file_s3_location)\n    output_location = async_response.output_path\n    print(output_location)\n    inferences += [(input_file, output_location)]\n    time.sleep(0.5)\n\nfor input_file, output_location in inferences:\n    output = get_output(output_location)\n    print(f\"입력 파일: {input_file}, 출력: {output}\")\n```\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference_3.png\" />\n\n## c. AutoScaling 설정\n\n비동기 추론에서 자동 스케일링은 실시간 추론과 마찬가지로 Application AutoScaling을 통해 설정됩니다. 이곳에서의 차이점은 스케일링할 수 있는 새로운 메트릭이 있다는 점입니다.\n\n비동기 추론 내에서 이미 구현된 내부 대기열이 있음을 이해했듯이, 자동 스케일링은 이 대기열에 있는 항목의 수에 따라 확장하거나 축소할 수 있습니다. 이는 CloudWatch 메트릭 \"ApproximateBackLogSize\"로 캡처됩니다. 이 요청은 이미 처리 중인 것이거나 아직 처리되지 않은 것입니다.\n\n<div class=\"content-ad\"></div>\n\nReal-Time Inference와 비슷한 방식으로 Boto3 SDK를 사용하여 정책을 설정했습니다. 최소 인스턴스 수를 0으로 정의했는데, 이 기능은 Asynchronous Inference에서만 지원됩니다.\n\n```js\nclient = boto3.client(\n    \"application-autoscaling\"\n)  # SageMaker를 포함한 다른 서비스의 Application Auto Scaling을 나타내는 일반 클래스\n\nresource_id = (\n    \"endpoint/\" + predictor.endpoint_name + \"/variant/\" + \"AllTraffic\"\n)  # application autoscaling이 엔드포인트를 참조하는 형식\n\n# 비동기 엔드포인트에서 인스턴스 수를 0으로 설정하여 Autoscaling 구성\nresponse = client.register_scalable_target(\n    ServiceNamespace=\"sagemaker\",\n    ResourceId=resource_id,\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n    MinCapacity=0,\n    MaxCapacity=5,\n)\n```\n\n최소 및 최대 인스턴스 수를 지정한 후, 확장 및 축소에 대한 쿨다운 기간을 정의할 수 있습니다. 여기에서 \"MetricName\"을 \"ApproximateBackLogSize\" 메트릭으로 지정하였음을 알려드립니다.\n\n```js\nresponse = client.put_scaling_policy(\n    PolicyName=\"Invocations-ScalingPolicy\",\n    ServiceNamespace=\"sagemaker\",  # 리소스를 제공하는 AWS 서비스의 이름 공간\n    ResourceId=resource_id,  # 엔드포인트 이름\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  # SageMaker는 인스턴스 수만 지원\n    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n    TargetTrackingScalingPolicyConfiguration={\n        \"TargetValue\": 5.0,  # 메트릭의 목표 값 - 여기서 메트릭은 SageMakerVariantInvocationsPerInstance입니다.\n        \"CustomizedMetricSpecification\": {\n            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n            \"Namespace\": \"AWS/SageMaker\",\n            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": predictor.endpoint_name}],\n            \"Statistic\": \"Average\",\n        },\n        \"ScaleInCooldown\": 600,  # 쿨다운 기간은 이전 작업의 영향이 나타나기 전에 추가 인스턴스를 시작하거나 종료하는 것을 방지합니다.\n        \"ScaleOutCooldown\": 100,  # ScaleOutCooldown - 확장 작업 완료 후 다른 확장 작업을 시작하기 전의 시간 간격.\n    },\n)\n```\n\n<div class=\"content-ad\"></div>\n\nAutoScaling을 테스트하려면 일정 기간 동안 요청을 보낼 수 있습니다. 스케일링 정책에 따르면 대상 값은 엔드포인트 뒤에 있는 큐에서 아직 처리 중이거나 처리되지 않은 요청 또는 호출을 5회로 설정됩니다.\n\n```js\nrequest_duration = 60 * 15 # 15분\nend_time = time.time() + request_duration\nprint(f\"{request_duration}초 동안 테스트가 실행됩니다.\")\nwhile time.time() < end_time:\n    predictor.predict(input_data)\n```\n\n일정 시간 동안 요청을 보내지 않은 후에는 인스턴스 수가 제로로 축소되고, 큐가 완전히 비어 있는 것을 주의하세요:\n\n![이미지](/assets/img/2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference_4.png)\n\n<div class=\"content-ad\"></div>\n\n# 3. 추가 자료 및 결론\n\n전체 예제 코드는 위 링크에서 찾을 수 있습니다. SageMaker 비동기 추론은 전적으로 실시간이나 일괄 처리가 아닌 특정 LLM 사용 사례에 사용할 수 있는 기능입니다. 이 기사가 귀하에게 LLM을 규모 확장하여 인퍼런스를 제공하는 다른 방법에 대한 유용한 소개였기를 바랍니다. 이 분야에서 더 많은 콘텐츠를 기대해 주세요!\n\n항상 읽어 주셔서 감사합니다. 읽은 후 의견을 자유롭게 남겨 주세요.\n\n이 기사를 즐겁게 보셨다면 LinkedIn에서 저와 연락하고 Medium 뉴스레터를 구독해 주시기 바랍니다.","ogImage":{"url":"/assets/img/2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference_0.png"},"coverImage":"/assets/img/2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference_0.png","tag":["Tech"],"readingTime":12},{"title":"LLM API 비교 모델별 가격 분석","description":"","date":"2024-06-23 19:58","slug":"2024-06-23-LLMAPIsPriceComparisonbyModel","content":"\n\n<img src=\"/assets/img/2024-06-23-LLMAPIsPriceComparisonbyModel_0.png\" />\n\n이 블로그 게시물은 점점 진화하는 대형 언어 모델(LLM) API 및 관련 가격 구조에 대해 다룹니다.\n\n우리의 목표는 성능이 우수한 LLM 중에서 가장 경쟁력 있는 LLM API 모델을 가격 측면에서 강조하는 지속적으로 업데이트되는 리소스를 만드는 것입니다.\n\n이 게시물을 쉽게 참조하기 위해 즐겨찾기해 두고, 새로운 LLM API 모델을 제안하거나 제공된 정보의 부정확성을 지적해 주십시오.\n\n<div class=\"content-ad\"></div>\n\n비교의 세부 사항에 대해 파헤치기 전에 맥락 개요부터 시작해보죠...\n\n## 모델은 어떻게 선택되나요?\n\n최고의 모델은 대규모 멀티태스크 언어 이해 (MMLU) 벤치마크 및 LMSYS 챗봇 아레나의 리더보드 테이블에서 추출됩니다.\n\n## 제공자는 어떻게 선택되나요?\n\n<div class=\"content-ad\"></div>\n\n일단 모델을 식별하면 다양한 공급업체에서 그 모델을 추적합니다. 여기서는 각 모델에 대한 최상의 가격 옵션을 제시합니다. 추론용 API 공급업체가 없는 경우 목록에서 제거됩니다.\n\n## 가격이 어떻게 계산됩니까?\n\n가격은 토큰당 계산되며, 백만 개 토큰당 미국 달러로 표시되며, 입력 및 출력 토큰 가격을 3대 1의 비율로 섞은 가격입니다.\n\n## 모델별 LLM API 요금 살펴보기\n\n<div class=\"content-ad\"></div>\n\n가장 강력한 LLM 모델들의 표:\n\n이 페이지를 쉽게 참조하기 위해 즐겨찾기를 해두세요. 댓글 섹션에서 기여해도 괜찮아요!","ogImage":{"url":"/assets/img/2024-06-23-LLMAPIsPriceComparisonbyModel_0.png"},"coverImage":"/assets/img/2024-06-23-LLMAPIsPriceComparisonbyModel_0.png","tag":["Tech"],"readingTime":1},{"title":"Transformer 아키텍처 완벽 설명","description":"","date":"2024-06-23 19:57","slug":"2024-06-23-TransformerArchitectureexplained","content":"\n\nTransfomers은 최근에 많은 소음을 일으키고 있는 머신 러닝의 새로운 개발입니다. 그들은 맥락을 잘 추적하는 데 놀라울 정도로 뛰어납니다. 그래서 그들이 쓰는 텍스트가 이치에 맞는 것입니다. 이 장에서는 변압기의 아키텍처와 작동 방식에 대해 알아볼 것입니다.\n\n![이미지](/assets/img/2024-06-23-TransformerArchitectureexplained_0.png)\n\n변압기 모델은 머신 러닝의 가장 흥미로운 새로운 발전 중 하나입니다. Attention is All You Need 논문에서 소개되었습니다. 변압기는 이야기, 수필, 시를 쓰거나, 질문에 답하거나, 다국어 간 번역하거나, 사람들과 대화하거나, 심지어 인간에게 어려운 시험도 패스할 수 있습니다! 하지만 그것들은 무엇인가요? 변압기 모델의 아키텍처는 그다지 복잡하지 않습니다. 그것은 단지 매우 유용한 구성 요소들을 연결한 것으로, 각각의 구성 요소는 자체 기능을 가지고 있습니다. 이 장에서는 이러한 구성 요소들을 모두 배우게 될 것입니다.\n\n요약하면, 변압기는 무엇을 하는 걸까요? 핸드폰에서 텍스트 메시지를 작성하고 있다고 상상해 보세요. 각 단어 뒤에는 세 개의 단어가 제안될 수 있습니다. 예를 들어, \"안녕, 어떻게\"를 입력하면 핸드폰은 \"당신\"이나 \"당신의\"와 같은 단어를 다음 단어로 제안할 수 있습니다. 물론, 핸드폰에서 제안된 단어를 계속 선택하면 이러한 단어로 이루어진 메시지가 이치에 맞지 않음을 빨리 알 수 있습니다. 각각 3개 또는 4개의 연속된 단어 집합을 살펴보면 이치가 맞을 수 있지만, 이러한 단어들은 의미 있는 내용으로 이어지지 않습니다. 이는 핸드폰에서 사용된 모델이 메시지의 전반적인 맥락을 가지고 있지 않기 때문에 발생합니다. 그 모델은 단순히 최근 몇 단어 뒤에 어떤 단어가 더 자주 나올지 예측하는 것입니다. 반면에 변압기는 쓰여지는 내용의 맥락을 추적하고, 그래서 그들이 쓰는 텍스트가 의미가 있는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n휴대폰은 텍스트 메시지에서 사용할 다음 단어를 제안할 수 있지만, 일관된 텍스트를 생성할 수 있는 능력은 없습니다.\n\n![Transformer Architecture](/assets/img/2024-06-23-TransformerArchitectureexplained_1.png)\n\n솔직하게 말하자면, 트랜스포머가 한 번에 한 단어씩 텍스트를 작성한다는 것을 처음 알게 되었을 때, 믿을 수가 없었습니다. 우선, 이것이 인간이 문장을 형성하고 생각하는 방법이 아닙니다. 우리는 먼저 기본적인 생각을 형성한 후, 점차 정제하고 단어를 추가합니다. 이것은 또한 ML 모델이 다른 작업을 하는 방식이 아닙니다. 예를 들어, 이미지는 이렇게 만들어지지 않습니다. 대부분의 신경망 기반의 그래픽 모델은 이미지의 대략적인 버전을 형성하고, 점진적으로 정제하거나 세부 정보를 추가하여 완벽하게 만듭니다. 그렇다면 왜 트랜스포머 모델이 한 단어씩 텍스트를 작성하는 걸까요? 한 가지 답은, 그렇게 하는 것이 정말 잘 작동하기 때문입니다. 더 만족스러운 답은, 트랜스포머가 맥락을 매우 잘 추적하여 다음 단어가 아이디어를 이어나가는 데 정확히 필요한 것이기 때문입니다.\n\n그러면 트랜스포머는 어떻게 훈련되는 걸까요? 많은 양의 데이터로, 사실상 인터넷의 모든 데이터로 말이죠. 그래서 \"안녕, 어떻게\"라는 문장을 트랜스포머에 입력하면, 인터넷의 모든 텍스트를 기반으로 \"당신\"이라는 다음 단어가 가장 좋다는 것을 단순히 알고 있습니다. 좀 더 복잡한 명령을 주면, 예를 들어, \"이야기를 써봐.\" 라고 주면, 좋은 다음 단어로 \"한 번\"을 사용할 수 있음을 알아낼 수도 있습니다. 그러면 이 단어를 명령에 추가하고, 좋은 다음 단어가 \"있던\", 이어갈 다음 단어가 무엇인지 찾아나갑니다. 그리고 한 단어씩, 이렇게 이어가면서 이야기를 써 나갈 것입니다.\n\n<div class=\"content-ad\"></div>\n\n명령: 이야기를 쓰세요\n응답: 한 번에\n\n다음 명령: 이야기를 쓰세요. 한 번에\n응답: 옛날에\n\n다음 명령: 이야기를 쓰세요. 한 번에 옛날에\n응답: 한\n\n다음 명령: 이야기를 쓰세요. 한 번에 옛날에 한\n응답: 시간\n\n<div class=\"content-ad\"></div>\n\n한때 가족들이 모여 있던 어느 편안한 마을에서 다섯 형제가 살았어요. 그들의 이름은 래리, 루이스, 레오나르도, 루이라, 그리고 루카스였죠. 모두가 가난한 환경에서 자라났지만, 그들은 서로를 깊이 사랑하는 특별한 유대 관계를 형성했어요.\n\n하루, 마을에 한 노인이 나타났어요. 그 노인은 썩은 소나무 막대기를 들고 다니며 지쳐 보이더라구요. 형제들은 노인을 도와주기로 했어요. 그들은 함께 얘기를 나누고, 노인에게 식사와 숙소를 제공했어요.\n\n이야기를 나누는 동안, 노인은 금지된 섬의 보물이 있는 곳을 가리키며 이야기를 풀어냈어요. 형제들은 노인을 따라가기로 결심했고, 어려운 여정을 통해 함께 고난과 역경을 극복하며 결국 보물을 찾아냈어요.\n\n그들은 이 경험을 통해 서로의 용기, 협력, 그리고 신뢰를 배웠어요. 이들은 함께한 경험이 더 이상 그들을 나눌 수 없는 특별한 유대 관계를 형성하는 데 도움이 되었죠. 이제 형제들은 당신의 도움이 주변에 필요한 이웃을 발견하고 노인처럼 따뜻한 마음으로 도와줄 준비가 되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-TransformerArchitectureexplained_2.png)\n\n# 토큰화\n\n토큰화는 가장 기본적인 단계입니다. 단어, 구두점 등을 포함한 토큰 데이터 세트로 이루어져 있습니다. 토큰화 단계는 각 단어, 접두사, 접미사, 구두점을 포함하여 해당 라이브러리의 알려진 토큰으로 보냅니다.\n\n![이미지](/assets/img/2024-06-23-TransformerArchitectureexplained_3.png)\n\n\n<div class=\"content-ad\"></div>\n\n# 포함\n\n입력이 토큰화되면 단어를 숫자로 변환해야 합니다. 이를 위해 임베딩을 사용합니다. 이전 장에서 텍스트 임베딩이 모든 텍스트를 숫자 목록으로 변환한다는 것을 배웠습니다. 두 개의 텍스트가 유사하면 해당 벡터의 숫자도 서로 유사합니다(요소별로, 즉 동일한 위치의 각 숫자 쌍이 유사함을 의미). 그렇지 않으면 두 개의 텍스트가 다르면 해당 벡터의 숫자도 다릅니다.\n\n![transformer](/assets/img/2024-06-23-TransformerArchitectureexplained_4.png)\n\n# 위치 인코딩\n\n<div class=\"content-ad\"></div>\n\n문장의 각 토큰에 해당하는 벡터를 가지고 나면, 다음 단계는 이 모든 것을 하나의 벡터로 결합하는 것입니다. 여러 벡터를 하나의 벡터로 변환하는 가장 일반적인 방법은 각 요소를 더하는 것입니다. 즉, 각 좌표를 따로 더합니다. 예를 들어, 길이가 2인 벡터 [1,2]와 [3,4]가 있다면, 이에 해당하는 합은 [1+3, 2+4]로 [4, 6]이 됩니다. 이 방법은 작동할 수 있지만 한 가지 주의할 점이 있습니다. 덧셈은 교환법칙이 성립하므로, 숫자를 다른 순서로 더하더라도 동일한 결과를 얻을 수 있습니다. 이 경우 \"I'm not sad, I'm happy\"와 \"I'm not happy, I'm sad\"는 같은 결과 벡터를 얻게 됩니다. 이러한 경우는 바람직하지 않습니다. 그래서 두 문장에 대해 서로 다른 벡터를 얻기 위한 방법을 고안해야 합니다. 여러 방법이 있지만, 우리는 그 중 하나를 선택할 것입니다: 위치 인코딩(Positional Encoding). 위치 인코딩은 단어의 임베딩 벡터에 미리 정의된 벡터 시퀀스를 추가하는 것으로, 이를 통해 각 문장에 대해 고유한 벡터를 얻고 같은 단어가 다른 순서로 나타난 문장에는 서로 다른 벡터가 할당됩니다. 아래 예시에서 단어 \"Write\", \"a\", \"story\", \".\"에 해당하는 벡터가 각 단어의 위치 정보를 담고 있는 수정된 벡터로 변환됩니다. \"Write (1)\", \"a (2)\", \"story (3)\", \". (4)\"으로 레이블이 지정됩니다.\n\n![Transformer Architecture Explained](/assets/img/2024-06-23-TransformerArchitectureexplained_5.png)\n\n# 트랜스포머 블록\n\n지금까지의 내용을 요약해보겠습니다. 단어가 들어오면 토큰으로 변환되고(tokenization), 토큰화된 단어들은 숫자로 변환됩니다(임베딩), 그리고 순서가 고려됩니다(위치 인코딩). 이렇게 모델에 입력되는 각 토큰에 대해 하나의 벡터가 생성됩니다. 이제 다음 단계는 이 문장에서 다음 단어를 예측하는 것입니다. 이 작업은 실제로 매우 큰 신경망으로 수행되며, 해당 목표에 정확하게 훈련된 모델을 사용하여 이 문장에서 다음 단어를 예측합니다.\n\n<div class=\"content-ad\"></div>\n\n큰 신경망을 교육할 수는 있지만, 주요 단계인 주의 메커니즘을 추가함으로써 크게 개선할 수 있습니다. Attention is All you Need 논문에서 소개된 이 주의 메커니즘은 트랜스포머 모델의 주요 구성 요소 중 하나로, 그들이 잘 동작하는 이유 중 하나입니다. 주의는 이전 섹션에서 설명되었지만, 지금은 이것을 텍스트의 각 단어에 맥락을 추가하는 방법으로 상상해 보세요.\n\n주의 구성 요소는 피드포워드 신경망의 각 블록에 추가됩니다. 따라서 다음 단어를 예측하는 것을 목표로 하는 대형 피드포워드 신경망을 상상해보면, 여러 개의 작은 신경망 블록으로 구성된 것이라고 생각할 수 있습니다. 각 블록에 주의 구성 요소가 추가됩니다. 각 트랜스포머 구성 요소는 트랜스포머 블록이라고 불리고 주로 다음 두 가지 구성 요소로 형성됩니다:\n\n- 주의 구성 요소.\n- 피드포워드 구성 요소.\n\n![image](/assets/img/2024-06-23-TransformerArchitectureexplained_6.png)\n\n<div class=\"content-ad\"></div>\n\n# 주의\n\n다음 단계는 주의입니다.  어텐션 메커니즘은 매우 중요한 문제, 즉 문맥의 문제를 다룹니다. 가끔은 동일한 단어라고 하더라도 다른 의미로 사용될 수 있습니다. 이것은 임베딩이 단어를 벡터로 보내기만 하고 사용 중인 단어의 정의가 무엇인지 알지 못하기 때문에 언어 모델을 혼란스럽게 만듭니다.\n\n어텐션이라는 기술은 언어 모델이 문맥을 이해하는 데 도움이 되는 매우 유용한 기술입니다. 어텐션이 어떻게 작동하는지 이해하기 위해 다음 두 문장을 살펴보겠습니다:\n\n- 문장 1: 강의 둑.\n- 문장 2: 은행에 있는 돈.\n\n<div class=\"content-ad\"></div>\n\n안녕하세요! '은행'이라는 단어가 두 번 나오지만 다른 의미를 갖고 있음을 알 수 있습니다. 제1문장에서는 강가의 땅을 가리키고, 두 번째 문장에서는 돈을 보관하는 기관을 가리키고 있죠. 컴퓨터는 이를 모르기 때문에 이 지식을 주입해야 합니다. 그럼 무엇이 도움이 될까요? 문장 안의 다른 단어들이 우리 구조에 도움이 될 수 있습니다. 첫 번째 문장에서는 'the'나 'of'는 도움이 되지 않지만, 'river'라는 단어는 강가의 땅을 언급하고 있음을 알려줍니다. 마찬가지로, 두 번째 문장에서는 'money'라는 단어가 우리에게 돈을 보관하는 기관을 가리킨다는 것을 이해할 수 있게 해줍니다.\n\n요약하자면, 어텐션은 문장(또는 텍스트 조각) 안의 단어들을 단어 임베딩에서 가깝게 이동시키는 역할을 합니다. 이렇게 하면 \"은행에 돈\"이라는 문장에서 \"은행\"이 \"돈\"이라는 단어와 가까워집니다. 마찬가지로, \"강가의 은행\"이라는 문장에서 \"은행\"은 \"강\"이라는 단어와 가까워집니다. 이렇게 두 문장 각각의 수정된 \"은행\"은 주변 단어의 일부 정보를 함께 전달하여 맥락을 추가합니다.\n\n트랜스포머 모델에서 사용되는 어텐션 단계는 실제로 훨씬 강력하며, 멀티헤드 어텐션이라고 불립니다. 멀티헤드 어텐션에서는 여러 다른 임베딩이 사용되어 벡터를 수정하고 맥락을 추가합니다. 멀티헤드 어텐션은 언어 모델이 텍스트를 처리하고 생성할 때 효과적인 수준으로 도달하도록 도와주었습니다.\n\n<div class=\"content-ad\"></div>\n\n# 소프트맥스 레이어\n\n이제 여러 개의 변형 블록으로 구성된 변압기 계층들이 형성됨을 알게 되었으니, 각각이 주의 및 피드포워드 레이어를 포함하는 많은 계층의 변압기를 문장에서 다음 단어를 예측하는 대규모 신경망으로 생각할 수 있습니다. 변압기는 모든 단어에 대한 점수를 출력하며, 문장에서 다음으로 가장 가능성이 높은 단어들에 가장 높은 점수를 부여합니다.\n\n변압기의 마지막 단계는 소프트맥스 레이어로, 이 점수를 확률로 변환(합이 1이 되는)하여 가장 높은 점수가 가장 높은 확률에 해당하도록 합니다. 그런 다음 이러한 확률 중에서 다음 단어를 샘플링할 수 있습니다. 아래 예시에서 변압기는 \"Once\"에 0.5의 가장 높은 확률을 부여하고 \"Somewhere\"에는 0.3, \"There\"에는 0.2의 확률을 부여합니다. 한 번 샘플링하면 단어 \"once\"가 선택되어 변압기의 출력이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n자 이제 뭐 할까요? 우리는 단계를 반복해요. 이제 텍스트 \"한 이야기를 써보세요. 한 번\"을 모델에 입력하면, 아마도 출력은 \"upon\"이 될 거에요. 이 단계를 계속 반복하면, 변환기는 \"한 번에 어느 날, ... 이었어요.\"와 같은 이야기를 쓰게 될 거에요.\n\n## 훈련 후\n\n이제 변환기가 어떻게 작동하는지 알았으니, 아직 해야 할 일이 있어요. 다음을 상상해보세요. 변환기에게 \"알제리의 수도는 무엇인가요?\"라고 물어봤을 때, \"알제\"라고 대답하고 넘어가길 원할 거에요. 그러나 변환기는 전체 인터넷으로 훈련받았어요. 인터넷은 큰 공간이고, 그것이 반드시 최선의 질문/답변 저장소라는 보장은 없어요. 예를 들어 많은 페이지가 답변이 없는 질문 목록을 갖고 있을 수 있어요. 이 경우 \"알제리의 수도는 무엇인가요?\" 다음 문장에는 \"알제리의 인구는 어떻게 되나요?\"나 \"부르키나파소의 수도는 무엇인가요?\"와 같은 다른 질문이 나올 수 있어요. 변환기는 자신의 응답을 고려하는 인간이 아니에요, 단지 인터넷(또는 제공된 데이터셋)에서 본 것을 모방할 뿐이에요. 그렇다면 변환기에게 질문에 대답하게 하려면 어떻게 해야 할까요?\n\n답은 후훈련(post-training)에 있어요. 사람에게 특정 작업을 가르치듯이, 변환기에게도 작업을 수행하도록 시킬 수 있어요. 변환기가 전체 인터넷에 훈련을 받은 후, 많은 질문과 그에 해당하는 답변의 대형 데이터셋으로 다시 훈련받아요. 변환기(사람과도 같이)는 학습한 마지막 것에 편향을 가지기 때문에 후훈련은 변환기가 요청받은 작업들에 성공하는 데 매우 유용한 단계로 입증되었어요.\n\n<div class=\"content-ad\"></div>\n\n포스트 트레이닝은 많은 다른 작업에도 도움이 됩니다. 예를 들어, 대화 데이터셋을 사용하여 트랜스포머를 포스트 트레이닝하면 챗봇으로 잘 작동하도록 돕거나, 이야기, 시, 코드를 작성하는 데 도움을 줄 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-23-TransformerArchitectureexplained_0.png"},"coverImage":"/assets/img/2024-06-23-TransformerArchitectureexplained_0.png","tag":["Tech"],"readingTime":8}],"page":"7","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true}