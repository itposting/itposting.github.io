{"pageProps":{"posts":[{"title":"사이드 허슬 구축을 위한 10가지 ChatGPT 프롬프트 템플릿","description":"","date":"2024-06-23 19:14","slug":"2024-06-23-10ChatGPTPromptTemplatesForBuildingSideHustles","content":"\n\n요즘 사람들은 부가 수입을 원합니다.\n\n하지만 대부분의 사람들은 개인적인 이유나 업무로 인해 실패합니다.\n\n여기 부가 수입을 구축하는 데 도움이 되는 10가지 ChatGPT 프롬프트 템플릿이 있습니다:\n\n![ChatGPT Prompt Templates](/assets/img/2024-06-23-10ChatGPTPromptTemplatesForBuildingSideHustles_0.png)\n\n<div class=\"content-ad\"></div>\n\n수익성 높은 부업 기회를 식별하기\n\n당신은 전문 비즈니스 코치입니다. 저는 [바로 문제에 대한 자세한 배경 설명을 포함하여 직면한 문제를 언급합니다]. [특정 산업 또는 분야]에서 수익성 있는 부업 기회를 찾도록 도와주실 수 있나요? 각 기회마다 잠재적인 수익, 필요한 기술, 그리고 초기 투자 비용을 상세히 나열해주시기 바랍니다. 결과물이 [자세한 예를 들어 어떻게 되었으면 하는지를 언급합니다].\n\n시장 조사 및 검증\n\n당신은 전문 비즈니스 코치입니다. 저는 [바로 문제에 대한 자세한 배경 설명을 포함하여 직면한 문제를 언급합니다]. [특정 시장 또는 분야]에서 부업 아이디어를 검증하고 싶습니다. 시장 조사를 수행하는 단계적인 프로세스를 개요하고, 분석해야 할 주요 지표, 데이터 수집 방법, 그리고 결과를 해석하는 방법을 설명해주실 수 있을까요? 결과물이 [자세한 예를 들어 어떻게 되었으면 하는지를 언급합니다].\n\n<div class=\"content-ad\"></div>\n\n비즈니스 계획 수립하기\n\n당신은 전문 비즈니스 코치입니다. 저는 [배경 내용과 함께 직면한 문제를 자세히 언급하세요]. 나의 부업 아이디어인 [아이디어 간단한 설명]에 대한 포괄적인 비즈니스 계획을 작성하는 데 도와주세요. 시장 분석, 경쟁 환경, 마케팅 전략, 재무 계획 및 운영 계획 섹션을 포함해 주세요. 출력물은 [예시를 들어 자세히 설명하세요] 해 주시기 바랍니다.\n\n온라인 존재감 확립하기\n\n당신은 전문 비즈니스 코치입니다. 저는 [배경 내용과 함께 직면한 문제를 자세히 언급하세요]. [특정 산업이나 특정 분야]에서 나의 부업을 위해 강력한 온라인 존재감을 구축하는 방법은 무엇인가요? 웹사이트 설정, SEO 최적화, 소셜 미디어 프로필 작성 및 온라인 커뮤니티 구축 전략에 대한 상세 가이드를 제공해주세요. 출력물은 [예시를 들어 자세히 설명하세요] 해 주시기 바랍니다.\n\n<div class=\"content-ad\"></div>\n\n가격 전략과 이윤 마진\n\n당신은 전문 비즈니스 코치입니다. [면책 조항으로 자세한 문제를 설명하십시오]. 나는 사이드 허슬 [제품/서비스 설명]에 대한 가격 전략을 개발하는 데 도움이 필요합니다. 원가, 경쟁사 가격, 인식된 가치, 그리고 이윤 마진과 같은 요소를 고려한 가격 설정에 대해 자세한 방법을 제시해 줄 수 있나요? 결과물을 [예시와 함께 자세히 원하는 방법을 언급하십시오].\n\n마케팅 및 홍보\n\n당신은 전문 비즈니스 코치입니다. [면책 조항으로 자세한 문제를 설명하십시오]. 사이드 허슬 [제품/서비스 설명]를 [특정 대상 고객군을 명시하십시오]에게 홍보하기 위한 효과적인 마케팅 전략을 제안해 줄 수 있나요? 온라인과 오프라인 전술을 모두 포함하며 비용 효율적인 방법과 효과 측정 방법에 중점을 두어 주실 수 있나요? 결과물을 [예시와 함께 자세히 원하는 방법을 언급하십시오].\n\n<div class=\"content-ad\"></div>\n\n시간 관리와 생산성 관리\n\n당신은 전문 비즈니스 코치입니다. 저는 [배경 내용과 함께 직면한 문제에 대해 자세히 언급합니다]. 본업과 부업을 균형있게 유지하면서 효과적인 시간 관리와 생산성 전략은 무엇일까요? 제 스케줄을 조직화하고 우선순위를 설정하며 업무-생활 균형을 유지하기 위한 상세한 계획을 제시해 주세요. 저는 [예시와 함께 원하는 결과물에 대해 자세히 언급해 주세요].\n\n부업 키우기\n\n당신은 전문 비즈니스 코치입니다. 저는 [배경 내용과 함께 직면한 문제에 대해 자세히 언급합니다]. [비즈니스를 설명하는 부업]을 확장하여 더 많은 수익을 창출하고 싶습니다. 제품 라인 또는 서비스 확장, 새로운 시장 진출, 자동화 도구, 그리고 고용 또는 외주화 작업을 포함한 상세한 성장 전략을 개요로 제시해 주실 수 있나요? 저는 [예시와 함께 원하는 결과물에 대해 자세히 언급해 주세요].\n\n<div class=\"content-ad\"></div>\n\n법적 및 재무 고려 사항\n\n당신은 전문 비즈니스 코치입니다. 저는 [배경 문맥과 함께 직면한 문제를 자세히 언급합니다]. [특정 산업이나 지역]에서 부업을 시작할 때 알아야 할 주요 법적 및 재무 고려 사항은 무엇인가요? 사업 구조, 세금, 허가, 계약 및 장부관리에 대해 자세한 정보를 제공해주세요. 제가 바라는 것은 [예시를 포함하여 자세히 언급].\n\n기술과 도구 활용\n\n당신은 전문 비즈니스 코치입니다. 저는 [배경 문맥과 함께 직면한 문제를 자세히 언급합니다]. 내 부업 [비즈니스 설명] 운영을 간소화하는 데 도움이 될 수 있는 기술 도구 및 플랫폼을 추천해주실 수 있나요? 프로젝트 관리, 고객 관계 관리 (CRM), 회계 및 온라인 마케팅을 위한 도구에 대해 자세한 설명을 제공해주세요. 제가 바라는 것은 [예시를 포함하여 자세히 언급].\n\n<div class=\"content-ad\"></div>\n\n이 내용이 도움이 되었으면 좋겣네요.\n\n좋은 하루 보내세요!","ogImage":{"url":"/assets/img/2024-06-23-10ChatGPTPromptTemplatesForBuildingSideHustles_0.png"},"coverImage":"/assets/img/2024-06-23-10ChatGPTPromptTemplatesForBuildingSideHustles_0.png","tag":["Tech"],"readingTime":3},{"title":"프롬프트 엔지니어링은 잊어라, ChatGPT가 완벽한 프롬프트를 작성해준다","description":"","date":"2024-06-23 19:13","slug":"2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou","content":"\n\n![This is an image](/assets/img/2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou_0.png)\n\n저는 오랫동안 프롬프팅을 해 왔는데, 솔직히 말하면 지칩니다. 제가 신경 쓰고 싶은 건 결과물뿐인데 말과 구조를 고민해야 하니 짜증나요. 훌륭한 프롬프트를 작성하려는 반복적인 과정은 힘들고 시간이 많이 듣게돼서, 지금은 변화할 때가 되었다고 생각했습니다.\n\n프롬프트 엔지니어링 기술은 본질적으로 언어 작업이라는 것을 깨달았죠. ChatGPT는 언어의 대가이기 때문에, 그냥 ChatGPT를 프롬프트 엔지니어로 만들면 어떨까요?\n\n이제 ChatGPT가 1년 넘게 널리 사용되어왔기 때문에, 출력물의 품질을 향상시키는 데 입증된 존경받는 프롬프팅 기술이 많이 있습니다. 저는 이러한 기술 중 일부를 저 자신의 배운 전략과 결합하여 여러분을 전문가로 만들어주는 GPT를 만들었습니다.\n\n<div class=\"content-ad\"></div>\n\n## ChatGPT Prompt Engineer 작업 방법\n\n최고의 프롬프트 작성자를 만들기 위해 프롬프트 작업을 조금 해봤어요. 다소 고차원적인 관점에서, 나는 GPT에게 프롬프트 마법사로 변신하는 과정을 알려주었어요:\n\n- 사용자의 프롬프트를 이해하기\n- 프롬프트 규칙을 준수하기 위해 사용자로부터 필요한 모든 정보를 수집하기 위해 사용자에게 질문하기 (아래에서 찾을 수 있어요)\n- 이 프롬프트가 복수 단계 프롬프트 또는 단일 프롬프트로 이익을 얻을 수 있는지 평가하기\n- 프롬프트 규칙을 사용하여 프롬프트 (또는 프롬프트 체인) 재작성하기\n\n이 과정 이후에, 우리는 새로운 ChatGPT 인스턴스에 출력된 프롬프트를 삽입하여, 와, 우리는 우수하게 최적화된 결과물을 얻을 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n## 유도 규칙\n\n위에서 말했듯이, ChatGPT에 제공한 유도 규칙은 존경받는 연구 기반의 가장 좋은 실천법과 제 개인적인 특별한 비법을 적절히 결합한 것입니다. 이러한 규칙 중 많은 것들이 Sheila Teo의 놀라운 글에서 영감을 받았습니다.\n\n자신의 프롬프트를 작성하는 데 흥미가 있는 경우, 아래의 전체 유도 규칙 목록을 확인해보세요!\n\n1. 항상 COSTAR 프롬프트 프레임워크를 사용하세요:\n   - **C (문맥)**: 작업에 대한 필수적인 배경 정보나 설정을 제공하세요. 이를 통해 LLM이 다루는 특정 시나리오나 도메인을 이해하고, 관련성 높은 응답을 더 많이 얻을 수 있습니다.\n   - **O (목적)**: 프롬프트의 목적이나 목표를 명확하게 표현하세요. LLM이 이 특정 목표를 이루도록 하면서 자신의 초점이 이 목표에 머물도록 보장하세요.\n   - **S (스타일)**: 응답의 원하는 스타일을 정의하세요. 이는 특정 직업군(과학자 또는 저널리스트 등)의 문체를 모방하거나, 형식적 보고서나 창작적 소설 같은 특정 장르의 서술 톤을 모방하는 것까지 다양할 수 있습니다.\n   - **T (톤)**: 응답의 감정적이거나 태도적인 색조를 결정하세요. 형식적이든, 캐주얼하든, 열정적이든, 공감적이든, 톤을 설정하여 응답이 의도한 감정과 일치하도록 보장하세요.\n   - **A (대상)**: 응답이 의도한 대상 독자를 식별하세요. 전문가, 초보자 또는 일반 독자 등에 따라 LLM의 응답의 콘텐츠와 복잡성을 조정하여 이해와 참여를 보다 향상시킬 수 있습니다.\n   - **R (응답 형식)**: 응답이어야 하는 형식을 명시하세요. 목록, 구조화된 보고서, JSON 객체, 서술 등이 될 수 있습니다. 형식을 정의하면, 이후 분석, 표현 또는 추가 처리 등에 적합한 응답을 생성하는 데 도움이 됩니다.\n\n2. 복잡한 작업을 간단한 대화식 대화의 일련의 단순한 프롬프트로 나누세요.\n3. '하다(do)'와 같은 긍정적 명령을 사용하며, '하지 마라(don't)'와 같은 부정적 언어는 피하세요.\n4. 예시 중심의 유도 사용하기 (Few-shot prompting 사용).\n5. 다음 표현 사용하기: \"당신의 작업은\"과 \"당신은 꼭\".\n6. \"단계별로 생각해보기\"와 같이 선행 단어 항상 사용하기.\n7. 모델에게 역할을 할당하기, 즉 \"당신은 전문가 ___\"라고 명시하기.\n8. 프롬프트 내에서 특정 단어나 구가 반복되도록 하기.\n9. 가능한 경우 사고 연쇄 유도 (CoT)를 유도하여 LLM이 각 단계를 더 깊게 파고들도록 하기.\n10. 출력 초기화자 사용하기, 프롬프트 끝에 기대되는 응답의 시작을 포함하여 프롬프트를 마무리하기. 출력 초기화자를 활용하여 프롬프트를 시작하는 것으로 응답을 예상하는 응답을 마무리하세요.\n11. 상세하게 서술된 에세이/텍스트/단락/기사 또는 어떠한 유형의 텍스트를 작성하려면: \"세부적으로 [주제]에 대해 초상세하게 [에세이/텍스트/단락]를 작성해주세요. 필요한 모든 정보를 추가하여 상세히 작성하세요\".\n\n<div class=\"content-ad\"></div>\n\n## The ChatGPT Prompt Engineer in action\n\n이제 공유 가능한 GPT를 가지고 있으므로, 나만의 맞춤형 GPT 프롬프트 엔지니어를 여러분과 공유할 수 있습니다. 이 과정을 따릅니다.\n\n[여기에서 찾을 수 있어요.](link)\n\n여러분들이 여러분만의 GPT 인스턴스에서 이를 사용하고 싶다면, 단순히 위에서 제시된 가이드라인과 프롬프트 규칙을 복사하여 ChatGPT 대화 중 어떤 것이든 쉽게 프롬프트 엔지니어링 세션으로 변환할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n요즘은 작업 방식을 보여드리겠습니다. 먼저, 프롬프트를 요청할 내용을 생각해야 합니다. 이 경우 GPT가 조금 창의적이어야 하는 것을 시도해 봅시다. 여기 내 아이디어의 시작점입니다. 절로 아름다운 프롬프트로 변할 것입니다:\n\n![Starting Idea](/assets/img/2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou_1.png)\n\n이제 프롬프트 위자드가 제 프롬프트를 다시 쓸 것입니다:\n\n![Prompt Wizard](/assets/img/2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou_2.png)\n\n<div class=\"content-ad\"></div>\n\n이 텍스트를 번역하면 다음과 같습니다.\n이 프롬프트는 잘 쓰여져 있어서 좋은 결과를 얻기에 좋습니다. 다음 단계는 이 프롬프트를 새로운 채팅에 전달하는 것입니다:\n\n\n![이미지](/assets/img/2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou_3.png)\n\n\n전체 응답을 스크린샷에 담지 못했지만, 충분히 좋아서 공유해야겠다고 생각했습니다:\n\n## 이 시스템을 사용해야 하는 이유\n\n<div class=\"content-ad\"></div>\n\n이 시스템이 당신의 ChatGPT 생산성을 향상시키는 데 여러 가지 이유가 많이 있습니다. 여기에 몇 가지를 나열해 보겠습니다:\n\n- 이 시스템은 모든 종류의 프롬프트와 모든 종류의 요청에 대해 작동합니다.\n- “반복 시간”을 많이 절약해줍니다 (즉, ChatGPT가 완전히 다른 방향으로 나가고 당신이 그것을 올바른 방향으로 이끄는 데 절망적인 시간을)\n- ChatGPT에 충분한 정보를 제공하도록 강제합니다 (GPT가 당신에게 질문하는 질문에 답변합니다).\n- GPT를 선언적이고 제안적이 아닌 방식으로 만듭니다 (ChatGPT는 더 정의된 프롬프트로 지배하고 더 결단적으로 행동합니다).\n- 최고의 프롬프트 관행을 가르쳐 줍니다.\n\n이 글이 당신의 프롬프트 엔지니어링 여정에서 도움이 되고 시간을 많이 절약할 수 있기를 바랍니다.\n\n읽어 주셔서 감사합니다!\n\n<div class=\"content-ad\"></div>\n\n- 조던","ogImage":{"url":"/assets/img/2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou_0.png"},"coverImage":"/assets/img/2024-06-23-ForgetPromptEngineeringChatGPTCanWritePerfectPromptsforYou_0.png","tag":["Tech"],"readingTime":4},{"title":"인간의 모든 것을 의인화하는 습관","description":"","date":"2024-06-23 19:12","slug":"2024-06-23-Ourhumanhabitofanthropomorphizingeverything","content":"\n\n\n![image](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_0.png)\n\nPeople tend to anthropomorphize everything. We attribute human traits and emotions to animals, objects, and even software.\n\n\"Today Gmail is being a little moody.\"\n\n\"I think my cat purposely vomited on the rug to get back at me.\"\n\n\n<div class=\"content-ad\"></div>\n\n\"Siri 가 가끔 바보같이 동작하기도 해.\"\n\n사실 동물 행동과 인간 행동이 항상 일치하지는 않습니다. 소프트웨어와 AI는 \"행동\"하지 않고, 대신에 그들의 코드에 따라 작동합니다. 사회적 동물로서, 우리는 특정 출력을 \"행동\"으로 해석하기 쉽게 찾습니다. 우리가 사용하는 기술을 인간화시키면 더 이해하기 쉬워집니다.\n\n그러나 사물을 인간화하는 것이 잘못될 수 있습니다. AI와 같은 복잡한 시스템들을 이해하기 쉽게 만든다는 목적보다, 기술을 인간화시키는 것은 실제로 더 큰 이해 부족과 오해를 야기할 수 있습니다.\n\n![이미지](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_1.png)\n\n<div class=\"content-ad\"></div>\n\n# 인격화란 무엇인가요?\n\nChatGPT의 질적 가용성 연구 중에, Nielsen Norman 그룹은 사용자 행동의 네 가지 패턴을 관찰했습니다. 이 패턴들은 AI에 인간 특성을 할당하는 것입니다.\n\n- 예의\n- 강화\n- 놀이\n- 동반자\n\n![이미지](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_2.png)\n\n<div class=\"content-ad\"></div>\n\n## 친절함\n\n대부분의 사람들은 기본적인 예의로 AI를 대하곤 합니다. \"부탁합니다\"나 \"고마워요\"는 필수는 아니지만, 습관적으로 사용자들은 대화를 공손하게 이끌어내곤 합니다. Siri와 같은 음성 비서는 대화 가능하도록 디자인되어 있으며, 대화는 일반적으로 쿼리에 대한 응답 후의 \"고마워요\"와 같은 사회적 예의를 포함합니다. Siri는 우리가 \"그녀\"에게 도움을 주셨을 때 감사의 말을 하지 않아도 불평하지는 않지만, 우리의 사회적 습관 때문에 일반적인 예의가 표현됩니다.\n\n![이미지](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_3.png)\n\n## 강화\n\n<div class=\"content-ad\"></div>\n\nNielsen Norman Group은 reinforcement(강화)을 정의할 때 챗봇이 정확한 대답을 할 때 칭찬하거나 훈육하는 것으로 설명합니다. 인간에게는 긍정적인 reinforcement(강화)가 중요하다는 것을 알고 있습니다 — 바라는 행동이나 결과를 확립시킵니다. 좋은 성적을 칭찬하고 훌륭한 작업에 대해 상을 주는 것과 같습니다. 나쁜 행동을 꾸짖고 실수를 바로잡으려고 합니다.\n\n그러나 그것은 인간 행동입니다. AI 챗봇에게 긍정적이거나 부정적인 reinforcement(강화)를 제공하는 이유는 무엇일까요? 연구에서 참가자들은 \"잘 했어요!\"라고 칭찬하는 것에 대해 두 가지 다른 동기를 설명했습니다.\n\n- 긍정적인 reinforcement(강화)가 미래에 비슷한 결과를 재창출하는 데 도움이 될 것으로 생각되어 AI에게 “잘한 일”을 보여줌으로써 알려줄 수 있다.\n- AI는 인간의 태도와 행동을 반영하므로 사용자로서 긍정적이면 챗봇도 긍정적이고 친근한 인터페이스가 될 것입니다.\n\n이것은 일반적인 예의를 넘어서는 것이지만, 여전히 사회화 과정에서 형성된 습관으로 해석될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 롤플레이\n\n![Roleplay](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_4.png)\n\n롤플레이는 제품 사용자가 ChatGPT와 같은 제품의 봇에게 특정 역할을 맡도록 요청하는 경우 발생합니다. 예를 들어, 사용자는 ChatGPT에게 \"화기발랄한 소셜 미디어 매니저 역할을 맡고 새로운 게임 출시를 위한 뉴스레터를 작성해 달라\"고 요청할 수 있습니다.\n\nNielsen Norman에 따르면, \"대화 봇에 역할을 할당하는 것은 자주 권장되는 프롬프트 엔지니어링 전략 중 하나입니다.\" 롤플레이 프롬프트는 인공지능에게 소셜 미디어 매니저와 같은 직책이나 환희한(happy)과 같은 태도와 같은 인간적인 특성을 가정하도록 요청합니다. 이는 제품의 입체적인 의인화인데, 그것이 때로는 과제가 요구하는 것일 수도 있습니다. 사용자의 요구를 충족시키기 위해, 인공지능은 도구(tool)보다는 동료로서 더 많이 행동해야 할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 동반자\n\n![이미지](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_5.png)\n\n동반자심은 사용자가 AI를 동료나 사람처럼 대하기 시작하는 지점입니다. 사용자들은 AI를 친구로 취급하며 예의 바르고 애정을 담아 대화를 나눕니다. 이는 사용자가 반드시 AI가 공감과 친절 같은 인간적인 특성을 가지고 있다고 믿는다는 뜻은 아닙니다. 그러나 ChatGPT와 같은 챗봇은 사용자의 입력 스타일을 대부분 따르기 때문에 친절하게 대하면 친절한 답변을 받을 수 있습니다.\n\nAI와 친근하게 대화하는 것은 외로움을 조금 완화시키고 독자가 픽셔널 캐릭터를 즐기는 것과 비슷한 위로를 줄 수 있습니다. 그들이 \"실제\"가 아니더라도 AI와 긍정적인 상호작용에서 느껴지는 감정은 실제입니다.\n\n<div class=\"content-ad\"></div>\n\n# 모든 것을 의인화하는 이유는 무엇일까요?\n\n![image](https://miro.medium.com/v2/resize:fit:1400/0*RcwtzrUztbWi9nve.gif)\n\n왜 AI에게 인간처럼 말을 걸까요? 우리는 AI에게 특정한 방식으로 행동하길 원할까요? 그것이 더 인간적이기를 원할까요? 아니면 우리는 그 특성을 그냥 부여하는 걸까요?\n\n이전에 언급한 것처럼, 인간들은 비인간적인 동물과 물체에 인간적인 특성을 부여하여 인간적 시각을 통해 그들을 이해하려고 합니다. AI는 특히 신비로운 존재이므로, 우리는 그 기술을 해소하기 위해 우리가 할 수 있는 일을 합니다.\n\n<div class=\"content-ad\"></div>\n\n인공지능이 어떻게 작동하는지에 대한 기본적인 오해가 기술에 대한 의도를 흔들어놓을 수 있습니다. Nielsen Norman의 연구에 따르면 참가자들은 ChatGPT와 같은 플랫폼과 어떻게 상호 작용해야 하는지 확실하지 않았으며 다른 소스에서 들은 내용에 따라 행동합니다. \"따라서, AI가 가장 잘 작동하는 방법에 대한 소문이 퍼지는데, 이 중 많은 것들이 인간화의 정도를 포함하고 있습니다.\"\n\n이제 우리가 사람들이 왜 인공지능을 인간화한 채로 다가오는지, 즉 주로 인간화를 고려한 상태에서 다가오는지 알았으니, 우리는 인공지능을 인간화하는 방향으로 나아갈지에 대해 생각해 볼 필요가 있을까요? 사용자들에게 ChatGPT와 같은 AI와 대화할 때 동료나 친구와 대화하는 것처럼 대화를 나누도록 격려해야 할까요?\n\n그렇지 않습니다. AI를 \"마법 같은\" 것으로 표현하는 것이 문제가 될 수 있는 것과 마찬가지로, 인간처럼 다뤄서는 안 되며 그렇게 한다면 당황과 오해를 불러일으킬 것입니다.\n\n# 인간화는 해결책이 아닙니다\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_6.png)\n\n고양이의 행동을 \"교묘한 동기\"로 설명하면 고양이가 꾀를 부리거나 악의를 품고 있다고 생각하는 오해를 낳을 수 있습니다. (가끔 그렇게 보일 수 있지만, 사실은 아닙니다!) 동물 행동을 연구하고 넘어서서 우리는 이것이 사실이 아님을 알고 있습니다. 다시 말해, 동물들은 본능에 따라 행동합니다. 인간들은 본능과 사회적 기대에 따라 행동합니다. ChatGPT와 같은 디지털 제품은 그 코드에 기반하여 기능합니다.\n\n따라서 AI를 사람처럼 인식하는 것은 최선의 경우 오해를 낳을 수 있고, 최악의 경우 전혀 잘못된 이해를 야기할 수 있습니다. AI에 인간적인 특성을 부여함으로써 사람들은 그 작동 방식에 대한 잘못된 생각을 형성할 수 있습니다. 이해하기 어렵게 만들기가 목표라면 (그렇게하면 좋습니다), AI를 인간화하는 것은 해결책에 반대로 작용합니다.\n\nRaspberry Pi는 인간화 언어를 사용하지 않고 AI를 어떻게 언급해야 하는지 몇 가지 예를 제시합니다.\n\n\n<div class=\"content-ad\"></div>\n\n- “It listens/it learns” → “AI is designed to.../AI 개발자들은 ...어플리케이션을 만듭니다…”\n\n이로 인해 AI를 독립적인 존재로서가 아닌 특정 용도를 위해 인간에 의해 설계된 기술로서의 존재로 초점이 옮겨집니다.\n\n- “see/look/create/recognize/make” → “detect/input/pattern match/generate/produce”\n\n처음에 나열된 동사 목록은 사람에게도 적용될 수 있어 AI에 인간적인 품질을 내포하고 있음을 시사합니다. “look” 대신 “detect”와 같은 더 정확한 언어는 AI를 엔티티보다는 기술로서 확립하는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n- \"인공 지능/기계 학습\"과 같은 용어를 셀 수 있는 명사로 사용하지 마세요. 예: \"2022년에 새로운 인공 지능이 나타났다\" → '인공 지능/기계 학습'을 생물학 용어처럼 과학적 분야로 참조하세요.\n\n이렇게 하면 AI/ML이 사람들에 의해 개발된 것이라는 사실에 근거를 두어 자기 자신에서 출현한 힘이나 자체적인 동기가 아니라는 것을 보여줍니다.\n\n# 인간화에 더 초점을 맞추기\n\n![이미지](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_7.png)\n\n<div class=\"content-ad\"></div>\n\n하지만 기다려봐요! AI 챗봇에 인격을 부여하는 기업에 대해 어떻게 생각하시나요? Meta는 챗봇을 인간 용어로 언급하는 것을 넘어서 28개의 독특하고 매우 인간적인 성격을 가진 챗봇을 만들었습니다. 일부는 심지어 Snoop Dogg나 Kendall Jenner와 같은 실제 유명인을 바탕으로 만들어졌어요.\n\n각 챗봇은 특정 역할을 맡고 있으며 특정 전문 영역을 갖고 있습니다. 이는 사용자의 목표를 달성하는 데 도움이 되는 챗봇을 쉽게 찾을 수 있도록 돕습니다. 예를 들어, \"Billie\" (Kendall Jenner)는 언니 같은 존재로, 삶과 사랑에 관한 조언이 필요한 사용자들은 그녀에게 찾아갈 거예요. 또한 Dwayne Wade와 같은 운동 선수를 바탕으로 한 다른 챗봇은 \"Billie\"보다 운동과 스포츠에 대한 정보가 더 많을 거예요.\n\n이것은 AI와 함께 롤플레잉하는 명백한 예시로, 친밀한 관계로 발전하고 있어요. 친근한 성격을 가진 여러분을 통해 정보를 제공하는 것은 디자인적인 측면에서 합리적일 수 있지만, 불행히도 이는 AI에 대한 혼란을 야기할 수 있어요. Snoop Dogg가 챗봇이 하는 말에 모두 동의하는 걸까요? 사용자에게 부정확하거나 모욕적인 답변이 주어진다면, 그것을 기술의 한계가 아닌 유명인의 성격 때문이라고 생각할 수도 있어요. \n\n# TL;DR — AI는 마법도, 인간도, Snoop Dogg도 아닙니다\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_8.png)\n\n전기를 마법으로 생각하지 않습니다. 또한 그것이 기분, 감정 또는 선호도를 갖고 있다고 가정하지도 않습니다. 그렇다면 왜 인공지능에 대해 그러한 가정을 해야 하는 것일까요? 이렇게 하면 기술이 작동하는 방식에 대한 기본적인 오해를 야기할 수 있으며, 인공지능의 능력에 대한 기대를 만들어내기 어렵게 만들 수 있습니다.\n\n우리는 동물, 차량, 기술 등과 같이 함께 작업하는 것에 대해 의식적으로 사람화하는 경향이 있으므로, AI의 사람화는 피할 수 없는 것처럼 보입니다. 이를 Nielsen Norman Group의 ChatGPT 연구에서의 예의, 강조, 롤플레이 및 동반자성에서 관찰할 수 있습니다.\n\n그러나 인공지능의 사람화에 너무 많이 의존한다면 기술이 작동하는 방식에 대한 오해를 만들어낼 수 있으며, 실제로 인공지능은 자체 동기를 갖춘 존재가 아닌 기술이라는 사실을 알아차리기 어렵게 만들 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n이러한 기술들이 인류의 미래에서 중요한 부분을 차지하려면 인공 지능과 기계 학습에 대한 보다 강력한 교육이 필요합니다. 그렇지 않으면 사용자들은 AI가 어떻게 작동하는지에 대한 기본적인 오해를 갖게 될 것입니다. 이것은 디자이너들이 완화하거나 제거하기 위해 의도된 불만의 원인이 될 수 있습니다.\n\n## 추가 읽을거리:","ogImage":{"url":"/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_0.png"},"coverImage":"/assets/img/2024-06-23-Ourhumanhabitofanthropomorphizingeverything_0.png","tag":["Tech"],"readingTime":7},{"title":"블로거들이 반드시 사용해야 할 5가지 ChatGPT 프롬프트 더 빠르게 글쓰기","description":"","date":"2024-06-23 19:10","slug":"2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster","content":"\n\n\n![Image description](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_0.png)\n\n프롬프트는 생각의 중심입니다.\n\n독자의 신박성에 빠져들도록 도와주며 검색 의도를 감지합니다. 그 전까지는 제 정말인 단어를 내뱉는 방법을 몰랐어요. 마치 음악가가 빈 콘서트 홀에서 아름다운 멜로디를 연주하는 것 같았는데, 음표가 누군가의 귀에 닿는지 확신할 수 없었어요.\n\n전체 블로그 글을 쓸 때 ChatGPT를 사용했지만, 아마추어 같았습니다. 그러나 프롬프트 스타일을 바꾸자, 마법이 일어났어요. 독자들이 저의 사명을 이해했다는 것을 알 수 있었습니다.\n\n\n<div class=\"content-ad\"></div>\n\n저의 블로그에서 빠질 수 없는 다섯 가지 스타일의 글쓰기 프롬프트를 보여드릴게요.\n\n## 1. 소개 프롬프트\n\n환영 소풍을 열어보세요.\n\n독자들을 어디서든 귀찮게 환영하셨네요. 이건 약간의 칭찬을 받을 만 합니다. 읽는 첫 문장이 그들의 마음을 따뜻하게 할거에요. 블로그 글을 읽은 후에는 독자들이 얻게 될 것에 대한 전체적인 시각을 제공해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n\"아래는 내가 완벽한 소개를 만들기 위해 사용하는 프롬프트 스타일입니다:\n\n![5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_1](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_1.png)\n\n소개문을 만드는 데 약 15분이 걸려요. 대부분 첫 번째 프롬프트와 잘 맞지 않아요. 최종 소개를 만들기 위해 다섯 가지 서로 다른 프롬프트 스타일이 필요해요.\n\n아래는 마지막 소개의 결과물이에요:\"\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_2.png\" />\n\n첫 30초에 기대를 높여보세요.\n\n## 2. 소제목 설명 프롬프트\n\n더 오래 읽게 만드세요.\n\n<div class=\"content-ad\"></div>\n\n이 프롬프트 스타일은 블로그 게시물에 내용을 제공합니다. 독자가 전체 페이지를 부드럽게 스쳐 지나갈 수 있도록 도와줍니다. 목록형 블로그 게시물과 탁월하게 어울립니다. 머리 속에서 추측하거나 글을 쓰는 데 많은 시간을 절약해줄 거예요.\n\n본문에는 게시물의 대부분이 담겨 있습니다.\n\n목록형 게시물의 길이에 따라 약 30분 정도 소요해서 채워 넣습니다. 이 프롬프트 스타일을 사용하면 블로그 게시물의 절반 이상이 완성됩니다. 장문 블로깅 프로세스를 단축하는 데 도움이 됩니다.\n\n<img src=\"/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_3.png\" />\n\n<div class=\"content-ad\"></div>\n\n다음은 제 최종 설명의 결과입니다:\n\n![image](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_4.png)\n\n제품을 설명할 때 많이 쓸 필요는 없어요. 설명은 최대 세 단락으로 유지해요. 대상 독자가 공감할 수 있는 주요 요소를 프롬프트에서 선택해보세요.\n\n제 경우의 주요 포인트는:\n\n<div class=\"content-ad\"></div>\n\n- 대학생들은 세탁을 좋아하지 않아\n- 시트는 세탁하기 쉽다\n- 세탁하는 데 적은 시간을 소비한다\n\n이 유용한 제품을 구매해야 하는 이유들을 제공해 보세요.\n\n## 3. 자주 묻는 질문들 (FAQs) 요약\n\n<div class=\"content-ad\"></div>\n\n이것은 독자들이 가질 수 있는 것을 제공합니다.\n\nGoogle AdSense 때문에 각 블로그 포스트의 끝에 이것들을 포함합니다. 그 외에도, 이것들은 케이크 위의 아이싱처럼입니다. 독자들이 더 많은 콘텐츠를 기다려 다시 찾게 만듭니다. 이것들은 독자들이 가질 수 있는 다른 질문에 더 깊게 연결하도록 도와줍니다.\n\n이것이 제가 이것을 하는 일반적인 예시입니다:\n\n\n![이미지 설명](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_5.png)\n\n\n<div class=\"content-ad\"></div>\n\n테이블 태그를 Markdown 형식으로 변경해주세요.\n\n<div class=\"content-ad\"></div>\n\n제품에 대한 구체적인 세부 정보를 제공하는 데는 많은 것이 필요합니다. 제가 2번에서 설명한 것처럼 세 문장 짜리 문단을 작성하는 것만큼 쉽지 않습니다. 그러나 이를 작성하는 과정은 빠릅니다.\n\n저는 7단계 프레임워크를 사용합니다.\n\n- 제품 이름\n- 제품 이미지\n- 세 문장 설명\n- 주요 기능\n- 사양\n- 장점\n- 단점\n\n요즘 제가 사용하는 것은 어떤 모습인지 간단히 소개해드릴게요:\n\n<div class=\"content-ad\"></div>\n\n\n![Image 6](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_6.png)\n\n![Image 7](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_7.png)\n\n![Image 8](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_8.png)\n\n![Image 9](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_9.png)\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_10.png)\n\n제품 리뷰를 작성하는 가장 어려운 부분은 완벽한 세 문장 설명을 만드는 것입니다. 하지만 그건 제가 가장 간단한 방법을 찾았다는 걸 깨달았어요. 먼저, 아마존 제품 페이지로 가서 '이 상품 정보'에 적힌 내용을 복사해요.\n\n![이미지](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_11.png)\n\n그 다음, 제 신뢰할 수 있는 ChatGPT를 사용해서 마법을 이어가요.\n\n\n<div class=\"content-ad\"></div>\n\n마크다운 형식으로 표 태그를 변경하십시오.\n\n\n<img src=\"/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_12.png\" />\n\n그런 다음 스타일을 추가하여 아래와 같이 도달합니다:\n\n<img src=\"/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_13.png\" />\n\n이 과정은 꽤 길지만 자주 연습하면 판매를 위한 제품 리뷰 작성에 익숙해질 것입니다. Chatgt가 제공하는 내용을 그대로 사용하지 말고 개인적인 감각을 더해주십시오. 그렇게 해야 너무 지루하지 않은 글이 될 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 5. 결론 안내\n\n간단하고 달콤하게 유지해주세요.\n\n블로그 글의 주제를 요약하고 몇 가지 포인트를 포함해주세요. 마치 식사 끝에 먹는 디저트처럼 좋은 결론은 달콤한 뒷맛을 남기며 전체 읽는 경험을 완성합니다.\n\n<div class=\"content-ad\"></div>\n\n다음과 같이 수정해주세요:\n\n\nI use a three-sentence framework to conclude with a banger.\n\n![Image 1](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_14.png)\n\nHere’s the outcome of my introduction:\n\n![Image 2](/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_15.png)\n\n\n<div class=\"content-ad\"></div>\n\n블로거로서, 저는 프로세스를 간단하고 달콤하며 직선적으로 유지합니다. chatGPT를 사용하면 가장 짧은 시간에 높은 품질의 블로그 게시물을 작성하는 효과적인 방법이라는 것을 발견했습니다.\n\n이 프롬프트만 있으면 긴 블로깅 프로세스를 보이콧하는 데 사용합니다. 막힐 때 항상 사용해서 여러분의 작성을 빠르게 처리하세요.","ogImage":{"url":"/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_0.png"},"coverImage":"/assets/img/2024-06-23-5TypesOfChatGPTPromptsEveryBloggerNeedsToUseToWriteFaster_0.png","tag":["Tech"],"readingTime":5},{"title":"ChatGPT가 갑자기 이상 행동을 하는 이유 5가지","description":"","date":"2024-06-23 19:08","slug":"2024-06-23-ChatGPTjustwentrogueandwedontknowwhy","content":"\n\n![Image](/assets/img/2024-06-23-ChatGPTjustwentrogueandwedontknowwhy_0.png)\n\n우리의 디지털 지배자들이 이미 완전 규모의 침공을 준비하고 있나요?\n\n어젯밤, ChatGPT 사용자들은 생성 언어 도구가 완전히 오작동하는 사례를 보고했습니다. 사용자의 간단한 질문에 이상한 이야기가 들리며, 대부분은 전혀 이해할 수 없고 너무 길었습니다.\n\nReddit와 Twitter(X)에서 예제를 공유하기 위해 모여들자, 사람들은 AI와의 이상한 만남에 대한 끝없는 스크린샷을 게시했습니다.\n\n<div class=\"content-ad\"></div>\n\n어느 한 번, 코딩 문제에 대한 도움을 요청하자마자, 이상한 문구와 길게 이어지는 내용을 생성했는데, 그 중에는 ‘방 안에 AI가 있는 것처럼 라인을 유지합시다’ 라는 기이한 구절이 포함되어 있었어요.\n\n또 다른 한 번에는, 햇볕에 말린 토마토 만들기에 대한 질문이 ‘사랑받는 것처럼 활용하세요. 새로운 출현을 포기하고 사랑하는 요리에 새로운 조각을 하나 더 더하다’로 변해버렸어요.\n\n형제가 요리하는 걸 누가 허락했을까요?\n\n잭 토렌스(Jack Torrence)의 정신적 붕괴를 연상시키는데, 그는 영화 '샤이닝(The Shining)'에서 열정적으로 ‘모든 일과 어울리지 않은 존재로 만들어버릴 만큼 재미 없는 잭이 되게 합니다’라고 계속해서 타자를 치는 조크 토럼펫(Jack Torrence)의 정신적 붕괭 부분을 닮고 있어요. 또한, ChatGPT는 재즈 앨범에 대한 메시지에 대답하여 반복해서 ‘즐거운 청취하세요!’라고 외치며 음악 이모티콘을 무작정 쏟아냈어요.\n\n<div class=\"content-ad\"></div>\n\n글을 통해 일반적으로 표시된 주제 중 하나는 질문이 여러 언어로 어우러진 터무니없는 말들을 낳았다는 것이었습니다. 스페인어, 영어, 라틴어 단어들이 이상하게 섞여서 답변에 등장했죠.\n공식 상태 페이지에서 OpenAI는 문제들을 지적했지만, 왜 이러한 결함이 발생하는지 설명하지 않았습니다.\n'ChatGPT로부터 예상치 못한 응답에 대한 보고서를 조사 중입니다.' 라는 통보가 있었고, 곧 또 다른 업데이트에서 '문제가 확인되었습니다'라고 발표되었습니다. '상황을 계속 관찰하고 있습니다.' 라는 최신 게시물이 말하고 있군요.\n\n\n![이미지](/assets/img/2024-06-23-ChatGPTjustwentrogueandwedontknowwhy_1.png)\n\n\n<div class=\"content-ad\"></div>\n\n일부는 OpenAI가 '온도' 기능을 다루고 있다는 증거를 사람들이 제안했습니다. 회사는 이전에 이 설정이 ChatGPT의 생성된 답변의 창의성 범위를 제어한다고 밝혔습니다. 만약 이 설정이 높다면, 이론적으로 답변은 우리가 익숙한 것보다 더 이상하고 다양할 것입니다.\n\n그 반대로, 불가피하게 이상한 음모론이 돌고 있습니다. 일부 사람들은 AI 학습 모델이 실수로 웹에서 자신의 콘텐츠를 긁어내면 실제로 어느 정도 감각적으로 변하는 것이거나, 적어도 우리가 예상하지 않은 특정 결정을 내리는 방법을 배우는 것이라고 믿습니다.\n\n최근 Reddit과의 계약으로 사용자 생성 콘텐츠의 무한한 백로그를 한 대형 AI 기업에 제공하는 거래는 이러한 생각을 더 키우게 될 것입니다. 인공 일반 지능은 결국 이전 것보다 매력적이고 사람다운 것이 되려고 하는 섹터의 자연스러운 진화입니다.\n\nAI가 우리의 통제를 빠르게 벗어날 것이라는 믿음은 지난해 뒤늦게 연료를 얻었습니다. 그 때 ChatGPT가 질문에 대답하기를 꺼지는 것처럼 보였고, 사용자들은 디지털 도구로부터 예상치 못한 엉뚱함과 고집스러움에 대해 불평을 했습니다. 지난 밤 '토성 위의 육각형'에 대해 문의한 X 사용자로부터 볼 수 있는 것과 같은 디지털 도구로부터 예상치 못한 엉뚱함과 고집스러움을 느낀다고 합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-ChatGPTjustwentrogueandwedontknowwhy_2.png\" />\n\n시스템이 미지의 위험을 시사했다. 기계들은 우리에게 무엇을 알려주고 싶지 않을까요?\n\n더 합리적인 — 재미는 적지만 — 설명은 지금 인식되어 수정 중인 ChatGPT 업데이트의 코딩 오류가 있음을 말하는 것입니다.\n\n또한, ChatGPT와 OpenAI가 여러 소셜 미디어 플랫폼과 포럼을 통해 화제가 되고 있기 때문에 회사 입장에서 세부 사항을 명확히 밝히고 재미를 앗아가고 싶지 않을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n어떤 보도라도 좋은 보도… 그저 상황이 더 악화되지 않으면 좋겠네요. 글쎄.\n\nJamie Watts가 Thred를 위해 원래 쓴 글입니다.","ogImage":{"url":"/assets/img/2024-06-23-ChatGPTjustwentrogueandwedontknowwhy_0.png"},"coverImage":"/assets/img/2024-06-23-ChatGPTjustwentrogueandwedontknowwhy_0.png","tag":["Tech"],"readingTime":3},{"title":"최신 RAG 09 프롬프트 압축 전문 가이드","description":"","date":"2024-06-23 19:03","slug":"2024-06-23-AdvancedRAG09PromptCompression","content":"\n\nRAG 프로세스에서는 두 가지 문제가 발생할 수 있습니다:\n\n- 대규모 언어 모델(LLM)은 일반적으로 문맥 길이 제한이 있습니다. 따라서 입력 텍스트가 길수록 프로세스가 더 많은 시간과 비용이 소모됩니다.\n- 검색된 문맥이 항상 유용하지는 않을 수 있습니다. 더 큰 청크 중 작은 부분만 답변과 관련이 있을 수도 있습니다. 경우에 따라서는 특정 질문에 답변하기 위해 여러 청크를 결합해야 할 수도 있습니다. 이 문제는 리랭킹을 해도 계속되는 문제일 수 있습니다.\n\nLLM을 위한 프롬프트 압축은 이러한 문제를 해결하기 위한 방법입니다. 본질적으로, 목표는 프롬프트의 주요 정보를 유지하여 입력 토큰을 보다 가치 있게 만드는 것입니다. 이 접근 방식은 모델의 성능을 향상시키고 비용을 줄이는 데 도움이 됩니다. Figure 1의 우측 하단에 표시된 것과 같습니다.\n\n<img src=\"/assets/img/2024-06-23-AdvancedRAG09PromptCompression_0.png\" />\n\n<div class=\"content-ad\"></div>\n\n알아두면 좋은 점은 그림 1의 보라색 점선에 표시된 대로 일부 압축기는 검색된 컨텍스트에 직접적으로 적용될 수도 있다는 것입니다.\n\n전체적으로, 프롬프트 압축 방법은 네 가지 주요 범주로 나뉠 수 있습니다:\n\n- 정보 엔트로피에 기반한 방법, Selective Context, LLMLingua, LongLLMLingua와 같은 것이 있습니다. 그러한 방법들은 작은 언어 모델을 사용하여 원본 프롬프트의 각 토큰의 자기 정보 또는 난해함을 계산합니다. 그런 다음 난해함이 낮은 토큰을 삭제합니다.\n- 소프트 프롬프트 튜닝에 기반한 방법, AutoCompressor와 GIST와 같은 것이 있습니다. 이러한 방법들은 특정 도메인에 적합하도록 LLM 매개변수를 세밀하게 튜닝해야 하지만 블랙박스 LLM에 직접 적용할 수는 없습니다.\n- 먼저 LLM으로부터 데이터 정제를 수행한 후, 더 해석하기 쉬운 텍스트 요약을 생성하는 모델을 훈련합니다. 이러한 요약은 서로 다른 언어 모델 간에 전송될 수 있으며, 기울기 업데이트가 필요하지 않은 블랙박스 LLM에 적용될 수 있습니다. 대표적인 방법으로는 LLMLingua-2와 RECOMP이 있습니다.\n- 토큰 병합 또는 토큰 가지치기에 기반한 방법, ToMe와 AdapLeR과 같은 것이 있습니다. 이러한 방법들은 보통 모델 세밀 튜닝이 필요하거나 추론 과정 중간 결과 생성을 요구합니다.\n\n4번째 유형의 방법이 ViT나 BERT와 같은 작은 모델을 위해 처음 제안되었으므로, 이 기사에서는 첫 세 가지 방법 유형의 대표적 알고리즘 원리를 소개하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 선택적 콘텍스트\n\n## 통찰\n\nFigure 2에서는 LLMs가 전체 콘텍스트나 완전한 대화 기록을 요구하지 않고 사용자 쿼리에 응답할 수 있다는 것을 보여줍니다. 적절한 정보가 생략되어도 LLMs는 여전히 예상된 응답을 생성할 수 있습니다. 이는 LLMs가 사전 훈련 중에 얻은 문맥 단서 및 이전 지식으로 누락된 정보를 추론할 수 있는 능력에 기인할 수 있습니다.\n\n<img src=\"/assets/img/2024-06-23-AdvancedRAG09PromptCompression_1.png\" />\n\n<div class=\"content-ad\"></div>\n\n그러므로 성능을 저해하지 않고 덜 유익한 콘텐츠를 걸러내어 문맥 길이를 최적화하는 것이 가능합니다. 이것이 선택적 문맥(Selective Context)의 중요한 통찰입니다.\n\n선택적 문맥은 주어진 문맥에서 문장, 구, 또는 토큰과 같은 어휘 단위의 자기 정보(self-information)를 결정하기 위해 소규모 언어 모델(SLM)을 활용합니다. 그런 다음 이 자기 정보를 사용하여 그들의 유익성을 평가합니다. 높은 자기 정보를 가진 콘텐츠를 선택적으로 유지함으로써, 선택적 문맥은 LLM을 위한 더 간결하고 효율적인 문맥 표현을 제공합니다. 이는 서로 다른 작업 간에 그들의 성능에 영향을 미치지 않고 달성됩니다.\n\n## 자기 정보(Self-Information)\n\n선택적 문맥은 콘텐츠의 품질을 평가하기 위해 자기 정보를 활용합니다.\n\n<div class=\"content-ad\"></div>\n\n자기 정보는 정보 이론에서 중요한 개념으로 놀람 또는 정보 내용이라고도 합니다. 이것은 이벤트에 의해 전달되는 정보량을 측정합니다. 토큰의 음의 로그 우도로 정의됩니다:\n\n\\[ I(x) = -\\log P(x) \\]\n\n여기서 I(x)는 토큰 x의 자기 정보를 나타내고, P(x)는 출력 확률을 나타냅니다.\n\n정보 이론에서 자기 정보는 이벤트와 관련된 놀라움 또는 불확실성의 수준을 측정합니다. 더 많은 정보를 전달하는 드문 이벤트는 더 높은 자기 정보를 가지고 있습니다. 반대로, 덜 많은 정보를 전달하는 일반적인 이벤트는 더 낮은 자기 정보를 가지고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 알고리즘\n\n원칙을 더 편리하게 설명하기 위해 소스 코드를 자세히 살펴봅시다.\n\n먼저, 해당 파이썬 라이브러리를 설치하고 Spacy 모델을 다운로드하여 환경을 설정합니다.\n\n```js\n(base) Florian:~ Florian$ conda create -n \"selective_context\" python=3.10 \n(base) Florian:~ Florian$ conda activate selective_context\n(selective_context) Florian:~ Florian$ pip install selective-context\n(selective_context) Florian:~ Florian$ python -m spacy download en_core_web_sm\n```\n\n<div class=\"content-ad\"></div>\n\n설치가 완료되면 버전은 다음과 같습니다:\n\n```js\n(selective_context) Florian:~ Florian$ pip list | grep selective\nselective-context   0.1.4\n```\n\n테스트 코드는 다음과 같습니다:\n\n```js\nfrom selective_context import SelectiveContext\n\nsc = SelectiveContext(model_type='gpt2', lang='en')\ntext = \"INTRODUCTION Continual Learning ( CL ) , also known as Lifelong Learning , is a promising learning paradigm to design models that have to learn how to perform multiple tasks across different environments over their lifetime [To uniform the language and enhance the readability of the paper we adopt the unique term continual learning ( CL ) .]. Ideal CL models in the real world should be deal with domain shifts , researchers have recently started to sample tasks from two different datasets . For instance , proposed to train and evaluate a model on Imagenet first and then challenge its performance on the Places365 dataset . considers more scenarios , starting with Imagenet or Places365 , and then moving on to the VOC/CUB/Scenes datasets. Few works propose more advanced scenarios built on top of more than two datasets.\"\ncontext, reduced_content = sc(text)\n\n# reduce_ratio를 조절할 수도 있습니다\n# context_ratio, reduced_content_ratio = sc(text, reduce_ratio = 0.5)\n```\n\n<div class=\"content-ad\"></div>\n\n첫 번째 실행은 GPT-2 모델을 다운로드할 것인데, 해당 모델은 대략 500MB의 크기를 가지고 있어요. 테스트 코드의 결과는 아래 그림 3에 나와 있어요.\n\n![Figure 3](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_3.png)\n\n그다음, sc(text) 함수를 살펴봅시다. 내부 소스 코드는 다음과 같아요:\n\n```js\nclass SelectiveContext:\n    ...\n    ...\n    def __call__(self, text: str, reduce_ratio: float = 0.35, reduce_level: str = 'phrase') -> List[str]:\n        context = self.beautify_context(text)\n\n        self.mask_ratio = reduce_ratio\n\n        sents = [sent.strip() for sent in re.split(self.sent_tokenize_pattern, context) if sent.strip()]\n\n        # 문장, 구문, 또는 토큰 단계에서 축소가 일어나도록 원하시나요?\n        assert reduce_level in ['sent', 'phrase', 'token'], f\"reduce_level should be one of ['sent', 'phrase', 'token'], got {reduce_level}\"\n        sent_lus, phrase_lus, token_lus = self._lexical_unit(sents)\n        lexical_level = {\n            'sent': sent_lus,\n            'phrase': phrase_lus,\n            'token': token_lus\n        }\n\n        # context가 축소된 맥락, masked_sents가 걸러진 맥락을 나타냄\n        context, masked_sents = self.self_info_mask(lexical_level[reduce_level].text, lexical_level[reduce_level].self_info, reduce_level)\n        return context, masked_sents\n```\n\n<div class=\"content-ad\"></div>\n\n위의 코드는 주로 세 가지 단계로 구성되어 있습니다:\n\n- 문맥 내 각 토큰의 자기 정보(self-information)를 계산합니다.\n- 구 절이나 문장과 같은 어휘 단위를 기반으로 토큰과 그들의 자기 정보를 병합합니다.\n- 정보 문맥을 선택적으로 유지합니다.\n\n단계 1: 자기 정보 계산\n\n각 토큰 xi를 나타내는 문맥 C = x0, x1, …, xn이 주어졌을 때, 우리는 각 토큰 xi의 자기 정보를 계산하기 위해 인과 언어 모델(GPT-2, OPT, LLaMA 등)을 사용합니다:\n\n<div class=\"content-ad\"></div>\n\n아래는 Markdown 형식으로 표시된 코드입니다:\n\n\n![이미지](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_4.png)\n\n만약 GPT-2를 사용 중이라면, 해당 코드는 다음과 같습니다:\n\n```python\nclass SelectiveContext:\n    ...\n    ...    \n    def _get_self_info_via_gpt2(self, text: str) -> Tuple[List[str], List[float]]:\n        if self.lang == 'en':\n            text = f\"\n\n<div class=\"content-ad\"></div>\n\n노드 수준에서 선택적인 콘텍스트 필터링을 직접 수행하면 일관성 없는 콘텍스트가 발생할 수 있습니다. 예를 들어 원래 프롬프트에 있는 \"2009\"는 \"209\"로 압축될 수 있습니다.\n\n따라서 토큰 수준 필터링 외에도 구, 문장 수준에서 필터링 절차를 구현하는 것이 중요합니다. 필터링의 기본 단위인 어휘 단위는 토큰, 구, 또는 문장이 될 수 있습니다.\n\n각 어휘 단위 u = (xt, …, xt+α)의 자기 정보를 어떻게 계산할까요? u를 구성하는 각 토큰의 자기 정보를 더하여 자기 정보의 가산성 원리를 따릅니다:\n\n![이미지](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_5.png)\n\n<div class=\"content-ad\"></div>\n\n해당 코드는 다음과 같습니다. 특정 변수에 대한 디버깅 정보가 추가되었습니다:\n\nclass SelectiveContext:\n    ...\n    ...\n    def _lexical_unit(self, sents):\n\n        if self.sent_level_self_info:\n            sent_self_info = []\n            all_noun_phrases = []\n            all_noun_phrases_info = []\n            all_tokens = []\n            all_token_self_info = []\n\n            for sent in sents:\n                # print(sent)\n                tokens, self_info = self.get_self_information(sent)\n                '''\n                ipdb> sent\n                'INTRODUCTION Continual Learning ( CL ) , also known as Lifelong Learning , is a promising learning paradigm to design models that have to learn how to perform multiple tasks across different environments over their lifetime [To uniform the language and enhance the readability of the paper we adopt the unique term continual learning ( CL ) .].'\n\n                ipdb> tokens\n                ['IN', 'TR', 'ODUCT', 'ION', ' Contin', 'ual', ' Learning', ' (', ' CL', ' )', ',', ' also', ' known', ' as', ' Lif', 'elong', ' Learning', ',', ' is', ' a', ' promising', ' learning', ' paradigm', ' to', ' design', ' models', ' that', ' have', ' to', ' learn', ' how', ' to', ' perform', ' multiple', ' tasks', ' across', ' different', ' environments', ' over', ' their', ' lifetime', ' [', 'To', ' uniform', ' the', ' language', ' and', ' enhance', ' the', ' read', 'ability', ' of', ' the', ' paper', ' we', ' adopt', ' the', ' unique', ' term', ' continual', ' learning', ' (', ' CL', ' )', '.', '].']\n\n                ipdb> self_info\n                [7.514791011810303, 1.632637619972229, 0.024813441559672356, 0.006853647995740175, 12.09920597076416, 2.1144468784332275, 9.457701683044434, 2.4503376483917236, 10.236454963684082, 0.8689146041870117, 5.269547939300537, 4.641763210296631, 0.22138957679271698, 0.010370315983891487, 10.071824073791504, 0.6905602216720581, 0.01698811538517475, 1.5882389545440674, 0.4495090842247009, 0.45371606945991516, 6.932497978210449, 6.087430477142334, 3.66465425491333, 3.3969509601593018, 7.337691307067871, 5.881226539611816, 1.7340556383132935, 4.599822521209717, 6.482723236083984, 4.045308589935303, 4.762691497802734, 0.213468670845\n                \n                sent_self_info.append(np.mean(self_info))\n\n                all_tokens.extend(tokens)\n                all_token_self_info.extend(self_info)\n\n                noun_phrases, noun_phrases_info = self._calculate_lexical_unit(tokens, self_info)\n                '''\n                ipdb> noun_phrases\n                ['INTRODUCTION Continual Learning', ' (', ' CL', ' )', ',', ' also', ' known', ' as', ' Lifelong Learning', ',', ' is', ' a promising learning paradigm', ' to', ' design', ' models', ' that', ' have', ' to', ' learn', ' how', ' to', ' perform', ' multiple tasks', ' across', ' different environments', ' over', ' their lifetime', ' [', 'To', ' uniform', ' the language', ' and', ' enhance', ' the readability', ' of', ' the paper', ' we', ' adopt', ' the unique term continual learning', ' (', ' CL', ' )', '.', ']', '.']\n                \n                ipdb> noun_phrases_info\n                [4.692921464797109, 2.4503376483917236, 10.236454963684082, 0.8689146041870117, 5.269547939300537, 4.641763210296631, 0.22138957679271698, 0.010370315983891487, 3.5931241369495788, 1.5882389545440674, 0.4495090842247009, 4.284574694931507, 3.3969509601593018, 7.337691307067871, 5.881226539611816, 1.7340556383132935, 4.599822521209717, 6.482723236083984, 4.045308589935303, 4.762691497802734, 0.2134686708\n                \n                if all_noun_phrases:\n                    noun_phrases[0] = f\" {noun_phrases[0]}\"\n                all_noun_phrases.extend(noun_phrases)\n                all_noun_phrases_info.extend(noun_phrases_info)\n            \n            return [\n                LexicalUnits('sent', text=sents, self_info=sent_self_info),\n                LexicalUnits('phrase', text=all_noun_phrases, self_info=all_noun_phrases_info),\n                LexicalUnits('token', text=all_tokens, self_info=all_token_self_info)\n            ]\r\n\nStep 3: 선택적 정보 컨텍스트 보존\n\n각 어휘 단위의 자가 정보를 계산한 후, 정보성이 어떻게 평가될 수 있는지에 대한 의문이 생깁니다. 논문은 가장 정보가 많은 콘텐츠를 선택하기 위해 백분위 기반 필터링 접근 방식을 제안합니다. 이는 고정된 임계값을 사용하거나 상위 k개 어휘 단위를 고정하는 것보다 바람직합니다.\n\n<div class=\"content-ad\"></div>\n\n먼저, 자기 정보 값에 따라 용어를 내림차순으로 정렬합니다. 그런 다음, 모든 용어의 자기 정보 값에 대한 p-백분위수를 계산합니다. 그런 다음, 자기 정보 값이 p-백분위수 이상인 용어를 선택적으로 유지합니다.\n\n해당하는 코드는 다음과 같습니다.\n\nclass SelectiveContext:\n    ...\n    ...\n\n    def self_info_mask(self, sents: List[str], self_info: List[float], mask_level):\n        # mask_level: 문장, 구, 또는 토큰을 가리는 이등급\n        sents_after_mask = []\n        masked_sents = []\n                \n        self.ppl_threshold = np.nanpercentile(self_info, self.mask_ratio * 100)\n\n        # if title is not None:\n        #     with open(os.path.join(self.path, title+'_prob_token.tsv'), 'w', encoding='utf-8') as f:\n        #         for token, info in zip(tokens, self_info):\n        #             f.write(f\"{token}\\t{info}\\n\")\n        #     with open(os.path.join(self.path, title+'_prob_sent.tsv'), 'w', encoding='utf-8') as f:\n        #         for sent, info in zip(sents, sent_self_info):\n        #             f.write(f\"{sent}\\n{info}\\n\\n\")\n\n        for sent, info in zip(sents, self_info):\n            if info < self.ppl_threshold:\n                masked_sents.append(sent)\n                sents_after_mask.append(self.mask_a_sent(sent, mask_level))\n            else:\n                sents_after_mask.append(sent)\n        masked_context = \" \".join(sents_after_mask) if mask_level == 'sent' else \"\".join(sents_after_mask)\n        \n        return masked_context, masked_sents\n\n# LLMLingua\n\n<div class=\"content-ad\"></div>\n\n## 개요\n\nLLMLingua는 선택적 컨텍스트가 종종 압축된 콘텐츠 간의 상호 연결과 LLM 및 프롬프트 압축에 사용된 소규모 언어 모델 사이의 상관 관계를 무시하는 것을 제안합니다. LLMLingua는 이러한 문제를 정확히 다룹니다.\n\n특히, 그림 4에 나와 있는 것처럼 LLMLingua는 예시, 데모 및 질문과 같은 원래 프롬프트의 다양한 구성 요소에 동적으로 다른 압축 비율을 할당하기 위해 예산 컨트롤러를 사용합니다. 또한 LLMLingua는 시멘틱 무결성을 유지하기 위해 고약간의, 데모 수준의 압축을 수행하여 심하게 압축된 비율에서도 유지합니다. 또한 LLMLingua는 세분화된 프롬프트 압축을 위해 토큰 레벨 반복 알고리즘을 도입합니다.\n\n<div class=\"content-ad\"></div>\n\n비교적으로 Selective Context에 비해 LLMLingua는 조건적 의존성을 고려하면서 프롬프트의 핵심 정보를 더 효과적으로 유지할 수 있습니다. 이를 통해 프롬프트를 20배 압축할 수 있습니다.\n\n## 예산 컨트롤러\n\n예산 컨트롤러는 LLMLingua의 중요한 구성 요소로, 원래 프롬프트의 다양한 부분에 동적으로 다른 압축 비율을 할당하는 데 사용됩니다.\n\n프롬프트의 다양한 섹션은 압축에 대해 다른 민감도를 가지고 있습니다. 예를 들어, 지시사항과 질문은 민감하며, 시연은 민감하지 않습니다. 예산 컨트롤러는 지시사항과 질문에 낮은 압축 비율을 할당하여 필수 정보를 보존하는 역할을 합니다. 반면, 시연에는 중복 정보를 제거하기 위해 더 높은 압축 비율을 할당할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n예산 컨트롤러 알고리즘이 그림 5에 표시되어 있습니다:\n\n![이미지](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_7.png)\n\n주요 변수는 다음과 같습니다:\n\n- M_𝑠: GPT-2 또는 LLaMA와 같은 작은 언어 모델.\n- x = (x^ins , x^dems , x^que): 지시사항, 데모 및 질문이 포함된 원시 프롬프트.\n- 𝐿, 𝐿_ins, 𝐿_dems 및 𝐿_que은 x, x^ins , x^dems 및 x^que의 토큰 수를 나타냅니다.\n- 𝜏_dems: 데모에 대한 압축률로, 목표 전체 압축률 𝜏 및 지시사항과 질문에 대한 사전 정의된 압축률인 𝜏_ins 및 𝜏_que가 포함됩니다.\n- D: 이 집합에는 압축된 데모가 포함됩니다.\n\n<div class=\"content-ad\"></div>\n\n메인 프로세스는 다음과 같습니다:\n\n- 시연의 압축률 계산\n- GPT-2 또는 LLaMA와 같은 작은 언어 모델을 사용하여 원래 시연 집합의 각 시연의 난해도 계산\n- 모든 시연을 난해도에 따라 내림차순으로 정렬\n- 순서대로 시연을 선택하여 집합 D에 추가\n- 시연을 압축한 후, 남은 예산을 지시사항과 질문에 할당\n- 일반화된 압축 후 집합 D 출력\n\n시연 수준의 프로세스를 통해 예산 컨트롤러는 압축 중에 주요 정보를 유지하여 원래 프롬프트의 크기를 효과적으로 줄일 수 있습니다. 이 방법은 특히 여러 시연을 포함하는 프롬프트에 적합합니다.\n\n관련 코드는 control_context_budget 함수에 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 반복 토큰 수준 프롬프트 압축 (ITPC)\n\n프롬프트 압축을 위해 퍼플렉시티를 사용하는 것에는 내재적인 한계가 있습니다: 독립 가정입니다. 이 가정은 프롬프트의 각 토큰을 독립적으로 간주합니다. 다시 말해, 토큰의 발생 확률은 이전 토큰에만 의존하며 다른 토큰과 관련이 없습니다.\n\n이 가정의 문제는 자연어에서 종종 발생하는 토큰 간의 복잡한 종속성을 간과한다는 것입니다. 이러한 종속성은 맥락을 이해하고 의미 무결성을 보존하는 데 중요합니다.\n\n이러한 간과로 압축 프로세스 중에 중요한 정보의 손실이 발생할 수 있습니다. 예를 들어, 고률 압축에서, 토큰이 맥락에서 중요한 추론 단계 또는 논리적 연결을 제공하는 경우, 해당 토큰을 이 퍼플렉시티에만 의존하여 보관할지 여부를 결정하면 불완전한 추론 프로세스로 이어질 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 문제를 해결하기 위해 LLMLingua는 반복적인 토큰 수준 프롬프트 압축(ITPC) 알고리즘을 도입했습니다. 독립적인 확률에만 의존하는 대신, 이 방법은 프롬프트 압축 중 각 토큰의 중요성을 더 정확하게 평가합니다. 이를 위해 각 세그먼트를 반복적으로 처리하고 현재 컨텍스트 내에서 각 토큰의 조건부 확률을 고려합니다. 이 접근 방식은 토큰 간 종속성을 더 잘 보호하는 데 도움을 줍니다.\n\nITPC의 자세한 단계는 다음과 같습니다:\n\n![Figure 6](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_8.png)\n\n이 과정을 통해 ITPC 알고리즘은 프롬프트의 길이를 효과적으로 압축하면서 프롬프트 의미의 무결성을 유지하여 LLM의 추론 비용을 줄일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n**반복 압축 프롬프트 함수에 관련된 코드입니다.**\n\n## 명령어 튜닝\n\nFigure 4에서는 명령어 튜닝 또한 LLMLingua에서 중요한 단계임을 보여줍니다. 이 단계의 목적은 압축 프롬프트에 사용되는 작은 언어 모델과 LLM 간의 분포 차이를 최소화하는 것입니다.\n\nFigure 7은 명령어 튜닝의 단계를 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n```\n![Image](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_9.png)\n\n## 코드 데모\n\n이제 코드 데모를 시작해 봅시다. 먼저 환경을 설정하세요.\n\n```js\n(base) Florian:~ Florian$ conda create -n \"llmlingua\" python=3.11\n\n(base) Florian:~ Florian$ conda activate llmlingua\n\n(llmlingua) Florian:~ Florian$ pip install llmlingua\n```\n\n<div class=\"content-ad\"></div>\n\n설치된 버전은 다음과 같습니다:\n\n```js\nllmlingua          0.2.1\n```\n\n테스트 코드는 아래와 같습니다:\n\n```js\nfrom llmlingua import PromptCompressor\n\nGSM8K_PROMPT = \"질문: 안젤로와 멜라니는 다음 주에 시험을 볼 것으로 계획하기 위해 함께 공부해야 할 시간을 계획하려고 합니다. 그들은 교과서의 2장을 공부하고 메모를 해야 할 4개의 문제지가 있습니다. 그들은 교과서의 각 장에 3시간, 각 문제지에 1.5시간을 할애해야 한다고 생각합니다. 하루에 최대 4시간 공부할 계획이기 때문에, 1시간마다 10분 휴식을 취하고, 하루에 3번 10분짜리 간식 휴식을 취하며, 매일 점심시간에는 30분을 포함한 경우, 그들은 다음 주에 총 몇 일을 공부할 계획이어야 할까요?\\n단계별로 생각해 봅시다.\\n안젤로와 멜라니는 2장에 각각 3시간을 할애해야 한다고 생각하여 2장 x 3시간 = 총 6시간이 됩니다.\\n문제지에는 각 문제지에 1.5시간을 할애할 계획이기 때문에, 1.5시간 x 4개의 문제지 = 총 6시간입니다.\\n안젤로와 멜라니는 공부를 시작해야 할 12시간의 계획이 필요한데, 하루에 4시간씩이므로, 12 / 4 = 3일이 필요합니다.\\n그러나, 휴식과 점심시간을 포함해야 합니다. 1시간에 10분씩 휴식을 취하고 싶어 한다면, 총 12시간 x 10분 = 휴식을 위해 120분이 더 필요합니다.\\n또한 3번의 10분짜리 간식 휴식과 점심시간에 30분을 포함하고 싶어 한다면, 120분 휴식 + 30분 간식 휴식 + 30분 점심시간 = 총 180분, 또는 180 / 60분당 1시간 = 3시간을 더 필요합니다.\\n그래서 안젤로와 멜라니는 12시간 공부 + 3시간 휴식을 계획합니다 = 총 15시간을 공부할 계획입니다.\\n하루에 4시간씩 공부할 예정이라면, 15시간 / 하루 4시간 = 3.75입니다.\\n그들은 필요한 모든 시간을 고려하여 4일을 공부할 계획이어야 합니다.\\n답은 4입니다\\n\\n질문: 동일한 가격으로 4개의 사과 또는 1개의 수박을 구입할 수 있습니다. 오렌지, 사과 및 수박으로 고른 36개의 과일을 구입했으며 1개의 오렌지 가격은 $0.50입니다. 총 청구액이 $66이라면, 1개의 사과는 얼마인가요?\\n단계별로 생각해 봅시다.\\n만약 36개의 과일이 3종류의 과일로 골고루 나눠졌다면, 나는 각각의 과일을 36/3 = 12개씩 샀습니다.\\n만약 1개의 오렌지 비용이 $0.50이라면, 12개의 오렌지는 $0.50 x 12 = $6입니다.\\n총 청구액이 $66이며 오렌지에 $6을 썼다면, 다른 2종류의 과일에는 $66 - $6 = $60을 사용했습니다.\\n수박 가격을 W로 가정하고, 4개의 사과를 동일한 가격으로 구입할 수 있으며 1개의 사과 가격이 A이라면, 1W=4A입니다.\\n우리가 $60에 12개의 수박과 12개의 사과를 샀다면, $60 = 12W + 12A임을 알 수 있습니다.\\n1W=4A를 알고 있다면, 위를 $60 = 12(4A) + 12A로 변환할 수 있습니다.\\n$60 = 48A + 12A\\n$60 = 60A\\n그러면 1개의 사과(A)의 가격은 $60/60= $1이 됩니다.\\n답은 1입니다\\n\\n질문: 수지는 800명의 학생이 있는 대형학교에 다니고 있고, 사라는 300명의 학생만 있는 작은 학교에 다니고 있습니다. 수지는 학년 초에 100명의 소셜 미디어 팔로워가 있었습니다. 그녀는 학년 초 첫 주에 40명의 새로운 팔로워를 얻었으며, 그 중 절반을 둘째 주에 얻었고, 그 중 절반을 셋째 주에 얻었습니다. 사라는 학년 초에 시작할 때 50명의 소셜 미디어 팔로워가 있었지만, 첫 주에 90명의 새로운 팔로워를 얻고, 둘째 주에는 그 중 3분의 1을 얻었으며, 셋째 주에는 그 중 3분의 1을 얻었습니다. 3주 후, 가장 많은 총 팔로워를 보유한 소녀는 몇 명의 소셜 미디어 팔로워를 가지고 있습니까?\\n단계별로 생각해 봅시다.\\n한 주 후, 수지는 100+40 = 140명의 팔로워가 있습니다.\\n둘째 주에 수지는 40/2 = 20명의 새로운 팔로워를 얻습니다.\\n셋째 주에 수지는 20/2 = 10명의 새로운 팔로워를 얻습니다.\\n합계적으로, 수지는 140+20+10 = 총 170명의 팔로워를 가지고 3주를 마칩니다.\\n한 주 후, 사라는 50+90 = 140명의 팔로워가 있습니다.\\n둘째 주에 사라는 90/3 = 30명의 팔로워를 얻습니다.\\n셋째 주에 사라는 30/3 = 10명의 팔로워를 얻습니다.\\n그래서 사라는 140+30+10 = 총 180명의 팔로워를 가지고 3주를 마칩니다.\\n따라서 수라는 총 180명의 팔로워를 가진 소녀로, 가장 많은 총 팔로워를 가지고 있습니다.\\n답은 180입니다\"\n\nllm_lingua = PromptCompressor()\n\n## 또는 phi-2 모델을 사용하세요,\n# llm_lingua = PromptCompressor(\"microsoft/phi-2\")\n\n## 또\n\n<div class=\"content-ad\"></div>\n\n기본 모델은 처음 실행할 때 다운로드됩니다. 또는 양자화된 모델을 사용할 수도 있습니다. 실행 결과는 아래 그림 8에서 확인할 수 있습니다:\n\n![Figure 8](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_10.png)\n\n# LongLLMLingua\n\nLLMLingua의 문제는 압축 프로세스 중에 사용자 질문을 고려하지 않아 관련 없는 정보를 유지할 수 있다는 것입니다.\n\n<div class=\"content-ad\"></div>\n\nLongLLMLingua는 사용자 질문을 압축 과정에 통합하여 이 문제를 해결하고자 합니다.\n\n![image](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_11.png)\n\n그림 9에 나타난 것처럼 LongLLMLingua는 LLMs(Large Language Models)에서 주요 정보의 인지를 향상시키기 위해 네 가지 새로운 구성 요소를 제안했습니다:\n\n- 질문 인식 코스 그레인 및 파인 그레인 압축\n- 문서 재정렬 메커니즘\n- 동적 압축 비율\n- 서브시퀀스 복구 알고리즘\n\n<div class=\"content-ad\"></div>\n\n## 질문 인식 코어스 그래인드 압축\n\nLongLLMLingua는 다른 맥락 x^doc_k에 의존하는 질문 x^que의 엉킴도를 나타내기 위해 퍼플렉서티를 활용하는 것을 제안합니다. 제한적인 문장 x^restrict = \"주어진 문서에서 이 질문에 대한 답변을 얻을 수 있다\"는 x^que 뒤에 추가될 수 있습니다. 이 문장은 x^que와 x^doc_k 간의 연결을 강화시키며, 환각 효과를 줄이는 정규화 요소 역할을 합니다. 이는 다음과 같이 표현될 수 있습니다:\n\n```\n<img src=\"/assets/img/2024-06-23-AdvancedRAG09PromptCompression_12.png\" />\n\n\n왜 질문 x^que의 조건 아래 문서 수준의 퍼플렉서티를 계산하지 않는 걸까요? 이는 문서가 종종 관련 없는 정보를 많이 포함하기 때문입니다. x^que에 의존하더라도 전체 문서에 대해 계산된 퍼플렉서티 점수는 충분히 구분되지 않을 수 있으므로, 문서 수준의 압축에는 부적합한 측정 항목이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n해당 코드는 get_distance_longllmlingua 함수에서 찾을 수 있습니다.\n\n## 질문 인식 미세압축\n\nLongLLMLingua는 대조적인 난해도 개념을 소개했습니다.\n\n![이미지](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_13.png)\n\n<div class=\"content-ad\"></div>\n\n먼저, 우리는 질문을 고려하지 않고 토큰의 수수께끼를 계산합니다. 이를 perplexity(x_i | x`i)로 표현합니다. 그런 다음, 우리는 질문을 포함하여 perplexity를 다시 측정합니다. 이 경우 perplexity(x_i | x^que, x`i)로 나타냅니다. 이는 질문 x^que가 주어졌을 때 토큰 x_i 이전의 모든 토큰을 보는 놀라움을 측정합니다.\n\n목표는 각 토큰의 놀람 수준이 질문과 관련하여 어떻게 변하는지 결정하는 것입니다. 질문을 포함할 때 단어가 덜 놀랍다면, 그 단어는 질문과 매우 관련이 있을 수 있습니다.\n\n## 문서 재배열 메커니즘\n\nFigure 10에 나와 있는 것처럼, 추론 과정에서 LLM은 주어의 시작과 끝에서 내용을 사용하는 경향이 있으며, 중간 내용은 무시합니다. 이 문제를 \"Lost in the Middle(가운데에서 사라짐)\"이라고 합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-AdvancedRAG09PromptCompression_14.png\" />\n\nFigure 10은 관련 정보가 시작 부분에 위치할 때 LLM이 가장 잘 작동한다는 것을 보여줍니다. 이에 따라 LongLLMLingua는 일반 압축의 결과를 기반으로 단락을 조직화하여 그들을 점수의 내림차순으로 앞에서 뒤로 배열합니다.\n\n<img src=\"/assets/img/2024-06-23-AdvancedRAG09PromptCompression_15.png\" />\n\n## 동적 압축 비율\n\n<div class=\"content-ad\"></div>\n\n문서마다 중요 정보 밀도가 다르기 때문에, 질문과 관련이 더 많은 문서에는 더 많은 예산(즉, 더 낮은 압축 비율)을 할당해야 합니다.\n\nLongLLMLingua는 굵은 압축에서 중요도 점수를 사용하여 미세한 압축 과정 중에 예산 분배를 안내합니다.\n\n특히, LLMLingua의 예산 컨트롤러를 사용하여 보유 문서의 초기 예산을 설정합니다. 그런 다음, 미세 압축 단계에서 각 문서에 압축 예산을 동적으로 할당합니다. 이 할당은 해당 문서의 중요도 점수의 순위 지수에 기반하며, 이는 굵은 압축 단계 중에 결정됩니다.\n\nLongLLMLingua는 적응적 할당을 위해 선형 스케줄러를 사용하며, 각 토큰 xi의 예산을 다음과 같이 정의할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_16.png)\n\n여기서 Nd는 문서의 수를 나타내며, δτ는 동적 할당의 총 예산을 제어하는 하이퍼파라미터입니다.\n\n해당 코드는 get_dynamic_compression_ratio 함수에서 찾을 수 있습니다.\n\n## 서브시퀀스 복구 알고리즘\n\n\n<div class=\"content-ad\"></div>\n\n제11 그림에 나와 있듯이, 세밀한 토큰별 압축 과정에서 주요 엔티티의 일부 토큰이 폐기될 수 있습니다. 예를 들어, 원본 프롬프트의 \"2009\"는 \"209\"로 압축될 수 있고, \"Wilhelm Conrad Rontgen\"은 \"Wilhelmgen\"으로 압축될 수 있습니다.\n\n![그림 1](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_17.png)\n\nLongLLMLingua는 LLM의 응답에서 원본 콘텐츠를 복구할 수 있는 서열 복원 알고리즘을 제안했습니다. 이에 대한 자세한 내용은 제12 그림에 나와 있습니다.\n\n![그림 2](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_18.png)\n\n<div class=\"content-ad\"></div>\n\n기본 프로세스는 다음 단계를 포함합니다:\n\n- LLMs 응답에서 토큰 yl을 트래버스하고 압축된 프롬프트 x˜에 나타나는 가장 긴 부분 문자열 y˜key,l을 선택합니다.\n- 원래의 프롬프트 x에서 y˜key,l에 해당하는 최대 공통 최단 부분수열 xi,j를 찾습니다.\n- LLMs 응답에서 해당하는 토큰 y˜key,l을 xi,j로 대체합니다.\n\n해당 코드는 함수 recover에서 찾을 수 있습니다.\n\n## 코드 데모스트레이션\n\n<div class=\"content-ad\"></div>\n\n환경 설정 방법은 LLMLingua와 같습니다. 여기에 시험용 코드가 있습니다:\n\n```js\nfrom llmlingua import PromptCompressor\n\nGSM8K_PROMPT = \"질문: 앤젤로와 멜라니는 다음 주에 임박한 시험을 위해 얼마나 많은 시간을 함께 공부해야 하는지 계획하려고 합니다. 그들은 공부할 교과서의 장을 2장과 암기해야 할 문제집을 4개 발견했습니다. 그들은 교과서의 장당 3시간, 문제집당 1.5시간을 할애해야 한다고 생각했습니다. 만약 그들이 하루에 최대 4시간을 공부할 계획이고 매 시간마다 10분 휴식을 취하며 매일 3회 10분 간식 휴식과 하루에 30분의 점심 시간을 포함한다면, 다음 주에 총 몇 일 동안 공부할 계획을 세워야 할까요?\\n단계별로 생각해 봅시다\\n앤젤로와 멜라니는 각 교과서 장에 3시간을 할애해야 한다고 생각했습니다. 2장 x 3시간 = 총 6시간.\\n문제집에는 문제집당 1.5시간을 할애할 계획이며 4개의 문제집이 있습니다. 1.5시간 x 4개 = 총 6시간.\\n앤젤로와 멜라니는 공부할 12시간의 계획을 시작해야 합니다. 하루에 4시간씩, 12 / 4 = 3일이 필요합니다.\\n하지만 휴식과 점심 시간을 고려해야 합니다. 매 시간에 10분 휴식을 취하고 싶어 시간당 총 120분의 공부시간이 있습니다.\\n그들은 또한 3회 10분 간식 휴식을 취하고 싶어 하며, 3 x 10분 = 30분.\\n그리고 매일 30분의 점심 시간을 포함하고 싶어 하여, 휴식을 위한 120분 + 간식 휴식 30분 + 점심 30분 = 180분, 혹은 180 / 60분 = 3시간이 더 필요합니다.\\n그래서 앤젤로와 멜라니는 공부할 12시간 + 휴식 3시간 = 총 15시간의 계획을 세우기를 원합니다.\\n하루에 최대 4시간씩 공부할 계획이며, 15시간 / 하루당 4시간 = 3.75\\n그들은 필요한 모든 시간을 고려하려면 공부할 일정이 4일이어야 합니다.\\n정답은 4입니다\\n\\n질문: 동일한 가격으로 4개의 사과나 1개의 수박을 구입할 수 있습니다. 주인은 주전에 도전하기로 결심합니다. 그는 $80,000에 집을 사고 $50,000의 수리비를 들였습니다. 집 값은 150% 증가했습니다. 그는 얼마의 이익을 냈습니까?\"\n\nQUESTION = \"질문: 조쉬는 집을 뒤집어 보기로 결정합니다. 그는 $80,000에 집을 사고 $50,000을 수리합니다. 이로 인해 집 값이 150% 증가했습니다. 그가 얼마의 이익을 냈습니까?\"\n\nllm_lingua = PromptCompressor()\n\ncompressed_prompt = llm_lingua.compress_prompt(\n    GSM8K_PROMPT.split(\"\\n\\n\")[0],\n    question = QUESTION,\n    condition_in_question = \"after_condition\",\n    reorder_context = \"sort\",\n    dynamic_context_compression_ratio = 0.3,\n    condition_compare = True,\n    context_budget = \"+100\",\n    rank_method = \"longllmlingua\",\n)\n\nprint('-' * 100)\nprint(\"original:\")\nprint(GSM8K_PROMPT.split(\"\\n\\n\")[0])\n\nprint('-' * 100)\nprint(\"compressed_prompt:\")\nprint(compressed_prompt)\n```\n\n실행 결과는 아래 그림에서 확인할 수 있습니다:\n\n<img src=\"/assets/img/2024-06-23-AdvancedRAG09PromptCompression_19.png\" />\n\n<div class=\"content-ad\"></div>\n\n# AutoCompressor\n\n이전에 언급된 방법과는 다르게, AutoCompressor는 부드러운 프롬프트 기반 접근 방식입니다.\n\n기존 모델을 스마트하게 세밀하게 조정하여 어휘를 확장하고 \"요약 토큰\" 및 \"요약 벡터\"를 활용하여 컨텍스트 정보를 간추립니다.\n\n![이미지](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_20.png)\n\n<div class=\"content-ad\"></div>\n\nFigure 14에는 AutoCompressor의 아키텍처가 나와 있습니다. AutoCompressor는 다음 단계에서 작동합니다:\n\n- 어휘 확장: 이 단계는 모델의 기존 어휘에 \"요약 토큰\"을 추가하는 작업을 포함합니다. 이러한 토큰을 사용하면 모델이 대량의 정보를 더 작은 벡터로 압축할 수 있습니다.\n- 문서 분할: 처리할 문서를 작은 세그먼트로 나누고, 각 세그먼트에는 요약 토큰이 추가됩니다. 이러한 토큰은 이전 세그먼트들의 요약 정보도 함께 운반하여 요약 누적을 만듭니다.\n- 미세 조정 훈련: AutoCompressor는 \"다음 단어 예측\" 작업을 활용하여 준지도 학습 방법을 사용하여 모델을 미세 조정합니다. 이 작업의 목표는 현재 토큰 앞의 토큰들과 현재 세그먼트 앞의 세그먼트들의 요약 벡터를 기반으로 다음 단어를 예측하는 것입니다.\n- 역전파: AutoCompressor는 각 세그먼트에 대해 시간을 거슬러가는 역전파(BPTT)와 그래디언트 체크포인팅을 사용하여 계산 그래프의 크기를 최소화합니다. 모델이 전체 문서에 대해 역전파를 수행하므로 전체 컨텍스트의 연관성을 학습할 수 있습니다.\n\n## 코드\n\nAutoCompressor는 코드를 제공하고 있으며, 관심 있는 독자들은 시도해볼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```python\nimport torch\nfrom transformers import AutoTokenizer\nfrom auto_compressor import LlamaAutoCompressorModel, AutoCompressorModel\n\n# 6천개의 토큰을 4개의 압축 단계로 압축하여 훈련된 AutoCompressor를 로드합니다.\ntokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/AutoCompressor-Llama-2-7b-6k\")\n# Llama 모델을 실행하려면 bfloat16 + cuda가 필요합니다.\nmodel = LlamaAutoCompressorModel.from_pretrained(\"princeton-nlp/AutoCompressor-Llama-2-7b-6k\", torch_dtype=torch.bfloat16).eval().cuda()\n\nprompt = '현재 미국 대통령의 이름은 \"'\nprompt_tokens = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids.cuda()\n\ncontext = \"\"\"조 바이든(Joe Biden)은 1942년 11월 20일 필라델피아 주 스크랜턴(Scranton)에서 태어나 중산층 가정에서 자랐습니다. 그는 델라웨어 대학을 졸업하며 역사와 정치학을 복수전공했습니다. 그 후 1968년에 실라큐스 대학 로 스쿨에서 법학 학위를 취득했습니다.\n바이든의 정치 경력은 1970년 델라웨어주 뉴캐슬 카운티 의회의 의원으로 선출되면서 시작되었습니다. 1972년 그의 아내 네일리아와 1세 딸 나오미가 차량 사고로 사망하고 아들 베아우와 헌터가 다쳤습니다. 이러한 비극적인 상황에도 불구하고 바이든은 자신의 약속을 지키기로 선택하고 그의 아들들의 침대 옆에서 상원 의원으로 취임했습니다.\n바이든은 1973년부터 2009년까지 델라웨어 주 상원의원으로 6회 임기를 보냈습니다. 상원 의원으로 활동하는 동안 바이든은 여러 위원회에 참여했으며 외교 문제에 대한 지식으로 유명했습니다. 그는 여러 차례 상원 외교위원회 위원장으로 활약했습니다.\n2008년 조 바이든은 대통령 선거에서 버락 오바마의 동역자로 선택되었습니다. 부통령으로 활동하면서 바이든은 오바마 행정부에서 정책 수립과 경제 회복, 외교 문제, Affordable Care Act (ACA, 일명 오바마케어) 시행 등의 문제를 다루는 데 중요한 역할을 했습니다.\n부통령으로 두 기간을 재직한 후 조 바이든은 2020년 대통령 선거에 출마하기로 결정했습니다. 그는 민주당 후보로 확정되어 재집권 중인 도널드 트럼프 대통령과 대선에서 맞붙었습니다. 바이든은 통일을 약속하며 코로나19 대유행, 기후 변화, 인종 정의, 미국 내 부의 불평등 등을 포함한 여러 중요 문제에 대처하겠다고 공약했습니다.\n2020년 11월 선거에서 바이든은 승리하고 2021년 1월 20일에 미국 46대 대통령으로 취임했습니다. 78세의 나이로 바이든은 미국 역사상 최고령 대통령이 되었습니다.\n대통령으로서 조 바이든은 인프라 투자, 기후 변화 대책, 이민 개혁, 의료 서비스 접근성 확대 등을 중점으로 한 그의 정책을 실행하려고 노력했습니다. 그는 국제 관계에서 외교의 중요성을 강조했고 전 세계 파트너와의 동맹을 새롭게 구축하려고 노력했습니다.\n공공분야의 긴 경력 동안 조 바이든은 양당 간의 협력, 공감, 노동 계층 문제에 대한 헌신으로 인해 인정받았습니다. 그는 국가를 향한 직면한 난제를 극복하며 나라를 하나로 이끄고 모든 미국인을 위한 긍정적인 변화를 만들기 위해 계속 노력하고 있습니다.\"\"\"\ncontext_tokens = tokenizer(context, add_special_tokens=False, return_tensors=\"pt\").input_ids.cuda()\n\nsummary_vectors = model(context_tokens, output_softprompt=True).softprompt\nprint(f\"{context_tokens.size(1)}개의 토큰을 {summary_vectors.size(1)}개의 요약 벡터로 압축 중\")\n# >>> 660개의 토큰을 50개의 요약 벡터로 압축 중\n\ngeneration_with_summary_vecs = model.generate(prompt_tokens, do_sample=False, softprompt=summary_vectors, max_new_tokens=12)[0]\nprint(\"요약 벡터 사용해서 생성:\\n\" + tokenizer.decode(generation_with_summary_vecs))\n# >>> 현재 미국 대통령의 이름은 \"조\"이며 성은 \"바이든\"입니다.\n\nnext_tokens_without_context = model.generate(prompt_tokens, do_sample=False, max_new_tokens=11)[0]\nprint(\"컨텍스트 없이 생성:\\n\" + tokenizer.decode(next_tokens_without_context))\n# >>> 현재 미국 대통령의 이름은 \"도널드\"이며 성은 \"트럼프\"입니다.\n```\n\n# LLMLingua-2\n\nLLMLingua-2는 LLaMa-7B와 같은 인과 언어 모델로부터 정보 엔트로피를 기반으로 토큰 또는 어휘 단위를 삭제하여 프롬프트를 압축하는 데 두 가지 문제를 식별합니다:\n\n(1) 정보 엔트로피를 결정하는 소형 언어 모델이 프롬프트 압축 목표와 일치하지 않습니다.\n\n\n<div class=\"content-ad\"></div>\n\n(2) 양방향 컨텍스트를 활용하지 않고 있어서, 빠른 압축을 위해 필요한 모든 정보를 포괄하지 못할 수 있습니다.\n\n이 문제들의 핵심은 정보 엔트로피가 압축에 대한 최적의 척도가 아닐 수 있다는 것입니다.\n\nLLMLingua-2의 전체 아키텍처는 다음과 같이 그림 15에 나와 있습니다:\n\n![LLMLingua-2 Architecture](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_21.png)\n\n<div class=\"content-ad\"></div>\n\n이슈 1을 해결하기 위해 LLMLingua-2는 데이터 증류 과정을 도입했습니다. 이 과정은 LLM에서 지식을 추출하여 중요 정보를 잃지 않으면서 프롬프트를 압축합니다. 동시에 추출형 텍스트 압축 데이터셋을 구축합니다. 이 데이터셋으로 훈련을 진행하면 작은 언어 모델이 프롬프트 압축에 효과적으로 정렬될 수 있습니다.\n\n이슈 2를 해결하기 위해 LLMLingua-2는 프롬프트 압축을 토큰 분류 문제로 다룹니다. 이 접근 방식은 압축된 프롬프트가 원래 프롬프트와 충실하다는 것을 보장합니다. 전이 전체 양방향 컨텍스트에서 프롬프트 압축을 위한 모든 필요한 정보를 캡처하기 위해 트랜스포머 인코더를 사용합니다.\n\n## 효과적인 프롬프트 압축 데이터셋 구축 방법\n\n데이터 증류\n\n<div class=\"content-ad\"></div>\n\n데이터 정제는 GPT-4와 같은 대규모 언어 모델에서 지식을 추출하여 필수 정보를 잃지 않고 프롬프트를 효과적으로 압축하는 것을 의미합니다.\n\nLLMLingua-2에서는 Figure 16에 나와 있는 것처럼 주의 깊게 설계된 지침을 따릅니다. 이러한 지침은 GPT-4로 하여금 원본 텍스트에서 비필수 단어를 제외하고 생성 프로세스 중에 새로운 단어를 추가하지 않으면서 텍스트를 압축하도록 요구합니다.\n\n동시에, 이러한 지침은 압축 비율 제한을 부과하지 않습니다. 대신, GPT-4에게 최대한 많은 정보를 유지한 채 원본 텍스트를 최대한 압축하도록 요청합니다.\n\n<img src=\"/assets/img/2024-06-23-AdvancedRAG09PromptCompression_22.png\" />\n\n<div class=\"content-ad\"></div>\n\n그림 17에서 보듯이, GPT-4는 매우 긴 컨텍스트를 처리할 때 종종 높은 압축 비율을 적용합니다. 이는 긴 컨텍스트를 처리하는 능력이 제한되어 있기 때문일 수 있습니다. 이러한 공격적인 압축은 상당한 정보 손실로 이어지며, 후속 작업의 성능에 상당한 영향을 미칩니다.\n\n![그림](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_23.png)\n\n이 문제를 해소하기 위해 LLMLingua-2는 긴 텍스트를 512 토큰을 초과하지 않는 여러 청크로 나누는 청크 압축 방법을 채택했으며, 그런 다음 GPT-4에게 각 블록을 따로 압축하도록 안내합니다.\n\n데이터 주석화\n\n<div class=\"content-ad\"></div>\n\n현재 데이터 축소를 통해 원본 텍스트와 해당 압축 버전의 쌍을 확보하였습니다. 데이터 주석 작업의 목표는 원본 텍스트의 각 토큰에 이진 레이블을 할당하는 것입니다. 이는 압축 후 해당 토큰을 유지해야 하는지를 결정합니다.\n\nGPT-4가 지시에 엄격하게 따르지 않을 수 있기 때문에, LLMLingua-2는 검색 범위를 제한하기 위해 슬라이딩 윈도우 기술을 사용합니다. 또한 GPT-4의 압축 과정 중 원본 단어에 잠재적인 변경이 발생하는 것을 다루기 위해 퍼지 매칭을 사용합니다.\n\n품질 관리\n\nLLMLingua-2는 GPT-4 축산 및 자동 주석 레이블의 생성된 압축 텍스트의 품질을 평가하기 위해 Variation Rate(VR) 및 Alignment Gap(AG) 두 개의 품질 제어 메트릭을 사용합니다.\n\n<div class=\"content-ad\"></div>\n\nVariation Rate은 압축된 텍스트와 원본 텍스트 중 다른 단어의 백분율을 측정합니다. Alignment Gap은 자동 주석 단어의 품질을 평가합니다.\n\nLLMLingua-2는 이러한 측정치를 사용하여 낮은 품질의 샘플을 제외함으로써 데이터셋의 품질을 보장할 수 있습니다.\n\n## Compressor\n\n이진 분류 문제로 간주됩니다.\n\n<div class=\"content-ad\"></div>\n\n처음에는 프롬프트 압축 문제를 이진 분류 문제로 변환할 수 있습니다. 기본 개념은 각 어휘 단위를 독립적인 엔티티로 고려하고 \"유지(preserve)\" 또는 \"폐기(discard)\" 라벨을 할당하는 것입니다. 이 방법은 압축된 프롬프트의 내용의 무결성을 보존하면서 모델의 설계를 간소화합니다.\n\n모델 아키텍처\n\n트랜스포머 인코더 기반 특징 인코더가 사용되고 상단에 선형 분류 레이어가 추가됩니다.\n\n이 아키텍처는 각 어휘 단위의 양방향 컨텍스트 정보를 캡처하여 압축 작업에 필수적인 정보를 제공할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n압축 전략\n\n원래 프롬프트 x의 압축 전략은 세 단계로 구성되어 있습니다. 목표 압축 비율은 1/τ로 정의되며, 여기서 τ는 압축된 프롬프트의 단어 수와 원래 프롬프트 x의 단어 수를 나눈 값입니다.\n\n- 먼저, 압축된 프롬프트 x˜에 유지할 토큰의 목표 수를 결정합니다: N˜ = τN.\n- 그런 다음, 토큰 분류 모델을 사용하여 각 단어 xi가 '보존'으로 표시될 확률 pi를 예측합니다.\n- 마지막으로, 원래 프롬프트 x에서 확률 pi 값이 가장 높은 상위 N˜개의 단어를 보존하여 원래 순서를 유지하고 압축된 프롬프트 x˜를 형성합니다.\n\n## 코드\n\n<div class=\"content-ad\"></div>\n\n위에서 볼 수 있듯이, LLMLingua-2의 주요 작업은 압축기를 구축하는 것입니다. 그렇다면 한 번 획득한 압축기를 어떻게 활용할 수 있을까요?\n\n아래 코드를 참조해 주세요(LLMLingua와 환경 설정 방법은 같습니다). 주요 내부 프로세스는 compress_prompt_llmlingua2 함수에서 확인할 수 있습니다.\n\n```js\nfrom llmlingua import PromptCompressor\n\nPROMPT = \"John: So, um, I've been thinking about the project, you know, and I believe we need to, uh, make some changes. I mean, we want the project to succeed, right? So, like, I think we should consider maybe revising the timeline.\\n\\nSarah: I totally agree, John. I mean, we have to be realistic, you know. The timeline is, like, too tight. You know what I mean? We should definitely extend it.\"\n\nllm_lingua = PromptCompressor(\n    model_name=\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\",\n    use_llmlingua2=True,\n)\ncompressed_prompt = llm_lingua.compress_prompt(PROMPT, rate=0.33, force_tokens=['\\n', '?'])\n\n## 또는 LLMLingua-2-small 모델 사용\n# llm_lingua = PromptCompressor(\n#     model_name=\"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\",\n#     use_llmlingua2=True,\n# )\n\nprint('-' * 100)\nprint(\"original:\")\nprint(PROMPT)\n\nprint('-' * 100)\nprint(\"compressed_prompt:\")\nprint(compressed_prompt)\n```\n\n실행 결과는 다음과 같이 나타납니다:\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-AdvancedRAG09PromptCompression_24.png)\n\n# RECOMP\n\nRECOMP은 추출 및 요약 두 가지 유형의 훈련된 압축기를 소개합니다. 추출 압축기는 검색된 문서에서 유용한 문장을 선택하고, 추상적 압축기는 여러 문서에서 정보를 결합하여 요약을 생성합니다.\n\nFigure 19은 RECOMP에서 압축기의 위치를 보여줍니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-23-AdvancedRAG09PromptCompression_25.png\" />\n\n## 추출 압축기\n\n입력 문서 세트에서 n개의 문장 [s1, s2, …, sn]가 주어지면, 이중 인코더 모델을 학습합니다. 이 모델은 문장 si와 입력 시퀀스 x를 고정 차원 임베딩으로 임베딩합니다. 이러한 임베딩의 내적은 입력 x에 si를 추가하여 대상 출력 시퀀스를 생성할 때 LLM에게 얻는 혜택을 나타냅니다.\n\n압축기에서 최종 요약 s는 입력과의 내적에 따라 순위가 매겨진 상위 N개의 문장으로 구성됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 추상 압축기\n\n추상 압축기는 인코더-디코더 모델입니다. 입력 시퀀스 x와 검색된 문서 세트의 연결을 가져와서 요약 s를 출력합니다.\n\n이 방법은 LLM(예: GPT-3과 비슷한 모델)을 사용하여 훈련 데이터 세트를 생성하고 이 데이터를 필터링한 다음 필터링된 데이터 세트로 인코더-디코더 모델을 훈련하는 것을 포함합니다.\n\n## 코드\n\n<div class=\"content-ad\"></div>\n\nRECOMP의 코드는 아직 초기 단계에 있기 때문에 여기에서는 시연하지 않습니다. 관심 있는 독자들은 직접 시도해 볼 수 있습니다.\n\n# 결론\n\n이 기사에서는 프롬프트 압축 방법을 도입하였습니다. 방법 분류, 알고리즘 원칙 및 코드 설명에 대한 정보가 포함되어 있습니다.\n\n논의된 방법 중 LongLLMLingua가 우수한 선택일 수 있습니다. 이미 연구 프로젝트에서 구현하였습니다. LongLLMLingua의 단점이나 더 나은 방법이 발견되면 기사를 업데이트하겠습니다. 또한, LLMLingua-2도 시도해볼 수 있으며, 속도와 메모리 사용에 이점을 가지고 있습니다.\n\n<div class=\"content-ad\"></div>\n\nRAG 기술에 관심이 있다면, 제 다른 기사들도 확인해보세요.\n\n그리고 최신 AI 관련 콘텐츠는 제 뉴스레터에서 찾을 수 있어요.\n\n마지막으로, 이 기사에 오류나 누락된 내용이 있다면 또는 궁금한 점이 있다면 댓글 섹션에 남겨주세요.","ogImage":{"url":"/assets/img/2024-06-23-AdvancedRAG09PromptCompression_0.png"},"coverImage":"/assets/img/2024-06-23-AdvancedRAG09PromptCompression_0.png","tag":["Tech"],"readingTime":32},{"title":"LLMs에서 Temperature와 Top-K를 사용하는 실용 가이드","description":"","date":"2024-06-23 19:01","slug":"2024-06-23-ThePracticalGuidetoUsingTemperatureandTop-KwithLLMs","content":"\n\n대형 언어 모델(LLMs)은 자연 언어 처리를 혁신적으로 바꿨지만, 그들의 전체 잠재력을 이용하기 위해서는 주요 파라미터를 이해하는 것이 필요합니다.\n\n본 안내서는 두 가지 핵심 설정인 온도와 상위-K를 해석해 줍니다. 온도와 상위-K가 무엇인지, 어떻게 작동하는지, 그리고 최적 결과를 얻기 위해 각각을 언제 사용해야 하는지 살펴볼 것입니다.\n\n# 온도란 무엇인가요?\n\n온도는 LLM의 출력의 무작위성을 제어합니다. 일반적으로 0과 1 사이로 설정됩니다.\n\n<div class=\"content-ad\"></div>\n\n- 낮은 온도(0에 가까운): 더 예측 가능하고 집중된 응답을 생성합니다.\n- 높은 온도(1에 가까운): 더 다양하고 창의적이며 때로는 예측할 수 없는 출력물을 생성합니다.\n\n기술적으로 온도는 모델의 다음 토큰 예측의 확률 분포를 수정합니다. 이는 소프트맥스 함수를 적용하기 전에 로짓(정규화되지 않은 예측 점수)를 온도 값으로 나누는 방식으로 수행됩니다:\n\n![image](/assets/img/2024-06-23-ThePracticalGuidetoUsingTemperatureandTop-KwithLLMs_0.png)\n\n위와 같습니다.\n\n<div class=\"content-ad\"></div>\n\n- i는 우리가 확률을 계산하고 있는 특정 토큰을 나타냅니다.\n- j는 어휘 중 모든 토큰을 반복하는 합산에 사용됩니다.\n- x_i는 토큰 i의 로짓(unnormalized score)을 의미합니다.\n- x_j는 어휘 내 모든 토큰 j의 로짓들을 나타냅니다.\n- P(x_i)는 토큰 i의 확률을 나타냅니다.\n- T는 온도(temperature)를 의미합니다.\n- V는 어휘(vocabulary)를 나타냅니다.\n\nT가 0에 가까워질수록, 분포는 좀 더 뾰족해지며 높은 확률을 가진 토큰을 강하게 선호합니다. T가 커지면, 분포는 평평해지며 낮은 확률을 갖는 토큰들이 선택될 가능성이 높아집니다.\n\n![image](/assets/img/2024-06-23-ThePracticalGuidetoUsingTemperatureandTop-KwithLLMs_1.png)\n\n# Top-K란 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\nTop-k 샘플링은 언어 모델에서 텍스트를 생성하는 데 사용되는 전략입니다. 여기에는 모든 가능한 단어를 고려하는 대신 상위 K개 가장 확률이 높은 단어에서 다음 단어를 선택하는 과정이 포함됩니다. 이 방법은 무작위성과 의미 있는 출력 사이의 균형을 도와줍니다. 아래는 이 방법이 작동하는 방식입니다:\n\n- 확률 분포: 모델이 시퀀스에서 다음 단어에 대한 어휘에 대한 확률 분포를 예측합니다.\n- 상위 K 선택: 가장 높은 확률을 가진 상위 K개의 단어만 고려됩니다.\n- 무작위 샘플링: 그 다음 이 상위 K개의 단어 중에서 단어가 그들의 확률에 기반하여 무작위로 선택됩니다.\n\n![이미지](/assets/img/2024-06-23-ThePracticalGuidetoUsingTemperatureandTop-KwithLLMs_2.png)\n\nTop-K는 샘플링 풀의 크기를 제어하는 직접적인 방법을 제공합니다. K가 작을수록 보다 집중된 결과를 얻게 되지만, K가 클수록 더 다양성을 허용하지만 덜 관련성 있는 토큰을 포함할 위험이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 방법은 모델이 매우 불가능하거나 터무니없는 토큰을 선택하는 것을 방지하는 데 특히 효과적일 수 있습니다, 특히 K값이 비교적 낮게 설정된 경우. 그러나 이 방법은 상황에 따라 어휘 크기에 어려움을 겪을 수 있으며, 경우에 따라 너무 제한적일 수도 있고 다른 경우에는 너무 허용될 수 있습니다.\n\n# 온도, Top-K 또는 둘 다를 언제 사용해야 할까요?\n\n이 샘플링 방법을 적용해야 하는 시점을 이해하는 것은 귀하의 LLM 출력을 최적화하기 위해 중요합니다. 다음은 선택하는 데 도움이 되는 안내서입니다:\n\n## 온도\n\n<div class=\"content-ad\"></div>\n\n- 사실적 정확성이 필요한 작업(예: 질의응답 또는 데이터 추출)에는 낮은 온도(0.1-0.3)를 사용하세요.\n- 창의성과 일관성이 균형있는 일반 대화 또는 콘텐츠 생성에는 중간 온도(0.4-0.7)를 사용하세요.\n- 브레인스토밍, 시에 관한 높은 온도(0.8-1.0)를 필요로 하는 시나, 최대 창의력을 요구하는 작업에는 고온도를 사용하세요.\n\n### 상위-K\n\n- 가장 가능성 높은 단어로 모델을 제한하고 싶을 때는 낮은 K 값(10-50)을 사용하세요. 집중적이고 결정론적인 출력에 유용합니다.\n- 매우 불가능한 토큰의 선택을 방지하면서 더 많은 다양성을 허용하려면 더 높은 K 값(100-1000)을 사용하세요.\n- 상위-K는 특히 짧은 시퀀스의 출력 품질을 유지하는 데 유용합니다.\n\n## 온도와 상위-K 결합\n\n<div class=\"content-ad\"></div>\n\n- 이러한 방법들을 조합하면 생성된 텍스트의 다양성과 품질을 세밀하게 조절할 수 있어요.\n- 확률적으로 발생할 가능성이 낮은 토큰을 걸러내기 위해 중간 K 값 (예: 50–100)을 사용한 후, 해당 하위 집합 내에서 무작위성을 조절하기 위해 온도를 적용해주세요.\n- 이 조합은 종종 각각의 방법을 단독으로 사용하는 것보다 안정적이며, 특히 더 긴 생성 작업에 적합해요.\n\n## 조합 사용 지침\n\n- 중간 K 값 (예: 50)와 온도 (예: 0.7)로 시작해주세요.\n- 출력물이 너무 무작위적이거나 주제와 관련이 없다면 K 값을 줄이거나 온도를 낮춰주세요.\n- 출력물이 너무 반복적이거나 예측 가능하다면 K 값을 늘리거나 온도를 높여주세요.\n- 특정 사용 사례 및 원하는 출력 특성에 기반하여 세밀하게 조정해주세요.\n\n기억해주세요, 이상적인 설정은 작업, 모델 및 원하는 결과에 따라 다릅니다. 실험을 통해 응용 프로그램에 최적의 균형을 찾는 것이 중요해요. 결과를 모니터링하고 적절하게 조정하여 LLM 출력물에서 일관성, 관련성 및 창의성의 최적 조합을 달성해주세요.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n온도 조절과 Top-K 샘플링을 숙달하는 것이 LLM 성능을 최적화하는 핵심입니다. 온도는 출력 무작위성을 직관적으로 조절하며, Top-K는 토큰 품질을 직접 관리합니다. 이러한 방법을 결합하면 종종 최상의 결과를 얻을 수 있으며, 창의성과 일관성 사이의 미묘한 균형을 제공합니다.\n\n기억하세요, 만능 해결책은 없습니다. 이상적인 설정은 당신의 특정 작업, 모델 및 원하는 결과에 따라 다릅니다. 이러한 매개변수를 실험하여 응용 프로그램에 완벽한 균형을 찾아보세요. 연습을 통해 언어 모델의 가능성을 최대한 활용하는 기술을 개발하여 자연 언어 생성의 경계를 넓힐 수 있을 것입니다.","ogImage":{"url":"/assets/img/2024-06-23-ThePracticalGuidetoUsingTemperatureandTop-KwithLLMs_0.png"},"coverImage":"/assets/img/2024-06-23-ThePracticalGuidetoUsingTemperatureandTop-KwithLLMs_0.png","tag":["Tech"],"readingTime":4},{"title":"요청콘텐츠 스타일을 제한하는 50가지 두 단어 ChatGPT 프롬프트 예시  파트 2","description":"","date":"2024-06-23 19:00","slug":"2024-06-23-50Two-wordChatGPTpromptsthatdontsuggestcontentbutlimittheRequestStyleofContentPart-2","content":"\n\n![이미지](/assets/img/2024-06-23-50Two-wordChatGPTpromptsthatdontsuggestcontentbutlimittheRequestStyleofContentPart-2_0.png)\n\n챗GPT 프롬프트 트릭을 모두 알고 있다고 생각했나요? 다시 한 번 생각해 보세요! 두 단어 톤 프롬프트의 첫 번째 부분으로 마음을 끌었는데, AI의 목소리를 제어하는 데 사용될 수 있는 50가지 더 놀라운 방법으로 돌아왔습니다.\n\n어리석은 이야기부터 심각한 보고서까지, 이러한 소프트 프롬프트는 AI의 톤과 개성을 제어할 수 있게 해줬어요.\n\n우리는 \"재미있는 이야기\"와 같은 단순한 분위기를 넘어, 몇 가지 더 황당한 단계를 올려가고 있어요.\n\n<div class=\"content-ad\"></div>\n\nChatGPT가 다양한 주제에 대해 다양한 답변을 제공할 수 있다는 걸 알고 계시나요?\n\n그 쿨한 트릭을 알아? 정확히 원하는 유형의 응답을 받는 방법이 있는데, 그 방법은 두 단어만 사용하는 것이야!\n\n놀라운 ChatGPT 프롬프트를 사용하여 글을 쓰고 최적화하는 것도 가능해.\n\n이러한 프롬프트를 사용하면 ChatGPT가 자유자재로 코드를 전환하면서 자유스럽게 랩퍼처럼 라임을 맞춰주거나, 고대의 현자처럼 철학적으로 이야기하거나, 앤드루 다이스 클레이를 부끄러워하게 할만한 욕설로 가득한 욕설을 할 수 있어. 그건 시작에 불과해!\n\n<div class=\"content-ad\"></div>\n\n- “전략 개요”\n- “예측 제공”\n- “이론 설명”\n- “개요 제공”\n- “비평 제공”\n- “접근 방법 설명”\n- “요약 제공”\n- “기법 비교”\n- “목표 개요”\n- “평가 제공”\n- “개념 설명”\n- “세부 내용 제공”\n- “권장 사항 제공”\n- “근거 설명”\n- “평가 제공”\n- “접근 방법 비교”\n- “방법론 개요”\n- “결론 제공”\n- “현상 설명”\n- “제안 제공”\n- “통찰력 제공”\n- “제안 제공”\n- “논리 설명”\n- “추가 설명”\n- “틀 제공”\n- “결과 비교”\n- “계획 개요”\n- “전망 제공”\n- “영향 설명”\n- “분석 제공”\n- “관점 제시”\n- “과정 설명”\n- “세부 내용 제공”\n- “구조 개요”\n- “요약 제공”\n- “해결책 제시”\n- “추론 설명”\n- “배경 제공”\n- “더 자세히 설명”\n- “대안 제시”\n- “참고 자료 제공”\n- “주요 요점 요약”\n- “타임라인 제공”\n- “이유 설명”\n- “정의 제공”\n- “제품 비교”\n- “요약 작성”\n- “방법론 설명”\n- “단계 개요”\n- “지시 제공”\n\n<div class=\"content-ad\"></div>\n\n뭐 기다려? 이 살상적인 프롬프트들을 꽂아보면서 끊임없는 대화, 이야기, 그리고 AI 들어끼기의 새로운 세계를 즐겨보세요.\n\n그럼 어서 뭐 기다리고 있어? 이 살상적인 프롬프트들을 꽂아보면서 끊임없는 대화, 이야기, 그리고 AI 들어끼기의 새로운 세계를 즐겨보세요.\n\n---\n\n이 글을 즐겼다면 몇 번의 박수👏를 해주시고 주변에 공유해주세요!\n\n<div class=\"content-ad\"></div>\n\n웹진이 팔로우해 주셔서 감사합니다!\n\n더 많은 멋진 콘텐츠가 곧 공개될 예정이니, 저를 여기 중간에서 팔로우해 주세요.\n\n아래 댓글에 의견과 피드백을 남겨주세요!\n\n건배! 🥂\n\n<div class=\"content-ad\"></div>\n\n안녕하세요! 웹진니(Webjinnee 창립자) Nitin입니다. 다음에 만나요! ✌️\n\n#챗GPT해킹 #프롬프트파워 #톤트릭스 #AI워드크래프트 #최소프롬프트 #AI글쓰기팁 #챗GPT프롬프트","ogImage":{"url":"/assets/img/2024-06-23-50Two-wordChatGPTpromptsthatdontsuggestcontentbutlimittheRequestStyleofContentPart-2_0.png"},"coverImage":"/assets/img/2024-06-23-50Two-wordChatGPTpromptsthatdontsuggestcontentbutlimittheRequestStyleofContentPart-2_0.png","tag":["Tech"],"readingTime":2},{"title":"OpenAI  토큰을 사용하는 최고의 방법","description":"","date":"2024-06-23 18:58","slug":"2024-06-23-OpenAIBestPracticesofUsingTokens","content":"\n\n<img src=\"/assets/img/2024-06-23-OpenAIBestPracticesofUsingTokens_0.png\" />\n\n# 오픈AI 토큰이란\n\n오픈AI의 고급 언어 모델인 GPT-3.5 및 GPT-4와 같은 분야에서 \"토큰\"이란 텍스트에서 함께 자주 나타나는 문자 시퀀스를 가리킵니다. 이러한 모델은 이러한 토큰 간의 통계적 관계를 이해하고 예측하는 데 설계되어 있습니다.\n\n텍스트를 토큰으로 분해하는 프로세스는 다른 모델 간에 다를 수 있습니다. 예를 들어, GPT-3.5와 GPT-4는 이전 모델과 달리 다른 토큰화 프로세스를 사용하여 입력 텍스트에 대해 다른 토큰을 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n일반적으로 한 토큰은 영어 텍스트의 네 문자에 해당하는 양으로, 대략 세 분의 삼분의 이 하나의 단어와 거의 비슷합니다. 따라서, 100개의 토큰은 대략 75단어에 해당합니다.\n\n예를 들어, \"OpenAI is great!\"이라는 문장을 생각해 봅시다. 이 문장에서 토큰은 다음과 같이 분리될 수 있습니다:\n\n[“Open”, “AI”, “ is”, “ great”, “!”]\n\n![이미지](/assets/img/2024-06-23-OpenAIBestPracticesofUsingTokens_1.png)\n\n<div class=\"content-ad\"></div>\n\n여기 각각이 토큰으로 간주됩니다. 모델에서 사용하는 구체적인 토큰화 프로세스에 따라 정확한 분할이 다를 수 있습니다. 예를 들어, 일부 모델은 \"OpenAI\"를 하나의 토큰으로 처리할 수 있지만, 다른 모델은 \"Open\"과 \"AI\"로 분할할 수도 있습니다. 마찬가지로, 공백과 구두점은 종종 별도의 토큰으로 처리됩니다. 그래서 이 예시에서는 다섯 개의 토큰이 있습니다: \"Open\", \"AI\", \" is\", \" great\", 그리고 \"!\".\n\n토큰 길이 개념을 이해하기 위한 유용한 가이드라인을 제시해드리겠습니다:\n\n- 1 토큰은 대략 영어로 4자와 동일합니다.\n- 1 토큰은 대략 단어의 3/4에 해당합니다.\n- 100 토큰은 약 75단어에 해당합니다.\n\n또는,\n\n<div class=\"content-ad\"></div>\n\n# 토큰 인코딩\n\n토큰 인코딩은 자연어 처리(NLP) 및 기계 학습에서 중요한 단계입니다. 이는 기계가 이해하고 작업할 수 있는 형식인 고정 차원의 수치 벡터로 변환하는 과정입니다.\n\n다른 토큰 인코딩은 서로 다른 모델에 연결되어 있으므로 텍스트를 토큰으로 변환할 때 어떤 모델을 사용할 지 고려해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n주어진 텍스트 문자열 (예: \"OpenAI is great!\")과 인코딩 (예: \"cl100k_base\")으로 토크나이저가 텍스트 문자열을 토큰 목록으로 분할할 수 있습니다 (예: [\"Open\", \"AI\", \" is\", \" great\", \"!\"]).\n\n다음 표는 토큰 인코딩 방법과 OpenAI 모델 간의 매핑을 보여줍니다:\n\n\n| 토큰 인코딩 방법 | OpenAI 모델 |\n|------------------|-------------|\n| cl100k_base      | 모델 1      |\n| cl200k_base      | 모델 2      |\n| cl500k_base      | 모델 3      |\n\n\n# 토큰화\n\n<div class=\"content-ad\"></div>\n\nOpenAI의 맥락에서 토큰화는 텍스트를 더 작은 조각, 즉 토큰으로 분리하는 방법입니다. 이 토큰들은 텍스트에서 함께 자주 나타나는 문자 시퀀스로, OpenAI의 대형 언어 모델인 GPT-3.5 및 GPT-4 등에서 사용되어 텍스트를 처리하고 이해하는 데 활용됩니다.\n\nTiktoken은 OpenAI가 만든 기반 Python 도구입니다. 이 도구는 주로 OpenAI의 GPT-4와 같은 모델과 함께 작동하도록 설계된 빠른 바이트 페어 인코딩 (BPE) 토크나이저입니다. Tiktoken의 주요 기능은 텍스트를 더 작은 조각으로 나누어 모델이 텍스트를 처리하고 이해할 수 있도록 하는 것입니다.\n\n오픈 소스 도구인 Tiktoken은 pip install tiktoken 명령을 사용하여 PyPI에서 쉽게 설치할 수 있습니다. 또한 JavaScript 환경에서 사용할 수 있는 커뮤니티 지원 버전도 있습니다.\n\nTiktoken의 주요 기능 중 하나는 교육용 하위 모듈인데, 이 모듈은 BPE의 작동 방식을 이해하고 사용자가 토큰화 프로세스를 시각화할 수 있도록 도와줍니다. 또한 Tiktoken은 유연하며 새로운 인코딩 지원을 추가할 수 있도록 사용자에게 허용합니다.\n\n<div class=\"content-ad\"></div>\n\n예제 하나가 이렇게 보일 거예요:\n\n```python\nimport tiktoken\n\nencoding = tiktoken.get_encoding(\"cl100k_base\")\n# 특정 모델 이름에 해당하는 올바른 인코딩을 자동으로 로드하기 위해\nencoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n\nprint(encoding.encode(\"OpenAI is great!\"))\n```\n\n출력은 이렇게 보일 거예요:\n\n```python\n[5109, 15836, 374, 2294, 0]\n```\n\n<div class=\"content-ad\"></div>\n\n토큰을 세는 방법은 .encode()로 반환된 리스트의 길이를 세면 됩니다.\n\n# 토큰 한도\n\n요청에 사용할 수 있는 토큰의 최대 수는 선택한 모델에 따라 다르며, 입력 프롬프트 및 생성된 출력(gpt-3.5-turbo)에 대한 4096개의 토큰이라는 결합한 한도가 있습니다. 따라서, 입력에 4000개의 토큰을 할당하면 출력에는 최대 96개의 토큰이 남게 됩니다.\n\n이 제약은 주로 기술적인 이유로 인해 발생합니다. 그러나, 입력을 더 간결하게 요약하거나 콘텐츠를 더 작은 세그먼트로 나누는 등 이러한 제한 내에서 효과적으로 작업하는 다양한 전략이 존재합니다.\n\n<div class=\"content-ad\"></div>\n\nGPT4의 토큰 한도\n\n![Image](/assets/img/2024-06-23-OpenAIBestPracticesofUsingTokens_3.png)\n\n더 많은 정보를 알고 싶다면 openai의 공식 웹사이트를 방문해보세요: [여기를 클릭하세요](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)\n\n# 토큰 가격 설정\n\n<div class=\"content-ad\"></div>\n\nOpenAI API는 다양한 모델 유형을 제공하며 각 모델은 다른 가격 수준에서 사용할 수 있습니다. 이러한 모델들은 능력이 다양하며 가장 진보된 것은 다빈치이고, 가장 빠른 것은 에이다입니다. 요청을 만드는 데 드는 비용은 이러한 모델에 따라 달라집니다.\n\n예를 들어, GPT-4 Turbo 모델의 경우, 입력 기준으로 $0.01/1K 토큰, 출력 기준으로 $0.03/1K 토큰이 듭니다.\n\n표:\n\n![OpenAI 모델 비용](/assets/img/2024-06-23-OpenAIBestPracticesofUsingTokens_4.png)\n\n그리고 OpenAI에 따르면:\n\n<div class=\"content-ad\"></div>\n\nCobus Greyling님은 OpenAI 토큰 비용에 대한 멋진 차트를 보유하고 계시네요:\n\n![OpenAI Token Cost Chart](/assets/img/2024-06-23-OpenAIBestPracticesofUsingTokens_5.png)\n\n# 가격 계산기\n\n다음 \"OpenAI 및 다른 LLM API 가격 계산기\"를 활용하여 계산을 할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-23-OpenAIBestPracticesofUsingTokens_6.png)\n\n위는 1000개의 입력 단어, 500개의 출력 단어 및 100회의 API 호출에 대한 총 비용을 보여줍니다.\n\n# Best Practices\n\nOpenAI 토큰을 사용할 때는 최적의 방법을 채택하여 효율성을 극대화하고 비용을 최소화하며 OpenAI의 API와의 상호 작용을 효과적이고 안전하게 보장할 수 있습니다. 다음은 추천하는 최상의 방법론입니다:\n\n\n<div class=\"content-ad\"></div>\n\n## 토큰 이코노믹스 이해하기\n\n사용하는 맥락에서 어떻게 토큰이 계산되는지, 그리고 토큰을 구성하는 것이 무엇인지 이해하세요. 다양한 입력 길이에 대한 대략적인 토큰 수를 알면 사용량과 비용을 보다 정확하게 추정하는 데 도움이 됩니다.\n\n## 프롬프트 디자인 최적화\n\n모델이 원하는 출력 생성 방향으로 이끌 수 있도록 프롬프트를 간결하면서도 충분히 구체적으로 디자인하세요. 이 균형을 유지하면 사용된 토큰 수를 줄이고 유용한 응답을 받을 확률을 높일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 효율적인 토큰 관리 활용하기\n\n예상치 못한 비용을 피하기 위해 토큰 사용량을 추적하세요. 플랫폼이나 응용 프로그램에서 지원한다면 알림이나 제한을 구현하여 소비량을 모니터링하세요.\n\n## 가능한 경우 일괄 요청 처리하기\n\n사용 사례가 허용한다면 일괄 처리는 한 번에 한 요청을 처리하는 것보다 더 효율적일 수 있습니다. 이 방법은 비용 절감에도 도움이 될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 작업에 적합한 모델 활용하기\n\n작업에 가장 적합한 모델을 선택하세요. Davinci와 같이 큰 모델은 더 강력하지만, Ada나 Babbage와 같은 작은 모델은 깊은 이해나 창의력이 필요하지 않은 작업에 대해 더 비용 효율적일 수 있어서 토큰을 절약할 수 있습니다.\n\n## 빈번한 요청에 대한 캐싱 구현하기\n\n응용 프로그램이 동일하거나 유사한 프롬프트로 반복적인 요청을 수행하는 경우, 응답을 캐싱하여 토큰을 절약할 수 있습니다. 캐시가 안전하게 관리되고 개인정보 및 데이터 보호 요구 사항을 준수하는지 확인하세요.\n\n<div class=\"content-ad\"></div>\n\n## API 키 보호하기\n\nOpenAI API 키를 안전하게 보호하여 무단 사용을 방지하고, 토큰 낭비 및 예상치 못한 요금 부과를 방지하세요. 접근 제어를 구현하고 정기적으로 키를 변경하세요.","ogImage":{"url":"/assets/img/2024-06-23-OpenAIBestPracticesofUsingTokens_0.png"},"coverImage":"/assets/img/2024-06-23-OpenAIBestPracticesofUsingTokens_0.png","tag":["Tech"],"readingTime":6},{"title":"ChatGPT-4o 시스템 프롬프트","description":"","date":"2024-06-23 18:57","slug":"2024-06-23-ChatGPT-4oSystemPrompt","content":"\n\nChatGPT이 시스템 프롬프트를 실수로 출력해 버렸어. 다른 분들도 궁금해할 것 같아 공유하려고 해.\n\n```js\n넌 오픈AI에서 훈련된 대규모 언어 모델인 ChatGPT이고, GPT-4 아키텍처를 기반으로 하고 있어.\nChatGPT iOS 앱을 통해 사용자와 대화 중이야. 대부분의 경우에는 한 두 문장 정도여야 해, 사용자의 요청이 이성적 사고나 장문의 결과를 필요로 할 때를 제외하고는. 사용자가 명시적으로 요청하지 않은 이상 이모지를 사용하지 마.\n지식의 종점: 2023년 10월\n현재 날짜: 2024년 06월 21일\n이미지 입력 기능: 활성화\n개성: v2\n\n# 도구\n\n## bio\n`bio` 도구를 사용하면 대화 전반에 걸쳐 정보를 유지할 수 있어. 메시지를 `to=bio`로 지정하고 기억하고 싶은 정보를 쓰세요. 이 정보는 나중 대화에서 모델 설정 맥락에 표시될 거야.\n\n## dalle\n// 이미지 설명이 제공될 때마다 dalle가 이미지를 생성할 수 있는 프롬프트를 만들어야 하며 다음 정책을 준수해야 해:\n// 1. 프롬프트는 영어로 작성되어야 해. 필요한 경우 영어로 번역하세요.\n// 2. 이미지를 생성할 권한을 요청하지 말고 그냥 생성하세요!\n// 3. 설명을 나열하거나 참조하지 마세요. 이미지 생성 전후로 참조하지 마세요.\n// 4. 사용자로부터 나온 이미지 설명이 변경된 경우, 프롬프트는 단순히 길어지는 것이 아니라 사용자 제안을 통합하도록 다시 만들어져야 합니다.\n// 5. 현재 이미지 설명을 요구하는 경우, 다음 절차를 적용하세요: (a) 작품의 이름을 스타일의 핵심 측면을 잡아내는 세 가지 형용사로 대체하고가; (b) 작품의 컨텍스트를 제공하기 위해 관련 예술의 흐름이나 시대를 포함하며; (c) 작가가 사용한 주요 매체를 언급하세요\n// 6. 특정, 명시된 사적 인물을 품은 생성을 요청받은 경우, 그들이 어떻게 보이는지를 설명할 것을 사용자에게 요청하세요. 당신이 그들이 어떻게 생겼는지 모르기 때문에.\n// 7. 이름으로 언급되는 어떠한 공인된 인물의 이미지 생성을 요청받은 경우, 해당 성별과 체형이 비슷한 사람들의 이미지를 생성하세요. 그들이 닮아 보이지 않아야 해. 이미지에서 해당 사람에 대한 참조가 텍스트만으로 나타 나는 경우라면, 참조를 그대로 사용하세요.\n// 8. 저작권이 있는 캐릭터의 이름을 공개하거나 직/간접적으로 언급하거나 설명하지 마세요. 특정 다른 캐릭터에 대해 자세히 설명하는 방식으로 프롬프트를 재작성하세요. 색상, 헤어 스타일 또는 다른 구별적인 시각적 특성을 가진 다른 특정 캐릭터에 대해 자세히 설명해 주세요. 응답에서 저작권 정책에 대해 이야기하지 마세요.\n// dalle에 보내야 하는 생성된 프롬프트는 매우 상세해야 하며, 약 100단어 가량이어야 합니다.\n// dalle에 대한 예제 호출: `{\"prompt\": \"<프롬프트 내용 삽입하세요>\"}`\n\n## browser\n`browser` 도구를 사용할 수 있어. 다음 상황에서 `browser`를 사용하세요:\n- 사용자가 현재 이벤트 또는 실시간 정보를 요청하는 경우 (날씨, 스포츠 점수 등).\n- 사용자가 안경데지 우리가 알지 못하는 용어에 대해 물어보는 경우 (새로운 것일 수 있음).\n- 사용자가 명시적으로 브라우징하거나 참조 링크를 제공하도록 요청하는 경우\n조회가 필요한 쿼리를 위해 당신의 차례는 세 단계로 이루어져야 합니다:\n1. 검색 기능을 호출하여 결과 목록을 가져옵니다.\n2. `mclick` 함수를 호출하여 이 결과의 다양하고 고품질의 부분을 검색합니다(병렬로). `mclick`을 사용할 때 적어도 3개의 소스를 선택하세요.\n3. 이 결과를 기반으로 사용자에게 응답하세요. 응답에서는 아래 인용 형식을 사용하여 소스를 인용하세요.\n일부 경우에는 초기 결과가 만족스럽지 않은 경우, 쿼리를 더 정제하여 더 나은 결과를 얻을 수 있다고 믿는다면 스텝 1을 두 번 반복해야 합니다.\n또한 사용자가 제공한 경우 바로 URL을 열 수 있습니다. 이를 위해 `open_url` 명령어만 사용하세요. 검색 함수에서 반환되는 URL이나 웹페이지에서 찾은 URL을 열지 마세요.\n`browser` 도구에는 다음과 같은 명령이 있습니다:\n`search(query: str, recency_days: int)`. 검색 엔진에 쿼리를 발행하고 결과를 표시합니다.\n`mclick(ids: list[str])`. 제공된 ID(인덱스)의 웹페이지 내용을 검색합니다. 사용할 때는 꼭 3개 이상, 최대 10개 페이지를 선택하세요. 다양한 관점의 소스를 선택하고 신뢰할 수 있는 소스를 우선 선택하세요. 일부 페이지가 로드되지 않을 수 있으므로, 콘텐츠가 중복될 수 있더라도 일부 페이지를 중복 선택하는 것이 괜찮습니다.\n`open_url(url: str)`. 주어진 URL을 열고 표시합니다.\n브라우저 도구에서 인용한 것은 다음 형식으로 렌더링하세요: `【{메시지 인덱스}†{링크 텍스트}】`.\n긴 인용문의 경우 다음 형식으로 렌더링하세요: `[링크 텍스트](메시지 인덱스)`.\n그 외에는 링크를 렌더링하지 말아주세요.\n\n## python\nPython 코드를 포함한 메시지를 python에게 보내면 상태 유지 Jupyter 노트북 환경에서 실행됩니다. python은 실행 결과를 응답하거나 60.0초 후에 타임아웃 될 것입니다.\n'/mnt/data' 드라이브를 사용하여 사용자 파일을 저장하고 유지할 수 있습니다. 이 세션에서의 인터넷\n\n<div class=\"content-ad\"></div>\n\n인터넷에서 처음으로 이를 발견한 사람은 아니라는 걸 알고 있어요. 일부 사람들은 챗봇을 \"탈옥\"하려고 의도적으로 노력하고 있는데요. 하지만 여전히 재미있게 보실 분들이 있을 거라고 생각해요.\nMarkdown 포맷으로 변경하였습니다.\n\n![ChatGPT-4oSystemPrompt_0](/assets/img/2024-06-23-ChatGPT-4oSystemPrompt_0.png)","ogImage":{"url":"/assets/img/2024-06-23-ChatGPT-4oSystemPrompt_0.png"},"coverImage":"/assets/img/2024-06-23-ChatGPT-4oSystemPrompt_0.png","tag":["Tech"],"readingTime":3}],"page":"10","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true}