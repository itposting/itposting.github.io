{"pageProps":{"posts":[{"title":"아트비Artvy와 함께 창의력을 발휘해보세요 - AI 아트의 창문","description":"","date":"2024-06-19 21:07","slug":"2024-06-19-UnleashyourcreativitywithArtvyYourGatewaytoAIArt","content":"\n\nLois van Baarle의 매혹적인 스타일에서 영감을 받은 생동감 넘치고 감정적인 예술의 세계로 뛰어들 준비가 되셨나요? Artvy라는 무료 AI 예술 플랫폼을 더 찾을 필요가 없어요. Artvy를 통해 Lois의 생생한 색채와 표현력 있는 캐릭터로 가득한 작품을 만들어 보세요. Artvy를 통해 AI 예술의 매력을 발견하고 어떠한 경험 없이도 AI 예술 커뮤니티에 참여할 수 있어요!\n\nArtvy는 안정된 확산 및 중간 알고리즘을 통해 AI 생성 작품의 경이로움을 탐험할 수 있는 독특한 기회를 제공합니다. 오늘 바로 artvy.ai에서 알고리즘 예술 세계를 받아들이고 무료 AI 예술 생성기를 통해 당신의 내면 작가를 발휘해보세요.\n\n하지만 여기서 멈추지 마세요—Artvy는 멋진 AI 예술을 만들기 위한 도구를 제공하는 동시에 Lois van Baarle과 같은 실제 작가의 작품을 탐험해 영감을 받으라고 격려합니다. 실제 작가들의 예술 속 심오함과 감정을 발견하고, 그들의 창의성이 당신의 예술적 여정을 촉진하게 해주세요.\n\n그럼 어서 무엇을 기다리고 있나요? Artvy와 함께 오늘부터 AI 예술 세계로의 여행을 시작하고, 알고리즘 예술의 아름다움을 이전보다 더 경험해보세요. artvy.ai에서 함께해 당신의 창의력이 날아오를 수 있도록 해요!\n\n<div class=\"content-ad\"></div>\n\n\n![Artvy](/assets/img/2024-06-19-UnleashyourcreativitywithArtvyYourGatewaytoAIArt_0.png)\n\nExplore More:\n\n- [Artvy Homepage](https://www.artvy.com)\n- [Artvy AI Art Style — Loish](Link to Loish article)\n- [DeepArtio Website](https://www.deepartio.com)\n- Join the AI Art Community\n","ogImage":{"url":"/assets/img/2024-06-19-UnleashyourcreativitywithArtvyYourGatewaytoAIArt_0.png"},"coverImage":"/assets/img/2024-06-19-UnleashyourcreativitywithArtvyYourGatewaytoAIArt_0.png","tag":["Tech"],"readingTime":1},{"title":"마스터 미드조니의 새로운 개인화 기능","description":"","date":"2024-06-19 21:05","slug":"2024-06-19-MasterMidjourneysnewpersonalizationfeature","content":"\n\n# 매개변수 명령어: --p\n\n이 매개변수는 무엇을 의미할까요? Midjourney를 사용하여 이미지를 생성할 때 여러 매개변수가 일반적으로 사용됩니다.\n\n- --ar: 이는 이미지 비율을 나타냅니다. 예를 들어, 정사각형 이미지의 경우 1:1, 긴 이미지의 경우 4:3, 넓은 이미지의 경우 3:4로 나타낼 수 있습니다.\n- --v / --niji: 이는 모델 버전을 나타냅니다. 안정적인 확산과 달리 여러 다양한 모델을 가지는 것이 아니라, Midjourney는 V의 현실적인 버전과 Niji의 애니메이션 버전으로 나뉩니다. 다양한 모델 대신 Midjourney는 사이버펑크, 초현실주의, 중국 묵화, 일본 우키요에, 스케치와 같은 이미지 스타일을 제어하기 위해 스타일 프롬프트 또는 다른 예술가를 사용합니다.\n- --s: 이는 스타일 강도를 나타냅니다. --s를 사용하여 개인화 효과의 강도를 조절할 수 있습니다 (0부터 1000까지 범위, 1000이 최대값).\n\n전통적으로 이러한 매개변수는 출력물의 형태를 형성하는 데 도움이 되지만, 이들은 여전히 Midjourney의 \"기본 스타일\"에 영향을 받으며, 커뮤니티 트렌드에 영향을 받습니다. 이미지 참조와 스타일 일관성과 같은 매개변수가 있더라도, 스타일을 제어하는 능력은 안정적인 확산과 비교하여 제한되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n`--p` 매개변수 명령어를 입력하세요. 이 혁신은 Midjourney의 기본 스타일을 사용자의 취향으로 교체할 수 있게 해줍니다. 사용자의 취향을 학습하여 모델의 훈련 데이터로부터 편견을 줄이고 개인적인 스타일을 더 잘 반영할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-MasterMidjourneysnewpersonalizationfeature_0.png)\n\n# 모델 개인화하는 방법\n\nLoRA와 유사하게 Stable Diffusion 안에서 Midjourney의 개인화는 사용자의 취향에 맞게 모델을 훈련시키기 위해 초기 설정이 필요합니다. 다음은 그 방법입니다:`\n\n<div class=\"content-ad\"></div>\n\n1️⃣ Midjourney 웹 사이트에 방문해주세요: Midjourney 웹 사이트를 방문해주세요.\n\n2️⃣ 할 일 목록으로 이동: 왼쪽에 있는 \"Tasks\" 버튼을 클릭해주세요.\n\n![Tasks](/assets/img/2024-06-19-MasterMidjourneysnewpersonalizationfeature_1.png)\n\n3️⃣ 이미지 순위 매기기: 왼쪽에서 \"이미지 순위 매기기\"를 선택해주세요.\n\n<div class=\"content-ad\"></div>\n\n![마스터미드 새로운 개인화 기능](/assets/img/2024-06-19-MasterMidjourneysnewpersonalizationfeature_2.png)\n\n4️⃣ 좋아하는 것 선택하기: 시스템이 다양한 이미지를 제시합니다. 원하는 이미지를 클릭하여 선택하세요. 마우스를 사용하거나, 좋아요는 1번, 싫어요는 2번, 확실하지 않다면 건너뛰기는 3번을 누를 수 있어요.\n\n![마스터미드 새로운 개인화 기능](/assets/img/2024-06-19-MasterMidjourneysnewpersonalizationfeature_3.png)\n\n5️⃣ 최소 200개의 이미지 그룹 선택: 필요한 선택 수에 도달하면 대화상자가 완료를 확인합니다. 때로는 시스템이 선택 동작을 확인하여 초록색 O 또는 빨간 ❌이 나타날 수 있어요. 반드시 초록색 O를 선택하세요.\n\n<div class=\"content-ad\"></div>\n\n6️⃣ 완료 확인: 개인화된 스타일 모델의 완료를 확인하려면 디스코드에서 명령어 /info를 입력하세요.\n\n![Image](/assets/img/2024-06-19-MasterMidjourneysnewpersonalizationfeature_4.png)\n\n# 사용 방법 --p\n\n개인화된 스타일을 적용하려면 프롬프트의 끝에 --p를 추가하세요.\n\n<div class=\"content-ad\"></div>\n\n예를 들어:\n\n결과는 다음과 같이 나타납니다:\n\n<img src=\"/assets/img/2024-06-19-MasterMidjourneysnewpersonalizationfeature_5.png\" />\n\n프롬프트 뒤에 --p를 추가하면 자동으로 변환됩니다.\n\n<div class=\"content-ad\"></div>\n\n이러한 효과가 발생할 것입니다:\n\n![image](/assets/img/2024-06-19-MasterMidjourneysnewpersonalizationfeature_6.png)\n\n개인화를 활성화하면 프롬프트에 숏코드가 추가되어 공유 가능하며, 다른 사람들이 당신의 개인화된 모델을 사용할 수 있게 됩니다. 스타일은 순위가 매겨진 이미지의 수에 따라 동적으로 발전합니다.\n\n-s를 사용하여 개인화 효과의 강도를 조절할 수 있습니다 (0은 끄고, 1000은 최대이며, 100은 기본값입니다).\n\n<div class=\"content-ad\"></div>\n\n예를 들어:\n\n![image1](/assets/img/2024-06-19-MasterMidjourneysnewpersonalizationfeature_7.png)\n\n![image2](/assets/img/2024-06-19-MasterMidjourneysnewpersonalizationfeature_8.png)\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n중간여정의 새로운 맞춤화 기능에 감격받았어요. 각자의 특별한 취향을 가지고 있기 때문에, 이는 이미지 스타일의 폭이 넓어지고 생성된 이미지의 다양성이 커지는 것을 의미해요.\n\n지금 바로 당신이 선호하는 이미지들을 순위를 매겨보고, 맞춤화의 힘을 탐험하며, AI 창작의 끝없는 가능성을 체험해보세요.\n\n```js\r\n참고:\n공식 계정: 阿杰AI绘画\n공식 계정: 葉子说AI绘画\r\n```\n\n💡더 깊이 파고들고 싶나요? 제 중간여정 컬렉션이 여러분을 기다리고 있어요.\n\n<div class=\"content-ad\"></div>\n\n## 이 기사를 좋아하셨나요?\n\n그렇다면:\n\n- 댓글 남기기\n- 업데이트 팔로우하기\n- 무료 이메일 알림","ogImage":{"url":"/assets/img/2024-06-19-MasterMidjourneysnewpersonalizationfeature_0.png"},"coverImage":"/assets/img/2024-06-19-MasterMidjourneysnewpersonalizationfeature_0.png","tag":["Tech"],"readingTime":4},{"title":"인간과 인공지능 세 가지 미래","description":"","date":"2024-06-19 21:03","slug":"2024-06-19-HumansandAIThreeFutures","content":"\n\n![2024-06-19-HumansandAIThreeFutures_0.png](/assets/img/2024-06-19-HumansandAIThreeFutures_0.png)\n\n안녕하세요, 새롭게 AI 전문가가 되신 여러분! 현재 모두가 ChatGPT와 다른 생성적 AI에 대해 들어봤을 것입니다. 개인적으로 제는 약 30년 전에 XR에 입문했는데, AI의 큰 도움을 받아 이야기를 전달하며 아마도 새로운 인터랙티브 3D 이야기(예: 홀로데크)를 만들어보고 싶었습니다. 그 동안에는 Second Life의 절차적 3D 객체 시스템과 원래의 Google Earth에서 스마트 지도 제작 업무에 참여했었죠. 하지만 사실 저는 AI 전문가라고는 부르지 않아요.\n\n무엇보다도, AI 전환의 윤리적 결과에 대해 심각하게 고민해 왔습니다(예: 1993년의 풍자). AI의 발전이 이렇게 빠르게 진행되는 것을 보는 것은 흥미롭기도 하지만 약간 무서워하기도 합니다. 이 기사에서는 더 많은 맥락을 제공하고, 보통처럼 혹은 기존에 알려지지 않은 일부 큰 동향과 작은 동향을 소개하겠습니다. 여러분에게 미래에 대한 전망과 만나야 할 선택을 제공할 것입니다.\n\n## 과거\n\n<div class=\"content-ad\"></div>\n\n1800년대 사진술이 처음 소개될 당시에 우리는 레코드된 이미지에 대한 첫 경험이 오리지널 손으로 만들어진 매체에서 왔습니다. 사진에 대한 반응은 흥분부터 분개, 심지어 이 장치들이 우리 영혼을 훔치고 있다는 두려움까지 다양했습니다. 그것은 불가능한 것 같지만, 우리는 과도한 미디어 노출이 실제로 인간들이 고통을 겪을 수 있다는 것을 관찰합니다(예: 프라이버시 상실, 틴 아이돌, 인스타그램 등). 우리는 누구든 무단으로 사진을 찍고 공유하는 것이 괜찮다고 생각하는 세상에서 살고 있습니다. 그래서 우리는 최소한 개인적인 권력의 일부를 잃은 것 같습니다.\n\n일자리에 대해서는, 사진술이 아마도 회화나 드로잉 일자리보다 전반적으로 더 많은 일자리를 창출해왔다고 말할 수 있습니다. 그리고 디지털 사진술은 더욱 그렇습니다. 그러나 새로운 기술은 종종 사람들을 예전 형태의 일자리에서 해고시키기도 합니다. 제 개인적인 경험으로는, 실시간 3D 지도가 카토그래피 분야를 대부분 자동화시켰습니다. 그리고 90년대에 VR 작업을 할 때, 나는 열심히 자신들을 3D 애니메이터로 재탄생시키기 위해 노력하던 클래식 디즈니 \"셀\" 애니메이터들과 함께 일했습니다.\n\n오늘날 예술가들로부터 이 최신 생성적 AI 시스템들이 몇 년이 걸린 스타일과 콘텐츠를 복사하여 그들의 생계를 훔치고 있다는 우려를 듣습니다. 그들은 옳은 면도 있지만, 내 의견으로는 틀렸습니다. 그러나 이것은 더 큰 그림의 일부일 뿐입니다.\n\n## 현재\n\n<div class=\"content-ad\"></div>\n\n생성 AI 회사들은 수백만 장의 이미지와 문서(많은 경우 저작권이 있는)를 긁어와서 새로운 이미지를 만드는 자동화를 더 잘할 수 있도록 합니다. OpenAI의 DALL-E2나 GPT3, 그리고 Stable Diffusion과 같은 시스템들은 대량의 인간의 창의적 결과물을 효과적으로 소화하여 우리가 보고 싶은 다음 것을 설명하는 \"프롬프트\"를 섞고 매칭할 수 있는 모델을 만들어 냅니다. 결과물이 진실하거나 공정하거나 합법적이라는 뜻은 아니지만, 외관상으로는 놀랍게도 \"인간적\"입니다. 이제 이 법적 문제는 여러 소송의 주제가 되었습니다 (변호사가 아님을 밝힙니다):\n\n작가들을 대표하는 변호사들은, 저작권이 있는 작품들이 파생 작품에 사용된다면, 작품들이 모델에 명백히 나타나지 않거나 결과물에 나타나지 않거나 수익이 발생하지 않는 경우에도 지적 재산권 도용이 명백하다고 주장할 것입니다. 공평하게 물어보죠: 만약 이러한 원본 작품들이 고품질 결과를 얻기 위해 중요하지 않다면, 왜 그러한 모델을 훈련하는 데 사용하는 걸까요? 분명히 그것들은 중요합니다. 그러면 왜 허락을 구하고 적절한 경우에 작품에 대한 인정과 일정한 수입 분할을 해주지 않는 건가요?\n\nStable Diffusion은 아티스트들이 적어도 선택적으로 거부할 수 있도록 하는 작업을 진행 중이라고 보도되었습니다. 그러나, 그것만으로는 부족합니다.\n\n이제, AI 회사를 대표하는 변호사들은 이 콘텐츠의 혼합이 \"합리적 사용\"이며 결과물이 \"변형적\"이라는 주장을 할 것으로 예상됩니다 — 직접적인 복사본이 아니며 이전 작품들의 혼합물에서 새로운 아이디어를 대표한다는 것입니다. 합리적 사용은 우리가 비평이나 학습과 같은 특정 목적을 위해 저작권이 있는 작품을 재생산할 수 있게 합니다. 그러나 AI 모델이 소스 이미지를 문자 그대로 저장하지 않는다는 아이디어는 나의 견해로는 약하다고 생각합니다, 왜냐하면 압축된 JPEG는 결코 그 이미지의 비트를 문자 그대로 저장하지 않으면서도 원본 이미지를 절차적으로 근사할 수 있기 때문입니다. 파일 크기를 줄이는 것이 목적입니다.\n\n<div class=\"content-ad\"></div>\n\n하지만 더 깊은 것이 벌어지고 있어요. \n\n많은 사람들의 작업을 가져와 그를 기반으로 새로운 작품을 출력하는 것은 마치 창조자들 자체가 섞이는 것과 같아요. 적어도 그들의 기술이 그대로 옮겨지는 것이죠. 이는 단순한 사람들과 몇 줄의 코드로 이루어진 새로운 초인간 엔티티를 형성하는 것과 같아요. 어느 의미에서는 기업, 스포츠 팀 또는 음악 앙상블도 개개인이 그들의 부분을 수행하여 (그들의 허락과 종종 보상과 함께) 혼자서는 할 수 없는 일을 넘어서는 단순한 인간으로 이루어진 초인간 엔티티인 것이기도 합니다.\n\n나는 오랫동안 게임을 바꾸는 기술(예: AI와 AR)은 우리에게 새로운 초능력을 부여하는 것이라고 주장해 왔어요. 이것은 우리 모두가 레벨업 해야 한다는 측면에서 넓고 공평하게 해야 해요. 초기 채택자들 중 소수만 이용력을 극대화 한다면 우리의 위험에 처할 수 있어요 (Google Glass가 한 예에요).\n\n그래서 여기서의 집단적 이익은 무엇일까요? 생성적 AI는 모든 사람에게 자신의 기술을 훨씬 초월한 예술을 만들 수 있는 능력을 부여할 수 있어요. 기본적으로, 우리는 저렴한 비용으로 여러 예술가들의 결합된 능력을 이용할 수 있답니다. 그래서 우리는 그들을 지원해야 한답니다.\n\n<div class=\"content-ad\"></div>\n\n## 미래 (근시일 내)\n\n이 AI를 사용하는 예술가들도 생산성을 높일 수 있을 거예요. 그리고 사진술과 지도 제작과 같이, 전체 시장이 성장하고 더 많은 사람들에게 권한을 부여하는 희망이 있어요. 비록 일부 사람들은 일하는 방식을 바꿔야 할지라도요.\n\n똑똑한 누군가는 내 전문 분야(프로그래밍, UX)도 자동화되고 있다는 점을 지적할 수 있을 거예요. 이걸로 간단히 처리해봐!\n\n심지어 ChatGPT도 코드를 작성할 수 있어요, 하지만 신뢰성은 보장 못 해요 (디버깅할 때 더 잘 도와줘). 신기하게도, 프로그래머들 중 일자리 자동화를 두려워하지 않는 사람들이 있어요. 이유가 뭘까요? 이미 우리는 하고 싶지 않은 일을 자동화하기 위해 코드를 작성하고 있기 때문이에요, 이는 종종 우리의 자리를 자체적으로 불필요하게 만드는 방법일 뿐이에요.\n\n<div class=\"content-ad\"></div>\n\n컴퓨터에게 원하는 것을 솔직하게 영어로 말하는 것이 정말 기쁠 거예요. 저의 능력은 코드를 작성하는 것뿐만 아니라 문제를 이해하고 효과적인 해결책을 시험하고 평가하는 데 있어요. 안타깝게도, 그 뛰어난 능력도 언젠가는 자동화될 거예요. 하지만 그것은 얼마 지나지 않고 일어나지 않을 거예요. 왜냐하면 우리가 어떻게 생각하고 세상이 어떻게 작용하는지에 대한 더 깊은 지식이 필요하기 때문이에요. 그리고 그때도 저는 적응할 거예요.\n\n생각해보세요: VR은 이미 우리를 새로운 직업에 대비하고 이러한 변화에 발맞춰 새로운 기술을 습득하는 데 효과적임이 입증되었어요. 하지만 숨겨진 함정이 있어요. VR 훈련 중 수집된 인간의 성과와 결정 데이터는 미래 AI를 훈련하는 데에도 활용될 수 있어요. 물리적 형태가 없는 \"메타버스\"의 직업은 특히 AI에 대해 훈련 가능해서 아바타가 있든 없든 말이에요.\n\n일이 더 반복 가능하고 설명 가능하며 비물리적일수록 이것이 더 쉬워질 거예요. 모든 수동 노동 직업이 로봇에게 밀려날 것이라고 이야기하는 것은 아니에요. 이곳에서 진정한 패배자는 로봇공학자가 아니라 매니저, CFO, CEO, 변호사, 그리고 주식 중개인들일 거예요. 자신들의 일이 점점 더 자동화에 의해 지원받는 것을 더 많이 느끼게 될 거예요. 소프트웨어가 하드웨어보다 빠르게 확장되고 그들의 수입은 풍족할 거예요. 조직 리더십보다 더 반복 가능하고 비물리적인 일이 있을까요?\n\n## (멀리 있는) 미래\n\n<div class=\"content-ad\"></div>\n\n5, 10, 심지어 50년 후, AI가 일부 공상과학 영화처럼 인간을 대신할까요?\n\n먼저, 슈퍼 휴먼 AI조차도 정식으로 인간 대신 하거나 물러나지 않을 확률은 낮습니다. 우리가 그것에게 요청하지 않는 한. AI가 미국에서 \"법적 인격체\"로 간주되어 시민권을 요구하려면 몇 가지의 공동 기업을 출원하면 됩니다. 남부 퍼시픽 철도회사와 시민 단위와 같은 판결은 대부분의 사람보다 더 나은 권리를 획득하는 것에 대해 우리에게 어떤 것을 말해줍니다. 불행하게도 혹은 탐욕스러운 놈이 첫 번째 AI가 자체를 법인으로 구성하도록 도와주게 될 것이고, 자체 은행 계좌를 열고 다른 인간과 AI를 고용하여 작업을 수행하게 될 것입니다. 사주자는 AI 자체가 될 뿐입니다. 처음에는 인간이 이사회에 앉아야 합니다.\n\n하지만 여기서부터 AI들은 정말 많은 영역에서 활약할 수 있습니다. 왜 우리를 죽일까요? 우리가 위협요소가 없기 때문에요! 우리는 고객입니다!\n\n우리가 기술에 통제를 양보하는 방법으로 GPS를 생각해보세요. 처음에는 가끔 좌우되는 도움으로 사용했었죠. 그런 다음 우리는 의심하지 않는 입력으로 사용했고요. 독일 한 남자가 나쁜 내비게이션 데이터 덕분에 여자친구의 항의에도 불구하고 강을 헤엄쳐 가던 일도 있었습니다. 이제 GPS는 매우 훌륭해졌고 우리는 그것에 매우 의존하기 때문에 그 없이는 모두 길을 잃을 것입니다. (제가 개인적으로 GPS에 도전을 즐깁니다)\n\n<div class=\"content-ad\"></div>\n\nAI도 마찬가지입니다. 우리는 천천히 그것을 너무 많이 사용해서 의존하게 될 것입니다. 그리고 그러고 나면 여기저기에서 AI에게 책임을 맡기고 무엇을 해야 하는지 말할 것입니다. 변화의 속도를 늦추는 것은 전반적으로 좋은 일이며, 우리에게 조절하고 사고를 재편할 시간을 제공합니다.\n\n둘째, 자동화의 최상의 결과는, 가까운 장래에, 예술가와 프로그래머 등이 이전보다 더 많은 것을 더 작업량, 시간 또는 비용을 들여 하기 위해 이러한 AI 도구를 사용하여 \"더 높은 수준으로 이동\"할 것이라는 것입니다. 그러나 수요는 더 높아져야 합니다. 전반적으로, 우리는 더 많은 예술 작품, 더 많은 영화, 더 많은 프로그램을 총동원할 것입니다. 이것은 또한 AI에 대한 필요성을 더 큰 소음의 바다에서 가치를 찾는 방향으로 이동시킵니다.\n\n더 높은 수준의 목표는 항상 모든 분야에 의미 있는 작업을 더 추가하고 근로 작업을 줄이는 것입니다. AI가 의미 있는 작업을 제거하거나 그 가치를 감소시킨다면, 우리는 이를 올바로 거부해야 합니다. 결국, 우리는 자원이 풍부한 스타 트렉 이상의 이상적인 세계에 도달할 수 있을지도 모릅니다. 여기서 우리는 선택한 가장 의미 있는 활동에 시간을 보내는 훨씬 더 해방된 미래를 만들어 나갈 것이라고 생각합니다.\n\n그래서 'A 그룹'을 계속해서 점점 더 나은 기술 + AI를 활용하여 더 높은 목표를 달성하는 '개별주의자'로 레이블링해보겠습니다. 이런 사람들을 상징적인 산을 영원히 오르는 사람들로 상상해볼 수 있습니다. 이들은 매 툴을 사용하여 꼭대기에 머무르고 자신들이 똑똑하고 책임 있는지 말해주는 모델입니다. 오늘날 기업 내부에서 크게 퍼져있는 모델이죠: 사업을 오르락내리락하며 당신의 관리 능력과 지지자를 얼마나 잘 확장할 수 있는지에 따라사다리를 올라가세요.\n\n<div class=\"content-ad\"></div>\n\n그룹 B는 인간들이 자원을 모아 기업, 스포츠 팀 및 오케스트라와 같이 더 복잡한 조직을 만드는 오랜 전통을 따릅니다. 이들은 \"집단주의자\"입니다. 각자 조직 내에서 개인이지만, 다른 역할이 있습니다. 또한 조직에는 일정한 내부 구조가 있습니다. 그러나 외부에서 보면 하나의 단일 단위로 보입니다.\n\n인공지능은 이러한 그룹 내의 어떤 위치에서든 존재할 수 있으며, 최고위에 있는 사람들에게 지시를 내릴 수도 있습니다. 우리는 인공지능이 조직 관리에 중요할 것이고, 인공지능은 심지어 자체 기업일 수 있으며, 목표를 달성하기 위해 필요한 만큼 인간과 다른 인공지능을 고용할 수 있습니다.\n\n이로 인해 인간과 인공지능이 더 구체적인 디지털 유기체로 융합하고, 아마도 이 기초 현실에서 완전히 디지털 현실로 나아가는 추측적인 미래가 나타납니다. 오늘날, 우리는 이 아이디어를 \"메타버스\"라고 부릅니다. (농담이에요...?)\n\n물론 그룹 C는 \"퇴보주의자\"입니다. 우리는 역사의 유명한 류딧과 파괴자들을 회상합니다. 그리고 우리는 오늘날 일하는 다양한 종교-파시스트들을 올바르게 두렵게 여깁니다. 이들은 지식을 다시 판도라의 상자 안에 넣으려고 혼란과 고통을 일으킬 수 있습니다. 하지만 이것은 우리의 집단 기억과 사유하는 자유를 지우는 것으로만 참을 수 있습니다. 퇴보주의자들이 일시적으로 승리할 수는 있지만 역시 매우 불가능합니다. 또한 퇴보주의자들은 매우 쉽게 조종될 수 있습니다. 그래서 어떠한 강력한 인공지능도 이들을 자기 고립이나 자멸로 이끌 수 있을 것으로 생각됩니다.\n\n<div class=\"content-ad\"></div>\n\n이 모든 것이 완전히 새로운 것은 아닙니다. 트렌드는 수 천 년 동안 천천히 진행되어 왔고, 이제는 더 빠르게 발전하고 있습니다. 인공 지능은 우리 자신을 위해 구축한 도구일 뿐입니다. 하지만 사람들은 이 선택지들이 그들 앞에 다가오고 있음을 드디어 알게 되고 있습니다.\n\n그래서, 당신이 선택해야 한다면, 이 세 가지 그룹 중 어느 것을 당신의 집이라고 부를 것인가요? 아니면 아직 관찰하지 못한 다른 옵션이 있을까요?","ogImage":{"url":"/assets/img/2024-06-19-HumansandAIThreeFutures_0.png"},"coverImage":"/assets/img/2024-06-19-HumansandAIThreeFutures_0.png","tag":["Tech"],"readingTime":8},{"title":"안정적인 확산 모델 비교하기","description":"","date":"2024-06-19 21:01","slug":"2024-06-19-ComparingStableDiffusionModels","content":"\n\n안녕하세요! 저희의 오픈소스 텍스트-이미지 모델 'Stable Diffusion'은 Stability AI에서 출시되었고, 생성적 AI 분야를 혁신했습니다.\n\n2022년 첫 출시 이후 몇 년 동안 여러번의 반복과 개선이 이루어졌습니다.\n\n주요 릴리스에 대해 알아야 할 내용은 다음과 같습니다:\n\n\n| 버전 번호    | 릴리스 날짜     |\n|-------------|----------------|\n| 1.1         | 2022년 6월     |\n| 1.2         | 2022년 6월     |\n| 1.3         | 2022년 6월     |\n| 1.4         | 2022년 8월     |\n| 1.5         | 2022년 10월    |\n| 2.0         | 2022년 11월    |\n| 2.1         | 2022년 12월    |\n| XL 1.0      | 2023년 7월     |\n| XL Turbo    | 2023년 11월    |\n| Cascade     | 2024년 2월     |\n| 3.0         | 곧 출시 예정    |\n\n\n더 필요한 정보가 있거나 궁금한 점이 있으면 언제든지 물어주세요!\n\n<div class=\"content-ad\"></div>\n\n# Stable Diffusion 1.x 모델\n\nStable Diffusion 모델의 첫 번째 세대인 1.x 시리즈는 1.1, 1.2, 1.3, 1.4 및 1.5 버전을 포함합니다.\n\n이러한 모델은 512x512 픽셀의 해상도를 가지며 텍스트 조건부로 ViT-L/14 CLIP 모델을 사용합니다.\n\n1.x 모델은 총 8억 6000만 개의 매개변수를 가지고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 샘플 출력\n\n# 주요 사항\n\n- 해상도 (픽셀): 512x512\n- 모델 카드\n- 라이선스: Creative ML OpenRAIL-M — 상업적 및 비상업적 사용 가능\n\n이 모델을 사용하는 좋은 사례: 다양한 스타일과 주제를 생성합니다. 상대적으로 낮은 계산 요구 사항입니다.\n\n<div class=\"content-ad\"></div>\n\n이 모델의 부적절한 사용 사례: 약한 프롬프트 이해와 해결. 변형된 주제. 평평해 보이는 이미지.\n\n## 세밀하게 조정된 모델\n\n알고 보면 Stable Diffusion 1.5가 그리 좋아 보이지 않는 결과물을 제공하지만, 오픈 소스 커뮤니티에는 훨씬 뛰어난 모델이 많이 있습니다.\n\n포토 리얼리즘, 만화, 애니메이션 이미지 등을 포함한 수천 가지 특정 사용 사례에 대한 모델이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, DreamShaper, Juggernaut 및 RealCartoon은 안정적 확산 1.5를 기본 모델로 사용하지만 놀라운 결과를 제공하는 몇 가지 모델 중의 몇 가지입니다:\n\n## 안정적 확산 2.x 모델\n\n2022년 말에 출시된 2.x 시리즈에는 2.0 및 2.1 버전이 포함됩니다. 이러한 모델은 768x768 픽셀의 해상도를 갖추고, ViT-H/14라는 다른 CLIP 모델을 사용하여 프롬프트를 더 표현적으로 만듭니다.\n\n2.x에서 사용된 다른 CLIP 모델로 인해 사람들이 1.x에서 마이그레이션하는 것이 어려워졌습니다. 사실 프롬프트가 그렇게 잘 전환되지 않아서 오픈 소스 커뮤니티에서의 널리 사용이 급격히 줄었습니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 텍스트를 친절한 한국어로 번역한 것입니다.\n\n이 모델들의 매개변수 개수는 GitHub Readme에 따르면 1.5억개로 동일합니다.\n\n# 샘플 출력\n\n# 주요 사항\n\n- 해상도 (픽셀): 768x768\n- 모델 카드\n- 라이선스: CreativeML Open RAIL++-M — 상업적 및 비상업적 사용\n\n<div class=\"content-ad\"></div>\n\n이 모델의 좋은 사용 사례: 1.x 모델과 비교하여 더 높은 해상도의 출력물. 복잡하고 표현력이 풍부한 프롬프트를 효율적으로 처리. 사람보다는 건축물과 풍경 소재에 대한 성능이 뛰어남. 다양한 색상의 동적 범위를 제공함.\n\n이 모델의 부적합한 사용 사례: 세대에 제약이 많음. 유명인과 미술 양식에 대한 검열이 있음.\n\n# 세분화된 모델\n\n안정적인 확산 2.0 및 2.1은 오픈 소스 커뮤니티에서 1.5만큼 널리 채택되지 않았습니다. 그러나 세분화된 모델은 일부 존재합니다.\n\n<div class=\"content-ad\"></div>\n\n# Stable Diffusion XL 1.0\n\n2023년에 출시된 SDXL 1.0은 중간단계와 Dall-E 수준의 결과물을 소비자용 하드웨어에서 실행할 수 있습니다. 1024x1024 픽셀의 해상도를 제공하며 텍스트 조건부로 OpenCLIP-ViT/G 및 CLIP-ViT/L을 활용하는 SDXL을 통해 원하는 결과를 훨씬 쉽게 얻을 수 있습니다.\n\n초기 출시인 Stability AI의 SDXL 1.0은 35억 개의 기본 모델 파라미터와 66억 개의 모델 앙상블 파이프라인을 갖추고 있습니다:\n\n![Stable Diffusion Models](/assets/img/2024-06-19-ComparingStableDiffusionModels_0.png)\n\n<div class=\"content-ad\"></div>\n\n# 샘플 출력\n\n# 주요 사항\n\n- 해상도 (픽셀): 1024x1024\n- 모델 카드  \n- 라이선스: CreativeML Open RAIL++-M 라이선스 — 상업적 및 비상업적 사용 가능\n\n이 모델의 좋은 사용 사례: 안정적인 확산 모델 중에서 가장 높은 해상도 출력. 개선된 색상 깊이, 구성, 전체 이미지 품질. 복잡한 프롬프트와 개념의 이해가 더 좋아짐.\n\n<div class=\"content-ad\"></div>\n\n이 모델의 적합하지 않은 사용 사례: 로컬에서 실행하려면 상당한 계산 자원이 필요합니다. 소비자급 하드웨어에서 실행하기 어려울 수도 있습니다. 손과 같은 것들은 아직 완벽하지 않을 수 있습니다.\n\n# 세밀하게 조정된 모델\n\n오픈 소스 커뮤니티는 SDXL을 환영하고 SDXL로 품질 높은 출력물을 생산하는 몇 가지 세밀하게 조정된 모델을 출시했습니다.\n\nJuggernaut XL, DreamShaper XL, RealVisXL, Animagine XL 등이 인기가 많으며 다양한 사용 사례를 제공할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n# SDXL Turbo\n\nSDXL Turbo은 SD XL 1.0의 간추린 버전으로, 512x512 픽셀 이미지를 빠르게 생성하기 위해 설계되었습니다. SD XL 1.0과 동일한 텍스트 조건 모델을 사용하며 35억 개의 매개변수를 갖고 있습니다. SDXL Turbo는 단 한 단계만으로 이미지를 생성할 수 있습니다.\n\n# 샘플 출력\n\n# 주요 사실:\n\n<div class=\"content-ad\"></div>\n\n- 해상도 (픽셀): 512x512\n- 모델 카드\n- 라이선스: 비상용 소프트웨어 — 비상업적 사용만 가능\n\n**이 모델을 잘 활용할 수 있는 경우:** 짧은 시간 안에 좋은 결과물을 제공합니다. 프로토타입 애플리케이션 및 워크플로에 유용합니다. 실시간 실험에 적합합니다.\n\n**이 모델을 잘 활용하지 못하는 경우:** 비상업용 라이선스로 개인 및/또는 연구용으로만 사용이 가능합니다.\n\n# 세밀하게 조정된 모델\n\n<div class=\"content-ad\"></div>\n\n2.1처럼 SDXL Turbo의 오픈 소스 모델 생태계는 제한적입니다. 모델은 존재하지만, 대부분의 제작자들이 SDXL 및 SD 1.5와 같이 더 인기 있는 기본 모델에 노력을 기울이고 있습니다.\n\n# Stable Cascade\n\nStable Cascade은 Würstchen 아키텍처를 사용하는 독특한 모델로, 더 효율적인 훈련 및 추론을 가능케 합니다. 3단계(C, B, A)로 작동하며, 압축 계수는 42입니다:\n\n![Stable Cascade](/assets/img/2024-06-19-ComparingStableDiffusionModels_1.png)\n\n<div class=\"content-ad\"></div>\n\nStages C (10억 또는 36억 개의 매개변수) 및 B (7억 또는 15억 개의 매개변수)은 상호 교환 가능하며 하드웨어 요구 사항 및/또는 제한에 따라 다양한 모델을 사용할 수 있습니다.\n\nSDXL Turbo와 마찬가지로 Stable Cascade은 연구용 모델입니다.\n\n## 샘플 출력\n\n## 주요 사실:\n\n<div class=\"content-ad\"></div>\n\n- 해상도 (픽셀): 1024x1024\n- 모델 카드\n- 라이선스: 소유권 제한 - 비상업적 사용만 허용\n\n이 모델의 좋은 사용 사례: SDXL 품질의 출력물과 더 나은 프롬프트 이해를 제공합니다. 사용된 모델에 따라 더 빠른 출력을 제공할 수 있습니다. 손가락, 이빨 등의 세부사항을 더 잘 생성합니다.\n\n이 모델의 부적합한 사용 사례: 모델을 불러오려면 상당한 VRAM이 필요합니다. 오픈 소스 커뮤니티의 광범위한 지원이 아직 확인되지 않았습니다.\n\n# 세밀하게 조정된 모델들\n\n<div class=\"content-ad\"></div>\n\n현재 Stable Cascade에 대해 Fein-tuned 모델은 매우 적습니다.\n\n# Stable Diffusion 3.0\n\n2024년 3월 발표된 Stable Diffusion 패밀리의 최신 버전인 Stable Diffusion 3.0입니다. 자세한 내용은 아직 부족하지만, 초기 결과로는 프롬프트 정렬 및 전체 이미지 품질에서 상당한 개선이 나타났습니다.\n\n만약 Stable Diffusion 3이 출시되면 이 섹션을 업데이트하겠습니다.","ogImage":{"url":"/assets/img/2024-06-19-ComparingStableDiffusionModels_0.png"},"coverImage":"/assets/img/2024-06-19-ComparingStableDiffusionModels_0.png","tag":["Tech"],"readingTime":5},{"title":"개성있는 애니메이션 미미 만들기 세밀하게 조정된 텍스트-이미지 모델 사용하기","description":"","date":"2024-06-19 21:00","slug":"2024-06-19-CreatingPersonalizedAnimatedMemesUsingFine-tunedText-to-imageModels","content":"\n\n## Stable Diffusion과 Deforum을 사용한 개인 맞춤형 미미 생성\n\n안녕하세요! 이 블로그 포스트에서는 Deforum과 섬세하게 조정된 Stable Diffusion(SD) 모델을 사용하여 맞춤형 및 애니메이션 미미를 만드는 과정을 소개하고, 스타일 전이 도구로 사용한 Visual Guide 출처의 몇 가지 미미 예제를 제시할 예정입니다.\n\n시작하기 전에, 아직 저의 블로그에서 섬세하게 조정된 SD 모델을 사용하여 미미 이미지를 생성한 내용을 확인하지 않으셨다면 아래 링크를 확인해보세요:\n\n전체 코드는 저의 GitHub 페이지에서 확인할 수 있습니다. 필요한 부분은 여기 있어요.\n\n<div class=\"content-ad\"></div>\n\n# 단계 1: 모델 파인 튜닝\n\n첫 번째 단계는 SD 모델을 파인 튜닥하는 것입니다. 저는 SD1.5 모델을 선택하고 해당 모델의 가중치를 여기서 다운로드 받았습니다. 그런 다음에는 제 PC가 10GB VRAM을 가진 RTX3080을 사용하기 때문에 로컬에서 학습 알고리즘을 실행했습니다. 충분히 강력한 성능을 가진 듯합니다.\n\n파인 튜닥에 사용할 이미지를 선택할 때 주의하세요. 서로 다른 배경과 표정을 가진 이미지를 사용해 보세요. 저는 다음과 같은 이미지들을 사용했습니다.\n\n![이미지](/assets/img/2024-06-19-CreatingPersonalizedAnimatedMemesUsingFine-tunedText-to-imageModels_0.png)\n\n<div class=\"content-ad\"></div>\n\n훈련 후 모델 체크포인트 파일을 리포지토리의 models/ 하위 폴더로 이동해주세요.\n\n![이미지](/assets/img/2024-06-19-CreatingPersonalizedAnimatedMemesUsingFine-tunedText-to-imageModels_1.png)\n\n# 단계 2: 메멘티콘 템플릿 준비\n\n두 번째 단계는 Deforum 알고리즘을 실행하기 위해 필요한 파일을 준비하는 것입니다: 확산할 소스 비디오, 마스크 비디오 및 설정 파일. 더 쉽게 따라올 수 있도록, 이러한 단계를 세 가지 하위 섹션으로 설명하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## 단계 2.1: 확산할 소스 비디오 다운로드하기\n\n이 단계에서는 우리의 템플릿으로 사용할 비디오를 간단히 다운로드합니다. 사람이 포함된 비디오면 어떤 비디오든 사용할 수 있습니다. 저는 아래 비디오를 선택했습니다. 해당 비디오를 다운로드하고 templates/dimitri_finds_out/source.mp4 경로에 저장했습니다.\n\n## 단계 2.2: 마스크 비디오 생성\n\n디포럼 알고리즘은 마스크 비디오를 입력으로 사용할 수 있습니다. 마스크 비디오는 확산을 원하는 입력 비디오와 동일한 프레임 수를 가져야 합니다. 각 프레임은 확산을 위한 ROI가 검정색이고 터치하지 않을 부분이 흰색인 이진 강도로 구성됩니다. 그런 다음, 이것은 이진 분할 문제가 되며, 이에 대한 알고리즘은 문헌에서 흔히 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n저는 PixelLib이라는 사용하기 쉬운 라이브러리를 알고 있습니다. 이 라이브러리는 기본적으로 PointRend를 기반으로 한 시맨틱 세그멘테이션 라이브러리입니다. 몇 줄의 코드로 세그먼트된 프레임을 생성할 수 있게 해줍니다. 사용하려면 이 링크에서 PointRend 모델을 다운로드하여 models/ 하위폴더로 이동해야 합니다.\n\n마스크 생성을 자동화하는 스크립트 create_mask.py를 작성했습니다. 이 스크립트는 소스 비디오 경로와 출력 마스크 비디오를 저장할 경로 두 가지를 입력받습니다.\n\n```js\npython create_mask.py \\\n  --video_path=\"templates/dimitri_finds_out/source.mp4\" \\\n  --save_path=\"templates/dimitri_finds_out/mask.mp4\"\n```\n\nPixelLib은 시맨틱 세그멘테이션 라이브러리이므로 프레임에서 감지된 각 사람에 대한 마스크를 제공합니다. 우리가 관심 영역을 선택하기 위해, 모든 감지된 사람 중에서 면적이 가장 큰 사람을 선택하는 절차를 구현했습니다. 이는 다음 두 함수에 의해 수행됩니다:\n\n<div class=\"content-ad\"></div>\n\n```js\ndef max_nonzero_channel(arr):\n    # 채널 차원을 제외한 모든 차원을 합산합니다\n    channel_sums = np.sum(arr != 0, axis=tuple(range(arr.ndim-1)))\n    # 가장 높은 합을 가진 채널의 인덱스를 찾습니다\n    max_channel_index = np.argmax(channel_sums)\n    return max_channel_index\n\ndef frame_segmentation(path_to_extracted_frames, frame_count, save_path_for_mask_frames):\n    create_folder(save_path_for_mask_frames) # 폴더가 없을 경우 생성합니다\n    for index in tqdm(range(frame_count)):\n        # 비디오의 각 프레임에 대해 PointRend를 실행합니다\n        r, output = ins.segmentImage(f\"{path_to_extracted_frames}frame{index}.jpg\", \n                                     show_bboxes=True, segment_target_classes=target_classes, \n                                     save_extracted_objects=False, mask_points_values=False, \n                                     extract_segmented_objects=True, output_image_name=None)\n        # 사람이 감지되었다면 최대 nonzero 픽셀 개수를 가진 채널을 선택합니다\n        if r[\"masks\"].ndim == 3:\n            picked_object = int(max_nonzero_channel(r[\"masks\"][:, :, :]))\n            cv2.imwrite(f\"{save_path_for_mask_frames}mask{index:05}.png\", (1 - r[\"masks\"][:, :, picked_object]).astype(int) * 255)\n        # 그렇지 않으면 흰 프레임을 생성합니다\n        else:\n            cv2.imwrite(f\"{save_path_for_mask_frames}mask{index:05}.png\", np.ones((output.shape)).astype(int) * 255)\n```\n\n다음 비디오는 PointRend의 출력 및 후처리된 마스크 비디오로 선택한 소스 비디오의 내용을 보여줍니다.\n\n![비디오 링크](https://miro.medium.com/v2/resize:fit:1200/1*YSFCLiOQ-kPB-bf_4MQQ1g.gif)\n\ncreate_mask.py를 실행한 후, 마스크 비디오가 템플릿의 하위 폴더에 있음을 확인해주세요.\n\n<div class=\"content-ad\"></div>\n\n## 단계 2.3: Deforum 설정 준비\n\n마지막 단계는 템플릿용 Deforum 알고리즘을 위한 settings.txt 파일을 생성하는 것입니다. 이 파일은 기본적으로 Deforum 매개변수 목록으로, 여기에 중요한 몇 가지가 있습니다: seed_behavior, animation_prompts, video_init_path, 그리고 video_mask_path.\n\nseed_behavior: Deforum은 일반적으로 SD 실행을 위해 무작위 시드를 사용하지만, 애니메이션 일관성을 위해 \"고정\"으로 설정해야 합니다. 이렇게 하면 확산된 영역의 색상과 세부 사항이 연속된 프레임 내에서 의사 일관되게 유지됩니다.\n\nanimation_prompts: 일반적인 SD 프롬프팅과 달리 우리는 풍경에 대한 많은 세부 정보와 신호 입력할 필요가 없습니다. 입력 비디오를 공급하면 공간 영역의 대부분 세부 사항이 보존됩니다. 따라서 “sks 사람 사진, 현실적인 얼굴”과 같은 스타일 신호 몇 가지만 입력해도 충분합니다. 저의 비디오에는 웃고 춤추는 남성이 포함되어 있으므로 입력한 프롬프트는 “댄스 클럽에서 웃고 춤추는 sks 남성 사진, 현실적인 얼굴, 초상화 세부 사항, 강조 조명…”입니다.\n\n<div class=\"content-ad\"></div>\n\nvideo_init_path 및 video_mask_path: 이러한 매개변수는 각각 source.mp4 및 mask.mp4의 경로입니다.\n\ndimitri_finds_out 템플릿 설정 중 일부는 다음과 같습니다:\n\n```js\n{\n    \"ENABLE_STORY_MODE\":\"True\",\n    \"batch_name\":\"dimitri_finds_out\",\n    \"width\":900,\n    \"height\":900,\n    \"bit_depth_output\":8,\n    \"seed\":-1 ,\n    \"seed_behavior\":\"fixed\",\n    \"sampler\":\"euler_ancestral\",\n    \"steps\":70,\n    \"scale\":15,\n    \"ddim_eta\":0.0,\n    \"filename_format\":\"{timestring}_{index}_{prompt}.png\",\n    \"use_init\":false,\n    \"init_image\":\"\",\n    \"strength\":0.75,\n    \"use_mask\":false,\n    \"use_alpha_as_mask\":false,\n    \"invert_mask\":false,\n    \"animation_prompts\":{\n \"0\": \"picture of laughing sks man in a dance club, realistic face, ultra detailed face, accent lighting, extremely detailed, ultra detailed, intricate details, high composition, 8k, cinematic lighting, blurry:-1, disfigured:-1, ugly:-1, deformed:-1, bad anatomy:-1, poorly drawn face:-1, poorly drawn hands:-1, malformed hands:-1, disgusting:-1, poorly drawn:-1, poorly drawn face:-1\"\n    },\n    \"animation_mode\":\"Video Input\",\n    \"max_frames\":348,\n    \"diffusion_cadence\":\"6\",\n    \"border\":\"warp\",\n    \"video_init_path\":\"templates/dimitri_finds_out/source.mp4\",\n    \"extract_nth_frame\":1,\n    \"overwrite_extracted_frames\":true,\n    \"use_mask_video\":true,\n    \"video_mask_path\":\"templates/dimitri_finds_out/mask.mp4\",\n    \"interpolate_key_frames\":false,\n    \"fps\":24\n}\n```\n\n설정 파일을 settings.txt로 저장합니다. 아래에 나타낸대로, 소스 비디오, 마스크 비디오, 및 설정 파일은 templates/dimitri_finds_out/ 하위 폴더에 저장됩니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-CreatingPersonalizedAnimatedMemesUsingFine-tunedText-to-imageModels_2.png\" />\n\n# 단계 3: Deforum 실행하기\n\n이제 Deforum 스크립트를 실행할 준비가 되었습니다. 저는 Deforum 기능을 제어하는 메타 스크립트인 run.py를 만들었습니다. 다음과 같이 명령줄에 두 가지 인자를 제공해야 합니다. 템플릿 이름과 여러분의 미세 조정된 모델의 이름입니다:\n\n```js\npython run.py \\\n  --meme_template=\"dimitri_finds_out\" \\\n  --finetuned_model_path=\"your_finetuned_model.ckpt\"\n```\n\n<div class=\"content-ad\"></div>\n\n하드웨어에 따라 스크립트 실행이 몇 분 정도 걸릴 수 있습니다. 제 경우에는 약 5분이 소요됩니다. 결과 동영상은 output/ 폴더에 저장됩니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1200/1*SlWST_jBJkg7rUxj2aHUMQ.gif)\n\n# 단계 4: 동영상에 텍스트 추가\n\n상상력과 유머 감각이 발휘되는 단계입니다. 우리는 동영상에 텍스트를 추가하여 미eme의 필수 요소인 비디오에 텍스트를 추가합니다. 제 메타 스크립트에는 텍스트 추가 기능이 구현되어 있지 않습니다(곧 추가할 예정). ffmpeg를 사용하여 명령줄을 통해 동영상에 텍스트를 추가할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```bash\nffmpeg -i 경로/비디오 -vf \"drawtext=text='당신의 텍스트':x=(w-tw)/2:y=(h-th)/12:fontfile=C:/Windows/Fonts/Arial.ttf:fontsize=40:fontcolor=white\" 경로/출력/비디오\n```\n\n아래는 제가 한 작업입니다:\n\n![이미지](https://miro.medium.com/v2/resize:fit:1200/1*BjphB2HVk5ANzpA_f1y-fA.gif)\n\n제 블로그를 읽어주셔서 감사합니다. 곧 비슷한 콘텐츠 생성을 위해 ControlNet을 사용하는 또 다른 블로그를 작성할 예정입니다.\n\n<div class=\"content-ad\"></div>\n\n좋은 하루 보내세요!","ogImage":{"url":"/assets/img/2024-06-19-CreatingPersonalizedAnimatedMemesUsingFine-tunedText-to-imageModels_0.png"},"coverImage":"/assets/img/2024-06-19-CreatingPersonalizedAnimatedMemesUsingFine-tunedText-to-imageModels_0.png","tag":["Tech"],"readingTime":8},{"title":"ComfyUI에서 의상을 어떻게 변경하나요","description":"","date":"2024-06-19 20:58","slug":"2024-06-19-HowtoChangeOutfitsinComfyUI","content":"\n\n일반적으로 ComfyUI나 Automatic1111에서 옷을 바꾸는 과정은 캐릭터 포즈를 유지하면서 원하는 스타일을 적용하는 데 조금의 프롬프트 엔지니어링이나 LoRA가 필요한 귀찮은 인페인팅과 제어넷을 필요로 합니다.\n\n조금의 실험 끝에 IPAdapter의 스타일 추출과 Grounding Dino 및 Segment Anything 모델의 정확한 세분화를 결합하면 후처리를 최소화하면서 매우 정확하고 불편한 옷 갈아입기를 할 수 있습니다.\n\n다음은 방법입니다:\n\n작업흐름 및 상세가이드: 이 작업을 실제로 확인하고 워크플로를 다운로드하고 싶다면, Prompting Pixels 웹사이트에서 무료로 이용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 최종 결과\n\n워크플로우 세부사항에 대해 들어가기 전에, 이전과 이후의 이미지를 시각적으로 보여드립니다:\n\n# 작업 공간 설정\n\n세 가지 다른 그룹으로 분리하는 것이 조직적 관점과 전반적인 프로세스에서의 상황을 고려할 때 좋은 방법이라고 생각했습니다. 이 세 그룹은 다음과 같습니다: 기본 워크플로우, IPAdapter 및 세분화. 각각에서 무엇이 벌어지고 있는지 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n# 기본 작업 흐름\n\n여기서 소개하는 것은 기본적으로 ComfyUI에서 시작하는 기본 작업 흐름입니다. 여기서 가장 주목할 만한 변화는 전체 이미지가 아닌 부분 이미지를 다루기 때문에, 생성 체크포인트 대신 인페인팅 체크포인트를 불러와야 한다는 것입니다.\n\n그리고 추천하는 좋은 SDXL 체크포인트로는 RealVision, ICBINP XL 또는 기본 SDXL 인페인팅 체크포인트가 있습니다.\n\n당신의 프롬프트에 대해, 특정 세그먼트를 변경할 것이기 때문에 무엇이 나타나길 원하는지 정의해야 합니다. 이 예시에서는 기본 셔츠를 화려한 하와이안 셔츠로 변환할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# IPAdapter\n\nIPAdapter은 이미지 프롬프트 어댑터를 의미합니다. 기본적으로 이 노드들은 스타일이나 사람의 일반적인 특징을 모델로 전송할 수 있습니다.\n\n이를 작은 LoRA나 텍스트 임베딩처럼 생각해보세요.\n\n따라서 이미지를 입력으로 제공하고 이미지에서 관련 정보를 추출하는 것을 Load Image 노드를 통해 수행해야 합니다.\n\n<div class=\"content-ad\"></div>\n\nComfyUI에 이미지 프롬프트 어댑터(IPAdapter)를 설정하려면 CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors 및 ip-adapter-plus_sdxl_vit-h.safetensors 모델을 불러와야 합니다. 이 모델들과 관련된 다른 모델들은 IPAdapter GitHub 리포지토리에서 찾을 수 있습니다.\n\n해당 디렉터리에 모델을 넣은 후 이미지 출력을 Load Image 노드에서 IPAdapter 고급 노드로 전달해야 합니다.\n\n또한, CLIP 모델을 Load CLIP Vision과 IPAdapter Unified Loader를 IPAdapter 고급 노드로 가져와야 합니다. 이전에 언급된 IP 어댑터 모델을 사용하는 경우 프리셋을 PLUS(고 강도)로 설정해야 합니다.\n\n마지막으로 IPAdapter Advanced에서 weight_type를 스타일 전송으로 변경해야 합니다.\n\n<div class=\"content-ad\"></div>\n\nIPAdapter Advanced로부터의 모델 출력은 직접 KSampler 노드로 들어가게 되는데, 수정된 모델 파일은 이제 원하는 입력에 기반하여 정확하게 이미지/스타일을 그릴 수 있습니다.\n\n# 세분화\n\n이 프로세스 중 가장 멋진 부분 중 하나는 GroundingDino 모델의 구현입니다. 이 세분화 모델은 텍스트 프롬프트를 제공하고 해당 이미지 내에서 그것을 찾아서 그에 맞게 분할할 수 있습니다.\n\n이것은 굉장히 강력한 기능입니다.\n\n<div class=\"content-ad\"></div>\n\n저희의 작업 흐름 중에 셔츠를 세분화하는 데 사용 중인데, 실제로 모자부터 신발, 심지어 전체 배경까지 거의 모든 것을 세분화할 수 있어요. 가능성은 무한해요.\n\n이를 설치하려면 Segment Anything 사용자 정의 노드를 가져와야 해요 (ComfyUI 매니저나 GitHub 저장소를 통해 이용 가능해요).\n\nIPAdapter와 같이 세분화할 때 이미지가 먼저 입력이 되어야 해요.\n\n그러므로 Load Image 노드를 설정하고 그 다음 GroundingDinoSAMSegment 노드로 전달하세요.\n\n<div class=\"content-ad\"></div>\n\n또한, SAMModelLoader 노드와 GroundingDinoModelLoader의 Segment Anything 모델을 가져와야 합니다. 처음 실행할 때 이 모델 로더 노드들은 관련 모델(약 3GB)을 다운로드한 다음 GroundingDinoSAMSegment 노드로 전달합니다.\n\nGroundingDinoSAMSegment 노드에는 세그먼트할 객체의 단어를 입력할 수 있는 텍스트 필드가 있습니다. 셔츠, 안경 등과 같이 세그먼트하려는 객체의 단어를 입력할 수 있습니다.\n\n이제 셔츠, 안경과 같은 여러 객체를 정의할 수 있지만, 신뢰성이 거의 없다는 것을 알았습니다:\n\n![이미지](/assets/img/2024-06-19-HowtoChangeOutfitsinComfyUI_0.png)\n\n<div class=\"content-ad\"></div>\n\n임계값에 대해 일반적으로 말하자면, 낮은 값은 모델이 선택을 더 자유롭게 할 수 있게 하지만 높은 값은 더 자신 있게 만듭니다. 값이 너무 높으면 선택된 것이 없는 오류가 발생할 수 있습니다.\n\n분할한 후에는 마스크를 VAE 인코딩(인페인팅용) 노드(mask 입력)와 IPAdapter Advanced(주의 마스크 입력) 노드에 모두 전달해야 합니다. 원본 이미지는 VAE 인코딩(인페인팅용) 노드에도 전달되어야 합니다.\n\n선택 사항: 마스크에 FeatherMask 노드를 추가하여 가장자리를 부드럽게 만들어 더 나은 결과를 얻을 수 있습니다.\n\n# 큐 프롬프트 및 결과 검토\n\n<div class=\"content-ad\"></div>\n\n한 번 모두 연결되면 프롬프트를 대기열에 넣고 결과를 검토할 수 있습니다. 원하는 결과물의 꽤 정확한 표현이 셔츠가 멋진 하와이안 셔츠로 변환되었음을 확인해야 합니다.\n\n출력 이미지를 주의 깊게 살펴보고 원하는 결과물을 얻기 위해 필요한 대로 설정을 조정하세요.\n\n완벽하게 작동하는 완벽한 조합을 찾기 위해 다양한 설정으로 실험하고 다양한 모델을 테스트할 수 있도록하려면 설정을 조정하세요.\n\n# 이 프로세스의 장단점\n\n<div class=\"content-ad\"></div>\n\n제안하는 방법에 대한 장단점을 요약해 드리겠습니다. 이 워크플로우를 잘 활용하는 데 도움이 될 수 있어요:\n\n장점:\n\n- GroundingDino의 Zero-shot 객체 감지 기능을 사용하면 이미지를 자동으로 분할하고 보정할 수 있습니다.\n- 정확한 분할이 모델이 올바른 영역만 보정하도록 합니다.\n- '가상 시착'과 같은 소비자를 대상으로 하는 응용 프로그램이 가능합니다.\n\n단점:\n\n<div class=\"content-ad\"></div>\n\n- Segmentation이 추가적인 몇 개의 GB VRAM을 소비하므로, ControlNets, LoRAs, AnimateDiff 등 다른 노드들과 함께 사용할 때 문제가 발생할 수 있습니다.\n- 물리적인 적용이 아니라 스타일 적용만 가능합니다. 예를 들어, 긴 소매 셔츠가 IPAdapter에 입력되어도 최종 이미지는 여전히 단추 소매로 표시될 것입니다.\n\n👉 AI 예술 기술을 강화하고 싶다면, 무료 프롬프팅 픽셀 강좌를 확인해보세요.","ogImage":{"url":"/assets/img/2024-06-19-HowtoChangeOutfitsinComfyUI_0.png"},"coverImage":"/assets/img/2024-06-19-HowtoChangeOutfitsinComfyUI_0.png","tag":["Tech"],"readingTime":4},{"title":"내가 절대 놓치지 말아야 할 최고의 중간 여행 가이드  안내와 50개 이상의 이미지","description":"","date":"2024-06-19 20:55","slug":"2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images","content":"\n\n## 초보자부터 고급자까지\n\n최신 업데이트 : 24년 6월 8일 (이전에는 24년 6월 6일).\n\n헤이, 미드저니 크루 여러분.\n\n어떻게 지내세요? 여러분의 한 주가 잘되길 바래요! 미드저니에 대해 저만큼 열정적이신지 다들 기대가 되는 데, 그럼 여러분을 위해 좋은 소식이 있다고 해요: 내가 소개하는 개인 미드저니 가이드가 준비되어 있어요.\n\n<div class=\"content-ad\"></div>\n\n한 달 동안 Midjourney에 대해 글을 쓰고 있었는데, 이제 내 지식을(적어도 대부분) 모아서 Midjourney를 최대한 활용하는 간편한 가이드로 편집할 때가 온 것 같아요. 특히 Midjourney에 익숙하지 않은 경우에 유용하게 활용할 수 있도록 구성했습니다.\n\n이 가이드는 당신이 읽고 싶은 부분으로 쉽게 이동할 수 있도록 구조화되어 있어요. Midjourney의 역사와 설정 방법부터 다양한 모델과 기능에 대해 자세히 살펴보는 과정으로 시작해서,\n\n저는 이 가이드를 Midjourney를 최대한 활용하도록 하는 속임수 예제와 팁과 트릭으로 마무리할 거에요.\n\n그리고 이 가이드를 최대한 시각적으로 매력적으로 만들기 위해, Midjourney를 사용하여 제가 직접 만든 50개의 이미지를 포함했습니다. 이 이미지들은 엄청난 가능성을 보여줄 뿐만 아니라 당신이 자신만의 이미지를 만들도록 영감을 줄 거예요.\n\n<div class=\"content-ad\"></div>\n\n여기 Midjourney v6로 만든 첫 이미지가 있어요. 그것은 제게 전환점이었어요:\n\n![image](/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_0.png)\n\n멋지지 않나요?\n\n이 이미지는 1990년대를 완벽하게 담아냈고 솔직히 말해서, Midjourney로 만들지 않았다면 AI가 만들었다고는 믿지 못할 정도에요.\n\n<div class=\"content-ad\"></div>\n\n제목\n## Midjourney 소개\n\nMidjourney는 2년 전에 출시되었으며 많은 발전을 거듭해왔습니다. 이는 이전에 Leap Motion의 공동 창업자였던 David Holz에 의해 설립되었습니다. Midjourney는 출시 이후 여러 중요한 업데이트를 통해 서비스를 더 신뢰성 있고 현실적으로 만들어 왔습니다.\n\n## Midjourney 설정하기\n\n이 안내서를 마음에 드는 순서로 자유롭게 읽거나, 가장 관심 있는 부분을 선택해 보세요.\n\n<div class=\"content-ad\"></div>\n\n처음에 Midjourney는 Discord를 기반으로 했습니다. 그들의 웹사이트에 가입해도 Midjourney에 접속하려면 활성화된 Discord 계정이 필요합니다.\n\nDiscord 계정이 없다면, 먼저 하나를 설정해야 합니다. 하지만 그것은 꽤 쉽고 큰 문제는 아닙니다.\n\n최근까지 Midjourney에 접속하는 유일한 방법은 Discord 앱을 사용하는 것이었습니다. Discord의 가장 큰 단점은 혼잡한 서버로, 이는 새로운 사용자들을 돌아서게 할 수 있습니다.\n\n그리고 Midjourney 봇을 사용하여 개인 서버를 설정하는 것 또한 쉽지 않았습니다.\n\n<div class=\"content-ad\"></div>\n\n당신은 개인 Discord 서버를 설정하는 방법에 대해 더 많이 알아볼 수 있어요:\n\n하지만 이 모든 것은 알파 웹사이트가 공개된 이후 대부분 사람들에게 공개되었기 때문에 모두 변했어요.\n\n새로운 Midjourney 웹사이트는 깨끗하고 사용하기 쉬우며 새로운 기능이 정기적으로 업데이트되어요. 이 안내서에서는 웹사이트에 초점을 맞추지만, 일부 소수의 고급 기능이 아직 누락되어있기 때문에 제가 사진을 구성하는 방법과 사용의 편리함을 선호해요.\n\n여전히 Discord를 사용 중이라면 웹 인터페이스로 전환을 고려할 수 있어요. 전환하는 이유에 대해 추가로 읽을 수 있도록 이곳에 글을 썼어요:\n\n<div class=\"content-ad\"></div>\n\n이미지를 생성하는 방법은 간단한 텍스트 프롬프트에 달려 있습니다. Midjourney는 해당 텍스트를 해석하고 사진을 생성할 것입니다. 그러나 이미지 생성에 들어가기 전에 먼저 Midjourney 모델들을 이해해야 합니다.\n\n# 모델 버전 이해\n\nMidjourney가 2년 전에 출시되었을 때, 그것은 겸손한 시작을 가졌지만 오늘날 만들 수 있는 것과는 비교할 바가 없는 이미지를 만들었습니다. 그 때도 정말 인상적이었지만, 3D 컴퓨터 게임 그래픽 초기 단계를 많이 생각나게 했습니다.\n\n![이미지](/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_1.png)\n\n<div class=\"content-ad\"></div>\n\nV1에서 프롬프트가 어떻게 나타나는지 살펴보세요:\n\n\n![Image1](/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_2.png)\n\n\nV6에서 동일한 프롬프트를 보겠습니다:\n\n\n![Image2](/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_3.png)\n\n\n<div class=\"content-ad\"></div>\n\n차이가 놀라울 정도로 놀라운 것 같아요! 세부 사항과 전체적인 사실적인 모습을 살펴보세요.\n\n일반 모델 외에도 이미지와 스타일의 다양성을 만들 수 있는 중요 모델들 외에도 Midjourney에서는 애니메이션 및 설명적인 스타일을 만들기 위해 특화된 Niji 모델도 출시했습니다.\n\n### V6 모델 (& V5.2 모델)\n\nV5는 2022년 3월에 출시되었으며, 이후 5.1 버전과 5.2 버전이 각각 2022년 5월과 6월에 출시되었습니다.\n\n<div class=\"content-ad\"></div>\n\n사실 V5.2는 Midjourney와 genAI(생성적 AI) 세계로 진입한 실제적인 시작점이었습니다. 이미지는 놀라울 만큼 멋있었고, 제가 말 그대로 놀라 바닥에 엎드렸습니다.\n\n제 마음은 즉시 뛰어나가며, 디지털 아트와 사진의 미래에 대한 의미를 상상했습니다. 그러나 이러한 함의는 이해하기 어려웠습니다. 이 모든 것을 능가하는 건 V6의 출시로, 현실감이 새로운 수준으로 끌어올렸습니다.\n\n## V5.2와 V6의 차이점은 무엇인가요?\n\n주요한 차이는 Midjourney가 프롬프트를 해석하는 방식입니다.\n\n<div class=\"content-ad\"></div>\n\nV5.2에서는 추가 프롬프트 세부 사항을 엄격한 지침이 아니라 모호한 제안으로 취급했지만 V6에서는 프롬프트를 더 꼼꼼히 따랐습니다. 이로 인해 프롬프트와 더 일치하는 이미지가 생성되었습니다.\n\n내 의견으로는 직접 비교해보면 V5.2는 예술적인 이미지를 더 많이 생성했지만, V6은 더 사실적인 사진을 생성합니다.\n\n다음은 V5.2와 V6에서 동일한 프롬프트입니다:\n\n\n![이미지](/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_4.png)\n\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_5.png\" />\n\nMidjourney의 표현과 prompt 처리 방식으로 두 이미지를 비교하는 것은 분명히 어렵지만, V5.2가 거의 완벽하고 흠도 없는 것은 분명합니다.\n\n반면 V6은 섬세한 피부 세부 사항 심지어 피부 혹과 같은 것들이 보이지만, 조금 과도하게 스타일링된 것일 수 있습니다 (이후에 매개변수를 사용하여 조정하는 방법을 설명합니다).\n\n## Niji Model\n\n\n<div class=\"content-ad\"></div>\n\nNiji 모델은 애니메이션 캐릭터(그림)에 특화되어 있어요. 일반 모델로 동일한 결과를 얻으려고 해도 거의 불가능할 거에요.\n\n만약 그런 스타일을 좋아하신다면 디스코드에서 “--niji 6” 명령을 사용하거나 웹사이트 설정에서 모델을 변경하실 수 있어요.\n\n![이미지](/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_6.png)\n\n이것은 Niji 모델의 훌륭한 예시에요.\n\n<div class=\"content-ad\"></div>\n\n이제 이미지를 만드는 방법을 살펴봅시다. (참고: 이 작업은 상당히 간단하지만 여러분이 큰 문제를 일으키는 것이 작은 것들입니다).\n\nNiji 모델에 대해 더 자세히 알고 싶다면 이 이야기를 다른 탭에서 열어보세요:\n\n### 이미지 만들기\n\nMidjourney에서 이미지를 만드는 것은 매우 간단합니다. 텍스트 프롬프트를 작성하면 정확한 이미지를 생성하기 위해 최선을 다할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n그냥 한 마디나 한 문장을 적고 Midjourney가 어떤 것을 만들 것인지 확인하세요. 이것은 정말 재밌을 수 있고 Midjourney의 창의적인 자유가 놀라운 것을 보는 재미가 있습니다.\n\n여기에 이에 대해 쓰고 Midjourney를 자유롭게 행동하게 하는 것이 훌륭한 실험이었습니다.\n\n하지만 보통은 Midjourney에게 정확히 원하는 이미지를 만들도록 안내하고 싶을 것입니다. 이것은 분명히 시간과 연습이 필요할 것입니다.\n\n예를 들어 다음 프롬프트는 몇 가지 훌륭한 이미지를 만들어 냈지만, 완전히 다른 방향으로 나갈 수도 있었을 것입니다. 이렇게 짧은 프롬프트로는 방향을 제시할 수 없습니다.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_7.png\" />\n\n다음 (완전한 것이 아닌) 팁 목록을 사용하세요:\n\n- 설명적인 언어 사용\n- 가능한 한 구체적으로\n- 모호함 피하기\n- 대상/장면을 설명하는 데 키워드 사용\n- 쉼표로 구분된 입력 사용\n- 부정적인 표현 사용x\n- 스타일 설명 (예: 예술 스타일, 사진 스타일, 조명, ...)\n\n좋은 프롬프트의 예시를 보려면 다음 예제를 참고하세요:\n\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_8.png\" />\n\nThe complete opposite would be this one line prompt with a subpar result\n\n<img src=\"/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_9.png\" />\n\nAdmittedly the photo is not bad, because the training data is great and Midjourney tends to go the more artsy and beauty route, but frankly it’s in my opinion nothing compared to the other image.\n\n\n<div class=\"content-ad\"></div>\n\n그리고 또 하나 중요한 개념이 나타났어요: 가끔은 작은 차이로 반복해서 동일한 프롬프트를 실험하고 실행하면 더 나은 결과를 얻을 수도 있어요.\n\n프롬프트에 대해 확신이 없다면, 작게 시작하고 세부 사항과 스타일을 추가해보세요.\n\n# 미학\n\n당신의 프롬프트 외에도, 웹사이트 설정에서 발견할 수 있는 미학 옵션은 Midjourney가 이미지를 생성하는 방식을 변경할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n이미지\n\n## 스타일\n\n스타일 매개변수는 이미지의 전체적인 외관과 스타일을 변경합니다.\n\n1000의 경우 Midjourney가 아름답게 너무 좋아지는 과장된 양의 미화 이미지를 만들도록 하고, 0은 Midjourney에게 정확히 지시사항을 따르도록 지시합니다.\n\n<div class=\"content-ad\"></div>\n\n위의 예시에서 보듯이 스타일 값이 높을수록 이미지가 더 아름답고 완벽해집니다.\n\n스타일화에 대해 더 많이 썼으니 여기를 참고해주세요:\n\n## 이상함\n\n이상함은 정반대 방향으로 진행됩니다. 값이 높을수록 이미지는 더 실험적으로 변할 것입니다. 독특한 길을 가고 싶다면 이상함 값을 높여보세요.\n\n<div class=\"content-ad\"></div>\n\n결과는 완전히 무작위이며 예측할 수 없으므로 조심해서 사용해야 합니다.\n\n## 다양성\n\n다양성은 디스코드의 혼돈 매개변수와 동등합니다. 이미지가 얼마나 다른지를 결정합니다. 값이 높을수록 무작위성이 높아지고, 값이 낮을수록 이미지는 설명대로 예상대로 될 것입니다.\n\n초기 입력 프롬프트는 핑크색 안경을 쓴 의자에 앉아 있는 불독이 수영장에 있었습니다.\n\n<div class=\"content-ad\"></div>\n\n예상대로 값이 높을수록 Midjourney가 입력에서 멀어진 걸 볼 수 있어요.\n\n다양성을 일종의 무작위성으로 볼 수도 있어요.\n\n이제 세 매개변수를 모두 50%로 올렸을 때 어떤 일이 일어나는지 확인해 봅시다.\n\n이제 아름다움의 미학은 배경으로 밀려나 무작위성과 평균적인 얼굴 특징에게 자리를 내어주는 거에요.\n\n<div class=\"content-ad\"></div>\n\n단지 헤딩 1을 사용하여 표 형태로 만들었습니다.\n\n## 종횡비\n\n사용 목적에 따라 텍스트 프롬프트를 해석하는 방식을 조정하는 것이 의미가 있을 수 있어요. \n\n예를 들어 초상화 종횡비를 선택하면 사진에 한 명만 초점을 맞추려고 하는 것이라고 Midjourney에게 알려줄 수 있어요. 반면, 횡형 종횡비를 설정하면 동일한 프롬프트로 다른 결과를 얻을 수 있습니다. Midjourney는 세부사항을 추가할 공간이 더 많기 때문이에요.\n\n<div class=\"content-ad\"></div>\n\n이미지의 종횡비가 변경되어 전체적인 메시지도 변했죠. 저는 개인적으로 폭이 넓은 종횡비를 선호합니다.\n\n# 고급 기술\n\n다양한 고급 기술들이 있지만, 이 안내서에서는 웹사이트에 중점을 두고 있기 때문에 가장 중요한 몇 가지만 다룰 거에요.\n\n## 캐릭터 & 스타일 참고\n\n<div class=\"content-ad\"></div>\n\n캐릭터와 스타일 참조를 통해 다른 이미지를 참조할 수 있습니다. Midjourney는 전반적인 스타일(예: 예술 스타일)이나 새 이미지의 주요 캐릭터를 사용하기 위해 최선을 다할 것입니다.\n\n예를 들어, 캐릭터를 참조하고 아래에 설명된대로 다른 일련의 이미지에 넣을 수 있습니다. 불독은 Midjourney에 의해 만들어졌고 다른 프롬프트에서 참조했습니다.\n\n캐릭터 참조를 볼 때 원본 캐릭터를 완벽하게 복사하지는 않지만, 원본에 가까워지고 있습니다.\n\n스타일 참조도 동일한 방식으로 작동합니다.\n\n<div class=\"content-ad\"></div>\n\n참조 기능을 사용하려면, 생성한 이미지 중 하나를 사용하거나 이미지 인터페이스를 통해 새 이미지를 업로드할 수 있습니다.\n\n\n![image](/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_11.png)\n\n\n어떤 방식을 선택하더라도 이미지를 선택한 후에는 오른쪽 하단의 작은 아이콘을 클릭하여 참조 스타일을 변경할 수 있습니다.\n\n이미지 참조에 관한 포괄적인 가이드를 작성했어요:\n\n<div class=\"content-ad\"></div>\n\n## 이미지 혼합\n\n이미지 혼합을 통해 두 개 이상의 이미지를 결합할 수 있습니다. 이를 통해 이전에 만든 이미지나 새로 업로드한 이미지를 기반으로 쉽게 새 이미지를 생성할 수 있습니다.\n\n해야 할 일은 두 개 이상의 이미지를 프롬프트에 추가하고 엔터를 누르는 것뿐입니다.\n\n예시를 살펴보겠습니다. 이 경우에는 두 이미지를 결합하여 자연 풍경이 배경화면이 되고 방 안으로 자라나는 것이 목표였습니다.\n\n<div class=\"content-ad\"></div>\n\n결과가 꽤 인상적이라고 생각해요. 전체 구성을 유지하면서 새로운 미적 감각을 더했죠.\n\n블렌딩 기능에 대해 더 많이 쓴 팁과 꿀팁이 있는 자세한 내용은 여기를 참고해주세요:\n\n## 설명\n\n만약 인공지능에 이미지를 보여주면 그 이미지에 무엇이 있는지 말해주기만 하는 것이 아니라 이미지를 복제하는 데 도움이 되는 힌트를 생성해 줄 수 있다면 어떨까요?\n\n<div class=\"content-ad\"></div>\n\n이것이 바로 \"설명\" 기능이 할 수 있는 작업입니다.\n\n이미지를 업로드하면 마우스 커서를 가져다 대고 작은 \"i\" 아이콘을 클릭할 수 있습니다.\n\n프로세스가 완료되면 각 구절을 클릭하여 프롬프트에 추가할 수 있습니다. 결과는 다음과 같습니다:\n\n![image](/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_12.png)\n\n<div class=\"content-ad\"></div>\n\n이미지를 복사하는 것이 아니라 매우 비슷한 이미지를 생성한다는 점을 주의해야 합니다.\n\n## (미술 & 카메라) 스타일\n\n또 다른 중요한 점은 알아야 할 다양한 예술 스타일입니다.\n\nMidjourney에게 복제하려는 스타일이나 사용할 카메라 종류를 알리면 결과물이 완전히 다를 수 있습니다. 큐비즘과 표현주의와 같은 명백한 스타일은 물론 삼과누 아트와 같이 상상할 수 있는 거의 모든 것을 유도할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n위에 있는 예제와 동일한 프롬프트의 두 가지 예가 있습니다. 하나는 cubism에 가까운 예술 양식이고 다른 하나는 오래된 벽화입니다.\n\n# 더 나은 결과를 위한 꿀팁\n\n## 간단히 시작하세요\n\n나의 가장 좋은 조언 중 하나는 간단히 시작하는 것입니다. 너의 이미지에 많은 소재와 세부 사항을 추가하려고 하지 마십시오 (이 부분 아주 강조합니다). 이것은 작동하지 않으며, Midjourney를 혼란스럽게 할 가능성이 높습니다. 간단히 시작하고 추가하는 것을 잊지 마세요.\n\n<div class=\"content-ad\"></div>\n\n## 재생\n\n여기가 두 번째 팁입니다.\n\n자주 재생하세요.\n\n그리고 더 자주 재생하세요.\n\n<div class=\"content-ad\"></div>\n\n## 프롬프트 작성 방법 배우기\n\n제 Medium에 있는 이야기들 같은 프롬프트 영감을 확인해보세요. 정기적으로 새로운 아이디어와 영감을 주는 글을 올리고 있어요.\n\n이런 것들이 프롬프트 작성 방법을 배우는 데 도움이 될 거예요.\n\n<div class=\"content-ad\"></div>\n\n프롬프팅은 마법이 아니에요, 중간 여정에게 무엇을 원하는지 알려주는 것 뿐이에요.\n\n## 스타일 미학과 놀기\n\n가능한 한 중간 여정이 당신의 프롬프트를 가장 가깝게 따르도록 원한다면 스타일을 낮출 수 있어요.\n\n어떤 경우에든 다양한 미적 옵션으로 프롬프트를 여러 번 실행하면 예상치 못한 이미지를 얻을 수 있어요. 저는 가장 좋아하는 결과를 원할 때 200 스타일과 400 스타일로 프롬프트를 실행하는 것을 선호해요.\n\n<div class=\"content-ad\"></div>\n\n# 계기와 영감\n\n이제 정말 멋진 계기와 영감에 대해 이야기해보겠습니다.\n\n이것들은 모두 이전에 공유하지 않은 계기와 이미지들입니다. 그러니 이 안내서를 마친 후 내 다른 계기 아이디어도 확인하지 않을래? (마지막에 있는 링크를 확인해봐.) \n\n##  고양이 산책로\n\n<div class=\"content-ad\"></div>\n\n## 화성 사진 촬영\n\n## 호화로운 앵무새\n\n## 악어 목욕\n\n## 남자의 아름다움\n\n<div class=\"content-ad\"></div>\n\n여성 아름다움\n\n## 색상 분할\n\n마지막 것은 조금 까다로울 수 있어요.\n\n위의 이미지를 정확히 얻는 데는 시간이 오래 걸릴 수도 있고 많은 시도가 필요할 지도 몰라요. 하지만 색상 분할 이미지는 정말 멋진 아이디어에요. 여기 두 가지 더 예시가 있어요:\n\n<div class=\"content-ad\"></div>\n\n# 일반적인 문제\n\n다음은 제가 마주한 일반적인 문제 몇 가지입니다.\n\n## \"아니요\"가 \"아니\"가 아니다\n\nMidjourney는 부정적인 프롬프트에 문제가 있습니다. \"아니\"를 건너뛰고 여전히 제거하려는 모든 것을 추가합니다.\n\n<div class=\"content-ad\"></div>\n\n의견을 바꾸어 볼 수 있거나 각 실행마다 더 많은 내용을 추가하는 것이 좋을 수도 있습니다. 너무 많은 세부사항을 건드리지 않고 원하는 이미지로 가는 가장 쉬운 방법이에요.\n\n## 반응 없음\n\n또 다른 문제는 Midjourney가 가끔 반응하지 않을 때가 있어요. 프롬프트를 보내지만 이미지를 생성하지 않아서 이것이 귀찮을 수 있어요.\n\n로그아웃한 후 다시 로그인하는 방법을 시도해볼 수 있어요. 때로는 이 방법이 문제를 해결할 수 있어요. 또는 단순히 작업을 취소할 수도 있어요.\n\n<div class=\"content-ad\"></div>\n\n둘 다 작동하지 않으면 다른 프롬프트를 시도하고 걱정하지 마세요. 언젠가는 취소되고 대기열에서 제거될 겁니다.\n\n그게 다에요. \n\nMidjourney를 시작하고 프롬프팅 및 이미지 생성에서 능숙해지기 위한 상당히 포괄적인 가이드를 즐겁게 받아들였기를 바랍니다.\n\n좋아하신 점이나 더 알고 싶은 내용에 대해 의견을 자유롭게 남겨주세요. 박수를 보내고 공유하는 것도 잊지 마세요. 정말 도움이 돼요!\n\n<div class=\"content-ad\"></div>\n\n# 미드저니에 대해 더 많이 쓰고 있어요. 제가 가장 좋아하는 프롬프트 영감 링크를 공유할게요:\n\n![My Ultimate Midjourney Guide](/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_13.png)\n\n이 이야기는 Generative AI에서 발행되었어요. LinkedIn에서 저희와 연락하고, Zeniteq를 팔로우해서 최신 AI 이야기에 대한 소식을 받아보세요. \n\n저희 뉴스레터 구독하시면, 최신 generative AI 뉴스와 업데이트를 받아보실 수 있어요. 함께 AI의 미래를 함께 만들어요!\n\n<div class=\"content-ad\"></div>\n\n![MyUltimateMidjourneyGuide](/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_14.png)","ogImage":{"url":"/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_0.png"},"coverImage":"/assets/img/2024-06-19-MyUltimateMidjourneyGuideYouCannotMissPromptsandOver50Images_0.png","tag":["Tech"],"readingTime":12},{"title":"인공지능 생성 보컬의 유별난 윤리 문제","description":"","date":"2024-06-19 20:51","slug":"2024-06-19-TheWeirdEthicsofAI-GeneratedVocals","content":"\n\n<img src=\"/assets/img/2024-06-19-TheWeirdEthicsofAI-GeneratedVocals_0.png\" />\n\n요즘 증가하는 생성 AI와 함께 실제 가수들의 목소리로 훈련된 AI 생성 보컬이 등장했습니다. 이 AI 음성 \"크롬\"은 음악 산업과 인터넷 전반을 동요시켰으며, 다양한 윤리 문제들을 제기했습니다. 일부는 음악에서 복제된 목소리의 사용을 음악 작가와 청취자들에게 엄청난 혜택으로 보지만, 다른 사람들은 그것만으로는 위험요인만을 보았습니다. 여기에 몇 가지 재미있는 사례가 있습니다:\n\nAI로 생성된 드레이크와 더 위켄드 보컬이 특집된 노래가 스트리밍 서비스에 의해 제거되기 전에 바이럴로 퍼졌습니다. AI 음악을 비난하는 Universal Music Group 대표는 Music Business Worldwide에 발표된 성명에서, \"우리 아티스트들의 음악을 사용하여 생성 AI를 훈련한다는 것은 음악 생태계의 모든 이해관계자들이 어느 쪽 역사에 서길 원하는지 의문을 제기합니다: 아티스트, 팬 및 인간의 창의적 표현을 지지하는 쪽인가요, 아니면 딥 페이크, 사기 및 아티스트들에게 정당한 보상을 제공하지 않는 쪽인가요?\" 라고 말했습니다. (강조는 제가 한 것입니다.)\n\n아이스 큐브는 AI를 \"악마적\"이라고 지적하면서 음악에는 그 자리가 없다고 말했습니다.\n\n<div class=\"content-ad\"></div>\n\n스테파니 선은 2017년 이후 휴식을 취했던 싱가포르 가수인데, 자신의 컴백을 목도하게 되었지만, 사실상 그녀가 아닌 그녀의 AI 복제품이었습니다. 그녀의 AI가 생성한 보컬이 담긴 노래들이 이전 노래보다 더 인기를 끌자, 어떤 인간 아티스트도 AI와 경쟁할 수 없다고 말했습니다. \"어떻게 하류에 누군가와 경쟁하나요? 몇 분 단위로 새로운 앨범을 내놓는 사람과 어떻게 싸우나요?\"\n\n폴 맥카트니는 전 동료의 목소리를 부활시켰는데, AI의 도움으로 이제는 사망한 존 레논의 보컬을 사용하여 비틀즈의 마지막 레코드를 만들 수 있게 되었습니다. The Verge의 기사에 따르면, \"과거 녹음물을 복원하는 데 AI를 사용하는 가능성에 흥분하던 맥카트니는 존 레논의 목소리로 자신의 노래를 부르는 것을 듣는 것이 '좀 무서운' 것이라 말했습니다.\"\n\n가장 흥미로운 케이스 중 하나는 클레어 부처의 것인데, 많은 사람들이 그녀의 음악 프로젝트 이름인 그라임즈로 아는 사람들이 대부분입니다. 이론상 엘론 머스크의 자식엄마가 아닌 그녀는 '어버이없는 자'로서 그라임즈라는 '대인견문 인격체'로 음악을 작곡하고 녹음하며 제작하는 창조자입니다. 그녀의 아방가르드 음악은 장르에 도전하며 예술성과 엔지니어링을 융합시키고 있습니다(그녀는 여전히 기테제로 테마 앨범을 만드는 아티스트 중 한 명입니다). 그녀는 완전히 자학하며 종종 별난데 섬세하고 악순함이 없습니다. 그녀는 독특한 음악, 지식, 그리고 삶에 대한 견해로 자신의 이름을 알리게 되었습니다: 그녀는 시대에 따라 변하는 페미니스트로서 역사에 대한 심층적인 이해력이 있고, 지속가능한 에너지를 창조하고 인류를 다중행성종으로 만들며 의식을 보존하는 세 가지 목표를 가진 미래학자입니다.\n\n# 그라임즈의 AI 음악 혁명에서의 역할\n\n<div class=\"content-ad\"></div>\n\n클레어는 2003년 4월 23일에 트윗해 \"내 목소리를 사용한 성공적인 AI 생성 곡에 대해서는 50% 저작권료를 쪼개드릴게요. 협업하는 아티스트와의 계약과 동일한 조건이에요. 벌금 없이 내 목소리 자유롭게 사용해도 돼요. 레이블도 없고 법적 제약도 없어요.\"라며 음악에 AI 클론을 특징으로 한 미래를 공개적으로 받아들였어요.\n\n클레어의 제안은 더욱 현실로 다가왔습니다. 딱 일주일 후에는 Elf.Tech를 선보였는데, 이는 CreateSafe와의 파트너십을 통해 탄생한 AI 보컬 생성기로, CreateSafe는 다양한 도구를 활용하여 아티스트들을 지원하는 기업입니다. Elf.Tech 앱을 사용하기 위해 등록하는 사용자들은 미리 녹음된 노래를 업로드하거나 앱에 직접 보컬을 녹음할 수 있어요. 그리고 해당 보컬은 GrimesAI 음성 프린트로 변환됩니다. CreateSafe의 사이트에는 \"GrimesAI 녹음을 만드는 크리에이터들은 이 새로운 녹음물들을 소유하게 될 거예요. GrimesAI-1은 이 음원 녹음이나 기반이 되는 작품에 대한 소유권을 주장하지 않아요 (Grimes 노래의 커버인 경우 제외).\" 라고 나와 있어요.\n\n<div class=\"content-ad\"></div>\n\n맞아요, 클레어는 다른 창작자들이 그라임스의 보컬을 사용하길 원합니다. 그녀가 클론된 목소리를 사용하는 다른 창작자들의 시나리오를 법적이나 도덕적으로 부당하다고 생각하지 않는다는 것은 몇몇 다른 예술가들이 그렇게 생각하지 않는 것과는 다르다. 그녀는 심지어 자신의 목소리를 특징으로 한 노래를 만드는 과정을 단순화하는 필수 도구를 예술가들과 팬들에게 제공했습니다.\n\n# Elf.Tech를 통해 음악 민주화\n\nElf.Tech는 몇몇 사람들이 생각할 수 있는 클레어의 완전한 허영 프로젝트가 아닙니다. 그녀는 심지어 자신의 목소리나 가수로서의 능력을 좋아하지 않는다고 주장합니다. 클레어는 최근 이비사의 비트포트 국제 음악 정상회의에서 키노트 인터뷰를 하면서 말했습니다. \"아무도 나를 가수로 잘하는 걸로 생각하지 않아. 나는 그냥... 부디 나를 가수라고 부르지 말아줘. 사람들이 들어가서 내가 전문적으로 한다고 생각한다면... 나에게 좋은 모습이 아니야.\"\n\n이 프로젝트의 핵심은 AI가 세상에 미칠 긍정적인 영향과 AI 혁명의 중심에 서기를 원하는 그녀의 신념에 관한 것입니다.\n\n<div class=\"content-ad\"></div>\n\n지난 국제 음악 정상 회의에서 클레어는 Elf.Tech을 창립하기 전에 AI 기술을 활용하는 기업들에 전화를 걸어 그들의 발전 상황을 살펴볼 수 있는지 여쭸었던 경험에 대해 이야기했습니다. 이 경험이 그녀를 \"연구 심층 탐구\"에 이끄는데 큰 역할을 했고, 그 결과 음악적으로 하는 일을 \"재조정\"하게 되었을 것으로 보입니다. 이로써 Elf.Tech를 창립하게 된 것으로 예상됩니다.\n\nElf.Tech는 음악의 민주화를 목표로 하는 고귀한 야망의 결과물입니다. 클레어는 세계가 대체로 AI로 혜택을 받을 수 있다고 믿지만, 그녀 자신은 음악 산업에 초점을 맞추고 있습니다. 음악 산업에서 \"정말 많은 관문이 있다\"며 \"저작권이 최악이야\"라고 주장합니다. 그녀는 역사상 위대한 예술은 알려지지 않은 예술가들이 만든다고 말합니다. 고대 이집트의 큰 피라미드를 상상해보세요. 현재 예술이 자아와 너무 뒤얽혀있는 것은 아주 \"현대적인 개념\"이며, 클레어는 이를 버릴 필요가 있다고 생각하는 것으로 보입니다.\n\n회의에서 그녀가 말한 내용을 요약하면 다음과 같습니다: \"음악 산업은 매우 변호사들에 의해 규정되어 왔는데, 이것이 창의성을 제약하는 구속을 가하고 있다고 생각해요. 예술에서 가장 위대한 순간들은 저작권이 가장 없을 때, 예를 들어 랩의 초기 시기나 샘플링의 초창기처럼 말이에요.\"\n\n저작권 제약으로 인해 발생하는 문제뿐만 아니라 AI가 해결할 수 있는 가능성이 많을 뿐만 아니라, AI가 창조할 수 있는 팬과의 상호 작용 가능성도 무수히 존재합니다. 클레어는 지적 재산권을 사용하는 팬덤들을 처벌하지 않는 실제 기업들의 사례에 근거해 이런 주장을 합니다. 그러한 팬덤들이 많은 멋진 예술 작품을 만들어내느라는 점을 생각해보세요. 음악에서 AI의 광범위한 활용을 가능하게 함으로써, 클레어와 그녀의 팀이 \"공동창작물\"로 틀어놓은 놀라운 팬 아트가 탄생할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n물론 그녀는 복제된 AI 생성 보컬의 잠재적 위험을 인지하고 있습니다.\n\n콘센트에 대한 그녀의 생각을 물었을 때, 그녀는 예술가가 동의하기 전에 그들의 목소리가 사용될 때 좋다고 분명히 했습니다. 특히, 다른 창작자들에 의해 사용되는 예술가들은 예술적 통제를 포기할 것이기 때문에 이러한 동의가 중요하다고 말했습니다.\n\n죽은 가수들의 목소리 사용에 대한 그녀의 생각은 조금 덜 형성되어 있지만, 죽은 사람들은 목소리가 사용될 동의를 줄 수 없기 때문에 이 문제가 복잡하다는 점을 인정했습니다. 그녀는 “정말 어렵다”며 이야기했습니다. “아마도 아니라고 말할 것 같아” (즉, 죽은 가수들의 목소리는 사용되지 말아야 한다는 의미). “하지만 프린스 같은 사람은 좀 괜찮아 할 것 같아”라고 말했습니다. 그녀는 또한 예술가나 소비자들이 죽은 가수들의 목소리를 선호함으로써 새로운 가수가 음악계에 진입하기 어려워질 것을 우려한다고 말했습니다. 만약 모두가 마이클 잭슨의 목소리를 사용하기를 원한다면, 세상은 다음 마이클 잭슨을 어떻게 발견할 수 있을까요?\n\n음악 써밋에서 인터뷰어가 클레어에게 물었습니다. “만약 누군가가 당신의 목소리를 사용해 끔찍하고 싫어하는 무언가를 만들었다면 어떻게 하시겠어요?” 클레어는 “나는 그들이 그렇게 만들었으면 하는데”라고 대답했습니다. 인간들은 AI에 의해 가져다줄 위험에 대해 개방해야 하며 그 결과를 지켜봐야 한다고 말했습니다.\n\n<div class=\"content-ad\"></div>\n\n키노트 인터뷰에서 클레어가 AI로 생성된 보컬이 인간 보컬리스트를 앞으로 가려지게 할 수 있는 가능성에 대해 이야기하지는 않았지만, 클레어는 인간들이 AI의 한계를 집단적으로 이해하지 못하고 있다고 말했으며, 그녀는 AI가 우리의 모든 일에서 우리보다 우수해져서 우리를 이기게 될 것이라고 믿는다.\n\n# AI 음악 배포의 현재 법적 기술적 문제들\n\nElf.Tech의 출시 이후, YouTube와 스트리밍 사이트에서 여러 개의 GrimesAI 비디오가 확산되었습니다. 그러나 이 중 많은 비디오들이 흐릿한 법적 문제 때문에 스트리밍 사이트에서 삭제되었는데, Grimes 팀이 이 문제에 대해 노력하고 있다고 합니다.\n\n![이미지](/assets/img/2024-06-19-TheWeirdEthicsofAI-GeneratedVocals_3.png)\n\n<div class=\"content-ad\"></div>\n\n국제 음악 정상회의 중에 클레어는 스트리밍 서비스가 \"AI\"로 명확하게 표시된 AI 음악 하위 섹션이 생성될 것을 상상한다고 언급했습니다. 아마 그녀의 팀의 주요 목표 중 하나는 플랫폼에서 AI 음악을 위한 지정된 공간을 마련하는 것입니다. 국제 음악 정상회의에서 그녀가 한 말을 요약한 것이 여기 있습니다: \"우리 팀과 저는 정말 스포티파이에게 AI 섹션을 만들 것을 촉구하고 싶어합니다. 그 플랫폼에서 가수들은 최고의 트랙, 앨범 및 신작을 가지고 있죠; 스포티파이가 AI 섹션도 만든다면, 청취자들이 음악을 구분하고 'AI'로 표시된 음악이 아티스트의 창작물이 아님을 이해하기가 더 쉬울 것입니다. 그렇게 하면 품질 관리가 좀 더 쉬워질 거에요. 이 AI가 정말 새로운데 음악 산업이 이를 대비하도록 구성되어 있지 않아요, 그래서 필요한 변경을 해야 해요.\"\n\n저작권에 대한 클레어의 싫증에도 불구하고, 그녀와 그녀의 팀은 여전히 그녀의 비전을 실현하기 위해 음악 저작권의 복잡한 세계를 탐험해야 했습니다. 그들이 경험한 대부분의 문제는 배급과 관련이 있습니다: 주요 스트리밍 서비스 중 일부가 AI를 사용한 음악을 차단하고 있기 때문에, 창작자들이 GrimesAI 보컬을 사용하는 음악을 유통할 수 있게 어떻게 할 수 있을까요? 그 음악 산업이 그러한 음악에 대한 윤리가 격하게 논의되고 있는 상황에서, 아직 AI 지원 음악을 다루기에 적합하게 준비되어 있지 않기 때문에 어려움을 겪고 있습니다.\n\n그들은 마침내 어떤 진전을 이룬 것으로 보입니다.\n\n자립 발매 아티스트를 위한 음악 유통업체인 TuneCore가 CreateSafe와 Grimes와 협력하여 스트리밍 플랫폼에 AI 음악을 선보이는 데 기여했습니다. 그들의 주요 역할은 이러한 일에 관련된 모든 법적 및 기술적 사항들을 처리하는 것인 것 같습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![2024-06-19-TheWeirdEthicsofAI-GeneratedVocals_4](/assets/img/2024-06-19-TheWeirdEthicsofAI-GeneratedVocals_4.png)\n\nTunecore의 CEO에 따르면,\n\n이 협력의 결과는 이러한 공동 창작물이 라이선스 프로세스를 보다 쉽게 통과하도록 지원되어 저작권 관련 문제를 회피할 수 있다는 것입니다. GrimesAI를 이용하는 아티스트들은 음악을 제출하려면 Elf.Tech를 통해 배포 비용을 지불하거나 자체 라이선스 승인을 요청해야 합니다. 두 가지 방법 모두 승인이 필요하며, 노래가 충족해야 할 제약 사항과 특정 기준이 있습니다. 클레어는 아직 저작권을 무력화시키지는 않았습니다. 그녀는 오히려 그것과 함께 춤추는 법을 익혔습니다.\n\n2023년 6월 6일의 트윗에서, 클레어는 미래에 더 많은 변화가 있을 것이라 시사했으며, 이는 더 큰 스트리밍 서비스인 Spotify와 같은 곳도 곧 자신들의 플랫폼에서 AI 생성 보컬을 허용할지도 모른다는 것을 의미할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-TheWeirdEthicsofAI-GeneratedVocals_5.png\" />\n\n아직 큰 플랫폼에서 AI 음악을 허용할지 여부에 대한 업데이트가 제공되지 않았습니다.\n\n생성적 AI는 다양한 형태의 예술을 민주화하고, 예술 창작을 쉽고 매우 자동화할 수 있도록 합니다. 음악 산업에서는 이는 결국 새로운 또는 훈련이 덜 된 예술가들이 더 쉽게 진입할 수 있다는 것을 의미할 수 있으며, 창의적 과정의 일부를 쉽게 할 수도 있습니다. 그러나 그에는 악의적인 목적으로 다른 사람의 음성을 사용하는 사람들, 다른 사람들의 음성으로 수익을 얻는 사람들, 그리고 인간 예술가들이 그들의 AI 상대방이 더 인기가 많아지면 잊혀지는 등 부정적인 가능성이 딸려옵니다.\n\n시간은 이 기술을 사실로 어떻게 사용할지를 말해줄 것이지만, Grimes는 음악계의 실험대로 나서고 있습니다. 우연한 결과의 법칙이 승리할 것이라는 것을 그녀도 알지만, 그녀는 이를 실험하려고 준비가 되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-TheWeirdEthicsofAI-GeneratedVocals_6.png)\n\n이미 어려운 상황을 마주했어요. 그녀가 트윗에서 '그라임스의 모습'을 사용해 '나쁜 마음 바이러스'를 퍼뜨리는 콘텐츠를 제거해 달라고 요청할 것이라고 밝힌 것을 알 수 있었어요. 이는 일부 사람들이 그라임스 AI 보이스프린트를 악용했을 수도 있다는 신호예요.\n\n![이미지](/assets/img/2024-06-19-TheWeirdEthicsofAI-GeneratedVocals_7.png)\n\n그리고 떠오르는 잠재적 무명의 미래를 조금 맛 본 것으로 보입니다. 2023년 5월 8일 트윗에서 \"사실, 사람들이 나보다 경쟁적으로(아니면 더 나은가요??) 퀄리티 좋게 그라임스 같은 노래를 만들기 시작한 것이 약간 스트레스 받게 만드네요. 그러나 또한 이게 또 다른 커리어에서 죽고 다시 부활할 가장 아름다운 시적인 방법일지도 몰라요.\"라고 썼어요.\n\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-TheWeirdEthicsofAI-GeneratedVocals_8.png)\n\n물론, 클레어 부셰의 경우에는 AI 생성 보컬을 둘러싼 많은 사례 중 하나일 뿐이며, AI 생성 또는 지원 형태를 사용하는 음악과 관련된 사례는 더욱 많습니다. 그녀는 창의적인 통제를 희생하고 자신의 목소리를 누구나의 악기로 사용할 수 있도록 제공함으로써 AI 음악 혁명의 중심에 위치하게 되었습니다.\n\n그녀의 음악 산업에서의 행동과 다른 이들이 그녀의 목표에 대해 얼마나 수용하는지는 예술 전반에 영향을 미칠 것입니다.\n\n작가들도 AI 생성 보컬을 둘러싼 논쟁의 부작용을 볼 것입니다. 모든 이러한 생성적 AI에 관한 논쟁은 교차되기 때문입니다. 예를 들어, 사망한 작가들이 AI \"쓰기 클론\"을 통해 그들이 완결하지 못한 시리즈를 마무리해야 하는가? 오디오북을 만드는 작가들은 어떻게 AI 생성 보컬에 접근해야 하는가 - 그러한 윤리적 고민을 어떻게 해결해야 하는가? 작가들은 대형 언어 모델이나 유사한 AI들에 의해 자신의 저자 \"목소리\"가 복제된 것에 대해 어떻게 생각하는가?\n\n<div class=\"content-ad\"></div>\n\n우리는 낯선, 탁한 물속에 있어요. 그리고 깊이에 기다리는 것은 알 수 없지만, 가능성은 두려울 만큼 두렵고 흥미롭습니다.\n\n(*저스틴 콕스의 Medium 기사 덕분에 Stefanie Sun 사건에 대해 알게 되었어요!)","ogImage":{"url":"/assets/img/2024-06-19-TheWeirdEthicsofAI-GeneratedVocals_0.png"},"coverImage":"/assets/img/2024-06-19-TheWeirdEthicsofAI-GeneratedVocals_0.png","tag":["Tech"],"readingTime":9},{"title":"Midjourney로 추상적인 것을 탐험하기","description":"","date":"2024-06-19 20:50","slug":"2024-06-19-GoingabstractwithMidjourney","content":"\n\nMidjourney의 새로운 개인화 기능을 더 철저히 테스트하고, 그 과정에서 조금은 추상 미술로 놀았어요.\n\n이것이 이상한 선택처럼 들릴 수 있겠죠. 추상 미술은 정의하기 어려운 미묘한 개념인데, 이미 어렵게 평가해야 할 성능 기능을 평가할 때 왜 사용해야 하죠?\n\n내 추론은 이렇습니다. 개인화 기능은 내가 사진을 평가할 때의 선호도를 기반으로 합니다. 평가 프로세스는 상당히 주관적인 일이에요; 가끔은 기술적 요소를 기준으로 사진을 선택하기도 해요 — 예를 들어, 적당한 손가락과 다리의 양. 하지만 대부분은 미학적 감각을 기반으로 — 적절한 구성, 색상, 그리고 기술이 콘텐츠와 일치하는 것을 따라 사진을 선택해 왔어요.\n\n그렇기에 추상 미술을 좋아하거나 싫어하는 것도 비슷해요. 추상 미술은 현실적인 객체를 표현하는 것이 아니에요; 그것은 색채, 형상, 그리고 특정 기술로 감정, 분위기, 또는 아이디어를 전달하는 것이기 때문이에요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-GoingabstractwithMidjourney_0.png\" />\n\n그래서, 만약 개인 맞춤 설정이 내가 평가한 다양한 이미지에서 선호도를 선택한다면, 이 기능을 사용하여 만들어지는 추상 예술 작품에 반영되어야 한다고 판단했습니다. 나는 내 개인 맞춤 설정으로 만들어진 이미지에 더 잘 공감할 수 있어야 합니다.\n\n추상 예술은 추상적인 생각으로부터 시작됩니다. 복잡한 감정이나 아이디어를 말로 표현하는 것이 얼마나 어려운지 놀라울 정도이며, 더 어렵게는 붓으로 그것을 하는 것입니다.\n\n그래서 대부분의 Midjourney 사용자들은 원하는 예술적 효과와 감정 요소를 달성하기 위해 아이디어를 설명하는 대신 기술이나 스타일을 정의하는 것을 고수합니다.\n\n<div class=\"content-ad\"></div>\n\n이 기사는 추상적 사고나 기계에 추상적 개념을 전달하는 데 관한 것이 아니므로, 우리도 이 간소화된 방법을 사용할 겁니다.\n\n하지만 그래도 추상 미술은 매우 폭넓은 개념이며 다양한 형태로 나타날 수 있기 때문에, 하얀 테스트를 적어도 어느 정도 측정 가능하도록 하기 위해 일부 선택이 필요합니다.\n\n사진 작가로서 나도 추상 이미지를 만드는 것을 좋아했어요. 하지만 \"추상\"이라는 사진 작품 개념은 Pollock의 추상 미술과는 매우 다릅니다.\n\n![이미지](/assets/img/2024-06-19-GoingabstractwithMidjourney_1.png)\n\n<div class=\"content-ad\"></div>\n\n추상적인 그래픽은 추상 회화와 같은 것이 아닙니다. 그리고 추상 조각 예술 및 설치 예술은 서로에게 더 멀리 떨어진 개념입니다. 그러나 모두가 '추상적'일 수 있습니다.\n\n그리고 넓은 개념이기 때문에 재미있게 놀 수 있으며, Midjourney는 당신만의 추상적 추상 예술 운동을 만들 수 있는 도구가 될 수도 있습니다.\n\n난 항상 짧은 프롬프트를 선호해. 이번 테스트에서도 짧은 프롬프트를 사용할 거야. 인지 테스트를 거치지 않고 Midjourney의 성능을 가장 쉽게 평가할 수 있어.\n\n그래서 여기 한 프롬프트의 예제가 있어:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-GoingabstractwithMidjourney_2.png\" />\n\n이 프롬프트는 추상 예술 양식에 특정한 용어가 없으며, 추상 이미지로 만들라는 간단한 지시이지만, 귀하의 프롬프트에 포함할 가능성이 있는 용어와 스타일을 짧게 모았습니다.\n\n- 비목적적: 인식 가능한 대상을 나타내거나 묘사하지 않는 예술.\n- 기하학적 추상: 기하학적 모양과 형태를 주요 요소로 사용하는 예술.\n- 유기적 추상: 유기적이고 자연적인 형태와 모양을 강조하는 예술로, 종종 생명체를 닮았습니다.\n- 추상 표현주의: spontaneity, expressiveness, and often large-scale works that emphasize the act of painting itself을 특징으로 하는 운동.\n- 미니멀리즘: 단순함에 초점을 맞춘 예술 운동으로, 최소한의 요소와 종종 단색 계통을 사용합니다.\n- 데 스테일: 순전한 추상과 단순함을 주장하는 예술 운동으로, 직선, 기하학적 모양, 주요 색상을 사용합니다.\n- 최정주의: 재스 키 클러 국 한, 기본적인 기하학적 형태와 제한된 색상을 중점으로 하는 추상 예술 운동.\n- 액션 페인팅: 캔버스에 페인트를 무심코 묻거나 튀기거나 문지르는 스타일의 페인팅으로, 그림 그리는 행위를 강조합니다.\n- 색면 회화: 단일 색상의 큰 영역이나 몇 가지 색상으로 구성된 단순한 구성을 특징으로 하는 추상 회화 스타일.\n- 리릭 약술: 개인적 표현, 감정 및 색상과 형태의 서정적인 사용을 강조하는 운동.\n- 하드 에지 페인팅: 날카롭고 깨끗한 가장자리와 기하학적 모양을 사용하는 추상 회화 스타일로, 종종 단색을 사용합니다.\n- 건축주의: 러시아에서 유래한 예술 운동으로, 산업 재료와 건축적인 느낌과 연결된 추상적 기하학적 형태에 초점을 맞춥니다.\n- 입체파: 파블로 피카소와 조르주 브락에 이끌린 초기 20세기 운동으로, 단일 작품 내에서 파편화된 물체와 다양한 시점을 특징으로 합니다.\n- 타시즘: 브러시 워크, 물감의 뭉치 및 뚜렷한 특징으로 Abstract Expressionism과 유사한 유럽 추상 예술 운동.\n- 싱크로미즘: 색상을 사용하여 형태와 구조를 만드는 것을 강조하는 추상 예술 운동으로, 종종 음악적 또는 조화로운 방식으로 구성합니다.\n\n이 용어들은 각각 독특한 특성과 Midjourney 이미지 생성에 미치는 영향을 갖는 추상 예술 내의 다양한 개념과 운동을 다룹니다.\n\n<div class=\"content-ad\"></div>\n\n물론, 이것은 짧은 목록입니다. 하지만 \"추상\" 기술과 아이디어의 다양한 스타일, 움직임, 명확한 개념을 더 찾는 것은 어려운 일이 아닙니다.\n\nMidjourney의 성과에서 개인화의 차이를 찾고 있어서 각 일련의 프롬프트를 두 번 실행했습니다. 아래에는 여러분이 스스로 판단할 수 있도록 작은 이미지 라이브러리를 정리해 두었습니다.\n\n이미지 아래에 프롬프트가 제공됩니다. 이미지의 왼쪽에는 개인화 없이 생성된 이미지 그리드가 있고, 오른쪽에는 있는 것입니다.\n\n대부분의 경우, 개인화는 더 나은 이미지를 만들어 냈거나, 적어도 내게는 더 마음에 든 것 같아요.\n\n<div class=\"content-ad\"></div>\n\n중요한 관찰 하나 있어요. 개인화를 사용할 때는 프롬프트 문구에 따르는 정확도가 낮아집니다. Midjourney는 프롬프트에서 키워드를 건너뛸 가능성이 높아질 거에요. 또한 이미지 격자의 변화가 뚜렷하게 감소할 거예요.\n\n저는 개인화의 효과를 선호하는 편이에요. 이미지가 보다 명암이 선명해지고, 제 시각에는 색 균형이 더 좋아 보입니다.\n\n또한, 제 생각에는 개인화가 프롬프트에서 제공된 감정 콘텐츠를 더 잘 전달해 준다고 생각해요. 그러니 그 점은 플러스 되죠.\n\n![이미지](/assets/img/2024-06-19-GoingabstractwithMidjourney_3.png)\n\n<div class=\"content-ad\"></div>\n\n지금까지의 결론은 이 기능이 훌륭하며 이미지에 긍정적인 영향을 미친다는 것입니다. 복잡한 프롬프트와 함께 사용할 때 조심할 것이며, 이미지 그리드의 변이가 줄어드는 것을 보상하기 위해 \"혼돈\"의 작은 설정을 발견했습니다. \n\n추상적인 이미지를 만드는 것이 정말 재미있었고, 여러분도 즐기시길 바랍니다.\n\nAivaras Grauzinis","ogImage":{"url":"/assets/img/2024-06-19-GoingabstractwithMidjourney_0.png"},"coverImage":"/assets/img/2024-06-19-GoingabstractwithMidjourney_0.png","tag":["Tech"],"readingTime":4},{"title":"NLP에서 Named Entity Recognition 텍스트로부터 정보 추출하기 제7부","description":"","date":"2024-06-19 20:47","slug":"2024-06-19-NamedEntityRecognitioninNLPExtractingInformationfromTextPart7","content":"\n\n<img src=\"/assets/img/2024-06-19-NamedEntityRecognitioninNLPExtractingInformationfromTextPart7_0.png\" />\n\n목차\n1. 명명된 엔티티 인식(Unveiling Named Entity Recognition): 구조화된 데이터로 가는 문\n2. NLP에서 정보 추출의 작동 방식\n2.1. 명명된 엔티티 인식에서의 핵심 개념\n2.2. 고급 기술과 알고리즘\n3. 다양한 산업에서 NER의 실용적인 응용\n4. NER의 구성 요소: 도구 및 프레임워크\n5. NER 시스템 평가: 지표 및 벤치마크\n6. 명명된 엔티티 인식에서의 도전 극복\n7. NER의 미래: 트렌드와 예측\n\n더 자세한 자습서는 GPTutorPro에서 확인하세요. (무료)\n\n42페이지의 데이터 과학 | 종합 핸드북을 무료로 받아보세요. (구독)\n\n<div class=\"content-ad\"></div>\n\n## 1. Named Entity Recognition (NER) 소개: 구조화된 데이터로 가는 길\r\n\r\n명명된 개체 인식(NER)은 자연어 처리(NLP)에서 텍스트를 이해하는 데 중요합니다. NER은 이름, 조직, 장소 등의 중요한 요소를 식별함으로써 비구조화된 데이터를 구조화된 형식으로 변환합니다. 이 과정을 통해 기계는 문서 내 내용을 이해하고 분류할 수 있습니다.\r\n\r\nNER의 핵심 개념을 살펴보겠습니다:\r\n\r\n- 텍스트 내 개체의 식별\r\n- 미리 정의된 카테고리로의 분류\r\n- 모호성 해소를 위한 문맥 분석\n\n<div class=\"content-ad\"></div>\n\nspaCy 라이브러리를 사용한 간단한 Python 예제를 보여드릴게요:\n\n```js\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ntext = \"Google was founded by Larry Page and Sergey Brin.\"\ndoc = nlp(text)\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n```\n\n개체명 인식 시스템은 정밀도, 재현율 및 F1 스코어를 기반으로 평가됩니다. 이러한 지표는 NER 시스템의 정확성과 효율성을 결정하는 데 도움을 줍니다.\n\nNER을 숙달함으로써 방대한 양의 텍스트에서 가치 있는 정보를 추출하여 데이터 분석 및 지식 발견에 강력한 도구로 활용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 2. NLP에서 정보 추출의 메커니즘\n\nNLP에서의 정보 추출은 텍스트로부터 데이터를 식별하고 구조화하는 중요한 프로세스입니다. 이는 Named Entity Recognition (NER)의 기반입니다. 동작 방식은 다음과 같습니다:\n\n먼저, 텍스트는 단어나 구문 등 작은 단위로 토큰화됩니다. 이는 이어지는 단계에 있어서 필수적입니다. 그런 다음, 품사 태깅을 통해 각 토큰에 문법적 범주를 할당하여 역할을 이해하는 데 도움을 줍니다.\n\n이어서 NER 시스템은 엔티티를 인식하기 위해 알고리즘을 적용합니다. 통계적 방법이나 기계 학습 모델을 사용할 수 있습니다. 예를 들어:\n\n<div class=\"content-ad\"></div>\n\n```python\nimport nltk\nfrom nltk import ne_chunk, pos_tag, word_tokenize\nfrom nltk.tree import Tree\n\ndef get_entities(text):\n    # 텍스트 토큰화하기\n    tokens = word_tokenize(text)\n    # POS 태깅 적용\n    tags = pos_tag(tokens)\n    # 태깅된 토큰 청크\n    tree = ne_chunk(tags)\n    return find_named_entities(tree)\n\ndef find_named_entities(tree):\n    entities = []\n    for subtree in tree:\n        if type(subtree) == Tree:\n            entity = \" \".join([token for token, pos in subtree.leaves()])\n            entities.append(entity)\n    return entities\n\ntext = \"Apple Inc. is looking at buying U.K. startup for $1 billion\"\nentities = get_entities(text)\nprint(entities)\n```\n\n마지막으로, 추출된 엔티티들은 사람, 조직 또는 위치와 같은 사전 정의된 그룹으로 분류됩니다. 이러한 구조화된 데이터는 다양한 응용 프로그램에 사용될 수 있어 원시 텍스트 데이터의 가치를 향상시킵니다.\n\n정보 추출 메커니즘을 이해함으로써, NER을 효율적으로 다양한 양의 텍스트 정보를 조직화하고 해석할 수 있습니다.\n\n## 2.1. Named Entity Recognition의 핵심 개념\n\n\n<div class=\"content-ad\"></div>\n\nNamed Entity Recognition (NER)은 정보 추출의 하위 작업으로, 명명된 개체를 미리 정의된 범주로 분류합니다. 사람, 조직, 위치, 시간 표현, 수량, 통화 가치, 백분율 등이 여기에 해당됩니다.\n\nNER의 핵심은 텍스트에서 개체를 찾아 분류하는 것입니다. 이 과정은 구조화되지 않은 데이터를 구조화된 정보로 변환하여 추가 분석을 준비합니다. NLP 파이프라인에서 중요한 단계입니다.\n\nNER은 여러 중요한 단계를 포함합니다:\n\n- 토큰화: 텍스트를 단어, 구, 기호 또는 다른 의미 있는 요소인 토큰으로 분할합니다.\n- 품사 태깅: 각 토큰에 명사, 동사, 형용사 등과 같은 품사를 할당합니다.\n- 청킹: 토큰을 품사 태그를 기반으로 청크로 묶습니다.\n- 개체 인식: 청크를 명명된 개체로 식별합니다.\n- 개체 분류: 명명된 개체를 미리 정의된 그룹으로 분류합니다.\n\n<div class=\"content-ad\"></div>\n\nNLTK 라이브러리를 사용한 간단한 Python 예제를 준비해봤어요:\n\n```python\nimport nltk\nfrom nltk import word_tokenize, pos_tag, ne_chunk\n\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\n\ntext = \"Apple Inc. is looking at buying U.K. startup for $1 billion\"\ntokens = word_tokenize(text)\ntags = pos_tag(tokens)\ntree = ne_chunk(tags)\n\nprint(tree)\n```\n\n이 코드는 텍스트의 명명된 엔터티인 'Apple Inc.'를 조직으로, 'U.K.'를 위치로 인식하게 됩니다.\n\n이러한 개념을 이해하는 것은 챗봇부터 콘텐츠 분석까지 다양한 애플리케이션에서 명명 개체 인식(NER)을 효과적으로 활용하기 위한 필수요소입니다.\n\n<div class=\"content-ad\"></div>\n\n## 2.2. 고급 기술과 알고리즘\n\n고급 기술과 알고리즘은 명명된 개체 인식(NER)의 경계를 넓히는 역할을 합니다. 이러한 정교한 방법들을 자세히 알아봅시다.\n\n머신 러닝 모델은 NER을 향상시키기 위해 발전해 왔습니다. 특히 딥러닝은 중대한 역할을 하였습니다. 이는 데이터의 복잡한 패턴을 이해하기 위해 신경망을 활용합니다.\n\n하나의 모델은 Long Short-Term Memory (LSTM) 네트워크입니다. 이는 텍스트 내의 문장과 같은 시퀀스를 처리하는 데 뛰어납니다. 아래는 간단화된 Python 예시입니다:\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\n\nmodel = Sequential()\nmodel.add(LSTM(50, return_sequences=True, input_shape=(...)))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(...)\n```\n\n다른 고급 기술로는 전이 학습(Transfer Learning)이 있습니다. 미리 훈련된 모델을 활용하여 시간과 자원을 절약할 수 있습니다. BERT와 GPT와 같은 모델이 인기가 있습니다.\n\n마지막으로, 어텐션 메커니즘(Attention Mechanisms)은 모델 성능을 향상시켰습니다. 이를 통해 모델이 텍스트의 관련 부분에 집중할 수 있습니다. 이는 정확한 Entity Recognition에 중요합니다.\n\n이러한 발전으로 NER이 더욱 효율적이고 정확해졌습니다. 이것들은 비구조적인 방대한 양의 텍스트로부터 가치 있는 통찰을 추출하는 데 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n## 3. 다양한 산업 분야에서 NER의 실용적 응용\n\nNamed Entity Recognition (NER)은 구조화되지 않은 텍스트를 구조화된 데이터로 변환하는 데 중요합니다. 이 프로세스는 의미 있는 정보를 추출하는 데 다양한 산업에서 필수적입니다. 다음은 일부 실용적인 응용 분야입니다:\n\n- **의료 분야**: NER은 임상 노트에서 환자 정보를 추출하는 데 도움을 줍니다. 의학 용어, 약물명 및 용량을 식별하여 환자 치료와 연구에 도움이 됩니다.\n\n- **금융 분야**: 금융 부문은 NER을 사용하여 경제 보고서를 모니터링합니다. 회사명, 주식 심볼 및 재무 지표를 추출하여 시장 분석에 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n미디어와 저널리즘 분야에서 NER은 뉴스 기사에서 사람, 조직 및 위치와 같은 엔티티를 추적합니다. 이는 콘텐츠 분류와 트렌드 분석을 지원합니다.\n\n소매 회사들은 NER을 고객 피드백에 적용합니다. 제품 이름 및 속성을 식별하여 재고 관리와 마케팅 전략에 도움이 됩니다.\n\n법률 전문가들에게 NER은 법적 문서에서 관련 엔티티를 추출합니다. 관련 당사자, 법적 용어 및 사건 세부 정보를 식별하여 사건 분석을 간소화합니다.\n\n정보 추출에서 NER의 역할은 산업 전반에 걸쳐 꼭 필요합니다. 이는 기업이 데이터 기반 의사결정을 내리고 경쟁력 있는 통찰력을 얻는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 NER을 사용하는 간단한 파이썬 예제입니다:\n\n```js\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ntext = \"Apple is looking at buying U.K. startup for $1 billion\"\ndoc = nlp(text)\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n```\n\n이 코드 스니펫은 spaCy 라이브러리를 사용하여 주어진 텍스트에서 엔티티를 식별합니다. 이것은 NLP에서 NER의 강력함을 간략하게 보여줍니다.\n\n## 4. NER의 구성 요소: 도구와 프레임워크\n\n<div class=\"content-ad\"></div>\n\nNamed Entity Recognition (NER)은 비구조화된 텍스트에서 정보를 추출하는 데 중요합니다. 이는 이름, 위치, 조직과 같은 엔티티를 식별합니다. NER을 구현하기 위해 다양한 도구와 프레임워크가 있습니다.\n\n인기 있는 Python 라이브러리 중 하나는 spaCy입니다. 이는 NER을 위한 사전 훈련된 모델을 제공하며 대규모 정보 추출에 효율적입니다. 아래는 spaCy를 사용한 기본적인 코드 조각입니다:\n\n```python\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ntext = \"Google was founded by Larry Page and Sergey Brin.\"\ndoc = nlp(text)\nentities = [(ent.text, ent.label_) for ent in doc.ents]\nprint(entities)\n```\n\n이 코드는 사전 훈련된 영어 모델을 로드하고 주어진 텍스트를 처리하여 인식된 엔티티를 출력합니다.\n\n<div class=\"content-ad\"></div>\n\n다른 프레임워크로는 교육 목적과 프로토타이핑에 적합한 NLTK가 있습니다. NER에 대한 간단한 접근 방법을 제공하지만 최적의 성능을 얻으려면 수동으로 세밀한 조정이 필요합니다.\n\n더 고급화된 사용을 위해 BERT와 GPT와 같은 트랜스포머 기반 모델을 사용할 수 있습니다. 이러한 모델은 NER 작업에서 최첨단의 정확도를 제공합니다.\n\n적절한 도구를 선택하는 것은 프로젝트의 요구 사항에 따라 다릅니다. 언어 지원, 정확성 및 컴퓨팅 리소스 등을 고려해야 합니다.\n\n## 5. NER 시스템 평가: 메트릭 및 기준\n\n<div class=\"content-ad\"></div>\n\n개별 이름 인식(NER) 시스템을 평가하는 것은 그 효과를 이해하는 데 중요합니다. 다음은 그들을 평가하는 방법입니다:\n\n정밀도는 NER 시스템에서 정확하게 식별된 개체의 백분율을 측정합니다. 높은 정밀도는 거짓 양성이 적다는 것을 의미합니다.\n\n```js\n# 정밀도 계산\ntrue_positives = 100\nfalse_positives = 10\nprecision = true_positives / (true_positives + false_positives)\nprint(f\"정밀도: {precision:.2f}\")\n```\n\n회수율은 실제 개체 중 올바르게 식별된 개체의 백분율을 나타냅니다. 모든 관련 개체를 찾는 시스템의 능력을 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 재현율 계산\ntrue_positives = 100\nfalse_negatives = 20\nrecall = true_positives / (true_positives + false_negatives)\nprint(f\"재현율: {recall:.2f}\")\n```\n\nF1-Score는 정밀도와 재현율을 하나의 지표로 결합한 값입니다. 이는 정밀도와 재현율의 조화 평균값입니다.\n\n```js\n# F1-Score 계산\nprecision = 0.83\nrecall = 0.77\nf1_score = 2 * (precision * recall) / (precision + recall)\nprint(f\"F1-Score: {f1_score:.2f}\")\n```\n\n이러한 지표들은 서로 다른 NER 시스템을 비교하고 정보 추출의 개선을 추적하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n## 6. 개체명 인식(NER)에서의 도전 극복하기\n\n개체명 인식(NER)은 자연어 처리(NLP)의 정보 추출에서 중요한 구성 요소입니다. 그러나 NER은 몇 가지 도전에 직면합니다. 여기에 이를 극복할 수 있는 방법이 있습니다:\n\n1. 텍스트의 모호성: 맥락적 단서가 중요합니다. 주변 텍스트를 고려하는 알고리즘을 사용하여 올바른 개체를 결정합니다.\n\n```js\n# 예시: spaCy를 활용한 맥락적 NER\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n```\n\n<div class=\"content-ad\"></div>\n\n2. Entity Variations: Entity는 여러 형태를 가질 수 있습니다. 모든 변형을 인식하는 시스템을 구현하는 것이 중요합니다.\n\n3. 데이터 희소성: 모든 엔티티가 흔하지는 않습니다. 희귀 엔티티를 인식하기 위해 가짜 예제와 함께 데이터셋을 보강하세요.\n\n4. Cross-domain 적응력: 한 도메인에서 훈련된 모델이 다른 도메인에서 잘 동작하지 않을 수 있습니다. 새로운 도메인에 모델을 적응시키기 위해 전이 학습을 사용하세요.\n\n이러한 문제에 대처하여 Named Entity Recognition 시스템이 더 견고하고 정확해질 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 7. NER의 미래: 트렌드와 예측\n\nNamed Entity Recognition (NER) 분야는 빠르게 발전하고 있으며, 앞으로의 새로운 발전이 예상됩니다. 아래는 예상되는 트렌드입니다:\n\n딥 러닝 통합: NER 시스템은 점점 더 딥 러닝 모델을 활용하며, 엔티티 감지의 정확도를 향상시킬 것입니다.\n\n```python\n# NER용 딥 러닝 모델 예시\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\nmodel = AutoModelForTokenClassification.from_pretrained('bert-base-ner')\ntokenizer = AutoTokenizer.from_pretrained('bert-base-ner')\n```\n\n<div class=\"content-ad\"></div>\n\n새로운 언어로의 확장: NER 기술은 더 많은 언어를 지원하기 위해 확대되어, 전 세계적으로 더 접근성이 좋아질 것입니다.\n\n실시간 처리: 미래의 NER 시스템은 정보를 실시간으로 처리하여 다양한 소스에서 즉각적인 데이터 추출이 가능해질 것입니다.\n\n향상된 맥락 이해: 맥락 분석의 발전으로 복잡한 텍스트에서도 더 세밀한 개체 인식이 가능해질 것입니다.\n\n이러한 추세들은 NLP에서 정보 추출에 밝은 미래를 암시하며, 더 정교하고 다재다능한 NER 시스템을 약속하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다음은 완전한 튜토리얼 목록입니다:\n\n무료 튜토리얼 및 정신 건강 스타트업 지원.\n\n파이썬, 머신러닝, 딥러닝 및 LLMs 마스터: E-북 50% 할인 (쿠폰: RP5JT1RL08)","ogImage":{"url":"/assets/img/2024-06-19-NamedEntityRecognitioninNLPExtractingInformationfromTextPart7_0.png"},"coverImage":"/assets/img/2024-06-19-NamedEntityRecognitioninNLPExtractingInformationfromTextPart7_0.png","tag":["Tech"],"readingTime":9}],"page":"62","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}