{"pageProps":{"posts":[{"title":"텍스트 임베딩 종합 가이드 2024 최신","description":"","date":"2024-06-23 19:53","slug":"2024-06-23-TextEmbeddingsComprehensiveGuide","content":"\n\n## 텍스트 임베딩의 진화, 시각화, 그리고 응용\n\n우리 인간은 텍스트를 읽고 이해할 수 있습니다 (적어도 일부분은요). 컴퓨터는 반대로 \"숫자로 생각\"하기 때문에 단어와 문장의 의미를 자동으로 파악할 수 없습니다. 만약 우리가 컴퓨터가 자연어를 이해하도록 하려면, 이 정보를 컴퓨터가 작업할 수 있는 형식인 숫자 벡터로 변환해야 합니다.\n\n수십 년 전에 사람들은 텍스트를 기계가 이해할 수 있는 형식으로 변환하는 방법을 배웠습니다 (그 중 하나는 ASCII였습니다). 이러한 방식은 텍스트를 렌더링하고 전송하는 데 도움이 되지만 단어의 의미를 부호화하지는 않습니다. 당시에는 키워드 검색 기술이 표준 검색 기술이었으며 특정 단어나 N-gram을 포함하는 모든 문서를 찾는 방식이었습니다.\n\n그 후 몇 10년이 지난 후, 임베딩이 등장했습니다. 우리는 단어, 문장, 심지어 이미지에 대한 임베딩을 계산할 수 있습니다. 임베딩도 숫자의 벡터입니다만, 의미를 포착할 수 있습니다. 그래서 의미 검색을 수행하거나 다양한 언어로 된 문서를 다루는 데 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 글에서는 임베딩 주제를 깊이 있게 다루어보고자 합니다:\n\n- 임베딩이 만들어지기 전의 역사와 진화에 대해,\n- OpenAI 도구를 사용하여 임베딩을 계산하는 방법,\n- 문장이 서로 가까운지 판단하는 방법,\n- 임베딩을 시각화하는 방법,\n- 가장 흥미로운 부분은 임베딩을 실제로 활용하는 방법입니다.\n\n이어서 나아가서 임베딩의 진화에 대해 배워보겠습니다.\n\n# 임베딩의 진화\n\n<div class=\"content-ad\"></div>\n\n우리는 텍스트 표현의 역사로 간단한 여행을 시작할 것입니다.\n\n## 단어 가방\n\n텍스트를 벡터로 변환하는 가장 기본적인 방법은 단어 가방입니다. 리처드 P. 페이만의 유명한 명언 중 하나를 살펴보겠습니다. \"우리는 아직 발견들을 만들어내는 시대에 살고 있다\". 이를 통해 단어 가방 접근법을 설명해보겠습니다.\n\n단어 가방 벡터를 얻는 첫 번째 단계는 텍스트를 단어(토큰)로 나눈 다음, 단어를 기본 형태로 줄이는 것입니다. 예를 들어, \"running\"은 \"run\"으로 변환됩니다. 이 과정을 어간 추출(stemming)이라고 합니다. NLTK Python 패키지를 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\n\ntext = 'We are lucky to live in an age in which we are still making discoveries'\n\n# 토큰화 - 텍스트를 단어로 나누기\nwords = word_tokenize(text)\nprint(words)\n# ['We', 'are', 'lucky', 'to', 'live', 'in', 'an', 'age', 'in', 'which',\n#  'we', 'are', 'still', 'making', 'discoveries']\n\nstemmer = SnowballStemmer(language=\"english\")\nstemmed_words = list(map(lambda x: stemmer.stem(x), words))\nprint(stemmed_words)\n# ['we', 'are', 'lucki', 'to', 'live', 'in', 'an', 'age', 'in', 'which',\n#  'we', 'are', 'still', 'make', 'discoveri']\r\n```\n\n자, 이제 우리 단어들의 기본 형태 리스트가 있습니다. 다음 단계는 이들 빈도를 계산하여 벡터를 만드는 것입니다.\n\n```js\r\nimport collections\nbag_of_words = collections.Counter(stemmed_words)\nprint(bag_of_words)\n# {'we': 2, 'are': 2, 'in': 2, 'lucki': 1, 'to': 1, 'live': 1, \n# 'an': 1, 'age': 1, 'which': 1, 'still': 1, 'make': 1, 'discoveri': 1}\r\n```\n\n사실, 만약 텍스트를 벡터로 변환하고 싶다면, 텍스트에 있는 단어뿐만 아니라 전체 어휘를 고려해야 합니다. \"i\", \"you\", \"study\"도 어휘에 있다고 가정하고, 파인만의 명언에서 벡터를 만들어 봅시다.\r\n\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_0.png)\n\n이 방법은 꽤 기본적이며 단어의 의미를 고려하지 않기 때문에 \"그 여자는 데이터 과학을 공부하고 있다\"와 \"젊은 여성이 AI와 ML을 배우고 있다.\"라는 문장이 서로 가까운 위치에 있지 않을 수 있습니다.\n\n## TF-IDF\n\n단어 가방 접근법의 약간 개선된 버전인 TF-IDF(Term Frequency — Inverse Document Frequency)입니다. 이것은 두 가지 지표의 곱셈입니다.\n\n<div class=\"content-ad\"></div>\n\n![Markdown Table](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_1.png)\n\n- 용어 빈도는 문서에서 단어의 빈도를 보여줍니다. 이를 계산하는 가장 흔한 방법은 이 문서에서 용어의 로우 카운트(단어 가방에 있는 것처럼)을 전체 용어(단어) 수로 나누는 것입니다. 그러나 로우 카운트, 부욜리언 \"빈도\", 정규화에 대한 다양한 접근 방법이 많이 있습니다. 위키피디아에서 다양한 접근 방법에 대해 더 배울 수 있습니다.\n\n![Markdown Table](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_2.png)\n\n- 역문서 주파수는 단어가 얼마나 많은 정보를 제공하는지를 나타냅니다. 예를 들어, \"a\"나 \"that\" 같은 단어는 문서 주제에 대해 추가 정보를 제공하지 않습니다. 대조적으로, \"ChatGPT\"나 \"생물정보학\" 같은 단어는 도메인을 정의하는 데 도움이 될 수 있습니다 (하지만 이 문장에는 해당하지 않음). 이는 전체 문서 수와 해당 단어를 포함하는 문서 수의 비율의 로그함수로 계산됩니다. IDF가 0에 가까울수록 단어가 흔하고 제공하는 정보가 더 적습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_3.png\" />\n\n그래서 결과적으로 우리는 일반적인 단어 (\"I\"나 \"you\"와 같은)이 낮은 가중치를 갖는 벡터를 얻게됩니다. 한편, 문서에서 여러 번 발생하는 드문 단어들은 더 높은 가중치를 갖게 됩니다. 이 전략은 약간 더 나은 결과를 제공하지만 여전히 의미적 의미를 잡아내기는 어렵습니다.\n\n이 방법론의 다른 어려움은 상당히 희소한 벡터를 생성한다는 점입니다. 벡터의 길이는 말뭉치 크기와 동일합니다. 영어에는 약 470,000개의 고유 단어가 있습니다(출처). 그러므로 우리는 거대한 벡터를 갖게 될 것입니다. 하지만 문장에는 50개 이상의 고유 단어가 나타나지 않을 것이므로 벡터의 값 중 99.99%는 0일 것입니다. 이는 어떤 정보도 인코딩하지 않습니다. 이에 대해 과학자들은 밀도 있는 벡터 표현에 대해 고민하기 시작했습니다.\n\n## Word2Vec\n\n<div class=\"content-ad\"></div>\n\n가장 유명한 밀집 표현 방법 중 하나는 구글이 2013년에 Mikolov 등이 제안한 \"효율적인 단어 표현 추정을 위한 Word2Vec\" 논문에서 소개한 word2vec입니다.\n\n논문에서 언급된 두 가지 word2vec 접근 방식은 Continuous Bag of Words(주변 단어를 기반으로 단어를 예측하는 방법)와 Skip-gram(반대 작업인 단어를 기반으로 문맥을 예측하는 방법)입니다.\n\n밀집 벡터 표현의 핵심 아이디어는 두 모델을 훈련하는 것입니다: 인코더와 디코더. 예를 들어, Skip-gram의 경우 \"christmas\"라는 단어를 인코더에 전달할 수 있습니다. 그런 다음, 인코더가 \"merry\", \"to\", \"you\"와 같은 단어를 얻을 것으로 예상하여 디코더에 전달할 수 있는 벡터를 생성할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_5.png)\n\n이 모델은 이제 단어의 의미를 고려하기 시작했습니다. 단어의 맥락에서 훈련되었기 때문입니다. 그러나 형태학(예: \"-less\"는 무언가의 부족을 의미함)을 무시합니다. 나중에는 GloVe에서 서브워드 스킵-그램을 살펴봄으로써 이 단점을 개선했습니다.\n\n또한, word2vec은 단어와만 작동할 수 있었지만, 우리는 전체 문장을 인코딩하고 싶습니다. 그러니, 트랜스포머로 다음 진화 단계로 넘어가 봅시다.\n\n## 트랜스포머와 문장 임베딩\n\n\n<div class=\"content-ad\"></div>\n\n다음 진화는 Vaswani 등이 발표한 \"Attention Is All You Need\" 논문에서 소개된 트랜스포머 접근 방식과 관련이 있었습니다. 트랜스포머는 정보가 풍부한 밀집 벡터를 생성할 수 있었고 현대 언어 모델의 주요 기술로 자리 잡게 되었습니다.\n\n저는 트랜스포머의 구조 세부 사항에 대해 다루지 않겠습니다. 왜냐하면 이것은 우리 주제와 관련이 그리 크지 않고 많은 시간이 소요되기 때문입니다. 더 배우고 싶다면 \"Transformers, Explained\" 또는 \"The Illustrated Transformer\"와 같은 다양한 자료가 많이 있습니다.\n\n트랜스포머를 사용하면 동일한 \"핵심\" 모델을 사용하여 다른 사용 사례에 대해 미세 조정할 수 있으며, 핵심 모델을 다시 학습시킬 필요가 없습니다(시간이 많이 소요되고 상당한 비용이 듭니다). 이것은 사전 훈련된 모델의 등장으로 이어졌습니다. 가장 인기 있는 최초의 모델 중 하나는 Google AI가 개발한 BERT(Bidirectional Encoder Representations from Transformers)였습니다.\n\n내부적으로 BERT는 여전히 word2vec과 유사한 토큰 수준에서 작동하지만, 우리는 여전히 문장 임베딩을 얻고 싶습니다. 따라서, 모든 토큰 벡터의 평균을 취하는 단순한 방법을 적용할 수 있습니다. 유감스럽게도, 이 방법은 좋은 성능을 보여주지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n2019년에 이 문제는 Sentence-BERT가 출시되면서 해결되었습니다. 이는 의미론적 텍스트 유사성 작업에서 이전 방법들을 모두 능가하며 문장 포함 벡터의 계산을 가능하게 했습니다.\n\n이 주제는 매우 방대하기 때문에 이 기사에서 모두 다 다룰 수는 없을 거예요. 그러니 진지하게 관심이 있다면 이 기사에서 문장 포함 벡터에 대해 더 배울 수 있습니다.\n\n우리는 임베딩의 발전을 간략히 다뤘고 이론에 대한 고수준 이해를 얻었습니다. 이제 실습으로 넘어가서 OpenAI 도구를 사용하여 어떻게 임베딩을 계산하는지 배워보겠습니다.\n\n# 임베딩 계산\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 OpenAI 임베딩을 사용할 것입니다. 최근에 출시된 새로운 모델인 text-embedding-3-small을 시도해볼 것입니다. 이 새로운 모델은 text-embedding-ada-002보다 성능이 더 좋게 나타났습니다:\n\n- 널리 사용되는 다국어 검색 (MIRACL) 벤치마크의 평균 점수가 31.4%에서 44.0%로 상승했습니다.\n- 영어 작업에 대한 자주 사용되는 벤치마크인 MTEB의 평균 성능도 향상되어 61.0%에서 62.3%로 상승했습니다.\n\nOpenAI는 또한 새로운 큰 모델인 text-embedding-3-large를 출시했습니다. 이제 이것이 가장 우수한 임베딩 모델입니다.\n\n데이터 소스로는 Stack Exchange Data Dump의 작은 샘플을 사용할 것입니다. 이는 Stack Exchange 네트워크에서 모든 사용자 기여 콘텐츠의 익명화된 덤프입니다. 저는 흥미로운 주제를 선택하고 각각에서 100개의 질문을 샘플링했습니다. 주제는 생성적 AI부터 커피 또는 자전거까지 다양합니다. 그래서 다양한 주제를 볼 수 있을 겁니다.\n\n<div class=\"content-ad\"></div>\n\n먼저, 모든 스택 오버플로우 질문에 대한 임베딩을 계산해야 합니다. 한 번 실행하고 결과를 로컬로 저장하는 것이 좋습니다(파일이나 벡터 저장소에). OpenAI Python 패키지를 사용하여 임베딩을 생성할 수 있습니다.\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\ndef get_embedding(text, model=\"text-embedding-3-small\"):\n   text = text.replace(\"\\n\", \" \")\n   return client.embeddings.create(input = [text], model=model)\\\n       .data[0].embedding\n\nget_embedding(\"We are lucky to live in an age in which we are still making discoveries.\")\n```\n\n결과적으로, 우리는 부동 소수점 숫자로 이루어진 1536차원 벡터를 얻습니다. 이제 이를 모든 데이터에 대해 반복하고 값을 분석할 수 있습니다.\n\n가장 궁금할 수 있는 주요 질문은 의미적으로 문장들이 얼마나 가까운지입니다. 답을 발견하기 위해 벡터 간의 거리 개념을 논의해 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 벡터 간 거리\n\n임베딩은 사실 벡터입니다. 따라서 두 문장이 얼마나 가까운지 이해하려면 벡터 간 거리를 계산할 수 있습니다. 더 작은 거리는 더 가까운 의미를 나타낼 것입니다.\n\n두 벡터 간의 거리를 측정하는 데 사용할 수 있는 다양한 메트릭이 있습니다:\n\n- 유클리디안 거리 (L2),\n- 맨하탄 거리 (L1),\n- 내적 (Dot product),\n- 코사인 거리.\n\n<div class=\"content-ad\"></div>\n\n그들에 대해 이야기해 봅시다. 간단한 예로, 우리는 두 개의 2D 벡터를 사용할 것입니다.\n\n```js\nvector1 = [1, 4]\nvector2 = [2, 2]\n```\n\n## 유클리디안 거리 (L2)\n\n두 지점(또는 벡터) 사이의 거리를 정의하는 가장 표준적인 방법은 유클리디안 거리 또는 L2 norm입니다. 이 측정 기준은 일상생활에서 가장 많이 사용되며, 예를 들어 2개의 도시 사이의 거리를 언급할 때 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\nL2 거리에 대한 시각적 표현과 공식이 있습니다.\n\n![Image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_6.png)\n\n파이썬 또는 numpy 함수를 사용하여 이 메트릭을 계산할 수 있습니다.\n\n```python\nimport numpy as np\n\nsum(list(map(lambda x, y: (x - y) ** 2, vector1, vector2))) ** 0.5\n# 2.2361\n\nnp.linalg.norm((np.array(vector1) - np.array(vector2)), ord = 2)\n# 2.2361\n```\n\n<div class=\"content-ad\"></div>\n\n# 맨해튼 거리 (L1)\n\n다른 일반적으로 사용되는 거리 측정 방법은 L1 노름 또는 맨해튼 거리입니다. 이 거리는 뉴욕의 맨해튼 섬에서 명명되었습니다. 이 섬은 거리가 격자 레이아웃으로 되어 있고, 맨해튼에서 두 지점 사이의 가장 짧은 경로는 격자 모양을 따라야 하므로 L1 거리가 됩니다.\n\n![image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_7.png)\n\n우리는 이를 처음부터 구현하거나 numpy 함수를 사용하여 구현할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\nsum(list(map(lambda x, y: abs(x - y), vector1, vector2)))\r\n# 3\r\n\r\nnp.linalg.norm((np.array(vector1) - np.array(vector2)), ord = 1)\r\n# 3.0\r\n```\r\n\r\n## 내적(Dot product)\r\n\r\n벡터 간 거리를 계산하는 다른 방법은 내적 또는 스칼라 곱을 계산하는 것입니다. 다음은 해당 공식이며 쉽게 구현할 수 있습니다.\r\n\r\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_8.png\" />\n\n<div class=\"content-ad\"></div>\n\n```python\nsum(list(map(lambda x, y: x*y, vector1, vector2)))\n# 11\n\nnp.dot(vector1, vector2)\n# 11\n```\n\n이 메트릭은 해석하기가 조금 까다로운 편이에요. 한편으로는 벡터가 한 방향을 향하고 있는지를 보여줍니다. 다른 한편으로는 결과는 벡터들의 크기에 크게 의존합니다. 예를 들어 두 쌍의 벡터 간의 내적을 계산해볼게요:\n\n- (1, 1) vs (1, 1)\n- (1, 1) vs (10, 10).\n\n두 경우 모두 벡터가 일직선상에 있지만, 두 번째 경우에 내적은 10배 크게 나와요: 2 대 20.\n\n<div class=\"content-ad\"></div>\n\n## 코사인 유사도\n\n많은 경우, 코사인 유사도가 사용됩니다. 코사인 유사도는 벡터의 크기(또는 노름)에 의해 정규화된 내적입니다.\n\n![Image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_9.png)\n\n이전처럼 직접 모든 것을 계산하거나 sklearn의 함수를 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndot_product = sum(list(map(lambda x, y: x*y, vector1, vector2)))\nnorm_vector1 = sum(list(map(lambda x: x ** 2, vector1))) ** 0.5\nnorm_vector2 = sum(list(map(lambda x: x ** 2, vector2))) ** 0.5\n\ndot_product/norm_vector1/norm_vector2\n\n# 0.8575\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncosine_similarity(\n  np.array(vector1).reshape(1, -1), \n  np.array(vector2).reshape(1, -1))[0][0]\n\n# 0.8575\n```\n\ncosine_similarity 함수는 2차원 배열을 기대합니다. 그래서 numpy 배열을 reshape 해주어야 합니다.\n\n이 메트릭의 물리적 의미에 대해 조금 이야기해 봅시다. Cosine similarity는 두 벡터 사이의 코사인 값과 같습니다. 벡터가 서로 가까울수록 메트릭 값이 높아집니다.\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_10.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n우리는 심지어 벡터 사이의 정확한 각도를 도 단위로 계산할 수도 있어요. 약 30도 정도의 결과를 얻었고, 꽤 합리적으로 보이네요.\n\n```js\nimport math\nmath.degrees(math.acos(0.8575))\n\n# 30.96\n```\n\n## 어떤 측정 지표를 사용할까요?\n\n우리는 두 벡터 사이의 거리를 계산하는 다양한 방법에 대해 토론해 왔고, 여러분은 어떤 방법을 사용할지 고려하기 시작할 수 있을 거예요.\n\n<div class=\"content-ad\"></div>\n\n내가 가진 임베딩을 비교하기 위해 어떤 거리든 사용할 수 있어요. 예를 들어, 다른 클러스터 사이의 평균 거리를 계산했어요. L2 거리와 코사인 유사도 모두 비슷한 결과를 보여줘요:\n\n- 클러스터 내의 객체들은 다른 클러스터보다 서로 더 가까워요. L2 거리에 대해 가까울수록 낮은 거리를 의미하지만 코사인 유사도에서는 가까운 객체일수록 값이 높아져요. 헷갈리지 마세요.\n- \"정치\"와 \"경제\" 또는 \"ai\"와 \"데이터과학\"과 같이 일부 주제들이 서로 아주 가까운 것을 알 수 있어요.\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_11.png\" />\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_12.png\" />\n\n<div class=\"content-ad\"></div>\n\n그러나 NLP 작업에 대해서는 일반적으로 코사인 유사도를 사용하는 것이 최선의 방법입니다. 몇 가지 그 이유는:\n\n- 코사인 유사도는 -1과 1 사이에 있으며, L1과 L2는 무제한이기 때문에 해석하기 쉽습니다.\n- 실용적인 측면에서 유클리드 거리의 제곱근보다 내적을 계산하는 것이 더 효과적입니다.\n- 코사인 유사도는 차원의 저주에 영향을 덜 받습니다 (이에 대해 뒤에서 더 얘기할 것입니다).\n\n위의 결과에서 인트라 및 인터 클러스터 거리 간의 차이가 크지 않다는 점을 알 수 있을 것입니다. 이 현상의 원인은 벡터의 고차원성 때문입니다. 이 효과는 \"차원의 저주\"라고 불리며, 차원이 높을수록 벡터 간 거리 분포가 좁아진다는 것을 알 수 있습니다. 이에 대해 더 자세히 알아보려면 이 글을 참조해보세요.\n\n간단히 설명드리겠습니다. OpenAI 임베딩 값의 분포를 계산하고 차원이 다른 300개의 벡터 집합을 생성했습니다. 그런 다음, 모든 벡터 사이의 거리를 계산하고 히스토그램을 그렸습니다. 차원이 증가함에 따라 벡터의 거리 분포가 좁아진다는 것을 쉽게 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_13.png)\n\n임베딩 사이의 유사성을 측정하는 방법을 배웠어요. 여기서 이론적인 부분은 마쳤고, 더 실용적인 부분(시각화 및 실용적인 응용)으로 넘어가겠습니다. 데이터를 보는 것이 가장 중요하니, 시각화부터 시작해봐요.\n\n# 임베딩 시각화\n\n데이터를 이해하는 가장 좋은 방법은 시각적으로 나타내는 것이에요. 아쉽지만, 임베딩은 1536차원이 있어서 데이터를 살펴보기가 꽤 어려워요. 그러나, 한 가지 방법이 있어요: 차원 축소 기술을 사용하여 벡터를 이차원 공간에 투영하는 것이에요.\n\n<div class=\"content-ad\"></div>\n\n## PCA\n\n가장 기본적인 차원 축소 기술은 PCA(주성분 분석)입니다. 이를 사용해 봅시다.\n\n먼저, sklearn에 전달하기 위해 임베딩을 2D numpy 배열로 변환해야 합니다.\n\n```python\nimport numpy as np\nembeddings_array = np.array(df.embedding.values.tolist())\nprint(embeddings_array.shape)\n# (1400, 1536)\n```\n\n<div class=\"content-ad\"></div>\n\n그럼, 우리는 PCA 모델을 n_components = 2로 초기화해야 해요 (2D 시각화를 생성하고 싶기 때문에), 전체 데이터에서 모델을 학습하고 새로운 값을 예측해야 해요.\n\n```js\nfrom sklearn.decomposition import PCA\n\npca_model = PCA(n_components = 2)\npca_model.fit(embeddings_array)\n\npca_embeddings_values = pca_model.transform(embeddings_array)\nprint(pca_embeddings_values.shape)\n# (1400, 2)\n```\n\n결과적으로, 우리는 각 질문에 대해 두 개의 특성을 가진 행렬을 얻었으므로, scatter plot에서 쉽게 시각화할 수 있어요.\n\n```js\nfig = px.scatter(\n    x = pca_embeddings_values[:,0], \n    y = pca_embeddings_values[:,1],\n    color = df.topic.values,\n    hover_name = df.full_text.values,\n    title = 'PCA embeddings', width = 800, height = 600,\n    color_discrete_sequence = plotly.colors.qualitative.Alphabet_r\n)\n\nfig.update_layout(\n    xaxis_title = 'first component', \n    yaxis_title = 'second component')\nfig.show()\n```\n\n<div class=\"content-ad\"></div>\n\n![img](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_14.png)\n\n각 주제의 질문들이 서로 꽤 가까이 위치해 있는 것을 볼 수 있어 좋습니다. 그러나 모든 클러스터가 혼재되어 있어서 개선할 부분이 있습니다.\n\n## t-SNE\n\nPCA는 선형 알고리즘이지만, 대부분의 관계는 실제로는 비선형입니다. 그래서 비선형성 때문에 클러스터를 분리할 수 없을 수도 있습니다. 비선형 알고리즘인 t-SNE을 사용해보고 더 나은 결과를 보여줄 수 있는지 확인해봅시다.\n\n<div class=\"content-ad\"></div>\n\n거의 동일한 코드를 사용했습니다. PCA 대신 t-SNE 모델을 사용했어요.\n\n```js\nfrom sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, random_state=42)\ntsne_embeddings_values = tsne_model.fit_transform(embeddings_array)\n\nfig = px.scatter(\n    x = tsne_embeddings_values[:,0], \n    y = tsne_embeddings_values[:,1],\n    color = df.topic.values,\n    hover_name = df.full_text.values,\n    title = 't-SNE embeddings', width = 800, height = 600,\n    color_discrete_sequence = plotly.colors.qualitative.Alphabet_r\n)\n\nfig.update_layout(\n    xaxis_title = 'first component', \n    yaxis_title = 'second component')\nfig.show()\n```\n\nt-SNE 결과가 훨씬 좋아 보여요. 대부분의 클러스터가 분리되어 있지만 \"genai\", \"datascience\", \"ai\" 는 분리되지 않았어요. 그러나 이건 예상한대로에요 - 이러한 주제를 내가 분리할 수 있을지 의심스러워요.\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_15.png\" />\n\n<div class=\"content-ad\"></div>\n\n이 시각화를 보면 임베딩이 의미적 의미를 인코딩하는 데 상당히 효과적임을 확인할 수 있어요.\n\n또한, 데이터를 3D로 시각화할 수 있는 사영(projection)을 만들어볼 수 있어요. 실용적일지는 확실하지 않지만, 데이터를 3D로 살펴보는 것은 흥미롭고 관심을 끌 수 있어요.\n\n```js\ntsne_model_3d = TSNE(n_components=3, random_state=42)\ntsne_3d_embeddings_values = tsne_model_3d.fit_transform(embeddings_array)\n\nfig = px.scatter_3d(\n    x = tsne_3d_embeddings_values[:,0], \n    y = tsne_3d_embeddings_values[:,1],\n    z = tsne_3d_embeddings_values[:,2],\n    color = df.topic.values,\n    hover_name = df.full_text.values,\n    title = 't-SNE embeddings', width = 800, height = 600,\n    color_discrete_sequence = plotly.colors.qualitative.Alphabet_r,\n    opacity = 0.7\n)\nfig.update_layout(xaxis_title = 'first component', yaxis_title = 'second component')\nfig.show()\n```\n\n![3D 시각화](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_16.png)\n\n<div class=\"content-ad\"></div>\n\n## 바코드\n\n임베딩을 이해하는 방법은 몇 개를 바코드처럼 시각화하여 상관 관계를 확인하는 것입니다. 나는 세 가지 임베딩 예시를 선택했습니다: 두 개는 서로에게 가장 가깝고, 나머지 하나는 데이터 세트에서 가장 멀리 떨어져 있는 예시입니다.\n\n```js\nembedding1 = df.loc[1].embedding\nembedding2 = df.loc[616].embedding\nembedding3 = df.loc[749].embedding\n```\n\n```js\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nembed_len_thr = 1536\n\nsns.heatmap(np.array(embedding1[:embed_len_thr]).reshape(-1, embed_len_thr),\n    cmap = \"Greys\", center = 0, square = False, \n    xticklabels = False, cbar = False)\nplt.gcf().set_size_inches(15,1)\nplt.yticks([0.5], labels = ['AI'])\nplt.show()\n\nsns.heatmap(np.array(embedding3[:embed_len_thr]).reshape(-1, embed_len_thr),\n    cmap = \"Greys\", center = 0, square = False, \n    xticklabels = False, cbar = False)\nplt.gcf().set_size_inches(15,1)\nplt.yticks([0.5], labels = ['AI'])\nplt.show()\n\nsns.heatmap(np.array(embedding2[:embed_len_thr]).reshape(-1, embed_len_thr),\n    cmap = \"Greys\", center = 0, square = False, \n    xticklabels = False, cbar = False)\nplt.gcf().set_size_inches(15,1)\nplt.yticks([0.5], labels = ['바이오인포매틱스'])\nplt.show()\n```  \n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_17.png)\n\n우리 경우에는 고차원 때문에 벡터가 서로 가까운지 쉽게 보기 어려울 수 있습니다. 그래도 나는 이 시각화를 좋아합니다. 몇 가지 경우에 도움이 될 수도 있으니, 나는 이 아이디어를 당신과 공유하고자 합니다.\n\n우리는 임베딩을 시각화하는 방법을 배웠고, 텍스트의 의미를 파악하는 능력에 대한 의문은 남지 않았습니다. 이제 실제로 임베딩을 어떻게 활용할 수 있는지에 대해 논의하는 가장 흥미로운 부분으로 넘어가 보겠습니다.\n\n# 실용적인 응용\n\n<div class=\"content-ad\"></div>\n\n물론, 임베딩의 주요 목표는 텍스트를 숫자의 벡터로 인코딩하거나 시각화하기 위해서만 하는 것이 아닙니다. 우리는 텍스트의 의미를 포착하는 능력에서 많은 이점을 얻을 수 있습니다. 실용적인 예제들을 함께 살펴보겠습니다.\n\n## 클러스터링\n\n먼저 클러스터링부터 시작해보죠. 클러스터링은 초기 레이블 없이 데이터를 그룹으로 분할할 수 있는 비지도학습 기술입니다. 클러스터링을 통해 데이터의 내부 구조적 패턴을 이해하는 데 도움을 받을 수 있습니다.\n\n가장 기본적인 클러스터링 알고리즘 중 하나인 K-평균을 사용할 것입니다. K-평균 알고리즘을 위해서는 클러스터의 개수를 지정해야 합니다. 실루엣 스코어를 사용하여 최적의 클러스터 수를 정의할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n2부터 50까지의 k (클러스터 수)를 시도해 보겠습니다. 각 k에 대해 모델을 훈련하고 실루엣 점수를 계산할 것입니다. 실루엣 점수가 높을수록, 더 좋은 클러스터링 결과를 얻을 수 있습니다.\n\n```python\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport tqdm\n\nsilhouette_scores = []\nfor k in tqdm.tqdm(range(2, 51)):\n    kmeans = KMeans(n_clusters=k, \n                    random_state=42, \n                    n_init='auto').fit(embeddings_array)\n    kmeans_labels = kmeans.labels_\n    silhouette_scores.append(\n        {\n            'k': k,\n            'silhouette_score': silhouette_score(embeddings_array, \n                                                 kmeans_labels, metric='cosine')\n        }\n    )\n\nfig = px.line(pd.DataFrame(silhouette_scores).set_index('k'),\n              title='<b>K-means 클러스터링을 위한 실루엣 점수</b>',\n              labels={'value': '실루엣 점수'}, \n              color_discrete_sequence=plotly.colors.qualitative.Alphabet)\nfig.update_layout(showlegend=False)\n```\n\n우리의 경우, k가 11일 때 실루엣 점수가 최대치에 도달합니다. 따라서 최종 모델에는 이 클러스터 수를 사용합시다.\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_18.png\" />\n\n<div class=\"content-ad\"></div>\n\n클러스터를 시각화해 보는 t-SNE를 이용한 차원 축소를 이미 이전에 수행한 것처럼 해보겠습니다.\n\n```js\ntsne_model = TSNE(n_components=2, random_state=42)\ntsne_embeddings_values = tsne_model.fit_transform(embeddings_array)\n\nfig = px.scatter(\n    x = tsne_embeddings_values[:,0], \n    y = tsne_embeddings_values[:,1],\n    color = list(map(lambda x: '클러스터 %s' % x, kmeans_labels)),\n    hover_name = df.full_text.values,\n    title = '클러스터링을 위한 t-SNE 임베딩', width = 800, height = 600,\n    color_discrete_sequence = plotly.colors.qualitative.Alphabet_r\n)\nfig.update_layout(\n    xaxis_title = '첫 번째 성분', \n    yaxis_title = '두 번째 성분')\nfig.show()\n```\n\n시각적으로 알고리즘이 클러스터를 상당히 잘 정의했음을 확인할 수 있습니다 — 그들은 꽤 잘 분리되어 있습니다.\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_19.png\" />\n\n<div class=\"content-ad\"></div>\n\n우리는 사실적인 주제 라벨을 가지고 있으므로, 클러스터링이 얼마나 좋은지를 심층적으로 판단할 수도 있어요. 각 클러스터에 대한 주제 혼합을 살펴봅시다.\n\n```js\ndf['cluster'] = list(map(lambda x: '클러스터 %s' % x, kmeans_labels))\ncluster_stats_df = df.reset_index().pivot_table(\n    index='cluster', values='id',\n    aggfunc='count', columns='topic').fillna(0).applymap(int)\n\ncluster_stats_df = cluster_stats_df.apply(\n  lambda x: 100*x/cluster_stats_df.sum(axis=1))\n\nfig = px.imshow(\n    cluster_stats_df.values, \n    x=cluster_stats_df.columns,\n    y=cluster_stats_df.index,\n    text_auto='.2f', aspect=\"auto\",\n    labels=dict(x=\"클러스터\", y=\"팩트 주제\", color=\"비율, %\"),\n    color_continuous_scale='pubugn',\n    title='<b>각 클러스터의 주제 비율</b>', height=550)\n\nfig.show()\n```\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_20.png\" />\n\n대부분의 경우, 클러스터링은 완벽하게 작동했어요. 예를 들어, 클러스터 5에는 거의 자전거에 관한 질문만 있고, 클러스터 6은 커피에 관한 것이에요. 그러나 유사한 주제를 구별하지 못했어요:\n\n<div class=\"content-ad\"></div>\n\n- \"ai,\" \"genai,\" and \"datascience\"은 동일한 클러스터에 있습니다.\n- \"economics\"와 \"politics\"은 같은 그룹에 속합니다.\n\n이 예제에서는 피처로써 임베딩만 사용했지만, 질문을 한 사용자의 나이, 성별 또는 국가와 같은 추가 정보가 있다면 모델에 포함시킬 수도 있습니다.\n\n## 분류\n\n임베딩을 분류 또는 회귀 작업에 사용할 수 있습니다. 예를 들어 고객 리뷰 감정을 예측하는 (분류)이나 NPS 점수를 예측하는 (회귀) 등 다양한 작업에 활용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n분류 및 회귀는 지도 학습이므로 레이블이 필요합니다. 다행히도, 우리는 질문의 주제를 알고 있으므로 모델을 적합시켜 예측할 수 있습니다.\n\n저는 랜덤 포레스트 분류기를 사용할 것입니다. 랜덤 포레스트에 대해 간단히 상기하고 싶다면 여기에서 확인할 수 있어요. 분류 모델의 성능을 올바르게 평가하려면 데이터 세트를 학습 및 테스트 세트(80% 대 20%)로 분할할 것입니다. 그런 다음 학습 데이터 세트에서 모델을 훈련하고 테스트 데이터 세트에서 품질을 측정할 수 있습니다(모델이 이전에 보지 못한 질문).\n\n```js\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nclass_model = RandomForestClassifier(max_depth = 10)\n\n# 특징 및 대상 정의\nX = embeddings_array\ny = df.topic\n\n# 데이터를 학습 및 테스트 세트로 분할\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state = 42, test_size=0.2, stratify=y\n)\n\n# 적합 및 예측\nclass_model.fit(X_train, y_train)\ny_pred = class_model.predict(X_test)\n```\n\n모델의 성능을 추정하기 위해 혼동 행렬을 계산해 보겠습니다. 이상적인 상황에서는 비대각 요소가 모두 0이어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nfig = px.imshow(\n  cm, x = class_model.classes_,\n  y = class_model.classes_, text_auto='d', \n  aspect=\"auto\", \n  labels=dict(\n      x=\"predicted label\", y=\"true label\", \n      color=\"cases\"), \n  color_continuous_scale='pubugn',\n  title = '<b>혼동 행렬</b>', height = 550)\n\nfig.show()\n```\n\n![이미지](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_21.png)\n\n군집화와 유사한 결과를 확인할 수 있습니다. 일부 주제는 쉽게 분류되고 정확도가 100%인 반면, 다른 주제들은 구별하기 어려운 경우도 있습니다(특히 \"ai\" 주제).\n\n하지만 전체적으로 91.8%의 정확도를 달성했으며, 이는 꽤 좋은 성과입니다.\n\n<div class=\"content-ad\"></div>\n\n## 이상 징후 찾기\n\n데이터에서 이상 징후를 찾기 위해 임베딩을 사용할 수도 있습니다. 예를 들어, t-SNE 그래프에서 \"여행\" 주제에 대한 몇 가지 질문이 군집에서 꽤 멀리 떨어져 있는 것을 볼 수 있었습니다. 이 테마를 살펴보고 이상 징후를 찾아보겠습니다. 이를 위해 이상 탐지 알고리즘인 Isolation Forest를 사용할 것입니다.\n\n```js\nfrom sklearn.ensemble import IsolationForest\n\ntopic_df = df[df.topic == 'travel']\ntopic_embeddings_array = np.array(topic_df.embedding.values.tolist())\n\nclf = IsolationForest(contamination=0.03, random_state=42)\ntopic_df['is_anomaly'] = clf.fit_predict(topic_embeddings_array)\n\ntopic_df[topic_df.is_anomaly == -1][['full_text']]\n```\n\n여기에서, 여행 주제에 대한 가장 흔하지 않은 댓글을 찾았습니다 (원본).\n\n<div class=\"content-ad\"></div>\n\n```js\n로마 구역의 곳곳에 있는 분수에서 물을 마셔도 안전한가요?\n\n로마를 방문했을 때 오래된 지역을 거닐며 다양한 종류의 분수를 보았습니다. 물이 끊임없이 흘러나오는 분수들이 많았는데, 땅으로 흘러가는 분수도 있고, 대야에 모이는 분수도 있었습니다.\n\n이런 분수에서 나오는 물을 마셔도 괜찮을까요? 방문객이 마실 수 있는 안전한 물일까요? 분수 사용에 대한 방문자들이 알아야 할 예절이 있을까요?\n```\n\n물에 관한 이야기이기 때문에 이 주석의 기능은 사람들이 물을 따르는 커피 주제와 밀접하게 관련되어 있습니다. 그래서 이 주석의 삽입 표현은 커피 클러스터와 꽤 가까운 것으로 보입니다.\n\nt-SNE 시각화에서 찾아보면 실제로 커피 클러스터에 가까운 것을 확인할 수 있습니다.\n\n![이미지](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_22.png)\n\n\n<div class=\"content-ad\"></div>\n\n## RAG — 검색 증가 생성\n\n최근 LLM의 인기가 높아지면서, 임베딩이 RAG 사용 사례에서 널리 사용되고 있습니다.\n\n우리는 많은 문서가 있는 경우(예: 스택 오버플로우의 모든 질문)에 검색 증가 생성이 필요합니다. 그리고 모든 정보를 항상 LLM에 전달할 수 없기 때문에\n\n- LLM은 컨텍스트 크기에 제한이 있습니다(현재 GPT-4 Turbo의 경우 128K입니다).\n- 우리는 토큰을 구매해야 하므로 모든 정보를 항상 전달하는 것이 더 비십니다.\n- LLM은 더 큰 컨텍스트에서 성능이 떨어집니다. 자세한 내용은 \"바늘 찾기\" - LLM의 압력 테스트를 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n대규모 지식 베이스와 함께 작업할 수 있도록 RAG 방법론을 활용할 수 있어요:\n\n- 모든 문서에 대한 임베딩을 계산하고 벡터 저장소에 저장합니다.\n- 사용자 요청을 받으면 해당 요청의 임베딩을 계산하여 저장소에서 관련 문서를 검색할 수 있어요.\n- 최종 답변을 얻기 위해 LLM에게 관련 문서만 전달하면 돼요.\n\nRAG에 대해 더 자세히 알고 싶다면 여기에 더 많은 내용을 담은 제 논문을 읽어보세요.\n\n# 요약\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 텍스트 임베딩에 대해 많은 세부 내용을 논의했습니다. 이제 여러분은 이 주제에 대해 완전하고 심도 있는 이해를 가졌을 것입니다. 저희 여정을 간단히 요약하면 다음과 같습니다:\n\n- 먼저, 텍스트 작업 방법의 진화를 살펴보았습니다.\n- 그 다음으로, 텍스트 간에 유사한 의미를 가지고 있는지를 이해하는 방법에 대해 논의했습니다.\n- 그 후에는 텍스트 임베딩 시각화의 다양한 접근 방법을 살펴보았습니다.\n- 마지막으로, 임베딩을 클러스터링, 분류, 이상 탐지 및 RAG와 같은 다양한 실용적인 작업에서 특징으로 사용해 보았습니다.\n\n# 참고\n\n이 기사에서는 크리에이티브 커먼즈 라이센스 하에 공개된 스택 엑스체인지 데이터 덤프에서 데이터 세트를 사용했습니다.\n\n<div class=\"content-ad\"></div>\n\n이 글은 다음 강좌에서 영감을 받았습니다:\n\n- DeepLearning.AI와 Google Cloud의 협력으로 진행되는 \"Understanding and Applying Text Embeddings\",\n- DeepLearning.AI와 Weaviate의 협력으로 진행되는 \"Vector Databases: From Embeddings to Applications\".","ogImage":{"url":"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_0.png"},"coverImage":"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_0.png","tag":["Tech"],"readingTime":24},{"title":"심리 상담 챗봇  정신 건강을 위한 LLMs 사용 방법","description":"","date":"2024-06-23 19:51","slug":"2024-06-23-TherapistChatbotLLMsforMentalHealth","content":"\n\n<img src=\"/assets/img/2024-06-23-TherapistChatbotLLMsforMentalHealth_0.png\" />\n\n요즘 빠르게 변화하는 세상에서 우리의 정신 건강을 우선시할 시간을 찾기는 어려울 수 있어요. 업무, 가족, 그리고 일상 생활의 요구들은 종종 자기 관리를 할 시간을 남기지 않아서 정기적인 치료 세션은 물론 스스로를 돌보는 시간을 가져다주지 않을 때가 많아요. 시간을 내기로 결심했다고 해도, 자격 있는 치료사의 이용 가능성이 부족하면 예약을 기다리는 사람들이 많아질 수 있어요. 그리고 마침내 소중한 세션이 확보되더라도, 비용이 부담스러울 수 있어 이미 부담스러운 마음에 금전적인 압박을 느끼게 해요.\n\n이 딜레마는 정신 건강 치료 분야에서 혁신적인 해결책에 대한 점점 더 커지는 필요성을 강조해요. 진출하게 된 것이 바로 언어 모델(Language Models, LLMs)을 기반으로 하는 치료사 챗봇들의 시대입니다. 이 가상 상담가들은 우리가 스마트폰이나 컴퓨터의 편안함 속에서 24시간 365일 즉각적인 지원을 받을 수 있도록하여 우리가 지원을 받는 방식을 혁신하려고 합니다.\n\n인공 지능(AI)과 자연어 처리(NLP)를 활용하여, 이 챗봇들은 실제 치료 대화를 시뮬레이션할 수 있어요. 이들은 공감, 지도, 그리고 개인의 필요에 맞게 맞춤형 대처 전략을 제공할 수 있어요. 이러한 접근 방식은 특히 사회적 편견이나 물리적인 장벽 때문에 전통적인 치료를 받기 주저하는 분들에게 접근하는 데 매우 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n그러나 중요한 점은 심리상담사 챗봇은 정신 건강 관리를 보완할 수 있지만 전문 상담의 대체품이 아니라는 것입니다. 훈련받은 심리상담사들의 세밀한 이해력과 공감적인 대응을 갖추지 못합니다. 그들은 교육과 경험에 기초한 맞춤형 치료를 제공할 수 있는 훈련받은 상담사들의 심리 마음을 나타내지 못합니다.\n\n심리건강 관리에 LLMs를 통합하는 목표는 접근성을 증대하고 확대하는 데 있습니다. 더 나은 심리적 안녕을 향한 여정을 하는 사람들에게 발판을 제공합니다. 심리 건강 문제에 직면한 모든 사람들에게 전문적인 도움을 구하는 것이 중요한 단계이며, 이러한 혁신은 자기 돌봄과 지원을 위한 우리 도구상자에 유망한 추가 요소로 작용합니다.\n\n그런데, 이 기술을 활용하여 우리 자신의 심리 상담사 AI를 만들어보는 방법에 대해 깊이 파고들어 보겠습니다!\n\n전체 프로젝트와 함께 제 GitHub는 다음에서 확인할 수 있습니다 — https://github.com/Dev-Pandey-0302/Therapist-Chatbot\n\n<div class=\"content-ad\"></div>\n\n유튜브의 Nicholas Renotte에 대한 큰 찬사를 드립니다 (Nicholas Renotte를 검색해보세요, 놀라운 데이터 과학자!)\n\n# 코딩을 시작합니다.\n\n이 프로젝트의 핵심은 Llama_cpp 프레임워크를 기반으로 구축될 것입니다. 간단한 접근 방식은 ollama를 사용하는 것일 수 있지만, 이 안내서에서는 Llama_cpp를 사용하는 데 중점을 둘 것입니다.\n\n먼저, 우리는 ~특정 부분이 누락되었습니다~\n\n<div class=\"content-ad\"></div>\n\n```js\r\ngit clone https://github.com/ggerganov/llama.cpp\r\n```\r\n\r\n원하는 기계에 llama_cpp를 설치할 거예요.\r\n\r\n다음으로, make 명령을 실행해야 해요:\r\n\r\n- Mac: cd llama.cpp && make\r\n- Windows (from here):\r\n\n\n<div class=\"content-ad\"></div>\n\n- 최신 Fortran 버전의 w64devkit을 다운로드하세요.\n- PC에 w64devkit을 압축 해제하세요.\n- w64devkit.exe를 실행하세요.\n- cd 명령어를 사용하여 llama.cpp 폴더로 이동하세요.\n- 여기서\n\n```js\nmake\n```\n\n위 명령어를 실행하세요. 그 후에는 의존성을 설치합니다. 가능하다면 의존성을 설치하기 전 가상 환경을 만들어도 좋습니다. 가상 환경을 만드는 방법을 모르신다면 아래 링크를 확인해보세요- https://www.freecodecamp.org/news/how-to-setup-virtual-environments-in-python/\n\n```js\npip install openai 'llama-cpp-python[server]' pydantic instructor streamlit gtts\n```\n\n<div class=\"content-ad\"></div>\n\n# GGUF 모델 다운로드\n\nGGUF가 무엇인가요?\n\n물어봐 주셔서 감사합니다!\n\n이 애플리케이션에서는 다음을 사용할 것입니다-\n\n<div class=\"content-ad\"></div>\n\nWesselvanGils/MentaLLaMA-chat-7b-GGUF-q8\n\n이는 8비트 양자화된 GGUF 모델입니다. 따라서 전용 서버가 필요하지 않고 로컬 머신에서 실행할 수 있습니다.\n\n이 모델을 오픈 소스로 만들어준 Wessel van Gils에게 감사드립니다. 여기에 그들의 GitHub이 있습니다!\n\n더 자세한 정보를 알고 싶다면 자유롭게 이 글을 읽어보세요.\n\n<div class=\"content-ad\"></div>\n\n모델 다운로드로 돌아가기\n\n여기로 이동해주세요- https://huggingface.co/WesselvanGils/MentaLLaMA-chat-7b-GGUF-q8\n\n파일 및 버전 탭을 클릭하고 .gguf 파일을 다운로드하세요. 용량이 7GB가 넘는 파일이라 시간이 조금 걸릴 수 있으니 참고해 주세요.\n\n# app.py 파일 만들기\n\n<div class=\"content-ad\"></div>\n\n이제 app.py 파일을 만들어 봅시다.\n\n이 튜토리얼에서는 streamlit을 사용할 것입니다. 그러나 gradio와 같은 대체 솔루션을 사용해도 괜찮습니다.\n\n```python\nfrom openai import OpenAI\nfrom gtts import gTTS\nfrom io import BytesIO, StringIO\n# streamlit 앱 프레임워크 사용\nimport streamlit as st\n\n# 클라이언트 생성\nclient = OpenAI(\n    api_key=\"sk-1234567890\",\n    base_url='http://localhost:8000/v1'\n)\n\n# 앱의 제목\nst.title(\"TherapyBot- Mental Health Support를 위한 챗봇\")\n\n# 의료 기록 업로드\nuploaded_file = st.file_uploader(\"\", type=[\"txt\"], label_visibility=\"collapsed\")\ncss = '''\n<style>\n    [data-testid='stFileUploader'] {\n        width: max-content;\n    }\n    [data-testid='stFileUploader'] section {\n        padding: 0;\n        float: left;\n    }\n    [data-testid='stFileUploader'] section > input + div {\n        display: none;\n    }\n    [data-testid='stFileUploader'] section + div {\n        float: right;\n        padding-top: 0;\n    }\n\n</style>\n'''\nst.markdown(css, unsafe_allow_html=True)\n\ndoc_data = \"\"\n# 파일 업로드 시\nif uploaded_file is not None:\n    # 파일을 문자열로 읽기\n    stringio = StringIO(uploaded_file.getvalue().decode(\"utf-8\"))\n    doc_data = stringio.read()\n    doc_data = \"This is my medical record - \" + doc_data + \" Please answer the following question based on the earlier medical record- \"\n    \n# 사용자 입력 받기\nprompt = st.chat_input('채팅을 시작하거나 의료 기록을 업로드하십시오. 어떻게 도와드릴까요?')\n...\n```\n\n이 특정 app.py 파일은 구글의 gtts 라이브러리를 사용한 텍스트 음성 변환 기능도 제공합니다. 이를 위해서는 인터넷 연결이 필요하지만, 오프라인으로 완전히 실행하려면 gtts를 import하지 않고 마지막 4줄을 주석 처리하세요.\n\n<div class=\"content-ad\"></div>\n\n이 파일을 저장한 후 로컬 서버를 실행해 보겠습니다.\n\n# 라마.cpp 서버\n\n터미널에서 다음을 실행하면 서버가 가동될 것입니다. 다운로드한 GGUF 모델의 경로가 올바른지 확인하세요.\n\n```js\npython -m llama_cpp.server --model D:\\CHATBOT_PROJ_NEW\\MentaLLaMA-chat-7b-GGUF-q8\\MentaLLaMA-chat-7b-GGUF-q8.gguf --n_gpu -1\n```\n\n<div class=\"content-ad\"></div>\n\n# App.py 실행\n\n로컬호스트 서버가 시작된 후, 별도의 터미널에서 다음 명령을 실행하세요\n\n```js\nstreamlit run app.py\n```\n\n축하합니다!\n\n<div class=\"content-ad\"></div>\n\n지역 컴퓨터에서 실행 중인 자체 치료사 AI를 만들었습니다. .txt 파일을 제공하든 일반적인 질문을 하든 자유롭게 진행해주세요!\n\n![이미지](/assets/img/2024-06-23-TherapistChatbotLLMsforMentalHealth_1.png)\n\n읽어 주셔서 감사합니다!","ogImage":{"url":"/assets/img/2024-06-23-TherapistChatbotLLMsforMentalHealth_0.png"},"coverImage":"/assets/img/2024-06-23-TherapistChatbotLLMsforMentalHealth_0.png","tag":["Tech"],"readingTime":5},{"title":"컴퓨터는 왜 이진수Binary 체계를 사용할까","description":"","date":"2024-06-23 19:49","slug":"2024-06-23-WhydoComputersevenuseBinary","content":"\n\n# 소개적인 쓰레기\n\n데이터 과학자의 주된 도구는 무엇인가요? 공정하고 명백한 답은 컴퓨터입니다. 컴퓨터는 우리보다 훨씬 빠르게 데이터를 처리하기 때문에 데이터를 활용하는 모든 작업을 컴퓨터 없이 하는 것을 상상해 보세요. 곁에 연필과 종이가 있어 손으로 수많은 표와 그래프, 계산식을 지쳐하며 그리는 과정을 생각해 보세요. 이 모든 작업은 컴퓨터에서 몇 초만에 처리될 것을 알고 있지만요. 데이터 과학은 컴퓨터 없이는 존재하지 않을 것이라고 하더군요.\n\n컴퓨터가 데이터 과학을 수행하는 데 필수적인 도구라는 사실을 감안하면, 해당 직업에는 컴퓨터 작동 원리를 이해하는 것이 필요할 것으로 예상됩니다. 결국, 도구를 이해하지 못할 때 작업을 올바르게 수행하는 것은 어려운 일입니다. 그러나 지망하는 데이터 과학자에게는 해당 분야에 대한 기초 지식을 습득하는 데 시간을 할애하는 것이 너무 쉬울 수도 있습니다.\n\n![그림](/assets/img/2024-06-23-WhydoComputersevenuseBinary_0.png)\n\n<div class=\"content-ad\"></div>\n\n알겠어요. 수학, 컴퓨터 과학 및 공학의 이론적 주제는 종종 Python, Tensorflow 또는 Amazon Web Services와 같은 응용 중심 학습 목표보다 즉각적으로 보상을 받기가 어려워서(또는 솔직히 말해 즉각적으로 고용가능하지 않아서) 부차적인 학습 목표로 밀려납니다. 신진 데이터 과학자가 왜 이러한 종류의 주제로 향하는지 이해할 수밖에 없죠. 이러한 주제는 프로젝트 포트폴리오가 빠르게 성장함에 따라 기술과 이해력의 즉각적이고 실질적인 향상을 제공하기 때문입니다.\n\n데이터 과학 여정을 시작하는 데 좋은 시작점이기는 하지만, 더 이론적인 측면을 학습하면 시작 단계를 넘어 다음 단계를 나아갈 수 있습니다. 이 문서의 목적은 당신에게 데이터 과학에서 매일 사용하는 기술의 이론적 배경을 소개하는 데 있습니다.\n\n좋아요, 이제 소개가 끝났으니 재미있는 내용으로 들어가볼게요. 우리는 아마도 잠깐이라도 생각해 봤을 질문으로 시작할 거에요: 컴퓨터가 왜 이진수를 사용할까요? 결과부터 말하자면: 그렇지 않으면 비효율적일 것이기 때문입니다. 이 문서의 목적은 왜 그것이 비효율적인지 설명하는 것입니다.\n\n# 이진수란 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n이미 이진수가 무엇인지를 알고 계시다면, 이 섹션을 건너 뛰셔도 좋아요. 아니라면, 이 내용이 귀하를 위한 것입니다!\n\n상상해보세요. 만약 우리가 지적인 외계 생명체를 만난다면. 손가락이 5개가 아니라 3개씩 두 손에 각각 있는 특이한 생물이 있다고 상상해보세요. 그들은 어떻게 세어갈까요? 우리는 1부터 9까지의 숫자를 사용하지만, 그 이후에는 첫 번째 숫자를 0으로 되돌리고 다음 자리를 1부터 열어 10을 만듭니다. 그러나 우리의 외계 친구들은 이것이 별로 이해하기 어렵다고 생각할지도 모릅니다. 왜냐하면 그들은 6개의 손가락밖에 없고 우리는 10개이기 때문입니다. 당신이 숫자에 할당하는 자리값이 얼마나 임의적인지 알게 되고 있나요? 만약 사람들이 10개의 손가락을 태어나지 않았다면, 우리는 아마 우리가 하는 방식으로 세지 않을 것입니다. 우리는 10부터 새로운 숫자를 시작하기 대신에, 6, 2, 16, 46 또는 다른 어떤 숫자든, 각 값을 나타내는 고유한 기호가 있으면 됩니다. 외계인이 어떻게 세는지와 우리가 어떻게 세는지 비교해 봅시다. 각 행에 있는 값은 서로 동일합니다:\n\n![image](/assets/img/2024-06-23-WhydoComputersevenuseBinary_1.png)\n\n우리는 이러한 새로운 숫자 시스템을 'X 진수' 숫자 시스템이라고 합니다. 여기서 X는 새로운 자리로 넘어가는 값입니다. 예를 들어, 우리의 숫자 시스템은 각 자리가 10보다 더 커지기 전에 끝나므로 10 진수 시스템이라고 합니다. 반면에, 우리의 외계인 친구들은 6 진수 시스템을 갖게 될 것입니다. 왜냐하면 그들의 숫자는 6보다 작아지기 때문입니다. 컴퓨터는 2 진수 시스템에서 작동하며 바이너리로 알려져 있습니다. 바이너리 시스템에 적용된 외계인의 6 진수 시스템에 적용된 동일한 계산 논리를 적용해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n\n![Binary Explanation](/assets/img/2024-06-23-WhydoComputersevenuseBinary_2.png)\n\n만약 그 이해가 당신에게 의미가 있다면, 이진법이 어떻게 작동하는지 이해하고 있는 것입니다. 너무 겁나지 않았으면 좋겠어요! 많은 사람들이 이것을 \"언어\"로 지칭해서 겁을 먹는데, 이진법은 그냥 다른 숫자 체계일 뿐이며, 우리가 익숙한 10진법처럼 작동합니다. 산술도 기본적으로 우리 체계에서 하는 것과 동일하게 작동합니다.\n\n우리 숫자가 10의 거듭제곱의 합으로 나타낼 수 있는 것과 같이, 이진수는 2의 거듭제곱의 합으로 나타낼 수 있습니다. 설명하는 가장 좋은 방법은 예를 통해 설명하는 것입니다. (참고로, 이진수는 숫자 앞에 0b를 붙여서 표시되기도 합니다. 이 글에서는 이 방식으로 계속해서 표시할 것입니다).\n\n10진법에서\n\n\n<div class=\"content-ad\"></div>\n\n`<img src=\"/assets/img/2024-06-23-WhydoComputersevenuseBinary_3.png\" />`\n\n마찬가지로, 2진법의 경우,\n\n`<img src=\"/assets/img/2024-06-23-WhydoComputersevenuseBinary_4.png\" />`\n\n위의 합산을 수행하면, ob1101은 13이 됩니다. 2진수를 10진수로 변환하는 쉬운 방법입니다.\n\n<div class=\"content-ad\"></div>\n\n# 왜 컴퓨터는 이진수를 사용할까요?\n\n이제 이진수가 어떻게 작동하는지 기본적으로 이해했으니, 이 기사의 중심 질문에 대해 다룰 수 있습니다. 컴퓨터가 정보를 이진수로 표현하는 이유는 무엇일까요? 컴퓨터가 정보를 이진수로 표현해야 하는 유일한 방법이기 때문은 아닙니다. 이것은 흔한 오해입니다. 사실, ENIAC과 같은 최초의 컴퓨터들 중 일부는 10진수를 사용했습니다. 현대 컴퓨터가 정보를 표현할 때 이진수를 사용하는 큰 이유는 세 가지가 있습니다.\n\n# 첫 번째 이유, 공간 효율성\n\n우리가 이진수 체계를 사용하는 큰 이유 중 하나는 다른 체계보다 간단하기 때문입니다. 간단히 말하면, 오로지 2 상태(0 또는 1)만을 표현하는 것은 더 쉽고 더 적은 물리적 부품이 필요합니다. 이것이 왜 그런지 이해하려면, 컴퓨터와 같은 디지털 시스템이... 음... 디지털이라는 것을 이해해야 합니다. 즉, 컴퓨터는 연속적인 수 대신 숫자의 이산적 표현을 사용하는 하드웨어를 사용합니다. 수학 수업에서 기억할 수 있겠지만, 이산적인 수는 정수(0, 1, -4 등)이고 연속적인 수는 정수와 각 소수부(0 1, -4, .2, -.2343 등)가 있는 수입니다. 그러나 컴퓨터는 전기를 사용하여 동작하는 실제 시스템이기 때문에 전압 수준을 정보로 표현하는 경우가 많습니다. 전압은 연속적인 값이며(3.22682393 볼트인 것이 가능합니다), 컴퓨터는 디지털입니다 — 오직 이산적인 값만을 사용하는 방식을 알고 있습니다. 어떻게 하면 연속적인 값(전압)을 디지털 시스템으로 표현할 수 있을까요? 우리는 전압 값을 디지털 값에 상응하는 전압 범위로 설정하여 그 방법을 찾습니다. 아래는 그러한 설정의 예시입니다:\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-WhydoComputersevenuseBinary_5.png)\n\n상기 도표를 사용하는 회로는 0부터 2 V 사이의 전압을 0b0으로 간주하고, 4V부터 6 V 사이를 0b1로, 그 이외의 값은 시스템이 문제가 있다는 것을 알려줍니다. 중간에 전환 존이 포함되어 있는 것을 주목하세요. 물리적 시스템에 대해 생각하는 것에 익숙하지 않다면, 우리가 0과 1 사이를 즉시 왔다갔다할 수 있어야 할 것 같다는 것이 이상할 수 있습니다. 그러나 물리적 시스템에서는 오차 여유를 만들어야 합니다. 그렇지 않으면 시스템이 필요 이상으로 매우 취약해질 수 있습니다. 토론을 벌이지는 않지만, 디지털 회로는 논리 게이트로 이루어져 있고, 논리 게이트는 켜고 끄기 위한 공간이 필요합니다. 이 낮음과 높음 사이의 전압 범위는 게이트가 \"종류\"로 켜진 상태입니다. 이 범위에 머무르면 구성 요소에 손상을 줄 수 있으므로, 구성 요소를 쉬게 두지 않는 전환 존을 계획합니다.\n\n만약 3진법 시스템을 만들고 싶다면, 또 다른 상태를 추가해야 합니다:\n\n![이미지](/assets/img/2024-06-23-WhydoComputersevenuseBinary_6.png)\n\n\n<div class=\"content-ad\"></div>\n\n자연스럽게 다른 상태를 추가하려면 더 많은 하드웨어 구성 요소가 필요합니다. 이는 더 높은 기수 체계를 사용하는 시스템을 설계할 때 하드웨어 면에서 복잡해지는 점을 의미합니다(설계 및 구성 요소 수 측면에서). 이것이 컴퓨터가 이진수를 사용하는 근본적인 이유입니다. 그러나 높은 기수 체계를 사용하는 것이 더 중요한 두 가지 영향을 주목해 보겠습니다.\n\n# 두 번째 이유, 전력 효율성\n\n전기 전력을 정의하는 것에 대해 잠시 이야기해 봅시다. 방정식은 정말로 간단합니다:\n\n![전기 전력](/assets/img/2024-06-23-WhydoComputersevenuseBinary_7.png)\n\n<div class=\"content-ad\"></div>\n\nP는 전력(Watts), I는 전류(Amps), R은 저항(Ohms), V는 전압(Voltz)를 나타냅니다. 각 새로운 논리 수준마다 전압을 높일수록, 전체 장치의 전력 소비가 지수적으로 증가함에 따라 동등한 전압이 가정되며, 이는 대부분의 경우 합리적인 가정입니다. 소비 전력이 많을수록 장치를 실행하는 데 소비되는 비용이 더 많아지므로 대부분의 경우 이러한 시스템을 피하는 이유가 큽니다.\n\n# 세 번째 이유, 열 효율성\n\n랩톱이 한 두 시간 동안 열심히 작동한 후 뒷면을 만져본 적이 있나요? 아마도 많은 열을 느꼈을 것입니다. 과도한 열은 전자제품의 적이며, 그 생성을 피하는 것이 엔지니어들에게 우선 과제입니다. 열에 관한 방정식을 살펴 보세요:\n\n![Heat Equation](/assets/img/2024-06-23-WhydoComputersevenuseBinary_8.png)\n\n<div class=\"content-ad\"></div>\n\nQ는 열(쥴), t는 시간(초), 그리고 R과 I는 다시 저항과 전류를 나타냅니다. 회로에 사용하는 각 새로운 구성 요소는 작동에 더 많은 전류를 그리는 결과를 가져올 것입니다. 이에 따라 열이 증가합니다.\n\n# 결론적인 발언\n\n이 기사에서는 컴퓨터가 왜 이진수를 사용하는지(이진수가 무엇인지 소개하는 것 외에도) 다루었습니다. 우리는 이것이 비효율적이라는 것을 알고 있고, 그 이유에 대한 메커니즘인 전압, 전류, 그리고 전력 등의 개념을 소개했습니다. 컴퓨팅에서 다른 숫자 체계를 사용하려면 디지털 논리 회로로 더 많은 상태를 나타내야 한다는 것을 보았습니다. 그 결과 전력과 회로 구성 요소가 더 필요하며 더 많은 열을 발생시키면서, 엔지니어와 소비자가 피하려고 하는 것입니다.\n\n하지만, 우리는 실제로 컴퓨터가 이 정보를 어떻게 나타내는지를 정말 다루지 않았습니다 — 그저 디지털 회로를 사용한다고 말했을 뿐입니다. 다음 몇 개의 기사에서는 이 질문을 더 자세히 탐구할 것입니다. 디지털 논리 회로가 어떻게 작동하는지 설명해 보겠습니다 — 가능한 최하위 수준인 반도체부터 시작해서요. 이에 관심이 있다면, 많이 기대해 주세요! 이 기사와 같이 저는 전기 공학 개념을 아는 것으로 상정하지 않고 성장 중인 데이터 과학자들을 대상으로 하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n아마도 당신은 새로운 데이터 과학자이거나 이미 경험 많은 데이터 과학자이실 것입니다. 이 기사가 흥미롭게 느껴졌으면 좋겠습니다. 컴퓨터 하드웨어가 데이터 과학이나 머신 러닝 개념과 정확히 관련이 없지만, 이것들은 우리의 전문 분야 도구이며, 이에 대한 기본적인 이해를 가지는 것이 가치 있다고 생각합니다. 동의하시고 이 기사가 흥미롭다고 생각하셨다면, 다음에는 실제 컴퓨터 계산 방식에 대해 다룰 예정이니, 그때 뵐 수 있기를 희망합니다!\n\n# 참고자료\n\n[1] R. Palaniappan, Digital Systems Design (2011), https://dvikan.no/ntnu-studentserver/kompendier/digital-systems-design.pdf","ogImage":{"url":"/assets/img/2024-06-23-WhydoComputersevenuseBinary_0.png"},"coverImage":"/assets/img/2024-06-23-WhydoComputersevenuseBinary_0.png","tag":["Tech"],"readingTime":7},{"title":"주식 예측에서 머신러닝이 실패하는 주요 이유 파트 01","description":"","date":"2024-06-23 19:47","slug":"2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01","content":"\n\n이 블로그 시리즈에서는 머신 러닝이 주식 가격을 예측하는 데 실패하는 이유 또는 일반적으로 머신 러닝 기반 투자 펀드가 실패하는 이유에 대해 논의하려 합니다. 이 블로그의 내용은 Marcos Lopez de Prado의 \"금융 머신 러닝 발전\"이라는 책에서 가져왔습니다. 이 책은 금융에 관심 있는 모든 사람들에게 필독서입니다. 이 책은 금융 데이터를 처리하는 동안 머신 러닝 실무자들이 범한 모든 실수를 언급합니다. 이 블로그를 통해 이 책에서의 학습 내용을 요약하려고 합니다.\n\n- Reason 1 : 메모리 vs 정상성 트레이드 오프 :\n\n주식의 가격을 예측하고 싶다고 가정해 봅시다. ARMA와 같은 모든 전통적인 방법이 정상성 데이터에 작용한다는 것을 알고 있습니다. 일반적으로 정상 데이터는 일련의 시리즈 전체에서 일정한 평균과 일정한 분산을 의미합니다. 데이터를 정상으로 만들기 위해 시계열 데이터에서 차분을 수행합니다. 1차 차분은 현재 주가 값을 이전 값에서 뺀 것을 의미합니다.\n\n![Image](/assets/img/2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01_0.png)\n\n<div class=\"content-ad\"></div>\n\n차이를 만들면 데이터가 정체성을 띄게 됩니다. 그러나 1차 차이를 구할 때는 데이터의 모든 과거적인 패턴을 잃어버리게 됩니다(그림 참조). 이로 인해 그 데이터의 내용을 잃게 됩니다. 그리고 기억력은 모델의 예측 능력을 결정하는 중요한 요소입니다. 데이터를 정체성을 갖도록 만드는 중간 과정에서 기억력을 잃게 됩니다. 이런 실수가 학술 논문이나 업무 현장에서 많이 발생합니다. 그렇다면 어떻게 데이터를 정체성을 갖게 하면서도 정보를 완전히 잃지 않을 수 있을까요? 그 대답은 없습니다. 따라서 더 많이 정체성을 갖도록 만드는 경우에는 더 많은 기억력을 잃게 됩니다. 릴라이언스의 과거 가격을 통해 이러한 경우를 이해해봅시다.\n\n![image](/assets/img/2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01_1.png)\n\n위 그래프에서 보듯이 데이터를 정체성을 갖도록 만들면 가격이 시간에 따라 어떻게 변동하는지에 대한 정보를 모두 잃고 따라서 그 기억력도 잃게 됩니다. 그리고 이에 따라 그런 데이터로 만든 모델의 예측 능력도 잃게 됩니다. 그렇다면 어떻게 해야 할까요? 부분적인 정체성을 달성하면서 부분적인 기억력을 잃지 않도록 하는 방법을 찾아야 합니다. 그렇게 되면 분수 차이화라는 개념이 등장합니다. 분수 차이화에서는 1차 차이화 대신 어떤 분수 값을 사용하여 차이화를 수행하게 되어 모든 정보를 완전히 잃지 않습니다. 그렇다면 분수 차이화를 어떻게 수행할까요? 함께 살펴봅시다.\n\n![image](/assets/img/2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01_2.png)\n\n<div class=\"content-ad\"></div>\n\n위에서 보시다시피 주문 차이를 확장했습니다. 이제 만약 d=0.3의 값을 넣으면 0.2의 분수 차이 값이 나올 것입니다. 이것은 무한급수이므로 어떤 지점까지 값을 취할 수 있고, 그 이후에는 시리즈를 잘라내도 괜찮습니다. 왜냐하면 B^n 계수 값이 높아질수록 거의 제로에 가까워질 것이기 때문입니다. 아래 그래프는 일정 지점 이후에 서로 다른 d 값에 대한 B^n 계수를 보여줍니다.\n\n이제 동일한 플롯을 동일한 차수의 분수 차이로 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n상단 차트에서 확인할 수 있듯이, 차분의 순서를 증가할수록 점점 더 많은 메모리를 잃고 더욱 안정화됩니다. 차분이 0일 때는 모든 메모리를 가지고 있지만, 시리즈는 안정적이지 않고 차분이 1일 때는 시리즈가 메모리를 가지고 있지 않지만 완전히 안정화됩니다. 그래서 우리는 어느 정도의 메모리를 잃으면서 데이터를 의미 있는 확률로 안정화할지 교환해야 합니다. 그래서 이제 d값을 어떻게 찾아야 할까요?\n\n시계열의 안정성을 위한 ADF(Augmented Dickey-Fuller) 검정이 관련됩니다. ADF 검정은 시리즈가 안정적인지 여부를 테스트하는 데 사용됩니다. 다양한 d 값에 대해 ADF 검정을 수행한 후, 아래 빨간색 선 그래프는 다양한 d 값에 대한 검정 통계 값입니다. 검정 통계 값이 수평선보다 낮다면 해당 시리즈가 안정적이라고 할 수 있습니다. 따라서 아래 차트에서 d=0.4가 시리즈를 안정화시키는 데 충분하다고 할 수 있습니다. 그래서 시리즈에서 최소한의 메모리 손실로 안정화된 시리즈를 얻으려면 d=0.4를 사용할 수 있습니다.\n\n2. 이유 2: 비효율적인 샘플링\n\n<div class=\"content-ad\"></div>\n\n다른 많은 실무자와 학술 논문 작성자들이 하는 또 다른 흔한 실수는 데이터 샘플링이 비효율적인 것입니다. 대부분의 경우 그들은 데이터를 시간 간격마다 샘플링합니다. 예를 들어, 5분마다 또는 10분마다 데이터를 샘플링합니다. 시간 프레임에 기반한 데이터 샘플링 시 주요 문제점이 있습니다.\n\n- 시장이 정규시간 간격에 맞춰 정보를 처리하지 않기 때문에 문제가 발생합니다. 예를 들어 시장은 오픈할 때보다 정오에 활동성이 높으므로, 높은 활동성 시간 동안 정보를 과소샘플링하고 낮은 활동성 시간 동안 정보를 과대샘플링합니다.\n- 시간 샘플링된 데이터는 연쇄상관, 이분산성 및 수익의 비정상성과 같은 부정적인 통계적 특성을 보입니다.\n\n이 문제를 극복하기 위해 다양한 바(bar)가 정의될 것입니다. \n\n- 틱 바(Tick bar): 타임스탬프, 거래량, 오픈 가격, 종가 등의 모든 변수를 일정 거래 횟수 이후 추출합니다. 예를 들어, 1000 거래가 이루어진 후 모든 변수를 샘플링합니다. Mandelbrot 및 Taylor [1967]은 거래 횟수에 따른 샘플링이 우수한 통계적 특성을 보인다는 것을 처음으로 깨달았습니다: \"고정된 거래 횟수에 따른 가격 변동이 가우시안 분포를 가질 수 있습니다.\"\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01_6.png)\n\n![이미지](/assets/img/2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01_7.png)\n\n- 볼륨 바: 틱 바는 주문 조각화(쪼개짐) 문제가 있습니다. 예를 들어, 어떤 가격에 어떤 가격에 10주를 매도하는 경우, 10주를 사 실 때 1틱으로 기록됩니다. 하지만 만약 1주를 10번 사면 10개의 거래로 기록됩니다. 이 문제를 해결하기 위해 일정 거래량이 발생한 후 정보를 샘플링합니다. 이것이 볼륨 바라고 알려져 있습니다.\n- 달러 바: 달러 바는 일정 거래가 발생한 후 정보를 샘플링하여 형성됩니다. 예를 들어, $5000의 거래가 발생한 후에 정보를 샘플링합니다. \"값\"은 반드시 $로만 측정되는 것은 아니고, Rs, 유로 등이 될 수 있습니다. 달러 바가 필요한 이유는 무엇일까요? 특정 기간 동안 100%의 평가 상승을 보인 주식을 분석하려고 할 때, 그 기간 끝에 $1,000가치의 그 주식을 판매하려면, 그 주식을 $1,000가치 살 때와는 반의 주식을 거래해야 합니다. 다시 말해, 거래된 주식 수는 실제 교환된 가치에 따라 결정됩니다. 그러므로, 주요 가격 변동이 있는 분석에 관여할 때, 거래된 값의 관점에서 바를 샘플링하는 것이 의미가 있습니다. 또 다른 주장은 보너스 주식, 주식 분할로 주식 수가 자주 변경되므로 거래량보다는 가격을 기준으로 샘플링하는 것이 더 합리적이라는 점입니다.\n\n다음 파트에서는 정보 주도의 일부 고급 정보 바에 대해 이야기해보겠습니다. 즉, 시장에 새로운 정보가 들어오면 정보를 샘플링하는 것을 의미합니다. 다음 블로그 시리즈에서 정보 주도형 바에 대해 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n이 블로그를 좋아하신다면 꼭 의겢이나 좋아요를 클릭해 주세요!\n\n참고:\n\n1) Marcos López de Prado의 금융 기계 학습 발전","ogImage":{"url":"/assets/img/2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01_0.png"},"coverImage":"/assets/img/2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01_0.png","tag":["Tech"],"readingTime":5},{"title":"컴퓨터는 실제로 어떻게 계산할까","description":"","date":"2024-06-23 19:45","slug":"2024-06-23-HowDoComputersActuallyCompute","content":"\n\n# 소개글\n\n컴퓨터가 숫자를 계산하는 장치라는 말은 항상 듣고 다닙니다. 그들은 인간이 희망하기 힘든 속도로 데이터를 처리하며, 초당 수천 개의 논리적인 결정을 내릴 수 있다는데요. 하지만 그들이 이를 물리적으로 어떻게 하는 걸까요? 이러한 매우 비싼 기기가 자신들의 창조자들을 (어느 면에서는) 능가할 수 있는 무엇이 그리 특별한 걸까요? 이 질문에 우리는 이 기사에서 다룰 것입니다.\n\n![이미지](/assets/img/2024-06-23-HowDoComputersActuallyCompute_0.png)\n\n이것은 내가 쓰는 새로운 시리즈, \"데이터 과학자를 위한 컴퓨터 하드웨어 입문\"의 두 번째 부분입니다. 물리학, 전기 공학 또는 저수준 컴퓨터 과학 관련 지식을 전제로 하지 않는 이 시리즈는 새로운 (또는 경험이 풍부한) 데이터 과학자가 전문 도구에 대한 이해를 깊이 있게 하고자 합니다. 결국, 어떤 뛰어난 장인이 자신의 도구에 대해 익숙하지 않겠습니까?\n\n<div class=\"content-ad\"></div>\n\n시리즈의 이전 설치 파일을 따라 가기 위해서는 필요하지 않아요 (원한다면 다음에서 읽을 수 있어요: 컴퓨터는 왜 이진수를 사용할까요?). 컴퓨터가 어떻게 작동하는지 궁금하지만 해당 분야에 대한 배경이 많이 없다면 이 시리즈는 시작하기에 좋은 장소가 될 거예요.\n\n멋진데요, 입문부터 떨어뜨렸어요. 이제 재미있는 부분으로 들어가볼까요?\n\n# 가장 기본적인 디지털 회로 요소: 트랜지스터\n\n디지털 회로에서 가장 기본적인 요소는 트랜지스터에요. 트랜지스터는 반도체 기반 구성 요소로 스위치처럼 작용할 수 있어요. 반도체(예를 들면 실리콘)는 특정 조건 하에서 전기를 전도하고, 다른 조건 하에서 전기를 차단하는 재료들의 한 종류에요. 반도체 재료를 재빠르게 활용함으로써 회로 구성 요소를 \"켜고\" \"끌\" 수 있어요. (트랜지스터로 가능한 다른 것들도 있지만, 이 기사에서는 다루지 않을 거예요). 재료를 \"재빠르게 활용\"하는 방법은 반도체로 트랜지스터를 만드는 것이에요.\n\n<div class=\"content-ad\"></div>\n\n트랜지스터는 베이스(base), 콜렉터(collector), 그리고 에미터(emitter)의 세 개의 I/O 위치를 가지고 있어요. 아래의 트랜지스터 다이어그램에서 콜렉터는 상단에 있고, 베이스는 중간 왼쪽에 위치하며, 에미터는 하단에 위치합니다. 이런 트랜지스터의 배치는 NPN (부정-긍정-부정) BJT(양극성 접합 트랜지스터)라고 불립니다. BJTs보다 더 인기 있는 많은 종류의 트랜지스터가 있어요. 실제로 디지털 응용에서 사용되는 트랜지스터는 주로 FETs (장효과 트랜지스터)이고, BJTs는 아닙니다. 그렇지만 기술적 세부사항이 약간 다르더라도 최종 결과는 대부분 동일합니다 — 모든 트랜지스터는 여전히 스위치 역할을 합니다. BJTs가 약간 더 간단하기 때문에, 이 점에 중점을 두겠어요.\n\n![트랜지스터 다이어그램](/assets/img/2024-06-23-HowDoComputersActuallyCompute_1.png)\n\n트랜지스터에 관한 한 가지를 기억해주세요. 베이스에 약간의 전기를 가하면 콜렉터에서 에미터로 더 많은 양의 전기가 흐를 수 있습니다. 이를 수도꼭지를 켜고 끄는 것처럼 생각해보세요. 우리는 스지(Ge이미터)를 통해 물(전기)이 물 공급원(콜렉터)에서 나가게 하기 위해 수도선을 돌리는 것처럼, 베이스에 약간의 전기를 가해서 전류를 제어합니다. BJTs의 경우, 우리는 전류를 다루어 전류 흐름을 조절합니다. FETs의 경우, 전압을 다루어 전압 \"흐름\"을 조절합니다. 그러나 두 방법 모두 개념은 동일합니다.\n\n좋아요, 이제 우리는 트랜지스터가 무엇이며, 스위치를 만들기 위해 반도체를 사용한다는 것을 이해했습니다. 우리는 베이스에 약간의 전기를 제어하여 콜렉터와 에미터 간의 전기 흐름을 켜고 끕니다. 믿든 안 믿든, 이 아이디어 — 스위치 — 가 컴퓨터 안에서 일어나는 모든 것의 기본 구성 요소입니다. 우리는 트랜지스터를 사용하여 논리 회로를 설계할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n# 논리 게이트: 컴퓨터가 어떻게 계산하는지\n\n논리 게이트는 기본 논리 기능을 구현하는 회로 요소들로 (다소 자명하게) 설계됩니다. 논리 게이트의 예로는 AND, OR, NOT 게이트가 있습니다. 논리 게이트의 출력은 입력 조건이 충족될 때 켜지고, 그렇지 않으면 꺼집니다.\n\n우리가 깊게 들어가기 전, 전기를 데이터로 어떻게 이해하는지 간단히 다시 살펴보겠습니다. 디지털 회로에 대해 이야기할 때, 보통 0을 나타내는 전압 범위와 1을 나타내는 전압 범위가 있습니다. 간단히 말해, 입력/출력에 전기가 적게 있다면 해당 입력/출력 값이 0임을 이해합니다. 전기가 조금 이상 있다면 1로 간주합니다. 이를 통해 이진수로 회로 작업을 수행할 수 있습니다. (이전 기사인 '컴퓨터가 왜 이진수를 사용할까?'를 읽었으면, 더 자세히 다룬 내용일 것입니다). 입력 또는 출력의 전기 양 대신 입력 또는 출력의 0 또는 1에 대해 이야기함으로써 문제를 단순화합니다. 그러나 이러한 숫자들이 전기 양으로 모델링된다는 것을 잊지 마세요.\n\n(부가 설명 — 간단히 설명하기 위해 약간 우외해주고 있습니다. 전압은 단순히 전기의 양이라고 말하기 정확하지 않습니다. 그러나 두 가지를 동일시함으로써 전체 개념을 조금 더 쉽게 이해하고, 디지털 논리 회로 작동 방식을 이해하는 데 지장이 없게 합니다.)\n\n<div class=\"content-ad\"></div>\n\n대박이네요! 이제 이해하셨으니 논리 게이트로 빠져들어볼까요! 예를 들어, AND 게이트를 살펴봅시다.\n\n![AND Gate](/assets/img/2024-06-23-HowDoComputersActuallyCompute_2.png)\n\n왼쪽에 있는 두 개의 도일은 입력이고, 오른쪽에 있는 도일이 출력입니다. AND 게이트 안에는 잘 배치된 수많은 트랜지스터가 있지만, 그것에 대해서는 나중에 다뤄보겠습니다. AND 게이트는 어떻게 작동할까요? 입력 두 곳에 모두 1(전기가 많음)이 있을 때 출력이 1이 됩니다. 그렇지 않으면 출력은 0이 됩니다. 이는 파이썬의 \"and\" 키워드와 마찬가지로 작동합니다! 사실, 파이썬(그리고 어떤 프로그래밍 언어든, 정말로)은 실제로 “and”, “or”, “not”과 같은 기능을 수행하기 위해 논리 게이트를 사용합니다. 멋지지 않나요? 파이썬에서 “and”를 호출하면 실제로 컴퓨터의 CPU 내부에 있는 단일 논리 게이트를 사용하고 있는 것이죠! 이 하드웨어 구성요소를 (거의) 직접 사용해오셨는데 그것을 몰랐다니 대단하시네요!\n\n<div class=\"content-ad\"></div>\n\n디지턈 회로에 대해 이야기할 때는 입력과 출력에 대해 정확하게 설명할 방법이 필요합니다. 이를 위해, 참진리표(truth table)라 불리는 것을 사용합니다. 여기 AND 게이트의 참진리표가 있어요.\n\n![AND Gate Truth Table](/assets/img/2024-06-23-HowDoComputersActuallyCompute_3.png)\n\n2개의 입력이 각각 2가지 상태를 가지기 때문에 입력에 대한 2² = 4개의 다른 배열이 있습니다. 따라서 참진리표는 4행을 가지게 됩니다. 우리는 두 입력이 모두 1이면 켜지는(1인) 회로를 원했기 때문에 참진리표에 1을 확인할 수 있습니다.\n\nAND 게이트의 경우, 참진리표는 명백하지만 더 복잡한 회로의 경우, 참진리표는 입력을 출력에 매핑하는 회로를 추적하는 데 매우 유용한 도구입니다.\n\n<div class=\"content-ad\"></div>\n\n## 논리 게이트의 하부 구조\n\n좋아요, 이제 논리 게이트가 어떻게 작동하는지 알게 되었어요. 그리고 트랜지스터의 기본 개념도 이해했군요. 이제 이 둘을 어떻게 결합하여 AND 게이트를 만들 수 있는지 알아볼까요? 아래의 회로를 살펴보세요.\n\n![image](/assets/img/2024-06-23-HowDoComputersActuallyCompute_4.png)\n\n위쪽에 전기원이 있습니다. 기억하세요, 트랜지스터는 스위치처럼 작동하기 때문에 소스에서 전기가 흐를 수 있기 위해서는 해당 트랜지스터의 베이스에서 전기가 존재해야 합니다. 양 입력에 각각 1이 입력되면 어떻게 될까요? 소스에서 출력까지 전기가 흐를 수 있기 때문에 출력 값은 1이 됩니다. 그러나 어느 하나의 트랜지스터라도 꺼져 있다면 (즉, 입력 중 하나에 0이 입력된 경우), 전기가 차단되어 출력 값이 0이 됩니다. 이는 AND에 대한 참 진리표와 일치합니다! 이제 AND의 논리를 구현하는 물리적인 구성 요소를 가졌습니다! OR 및 NOT에 대해서도 비슷한 설계 과정을 따를 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n사이드 노트: 그렇다고 계속 괴롭힐 필요는 없어요 — 이 회로가 AND를 위한 실제 회로보다는 조금 더 간단합니다. 실제로, 원하는대로 전기를 흘리기 위해 저항기와 전기 접지와 같은 몇 가지 전기 부품이 필요합니다. 그러나 이 간단화된 버전은 일어나는 대부분의 것을 대표하고 있으며 주된 개념도 전달하고 있습니다.\n\n# 논리 게이트 결합: 복잡한 계산\n\n그래서, 우리는 트랜지스터 수준에서 논리 게이트가 어떻게 동작하는지 알고 있습니다. AND, OR, NOT와 같은 함수를 구현할 수 있습니다. 그러나 거기서부터 더 복잡한 것으로 어떻게 나아가야 할까요? 결국, 논리는 컴퓨터가 수행할 수 있는 유일한 계산이 아닙니다. 저희는 데이터 과학을 할 때 덧셈, 곱셈, 나눗셈 등을 자주 진행하니까 뭔가 더 있어야 한다는 것이 맞죠.\n\n이러한 고차 함수 중 많은 것들이 논리 게이트를 사용하여 구현될 수 있다는 것이 밝혀졌습니다. 예를 들어, 덧셈을 살펴보죠. AND와 XOR 두 개의 게이트만 사용합니다. XOR은 베타식 OR의 약자입니다. 입력 중 하나라도 1이면 XOR의 출력이 켜집니다. 그러나 모두 1이거나 아무도 아니면 출력은 0이 됩니다. XOR의 진리표는 다음과 같습니다.\n\n<div class=\"content-ad\"></div>\n\n아래 서킷을 살펴보세요:\n\n![circut](/assets/img/2024-06-23-HowDoComputersActuallyCompute_6.png)\n\n만약 하실 마음이 있다면 아래 진리표를 스스로 채워보세요. 머리 속에서 채워도 괜찮아요. 결과물이 궁금하시겠죠?\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-HowDoComputersActuallyCompute_7.png)\n\n두 입력이 모두 0 인 경우에는 어떻게 되나요? 그럼 AND 와 XOR가 꺼져서, Output과 Carry가 모두 꺼집니다.\n\n![이미지](/assets/img/2024-06-23-HowDoComputersActuallyCompute_8.png)\n\n0 과 1 은 어떤가요? XOR이 켜져 있고 AND는 꺼져 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n아래는 마크다운 형식으로 변경해야 합니다.\n\n\n<img src=\"/assets/img/2024-06-23-HowDoComputersActuallyCompute_9.png\" />\n1 and 0 would be the same, then.\n\n<img src=\"/assets/img/2024-06-23-HowDoComputersActuallyCompute_10.png\" />\nWhat about 1 and 1? The XOR is off and AND is on!\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-HowDoComputersActuallyCompute_11.png\" />\n\n이진 덧셈 회로의 진리표를 보셨나요? Carry는 다음 자리로 1을 옮기는 것을 나타내며 (일반 덧셈에서 1의 자리에서 10의 자리로 1을 옮기는 것과 같습니다), 출력은 해당 자리의 값을 나타냅니다! 수동으로 확인해볼 수도 있어요: 1 + 1 = 2. 2를 이진수로 표현하면 0b10입니다. 1은 Carry로, 0은 출력으로 표현됩니다.\n\n1비트 더하는 회로는 두 개의 간단한 게이트로 표현할 수 있어요. 더 많은 비트를 추가하려면 동일한 더하는 회로를 더 이어붙이기만 하면 돼요 (이를 캐스케이딩이라고 해요). 논리 게이트와 진리표를 사용하여 다양하고 중요한 회로를 만들어낼 수 있어요.\n\n# 마무리맺음\n\n<div class=\"content-ad\"></div>\n\n이번 글에서는 트랜지스터가 논리 게이트를 만드는 데 어떻게 사용되는지 다루었으며, 논리 게이트가 결합되어 컴퓨터가 수행하는 계산 기능을 많이 생성하는 방법도 다뤘습니다. 아직 궁금증이 남아 계신다면 댓글에서 질문해 주세요! 하나 더 언급하지 못한 점은 진리 표에서 회로 설계로 어떻게 이어지는지 입니다. 이 글은 하드웨어 작동 방식보다는 설계 프로세스에 더 초점을 맞추었기 때문에 여기에는 잘 맞지 않는 부분이었습니다. 그러나 답변을 드리겠습니다. - 부울 대수를 사용합니다. 이 시리즈의 후속 섹션에서 부울 대수에 대해 다룰 예정입니다.\n\n이로써 마무리 지었습니다! 본 글은 저의 컴퓨터 하드웨어 시리즈 중 두 번째 글입니다. 다음에는 컴퓨터가 실제로 기억하는 방법에 대해 이야기할 예정입니다!\n\n이 시리즈를 좋아하시나요? 개선할 점이 있나요? 다루어보기를 원하는 하드웨어 주제가 있나요? 깊이 들어갈 부분이 있나요? 과하게 다루어진 부분이 있나요? 댓글에서 제안해 주시면 대답해 드리겠습니다. 피드백은 이 글이 하드웨어 배경이 없는 독자를 위해 적절한 수준에서 쓰여져 있는지, 그리고 흥미롭고 교육적인 내용으로 써졌는지 확인하는 데 도움이 됩니다. 읽어 주셔서 감사합니다. 다음에 뵙겠습니다!","ogImage":{"url":"/assets/img/2024-06-23-HowDoComputersActuallyCompute_0.png"},"coverImage":"/assets/img/2024-06-23-HowDoComputersActuallyCompute_0.png","tag":["Tech"],"readingTime":7},{"title":"에이전트 AI의 핵심 과제인 계획 문제 강화학습으로 해결하는 방법","description":"","date":"2024-06-23 19:42","slug":"2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning","content":"\n\n창의력을 발휘하여 복잡한 비즈니스 전략 문제를 해결하기 위해 원활하게 협업하는 AI 에이전트 팀을 상상해보세요. 시장 동향을 조사하는 한 명의 에이전트, 재무 데이터를 분석하는 다른 한 명, 그리고 권고 사항을 준비하는 세 번째 에이전트가 모두 공통 목표를 향해 노력하고 있습니다.\n\n이 협력적 인 인공 지능의 논리, 즉 앤젠틱 AI를 알아보면 자동화와 문제 해결의 다음 단계를 나타냅니다. AI 시스템이 더욱 정교해지면서 미리 정의된 고정적인 프로세스를 벗어나 유연성, 적응력 및 AI 에이전트 간의 팀워크를 받아들이는 데 관심이 증가하고 있습니다.\n\n앤젠틱 AI는 기존의 전통적인 자동화 기술로 해결하기 어려웠던 복잡한, 개방형 작업을 자동화하는데 큰 약속을 합니다. 복잡한 문제를 전문화된 역할로 분해하고 개별 AI 에이전트의 고유한 능력을 활용함으로써, 다양한 에이전트 시스템은 이전에 상상도 못 했던 방식으로 지능적인 자동화를 조율할 수 있습니다. CrewAI, Langraph, Autogen과 같은 개척적인 프레임워크는 이 새로운 패러다임을 위한 길을 열며, 개발자가 복잡한 워크플로를 자율적으로 탐색하고 실행할 수 있는 AI 에이전트 팀을 디자인하고 배포할 수 있도록 도와주고 있습니다.\n\n그러나 이 새로운 협업 AI 영역으로 나아가면 앤젠틱 시스템의 핵심에 있는 근본적인 도전 과제를 마주하게 됩니다: 계획.\n\n<div class=\"content-ad\"></div>\n\nAI 에이전트들이 효과적으로 행동을 계획하고 서로 협력하며 동적이고 개방적인 환경에서 자신들의 전략을 적응시킬 수 있게 하는 방법은 무엇일까요?\n\n이 문서는 계획이 에이전트 AI의 핵심 과제이며 강화 학습(RL)이 이 중요한 문제에 대한 유망한 해결책을 제시한다고 주장합니다.\n\n다음 섹션에서는 에이전트 AI의 부상과 주요 원칙에 대해 탐구하고, 이러한 시스템에서 계획이 이러한 의미 있는 과제로 작용하는 이유를 설명하며, 강화 학습 기법이 이러한 어려움을 해결할 수 있는 방법을 살펴볼 것입니다.\n\n에이전트 AI에서 계획과 강화 학습의 상호작용을 이해함으로써, 지능적 자동화와 협력적인 인공지능의 미래에 대한 중요한 통찰을 얻을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Planning as the Core Challenge in Agentic AI: Solving it with Reinforcement Learning](/assets/img/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning_0.png)\n\n# Agentic AI의 부상\n\nAgentic AI는 인공지능 시스템을 개념화하고 구현하는 방식에서 패러다임이 바뀌었다. 핵심적으로, Agentic AI는 자율적인 AI 에이전트들이 복잡하고 개방적인 과제에 대처하기 위해 팀 또는 \"크루\"로 함께 일하는 모습을 상상한다. 이 접근 방식은 단일 모델 AI 시스템의 제약을 넘어서 전문화와 협력의 힘을 활용하여 더 정교하고 유연한 문제 해결 능력을 실현한다.\n\n이 Agentic AI 혁명의 전선에는 다수의 에이전트들 사이의 협력에 대한 독특한 접근 방식을 제공하는 여러 중요한 프레임워크들이 등장했다:\n\n\n<div class=\"content-ad\"></div>\n\n- CrewAI: 이 프레임워크는 특정 역할을 갖는 AI 팀을 설계할 수 있게 해 주어, 그들이 특정 작업에 따라 선별된 연구 및 분석 도구 세트를 갖추도록 돕습니다.\n- Langraph: Langraph는 더 구조화된 방식을 채택하여, 명시적인 방향 그래프를 사용하여 에이전트 간의 작업 흐름을 정의합니다. 이를 통해 개발자들은 에이전트 조정과 작업 할당에 대해 세밀한 제어를 할 수 있습니다.\n- Autogen: Autogen은 에이전트 간의 다중 대화로부터 발생하는 신생 작업 흐름에 의존하여, 보다 동적이고 적응적인 협업 패턴을 가능하게 합니다.\n\n이 프레임워크들은 구체적인 구현에서 차이가 있지만, 모두 에이전틱 AI 접근 방식을 정의하는 중심 원칙을 공유합니다:\n\n전문화와 협업: 이러한 시스템 전반에 걸쳐 두드러지는 공통점 중 하나는 다수의 특화된 에이전트를 활용하여 스스로 작업하는 방식입니다. 단일 대형 모델에 의존하는 대신, 에이전틱 AI는 작업을 하위 작업으로 분해하여 각각 다른 역할과 기술을 갖춘 에이전트에 위임합니다. 이러한 전문화는 각 에이전트가 자신의 전문 분야에 집중할 수 있도록 하고, 협업은 팀이 어떤 개별 에이전트에겐 도전적일 수 있는 문제들을 해결할 수 있도록 돕습니다.\n\n예를 들어 채용 상황에서, 크루는 기술 직군 연구, 인적 프로필 엔지니어링, 이력서 전략 및 면접 준비에 특화된 에이전트로 구성될 수 있습니다. 이러한 특화된 에이전트들이 함께 작업하여 단일 일반적 AI보다 개인을 고용 전 과정에서 효과적으로 안내할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n언어 모델과 외부 도구의 활용: 에이전트 AI 시스템에서 또 다른 중요한 패턴은 각 에이전트를 뒷받침하는 \"두뇌\"로서 대형 언어 모델(LLMs)을 사용하는 것입니다. 이러한 미리 학습된 모델은 에이전트가 열린 대화를 할 수 있게 하며 자연어 질의를 해석하고 유창한 응답을 생성하며 판단을 내릴 수 있도록 합니다.\n\n그러나 에이전트 AI는 언어 모델만을 의존하지는 않습니다. 에이전트의 지식을 기반을 다지고 그들의 능력을 확장하기 위해, 이러한 시스템은 또한 외부 도구와 데이터 소스에 연결합니다. 웹에서 단락을 검색하거나 구조화된 데이터베이스를 질의하거나 타사 API를 호출하는 등의 방식으로, 에이전트들은 실제 세계 정보를 활용하여 자신들의 결정과 행동에 영감을 얻습니다.\n\n이러한 언어적 유연성과 외부 기반의 결합으로 인해 에이전트 AI 시스템은 넓은 세계로부터 통찰을 얻으면서 일관된 대화를 유지할 수 있습니다. 이는 인간이 언어를 지식과 행동의 관문으로 활용하는 방식을 재현하는 데 필수적인 한 걸음입니다.\n\n에이전트 상태 및 워크플로우 관리: 에이전트 AI 설계의 가장 다양한 측면은 플랫폼이 에이전트 팀의 상태와 워크플로우 오케스트레이션을 어떻게 다루는지입니다. 에이전트 작업은 종종 다수의 단계와 에이전트 출력 간의 의존성을 포함하므로 일관된 전역 상태와 제어 흐름을 유지하는 것이 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n이 도전 과제에 대한 접근 방식은 플랫폼마다 다양합니다. Langraph는 워크플로우를 정의하기 위해 명시적인 방향 그래프를 사용하여 개발자에게 세밀한 제어를 제공합니다. Autogen은 에이전트 간의 멀티턴 대화에서 발생하는 신흥 워크플로에 더 의존합니다. CrewAI는 상호 작용을 안내하는 고수준 태스크 플로우를 갖추고 있지만 에이전트들이 서브태스크를 자율적으로 위임하고 응답할 수 있는 유연성을 가지고 있습니다.\n\n이러한 차이점에도 불구하고, 에이전트 상태 및 워크플로우 관리를 위한 일관된 우선순위 목록이 도출됩니다:\n\n- 에이전트가 시간이 지남에 따라 다른 에이전트들의 작업 및 결정을 발전시킬 수 있는 메커니즘 제공\n- 태스크 분할 및 에이전트 조정 패턴의 유연한 정의 가능\n- 에이전트 역할, 도구 및 위임 권한의 태스크별 맞춤화 허용\n- 예외 처리 및 에이전트 출력 간 비선형 종속성 그래프 우아하게 처리\n\n보다시피, 에이전트 AI의 부상은 유연하고 지능적인 자동화의 엄청난 잠재력을 가져옵니다. 특화, 협업, 외부 데이터를 기반으로 한 언어 모델의 강점을 활용하여 이러한 시스템은 기존의 전통적인 AI 접근 방식으로는 이루기 힘든 복잡하고 개방적인 작업에 대처할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n그러나 이러한 잠재력은 상당한 도전과 함께 옵니다. 그 중에서도 가장 중요한 것은 계획 문제입니다. 어떻게 하면 다양한 AI 에이전트 팀이 효과적으로 행동을 조정하고 불확실성 하에서 결정을 내리며, 동적 환경에서 전략을 적응할 수 있도록 할 수 있을까요? 이것이 에이전트 AI 시스템의 핵심 도전에 대한 핵심을 담고 있습니다.\n\n# 핵심 도전으로서의 계획\n\n에이전트 AI 시스템이 복잡성과 능력을 키우면 효과적인 계획의 필요성이 점점 더 중요해집니다. 이 문맥에서의 계획은 AI 에이전트들이 목표를 달성하기 위해 행동 순서를 결정하고, 다른 에이전트들과 협력하며, 변화하는 상황에 적응하는 과정을 말합니다. 계획은 지적 행동의 기본적인 측면이지만, 에이전트 AI의 영역에서 특히 어려운 도전을 제기합니다.\n\n왜 계획이 특히 복잡한가요, 특히 다중 에이전트 시나리오에서? 이러한 어려움에 기여하는 몇 가지 주요 요소가 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 고차원 상태 및 행동 공간: 에이전틱 인공지능에서 상태 공간(환경 및 에이전트의 모든 가능한 구성)과 행동 공간(에이전트가 취할 수 있는 모든 가능한 행동)은 매우 크고 복잡합니다. 이는 각각의 능력과 잠재적 행동을 갖는 여러 에이전트가 상호작용하는 경우 조합 폭발로 인한 것입니다.\n- 부분 관측성: 에이전트들은 종종 환경의 상태와 다른 에이전트의 행동에 대해 불완전한 정보를 갖습니다. 이러한 불확실성으로 인해 행동의 결과를 예측하고 효과적으로 계획하기 어려워집니다.\n- 비정상적인 환경: 다중 에이전트 시스템에서는 환경이 에이전트가 행동을 취하고 서로 상호작용함에 따라 지속적으로 변화합니다. 이러한 비정상성은 시간이 지남에 따라 작용의 효과가 일관되지 않아 계획 과정을 복잡하게 만듭니다.\n- 장기 의존성: 에이전틱 AI의 많은 작업은 단계 간에 의존성을 가진 장기적인 행동 시퀀스를 필요로 합니다. 이러한 확장된 시간 경계를 통해 계획을 수행하는 것은 계산적으로 어려우며 즉시적 보상과 장기적 목표를 균형있게 유지해야 합니다.\n- 조정 및 통신 부담: 다중 에이전트 시스템에서 효과적인 계획은 에이전트 간의 조정이 필요하며 이는 의사 결정 과정에서 추가 복잡성과 병목현상을 초래할 수 있습니다.\n\n이러한 도전에 대처하기 위해 연구자들은 에이전틱 AI의 계획 문제를 마르코프 결정 과정(MDP)으로 정의하고 있습니다. MDP는 상황에 따라 결과가 일부적으로 무작위이고 일부적으로 의사 결정자의 통제 아래 있는 상황에서 의사 결정을 모델링하기 위한 수학적인 프레임워크를 제공합니다.\n\n에이전틱 AI의 맥락에서 MDP의 구성 요소를 다음과 같이 정의할 수 있습니다:\n\n- 상태 공간 (S): 모든 가능한 사고 과정 및 환경 구성의 공간\n- 동작 공간 (A): 사고나 문서 검색의 모든 가능한 조합\n- 전이 역학 (P): 이전 사고와 행동을 기반으로 새로운 사고가 생성되는 방법\n- 보상 함수 (R): 답변의 품질이나 목표에 대한 진전을 평가하는 것\n- 할인 계수 (γ): 단기 vs. 장기적 보상의 우선순위\n- 문제 기간 (T): 허용되는 추론 단계의 최대 수\n\n<div class=\"content-ad\"></div>\n\n계획 문제를 MDP로 프레임화함으로써, 강화 학습 분야의 다양한 기술을 활용하여 AI의 계획 과제를 해결할 수 있습니다. 그러나 이 정식화는 계획 과정에서 근본적인 긴장을 강조하기도 합니다: 탐험-활용 딜레마.\n\n탐험-활용 딜레마는 새로운 것을 탐색하거나 잘 알려진 좋은 솔루션을 활용하는 사이의 교환 비용을 가리킵니다. 에이전트 AI 계획의 맥락에서 이것은 다음과 같은 균형으로 나타납니다:\n\n- 탐험: 새로운 사고 조합을 시도하거나 다양한 문서를 검색하거나 혁신적인 추론 방향을 추구하여 중요한 솔루션에 이르는 일들에 대한 베스트.\n- 활용: 이미 알려진 효과적인 전략에 초점을 맞추거나 성공적인 사고 과정을 발전시키거나 최대의 즉각적 보상을 위해 기존 솔루션을 정제하는 일에 베스트.\n\n탐험과 활용 사이의 적절한 균형을 찾는 것은 에이전트 AI 시스템에서의 효과적인 계획에 중요합니다. 탐험이 과도하면 낭비되는 컴퓨팅 자원과 일관성 없는 성능을 야기할 수 있으며, 활용이 지나치다면 최적의 솔루션을 허술하게 만들거나 새로운 상황에 적응하지 못하는 문제를 야기할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n전통적인 계획 접근 방식인 상징적 AI나 철저한 검색을 기반으로 한 방법은 종종 에이전틱 AI의 맥락에서 이러한 도전 과제를 해결하는 데 어려움을 겪는다. 이러한 방법들은 일반적으로 환경의 완전한 지식, 결정론적 행동 결과, 명확히 정의된 목표 상태에 의존하는데, 이는 에이전틱 AI가 활동하는 복잡하고 불확실하며 개방적인 도메인에서 거의 적용되지 않는 가정들이다.\n\n대신 필요한 건 유연하고 적응적인 계획 접근 방식으로, 다음과 같은 기능을 갖추어야 한다:\n\n- 고차원 상태 및 행동 공간을 효율적으로 처리\n- 부분 관찰 가능성과 불확실성 다루기\n- 비정상적인 환경에 적응하기\n- 복잡한 종속성이 있는 긴 시간 범위에 계획 수립\n- 탐색과 활용을 동적으로 균형있게 유지\n- 여러 전문화된 에이전트 간의 행동 조정\n\n여기서 강화 학습이 등장하여 에이전틱 AI 시스템이 제기하는 독특한 계획 도전 과제를 해결하기에 적합한 강력한 기법 세트를 제공한다.\n\n<div class=\"content-ad\"></div>\n\n# 강화 학습 및 고급 기술로서의 솔루션\n\n강화 학습(RL)은 에이전트형 AI에서 복잡한 계획 도전에 대처하는 유망한 접근법으로 부각되었습니다. RL은 에이전트가 환경과 상호 작용하면서 보상이나 처벌의 형태로 피드백을 받아 결정을 내리는 방식의 머신러닝 유형입니다.\n\n이러한 학습 패러다임은 에이전트형 AI에서 계획 문제에 특히 적합한 이유가 여럿 있습니다:\n\n- 경험으로부터 학습: RL 에이전트들은 환경의 완전한 모델을 요구하지 않고 시행착오를 통해 최적의 전략을 학습할 수 있습니다. 이는 에이전트형 AI가 작동하는 복잡한, 부분 관측 가능한 도메인에서 중요합니다.\n- 탐험과 이용 사이의 균형 유지: RL 알고리즘에는 탐사-이용 교환을 관리하는 내장 기구가 있어, 에이전트들이 새로운 전략을 발견하면서도 알려진 좋은 해결책을 활용할 수 있게 합니다.\n- 불확실성 다루기: RL 방법은 확률적 환경에서 작동하도록 설계되어, 다중 에이전트 시스템에 내재된 불확실성에 탄력적으로 대처할 수 있습니다.\n- 장기 계획: 많은 RL 알고리즘은 명시적으로 장기 보상을 최적화하기 위해 설계되어 있어, 긴 시간 대역으로 계획을 수립하고 행위 간의 복잡한 종속성을 포착할 수 있습니다.\n- 적응성: RL 에이전트들은 새로운 경험을 기반으로 전략을 지속적으로 업데이트할 수 있어, 변동성 있는 환경에 적합합니다.\n\n<div class=\"content-ad\"></div>\n\n특히 계획 도전 과제를 해결하는 데 유망한 강화 학습 기법 중 하나는 몬테 카를로 트리 탐색(Monte Carlo Tree Search, MCTS)입니다. MCTS는 휴리스틱 탐색 알고리즘이며, 랜덤 샘플링과 트리 탐색을 결합하여 복잡한 공간에서 결정을 내립니다. 이 기법은 다양한 분야에 성공적으로 적용되었으며, 알파고(AlphaGo)와 같은 게임 플레이 인공지능에서 사용되었습니다.\n\n에이전틱 인공지능 계획의 맥락에서, MCTS는 가능한 사고 과정과 행동 시퀀스의 광범위한 공간을 효율적으로 탐색하는 데 사용될 수 있습니다. MCTS의 주요 단계는 다음과 같습니다:\n\n- 선택(Selection): 루트에서 시작하여 탐색하는 동안 탐색과 활용을 균형있게 고려하는 트리 정책(예: 상한 신뢰 경계)을 사용합니다.\n- 확장(Expansion): 새로운 자식 노드를 추가하여 트리를 확장합니다.\n- 시뮬레이션(Simulation): 새 노드에서 랜덤 시뮬레이션을 실행하여 값을 추정합니다.\n- 역전파(Backpropagation): 루트에 되돌아가는 경로를 따라 노드 통계를 업데이트합니다.\n\n이러한 단계를 반복적으로 적용함으로써, MCTS는 검색 공간의 가장 유망한 지역에 계산 리소스를 집중할 수 있어 에이전틱 인공지능 계획에서 마주치는 고차원 상태 및 행동 공간에 적합합니다.\n\n<div class=\"content-ad\"></div>\n\n다른 중요한 강화 학습 개념 중 하나는 에이전틱 AI 계획에 적용할 수 있는 Q-러닝입니다. Q-러닝은 모델이 없는 강화 학습 알고리즘으로, 주어진 상태에서 특정 행동을 취했을 때 기대되는 누적 보상(Q-값)을 추정하는 방법을 학습합니다. 에이전틱 AI 환경에서는 Q-러닝을 사용하여 다양한 사고 과정이나 문서 검색의 가치를 추정할 수 있습니다.\n\n이 분야의 최근 발전은 이러한 기본적인 강화 학습 개념을 바탕으로 한 몇 가지 혁신적인 접근 방식의 발전을 이끌어내어, 에이전틱 AI 시스템에서 계획 및 추론의 특정 도전 과제를 해결하기 위한 기술적 발전을 이끌어내고 있습니다.\n\n또한 특히 유망한 세 가지 혁신적 기술을 살펴보겠습니다:\n\n## Q*: 딥러닝 계획을 통한 다단계 추론 개선\n\n<div class=\"content-ad\"></div>\n\nWang 및 다른 사람(2024)이 소개한 Q* 프레임워크는 대형 언어 모델(Large Language Models, LLMs)의 다단계 추론 능력을 향상시키는 데 중요한 발전을 이끌어냅니다. Q*는 A* 검색의 능력을 결합하여 학습된 Q-value 모델로 LLMs을 복잡한 추론 작업 중에서 가장 유망한 다음 단계를 선택하도록 안내합니다.\n\nQ*의 주요 특징은 다음과 같습니다:\n\n- 각 노드가 주어진 문제에 대한 부분 솔루션을 나타내는 그래프로 추론 프로세스를 모델링합니다.\n- A* 검색을 위한 학습된 Q-value 모델을 휴리스틱 함수로 사용하여, 전체 문제를 해결하는 데 각 잠재적인 다음 단계가 얼마나 유망한지를 추정합니다.\n- 가능한 추론 경로의 방대한 공간을 효율적으로 탐색하기 위해 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)을 사용합니다.\n- LLM이 자체 정제된 답변에 대한 점수를 매기는 자가 평가 메커니즘을 통합하여 추론 프로세스를 지속적으로 향상시킬 수 있습니다.\n\nQ* 프레임워크는 에이전트 AI 계획에서 몇 가지 중요한 도전에 대처합니다:\n\n<div class=\"content-ad\"></div>\n\n- 긴 콘텍스트 처리: Q*는 전통적인 LLM의 고정된 콘텍스트 창의 제약을 극복하기 위해 지식 원본에서 대규모 문서 배치를 처리할 수 있습니다.\n- 무관한 정보에 대한 견고성: 다양한 추론 분기를 탐색함으로써 Q*는 실패한 정보 검색 및 오도된 문서에 대해 저항력을 갖습니다.\n- 적응성: 이 프레임워크는 기본 LLM의 작업별 특정 조정 없이 다양한 추론 작업에 적용할 수 있습니다.\n\n실험 결과에 따르면 Q*는 다양한 수학적 추론 및 코드 생성 작업에서 기준선 방법을 크게 앞섰으며, 지식 기반 AI 시스템의 계획 및 추론 능력을 향상시킬 잠재력을 입증했습니다.\n\n## 병렬 함수 호출을 위한 LLM 컴파일러\n\nQ*가 추론 프로세스 자체를 개선하는 데 초점을 맞추고 있을 때, LLM 컴파일러 접근 방식은 에이전틱 AI 계획의 또 다른 중요한 측면을 다룹니다: 병렬 함수 호출의 효율적인 조율. 이 기법은 고전 컴파일러 설계에서 영감을 받아 대규모 언어 모델에서 여러 함수 호출을 실행을 최적화하는 것을 목표로 합니다.\n\n<div class=\"content-ad\"></div>\n\nLLM 컴파일러 방식의 주요 측면은 다음과 같습니다:\n\n- 사용자 입력을 상호 의존성을 가진 일련의 작업으로 자동 분해합니다.\n- 독립적인 작업을 병렬로 실행하여 복잡한 워크플로에서 발생하는 지연 시간을 크게 감소시킵니다.\n- 작업의 방향성 비순환 그래프(DAG)를 생성하는 계획 단계를 통해 효율적인 일정 계획 및 실행이 가능합니다.\n- 외부 도구 및 API와 통합하여 LLM의 능력을 언어 처리 이상으로 확장합니다.\n\nLLM 컴파일러는 공별한 AI 계획에서 여러 중요한 도전 과제를 다룹니다:\n\n- 효율성: 병렬화 가능한 패턴을 식별하고 함수 호출 의존성을 관리함으로써, 컴파일러는 복잡한 작업의 지연 시간을 크게 줄일 수 있습니다.\n- 확장성: 이 방식은 다수의 함수 호출과 데이터 의존성이 포함된 대규모 및 복잡한 작업을 다루도록 설계되었습니다.\n- 유연성: 컴파일러는 다양한 종류의 LLM 및 작업 부하에 적응할 수 있어 다양한 AI 응용 프로그램에 대한 다재다능한 도구가 됩니다.\n\n<div class=\"content-ad\"></div>\n\n초기 결과에 따르면, LLM 컴파일러는 순차 실행 방법과 비교하여 상당한 속도 향상을 달성할 수 있다는 것이 밝혀졌습니다. 최대 3.7배의 대기 시간 개선과 일부 작업에서 최대 6.7배까지의 비용 절감이 가능합니다.\n\n## 수학 올림피아드 솔루션을 위한 몬테카를로 트리 자기 수정\n\n타 분야에서 MCTS의 성공을 바탕으로, 연구자들은 복잡한 수리 추론 작업, 특히 수학 올림피아드에서 마주하는 작업들을 처리하기 위해 특별히 개발된 몬테카를로 트리 자기 수정(MCTSr) 알고리즘을 개발했습니다.\n\nMCTSr의 주요 기능은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- 대규모 언어 모델과 몬테카를로 트리 탐색을 통합하여 문제 해결 능력을 향상시킵니다.\n- 선택, 자가 세부화, 자가 평가 및 역전파 단계를 포함하는 반복적인 과정입니다.\n- 모델이 솔루션을 반복적으로 향상시킬 수 있는 피드백 안내형 세분화 과정입니다.\n- 진정으로 개선된 솔루션이 높은 점수를 받도록 하는 엄격하고 비판적인 점수 매커니즘입니다.\n\nMCTSr은 수학적 추론과 계획에서 여러 가지 도전에 대응합니다:\n\n- 복잡한 다단계 문제 다루기: 이 알고리즘은 다단계 추론 단계와 전략적 사고가 필요한 복잡한 수학적 작업을 다루도록 설계되었습니다.\n- 지속적인 개선: 자가 세부화 및 자가 평가 메커니즘을 통해 MCTSr은 솔루션 품질을 점진적으로 향상시킬 수 있습니다.\n- 다양한 문제 유형에 대한 적응성: 이 프레임워크는 초등학교 산술부터 올림피아드 수준의 도전 과제까지 다양한 수학 영역에서 성공을 거두었습니다.\n\n실험 결과에서는 MCTSr이 LLaMA-3 8B와 같은 훨씬 작은 모델을 사용하여 수학 올림피아드 문제에서 GPT-4 수준의 성능을 달성할 수 있다는 것을 입증하였으며, 이는 인공지능 시스템의 추론 능력을 혁신적으로 향상시킬 잠재력을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n이 세 가지 접근법인 Q*, LLM Compiler 및 MCTSr은 에이전틱 AI의 계획 및 추론 기술의 최신 동향을 대표합니다. 이러한 방법들은 강화 학습 원칙과 혁신적인 탐색 및 최적화 전략을 결합하여 AI 주도 문제 해결에서 가능한 범위를 넓히고 있습니다.\n\n그러나 이러한 고급 기술을 에이전틱 AI 계획에 적용하는 데는 다음과 같은 도전 과제가 있습니다:\n\n- 계산 복잡성: 이러한 방법들은 대부분 고도의 계산 과정을 통합하며, 대규모 응용 프로그램에는 리소스가 많이 필요할 수 있습니다.\n- 탐험과 활용의 균형: 새로운 솔루션을 발견하고 기존의 좋은 전략을 활용하는 적절한 균형을 찾는 것은 여전히 까다로운 작업입니다.\n- 해석 가능성: 이러한 시스템이 더 복잡해지면서 의사 결정 과정에서의 투명성과 해석 가능성을 보장하는 것이 점점 어려워지고 있습니다.\n- 일반화: 이러한 방법은 특정 도메인에서 인상적인 결과를 보여주었지만, 다양한 작업 유형 간의 일반화 능력을 평가하기 위해 추가 연구가 필요합니다.","ogImage":{"url":"/assets/img/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning_0.png"},"coverImage":"/assets/img/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning_0.png","tag":["Tech"],"readingTime":12},{"title":"LLM 출력 구조화하는 방법 안내","description":"","date":"2024-06-23 19:40","slug":"2024-06-23-GuidinganLLMsResponsetoCreateStructuredOutput","content":"\n\n![2024-06-23-GuidinganLLMsResponsetoCreateStructuredOutput_0.png](/assets/img/2024-06-23-GuidinganLLMsResponsetoCreateStructuredOutput_0.png)\n\n이 기사는 Python에서 유효성 검사 라이브러리를 사용하여 GPT-4 또는 Llama 3와 같은 LLM 응답을 구조화하는 방법을 가르쳐줍니다.\n\nJSON 형식에서 구조화된 정보를 추출해야 하는 필요성은 매우 중요한 주제이며, 이것은 데이터 마이닝 작업에서 정확한 정보를 비구조적 형식(예: 자유 텍스트)에서 추출하는 데 기본적입니다.\n\n또한, LLM의 출력 토큰을 생성하는 과정에서 발생하는 확률적 특성으로 인해 GPT와 같은 상업용 시스템에서도 구조화된 응답 형식이 신뢰할 수 없습니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 유효성 검사와 스키마 모델링을 위해 Pydantic와 Instructor와 같은 여러 라이브러리를 사용할 것이고, LLM 부분에는 OpenAI와 ollama를 활용할 것입니다. 제안된 내용은 OpenAI나 Anthropic과 같은 폐쇄 소스 모델뿐만 아니라 Llama 3와 같은 오픈 소스 모델에 대해서도 유효합니다.\n\n본 기사를 통해 아래 내용을 배울 수 있습니다:\n\n- 데이터 모델을 정의하는 방법과 그것이 무엇인지\n- LLM이 출력 형식을 준수하는지를 유효성 규칙을 통해 확인하는 방법\n- Instructor와 Pydantic 라이브러리를 사용하는 방법\n\n즐거운 독해 되세요!\n\n<div class=\"content-ad\"></div>\n\n# 구조화된 출력이 필요한 이유\n\nGPT-4와 같은 LLM은 특정 패턴을 따르지 않고도 상당한 가치를 제공할 수 있습니다. 하지만 데이터를 다루는 프로그래머들에게는 사용자의 의지에 따라 가능한 출력 패턴을 준수하는 것이 중요합니다.\n\nGPT-3.5의 특정 버전부터 OpenAI는 완성 API에 response_format 매개변수를 추가했습니다. 이를 통해 사용자는 json_object와 같은 다른 키를 정의하여 모델을 입력한 프롬프트에 더 적합한 응답 방향으로 안내할 수 있습니다.\n\n다음은 예시입니다:\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=\"gpt-3.5-turbo-0125\",\n  response_format={ \"type\": \"json_object\" },\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n  ]\n)\nprint(response.choices[0].message.content)\n\n>>> \"content\": \"{\\\"winner\\\": \\\"Los Angeles Dodgers\\\"}\"\n```\n\n하지만 이러한 로직이 항상 작동하는 것은 아닙니다. 실제로 OpenAI의 문서에서는 GPT가 이를 생성하는 데 도움을 주기 위해 프롬프트에 \"JSON\"이라는 단어를 명확하게 작성할 것을 제안합니다. 이는 \"response_format={ \"type\": \"json_object\" }\"를 사용할 때 프롬프트 어딘가에 이를 작성해야만 하는 중요한 팁이기 때문에 강제적으로 작성해야 합니다.\n\n## LLM이 일관된 JSON 출력을 생성하기 어려운 이유는 무엇인가요?\n\nLLM은 입력 프롬프트가 주어졌을 때 이전 토큰 다음에 더 많이 나올 가능성이 있는 다음 토큰을 반환하는 기계로서의 역할을 합니다. 실제로 이러한 형식을 보고 이해하려면 모델이 훈련 단계에서 명시적으로 이러한 형식을 보고 이해하기 위해 안내받아야만 하므로 이러한 패턴을 \"자연\"에서 만나기 어렵습니다.\n\n\n<div class=\"content-ad\"></div>\n\n최신 LLM의 JSON 모드는 출력이 특정 패턴과 일치한다고 보장하지 않습니다. 단지 유효하고 오류 없이 파싱된다는 것만을 보장합니다.\n\n따라서 이러한 출력물 안에 무엇이 포함되어 있는지를 유효성 검사할 수 있고, 데이터 모델과 일치하지 않는 경우 예외와 오류를 발생시키는 것이 중요합니다.\n\n# 사용 사례\n\n우리는 GPT-4 또는 Llama3와 같은 LLM에 간단한 질문에서 시작하여 JSON에서 정보를 추출하는 예제를 살펴볼 것입니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 무엇이든 물어볼 수 있지만, 모델에게 시간이 지남에 따른 축구 월드컵 우승팀에 관한 질문을 하려고 합니다.\n\n특히 우리는 다음을 추출하고 싶습니다.\n\n- 결승 일자\n- 대회의 개최 국가\n- 우승 팀\n- 최다 득점자\n\n우리는 데이터의 정확성을 확인하는 것이 아니라, LLM의 문장 응답을 다음으로 보여줄 스키마에 맞추는 것에만 신경을 쓸 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 이 예제를 살펴보고 다른 것들도 살펴볼 수 있을 것 같아요.\n\n## 필수 종속성\n\n이제 이 튜토리얼을 실행하기 위해 설치해야 할 종속성을 살펴봅시다.\n\n당연히, 이미 활성화된 개발 환경이 있다고 가정하고 Pydantic, Instructor, OpenAI 클라이언트 및 ollama를 설치할 거예요.\n\n<div class=\"content-ad\"></div>\n\n- Pydantic: 커뮤니티에서 널리 사용되는 데이터 모델 정의 및 유효성 검사 라이브러리로, 사용 편의성, 효율성 및 데이터 과학에서의 중요성으로 유명합니다.\n- Instructor: LLMs와 작업하기 위해 특별히 제작된 Pydantic을 감싸는 래퍼로, 유효성 검사 로직을 생성할 수 있는 라이브러리입니다.\n- OpenAI: GPT와 다른 OpenAI 모델에 쿼리를 요청하기 위한 유명한 클라이언트입니다.\n- ollama: llama3와 같은 오픈 소스 LLM에 대한 매우 편리한 인터페이스입니다.\n\n개발 환경에서는 다음 명령어를 사용하여 시작합니다.\n\n```bash\npip install pydantic instructor openai ollama\n```\n\n오픈 소스 모델을 테스트하고자 하기 때문에 다음 단계는 ollama를 시스템 전역에 설치하는 것입니다. ollama의 설치 및 사용 방법은 이 특별한 기사에서 읽어보실 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 개발에 집중할 수 있겠네요.\n\n## 데이터 모델 정의\n\n데이터 모델은 데이터를 구조화하기 위해 따를 논리적인 패턴입니다. 데이터베이스의 테이블을 정의하는 것부터 입력 데이터를 유효성 검사하는 데까지 여러 맥락에서 사용됩니다.\n\n아래 포스트에서 Pydantic을 활용한 데이터 과학과 머신러닝에서의 데이터 모델링에 대해 이미 약간 다룬 적이 있습니다 👇\n\n<div class=\"content-ad\"></div>\n\n파이던틱 데이터 모델을 만들어 보면 좋겠어요:\n\n```js\nfrom pydantic import BaseModel, Field\nfrom typing import List\nimport datetime\n\nclass SoccerData(BaseModel):\n    date_of_final: datetime.date = Field(..., description=\"최종 이벤트 날짜\")\n    hosting_country: str = Field(..., description=\"대회를 개최하는 국가\")\n    winner: str = Field(..., description=\"최종 경기에서 우승한 축구팀\")\n    top_scorers: list = Field(\n        ..., description=\"대회의 상위 3명 스코어러 목록\"\n    )\n\nclass SoccerDataset(BaseModel):\n    reports: List[SoccerData] = []\n```\n\n이 스크립트에서는 Pydantic에서 BaseModel 및 Field 클래스를 가져와 데이터 모델을 만드는 작업을 시작합니다. 사실, 최종 결과가 가져야 할 구조를 만들고 있습니다.\n\nPydantic은 모델에 들어가는 데이터 유형을 선언해야 합니다. 예를 들어 datetime.date는 날짜 필드가 문자열이 아니라 날짜여야 함을 강제합니다. 동시에 top_scorers 필드는 반드시 목록이어야 하며, 그렇지 않으면 Pydantic이 유효성 검사 오류를 반환할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n마침내, 여러 인스턴스를 수집하는 데이터 모델을 만들었습니다. 이것은 SoccerData 모델의 모음을 수집하는 SoccerDataset이라고 합니다. 이 모델은 한 개 이상의 보고서가 있는지 확인하기 위해 강사에 의해 사용될 것입니다.\n\n# 시스템 프롬프트 생성\n\n매우 간단히, 모델이 수행해야 하는 작업을 영어로 적어봅시다. 예를 통해 결과의 의도와 구조를 강조하면서 설명합니다.\n\n```js\nsystem_prompt = \"\"\"당신은 숙련된 스포츠 기자입니다. 특정 연도의 축구 월드컵에서 우승한 팀에 대한 작은 리포트를 작성할 것입니다. 대회 결승전 날짜, 대회 전체에서 상위 3 스코어러, 우승 팀, 그리고 대회를 주최한 국가를 보고합니다. 다음 필드를 포함하는 JSON 객체를 반환하세요: date_of_final, hosting_country, winner, top_scorers.\\\n \n만약 다수 연도가 입력되면, 보고서를 쉼표로 구분하세요.\\\n \n다음은 예시입니다.\n [\n    {\n        \"date_of_final\": \"1966\",\n        \"hosting_country\": \"England\",\n        \"winner\": \"England\",\n        \"top_scorers\": [\"Player A\", \"Player B\", \"Player C\"]\n    },\n    {\n        \"date_of_final\": ...\n        \"hosting_country\": ...\n        \"winner\": ...\n        \"top_scorers\": ...\n    },\n\n]\n\n다음 연도들에 대해 보고해야 할 것입니다:\n\n \"\"\"\n```\n\n<div class=\"content-ad\"></div>\n\n시스템 프롬프트로 사용되며 단순히 쉼표로 구분된 연도를 전달할 수 있습니다.\n\n# 강사 코드 생성\n\n여기서는 Instructor를 사용하여 JSON 유효성 검사 및 구조화의 주요 로직을 만들 것입니다. 이를 통해 GPT를 API를 통해 호출하는 OpenAI에서 제공하는 인터페이스와 유사한 인터페이스를 사용합니다.\n\n먼저 우리는 query_gpt라는 함수를 사용하여 OpenAI를 사용하여 프롬프트를 매개변수화할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom openai import OpenAI\nimport instructor\n\ndef query_gpt(prompt: str) -> list:\n    client = instructor.from_openai(OpenAI(api_key=\"...\"))\n    resp = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=SoccerDataset,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    )\n    return resp.model_dump_json(indent=4)\n```\n\nOpenAI API 키를 새롭게 생성된 클라이언트에 전달하는 것을 잊지 말자. 우리는 GPT-3.5-Turbo를 사용하고, 응답 모델로 SoccerDataset을 전달할 것이다. 또한, 이 기사를 작성하는 시점에서 가장 강력한 모델인 \"gpt-4o\"를 사용할 수도 있다.\n\n모든 것을 함께 조합하여 소프트웨어를 실행해 보자. 사용자 프롬프트로 입력할 내용으로 \"2010, 2014 및 2018\"년을 내용으로 전달하여 구조화된 보고서를 생성하고자 한다. \n\n```js\nfrom openai import OpenAI\nimport instructor\n\nfrom typing import List\nfrom pydantic import BaseModel, Field\nimport datetime\n\n\nclass SoccerData(BaseModel):\n    date_of_final: datetime.date = Field(..., description=\"최종 이벤트의 날짜\")\n    hosting_country: str = Field(..., description=\"대회를 주최하는 나라\")\n    winner: str = Field(..., description=\"최종 경기에서 승리한 축구팀\")\n    top_scorers: list = Field(\n        ..., description=\"대회의 상위 3명의 득점수 리스트\"\n    )\n\n\nclass SoccerDataset(BaseModel):\n    reports: List[SoccerData] = []\n\n\nsystem_prompt = \"\"\"당신은 전문 스포츠 기자입니다. 특정 연도의 축구 월드컵에서 승자를 작은 보고서로 작성해야 합니다.\n대회 최종일, 대회의 전체 득점수 상위 3명, 우승 팀 및 대회를 개최하는 국가를 보고해야 합니다.\n다음 필드를 포함한 JSON 객체를 반환하세요: date_of_final, hosting_country, winner, top_scorers.\n\n쿼리가 유효하지 않은 경우 빈 보고서를 반환하세요.\n\n여러 연도가 입력된 경우 보고서를 쉼표로 구분하세요.\n\n예시입니다\n[\n    {\n        \"date_of_final\": \"1966\",\n        \"hosting_country\": \"England\",\n        \"winner\": \"England\",\n        \"top_scorers\": [\"Player A\", \"Player B\", \"Player C\"]\n    },\n    {\n        \"date_of_final\": ...\n        \"hosting_country\": ...\n        \"winner\": ...\n        \"top_scorers\": ...\n    },\n\n]\n\n다음 보고가 필요한 연도입니다:\n\n\"\"\"\n\ndef query_gpt(prompt: str) -> list:\n    client = instructor.from_openai(OpenAI())\n    resp = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=SoccerDataset,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    )\n    return resp.model_dump_json(indent=4)\n\nif __name__ == \"__main__\":\n  resp = query_llm(\"2010, 2014, 2018\")\n  print(resp)\n```\n\n<div class=\"content-ad\"></div>\n\n위 문구를 Markdown 형식으로 변환한 결과입니다:\n\n```json\n{\n    \"reports\": [\n        {\n            \"date_of_final\": \"2010-07-11\",\n            \"hosting_country\": \"South Africa\",\n            \"winner\": \"Spain\",\n            \"top_scorers\": [\n                \"Thomas Müller\",\n                \"David Villa\",\n                \"Wesley Sneijder\"\n            ]\n        },\n        {\n            \"date_of_final\": \"2014-07-13\",\n            \"hosting_country\": \"Brazil\",\n            \"winner\": \"Germany\",\n            \"top_scorers\": [\n                \"James Rodríguez\",\n                \"Thomas Müller\",\n                \"Neymar\"\n            ]\n        },\n        {\n            \"date_of_final\": \"2018-07-15\",\n            \"hosting_country\": \"Russia\",\n            \"winner\": \"France\",\n            \"top_scorers\": [\n                \"Harry Kane\",\n                \"Antoine Griezmann\",\n                \"Romelu Lukaku\"\n            ]\n        }\n    ]\n}\n```\n\n멋지네요. GPT-3.5-Turbo가 우리의 지시를 완벽하게 따르고, Instructor가 데이터 모델과 일치하는 구조를 만들어내었습니다. 실제로 이 결과는 GPT와 같은 대형 언어 모델이 일반적으로 반환하는 문자열이 아니라, 파이썬 사전의 리스트입니다.\n\n이제 이상한 입력을 넣어보려고 해봅시다.\n\n<div class=\"content-ad\"></div>\n\n```js\nif __name__ == \"__main()\":\n      print(query_gpt(\"안녕, 어떻게 지내?\"))\n\n>>>\n{\n \"리포트\": []\n}\n```\n\nLLM은 시스템 프롬프트를 통해 잘못된 쿼리를 처리하는 방법을 요청했기 때문에 올바르게 비어있는 리포트를 반환합니다.\n\n# Instructor와 함께 오픈 소스 템플릿 사용\n\nInstructor를 사용하여 GPT를 어떻게 사용하여 구조화된 JSON 출력을 얻는지 알아보았습니다. 이제 llama3와 같은 오픈 소스 템플릿을 사용하는 방법을 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n새로운 함수인 query_llama을 생성해 봅시다.\n\n```js\ndef query_llama(prompt: str) -> list:\n    client = instructor.from_openai(\n        OpenAI(\n            base_url=\"http://localhost:11434/v1\",\n            api_key=\"ollama\",  # 요청은 필요하지만 영향을 미치지 않습니다\n        ),\n        mode=instructor.Mode.JSON,\n    )\n    resp = client.chat.completions.create(\n        model=\"llama3\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ],\n        response_model=SoccerDataset,\n    )\n    return resp.model_dump_json(indent=4)\n```\n\nGPT 코드와 약간의 차이가 있습니다. 함께 살펴보겠습니다.\n\n- ollama는 GPT와 동일한 인터페이스를 통해 호출되지만, 기본 URL 포인터(base_url) 및 필수적이지만 올바른 작동에 필요하지 않은 API 키를 변경합니다(왜냐면 모르겠어요)\n- JSON 모드를 mode 매개변수를 통해 설명해야 합니다.\n새로운 함수를 실행해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n함수를 실행해 봅시다.\n\n```js\r\nif __name__ == \"__main__\":\n    print(query_llama(\"2010, 2014, 2018\"))\r\n```\n\n그리고 여기에 결과가 있습니다:\n\n```js\r\n{\n    \"reports\": [\n        {\n            \"date_of_final\": \"2010-07-11\",\n            \"hosting_country\": \"South Africa\",\n            \"winner\": \"Spain\",\n            \"top_scorers\": [\n                \"Thomas Müller\",\n                \"Wolfram Toloi\",\n                \"Landon Donovan\"\n            ]\n        },\n        {\n            \"date_of_final\": \"2014-07-13\",\n            \"hosting_country\": \"Brazil\",\n            \"winner\": \"Germany\",\n            \"top_scorers\": [\n                \"James Rodríguez\",\n                \"Miroslav Klose\",\n                \"Thomas Müller\"\n            ]\n        },\n        {\n            \"date_of_final\": \"2018-07-15\",\n            \"hosting_country\": \"Russia\",\n            \"winner\": \"France\",\n            \"top_scorers\": [\n                \"Harry Kane\",\n                \"Kylian Mbappé\",\n                \"Antoine Griezmann\"\n            ]\n        }\n    ]\n}\n```\n\n<div class=\"content-ad\"></div>\n\n우리는 올바른 JSON이 있는 목록을 가지고 있어요! 이 모든 것은 Llama 3로 로컬에서 이루어져요.\n\n이전에 말한대로, 유효성 검사는 구조를 기반으로 하고 있어요. 실제로, 이 내용은 GPT에서 생성된 내용과 다를 수 있어요.\n\n어떻게 마커들이 다른지 살펴봅시다. 아마도 우리가 받고 싶은 마커들을 명확히 지정하면 올바른 목록을 얻을 수도 있겠죠.\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\nPydantic, Instructors, 그리고 ollama를 사용하여 LLM의 출력을 JSON과 같은 구조화된 형식으로 변환하는 방법을 살펴봤습니다.\n\n이 과정에서 모델이 실제로 지도되므로 결정론적이지 않습니다. JSON이 LLM의 결정론적이지 않은 성질로 인해 준수되지 않을 수 있는 경우가 있을 것입니다.","ogImage":{"url":"/assets/img/2024-06-23-GuidinganLLMsResponsetoCreateStructuredOutput_0.png"},"coverImage":"/assets/img/2024-06-23-GuidinganLLMsResponsetoCreateStructuredOutput_0.png","tag":["Tech"],"readingTime":12},{"title":"검색 증강 생성RAG을 최적화하는 4가지 전략","description":"","date":"2024-06-23 19:38","slug":"2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG","content":"\n\n# 개인 데이터 및 개인 인프라를 활용한 고급 AI 솔루션\n\n![image](/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_0.png)\n\n머신 러닝 모델을 최적화하기 위해 권장 사항을 몇 번이나 따라보았는데, 여전히 특정 요구 사항과 잘 맞지 않는 솔루션에 부딪힌 적이 몇 번이나 있나요? 대답을 알아요: 많은, 아니면 모든 경우에 해당할 거예요. 모든 것이 데이터에 달려 있기 때문이죠. 특정 상황에 가장 적합한 방법을 찾을 때까지 테스트하고 실패하고 또 다시 테스트해야만 해요. 이 기사에서는 고급 AI 솔루션을 위해 개인 데이터와 개인 인프라를 활용하여 검색 증강 생성(Retrieval-Augmented Generation, RAG)을 최적화하는 네 가지 전략을 제시합니다.\n\n이전 기사에서는 LLaMA 3 같은 공개 모델에 개인 지식을 포함시키기 위해 검색 증강 생성(RAG) 전략을 활용하는 방법을 설명했습니다. RAG를 사용하면 개인 데이터를 개인 인프라에 저장하면서도 민감한 정보를 다른 사람들과 공유하지 않고 사용할 수 있습니다. RAG를 활용하는 장점은 명백하지만, 구현에는 여러 부분에서 중요한 조정이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n## RAG Recap\n\n저는 이전 글에서 설명한 RAG에 대한 간단한 개요부터 시작하겠습니다. 두 가지 주요 프로세스가 있습니다. 첫 번째는 \"데이터 수집 프로세스\"로, 다양한 소스에서 데이터를 수집하여 텍스트로 변환한 후 작은, 일관된 및 의미론적으로 관련 있는 부분으로 분할하고 결과를 벡터 데이터베이스에 저장합니다. 두 번째는 \"추론 프로세스\"로, 사용자 쿼리로 시작하여 첫 번째 프로세스의 결과를 사용하여 관련 데이터 부분을 식별하고 마지막으로 모델의 컨텍스트를 풍부하게하여 출력을 얻습니다.\n\n다음 다이어그램에서 두 프로세스의 자세한 내용을 볼 수 있습니다:\n\n![다이어그램](/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_1.png)\n\n<div class=\"content-ad\"></div>\n\n지난 기사에서는 ColdF 라는 가상의 회사에서 데이터를 사용하여 \"데이터 수집\" 및 \"추론\" 프로세스를 생성했습니다. 이 기사에서는 이러한 프로세스의 결과를 평가하고 최적화하는 기본적인 방법을 설명하겠습니다.\n\n## 개선 구성 요소\n\n먼저, 우리의 RAG 프로세스에서 중요한 지점을 식별하는 것부터 시작해봅시다:\n\n- 청킹 접근: 의미 있고 맥락적으로 관련 있는 데이터 세그먼트를 보장하도록 청킹 크기를 최적화합니다.\n- 임베딩 모델: 의미 표현을 개선하기 위해 모델 선택 및 세부 조정을 수행합니다.\n- 벡터 검색 방법: 효과적인 유사성 측정 및 검색 매개변수 선택합니다.\n- 모델에 피드할 최종 프롬프트: 효율적인 결과 품질을 개선하기 위해 효과적인 프롬프트를 작성합니다.\n\n<div class=\"content-ad\"></div>\n\n## RAG 파이프라인에서의 A/B 테스팅\n\n각 개선 구성 요소를 식별한 후, 전략은 각 구성 요소의 두 가지 다른 구성을 가진 두 버전을 비교하여 어떤 것이 더 나은 성능을 발휘하는지 결정합니다. 이는 두 버전을 실행하고 미리 정의된 지표에 대한 성능을 측정하는 것을 포함합니다. 하지만 성능을 어떻게 측정할까요? 그리고 어떤 지표를 사용해야 할까요? 이에 대한 답변으로 우리는 “RAGAS: 자동화된 검색 증강 생성 평가”¹ 논문을 사용합니다. 이 논문은 세 가지 주요 지표를 제안합니다:\n\n- 충실도: 답변에 있는 정보가 문맥에 제공된 정보와 일치하는지 확인합니다. 답변이 충실하다면 그 안에 있는 모든 내용이 문맥에서 직접 찾거나 추론할 수 있습니다. 예를 들어, 문맥이 “5월에 리스본을 방문했을 때, 앨리스와 나는 알파마, 바이로 알토, 벨렘 타워 그리고 다른 여러 곳으로 갔다”이고 답변이 “5월에 앨리스는 알파마, 바이로 알토, 벨렘 타워 그리고 다른 여러 곳으로 갔다”라면 문맥은 추출된 모든 내용을 지지하므로 충실도 점수는 100%입니다. 그러나 답변이 “5월에 앨리스는 알파마와 상 호르헤 성에 갔다”라면 답변에서 추출된 두 내용 중 (예: “앨리스는 알파마로 갔다” 및 “앨리스는 상 호르헤 성에 갔다”) 하나만이 문맥에서 지지되므로 충실도 점수는 50%입니다.\r\n- 답변 관련성: 생성된 답변이 완전하고 직접적으로 질문에 대답하는지 확인합니다. 정보가 정확하든 아니든 관련성이 있습니다. 예를 들어, 질문이 “포르투갈의 수도는 무엇인가?”이고 답변이 “리스본은 포르투갈의 수도입니다”라면 이 답변은 질문에 직접 대답하므로 관련성이 있습니다. 답변이 “리스본은 다양한 명소가 많은 아름다운 도시입니다”라면 부분적으로 관련성이 있을 수 있지만 질문에 대답하는 데 직접 필요한 정보가 아닌 추가 정보가 포함되어 있습니다. 이 지표는 답변이 집중되고 핵심을 유지하도록 보장합니다.\r\n- 문맥 관련성: 문맥에서 제공된 정보가 질문에 대답하는 데 얼마나 도움이 되는지 확인합니다. 필요한 것만 포함되고 불필요한 관련 없는 정보는 제거되므로 질문에 직접적으로 도움이 되지 않는 부가 정보를 제거합니다. 예를 들어, 질문이 “5월에 앨리스가 리스본에서 어떤 장소를 방문했나요?”이고 문맥이 “5월에 리스본을 방문했을 때, 앨리스는 알파마, 바이로 알토, 벨렘 타워 그리고 다른 여러 곳으로 갔다”이면 이 문맥은 앨리스가 5월에 어떤 장소를 방문했는지에 대한 필수 정보만을 제공하므로 매우 관련성이 높습니다. 그러나 문맥이 “5월에 리스본을 방문했을 때, 앨리스는 많은 흥미로운 사람을 만나 맛있는 음식을 먹었으며 다양한 장소를 갔다”이면 이 문맥은 질문에 대답하는 데 필요하지 않은 부가 정보를 포함하고 있으므로 관련성이 없는 것으로 간주됩니다. 이 지표는 질문에 대답하는 데 도움이 되는 정보만 포함되어 불필요한 자세를 피하도록 보장합니다. 이 지표는 문맥 정밀도로도 불립니다.\n\n이 논문은 또한 이러한 지표가 LLM을 통해 완전 자동화된 방식으로 측정될 수 있다는 방법에 대해 설명합니다.\n\n\n<div class=\"content-ad\"></div>\n\n이 평가에서 사용할 라이브러리인 Ragas는 이러한 주요 메트릭의 진화를 보여주며 새로운 메트릭을 추가했습니다:\n\n- Context Recall: 이 메트릭은 문맥과 실제 답변 간의 일치 정도를 측정합니다. Context Relevance와 마찬가지로 생성된 답변 대신 실제 답변을 사용합니다. 이 메트릭을 얻으려면 참 값이 필요합니다. 이러한 전략의 효과를 평가하기 위해 ColdF 데이터를 바탕으로 실제 답변이 포함된 10개의 질문 세트를 준비했습니다.\n\nFaithfulness와 Answer Relevance는 생성기 메트릭으로서, 각각 환영과 답변이 질문과 얼마나 직접적인지를 측정합니다.\n\nContext Relevance와 Context Recall은 검색기 메트릭으로, 벡터 데이터베이스에서 올바른 데이터 청크를 검색하고 필요한 모든 정보를 얻는 능력을 측정합니다.\n\n<div class=\"content-ad\"></div>\n\n이전에 제시한 네 가지 메트릭을 평가하려면 질문, 생성된 답변, 맥락 및 실제 답변이 필요합니다.\n\nLangChain을 사용하여 RAG 프로세스를 구현할 것입니다. 코드를 실행하려면 Python이 설치되어 있어야 하며(version 3.11.9) 다음 라이브러리가 필요합니다:\n\n- ollama==0.2.1\n- chromadb==0.5.0\n- transformers==4.41.2\n- torch==2.3.1\n- langchain==0.2.0\n- ragas==0.1.9\n\n다음은 LangChain을 사용한 코드 스니펫입니다:\n\n<div class=\"content-ad\"></div>\n\n```js\r\n# 필요한 라이브러리 및 모듈 가져오기\nfrom langchain.embeddings.base import Embeddings\nfrom transformers import BertModel, BertTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer, RobertaModel, RobertaTokenizer\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_text_splitters import MarkdownHeaderTextSplitter\nimport requests\nfrom langchain_chroma import Chroma\nfrom langchain import hub\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.chat_models import ChatOllama\nfrom operator import itemgetter\n\n# DPRQuestionEncoder를 사용하여 사용자 지정 임베딩 클래스 정의\nclass DPRQuestionEncoderEmbeddings(Embeddings):\n    show_progress: bool = False\n    \"\"\"tqdm을 설치해야 하는지 여부 표시합니다.\"\"\"\n    \n    def __init__(self, model_name: str = 'facebook/dpr-question_encoder-single-nq-base'):\n        # 지정된 모델 이름으로 토크나이저와 모델 초기화\n        self.tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(model_name)\n        self.model = DPRQuestionEncoder.from_pretrained(model_name)\n        \n    def embed(self, texts):\n        # texts가 리스트인지 확인\n        if isinstance(texts, str):\n            texts = [texts]\n        \n        embeddings = []\n        if self.show_progress:\n            try:\n                from tqdm import tqdm\n                iter_ = tqdm(texts, desc=\"임베딩 중\")\n            except ImportError:\n                logger.warning(\n                    \"tqdm을 가져올 수 없어 진행률 표시줄을 표시할 수 없습니다. \"\n                    \"`pip install tqdm`으로 설치하세요.\"\n                )\n                iter_ = texts\n        else:\n            iter_ = texts\n\n        for text in iter_:\n            # 입력 텍스트 tokenize\n            inputs = self.tokenizer(text, return_tensors='pt')\n            # 모델을 사용하여 임베딩 생성\n            outputs = self.model(**inputs)\n            # 임베딩 추출하고 리스트로 변환\n            embedding = outputs.pooler_output.detach().numpy()[0]\n            embeddings.append(embedding.tolist())\n        \n        return embeddings\n    \n    def embed_documents(self, documents):\n        return self.embed(documents)\n    \n    def embed_query(self, query):\n        return self.embed([query])[0]\n\n# 프롬프트 생성을 위한 템플릿 정의\ntemplate = \"\"\"\n### CONTEXT\n{context}\n\n### QUESTION\nQuestion: {question}\n\n### INSTRUCTIONS\nCONTEXT 마크다운 텍스트를 사용하여 사용자의 질문에 답변하세요.\n간결하고 명료한 답변을 제공하세요.\n질문에 답변하기 위해 CONTEXT에 근거만 사용하세요.\nCONTEXT에 필요한 정보가 없는 경우 'NONE'을 반환하세요.\n\"\"\"\n\n# 템플릿을 사용하여 ChatPromptTemplate 인스턴스 생성\nprompt = ChatPromptTemplate.from_template(template)\n\n# URL에서 텍스트 데이터 가져오기\nurl = \"https://raw.githubusercontent.com/cgrodrigues/rag-intro/main/coldf_secret_experiments.txt\"\nresponse = requests.get(url)\nif response.status_code == 200:\n    text = response.text\nelse:\n    raise Exception(f\"파일을 가져오는 데 실패했습니다: {response.status_code}\")\n\n# 마크다운 텍스트를 분할할 헤더 정의\nheaders_to_split_on = [\n    (\"#\", \"Header 1\")\n]\n\n# 지정된 헤더로 MarkdownHeaderTextSplitter 인스턴스 생성\nmarkdown_splitter = MarkdownHeaderTextSplitter(\n    headers_to_split_on, strip_headers=False\n)\n\n# 마크다운 splitter를 사용하여 텍스트 분할\ndocs_splits = markdown_splitter.split_text(text)\n\n# Chat 모델 초기화\nllm = ChatOllama(model=\"llama3\")\n\n# 사용자 지정 임베딩을 사용하여 문서에서 Chroma vector store 생성\nvectorstore = Chroma.from_documents(documents=docs_splits, embedding=DPRQuestionEncoderEmbeddings())\n\n# Vector store에서 retriever 생성\nretriever = vectorstore.as_retriever()\n\n# 문서 형식 지정을 위한 함수 정의\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# 검색 보완 생성 (RAG) chain 생성\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n    | {\"answer\": prompt | llm | StrOutputParser(), \n       \"context\": itemgetter(\"context\")}\n)\n\n# 질문으로 RAG chain 실행\nresult = rag_chain.invoke(\"Who led the Experiment 1?\")\nprint(result)\r\n```\r\n\r\n위 코드의 끝에 RAG Chain이 정의되어 있으며 이 코드를 사용하여 메트릭을 평가할 수 있습니다:\r\n\r\n```js\r\n# 필요한 라이브러리 및 모듈 가져오기\nimport pandas as pd\nfrom datasets import Dataset\nfrom ragas import evaluate\nfrom ragas.metrics import (\n        context_precision,\n        faithfulness,\n        answer_relevancy,\n        context_recall\n)\nfrom langchain_community.chat_models import ChatOllama\n\ndef get_questions_answers_contexts(rag_chain):\n    \"\"\" 질문 및 답변 목록을 읽고 ragas 데이터셋을 반환합니다. \"\"\"\n    # 파일 URL\n    url = 'https://raw.githubusercontent.com/cgrodrigues/rag-intro/main/coldf_question_and_answer.psv'\n\n    # URL에서 파일 가져오기\n    response = requests.get(url)\n    data = response.text\n   \n    # 데이터를 줄 단위로 분할\n    lines = data.split('\\n')\n\n    # 각 줄을 파이프 기호로 나누어 튜플 생성\n    rag_dataset = []\n\n    for line in lines[1:10]: # 처음 10개 질문만\n        if line.strip():  # 공백이 아닌지 확인\n            question, reference_answer = line.split('|')\n            result = rag_chain.invoke(question)\n            generated_answer = result['answer']\n            contexts = result['context']\n\n            rag_dataset.append({\n                \"question\": question,\n                \"answer\": generated_answer, \n                \"contexts\": [contexts], \n                \"ground_truth\": reference_answer\n            })\n\n          \n    rag_df = pd.DataFrame(rag_dataset)\n    rag_eval_datset = Dataset.from_pandas(rag_df)\n    \n    # ragas 데이터셋 반환\n    return rag_eval_datset\n\ndef get_metrics(rag_dataset):\n    \"\"\" RAG Dataset에 대해 신의성, 답변 관련성, 컨텍스트 정밀도, \n        컨텍스트 재현율 메트릭 계산 \"\"\"\n    # 계산할 메트릭 목록\n    metrics = [\n        faithfulness,\n        answer_relevancy,\n        context_precision,\n        context_recall\n    ]\n        \n    # LLaMA 3 모델을 사용할 지역 ChatOllama\n    langchain_llm =  ChatOllama(model=\"llama3\")\n    langchain_embeddings = DPRQuestionEncoderEmbeddings('facebook/dpr-question_encoder-single-nq-base')\n\n    # 메트릭 반환\n    results = evaluate(rag_dataset, metrics=metrics, llm=langchain_llm, embeddings=langchain_embeddings)\n    return results\n\n# RAG 데이터셋 가져오기\nrag_dataset = get_questions_answers_contexts(rag_chain)\n\n# 메트릭 계산\nresults = get_metrics(rag_dataset)\nprint(results)\r\n```\r\n\r\n위 코드를 실행하여 결과를 살펴보세요.\n\n<div class=\"content-ad\"></div>\n\n```json\r\n{\n  '신뢰성': 0.8611, \n  '답변 관련성': 0.8653, \n  '맥락 정밀도': 0.7778, \n  '맥락 회수율': 0.8889\n}\r\n```\n\n<img src=\"/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_2.png\" />\n\n이미 언급된 것처럼, 첫 번째 두 지표(예: 신뢰성 및 답변 관련성)는 생성 과정과 관련이 있습니다. 이는 이러한 지표를 향상시키기 위해서는 언어 모델이나 모델에 입력되는 프롬프트를 변경해야 한다는 것을 의미합니다. 마지막 두 지표(예: 맥락 정밀도 및 맥락 회수율)는 검색과 관련이 있으며, 이는 이러한 지표를 향상시키기 위해서는 문서가 저장되고 색인화되며 선택되는 방식을 개선해야 한다는 것을 의미합니다.\n\n## 청킹 접근 방식\n\n<div class=\"content-ad\"></div>\n\n청킹 접근 방식은 데이터가 검색을 위해 최적의 세그먼트로 분할되도록 보장합니다. 이 패러다임은 다양한 청크 크기를 실험하여 너무 작아서 문맥이 빠진다거나 너무 크면 (검색 시스템을 압도하는) 문제가 발생하지 않도록 균형을 찾는 것을 포함합니다. 베이스라인에서는 각 실험을 기준으로 문서를 청크로 분할합니다. 그렇기 때문에 실험의 일부가 희석되어 최종 임베딩에 포함되지 않을 수 있습니다. 이 상황을 해결하기 위한 한 가지 가능한 접근 방식은 부모 문서 검색기를 사용하는 것입니다. 이 방법은 특정 관련 문서 단편이나 문단뿐만 아니라 그들의 부모 문서도 검색합니다. 이 접근 방식은 관련 단편 주변의 문맥이 보존되도록 보장합니다. 아래 코드는 이 접근 방식을 테스트할 때 사용되었습니다:\n\n```js\n# 필요한 라이브러리 및 모듈 가져오기\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\n# 부모 문서 검색기 생성\nparent_document_retriever = ParentDocumentRetriever(\n    vectorstore = Chroma(collection_name=\"parents\", \n                         embedding_function=DPRQuestionEncoderEmbeddings('facebook/dpr-question_encoder-single-nq-base')),\n    docstore = InMemoryStore(),\n    child_splitter = RecursiveCharacterTextSplitter(chunk_size=200),\n    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500),\n)\n\nparent_document_retriever.add_documents(docs_splits)\n\n\n# 검색 보강 생성 (RAG) 체인 생성\nrag_chain_pr = (\n    {\"context\": parent_document_retriever | format_docs, \"question\": RunnablePassthrough()}\n    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n    | {\"answer\": prompt | llm | StrOutputParser(), \n       \"context\": itemgetter(\"context\")}\n)\n\n# RAG 데이터세트 가져오기\nrag_dataset = get_questions_answers_contexts(rag_chain_pr)\n\n# 메트릭스 계산\nresults = get_metrics(rag_dataset)\nprint(results)\r\n```\n\n결과는 다음과 같습니다:\n\n```js\r\n{\n  'faithfulness': 0.6667, \n  'answer_relevancy': 0.4867, \n  'context_precision': 0.7778, \n  'context_recall': 0.6574\n}\r\n```\n\n<div class=\"content-ad\"></div>\n\n아래는 성능 향상에 기여하지 않는 것을 보여줍니다. 감소하는 컨텍스트 회상은 회수 과정이 제대로 작동하지 않고 컨텍스트에 완전한 정보가 없음을 나타냅니다. 충실성 및 답변 관련성 지표의 변화는 풍부하지 않은 컨텍스트에서 나옵니다. 이 경우, 청킹 및 회수를 위한 다른 방법을 평가해 볼 수 있습니다.\n\n## 포함 모델\n\n포함 모델은 텍스트 청크를 밀집 벡터 표현으로 변환합니다. 서로 다른 모델은 다양한 주제에서 훈련될 수 있으며 때로는 임베딩을 개선할 수 있습니다. 임베딩 방법의 선택은 계산 효율성과 임베딩 품질 사이의 균형을 고려해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 Dense Passage Retrieval (“facebook/dpr-question_encoder-single-nq-base”), Sentence-BERT (“paraphrase-MiniLM-L6-v2”), 또는 Chroma의 기본 모델 (“all-MiniLM-L6-v2”)과 같은 다양한 임베딩 모델을 비교합니다. 각 모델은 강점을 갖고 있으며 도메인 특정 데이터에서 평가하여 가장 정확한 의미 표현을 제공하는지를 결정하는 데 도움이 됩니다.\n\n임베딩 모델을 변경하기 위해 새로운 클래스 \"SentenceBertEncoderEmbeddings\"를 정의하는 것이 필요합니다. 이 새로운 클래스는 모델인 Sentence-BERT 모델을 구현합니다. 이 새로운 클래스는 우리 이전 임베딩인 \"DPRQuestionEncoderEmbeddings\"를 대체할 것이며, 이것은 아래의 코드로 Sentence-BERT 모델을 테스트하는 데 사용한 코드입니다:\n\n```js\n코드 내용\n```\n\n결과는 요렇습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n{\n  'faithfulness': 0.5278, \n  'answer_relevancy': 0.5306, \n  'context_precision': 0.5556, \n  'context_recall': 0.7997\n}\n```\n\n<img src=\"/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_4.png\" />\n\n이 경우, 인코더의 변경은 메트릭스에서 성능이 저하된 것을 의미합니다. 이는 DPR이 Sentence-BERT보다 검색 정확도가 더 높기 때문에, 정확한 문서 검색이 중요한 경우에는 DPR이 더 적합하다는 것을 의미합니다. Sentence-BERT로 전환할 때 '신뢰성' 및 '답변 관련성' 메트릭스의 상당한 하락은 높은 검색 정밀성을 필요로 하는 작업에 적합한 임베딩 모델을 선택하는 중요성을 강조합니다.\n\n## 벡터 검색 방법\n\n\n<div class=\"content-ad\"></div>\n\n벡터 검색 방법은 유사성 측정을 기반으로 가장 관련성 높은 청크를 검색합니다. 흔한 방법으로는 유클리드 (L2) 거리, 코사인 유사도 등이 있습니다. 이 검색 방법을 변경하면 최종 출력 품질을 향상시킬 수 있습니다.\n\n다음은 코드입니다:\n\n```js\n# 필요한 라이브러리 및 모듈 가져오기\nimport pandas as pd\nfrom datasets import Dataset\nfrom ragas import evaluate\nfrom ragas.metrics import (\n        context_precision,\n        faithfulness,\n        answer_relevancy,\n        context_recall\n)\nfrom langchain_community.chat_models import ChatOllama\n\n# 문서에서 Chroma 벡터 저장소 생성\n# 사용자 정의 임베딩을 사용하고 코사인 유사도 검색으로 변경\nvectorstore = Chroma.from_documents(collection_name=\"dist\", \n                                    documents=docs_splits, \n                                    embedding=DPRQuestionEncoderEmbeddings(), \n                                    collection_metadata={\"hnsw:space\": \"cosine\"})\n\n# 벡터 저장소로부터 리트리버 생성\nretriever = vectorstore.as_retriever()\n\n# 검색 증강 생성 (RAG) 체인 생성\nrag_chain_dist = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n    | {\"answer\": prompt | llm | StrOutputParser(), \n       \"context\": itemgetter(\"context\")})\n\n# RAG 데이터 세트 가져오기\nrag_dataset = get_questions_answers_contexts(rag_chain_dist)\n\n# 메트릭 계산\nresults = get_metrics(rag_dataset)\nprint(results)\n```\n\n이것이 결과입니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n{\n  'faithfulness': 0.9444, \n  'answer_relevancy': 0.8504, \n  'context_precision': 0.6667, \n  'context_recall': 0.8889\n}\n```\n\n![Image](/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_5.png)\n\n'충실성'에서의 개선은 벡터 검색에 코사인 유사도를 사용하여 검색된 문서를 쿼리와 더 잘 일치시키는 것을 나타냅니다. '문맥 정밀도'가 감소했지만 전체적으로 높은 '충실성'과 '문맥 회수율'은 이 맥락에서 코사인 유사도가 보다 효과적인 벡터 검색 방법이라는 것을 보여주며, 검색 성능을 최적화하기 위한 벡터 검색 방법의 선택의 중요성을 뒷받침합니다.\n\n## 모델에 피드 할 최종 프롬프트\n\n\n<div class=\"content-ad\"></div>\n\n최종 프롬프트 구성은 검색된 데이터를 모델의 쿼리에 통합하는 것을 의미합니다. 프롬프트의 작은 변화는 결과에 큰 영향을 미칠 수 있어 시행 착오 과정이 될 수 있습니다. 프롬프트 내의 예시를 제공하면 모델이 더 정확하고 관련성 높은 결과물을 생성할 수 있습니다.\n\n## 결론\n\n검색 증강 생성(Retrieval-Augmented Generation, RAG) 파이프라인을 최적화하는 것은 특정 데이터와 애플리케이션 컨텍스트에 매우 의존적인 반복적인 과정입니다. 본 논문에서는 네 가지 주요 전략을 탐구했습니다: 청킹 접근 방식의 개선, 임베딩 모델의 선택과 세밀한 튜닝, 효과적인 벡터 검색 방법의 선택, 정확한 프롬프트 작성. 이러한 구성 요소 각각이 RAG 시스템의 성능 향상에 중요한 역할을 합니다.\n\n결과는 일반적인 해결책이 없음을 강조했습니다. 예를 들어, 우리의 맥락에서 Dense Passage Retrieval (DPR)은 Sentence-BERT보다 우수한 성과를 보였지만, 이는 다른 데이터셋이나 요구 사항에 따라 달라질 수 있습니다. 마찬가지로, 벡터 검색에서 코사인 유사도로 전환하면 더 나은 충실도와 콘텍스트 회수가 나타나는 것으로 나타났는데, 검색 과정의 미묘한 변경이 영향을 미치는 것을 보여주었습니다.\n\n<div class=\"content-ad\"></div>\n\nRAG 파이프라인을 최적화하는 여정은 지속적인 테스트, 실패로부터 교훈을 얻고, 정보에 기반한 조정을 포함합니다. 이 반복적인 방식을 받아들이면, AI 솔루션을 더 효과적으로 사용자의 요구에 맞게 맞춤화할 수 있습니다. 성공의 열쇠는 데이터를 이해하고, 다양한 전략을 실험해보며, 끊임없이 프로세스를 개선하는 데 있습니다.\n\n내 프로필과 이메일 목록을 구독하여 최신 작업을 업데이트 받아보세요. 함께하면, AI 최적화의 복잡성을 탐험하고 데이터 기반 솔루션의 완전한 잠재력을 발휘할 수 있습니다. \n\n## 참고 자료\n\n[1] Es, S., James, J., Espinosa-Anke, L., & Schockaert, S. (2023). RAGAS: Automated Evaluation of Retrieval Augmented Generation. Exploding Gradients, CardiffNLP, Cardiff University, AMPLYFI.","ogImage":{"url":"/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_0.png"},"coverImage":"/assets/img/2024-06-23-4StrategiestoOptimizeRetrieval-AugmentedGenerationRAG_0.png","tag":["Tech"],"readingTime":17},{"title":"대형 언어 모델LLMs로 Next-Token 예측을 분류로 전환하는 방법","description":"","date":"2024-06-23 19:36","slug":"2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs","content":"\n\n![이미지](/assets/img/2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs_0.png)\n\n대규모 언어 모델(Large Language Models, LLMs)은 방대한 양의 인터넷 데이터로 훈련되어 다양한 자연어 작업을 수행할 수 있습니다. 그 중 하나인 분류는 주제를 미리 정의된 레이블로 분류하는 지도 학습 작업입니다. 제로샷 및 퓨샷 분류는 인기 있는 기술로, LLMs가 훈련 데이터 없이 또는 몇 가지 예제로 분류 작업을 수행할 수 있습니다. 그러나 보다 정확도를 높이기 위해 가이드 미세 조정을 통해 LLMs의 성능을 향상시킬 수 있음이 입증되었습니다.\n\n# 가이드 미세 조정 LLMs\n\n가이드 미세 조정을 위한 일반적인 방법은 질문-답변 쌍으로 구성된 데이터셋을 작성하는 것입니다. 사전 훈련된 LLMs는 이러한 쌍을 사용하여 지도 학습 방식으로 추가로 미세 조정됩니다.\n\n<div class=\"content-ad\"></div>\n\n당신은 지난 포스트에서 이 접근법을 확인할 수 있어요.\n\n선택적으로, 데이터셋을 좋아하는 조합과 덜 선호하는 조합의 쌍으로 구성된 직접 선호도 최적화(DPO)를 사용하여 성능을 더 개선할 수 있어요. 상위 순위의 오픈소스 LLMs가 이러한 접근법 중 하나를 사용하여 훈련되는 것은 놀라운 일이 아니에요.\n\n# LLMs는 딥 뉴럴 네트워크입니다\n\n직관적으로, 이러한 방법은 LLMs가 토큰을 기반으로 하는 아키텍처를 사용한다는 사실을 활용해요. 지시된 데이터와 선호도 데이터셋 모두에서 텍스트 쌍이 토큰으로 변환됩니다. 교차 엔트로피 손실과 디코더만을 사용하는 오토레그레시브 속성을 이용하여, LLMs의 가중치가 업데이트되는데, 레이블 토큰은 입력 토큰으로부터 복사되지만 하나씩 밀려서 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n주어진 것을 감안하면 우리는 어휘를 분류 라벨로 교체할 수 있습니다! 다음 토큰을 어휘에서 예측하는 대신, 앞선 문맥 토큰에서 분류 작업의 범주를 예측하는 데 관심이 있습니다. 이것은 일반적으로 lm_head로 구현되는 LLMs의 헤드를 변경함으로써 가능합니다. 텍스트 생성에서, lm_head는 (임베딩 차원, 어휘 크기)의 모양을 가지고 있습니다. 분류를 위해 우리는 이것을 (임베딩 차원, 분류 수)로 수정합니다.\n\n# 분류 작업을 위해 LLMs 학습하기\n\n## 모델 및 토크나이저 불러오기\n\n이 접근 방식이 작동하는지 확인하기 위한 실험을 수행해 봅시다. 먼저, HuggingFace에서 사전 훈련된 LLM 및 해당 토크나이저를 사용하여 시작하겠습니다. 본 연구에서는 경량화된 38억 개의 파라미터 모델 microsoft/Phi-3-mini-4k-instruct를 선택했습니다.\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"microsoft/Phi-3-mini-4k-instruct\"\ndevice_map = \"auto\"\ntrust_remote_code = True\n\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name, device_map=device_map, trust_remote_code=trust_remote_code\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side = \"right\"\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\n```\n\n우리의 분류 실험을 간단히 하기 위해 이진 분류 작업을 선택할 것입니다. 나중에 여러 클래스로 확장할 수 있음을 알고 있습니다. 이진 분류에서 클래스 수는 두 개이므로 torch.nn.Linear(hidden_size, 2)입니다. to(\"cuda:`number`\") 함수는 이 레이어가 할당된 GPU 기기를 지정합니다. 이 레이어가 model_name = \"auto\"을 사용하여 초기로드된 모델이 있는 동일한 기기에 할당되었는지 확인해주세요.\n\n## Modify LLMs Head\n\n```python\n# 모델 수정 및 세밀 조정\nhidden_size = 3072\nmodel.lm_head = torch.nn.Linear(hidden_size, 2).to(\"cuda:3\")\n```\n\n<div class=\"content-ad\"></div>\n\n또 다른 흥미로운 실험은 fe-fine-tuning을 위해 필요한 레이어를 결정하는 것입니다. 이 게시물에서는 모델이 모든 다른 레이어에서 토큰의 문맥적 의미를 학습했다는 가정에 기반하여, 마지막 블록의 마지막 정규화 레이어에 초점을 맞출 것입니다. 이 레이어는 lm_head 이전의 끝에서 두 번째 레이어입니다.\n\n먼저, 모든 레이어의 Weight를 'param.requires_grad = False'로 지정하여 동결시킵니다. 그리고 나서 마지막 블록의 마지막 정규화 레이어를 찾아 'param.requires_grad = True'로 가중치를 조정 가능하도록 변경합니다. HuggingFace의 모델 클래스에서는 아래 코드 스니펫에서 보여주는 대로 dot 연산을 사용하여 어떤 레이어로든 이동할 수 있습니다.\n\n```js\n# 마지막 블록과 마지막 정규화 레이어만 fine-tune\nfor param in model.parameters():\n    param.requires_grad = False\n\n# 마지막 정규화 레이어만 fine-tune\nlast_block = model.model.layers[-1].to(\"cuda:3\")\nfinal_norm = model.model.norm.to(\"cuda:3\")\n\nfor param in final_norm.parameters():\n    param.requires_grad = True\n```\n\n## Cross Entropy Loss 정의\n\n<div class=\"content-ad\"></div>\n\n이전 게시물에서 lm_head의 가장 중요한 로짓이 문장의 마지막 토큰과 관련이 있다고 설명했습니다. 이는 self-attention 메커니즘에서 기인하는데, 마지막 토큰은 이전 모든 문맥 토큰들로부터의 주의 점수를 가지고 있습니다. 따라서 우리는 logits[:, -1, :]를 사용합니다.\n\n```python\nimport torch\n\ndef calculate_loss_batch(input_batch, target_batch, model):\n    input_batch, target_batch = input_batch.to(\"cuda\"), target_batch.to(\"cuda\")\n    logits = model(input_batch).logits[:, -1, :]  # 마지막 출력 토큰의 로짓\n    loss = torch.nn.functional.cross_entropy(logits, target_batch).to(\"cuda\")\n    return loss\n```\n\n## 데이터셋 준비\n\n데이터셋은 텍스트와 레이블 두 개의 리스트로 구성되어 있습니다. 먼저, 토크나이저를 사용하여 텍스트 문자열을 토큰화된 ID로 변환합니다. 토크나이저는 정의된 최대 길이보다 긴 토큰을 자르거나 최대 길이보다 짧은 경우 패딩합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\necoding = tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n)\n```\n\n그 다음으로, torch에서 DataLoader 객체를 사용하여 데이터를 모델 튜닝을 위해 반복적으로 공급합니다. BinaryClassification Dataset 객체는 torch의 Dataset 객체를 상속하며, 이 데이터셋을 DataLoader 객체로 로드합니다.\n\n```js\nclass BinaryClassificationDataset(Dataset):\n      pass\n\ndataset = BinaryClassificationDataset(texts, labels, tokenizer, max_length)\n\n# DataLoader\ndataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n```\n\n1단계와 2단계를 합쳐서, 이것이 완전한 구현입니다. 메인 함수에 예제를 제공하여 BinaryClassificationDataset 객체를 인스턴스화하고 DataLoader 객체를 만들어 데이터를 생성하는 방법을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n```python\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\n\nclass BinaryClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].squeeze(0)  # Remove batch dimension\n        attention_mask = encoding['attention_mask'].squeeze(0)  # Remove batch dimension\n\n        return {\n            'input_ids': input_ids.long(),\n            'attention_mask': attention_mask.long(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    texts = [\"Hello, this is a sample sentence.\", \"Another sample text for classification.\"]\n    labels = [0, 1]\n\n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    # Define dataset\n    max_length = 128\n    dataset = BinaryClassificationDataset(texts, labels, tokenizer, max_length)\n\n    # DataLoader\n    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n\n    # Iterate through the dataloader\n    for batch in dataloader:\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        labels = batch['labels']\n\n        print(\"Input IDs:\", input_ids)\n        print(\"Attention Mask:\", attention_mask)\n        print(\"Labels:\", labels)\n      \n```\n\n## 모델 훈련\n\n손실 함수를 정의하고 데이터셋을 준비한 후, 모델 훈련을 시작할 수 있습니다. calculate_loss_loader 함수는 losses를 0으로 초기화합니다. 지정된 배치 수(num_batches)를 지정하면, calculate_loss_batch 함수를 사용하여 각 배치의 손실을 계산합니다. DataLoader 객체를 사용하여 calculate_loss_batch 함수에 배치 데이터를 반복적으로 제공하고, 각 반복에 대해 손실을 누적합니다. 재현성을 위해 torch.manual_seed(1234)를 사용합니다. 이 실험에서 0.68의 손실을 달성했습니다.\n\n```python\ndef calculate_loss_loader(data_loader, model, num_batches=None):\n    losses = 0.\n    if len(data_loader) == 0:\n        return \"Please provide dataset, data loader is empty.\"\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches = min(num_batches, len(data_loader))\n    for index, batch in enumerate(data_loader):\n        if index < num_batches:\n            loss = calculate_loss_batch(\n              batch[\"input_ids\"], batch[\"labels\"], model\n            )\n            losses += loss.item()\n        else:\n            break\n    return total_loss / num_batches\n\n\ntorch.manual_seed(1234) # 재현성을 위해\nwith torch.no_grad():\n    train_loss = calculate_loss_loader(dataloader, model, num_batches=2)\n```\n\n<div class=\"content-ad\"></div>\n\n# 트레이드오프\n\n다음 토큰 예측에서 트랜스포머를 처음 원리를 통해 분류 예측으로 활용할 수 있다는 것은 흥미롭습니다. 미리 훈련된 트랜스포머 모델은 언어 패턴의 풍부한 표현을 포착합니다. 그러나 가벼운 모델이지만 38 억 개의 매개변수를 가지고 있습니다. 독자들은 정확성과 교육 그리고 추론 처리량(초당 생성된 토큰 수) 사이의 트레이드오프를 인식해야 합니다. 저는 기준선으로 더 작은 모델을 시작하는 것을 제안합니다. 예를 들어, 6700만 개의 매개변수를 가진 distilbert-base-uncased 모델을 사용할 수 있습니다. 또는 XGBoost와 같은 더 전통적인 머신러닝 모델을 시도할 수도 있습니다.\n\n# 결론\n\n본 블로그 포스트에서는 처음 원리를 사용하여 다음 토큰 예측 문제를 분류 레이블 예측으로 변환하는 방법을 보여드렸습니다. 이 데모를 통해 LLM의 복잡한 구조를 해체하고 도메인별 문제에 동일한 개념을 적용하는 지식을 습득할 수 있기를 희망합니다. 전체 분류 작업에 대해 트랜스포머가 최적화된 솔루션이 아닐 수 있으며, 다른 더 작은 모델과 데이터를 학습하고 평가해야 함을 상기해야 합니다. 읽어 주셔서 감사합니다. 행복한 학습 되세요!","ogImage":{"url":"/assets/img/2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs_0.png"},"coverImage":"/assets/img/2024-06-23-TransformingNext-TokenPredictionintoClassificationwithLLMs_0.png","tag":["Tech"],"readingTime":9},{"title":"서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기","description":"","date":"2024-06-23 19:35","slug":"2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification","content":"\n\n데이터와 컴퓨터 프로그램의 세계에서 머신 러닝이라는 개념은 어려운 문제 같을 수도 있어요. 복잡한 수학과 이해하기 어려운 개념이 가득한 것 같죠.\n\n그래서 오늘은 여기서 멈추어서, 제 MLBasics 시리즈의 새로운 이슈를 통해 모든 것이 어떻게 작동하는지 기본적인 사항을 살펴보고 싶어요.\n\n오늘의 안건은 서포트 벡터 머신을 이해하는 것이에요.\n\n이 강력한 도구는 데이터를 명확한 범주로 분류하는 데 도움이 되지만...\n\n<div class=\"content-ad\"></div>\n\n어떻게 동작하는 건가요?\n\nSupport Vector Machines 모델을 간단히 설명해 보겠습니다👇🏻\n\n# Support Vector Machine이란?\n\nSupport Vector Machine (SVM)은 두 가지 다른 클래스로 데이터 포인트를 가장 잘 분리하는 초평면을 찾으려는 지도 학습 알고리즘입니다.\n\n<div class=\"content-ad\"></div>\n\n이 문제는 이를 수행할 수 있는 무한한 수의 초평면이 존재한다는 점이 어렵습니다. 그래서 SVM의 목표는 클래스를 최대 여백으로 가장 잘 분리하는 초평면을 식별하는 것입니다.\n\n![image](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png)\n\n# SVM의 주요 개념\n\n더 깊이 파고들기 전에, 몇 가지 핵심 용어를 이해해 보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n- Support Vectors(서포트 벡터): 이들은 초평면에 가장 가까운 데이터 포인트로, 초평면의 위치와 방향에 큰 영향을 미칩니다.\n- 여백(Margin): 여백은 초평면과 각 클래스에서 가장 가까운 데이터 포인트 사이의 거리입니다. 더 큰 여백은 분류기의 일반화를 더 잘 시킬 것입니다.\n- 초평면(Hyperplane): 2차원 공간에서 데이터를 두 부분으로 나누는 선입니다. 고차원에서는 평면이나 고차원의 유사 구조체입니다.\n\n![이미지](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_1.png)\n\n# SVM이 작동하는 방식\n\n두 종류의 데이터 포인트가 있는 데이터셋을 상상해보세요.\n\n<div class=\"content-ad\"></div>\n\n- 파란색 🔵\n- 노란색 🟨\n\n새 데이터 포인트를 파란색 또는 노란색 중 하나로 분류하고 싶습니다. 주요 과제는 두 클래스를 분리할 수 있는 다양한 하이퍼플레인이 존재한다는 것인데, 그런 다음 큰 질문이 있습니다:\n\n어떻게 최적의 하이퍼플레인을 찾을까요?\n\n![이미지](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_2.png)\n\n<div class=\"content-ad\"></div>\n\n가장 좋은 초평면은 두 클래스로부터 최대 거리를 가지는 것입니다. 이는 가능한 다양한 초평면을 찾고 두 클래스로부터 최대 거리를 가지는 것을 선택함으로써 수행됩니다.\n\n# SVM 뒤에 숨겨진 수학적 직관\n\n데이터를 분류하는 방법을 이해하기 위해 수학적 측면을 살펴보겠습니다.\n\n점곱은 하나의 벡터를 다른 벡터에 따라 투영하는 것을 말합니다. 그래서 우리는 한 쪽의 점과 다른 쪽의 초평면이 어디에 있는지 결정하는 데 활용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n임의의 점 X를 고려해 보면:\n\n- 만약 X⋅W ` c 이면 — 이것은 양성 샘플입니다.\n- 만약 X⋅W ` c 이면 — 이것은 음성 샘플입니다.\n- 만약 X⋅W = c 이면 — 이것은 결정 경계 상에 있습니다.\n\n쉽죠?\n\n그러니까 조금 되감아보고 이 방정식들이 어디에서 왔는지 이해해 봅시다:\n\n<div class=\"content-ad\"></div>\n\n## #1. 하이퍼플레인을 찾는 방법 결정\n\n우리가 “분리선”을 얻기 위해, 서포트 벡터와 하이퍼플레인 사이의 거리 d를 먼저 계산할 수 있습니다. 여유 공간은 하이퍼플레인으로부터 가장 가까운 서포트 벡터까지의 거리의 두 배이며, 이 여유 공간 내에는 어떤 점도 있어서는 안 됩니다.\n\n## #2. 거리 “d” 투영\n\n거리 d는 두 서포트 벡터 사이의 차이를 하이퍼플레인의 법선 벡터 w의 방향으로 투영하면 얻을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_3.png)\n\n여러분 중 많은 분들이 여기에 도착한 방법을 모르실 것 같아요. 그래서 한 발 물러나서 이 함수가 처음부터 무엇을 의미하는지 더 잘 이해해보도록 해요.\n\nA와 B라는 두 벡터가 있다고 상상해봅시다. 그들 사이에 θ도를 생성해요. 이 스칼라 곱을 사용하여 A가 B 위에 떨어지는 투영을 쉽게 찾을 수 있어요.\n\n즉, A의 B에 대한 투영을 찾을 수 있어요. A와 B 벡터를 알면 다음 수식에서도 확인할 수 있듯이요.\n\n<div class=\"content-ad\"></div>\n\n그래서 이제 우리가 이 기본 원리를 이해했으니, SVM 모델로 돌아가 봅시다. SVM에 동일한 수학적 개념을 적용할 수 있습니다. 여기서 A는 지원 벡터 머신으로 정의된 벡터이고 B는 우리가 분할 초평면의 법선 벡터입니다.\n\n## #3. 제약 조건 정의하기\n\n이제 여백을 활용하여 제약 조건을 정의할 수 있습니다. 최대 여백 초평면이 (2D 예제에서) 선 방정식을 따라야 한다는 것을 알고 있습니다.\n\n이것은 초평면에 놓인 것은 양수 값을 가질 것이며(양쪽 초평면에 해당), 그 아래에 있는 것은 음수 값을 가질 것입니다(음쪽 초평면에 해당).\n\n<div class=\"content-ad\"></div>\n\n위 두 초평면 사이의 간격을 \"마진\"이라고 합니다.\n\n## SVM의 마진\n\n마진은 SVM에서 중요한 개념으로, 초평면 주변에 데이터 포인트가 없는 버퍼 영역으로 작용합니다. 이 마진이 넓을수록 모델이 보이지 않는 데이터에 대해 일반화할 수 있으며, 과적합 가능성을 줄입니다.\n\n양수 또는 음수로 점을 분류하기 위해 초평면과의 상대적 위치를 기반으로 결정 규칙을 설정합니다.\n\n<div class=\"content-ad\"></div>\n\n- 한 쪽에 있는 점들은 한 범주로 분류됩니다 (파란색 🔵)\n- 다른 한 쪽에 있는 점들은 반대 범주에 속합니다 (노란색 🟨).\n\n마진을 최대화함으로써 SVM은 의사결정 경계를 최적으로 배치하여 가능한 높은 신뢰도로 클래스를 분리합니다.\n\n그러면 어떻게 최대화할까요?\n\n# 최적화와 제약 사항\n\n<div class=\"content-ad\"></div>\n\nSVM은 여백을 최대화하기 위한 최적화 문제를 해결하는 것을 포함합니다. 이는 선택한 초평면이 각 클래스의 가장 가까운 데이터 포인트에서 충분한 거리를 유지하도록 하는 것을 의미합니다. 이를 서포트 벡터라고 합니다.\n\n이미 이전에 발견한 선 방정식을 기반으로 한 분류 알고리즘이 있습니다. 그래서 출력을 다음과 같이 정의할 수 있습니다:\n\n- +1 또는 🔵는 양쪽의 데이터를 나타냅니다.\n- -1 또는 🟨는 음쪽의 데이터를 나타냅니다.\n\n![이미지](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_4.png)\n\n<div class=\"content-ad\"></div>\n\n하지만 여전히 w 벡터와 b 매개변수를 찾아야 합니다.\n\n그래서... 어떻게 할까요?\n\n마진 경계에 위치하는 서포트 벡터는 우리의 양의 및 음의 초평면 내에 포함되어 있기 때문에 다음 제약 조건을 만족합니다.\n\n그래서 이를 쉽게 일반화할 수 있어요...\n\n<div class=\"content-ad\"></div>\n\n## 일반 제약 조건 방정식\n\n모든 데이터 포인트 (x, y)가 마진을 넘어가지 않도록 하기 위해, 모든 데이터 포인트에 대한 제약 조건은 다음과 같이 요약될 수 있습니다:\n\n![equation](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_5.png)\n\n그리고 수행할 단계가 하나 더 남았습니다...\n\n<div class=\"content-ad\"></div>\n\n## 최적화 목표\n\n이제 일반적인 제약 방정식을 가지고 있으므로, 벡터 w의 절대값을 최소화하면서 제약 조건을 충족시킬 수 있습니다.\n\n이는 다음과 같이 수학적으로 정의될 수 있습니다:\n\n![equation](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_6.png)\n\n<div class=\"content-ad\"></div>\n\n이 최적화 문제를 해결함으로써, 클래스 간의 최상의 분리를 보장하는 최대 마진을 가지는 초평면을 정의하는 벡터 w와 b의 최적값을 찾을 수 있습니다.\n\n# 결론\n\n서포트 벡터 머신은 데이터 과학자의 무기 중 강력한 도구로, 이진 분류에 효과적인 방법을 제공합니다.\n\n클래스 간의 간격을 최대화하는 데 초점을 맞추면, SVM은 새로운 데이터에 대해 잘 일반화되는 견고한 분류기를 생성하여, 오버피팅의 위험을 줄입니다.\n\n<div class=\"content-ad\"></div>\n\nSVM의 수학적 기반은 최적 초평면의 식별을 보장하여 다양한 분류 작업에 신뢰할 수 있는 선택지로 만듭니다.\n\n복잡한 데이터셋을 다루거나 모델 성능을 향상시키려는 경우, SVM에 대한 이해와 구현은 머신러닝 도구상자를 크게 향상시킬 수 있습니다.\n\nMLBasics 이슈를 좋아하셨나요? 그렇다면 DataBites 뉴스레터를 구독하여 최신 소식을 받아보세요!\n\n내용을 메일로 받아보실 수 있습니다!\n\n<div class=\"content-ad\"></div>\n\n![2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_7.png](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_7.png)\n\nX, Threads, LinkedIn에서도 만나볼 수 있어요! 거기서는 머신러닝, SQL, Python, 데이터 시각화에 관한 일일 치트시트를 올려요.\n\n다른 멋진 글도 여기 한번 확인해보세요! 😄","ogImage":{"url":"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png"},"coverImage":"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png","tag":["Tech"],"readingTime":6}],"page":"8","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true}