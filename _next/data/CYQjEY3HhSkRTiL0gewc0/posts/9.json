{"pageProps":{"posts":[{"title":"약물 발견의 출발점으로서 방대한 화학 공간을 구조 기반 가상 스크리닝하기 위한 방법","description":"","date":"2024-06-23 19:33","slug":"2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery","content":"\n\n\n![Structure-based virtual screening](/assets/img/2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery_0.png)\n\n가상 스크리닝은 유형의 분자 제약을 넘어서 널리 합성 가능한 화합물 억양 개 중에서 선도물을 찾을 수 있게 합니다. 현재 약물 설계를 위한 여러 방법들이 있지만, 아직 완벽한 방법은 없기 때문에 새로운 접근법의 개발은 새로운 목표와 도전이 이 분야에 오는 한 계속될 것입니다.\n여기서는 구조 기반 가상 스크리닝 접근법에 집중하고 있으며, 널리 사용되고 있는 새로운 기계 학습 기술과 시너지를 이루고 있는 것을 성공적으로 소개하고 있습니다.\n\n# 거대 라이브러리 도킹\n\n거대 라이브러리 도킹 방법은 여기서 논의된 다른 방법 중 약물 개발에 가장 오랫동안 사용된 방법 중 하나입니다. 높은 활성 물질 수가 이 방법으로 발견되어 왔으며, 서브나노 몰 활동까지 초점이 맞춰졌습니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 초대형 라이브러리 도킹에 필요한 두 가지 필수 입력은 대상 구조와 스크리닝 데이터범입니다.\n\n단백질의 고해상도 X-선 및 cryoEM 구조 및 실험적인 것과 유사한 인공지능으로 얻은 구조들이 현재 사용 가능합니다. 여기서 가장 중요한 것 중 하나는 모델 성능을 벤치마킹하고 최적화하여 연구된 리간드와 단백질 간의 주요 상호작용을 복제하는 것이며, 이는 원래의 결정학 모델에서 발생하는 상호작용과 유사합니다.\n\n구조 기반 가상 스크리닝에서 이제 가장 자주 사용되는 데이터범 유형은 온디맨드 생성 라이브러리와 상용 인실리코 빌딩 블록에 기반한 집중 데이터범입니다. 첫 번째 데이터범은 빠른 아날로그 액세스를 제공하여 구조-활성-관계를 얻고 히트-투-리드 생성을 가속화할 수 있습니다. 그들의 단점은 특정 화합물종을 함유한 분자의 제한된 가용성입니다. 집중 데이터범은 원하는 프레임을 함유하는 대규모 라이브러리를 생성함으로써 이 제한을 해소합니다.\n\n도킹 모델이 교정된 후 분자 라이브러리를 가상으로 스크리닝할 수 있습니다. 분자들과 단백질 결합부위의 상호작용은 점수 함수를 사용하여 계산됩니다. 얻은 예측은 필터링되고 클러스터링됩니다. 이러한 필터는 주요 도킹 함수에서 놓친 문제 있는 특징을 포착하고, 알려진 리간드와의 상이함을 보장하며, 우선순위가 있는 화합물들 사이에서 다양성을 촉진할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n마지막으로, 획득한 리간드-생물학적 타겟 복합체를 시각적으로 검사합니다. 표적 단백질 결합 부위와 주요 상호작용을 형성하는 분자들은 후속 실험적 검증을 위해 선택됩니다.\n\n거대 라이브러리 도킹의 주요 문제는 최상위 등수 분자들 사이에 거짓 양성 결과가 누적된다는 점입니다. 상호작용 지문, 보다 정확한 리간드 변형 설명, 및 수용체 유연성은 이 상황을 극복하는 데 도움이 될 수 있습니다. 또한, 분자 도킹과 분자 역학 자유 에너지 계산의 조합과 같은 보다 엄격한 계산 방법을 사용하여 거대 화면에서 상위 순위 분자들의 작은 집합을 다시 점수 매기는 것으로 이 문제를 해결할 수 있습니다.\n\n지금은 딥러닝 기반 도킹 방법이 활발히 개발되어 거대 라이브러리 스크리닝의 속도와 정확도를 향상시키고 있지만, 유사하지 않은 타겟에 일반화하고 물리적으로 유효한 리간드 구조를 생성하는 모델의 능력에 대한 우려가 여전히 있습니다.\n\n# 기계 학습 가속 가상 스크리닝\n\n<div class=\"content-ad\"></div>\n\n가상 스크리닝 파이프라인에 머신 러닝 방법을 통합하면 가장 큰 사용 가능한 온디맨드 라이브러리를 효율적으로 탐색할 수 있습니다. 이 접근 방식은 100배까지 다양한 화합물을 축소하고 동시에 최상위 점수화된 분자를 풍부하게 만들기로 입증되었습니다.\n\n머신 러닝 가속 가상 스크리닝 파이프라인의 본질적인 부분은 다음과 같은 순차적 단계에 따라 진행됩니다. 먼저, 관심 대상과의 관련 도킹 점수에 기반하여 온디맨드 라이브러리 샘플의 해당 훈련 세트가 생성됩니다. 그런 다음, 머신 러닝 모델을 훈련시키고 나머지 라이브러리의 점수를 예측하는 데 사용됩니다. 유망한 분자들은 머신 러닝 모델의 더 깊은 개선(액티브 러닝) 또는 실험적 평가에 사용됩니다. 원하는 결과가 달성될 때까지 이러한 작업들은 반복될 수 있습니다.\n\n머신 러닝 가속 가상 스크리닝의 주요 단점이 몇 가지 있습니다. 첫째로, 더 정교한 알고리즘으로 인해 상당한 계산 비용이 필요하며 도킹 점수 기능의 정확도가 낮아 실제 잠재성을 충분히 발휘하지 못할 수 있습니다. 둘째로, 사용 가능한 데이터셋이 특정 유형의 화합물이나 대상으로 편향될 수 있어 새로운, 보지 못한 화합물에 대한 모델의 일반화 능력이 제한됩니다. 셋째로, 일부 최상위 점수 분자가 생물학적 실험에서 무효일 가능성이 있지만, 이들이 훈련 세트에 포함된다면 머신 러닝 모델이 가상 스크린의 높은 거짓 양성률을 전파하거나 심지어 증가시킬 수 있습니다.\n\n**# 프래그먼트 기반 가상 스크리닝**\n\n<div class=\"content-ad\"></div>\n\n분자 단편의 내재적으로 낮은 분자 복잡성은 그들이 대상 단백질 결합 부위를 보완하고 vitro에서 테스트될 가능성을 높입니다. 그러나 대형 분자와는 달리 단편은 높은 효능이나 선택성을 갖지 않으며, 추후 광범위한 화학적 발전이 필요하여 선도 후보를 얻기 위해. 새로운 단편 기반 가상 선별 방법으로 이 과정을 최적화하는 것이 가능해졌습니다.\nV-SYNTHES 방법은 대규모 화합물 모델링 도전을 작은 단계로 분해하여 현재 명시적 도킹으로 달성할 수 있는 것보다 몇 차원 더 큰 라이브러리 탐색으로 탐색합니다. 이러한 방법은 대형 라이브러리 도킹의 주요 한계인 화학 공간 크기에 선형적으로 비례하는 컴퓨팅 리소스의 상당한 증가를 다룹니다.\nV-SYNTHES에는 라이브러리 준비, 열거, 도킹 및 히트 선택의 반복적 단계가 포함됩니다. 준비 과정에서 단편 유사 화합물 라이브러리가 생성되며 (\"최소 열거 라이브러리\"(MEL)), 사용된 화학 라이브러리 공간 전체에 대한 모든 반응의 골조-신톤 조합의 모든 가능성을 대표합니다. 다음 단계에서, MEL 화합물은 대상 수용체에 도킹됩니다. 최상의 점수를 받은 신톤이 선택되고 해당 골조에 초점을 맞춘 화학 라이브러리는 반복적인 열거와 도킹이 수행됩니다. 반복 횟수는 화학 공간에서의 전체 화합물을 나타내는 분자가 완성될 때까지 되풀이됩니다. 마지막으로 라이브러리의 최종 열거된 하위 집합에 도킹 스크린을 수행하고 최우선 히트는 방해 화합물, 물리-화학적 특성, 약물 유사성, 독창성 및 화학 다양성에 대한 후처리 필터링을 거칩니다. 그 결과 화합물 세트가 합성되어 실험적으로 테스트됩니다.\n최근 제안된 기계 학습 방법인 FRAME은 구조 안내 방식을 사용하여 단편의 점진적 발전을 기반으로 합니다. 여기서 딥 러닝은 초기 단편에서 결합된 단백질 구조에 기초하여 적합한 성장 벡터를 식별하고 SE(3)-등변 신경망 활용을 통해 다양한 화학적 발전을 평가합니다. FRAME의 주요 단점은 제한된 합성 접근성으로 이어질 수 있다는 것입니다.\n\n저희 Chemspace는 Enamine REAL Space에 접근하여 V-SYNTHES를 사용하여 속도와 효과면에서 가장 효과적인 구조 기반 가상 리간드 스크리닝 방법을 사용합니다. 저희 전문가들이 최단 시간 내에 당신이 원하는 대상에 대한 최상의 히트 화합물을 기가 스케일 라이브러리에서 검색하는 데 도와드릴 것입니다. 본 서비스에 대해 자세히 읽을 수 있습니다.\n\n본 검토는 다음 논문을 바탕으로 작성되었습니다: J. Carlsson et al., Curr. Opin. Struct. Biol. 2024; https://doi.org/10.1016/j.sbi.2024.102829","ogImage":{"url":"/assets/img/2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery_0.png"},"coverImage":"/assets/img/2024-06-23-Structure-basedvirtualscreeningofvastchemicalspaceasastartingpointfordrugdiscovery_0.png","tag":["Tech"],"readingTime":4},{"title":"BERT 미세 조정으로 텍스트 분류하는 방법","description":"","date":"2024-06-23 19:32","slug":"2024-06-23-FinetuneBERTfortextclassification","content":"\n\n섬세 조정은 대형 언어 모델이 사용자 지정 데이터에 적응하고 텍스트 분류와 같은 하향 작업을 잘 수행할 수 있도록 돕는 중요한 기술입니다.\n\n본 문서는 섬세 조정의 기본에 초점을 맞추고, LORA, QLORA 등 다른 기술에 대해 깊게 다루지는 않습니다. 시작하는 가장 좋은 방법은 BERT로 실험을 해보는 것입니다.\n\n주로 두 가지 방법으로 이 작업을 수행할 수 있습니다:\n\n- 허깅페이스 트레이너 API 사용: 사용하기 쉽지만 매우 사용자 정의가 어려움\n- PyTorch 사용: 트레이너보다 조금 어려우나 프로세스에 대한 더 많은 사용자 정의와 제어를 제공합니다\n\n<div class=\"content-ad\"></div>\n\n데이터셋\n\n우리는 Hugging Face에서 제공하는 Yelp Reviews 데이터셋을 사용할 예정입니다. 이 데이터셋은 다음 두 열로 구성되어 있습니다:\n\n- 레이블: 1부터 5까지의 별표가 부여된 등급입니다.\n- 텍스트: 리뷰 내용입니다.\n\n저희의 목표는 리뷰 텍스트로부터 별의 개수를 예측할 수 있는 모델을 훈련하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n휍핑페이스 트레이너 API를 사용하여 파인튜닝하기\n\n- 모든 라이브러리를 설치하세요 :\n\n```js\n!pip install --upgrade transformers datasets evaluate huggingface_hub torch\n```\n\n참고: 이 라이브러리들의 최신 버전을 항상 사용하도록 하세요.\n\n<div class=\"content-ad\"></div>\n\n2. 데이터셋 로드: Hugging Face에서 제공하는 datasets 라이브러리를 사용하여 데이터셋을 로드할 수 있어요.\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"yelp_review_full\")\n```\n\n데이터셋을 확인해봐요. 어떤 데이터를 다루게 될지 알아봅시다.\n\n```python\ndataset[\"train\"][1]\n```\n\n<div class=\"content-ad\"></div>\n\n우리 데이터가 어떻게 보이는지 확인해보세요.\n\n```js\n{'label': 1,\n 'text': \"안타깝게도 Dr. 골트버그의 환자로서 느끼는 좌절은 뉴욕의 다른 많은 의사들과 겪어온 경험의 반복입니다 - 좋은 의사, 하지만 최악의 스태프. 그의 스텝은 단순히 전화를 받지 않는 것 같습니다. 답변을 받으려면 보통 반복적인 전화로 2시간이 걸립니다. 누가 그런 시간을 가진 사람이며 누가 그것과 소통하길 원하겠습니까? 다른 많은 의사들과도 이 문제를 겪어왔고, 이해가 안 가네요. 사무원이 있고 의료 필요가 있는 환자가 있는데, 왜 전화를 받는 사람이 없는 건지요? 이해할 수 없고, 신경질만 나게 합니다. Dr. 골트버그에게 2점을 주어야 하는 점이 유감입니다.\"}\n```\n\n3. 토크나이저를 로드하고 텍스트를 토큰화하는 함수를 만들어보세요:\n\n```js\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n```\n\n<div class=\"content-ad\"></div>\n\n토크나이저는 텍스트를 입력_ids, 토큰_유형_ids 및 어텐션_마스크로 이해할 수 있는 세 개의 열로 변환합니다.\n\n데이터셋에서 작은 배치를 만들기(선택 사항)\n\n```js\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n```\n\n4. 모델 불러오기:\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n```\n\n분류할 레이블 수를 초기화하려면 num_labels 매개변수를 사용하세요.\n\n5. 훈련 인수 초기화\n\n```python\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\")\n```\n\n<div class=\"content-ad\"></div>\n\nhttps://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments에서 제공되는 매개변수에 대한 자세한 정보를 확인할 수 있습니다.\n\n6. 메트릭 계산 함수 설정:\n\n```js\nimport numpy as np\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n```\n\n7. 학습 시작:\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom transformers import Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()\n```\n\n트레이닝을 시작하려면 wandb 키를 입력하라는 프롬프트가 나타납니다. 키를 입력하면 트레이닝 프로세스가 시작됩니다. 트레이닝이 완료되면 아래와 같은 결과를 보게 될 것입니다.\n\n![트레이닝 결과](/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png)\n\n선택적으로 노트북에서 허깅페이스로 모델을 저장할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n```js\nfrom huggingface_hub import login\nlogin()\nmodel.push_to_hub(\"HuggingfaceUsername/yourModelName\")\n```\n\n8. 추론 실행:\n\n모델을 테스트하려면 PyTorch를 사용할 수 있습니다.\n\n```js\nimport torch\nimport torch.nn.functional as F\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"HuggingfaceUsername/yourModelName\")\ns=\"The was awesome and I loved it\"\ntt=tokenizer(s,return_tensors=\"pt\", padding=True, truncation=True)\n```\n\n\n<div class=\"content-ad\"></div>\n\n모델을 평가 모드로 설정하면 더 이상 가중치를 업데이트할 필요가 없어지고, 이제 분류 작업에 사용할 수 있습니다.\n\n```python\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(**tt)\n```\n\n결과를 확인해보겠습니다.\n\n```python\nSequenceClassifierOutput(loss=None, logits=tensor([[-2.3995, -2.0111, -0.8381,  2.4683,  2.8968]]), hidden_states=None, attentions=None)\n```\n\n<div class=\"content-ad\"></div>\n\n여기서 중요한 변수는 로짓 변수입니다. 이 경우 로짓은 텍스트가 특정 클래스에 속할 확률을 나타냅니다. 현재 로짓은 이해하기 어려운 형식으로 표시됩니다. 이를 이해할 수 있는 형식으로 변환하려면 이해할 수 있는 숫자로 변환해야 합니다.\n\n```js\nlogits = outputs.logits\nprint(\"로짓:\", logits)\n\n# 소프트맥스를 사용하여 로짓을 확률로 변환합니다\nprobabilities = F.softmax(logits, dim=-1)\nprint(\"확률:\", probabilities)\n\n# 예측된 클래스를 결정합니다\npredicted_class = torch.argmax(probabilities, dim=-1)\nprint(\"예측된 클래스:\", predicted_class.item())\n```\n\n여기서 출력은 4입니다.\n\nPyTorch를 사용한 파인 튜닝\n\n<div class=\"content-ad\"></div>\n\n모델이 이해할 수 있도록 몇 가지 전처리 단계가 필요합니다.\n\n- 열 삭제\n\n```js\ntokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\ntokenized_datasets.set_format(\"torch\")\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n```\n\n2. 데이터로더(Dataloader) 생성\n\n<div class=\"content-ad\"></div>\n\n```js\nimport torch\nfrom torch.utils.data import DataLoader\ntraindataloader=DataLoader(small_train_dataset,batch_size=8,shuffle=True)\ntestdataloader=DataLoader(small_eval_dataset,batch_size=8)\n```\n\n3. 모델을 다운로드하고 GPU에 로드해주세요.\n\n```js\nfrom transformers import AutoModelForSequenceClassification\nmodel=AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n```\n\n4. 옵티마이저(optimizer)와 학습률 스케줄러(learning rate scheduler)를 생성하세요.\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom torch.optim import AdamW, SGD\nfrom transformers import get_scheduler\noptimizer = SGD(model.parameters(), lr=5e-5)\nnum_epochs = 3\nnum_training_steps = num_epochs * len(traindataloader)\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n```\n\n원하는 옵티마이저와 학습률 스케줄러를 조정하여 가장 적합한 것을 선택할 수 있어요.\n\n5. 학습 및 평가\n\n모델을 model.train()을 사용하여 학습 모드로 설정해주세요.\n\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in traindataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n```\n\n```js\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\nmodel.eval()\nfor batch in testdataloader:\n    b = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**b)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n\nmetric.compute()\n```\n\n제 Kaggle 노트북에서 스크립트를 확인할 수 있습니다. https://www.kaggle.com/code/exterminator11/finetune-bert. 행운을 빕니다!","ogImage":{"url":"/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png"},"coverImage":"/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png","tag":["Tech"],"readingTime":8},{"title":"자율 에이전트의 흥망성쇠 그 발전과 한계 ","description":"","date":"2024-06-23 19:30","slug":"2024-06-23-TheRiseandFallofAutonomousAgents","content":"\n\n2023년 ChatGPT가 인기를 끌자, 창조적 AI 공간에서 골드 러시 분위기가 등장했습니다. 전 세계적으로 사람들은 미래에 대한 AI의 변혁적 잠재력을 인식했습니다. 이 골드 러시적 마인드셋은 우리를 핵심 질문으로 이끕니다: 여기에는 골드가 어디에 있을까요?\n\n다른 말로 하면, 미래에는 어떤 일이 일어날까요? 보통 우리에게는 미래로 멀리 전망하는 것이 어려우며, 다가오는 변화를 예측하기 위해 짧은 시간대에 초점을 맞추는 것이 더 좋습니다. 그러나 창조적 AI 분야에서는 이것이 다르다고 보이죠. 우리가 향하는 방향을 알 수는 있겠지만, 다음에 무엇이 올지 예측하는 것은 널리 어렵습니다.\n\n2023년 4월, AutoGPT와 BabyAGI와 같은 자율 에이전트 워크플로우가 GitHub에서 인기를 얻기 시작했습니다. 인기가 폭증하며, AutoGPT는 딱 한 달 만에 5만 명 이상의 프로그래머들의 주목을 끌었습니다.\n\n![이미지](/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_0.png)\n\n<div class=\"content-ad\"></div>\n\n# 자율 에이전트 워크플로 이해\n\n자율 에이전트 워크플로의 구체적인 내용을 알아보기 전에 먼저 '에이전트'가 무엇을 의미하는지 명확히 해보겠습니다. '에이전트'라는 용어는 컴퓨터 과학의 맥락에서 20세기로 거슬러 올라가는 뿌리를 가지며, 1980년대 후반에 큰 인기를 얻었습니다.\n\n일반적으로, 에이전트는 \"행동\"이 가능한 개체입니다 (‘agency' 개념에서 파생됨). 에이전트는 다음과 같은 세 가지 주요 기능으로 정의될 수 있습니다:\n\n- 지각: 센서 또는 텍스트 입력을 통해.\n- 결정: 인식에 기반한 결정을 내림.\n- 행동: 그 결정에 기반한 행동을 실행함.\n\n<div class=\"content-ad\"></div>\n\n2023년 초에 발표된 논문 'ReAct: 언어 모델에서의 추론과 실행의 시너지 효과'는 특히 대규모 언어 모델의 맥락에서 마일스톤을 달성했습니다. 인기 있는 프레임워크인 LangChain은 ReAct 로직을 구현하여 자료와 관련된 테마에 상당 부분의 리포지토리를 할애했습니다. 이 접근 방식은 에이전트 논리를 보다 넓은 관중에게 접근 가능하게 만들면서 에이전트 논리의 최신 발전을 통합했습니다.\n\n에이전트를 뛰어넘게 하는 것은 에이전트에게 일련의 도구를 제공하여 대규모 언어 모델의 훈련 데이터의 한계를 극복하는 능력입니다. 이러한 도구는 본질적으로 소프트웨어 함수로, 에이전트가 API 요청(예: Google 검색 쿼리 실행), 웹 사이트 읽기, 프로젝트 보드에 액세스, 계산 수행, 데이터베이스에서 SQL 쿼리 실행 또는 보호된 환경에서 코드 작성 및 실행과 같은 작업을 수행할 수 있게 합니다. 도구는 대규모 모델이 외부 세계와 상호작용할 수 있게 합니다. 모델은 원하는 결과를 달성하기 위해 실행할 함수를 결정할 수 있습니다.\n\nReAct 로직은 각 언어 모델 프로세스 단계에서 'Thought', 'Act/Action' 및 'Observation' 요소를 포함합니다. 'Thought'는 추론을 통해 다음 작업과 그 근간에 있는 결정을 향상시킵니다. 'Action'은 어떤 도구를 사용할지와 어떤 매개변수를 사용할지를 지정하며, 'Observation'은 'Action'에 따라 도구 실행 결과를 포함합니다. 초기 조사에 대한 답변을 위해 이러한 단계들이 필요한 정보가 수집될 때까지 반복됩니다.\n\n![이미지](/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_1.png)\n\n<div class=\"content-ad\"></div>\n\n# OpenAI에서 구현됨\n\nOpenAI의 중요한 역할은 에이전트의 잠재력에 대한 인식으로 두드러지며, 이를 기능으로 통합하여 ChatGPT 플러그인 스토어를 열어두었습니다. 또한, GPT-XX-0631 이후의 OpenAI 모델 업데이트는 JSON 출력을 더 잘 제공할 수 있도록 지속적으로 개선되었습니다. 이 향상은 도구, 플러그인 또는 기능을 신뢰할 수 있게 실행하는 데 주로 필수적입니다. 그 이후 API는 또한 GPT 모델 쿼리에서 반환된 텍스트 블록과 별도로 함수의 정의 및 실행을 허용합니다. 일반적으로 JSON 출력은 함수 실행에 필요한 매개변수를 정의하므로, 전통적인 오픈 소스 프로젝트의 경우 (MistralAI와 같이 API 클라이언트에서 함수 호출을 제공하는 몇 가지 예외를 제외하고) 출력에서 추출해야 합니다.\n\n# 자율 에이전트 워크플로우\n\n자율 에이전트로 돌아와서, 클래식 ReAct 에이전트가 특정 작업이나 질문을 다루는 데 설계되었다면, 자율 에이전트 워크플로우는 한 걸음 더 나아갑니다. 작은 작업이 아니라 넓은 목표가 초기 입력으로 표현됩니다. 일반적으로 단일 프로세스 단계는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- (LLM) - 작업/목표를 하위 작업으로 분해합니다.\n- 작업을 작업 풀에 추가합니다.\n- (LLM) - 작업 풀을 우선 순위로 나열합니다.\n- 가장 중요한 작업을 선택합니다.\n- (LLM-Agent) - 가장 중요한 작업을 완료합니다.\n- (LLM-Agent) - 다음 작업을 정의합니다.\n\n다시 말해, '에이전트'들은 이제 '프로젝트 관리 에이전트'에 의해 조정되며 큰 작업을 목표를 달성할 때까지 처리 가능한 작은 작업으로 나누어줍니다. 이 프로세스는 새로운 도구를 만들고, 새로운 에이전트를 조정하고, 정보를 단기 또는 장기 기억에 저장하는 것을 포함합니다. 이 이터레이션 프로세스는 이론적으로 거의 모든 작업을 해결할 수 있습니다. 사용자들은 대형 언어 모델 (GPT-4)에 의해 생성된 창의적인 솔루션을 보고했습니다. 워크플로는 웹사이트나 프로필을 생성할 때 캡차를 우회하기 위해 작업에 막혔는데, '인간'을 위해 작업 플랫폼 (Fiverr)에 작업을 게시하여 문제를 해결했습니다.\n\n# 다중 에이전트 협업 워크플로\n\n우리는 독립적인 에이전트 워크플로가 개별 에이전트의 능력을 활용하여 특정 기능을 수행하는 능력을 증진함으로써 복잡한 작업을 해결할 수 있는 능력을 크게 향상시켰다는 것을 보았습니다. 그러나 이러한 기술의 진정한 잠재력은 독립적인 에이전트보다는 다중 에이전트의 협력 노력에 더 명백해집니다.\n\n<div class=\"content-ad\"></div>\n\n다중 에이전트 협업은 특화된 역량을 갖춘 각종 에이전트들이 협력하여 단일 에이전트 시스템으로는 해결할 수 없는 복잡한 목표를 달성하는 것을 포함합니다. 이 방식은 여러 에이전트의 집단적 강점을 활용하여 도전적인 문제에 대한 더 정교하고 확장 가능하며 유연한 해결책을 제공합니다. 성공적인 다중 에이전트 협업의 핵심은 다양한 에이전트의 행동을 조율하고 공통 목표를 향한 효과적인 의사 소통과 협력을 보장하는 능력에 있습니다.\n\nAutoGen은 대규모 언어 모델(LLMs)을 활용하는 워크플로의 조율, 최적화 및 자동화를 촉진하는 Microsoft의 프레임워크입니다. AutoGen의 핵심 아이디어는 특화된 역할과 기능을 갖춘 각 에이전트들이 더 효과적으로 협력할 수 있는 시스템 설계를 용이하게 하는 것입니다. 이러한 에이전트들은 자율적으로 작업을 수행하거나 서로 간 대화를 나누며, 프로그래밍 및 사람 사용자 또는 다른 도구로부터의 입력을 바탕으로 결정을 내릴 수 있습니다. 이는 단일 에이전트나 순수히 인간 팀이 다루기 어려운 복잡한 문제를 해결하는 더 동적이고 유연한 방식을 가능케 합니다.\n\nAutoGen은 두 가지 주요 기능을 통해 이를 실현합니다:\n\n- 개발자가 특화된 기능과 역할을 갖춘 에이전트들의 집합을 정의함으로써, 이러한 에이전트들의 모듈화와 재사용성을 강화합니다.\n- 에이전트 간 상호작용 행동을 위한 프레임워크를 수립하여, 미리 정의된 프로토콜 또는 자동화된 채팅을 통해 효과적으로 소통하고 협력할 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_2.png)\n\n# 도전과 제한 사항\n\n클래식 ReAct 에이전트와 비슷한 원칙으로 디자인된 에이전트들은 능력에 제한이 있습니다. 이 제한은 토큰 창문 제약 때문에 발생하는데, 여기서는 어떻게 그리고 언제 사용해야 하는지에 대한 설명이 정의될 수 있는 기능의 수가 제한됩니다. 단일 작업 또는 도구로 구비된 에이전트는 만족스러운 결과를 산출할 수 있지만, 도구의 수가 증가함에 따라 신뢰성이 감소합니다.\n\n토큰 창문의 확장이 있더라도, \"중간\" 문제가 지속됩니다. 즉, 입력 프롬프트의 중간에 위치한 정보가 덜 주목을 받습니다. 따라서 16,000 토큰을 초과하는 문맥 창문 내에 수백 개의 도구를 수용할 수 있는 잠재력이 있더라도 대부분은 여전히 간과됩니다. 게다가, 의사 결정은 여전히 과정에서 중요한 도전 과제입니다. 제한된 도구 수가 있더라도, MultiActionAgents는 적합한 도구를 선택하거나 모든 가능한 도구를 활용해야 한다고 믿는 것에 어려움을 겪습니다.\n\n<div class=\"content-ad\"></div>\n\n# 자율 에이전트의 과제\n\n자율 에이전트 개발이 미진한 이유 중 하나는 그들이 만들어내는 오픈 루프 시스템에 따른 비용 때문입니다. 보통 이러한 워크플로우는 가장 효율적인 해결책으로 이끌어주지 않습니다. 대신 계속하여 새로운 작업을 정의합니다. 오케스트레이션 에이전트들은 작업 풀을 모니터링하는 데 어려움을 겪어야 합니다. 한 번 이상으로 정의된 작업이 없도록 보장하는 것이 중요합니다. 동시에 최선의 결과를 얻으려면 최신 모델인 GPT-4를 사용해야 하며, 간단한 목표조차도 수천 번의 LLM 호출을 유발할 수 있어 빠르게 비용이 증가할 수 있습니다.\n\n뿐만 아니라, 시스템은 종종 막다른 곳에 이르게 되어 폐쇄 루프를 만들어냅니다:\n\n- 작업 A는 작업 B와 C에 의해 완료되어야 하며,\n- 그리고 작업 C의 분할 결과로써 작업 A가 다시 정의되는 일이 발생합니다.\n\n<div class=\"content-ad\"></div>\n\n모델 업데이트의 관찰 가능한 저하와 게으름으로 문제가 악화되고 있습니다. 저는 연구를 수행하고 발견물로부터 PowerPoint 프레젠테이션을 작성할 수 있는 반자동 워크플로우를 개발했었는데, GPT-3.5 turbo로 시도 중 3번 중 2번은 성공했습니다. 하지만 GPT-4에서는 이러한 결과를 재현할 수 없었습니다.\n\nLangChain의 미리 정의된 agent + 도구킷에서도 비슷한 상황이 나타납니다. 예를 들어, Pandas 및 SQL agent는 2023년 중반까지 신뢰할 수 있는 결과를 제공했지만 이제 80%의 경우에 오류로 이어지고 있습니다. 이는 agent의 인기가 점차 감소하거나 적어도 침체로 이어지고 있다는 것을 보여줍니다.\n\n# 해결 방안\n\n덜 발전된 모델일지라도 실행 로직과 프롬프트 엔지니어링을 통해 개선의 여지가 있습니다. 한 가지 전략은 MultiActionAgent를 사용하는 대신 agent가 수행할 수 있는 작업 범위를 좁히는 것입니다. 각 도구에 대해 SingleActionAgent가 생성되고, 적합한 SingleActionAgent를 선택하는 Routing Agent가 지정됩니다. 이 방법은 GPT-3.5 turbo와 같이 강력하지 않은 모델에도 만족스러운 결과를 얻을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n그러나 이 방법은 이전의 종단 간 에이전트 전략과 비교해 개발 노력이 상당히 많이 필요합니다. 더 비용 효율적인 모델을 활용하도록 솔루션을 분해하면 재정적으로 한 번에 더 많은 LLM 호출을 할 수 있습니다. 이를 통해 주요 투표의 실행이나 자가 비평 방법을 사용하여 하나의 LLM 호출에 대한 고려 사항의 반복적인 유효성 검증이 가능해집니다.\n\n프로세스를 분할하는 것이 가장 합리적인 해결책 중 하나로 보입니다. 현재 에이전트 워크플로우는 한 프롬프트 접근 방식에 의존하며, 결정, 문제 해결 및 추가 프로세스는 반복적으로 해결되고 동일한 프롬프트로 이루어지도록 의도되어 있습니다. 프롬프트 엔지니어링에 익숙한 사람들은 특정 상황에는 특정 프롬프트가 필요하다는 것을 알고 있습니다.\n\n# 전망\n\n현재 직면한 문제들이 시간이 지남에 따라 완화될 것입니다. 더 강력한 모델이 시장에 등장하고 오늘날의 최고 모델들이 더 저렴해질 것입니다. 시장이 많은 비판하는 점진적인 악화를 수정할 수 있는 능력을 가지고 있다고 확신합니다. 필요한 곳에는 해결책이 따를 것입니다. 의심의 여지 없이, 생성모델의 다음 성취는 우리의 능력을 혁신할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n현재 사용성이 매우 낮고 신뢰할 수 없지만, 미래는 모든 종류의 프로세스에서 활용될 AI 엔티티(에이전트)에 의해 형성될 것입니다. 컴퓨터 과학 분야의 최근 논문은 에이전트 프로세스(시행착오 학습)에 보상 학습 모델을 통합하는 해결책을 탐구하고 있습니다. 이는 에이전트가 보상 모델을 정의하고 훈련시킨 다음, 극도로 복잡한 작업을 해결하는 도구로 사용할 수 있다는 것을 의미합니다.\n\n에이전트와 자율 에이전트는 의심의 여지 없이 인공 일반 지능(AGI) 달성을 향한 여정에서 중요한 역할을 할 것입니다.\n\n## 출처:\n\n- Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., & Zhou, E. (2023). The Rise and Potential of Large Language Model Based Agents: A Survey. arXiv:2309.07864에서 검색됨\n- Yao, S., et al. (2022). REACT: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629에서 검색됨\n- LangChain. (n.d.). 에이전트 유형: REACT. LangChain 문서에서 확인됨\n- Significant-Gravitas. (n.d.). AutoGPT. GitHub에서 확인됨\n- Microsoft Research. (n.d.). AutoGen: 다음 세대 대형 언어 모델 애플리케이션 활성화. Microsoft Research 블로그에서 확인됨\n- Microsoft Research. (n.d.). AutoGen 프로젝트 페이지. Microsoft Research 프로젝트에서 확인됨","ogImage":{"url":"/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_0.png"},"coverImage":"/assets/img/2024-06-23-TheRiseandFallofAutonomousAgents_0.png","tag":["Tech"],"readingTime":8},{"title":"대형 언어 모델LLM로 민감한 데이터 처리하는 방법 총정리","description":"","date":"2024-06-23 19:28","slug":"2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels","content":"\n\n## 목차\n\n소개\n\n데이터 민감도는 무엇을 정의하고 있으며 누가 정의하고 있나요?\n데이터 익명화와 익명변환은 무엇인가요?\n민감한 데이터를 처리하기 위해 AI를 활용하는 것이 특별한 이유는 무엇인가요?\n\n실습 안내 — LLM 기반 데이터 프로파일러 구현\n\n<div class=\"content-ad\"></div>\n\n로컬 LLM 설정\n1. 도커를 사용하여 모델 서버 설정하기\n2. 프롬프트 빌드하기\nAzure OpenAI 설정\n\n고수준 솔루션 아키텍처\n결론\n참고 자료\n\n예상되는 하루 평균 데이터 생성량은 328.77 백만 테라바이트입니다. 대부분의 데이터는 데이터 주도형 애플리케이션으로 흐르며 매초마다 처리되고 풍부해집니다. 주요 제품들 사이에서 LLM의 채택과 통합이 확대되어 텍스트 데이터 활용의 사용 사례와 이점이 더욱 증가했습니다.\n\n대규모 데이터를 처리하는 조직은 민감한 데이터 처리 요구 사항을 준수하는 데 어려움을 겪습니다. 이는 데이터 보안이나 데이터 법률 및 규정을 준수하는 것과 관련이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n민감한 데이터 침해의 직접적 및 간접적 영향은 특히 민감한 데이터가 관련될 때 기관에 중대한 재정적 결과를 초래할 수 있습니다. 이는 즉각적인 비용 영향을 넘어서 해당 기관의 고객 기반의 신뢰와 충성을 흔들어놓을 수 있습니다.\n\n![Image](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_0.png)\n\n# 무엇이 그리고 누가 데이터의 민감성을 정의할까요?\n\n민감한 데이터는 데이터 보호와 개인 정보 보호 맥락에서 중요한 개념입니다. 높은 수준에서 민감한 데이터는 비밀유지되고 무단 접근으로부터 보호되어야 하는 정보로 이루어져 있습니다. 민감한 데이터의 중요성은 그것을 보호하기 위해 마련된 법률 및 규정들에 의해 강조됩니다. 예를 들어, EU의 일반 데이터 보호 규정(GDPR)과 캘리포니아 소비자 개인 정보 보호 법(CCPA) 등 다양한 국가에서 적용되는 법률 및 규정이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n민감한 데이터에는 다음과 같은 다양한 정보 카테고리가 포함됩니다:\n\n- 개인 식별 가능 정보 (일반 데이터 보호 규정)\n- 보호된 건강 정보 (건강 보험 이동성 및 책임성 법)\n- 교육 정보 (가족 교육 권리 및 개인 정보 보호 법)\n- 기밀 비즈니스 정보\n- 금융 정보\n- 고용 정보\n- 법적 정부 발급 정보\n- ...\n\n게다가 GDPR은 특히 개인 식별 가능 정보(PII)와 관련하여 민감한 개인 데이터의 특별한 카테고리를 다음과 같이 명시하고 있습니다:\n\n- 인종이나 민족 출신\n- 정치적 견해\n- 종교나 철학적 신념\n- 노동 조합 가입\n- 유전자 정보\n- 생체 인식 정보\n- 건강 정보\n- 성생활이나 성적 취향\n\n<div class=\"content-ad\"></div>\n\n법적 프레임워크에 따라 민감한 데이터가 무엇인지에 대한 명확한 설명과 경계가 존재하지만, 기관이 운영중인 도메인이나 섹터에 따라 이러한 내용은 더 많은 민감한 기밀 정보를 포함할 수 있습니다. 이로써 기관은 자체 데이터 기밀성 분류를 정의하고 해당 분류를 데이터 민감도 도구에 통합하거나 사용자 정의 솔루션에 통합하는 것으로 나아갈 것입니다.\n\n# 데이터 익명화와 익명도\n\n익명화된 데이터란 특정 개인과 연결되지 않는 데이터를 의미하며, 추가보조 데이터를 통해서도 그러한 연결이 불가능합니다. 완전히 익명화된 데이터는 GDPR과 같은 개인정보 보호 규정의 적용 범위에서 벗어납니다.\n\n추가 정보와 함께 특정 개인에게 속한다고 할 수 있는 데이터는 익명화된 것이라고 합니다. 이는 원본 데이터를 가짜 식별자로 대체하는 것만으로 달성될 수 있으며, 필요한 경우 되돌릴 수도 있습니다. 예를 들어, 암호화는 데이터 익명화의 한 방법입니다.\n\n<div class=\"content-ad\"></div>\n\n개인정보 의사화는 GDPR의 제 4조에 정의되어 있습니다:\n\n개인정보 익명화 및 의사화는 민감한 데이터 식별 후 수행되는 작업입니다. 데이터를 익명화 또는 의사화해야 하는지는 특정 사용 사례 및 미래 어느 시점에서 익명화를 반대로 해야 하는지 여부에 따라 다릅니다.\n\n![Image](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_1.png)\n\nLLM은 데이터 익명화에서 일부 역할을 합니다. 텍스트 데이터에 마스크를 적용할 수 있지만 별도의 복잡성 없이는 모든 요구 사항, 특히 식별 해제와 관련된 요구 사항을 모두 충족시킬 수 없습니다.\n\n<div class=\"content-ad\"></div>\n\n위에 제시된 정의는 나중에 이 용어들을 참조할 때 충분한 간략한 개요를 제공합니다. 주제를 더 깊게 파헤치지 않아도 됩니다. 많은 기사들이 이것을 자세히 다루고 있고, 나는 다른 기사 중 하나에서 이를 더 깊이 설명합니다.\n\n# 민감한 데이터를 처리하는 데 인공 지능을 활용하는 것이 무엇이 특별한가요?\n\n민감한 텍스트 데이터는 대용량 텍스트 필드와 문서에 간접적으로 포함될 수 있어 휴리스틱 기술을 사용해서 감지하기 어려울 수 있습니다. 이러한 기술과 방법들은 주로 미리 정의된 규칙과 패턴(예: 명명된 개체 인식)을 의존하므로 그 능력이 제한될 수 있습니다.\n\n대부분의 경우 중요한 민감한 데이터는 문장 안에서 미묘하게 제시되지 않습니다. 메시징 애플리케이션, 고객 지원 서비스, 이메일 등에서 생성된 데이터는 전체 텍스트의 맥락 안에 민감한 데이터가 포함될 수 있습니다. 이러한 복잡한 상황은 데이터 마스커들이 구문 분석된 텍스트를 기억하고 맥락을 이해해야 한다는 것을 요구합니다. 그러면 휴리스틱 기술의 사용이 방해되어 모든 개인정보 보호 요구 사항을 충족시키기에 부적합하다고 판단됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 실습 안내 — LLM-Powered 데이터 식별기 구현\n\n적절한 프롬프트를 제공하면 고급 LLM 모델은 훈련 데이터를 사용하여 문장 내의 민감한 데이터를 식별하고 가리는 데 성공할 수 있습니다. 이 예제에서는 Llama2 Model과 Azure의 GPT4 Model을 사용하여 데이터 민감도 식별기를 설정하는 방법을 시험해보겠습니다. LangChain 프레임워크를 사용하여 모델을 검색하고 프롬프트를 제공할 것입니다.\n\n우리는 모델을 로드하고 응용 프로그램을 실행하는 데 사용할 환경을 설정하는 것부터 시작하겠습니다.\n\n이를 위해 먼저 Python 3.8 이상이 설치되어 있는지 확인하세요.\n\n<div class=\"content-ad\"></div>\n\n# Local LLM Setup\n\n이 섹션에서는 데이터 민감도 감지를 위한 로컬 Llama2 모델을 준비하고 실행하는 데 필요한 단계를 나열합니다.\n\n## 1. 도커를 사용하여 모델 서버 설정\n\nLangChain과 통신할 수 있는 모델 서버를 설정하기 위해 도커를 사용할 것입니다. 모델을 로컬로 설정하는 다른 방법도 있으며, 여기에서 더 많은 정보를 찾을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n우선 도커가 설치되어 실행 중인지 확인한 후, 아래 명령을 실행하여 이미지를 다운로드하고 모델 서버를 설정하세요.\n\n```js\ndocker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n```\n\n서버가 작동 중인지 확인하려면 브라우저에서 http://localhost:11434 링크를 열면 \"Ollama is running\" 문구가 표시됩니다.\n\n컨테이너가 실행되면 모델을 설치하세요. 다양한 모델을 테스트하고 싶다면 여기서 확인할 수 있는 목록을 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n```js\n도커 실행 -it 올라마 올라마 실행 llama2\n```\n\n설치가 완료되면 모델을 로드하고 Python 스크립트를 설정할 것입니다.\n\n먼저 LangChain을 설치해 봅시다.\n\n```js\npip install langchain\npip install langchain-community\n```\n\n<div class=\"content-ad\"></div>\n\n스크립트에서 필요한 패키지를 가져오세요.\n\n```js\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_community.llms import Ollama\n```\n\n모델을 불러오세요.\n\n```js\nllm = Ollama(model=\"llama2\")\n```\n\n<div class=\"content-ad\"></div>\n\n## 2. 프롬프트 작성\n\nLLM이 이미 문장의 내장된 맥락을 이해할 수 있는 기능을 갖추고 있다고 생각할 때, 과제가 의도대로 실행되는 것을 보장하는 방식으로 프롬프트를 구성하는 것이 중요합니다.\n\n예를 들어, 다음 사항을 LLM이 보장해야 합니다:\n\n- 문맥을 이해하는 데 키워드 일치만 의존하지 않고 민감한 정보를 식별하는 능력\n- * 데이터 보호 법과 규정 (GDPR, CCPA 등)을 준수하는 감지\n- 모델이 정밀도와 재현율 사이의 균형을 유지\n- 문장 구조가 변경되지 않고 감지된 섹션이 처리되는 것을 보장\n- 추가적인 내용 없이 제공된 데이터만 반환되는 것을 보장\n\n<div class=\"content-ad\"></div>\n\n```js\ntemplate = \"\"\"\n민감한 데이터 식별자 및 마스킹 기능이 있습니다. \n텍스트에서 민감한 정보를 식별하고 \"****\"를 사용하여 마스킹하는 기능이 가능합니다. \n민감한 데이터는 종종 명시적으로 언급되지 않을 수 있으며, 텍스트의 맥락에 내재될 수도 있습니다(예: 건강, 금융, 주소 등의 주제)\n개인 식별 데이터가 감지되고 마스킹되도록 하십시오.\nGDPR, CCPA 및 HIPA와 같은 데이터 보호 법률 및 규정을 고려하도록 하십시오.\n입력 텍스트가 수정되거나 변경되지 않도록 하고 감지된 민감 정보만 마스킹하도록 하십시오.\n마스킹한 정보에 대한 높은 신뢰도를 보장하십시오.\n반환된 내용에는 필요한 마스킹이 적용된 입력 텍스트 이외의 내용이 포함되어서는 안 됩니다.\n민감한 텍스트가 감지되지 않으면 추가 콘텐츠 없이 입력값을 그대로 반환하십시오.\n\n문장:\n{sentence}\n\"\"\"\n\noutput_parser = StrOutputParser()\n\n# 설정 구문\nprompt = PromptTemplate.from_template(template)\n\n# 체인 생성\nchain = prompt | llm | output_parser\n```\n\n요청은 모델이 지시된대로 작업을 성공적으로 실행할 수 있도록 필요한 요구사항을 정의합니다. 국가에 따라 준수해야하는 특정 요구 사항에 대한 추가 매개 변수화가 가능합니다. 다음과 같은 것이 추가될 수 있습니다.\n\n- 어떤 법률을 준수해야 하는지 명시하는 것은 국가에 따라 중요합니다.\n- 민감한 데이터 마스킹 시 운영할 때의 신뢰도\n\n첫 번째 예제를 실행해 봅시다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\nsentence = \"\"\"\n지난 주, 여름 계획을 논의하는 동안,\n마이크는 바칼리로의 솔로 여행을 드디어 떠날 것을 시사했어요\n그는 보너스를 받은 후에 모아놓은 돈으로요.\n그의 보너스로 1만 달러 이상을 받았어요\n\"\"\"\n\n# 감지 실행\nresponse = chain.invoke({'sentence':sentence})\n\n# 최종 응답 출력\nprint(response)\r\n```\n\n![이미지](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_2.png)\n\nLlama2 모델의 응답은 민감한 데이터를 식별하고 지시에 따라 가리는 능력이 있었습니다. 모든 이름이 가려지고, \"보너스\"와 그 금액도 가렸으므로, 해당 모델이 숫자가 민감한 금융 정보와 관련되어 있는 것을 감지할 수 있었음을 나타냅니다.\n\n참고사항\n\n\n<div class=\"content-ad\"></div>\n\n* 데이터 보호 법률과 규정이 정기적으로 업데이트되므로, 모델은 최신 버전에 액세스할 수 없습니다.\n\n** 응용 프로그램은 제공된 법률 및 규정 문서의 특정 버전에서 실행되도록 RAG (검색 증강 생성)을 활용하도록 조정될 수 있습니다.\n\n# Azure OpenAI 설정\n\nAzure OpenAI의 GPT4 모델을 사용하려면 Azure OpenAI 서비스를 생성해야 합니다. Microsoft은 여기에서 리소스를 생성하는 단계에 대해 명확한 설명서를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n리소스를 생성한 후에는 배포로 이동하여 기본 버전을 사용하여 gpt4 모델을 배포합니다.\n\n![image](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_3.png)\n\n모델이 성공적으로 배포되면 API에 액세스하기 위해 사용된 Azure OpenAI 키를 검색하는 것을 잊지 마세요.\n\nAzure OpenAI와 작업하기 위해 필요한 LangChain 패키지를 설치해봅시다.\n\n<div class=\"content-ad\"></div>\n\n```js\npip install langchain-openai\n```\n\n로컬 설정을 구축할 때와 같은 단계를 따라가되, Azure OpenAI와 작업하기 위해 import 및 프롬프트 템플릿을 조정해야 합니다.\n\n```js\nimport os\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# 필요한 환경 변수 추가\nos.environ[\"OPENAI_API_VERSION\"] = <API VERSION>\nos.environ[\"AZURE_OPENAI_API_KEY\"] = <Your AZURE OPENAI KEY>\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = <Your AZURE  OPENAI ENDPOINT>\n\n# 프롬프트 템플릿 정의\ntemplate = \"\"\"\n당신은 민감한 데이터 식별자 및 마스커입니다.\n텍스트에서 민감한 정보를 식별하고 \"****\"를 사용하여 마스킹하는 능력이 있습니다.\n민감한 데이터는 텍스트의 맥락 속에 내장될 수 있으며 항상 명시적으로 언급되지 않을 수도 있습니다(예: 건강, 재정, 주소와 관련된 주제 등).\n개인 식별 가능 데이터가 감지되고 마스킹되었는지 확인하세요.\n데이터 보호법과 규정(GDPR, CCPA, HIPA 등)을 고려해 감지가 이뤄지도록 하세요.\n입력 텍스트가 변경되거나 수정되지 않고 감지된 민감한 정보만 마스킹되도록 하세요.\n마스킹된 민감한 정보에 대해 높은 신뢰도를 보장하세요.\n반환된 콘텐츠에 요구된 마스킹이 적용된 입력 텍스트 이외의 것이 포함되어서는 안 됩니다.\n민감한 텍스트가 감지되지 않은 경우에는 추가 내용 없이 입력을 그대로 반환하세요.\n\n문장:\n{sentence}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\n# 모델 선택\nllm = AzureChatOpenAI(\n    azure_deployment=\"gpt4\",\n)\n\n# 체인 설정\nchain = prompt | llm\n\nsentence = \"\"\"\n지난주 여름 계획을 논의하던 중, \n마이크가 발리로의 단독 여행을 드디어 가기로 하는 듯하다는 신호를 주었어요.\n보너스가 들어와서 이제 막 그 여행을 위해 돈 모았다고 하더라구요.\n그는 1만 달러 이상의 보너스를 받았어요.\n\"\"\"\n\n# LLM 실행\nresponse = chain.invoke({'sentence':sentence})\n\nprint(response.content)\n```\n\n<img src=\"/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_4.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n3번의 테스트를 거친 후에도 출력 결과로 나온 것은 모델이 민감한 정보를 식별하고 필요한 곳에서 이를 가려 주었지만 문장의 내용을 변경하지 않았다는 것을 보여줬어요.\n\n다른 예시를 사용해 봅시다:\n\n```js\nsentence = \"\"\"\n에마와의 통화 중, 그녀가 내년에 월 $3000으로 인상된 임대료 때문에 이사를 가겠다고 가벼운 말투로 언급했어. 그녀가 이 세부 정보를 비공개로 유지한 걸 알았어. 어떤 재정적 걱정 때문일 수도 있어.\n\"\"\"\n```\n\n![AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_5.png 이미지](/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_5.png)\n\n<div class=\"content-ad\"></div>\n\n두 모델 모두 제공된 예시에서 민감한 데이터를 정확하게 식별할 수 있었습니다. 그러나 이 예시에는 상대적으로 짧은 텍스트만 포함되어 있었습니다. 보다 긴 텍스트는 특히 민감한 데이터가 보다 나중에 노출되는 경우에 결과에 더 큰 영향을 줄 수 있습니다. 모델이 숫자와 해당 문맥 간의 연결을 놓칠 수 있기 때문입니다.\n\n다음 섹션에서는 휴리스틱과 LLMs를 모두 활용하는 민감도 감지의 고수준 하이브리드 솔루션 접근 방식을 논의할 것입니다.\n\n# 고수준 솔루션 아키텍처\n\n효율적이고 신뢰할 수 있는 민감한 데이터 식별 LLM 애플리케이션을 개발하는 것은 이미 중요한 이정표입니다. 그러나 이러한 애플리케이션을 데이터 아키텍처에 배치하는 것은 신중히 고려되어야 합니다. 대량 및 속도가 빠른 텍스트 데이터를 보유한 기업은 해결책이 비효율적이 되어 큰 비용과 노력을 필요로 할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다이어그램은 각 단계에서 다양한 구성 요소에 의해 텍스트 데이터가 처리되는 방법을 단계별로 보여줍니다. 이를 통해 완전히 준수된 데이터 상태를 달성할 수 있습니다.\n\n이러한 해결책을 구축할 때 고려해야 할 중요한 질문들:\n\n- 사용 사례가 LLM을 배포하고 유지하는 과부하를 정당화하는가? 휴리스틱 기술이 충분한가?\n- 시장에 사용 사례에 대한 기존 솔루션이 있는가?\n- 데이터의 대기 시간과 가용성 요구 사항을 충족할 것인가?\n- 데이터는 컨텍스트 내에서 중요 데이터를 포함하고 있는가?\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n기존 휴리스틱 접근 방식을 활용하면 민감한 데이터를 감지하는 데 도움이 될 수 있지만, 문장의 맥락 속에 간접적으로 나타나는 민감한 데이터를 표준 방법으로 감지하기 어려운 상황에서는 어려움이 있습니다. 큰 양의 텍스트를 이해하는 내재 기능을 갖춘 LLM(Large Language Models)은 민감한 데이터 감지 및 분류 문제에 대처하기 위한 차세대 도구로 기능할 수 있습니다.\n\n본 문서에서는 LLM이 이 문제에 대해 대상화될 수 있는 예시를 보여주었습니다. 예시의 프롬프트는 LLM이 과업의 일반적 요구 사항을 충족할 수 있는 능력이 있음을 입증했습니다. 휴리스틱과 LLM 간의 혼합 접근 방식 도입은 데이터 솔루션 아키텍처에 보여진 것처럼 더 나은 결과를 보장하고 추가적인 안전장치를 제공할 수도 있습니다. 이 문서는 LLM을 사용하여 민감한 데이터를 처리하는 가능성에 대한 일부 조감도를 보여주었고, 일부 추가적인 사용 사례와 가능성은 다음과 같습니다:\n\n- RAG를 위해 데이터 카탈로그 메타데이터 통합\n- 민감도 수준에 대한 분류 도입\n- 도메인별 민감한 데이터 지식 통합\n- …\n\n- 새 이야기를 게시할 때 알림을 받으려면 구독해주세요.\n- LinkedIn에서 언제든지 연락 주세요.\n\n<div class=\"content-ad\"></div>\n\n만약 민감한 데이터 처리에 대한 보다 자세한 아키텍처에 관심이 있다면 — 내 다른 기사들을 여기에서 확인해보세요.\n\n# 참고 자료\n\n유럽 의회 및 이사회 2016년 4월 27일 제 2016/679 규정 (일반 데이터 보호 규정) (EEA와 관련된 텍스트)에 관한 자연인의 보호에 대한 데이터 처리 및 그와 같은 데이터의 자유 이동의 처리 및 95/46/EC 지침의 폐지. General Data Protection Regulation (GDPR) — 공식 법적 텍스트 (gdpr-info.eu)에서 확인할 수 있습니다.\n\n데이터 침해 비용 2023 IBM. 다음 위치에서 확인 가능: https://www.ibm.com/security/data-breach (2024년 3월 1일 열람).\n\n<div class=\"content-ad\"></div>\n\n| Source | Title | Date |\n|--------|-------|------|\n| LangChain | [Website](https://www.langchain.com) | - |\n| California Consumer Privacy Act (CCPA) | State of California — Department of Justice — Office of the Attorney General | March 13, 2024. [Link](https://oag.ca.gov/privacy/ccpa) |\n| Taylor, P. | Data Growth Worldwide 2010–2025 | November 16, 2023. Statista [Link](https://www.statista.com/statistics/871513/worldwide-data-created/) |","ogImage":{"url":"/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_0.png"},"coverImage":"/assets/img/2024-06-23-AllYouNeedtoKnowaboutSensitiveDataHandlingUsingLargeLanguageModels_0.png","tag":["Tech"],"readingTime":11},{"title":"LangChain, Semantic Kernel, AutoGen 최신 비교 및 분석","description":"","date":"2024-06-23 19:27","slug":"2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore","content":"\n\nJane Huang과 Kirk Li가 씀\n\n이 기사에서는 대형 언어 모델 (LLMs)을 활용한 응용 프로그램을 개발하기 위한 다양한 전략을 비교 분석하며, OpenAI의 Assistant API, LangChain, Semantic Kernel, AutoGen 등과 같은 프레임워크를 아우르고 있습니다. LLMs의 동적인 환경에서는 적절한 프레임워크를 선택하는 것이 이러한 모델을 응용 프로그램에 매끄럽게 통합하기 위해 중요합니다. 다행히 LLM을 백엔드로 하는 시스템을 구축하는 데는 처음부터 시작할 필요가 없습니다.\n\n![이미지](/assets/img/2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore_0.png)\n\nOpenAI의 Assistant API는 응용 프로그램 내에서 AI 어시스턴트를 개발하는 데 도움이 되는 강력한 도구로 부상했습니다. 제공하는 편의성에도 불구하고 일부 유경험 개발자들은 비용과 실제 서비스에서의 관측 가능성 문제에 대해 우려를 표명하며, 잠재적인 단점에 대해 거론했습니다. Assistant API는 개발 노력을 크게 줄이지만, 가격 모델의 장기적 지속 가능성에 대한 불확실성이 남아있습니다.\n\n<div class=\"content-ad\"></div>\n\n대조적으로 LangChain, Semantic Kernel 및 AutoGen과 같은 대체 프레임워크는 개발자들에게 AI 응용 프로그램에 대한 제어와 유연성을 제공합니다. 이러한 대안들은 각각 특정 선호도와 프로젝트 요구 사항을 고려한 선택지를 제시합니다. 오늘날 사용 가능한 또 다른 주목할 만한 옵션은 SDK를 활용하지 않거나 OpenAI에 Assistant API로 복잡성을 맡기지 않고 작동하는 \"자체 구축\" 솔루션입니다. 이러한 선택지는 유연성뿐만 아니라 필요한 개발 노력 수준에서도 차이를 나타냅니다. 다양한 대안을 제공함으로써, 본 글은 개발자들이 자신의 프로젝트에 대한 독특한 요구 사항과 포부에 부합하는 판단력을 가지도록 돕고자 합니다.\n\n오늘날, 우리의 초점은 LangChain, Semantic Kernel, AutoGen과 같은 프레임워크가 제공하는 옵션에 주변합니다. 이러한 프레임워크는 각각 다른 선호도와 프로젝트 요구 사항을 고려한 것입니다. 이 글의 저자들은 이 글에서 논의된 프레임워크의 어떤 측면에 대해서도 새로운 것을 주장하지 않음을 유의해주십시오. 이 내용들은 링크를 통해 공개된 문서에서 출처를 얻은 것으로, 저자들이 다양한 프레임워크에 대한 학습 및 프로젝트 경험을 요약한 것입니다. 인공 지능 기술의 급격한 발전으로 인해, 본 글이 항상 시간에 따른 최신 발전을 포함하지 못할 수 있다는 점을 인식하는 것이 중요합니다.\n\n일반적으로, LangChain과 Semantic Kernel은 LLMs를 응용 프로그램에 통합하는 공통 목표를 가지고 있지만 접근 방식과 기능에서 차이가 있습니다. LangChain은 메모리와 컨텍스트 창을 명시적으로 구성해야 하지만 Assistant API는 이러한 측면을 자동화합니다. OpenAI의 Assistant API는 개발 노력을 최소화하는 반면, LangChain과 Semantic Kernel과 같은 프레임워크는 AI 응용 프로그램에 대한 심층적인 이해와 제어를 원하는 개발자들에게 매력적입니다. 이러한 프레임워크들은 AI 모델과 기존 코드 간의 간극을 메우는 SDK를 제공함으로써 돋보입니다. 이러한 SDK는 실제 세계 조치와 AI 응답의 통합을 용이하게 하여, 복잡한 비즈니스 프로세스를 자동화할 수 있는 완전 자동화된 AI 에이전트 구축에 이상적인 솔루션입니다. 플러그인, 도구 및 콘넥터를 통한 확장성은 다양한 기존 코드를 원활하게 연결함으로써 다른 공급 업체의 AI 서비스를 통합할 때 유연성을 제공합니다.\n\n반면, AutoGen은 다중 에이전트 프레임워크로 위치하며, LangChain의 단일 에이전트 초점과는 다릅니다. 이는 다중 에이전트 협업을 특징으로 하는 애플리케이션을 생성할 수 있어, 복잡한 에이전트 상호작용을 지향하는 개발자들을 위한 다재다능한 옵션을 제공합니다. 이러한 차이를 이해하는 것은 프로젝트 요구 사항과 원하는 협업 기능에 따라 이러한 프레임워크 중에서 선택하는 개발자들에게 중요합니다. 2024년 1월 말에, LangChain의 창시자들은 에이전트 실행 시간을 맞추기 위해 설계된 또 다른 다중 에이전트 워크플로인 LangGraph를 소개했습니다. 이 출시는 AutoGen과 비교했을 때 마음의 모델에서 상당한 변화를 제시합니다. 핵심적인 차이점은 프레임워크가 에이전트를 구성하는 방식에 있습니다. LangGraph는 고유한 에이전트 및 이들의 전이 확률을 명확하게 정의하는 방식을 촉진하며, 그것들을 그래프로 묘사합니다. 이에 반해, AutoGen은 이 과정을 더 \"대화\"로 보고 있습니다. 더불어, LangGraph는 LangChain 생태계에 원활하게 통합되어 있습니다. 이 통합을 통해 사용자들은 모든 LangChain 통합을 활용하고 LangSmith 감시 기능을 활용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n우리의 비교 분석을 시작하기 위해, 테이블 1에 명시된 여러 프레임워크의 기본적인 특성을 자세히 살펴보겠습니다. (화면의 너비 제한으로 인해 현재 웹페이지에서 보이지 않는 전체 내용을 볼 수 있도록 스크롤 막대를 드래그해주세요). 이 분석에서는 특히 세 가지 최고로 인정받는 프레임워크를 비교합니다. 특정 작업을 위해 개발자들이 개발 프로세스 중에 활용할 수 있는 흥미로운 특화된 라이브러리들인 가이던스, 가드레일, 람마 인덱스, 타입챗과 같은 추가로 흥미로운 라이브러리들이 있습니다. 그러나 이 기사의 목적상 이러한 라이브러리들을 자세히 다루지는 않겠습니다.\n\n## 테이블 1: 기본 특성 개요\n\n## 테이블 2: 샘플 레슨\n\n인터넷에는 소개에 관한 많은 유익한 수업들이 온라인으로 찾아볼 수 있습니다. 몇 가지 예시는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n# 프레임워크의 구성 요소\n\n이제, 더 깊이 탐구하기 위해 표 3-13에 나타난 프레임워크의 다양한 구성 요소를 면밀히 검토하고 비교해 보겠습니다.\n\n## 표 3: 구성 요소 개요: 작업 조율\n\n## 표 4: 구성 요소 개요: 메모리 관리\n\n<div class=\"content-ad\"></div>\n\n## 테이블 5: 구성 요소 개요: 재사용 가능한 구성 요소\n\n## 테이블 6: 구성 요소 개요: 프롬프트 템플릿\n\n## 테이블 7: 구성 요소 개요: 문서 로더\n\n## 테이블 8: 구성 요소 개요: 문서 변환 및 분할\n\n<div class=\"content-ad\"></div>\n\n## 테이블 9: 구성 요소 개요: 호출 순서 구성\n\n## 테이블 10: 구성 요소 개요: 벡터 저장소\n\n## 테이블 11: 구성 요소 개요: 검색기\n\n## 테이블 12: 구성 요소 개요: 모델 입출력\n\n<div class=\"content-ad\"></div>\n\n## 테이블 13: 구성 요소 개요: 데이터 연결\n\n# 결론\n\nLLM(언어 모델 라이브러리)의 환경이 계속 발전함에 따라, 복잡한 AI 애플리케이션을 구축하려는 개발자들에게는 프레임워크 선택이 중요한 결정이 됩니다. Assistant API의 간편한 편리성이나 LangChain, LangGraph, Semantic Kernel, AutoGen과 같은 프레임워크가 제공하는 세밀한 제어라는 각 옵션은 각각의 장점과 고려해야 할 사항이 있습니다. 어떤 SDK를 사용할지 결정하는 것은 특정한 요구 사항, 선호도, 그리고 개발자의 목표뿐만 아니라 수행 중인 프로젝트의 성격에 달려 있습니다. 일반적인 해결책이 아니라 다양한 SDK들을 조화롭게 결합하여 사용하는 것이 종종 최적의 해결책일 수 있습니다. Semantic Kernel과 AutoGen의 원활한 통합에 대해 탐구한 John Maeda의 흥미로운 블로그 게시물과 함께, Matthew Bolanos는 오픈AI 어시스턴트를 통합하고 있으며 오픈AI 어시스턴트를 활용한 시각 등을 설명하는 \"Semantic Kernel의 미래: OpenAI 어시스턴트,\" \"OpenAI 어시스턴트: Semantic Kernel과 오픈AI 어시스턴트 사용에 대한 첫인상,\" 그리고 \"OpenAI 어시스턴트: 템플릿화된 어시스턴트 지시의 힘\" 시리즈를 Microsoft의 플랫폼에 발표하고 있습니다. Microsoft은 이미 OpenAI 어시스턴트 API를 사용하는 실험적인 구현을 갖고 있으나, 팀은 어떠한 모델로 만들어진 에이전트도 수용할 수 있는 에이전트 인터페이스의 완전한 추상화를 목표로 하고 있습니다. 이를 위해 Microsoft의 Semantic Kernel 팀 구성원들은 AutoGen팀의 연구 결과를 활용하여 에이전트가 팀으로 협업하는 시나리오를 포함한 다양한 경험을 수용할 수 있는 추상화를 개발하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n더욱 풍부한 대화를 위해 LangChain은 프레임워크와 OpenAI 어시스턴트 간 상호작용을 명료하게 설명하는 포괄적인 문서를 보급했습니다. Gagan Bansal은 OpenAI 어시스턴트를 AutoGen에 통합하는 것을 탐구함으로써 대화에 기여했으며, GPTAssistantAgent에 대한 통찰을 통해 이에 대해 자세히 논의했습니다. 이러한 동적인 환경에서 다양한 SDK 간의 협업 가능성에 대해 정보를 가지고 있는 것이 AI 애플리케이션에서 대형 언어 모델의 전체 잠재력을 이용하는 데 중요합니다.\n\nCasey Doyle가 작업을 검토하는 데 도움을 준 데 대해 감사드립니다.","ogImage":{"url":"/assets/img/2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore_0.png"},"coverImage":"/assets/img/2024-06-23-AcomparativeoverviewofLangChainSemanticKernelAutoGenandmore_0.png","tag":["Tech"],"readingTime":5},{"title":"대형 언어 모델LLM을 위한 토큰 마스킹 전략들","description":"","date":"2024-06-23 19:24","slug":"2024-06-23-TokenMaskingStrategiesforLLMs","content":"\n\n## 다양한 언어 모델에서 사용되는 다양한 가리기 기술, 그 이점 및 Pytorch를 사용하여 낮은 수준에서 작동하는 방법에 대해 자세히 알아보세요.\n\n![이미지](/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_0.png)\n\n토큰 마스킹은 분류 변형 및 생성 모델에서 언어 모델을 훈련하는 데 널리 사용되는 전략입니다. BERT 언어 모델이 소개했으며 많은 변형(RoBERTa, ALBERT, DeBERTa 등)에서 사용되었습니다.\n\n그러나 토큰 마스킹은 텍스트 손상이라는 큰 그룹 내의 전략입니다. BART 연구 논문에서는 다양한 텍스트 손상 전략을 사용하여 인코더-디코더 생성 모델을 훈련하는 많은 실험이 수행되었습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_1.png\" />\n\n텍스트 손상에 대한 다양한 기술을 논의하기 전에, 대형 언어 모델(Large Language Models, LLMs)의 모든 텍스트 손상 방법에 대한 표준 개념에 대해 이야기하겠습니다.\n\n# 지도 학습에서 자기 지도 학습으로\n\n대규모 양의 텍스트가 언어 모델의 초기 교육에 사용되며, 모델이 언어를 올바르게 표현하도록 학습하고, 이 지식을 그 매개 변수 가중치에 암묵적으로 저장합니다.\n\n<div class=\"content-ad\"></div>\n\n이 방대한 양의 텍스트는 학습을 위한 레이블이 있어야 합니다. 모델 입력 데이터를 처리한 후 참조 데이터를 사용하여 교차 엔트로피를 계산해야 합니다. 그러나 이렇게 많은 데이터에 주석을 다는 것은 현실적이지 않습니다. 따라서 우리는 자동 레이블 생성을 찾게 되었고, 지도 문제를 자가지도 문제로 전환하게 되었습니다.\n\n이 경우, 손상된 시퀀스는 모델의 학습 입력으로 작용하고, 기존 시퀀스의 전체 또는 일부가 학습 데이터의 레이블로 작용합니다. 이는 모델의 성격(인코더 또는 인코더-디코더)에 따라 다릅니다.\n\n# 손상 확률\n\n자동 레이블을 사용하여, 모델은 데이터에 주석을 달지 않고 각 학습 예제와 연결된 레이블을 학습합니다.\n\n<div class=\"content-ad\"></div>\n\n텍스트 손상(특히 토큰 마스킹, 토큰 삭제 및 텍스트 인필링에서)에서 각 단어는 일반적으로 15–20% 정도의 확률에 따라 손상될 것입니다. 이 확률은 모델이 각 문장의 맥락을 학습할 수 있도록 낮게 유지됩니다.\n\n문장 순서 바꾸기 또는 문서 회전과 같은 일부 텍스트 손상 기술은 특정 확률로 단어를 손상시키지 않습니다. 이로 인해 다른 손상 기술과 호환되도록 할 수 있습니다. 아래에서 논의할 것과 같이요.\n\n# 분류와 생성 사이의 차이점\n\n텍스트 손상을 통해 언어 모델을 학습할 때, 레이블은 분류 모델(인코더만)인지 생성 모델(인코더-디코더)인지에 따라 달라집니다.\n\n<div class=\"content-ad\"></div>\n\n분류 모델에서는 레이블을 사용하여 입력의 오염된 영역에만 주의를 기울입니다. 따라서 만약 문장 전체에서 단어가 마스킹되었다면, 레이블은 초기 시퀀스가 되어 오염된 시퀀스에만 주의를 기울입니다.\n\n생성 모델의 경우, 모델이 텍스트를 연속적으로 생성할 수 있어야 하므로 출력 레이블은 초기 정상 시퀀스가 되며 전체 시퀀스 자체에 주의를 기울입니다.\n\n# 설정\n\n이제 우리는 텍스트 오염으로 언어 모델을 학습할 때의 공통점을 간단히 소개했으니, 텍스트를 손상시키는 다양한 기술과 각 경우에 코드 예시를 제시하는 것을 논의해보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 서로 다른 전략들이 어떻게 작동하는지 보기 위해 코드 예제에서 문서로 시작하겠습니다. 우리는 전처리에 매우 유용한 여러 자연어 처리 도구를 갖춘 Stanford NLP에서 개발된 라이브러리인 Stanza를 사용할 것입니다.\n\n```js\nimport stanza\nstanza.download('en')\n\n# 예제에서 사용되는 텍스트\ntext = \"헌팅턴병은 유전적인 신경퇴행성 질환으로 헌팅틴 유전자 내 다형성 CAG 반복이 확장되면서 발생합니다. 번역 초기 인자 4E-BP의 인산화는 번역 조절의 변경으로 이어져서 원치 않는 단백질 합성과 신경 기능에 영향을 줍니다. 돌연변이 헌팅틴(mhtt) 유전자의 전사 결과는 잘 알려지지 않았습니다. 발병 연령의 가변성은 성인과 소아형 헌팅턴병을 분리하는 중요한 요소입니다. 고령 유전자, 어머니의 보호(즉, 과도한 아버지의 유전), 우수한 노화 유전자 및 환경 임계치가 고려되는 요소입니다. 분자 병인학에 중점을 둔 부분으로는 운동 장애, 인지 장애 및 신경 정신 장애가 포함됩니다. 진단 부분도 고려되었습니다. 이는 유전자 검사 및 주요 및 이차 증상을 포함합니다. 헌팅턴병의 유전학과 병리학에도 주목합니다.\"\n\n# 각 다른 문장을 리스트 요소로 얻기 위해 Stanza 모델을 사용할 것입니다.\nnlp = stanza.Pipeline('en', use_gpu=False)\ndoc = nlp(text)\nsentences = [sentence.text for sentence in doc.sentences]\n```\n\n# 토큰 가림\n\nBERT는 이 전략을 도입했는데, 이는 첫 번째이자 가장 잘 알려진 시퀀스 손상 전략입니다. 입력 시퀀스를 무작위 단어로 가려서 훈련 중에 레이블로 사용될 단어를 손상하는 것으로 구성되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n분류 모델에서는 Huggingface transformers에서 DataCollatorForLanguageModeling 클래스를 직접 사용하여 BERT 또는 RoBERTa와 같은 모델을 학습할 수 있는 필요한 레이블을 생성할 수 있습니다.\n\n```js\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling\nimport torch\n\ndef load_dataset_mlm(sentences, tokenizer_class=AutoTokenizer, \n                     collator_class=DataCollatorForLanguageModeling, \n                     mlm=True, mlm_probability=0.20):\n    tokenizer = tokenizer_class.from_pretrained('google-bert/bert-base-uncased')\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True, \n                       truncation=True)\n    \n    # 랜덤 마스킹 설정\n    data_collator = collator_class(\n        tokenizer=tokenizer, \n        mlm=mlm,  \n        mlm_probability=mlm_probability \n    )\n\n    \"\"\"콜레이터는 텐서들 튜플을 기대하므로 입력 텐서들을 분리한 다음\n    첫 번째 차원을 삭제하고 튜플로 전달해야 합니다.\"\"\"\n    tuple_ids = torch.split(inputs['input_ids'], 1, dim=0)\n    tuple_ids = list(tuple_ids)\n    for tensor in range(len(tuple_ids)):\n        tuple_ids[tensor] = tuple_ids[tensor].squeeze(0)\n    tuple_ids = tuple(tuple_ids)\n    \n    # 각 문장의 input_ids, attention_masks 및 레이블 가져오기\n    batch = data_collator(tuple_ids)\n    return batch['input_ids'], inputs['attention_mask'], batch['labels']\n\n\ninput_ids, attention_mask, labels = load_dataset_mlm(sentences)\n\n\"\"\"\ninput_ids[0]:\ntensor([  101, 16364,  1005,  1055,   103,  2003,  1037,   103, 10976,  3207,\n          103, 25284,   103, 25426, 16870,  4295,  3463,  2349,  2000,   103,\n         1997, 26572, 18078,  6187,  2290, 17993,  1999,  1996,  5933,  7629,\n          103,   103,   102,     0,     0])\n\nattention_mask[0]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n\nlabels[0]:\ntensor([ -100,  -100,  -100,  -100,  4295,  -100,  -100, 11265,  -100,  -100,\n         6914,  -100,  8285,  -100,  2389,  -100,  -100,  -100,  -100,  4935,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         4962,  1012,  -100,  -100,  -100))\n\n\"\"\"\n```\n\n생성된 input_ids에는 원본 텍스트의 각 토큰에 대한 정수 번호가 있습니다. 특별한 토큰은 마스킹된 단어를 나타내며 (BERT에서는 이 토큰이 103입니다), 이 특별한 토큰은 사용하는 언어 모델에 따라 다르며 서로 다른 토크나이저는 서로 다른 주의 마스크 식별자를 반환합니다.\n\n또한, Huggingface는 모델 내에서 다른 작업을 수행하여 고유 토큰에 대해 다른 작업을 지정하므로 \"-100\"으로 표시된 토큰은 모델에서 무시해야 함을 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\nBART와 같은 생성 모델의 경우, DataCollatorForLanguageModeling 클래스를 사용하여 토큰 마스킹 전략을 구현할 수 있습니다. 그러나 생성 모델에 맞게 태그를 조정하기 위해 작은 변경을 도입해야 합니다.\n\n```js\nfrom transformers import BartTokenizer, DataCollatorForLanguageModeling\nimport torch\n\ndef load_dataset_mlm(sentences, tokenizer_class=BartTokenizer, \n                     collator_class=DataCollatorForLanguageModeling, \n                     mlm=True, mlm_probability=0.20):\n    tokenizer = tokenizer_class.from_pretrained('facebook/bart-base')\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True, \n                       truncation=True)\n    \n    # 랜덤 마스킹 구성\n    data_collator = collator_class(\n        tokenizer=tokenizer, \n        mlm=mlm,  # 마스크된 언어 모델링을 위해 True\n        mlm_probability=mlm_probability  # 각 토큰이 마스킹될 확률\n    )\n\n    \"\"\"Collator는 텐서의 튜플을 예상하므로 입력 텐서를 분할하고 첫 번째 차원을 제거한 후 튜플로 전달해야합니다.\"\"\"\n    tuple_ids = torch.split(inputs['input_ids'], 1, dim=0)\n    tuple_ids = list(tuple_ids)\n    for tensor in range(len(tuple_ids)):\n        tuple_ids[tensor] = tuple_ids[tensor].squeeze(0)\n    tuple_ids = tuple(tuple_ids)\n    \n    # 각 문장에 대한 input_ids, attention_masks 및 labels 가져오기\n    batch = data_collator(tuple_ids)\n    batch['labels'] = inputs['input_ids']\n    return batch['input_ids'], inputs['attention_mask'],  batch['labels']\n\ninput_ids, attention_mask, labels = load_dataset_mlm(sentences)\n\n\"\"\"\ninput_ids[0]:\ntensor([    0, 38831,  2577,  1054,    18,  2199,    16,    10, 14913, 28904,\n         5777,  3693, 32226, 38868,  2199,   775,   528,     7,  2919,     9,\n        48052,   636,   230,  3450, 35315,    11,     5, 50264, 50264, 50264,\n            4,     2])\n\nattention_mask[0]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1])\n\nlabels[0]:\ntensor([    0, 38831,  2577,  1054,    18,  2199,    16,    10, 14913, 28904,\n         5777,  3693, 32226, 38868,  2199,   775,   528,     7,  2919,     9,\n        48052,   636,   230,  3450, 35315,    11,     5,  8217, 24276, 10596,\n            4,     2])\n\"\"\"\n```\n\n여기서 각 입력 토큰은 마스크 여부에 관계없이 해당하는 토큰을 레이블링하며, 이는 분류 모델과는 달리 모델이 제공된 시퀀스에 기반한 텍스트 시퀀스를 생성할 수 있어야 하기 때문입니다. BART의 경우, 각 마스크를 나타내는 토큰은 ID 50264를 갖습니다.\n\n# 토큰 삭제\n\n<div class=\"content-ad\"></div>\n\n이 전략은 마스킹에 대해 다른 접근 방식을 사용합니다. 특정 확률로 텍스트의 원래 시퀀스에서 단어가 제거되어 모델은 빈칸과 해당 위치를 찾아야 합니다. 표준 마스킹은 마스크가 이미 모델의 입력에서 지정되어 있기 때문에 위치를 학습하지 않습니다.\n\n```js\ndef token_deletion(sentences, tokenizer_class=BartTokenizer, collator_class=DataCollatorForLanguageModeling, \n                 mlm=True, mlm_probability=0.20):\n    tokenizer = tokenizer_class.from_pretrained('facebook/bart-base')\n    inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n    \n    data_collator = collator_class(\n        tokenizer=tokenizer, \n        mlm=mlm,\n        mlm_probability=mlm_probability \n    )\n\n    tuple_ids = torch.split(inputs['input_ids'], 1, dim=0)\n    tuple_ids = list(tuple_ids)\n    for tensor in range(len(tuple_ids)):\n        tuple_ids[tensor] = tuple_ids[tensor].squeeze(0)\n    tuple_ids = tuple(tuple_ids)\n \n    batch = data_collator(tuple_ids)\n\n    # We use the initial inputs as labels\n    batch['labels'] = batch['input_ids'].clone()\n    \n    # We remove tokens with mask identifier and thus make token deletion\n    # Change the value to the mask identifier of the specific token model\n    # It is necessary to know the identifier of the mask token for \n    # that specific model\n    mask = batch['input_ids'] != 50264\n    initial_size = batch['input_ids'].size(1)\n    total_sentences = batch['input_ids'].size(0)\n\n    # When we remove the specific token, we must fill with the padding \n    # token otherwise the tensor size is not respected.\n    for i in range(total_sentences):\n        new_tensor = batch['input_ids'][i][mask[i]]\n        new_tensor = F.pad(new_tensor, (0, initial_size - new_tensor.size(0)), value=1)\n        batch['input_ids'][i] = new_tensor\n        attention_mask = batch['input_ids'][i] == 1\n        inputs['attention_mask'][i][attention_mask] = 0\n        \n    return batch['input_ids'], inputs['attention_mask'], batch['labels']\n\ninput_ids, attention_mask, labels = token_deletion(sentences)\n\n\"\"\"\ninput_ids[0]:\ntensor([    0, 38831,  2577,  1054,  2199, 14913, 28904,  3693, 32226, 38868,\n         2199,   775,   528,     7,  2919,     9, 23404,   636,   230, 35315,\n           11,     5, 24276, 10596,     4,     2,     1,     1,     1,     1,\n            1,     1])\n\nattention_mask[0]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 0, 0, 0, 0, 0])\n\nlabels[0]:\ntensor([    0, 38831,  2577,  1054, 50264,  2199, 50264, 50264, 14913, 28904,\n        50264,  3693, 32226, 38868,  2199,   775,   528,     7,  2919,     9,\n        23404,   636,   230, 50264, 35315,    11,     5, 50264, 24276, 10596,\n            4,     2])\n\n\"\"\"\n```\n\nBART를 사용하여 Token Deletion을 훈련할 때, 일부 텍스트 생성 벤치마킹은 질문 응답, 요약 생성 작업 및 대화 작업에 긴 시퀀스를 사용할 때 약간의 개선이 나타납니다.\n\n# 텍스트 채워넣기\n\n<div class=\"content-ad\"></div>\n\n텍스트 인필링은 토큰 마스킹과 비슷합니다. 특정 확률로 원본 텍스트에 마스크를 씌우게 됩니다. 이 경우에는 마스킹이 하나의 단어 이상을 덮을 수 있다는 차이가 있습니다. BART에서 텍스트 인필링을 적용할 때, 람다 값이 3인 포아송 분포를 사용하여 마스킹을 수행합니다. 이는 평균적으로 문장에서 텍스트가 마스킹될 때마다 세 개의 단어가 하나의 토큰 마스크로 마스킹됨을 의미합니다. 그러나 확률 분포이기 때문에 더 많거나 더 적은 개수의 마스킹된 단어가 있을 수도 있습니다.\n\n![image](/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_2.png)\n\n저희는 NumPy 라이브러리와 우리 언어 모델에 특화된 토크나이저를 사용하여 텍스트 인필링을 구현할 것입니다. 아래는 예시 코드입니다.\n\n<div class=\"content-ad\"></div>\n\n## 문장 순열\n\n문장 순열에서 모델 입력 시퀀스에 맞는 문장 수(소형 모델에서 입력 시퀀스는 512에서 1024 토큰 사이)를 고려하는 것이 매우 중요합니다. 순서 테스트 후, 시퀀스에 맞게 문장 수를 결정한 다음, 그것들을 리스트나 배열로 나누어야 하고, 예시 코드에서와 같이 중복되지 않는 방식으로 무작위로 선택해야 합니다.\n\n```js\n# 주어진 \"문장\" 세트 중 첫 \"number_sentences\"를 선택하고 그 문장들을 무작위로 반환합니다.\ndef sentence_permutation(sentences, number_sentences):\n    new_sentences = sentences[:number_sentences]\n    random.shuffle(new_sentences)\n    new_sentences = sentence_joiner(new_sentences)\n    return new_sentences\n\ndef permuted_data_generation(sentences: list, total_sentences: int):\n    training_sentences = []\n    training_labels = []\n    sentences_copy = sentences.copy()\n    # 문장 목록의 크기에서 1을 뺀 횟수만큼 sentence_permutation을 적용하여 \n    # 텍스트의 각 새 문장 예제를 얻고 가장 오래된 문장을 제거합니다.\n    for _ in range(len(sentences)-total_sentences+1):\n        new_sentences = sentence_permutation(sentences_copy, total_sentences)\n        joined_sentences = sentence_joiner(sentences_copy[:total_sentences])\n        sentences_copy = sentences_copy[1:]\n        training_sentences.append(new_sentences)\n        training_labels.append(joined_sentences)\n\n    return training_sentences, training_labels\n\n\ndef permutation_training(sentences: list, sentences_labels: list, \n                         tokenizer_class=BartTokenizer, \n                         collator_class=DataCollatorForLanguageModeling, \n                         mlm=True, mlm_probability=0.0):\n    # permuted 문장으로부터 input_ids와 attention mask를 얻습니다\n    input, attention_mask, _ = load_dataset_mlm(sentences, tokenizer_class, collator_class, mlm, mlm_probability)\n    \n    # 원본 문장으로부터 라벨 가져오기\n    labels, _, _ = load_dataset_mlm(sentences_labels, tokenizer_class, collator_class, mlm, mlm_probability)\n\n    return input.squeeze(0), attention_mask.squeeze(0), labels.squeeze(0)\n\ninput_ids, attention_mask, labels = permutation_training(training_sentences, training_labels_sentences)\n\n\"\"\"\ninput_ids[0]:\ntensor([    0, 38831,  2577,  1054,    18,  2199, ...\n\nattention_mask[0]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, ...\n\nlabels[0]:\ntensor([    0, 38831, 2577, 1054, 18, 2199, ...\n\"\"\"\n```\n\n<div class=\"content-ad\"></div>\n\n위 예제에서는 모델에 데이터를 입력할 때 원래 순서에서 먼저 나온 문장을 제거한 후, 주어진 문장 개수에 따라 문장 순열을 수행하기 전에 새로운 문장을 추가합니다. 이렇게 함으로써 입력 시퀀스의 문장을 다시 정렬하더라도 각 새로운 예제마다 새로운 문장이 나타나는 문맥 창을 유지하고 가장 오래된 문장을 삭제합니다.\n\n# 문서 회전\n\n문서 회전을 적용하려면 사용된 각 배치의 차원을 고려해야 합니다. 패딩을 적용하는 경우 패딩은 문서의 나머지 부분과 함께 회전되지 않고 원래 위치를 유지해야합니다.\n\n```js\ndef sentence_joiner(sentences: list):\n  return ' '.join(sentences)\n\n# 이 함수를 사용하여 토크나이저에 입력 데이터를 형성할 수 있는 원하는 만큼의 문장을 모을 수 있습니다.\ndef rotated_data_generation(sentences: list, total_sentences: int):\n  training_sentences = []\n  sentences_copy = sentences.copy()\n  for _ in range(len(sentences)-total_sentences+1):\n    new_sentences = sentences_copy[:total_sentences]\n    new_sentences = sentence_joiner(new_sentences)\n    sentences_copy = sentences_copy[1:]\n    training_sentences.append(new_sentences)\n  return training_sentences\n\n# 이전 함수에서 회전된 문장을 적용합니다.\ndef document_rotation_training(sentences, tokenizer_class=BartTokenizer):\n  tokenizer = tokenizer_class.from_pretrained('facebook/bart-base')\n  tokens = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n  tokens['input_ids'] = tokens['input_ids'].squeeze(0)\n  tokens['labels'] = tokens['input_ids'].clone()\n \n  iterations = tokens['input_ids'].size(0)\n  for i in range(iterations):\n    # 어텐션 마스크를 가져와 리스트로 변환합니다.\n    attention_mask = tokens['attention_mask'][i].tolist()\n    # 패딩이 시작되는 위치를 계산합니다.\n    if 0 in attention_mask:\n      padding_start_position = attention_mask.index(0)\n    else:\n      padding_start_position = False\n    # 패딩이 있는 경우 문서의 나머지 부분과 함께 회전되지 않도록 패딩 위치를 고려합니다.\n    if padding_start_position:\n      random_token = torch.randint(1, padding_start_position-1, (1,))\n      tokens['input_ids'][i] = torch.cat((tokens['input_ids'][i][0].unsqueeze(0), \n                                      tokens['input_ids'][i][random_token.item():padding_start_position-1],\n                                      tokens['input_ids'][i][1:random_token.item()],\n                                      tokens['input_ids'][i][padding_start_position-1:-1],\n                                      tokens['input_ids'][i][-1].unsqueeze(0)), 0)\n                                        \n    # 패딩이 없는 경우 패딩을 고려하지 않고 문서를 회전합니다.\n    else:\n      random_token = torch.randint(1, tokens['input_ids'].size(0)-1, (1,))\n      tokens['input_ids'][i] = torch.cat((tokens['input_ids'][i][0].unsqueeze(0),\n                                      tokens['input_ids'][i][random_token.item():-1],\n                                      tokens['input_ids'][i][1:random_token.item()],\n                                      tokens['input_ids'][i][-1].unsqueeze(0)), 0)\n  return tokens['input_ids'], tokens['attention_mask'].squeeze(0), tokens['labels']\n\ndata = rotated_data_generation(sentences, 3)\ninput_ids, attention_mask, labels = document_rotation_training(data)\n\n\"\"\"\ninput_ids[2]:\ntensor([    0,  2433,    61,    32,   551,    88,  1316,    32,    12,  4138,\n        15557, 47605,     6, 22835,  2591,   939,     4,   242, 10079, 38422,\n         9235,     6, 10295, 22540, 14819,     8,  3039, 11543,     4,   347,\n        37347,  8457,     9, 41419,  8217,  1054,    36,   119, 49491,    43,\n        10596, 37118,    32,    45,   157,   684,     4, 41058,  4484,     9,\n         1046,     9, 23808,    16,    41,   505,  3724,     9, 18073,    18,\n         2199, 18246,  4194,     8, 13430,  3505,     4,    20,     2,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n\nattention_mask[2]:\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])\n\nlabels[2]:\ntensor([    0,   347, 37347,  8457,     9, 41419,  8217,  1054,    36,   119,\n        49491,    43, 10596, 37118,    32,    45,   157,   684,     4, 41058,\n         4484,     9,  1046,     9, 23808,    16,    41,   505,  3724,     9,\n        18073,    18,  2199, 18246,  4194,     8, 13430,  3505,     4,    20,\n         2433,    61,    32,   551,    88,  1316,    32,    12,  4138, 15557,\n        47605,     6, 22835,  2591,   939,     4,   242, 10079, 38422,  9235,\n            6, 10295, 22540, 14819,     8,  3039, 11543,     4,     2,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n\n\"\"\"\n```\n\n<div class=\"content-ad\"></div>\n\n짧은 텍스트 시퀀스는 문서 회전 및 문장 순열 기술을 의미 없게 만듭니다. 반면에 다른 언급된 방법들(토큰 마스킹, 토큰 삭제 및 텍스트 채우기)은 짧고 긴 텍스트 시퀀스에서 도움이 될 수 있습니다.\n\n시퀀스 순열과 마찬가지로 각 데이터 입력마다 가장 오래된 문장을 제거하고 새로운 문장을 추가하여 문맥 윈도우를 유지할 수 있습니다.\n\n# 결론\n\n본 글은 시퀀스 왜곡으로 언어 모델을 학습시키는 다양한 방법에 대해 논의했습니다. 이들은 가장 유명하지만 대부분의 모델은 토큰 마스킹만을 사용합니다.\n\n<div class=\"content-ad\"></div>\n\n요약하면, 가장 효과적인 전략은 텍스트를 변경하는 대신 텍스트를 손상시키는 것입니다. 그러나 이 두 가지 접근 방식은 모델 훈련 중에 결합될 수 있으며, BART의 경우 Text Infilling 및 Sentence Permutation을 사용하여 흥미로운 결과를 얻을 수 있었습니다.\n\n이 훈련 방식은 인코더 또는 인코더-디코더 트랜스포머 모델에서 사용할 수 있습니다. 내가 아는 한, 이 접근 방식을 사용하는 디코더 전용 모델은 없습니다. 왜냐하면 그런 경우에는 자기 회귀 언어 모델링 또는 인과 언어 모델링이 사용됩니다. 이는 GPT 모델과 같이 자기 어텐션 메커니즘 없이 인코더를 사용할 때 각 토큰의 예측이 이전 토큰에만 의존하며 그 다음 토큰에는 의존하지 않기 때문입니다. BERT와 같은 인코더 전용 모델에서는 양방향 어텐션이 존재하여 각 토큰이 이전 및 이후 토큰에 따라 어느 위치에 있어야 하는지 예측할 수 있습니다.\n\n미래 글에서는 어떻게 인과 언어 모델링이 작동하는지와 더 고급 시퀸스 손상 기술에 대해 깊이 알아볼 것입니다.\n\n![이미지](/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_3.png)\n\n\n\n<div class=\"content-ad\"></div>\n\n행복한 코딩하세요!","ogImage":{"url":"/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_0.png"},"coverImage":"/assets/img/2024-06-23-TokenMaskingStrategiesforLLMs_0.png","tag":["Tech"],"readingTime":18},{"title":"반품 및 환불 처리를 위한 챗봇 활용 엔터프라이즈 워크플로우 통합 방법","description":"","date":"2024-06-23 19:23","slug":"2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds","content":"\n\n<img src=\"/assets/img/2024-06-23-엔터프라이즈워크플로우통합챗봇을활용한반품및환불.png\" />\n\n고객 서비스 랜드스케이프가 급격히 변화하고 있습니다. 기업들은 영업을 최적화하고 고객 경험을 높이기 위해 AI 솔루션을 채택하고 있습니다. 기본 챗봇은 FAQ에 대한 답변으로 흔히 사용되고 있지만, 반품이나 환불 처리와 같이 복잡한 시나리오에 대해 어려움을 겪는 경우가 많습니다. 이러한 복잡한 상황에서 가장 큰 수익률을 얻을 수 있는 영역입니다.\n\n# 기본 챗봇 응용\n\nLLM 챗봇은 대부분 고객 서비스에 사용되며, 기본 지원을 제공하고 자주 묻는 질문에 답변합니다. 이러한 챗봇은 일반적으로 응답 생성을 위해 소스 문서나 HTML 페이지의 라이브러리에 의존합니다. 이 문서들은 청크(chunk) 단위로 분리되어 벡터 저장소(Vector Store)에 저장됩니다. 사용자가 질문을 하면 벡터 저장소에서 유사도 검색이 수행되어 관련 문서 청크가 검색되고, 이후 OpenAI와 같은 LLM으로 응답을 생성하기 위해 전송됩니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds_1.png)\n\n챗봇이 도움이 될 수는 있지만, 고객 서비스 자동화의 진정한 가치는 고객 지원에서 정례적으로 발생하는 복잡한 상황을 해결할 수 있는 능력에 있습니다.\n\n# 챗봇을 활용하여 복잡한 워크플로우 문제 해결하기\n\n자연어 처리 및 생성 기술의 놀라운 발전에도 불구하고, LLM을 기반으로 한 챗봇은 여전히 제품 반품 또는 환불과 같이 복잡한 상황에서 특히 고객이 만족스러운 경험을 얻는 데 어려움을 겪고 있습니다. 왜 그런지 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n<table>\n<tr>\n<th>Issue</th>\n<th>Description</th>\n</tr>\n<tr>\n<td>Contextual Blindspots</td>\n<td>LLMs (Large Language Models) may sound human-like, but often miss important context. For instance, a chatbot might not realize a prior promise for faster shipping, causing frustration.</td>\n</tr>\n<tr>\n<td>Isolated Operations</td>\n<td>Traditional LLM chatbots are often disconnected from essential systems like order management or knowledge bases. This means a chatbot may not access specific booking details when a customer wants to change a flight.</td>\n</tr>\n<tr>\n<td>Inconsistent Service</td>\n<td>LLMs can provide conflicting responses, especially in complex situations with strict policies. This can lead to confusion and distrust, like differing information on warranty policies.</td>\n</tr>\n<tr>\n<td>Language Barriers</td>\n<td>Most LLM chatbots support only one language, limiting help for a global customer base. For example, an English-only chatbot may not effectively assist Spanish-speaking customers.</td>\n</tr>\n</table>\n\n# Why Processing Refunds can be Challenging for AI (Even the Super Smart Ones!)\n\nEven highly intelligent AI systems face difficulties when it comes to handling refund processes. Here are some essential capabilities that chatbots need to possess for successfully managing complex tasks like refunds:\n\n<div class=\"content-ad\"></div>\n\n- 많은 규칙을 따라야 합니다: 온라인으로 셔츠를 주문했지만 잘못된 사이즈로 도착했어요. 환불 정책에 따르면 구매 후 30일 이내에 원래 영수증이 있는 새 제품만 환불을 받을 수 있을 수도 있습니다. AI는 이 모든 규칙과 함께 주문 세부 정보 (날짜, 사이즈 등)와 교환 사유 (잘못된 사이즈)를 이해해야 올바른 결정을 내릴 수 있어야 해요. 이 모든 것을 동시에 이해하는 것은 수 많은 퍼즐 조각을 맞추는 것과 같아요!\n- 당신의 말을 이해하기: 교환에 관해 AI에게 “이 셔츠는 너무 커!” 라고 메시지를 보낼 수도 있어요. AI의 자연어 처리 능력은 당신이 환불 정책에 정확한 단어를 사용하지 않아도 (예: “잘못된 사이즈”) 당신이 무슨 의미인지 파악하는 데 능숙해야 해요. 이는 용어가 많은 문자 메시지를 번역하는 것과 같아요 — AI는 당신의 말 뒤에 감추어진 의도를 이해해야 해요.\n- 다른 시스템과 대화하기: 환불을 처리하려면 AI가 여러 컴퓨터 프로그램을 확인해야 할 수도 있어요. 세부 정보를 확인하기 위해 주문 내역을 확인해야 하거나 적용 가능한 규칙을 확인하기 위해 환불 정책 데이터베이스를 참고하거나 환불을 시작하기 위해 금융 시스템에 연결해야 할 수도 있어요. 이 모든 것을 처리해야 하는 것은 여러 공을 질러야 하는 것과 같아요 — AI는 이러한 다양한 시스템과의 소통을 처리하여 환불을 올바르게 처리해야 해요.\n- 상황에 적응하기: 환불 처리를 위한 AI 시스템은 “왜 환불을 요청하시나요?” 나 “제품이 손상되었나요, 아니면 생각이 바뀌었나요?”와 같이 명확히 질문하여 상황에 적응해야 해요. 세부 사항을 확인한 후 자신의 방식으로 접근을 조정해야 해요. 하자가 있는 경우, 사진을 요청하고 전액 환불을 처리할 수 있어야 해요. 생각이 바뀐 경우, 반품 안내나 재설치 수수료가 있는 라벨을 제공하거나 당신에게 가장 잘 맞는 옵션을 제안하며 교환이나 상품권을 제공할 수 있어야 해요.\n\n<div class=\"content-ad\"></div>\n\n많은 기업들이 처음에는 기존 문서에서 FAQ를 처리하기 위해 챗봇을 도입하지만, 실제 기회는 챗봇이 복잡한 업무 흐름을 관리할 수 있는 능력에 있습니다. 이러한 시나리오는 챗봇이 특정 정책을 조회하고 내부 시스템과 상호 작용하며 명확한 질문을 하고 다양한 상황에 적응하며 필요할 때 실시간 대화 대상에게 매끄럽게 연결되어야 합니다. 이것이 다음 12개월 동안 가장 중요한 챗봇 혁신이 일어날 곳이며, 기업에 가장 높은 ROI를 제공할 것입니다.","ogImage":{"url":"/assets/img/2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds_0.png"},"coverImage":"/assets/img/2024-06-23-EnterpriseWorkflowIntegrationLeveragingChatbotsforReturnsandRefunds_0.png","tag":["Tech"],"readingTime":4},{"title":"모듈형 RAG와 RAG Flow 첫 번째 파트","description":"","date":"2024-06-23 19:19","slug":"2024-06-23-ModularRAGandRAGFlowPart","content":"\n\n# 소개\n\n지난 일년 동안, 검색 증가 생성(Retrieval-Augmented Generation, RAG) 개념은 LLM 애플리케이션 구현 방법으로서 상당한 주목을 받았습니다. 저희는 RAG에 대한 포괄적인 조사 보고서를 작성했는데, 이 보고서는 Naive RAG에서 Advanced RAG 및 Modular RAG로의 전환에 대해 탐구했습니다. 그러나 이 조사는 주로 증가(Source/Stage/Process)를 통해 RAG 기술을 규명하였습니다.\n\n본 글은 특히 Modular RAG 패러다임을 중심으로 합니다. 우리는 Module Type, Module 및 Operator로 구성된 세 단계의 Modular RAG 패러다임을 더 확실히 정의했습니다. 이 패러다임에 따라, 현재 RAG 시스템 내 핵심 기술인 6가지 주요 Module Types, 14개의 Modules 및 40여 개의 Operators에 대해 상세히 다루어 RAG에 대한 포괄적인 이해를 제공하고 있습니다.\n\n다양한 Operator를 조합함으로써, 우리는 다양한 RAG 흐름을 유도할 수 있으며, 이 개념을 이 글에서 명확히 설명하고자 합니다. 광범위한 연구를 기반으로, 일반적인 패턴, 몇 가지 구체적인 구현 사례 및 최상의 업계 사례를 요약하였습니다. (공간 제약으로 인해 이 부분은 Part II에서 다룰 예정입니다.)\n\n<div class=\"content-ad\"></div>\n\n이 글의 목적은 현재 RAG 개발 상태에 대한 더 정교한 이해를 제공하고 미래 발전을 위한 길을 만들어주는 것입니다. 모듈식 RAG는 새로운 연산자, 모듈, 그리고 새로운 플로우의 구성을 용이하게 하는 다양한 기회를 제공합니다.\n\n# 모듈식 RAG란 무엇인가요?\n\nRAG의 발전은 다음과 같은 중요한 측면들에 반영되어 보다 다양하고 유연한 과정을 이끌어내고 있습니다:\n\n- 향상된 데이터 획득: RAG는 기존의 비구조적 데이터를 넘어 반구조적 및 구조적 데이터를 포함하고, 구조적 데이터의 전처리에 중점을 두어 검색의 품질을 향상시키고 모델이 외부 지식 소스에 의존하는 것을 줄이는 방향으로 확장되었습니다.\n- 통합된 기술: RAG는 세부 조정, 어댑터 모듈 사용, 강화 학습을 포함한 다른 기술들과 통합하여 검색 능력을 강화하고 있습니다.\n- 적응형 검색 프로세스: 검색 프로세스는 검색한 내용을 활용하여 생성을 안내하고 그 반대로 하는 등 다단계 검색 강화를 지원하도록 진화했습니다. 또한, 자율적인 판단과 LLM 사용을 통해 검색 필요성을 판단하여 질문에 대한 효율을 높였습니다.\n\n<div class=\"content-ad\"></div>\n\n모듈식 RAG의 정의\n\n위에서는 RAG의 신속한 발전이 체인 스타일의 고급 RAG 패러다임을 능가하여 모듈식 특성을 과시하고 있음을 볼 수 있습니다. 현재의 조직 부재와 추상화 부족을 해결하기 위해, Naive RAG와 Advanced RAG의 개발 패러다임을 무리없이 통합하는 모듈식 RAG 접근 방식을 제안합니다.\n\n모듈식 RAG는 매우 확장 가능한 패러다임을 제시하며, RAG 시스템을 모듈 유형, 모듈 및 연산자의 세 단계 구조로 나누고 있습니다. 각 모듈 유형은 RAG 시스템의 핵심 프로세스를 나타내며, 여러 기능 모듈을 포함하고 있습니다. 각 기능 모듈에는 또 다른 여러 특정 연산자가 포함되어 있습니다. 전체 RAG 시스템은 여러 모듈과 해당 연산자들의 순열과 조합으로 이루어진 RAG Flow를 형성하게 됩니다. Flow 내에서 각 모듈 유형에서 다른 기능 모듈을 선택할 수 있으며, 각 기능 모듈 내에서는 하나 이상의 연산자를 선택할 수 있습니다.\n\n이전 패러다임과의 관계\n\n<div class=\"content-ad\"></div>\n\n모듈식 RAG은 RAG 시스템을 다층 구조의 모듈식 형태로 조직합니다. 고급 RAG는 RAG의 모듈식 형태이며, Naive RAG는 고급 RAG의 특수한 경우입니다. 이 세 가지 패러다임 간의 관계는 상속과 발전의 하나입니다.\n\n모듈식 RAG의 장점\n\n모듈식 RAG의 이점은 명백하며, 기존의 RAG 관련 작업에 대한 신선하고 포괄적인 시각을 제공합니다. 모듈식 구성을 통해 관련 기술과 방법들이 명확하게 요약됩니다.\n\n- 연구적 시각. 모듈식 RAG는 확장성이 높아 연구자들이 현재 RAG 개발에 대한 포괄적인 이해를 기반으로 새로운 모듈 유형, 모듈, 그리고 연산자를 제안하기 쉽습니다.\n- 응용 시각. RAG 시스템의 설계 및 구성이 더 편리해지며, 사용자들이 기존 데이터, 사용 시나리오, 하향 작업 등에 따라 RAG Flow를 사용자 정의할 수 있습니다. 개발자들은 또한 현재 Flow 구성 방법을 참고하고, 다른 응용 시나리오와 도메인에 기반하여 새로운 플로우와 패턴을 정의할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Module](/assets/img/2024-06-23-ModularRAGandRAGFlowPart_0.png)\n\n## Module Type — Module — Operators\n\n### 1. Indexing\n\nIndexing, the process of breaking down text into manageable chunks, is a crucial step in organizing the system, facing three main challenges:\n\n\n<div class=\"content-ad\"></div>\n\n- 완전하지 않은 내용 표현. 청크의 의미 정보는 세분화 방법에 영향을 받아 중요 정보가 잃거나 긴 문맥 속에서 잠기는 결과를 초래합니다.\n- 부정확한 청크 유사성 검색. 데이터 양이 증가함에 따라 검색에서의 잡음이 커져 잘못된 데이터와 빈번히 일치하게 되어 검색 시스템을 취약하고 신뢰할 수 없게 만듭니다.\n- 명확하지 않은 참조 궤적. 검색된 청크는 어느 문서에서든 유래할 수 있으며 인용 트레일이 없어, 다수의 다른 문서에서 유사하게 의미가 있는 혹은 완전히 다른 주제의 내용을 포함할 수 있습니다.\n\n## 청크 최적화\n\n더 큰 청크는 더 많은 문맥을 포착할 수 있지만, 더 많은 잡음을 생성하여 더 오랜 처리 시간과 높은 비용이 필요합니다. 반면 더 작은 청크는 필요한 문맥을 완전히 전달하지 않을 수 있지만, 더 적은 잡음을 가지고 있습니다.\n\n- 슬라이딩 윈도우\n\n<div class=\"content-ad\"></div>\n\n이러한 요구 사항을 균형 있게 조절하는 한 가지 간단한 방법은 중첩 청크를 사용하는 것입니다. 슬라이딩 창을 활용하면 의미적 전환을 향상시킬 수 있습니다. 그러나 의미적 고려 사항이 부족하다는 제한 사항이 있습니다. - 작은 청크에서 큰 청크로 핵심 아이디어는 검색에 사용되는 청크와 합성에 사용되는 청크를 분리하는 것입니다. 더 작은 청크를 사용하면 검색의 정확도가 향상되고, 더 큰 청크는 더 많은 컨텍스트 정보를 제공할 수 있습니다.\n\n![이미지](/assets/img/2024-06-23-ModularRAGandRAGFlowPart_1.png)\n\n<div class=\"content-ad\"></div>\n\n특히 한 가지 방법은 더 작은 청크를 검색한 다음 부모 ID를 참조하여 더 큰 청크를 반환하는 것일 수 있습니다. 또는 개별 문장을 검색하고 문장 주변 텍스트 창을 반환할 수도 있습니다.\n\n자세한 정보 및 LlamaIndex 구현.\n\n- 요약\n\n이는 작은 것에서 큰 것으로의 개념과 유사하며, 먼저 더 큰 청크의 개요가 생성되고, 이후 개요에 대해 검색이 수행됩니다. 그런 다음 더 큰 청크에 대해 보조 검색을 수행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 메타데이터 첨부\n\n청크에 페이지 번호, 파일 이름, 작성자, 타임스탬프, 요약 또는 청크가 대답할 수 있는 질문과 같은 메타데이터 정보를 포함시킬 수 있습니다. 그 결과로 검색 범위를 제한할 수 있는 이 메타데이터를 기반으로 검색이 필터링될 수 있습니다. LlamaIndex에서 이 구현을 확인해보세요.\n\n# 구조적 구성\n\n정보 검색을 강화하는 효과적인 방법 중 하나는 문서에 대한 계층적 구조를 설정하는 것입니다. 청크 구조를 구성함으로써 RAG 시스템은 적절한 데이터의 검색과 처리를 가속화할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 계층적 색인\n\n문서의 계층 구조에서 노드는 부모-자식 관계로 배열되어 있으며 이들에 연결된 청크가 있습니다. 각 노드에는 데이터 요약이 저장되어 있어 데이터를 신속하게 탐색하고 RAG 시스템이 어떤 청크를 추출해야 하는지 결정하는 데 도움이 됩니다. 이 접근 방식은 블록 추출 문제로 인한 오류를 완화하는 데도 도움이 됩니다.\n\n구조적 색인을 구축하는 주요 방법은 다음과 같습니다:\n\n- 구조 인식: 문서에서 단락과 문장을 분할\n- 콘텐츠 인식: PDF, HTML, Latex에 내재된 구조\n- 의미 인식: NLP 기술을 활용한 텍스트의 의미 인식과 분할, NLTK와 같은 기법 사용\n\n<div class=\"content-ad\"></div>\n\n대규모로 아커스의 계층적 인덱스를 확인하실 수 있습니다.\n\n- KG 조직 문서\n\n지식 그래프(KGs)를 활용하여 문서의 계층 구조를 구축함으로써 일관성을 유지할 수 있습니다. 이는 다양한 개념과 엔티티 간의 연결을 명확히 하고 환각 가능성을 크게 줄입니다.\n\n다른 이점은 정보 검색 프로세스를 LLM이 이해할 수 있는 지침으로 변환하여 지식 검색의 정확성을 향상시키고 LLM이 맥락에 부합한 응답을 생성할 수 있도록 함으로써 RAG 시스템의 전체적인 효율성을 향상시킵니다.\n\n<div class=\"content-ad\"></div>\n\nNeo4j 구현 및 LllmaIndex Neo4j 쿼리 엔진을 확인해보세요.\n\nKG를 사용하여 여러 문서를 조직하는 경우, 이 연구 논문 KGP: 지식 그래프 프롬프팅을 참고하실 수 있습니다.\n\n# 2. 사전 검색\n\nNaive RAG의 주요 도전 중 하나는 사용자의 원본 쿼리에 직접 의존하고 있다는 것입니다. 정확하고 명확한 질문을 구성하는 것은 어려우며, 무분별한 쿼리는 하위 수준의 검색 효과를 초래합니다.\n\n<div class=\"content-ad\"></div>\n\n이 단계에서의 주요 도전 과제는 다음과 같습니다:\n\n- 제대로 작성되지 않은 질문들입니다. 질문 자체가 복잡하며, 언어가 잘 정리되지 않았습니다.\n- 언어 복잡성 및 모호함입니다. 언어 모델은 종종 전문 용어나 다의어적 약어를 다룰 때 어려움을 겪습니다. 예를 들어, “LLM”이 큰 언어 모델을 의미하는지 법적 맥락에서의 법학 석사를 나타내는지를 구분하지 못할 수 있습니다.\n\n## 질의 확장\n\n단일 질문을 복수의 질문으로 확장하면 질문의 내용을 풍부하게 만들어 특정 뉘앙스의 부족을 보충함으로써 생성된 답변의 최적적인 관련성을 보장할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 다중 쿼리\n\nLLM을 통해 쿼리를 확장하기 위해 prompt 엔지니어링을 사용하여 이러한 쿼리들을 병렬로 실행할 수 있습니다. 쿼리의 확장은 무작위가 아니라 신중하게 설계된 것입니다. 이 설계의 두 가지 중요한 기준은 쿼리의 다양성과 범위입니다.\n\n여러 쿼리를 사용하는 한 가지 어려움은 사용자의 원래 의도가 희석될 수 있다는 것입니다. 이를 완화하기 위해 우리는 prompt 엔지니어링에서 모델에게 원본 쿼리에 더 높은 중요도를 할당하도록 지시할 수 있습니다.\n\n- 하위 쿼리\n\n<div class=\"content-ad\"></div>\n\n서브 질문 계획 프로세스는 원본 질문을 완전히 대답할 수 있는 필요한 서브 질문을 생성하는 것을 나타냅니다. 관련 컨텍스트를 추가하는 이 프로세스는 원칙적으로 쿼리 확장과 유사합니다. 구체적으로, 복잡한 질문은 적은 것에서 많은 것으로 유도하는 방법을 사용하여 일련의 보다 간단한 서브 질문으로 분해될 수 있습니다.\n\n- CoVe\n\n쿼리 확장에 대한 다른 접근 방식은 Meta AI가 제안한 Chain-of-Verification(CoVe)의 사용을 포함합니다. 확장된 쿼리는 LLM에 의해 유효성을 검사하여 환각을 줄이는 효과를 얻습니다. 유효성이 검증된 확장된 쿼리는 전형적으로 더 높은 신뢰성을 보입니다.\n\n# 쿼리 변환\n\n<div class=\"content-ad\"></div>\n\n- 다시 작성\n\n원본 쿼리는 실제 상황에서 LLM 검색에 항상 최적이 아닙니다. 따라서, 우리는 LLM에 대해 쿼리를 다시 작성하도록 유도할 수 있습니다. 쿼리 다시 작성을 위해 LLM을 사용하는 것 외에도, RRR(Rewrite-retrieve-read)과 같은 특수한 작은 언어 모델을 활용할 수 있습니다.\n\n타오바오 프로모션 시스템에서 쿼리 다시 작성 방법인 BEQUE:Query Rewriting for Retrieval-Augmented Large Language Models의 구현은 장인 쿼리에 대한 회수 효과를 현저히 향상시켰으며, GMV 상승으로 이어졌습니다.\n\n- HyDE\n\n<div class=\"content-ad\"></div>\n\n쿼리에 대답할 때 LLM은 벡터 데이터베이스의 쿼리 및 계산된 벡터를 직접 검색하는 대신 가정된 답변인 가상 문서를 작성합니다. 이는 문제나 쿼리의 임베딩 유사성을 찾는 대신 답변 간의 임베딩 유사성에 초점을 맞추고 있습니다. 또한 쿼리 간의 검색에 초점을 맞춘 Reverse HyDE도 포함되어 있습니다.\n\n![이미지](/assets/img/2024-06-23-ModularRAGandRAGFlowPart_2.png)\n\n- Step-back Prompting\n\nGoogle DeepMind가 제안한 Step-back Prompting 방법을 사용하면 원본 쿼리를 추상화하여 고수준 개념 질문인 스텝백 질문을 생성할 수 있습니다. RAG 시스템에서는 스텝백 질문과 원본 쿼리 모두 검색에 사용되며, 두 결과 모두 언어 모델 답변 생성의 기초로 활용됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 쿼리 라우팅\n\n다양한 쿼리에 따라 다른 RAG 파이프라인으로 라우팅되며, 다양한 시나리오를 수용할 수 있는 유연한 RAG 시스템에 적합합니다.\n\n- 메타데이터 라우터/ 필터\n\n첫 번째 단계는 쿼리에서 키워드(엔티티)를 추출한 후, 키워드 및 청크 내의 메타데이터를 기반으로 필터링하여 검색 범위를 좁히는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n- Semantic Router\n\n라우팅하는 또 다른 방법은 쿼리의 의미 정보를 활용하는 것입니다. 특정 방법은 Semantic Router를 활용합니다. 물론 의미론적 및 메타데이터 기반 방법을 결합하여 질의 라우팅을 향상시키는 하이브리드 라우팅 방법도 사용할 수 있습니다.\n\nSemantic router 리포지토리를 확인해보세요.\n\n# 쿼리 구성\n\n<div class=\"content-ad\"></div>\n\n유저의 쿼리를 다른 쿼리 언어로 변환하여 대체 데이터 소스에 접근하는 작업입니다. 일반적으로 사용되는 방법은 다음과 같습니다:\n\n- 텍스트를 Cypher로 변환\n- 텍스트를 SQL로 변환\n\n많은 시나리오에서는 구조화된 쿼리 언어(SQL, Cypher 등)가 의미 정보 및 메타데이터와 결합되어 더 복잡한 쿼리를 작성하는 데 자주 사용됩니다. 자세한 내용은 Langchain 블로그를 참조해주세요.\n\n# 3 조회\n\n<div class=\"content-ad\"></div>\n\n검색 과정은 RAG에서 중요한 역할을 합니다. 강력한 PLM을 활용하여 쿼리와 텍스트를 잠재적 공간에 효과적으로 표현하여 질문과 문서 간의 의미 유사성을 성립하는 데 도움이 됩니다.\n\n고려해야 할 세 가지 주요 사항:\n\n- 검색 효율성\n- 임베딩 품질\n- 작업, 데이터 및 모델의 정렬\n\n# 검색기 선택\n\n<div class=\"content-ad\"></div>\n\nChatGPT가 출시된 이후에는 임베딩 모델 개발에 대한 열기가 높아졌어요. Hugging Face의 MTEB 리더보드는 8가지 작업(클러스터링, 분류, 이중 텍스트, 쌍 분류, 재랭킹, 정보 검색, 시맨틱 텍스트 유사성 및 요약)에 걸쳐 거의 모든 가능한 임베딩 모델을 평가합니다. 또한, C-MTEB는 중국어 임베딩 모델의 능력을 평가하며, 6가지 작업과 35개의 데이터셋을 다루고 있어요.\n\nRAG 애플리케이션을 구축할 때, \"어떤 임베딩 모델을 사용해야 할까?\"에 대한 보편적인 해답은 없습니다. 그러나 특정 임베딩이 특정 사용 사례에 더 적합하다는 점을 알 수 있을 거예요.\n\nMTEB/C-MTEB 리더보드를 확인해보세요.\n\n- Sparse Retriever\n\n<div class=\"content-ad\"></div>\n\n희박 인코딩 모델은 다소 구식 기술로 간주될 수 있지만 단어 빈도 통계와 같은 통계적 방법에 기반하고 있어서 높은 인코딩 효율성과 안정성을 유지하고 있습니다. 일반적인 계수 인코딩 모델로는 BM25와 TF-IDF가 있습니다.\n\n- 조밀 검색기\n\n신경망 기반의 밀도 인코딩 모델에는 여러 종류가 있습니다:\n\n- BERT 아키텍처에서 구축된 인코더-디코더 언어 모델인 ColBERT와 같은 모델.\n- BGE 및 Baichuan-Text-Embedding과 같은 포괄적인 다중 작업 파인튜닝 모델.\n- OpenAI-Ada-002 및 Cohere Embedding과 같은 클라우드 API 기반 모델.\n- 대규모 데이터 애플리케이션을 위해 설계된 다음 세대 가속화 인코딩 프레임워크 Dragon+.\n- 혼합/하이브리드 검색\n\n<div class=\"content-ad\"></div>\n\n두 가지 임베딩 접근 방식은 서로 다른 관련 기능을 캡쳐하고 상호 보완적인 관련 정보를 활용함으로써 상호 이득을 얻을 수 있습니다. 예를 들어, 희박 검색 모델은 밀도 검색 모델을 교육하기 위한 초기 검색 결과를 제공하는 데 사용될 수 있습니다. 게다가 PLM(Pre-trained Language Models)은 희소 검색을 강화하기 위해 용어 가중치를 학습하는 데 활용될 수 있습니다. 구체적으로, 희소 검색 모델이 밀도 검색 모델의 제로샷 검색 기능을 강화하고 희귀 엔티티를 포함하는 쿼리를 처리하는 데 밀도 리트리버를 돕는 것이 증명되었습니다.\n\n![이미지](/assets/img/2024-06-23-ModularRAGandRAGFlowPart_3.png)\n\n# 리트리버 파인튜닝\n\n특히 의료, 법률 및 소유 용어가 풍부한 기타 전문 분야와 같이 사전 훈련된 모델이 임베딩 공간에서 유사하다고 판단하는 컨텍스트가 벗어날 수 있는 경우, 임베딩 모델을 조정함으로써 이 문제를 해결할 수 있습니다. 이러한 조정은 추가적인 노력이 필요하지만 검색 효율성과 도메인 정렬을 크게 향상시킬 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- SFT\n\n도메인별 데이터를 기반으로 자체 세부 조정 데이터 세트를 구축할 수 있습니다. LlamaIndex를 사용하면 빠르게 작업을 완료할 수 있어요.\n\n- LSR (LM-지도 학습 리트리버)\n\n직접 데이터 세트에서 세부 조정 데이터 세트를 구성하는 대신 LSR은 LM이 생성한 결과를 우선적 신호로 사용하여 RAG 프로세스 중에 임베딩 모델을 세부 조정합니다.\n\n<div class=\"content-ad\"></div>\n\n- RL(강화 학습)\n\nRLHF(사람 피드백에서 강화 학습)에서 영감을 받아 LM 기반 피드백을 활용하여 강화 학습을 통해 리트리버를 강화합니다.\n\n- 어댑터\n\n가끔 전체 리트리버를 세밀하게 조정하는 것은 비용이 많이 들 수 있으며, 특히 API 기반 리트리버를 직접 세밀하게 조정할 수 없는 경우에는 어댑터 모듈을 통합하고 세밀 조정을 진행함으로써 이를 완화할 수 있습니다. 어댑터를 추가하는 또다른 이점은 특정 다운스트림 작업과의 더 나은 조정을 달성할 수 있는 능력입니다.\n\n<div class=\"content-ad\"></div>\n\n- Task Specific.PRCA: 검색 질의응답을 위한 블랙박스 대규모 언어 모델에 플러그인 리워드 기반 컨텍스트 어댑터를 적합화하는 작업.\n- Task Agnostic. AAR(Augmentation-Adapted Retriver)는 여러 하향 작업을 수용하기 위해 설계된 범용 어댑터를 소개합니다.\n\n## 4. 검색 후\n\n전체 문서 청크를 검색하여 이를 LLM의 컨텍스트 환경에 직접 공급하는 것은 최적의 선택이 아닙니다. 문서 후처리는 LLM이 컨텍스트 정보를 보다 효과적으로 활용하는 데 도움이 될 수 있습니다.\n\n주요 도전 과제는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- 중간에 길들이 미아 되다. 인간처럼 LLM은 긴 텍스트의 처음과 끝만을 기억하고, 중간 부분은 잊어 버리기 쉽습니다.\n- 소음/반사실적 청크. 소음이나 사실적으로 모순된 문서를 검색하면 최종 검색 결과 생성에 영향을 줄 수 있습니다.\n- 맥락 창. 큰 모델에서 관련 콘텐츠를 상당량 검색하더라도 맥락 정보의 길이에 제한이 있어 모든 이 콘텐츠를 포함시키기 어렵습니다.\n\n# 재랭크\n\n검색된 문서 청크들을 콘텐츠나 길이를 변경하지 않고, LLM에 대한 더 중요한 문서 청크의 가시성을 높이기 위해 재랭크합니다. 구체적으로는:\n\n- 규칙 기반 재랭크\n\n<div class=\"content-ad\"></div>\n\n특정 규칙에 따라 메트릭을 계산하여 청크를 다시 순위 지정합니다. 일반적인 메트릭은 다음과 같습니다:\n\n- 다양성\n- 관련성\n- MRR (최대 주변 관련성, 1998)\n\nMMR의 아이디어는 중복을 줄이고 결과 다양성을 높이는 데 있습니다. 텍스트 요약에 사용되며 쿼리 관련성과 정보의 독창성을 병합 기준으로 사용하여 최종 키워드 목록에서 구절을 선택합니다.\n\nHayStack에서 해당 순위 지정 구현을 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n- 모델 기반 재순위\n\n언어 모델을 활용하여 문서 청크를 재정렬하고, 선택지는 다음과 같습니다:\n\n- BERT 시리즈의 인코더-디코더 모델인 SpanBERT\n- Cohere rerank 또는 bge-raranker-large와 같은 특화된 재순위 모델\n- GPT-4와 같은 일반적인 대규모 언어 모델\n\n# 압축 및 선택\n\n<div class=\"content-ad\"></div>\n\nRAG 프로세스에서 흔한 오해는 가능한 많은 관련 문서를 검색하여 연결하여 긴 검색 프롬프트를 형성한다는 것이 유익하다는 믿음입니다. 그러나 과도한 문맥은 더 많은 잡음을 도입할 수 있으며, LLM이 주요 정보를 인식하는 것을 약화시키고 \"중간에서 잃어버림\"과 같은 문제로 이어질 수 있습니다. 이러한 문제를 해결하는 일반적인 접근법은 검색된 콘텐츠를 압축하고 선택하는 것입니다.\n\n- （Long)LLMLingua\n\nGPT-2 Small이나 LLaMA-7B와 같은 정련된 작은 언어 모델을 사용하여 중요하지 않은 토큰을 검색 프롬프트에서 감지하고 제거하여, 사람들이 이해하기 어려운 형태에서 LLM이 잘 이해하는 형태로 변환됩니다. 이 접근 방식은 검색 프롬프트 압축을 위한 직접적이고 실용적인 방법을 제시하여, LLM의 추가 훈련 필요를 없애면서 언어 무결성과 압축 비율을 균형있게 유지합니다.\n\nLLMLingua 프로젝트를 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n- Recomp\n\nRecomp은 두 가지 유형의 압축기를 소개합니다: 검색된 문서에서 적절한 문장을 선택하는 추출 압축기 및 여러 문서에서 정보를 결합하여 간결한 요약을 생성하는 추상적 압축기입니다. 두 압축기 모두 생성된 요약이 언어 모델의 입력에 앞부분에 추가될 때 최종 작업에서 언어 모델의 성능을 향상시키도록 훈련되었으며, 요약의 간결성을 보장합니다. 검색된 문서가 입력과 관련이 없거나 언어 모델에 추가 정보를 제공하지 않는 경우, 압축기는 빈 문자열을 반환할 수 있어 선택적 확장을 구현할 수 있습니다.\n\n- 선택적 컨텍스트\n\n입력 컨텍스트에서 중복되는 콘텐츠를 식별하고 제거함으로써 입력을 최적화하여 언어 모델의 추론 효율성을 향상할 수 있습니다. 선택적 컨텍스트는 \"불용어 제거\" 전략과 유사합니다. 실제로, 선택적 컨텍스트는 기본 언어 모델에 의해 계산된 자기 정보에 기반하여 어휘 단위의 정보 콘텐츠를 평가합니다. 더 높은 자기 정보를 갖는 콘텐츠를 유지함으로써, 이 방법은 언어 모델 처리를 위한 보다 간결하고 효율적인 텍스트 표현을 제공하며, 다양한 응용 프로그램에서의 성능을 저해시키지 않습니다. 그러나 압축된 콘텐츠와 대상 언어 모델 및 압축을 위해 사용되는 작은 언어 모델 간의 일치를 간과합니다.\n\n<div class=\"content-ad\"></div>\n\n- 태깅-필터\n\n태깅은 비교적 직관적이고 명료한 방식입니다. 특히, 문서들이 먼저 라벨이 붙여지고, 그 후 쿼리의 메타데이터를 기반으로 필터링됩니다.\n\n- LLM-비평\n\n다른 명확하고 효과적인 방법은 LLM이 최종 답변을 생성하기 전에 검색된 콘텐츠를 평가하도록 하는 것입니다. 이를 통해 LLM은 LLM 비평을 통해 적합하지 않은 문서들을 걸러낼 수 있습니다. 예를 들어, Chatlaw에서 LLM은 참조된 법적 규정에 대한 자체 제안을 받아 그들의 적합성을 평가하게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 5세대\n\n사용자의 쿼리와 검색된 컨텍스트 정보에 기반하여 답변을 생성하기 위해 LLM을 활용하십시오.\n\n# 생성기 선택\n\n시나리오에 따라 LLM의 선택은 다음 두 가지 유형으로 분류될 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 클라우드 API 기반 생성기\n\n클라우드 API를 기반으로하여 OpenAI의 ChatGPT, GPT-4, Anthropic Claude 등을 활용하여 타사 LLMs를 호출합니다. 이점:\n\n- 서버 압력이 없음\n- 높은 동시성\n- 보다 강력한 모델 활용 가능\n\n단점:\n\n<div class=\"content-ad\"></div>\n\n- 데이터가 제3자를 통해 전달되어 데이터 개인 정보 보호에 대한 우려가 생깁니다\n- 모델을 조정할 수 없음 (대부분의 경우)\n- 온-프레미스\n\n로컬에 배포된 오픈 소스 또는 자체 개발된 LLMs, 예를 들어 람마 시리즈, GLM 등니다. 로컬에 배포된 모델은 클라우드 API 기반 모델과 정반대의 장단점을 가지고 있습니다. 로컬에 배포된 모델은 더 큰 유연성과 더 나은 개인 정보 보호를 제공하지만 더 많은 계산 리소스가 필요합니다.\n\n# 생성기 세부 조정\n\nLLM 사용뿐만 아니라 시나리오와 데이터 특성에 기반한 목표 지향적인 세부 조정은 더 나은 결과를 얻을 수 있습니다. 이것은 온-프레미스 설정을 사용하는 가장 큰 장점 중 하나이기도 합니다. 일반적인 세부 조정 방법은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- SFT\n\n특정 도메인에서 데이터가 부족한 경우, LLM에게 추가 지식을 제공하여 fine-tuning을 통해 LLM에게 도움이 될 수 있습니다. Huggingface의 fine-tuning 데이터 또한 초기 단계로 활용될 수 있습니다.\n\nFine-tuning의 또 다른 이점은 모델의 입력과 출력을 조정할 수 있는 능력입니다. 예를 들어, 이것은 LLM이 특정 데이터 형식에 적응하고 지시된 특정한 스타일로 응답을 생성할 수 있도록 할 수 있습니다.\n\n- RL\n\n<div class=\"content-ad\"></div>\n\nLLM 출력물을 인간 또는 검색기 선호사항에 맞게 조정하는 것은 가능한 접근 방식입니다. 예를 들어 최종 생성된 답변에 대해 수동으로 주석을 달고 그에 대한 피드백을 통해 강화 학습을 제공하는 것이 있습니다. 인간 선호도에 맞추는 것뿐만 아니라 세밀하게 조정된 모델과 검색기의 선호도에도 부합시킬 수 있습니다.\n\n- 증류\n\n강력한 사유재 모델이나 더 큰 매개변수의 오픈소스 모델에 액세스 할 수 없는 상황에서, 강력한 모델(e.g. GPT-4)을 증류하는 간단하고 효과적인 방법이 있습니다.\n\n- 이중 미세 조정\n\n<div class=\"content-ad\"></div>\n\nRAG 프로세스를 제어하는 데 사용되는 모듈을 가리키는 Orchestration입니다. RAG는 더 이상 고정 프로세스를 따르지 않고, 주요 시점에서 결정을 내리고 결과에 따라 동적으로 다음 단계를 선택하는 것을 의미합니다. 이것은 Naive RAG와 비교해 모듈화된 RAG의 주요 특징 중 하나입니다.\n\n<div class=\"content-ad\"></div>\n\n판사 모듈은 RAG 프로세스의 중요한 부분을 평가하여 외부 문서 저장소를 검색해야 하는 필요성, 답변의 만족도 및 추가적인 탐색이 필요한지를 결정합니다. 주로 반복적이고 반복적이며 적응적인 검색에 사용됩니다. 구체적으로는 다음 두 가지 연산자가 주로 포함됩니다:\n\n- 규칙 기반\n\n미리 정의된 규칙을 기반으로 후속 조치가 결정됩니다. 일반적으로 생성된 답변은 평가되고, 그런 다음 점수가 미리 정의된 임계값을 충족하는지에 따라 계속할지 중지할지가 결정됩니다. 일반적인 임계값에는 토큰의 확신 수준이 포함됩니다.\n\n- 프롬프트 기반\n\n<div class=\"content-ad\"></div>\n\nLLM은 다음 조치를 자율적으로 결정합니다. 이를 실현하는 데 주로 두 가지 접근 방식이 있습니다. 첫 번째는 대화 기록을 기반으로 LLM에게 반영하거나 판단하도록 하는 것으로, ReACT 프레임워크에서 볼 수 있습니다. 이 방법의 장점은 모델을 미세 조정할 필요가 없다는 것입니다. 그러나 판단의 출력 형식은 LLM이 지침을 준수하는 정도에 따라 달라집니다. FLARE는 프롬프트 기반 사례입니다.\n\n- 조정 기반\n\n두 번째 접근 방식은 LLM이 특정 작업을 트리거하기 위해 특정 토큰을 생성하는 것인데, 이 방법은 Toolformer로 거슬러 올라갈 수 있으며 RAG에서 적용되며 Self-RAG에 적용됩니다.\n\n# 퓨전\n\n<div class=\"content-ad\"></div>\n\n이 개념은 RAG Fusion에서 유래되었습니다. 질의 확장 섹션에서 언급된 바와 같이 현재 RAG 프로세스는 더 이상 단일 파이프라인이 아닙니다. 종종 다양한 분기를 통해 검색 범위나 다양성을 확대해야 합니다. 따라서 여러 분기로 확장한 뒤에는 퓨전 모듈이 여러 대답을 병합하는데 필요합니다.\n\n- 가능성 앙상블\n\n퓨전 방법은 여러 분기에서 생성된 서로 다른 토큰의 가중치값에 기반하여 최종 출력물의 종합적 선택을 이끌어냅니다. 주로 가중 평균을 사용합니다. REPLUG 참조.\n\n- RRF (Reciprocal Rank Fusion)\n\n<div class=\"content-ad\"></div>\n\nRRF는 여러 검색 결과 목록의 순위를 결합하여 하나의 통합된 순위를 생성하는 기술입니다. 워털루 대학교 (캐나다)와 Google과의 협력으로 개발된 RRF는 단일 분기 아래의 청크를 재배치하는 것보다 더 효과적인 결과를 생성합니다.\n\n결론\n\nRAG Flow에 관한 내용은 곧 출판될 PART II에서 소개될 예정입니다.\n\n이번이 Medium에 기사를 처음 게시하는 것이라서 많은 기능에 아직 익숙해지고 있습니다. 피드백과 비평은 언제나 환영합니다.","ogImage":{"url":"/assets/img/2024-06-23-ModularRAGandRAGFlowPart_0.png"},"coverImage":"/assets/img/2024-06-23-ModularRAGandRAGFlowPart_0.png","tag":["Tech"],"readingTime":16},{"title":"초보자를 위한 통합 가이드 Python을 사용한 LLM 로컬 배포 쉽게 하는 방법","description":"","date":"2024-06-23 19:17","slug":"2024-06-23-LLMsMadeAccessibleABeginnersUnifiedGuidetoLocalDeploymentviaPython","content":"\n\n![2024-06-23-LLMsMadeAccessibleABeginnersUnifiedGuidetoLocalDeploymentviaPython_0.png](/assets/img/2024-06-23-LLMsMadeAccessibleABeginnersUnifiedGuidetoLocalDeploymentviaPython_0.png)\n\n인터넷에는 개인 컴퓨터에서 대형 언어 모델 (LLMs)을 실행하는 방법에 대한 가이드가 넘쳐나지만, 이 방대한 정보를 효율적으로 탐색하는 것은 특히 새로운 사용자에게는 어려운 일일 수 있습니다. 대부분의 가이드는 특정 접근 방식에 초점을 맞추기 때문에 초보자들이 포괄적인 길을 찾기 어려울 수 있습니다. 여기서 저의 기사가 등장하여, 몇 가지 유명한 방법들을 하나로 통합하고 초보자들을 위해 특별히 제작된 이를 제공합니다. 이 가이드의 독특한 접근 방식은 Python 기반 방법에 집중하고 있으며, Python의 간단함과 AI 커뮤니티 내에서의 광범위한 사용을 인정하고 있습니다. 이 가이드는 깔끔하고 단계별 지침서, 다양한 Python 라이브러리와 도구에 대한 통찰, 그리고 실용적인 팁을 제공하여 당신의 LLMs 세계로의 진입을 원활하게 만들어주려고 만들어졌습니다. 기술적인 세부 사항의 다양성이 혼란스럽게 느껴졌거나 선택지가 많았던 분들에게는 이 기사가 꼭 필요한 리소스로, 당신의 컴퓨터에서 LLMs를 직접 실행하기 위한 접근 가능하고 깨우침을 주는 여정을 보장합니다. 여러 프레임워크에서 모델을 작업하는 빠른 소개를 제공하는 이 기사는 CPU 또는 GPU를 사용하더라도 모든 측면을 깊이 다루지는 않습니다. 더 포괄적인 통찰을 위해서는 관련 문서를 참고하는 것이 좋습니다.\n\n이 가이드는 다음을 사용하여 LLMs를 로컬로 실행하는 방법을 공유할 것입니다\n- Huggingface: 방대한 모델 저장소와 직관적 인터페이스를 갖춘 포괄적인 라이브러리.\n- Llama.cpp Python: llama.cpp를 활용해 성능이 C++ 수준인 LLMs를 사용하는 Python 친화적 프론트 엔드.\n- llama.cpp 기반 GPT-3.5에 대한 API 드롭인 대체: 모델을 독립된 프로세스에서 실행하면, 동일한 기계 또는 서버에서 GPT 3.5 클라이언트들이 사용하는 API와 유사한 방식으로 추론이 가능해집니다.\n\n<div class=\"content-ad\"></div>\n\n상기한 방법들은 13세대 인텔 랩터 레이크 프로세서가 탑재된 Windows 11 시스템에서 WSL2을 사용하여 성공적으로 테스트되었으며, NVIDIA RTX 3060 카드와 16GB 메모리, CUDA 버전 12.1을 사용하고 있습니다.\n\n# 대형 언어 모델(LLM) 실행에 대한 고려 사항\n\n대형 언어 모델(LLM)의 세계로 진입하면 효율적인 배포와 운영을 위한 중요한 고려 사항이 드러납니다. 하드웨어 요구 사항부터 모델 액세스 프로토콜까지 각 요소가 LLM의 모든 잠재력을 활용하는 데 중요한 역할을 합니다.\n\nLLM의 효율적인 운영을 위해서는 강력한 GPU가 필수적입니다. 퀀터제이션을 필요로 하는 모델들에게 특히 중요한데, GPT-2와 같은 작은 모델은 CPU에서 실행할 수 있지만 시스템 RAM에 제한을 받습니다. 예를 들어, GPT-2의 500MB 크기는 현대 CPU에서 관리가 가능하지만,  Llama 2 7B(13GB 크기)와 같은 큰 모델은 기술적으로 CPU에서 실행할 수 있지만, 충분한 RAM이 있어도 추론 속도가 느려집니다.\n\n<div class=\"content-ad\"></div>\n\nHuggingFace 라이브러리는 자동 모델 다운로드를 용이하게 하고 다양한 모델에 액세스할 수 있도록 도와줍니다. Meta의 Llama2와 같은 일부 모델에 액세스하려면 Hugging Face를 통해 계정 설정, 애플리케이션 검토 및 Meta의 사용 정책 준수가 필요한 승인 프로세스가 필요합니다.\n\nGPU 배포는 GPU RAM 용량에 따라 LLM에 가장 적합합니다. 일반적으로 모델은 float16 또는 float32 포맷이며, NVidia 4060 Ti와 같이 더 높은 용량을 갖는 GPU에서는 전체 모델 로딩이 가능합니다(16GB RAM). RAM이 적은 GPU의 경우 양자화는 원래 크기의 일부로 메모리 요구사항을 줄여 호환성 및 추론 속도를 향상시키지만, 정확도 손실은 최소화됩니다. GPU 기반 LLM 작업에는 전반적으로 양자화를 권장합니다. 양자화는 대규모 언어 모델(LLMs)이 로컬 기계에서 효율적으로 작동할 수 있도록 하는 데 중요한 역할을 합니다. 기본적으로 양자화는 모델의 가중치를 표현하는 데 사용되는 숫자의 정밀도를 줄이는 프로세스로, 성능을 크게 희생하지 않으면서 모델의 크기를 크게 줄입니다. GPU 추론에 대해, 이 가이드는 Nvidia GPU를 CUDA 프레임워크와 함께 사용하는 데 초점을 맞추고 있습니다.\n\nGoogle Colab은 Nvidia GPU에 액세스하여 개인 하드웨어가 필요하지 않게 해주는 편리한 클라우드 기반 솔루션을 제공합니다. 주요 라이브러리와의 쉬운 통합을 지원하여 모델 실행이 간편해집니다. 그러나 무료 계정 사용자는 액세스 기간 제한 및 GPU 가용성과 같은 제한 사항을 직면할 수 있으며, 이는 긴 기간 또는 더 많은 리소스를 필요로 하는 프로젝트에 영향을 줄 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 이러한 구성 요소를 각각 설정하는 구체적인 내용을 살펴보겠습니다.\n\n---\n\n## HuggingFace 사용하기\n\nHugging Face는 자연 언어 처리(NLP) 분야에서 특히 열린 소스 라이브러리인 Transformers를 갖고 인공 지능 분야를 혁신시켰습니다. 이 라이브러리는 텍스트 분류 및 언어 생성과 같은 작업을 위한 다양한 사전 훈련된 모델을 제공하여 복잡한 NLP 작업의 구현을 간단하게 합니다. 그들의 작업은 다양한 분야에서 AI 응용 프로그램을 신속하게 개발할 수 있도록 하며 모델을 처음부터 훈련할 필요가 없게 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 설정\n\n작동되는 구성:\n\n- Python 3.8+ (3.10 버전에서 테스트됨)\n- Windows 11, Windows Subsystem for Linux (WSL) 및 Ubuntu 22.04에서 확인됨.\n- Transformer 버전 4.36.1에서 테스트됨 (이후 버전은 inference.py에 변경이 필요할 수 있음)\n- Intel 12세대 이상\n- (선택 사항) GPU (예: Nvidia RTX3060 12 GB) 및 CUDA 12.1이 장착되어 있음\n\n다음 섹션에서는 CPU 전용 및 GPU용으로 Linux 및 Windows에 대한 설치 지침을 나열할 것입니다. 해당 지침에 따라 설치하세요.\n\n<div class=\"content-ad\"></div>\n\nLinux/WSL (CPU/GPU)\n\nCPU만 설정하는 경우에 필요한 단계이며 GPU 설정을 구성하기 위한 예비 요구 사항으로도 사용됩니다.\n\n```js\n$ sudo apt install python3.10-venv python3.10-tk\n$ pip install virtualenv \n$ python3.10 -m venv  venv\n$ source ./venv/bin/activate\n(venv)$ pip3 install tk numpy torch bertviz ipython transformers accelerate huggingface_hub hf_transfer\n```\n\nLinux/WSL (GPU)\n\n<div class=\"content-ad\"></div>\n\n리눅스에서 GPU 지원을 위해 추가 패키지 설치하기\n\n```js\n(가상환경)$ pip3 install bitsandbytes accelerate autoawq optimum auto-gptq\n```\n\nWindows 설치 (CPU/GPU)\n\n이 단계들은 CPU에서 설정하는 데 필수적이며, 윈도우에서 GPU 설정을 구성하는 데 기본 요구사항으로 작용합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n> pip install virtualenv\n> python3.10 -m venv  venv\n> venv\\Scripts\\activate\n(venv)> pip3 install tk numpy torch bertviz ipython transformers huggingface_hub hf_transfer\n```\n\n+Windows (GPU)\n\n윈도우에서 GPU용 추가 패키지를 설치/업데이트 해주세요.\n\n```js\n(venv)> pip3 install accelerate autoawq optimum auto-gptq\n(venv)> pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n(venv)> python -m pip install bitsandbytes==0.39.1 --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\n```\n\n<div class=\"content-ad\"></div>\n\nNVidia GPU에서 Large Language Models (LLMs)를 실행하기 위해 몇 가지 설치가 필요합니다:\n\n- PyTorch 및 CUDA 설정: NVidia GPU 기능을 활용하기 위해 torch, torchvision 및 torchaudio를 CUDA 지원과 함께 설치해야 합니다. 일반적으로 Linux 사용자는 기본 PyTorch 설치에서 CUDA 지원을 받지만, Windows 사용자는 torch+cuda 설치를 위한 index-url을 지정해야 합니다. 최신 업데이트에 따르면 PyTorch는 CUDA toolkit 버전 12.1까지 호환됩니다. CUDA 호환성 및 설치 안내에 대한 최신 정보는 PyTorch 웹사이트 (PyTorch — 로컬에서 시작하기)에서 확인할 수 있습니다.\n- bitsandbytes 설치 변형: bitsandbytes의 표준 설치는 Linux에 최적화되어 있으며, 하나의 릴리스 빌드에서 여러 CUDA 버전을 수용합니다. 그러나 Windows 사용자는 소스에서 직접 빌드하거나 비공식 GitHub 저장소를 사용하여 설치해야 합니다. Windows용 구체적인 설치 지침은 이 기사의 이전 섹션에서 찾을 수 있습니다. Windows를 위한 추가 정보 및 안내는 bitsandbytes 비공식 Windows 저장소(https://github.com/jllllll/bitsandbytes-windows-webui)에서 확인할 수 있습니다.\n- Cuda Toolkit 필요성: GPU 기반 작업을 용이하게 하기 위해 https://developer.nvidia.com/cuda-12-1-0-download-archive에서 Cuda toolkit을 설치하세요. 사용 중인 torch+cuda 버전과 CUDA toolkit 버전을 일치시키는 것이 좋습니다. 현재 최신 torch는 CUDA 버전 12.1을 지원합니다.\n- Microsoft C++ 빌드 도구: Windows에서 GPU 프로세스를 실행하려면 Microsoft C++ 빌드 도구를 설치해야 합니다. 이는 accelarate 및 autoawq 패키지의 psutils 종속성으로 인한 것입니다.\n- autoawq 패키지는 GPU에서 AWQ 기반 양자화 모델을 실행해야 하는 경우에만 필요합니다.\n\nWindows 시스템의 특정 요구 사항을 고려하여 NVidia GPU에서 LLMs를 효과적으로 실행하기 위한 중요한 단계입니다. huggingface 설치 및 모델 다운로드에 대한 자세한 정보는 link1과 link2를 확인하세요.\n\nHuggingface에 계정을 생성하면 토큰을 생성할 수 있는 능력이 생깁니다. 이 토큰은 export(예: HF_TOKEN)을 통해 환경에 구성하거나 Huggingface API와 함께 매개변수로 사용하여 모델 다운로드 및 다른 포턜 작업에 사용할 수 있습니다. 이 토큰은 서비스에 액세스하기 위한 패스로 생각할 수 있습니다. 또한 Llama와 같은 특정 모델을 다운로드하려면 승인이 필요할 수 있으며, 계정을 사용하여 Huggingface 포턜을 통해 요청할 수 있습니다. 부여된 모든 승인은 계정과 그 계정으로 생성된 모든 토큰과 연결됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 추론\n\n이 스크립트는 추론 모드를 구체화하여 모델 실행에 적합한 하드웨어(CPU 또는 GPU)를 선택합니다. 만약 추론 모드가 'cpu'로 설정되어 있다면, GPT-2 모델이 로드되며, 작은 크기(~500MB)로 CPU 사용에 이상적이기 때문에 'cpu'로 설정된 device_map을 사용하여 타겟팅된 실행이 가능합니다. 반대로, GPU 사용 시에는 더 큰 Meta LLaMA 2 7B 모델(~13GB)을 선택하며, 'auto'로 설정된 device_map을 활용하여 유연한 장치 할당을 하고 load_in_4bit을 활성화하여 메모리 사용량을 최소화하여 한정된 메모리를 가진 GPU에서 큰 모델의 작동을 촉진합니다. pipeline 함수는 로드된 모델과 토크나이저를 사용하여 텍스트 생성 파이프라인을 생성하며, max_new_tokens와 같은 매개변수를 설정하여 생성된 텍스트의 길이를 제어합니다. 또한 streamer가 파이프라인에 전달되어, 스트리밍된 텍스트 생성이 사용될 것을 나타냅니다.\n\ninference.py\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, pipeline, BitsAndBytesConfig\n\ninference_type = 'cpu'\nif inference_type == 'cpu':\n    # CPU 추론\n    model_name = 'gpt2'  # ~500MB 원본 크기\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map='cpu'\n    )\nelse:    \n    # GPU 추론; 'gpu'\n    model_name = 'meta-llama/Llama-2-7b-hf'  # ~13GB 원본 크기\n    quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name, \n        device_map='auto',\n        quantization_config=quantization_config)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nstreamer = TextStreamer(\n     tokenizer,\n     skip_prompt=False,\n     skip_special_tokens=False\n)\ntext_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=100,  # 생성할 토큰의 최대 수\n    streamer=streamer,     \n)\nprompt='I like apple pie'\ntext_pipeline(prompt) \n``` \n\n<div class=\"content-ad\"></div>\n\n제공된 예제에서는 사전 학습된 모델을 사용하고 있습니다. 이 모델은 일련의 토큰과 같은 특정 키워드를 필요로 할 수도 있습니다. 모델이 이러한 토큰을 요구하는지 확인하려면 tokenizer_config.json 파일에서 add_bos_token 및 add_eos_token 설정을 확인할 수 있습니다. 이를 자동으로 처리하는 실용적인 방법은 text_pipeline 추론 호출에서 add_special_tokens=True (기본값 False)로 설정하는 것입니다. 이렇게 하면 프레임워크가 모델로 전송되기 전에 프롬프트에 특수 키워드를 삽입할지 여부를 결정할 수 있습니다. 위 예시에서는 해당 매개변수를 사용하지 않았습니다.\n\n스크립트 실행\n\n```js\n(venv)$ python inference.py\n```\n\n출력:\n\n<div class=\"content-ad\"></div>\n\nBitsAndBytesConfig가 load_in_4bit=True로 전달되어 있습니다. 이는 변환 라이브러리에게 모델의 가중치를 4비트로 줄이라는 지시를하는 것이고, 이는 모델의 원래 크기의 25%에 해당합니다 (Llama 2는 float16 형식입니다). 이는 추론 속도를 향상시킵니다. bnb_4bit_compute_dtype는 torch.bfloat16으로 설정되어 있어 GPU에서 실제 계산이 여전히 float16로 수행되도록 합니다. 선택할 수 있는 대안 옵션은 load_in_8bit=True이며, load_in_4bit 옵션과 상호 배제됩니다. 8비트 모드는 약간 향상된 정확성을 제공하지만 모델 크기를 원래 크기의 50%로 줄이고 추론 속도를 낮춥니다. 양자화 옵션을 선택하지 않은 경우 변환 라이브러리는 가용한 VRAM에 따라 전체 모델을 GPU의 비디오 RAM에로드하는 것을 기본값으로 설정합니다.\n\n# 사전 양자화 모델 로드\n\nGPU 추론 시 모델을 로드할 때마다 양자화하는 대신, 디스크에서 사전 양자화 된 모델을로드할 수 있습니다. 인기있는 양자화 형식 중 두 가지는 GPTQ 및 AWQ입니다. Huggingface는 둘 다 지원합니다.\n\nAWQ 유형 모델의 경우 아래 코드로 모델로드 라인을 대체하면 됩니다 (GPU 추론 경로에서). BitsAndBytesConfig 매개 변수가 생략됩니다. 왜냐하면 모델이 이미 양자화되어 있기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nmodel_name='TheBloke/zephyr-7B-alpha-AWQ' \nmodel = AutoModelForCausalLM.from_pretrained( \n    model_name, \n    device_map='auto')\n```\n\nGPTQ 모델의 경우 아래 라인으로 모델 로드를 대체합니다.\n\n```js\nmodel_name='TheBloke/zephyr-7B-beta-GPTQ' \nmodel = AutoModelForCausalLM.from_pretrained( \n    model_name, \n    revision='main', \n    device_map='auto')\n```\n\nGitHub 브랜치에 해당하는 revision을 사용했습니다. zephyr-7B-beta-GPTQ 양자화 버전은 서로 다른 브랜치에 저장되어 있습니다. 자세한 내용은 https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ의 ‘Files and versions’ 탭을 참조해주세요.\n\n<div class=\"content-ad\"></div>\n\n# GGUF 양자화\n\nGGUF는 llama.cpp에서 소개된 인기 있는 양자화 방법입니다. ctransformers는 GGML/GGUF 라이브러리를 사용하여 C/C++로 구현된 Transformer 모델용 Python 바인딩입니다. 또한 RAG Langchain과 통합되어 있습니다.\n\n귀하의 구성에 따라 ctransformers 패키지를 설치하세요.\n\n```js\n(venv)$ pip install ctransformers\n```\n\n<div class=\"content-ad\"></div>\n\nhuggingface-cli를 사용하여 GGUF 형식 모델을 다운로드하거나 from_pretrained() 호출을 통해 해당 모델의 첫 API 호출 시 자동으로 다운로드됩니다. 아래 코드 샘플은 GGUF 형식의 모델에 대해 ctransformers를 활용하는 방법을 보여줍니다. from_pretrained 메서드는 이전에 다운로드되지 않은 경우 Hugging Face에서 모델을 자동으로 가져옵니다. zephyr GGUF와 같은 일부 모델 유형은 동일한 리포지토리에 파일로 저장됩니다. 여기서 리포지토리 이름은 'TheBloke/zephyr-7B-beta-GGUF'이고 모델 파일은 zephyr-7b-beta.Q4_K_M.gguf입니다.\n\ninference.py\n\n```js\nfrom ctransformers import AutoModelForCausalLM\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\n    'TheBloke/zephyr-7B-beta-GGUF',\n    model_file='zephyr-7b-beta.Q4_K_M.gguf',\n    model_type='llama',\n    gpu_layers=32\n)\nprompt='<s>I like apple pie'\n# Do inference with streaming\nstream=llm(prompt, stream=True)\nfor chunk in stream:\n    print(chunk, end=\"\", flush=True)\n```\n\n사용 중인 특정 모델에 따라 gpu_layers 설정을 조정하세요. 이 경우, GPU에 32개 레이어를 할당하도록 설정합니다. 추론에 stream=True을 설정하면 모델이 생성하는 토큰을 실시간으로 표시할 수 있습니다. zephyr용 다양한 양자화된 모델 버전은 https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF에서 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n\n# LLama.cpp python\n\nLLama.cpp은 다양한 백엔드(CUDA, Metal, OpenCL, SYCL)를 지원하여 적응성을 높인 유연한 C/C++ 라이브러리로, Meta의 LLaMA 모델과 같은 대규모 언어 모델에서 추론을 실행할 수 있는 능력으로 AI 분야에서 인식을 얻고 있습니다. llama-cpp-python 패키지를 통한 Python 통합은 사용자가 C/C++의 성능을 누리면서도 Python의 간편함을 누릴 수 있도록 합니다.\n\nllama-cpp-python을 설치하는 선호하는 방법은 소스에서 컴파일하는 것입니다. 이 방법을 권장하는 이유는 기밀된 C/C++ 라이브러리인 llama.cpp이 특정 시스템에 맞춘 컴파일러 최적화를 활용하기 때문입니다. 미리 빌드된 이진 파일을 선택하면 이러한 최적화를 포기하거나 다양한 플랫폼용 이진 파일을 관리해야 할 수도 있습니다. llama-cpp-python을 컴파일하면 llama.cpp 라이브러리를 자동으로 라이브러리 파일(lib)로 빌드합니다. 이 라이브러리 파일은 특정 바인딩을 통해 Python에서 활용되어, Python 스크립트가 llama.cpp의 기능에 액세스하고 사용할 수 있도록 합니다. 컴파일 세부 정보는 llama-cpp-python의 github 저장소를 참조하세요.\n\n<div class=\"content-ad\"></div>\n\n시스템에는 해당 버전의 BLAS 라이브러리가 설치되어 있어야 합니다. llama.cpp README의 ‘BLAS 빌드’ 섹션을 확인해보세요.\n\nIntel ARC GPU는 SYCL을 통해 지원되며, 이를 위해서는 Intel OneAPI가 설치되어 있어야 합니다. README의 SYCL 섹션을 확인해보세요.\n\nllama-cpp-python의 자세한 빌드 지시사항은 https://github.com/abetlen/llama-cpp-python에서 찾을 수 있습니다.\n\n간단한 지시사항은 여기에 있습니다.\n\n<div class=\"content-ad\"></div>\n\nCPU 또는 GPU용 llama-cpp-python 패키지를 설치해 보세요.\n\n```js\n# CPU 빌드용\n(venv)$ CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n\n# Nvidia GPU 빌드용\n(venv)$ CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n\n# Intel ARC GPUS를 위한 SYCL을 통한 빌드\nsource /opt/intel/oneapi/setvars.sh   \nCMAKE_ARGS=\"-DLLAMA_SYCL=on -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx\" pip install llama-cpp-python\n```\n\nllama-cpp-python은 Hugging Face에서 얻을 수 있는 GGUF 모델 형식을 요구합니다. 그러나 llama의 API 사양으로 인해, 모델을 수동으로 다운로드하여 다른 디렉토리에 저장한 다음 해당 디렉토리 경로를 llama-cpp-python에 지정해야 합니다. 이번에는 우리의 정규 절차에서 벗어나, 명시적 로컬 폴더로 직접 다운로드하는 방식으로 사용 설명서 tuning된 Llama 2 모델을 사용하겠습니다.\n\n```js\n(venv)$ huggingface-cli download TheBloke/Llama-2-7b-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks True\n```\n\n<div class=\"content-ad\"></div>\n\nlocal-dir-use-symlinks를 True로 설정하여 활성화하면 현재 디렉토리에 모델을 캐시 디렉토리에 이미 다운로드된 모델을 가리키는 심볼릭 링크를 생성하려고 시도하므로 시간과 공간을 효율적으로 사용할 수 있습니다. 심볼릭 링크를 사용하지 않고 현재 디렉토리에 모델의 완전한 사본이 필요하다면 이 옵션을 생략하는 것이 가장 좋습니다.\n\n현재 디렉토리에 모델을 다운로드한 경우 model_path는 절대 경로 또는 상대 경로로 지정해야 합니다.\n\ninference.py\n\n```python\nfrom llama_cpp import Llama\nimport llama_cpp\nllm = llama_cpp.Llama(model_path=\"./llama-2-7b-chat.Q4_K_M.gguf\",\n                verbose=True, n_gpu_layers=-1, chat_format=\"llama-2\")\nprompt = '[INST] Hi there, write me 3 random quotes [/INST]'\nstream = llm(prompt, max_tokens=2048, echo=False, temperature=0, stream=True)\nresult = \"\"\nfor output in stream:\n    result += output['choices'][0]['text']\n    print(output['choices'][0]['text'], end=\"\")\n```\n\n<div class=\"content-ad\"></div>\n\n위의 코드는 시스템이 Llama2 7B 모델의 모든 레이어(n_gpu_layers=-1)를 GPU에로드하도록 지시합니다. 사용 가능한 VRAM에 따라 n_gpu_layers 값을 수정하여 CPU와 GPU 사이의 작업 부하를 분산시킬 수 있으며, 예를 들어 대규모 모델을 효과적으로 관리하기 위해 작업을 균등하게 분할할 수 있습니다.\n\n프롬프트 구문의 중요성\n\n위에서 사용된 모델은 채팅용으로 조정된 모델이므로 프롬프트 형식은 매우 중요합니다([INST] 사용). 이는 모델의 이해와 작업 실행에 상당한 영향을 미치기 때문입니다. 지시에 따라 응답을 생성할 수 있도록 특별히 설계된 지시에 조정된 모델은 상세 명령을 따르고 프롬프트에서 제공된 지침에 따라 응답을 생성합니다. 올바른 구문은 모델이 지시의 의도를 정확하게 이해하고 결과가 정확하고 타당한 것을 보장하는 데 중요합니다. 또한 다양한 모델 아키텍처는 다른 프롬프트 구문에 최적으로 반응할 수 있도록 세밀하게 조정됩니다.\n\n<div class=\"content-ad\"></div>\n\n# llama.cpp 기반의 GPT-3.5 대체품입니다.\n\n이 섹션은 llama.cpp를 OpenAI의 GPT 엔드포인트 대신 사용할 수 있는 방법을 탐구합니다. llama.cpp 모델을 활용하여 OpenAI의 서비스에 의존하지 않고 로컬 llama.cpp 모델을 사용하여 GPT 기반 애플리케이션을 운영할 수 있습니다. 로컬 API 서버를 실행함으로써 OpenAI의 GPT API 엔드포인트의 기능을 모방하면서도 llama 모델을 사용하여 요청을 처리할 수 있습니다. 이는 GPT-3.5 또는 GPT-4를 대상으로 한 애플리케이션이 llama.cpp를 사용하도록 원활하게 전환할 수 있음을 의미합니다. 최종 목표는 비용을 절감하는 것뿐만 아니라 데이터가 로컬 환경 내에서 개인 정보 보호 및 안전성을 유지하도록 하는 것입니다.\n\n![이미지](/assets/img/2024-06-23-LLMsMadeAccessibleABeginnersUnifiedGuidetoLocalDeploymentviaPython_2.png)\n\n먼저 llama-cpp-python과 서버 지원 및 필수 요소를 설치해야 합니다. 패키지가 처음에 CPU 사용을 위해 설정되어 있고 이제 GPU 사용으로 전환하고 싶은 경우(또는 그 반대인 경우) 새 대상을 위해 설치 명령을 다시 실행하여 다시 설치해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# CPU 빌드용\n(venv)$ CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python[server] --upgrade --force-reinstall --no-cache-dir\n\n# Nvidia GPU 빌드용\n(venv)$ CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python[server] --upgrade --force-reinstall --no-cache-dir\n```\n\nopenai 패키지 설치\n\n```js\n(venv)$ pip install openai\n```\n\nhuggingface-cli 도구를 사용하여 로컬 폴더에 모델 다운로드하기\n\n\n<div class=\"content-ad\"></div>\n\n```js\n(venv)$ huggingface-cli TheBloke/Llama-2-7b-Chat-GGUF 다운로드 llama-2-7b-chat.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks True\n```\n\n모델에 입력하기 전에 프롬프트를 Llama-2 형식으로 구조화하는 방법과 함께 서버를 실행합니다.\n\n```js\n(venv)$ python3 -m llama_cpp.server --model ./llama-2-7b-chat.Q4_K_M.gguf --n_gpu_layers 35 --chat_format llama-2\n```\n\n서버는 기본적으로 포트 8000을 사용하도록 구성되어 있습니다. 아래의 클라이언트 스크립트에서 서비스에 연결하도록 설계된 경우 사용자 정의 포트 실행에 필요한 포트 번호를 수정하세요. 모델 매개변수에는 더미 값(e.g. xxxxx)이 할당되어 있으며, 이는 서버에서 무시되며 본 예제에서는 간편함을 위해 인증 프로세스가 포함되어 있지 않습니다. 또한 서버가 올바르게 실행 중인지 확인하려면 브라우저에서 http://localhost:8000/docs#/을 방문하세요.\n\n\n<div class=\"content-ad\"></div>\n\nclient.py\n\n```python\nfrom openai import OpenAI\nclient = OpenAI(base_url=\"http://127.0.0.1:8000/v1\") \nstream = client.chat.completions.create(\n  model=\"xxxxx\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a well-read scholar with a deep appreciation for literature, especially when it comes to the subject of artificial intelligence (AI)\"},\n    {\"role\": \"user\", \"content\": \"Give me 3 quotes on AI.\"}\n  ],\n  temperature=0.01,\n  stream=True,  \n)\nfor chunk in stream:\n    if not chunk.choices or chunk.choices[0].delta.content is None:\n        continue\n    print(chunk.choices[0].delta.content, end=\"\")\nprint(\"\\n\")\n```\n\nRun the client script in another terminal.\n\n```bash\n(venv)$ python client.py\n```\n\n<div class=\"content-ad\"></div>\n\n수신된 출력 스트림은 터미널 창에 표시됩니다.\n\n출력 (모델에서 받은 대로):\n\n\n---\n# 참고 자료\n\n\n<div class=\"content-ad\"></div>\n\n- Hugging Face [https://huggingface.co/]\n- llama.cpp에 대한 Python 바인딩 [https://github.com/abetlen/llama-cpp-python]\n- 양자화 [https://huggingface.co/docs/optimum/concept_guides/quantization]\n- 당신에게 적합한 양자화 방법은 무엇인가요? (GPTQ vs. GGUF vs. AWQ) [https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be]\n- Nvidia CUDA [https://developer.nvidia.com/cuda-12-1-0-download-archive]","ogImage":{"url":"/assets/img/2024-06-23-LLMsMadeAccessibleABeginnersUnifiedGuidetoLocalDeploymentviaPython_0.png"},"coverImage":"/assets/img/2024-06-23-LLMsMadeAccessibleABeginnersUnifiedGuidetoLocalDeploymentviaPython_0.png","tag":["Tech"],"readingTime":18},{"title":"LLMs의 작동 원리 - 대형 언어 모델의 비밀 밝혀보기","description":"","date":"2024-06-23 19:16","slug":"2024-06-23-HowLLMsWork","content":"\n\n\n![LLMs operation](/assets/img/2024-06-23-HowLLMsWork_0.png)\n\nLLMs (Large Language Models) operate by predicting the next token based on a sequence of previous tokens. Each generated token is then used as input to generate the next one, enabling the model’s text generation capabilities.\n\n## Step 1: Prompts\n\nThe process starts with receiving a prompt, which is tokenized and converted into embeddings, or vector representations, of the input text. These embeddings, initially random, are learned during model training and represent a non-contextualized vector form of the input token.\n\n\n<div class=\"content-ad\"></div>\n\n## 단계 2: 인코딩\n\n이 모델은 층별 어텐션 및 피드포워드 계산을 수행하여 어휘 내 각 단어에 숫자(logit)를 할당합니다(디코더 모델인 GPT-X, LLaMA 등) 또는 컨텍스트화된 임베딩을 출력합니다(버트, 로버타, 일렉트라 등과 같은 인코더 모델).\n\n## 단계 3: 정규화\n\n디코더 모델의 경우, 최종 단계는 Softmax 함수를 사용하여 비정규화된 로짓을 정규화된 확률 분포로 변환하는 것을 포함합니다. 이는 생성된 텍스트에서 다음 단어를 결정합니다.\n\n<div class=\"content-ad\"></div>\n\n# 추가 세부 정보\n\n## 토큰화\n\n원시 입력 텍스트는 토큰화를 통해 종종 하위 단위 또는 단어로 분해됩니다. 이 과정을 통해 입력이 모델의 고정 된 어휘와 일치하도록되어 모델에 인식되도록합니다.\n\n## 임베딩\n\n<div class=\"content-ad\"></div>\n\n각 토큰은 임베딩 행렬을 사용하여 고차원 벡터로 매핑됩니다. 이 벡터 표현은 토큰의 의미적 의미를 포착하고 모델의 후속 계층에 입력으로 작용합니다. 이러한 임베딩에는 위치 인코딩이 추가되어 토큰 순서에 대한 정보를 제공하며, 내재된 시퀀스 인식이 부족한 트랜스포머와 같은 모델에 중요합니다.\n\n## 트랜스포머 아키텍처\n\n현대 LLM의 핵심인 트랜스포머 아키텍처는 여러 계층으로 구성됩니다. 각 계층에는 멀티 헤드 셀프 어텐션 메커니즘과 위치별 피드포워드 네트워크가 포함됩니다.\n\n![트랜스포머 아키텍처](/assets/img/2024-06-23-HowLLMsWork_1.png)\n\n<div class=\"content-ad\"></div>\n\n셀프 어텐션 메커니즘은 토큰이 다른 토큰과의 관련성을 평가할 수 있게 하여 모델이 입력의 관련 부분에 집중할 수 있게 합니다. 그 결과로 얻는 정보는 각 위치에서 독립적으로 피드포워드 신경망을 통해 처리됩니다.\n\n셀프-어텐션 또는 피드포워드 네트워크 등 각 서브 레이어에는 잔차 연결 후 레이어 정규화가 이어집니다. 이 설정은 활성화를 안정화시키고 학습을 가속화하는 데 도움이 됩니다.\n\n트랜스포머 레이어를 통과한 후 각 토큰의 최종 표현은 로짓 벡터로 변환됩니다. 각 로짓은 모델 어휘 중 단어에 해당하며 해당 단어가 시퀀스에서 다음 단어일 가능성을 나타냅니다.\n\n소프트맥스 함수는 로짓에 적용되어 확률로 변환됩니다. 이 정규화에 의해 확률은 합이 1이 되도록 보장되며 각 확률은 0과 1 사이에 있습니다. 가장 높은 확률을 가진 단어가 시퀀스에서 다음 단어로 선택됩니다.\n\n<div class=\"content-ad\"></div>\n\n이 테이블 태그를 마크다운 형식으로 변경해 주세요.","ogImage":{"url":"/assets/img/2024-06-23-HowLLMsWork_0.png"},"coverImage":"/assets/img/2024-06-23-HowLLMsWork_0.png","tag":["Tech"],"readingTime":2}],"page":"9","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true}