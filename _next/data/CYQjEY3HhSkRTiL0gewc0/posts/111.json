{"pageProps":{"posts":[{"title":"SQL 트랜잭션 및 ACID 속성","description":"","date":"2024-05-27 13:02","slug":"2024-05-27-SQLTransactionsandACIDProperties","content":"\n![image](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_0.png)\n\n# SQL에서 거래 소개\n\nSQL을 데이터베이스로 사용하는 은행 시스템을 상상해봅시다. 사용자 A가 사용자 B의 계좌로 돈을 입금하려고 합니다. 돈을 송금하면 사용자 A의 계좌 잔액에서 해당 금액을 빼내고 이 돈을 사용자 B의 계좌에 입금하려는데, 갑자기 데이터베이스가 크래시될 경우 어떻게 될까요?\n\n![image](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_1.png)\n\n<div class=\"content-ad\"></div>\n\n사용자 A의 잔고에서 인출한 돈이 사라졌다는 것을 의미합니까? SQL 데이터베이스에서는 그렇지 않습니다. 왜냐하면 SQL 트랜잭션을 사용하기 때문입니다.\n\n# 트랜잭션과 ACID 속성\n\n트랜잭션은 단일 원자 단위로 수행되는 하나 이상의 SQL 작업 시퀀스입니다. 이는 데이터베이스에서 데이터 일관성을 보장하기 위한 목적으로 사용됩니다. 트랜잭션은 주로 ACID 약어로 불리는 다음 속성을 갖습니다:\n\n- 원자성: 전체 트랜잭션은 전체가 성공하거나 전체가 실패하는 단위로 처리됩니다.\n- 일관성: 트랜잭션은 데이터베이스를 하나의 유효한 상태에서 다른 유효한 상태로 변환시키며 데이터베이스 불변을 유지합니다.\n- 고립성: 동시에 실행되는 트랜잭션에 의해 수행된 수정 사항은 서로 분리되어 커밋될 때까지 격리됩니다.\n- 지속성: 트랜잭션이 커밋되면 시스템 장애가 발생하더라도 지속되어 유지됩니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-SQLTransactionsandACIDProperties_2.png\" />\n\n# SQL 트랜잭션에서의 중요 명령어\n\nSQL 트랜잭션의 시작을 BEGIN TRANSACTION 키워드로 표시합니다.\n\n<img src=\"/assets/img/2024-05-27-SQLTransactionsandACIDProperties_3.png\" />\n\n<div class=\"content-ad\"></div>\n\n모든 트랜잭션 중에 발생한 변경 사항을 저장하려면 변경 사항을 데이터베이스에 COMMIT합니다.\n\n![이미지](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_4.png)\n\n트랜잭션 중에 문제가 발생하면 ROLLBACK 명령을 사용하여 트랜잭션 중에 수행된 모든 변경 사항을 되돌릴 수 있으며 데이터베이스를 트랜잭션 시작 시점의 상태로 되돌릴 수 있습니다.\n\n![이미지](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_5.png)\n\n<div class=\"content-ad\"></div>\n\n## 예시\n\n우리 간단한 은행 애플리케이션 예제로 돌아가 봅시다. 여기서는 계좌 A에서 계좌 B로 $100을 이체해야 합니다. 이 과정은 두 단계로 이루어집니다:\n\n- 계좌 A에서 금액을 차감하기\n- 그 금액을 계좌 B에 추가하기\n\n이 두 가지 단계는 모두 성공적으로 완료되어야 합니다. SQL 트랜잭션으로 이를 어떻게 작성할 수 있는지 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n\nBEGIN TRANSACTION;\n\n- Account A 잔액에서 100을 차감합니다.\n  UPDATE Accounts\n  SET balance = balance - 100\n  WHERE account_id = 'A';\n  -- Account A에 충분한 잔액이 있는지 확인하고, 부족하다면 롤백합니다.\n  IF @@ROWCOUNT = 0\n  ROLLBACK;\n  -- Account B 잔액에 100을 추가합니다.\n  UPDATE Accounts\n  SET balance = balance + 100\n  WHERE account_id = 'B';\n  -- 모든 것이 잘 되었다면 트랜잭션을 커밋합니다.\n  COMMIT;\n\n\n이 트랜잭션은 다음을 수행합니다:\n\n- 트랜잭션을 시작하여 다음 작업이 단일 원자적 프로세스의 일부임을 보장합니다.\n- Account A에서 $100을 차감합니다: account_id와 balance 열이 있는 'accounts'라는 테이블이 있다고 가정합니다.\n- Account A에 충분한 자금이 있는지 확인합니다: Account A에 충분한 금액이 없으면, ROLLBACK TRANSACTION을 사용하여 모든 변경 사항이 취소되는 롤백이 수행됩니다.\n- Account B에 $100을 추가합니다: Account A에 충분한 금액이 있다면, Account B에 $100이 추가됩니다.\n- 트랜잭션을 커밋합니다: 두 개의 업데이트가 모두 성공적으로 수행되면, COMMIT TRANSACTION 명령이 실행되어이 트랜잭션 중에 수행된 변경 사항을 영구적으로 적용합니다.\n\n이것은 두 계정이 적절히 업데이트되거나 어느 시점에서 문제가 발생할 경우 변경 내용이 적용되지 않으므로 데이터의 무결성이 유지됩니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n# SQL 트랜잭션의 격리 수준\n\n데이터베이스 트랜잭션의 격리 수준은 트랜잭션 무결성이 유지되는 방식 및 각 트랜잭션이 다른 트랜잭션과 얼마나 격리되는지를 결정합니다.\n\nSQL 표준은 일관성과 성능 간의 균형을 맞추기 위해 네 가지 격리 수준을 정의합니다.\n\n## 1. Read Uncommitted\n\n<div class=\"content-ad\"></div>\n\n- 설명: 격리의 가장 낮은 수준입니다. 트랜잭션이 커밋되기 전에 다른 트랜잭션이 작업한 변경 사항을 볼 수 있습니다.\n- 예시에 미치는 영향: 돈을 송금하는 도중에 다른 트랜잭션이 계좌 A 또는 B의 잔액을 업데이트하고 있다면, 이 트랜잭션은 이러한 커밋되지 않은 값을 읽을 수 있습니다. 이는 실제로 존재하지 않는 잔액을 보는 등의 문제를 발생시킬 수 있습니다 (다른 트랜잭션이 실패하고 롤백될 경우).\n\n## 2. Read Committed\n\n- 설명: 트랜잭션이 커밋된 데이터만 읽을 수 있도록 보장합니다.\n- 예시에 미치는 영향: 이 수준은 'Read Uncommitted'의 문제를 피하기 위해 커밋된 계좌 A와 B의 잔액만 읽도록 합니다. 하지만 트랜잭션 내에서 잔액을 여러 번 읽는 경우, 다른 트랜잭션이 데이터를 수정하는 경우 다른 값들을 보게 될 수 있습니다 (반복할 수 없는 읽기).\n\n## 3. Repeatable Read\n\n<div class=\"content-ad\"></div>\n\n- 설명: 거래가 데이터를 두 번째로 읽을 때 동일한 데이터 값을 찾을 수 있도록 보장합니다(반복되지 않는 읽기를 피함).\n- 예시에 미치는 영향: 이 수준은 거래 내에서 동일한 데이터의 여러 번의 읽기 사이에 다른 사람에 의해 생긴 변경 사항을 볼 수 없도록 합니다. 이는 잔액 확인 및 업데이트 작업 중 일관된 읽기 결과를 유지하는 데 도움이 됩니다. 그러나 다른 거래에 의해 추가된 새로운 행이 발생하는 유령 읽기를 막을 수는 없을 수도 있습니다.\n\n## 4. Serializable\n\n- 설명: 최고 수준의 격리. 거래가 직렬로 실행된 것처럼 완전히 격리됩니다.\n- 예시에 미치는 영향: 이는 완전한 격리를 보장합니다. 다른 거래가 이체 프로세스에 간섭할 수 없습니다. 모든 동시성 문제(더티 리드, 반복되지 않는 읽기 및 유령 읽기)를 방지하지만 동시성이 감소하고 잠금으로 인한 잠재적 성능 문제가 발생할 수 있습니다.\n\n다양한 격리 수준에서 여러 현상이 발생할 수 있으며, 더티 리드, 반복되지 않는 읽기 또는 유령 읽기와 같은 현상이 발생할 수 있습니다. 아래에서 이 용어가 의미하는 바를 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n# Dirty Reads\n\n더티 리드는 트랜잭션이 동시에 커밋되지 않은 다른 트랜잭션에 의해 작성된 데이터를 읽을 때 발생합니다. 결과적으로 다른 트랜잭션이 롤백하면 처음 트랜잭션이 데이터를 읽게 됩니다. 하지만 해당 데이터는 데이터베이스에 공식적으로 기록된 적이 없습니다.\n\n예시:\n\n- 트랜잭션 1이 시작되고 계좌 A에서 계좌 B로 $100을 이체합니다.\n- 트랜잭션 1이 커밋되기 전에 트랜잭션 2가 시작되고 계좌 A의 잔액을 읽습니다.\n- 트랜잭션 1이 실패하고 롤백되면, 트랜잭션 2는 공식적으로 커밋되지 않은 잔액을 읽게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n\n![image](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_6.png)\n\n# Non-Repeatable Reads (Read Uncommitted)\n\n이것은 트랜잭션 진행 중 같은 행이 두 번 조회되고, 두 번 조회 사이에 행 내의 값이 다른 경우 발생합니다. 본질적으로 다른 트랜잭션이 두 번의 조회 사이에 행을 수정한 경우입니다.\n\n예시:\n\n\n\n<div class=\"content-ad\"></div>\n\n- 거래 1이 시작되고 계정 A의 잔액을 읽습니다.\n- 거래 2가 계정 A에서 계정 B로 $100을 이체하고 커밋합니다.\n- 거래 1이 다시 계정 A의 잔액을 읽으면 이전과 다른 잔액을 볼 수 있습니다.\n\n![이미지](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_7.png)\n\n# 환상 읽기\n\n환상 읽기란 다른 트랜잭션에서 읽고 있는 레코드에 새로운 행이 추가되거나 (또는 기존 행이 삭제되는 경우) 트랜잭션 중에 발생합니다. 이는 동일한 트랜잭션에서의 후속 읽기가 원래 읽기의 일부가 아니었던 새로 추가된 행을 포함하거나 삭제되지 않은 행을 제외한 행 집합을 반환할 수 있음을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n예시:\n\n- Transaction 1은 계정 A의 거래 수를 세는 쿼리를 시작합니다.\n- Transaction 2는 계정 A에 새로운 거래 기록을 삽입하고 커밋합니다.\n- Transaction 1은 다시 계정 A의 거래 수를 세어보고 이전보다 더 많은 거래를 발견합니다.\n\n![그림](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_8.png)\n\n높은 격리 수준은 발생할 수 있는 현상의 종류를 줄이지만 더 낮은 동시성과 잠재적인 성능 영향을 감수해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n만약 SQL 데이터베이스의 확장 메커니즘에 대해 더 알고 싶다면, 데이터베이스 샤드, 복제 등을 다루는 깊이 있는 Database Essentials 비디오를 확인해보세요:\n\n여기 처음 오신 분들을 위해, 저는 Hayk입니다. 저는 웹 개발자 분들이 첫 번째 기술 직을 확보하거나 웹 개발 마스터리 커뮤니티에서 시니어 역할로 진출하는 데 도와드리고 있어요.\n\n웹 개발에 대한 주간 통찰력을 놓치고 싶지 않다면, 내 뉴스레터를 구독해주세요.\n\n","ogImage":{"url":"/assets/img/2024-05-27-SQLTransactionsandACIDProperties_0.png"},"coverImage":"/assets/img/2024-05-27-SQLTransactionsandACIDProperties_0.png","tag":["Tech"],"readingTime":6},{"title":"90의 작업을 처리할 수 있는 10가지 SQL 문장","description":"","date":"2024-05-27 13:00","slug":"2024-05-27-10SQLStatementsThatCanHandle90ofTasks","content":"\n구조화된 쿼리 언어(SQL)는 관계형 데이터베이스를 관리하고 질의하는 강력한 도구입니다. 초보자든 경험 많은 데이터 전문가든 상관없이, 여러분은 자주 사용하게 될 특정 SQL 문을 발견할 것입니다. 본 문서에서는 데이터베이스 작업의 90%를 처리할 수 있는 10가지 필수 SQL 문을 다룹니다. 코드 예시를 함께 제공할 것입니다.\n\n![이미지](/assets/img/2024-05-27-10SQLStatementsThatCanHandle90ofTasks_0.png)\n\n# 1. 소개\n\n# SQL의 중요성\n\n<div class=\"content-ad\"></div>\n\nSQL은 관계형 데이터베이스와 상호 작용하기 위한 표준 언어입니다. 데이터를 검색하거나 데이터베이스 구조를 수정하는 등 다양한 작업을 수행할 수 있습니다. SQL을 이해하는 것은 데이터 작업을 하는 사람에게 필수적이며, 데이터 분석, 보고 및 애플리케이션 개발을 위한 기초를 제공합니다.\n\n# 2. SELECT 문\n\n# 데이터 검색\n\nSELECT 문은 하나 이상의 테이블에서 데이터를 검색하는 데 사용됩니다. 검색할 열을 지정하고 결과를 필터링할 조건을 추가할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 테이블에서 모든 열을 검색합니다\nSELECT * FROM employees;\n\n-- 특정 열을 검색합니다\nSELECT first_name, last_name FROM employees;\n\n-- 결과를 필터링하기 위해 조건을 추가합니다\nSELECT product_name, price FROM products WHERE price > 50;\n```\n\n# 3. INSERT INTO 문\n\n# 새 데이터 추가\n\nINSERT INTO 문을 사용하면 테이블에 새로운 데이터 행을 추가할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 단일 행 삽입\nINSERT INTO customers (first_name, last_name, email) VALUES ('John', 'Doe', 'john@example.com');\n\n-- 여러 행 삽입\nINSERT INTO orders (order_date, total_amount) VALUES\n    ('2023-01-15', 150.00),\n    ('2023-01-16', 220.50),\n    ('2023-01-17', 75.25);\n```\n\n# 4. UPDATE 문\n\n# 기존 데이터 수정하기\n\nUPDATE 문은 테이블의 기존 데이터를 수정하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 하나의 행 업데이트하기\nUPDATE products SET price = 25.99 WHERE product_id = 101;\n\n-- 여러 행 업데이트하기\nUPDATE employees SET manager_id = 105 WHERE department = 'Sales';\n```\n\n# 5. DELETE 문\n\n# 데이터 삭제\n\nDELETE 문은 테이블에서 행을 제거하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 한 행 삭제\nDELETE FROM customers WHERE customer_id = 201;\n\n-- 조건을 충족하는 모든 행 삭제\nDELETE FROM orders WHERE order_date < '2023-01-15';\n```\n\n## 6. CREATE TABLE 문\n\n### 새 테이블 생성\n\nCREATE TABLE 문은 지정된 열과 데이터 유형을 가진 새 테이블을 생성하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n\nCREATE TABLE products (\n    product_id INT PRIMARY KEY,\n    product_name VARCHAR(255),\n    price DECIMAL(10, 2)\n);\n\n\n## 7. ALTER TABLE 문\n\n## 테이블 수정\n\nALTER TABLE 문을 사용하면 기존 테이블을 추가, 수정 또는 삭제하여 테이블을 수정할 수 있습니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 새 열 추가\nALTER TABLE employees ADD COLUMN hire_date DATE;\n\n-- 열 데이터 유형 수정\nALTER TABLE customers ALTER COLUMN phone_number VARCHAR(15);\n```\n\n# 8. DROP TABLE 문\n\n# 테이블 삭제하기\n\nDROP TABLE 문은 기존 테이블과 해당 데이터를 모두 삭제하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 테이블 삭제\nDROP TABLE products;\n```\n\n## 9. WHERE 절\n\n## 데이터 필터링\n\nWHERE 절은 지정된 조건에 따라 행을 필터링하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 가격이 50보다 큰 제품 조회\nSELECT product_name, price FROM products WHERE price > 50;\n\n-- 영업 부서의 직원 조회\nSELECT first_name, last_name FROM employees WHERE department = 'Sales';\n```\n\n# 10. JOIN 절\n\n# 여러 테이블에서 데이터 결합\n\nJOIN 절을 사용하여 서로 관련된 열을 기반으로 두 개 이상의 테이블에서 행을 결합합니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 고객 이름과 주문 조회하기\nSELECT c.first_name, c.last_name, o.order_date\nFROM customers AS c\nJOIN orders AS o ON c.customer_id = o.customer_id;\n```\n\n# 11. GROUP BY 절\n\n# 데이터 집계\n\nGROUP BY 절은 특정 열의 값을 가진 행을 그룹화하는 데 사용되며, 종종 SUM 및 COUNT와 같은 집계 함수와 함께 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 각 제품별 총 매출 계산\nSELECT product_id, SUM(quantity * price) AS total_sales\nFROM order_details\nGROUP BY product_id;\n```\n\n# 12. 결론\n\n# 기초 마스터\n\n이 10가지 SQL 문은 관계형 데이터베이스 작업 시 대부분의 작업을 다룹니다. 이 문장들을 이해하고 숙달함으로써 데이터베이스 관리 및 데이터 조작에 대한 견고한 기초를 갖게 될 것입니다. SQL은 다양한 기능을 제공하는 언어이며, 데이터 작업에 더욱 강력한 방법을 발견하면서 더욱 쉽게 작업할 수 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# SQL 기초 지식\n\n시간을 내어 주셔서 감사합니다! 🚀\nSQL 기초 지식에서 더 많은 콘텐츠를 찾아보실 수 있어요! 💫\n","ogImage":{"url":"/assets/img/2024-05-27-10SQLStatementsThatCanHandle90ofTasks_0.png"},"coverImage":"/assets/img/2024-05-27-10SQLStatementsThatCanHandle90ofTasks_0.png","tag":["Tech"],"readingTime":4},{"title":"사용 기반 API 요금 청구 및 미터링을 위한 실시간 분석 솔루션","description":"","date":"2024-05-27 12:58","slug":"2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering","content":"\n\n![Real-Time Analytics Solution](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_0.png)\n\nDisclaimer: The author of this article is a Developer Advocate at Redpanda, which is a critical component of the solution discussed. The author also brings prior expertise in API Management and Apache Pinot to the table. Hence, the proposed solution is a combination of these technologies aimed at solving a prevalent problem.\n\nAn API business refers to a company that packages its services or functionalities as a set of API (Application Programming Interface) products. These APIs can be sold to new and existing customers, who can then integrate these functionalities into their own applications. The company can generate revenue by charging these customers based on their usage of the APIs.\n\nA company operating an API business needs a data infrastructure component to track API call volume and bill consumers accordingly.\n\n\n<div class=\"content-ad\"></div>\n\n이 게시물에서는 Apache APISIX, Redpanda 및 Apache Pinot를 사용하여 실시간 API 사용 추적 솔루션을 구축하기 위한 참조 아키텍처를 제시합니다. 이 게시물은 \"어떻게\"보다는 \"왜\"에 중점을 두었습니다. 이를 솔루션 설계 연습으로 간주하고 심층 튜토리얼이 아니라는 것을 고려해 주세요. 저는 솔루션 패턴을 청사진으로 추출하여 향후 프로젝트에서 재사용할 수 있도록 돕고자 합니다.\n\n다른 말 없이 시작해 봅시다.\n\n# APIs 및 API 관리\n\nAPI 및 API 관리 개념에 대해 처음이라면, 먼저 간단히 소개해 드리겠습니다.\n\n<div class=\"content-ad\"></div>\n\n디지털 비즈니스에서 API는 비즈니스 작업에 프로그래밍 방식으로 액세스할 수 있도록 해줘서 인간을 제외할 수 있어요. 이러한 비즈니스 작업에는 주문 생성, 자금 이체, CRM에서 고객 주소 업데이트 등이 포함될 수 있어요.\n\n비즈니스에서 전형적인 API 환경에는 세 가지 당사자가 관련돼요:\n\n- 백엔드 시스템 — 비즈니스 작업을 실행하는 시스템이에요.\n- 소비자 — 비즈니스 작업을 사용하려는 1차 및 3차 애플리케이션이에요.\n- API 프록시 — 프록시로서 작용하며 중간자 역할을 하는 요소에요.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_1.png)\n\n<div class=\"content-ad\"></div>\n\nAPI의 역할은 내부 비즈니스 시스템을 소비자로부터 분리하여, 소비자가 백엔드 시스템의 복잡성을 처리할 필요 없이 이를 제공하는 것입니다. 이러한 방식으로, API는 추상화 계층 역할을 합니다. API는 HTTP를 포함한 다양한 통신 프로토콜을 통해 작동하며, RESTful 및 GraphQL API 스타일을 볼 수 있습니다.\n\n운영 중에는 조직이 API 수명주기의 다양한 단계를 관리하기 위해 전체 수명주기 API 관리 플랫폼을 사용합니다. API 프록시 디자인, 배포, 런타임 정책 참여 및 모니터링과 같은 API 수명주기의 각 단계에 대한 전용 구성 요소를 번들로 제공하는 API 관리 플랫폼이 있습니다. 이 문서의 문맥에서 Apache APISIX를 사용할 것이며, 이는 Apache 라이선스 하에 배포되는 오픈 소스 API 관리 플랫폼입니다.\n\n그렇다고 해서 여기서 구축하는 솔루션이 APISIX와 무조건적으로 결합된 것은 아닙니다. 시장에서 구할 수 있는 대부분의 전체 수명주기 API 관리 제품과 통합할 수 있습니다. 단, 적합한 통합 인터페이스를 제공해야 합니다.\n\n![Real-Time Analytics Solution for Usage-Based API Billing and Metering](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_2.png)\n\n번역 시 일부 용어는 컨텍스트에 맞게 번역되었습니다.\n\n<div class=\"content-ad\"></div>\n\n# API를 활용하여 수익을 창출하는 방법\n\n이제, API를 활용하여 수익을 창출하는 방법에 대해 살펴봅시다. 즉, 사용량에 따라 소비자에게 요금을 부과하는 수익 모델을 찾아보는 것입니다.\n\n더 나은 설명을 위해 현실적인 예시를 들어보겠습니다.\n\n부동산 감정 회사를 고려해보세요. 이 회사는 주택 구매자와 판매자에게 즉각적인 부동산 평가를 제공합니다. 이 평가는 우편번호, 부동산 유형, 지역과 같은 간단한 요소를 기반으로 합니다. 현재, 이 회사는 웹 기반 사용자 인터페이스만 제공하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_3.png)\n\n비지니스 운영을 확장하고 더 많은 고객을 유치하며 새로운 시장 세그먼트에 진입하기 위해 이 회사는 API 비지니스로 진출하기로 결정했습니다. 이 말은 평가 엔진을 API 제품 세트로 패키징하여 새로운 및 기존 소비자에게 판매하고 그들의 API 호출 사용량에 따라 청구할 것을 의미합니다.\n\n이를 위해 먼저 평가 엔진을 분리하고 API 관리 플랫폼 뒤에 배치하여 달성합니다. 이를 통해 소비자들이 일련의 API를 통해 기능에 액세스할 수 있게 됩니다.\n\n![image](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_4.png)\n\n\n<div class=\"content-ad\"></div>\n\n평가 API는 부동산 회사, 은행, 보험사, 정부 등 다양한 분야의 잠재 고객들을 유치할 것입니다:\n\n- 부동산 회사 — 주택 구매자를 위한 정확한 평가값 제공.\n- 은행 — 모기지 승인 전 주택 평가.\n- 보험 제공업자 — 주택 및 내용 보험에 대한 더 정확한 견적 제공.\n- 정부 — 부동산 세금 쉽게 계산.\n\n## API 수익화 모델\n\n이 회사는 어떻게 수익을 창출할까요? 평가 API를 API 제품 세트로 포장하여 구독 계층과 함께 판매하세요!\n\n<div class=\"content-ad\"></div>\n\n가입 등급은 소비자가 매달 고정된 API 호출 횟수를 사용할 수 있는 할당량입니다. 그 할당량을 초과하면 사용자는 제한을 받거나 초과 사용량에 대해 요금을 지불해야 합니다.\n\n예를 들어, 가치평가 API는 다음과 같이 세 가지 가입 등급으로 제공될 수 있습니다.\n\n- 브론즈: 매달 10,000건의 요청\n- 골드: 매달 100,000건의 요청\n- 플래티넘: 매달 무제한의 요청\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_5.png)\n\n<div class=\"content-ad\"></div>\n\n고객은 예상 사용량에 따라 다양한 티어 중에서 선택하여 API에 가입할 수 있습니다. 한 달의 끝에 회사는 실제 사용량을 기반으로 고객에게 청구할 것입니다.\n\n우리의 목표는 각 고객의 API 사용량을 효율적이고 신뢰할 수 있는 방법으로 측정하는 것입니다.\n\n# 솔루션 계획\n\n이제 우리가 해결하려는 문제를 이해했으니, 구현에 들어가기 전에 몇 가지 설계 결정사항을 설명해 드리겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## KPI 지표\n\n첫 번째 단계는 솔루션으로부터 기대하는 KPI 또는 지표를 식별하는 것입니다. 특히 다음 다섯 가지에 관심이 있습니다.\n\n- API 사용량 — 시간당 소비자 당 API 호출 횟수\n- API 지연 — 느린, 느린 API를 식별하기 위한 종단 간 지연 시간\n- 고유 사용자 — API 당 고유 호출 수는?\n- 지리적 사용량 분포 — 주로 API 사용자가 어디에서 왔는가?\n- 오류 수 — 호출 오류가 많으면 백엔드에 문제가 있다는 것을 의미합니다.\n\n이상적으로 이런 것들이 모두 이렇게 시각화된 대시보드에서 보고 싶습니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 마크다운 형식으로 표시 변환한 코드입니다.\n\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_6.png)\n\n## 이해 관계자\n\n두 번째 디자인 결정은 솔루션 이해 관계자 - 이러한 지표를 전달해야 하는 대상. 주로 세 가지 당사자가 있습니다.\n\n고객 및 협력사 - 소비자는 실시간 대시 보드에서 할당량 사용량과 청구 추정을 확인하는 것을 좋아합니다.\n\n\n<div class=\"content-ad\"></div>\n\nAPI 운영 팀 - 이 팀은 API 관리 인프라를 관리합니다. API의 건강 정보에 특히 관심이 있으며, 지연시간, 처리량, 오류 등을 주로 다룹니다.\n\nAPI 제품 팀 - 이 팀은 API 제품을 소유하고 있습니다. 그들은 소비자의 인구 통계, API 중에서 더 인기 있는 것이 무엇인지 등을 확인하기 위해 즉시 실험을 실행하고 싶어 합니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_7.png)\n\n## 일괄 처리 또는 실시간 처리?\n\n<div class=\"content-ad\"></div>\n\n최종 디자인 결정으로, 실시간 및 일괄 메트릭 사이에 80:20의 분할을 설정하겠습니다.\n\n데이터는 시간이 경과함에 따라 가치를 잃습니다. 데이터를 빨리 처리할수록 적절한 조치를 취할 수 있습니다. 그래서 우리는 API 트래픽 및 헬스 메트릭을 실시간으로 처리하겠습니다.\n\n소비자 API 키가 유출된 상황을 생각해보세요. 악의적인 API 클라이언트가 훔친 키나 인증 토큰을 사용하여 소비자를 대신해 API를 호출할 수 있습니다. 시스템은 이 API 키에서의 급격한 트래픽 증가를 감지하여 비정상으로 식별하고 키를 차단하면서 소비자에게 경보를 보낼 수 있습니다. 경보를 받은 소비자는 즉시 API 키를 재발급하여 비용을 최소화할 수 있습니다.\n\n그러나 모든 사용 사례가 실시간 처리를 필요로 하는 것은 아닙니다. 어떤 사용 사례는 자연스럽게 일괄 처리에 적합합니다.\n\n<div class=\"content-ad\"></div>\n\n- 고객을 위한 월별 사용량에 기반한 청구 보고서.\n- 업무팀을 위한 주간 API 건강 보고서.\n- 제품팀을 위한 매일 API 트래픽 보고서.\n\n# 구현\n\n지금은 이 기사의 중간 지점에 도달했으며 지금까지의 토론을 바탕으로 다음 솔루션 아키텍처를 제시합니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_8.png)\n\n<div class=\"content-ad\"></div>\n\n다이어그램이 복잡하고 많은 알 수없는 기술이 있어서 알겠습니다. 그래서, 세 개의 레이어로 나누어서 각각에 대해 자세히 설명하겠습니다.\n\n## 데이터 수집\n\n이전에 언급했듯이, API 관리 시스템은 디자인 및 런타임 측면에서 트래픽 모양 만들기, 인증 및 구독 관리와 같은 API 라이프사이클 관리 작업을 수행하는 여러 이동 부품을 가지고 있습니다. 이에 대한 추가 측면도 있습니다.\n\n![image](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_9.png)\n\n<div class=\"content-ad\"></div>\n\n그러나, 우리가 가장 관심 있는 구성 요소는 API 게이트웨이입니다. 이 곳은 모든 API 트래픽이 백엔드로 흐르는 곳입니다.\n\n우리의 첫 번째 작업은 API 게이트웨이에서 접촉점을 확인하는 것입니다. 이를 통해 왕복하는 API 요청과 응답을 수집할 수 있습니다. 그런 다음 이 정보를 실시간으로 분석용 데이터 저장소로 이동시키는 데이터 파이프라인을 구축할 것입니다. 이를 통해 향후 질의를 용이하게 할 것입니다.\n\n그러나, 이 쓰기 경로를 구현할 때 직접 데이터를 기본 데이터 저장소에 쓰는 것은 여러 문제점을 야기할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nAPIM 시스템과 분석 인프라 사이의 강한 결합 - 나중에 새 APIM 공급업체로 전환할 때 분석 인프라의 상당 부분을 다시 작성해야 할 수도 있습니다.\n\n동기 쓰기 - 운영 중에 두 시스템 모두 사용 가능해야 하므로 분석 시스템을 유지보수 목적으로 중지하기 어려울 수 있습니다.\n\n확장 가능한 데이터 수집 - API 게이트웨이의 돌발적인 트래픽 급증으로 인해 분석 시스템이 과부하되어 두 시스템 모두 협조하여 확장해야 할 수 있습니다.\n\n이로 인해 APIM과 분석 인프라를 분리하는 방법을 모색할 필요가 있습니다. Apache Kafka와 같은 스트리밍 데이터 플랫폼은 API 게이트웨이에서 높은 처리량 데이터 스트림을 낮은 지연 시간으로 수신할 수 있으므로 여기에 적합할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n해당 솔루션에서는 성능과 간편함 측면에서 Kafka보다 우월한 Redpanda, Kafka API 호환 스트리밍 데이터 플랫폼을 사용할 것입니다. 하지만 만약 Kafka만 사용하길 원한다면 괜찮습니다. 해당 솔루션은 두 기술에 모두 매끄럽게 작동합니다.\n\nRedpanda를 중심으로 한 데이터 파이프라인은 다음과 같이 구성됩니다:\n\n![Redpanda Data Pipeline](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_11.png)\n\nRedpanda의 추가로 두 시스템이 분리되었고 쓰기 경로가 비동기로 동작합니다. 이는 분석 시스템이 유지 보수를 위해 오프라인 상태로 들어갈 수 있고, 중단된 지점부터 다시 재개할 수 있도록 합니다. 게다가 Redpanda는 갑작스러운 트래픽 급증을 흡수하여 분석 시스템이 과부하를 받거나 API 게이트웨이에 맞춰 스케일링할 필요가 없도록 해줍니다.\n\n<div class=\"content-ad\"></div>\n\n이제 APISIX와 Redpanda 사이의 연결을 어떻게 만들어야 할지 궁금할 것입니다. 다행히도, APISIX는 Kafka를 위한 내장 데이터 싱크를 제공합니다. 게이트웨이로 API 요청이 발생하고 응답이 반환될 때, 이 싱크는 실시간으로 Kafka 토픽에 레코드를 발행합니다. 우리는 Redpanda와 함께 이 싱크를 사용할 수 있습니다. 왜냐하면 Redpanda가 Kafka API와 호환되기 때문입니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_12.png)\n\nAPISIX는 개별 API 호출을 JSON 이벤트로 형식화하고 지연 시간, HTTP 상태 및 타임스탬프와 같은 중요한 메트릭을 포함시킵니다. 이러한 정보들은 분석 데이터 저장소에서 관련 있는 차원으로 매핑될 것입니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_13.png)\n\n<div class=\"content-ad\"></div>\n\nAPI 관리 플랫폼에 Kafka 싱크가 없는 경우 어떻게 할까요? 그럼 대안으로 API 게이트웨이의 HTTP 액세스 로그를 Kafka로 스트림 처리할 수도 있습니다. 이를 위해 Filebeat나 유사한 도구를 사용할 수 있습니다.\n\n## 분석 데이터베이스\n\n이제 Redpanda에 API 호출 이벤트가 랜딩되고 있으니, 다음 단계는 적합한 분석 데이터베이스 기술을 식별하는 것입니다.\n\nOLTP 데이터베이스, 키-값 저장소 또는 데이터 웨어하우스가 될 수 있을까요? 다음 기대 기준에 따라 각각을 평가해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n- 스트리밍 데이터 수집 - 실시간 데이터 원본인 Kafka에서 가져와야 합니다. 여기서는 배치 데이터 로딩이 없어야 합니다. 스트리밍 수집은 더 높은 데이터 신선도를 보장합니다.\n- 낮은 지연 쿼리 - 쿼리 지연 시간은 하위 초 범위 내여야 하며 사용자를 위한 분석 대시보드를 만족시켜야 합니다.\n- 높은 쿼리 처리량 - 사용자를 대상으로 하는 분석 대시보드에서 동시에 발생하는 쿼리를 처리할 수 있어야 하며 지연 시간을 훼손하지 않아야 합니다.\n\n위의 모든 조건을 충족하는 분석 데이터베이스로 Apache Pinot을 선택했습니다.\n\nApache Pinot은 실시간 분산 OLAP 데이터베이스로, 스트리밍 데이터에서 OLAP 워크로드를 처리하도록 설계되어 극히 낮은 지연 시간과 높은 동시성을 제공합니다. Pinot은 Kafka와 원활하게 통합되어 Kafka 주제에서 데이터가 생성될 때마다 실시간 수집을 허용합니다. 수집된 데이터는 색인이 작성되고 열 형식으로 저장되어 효율적인 쿼리 실행을 가능케 합니다.\n\n아키텍처에서 Pinot을 사용하면 엔드 투 엔드 데이터 파이프라인은 다음과 같이 보입니다. Pinot은 Redpanda와 API 호환성으로 원활하게 통합됩니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_14.png\" />\n\n파이프라인에 스트림 프로세서가 필요한가요? 정말 필요한지는 사용 사례에 따라 다릅니다.\n\n스트림 프로세서 대신 Redpanda의 Wasm 기반 인브로커 데이터 변환을 사용하여 API 이벤트 페이로드에서 민감한 필드를 제거할 수 있습니다. 그러나 아파치 Flink와 같은 상태 보유형 스트림 프로세서는 다음과 같을 때 파이프라인에 더 많은 가치를 더할 수 있습니다:\n\n- 실시간 결합 및 보강이 필요할 때 — 핀오토로 전달할 추가 차원이 필요하며, 이는 여러 스트림을 결합하여 파생할 수 있습니다. 예: IP 지오코딩.\n- 알림 — 사용량의 이상 현상을 기반으로 알림을 트리거하고 하류 이벤트 주도형 워크플로를 실행합니다.\n\n<div class=\"content-ad\"></div>\n\n## Serving layer\n\n우리의 분석 데이터 파이프라인이 이제 완성되었습니다. 모든 파이프라인 구성 요소는 데이터 인프라 구조층에 있습니다. 필요하다면 Pinot 쿼리 콘솔에 액호크 SQL 쿼리를 실행하여 메트릭을 생성할 수 있습니다.\n\n그러나 솔루션의 모든 이해 관계자/사용자가 그렇게 하길 원하는 것은 아닙니다. 우리는 각 사용자 그룹에게 메트릭을 직관적이고 편안하게 찾을 수 있는 방식으로 제시해야 합니다. 이것이 우리가 분석의 마지막 단계인 서빙 레이어를 구현하는 곳입니다.\n\n<img src=\"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_15.png\" />\n\n<div class=\"content-ad\"></div>\n\n우리의 우선순위는 소비자들입니다. 그들은 사용량과 청구 예상을 시각화하는 실시간 대시보드가 필요합니다. 이를 위해 Streamlit과 같은 프레임워크를 활용하여 Python 기반 데이터 어플리케이션을 개발할 수 있습니다. Pinot Python 드라이버 pinotdb를 사용하면 애플리케이션과 Pinot 쿼리 환경을 연결할 수 있습니다.\n\nBI 및 즉석 탐색이 필요한 사용자 그룹, 특히 API 제품 소유자는 Tableau와 Apache Superset과 같은 선호하는 BI 도구를 연동할 수 있는 Pinot의 ODBC 인터페이스를 사용할 수 있습니다.\n\n일괄 작업을 위해 Pinot는 Presto나 Trino와 같은 쿼리 연합 엔진에 Pinot 커넥터를 통해 연동할 수 있습니다.\n\n# 요약\n\n<div class=\"content-ad\"></div>\n\n다음은 파이프라인 구현 단계 순서를 나열하여 글을 마무리해 봅시다.\n\n1. Redpanda 클러스터를 프로비저닝하고 토픽을 생성하고 ACL을 설정합니다.\n2. APISIX에서 Kafka 싱크를 구성합니다.\n3. Pinot 스키마와 테이블을 생성합니다.\n4. 필요에 따라 데이터를 가공합니다.\n5. 대시보드를 생성하거나 연결합니다.\n\n이 솔루션은 비즈니스 자체에서 호스팅하고 관리하는 자체 호스팅 배포 모델을 전제로 합니다. 그러나 동일한 설계 원칙이 이 도구들의 호스팅 버전을 선택해도 적용될 수 있다는 점을 알아두는 것이 중요합니다. 아키텍처의 각 구성 요소는 호스팅 서비스로 대체될 수 있으며, 이를 통해 다양한 배포 전략에 대응할 수 있는 유연한 해결책이 될 수 있습니다.\n\n이전에 언급했듯이, 이 글은 \"어떻게\"보다는 \"왜\"에 대해 주로 다루고 있습니다. 목표는 정확한 실행 방법보다는 근본적인 해결책 패턴을 이해하는 것입니다. 이 글을 다음 실시간 분석 프로젝트의 청사진으로 삼아 보세요. 필요에 따라 다른 기술을 통합하여 조정할 수 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_0.png"},"coverImage":"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_0.png","tag":["Tech"],"readingTime":11},{"title":"데이터 폭풍 속을 해마하는 여정 정교한 레이크하우스 플랫폼 구축하기","description":"","date":"2024-05-27 12:55","slug":"2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform","content":"\n![이미지](/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_0.png)\n\n목요일 오후 늦은 시간, 우리는 깨달음을 얻었습니다. 어두운 사무실에서 깜박이는 화면들을 둘러싸고, 두 명의 예리한 데이터 분석가 동료와 데이터 과학자로 구성된 헌신적인 팀, 그리고 나, 데이터 엔지니어는 PostgreSQL과 MySQL 데이터베이스에서 데이터를 조인하려고 깊이 파고들고 있었습니다. 이 작업은 간단할텐데 어쩌면 서로 다른 부서에서 오는 불일치된 데이터 구조와 충돌하는 스키마로 애를 쓰고 있었습니다. 이러한 이질적인 데이터 세트를 수동으로 맞추려고 할수록 복잡성이 압도되는 느낌이었습니다. 혼돈스러운 정보 동기화 시도마다 실패할 때마다 공기는 분노로 더 두꺼워졌습니다. \"이렇게 일할 수는 없어,\" 살짝 중얼거렸던 저의 목소리엔 스트레스가 느껴졌습니다. 우리의 현재 시스템이 현실에 부합하지 않다는 것은 명백했습니다—우리는 흩어진 데이터 정복 뿐만 아니라 이 넓은 데이터 정글을 이해할 수 있는 통합 플랫폼이 필요했던 것입니다. 이 순간이 우리에게 중대한 변화가 필수적이라는 것을 알게 된 시점이었고, 큰 변화가 곧 찾아올 것임을 알게 된 시점이었습니다.\n\n이 여정을 시작하면서, 저희는 우리 회사의 의사 결정 프로세스의 기반이 되는 견고한 데이터 플랫폼 아키텍처를 만들었습니다. 이 블로그 글에서는 다양한 데이터를 단일하고 강력한 분석 엔진으로 통합하는 것뿐만 아니라 지속적으로 발전하고 시간이 지남에 따라 더 많은 데이터 소스를 통합하는 유연하고 확장 가능한 데이터 인프라를 구축하는 과정에서 우리가 직면한 인사이트와 도전에 대해 탐구할 것입니다.\n\n대용량의 원시 데이터를 원래 형식 그대로 저장할 수 있는 유연성으로 기업들에게 빠르게 적응하고 효율적으로 확장할 수 있는 도구로써 데이터 레이크가 부각되었습니다. 그러나 계속 진행함에 따라 데이터 관리에 더 구조적인 접근이 필요하다는 것을 깨달았고, 그로 인해 레이크하우스 구조를 채택하게 되었습니다. 이 하이브리드 모델은 데이터 레이크의 확장성과 유연성을 데이터 웨어하우스의 관리 기능과 결합하여 데이터 전략을 향상시킵니다. 이 이야기는 이러한 기술을 활용하기 위해 우리가 취한 실질적인 단계를 살펴보며, 데이터 레이크를 레이크하우스 프레임워크로 통합함으로써 데이터 주도 기업에게 혁신적인 자산이 될 수 있는 방법을 밝힐 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 레이크하우스 이전의 데이터 전략\n\n레이크하우스를 개발하기 전에, 데이터 관리는 간단했지만 우리의 요구사항이 증가함에 따라 비효율적으로 변했습니다. 처음에는 BI용 Apache Superset을 사용하여 3개의 주요 데이터 소스를 관리했는데, 처음에는 최소한의 복잡성으로 우리의 요구사항을 충족시켰습니다.\n\n그러나 우리의 데이터 요구사항이 증가함에 따라 시스템의 한계가 나타나기 시작했습니다. 두 가지 다른 소스에서 데이터를 조인해야 할 필요가 발생했을 때 중대한 도전이 발생했습니다. 당시 우리의 솔루션은 매우 효율적이지 못했습니다: 필요한 데이터를 한 소스에서 다른 소스로 수동으로 복제했습니다. 이 프로세스는 시간이 많이 걸릴 뿐만 아니라 데이터를 동기화하기 위해 빈번한 업데이트가 필요했기 때문에 오류를 발생시킬 가능성도 있었습니다.\n\n또한 다양한 팀과 프로젝트가 발전함에 따라 Superset 내에서 여러 데이터셋이 생성되었는데, 각각이 특정한 분석 요구에 맞게 조정되었습니다. 불행히도, 이로 인해 여러 데이터셋에 중복 변환 요소가 코딩되어 복잡성이 증가했을 뿐만 아니라 이러한 변환을 유지하고 업데이트하는 것이 점점 더 부담스러워졌습니다.\n\n<div class=\"content-ad\"></div>\n\n## 데이터 아키텍처 결정: Lake, Warehouse 또는 Lakehouse?\n\n저희 데이터 인프라에 적합한 아키텍처를 선택하는 것은 중요한 결정이었습니다. 세 가지 주요 옵션 중에서 선택을 고민했습니다: 데이터 레이크, 데이터 웨어하우스 및 레이크하우스. 각각에 대한 간단한 개요를 살펴보겠습니다:\n\n• **데이터 레이크**: 데이터 레이크는 원시 형식으로 방대한 양의 데이터를 저장합니다. 다양한 소스에서 대량의 다양한 데이터를 처리하는 데 이상적이며 높은 유연성과 확장성을 제공합니다. 그러나 구조화된 데이터 환경의 처리 효율성 일부가 부족합니다.\n\n• **데이터 웨어하우스**: 이것은 질의 및 분석에 최적화된 구조화된 형식으로 데이터를 저장하는 시스템입니다. 데이터 웨어하우스는 구조화된 데이터에 대한 빠른 쿼리 성능에 뛰어나지만 변경사항 및 새로운 데이터 유형의 수용에 있어서 덜 유연할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n• Lakehouse: 데이터 레이크와 데이터 웨어하우스의 장점을 결합한 하이브리드 모델입니다. 데이터 레이크의 넓은 저장 공간과 유연성을 제공하면서 데이터 웨어하우스의 효율적인 쿼리 기능을 갖추고 있습니다.\n\n신중한 고려 끝에 저희는 여러 가지 이유로 레이크하우스 아키텍처를 도입하기로 결정했습니다:\n\n1. 유연성: 레이크하우스 아키텍처는 필요한 적응성을 제공했습니다. 기존의 데이터 웨어하우스는 새로운 데이터 소스나 유형을 빠르게 통합하는 것이 어려워 변경에 제한이 있고 느립니다.\n\n2. 단순화된 아키텍처: 처음에는 기존의 ETL 프로세스를 데이터 웨어하우스와 데이터 레이크에 별도로 구축하는 것을 고려했지만, 두 개의 별도 시스템을 유지할 명확한 이유를 찾지 못했습니다. 레이크하우스 모델은 강력한 쿼리 및 저장 기능을 하나의 보다 관리하기 쉬운 시스템으로 통합한 간소화된 접근 방식을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n다음은 주요 구성 요소에 대한 개요입니다:\n\nOur Data Lakehouse stack\n\n우리의 레이크하우스 아키텍처는 AWS 기술과 오픈 소스 솔루션의 최선을 활용하여 데이터를 효율적으로 관리하고 분석하는 것을 목표로 합니다.\n\n이미지를 Markdown 형식으로 변경했습니다.\n\n\n![Lakehouse Overview](/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_1.png)\n\n\n<div class=\"content-ad\"></div>\n\n**데이터 저장:**\n\n우리는 데이터 저장을 위해 AWS S3를 활용하며, 환경을 개발 및 프로덕션용 2개의 전용 버킷으로 구성합니다. 메달리온 아키텍처를 채택하여 각 버킷 내에 bronze, silver 및 gold 세 가지 독립적인 레이어(폴더)를 설정했습니다. 각 레이어는 데이터 관리 수명주기에서 특정 목적을 제공합니다. 메달리온 아키텍처는 데이터를 세 개의 레이어로 분류하는 레이크하우스 시스템에 사용되는 계층화된 데이터 처리 모델입니다:\n\n- Bronze Layer (Raw Layer): 이 기본 레벨에서는 다양한 소스로부터 도착한 대로 데이터를 정확히 저장하여 JSON, CSV 등의 원래 형식으로 보존합니다. 이 레이어는 주로 데이터 엔지니어링 팀이 디버깅 및 데이터 무결성 확인을 위해 액세스하는 데 중요하며 데이터 과학자가 초기 인사이트를 얻고 데이터 품질을 측정하기 위해 탐색 분석을 시작하는 중요한 역할을 합니다. 초기 인사이트는 더 나은 데이터 처리 전략을 안내하는 데 중요합니다.\n- Silver Layer (Cleansed Layer): 데이터가 실버 레이어로 이동하면 필요한 클렌징 및 변환 프로세스를 수행합니다. 여기서 우리는 불일치를 수정하고 데이터를 풍부하게하여 구체적인 비즈니스 규칙을 적용하여 구조화되고 유용하게 만듭니다. 우리의 분석 엔지니어는 이 클렌징 된 데이터와 작업하여 복잡한 변환을 실행하고 내부 분석을 이끄는 자세한 보고서를 생성합니다. 더 나아가 이 레이어는 우리의 데이터 과학자가 정교한 모델을 구축하는 데 의존하는 정돈된 데이터 환경을 제공합니다.\n- Gold Layer (Aggregated Layer): 이 곳에서 데이터는 가장 높은 가치를 얻으며, 비즈니스 수준의 집계 및 핵심 성능 지표로 변환됩니다. 신속한 검색 및 고속 분석을 위해 최적화된 골드 레이어는 주로 의사 결정자를 위해 액세스됩니다. 이들은 회사 전반의 전략 및 운영에 영향을 미치는 실행 가능한 인사이트를 위해 정제된 데이터에 의존합니다. 더불어 이 레이어는 기업 수준의 보고서 및 대시보드의 기반 역할을 합니다.\n\n실버 및 골드 레이어에서는 쿼리 성능을 최적화하기 위해 데이터 파일을 Parquet 파일로 저장합니다. Parquet의 효율적인 열 지향 저장 형식 덕분에 쿼리 성능이 최적화됩니다. 또한, 이러한 Parquet 파일 위에서 Apache Iceberg를 활용하여 레이크하우스 아키텍처에 여러 가지 중요한 기능을 제공합니다. Apache Iceberg를 사용하면 데이터 레이크를 전통적인 데이터베이스처럼 다루되 더 큰 유연성과 확장성을 갖습니다. 스냅샷, 트랜잭션, 업서트 및 삭제와 같은 복잡한 작업을 지원함으로써 데이터 레이크를 더 동적이고 다재다능한 시스템으로 변환할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 카탈로그:\n\n저희의 데이터 카탈로그 관리에는 비용 효율성, 다른 AWS 서비스와의 깊은 통합, 그리고 직관적인 메타데이터 관리 기능으로 인해 AWS Glue Catalog를 선택했습니다. AWS Glue Catalog는 중앙 메타데이터 저장소로 기능하며, 이를 통해 각종 AWS 서비스 간의 데이터 자산을 보다 쉽게 관리하고 접근할 수 있습니다. AWS Glue Crawler를 활용하여 S3에 저장된 데이터를 자동으로 발견하고 분류하여 데이터 카탈로그 테이블을 손쉽게 생성하고 업데이트할 수 있습니다.\n\n하지만 AWS Glue Catalog는 운영 요구에 맞게 데이터 검색을 용이하게 하는 측면에서 제한이 있음을 인지하고 있습니다. 잘 통합되어 비용 효율적이지만 세련된 데이터 카탈로그의 세부 기능 중 일부를 지원하지 않습니다. 특히 대규모 데이터 작업에 필수적인 향상된 검색 및 발견 도구와 같은 기능을 지원하지 않습니다. 이는 모델링을 위해 다양한 데이터세트에 빠르게 액세스해야 하는 데이터 과학자, 비즈니스 결정에 신속한 통찰을 얻어야 하는 데이터 분석가, 그리고 철저한 데이터 탐색에 의존하여 포괄적인 보고서를 작성하는 비즈니스 인텔리전스 전문가를 포함한 여러 팀 멤버에 영향을 줄 수 있습니다. 앞으로는 저희 조직의 중요한 역할들의 요구를 충족시키기 위해 데이터 검색을 지원하는 더 편리하고 포괄적인 데이터 카탈로그 솔루션을 탐구할 계획입니다.\n\n데이터 액세스 및 쿼리 엔진:\n\n<div class=\"content-ad\"></div>\n\nAWS Athena는 주요 쿼리 엔진으로 사용되며 AWS Glue 카탈로그와 원활하게 통합됩니다. 이 간편한 설정을 통해 데이터 레이크를 효과적으로 쿼리할 수 있어 Athena는 우리 데이터 아키텍처의 중요한 구성 요소입니다. Athena를 사용하는 주요 장점 중 하나는 비용 효율성입니다. Athena는 쿼리 중 스캔된 데이터 양에 따라 요금이 부과되기 때문에 현재 우리의 쿼리는 과도한 데이터 양을 스캔하지 않아 비용을 상당히 낮게 유지할 수 있었습니다.\n\n그러나 우리는 데이터 레이크 사용을 확대함에 따라(특히 애플리케이션 내 차트를 통한 직접 데이터 쿼리 통합이 예정된) Athena와 관련된 비용이 증가할 수 있다는 점을 알고 있습니다. 이러한 잠재적 시나리오에 대비하기 위해 Trino로 전환을 고려 중이며, 이는 EKS에서 실행되어 AWS Glue 메타스토어에 연결될 것입니다. Athena와 Trino 사이의 기본적인 유사성으로 인해 이 마이그레이션은 간단할 것으로 예상됩니다.\n\n현재 Athena에서 두 가지 서로 다른 워크그룹을 활용하고 있습니다 - SQL 쿼리를 위한 하나와 Spark(Python) 연산을 위한 다른 하나입니다. 앞으로, 우리는 이러한 설정을 세분화하여 변환, 고객 분석 등과 같은 다양한 비즈니스 요구에 대해 별도의 워크그룹을 생성하여 운영 효율성과 비용 관리를 향상시킬 계획입니다.\n\n데이터 거버넌스:\n\n<div class=\"content-ad\"></div>\n\nAWS Lake Formation은 저희 데이터 거버넌스에 중요한 역할을 합니다. 레이크하우스 아키텍처에서 데이터 보안과 권한 관리를 크게 향상시킵니다. 이는 PHI 및 민감한 데이터를 다루는 데 핵심적인 엄격한 접근 제어를 시행하는 데 도움이 됩니다.\n\n강력한 접근 제어를 위한 LF-Tags 구현: 데이터가 안전하게 액세스되고 엄격한 정책을 준수하는 데 필요한 접근 권한을 정교하게 제어하기 위해, 우리는 데이터베이스 및 테이블 수준에서 권한을 세밀하게 제어하기 위해 LF-Tags를 활용합니다. 우리의 태그 전략은 체계적으로 설계되어 있으며, 데이터베이스는 일반적으로 태그가 지정되며, 더 구체적인 요구 사항에 따라 테이블 수준에서 권한을 관리합니다. 우리가 사용하는 가능한 태그에는 다음과 같은 것들이 있습니다:\n\n- 환경: dev, prod\n- 부서: app, internal, devops, hr, customers, infra, sales, ds\n- PHI: true\n- 데이터 레이크 레이어: gold, silver, bronze\n- 클라이언트 대면: true (데이터를 고객에게 노출할 수 있는지 여부를 나타냄)\n\n각 데이터베이스와 테이블에 여러 태그를 적용하여 세밀한 역할 기반 접근 제어를 가능하게 합니다. 예를 들어, 우리 애플리케이션에서 데이터를 쿼리하는 고객을 위한 역할은 client_facing: true, data_lake_layer: gold, 그리고 environment: prod와 같은 태그 조합을 통해 액세스 권한을 부여받을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n최초에 AWS Lake Formation을 설정하는 것은 간단하지 않았습니다. 이 플랫폼은 강력하지만 직관적이지 않았고, 권한 행동을 우리의 거버넌스 요구에 맞게 조정하는 데 상당한 노력과 시간이 걸렸습니다. 이러한 도전을 극복하기 위해서는 가파른 학습 곡선이 필요했는데, 다양한 구성을 실제로 실험해야 했고, 우리가 필요로 하는 상세한 액세스 제어를 효과적으로 구현하고 관리하는 방법을 이해해야 했습니다.\n\n![이미지](/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_2.png)\n\n데이터 수집:\n\n우리는 단순하게 사용할 수 있고 기관 전체의 데이터를 수집해야 할 필요가 있는 미래의 요구를 예측하여 다양한 커넥터를 지원하는 플랫폼을 찾았습니다. 두 플랫폼인 Airbyte(오픈 소스 솔루션)와 Rivery(SaaS 솔루션)을 비교하는 POC를 진행한 후, 몇 가지 설득력있는 이유로 Airbyte를 선택했습니다.\n\n<div class=\"content-ad\"></div>\n\n먼저, 저희의 결정은 데이터 양에 따라 청구되지 않는 비용 효율적인 솔루션에 더 기울였습니다. 저희는 증가하는 비용을 걱정하지 않고 자유롭게 데이터를 가져오길 선호했습니다. 게다가 Airbyte의 개발 도구는 특히 인상적이었습니다. 플랫폼의 Connector Builder SDK는 Connector Builder UI와 로우코드 커넥터 개발 환경이 모두 포함되어 있어 필요한 간편함과 유연성을 제공했습니다. 이 기능 덕분에 우리는 우리의 특정 요구에 맞게 데이터 커넥터를 쉽게 구축하고 맞춤화할 수 있었습니다.\n\nAirbyte는 대규모이자 활발한 커뮤니티를 자랑하지만 제품에 몇 가지 어려움을 겪었습니다. 처음에는 데이터 수집 속도가 느렸습니다. 특히 PostgreSQL에서 20GB를 2일 이상으로 전송하는 데 시간이 걸렸습니다. 먼저 AWS-data-lake 목적지를 사용해 보았지만 느리고 지속적인 동기화를 지원하지 않았습니다. 이 문제를 해결하기 위해 이 문제를 고치기위한 pull request를 제출했지만 3개월이 걸렸습니다. 더 나은 해결책을 찾기위해 여러 다른 목적지를 실험해 보았습니다. Parquet을 사용할 때 타임스탬프가 struct로 형식화되는 짜증나는 문제가 있는 S3 목적지가 있었습니다. 이 구체적인 문제는 2년째 해결되지 않은 상태로 있는데, 이는 지원 측면에서 중요한 차이점을 보여줍니다. 유망한 Iceberg 목적지는 AWS Glue Catalog를 지원하지 않았습니다. 그래서 AWS-Glue 목적지를 시도했지만 JSON 출력만 지원해서 비효율적이라는 것을 발견했습니다.\n\n최종적으로 이러한 옵션 중 어느 것도 우리의 요구 사항을 완전히 충족시키지 못했기 때문에, 우리는 자체적으로 사용자 정의 AWS-data-lake 목적지를 개발하기로 결정했습니다. 우리는 원래 코드를 복제하고 우리의 요구 사항에 맞게 특별히 맞춤화하여 데이터 수집 프로세스를 크게 향상시킨 맞춤형 솔루션을 만들었습니다.\n\n이러한 어려움에도 불구하고 Airbyte는 효과적으로 우리의 요구 사항을 모두 충족시켰습니다. 오늘날, Airbyte를 사용하여 약 15가지 다른 데이터 소스를 데이터 레이크에 성공적으로 통합했으며, 데이터 수집 능력과 전체 데이터 전략을 크게 향상시켰습니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 처리:\n\n저희의 데이터 처리 워크플로우는 dbt Core에 의해 강력하게 주도되며, ELT (추출, 로드, 변환) 접근 방식을 사용합니다. 모든 데이터가 브론즈층에서 시작되어 점진적으로 실버층과 골드층으로 변환됩니다.\n\n저희는 dbt Athena 어댑터를 사용하고 있으며, SQL 및 Python (PySpark) 모델을 지원합니다. 이 다양성은 더 복잡한 변환을 효과적으로 처리하는 데 중요합니다. dbt-Athena 어댑터는 활기찬 커뮤니티의 혜택을 받고 있으며, 정기적인 업데이트로 계속 발전하고 있습니다. 처음에는 Python Athena 통합을 채택하는 데 약간 주저했었는데, 그 당시의 혁신성과 제한된 추적 레코드 때문이었습니다. 그러나 철저한 테스트와 유효성 검사를 거친 후에는 어떠한 문제도 발생하지 않았고, 안정성과 효율성을 확인하며 우리의 프로덕션 환경에 성공적으로 구현했습니다.\n\ndbt에서 테이블 속성을 구성하는 것은 직관적이고 유연하며, 우리의 데이터 관리 능력을 크게 향상시킵니다. 예를 들어, 증분 테이블을 널리 사용하는데, 이는 새 데이터 또는 변경된 데이터만 효율적으로 가져오는 데 중요합니다. Iceberg 테이블 형식을 활용하여 병합 증분 전략을 채택하면 데이터세트를 중복 처리 없이 원활하게 업데이트할 수 있습니다. 또한, dbt에서 데이터 파티션을 관리하는 것도 간단해졌습니다. 파티션은 테이블 속성 내에서 직접 선언할 수 있습니다. 아래는 우리의 골드층 테이블에 대한 테이블 속성 구성 예시입니다. 우리가 재료화 전략, 파티셔닝 및 데이터 형식을 어떻게 지정하는지 보여주고 있습니다:\n\n<div class=\"content-ad\"></div>\n\n\n{ config(\n    materialized='incremental',\n    incremental_strategy='merge',\n    unique_key=['session_id'],\n    partitioned_by=['day'],\n    table_type='iceberg',\n    format='parquet'\n) }\n\n\n우리는 dbt에 대해 매우 만족하고 있습니다; 이 도구는 우리의 데이터 변환을 어떻게 관리하는지에 혁명을 일으켰습니다. 이 도구는 견고한 버전 관리, 코드의 재사용성 및 데이터 흐름의 명확한 문서화를 제공하여 복잡한 변환 작업을 관리하고 레이크하우스 아키텍처 전반에서 데이터 무결성을 유지하는 것을 크게 간소화합니다.\n\ndbt와의 성공을 토대로, 우리는 현재 데이터 관리 역량을 더욱 강화하기 위해 새로운 도구를 탐색 중입니다. Montara.io가 강력한 기능 세트를 제공하여 워크플로우를 최적화하는 우리의 dbt Git 리포지토리와 직접 통합되었습니다. Montara는 자동 CI/CD, 팀원들이 dbt 전문 지식이 적은 경우에도 모델을 작성하고 테스트할 수 있는 사용자 친화적인 UI를 제공하며 데이터 계보 표시, 데이터 카탈로그 및 관찰 가능성과 같은 가치 있는 도구를 제공합니다.\n\nMontara에 감명을 받았습니다; 이 도구는 우리의 dbt 워크플로우를 크게 간소화시켜 팀 전체에서 데이터 변환을 보다 접근 가능하고 관리하기 쉽게 만듭니다. 이 도구가 비교적 새로운 것이며 여전히 발전 중이므로 가끔씩 일부 문제와 기능의 빈틈을 겪기도 하지만, 우리의 경험은 전반적으로 매우 긍정적입니다. Montara 팀은 우수한 지원을 제공하며 우리와 긴밀히 협력하여 발생하는 어려움을 신속하게 해결하고 계속되는 제품 향상에 우리의 피드백을 통합합니다. 이 협력적인 접근은 문제를 신속하게 해결할 뿐만 아니라 Montara.io가 우리의 데이터 인프라 요구와 완벽하게 일치하도록 발전하도록 보장합니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n분석 및 BI 도구:\n\nApache Superset은 저희가 선택한 분석 및 비즈니스 인텔리전스 도구로, 오픈 소스와 강력한 데이터 시각화 능력으로 유명합니다. 다른 BI 도구와 비교했을 때 유연성과 비용 효율성을 강점으로 삼아 Superset을 선택했습니다. 다양한 사용자 정의 옵션과 사용자 친화적 인터페이스를 통해 우리 팀은 대시보드와 보고서를 자신들의 필요에 맞게 맞춤화할 수 있으며, 특히 Athena를 주 데이터 원본으로 사용하는 저희 독특한 분석 환경에 특히 적합합니다.\n\n데이터 분석가들은 저희 회사의 다양한 부서를 위한 대시보드를 만들기 위해 주로 Superset을 사용합니다. 더불어, Superset의 기능을 활용하여 차트를 애플리케이션에 직접 임베드하여 고객에게 유용한 통찰을 제공합니다.\n\n현재, 일부 차트는 브론즈 계층의 데이터에 직접 접근하여 실시간으로 변환 작업을 수행합니다. 그러나 더 이상 원시 데이터 쿼리의 부하를 줄이기 위해 이 접근 방식을 수정 중이며, 최종적으로 LF-tags를 사용하여 브론즈 계층의 액세스를 제한할 계획입니다.\n\n<div class=\"content-ad\"></div>\n\nSuperset은 대시보딩에 편리하고 효과적인 도구라고 생각하지만, 시간이 지남에 따라 생성된 데이터셋이 증가하면 어느 정도 어수석해질 수 있습니다. Superset의 각 데이터셋은 개별적으로 구성되어 있어 대시보드의 수와 복잡성이 증가함에 따라 중복과 관리 도전이 발생할 수 있습니다. 그럼에도 불구하고, 이러한 어려움에도 불구하고, Superset은 우리의 요구 사항을 잘 충족시켜 주며 조직 전반에서 데이터를 시각화하고 상호 작용하는 다재다능한 플랫폼을 제공합니다.\n\n오케스트레이션과 워크플로우 관리\n\nApache Airflow는 데이터 환경 내에서 워크플로우를 조정하고 관리하는 데 중요한 역할을 합니다. 오픈 소스 도구인 Airflow는 유연성, 확장성, 그리고 강력한 커뮤니티 지원을 제공하여 우리의 운영 요구에 필수적인 요소를 제공합니다. Airflow를 활용하여 모든 데이터 파이프라인이 데이터 레이크로 정확하게 트리거되어 데이터의 신선도와 신뢰성을 유지하도록 합니다.\n\n현재, 저희는 저희 레이크하우스 운영에 필수적인 세 가지 주요 DAGs (방향이 있는 비순환 그래프)를 관리하고 있습니다. 첫 번째 DAG는 AirbyteOperator를 활용하여 동기화를 위해 필요한 모든 업무를 트리거하여 브론즈 레이어에 데이터를 효율적으로 삽입하는 작업을 담당합니다. 두 번째 DAG는 dbt 변환을 실행하여 데이터를 처리하고 실버 및 골드 레이어로 옮기는 업무를 담당합니다. 세 번째 DAG는 전체 워크플로를 감독하며 데이터 처리의 원활한 흐름을 유지하기 위해 순차적으로 삽입 DAG 및 이후에 dbt DAG를 트리거합니다.\n\n<div class=\"content-ad\"></div>\n\n또한, 이러한 워크플로우 내에 Slack 알림을 통합했습니다. 이 설정은 DAG(작업 방향성 비순환 그래프) 실패 시 실시간 알림을 제공하여 지속적인 운영 및 데이터 무결성 유지를 위해 즉각적인 모니터링과 대응이 가능하게 합니다.\n\n## 결론: 전략적 이점을 위한 데이터 활용\n\n마지막으로, 데이터 레이크하우스 아키텍처를 구축하고 정제하는 우리의 여정은 도전적이고 보람찼습니다. 우리는 데이터 관리 역량을 혁신한 여러 도구와 기술을 성공적으로 통합하여, 다양한 데이터를 통합하여 동적 의사 결정을 지원하는 견고한 분석 엔진으로 변화시켰습니다. Apache Superset, AWS S3, AWS Glue Catalog, Apache Airflow, 그리고 dbt를 활용한 우리의 사용은 복잡한 데이터 과제에 대응하기 위해 첨단 기술을 채용하는 데 드러난 우리의 의지를 보여줍니다.\n\n이러한 도구들은 우리의 운영 효율을 향상시키는데 그치지 않고 회사 전반에서 더 통찰력있는 데이터 분석과 보고의 길을 열었습니다. 우리의 데이터 인프라를 계속 발전시키면서, 우리는 데이터 능력을 더욱 향상시킬 수 있는 새로운 기술과 방법을 탐구하는 데 헌신하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n저는 비슷한 여정을 걸어가고 있는 독자들로부터의 피드백과 질문을 환영합니다. LinkedIn에서 저와 연락하셔서 더 자세한 토론을 나누거나 아이디어를 교환해 주세요.\n\n","ogImage":{"url":"/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_0.png"},"coverImage":"/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_0.png","tag":["Tech"],"readingTime":13},{"title":"DBT 증분 전략과 동등성","description":"","date":"2024-05-27 12:53","slug":"2024-05-27-DBTIncrementalStrategyandIdempotency","content":"\n\n![Screenshot](/assets/img/2024-05-27-DBTIncrementalStrategyandIdempotency_0.png)\n\n# 배경\n\n안녕하세요, 저는 데이터 엔지니어인 Todd입니다. 저는 Nowcast에서 데이터 온보딩에 주로 관여하고 있습니다. 이 기술 블로그에서는 Nowcast에서의 ETL 파이프라인 디자인의 간략한 역사를 소개하고, Airflow와 DBT의 \"Incremental Models\" 사이에서 발생한 문제를 설명하고 우리가 개발한 해결책을 소개하겠습니다.\n\n# Python으로 ETL\n\n\n<div class=\"content-ad\"></div>\n\n역사적으로 Nowcast에서는 ETL 파이프라인을 Python을 사용하여 작성했습니다. 이 파이프라인은 AWS S3, Athena, RDBMS 등에 저장된 데이터에 변환을 적용하는 많은 Python 스크립트로 구성되어 있었습니다. 우리는 이러한 스크립트를 포함하는 도커 이미지를 작성하여 ECR에 업로드하고, Airflow에서 ECS 작업을 호출했습니다. 이러한 스크립트는 보통 데이트와 같은 파티션 필드를 매개변수로 사용하여 멱등성이 있도록 설계되었습니다. 즉, 2024-01-01을 전달하면 2024-01-01의 데이터가 처리되었습니다.\n\n이러한 스크립트 중 하나를 호출할 때, 실제로 실행되는 명령은 아래와 같이 보일 것입니다. 이때 데이트 매개변수는 Airflow에서 관리됩니다:\n\n```js\npython transform_data.py 2024-01-01 --some --other --arguments\n```\n\n# Airflow\n\n<div class=\"content-ad\"></div>\n\nAirflow은 Nowcast에서 많은 해동안 사용되어온 스케줄링 및 워크플로우 관리 도구입니다. 기본적으로 두 가지로 사용되고 있어요:\n\n1. 작업 스케줄러\n2. 작업 의존성 관리\n\n역사적으로 Airflow는 매일 실행되며 여러 Python 스크립트에 '실행 날짜' 매개변수를 전달하여 데이터를 처리합니다. 문제가 발생하거나 특정 기간의 작업을 다시 실행해야 할 때는 Airflow DAG에서 해당 작업을 다시 실행할 수 있습니다. 예를 들어, 2024년 01월 01일에 어떤 데이터 변환 스크립트가 실패하면 문제를 식별하고 수정한 후 해당 스크립트를 다시 실행할 수 있어요. 이는 스크립트가 한 번에 하나의 파티션만 처리하고 날짜를 매개변수로 입력받기 때문에 가능한 일입니다.\n\n# DBT에서 ETL\n\n2022년 말쯤 Python ETL 플로우를 Snowflake로 이전하기 시작했습니다. 그 결과 더 빠르고 저렴하며 깨끗한 파이프라인이 만들어졌어요. 우리는 파이프라인 실행 도구로 DBT를 사용하기로 결정했습니다 — DBT는 SQL 위에 위치한 레이어로 DB 모델 정의, 템플릿, 의존성 관리 및 데이터 회귀 테스트와 같은 다양한 기능을 포함하고 있어요. 빠르고 효율적으로 ETL 파이프라인을 구축하는 데 매우 유용한 도구입니다. 파이썬에서는 파이프라인의 각 변환을 스크립트로 작성하지만, DBT에서는 템플릿화된 SQL CTAS 쿼리로 작성됩니다. 이 쿼리들은 복잡한 수천 줄의 코드로 이루어진 스크립트와 비교했을 때 매우 읽기 쉽습니다.\n\n<div class=\"content-ad\"></div>\n\nBest practise in DBT is to use Incremental Models:\n\n# Incremental Models\n\nIncremental models are an efficient way of defining how to (incrementally) add data to our SQL models — consider we have a table that describes credit card transactions — we can make a DBT model (CTAS) that looks something like this:\n\n```js\n{\n materialized=\"table\"\n}\nselect\n transaction_id,\n transaction_date,\n user_id,\n store_name_description,\n transaction_amount\nfrom { ref('external_table_transaction') }\n```\n\n<div class=\"content-ad\"></div>\n\n이 코드는 table external_table_transaction에서 거래 데이터를 불러오는 테이블을 만듭니다. 문제는이 쿼리를 다시 실행할 때마다 전체 테이블을 다시로드한다는 것입니다. 테이블에 데이터가 많아질수록 쿼리가 느려지고 비용이 많이 발생합니다. 이 문제의 해결책은 증분 모델을 사용하는 것입니다:\n\n```js\n{\n config(\n materialized=\"incremental\",\n unique_key=[\"transaction_id\"],\n incremental_strategy=\"delete+insert\",\n )\n}\nselect\n transaction_id,\n transaction_date,\n user_id,\n store_name_description,\n transaction_amount\nfrom { ref('external_table_transaction') }\n {- if is_incremental() }\n where transaction_date = (select max(transaction_date) + 1 as next_date from { this })\n{- endif }\n```\n\n여기에서 우리는 DBT를 강력하게 만드는 일부 매크로/템플릿 기능을 볼 수 있습니다. 이제 기본적으로 하는 것은 테이블의 최신 데이터보다 1일이 더 늦은 거래 데이터만 external_table_transaction에서 로드해야 한다는 것입니다. 이것은 간단하면서도 강력합니다. 업데이트마다 계속 커지는 수십억 개의 데이터 행 처리 대신 이제 이전에 볼 수 없던 행만 처리하면 됩니다. 그리고 필요하다면 전체 갱신으로 테이블을 다시로드할 수 있는 옵션도 있습니다.\n\n# 문제\n\n<div class=\"content-ad\"></div>\n\n점진적 모델은 매우 매력적입니다 — 수학적으로 아름답고 데이터 스트림을 다룰 때 매우 잘 작동합니다. 문제는 처리하려는 데이터를 제어해야 할 때 발생합니다 — 점진적 모델은 특정 파티션만 다시 실행할 수 없으며 대신에 증분 모델의 규칙에 따라 데이터를 로드합니다. 이론적으로는 문제가 되지 않을 수도 있지만, 점진적 모델이 이상적인 환경에서 실행된다면 모든 데이터가 정확히 한 번만 로드될 것입니다 — 하지만 현실은 복잡합니다 — DAG가 깨지고, 데이터가 늦게 전달되거나 아예 제공되지 않는 경우가 발생하며 때로는 역사적 기록을 다시 로드해야 할 때가 있습니다. 게다가 Airflow 파이프라인이 어떤 이유로든 실패할 경우 DBT 작업이 Airflow 실행과 동기화되지 않을 수 있습니다. Nowcast로 마이그레이션한 이후 DBT를 사용하면서 경험한 점진적 모델과 관련된 이슈 목록이 아래에 나와 있습니다:\n\n- 한 파이프라인에서 수리가 진행된 것이 있었는데, 이는 2년 전으로 거슬러 올라가야 했으므로 역사적 데이터를 로드해야 했는데 (증분) 데이터 파이프라인이 역사적 재실행을 처리할 수 없어서 즉시 처리해야 했습니다.\n- 다른 DAG에서 상류 이슈로 3일 동안 깨졌으며, 3일 동안 데이터가 로드되지 않았고, DAG가 4일째 실행될 때 1일부터 데이터를 로드했으므로 동기화가 맞지 않았습니다.\n- 세 번째 파이프라인에서 상류 스킵 날짜(데이터가 빠진 날)가 발생했고, 점진적 모델은 데이터를 로드하기 위해 데이터에서 최대 날짜에 `1`을 추가하는 방식으로 처리했으나 해당 날짜가 나타나지 않아 데이터가 로드되지 않은 채로 수동 처리가 필요해졌습니다.\n\n하지만 우리는 단순히 점진적 모델을 포기할 수 없습니다 — 일부 파이프라인은 수십억 개의 행을 처리해야 하므로, 테이블을 대량으로 처리할 쿼리를 작성하면 느리고 비용이 많이 소요될 것입니다.\n\n# 동형성(idempotency) 및 분할의 중요성.\n\n<div class=\"content-ad\"></div>\n\n점진적 모델의 주요 문제는 이 모델이 멱등성을 갖지 않으며 특정 파티션에 대해 실행 구성이 불가능하다는 것입니다. 우리가 ETL 파이프라인에 대해 예전에 채택한 방식은 멱등 스크립트가 여러 번 다시 실행할 수 있는 횟수에 제한이 없는 것이었습니다. 과거 데이터에 문제가 발생하면 특정 파티션을 다시 생성할 수 있었고, 스크립트가 멱등성을 가졌기 때문에 특정한 날짜를 여러 번 실행해도 문제가 발생하지 않았습니다. 하지만 점진적 모델은 데이터의 특정 파티션을 다시 실행할 수 있는 능력이 없으며, 대신 모든 데이터를 스트림처럼 처리하여 보지 않은 데이터만을 로드합니다. 다시 말해 특정 규칙을 충족하는 데이터를 로드하는 것이죠.\n\n우리가 Airflow라는 스케줄링 도구를 사용하고 있기 때문에 데이터 파이프라인은 어떤 종류의 시간적 분할과 일치해야 합니다. 시간별, 일별, 주별, 월별 등 다양한 분할 방식이 될 수 있지만 중요한 점은 Airflow가 어떤 일정에 따라 실행되고 있다는 것입니다. 만약 과거 Airflow 작업을 다시 실행한다면 해당 작업을 호출할 때 해당하는 시간적 파티션에 맞게 실행되기를 기대하지만, 점진적 모델은 항상 앞으로만 '보기' 때문에 과거의 파티션에 대해 구성되지 않습니다. 이것은 Airflow에서 작업을 실행할 때 예상하는 것과는 다릅니다.\n\n하루마다 실행되는 2개의 Airflow DAG를 고려해보죠. 하나의 DAG는 매개변수로 날짜를 사용하여 해당 파티션만 실행하는 작업을 가지고 있습니다. 다른 DAG는 점진적 모델을 사용하며 실행할 때 보지 않은 데이터를 처리합니다. 둘 다 정상적으로 실행될 때 이전에 보지 못한 일별 데이터를 처리하게 되며 두 DAG는 동일하게 동작합니다. 하지만 문제가 발생하여 특정 날짜인 2024년 1월 1일을 다시 로드해야 할 때는 어떨까요? 파티션화된 DAG는 예상대로동작하여 2024년 1월 1일을 다시 실행할 것이지만, 점진적 모델은 Airflow에 전달되는 날짜와 관계없이 이전에 본 적 없는 데이터만을 로드할 것입니다.\n\n점진적 모델의 한계에 대해 논평한 댓글에서는:\n\n<div class=\"content-ad\"></div>\n\n간단히 말하면 - Airflow와 같은 일정 관리 도구를 사용할 때 시간 분할을 기대하는 경우, 점진적 모델이 잘 작동하지 않습니다.\n\n# 해결책\n\n해결책은 간단합니다 - DBT 변수를 사용할 수 있습니다. 또한 점진적 모델의 기능을 완전히 포기할 필요가 없습니다. 하나 이상의 변수를 추가하여 하나 이상의 분할에 명시적으로 실행할 수 있습니다:\n\n```js\n{- set target_date = var(\"target_date\", \"\") }\n{\n config(\n materialized=\"incremental\",\n unique_key=[\"transaction_id\"],\n incremental_strategy=\"delete+insert\",\n )\n}\nselect\n transaction_id,\n transaction_date,\n user_id,\n store_name_description,\n transaction_amount\nfrom { ref('external_table_transaction') }\n{- if target_date != \"\" }\n where transaction_date = '{ target_date }'\n{- else }\n {- if is_incremental() }\n where transaction_date = (select max(transaction_date) + 1 as next_date from { this })\n {- endif }\n{- endif }\n```\n\n<div class=\"content-ad\"></div>\n\n이것은 DBT 모델에 `target_date`라는 새 매개변수를 추가합니다. `target_date`가 정의되지 않은 경우 모델은 증분 동작으로 실행되지만, 변수가 전달된 경우 지정된 파티션에 대해 실행됩니다. 이 모델 구조화 방식은 Airflow에서 호출될 때 훨씬 더 잘 작동합니다.\n\n게다가, 이 모델은 이제 멱등성이 생겼습니다. 즉, 원본 데이터가 동일한 경우 동일한 쿼리와 매개변수로 실행하고 동일한 결과를 얻을 수 있습니다. 반면 증분 모델의 경우 로드된 데이터는 테이블 내용 및 상위 스트림에서 발생한 변경 내용에 따라 달라집니다.\n\n이 솔루션은 병렬, 증분 및 파티션화의 3가지 모드를 효과적으로 제공합니다. 따라서 Airflow와 DBT의 의도된 증분 전략과 잘 어울리며 이를 사용할 경우 잘 작동합니다. 아래와 같이 인수 없이 DBT를 실행하면 증분 모델을 사용할 것입니다:\n\n```js\ndbt run --select my_model\n```\n\n<div class=\"content-ad\"></div>\n\n명시적으로 새로 고침을 실행하면 대량 적재가 발생합니다:\n\n```js\ndbt run --select my_model --full-refresh\n```\n\n그리고 추가한 target_date 매개변수를 전달하면 특정 파티션에 대해서만 실행되도록 할 수 있습니다:\n\n```js\ndbt run --select my_model  --vars \"{target_date : '2024-01-01'}\"\n```\n\n<div class=\"content-ad\"></div>\n\n이제 Airflow가 전달되는 날짜 매개변수를 제어할 수 있는 명령으로 돌아왔어요. 이렇게 하면 훨씬 더 부드러운 통합이 가능해요!\n\n# 참고 자료\n\n이 문제를 연구하는 데 사용된 다음 문서들입니다:\n\nDBT — 증분성의 한계에 대해\nMedium — DBT와 Airflow를 사용한 멱등데이터 파이프라인\n\n<div class=\"content-ad\"></div>\n\n# Nowcast의 엔지니어링\n\n만약 DBT에서 데이터 파이프라인을 구축하는 방법에 대해 알고 싶으면 아래 링크를 사용하여 친목을 돈 미팅을 예약해보세요. '문의 사항'란에 'Todd와 이야기하고 싶어요'라고 작성해주세요.\n\nNowcast는 현재 데이터 엔지니어를 채용 중입니다! 관심이 있으시면 [여기에서 지원하세요](application_link).\n","ogImage":{"url":"/assets/img/2024-05-27-DBTIncrementalStrategyandIdempotency_0.png"},"coverImage":"/assets/img/2024-05-27-DBTIncrementalStrategyandIdempotency_0.png","tag":["Tech"],"readingTime":7},{"title":"데이터 품질 관리의 과거, 현재, 그리고 미래 2024년에 알아야 할 테스트, 모니터링, 그리고 데이터 관찰 가능성","description":"","date":"2024-05-27 12:51","slug":"2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024","content":"\n\n## 데이터 환경이 진화하고 있으며, 데이터 품질 관리도 함께 발전해야 합니다. 다음은 AI 시대에 데이터 품질 관리가 향하는 방향과 세 가지 일반적인 접근 방식에 대한 정보입니다.\n\n![이미지](/assets/img/2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024_0.png)\n\n서로 다른 용어일까요? 같은 문제에 대한 독특한 접근 방식일까요? 아니면 다른 것일까요?\n\n그리고 더 중요한 것은 — 모두 세 가지가 정말 필요한가요?\n\n<div class=\"content-ad\"></div>\n\n데이터 엔지니어링에서처럼, 데이터 품질 관리도 초속으로 진화하고 있어요. 기업에서 데이터와 AI의 급부상으로 인해, 현대 비즈니스에 있어 데이터 품질은 제로 데이 위험이 되었고 데이터 팀이 해결해야 할 핵심 문제가 되었어요. 중첩 용어가 많아서 어떻게 모두 맞는지 또는 맞는지 여부가 항상 명확하지 않아요.\n\n그러나 몇몇이 주장하는 것과는 달리, 데이터 품질 모니터링, 데이터 테스트 및 데이터 가시화는 데이터 품질 관리에 대한 대안적인 접근 방식도 아니고, 상충되는 것도 아니에요. 이것들은 하나의 해결책의 보완적 요소들이에요.\n\n이 글에서, 이 세 가지 방법론의 구체적인 내용, 각각이 어디에서 가장 잘 작동하며, 어디서 약점이 있는지, 그리고 2024년에 데이터 신뢰를 증진할 수 있는 데이터 품질 실무를 최적화하는 방법에 대해 살펴볼게요.\n\n# 현대 데이터 품질 문제 이해하기\n\n<div class=\"content-ad\"></div>\n\n현재 솔루션을 이해하기 전에 문제를 이해해야 합니다. 시간이 지남에 따라 어떻게 변화했는지 알아야 합니다. 다음 유사성을 고려해 봅시다.\n\n상상해보세요. 당신이 지역 수도 공급을 책임지는 엔지니어라고 상상해봅시다. 당신이 이 직무를 맡을 때, 그 도시에는 단 1,000 명의 주민이 있었습니다. 그러나 도시 아래에 금이 발견되자, 당신의 1,000 명 주민들의 작은 커뮤니티가 1,000,000 명의 진정한 도시로 변모했습니다.\n\n이것이 당신이 하는 일에 어떻게 영향을 미칠까요?\n\n먼저, 작은 환경에서는 실수 포인트가 상대적으로 적습니다. 파이프가 고장나면, 근본 원인을 냉동 파이프, 누군가가 수도관에 파고들면, 일반적인 몇 가지 원인 중 하나로 좁힐 수 있고, 1~2 명의 직원이 리소스로 문제를 빠르게 해결할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n100만 명의 신규 거주민을 디자인하고 유지하기 위한 뱀과 같은 파이프라인, 수요 충족을 위해 필요한 광란스러운 속도, 그리고 팀의 한계적인 능력(및 가시성) 때문에 예상했던 모든 문제를 찾아 해결하거나 감시해야 할 수 있는 능력이 더 이상 동일하지 않습니다. \n\n현대 데이터 환경도 마찬가지입니다. 데이터 팀은 금광을 발견했고 이해 관계자들은 그 발전 상황에 참여하고 싶어합니다. 데이터 환경이 커질수록 데이터 품질 유지가 더 어려워지며 전통적인 데이터 품질 방법이 덜 효과적일 수 있습니다.\n\n그들의 주장이 완전히 틀렸다고 할 수는 없습니다. 하지만 그것만으로 충분하지는 않습니다.\n\n# 그래서 데이터 모니터링, 테스트 및 관찰의 차이는 무엇일까요?\n\n<div class=\"content-ad\"></div>\n\n매우 명확하게, 이러한 방법 중 각각은 데이터 품질에 대응하려는 시도입니다. 따라서, 당신이 해결해야 할 문제가 그것이라면, 이 중 하나는 원칙적으로 그 문제를 확인할 것입니다. 하지만, 이 모두가 데이터 품질 솔루션이라는 것은 실제로 데이터 품질 문제를 해결해주지 않을 수 있다는 뜻입니다.\n\n이러한 솔루션들이 언제 어떻게 사용되어야 하는지는 그것보다는 조금 더 복잡합니다.\n\n가장 간단하면서, 데이터 품질을 문제로 생각할 수 있고, 테스트 및 모니터링을 품질 문제를 식별하는 방법으로 생각할 수 있으며, 데이터 가시성은 더 품질 문제를 해결할 수 있는 더 심도 있는 가시성과 해결 기능을 결합하고 확장하는 다양하고 포괄적인 접근 방식으로 생각할 수 있습니다.\n\n더 간단히 말하여, 모니터링과 테스팅은 문제를 확인하고, 데이터 가시성은 문제를 확인하고 해결책을 제시함으로써 실질적인 대응이 가능합니다.\n\n<div class=\"content-ad\"></div>\n\n여기 데이터 관찰이 데이터 품질 성숙도 곡선에서 어디에 위치하는지 시각화하는 빠른 그림이 있습니다.\n\n![Data Quality Maturity Curve](/assets/img/2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024_1.png)\n\n이제 각 방법에 대해 조금 더 자세히 알아보겠습니다.\n\n# 데이터 테스트\n\n<div class=\"content-ad\"></div>\n\n데이터 품질에 대한 전통적인 두 가지 방법 중 첫 번째는 데이터 테스트입니다. 데이터 품질 테스트(또는 간단히 데이터 테스트)는 사용자 정의 제약 조건이나 규칙을 사용하여 데이터 집합 내에서 특정 알려진 문제를 식별하는 감지 방법으로, 데이터 무결성을 확인하고 특정 데이터 품질 기준을 보장합니다.\n\n데이터 테스트를 생성하기 위해 데이터 품질 소유자는 SQL이나 dbt와 같은 모듈화된 솔루션을 활용하여 특정 문제(예: 과도한 널 비율 또는 잘못된 문자열 패턴)를 감지하는 일련의 수동 스크립트를 작성할 것입니다.\n\n데이터 요구 사항 — 따라서 데이터 품질 요구 사항도 — 가 매우 작은 경우, 많은 팀이 간단한 데이터 테스트에서 필요한 것을 충분히 얻을 수 있을 것입니다. 그러나 데이터가 커지고 복잡해지면, 새로운 데이터 품질 문제에 직면하게 되고 이를 해결하기 위한 새로운 능력이 필요해질 것입니다. 그 시간은 빨리 다가오게 될 것입니다.\n\n데이터 테스트는 데이터 품질 프레임워크의 필수 구성 요소로 남을 것이지만, 몇 가지 중요한 영역에서 제한이 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 깊은 데이터 지식이 필요합니다 — 데이터 테스트에는 데이터 엔지니어가 1) 품질을 정의하기 위해 충분한 전문 분야 지식이 필요하고, 2) 데이터가 어떻게 실패할 수 있는지에 대한 충분한 지식이 필요합니다.\n- 알 수 없는 문제에 대한 검토가 불가능합니다 — 데이터 테스트는 예상되는 문제에 대해서만 알려줄 뿐, 예상치 못한 사건에 대해서는 알려주지 않습니다. 특정 문제를 커버하기 위해 테스트가 작성되지 않은 경우, 테스트는 해당 문제를 발견할 수 없습니다.\n- 확장성이 없습니다 — 30개 테이블에 대해 10개의 테스트를 작성하는 것은 3,000개 테이블에 대해 100개의 테스트를 작성하는 것과 많은 차이가 있습니다.\n- 제한된 가시성 — 데이터 테스트는 데이터 자체만을 테스트하므로 문제가 데이터, 시스템 또는 해당 시스템을 제공하는 코드와 관련이 있는지 알려줄 수 없습니다.\n- 해결 방법이 없습니다 — 데이터 테스트로 문제를 감지해도, 이를 해결하는 데나 영향을 받는 내용을 이해하는 데는 도움이 되지 않습니다.\n\n어떤 규모에 있어서도, 테스트는 데이터에서 \"불!\"이라고 외치는 것과 같다. 그 후 아무도 어디서 이것을 본 것인지 알려주지 않고 걷어나가는 데이터 버전입니다.\n\n# 데이터 품질 모니터링\n\n데이터 품질 모니터링은 데이터 품질에 대한 또 다른 전통적이면서 다소 세련된 접근 방식으로, 수동 임계값 설정 또는 머신러닝을 통해 계속해서 모니터링하고 데이터에서 숨어있는 알 수 없는 이상 현상을 식별하는 영구적인 솔루션입니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, 데이터가 제때 도착했나요? 예상했던 행 수를 얻었나요?\n\n데이터 품질 모니터링의 주요 이점은 알려지지 않은 알려지지 않은 사항에 대해 보다 넓은 범위의 커버리지를 제공하며, 모든 데이터셋마다 테스트를 작성하거나 복제하여 공통 문제를 수동으로 식별해야 하는 데이터 엔지니어들을 해방시켜줍니다.\n\n어느 면에서는, 데이터 품질 모니터링이 테스트보다 전체적인 측면에서 더 ganzonden입니다. 시간이 흘러도 해당 메트릭을 비교하고 팀이 이미 알려진 문제의 데이터에 대한 단일 단위 테스트에서 보지 못할 패턴을 발견할 수 있도록 해줍니다.\n\n유감스럽게도, 데이터 품질 모니터링은 몇 가지 중요한 측면에서 부족함이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 컴퓨팅 비용 증가 - 데이터 품질 모니터링은 비용이 많이 듭니다. 데이터 테스트와 마찬가지로 데이터 품질 모니터링은 데이터를 직접 쿼리하지만, 알려지지 않은 알려지지 않은 사항을 식별하기 위해 넓게 적용되어야 하므로 큰 컴퓨트 비용이 듭니다.\n- 가치창출 시간이 느림 - 모니터링 임계값은 머신 러닝으로 자동화할 수 있지만, 먼저 각 모니터를 직접 구축해야 합니다. 이는 데이터 환경이 시간이 지남에 따라 확장됨에 따라 각 문제에 대해 많은 양의 코딩을 하고 그 모니터를 수동으로 확장해야 한다는 것을 의미합니다.\n- 제한된 가시성 - 데이터가 다양한 이유로 손상될 수 있습니다. 테스트와 마찬가지로 모니터링은 데이터 자체만을 살펴보기 때문에 이상 사항이 발생했음을 알려줄 뿐, 그 이유를 알려주지는 않습니다.\n- 해결책이 없음 - 모니터링은 테스트보다 더 많은 이상 사항을 감지할 수는 있지만, 여전히 어떤 것이 영향을 받았는지, 누가 그것을 알아야 하는지 또는 그 중 어느 것이 중요한지를 알려줄 수 없습니다.\n\n게다가, 데이터 품질 모니터링이 경고를 전달하는 데에만 더 효과적일 뿐 관리하지는 않는다는 점 때문에 여러분의 데이터 팀은 시간이 지남에 따라 실제로 데이터 신뢰성을 향상시키기보다 경보 피로를 경험할 가능성이 훨씬 더 큽니다.\n\n# 데이터 관측성\n\n이것이 데이터 관측성입니다. 위에서 언급된 방법들과는 달리 데이터 관측성은 종합적인 공급업체 중립적 솔루션을 의미하며, 확장 가능하고 실행 가능한 완전한 데이터 품질 커버리지를 제공하도록 설계되었습니다.\n\n<div class=\"content-ad\"></div>\n\n소프트웨어 엔지니어링의 최고의 실천 방법을 모티브로 한 데이터 관찰은 데이터 품질 관리의 종단간 AI 지원 접근법으로, 데이터 품질 문제에 대한 \"무엇, 누가, 왜, 어떻게\"를 단일 플랫폼 내에서 해결하기 위해 설계되었습니다. 이는 기존 데이터 품질 방법의 한계를 보완하기 위해 테스트와 완전 자동화된 데이터 품질 모니터링을 결합하여 단일 시스템으로 확장한 후, 그것을 데이터, 시스템 및 코드 수준으로 확장하여 데이터 환경을 커버합니다.\n\n중요 사건 관리 및 해결 기능 (자동 열 수준 라인형 및 경보 프로토콜과 같은)과 결합된 데이터 관찰은 데이터 팀이 수집부터 사용까지 데이터 품질 문제를 감지, 분류 및 해결할 수 있도록 돕습니다.\n\n더불어 데이터 관찰은 데이터 엔지니어, 분석가, 데이터 소유자 및 이해 관계자를 포함한 팀 간 협업을 촉진하여 교차 기능적 가치를 제공하도록 설계되었습니다.\n\n데이터 관찰은 전통적인 데이터 품질 실무의 단점을 다음 네 가지 핵심 방식으로 해결합니다:\n\n<div class=\"content-ad\"></div>\n\n- 견고한 사건 분류 및 해결 - 가장 중요한 것은 데이터 관찰성이 사건을 빨리 해결할 수 있는 리소스를 제공합니다. 태깅 및 경보 외에도 데이터 관찰성은 자동 열 수준 계보를 통해 원인 분석 프로세스를 빠르게 처리하여 팀이 영향을 받은 것, 누가 알아야 하는지, 고치러 가야 할 곳을 한 눈에 볼 수 있도록 돕습니다.\n- 완벽한 가시성 - 데이터 관찰성은 데이터 소스를 초월하여 인프라, 파이프라인 및 데이터 이동 및 변환하는 포스트 인게스션 시스템까지 확대하여 회사 전반의 도메인 팀을 위해 데이터 문제를 해결합니다.\n- 가치 실현 속도 향상 - 데이터 관찰성은 ML 기반 모니터를 사용하여 설정 프로세스를 완전히 자동화하고 코딩이나 임계값 설정 없이 즉시 커버리지를 제공하여 환경에 따라 시간이 경과함에 따라 자동으로 확장되는 커버리지를 빠르게 얻을 수 있습니다 (사용자가 정의한 테스트가 쉬워지는 사용자 정의 테스트를 위한 커스텀 인사이트 및 단순화된 코딩 도구도 포함됨).\n- 데이터 제품 건강 추적 - 데이터 관찰성은 전통적인 테이블 형식을 벗어나 모니터링 및 건강 추적을 확장하여 특정 데이터 제품이나 중요 자산의 건강 상태를 모니터링, 측정 및 시각화합니다.\n\n# 데이터 관찰성과 AI\n\n우리는 모두 \"쓰레기를 넣으면 쓰레기가 나온다\"는 말을 들어본 적이 있을 것입니다. 그런데, 그 말은 AI 애플리케이션에 대해 두 배로 참된 것입니다. 그러나 AI는 단순히 출력물을 제공하기 위해 더 나은 데이터 품질 관리가 필요한 것뿐만 아니라 진화하는 데이터 자산의 확장성을 극대화하기 위해 데이터 품질 관리 자체가 AI에 의해 지원되어야 합니다.\n\n데이터 관찰성은 기업 데이터 팀이 AI에 신뢰할 수 있는 데이터를 효과적으로 제공할 수 있도록 해주는 사실상 유일한 데이터 품질 관리 솔루션이자 가능성이라고 볼 수 있습니다. 이를 달성하는 한 가지 방법은 AI로 구동되는 솔루션이기도 하기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\nAI를 활용하여 모니터 생성, 이상 징후 감지, 원인 분석을 통해 데이터 관찰력은 실시간 데이터 스트리밍, RAG 아키텍처 및 기타 AI 사용 사례를 위한 초스케일 데이터 품질 관리를 가능하게 합니다.\n\n# 그렇다면, 2024년에는 데이터 품질이 어떻게 변화할까요?\n\n기업 및 그 이상을 위한 데이터 에스테이트가 계속 발전함에 따라, 전통적인 데이터 품질 방법으로는 데이터 플랫폼이 망가질 수 있는 모든 방법을 감시할 수 없거나 그 문제를 해결하는 데 도움을 줄 수 없습니다.\n\n특히 AI 시대에는 데이터 품질이 비즈니스 리스크뿐만 아니라 존립적인 리스크이기도 합니다. 모델로 공급되는 데이터의 전부를 신뢰할 수 없다면, AI의 출력도 신뢰할 수 없게 됩니다. AI의 빠른 규모에서는 전통적인 데이터 품질 방법으로는 이러한 데이터 자산의 가치나 신뢰성을 보호하는 데 충분하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n효과적인 데이터 품질 관리를 위해서는 테스트와 모니터링이 하나로 통합된 플랫폼에 솔루션이 필요합니다. 이 솔루션은 데이터 환경 전체를 객관적으로 모니터링하고 데이터 팀이 문제를 신속히 해결할 수 있는 자원을 제공해야 합니다.\n\n다시 말해, 최신 데이터 팀이 필요한 것은 데이터 관측성입니다.\n\n첫 번째 단계. 감지하기. 두 번째 단계. 해결하기. 세 번째 단계. 성공하기.","ogImage":{"url":"/assets/img/2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024_0.png"},"coverImage":"/assets/img/2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024_0.png","tag":["Tech"],"readingTime":8},{"title":"어떻게 Apache Airflow에서 2000개 이상의 DBT 모델을 조율하는지","description":"","date":"2024-05-27 12:49","slug":"2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow","content":"\n\n![How we orchestrate 2000 DBT models in Apache Airflow](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_0.png)\n\n요즘에는 DBT (Data Build Tool)가 매우 표현력 있는 SQL과 Jinja 템플릿을 사용하여 변환을 선언하는 방식을 통해 다양한 처리 엔진에 연결되는 데이터 변환 워크플로우로 자리 잡았습니다. 이에 더불어 DBT는 문서 작성, 테스트, 그리고 기본 기능을 확장하는 커뮤니티 제작 패키지에 대한 좋은 지원을 제공합니다. ELT에서의 T를 훨씬 더 쉽고 즐겁게 만들었습니다.\n\nDBT Core는 모델 간의 계보를 다루지만, 프로덕션 환경에서 실행되어야 하는 위치와 시기에 대한 솔루션을 제공하지 않습니다. 다시 말해, 오케스트레이션은 기본적으로 제공되지 않습니다.\n\n본 글에서는 Airflow를 활용하여 DBT Core 프로젝트를 오케스트레이션하는 방법을 살펴볼 수 있습니다. 이를 통해 데이터 분석가 및 심지어 제품 소유자도 자신만의 데이터 모델을 생성하고 유지할 수 있는 직관적인 파이프라인을 만들었습니다. SQL과 Git의 기본 지식만 있으면, 비즈니스의 다양한 사람들이 몇 분 만에 자신의 모델을 Airflow DAG로 전환하여 분산 및 확장 가능한 환경에서 실행할 수 있습니다. 이는 경보, 데이터 품질 테스트, 그리고 내장된 액세스 제어와 함께, Airflow DAG가 무엇인지 알 필요 없이 UI에서 상호 작용할 수 있습니다 😄\n\n<div class=\"content-ad\"></div>\n\n주요 부분으로 나눠 보겠습니다:\n\n- Mono vs Multi DAG 접근 방식\n- 프로젝트 구조 및 DAG 레이아웃\n- DAG 생성 파이프라인\n- DBTOperator를 생성한 방법과 이유\n- 결론 및 앞으로의 계획\n\n# Mono vs Multi DAG 접근 방식\n\n이 문제에 대한 직관적인 방법은 전체 DBT 프로젝트를 \"하나의 큰 DAG\"로 모델링하는 것입니다. 이는 DBT 계보를 고려하여 작업을 연결하기 쉽게 만들어주며 Airflow에서 전체 DBT 프로젝트의 멋진 계보 뷰를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n하지만 Monon DAG 방식에는 이 프로젝트를 시작할 때 우리에게 중요한 몇 가지 단점이 있습니다:\n\n- DAG 레벨에서 일정이 설정되므로 프로젝트 전체가 동일한 일정으로 실행됩니다. 이는 프로젝트 전체에서 모델에 대해 다른 SLA가 있는 경우 문제가 될 수 있습니다.\n- 큰 DAG는 탐색하기 어려울 수 있습니다. 프로젝트에 2000개 이상의 모델이 있는 경우 이 거대한 DAG를 통해 길을 찾는 것은 분석가나 비즈니스 사용자들에게 특히 Airflow에 익숙하지 않은 사람들에게 도전이 될 수 있습니다.\n- 접근 제어에 효율적이지 않습니다. 서로 다른 팀이 DBT 프로젝트의 다른 부분을 소유하고 있기 때문에 Airflow에서도 이 분리를 활용해야 합니다. 예를 들어, 당신의 팀만 당신의 모델을 수동으로 트리거하거나 완전한 새로 고침을 수행할 수 있어야 합니다. 하나의 큰 DAG만 있는 것은 전체 프로젝트에 대한 하나의 접근 제어 계층을 의미합니다.\n- 모델 실패의 경우 알림을 분할하기가 어려울 수 있습니다. 다시 말하지만, 모델 실패의 경우 관련 팀에만 알림을 보내기를 원했습니다.\n\n매우 중요한 참고: DBT는 여러 프로젝트를 네이티브로 지원하기 전에 프로젝트를 시작했습니다. DBT 코어에서 완전히 지원되지 않았지만, DBT 매쉬는 프로젝트를 분할하고 프로젝트 당 하나의 DAG를 갖는 경험을 보다 수월하게 만들 수 있는 방법일 수 있습니다.\n\n## DBT 프로젝트를 여러 DAG로 분리하기\n\n<div class=\"content-ad\"></div>\n\n위에서 언급한 문제를 해결하기 위해, 우리는 조직에 맞는 그룹화 규칙에 따라 프로젝트를 다른 DAG로 나누기로 결정했습니다. 이를 통해 프로젝트의 다른 부분에 대해 서로 다른 SLA를 가질 수 있고, DAG 수준에서 액세스 제어 및 콜백 함수에서 알림/알림 대상을 다르게 설정할 수 있습니다. 또한, 팀은 자신들의 DAG만 쉽게 필터링할 수 있으며, Airflow에서 모델을 더 잘 탐색할 수 있습니다.\n\n그러나, 어떤 모델을 어떤 DAG에 그룹화할지 결정하는 방법 및 종속 DAG를 어떻게 연결할지에 대한 자연스러운 질문들이 제기됩니다.\n\n이 중요한 질문들은 우리를 오늘날의 솔루션을 개발하는 데 이끌었습니다. 여기에서 언급해야 할 중요한 점은 Airflow에서 DBT 라인어지 전체를 볼 수 있는 것이 우리에게 그다지 중요하지 않다는 점입니다. 우리는 데이터 탐색을 위해 Datahub를 사용하며, 이는 매우 좋은 라인어지보기를 제공합니다. 따라서, 우리는 Airflow를 가능한 가장 효율적인 방법으로 모델 실행을 관리하기 위한 도구로 사용하기로 결정했으며, 데이터 발견 도구로 사용하지는 않기로 했습니다.\n\n# 프로젝트 구조 및 DAG 배치\n\n<div class=\"content-ad\"></div>\n\n이전에 언급된 질문들을 고려할 때, 우리는 모델 그룹 개념을 고안해냈습니다. 모델 그룹은 서로 깊게 관련된 데이터 변환의 집합입니다. 예를 들어 함께 새로 고쳐져야하고 단위로서만 의미가 있는 같은 데이터 마트의 테이블들입니다. 또한, 이러한 테이블들은 단일 팀에 의해 소유되고 유지보수됩니다. 모델 그룹은 비즈니스 목표를 달성하기 위해 고안되었습니다. 중간 변환 수행 및 테이블 그룹 준비, 데이터 마트 생성, KPI 계산 등을 수행합니다.\n\n따라서, 우리는 각 모델 그룹 당 하나의 DAG를 가지기로 결정했습니다. 모델들이 서로 밀접하게 관련되어 있고 함께 스케줄되어야하기 때문입니다.\n\n아래에 제시된 최소주의 프로젝트 구조는 레이아웃을 이해하는 데 도움이 될 수 있습니다.\n\n![이미지](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_1.png)\n\n<div class=\"content-ad\"></div>\n\n다음과 같이 설명해보겠습니다:\n\n- dbt_project.yml: 이것은 프로젝트의 루트에 있는 일반 dbt_project 파일입니다. 여기에는 특별한 내용이 없습니다.\n- deployment.yml: 이 파일에는 배포할 모델 그룹을 등록합니다. 즉, 모델 그룹을 DAG로 변환하기 위한 작업을 수행합니다. 실행 일정, 태그, 소유자 등을 지정합니다. 다음과 같이 보일 것입니다:\n\n```js\n# deployment.yml\n---\nmodel_groups:\n  - name: model_group_a # 폴더의 이름입니다.\n    schedule: 0 0 * * * # DAG 일정입니다.\n    owner: Team_A # Airflow에서 DAG의 소유자 (역할)입니다.\n    tags: [tag1, tag2] # Airflow DAG용 태그입니다.\n    description: 추가 변환을 위해 테이블을 준비합니다. # DAG 설명입니다.\n\n  - name: model_group_b\n    schedule: 0 2 * * *\n    owner: Team_A\n    tags: [tag1, tag2]\n    description: 여러 테이블을 조인하여 데이터 마트를 생성합니다.\n```\n\n- model_group_a 및 model_group_b: SQL 모델(동일한 방식으로 DBT Python 모델도 작동합니다)이 포함된 폴더입니다. 이 예제에서는 model_group_a의 model2.sql이 종속성으로 model_group_a의 model1.sql을 참조한다고 가정합니다. 모델 그룹은 DBT 프로젝트의 폴더이며 모델을 포함합니다. 원하는 만큼 모델을 넣을 수 있으며 하위 폴더에 대한 DAG 생성도 허용합니다.\n\n<div class=\"content-ad\"></div>\n\nAirflow에서는 이 구조가 다음과 같이 보일 것입니다.\n\n이 구조를 통해 몇 가지 중요한 점을 보장할 수 있습니다:\n\n- 의존하는 DAG는 센서로 연결됩니다: 이를 통해 각 모델 그룹이 다른 일정에 따라 실행되도록하고 동시에 실패가 하향으로 전파되는 것을 방지할 수 있습니다. 센서 검사에 실패하면 하향 모델은 건너뛰게 됩니다. 여기 중요한 점은 우리가 기본 Airflow 외부 작업 센서를 분기시켜야 했단 점입니다. 이는 우리가 상류 모델 실행의 최종 상태를 확인하려고 했기 때문입니다. 기본 센서는 특정 실행 날짜만 터치할 수 있기 때문입니다.\n- 동일한 DAG 내에서 실행의 계통은 dbt-test 작업을 기반으로 합니다: 이는 데이터 품질 오류가 하향으로 전파되는 것을 방지하여, 데이터 품질 문제가 계속 악화되는 눈덩이 효과를 피할 수 있습니다.\n- 각 DAG (모델 그룹)에는 소유자가 있습니다: 이는 해당 DAG에서 수동 작업(전체 갱신 실행 트리거, 작업 지우기 등)을 취할 수 있는 사람이 적합한 팀 멤버뿐이라는 것을 의미합니다.\n- DAG의 수와 크기는 유연하며, DBT 프로젝트 레이아웃을 따릅니다: 모든 DAG가 모델 그룹을 기준으로 동적으로 생성되기 때문에, 그 크기나 세분성은 원하는 대로 조절할 수 있습니다. DBT 프로젝트 안의 모델 그룹에 포함된 모델 수가 DAG 레이아웃을 지배합니다.\n\n# DAG 생성 파이프라인\n\n<div class=\"content-ad\"></div>\n\n이제 팀원이 DBT 저장소에서 PR을 생성하는 순간부터 어떤 일이 발생하는지 살펴보겠습니다. 간단히 말해서, DBT 프로젝트의 배포 파이프라인은 다음과 같습니다.\n\n![DBT 프로젝트 배포 파이프라인](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_2.png)\n\n매 PR에 CI 단계로서 아래 두 가지 매우 중요한 요소가 도입되었습니다:\n\n- 조직 거버넌스 요구 사항 확인: 각 모델은 소유자와 적절한 태그, 설명 등을 가져야 합니다. 이는 매우 중요한데, 데이터 카탈로그를 풍부하고 의미 있는 것으로 만들어주기 때문입니다.\n- 스테이징 환경에서 업데이트된 모델 실행: 이를 통해 도입되는 변경 사항이 업데이트된 모델 및 하위 종속성에 대한 성공적인 실행을 보장합니다. DBT CI 실행을 위한 우리의 스테이징 영역에는 생산 모델의 대표적인 샘플이 포함되어 있어 CI 테스트 실행 비용을 최소화합니다. DBT 모델을 CI에서 적절히 테스트하는 것은 별도의 포스트가 필요하며 이에 대해 별도의 글이 필요할 것입니다.\n\n<div class=\"content-ad\"></div>\n\nPR이 병합되면 DAG 생성 프로세스가 시작됩니다. 이 프로세스는 DBT manifest.json 파일을 구문 분석하여 전체 그래프를 가져오는 방식으로 작동합니다. 그런 다음 deployment.yaml에 정의된 모델 그룹 규칙에 따라 다른 DAG가 생성됩니다.\n\n여기서 중요한 개념은 DBT manifest를 구문 분석할 때 \"내부\" 및 \"외부\" 모델을 구분하는 것입니다. 내부 모델은 해당 모델 그룹에 포함된 모델이며, 외부 모델은 주어진 모델 그룹 외부의 종속성입니다. 이 구분을 통해 외부 최신 작업 센서인 ExternalLatestTaskSensor를 사용하여 적절한 센서를 할당할 수 있습니다. 이 센서는 Airflow 외부 작업 센서의 파생 버전입니다. 우리는 메타데이터 데이터베이스 쿼리를 수정하여 상위 작업의 최신 상태를 가져와서 (실행 날짜별로 정렬) 센서가 상위 작업의 최신 dbt-test 결과를 확인할 수 있도록 했습니다.\n\n따라서 각 모델 그룹은 개별 일정에 따라 실행될 수 있도록 센서로 연결됩니다. 우리가 고려한 다른 옵션은 TriggerDagRunOperator를 사용하는 것이었지만, 이는 상위 최상위 모델에서만 일정을 설정할 수 있도록 했습니다.\n\n이미지 소스:\n![How we orchestrate 2000 DBT models in Apache Airflow](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_3.png)\n\n<div class=\"content-ad\"></div>\n\n흐름그림(DAGs)을 생성하는 작업 자체는 Jinja를 사용한 템플릿화를 통해 이루어집니다. 결국 우리는 단지 Python 파일들을 생성하는 것이니까요 😃. 특정 DAG에 포함할 모델들, 그들의 \"내부\" 선조 및 \"외부\" 모델 의존성(센서)을 결정하면 됩니다.\n\n마지막으로 생성 작업이 완료되면, DAG와 DBT 프로젝트 자산은 Airflow의 자산 버킷에 푸시됩니다. 거기서 다른 프로세스(Airflow에서 실행 중)가 이를 가져갈 것입니다. Airflow 측면에서 작동 방식을 알고 싶다면, 저의 Airflow 글을 참조해주세요.\n\n# 우리가 DBTOperator를 만든 방법과 이유\n\nAirflow에서 DBT를 실행 중이라면 BashOperator를 사용하여 dbt 명령을 실행하거나, 그 작업을 처리할 DBTOperator를 생성할 수 있습니다. 후자의 옵션은 전자보다 많은 이점을 가지고 있으며, 왜 여러분이 자체 DBTOperator를 만들어야 하는지 설명하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n저희는 airflow-dbt 프로젝트의 오픈 소스 구현을 사용하여 DBTOperator 여정을 시작했습니다. 몇 달 동안 잘 사용해 왔지만, 우리만의 Operator를 만드는 것이 가장 좋을 것이라는 것을 깨달았습니다.\n\n우리는 서브프로세스 명령이 아닌 DBT 프로그래밍 방식의 호출을 사용하고 싶었습니다. 이는 실행 결과를 더 잘 처리하는 방법을 제공하며 또한 모범 사례를 준수합니다. dbt cli를 위한 Python 진입점을 사용한 후 코드가 더 깔끔하고 가독성이 향상되었습니다.\n\n가장 중요한 것은 DBT Orchestration 솔루션의 명백한 제한 사항을 해결하고자 했습니다, 특히 버그 수정을 위한 수동 개입을 처리할 때입니다. 이러한 제한 사항 중 일부는 아래에 나열되어 있습니다.\n\n## 증분 모델의 스키마 변경\n\n<div class=\"content-ad\"></div>\n\nDBT에서 기본으로 제공하는 on_schema_change 옵션 중 우리 문제를 해결하는 데는 거의 모든 경우에서 부가 정보를 백필할 필요가 있기 때문에 문제 해결이 되지 않았습니다. 예를 들어 열이 추가될 때 정보를 백필해야 하는 경우가 대부분이었습니다. 그래서 스키마 변경 시 유일한 옵션은 전체 새로 고침을 트리거하는 것이었습니다. 우리는 예상된 소스 스키마 변경으로 인해 많은 모델이 실패했고, 그 당시 \"트리거\"를 하려면 Snowflake에서 테이블을 삭제해야 했습니다 😅.\n\n물론, 이것은 이상적이지 않습니다. 그래서 우리가 처음으로 구현한 것 중 하나는 사용자 지정 DBTOperator에서 실행이 실패한 후 dbt-run 실행 로그를 구문 분석하여 실패가 스키마 변경으로 인한 것인지 감지하면 --full-refresh 플래그를 전달하여 해당 모델을 자동으로 다시 트리거하는 기능이었습니다. 이 간단한 기능 덕분에 DBT 모델의 일일 유지 보수 시간이 단축되었습니다.\n\n## 대규모 모델이나 전체 새로 고침의 초기 처리\n\n가끔 아주 큰 모델의 초기 처리를 할 때나 여러 이유로 수동으로 전체 새로 고침을 트리거할 때, Snowflake DBT Warehouse를 과부하시키는 경우가 있습니다. 그를 피하기 위해 DBTOperator에 기능을 만들어서 해당 모델을 실행하는 데 사용하는 웨어하우스를 동적으로 변경하고 크기(소형, 중형, 대형 등)를 설정하는 기능을 만들었습니다.\n\n<div class=\"content-ad\"></div>\n\n이렇게 함으로써, 모든 기본적인 작은 증분 모델을 동시에 동일한 DBT Warehouse에서 실행할 수 있으면서, 전용 리소스가 할당된 격리된 Warehouse에서 대규모 실행을 수행할 수 있습니다. 이는 Snowflake 쿼리 실행 대기열의 증가를 방지합니다.\n\n또한, 데이터 분석가들이 Airflow 인터페이스에서 직접 전체 새로 고침을 트리거할 수 있게하여, 테이블을 삭제할 필요가 없습니다. DBTOperator가 하는 일은 적절할 때 dbt run 명령에 --full-refresh 플래그를 전달하는 것 뿐입니다.\n\n## 모델 그룹에서 개별 모델 수동 트리거\n\n때로는 데이터 분석가들이 DAG의 모델 그룹에서 하나 또는 두 개의 모델만 실행하도록 트리거해야 할 필요가 있습니다. 가끔 이러한 실행은 지정된 모델의 전체 새로 고침이어야 할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n위 문제를 해결하기 위해, Airflow DAG의 매개변수로 사용 가능한 옵션을 만들어 선택한 분석가가 DAG에서 특정 모델만 트리거할 수 있도록 했습니다. 그 특정 DagRun에 선택되지 않은 다른 모든 모델은 건너뛰게 됩니다. 이 접근 방식은 한 두 개의 모델만 실행해야 할 때 모든 모델을 실행하여 리소스를 낭비하는 것을 방지합니다.\n\nAirflow의 clear task 옵션을 사용하는 것도 해결책이지만, 사용자가 전체 리프레시를 실행하거나 그 모델을 실행하는 데이터 웨어하우스를 변경해야 하는 경우에는 제한적입니다. Airflow에서 작업을 지우기만 해서 매개변수로 실행을 사용자 정의할 수는 없습니다. 이 사용자 정의 옵션을 통해 분석가가 더 정확하게 그들의 요구 사항을 지정할 수 있어 모델 실행의 효율성과 유연성을 향상시킬 수 있습니다.\n\n## 모델 수정 후 하류 종속성의 트리거\n\n우리의 Airflow-DBT 구조에서 모델 그룹에 따라 많은 DAG가 있으며, 일부 모델은 4~5개의 DAG로 구성된 긴 종속성 체인을 가지고 있습니다. 그 체인의 첫 번째 DAG에서 모델 실행(실행 또는 테스트)이 실패하면 다른 DAG에서의 모든 하류 모델이 오류 전파를 방지하기 위해 건너뛰게 됩니다. 첫 번째 모델이 수정된 후에도 우리는 모든 하류 DAG를 다시 실행할 수 있도록 어떻게 보장할 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\n이전에는 이 작업을 수동으로 처리했었어요 😞. 데이터 분석가들은 모델 수정이 적용된 후 재시작해야 하는 하향 DAG들을 계속 추적해야 했어요. 이 과정은 시간이 많이 소요되고 오류가 발생하기 쉬웠어요.\n\n이 문제를 해결하기 위해, 우리는 Airflow DAG에서 사용되는 DBTOperator의 사용자 정의 로직에 의해 구동되는 하향 트리거 옵션을 만들었어요. DAG 실행 시 이 값을 설정하면 모든 모델이 성공하면 DAG는 자동으로 모든 하향 종속성을 인지하고 해당 종속성들의 DagRun을 트리거합니다. 이를 통해 버그 수정 후 DAG를 수동으로 트리거하는 프로세스가 불필요해졌어요.\n\n우리는 dbt ls 명령을 사용하여 종속성 그래프에 있는 모델을 나열하는데 구현이 간단했어요. 그런 다음, 해당 모델을 DAG와 매핑하고 Airflow의 trigger_dag() 함수를 사용하여 하향 실행을 자동으로 트리거했어요.\n\n더 중요한 것은, 이 프로세스가 \"체인 반응\"으로 자동으로 계속된다는 것이에요: 트리거된 DAG는 완료되면 하향 종속성도 트리거하도록 인수 플래그를 받아 이 프로세스는 체인에서 마지막 DAG가 완료될 때까지 계속됩니다.\n\n<div class=\"content-ad\"></div>\n\n## DAG 매개변수를 통한 DBT 실행 인터페이스\n\nDBTOperator에 위에서 언급한 솔루션들을 구현한 후, 우리는 또한 DAG 매개변수를 생성하여 일부 구성을 사용자에게 노출시켜 수동 실행을 사용자 정의할 수 있도록 했습니다.\n\n![image](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_4.png)\n\n이로써 데이터 분석가들과 분석 엔지니어들의 일상에 큰 변화가 생겼습니다. 이제 필요할 때 수동 실행을 완전히 사용자 정의할 수 있게 되었습니다.\n\n<div class=\"content-ad\"></div>\n\n이러한 모든 매개변수는 이전에 설명한 대로 DAG를 자동으로 생성할 때 템플릿에 동적으로 추가됩니다. 따라서 예를 들어, 해당 모델 그룹의 사용 가능한 모델을 사용하여 Models 드롭다운을 채우게 됩니다.\n\n또한 DAG 생성 파이프라인에서 사용되는 \"매개변수 주입\" 방법은 매우 확장 가능하여 미래에 필요에 따라 더 많은 매개변수를 생성할 수 있습니다. \n\n# 결론과 앞으로의 방향\n\n저는 이 게시물이 Airflow에서 DBT 오케스트레이션에 대한 다른 관점을 제공할 수 있기를 바랍니다. 이 구현은 2년이 지난 후에도 여전히 우리의 요구를 충족시키지만, 완벽하거나 이상적이지는 않으며 개선할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n비슷한 오픈 소스 구현으로는 뛰어난 Astronomer Cosmos 프로젝트를 찾을 수 있어요. 여기서 재미있는 기능은 각 모델에 대한 실행 및 테스트를 결합하기 위해 작업 그룹을 사용한다는 점이에요 (우리도 그랬죠 😍) 그리고 프로젝트 구성을 통해 매우 쉽고 깔끔하게 DbtDag를 선언하는 방식이에요.\n\n프로젝트를 둘 이상의 DAG로 분할하는 것도 가능해요, 생성자가 dbt select 인수를 허용하기 때문에요. 따라서 태그를 전달하고 다른 태그에 따라 프로젝트를 분할할 수 있어요. 그러나 저는 DAG 간의 가능한 상호 작용 (모델 참조)을 어떻게 다루는지는 분명하지 않아요. DBT 조정 여정을 시작한다면 매우 깔끔한 추상화를 제공하기 때문에 꼭 확인해보세요.\n\n지금은 우리 앞에 있는 것이 데이터 계약의 구현이며 그것이 우리가 DBT와 상호 작용하는 방식에 큰 영향을 미치고 있어요. 소스 시스템과 데이터 레이크의 테이블 사이를 잇기 위해 계약을 사용함으로써 연결자(데이터 추출기)의 프로비저닝과 업무 지식이 필요하지 않은 초기(기본) 변환(유형 캐스팅, 열 이름 표준화, 복잡한 필드의 언네스팅)을 수행하는 DBT 모델을 자동화할 수 있어요. 결과적으로, 이전에 설명한 일부 모델 그룹은 데이터 계약을 기반으로 완전히 자동화된 방식으로 생성되고 있어요.\n\nDBT-Airflow 구현에 대해 더 논의할 준비가 되어 있고 커뮤니티가 이 문제를 해결하는 방법에 대해 듣는 것에 매우 열려 있어요. 따라서 유사한 구현이 있다면 어떻게 하는지 알려주세요 😆. 진정으로 높은 가치를 제공하는 멋진 솔루션을 만들 수 있는 것은 연결된 커뮤니티 덕분이에요.","ogImage":{"url":"/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_0.png"},"coverImage":"/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_0.png","tag":["Tech"],"readingTime":12},{"title":"이미지를 클러스터링하는 방법","description":"","date":"2024-05-27 12:46","slug":"2024-05-27-HowtoClusterImages","content":"\n\n![FiftyOne](https://miro.medium.com/v2/resize:fit:1400/1*b6uzxatq8ELEOu-1SjMmGg.gif)\n\n# FiftyOne, Scikit-learn 및 Feature Embeddings을 사용하기\n\n2024년 깊은 학습의 계산 집약적인 환경에서 \"클러스터\"라는 단어는 주로 GPU 클러스터를 논할 때 가장 자주 나타납니다. 이는 매우 최적화된 행렬 곱셈 기계의 대규모 컬렉션으로, 동등하게 거대한 생성 모델을 훈련시키기 위해 설정된 것입니다.\n모두가 더 크고 더 나은 모델을 훈련시키고, AI 모델 성능의 한계를 끌어올리며, 최신의 구조적 진보를 자료에 적용하는 데 주력합니다.\n\n그런데 더 나은 모델을 구축하려고 할 때 더 중요할 수 있는 다른 유형의 클러스터가 있는데요. 저는 CPU나 TPU, 또는 다른 어떤 종류의 하드웨어에 대해 언급하는 게 아닙니다. 심지어 모델 훈련 과정에 대해서도 말하는 게 아닙니다. 저는 데이터를 깊이 이해하는 데 도움이 되는 옛날의 비지도 학습 작업인 클러스터링을 말하는 것입니다. 끝내 우리에게 예측력이 흐르는 원천이기 때문이죠.\n\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-05-27-HowtoClusterImages_0.png)\n\nIn this blog, we’ll cover the basics of clustering and show you how to structure your visual data using the open-source machine learning libraries Scikit-learn and FiftyOne!\n\n# What is Clustering?\n\n![Image](/assets/img/2024-05-27-HowtoClusterImages_1.png)\n\n\n<div class=\"content-ad\"></div>\n\n## 클러스터링의 기본 요소\n\n바닥에 펼쳐진 다양한 모양과 크기의 레고 블록이 많이 있다고 상상해보세요. 이제 레고 블록을 정리해야 할 때, 모든 블록을 보관할 충분히 큰 용기가 없다는 것을 깨닫게 됩니다. 운좋게도, 각각이 거의 동일한 수의 조각을 수납할 수 있는 작은 상자 네 개를 찾을 수 있습니다. 단순히 임의로 레고를 각 상자에 넣고 하루를 끝내도 좋습니다. 하지만, 그 다음에 특정 조각을 찾으러 갈 때, 그것을 찾기 위해 꽤나 많은 시간이 걸릴 것입니다.\n\n대신 비슷한 조각을 동일한 상자에 넣는 것이 훨씬 많은 시간과 고민을 덜어줄 것입니다. 그러나 Legos를 어떤 기준으로 상자에 넣을 것입니까? 다른 색깔에 상자를 할당할 것입니까? 아니면 하나의 상자에 모든 네모난 조각을, 다른 하나에는 원형 조각을 넣을 것입니까? 실제 사용하는 Legos에 달렸습니다! 요약하자면, 이것이 클러스터링입니다.\n\n보다 형식적으로 말하면, 클러스터링 또는 군집 분석은 데이터 포인트를 그룹화하기 위한 기술입니다. 클러스터링 알고리즘은 다수의 개체를 입력 받아 각 개체에 대한 할당을 출력합니다. 그러나 분류와 달리 클러스터링은 개체를 사전 설정된 버킷으로 강제로 분류하는 클래스 목록으로 시작하지 않습니다. 오히려 데이터를 통해 주어진 버킷을 발견하려고 시도합니다. 다시 말해, 클러스터링은 데이터에서 구조를 발견하는 데 관한 것이며, 이미 존재하는 구조에서 레이블을 예측하는 것이 아닙니다.\n\n<div class=\"content-ad\"></div>\n\n마지막으로 강조해야 할 점은 클러스터링이 레이블을 예측하는 것과는 관련이 없다는 점입니다. 분류, 탐지 및 분할 작업과는 달리 클러스터링 작업에 대한 'ground truth' 레이블이 없습니다. 이러한 알고리즘을 비지도학습이라고 하며, 지도학습과 자기지도학습과 대비됩니다.\n\n클러스터링은 교육이 필요하지 않습니다. 클러스터링 알고리즘은 데이터 포인트(객체)의 특징을 입력으로 받아 이러한 특징을 사용하여 객체를 그룹으로 분할합니다. 성공적일 때, 이러한 그룹은 독특한 특성을 강조하여 데이터 구조를 파악할 수 있게 해줍니다.\n\n💡 이는 클러스터링이 데이터를 탐색하는 데 매우 강력한 도구임을 의미합니다 - 특히 데이터가 레이블이 없는 경우에 유용합니다!\n\n## 클러스터링은 어떻게 작동합니까?\n\n<div class=\"content-ad\"></div>\n\n주의 깊게 살펴보셨다면 클러스터링과 클러스터링 알고리즘 사이에 섬세한 차이를 발견할 수 있을 겁니다. 이는 클러스터링이 여러 기법을 포괄하는 더 큰 범주이기 때문이에요!\n\n클러스터링 알고리즘은 몇 가지 다른 유형으로 나눠집니다. 이들은 클러스터 멤버십을 할당하는데 사용하는 기준에 의해 구분됩니다. 가장 일반적인 클러스터링 유형 중 일부는 다음과 같아요:\n\n1. 중심 기반 클러스터링: 예를 들어, K-평균 및 평균 변이 클러스터링과 같은 기법들이 속합니다. 이러한 방법들은 각 클러스터를 정의하기 위해 중심점을 찾으려고 합니다. 클러스터 내의 점들 간 일정한 일관성 개념을 최대화하는 중심점을 살펴봐요. 이 클러스터링 유형은 대규모 데이터셋에 잘 확장되지만 이상점과 무작위 초기화에 민감합니다. 종종 여러 번 실행하고 가장 나은 결과를 선택합니다. K-평균과 같은 기법이 고차원 데이터에서 어려움을 겪을 수 있고, 고차원 축소 기법과 함께 사용할 때 구조를 더 잘 발견할 수 있는 것을 발견할 거에요. 아래에서 두 가지를 어떻게 함께 사용하는지 설명하겠습니다.\n\n2. 밀도 기반 클러스터링: DBSCAN, HDBSCAN 및 OPTICS와 같은 기법은 특징 공간이 얼마나 희소하거나 밀집되어 있는지에 기반하여 클러스터를 선택합니다. 개념적으로, 이 알고리즘은 고밀도 영역을 클러스터로 취급하고, 특징 공간에서 포인트가 충분히 퍼져있을 때 클러스터를 분리합니다. DBSCAN과 같이 간단한 밀도 기반 기법은 데이터가 밀접하게 위치하지 않을 수 있는 고차원 데이터에서 작업하는 데 어려움을 겪을 수 있습니다. 그러나 HDBSCAN과 같은 더 정교한 기법은 이러한 한계를 극복하고 고차원 특징에서 탁월한 구조를 발견할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n계층적 군집화: 이러한 기술은 다음 중 하나를 추구합니다:\n\n- 개별 점부터 시작하여 군집을 반복적으로 결합하여 더 큰 복합체를 구성하거나\n- 하나의 군집에 있는 모든 객체로 시작하여 군집을 작은 구성요소로 반복적으로 분할하는 방식\n\n데이터셋이 커질수록 구성기술은 계산적으로 소모적이 되지만, 작고 중간 규모의 데이터셋 및 저차원 특징에 대해서는 성능이 꽤 뛰어날 수 있습니다.\n\n📚 가장 일반적으로 사용되는 10가지 이상의 군집화 알고리즘에 대한 포괄적인 논의를 원한다면, Scikit-learn의 직관적이고 잘 쓰여진 가이드를 확인해보세요!\n\n<div class=\"content-ad\"></div>\n\n## 어떤 기능에 클러스터링 해야 할까요?\n\n우리는 이 글에서 시작한 레고 블록들에 대한 토론 중, 특징들(길이, 너비, 높이, 곡률 등)을 독립적인 엔티티로 생각할 수 있는 데이터 테이블의 열로 볼 수 있습니다. 이 데이터를 정규화하여 각 특징이 다른 특징들을 지배하지 않도록 한 후, 각 레고 블록에 대한 수치값의 행을 특징 벡터로 클러스터링 알고리즘에 전달할 수 있습니다. 클러스터링은 이와 같은 많은 응용 분야를 가졌으며, 가벼운 전처리가 된 수치값이나 시계열 데이터로 작동합니다.\n\n이미지와 같은 구조화되지 않은 데이터는 몇 가지 간단한 이유로 이 프레임워크에 매끄럽게 들어가지 않습니다:\n\n- 이미지는 크기(가로세로 비율과 해상도)가 다를 수 있습니다.\n- 원시 픽셀 값은 매우 노이즈가 있을 수 있습니다.\n- 픽셀 간의 상관 관계는 매우 비선형 일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n우리가 모든 이미지 크기를 재정렬하고 표준화하며, 픽셀 값 정규화를 하고, 노이즈 제거를 하고, 다차원 배열을 \"특성 벡터\"로 평평하게 만들면, 이러한 처리된 픽셀 배열을 특성으로 다루는 것은 비지도 클러스터링 알고리즘에 많은 양의 스트레스를 줄 수 있습니다. 이는 MNIST와 같은 간단한 데이터 세트에 대해서는 작동할 수 있지만, 실제로는 종종 선택사항이 아닙니다.\n\n다행히도, 깊은 신경망이라 불리는 강력한 비선형 함수 근사 도구들이 있습니다! 이미지 도메인에 주목한다면, CLIP 및 DINOv2와 같은 모델이 있습니다. 이러한 모델의 출력은 입력 데이터의 의미 있는 표현이며, 이미지 분류와 같은 특정 작업을 위해 훈련된 모델들도 있습니다. 일반적으로 네트워크의 끝에서 두 번째 층의 출력을 취합니다. 또한 변수적 오토인코더(VAE) 네트워크도 있으며, 일반적으로 중간 층에서 표현을 취하는 것이 일반적입니다!\n\n💡다양한 모델에는 서로 다른 아키텍처가 있으며, 서로 다른 데이터 세트에서 훈련되었으며 서로 다른 작업을 향해 훈련되었습니다. 이러한 모든 요소는 모델이 학습하는 특성의 유형에 영향을 줍니다. 당신의 과제를 해보세요 📚:)\n\n<div class=\"content-ad\"></div>\n\n## 설정 및 설치\n\n지금까지 배경 지식을 살펴보았으니, 그 이론을 실제로 적용하여 비구조적 데이터를 구조화하는 클러스터링 방법을 배워봅시다. 이를 위해 가장 흔한 클러스터링 알고리즘을 구현한 scikit-learn과 unstructured data의 관리 및 시각화를 간소화하는 fiftyone이라는 두 개의 오픈 소스 머신 러닝 라이브러리를 활용할 것입니다:\n\n```js\npip install -U scikit-learn fiftyone\n```\n\nFiftyOne Clustering Plugin은 우리의 일상을 더욱 쉽게 만들어줍니다. 이 플러그인은 scikit-learn의 클러스터링 알고리즘과 이미지 사이의 연결 조직 역할을 하며, FiftyOne App 내에서 간단한 UI로 이러한 작업을 모두 래핑합니다. 플러그인을 CLI에서 설치할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n\nfiftyone 플러그인을 https://github.com/jacobmarks/clustering-plugin 에서 다운로드하세요.\n\n또한 CLIP 모델을 사용하여 이미지 피처를 생성할 수 있게 해주는 OpenAI의 CLIP GitHub 저장소와, 이러한 피처를 2D로 시각화하기 위해 Uniform Manifold Approximation and Projection (UMAP)이라는 차원 축소 기술을 적용할 수 있게 해주는 umap-learn 라이브러리를 두 가지 더 필요로 할 것입니다:\n\n```js\npip install umap-learn git+https://github.com/openai/CLIP.git\n```\n\n이 두 라이브러리가 꼭 필요한 것은 아닙니다. FiftyOne Model Zoo에서 임베딩을 노출하는 임의의 모델로 피처를 생성하고, PCA나 tSNE와 같은 대체 기술로 차원 축소를 수행할 수도 있습니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n필요한 모든 라이브러리를 설치한 후에, Python 프로세스에서 관련 FiftyOne 모듈을 가져와 FiftyOne Dataset Zoo에서 데이터셋을 로드하세요 (또는 원하는 데이터를 사용하세요!). 이 가이드에서는 MS COCO 데이터셋의 검증 분할 (5,000 샘플)을 사용할 것입니다:\n\n```python\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\n# FiftyOne 모듈 import 및 Zoo에서 데이터셋 로드\ndataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\")\n\n# 라벨을 삭제하여 라벨이 없는 데이터로 시작하는 시뮬레이션\ndataset.select_fields().keep_fields()\n\n# 이름 변경하고 데이터베이스에 유지\ndataset.name = \"clustering-demo\"\ndataset.persistent = True\n\n# 데이터셋을 시각화하기 위해 앱 실행\nsession = fo.launch_app(dataset)\n```\n\n만약 Jupyter Notebook에서 작업 중이라면, auto=False를 전달하고 session.url이 가리키는 곳에 브라우저 탭을 열어주세요 (일반적으로 http://localhost:5151/) 앱을 전체 화면에서 볼 수 있습니다.\n\n<img src=\"/assets/img/2024-05-27-HowtoClusterImages_2.png\" />\n\n<div class=\"content-ad\"></div>\n\n## 기능 생성하기\n\n이제 우리가 데이터를 가지고 있으니, 클러스터링에 사용할 기능을 생성해야 합니다. 이 튜토리얼에서는 두 가지 다른 기능을 살펴볼 것입니다: CLIP Vision Transformer에 의해 생성된 512차원 벡터 및 이러한 고차원 벡터를 UMAP 차원 축소 루틴을 통해 실행하여 생성된 2차원 벡터.\n\nFiftyOne 샘플 컬렉션에 차원 축소를 실행하기 위해, FiftyOne Brain의 `compute_visualization()` 함수를 사용할 것입니다. 해당 함수는 method 키워드 인수를 통해 UMAP, PCA 및 tSNE를 지원합니다. 우리는 데이터셋의 `compute_embeddings()` 메서드를 사용하여 CLIP 임베딩을 생성한 다음 이를 명시적으로 차원 축소 루틴에 전달할 수 있습니다. 그러나 대신, `compute_visualization()`에 CLIP로 임베딩을 계산하도록 암시적으로 지시하고 이러한 임베딩을 필드 \"clip_embeddings\"에 저장한 다음 이를 사용하여 2D 표현을 얻을 수 있습니다:\n\n```js\nres = fob.compute_visualization(\n  dataset,\n  (model = \"clip-vit-base32-torch\"),\n  (embeddings = \"clip_embeddings\"),\n  (method = \"umap\"),\n  (brain_key = \"clip_vis\"),\n  (batch_size = 10)\n);\ndataset.set_values(\"clip_umap\", res.current_points);\n```\n\n<div class=\"content-ad\"></div>\n\nbrain_key 인자를 사용하면 이러한 결과에 이름으로 프로그래밍적으로 또는 FiftyOne 앱에서 앞으로 액세스할 수 있습니다. 마지막 줄은 생성된 2D 벡터의 배열을 가져와 데이터셋의 새 필드 \"clip_umap\"에 저장합니다.\n\n앱을 새로 고침하고 Embeddings 패널을 열면 데이터셋의 2D 표현을 볼 수 있습니다. 플롯의 각 점이 단일 이미지에 해당합니다:\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/0*vsnStl9tEHaj90yr.gif)\n\n## 클러스터 계산 및 시각화\n\n<div class=\"content-ad\"></div>\n\n저희의 특징 벡터를 사용하면 FiftyOne 클러스터링 플러그인을 사용하여 데이터에 구조를 부여할 수 있습니다. FiftyOne 앱에서 키보드의 역따옴표 키를 누른 후 compute_clusters를 입력하십시오. 드롭다운 목록에서 항목을 클릭하여 클러스터링 모달을 엽니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*YG-ldErYMMZd8gV2lNSXPA.gif)\n\n클러스터링 실행 결과에 액세스하기 위해 run_key(앞에서 본 brain_key와 유사함)를 입력하십시오. 이렇게 하면 input 폼이 동적으로 업데이트되는 것을 확인할 수 있습니다. 이 시점에서 클러스터링할 특징과 사용할 클러스터링 알고리즘을 선택해야 합니다!\n\n클러스터링 방법으로 \"kmeans\"를 선택하고 특징 벡터로 \"clip_umap\"을 선택하십시오. 클러스터 수를 20으로 설정하고 다른 모든 매개변수는 기본값을 사용하십시오. 엔터 키를 눌러 클러스터링 알고리즘을 실행하십시오. 몇 초만 기다리면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n컴퓨팅이 완료되면 샘플에 새로운 필드가 추가된 것을 알 수 있습니다. 이 필드에는 정수의 문자열 표현이 포함되어 있고, 해당 샘플이 할당된 클러스터를 나타냅니다. 이 값들을 직접 필터링하고 샘플 그리드에서 한 클러스터씩 볼 수 있습니다:\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*mptQ960wfczV7HWhnWC--w.gif)\n\n더 흥미로운 점은 우리의 임베딩 플롯에서 이 클러스터 레이블에 의해 색칠된 것입니다:\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*-vJwIeVRsfFs2y4_DPbzqA.gif)\n\n<div class=\"content-ad\"></div>\n\n클러스터링된 데이터를 이런 식으로 시각화하면 클러스터링 루틴을 간단히 점검할 수 있고 데이터의 구조에 대한 직관적인 시각을 제공할 수 있습니다. 이 예시에서는 테디 베어들의 클러스터가 다른 데이터와 꽤나 잘 분리된 것을 볼 수 있습니다. 또한, 이 클러스터링 루틴은 농장 동물과 코끼리, 얼룩말과 같은 더 이국적인 동물들 사이의 경계를 찾아냈습니다.\n\n이제, 클러스터링 실행을 새로 만들어보세요. 클러스터의 수를 30개로 늘려볼 거에요 (이 새로운 필드에서 임베딩을 색칠하는 걸 잊지 마세요). 약간의 임의성에 따라 (이 루틴의 초기화가 모두 무작위임에 주의하세요), 이제는 코끼리와 얼룩말이 각자의 클러스터를 차지할 가능성이 높아질 것입니다.\n\n초기 클러스터 세트로 돌아와서, 임베딩 플롯에서 마지막 영역 중 하나를 살펴봐봅시다. 축구를 하는 사람들의 이미지 몇 장이 주로 테니스 이미지의 클러스터에 속해있는 것을 확인할 수 있습니다. 이는 2D 차원 축소된 벡터들을 임베딩 벡터 그 자체 대신에 우리의 클러스터링 루틴으로 전달했기 때문입니다. 2D 투영은 시각화에 도움이 되지만, UMAP과 같은 기술은 구조를 상당히 잘 보존하는데, 상대적 거리는 정확히 보존되지 않고 일부 정보가 손실됩니다.\n\n만약 대신 CLIP 임베딩을 직접 클러스터링 계산에 동일한 하이퍼파라미터로 전달한다면, 이러한 축구 이미지들은 다른 축구 이미지와 함께 피리스비나 야구와 같은 다른 필드 스포츠들과 함께 같은 클러스터에 할당됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*2cjbVS8JBKe8CNyyTzBQRQ.gif)\n\n주요 포인트는 고차원 특성이 저차원 특성보다 나은 것도 그 반대도 아니라는 점입니다. 각 선택에는 어떤 대가가 따릅니다. 이것이 다른 기술, 하이퍼파라미터 및 특성을 실험해보아야 하는 이유입니다.\n\n이것을 더 명확하게 보여주기 위해 HDBSCAN을 클러스터링 알고리즘으로 사용해보겠습니다. 이 알고리즘은 클러스터 수를 지정할 수 없게 하며, 대신 min_cluster_size 및 max_cluster_size와 같은 매개변수를 사용하며 클러스터를 병합할 때 기준을 명시합니다. 우리는 CLIP 임베딩을 특성으로 사용할 것이며, 대략적인 시작점으로 10부터 300까지의 요소로 이루어진 클러스터 만 원한다고 말할 것입니다. 클러스터가 너무 크면 도움이 되지 않을 수 있고, 너무 작으면 신호가 아닌 잡음에 반응할 수 있습니다. 특정 값은 물론 데이터셋에 따라 다를 것입니다!\n\n클러스터 레이블에 따라 색을 입히면 결과가 조금 어수선해 보입니다. 그러나 각 클러스터에 대한 이미지를 개별적으로 살펴보면 데이터셋에서 매우 흥미로운 샘플 집합을 식별한 것을 확인할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*hKcCc7r-OCrFsgfwMT-WYA.gif)\n\n💡 HDBSCAN의 경우, 모든 백그라운드 이미지에 \" -1 \" 레이블이 지정됩니다. 이러한 이미지는 최종 클러스터 중 어느 것에도 병합되지 않습니다.\n\n## 클러스터링 실행 추적하기\n\n다양한 피처 조합, 클러스터링 기술 및 하이퍼파라미터를 테스트하면서 특정 군집 세트를 생성하는 데 사용한 \"구성\"을 추적하고 싶을 수 있습니다. 다행히도 FiftyOne Clustering 플러그인은 사용자 지정 실행을 사용하여 이 모든 작업을 처리합니다. 플러그인은 run_key로 실행을 선택하고 모든 실행 매개변수를 앱에서 멋지게 서식이 있는 출력으로 볼 수 있는 get_clustering_run_info 연산자를 노출합니다.\n\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-27-HowtoClusterImages_3.png)\n\n데이터셋의 `get_run_info()` 메서드에 run_key를 전달하여 이 정보에 프로그래밍 방식으로 액세스할 수도 있어요!\n\n## GPT-4V로 클러스터 레이블링하기\n\n지금까지 우리의 클러스터는 단지 번호만 있었는데, 이는 우리가 사용한 일종의 정리 수단이에요. 그러나 만약 우리가 데이터셋에서 특정 특성에 따라 클러스터를 형성한다면, 그 특성을 식별하고 샘플을 대강 레이블로 사용해야 할 거에요. 단순하게 우리는 각각의 클러스터를 개별적으로 살펴보고 주어진 클러스터 내의 이미지만 선택하고 시각화해서 클러스터를 직접 태그해 볼 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n혹시… 우리가 모두가 함께 할 수 있는 멀티모달 대형 언어 모델을 사용해보는 건 어떨까요? FiftyOne Clustering 플러그인은 GPT-4V의 멀티모달 이해 능력을 활용하여 각 클러스터에 개념적 레이블을 제공합니다.\n\n이 기능을 사용하려면 OpenAI API 키 환경 변수가 필요합니다(필요한 경우 계정을 생성할 수 있음). 아래와 같이 설정할 수 있습니다:\n\n```js\nexport OPENAI_API_KEY=sk-...\n```\n\n이 기능은 label_clusters_with_gpt4v 연산자를 통해 제공되며, 각 클러스터에서 이미지 다섯 개를 무작위로 선택하여 GPT-4V에 과제별 프롬프트와 함께 공급하고 결과를 처리합니다.\n\n<div class=\"content-ad\"></div>\n\n클러스터의 개수에 따라 (GPT-4V는 느릴 수 있으며, 이는 클러스터 수의 선형적인 비례로 확장됩니다) 작업 실행을 위임하고 싶을 수 있습니다. 이를 위해 운영자 모달에서 상자를 선택하고 다음 명령어를 사용하여 작업을 시작할 수 있습니다:\n\n```js\nfiftyone delegated launch\n```\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*qkTocuUvrFm2ye8nDRuQNA.gif\" />\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n이 워크스루에서는 scikit-learn과 FiftyOne을 사용하여 인기있는 클러스터링 알고리즘과 심층 신경망을 결합하여 비구조화된 데이터에 구조를 부여하는 방법을 다뤘어요. 그 과정에서 특징 벡터, 알고리즘 및 선택하는 하이퍼파라미터가 클러스터링 계산의 최종 결과에 큰 영향을 미칠 수 있다는 점을 알 수 있었어요. 클러스터가 무엇을 선택하고 데이터의 구조를 얼마나 잘 식별하는지에 대해서도 그의 결과가 영향을 줄 수 있어요.\n\n이러한 클러스터링 루틴을 데이터에 적용한 후에 몇 가지 중요한 질문이 생기게 돼요:\n\n- 이러한 클러스터링 실행을 어떻게 양적으로 비교하고 대조할 수 있을까요?\n- 여러 클러스터링 실행에서 얻은 통찰력을 종합하여 데이터를 더 잘 이해하는 데 도움이 될까요?\n- 이러한 통찰력을 활용하여 더 나은 모델을 훈련하는 데 어떻게 사용할까요?\n\n이러한 질문에 답하면 클러스터링의 장점을 누리는 데 도움이 될 거예요. 만약 이 게시물을 즐겼고 이러한 후속 주제를 다루길 원한다면 알려주세요 👋!\n\n<div class=\"content-ad\"></div>\n\n# 다음에는 무엇을 해야 할까요\n\n더 심층적으로 클러스터링 세계로 들어가고 싶다면, 다음 몇 가지 방법을 살펴보세요:\n\n- 임베딩 모델 선택: 이 가이드에서는 CLIP, 시맨틱 기반 모델을 사용했습니다. 허깅페이스의 Transformers 라이브러리나 OpenCLIP와 같은 다른 시맨틱 모델을 사용하면 어떻게 달라지는지 확인해보세요. 또한 ResNet50와 같은 \"픽셀 및 패치\" 컴퓨터 비전 모델이나 DINOv2와 같은 자기 지도 학습 모델을 사용할 때 어떻게 변하는지 살펴보세요.\n- 클러스터링 하이퍼파라미터: 이 가이드에서는 클러스터 수에 거의 손 대지 않았습니다. 이 수를 증가하거나 감소시킬 때 결과가 달라질 수 있습니다. k-평균 클러스터링과 같은 몇 가지 기술에서는 최적의 클러스터 수를 추정하기 위해 사용할 수 있는 휴리스틱이 있습니다. 여기서 멈추지 마시고 다른 하이퍼파라미터들도 실험해보세요!\n- 개념 모델링 기법: 이 가이드에서 제공된 내장 개념 모델링 기법은 GPT-4V와 약간의 가볍게 힌트를 사용하여 각 클러스터의 핵심 개념을 식별합니다. 이것은 오픈 엔드 문제를 다루는 한 가지 방법일 뿐입니다. 이미지 캡션 및 주제 모델링을 사용하거나 자신만의 기법을 만들어보세요!\n\n이 글이 마음에 들었고 FiftyOne의 활기찬 오픈 소스 커뮤니티와 소통하고 싶다면:\n\n<div class=\"content-ad\"></div>\n\n- 🎉 FiftyOne Slack 커뮤니티에 거의 3,000명의 AI 애호가 및 실무자들과 함께 참여해보세요!\n- 🎉 AI 모임 네트워크의 12,000명 이상에 가입하여 다가오는 이벤트에 대한 소식을 받아보세요!\n- 💪 FiftyOne 프로젝트를 오픈소스인 GitHub에서 기여해보세요!\n","ogImage":{"url":"/assets/img/2024-05-27-HowtoClusterImages_0.png"},"coverImage":"/assets/img/2024-05-27-HowtoClusterImages_0.png","tag":["Tech"],"readingTime":13},{"title":"칠레의 공공 안전 문제의 정치적 측면","description":"","date":"2024-05-27 12:45","slug":"2024-05-27-ThepoliticaldimensionofthepublicsecurityprobleminChile","content":"\n\n칠레는 항상 중간-높은 수준의 발전, 민주주의 및 안전을 갖춘 국가로 알려져 왔습니다. 그러나 최근에는 이 세 가지 요소가 약화되고 있습니다.\n\n2013년에 칠레의 GDP 인당는 19,196달러(인플레이션 조정)였지만, 2023년에는 16,849달러입니다. 다시 말해, 오늘날의 칠레 경제는 10년 전보다 나쁩니다.\n\n민주주의 측면에서 정치 참여와 정치 경쟁이 늘어난 것은 사실입니다. 그러나 동시에 민주가치, 정당에 대한 신뢰, 일반적으로 민주주의 기관 및 민주주의의 반응성(기관의 대중 요구에 대한 민감성)은 더 약해졌습니다. 실제로 The Economist의 민주주의 지수에서 칠레는 2013년에 7.81의 종합 점수를 기록했으나, 2023년에는 6.62로 하락하여 1.19 포인트 또는 약 15.2% 감소한 것으로 나타났습니다. 또한, V-Dem에 따르면, 칠레의 \"반응성\" 점수는 2013년에 0.75였지만, 2023년에는 0.60으로 하락했습니다. 마지막으로, CEP 조사에 따르면, 2013년에 정당에 대한 신뢰는 단 5%에 불과했으나, 2023년에는 약 2%로 감소하여 국가의 정치 기관에 대한 불신이 확대되고 있다는 점을 강조합니다.\n\n그리고 안전 상황도 좋지 않습니다. 2013년에는 10만 명당 고의적 살인이 3.16건이었지만, 2023년에는 10만 명당 6.3건으로 크게 증가했습니다. 범죄 증가의 주요 원인은 다양한 기관의 취약성에 기인합니다. 예를 들면, 정보 수집 시스템의 약점, 교정 시설 내 수감자 통제의 미비, 안전 기관 간 협조의 부재, 테러에 대한 전략 부재, 통제되지 않은 국경, 체포 및 유죄 판결의 확률 저하, 그리고 초기 유아 시기부터의 사회 예방 정책 부재 등이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n위기에 대응하기 위해 행정부와 의회가 노력해 왔습니다. 그러나 그들의 행동에는 항상 공화주의적 감각의 부재가 우세한 것 같습니다. 결과적으로 경제 성장을 촉진하는 정책은 연기되고, 안보 조치는 제때에 시행되지 않았으며, 정치 체계는 아직 개혁되지 않았습니다. 따라서 칠레가 갇혀 있는 구멍을 벗어날 수 있는 능력은 제도적 인센티브보다는 성실함의 예와 현 로직을 변화시킬 수 있는 공화주의 가치에 안내되는 위대한 지도자들에 달려 있습니다. 이것은 바로 아렌트가 \"세상에 없던 것을 삽입하는 능력\"으로 묘사한 인간의 행동입니다. 그러나 현재 논리를 변화시킬 수 있는 행동의 예다운을 제공하는 공화주의 덕목이 이를 안내해야 합니다.\n\n이러한 덕목을 기르고 문제를 해결할 수 있을까요? 네, 그러나 먼저 우리 지도자들은 이 문제가 단순히 법률 변경으로 해결할 수 없다는 것을 인식해야 합니다. 각자의 변화로 시작되는 문화적 변화가 필요합니다. 이는 용기, 용감함, 궁극적으로는 칠레에 대한 애정을 요구합니다.\n\n![이미지](/assets/img/2024-05-27-ThepoliticaldimensionofthepublicsecurityprobleminChile_0.png)","ogImage":{"url":"/assets/img/2024-05-27-ThepoliticaldimensionofthepublicsecurityprobleminChile_0.png"},"coverImage":"/assets/img/2024-05-27-ThepoliticaldimensionofthepublicsecurityprobleminChile_0.png","tag":["Tech"],"readingTime":2},{"title":"시간에 따른 변화를 시각화하는 멋진 전략들","description":"","date":"2024-05-27 12:44","slug":"2024-05-27-AwesomeStrategiestoVisualizeChangewithTime","content":"\n이 기사는 두 시점을 대조하는 효과적인 전략을 시각화하는 방법을 요약하고, 영감을 주는 그래픽 예시(소스 코드 링크 포함)로 설명합니다.\n\n더 많은 흥미로운 데이터 시각화는 DataBrewer.co에서 찾아볼 수 있습니다. 이는 R 및 Python에서 데이터 분석과 데이터 시각화 기술을 공유하는 훌륭한 플랫폼입니다.\n\n# 두 시간대 비교에 선분 사용하기\n\n다음 그래프는 각 나라의 1800년과 2015년 인간의 수명 기대값, 인구 규모, 그리고 일인당 GDP를 비교합니다. 포인트는 다른 나라의 특정 시점을 나타내며, 선분은 각 나라에서 변화의 시각적 궤적을 제공합니다. 더불어 연한 회색 사각형의 사용은 두 시기 간의 차이를 강조하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n![그래프](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_0.png)\n\n이 그래프는 Lisa Charlotte Rost가 Datawrapper로 원래 만들었습니다. 이 작품을 R ggplot2로 재현한 것을 찾으려면 여기를 확인하세요.\n\n# 방향별 변경 강조를 위해 화살표 사용하기\n\n선분 외에도 화살표는 한 시점에서 다른 시점으로의 변화 방향을 강조하는 특징적인 요소입니다. 아래 그래프는 2000년부터 2020년까지 국회에서 여성들이 보유한 의석 변화를 보여주기 위해 이 전략을 사용하고 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_1.png)\n\n# 연속적인 변화를 나타내는 선 활용하기\n\n선은 데이터 포인트의 연속적인 변화를 직관적으로 보여주는 지표입니다. 아래 그래프는 1880년대부터 2000년대까지 주요 서양 국가들에서 흡연 인기의 동향을 보여주며, 미국, 독일 및 프랑스의 변화를 선명한 색상으로 강조했습니다.\n\n![이미지](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_2.png)\n\n\n<div class=\"content-ad\"></div>\n\n각 나라에서 시간에 따라 변화 추적하는 것 이외에도, 평균 추세선과 변동성 리본을 추가하여 시각 정보를 풍부하게 할 수 있습니다. 다음 그림에서는 각 나라의 인간 기대수명 변화를 나타냅니다. 대륙별로, 중앙 두꺼운 선은 평균 수명을 보여주고, 음영 처리된 리본은 이 평균 추세 주변의 변동성을 나타냅니다. 여기에는 평균을 중심으로 평균을 중심으로 한 표준편차의 값이 있습니다.\n\n![이미지](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_3.png)\n\n# 각 시점에서의 총 값들을 나타내는 리본 사용하기\n\n위의 선 그래프는 각 나라의 동적인 변화를 보여주었습니다. 각 시간별 y 값의 누적 합이 관심사일 때, 영역 그래프를 사용하여 이 목표를 달성할 수 있습니다. 영역 그래프는 기본적으로 쌓인 선 플롯이며, 선들 사이를 채운 색깔로 연속적인 밴드나 리본을 형성합니다.\n\n<div class=\"content-ad\"></div>\n\n다음 그래프는 지난 200년 동안 전 세계에서 미국으로의 이민 인구 변동을 나타냅니다. 각 시간 지점마다 국가별 및 총 인구 수를 보여줍니다.\n\n![Migration Population Dynamics](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_4.png)\n\n위 그래프는 Mirko Lorenz가 Datawrapper를 사용하여 처음 만들었습니다. 이 작품을 R ggplot2를 이용하여 ggalluvial 패키지를 사용해 재현한 것을 여기에서 찾아볼 수 있습니다.\n\n면적/알류비움 그래프의 변형인 스트림 플롯은 미적 강화를 더해줍니다. 다음 그래프는 지난 4개 십년 동안 영화 장르의 인기 변화를 보여줍니다. 멋진 R ggstream 패키지를 사용하여 만든 이 작품의 세부 사항은 David Sjoberg의 훌륭한 블로그 기사를 확인하세요.\n\n<div class=\"content-ad\"></div>\n\n\n![Heatmap](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_5.png)\n\n# 시간 변화를 시각화하는 멋진 전략\n\n히트맵은 시간에 따른 변화를 보여주는 매력적인 도구로, 숫자 값을 색상 척도에 매핑합니다. 아래 그림은 백신 도입 전후 미국 주별 여덟 가지 감염병의 연간 발병 건수를 시각화한 것입니다. 백신이 병의 확산을 통제하는 능력을 인상적으로 보여줍니다.\n\n![Yearly Incidences](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_6.png)\n\n\n<div class=\"content-ad\"></div>\n\n표는 원래 Tynan DeBold와 Dov Friedman에 의해 WSJ에서 발표되었습니다. R ggplot2에서 이 작업의 재현을 보려면 여기를 확인하십시오.\n\n색상이 변화의 전반적인 추세를 효과적으로 반영하지만 단순한 y-축과 비교했을 때 정확한 숫자 값에 대해 해석하기가 쉽지 않습니다. 히트맵에 축을 한 방향 또는 다른 방향으로 보강하는 것이 도움이 될 수 있습니다. 다음 히트맵은 지난 200년 동안의 월별 태양 흑점 활동을 보여줍니다. 히트맵 옆에 동기화된 선 플롯이 있어 y-축을 따라 태양 흑점 활동의 변화를 연속적으로 나타내어 흑점 활동 변동을 보다 직관적으로 해석할 수 있습니다.\n\n![이미지](/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_7.png)\n\n# 애니메이션을 통한 변화 시각화\n\n<div class=\"content-ad\"></div>\n\n2007년 한스 로슬링이 전설적인 TED 토크 '당신이 본 적 없는 최고의 통계'에서 데이터 애니메이션을 유명하게 만들었습니다. 그럼에도 오늘날에도 데이터를 애니메이션으로 렌더링하는 것은 많은 데이터 분석가들에게 끔찍한 작업으로 여겨집니다. 그러나 괜찮아요! 멋진 R gganimate 패키지를 사용하면 정적 플롯을 애니메이션으로 렌더링하는 것은 시간 변수를 기반으로 데이터 집합을 \"faceting\"하는 추가 단일 코드 라인을 추가하는 것만큼 간단할 수 있습니다.\n\n다음의 애니메이션 인구 피라미드는 독일의 과거(어두운 색상)와 모의 데이터를 사용한 미래(밝은 색상)의 인구 구조를 보여줍니다. R ggplot2를 사용하여 이 애니메이션을 만드는 지침을 자세히 알고 싶다면 여기를 확인하세요.\n\n위에 소개된 모든 그래픽은 R ggplot2에서 만들었습니다. 시각화 및 데이터 과학 분야에서 패러다임을 바꾸는 도구입니다. 만일 당신이 시스템적이고 재미있는 방법으로 ggplot2를 배우고 싶다면, 내 책으로 ggplot2에서 만들어진 코믹 스타일 ebook을 확인해 보는 것을 권유합니다. 나의 책에서 배운 내용을 놀랍도록 빠르게 당신만의 멋진 시각화로 변환하는 것을 경험할 수 있을 거예요!\n","ogImage":{"url":"/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_0.png"},"coverImage":"/assets/img/2024-05-27-AwesomeStrategiestoVisualizeChangewithTime_0.png","tag":["Tech"],"readingTime":4}],"page":"111","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":13,"currentPageGroup":5},"__N_SSG":true}