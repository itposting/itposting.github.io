{"pageProps":{"posts":[{"title":"드디어 7B 파라미터 모델이 GPT-4를 이겼어요","description":"","date":"2024-06-19 16:28","slug":"2024-06-19-Finally7BParameterModelbeatsGPT-4","content":"\n\n우리는 작고 매우 효율적인 모델의 시대로 접어들고 있어요!\n\n![Image](/assets/img/2024-06-19-Finally7BParameterModelbeatsGPT-4_0.png)\n\n# 문맥\n\n몇 일 전 새로운 최첨단 오픈소스 모델에 대해 보도했었죠. 이 모델은 GPT-4를 비롯한 다른 모델들을 능가하는 것으로 나타났습니다.\n\n<div class=\"content-ad\"></div>\n\n이 모델은 SQLCoder-70B입니다.\n\n간략히 말해서, 최근 Meta의 CodeLlama-70B를 기반으로, Defog는 자체 수작업 데이터셋을 활용하여 새로운 섬세하게 조정된 모델을 만들었습니다.\n\n결과는? 직접 확인해보세요:\n\n![image](/assets/img/2024-06-19-Finally7BParameterModelbeatsGPT-4_1.png)\n\n<div class=\"content-ad\"></div>\n\n모델은 GPT-4 및 다양한 SQL 작업들을 크게 개선했습니다!\n\n더 알아보기: 여기에서 모든 내용을 읽고 모델을 테스트할 수 있어요.\n\n# SQLCoder-70B에서 SQLCoder-7B로\n\n안타깝게도, 70B 파라미터 모델은 아직 오프라인 통합이나 노트북에서 실행하는 것에는 너무 크다고 생각됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 모델 증류\n\n모델 증류는 작고 간단한 \"학생\" 모델에게 크고 복잡한 \"선생님\" 모델처럼 행동할 수 있도록 가르치는 기계 학습 과정입니다. 선생님의 결과를 배우면서 학생은 비슷한 결정을 내릴 수 있으며, 그 크기나 복잡성이 커질 필요 없이 사용을 빠르고 저렴하게 만들어줍니다. 특히 핸드폰이나 태블릿과 같은 장치에서 사용할 때 유용합니다.\n\n# SQLCoder-7B\n\n모델 증류를 활용하여 Defog는 작은 7B 매개변수 모델을 훈련시키고 표준 벤치마크에서 평가했습니다.\n\n<div class=\"content-ad\"></div>\n\n결과\n\n- 콤팩트 모델\n- SQLCoder 70B 파라미터 모델보다 약간 성능이 떨어지지만,\n- 그래도 전반적으로 GPT-4를 이긴다!\n- SQLCode-7B-2 (7B 모델의 두 번째 반복)와 비교한 성능은 90.5%입니다!\n\n# 결론\n\nSQLCoder-7B의 성공은 특정 도메인에서 프로프리어터리 모델인 GPT-4의 성능을 능가할 수 있는 대규모 기초 모델 위에 세밀하게 조정된 niche, 오픈 소스 모델의 타당한 예입니다.\n\n<div class=\"content-ad\"></div>\n\n메타의 70B 매개변수 CodeLlama에서 파생된 이 모델은 전문 데이터 세트와 대상 지정된 세세한 조정을 통해 SQL 작업과 같은 분야에서 우수한 성능을 달성할 수 있는 잠재력을 보여줍니다!\n\n다가올 몇 달 동안 더 많은 모델이 등장할 것으로 기대되며, 이들은 작고 효율적인 오픈 소스 모델을 사용하여 특정 문제를 해결하기 위해 개발될 것입니다. 이러한 추세는 강력하면서도 다양한 기기와 애플리케이션에 접근 가능하고 적응 가능한 AI 솔루션을 만들기로 대표적인 변화를 보여줍니다.","ogImage":{"url":"/assets/img/2024-06-19-Finally7BParameterModelbeatsGPT-4_0.png"},"coverImage":"/assets/img/2024-06-19-Finally7BParameterModelbeatsGPT-4_0.png","tag":["Tech"],"readingTime":2},{"title":"PostgreSQL 및 데이터베이스 시스템을 포괄적으로 다루는 PostgreSQL 학습 안내서 파트 2","description":"","date":"2024-06-19 16:26","slug":"2024-06-19-LearningPostgresAComprehensiveGuidetoPostgreSQLandDatabaseSystemsPart2","content":"\n\n![그림을 표시할 수 없습니다](/assets/img/2024-06-19-LearningPostgresAComprehensiveGuidetoPostgreSQLandDatabaseSystemsPart2_0.png)\n\nPostgreSQL(줄여서 Postgres)는 신뢰성, 견고함, 다양한 기능으로 유명한 강력한 오픈 소스 객체-관계형 데이터베이스 시스템입니다. 이는 캘리포니아 대학 버클리 캠퍼스의 post-Ingres 프로젝트의 일환으로 개발되었으며, 1996년 처음으로 공개되었습니다.\n\n## SQL이란?\n\nCodd 박사는 관계형 테이블에서 데이터를 조작하기 위한 DSL/Alpha라는 언어를 제안했습니다. Codd의 논문이 발표된 후, IBM은 그의 아이디어를 기반으로 한 프로토타입을 구축하기 위해 그룹을 위임했습니다.\n\n<div class=\"content-ad\"></div>\n\n이 그룹은 DSL/Alpha의 간소화된 버전인 SQUARE를 개발했습니다. SQUARE의 추가 세부 조정으로 SEQUEL이라는 언어가 만들어졌고, 이 언어는 결국 SQL로 줄여졌습니다.\n\nSQL은 처음에 관계형 데이터베이스에서 데이터를 조작하는 데 사용되던 언어였지만, 지금은 다양한 데이터베이스 기술을 통해 데이터를 조작하기 위해 발전해 왔습니다.\n\n## PostgreSQL의 간단한 역사\n\nPostgreSQL은 선조인 Ingres에서 이름을 따왔습니다. Ingres는 Michael Stonebraker 교수가 개발한 관계형 데이터베이스였습니다. 1986년에 Stonebraker 교수는 Ingres 이후의 새로운 기능을 개발하기 위한 프로젝트를 시작했고, 이 프로젝트를 POSTGRES (Post-Ingres의 약어)라고 명명했습니다. 목표는 객체-관계형 데이터베이스를 만들어 사용자가 자신의 객체(데이터 유형 및 함수와 같은)로 데이터베이스를 확장할 수 있게 하는 것이었습니다.\n\n<div class=\"content-ad\"></div>\n\n1994년에 MIT 라이선스 하에 POSTGRES 버전 4.2가 출시되어 전 세계 개발자들과의 협업이 가능해 졌습니다. 그 당시 POSTGRES는 QUEL이라는 내부 쿼리 언어를 사용했습니다. 버클리 대학의 두 학생 Andrew Yu와 Jolly Chen은 QUEL을 더 현대적인 SQL 언어로 대체했습니다. 이 혁신으로 프로젝트는 중요한 업데이트를 강조하기 위해 Postgre95로 이름이 변경되었습니다.\n\n1996년에는 프로젝트가 코드를 호스팅할 공개 서버를 구축하고 Marc G. Fournier, Tom Lane, Bruce Momjian을 포함한 다섯 명의 개발자가 새롭게 명명된 PostgreSQL에서 작업하기 시작했습니다. 그 이후로 프로젝트는 적극적으로 유지보수되고 업데이트되어 왔습니다.\n\n# 데이터베이스 유형\n\n## 관계형 데이터베이스\n\n<div class=\"content-ad\"></div>\n\n- 데이터를 쿼리하고 관리하기 위해 SQL을 사용하세요.\n- MySQL, PostgreSQL, Oracle, Microsoft SQL Server\n\n### NoSQL 데이터베이스\n\n- 비구조화된 데이터와 확장성을 위해 설계되었습니다.\n- 문서 저장소: 데이터를 문서 형식으로 저장하며 일반적으로 JSON 또는 BSON 형식으로 저장됩니다 (MongoDB, CouchDB)\n- 키-값 저장소: 데이터를 키-값 쌍으로 저장합니다 (Redis, DynamoDB)\n- 열 패밀리 저장소: 행 대신 열에 데이터를 저장합니다 (Cassandra, HBase)\n- 그래프 데이터베이스: 노드와 엣지로 데이터를 저장하며 상호 연결된 데이터에 적합합니다. (Neo4j, Amazon Neptune)\n\n### 객체 지향 데이터베이스\n\n<div class=\"content-ad\"></div>\n\n- 객체 지향 프로그래밍과 유사하게 객체로 데이터 저장\n- db4o, ObjectDB\n\n## 계층형 데이터베이스\n\n- 부모-자식 관계를 가진 트리 구조로 데이터 구성\n- IBM 정보 관리 시스템 (IMS)\n\n## 네트워크 데이터베이스\n\n<div class=\"content-ad\"></div>\n\n- 계층적 데이터베이스와 유사하지만 여러 부모 노드와의 복잡한 관계를 허용합니다.\n- 통합 데이터 저장소 (IDS)\n\n## 시계열 데이터베이스\n\n- 시간 기록 데이터를 저장하고 조회하는 데 최적화됨\n- InfluxDB, TimescaleDB\n\n## 공간 데이터베이스\n\n<div class=\"content-ad\"></div>\n\n- 지리적 위치와 같은 공간 데이터를 저장하고 쿼리하는 데 사용됨\n- PostGIS (PostgreSQL의 확장), Oracle Spatial\n\n## 멀티모델 데이터베이스\n\n- 단일 데이터베이스 엔진 내에서 다중 데이터 모델 (문서, 그래프, 키-값)을 지원함\n- ArangoDB, OrientDB\n\n## NewSQL 데이터베이스\n\n<div class=\"content-ad\"></div>\n\n- NoSQL 시스템의 확장성과 전통적인 RDBMS의 ACID 보장을 결합해 보세요!\n- Google Spanner, CockroachDB\n\n## In-Memory Databases\n\n- 빠르고 효율적인 읽기 및 쓰기 성능을 달성하기 위해 주로 메모리에 데이터를 저장해 보세요!\n- 예시: Redis, SAP HANA\n\n## 클라우드 데이터베이스\n\n<div class=\"content-ad\"></div>\n\n- 클라우드 플랫폼에서 호스팅되며 확장성, 고가용성 및 관리 서비스를 제공합니다\n- Amazon RDS, Google Cloud SQL, Microsoft Azure SQL Database\n\n## 열 지향 데이터베이스\n\n- 행이 아닌 열별로 데이터를 저장하여 읽기 중심 작업 및 분석 쿼리에 최적화되어 있습니다\n- Amazon Redshift, Google BigQuery, Apache HBase\n\n## 포스트그레스 기능\n\n<div class=\"content-ad\"></div>\n\n- 오픈 소스: PostgreSQL은 PostgreSQL 라이선스에 따라 배포되어 무료로 사용, 수정, 및 배포가 가능합니다.\n- SQL 표준 지원: PostgreSQL은 SQL 표준과 완전히 호환되며 기본 SQL을 넘어서 많은 확장 기능을 제공합니다.\n- 객체-관계 모델: 전통적인 관계형 모델 외에도 PostgreSQL은 테이블 상속 및 사용자 정의 데이터 유형과 같은 객체 지향 기능도 지원합니다.\n- 확장성: 사용자는 사용자 정의 데이터 유형, 함수, 연산자, 및 인덱스를 생성할 수 있습니다. PostgreSQL은 데이터베이스에 새로운 기능을 추가할 수 있는 확장도 지원합니다.\n- 트랜잭션 지원: PostgreSQL은 트랜잭션 안정성을 보장하기 위해 ACID 속성(원자성, 일관성, 고립성, 지속성)을 지원합니다.\n- JSON 및 XML 지원: JSON 및 XML 형식으로 데이터를 저장하고 처리할 수 있어 반구조적 데이터 작업에 유용합니다.\n- 복제 및 고가용성: PostgreSQL은 비동기 및 동기식 복제, 논리적 복제, 클러스터링 등 다양한 복제 및 고가용성 옵션을 제공합니다.\n\n## 용어\n\n- 테이블: 메모리나 디스크(영구 저장소)에 저장될 수 있는 행과 열의 집합\n- 행: 단일 특정 데이터 항목(엔티티)을 설명하는 열의 집합\n- 열: 개별 데이터 항목\n- 기본 키: 테이블 내에서 값이 고유하고 null일 수 없는 하나 이상의 열\n- 외래 키: 다른 테이블을 참조하는 하나 이상의 열\n\n## 결론\n\n<div class=\"content-ad\"></div>\n\n우리는 포스트그레SQL에 대한 소개로 시작했습니다. 이를 통해 캘리포니아 대학 버클리 캠퍼스의 post-Ingres 프로젝트에서 발전한 객체-관계형 데이터베이스 시스템의 기원을 강조했어요.\n\n1986년 마이클 스톤브레이커 교수가 처음 시작한 이래로, PostgreSQL이란 이름으로 변경된 중요한 업데이트를 거쳐 다양한 발전 과정을 거친 이야기를 전해왔습니다.\n\n또한 SQL의 기원에 대해 살펴보았습니다. 이 언어는 관계형 데이터베이스를 관리하고 쿼리하는 데 중요한 역할을 하는데, 1970년 E. F. Codd 박사가 제안한 데이터를 테이블 집합으로 표현하는 혁신적인 작업부터 시작하여 DSL/Alpha, SQUARE, SEQUEL, 그리고 최종적으로 SQL까지 데이터베이스 쿼리 언어의 발전 과정을 따라갔습니다.\n\n우리는 MySQL, PostgreSQL, Oracle과 같은 전통적인 관계형 데이터베이스에서부터 MongoDB, Redis, Cassandra, Neo4j와 같은 비구조화 데이터용 NoSQL 데이터베이스, 그리고 객체지향, 계층형, 네트워크, 시계열, 공간, 다중모델, NewSQL, 인메모리, 클라우드, 열 지향 데이터베이스 등을 포괄하는 다양한 데이터베이스 시스템의 풍요로운 풍경도 살펴보았습니다. 각각은 서로 다른 사용 사례와 데이터 요구 사항에 적합합니다.\n\n<div class=\"content-ad\"></div>\n\n그런 다음 PostgreSQL의 강력한 기능을 강조했습니다. 이는 오픈 소스 성격, SQL 표준 준수, 객체-관계 기능, 확장성, 트랜잭션 지원 및 JSON, XML, 복제, 및 고 가용성을 처리하기 위한 고급 기능을 강조했습니다.\n\n마지막으로, 테이블, 행, 열, 주 키 및 외래 키와 같은 데이터베이스와 관련된 중요 용어를 설명하여 이러한 핵심 개념에 대한 기본적인 이해를 제공했습니다.","ogImage":{"url":"/assets/img/2024-06-19-LearningPostgresAComprehensiveGuidetoPostgreSQLandDatabaseSystemsPart2_0.png"},"coverImage":"/assets/img/2024-06-19-LearningPostgresAComprehensiveGuidetoPostgreSQLandDatabaseSystemsPart2_0.png","tag":["Tech"],"readingTime":5},{"title":"NoSQL을 사용하는 이유와 SQL을 선택하는 3가지 이유","description":"","date":"2024-06-19 16:25","slug":"2024-06-19-3ReasonstoUseNoSQLoverSQL","content":"\n\n`<img src=\"/assets/img/2024-06-19-3ReasonstoUseNoSQLoverSQL_0.png\" />`\n\nNoSQL과 SQL 데이터베이스 중에서 결정하는 것은 매우 어려운 선택일 수 있습니다.\n\n둘 다 장단점이 있지만, 오늘은 NoSQL이 SQL에 비해 확실한 3가지 장점에 초점을 맞출 것입니다.\n\nDISCLAIMER ⚠️: SQL 데이터베이스에 대해 반대 의견을 제시하는 것이 아니며, 이 자리는 이러한 훌륭한 기술들에 대한 공식적인 토론을 촉발하는 것을 목표로 합니다.\n\n<div class=\"content-ad\"></div>\n\n그러니 더 이상 망설일 필요 없어요... 바로 시작해 봅시다!\n\n# 1. 샤딩\n\n샤딩은 데이터베이스의 데이터를 여러 서버에 분산시키는 기술입니다.\n\n샤딩은 수평 확장으로도 알려져 있어요.\n\n<div class=\"content-ad\"></div>\n\n대안은 수직 스케일링이며, 이는 데이터베이스를 업그레이드하여 스케일을 조정하는 기술로 단일 서버를 업그레이드하는 것을 의미합니다(예: RAM 추가, CPU 업그레이드).\n\nNoSQL 데이터베이스는 자연적으로 수평 스케일링(Sharding)에 더 적합하며, SQL 데이터베이스는 수직 스케일링에 더 적합합니다.\n\nNoSQL이 샤딩에 더 적합한 이유는 데이터가 자체를 포함하고 있기 때문에 데이터와 해당 테이블 간의 의존성이 없다는 점입니다.\n\nNoSQL에서 샤딩이 더 쉬운 이유를 이해하기 위해 다음 예제를 살펴보십시오:\n\n<div class=\"content-ad\"></div>\n\n마치 미디엄처럼 블로깅 플랫폼을 상상해보세요. 블로그 게시물 객체에는 댓글과 좋아요가 포함됩니다.\n\n![2024-06-19-3ReasonstoUseNoSQLoverSQL_1 이미지](/assets/img/2024-06-19-3ReasonstoUseNoSQLoverSQL_1.png)\n\nNoSQL에서는 이 객체가 다른 서버에 있는 다른 테이블에서 데이터를 조인할 필요 없이 그대로 저장됩니다.\n\n그러나 SQL 데이터베이스에서는 게시물, 댓글 및 좋아요를 조인해야 합니다.\n\n<div class=\"content-ad\"></div>\n\nSQL 데이터베이스가 샤드로 분할되면 어렵습니다. 서로 다른 서버에 위치한 코멘트와 조회수를 JOIN하는 것이 훨씬 어려워집니다.\n\n## 2. 규모별 성능\n\n간단한 데이터 몇 천 개를 다룬다면, NoSQL과 SQL 데이터베이스 간의 성능은 비슷해야 합니다.\n\n이제 수십억 개의 복잡한 관계형 데이터로 확장해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n이 상황에서 NoSQL은 여러 서버에 걸쳐 행을 분할할 수 있기 때문에 성능이 유지됩니다.\n\n게다가, NoSQL 쿼리는 데이터의 복잡성이 증가함에 따라 매우 잘 확장되며, 실제로 쿼리 시간은 비교적 일관적이고 빠릅니다.\n\n반면에 대규모 SQL 데이터베이스를 분할하는 것은 악몽이며, 수직 확장은 가능하지만 샤딩과 비교했을 때 비용 효율적이고 확장 가능하지 않습니다.\n\n더 나쁜 것은 SQL 스키마가 복잡해지면 여러 JOIN 쿼리가 쌓여 데이터베이스의 성능을 떨어뜨릴 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 3. 간편함\n\nNoSQL의 핵심 목적은 사전 정의된 구조를 준수하지 않고 대량의 비구조화된 데이터를 저장할 수 있게 하는 것입니다.\n\n결과적으로 NoSQL 데이터베이스는 SQL 데이터베이스처럼 엄격한 스키마를 강제하지 않습니다.\n\n이는 프로젝트가 시작되기 전에 스키마를 계획하는 시간을 적게 소비하게 되어 빠른 개발을 이끌어낼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n그래도 이렇게 계획을 다 해놓았음에도 불구하고, 데이터베이스 스키마를 언젠가는 더 다듬어야 할 가능성이 높습니다.\n\n기존 필드를 제거하거나 수정하여 SQL 스키마를 개선하는 것도 문제가 될 수 있습니다.\n\n기존 테이블에서 데이터를 업데이트된 스키마를 기반으로 새 테이블로 이관해야 하기 때문에 시간이 많이 소요될 수 있습니다.\n\n그러나 NoSQL을 사용하면 간단히 열을 제거하거나 수정할 수 있습니다!\n\n<div class=\"content-ad\"></div>\n\n## 결론\n\n우리는 SQL 대안 대신 NoSQL 데이터베이스를 사용하는 3가지 이유를 탐색했습니다.\n\nNoSQL의 간결함과 성능 장점은 대부분의 애플리케이션에 대해 유망한 선택지로 만들어 주며, 특히 빠르게 확장될 것으로 예상되는 애플리케이션에는 특히 적합합니다.\n\n이는 SQL 데이터베이스가 나쁘다는 것을 의미하는 것이 아니라, 데이터를 확장하는 방법을 다시 생각하고 작업에 적합한 도구를 선택하도록 고려해야 한다는 것을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n# 제휴사\n\n- All-in-One SaaS 프로젝트 템플릿\n- Figma 홈: 제가 모든 프로젝트에서 사용하는 UI 디자인 툴\n- Figma 프로페셔널: 당신이 필요로 하는 유일한 UI 디자인 툴\n- FigJam: 직관적인 다이어그램 및 아이디어 도출로 마음을 자유롭게 펼치세요\n- Notion: 제 인생 전반을 조직하는 데 사용되는 도구\n- Notion AI: ChatGPT보다 더 나은 AI 도구로 Notion 업무 흐름을 업그레이드하세요\n\n# 참고 자료\n\n- NoSQL vs RDBMS와 확장성\n- 샤딩(Sharding)이란 무엇인가요?","ogImage":{"url":"/assets/img/2024-06-19-3ReasonstoUseNoSQLoverSQL_0.png"},"coverImage":"/assets/img/2024-06-19-3ReasonstoUseNoSQLoverSQL_0.png","tag":["Tech"],"readingTime":3},{"title":"BigQuery 비용을 dbt 증분 모델로 100-200배 절감하기","description":"","date":"2024-06-19 16:23","slug":"2024-06-19-ReducingBigQueryCostsby100200xwithdbtIncrementalModels","content":"\n\n템퍼스(Temporus)에서 제 팀이 다루는 많은 모델은 크지만 \"빅 데이터\" 수준은 아닙니다. 보통 우리의 테이블은 수억 행 정도를 갖고 있으며, 가끔 10억 행을 넘기기도 하지만 성능에 대해 걱정할 만큼 자주 발생하지는 않습니다. 그러나 최근에 쿼리 중 하나가 2시간 후에 타임 아웃되었고, 한 테이블이 각 실행에 거의 9,000 슬롯 시간을 사용하고 있다는 것을 깨달았습니다.\n\n결국 데이터 로드 접근 방식을 변경하여 슬롯 사용량을 8,970시간에서 1.4시간으로 줄였습니다. 앞으로 몇 달 동안 다른 데이터 마트에서도 높은 성능의 점진적 모델을 전개할 예정입니다. 아래에서는 발생한 문제와 그 해결 방법에 대해 설명하겠습니다.\n\n제 팀이 다루는 대부분의 데이터는 구조화되지 않은 텍스트입니다. 해당 텍스트를 분류하기 위해 알고리즘을 실행하면 구조화되지 않은 텍스트의 하나의 항목이 이름이 지정된 entity(개체)의 수십 행으로 팽창될 수 있습니다.\n\n여러 해 동안 소스 데이터가 빅 데이터에서 Big Data로 커졌습니다. 수십억 건의 구조화되지 않은 텍스트 페이지와 그 결과로 더 많은 행의 테이블 데이터가 포함되어 있습니다. 우리가 많은 양의 데이터를 생성하고 있기 때문에, dbt에서 일반적으로 사용하는 완전 갱신(full-refresh) 방식에서 벗어나 점진적인 방식을 활용해야 한다고 가정했었으나, 그로 인한 결과를 고려하지 않았습니다.\n\n<div class=\"content-ad\"></div>\n\n우리가 dbt 제한 시간인 2시간을 초과하면서, 우리는 원시 예측 이후의 테이블을 완전히 잘라내고 로드하는 현재 방식을 다시 검토해야 했습니다.\n\n# 문제 해결 진단\n\n우리의 테이블을 생성하는 SQL 쿼리는 절대 효율적이 아니었습니다 — 문제가 심각해지기 전까지는 그것을 깨닫지 못했습니다. 하지만 어떤 부분이 비효율적이었을까요? 쿼리를 수정함으로써 성능을 개선할 수 있을까요? 아니면 우리 전체 접근법을 바꿔야 했을까요?\n\n아래는 우리가 실행하던 쿼리의 단순화된 버전입니다 —\n\n<div class=\"content-ad\"></div>\n\n```js\n{\n  config(\n    alias=\"ner_model\",\n    cluster_by=[\"label\"],\n    materialized=\"incremental\",\n    schema=\"warehouse\",\n    unique_key=[\"attachment_id\", \"page_index\", \"ent_char_start\"]\n  )\n}\n\nWITH\n\nlake AS (\n  SELECT\n    *\n  FROM { source('lake', 'source_ner_predictions') }\n  { if is_incremental() }\n    WHERE _predicted_at > (SELECT MAX(_predicted_at) FROM { this } )\n  { endif }\n),\n\n-- TMO 우선적 리터럴 형식을 위한 열 추가\nSELECT distinct\n  *\nFROM lake\nLEFT JOIN { ref('tmo') } AS tmo on tmo.key = lake.key\nLEFT JOIN { ref('other_table') } AS cw on lake.id = cw.id\r\n```\n\n위 쿼리에서 지연에 기여할 수 있는 별도 요소들은 다음과 같습니다:\n\n- 하나의 열에 Clustering.\n- 세 개의 열에 Compound Unique Keys.\n- DISTINCT 문.\n- 다양한 JOIN 연산.\n- WHERE 절 필터링이 있는 dbt is_incremental() 구문.\n\n이 중 주요 원인으로 눈에 띄는 항목이 있기를 바라며, 이미 dbt의 incremental 기능을 사용 중이라면 왜 쿼리에 시간이 오래 걸릴 수 있는지 살펴봅시다. 각각이 지연에 기여할 수 있는 이유를 살펴봅시다.\n\n<div class=\"content-ad\"></div>\n\n# 열 클러스터링\n\n클러스터링은 특정 열을 기준으로 표 내에서 정렬하는 것을 말합니다. 클러스터링을 통해 스캔/필터링 작업의 성능을 향상시킬 수 있습니다. 그러나 데이터베이스가 쓰기 작업 중에 클러스터 순서를 유지해야 하기 때문에 스캔이 발생하고 데이터의 처리 속도가 느려집니다. BigQuery 클러스터링은 쓰기 작업을 느리게 만들지만 읽기 작업을 빠르게 합니다.\n\n# 복합 고유 키 제약 조건\n\n고유 키는 BigQuery에서 기본적으로 지원되지 않는 기능입니다. 그러나 증분 모델을 사용하는 경우 dbt를 사용하여 이를 구성할 수 있습니다. 이후 dbt는 고유 키에 따라 삽입을 통해 주 키 제약 조건을 효과적으로 적용하는 코드를 컴파일합니다. dbt/BigQuery에서 고유 키의 문제는 증분 로드 시 고유 키를 가진 모든 새로운 데이터를 기존 데이터의 고유 키와 비교하기 위해 전체 테이블 스캔을 수행한다는 점입니다. 전체 테이블 스캔 및 따라서 dbt의 고유 키 제약 조건은 비용이 많이 발생하며 실행 시간이 급격히 느려집니다.\n\n<div class=\"content-ad\"></div>\n\n# DISTINCT 문\n\nBigQuery의 DISTINCT 문은 각 열에 대해 GROUP BY를 실행하는 것보다 훨씬 빠릅니다. 그러나 기본적으로 똑같은 일을 수행합니다. 그러나 DISTINCT는 여전히 전체 테이블 스캔을 호출합니다. BigQuery에서 각 행이 고유하다/유일하다는 것을 어떻게 더 확실히 할 수 있을까요?\n\n# 조인 및 WHERE 절\n\n조인에는 전체 테이블 스캔이 필요하며 WHERE 절도 필요합니다. 이 부분은 조금 더 명백하며 아마도 대부분의 사람들이 처음에 찾을 곳입니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 WHERE 절이 is_incremental() 블록 안에 중첩되어 있기 때문에 쿼리가 효율적일 것으로 생각했어요. 이론적으로 BigQuery는 런타임에서 기존 테이블을 스캔하고 is_incremental() 블록에서 들어오는 새로운 데이터의 작은 하위 집합과 비교하게 될 거에요!\n\n하지만 실제로는 이런 일이 벌어지고 있어요 —\n\n![이미지](/assets/img/2024-06-19-ReducingBigQueryCostsby100200xwithdbtIncrementalModels_0.png)\n\n쿼리의 다른 구성 요소 때문에 새로운 약 15백만 개의 행이 고유 키 절을 위반하는지 확인하기 위해 BigQuery가 거의 500억 개의 데이터 행을 계속 스캔하게 된 거예요. 새로운 접근 방식이 필요했답니다.\n\n<div class=\"content-ad\"></div>\n\n# 문제 해결하기\n\n성능에 영향을 미치는 조합을 찾기 위해 몇 가지 실험을 진행했지만,\n\n최종적으로는 내 실험 중 어느 것도 최종 솔루션을 밝혀내지 못했어요 — 단지 약간의 힌트만 주었을 뿐이에요.\n\n첫째, 우리의 실험은 파티션을 사용하는 것이 병합 작업을 돕는다는 점을 시사했습니다. 파티션을 사용하지 않은 실행의 성능이 느린 것을 볼 수 있었어요 (가장 오른쪽 및 상단 왼쪽 셀). 둘째, 데이터에서는 클러스터링이 성능을 저하시킬 수 있지만, 절대적으로 성능에 해를 끼치는 것은 아닌 것 같았어요. 마지막으로, BigQuery가 결정론적이지 않을 수 있다는 직관을 가졌는데, 여기서 일부 실행에서의 재현성 부족을 통해 그 대안을 확인했어요. 몇 가지 빠른 구글 검색 결과, LIMIT 절은 결정론적인 결과를 도출하지 않는다는 것을 알 수 있었고, 이것이 여기 일부 이상 현상을 설명해 줍니다.\n\n<div class=\"content-ad\"></div>\n\n최종적으로 이러한 실험들은 우리 문제를 즉각적으로 해결할 수 있는 내용을 밝혀내지 않았어요. 우리가 의존할 수 있는 단 하나의 해결책은 없었죠.\n\n문제에 대한 회고를 하며, 우리는 테이블의 모든 데이터가 고유해야 한다는 요구사항이 성능 저하의 주요 요인임을 깨달았어요. 우리는 그 요구사항을 의심한 적이 없었죠. 대신, 중복 데이터가 있을 것이라고 가정하고 제거해야 한다고 생각했어요.\n\n그러나 근본적으로 BigQuery는 삽입/갱신/삭제 트랜잭션을 다루는 것에 적합하게 설계되지 않았어요. BigQuery는 대량 데이터에 대해 완전 갱신만 또는 추가만을 사용할 때 가장 잘 작동하는 OLAP 데이터 웨어하우스에요. DML 할당량 제한이 없어진 것은 2020년에 이루어진 일이었구요, 그 전까지는 24시간 동안 1,000개의 DML 문만 실행할 수 있었어요. 우리의 초기 설계는 BigQuery를 데이터 웨어하우스가 아닌 트랜잭션 데이터베이스처럼 취급한 것이었어요.\n\n우리는 데이터 프로파일링을 수행했고, 중복 데이터가 발생한 것은 24시간 단위였음을 발견했어요. 그래서 우리는 전날 데이터를 수정할 필요가 없었고, 그 데이터를 스캔할 필요도 없었을지도 몰라요! 대부분의 경우, 우리는 unique_key라는 개념을 버리고, 대신에 가장 최신 데이터가 고유하다는 것을 확실히 하는 데 집중할 수 있었어요.\n\n<div class=\"content-ad\"></div>\n\n# 솔루션 구현\n\n우리의 솔루션은 세 가지 주요 구성 요소가 있었습니다:\n\n- 타임스탬프를 기반으로 파티셔닝을 강제하는 것으로, 데이터의 각 날짜를 파티션에 저장했습니다. 이렇게 하면 최신 데이터 로드에만 집중하면 되므로 효율적입니다.\n- dbt의 is_incremental() 매크로를 활용하여 최신 데이터만 가져오도록 합니다.\n- 전체 테이블 스캔을 유발하는 요소를 제거하고, 작업 중인 특정 파티션에 대해 스캔을 집중합니다.\n\n그 결과는 다음과 같았습니다 —\n\n<div class=\"content-ad\"></div>\n\n```js\n{\n  config(\n    cluster_by=[\"label\"],\n    materialized=\"incremental\",\n    partition_by={\n        \"field\": \"_rwde_predicted_at\",\n        \"data_type\": \"timestamp\",\n        \"granularity\": \"day\"\n    },\n    incremental_strategy = 'insert_overwrite'\n  )\n}\n\nlake_base AS (\n  SELECT\n    primary_key,\n    other_columns,\n    MAX(_rwde_predicted_at) OVER (PARTITION BY primary_key) as _rwde_predicted_at\n  FROM { source('lake', 'ner_model') }\n  { if is_incremental() }\n      WHERE _rwde_predicted_at > (select max(_rwde_predicted_at) from {this})\n  { endif }\n)\n\nSELECT DISTINCT * from lake_base\n```\n\n여기 중요한 구성 요소가 있습니다:\n\n- 클러스터링 — 여전히 downstream 중요성을 향상시키는 라벨이라는 컬럼을 기준으로 클러스터링을 원합니다. 기억하세요, 클러스터링은 쓰기 및 삽입 작업에는 성능에 악영향을 미치지만 읽기 작업에는 큰 도움이 됩니다. 새로운 증분 접근 방식을 사용하여 처리되는 데이터 양을 줄였기 때문에 이제 런타임을 크게 늦출 필요 없이 클러스터화 할 수 있습니다.\n- 분할 — 일별로 분할함으로써 예측값을 매일 다른 테이블로 분리하게 됩니다. 테이블을 분할하고 dbt에 기본으로 내장된 is_incremental() 플래그를 사용하여 dbt가 작은 데이터 서브셋만 읽고 스캔하도록 강제할 수 있으며 이전 데이터를 다시 처리하지 않을 수 있습니다.\n- 증분 전략 — dbt에는 여기 및 여기에 증분 전략에 대한 훌륭한 문서가 있지만 insert_overwrite를 선택함으로써 전체 파티션을 교체하여기존 파티션에 스캔 및 병합하지 않도록 합니다.\n\n참고: 매일 데이터를 한 번만 가져오고 따라서 매일 예측값을 생성하는 것입니다. dbt labs의 Jerco가 가르쳐준 것처럼 —\n\n<div class=\"content-ad\"></div>\n\n인크리멘탈 전략은 구현하기 전에 면밀히 검토하는 것이 중요합니다. 아래 다이어그램은 insert_overwrite 전략이 무엇을 하는지 보여줍니다 —\n\n![이미지](/assets/img/2024-06-19-ReducingBigQueryCostsby100200xwithdbtIncrementalModels_1.png)\n\n위 코드를 구현하면 이 특정 테이블의 BigQuery 스캔 시간이 32,295,356,894 ms (8,970 슬롯 시간)에서 5,024,974 ms (1.4 슬롯 시간)으로 줄어듭니다!!\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n빅쿼리 비용은 슬롯 사용량을 기준으로 계산됩니다. 이는 일반적으로 쿼리 실행 시간과 밀접한 관련이 있습니다. 여기에는 일부 쿼리가 병렬 처리되어 더 많은 슬롯 시간을 사용할 수 있는 경우가 있는데요. 1시간 쿼리가 30분 쿼리보다 비싼 것은 아니지만, 대부분의 경우 그렇습니다.\n\n빅쿼리의 사용 방식으로 인해 매일 테이블을 스캔하여 변경된 행이나 추가된 행을 확인하는 것보다 전체 데이터를 삭제한 후 다시 로드하는 것이 훨씬 저렴할 수 있습니다. 이는 직감과 반대되는 방식일 수 있습니다. 새로운 데이터를 삽입하지 않더라도, 생성된 쿼리가 전체 테이블 스캔보다 성능이 우수하다면 해당 전체 테이블 스캔을 피해 비용을 절약할 수 있습니다.\n\n우리의 경우, 비용을 줄일 것으로 가정하고 문제 모델을 점진적으로 전환했지만, 실제로는 전체 새로고침이 더 저렴했을 것입니다. 스캔을 최소화하면서 시간은 걸리지만, 비용은 더 저렴했을 것입니다. 그러나 우리의 새로운 접근 방식은 더 빠르고 저렴하기까지 합니다. 커피 한 잔 사는 것보다 더 나은 선택입니다.\n\n빅쿼리 최적화에 더 관심이 있다면, 제가 작성한 이전 기사를 확인해보세요. 몇 가지 빠른 수정 사항을 강조하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# Stackademic\n\n끝까지 읽어 주셔서 감사합니다. 가기 전에:\n\n- 작가를 칭찬하고 팔로우해 주시면 감사하겠습니다! 👏\n- 다음 계정을 팔로우해 주세요: X | LinkedIn | YouTube | Discord\n- 다른 플랫폼도 방문해 주세요: In Plain English | CoFeed | Venture","ogImage":{"url":"/assets/img/2024-06-19-ReducingBigQueryCostsby100200xwithdbtIncrementalModels_0.png"},"coverImage":"/assets/img/2024-06-19-ReducingBigQueryCostsby100200xwithdbtIncrementalModels_0.png","tag":["Tech"],"readingTime":8},{"title":"파이썬에서 SQLite3를 활용하여 시작하기 테이블 생성 및 행 추출하기","description":"","date":"2024-06-19 16:21","slug":"2024-06-19-GetStartedwithSQLite3inPythonCreatingTablesandFetchingRows","content":"\n\n<img src=\"/assets/img/2024-06-19-GetStartedwithSQLite3inPythonCreatingTablesandFetchingRows_0.png\" />\n\n# 개요\n\n- 소개 — SQLite 및 SQLite3이란 무엇인가?\n- 첫 번째 SQLite 데이터베이스 만들기\n- 커넥터 및 커서\n- 테이블 생성\n- 데이터베이스에 행 삽입\n- 데이터베이스에서 행 가져오기\n- 마무리\n\n<div class=\"content-ad\"></div>\n\n현대 IT 전문가들에게 가장 중요한 핵심 기술 중 하나는 구조화된 질의 언어(SQL)입니다. 이는 관계형 데이터베이스와 상호 작용하기 위해 사용되는 선언형 언어입니다. 데이터 엔지니어와 분석가들은 주로 SQL을 사용하여 데이터 파이프라인을 실행하고 데이터 내에서 유용한 관계를 조사합니다.\n\nSQL 경험이 없을 때는 PostgreSQL 또는 MySQL과 같은 일반적인 데이터베이스 관리 시스템(DBMS)으로 넘어가는 것이 다소 무섭습니다. 다행히 SQLite는 SQL 기초를 배우기에 좋은 옵션입니다. 별도의 서버 프로세스가 없기 때문에 설정하기 쉽고 관리하기 쉽습니다. 따라서 데이터 엔지니어와 데이터 분석가들은 일반적으로 SQLite보다 다른 데이터베이스 관리 시스템을 사용하지만 SQL을 배우기 좋은 장소입니다. 실제로 SQLite는 세계에서 가장 널리 사용되는 DBMS입니다!\n\n게다가, Python 라이브러리인 sqlite3은 SQLite와 상호 작용하기 위한 간단한 인터페이스입니다. 이 블로그 포스트에서는 SQLite와 sqlite3 라이브러리를 사용하여 두 가지 주요 개념을 배웁니다:\n\n- CREATE TABLE, INSERT INTO, 그리고 SELECT — FROM과 같은 가장 기본적이고 유용한 SQL 명령어 사용 방법.\n- 어떻게 프로그래밍 언어(우리의 경우 Python)를 사용하여 관계형 데이터베이스와 상호 작용할 수 있는지.\n\n<div class=\"content-ad\"></div>\n\nSQLite 데이터베이스를 설정하고 Python에서 sqlite3를 사용하여 데이터베이스 연결을 만들고 데이터범들을 삽입/검색할 것입니다. SQL 전문가가 되는 것이 목표는 아니고, SQL이 어떻게 사용되는지 보고 시작하기 위한 몇 가지 기본 명령을 배우는 것입니다. 더 배우고 싶다면, 이 블로그 포스트와 같이 시작하는 8개의 무료 YouTube 비디오가 있습니다.\n\n# 우리 첫 번째 SQLite 데이터베이스 생성\n\nSQLite 공식 웹페이지에서 SQLite 다운로드에 대한 정보를 찾을 수 있습니다. 그러나 대부분의 경우 SQLite는 이미 대부분의 기기에 포함되어 있으므로 필요하지 않습니다. 또한 Python에서 sqlite3 라이브러리가 필요하지만, 이것은 표준 라이브러리에 포함되어 있고 대부분의 Python 배포판에 포함되어 있습니다. 따라서 대부분의 경우 설치할 것이 없을 것입니다 😃\n\n이미 모든 것이 설치되어 있는지 확인하려면 새로운 Python 파일을 열고 다음 단일 명령어를 작성하십시오:\n\n<div class=\"content-ad\"></div>\n\n```js\r\nimport sqlite3\r\n```\r\n\r\n만약 위의 파일이 잘 실행된다면, SQLite와 Python 라이브러리 sqlite3이 모두 설치된 것입니다. 준비가 된 것입니다!\r\n\r\nimport 단계 이후, 데이터베이스에 연결을 생성해야 합니다. 이는 sqlite3 라이브러리의 connect() 함수를 사용하여 수행됩니다:\r\n\r\n```js\r\n# 데이터베이스에 연결 생성\r\nconnection = sqlite3.connect(\"music.db\")\r\n```\n\n<div class=\"content-ad\"></div>\n\nconnect() 함수에 전달된 인수는 데이터베이스의 이름이 될 것입니다. 아직 데이터베이스가 없기 때문에, 이는 우리를 위해 새 데이터베이스를 단순히 생성할 것입니다. 이제 Python 파일을 실행하면, 작업 중인 디렉토리에 music.db라는 새 파일이 나타날 것입니다. 이것이 우리의 데이터베이스입니다!\n\n관계형 데이터베이스는 여러 테이블로 구성됩니다. 이것이 처음이시라면, 이를 엑셀 시트 모음과 같다고 생각할 수 있습니다. 이는 관계형 데이터베이스가 얼마나 강력한지를 과소평가하지만, 처음에는 좋은 정신적 모델입니다.\n\n연결 객체를 만든 후에는 커서를 만들어야 합니다. 커서는 데이터베이스에 대해 SQL 명령을 실행할 수 있습니다. 이를 만들기 위해, 연결 객체에서 .cursor() 메서드를 사용합니다.\n\n```js\n# 커서 만들기\ncursor = connection.cursor()\n```\n\n<div class=\"content-ad\"></div>\n\n현재 변수 cursor에는 데이터베이스에서 데이터를 삽입하고 가져올 수 있는 커서 개체가 포함되어 있습니다. 지금까지 다음 코드를 작성했어야 합니다:\n\n```js\nimport sqlite3\n\n# 데이터베이스 연결 생성\nconnection = sqlite3.connect(\"music.db\")\n\n# 커서 생성\ncursor = connection.cursor()\n```\n\n# 테이블 생성\n\n우선 데이터베이스에 테이블이 필요합니다. 우리는 80년대 노래를 나타내는 데이터를 사용할 것입니다. 커서 개체에서 execute() 메서드를 호출하여 SQL 문을 실행할 수 있습니다. 우리가 배울 첫 번째 문은 CREATE TABLE 문입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 테이블 만들기\ncursor.execute(\"CREATE TABLE IF NOT EXISTS songs(name, artist, album, year, duration)\")\n```\n\n위 명령어에서 볼 수 있듯이, 우리는 노래라는 테이블을 생성합니다. 이 테이블은 이름, 아티스트, 앨범, 연도 및 재생 시간 다섯 가지 열을 가지고 있습니다. 옵션인 IF NOT EXISTS는 테이블이 이미 존재하지 않을 경우에만 테이블을 생성하도록 보장합니다. 테이블이 이미 존재하는 경우에는 명령이 실행되지 않습니다.\n\n지금은 테이블이 비어 있지만, 스키마는 명확합니다. 우리는 각 노래의 이름, 아티스트, 앨범, 연도 및 재생 시간과 같은 관련 정보를 기록하는 테이블을 설정하고 있습니다. 곧 이 테이블을 다양한 노래를 나타내는 행으로 채워 넣을 것입니다.\n\nPython 파일을 실행한 후에 당신의 첫 번째 생각은 현재 디렉토리에 있는 music.db 데이터베이스 파일을 열어서 어떤 일이 발생했는지 조사하는 것이 될 것입니다. 그러나 music.db 파일의 정보는 이렇게 직접 열어서 확인할 목적으로는 설정되지 않았습니다. 여기에 있는 정보는 열람하려는 목적으로는 표시되지 않은 형식으로 보일 것입니다. 우리는 데이터베이스의 정보를 읽기 위해 더 많은 SQL 명령어를 작성해야 할 것입니다.\n\n<div class=\"content-ad\"></div>\n\nSQL 명령어를 처음 배우셨군요! SQL 명령어와 Python 라이브러리 sqlite3을 구분해야 합니다. SQL 명령어는 CREATE TABLE ... 이라는 문장 뿐입니다. 연결(connection)과 커서(cursor)는 데이터베이스와 상호 작용하는 데 사용되는 Python 객체입니다.\n\n# 데이터베이스에 행 삽입하기\n\n이제 단일 테이블이 있는 데이터베이스를 가지게 되었어요. 하지만 테이블은 비어 있습니다! 데이터베이스를 유용하게 활용하기 위해서는 데이터가 필요합니다. 이제 SQL 키워드 INSERT INTO을 사용하여 데이터를 데이터베이스 테이블에 삽입하는 방법을 살펴봅시다. 먼저 각 노래를 정보 튜플로 나타낸 노래 목록을 만듭니다:\n\n```js\n# 노래 테이블의 행\nsongs = [\n    (\"I Wanna Dance with Somebody (Who Loves Me)\", \"Whitney Houston\", \"Whitney\", 1987, 291),\n    (\"Dancing in the Dark\", \"Bruce Springsteen\", \"Born In The U.S.A.\", 1984, 241),\n    (\"Take On Me\", \"a-ha\", \"Hunting High and Low\", 1985, 225),\n    (\"Africa\", \"TOTO\", \"Toto IV\", 1982, 295),\n    (\"Never Gonna Give You Up\", \"Rick Astley\", \"Whenever You Need Somebody\", 1987, 213)\n]\n```\n\n<div class=\"content-ad\"></div>\n\n각 튜플에는 노래 테이블의 열과 대응하는 다섯 부분이 있음을 확인할 수 있습니다. 첫 번째 노래에 대해 다음과 같습니다:\n\n- 이름: I Wanna Dance with Somebody (Who Loves Me)\n- 아티스트: Whitney Houston\n- 앨범: Born In The U.S.A.\n- 발매 연도: 1987년\n- 재생 시간 (초): 291\n\n이제 준비된 행이 있으니, 이를 music.db 데이터베이스의 songs 테이블에 삽입해야 합니다.\n\n이 작업을 수행하는 한 가지 방법은 한 번에 한 개의 행을 테이블에 삽입하는 것입니다. 다음 코드는 첫 번째 노래를 테이블에 삽입합니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n# 데이터베이스에 단일 값을 삽입합니다\ncursor.execute(\"INSERT INTO songs VALUES(?, ?, ?, ?, ?)\", songs[0])\nconnection.commit()\n```\n\n여기서는 노래 목록에서 첫 번째 노래를 선택하여 이를 테이블 songs에 삽입합니다. 테이블을 삽입할 때는 SQL 명령어 INSERT INTO table VALUES를 사용합니다. 마지막으로 .commit() 메소드를 사용하여 트랜잭션이 완전히 완료되도록 합니다.\n\n이 접근법을 Python의 for 루프로 반복하면 다음 코드를 사용하여 테이블에 모든 행을 삽입할 수 있습니다:\n\n```js\n# 루핑을 통해 모든 값들을 테이블에 삽입합니다\nfor song in songs:\n    cursor.execute(\"INSERT INTO songs VALUES(?, ?, ?, ?, ?)\", song)\n    connection.commit()\n```\n\n<div class=\"content-ad\"></div>\n\n이곳에는 새로운 SQL 명령이 없습니다. 노래 테이블에 모든 행이 삽입되도록 하는 몇 가지 Python 논리만 있습니다.\n\n위 방법의 단점은 많은 행을 삽입해야 할 때 상당히 느리다는 것입니다. 우리의 예제에서는 노래가 몇 곡뿐이기 때문에 모든 것이 빠릅니다. 그러나 관계형 데이터베이스의 테이블에는 수백만 또는 심지어 수십억 개의 행이 들어갈 수 있습니다. 그런 경우에는 Python에서 루핑이 삽입 작업을 느리게 만들 수 있습니다.\n\n이를 해결하기 위한 방법은 한꺼번에 모든 행을 삽입하는 것이며, 순환하지 않는 것입니다. 지금까지 사용한 .execute() 메서드 대신 커서 객체의 .executemany() 메서드를 사용하여이 작업을 수행할 수 있습니다. 다음 코드는 모든 행을 한 번에 삽입하는 방식입니다:\n\n```js\n# 일괄 처리 방식으로 한꺼번에 모든 값을 삽입할 수 있음\ncursor.executemany(\"INSERT INTO songs VALUES(?, ?, ?, ?, ?)\", songs)\nconnection.commit()\n```\n\n<div class=\"content-ad\"></div>\n\n데이터베이스 music.db에 테이블 songs가 있고 몇 개의 행이 이미 삽입되어 있습니다. 지금까지 작성한 코드(주석 제외)는 다음과 같습니다:\n\n```js\nimport sqlite3\n\nsongs = [\n    (\"I Wanna Dance with Somebody (Who Loves Me)\", \"Whitney Houston\", \"Whitney\", 1987, 291),\n    (\"Dancing in the Dark\", \"Bruce Springsteen\", \"Born In The U.S.A.\", 1984, 241),\n    (\"Take On Me\", \"a-ha\", \"Hunting High and Low\", 1985, 225),\n    (\"Africa\", \"TOTO\", \"Toto IV\", 1982, 295),\n    (\"Never Gonna Give You Up\", \"Rick Astley\", \"Whenever You Need Somebody\", 1987, 213)\n]\n\nconnection = sqlite3.connect(\"music.db\")\n\ncursor = connection.cursor()\n\ncursor.execute(\"DROP TABLE IF EXISTS songs\")\n\ncursor.execute(\"CREATE TABLE IF NOT EXISTS songs(name, artist, album, year, duration)\")\n\ncursor.executemany(\"INSERT INTO songs VALUES(?, ?, ?, ?, ?)\", songs)\nconnection.commit()\n```\n\n자세히 살펴보면 코드에 새 줄을 sneaked했습니다. 이 줄은 SQL 명령문 DROP TABLE IF EXISTS songs를 실행하는 줄입니다. 위의 코드를 실행하면 먼저 테이블이 있으면 삭제한 다음 다시 만듭니다.\n\n이렇게 하면 다른 결과가 나오지 않도록 합니다. 위의 Python 파일을 실행하면 데이터베이스 상태를 재설정하고 다음 섹션에서 동일한 결과를 얻어야 합니다. 프로덕션 시스템에서 이런 문을 사용하면 행을 삽입할 때마다 전체 테이블이 다시 만들어져 매우 비용이 발생할 수 있습니다. 그러나 여기서 하는 실험에는 적합합니다.\n\n<div class=\"content-ad\"></div>\n\n# 데이터베이스에서 행을 불러오는 중\n\n![이미지](/assets/img/2024-06-19-GetStartedwithSQLite3inPythonCreatingTablesandFetchingRows_1.png)\n\n이제 데이터베이스에서 행을 다시 불러오는 시간이에요. 이를 위해 SQL 키워드 SELECT와 FROM을 사용할 거에요. 데이터베이스에서 한 곡을 가져오는 것부터 시작해볼까요:\n\n```js\n# 한 곡 가져오기\nsingle_song = cursor.execute(\"SELECT * FROM songs\").fetchone()\nprint(single_song)\n```\n\n<div class=\"content-ad\"></div>\n\n보통은 SQL 문을 실행하기 위해 커서 개체에 .execute() 메서드를 사용합니다. 문 SELECT * FROM songs는 데이터베이스에서 모든 열과 모든 행을 불러옵니다. 그러므로 이는 모든 것을 우리에게 제공합니다. 그러나 sqlite3에서 .fetchone() 메서드를 사용하여 이러한 행 중 하나만 불러옵니다. 이렇게 하면 우리가 Python 파일을 실행할 때 하나의 노래만 출력하게 됩니다.\n\n와일드카드 심볼 *을 사용하여 모든 열을 다시 검색했습니다. 일부 열만 필요한 경우 다음과 같이 지정할 수 있습니다:\n\n```js\n# 하나의 노래의 이름과 아티스트 열만 불러오기\nname_and_artist = cursor.execute(\"SELECT name, artist FROM songs\").fetchone()\nprint(name_and_artist)\n```\n\n.fetchone() 메서드 외에도 .fetchmany(number_of_rows) 및 .fetchall() 메서드를 사용하여 더 많은 행을 가져올 수 있습니다. 다음 코드는 .fetchall() 메서드를 사용하여 모든 노래를 선택합니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n# 모든 행과 열을 다시 가져오기\nfull_songs = cursor.execute(\"SELECT * FROM songs\").fetchall()\nprint(full_songs)\n```\n\n파이썬으로 정보를 가져온 후에는 유용한 통찰을 얻기 위해 표준 파이썬 논리를 사용할 수 있습니다. 다음 코드는 데이터베이스의 모든 노래에 대한 평균 재생 시간을 찾아내는 예시입니다.\n\n```js\n# Python 논리로 평균 재생 시간 얻기\naverage_duration = 0\nfor song in full_songs:\n    average_duration += song[-1]\naverage_duration = average_duration / len(full_songs)\nprint(f\"평균 80년대 노래 재생 시간은 {int(average_duration // 60)}분 {int(average_duration % 60)}초 입니다.\")\n```\n\n우리는 조금 많이 오고 간다고 생각할 수도 있습니다. 이미 파이썬 스크립트에 원본 노래 목록이 있으므로, 왜 먼저 이를 데이터베이스에 삽입한 다음 다시 검색해야 할까요? 이것은 튜토리얼이 조금 인위적인 부분입니다. 실제로 데이터베이스로부터 데이터를 삽입하고 검색하는 파이썬 스크립트가 동일하지는 않습니다. 데이터베이스에 데이터를 삽입하는 여러 파이썬 스크립트(또는 다른 인터페이스)가 있을 수도 있습니다. 따라서 데이터베이스에서 데이터를 가져오고 평균 재생 시간을 계산하는 것이 이 정보를 얻는 유일한 방법일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n마지막으로, 작업을 마무리하기 전에 데이터베이스 연결을 닫아야 합니다. 이전에 connection() 함수로 데이터베이스에 연결을 열었습니다. 이를 닫지 않으면 계속 열려 있어 복잡한 애플리케이션에서 성능 및 지속성 문제를 일으킬 수 있습니다. 데이터베이스 연결이 항상 닫혀 있는지 확인하는 것이 좋은 관행입니다. sqlite3에서는 연결 객체의 .close() 메서드를 사용하여 이 작업을 수행할 수 있습니다:\n\n```js\n# 연결 닫기\nconnection.close()\n```\n\n다음 코드는 우리가 수행한 모든 작업을 보여줍니다:\n\n```js\nimport sqlite3\n\nsongs = [\n    (\"I Wanna Dance with Somebody (Who Loves Me)\", \"Whitney Houston\", \"Whitney\", 1987, 291),\n    (\"Dancing in the Dark\", \"Bruce Springsteen\", \"Born In The U.S.A.\", 1984, 241),\n    (\"Take On Me\", \"a-ha\", \"Hunting High and Low\", 1985, 225),\n    (\"Africa\", \"TOTO\", \"Toto IV\", 1982, 295),\n    (\"Never Gonna Give You Up\", \"Rick Astley\", \"Whenever You Need Somebody\", 1987, 213)\n]\n\nconnection = sqlite3.connect(\"music.db\")\n\ncursor = connection.cursor()\n\ncursor.execute(\"DROP TABLE IF EXISTS songs\")\n\ncursor.execute(\"CREATE TABLE IF NOT EXISTS songs(name, artist, album, year, duration)\")\n\ncursor.executemany(\"INSERT INTO songs VALUES(?, ?, ?, ?, ?)\", songs)\nconnection.commit()\n\nfull_songs = cursor.execute(\"SELECT name, artist, album, year, duration FROM songs\").fetchall()\n\naverage_duration = 0\nfor song in full_songs:\n    average_duration += song[-1]\naverage_duration = average_duration / len(full_songs)\nprint(f\"평균 80년대 노래의 재생 시간은 {int(average_duration // 60)}분 {int(average_duration % 60)}초 입니다.\")\n\nconnection.close()\n```\n\n<div class=\"content-ad\"></div>\n\n# 마무리\n\n![이미지](/assets/img/2024-06-19-GetStartedwithSQLite3inPythonCreatingTablesandFetchingRows_2.png)\n\n이 블로그 포스트가 SQL 명령어와 파이썬의 sqlite3 라이브러리에 대해 이해하는 데 도움이 되었기를 바랍니다. 만약 AI, 데이터 과학, 또는 데이터 엔지니어링에 관심이 있다면 저를 팔로우하거나 LinkedIn에서 연락을 취해 주세요.\n\n제 글이 마음에 드셨나요? 더 많은 콘텐츠를 보시려면 제 다른 글도 확인해보세요!\n\n<div class=\"content-ad\"></div>\n\n- 데이터 과학자로 성공하기 위해 필요한 소프트 스킬\n- 데이터 과학자로서 고품질 파이썬 쓰는 법\n- 아름다운 타입 힌트를 활용하여 파이썬 코드 현대화하기\n- 파이썬으로 결측값 시각화는 놀랍게 쉬워요\n- PyOD로 파이썬에서 이상 감지/이상 값 탐지 소개하기 🔥","ogImage":{"url":"/assets/img/2024-06-19-GetStartedwithSQLite3inPythonCreatingTablesandFetchingRows_0.png"},"coverImage":"/assets/img/2024-06-19-GetStartedwithSQLite3inPythonCreatingTablesandFetchingRows_0.png","tag":["Tech"],"readingTime":11},{"title":"데이터 과학 채용 면접이 오지 않나요 시각성 문제가 있을 수 있습니다","description":"","date":"2024-06-19 16:19","slug":"2024-06-19-NotGettingDataScienceJobInterviewsYouHaveAVisibilityProblem","content":"\n\n취업중이신가요? 무료 5페이지 프로젝트 아이디어 안내서를 활용하여 개인 프로젝트를 개발하여 경쟁력을 확보하세요.\n\n다음에 데이터 과학 직무에 지원할 때 모든 것을 시도해봤다고 생각할 때, 기억해 두세요. 제가 한테서 일했던 사람 중 한 명이 작업 후... 웃기죠. 패러디 랩으로 직장을 구한 사람이 있었어요. 그 후보자는 이민엠이나 칠디시 감비노와 협업을 희망하지는 않지만 작업은 저명한 인턴쉽을 획들하는 데에 성공했어요. 이 작업은 10,000명 이상의 지원자들 중에서 선발되었습니다.\n\n엔터테인먼트 제작이라는 엄청난 경쟁이 일상인 세계에 파도를 쳐들기 위해 산업 경험(혹은 엘리트 컨택트)이 부족하다고 인식한 지원자 Jake는 도전적인 프로젝트에 착수했어요. 그는 Blackalicious의 \"알파벳 에어로빅\"을 샷-포-샷으로 재창작할 것이라고 결심했습니다.\n\n2014년 다니엘 래드클리프가 쇼에서 이 노래를 부르는 모습을 봤었기 때문에 이 노래를 선택한 게 아니었습니다.\n\n<div class=\"content-ad\"></div>\n\n물론, 예기치 못한 일이 있었습니다. 제이크는 쇼 직원들의 일에 관심을 갖고 가사를 재작성했습니다.\n\n그가 관례적인 인터뷰 요청을 기다리는 동안, 예상치 못한 반응을 받았습니다; 주인공 지미 팰런이 직접 그를 고용했습니다.*\n\n이 이야기를 비관적으로 해석하면: 그가 극단적인 조치를 취하지 않았다면 그 직업을 얻지 못했을 것이라는 겁니다.\n\n저는 보다 낙천적인 태도를 취하여, 그가 목적을 가지고 창의적이고, 무엇보다 중요한 점은 그가 노력했다는 것을 생각합니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 과학 직무에 지원하는 모든 사람은 Jake의 예를 통해 자신만의 독특한 명함을 활용해 볼 수 있고, 당신을 뒷받침하는 업무가 없는 상태에서 어떤 대담한 내기도 무례하게 들릴 수 있다는 것을 깨닫게 될 거예요.\n\n![Jake's Example](/assets/img/2024-06-19-NotGettingDataScienceJobInterviewsYouHaveAVisibilityProblem_0.png)\n\n솔직히 말해서, 제이크와 저는 2017년 가을 Tonight에서 잠깐 겹쳤어요.\n\n만약 10,000명의 지원자 풀이 당신에게 와 닿는다면, 아마도 LinkedIn에서 많은 시간을 취업 지원에 사용했고 아마도 이 프롬프트를 본 적이 있을 거예요: \"다른 지원자 100명 (또는 1,000명 또는 10,000명)과 어떻게 비교되는지 확인해 보세요.\"\n\n<div class=\"content-ad\"></div>\n\n거의 모든 삶의 측면에서 경쟁 분석에 대한 자리가 있지만, 구직자가 너무 많은 정보(TMI)를 제공하는 것도 있습니다.\n\n그런 측정치가 후보자들의 유효성에 대해 투명하지 않다는 것도 말이죠. 기본 자격 요건을 충족하는 사람은 누구이며, 실제로 회사 사이트에 방문했는지, 그리고 “지금 신청”을 한 사람은 누군지, 아마도 가장 중요한 것은 봇인가요.\n\n좋은 소식은 대부분의 직업의 기본 자격 요건을 충족했다면 좋은 상태에 있습니다. 하지만, 당신이 그러하듯이, 특히 입문자 포지션은 경험을 얻기 위해서는 경험이 필요한 악순환입니다.\n\n특히 데이터 과학 시장이 점점 더 빡빡해지고 있는 상황에서, 혼자서도 존중받을만한 지원자로 자리잡기 위해서는 자기 인식과 균형이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n근무자 유리한 채용 주기에서는, 단지 무슨 일을 잘하거나 하고자하는 데 있어 개인적으로 뛰어나다는 것만으로 충분하지 않습니다.\n\n이제 공개적으로 활동해 보세요. 커뮤니티의 중심 지점에 참여하거나 시작하여, 자신을 채용 관리자와 채용 담당자들에게 눈에 띄게 만들어 주세요. 이를 통해 이력서를 던지거나 자동 거부 이메일을 발송하고 있는 채용 담당자와 차별화되어 보일 수 있습니다.\n\n명확하게 이렇게 말하고 싶어요. 더 많은 시야를 확보해야 한다고 할 때, LinkedIn 제목을 넣어서 프로필의 순위를 높여야 하는 의미가 아닙니다. 솔직히 말하자면, 당신이 자격이 없는 일이나 관심 없는 직무에 대해 연락을 받는 것은 원하는 것이 아닐 것입니다.\n\n그것은 모두의 시간을 낭비하는 일이 될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n졸업 후 하루만에 링크드인에 먼저 자리를 구할 것을 의미하지는 않아요. 데이터 과학 분야의 적임자로서 인식받기 위한 다음 단계는 다음과 같아요:\n\n- 데이터 과학 분야에서 돋보이는 독특한 기술 또는 가치 제안을 식별해 보세요.\n- 능력을 과시하는 (이 부분에 대해 의도적으로 개방적으로 말하고 있어요) 어떤 일을 하고, 그 결과물을 명함처럼 활용해 보세요.\n- 관리자들이 자주 방문할 것으로 예상되는 커뮤니티나 포럼을 식별하고 연결하세요. 링크드인도 좋지만, 매니저들이 머물 수 있는 많은 다른 플랫폼이 있어요. 예를 들어, GitHub, Glassdoor 및 Medium 등이 있어요.\n- 자신의 명함을 과시하세요.\n- 과정을 설명하세요.\n- 귀하의 작업에 관심 있는 사람들과 연락을 취하세요 - 아마도 인재영입자/매니저들일 거에요.\n\n## 가장 이상적인 상황\n\n<div class=\"content-ad\"></div>\n\nML 모델이 마음에 들지 않지만 대시보드 디자인에 큰 만족을 느끼는군요. 그러나 모든 사람들이 대시보드를 만들고 싶어하며 또 다른 Titanic 생존자의 막대 플롯을 보고 싶어하지 않는다고요.\n\n우연히도, 데이터 과학 숙제를 하면서 MLB를 많이 보고 있었습니다. 조금씩 통계 애호가가 되었습니다. 심지어 근처의 MLB 팀에서 데이터 분석가로 지원하는 것도 고려해봤습니다 (아마 뉴욕에 여러 팀이 있을 것으로 생각됩니다).\n\n동시에, Michael Lewis의 Moneyball을 읽고 있었고 Billy Beane가 섭외한 선수들에 대한 통계를 파헤치고 있습니다.\n\n수업에 지루해지자, 그 시대의 오클랜드 에스를 위한 스카우팅 대시보드를 만들어 사이드 프로젝트를 시작하기로 결심했습니다. 몇 주 동안 노력하여 대시보드를 만들어 자랑스러워하며 LinkedIn에 공유하기로 결정했습니다.\n\n<div class=\"content-ad\"></div>\n\nReddit의 r/dataisbeautiful(2000만 명 이상의 데이터 시각화 커뮤니티) 커뮤니티와 공유하도록 권장하는 연결 메시지가 도착했어요. 초기 대시보드에 대해 게시하는 것 외에도 Medium에서 접근 방법과 얻은 교훈에 대해 설명한 두 부분으로 된 시리즈를 작성했어요. 이 모든 것을 r/dataisbeautiful 게시물에 링크해놓았어요.\n\n잠복 중인 채용 매니저가 연락을 해와서 면접에 초대해 주었어요.\n\n비록 이 극적인 시나리오가 그렇게 매끄럽게 일어나는 경우는 드물겠지만, 표면상으로는 우연한 일들처럼 보이는 것에 드는 수많은 시간과 노력을 강조하고 싶어요.\n\n먼저, 우리 가정의 후보는 MLB 게임을 100시간 이상 지켜가며 데이터를 세심하게 기록하고 분석했어요. 그들은 인상적인 도메인 지식을 발전시키는데 그치지 않고 동시에 주도적으로 자신만의 데이터를 확보했습니다.\n\n<div class=\"content-ad\"></div>\n\n그들은 수업 외에도 관련성 있는 (하지만 재미있는) 데이터 과학 독해 자료를 읽으며 '머니볼'을 읽었습니다.\n\nLooker와 같은 기본 도구 대신 프로그래밍 방식으로 대시보드를 구축함으로써, 그들은 효과적인 데이터 분석가가 되기 위해 필요한 기술과 디자인 능력을 보여 주었습니다.\n\n그들이 자신의 작업을 공유하는 용기를 가지면, 그것은 주기적이고 쓰고 다듬어진 결과물이기 때문에 숙제 과제보다는 캡스톤 프로젝트나 학위 논문과 유사합니다.\n\n그리고 Medium에 '비하인드 더 신스'를 게시함으로써, 후보자는 채용 공고에서 자주 볼 수 있는 기술을 갖추고 있다는 것을 보여줍니다: \"비기술 관객을 위해 복잡한 기술 개념을 단순화할 수 있는 능력이 필요합니다.\"\n\n<div class=\"content-ad\"></div>\n\n이건 누군가가 \"나를 봐\"라고 하는 경우가 아니라, 누군가가 자신의 기술을 개발하는 이야기입니다.\n\n왜냐하면 짐작했을 것처럼, 이 이야기에는 행복한 결말이 없는 많은 방법들이 있다.\n\n제이크 이야기에서 가장 놀라운 측면 중 하나는 그의 고용에 대해 알게 된 것이 아닌, 그의 예가 무엇이 아닌지를 보는 사람들을 위해 느낀 간접적인 창피 기분이었습니다. 단축키가 아닌 것에 대해 그가 보여준 그 예에서 느낀 놀라움이었습니다.\n\n제이크가 비디오를 게시한 직후, NBCU의 엄격한 고용 절차 우회를 위한 빠른 길을 드러냈습니다. (디즈니 용어에 대해 죄송합니다)\n\n<div class=\"content-ad\"></div>\n\n\"투나잇 쇼 랩\"을 검색하면 이제 제이크가 출었던 영상 뿐만 아니라 똑같은 것을 하는 다른 수백 개의 영상도 나옵니다. 하지만 그들은 제작 품질이 낮은 나쁜 작품들이었습니다.\n\n요약하자면 이들은 독특한 아이디어의 모방이 아니라 대중들 사이에서 돋보이려는 진정성과 신중한 노력이 반영되지 않았기 때문에 나쁘다고 할 수 있습니다.\n\n부가 프로젝트를 만들 때 가능한 한 자신이 진정으로 알고 관심을 가지고 있는 것을 선택하세요. 제이크와 달리, 첫 번째 시도에서 당신의 후보 자격을 알릴 수 있는 확률은 매우 낮습니다.\n\n그래서 프로필을 끌어올리는 마지막 구성 요소는 일관성입니다.\"\n\n<div class=\"content-ad\"></div>\n\n고등학교 선생님 중 한 분이 계속 반복하던 주제를 가져왔어요. \"일관성이 최고야.\"\n\n당신의 지원을 시각적으로 부각시킬 수 있는 방법이 무엇이든 선택하셨다면, 그것이 반복 가능한 것임을 확인하세요. 첫 대시보드로 두 번째 눈길을 끄는 데 필요한 것은 아니기 때문에 몇 번 반복하는 것이 좋습니다.\n\n당신의 가시성과 신뢰성을 높이기 위해 전문가들이 깜짝 놀랐던 Medium에 처음 글을 쓸 때처럼, 큰 팔로워나 인기 있는 글이 없더라도 훌륭한 인터뷰를 기대할 수 있습니다.\n\n다시 말해, 그들이 감동받은 것은 결과물이 아니라 작품을 홍보하기 위한 용기와 노력(제품을 만드는 기술에 더해)이었습니다.\n\n<div class=\"content-ad\"></div>\n\n제이크의 원본 비디오는 30만 회 조회되었고, 그가 고용된 세그먼트는 수백만 회 조회를 기록했습니다. 때때로 좋은 어플리케이션은 그 자체로 이력서에 올라갈 만한 가치가 있는 것을 증명했죠.\n\n도와주실 수 있을까요? 이 블로그 이외에 어떻게 도와드릴 수 있는지 알려주시기 위해 3가지 질문으로 된 설문 조사에 한 분만 투자해 주세요. 모든 응답자께는 무료 선물이 제공됩니다.","ogImage":{"url":"/assets/img/2024-06-19-NotGettingDataScienceJobInterviewsYouHaveAVisibilityProblem_0.png"},"coverImage":"/assets/img/2024-06-19-NotGettingDataScienceJobInterviewsYouHaveAVisibilityProblem_0.png","tag":["Tech"],"readingTime":6},{"title":"데이터 엔지니어가 직면하는 다섯 가지 무자비한 진실","description":"","date":"2024-06-19 16:18","slug":"2024-06-19-FiveBrutalTruthsAboutBeingaDataEngineer","content":"\n\n\n![Five Brutal Truths About Being a Data Engineer](/assets/img/2024-06-19-FiveBrutalTruthsAboutBeingaDataEngineer_0.png)\n\nI like my job! 🙂\n\nI really do!\n\nBut sometimes I have that feeling...\n\n\n<div class=\"content-ad\"></div>\n\n저는 그냥 노트북을 창문 밖으로 버리고 농부가 되고 싶어요. 😡\n\n모든 직업에는 면접 과정 중에 거의 다루지 않는 혹독한 진실이 있어요.\n\n당신이 데이터 엔지니어로 일할 때 알아야 할 5가지 혹독한 진실을 배울 거에요.\n\n나는 이것들을 제 경험을 바탕으로 만들었어요.\n\n<div class=\"content-ad\"></div>\n\n# 데이터는 종종 지저분하고 불완전해요\n\n![image](/assets/img/2024-06-19-FiveBrutalTruthsAboutBeingaDataEngineer_1.png)\n\n프로젝트를 들어가서 기대하는 것은 깔끔하고 완전한 데이터셋(튜토리얼이나 수업 자료처럼)을 다루는 것이라면...\n\n그냥 잊으세요!\n\n<div class=\"content-ad\"></div>\n\n진짜 데이터셋들은 깨끗하지 않고, 완전하지 않고, 그저... \"아름답지\" 않아요! 😒\n\n데이터 엔지니어들은 분석, 시각화 또는 모델에 사용하기 전에 데이터를 정리하고 처리하는 데 상당한 시간을 투자합니다.\n\n# 요구 사항은 자주 변경될 수 있습니다\n\n![이미지](/assets/img/2024-06-19-FiveBrutalTruthsAboutBeingaDataEngineer_2.png)\n\n<div class=\"content-ad\"></div>\n\n이것은 소프트웨어 엔지니어링 개발의 전형적인 사례이며, 따라서 데이터 엔지니어링에서도 매우 \"고전\"적이다.\n\n비즈니스 요구사항이 신속하게 변화할 수 있으며, 데이터 엔지니어들은 적응력을 갖추고 따라가야 합니다.\n\n이는 프로젝트 중간에 방향을 바꾸거나 새로운 기술을 신속하게 배워야 할 수도 있음을 의미할 수 있습니다.\n\n따라서 예산 압력으로 비즈니스가 오늘 변경해야 할 필요가 있다면...\n\n<div class=\"content-ad\"></div>\n\n네가 할 일이니까, 오늘은 바뀔거야.\n\n<div class=\"content-ad\"></div>\n\n# 나쁜 코드에 많은 시간을 낭비할 것입니다\n\n어디서나 나쁜 전문가가 존재합니다!\n\n모든 직책과 모든 회사에서, 적어도 한 명의 나쁜 전문가를 찾을 수 있습니다.\n\n그래서 이해관계자가 파이프라인이나 API에서 작은 변경을 요청하면... 나쁜 코드를 찾을 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n바로 숨 한 번 깊이 들이마시고 일에 전념하세요. 🤐\n\n# 때로는 데이터를 활용하지 못할 수도 있어요\n\n![Image](/assets/img/2024-06-19-FiveBrutalTruthsAboutBeingaDataEngineer_3.png)\n\n상상해보세요:\n\n<div class=\"content-ad\"></div>\n\n- 프로젝트에 두 주 동안 집중해서 일했어요;\n- 세 번째 주에 제공한 데이터가 이후 스테이크홀더들에게 제공됐어요;\n- 스테이크홀더들은 제공한 데이터에 매우 만족했고 \"다음 학기에 매우 중요하다\"고 말했어요;\n- 다음 학기에는 프로젝트 파이프라인이 3일 동안 고장나도 한 사람도 불평하지 않았어요!! 이상하네요...\n- 그런데 결국 데이터를 사용하지 않는다는 걸 알게 되었어요... 😣\n\n이러한 상황은 불행히도 흔한 일입니다.\n이런 일이 발생할 수 있다는 점을 인식하고 그냥... 그렇게 내버려두어야 합니다.\n\n# 분야는 끊임없이 진화하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n아래에 있는 두 이미지는 2023년과 2021년의 데이터 엔지니어링 생태계를 보여줍니다.\n\n두 해 동안 생태계가 많이 성장했다는 것을 알 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n새로운 기술과 도구들이 언제나 계속해서 등장하고 있어요.\n\n정말 빠르죠!!\n\n만약 오늘은 A 도구로 데이터 조정을 하고 있다면, 아마 내년에는 B 도구로 작업할지도 몰라요.\n\n가능한 해결책은 항상 최신 트렌드와 최신 기술에 대해 숙지하고 있어야 한다는 거예요.\n\n<div class=\"content-ad\"></div>\n\n만약 당신도 빠르게 움직이지 않는다면 뒤처질 것입니다.\n\n이 기사를 확인해보세요:\n\n## 결론\n\n데이터 엔지니어가 되는 것은 결코 간단하고 쉬운 여정이 아닙니다.\n\n<div class=\"content-ad\"></div>\n\n혼잡하고 불완전한 데이터로 작업하고, 자주 바뀌는 요구 사항에 적응하고, 나쁜 코드와 씨름하며, 사용되지 않는 데이터의 좌절을 겪으며, 끊임없이 발전하는 분야를 따라잡는 것은 받아들이기 어려울 수 있어요.\n\n이 기사를 마음에 드셨나요? 더 많은 기사를 위해 저를 팔로우해주세요.","ogImage":{"url":"/assets/img/2024-06-19-FiveBrutalTruthsAboutBeingaDataEngineer_0.png"},"coverImage":"/assets/img/2024-06-19-FiveBrutalTruthsAboutBeingaDataEngineer_0.png","tag":["Tech"],"readingTime":3},{"title":"클래스워드 - 데이터베이스 열 이름 짓는데 내가 가장 좋아하는 컨벤션","description":"","date":"2024-06-19 16:16","slug":"2024-06-19-ClasswordsMyFavoriteConventionforNamingDatabaseColumns","content":"\n\n![이미지](/assets/img/2024-06-19-ClasswordsMyFavoriteConventionforNamingDatabaseColumns_0.png)\n\n클래스워드는 그냥 네이밍 프로토콜이 아니에요; 각 컬럼의 목적과 내용을 즉시 이해할 수 있도록 하는 명확한 커뮤니케이션 도구로서 작용해요. 핵심 아이디어는 간단해요: 각 데이터베이스 컬럼의 이름이 그 안에 포함된 데이터를 어떻게 나타내는지 전달되도록 해요. 날짜, 텍스트 설명, 숫자 값과 같이 특정 유형의 정보를 식별하거나, 클래스워드는 각 데이터 조각 뒤에 숨은 의도를 명확하게 합니다.\n\n이 규칙은 원하는 어떤 네이밍 스타일과도 잘 어울려요 — 카멜케이스, 패스칼케이스, 또는 스네이크케이스. 이 글에서는 가독성과 인기 때문에 스네이크케이스를 선호하지만, 클래스워드 사용의 본질은 이 문법적 선택을 초월합니다.\n\n좀 더 깊게 파고들어서, 클래스워드 적용 과정에 대해 공유하고 싶은 개인적인 감상이 있어요. 컬럼 이름에 클래스워드를 할당해야 할 때마다, 그 컬럼이 어떤 데이터를 보유하게 될지 심도 있게 생각하도록 강요받아요. 이 데이터가 정말로 무엇을 의미하는 걸까요? 데이터의 세부 사항을 충분히 이해했을까요? 이 내성적인 순간은 귀중해요. 네이밍만 하는 게 아니라, 이 데이터베이스를 사용하는 나와 다른 누군가가 해당 내용을 포괄적으로 이해할 수 있도록 하는 거예요. 클래스워드로 신중하게 네이밍하는 이 실천은, 내 경험 상으로 더 직관적이고 효율적인 데이터 관리 환경을 육성하는 데 중요한 역할을 해요.\n\n<div class=\"content-ad\"></div>\n\n# 클래스워드란 무엇인가요?\n\n클래스워드는 본질적으로 데이터베이스 열의 이름 뒤에 추가하는 키워드, 즉 데이터 유형을 나타내는 접미사입니다. 이 관례는 데이터의 형태와 기능 사이에 다리 역할을 하며, 데이터베이스와 상호작용하는 사람들이 해당 열이 어떤 종류의 정보를 포함하는지 즉각 파악할 수 있도록 돕습니다. 클래스워드 사용은 본질적으로 데이터베이스 구조에 의미론적 명확성의 한 층을 포함시키는 데 관한 것입니다.\n\n클래스워드의 아름다움은 그들의 간단함과 의미 전달 능력에 있습니다. 예를 들어, 열 이름 뒤에 _id를 추가하는 것은 이 열이 엔티티의 고유 식별자로 기능함을 나타냅니다. 비슷하게, _size_kb와 같은 접미사는 데이터의 크기를 킬로바이트 단위로 표현한 정보를 저장하는 열임을 알려줍니다. 이 의미론적 풍부함은 단순히 명명 규칙을 준수하는 데 그치는 것이 아니라, 데이터베이스에 명확성을 주입함으로써 일당팀을 초월하여 미래의 데이터 엔지니어나 데이터 분석가에게 다가갑니다.\n\n예를 들어, 상태라는 이름으로만 된 열을 고려해 봅시다. 추가적인 맥락이나 데이터 탐색 없이는 해당 열이 어떤 상태를 나타내는지 알기 어려울 것입니다. 이 상태는 상태 코드인가요, 상태에 대한 텍스트 설명인가요, 아니면 활성/비활성 상태를 나타내는 불리언 플래그인가요? 이 모호함은 일상적인 작업뿐만 아니라 데이터 분석과 보고서 작성에서도 혼란을 야기할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n클래스와드를 적용하면 가독성을 크게 향상시킬 수 있습니다. 예를 들어:\n\n- status_code — 숫자 또는 상징적 코드를 포함하는 열로, 아마도 문서 내 열거체나 룩업 테이블을 참조하는 다양한 상태를 나타내는 것으로 보입니다.\n- status_name — 상태의 설명적인 이름을 저장하는 열을 나타내며 열 내에서 직접 가독성 있는 맥락을 제공합니다.\n\n# 기본 Classwords\n\n카테고리 별로 기본 클래스와드를 더 자세히 살펴봅시다.\n\n<div class=\"content-ad\"></div>\n\n## 텍스트 클래스워드\n\n텍스트 클래스워드는 데이터베이스 내 텍스트 데이터의 성격과 기능을 정의하는 데 중요한 역할을 합니다.\n\n- 식별자 (또는 id) — 이 클래스워드는 고유성을 위해 중요합니다. 이는 엔티티의 인스턴스를 식별하는 숫자나 문자열입니다 (보통 시스템이 생성하는 것으로, 시퀀스에서 나온 숫자 값과 같은); 예: request_id.\n- 코드[_`표준`] — 코드는 보통 분류를 위해 사용되는 간략한 표현입니다. 예를 들어, status_code는 전체 이름을 저장하지 않고도 빠르게 상태를 식별하는 데 도움이 됩니다. 이 클래스워드를 사용할 때 해당하는 코딩 표준을 나타내는 것이 좋습니다 (해당하는 경우). 예: airport_code_iata는 IATA 공항 코드를 저장한다는 것을 명확히 나타냅니다 (ICAO나 다른 것이 아닌).\n- 이름 — 이름은 직관적이지만 인간이 읽기 쉽게 식별하기 위해 중요합니다. 예를 들어, product_name을 다룰 때, 이 컬럼이 제품 이름을 저장한다는 것이 즉시 명확합니다.\n- 설명 (또는 desc) — 설명은 더 자세한 통찰력을 제공합니다. 이를 간결하고 유익하게 유지하는 것이 좋은 실천법입니다. 예를 들어, service_description은 제공된 서비스에 대한 간략한 개요를 제공할 수 있습니다.\n- 지표 (또는 ind) — 지표는 \"예\" 또는 \"아니오\" 또는 \"참\" 또는 \"거짓\"을 나타내는 플래그입니다. 예를 들어, is_active_indicator, has_dependencies_indicator와 같이 Boolean 값으로 종종 사용되며 빠른 확인에 이상적입니다.\n- 숫자 — 이름과 달리, 여기서의 숫자는 종종 수학적인 의미에서의 숫자가 아닙니다. phone_number 및 serial_number가 그러한 예시로, 이 값이 식별에 사용되지만 계산을 위해 사용되지 않는 것입니다.\n- 텍스트 — 한 두 개의 단어로 충분하지 않을 때, comment_text 또는 feedback_text가 등장하여 확장된 문자열을 허용합니다.\n\n## 캘린더 클래스워드\n\n<div class=\"content-ad\"></div>\n\n달력 클래스워드는 시간 데이터를 관리하고 해석하는 데 필수적이며, 다양한 차원에서 시간을 추적하는 데 프레임워크를 제공합니다.\n\n- date — 달력 날짜만 필요할 때, event_date는 어떤 일이 언제 일어나는지를 알려주며 정확한 시간에 대해 걱정할 필요가 없습니다.\n- datetime[_`timezone`] (또는 dt[_`timezone`])**— 보다 정확한 타이밍을 위해, meeting_start_datetime은 시간까지 포함한 날짜를 보여주어 초까지 표시할 수 있습니다. 시간대 정보가 전달되지 않는 데이터 유형을 사용하는 경우, column 이름에 적절한 접미사를 추가하여 명확하게 만들어 주세요. 예를 들어, scheduled_start_dt_utc.\n- timestamp[_`timezone`] (또는 ts[_`timezone`])**— 시스템에서 생성되며 종종 초 단위까지 포함하는 시간대 정보가 있는 경우, record_created_timestamp는 행동이나 이벤트의 정확한 순간을 제공합니다. 마찬가지로, 시간대 정보가 포함되지 않는 데이터 유형의 경우, 적절한 접미사를 추가하여 해당 시간대를 나타내세요. 예를 들어, actual_start_ts_utc.\n\n## 숫자형 클래스워드\n\n숫자형 클래스워드는 양적 데이터에 순서와 명확성을 제공하여, 다양한 유형의 숫자 정보를 구별합니다. 아이템을 계산하거나 값, 비율을 측정하든, 이러한 클래스워드는 정확한 데이터 표현과 분석에 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n- count — 방문자 수 등을 합산하는 데 사용하며, 참여도나 트래픽을 측정하는 데 유용합니다.\n- amount[_`화폐단위`] — 이 클래스워드는 재무 데이터에 필수적입니다. sale_amount_usd는 미국 달러로 거래 가치를 명확히 나타냅니다(화폐 표기는 조건적*).\n- `quantity_property`[_`측정단위`] — 측정 가능한 속성에 대해 사용되며, cable_length_meters는 단순히 길이뿐 아니라 단위까지 명시합니다(측정단위 표기는 조건적*). 길이, 무게, 부피, 크기, 거리, 지속 시간 등 측정 가능한 모든 속성에 사용할 수 있습니다. 다른 예시로는: file_size_kb, job_duration_seconds, package_weight_kg, avg_speed_mph\n- ratio — aspect_ratio와 같은 비율은 연결된 단위 없이 비교하는 데 필수적입니다.\n- factor — tax_rate_factor와 같은 요소는 계산에 유용한 곱셈 효과를 제공합니다.\n- percent (또는 pct) — 이익률을 나타내는 profit_margin_percent와 같은 백분율은 상대적인 측정을 제공하며 보통 이해하기 쉬운 형식으로 표시됩니다. 그러나 이 값이 1에 기반한 1%를 나타내는지 100에 기반한 1%를 나타내는지 명확히하면 좋습니다.\n\n* 화폐나 측정단위를 표기하는 접미사의 조건부 포함은 관련 열이 해당 통화나 단위를 나타내는 열을 포함하는지 여부에 따라 결정됩니다. 그러한 열이 있는 경우 접두사가 필요하지 않습니다. 그러므로 tax_amount_usd를 사용하거나 tax_amount와 tax_amount_currency_code(기록이 다양한 통화로 세금액을 저장하는 경우) 쌍을 사용할 수 있습니다.\n\n# 도메인별 클래스워드\n\n도메인별 클래스워드를 네이밍 규칙에 통합하는 것은 유익할 뿐만 아니라 특정 분야의 세세한 점을 포착하는 데 필수적입니다. 이러한 전문화된 클래스워드는 이전에 논의한 더 일반적인 클래스워드에서 파생되며, 데이터 스키마에 추가적인 정확성, 네이밍 간소화 및 맥락을 추가합니다.\n\n<div class=\"content-ad\"></div>\n\n예시:\n\n- uri (identifier 클래스워드의 전문화) — profile_picture_uri와 같은 URI 열은 인터넷 또는 현실 세계의 리소스에 대한 고유 식별자(또는 주소)를 저장합니다.\n- address (text 클래스워드의 전문화) — 주소와 유사한 열을 자주 사용한다면 postal_address_text와 같이 부르는 대신, 전문화된 address 키워드를 소개하고 postal_address와 같이 약칭 사용을 고려해보세요.\n- email (address 클래스워드의 전문화) — 이는 이메일 주소를 포함하는 열을 명명하는 유용한 단축키입니다. email 클래스워드는 uri를 특수화한 클래스워드도 될 수 있습니다. 이러한 출처를 확인하면 URI에 특화된 이메일 구문(예: \"mailto:someone.special@example.com\") 대신 간단한 \"someone.special@example.com\"이 아니라는 것을 나타낼 수 있습니다.\n- sku (code 클래스워드의 전문화) — 상업 분야에서 각 제품에 대한 고유 식별자인 Stock Keeping Unit에 유용한 전문화된 클래스워드를 사용할 수 있습니다. 예시: product_sku, 재고 관리 시스템에서 제품을 식별하기 위한 편리한 약어입니다.\n- json(text 클래스워드의 전문화) — config_settings_json과 같은 JSON 열은 구조화된 데이터를 압축된 형식으로 저장하며, 설정이나 데이터 교환에 적합합니다.\n- geojson (json 클래스워드의 전문화) — GIS 분야에서 GeoJSON 형식으로 지리 데이터를 저장하는 데 사용될 수 있습니다. 예시: parcel_border_geojson은 토지 경계를 나타내기 위한 지리 공간 데이터를 포함합니다.\n\n# Context에 유의하세요\n\n테이블 스키마를 설계할 때 항상 해당 테이블이 제공하는 문맥을 고려해보세요. 예를 들어, product라는 이름의 테이블에서는 모든 열 이름에 product_를 접두어로 붙이는 것은 중복되고 불필요하게 장황합니다. product_name, product_description 또는 product_launch_date와 같은 이름 대신 테이블 내에서 name, description 및 launch_date와 같은 간결한 이름을 사용하는 것이 반복적인 꼬리표 없이 필요한 모든 문맥을 제공합니다. 아래 쿼리가 충분히 읽기 쉽지 않나요?\n\n<div class=\"content-ad\"></div>\n\n```js\nSELECT name, description, launch_date\nFROM product\n```\n\n이 규칙은 나에게 Smurf 네이밍 컨벤션을 떠올리게 합니다. 이는 클래스를 네임스페이스에 맥락을 고려해 네이밍하는 프로그래밍 안티 패턴입니다. 하지만, 테이블 내의 열을 네이밍하는 것을 생각할 때, 좋은 비유가 될 수 있습니다.\n\n# 클래스워드 사용의 주요 이점\n\n데이터베이스 열 네이밍 규칙으로 클래스워드를 포함하는 것은 데이터 관리 프로세스를 스트림라인하게 하고 데이터베이스 스키마의 명확성을 높일 수 있는 다양한 이점을 제공합니다. 이것은 왜 나는 클래스워드를 내 데이터베이스 디자인 도구에서 없어서는 안 되는 부분으로 여기게 되었는지에 대한 이유입니다.\n\n<div class=\"content-ad\"></div>\n\n- 향상된 가독성과 명확성 — Classwords는 데이터베이스 열 이름의 가독성과 이해도를 크게 향상시켜줍니다. 이는 팀 구성원들의 학습 곡선을 최소화하고 상세 문서에 대한 의존을 줄여줍니다. 이로써 각 열의 목적과 데이터 유형을 한눈에 파악하기가 더욱 쉬워집니다.\n- AI 기반 SQL 쿼리 생성의 향상 — Classwords는 자연어 프롬프트로부터 SQL 쿼리를 생성하는 GenAI 모델의 정확성을 향상시킵니다. 이들은 AI 시스템이 데이터베이스 구조를 더 잘 이해하도록 돕고, 대화 인터페이스를 통해 더 쉽게 데이터 추출을 용이하게 합니다. 이를 통해 데이터베이스가 AI 친화적으로 변화합니다.\n- 모든 구문 기반 명명 스타일과의 호환성 — Classwords는 다재다능하며 문법적 명명 패턴인 camelCase, PascalCase 또는 snake_case와 쉽게 통합될 수 있습니다. 이 유연성은 classwords를 채택함에 있어 기존의 명명 규칙을 완전히 변경할 필요가 없게 만들어줍니다. 대신 의미론적 의미를 추가하여 기존 규칙을 향상시킵니다.\n\n이 클래스워드에 대한 탐구가 데이터베이스를 보다 관리 가능하고 이해하기 쉽도록 만드는 가치를 잘 보여줬으면 좋겠습니다. 제 경험은 이들이 데이터 무결성과 팀 협업에 미치는 중요한 영향을 입증합니다. 데이터베이스 관행에 classwords를 도입하여 더 명확하고 일관된 명명 규칙을 추구하는 것을 장려합니다.\n\n본문이 마음에 드셨다면, Database Naming Convention Checklist도 관심 있게 보실 수 있습니다.\n\n# 🖐🤓👉 재미있는 사실\n\n<div class=\"content-ad\"></div>\n\n헝가리안 표기법의 원래 의도가 종종 오해를 받았다는 사실을 알고 계셨나요? 대중적인 믿음과는 달리, 헝가리안 표기법은 데이터 유형과 관련된 것이 아닌 의미론적 유형 접두사를 사용하도록 설계되었습니다. 변수의 유형이 아닌 목적에 대한 통찰력을 제공하기 위한 것이었습니다. 오늘날 일부에서는 안티 패턴으로 간주되지만, 헝가리안 표기법의 본질은 의미 있는 명명의 중요성을 강조하며, 이는 데이터베이스 관리에서의 클래스워드의 가치와 resonates 한 원칙입니다.\n\n더 자세한 이해를 위해, 마이크로소프트의 문서에서 헝가리안 표기법을 설명한 원래 논문을 살펴보세요.\n\n혹시 이제 안티 패턴으로 간주해야 할지 다시 생각할 때가 온 것일지도 모릅니다! 🤔","ogImage":{"url":"/assets/img/2024-06-19-ClasswordsMyFavoriteConventionforNamingDatabaseColumns_0.png"},"coverImage":"/assets/img/2024-06-19-ClasswordsMyFavoriteConventionforNamingDatabaseColumns_0.png","tag":["Tech"],"readingTime":8},{"title":"벡터 DB를 사용하여 실시간 뉴스 검색 엔진을 구축하는 방법","description":"","date":"2024-06-19 16:12","slug":"2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs","content":"\n\n아파치 카프카, 바이트왁스, 그리고 업스태시 벡터 데이터베이스를 활용한 라이브 뉴스 집계 스트리밍 파이프라인 구현을 위한 실용적인 안내서입니다.\n\nearthweb.com에서 실시한 연구에 따르면, 매일 온라인 및 오프라인에서 유입되는 뉴스 기사는 200-300만 건 사이에 있다고 합니다!\n\n모든 방향에서 우리에게 쏟아지는 뉴스로 인해 정신없을 때가 많습니다. 그래서 우리는 실제로 관심 있는 뉴스를 빠르게 받아볼 수 있는 짧고 빠른 방법을 찾고 있습니다.\n\n본 문서에서는 이러한 문제를 해결하고자 합니다! 좀 더 구체적으로, 여러 출처로부터 데이터를 수집하고 해당 정보 채널을 당신의 관심사에 맞는 종단점으로 좁힐 수 있는 시스템을 구축할 것입니다 — 뉴스 검색 엔진입니다!\n\n<div class=\"content-ad\"></div>\n\n이론에 대해서만 이야기하거나 이러한 시스템을 구현하는 방법을 알려주는 것이 아니라, 우리는 단계별로 설명하고 보여줄 거예요!\n\n시작하기 전에, 이 기사에서 제공하는 모든 것은 Decoding ML Articles GitHub 저장소에서 완전한 코드 지원을 받습니다.\n\n# 목차\n\n- 아키텍처 개요\n- 도구 고려 사항\n- 전제 조건\n3.1 새로운 Upstash Kafka 클러스터 생성\n3.2 새로운 Upstash Vector 인덱스 생성\n3.3 2개의 라이브 뉴스 API에 등록\n3.4 설치\n- 데이터 수집\n4.1 기사 가져오기 관리자\n4.2 Kafka 메시지 제작\n4.3 Pydantic을 사용한 데이터 교환 모델\n4.4 KafkaProducers 실행\n- 수집 파이프라인\n5.1 Kafka에서 메시지 수신\n5.2 Bytewax 데이터플로 구현\n5.3 기사 정제, 형식 지정, 청크화, 삽입\n5.4 벡터 작성 및 VectorDB에 업서트\n- 파이프라인 시작\n- 사용자 인터페이스\n- 결론\n- 참고문헌\n\n<div class=\"content-ad\"></div>\n\n# 아키텍처 개요\n\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_0.png)\n\n요약하자면, 우리는 뉴스 API에서 뉴스 기사를 가져와서 생생한 시스템을 구축할 것입니다. 가져온 데이터를 정의된 형식으로 파싱하고 포맷팅한 다음 첫 번째 단계로 Kafka 토픽에 메시지를 직렬화하고 스트리밍할 것입니다. \n두 번째 단계에서는 Bytewax를 사용하여 Kafka 토픽에서 메시지를 더 청소, 파싱, 청크, 임베드, 벡터를 업서팅하여 벡터 데이터베이스로 보내고, 데이터베이스와 상호 작용할 수 있는 UI로 마무리됩니다.\n\n# 툴 고려 사항\n\n<div class=\"content-ad\"></div>\n\n올바른 도구를 선택하는 것이 고성능, 확장성, 및 구현 용이성을 달성하는 핵심이었습니다. 프로젝트 전체에서 사용된 도구를 살펴보겠습니다:\n\n- Upstash Serverless Kafka: 인프라 관리에 대해 걱정할 필요 없이 강력하고 확장 가능한 이벤트 스트리밍을 위해 사용됩니다.\n- Python 스레딩: 여러 뉴스 API에서 동시에 데이터를 가져오면서 스레드 안전한 Kafka Producer 인스턴스를 공유하여 메모리 풋프린트와 성능을 최적화합니다.\n- Pydantic 모델: 일관된 및 유효한 데이터 교환 구조를 보장하기 위해 사용됩니다.\n- Bytewax: 스트리밍 데이터를 처리하고 변환하는 데 간편하고 빠른 속도를 제공하기 때문에 사용됩니다.\n- Upstash Vector Database: Serverless로 구성이 쉽고 Python, JS, 및 GO 내에서 쉽게 통합됩니다. UI 콘솔 대시보드에서 풍부한 탐색 옵션과 실시간 상태 메트릭을 제공하는 것이 큰 장점입니다.\n\n하드웨어 요구 사항에 따르면 GPU는 필요하지 않습니다.\n\n비용은 얼마입니까? — 무료입니다.\n이 안내서는 무료 티어 플랜만 사용하도록 설정했으므로 사용한 플랫폼에 대해 지불할 필요가 없습니다!\n\n<div class=\"content-ad\"></div>\n\n# 준비 사항\n\n어떠한 구현을 하기 전에, 각 서비스에 접근할 수 있는지 확인해야 합니다. 따라서 다음을 설정해야 합니다:\n\n- 새로운 Upstash Kafka 클러스터\n- 새로운 Upstash Vector Index\n- 뉴스 API 등록\n- 환경 설치\n\n처음 시작할 때는 약 5분 정도 걸립니다. 함께 해보세요!\n\n<div class=\"content-ad\"></div>\n\n## 새로운 Upstash Kafka 클러스터 생성\n\n먼저 Upstash에 등록해야 합니다. 로그인 후에 다음과 같은 대시보드가 나타납니다:\n\n![대시보드 이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_1.png)\n\n다음으로 상단 바에서 Kafka를 선택하고 + 클러스터 생성 버튼을 클릭하여 새 클러스터를 만들어야 합니다. 클릭하면 다음 모달이 나타납니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_2.png\" />\n\n클러스터에 이름을 지정하고 본인의 위치와 가장 가까운 지역을 선택한 후, 클러스터 생성을 클릭하여 완료하세요. 완료되면 새로운 Kafka 클러스터가 아래에 표시됩니다. 새로운 Kafka 클러스터를 선택하고 아래 화면으로 이동하게 됩니다:\n\n<img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_3.png\" />\n\n이 뷰에서 주요 구성 요소를 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n- 상세 정보: 클러스터 개요 및 Upstash가 제공하는 기능을 보여줍니다.\n- 사용량: 생성/소비된 메시지 수, 비용 영향 등을 보여주는 차트입니다.\n- 주제: 이 탭에서는 Kafka 주제를 만들고 세부 정보를 모니터링할 수 있습니다.\n\n클러스터를 생성한 다음 해야 할 다음 단계는 메시지를 생성(보내기)하고 소비(받기)할 수 있는 주제를 정의하는 것입니다.\n\n주제 탭 아래에서 \"주제 생성\"을 선택하면 다음과 같은 화면이 나타납니다:\n\n<img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_4.png\" />\n\n<div class=\"content-ad\"></div>\n\n주제 이름을 지정하고, 나머지는 기본 상태로 두어서 Create를 클릭하세요.\n\n카프카 클러스터를 성공적으로 생성했습니다. 이제 클러스터에 연결하는 데 도움이 되는 환경 변수를 복사하고 설정해야 합니다. 이를 위해 클러스터 대시보드로 이동하여 세부 정보 탭에서 엔드포인트, 사용자 이름 및 비밀번호를 복사하여 .env 파일에 붙여넣으세요.\n그 후, Topics로 이동하여 카프카 토픽 이름을 복사하세요.\n\n지금까지 .env 파일이어야 하는 모습은 다음과 같습니다:\n\n```js\nUPSTASH_KAFKA_UNAME=\"[사용자 이름]\"\nUPSTASH_KAFKA_PASS=\"[비밀번호]\"\nUPSTASH_KAFKA_ENDPOINT=\"[엔드포인트]\"\nUPSTASH_KAFKA_TOPIC=\"[토픽 이름]\"\n```\n\n<div class=\"content-ad\"></div>\n\n## 새 Upstash Vector 색인 만들기\n\n이제 새로운 Vector 데이터베이스를 만들어 보겠습니다. 이를 위해 대시보드에서 Vector를 선택하고 + Index 작성을 클릭하세요. 그러면 다음 뷰로 이동됩니다:\n\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_5.png)\n\n벡터 데이터베이스에 이름을 할당하고, 위치에 가장 가까운 지역을 선택한 다음 Embedding을 생성할 때 사용할 모델로 sentence-transformers/all-MiniLM-L6-v2을 선택하세요. 뉴스 기사의 임베딩을 생성할 때 사용할 모델과 벡터 간 거리 비교에 코사인 유사도 메트릭을 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n새로운 Vector Index를 만든 후에는 Kafka Cluster와 동일한 작업 흐름을 따를 수 있습니다. Index Name, Endpoint, Token을 복사하고 .env 파일에 붙여넣기하세요.\n\n현재 .env 파일은 다음과 같이 보여야 합니다:\n\n```js\nUPSTASH_KAFKA_UNAME=\"[여기에 사용자명 입력]\"\nUPSTASH_KAFKA_PASS=\"[여기에 비밀번호 입력]\"\nUPSTASH_KAFKA_ENDPOINT=\"[여기에 엔드포인트 입력]\"\nUPSTASH_KAFKA_TOPIC=\"[여기에 토픽 이름 입력]\"\n\nUPSTASH_VECTOR_ENDPOINT=\"[Vector 엔드포인트 입력]\"\nUPSTASH_VECTOR_TOPIC=\"[Vector 이름 입력]\"\nUPSTASH_VECTOR_KEY=\"[Vector 토큰 입력]\"\n``` \n\n## 뉴스 API에 등록하기\n\n<div class=\"content-ad\"></div>\n\n다음 APIs를 사용하여 기사를 가져올 예정입니다:\n\n- 🔗 NewsAPI\n\n하루에 100번의 API 호출을 할 수 있는 무료 개발자 플랜을 제공합니다.\n\n2. 🔗 NewsData\n\n<div class=\"content-ad\"></div>\n\n무료 요금제가 제공되며 하루에 200개의 크레딧을 받습니다. 각 크레딧은 10개의 기사와 동일하며, 이는 하루에 총 2000개의 기사를 가져올 수 있다는 것을 의미합니다.\n\n저희 현재의 사용 사례에는 이 API들이 충분한 기능을 제공하여 구축 중인 뉴스 검색 엔진을 구현하고 유효성을 검사할 수 있습니다. 동시에 기존 워크플로우가 동일하게 유지되므로 개선 및 확장할 여지도 남겨두고 있습니다.\n무료 요금제에 따른 유일한 제약은 타임드-배치 페치를 수행할 수 없다는 것입니다. 즉, 이 API들을 쿼리할 때 from_date, to_date를 사용할 수 없습니다. 하지만 이는 문제가 되지 않습니다.\n대신 페치 호출 간의 대기 시간을 이용하여 이 동작을 모방할 예정입니다.\n\n다음 단계는 두 플랫폼에 등록하는 것입니다 — 걱정 마세요, 가능한 간단합니다.\n\n- NewsAPI에 등록한 후, /account로 이동하여 API_KEY 필드를 확인한 후 이를 .env 파일의 NEWSAPI_KEY에 복사하여 붙여넣으십시오.\n- NewsData에 등록한 후, /api-key로 이동하여 API KEY를 확인한 후 이를 .env 파일의 NEWSDATAIO_KEY에 복사하여 붙여넣으십시오.\n\n<div class=\"content-ad\"></div>\n\n지루한 부분은 끝났습니다. 이제 이러한 API에 액세스할 수 있고, 기사를 가져올 수 있습니다. 각 API에서 페이로드가 어떻게 보이는지 살펴봅시다:\n\n![image](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_6.png)\n\n## 사전 준비 조치 요약\n\nKafka 클러스터를 생성하고, 벡터 인덱스를 생성하고, 뉴스 API에 등록하는 이 3단계를 모두 마친 후에 .env 파일은 다음과 같은 모습이어야 합니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nUPSTASH_KAFKA_UNAME=\"[여기에 사용자 이름 입력]\"\nUPSTASH_KAFKA_PASS=\"[여기에 암호 입력]\"\nUPSTASH_KAFKA_ENDPOINT=\"[여기에 엔드포인트 입력]\"\nUPSTASH_KAFKA_TOPIC=\"[토픽 이름 입력]\"\n\nUPSTASH_VECTOR_ENDPOINT=\"[벡터 엔드포인트 입력]\"\nUPSTASH_VECTOR_TOPIC=\"[벡터 이름 입력]\"\nUPSTASH_VECTOR_KEY=\"[벡터 토큰 입력]\"\n\nNEWSAPI_KEY=\"[NEWSAPI 키 입력]\"\nNEWSDATAIO_KEY=\"[NEWSDATA 키 입력]\"\nNEWS_TOPIC = \"news\" # 이것은 가져올 기사의 카테고리입니다\n\n다음 단계는 구현 세부 정보에 들어가기 전에 환경과 필수 패키지를 설치하는 것입니다.\n다음은 Makefile 설치 단계의 모습입니다:\n\n# Makefile\n...\ninstall:\n @echo \"$(GREEN) [CONDA] [$(ENV_NAME)] 파이썬 환경 생성 $(RESET)\"\n conda create --name $(ENV_NAME) python=3.9 -y\n @echo \"환경 활성화 중...\"\n @bash -c \"source $$(conda info --base)/etc/profile.d/conda.sh && conda activate $(ENV_NAME) \\\n   && pip install poetry \\\n   poetry env use $(which python)\"\n @echo \"패키지 설치 중\"\n @echo \"pyproject.toml 위치로 변경 중...\"\n @bash -c \" PYTHON_KEYRING_BACKEND=keyring.backends.fail.Keyring poetry install\"\n...\n\n환경을 준비하려면 make install을 실행하세요.\n\n<div class=\"content-ad\"></div>\n\n이제 이 소스로부터 기사를 가져오는 핸들러 구현을 조사해 봅시다.\n\n# 데이터 수집\n\n이 모듈의 목적은 두 API를 쿼리하는 기능을 캡슐화하고, 페이로드를 구문 분석하여 두 페이로드에 모두 존재하는 속성을 사용하여 공통 문서 형식으로 포매팅하고, 클러스터로 메시지를 보내기 위해 공유 KafkaProducer 인스턴스를 사용하는 것입니다.\n\n자세히 살펴볼 내용은 다음 하위 모듈들입니다:\n\n<div class=\"content-ad\"></div>\n\n- Articles Fetching Manager Class\n- 카프카 클러스터로 메시지를 보내는 방법\n- Pydantic 데이터 모델\n- 파이프라인 실행\n\n## Articles Fetching Manager Class\n\n구현 내용에 대해 알아보겠습니다:\n\nimport datetime\nimport functools\nimport logging\nfrom typing import Callable, Dict, List\n\nfrom newsapi import NewsApiClient\nfrom newsdataapi import NewsDataApiClient\nfrom pydantic import ValidationError\n\nfrom models import NewsAPIModel, NewsDataIOModel\nfrom settings import settings\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n\ndef handle_article_fetching(func: Callable) -> Callable:\n    \"\"\"\n    뉴스 기사 가져오기 기능에 대한 예외 처리를 담당하는 데코레이터입니다.\n\n    이 데코레이터는 기사 가져오기 기능을 감싸서 발생하는 예외를 catch하고 로깅합니다.\n    오류가 발생하면 오류를 기록하고 빈 목록을 반환합니다.\n\n    Args:\n        func (Callable): 감쌀 기사 가져오기 함수.\n\n    Returns:\n        Callable: 감싼 함수.\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except ValidationError as e:\n            logger.error(f\"기사 처리 중 유효성 검사 오류 발생: {e}\")\n        except Exception as e:\n            logger.error(f\"소스로부터 데이터를 가져오는 도중 오류 발생: {e}\")\n            logger.exception(e)\n        return []\n\n    return wrapper\n\nclass NewsFetcher:\n    \"\"\"\n    다양한 API에서 뉴스 기사를 가져오는 클래스입니다.\n\n    속성:\n        _newsapi (NewsApiClient): NewsAPI 클라이언트.\n        _newsdataapi (NewsDataApiClient): NewsDataAPI 클라이언트.\n\n    메서드:\n        fetch_from_newsapi(): NewsAPI로부터 기사 가져오기.\n        fetch_from_newsdataapi(): NewsDataAPI로부터 기사 가져오기.\n        sources: 호출 가능한 가져오기 함수 목록을 반환합니다.\n    \"\"\"\n\n    def __init__(self):\n        self._newsapi = NewsApiClient(api_key=settings.NEWSAPI_KEY)\n        self._newsdataapi = NewsDataApiClient(apikey=settings.NEWSDATAIO_KEY)\n\n    @handle_article_fetching\n    def fetch_from_newsapi(self) -> List[Dict]:\n        \"\"\"NewsAPI에서 상위 뉴스 가져오기.\"\"\"\n        response = self._newsapi.get_everything(\n            q=settings.NEWS_TOPIC,\n            language=\"en\",\n            page=settings.ARTICLES_BATCH_SIZE,\n            page_size=settings.ARTICLES_BATCH_SIZE,\n        )\n        return [\n            NewsAPIModel(**article).to_common()\n            for article in response.get(\"articles\", [])\n        ]\n\n    @handle_article_fetching\n    def fetch_from_newsdataapi(self) -> List[Dict]:\n        \"\"\"NewsDataAPI에서 뉴스 데이터 가져오기.\"\"\"\n        response = self._newsdataapi.news_api(\n            q=settings.NEWS_TOPIC,\n            language=\"en\",\n            size=settings.ARTICLES_BATCH_SIZE,\n        )\n        return [\n            NewsDataIOModel(**article).to_common()\n            for article in response.get(\"results\", [])\n        ]\n\n    @property\n    def sources(self) -> List[callable]:\n        \"\"\"뉴스 가져오기 함수 목록입니다.\"\"\"\n        return [self.fetch_from_newsapi, self.fetch_from_newsdataapi]\n\n<div class=\"content-ad\"></div>\n\n이 구현에서 고려해야 할 몇 가지 중요 사항이 있습니다:\n\n- NewsAPIModel과 NewsDataIOModel은 특정 페이로드 형식에 익숙한 Pydantic 모델입니다.\n- 우리는 handle_article_fetching 데코레이터를 사용하여 원시 페이로드를 Pydantic 모델로 변환할 때 유효성 오류나 더 넓은 예외를 잡습니다.\n- 우리에게는 API를 쿼리하는 callable 메서드를 반환하는 sources라는 속성이 있습니다. 이것은 데이터 수집 모듈 내에서 사용될 것이며 멀티 프로듀서 스레드를 생성하여 Kafka 클러스터로 메시지를 전송합니다. 다음에 이어서 설명하겠습니다.\n\n## Kafka 메시지 생성\n\n다음에 우리가 구현할 작업 흐름입니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_7.png\" />\n\n여기서 중요한 포인트들입니다:\n\n- API에서 가져오는 작업에 별도 스레드를 사용합니다.\n- 메시지를 보내기 위해 공통 카프카 프로듀서 인스턴스를 공유합니다.\n- 데이터 교환을 보증하기 위해 Pydantic 모델을 사용합니다.\n\n기사를 가져오는 데 별도 스레드를 사용하고, 클러스터로 메시지를 보내기 위해 단일 카프카 프로듀서 인스턴스를 사용하는 것이 우리의 사용 사례에서 권장되는 방법입니다. 그 이유는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- 효율성 및 성능: KafkaProducer는 스레드 안전합니다. 새 인스턴스를 만드는 것은 네트워크 연결과 일부 설정이 필요합니다. 여러 스레드 간에 하나의 단일 인스턴스를 공유하면 이러한 작업과 관련된 오버헤드를 줄일 수 있습니다.\n- 처리량: 단일 프로듀서 인스턴스는 메시지를 Kafka 클러스터로 보내기 전에 메시지를 일괄 처리합니다.\n- 자원: 사용 사례에 완전히 적용되지는 않지만, 우리는 오직 2개의 프로듀서 스레드만 가지고 있기 때문에 인스턴스 수를 제한함으로써 시스템 자원 이용률을 최적화할 수 있습니다.\n\n여기 Kafka로 메시지 처리를 담당하는 주요 기능이 있습니다:\n\ndef run(self) -> NoReturn:\n        \"\"\"지속적으로 Kafka 주제로 메시지를 가져와 보냅니다.\"\"\"\n        while self.running.is_set():\n            try:\n                messages: List[CommonDocument] = self.fetch_function()\n                if messages:\n                    messages = [msg.to_kafka_payload() for msg in messages]\n                    self.producer.send(self.topic, value=messages)\n                    self.producer.flush()\n                logger.info(\n                    f\"프로듀서 : {self.producer_id}이(가) {len(messages)}개의 메시지를 전송함.\"\n                )\n                time.sleep(self.wait_window_sec)\n            except Exception as e:\n                logger.error(f\"프로듀서 작업자 {self.producer_id}에서 오류 발생: {e}\")\n                self.running.clear()  # 오류 시 스레드를 중지합니다\n\n구현에서 고려해야 할 중요 사항:\n\n<div class=\"content-ad\"></div>\n\n- 우리는 fetch sources의 수만큼 KafkaProducerThread 인스턴스가 생성됩니다.\n- 우리는 모든 스레드를 KafkaProducerSwarm 아래에 랩합니다.\n- 모든 스레드 사이에서 단일 KafkaProducer 인스턴스를 공유하며, 이는 클러스터와 통신할 것입니다.\n- 우리는 N개의 fetching 스레드로 확장할 수 있지만 여전히 단일 KafkaProducer 인스턴스를 유지하기 위해 싱글톤 디자인 패턴을 따릅니다.\n\n## Pydantic을 사용한 데이터 교환 모델\n\n위에서 제시한 코드 스니펫 구현에서, 이전에 설명되지 않았던 *Document, *Model 객체의 사용을 관찰했을 수 있습니다. 이 섹션에서 이들이 무엇인지 자세히 살펴보겠습니다.\n\n이들은 데이터 교환을 위한 Pydantic 모델들이며, 우리가 구축 중인 응용 프로그램 내에서 이러한 모델들은:\n\n<div class=\"content-ad\"></div>\n\n- NewsDataIOModel: NewsData API에서 가져온 원시 페이로드를 래핑하고 포맷합니다.\n- NewsAPIModel: NewsAPI API에서 가져온 원시 페이로드를 래핑하고 포맷합니다.\n- CommonDocument: 위에서 언급한 다양한 뉴스 형식 사이의 공통 형식을 설정합니다.\n- RefinedDocument: metadata 아래에 유용한 필드를 그룹화하고 기사 설명 텍스트와 같은 주요 필드를 강조하는 공통 형식을 필터링합니다.\n- ChunkedDocument: 텍스트를 청크로 나누고 chunk_id와 document_id 사이의 계보를 보장합니다.\n- EmbeddedDocument: 청크를 임베드하여 chunk_id와 document_id 사이의 계보를 보장합니다.\n\n예를 들어, 위 CommonDocument 모델은 다양한 뉴스 페이로드 형식 사이의 연결 역할을 나타내므로 이와 같이 구성됩니다:\n\nclass CommonDocument(BaseModel):\n    article_id: str = Field(default_factory=lambda: str(uuid4()))\n    title: str = Field(default_factory=lambda: \"N/A\")\n    url: str = Field(default_factory=lambda: \"N/A\")\n    published_at: str = Field(\n        default_factory=lambda: datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    )\n    source_name: str = Field(default_factory=lambda: \"Unknown\")\n    image_url: Optional[str] = Field(default_factory=lambda: None)\n    author: Optional[str] = Field(default_factory=lambda: \"Unknown\")\n    description: Optional[str] = Field(default_factory=lambda: None)\n    content: Optional[str] = Field(default_factory=lambda: None)\n\n    @field_validator(\"title\", \"description\", \"content\")\n    def clean_text_fields(cls, v):\n        if v is None or v == \"\":\n            return \"N/A\"\n        return clean_full(v)\n\n    @field_validator(\"url\", \"image_url\")\n    def clean_url_fields(cls, v):\n        if v is None:\n            return \"N/A\"\n        v = remove_html_tags(v)\n        v = normalize_whitespace(v)\n        return v\n\n    @field_validator(\"published_at\")\n    def clean_date_field(cls, v):\n        try:\n            parsed_date = parser.parse(v)\n            return parsed_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n        except (ValueError, TypeError):\n            logger.error(f\"Error parsing date: {v}, using current date instead.\")\n\n    @classmethod\n    def from_json(cls, data: dict) -> \"CommonDocument\":\n        \"\"\"JSON 객체에서 CommonDocument를 만듭니다.\"\"\"\n        return cls(**data)\n\n    def to_kafka_payload(self) -> dict:\n        \"\"\"Kafka 페이로드의 공통 표현을 준비합니다.\"\"\"\n        return self.model_dump(exclude_none=False)\n\n해석해보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n- 뉴스 기사 형식에 공통 속성 시리즈가 포함되어 있습니다.\n- 각 필드를 유효성 검사하거나 field_validator 데코레이터를 사용하여 기본값을 지정합니다.\n- to_kafka_payload 메서드는 메시지 직렬화를 보장하여 Kafka 클러스터로 전송하기 전에 처리합니다.\n\n## 텍스트 필드 클린업 프로세스\n\n클린업 프로세스는 간단합니다. 텍스트를 정리하고 다음을 보장하기 위해 메서드를 사용합니다:\n\n- 끝에 있는 공백이나 \\n, \\t를 제거합니다.\n- ul/li 목록 항목을 제거합니다.\n- 텍스트 내에 HTML 태그가 있으면 제거합니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 이러한 변환을 간소화하기 위해 구조화되지 않은 [7] Python 라이브러리를 사용하고 있습니다.\n\n## KafkaProducers 실행\n\n지금까지 다음 모듈을 수행/구현했습니다:\n\n- 필요한 모든 서비스에 등록\n- Kafka 클러스터 및 벡터 데이터베이스 생성\n- 뉴스 기사 검색 핸들러 구현\n- 데이터 교환을 위한 Pydantic 모델 구현\n- KafkaProducer 로직 구현\n\n<div class=\"content-ad\"></div>\n\n작업이 완료되면 이제 안전하게 우리의 파이프라인에서 생산 단계를 실행하고 Upstash의 KafkaCluster에서 메시지를 확인할 수 있습니다.\n\n그럼 시작해봐요!\n프로젝트의 루트 디렉토리에서, Makefile에 데이터 수집을 실행하는 명령어가 있습니다:\n\n....\n\nrun_producers:\n @echo \"$(GREEN) [실행 중] 데이터 수집 파이프라인 Kafka 프로듀서 $(RESET)\"\n @bash -c \"poetry run python -m src.producer\"\n\n...\n\n이 🔗Makefile은 우리가 구축 중인 솔루션과 상호작용하기 위한 유용한 명령어가 포함되어 있습니다. 이 경우에는 make run_producers를 사용하여 run_producers를 실행해야 합니다. 이렇게 하면 KafkaSwarm이 시작되고 NewsAPIs에서 기사를 가져와 형식을 지정한 다음 Kafka 클러스터로 보내는 스레드를 다룰 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_8.png)\n\n로그를 통해 프로듀서 스레드가 각각 5개의 메시지를 보냈다는 것을 확인했습니다. 메시지들이 클러스터에 도달했는지 확인하려면 Upstash 콘솔로 이동하여 Kafka 클러스터 → 메시지를 확인하십시오. 다음과 같은 화면이 나타날 것입니다:\n\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_9.png)\n\n이 시점에서는 API에서 뉴스 기사를 가져와 형식을 맞춘 후 Kafka로 메시지를 보내는 데이터 수집 파이프라인의 구현 및 테스트가 완료되었습니다. 다음으로는 Kafka에서 새 메시지를 처리하는 \"컨슈머\" 또는 적재 파이프라인을 구현할 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 데이터 수집 파이프라인\n\n우리가 Kafka 주제에서 메시지를 받았다는 것을 확인한 후에는 \"소비자\" 파이프라인을 구현해야 합니다. 이는 다음을 의미합니다:\n\n- Kafka 주제에서 메시지 읽기\n- 파싱, 형식 지정, 청크화, 임베딩 생성\n- 벡터 객체 생성 및 Upstash Vector Index에 업서트\n\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_10.png)\n\n<div class=\"content-ad\"></div>\n\n이를 위해 Bytewax [4]를 사용하여 이러한 단계를 올바른 순서로 연결하는 DataFlow를 정의할 것입니다.\n\n바로 구현에 들어가서 주요 개념을 설명해보겠습니다!\n\n- Bytewax Flow에 입력으로 Kafka Source를 정의합니다.\n\n```js\nimport json\nfrom typing import List\n\nfrom bytewax.connectors.kafka import KafkaSinkMessage, KafkaSource\n\nfrom logger import get_logger\nfrom models import CommonDocument\nfrom settings import settings\n\nlogger = get_logger(__name__)\n\ndef build_kafka_stream_client():\n    \"\"\"\n    Build a Kafka stream client to read messages from the Upstash Kafka topic using the ByteWax KafkaSource connector.\n    \"\"\"\n    kafka_config = {\n        \"bootstrap.servers\": settings.UPSTASH_KAFKA_ENDPOINT,\n        \"security.protocol\": \"SASL_SSL\",\n        \"sasl.mechanisms\": \"SCRAM-SHA-256\",\n        \"sasl.username\": settings.UPSTASH_KAFKA_UNAME,\n        \"sasl.password\": settings.UPSTASH_KAFKA_PASS,\n        \"auto.offset.reset\": \"earliest\",  # Start reading at the earliest message\n    }\n    kafka_input = KafkaSource(\n        topics=[settings.UPSTASH_KAFKA_TOPIC],\n        brokers=[settings.UPSTASH_KAFKA_ENDPOINT],\n        add_config=kafka_config,\n    )\n    logger.info(\"KafkaSource client created successfully.\")\n    return kafka_input\n\ndef process_message(message: KafkaSinkMessage):\n    \"\"\"\n    On a Kafka message, process the message and return a list of CommonDocuments.\n    - message: KafkaSinkMessage(key, value) where value is the message payload.\n    \"\"\"\n    documents: List[CommonDocument] = []\n    try:\n        json_str = message.value.decode(\"utf-8\")\n        data = json.loads(json_str)\n        documents = [CommonDocument.from_json(obj) for obj in data]\n        logger.info(f\"Decoded into {len(documents)} CommonDocuments\")\n        return documents\n    except StopIteration:\n        logger.info(\"No more documents to fetch from the client.\")\n    except KeyError as e:\n        logger.error(f\"Key error in processing document batch: {e}\")\n    except json.JSONDecodeError as e:\n        logger.error(f\"Error decoding JSON from message: {e}\")\n        raise\n    except Exception as e:\n        logger.exception(f\"Unexpected error in next_batch: {e}\")\n```\n\n<div class=\"content-ad\"></div>\n\n이 구현에서 중요한 점들:\n\n- build_kafka_stream_client : 미리 정의된 Bytewax KafkaSource 커넥터를 사용하여 KafkaConsumer의 인스턴스를 생성합니다.\n- process_message : Kafka Topic에서 메시지를 처리할 콜백 함수입니다.\n\n2. Bytewax 플로우의 출력으로 Upstash Vector (Index)를 정의합니다.\n\n```js\nfrom typing import Optional, List\n\nfrom bytewax.outputs import DynamicSink, StatelessSinkPartition\nfrom upstash_vector import Index, Vector\nfrom models import EmbeddedDocument\nfrom settings import settings\nfrom logger import get_logger\n\n\nlogger = get_logger(__name__)\n\n\nclass UpstashVectorOutput(DynamicSink):\n    \"\"\"Upstash 벡터 출력을 나타내는 클래스입니다.\n\n    이 클래스는 at-least-once 처리를 지원하는 동적 출력 유형인 Upstash 벡터 출력을 생성하는 데 사용됩니다.\n    resume 이후의 메시지는 resume 즉각적으로 복제됩니다.\n\n    Args:\n        vector_size (int): 벡터의 크기.\n        collection_name (str, optional): 컬렉션의 이름입니다. 기본값은 constants.VECTOR_DB_OUTPUT_COLLECTION_NAME입니다.\n        client (Optional[UpstashClient], optional): Upstash 클라이언트입니다. 기본값은 None입니다.\n    \"\"\"\n\n    def __init__(\n        self,\n        vector_size: int = settings.EMBEDDING_MODEL_MAX_INPUT_LENGTH,\n        collection_name: str = settings.UPSTASH_VECTOR_TOPIC,\n        client: Optional[Index] = None,\n    ):\n        self._collection_name = collection_name\n        self._vector_size = vector_size\n\n        if client:\n            self.client = client\n        else:\n            self.client = Index(\n                url=settings.UPSTASH_VECTOR_ENDPOINT,\n                token=settings.UPSTASH_VECTOR_KEY,\n                retries=settings.UPSTASH_VECTOR_RETRIES,\n                retry_interval=settings.UPSTASH_VECTOR_WAIT_INTERVAL,\n            )\n\n    def build(\n        self, step_id: str, worker_index: int, worker_count: int\n    ) -> StatelessSinkPartition:\n        return UpstashVectorSink(self.client, self._collection_name)\n\n\nclass UpstashVectorSink(StatelessSinkPartition):\n    \"\"\"\n    Upstash Vector 데이터베이스 컬렉션에 문서 임베딩을 작성하는 싱크입니다.\n    이 구현은 효율성을 높이기 위해 배치 업서트를 활용하며, 오류 처리 및 로깅을 향상시키고 가독성 및 유지 보수성을 위해 Pythonic한 모법을 따릅니다.\n\n    Args:\n        client (Index): 쓰기에 사용할 Upstash Vector 클라이언트입니다.\n        collection_name (str, optional): 쓸 컬렉션의 이름입니다. 기본값은 UPSTASH_VECTOR_TOPIC 환경 변수의 값입니다.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Index,\n        collection_name: str = None,\n    ):\n        self._client = client\n        self._collection_name = collection_name\n        self._upsert_batch_size = settings.UPSTASH_VECTOR_UPSERT_BATCH_SIZE\n\n    def write_batch(self, documents: List[EmbeddedDocument]):\n        \"\"\"\n        구성된 Upstash Vector 데이터베이스 컬렉션에 문서 임베딩의 배치를 작성합니다.\n\n        Args:\n            documents (List[EmbeddedDocument]): 쓸 문서들입니다.\n        \"\"\"\n        vectors = [\n            Vector(id=doc.doc_id, vector=doc.embeddings, metadata=doc.metadata)\n            for doc in documents\n        ]\n\n        # 효율성을 위한 배치 업서트\n        for i in range(0, len(vectors), self._upsert_batch_size):\n            batch_vectors = vectors[i : i + self._upsert_batch_size]\n            try:\n                self._client.upsert(vectors=batch_vectors)\n            except Exception as e:\n                logger.error(f\"배치 업서트 중 예외 발생 {e}\")\n```\n\n<div class=\"content-ad\"></div>\n\n이 구현에서 중요한 사항들입니다:\n\n- UpstashVectorOutput: 다양한 대상으로 데이터를 전달하기 위해 설계된 Bytewax DynamicSink 추상화를 인스턴스화합니다. 우리의 경우, 이는 Upstash Vector Index 클라이언트 연결 위에 래핑될 것입니다.\n- UpstashVectorSink: 우리의 DynamicSink을 래핑하고 업서트 벡터를 우리의 VectorDatabase에 처리하는 기능을 담당합니다. 이 StatelessSinkPartition은 DynamicSink가 어떠한 상태도 유지하지 않고 우리의 Sink에 대한 모든 입력을 write_batch 기능 구현에 따라 처리합니다.\n\n## 나머지 Bytewax Flow 빌드하기\n\n여기 Upstash Kafka Topic에서 메시지를 가져와 정제, 수정, 분할, 삽입하고 Upstash Vector Index에 벡터를 업서트하는 저희 DataFlow의 전체 구현입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n\"\"\"\n    이 스크립트는 Upstash 사용 사례에 대한 ByteWax 데이터플로 구현을 정의합니다.\n    데이터플로에는 다음 단계가 포함되어 있습니다:\n        1. 입력: 카프카 스트림에서 데이터를 읽기\n        2. 정제: 입력 데이터를 공통 형식으로 변환 \n        3. 청크 분리: 입력 데이터를 더 작은 청크로 분리\n        4. 임베드: 입력 데이터에 대한 임베딩 생성\n        5. 출력: 출력 데이터를 Upstash 벡터 데이터베이스에 쓰기\n\"\"\"\n\nfrom pathlib import Path\nfrom typing import Optional\n\nimport bytewax.operators as op\nfrom vector import UpstashVectorOutput\nfrom consumer import process_message, build_kafka_stream_client\nfrom bytewax.connectors.kafka import KafkaSource\nfrom bytewax.dataflow import Dataflow\nfrom bytewax.outputs import DynamicSink\nfrom embeddings import TextEmbedder\nfrom models import ChunkedDocument, EmbeddedDocument, RefinedDocument\nfrom logger import get_logger\n\nlogger = get_logger(__name__)\n\n\ndef build(\n    model_cache_dir: Optional[Path] = None,\n) -> Dataflow:\n    \"\"\"\n    Upstash 사용 사례에 대한 ByteWax 데이터플로를 구축합니다.\n    다음과 같은 데이터플로를 따릅니다:\n        * 1. Tag: ['kafka_input']   = KafkaSource에서 입력 데이터 읽기\n        * 2. Tag: ['map_kinp']      = KafkaSource에서 CommonDocument로 메시지 처리\n            * 2.1 [Optional] Tag ['dbg_map_kinp'] = ['map_kinp'] 후 디버깅\n        * 3. Tag: ['refine']        = 메시지를 정제된 문서 형식으로 변환\n            * 3.1 [Optional] Tag ['dbg_refine'] = ['refine'] 후 디버깅\n        * 4. Tag: ['chunkenize']    = 정제된 문서를 더 작은 청크로 나누기\n            * 4.1 [Optional] Tag ['dbg_chunkenize'] = ['chunkenize'] 후 디버깅\n        * 5. Tag: ['embed']         = 청크에 대한 임베딩 생성\n            * 5.1 [Optional] Tag ['dbg_embed'] = ['embed'] 후 디버깅\n        * 6. Tag: ['output']        = 임베딩을 Upstash 벡터 데이터베이스에 쓰기\n    노트:\n        각 선택적 태그는 문제 해결을 위해 활성화할 수 있는 디버깅 단계입니다.\n    \"\"\"\n    model = TextEmbedder(cache_dir=model_cache_dir)\n\n    dataflow = Dataflow(flow_id=\"news-to-upstash\")\n    stream = op.input(\n        step_id=\"kafka_input\",\n        flow=dataflow,\n        source=_build_input(),\n    )\n    stream = op.flat_map(\"map_kinp\", stream, process_message)\n    # _ = op.inspect(\"dbg_map_kinp\", stream)\n    stream = op.map(\"refine\", stream, RefinedDocument.from_common)\n    # _ = op.inspect(\"dbg_refine\", stream)\n    stream = op.flat_map(\n        \"chunkenize\",\n        stream,\n        lambda refined_doc: ChunkedDocument.from_refined(refined_doc, model),\n    )\n    # _ = op.inspect(\"dbg_chunkenize\", stream)\n    stream = op.map(\n        \"embed\",\n        stream,\n        lambda chunked_doc: EmbeddedDocument.from_chunked(chunked_doc, model),\n    )\n    # _ = op.inspect(\"dbg_embed\", stream)\n    stream = op.output(\"output\", stream, _build_output())\n    logger.info(\"성공적으로 ByteWax 데이터플로를 생성했습니다.\")\n    logger.info(\n        \"\\t단계: Kafka 입력 -> 매핑 -> 정제 -> 청크 분리 -> 임베딩 -> 업로드\"\n    )\n    return dataflow\n\n\ndef _build_input() -> KafkaSource:\n    return build_kafka_stream_client()\n\n\ndef _build_output() -> DynamicSink:\n    return UpstashVectorOutput()\n```\n\n<div class=\"content-ad\"></div>\n\n- kafka_input: 카프카 메시지를 가져와 CommonDocument Pydantic 형식으로 변환하는 단계입니다.\n- map_kinp: 카프카 입력을 의미하며, 수신된 메시지에 flat map을 적용하여 List[CommonDocument] Pydantic 객체를 생성합니다.\n- refine: List[CommonDocument]를 순회하고 RefinedDocument 인스턴스를 생성하는 단계입니다.\n- chunkenize: List[RefinedDocument]를 순회하고 ChunkedDocument 인스턴스를 생성하는 단계입니다.\n- embed: List[ChunkedDocuments]를 순회하고 EmbeddedDocument 인스턴스를 생성하는 단계입니다.\n- output: List[EmbeddedDocument]를 순회하고 Vector 객체를 생성하여 Upstash Vector Index에 업서트하는 단계입니다.\n\n# 파이프라인 시작\n\n지금까지 구현한 것은 다음과 같습니다:\n\n- 데이터 수집 파이프라인: 주기적으로 NewsAPI에서 원시 페이로드를 가져와 형식을 지정한 뒤, 카프카 토픽으로 메시지를 전송하는 단계입니다.\n- 인제션 파이프라인: 이는 Bytewax DataFlow로, 카프카 토픽에 연결되어 메시지를 소비하고, 최종적으로 벡터를 업서트하는 Vector 데이터베이스에 업데이트합니다.\n\n<div class=\"content-ad\"></div>\n\n프로젝트 루트에있는 Makefile에서 미리 정의된 명령을 사용하여 이 두 서비스를 모두 시작할 수 있습니다:\n\n```js\n# 카프카 메시지를 생성하기 위해 데이터 수집 파이프라인 실행\nmake run_producers\n\n# 카프카 메시지를 소비하고 벡터를 업데이트하기 위해 인제스처리 파이프라인 실행\nmake run_pipeline\n```\n\n그리고... 완료되었습니다!\n성공적으로 프로듀서/컨슈머 서비스를 시작했습니다.\n남은 모듈은 UI입니다. 벡터 데이터베이스와 뉴스 기사를 검색하는 데 상호 작용하는 데 사용됩니다.\n\n# 사용자 인터페이스\n\n<div class=\"content-ad\"></div>\n\nUI는 다음과 같은 기능을 갖춘 기본 Streamlit [8] 애플리케이션입니다:\n\n- 텍스트 검색 창\n- 벡터 데이터베이스에서 가져온 기사로 채워진 카드가 표시되는 div 섹션\n\n카드에는 다음과 같은 데이터 필드가 포함되어 있습니다:\n\n- 발행일\n- 유사도 점수\n- 기사 이미지\n- SeeMore 버튼을 클릭하면 원본 기사 URL로 이동합니다.\n\n<div class=\"content-ad\"></div>\n\n한번 메시지/질문을 텍스트 상자에 입력하면 입력이 정리되고 (소문자로 변환, 비ASCII 문자 제거 등) 그리고 삽입됩니다. 새로운 삽입물을 사용하여 벡터 데이터베이스를 쿼리하여 가장 유사한 항목을 가져옵니다. 그 결과는 구성되어 렌더링될 것입니다.\n\n다음은 예시입니다:\n\n\n![How to build a real-time News Search Engine using VectorDBs - Part 1](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_11.png)\n\n![How to build a real-time News Search Engine using VectorDBs - Part 2](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_12.png)\n\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n축하합니다!\n\n성공했습니다! 멋진 프로젝트만큼이나 라이브로 출시할 준비가 된 뉴스 검색 엔진을 만들었습니다. 단순히 무작정 던지는 것이 아니라, 우리는 최고의 소프트웨어 개발 관행을 따르기도 했습니다.\n\nPydantic을 사용하여 데이터를 잘 처리했고, 유닛 테스트를 작성하고, 스레딩을 활용하여 작업을 가속화했으며, Upstash의 서버리스 카프카와 벡터 데이터베이스를 활용하여 파이프라인을 쉽게 설정할 뿐만 아니라 빠르고 확장 가능하며 오류 대비 가능하도록 만들었습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 당신은 이 청사진을 대부분의 데이터 기반 아이디어에 적용할 수 있는 능력을 갖게 되었어요. 이건 이 프로젝트뿐만 아니라 앞으로 만들게 될 멋진 것들을 위한 큰 승리에요.\n\n# 참고 자료\n\n[1] News Search Engine using Upstash Vector — Decoding ML Github (2024)\n[2] Upstash Serverless Kafka\n[3] Upstash Serverless Vector Database\n[4] Bytewax Stream Processing with Python\n[5] Singleton Pattern\n[6] sentence-transformers/all-MiniLM-L6-v2\n[7] unstructured Python Library\n[8] Streamlit Python\n\n# 더 읽을 거리\n\n<div class=\"content-ad\"></div>\n\n이 글과 관련성 순으로 정렬되었습니다.","ogImage":{"url":"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_0.png"},"coverImage":"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_0.png","tag":["Tech"],"readingTime":28},{"title":"파이썬으로 간단한 ETL 파이프라인 구축하기 초보자를 위한 안내","description":"","date":"2024-06-19 16:10","slug":"2024-06-19-BuildingaSimpleETLPipelineinPythonABeginnersGuide","content":"\n\n소개:\n\nETL (추출, 변환, 로드)은 데이터 엔지니어링에서 필수적인 프로세스로, 다양한 소스에서 데이터를 수집, 변환 및 저장하는 기능을 제공합니다. 이 안내서에서는 Python을 사용하여 날씨 데이터를 API에서 추출, 변환하고 CSV 파일에 저장하는 간단한 ETL 파이프라인을 구축하는 방법을 안내해 드리겠습니다.\n\n설치:\n\n시작하려면 Python 3.8 이상 버전 및 몇 가지 라이브러리가 설치되어 있어야 합니다. 터미널을 열고 다음 명령을 실행하세요:\n\n<div class=\"content-ad\"></div>\n\n```js\npip install requests pandas\n```\n\n**단계 1: 날씨 데이터 추출 기능용 액세스 키 얻기**\n\n날씨 데이터를 가져오기 위해 OpenWeatherMap API를 사용할 것입니다. 먼저 API 키를 얻어야 합니다.\n\n1. API 키 등록하세요:\n\n\n<div class=\"content-ad\"></div>\n\n- OpenWeatherMap 웹사이트로 이동해 주세요.\n- 무료 계정을 등록하고 API 키를 획득해 주세요.\n\n**2. API 문서:**\n\n- 현재 날씨 데이터 섹션으로 이동해 주세요.\n- \"Current Weather Data\"를 클릭한 다음 \"API 문서\"를 클릭해 주세요.\n- 요청하는 방법을 이해하기 위해 \"도시 이름으로 내장된 API 요청\"으로 스크롤해 주세요.\n\n**단계 2: 함수 추출**\n\n<div class=\"content-ad\"></div>\n\n이제 API 키가 있으니, API에서 날씨 데이터를 가져오는 추출 함수를 작성해 봅시다.\n\n```js\n# 중요한 라이브러리들\n\nimport requests\nimport pandas as pd\n```\n\n```js\ndef fetch_weather_data(city: str, api_key: str):\n    url = f'http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric'\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.json()\n    else:\n        return None\n\n# 예시 사용법\napi_key = '당신의_openweathermap_api_키'\ncity = '런던'\nweather_data = fetch_weather_data(city, api_key)\nprint(weather_data)\n```\n\nfetch_weather_data 함수는 도시 이름과 API 키를 사용하여 API 요청 URL을 작성하고, API로 GET 요청을 보내 성공적인 요청이면 JSON 형식의 응답 데이터를 반환합니다.\n\n<div class=\"content-ad\"></div>\n\n### 단계 3: 변환 함수\n\n이제 데이터를 보다 구조화된 형식으로 변환할 것입니다:\n\n```js\ndef transform_weather_data(data: dict):\n    if not data:\n        return None\n\n    weather_info = {\n        'city': data['name'],\n        'temperature': data['main']['temp'],\n        'humidity': data['main']['humidity'],\n        'weather': data['weather'][0]['description'],\n        'wind_speed': data['wind']['speed'],\n        'wind_deg': data['wind']['deg']\n    }\n    return weather_info\n\n# 예제 사용법\ntransformed_data = transform_weather_data(weather_data)\nprint(transformed_data)\r\n```\n\n### 단계 4: 모든 단계를 결합하는 함수\n\n<div class=\"content-ad\"></div>\n\n마지막으로, 변환된 데이터를 CSV 파일로 로드하겠습니다:\n\n```js\ndef load_data_to_csv(data: dict, file_path: str):\n    df = pd.DataFrame([data])\n    df.to_csv(file_path, index=False)\n\n# 사용 예시\noutput_file = 'weather_data.csv'\nload_data_to_csv(transformed_data, output_file)\nprint(f'{output_file}에 데이터 저장 완료')\n```\n\n모든 단계를 통합한 완전한 스크립트입니다:\n\n<div class=\"content-ad\"></div>\n\n```python\ndef fetch_weather_data(city: str, api_key: str):\n    url = f'http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric'\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.json()\n    else:\n        return None\n\ndef transform_weather_data(data: dict):\n    if not data:\n        return None\n\n    weather_info = {\n        'city': data['name'],\n        'temperature': data['main']['temp'],\n        'humidity': data['main']['humidity'],\n        'weather': data['weather'][0]['description'],\n        'wind_speed': data['wind']['speed'],\n        'wind_deg': data['wind']['deg']\n    }\n    return weather_info\n\ndef load_data_to_csv(data: dict, file_path: str):\n    df = pd.DataFrame([data])\n    df.to_csv(file_path, index=False)\n\ndef main():\n    api_key = 'your_openweathermap_api_key'\n    city = 'London'\n    weather_data = fetch_weather_data(city, api_key)\n    if weather_data:\n        transformed_data = transform_weather_data(weather_data)\n        output_file = 'weather_data.csv'\n        load_data_to_csv(transformed_data, output_file)\n        print(f'Data saved to {output_file}')\n    else:\n        print('Failed to fetch data')\n\nif __name__ == '__main__':\n    main()\n```\n\n**결론**\n\n이번 튜토리얼에서는 Python으로 간단한 ETL(추출, 변환, 적재) 파이프라인을 만들었습니다. API에서 데이터를 가져와 변환한 뒤 CSV 파일로 저장했습니다. 이 기본 프로젝트를 통해 데이터 엔지니어링 및 분석에서 중요한 ETL 프로세스를 이해할 수 있습니다.\n\n**추가 자료**\n\n\n<div class=\"content-ad\"></div>\n\n- OpenWeatherMap API","ogImage":{"url":"/assets/img/2024-06-19-BuildingaSimpleETLPipelineinPythonABeginnersGuide_0.png"},"coverImage":"/assets/img/2024-06-19-BuildingaSimpleETLPipelineinPythonABeginnersGuide_0.png","tag":["Tech"],"readingTime":4}],"page":"75","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}