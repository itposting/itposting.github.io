{"pageProps":{"posts":[{"title":"다중 헤드 어텐션  공식적으로 설명하고 정의하기","description":"","date":"2024-06-19 19:01","slug":"2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined","content":"\n\n![이미지](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_0.png)\n\n다중 헤드 어텐션은 자연어 처리를 혁신적으로 바꾼 트랜스포머에서 중요한 역할을 합니다. 이 메커니즘을 이해하는 것은 현재 최첨단 언어 모델의 명확한 그림을 그리는 데 필수적인 단계입니다.\n\n몇 년 전에 소개되었고 그 후 널리 사용되고 논의되었음에도 불구하고 모호한 표기법과 형식적인 정의의 부족으로 인해 새로운 이들이 빠르게 다중 헤드 어텐션 메커니즘을 해소할 수 없었습니다.\n\n이 기사에서는 다중 헤드 어텐션의 포관적이고 모호하지 않은 형식화를 제시하여 이 메커니즘을 쉽게 이해할 수 있도록하는 것이 목표입니다.\n\n<div class=\"content-ad\"></div>\n\n새로운 개념을 더 잘 이해하기 위해서는 스스로 사용하는 것이 중요합니다. 이 기사에는 다중 헤드 어텐션에 대해 정확히 이해하기 위한 여러 연습 문제/질문과 해결책이 함께 제시됩니다.\n\n참고: 다중 헤드 어텐션의 정의와 설명을 시작하기 전에, 라텍스 지원 부족으로 수식을 이미지로 변환하여 수학적 객체를 표시했습니다.\n\n## 입력\n\n먼저, 다중 헤드 어텐션 메커니즘의 입력이 무엇인지 명확히 합시다. 우리는 다중 헤드 어텐션 레이어의 입력을 다음과 같이 정의합니다:\n\n<div class=\"content-ad\"></div>\n\n이곳이 있어요! \n\nX는 n개의 행과 d개의 열로 이루어진 행렬이에요. 그 두 차원은 다음과 같이 대응되어요:\n\n- n: 입력 시퀀스의 길이 (토큰 수)\n- d: 토큰 벡터의 차원.\n\n## 시퀀스 길이\n\n<div class=\"content-ad\"></div>\n\n일반적으로 모든 시퀀스에 대해 기본 고정 길이 n을 설정합니다. 일반적으로 \"Attention Is All You Need\" (Vaswani et al., 2017에서 사용된 값과 같은) 256 또는 512와 같은 값이 사용됩니다. 그런 다음, 모든 입력 시퀀스는 정확히 n개의 토큰을 가지도록 패딩되거나 자르게됩니다.\n\n이것은 시퀀스의 단어 수로 볼 수 있습니다.\n\n따라서 X의 각 행은 입력 시퀀스의 토큰에 해당합니다.\n\n## 토큰 차원\n\n<div class=\"content-ad\"></div>\n\n텍스트를 토큰화하면 각각의 토큰이 특성 벡터로 표현된 시퀀스로 변환됩니다.\n\n토큰 벡터의 가장 간단한 예시는 원핫 인코딩입니다. 예를 들어, 1,000개의 가능한 토큰 세트가 있다면, 각 토큰은 1,000차원의 이진 벡터로 모델링될 수 있습니다. 여기서 각 토큰은 0으로 설정된 999개의 구성 요소와 주어진 토큰에 연결된 구성 요소에 1로 설정된 1개를 가지게 됩니다. 여기서 d는 1,000입니다.\n\n따라서 입력 시퀀스의 n개의 각 토큰은 X의 행으로 표시되며 d개의 열이 있습니다.\n\n## 연습문제 1 — 입력 시퀀스 모델링\n\n<div class=\"content-ad\"></div>\n\n아래의 가정에 따라 문장 \"attention is all you need\"을 모델링하고자 합니다:\n\n- 입력 시퀀스의 고정된 길이는 n = 8입니다.\n- 모델링될 수 있는 토큰의 집합 T = '“all”, “attention”, “cat”, “is”, “need”, “transformer”, “you”'입니다.\n- 토큰 벡터는 가능한 토큰 집합 T를 기반으로 한 원핫 인코딩 표현입니다.\n- \"End-of-Sequence\" 토큰과 같은 특수 토큰은 고려하지 않습니다. (만약 이 가정을 이해하지 못한다면, 괜찮습니다.)\n\nQ1: 여기서 토큰 벡터의 차원 d는 무엇인가요?\n\nQ2: 방금 전에 한 가정에 따라 \"attention is all you need\"의 입력 행렬 X를 제시해주세요.\n\n<div class=\"content-ad\"></div>\n\n## 요약\n\n## 해결책 — 연습 문제 1\n\nQ1: 여기서 가능한 토큰 집합 T에는 일곱 개의 토큰이 있고, 토큰을 원핫 인코딩으로 모델링한다고 가정했습니다. 따라서 d = 7 (T에있는 토큰 수).\n\n<div class=\"content-ad\"></div>\n\nQ2: 먼저, n = 8이고 d = 7인 것을 알았기 때문에, 여덟 개의 행과 일곱 개의 열을 가진 행렬 X를 얻어야 한다는 것을 알 수 있습니다.\n\n각 행은 입력 문장의 토큰에 대응해야 합니다. 그러나 알 수 있듯이, “attention is all you need”에는 다섯 개의 토큰만 포함되어 있습니다. 행렬 X에 여덟 개의 행을 어떻게 채울 수 있을까요?\n\n한 가지 옵션은 입력 시퀀스를 빈 토큰으로 채우는 것입니다. 이렇게 하면 X의 맨 아래에 세 개의 빈 행이 생기게 됩니다.\n\n참고: 또 다른 접근 방식은 빈 토큰을 가능한 토큰 집합에 포함하는 것입니다. 즉, T = '“all”, “attention”, “cat”, “is”, need”, “transformer”, “you”, ∅'와 같이 ∅가 빈 토큰을 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_2.png\" />\n\n이제 한-hot 벡터를 올바르게 작성하여 X를 완료했습니다. 최종적으로 다음 행렬이 나와야 합니다:\n\n<img src=\"/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_3.png\" />\n\n# Query, Key, Value\n\n<div class=\"content-ad\"></div>\n\n\"다중 헤드 어텐션”에서는 “다중 헤드”가 있습니다. 이는 이러한 메커니즘을 사용하는 레이어가 여러 개의 헤드를 사용한다는 것을 의미합니다.\n\n만일 “헤드”라는 용어가 이해되지 않는다면, 이를 “매핑”이나 “레이어”로 생각하실 수 있습니다.\n\nh를 다중 헤드 어텐션 레이어에서 사용될 어텐션 헤드의 수로 정의해봅시다.\n\n이제 쿼리(질의), 키(검색어), 값에 대해 정의해보겠습니다.\"\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_4.png)\n\n서로 다른 이름을 가지고 있지만, 쿼리, 키, 값은 본질적으로 동일한 정의를 가지고 있습니다: 입력 행렬과 가중치 행렬의 곱입니다.\n\n여기서 k와 v가 양의 정수일 때, 가중치 행렬은 다음 행렬 공간에 있습니다:\n\n![image](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_5.png)\n\n\n<div class=\"content-ad\"></div>\n\n가중 행렬은 h 개의 헤드 간에 다릅니다. 따라서 아래 첨자는 i이며, i는 1에서 h까지 범위입니다.\n\n쿼리, 키 및 값 사이에는 가중 행렬이 다르므로 \"Q\", \"K\" 및 \"V\"의 지수가 있어서 이들을 구별합니다.\n\n논문 \"Attention Is All You Need\" (Vaswani et al., 2017)에서는 일반적으로 k 및 v가 k = v = d / h로 설정됩니다.\n\n## 문제 2 — 쿼리, 키 및 값\n\n<div class=\"content-ad\"></div>\n\n쿼리 Q의 차원은 무엇인가요? 키 K의 차원은 무엇인가요? 값 V의 차원은 무엇인가요?\n\n(아래 \"요약\" 이후에 솔루션 있음.)\n\n## 요약\n\n<div class=\"content-ad\"></div>\n\n## 해결책 — 연습문제 2\n\n행렬 곱셈을 주의 깊게 읽은 후, 쿼리, 키, 밸류 행렬의 다음 차원을 얻어야 합니다:\n\n![Matrix Dimensions](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_6.png)\n\n# 어텐션\n\n<div class=\"content-ad\"></div>\n\n주의는 다음 매핑으로 정의될 수 있습니다:\n\n![매핑](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_7.png)\n\n여기서 softmax 함수가 행을 따라 실행됩니다:\n\n![softmax](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_8.png)\n\n<div class=\"content-ad\"></div>\n\n알림으로써, 소프트맥스 함수는 벡터의 값을 정규화하는 방법입니다. 이 함수를 사용하면 합이 1이 되도록 벡터를 변환하여 소프트맥스의 출력이 확률 분포를 나타낼 수 있게 합니다.\n\n## Exercise 3 — 어텐션과 행렬의 차원\n\n다음 행렬들의 차원은 무엇인가요?\n\n![행렬 이미지](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_9.png)\n\n<div class=\"content-ad\"></div>\n\n## Exercise 4 — 주의 집중 해석\n\n다음 구절을 해당 수식과 일치시키세요. 주어진 구절은 이전에 소개된 적은 없지만, 수학적 표현을 해석함으로써 여기서 추측할 수 있습니다.\n\n이미지 태그를 Markdown 형식으로 변경하세요.\n\n## Exercise 5 — 행렬 해석\n\n<div class=\"content-ad\"></div>\n\nQ1: 주어진 쿼리 Q에 대해,\n\n- Q의 한 행은 무엇을 나타내나요?\n- Q의 한 열은 무엇을 나타내나요?\n\n(동일한 해석은 키 K 및 값 V에 대해서도 가능합니다.)\n\nQ2: 주어진 쿼리 Q와 주어진 키 K, 행 i, 열 j에 대해, 아래와 같은 계수가 나타내는 것은 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_11.png)\n\nQ3: 특정 쿼리 Q와 주어진 키 K에 대해 다음 매트릭스의 한 행을 어떻게 해석할 수 있나요?\n\n![Image](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_12.png)\n\n(아래 \"Recap\" 이후에 해답이 있습니다.)\n\n\n<div class=\"content-ad\"></div>\n\n## 요약\n\n![image](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_13.png)\n\n## 해결책 — 연습 문제 3\n\n아래 행렬들에 대한 차원을 순차적으로 얻습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_14.png)\n\n## Solution — Exercise 4\n\n![Image](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_15.png)\n\nThe (scaled) attention scores are the dot products between the projections of the input tokens both in the query space and the key space. They provide a raw score of “how much each input token attends to each input token”.\n\n\n<div class=\"content-ad\"></div>\n\n위 균도 가중치는 이전에 언급한 주의 점수에서 확률 분포를 만듭니다.\n\n스케일링 닷-프로덕트 주의는 값 공간에서 주의 가중치와 입력 투영을 기반으로 입력 시퀀스의 새로운 잠재 표현을 제공합니다.\n\n## 해결책 — 연습 문제 5\n\n질문 1: 쿼리 Q가 쿼리 공간에서 입력 X의 투영이며, Q의 치수에 기초하여 추론할 수 있는 내용은 무엇입니까?\n\n<div class=\"content-ad\"></div>\n\n- Q의 한 행은 질의 공간에서 표현된 입력 토큰 벡터를 나타냅니다.\n- Q의 한 열은 질의 공간의 잠재적 차원을 나타냅니다.\n\nQ2: Q와 K의 전치(Transpose)의 곱셈에서 i번째 행과 j번째 열의 계수는 입력 시퀀스의 i번째 토큰이 j번째 토큰과 얼마나 관련되어 있는지를 나타냅니다.\n\n이 계수가 높을수록, i번째 토큰과 j번째 토큰이 더 연관되어(~유사하게) 있습니다.\n\nQ3: 먼저, 어텐션 가중치 행렬의 각 행에 대한 합이 1과 같아집니다.\n\n<div class=\"content-ad\"></div>\n\n해당 행렬의 i행은 입력 시퀀스 토큰이 시퀀스 의미에 기여하는 정도를 모델링한 확률 분포로 해석할 수 있습니다. 토큰 i를 살펴보는 경우 시퀀스 의미에 어떤 기여를 하는지를 나타냅니다.\n\n# 다중 헤드 어텐션\n\n이제, 다중 헤드 어텐션 메커니즘에서 각 헤드들은 어떻게 작용할까요?\n\n## 연결\n\n<div class=\"content-ad\"></div>\n\n서로 다른 헤드의 출력은 행을 따라 연결되고 출력 가중치 행렬과 곱해집니다:\n\n![image](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_16.png)\n\n다시 말해, 이것은 기술적인 내용이 아니며, 가장 어려운 부분은 여러 행렬의 차원을 명심하는 것입니다.\n\n## Exercise 6 — Multi-head dimensions\n\n<div class=\"content-ad\"></div>\n\n다음 행렬의 차원은 무엇입니까?\n\n![image](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_17.png)\n\n## Exercise 7 — 결과 해석\n\n여러 헤드 어텐션은 처음에 기계 번역에 적용되어 영어와 독일어 간의 문장을 번역했습니다.\n\n<div class=\"content-ad\"></div>\n\n입력 X와 출력 Y = MultiHead(X)의 차원을 주석으로 남겨주세요.\n\n## 연습 8 — 왜 여러 개?\n\n왜 여러 개의 어텐션 헤드가 필요할까요? 특히, 서로 다른 헤드가 어디에서 상호 작용하는지 확인할 수 있나요?\n\n(아래의 \"요약\" 이후 답안이 제시됩니다.)\n\n<div class=\"content-ad\"></div>\n\n## 요약\n\n## 해결책 — 연습문제 6\n\n다중 헤드 어텐션에 개입하는 이 행렬들의 차원은 다음과 같습니다:\n\n![이미지](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_18.png)\n\n<div class=\"content-ad\"></div>\n\n## 해결 방법 — 연습문제 7\n\n입력 X의 차원과 출력 Y = MultiHead(X)의 차원이 일치하는 것을 알아채셨을 것입니다. 둘 다 n x d입니다.\n\n따라서, 다중 헤드 어텐션의 출력은 입력의 재구성으로 해석될 수 있거나, 적어도 동일한 고정 길이의 다른 시퀀스로 해석될 수 있습니다.\n\n다중 헤드 어텐션이 머신 번역을 위해 처음에 소개되었기 때문에, 출력은 다음과 같이 해석할 수 있습니다: 영어로 모델링된 입력 시퀀스에 대한 입력이 주어지면, 다중 헤드 어텐션 레이어는 독일어(또는 다른 어떤 언어든)로의 입력 시퀀스 번역을 모델링하는 행렬 Y를 출력합니다.\n\n<div class=\"content-ad\"></div>\n\n## 해결책 — 연습문제 8\n\n먼저, 여러 다른 헤드를 사용하는 아이디어는 토큰 간에 다른 관계에 대해 여러 헤드가 참석하도록하는 것입니다.\n\n실제로, 다양한 헤드들은 다중 헤드 어텐션의 매우 끝에서 함께 상호작용합니다:\n\n![다중 헤드 어텐션](/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_19.png)\n\n<div class=\"content-ad\"></div>\n\n각 토큰에 대해 모든 헤드의 출력 값을 합산합니다. 이는 서로 다른 헤드가 상호 작용하는 곳입니다.\n\n# 결론\n\n종합하면, 다중 헤드 어텐션은 입력 시퀀스 X를 출력 시퀀스 Y로 변환하기 위해 두 가지를 활용하는 구성 요소로 이루어져 있습니다:\n\n- 어텐션 메커니즘\n- 여러 어텐션 헤드의 연결\n\n<div class=\"content-ad\"></div>\n\n이 메커니즘을 올바르게 이해하기 위해서는 계산 중 사용된 모든 행렬의 차원을 명확히 확인하는 것이 가장 중요합니다.","ogImage":{"url":"/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_0.png"},"coverImage":"/assets/img/2024-06-19-Multi-HeadAttentionFormallyExplainedandDefined_0.png","tag":["Tech"],"readingTime":9},{"title":"N-HiTS  시계열 예측을 위한 딥 러닝을 더 효율적으로 만드는 방법","description":"","date":"2024-06-19 18:57","slug":"2024-06-19-N-HiTSMakingDeepLearningforTimeSeriesForecastingMoreEfficient","content":"\n\n<img src=\"/assets/img/2024-06-19-N-HiTSMakingDeepLearningforTimeSeriesForecastingMoreEfficient_0.png\" />\n\n2020년에 N-BEATS는 시계열 예측에서 통계적 및 혼합 모델보다 뛰어난 성과를 보인 첫 딥러닝 모델이었습니다.\n\n그 후 2년 뒤인 2022년에 새로운 모델이 N-BEATS를 제쳐놓았습니다. Challu와 Olivares 등이 딥 러닝 모델 N-HiTS를 발표했습니다. 이들은 N-BEATS의 긴 예측 지표에 대한 두 가지 단점을 해결했습니다:\n\n- 정확도의 감소 및\n- 계산량의 증가.\n\n<div class=\"content-ad\"></div>\n\nN-HiTS는 시계열 예측을 위한 Neural Hierarchical Interpolation의 약자입니다.\n\n이 모델은 N-BEATS 및 그 신경 기반 확장 아이디어에 기반을 두고 있습니다. 신경 기반 확장은 계층화된 스택을 통해 여러 블록에서 이루어집니다.\n\n이 글에서는 N-HiTS 뒤에 숨겨진 구조, 특히 N-BEATS와의 차이에 대해 살펴볼 것입니다. 하지만 너무 걱정하지 마세요, 깊이 파고들기가 이해하기 쉬울 거에요. 하지만 N-HiTS 작동 방식을 이해하는 것만으로 충분하지 않습니다. 따라서 Python에서 어떻게 N-HiTS 모델을 쉽게 구현하고 하이퍼파라미터를 튜닝할 수 있는지 보여드릴 거에요.\n\n# 핵심 아이디어가 같다면, N-BEATS와 N-HiTS의 차이는 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n각 모델이 각 스택의 입력과 출력을 처리하는 방식에 차이가 있습니다. N-HiTS의 주요 아이디어는 서로 다른 시간 스케일의 예측을 결합하는 것입니다.\n\n이를 위해 N-HiTS는\n\n- 입력의 다중 속도 데이터 샘플링 및\n- 출력의 계층적 보간\n\n을 적용합니다.\n\n이 방법으로 N-HiTS는 더 긴 시계열에 대해 더 나은 정확도와 낮은 계산 비용을 달성합니다.\n\n<div class=\"content-ad\"></div>\n\n다중 비율 데이터 샘플링은 스택이 단기 또는 장기적 영향에 특화되도록 만듭니다. 이러한 스택이 각각의 구성 요소를 학습하기 쉬워집니다. 장기적 행동에 중점을 둔 것은 N-BEATS에 비해 향상된 장기적 시계열 예측을 이끌어냅니다.\n\n계층적 보간은 각 블록이 서로 다른 시간 단위로 예측하도록 허용합니다. 그런 다음 모델은 각 블록의 시간 단위에 맞게 예측을 보간하여 최종 예측과 일치시킵니다. 재샘플링과 보간은 학습 가능한 매개변수의 수를 줄입니다. 이는 훈련 시간이 더 짧고 가벼운 모델로 이어집니다.\n\n이제 N-HiTS가 어떤 점에서 다르게 작동하는지 알았으니, 이러한 변화가 아키텍처에 어떻게 포함되어 있는지 살펴보겠습니다.\n\n# N-HiTS는 자세히 어떻게 작동하나요?\n\n<div class=\"content-ad\"></div>\n\nN-HiTS 모델은 다음과 같은 아키텍처를 가지고 있어요:\n\n![N-HiTS 모델](/assets/img/2024-06-19-N-HiTSMakingDeepLearningforTimeSeriesForecastingMoreEfficient_1.png)\n\n우리가 볼 수 있듯이, N-BEATS와 많은 유사성을 가지고 있어요.\n\n첫째, N-HiTS는 시계열을 lookback 및 forecast 기간으로 분리해요. 둘째, 해당 모델은 다층 스택과 블록으로 구성되어 있어요, backcast와 forecast를 생성해요. 각 블록에서 다층 퍼셉트론은 backcast와 forecast를 위한 기저 확장 계수를 생성해요. backcast는 블록이 포착한 시계열의 일부를 보여줘요. 우리는 이전 블록의 backcast를 제거하고 시계열을 블록에 전달하기 전에요. 이를 통해 각 블록은 서로 다른 패턴을 학습하게 되어요, 블록 간에 잔차만 전달하기 때문이에요. 해당 모델은 모든 블록의 forecast를 합하여 최종 예측을 생성해요.\n\n<div class=\"content-ad\"></div>\n\n유사한 부분에 대해서는 이 정도로 설명을 유지하겠습니다. 더 자세한 정보는 제 N-BEATS 글을 참조하시기 바랍니다.\n\n하지만 차이점에 대해서는 더 깊이 파헤쳐 보겠습니다: 다중 속도 데이터 샘플링과 계층적 보간.\n\n## 입력의 다중 속도 신호 샘플링\n\nN-HiTS는 MaxPool 레이어를 통해 블록 수준에서 다중 속도 샘플링을 수행합니다.\n\n<div class=\"content-ad\"></div>\n\nMaxPool 레이어는 선택된 커널 크기 내에서 가장 큰 값을 취함으로써 입력을 평활화합니다. 따라서 커널 크기가 샘플링 속도를 결정합니다. 커널 크기가 클수록 평활화가 더 강해집니다.\n\n![이미지](/assets/img/2024-06-19-N-HiTSMakingDeepLearningforTimeSeriesForecastingMoreEfficient_2.png)\n\nMaxPool 레이어의 커널 크기는 스택 레벨에서 정의합니다. 따라서 동일한 스택 내의 각 블록은 동일한 커널 크기를 갖습니다.\n\nN-HiTS는 리샘플링을 위해 위에서 아래로 접근하는 방식을 사용합니다. 첫 번째 스택은 큰 커널 크기를 통해 장기적인 효과에 초점을 맞춥니다. 후속 스택은 작은 커널 크기를 통해 단기적인 효과에 초점을 맞춥니다.\n\n<div class=\"content-ad\"></div>\n\n## 출력의 계층 적 보간\n\nN-HiTS는 각 스택의 예측 수, 즉 기수를 줄이기 위해 계층 적 보간을 사용합니다. 더 작은 기수는 장기 예측을 위한 계산 요구 사항을 줄여줍니다.\n\n이게 무슨 말인가요?\n\n시계열의 다음 24시간을 예측하려고 한다고 가정해 봅시다. 우리는 우리 모델이 24개의 예측을 출력할 것을 기대합니다(각 시간당 하나). 시간당 데이터의 다음 두 주를 예측하려면 336개의 예측이 필요합니다(14 * 24). 그게 맞죠?\n\n<div class=\"content-ad\"></div>\n\n그러나 이것이 문제가 되는 곳입니다. N-BEATS 모델을 예를 들어봅시다. 최종 예측은 각 스택의 부분 예측을 결합한 것입니다. 따라서 각 스택은 336개의 값을 예측해야 하며, 이는 계산 비용이 많이 듭니다. N-BEATS는 더 긴 예측 범위에서 같은 문제를 겪는 모델 중 하나에 불과합니다. 다른 딥러닝 접근 방식인 트랜스포머나 순환 신경망과 같은 모델들도 같은 문제에 직면하게 됩니다.\n\nN-HiTS는 각 스택이 서로 다른 시간 스케일에서 예측하도록하여 이 도전을 극복합니다. N-HiTS는 각 스택의 시간 스케일을 내삽을 사용하여 최종 출력에 일치시킵니다.\n\n이를 위해 N-HiTS는 표현 간격 비율 개념을 사용합니다. 이 비율은 예측 기간 내의 예측 수를 결정합니다. 작은 표현 간격 비율은 스택이 더 적은 예측을 하도록 만듭니다. 따라서 스택의 기수가 작아집니다. 예를 들어, 1/2의 표현 간격 비율을 선택합니다. 이는 스택이 최종 예측에서 원하는 매 두 번째 값을 예측하게 만듭니다.\n\n표현 간격 비율은 출력을 입력의 재샘플링과 관련시킵니다. 입력의 재샘플링과 결합되어, 각 스택은 서로 다른 주파수에서 작동합니다. 따라서 각 스택은 서로 다른 속도로 시계열을 처리하는 데 전문화될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nN-HiTS의 저자들은 입력에 가까운 스택은 장기적 효과에 초점을 맞춰야 한다고 제안합니다. 따라서 이러한 스택은 표현 비율이 작아야 합니다. 예를 들어, 세 개의 스택을 가질 수 있습니다. 첫 번째 스택은 주간 행동에 특화되어 있고, 두 번째는 일일 행동에 특화되어 있으며, 세 번째는 시간당 행동에 특화되어 있습니다.\n\n![N-HiTS](/assets/img/2024-06-19-N-HiTSMakingDeepLearningforTimeSeriesForecastingMoreEfficient_3.png)\n\n하지만 표현 비율에 대한 합리적인 선택은 무엇인가요?\n\n그것은 시계열에 따라 다릅니다. 저자들은 두 가지 옵션을 권장합니다.\n\n<div class=\"content-ad\"></div>\n\n- 다양한 주파수 범위를 다룰 때 매개변수의 수를 줄이기 위해 스택 간 지수적으로 증가하는 표현 비율을 사용합니다.\n- 일간, 주간 등의 시계열 주기를 활용합니다.\n\n# N-HiTS를 활용한 예측 예시\n\nN-HiTS가 어떻게 작동하는지 알게 되었으니, 모델을 예측 작업에 적용해 보겠습니다.\n\nN-BEATS 글에서와 마찬가지로, 우리는 독일의 도매 전기 가격을 다음 두 주 예측할 것입니다. 우리는 CC-BY-4.0 라이선스로 제공되는 \"유럽 도매 전기 가격\" 데이터를 사용할 것입니다. Nixtla의 neuralforecast 라이브러리에서 N-HiTS 구현을 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-N-HiTSMakingDeepLearningforTimeSeriesForecastingMoreEfficient_4.png)\n\n상세한 데이터 탐색을 하지 않아도 두 가지 계절성 구성 요소를 확인할 수 있습니다:\n\n- 일별: 전기 소비량이 이 시간대에 보통 높기 때문에 아침과 저녁 시간에 가격이 더 높습니다.\n- 주간별: 전기 소비량이 평일에 더 높기 때문에 주말보다 평일에 가격이 높습니다.\n\nN-BEATS 논문에서 동일한 데이터 집합을 사용했기 때문에 데이터 준비, 훈련-테스트 분할, 결과 시각화 및 기준 모델에 대한 모든 코드를 재사용할 수 있습니다. 따라서 이곳에 코드 조각을 표시하지 않겠습니다.\n\n<div class=\"content-ad\"></div>\n\n코드 작성에 들어가기 전에 가능한 정확한 예측을 얻으려는 것이 아니라 어떻게 N-HiTS를 적용할 수 있는지를 보여주는 것입니다.\n\n## 베이스라인 모델\n\n하지만, 베이스라인으로 간단한 모델부터 시작해 보겠습니다.\n\n이곳에서는 이전에 N-BEATS 기사에서 사용한 계절성이 있는 단순 모델을 사용할 것입니다. 따라서 자세한 내용은 다루지 않고 결과만 보여드리겠습니다.\n\n<div class=\"content-ad\"></div>\n\n훈련 세트의 마지막 주 데이터를 사용하여 예측하면 MAE가 17.84로 나옵니다. 이미 상당히 좋은 성적입니다.\n\n![이미지](/assets/img/2024-06-19-N-HiTSMakingDeepLearningforTimeSeriesForecastingMoreEfficient_5.png)\n\n## N-HiTS 모델 훈련\n\n첫 번째 N-HiTS 모델을 훈련해 봅시다. Nixtla의 neuralforecast 라이브러리를 사용하므로 구현은 간단합니다. 예측과 과거 기간을 정의하는 N-HiTS 모델을 초기화합니다. 이 경우, 과거 기간을 1주로 설정했습니다.\n\n<div class=\"content-ad\"></div>\n\n그러면 몇 가지 사용자 정의 옵션이 있습니다.\n\n- 모델을 선택하여 스택 및 블록 수, MLP 레이어 크기, 활성화 함수, MaxPooling을 위한 커널 크기, 풀링 유형 등을 사용자 정의할 수 있습니다.\n- 손실 함수, 학습률, 배치 크기 등을 선택하여 학습을 사용자 정의할 수 있습니다.\n- 입력 데이터의 스케일링을 할 수 있습니다.\n\nNixtla의 문서 전체 설명을 보십시오.\n\nN-BEATS 모델과 대조적으로 몇 가지 차이점을 볼 수 있습니다. 우리는 모델을 사용자 정의하기 위해 더 많은 매개변수를 가지고 있습니다. 커널 크기 및 풀링 유형을 선택하여 다중 속도 데이터 샘플링을 사용자 정의할 수 있습니다. 계층적 보간은 보간 유형 및 표현성 비율을 통해 사용자 정의할 수 있습니다. 코드 스니펫에서 이미 일부 하이퍼파라미터를 조정해 보았습니다.\n\n<div class=\"content-ad\"></div>\n\n모델을 초기화한 후에는 neuralforecast 클래스로 래핑하고 모델을 적합시킵니다. N-BEATS 기사를 읽으셨다면 이 단계에 익숙할 것입니다.\n\n결과는 기준선보다 더 좋아졌습니다. MAE는 17.84에서 17.01로 낮아졌습니다.\n\n![이미지](/assets/img/2024-06-19-N-HiTSMakingDeepLearningforTimeSeriesForecastingMoreEfficient_6.png)\n\n## N-HiTS 모델의 하이퍼파라미터 튜닝\n\n<div class=\"content-ad\"></div>\n\n좋은 하이퍼파라미터를 찾기 위해 시간을 쓰지 말고 최적화를 실행할 수 있어요.\n\n이것은 복잡하지 않아요. 많은 코드 줄을 추가할 필요가 없습니다. NHITS 모델을 Nixtla의 AutoNHITS 모델로 교체하기만 하면 돼요. AutoNHITS 모델이 하이퍼파라미터 튜닝을 대신 해줍니다. 우리는 백엔드(ray 또는 optuna)와 하이퍼파라미터의 탐색 범위를 선택하기만 하면 돼요.\n\n이 두 가지 선택은 NHITS 모델을 실행하는 것과 달리 유일한 차이점입니다. 다른 모든 단계는 동일합니다.\n\nOptuna를 선택하고 사용자 정의 구성 파일을 사용해 시작해봐요.\n\n<div class=\"content-ad\"></div>\n\n\"우리는 '최적화된' N-HiTS 모델이 베이스라인과 N-HiTS 모델에 비해 정확도가 더 낮다는 것을 알 수 있습니다 (MAE는 22.63입니다).\n\n![](/assets/img/2024-06-19-N-HiTSMakingDeepLearningforTimeSeriesForecastingMoreEfficient_7.png)\n\n아마도 저의 검색 공간 선택이 좋지 않았을 수도 있습니다. 따라서 더 많은 시도를 해보거나 더 나은 결과를 얻기 위해 다른 검색 공간을 사용할 수 있습니다. 또는 AutoNHITS의 기본 설정을 사용할 수도 있습니다. 모델에 구성을 전달하지 않고 바로 사용하거나 기본 설정을 약간 변경하여 사용할 수 있습니다.\n\n## 외생 변수가 포함된 N-HiTS\"\n\n<div class=\"content-ad\"></div>\n\n이 기사를 마치기 전에 마지막으로 보여드릴것이 있습니다. N-HiTS 모델에서 외생 변수를 사용할 수도 있습니다. 이를 위해 초기화 동안 futr_exog_list로 외생 변수를 NHITS 모델에 전달하기만 하면 됩니다. 예를 들어, 주간 계절성이 있는 경우 모델에 요일을 전달할 수 있습니다.\n\n요일을 외생 변수로 추가한 결과 MAE는 21.62가 나왔습니다. 다양한 외생 변수나 다양한 하이퍼파라미터를 시도하면 정확도를 향상시킬 수 있을 것입니다.\n\n![마지막에 관해](/assets/img/2024-06-19-N-HiTSMakingDeepLearningforTimeSeriesForecastingMoreEfficient_8.png)\n\n# N-HiTS에 대한 마지막 메모\n\n<div class=\"content-ad\"></div>\n\nN-HiTS는 논문에서 다양한 데이터 세트에서 매우 좋은 성능을 보였습니다. 그러나 이것은 모든 문제와 데이터 세트에 대해 N-HiTS가 최적의 해결책이 될 것을 의미하지는 않습니다. 특히 당신의 경우에는요.\n\n예시에서 보듯이, N-HiTS가 단순한 계절성 naive baseline 모델을 거의 이길 수 있었습니다. 하지만 그것에 도달하는 데 시간이 걸렸습니다. 먼저, 모델을 설정하고 좋은 하이퍼파라미터 세트를 찾는 데 더 많은 시간을 소비했습니다. 둘째, 훈련은 기준 모델보다 30배가 넘는 시간이 걸렸습니다.\n\n그래서 이것이 회사 프로젝트였다면, 나는 기준 모델을 선택했을 것입니다. 비록 N-HiTS가 약간의 정확도 향상을 제공하지만, 추가 복잡성은 고생할 가치가 없습니다.\n\n따라서, N-HiTS가 사용하기 쉽고 유망한 모델처럼 보여도, 해당 모델로 프로젝트를 시작하지 마십시오. 간단한 기준 모델로 시작하십시오. 기준을 토대로하여, N-HiTS가 문제에 대한 좋은 선택인지를 결정할 수 있습니다. 예를 들어, N-HiTS가 추가된 복잡성에 비해 얼마나 가치를 더하는지를 고려할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n이 기사는 매우 길었지만 다뤄야 할 내용이 많았습니다. 여기까지 함께 지켜주셨다면, 이제 다음 내용을 이해하실 수 있을 것입니다.\n\n- N-HiTS 모델이 어떻게 작동하는지에 대해 매우 좋은 이해를 갖게 되었을 것이고,\n- N-HiTS가 N-BEATS와 어떻게 다른지를 알게 되었을 것이며,\n- 실전에서 N-HiTS 모델을 사용할 수 있을 것이며,\n- 하이퍼파라미터 튜닝 중에 모델의 내부 동작을 변경할 수 있을 것입니다.\n\nN-HiTS 모델에 대해 더 깊이 알아보고 싶다면 N-HiTS 논문을 확인해보세요. 그렇지 않다면, 댓글을 남기거나 다음 기사에서 만나요!","ogImage":{"url":"/assets/img/2024-06-19-N-HiTSMakingDeepLearningforTimeSeriesForecastingMoreEfficient_0.png"},"coverImage":"/assets/img/2024-06-19-N-HiTSMakingDeepLearningforTimeSeriesForecastingMoreEfficient_0.png","tag":["Tech"],"readingTime":9},{"title":"시계열 분할 기술 정확한 모델 유효성 검증 보장하기","description":"","date":"2024-06-19 18:56","slug":"2024-06-19-TimeSeriesSplittingTechniquesEnsuringAccurateModelValidation","content":"\n\n시계열 데이터 작업 중이시군요. 멋지네요! 그러나 모델 훈련에 들어가기 전에 데이터를 나누는 방법에 대해 이야기해보죠. 시계열 데이터를 나눌 때는 날짜 순서를 유지하고 데이터 누수를 피하는 것이 중요합니다. 모델이 정확하고 신뢰할 수 있도록 유지하는 효과적인 시계열 분할 기술을 살펴봅시다.\n\n# 1. TimeSeriesSplit\n\n`TimeSeriesSplit`을 데이터 분할의 믿음직한 타임키퍼로 생각해보세요. 이는 데이터를 연속적인 폴드로 나누어, 각 훈련 세트가 과거 데이터에서 형성되고 각 테스트 세트가 미래 데이터에서 형성되도록 보장합니다.\n\n![Time Series Splitting Techniques Ensuring Accurate Model Validation](/assets/img/2024-06-19-TimeSeriesSplittingTechniquesEnsuringAccurateModelValidation_0.png)\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom sklearn.model_selection import TimeSeriesSplit\ntscv = TimeSeriesSplit(n_splits=5)\nfor train_index, test_index in tscv.split(X):\n X_train, X_test = X[train_index], X[test_index]\n y_train, y_test = y[train_index], y[test_index}\n```\n\n# 2. Sliding/Rolling Window Split\n\nIn the rolling window approach, your model moves forward in time with a fixed-size training window that slides along your dataset. It’s like taking steps into the future while always keeping an eye on the past.\n\n![TimeSeriesSplittingTechniques](/assets/img/2024-06-19-TimeSeriesSplittingTechniquesEnsuringAccurateModelValidation_1.png)\n\n\n<div class=\"content-ad\"></div>\n\n\n파이썬 코드:\n\n```python\nfor date in pd.date_range('2021-01-01', '2021-12-31', freq='M'):\n    delta = date - pd.offsets.MonthBegin(1)\n    train = series.loc[delta:date-pd.offsets.Day(1)]\n    valid = series.loc[date:date+pd.offsets.MonthEnd(1)]\n```\n\n## 3. 확장 창 분할\n\n확장 창 분할을 사용하면 모델은 오래된 학습 세트로 시작하여 점차적으로 더 많은 관측 값을 포함하게 됩니다. 시간이 흐름에 따라 더 많은 데이터를 통합하여 지식을 축적하는 것과 같습니다.\n\n\n![시각](/assets/img/2024-06-19-TimeSeriesSplittingTechniquesEnsuringAccurateModelValidation_2.png)\n\n\n<div class=\"content-ad\"></div>\n\n```python\nfor date in pd.date_range('2021-01-01', '2021-12-31', freq='M'):\n train = series.loc[:date-pd.offsets.Day(1)]\n valid = series.loc[date:date+pd.offsets.MonthEnd(1)]\n```\n\n# 4. Sliding Window with Gap Split\n\nThe sliding window with a gap introduces a buffer zone between your training and validation sets, ensuring no information from the future leaks into your model’s training. It’s like building a fence to keep your model focused on the present.\n\n![TimeSeriesSplittingTechniquesEnsuringAccurateModelValidation](/assets/img/2024-06-19-TimeSeriesSplittingTechniquesEnsuringAccurateModelValidation_3.png)\n\n<div class=\"content-ad\"></div>\n\n```js\nfor date in pd.date_range('2021–01–01', '2021–12–31', freq='M'):\n delta = date - pd.offsets.MonthBegin(1)\n train = series.loc[delta:date-pd.offsets.Day(1)]\n valid = series.loc[date+pd.offsets.MonthEnd(1)+pd.offsets.Day(1):date+pd.offsets.MonthEnd(2)]\n```\n\n# 결론\n\n내 경험상, 시계열 데이터의 올바른 분할 기술을 선택하는 것은 견고하고 신뢰할 수 있는 모델을 구축하는 데 중요합니다. 개인적으로 증가하는 창 분할이 장기적인 추세를 포착하는 데 특히 효과적이라고 생각합니다. 이는 모델이 점차적으로 더 많은 데이터 포인트로부터 배울 수 있기 때문입니다. 그러나 계산 리소스를 관리하기 위해 고정 크기의 학습 세트를 유지하고 싶은 경우, 슬라이딩 창 접근 방식이 잘 작동합니다. 궁극적으로, 최적의 기술은 특정 사례와 데이터의 특성에 따라 다릅니다. 시계열 예측 요구 사항에 가장 적합한 방법을 찾기 위해 다양한 방법을 실험해보세요.\n\n# 참고 자료:\n\n<div class=\"content-ad\"></div>\n\nhttps://robjhyndman.com/hyndsight/tscv/\nhttps://otexts.com/fpp3/tscv.html\nhttps://forecastegy.com/posts/time-series-cross-validation-python/","ogImage":{"url":"/assets/img/2024-06-19-TimeSeriesSplittingTechniquesEnsuringAccurateModelValidation_0.png"},"coverImage":"/assets/img/2024-06-19-TimeSeriesSplittingTechniquesEnsuringAccurateModelValidation_0.png","tag":["Tech"],"readingTime":4},{"title":"죄를 사과하지 않는 일로 상처 받은 모든 것에 대해 치유되길 바라요","description":"","date":"2024-06-19 18:55","slug":"2024-06-19-ihopeyouhealfromthethingsthatnooneeverapologizedfor","content":"\n\n\n![image](/assets/img/2024-06-19-ihopeyouhealfromthethingsthatnooneeverapologizedfor_0.png)\n\n이런 경험 해 본 적 있나요? 앞으로 나아가고 상처를 극복하려는 마음을 느끼지만, 아직 미해결된 상처와 배신의 무게로 인해 막혀 있는 느낌이 들 때가 있지 않나요? 진짜 지칠 때가 있죠.\n\n\"당신을 깨뜨린 사람들의 발 밑에서 치유를 찾지 마세요\".\n\n우리 모두 인정합시다, 조용히 우리를 부순 모든 것으로부터 회복하는 게 힘들어요. 용서하기 어려워요. 부서진 후에 다시 완전해지는 게 어려워요. 다른 사람들이 당신을 쳐다보는 곳에서 숨쉬기 힘들어요.\n\n\n<div class=\"content-ad\"></div>\n\n왜 그들에게 상처 주는 말을 그렇게 쉽게 하는 걸까요? 그들이 그들이 하는 말을 알고 있는 걸까요? 아니면 나는 너무 많은 의미를 부여하고 있는 걸까요?\n\n나는 너희들을 절대 잊지 않을 거에요.\n\n매일 밤 울었던 모든 일들을 위해. 너에게 새겨진 모든 말들을 위해. 너를 상처 입힌 모든 흔적들을 위해. 너가 무시했던 것들을 위해. 모든 것이 조용히 너를 파괴했던 것들을 위해.\n\n너가 모든 것에서 치유되기를 바래요, 비록 그들 중 아무도 사과하지 않아도요.\n\n<div class=\"content-ad\"></div>\n\n너가 머릿속을 맴도는 많은 생각 속에서 평화롭게 자는 게 얼마나 어려운지 알아. \"괜찮아, 선택할 수 있는 게 없어\" 라고 스스로 위로하는 것이 얼마나 어려운지 알아. 조용히 울기가 얼마나 힘든지 알아. 자신의 울음소리를 듣는 것이 얼마나 아픈지 알아.\n\n그들을 용서하기를 바란다.\n\n그리고 또한,\n\n네 자신을 용서하기를 바란다.\n\n<div class=\"content-ad\"></div>\n\n치유에는 많은 시간이 걸리지만, 당신은 여기서 벗어날 수 있을 거에요. 산이 높아도 오를 수 있다는 걸 알아요. 바다가 깊어도 헤쳐나갈 수 있다는 걸 알아요.\n\n그들이 한 말이 마음을 찌르는 듯해도, 언제나 내가 여기 있다는 걸 기억해주길 바래요. 모든 고통을 없애주지는 못하지만, 나는 당신과 함께할 준비가 돼 있어요.\n\n당신과 마찬가지로, 나도 당신의 경험의 피해자에요.\n\n어떤 면에서 우리 모두는 이 경험들의 피해자에요. 우리는 어둠을 헤치며 길을 찾기 위해 같이 힘겨워하고 있어요. 함께 이겨내요.","ogImage":{"url":"/assets/img/2024-06-19-ihopeyouhealfromthethingsthatnooneeverapologizedfor_0.png"},"coverImage":"/assets/img/2024-06-19-ihopeyouhealfromthethingsthatnooneeverapologizedfor_0.png","tag":["Tech"],"readingTime":2},{"title":"장기 단기 메모리 LSTM  RNN 개선하기","description":"","date":"2024-06-19 18:53","slug":"2024-06-19-LongShortTermMemoryLSTMImprovingRNNs","content":"\n\n\n![그림](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_0.png)\n\n이 글에서는 Long-Short-Term Memory Networks (LSTM)에 대해 소개하겠습니다. 이는 일반적인 바닐라 순환 신경망(RNN)의 변형으로서 장기 의존성을 처리하는 데 능숙합니다.\n\n이들은 의사결정에 필요하거나 중요하지 않다고 판단되는 특정 정보를 기억하거나 잊는 서로 다른 \"게이트\"를 사용합니다.\n\nLSTM은 RNN의 최신 버전으로, 산업 내에서 폭넓게 사용되며 우리가 오늘날 보는 모든 멋진 대형 언어 모델 (LLMs)의 기반이 됩니다.\n\n\n<div class=\"content-ad\"></div>\n\nRNN 개요\n\nRecurrent Neural Networks(RNN)은 일반적인 피드포워드 신경망의 변형으로, 자연어나 시계열 데이터와 같은 순차 데이터를 더 잘 처리할 수 있도록 합니다.\n\n이는 이전 입력과 출력을 다음 레이어로 전달하는 숨겨진 순환 뉴런을 가지고 수행됩니다. 아래는 예시입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_1.png)\n\n네트워크를 통해 전달되는 벡터 h를 주목해보세요. 이것이 순환 신경망(RNNs) 뒤에 숨겨진 주요 기능인 은닉 상태입니다. 이것이 시퀀스 데이터에 대해 잘 작동하는 이유입니다.\n\n은닉 상태는 이전에 계산된 은닉 상태와 해당 시간 단계에서의 새 입력을 결합합니다. 그런 다음 해당 시간 단계의 최종 출력을 계산하기 위해 시그모이드 활성화 함수가 적용됩니다. 수학적으로 표현하면:\n\n![image](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_2.png)\n\n\n<div class=\"content-ad\"></div>\n\n아래에서:\n\n- Y: 출력 벡터\n- X: 기능의 입력 벡터\n- h: 숨겨진 상태\n- V: 출력을 위한 가중 행렬\n- U: 입력을 위한 가중 행렬\n- W: 숨겨진 상태를 위한 가중 행렬\n\nV, U 및 W의 가중 행렬은 시간에 걸쳐 백프로파게이션을 통해 찾아집니다. 이는 백프로파게이션 알고리즘의 한 변형에 불과합니다.\n시각적으로는 이렇게 보입니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_3.png)\n\nFor example, when predicting Y_1, the RNN would use the inputs of X_1 plus the output from the previous time step from Y_0. As Y_0 influences Y_1, we can then see that Y_0 will also indirectly influence Y_2, demonstrating the recurrent nature.\n\nIf you want a full intro to RNNs then check out my previous blog.\n\n# Vanishing & Exploding Problem\n\n\n<div class=\"content-ad\"></div>\n\nRNN의 긍정적인 측면 중 하나는 각 계층이 U, W 및 V의 가중 행렬을 공유하지만, 정규 피드포워드 신경망은 각 계층마다 자체 가중 행렬을 갖는다는 것입니다. 이로 인해 RNN은 더 메모리를 효율적으로 사용할 수 있습니다.\n\n그러나 이 가중 행렬 공유는 그들의 중요한 결함 중 하나인 사라지는 그래디언트와 폭주하는 그래디언트 문제를 야기합니다.\n\nRNN은 backpropagation through time (BPTT)라는 backpropagation 알고리즘의 변형을 사용하여 학습합니다. 이 알고리즘은 일반적인 backpropagation과 유사하지만, 각 시간 단계에서 업데이트해야 하는 계층 간의 공유 가중 행렬로 인해 더 '중첩된' 계산이 필요합니다.\n\nBPTT의 일반적인 공식은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image 1](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_4.png)\n\nRNN에서 J는 임의의 가중치 행렬이며 U, W 또는 V일 수 있으며 E는 총 오차입니다.\n\nRNN은 일반적인 신경망보다 더 깊은 경향이 있습니다(각 시간 단계는 하나의 레이어입니다). 따라서 그래디언트가 1보다 작거나 큰 경우에는 역방향으로 전파될 때 그래디언트가 소멸되거나 폭발될 수 있습니다.\n\n![image 2](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_5.png)\n\n\n<div class=\"content-ad\"></div>\n\n관심 있는 독자를 위해, 왜 이런 일이 발생하는지의 수학적인 전체 분석은 여기서 찾을 수 있어요. 이것은 고유값과 야코비안 행렬과 같은 재미있는 것들이 관련돼 있습니다!\n\n만약 역전파를 통한 시간 알고리즘 (BTTP)과 그라디언트가 사라지거나 폭발하는 문제의 전체적인 분석을 원한다면, 제 이전 게시물을 확인해보세요.\n\n사라지는 그라디언트와 폭발하는 그라디언트 문제를 잘 보여주는 좋은 예시는 Stanford의 CS224D 수업에서 제시되었습니다. 두 개의 문장을 생각해보세요:\n\n- \"제인이 방으로 들어갔어요. 존도 들어왔어요. 제인이 ___에게 안녕했어요.\"\n- \"제인이 방으로 들어갔어요. 존도 들어왔어요. 늦었고, 모두가 긴 하루 일과를 마치고 집으로 향했어요. 제인이 ___에게 안녕했어요.\"\n\n<div class=\"content-ad\"></div>\n\n어떤 경우에도 빈 공간은 아마도 존을 가리키는 것입니다. RNN은 이 맥락을 학습하여 두 문장 모두 출력이 존임을 이해해야 합니다.\n\n그러나 실험 결과, 문장 1이 문장 2보다 정확하게 예측되는 경향이 있었습니다. 이는 문장 2에서 그래디언트가 소멸되어 예측을 할 때 먼 맥락을 효율적으로 인식하지 못하기 때문입니다.\n\n이것은 분명히 문제입니다. RNN은 이와 같은 시나리오에 대한 \"기억\"을 갖도록 설계되었기 때문입니다.\n\n그래서, 이 문제에 대해 어떻게 해결할까요?\n\n<div class=\"content-ad\"></div>\n\n# 롱-숏텀 메모리\n\n## 개요\n\nLSTMs는 1997년 Hochreiter & Schmidhuber에 의해 소개되었으며, 그 기본 아이디어는 순환 셀 내부에 \"게이트\"가 있다는 것입니다. 이러한 게이트는 순환 셀이 장기 기억을 구축할 때 무엇을 기억하고 잊어야 하는지를 제어합니다.\n\n일반적인 RNN에서는 순환 셀이 다음과 같이 보입니다:\n\n\n|  Table  |  Tag   |\n|---------|--------|\n\n\n<div class=\"content-ad\"></div>\n\n\n![Long Short Term Memory - LSTM Improving RNNs](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_6.png)\n\nHowever, the LSTM cell is a lot more complicated:\n\n![LSTM Cell](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_7.png)\n\nI appreciate there is a lot going on here, but lets break it down step by step.\n\n\n<div class=\"content-ad\"></div>\n\n## 셀 상태\n\n핵심적인 차이 중 하나는 셀 상태 C의 도입입니다. 이 셀 상태에는 컨텍스트 및 과거 패턴과 같이 기본적인 정보가 포함되어 있습니다. 바로 메모리입니다. 이 셀 상태는 셀을 통과하며 선형 상호 작용을 하는 여러 개의 게이트에 의해 조정될 수 있습니다.\n\n셀 상태와 은닉 상태를 혼동하기 쉽지만, 일반적으로 셀 상태는 네트워크의 전체 메모리를 포함하도록 설계되었으며, 은닉 상태는 단기 의존성을 위해 사용되고 실제로 최근 정보만을 가지고 있습니다. 또한 예측을 위해 셀의 출력에 사용됩니다.\n\n## 잊기 게이트\n\n<div class=\"content-ad\"></div>\n\nLSTM의 첫 번째 단계는 forget gate입니다. 이 게이트는 이전 셀 상태 C_'t-1'에서 어떤 이전 정보를 삭제할지 결정하는 역할을 합니다.\n\n![이미지](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_8.png)\n\n여기서:\n\n- σ: 시그모이드 활성화 함수.\n- W_f: forget gate의 가중치 행렬.\n- h_'t−1': 이전 시간 단계의 출력.\n- x_t: 시간 단계 t의 입력.\n- b_f: forget gate의 편향.\n- f_t: 0과 1 사이의 값을 가지는 forget gate 출력.\n- X_t: 현재 입력.\n\n<div class=\"content-ad\"></div>\n\n출력 f_t는 이전 셀 상태 C_'t-1'에 곱해져 어떤 요소를 잊어야 하는지 수정합니다. 시그모이드 함수 덕분에 값은 0과 1 사이에 있으며, 0은 잊기를 의미하고 1은 기억에 추가됩니다.\n\nW_f 행렬에서 올바른 값을 찾아 역전파를 통해 이 정보를 학습합니다. 이를 통해 우리는 기억할지 잊을지를 결정할 수 있습니다.\n\n## 입력 게이트 및 후보 셀 상태\n\n입력 게이트 i_t는 다음 단계이지만 현재 시간 단계에서 셀 상태에 추가할 새로운 기억을 결정합니다. 후보 셀 상태 C*_t는 셀 상태에 추가할 가능한 모든 정보를 보유합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_9.png)\n\nWhere:\n\n- σ: 시그모이드 활성화 함수.\n- tanh⁡: 쌍곡 탄젠트 활성화 함수.\n- W_i: 입력 게이트용 가중치 행렬.\n- W_c: 후보 셀 상태용 가중치 행렬.\n- b_i: 입력 게이트용 편향\n- b_c: 후보 셀 상태용 편향.\n- C*_t: 후보 셀 상태, -1과 1 사이의 출력 값.\n- i_t: 0과 1 사이의 입력 게이트 출력.\n- h_'t-1': 이전 숨은 상태.\n- X_t: 현재 입력.\n\ntanh을 사용하면 셀 상태를 증가시키거나 감소시킬 수 있습니다. tanh는 출력을 -1과 1 사이로 압축하기 때문입니다. 시그모이드는 기억에 새로운 것을 추가하기 위해 이전 게이트와 유사한 이유로 사용됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n입력 게이트는 특히 이전 셀 상태C_'t-1'입니다.\n\n## 셀 상태 업데이트\n\n우리는 후보 셀 상태 C*_t 에서 새로운 셀 상태 C_t로 관련 정보만 추가하려고 합니다. 이를 위해 후보 셀 상태를 입력 게이트 i_t 와 곱하고 이를 잊어버린 게이트 f_t 와 이전 셀 상태 C_'t-1' 의 곱과 더할 수 있습니다.\n\n![image](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_10.png)\n\n<div class=\"content-ad\"></div>\n\n모든 작업은 좀 더 친숙한 느낌을 주려고 세포 상태를 갱신했습니다. 관련 없는 정보는 잊어버리고 필요한 새로운 정보를 추가했어요.\n\n## 출력 게이트\n\n마지막 단계는 셀에서 어떤 것을 예측으로 출력할지 결정하는 것이었지요. 먼저 출력 게이트 o_t를 계산하고, 이는 우리가 출력할 셀 상태의 어느 부분을 결정하는데 사용됩니다. 이는 기본적으로 일반 RNN의 일반적인 숨겨진 상태 반복 셀과 비슷한 역할을 해요.\n\n그 출력값은 새로운 셀 상태의 tanh 값과 곱해져서 원하는 값만을 출력합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_11.png)\n\nWhere:\n\n- σ: 시그모이드 함수.\n- tanh: 쌍곡선 탄젠트 활성화 함수.\n- W_o: 가중치 행렬.\n- b_o: 편향.\n- o_t: 출력 상태.\n- h_t: 새로운 은닉 상태.\n- h_'t-1': 이전 은닉 상태.\n- X_t: 현재 입력.\n- C_t: 새로운 셀 상태.\n\n그게 전부에요! 언급할 중요한 점은 모든 가중치 행렬이 BPTT를 사용하여 어떤 요소를 잊고 기억할지 학습해야 한다는 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 변형\n\n이것은 표준 LSTM일 뿐이고 다른 변형들이 있습니다. 그중 가장 흔한 것은 다음과 같습니다:\n\n- 양방향\n- 합성곱\n- 쌓인\n- 청구 접속\n- 게이트 순환 유닛\n\n이 글에서 이러한 모두를 다루는 것은 범위를 벗어나지만, 관심 있는 독자는 위에 제공된 링크에서 자세히 알아볼 수 있습니다. 다음 글에서는 게이트 순환 유닛에 대해 다룰 예정입니다.\n\n<div class=\"content-ad\"></div>\n\n# 요약 및 더 깊은 생각\n\nLSTMs는 처음에는 무서워 보일 수 있지만, 이 글을 통해 조금 더 친숙해졌으면 좋겠어요! 다양한 계산이 있지만, 이 모든 것들은 매우 유사합니다. 잊어버릴 것을 결정하는 forget gate와 기억에 추가할 새로운 정보를 결정하는 input gate 두 가지 기본 구성 요소가 있습니다. LSTMs의 장점은 이러한 gate들 덕분에 장기 기억력이 더 나은 것입니다.\n\n# 추가로!\n\n저는 '데이터 소스 공유', 매주 공유하는 더 나은 데이터 과학자가 되는 팁, 업계에서의 일반적인 경험, 지난 주에 한 생각들을 나누는 무료 뉴스레터를 운영하고 있어요.\n\n<div class=\"content-ad\"></div>\n\n# 나와 소통해요!\n\n- 링크드인, 트위터, 또는 인스타그램에서 연락해요.\n- 내 YouTube 채널에서는 기술적인 데이터 과학과 머신러닝 개념을 배우세요!\n\n# 참고 자료\n\n- LSTM에 대한 훌륭한 블로그 포스트\n- 스탠포드 RNN 체트시트\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. Aurélien Géron. 2019년 9월. 출판사: O'Reilly Media, Inc. ISBN: 9781492032649.","ogImage":{"url":"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_0.png"},"coverImage":"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_0.png","tag":["Tech"],"readingTime":8},{"title":"왜 나는 글쓰기를 그만두지 않을 것인지 이유","description":"","date":"2024-06-19 18:52","slug":"2024-06-19-ThisIsWhyIWillNotStopWriting","content":"\n\n과거 작품들을 훑어보면 낯선 사람처럼 느껴지기도 해요. 마치 박물관에서 유물을 뒤적이는 손님 같아요. 예전에 썼던 것들이 다시 유용하게 쓰일 때가 있어서 더욱 놀랍네요. 마치 필요성을 예견하고 미리 해결책을 준비한 것 같아요.\n\n가끔 써본 말들이 당시 느꼈던 감정을 상기하려고 노력해요. 때로는 성공도 해보지만, 때로는 그렇지 못할 때도 있어요. 때로는 한 마디로 당시의 감정과 머릿 속 상태를 정확히 떠올릴 수도 있고, 다른 때에는 여러 단어를 반복해도 당시의 감정과 연관을 찾지 못할 때도 있어요.\n\n![image](/assets/img/2024-06-19-ThisIsWhyIWillNotStopWriting_0.png)\n\n저는 강력히 믿어요. 우주는 현재의 관점을 통해 우리의 과거를 다시 방문하는 것 같아요. 그래서 어떤 경험이 새롭긴 하지만, 과거 어느 때와도 전혀 무관한 것은 아니에요. 감정은 마치 그릇처럼 앞뒤로 흔들리면서, 과거의 감정을 혀끝으로 맛 볼 수 있을 정도로 순간순간 변하기도 해요.\n\n<div class=\"content-ad\"></div>\n\n그래서 난 글쓰기를 멈추지 않을 거야. 우주가 다른 이들을 위해 썼다는 핑계로 나에게 미래 편지를 쓰라고 요청하는 것처럼. 오늘 썼던 이 말들이 다음 10년 후에 도움이 될지도 몰라. 이것은 내 감정을 담는 창고이며, 반복되는 패턴을 발견했기 때문이다.","ogImage":{"url":"/assets/img/2024-06-19-ThisIsWhyIWillNotStopWriting_0.png"},"coverImage":"/assets/img/2024-06-19-ThisIsWhyIWillNotStopWriting_0.png","tag":["Tech"],"readingTime":1},{"title":"위성 이미지에서 GANs적대적 생성 신경망을 사용하여 구름 제거하기","description":"","date":"2024-06-19 18:49","slug":"2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks","content":"\n\n## 파이썬으로부터 GAN(Generative Adversarial Networks) 만들어 보기\n\n![이미지](/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_0.png)\n\nGAN(Generative Adversarial Networks)이라는 아이디어는 2014년 Goodfellow와 그 동료들에 의해 소개되었고, 곧 그 이후에 컴퓨터 비전 및 이미지 생성 분야에서 극도로 인기를 끌게 되었습니다. 인공지능 분야에서의 급속한 발전과 새로운 알고리즘의 수가 늘어나는 것을 고려하더라도, 이 개념의 단순함과 창의성은 여전히 매우 인상적입니다. 그래서 오늘은 이러한 네트워크가 얼마나 강력할 수 있는지를 보여주기 위해 위성 RGB(빨강, 녹색, 파랑) 이미지에서 구름을 제거하는 시도를 해보려고 합니다.\n\n적절히 균형 잡히고 충분히 크며 올바르게 전처리된 컴퓨터 비전 데이터셋을 준비하는 데에는 상당한 시간이 소요되므로, 저는 Kaggle에 어떤 것이 있는지 살펴보기로 결정했습니다. 이 작업에 가장 적합하다고 생각한 데이터셋은 EuroSat이며, 이는 오픈 라이선스를 가지고 있습니다. 이 데이터셋은 Sentinel-2에서 64x64 픽셀의 27000개의 레이블이 지정된 RGB 이미지로 구성되어 있고, 다중 클래스 분류 문제를 해결하기 위해 만들어졌습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_1.png)\n\n우리는 분류 자체에 흥미가 없지만 EuroSat 데이터셋의 주요 기능 중 하나는 모든 이미지에 맑은 하늘이 있습니다. 그것이 정확히 우리가 필요한 것입니다. [3]에서 이 접근법을 채택하여, 우리는 이 Sentinel-2 샷을 대상으로 사용하고 입력을 추가하여 (구름) 노이즈를 생성할 것입니다.\n\n그래서 우리가 GANs에 대해 실제로 이야기하기 전에 데이터를 준비해 봅시다. 우선, 데이터를 다운로드하고 모든 클래스를 하나의 디렉토리로 병합해야 합니다.\n\n🐍전체 Python 코드: GitHub.\n\n\n<div class=\"content-ad\"></div>\n\n```js\nimport numpy as np\nimport pandas as pd\nimport random\n\nfrom os import listdir, mkdir, rename\nfrom os.path import join, exists\nimport shutil\nimport datetime\n\nimport matplotlib.pyplot as plt\nfrom highlight_text import ax_text, fig_text\nfrom PIL import Image\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n```\n\n```js\nclasses = listdir('./EuroSat')\npath_target = './EuroSat/all_targets'\npath_input = './EuroSat/all_inputs'\n\n\"\"\"UNPACK한 아카이브 파일의 파일 이름을 변경하기 위해 단 한 번만 실행하세요\"\"\"\nmkdir(path_input)\nmkdir(path_target)\nk = 1\nfor kind in classes:\n  path = join('./EuroSat', str(kind))\n  for i, f in enumerate(listdir(path)):\n    shutil.copyfile(join(path, f),\n                  join(path_target, f))\n    rename(join(path_target, f), join(path_target, f'{k}.jpg'))\n    k += 1\n```\n\n중요한 두 번째 단계는 노이즈 생성입니다. 다양한 방법을 사용할 수 있지만, 예를 들어 일부 픽셀을 무작위로 마스킹하거나 가우시안 노이즈를 추가하는 등의 방법이 있습니다. 그러나 이 글에서는 저는 새로운 방식인 Perlin 노이즈를 시도해 보고 싶습니다. Perlin 노이즈는 80년대에 Ken Perlin이 영화 연기 효과를 개발할 때 발명했습니다. 이 종류의 노이즈는 일반적인 랜덤 노이즈에 비해 더 유기적인 외관을 가지고 있습니다. 저에게 이를 증명하는 기회를 주세요.\n\n```js\ndef generate_perlin_noise(width, height, scale, octaves, persistence, lacunarity):\n    noise = np.zeros((height, width))\n    for i in range(height):\n        for j in range(width):\n            noise[i][j] = pnoise2(i / scale,\n                                  j / scale,\n                                  octaves=octaves,\n                                  persistence=persistence,\n                                  lacunarity=lacunarity,\n                                  repeatx=width,\n                                  repeaty=height,\n                                  base=0)\n    return noise\n\ndef normalize_noise(noise):\n    min_val = noise.min()\n    max_val = noise.max()\n    return (noise - min_val) / (max_val - min_val)\n\ndef generate_clouds(width, height, base_scale, octaves, persistence, lacunarity):\n    clouds = np.zeros((height, width))\n    for octave in range(1, octaves + 1):\n        scale = base_scale / octave\n        layer = generate_perlin_noise(width, height, scale, 1, persistence, lacunarity)\n        clouds += layer * (persistence ** octave)\n\n    clouds = normalize_noise(clouds)\n    return clouds\n\ndef overlay_clouds(image, clouds, alpha=0.5):\n\n    clouds_rgb = np.stack([clouds] * 3, axis=-1)\n\n    image = image.astype(float) / 255.0\n    clouds_rgb = clouds_rgb.astype(float)\n\n    blended = image * (1 - alpha) + clouds_rgb * alpha\n\n    blended = (blended * 255).astype(np.uint8)\n    return blended\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n가로, 세로 = 64, 64\n옥타브 = 12  # 합쳐지는 잡음 레이어의 수\n지속성 = 0.5  # 낮은 지속성은 높은 주파수 옥타브의 진폭을 줄입니다.\n라쿠나리티 = 2  # 높은 라쿠나리티는 높은 주파수 옥타브의 주파수를 늘립니다.\nfor i in range(len(listdir(path_target))):\n  기본_스케일 = random.uniform(5, 120)  # 잡음 주파수\n  알파 = random.uniform(0, 1)  # 투명도\n\n  구름 = generate_clouds(가로, 세로, 기본_스케일, 옥타브, 지속성, 라쿠나리티)\n\n  이미지 = np.asarray(Image.open(join(path_target, f'{i+1}.jpg')))\n  이미지 = Image.fromarray(overlay_clouds(이미지, 구름, 알파))\n  이미지.save(join(path_input, f'{i+1}.jpg'))\n  print(f'{i+1}/{len(listdir(path_target))}번째 처리 완료')\n```\n\n```js\n인덱스 = np.random.randint(27000)\nfig, ax = plt.subplots(1,2)\nax[0].imshow(np.asarray(Image.open(join(path_target, f'{인덱스}.jpg')))\nax[1].imshow(np.asarray(Image.open(join(path_input, f'{인덱스}.jpg')))\nax[0].set_title(\"원본\")\nax[0].axis('off')\nax[1].set_title(\"입력\")\nax[1].axis('off')\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_2.png\" />\n\n위에서 볼 수 있듯이 이미지의 구름은 매우 현실적이며 다양한 \"밀도\"와 질감을 가지며 실제 구름과 유사합니다.\n\n<div class=\"content-ad\"></div>\n\n만약 저처럼 Perlin 소음에 흥미를 느낀다면, 게임 개발 산업에서 이 소음이 어떻게 적용될 수 있는지에 대한 정말 멋진 비디오가 있어요!\n\n이제 우리가 사용할 준비가 된 데이터셋이 있으니, GANs에 대해 이야기해 보겠습니다.\n\n# 생성적 적대 신경망\n\n이 아이디어를 더 잘 설명하기 위해, 동남아시아를 여행하다가 밖이 너무 춥다고 느낄 때 후디가 절실하게 필요하다고 상상해 보세요. 가장 가까운 거리 시장에 가보니, 몇 가지 브랜드 의류가 있는 작은 가게를 발견했어요. 판매자가 유명한 브랜드 ExpensiveButNotWorthIt의 후디를 시도해보라며 괜찮은 후디를 가져다줍니다. 더 자세히 살펴보고 분명히 가짜라고 결론 내리게 됩니다. 판매자가 말합니다: '잠시만요, 진짜 것이 있어요.' 그가 다른 후디를 가져오는데, 브랜드 제품과 더 닮았지만 여전히 가짜입니다. 이와 같은 반복 작업을 몇 번 거친 후, 판매자가 전설적인 ExpensiveButNotWorthIt의 구별이 어려운 사본을 가져와 여러분은 기꺼이 구매하게 됩니다. 이것이 바로 GANs가 작동하는 방식입니다!\n\n<div class=\"content-ad\"></div>\n\nGAN의 경우, 당신은 판별자(D)라고 불립니다. 판별자의 목표는 진짜 물체와 가짜 물체를 구별하거나 이진 분류 작업을 수행하는 것입니다. 이에 반해, 생성자(G)는 높은 품질의 가짜를 생성하려고 하는 판매자라고 불립니다. 판별자와 생성자는 서로 능가하기 위해 독립적으로 훈련됩니다. 따라서 최종적으로 우리는 높은 품질의 가짜를 얻습니다.\n\n![이미지](/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_3.png)\n\n훈련 과정은 일반적으로 다음과 같이 진행됩니다:\n\n- 입력 노이즈를 샘플링합니다 (우리의 경우 구름이 있는 이미지).\n- 노이즈를 생성자(G)에 공급하고 예측을 수집합니다.\n- D 손실을 계산합니다. G의 출력에 대한 하나와 실제 데이터에 대한 다른 예측을 얻어서 이루어집니다.\n- D의 가중치를 업데이트합니다.\n- 다시 입력 노이즈를 샘플링합니다.\n- 노이즈를 생성자(G)에 공급하고 예측을 수집합니다.\n- G 손실을 계산합니다. G의 예측을 D에 공급하여 이루어집니다.\n- G의 가중치를 업데이트합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Erasing Clouds from Satellite Imagery Using GANs](/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_4.png)\n\nIn other words, we can define a value function V(G,D):\n\n![Value function V(G,D)](/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_5.png)\n\nwhere we want to minimize the term log(1-D(G(z))) to train G and maximize log D(x) to train D (in this notation x — real data sample and z — noise).\n\n\n<div class=\"content-ad\"></div>\n\n이제 파이토치에서 구현해 봅시다!\n\n원본 논문에서 저자들은 Multilayer Perceptron (MLP)을 사용하는 것에 대해 언급합니다; 이것은 ANN으로 간단히도 불립니다만, 저는 미세한 접근을 시도하고 싶습니다 — Generator로 UNet [5] 아키텍처를 사용하고, Discriminator로는 ResNet [6]을 사용하고 싶습니다. 이들은 둘 다 잘 알려진 CNN 아키텍처이기 때문에 여기서 설명하지는 않겠습니다 (댓글에서 별도의 글을 쓸지 여부를 알려주세요).\n\n이제 구축해 봅시다. Discriminator:\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch.utils.data import Subset\n```\n\n<div class=\"content-ad\"></div>\n\n```js\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Sequential(\n                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n                        nn.BatchNorm2d(out_channels),\n                        nn.ReLU())\n        self.conv2 = nn.Sequential(\n                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n                        nn.BatchNorm2d(out_channels))\n        self.downsample = downsample\n        self.relu = nn.ReLU()\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.conv2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block=ResidualBlock, all_connections=[3,4,6,3]):\n        super(ResNet, self).__init__()\n        self.inputs = 16\n        self.conv1 = nn.Sequential(\n                        nn.Conv2d(3, 16, kernel_size = 3, stride = 1, padding = 1),\n                        nn.BatchNorm2d(16),\n                        nn.ReLU()) #16x64x64\n        self.maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2) #16x32x32\n\n\n        self.layer0 = self.makeLayer(block, 16, all_connections[0], stride = 1) #connections = 3, shape: 16x32x32\n        self.layer1 = self.makeLayer(block, 32, all_connections[1], stride = 2)#connections = 4, shape: 32x16x16\n        self.layer2 = self.makeLayer(block, 128, all_connections[2], stride = 2)#connections = 6, shape: 1281x8x8\n        self.layer3 = self.makeLayer(block, 256, all_connections[3], stride = 2)#connections = 3, shape: 256x4x4\n        self.avgpool = nn.AvgPool2d(4, stride=1)\n        self.fc = nn.Linear(256, 1)\n\n    def makeLayer(self, block, outputs, connections, stride=1):\n        downsample = None\n        if stride != 1 or self.inputs != outputs:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inputs, outputs, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(outputs),\n            )\n        layers = []\n        layers.append(block(self.inputs, outputs, stride, downsample))\n        self.inputs = outputs\n        for i in range(1, connections):\n            layers.append(block(self.inputs, outputs))\n\n        return nn.Sequential(*layers)\n\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.avgpool(x)\n        x = x.view(-1, 256)\n        x = self.fc(x).flatten()\n        return F.sigmoid(x)\n```\n\nGenerator:\n\n```js\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass UNet(nn.Module):\n    def __init__(self):\n      super().__init__()\n      self.conv_1 = DoubleConv(3, 32) # 32x64x64\n      self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2) # 32x32x32\n\n      self.conv_2 = DoubleConv(32, 64)  #64x32x32\n      self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2) #64x16x16\n\n      self.conv_3 = DoubleConv(64, 128)  #128x16x16\n      self.pool_3 = nn.MaxPool2d(kernel_size=2, stride=2) #128x8x8\n\n      self.conv_4 = DoubleConv(128, 256)  #256x8x8\n      self.pool_4 = nn.MaxPool2d(kernel_size=2, stride=2) #256x4x4\n\n      self.conv_5 = DoubleConv(256, 512)  #512x2x2\n\n      #DECODER\n      self.upconv_1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2) #256x4x4\n      self.conv_6 = DoubleConv(512, 256) #256x4x4\n\n\n      self.upconv_2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) #128x8x8\n      self.conv_7 = DoubleConv(256, 128)  #128x8x8\n\n      self.upconv_3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2) #64x16x16\n      self.conv_8 = DoubleConv(128, 64)  #64x16x16\n\n      self.upconv_4 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2) #32x32x32\n      self.conv_9 = DoubleConv(64, 32)  #32x32x32\n\n      self.output = nn.Conv2d(32, 3, kernel_size = 3, stride = 1, padding = 1) #3x64x64\n\n    def forward(self, batch):\n\n      conv_1_out = self.conv_1(batch)\n      conv_2_out = self.conv_2(self.pool_1(conv_1_out))\n      conv_3_out = self.conv_3(self.pool_2(conv_2_out))\n      conv_4_out = self.conv_4(self.pool_3(conv_3_out))\n      conv_5_out = self.conv_5(self.pool_4(conv_4_out))\n\n      conv_6_out = self.conv_6(torch.cat([self.upconv_1(conv_5_out), conv_4_out], dim=1))\n      conv_7_out = self.conv_7(torch.cat([self.upconv_2(conv_6_out), conv_3_out], dim=1))\n      conv_8_out = self.conv_8(torch.cat([self.upconv_3(conv_7_out), conv_2_out], dim=1))\n      conv_9_out = self.conv_9(torch.cat([self.upconv_4(conv_8_out), conv_1_out], dim=1))\n\n      output = self.output(conv_9_out)\n\n\n      return F.sigmoid(output)\n```\n\n이제 데이터를 훈련/테스트 세트로 분할하고 torch 데이터 세트로 래핑해야합니다:\n\n<div class=\"content-ad\"></div>\n\n```python\nclass dataset(Dataset):\n    def __init__(self, batch_size, images_paths, targets, img_size=64):\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.images_paths = images_paths\n        self.targets = targets\n        self.len = len(self.images_paths) // batch_size\n\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n        self.batch_im = [self.images_paths[idx * self.batch_size:(idx + 1) * self.batch_size] for idx in range(self.len)]\n        self.batch_t = [self.targets[idx * self.batch_size:(idx + 1) * self.batch_size] for idx in range(self.len)]\n\n    def __getitem__(self, idx):\n        pred = torch.stack([\n            self.transform(Image.open(join(path_input, file_name)))\n            for file_name in self.batch_im[idx]\n        ])\n        target = torch.stack([\n            self.transform(Image.open(join(path_target, file_name)))\n            for file_name in self.batch_im[idx]\n        ])\n        return pred, target\n\n    def __len__(self):\n        return self.len\n```\n\n멋져요. 이제 훈련 루프를 작성할 시간입니다. 그 전에 손실 함수와 옵티마이저를 정의해 봅시다:\n\n```python\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nbatch_size = 64\nnum_epochs = 15\nlearning_rate_D = 1e-5\nlearning_rate_G = 1e-4\n\ndiscriminator = ResNet()\ngenerator = UNet()\n\nbce = nn.BCEWithLogitsLoss()\nl1loss = nn.L1Loss()\n\noptimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate_D)\noptimizer_G = optim.Adam(generator.parameters(), lr=learning_rate_G)\n\nscheduler_D = optim.lr_scheduler.StepLR(optimizer_D, step_size=10, gamma=0.1)\nscheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=10, gamma=0.1)\n```\n\n이전 GAN 알고리즘 그림의 손실 함수와는 다른 것을 볼 수 있습니다. 특히 L1 손실을 추가했습니다. 이 아이디어는 우리가 무작위로 이미지를 생성하는 것이 아니라 입력에서 대부분의 정보를 유지하고 노이즈만 제거하려고 한다는 것입니다. 따라서 G 손실은 다음과 같을 것입니다:\n\n<div class=\"content-ad\"></div>\n\n\nG_loss = log(1 − D(G(z))) + 𝝀 |G(z)-y|\n\ninstead of just\n\nG_loss = log(1 − D(G(z)))\n\n𝝀 is an arbitrary coefficient, which balances two components of the losses.\n\n\n<div class=\"content-ad\"></div>\n\n이제 데이터를 분할하여 훈련 과정을 시작해봅시다:\n\n```js\ntest_ratio, train_ratio = 0.3, 0.7\nnum_test = int(len(listdir(path_target)) * test_ratio)\nnum_train = int((int(len(listdir(path_target))) - num_test))\n\nimg_size = (64, 64)\n\nprint(\"훈련 샘플 수:\", num_train)\nprint(\"테스트 샘플 수:\", num_test)\n\nrandom.seed(231)\ntrain_idxs = np.array(random.sample(range(num_test + num_train), num_train))\nmask = np.ones(num_train + num_test, dtype=bool)\nmask[train_idxs] = False\n\nimages = {}\nfeatures = random.sample(listdir(path_input), num_test + num_train)\ntargets = random.sample(listdir(path_target), num_test + num_train)\n\nrandom.Random(231).shuffle(features)\nrandom.Random(231).shuffle(targets)\n\ntrain_input_img_paths = np.array(features)[train_idxs]\ntrain_target_img_path = np.array(targets)[train_idxs]\ntest_input_img_paths = np.array(features)[mask]\ntest_target_img_path = np.array(targets)[mask]\n\ntrain_loader = dataset(batch_size=batch_size, img_size=img_size, images_paths=train_input_img_paths, targets=train_target_img_path)\ntest_loader = dataset(batch_size=batch_size, img_size=img_size, images_paths=test_input_img_paths, targets=test_target_img_path)\n```\n\n이제 훈련 루프를 실행해봅시다:\n\n```js\ntrain_loss_G, train_loss_D, val_loss_G, val_loss_D = [], [], [], []\nall_loss_G, all_loss_D = [], []\nbest_generator_epoch_val_loss, best_discriminator_epoch_val_loss = -np.inf, -np.inf\nfor epoch in range(num_epochs):\n\n    discriminator.train()\n    generator.train()\n\n    discriminator_epoch_loss, generator_epoch_loss = 0, 0\n\n    for inputs, targets in train_loader:\n        inputs, true = inputs, targets\n\n        '''1. 판별자 (ResNet) 훈련하기'''\n        optimizer_D.zero_grad()\n\n        fake = generator(inputs).detach()\n\n        pred_fake = discriminator(fake).to(device)\n        loss_fake = bce(pred_fake, torch.zeros(batch_size, device=device))\n\n        pred_real = discriminator(true).to(device)\n        loss_real = bce(pred_real, torch.ones(batch_size, device=device))\n\n        loss_D = (loss_fake + loss_real) / 2\n\n        loss_D.backward()\n        optimizer_D.step()\n\n        discriminator_epoch_loss += loss_D.item()\n        all_loss_D.append(loss_D.item())\n\n        '''2. 생성자 (UNet) 훈련하기'''\n        optimizer_G.zero_grad()\n\n        fake = generator(inputs)\n        pred_fake = discriminator(fake).to(device)\n\n        loss_G_bce = bce(pred_fake, torch.ones_like(pred_fake, device=device))\n        loss_G_l1 = l1loss(fake, targets) * 100\n        loss_G = loss_G_bce + loss_G_l1\n        loss_G.backward()\n        optimizer_G.step()\n\n        generator_epoch_loss += loss_G.item()\n        all_loss_G.append(loss_G.item())\n\n    discriminator_epoch_loss /= len(train_loader)\n    generator_epoch_loss /= len(train_loader)\n    train_loss_D.append(discriminator_epoch_loss)\n    train_loss_G.append(generator_epoch_loss)\n\n    discriminator.eval()\n    generator.eval()\n\n    discriminator_epoch_val_loss, generator_epoch_val_loss = 0, 0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs, targets\n\n            fake = generator(inputs)\n            pred = discriminator(fake).to(device)\n\n            loss_G_bce = bce(fake, torch.ones_like(fake, device=device))\n            loss_G_l1 = l1loss(fake, targets) * 100\n            loss_G = loss_G_bce + loss_G_l1\n            loss_D = bce(pred.to(device), torch.zeros(batch_size, device=device))\n\n            discriminator_epoch_val_loss += loss_D.item()\n            generator_epoch_val_loss += loss_G.item()\n\n    discriminator_epoch_val_loss /= len(test_loader)\n    generator_epoch_val_loss /= len(test_loader)\n\n    val_loss_D.append(discriminator_epoch_val_loss)\n    val_loss_G.append(generator_epoch_val_loss)\n\n    print(f\"------에포크 [{epoch+1}/{num_epochs}]------\\n훈련 손실 D: {discriminator_epoch_loss:.4f}, 검증 손실 D: {discriminator_epoch_val_loss:.4f}\")\n    print(f'훈련 손실 G: {generator_epoch_loss:.4f}, 검증 손실 G: {generator_epoch_val_loss:.4f}')\n\n    if discriminator_epoch_val_loss > best_discriminator_epoch_val_loss:\n        discriminator_epoch_val_loss = best_discriminator_epoch_val_loss\n        torch.save(discriminator.state_dict(), \"discriminator.pth\")\n    if generator_epoch_val_loss > best_generator_epoch_val_loss:\n        generator_epoch_val_loss = best_generator_epoch_val_loss\n        torch.save(generator.state_dict(), \"generator.pth\")\n\n    fig, ax = plt.subplots(1,3)\n    ax[0].imshow(np.transpose(inputs.numpy()[7], (1,2,0)))\n    ax[1].imshow(np.transpose(targets.numpy()[7], (1,2,0)))\n    ax[2].imshow(np.transpose(fake.detach().numpy()[7], (1,2,0)))\n    plt.show()\n```\n\n<div class=\"content-ad\"></div>\n\n코드가 끝나면 손실을 그래프로 그려볼 수 있어요. 이 코드는 이 멋진 웹사이트에서 일부 채택되었어요:\n\n```js\nfrom matplotlib.font_manager import FontProperties\n\nbackground_color = '#001219'\nfont = FontProperties(fname='LexendDeca-VariableFont_wght.ttf')\nfig, ax = plt.subplots(1, 2, figsize=(16, 9))\nfig.set_facecolor(background_color)\nax[0].set_facecolor(background_color)\nax[1].set_facecolor(background_color)\n\nax[0].plot(range(len(all_loss_G)), all_loss_G, color='#bc6c25', lw=0.5) \nax[1].plot(range(len(all_loss_D)), all_loss_D, color='#00b4d8', lw=0.5)\n\nax[0].scatter(\n      [np.array(all_loss_G).argmax(), np.array(all_loss_G).argmin()],\n      [np.array(all_loss_G).max(), np.array(all_loss_G).min()],\n      s=30, color='#bc6c25',\n   )\nax[1].scatter(\n      [np.array(all_loss_D).argmax(), np.array(all_loss_D).argmin()],\n      [np.array(all_loss_D).max(), np.array(all_loss_D).min()],\n      s=30, color='#00b4d8',\n   )\n\nax.text(\n      np.array(all_loss_G).argmax()+60, np.array(all_loss_G).max()+0.1,\n      f'{round(np.array(all_loss_G).max(),1)}',\n      fontsize=13, color='#bc6c25',\n      font=font,\n      ax=ax[0]\n   )\nax.text(\n      np.array(all_loss_G).argmin()+60, np.array(all_loss_G).min()-0.1,\n      f'{round(np.array(all_loss_G).min(),1)}',\n      fontsize=13, color='#bc6c25',\n      font=font,\n      ax=ax[0]\n   )\n\nax.text(\n      np.array(all_loss_D).argmax()+60, np.array(all_loss_D).max()+0.01,\n      f'{round(np.array(all_loss_D).max(),1)}',\n      fontsize=13, color='#00b4d8',\n      font=font,\n      ax=ax[1]\n   )\nax.text(\n      np.array(all_loss_D).argmin()+60, np.array(all_loss_D).min()-0.005,\n      f'{round(np.array(all_loss_D).min(),1)}',\n      fontsize=13, color='#00b4d8',\n      font=font,\n      ax=ax[1]\n   )\nfor i in range(2):\n    ax[i].tick_params(axis='x', colors='white')\n    ax[i].tick_params(axis='y', colors='white')\n    ax[i].spines['left'].set_color('white') \n    ax[i].spines['bottom'].set_color('white') \n    ax[i].set_xlabel('Epoch', color='white', fontproperties=font, fontsize=13)\n    ax[i].set_ylabel('Loss', color='white', fontproperties=font, fontsize=13)\n\nax[0].set_title('Generator', color='white', fontproperties=font, fontsize=18)\nax[1].set_title('Discriminator', color='white', fontproperties=font, fontsize=18)\nplt.savefig('Loss.jpg')\nplt.show()\n# ax[0].set_axis_off()\n# ax[1].set_axis_off()\n```\n\n또한 테스트 데이터셋에서 임의의 샘플을 시각화할게요:\n\n```js\nrandom.Random(2).shuffle(test_target_img_path)\nrandom.Random(2).shuffle(test_input_img_paths)\nsubset_loader = dataset(batch_size=5, img_size=img_size, images_paths=test_input_img_paths,\n                        targets=test_target_img_path)\ngenerator = UNet()\ngenerator.load_state_dict(torch.load('generator.pth'))\n\ngenerator.eval()\nfor X, y in subset_loader:\n    fig, axes = plt.subplots(5, 3, figsize=(9, 9))\n\n    for i in range(5):\n        axes[i, 0].imshow(np.transpose(X.numpy()[i], (1, 2, 0)))\n        axes[i, 0].set_title(\"Input\")\n        axes[i, 0].axis('off')\n        \n        axes[i, 1].imshow(np.transpose(y.numpy()[i], (1, 2, 0)))\n        axes[i, 1].set_title(\"Target\")\n        axes[i, 1].axis('off')\n        \n        generated_image = generator(X[i].unsqueeze(0)).detach().numpy()[0]\n        axes[i, 2].imshow(np.transpose(generated_image, (1, 2, 0)))\n        axes[i, 2].set_title(\"Generated\")\n        axes[i, 2].axis('off')\n    \n    # 레이아웃 조정\n    plt.tight_layout()\n    plt.savefig('Test.jpg')\n    plt.show()\n    break \n```\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_6.png\" />\n\n여기서 보시다시피, 결과는 완벽하지 않고 땅커버 유형에 매우 의존합니다. 그럼에도 불구하고, 구축된 모델은 이미지에서 구름을 제거하며, G 및 D 깊이를 늘리는 것으로 성능을 향상시킬 수 있습니다. 다른 유망한 전략은 서로 다른 땅커버 유형을 위해 별도의 모델을 훈련시키는 것입니다. 예를 들어, 작물밭과 물 투구는 분명히 다른 공간적 특징을 가지고 있기 때문에 일반화 모델의 능력에 영향을 주는 경우가 있습니다.\n\n이 기사가 지리정보 도메인에서 심층 학습 알고리즘을 적용하는 데 새로운 시각을 제공해 드렸기를 바랍니다. 내 생각에는, GANs는 데이터 과학자가 활용할 수 있는 가장 강력한 도구 중 하나이며, 여러분의 도구 상자의 필수적인 부분이 되길 희망합니다!\n\n===========================================\n\n<div class=\"content-ad\"></div>\n\n참고문헌:\n\n1. Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville 및 Yoshua Bengio. “Generative adversarial nets.” Advances in neural information processing systems 27 (2014). [논문 링크](https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)\n\n2. Helber, Patrick, Benjamin Bischke, Andreas Dengel 및 Damian Borth. “Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 12, no. 7 (2019): 2217–2226. [논문 링크](https://arxiv.org/pdf/1709.00029)\n\n3. Wen, Xue, Zongxu Pan, Yuxin Hu 및 Jiayin Liu. “Generative adversarial learning in YUV color space for thin cloud removal on satellite imagery.” Remote Sensing 13, no. 6 (2021): 1079. [논문 링크](https://www.mdpi.com/2072-4292/13/6/1079)\n\n<div class=\"content-ad\"></div>\n\n5. Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. “U-net: Convolutional networks for biomedical image segmentation.” In Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5–9, 2015, proceedings, part III 18, pp. 234–241. Springer International Publishing, 2015. [Link](https://arxiv.org/pdf/1505.04597)\n\n6. He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. [Link](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n\n===========================================\n\n<div class=\"content-ad\"></div>\n\n제 Medium의 모든 게시물은 무료이며 공개되어 있습니다. 그래서 여기서 저를 팔로우해 주시면 정말 감사하겠습니다!\n\nP.s. 저는 (지리)데이터 과학, 머신 러닝/인공지능, 기후 변화에 대해 열정적으로 관심을 가지고 있습니다. 그래서 어떤 프로젝트에서 함께 작업하고 싶다면 LinkedIn에서 연락 주세요.\n\n🛰️더 많은 소식을 받아보려면 팔로우하세요!🛰️","ogImage":{"url":"/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_0.png"},"coverImage":"/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_0.png","tag":["Tech"],"readingTime":25},{"title":"파이토치PyTorch를 사용하여 처음부터 Large Language Model LLM을 만들어 보세요","description":"","date":"2024-06-19 18:43","slug":"2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch","content":"\n\n## LLM 만들고 트레이닝하는 단계별 가이드입니다. 이 모델의 목표는 영어를 말레이어로 번역하는 것입니다.\n\n![이미지](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_0.png)\n\n이 글을 마치면 어떤 결과를 얻을 수 있을까요? 직접 코딩하면서 Large Language Model (LLM)을 만들고 트레이닝할 수 있게 될 거에요. 영어를 말레이어로 번역하는 LLM을 만들지만, 다른 언어 번역 작업을 위해 이 LLM 아키텍처를 쉽게 수정할 수 있습니다.\n\nLLM은 ChatGPT, Gemini, MetaAI, Mistral AI 등과 같은 인기 있는 AI 챗봇의 핵심 기반이 됩니다. 모든 LLM의 핵심에는 Transformer라는 아키텍처가 있습니다. 따라서, 먼저 유명한 논문 \"Attention is all you need\"을 바탕으로 Transformer 아키텍처를 구축할 것입니다 - https://arxiv.org/abs/1706.03762.\n\n<div class=\"content-ad\"></div>\n\n![transformer_step_by_step](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_1.png)\n\n먼저, 우리는 트랜스포머 모델의 모든 구성 요소를 블록 단위로 구축할 것입니다. 그런 다음, 모든 블록을 조합하여 모델을 구축할 것입니다. 그 후에는 Hugging Face 데이터셋에서 얻을 데이터셋으로 모델을 훈련하고 유효성을 검사할 것입니다. 마지막으로 새 번역 텍스트 데이터에 대한 번역을 수행하여 모델을 테스트할 것입니다.\n\n중요 사항: 저는 트랜스포머 아키텍처의 모든 구성 요소를 단계별로 코딩하고 '무엇, 왜, 어떻게'에 대한 개념에 대한 필요한 설명을 제공할 것입니다. 또한 설명이 필요한 특정 코드 라인에 대해 주석을 제공할 것입니다. 이렇게 하면 직접 코딩하면서 전체 워크플로에 연결할 수 있을 것이라고 믿습니다.\n\n![transformer_architecture](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_2.png)\n\n<div class=\"content-ad\"></div>\n\n함께 코딩해요!\n\n## 단계 1: 데이터셋 로드\n\nLLM 모델이 영어에서 말레이어로 번역하는 작업을 할 수 있도록 하려면 소스(영어)와 대상(말레이어) 언어 쌍이 있는 데이터셋을 사용해야 합니다. 따라서, Huggingface에서 \"Helsinki-NLP/opus-100\"라는 데이터셋을 사용할 것입니다. 이 데이터셋은 1백만 개의 영어-말레이어 훈련 데이터셋을 가지고 있어서 좋은 정확도를 얻기에 충분하며, 검증 및 테스트 데이터셋에 각각 2,000개의 데이터가 있습니다. 데이터는 이미 사전 분할되어 있어서 데이터셋을 다시 분할할 필요가 없습니다.\n\n\n# 필요한 라이브러리 가져오기\n# 아직 안 했다면 (!pip install datasets, tokenizers)를 사용하여 데이터셋 및 토크나이저 라이브러리 설치하기.\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# GPU가 사용 가능하다면 \"cuda\"로 장치 값을 할당하여 GPU에서 훈련합니다. 사용할 수 없는 경우 기본값인 \"cpu\"로 되돌릴 것입니다.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n\n# Huggingface 경로에서 훈련, 검증, 테스트 데이터셋을 로드합니다.\nraw_train_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ms\", split='train')\nraw_validation_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ms\", split='validation')\nraw_test_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ms\", split='test')\n\n# 데이터셋 파일을 저장할 디렉토리 생성\nos.mkdir(\"./dataset-en\")\nos.mkdir(\"./dataset-my\")\n\n# 각 에폭 후 모델 훈련 중에 모델을 저장할 디렉토리 생성 (단계 10에서 사용).\nos.mkdir(\"./malaygpt\")\n\n# 소스 및 대상 토크나이저를 저장할 디렉토리 생성.\nos.mkdir(\"./tokenizer_en\")\nos.mkdir(\"./tokenizer_my\")\n\ndataset_en = []\ndataset_my = []\nfile_count = 1\n\n# 토크나이저를 훈련하기 위해 (단계 2에서) 훈련 데이터셋을 영어 및 말레이어로 분리합니다.\n# 각 파일에 50,000개씩 작은 데이터 파일을 만들어 dataset-en 및 dataset-my 디렉토리에 저장합니다.\nfor data in tqdm(raw_train_dataset[\"translation\"]):\n    dataset_en.append(data[\"en\"].replace('\\n', \" \"))\n    dataset_my.append(data[\"ms\"].replace('\\n', \" \"))\n    if len(dataset_en) == 50000:\n        with open(f'./dataset-en/file{file_count}.txt', 'w', encoding='utf-8') as fp:\n            fp.write('\\n'.join(dataset_en))\n            dataset_en = []\n\n        with open(f'./dataset-my/file{file_count}.txt', 'w', encoding='utf-8') as fp:\n            fp.write('\\n'.join(dataset_my))\n            dataset_my = []\n        file_count += 1\n\n\n<div class=\"content-ad\"></div>\n\n## 단계 2: 토크나이저 생성\n\n트랜스포머 모델은 원시 텍스트를 처리하지 않으며, 숫자만 처리합니다. 따라서 원시 텍스트를 숫자로 변환하기 위해 어떤 작업을 해야 할 것입니다. 이를 위해 저희는 GPT3와 같은 모델에서 사용되는 서브워드 토크나이저인 BPE 토크나이저를 사용할 것입니다. 먼저 우리가 단계 1에서 준비한 코퍼스 데이터(이 경우 교육 데이터 셋)로 BPE 토크나이저를 먼저 학습할 것입니다. 아래 다이어그램과 같이 진행됩니다.\n\n![image](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_3.png)\n\n학습이 완료되면 토크나이저는 영어와 말레이 언어용 어휘를 생성합니다. 어휘는 코퍼스 데이터에서 고유한 토큰들의 컬렉션입니다. 번역 작업을 수행하기 때문에 두 언어에 대한 토크나이저가 필요합니다. BPE 토크나이저는 원시 텍스트를 가져와 어휘 내의 토큰들과 매핑한 후, 입력된 원시 텍스트의 각 단어에 대해 토큰을 반환합니다. 토큰은 단일 단어나 서브워드가 될 수 있습니다. 이는 다른 토크나이저에 비해 서브워드 토크나이저의 장점 중 하나입니다. 그리고 토크나이저는 그 고유한 인덱스 또는 위치 ID를 반환하고, 이는 위의 흐름에서 임베딩을 생성하는 데 추가로 사용될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 토크나이저 라이브러리 클래스 및 모듈 가져오기.\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\n# 토크나이저를 훈련시킬 학습 데이터셋 파일 경로.\npath_en = [str(file) for file in Path('./dataset-en').glob(\"**/*.txt\")]\npath_my = [str(file) for file in Path('./dataset-my').glob(\"**/*.txt\")]\n\n# [원본 언어 토크나이저(영어) 생성].\n# [UNK] - 알 수 없는 단어를 나타내는 특수 토큰 생성, [PAD] - 패딩 토큰으로 모델 간 시퀀스 길이를 일정하게 유지하기 위함.\n# [CLS] - 문장 시작을 표시하는 토큰, [SEP] - 문장 끝을 표시하는 토큰 등의 추가 특수 토큰 생성.\ntokenizer_en = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer_en = BpeTrainer(min_frequency=2, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\", \"[SEP]\", \"[MASK]\"])\n\n# 토큰을 공백을 기준으로 분리.\ntokenizer_en.pre_tokenizer = Whitespace()\n\n# 데이터셋 파일로 토크나이저 훈련.\ntokenizer_en.train(files=path_en, trainer=trainer_en)\n\n# 향후 사용을 위해 토크나이저 저장.\ntokenizer_en.save(\"./tokenizer_en/tokenizer_en.json\")\n\n# [타겟 언어 토크나이저(말레이어) 생성].\ntokenizer_my = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer_my = BpeTrainer(min_frequency=2, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\", \"[SEP]\", \"[MASK]\"])\ntokenizer_my.pre_tokenizer = Whitespace()\ntokenizer_my.train(files=path_my, trainer=trainer_my)\ntokenizer_my.save(\"./tokenizer_my/tokenizer_my.json\")\n\ntokenizer_en = Tokenizer.from_file(\"./tokenizer_en/tokenizer_en.json\")\ntokenizer_my = Tokenizer.from_file(\"./tokenizer_my/tokenizer_my.json\")\n\n# 각 토크나이저의 크기 확인.\nsource_vocab_size = tokenizer_en.get_vocab_size()\ntarget_vocab_size = tokenizer_my.get_vocab_size()\n\n# 토큰 ID 변수 정의, 모델 훈련에 사용.\nCLS_ID = torch.tensor([tokenizer_my.token_to_id(\"[CLS]\")], dtype=torch.int64).to(device)\nSEP_ID = torch.tensor([tokenizer_my.token_to_id(\"[SEP]\")], dtype=torch.int64).to(device)\nPAD_ID = torch.tensor([tokenizer_my.token_to_id(\"[PAD]\")], dtype=torch.int64).to(device)\n```\n\n## Step 3: 데이터셋 및 DataLoader 준비\n\n이 단계에서는 나중에 구축할 모델을 훈련하고 검증하기 위해 소스 언어와 타겟 언어 각각에 대한 데이터셋을 준비할 것입니다. 우리는 원시 데이터셋을 입력으로 받아 소스(토크나이저_en)와 타겟(토크나이저_my) 텍스트를 각각 인코딩하는 기능을 정의하는 클래스를 생성할 것입니다. 마지막으로, 훈련 및 검증 데이터셋을 위해 DataLoader를 생성하겠습니다. 이 DataLoader는 배치 단위로 데이터셋을 반복하며(예: 배치 크기는 10으로 설정될 수 있음), 데이터 크기와 사용 가능한 처리 능력에 따라 배치 크기를 조정할 수 있습니다.\n\n```js\n# 이 클래스는 원시 데이터셋과 max_seq_len (전체 데이터셋에서 시퀀스의 최대 길이)을 가져옵니다.\nclass EncodeDataset(Dataset):\n    def __init__(self, raw_dataset, max_seq_len):\n        super().__init__()\n        self.raw_dataset = raw_dataset\n        self.max_seq_len = max_seq_len\n    \n    def __len__(self):\n        return len(self.raw_dataset)\n\n    def __getitem__(self, index):\n        \n        # 주어진 인덱스의 원시 텍스트를 가져와 소스 및 타겟 텍스트로 분리함.\n        raw_text = self.raw_dataset[index]\n        \n        # 소스 텍스트와 타겟 텍스트를 인코딩하기 위해 소스 토크나이저(tokenizer_en) 및 타겟 토크나이저(tokenizer_my)를 사용합니다.\n        source_text_encoded = torch.tensor(tokenizer_en.encode(source_text).ids, dtype = torch.int64).to(device)    \n        target_text_encoded = torch.tensor(tokenizer_my.encode(target_text).ids, dtype = torch.int64).to(device)\n\n        # 모델 훈련을 위해 각 입력 시퀀스의 길이가 max_seq_len과 동일하도록 만들기 위해 필요한 만큼의 패딩을 추가합니다.\n        num_source_padding = self.max_seq_len - len(source_text_encoded) - 2 \n        num_target_padding = self.max_seq_len - len(target_text_encoded) - 1 \n\n        encoder_padding = torch.tensor([PAD_ID] * num_source_padding, dtype = torch.int64).to(device)\n        decoder_padding = torch.tensor([PAD_ID] * num_target_padding, dtype = torch.int64).to(device)\n\n        # 인코더 입력은 문장 시작 토큰인 CLS_ID로 시작하여 소스 인코딩으로 이어지고 문장 끝 토큰인 SEP가 뒤따릅니다.\n        # 필요한 max_seq_len에 도달하기 위해 마지막에 PAD 토큰이 추가됩니다.\n        encoder_input = torch.cat([CLS_ID, source_text_encoded, SEP_ID, encoder_padding]).to(device)\n\n        # 디코더 입력은 문장 시작 토큰인 CLS_ID로 시작하여 타겟 인코딩이 뒤따릅니다.\n        # 필요한 max_seq_len에 도달하기 위해 마지막에 PAD 토큰이 추가됩니다. 디코더 입력에는 문장 끝 토큰인 SEP는 포함되지 않습니다.\n        decoder_input = torch.cat([CLS_ID, target_text_encoded, decoder_padding]).to(device)\n\n        # 타겟 레이블은 타겟 인코딩이 먼저 오고 문장 끝 토큰인 SEP가 뒤따릅니다. 시작 문장 토큰인 CLS는 없습니다.\n        # 필요한 max_seq_len에 도달하기 위해 마지막에 PAD 토큰이 추가됩니다.\n        target_label = torch.cat([target_text_encoded,SEP_ID,decoder_padding]).to(device)\n\n        # 인코딩 시 추가된 패딩 토큰을 모델이 학습하지 않도록 하기 위해 인코더 마스크를 사용하여 padding 토큰 값을 계산하기 전에 무효화합니다.\n        encoder_mask = (encoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int().to(device)\n\n        # 디코딩 단계에서는 현재 토큰 이후의 토큰에 영향을 받지 않도록 하기 위해 인과 마스크를 구현합니다.\n        decoder_mask = (decoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)).to(device)\n\n        return {\n            'encoder_input': encoder_input,\n            'decoder_input': decoder_input,\n            'target_label': target_label,\n            'encoder_mask': encoder_mask,\n            'decoder_mask': decoder_mask,\n            'source_text': source_text,\n            'target_text': target_text\n        }\n\n# 인과 마스크는 현재 토큰 이후에 올 토큰을 마스킹하여 softmax 함수 이후 -inf로 대체됩니다. 이를 통해 모델은 이러한 값들을 무시하거나 이를 통해 학습을 어렵게 합니다.\ndef causal_mask(size):\n  # 인과 마스크의\n\n<div class=\"content-ad\"></div>\n\n## 단계 4: 입력 임베딩 및 위치 인코딩\n\n입력 임베딩: 단계 2의 토큰 생성기에서 생성된 토큰 ID 시퀀스가 임베딩 레이어로 공급될 것입니다. 임베딩 레이어는 토큰 ID를 어휘와 매핑하고 각 토큰에 대해 512 차원의 임베딩 벡터를 생성합니다. [512 차원은 어텐션 논문에서 가져왔습니다]. 임베딩 벡터는 토큰의 의미를 캡쳐할 수 있으며, 그것은 학습된 데이터셋에 기반하여 학습되었습니다. 임베딩 벡터 내의 각 차원 값은 토큰과 관련된 특징을 나타냅니다. 예를 들어, 토큰이 '개'인 경우, 일부 차원 값은 눈, 입, 다리, 키 등을 나타낼 것입니다. n차원 공간에 벡터를 그린다면, 비슷해 보이는 객체인 개와 고양이는 서로 가깝게 위치하고, 비슷해 보이지 않는 학교, 집 임베딩 벡터는 훨씬 더 멀리 위치해 있을 것입니다.\n\n위치 인코딩: 트랜스포머 아키텍처의 장점 중 하나는 어떤 수의 입력 시퀀스든 병렬로 처리할 수 있다는 것이며, 이는 훈련 시간을 많이 줄이고 예측을 훨씬 빠르게 만듭니다. 그러나 단점 중 하나는 병렬로 많은 토큰 시퀀스를 처리하는 동안, 문장 내 토큰의 위치가 순서대로 되지 않을 수 있다는 것입니다. 이로 인해 토큰의 위치에 따라 문장의 의미나 문맥이 달라질 수 있습니다. 따라서 이 문제를 해결하기 위해 어텐션 논문은 위치 인코딩 방법을 구현했습니다. 이 논문은 각 토큰의 512 차원에 대해 두 가지 수학 함수(sin과 cosine)를 적용하는 것을 제안했습니다. 아래는 간단한 sin과 cosine 수학 함수입니다.\n\n![이미지](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_4.png)\n\n<div class=\"content-ad\"></div>\n\nSin 함수는 임베딩 벡터의 각 짝수 차원 값에 적용되고, 코사인 함수는 홀수 차원 값에 적용됩니다. 최종적으로, 결과적인 위치 인코더 벡터는 임베딩 벡터에 추가됩니다. 이제 우리는 토큰의 의미와 위치를 모두 잡을 수 있는 임베딩 벡터를 갖게 되었습니다. 주의할 점은 위치 인코딩의 값이 각 시퀀스에서 동일하다는 것입니다.\n\n# 입력 임베딩과 위치 인코딩\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int):\n        super().__init__()\n        self.d_model = d_model\n        \n        # PyTorch의 임베딩 레이어 모듈을 사용하여 토큰 ID를 어휘에 매핑한 후 임베딩 벡터로 변환합니다.\n        # vocab_size는 훈련 데이터셋의 어휘 크기이며, 토큰화기가 코퍼스 데이터셋 훈련 중에 생성한 것입니다.\n        self.embedding = nn.Embedding(vocab_size, d_model)\n    \n    def forward(self, input):\n        # 입력 시퀀스를 임베딩 레이어에 공급할 때, d_model의 제곱근을 곱하는 추가로 정규화 작업을 수행합니다.\n        embedding_output = self.embedding(input) * math.sqrt(self.d_model)\n        return embedding_output\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, max_seq_len: int, d_model: int, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # 임베딩 벡터와 동일한 모양의 행렬을 만듭니다.\n        pe = torch.zeros(max_seq_len, d_model)\n        \n        # PE 함수의 위치 부분을 계산합니다.\n        pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n\n        # PE 함수의 나눗셈 부분을 계산합니다. 지수 함수의 표현이 논문 표현과 약간 다르지만 더 잘 작동하는 것으로 보입니다.\n        div_term = torch.exp(torch.arange(0, d_model, 2).float()) * (-math.log(10000)/d_model)\n        \n        # sin 및 cosine 수학 함수 결과로 홀수 및 짝수 행렬 값을 채웁니다.\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n        \n        # 입력 시퀀스가 배치로 예상되므로 추가적인 batch_size 차원이 0 위치에 추가됩니다.\n        pe = pe.unsqueeze(0)    \n    \n    def forward(self, input_embdding):\n        # 입력 임베딩 벡터에 위치 인코딩을 추가합니다.\n        input_embdding = input_embdding + (self.pe[:, :input_embdding.shape[1], :]).requires_grad_(False)  \n        \n        # 과적합을 방지하기 위해 드롭아웃을 수행합니다.\n        return self.dropout(input_embdding)\n\n## 단계 5: 멀티 헤드 어텐션 블록\n\n트랜스포머가 LLM의 핵심인 것처럼, 셀프 어텐션 메커니즘은 트랜스포머 아키텍처의 핵심입니다.\n\n<div class=\"content-ad\"></div>\n\n자가 주의가 필요한 이유는 무엇인가요? 아래 간단한 예를 통해 답변해보겠습니다.\n\n![Example](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_5.png)\n\n문장 1과 문장 2에서 “bank”라는 단어는 명백히 두 가지 다른 의미를 가지고 있습니다. 그러나 “bank” 단어의 임베딩 값은 두 문장 모두에서 동일합니다. 이는 적절하지 않습니다. 우리는 임베딩 값이 문맥에 따라 변경되어야 한다는 것을 원합니다. 따라서 문장의 전체 의미에 기반하여 문맥적 의미를 나타낼 수 있는 동적 임베딩 값을 가지는 메커니즘이 필요합니다. 자가 주의 메커니즘은 문장의 전체 의미에 기반하여 문맥적 의미를 나타낼 수 있는 임베딩 값을 동적으로 업데이트할 수 있습니다.\n\n자가 주의가 이미 좋은데, 왜 다중 머리 자가 주의가 필요할까요? 답을 알아보기 위해 아래 예시를 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n```\n![image](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_6.png)\n\n이 예에서는 self-attention을 사용할 때 문장의 한 측면에만 집중할 수 있는 가능성이 있습니다. 아마도 \"what\" 측면만을 포착할 수 있을 것입니다. 예를 들어, \"존이 무엇을 했나요?\"와 같이요. 그러나 \"언제\"나 \"어디\"와 같은 다른 측면들도 모델이 더 나은 성능을 발휘하기 위해 동등한 중요성을 갖습니다. 그래서, Self-Attention 메커니즘이 한 문장 내에서 여러 관계를 학습하도록 하는 방법을 찾아야 합니다. 이것이 Multi-Head Self Attention(Multi-Head Attention으로도 교차 사용 가능)이 해결해 주는 곳이죠. Multi-Head Attention에서는 단일 헤드 임베딩을 여러 헤드로 분할하여 각 헤드가 문장의 다른 측면을 살펴보고 그에 맞게 학습합니다. 이것이 우리가 원하는 바입니다.\n\n이제 왜 Multi-Head Attention이 필요한지 알게 되었습니다. 이제 어떻게 작용하는지 살펴보겠습니다. Multi-Head Attention은 실제로 어떻게 작동하는 걸까요? 바로 살펴보겠습니다.\n\n행렬 곱셈에 익숙하시다면, 이 메카니즘을 이해하는 것은 꽤 쉬운 작업일 것입니다. 먼저 전체 플로우 다이어그램을 살펴보고 Multi-Head Attention의 입력부터 출력까지의 플로우를 아래 일목요연하게 설명하겠습니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_7.png)\n\n1. 먼저, 인코더 입력의 3개 복사본을 만들어봅시다 (입력 임베딩과 위치 인코딩의 조합, 이것은 단계 4에서 했었습니다). 각각을 Q, K, V라고 이름 붙여봅시다. 이들은 단지 인코더 입력의 복사본일 뿐입니다. 인코더 입력 형태: (seq_len, d_model), seq_len: 최대 시퀀스 길이, d_model: 이 경우에는 임베딩 벡터 차원이 512입니다.\n\n2. 다음으로, Q를 가중치 W_q, K를 가중치 W_k, V를 가중치 W_v와 행렬 곱셈을 수행하겠습니다. 각 가중치 행렬의 형태는 (d_model, d_model)입니다. 새로 얻게 된 쿼리, 키, 밸류 임베딩 벡터의 형태는 (seq_len, d_model)입니다. 가중치 매개변수들은 모델에 의해 무작위로 초기화되며 나준에 모델이 훈련을 시작할 때 업데이트될 것입니다. 어째서 우리가 처음부터 하는 가중치 행렬 곱셈이 필요한 것일까요? 왜냐하면 이것들은 쿼리, 키, 밸류 임베딩 벡터에 더 나은 표현을 제공하기 위해 필요한 학습 가능한 매개변수들이기 때문입니다.\n\n3. 어텐션 논문에 따르면, 헤드(heads) 수는 8입니다. 각 새로운 쿼리, 키, 밸류 임베딩 벡터는 8개의 더 작은 유닛으로 나뉘어집니다. 임베딩 벡터의 새로운 형태는 (seq_len, d_model/num_heads) 또는 (seq_len, d_k)입니다. [ d_k = d_model/num_heads ].\n\n\n<div class=\"content-ad\"></div>\n\n4. 각 쿼리 임베딩 벡터는 자신 및 시퀀스의 모든 다른 임베딩 벡터의 키 임베딩 벡터의 전치와 닷 프로덕트 연산을 수행합니다. 이 닷 프로덕트는 주의 점수를 제공합니다. 주의 점수는 주어진 토큰이 주어진 입력 시퀀스의 다른 모든 토큰과 얼마나 유사한지를 보여줍니다. 점수가 높을수록 유사도가 더 높습니다.\n\n- 주의 점수는 나중에 매트릭스 전체에 걸쳐 점수 값을 정규화하는 데 필요한 d_k의 제곱근으로 나눠집니다. 그러나 왜 d_k로 나눠 정규화해야 하는 걸까요? 어떤 다른 숫자여도 괜찮을 텐데요. 주된 이유는 임베딩 벡터 차원이 증가함에 따라 주의 매트릭스의 총 분산이 비례해서 증가하기 때문입니다. 그래서 d_k로 나누면 분산 증가를 균형시킬 수 있습니다. 만약 d_k로 나누지 않으면, 어떤 높은 주의 점수라도 소프트맥스 함수는 매우 높은 확률 값을 제공하고, 반대로 낮은 주의 점수 값에 대해서는 소프트맥스 함수가 매우 낮은 확률 값을 제공할 것입니다. 이로 인해 모델은 그러한 확률 값이 있는 특징만 학습하려 하고, 낮은 확률 값이 있는 특징을 무시하기 쉽습니다. 이는 그라디언트가 소실되는 문제로 이어지게 됩니다. 따라서 주의 점수 매트릭스를 정규화하는 것이 매우 중요합니다.\n- 소프트맥스 함수를 수행하기 전에, 인코더 마스크가 None이 아닌 경우, 주의 점수는 인코더 마스크와 매트릭스 곱셈이 될 것입니다. 마스크가 인과적 마스크인 경우, 입력 시퀀스에서 그 이후에 오는 임베딩 토큰들에 대한 주의 점수 값은 -ve 무한대로 대체됩니다. 소프트맥스 함수는 -ve 무한대 값을 거의 0 값으로 변환할 것입니다. 그래서 모델은 현재 토큰 이후에 나오는 특징을 학습하지 않을 것입니다. 이것이 우리 모델 학습에 미래 토큰이 영향을 미치는 것을 방지하는 방법입니다.\n\n5. 소프트맥스 함수가 주의 점수 매트릭스에 적용되고 (seq_len, seq_len) 모양의 가중치 매트릭스가 출력됩니다.\n\n6. 이 가중치 매트릭스는 해당 값 임베딩 벡터와 매트릭스 곱셈을 수행할 것입니다. 결과적으로 (seq_len, d_v) 모양의 8개의 주의 헤드가 생성됩니다. [ d_v = d_model/num_heads ].\n\n<div class=\"content-ad\"></div>\n\n그러면, 모든 헤드들이 새로운 형태(seq_len, d_model)를 갖는 단일 헤드로 연결됩니다. 이 새로운 단일 헤드는 출력 가중치 행렬 W_o(d_model, d_model)과 행렬 곱셈을 수행합니다. Multi-Head Attention의 최종 출력은 단어의 문맥적 의미와 입력 문장의 여러 측면을 학습하는 능력을 나타냅니다.\n\n이제 Multi-Head Attention 블록 코딩을 시작해봅시다. 이것은 훨씬 쉽고 간단할 거예요.\n\n```js\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout_rate: float):\n        super().__init__()\n        # 과적합을 방지하기 위해 드롭아웃을 정의합니다.\n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # 가중치 행렬은 도입되며 모두 학습 가능한 매개변수입니다.\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        self.num_heads = num_heads\n        assert d_model % num_heads == 0, \"d_model은 헤드 수로 나눌 수 있어야 합니다.\"\n        \n        # d_k는 각 분할된 self attention 헤드의 새로운 차원입니다\n        self.d_k = d_model // num_heads\n\n    def forward(self, q, k, v, encoder_mask=None):\n        \n        # 여러 시퀀스 배치로 모델을 한 번에 병렬로 학습하게 될 것이므로, 모양에 배치 크기를 포함해야 합니다.\n        # 쿼리, 키 및 값은 해당 가중치와 입력 임베딩의 행렬 곱으로 계산됩니다.\n        # 모양 변화: q(배치 크기, seq_len, d_model) @ W_q(d_model, d_model) => query(배치 크기, seq_len, d_model) [키와 값도 동일함].\n        query = self.W_q(q) \n        key = self.W_k(k)\n        value = self.W_v(v)\n\n        # 쿼리, 키, 밸류를 헤드 수로 분할합니다. d_model은 d_k마다 8개 헤드로 분할됩니다.\n        # 모양 변화: query(배치 크기, seq_len, d_model) => query(배치 크기, seq_len, num_heads, d_k) -> query(배치 크기, num_heads, seq_len, d_k) [키와 값도 동일함].\n        query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1,2)\n        key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1,2)\n        value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1,2)\n\n        # :: SELF ATTENTION BLOCK STARTS ::\n\n        # 유사도 또는 쿼리 간의 관련성을 찾기 위해 주의 점수가 계산됩니다.\n        # 모양 변화: query(배치 크기, num_heads, seq_len, d_k) @ key(배치 크기, num_heads, seq_len, d_k) => attention_score(배치 크기, num_heads, seq_len, seq_len).\n        attention_score = (query @ key.transpose(-2,-1)) / math.sqrt(self.d_k)\n\n        # 만약 마스크가 제공된다면, 주의 점수를 마스크 값에 따라 수정해야 합니다. 자세한 내용은 4번을 참조하세요.\n        if encoder_mask is not None:\n            attention_score = attention_score.masked_fill(encoder_mask == 0, -1e9)\n        \n        # softmax 함수는 모든 주의 점수 중에서 확률 분포를 계산합니다. 더 높은 주의 점수에 더 높은 확률 값을 할당합니다. 즉, 보다 유사한 토큰은 더 높은 확률 값을 가집니다.\n        # 모양 변화: attention_score와 동일합니다.\n        attention_weight = torch.softmax(attention_score, dim=-1)\n\n        if self.dropout is not None:\n            attention_weight = self.dropout(attention_weight)\n\n        # Self attention 블록의 최종 단계는 주의 가중치를 값 임베딩 벡터와의 행렬 곱셈입니다.\n        # 모양 변화: attention_score(배치 크기, num_heads, seq_len, seq_len) @ value(배치 크기, num_heads, seq_len, d_k) => attention_output(배치 크기, num_heads, seq_len, d_k)\n        attention_output = attention_score @ value\n        \n        # :: SELF ATTENTION BLOCK ENDS ::\n\n        # 이제, 모든 헤드들이 다시 단일 헤드로 결합됩니다.\n        # 모양 변화: attention_output(배치 크기, num_heads, seq_len, d_k) => attention_output(배치 크기, seq_len, num_heads, d_k) => attention_output(배치 크기, seq_len, d_model)        \n        attention_output = attention_output.transpose(1,2).contiguous().view(attention_output.shape[0], -1, self.num_heads * self.d_k)\n\n        # 마침내 attention_output을 출력 가중치 행렬로 행렬 곱하여 최종 Multi-Head attention 출력을 얻습니다.\n        # multihead_output의 모양은 임베딩 입력과 동일합니다.\n        # 모양 변화: attention_output(배치 크기, seq_len, d_model) @ W_o(d_model, d_model) => multihead_output(배치 크기, seq_len, d_model)\n        multihead_output = self.W_o(attention_output)\n        \n        return multihead_output\n```\n\n## Step 6: 피드포워드 네트워크, 레이어 정규화 및 AddAndNorm\n\n<div class=\"content-ad\"></div>\n\n피드포워드 네트워크: 피드포워드 네트워크는 두 개의 선형 레이어(첫 번째는 d_model 노드를 가지고 두 번째는 d_ff 노드를 가지며, 주어진 값은 어텐션 논문에 따라 할당됩니다)를 통해 임베딩 벡터의 모든 기능을 학습하는 딥 신경망을 사용합니다. 첫 번째 선형 레이어의 출력에는 ReLU 활성화 함수가 적용되어 임베딩 값을 비선형으로 만들고, 과적합을 피하기 위해 드롭아웃이 적용됩니다.\n\n레이어 정규화: 네트워크 내 임베딩 벡터의 값 분포가 일관되게 유지되도록 임베딩 값에 레이어 정규화를 적용합니다. 이는 원활한 학습을 보장합니다. 네트워크가 필요로 하는대로 임베딩 값을 스케일링하고 이동시키기 위해 gamma와 beta라는 추가 학습 매개변수를 사용할 것입니다.\n\nAddAndNorm: 이는 스킵 연결과 레이어 정규화(이전에 설명함)로 구성됩니다. 순방향 패스에서 스킵 연결은 이전 레이어의 기능이 계산 결과에 필요한 기여를 할 수 있도록 나중 단계에서도 해당 기능을 기억할 수 있습니다. 마찬가지로 역전파 중에도 스킵 연결은 각 단계에서 하나 덜의 역전파를 수행해 사라지는 기울기를 방지합니다. AddAndNorm은 인코더(2번)와 디코더 블록(3번) 모두에 사용됩니다. 이는 이전 레이어에서 입력을 받아 이전 레이어의 출력에 추가하기 전에 먼저 정규화합니다.\n\n```js\n# Feedfoward Network, Layer Normalization and AddAndNorm Block\nclass FeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout_rate: float):\n        super().__init__()\n\n        self.layer_1 = nn.Linear(d_model, d_ff)\n        self.activation_1 = nn.ReLU()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_2 = nn.Linear(d_ff, d_model)\n    \n    def forward(self, input):\n        return self.layer_2(self.dropout(self.activation_1(self.layer_1(input))))\n\nclass LayerNorm(nn.Module):\n    def __init__(self, eps: float = 1e-5):\n        super().__init__()\n        # 엡실론은 매우 작은 값으로, 잠재적으로 0으로 나누는 문제를 방지하는 데 중요한 역할을 합니다.\n        self.eps = eps\n\n        # 스케일링과 이동을 위해 추가 학습 매개변수인 감마와 베타를 도입합니다.\n        self.gamma = nn.Parameter(torch.ones(1))\n        self.beta = nn.Parameter(torch.zeros(1))\n    \n    def forward(self, input):\n        mean = input.mean(dim=-1, keepdim=True)      \n        std = input.std(dim=-1, keepdim=True)      \n\n        return self.gamma * ((input - mean)/(std + self.eps)) + self.beta\n        \nclass AddAndNorm(nn.Module):\n    def __init__(self, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_norm = LayerNorm()\n\n    def forward(self, input, sub_layer):\n        return input + self.dropout(sub_layer(self.layer_norm(input)))\n```\n\n<div class=\"content-ad\"></div>\n\n## 단계 7: 인코더 블록과 인코더\n\n인코더 블록: 인코더 블록 안에는 두 가지 주요 구성 요소가 있습니다: 멀티헤드 어텐션과 피드포워드입니다. Add & Norm 단위가 2개 있습니다. 먼저 어텐션 논문의 흐름에 따라 EncoderBlock 클래스에 모든 이러한 구성 요소를 조립할 것입니다. 논문에 따르면 이 인코더 블록은 6번 반복된다고 합니다.\n\n인코더: 그런 다음 EncoderBlock 목록을 가져와 쌓아 최종 Encoder 출력을 제공할 Encoder라는 추가 클래스를 생성할 것입니다.\n\n```python\nclass EncoderBlock(nn.Module):\n    def __init__(self, multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float):\n        super().__init__()\n        self.multihead_attention = multihead_attention\n        self.feed_forward = feed_forward\n        self.add_and_norm_list = nn.ModuleList([AddAndNorm(dropout_rate) for _ in range(2)])\n\n    def forward(self, encoder_input, encoder_mask):\n        # 인코더 입력을 스킵 연결에서 가져와 멀티헤드 어텐션 블록의 출력과 더하는 첫 번째 AddAndNorm 단위입니다.\n        encoder_input = self.add_and_norm_list[0](encoder_input, lambda encoder_input: self.multihead_attention(encoder_input, encoder_input, encoder_input, encoder_mask))\n        \n        # 멀티헤드 어텐션 블록의 출력을 스킵 연결에서 가져와 피드포워드 레이어의 출력과 더하는 두 번째 AddAndNorm 단위입니다.\n        encoder_input = self.add_and_norm_list[1](encoder_input, self.feed_forward)\n\n        return encoder_input\n\nclass Encoder(nn.Module):\n    def __init__(self, encoderblocklist: nn.ModuleList):\n        super().__init__()\n\n        # Encoder 클래스는 encoderblock 목록을 가져와 초기화합니다.\n        self.encoderblocklist = encoderblocklist\n        self.layer_norm = LayerNorm()\n\n    def forward(self, encoder_input, encoder_mask):\n        # 모든 인코더 블록을 반복합니다 - 총 6번.\n        for encoderblock in self.encoderblocklist:\n            encoder_input = encoderblock(encoder_input, encoder_mask)\n\n        # 최종 인코더 블록 출력을 정규화하고 반환합니다. 이 인코더 출력은 나중에 디코더 블록의 교차 어텐션에서 키 및 값으로 사용될 것입니다.\n        encoder_output = self.layer_norm(encoder_input)\n        return encoder_output\n```\n\n<div class=\"content-ad\"></div>\n\n## Step 8: 디코더 블록, 디코더 및 프로젝션 레이어\n\n디코더 블록: 디코더 블록에는 세 가지 주요 구성 요소가 있습니다: 마스킹된 멀티헤드 어텐션, 멀티헤드 어텐션 및 피드포워드입니다. 디코더 블록에는 Add & Norm의 3개 단위도 있습니다. 우리는 이러한 구성 요소들을 Attention 논문의 흐름에 따라 DecoderBlock 클래스에 모두 조합할 것입니다. 논문에 따르면 이 디코더 블록은 6번 반복됩니다.\n\n디코더: 우리는 DecoderBlock의 리스트를 가져와 스택하여 최종 디코더 출력을 생성할 Decoder라는 추가 클래스를 만들 것입니다.\n\n디코더 블록에는 두 가지 타입의 멀티헤드 어텐션이 있습니다. 첫 번째는 Masked Multi-Head 어텐션입니다. 이는 디코더 입력을 쿼리, 키, 밸류로 사용하며 디코더 마스크(인과 마스크로도 알려짐)를 사용합니다. 인과 마스크는 모델이 순서에 앞서있는 임베딩을 볼 수 없게 합니다. 이 동작 방식에 대한 자세한 설명은 3단계와 5단계에 제공되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n프로젝션 레이어: 마지막 디코더 출력은 프로젝션 레이어로 전달됩니다. 이 레이어에서는 먼저 디코더 출력이 먼저 선형 레이어로 공급되어 임베딩의 모양이 아래 코드 섹션에서 제공된대로 변경될 것입니다. 그런 다음 softmax 함수가 디코더 출력을 어휘에 대한 확률 분포로 변환하고 가장 높은 확률을 가진 토큰이 예측 출력으로 선택됩니다.\n\n```js\nclass DecoderBlock(nn.Module):\n    def __init__(self, masked_multihead_attention: MultiHeadAttention, multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float):\n        super().__init__()\n        self.masked_multihead_attention = masked_multihead_attention\n        self.multihead_attention = multihead_attention\n        self.feed_forward = feed_forward\n        self.add_and_norm_list = nn.ModuleList([AddAndNorm(dropout_rate) for _ in range(3)])\n\n    def forward(self, decoder_input, decoder_mask, encoder_output, encoder_mask):\n        # 첫 번째 AddAndNorm 유닛은 디코더 입력을 스킵 연결에서 가져와 마스킹된 멀티헤드 어텐션 블록의 출력과 더합니다.\n        decoder_input = self.add_and_norm_list[0](decoder_input, lambda decoder_input: self.masked_multihead_attention(decoder_input, decoder_input, decoder_input, decoder_mask))\n        # 두 번째 AddAndNorm 유닛은 스킵 연결로부터 마스킹된 멀티헤드 어텐션 블록의 출력을 가져와 멀티헤드 어텐션 블록의 출력과 더합니다.\n        decoder_input = self.add_and_norm_list[1](decoder_input, lambda decoder_input: self.multihead_attention(decoder_input, encoder_output, encoder_output, encoder_mask)) # 교차 어텐션\n        # 세 번째 AddAndNorm 유닛은 멀티헤드 어텐션 블록의 출력을 스킵 연결로부터 가져와 피드포워드 레이어의 출력과 더합니다.\n        decoder_input = self.add_and_norm_list[2](decoder_input, self.feed_forward)\n        return decoder_input\n\nclass Decoder(nn.Module):\n    def __init__(self, decoderblocklist: nn.ModuleList):\n        super().__init__()\n        self.decoderblocklist = decoderblocklist\n        self.layer_norm = LayerNorm()\n\n    def forward(self, decoder_input, decoder_mask, encoder_output, encoder_mask):\n        for decoderblock in self.decoderblocklist:\n            decoder_input = decoderblock(decoder_input, decoder_mask, encoder_output, encoder_mask)\n\n        decoder_output = self.layer_norm(decoder_input)\n        return decoder_output\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int):\n        super().__init__()\n        self.projection_layer = nn.Linear(d_model, vocab_size)\n\n    def forward(self, decoder_output):\n        # 프로젝션 레이어는 먼저 디코더 출력을 받아 (d_model, vocab_size) 모양의 선형 레이어로 전달합니다.\n        # 모양 변경: decoder_output(batch_size, seq_len, d_model) @ linear_layer(d_model, vocab_size) => output(batch_size, seq_len, vocab_size)\n        output = self.projection_layer(decoder_output)\n        \n        # 어휘상의 확률 분포를 출력하기 위해 softmax 함수를 사용합니다.\n        return torch.log_softmax(output, dim=-1)\n```\n\n## 단계 9: 트랜스포머 생성 및 구축\n\n마침내, 트랜스포머 아키텍처의 모든 구성 요소 블록을 구축했습니다. 유일한 미완료 작업은 이 모든 것을 함께 조립하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n먼저, 컴포넌트 클래스의 모든 인스턴스를 초기화하는 Transformer 클래스를 생성합니다. Transform 클래스 내부에는 먼저 인코더 부분의 모든 작업을 수행하고 인코더 출력을 생성하는 encode 함수를 정의합니다.\n\n두 번째로, Transformer의 디코더 부분의 모든 작업을 수행하고 디코더 출력을 생성하는 decode 함수를 정의합니다.\n\n세 번째로, 디코더 출력을 가져와 예측을 위해 해당 어휘에 매핑하는 project 함수를 정의합니다.\n\n이제 Transformer 아키텍처가 준비되었습니다. 이제 아래 코드에서 필요한 모든 매개변수를 사용하여 번역 LLM 모델을 구축할 수 있는 함수를 정의합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nclass Transformer(nn.Module):\n    def __init__(self, source_embed: EmbeddingLayer, target_embed: EmbeddingLayer, positional_encoding: PositionalEncoding, multihead_attention: MultiHeadAttention, masked_multihead_attention: MultiHeadAttention, feed_forward: FeedForward, encoder: Encoder, decoder: Decoder, projection_layer: ProjectionLayer, dropout_rate: float):        \n        super().__init__()\n        \n        # Transformer 아키텍처의 모든 구성 요소 클래스의 인스턴스를 초기화합니다.\n        self.source_embed = source_embed\n        self.target_embed = target_embed\n        self.positional_encoding = positional_encoding\n        self.multihead_attention = multihead_attention        \n        self.masked_multihead_attention = masked_multihead_attention\n        self.feed_forward = feed_forward\n        self.encoder = encoder\n        self.decoder = decoder\n        self.projection_layer = projection_layer\n        self.dropout = nn.Dropout(dropout_rate)\n    \n    # Encode 함수는 인코더 입력을 받아서 모든 인코더 블록 내에서 필요한 처리를 수행하고 인코더 출력을 제공합니다.\n    def encode(self, encoder_input, encoder_mask):\n        encoder_input = self.source_embed(encoder_input)\n        encoder_input = self.positional_encoding(encoder_input)\n        encoder_output = self.encoder(encoder_input, encoder_mask)\n        return encoder_output\n\n    # Decode 함수는 디코더 입력을 받아서 모든 디코더 블록 내에서 필요한 처리를 수행하고 디코더 출력을 제공합니다.\n    def decode(self, decoder_input, decoder_mask, encoder_output, encoder_mask):\n        decoder_input = self.target_embed(decoder_input)\n        decoder_input = self.positional_encoding(decoder_input)\n        decoder_output = self.decoder(decoder_input, decoder_mask, encoder_output, encoder_mask)\n        return decoder_output\n\n    # Projec 함수는 디코더 출력을 투영 레이어로 받아들이고 출력을 어휘로 매핑하여 예측합니다.\n    def project(self, decoder_output):\n        return self.projection_layer(decoder_output)\n\ndef build_model(source_vocab_size, target_vocab_size, max_seq_len=1135, d_model=512, d_ff=2048, num_heads=8, num_blocks=6, dropout_rate=0.1):\n    \n    # Transformer 아키텍처에 필요한 모든 매개변수 값을 정의하고 할당합니다.\n    source_embed = EmbeddingLayer(source_vocab_size, d_model)\n    target_embed = EmbeddingLayer(target_vocab_size, d_model)\n    positional_encoding = PositionalEncoding(max_seq_len, d_model, dropout_rate)\n    multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n    masked_multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n    feed_forward = FeedForward(d_model, d_ff, dropout_rate)    \n    projection_layer = ProjectionLayer(target_vocab_size, d_model)\n    encoder_block = EncoderBlock(multihead_attention, feed_forward, dropout_rate)\n    decoder_block = DecoderBlock(masked_multihead_attention, multihead_attention, feed_forward, dropout_rate)\n\n    encoderblocklist = []\n    decoderblocklist = []\n\n    for _ in range(num_blocks):\n        encoderblocklist.append(encoder_block)   \n         \n    for _ in range(num_blocks):\n        decoderblocklist.append(decoder_block)\n    \n    encoderblocklist = nn.ModuleList(encoderblocklist)            \n    decoderblocklist = nn.ModuleList(decoderblocklist)\n        \n    encoder = Encoder(encoderblocklist)\n    decoder = Decoder(decoderblocklist)\n    \n    # 모든 매개변수 값을 제공하여 Transformer 클래스를 인스턴스화합니다.\n    model = Transformer(source_embed, target_embed, positional_encoding, multihead_attention, masked_multihead_attention, feed_forward, encoder, decoder, projection_layer, dropout_rate)\n\n    for param in model.parameters():\n        if param.dim() > 1:\n            nn.init.xavier_uniform_(param)\n    \n    return model\n\n# 마침내, build_model을 호출하고 model 변수에 할당합니다.\n# 이 모델은 이제 데이터셋을 훈련하고 검증하는 데 완전히 준비된 상태입니다.\n# 훈련 및 검증 후 이 모델을 사용하여 새로운 번역 작업을 수행할 수 있습니다.\n\nmodel = build_model(source_vocab_size, target_vocab_size)\n```\n\n## 단계 10: 생성한 LLM 모델의 훈련 및 검증\n\n지금은 모델을 훈련할 시간입니다. 훈련 프로세스는 매우 간단합니다. 우리는 단계 3에서 생성한 훈련 DataLoader를 사용할 것입니다. 총 훈련 데이터셋 수가 100만이므로 GPU 장치에서 모델을 훈련하는 것을 강력히 권장합니다. 20 epoch를 완료하는 데 약 5시간이 소요되었습니다. 각 epoch 이후에는 모델 가중치와 옵티마이저 상태를 저장하여 중지된 지점부터 훈련을 다시 시작하는 것이 더 쉽기 때문에 이전 지점에서 훈련을 재개하는 것보다 더 나을 것입니다.\n\n매 에포크 이후에는 검증을 시작합니다. 검증 데이터셋 크기는 2000으로 매우 합리적입니다. 검증 프로세스 중에는 디코더 출력이 [SEP] 토큰을 받을 때까지 한 번만 인코더 출력을 계산하면 됩니다. 이것은 디코더가 [SEP] 토큰을 받기 전까지 동일한 인코더 출력을 계속 보내야 하기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n디코더 입력은 먼저 문장 시작 토큰 [CLS]으로 시작됩니다. 각 예측 후에는 디코더 입력이 다음 생성된 토큰을 붙여넣을 것이며, 문장 끝 토큰 [SEP]에 도달할 때까지 이를 반복합니다. 마지막으로, 프로젝션 레이어는 출력을 해당 텍스트 표현으로 매핑합니다.\n\n```js\ndef training_model(preload_epoch=None):   \n\n    # 전체 훈련 및 검증 주기는 20번 실행됩니다.\n    EPOCHS = 20\n    initial_epoch = 0\n    global_step = 0    \n    \n    # Adam은 현재 상태를 유지하고 계산된 기울기에 기반하여 매개변수를 업데이트하는 가장 일반적으로 사용되는 최적화 알고리즘 중 하나입니다.         \n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    \n    # preload_epoch가 None이 아닌 경우, 이는 최근 저장된 가중치, 최적화기로 훈련이 시작될 것을 의미합니다. 새로운 에포크 번호는 preload epoch + 1이 됩니다.\n    if preload_epoch is not None:\n        model_filename = f\"./malaygpt/model_{preload_epoch}.pt\"\n        state = torch.load(model_filename)\n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n\n    # CrossEntropyLoss 손실 함수는 프로젝션 출력과 대상 라벨 사이의 차이를 계산합니다.\n    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_en.token_to_id(\"[PAD]\"), label_smoothing=0.1).to(device)\n\n    for epoch in range(initial_epoch, EPOCHS):\n\n        # ::: 훈련 블록 시작 :::\n        model.train()  \n        \n        # 훈련 데이터로더로 훈련을 진행합니다.     \n        for batch in tqdm(train_dataloader):\n            encoder_input = batch['encoder_input'].to(device)   # (batch_size, seq_len)\n            decoder_input = batch['decoder_input'].to(device)    # (batch_size, seq_len)\n            target_label = batch['target_label'].to(device)      # (batch_size, seq_len)\n            encoder_mask = batch['encoder_mask'].to(device)       \n            decoder_mask = batch['decoder_mask'].to(device)         \n\n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(decoder_input, decoder_mask, encoder_output, encoder_mask)\n            projection_output = model.project(decoder_output)\n\n            # projection_output(batch_size, seq_len, vocab_size)\n            loss = loss_fn(projection_output.view(-1, projection_output.shape[-1]), target_label.view(-1))\n            \n            # 역전파\n            optimizer.zero_grad()\n            loss.backward()\n\n            # 가중치 업데이트\n            optimizer.step()        \n            global_step += 1\n\n        print(f'Epoch [{epoch+1}/{EPOCHS}]: Train Loss: {loss.item():.2f}')\n        \n        # 각 에포크가 끝난 후 모델 상태를 저장합니다.\n        model_filename = f\"./malaygpt/model_{epoch}.pt\"\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)        \n        # ::: 훈련 블록 끝 :::\n\n        # ::: 검증 블록 시작 :::\n        model.eval()        \n        with torch.inference_mode():\n            for batch in tqdm(val_dataloader):                \n                encoder_input = batch['encoder_input'].to(device)   # (batch_size, seq_len)                        \n                encoder_mask = batch['encoder_mask'].to(device)\n                source_text = batch['source_text']\n                target_text = batch['target_text']\n                \n                # 소스 시퀀스에 대한 인코더 출력 계산\n                encoder_output = model.encode(encoder_input, encoder_mask)\n\n                # 예측 작업을 위해 디코더 입력으로 들어가는 첫 번째 토큰은 [CLS] 토큰입니다.\n                decoder_input = torch.empty(1,1).fill_(tokenizer_my.token_to_id('[CLS]')).type_as(encoder_input).to(device)\n\n                # [SEP] - 끝 토큰이 받아질 때까지 새로운 출력을 입력에 계속 추가해야 하므로, 이를 구현합니다.\n                while True:                     \n                    # 최대 길이를 받았는지 확인하고, 그렇다면 중지합니다.\n                    if decoder_input.size(1) == max_seq_len:\n                        break\n\n                    # 새로운 출력이 추가될 때마다 새로 마스크를 만들어 다음 토큰 예측을 위해 디코더 입력에 추가합니다.\n                    decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n\n                    decoder_output = model.decode(decoder_input,decoder_mask,encoder_output,encoder_mask)\n                    \n                    # 프로젝션을 다음 토큰에만 적용합니다.\n                    projection = model.project(decoder_output[:, -1])\n\n                    # 가장 높은 확률을 갖는 토큰을 선택하여 탐욕 탐색 구현을 합니다.\n                    _, new_token = torch.max(projection, dim=1)\n                    new_token = torch.empty(1,1). type_as(encoder_input).fill_(new_token.item()).to(device)\n\n                    # 새로운 토큰을 다시 디코더 입력에 추가합니다.\n                    decoder_input = torch.cat([decoder_input, new_token], dim=1)\n\n                    # 새로운 토큰이 종료 토큰인 경우, 받아들였다면 중지합니다.\n                    if new_token == tokenizer_my.token_to_id('[SEP]'):\n                        break\n\n                # 전체로 추가된 디코더 입력을 디코더 출력으로 할당합니다.\n                decoder_output = decoder_input.sequeeze(0)\n                model_predicted_text = tokenizer_my.decode(decoder_output.detach().cpu.numpy())\n                \n                print(f'SOURCE TEXT\": {source_text}')\n                print(f'TARGET TEXT\": {target_text}')\n                print(f'PREDICTED TEXT\": {model_predicted_text}')   \n                # ::: 검증 블록 끝 :::             \n\n# 이 함수는 20번의 에포크에 대해 훈련 및 검증을 실행합니다.\ntraining_model(preload_epoch=None)\n```\n\n## 단계 11: 새 번역 작업을 현재 모델로 테스트하는 함수 생성\n\n번역 기능에 일반적인 이름인 malaygpt를 할당합니다. 이는 사용자 입력 영어 원시 텍스트를 입력으로 받아 말레이어 언어로 번역된 텍스트를 출력합니다. 이 함수를 실행해 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndef malaygpt(user_input_text):\n  model.eval()\n  with torch.inference_mode():\n    user_input_text = user_input_text.strip()\n    user_input_text_encoded = torch.tensor(tokenizer_en.encode(user_input_text).ids, dtype=torch.int64).to(device)\n\n    num_source_padding = max_seq_len - len(user_input_text_encoded) - 2\n    encoder_padding = torch.tensor([PAD_ID] * num_source_padding, dtype=torch.int64).to(device)\n    encoder_input = torch.cat([CLS_ID, user_input_text_encoded, SEP_ID, encoder_padding]).to(device)\n    encoder_mask = (encoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int().to(device)\n\n    # Computing the output of the encoder for the source sequence\n    encoder_output = model.encode(encoder_input, encoder_mask)\n    # for prediction task, the first token that goes in decoder input is the [CLS] token\n    decoder_input = torch.empty(1, 1).fill_(tokenizer_my.token_to_id('[CLS]')).type_as(encoder_input).to(device)\n\n    # since we need to keep adding the output back to the input until the [SEP] - end token is received.\n    while True:\n        # check if the max length is received\n        if decoder_input.size(1) == max_seq_len:\n            break\n        # recreate mask each time the new output is added to the decoder input for the next token prediction\n        decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n        decoder_output = model.decode(decoder_input, decoder_mask, encoder_output, encoder_mask)\n\n        # apply projection only to the next token\n        projection = model.project(decoder_output[:, -1])\n\n        # select the token with the highest probability which is a greedy search implementation\n        _, new_token = torch.max(projection, dim=1)\n        new_token = torch.empty(1, 1).type_as(encoder_input).fill_(new_token.item()).to(device)\n\n        # add the new token back to the decoder input\n        decoder_input = torch.cat([decoder_input, new_token], dim=1)\n\n        # check if the new token is the end of the token\n        if new_token == tokenizer_my.token_to_id('[SEP]'):\n            break\n    # the final decoder out is the concatenated decoder input until the end token\n    decoder_output = decoder_input.squeeze(0)\n    model_predicted_text = tokenizer_my.decode(decoder_output.detach().cpu.numpy())\n\n    return model_predicted_text\n```\n\nTesting Time! Let’s do some translation testing.\n\n![image](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_8.png)\n\n“The translation seems to be working pretty well.”\n\n<div class=\"content-ad\"></div>\n\n여기까지입니다! 이제는 PyTorch를 사용하여 처음부터 Large Language Model을 만들 수 있을 것이라고 매우 자신합니다. 물론 이 모델을 다른 언어 데이터셋에서도 훈련시키고 해당 언어로 번역 작업을 수행할 수 있습니다. 이제 처음부터 transformer를 만드는 방법을 배웠으니, 이제 시장에서 사용 가능한 대부분의 LLM에 대한 학습 및 응용을 스스로 할 수 있다는 것을 확신할 수 있습니다.\n\n그다음은 무엇일까요? 현재 시장에서 인기있는 오픈소스 LLM 모델 중 하나인 Llama 3 모델을 파인 튜닝하여 완전히 기능적인 애플리케이션을 만들 것입니다. 전체 소스 코드도 함께 공유할 예정입니다.\n\n그러니 기대해 주시고, 읽어 주셔서 정말 감사합니다!\n\n저와 연결해 보세요: https://www.linkedin.com/in/tamangmilan\n\n<div class=\"content-ad\"></div>\n\n**Google Colab 노트북 링크**\n\n**참고 자료**\n\n- Attention Is All You Need — 논문, Ashish Vaswani, Noam Shazeer, 그리고 팀\n- Attention in transformers, 시각적으로 설명된 내용, 3Blue1Brown — 유튜브\n- GPT 구축하기, Andrej Karpathy, 유튜브\n- https://github.com/hkproj/pytorch-transformer — Umar Jamil","ogImage":{"url":"/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_0.png"},"coverImage":"/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_0.png","tag":["Tech"],"readingTime":40},{"title":"혁신 수용하기 로보틱스로 미래를 발견하다","description":"","date":"2024-06-19 18:41","slug":"2024-06-19-TitleEmbracingInnovationUnveilingtheFuturewithRobotics","content":"\n\n로봇이 인간과 함께 일상 업무를 보조하며 혁신적인 발견을 하며 놀라운 기회를 창출하는 세계를 상상해보세요. 이것은 과학 소설이 아니라, 로봇 기술을 주도하는 기술의 미래입니다.\n\n![로봇 이미지](/assets/img/2024-06-19-TitleEmbracingInnovationUnveilingtheFuturewithRobotics_0.png)\n\n이 블로그 포스트에서는 로봇 기술이 인류에게 제공하는 매혹적인 장점을 살펴보며, 우리 현재를 재정의하고 약속된 내일을 모습을 만들어가는 것을 탐구할 것입니다.\n\nSection 1: 로봇 기술의 부상 — 역사적 관점\n\n<div class=\"content-ad\"></div>\n\n고대 문명의 초기 자동화 장치부터 오늘날의 정교한 기계까지, 로봇학의 진화는 놀라울 정도로 탁월하였습니다. 이러한 역사를 이해함으로써, 우리는 어디까지 왔는지 그리고 앞으로 어떤 발전이 기대되는지에 대한 소중한 통찰력을 얻을 수 있습니다.\n\nSection 2: 산업 전반에서 효율 향상\n\n로봇은 제조업, 의료, 농업, 물류 등 다양한 산업에서 생산성을 향상시키고, 오류를 최소화하며, 안전 조치를 개선하고, 비용을 줄이는 방식으로 혁명을 일으키고 있습니다. 대표적인 사례로는 공장 생산 라인의 로봇 팔이나, 수술을 정밀한 미세 수준으로 지원하는 로봇 등이 있습니다.\n\nSection 3: 로봇학 — 혁신을 위한 등대\n\n<div class=\"content-ad\"></div>\n\n로봇 기술은 최신 연구를 주도하는 원동력입니다. 새로운 소재 개발부터 외부 우주에서 미지의 영역을 탐험하는 등의 분야에 활용되고 있습니다. NASA의 화성 로버와 해저 로봇과 같은 사례 연구들은 이전에 우리가 닿을 수 없었던 발견의 길을 열고 있습니다.\n\n4장: 개인 로봇을 통한 삶의 질 향상\n\n로봇 지원은 전문 분야에만 국한되지 않고 개인적인 생활에도 확장되고 있습니다. 노인이나 장애인을 로봇 이동 보조기와 동반자 로봇을 통해 지원함으로써, 이 기술은 독립성과 감정적 지원을 제공하여 총체적인 삶의 질을 향상시킵니다.\n\n5장: 경제적 이점과 일자리 창출\n\n<div class=\"content-ad\"></div>\n\n일부 사람들은 일자리 치환에 대해 걱정하기도 하지만, 로봇공학이 다양한 산업 분야에서 일자리 창출 기회를 제공하는 것을 명심하는 것이 중요합니다. 제조 로봇부터 특수 조립 작업자가 필요한 서비스 업데이트나 수리 서비스 제공 업체까지 다양한 산업에서 일자리가 창출됩니다. 더 나아가, 로봇이 반복적인 작업을 대신하면서 인간들은 창의적이고 분석적 역할에 집중할 수 있게 됩니다.\n\n섹션 6: 미래 가능성 — 로봇공학의 다음 단계\n\n인공지능(AI)은 인간과 기계 간에 전례없는 협력을 약속하는 로봇공학 세계에 환상적인 요소입니다. 자율 주행 자동차, 연구용 AI 증강 로봇 또는 심지어 외계 탐사를 위한 혁신은 바로 뒤에 있습니다. 이 모든 것은 로봇공학 기술의 발전 덕분에 가능해졌습니다!\n\n우리가 이처럼 혁신적인 로봇공학의 혜택을 받아들일 때, 그들이 사회에 다양한 혜택을 제공함이 명백합니다. 작업을 효율화하고 산업 전반에 혁신을 촉진함으로써 로봇은 인간 경험을 재정의하며, 기술이 우리의 능력을 대체하는 것이 아니라 보완하는 방향으로 우리를 밝은 미래로 이끕니다.\n\n<div class=\"content-ad\"></div>\n\n윤리적 가이드에 기반을 둔 로보틱스 발전과 교육 투자가 중요하다. 이를 통해 인간들이 기계 동료들과 함께 협력적인 미래를 준비할 수 있습니다. 저희가 함께하는 이 흥미진 여정에서 로보틱스의 가능성을 하나씩 발견해나가봅시다!\n\n참고 문헌:\n\n1. “A Brief History of Robotics,” The Conversation, 접속일: [날짜]\n\n2. “Robots at Work in the American Economy” — Boston Federal Reserve Working Paper Series\n\n<div class=\"content-ad\"></div>\n\n3. \"로봇이 어떻게 삶을 변화시키고 있는지,\" BBC 퓨처, 접속일 [날짜].","ogImage":{"url":"/assets/img/2024-06-19-TitleEmbracingInnovationUnveilingtheFuturewithRobotics_0.png"},"coverImage":"/assets/img/2024-06-19-TitleEmbracingInnovationUnveilingtheFuturewithRobotics_0.png","tag":["Tech"],"readingTime":3},{"title":"실패로부터 배운 것 내 가장 큰 AI 프로젝트 실수와 교훈들","description":"","date":"2024-06-19 18:40","slug":"2024-06-19-LearningfromFailuresMyBiggestAIProjectMistakesandTakeaways","content":"\n\n![Learning from Failures](/assets/img/2024-06-19-LearningfromFailuresMyBiggestAIProjectMistakesandTakeaways_0.png)\n\n옛날 어느 날, 저는 챗봇을 만들기로 결심했습니다. 그냥 챗봇이 아니라, 한마디로 아재토론가와 노자처럼 똑똑한 챗봇을 만들기로 한 거예요. 야심 차게 정말 대단한 계획이었어요, 그렇죠? 하지만 야심은 때로 우스꽝스런 실수들이 나오게 할 수 있어요. 제가 만든 캐릭터, 그를 \"똑똑이\"라고 이름 짓기로 했어요, 대화에서 배우고 적응하는 챗봇이었는데요. 그러나 열정에 휩싸인 나머지, 적절한 학습 경계 설정의 중요성을 간과한 모양이었습니다.\n\n결과는? 기상천외한 질문에 심오한 조언을 하는 챗봇이 되었어요. \"오늘 날씨가 어때?\" 라는 간단한 질문에 존재주의적인 답변이 돌아오는 상황을 상상해보세요. 날씨 예보를 요청했는데 \"근본적인 삶 속에서 날씨란 무엇일까요?\" 라는 답이 받아쳐진다고 생각해보세요. 제가 최고인 순간은 아니었습니다. 이곳에서 얻은 교훈은 분명해요: 야망은 실용성을 가져야 하며, AI 프로젝트에는 철저한 지침이 필요해요. 챔피언이자 위인단에 속한 다음 이야기는 지역 기술 워크샵을 위해 디자인한 로봇 로버 이야기죠. 계획은 간단했습니다: 로봇 공학에 대해 가르치기 위해 장애물을 피해 이동할 수 있는 로버와 상호작용할 수 있도록 하는 거였어요. 쉬운 일이었죠, 맞죠? 아니요. 인상을 주려는 나의 노력으로, 디자인을 너무 복잡하게 만들어서 부드럽고 기민한 로버 대신, 사실상 바퀴 달린 리모컨 커피 테이블을 얻게 되었습니다.\n\n<div class=\"content-ad\"></div>\n\n차고에서 처음으로 출항한 로버는 프로젝트 디스플레이를 넘어뜨리고, 차 고양이를 겁나게 만들었으며, 마침내 의자 다리를 \"통해\" 이동하려는 루프에 갇혔습니다. 하지만 어린이들의 웃음소리는 전염성 있었고, 상황은 디자인에서의 단순함의 중요성에 대한 귀중한 가르침의 순간으로 변했습니다. 때로는, 로버가 작은 자동차처럼 작은 회전 반경을 가지고 있을 때, 더 적은 것이 정말 더 많은 것일 수 있습니다.\n\n제 마지막 엔지니어링 도전으로, 특별히 안내드릴 것이 있는데요. 저는 개인 맞춤식 칭찬을 생성할 수 있는 인공지능을 만들어보려고 시도했던 적에 대해 얘기해볼게요. 기술을 통해 긍정을 전파하는 것이었죠. 의도 자체는 찬양할만한 일이었죠. 그러나 제 열정적인 마음에, 애정 스위치를 조금 너무 높게 설정했던 것 같습니다. 부드러운 격려를 기대했던 사용자들은 할머니 할아버지도 부끄러워할 만한 칭찬 폭풍을 받았다니까요.\n\n흥미로운 것부터 약간 당황스러운 피드백까지 다양했지만, 이것은 인공지능 개발에서 중요한 측면을 강조했습니다: 보정이 중요하다는 것. 또한, 약간의 격려만 전달하려는 의도로 시작했지만 낮센 낮은 기분을 메는 칭찬 폭풍을 유발했다는 것을 깨달았습니다.\n\n소프트웨어 엔지니어링에서, 특히 인공지능과 로봇공학에 대해서는, 성공으로 통하는 길은 언제나 실패한 실험들로 가득합니다 — 제 개인적인 챗봇들이나 정체를 앓는 로버와 같이 말이에요. 하지만, 이러한 실패들 속에서 우리는 더 나은 모습으로 형성되며, 이러한 경험들을 통해 더 능숙하고 창의적인 창조자가 되는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n확실히, 유머, 겸손함, 그리고 실수를 받아들일 용의가 나를 진정한 지침으로 이끌어 왔어요.","ogImage":{"url":"/assets/img/2024-06-19-LearningfromFailuresMyBiggestAIProjectMistakesandTakeaways_0.png"},"coverImage":"/assets/img/2024-06-19-LearningfromFailuresMyBiggestAIProjectMistakesandTakeaways_0.png","tag":["Tech"],"readingTime":2}],"page":"67","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}