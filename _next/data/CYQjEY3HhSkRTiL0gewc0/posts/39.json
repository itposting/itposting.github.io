{"pageProps":{"posts":[{"title":"모든 문제를 해결하는 하나의 계층  시맨틱 레이어란 무엇인가","description":"","date":"2024-06-22 17:05","slug":"2024-06-22-SemanticLayerOneLayertoServeThemAll","content":"\n\n## 기술과 비즈니스 간의 간극을 메우다\n\n![이미지](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_0.png)\n\n- 시멘틱 레이어는 복잡한 데이터 구조와 비즈니스 용어 사이의 다리 역할을 하며, 데이터의 통합된 보기를 제공하여 접근을 간소화하고 조직적 의사결정의 일관성을 보장합니다.\n- 시멘틱 레이어를 활용하면 데이터 거버넌스를 개선하고 AI 통합을 용이하게 하여 신뢰할 수 있고 투명한 분석 및 정보에 기초한 결정이 가능해지며, 조직 프로세스를 효율성과 적응성을 높일 수 있습니다.\n\n# 목차\n\n<div class=\"content-ad\"></div>\n\n# 소개\n\n시맨틱 레이어는 현대 데이터 관리에서 점점 더 중요한 요소가 되고 있습니다. 이러한 레이어의 부재는 제한된 데이터 가용성, 보고서 상의 불일치 및 잘못된 의사 결정으로 이어지며 IT 자원에 부담을 가중시킵니다. 데이터 민주화에 대한 노력은 다양한 비즈니스 인텔리전스 도구와 데이터 소스의 증가로 어려움이 더해져 종종 일관되지 않은 분석 결과와 거버네스 문제를 야기합니다.\n\n시맨틱 레이어를 도입함으로써 이러한 문제를 해결할 수 있습니다. 비즈니스 데이터의 일관된 표현으로 작용하면서 복잡한 데이터 구조를 익숙한 비즈니스 용어로 변환함으로써 조직 전체에서 데이터의 통합된 관점이 만들어지며, 접근이 간소화되고 일관성이 보장됩니다. 전문가들은 데이터 엔지니어링과 비즈니스 분석 사이의 간극을 좁히는 중요성을 강조하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 기사는 시맨틱 레이어의 정의와 기능을 살펴보고 기업 데이터를 조직화하고 추상화하여 의사 결정 프로세스를 용이하게 하는 중추적 역할을 설명합니다. 시맨틱 레이어를 구현하는 다양한 이점을 탐구하며 데이터 일관성, 거버넌스 및 민첩성이 향상되는 등의 이점을 논의합니다. 현대 기업 데이터와 AI 관리의 복잡성을 다루면서, 시맨틱 레이어는 운영 효율성 향상과 더 많은 정보에 기반한 의사 결정 능력을 약속하는 중심 요소로 등장합니다.\n\n# 시맨틱 레이어가 필요한 이유는?\n\nMicah Horner (TimeXtender)는 간결히 설명합니다. 그는 시맨틱 레이어가 없으면 조직이 데이터를 효과적으로 활용하는 데 제한이 발생한다고 주장합니다:\n\n- 제한된 데이터 접근 및 사용: 데이터 격리와 복잡성은 비기술적 사용자가 데이터에 액세스하기 어렵게 만들며, 시맨틱 레이어 없이는 정보 기반 결정과 데이터 기반 이니셔티브가 저해됩니다.\n- 통일된 데이터 언어 부재: 부서 간 다른 용어들은 혼란과 오해를 야기하며, 시맨틱 레벨이 없으면 비즈니스 목표를 조정하기 어렵게 합니다.\n- 보고 및 분석의 일관성 부재: 일관되지 않은 데이터 정의와 계산으로 인해 모순된 결과와 신뢰성 없는 결정이 발생하며, 비용이 발생합니다.\n- IT 부담 증가: 시맨틱 레이어가 없으면 IT 팀은 데이터 액세스 요청 및 문제에 너무 많은 시간을 할애하며, 전략적 이니셔티브로부터 자원을 분산시킵니다.\n- 제한된 민첩성과 확장성: 수동 데이터 통합 프로세스로 인해 비즈니스 변화에 대응하거나 영업을 확장하는 능력이 제한되어 시장 변화에 대한 대응 능력이 제한됩니다.\n- 데이터 거버넌스 및 규정 준수 위험: 일관되지 않은 데이터 관리는 기업 거버넌스와 규정 준수에 위험을 초래하며, 법적 및 재정적 결과가 발생할 수 있습니다.\n- 소실된 통찰과 경쟁 우위: 접근할 수 없는 데이터와 공유된 이해 부재로 인해 시장에서의 놓친 통찰과 경쟁 우위가 감소합니다.\n\n<div class=\"content-ad\"></div>\n\nKieran O’Driscoll(AtScale), Kyle Hale and Soham Bhatt (Databricks)가 블로그 글에서 대부분의 기업이 여전히 데이터 민주화에 어려움을 겪고 있다고 설명합니다.\n\n의사결정자에게 데이터를 제공하는 것은 특히 대규모 조직에서 어려운 과제입니다. 기업의 절반 이상이 세 개 이상의 비즈니스 인텔리전스 도구를 사용하며, 데이터 과학자와 애플리케이션 개발자는 각자 선호하는 도구를 가지고 있습니다.\n\n다양한 도구와 쿼리 언어는 상반되는 분석 결과를 내놓습니다. 서로 다른 데이터 복사본이나 Tableau Hyper Extracts, Power BI Premium Imports, 또는 SSAS와 같은 OLAP 솔루션을 사용하는 여러 사업 부서들은 이 문제를 더욱 악화시킵니다.\n\n다양한 마트, 데이터 웨어하우스 및 보고 도구에 데이터를 저장하면 단일 진실의 버전을 유지하는 것이 어렵습니다. 이로 인해 데이터 이동, ETL, 보안 및 복잡성이 증가하며 데이터 거버넌스 문제가 발생하고 잠재적으로 오래된 데이터에 의존하게 됩니다.\n\n<div class=\"content-ad\"></div>\n\nAtScale은 기업이 의사소통의 간극을 좁히기 위해 시멘틱 레이어가 필요한 이유를 매우 명확하게 설명하는 예를 사용합니다.\n\nDonald Farmer(AtScale)은 데이터 엔지니어와 비즈니스 분석가 간의 심각한 도전 과제를 강조하고 시멘틱 레이어가 이 간극을 메우는 데 도움이 되는 이유에 대해 설명합니다.\n\n많은 조직은 데이터 엔지니어(코드 기반 환경을 선호하는)와 비즈니스 분석가(비코드 인터페이스를 선호하는) 간의 연결 부재로 데이터 기반 의사결정에 어려움을 겪고 있습니다. 이러한 불일치는 비효율성, 일관되지 않은 데이터 정의 및 부정확한 의사결정을 초래합니다. 견고한 시멘틱 레이어는 엔지니어를 위한 API와 분석가를 위한 직관적 인터페이스를 제공함으로써 이러한 간극을 메울 수 있습니다. 이는 조직의 기술과 기술을 최적화합니다.\n\n시멘틱 레이어는 다양한 작업 스타일을 수용하는 통합 플랫폼을 제공하며 협업, 데이터 거버넌스 및 혁신을 강화합니다. 엔지니어가 데이터에 프로그래밍적으로 액세스하고 조작하는 데 사용할 수 있는 견고한 API를 제공하여 워크플로에 원활하게 통합됩니다. 동시에, 기술 전문지식이 많이 필요하지 않는 분석가를 위한 직관적 인터페이스를 제공합니다. 이 통합된 기반은 일관된 정의와 지표를 보장하며 더 나은 의사소통과 조정을 촉진합니다. 공유 작업 영역, 버전 관리 및 주석 기능과 같은 기능은 협업과 지식 공유를 촉진하며 조직이 효과적으로 데이터를 활용하고 비즈니스 가치를 실현할 수 있도록 돕습니다.\n\n<div class=\"content-ad\"></div>\n\n# 시맨틱 레이어란 무엇인가요\n\n위키피디아에서는 시맨틱 레이어를 다음과 같이 정의합니다:\n\nAtScale에서는 시맨틱 레이어를 다음과 같이 정의합니다:\n\n그리고 추가 설명...\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_1.png)\n\nLulit Tesfaye (Enterprise Knowledge) defines the Semantic Layer as follows:\n\nand further …\n\nSummarizing these definitions, the semantic layer can be defined as follows:\n\n\n<div class=\"content-ad\"></div>\n\n시맨틱 레이어는 복잡한 기업 데이터를 친숙한 비즈니스 용어로 번역하고 서로 다른 데이터 원본을 매핑하고 데이터 관계를 관리하여 통합된 보기를 제공합니다. 사용자를 위해 데이터 모델을 간소화하며 구조화된, 비구조화된 및 반구조화된 데이터를 포함합니다. 메타데이터 레이어 역할을 하여 콘텐츠의 관리와 분석을 향상시킵니다. 데이터베이스와 최종 사용자 사이에 추상화 레이어 역할을 하며 일관된 데이터 뷰를 제공하고 SQL 지식 없이 직관적인 쿼리를 지원합니다. 또한 접근 제어, 데이터 품질 보증 및 정책 강제를 통해 데이터 관리를 지원합니다.\n\n# 시맨틱 레이어가 비즈니스 요구를 어떻게 지원하는가\n\n시맨틱 레이어는 데이터의 통합된 보기를 제공하여 일관된 액세스와 쿼리를 가능하게 합니다. 사용자 경험을 향상시키고 조직의 효율성을 높이며 기업 전반에 걸친 분석에 표준화된 접근 방법을 제공하여 다수의 혜택을 얻을 수 있습니다:\n\n- 진실의 단일 원천: 데이터는 원본에 관계 없이 표준 형식으로 제공되어 사용자가 다양한 도구와 기술로 분석할 수 있습니다. 시맨틱 레이어를 사용하는 기업은 단일 데이터원을 제한되지 않고 부서 간 분석을 수행할 수 있습니다.\n- 간소화된 데이터 액세스: 시맨틱 레이어는 복잡한 데이터 구조의 간소화된 통합 보기를 제공하며 사용자가 심도 있는 기술 지식 없이 데이터에 액세스하고 이해하는 것을 용이하게 합니다. 이로써 기업에서 데이터 액세스를 민주화할 수 있습니다.\n- 분석과 AI의 민주화: 데이터 분석이 확대됨에 따라 모든 요구 사항에 대해 단일 BI 또는 ML 플랫폼에 의존하는 것은 비현실적입니다. 시맨틱 레이어 플랫폼은 다양한 데이터 플랫폼, 프로토콜 및 도구를 연결하여 데이터와 사용을 분리하고 분석 및 ML의 민주화를 가능하게 합니다. [계속]\n\n<div class=\"content-ad\"></div>\n\n# 시맨틱 레이어 구현\n\n기업은 다양한 소스에서 데이터를 추상화하고 그 문맥을 이해하며 실행 가능한 통찰을 추출하는 도구가 필요합니다. 이를 통해 모든 사용자에 대한 데이터 문해력을 제고할 수 있습니다. 현대적인 \"범용\" 시맨틱 레이어 플랫폼은 시맨틱 레이어의 원래 강점을 강화하여 거버넌스 중앙화를 실현하고 비즈니스 지향적 데이터 관점을 촉진합니다.\n\n## 시맨틱 레이어의 구성 요소\n\nMicah Horner (TimeXtender)는 시맨틱 레이어의 구성 요소를 설명하며 복잡한 데이터 구조를 직관적인 사용자 경험으로 손쉽게 전환하는 방법을 소개합니다:\n\n<div class=\"content-ad\"></div>\n\n1. 데이터 수집\n\n- 여러 출처에서 데이터 수집: 데이터베이스, 스프레드시트, API 등에서 데이터를 수집하여 중요한 정보를 중앙 집중화하여 후속 처리 및 언어 계층 생성을 시작합니다.\n\n2. 데이터 준비\n\n- 데이터 변환 및 준비: 수집된 데이터를 정리, 유효성 검사하고 변환하여 정확성과 분석에 사용하기 쉬운 신뢰할 수 있는 데이터 세트를 만듭니다.\n- 데이터의 차원 모델링: 데이터를 차원과 사실로 구조화하여 복잡한 관계를 단순화하며 의미 있는 통찰력을 제공하는 언어 계층 구축에 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n3. 데이터 전달\n\n- 의미론적 레이어: 기술적 데이터를 비즈니스 친화적 용어로 번역하여 데이터를 이해하기 쉽고 모든 사용자에게 관련성 있게 만드는 의미 모델을 생성합니다.\n- 데이터 제품: 부서별 모델(데이터 제품)을 개발하여 맞춤형 데이터 액세스를 제공하여 각 팀이 필요로 하는 데이터를 과다하게 받지 않고 얻을 수 있도록 합니다.\n\nDavid P. Mariani(AtScale)은 A16Z 데이터 스택의 메트릭 레이어에서 변환 서비스를 활용한 범용 의미론적 레이어의 구현을 주장합니다. 데이터 모델링, 워크플로우 관리 및 권한 및 보안 등의 기능을 수행합니다. 이 레이어는 적절하게 조정되었을 때 다음과 같은 중요한 이점을 제공합니다:\n\n- 어떤 분석 도구에서도 접근할 수 있는 기업 메트릭과 계층적 차원을 위한 단일 진실의 소스 생성\n- 수월하게 업데이트하거나 새로운 메트릭을 정의하고 데이터의 도메인별 뷰를 설계하며 새로운 원시 데이터 자산을 통합하는 민첩성 제공\n- 분석 성능을 최적화하고 클라우드 자원 소비를 모니터링 및 최적화\n- 액세스 제어, 정의, 성능 및 자원 소비와 관련된 거버넌스 정책 강화\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_2.png)\n\n성공은 사용자가 의미적 확장 없이 자유롭게 혁신할 수 있는 중앙에서 관리되는 의미론적 레이어를 활용하는 데 달려 있어요.\n\n메트릭 레이어: 메트릭 레이어는 기업 메트릭의 진실된 단일 원천으로서, 다양한 분석 도구에서 접근할 수 있습니다. 이는 BI 도구, 응용 프로그램, 역방향 ETL 및 데이터 과학 도구를 위한 메트릭 저장소를 제공하며, 설계 및 변경 관리가 그 기능의 일부입니다. 효과적인 메트릭 레이어는 일관성, 효율성 및 사용자 경험이 원활하도록 하기 위한 쿠레이션, 변경 관리, 탐색 및 서비스 기능이 필요합니다.\n\n데이터 모델링: 데이터 모델링은 데이터 레이크하우스나 창고의 논리적 데이터 개념을 물리적 구조에 매핑하는 과정을 말합니다. 비주얼 프레임워크나 코드 기반 언어를 사용할 수 있습니다. 중요한 활동에는 데이터를 \"분석용으로 준비함\", 일관된 차원 정의, 메트릭 설계가 포함됩니다. 이는 비즈니스 의미론을 데이터 모델에 포함시키며, 복합적인 분석 접근을 통해 일관성, 거버넌스 및 혁신을 촉진합니다.\n\n<div class=\"content-ad\"></div>\n\n워크플로우 관리: 워크플로우 관리는 의미론적 레이어를 위한 물리적 변환을 조율하여 비용과 성능을 최적화합니다. 사용자는 데이터 집계물 생성이 필요한 최소한의 쿼리 대기 시간을 요구하는데, 이는 클라우드 규모의 데이터로 인한 것입니다. 성능 관리는 자동으로 생성물을 처리하며, 클라우드 및 노동 비용을 고려하면서 동적으로 적응합니다. 의미론적 레이어 데이터를 활용하여 워크플로우 관리는 성능과 비용을 최적화합니다.\n\n자격 및 보안: 의미론적 레이어의 자격 및 보안은 쿼리 시에 데이터 거버넌스 정책을 동적으로 시행하여 사용자가 올바른 데이터에 액세스하는 것을 보장합니다. 여러 자격과 일관된 정의를 관리함으로써 신뢰와 무결성을 유지합니다. 성능 최적화는 사용자의 자격 및 유즈 케이스 우선순위를 고려합니다. 실시간 정책 시행이 중요한데, 의미론적 레이어를 넘어 더 넓은 보안 서비스를 제공합니다.\n\n## 모던 데이터 스택 내 의미론적 레이어 통합\n\nDavid P. Mariani은 또한 모던 데이터 스택의 레이어가 주변 레이어와 원활하게 통합되어야 한다고 언급합니다.\n\n<div class=\"content-ad\"></div>\n\n의미 계층은 데이터 플랫폼, 분석 및 출력 계층, 그리고 메타데이터 및 서비스 계층과 깊게 통합되어야 합니다.\n\n클라우드 데이터 플랫폼에서의 범용 의미 계층은 데이터를 데이터 웨어하우스나 레이크하우스에 중앙 집중화합니다. 하이브리드/멀티 클라우드 설정에서는 플랫폼 간 쿼리를 위해 데이터 가상화가 필요합니다. 효율적인 워크플로 관리는 다양한 데이터 플랫폼 아키텍처와의 밀접한 통합을 필요로 합니다:\n\n쿼리 엔진 오케스트레이션: 고객으로부터 쿼리를 플랫폼별 SQL로 동적으로 변환하며, 플랫폼 고유 기능에 최적화되고 의미 모델에서 논리적-물리적 매핑을 반영합니다.\n\n변환 오케스트레이션: 뷰를 물리적 테이블로 재질화하며, 데이터 플랫폼 내에서 성능 및 비용을 최적화합니다.\n\n<div class=\"content-ad\"></div>\n\n### 워크플로 오케스트레이션: \n데이터 플랫폼 내에서 새로운 데이터 또는 메타데이터를 생성하며, 사용자 또는 AI/ML 상호 작용을 기반으로 합니다.\n\n### 사용자 정의 함수 (UDF): \n클라우드 데이터 플랫폼의 함수 라이브러리를 활용하여 분석 및 출력을 수행하여 시맨틱 레이어 기능을 향상시킵니다.\n\n### 메타데이터 및 지원 서비스: \n시맨틱 레이어는 메타데이터 및 지원 서비스와 통합하여 데이터 패브릭 생태계의 다양한 도구들과 협력합니다.\n\n- 시맨틱 레이어는 기업용 데이터 카탈로그 도구에 메트릭 및 데이터 모델 검색을 위한 메타데이터 및 계보를 공유해야 합니다.\n- 시맨틱 레이어는 다른 도구로부터 메타데이터를 가져와 시맨틱 데이터 모델을 자동화하고 표준화하기 위한 능력이 있어야 합니다.\n- 시맨틱 레이어에는 사용자 액세스, 가동 시간 및 시스템 성능을 관리하기 위한 모니터링 엔드포인트가 있어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n## AtScale과 Databricks를 활용한 의미론적 레이크하우스 구축\n\nAtScale과 Databricks의 협업은 물리적 테이블에 추상화 레이어를 제공하여 의미론적 레이크하우스를 만들고 있습니다. 이를 통해 엔티티, 속성 및 조인을 정의하여 데이터 소비를 단순화하여 분석가와 최종 사용자에게 비즈니스 친화적인 뷰를 제공합니다.\n\nAtScale의 의미론적 레이어는 분석 도구와 Databricks 레이크하우스 사이에 위치하여 데이터를 추상화하여 쉽게 소비할 수 있도록 합니다. Hive SQL, SSAS 큐브 또는 웹 서비스를 통해 연결하여 최적화된 SQL 실행을 위해 쿼리를 Databricks로 전달하여 성능과 확장성을 보장합니다.\n\nAtScale의 Universal Semantic Layer는 쿼리 패턴을 식별하고 자동으로 집계를 관리하는 자율 성능 최적화를 사용합니다. 이로써 수동 노력을 없애고 Delta Lake에서 \"Diamond Layer\" 집계를 생성하여 BI 보고 성능을 향상시키고 분석 데이터 파이프라인과 엔지니어링을 단순화합니다.\n\n<div class=\"content-ad\"></div>\n\n툴에 독립적인 의미론적 레이크하우스를 만들기\n\nDatabricks 레이크하우스 플랫폼은 데이터, 분석 및 AI 워크로드를 통합합니다. AtScale의 의미론적 레이크하우스는 BI 및 AI/ML을 지원하여 Tableau, Power BI, Excel 및 Looker를 통해 일관된 사용을 가능하게 하는 툴에 독립적인 의미론적 레이어를 확장합니다.\n\n![image](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_3.png)\n\nAtScale의 유니버설 의미론적 레이어는 BI 및 AI/ML 팀을 통합하여 기업 데이터에 일관된 액세스를 제공합니다. 엑셀을 사용하는 비즈니스 사용자와 노트북을 사용하는 데이터 과학자들이 Databricks 레이크하우스의 모든 기능을 활용할 수 있도록 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n# 데이터 분석의 의미론적 층\n\n조직이 민첩성을 유지하고 신속하게 정보된 결정을 내리길 원하는 경우, 데이터 분석의 유연성은 매우 중요합니다. 의미론적 층은 데이터 층을 시각화 층에서 추상화함으로써 이러한 유연성을 가능케하며 핵심적인 역할을 합니다. 이 추상화를 통해 분석가는 차트, 표 및 그래프를 포함한 데이터의 다른 관점을 쉽게 만들고 분석 필드 및 측정 항목을 손쉽게 변경할 수 있습니다. 결과적으로, 분석가는 빠르게 새로운 통찰을 얻을 수 있게 되며 기업이 변화하는 시장 조건에 빠르게 대응할 수 있도록 전략을 조정하는 데 도움을 줄 수 있습니다.\n\n리처드 마카라(Reconfigured)는 의미론적 층을 갖는 기업들이 사용 가능한 데이터가 업데이트되는 즉시에 액세스할 수 있기 때문에 의사결정에서 더 유연해지고 있다고 언급합니다. 이 실시간 데이터 액세스는 비즈니스 환경의 변화에 적극적으로 대응하고 새로운 기회와 잠재적 위험을 식별하는 데 도움을 줄 수 있습니다. 이러한 유연성을 활용할 수 있는 능력은 투자하는 회사들이 경쟁 우위를 확보하고 현대의 빠르게 변화하는 비즈니스 환경에서 정보에 기반한 결정을 내리는 데 중요합니다. 이는 사용자들이 다음과 같은 일들을 수행할 수 있게 합니다:\n\n- 데이터 표현 방법을 빠르고 쉽게 변경\n- 비즈니스 요구 사항의 변화에 대응\n- 다양한 소스에서 데이터에 쉽게 액세스하고 분석\n- 기존 보고서에서 쉽게 보이지 않는 추세, 이상점 및 다른 통찰력 식별 \n- 협업 데이터 분석 프로젝트에 더 쉽게 참여\n\n<div class=\"content-ad\"></div>\n\n데이터 분석의 유연성이 기업이 비즈니스 환경 변화에 신속히 대응하고 데이터 분석 및 보고서 작성에서 혁신을 유발하는 데 도움이 됩니다. 다양한 데이터 원본과 분석 요구 사항을 수용하는 다재다능한 도구를 제공하여, 기업은 경쟁력을 향상시키고 데이터 가치를 극대화할 수 있습니다.\n\n의미 계층은 데이터 분석을 강화시켜 동일한 데이터 원본에 관계 없이 통합 데이터 원천을 제공함으로써 다부서 간 다재다능한 분석을 가능하게 합니다. 비즈니스 요구 사항의 변화에 따라 데이터 모델을 쉽게 조정할 수 있어 사용자를 방해하지 않고 유연하게 대응할 수 있습니다. 추상화 능력은 복잡한 보고서와 시각화물을 간단하게 작성할 수 있도록 도와 분석적 창의력을 촉진합니다. 데이터 원본의 기술적 제약이 분석을 제한하지 않아 유연성이 더욱 향상됩니다. 사용자는 데이터에 효율적으로 액세스하고 분석하여 의사 결정을 강화할 수 있습니다. 외부 소스와 맞춤형 데이터 계층의 통합은 다차원 분석을 용이하게 합니다.\n\n요컨대, 의미 계층은 현대 기업이 데이터 분석을 미래 지향적으로 준비하고 효율성을 향상시키며 정보에 기반한 의사 결정을 가능하게 함으로써 기업을 강화합니다.\n\n## 데이터 분석에서 의미 계층의 혜택\n\n<div class=\"content-ad\"></div>\n\nSean Leslie (data.world)은 의미론적 레이어가 데이터 분석에 도움이 되는 여러 이유 중에 다음을 강조합니다:\n\n데이터 통합 및 추상화를 단순화합니다: 의미론적 레이어는 다양한 소스에서 데이터를 통합하여 통일된 관점을 제공하고, 데이터 통합을 간소화하여 데이터 분석가와 비즈니스 사용자가 효율적으로 데이터를 조합하고 접근할 수 있도록 합니다.\n\n데이터 이해도와 접근성을 향상시킵니다: 의미론적 레이어는 공통의 비즈니스 어휘를 사용하여 기술적 데이터와 비즈니스 사용자를 연결하여 셀프 서비스 분석 및 BI 도구를 통해 쉬운, 비즈니스에 맞는 데이터 탐색 및 분석을 가능하게 합니다.\n\n데이터 지배 및 보안을 용이하게 합니다: 의미론적 레이어는 데이터 일관성을 위해 비즈니스 규칙을 적용하고 데이터 무결성을 유지하며 역할에 따라 액세스 컨트롤을 시행하여 안전하고 규정 준수된 데이터 액세스를 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n# 의미 계층이 데이터 거버넌스에 미치는 영향\n\n리처드 마카라 (Reconfigured)는 데이터 거버넌스가 데이터 보안, 가용성, 사용 가능성 및 무결성을 관리하는 과정을 지칭한다고 설명합니다. 이는 데이터 자산을 관리하기 위한 정책, 절차 및 통제 방법을 수립하는 것을 포함합니다. 데이터 거버넌스의 목적은 비즈니스 가치 증대, 리스크 감소 및 규정 준수를 보장하는 것입니다.\n\n데이터 거버넌스는 정책과 통제를 통해 올바른 데이터 관리를 보장합니다. 그 중요성은 데이터 무결성, 규정 준수, 보안 유지 및 효과적인 의사 결정을 원활하게 하는 데 있습니다:\n\n- 규정 준수: 데이터 거버넌스는 GDPR, HIPAA 등 관련 법률 및 규정을 준수하는 것을 보장합니다.\n- 신뢰할 수 있는 데이터: 데이터 거버넌스는 데이터 표준화 및 메타데이터 관리 정책을 수립하는 데 도움을 줄 수 있으며, 이는 더 정확한 데이터와 더 나은 분석으로 이어질 수 있습니다.\n- 더 나은 의사 결정: 데이터 거버넌스는 믿을 만한 데이터를 생성하는 데 도움을 줄 수 있어 더 나은 비즈니스 결정을 내릴 수 있게 합니다.\n- 리스크 완화: 데이터 거버넌스 정책이 있을 때 조직은 데이터 손실, 데이터 유출 및 관련 문제와 관련된 리스크를 완화할 수 있습니다.\n- 데이터 보안: 데이터 거버넌스에는 권한이 없는 또는 악의적인 접근을 방지하는 데이터 보안 정책이 포함될 수 있습니다.\n- 데이터 정책 강화: 강력한 데이터 거버넌스를 통해 조직은 각 기능 및 사업 단위 전반에서 데이터 정책의 준수를 보장할 수 있습니다.\n- 비용 절감: 효과적인 데이터 거버넌스를 통해 부정확한 데이터 의사 결정, 신뢰할 수 없는 분석 및 데이터 중복으로 인한 비용을 줄일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 거버넌스는 조직 내 데이터 품질과 데이터 관리에 상당한 영향을 미칩니다. 빅데이터와 생성적 AI의 급격한 증가로 인해 그 중요성은 높아지고 있으며, 성장하는 정보 양을 효과적으로 처리하기 위한 적응 가능한 데이터 전략이 필요합니다.\n\n## 어떻게 의미론적 레이어가 데이터 거버넌스를 개선하는가\n\n의미론적 레이어는 다양한 소스로부터의 복잡한 데이터를 비즈니스 용어로 단순화하여 조직 전반에 걸쳐 일관된 이해를 보장합니다. 데이터 정의를 중앙 집중화하여 모델링을 단순화하고 변화를 효율적으로 관리할 수 있도록 합니다. 보고 및 분석을 위한 단일 액세스 포인트로 작용하여 데이터 검색을 향상시킵니다. 이는 데이터 거버넌스를 통해 일관성, 표준화, 효율성을 촉진함으로써 데이터 품질, 정확성, 보안 및 규제 준수를 효과적으로 관리합니다:\n\n데이터의 명확한 정의: 의미론적 레이어는 데이터의 일관된 이해를 보장하여 명확한 정의와 문맥을 통해 일관성 부족으로 인한 혼돈과 오류를 제거합니다.\n\n<div class=\"content-ad\"></div>\n\n향상된 데이터 품질: 시맨틱 레이어는 데이터 일관성과 정확성을 보장하여 에러를 최소화하고 데이터 품질을 향상시킵니다. 표준화된 정의는 결정 신뢰성, 효율성, 고객 경험을 향상시키며 에러를 방지하고 정확한 데이터 관리와 검색을 용이하게 합니다.\n\n데이터 모델링의 유연성: 시맨틱 레이어는 데이터 모델링을 간소화하여 여러 소스에 대한 통일된 모델을 제공하며 일관성을 보장하고 진화하는 비즈니스 요구 사항에 적응하며 관리와 유지 보수 프로세스를 간소화하여 전체 데이터 관리 효율성을 향상시킵니다.\n\n제어된 접근: 시맨틱 레이어는 정확한 데이터 접근 제어를 가능케 하여 인가된 사용자가 미리 정의된 규칙에 따라 데이터를 처리할 수 있도록 하여 관리와 데이터 보안을 보장합니다.\n\n변경 관리: 시맨틱 레이어는 데이터 정의를 중앙 집중화하여 변경 관리를 간소화하고 여러 소스 간에 수동 업데이트를 제거하여 효율성을 향상시키고 일관성을 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n감사 기능: 시맨틱 레이어는 데이터 접근을 규제하여 감사 기능을 향상시키며, 사용자 활동 추적을 가능하게 하고 책임 소지 및 데이터 사용 및 관리의 투명성을 촉진합니다.\n\n효율적인 워크플로우: 시맨틱 레이어는 데이터 분석, 공유 및 협업을 위한 통합 환경을 제공하여 작업 흐름을 최적화하고 중복을 최소화하며 전반적인 효율성을 향상시킵니다.\n\n데이터 일관성: 시맨틱 레이어는 정의된 규칙과 표준에 따라 통제되어 일관된 데이터 접근성과 무결성을 보장하므로 조직 전반에 걸쳐 보다 정확하고 신뢰할 수 있는 통찰력을 얻을 수 있습니다.\n\n향상된 데이터 계보: 시맨틱 레이어는 데이터 계보 추적을 가능하게 하여 소스에서 비즈니스 개념 매핑을 단순화하고 변환을 투명하게 문서화합니다. 이로써 조기 오류 탐지를 통해 데이터 품질, 규정 준수 및 개선된 의사 결정을 보장하여 거버넌스를 강화합니다.\n\n<div class=\"content-ad\"></div>\n\n간소화된 데이터 감사: 의미론적 레이어를 통해 더 나은 데이터 거버넌스를 위해 세밀한 수준에서 데이터를 추적하고 감사할 수 있습니다. 이를 통해 데이터의 오류와 불일치를 식별하기가 더 쉬워집니다.\n\n간소화된 데이터 거버넌스: 의미론적 레이어는 메타데이터와 모델을 중앙 집중화하여 데이터 거버넌스를 간소화하며 정책 이행, 자동화된 품질 점검, 일관된 기준을 유지하는 데 도움을 줍니다. 이는 모든 출처에서의 데이터 정확성과 효율성을 향상시키고 리스크를 완화합니다.\n\n개인정보 보호 및 보안 강화: 점점 더 많은 데이터가 처리되고 수집되는 상황에서 데이터 보안과 개인정보 보호의 필요성도 더욱 커졌습니다. 의미론적 레이어는 데이터에 대한 액세스를 승인된 인원만 가능하게 해 개인정보 보호를 강화합니다.\n\n거버넌스 및 감사: 범용적인 의미론적 레이어를 사용하면 변경 이력을 기록하고 명확한 소유권을 파악할 수 있습니다. 또한 새로운 메트릭을 정의할 수 있는 사용자를 명시하거나 제한하는 것이 더 쉬워집니다.\n\n<div class=\"content-ad\"></div>\n\n향상된 규정 준수: 더 나은 데이터 거버넌스를 통해 의미론적 계층은 모든 데이터가 규정 요구 사항에 따라 처리되도록 보장하여 데이터 침해의 위험을 줄입니다.\n\n의미론적 계층을 구현하면 데이터 거버넌스에 상당한 혜택이 있습니다. 데이터의 통합된 표시를 제공함으로써 관리 프로세스를 간소화하고 데이터 품질과 보안을 향상시켜 효율적인 데이터 관리와 상식적인 의사 결정을 위한 필수적인 도구가 됩니다.\n\n## 의미론적 계층으로 AI 강화하기\n\nAtScale의 David P. Mariani는 선도적인 데이터 기관이 진단형, 예측형, 권고형 분석을 통합하는 AI와 함께 증강 분석에 중점을 두고 있다고 지적하며, 데이터 이동을 줄이며 효율성을 향상시키기 위해 데이터 과학/기계 학습 플랫폼을 통합한다고 합니다.\n\n<div class=\"content-ad\"></div>\n\n이문 찌리(Progress)는 기업 데이터와 생성 AI를 통합하는 것이 신뢰성, 투명성, 보안을 향상시키며 데이터 품질과 확장성을 향상시킬 수 있다고 강조했습니다. 의미론적 레이어를 활용하면 기업은 오류를 줄이고 신뢰성을 보장하며 거버넌스 표준을 준수하면서 혁신적인 비용 절감 기회를 창출하고 의사 결정 및 운영 효율성을 향상시킬 수 있습니다. 의미론적 레이어의 두 가지 주요 이점:\n\n\\[ \\begin{aligned} \n1. 생성 AI의 결과물에서 환각 감소: \\end{aligned} \\]\n생성 AI 모델은 종종 인간의 추론과 이해 부족으로 인해 알려진 환각이라고 불리는 잘못된 답변을 생성합니다. 이러한 오류는 15-20%의 확률로 발생하며 의미론적 데이터 플랫폼을 사용하여 데이터를 맥락화하고 조화시킴으로써 감소할 수 있습니다. 적절한 데이터 정리, 정리 및 모델링은 AI의 정확성을 향상시키는 데 필수적입니다.\n\n\\[ \\begin{aligned} \n2. 생성 AI의 결과물의 신뢰성과 신뢰성 향상: \\end{aligned} \\]\n생성 AI 모델은 종종 비관련, 부정확 또는 편향적인 결과물을 생성하는데, 이는 중요한 비즈니스 결정에 문제를 일으킬 수 있습니다. 기업 의미론적 데이터 시스템과 생성 AI를 통합함으로써 기업은 AI 결과물의 정확성과 신뢰성을 향상시킬 수 있습니다. 사설 의미론적으로 태그된 데이터를 사용해 생성 AI는 조직의 고유한 맥락에 대한 깊은 통찰력을 얻게 됩니다. 이 통합으로 인해 AI 시스템은 실시간 업데이트된 데이터에 접근하여 \"훈련 데이터 차단\" 문제를 해결하고 생성된 답변의 전반적인 품질을 향상할 수 있습니다.\n\n큐브는 대규모 언어 모델(LLM)에 대한 핵심적인 맥락을 제공하는 의미론적 레이어가 AI 기반 데이터 경험에서 중요하다고 강조합니다. 데이터를 의미 있는 비즈니스 정의로 조직하고 쿼리 인터페이스를 제공하여 LLM이 데이터의 맥락을 이해하도록 보장함으로써 오류와 환각을 줄이고 혁신적인 AI 응용프로그램을 가능하게 하며 쿼리 프로세스를 간소화합니다.\n\n<div class=\"content-ad\"></div>\n\nLLMs은 혁신적이지만, \"입력이 쓰레기면 출력도 쓰레기\" 문제 때문에 정확한 결과물을 생성하는 데 한계가 있습니다. LLMs는 환각을 일으킬 수 있습니다. 그들에게 단순히 데이터베이스 스키마를 공급하는 것만으로 올바른 SQL을 생성하는 데 충분하지 않습니다. 데이터 컨텍스트를 포함하여 데이터를 개념적으로 이해하기 위해 의미론적 계층이 필요합니다. 이 계층은 쿼리를 위해 데이터를 비즈니스 정의로 구성하고, LLM이 이를 통해 쿼리할 수 있도록하여 정확성을 보장합니다. 따라서 의미론적 계층은 LLM이 환각하는 문제를 해결함으로써 필요한 컨텍스트를 제공하고, 쿼리와 데이터 출력의 정확성을 보장합니다.\n\n![이미지](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_4.png)\n\nLLMs와 의미론적 계층을 결합하면 새로운 AI 주도 데이터 경험을 제공합니다. 이러한 계층은 AI 에이전트에 필수적인 컨텍스트를 제공하여 정확한 데이터 쿼리를 가능하게 하고, 조직이 사용자 정의 LLM 애플리케이션을 구축할 수 있도록 돕습니다. 데이터 웨어하우스 상단에 위치한 의미론적 계층은 AI 기술과의 원활한 통합을 촉진합니다.\n\n![이미지](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_5.png)\n\n<div class=\"content-ad\"></div>\n\n의미론적 계층 데이터 모델은 LLM이 데이터를 이해하고 올바른 쿼리를 생성하기 위한 맥락으로 사용하는 구조와 정의를 제공합니다. 복잡한 조인 및 계산을 추상화하여, 의미론적 계층은 비즈니스 수준 용어를 기반으로 한 단순화된 인터페이스를 제공하여 오류를 줄이고 환각을 방지합니다.\n\n아르투르 키둔노프(Cube)는 모든 BI 도구 간 데이터 소비를 효율적으로 만들어 실수를 줄이고 일관된 데이터 원천과 신뢰를 유지하는 통합된 의미론적 계층의 중요성을 강조합니다. 모든 데이터 환경에 대한 메트릭 및 메타데이터를 정의하여 BI 소프트웨어부터 AI 도구에 이르기까지 다양한 플랫폼에서 데이터에 접근할 수 있도록 보장합니다. 이 다양성은 데이터 전달의 진화하는 풍경을 수용하고, 차세대 데이터 주도 애플리케이션을 지원합니다.\n\n일관된 데이터 없이는 AI도 불가능합니다: 고품질의 데이터는 AI에게 방대한 데이터셋으로부터 신뢰할 수 있는 통찰력을 제공할 수 있게 해줍니다. 통합된 의미론적 계층은 이 맥락에서 중요한 역할을 하며, 비즈니스 컨텍스트와 정의를 제공하여 AI 도구가 오류를 방지할 수 있습니다. AI는 비즈니스 사용자를 위해 자연어 쿼리를 통해 데이터 큐레이션과 민주화를 용이하게하는 정의 및 코드 개선을 제안하여 의미론적 계층을 향상시킬 수 있습니다.\n\n적용된 AI의 부상: AI의 보급은 대규모 플랫폼뿐만 아니라 도메인별 응용 프로그램으로도 확대되면서, 일관되고 정확한 데이터를 위해 통합된 의미론적 계층이 필요해졌습니다. 의미론적 계층은 입력이 정확하고 관련성이 있으며 일관되도록 보장합니다. 이는 AI 주도 경험에서 정확한 결과와 경쟁 우위를 위해 중요합니다.\n\n<div class=\"content-ad\"></div>\n\nAI에 준비된 범용 의미론적 층의 장점: AI에 준비된 범용 의미론적 층은 다양한 데이터 플랫폼을 연결하고, 데이터 민주화를 촉진하며 고객을 대상으로 하는 애플리케이션을 지원하는 데 필수적입니다.\n\nPatrk Liu Tran(Validio)은 의미론적 층이 AI 및 LLM과 혁신적인 데이터 경험을 창출하려는 기업들로부터 많은 관심을 받고 있다고 강조하고 있습니다. 중요한 목표 중 하나는 자연어 쿼리를 LLM에 가능하게 함으로써 데이터 검색을 간소화하고 분석가들을 하찮은 작업으로부터 해방시키는 것입니다.\n\n의미론적 층과 LLM을 통합하면 정확도가 최대 300%까지 개선됩니다. 메트릭스를 미리 정의하고 잘못된 가정의 위험을 줄임으로써 정확성이 높아집니다. 의미론적 층을 사용하면 LLM이 합의된 비즈니스 메트릭스에 따라 작동하여 정밀성을 향상시킵니다. LLM이 보다 보편화되면 데이터 중심적 기관들에게 의미론적 층의 중요성이 점점 더 분명해지고 있습니다. 이러한 접근법은 더욱 효율적이고 정확한 데이터 처리 및 분석을 약속하며, 결과적으로 더 나은 의사 결정 및 운영 효율성을 가능하게 합니다.\n\n# 미래 지향 및 신흥 트렌드\n\n<div class=\"content-ad\"></div>\n\n\nTomasz Tunguz(Theory Ventures)은 시맨틱 모델이 중요한 추세로 떠오를 것이라며, 조직간의 정의를 통일하여 사람의 이해와 대형 언어 모델의 의미 합성을 위한 단순화된 분석을 개선하고 재사용성과 구성 가능성을 향상시킬 것이라고 언급했습니다.\n\nTimeXTender에 따르면 시맨틱 레이어 기술은 지속적으로 발전하며 데이터 관리 전략에 영향을 미치고 다양한 산업에서 새로운 응용 가능성을 열어줄 것입니다.\n\nAtScale은 시맨틱 레이어가 계속해서 중요성을 갖게 될 것으로 보며, 조직 간의 정의를 통일하여 분석을 단순화하고 재사용성을 촉진하며 인간의 이해와 대규모 언어 모델의 의미 합성을 가능하게 합니다.\n\nGenAI와 같은 책임 있는 AI 도입은 투명성과 편향과 같은 윤리적 측면을 우선시합니다. AI 관리자와 같은 신생 직무들은 윤리적인 실행을 보장합니다. 기업은 점점 데이터를 제품으로 취급하며 맞춤형 비즈니스 결과를 위해 재사용 가능한 데이터 제품을 만들어냅니다. 생성적 AI와 대규모 언어 모델(LLMs)의 통합은 데이터 탐색을 변형시키며, MDX 생성은 쿼리 유연성을 높이고 자연 언어 인터페이스는 데이터 접근을 민주화합니다. 예측과 이상 탐지와 같은 AI 기능들은 의사 결정력을 강화하여 분석 접근성을 넓히고 통찰력을 심화시켜 혁신을 이끌어갈 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n의미적 레이어는 현대 데이터 관리의 다양한 도전에 대한 중심적인 해결책이 됩니다. 데이터의 통합된 관점을 제공하고 액세스를 간소화함으로써, 데이터의 제한된 가용성과 일관성 없는 보고 문제를 해결합니다. 또한 이러한 통합은 개선된 데이터 거버넌스를 촉진하고 AI 통합을 용이하게 하여 조직의 효율성과 유연성을 증가시킵니다. 의미적 레이어의 중요성은 데이터 분석을 혁신하고 AI 애플리케이션을 지원하여 보다 정보화된 의사 결정 과정을 가능케하는 변화력 있는 잠재력에 있습니다. 기업이 점점 데이터 중심적인 세계의 복잡성에 대처함에 따라, 의미적 레이어는 데이터 자원을 관리하고 활용하는 데 더 큰 유연성과 효과성을 제공하는 기반 요소입니다.\n\n# 참고문헌\n\nAtScale 및 Databricks를 활용한 의미적 레이크하우스 구축\n\n<div class=\"content-ad\"></div>\n\n**범용 의미 계층의 힘**\n\n두 세계를 하나로: 데이터 엔지니어와 비즈니스 애널리스트 결합하기\n\n의미 계층이란 무엇인가요?\n\n데이터 전략에서 의미 계층이 차지하는 위치\n\n<div class=\"content-ad\"></div>\n\n시맨틱 레이어가 데이터 거버넌스에 미치는 영향\n\n데이터 시각화에서 시맨틱 레이어 사용의 이점\n\n시맨틱 레이어에 대한 궁극적인 가이드\n\n유니버설 시맨틱 레이어로 현대 비즈니스 인텔리전스를 재구축하기\n\n<div class=\"content-ad\"></div>\n\n현대 데이터 스택의 의미론적 레이어\n\n의미론적 레이어란 무엇이며, 데이터를 지식으로 변환하는 방법은 무엇인가요?\n\n의미론적 레이어를 구현하는 세 가지 방법\n\n의미론적 레이어란 무엇인가요? (구성 요소 및 기업 애플리케이션)\n\n<div class=\"content-ad\"></div>\n\nAtScale은 AI, GenAI 모델을 위한 의미론적 레이어 지원 추가\n\n의미론적 레이어의 의미\n\n의미론적 레이어: AI 기반 데이터 경험의 중추\n\n의미론적 레이어는 AI 기반 분석을 위한 빠져있던 조각입니다\n\n<div class=\"content-ad\"></div>\n\n기업을 위한 생성 AI의 혜택과 의미론적 데이터 통합\n\n만약 범용 의미론적 레이어가 없다면 실시간 AI 경험을 진화시킬 수 없습니다\n\n의미론적 레이어 101: 데이터 팀이 데이터보다는 지표에 집중해야 하는 이유\n\n기업 분석과 생성 AI를 위한 2024 의미론적 레이어 혁신\n\n<div class=\"content-ad\"></div>\n\n현대 데이터 인프라를 위한 신흥 아키텍처","ogImage":{"url":"/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_0.png"},"coverImage":"/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_0.png","tag":["Tech"],"readingTime":20},{"title":"Rockset이 OpenAI에 인수되다 사용자들에게 어떤 의미가 있을까","description":"","date":"2024-06-22 17:03","slug":"2024-06-22-RocksetIsAcquiredbyOpenAIWhatDoesItMeanforItsUsers","content":"\n\n2024년 6월 21일, OpenAI가 데이터 색인 및 쿼리 기능으로 유명한 실시간 분석 데이터베이스인 Rockset을 인수했다고 발표했습니다. 이 인수는 Rockset 사용자들에게 중요한 변화를 알립니다. 사용자들은 이 플랫폼을 이탈해야 하는 제한된 시간을 가지고 있으며 다음 단계에 대해 궁금해하고 있습니다. 본 문서는 Rockset 사용자들이 이 전환을 안내하며, OpenAI가 왜 이러한 결정을 내렸는지, 즉각적으로 필요한 조치는 무엇인지, 그리고 Rockset 사용자들과 실시간 분석 요구 사항을 위한 이상적인 대안으로 어떤 솔루션이 적합한지에 대해 통찰을 제공할 것입니다.\n\n# OpenAI가 Rockset을 인수한 이유는 무엇인가요?\n\nOpenAI는 Rockset의 기술을 통합하여 제품 전반에 걸쳐 검색 인프라를 강화하려고 합니다. 이는 인공지능 업계에서 실시간 데이터 액세스와 처리의 중요성을 명확하게 보여 주는 지표입니다. 게다가 Rockset 인수를 통해 OpenAI는 실시간 분석 전문가 팀을 흡수하여 OpenAI의 능력을 계속 강화할 것입니다.\n\n# Rockset 사용자가 해야 할 첫 번째 일\n\n<div class=\"content-ad\"></div>\n\nRockset 사용자 분들에게 시한부가 닥쳐오고 있습니다. Rockset의 자세한 FAQ에 따르면, 계약 없는 월간 요금제 사용자들은 2024년 9월 30일까지 오프보딩을 진행해야합니다. 계약된 고객들은 Rockset 계정 팀과 협력하여 적절한 오프보딩 계획을 개발할 수 있지만, 모든 고객들은 빠르게 Rockset 대체안을 찾아야 합니다. 인수가 계획된 상황에서, 이제는 Rockset 사용자들이 다음 단계를 취해야 할 때입니다.\n\n![이미지](/assets/img/2024-06-22-RocksetIsAcquiredbyOpenAIWhatDoesItMeanforItsUsers_0.png)\n\nRockset 사용자들은 다음 단계를 시작해야 합니다:\n\n- 현재 사용량과 요구 사항을 평가하십시오: 솔루션을 평가하기 전에 무엇을 찾고 있는지 알아두는 것이 좋습니다. 이는 많은 시간을 절약할 수 있습니다.\n- 비슷하거나 더 나은 기능을 제공하는 대체 플랫폼 목록 작성 시작: 기존에 Rockset을 어떻게 사용했느냐에 따라 비즈니스의 요구 사항이 단순할 수도 복잡할 수도 있습니다. 각 플랫폼은 장단점이 있습니다. 비즈니스에 중요한 성능과 능력을 제공할 수 없는 솔루션을 평가하는데 소중한 시간을 낭비하지 않도록 어떤 플랫폼이 무엇을 할 수 있어야 하는지 알아두는 것이 중요합니다.\n- 작업 중단 없이 마이그레이션 과정을 계획하기 시작: 오픈 소스든 상용 솔루션이든, 솔루션과 함께 제공되는 지원이나 커뮤니티를 평가하는 것이 중요합니다. 성공적인 POC를 진행할 수 있는 옆에서 지원해줄 파트너를 찾거나 24시간 내내 문제 해결을 도와줄 활성 Slack 커뮤니티를 찾는 것은 마이그레이션이 원활히 진행되도록 하는 데 도움이 될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# Rockset 사용자를 위한 대안\n\nRockset 사용자가 다음 단계를 계획할 때, 각 합당한 대안을 탐색하는 것이 중요합니다. 사용 사례와 성능 요구에 따라, 다양한 플랫폼이 원하는 능력을 제공할 수 있습니다. 고려할만한 몇 가지 옵션은 다음과 같습니다:\n\n# 오픈 소스 실시간 분석 SQL 워크로드를 위한:\n\n- Apache Druid: Druid는 대규모 데이터에서 실시간 및 일괄 처리 쿼리를 하위 초 단위로 제공하는 고성능 실시간 분석 데이터베이스입니다.\n- ClickHouse: ClickHouse는 고속 오픈 소스 열 지향 데이터베이스 관리 시스템으로, SQL 쿼리를 사용하여 실시간으로 분석 데이터 보고서를 생성할 수 있습니다.\n- StarRocks: 확장 가능한 JOIN 쿼리를 실행하고 정규화 파이프라인 없이 실시간 분석을 제공하기에 적합합니다. StarRocks는 기본적으로 실시간 데이터 업서트를 지원하며, 열 지향 저장소에서 직접 가변 데이터로 두 번째 수준의 데이터 신선도를 제공할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 프로프리업 관리형 솔루션을 통한 실시간 분석 SQL 워크로드:\n\n- Imply: 엔터프라이즈 지원을 통한 클라우드 상의 관리형 Apache Druid.\n- CelerData: StarRocks 프로젝트의 주도 및 유지보수를 지원하는 클라우드 관리형 StarRocks.\n\n# 오픈 소스 벡터 검색(VectorDB)을 위한:\n\n- Weaviate: Weaviate는 객체 및 벡터를 저장하는 오픈 소스 벡터 데이터베이스로, 클라우드 네이티브 데이터베이스의 내결함성 및 확장성과 함께 벡터 검색을 구조화된 필터링과 결합할 수 있습니다.\n- Milvus: 차세대 AI 애플리케이션을 위한 클라우드 네이티브 벡터 데이터베이스 및 스토리지\n- Qdrant: 다음 세대 AI를 위한 고성능 대규모 벡터 데이터베이스.\n\n<div class=\"content-ad\"></div>\n\n# 관리되는 벡터 검색 (VectorDB):\n\n- SingleStore: SQL 능력 이외에도 SingleStore는 관리되는 벡터 검색 기능을 제공하여 양종의 워크로드에 대한 포괄적인 솔루션을 제공합니다.\n- Zilliz: Milvus 뒤의 회사인 Zilliz는 Milvus의 혜택을 추가 지원 및 유지 보수로 제공하는 관리되는 벡터 검색 서비스를 제공합니다.\n- Pinecone: 배포 및 벡터 검색 애플리케이션의 스케일링을 간소화하고 높은 가용성과 성능을 보장하는 완전히 관리되는 벡터 검색 플랫폼입니다.\n\n전환을 해야 하는 시긴데요. 중요한 인프라가 그대로 유지되고 작동되도록 해야 합니다. 이 플랫폼마다 고유한 장점이 있으며, 당신의 특정 요구 사항을 기반으로 평가하여 성공적인 마이그레이션이 이루어질 것입니다.\n\n# 왜 StarRocks가 Rockset 이주민들에게 가장 좋은 다음 단계인가요?\n\n<div class=\"content-ad\"></div>\n\n많은 Rockset 사용자들이 실시간 분석 요구를 위해 채택했습니다. 따라서 특히 현재 실시간 분야에서 한 명의 선도 업체 중 하나를 언급하는 것이 중요합니다: StarRocks. 실시간 분석을 위한 강력하고 효율적인 대안을 찾는 Rockset 사용자들에게 StarRocks는 매력적인 선택지를 제시합니다. 이유는 다음과 같습니다:\n\n- 확장 가능한 JOIN 쿼리: StarRocks를 사용하면 확장 가능한 JOIN 쿼리를 실행할 수 있으며 데노멀라이제이션 파이프라인이 필요없이 실시간 분석을 제공하여 데이터 처리를 간소화하고 성능을 향상시킵니다.\n- 실시간 데이터 업서트: Rockset에서 StarRocks로 전환할 때 데이터 신선도를 유지할 수 있습니다.\n- 우수한 성능: 컬럼형 스토리지, 벡터화 및 SIMD를 활용하여 StarRocks는 Rockset보다 우수한 성능을 달성하며 저장 공간의 일부만 사용하여 비용 효율적인 솔루션이 됩니다.\n- 오픈 소스 커뮤니티: Apache 라이선스를 받은 리눅스 재단 프로젝트인 StarRocks는 거대하고 성장 중인 글로벌 커뮤니티가 언제든지 도와줄 준비가 되어 있습니다.\n\n# 다음 단계\n\nOpenAI에 의한 Rockset 인수는 사용자들에게 도전과 기회를 제시합니다. 전환은 어렵게 느껴질 수 있지만, 우수한 성능과 확장 가능성을 제공하는 플랫폼으로 업그레이드할 기회이기도 합니다. StarRocks의 실시간 분석 성능에 대해 더 알아보고 이주하는 동안 지원을 받으려면 Slack 커뮤니티에 가입하세요.\n\n<div class=\"content-ad\"></div>\n\n마지막 순간까지 기다리지 말고 오늘 Rockset에서의 원활한 이전을 보장하려면 이전 계획을 시작하세요.","ogImage":{"url":"/assets/img/2024-06-22-RocksetIsAcquiredbyOpenAIWhatDoesItMeanforItsUsers_0.png"},"coverImage":"/assets/img/2024-06-22-RocksetIsAcquiredbyOpenAIWhatDoesItMeanforItsUsers_0.png","tag":["Tech"],"readingTime":4},{"title":"파이썬과 Streamlit으로 멀티페이지 금융 대시보드 만들기 처음부터 끝까지 완성하기","description":"","date":"2024-06-22 17:01","slug":"2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch","content":"\n\n<img src=\"/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_0.png\" />\n\n개인화된 작업 공간을 상상해보세요. 여기서는 상품, ETF, 과소평가된 주식, 그리고 변동성 있는 암호화폐 시장을 각각 별도의 탭에서 동시에 모니터링할 수 있습니다. 이러한 설정은 데이터를 손쉽게 접근할 수 있는 것 이상을 제공합니다. 표준 도구들이 제공하는 유연성 부족으로 자주 허용되지 않는 방식으로 이 데이터를 통합하고 상호작용할 수 있게됩니다. 여러분의 대시보드를 구축함으로써, 관련성 있는 지표를 강조함으로써 유연성을 얻을 수 있고, 사용자 정의 필터를 적용하고, 심지어 시장 변동에 실시간으로 반응하는 고급 분석 도구를 통합할 수 있습니다.\n\n투자 결정에 중요한 고유한 지표들을 간과할 수 있는 범용 인터페이스에 의존할 필요가 없습니다. 여러분의 대시보드를 생성함으로써, 데이터 분석 과정을 통제하고 시장에 대한 더 깊은 이해를 기르는 전략적인 과정이 됩니다. 이제 이 혁신적인 도구를 만드는 것을 시작해봅시다.\n\n데이터 작업에 파이썬보다 더 나은 것이 무엇인가요? 그리고 데이터를 분석하는 데 가장 사용자 친화적인 방법이 무엇인가요? 우리는 파이썬에 대해 더 많은 이해가 있고, Streamlit을 소개할게요. 이 Streamlit은 데이터 분석을 위한 대화형 웹 애플리케이션을 빠르고 쉽게 구축할 수 있는 강력한 오픈 소스 파이썬 라이브러리입니다.\n\n<div class=\"content-ad\"></div>\n\n모든 난이도의 개발자를 대상으로 설계된 Streamlit은 데이터 스크립트를 공유 가능한 웹 앱으로 쉽게 변환할 수 있도록 도와줍니다.\n\n# Streamlit을 특별하게 만드는 요소는 무엇인가요?\n\n1. 사용 용이성: Streamlit의 매력은 그 간단함에 있습니다. 직관적인 Python 코드로 앱을 작성하므로 HTML, CSS 또는 JavaScript와 같은 복잡한 웹 기술을 알 필요가 없습니다. 이 사용자 친화적인 방식은 웹 프로그래밍 경험이 부족한 데이터 과학자와 분석가들에게 앱 개발의 가능성을 열어줍니다.\n\n2. 빠른 프로토타이핑: Streamlit을 사용하면 코드를 수정하면 해당 변경 사항이 앱 인터페이스에 자동으로 업데이트되어 이터레이션 과정이 매우 신속해집니다. 이 기능을 이용하면 앱을 동적으로 조정하고 실시간으로 결과를 확인할 수 있어 빠른 프로토타이핑과 실험에 매우 유용합니다.\n\n<div class=\"content-ad\"></div>\n\n3. 다양한 내장 위젯: Streamlit에는 슬라이더, 체크박스, 드롭다운과 같은 다양한 내장 위젯이 포함되어 있어 데이터와 상호 작용하기가 매우 쉽습니다. 이러한 요소들은 코드를 최소한으로 사용하여 추가할 수 있어 사용자가 표시된 데이터나 수행 중인 계산을 조작할 수 있게 해줍니다.\n\n4. 데이터 시각화 지원: Streamlit은 Matplotlib, Seaborn, Plotly와 같은 주요 Python 데이터 시각화 라이브러리와 완벽하게 통합되어 있습니다. 이 통합을 통해 차트, 지도 및 그래프를 쉽게 앱에 통합하여 데이터를 더욱 매력적이고 정보를 제공하는 방식으로 시각화할 수 있습니다.\n\n자, 그럼 더 이상의 말이 필요 없죠. 이를 만들어 봅시다. 먼저, 필요한 것을 이해해봅시다: 사용자로서, 주요 재정 데이터를 한 곳에 표시하고 4가지 관심 영역(Crypto, ETF, 주식, 상품) 간에 쉽게 탐색할 수 있기를 원합니다. 아래는 이 모든 세그먼트에 대한 요구 사항입니다.\n\n- Cryptos: 시가 총액을 기준으로 상위 500개의 가상 화폐에 대한 실시간 가격 및 이전 종가, 시장 변동 폭, 7일 변동 사항을 표시합니다.\n- ETFs: 펀드의 성과 지표: 현재 가격, 52주 최고 및 최저가, 연 별 ETF 수익률(%), 배당 수익률 및 ETF의 실제 옵션에 대한 정보입니다.\n- Stocks: 주식의 현재 가격을 수집하고 주요 지표인 주가수익(P/E) 비율, 주당순이익(EPS)을 함께 제시합니다. 그것을 기반으로 설정한 임계값과 함께, 주식의 공정 가치를 계산하여 현재 주식 가치와 공정 주식 가치 간의 차이를 보여줍니다(차이가 충분히 크다면, 아마도 가격이 저평가된 주식일 것입니다).\n- Commodities: 현재 시장 가격, 과거 추이 (그래프), 판매 단위를 표시합니다(주식과는 다르게 각 상품에는 다른 측정 단위가 있으며, 이전 종가로부터의 가격 변동).\n\n<div class=\"content-ad\"></div>\n\n\"알았다고 했잖아!\", 버니 맥의 캐릭터가 \"오션스 13”에서 강조했던 것처럼. 코드 작업을 시작해봐요.\n\n새로운 파이썬 프로젝트를 만들고 중요한 몇 가지 파일을 포함해보세요: ETF 목록과 코드로 분석할 주식 목록이 필요할 거에요. 두 파일 모두 준비해 놨어요 (그동안 가지고 있었고, 관심이 있는 분들께 DM을 통해 공유할 수 있어요).\n\n# Streamlit 환경 설정하기\n\nStreamlit과 데이터 처리 및 시각화를 위해 pandas와 matplotlib/plotly와 같은 필요한 라이브러리를 설치해보세요.\n\n<div class=\"content-ad\"></div>\n\n\n```js\npip install streamlit pandas matplotlib plotly\n```\n\n메인 스크립트(streamlit_app.py)를 만들고 각 대시보드 패널을 위한 개별 스크립트를 작성하세요. 앱을 구성하기 위해 Streamlit의 레이아웃 기능을 활용해주세요:\n\n- 주요 네비게이션: st.sidebar.radio 또는 st.sidebar.selectbox를 사용하여 다른 금융 세그먼트 간에 탐색할 수 있게 사이드바를 활용하세요.\n- 대시보드 콘텐츠: 암호화폐, ETF, 주식 및 상품 모듈에 있는 각 페이지 함수(app())에서 데이터를 표시하는 데 필요한 표, 차트 및 상호 작용 위젯을 설정해주세요.\n\n우리의 streamlit_app.py 파일은 프로젝트의 루트 폴더에 있어야 합니다. 또한 대시보드 페이지가 있는 pages 폴더가 필요합니다 (각 대시보드마다 한 페이지씩).\n\n\n<div class=\"content-ad\"></div>\n\n\n## 내 Streamlit 어플리케이션 폴더 구조:\n- **pages/**: 대시보드의 여러 페이지를 위한 디렉토리\n    - **__init__.py**: 'pages'를 파이썬 패키지로 만듦\n    - **commodities.py**: 상품 대시보드 모듈\n    - **cryptos.py**: 암호화폐 대시보드 모듈\n    - **etfs_value.py**: ETF 대시보드 모듈\n    - **underpriced_stocks.py**: 저평가 주식 대시보드 모듈\n- **stramlit_app.py**: 주 Streamlit 어플리케이션 파일\n\n\n주 Streamlit 어플리케이션 코드부터 시작해봅시다. (특정 대시보드 페이지에 대한 중요 부분은 이미 이전 게시물 몇 개에서 살펴봤습니다.)\n\n```python\nimport streamlit as st\nfrom pages import commodities, cryptos, etfs_value, underpriced_stocks\n\n# 페이지 딕셔너리\npages = {\n    \"상품\": commodities,\n    \"암호화폐\": cryptos,\n    \"ETFs 가치\": etfs_value,\n    \"저평가 주식\": underpriced_stocks\n}\n\nst.sidebar.title('Navigation')\nchoice = st.sidebar.radio(\"페이지 선택:\", list(pages.keys()))\n\npage = pages[choice]\npage.app()  # 각 모듈이 페이지를 실행하기 위한 app 함수를 갖고 있다고 가정\n```\n\n앱 구조를 보면 매우 간단합니다. 필요한 대시보드 페이지와 Streamlit 라이브러리를 import하고, 페이지 딕셔너리를 작성하고, 사이드바 제목을 설정하고, 선택 메커니즘을 구현합니다. 이를 통해 사용자가 앱의 다른 섹션으로 이동할 수 있도록 합니다. 마지막으로 page.app() 메서드를 호출하여 사용자의 선택에 따라 적절한 페이지를 동적으로 로드합니다. 이 방법은 각 페이지 모듈 내의 특정 기능과 연결되어 대시보드를 렌더링하는 데 사용됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n작은 이정표를 달성했어요: 더 큰 목표를 향해 나아가요.\n대시보드 페이지 만들기\n\npages/commodities.py\n\n```js\nimport streamlit as st\nimport yfinance as yf\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 상품, 측정 단위 및 이름 정의\ncommodities_info = {\n    \"CL=F\": {\"unit\": \"배럴\", \"name\": \"원유 (WTI)\"},\n    \"BZ=F\": {\"unit\": \"배럴\", \"name\": \"브렌트 원유\"},\n    \"NG=F\": {\"unit\": \"mmBtu\", \"name\": \"천연가스\"},\n    \"HO=F\": {\"unit\": \"갤런\", \"name\": \"난방유\"},\n    // (중략)\n}\n\n@st.cache  # 데이터 캐싱을 통해 과도한 API 호출을 방지합니다\ndef fetch_commodity_data(tickers, period=\"6d\", interval=\"1d\"):\n    try:\n        data = yf.download(tickers, period=period, interval=interval)\n        return data\n    except Exception as e:\n        st.error(f\"상품 데이터 검색에 실패했습니다: {str(e)}\")\n        return pd.DataFrame()  # 실패 시 빈 DataFrame 반환\n\ndef app():\n    st.title(\"상품 대시보드\")\n\n    // (중략)\n\nif __name__ == \"__main__\":\n    app()\r\n```\n\n이 Streamlit 앱은 선택한 기간 동안 상품 가격과 변동을 보여주는 대시보드를 표시하는 데 사용됩니다. 사용자들은 특정 상품, 시간대 및 데이터 세부 사항을 기반으로 사용자 정의로 표시를 조정할 수 있습니다. 특정 라이브러리를 가져온 후, commodities_info라는 딕셔너리를 만들었습니다. 이 딕셔너리는 관심 상품을 정의하며, 해당 시장 티커 심볼, 측정 단위 및 이름을 포함합니다. 이 딕셔너리는 특정 상품을 티커 심볼로 참조하면서 사용자 친화적인 이름과 단위를 표시하는 데 앱 전반에서 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n다음 부분은 매우 중요합니다: @st.cache 데코레이터를 사용하여 함수 호출 결과를 캐싱하는 중요한 임무를 수행하고 있습니다. 이를 통해 입력을 기반으로 함수 호출 결과를 캐싱함으로써 yfinance에 대한 API 호출 수를 줄여 대역폭을 절약할 뿐만 아니라 첫 로드 후 사용자 상호작용 속도도 높일 수 있습니다.\n\n- tickers: 상품 기호 목록.\n- period: 데이터를 가져올 시간 기간을 지정하는 문자열 (기본값은 \"6d\" 또는 6일). 대시보드에서 사용자가 더 많은 내용을 볼 수 있도록 변경할 수 있습니다.\n- interval: 데이터 포인트의 정밀도 (기본값은 \"1d\" 또는 매일). 대시보드에서도 사용자 정의가 가능합니다.\n\n## 앱 기능\n\n이 함수는 주요 응용 프로그램 인터페이스를 정의합니다:\n\n<div class=\"content-ad\"></div>\n\n- st.title(\"상품 대시보드\"): 대시보드의 제목을 설정합니다.\n- 사이드바 입력란을 통해 사용자는 데이터의 기간과 간격을 선택하고 어떤 상품을 표시할지 선택할 수 있습니다.\n\n## 데이터 로드 및 표시\n\n상품이 선택된 경우, 앱은 fetch_commodity_data 함수를 사용하여 데이터를 검색합니다. 성공적인 데이터 검색은 최신 및 이전 종가를 사용하여 변동률을 계산하는 데이터 처리를 트리거합니다. 이 데이터는 그런 다음 데이터 프레임에 표시됩니다. 그 후에는 시각적인 그래픽을 위한 플로팅 함수를 정의하고 있습니다 (이 대시보드에서 그래프와 플롯만 사용 가능합니다. 다른 대시보드의 경우 주식/ETF/암호화폐의 수가 100보다 많아 시스템에 불필요한 부하가 될 수 있습니다).\n\n## 실행 시작점\n\n<div class=\"content-ad\"></div>\n\n```js\nif __name__ == \"__main()\":\n    app()\n```\n\n이 줄은 스크립트가 직접 실행되었는지 확인한 후 streamlit 애플리케이션을 시작하는 app() 함수를 호출합니다.\n\n축하드립니다! 네가 만든 대시보드 페이지 중 첫 번째를 성공적으로 만들었어요!\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_1.png)\n\n<div class=\"content-ad\"></div>\n\n앗, 분할 취소를 요청해주셨군요. 물론이죠, 질문이 있으시면 언제든지 물어보세요! 😉\n\n<div class=\"content-ad\"></div>\n\nMarkdown 형식으로 표를 나타냅니다.\n\n파일 pages/underpriced_stocks.py\n\n```js\nimport streamlit as st\nimport yfinance as yf\nimport pandas as pd\nimport requests\n\n# API 액세스를 위한 상수\nAPI_KEY = 'Your API Key'\nBASE_URL = 'https://financialmodelingprep.com/api/v3'\n\n@st.cache_resource\ndef fetch_sp500_tickers():\n    \"\"\"\n    API를 사용하여 현재 S&P 500 소속 티커를 가져옵니다.\n    \"\"\"\n    url = f\"{BASE_URL}/sp500_constituent?apikey={API_KEY}\"\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            data = response.json()\n            tickers = [item['symbol'] for item in data]\n            return tickers\n        else:\n            st.error(f\"티커를 가져오지 못했습니다: HTTP 상태 코드 {response.status_code}\")\n            return []\n    except Exception as e:\n        st.error(f\"요청 실패: {e}\")\n        return []\n\n@st.cache\ndef fetch_stock_data(tickers):\n    \"\"\"\n    주어진 티커에 대한 주식 데이터를 가져와 주식이 저평가되었는지 계산합니다.\n    \"\"\"\n    data = []\n    for symbol in tickers:\n        stock = yf.Ticker(symbol)\n        try:\n            info = stock.info\n            if 'currentPrice' in info and 'trailingEps' in info:\n                current_price = info['currentPrice']\n                eps = info['trailingEps']\n                pe_ratio = info.get('trailingPE', float('inf'))  # 사용 가능한 경우 trailing P/E 사용\n\n                # 목표 P/E 비율 가정\n                target_pe = 15\n                fair_value = eps * target_pe\n\n                underpriced = current_price < fair_value\n                price_gap = ((fair_value - current_price) / current_price) * 100 if current_price else 0\n\n                data.append({\n                    'Symbol': symbol,\n                    'Current Price': current_price,\n                    'EPS': eps,\n                    'Fair Market Value': fair_value,\n                    'Underpriced': '네' if underpriced else '아니요',\n                    'Price Gap (%)': round(price_gap, 2)\n                })\n        except Exception as e:\n            print(f\"{symbol}에 대한 데이터를 가져오지 못했습니다: {e}\")\n\n    return pd.DataFrame(data)\n\ndef app():\n    \"\"\"\n    S&P 500 주식 및 저평가 상태를 표시하는 Streamlit 앱입니다.\n    \"\"\"\n    st.title(\"S&P 500 주식 분석\")\n\n    tickers = fetch_sp500_tickers()\n\n    if tickers:\n        st.write(\"S&P 500 회사의 티커가 로드되었습니다.\")\n        df = fetch_stock_data(tickers)\n        if not df.empty:\n            st.dataframe(df)\n        else:\n            st.write(\"제공된 티커에 대한 데이터를 찾을 수 없습니다.\")\n    else:\n        st.write(\"주식 티커를 로드할 수 없습니다. API 설정 및 네트워크 연결을 확인해주세요.\")\n\nif __name__ == \"__main__\":\n    app()\n\n```\n\n암호화폐처럼 동일한 방식으로: 필요한 라이브러리 가져오기, financialmodelingprep 라이브러리에서 SP500 티커 가져오기 및 yfinance에서 데이터 가져오기: 각 주식별로 데이터를 가져오는 동안 limitation에 도달하는 것을 피하기 위해 이 두 작업을 서로 다른 소스 사이에 분리했습니다. financialmodelingprep의 최소 결제 요금제(매월 19.99 미국 달러)에는 분당 300회의 호출 제한이 있으므로 우리가 주식을 하나씩 가져올 때 쉽게 이 제한에 도달할 것입니다.\n\n<div class=\"content-ad\"></div>\n\nyfinance에서 무엇을 얻고 있습니까? 적절한 기준을 설정하고 특정 주식이 성장 잠재력이 있는지 고려하는 데 도움이 되는 여러 가지 지표 목록을 얻고 있습니다.\n현재 가격: 주식의 최신 거래 가격입니다.\nEPS (주당 수익): 회사가 주당 주식에 대해 벌어들이는 돈을 나타냅니다.\n목표 P/E 비율: 이는 많은 가치 투자자들을 위한 전형적인 기준인 15로 설정됩니다. 여기서는 해당 주식의 이익에 기초하여 합리적인 가격으로 간주될 수 있는 것을 예상하기 위해 사용됩니다. 목표 P/E 15는 성장과 가치 속성을 균형 있게 고려할 수 있는 중도 기준으로 선택되었습니다. 브로드 산업 범위에 역사적으로 적용되었던 산업에 대해 사용된 보수적인 수치로, 오버밸류된 시장에서 상대적 가치 평가가 낮은 주식을 식별하는 데 도움을 줄 수 있습니다.\n공정시장가치 계산: EPS * 목표 P/E로 계산됩니다. 이는 주식이 목표 P/E 비율인 15로 가치 평가되었다면 해당 주식의 공정 가치를 나타냅니다. 낮은 P/E는 주식이 수익에 비해 저평가되었을 수 있다는 것을 시사할 수 있습니다.\n저평가 여부 확인: 만일 현재 시장 가격이 계산된 공정시장가치보다 낮다면 해당 주식이 저평가되었다고 간주됩니다.\n가격 격차(%): 공정시장가치와 현재 가격 사이의 백분율 차이를 보여주며, 주식 가격이 추정된 공정 가치에 도달하기 위해 얼마나 증가해야 하는지를 나타냅니다.\n\n![Financial Dashboard](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_3.png)\n\n우리는 4개 중 3개를 얻었습니다: 마지막은 ETF 분석 대시보드입니다.\n\n파일: pages/etfs_value.py\n\n<div class=\"content-ad\"></div>\n\n```python\nimport streamlit as st\nimport yfinance as yf\nimport pandas as pd\n\nst.set_page_config(layout=\"wide\")\n\n@st.cache_resource(ttl=300, show_spinner=True)\ndef fetch_options_data(symbol):\n    \"\"\" Yahoo Finance에서 ETF 심볼의 옵션 데이터를 가져옵니다. \"\"\"\n    etf = yf.Ticker(symbol)\n    try:\n        expiration_dates = etf.options\n        options_info = []\n        for expiration_date in expiration_dates:\n            options_chain = etf.option_chain(expiration_date)\n            puts = options_chain.puts\n            calls = options_chain.calls\n            options_info.append({\n                '만기일': expiration_date,\n                '풋 옵션 개수': len(puts),\n                '콜 옵션 개수': len(calls)\n            })\n        return options_info\n    except Exception as e:\n        st.error(f\"{symbol}에 대한 옵션 데이터를 가져올 수 없습니다: {e}\")\n        return []\n\ndef format_assets(assets):\n    \"\"\" 큰 숫자를 읽기 쉬운 형식으로 변환합니다. \"\"\"\n    if assets >= 1e9:\n        return f\"{assets / 1e9:.2f}B\"\n    elif assets >= 1e6:\n        return f\"{assets / 1e6:.2f}M\"\n    return str(assets)\n\n@st.cache_data(show_spinner=True)\ndef fetch_data(symbol):\n    \"\"\" Yahoo Finance에서 ETF에 대한 금융 데이터 및 메트릭을 가져옵니다. \"\"\"\n    etf = yf.Ticker(symbol)\n    info = etf.info\n    options_info = fetch_options_data(symbol)\n    return {\n        '이름': info.get('longName', 'N/A'),\n        '최신 가격': f\"${info.get('previousClose', 'N/A')}\",\n        '52주 최고가': f\"${info.get('fiftyTwoWeekHigh', 'N/A')}\",\n        '52주 최저가': f\"${info.get('fiftyTwoWeekLow', 'N/A')}\",\n        '1년 수익률': f\"{info.get('ytdReturn', 'N/A') * 100:.2f}%\" if info.get('ytdReturn') is not None else \"N/A\",\n        '3년 수익률': f\"{info.get('threeYearAverageReturn', 'N/A') * 100:.2f}%\" if info.get('threeYearAverageReturn') is not None else \"N/A\",\n        '5년 수익률': f\"{info.get('fiveYearAverageReturn', 'N/A') * 100:.2f}%\" if info.get('fiveYearAverageReturn') is not None else \"N/A\",\n        '총 자산': format_assets(info.get('totalAssets', 'N/A')),\n        '배당 수익률': f\"{info.get('yield', 'N/A') * 100:.2f}%\" if info.get('yield') is not None else \"N/A\",\n        '평균 거래량': info.get('averageVolume', 'N/A'),\n        '옵션 상세정보': \"; \".join([f\"만기일: {opt['만기일']}, 풋: {opt['풋 옵션 개수']}, 콜: {opt['콜 옵션 개수']}\" for opt in options_info]),\n    }\n\ndef app():\n    \"\"\" ETF 분석을 표시하는 Streamlit 애플리케이션의 진입점입니다. \"\"\"\n    st.title(\"ETF 분석\")\n    refresh_button = st.button(\"데이터 새로고침\")\n\n    if refresh_button:\n        st.experimental_rerun()\n\n    file_path = \"etfs.txt\"\n    try:\n        with open(file_path, 'r') as file:\n            symbols = [line.strip().upper() for line in file.readlines()]\n            data = [fetch_data(symbol) for symbol in symbols]\n            df = pd.DataFrame(data)\n            st.table(df)\n    except FileNotFoundError:\n        st.error(\"ETF 심볼 파일을 찾을 수 없습니다. 현재 디렉토리에 'etfs.txt' 파일이 있는지 확인해 주세요.\")\n\nif __name__ == \"__main__\":\n    app()\r\n```\n\n<div class=\"content-ad\"></div>\n\n데이터를 가져와서 서식을 지정한 후 Streamlit의 st.table() 함수를 사용하여 각 ETF의 주요 지표를 명확하고 조직적으로 보여줍니다. 이 테이블에는 최신 가격, 올해의 최고가와 최저가, 수익률, 총 자산, 배당 수익률 및 자세한 옵션 데이터와 같은 세부 정보가 포함되어 있습니다.\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_4.png)\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_5.png)\n\nETF의 사용 가능한 옵션 수가 다르기 때문에 테이블의 높이와 가시성에 영향을 줍니다. 그래서 이 대시보드의 스크린샷을 2개 두었습니다.\n\n<div class=\"content-ad\"></div>\n\n그래요, 우리 최종 대시보드가 준비되었어요. 터미널을 열고 마법의 열쇠를 입력해볼까요? \"알라딘의 비밀 금고 여는 방법\"이 아니라 이런 모습을 하겠죠.\n\n```js\nstreamlit run streamlit_app.py\n```\n\n그리고, 와! 대시보드가 실행 중이에요.\n\n당신의 IDE 터미널에서 대시보드에 액세스하는 URL을 확인할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_6.png)\n\n로컬 URL 링크를 클릭하세요. 기본 브라우저에서 페이지를 열고 성취 결과를 확인할 수 있을 겁니다.\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_7.png)\n\n테이블에 대해 \"csv로 다운로드\", 검색 및 전체화면 옵션이 제공되었는지 확인하세요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_8.png\" />\n\n요약하자면 — 우리는 큰 한걸음을 내디디었어요: 어떤 데이터 표시 소스의 UI 한계에 국한되어 사용자일 뿐이었던 것으로부터, 이제 우리는 직접 대시보드를 개발할 수 있게 되었어요: 신뢰할 만한 정보 소스를 찾아내고, 원하는 형태로 정보를 제공받을 수 있도록 결정하고, 그에 맞게 조작할 수 있게 되었죠. 너무 복잡하지 않죠, 아마도 자기 계발과 금융 교육 여정에서 다음 목표에 도달하기 위한 단계에 또 다른 발걸음인 것 같아요. 코딩에 행운을 빕니다!\n\n# 쉽게 말해보면 🚀\n\nIn Plain English 커뮤니티의 일원이 되어 주셔서 감사합니다! 떠나시기 전에:\n\n<div class=\"content-ad\"></div>\n\n- 글쓴이를 클립하고 팔로우 해주세요! 👏️️\n- 팔로우하기: X | LinkedIn | YouTube | Discord | 뉴스레터\n- 다른 플랫폼 방문하기: Stackademic | CoFeed | Venture | Cubed\n- PlainEnglish.io 에서 더 많은 콘텐츠를 확인하세요!","ogImage":{"url":"/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_0.png"},"coverImage":"/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_0.png","tag":["Tech"],"readingTime":15},{"title":"Z-점수를 이용해 연령대별 달리기 성능을 비교할 수 있을까요","description":"","date":"2024-06-22 16:58","slug":"2024-06-22-CanWeUseZ-ScorestoCompareRunningPerformancesBetweenAgeGroups","content":"\n\n![image](/assets/img/2024-06-22-CanWeUseZ-ScorestoCompareRunningPerformancesBetweenAgeGroups_0.png)\n\n다른 연령대 간 경주 결과를 효과적으로 비교하려면 어떻게 해야 할까요?\n\n그것이 제가 계속 탐구하고 있는 질문입니다.\n\n우리는 나이가 들수록 느리게 움직이게 됩니다. 어떤 사람들에게는 더 큰 영향을 미치지만 60세 남성이 25세 남성과 레벨한 경쟁을 할 수 없다는 것이 결론입니다.\n\n<div class=\"content-ad\"></div>\n\n마스터 러너들을 위해 흥미로운 것을 유지하고 포용적인 러닝 커뮤니티를 유지하기 위해 많은 노력이 기울여졌습니다. 그것을 가능하게 하는 시스템인 연령 등급화라는 것을 개발하는 데 많은 노력이 기울여졌습니다. 최근 몇 주 동안, 몇 가지 대안을 탐색하기 위해 데이터를 사용해보았습니다.\n\n이전에 백분위수를 다른 비교 방법으로 제안했습니다. 이 주제에 대한 후속 기사는 여기에서 읽을 수 있습니다.\n\n잠재력이 있는 부분도 있지만 일부 단점도 있습니다. 그 주제로 나중에 돌아가서 그 중 몇 가지를 해결하고 상황을 개선해보겠습니다.\n\n그러나 오늘은 또 다른 비교 가능한 방법, 즉 Z-점수에 집중하고 싶습니다.\n\n<div class=\"content-ad\"></div>\n\n이 통계 도구가 다른 종류의 러너들의 경주 결과를 효과적으로 비교하는 데 도움이 될까요? 한번 살펴보고 알아봅시다.\n\n# Z-스코어란 무엇인가요?\n\n먼저, 기본적인 통계를 살펴봅시다. 개별 경주 결과의 z-스코어를 계산하기 전에 몇 가지 중요한 개념을 알 필요가 있습니다.\n\n샘플 경주 결과가 있다고 가정해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n런너를 '전형적'이거나 '평균적'이라고 표현하는 다양한 방법이 있습니다. 우리는 평균을 사용할 것입니다. 평균을 계산하려면 모든 결과를 더한 다음 결과의 개수로 나누면 됩니다.\n\n우리 경우, 35세 미만의 남성들의 360,075개의 경주 결과를 포함한 샘플이 있습니다. 만약 수학을 해본다면 (저는 Python의 Pandas 패키지를 사용하여 컴퓨터에 일을 시켰습니다),이 그룹의 평균 완주 시간은 4:16:36입니다 (나중에 샘플에 대해 더 언급하겠습니다).\n\n그러나 이것은 이야기의 일부에 불과합니다. 많은 러너가 4:16 정도의 완주 시간을 기록하지만, 이 샘플에서 가장 빠른 러너는 2:03:45로 완주했습니다. 다른 사람들은 6, 7 또는 8시간이 걸렸습니다.\n\n분산과 편차라는 다른 기본 개념이 있습니다. 개별 결과가 평균에서 얼마나 벗어나 있는가? 결과들은 밀접하게 뭉쳐 있거나 퍼져 있나요?\n\n<div class=\"content-ad\"></div>\n\n수학적인 내용에는 들어가지 않고, 우리는 판다스를 사용하여 이 샘플의 표준 편차를 계산할 수 있습니다: 54:07. 이는 주어진 경주 결과와 평균 완주 시간 사이의 평균 거리를 나타냅니다.\n\nZ-점수는 특정 결과가 평균으로부터 얼마나 떨어져 있는지를 이해하는 표준화된 방법입니다 — 해당 샘플의 표준 편차를 기반으로 합니다.\n\n만약 한 러너가 평균(3:22:29)보다 54:07 빨리 완주했다면, 해당 결과의 z-점수는 -1일 것입니다 — 평균보다 한 표준 편차 떨어진 값입니다. 만약 한 러너가 두 배 빠르게(2:28:22) 완주했다면, 그것은 평균보다 두 배의 표준 편차 떨어진 값일 것입니다.\n\n위의 시각화는 이 개념을 설명해 줍니다.\n\n<div class=\"content-ad\"></div>\n\n히스토그램은 360,075개의 개별 레이스 결과를 나타내며 각 5분 간격에 속하는 결과의 백분율을 보여줍니다 (즉, 3:55에서 4:00 사이).\n\n점선은 평균 값이 있는 곳 뿐만 아니라 평균 값의 한 표준 편차 위 및 아래, 그리고 두 표준 편차 위 및 아래를 나타냅니다.\n\n평균 값으로부터 멀어질수록 해당 시간을 뛰어넘은 참가자가 줄어든다는 것을 알 수 있습니다.\n\n이 개념을 다른 샘플, 예를 들어 다른 연령대에 적용해보면 실제 평균 값과 표준 편차가 달라질 수 있지만, 일반적인 원칙은 유지될 것입니다. 평균 값에서 두 표준 편차 떨어진 곳에서 레이스를 마친 선수는 한 표준 편차 떨어진 곳에서 레이스를 마친 선수보다 훨씬 적을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이것은 잠재적으로 개별 결과가 평균 이하 얼마나 떨어져 있는지를 측정하여 \"좋은\" 결과를 비교하는 방법을 제공할 수 있습니다.\n\n# 각 연령 그룹의 평균과 표준 편차 계산\n\n첫 번째 단계는 많은 데이터를 수집하는 것입니다 — 이미 수행했습니다. 여기서 더 많은 내용을 읽을 수 있습니다. 하지만, 여기에 짧게 설명하겠습니다.\n\n이 분석을 용이하게 하기 위해, 샘플로 사용할 일련의 경주를 식별했습니다. 이는 2010년부터 2019년까지 미국에서 9월, 10월 또는 11월에 개최된 500명 이상의 완주자가 있는 모든 마라톤을 포함합니다. 그런 다음, 각 완주자의 성별, 연령 및 완주 시간을 수집했는데 — 총 2,017,493개의 결과가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n몇 가지 정리와 준비 작업을 한 후, 이 결과를 Pandas 데이터프레임에 로드하여 쉬운 분석이 가능하게 했어요. 이 시리즈를 마치면 Kaggle에 완전한 데이터셋을 공유할 예정이에요. 만약 여러분이 자체적으로 분석을 하고 싶다면 이 데이터셋을 사용해보세요.\n\nPandas로 결과를 처리하면 성별 및 연령대로 결과를 그룹화하고 각 그룹의 평균과 표준 편차를 계산하는 것이 상당히 간단해집니다. 참고로, 나는 Boston Marathon 참가 자격을 얻기 위해 BAA가 사용하는 연령 구간을 사용하고 있어요. 그리고 샘플이 너무 작기 때문에 80세 이상의 러너는 포함하지 않았어요.\n\n위의 시각화는 각 연령대별 평균 완주 시간을 나타냅니다. 빨간 점은 남성이고, 초록 점은 여성입니다.\n\n여기서 놀라운 점은 없어요. 오른쪽으로 이동할수록 평균 완주 시간이 더 느려집니다. 어린 러너들 사이에는 큰 차이가 없지만, 50대와 60대로 갈수록 그 차이가 뚜렷해집니다.\n\n<div class=\"content-ad\"></div>\n\n평균적으로, 동일 연령대의 남성보다 여성이 느린 속도로 완주합니다. 그러나 연령에 따른 추세는 여성과 남성 모두 유사합니다.\n\n위의 시각화 자료는 각 그룹의 표준 편차를 보여줍니다.\n\n이건 정확히 내가 예상한 것과는 조금 다르네요 — 그리고 이는 문제가 될 수도 있습니다.\n\n각 그룹 사이의 표준 편차는 거의 비슷합니다. 노인 남성을 제외하고, 모든 그룹의 표준 편차는 50분에서 55분 사이에 있습니다. 세 가지 노인 남성 연령대 중에서는 약간 더 높습니다(55분에서 60분까지).\n\n<div class=\"content-ad\"></div>\n\n저는 정확히 무엇을 기대했는지 확신할 수 없지만, 이것이 어떤 방식으로든 평균과 함께 조정될 것으로 생각했습니다. 아래에서 어떻게 진행되는지 확인해볼게요. 그러나 한 그룹이 다른 그룹보다 더 극단적인 z-점수를 가지거나 다른 그룹과 비교할 때 과대평가될 수도 있다는 느낌이 듭니다.\n\n# 개별 결과에 정규화된 Z-점수 적용하기\n\n각 그룹에 대한 평균과 표준 편차를 계산한 후, 결과의 일부(2019)를 취하여 각 결과에 대한 z-점수를 계산했습니다.\n\n실제 실습 중에 어떻게 작용하는지 몇 가지 레이스를 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 미네아폴리스의 트윈시티 마라톤을 시작으로 하겠습니다. 여기 주요 10명의 완주자들을 z-점수로 정리해 봤어요. 비교를 위해 2020년 연령 등급 표에 따른 연령 등급 점수도 포함했어요.\n\n```js\n| 성별    | 나이  | 완주 시간 | z-점수 | 연령 등급 점수 |\n|---------|------|----------|---------|--------------|\n| F       | 40   | 02:34:07 | -2.55  | 89.12        |\n| F       | 27   | 02:31:29 | -2.54  | 88.5         |\n| F       | 24   | 02:32:49 | -2.51  | 87.73        |\n| F       | 30   | 02:35:50 | -2.45  | 86.03        |\n| F       | 33   | 02:36:34 | -2.44  | 85.69        |\n| F       | 31   | 02:38:46 | -2.4   | 84.44        |\n| F       | 26   | 02:40:08 | -2.37  | 83.72        |\n| F       | 37   | 02:40:24 | -2.37  | 84.42        |\n| F       | 30   | 02:40:13 | -2.37  | 83.68        |\n| F       | 29   | 02:41:13 | -2.35  | 83.16        |\n```\n\n여기서 눈에 띄는 것은 이 10명의 완주자가 모두 여성이라는 점이에요. 그들은 높은 연령 등급을 가지고 있으며 훌륭한 완주 시간을 보여주지만... 비슷한 시간을 보이는 남성이 전혀 없다는 것이 납득하기 어렵네요.\n\n두 번째 예로, 작은 규모의 대회인 뉴저지의 애틀랜틱 시티 마라톤을 살펴보겠습니다 (그리고 이 마라톤이 거친 첫 번째 마라톤입니다).\n\n<div class=\"content-ad\"></div>\n\n\n| Gender | Age | Finish  | zScore | Age Grade |\r\n|--------|-----|---------|--------|-----------|\r\n| F      | 28  | 02:42:48| -2.32  | 82.35     |\r\n| M      | 35  | 02:19:15| -2.27  | 87.84     |\r\n| M      | 32  | 02:21:46| -2.13  | 85.83     |\r\n| M      | 61  | 03:00:04| -2.06  | 83.75     |\r\n| F      | 41  | 03:07:27| -1.92  | 73.72     |\r\n| M      | 34  | 02:33:10| -1.92  | 79.66     |\r\n| M      | 56  | 02:58:22| -1.91  | 80.66     |\r\n| M      | 56  | 03:01:43| -1.84  | 79.17     |\r\n| F      | 41  | 03:12:30| -1.83  | 71.79     |\r\n| F      | 24  | 03:11:47| -1.76  | 69.91     |\r\n\n\r\n여기에는 결과에서 남성과 여성이 섞여 있습니다. 그러나 상위 두 결과를 살펴보세요.\r\n\r\n최고의 결과는 z-점수가 -2.32인 28세 여성으로, 2시간 42분을 달렸습니다. 이는 훌륭한 시간입니다 (그녀는 첫 번째 여성 완주자이자 총 완주자 중 네 번째이지만, 이것이 2:19를 달린 35세 남성보다 더 나은 결과일까요?\r\n\r\n여기 마지막 예시 — 버지니아주 리치먼드 마라톤을 살펴보겠습니다.\r\n\n\n<div class=\"content-ad\"></div>\n\n\n| Gender   |   Age | Finish   |   zScore |   Age Grade |\n|----------|-------|----------|----------|-------------|\n| F        |    23 | 02:36:19 |    -2.44 |       85.77 |\n| F        |    30 | 02:36:30 |    -2.44 |       85.67 |\n| F        |    28 | 02:40:08 |    -2.37 |       83.72 |\n| F        |    29 | 02:43:31 |    -2.3  |       81.99 |\n| F        |    23 | 02:47:03 |    -2.24 |       80.26 |\n| F        |    36 | 02:47:54 |    -2.23 |       80.38 |\n| F        |    35 | 02:48:45 |    -2.21 |       79.76 |\n| F        |    26 | 02:49:08 |    -2.2  |       79.27 |\n| F        |    28 | 02:49:29 |    -2.19 |       79.1  |\n| F        |    28 | 02:50:19 |    -2.17 |       78.72 |\n\n\n여기서 다시 한번 말해요, 상위 10명의 완주자는 모두 여성입니다. 상위 두 명은 (2:36) 꽤 인상적하지만, 여전히 이 목록에 들어갈만한 남성이 없다는 것은 믿기 어렵습니다.\n\n이 여성들은 모두 젊으며, 35~39세 그룹을 넘는 사람은 한 명도 없습니다.\n\n조금 더 자세히 살펴보기 위해, 리치먼드 마라톤에서 z-점수로 상위 열 명의 남성입니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\n| 성별 | 나이 | 완주 시간 | z-점수 | 나이 등급 |\n|------|------|-----------|--------|-----------|\n| M    | 31   | 02:19:43  | -2.17  | 87.07     |\n| M    | 25   | 02:20:54  | -2.15  | 86.34     |\n| M    | 30   | 02:21:34  | -2.14  | 85.93     |\n| M    | 24   | 02:22:00  | -2.13  | 85.67     |\n| M    | 35   | 02:27:14  | -2.11  | 83.08     |\n| M    | 25   | 02:24:14  | -2.09  | 84.34     |\n| M    | 55   | 02:48:50  | -2.08  | 84.45     |\n| M    | 22   | 02:25:32  | -2.06  | 83.59     |\n| M    | 26   | 02:26:26  | -2.05  | 83.08     |\n| M    | 42   | 02:36:17  | -2.01  | 81.55     |\n``` \n\n그래서 몇몇 정말 우수한 남성분들이 있었어요. 그렇지만 31살 남성분이 2시간 19분만에 완주했는데도 z-점수가 -2.17밖에 되지 않았어요. 우연히도, 그게 2시간 50분에 완주한 여성 10위보다 뒤에 오네요.\n\n여기에는 조금 더 다양한 연령대가 있어요(55살과 42살), 하지만 이 완주자들의 대부분은 35세 미만의 남성분들이에요. \n\n# z-점수를 사용하는 데 문제점\n\n\n<div class=\"content-ad\"></div>\n\n이 문제의 가장 분명한 문제는 이 측정 방법이 여성의 결과를 과대평가하기 쉽다는 것입니다 — 특히 젊은 여성의 결과를 말이죠. 큰 규모의 두 마라톤에서 전체 상위 열 명 리스트가 모두 여성으로 차지되는 경우가 있었습니다.\n\n이게 왜 발생하는 걸까요?\n\n각 연령 그룹별 평균과 표준 편차를 생각해 보십시오.\n\n35세 미만 여성의 평균 완주 시간은 4시간 43분 20초입니다. 이는 동일 연령대 남성보다 27분 늦고, 55-59세 이상의 모든 남성 연령 그룹보다도 느립니다.\n\n<div class=\"content-ad\"></div>\n\n동시에, 그들의 표준 편차(51:59)는 가장 낮은 편 중 하나입니다. 동일 연령의 남성들보다 약 두 분 더 낮습니다.\n\n이것들이 결합되면, 최고의 여성들이 평균보다 훨씬 낮은 점수를 얻을 수 있는 여지가 많아지며, 다른 연령 그룹에 비해 낮은 z-점수를 달성할 수 있습니다.\n\n해당 시점(2019년)에서, 마라톤 남성들의 세계 신기록은 2:01:39로, (2018년 베를린에서 일리우드 킵초게가 달성했습니다). 이는 -2.51의 z-점수를 달성할 것입니다.\n\n한편, 그때의 여성 세계 기록은 2:14:04로, (2019년 시카고에서 브리짓 코스게이가 달성했습니다). 이는 -2.86의 z-점수를 달성할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 시스템에서는 35세 미만의 여성에게 큰 내부적 이점이 있습니다. 2:30으로 달리는 여성이라도 Kipchoge의 세계 기록 아래에있는 z-점수(-2.55)를 달성할 수 있습니다.\n\n이 시각화는 각 연령 그룹의 달리기자 중 얼마나 많은 비율이 z-점수가 -2(파란색) 미만 또는 -1과 -2 사이 (보라색)인지 보여줍니다.\n\n그룹이 여전히 작지만, 35세 미만의 여성 중 2(1.27%) 미만의 점수를 받은 여성이 남성(0.79%)보다 더 많습니다.\n\n35~39세에는 -2 이하의 점수를 받은 여성(0.89%)이 남성(0.38%)보다 약 두 배 더 많습니다.\n\n<div class=\"content-ad\"></div>\n\n그런 노릇하며 60대와 70대 러너 사이에서도 재밌는 일들이 벌어지고 있어요. 하지만 이 그룹들이 너무 작아서 개별 레이스 결과에 명확히 나타나지 않을 수도 있어요.\n\n# 그래서 Z-점수는 레이스 결과를 이해하는 데 유용할까요?\n\n만약 연령 등급을 완전한 대체품으로 찾고 있다면, 그렇지 않다고 말할 거예요.\n\n이것은 불균형적이고, 일부 그룹이 다른 그룹보다 우위를 가지고 있다는 것이 분명해요. 젊은 여성들 사이에서 높은 평균 완주 시간은 그들이 그 평균 아래서 끝내고 낮은 Z-점수를 받을 공간이 훨씬 더 많다는 것을 명백히 보여줘요.\n\n<div class=\"content-ad\"></div>\n\n개념적으로, 일반적인 비교와 판단을 하려면 유용한 것 같아요. 어떤 러너가 평균값보다 표준 편차 하나나 둘 미만인지 아는 것은 그들이 얼마나 뛰어난지와 그 결과가 얼마나 어려운지에 대한 일반적인 감각을 제공해줍니다.\n\n하지만 이것을 극단적인 경우에 더 나은 비교를 하기 위해 보정하는 좋은 방법은 없다고 생각해요. \n\n나이 등급은 보정에 문제가 있을 수 있지만, 이 시스템을 개선된 것으로 보지는 않아요.\n\n어쩌면 다른 사람이 이 데이터를 가지고 더 나은 결과를 얻을 수 있을지도 모르겠지만, 저는 일단 지금은 이걸 잠시 내려놓고 리스트에서 지우려고 해요.\n\n<div class=\"content-ad\"></div>\n\n# 그럼 다음은 무엇인가요?\n\n지금까지 우리는 두 가지 대안을 살펴봤어요 — 백분위수와 z-점수.\n\n앞으로 나아가서, 이 두 가지 대안이 나이 등급과 어떻게 비교되는지 다시 한 번 살펴보고 싶어요. 또한 여러분이 직접 결과를 측정하고 비교할 수 있는 계산기를 만들어 공유하고 싶어요.\n\n그 다음으로, 2023년 데이터로 업데이트하고, 백분위수 시스템을 약간 수정하여 2023년 최신 나이 등급 테이블과 비교하고 싶어요.\n\n<div class=\"content-ad\"></div>\n\n이 시리즈를 마무리 짓는 것이 좋을 것 같아요. 만약 여러분이 자신의 분석을 원하신다면, 전체 데이터셋을 Kaggle에 공유할 예정이니까 참고해주세요.\n\n그 중에 하나라도 관심이 있다면, 다음 몇 개의 글을 받아보시려면 이메일 업데이트 구독을 확실하게 해주세요. 저는 다음 2~3주 안에 글을 올릴 예정이에요.\n\n만약 이 분석을 돕는 피드백이나 아이디어가 있다면, 응답을 남겨주세요. 항상 두 번째(또는 세 번째, 네 번째) 의견을 가지고 있으면 도움이 되죠!\n\n저는 열정적인 달리기 사랑자이자 데이터 좋아하는 사람입니다. 이제 40살이 되었는데, 연령대별 결과를 비교하는 것이 특히 저에게 흥미로운 분야에요. 제 활동을 계속 지켜보려면 아래와 같이 따라갈 수 있어요:\n\n<div class=\"content-ad\"></div>\n\n- 제 훈련에 대해 알아보려면 Running with Rock을 팔로우하세요\n- 마라톤 훈련 계획을 선택하는 팁을 읽어보세요\n- Strava에서 제를 스토킹하세요","ogImage":{"url":"/assets/img/2024-06-22-CanWeUseZ-ScorestoCompareRunningPerformancesBetweenAgeGroups_0.png"},"coverImage":"/assets/img/2024-06-22-CanWeUseZ-ScorestoCompareRunningPerformancesBetweenAgeGroups_0.png","tag":["Tech"],"readingTime":10},{"title":"파이썬을 사용한 탐색적 데이터 분석 EDA 방법","description":"","date":"2024-06-22 16:56","slug":"2024-06-22-ExploratoryDataAnalysisEDAUsingPython","content":"\n\n## Python에서 탐색적 데이터 분석 및 데이터 시각화에 대한 기본 예제\n\n![image](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_0.png)\n\n# 소개\n\n탐색적 데이터 분석(EDA)는 데이터 분석 과정에서 중요한 단계로, 데이터셋을 더 잘 이해하기 위한 작업입니다. EDA를 통해 데이터의 주요 특징, 변수 간의 관계, 문제에 관련이 있는 변수들을 이해할 수 있습니다. 또한 EDA를 통해 데이터에서 누락된 값, 중복된 값, 이상값 및 오류를 식별하고 처리할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 게시물에서는 Pandas, Numpy, Matplotlib 및 Seaborn과 같은 다양한 라이브러리를 사용하여 EDA(탐색적 데이터 분석)를 수행할 때 Python을 사용합니다. Pandas는 데이터 조작 및 분석을 위한 라이브러리입니다. Numpy는 숫자 계산을 위한 라이브러리입니다. Matplotlib 및 Seaborn은 데이터 시각화를 위한 라이브러리입니다.\n\n이 프로젝트에서는 2023년 Goodreads Choice Awards 베스트 북을 분석할 것입니다.\n\n![image](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_1.png)\n\n분석을 수행하고 연습을 시작하려면 Kaggle에서 이 데이터셋을 찾을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 데이터 이해하기\n\n## 필요한 라이브러리 가져오기\n\n```js\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\n## 데이터셋을 판다스 데이터프레임에 불러오기\n\n<div class=\"content-ad\"></div>\n\n```python\n# CSV 파일에 데이터가 저장되어 있으므로, Pandas 함수를 사용하여 CSV 파일을 읽을 것입니다.\ndf = pd.read_csv('Good_Reads_Book_Awards_Crawl_2023_12_27_11_14.csv')\n\ndf.sample(5) # 데이터 샘플을 표시합니다\n\n```\n\n![image](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_2.png)\n\n다음으로 데이터셋에서 일부 불필요한 열을 제거할 것이며, 이 단계는 선택 사항입니다. 사용하지 않을 열을 제거하여 DataFrame의 크기를 줄이는 것이 좋습니다.\n\n```python\n# 사용하지 않을 열은 source_URL, Book Description, About the Author입니다\ndf.drop(['source_URL','Book Description','About the Author'], axis=1, inplace=True)\n```\n\n<div class=\"content-ad\"></div>\n\n## 데이터프레임 확인하기\n\n이제 각 열의 데이터 유형을 확인하고 숫자 열의 요약을 확인하여 다음 단계를 결정할 수 있습니다.\n\n```js\ndf.info()\n```\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_3.png)\n\n<div class=\"content-ad\"></div>\n\n.info()를 통해 데이터 세트는 누락된 값이 없는 상태로 괜찮아 보입니다. 또한 데이터 세트의 형태(컬럼 수: 12, 행 수: 299) 및 각 컬럼의 데이터 유형과 같은 정보를 제공해줍니다.\n\n```js\ndf.describe()\n```\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_4.png)\n\n.describe() 메서드는 DataFrame의 숫자형 열에 대한 요약 통계를 제공합니다. 각 숫자형 열의 평균값, 중간값, 표준 편차, 최솟값 및 최댓값을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n## Int/Float 크기를 축소하고 데이터 유형 할당하기\n\n데이터의 외관과 숫자 구성을 식별했다면, 데이터 분석의 후속 단계를 결정할 수 있습니다. .info()로부터 데이터의 크기인 28.2 KB와 각 열의 데이터 유형을 알 수 있습니다. .describe() 메서드는 각 열의 최솟값과 최댓값, 그리고 평균값과 같은 숫자 열의 통계를 보여줍니다.\n\n이 결과를 통해 Number of Ratings와 Number of Reviews와 같이 여전히 숫자 열이어야 하는 열이 누락되어 있는 것을 알 수 있습니다. 이러한 열들은 천 단위 구분자로 쉼표 “,”를 사용합니다. Readers Choice Votes와 같이 일반 서식으로 저장된 열은 천 단위 구분자가 없습니다. 숫자를 쉼표로 구분하지 않고 일반 숫자를 사용하는 이유는 식별자일 수도 있고 여러 자리 숫자일 수도 있어 숫자를 분리하는 것이 적절하지 않을 수 있기 때문입니다. 따라서 쉼표를 제거하고 공백으로 대체해야 합니다.\n\n```js\nnumeric_columns = ['Number of Ratings', 'Number of Reviews']\n\n# 해당 열에서 캐릭터 쉼표를 제거하고 Int32로 변환\nfor column in numeric_columns:\n    df[column] = df[column].replace(',', '', regex=True).astype('int32')\n```\n\n<div class=\"content-ad\"></div>\n\n이 방법은 위의 열에서 모든 쉼표를 제거할 것입니다. .astype(`int32`) 이 부분은 남겨두셔도 좋습니다. 왜냐하면 판다스가 자동으로 데이터 유형을 int64로 할당해줄 것이기 때문입니다. 여기서 64와 32는 각 행당 메모리 양이나 비트 수를 나타냅니다.\n\n그대로 둘 수 있지만, 데이터프레임을 더 효율적으로 만들기 위해 이러한 값을 낮은 값으로 다운캐스팅하고 있습니다. 아래 숫자형 열의 범위를 살펴보면:\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_5.png)\n\n각 숫자형 열의 가장 작은 값과 가장 큰 값을 보여줍니다. 예를 들어 'Readers Choice Votes' 열에서, 가장 작은 값은 935이고 가장 큰 값은 397565입니다.\n\n<div class=\"content-ad\"></div>\n\n값의 범위를 알면 해당 값을 저장하는 데 필요한 비트를 결정할 수 있습니다. 참고로:\n\n- Int8 변수는 -128에서 127까지의 값을 저장할 수 있습니다.\n- Int16 변수는 -32,768에서 32,767까지의 값을 저장할 수 있습니다.\n- Int32 변수는 -2,147,483,648에서 2,147,483,647까지의 값을 저장할 수 있습니다.\n- Int64 변수는 -9,223,372,036,854,775,808에서 9,223,372,036,854,775,807까지의 값을 저장할 수 있습니다.\n\nInt32가 값 범위에 딱 맞는 옵션입니다. Int64를 사용할 수도 있지만 더 많은 메모리를 사용하고 DataFrame을 덜 효율적으로 만든다는 점에서 적절하지 않습니다.\n\n실수의 경우에는 약간 다릅니다. 실수는 데이터가 저장할 수 있는 소수 자릿수에 실제로 영향을 줍니다. Float16는 4자리 소수를 저장하고, Float32는 8자리 소수를 저장하며, Float64는 16자리 소수를 저장할 수 있습니다. DataFrame에서 많은 소수 자릿수를 사용할 필요는 없지만 원래 값과 동일하게 유지하고 싶은 경우 Float16을 사용하는 것이 가장 좋은 선택입니다.\n\n<div class=\"content-ad\"></div>\n\n이제 각 열의 값을 범위로 알고 있으므로 해당 열을 올바른 데이터 유형으로 할당할 것입니다.\n\n또한 일부 열은 텍스트 값을 저장합니다. 해당 열의 데이터 유형을 문자열 또는 범주로 할당할 수 있습니다. 판다스 문서에 따르면 범주형 데이터 유형은 소수의 다른 값으로 구성된 문자열 변수(예: 성별, 사회 계급, 혈액형, 국가 소속 등)에 유용합니다. 이 정의에 따라 'Column Readers Choice Category'가 범주형 데이터 유형을 사용하는 가장 적합한 선택입니다.\n\n```js\n# 나머지 열을 올바른 데이터 유형으로 변환합니다.\nconvert_dict = {'Readers Choice Votes': 'int32',\n                'Readers Choice Category': 'category',\n                'Title': 'string',\n                'Author': 'string',\n                'Total Avg Rating': 'float16',\n                'Number of Pages': 'int16',\n                'Edition': 'category',\n                'First Published date': 'datetime64[ns]',\n                'Kindle Price': 'float16'}\ndf = df.astype(convert_dict)\n```\n\n'Kindle Version and Price' 열의 경우 이미 가격 데이터를 저장하는 'Kindle Price'라는 다른 열이 있으므로 가격을 제거할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```jsx\n#텍스트에서 통화를 분리하여 새 열에 넣습니다\ndf['Kindle Version'] = df['Kindle Version and Price'].str.extract('([a-zA-Z ]+)', expand=False).str.strip()\n\n#열을 올바른 데이터 유형으로 변경합니다\ndf['Kindle Version'] = df['Kindle Version'].astype('category')\n\n#이전 열을 제거합니다\ndf = df.drop('Kindle Version and Price', axis=1)\n```\n\n이제 데이터프레임을 다시 확인해보겠습니다:\n\n```jsx\ndf.info()\n```\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_6.png)\n\n<div class=\"content-ad\"></div>\n\n```js\ndf.describe()\n```\n\n![Exploratory Data Analysis using Python](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_7.png)\n\n```js\ndf.sample(10)\n```\n\n![Exploratory Data Analysis using Python](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_8.png)\n\n\n<div class=\"content-ad\"></div>\n\n최근 결과에서 보듯이 데이터프레임의 메모리 크기를 크게 줄일 수 있습니다. 데이터 유형을 변경하고 정수 및 부동 소수점을 다운캐스팅하여 메모리 크기를 줄이는 방법입니다. 이 단계는 선택 사항이지만 대규모 데이터셋을 다룰 때 매우 유용할 수 있습니다.\n\n# 데이터 분석 및 시각화\n\n## 카테고리 분포\n\n첫 번째 분석은 데이터셋 내의 다양한 카테고리별 책 분포를 찾는 것입니다. 그런 다음 seaborn 모듈을 사용하여 시각화할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ncat_counts = df['독자 선호 카테고리'].value_counts()\nprint(cat_counts)\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=cat_counts.index, y=cat_counts.values, palette='Blues_d')\nplt.title('카테고리별 분포')\nplt.xlabel('카테고리')\nplt.ylabel('책 수')\nplt.xticks(rotation=30, ha='right')\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_9.png\" />\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_10.png\" />\n\n우리 데이터는 모든 카테고리에 고르게 분포되어 있습니다. 다만, 데뷔 소설 카테고리만 19권의 책이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다음은 각 카테고리별 투표, 평점, 리뷰, 페이지 및 가격 분포를 분석할 것입니다. 이 분포를 시각화하기 위해 상자 그림을 사용할 것입니다.\n\n```js\nfig, axes = plt.subplots(3, 2, figsize=(16, 18), sharey=False, sharex=True)\n\n# 첫 번째 플롯: 독자 선호 투표 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Readers Choice Votes', palette='Set3', ax=axes[0, 0])\naxes[0, 0].set_title('각 카테고리별 독자 선호 투표 분포')\naxes[0, 0].set_ylabel('투표')\n\n# 두 번째 플롯: 평균 평점 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Total Avg Rating', palette='Set3', ax=axes[0, 1])\naxes[0, 1].set_title('각 카테고리별 평균 평점 분포')\naxes[0, 1].set_ylabel('평균 평점')\n\n# 세 번째 플롯: 평가 수 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Number of Ratings', palette='Set3', ax=axes[1, 0])\naxes[1, 0].set_title('각 카테고리별 평가 수 분포')\naxes[1, 0].set_ylabel('평가 수')\n\n# 네 번째 플롯: 리뷰 수 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Number of Reviews', palette='Set3', ax=axes[1, 1])\naxes[1, 1].set_title('각 카테고리별 리뷰 수 분포')\naxes[1, 1].set_ylabel('리뷰 수')\n\n# 다섯 번째 플롯: 페이지 수 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Number of Pages', palette='Set3', ax=axes[2, 0])\naxes[2, 0].set_title('각 카테고리별 페이지 수 분포')\naxes[2, 0].set_ylabel('페이지 수')\n\n# 여섯 번째 플롯: 킨들 가격 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Kindle Price', palette='Set3', ax=axes[2, 1])\naxes[2, 1].set_title('각 카테고리별 킨들 가격 분포')\naxes[2, 1].set_ylabel('킨들 가격')\n\nfor ax in axes[2, :]:\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha='right')\n    \nfig.tight_layout()\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_11.png\" />\n\n대부분의 분포가 편향되어 있으며, 몇몇 카테고리에서 극단값이 있습니다. 평균 평점 분포는 데이터가 정규 분포되어 있습니다. 편향된 데이터의 중심 경향성을 측정하는 가장 좋은 방법은 중앙값을 사용하는 것입니다. 중앙값은 극단값(이상치)에 민감하지 않기 때문에 적합합니다.\n\n<div class=\"content-ad\"></div>\n\n## 카테고리별 분석\n\n이제 투표, 평점, 리뷰, 페이지 및 가격별로 카테고리를 조사하여 2023년 가장 인기 있는 카테고리를 찾아보겠습니다.\n\n```js\n# 집계할 열을 결정합니다.\naggregations = {'Readers Choice Votes': 'sum', \n                'Total Avg Rating': 'mean',\n               'Number of Ratings': 'sum',\n               'Number of Reviews': 'sum',\n                'Number of Pages': 'median',\n                'Kindle Price': 'median',\n               }\n\n# 책 분야별로 그룹화합니다.\ncategory_vote = df.groupby('Readers Choice Category').agg(aggregations).sort_values('Readers Choice Votes', ascending=False)\n\n# 각 카테고리의 총 투표, 총 평점 및 총 리뷰의 백분율을 계산합니다.\ntotal_votes = category_vote['Readers Choice Votes'].sum()\ntotal_ratings = category_vote['Number of Ratings'].sum()\ntotal_reviews = category_vote['Number of Reviews'].sum()\npercent_of_total_votes = (category_vote['Readers Choice Votes'] / total_votes) * 100\npercent_of_total_ratings = (category_vote['Number of Ratings'] / total_ratings) * 100\npercent_of_total_reviews = (category_vote['Number of Reviews'] / total_reviews) * 100\n\n# 새로운 Votes, Ratings 및 Reviews의 데이터프레임을 생성합니다.\nresult_df = pd.DataFrame({\n    'Votes (합산)': category_vote['Readers Choice Votes'], \n    '% 투표': percent_of_total_votes, \n    '평균 평점': category_vote['Total Avg Rating'].round(2),\n    'Number of Ratings': category_vote['Number of Ratings'],\n    '% 총 평점': percent_of_total_ratings.round(2),\n    'Number of Reviews': category_vote['Number of Reviews'],\n    '% 총 리뷰': percent_of_total_reviews.round(2),\n    'Median Pages': category_vote['Number of Pages'],\n    'Median Kindle Price': category_vote['Kindle Price'].round(2)\n    })\n\n# 가장 많이 투표된 카테고리 찾기\nmax_voted_cat = result_df['Votes (합산)'].idxmax()\nmax_votes = result_df['Votes (합산)'].max()\navg_rat = result_df.loc[max_voted_cat, '평균 평점']\n\n# 가장 많이 평가된 카테고리 찾기\nmax_rated_cat = result_df['Number of Ratings'].idxmax()\nmax_rates = result_df['Number of Ratings'].max()\npct_max_rates = result_df['% 총 평점'].max()\n\n# 가장 많이 리뷰된 카테고리 찾기\nmax_reviewed_cat = result_df['Number of Reviews'].idxmax()\nmax_reviews = result_df['Number of Reviews'].max()\npct_max_reviews = result_df['% 총 리뷰'].max()\n\n# 결과 출력\nprint(f\"'{max_voted_cat}' 카테고리가 2023년 가장 많이 투표된 카테고리로 선정되었습니다. {max_votes:,}표를 획득했습니다.\")\nprint(f\"'{max_rated_cat}' 카테고리가 2023년 가장 많이 평가된 카테고리로 선정되었습니다. 평균 평점은 {format(avg_rat, '.2f')}이며, 평가 횟수는 {max_rates:,}회이며, 전체 평점의 {format(pct_max_rates, '.2f')}%를 차지하고 있습니다.\")\nprint(f\"'{max_reviewed_cat}' 카테고리가 2023년 가장 많이 리뷰된 카테고리로 선정되었습니다. 리뷰 수는 {max_reviews:,}개이며, 전체 리뷰의 {format(pct_max_reviews, '.2f')}%를 차지하고 있습니다.\")\n\nresult_df\n```\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_12.png)\n\n\n<div class=\"content-ad\"></div>\n\n다음으로는 데이터를 시각화하여 더 나은 이해와 시각화를 하려고 합니다.\n\n```js\nfig, axes = plt.subplots(3, 2, figsize=(16, 18), sharey=False)\n\n# 첫 번째 그래프\nsns.barplot(x=result_df.index, y=result_df['Votes (sum)'], palette='Blues_d', order=result_df.index, ax=axes[0, 0])\naxes[0, 0].set_title('각 카테고리별 독자 투표수')\naxes[0, 0].set_ylabel('투표수')\naxes[0, 0].set_xticklabels(labels=result_df.index, rotation=30, ha='right')\n\n# 두 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Avg Ratings', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Avg Ratings'], palette='Blues_d', order=result_df_sorted.index, ax=axes[0, 1])\naxes[0, 1].set_title('각 카테고리별 평균 평점')\naxes[0, 1].set_ylabel('평균 평점')\naxes[0, 1].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\n# 세 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Number of Ratings', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Number of Ratings'], palette='Blues_d', order=result_df_sorted.index, ax=axes[1, 0])\naxes[1, 0].set_title('각 카테고리별 평점 수')\naxes[1, 0].set_ylabel('평점 수')\naxes[1, 0].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\n# 네 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Number of Reviews', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Number of Reviews'], palette='Blues_d', order=result_df_sorted.index, ax=axes[1, 1])\naxes[1, 1].set_title('각 카테고리별 리뷰 수')\naxes[1, 1].set_ylabel('리뷰 수')\naxes[1, 1].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\n# 다섯 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Median Pages', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Median Pages'], palette='Blues_d', order=result_df_sorted.index, ax=axes[2, 0])\naxes[2, 0].set_title('각 카테고리별 평균 페이지 수')\naxes[2, 0].set_ylabel('페이지 수')\naxes[2, 0].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\n# 여섯 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Median Kindle Price', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Median Kindle Price'], palette='Blues_d', order=result_df_sorted.index, ax=axes[2, 1])\naxes[2, 1].set_title('각 카테고리별 평균 킨들 가격')\naxes[2, 1].set_ylabel('킨들 가격 ($)')\naxes[2, 1].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\nplt.tight_layout()\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_13.png\" />\n\n그럼 여기까지입니다. 평균 평점이 가장 높지 않은에도 불구하고, Romance이 2023년에 가장 인기 있는 책 카테고리로 선정되었습니다. 다른 카테고리보다 투표, 평점 및 리뷰에서 우위를 차지했습니다. Ratings와 Reviews에서 두 번째로 높은 등수를 차지하며 숫자가 두 배 더 많습니다. 한편, Humor와 History & Biography는 2023년에 가장 인기 없는 두 가지 책 카테고리로 순위했습니다.\n\n<div class=\"content-ad\"></div>\n\n각 카테고리의 가격은 매우 유사한 편이지만, Romance과 Romantasy는 모든 카테고리 중에서 가장 낮은 중간 가격을 가지고 있습니다. 이에도 불구하고 투표 수, 평점 및 리뷰 수가 많습니다.\n\n## 상관 관계 찾기\n\n이제 질문이 등장합니다. 투표 수, 리뷰, 평점 또는 페이지 수와 가격 사이에 어떤 상관 관계가 있는지 알아볼까요? 페이지 수가 많으면 평점이 높을까요? 아니면 낮은 가격 카테고리일수록 더 많은 리뷰와 높은 평점을 받게 될까요? 알아보도록 하겠습니다.\n\n```js\n# 열 할당\ncolumns_of_interest = ['리뷰 수', '평가 수', '페이지 수', '총평균 평점', '최다선정 표', '킨들 가격']\n\n# 상관 행렬 계산\ncorrelation_matrix = df[columns_of_interest].corr()\n\n# 상관 행렬 표시\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\nplt.title('상관 행렬')\nplt.xticks(rotation=30, ha='right')\nplt.show()\n```\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_14.png\" />\n\n이 행렬을 통해 독자 선호투표와 리뷰 수와 평가 수 간에 높은 상관 관계가 있다는 것을 알 수 있습니다. 높은 리뷰와 평점은 높은 투표와 관련이 있습니다. 페이지 수와 가격은 투표, 평점, 리뷰 사이에 강한 연결이 없습니다. 즉, 책의 가격과 두께가 투표, 평점, 리뷰에 실제로 영향을 미치지 않는다는 것을 의미합니다.\n\n## 책으로 분석하기\n\n2023년 가장 투표를 많이 받은 책이 무엇인지 확인해 보는 시간입니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\nmost_voted_books = df[['Title', 'Readers Choice Category', 'Readers Choice Votes', 'Total Avg Rating', 'Number of Ratings', 'Number of Reviews', 'Number of Pages']].sort_values(by=['Readers Choice Votes', 'Number of Ratings', 'Number of Reviews'], ascending=False).head(20)\n\nplt.figure(figsize=(14, 6))\nsns.barplot(x=most_voted_books['Title'], y=most_voted_books['Readers Choice Votes'], data=most_voted_books, palette='Blues_d')\nplt.title('2023년 가장 많이 투표된 책')\nplt.xlabel('책 제목')\nplt.ylabel('투표 수')\nplt.xticks(rotation=30, ha='right')\nplt.show()\n\nmost_voted_books\n```\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_15.png\" />\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_16.png\" />\n\n그래서 이겨낸 책이 결정되었습니다. Fourth Wing은 2023 독자 선호 투표에서 가장 인기 있는 책으로 우승을 차지했습니다. 2위인 Yellowface보다 거의 2배 많은 표를 받아 거의 백만 개의 평가를 받았습니다. 로맨티지 카테고리에 해당하는 투표 중 절반 이상을 차지했어요.\n\n<div class=\"content-ad\"></div>\n\n\n이제 모든 카테고리에서 수상자들을 살펴보겠습니다.\n\n```js\nmax_votes_index = df.groupby('Readers Choice Category')['Readers Choice Votes'].idxmax()\ntitles_with_max_votes = df.loc[max_votes_index, ['Readers Choice Category', 'Title', 'Readers Choice Votes', 'Total Avg Rating', 'Number of Ratings', 'Number of Reviews', 'Number of Pages']].sort_values('Readers Choice Votes', ascending=False)\ntitles_with_max_votes\n```\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_18.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n다음으로, 2023년에 각 월별로 발행된 책의 수를 알아내기 위해 barplot을 사용하여 분석할 예정입니다.\n\n```js\nimport calendar\ndf['First Published date'] = pd.to_datetime(df['First Published date'])\n\n# 2023년에 발행된 책만 추출합니다.\nbooks_2023 = df[df['First Published date'].dt.year == 2023]\n\n# 각 월별 발행된 책의 수를 계산합니다.\nbooks_per_month = books_2023.groupby(books_2023['First Published date'].dt.month)['Title'].count().reset_index()\nbooks_per_month['Month'] = books_per_month['First Published date'].apply(lambda x: calendar.month_abbr[x])\n\nplt.figure(figsize=(14, 8))\nsns.barplot(data=books_per_month, x='Month', y='Title', palette='Blues_d')\nplt.title('2023년에 발행된 책의 분포')\nplt.xlabel('월')\nplt.ylabel('발행된 책의 수')\nplt.show()\n\nbooks_per_month[['Month','Title']]\n```\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_19.png\" />\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_20.png\" />\n\n<div class=\"content-ad\"></div>\n\n이 그래프를 통해 우리는 11월에 가장 적은 책이 출간되었음을 발견했습니다. 반면에 9월과 1월은 가장 많은 책이 출간된 달이었습니다.\n\n# 결론\n\n우리의 분석을 통해 2023년에 어떤 카테고리가 가장 인기가 많고 가장 인기가 적은지를 결정했습니다. 또한 페이지, 투표, 평점 및 리뷰 사이의 연결 여부를 확인하기 위해 분포 분석과 상관 분석을 수행했습니다.\n\n내가 방금 보여준 것은 Python을 데이터 분석과 시각화 도구로 사용하는 놀라운 방법의 일감입니다. 이 기사에서는 데이터 집합을 더 깊게 이해할 수 있도록 중요한 기본 단계를 다루었는데, 이를 위해 분석을 위한 pandas와 시각화를 위한 matplotlib/seaborn과 같은 라이브러리를 사용했습니다.\n\n<div class=\"content-ad\"></div>\n\n제 글을 읽어 주셔서 감사합니다. 읽으시는 데 즐거움을 느끼셨기를 바라고 파이썬에서의 탐색적 데이터 분석을 이해하는 데 도움이 되었기를 바랍니다.\n\n여기서 저의 전체 코드를 제 Github에서 찾아볼 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_0.png"},"coverImage":"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_0.png","tag":["Tech"],"readingTime":18},{"title":"포인트 클라우드 시각화 및 렌더링을 위한 블렌더 핸드북","description":"","date":"2024-06-22 16:51","slug":"2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering","content":"\n\n## BLENDER\n\n이 튜토리얼에서는 대량의 포인트 클라우드 데이터 세트를 조작하고 시각화하는 데 최고의 3D 도구 중 하나를 활용하는 방법에 대해 채우고자 합니다.\n\n![이미지](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_0.png)\n\n이 도구는 Blender라고 합니다. 다양한 데이터 시각화 기술을 실험하여 복잡한 분석 시나리오에 대응할 수 있습니다. 그리고 이것이 바로 저희를 함께 모이게 한 이유입니다.\n\n<div class=\"content-ad\"></div>\n\n블렌더의 확장된 데이터 시각화 능력과 결합하여 리얼리티 캡처 데이터셋(포인트 클라우드 형태)을 처리하는 최상의 기본적인 워크플로우는 무엇인가요?\n\n🦊Florent: 리얼리티 캡처는 상당히 혼란스러울 수 있는 \"새로운\" 용어로, 몇몇 소프트웨어와 회사들이 이 용어에서 이름을 딴 것을 알 수 있습니다. 이 \"전문 분야\"를 \"3D 맵핑\"의 특화된 분야로 볼 수 있으며, 목표는 LiDAR나 패시브 카메라(포토그래미터리와 3D 컴퓨터 비전을 통해)와 같은 다양한 센서를 사용하여 실제 세계의 3D 기하학을 캡처하는 것입니다. 이 과정을 보여드리는 기사는 여기에서 확인하실 수 있습니다: 포토그래메트리를 활용한 3D 재구성 가이드\n\n이 가이드에서는 아래에 설명된 9단계의 프로세스로 나누어 설명하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다음은 데이터 시각화를 위한 분석 내용으로 여러 경로 추출 시갠 제품을 생성하도록 해줍니다. 하지만 바로 시작하기 전에, 데이터 시각화에 대해 어떻게 생각하시나요?\n\n🎵 독자분들께: 이 실전 안내서는 UTWENTE의 F. Poux 및 P. Raposo 공동 저자와 함께 작업한 결과물입니다. 트웬테 대학교 ITC 학부에서 부여된 ITC -프로젝트에서 디지털 트윈스로부터의 재정적 기여를 인정합니다. 모든 이미지는 Florent Poux의 저작권 소유입니다.\n\n# 데이터 시각화를 위한 분석\n\n<div class=\"content-ad\"></div>\n\n무언가를 생각할 때 어떤 시각적 표현을 만들어 놓는다고 느끼시나요? 희귀종인 나르월(Monodon monoceros) 같은 것을 언급한다면 이미 알고 있는 \"데이터/지식 점\"이 있다면, 바로 이 '바다의 유니콘'으로 불리는 이 생물의 긴 나선 모양의 엄니를 떠올리게 될 것입니다.\n\n🦊 Florent: 요즘 나르월에 좀 관심을 갖고 있어요 🦄. 직접 본 적은 없지만, 이제 북극 바다인 캐나다와 그린란드 인근으로 가야 이 아름다운 생물들을 발견할 수 있는 기회가 된다는 걸 알았어요. 그들은 '떼'를 이루는 사교적 동물로 발견되며 굉장한 다이빙 능력을 갖고 있다고도 배워요. 1.5분 정도 밑물에 참는다고 해도 행복한데; 나르월은 1,500m(4,921피트) 깊이까지 다이빙 하며 종종 25분 이상 숨을 참을 수 있다고 해요. 정말 멋진 힘이에요!\n\n그리고 이것은 우리에게 얼마나 본능적으로 \"데이터 시각화\"가 중요한지를 보여줍니다. 우리는 강력한 능력을 갖고 있어서 다양한 주제를 시각적으로 지원하여 종합하거나 표현할 수 있어요. 분석 작업의 경우, 아래에 나와 있는 것처럼 이러한 '지원'과 그 범위를 구체화할 수 있습니다.\n\n하지만 더욱 강력한 건 현대 컴퓨팅 시스템을 통해 실제 물체를 나타내는 3D 지오메트리와 속성을 다루는 능력입니다. 실제로, 이것은 우리가 좀더 관련성을 갖고 설정한 목표에 맞는 더 나은 시각화를 만들 수 있도록 하는 필수적인 도구로 작용합니다.\n\n<div class=\"content-ad\"></div>\n\n그리고 중요성에 대해 이야기할 때, 자크 베르탱(1918~2010)에 대해 들어보셨나요? 벨기에 가수 자크 브렐이나 프랑스 전 대통령 자크 시라크, 해군 장교 자크 쿠스토가 아니라 자크 베르탱에 관해 한 번 이야기해 보렐까요?\n\n## 베르탱의 시각 변수\n\n우리가 지리 공간 맥락에 둔 발코니 데이터 시각화에 대해 이야기하는 것은 어려운 일입니다. 그리고 베르탱의 시각 변수를 논하지 않고 싶진 않습니다. 이들은 프랑스 지도 제작자 자크 베르탱에 의해 소개된 기본적인 속성들로, 데이터를 그래픽적으로 나타내는 데 사용될 수 있습니다. 이들은 정보를 시각적으로 명확하고 효과적으로 인코딩하는 방법을 제공합니다. 아래에 이러한 변수들을 요약한 그림이 있습니다.\n\n이 시각 변수들은 매력적인 시각화를 디자인하기 위한 구조화된 프레임워크를 제공합니다. 이러한 변수들을 전략적으로 선택하고 결합함으로써, 우리는 복잡한 정보를 효과적으로 전달하는 시각적 표현물을 만들어 낼 수 있습니다. 이를 통해 시청자가 데이터를 이해하고 해석하기 쉽게 만들어줍니다.\n\n<div class=\"content-ad\"></div>\n\n🦊 플로랑: 내가 너의 마음에 이 작은 씨앗을 심음으로써, 너가 이 글을 읽으면서 다양한 렌더링 옵션들과 놀면서 변수가 데이터 시각화 제품의 목표에 전달되는 메시지에 어떻게 영향을 미치는지에 대해 어떤 평행을 유발할 수 있기를 바란다!\n\n그리고 이제, 더 이상 말미암은 것 없이, 엄청난 시각화를 만드는 시간이다. 준비되었나요?\n\n# 단계 1. 블렌더 환경 설정하기\n\n우리의 환경을 설정하는 것으로 시작해보겠습니다. 이를 위해 브라우저를 열고 Blender를 검색하세요. Blender 웹사이트에 도착하면 다운로드 섹션으로 이동하여 Blender를 다운로드하세요.\n\n<div class=\"content-ad\"></div>\n\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*Duf49e4mj8YYS2Bj9S2wJg.gif)\n\nBlender의 버전은 적어도 3.6 이상이어야 합니다. 이 버전은 특히 포인트 클라우드에 대한 몇 가지 조정이 있습니다.\n\n🦊 Florent: 낮은 버전을 사용 중이라면 이 튜토리얼에서 사용할 몇 가지 포인트 클라우드 가져오기 함수를 놓칠 수 있습니다. 그러나 낮은 버전을 사용한다면 함수를 직접 작성할 수 있지만, 이로 인해 이 튜토리얼의 범위가 확장되고 다른 세션에서 보여줍니다. 더 높은 버전인 4.0과 같은 경우에는 과정이 더 간단합니다.\n\nOS에 Blender 3.6.4 이상을 성공적으로 다운로드했다면, 설치를 해보세요. 이 튜토리얼에서는 Windows를 사용하겠지만, Linux나 MacOS에서도 동일하게 작동해야 합니다. Blender를 설치했다면, 두 번째 단계로 넘어가서 3D 포인트 클라우드 데이터셋을 수집할 차례입니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 단계 2. 3D 포인트 클라우드 전처리\n\n우리는 중요한 장면의 포인트 클라우드 데이터를 수집해야 합니다. 오늘은 제가 친구 로만 로브룩과 함께 촬영한 오래된 공장을 가져왔어요. 아래의 안내서는 포토그램메트리와 LiDAR 처리를 사용하여 데이터셋을 얻는 방법을 설명합니다.\n\n데이터 전처리 단계를 돕기 위해, 이미 \"PLY\" 파일 형식으로 준비된 포인트 클라우드 데이터를 제공해드렸어요.\n\n🦚 참고: \"PLY\" 파일 형식은 \"다각형 파일 형식\"을 나타내며 3D 기하학 데이터를 저장하는 데 널리 사용됩니다. 이 형식은 Stanford 대학의 Greg Turk와 Marc Levoy에 의해 개발되었으며 3D 모델을 정점(공간상의 점)과 다각형(일반적으로 삼각형 또는 사각형이지만, 다른 다각형 유형도 사용할 수 있음)의 집합으로 표현할 수 있습니다. 이는 다양하며 각 정점 또는 면과 관련된 데이터 속성 범위를 지원하며 색상, 법선 벡터, 텍스처 좌표 등을 포함할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n포인트 클라우드의 경우, 우리는 얼굴이 없고 점만 가지고 있습니다. 이 점들은 공간에서의 x, y, z 좌표로 정의되는 3D 점들입니다. \"PLY\" 파일을 준비할 때, 각 점이 연관된 하나의 추가 속성(색상)을 가지도록 합니다.\n\n🦊Florent: \"PLY\" 파일은 ASCII 및 BINARY 형식으로 제공됩니다. ASCII PLY 형식은 데이터를 평문으로 표현하여 사람이 읽을 수 있지만 파일 크기가 커질 수 있습니다. BINARY PLY는 이진 인코딩을 사용하여 파일 크기를 줄이지만 사람이 읽을 수 없는 내용을 가지고 있습니다. 양식을 사용하면 파일이 어떤 내용을 포함하고 있는지 빠르게 파악할 수 있으며, 이는 각 정점의 수와 관련된 속성의 유형 등을 나타냅니다.\n\n그러나 데이터셋을 캡처한 후, 추가 단계를 거쳤습니다: 시나리오 내의 구성 요소를 구분하기 위해 비지도 분할 체계를 적용했습니다.\n\n![이미지](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_2.png)\n\n<div class=\"content-ad\"></div>\n\n초기 장면을 하위 요소로 효율적으로 분할해서 관심 지점을 담고 있는 \"레이어\"로 데이터를 준비할 수 있었어요.\n\n구분된 포인트 클라우드 결과물은 드라이브 폴더에서 찾을 수 있어요. 각 클라우드는 색상이 있는 PLY 파일이에요: [드라이브 폴더](Access to the Drive Folder)에서 확인할 수 있어요. 이 폴더에는 1에서 9까지 번호가 매겨진 아홉 개의 포인트 클라우드가 들어 있어요. 각각이 장면의 일부를 갖고 있어요.\n\n![이미지](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_3.png)\n\n이 분할된 요소를 사용하기 전에, 전체 포인트 클라우드를 하나의 entity로 내보내기도 했어요. 여기서 다운로드할 수 있어요: [포인트 클라우드 데이터 다운로드](Point Cloud Data Download).\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_4.png)\n\nStep 3 이동 전에 전체 포인트 클라우드 데이터 세트를 다운로드하여 전체 장면을 설정합니다.\n\n# Step 3. Blender에 포인트 클라우드 가져오기\n\n이제 3D 데이터 세트와 Blender가 모두 설치되었으므로 사용할 시간입니다!\n\n\n<div class=\"content-ad\"></div>\n\n🦊Florent: 블렌더 내에서 3D 포인트 클라우드를 가져오는 것은 도전적일 수 있으며, 최상의 및 가장 빠른 방법을 찾는 데 시간이 걸렸어요. 그러나, 적어도 10시간 이상의 복잡한 시행착오를 거칠 필요 없이 작동하는 방법을 발견했답니다. Blender 버전 (3.6.4+)을 사용하면 색상 정보가 포함된 포인트 클라우드를 가져올 수 있게 되었는데, 이전에는 불가능했어요.\n\nBlender 3.6을 실행하려면, 프로그램을 그냥 열어주세요. 안으로 들어가면 작은 팝업 창에 버전 번호가 표시되어 있을 거예요 (제 경우엔 3.6.4로 보였어요). 이 창을 닫으려면 화면 어디든 클릭하세요. Blender에 들어가면 3D 씬 안에 자신을 발견하게 될 겁니다. 이 씬에는 큐브, 카메라, 그리고 빛(보기 어려울 수 있어요)이 포함되어 있어요. 카메라 관점에서 씬을 탐색하려면 휠을 클릭하고 드래그하여 중심 지점 주변을 회전하세요. (카메라를 옆으로 이동시키려면) 휠을 끌면서 Shift 키를 누르세요.\n\n아래와 같이 표시된 바와 같이, 시작하기 전에 큐브를 선택하고, 이 시나리오에 사용되지 않을 것이기 때문에 삭제하세요.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*A8Ih-UvMf1OViKKcwz7k3Q.gif)\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*3Q10Ik1O01ccdsxOVjD0Sg.gif)\n\n그런 다음 `파일`로 가서 “실험적 Stanford .PLY” 옵션을 선택하여 아래와 같이 포인트 클라우드를 가져올 수 있습니다.\n\n🦊Florent [업데이트]: 최신 버전의 Blender 4.0+에서는 비실험적인 Stanford PLY (.ply)를 사용할 수 있습니다. 이제 이 실험적 기능을 통합하고 RGB 정보를 읽을 수 있습니다.\n\n![이미지](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_5.png)\n\n\n<div class=\"content-ad\"></div>\n\n이제 이전에 다운로드한 .PLY 데이터 세트를 가져올 수 있습니다. 파일을 클릭하여 가져오세요. 가져온 후 1500만 점의 포인트 클라우드를 로드하는 데 몇 초만 소요됨을 알 수 있습니다. 세부 사항을 더 잘 확인하려면 휠을 사용하여 회전 및 확대할 수 있습니다.\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*jsj7jy6NgZqnQ9eCFdSlqQ.gif\" />\n\n좋아요, 이 단계에서는 블렌더 내에 포인트 클라우드가 있습니다. 그런데 이제 어떻게 해야 하죠? 블렌더에서 3D 포인트 클라우드를 가장 잘 사용할 수 있는 템플릿 레이아웃을 제공해 드리겠습니다.\n\n# 단계 4. 3D 데이터 시각화를 위한 블렌더 UI 설정\n\n<div class=\"content-ad\"></div>\n\n블렌더 UI를 초기 레이아웃보다 더 잘 정리해 봅시다. 여기서는 조금 기술적이니 충분한 집중력과 에너지가 필요할 거에요!\n\n먼저 Geometry Node Editor를 위로 옮길 거예요. 그런 다음 화면을 두 부분으로 분할할 거에요. 이를 위해 왼쪽 모퉁이에 있는 작은 십자를 클릭하고 드래그할 거에요. 오른쪽에는 우리의 geometries에 재질을 적용하는 Shader Editor를 열거에요. 이미 Properties 메뉴가 열려 있으니 그대로 남겨둘 거에요. 왼쪽에 화면을 늘려 Text Editor와 Python Console을 열 거에요. 이렇게 하면 필요한 모든 것을 손끝에 두게 될 거에요.\n\n🦊 Florent: 그렇습니다! 잘 읽으셨어요! 우리는 Blender 내에서 Python을 사용할 수 있어요! 정말 멋진 기능이죠. 이는 이 튜토리얼의 범위를 넓혀주며 다음 집중 에피소드에서 다룰 예정이에요.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*B2Bde5Z9sSLC4oojCbjkgg.gif)\n\n<div class=\"content-ad\"></div>\n\n그럼요, 이제 시작할 준비가 모두 완료되었습니다. 우리는 포인트 클라우드를 성공적으로 로드했지만 모두 검은 색으로 보입니다. 그러나 걱정하실 필요는 없어요. 곧 이 문제를 해결할 거에요. 중요한 점은 이제 카메라, 메시 및 광원이 포함된 씬 콜렉션을 갖고 있다는 것입니다.\n\n메시를 클릭하면 각 벡터의 위치 및 RGBA 또는 투명도 정보를 가진 16.3백만 개의 벡터를 포함하는 스프레드시트 뷰어에서 확인할 수 있습니다.\n\n현재 모든 포인트는 재질이 없기 때문에 메시가 검은색으로 보입니다(또는 선택한 경우 주황색으로 나타날 수 있습니다).\n\n우리의 목표는 이 메시 객체를 포인트 클라우드 객체로 변환하고 각 포인트의 색상을 읽는 것입니다. 이를 위해 지오메트리 노드를 생성할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n🦊Florent: 간단히 적어둡니다. Control+C 및 Control+V와 같은 키보드 바로 가기는 마우스 위치에만 작동합니다. 특정 영역에서 무언가를 복사하고 붙여넣기하려면 마우스가 해당 영역에 있는지 확인하세요.\n\n# 단계 5. 3D 도형을 위한 Geometry 노드\n\nBlender의 Geometry 노드는 사용자가 3D 지오메트리를 절차적으로 생성하고 조작할 수 있도록 2.93 버전에서 소개된 강력한 기능입니다. 이는 개별 정점, 에지 또는 면을 수동으로 조작하지 않고도 복잡한 3D 지오메트리를 생성, 수정 및 애니메이션화할 수 있다는 것을 의미합니다. 실제로 노드 기반 절차적 워크플로우 덕분에 전통적인 모델링 기술 대신 시각적 인터페이스를 사용하여 지오메트리를 생성하고 편집할 수 있습니다. 이제 이것을 3D 포인트 클라우드로 시험해보겠습니다.\n\n먼저 메시를 선택한 후 Geometry 노드 창으로 이동해야 합니다. 거기서 \"New\"를 클릭하여 지오메트리 노드를 생성합니다. 이를 간단히 설명하겠습니다. 그것은 geometry라는 두 개의 그룹 입력과 출력으로 구성됩니다.\n\n<div class=\"content-ad\"></div>\n\n\"지오메트리를 수정하기 위해 \"메쉬를 포인트로\" 노드를 추가할 거에요. 추가하면 노드가 자동으로 연결되어, 마법같은 일이 일어날 거예요. 모든 포인트에는 공 모양의 지오메트리가 부착되고, 각 공의 반지름을 0.01 (1센티미터)로 설정하여 수정할 수 있어요. 보시다시피, 모든 포인트가 노드에 표시돼 있어요.\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*7Ycq0Rlt2b1TT712hE7xtA.gif\" />\n\n정말 멋지죠, 하지만 한 가지 문제가 있어요 - 이 모드에서는 색상을 볼 수 없어요. 걱정하지 마세요, 이건 정상이에요! 블렌더에는 쉐이딩, 솔리드, 소재 미리보기, 그리고 렌더링 표시 미리보기와 같은 다양한 디스플레이 모드가 있어요. 우리는 와이어프레임이 없기 때문에 (메쉬가 아니기 때문에), 거기를 클릭하면 벡터(포인트)만 표시될 거예요. 우리가 솔리드 렌더링을 보려면 적절한 디스플레이 모드를 선택해야 해요.\n\n<img src=\"/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_6.png\" />\"\n\n<div class=\"content-ad\"></div>\n\n렌더링된 디스플레이 미리보기는 재질을 설정한 후에 작동합니다. 하지만 우리는 아직 재질을 하나도 가지고 있지 않기 때문에 작동하지 않습니다. 따라서 논리적으로 다음 단계는 각 포인트에 재질을 부착하는 것입니다.\n\n# 단계 6. 색상을 위한 쉐이더 노드\n\nGeometry Nodes에 해당하는 것으로 Shader Nodes가 있습니다. 이들은 Blender의 재료 시스템의 기본 구성 요소입니다. 서로 다른 노드를 연결함으로써 복잡한 재료를 만들 수 있습니다. 각 노드는 재료의 외관을 나타내는 특정 측면을 대표합니다.\n\n재질은 물체가 빛과 상호 작용하는 방식을 결정하여 외관을 부여합니다. Shader Nodes는 Shader Editor에서 편집됩니다. 이는 Blender 내에서 전용 워크스페이스로, 노드를 사용하여 재료를 만들고 편집하는 시각적 인터페이스를 제공합니다. 이제 시작해보죠.\n\n<div class=\"content-ad\"></div>\n\nShader Editor에서 \"새로운 재질\"을 만듭니다. \"새로 만들기\" 버튼을 클릭하여 'material.001'이라는 이름의 재질을 만들 수 있습니다. 이 재질에는 주요 BSDF와 재질 출력이 포함되어 있습니다. 그러나 사용하기 위해서는 몇 가지 속성을 추가해야 합니다. 속성을 추가하기 위해 \"추가\" 버튼을 클릭하고 \"속성\" 옵션을 찾습니다. 그런 다음 컬러 매개변수를 선택하고 이를 우리의 주요 BSDF의 베이스 컬러 매개변수에 연결합니다.\n\n🦊 Florent: BSDF는 양방향 산란 분포 함수를 나타냅니다. 결국에는 특정한 광선이 주어진 각도에서 반사(산란)될 확률을 결정하는 수학적인 함수입니다.\n\n다음 단계는 색상에 대한 올바른 속성을 읽고 설정하는 것입니다. 스프레드시트 뷰에서는 더 이상 메쉬가 아닌 \"Col\" 속성이 있는 포인트 클라우드 유형으로 나타납니다.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_7.png\" />\n\n따라서 쉐이더 편집기 필드에서 그 이름(\"Col\")을 사용해야 합니다. 이 단계 이후에는 아무 변화가 없는 것처럼 보일 것입니다. 이는 우리가 추가할 두 가지 요소(포인트 클라우드에 재료 설정 및 렌더링 엔진 설정)가 아직 남아 있기 때문에 예상된 동작입니다. 재료 설정부터 시작해봅시다.\n\n## 포인트 클라우드 지오메트리 노드에 재료 추가하기\n\n우리의 지오메트리에 새 재료를 부착하려면, \"Set material\"이라는 새로운 노드를 지오메트리 노드에서 만들어야 합니다. 이 과정은 간단합니다. 노드를 가운데에 놓으면 자동으로 간격을 맞출 것입니다. 그러고나서 드롭다운에서 재료를 선택하면 끝입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*Ph-9AagMnvT_HvdXcdv0wQ.gif)\n\n이게 전부에요. 이제 두 번째 문제를 해결하는 데로 넘어가 봅시다: 3D 포인트 클라우드를 위한 적절한 렌더링 엔진을 설정하세요.\n\n## 3D 포인트 클라우드를 지원하는 렌더링 엔진 설정\n\n해당 단계를 완료했다면 여전히 화면에 아무것도 보이지 않아서 답답할 수 있습니다. 포인트 클라우드를 다루고 있기 때문에 특정 유형의 후처리 렌더러 인 Cycles가 필요합니다.\n\n\n<div class=\"content-ad\"></div>\n\n🦊 Florent: 싸이클(Cycles)은 Blender에서 사용되는 렌더링 엔진으로 현실적인 이미지를 생성하는 데 사용됩니다. 이는 빛의 행동을 시뮬레이션하는 패스 추적 렌더러로, 반사, 굴절, 그리고 전체 조명 같은 복잡한 효과를 가능하게 합니다. 싸이클은 고품질 출력으로 유명하며 Blender에서 사실적인 장면을 만드는 데 널리 사용됩니다. 저는 정말 좋아해요.\n\n싸이클로 전환하려면 속성 영역으로 이동하고 렌더 탭을 클릭하세요. 기본 렌더 엔진은 Eevee라고 불리지만, 이를 싸이클로 변경해야 합니다. 한 번 변경하면 다른 설정을 수정할 필요는 없지만, 과도한 렌더링 시간을 피하기 위해 30초의 시간 제한 설정을 권장합니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*2YlfnGW6IYTLs2C07j8fGQ.gif)\n\n이제 렌더러를 설정했으므로 최종 결과물을 보기 위해 미리보기 렌더링을 클릭할 수 있습니다. 결과가 마음에 들지 않는다면 재미있는 상황이네요. 하하, 그것은 함정이었어요!\n\n<div class=\"content-ad\"></div>\n\n시각적으로 조명이 필요합니다! 재료가 빛에 반응하는 효과를 얻기 위해 장면에 조명을 추가해야 합니다. 준비되셨나요?\n\n# 단계 7. 3D 장면 조명 및 렌더링 설정\n\nBlender에서 조명은 장면을 현실적으로 조명하기 위해 조명 소스를 배치하고 구성하는 것을 의미합니다. 포인트, 스폿, 선, 영역, 및 방출 소재와 같은 다양한 유형의 조명을 시뮬레이션할 수 있습니다. 뿐만 아니라, 조명의 세기, 색상, 감쇠 및 그림자 속성을 조절할 수도 있습니다. 현실 세계에서 빛이 어떻게 작용하는지 이해하고 이러한 설정을 실험하는 것은 Blender에서 강렬한 시각적 결과를 얻는 데 중요합니다. 그러므로 저는 장면에서 다양한 기하학 및 조명 소스를 다루는 데 도움을 드리겠습니다.\n\n먼저, 조명을 선택하고 움직이기 아이콘을 클릭하여 원하는 위치로 정확히 이동시킵니다. 이렇게 하면 모든 것이 부드럽고 더 현실적으로 보이게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*_CNOkMDpuqilMNLkjkKJcQ.gif)\n\n🦊 Florent: 신이 가려진 장면의 경우에도 빛을 배치하여 더 아름답게 보이게 할 수 있어요. 우리는 조명을 베이크된 색상 위치에 배치함으로써 탁월한 사실적인 효과를 얻을 거에요.\n\n빛을 배치한 후, 나는 특정 각도에서 장면을 보기 위해 뷰포트 렌더링을 클릭해요. 그리고 이제.... 타다! 전체 장면이 렌더링되어 놀라운 것처럼 보여요.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*_RuwKTykijg3v-VhZ4uISQ.gif)\n\n\n<div class=\"content-ad\"></div>\n\n거기서는 이미 하얀색인 점들에 대한 빛 효과를 수정할 수 있어요. 빛을 움직이면 실시간으로 변화를 관찰할 수 있어요.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*V3x1j0z6cu7gequS7LUYpg.gif)\n\n이 흥미진진한 기능은 3D 포인트 클라우드 데이터를 메싱 단계를 거치지 않고 직접 사용할 수 있게 해줘요. 노력을 많이 들이지 않고 일관된 시간 프레임 내에서 기본으로 사용할 수 있어요. 이 마일스톤을 달성하여 축하드려요! 다음 단계는 아직 남아 있어요.\n\n# 단계 8. 스토리보드 정의\n\n<div class=\"content-ad\"></div>\n\n좋아요, 이제 무거운 주제에 들어가 봅시다. 이 새로운 기술 세트를 구체적인 응용 프로그램에 사용해 보겠습니다: 추출 경로 계획.\n\n![이미지](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_8.png)\n\n여기가 간략한 설명입니다. 산업 연구원이 알려준 독특한 유물을 회수하기 위해 추출팀을 이끄는 중입니다. 이 고대 유물은 양 제조 공정에 대한 새로운 비밀을 밝히는 데 중요하며, 전 세계 산업 공정을 뒤집을 수 있는 가능성이 있습니다. 문제는 해당 사이트가 매우 오염되어 있으며 목표물을 회수할 시간이 60초 밖에 없다는 것입니다.\n\n당신의 연락망 덕분에 그곳의 3D 스캔 자료를 손에 넣었고, 이제 추출의 성공을 보장하기 위한 최적의 네비게이션 지도를 작성할 차례입니다. 이를 달성하기 위해 최초로 추출 계획에 포함해야 할 다섯 가지 주요 포인트를 포함한 명세서를 설정했어요:\n\n<div class=\"content-ad\"></div>\n\n- 초기 포인트 클라우드의 다양한 (3-5) 관점 뷰\n- 관심 대상의 다양한 돋보인 객체로 아티팩트의 상대적인 공간 안에 잘 배치\n- 객체를 추출하기 위한 정확한 \"강조\" (원뿔)\n- 추출 경로 정의, 위쪽에서 본 보기\n- 추출 경로의 다양한 시점을 퍼스트-퍼슨으로 본 뷰\n\n데이터 시각화 전문가로서, 자크 베르탱의 작업을 회상하고 시각 변수를 활용하여 명확한 커뮤니케이션 지원을 달성하는 데 최선을 다해보세요.\n\n# 단계 9. 3D 씬 추출 경로 계획\n\n프로세스를 시작하기 전에, 제 Step 2 (포인트 클라우드 사전 처리)에서 분해된 하위 요소들을 다운로드하는 것을 권장합니다. 그런 다음, 모든 포인트 클라우드를 로드하고 위에서 언급한 단계를 따라 각 포인트 클라우드를 다음과 같이 얻을 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_9.png\" />\n\n🦊Florent: 작업 속도를 높이기 위해 몇 가지 권장 사항을 안내해드릴게요. 먼저, 포인트 클라우드를 가져오기 전에 Scene Collection 메뉴에서 새 컬렉션을 만드세요 (우클릭 `새 컬렉션`), 그 안에 모든 포인트 클라우드를 끌어다 놓을 거에요. 가져오고 나면 전체 포인트 클라우드를 선택한 후 Geometry Node Editor에서 Geometry Nodes를 복사하세요. 그런 다음 로드된 모든 포인트 클라우드를 순차적으로 선택하여 새 Geometry Node를 만들고 미리 채워진 것을 지우고 \"템플릿\"을 붙여 넣으세요. 이를 모든 포인트 클라우드에 대해 반복하면 준비 완료입니다.\n\n준비되셨으면 명세서 항목 목록을 진행할 수 있어요\n\n## 1. 포인트 클라우드 렌더링\n\n<div class=\"content-ad\"></div>\n\n다양한 관점을 생성하기 위해서는 장면을 탐색하여 교차점을 명확히 보여줄 수 있는 최적의 장소를 찾아야 합니다. 예를 들어 먼저 아래에 표시된 대로 다양한 장소에 조명을 추가해야 할 수도 있습니다.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*G1CAE69MWwZslrCOVymaUA.gif)\n\n이 작업을 완료하고 만족스러운 시점을 찾았다면 현재 시점을 카메라 위치로 사용하는 방식으로 Ctrl + Shift + 0을 눌러 카메라를 배치해야 합니다.\n\n🧙‍♂️ 전문가: 카메라 설정을 조정하고 싶다면 초점 거리(Focal Length)를 변경하여 할 수 있습니다. 아래 그림에서처럼 저는 25mm 초점 거리를 사용했습니다. 또한 점의 반경도 독립적으로 조절할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n거기서 Render Image 탭 버튼을 누르면 현재 카메라 위치에서 이미지를 내보낼 수 있습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*9afTGK9vyFB7zGJ1lxhU2w.gif)\n\n좋아요, 이제 우리는 주변 상황의 주요 아이디어를 갖고 있어요. 가스로 가득 찼어요! ☣️\n\n## 2. 관심 대상\n\n<div class=\"content-ad\"></div>\n\n목표는 테이블 위의 의자, 가스 탱크, 양털 기계 및 목표물을 강조하여 공간과 목표를 보다 명확하게 전달하는 것입니다. 자크 베르탱의 작업을 따라 가능한 조정할 수 있는 매개변수를 조정할 수 있습니다. 저는 각 개별 객체에 부여한 기본 색상으로 새로운 소재를 만들었습니다:\n\n![각 객체에 대해 이렇게 함으로써 우리는 시선을 더 잘 이끄는 렌더를 생성할 수 있습니다:](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_10.png)\n\n각 개체에 대해 이렇게 하면 눈을 더 정확히 이끌 수 있는 렌더를 생성할 수 있습니다:\n\n![각 객체에 대해 이렇게 함으로써 우리 눈을 더 잘 이끌 수 있는 렌더를 생성할 수 있습니다:](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_11.png)\n\n<div class=\"content-ad\"></div>\n\n🌱 성장 중: 전달된 메시지를 어떻게 개선할 수 있을까요? 변수 조정(색상)의 선택이 타당하다고 생생하십니까?\n\n## 3. 추출 대상 강조\n\n멋지네요! 여기서부터, 관심 대상에 더 많은 강조를 더하고 싶어합니다. 다시 말씀드리지만, 가능성은 많습니다. 저희는 눈에 뷰 가이드 역할을 하는 3D 원뿔 메시를 사용하는 방법을 안내해 드리겠습니다.\n\n우선, 원뿔 메시 오브젝트를 생성해야 합니다. '메쉬' - '원뿔 추가' 옵션으로 이동하여 씬의 중심에 원뿔 메시를 생성할 수 있습니다. 그 다음, 해당 원뿔을 선택한 후, 크기를 조절하고 왼쪽에 있는 버튼을 사용하여 회전시키십시오. 마지막으로, 아래에 표시된 대로 목표물 위에 오브젝트의 위치를 변경하고 재료를 부여하면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](https://miro.medium.com/v2/resize:fit:1400/1*73JNEOXDwlsNfg2M6xqIwg.gif)\n\nFrom these steps, and after creating a render from a camera position, you should get something looking like this:\n\nBeautiful! It's time to get onto the extraction route.\n\n## 4. Top-down Extraction Route\n\n\n<div class=\"content-ad\"></div>\n\nGrease Pencil을 사용하면 마우스로 그림을 그릴 수 있어요. 올바르게 사용하려면 먼저 `Grease Pencil`을 추가해야 해요. 그 다음 \"Top View\"로 이동해서 \"Draw Mode\"를 눌러 경로를 그릴 수 있어요. 경로를 그린 후에는 Object Mode로 돌아가서 그리스 경로를 적절한 위치로 이동시켜야 해요. 아래 이미지를 참고해주세요.\n\n![그림1](https://miro.medium.com/v2/resize:fit:1400/1*HYgQi-DuB38Piisdo1Y9IQ.gif)\n\n마지막 단계는 경로에 색상을 추가하기 위해 그리스에 텍스처를 추가하는 거에요:\n\n![그림2](https://miro.medium.com/v2/resize:fit:1400/1*mE_t1JPB6wJvlHk8ECXSzQ.gif)\n\n<div class=\"content-ad\"></div>\n\n여기 있습니다. 우리는 추출 경로의 매력적인 전경을 얻기 위해 렌더링을 실행합니다!\n\n![image](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_12.png)\n\n## 5. POV 추출 경로\n\n마지막 단계는 경로의 일인칭 시점을 얻는 것입니다. 만약 직접 렌더링을 한다면, 다음과 같은 결과물이 나올 것입니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_13.png\" />\n\n사실 그리스 펜슬에는 깊이 테스트가 없습니다. 여기서 또 다른 꿀팁을 알려드릴게요. Z-깊이 테스트를 활성화하려면 ` 에디터 패널 ` 뷰 레이어 속성 ` 패스 -` 데이터로 이동하여 Z를 활성화하세요.\n\n<img src=\"/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_14.png\" />\n\n이렇게 하면 렌더를 다시 생성할 때 가려지는 부분 테스트를 통과할 수 있어서 포인트 클라우드를 처리하는 데 훌륭합니다!\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n이것은 계속해서 발전하는 여정이었습니다! 우리는 블렌더에서 3D 포인트 클라우드를 통합하고 처리하는 과정을 분석했습니다. 포인트 클라우드 데이터의 가져오기와 내보내기부터 씬 설정과 렌더링의 복잡성까지 세심하게 다루며, 블렌더의 기능의 복잡성을 탐험하고 3D 시각화와 렌더링의 전체 잠재력을 활용할 수 있게 되었습니다.\n\n지금까지 따라오면서, 블렌더에서 포인트 클라우드 데이터를 다루는 프로젝트에 자신감을 갖고 해결할 수 있는 지식과 기술을 얻었습니다. 이를 통해 3D 작업에서 창의성과 정밀도에 대한 새로운 가능성을 열 수 있게 되었습니다.\n\n🦊 플로랑: 블렌더의 강력한 도구와 새로운 전문 지식을 바탕으로, 몰입감 있고 고품질의 실내 시각화를 위한 가능성이 이제 여러분 손안에 있습니다. 그래서 이 안내서를 들고 자신감 있게 여러분의 다음 3D 여정에 돌입해보세요. 원시 포인트 데이터를 멋진 시각적 표현으로 변환할 수 있다는 것을 인지하며, 행복한 블렌딩이 되길 바랍니다!\n\n<div class=\"content-ad\"></div>\n\n# 참고 자료\n\n- Poux, Florent, Valembois, Q., Mattes, C., Kobbelt, L., & Billen, R. (2020). Initial user-centered design of a virtual reality heritage system: Applications for digital tourism. Remote Sensing, 12(16), 2583. [DOI](https://doi.org/10.3390/rs12162583)\n- Poux, Florent, Neuville, R., Van Wersch, L., Nys, G. A., Billen, R., Van Wersch, L., … & Billen, R. (2017). 3D Point Clouds in Archaeology: Advances in Acquisition. Processing and Knowledge Integration Applied to Quasi-Planar Objects, 96. [DOI](https://doi.org/10.3390/geosciences7040096)\n\n![이미지](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_15.png)\n\n# 🔷기타 자료\n\n<div class=\"content-ad\"></div>\n\n- 🍇 데이터에 액세스하려면 여기를 방문하세요: 3D 데이터셋\n- 👨‍🏫 3D 온라인 데이터 과학 코스: 3D 아카데미\n- 📖 3D 자습서의 초기 액세스를 위해 구독하세요: 3D AI 자동화\n- 🧑‍🎓 석사 학위 취득: ITC Utwente\n\n# 🎓작가의 추천\n\n데이터 획득부터 가상 투어 생성까지 엔드 투 엔드 시스템을 구축하려면, 여기서 제공된 이전에 게시된 기사를 살펴보십시오. 데이터 처리에 사용되는 방법을 보여주는 것도 포함돼 있습니다. 즐거운 기술 습득되길 바랍니다!","ogImage":{"url":"/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_0.png"},"coverImage":"/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_0.png","tag":["Tech"],"readingTime":19},{"title":"위성 열 영상을 1000m에서 10m로 다운스케일링하는 방법 Python","description":"","date":"2024-06-22 16:48","slug":"2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython","content":"\n\n![image](/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_0.png)\n\n# 목차\n\n- 🌅 소개\n- 💾 Sentinel-3 (1000 m) 및 Sentinel-2 이미지 다운로드\n- ⚙️ Sentinel-3 이미지 처리\n- 🌡️ 온도-NDVI 공간\n- 📐 열화상 이미지 선명화 (1000 m에서 10 m)\n- 🗺️ 선명화된 열화상 이미지 시각화\n- 📄 결론\n- 📚 참고 자료\n\n## 🌅 소개\n\n<div class=\"content-ad\"></div>\n\n위성에서 촬영한 열화상 이미지를 축소하는 연구는 열화상 이미지를 제공하는 위성들의 공간 및 시간 해상도 사이의 상충 관계 때문에 광범위하게 연구되었습니다. 예를 들어, Landsat-8의 재방문 주기는 16일이며, 원래 열 해상도는 100미터입니다. 반면에 Sentinel-3은 매일 열화상 이미지를 제공할 수 있지만, 공간 해상도는 1000미터입니다.\n\n![이미지](/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_1.png)\n\n열화상 이미지의 굵은 해상도를 해결하는 한 가지 방법은 NASA의 Landsat-9와 같은 열 센서가 장착된 추가 위성을 발사하는 것일 수 있습니다. Landsat-9의 경우, Landsat-8과 Landsat-9의 임시 해상도는 8일입니다 (하나의 위성보다는 16일), 맑은 하늘을 전제로 할 때.\n\n그러나 이 접근 방식은 수십억 달러의 투자와 몇 년의 노력이 필요합니다. 대신, 연구자들은 통계적 방법에 집중하여, 공간 해상도는 높지만 임시 해상도는 낮은 위성의 시정/근적외선 (VNIR) 밴드를 열화상 이미지와 상관시킴으로써 열화상 이미지의 낮은 공간 해상도 (하지만 높은 임시 해상도)와 연관시키는데 초점을 맞추었습니다. 예를 들어, 연구들은 Sentinel-2의 VNIR 밴드로부터 계산된 정규화 된 차이 채취 지수 (NDVI)가 Sentinel-3의 열화상 이미지와 역 상관 관계가 있음을 보여주었습니다.\n\n<div class=\"content-ad\"></div>\n\n답변을 요약하면 Sentinel-2의 NDVI와 Sentinel-3의 열화상 이미지 간의 상관 관계가 충분히 강하다면, 해당 방정식을 10m 해상도로 조정하여 10m 해상도의 열화상 이미지를 생성할 수 있습니다.\n\n위성 대역과 센서 스펙트럼에 대해 자세히 알고 싶다면 다음을 참조하십시오:\n\n이 게시물에서는 Sentinel-3로부터 낮은 공간 해상도 열화상 이미지와 Sentinel-2로부터 높은 공간 해상도 VNIR 이미지를 다운로드할 것입니다. 이 두 이미지는 각각의 위성에 의해 동시에 촬영되었습니다. 그런 다음, VNIR 대역을 사용하여 NDVI를 계산하고((NIR-Red)/(NIR+Red)), 이를 1000m로 업스케일하고 NDVI와 열대 대역 간의 상관 관계(둘 다 1000m 해상도)를 탐색할 것입니다. 마지막으로, 이 상관 관계를 사용하여 10m 해상도의 온도 지도를 생성할 것입니다.\n\n## 💾 Sentinel-3(1000 m) 및 Sentinel-2 이미지(10 m) 다운로드\n\n<div class=\"content-ad\"></div>\n\n이미 R 및 Python에서 Sentinel-2 이미지를 다운로드하는 방법에 대해 세 번의 게시물을 작성했습니다. 또한 Python에서 Sentinel-3 이미지를 다운로드하는 방법에 대한 게시물도 있습니다. 이곳에서는 해당 단계들을 반복하고 싶지 않아서 이 게시물을 참조하시기 바랍니다:\n\nR에서 Sentinel-2 이미지 다운로드:\n\nPython에서 Sentinel-2 이미지 다운로드:\n\nPython에서 Sentinel-3 이미지 다운로드:\n\n<div class=\"content-ad\"></div>\n\n만약 코드를 작성하지 않고 이미지를 다운로드하고 싶지 않다면, 다음 게시물을 확인해보세요:\n\n열화상 이미지를 축소화하기 위한 통계적 방법을 적용하는 중요한 단계는 위성에서 동시에 촬영된 선명한 이미지를 찾는 것입니다. 이미지를 다운로드하기 전에 날짜, 구름 양, 그리고 귀하의 관심 지역(AOI)에 기반하여 메타데이터를 필터링할 수 있습니다. Sentinel 메타데이터(관심 지역, 구름 양 등)를 필터링하고 처리하는 방법을 더 알고 싶다면 다음을 참조해보세요:\n\n이 게시물에서, 제 관심 지역(AOI)는 캘리포니아에 위치하며, 2023년 6월 19일에 Sentinel-2 및 Sentinel-3로 촬영된 선명한 이미지를 발견했습니다. 다른 위치나 날짜를 검색하고 싶다면 자유롭게 찾아보세요. 그러나 제가 다운로드한 이미지를 사용하길 원한다면, 다음 정보가 있습니다:\n\nSentinel-2: S2B_MSIL2A_20230620T183919_N0509_R070_T10SFG_20230620T224951\n\n<div class=\"content-ad\"></div>\n\n\nsatellite = “SENTINEL-2”\n\nlevel = “S2MSI2A”\n\nAOI = “POLYGON ((-121.0616 37.6391, -120.966 37.6391, -120.966 37.6987, -121.0616 37.6987, -121.0616 37.6391))”\n\nstart_date = “2023–06–19” ; end_date = “2023–06–21”\n\n\n<div class=\"content-ad\"></div>\n\n\nsatellite = “SENTINEL-3”\n\nlevel= “LST”\n\nAOI = “POLYGON ((-121.0616 37.6391, -120.966 37.6391, -120.966 37.6987, -121.0616 37.6987, -121.0616 37.6391))”\n\n\n<div class=\"content-ad\"></div>\n\nstart_date = “2023–06–19” ; end_date = “2023–06–21”\n\nSentinel-2에서 NIR 및 레드 밴드(NDVI를 계산하는 데 필요한 밴드)를 10m로 다운로드한 후 Sentinel-3에서 열화상 이미지를 다운로드하면 디렉토리에 이 세 파일이 있어야 합니다:\n\n![Image](/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_2.png)\n\n## ⚙️ Sentinel-3 이미지 처리\n\n<div class=\"content-ad\"></div>\n\nSentinel-3 이미지는 Sentinel-2보다 훨씬 넓은 장면을 커버합니다. 따라서 Sentinel-3 각 픽셀의 평균 NDVI 값을 필요로 하므로 Sentinel-3 이미지를 Sentinel-2 이미지의 범위에 따라 클리핑해야 합니다. 이를 위해 첫 번째 단계는 Sentinel-3 이미지를 Sentinel-2 이미지와 동일한 투영으로 재매핑하는 것입니다. 이를 수행하기 위해 다음과 같은 코드를 사용할 수 있습니다:\n\n```js\nimport numpy as np\nimport rasterio\nfrom rasterio.warp import calculate_default_transform, reproject, Resampling\nfrom pyproj import Transformer\n\ninput_raster = 'Sentinel-3_L2_LST_reproj.tif'\noutput_raster = 'Sentinel-3_L2_LST_reproj_32610.tif'\ndst_crs = 'EPSG:32610'\n\n# 입력 래스터 파일 읽기\nwith rasterio.open(input_raster) as src:\n    # 목적지 CRS에 대한 변환, 너비 및 높이 가져오기\n    transform, width, height = calculate_default_transform(src.crs, dst_crs, src.width, src.height, *src.bounds)\n\n    # 목적지 설정\n    kwargs = src.meta.copy()\n    kwargs.update({\n        'crs': dst_crs,\n        'transform': transform,\n        'width': width,\n        'height': height,\n        'dtype': np.float32,\n    })\n\n    # 목적지 생성 및 재매핑된 데이터 작성\n    with rasterio.open(output_raster, 'w', **kwargs) as dst:\n        # 재매핑 수행\n        for i in range(1, src.count + 1):\n            reproject(\n                source=src.read(1).astype(np.float32) * src.scales[0] + src.offsets[0],\n                destination=rasterio.band(dst, i),\n                src_transform=src.transform,\n                src_crs=src.crs,\n                dst_transform=transform,\n                dst_crs=dst_crs,\n                resampling=Resampling.bilinear)\n```\n\n이 스크립트에서는 이전 단계에서 다운로드한 Sentinel-3 래스터 이미지를 읽고, 지정된 CRS로 재매핑하기 위한 변환 매개변수를 계산하고, 재매핑된 데이터를 포함한 새로운 래스터 파일을 내보냅니다. 이러한 단계를 거치면 디렉토리에 다음 네 가지 파일이 있어야 합니다:\n\n![image](/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_3.png)\n\n\n<div class=\"content-ad\"></div>\n\nSentinel-3 열화상 이미지가 Sentinel-2 좌표 시스템으로 변경되었으므로 이제 Sentinel-2 데이터 범위를 기반으로 열화상 이미지를 잘라내는 작업을 할 수 있습니다. 아래 코드를 사용하여 작업할 수 있어요:\n\n```js\nimport rasterio\nimport numpy as np\n\n# 두 래스터 파일 열기\nwith rasterio.open('T10SFG_20230620T183919_B08_10m.jp2') as small_raster:\n    with rasterio.open('Sentinel-3_L2_LST_reproj_32610.tif') as big_raster:\n\n        # 더 작은 래스터의 범위 가져오기\n        min_x, min_y, max_x, max_y = small_raster.bounds\n\n        # 더 큰 래스터에서 더 작은 래스터의 범위 내의 데이터 읽기\n        window = rasterio.windows.from_bounds(min_x, min_y, max_x, max_y, big_raster.transform)\n        data = big_raster.read(window=window)\n\n        # 더 큰 래스터의 메타데이터 업데이트하여 더 작은 래스터의 범위와 일치시키기\n        clipped_meta = big_raster.meta.copy()\n        clipped_meta.update({\n            'height': window.height,\n            'width': window.width,\n            'transform': rasterio.windows.transform(window, big_raster.transform),\n            'dtype': data.dtype\n        })\n\n        # 잘라낸 데이터 쓰기\n        with rasterio.open('Sentinel-3_L2_LST_reproj_32610_clipped.tif', 'w', **clipped_meta) as clipped_raster:\n            clipped_raster.write(data)\n```\n\n이 스크립트에서는 두 래스터 파일(Sentinle-3 및 Sentinel-2 이미지)을 읽고, 더 작은 래스터의 범위(Sentinel-2)를 추출하고, 더 큰 래스터(Sentinle-3 열화상 이미지)에서 해당 데이터를 읽어와서 잘라낸 Sentinle-3의 메타데이터를 Sentinel-2 이미지와 일치하도록 업데이트한 후, 잘라낸 열화상 이미지를 새 TIFF 파일에 작성합니다.\n\n<img src=\"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_4.png\" />\n\n<div class=\"content-ad\"></div>\n\nNDVI 및 클리핑된 온도 맵을 옆으로 나란히 플롯해 봅시다:\n\n```js\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport rasterio\nfrom rasterio.plot import show\n\n# 파일 경로\nred_path = '/content/T10SFG_20230620T183919_B04_10m.jp2'\nnir_path = '/content/T10SFG_20230620T183919_B08_10m.jp2'\n\nclipped_temperature_path = '/content/Sentinel-3_L2_LST_reproj_32610_clipped.tif'\n\n# 래스터 데이터 읽기\nwith rasterio.open(red_path) as red_src:\n  red = red_src.read(1)\n\nwith rasterio.open(nir_path) as nir_src:\n  nir = nir_src.read(1)\n\nwith rasterio.open(clipped_temperature_path) as clipped_temp_ds:\n  clipped_temperature = clipped_temp_ds.read(1)\n\n# NDVI 계산\nndvi = (nir - red) / (nir + red)\n\n# NDVI와 온도를 옆으로 플롯\nfig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n\n# NDVI 플롯\nim1 = ax1.imshow(ndvi, cmap=ndvi_cmap, vmin=0, vmax=0.6)\nax1.set_title('NDVI', fontweight='bold', fontsize=14)\nfig.colorbar(im1, ax=ax1, shrink=0.5)\n\n# 클리핑된 온도 플롯\nim2= ax2.imshow(clipped_temperature, cmap=ndvi_cmap.reversed(), vmin=300, vmax=315)\nax2.set_title('Clipped Temperature', fontweight='bold', fontsize=14)\nfig.colorbar(im2, ax=ax2, shrink=0.5)\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_5.png\" />\n\n위와 같이 Sentinel-3 열화상 이미지를 Sentinel-2 맵의 범위에 맞게 성공적으로 클립했습니다. 그러나 Sentinel-2 이미지도 잘린 상태이며, 온도 픽셀의 평균 NDVI 값을 얻기 위해 존속 통계를 실행하면 많은 NaN 값을 얻게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_6.png)\n\n다음 단계에서 데이터프레임에서 NaN 값을 제외하여 수정할 것입니다.\n\n## 🌡️ 온도-NDVI 공간\n\n같은 투영 및 범위를 갖는 두 개의 명확한 이미지, 즉 센티넬-3에서의 열화상 이미지와 센티넬-2에서의 VNIR 이미지가 있을 때, 각 온도 픽셀의 평균 NDVI 값을 얻기 위해 존 채택 통계를 실행할 수 있습니다. 기본적으로 존 채택 통계 방식을 통해 10m에서 1000m로 NDVI 지도를 집계하여 온도-NDVI 공간에서 온도 값에 대한 NDVI 값을 플로팅할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n소개에서 언급했듯이 열값은 NDVI 값과는 역상관 관계에 있어야 합니다. 높은 NDVI는 식물의 비율이 더 높은 것을 나타내며 더 차가운 픽셀에 해당하고, 낮은 NDVI는 적은 식물이나 벌거벗은 토양에 해당하여 더 따뜻한 픽셀에 해당합니다. 우리의 AOI(관심 영역)에서 열과 NDVI 값 사이의 공간을 탐색하기 위해 존솔 통계를 수행해 보겠습니다:\n\n```js\nimport rasterio\nimport rasterio.features\nimport rasterio.mask\nimport pandas as pd\nimport geopandas as gpd\nimport rasterstats\n\nimport rasterio\nfrom rasterio.features import shapes\nmask = None\n\n# 입력 래스터 열기\nwith rasterio.open('Sentinel-3_L2_LST_reproj_32610_clipped.tif') as src:\n    # 래스터 밴드 읽기\n    image = src.read(1).astype(np.float32) * src.scales[0] + src.offsets[0]\n    results = (\n        {'properties': {'Temperature': v}, 'geometry': s}\n        for i, (s, v)\n        in enumerate(\n            shapes(image, mask=mask, transform=src.transform)))\n    geoms = list(results)\n    gpd_polygonized_raster = gpd.GeoDataFrame.from_features(geoms)\n\n# 래스터 열기\nwith rasterio.open('T10SFG_20230620T183919_B08_10m.jp2') as nir_src:\n    with rasterio.open('T10SFG_20230620T183919_B04_10m.jp2') as red_src:\n\n        # 데이터를 float32로 읽기\n        nir = nir_src.read(1).astype(np.float32) * nir_src.scales[0] + nir_src.offsets[0]\n        red = red_src.read(1).astype(np.float32) * red_src.scales[0] + red_src.offsets[0]\n\n        # NDVI 계산\n        ndvi = (nir - red) / (nir + red)\n\n        # 각 다각형에 대한 존솔 통계 계산\n        stats = rasterstats.zonal_stats(gpd_polygonized_raster.geometry, ndvi, affine=nir_src.transform, stats='mean')\n\n        # NDVI의 평균값을 데이터프레임에 추가\n        gpd_polygonized_raster['NDVI'] = [s['mean'] for s in stats]\n\n# 다각형 레이어를 shapefile로 저장\ngpd_polygonized_raster.to_file('output_polygons.shp')\n\n# geodataframe로부터 pandas 데이터프레임 생성\nstats_df = pd.DataFrame(gpd_polygonized_raster.drop(columns='geometry'))\n\n# 데이터프레임 출력\nprint(stats_df)\n```\n\n이 스크립트에서는 열 래스터를 여각화하고, Sentinel-2 이미지에서 NDVI를 계산하고, 각 온도 픽셀에 대한 존솔 통계(평균 NDVI)를 계산합니다.\n\n<div class=\"content-ad\"></div>\n\n위 표와 이전에 논의한 것처럼, Sentinel-3에서 열화 데이터를 가지고 있는 일부 픽셀에는 NaN 값이 있습니다. 이는 Sentinel-2 이미지의 자른 부분 때문입니다. Sentinel-2에서 NaN 값을 가진 행을 제외하겠습니다:\n\n```python\n# NaN 값이 있는 행 삭제\ndf_clean = stats_df.dropna(subset=['NDVI'])\ndf_clean\n```\n\n결과는 다음과 같습니다:\n\n<img src=\"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_8.png\" />\n\n<div class=\"content-ad\"></div>\n\n이 깔끔한 데이터프레임을 사용하여 온도-NDVI 공간을 그래프로 표현할 수 있어요:\n\n```js\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import linregress\n\n# 산점도 생성\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='NDVI', y='Temperature', data=df_clean, palette='coolwarm')\n\n# 그래프 제목 및 축 레이블 설정\nplt.title('NDVI 대 온도 그래프', fontsize=16, fontweight='bold')\nplt.xlabel('NDVI', fontsize=14)\nplt.ylabel('Temperature', fontsize=14)\n\n# 그래프 보여주기\nplt.show()\n```\n\n결과는 다음과 같아요:\n\n<img src=\"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_9.png\" />\n\n<div class=\"content-ad\"></div>\n\n위 그림에서 NDVI와 온도 사이의 역상관 관계를 관찰할 수 있지만 몇 가지 점이 이상치로 나타납니다. 이는 식물 및 맨 소토 외의 다른 특징을 나타내는 이미지의 픽셀 때문일 수 있습니다. 이러한 픽셀을 제거하기 위해 NDVI 값이 0.1에서 0.6 사이이고 온도 값이 300 켈빈에서 330 켈빈 사이인 값만 유지한 다음, 그에 맞게 그림을 업데이트하겠습니다:\n\n```js\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import linregress\n\n# 지정된 조건에 기반하여 데이터프레임 필터링\nfiltered_df = df_clean[(df_clean['NDVI'] >= 0.1) & (df_clean['NDVI'] <= 0.6) &\n                 (df_clean['Temperature'] >= 300) & (df_clean['Temperature'] <= 330)]\n\n# 온도 대 NDVI의 산점도 그리기\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='NDVI', y='Temperature', data=filtered_df, palette='coolwarm')\n\n# 선형 회귀 모델 적합\nslope, intercept = np.polyfit(filtered_df['NDVI'], filtered_df['Temperature'], 1)\n\n# 적합된 선 그리기\nx_line = np.linspace(min(filtered_df['NDVI']), max(filtered_df['NDVI']), 100)\ny_line = slope * x_line + intercept\nplt.plot(x_line, y_line, 'r', label='Fitted line')\n\n# 방정식 문자열 작성\nequation_str = f'y = {slope:.2f}x + {intercept:.2f}'\n\n# 그림에 방정식 표시\nplt.text(min(filtered_df['NDVI']), max(filtered_df['Temperature']), equation_str, fontsize=12, color='red')\n\n# 그림 제목과 축 레이블 설정\nplt.title('1-1. 온도 vs NDVI 그래프', fontsize=16, fontweight='bold')\nplt.xlabel('온도', fontsize=14)\nplt.ylabel('NDVI', fontsize=14)\n\n# 그림 표시\nplt.show()\n```\n\n결과는 다음과 같습니다:\n\n<img src=\"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_10.png\" />\n\n<div class=\"content-ad\"></div>\n\n이제는 역상관 관계가 더 잘 보이고 있으며, NDVI 값과 온도 간 관계를 설명하는 방정식도 나와 있습니다.\n\n## 📐 열화상 이미지 선명하게하기 (1000m에서 10m)\n\n이번 단계에서는 1000m 해상도에서 집계된 NDVI와 온도 간의 관계를 발견한 방정식이 10m 해상도에도 유효할 수 있다고 가정할 것입니다. 이 방정식을 원래의 NDVI 지도에 적용하여 10m 해상도에서 온도를 추정할 수 있습니다.\n\n```js\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# NIR 및 RED 파일 열기\nwith rasterio.open('T10SFG_20230620T183919_B08_10m.jp2') as src:\n    nir = src.read(1)\n    meta = src.meta\n\nwith rasterio.open('T10SFG_20230620T183919_B04_10m.jp2') as src:\n    red = src.read(1)\n\n# NDVI 계산\nndvi = (nir - red) / (nir + red)\n\n# NDVI를 사용하여 온도 추정\ntemp = -21.85 * ndvi + 314.9\n\n# NDVI를위한 컬러 램프 만들기\nndvi_cmap = plt.cm.RdYlGn\n\n# NDVI 및 온도를 나란히 그리기\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n\n# NDVI 그리기\nim1 = ax1.imshow(ndvi, cmap=ndvi_cmap, vmin=0, vmax=0.6)\nax1.set_title('NDVI', fontweight='bold', fontsize=14)\nfig.colorbar(im1, ax=ax1, shrink=0.7)\n\n# 온도 그리기\nim2 = ax2.imshow(temp, cmap=ndvi_cmap.reversed(), vmin=300, vmax=315)\nax2.set_title('Temperature', fontweight='bold', fontsize=14)\nfig.colorbar(im2, ax=ax2, shrink=0.7)\n\nplt.show()\n```\n\n<div class=\"content-ad\"></div>\n\nNIR 및 빨간색 대역을 읽어 NDVI를 계산하고 유도된 방정식에 기반하여 온도 값을 추정합니다. 다음으로, NDVI 및 온도에 대한 결과를 시각화하여 옆에 플롯을 표시합니다. 지도는 다음과 같습니다:\n\n![맵](/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_11.png)\n\n## 🗺️ 날카로워진 열 영상의 시각화\n\n이 섹션에서는 시각화 측면에 더욱 집중할 것입니다. 우리는 이미지의 중앙 영역을 확대하고, 이미지를 중앙을 기준으로 자릅니다. 또한 온도 차트를 제시하기 위해 Sentinel-2의 NDVI 지도, 원본 Sentinel-3 열 영상(1000m 해상도) 및 분석 결과를 기반으로한 날카로운 Sentinel-3 영상(10m 해상도)을 비교 및 자세한 검토를 위해 옆에 함께 제시할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```python\n# NDVI 및 온도를 옆으로 나란히 플로팅합니다\nfig, axs = plt.subplots(ncols=3, figsize=(15, 5))\n\n# NDVI 플로팅\nvmin, vmax = 0, 0.6\nndvi_subset = ndvi[int(0.75 * ndvi.shape[0]):, int(0.75 * ndvi.shape[1]):]\nim1 = axs[0].imshow(ndvi_subset, cmap=ndvi_cmap, vmin=vmin, vmax=vmax)\naxs[0].set_title('NDVI', fontweight='bold', fontsize=14)\naxs[0].set_xticks([])\naxs[0].set_yticks([])\nfig.colorbar(im1, ax=axs[0], shrink=0.7)\n\n# 온도 플로팅\nwith rasterio.open('Sentinel-3_L2_LST_reproj_32610_clipped.tif') as src:\n    original_temp = src.read(1)\n\nvmin, vmax = 300, 315\ntemp_subset = original_temp[int(0.75 * original_temp.shape[0]):, int(0.75 * original_temp.shape[1]):]\nim3 = axs[1].imshow(temp_subset, cmap=ndvi_cmap.reversed(), vmin=vmin, vmax=vmax)\naxs[1].set_title('Temperature', fontweight='bold', fontsize=14)\naxs[1].set_xticks([])\naxs[1].set_yticks([])\nfig.colorbar(im3, ax=axs[1], shrink=0.7)\n\nvmin, vmax = 300, 315\ntemp_subset = temp[int(0.75 * temp.shape[0]):, int(0.75 * temp.shape[1]):]\nim2 = axs[2].imshow(temp_subset, cmap=ndvi_cmap.reversed(), vmin=vmin, vmax=vmax)\naxs[2].set_title('Temperature', fontweight='bold', fontsize=14)\naxs[2].set_xticks([])\naxs[2].set_yticks([])\nfig.colorbar(im2, ax=axs[2], shrink=0.7)\n\n# 서브플롯 간 간격 조정\nfig.subplots_adjust(wspace=0.2)\n\nplt.show()\r\n```\n\n지도는:\n\n<img src=\"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_12.png\" />\n\n## 📄 결론\n\n\n<div class=\"content-ad\"></div>\n\nVisible과 Near-Infrared (VNIR) 대역과 열 이미지 간의 직접적 상관 관계는 열 이미지의 해상도를 향상시키는 유용한 방법으로 입증되었습니다. 이 기술은 위성의 적절한 공간 해상도를 가진 위성이 없을 때 온도를 높은 공간 해상도로 추정하는 데 실용적으로 활용됩니다. 이 다운스케일링 방법은 고해상도 열지도가 필요할 때 유용한 도구로 작용하며 소규모 온도 변화에 대한 세부 정보를 제공합니다. 앞으로 더 많은 고급 열 센서를 갖춘 위성을 발사함에 따라 빈도가 더 높은 고해상도 열 이미지를 얻을 수 있게 될 것입니다. 그 전까지는 이 방법이 더 높은 해상도의 열 이미지를 구현하는 비용 효율적인 선택지로 남아 있습니다.\n\n## 📚 참고 자료\n\nCopernicus 센티넬 데이터 [2024] - 센티넬 데이터에 대한 정보\n\nCopernicus 서비스 정보 [2024] - Copernicus 서비스 정보에 관한 정보\n\n<div class=\"content-ad\"></div>\n\n아감, N., 쿠스타스, W. P., 앤더슨, M. C., 리, F., 닐, C. M. U. (2007). 열화상 이미지 공간 개선을 위한 식물 지수 기반 기술. Remote Sensing of Environment, 107(4), 545–558. ISSN 0034–4257.\n\n가오, F., 쿠스타스, W. P., 앤더슨, M. C. (2012). 육지 위의 열화상 위성 이미지 개선을 위한 데이터 마이닝 접근 방식. Remote Sensing, 4, 3287–3319.\n\n휴리나, H., 코헨, Y., 카르니엘리, A., 파노프, N., 쿠스타스, W. P., 아감, N. (2019). Sentinel-3 위성 이미지의 열화상 개선을 위한 TsHARP 유틸리티 평가. Remote Sensing, 11, 2304.\n\n📱 저와 더 다양한 콘텐츠를 공유하려면 다른 플랫폼에서 연락하세요! LinkedIn, ResearchGate, Github 및 Twitter.\n\n<div class=\"content-ad\"></div>\n\n여기 해당 링크를 통해 제공되는 관련 게시물들이 있습니다:","ogImage":{"url":"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_0.png"},"coverImage":"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_0.png","tag":["Tech"],"readingTime":17},{"title":"경기 결과 비교 에이지 그레이딩, 퍼센타일, Z-스코어 세 가지 방법","description":"","date":"2024-06-22 16:46","slug":"2024-06-22-AgeGradingPercentilesandZ-ScoresThreeWaystoCompareRaceResults","content":"\n\n\n![Image](/assets/img/2024-06-22-AgeGradingPercentilesandZ-ScoresThreeWaystoCompareRaceResults_0.png)\n\n다른 연령 그룹의 두 러너간의 경주 결과를 효과적이고 공정하게 비교하는 방법은 무엇일까요?\n\n이 질문에 대해 계속해서 다루고 있는 시리즈 기사들 중 하나입니다.\n\n현재 시스템인 연령 등급은 유용하지만 일부 결함이 있습니다. 많은 데이터를 수집하고 분석한 후, 연령 등급의 두 대체안인 백분위 및 z-점수를 제안했습니다.\n\n\n<div class=\"content-ad\"></div>\n\n오늘은 한 걸음 물러나서 이 세 가지 다른 방법을 비교해 보고 싶었어요. 특히 연령 측정의 한 가지 문제는 어떤 그룹에 대해 다른 그룹보다 더 호의적으로 치우치는 것 같다는 점이에요. 대안들도 같은 문제를 가지고 있나요?\n\n또한 달리기 선수의 연령, 성별 및 시간을 입력할 수 있는 계산기를 설정했어요. 이를 통해 모든 세 개의 점수를 얻어 비교할 수 있어요.\n\n하지만 먼저, 지금까지 무엇을 살펴봤는지 간단히 되짚어볼까 해요. 이 주제에 대해 읽은 첫 번째 문서라면 여기서 확인하세요. 처음부터 따라오고 있다면 다음 섹션으로 스킵해도 돼요.\n\n# 이전의 연령 측정에 관한 이전 기사들 재고하기\n\n<div class=\"content-ad\"></div>\n\n이 시리즈는 한 가지 질문으로 시작했어요 — 서로 다른 연령 그룹과 성별 간의 경주 결과를 어떻게 효과적으로 비교할까요?\n\n첫 번째 기사에서는 연령 등급화의 역사에 대해 살펴보고 시스템을 기본적으로 비판했어요.\n\n이 질문은 중요하고, 연령 등급화는 아무것도 없는 것보다 훨씬 나아요. 그러나 모든 사람의 결과를 가상의 최고 결과와 비교하려는 데에는 약간 문제가 있다고 생각돼요. 그 표준을 결정하는 것은 어렵고, 그 특정 표준은 다소 임의적일 수 있어요.\n\n또한, 평균 러너에게 그들의 결과가 전반적인 상황에 어떻게 맞는지 이해하는 데에는 그리 큰 도움이 되지 않을 수도 있어요 — 왜냐하면 얼마나 잘하든 항상 그 최고 가능 시간으로부터 멀리 떨어져 있기 때문이에요.\n\n<div class=\"content-ad\"></div>\n\n연령 채정은 최고의 러너들의 통계와 연구에 기반을 두고 개발되었지만, 이 문제를 다른 시각에서 바라보고 싶었습니다. 다른 모든 러너들과 어떻게 비교할까요?\n\n통계는 여러 가지 방법을 제시하여 이러한 비교를 할 수 있지만, 먼저 함께 작업할 대규모이며 대표적인 데이터 세트가 필요했습니다. 시리즈의 두 번째 기사에서 제 작업한 샘플을 소개했습니다.\n\n간략하게 말하자면, 2010년부터 2019년까지 9월, 10월 또는 11월에 열린 미국 마라톤에 중점을 두었습니다.\n\n개별적으로 포함할 경주를 어떻게 고려할지 걱정하지 않고 전체 '시즌'의 러닝을 포착하기 위해 이 세 달을 선택했습니다. 어떤 경주는 빠를 수도 있고, 어떤 경주는 느릴 수도 있습니다. 종합적으로 말하면, 이들은 꽤 대표적입니다.\n\n<div class=\"content-ad\"></div>\n\n500명 이상의 참가자가 있는 레이스로 사정을 좁혔어요. 이로 인해 샘플 크기가 크게 줄진 않았지만, 스크레이핑 프로세스는 좀 더 쉬워졌어요.\n\n데이터를 모두 수집한 후에는 데이터셋 안에 무엇이 있는지 살펴보았어요.\n\n이 데이터셋에는 200만 명이 넘는 개별 레이스 결과가 포함되어 있어요. 가장 많은 참가자들은 35세 미만 연령 그룹에 있지만, 각 연령 그룹은 상당히 잘 대표되어 있어요 — 적어도 70대까지는요. 70~74세 남성들은 상당히 많았지만, 75~79세 남성 및 70~74세 여성은 더 작은 그룹이었어요. 75~79세 여성 그룹은 더 작았고, 80세 이상 러너들에 대해 유용한 분석을 하기에 충분한 데이터가 없었어요.\n\n손에 있는 데이터를 활용하여, 첫 번째 대안으로 백분위수를 제안할 수 있었어요.\n\n<div class=\"content-ad\"></div>\n\n본질적으로는 이 방법은 분포를 살펴보고 특정 결과가 그 분포 속 어디에 들어가는지를 알아내는 것입니다. 모든 결과를 살펴보면, 특정 시간보다 느리게 끝낸 러너들의 나이 그룹에서 어느 정도의 백분율이 되는지 말할 수 있고, 그 시간을 넘긴 백분율을 말할 수도 있습니다.\n\n이 나이 그룹 간의 시간 분포가 꽤 일관되다고 가정하면, 한 나이 그룹에서 상위 5%에 있는 러너는 대체로 다른 나이 그룹에서 상위 5%에 있는 러너와 유사할 것이라고 볼 수 있습니다.\n\n추가 기사에서는 각각의 분포를 더 자세히 살펴보아 동일하게 구성되어 있는지 확인했습니다. 대체로 그렇게 보였습니다. 작고 나이가 든 그룹에선 조금 덜 깔끔해지긴 했지만 — 분포의 일반적인 모양은 같았습니다.\n\n그러나 추가 분석 결과, 내가 개발한 현재의 표가 어떤 그룹을 다른 그룹에 비해 좀 더 선호하는 경향이 있어 보입니다 — 특히 99.9번째 백분위에서 그렇습니다. 이 부분은 미래 버전에서 수정할 수 있을 것 같지만, 그건 나중 문제가 되겠죠.\n\n<div class=\"content-ad\"></div>\n\n마지막으로, 최근의 논문에서는 z-점수가 레이스 결과를 비교하는 더 나은 방법을 제공하는지 살펴보았어요. \n\nz-점수는 특정 데이터 포인트가 평균보다 얼마나 높거나 낮은지를 측정하는 지표에요. 간단히 말해, 각 분포의 평균과 표준 편차를 계산하고, 그런 다음 결과마다 표준화된 숫자가 할당되어 얼마나 빠르거나 느린지를 나타냅니다.\n\n일반적으로, 이 방법은 작동합니다. z-점수가 -2 이하인 결과는 분포의 외곽에 있고 분명히 인상적입니다. 그러나 이 방법은 불균형적이며, 분명히 다른 연령대의 여성보다 젊은 여성을 선호해요.\n\n퍼센타일은 더 나은 비교를 제공하기 위해 조정되고 보정될 수 있다고 생각하지만, z-점수는 그런 가능성이 없다고 생각해요. 그럼에도 불구하고, 현재 그들을 그대로 유지하여 맥락을 제공하는 데 도움을 주도록 하겠어요.\n\n<div class=\"content-ad\"></div>\n\n# 다양한 연령 그룹에 공정한가요?\n\n지금은 연령 그룹 간 레이스 결과를 비교하는 세 가지 시스템이 있습니다 — 기존 나이 등급 시스템, 각 연령 그룹의 백분위표가 있는 테이블, 그리고 주어진 결과에 대한 z-점수를 계산하는 수단입니다.\n\n이 세 가지 다른 방법 중에서 얼마나 공정한가요? 어떤 그룹을 선호하나요?\n\n이 질문을 바라보는 한 가지 방법은 각 연령 그룹을 대표하는 상위 러너들의 비율을 각 방법에 근거하여 살펴봄으로써 전체 샘플 중 각 연령 그룹의 비율을 비교하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n일반적으로, 그러한 분포가 꽤 유사할 것으로 기대됩니다. 아마도 더 많은 엘리트 선수들 때문에 보다 어린 연령 그룹이 과대표시될 것입니다. 그렇지만, 시스템이 공평하다면 — 큰 격차가 없어야 합니다.\n\n아래 시각화는 2019년에 각 등급 방법으로 상위 100명의 완주자를 보여줍니다. 또한 각 연령 그룹의 모든 참가자들의 백분율을 보여줍니다.\n\n각 연령 그룹 당 네 개의 막대 막대가 있으며, 막대 위를 가리키면 무슨 것을 나타내는지 알려줍니다.\n\n왼쪽 막대는 해당 연령 그룹의 전체 표본에서 참가자의 백분율입니다. 모든 것이 고률적으로 균등하게 분포되고 무작위로 되어 있다면, 100명의 그룹에서 몇 명의 참가자가 나타날 것으로 기대되는지를 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n두 번째 막대는 상위 100명 중 연령 등급에 따라 뛰는 사람의 수를 보여줍니다. 그런 다음 백분위수 기준으로 측정한 수, 마지막으로 Z-점수에 따른 숫자입니다.\n\n먼저, 연령 등급과 전반적인 분포 간의 비교부터 시작해봅시다. 35세 미만의 남성이 여기서 큰 이상점을 형성합니다. 이들은 100명의 러너 중 약 18명을 차지합니다. 그러나 상위 연령 등급 점수의 38명은 35세 미만의 남성입니다. 한편, 35-39세 남성, 40-44세 남성, 그리고 35세 미만의 여성은 모두 상당히 소외됐습니다.\n\n이제 가장 젊은 연령 그룹이 과대표되이 될 수 있는 것은 합리적인 결과일 수 있습니다 — 그 연령 그룹에는 프로러너가 더 많고, 다른 그룹에는 덜할 수 있습니다. 그러나 왜 젊은 남성이 과도하게 대표됐는지, 반면 젊은 여성은 그렇지 않은지 설명할 수 없습니다.\n\n한 칸 더 옮겨서 백분위수를 전반적인 분포와 비교하면 문제가 반대로 전환됩니다. 35세 이하의 여성은 과대표되어 있습니다. 35세 미만의 남성은 여전히 과대표이지만, 다소 균형이 맞습니다. 다른 연령 그룹에 대해서는 그렇게 나쁘지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n드디어, 전체 그래프에서 가장 부조화스러운 막대는 z-점수로 측정된 35세 미만 여성들입니다. 내가 그 글에서 언급한 것처럼, 그들은 그 모델에서 크게 유리합니다. 샘플의 모든 러너 중에 여성들이 35세 미만인 것은 100명 중 17명뿐인데, z-점수로 측정한 상위 완주자 중 53명이 그 나이 그룹에 속합니다. 이것은 문제입니다.\n\n탑 100은 소수 그룹이기 때문에 특이치에 더 취약합니다. 그래서 우리가 탑 500까지 확대해보면 어떻게 될까요?\n\n나이 평가를 보면, 남성은 여전히 현실과 거리가 멀어 보입니다. 그리고 여성들 중에서 35세 미만인 그룹은 여전히 매우 부족하게 표시됩니다. 다른 남성 연령 그룹들은 그리 부조화스럽지 않아 보이지만, 다른 젊은 여성들(35-39세 및 40-44세)도 상당히 부족하게 표시됩니다.\n\n백분위를 보면, 35세 미만의 남성들은 현실적인 위치로 돌아오지만, 35세 미만 여성들은 이제 과대표시되어 있습니다. 35-39세 여성들도 꽤 잘 하고 있습니다. 여성들 35-39세도 꽤 잘 하고 있습니다. 35세 미만의 남성들은 과대표시되어 있지만, 다시 말하지만, 차이는 그리 크지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n또 다른 문제는 이전에 주목하지 못했던 것인데, 45-49세, 50-54세 및 55-59세의 남성들이 여기에 상당히 소수를 차지하고 있습니다. 그들은 표본의 큰 부분을 차지하지만 상위 500명 중에는 매우 적습니다.\n\n상위 1000명으로 돌아가면 더 부드럽고 일관된 그림이 나타납니다.\n\n다시 한 번, 연령 등급은 젊은 남성을 매우 선호하며 젊은 여성을 불리하게 대우합니다. Z-점수는 젊은 여성들을 매우 선호하고 나이 든 남성을 불리하게 대우합니다.\n\n이 정밀도 수준에서 백분위수에 따른 달리기자들의 분포가 가장 균등해 보입니다. 젊은 남성과 여성 모두 과다 표시되어 있지만 그 정도는 크지 않습니다. 반면 50대의 달리기자들은 그렇게 잘 나오지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n하루가 끝날 때, 이 데이터를 통해 세 가지 방법 모두 어떤 측면에서 균형이 잡히지 않았다고 생각합니다. 연령 성적은 어린 남성의 결과를 선호하는 경향이 있고, Z-점수는 어린 여성을 매우 선호합니다. 백분위수는 자체적인 불균형이 있지만 조금 더 정확하게 보정할 수도 있습니다.\n\n또한 주목할 점은 최고의 러너 사이를 구별하는 것 — 상위 1,000명은 표본에서 상위 0.5% 정도에 해당합니다 — 이 백분위수에는 강점이 없다는 것입니다.\n\n# 이 온라인 계산기를 사용하여 자신만의 연령 성적을 계산하세요\n\n이 분석을 수행하기 위해 Python과 Pandas 패키지를 사용하여 많은 작업을 해왔습니다. 큰 샘플 집합을 준비하고, 룩업 테이블과 계산을 바탕으로 각 시스템에 기반한 결과를 빠르게 도출할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n하지만 그것이 독자인 당신을 돕지는 않아요.\n\n그것을 해결하고자, 여기에서 사용할 수 있는 온라인 계산기를 제작해 보았어요.\n\n참고로, 이 계산기는 여전히 베타 버전이고 몇 가지 개선이 필요해요. 특히, 어떠한 종류의 오류 처리도 구현하지 않았기 때문에 시간을 제대로 입력하지 않거나 예상치 못한 값이 입력되면 결과가 표시되지 않을 수 있어요.\n\n하지만, 이것을 통해 세 가지를 입력할 수 있어요 — 러너의 나이, 성별, 그리고 완주 시간 — 그리고 그들의 점수를 각각 세 가지 등급 시스템에 기반하여 볼 수 있어요. 나이 등급화된 시간, 나이 등급 점수, 동료들 사이에서의 백분위 순위, 그리고 결과의 Z-점수를 볼 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n미래에 조금 더 발전시킬 계획이지만, 지금은 초안 시스템이 있어서 여러분과 공유하는 것이 중요하다고 생각했어요.\n\n# 다음 단계는?\n\n지금까지 가장 큰 장애물 중 하나는 온라인 계산기를 만드는 것이었습니다. 그것이 해결되었으니, 프로젝트의 마지막 부분에 대해 앞으로 관리할 수 있게 되었습니다: 새 데이터 수집 및 모델 업데이트가 남아 있습니다.\n\n기술적 문제들에 대처하고자 2010년부터 2019년을 다루는 예전 (크고 더 많은) 데이터셋으로 시작하려고 했습니다. 이제 이 일을 하는 방법에 대해 잡았으니, 2023년부터 새 데이터를 수집하고 백분위 모델과 z-점수 모델을 업데이트할 계획입니다. 또한 이를 2023년 연령별 테이블과 비교할 예정입니다.\n\n<div class=\"content-ad\"></div>\n\n그것은 다음 주 또는 그 다음 주 중순쯤 되실 예정입니다. 새 데이터를 수집한 뒤에 공유할 계획이에요.\n\n그 후에는 이를 마무리하는 것이 시기적으로 적절하다고 생각하고 있어요. 그러나 작업이 완료되면 Kaggle에 전체 데이터 세트를 공유할 계획이기 때문에 다른 분들이 자신만의 분석을 할 수 있도록 도와드릴 거에요.\n\n따라서 이 시리즈의 마지막 부분은 아마도 제시된 일부 비평에 응답하고 일반적인 결론을 도출하며 데이터 세트에 대한 접근 권한을 공유할 예정이에요.\n\n이 중에 관심 있으신 분이 있으면, 다음 몇 개의 기사를 받아보려면 이메일 업데이트를 구독하시기 바랍니다. 다음 두 주 또는 세 주 안에 게시할 예정이에요.\n\n<div class=\"content-ad\"></div>\n\n분석에 도움이 될 피드백이나 아이디어가 있다면 응답을 남겨주세요. 두 번째(또는 세 번째, 네 번째) 의견을 듣는 것이 항상 도움이 됩니다!\n\n저는 열정적인 러너이자 데이터 열정가입니다. 막 그랬더니 40살이 되었어요. 그래서 연령별 결과를 비교하는 것이 특히 흥미로워요. 제가 하는 일을 따라갈 수 있는 방법은 다음과 같습니다:\n\n- 훈련에 대한 소식을 듣고 싶다면 Running with Rock을 팔로우하세요.\n- 마라톤 훈련 계획 선택에 대한 팁을 읽어보세요.\n- Strava에서 저를 스토킹하세요.","ogImage":{"url":"/assets/img/2024-06-22-AgeGradingPercentilesandZ-ScoresThreeWaystoCompareRaceResults_0.png"},"coverImage":"/assets/img/2024-06-22-AgeGradingPercentilesandZ-ScoresThreeWaystoCompareRaceResults_0.png","tag":["Tech"],"readingTime":8},{"title":"NLP 분석을 위한 Youtube 댓글 스크래핑 방법","description":"","date":"2024-06-22 16:44","slug":"2024-06-22-ScrapingYoutubeCommentsforNLPAnalysis","content":"\n\n![image](/assets/img/2024-06-22-ScrapingYoutubeCommentsforNLPAnalysis_0.png)\n\n간단한 NLP 프로젝트를 시도해보고자 했는데, 유튜브 동영상에서 댓글을 가져와 분석해보려고 합니다.\n\n사실 향수에 대해 연구 중이었는데, 인스타그램을 무심코 스크롤하다가 향수 광고에 끌려들어가게 되었습니다. 그렇게해서 향수 리뷰 동영상들을 찾게 되었고, Demi Rawling의 향수 리뷰 동영상을 발견하게 되었습니다... (너무 핫하네요).\n\n유튜브 링크 - https://youtu.be/oJqc2tLMObg\n\n<div class=\"content-ad\"></div>\n\n이 동영상은 탑 10 톰 포드 향수에 관한 것입니다. 다소 오래된 동영상이긴 하지만(4년 전 영상입니다)\n\n먼저, 만약 설치되어 있지 않다면 이곳에서 ChromeDriver를 설치해주세요. 저는 VS Code에서 실행을 시도해봤어요. 필요한 라이브러리는 다음과 같아요...\n\n```js\nimport sys\nimport time\nimport pandas as pd\nfrom datetime import datetime\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n```\n\n그런 다음 스크래핑 부분이 나옵니다...\n\n<div class=\"content-ad\"></div>\n\n```python\nimport pandas as pd\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport time\nimport sys\nfrom datetime import datetime\n\n# WebDriver를 초기화하고 성능을 향상시키기 위한 옵션 설정\noptions = webdriver.ChromeOptions()\noptions.add_argument(\"--headless\")  # 헤드리스 모드로 실행\noptions.add_argument(\"--disable-gpu\")  # GPU 렌더링 비활성화\noptions.add_argument(\"--no-sandbox\")  # OS 보안 모델 우회\noptions.add_argument(\"--disable-dev-shm-usage\")  # 제한된 리소스 문제 극복\noptions.add_argument(\"--start-maximized\")  # 창 최대화\n\ndriver = webdriver.Chrome(options=options)\n\ndata = []\nyoutube_video_url = \"https://youtu.be/oJqc2tLMObg\"\nwait = WebDriverWait(driver, 30) \n\n# YouTube 비디오 URL 열기\ndriver.get(youtube_video_url)\nprint(\"YouTube URL을 열었습니다.\")\n\n# 댓글을 로드하기 위해 스크롤 다운\nfor item in range(150):  # 여기에 스크롤 횟수 정의\n    try:\n        body = wait.until(EC.visibility_of_element_located((By.TAG_NAME, \"body\")))\n        body.send_keys(Keys.END)\n        sys.stdout.write(f\"\\r{item + 1}번 스크롤 중\")\n        sys.stdout.flush()\n        time.sleep(1.5)  # 로딩을 위한 시간 증가\n    except Exception as e:\n        print(f\"스크롤 중 예외 발생: {e}\")\n        break\n\n# 댓글 추출\ntry:\n    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#contents #contents\")))\n    comments = driver.find_elements(By.CSS_SELECTOR, \"#content #content-text\")\n    print(f\"\\n{len(comments)}개의 댓글 요소를 찾았습니다.\")\n\n    user_id = 1  # 고유한 사용자 ID 초기화\n    for comment in comments:\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        data.append({\"사용자 ID\": user_id, \"댓글\": comment.text, \"타임스탬프\": timestamp})\n        user_id += 1\n\n    # 중복 제거\n    data = [dict(t) for t in {tuple(d.items()) for d in data}]\n    print(f\"캡처된 댓글 수: {len(data)}\")\nexcept Exception as e:\n    print(f\"댓글 추출 중 예외 발생: {e}\")\n\ndriver.quit()\n\n# DataFrame 생성\ndf = pd.DataFrame(data, columns=[\"사용자 ID\", \"댓글\", \"타임스탬프\"])\n\n# DataFrame 표시\nprint(df)\n\n# DataFrame을 CSV 파일로 저장 (선택 사항)\ndf.to_csv(\"youtube_comments.csv\", index=False)\n``` \n\n여기서 댓글을 스크래핑하기 위해 스크롤 수를 사용했어요. 댓글 수의 최대값으로 변환할 수도 있지만, 현재 이 방법이 가장 잘 작동합니다.\n\n그래서 이 몇 가지 가정과 제약들이 있어요.\n\n가정과 제약사항: 각 댓글의 사용자 이름을 스크래핑하여 하나의 작성자가 여러 번 댓글을 작성했을 때 합칠 수 있도록 시도했지만 크롬 드라이버에서 많은 시간이 소요되고 있는 것을 고려하여 무모하게 반복된 댓글이 작성되지는 않았을 것이라고 가정하고, 추출된 날짜를 추가한 각 댓글을 고유한 것으로 간주했어요.\n\n\n<div class=\"content-ad\"></div>\n\n크롬 드라이버를 헤드리스 모드로 실행하여 응답 속도를 높였어요. 코드에서 주석 처리하고 실행해보셔도 돼요. 만약 더 빠른 스크래핑 방법을 찾으시면 댓글로 알려주세요. François St-Amant의 코드를 참고했어요.\n\n링크 — [여기](https://towardsdatascience.com/how-to-scrape-youtube-comments-with-python-61ff197115d)\n\n```python\nimport pandas as pd\ndf = pd.DataFrame(data, columns=['comment'])\ndf.head()\n```\n\n![이미지](/assets/img/2024-06-22-ScrapingYoutubeCommentsforNLPAnalysis_1.png)\n\n<div class=\"content-ad\"></div>\n\n지금은 Perplexity.ai와 함께 이 동영상에서 언급된 향수 목록을 두 번째 데이터프레임으로 가지고 있어요. 이 목록은 수동으로 가져왔지만 파이썬을 사용하여 동영상을 구문 분석하여 수행할 수도 있지만 그것은 다른 날을 위해 저장합시다. 또한 이에 대한 YouTube 동영상 요약기를 확인할 수도 있는데, 이것은 제 친구 Priyanshu Shukla가 만들었어요 – https://medium.com/@priyanshu-shkl7/implementing-generative-ai-into-your-apps-web-scraping-with-genai-f08711a404cb\n\n![이미지](/assets/img/2024-06-22-ScrapingYoutubeCommentsforNLPAnalysis_2.png)\n\n이제 NLP 부분으로 넘어가볼게요. 간단히 말씀드리면, 댓글 내에서 언급된 Tom Ford의 다양한 향수들을 얻으려고 노력 중이에요. 기본적으로 언급에 따라 각 향수를 지정하고 활동이 가장 많은 순서대로 정렬 중이에요.\n\n이를 달성하기 위해 다양한 방법이 있지만, 저는 NLTK(Natural Language Toolkit)를 사용했어요. NLTK는 파이썬에서 인간의 언어 데이터를 처리하는 강력한 라이브러리예요. 이는 자연어 처리(NLP) 작업에 널리 사용되며, 의미론적 키워드 일치 및 감성 분석과 같은 작업에 적합해요.\n\n<div class=\"content-ad\"></div>\n\nHuggingFace에서 제공하는 최고의 sentence transformers 중 하나를 사용하여 의미론적 문자열 매칭을 수행하거나 사용할 수 있습니다. 또한, 동일한 작업을 수행하기 위해 사용 가능한 LLM들을 사용할 수도 있지만 대규모 데이터셋에 대한 프로덕션 배포의 경우 sentence transformers가 가장 적합할 것입니다.\n\n```python\n!pip install nltk\nimport nltk\nnltk.download('vader_lexicon')\n```\n\n```python\nimport pandas as pd\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\ndef calculate_sentiment_scores(comments, perfumes):\n    sentiment_scores = {str(perfume): [] for perfume in perfumes}\n    \n    sid = SentimentIntensityAnalyzer()\n    \n    for _, row in comments.iterrows():\n        comment = str(row['Comment'])\n        user_id = row['User ID']\n        sentiment_dict = sid.polarity_scores(comment)\n        \n        compound_score = sentiment_dict['compound']\n        \n        for perfume in perfumes:\n            if str(perfume).lower() in comment.lower():\n                sentiment_scores[str(perfume)].append((user_id, comment, compound_score))\n    \n    return sentiment_scores\n\n# Load the comments DataFrame\ndf_comments = df\n\n# Load the perfumes DataFrame\ndf_perfumes = df2\n\n# Extract perfumes from df_perfumes\nperfumes = df2.iloc[:, 0].tolist()\n\n# Calculate sentiment scores for each perfume in the comments\nsentiment_scores = calculate_sentiment_scores(df_comments, perfumes)\n\n# Create a new DataFrame with 'user_id', 'comment', 'perfume', and 'sentiment_score' columns\ndata = []\nfor perfume, user_comment_list in sentiment_scores.items():\n    for user_id, comment, sentiment_score in user_comment_list:\n        data.append([user_id, comment, perfume, sentiment_score])\n\ndf_result = pd.DataFrame(data, columns=['user_id', 'comment', 'perfume', 'sentiment_score'])\n\n# Print the resulting DataFrame\nprint(\"User Comments with Sentiment Scores:\")\ndf_result\n```\n\n![YouTube Comments](/assets/img/2024-06-22-ScrapingYoutubeCommentsforNLPAnalysis_3.png)\n\n\n<div class=\"content-ad\"></div>\n\n위에서 보듯이, 각 댓글에는 때때로 하나 이상의 향수가 언급되어서 각 향수에 대한 점수가 계산됩니다. 그래서 \"나는 블랙 오키드를 소유하고 있다\"는 댓글은 Beau de jour를 가지고 있습니다. 그래서 Beau de jour와 black orchid 각각 한 번씩 나타납니다.\n\n하지만 이에 더해, 만일 우리가 이 감정 분석과 키워드 일치를 확인해야 한다면, 이를 위해 Tableau로 간단한 대시보드를 구축했습니다. Tableau 대시보드를 사용하면 빠르게 향수들과 그들의 감정 점수를 필터링할 수 있습니다.\n\n[![테이블로 대시보드](/assets/img/2024-06-22-ScrapingYoutubeCommentsforNLPAnalysis_4.png)](https://public.tableau.com/views/YoutubeSentimentAnalysis/YoutubeComments-NLPAnalysis?:language=en-GB&:sid=&:display_count=n&:origin=viz_share_link)\n\n<div class=\"content-ad\"></div>\n\n어떤 생각이든 자유롭게 공유해 주시고, 댓글을 남기셔서 제 소식을 받아보세요! 읽어 주셔서 감사합니다!","ogImage":{"url":"/assets/img/2024-06-22-ScrapingYoutubeCommentsforNLPAnalysis_0.png"},"coverImage":"/assets/img/2024-06-22-ScrapingYoutubeCommentsforNLPAnalysis_0.png","tag":["Tech"],"readingTime":7},{"title":"실시간 통합 기능이 탑재된 Python 대시보드 만들기 ","description":"","date":"2024-06-22 16:43","slug":"2024-06-22-BeautifuldashboardsinPythonwithfirst-classreal-timeintegration","content":"\n\n## deephaven.ui\n\n작성자: Alex Peters\n\n![이미지](/assets/img/2024-06-22-BeautifuldashboardsinPythonwithfirst-classreal-timeintegration_0.png)\n\n파이썬 개발자들은 실시간 데이터 혁명에 준비가 되어 있나요? 가장 일반적인 파이썬 툴킷들 중에는 필수 대시보드 패키지도 포함되어 있지 않습니다. 그래서 우리는 강력한 Deephaven 실시간 데이터 엔진 위에 구축된 실시간 대시보드 라이브러리인 deephaven.ui를 자랑스럽게 소개합니다. 파이썬에서 실시간 대시보드를 만드는 것이 이렇게 쉬워진 적이 없었습니다.\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 deephaven.ui의 시작을 탐구하고 이를 사용하여 스트리밍 대시보드를 만드는 예제를 제시할 것입니다:\n\n![예시 이미지](https://miro.medium.com/v2/resize:fit:1400/1*4bXAHvIHjwx2tT-X1i27FQ.gif)\n\n# 왜 귀찮게 할까요?\n\nPython과 대시보드는 오랜 시간 동안 함께 해왔습니다. Python 개발자들은 종종 아이디어를 효과적이고 매력적인 방식으로 제시해야 할 필요가 있는데, 그러기 위해 필요한 모든 프론트엔드 기술을 배우는 시간이 부족합니다. Dash, Streamlit, Shiny와 같은 플랫폼은 이 문제에 대한 Pythonic한 해결책으로 등장했습니다. 개발자들은 프론트엔드 지식이 거의 없이 아름다운 대시보드를 만들고 한 번의 클릭으로 넓은 커뮤니티와 자신의 작품을 공유할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n그러나 큰 문제가 하나 있습니다. 이러한 플랫폼은 매우 제한적인 실시간 지원을 제공합니다. 여기저기 살펴보면 이러한 플랫폼을 사용하여 대략 실시간 대시보드를 만드는 예제 몇 가지를 찾을 수 있지만, 그들의 실시간 지원은 여과 없이 그런 데로 납치한 것처럼 보입니다. 중요한 것이 그들이 그 실행 모델의 중심요소로 만들 계획이 전혀 없었습니다. 그들은 그저 실시간 지원을 위해 대시보드를 규칙적인 간격으로 새롭게 표시하는 스냅샷 기반 접근 방식을 취합니다. 저사양의 경우에는 작동할지 몰라도, 현대의 대용량 스트리밍 데이터에는 이를 견딜 수 없는 문제가 발생합니다.\n\n왜 이것이 중요한가요? 스트리밍 데이터는 앞으로 시장을 지배할 것으로 전망됩니다. 스트리밍 데이터 파이프라인은 연결된 장치와 빠른 통신을 가능하게 하며, 사기 탐지나 맞춤형 제안과 같은 사용 사례를 구동합니다. 포브스는 2025년까지 생성된 데이터의 거의 30%가 실시간일 것으로 추정하여, 경쟁력 유지를 위해 실시간 데이터 통합이 중요해질 것이라고 합니다. 포츈 비즈니스 인사이츠는 거대한 예측을 내놓았습니다: 2030년까지 스트리밍 분석 시장 규모는 1,250억 달러를 넘길 것으로 예상합니다.\n\n그래서 파이썬 데이터 과학자와 점점 더 필요해질 실시간 대시보드 사이에는 격차가 있습니다. 이 문제에 대한 훌륭한 해결책을 찾아나섰지만, 우선 순위 목록의 맨 위에 스트리밍 대시보드가 있는 솔루션을 찾을 수 없었습니다. 그래서 우리는 직접 제작했습니다: deephaven.ui. 이 새로운 대시보드 패러다임은 유연한 React Spectrum 기반 구문, 모듈식 구성 요소 설계, 그리고 가장 중요한, 실시간을 최우선으로 합니다.\n\ndeephaven.ui는 새롭게 출시되어 매우 열심히 개발 중인 상태이지만, 이미 최상의 플랫폼으로 자리잡고 있습니다. 곧 제공될 방대한 문서를 통해 deephaven.ui의 모든 면을 손쉽게 탐색할 수 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\ndeephaven.ui가 어떻게 작동하는지 보고 싶다면 데모 시스템의 새로운 deephaven.ui 노트북을 확인해보세요. 현재 이 노트북은 데모의 Code Studio 쪽에만 있습니다.\n\n# deephaven.ui로 시작하기\n\ndeephaven.ui를 시작하는 가장 쉬운 방법은 미리 설치된 Docker 이미지를 실행하는 것입니다:\n\n```js\ndocker run --rm --name deephaven-ui -p 10000:10000 --pull=always ghcr.io/deephaven/server-ui:latest\n```\n\n<div class=\"content-ad\"></div>\n\n그럼 http://localhost:10000/ide/로 이동하여 deephaven.ui를 사용해보세요.\n\n아래는 mock 스트리밍 데이터셋 아래에 있는 상대적으로 복잡한 deephaven.ui 대시보드입니다. 이것을 IDE로 복사하여 붙여넣기하고 deephaven.ui가 얼마나 유연하고 반응성 있는지 살펴보세요:\n\n```js\nfrom deephaven import ui, agg, empty_table\n\nfrom deephaven.stream.table_publisher import table_publisher\nfrom deephaven.stream import blink_to_append_only\n\nfrom deephaven.plot import express as dx\nfrom deephaven import updateby as uby\nfrom deephaven import dtypes as dht\n\nstocks = dx.data.stocks().reverse()\n\n...\n\ndashboard = ui.dashboard(my_layout(stocks, _stocks_with_stats))\n```","ogImage":{"url":"/assets/img/2024-06-22-BeautifuldashboardsinPythonwithfirst-classreal-timeintegration_0.png"},"coverImage":"/assets/img/2024-06-22-BeautifuldashboardsinPythonwithfirst-classreal-timeintegration_0.png","tag":["Tech"],"readingTime":3}],"page":"39","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true}