{"pageProps":{"post":{"title":"구글의 오픈 소스 모델 Gemma 2 출시","description":"","date":"2024-06-30 19:12","slug":"2024-06-30-TheOpenSourceModelGemma2byGoogleisHere","content":"\n\n취약하지만 접근성이 높은 LLM입니다.\n\n![Gemma 2](/assets/img/2024-06-30-TheOpenSourceModelGemma2byGoogleisHere_0.png)\n\n다시 한 번 다른 언어 모델을 소개합니다… 이번에는 구글의 오픈 소스 모델인 Gemma 2입니다!\n\n인공 지능 분야는 대규모 언어 모델(LLM)의 능력이 폭발적으로 증가했습니다… 대규모 데이터셋으로 훈련된 이 복잡한 시스템들은 인간과 유사한 텍스트를 이해하고 생성하는 놀라운 능력을 보여주며, 한 때 인간 지능의 독점 영역으로 여겨졌던 경계를 넘어섰습니다. 그렇지만!!! 이러한 진전은 종종 엄청난 계산 리소스 비용을 지불해야 했기에 많은 연구원과 개발자에게는 액세스가 제한되는 문제가 있었습니다. 이 맥락에서 구글이 개발한 Gemma 2가 소개됩니다. Gemma 2는 경쟁력 있는 성능을 실용적인 크기 내에서 제공하기 위해 섬세하게 설계된 새로운 세대의 오픈 소스 LLM이며, 강력한 인공 지능 도구에 대한 접근성을 더 쉽게 만들어줍니다.\n\n<div class=\"content-ad\"></div>\n\n더 알고 싶으신가요? 만약 그렇다면, 여기 있어요:\n\n- Gemma 2: 모델 패밀리\n- 건축 혁신: 효율성을 위한 건축 블록\n- 지식 증류: 현명한 스승으로부터 배우기\n- 성능 벤치마크: 새로운 표준 설정\n- 책임과 안전: 윤리적 요구사항\n- 결론\n\n# Gemma 2: 모델 패밀리\n\nGemma 2는 단일 모델이 아니라 특정 계산 제약 조건에 맞게 맞춘 여러 모델로 이루어진 패밀리입니다. 현재 라인업에는 90억과 270억 개 파라미터 모델이 포함되어 있으며, 곧 20억 개 파라미터 모델이 출시될 예정입니다. 이 범위는 개발자들이 자원 및 애플리케이션 요구사항에 가장 적합한 모델을 선택할 수 있도록 유연성을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n# 건축 혁신: 효율의 기반 요소들\n\n\"Gemma 2\"는 자신의 기반이 되는 디코더 전용 트랜스포머 아키텍처를 선배 모델인 \"Gemma (1)\"로부터 계승합니다. 그러나, 더 나은 효율성과 성능을 도모하기 위해 주요 아키텍처 세부 개선점을 통합하였습니다:\n\n- 로컬 및 글로벌 어텐션 교차: \"Gemma 2\"는 로컬 슬라이딩 윈도우 어텐션과 글로벌 어텐션의 레이어를 전략적으로 교대로 배치합니다. 이 접근 방식은 텍스트 내에서 로컬 컨텍스트와 보다 넓은 관계를 모두 포착하여 미묘한 균형을 이룹니다. 로컬 어텐션은 일정한 토큰 윈도우에 초점을 맞추어 계산 부담을 줄이는 반면, 글로벌 어텐션은 시퀀스의 모든 토큰을 고려하여 장거리 의존성을 식별하는 모델의 능력을 유지합니다. 이를 예로 들어보겠습니다. \"The cat sat on the mat, but it was thinking about the delicious fish it had for dinner\"라는 문장을 고려해보면, 로컬 어텐션은 \"cat\"과 \"sat\" 사이의 관계를 이해하는 데 초점을 맞출 수 있고, 글로벌 어텐션은 \"it\"을 \"cat\"과 \"fish\"에 연결하여 대명사의 참조를 포착할 수 있습니다.\n- 그룹화된 쿼리 어텐션 (GQA): GQA는 효율성을 희생하지 않으면서 계산 요구를 더 줄이기 위한 혁신적인 어텐션 메커니즘입니다. 모든 가능한 단어 쌍 사이의 어텐션 점수를 계산하는 대신, GQA는 쿼리와 키를 작은 그룹으로 나누어 이 그룹 내에서 어텐션을 계산합니다. 이 스마트한 파티셔닝은 특히 더 긴 시퀀스에 대해 처리 속도를 크게 향상시킵니다.\n- 로짓 소프트 캡핑: 훈련 안정성을 향상시키기 위해 \"Gemma 2\"는 로짓 소프트 캡핑을 사용합니다. 이 기법은 내부 표현에서 극단적인 값들을 방지하여 보다 견고하고 예측 가능한 동작을 유도합니다.\n- RMSNorm을 사용한 포스트-노름 및 프리-노름: \"Gemma 2\"는 각 레이어의 입력과 출력을 정규화하기 위해 RMSNorm (Root Mean Square 레이어 정규화)을 활용합니다. 이 선택은 더 부드러운 훈련과 개선된 모델 수렴에 기여합니다.\n\n\"Gemma 2\"의 모든 특징을 다시 한번 알아보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-30-TheOpenSourceModelGemma2byGoogleisHere_1.png)\n\n# 지식 증류: 현명한 선생님으로부터 배우기\n\nLLM 교육은 기본적으로 다음 토큰 예측에 의존하며, 모델은 시퀀스에서 다음 단어를 예측하는 방법을 학습합니다. 이 방법은 효과적이지만 최적의 성능을 위해 방대한 데이터 세트가 필요할 수 있습니다. 특히 2B 및 9B 모델을 포함하는 Gemma 2는 지식 증류라는 더 정교한 기술을 채택합니다.\n\n현명한 선생님으로부터 배우는 학생을 상상해보십시오. 학생이 선생님의 행동을 그대로 흉내 내는 대신, 학생은 선생님의 추론과 사고 과정을 이해함으로써 더 깊은 통찰을 얻게 됩니다. 비슷하게, 지식 증류에서는 더 큰 사전 훈련된 \"선생님\" 모델로부터 더 작은 \"학생\" 모델이 배우게 됩니다. 학생 모델은 다음 토큰을 예측하는 것뿐만 아니라 선생님이 예측한 모든 가능한 다음 토큰에 대한 확률 분포를 근사화하는 방식으로 교육됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n이 접근 방식은 중요한 장점을 제공합니다:\n\n- 데이터 효율성: 학생 모델은 훈련 데이터가 적더라도 더 큰 데이터셋에서 얻은 선생님의 지식을 활용하여 비교 가능한 성능을 달성합니다.\n- 빠른 훈련: 증류는 더 풍부한 기울기를 제공하여 학생 모델을 보다 최적의 솔루션 공간으로 이끌어 훈련을 가속화합니다.\n- 향상된 성능: 더 강력한 모델에서 학습하여 학생 모델은 종종 동일한 데이터셋에서 전통적인 다음 토큰 예측 훈련을 통해 달성할 수 있는 성능을 능가합니다.\n\n# 성능 기준: 새로운 기준 설정하기\n\nGemma 2는 엄격한 기준에 따라 빛을 발하며, 유사한 크기의 다른 오픈 모델을 지속적으로 능가하고 규모가 훨씬 큰 모델에도 도전합니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-30-TheOpenSourceModelGemma2byGoogleisHere_2.png)\n\n당연히 현재의 최첨단 모델인 GPT-4o와 Claude 3.5 Sonnet과는 비교할 수 없습니다. GPT-4o와 Claude 3.5 Sonnet은 각각 HumanEval인 python 코딩의 벤치마크에서 90.2% 및 92.0%와 같이 인상적인 값을 달성할 수 있습니다. 또한, 3.5 Sonnet은 학교 수학의 벤치마크인 GSM8K에서 0-shot 방식으로 96.4%의 성과를 보여줍니다. \n\n그러나, Gemma 2의 강점은 접근성과 현실성에 있습니다. 더 작은 모델 크기로 인해 보다 넓은 하드웨어 범위에 배포할 수 있어, 자원이 제한된 개발자와 연구자들에게 혜택을 줄 수 있습니다.\n\n# 책임과 안전: 윤리적 필수성\n\n<div class=\"content-ad\"></div>\n\nLLM의 잠재적인 영향은 책임 있는 개발과 배포에 강한 헌신이 필요합니다. Gemma 2는 안전과 윤리 고려 사항을 핵심으로 구축되었습니다.\n\n- 안전 정책 및 교육 시간 완화: Gemma 2의 훈련 데이터는 해로운 및 편향적인 콘텐츠를 제거하기 위해 엄격하게 필터링됩니다. 게다가, 모델은 안전 정책으로 세밀하게 조정되어 부적절하거나 유해한 결과물을 생성할 위험을 최소화합니다.\n- 견고하고 투명한 평가: Gemma 2는 자동화된 벤치마크와 인간 평가를 결합하여 능력과 잠재적 위험을 평가하기 위해 철저히 평가됩니다. 이러한 평가는 안전, 공정성, 편향, 강건성을 포함한 다양한 측면을 다룹니다.\n- 책임 있는 생성적 AI 툴킷: 개발자들을 지원하기 위해 구글은 포괄적인 책임 있는 생성적 AI 툴킷을 제공합니다. 이 툴킷은 Gemma 2 모델의 안전하고 책임 있는 배포를 보장하기 위한 리소스, 도구 및 모베스트 프랙티스를 제공합니다.\n\n# 결론\n\nGemma 2는 오픈 LLM 기술의 상쾌한 바람으로, 첨단 성능, 실용적 효율성, 책임 있는 인공지능에 대한 강력한 헌신의 조합을 제공합니다: 이 강력한 도구들에 대한 이용을 민주화함으로써, Gemma 2는 연구 및 교육부터 콘텐츠 제작에 이르기까지 다양한 분야에서 혁신의 새로운 물결을 촉진할 수 있습니다. 사용하려면, 모델의 가중치는 예상대로 Hugging Face에서 이용 가능합니다!\n\n<div class=\"content-ad\"></div>\n\n멋진 마크다운 작성하셨네요! 부담 갖지 마시고 계속 찾아뵙세요.","ogImage":{"url":"/assets/img/2024-06-30-TheOpenSourceModelGemma2byGoogleisHere_0.png"},"coverImage":"/assets/img/2024-06-30-TheOpenSourceModelGemma2byGoogleisHere_0.png","tag":["Tech"],"readingTime":5},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>취약하지만 접근성이 높은 LLM입니다.</p>\n<p><img src=\"/assets/img/2024-06-30-TheOpenSourceModelGemma2byGoogleisHere_0.png\" alt=\"Gemma 2\"></p>\n<p>다시 한 번 다른 언어 모델을 소개합니다… 이번에는 구글의 오픈 소스 모델인 Gemma 2입니다!</p>\n<p>인공 지능 분야는 대규모 언어 모델(LLM)의 능력이 폭발적으로 증가했습니다… 대규모 데이터셋으로 훈련된 이 복잡한 시스템들은 인간과 유사한 텍스트를 이해하고 생성하는 놀라운 능력을 보여주며, 한 때 인간 지능의 독점 영역으로 여겨졌던 경계를 넘어섰습니다. 그렇지만!!! 이러한 진전은 종종 엄청난 계산 리소스 비용을 지불해야 했기에 많은 연구원과 개발자에게는 액세스가 제한되는 문제가 있었습니다. 이 맥락에서 구글이 개발한 Gemma 2가 소개됩니다. Gemma 2는 경쟁력 있는 성능을 실용적인 크기 내에서 제공하기 위해 섬세하게 설계된 새로운 세대의 오픈 소스 LLM이며, 강력한 인공 지능 도구에 대한 접근성을 더 쉽게 만들어줍니다.</p>\n<div class=\"content-ad\"></div>\n<p>더 알고 싶으신가요? 만약 그렇다면, 여기 있어요:</p>\n<ul>\n<li>Gemma 2: 모델 패밀리</li>\n<li>건축 혁신: 효율성을 위한 건축 블록</li>\n<li>지식 증류: 현명한 스승으로부터 배우기</li>\n<li>성능 벤치마크: 새로운 표준 설정</li>\n<li>책임과 안전: 윤리적 요구사항</li>\n<li>결론</li>\n</ul>\n<h1>Gemma 2: 모델 패밀리</h1>\n<p>Gemma 2는 단일 모델이 아니라 특정 계산 제약 조건에 맞게 맞춘 여러 모델로 이루어진 패밀리입니다. 현재 라인업에는 90억과 270억 개 파라미터 모델이 포함되어 있으며, 곧 20억 개 파라미터 모델이 출시될 예정입니다. 이 범위는 개발자들이 자원 및 애플리케이션 요구사항에 가장 적합한 모델을 선택할 수 있도록 유연성을 제공합니다.</p>\n<div class=\"content-ad\"></div>\n<h1>건축 혁신: 효율의 기반 요소들</h1>\n<p>\"Gemma 2\"는 자신의 기반이 되는 디코더 전용 트랜스포머 아키텍처를 선배 모델인 \"Gemma (1)\"로부터 계승합니다. 그러나, 더 나은 효율성과 성능을 도모하기 위해 주요 아키텍처 세부 개선점을 통합하였습니다:</p>\n<ul>\n<li>로컬 및 글로벌 어텐션 교차: \"Gemma 2\"는 로컬 슬라이딩 윈도우 어텐션과 글로벌 어텐션의 레이어를 전략적으로 교대로 배치합니다. 이 접근 방식은 텍스트 내에서 로컬 컨텍스트와 보다 넓은 관계를 모두 포착하여 미묘한 균형을 이룹니다. 로컬 어텐션은 일정한 토큰 윈도우에 초점을 맞추어 계산 부담을 줄이는 반면, 글로벌 어텐션은 시퀀스의 모든 토큰을 고려하여 장거리 의존성을 식별하는 모델의 능력을 유지합니다. 이를 예로 들어보겠습니다. \"The cat sat on the mat, but it was thinking about the delicious fish it had for dinner\"라는 문장을 고려해보면, 로컬 어텐션은 \"cat\"과 \"sat\" 사이의 관계를 이해하는 데 초점을 맞출 수 있고, 글로벌 어텐션은 \"it\"을 \"cat\"과 \"fish\"에 연결하여 대명사의 참조를 포착할 수 있습니다.</li>\n<li>그룹화된 쿼리 어텐션 (GQA): GQA는 효율성을 희생하지 않으면서 계산 요구를 더 줄이기 위한 혁신적인 어텐션 메커니즘입니다. 모든 가능한 단어 쌍 사이의 어텐션 점수를 계산하는 대신, GQA는 쿼리와 키를 작은 그룹으로 나누어 이 그룹 내에서 어텐션을 계산합니다. 이 스마트한 파티셔닝은 특히 더 긴 시퀀스에 대해 처리 속도를 크게 향상시킵니다.</li>\n<li>로짓 소프트 캡핑: 훈련 안정성을 향상시키기 위해 \"Gemma 2\"는 로짓 소프트 캡핑을 사용합니다. 이 기법은 내부 표현에서 극단적인 값들을 방지하여 보다 견고하고 예측 가능한 동작을 유도합니다.</li>\n<li>RMSNorm을 사용한 포스트-노름 및 프리-노름: \"Gemma 2\"는 각 레이어의 입력과 출력을 정규화하기 위해 RMSNorm (Root Mean Square 레이어 정규화)을 활용합니다. 이 선택은 더 부드러운 훈련과 개선된 모델 수렴에 기여합니다.</li>\n</ul>\n<p>\"Gemma 2\"의 모든 특징을 다시 한번 알아보겠습니다:</p>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-30-TheOpenSourceModelGemma2byGoogleisHere_1.png\" alt=\"image\"></p>\n<h1>지식 증류: 현명한 선생님으로부터 배우기</h1>\n<p>LLM 교육은 기본적으로 다음 토큰 예측에 의존하며, 모델은 시퀀스에서 다음 단어를 예측하는 방법을 학습합니다. 이 방법은 효과적이지만 최적의 성능을 위해 방대한 데이터 세트가 필요할 수 있습니다. 특히 2B 및 9B 모델을 포함하는 Gemma 2는 지식 증류라는 더 정교한 기술을 채택합니다.</p>\n<p>현명한 선생님으로부터 배우는 학생을 상상해보십시오. 학생이 선생님의 행동을 그대로 흉내 내는 대신, 학생은 선생님의 추론과 사고 과정을 이해함으로써 더 깊은 통찰을 얻게 됩니다. 비슷하게, 지식 증류에서는 더 큰 사전 훈련된 \"선생님\" 모델로부터 더 작은 \"학생\" 모델이 배우게 됩니다. 학생 모델은 다음 토큰을 예측하는 것뿐만 아니라 선생님이 예측한 모든 가능한 다음 토큰에 대한 확률 분포를 근사화하는 방식으로 교육됩니다.</p>\n<div class=\"content-ad\"></div>\n<p>이 접근 방식은 중요한 장점을 제공합니다:</p>\n<ul>\n<li>데이터 효율성: 학생 모델은 훈련 데이터가 적더라도 더 큰 데이터셋에서 얻은 선생님의 지식을 활용하여 비교 가능한 성능을 달성합니다.</li>\n<li>빠른 훈련: 증류는 더 풍부한 기울기를 제공하여 학생 모델을 보다 최적의 솔루션 공간으로 이끌어 훈련을 가속화합니다.</li>\n<li>향상된 성능: 더 강력한 모델에서 학습하여 학생 모델은 종종 동일한 데이터셋에서 전통적인 다음 토큰 예측 훈련을 통해 달성할 수 있는 성능을 능가합니다.</li>\n</ul>\n<h1>성능 기준: 새로운 기준 설정하기</h1>\n<p>Gemma 2는 엄격한 기준에 따라 빛을 발하며, 유사한 크기의 다른 오픈 모델을 지속적으로 능가하고 규모가 훨씬 큰 모델에도 도전합니다.</p>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-30-TheOpenSourceModelGemma2byGoogleisHere_2.png\" alt=\"image\"></p>\n<p>당연히 현재의 최첨단 모델인 GPT-4o와 Claude 3.5 Sonnet과는 비교할 수 없습니다. GPT-4o와 Claude 3.5 Sonnet은 각각 HumanEval인 python 코딩의 벤치마크에서 90.2% 및 92.0%와 같이 인상적인 값을 달성할 수 있습니다. 또한, 3.5 Sonnet은 학교 수학의 벤치마크인 GSM8K에서 0-shot 방식으로 96.4%의 성과를 보여줍니다.</p>\n<p>그러나, Gemma 2의 강점은 접근성과 현실성에 있습니다. 더 작은 모델 크기로 인해 보다 넓은 하드웨어 범위에 배포할 수 있어, 자원이 제한된 개발자와 연구자들에게 혜택을 줄 수 있습니다.</p>\n<h1>책임과 안전: 윤리적 필수성</h1>\n<div class=\"content-ad\"></div>\n<p>LLM의 잠재적인 영향은 책임 있는 개발과 배포에 강한 헌신이 필요합니다. Gemma 2는 안전과 윤리 고려 사항을 핵심으로 구축되었습니다.</p>\n<ul>\n<li>안전 정책 및 교육 시간 완화: Gemma 2의 훈련 데이터는 해로운 및 편향적인 콘텐츠를 제거하기 위해 엄격하게 필터링됩니다. 게다가, 모델은 안전 정책으로 세밀하게 조정되어 부적절하거나 유해한 결과물을 생성할 위험을 최소화합니다.</li>\n<li>견고하고 투명한 평가: Gemma 2는 자동화된 벤치마크와 인간 평가를 결합하여 능력과 잠재적 위험을 평가하기 위해 철저히 평가됩니다. 이러한 평가는 안전, 공정성, 편향, 강건성을 포함한 다양한 측면을 다룹니다.</li>\n<li>책임 있는 생성적 AI 툴킷: 개발자들을 지원하기 위해 구글은 포괄적인 책임 있는 생성적 AI 툴킷을 제공합니다. 이 툴킷은 Gemma 2 모델의 안전하고 책임 있는 배포를 보장하기 위한 리소스, 도구 및 모베스트 프랙티스를 제공합니다.</li>\n</ul>\n<h1>결론</h1>\n<p>Gemma 2는 오픈 LLM 기술의 상쾌한 바람으로, 첨단 성능, 실용적 효율성, 책임 있는 인공지능에 대한 강력한 헌신의 조합을 제공합니다: 이 강력한 도구들에 대한 이용을 민주화함으로써, Gemma 2는 연구 및 교육부터 콘텐츠 제작에 이르기까지 다양한 분야에서 혁신의 새로운 물결을 촉진할 수 있습니다. 사용하려면, 모델의 가중치는 예상대로 Hugging Face에서 이용 가능합니다!</p>\n<div class=\"content-ad\"></div>\n<p>멋진 마크다운 작성하셨네요! 부담 갖지 마시고 계속 찾아뵙세요.</p>\n</body>\n</html>\n"},"__N_SSG":true}