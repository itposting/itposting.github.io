{"pageProps":{"posts":[{"title":"DBT  Airflow  ","description":"","date":"2024-06-19 09:45","slug":"2024-06-19-dbtAirflow","content":"\n\n![image](/assets/img/2024-06-19-dbtAirflow_0.png)\n\n요즘의 동적이고 경쟁적인 환경에서 기업은 데이터 기반 의사결정에 크게 의존하고 있습니다. 이를 실현하기 위해 조직은 믿을 수 있는 견고한 데이터 플랫폼과 고품질 데이터가 필요합니다. 이는 데이터를 효과적으로 수집하고 저장하는 시스템뿐만 아니라 데이터의 정확성과 신뢰성을 보장하는 것을 의미합니다.\n\n수백 개 또는 수천 개의 데이터 모델을 관리하는 것은 항상 간단한 과정은 아닙니다. 데이터 팀은 종종 데이터 자산을 효과적으로 구축, 테스트 및 유지하는 데 고민합니다. 플럼에서는 데이터 빌드 도구(dbt)를 활용하여 데이터 모델을 효과적으로 관리할 수 있는 CLI 도구에 의존합니다.\n\n그러나 dbt는 명령줄 인터페이스의 특성 때문에 도전적인 요소를 가지고 있습니다. 그렇다면, 제품 환경에 배포할 때 dbt의 장점을 팀이 어떻게 활용할 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\ndbt Cloud가 당연한 선택처럼 보일 수 있지만, 모든 팀 또는 회사가 이 제품에 투자하기에 준비되어 있는 것은 아닙니다. 이러한 상황에서는 dbt 모델의 실행을 조율하고 그 사이의 종속성을 유지하는 대체 방법이 필요합니다.\n\n# 왜 자체 통합을 구축하기로 투자했는가\n\ndbt Cloud에 필요한 투자 외에도, 우리 팀은 특정 기능을 지원받길 원했습니다. 그러나 당시에는 플랫폼에서 이러한 기능 중 일부만 지원되었습니다. 아래에서 가장 기본적인 것들을 개요하겠습니다.\n\n![2024-06-19-dbtAirflow_1.png](/assets/img/2024-06-19-dbtAirflow_1.png)\n\n<div class=\"content-ad\"></div>\n\n- dbt 프로젝트 일정 설정: 우리는 dbt 프로젝트를 cron 작업 스타일로 일정을 잡고, 특정 시간 간격을 지정합니다.\n- 작업 세분화: 각 dbt 엔티티(모델, 테스트, 시드, 스냅샷을 포함)는 개별적으로 처리되어 필요한 경우에 개별적으로 트리거될 수 있도록 합니다.\n- dbt 의존성 유지: 또한, dbt 엔티티 간의 종속성을 유지해야 합니다. 예를 들어, 모델 A가 테스트를 포함하여 다른 모델 B의 상위 의존성으로 작용하는 경우 실행 순서를 유지해야 합니다.\n\n```js\n[dbt run A] -> [dbt test A] -> [dbt run B]\n```\n\n4. 다른 워크플로우 트리거: 특정 dbt 엔티티의 성공적인 완료 후에 특정 워크플로우를 시작해야 하며, 전체 dbt 프로젝트가 완료될 필요가 없는 경우가 있습니다. 또한, 두 dbt 엔티티 실행 사이에 특정 워크플로우를 시작해야할 경우가 있을 수 있습니다.\n\n```js\n[dbt run A] -> [dbt와 무관한 워크플로우 실행] -> [dbt run B]\n```\n\n<div class=\"content-ad\"></div>\n\n5. 알림: 문제가 발생했을 때, 슬랙으로 알림을 받아야 합니다.\n\n6. 컨테이너화된 dbt 프로젝트 실행: 각각의 dbt 프로젝트는 자체 Docker 이미지를 가져야 합니다. 필요시 다른 dbt 버전에서 실행할 수 있는 유연성을 제공합니다.\n\n7. 모델 하위 집합 트리거링: dbt 엔티티는 dbt 태그를 기반으로 필터링될 수 있어서 필요한 특정 모델 그룹을 실행할 수 있습니다.\n\n8. 여러 일정 생성: 동일한 프로젝트를 서로 다른 일정으로 실행해야 할 수도 있습니다. 이는 모델 하위 집합을 트리거할 필요와 밀접하게 관려이 있습니다. 예를 들어, 모델을 시간별, 일별, 주간별로 태그 지정하여 해당 일정에 맞춰 실행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다양한 대안을 탐색해 본 결과, 천문학자가 개발한 Cosmos 패키지를 포함해 어떤 것도 당시의 우리 사용 사례와 명확한 요구 사항에 적합하다는 것을 입증하지 못했습니다.\n\n이러한 도전에 직면하자, 우리는 우리의 요구에 맞게 제작된 자체 Python 패키지를 개발하기로 전략적인 결정을 내렸습니다. 이 맞춤형 솔루션은 우리에게 dbt와 Airflow를 원활하게 통합할 수 있는 유연성과 기능이 제공되어, 데이터 팀이 효과적으로 데이터 모델을 관리하고 최적화할 수 있게 해줍니다.\n\n# 왜 Airflow를 선택했나요?\n\n데이터 파이프라인 Orchestration 전략을 설계할 때, 우리는 특정 간격(시간별, 일별 또는 주별)으로 작업을 예약하고 관리할 수 있는 능력을 지향했습니다. 팀이 Airflow 및 Google Cloud의 Cloud Composer 서비스에 익숙하고 기존 인프라가 있었기 때문에 Airflow를 선택하는 것이 우리의 요구에 적합한 자연스러운 선택이었습니다.\n\n<div class=\"content-ad\"></div>\n\n또한 Airflow와 dbt는 Directed Acyclic Graphs (DAGs) 개념을 중심으로 하고 있어, dbt DAGs를 Airflow DAGs로 변환하여 Orchestration(오케스트레이션)할 수 있습니다.\n\nDAGs는 노드가 닫힌 순환 루프를 형성하지 않고 방향성을 가지고 연결된 작업 또는 데이터 모델을 나타내는 그래프입니다. dbt에서 DAGs는 데이터 모델 간의 관계와 종속성을 나타내고, Airflow에서는 DAGs는 데이터 파이프라인에 포함된 단계와 종속성을 시각화합니다.\n\n# 0부터 1까지: Cloud Composer에서 dbt 실행하기\n\n이 초기 단계에서 우리의 작업은 상대적으로 간단했습니다: Airflow를 사용하여 dbt 프로젝트를 실행하고 테스트하되, 각 모델, 테스트, 스냅샷 또는 시드를 개별 작업으로 설정할 필요가 없었습니다.\n\n<div class=\"content-ad\"></div>\n\n필요한 것은 GitHub Actions를 사용하여 CI/CD 파이프라인을 설정하는 것이었습니다. 이 파이프라인은 관련된 dbt 프로젝트를 Cloud Composer 버킷으로 복사하고 Airflow 내에서 DAG를 구성합니다. 이 DAG에는 모델을 실행하는 하나의 작업 및 해당 작업을 테스트하는 다른 작업이 포함됩니다.\n\n![2024-06-19-dbtAirflow_2](/assets/img/2024-06-19-dbtAirflow_2.png)\n\n이것은 Airflow에서 dbt 프로젝트를 실행하는 가장 간단한 방법일 수 있습니다. DAG는 BashOperator로 생성된 두 개의 작업으로 구성됩니다. 기본적으로, 이는 로컬에서 dbt run 및 dbt test 명령을 실행하는 dbt CLI를 사용한 프로세스를 반영합니다.\n\n그러나 이 방법에는 여러 가지 제약이 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 만약에 dbt_run 작업 내에서 모델 실행이 실패한다면, 문제를 해결하기 위해 작업을 다시 실행해야 합니다. 이는 이전에 성공했던 모델들을 포함하여 모든 모델을 다시 실행해야 하므로 실행 시간과 비용이 증가하는 것을 의미합니다.\n- 모델 실행과 테스트는 완전히 독립적입니다. 모든 테스트는 워크플로우의 끝에 실행됩니다. 따라서 다른 모델들의 상위 종속성인 모델들에서 문제가 발생할 경우 너무 늦기 전까지 알아차리지 못할 수 있습니다.\n- 만약에 dbt_test 작업에서 모델 테스트가 실패한다면, 이 문제를 해결하기 위해 영향을 받는 모든 모델을 수정하기 위해 dbt_run과 dbt_test 작업을 검토하고 재실행해야 합니다.\n- Airflow 작업은 BashOperator를 사용하여 생성되며, Airflow 환경에 dbt 패키지 의존성이 설치되어 있어야 합니다. 이는 패키지 버전 호환성 문제로 인해 문제가 될 수 있으며, 특히 GCP의 Cloud Composer나 AWS의 MWAA와 같은 관리형 Airflow 서비스에서 더욱 그렇습니다.\n\n이 초기 시도는 Proof-of-Concept(PoC)로서의 느낌이 더 큰 노력입니다. 제한사항과 확장성 부족에도 불구하고, 팀의 기대에 부합하는 해결책을 개발할 자신감을 제공했습니다.\n\n우리의 다음 과제는 dbt 프로젝트를 Airflow DAG로 변환하여 각 dbt 엔티티가 해당하는 Airflow 작업으로 매핑되고 모든 종속성이 유지되도록 하는 것이었습니다.\n\n# dbt-airflow 빌딩\n\n<div class=\"content-ad\"></div>\n\ndbt DAG를 Airflow DAG로 변환하기 위해 첫 번째 작업은 dbt 엔터티에서 의존성을 추출하는 것이었습니다. 이는 데이터 모델 간의 관계를 구문 분석하고 DAG 내의 Airflow 작업에 매핑하는 것을 포함했습니다.\n\ndbt 의존성을 Airflow 작업으로 표현함으로써 데이터 워크플로를 원활하게 조정할 수 있었습니다. 작업(예: 모델 실행 또는 테스트)이 실패하면 분석가와 분석 엔지니어는 DAG의 나머지 부분을 해제하기 위해 기초 모델 문제에 대처해야 했습니다.\n\n이 접근 방식은 처음에는 방해적으로 보일 수 있지만, 물론 목표는 신뢰할 수 없는 데이터로 모델을 구축하는 것이 아닙니다. 그러나 초기 오류 감지는 우리 설계의 중요한 측면이었습니다. 이렇게 함으로써 비용을 최소화하고 계산 속도를 절약하며 데이터에 대한 신뢰를 유지할 수 있었습니다.\n\n## 반복 1: manifest.json 파일 활용\n\n<div class=\"content-ad\"></div>\n\n첫 번째 반복에서는 dbt가 생성한 manifest.json 파일을 읽는 파서를 개발했습니다. 이 메타데이터 파일은 dbt가 실행, 테스트 및 컴파일 중에 생성됩니다.\n\n이 파일의 내용을 사용하여 모든 dbt 엔티티와 해당 의존성을 추출할 수 있었습니다. 이 이정표는 dbt-airflow shaping의 탄생을 알리는 중요한 순간이었습니다.\n\n첫 릴리스는 manifest.json 파일에서 프로젝트를 읽고 모델 의존성을 추출하여 Airflow DAG 내의 TaskGroup으로 변환하는 능력을 갖추었습니다.\n\n이제 특정 작업이 실패할 때와 같이, 하류 의존성은 누군가 기본 문제를 해결할 때까지 일시 중지되어 DAG의 나머지 부분이 차단 해제됩니다.\n\n<div class=\"content-ad\"></div>\n\n이 방법을 통해 문제가 발생했을 때 전체 모델 또는 테스트 세트를 다시 실행할 필요가 없어졌어요. 결과적으로, 이 전략은 Airflow에서 실행되는 dbt 프로젝트의 실행 및 유지에 관련된 비용을 크게 줄였어요.\n\n초기 Airflow DAG는 다음과 같았어요;\n\n```js\nimport functools\nfrom datetime import datetime \nfrom datetime import timedelta\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import PythonOperator\nfrom dbt_airflow.core.config import DbtAirflowConfig\nfrom dbt_airflow.core.config import DbtProfileConfig\nfrom dbt_airflow.core.config import DbtProjectConfig\nfrom dbt_airflow.core.task_group import DbtTaskGroup\nfrom dbt_airflow.operators.execution import ExecutionOperator\n\n\nwith DAG(\n    dag_id='test_dag',\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2),\n        },\n    on_failure_callback=functools.partial(\n        our_callback_function_to_send_slack_alerts\n    ),\n) as dag:\n\n    t1 = EmptyOperator(task_id='extract')\n    t2 = EmptyOperator(task_id='load')\n\n    tg = DbtTaskGroup(\n        group_id='transform',\n        dbt_project_config=DbtProjectConfig(\n            project_path=Path('/path/to/example_dbt_project/'),\n            manifest_path=Path('/path/to/example_dbt_project/target/manifest.json'),\n        ),\n        dbt_profile_config=DbtProfileConfig(\n            profiles_path=Path('/path/to/example_dbt_project/profiles'),\n            target='dev',\n        ),\n        dbt_airflow_config=DbtAirflowConfig(\n            execution_operator=ExecutionOperator.BASH,\n        ),\n    )\n\n    t1 >> t2 >> tg\n```\n\n프로젝트를 Cloud Composer에 배포하는 관점에서 이번 반복에서는 아무런 변경 사항이 없었어요.\n\n<div class=\"content-ad\"></div>\n\n\n![Screenshot 1](/assets/img/2024-06-19-dbtAirflow_3.png)\n\nBy the end of this first iteration, we were able to meet half of the requirements we specified during ideation:\n\n![Screenshot 2](/assets/img/2024-06-19-dbtAirflow_4.png)\n\n## Iteration 2: Introducing Extra Tasks\n\n\n<div class=\"content-ad\"></div>\n\n이번 반복에서는 초기 설계의 핵심 측면으로 초점을 옮겼습니다: 기존 모든 모델을 Airflow로 원활하게 마이그레이션할 수 있는 능력입니다.\n\nMachine Learning 워크플로우를 모델 실행 사이에 통합하여 하향 모델이 필요로 하는 데이터를 생성할 수 있도록 목표를 설정했습니다. 이 접근은 결과적으로 강하게 결합된 시스템을 만들어 내어 최선의 방법에 부합하지 않을 수 있지만, 이는 당시 실행 흐름을 대표했습니다. dbt-airflow가 이 흐름을 재현할 수 있도록 보장하는 것이 우선이었고, 이러한 구성 요소를 분리하는 구조 변경을 고려하기 전에 기술적 부채에 대한 대응은 그때의 주요 초점이 아니었습니다.\n\n그러나 이것은 다른 작업을 가속화할 수 있는 가치 있는 기능이었습니다. 예를 들어, 특정 dbt 모델에서 데이터에 의존하는 워크플로우는 해당 모델이 완료된 직후에 즉시 트리거될 수 있었고, 전체 DAG가 완료될 때까지 기다릴 필요가 없었습니다.\n\n기술적으로는 관심있는 dbt 프로젝트를 구문 분석한 후 생성된 Airflow 작업 이후에 Airflow Operator를 도입하려고 했습니다. 이를 달성하기 위해 Extra Task의 상위 또는 하위 작업을 명시해야 했습니다.\n\n<div class=\"content-ad\"></div>\n\n'Extra Tasks'를 표현하기 위해 우리는 ExtraTask라는 객체를 개발했습니다. 이 객체는 렌더링된 dbt 프로젝트 내에서 이러한 작업을 소개하는 데 활용될 수 있습니다.\n\n```js\nfrom airflow.operators.python import PythonOperator\nfrom dbt_airflow.core.task import ExtraTask\n\n\nExtraTask(\n    task_id='test_task',\n    operator=PythonOperator,\n    operator_args={\n        'python_callable': lambda: print('Hello world'),\n    },\n    upstream_task_ids={\n        'model.example_dbt_project.int_customers_per_store',\n        'model.example_dbt_project.int_revenue_by_date',\n    },\n)\n```\n\n다음은 dbt-airflow의 Extra Task 기능을 활용하여 유명한 Sakila 프로젝트를 사용하여 생성된 더미 dbt 프로젝트를 렌더링하고 실행하는 예시 DAG입니다.\n\n```js\nimport functools\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import PythonOperator\nfrom dbt_airflow.core.config import DbtAirflowConfig\nfrom dbt_airflow.core.config import DbtProfileConfig\nfrom dbt_airflow.core.config import DbtProjectConfig\nfrom dbt_airflow.core.task import ExtraTask\nfrom dbt_airflow.core.task_group import DbtTaskGroup\nfrom dbt_airflow.operators.execution import ExecutionOperator\n\n\nwith DAG(\n    dag_id='test_dag',\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2),\n    },\n    'on_failure_callback': functools.partial(\n        our_callback_function_to_send_slack_alerts\n    ),\n) as dag:\n\n    t1 = EmptyOperator(task_id='extract')\n    t2 = EmptyOperator(task_id='load')\n\n    extra_tasks = [\n        ExtraTask(\n            task_id='test_task',\n            operator=PythonOperator,\n            operator_args={\n                'python_callable': lambda: print('Hello world'),\n            },\n            upstream_task_ids={\n                'model.example_dbt_project.int_customers_per_store',\n                'model.example_dbt_project.int_revenue_by_date',\n            },\n        ),\n        ExtraTask(\n            task_id='another_test_task',\n            operator=PythonOperator,\n            operator_args={\n                'python_callable': lambda: print('Hello world 2!'),\n            },\n            upstream_task_ids={\n                'test.example_dbt_project.int_customers_per_store',\n            },\n            downstream_task_ids={\n                'snapshot.example_dbt_project.int_customers_per_store_snapshot',\n            },\n        ),\n        ExtraTask(\n            task_id='test_task_3',\n            operator=PythonOperator,\n            operator_args={\n                'python_callable': lambda: print('Hello world 3!'),\n            },\n            downstream_task_ids={\n                'snapshot.example_dbt_project.int_customers_per_store_snapshot',\n            },\n            upstream_task_ids={\n                'model.example_dbt_project.int_revenue_by_date',\n            },\n        )\n    ]\n\n    tg = DbtTaskGroup(\n        group_id='my-dbt-project',\n        dbt_project_config=DbtProjectConfig(\n            project_path=Path('/path/to/example_dbt_project/'),\n            manifest_path=Path('/path/to/example_dbt_project/target/manifest.json'),\n        ),\n        dbt_profile_config=DbtProfileConfig(\n            profiles_path=Path('/path/to/example_dbt_project/profiles'),\n            target='dev',\n        ),\n        dbt_airflow_config=DbtAirflowConfig(\n            extra_tasks=extra_tasks,\n            execution_operator=ExecutionOperator.BASH,\n        ),\n    )\n\n    t1 >> t2 >> tg\n```\n\n<div class=\"content-ad\"></div>\n\n두 번째 반복을 마칠 때, 우리는 더욱 우리가 목표로 한 최종 제품에 한 발짝 더 가까워졌어요.\n\n![이미지](/assets/img/2024-06-19-dbtAirflow_5.png)\n\n## 반복 3: 필터링 태그 및 다중 스케줄 생성\n\n이 반복에서, 우리의 목표는 특정 DAG 내에서 특정 dbt 엔티티의 하위 집합만 렌더링할 수 있게 하는 기능을 구현하는 것이었어요. 이는 여러 목적을 달성하기 위한 것이었죠:\n\n<div class=\"content-ad\"></div>\n\n- 동일한 모델에 대해 시간별, 일별 또는 주간별로 다른 일정을 생성하려면 해당 태그가 지정된 모델만 선택하여 선택하면 됩니다.\n- 특정 이유로 특정 태그가 지정된 모델을 특정 DAG에 포함하지 않으려면 필터링을 해야 합니다.\n- 다른 워크플로에서 특정 시점에 일부 모델을 새로 고침해야 할 때 일부 모델만 렌더링해야 할 수 있습니다.\n\n이를 달성하기 위해, DbtAirflowConfig 객체에 'include_tags'와 'exclude_tags' 두 가지 추가 인수를 도입했습니다. 두 인수 모두 렌더링된 프로젝트에서 Entity를 포함하거나 제외하는 데 사용되는 dbt 태그와 일치하는 문자열 목록을 수용합니다.\n\n이 기능 추가의 주요 도전 과제는 필터링된 노드가 올바른 종속성을 유지하는 것이어서 예상보다 복잡했습니다.\n\n이 반복에서 우리는 이 프로젝트를 시작할 때 목표로 했던 최종 버전에 한 걸음 가까워졌습니다!\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-dbtAirflow_6.png\" />\n\n## 이터레이션 4: 컨테이너화된 프로젝트 실행\n\n에어플로우에서 BashOperator를 통해 dbt 명령을 실행할 때는 Airflow 환경 내에서 결과가 자료화되는 대상 시스템을 기준으로 dbt-core 및 해당 dbt 어댑터를 설치하는 것이 중요합니다.\n\n그러나 이 접근 방식은 패키지의 호환성 문제 등의 위험과 어려움을 야기할 수 있습니다. 특히 제공 업체의 다른 클라우드 서비스와의 통합을 용이하게하기 위해 미리 정의된 종속성을 함께 제공하는 클라우드의 관리형 Airflow 서비스인 클라우드 컴포저와 같은 서비스를 사용할 때 이 문제가 발생할 가능성이 높아집니다.\n\n<div class=\"content-ad\"></div>\n\n그리고, 여러 개의 dbt 프로젝트를 관리하는 것은 서로 다른 dbt 버전에서 실행해야 할 수도 있습니다. 안타까운 점은 Airflow 환경에 dbt를 직접 설치하면 기술적으로 이것이 불가능하다는 것입니다.\n\n이러한 도전에 대처하고 버전 관리의 유연성을 보장하기 위해 우리는 dbt 프로젝트를 컨테이너화하고 k8s에서 실행하기로 결정했습니다. 이 방법은 다양한 사용 사례를 지원할뿐만 아니라 필요에 따라 dbt 버전을 업그레이드 또는 다운그레이드할 수 있는 유연성을 제공합니다.\n\n![이미지](/assets/img/2024-06-19-dbtAirflow_7.png)\n\n새 CI/CD 파이프라인이 트리거될 때마다, dbt 프로젝트를 포함하는 Docker 이미지가 빌드되어 Google Cloud의 Artifact Registry로 푸시됩니다. 그러나 dbt-airflow는 여전히 DAG를 렌더링하기 위해 manifest.json 파일에 의존하므로 전체 dbt 프로젝트를 복사하는 대신 manifest 파일만 Cloud Composer GCS 버킷으로 복사해야 합니다.\n\n<div class=\"content-ad\"></div>\n\ndbt-airflow에서는 라이브러리를 확장하여 KubernetesPodOperator를 사용하여 Airflow Tasks를 실행하는 지원을 추가했어요. 이 기능을 통해 DbtAirflowConfig 내에서 execution_operator=ExecutionOperator.KUBERNETES를 지정할 수 있게 되었어요. 이 설정을 사용하면 TaskGroup 내의 각 작업이 자체 Kubernetes pod에서 실행되며 지정된 컨테이너 및 추가 구성을 활용할 수 있어요.\n\n```python\nimport functools\nfrom datetime import datetime \nfrom datetime import timedelta\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import PythonOperator\nfrom dbt_airflow.core.config import DbtAirflowConfig\nfrom dbt_airflow.core.config import DbtProfileConfig\nfrom dbt_airflow.core.config import DbtProjectConfig\nfrom dbt_airflow.core.task_group import DbtTaskGroup\nfrom dbt_airflow.operators.execution import ExecutionOperator\n\n\nwith DAG(\n    dag_id='test_dag',\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2),\n    },\n    'on_failure_callback': functools.partial(\n        our_callback_function_to_send_slack_alerts\n    ),\n) as dag:\n\n    t1 = EmptyOperator(task_id='extract')\n    t2 = EmptyOperator(task_id='load')\n\n    tg = DbtTaskGroup(\n        group_id='transform',\n        dbt_airflow_config=DbtAirflowConfig(\n            create_sub_task_groups=True,\n            execution_operator=ExecutionOperator.KUBERNETES,\n            operator_kwargs={\n                'name': f'dbt-project-1-dev',\n                'namespace': 'composer-user-workloads',\n                'image': 'gcp-region-docker.pkg.dev/gcp-project-name/ar-repo/dbt-project-1:latest',\n                'kubernetes_conn_id': 'kubernetes_default',\n                'config_file': '/home/airflow/composer_kube_config',\n                'image_pull_policy': 'Always',\n            },\n        ),\n        dbt_project_config=DbtProjectConfig(\n            project_path=Path('/home/project-1/'),  # 도커 컨테이너 내의 경로\n            manifest_path=Path('/home/airflow/gcs/dags/dbt/project-1/target/manifest.json'),  # Cloud Composer GCS 버킷에 있는 경로\n        ),\n        dbt_profile_config=DbtProfileConfig(\n            profiles_path=Path('/home/project-1/profiles/'),  # 도커 컨테이너 내의 경로\n            target=dbt_profile_target,\n        ),\n    )\n\n    t1 >> t2 >> tg\n```\n\n그리고 이 기능 추가로 프로젝트 초반에 지정된 초기 요구 사항 집필을 성공적으로 구현했어요.\n\n<img src=\"/assets/img/2024-06-19-dbtAirflow_8.png\" />\n\n<div class=\"content-ad\"></div>\n\n# dbt-airflow을 시작해 보세요\n\ndbt 프로젝트를 배포하고 일정을 예약하는 방법을 찾고 있다면, dbt-airflow를 사용하면 귀찮음을 덜 수 있습니다. Plum의 데이터팀은 이미 1년 이상 운영 중인 이 도구를 안정적이고 확장 가능하다고 입증했습니다. 그래서 여러분의 프로젝트에도 실제 가치를 제공할 수 있다고 자신합니다. 이 패키지는 플랫폼에 구애받지 않으며 dbt에서 지원하는 모든 대상과 함께 사용할 수 있습니다.\n\n이 프로젝트는 GitHub에서 유지되고 PyPI에서도 사용할 수 있습니다.\n\n또한 공식 문서에서 더 많은 세부 정보를 찾을 수 있습니다. 사실, 이 문서에서는 패키지가 제공하는 전체 기능 중 일부만 다루었습니다.\n\n<div class=\"content-ad\"></div>\n\n프로젝트에 기여하는 것을 장려합니다. 무언가 부족한 부분이 있으면 참여해 주시기를 바랍니다.\n\n코딩 즐기세요,\n\nPlum Data Engineering Team ❤","ogImage":{"url":"/assets/img/2024-06-19-dbtAirflow_0.png"},"coverImage":"/assets/img/2024-06-19-dbtAirflow_0.png","tag":["Tech"],"readingTime":16},{"title":"데이터 엔지니어를 위한 자료 구조와 알고리즘 면접 질문","description":"","date":"2024-06-19 09:44","slug":"2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer","content":"\n\n데이터 엔지니어링 면접 준비는 데이터 구조와 알고리즘(DSA)에 대한 튼튼한 이해력이 필요합니다. 여러분이 잘 준비될 수 있도록, 자주 물어지는 DSA 면접 질문들을 정리한 목록을 제공해 드립니다. 연습 문제 링크가 포함되어 있어 다양한 주제를 다루며 다음 면접에 잘 준비될 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer_0.png)\n\n## 가장 자주 물어지는 DSA 문제 목록\n\n- 두 수의 합: [문제 링크](https://leetcode.com/problems/two-sum/description/)\n- 중복되는 문자가 없는 가장 긴 부분 문자열: [문제 링크](https://leetcode.com/problems/longest-substring-without-repeating-characters/)\n- 합이 k인 부분 배열: [문제 링크](https://leetcode.com/problems/subarray-sum-equals-k/description/)\n- 중복 숫자 찾기: [문제 링크](https://leetcode.com/problems/find-the-duplicate-number/description/)\n- 배열에서 Leaders: [문제 링크](https://www.geeksforgeeks.org/problems/leaders-in-an-array-1587115620/1)\n- 첫 번째 문자열에서 두 번째 문자열에 있는 문자 제거: [문제 링크](https://www.geeksforgeeks.org/remove-characters-from-the-first-string-which-are-present-in-the-second-string/)\n- 두 수 더하기: [문제 링크](https://leetcode.com/problems/add-two-numbers/description/)\n- 0과 1 문제: [문제 링크](https://leetcode.com/problems/ones-and-zeroes/description/)\n- 주식 매수와 매도의 최적 시기 II: [문제 링크](https://leetcode.com/problems/best-time-to-buy-and-sell-stock-ii/description/)\n- 최대 연속 1 개: [문제 링크](https://leetcode.com/problems/max-consecutive-ones/description/)\n- 끝에서 N번째 노드 제거: [문제 링크](https://leetcode.com/problems/remove-nth-node-from-end-of-list/description/)\n- 무작위 포인터가 있는 목록 복사: [문제 링크](https://leetcode.com/problems/copy-list-with-random-pointer/)\n- 문자열 뒤집기: [문제 링크](https://leetcode.com/problems/reverse-string/description/)\n- 첫 번째 누락된 양수: [문제 링크](https://leetcode.com/problems/first-missing-positive/description/)\n- 연속적인 1의 최대 개수 III: [문제 링크](https://leetcode.com/problems/max-consecutive-ones-iii/description/)\n- 홀수 짝수 연결 목록: [문제 링크](https://leetcode.com/problems/odd-even-linked-list/)\n- 목록 분할: [문제 링크](https://leetcode.com/problems/partition-list/description/)\n- 연결된 목록 순환이 있습니까? [문제 링크](https://leetcode.com/problems/linked-list-cycle/description/)\n- 유효한 괄호: [문제 링크](https://leetcode.com/problems/valid-parentheses/description/)\n- 부분 문자열의 모든 발생 제거: [문제 링크](https://leetcode.com/problems/remove-all-occurrences-of-a-substring/description/)\n- 첫 번째 나타나는 아닌 반복 문자 찾기: [문제 링크](https://www.geeksforgeeks.org/given-a-string-find-its-first-non-repeating-character/)\n- 목록에서 양수와 음수 수 세기: [문제 링크](https://www.geeksforgeeks.org/python-program-to-count-positive-and-negative-numbers-in-a-list/)\n- 목록에서 두 번째로 큰 숫자 찾기: [문제 링크](https://www.geeksforgeeks.org/python-program-to-find-second-largest-number-in-a-list/)\n- 최대 유효 괄호: [문제 링크](https://leetcode.com/problems/longest-valid-parentheses/description/)\n- 목록 뒤집기: [문제 링크](https://www.geeksforgeeks.org/python-reversing-list/)\n- 가장 긴 공통 접두어: [문제 링크](https://leetcode.com/problems/longest-common-prefix/)\n- 최대 부분 배열: [문제 링크](https://leetcode.com/problems/maximum-subarray/description/)\n- 평균 최대 부분 배열 I: [문제 링크](https://leetcode.com/problems/maximum-average-subarray-i/description/)\n- 배열의 곱: [문제 링크](https://leetcode.com/problems/product-of-array-except-self/description/)\n- 마지막 단어의 길이: [문제 링크](https://leetcode.com/problems/length-of-last-word/description/)\n- 문자열 뒤집기: [문제 링크](https://leetcode.com/problems/reverse-string/description/)\n- 목록 정렬: [문제 링크](https://leetcode.com/problems/sort-list/)\n- 최소 부분 배열 합: [문제 링크](https://leetcode.com/problems/minimum-size-subarray-sum/description/)\n- 최소 창 하위 문자열: [문제 링크](https://leetcode.com/problems/minimum-window-substring/description/)\n- 배열 K회 회전하기: [문제 링크](https://leetcode.com/problems/rotate-array/description/)\n- 과반 요소: [문제 링크](https://leetcode.com/problems/majority-element/description/)\n- 가장 긴 팰린드롬 부분 문자열: [문제 링크](https://leetcode.com/problems/longest-palindromic-substring/)\n- 엑셀 시트 열 제목: [문제 링크](https://leetcode.com/problems/excel-sheet-column-title/description/)\n- 애너그램 확인: [문제 링크](https://leetcode.com/problems/valid-anagram/description/)\n- 주식 매수 및 매도 최적 시기 III: [문제 링크](https://leetcode.com/problems/best-time-to-buy-and-sell-stock-iii/description/)\n- 오른쪽에서 가장 큰 요소로 요소 바꾸기: [문제 링크](https://leetcode.com/problems/replace-elements-with-greatest-element-on-right-side/description/)\n- 문장에서 가장 흔한 단어: [문제 링크](https://leetcode.com/problems/most-common-word/description/)\n- 주어진 합계를 갖는 부분 배열 찾기: [문제 링크](https://www.geeksforgeeks.org/find-subarray-with-given-sum/)\n- 단어 분리: [문제 링크](https://leetcode.com/problems/word-break/description/)\n- 문자열에서 첫 번째 발생지의 인덱스 찾기: [문제 링크](https://leetcode.com/problems/find-the-index-of-the-first-occurrence-in-a-string/description/)\n- 최대 연속 1 개: [문제 링크](https://leetcode.com/problems/max-consecutive-ones/description/)\n- 연속 배열: [문제 링크](https://leetcode.com/problems/contiguous-array/description/)\n- 주식 매수 및 매도 최적 시기: [문제 링크](https://leetcode.com/problems/best-time-to-buy-and-sell-stock/description/)\n- 쌍으로 노드 교환: [문제 링크](https://leetcode.com/problems/swap-nodes-in-pairs/)\n- 두 수 II: [문제 링크](https://leetcode.com/problems/two-sum-ii-input-array-is-sorted/description/)\n\n<div class=\"content-ad\"></div>\n\n이 문제들을 연습함으로써, 데이터 엔지니어링 인터뷰에서 가장 일반적인 자료 구조와 알고리즘 문제에 대비할 준비가 더 잘 될 것입니다. 행운을 빕니다!","ogImage":{"url":"/assets/img/2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer_0.png"},"coverImage":"/assets/img/2024-06-19-DataStructuresandAlgorithmsInterviewQuestionsforDataEngineer_0.png","tag":["Tech"],"readingTime":6},{"title":"관계형 데이터베이스에서 데이터 레이크하우스로 데이터 관리의 간단한 역사","description":"","date":"2024-06-19 09:43","slug":"2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement","content":"\n\n## 데이터 관리의 진화 및 데이터 엔지니어링을 위한 솔루션\n\n비즈니스 인텔리전스와 데이터 엔지니어링은 산업 분야에서 비교적 새로운 분야입니다. 20세기 초부터 어느 정도 비즈니스 분석이 수행되어 왔습니다. 그러나 디지털 정보의 대량 분석이 필요한 건 정보 시대에만 발생한 문제입니다. 간단합니다 – 데이터를 소유(또는 수집)하고 그로부터 더 나은 통찰력을 얻는 사람들이 성공할 것입니다.\n\n데이터 엔지니어로써, 데이터 관리 접근 방식이 역사를 통해 어떻게 변화해왔는지, 데이터 처리와 관련된 문제를 어떻게 해결해 왔는지 이해하는 것은 흥미로운 일입니다. 역사에 대해 간단히 탐험해보고 데이터 관리 방식이 어떻게 시간이 지남에 따라 발전해 왔는지 알아보겠습니다.\n\n![이미지](/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_0.png)\n\n<div class=\"content-ad\"></div>\n\n## 1980년대\n\n1980년대에는 대형 기업들이 관계형 SQL 데이터베이스를 사용하여 질의를 수행하여 통찰을 얻었습니다. SQL 기술이 1974년경에 처음 등장했기 때문에 이 주요 도구가 현재 50년이라는 점을 인식하는 것이 중요합니다. 기업들이 분석 질의를 위한 관계형 데이터 관리에서 제한을 경험하면서 비즈니스 데이터 웨어하우스라는 개념이 소개되었습니다.\n\n지금은 익숙하지만, 그 당시에는 이것이 상당한 혁신이었습니다. 이 접근 방식의 주요 성과는 다음과 같습니다:\n\n- 더 빠른 비즈니스 인텔리전스 (BI) 프로세스.\n- 구조화된 데이터와 효율적으로 작업할 수 있는 능력.\n\n<div class=\"content-ad\"></div>\n\n그러나 주목할만한 단점들도 있었어요:\n\n- 반정형 및 비정형 데이터를 지원하지 않는 한계.\n- 규모와 속도에 대한 도전.\n- 대용량 데이터 크기를 처리할 때 장기간의 처리 시간이 발생할 수 있음.\n\n![이미지](/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_1.png)\n\n## 2000–2010년대\n\n<div class=\"content-ad\"></div>\n\n2000년대에 대형 데이터가 등장하면서 전통적인 데이터 웨어하우스에 도전이 제기되었습니다. Google과 Yahoo와 같은 기업들은 구조화되지 않은 대량의 데이터를 생성했는데, 이는 구조화된 데이터 웨어하우스로는 효과적으로 처리할 수 없는 양이었습니다. 게다가 대규모의 구조화되지 않은 데이터(예: 이미지)가 필요한 머신 러닝의 등장은 기존 데이터 관리 시스템의 한계를 더욱 부각시켰습니다.\n\n이에 대응하여 데이터 레이크 개념이 소개되었는데, Google이 대형 데이터의 분산 처리를 위해 MapReduce를 선도했습니다. 이는 Hadoop의 개발로 이어지며, Hadoop 분산 파일 시스템(HDFS)을 저장 계층으로 사용하여 효율적인 데이터 저장 및 처리가 가능하게 했습니다. 이후에는 MapReduce와 이후에는 Spark를 사용하여 데이터를 처리할 수 있었습니다.\n\n데이터 레이크는 다음과 같은 장점을 제공했습니다:\n\n- 비용 효율적인 클라우드 저장소.\n- 머신 러닝(ML) 지원.\n- 유연한 데이터 저장.\n- 스트리밍 데이터 처리.\n\n<div class=\"content-ad\"></div>\n\n따라서, 테이블 태그를 마크다운 형식으로 변경하면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n## 2020년대\n\n관계형 데이터베이스를 사용하는 것에서 분산 저장 시스템을 활용해 대용량 데이터를 처리하는 것으로 발전했습니다. 대용량 데이터의 문제를 해결한 것 같았지만, 새로운 도전이 나타났습니다. 기업들은 여전히 데이터를 완전히 활용하기 어려워하며, 상당량의 데이터를 활용하지 못한 채 놓아두고 있습니다.\n\n2020년에 Databrick은 데이터 관리의 혁신적인 접근 방식인 데이터 레이크하우스(Data Lakehouse)에 대한 중요 논문을 발표했습니다. 이 방식은 데이터 웨어하우스와 데이터 레이크를 결합하여 클라우드 저장 서비스의 비용 효율성을 활용하면서 더 '웨어하우스'적인 방식으로 운영합니다. 데이터 레이크하우스는 BI 도구를 활용하고 데이터 과학/머신러닝 솔루션에 의존하는 대기업들에게 특히 유용합니다.\n\n이 접근을 옹호하는 회사가 Databricks이지만, 대부분의 도구가 오픈 소스이기 때문에 클라우드에서 이러한 솔루션을 독립적으로 구축하는 것도 가능합니다. 데이터 레이크하우스 시스템을 다룰 때는 저장 수준에서 적절한 기술을 갖추는 것이 중요합니다. 이러한 시스템은 table formats으로 언급되며, Apache Iceberg, Delta Lake, Apache Hudi와 같은 가장 일반적으로 사용되는 형식들이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n기타 측면에서는 이전 솔루션과 유사하며 변환을 위해 Spark를 사용하는 것이 일반적입니다. 그러나 구체적인 툴킷은 회사마다 및 서로 다른 클라우드 제공업체 사이에 상당히 다를 수 있습니다. 데이터 레이크하우스는 지속적으로 발전하고 있으며 매년 새로운 기술이 등장하여 대규모 데이터를 처리하기가 더 쉬워지고 있습니다.\n\n![image](/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_3.png)\n\n# 결론\n\n요약하자면, 데이터 관리 솔루션의 진화에 대한 완전한 여정을 완료했습니다, 주로 분석을 위한 것입니다. 모든 접근 방식에는 고유의 사용 사례가 있으며 전 세계의 기업 및 팀에서 적극적으로 활용되고 있습니다. 내 의견으로는 데이터 웨어하우스가 가장 이해하기 쉬운 것으로, SQL 관계형 데이터베이스와 매우 유사합니다. 그러나 데이터 레이크하우스도 SQL을 언어 중 하나로 사용하지만 기술은 더 복잡합니다.\n\n<div class=\"content-ad\"></div>\n\n선택을 할 때는 다음 질문을 고려해 보세요:\n\n- 회사에서 개발 중인 ML 및 AI 도구가 있나요? 이들에게 데이터를 제공해야 할 필요가 있나요? 그렇지 않다면, 구조화된 데이터는 문제 해결에 충분하며 데이터 웨어하우스 솔루션을 사용할 수 있습니다.\n- 데이터의 크기를 고려해 보세요. 데이터 웨어하우스에서 저장 공간은 비실할 수 있습니다.\n- 마지막으로, 데이터 팀의 품질도 중요한 문제입니다. 데이터 레이크하우스를 사용하는 경우, 비용이 더 많이 드는 데이터 엔지니어 몇 명이 필요합니다.\n\n## 개인적인 반성:\n\n데이터 관리의 진화에 대한 간략한 탐험을 통해 다양한 접근 방법에 대해 더 잘 이해하실 수 있게 되었다고 믿습니다. 이 정보는 해결책을 비교하고 문제에 적합한 것을 선택할 때 필수적입니다.","ogImage":{"url":"/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_0.png"},"coverImage":"/assets/img/2024-06-19-RelationalDatabasestoDataLakehousesABriefHistoryofDataManagement_0.png","tag":["Tech"],"readingTime":4},{"title":"높은 영향력을 가진 데이터 거버넌스 팀","description":"","date":"2024-06-19 09:40","slug":"2024-06-19-High-impactdatagovernanceteams","content":"\n\n## 영향을 주는 방법, 초점을 맞출 곳, 최고의 데이터 관리 팀에서 성공하기 위해 필요한 기술\n\n고영향력을 지닌 데이터 관리 팀에 대한 가장 좋은 비유는 원활하게 작동하는 주방과 유사합니다. 그들은 주방을 깨끗하게 유지하고 모든 칼이 날카롭고 모든 물건이 제 위치에 있도록 돕습니다. 이는 요리사들이 더 빨리 작업하게 하고 실수를 줄이며 나쁜 식품 위생 평가를 방지합니다.\n\n![고영향 데이터 관리 팀](/assets/img/2024-06-19-High-impactdatagovernanceteams_0.png)\n\n하지만 데이터 관리 팀은 선을 넘지 않아야 합니다. 조심하지 않으면 이들의 노력은 추가 부담처럼 보일 수 있고, 사업에 미미한 영향을 가져올 수 있는 정책과 프로세스를 도입할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 고영향 데이터 거버넌스 팀이 주로 초점을 맞춰야 할 영역 및 취해야 할 구체적인 단계를 살펴봅니다.\n\n![이미지](/assets/img/2024-06-19-High-impactdatagovernanceteams_1.png)\n\n데이터 거버넌스 팀이 어떻게 작동해야 하는지에 대해 일반화된 해결책은 없지만, 이러한 주요 주제들은 대부분의 주요 기업 채용 공고에서 요구 사항으로 계속해서 나타납니다.\n\n- 조직 전체에서 데이터 활용 가능하게 만들기\n- 데이터 품질을 유지하고 최우선 사항을 체계적으로 개선하는 데 도움\n- 소유권 모델을 통해 책임이 명확하게 보장\n- 리스크, 개인 정보 보호 및 규정 준수 관리\n\n<div class=\"content-ad\"></div>\n\n# 조직 전체에서 데이터를 활용할 수 있도록\n\n문제는 거의 결코 데이터가 충분하지 않다는 것이 아닙니다. 오히려 기존 데이터가 활용되지 않거나 사람들이 찾을 수 없다는 문제입니다. 조직이 성장함에 따라 생성되는 데이터량은 상당히 증가합니다. 시간이 지남에 따라 이에는 여러 가지 영향이 있습니다.\n\n이것은 천천히 발생하다가 갑자기 전개됩니다.\n\n팀 간의 간헐적인 노력으로는 이 문제를 해결하기 어렵습니다. 데이터 거버넌스 팀은 이를 표준화하고 촉진하는 데 도움을 줄 수 있는 독특한 위치에 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 중요한 데이터 자산 문서화\n\n데이터 관리 팀은 데이터 문서화에 대한 기대치를 설정하고 시행하여 후순위가 되지 않도록 해야 합니다. 예를 들어:\n\n- 데이터 카탈로그나 dbt yml 파일에 문서 작성이 이루어지도록 강제합니다. 이를 통해 팀간에 일관된 접근 방식을 구축할 수 있습니다.\n- 문서화는 ‘골드’ 표준 테이블에 필수로 이루어져야 하지만 ‘브론즈’와 ‘실버’는 선택 사항임을 명확히 합니다.\n- 잘 문서화된 테이블이나 필드의 명확한 예시를 제시합니다 (예: ‘세일즈포스 식별자에 의해 정의된 고객의 사용자 ID’와 같이 작성하며, ‘고객들의 사용자 ID’ 등과 같이 작성하지 않습니다).\n\n## 핵심 지표가 일관된 방식으로 정의되도록 보장하기\n\n<div class=\"content-ad\"></div>\n\n어떤 규모에 도달하면, 동일한 지표에 대한 서로 다른 정의를 반드시 마주하게 됩니다. 기본 데이터가 잘못되었다고 할지라도, 이는 의사 결정이 더뎌지고, 미팅에서는 지표가 올바른지 논의하는 데 시간을 보내게 만듭니다. 그 대신에 이에 대해 어떻게 대처할지에 대해 논의할 시간이 생깁니다.\n\n데이터 가버넌스 팀은 명확한 지침을 설정해야 합니다. 그러한 지침 중 하나는 다음과 같을 수 있습니다: \"주요 지표는 가시성을 향상하고 버전 관리하는 데 dbt에서 정의되어야 하며 BI 도구의 계산된 필드가 아니어야 합니다.\" 다른 지침으로는 주요 지표에 RAG(빨강, 주황, 초록) 임계값을 메트릭 정의와 함께 포함시켜 이를 한 곳에서 유지할 수 있도록 하는 것이 있습니다. 이는 해당 임계값이 위반될 경우 관련 이해 관계자에게 자동 경고를 설정할 수 있다는 장점이 있습니다.\n\n데이터 가버넌스 팀은 동일한 지표에 대한 다양한 정의를 찾고, 여러 정의를 발견하면 팀에 이를 통합하도록 장려해야 합니다. 이는 핵심 대시보드, Slack 채널 및 KPI가 사용되는 다른 위치를 정기적으로 평가함으로써 가장 잘 수행됩니다.\n\n## 데이터 검색 가능하게 하기\n\n<div class=\"content-ad\"></div>\n\n만약 데이터 이용자들이 — 데이터 팀 내에서든 외부에서든 — 자신의 데이터를 찾지 못한다면, 그 데이터는 사실상 존재하지 않는 것과 다름없습니다. 필요한 데이터를 찾는 것이 마치 건초더미 속에서 바늘을 찾는 것처럼 느껴진다면, 문제의 일부 해결책은 툴셋을 업데이트하고 데이터 카탈로그를 도입하여 모든 데이터 자산을 발견할 수 있게 하는 것일 수 있습니다. 하지만 기본을 올바르게 설정하지 않는다면 어떤 도구도 도움이 되지 않을 것입니다.\n\n데이터 거버넌스 팀은 간단하지만 표준화된 규칙을 시행해야 합니다. 예를 들어, 데이터 자산에 대한 명확한 명명 규칙을 정하면 좋습니다. 대시보드의 경우 '⭐️ 제품 A/B 테스트 추적 [사업용 은행]'과 같이 명명 규칙을 사용하여 해당 대시보드가 중요(⭐️)하다는 것과 어떤 도메인에 속하는지 명확히 할 수 있습니다 (사업용 은행).\n\n사용 중인 데이터가 있는 곳에 가능한 가까이 위치시키는 것이 좋습니다. Looker와 같은 BI 툴을 사용하면 대부분의 데이터 이용자가 데이터를 소비할 수 있습니다. 각 도메인으로 진입할 수 있는 잘 구조화된 '홈페이지'를 만들어서 가장 중요한 대시보드에 대한 링크를 제공하는 것은 대시보드를 조직화하고 모든 사람이 쉽게 찾을 수 있도록 하는 좋은 방법입니다.\n\n<div class=\"content-ad\"></div>\n\n## 대시보드의 사용성 강화\n\n당신이 좋아하든 싫어하든, 얼마나 멋진 데이터 모델과 파이프라인을 만들든간에, 이를 소비하는 이해관계자들의 대시보드 경험은 데이터와 당신의 팀의 산출물에 대한 인식에 큰 영향을 미칠 것입니다. 대시보드를 제품처럼 다루어 외관, 느낌, 일관성, 그리고 성능을 강화해야 합니다.\n\nTypeform은 Looker의 개선 사항을 정리하면서 추가 단계를 거쳤습니다. 이들이 어떻게 그것을 수행했는지 여기에서 자세히 읽어보세요.\n\n![이미지](/assets/img/2024-06-19-High-impactdatagovernanceteams_3.png)\n\n<div class=\"content-ad\"></div>\n\n## 불필요한 데이터 자산 제거\n\n데이터 자산의 확산은 데이터가 시간이 지남에 따라 사용하기 어려워지는 주요 이유 중 하나입니다. 데이터 스택을 정리하는 가장 효과적인 방법은 필요하지 않은 데이터 자산을 제거하는 것입니다. 필요하지 않은 데이터 자산의 명확한 정의는 없지만, 주의해야 할 신호는 다음과 같습니다.\n\n- 사용률이 낮은 대시보드\n- 다른 대시보드와 크게 중첩된 대시보드\n- 하류 종속성이 없는 데이터 모델\n- 다운스트림에서 사용되지 않는 데이터 모델의 열\n\n불필요한 데이터 스택을 정리하는 것은 쉬운 작업이 아니며, 가능하다면 사용되지 않는 테이블, 데이터 모델, 열 및 대시보드를 계속해서 제거하는 것에 투자해야 합니다. 그러나 많은 확장 중인 기업은 천 개 이상의 데이터 모델과 대시보드가 생기기 전까지는 뒷걸음치기를 하다가 이 문제에 대처해야 하는 경우가 많습니다.\n\n<div class=\"content-ad\"></div>\n\n요번에 전해드릴 이야기는 Looker 대시보드의 수를 급격히 줄이고 자가 서비스를 거의 불가능하게 만들었던 회사의 이야기입니다.\n\n- 데이터 팀은 Looker 사용량을 위한 Looker 대시보드를 만들고 매월 #data-team 슬랙 채널로 보고서를 보냈습니다 (Looker를 통해 시스템 활동 탐색에서 이 데이터에 액세스할 수 있습니다). 보고서에는 a) 대시보드 수, b) 조회 수, c) 사용자 수, d) 임원 (VP/C-레벨) 사용자 수가 포함되어 있었습니다.\n- 대시보드를 제작한 도메인과 사용량으로 정렬할 수 있는 기능을 통해 사용하지 않는 대시보드와 삭제 가능한 대시보드 소유자를 빠르게 확인할 수 있었습니다.\n- 팀은 \"계층별 대시보드\" 개념을 도입했습니다. 계층 1 대시보드의 예시로는 보드 수준의 KPI 대시보드가 있을 수 있고, 계층 3 대시보드는 소규모 제품 팀용일 수 있습니다. 계층 1 대시보드에 대해 디자인 일관성, 피어 리뷰 요구 사항 및 사용 모니터링의 구체적인 요구 사항을 정했습니다.\n- 임원 사용자가 정기적으로 계층 1이 아닌 대시보드를 사용할 경우, 해당 대시보드는 계층 1 표준으로 검토되었습니다.\n\n이와 비슷한 단계를 데이터 모델을 정리하는 데 사용할 수 있습니다. 특히 dbt에서 잘 구성된 열 수준의 계보가 있는 경우 팀이 크게 혜택을 입었습니다. 이는 dbt에서 비즈니스 인텔리전스 도구까지 연장되는 정확하고 빠른 후속 영향을 평가하는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n## 중요한 데이터를 정의하세요\n\n규모에 관계없이, 데이터 거버넌스 팀의 의도가 얼마나 좋더라도 모든 데이터에서 높은 사용성을 시행하는 것은 불가능해집니다. 데이터 거버넌스 팀은 데이터 중요성이 어떻게 정의되어야 하며 이에 대한 영향이 무엇인지 명확하게 설정하는 것이 중요합니다. 예를 들어, '브론즈, 실버, 골드' 모델을 사용하고 실버 및 골드만 문서화 표준을 준수해야 한다고 결정할 수 있습니다.\n\n이 연습을 완료하는 동안 데이터를 체인으로 고려하고 하류 사용 사례에 되돌아갈 수 있도록하며 중요합니다.\n\n![Image](/assets/img/2024-06-19-High-impactdatagovernanceteams_5.png)\n\n<div class=\"content-ad\"></div>\n\n당신의 비즈니스 중요 데이터를 식별하는 방법을 안내하는 가이드를 읽어보세요. 비즈니스 중요 데이터 모델과 대시보드를 식별하는 실용적인 단계와 데이터에 대한 신뢰를 증진하는 방법을 알려드립니다.\n\n데이터 품질을 주의 깊게 살피면서 중요한 것을 체계적으로 개선하는 데 도움이 됩니다.\n\n데이터 팀이 직면하는 가장 큰 어려움으로 일관적으로 순위에 있던 것이 데이터 품질입니다. dbt는 최근 수천 명의 데이터 전문가들에게 최대 어려움에 대해 물었고, 57%의 표를 받아 데이터 품질이 가장 중요한 문제로 떠오르는군요.\n\n이에는 좋은 이유가 있습니다. 낮은 데이터 품질은 부정확한 결정, 시스템 장애 및 데이터에 대한 신뢰 손실로 이어질 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다행히 대부분의 데이터 전문가들은 검증된 데이터가 데이터 자산을 개발하는 일부분이라는 것에 익숙해졌습니다. 개별 자산에 적합하지만 개인의 해석에 많은 부분을 맡길 수 있습니다. 우리는 데이터 거버넌스 팀이 소유자 도메인 및 중요성과 같은 주요 차원을 횡단하여 데이터 품질을 체계적으로 평가하는 것을 권장합니다.\n\n## 데이터 품질 보고\n\n데이터 품질에 대한 보고를 하는 이유는 많지만 각 팀에게 자체 메트릭을 정의하도록 요청하는 것은 혼란을 초래하는 확실한 방법입니다. 데이터 품질을 일관되게 측정하고 전달하는 것은 비즈니스 결과에 여러 이점을 가져다 줄 수 있습니다:\n\n- 가동 시간 — 각 실행에서 성공적으로 통과하는 설치된 제어의 %는 무엇인가요?\n- 커버리지 — 필요한 제어가 설치된 데이터 자산의 %는 얼마나 되나요?\n\n<div class=\"content-ad\"></div>\n\n**품질 지표를 의미론적으로 유사한 영역으로 그룹화하면 비즈니스 사용 사례에 더 가까운 방식으로 데이터 품질에 대해 이야기할 수 있습니다. (1) 시기 적절성-데이터가 비즈니스와 합의된 SLA에 따라 신선하고 최신한가, (2) 정확성-모든 데이터가 사용 가능한가, (3) 완전성-데이터가 의미론적으로 올바른가, 그리고 (4) 일관성-가용 데이터가 시스템 전반에서 일관성이 있는가  우리는 또한 \"나쁨\", \"양호\", 또는 \"좋음\"이라고 하는 것을 명확히 정의하는 것을 제안합니다. 예를 들어, 50% 미만의 점수는 \"나쁨\"으로 표시되어 조치가 필요함을 의미합니다.**\n\n![이미지](/assets/img/2024-06-19-High-impactdatagovernanceteams_6.png)\n\n**이 수준의 통찰력을 통해 데이터 품질에 관한 질문을 시작할 수 있습니다. 예를 들어**\n\n**널 값 확인에 대한 좋은 커버리지를 가졌지만, 어떤 것이 실패하는지 조사하고, 고칠 수 있는지, 그리고 제거해야 하는지를 고려해야 합니다.**\n\n<div class=\"content-ad\"></div>\n\n우리의 어설션 테스트의 가동 시간이 크게 감소한 이유는 무엇일까요?\n\n저희가 일치성 테스트의 높은 가동 시간을 유지하고 있지만, 커버리지가 낮기 때문에 안전한 느낌을 준다고 생각하십니까?\n\n데이터 품질을 측정하는 방법에 대한 더 많은 지침은 \"데이터 품질 측정: 이론을 실무로\"라는 가이드를 참조해주세요.\n\n## 데이터 품질 기대치를 위한 가이드라인 설정\n\n<div class=\"content-ad\"></div>\n\n대부분의 상황에서는 데이터 사용 방식에 따라 기대 값을 명시하는 것이 좋습니다. 모든 자산에 동일한 기준을 적용하길 원하지 않을 수 있으므로 이를 권장합니다. 마찬가지로 ‘gold’ 자산에만 문서화를 강제할 수 있듯이, 데이터 거버넌스 팀은 데이터 테스트에 대한 기대 사항을 설정하는 역할을 해야 합니다. 비즈니스에 중요한 데이터, 노출되는 데이터 (예: 대시보드에 표시되는 데이터) 및 SLA(데이터가 제 시간에 도착하지 않으면 하류 영향이 있는 경우)를 고려하여 필요한 체크 항목을 평가하고 중요한 것을 측정할 수 있도록 돕습니다.\n\n<img src=\"/assets/img/2024-06-19-High-impactdatagovernanceteams_7.png\" />\n\ndbt yml 파일은 이 메타데이터를 정의하기에 좋은 장소입니다. 이를 통해 사유 테스트(예: 사전 커밋 dbt 패키지에서 제공하는 check-model 태그)를 사용하여 각 데이터 모델이 필요한 중요도나 도메인 소유자 태그와 같은 메타데이터를 가지고 있는지 확인할 수 있습니다.\n\n# 수행 책임이 분명한 소유권 모델을 통해 책임을 보장하세요\n\n<div class=\"content-ad\"></div>\n\n데이터 스택이 복잡해지면 한 사람이 모든 것을 머릿속에 간직하기 어려워집니다. 또한 문제를 발견한 사람이 그 문제를 해결할 적절한 사람이 아닌 경우가 더 많아집니다. 동시에 상하위 종속성의 수는 급증하여 적절한 상위 소유자를 찾거나 영향을받는 이해관계자에게 알릴 때 어려움을 겪게 됩니다.\n\n많은 데이터 팀에게 물어보지 않아도 꿈 같은 상황을 이해할 수 있습니다: 상위 생성자가 자신의 데이터의 품질을 소유하고 관리하며, 관련 데이터 팀이 책임을 질 수 있고, 이해관계자가 문제를 발견하는 날이 지나간 세상입니다.\n\n좋은 소유권은 말로만 하기 쉽지만, 실패한 소유권 이니셔티브가 부족하지 않습니다.\n\n데이터 거버넌스 팀은 명확히 정의된 역할과 책임을 갖는 일관된 소유권 모델을 구축하는 데 궁극적으로 책임이 있습니다. 단계별로 나누어서 누락된 부분을 파악하기 쉽게 만들어봅시다: (1) 메타데이터를 통합하고, (2) 관련 테스트로 문제를 감지하며, (3) 소유권을 할당하고, (4) 관련 인물에게 문제를 통지하여 그들이 대처할 수 있는 방법으로 행동을 취하도록 합시다.\n\n<div class=\"content-ad\"></div>\n\n\n![High impact data governance teams](/assets/img/2024-06-19-High-impactdatagovernanceteams_8.png)\n\n소유권은 기술적인 도전과 문화적인 측면 두 가지를 모두 고려해야 하는 문제입니다. 소유권 프로젝트를 성공적으로 이끌어내려면 두 측면에 모두 초점을 맞춰야 합니다.\n\n- 소유자에 대한 기대 설정 — 기대는 데이터 자산의 중요도와 연결되어야 합니다. 예를 들어, 무엇이 높은 심각도의 문제를 만들며 그 결정은 누가 하는지를 알아봅니다. 자세한 내용은 데이터 문제의 심각성 수준 설계를 시작하는 방법에서 확인할 수 있습니다.\n- 소유권 정의 — 소유권을 정의할 수 있는 장소는 dbt yml 파일부터 데이터 카탈로그, Confluence 페이지, 스프레드시트까지 다양합니다. 어디에 소유권을 정의해야 하는지 명확히 하고 모두가 동일한 방식으로 수행할 수 있도록 도와주세요.\n- 올바른 상황에서 올바른 사람에게 통보 — 데이터 소유권을 종합적으로 고려하길 권장합니다 — 상위팀이 소유한 데이터 소스부터 최종 사용자가 소유한 대시보드까지. 간단히 말해 우리의 권장사항을 다음 그룹으로 분류해보겠습니다: (1) 데이터 팀, (2) 상위팀, (3) 비즈니스 이해관계자.\n\n![High impact data governance teams](/assets/img/2024-06-19-High-impactdatagovernanceteams_9.png)\n\n\n<div class=\"content-ad\"></div>\n\n4. 문화 소유권 도전 극복하기 — 소유자로 할당받는 것과 책임을 져야 하는 것은 다릅니다. 소유권을 성공적으로 이행하는 것은 기술적인 도전만큼 문화적인 도전입니다. 사람들에게 영향을 미치고 책임을 지게 하는 것은 데이터 관리 팀에 대한 큰 이득입니다. 소유권에 대한 행동을 촉진하고 적절한 분야를 찾아 소유권을 정의함으로써 초기 승리를 증명하는 것은 기술적 구현만큼 중요합니다.\n\n우리의 가이드인 데이터 소유권: 현실적인 가이드를 읽어보세요. 데이터 팀, 상위 팀 및 비즈니스 이해자에게 소유권을 정의하고 활성화하기 위한 전체 도구 모음을 제공합니다.\n\n# 리스크, 개인정보 보호 및 규정 준수 관리\n\n회사의 수명주기에서 특정 시점, 일반적으로 데이터 관리 팀이 참여될 때, 관리해야 할 중요한 리스크가 있습니다. 이는 핀테크를 위한 규정 데이터, 곧 예정된 IPO를 위한 금융 데이터의 정확성, 또는 회사가 개인 식별 정보 데이터를 책임적으로 처리해야 한다는 일반적인 인식과 관련이 있을 수 있습니다.  \n\n<div class=\"content-ad\"></div>\n\n당신의 역할은 데이터 거버넌스 전문가로 명확합니다: 회사가 의무를 준수하고 리스크를 최소화하면서도 데이터 팀의 업무를 느리게 만들지 않도록 보장하는 것입니다.\n\n다음은 이를 수행할 수 있는 몇 가지 방법입니다.\n\nPII 관련 데이터 – 대부분의 회사에서 사용자 이메일과 같은 일부 데이터는 모든 사람이 쉽게 쿼리할 수 없어야 합니다. 데이터가 PII로 태그되고 이 데이터 주변에 가드레일을 자동으로 시행하도록 강요하세요. 예를 들어, 접근 권한이 사용 사례별로 만 부여되고, 예를 들어 7일 후에 만료되는 원시 데이터가 있는 별도의 데이터 웨어하우스를 만드세요.\n\n사용자 데이터 삭제 요청 – 사용자 데이터를 처리하면 결국 사용자 데이터 삭제 요청을 만나게 될 것입니다. 이에 대해 조속히 생각할수록 처음 요청이 들어왔을 때 모든 사용자 데이터를 삭제하는 것이 더 쉬워집니다. 이러한 사고방식이 발생하기 전에 열어둔 적절한 도구, 예를 들어 열 수준의 계보,는 걸어두면 소요 시간이 크게 줄어듭니다.\n\n<div class=\"content-ad\"></div>\n\n인시던트 관리 프로세스 및 SLA 정의 - 어떤 소유권 모델이나 데이터 품질 점검을 하더라도 데이터 문제는 발생할 수 있습니다. 잘 정의된 인시던트 관리 프로세스에는 여러 가지 이점이 있습니다: 모두가 중요한 것에 대해 공유된 이해를 구축하는 데 도움이 되며, 문제에 대한 어떤 맥락도 없는 사람들을 쉽게 참여시킬 수 있습니다. 또한 지난 인시던트를 쉽게 되돌아보고 중요한 위반 사항에 대해 보고할 수 있습니다.\n\n인시던트 관리 테이블을 마크다운 형식으로 변경하십시오.\n\n# 데이터 가버넌스 전문가로서 탐색해야 할 핵심 기술\n\n데이터를 사용 가능하게 만들기, 품질을 보장하기, 소유권 구축하기, PII 및 리스크를 속도와 균형있게 조절하기 - 이것은 데이터 가버넌스 팀에게는 작은 일이 아닙니다! 데이터 가버넌스 전문가가 성공하기 위해 필요한 필수 기술은 아래와 같습니다.\n\n<div class=\"content-ad\"></div>\n\n기술적 이해력 - 코드 기반에 기여하지 않고 직접 dbt 모델을 작성하지는 않더라도 데이터 팀이 사용하는 도구 및 제한 사항을 고수준으로 이해하면 유익합니다. 이렇게 하면 다른 팀의 고민을 더 잘 이해할 수 있으며 데이터 거버넌스 프로세스가 기존 워크플로에 어떻게 맞는지의 장단점을 파악할 수 있습니다.\n\n우선순위 균형 유지 - 모든 것이 항상 중요하다는 느낌일 수 있습니다. 그럼에도 불구하고 특정 이니셔티브에 우선순위를 두는 회사 전체의 상황을 주시하는 것이 여러분의 역할입니다. 예를 들어, 곧 예정된 IPO나 규정 위반과 같은 경우, 재무 데이터에 중점을 두는 것이 좋을 수 있으며 마케팅 및 제품 분야에는 덜 주의를 기울일 수 있습니다.\n\n공급업체 선정 프로세스 운영 - 위의 문제를 해결하는 데 도움이 되는 적어도 몇 가지 도구를 도입하고 싶을 것입니다. 데이터 카탈로그 및 데이터 관찰 도구와 같은 카테고리의 도구들에 주목해야 합니다. 도구에 투자를 결정하면, 잘 구조화된 프로세스를 운영하고 컨셉 증명을 계획하며 데이터 팀 구성원 모두가 주장을 듣게 해야 합니다. 회사가 이 카테고리의 도구를 처음 구매하는 경우일 수 있으므로 여러 개의 데모를 받고 기존 고객들과의 참조 검사를 진행하여 도구가 여러분에게 적합한지 확인하는 좋은 방법일 수 있습니다.\n\n조직적 지원 확보 - 누구도 즐거움을 위해 데이터 품질, 문서화 또는 소유권에 관심을 갖지 않습니다. 데이터 거버넌스 팀으로서 문서화, 데이터 품질 및 소유권이 왜 중요한지에 대한 꿈을 판매해야 합니다. 리더십 팀과 정기적으로 인사이트를 공유하는 것은 그들이 이에 투자해야 할 가치를 파악하고 게임에 참여하도록 하는 좋은 방법일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 모두 모아보기\n\n데이터 거버넌스 팀의 역할은 부엌을 청소하고 칼이 날카로운지 확인하며 모든 것이 제자리에 있는지 확인하는 것입니다.\n\n이겈하는 것은 쉽지 않은 작업이며 우선 순위를 조정하고 이해 관계자를 연루시키며 벤더 선택 프로세스를 실행하는 것을 포함합니다. 이를 잘 수행할 때의 좋은 예로는 덴마크 핀테크 루나(Lunar)의 경우가 있습니다. 루나는 소유권부터 중요도 및 모니터링에 이르기까지 데이터 거버넌스 프레임워크를 성공적으로 롤아웃하고 C-레벨의 찬성을 받았습니다. 자세한 내용은 이곳에서 데이터 거버넌스를 프레임워크로 구축하는 데 영감을 얻어보십시오.\n\n요약:\n\n<div class=\"content-ad\"></div>\n\n- 조직 전반에 걸쳐 데이터를 사용할 수 있도록 합니다 — 데이터가 잘 문서화되어 있고 주요 지표가 일관되게 정의되어 있으며 주요 데이터 자산이 발견 가능하며 대시보드가 잘 보이고 느껴지고 잘 수행될 수 있도록 합니다.\n- 데이터 품질을 계속해서 확인하여 가장 중요한 부분을 체계적으로 개선하도록 지원합니다 — 데이터 거버넌스 팀이 데이터 테스트를 작성하지는 않겠지만, 데이터 품질을 개선하도록 기대치를 강조하고 보고할 수 있습니다.\n- 소유권 모델을 통해 책임이 명확하게 보장합니다 — 소유권에 대한 주요 측면들, 소유권이 어떻게 정의되고 실행되며 각 팀이 자체적인 해석을 하지 않도록 데이터 거버넌스 팀이 중앙에서 정의해야 합니다.\n- 위험, 프라이버시 및 규정 준수를 관리합니다 — 종종 매력적이지 않은 분야로 여겨지지만, PII 데이터에 대한 불명확한 프로세스, 사용자 삭제 요청 및 사건들은 나중에 큰 문제를 일으킬 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-High-impactdatagovernanceteams_0.png"},"coverImage":"/assets/img/2024-06-19-High-impactdatagovernanceteams_0.png","tag":["Tech"],"readingTime":12},{"title":"유튜브 트렌드 분석 파이프라인 Airflow, Spark, S3 및 Docker를 이용한 ETL","description":"","date":"2024-06-19 09:37","slug":"2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker","content":"\n\n이 기사에서는 Apache Airflow와 PySpark를 사용하여 자동 ETL (추출, 변환, 로드) 파이프라인을 만드는 방법을 안내합니다. 이 파이프라인은 YouTube Data API에서 트렌드 비디오 데이터를 가져와 처리한 후 처리된 데이터를 S3에 저장할 것입니다.\n\nTwitter API를 사용한 파이프라인을 보여주는 Darshil Parmar의 YouTube 비디오를 시청한 후, 유사한 프로젝트에 도전하기로 영감을 받았습니다. 그러나 Twitter API의 가격 정책 변경으로 인해, 시청자가 YouTube Data API를 대체로 제안했고 이것이 제 흥미를 자극했습니다.\n\n프로젝트에 돌입하기 전에 두 가지 필수 사항이 있습니다:\n\n1. Youtube Data API 키 획득\n\n<div class=\"content-ad\"></div>\n\n- Google Developers Console을 방문해 주세요.\n- 새 프로젝트를 생성해 주세요.\n- \"YouTube Data API\"를 검색하고 활성화해 주세요.\n- 새 자격 증명을 생성하고 프로젝트에서 나중에 사용할 API 키를 복사해 주세요.\n\n자세한 지침은 YouTube Data API 시작 가이드를 참조해 주세요.\n\n2. AWS 액세스 키 ID 및 비밀 액세스 키 획득\n\n- AWS Management Console에 로그인해 주세요.\n- IAM(Identity and Access Management) 섹션으로 이동하고 새 사용자를 생성해 주세요.\n- 필요한 S3 액세스 정책을 부여하고 액세스 키를 생성해 주세요.\n- 프로젝트에서 사용할 액세스 키 ID와 비밀 액세스 키를 안전하게 저장해 주세요.\n\n<div class=\"content-ad\"></div>\n\n이제 실제 프로젝트를 시작하겠습니다! 준비됐나요 여러분!!\n\n![YouTube Trend Analysis Pipeline ETL with Airflow, Spark, S3, and Docker](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png)\n\n이 글은 4가지 주요 단계로 구성되어 있어요:\n\n- 소프트웨어 설치 및 설정\n- Youtube Data API에서 데이터 추출\n- PySpark를 사용하여 데이터 변환\n- AWS S3로 데이터 로드\n\n<div class=\"content-ad\"></div>\n\n# 1. 소프트웨어 설치 및 설정:\n\n- VS Code — [VS Code 다운로드 및 설치](https://code.visualstudio.com/).\n- Docker Desktop — [Docker Desktop 다운로드 및 설치](https://www.docker.com/products/docker-desktop).\n- (선택사항) Windows Subsystem for Linux (WSL) — 데이터 엔지니어링에 사용되는 Apache Airflow 및 PySpark와 같은 많은 도구 및 라이브러리가 Unix 계열 시스템을 위해 개발되었습니다. 이러한 도구를 Windows에서 사용할 때 발생할 수 있는 호환성 문제를 피하기 위해 WSL을 통해 네이티브 Linux 환경에서 실행할 수 있습니다.\n  - ` 관리자 권한으로 PowerShell을 엽니다.\n  - ` 다음 명령을 실행하세요: wsl --install.\n  - ` 명령에 따라 WSL을 설치하고 Microsoft Store에서 Linux 배포판(예: Ubuntu)을 선택하세요.\n  - ` Linux 배포판에 사용자 이름 및 암호를 설정하세요.\n\n이 프로젝트를 실행하는 데 WSL이 반드시 필요한 것은 아닙니다. Docker Desktop은 Windows에서 네이티브로 실행될 수 있으며 Docker 자체가 관리하는 가벼운 Linux 가상 머신(VM)을 사용합니다. 그러나 Docker Desktop과 함께 WSL을 사용하면 Windows에서 직접 Linux 명령 및 작업을 실행할 수 있어 보다 네이티브한 개발 경험을 제공합니다.\n\n이제 설정을 시작해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n부분 1 — 도커 이미지 만들기\n\n- 프로젝트용 새 폴더를 만들고 \"Airflow-Project\"로 이름을 지어주세요.\n- 해당 폴더에서 명령 프롬프트를 엽니다.\n- 명령 프롬프트에서 아래 명령을 실행하세요:\n\n```bash\ncode .\n```\n\n- 이 명령은 VS Code에서 해당 폴더를 프로젝트로 엽니다.\n- VS Code에서 \"dockerfile\"이라는 새 파일을 만들고 아래 코드를 붙여넣으세요:\n\n<div class=\"content-ad\"></div>\n\n```js\nFROM apache/airflow:latest\n\n# 시스템 종속성을 설치하기 위해 루트 사용자로 전환합니다\nUSER root\n\n# git, OpenJDK를 설치하고 apt 캐시를 정리합니다\nRUN apt-get update && \\\n    apt-get -y install git default-jdk && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Python 패키지를 설치하기 위해 airflow 사용자로 전환합니다\nUSER airflow\n\n# 필요한 Python 패키지를 설치합니다\nRUN pip install --no-cache-dir pyspark pandas google-api-python-client emoji boto3\n```\n\n이 Docker 파일은 프로젝트를 실행하는 데 필요한 모든 패키지를 포함하고 있어요.\n\n- 파일을 마우스 오른쪽 버튼으로 클릭하고 VS Code에서 \"이미지 빌드\" 옵션을 선택하세요. 이름을 입력하라는 프롬프트가 나타나면 \"airflow-project\"를 입력하세요. 이 명령은 Docker 이미지를 생성합니다. 그러나 이미지를 사용하려면 docker-compose.yml 파일을 생성하고 이미지를 사용하도록 구성해야 합니다.\n\n(재미있는 사실: 파일에서 Python 설치가 없는 이유 궁금하신가요? 실제로 Dockerfile에서 사용된 기본 이미지인 apache/airflow:latest에는 Python이 이미 설치되어 있어요. 왜냐하면 Airflow 자체가 Python으로 작성되어 있기 때문에 주로 워크플로 및 작업 정의에 Python을 사용합니다. 따라서 Dockerfile에서 별도로 Python을 설치할 필요가 없답니다!)\n\n<div class=\"content-ad\"></div>\n\n파트 2 — 도커 컴포즈 파일 생성하기\n\n도커 컴포즈를 사용하면 멀티 컨테이너 도커 애플리케이션을 쉽게 다룰 수 있습니다. 이를 통해 단일 명령으로 여러 도커 컨테이너를 정의하고 실행할 수 있으며 각 서비스의 환경 변수, 볼륨, 포트 및 기타 설정을 명확하고 조직적인 방식으로 구성할 수 있습니다. 도커 컴포즈를 사용하면 단일 명령어인 docker-compose up 또는 docker-compose down을 사용하여 여러 서비스를 쉽게 시작, 중지 및 관리할 수 있습니다.\n\n- \"docker-compose.yml\" 파일을 생성하고 다음 코드를 파일에 붙여넣습니다:\n\n```js\nversion: '3'\nservices:\n\n  airflowproject:\n    image: airflow-project:latest\n    environment:\n      - AWS_ACCESS_KEY_ID=your-aws-access-key\n      - AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key\n      - YOUTUBE_API_KEY=your-youtube-api-key\n    volumes:\n      - ./airflow:/opt/airflow\n    ports:\n      - \"8080:8080\"\n    command: airflow standalone\n```\n\n<div class=\"content-ad\"></div>\n\n- 이제 파일을 마우스 오른쪽 버튼으로 클릭한 후 VS Code에서 'Compose Up' 옵션을 선택하세요. 환경을 설정하기 위해 클릭하세요.\n- 깜짝 놀랄 일이 벌어졌어요! 이 작업을 완료한 후에는 VS Code 프로젝트 디렉토리에 \"airflow\"라는 새 폴더가 나타날 수 있습니다.\n\nDocker 데스크톱을 열어서 모든 것이 올바르게 완료되었는지 확인하세요. 올바르게 완료된 경우 다음과 같은 화면이 표시됩니다.\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_1.png)\n\n- 이제 Airflow 프로젝트를 클릭하여 Airflow가 8080 포트에서 실행 중임을 나타내는 로그가 표시되는 화면을 엽니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_2.png)\n\n- 포트를 클릭하면 Airflow 로그인 페이지로 이동합니다. 이 링크를 처음 열어보는 경우 자격 증명을 제공해야 합니다.\n- 사용자 이름은 \"admin\"이고 비밀번호는 compose up 명령을 실행한 후 생성된 Airflow 폴더 내의 \"standalone_admin_password.txt\" 파일에 있습니다.\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_3.png)\n\n- 로그인 페이지에서 자격 증명을 입력한 후, 로컬 호스트에서 Airflow가 실행 중인 것을 확인할 수 있습니다. 다음과 같이 나타납니다:\n\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_4.png\" />\n\n당신의 환경 설정 완료입니다! 휴―!!\n\n# 2. YouTube 데이터 API에서 데이터 추출하기:\n\n<div class=\"content-ad\"></div>\n\n- Airflow 폴더 아래에 \"dags\"라는 이름의 폴더를 만들고, dags 폴더 아래에 \"youtube_etl_dag.py\"라는 파이썬 파일을 만듭니다.\n- 이제 \"youtube_etl_dag.py\" 파일에 다음을 import하세요.\n\n```js\nimport logging\nimport os\nimport re\nimport shutil\nfrom datetime import datetime, timedelta\n\nimport boto3\nimport emoji\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_date, udf\nfrom pyspark.sql.types import (DateType, IntegerType, LongType, StringType,\n                               StructField, StructType)\n\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n```\n\n- 이 프로젝트를 실행하는 데 위의 모든 라이브러리가 필요합니다(코드 작성을 시작하면 모두 유용해집니다)\n- VS Code에서 오류가 발생하는 것을 볼 수 있습니다. 그 이유는 모든 종속성이 도커에 설치되어 있지만 로컬 머신에는 설치되어 있지 않기 때문이므로 신경 쓰지 마십시오.\n- Airflow에서 구문 오류가 있으면 화면 상단에 표시되고, 논리 오류/예외는 Airflow 로그에서 확인할 수 있습니다.\n\n```js\n# DAG와 기본 인수 정의\ndefault_args = {\n    'owner': 'airflow',  # DAG 소유자\n    'depends_on_past': False,  # 과거 DAG 실행에 종속하는지 여부\n    'email_on_failure': False,  # 실패 시 이메일 알림 비활성화\n    'email_on_retry': False,  # 재시도 시 이메일 알림 비활성화\n    'retries': 1,  # 재시도 횟수\n    'retry_delay': timedelta(minutes=5),  # 재시도 간의 지연 시간\n     'start_date': datetime(2023, 6, 10, 0, 0, 0),  # 매일 자정(00:00) UTC에 실행\n}\n\ndag = DAG(\n    'youtube_etl_dag',  # DAG 식별자\n    default_args=default_args,  # 기본 인수 할당\n    description='간단한 ETL DAG',  # DAG 설명\n    schedule_interval=timedelta(days=1),  # 일별 스케줄 간격\n    catchup=False,  # 누락된 DAG 실행을 복구하지 않음\n)\n```  \n\n<div class=\"content-ad\"></div>\n\n매일 자정(0시)에 실행되는 DAG인 'youtube_etl_dag'을 정의하고 있습니다. 이 DAG은 Airflow에서 관리 및 트리거되며, VS Code에서 별도로 실행할 필요가 없습니다. Python 파일을 업데이트하면 Airflow에서 자동으로 변경 사항을 감지하고 반영할 것입니다.\n\n현재 Airflow에는 DAG이 표시되지만 아직 정의된 작업이 없어서 어떤 작업도 표시되지 않습니다. DAG를 기능적으로 만들기 위해 데이터 추출 작업을 만들어봅시다.\n\n```js\n# YouTube API에서 데이터를 추출하기 위한 Python callable 함수\ndef extract_data(**kwargs):\n    api_key = kwargs['api_key']\n    region_codes = kwargs['region_codes']\n    category_ids = kwargs['category_ids']\n    \n    df_trending_videos = fetch_data(api_key, region_codes, category_ids)\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    # DataFrame을 CSV 파일로 저장\n    df_trending_videos.to_csv(output_path, index=False)\n\ndef fetch_data(api_key, region_codes, category_ids):\n    \"\"\"\n    YouTube API에서 여러 나라와 카테고리의 인기 동영상 데이터를 가져옵니다.\n    \"\"\"\n    # 비디오 데이터를 저장할 빈 리스트를 초기화합니다.\n    video_data = []\n\n    # YouTube API 서비스 빌드\n    youtube = build('youtube', 'v3', developerKey=api_key)\n\n    for region_code in region_codes:\n        for category_id in category_ids:\n            # 각 지역 및 카테고리에 대해 next_page_token을 None으로 초기화\n            next_page_token = None\n            while True:\n                # 인기 동영상을 가져오기 위해 YouTube API에 요청을 보냅니다.\n                request = youtube.videos().list(\n                    part='snippet,contentDetails,statistics',\n                    chart='mostPopular',\n                    regionCode=region_code,\n                    videoCategoryId=category_id,\n                    maxResults=50,\n                    pageToken=next_page_token\n                )\n                response = request.execute()\n                videos = response['items']\n\n                # 각 비디오를 처리하고 데이터를 수집합니다.\n                for video in videos:\n                    video_info = {\n                        'region_code': region_code,\n                        'category_id': category_id,\n                        'video_id': video['id'],\n                        'title': video['snippet']['title'],\n                        'published_at': video['snippet']['publishedAt'],\n                        'view_count': int(video['statistics'].get('viewCount', 0)),\n                        'like_count': int(video['statistics'].get('likeCount', 0)),\n                        'comment_count': int(video['statistics'].get('commentCount', 0)),\n                        'channel_title': video['snippet']['channelTitle']\n                    }\n                    video_data.append(video_info)\n\n                # 결과의 더 많은 페이지가 있는 경우 다음 페이지 토큰을 가져옵니다.\n                next_page_token = response.get('nextPageToken')\n                if not next_page_token:\n                    break\n\n    return pd.DataFrame(video_data)\n\n# DAG를 위한 데이터 추출 작업 정의\nextract_task = PythonOperator(\n    task_id='extract_data_from_youtube_api',\n    python_callable=extract_data,\n    op_kwargs={\n        'api_key': os.getenv('YOUTUBE_API_KEY'),\n        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],\n        'category_ids': ['1', '2', '10', '15', '20', '22', '23']\n    },\n    dag=dag,\n)\n\nextract_task # 이 작업을 실행하도록 DAG를 설정함\n```\n\n이 코드에서 두 가지 주요 작업이 이루어지고 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- DAG에 extract_task라는 작업을 만들고 있습니다.\n- extract_task에서 호출되는 callable 함수인 extract_data를 정의하고 있습니다. 이 함수는 YouTube Data API에서 데이터를 가져와 \"Youtube_Trending_Data_Raw\"로 시작하는 CSV 파일에 pandas DataFrame을 사용하여 저장합니다.\n\nYouTube Data API 문서를 참조하여 API의 다른 부분에서 사용 가능한 데이터에 대해 자세히 이해할 수 있습니다. 우리는 트렌딩 비디오 데이터에 관심이 있으므로 API의 해당 부분에 집중할 것입니다. next_page_token은 모든 페이지에서 데이터를 검색하도록 보장합니다.\n\n코드를 수정한 후 Airflow 페이지에 변경 사항이 반영되어야 합니다. DAG를 수동으로 실행하려면 왼쪽 상단에 있는 실행 버튼을 클릭하시면 됩니다. 그래프에서 작업 상태 (대기, 실행 중, 성공 등)는 다른 색상으로 나타납니다. DAG가 실행 중일 때 로그를 보실 수도 있습니다.\n\n<img src=\"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_5.png\" />\n\n<div class=\"content-ad\"></div>\n\n런 버튼을 클릭하면 데이터를 가져오고 파일에 저장하는 데 시간이 걸립니다. 작업의 각 단계에서 그래프 색상이 변경되는 것을 볼 수 있을 거에요. 멋지죠? :)\n\n작업 상태가 성공을 나타내는 녹색으로 변하면, 새 파일인 \"Youtube-Trending-Data-Raw\"가 생긴 것을 확인할 수 있어요.\n\n우리의 Raw 데이터는 이렇게 생겼어요:\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_6.png)\n\n<div class=\"content-ad\"></div>\n\n이제 추출 작업이 완료되었습니다. 다음 작업으로 넘어가 봅시다!\n\n## 3. PySpark를 사용하여 데이터 변환하기:\n\n원시 데이터 파일을 살펴보면 데이터에 많은 해시태그와 이모지가 있는데, 이는 우리 프로젝트에는 필요하지 않습니다. 데이터를 전처리하고 정리하여 추가 분석에 유용하도록 만들어 봅시다.\n\n이 작업에 PySpark를 사용할 것입니다. PySpark는 대용량 데이터 세트를 처리하고 변환 작업을 수행하기 위해 설계된 강력한 프레임워크입니다. 데이터 세트가 특히 크지 않기 때문에 Pandas를 사용할 수도 있지만, 전에 PySpark를 사용한 적이 있어 이번에도 PySpark를 사용하기로 결정했습니다. 최근 PySpark를 공부하고 있으며, 이론을 공부하는 것보다 실제 구현이 더 흥미롭다고 느낍니다.\n\n<div class=\"content-ad\"></div>\n\n\n# Python callable function to extract data from YouTube API\ndef extract_data(**kwargs):\n    api_key = kwargs['api_key']\n    region_codes = kwargs['region_codes']\n    category_ids = kwargs['category_ids']\n    \n    df_trending_videos = fetch_data(api_key, region_codes, category_ids)\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    # Save DataFrame to CSV file\n    df_trending_videos.to_csv(output_path, index=False)\n\ndef fetch_data(api_key, region_codes, category_ids):\n    \"\"\"\n    Fetches trending video data for multiple countries and categories from YouTube API.\n    Returns a pandas data frame containing video data.\n    \"\"\"\n    video_data = []\n\n    # Build YouTube API service\n    youtube = build('youtube', 'v3', developerKey=api_key)\n\n    for region_code in region_codes:\n        for category_id in category_ids:\n            # Initialize the next_page_token to None for each region and category\n            next_page_token = None\n            while True:\n                # Make a request to the YouTube API to fetch trending videos\n                request = youtube.videos().list(\n                    part='snippet,contentDetails,statistics',\n                    chart='mostPopular',\n                    regionCode=region_code,\n                    videoCategoryId=category_id,\n                    maxResults=50,\n                    pageToken=next_page_token\n                )\n                response = request.execute()\n                videos = response['items']\n\n                # Process each video and collect data\n                for video in videos:\n                    video_info = {\n                        'region_code': region_code,\n                        'category_id': category_id,\n                        'video_id': video['id'],\n                        'title': video['snippet']['title'],\n                        'published_at': video['snippet']['publishedAt'],\n                        'view_count': video['statistics'].get('viewCount', 0),\n                        'like_count': video['statistics'].get('likeCount', 0),\n                        'comment_count': video['statistics'].get('commentCount', 0),\n                        'channel_title': video['snippet']['channelTitle']\n                    }\n                    video_data.append(video_info)\n\n                # Get the next page token, if there are more pages of results\n                next_page_token = response.get('nextPageToken')\n                if not next_page_token:\n                    break\n\n    return pd.DataFrame(video_data)\n\ndef preprocess_data_pyspark_job():\n    spark = SparkSession.builder.appName('YouTubeTransform').getOrCreate()\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    df = spark.read.csv(output_path, header=True)\n    \n    # Define UDF to remove hashtag data, emojis\n    def clean_text(text):\n     if text is not None:\n        # Remove emojis\n        text = emoji.demojize(text, delimiters=('', ''))\n        \n        # Remove hashtag data\n        if text.startswith('#'):\n            text = text.replace('#', '').strip()\n        else:\n            split_text = text.split('#')\n            text = split_text[0].strip()\n        \n        # Remove extra double quotes and backslashes\n        text = text.replace('\\\\\"', '')  # Remove escaped quotes\n        text = re.sub(r'\\\"+', '', text)  # Remove remaining double quotes\n        text = text.replace('\\\\', '')  # Remove backslashes\n        \n        return text.strip()  # Strip any leading or trailing whitespace\n\n     return text\n    # Register UDF\n    clean_text_udf = udf(clean_text, StringType())\n\n    # Clean the data\n    df_cleaned = df.withColumn('title', clean_text_udf(col('title'))) \\\n                   .withColumn('channel_title', clean_text_udf(col('channel_title'))) \\\n                   .withColumn('published_at', to_date(col('published_at'))) \\\n                   .withColumn('view_count', col('view_count').cast(LongType())) \\\n                   .withColumn('like_count', col('like_count').cast(LongType())) \\\n                   .withColumn('comment_count', col('comment_count').cast(LongType())) \\\n                   .dropna(subset=['video_id'])\n    \n    # Generate the filename based on the current date\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'\n    \n    # Write cleaned DataFrame to the specified path\n    df_cleaned.write.csv(output_path, header=True, mode='overwrite')   \n\n\n# Define extract task for the DAG\nextract_task = PythonOperator(\n    task_id='extract_data_from_youtube_api',\n    python_callable=extract_data,\n    op_kwargs={\n        'api_key': os.getenv('YOUTUBE_API_KEY'),\n        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],\n        'category_ids': ['1', '2', '10', '15', '20', '22', '23']\n    },\n    dag=dag,\n)\n\n# Define preprocessing task for the DAG\npreprocess_data_pyspark_task= PythonOperator(\n    task_id='preprocess_data_pyspark_task',\n    python_callable=preprocess_data_pyspark_job,\n    dag=dag\n)\n\nextract_task >> preprocess_data_pyspark_task\n\n\n여기서는 이 코드가 하는 일을 설명해 드렸습니다.\n\n- \"preprocess_data_pyspark_task\"라는 작업을 만듭니다.\n- 이 작업은 preprocess_data_pyspark_job 함수를 호출합니다.\n- preprocess_data_pyspark_job 함수는 데이터를 정리합니다.\n- 그리고 정리된 데이터는 \"Transformed_Youtube_Data_currentDate\"라는 폴더에 저장됩니다.\n- 이 폴더 안에는 정리된 데이터가 담긴 \"part-\" 접두사가 붙은 새 CSV 파일이 생성됩니다.\n\n만약 Airflow를 보신다면 아래와 같이 첫 번째 작업에 새로운 작업이 추가된 것을 보실 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n아래는 우리가 변환한 데이터의 모습입니다:\n\n![Transformed Data](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_8.png)\n\n이 작업은 완료되었습니다. 이제 최종 작업으로 넘어갈 차례입니다.\n\n<div class=\"content-ad\"></div>\n\n# 4. S3로 데이터 로드하기:\n\n이 작업을 시작하기 전에 처음에 설정한 IAM 사용자를 사용하여 S3 버킷을 생성하고 버킷 이름을 메모해주세요.\n\n우리의 최종 코드입니다!\n\n```js\nimport logging\nimport os\nimport re\nimport shutil\nfrom datetime import datetime, timedelta\n\nimport boto3\nimport emoji\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_date, udf\nfrom pyspark.sql.types import (DateType, IntegerType, LongType, StringType,\n                               StructField, StructType)\n\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\n# DAG 및 기본 인자 정의\ndefault_args = {\n    'owner': 'airflow',  # DAG 소유자\n    'depends_on_past': False,  # 이전 DAG 실행에 의존 여부\n    'email_on_failure': False,  # 실패 시 이메일 알림 비활성화\n    'email_on_retry': False,  # 재시도 시 이메일 알림 비활성화\n    'retries': 1,  # 재시도 횟수\n    'retry_delay': timedelta(minutes=5),  # 재시도 사이 간격\n    'start_date': datetime(2023, 6, 10, 0, 0, 0),  # 매일 자정(00:00) UTC에 실행\n}\n\n# DAG 정의\ndag = DAG(\n    'youtube_etl_dag',  # DAG 식별자\n    default_args=default_args,  # 기본 인수 할당\n    description='간단한 ETL DAG',  # DAG 설명\n    schedule_interval=timedelta(days=1),  # 스케줄 간격: 매일\n    catchup=False,  # 누락된 DAG 실행을 복구하지 않음\n)\n\n# YouTube API에서 데이터를 추출하는 Python 유형의 함수\ndef extract_data(**kwargs):\n    api_key = kwargs['api_key']\n    region_codes = kwargs['region_codes']\n    category_ids = kwargs['category_ids']\n    \n    df_trending_videos = fetch_data(api_key, region_codes, category_ids)\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    # DataFrame을 CSV 파일로 저장\n    df_trending_videos.to_csv(output_path, index=False)\n\n# YouTube API에서 데이터 가져오는 함수\ndef fetch_data(api_key, region_codes, category_ids):\n    \"\"\"\n    YouTube API에서 여러 국가 및 카테고리의 트렌드 비디오 데이터를 가져옵니다.\n    비디오 데이터가 포함된 Pandas 데이터 프레임 반환.\n    \"\"\"\n    # 비디오 데이터를 보관할 빈 리스트 초기화\n    video_data = []\n\n    # YouTube API 서비스 빌드\n    youtube = build('youtube', 'v3', developerKey=api_key)\n\n    for region_code in region_codes:\n        for category_id in category_ids:\n            # 각 지역 및 카테고리마다 next_page_token을 None으로 초기화\n            next_page_token = None\n            while True:\n                # YouTube API에 트렌드 비디오를 가져오도록 요청\n                request = youtube.videos().list(\n                    part='snippet,contentDetails,statistics',\n                    chart='mostPopular',\n                    regionCode=region_code,\n                    videoCategoryId=category_id,\n                    maxResults=50,\n                    pageToken=next_page_token\n                )\n                response = request.execute()\n                videos = response['items']\n\n                # 각 비디오 처리 및 데이터 수집\n                for video in videos:\n                    video_info = {\n                        'region_code': region_code,\n                        'category_id': category_id,\n                        'video_id': video['id'],\n                        'title': video['snippet']['title'],\n                        'published_at': video['snippet']['publishedAt'],\n                        'view_count': video['statistics'].get('viewCount', 0),\n                        'like_count': video['statistics'].get('likeCount', 0),\n                        'comment_count': video['statistics'].get('commentCount', 0),\n                        'channel_title': video['snippet']['channelTitle']\n                    }\n                    video_data.append(video_info)\n\n                # 결과의 추가 페이지가 있는 경우 다음 페이지 토큰 가져오기\n                next_page_token = response.get('nextPageToken')\n                if not next_page_token:\n                    break\n\n    return pd.DataFrame(video_data)\n\n# PySpark 작업 전처리 함수\ndef preprocess_data_pyspark_job():\n    spark = SparkSession.builder.appName('YouTubeTransform').getOrCreate()\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    df = spark.read.csv(output_path, header=True)\n    \n    # 해시태그 데이터, 이모지 제거를 위한 UDF 정의\n    def clean_text(text):\n     if text is not None:\n        # 이모지 제거\n        text = emoji.demojize(text, delimiters=('', ''))\n        \n        # 해시태그 및 이후 모든 것 제거\n        if text.startswith('#'):\n            text = text.replace('#', '').strip()\n        else:\n            split_text = text.split('#')\n            text = split_text[0].strip()\n        \n        # 추가 이중 인용부호와 백슬래시 제거\n        text = text.replace('\\\\\"', '')  # 이스케이프된 따옴표 제거\n        text = re.sub(r'\\\"+', '', text)  # 남은 이중 인용부호 제거\n        text = text.replace('\\\\', '')  # 백슬래시 제거\n        \n        return text.strip()  # 선행 또는 후행 공백 제거\n\n     return text\n    # UDF 등록\n    clean_text_udf = udf(clean_text, StringType())\n\n    # 데이터 정리\n    df_cleaned = df.withColumn('title', clean_text_udf(col('title'))) \\\n                   .withColumn('channel_title', clean_text_udf(col('channel_title'))) \\\n                   .withColumn('published_at', to_date(col('published_at'))) \\\n                   .withColumn('view_count', col('view_count').cast(LongType())) \\\n                   .withColumn('like_count', col('like_count').cast(LongType())) \\\n                   .withColumn('comment_count', col('comment_count').cast(LongType())) \\\n                   .dropna(subset=['video_id'])\n    \n    # 현재 날짜를 기반으로 파일 이름 생성\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'\n    \n    # 정리된 DataFrame을 지정된 경로에 작성\n    df_cleaned.write.csv(output_path, header=True, mode='overwrite')   \n\n# S3로 데이터 업로드 함수\ndef load_data_to_s3(**kwargs):\n    bucket_name = kwargs['bucket_name']\n    today = datetime.now().strftime('%Y/%m/%d')\n    prefix = f\"processed-data/{today}\"\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    local_dir_path  = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'\n    upload_to_s3(bucket_name, prefix, local_dir_path)\n\ndef upload_to_s3(bucket_name, prefix, local_dir_path):\n    aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n    aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n\n    s3_client = boto3.client(\n        's3',\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key\n    )\n\n    for root, dirs, files in os.walk(local_dir_path):\n         for file in files:\n            if file.endswith('.csv'):\n                file_path = os.path.join(root, file)\n                s3_key = f\"{prefix}/{file}\"\n                logging.info(f\"Uploading {file_path} to s3://{bucket_name}/{s3_key}\")\n                s3_client.upload_file(file_path, bucket_name, s3_key)\n\n# DAG의 추출 작업 정의\nextract_task = PythonOperator(\n    task_id='extract_data_from_youtube_api',\n    python_callable=extract_data,\n    op_kwargs={\n        'api_key': os.getenv('YOUTUBE_API_KEY'),\n        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],\n        'category_ids': ['1', '2', '10', '15', '20', '22', '23']\n    },\n    dag=dag,\n)\n\n# DAG의 데이터 전처리 작업 정의\npreprocess_data_pyspark_task= PythonOperator(\n    task_id='preprocess_data_pyspark_task',\n    python_callable=preprocess_data_pyspark_job,\n    dag=dag\n)\n\n\n\n<div class=\"content-ad\"></div>\n\n이제 저희가 만든 최종 작업인 load_data_to_s3_task를 소개합니다. 이 작업은 load_data_to_s3 함수를 호출하여 파일을 S3 버킷에 업로드합니다. 업로드가 잘 되었는지 확인하려면 S3 버킷의 내용을 확인하세요.\n\n마침내 우리의 Airflow는 이렇게 생겼습니다!\n\n![Airflow](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_9.png)\n\n이제 이 데이터를 Tableau나 다른 BI 도구에 연결하여 흥미로운 대시보드를 만들고 인사이트를 시각화해 보세요!\n\n<div class=\"content-ad\"></div>\n\n함께 이 파이프라인을 따라 오면서 새로운 기술 몇 가지를 배웠으면 좋겠어요! 🚀 성공적으로 여기까지 왔다면 축하해요! 🎉 이 새롭게 얻은 지식이 데이터 엔지니어링에서의 향후 모험에 큰 도움이 되길 바래요!\n\n이 프로젝트의 Github 저장소를 첨부합니다:\n\n만약 이 글을 좋아하셨다면, 공유하고, 좋아요를 눌러주시고, 아래에 댓글을 남겨주시고 구독해주세요. 🎉👏📝\n\n커튼을 닫습니다! 🎭","ogImage":{"url":"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png"},"coverImage":"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png","tag":["Tech"],"readingTime":25},{"title":"1771 Technologies가 선보이는 Graphite Grid 소개","description":"","date":"2024-06-19 09:36","slug":"2024-06-19-IntroducingGraphiteGridby1771Technologies","content":"\n\n![그래픽그리드 소개](/assets/img/2024-06-19-IntroducingGraphiteGridby1771Technologies_0.png)\n\n# 약간의 역사\n\n2023년에 1771 Technologies를 설립할 때, 우리는 ‘그냥 작동하는’ 소프트웨어를 개발하기를 목표로 했습니다. 개발자를 위한 제품을 개발하고자 했으며, 복잡한 문제를 문제없이 해결하면서도 사용성과 기능성을 희생시키지 않는 것이 우리의 목표였습니다.\n\n우리 회사는 ‘무결한 간단함’이라는 용어의 살아있는 구현입니다. 우리의 미션은 가장 요구가 많은 사용 환경에도 견디는 신뢰할 수 있고 기능적인 소프트웨어를 만들어 사용자에게 최고의 품질을 제공하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# Graphite Grid의 탄생\n\n당사의 솔루션이 완벽한 간편함을 제공한다는 약속을 지속적으로 이행함에 따라, 우리는 혁신적인 소프트웨어 제품인 Graphite Grid를 소개하게 되어 매우 기쁩니다. 이 React 데이터 그리드 라이브러리는 단순히 또 다른 도구가 아니라 성능 중심 소프트웨어에서의 게임 체인저입니다.\n\nGraphite Grid는 철저한 개발, 테스트 및 정제를 거쳐 귀하의 조직이 필요로 하는 유일한 JavaScript 데이터 그리드가 되도록 보장합니다. Graphite Grid를 사용하면 고객은 데이터의 복잡성에 적응하는 도구로 스마트한 결정을 내릴 수 있으며, 타협 없이 견고한 기반 위에 애플리케이션을 구축할 수 있습니다.\n\n# 여정이 시작됩니다\n\n<div class=\"content-ad\"></div>\n\n자바스크립트와 리액트 커뮤니티에서 소프트웨어 제품의 훌륭한 기능과 삶을 바꿀 기능에 대해 블로그 글을 쓰는 것은 새로운 일이 아닙니다. 그래서 우리는 소프트웨어가 설득하는 모든 작업을 하도록 선호합니다.\n\n전통적인 데이터 그리드의 제한에서 벗어나 개발 프로세스를 혁신하고 싶다면, 1771technologies.com을 방문하여 더 많은 정보를 알아보세요.\n\n당신이 여정에 머무르기를 바랍니다.","ogImage":{"url":"/assets/img/2024-06-19-IntroducingGraphiteGridby1771Technologies_0.png"},"coverImage":"/assets/img/2024-06-19-IntroducingGraphiteGridby1771Technologies_0.png","tag":["Tech"],"readingTime":2},{"title":"분석력을 향상시키는 5가지 유용한 시각화 방법","description":"","date":"2024-06-19 09:34","slug":"2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis","content":"\n\n아래는 Markdown 형식으로 표를 변경한 코드입니다.\n\n\n![Visualization](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_0.png)\n\n# 소개\n\nSeaborn은 오랫동안 사용되어 왔습니다.\n\n비전문가들도 강력한 그래픽을 구축할 수 있도록 도와주기 때문에, 데이터 시각화 라이브러리 중에서 가장 유명하고 많이 사용되는 것 중 하나라고 생각합니다. 또한 통계에 근거한 통찰력을 얻는 데 도움이 되기 때문입니다.\n\n\n<div class=\"content-ad\"></div>\n\n저는 통계학자가 아닙니다. 데이터 과학에 관심을 가지고 있어서 그 분야에 대한 통계 개념을 배워 직무를 더 잘 수행하기 위해 노력하고 있어요. 그래서 히스토그램, 신뢰구간, 그리고 선형 회귀 분석에 매우 적은 양의 코드로 간단하게 접근할 수 있어서 정말 좋아해요.\n\nSeaborn의 구문은 매우 기본적입니다: sns.type_of_plot(data, x, y)요. 이 간단한 템플릿을 사용하여 막대 그래프, 히스토그램, 산점도, 선 그래프, 상자 그림 등 다양한 시각화를 만들 수 있어요.\n\n하지만 이 게시물은 그것들에 대해 이야기하려는 것이 아니에요. 여러분의 분석에 차이를 만들어 줄 다른 향상된 종류의 시각화에 대해 이야기할 거예요.\n\n어떤 시각화들이 있는지 함께 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 시각화\n\n이 시각화를 만들고 이 연습과 함께 코드를 작성하려면 seaborn을 import합니다. import seaborn as sns.\n\n여기에서 사용된 데이터셋은 Paulo Cortez가 작성하고 크리에이티브 커먼즈 라이센스하에 UCI 저장소에 기즐한 학생 성적 데이터입니다. 아래 코드를 사용하여 파이썬에서 바로 가져올 수 있습니다.\n\n```js\n# UCI Repo 설치\npip install ucimlrepo\n\n# 데이터셋 로딩\nfrom ucimlrepo import fetch_ucirepo \n  \n# 데이터셋 가져오기 \nstudent_performance = fetch_ucirepo(id=320) \n  \n# 데이터 (팬더스 데이터프레임 형식) \nX = student_performance.data.features \ny = student_performance.data.targets\n\n# 시각화를 위해 X와 Y 수집\ndf = pd.concat([X,y], axis=1)\n\ndf.head(3)\n```\n\n<div class=\"content-ad\"></div>\n\n\n![](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_1.png)\n\nNow let's talk about the 5 visualizations.\n\n## 1. Stripplot\n\nThe first plot picked is the stripplot. And you will quickly see why this is interesting. If we use this simple line of code, it will display the following viz.\n\n\n<div class=\"content-ad\"></div>\n\n```js\n# 플롯\nsns.stripplot(data=df);\n```\n\n<img src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_2.png\" />\n\n와우! 거의 Pandas의 `df.describe()`와 같은 차트입니다. x축에는 모든 수치 변수가 나타납니다. y축은 해당 변수의 범위 값입니다. 따라서 그림을 보면 몇 가지 흥미로운 통찰력을 빠르게 얻을 수 있습니다.\n\n- 시각적으로 적어도 이상치 실례가 적습니다.\n- 대부분의 수치 변수는 0에서 5 사이로 범위가 있습니다. 이는 데이터의 설명에 해당 변수가 범주화되었음을 보여줍니다. 따라서 여기서 가장 좋은 방법은 해당 변수를 범주형으로 변환하는 것입니다.\n- 이 데이터셋의 학생들은 15 ~ 23세 사이입니다.\n- 첫 번째 학기(G1)와 두 번째 학기(G2) 성적의 분포는 매우 유사합니다.\n\n<div class=\"content-ad\"></div>\n\n정말 대단하죠! 코드 한 줄로 생성된 이 플롯에서 우리가 얻은 정보의 양을 보세요!\n\n이 그래픽에 대해 더 탐구할 만한 다른 부분이 많이 있어요. 몇 가지 다른 인수를 추가하고 개선할 수 있어요. 실수와 최종 성적 G3에 대한 그래픽을 만들어서 학교별로 분리해 봅시다.\n\n이렇게요!\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_3.png\" />\n\n좋아요! 실패 횟수가 적은 학생들이 높은 성적을 받고 있는 것을 예상대로 확인할 수 있어요. 그리고 두 학교 모두 성적면에서 꽤 유사해 보여요.\n\n이제 다음으로 넘어가 봅시다.\n\n## 2. Catplot\n\n<div class=\"content-ad\"></div>\n\n음-음... 우리는 stripplot을 만들었지만, 그것은 숫자형 변수에만 해당해요. 그럼 범주형 변수는 어떨까요?\n\n여기서 catplot이 유용합니다. 이것은 카테고리별 관측치를 플롯으로 나타내줄 거에요. 학교별 성적을 확인하고 싶다면, 이렇게 간단해요.\n\n```js\n#CatPlot\nsns.catplot(data=df, x='school', y='G3');\n```\n\n![이미지](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_4.png)\n\n<div class=\"content-ad\"></div>\n\n기본적으로 카테고리별로 산점도가 표시됩니다. 그러나 kind 인수를 사용하여 막대, 상자, 바이올린과 같은 다른 유형으로 변경할 수 있습니다.\n\n```js\n#플롯\nsns.catplot(data=df, x='school', y='G3', kind='box');\nsns.catplot(data=df, x='school', y='G3', kind='bar');\n```\n\n![이미지](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_5.png)\n\n이제 상자 그림이 분포를 정확하게 보여주지 않는다는 점을 살펴봅니다. 이것은 요즘 이야기가 되고 있는 주제입니다. 사람들은 통계에 익숙하지 않은 사람들을 포함해 상자 플롯을 이해하는 데 어려움을 겪는다. 따라서 이 유형의 플롯에 대한 기본값은 산점도입니다.\n\n<div class=\"content-ad\"></div>\n\n하지만 백분위수 개념을 알고 상자 그림을 보는 데 편한 사람조차도 상자의 선안에 얼마나 많은 데이터가 있는지 파악하기 어려울 수 있습니다.\n\n그래서 seaborn 팀은 이 문제를 해결하려고 다음에 제시될 향상된 상자 그림을 사용했습니다.\n\n## 3. Boxenplot\n\nBoxenplot은 향상된 상자 그림입니다. 왜냐하면 상자 그림의 선분이 없으며 상자의 크기가 각 백분위수의 데이터 양에 따라 다르기 때문입니다. 한 개를 그려봅시다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# BoxenPlot\nsns.boxenplot(data=df, x='traveltime', y='G3');\n```\n\n다음 그림을 보면 상자의 너비가 각 백분위별 관측치의 양에 따라 변하는 것을 볼 수 있습니다. 여행 시간 범주 1(15분 이하)은 대부분의 데이터 포인트가 중간인 10에서 14 사이에 있으며, 분포는 매우 정규와 유사합니다.\n\n<img src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_6.png\" />\n\nTravel Time == 1의 학점만을 분리해 보면 다음과 같은 그래픽을 볼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nsns.displot(df.query('traveltime == 1'),\n              x='G3', kind='kde', aspect=2)\nplt.title('Distribution KDE of Final Grade on Travel Time == 1');\n```\n\n왼쪽의 첫 번째 상자 그림과 매우 관련이 있는 것을 관찰할 수 있습니다: 하단에서 격리된 몇 개의 점에 이어 거의 정규 분포에 가까운 분포가 있습니다.\n\n<img src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_7.png\" />\n\n실제로 상자 그림을 향상시킨 것입니다. 그러나 더 많은 그래픽 유형을 공부할 필요가 있습니다. 계속 전진하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## 4. lmplot\n\nlmplot은 Linear Model plot의 줄임말로, 데이터셋의 간단한 선형 모델을 시각화하는 가장 쉬운 방법입니다. Grade 1과 최종 학년 점수 간의 관계를 확인하고 싶다면, 다음 코드로 선형 모델을 시각화할 수 있습니다.\n\n```js\nsns.lmplot(data=df, x=\"G1\", y='G3')\n```\n\n결과가 표시됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_8.png\" />\n\n하지만 이 함수의 장점이 최고라고 할 수는 없어요. 우리는 색조를 추가하여 다중 수준 회귀를 모의할 수 있어요. 이 경우에는 두 성별이 유사한 성능을 발휘하고 있어서, 그 범주에 기반한 계층 선형 모델을 만드는 것이 의미가 없겠죠.\n\n```js\nsns.lmplot(data=df, x=\"G1\", y= 'G3', hue='sex')\n```\n\n<img src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_9.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n게다가, 대상 변수와 비교하는 다양한 선형 모델을 추가하는 것도 간단합니다. 예를 들어, col 인수를 사용하여 서로 다른 학교에 대한 모델을 추가해봅시다.\n\n```js\nsns.lmplot(data=df, x=\"G1\", y='G3', hue='sex', col='school')\n```\n\n<img src=\"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_10.png\" />\n\n보다 깊은 분석을 위한 매우 유용한 플롯입니다.\n\n<div class=\"content-ad\"></div>\n\n자, 이 글의 마지막 유형을 살펴보겠습니다.\n\n## 5. residplot\n\nresidplot은 선형 회귀의 잔차를 그리는 것입니다. 하지만, 그것이 왜 중요한지요?\n\n음, 선형 모델의 잔차로 하는 테스트 중 하나는 등분산성입니다. 즉, 잔차가 균일해야 한다는 것을 확인하는 것이죠. 우리가 선 (linear)을 따르는 관계를 분석한다면, 오류도 일정 범위 내에서 선을 따를 것이라는 것이 합리적입니다. 그래서 residplot을 볼 때에는 유사한 분산을 가진 직사각형 모양을 보고 싶습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nsns.residplot(data=df, x=\"G1\", y= 'G3')\n```\n\nGrade 1로 최종 성적을 예측하는 선형 모델의 잔차를 보면 거의 균일한 집합을 볼 수 있지만 몇 가지 예외가 있습니다.\n\n![residual plot](/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_11.png)\n\n# 떠나시기 전에\n\n\n<div class=\"content-ad\"></div>\n\n좋아요. 이제 우리는 분석에 사용할 수 있는 또 다른 5가지 그래픽 유형을 갖추었습니다.\n\n좋은 탐색적 데이터 분석은 시간이 걸립니다. 그 과정에서 많은 질문들이 나타나고 데이터에 대한 이해를 풍부하게 만들어줍니다. 그래서 더 많은 작업이 필요한 질문들에 대해 심층적으로 파고들기 위한 향상된 도구 몇 개를 가지고 있는 것이 중요합니다.\n\n- stripplot: 데이터 포인트를 전체로 시각화하는 데 도움이 됩니다. describe 함수 시각화와 유사합니다.\n- catplot: stripplot과 비슷하지만 범주에 대한 것입니다. 점, 막대, 상자와 같은 여러 형태로 나타낼 수 있습니다.\n- boxenplot: 상자 그림의 향상된 버전으로 \"수염에 얼마나 많은 데이터가 있는지\"와 같은 공백을 채웁니다.\n- lmplot: 두 변수에 대한 빠르고 간단한 선형 모델을 생성할 수 있습니다. hue 인수를 사용하여 다중 수준 회귀를 시각화하거나 col 인수로 facet grid를 생성할 수도 있습니다.\n- residplot: 선형 모델의 잔차를 살펴보고 잔차의 등분산성에 대한 이탈이 어디서 발생하는지 확인하는 데 유용합니다.\n\n이 내용이 마음에 들면 팔로우해주세요.\n\n<div class=\"content-ad\"></div>\n\n또한, LinkedIn에서 나를 찾을 수 있습니다. \n\n# 참고문헌","ogImage":{"url":"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_0.png"},"coverImage":"/assets/img/2024-06-19-5UsefulVisualizationstoEnhanceYourAnalysis_0.png","tag":["Tech"],"readingTime":7},{"title":"여기에는 왜 당신의 친구들이 당신보다 더 많은 친구를 가지고 있는지에 대한 이유가 있습니다","description":"","date":"2024-06-19 09:31","slug":"2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou","content":"\n\n![이미지](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_0.png)\n\n친구 역설은 대부분의 사람이 평균적으로 자신의 친구보다 친구가 더 많은 사회 현상입니다. 때로는 이겁니다. 이것이 일반적으로 사실인지에 대한 사실은 인기 있는 주제에 대한 기사에서 확실히 나타나지 않습니다. 한번 조사해 보겠습니다!\n\n이것을 결정하는 것이 어려울 것이라고 생각했을 것 같았지만, 아마 이것이 발견되기까지 1991년까지 걸렸기 때문일지도 모릅니다. 스콧 펠드의 원래 논문인 \"왜 당신의 친구들은 당신보다 더 많은 친구를 갖고 있을까\"에서 그는 이것이 자존감 부족의 원인일 수도 있다고 제안했습니다. 그렇다고 합니다, 사람들이 친구의 친구들마다 명부를 작성하며 친구의 목록을 유지하지는 않거든요. 이것은 1991년에는 진실일지 모릅니다만, 이제 페이스북 시대에는 이것을 쉽게 할 수 있으며 아마 많은 사람들이 그렇게 하고 있을 것입니다.\n\n# 내 책에 얼마나 많은 얼굴이 있을까\n\n<div class=\"content-ad\"></div>\n\n사실, 왜 하지 않을까요? 한번 해볼까요? 페이스북에는 374명의 친구가 있어요. 왜 그런지는 잘 모르겠어요. 어쨌든, 열 명의 친구를 무작위로 선택하여 친구들이 가진 친구의 수를 세 보겠습니다. 아래는 그 결과에요:\n\n```js\n친구 1 - 522\n친구 2 - 451 \n친구 3 - 735\n친구 4 - 397\n친구 5 - 2074\n친구 6 - 534\n친구 7 - 3607\n친구 8 - 237\n친구 9 - 1171\n친구 10 - 690\n```\n\n이들의 친구들이 가진 친구의 수 평균은 1042명입니다. 그러므로 내 친구들이 평균적으로 가지고 있는 친구의 수보다 내가 더 적은 친구를 가지고 있다는 사실인데요. 또한, 이 10명 중에는 제가 친구보다 친구가 더 많은 사람의 숫자가 한 명 빼고 열 명 중 아홉 명이 속해 있어요 — 안쓰럽고 외로운 친구 8번이네요.\n\n하지만 이것이 역설이 아니에요 — 특히 저를 알고 있으면요. 제 네트워크에서 다른 누군가를 한번 살펴볼까요? 친구 2라고 가정해볼게요. 친구 2는 451명의 친구가 있고, 페이스북은 사용자의 개인정보를 자연스럽게 보호하기 때문에 친구 2의 친구들의 프로필을 클릭하여 그들이 가지고 있는 친구의 수를 확인할 수 있어요. 친구 2의 열 명의 무작위 친구들이 가진 친구의 수는 각각 790, 928, 383, 73, 827, 1633, 202, 457, 860, 121입니다. 친구 2의 친구들이 가진 친구의 수 평균은 627명으로, 451명보다 많습니다. 또한, 이 중 여섯 명이 451명보다 많은 친구를 가지고 있어요. 따라서 친구 2는 대부분의 친구들보다 친구가 더 적은 상태에요. 내 친구 모두에 대해 이 연습을 반복한다면, 더 많은 친구를 가진 대부분의 친구들이 자기 친구보다 더 적은 친구를 가지고 있음을 계속해서 발견할 수 있을 거예요, 비록 대부분이 이미 제보다 친구가 더 많은 상태일지라도!\n\n<div class=\"content-ad\"></div>\n\n이것은 역설적이라고 느껴질 것 같아요. 이게 왜 역설적으로 느껴지는지에 대한 직관은 다음과 같은 비유에서 옵니다. 자신의 키를 생각해보세요. 아마도 평균보다 키가 작을 수도 있죠. 아마도 자신보다 키가 작은 친구들이 있을 것입니다. 그래도 평균보다 키가 큰 사람이 적어도 절반은 있을 것을 기대합니다. 결국 이것이 \"평균\"에 대한 직관이죠. 그렇다면 왜 친구들에 대해서는 같은 이치가 적용되지 않을까요? 즉, 친구 2는 우리 친구들보다 친구가 적지만, 분명 많은 친구를 가진 사람들은 그들의 친구들보다 더 많은 친구를 가지고 있을 것입니다. 이는 사실이지만, 이러한 인기 있는 사람들은 극히 드물며, 이것이 친구 수와 키 사이의 차이입니다.\n\n전형적인 소셜 네트워크에서는 많은 사람들이 적은 친구를 가지고 있고, 적은 사람들이 많은 친구를 가지고 있습니다. 매우 인기 있는 사람들은 많은 사람들의 친구들 목록에 포함되어 있습니다. 그리고 인기 없는 사람들은 많은 사람들의 친구들 목록에 포함되지 않습니다. 다시 페이스북 친구 관계를 예로 들어보겠습니다. 분명히 제 페이스북 친구는 적은 편이에요. 그래서 제 친구들보다 더 많은 친구를 갖고 있는 사람들이 많을 것으로 보입니다. 그렇지만 - 여기서 가장 중요한 점은 - 그 사람들은 나와 제 적은 수의 친구를 자신들의 친구 목록에 포함시키지 않을 가능성이 높습니다. 친구 2와 나는 공통의 친구가 두 명뿐이죠. 그래서 친구 2의 친구들은 그들의 친구들이 가진 친구 수를 세어보면 나와 제 적은 수의 친구가 결과를 편향시키는 모습은 볼 수 없을 겁니다.\n\n# 그게 \"가라테 키드\"가 왜 외로웠는지 그 이유일까요?\n\n몇 가지 더 작은 예시를 살펴보면 모두와 그들 친구들을 네트워크에서 세어볼 수 있는데요. 가장 유명한 소셜 네트워크 (적어도 네트워크 이론가들에게는)는 자크리의 가라테 도장입니다. 이렇게 생겼다고 해요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_1.png\" />\n\n이 그래프는 1970년대의 한 가라테 클럽에 있는 34명의 사람들과 수업 외에 상호 작용하는 친구들을 보여줍니다. 색상은 각 회원이 가진 친구의 수를 안내하는 데 사용됩니다. 한 사람이 한 명의 친구만 가지고 있고 또 다른 사람은 아홉 명의 친구를 가지고 있다는 것을 볼 수 있습니다! 이미 이 그래프는 손으로 세는 것이 너무 큽니다. 그러나 컴퓨터는 네트워크의 각 노드를 단계별로 통과하여 친구와 친구의 친구를 세는 데 도움을 줄 수 있습니다. 그래서 Zachary의 가라테 클럽에서는 다음과 같습니다.\n\n```js\n평균적으로 친구보다 친구가 더 적은 사람의 비율은 85.29%입니다.\n```\n\n```js\n자신의 대부분의 친구보다 친구가 적은 사람의 비율은 70.59%입니다.\n```\n\n<div class=\"content-ad\"></div>\n\n![img](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_2.png)\n\n위의 히스토그램은 데이터를 자세히 보여줍니다. 많은 친구를 가진 소수의 사람들이 다시 한 번 중요한 특징을 보실 수 있습니다. 분명히 그 소수의 사람들은 그들의 친구들보다 더 많은 친구를 가지고 있을 것입니다. 그러나 이것이 핵심입니다 — 그런 사람들은 소수뿐입니다! \"친구의 친구\"의 전체 분포를 살펴보면 상당히 평평해지고, 많은 친구를 만나기가 더 가능해집니다. 아래의 두 히스토그램은 네트워크 내 각 사람의 평균 및 중앙값 친구 수를 보여줍니다. 어느 경우든 각 사람이 대략 아홉 명의 친구를 가지고 있다는 것에 유의하십시오. 이는 네트워크 내 대부분의 사람들보다 훨씬 많습니다! 실제로 34명 중 4명만이 아홉 명 이상의 친구를 가지고 있습니다.\n\n# 큰 소셜 네트워크에 대해 생각해보세요\n\n이제 페이스북으로 돌아가 봅시다. 이번에는 십 명의 개인으로부터 수집된 데이터를 사용할 것이며, 익명화되어 공개될 것입니다. 이 데이터는 다양한 사회적 관계를 가진 이 열 명과 연결된 약 4000명의 사람들을 포함합니다. 해당 소셜 네트워크는 다음과 같습니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 마크다운 형식으로 변경합니다.\n\n\n![Network Details](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_3.png)\n\n우리는 이 네트워크의 친구와 친구의 친구도 세어볼 수 있습니다. 자세한 내용은 이렇습니다.\n\n![Details](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_4.png)\n\n요약하면, 결론은 이렇습니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\n평균적으로 친구보다 더 적은 친구를 갖고 있는 사람의 비율은 87.47%입니다.\n```\n\n```js\n자신의 대다수 친구보다 더 적은 친구를 갖고 있는 사람의 비율은 71.92%입니다.\n```\n\n매우 유사해요! 하지만 이것이 우연일 수도 있습니다. 결국 데이터 포인트는 두 개 뿐이죠. 모든 소셜 네트워크의 이 아이디어를 테스트하는 방법은 무엇인가요? 시뮬레이션!\n\n# 가짜로 만든다면 상관없어요\n\n<div class=\"content-ad\"></div>\n\n시뮬레이션은 어떤 것을 이해하기 위해 그것의 규모 모델을 연구하는 도구입니다. 바람 터널에서의 모형 비행기 날개는 대표적인 예입니다. 오늘날 많은 시뮬레이션은 컴퓨터 상에서 전적으로 이루어집니다. 소셜 네트워크의 맥락에서도 다양한 모형이 있지만, 순전히 게을러서 바바라시-알버트 모형이라 불리는 것을 선택할 것입니다. 왜냐하면 사용 중인 컴퓨터 패키지인 NetworkX에 이미 구현되어 있기 때문입니다. 만약 우리가 34명의 사람(카라테 클럽과 동일한 수)으로 이루어진 모조 소셜 네트워크를 생성한다면, 이렇게 보일 것입니다.\n\n![image](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_5.png)\n\n카라테 클럽과 그리 다르지 않게 보이지 않나요? 숫자 데이터도 유사합니다.\n\n![image](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_6.png)\n\n<div class=\"content-ad\"></div>\n\n이 예시에서 중요한 숫자들은 다음과 같습니다.\n\n```js\n평균적으로 자기 친구들보다 더 적은 친구를 가진 사람들의 비율은 79.41%입니다.\n```\n\n```js\n자기 친구들 대부분보다 더 적은 친구를 가진 사람들의 비율은 70.59%입니다.\n```\n\n이것도 좋지만, 시뮬레이션의 진정한 매력은 여러 예시를 빠르게 확인할 수 있다는 점입니다. 위의 것은 단지 하나의 가짜 소셜 네트워크에 불과합니다. 우리의 결론에 대한 완전한 확신을 갖기 위해서 (물론 모델 가정에 상대적으로), 무작위로 생성된 다수의 소셜 네트워크에서 많은 시뮬레이션을 실행해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 모든 그래프를 시뮬레이션해 봐요!\n\n만약 34명의 사람으로 이루어진 10,000개의 랜덤 소셜 네트워크에서 위 연습을 반복한다면,\n\n```js\n평균적으로 자신의 친구들보다 친구 수가 적은 사람들의 비율은 79.31% 입니다.\n```\n\n```js\n자신의 대부분의 친구들보다 친구 수가 적은 사람들의 비율은 65.31% 입니다.\n```\n\n<div class=\"content-ad\"></div>\n\n그래서 우리는 이제 34명의 사람이 있는 Barabási–Albert 모델과 같은 특성을 가진 어떤 소셜 네트워크에서도 친구 역설이 계속된다고 자신 있게 말할 수 있습니다. 하지만 쉽게 할 수 있는 다른 것은 네트워크의 사람 수를 바꾸어서 대규모 네트워크에서도 추세가 지속되는지 확인하는 것입니다. 사람 수를 늘리면 상황이 훨씬 나빠집니다. 소셜 네트워크의 사람 수가 증가함에 따라 자신보다 친구가 적은 사람들의 비율(대부분 또는 평균적으로)도 증가합니다.\n\n![이미지](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_7.png)\n\n# 멋져, 멋져. 우울한 소식 이외에는 없니?\n\n이러한 역설은 친구를 넘어서도 유지된다는 것이 증명되었습니다. 자신과의 비교에서 소득, 트위터 팔로워 및 얼마나 행복한지에 대해 친구들과 비교했을 때도 아마 당신은 부족하다고 생각될 것입니다. 하지만, 좋아, 이런 건 충분해요 - 이제 우리 모두 기분 나빠하게 느끼잖아요! 분명히 이런 모든 것에서 긍정적으로 얻을 수 있는 점이 있을 거에요, 그렇죠? 네!\n\n<div class=\"content-ad\"></div>\n\n친구들이 당신보다 더 많은 연락처를 가지고 있기 때문에, 그들이 바이러스나 다른 커뮤니티를 통해 전파되는 것 등을 먼저 포착할 가능성이 더 높을 것입니다. 실제로 연구자들은 질병의 전파를 측정하기 위해 무작위로 선택된 사람들을 추적하는 대신에, 그 무작위로 선택된 사람들에게 친구의 이름을 말하고 그 친구를 추적하는 것이 훨씬 효율적임을 보였습니다! 연구에서, 친구 그룹은 처음 선택된 사람들보다 2주 정도 더 빨리 아프게 되었습니다. 친구 역설에 대해 발견될 대부분의 다른 응용 프로그램들이 아마도 있을 것입니다.\n\n그러나 마지막으로 물어보고 싶은 점이 하나 있습니다: 친구 역설은 어떤 소셜 네트워크에서든 반드시 발생해야 하는가요? 이에 대한 답은 예와 아니요입니다. 역설을 사용하는 그 평균에 대한 주장에 대해서는 수학적으로 증명될 수 있는 만큼 어떤 소셜 네트워크에도 해당된다는 것이 사실입니다. 즉, 평균적인 사람들이 자신의 친구들의 수보다 작거나 같다는 주장은 상상할 수 있는 모든 소셜 네트워크에 대해 참이다. 친구들이 다수를 차지한다는 주장을 사용할 경우(대다수의 친구가 엄격히 더 많은 친구를 가지고 있는 경우), 답은 아니요입니다. 키 요소는 인기있는 사람들의 존재 또는 비존재에 있습니다. 우리는 친구들보다 더 많은 친구를 가진 친구가 대부분인 완전히 무작위인 소셜 네트워크를 만들 수도 있습니다. 다음 네트워크를 고려해보세요. 다시 34명을 기준으로 한 네트워크입니다.\n\n![네트워크 그림](/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_8.png)\n\n<div class=\"content-ad\"></div>\n\n```js\n평균적으로 친구보다 친구가 더 많은 사람의 비율은 44.12%입니다.\n```\n\n```js\n친구들보다 더 적은 친구를 가진 사람들의 비율은 44.12%입니다.\n```\n\n친구들의 구체적인 분포는 차이를 명확히 보여줍니다. 친구의 수는 평균 주변으로 골고루 분포되어 있습니다. 이는 사람들의 키를 보는 것과 유사합니다. 대략 절반의 사람들은 자신의 친구보다 더 적은 친구를 가지고 있고, 나머지 반은 자신의 친구보다 더 많은 친구를 가지고 있으며, 나머지는 자신의 친구들의 평균치와 정확히 같은 수의 친구를 가지고 있습니다.\n\n<img src=\"/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_9.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n물론, 모두가 정확히 같은 수의 친구를 가지고 있다면, 그들은 친구들보다 친구 수가 똑같을 것이라는 것은 다소 명백할 것입니다! 그러나 친구들의 분포가 더 균등하게 퍼져 있을 때에도 이는 마찬가지입니다. 이것은 무엇을 시사합니까? 저는 이것이 평등한 공동체에 대한 호의적인 주장을 뒷받침한다는 것을 시사하며, 큰 수의 연결을 유지하는 과정에서 발생하는 복잡성을 수용하기 위해 친구 관계, 기업, Twitter 팔로워 등과 같은 계층적 네트워크가 자연스럽게 성장하는 것으로 보입니다. 그러나 이것은 다른 블로그 포스트의 주제입니다. 한편,\n\n이것을 공유하지 마세요. 그 대신에, 친구에게 공유하라고 말하세요.\n\n(이 기사에 대한 모든 시뮬레이션과 데이터는 아래의 GitHub에서 확인하실 수 있습니다.)","ogImage":{"url":"/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_0.png"},"coverImage":"/assets/img/2024-06-19-Hereswhyyourfriendshavemorefriendsthanyou_0.png","tag":["Tech"],"readingTime":8},{"title":"2일차 상호작용형 데이터 시각화  라이브러리 선택하기 ","description":"","date":"2024-06-19 09:30","slug":"2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary","content":"\n\n![Data visualisation](/assets/img/2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary_0.png)\n\n데이터 시각화, 이 주제는 다양한 산업에서 실험되어 왔고, 많은 분들이 이미 경험해 보셨을 거에요, 그런가요? 음, 여전히 많은 사람들이 자신의 데이터를 시각화하는 데 어려움을 겪고 있다는 사실에 놀라실 겁니다. 데이터를 시각화하는 차트를 만들기 위해서는 눈에 잘 띄고 원하는 모든 요소를 포함할 수 있는 새로운 코딩 기술이 필요합니다. 파이썬이라는 말을 들어본 적이 있나요? 코딩을 배우는 데에는 상당한 노력과 땀, 그리고 눈물이 필요해요. 하지만 우리는 좀 다르게 하고 싶어요.\n\n그래서 우리의 첫 번째 글에서는 업로드된 데이터를 처리하고 다른 웹 앱에 포함될 수 있는 멋진 대화 형 그래프를 토출하기를 원하는 에이전트를 원했다는 이야기를 나눴어요. 두 가지 선택지가 있었어요. 우리만의 그래프 라이브러리를 만드는 불편함을 감수할 건지, 아니면 이미 있는 오픈 소스를 사용할 건지 말이죠. 삶을 어렵게 만들 이유가 없죠? 그래서 우리는 오픈 소스 옵션을 선택했어요. 우리가 선택한 라이브러리를 보여드릴게요. 만약 데이터 시각화가 당신의 분야라면 꼭 한 번 확인해보세요...\n\n# 선택된 라이브러리\n\n<div class=\"content-ad\"></div>\n\n생성된 차트가 상호작용적이 되려면 클라이언트 측(브라우저 내)에서 렌더링되어야 했습니다. 서버 측에서 차트를 생성하려면 차트를 정적 png 또는 jpeg 파일로 저장한 다음 해당 파일을 클라이언트에 제공하여 표시해야 합니다. 문제는 이러한 차트가 정적이 된다는 점입니다. ChatGPT가 데이터를로드하면 시각화 차트를 생성하는 방식과 유사합니다. 우리가 커뮤니티를 위해 만들고자 했던 것은 이것보다 더 뛰어난 것이었습니다. 그래서 몇 차례의 연구 끝에 우리는 결정을 내렸습니다.👇\n\n## ECharts\n\n# ECharts란?\n\nECharts는 멋진 오픈 소스 자바스크립트 데이터 시각화 라이브러리로, 강력한 렌더링 엔진을 제공하여 디자인이 우아하고 브라우저에서 표시될 때 상호작용적인 다양한 데이터 시각화 차트를 만들 수 있습니다. 📈🙌\n\n<div class=\"content-ad\"></div>\n\nECharts의 각 차트에는 다른 데이터 구조가 있어서 차트가 제대로 표시됩니다. 놀라운 다양한 차트뿐만 아니라 활기찬 커뮤니티도 자랑합니다, 그래서 지원이 필요할 때 언제나 단기간에 부족함이 없습니다 💪.\n\n아래의 예시들을 확인해보세요👇\n\n![Example 1](/assets/img/2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary_1.png)\n\n![Example 2](/assets/img/2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary_2.png)\n\n<div class=\"content-ad\"></div>\n\n# 이것이 AI 에이전트와 어떤 관련이 있는지 알아보겠어요.\n\n알겠어요, ECharts를 선택했죠? ✅ 다음 단계는 에이전트의 인프라를 설정해야 해요. 앱의 클라이언트 측에 전달할 수 있는 구성요소를 만들어야 해요. 이를 통해 ECharts가 제공하는 모든 차트 유형을 보여줄 수 있어요 🤝. 그래서 계획은 이렇습니다: 에이전트는 업로드된 데이터의 스키마에 따라 차트를 선택할 거예요. 그런 다음 차트가 필요로 하는 데이터 구조를 결정하고, 원시 데이터를 처리하여 해당 구조를 채우고, 마지막으로 브라우저에서 렌더링할 수 있는 형태로 패키지화해야 해요. 계획처럼 듣긴 하죠 🎯? AI 에이전트를 위한 시스템 아키텍처를 설계해 봅시다. Article 3에서 만납시다 🤝.\n\n![image](/assets/img/2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary_3.png)\n\n# 함께 참여하세요!\n\n<div class=\"content-ad\"></div>\n\n계속 주목해주세요! 앞으로 30일 동안 매일 글을 게시할 예정이에요. 저희 진행 상황에 대한 업데이트도 있을 거예요 🚀. 만약 이 에이전트의 첫 사용자 중 한 명이 되고 싶다면 www.cubode.com 으로 이동해서 첫 액세스를 받으세요.","ogImage":{"url":"/assets/img/2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary_0.png"},"coverImage":"/assets/img/2024-06-19-Day2InteractiveDataVisualisationChoosingALibrary_0.png","tag":["Tech"],"readingTime":3},{"title":"SQL과 Power BI를 활용한 고객 세분화 보고서","description":"","date":"2024-06-19 09:28","slug":"2024-06-19-CustomerSegmentationReportwithSQLandPowerBI","content":"\n\n이 프로젝트에서는 RFM(Recency, Frequency, Monetary) 개념을 사용하여 판매 데이터 세트를 분석할 것입니다. 저의 목표는 고객을 세분화하여 최상위 구매자와 무시되고 있을 수 있는 고객 및 그들을 유지하기 위해 타겟 광고 노력이 더 필요한 사람들을 식별하는 것입니다.\n\nRFM 보고서는 세 가지 주요 지표를 사용하여 고객을 세분화하는 방법입니다:\n\n1. Recency(최근성): 고객의 마지막 구매가 얼마나 오래되었나요?\n   \n2. Frequency(빈도): 고객이 얼마나 자주 구매하나요?\n\n<div class=\"content-ad\"></div>\n\n3: 금액: 고객이 구매에 얼마나 지출하는가?\n\n## 프로젝트 계획\n\n- 데이터 정리\n- 데이터셋 개요\n- 분석\n- 최근성, 빈도, 금액 (RFM) 보고서\n- RFM을 사용한 고객 세분화\n- Power BI 대시보드\n\n## 1. 데이터 정리\n\n<div class=\"content-ad\"></div>\n\n데이터를 조사한 결과 특정 기능에 누락된 값이 있는 것을 발견했어요. 그래서 이를 \"없음\"으로 대체해 주었어요. 다행히도, 저가 분석할 주요 기능들에는 누락된 값이 없었어요.\n\n```js\n-- 데이터 확인하기 --\nselect*\nfrom [dbo].[sales_data];\n\n-- 데이터 정리하기 --\n\n-- Addressline2, Postalcode 및 State 열의 누락된 값 찾기\nselect *\nfrom [dbo].[sales_data]\nwhere ADDRESSLINE2 IS NULL OR STATE IS NULL OR POSTALCODE IS NULL;\n\n-- 누락된 값 처리하기\nupdate [dbo].[sales_data]\nset ADDRESSLINE2 = case when ADDRESSLINE2 IS NULL then 'None' else ADDRESSLINE2 end,\n    STATE = case when STATE IS NULL then 'Unknown States' else STATE end,\n    POSTALCODE = case when POSTALCODE IS NULL then 'None' else POSTALCODE end;\n```\n\n![이미지](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_0.png)\n\n## 2. 데이터셋 개요\n\n<div class=\"content-ad\"></div>\n\n2.1 몇 가지 변수에서의 독특한 관찰\n\n회사는 \"Productline\" 열에서 나타나는 것처럼 열차, 오토바이, 비행기, 클래식 자동차 등의 판매에 특화되어 있습니다. 데이터는 2003년부터 2005년까지의 기간을 다룹니다.\n\n```js\n-- 일부 특성의 고유 관측치\nselect distinct STATUS from [dbo].[sales_data]\nselect distinct YEAR_ID from [dbo].[sales_data]\nselect distinct PRODUCTLINE from [dbo].[sales_data]\nselect distinct COUNTRY from [dbo].[sales_data]\nselect distinct DEALSIZE from [dbo].[sales_data]\nselect distinct TERRITORY from [dbo].[sales_data]\n```\n\n<img src=\"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_1.png\" />\n\n<div class=\"content-ad\"></div>\n\n1.2 각 연도별 고유한 월\n\n2005년의 월만 표시한 것은 그렇다고 하더라도, 2003년과 2004년은 둘 다 모든 12개월을 포함했다는 것을 기억하는 것이 중요합니다. 아래 출력에서 확인할 수 있듯이, 2005년 데이터는 5월까지만 있는 것을 알 수 있습니다.\n\n```js\n-- 각 연도별 고유한 월\nselect distinct MONTH_ID as Month_Id_2003  from [dbo].[sales_data]  where YEAR_ID = 2003 order by MONTH_ID ASC;\n\nselect MONTH_ID as Month_Id_2004 from [dbo].[sales_data]  where YEAR_ID = 2004 order by MONTH_ID ASC;\n\nselect distinct MONTH_ID  as Month_Id_2005 from [dbo].[sales_data]  where YEAR_ID = 2005 order by MONTH_ID ASC;\n```\n\n<img src=\"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_2.png\" />\n\n<div class=\"content-ad\"></div>\n\n## 2. 분석\n\n2.1 각 제품 라인에서 얼마나 벌고 있나요?\n\n클래식 자동차가 가장 많은 구매를 했고, 가장 적은 구매는 기차였습니다.\n\n```js\n--- 제품 라인별로 매출을 그룹화하여 계산해 봅시다\nselect PRODUCTLINE, SUM(sales) Revenue \nfrom   [dbo].[sales_data]\ngroup by PRODUCTLINE\norder by 2 desc\n```\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_3.png\" />\n\n2.2 매년 매출액은 어떻게 되나요?\n\n결과를 통해 2004년이 가장 높은 매출을 기록했으며, 2003년이 이어서 나왔으며, 2005년의 매출액은 가장 낮았다는 것을 알 수 있습니다. 이 차이는 2005년이 단 5개월에 불과하기 때문에 전체 매출량이 2003년과 2004년의 연간 데이터와 비교하여 상대적으로 작기 때문입니다.\n\n```js\n-- 매년 매출액\nselect YEAR_ID, SUM(sales) Revenue \nfrom [dbo].[sales_data]\ngroup by YEAR_ID\norder by 2 desc \n```\n\n<div class=\"content-ad\"></div>\n\n\n![Customer Segmentation Report](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_4.png)\n\n2.3. What was the best month for sales in a specific year and how much was earned that month?\n\nIn 2003\n\nThere were more purchases in November and the next highest sales was October.\n\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 2003년 정보에 대하여\nselect MONTH_ID, sum(sales) as 수익, count(ordernumber) as 빈도 \nfrom [dbo].[sales_data]\nwhere YEAR_ID = 2003\ngroup by MONTH_ID\norder by 2 desc\n```\n\n![고객 세분화 보고서](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_5.png)\n\n2004년에도\n\n2004년에도 11월에 가장 높은 매출이 있었으며, 다음으로 10월에 높은 매출이 있었습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 2004 년도를 위한\nselect MONTH_ID, sum(sales) as Revenue, count(ordernumber) as Frequency \nfrom [dbo].[sales_data]\nwhere YEAR_ID = 2004\ngroup by MONTH_ID\norder by 2 desc\n```\n\n<img src=\"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_6.png\" />\n\n2005년도에\n\n2005년에는 최다 구매가 5월에 발생하여 올해의 판매 정점을 나타냈습니다. 세 연도 동안의 처음 다섯 개월 구매를 비교하면, 2005년의 판매량이 비교적 높고, 그 다음이 2004년이며, 가장 낮은 것은 2003년입니다. 이는 회사의 매출이 연도를 통틀어 꾸준히 증가하고 있다는 것을 시사합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 2005년 기준\nselect MONTH_ID, sum(sales) as 매출액, count(ordernumber) as 주문빈도\nfrom [dbo].[sales_data]\nwhere YEAR_ID = 2005\ngroup by MONTH_ID\norder by 2 desc\n```\n\n![이미지](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_7.png)\n\n2.4. 회사가 11월에 일반적으로 가장 높은 매출액을 기록하는 제품은 무엇인가요?\n\n11월에는 클래식 및 빈티지 자동차의 구매가 두드러지게 증가하여 2003년부터 2004년까지 전반적인 높은 매출에 크게 기여했습니다. 반면에 기차는 동일한 달에 회사의 수익 최소화를 보여주었습니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 2003년 11월\nselect MONTH_ID as November_2003, PRODUCTLINE, sum(sales) as Revenue, count(ordernumber) as Frequency \nfrom [dbo].[sales_data]\nwhere YEAR_ID = 2003 and MONTH_ID=11\ngroup by MONTH_ID, PRODUCTLINE\norder by 3 desc\n\n-- 2004년 11월\nselect MONTH_ID as November_2004,PRODUCTLINE, sum(sales) as Revenue, count(ordernumber) as Frequency \nfrom [dbo].[sales_data]\nwhere YEAR_ID = 2004 and MONTH_ID=11\ngroup by MONTH_ID, PRODUCTLINE\norder by 3 desc\n```\n\n<img src=\"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_8.png\" />\n\n## 4. 최근성, 주파수 및 금액 (RFM) 보고서\n\n최근성은 마지막 주문 또는 구매 이후 경과한 일 수를 나타내며, Euro Shopping Channel과 La Rochelle Gifts는 데이터에서 가장 최근 구매를 한 날짜에 해당합니다(최대 날짜). 최근성 값이 높을수록 마지막 구매 이후의 기간이 길어집니다. RFM 결과에 따르면 Toys of Finland. Co는 데이터 집합에서 현재 또는 최대 날짜로부터 100일 이상이 지난 마지막 구매를 했습니다.\n또한, 주문 사이의 평균 일 수를 살펴보면 고객들이 얼마나 자주 구매를 하는지에 대한 통찰을 제공합니다. Euro Shopping Channel은 평균 3일 간격으로 가장 꾸준한 고객 중 하나로 나타납니다.\n마찬가지로, 고객 당 금액 값을 검토하면 Euro Shopping Channel이 가장 많이 지출한 것을 확인할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- RFM 분석\nselect CUSTOMERNAME,\n   sum(sales) as MonetaryValue,\n   avg(sales) as AvgMonetaryValue,\n   count(*) as Frequency_of_orders,\n   count(distinct convert(DATE, orderdate)) AS Number_of_unique_order_dates,\n   convert(INT, round(cast(datediff(dd, min(ORDERDATE), max(ORDERDATE)) as decimal)/(count(*)-1),0)) as [Aveg_day_between_orders],\n   datediff(DD,max(orderdate),( select max(orderdate) from [dbo].[sales_data])) as recency \nfrom [dbo].[sales_data]\ngroup by CUSTOMERNAME\nhaving count(*)>1\norder by recency\n```\n\n![RFM Analysis](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_9.png)\n\n## 5.0 RFM 보고서 기반의 고객 세분화\n\n제공된 결과는 RFM 기준에 따른 고객 세분화에 대한 통찰을 제공합니다. rfm_recency=1에 속한 고객은 최근 구매 이후 지난 기간이 길어서, 데이터 세트에서 최대 날짜로부터 112일까지 구매를 한 Boards & Toys Co.와 같이 특징 지어집니다. 반면, rfm_recency=4는 최근 구매를 나타내며, 데이터 세트에서 가장 최근 구매가 14일 전인 Salzburg Collectables과 같은 상황입니다. 이는 rfm_recency=1 또는 2에 속한 고객이 마지막 주문 이후 상당한 시간이 지나자 \"잠재적 이탈 고객\"일 수 있다는 것을 나타냅니다.\n\n\n<div class=\"content-ad\"></div>\n\n비슷하게, rfm_monetary에 대해서는 소비가 높은 고객이 rfm_monetary=4로 분류되며, 소비가 적은 고객은 rfm_monetary=1로 분류됩니다. rfm_frequency에 대해, rfm_frequency=4인 고객은 가장 많은 주문을 보유하고 있으며, rfm_frequency=1은 가장 적은 주문을 가진 고객을 나타냅니다.\n\n이러한 RFM 카테고리를 분석하여, 최근 구매를 보유한 최고의 고객은(rfm_recency=4), 가장 높은 금액을 지출한 고객(rfm_monetary=4) 및 가장 많은 주문을 가진 고객(rfm_frequency=4)으로 식별할 수 있습니다.\n\n\n---고객을 네 가지 세분류로 분류합니다.\n\nwith customer_segment as\n(\n select \n   CUSTOMERNAME,\n   sum(sales) as MonetaryValue,\n   count(ordernumber) as Frequency_of_orders,\n   max (orderdate) as last_order_date,\n   (select max(orderdate) from [dbo].[sales_data]) as max_order_date,\n   datediff(dd, max(orderdate),(select max(orderdate) from [dbo].[sales_data])) as Recency \n from [dbo].[sales_data]\n group by CUSTOMERNAME \n)\nselect*,\n  NTILE(4) OVER (order by Recency desc) as rfm_recency,\n  NTILE(4) OVER (order by Frequency_of_orders) as rfm_frequency,\n  NTILE(4) OVER (order by MonetaryValue) as rfm_monetary\nfrom customer_segment \n\n\n![Customer Segmentation Report](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_10.png)\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_11.png\" />\n\n5.1 상기 Segmentation을 기반으로 고객에게 Segment 이름 할당하기.\n고객 Segmentation을 고려할 때, 복합 코드가 444인 고객이 최고의 고객을 대표합니다. 첫 번째 4는 rfm_recency 그룹 4에 속한 고객을 나타내며 최근 구매를 의미합니다. 두 번째 4는 rfm_frequency 그룹 4에 속한 고객을 나타내어 많은 주문을 했음을 의미합니다. 마지막 4는 rfm_monetary 그룹 4에 속한 고객들을 나타내어 더 많은 돈을 소비했다는 것을 의미합니다.\n\n반대로, 복합 그룹이 111인 고객들은 오랜 시간 구매를 하지 않은, 주문 수가 적고 지금까지 가장 적은 금액을 소비한 고객들입니다.\n\n```js\n-- 우리 최고의 고객은 누구인가요?\n\nDROP TABLE IF EXISTS #customer_segment; -- 테이블 #rfm 생성\nwith customer_segment as\n(\n select \n CUSTOMERNAME,\n sum(sales) as MonetaryValue,\n avg(sales) as AvgMonetaryValue,\n COUNT(ordernumber) as Frequency,\n MAX(orderdate) as last_order_date,\n (select max(orderdate) from [dbo].[sales_data]) as max_oder_date,\n DATEDIFF(DD, MAX(orderdate),(select max(orderdate) from [dbo].[sales_data])) as Recency \n from [dbo].[sales_data]\n group by CUSTOMERNAME \n),\n\nrfm_segmentation as \n  (\n  select*,\n  NTILE(4) OVER (order by Recency desc)as rfm_recency,\n  NTILE(4) OVER (order by Frequency) as rfm_frequency,\n  NTILE(4) OVER (order by MonetaryValue) as rfm_monetary\n   --(NTILE(4) OVER (order by Recency)) + (NTILE(4) OVER (order by Frequency)) + (NTILE(4) OVER (order by AvgMonetaryValue)) AS CompositeScore\n  from customer_segment \n )\n\nselect \n c.*, rfm_recency + rfm_frequency + rfm_monetary as rfm_cell,\n CAST(rfm_recency as varchar)+ CAST(rfm_frequency as varchar)+ CAST(rfm_monetary as varchar) as rfm_cell_string\n\ninto #customer_segment -- 스크립트를 실행하면서 테이블을 생성하는 데 도움이 됩니다. 맨 위에 #rfm 테이블이 이미 생성되었습니다.\nfrom rfm_segmentation c\n\n\n\nselect CUSTOMERNAME, rfm_recency,rfm_frequency, rfm_monetary,rfm_cell_string,\n  CASE\n  when rfm_cell_string in (111, 112, 121, 122, 211, 212, 114, 141, 123, 131,132,141,142,221, 231,232,242,241) then '잃어버린 나 새로운 고객'\n  when rfm_cell_string in (222,223, 233,134,143,133,144,234,232,244) then '사라져가지만 돈을 많이 쓰는 고객' -- 회사는 그들이 떠나는 것을 방치해서는 안됩니다\n  when rfm_cell_string in (411,412,421,422,342,442,322,332,311) then '정기적이지만 소비가 적은 고객'\n  when rfm_cell_string in (424,423,432,324,414,324,331) then '좋은 고객들' \n  when rfm_cell_string in (444,443,434,344,333,334, 343,433) then '최고의 고객들' \n  end rfm_segment\n\nfrom #customer_segment\n```\n\n<div class=\"content-ad\"></div>\n\n![RFM Report with Power BI](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_12.png) \n\n![RFM Report with Power BI](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_13.png) \n\n## 6. Power BI를 사용한 RFM 보고서\n\nPower BI 대시보드를 통해 Euro Shopping Channel이 최고의 성과를 거두었음이 명확히 드러납니다. 해당 채널은 가장 많은 고유 주문을 하여 총 25개를 달성하였고, 또한 최근 구매자 중에서도 속해 있습니다.\n\n<div class=\"content-ad\"></div>\n\n대시보드에서 주목할 만한 관찰 중 하나는 고객 세그먼트별 매출을 분석할 때 \"우수 고객\"으로 표시된 범주가 의외로 가장 낮은 매출을 보인다는 것입니다. 이는 세분화 규칙을 적용한 후 이 범주에 속하는 고객이 단 하나뿐이기 때문입니다. 따라서 이 한 명의 고객으로부터 발생한 총 매출이 다른 세그먼트에 속한 여러 고객들의 집계 매출보다 낮을 수밖에 없는 것입니다.\n\n![이미지](/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_14.png)\n\n끝\n프로젝트를 읽어 주셔서 감사합니다.\nLinkedIn: www.linkedin.com/in/kofi-duodu-siriboe-a184021b2","ogImage":{"url":"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_0.png"},"coverImage":"/assets/img/2024-06-19-CustomerSegmentationReportwithSQLandPowerBI_0.png","tag":["Tech"],"readingTime":11}],"page":"80","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}