{"pageProps":{"posts":[{"title":"트라이햅미 케노비 라이트업","description":"","date":"2024-06-19 15:01","slug":"2024-06-19-TryHackMeKenobiWriteup","content":"\n\n리눅스 머신을 해킹해 봅시다.\n\n![TryHackMeKenobiWriteup_0](/assets/img/2024-06-19-TryHackMeKenobiWriteup_0.png)\n\n**Task 1** 취약한 머신 배포하기\n\n열려 있는 포트: 7 (nmap `머신의 IP 주소`-vvv)\n\n<div class=\"content-ad\"></div>\n\n과제 2 Samba 공유 목록 열거\n\nSamba는 Linux 및 Unix 용 표준 Windows 상호 운용성 프로그램 스위트입니다. 회사의 내부망 또는 인터넷에서 파일, 프린터 및 기타 공유 리소스에 대한 액세스 및 사용을 끝 사용자에게 허용합니다. 이는 종종 네트워크 파일 시스템으로 불립니다.\n\nSamba는 Server Message Block (SMB)의 공통 클라이언트/서버 프로토콜에 기반을 두고 있습니다. SMB는 Windows 전용으로 개발되었으며, Samba가 없으면 같은 네트워크에 속해 있더라도 다른 컴퓨터 플랫폼이 Windows 기계로부터 격리될 수 있습니다.\n\nnmap을 사용하여 기계를 SMB 공유에 대해 열거할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nNmap은 다양한 네트워킹 작업을 자동화하여 실행할 수 있는 능력을 가지고 있어요. 공유를 열거하는 스크립트가 있어요!\n\n\nnmap -p 445 --script=smb-enum-shares.nse,smb-enum-users.nse 10.10.3.132\n\n\nSMB에는 445번과 139번이라는 두 개의 포트가 있어요.\n\n![이미지](/assets/img/2024-06-19-TryHackMeKenobiWriteup_1.png)\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-TryHackMeKenobiWriteup_2.png\" />\n\n위의 nmap 명령어를 사용하여 찾은 공유는 몇 개인가요?\n\n발견된 공유: 3\n\n대부분의 Linux 배포판에는 이미 smbclient가 설치되어 있습니다. 공유 중 하나를 검사해 봅시다.\n\n<div class=\"content-ad\"></div>\n\nsmbclient //`machine’s ip`/anonymous\n\n당신의 컴퓨터를 사용하여, 해당 컴퓨터의 네트워크 공유에 연결하세요.\n\n![이미지](/assets/img/2024-06-19-TryHackMeKenobiWriteup_3.png)\n\n연결되었으면 공유된 파일 목록을 나열하세요. 어떤 파일을 볼 수 있나요?\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-TryHackMeKenobiWriteup_4.png)\n\n파일: log.txt\n\nSMB 공유를 재귀적으로 다운로드할 수도 있습니다. 아이디와 비밀번호를 'nothing'으로 제출하실 수 있습니다.\n\nsmbget -R smb://`머신의 아이피`/anonymous\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-TryHackMeKenobiWriteup_5.png)\n\n공유된 파일을 열어보세요. 발견된 몇 가지 흥미로운 정보들이 있습니다.\n\n유저를 위해 SSH 키를 생성할 때 Kenobi에 대한 생성된 정보\n\nProFTPD 서버에 관한 정보.\n\n\n<div class=\"content-ad\"></div>\n\nFTP는 어떤 포트에서 실행되나요?\n\n![이미지](/assets/img/2024-06-19-TryHackMeKenobiWriteup_6.png)\n\n포트: 21\n\n이전에 실행한 nmap 포트 스캔에서 포트 111에서 rpcbind 서비스가 실행 중인 것을 확인했을 것입니다. 이 서비스는 원격 프로시저 호출(RPC) 프로그램 번호를 유니버설 주소로 변환하는 서버에 불과합니다. RPC 서비스가 시작되면 rpcbind에게 해당 서비스가 수신 대기하고 있는 주소 및 제공할 RPC 프로그램 번호를 알려줍니다.\n\n<div class=\"content-ad\"></div>\n\n우리 경우에는 포트 111은 네트워크 파일 시스템에 액세스하는 것입니다. 이를 열어보기 위해 nmap을 사용합시다.\n\nnmap -p 111 — script=nfs-ls,nfs-statfs,nfs-showmount 10.10.3.132\n\n어떤 mount를 볼 수 있나요?\n\n![네트워크 파일 시스템](/assets/img/2024-06-19-TryHackMeKenobiWriteup_7.png)\n\n<div class=\"content-ad\"></div>\n\n`Mount: /var`\n\n**Task 3: ProFtpd를 사용하여 초기 액세스 얻기**\n\nProFtpd는 Unix 및 Windows 시스템과 호환되는 무료 오픈 소스 FTP 서버입니다. 과거 소프트웨어 버전에서 취약점이 발견되기도 했습니다.\n\n아래의 질문에 답해주세요.\n\n<div class=\"content-ad\"></div>\n\nProFtpd의 버전을 확인해 봅시다. FTP 포트에 머신에 netcat을 사용하여 연결하세요.\n\n```bash\nnc `머신의 IP` 21\n```\n\n버전은 무엇인가요?\n\n버전: 1.3.5\n\n<div class=\"content-ad\"></div>\n\nsearchsploit을 사용하여 특정 소프트웨어 버전에 대한 취약점을 찾을 수 있어요.\n\nSearchsploit은 기본적으로 exploit-db.com의 명령 줄 검색 도구에요.\n\nProFTPd를 실행 중인 시스템에 대한 exploit이 얼마나 있죠?\n\nsearchsploit ProFTPD `버전 번호`\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-TryHackMeKenobiWriteup_8.png)\n\nExploits running: 4\n\nProFtpd의 mod_copy 모듈(http://www.proftpd.org/docs/contrib/mod_copy.html)에서 exploit을 발견했어요.\n\nmod_copy 모듈은 SITE CPFR 및 SITE CPTO 명령을 구현하며, 이를 사용하여 서버의 한 곳에서 다른 곳으로 파일/디렉터리를 복사할 수 있습니다. 인증되지 않은 클라이언트는 이러한 명령을 이용하여 파일을 파일 시스템의 어디서든 원하는 대상으로 복사할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\nFTP 서비스가 Kenobi 사용자로 실행되고 (공유 파일에서) 그 사용자를 위해 SSH 키가 생성되었다는 것을 알고 있습니다.\n\n이제 SITE CPFR 및 SITE CPTO 명령을 사용하여 Kenobi의 개인 키를 복사할 것입니다.\n\n![image](/assets/img/2024-06-19-TryHackMeKenobiWriteup_9.png)\n\n우리는 /var 디렉토리가 우리가 볼 수 있는 마운트임을 알고 있었습니다(task 2, question 4). 이제 Kenobi의 개인 키를 /var/tmp 디렉토리로 이동했어요.\n\n<div class=\"content-ad\"></div>\n\n'/var/tmp' 디렉토리를 우리의 머신에 연결해 봅시다.\n\n\nsudo mkdir /mnt/kenobiNFS\nsudo mount machine_ip:/var /mnt/kenobiNFS\nls -la /mnt/kenobiNFS\n\n\n<div class=\"content-ad\"></div>\n\n\n![Screenshot 10](/assets/img/2024-06-19-TryHackMeKenobiWriteup_10.png)\n\nWe now have a network mount on our deployed machine! We can go to `/var/tmp` and get the private key then login to Kenobi’s account.\n\n![Screenshot 11](/assets/img/2024-06-19-TryHackMeKenobiWriteup_11.png)\n\n![Screenshot 12](/assets/img/2024-06-19-TryHackMeKenobiWriteup_12.png)\n\n\n<div class=\"content-ad\"></div>\n\n케노비의 사용자 플래그(/home/kenobi/user.txt)는 무엇인가요?\n\n![이미지](/assets/img/2024-06-19-TryHackMeKenobiWriteup_13.png)\n\n사용자 플래그: d0b0f3f53b6caa532a83915e19224899\n\n작업 4 경로 변수 조작을 통한 권한 상승\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-TryHackMeKenobiWriteup_14.png\" />\n\n우선 SUID, SGID 및 Sticky Bits가 무엇인지 이해해 봅시다.\n\n파일 권한\n\n<div class=\"content-ad\"></div>\n\n디렉토리에 대해서\n\nSUID 비트\n\n사용자가 파일 소유자의 권한으로 파일을 실행합니다.\n\n-\n\n\n<div class=\"content-ad\"></div>\n\nSGID 비트\n\n사용자가 그룹 소유자의 권한으로 파일을 실행합니다.\n\n디렉토리에 만들어진 파일은 동일한 그룹 소유자를 가집니다.\n\nSticky Bit\n\n<div class=\"content-ad\"></div>\n\n아래 명령어를 사용하여 시스템에서 이 유형의 파일을 검색할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n다음 명령어를 사용하여 특이한 파일을 찾을 수 있습니다: \n```bash\nfind / -perm -u=s -type f 2>/dev/null\n```\n\n찾은 파일 중에서 특이한 것은 다음과 같습니다:\n\n![특이한 파일](/assets/img/2024-06-19-TryHackMeKenobiWriteup_15.png)\n\n파일: /usr/bin/menu\n\n<div class=\"content-ad\"></div>\n\n바이너리를 실행하면 몇 개의 옵션이 나타납니까?\n\n![이미지](/assets/img/2024-06-19-TryHackMeKenobiWriteup_16.png)\n\n옵션: 3\n\n`strings`는 리눅스에서 바이너리 파일에서 사람이 읽을 수 있는 문자열을 찾는 명령어입니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-TryHackMeKenobiWriteup_17.png\" />\n\n여기에서 볼 수 있듯이, 해당 이진 파일은 전체 경로를 사용하지 않고 (예: /usr/bin/curl 또는 /usr/bin/uname를 사용하지 않음) 실행 중입니다.\n\n<img src=\"/assets/img/2024-06-19-TryHackMeKenobiWriteup_18.png\" />\n\n이 파일은 루트 사용자 권한으로 실행되므로, 경로를 조작하여 루트 셸을 얻을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지1](/assets/img/2024-06-19-TryHackMeKenobiWriteup_19.png)\n\n![이미지2](/assets/img/2024-06-19-TryHackMeKenobiWriteup_20.png)\n\n우리는 /bin/sh 쉘을 복사하여 curl이라고 이름 지었으며 올바른 권한을 부여한 다음 그 위치를 경로에 넣었습니다. 이는 /usr/bin/menu 이진 파일이 실행될 때 경로 변수를 사용하여 \"curl\" 이진 파일을 찾게 한다는 것을 의미합니다. 실제로 이는 /usr/sh의 버전이며 이 파일이 root로 실행되는 동안 우리의 쉘도 root로 실행됩니다!\n\n루트 플래그(/root/root.txt)는 무엇인가요?\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-TryHackMeKenobiWriteup_21.png)\n\nRoot Flag: 177b3cd8562289f37382721c28381f02\n","ogImage":{"url":"/assets/img/2024-06-19-TryHackMeKenobiWriteup_0.png"},"coverImage":"/assets/img/2024-06-19-TryHackMeKenobiWriteup_0.png","tag":["Tech"],"readingTime":6},{"title":"리눅스 프로세스 분석","description":"","date":"2024-06-19 14:58","slug":"2024-06-19-LinuxProcessAnalysis","content":"\n\n리눅스 시스템의 내부 작동을 이해하는 데에는 프로세스, 서비스, 그리고 cron 작업에 대한 이해가 필수적입니다. 각 구성 요소는 시스템 전반적인 기능에서 고유한 역할을 갖고 있으며, 정해진 작업을 실행하거나 루틴한 작업을 자동화하고 사용자 상호 작용을 가능하게 함으로써 서비스를 제공합니다. 그러나 이러한 중요한 역할로 인해 시스템에 침투한 악의적인 사용자들에 의해 악용될 수 있습니다. 공격자들은 취약점이나 잘못된 구성으로 특권을 상승시키고 측면으로 이동하거나 영구적인 손목들을 수립하거나 백도어를 설치하여 시스템의 무결성과 보안을 더욱 훼손할 수 있습니다. 이에 따라, 이러한 구성 요소에 대한 위협을 효과적으로 탐지하고 완화하기 위해 검증 분석 기술의 견고한 아카데미를 구축하는 것이 점점 더 중요해지고 있습니다.\n\n리눅스에서 프로세스는 프로그램의 실행 중인 인스턴스를 말합니다. 리눅스에서 프로그램이나 명령을 실행할 때, 운영 체제는 해당 프로그램을 실행하기 위한 프로세스를 생성합니다. 각 프로세스는 고유한 식별자인 프로세스 ID (PID)를 가지고 있으며, 운영 체제가 이를 관리하고 추적하는 데 도움을 줍니다.\n\n프로세스는 부모-자식 관계를 가질 수 있어서 계층적인 구조를 형성합니다. 한 프로세스가 다른 프로세스를 생성할 때 (예: 한 쉘 세션에서 하위 쉘에서 추가 프로세스를 생성하는 경우), 새로운 프로세스는 그것을 만든 프로세스의 자식이 되며, 부모로부터 생성된다고 합니다. 이 관계는 운영 체제에서 프로세스와 자원 할당을 관리하는 데 중요합니다.\n\n시스템에서 실행 중인 프로세스를 검사하는 데에 다양한 도구와 유틸리티를 사용할 수 있습니다. 이 열거를 통해 악성 활동이나 의심스러운 부모-자식 프로세스 관계를 식별할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# ps\n\nps 명령어는 UNIX류 운영 체제에서 활성 프로세스에 대한 스냅샷을 보고하는 다재다능한 유틸리티입니다. 기본적으로 현재 (활성 실행 중인) 콘솔 세션과 관련된 프로세스에 대한 정보를 표시하지만 시스템 전반 및 다른 사용자의 프로세스 정보를 수집하는 데도 사용할 수 있습니다. 이 유틸리티는 /proc 가상 파일 시스템의 파일을 읽어 해당 정보를 검색합니다. 이 가상 파일 시스템은 실행 중인 시스템의 각 프로세스와 대응하는 계층적 디렉터리 구조입니다. 명령어를 실행하려면 간단히 ps를 입력하십시오:\n\n![ps command output](/assets/img/2024-06-19-LinuxProcessAnalysis_0.png)\n\n위에서 볼 수 있듯이 해당 명령어는 다음 열을 포함하는 테이블 형식으로 출력을 제공합니다:\n\n<div class=\"content-ad\"></div>\n\nPID: 각 프로세스에 대한 고유 식별자 (프로세스 ID)입니다.\nTTY: 프로세스와 연결된 터미널입니다.\nTIME: 프로세스가 소비한 누적 CPU 시간입니다.\nCMD: 프로세스와 관련된 명령어입니다.\n\n또한 사용자(Janice)에 대한 프로세스를 보려면 -u 또는 --user 옵션을 사용한 후 사용자의 사용자 이름을 입력하면 됩니다.\n\nps 명령어와 함께 제공할 수 있는 유용한 옵션 세트는 -eFH입니다. 이 옵션 세트는 시스템에서 실행 중인 모든 프로세스에 대한 전체 개요를 계층적 형식으로 반환하므로, 그들이 연결되어 있는 터미널이나 세션과 관계없이 사용 가능합니다. 이 옵션 세트는 시스템 모니터링 및 포렌식 분석에 유용한 도구로 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-LinuxProcessAnalysis_1.png)\n\n위 명령어에서 볼 수 있듯이, -e를 사용하여 모든 프로세스를 선택하고, 결과를 -F(extra full) 형식으로 반환하며, 프로세스 계층도(포레스트) -H를 보여줍니다.\n\n# lsof\n\n실행 중인 시스템 내에서 모든 열려있는 파일과 해당 프로세스 목록을 보고하는 더 강력한 방법을 살펴봅시다. lsof (List Open Files)는 시스템에서 프로세스에 의해 열린 파일에 대한 정보를 나열하는 유틸리티입니다. 이 경우, 앞서 식별한 Netcat 프로세스가 열어둔 파일을 보려면 lsof를 실행하고 PID를 제공하면 됩니다:\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-LinuxProcessAnalysis_2.png\" />\n\n# Advanced Usage\n\n- Network Files with Specific Protocol:\n\n- Explains how to filter network connections by protocol (TCP/UDP).\n\n\n<div class=\"content-ad\"></div>\n\n- 특정 포트로 네트워크 파일:\n\n- 해당 포트 번호로 필터링하여 해당 포트를 사용 중인 프로세스를 확인하는 방법을 보여줍니다.\n\n2. 특정 명령어로 열린 파일:\n\n- 특정 명령어로 열린 파일을 나열하는 방법을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n앞으로 pstree를 사용하여 부모-자식 관계 프로세스의 과정을 더 자세히 분석해보는 것이 좋을 것 같아요. 그렇게 하면 프로세스의 발원지를 파악하고 잠재적인 공격 경로에 대한 통찰을 얻을 수 있을 거에요.\n\n# pstree\n\npstree는 프로세스를 트리 형식으로 시각적으로 표시하는 명령줄 유틸리티로, 프로세스 간의 부모-자식 관계를 보여줍니다. 이 도구를 사용하면 의심스러운 프로세스의 발원지를 식별하고 시스템 내의 다른 프로세스와의 관계를 이해하는 데 도움이 될 수 있어요.\n\n![LinuxProcessAnalysis_3](/assets/img/2024-06-19-LinuxProcessAnalysis_3.png)\n\n<div class=\"content-ad\"></div>\n\n우리가 pstree를 사용하여 식별한 부모 프로세스의 심층적인 프로세스 분석을 수행할 수 있으며, 이를 위해 해당 부모 프로세스의 부모 프로세스(-s)와 해당 프로세스의 PID(-p)를 나열하는 옵션을 제공할 수 있습니다:\n\n![Linux Process Analysis](/assets/img/2024-06-19-LinuxProcessAnalysis_4.png)\n\n-p 옵션과 함께 pstree 명령어를 사용합니다.\n\n# cron 작업\n\n<div class=\"content-ad\"></div>\n\nCronjobs은 미리 정의된 간격으로 자동으로 실행되는 예약된 작업입니다. Cron 데몬은 이러한 작업을 관리하는 백그라운드 프로세스로, crontab이라는 설정 파일을 기반으로 합니다. 사용자는 /var/spool/cron/crontabs 디렉토리에 crontab 파일을 저장할 수 있습니다. 시스템 전역 cronjob을 관리하는 주요 crontab 파일은 /etc/crontab에 있습니다.\n\nCron 항목은 공백으로 구분된 필드로 구성된 특정 형식을 따릅니다. 먼저 Bob이라는 사용자를 위한 예제 crontab 파일을 살펴보겠습니다. 일반적으로 이 파일은 /var/spool/cron/crontabs/bob에 있을 것입니다.\n\n예시 Cron 항목\n\n```js\n10 05 * * * /home/bob/backup_tmp.sh\n```\n\n<div class=\"content-ad\"></div>\n\n이 cron 스케줄은 실행할 명령어가 뒤를 따라오는 다섯 가지 필드로 구성되어 있습니다:\n\n- 분(10): 첫 번째 필드는 명령어가 실행될 분을 지정합니다. 이 경우 10으로, 시간의 10분에 명령어가 실행됨을 나타냅니다.\n- 시(05): 두 번째 필드는 명령어가 실행될 시간을 지정합니다. 05로, 새벽 5시 10분에 명령어가 실행됩니다.\n- 요일(*): 세 번째 필드는 명령어가 실행될 달의 일을 지정합니다. 여기서는 ****로, 와일드카드 값으로 매 달 모든 날에 실행됩니다. 날짜를 구체적으로 지정하려면 1에서 31까지의 숫자를 사용할 수 있습니다. 예를 들어 매월 15일에 명령을 실행하려면 15를 사용합니다.\n- 월(*): 네 번째 필드는 명령어가 실행될 달을 지정합니다. 또한 ****로, 매달 명령어가 실행됨을 나타냅니다. 숫자(1은 1월, 2는 2월 등)나 약어(Jan은 1월, Feb은 2월 등)로 월을 지정할 수 있습니다. 예를 들어 2나 Feb를 사용하여 명령어를 2월에만 실행할 수 있습니다.\n- 요일(*): 다섯 번째 필드는 명령어가 실행될 요일을 지정합니다. 여기서는 ****로, 매주 모든 요일에 실행됩니다. 0부터 7까지 숫자로 요일을 지정할 수 있으며, 0과 7은 일요일, 1은 월요일 등으로 대응됩니다. 또는 약어(Sun, Mon, Tue 등)를 사용할 수도 있습니다. 예를 들어 1이나 Mon을 사용할 수 있습니다.\n- 명령어(/home/bob/backup_tmp.sh): 마지막 필드에는 실행할 명령어가 포함됩니다.\n\n또한 시스템 레벨의 /etc/crontab은 사용자 레벨의 crontab과 다릅니다. 시스템 전역 crontab에는 명령이 실행될 사용자(root 또는 www-data 등)를 지정하는 추가 필드가 포함됩니다.\n\n모두 합쳐서 이 cron 스케줄은 매일 새벽 5시 10분에 명령어 /home/bob/backup_tmp.sh가 실행되도록 구성되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다음으로, 철저한 분석에 중요한 추가 시스템 cron 작업 디렉토리가 있습니다. 이러한 디렉토리는 /etc/ 디렉토리 아래에서 다음과 같은 네이밍 컨벤션으로 찾을 수 있습니다:\n\n- /etc/cron.hourly/ — 한 시간마다 실행되는 시스템 cron 작업.\n- /etc/cron.daily/ — 매일 한 번 실행되는 시스템 cron 작업.\n- /etc/cron.weekly/ — 매주 한 번 실행되는 시스템 cron 작업.\n- /etc/cron.monthly/ — 매월 한 번 실행되는 시스템 cron 작업.\n- /etc/cron.d/ — 추가적인 사용자 정의 시스템 cron 작업.\n\n시스템 수준의 cron 작업을 조사한 후에 시스템 사용자가 시스템에 구성한 다양한 사용자 수준의 cron 작업을 살펴볼 수 있습니다. 사용자 수준의 cron 작업은 개별 사용자 구성 파일에 특화되어 있으며 시스템 전역 cron 작업과는 별도로 관리됩니다. /var/spool/cron/crontabs/ 디렉토리로 이동할 수 있습니다. cron을 사용할 수 있는 권한을 갖춘 각 사용자는 이 디렉토리 안에 자신의 사용자 이름으로 된 파일을 갖게 됩니다.\n\n이 정보를 나열하는 여러 가지 방법이 있습니다. 가장 쉬운 방법은 관리자 권한으로 디렉토리 내용을 나열하는 것입니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-LinuxProcessAnalysis_5.png\" />\n\n큰 아이디어로 한 줄짜리 명령어를 사용하여 시스템의 사용자를 빠르게 순환하고 사용자 수준의 크론 작업이 구성되어 있는지 식별할 수 있습니다. 그렇다면 크론 작업 항목의 내용을 터미널에 출력할 수 있습니다. 이 유형의 혼합 자동화는 조사 속도를 높이고 철저한 커버리지를 보장할 수 있습니다.\n\n이 명령을 분해해 보면, /etc/passwd의 항목을 필터링하여 시스템의 모든 사용자를 반복하는 중입니다. 이 반환된 각 사용자에서, 각 사용자를 반복하고 앞서 실행한 crontab 명령을 사용하여 그들의 crontab 항목을 가져옵니다. 해당 사용자에 대한 crontab 항목이 있으면, 그들의 사용자 이름과 crontab 파일에서 해당 항목을 반환합니다.\n\n위 출력에서 여러 사용자의 crontab 항목 값들을 빠르게 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# Cron Execution Logs\n\nCron 로그는 cron 데몬에 의해 관리되는 예약된 작업의 실행을 기록하며, cron 작업이 실행된 시간과 연관된 출력 또는 오류 메시지의 연대기적 기록을 제공합니다. 수사적 관점에서 이러한 로그는 cron 작업의 실행 흔적을 발견하고 의심스러운 활동이나 시스템 침해를 발견하는 데 귀감이 될 수 있습니다. 예를 들어, 악의적인 사용자가 기존 cron 작업을 남용하거나 악의적인 cron 작업을 생성하고 자신의 흔적을 감추려고 시도한 경우, 로그는 실행의 이상한 패턴이나 예상치 못한 명령어 실행을 드러낼 수 있습니다.\n\nDebian 기반 시스템에서는 cron 실행 로그가 일반적으로 /var/log/syslog에 저장됩니다. 이 파일은 cron 데몬의 메시지를 포함하여 시스템 로그를 집계합니다. Red Hat Enterprise Linux (RHEL) 및 CentOS와 같은 일부 Linux 배포판에서는 이러한 로그를 적절히 /var/log/cron에서 찾을 수 있습니다.\n\n조사 중인 시스템이 syslog 파일에 cron 로그를 저장하기 때문에 우리는 내용을 grep하여 cron과 관련된 로그를 필터링할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n위의 명령은 많은 양의 출력물을 생성할 수 있습니다. 특정 기준에 따라 결과를 더 필터링하여 관련 정보에 집중하는 것이 좋습니다. 필터링 아이디어로는 다음과 같은 예시가 있습니다:\n\n# Pspy\n\nPspy는 루트 권한이 필요하지 않고 리눅스 프로세스를 모니터링하는 강력한 오픈 소스 도구입니다. 실행 중인 프로세스에 대한 실시간 정보를 캡처하고 표시하는 데 사용되며, 실행 명령, 사용자 ID, 프로세스 ID (PID), 부모 프로세스 ID (PPID), 타임스탬프 및 기타 관련 세부 정보를 제공합니다. /proc 가상 파일 시스템에서 데이터를 직접 읽어와 시스템 파일을 수정하거나 상위 권한을 요구하지 않고 프로세스 활동에 대한 실시간 통찰력을 제공합니다.\n\n열거 목적에 용이하지만 사건 대응자들은 실행 아티팩트를 수집하고 단기간 실행되는 프로세스를 포착하는 능력을 활용할 수 있습니다. 실시간 모니터링 덕분에 시스템을 통해 생성된 다양한 cron 작업에 의해 생성된 프로세스를 감지할 수 있어 cron 작업이 실행될 때 어떤 프로세스가 발생하는지에 대한 더 많은 통찰력을 제공할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 시스템에서는 Pspy가 이미 사전 설치되어 있으며 사용 중인 바이너리와 함께 마운트되어 있습니다. 따라서 경로에 있으며 pspy64를 실행하여 간단히 호출할 수 있습니다. 실시간 모니터링을 시작하고 몇 분 동안 실행하여 이벤트를 캡처해야 합니다. Ctrl + C를 눌러 중지할 수 있습니다:\n\n![Pspy Image](/assets/img/2024-06-19-LinuxProcessAnalysis_6.png)\n\n# 추가적인 팁:\n\n- 인내심을 갖고 철저히 작업하세요. 어떤 프로세스는 가끔만 나타나거나 특정 시스템 이벤트 중에만 나타날 수 있습니다.\n- 특정 프로세스나 사용자를 필터링하기 위해 pspy와 함께 grep 명령을 사용하세요.\n\n<div class=\"content-ad\"></div>\n\n# 요약\n\n리눅스 프로세스, 서비스 및 cron 작업을 이해하는 것은 시스템 무결성과 보안에 중요합니다. 각각 고유한 프로세스 ID (PID)를 가진 프로세스는 부모-자식 관계를 가질 수 있습니다. ps 및 lsof와 같은 도구는 실행 중인 프로세스와 열려 있는 파일을 검사할 수 있습니다. pstree 유틸리티는 프로세스를 트리로 표시하여 부모-자식 관계를 보여줍니다. 크론 작업은 크론 데몬에 의해 실행되는 예약된 작업이며, 크론 로그는 이러한 작업의 실행을 기록합니다. pspy 도구는 리얼타임으로 리눅스 프로세스를 모니터링하여 실행 아티팩트를 캡처하고 크론 작업에 의해 생성된 프로세스를 감지합니다.","ogImage":{"url":"/assets/img/2024-06-19-LinuxProcessAnalysis_0.png"},"coverImage":"/assets/img/2024-06-19-LinuxProcessAnalysis_0.png","tag":["Tech"],"readingTime":8},{"title":"리눅스 보안 여정 - 안전한 실행 모드","description":"","date":"2024-06-19 14:57","slug":"2024-06-19-TheLinuxSecurityJourneySecureExecutionMode","content":"\n\n일반적으로 \"Secure Execution Mode\"에서는 보조 벡터의 \"AT_SECURE\" 항목에 0이 아닌 값이 포함되어 있는 경우 이진 파일이 실행됩니다. LSM(https://medium.com/@boutnaru/linux-security-lsm-linux-security-modules-907bbcf8c8b4)이 값을 설정한 경우, 작업/프로세스의 \"실제 UID\"(https://medium.com/@boutnaru/the-linux-security-journey-ruid-real-user-id-b23abcbca9c6) 및 \"유효 UID\"(https://medium.com/@boutnaru/the-linux-security-journey-euid-effective-user-id-65f351532b79)가 다른 경우(그룹 값도 동일)에도 이 값을 0이 아닌 값으로 만들 수 있습니다. 또한, 일반 사용자가 실행한 이진 파일이 프로세스에 권한을 부여했을 경우에도 이 값이 0이 아닌 값이 됩니다(https://man7.org/linux/man-pages/man8/ld.so.8.html).\n\n전반적으로, 안전한 실행 모드는 동적 링커/로더의 기능입니다. 이 모드가 활성화되어 있는 경우 특정 환경 변수가 이진 파일을 실행할 때 무시됩니다. 이러한 변수의 예로는 \"LD_LIBRARY_PATH\", \"LD_DEBUG\"(단, /etc/suid-debug가 있는 경우는 제외), \"LD_DEBUG_OUTPUT\", \"LD_DEBUG_WEAK\"(glibc 2.3.4부터), \"LD_ORIGIN_PATH\", \"LD_PROFILE\"(glibc 2.2.5부터), \"LD_SHOW_AUXV\"(glibc 2.3.4부터) 및 \"LD_AUDIT\"이 있습니다(https://manpages.ubuntu.com/manpages/focal/en/man8/ld.so.8.html) — 아래 스크린샷에서 확인할 수 있습니다.\n\n마지막으로, 안전한 실행 모드의 목표는 \"setuid\"/\"setgid\"로 실행할 수 있는 이진 파일이 임의의 코드를 로드/실행하는 능력을 차단하여 특권 상승을 수행하는 것을 방지하는 것입니다(사용자 한 명이 실행하지만 다른 사용자(루트일 수도 있음) 권한으로 실행될 때).\n\n다음 글에서 만나요 ;-) 트위터에서 제 소식을 확인하려면 @boutnaru(https://twitter.com/boutnaru)를 팔로우해주세요. 또한, 미디엄(https://medium.com/@boutnaru)에서 다른 글도 읽어볼 수 있습니다. 무료 eBook은 https://TheLearningJourneyEbooks.com에서 찾아볼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![2024-06-19-TheLinuxSecurityJourneySecureExecutionMode](/assets/img/2024-06-19-TheLinuxSecurityJourneySecureExecutionMode_0.png) \n","ogImage":{"url":"/assets/img/2024-06-19-TheLinuxSecurityJourneySecureExecutionMode_0.png"},"coverImage":"/assets/img/2024-06-19-TheLinuxSecurityJourneySecureExecutionMode_0.png","tag":["Tech"],"readingTime":2},{"title":"리눅스 디렉터리 구조","description":"","date":"2024-06-19 14:55","slug":"2024-06-19-LinuxDirectoryStructure","content":"\n\n# Linux 디렉토리 구조에 대한 자세한 설명\n\n# 부팅 프로세스\n\n부팅 로더 (/boot)\n\n- Linux 시스템이 시작되면 BIOS 또는 UEFI 펌웨어가 하드웨어를 초기화하고 부팅로더(예: GRUB)를 /boot 디렉토리에서 불러옵니다.\n- 부팅로더는 커널(/boot/vmlinuz)과 초기 RAM 디스크(/boot/initrd.img)를 메모리에 불러옵니다.\n\n<div class=\"content-ad\"></div>\n\n커널 초기화 (/proc, /sys)\n\n- 커널은 시스템 구성 요소를 초기화하고 루트 파일 시스템을 마운트합니다.\n- 초기화 과정에서 커널은 프로세스와 시스템 하드웨어에 관한 정보를 /proc 및 /sys에 채웁니다.\n\n# 시스템 초기화\n\n시스템 및 서비스 초기화 (/etc, /lib, /sbin, /run)\n\n<div class=\"content-ad\"></div>\n\n- 커널이 초기화된 후에는 부팅 프로세스를 담당하는 init 프로세스(또는 현대의 Linux 시스템에서는 systemd)가 시작됩니다.\n- systemd는 /etc/systemd 및 /etc의 다른 디렉토리에서 구성 파일을 읽어 필요한 시스템 서비스를 시작합니다.\n- 부팅 및 시스템 서비스 실행에 필요한 필수 라이브러리는 /lib에 위치합니다.\n- 시스템 관리 이진 파일은 /sbin 및 /usr/sbin에 있어 서비스를 관리하는 데 도움이 됩니다.\n- /run은 시스템 작동 중에 일시적인 시스템 정보를 저장하는 데 사용됩니다.\n\n# 사용자 환경 설정\n\n사용자 환경 (/home, /usr, /opt)\n\n- 시스템 서비스가 시작되면 사용자가 로그인하여 개인 환경이 설정됩니다.\n- 각 사용자는 /home 아래에 홈 디렉토리를 갖고 있어 개인 파일과 설정이 저장됩니다.\n- 사용자 응용 프로그램 및 도구는 일반적으로 /usr/bin 및 /usr/local/bin에 저장됩니다.\n- 선택적 및 제3자 응용 프로그램은 /opt에 설치됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 시스템 작동\n\n장치 관리 (/dev)\n\n- /dev는 하드 드라이브, 터미널, 프린터 등의 하드웨어 구성 요소를 나타내는 장치 파일을 포함합니다.\n- 장치 파일을 통해 소프트웨어가 표준 입력/출력 작업을 사용하여 하드웨어와 상호 작용할 수 있습니다.\n\n구성 및 관리 (/etc)\n\n<div class=\"content-ad\"></div>\n\n- /etc에 있는 설정 파일들은 시스템 동작, 네트워크 설정, 사용자 계정 정보, 서비스 구성을 정의합니다.\n- 관리자들은 이러한 파일을 편집하여 시스템 설정을 관리합니다.\n\n변수 데이터 (/var)\n\n- 시스템과 애플리케이션이 생성한 로그, 캐시, 스풀 파일 등과 같은 동적 데이터는 /var에 저장됩니다.\n- 로그 파일 (/var/log)은 시스템 및 애플리케이션 활동을 기록하여 모니터링 및 문제 해결을 위해 사용됩니다.\n- 메일 스풀 (/var/spool/mail)은 수신된 이메일을 저장합니다.\n\n임시 파일 (/tmp, /var/tmp)\n\n<div class=\"content-ad\"></div>\n\n- 애플리케이션이 /tmp 및 /var/tmp에 임시 파일을 저장합니다.\n- /tmp에있는 파일은 일반적으로 재부팅 시 삭제되지만, /var/tmp에있는 파일은 다시 부팅해도 유지됩니다.\n\n# 예: 사용자 애플리케이션 실행\n\n애플리케이션 실행\n\n- 사용자는 /usr/bin이나 /usr/local/bin에서 애플리케이션을 실행합니다.\n- 해당 애플리케이션은 /lib이나 /usr/lib에서 공유 라이브러리를 로드할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n하드웨어와 상호 작용하기\n\n- 애플리케이션이 하드웨어와 상호 작용해야 할 때, /dev 디렉토리에 해당 장치 파일에 접근합니다.\n\n사용자 데이터 저장 및 접근\n\n- 사용자별 데이터 및 구성은 사용자의 홈 디렉토리 (/home/사용자이름)의 파일에서 읽거나 쓰여집니다.\n\n<div class=\"content-ad\"></div>\n\n로그 및 임시 데이터\n\n- 애플리케이션은 활동을 /var/log에 있는 파일에 기록할 수 있습니다.\n- 애플리케이션이 생성한 임시 데이터는 /tmp 또는 /var/tmp에 저장됩니다.\n\n# 시스템 유지 보수\n\n소프트웨어 업데이트\n\n<div class=\"content-ad\"></div>\n\n- 시스템 및 응용 프로그램 업데이트는 /bin, /sbin, /lib, /usr/bin, /usr/sbin 또는 /opt에 새 파일을 설치할 수 있습니다.\n\n구성 변경\n\n시스템 관리자는 시스템 또는 서비스 동작을 변경하기 위해 /etc에 있는 구성 파일을 편집할 수 있습니다.\n\n모니터링 및 문제 해결\n\n<div class=\"content-ad\"></div>\n\n`/var/log`에 있는 로그는 시스템 상태를 모니터링하고 문제를 해결하기 위해 검토됩니다.\n\n아키텍처\n\n![Image](/assets/img/2024-06-19-LinuxDirectoryStructure_0.png)\n\n디렉토리 구조 개요\n\n<div class=\"content-ad\"></div>\n\n\n/\n├── bin\n├── boot\n├── dev\n├── etc\n├── home\n│ └── user\n├── lib\n├── media\n├── mnt\n├── opt\n├── proc\n├── root\n├── run\n├── sbin\n├── srv\n├── sys\n├── tmp\n├── usr\n│ ├── bin\n│ ├── lib\n│ ├── local\n│ └── sbin\n└── var\n ├── log\n ├── spool\n └── tmp\n\n\n\n|- / (Root)\n\n설명: 모든 다른 디렉토리가 뻗어나온 최상위 디렉토리입니다.\n예시: /bin, /etc, /home은 모두 /의 하위 디렉토리입니다.\n\n\n\n|- /bin\n\n설명: 모든 사용자가 필요로 하는 기본 명령어와 도구를 위한 필수 바이너리 실행 파일이 들어 있습니다.\n예시:\n/bin/ls (디렉토리 내용 나열)\n/bin/bash (Bourne Again Shell)\n/bin/cp (파일과 디렉토리 복사)\n\n\n\n|- /boot\n\n설명: 부트로더 파일과 커널 이미지가 들어 있습니다.\n예시:\n/boot/vmlinuz (리눅스 커널)\n/boot/initrd.img (초기 RAM 디스크 이미지)\n/boot/grub/grub.cfg (GRUB 부트로더 구성 파일)\n\n\n<div class=\"content-ad\"></div>\n\n```js\n| - /dev \n\n설명: 하드웨어 장치를 나타내는 장치 파일이 포함되어 있습니다.\n예시:\n/dev/sda1 (첫 번째 하드 드라이브의 첫 번째 파티션)\n/dev/tty1 (터미널 장치)\n/dev/null (데이터를 버리는 널 장치)\n\n```\n\n```js\n| - /etc \n\n설명: 시스템 전체의 구성 파일 및 초기화를 위한 쉘 스크립트가 포함되어 있습니다.\n예시:\n/etc/passwd (사용자 계정 정보)\n/etc/fstab (파일 시스템 마운트 포인트)\n/etc/hosts (호스트 이름의 정적 테이블 조회)\n\n```\n\n```js\n| - /home \n\n설명: 시스템에 존재하는 각 사용자의 개인 디렉토리가 포함되어 있습니다. (사용자별 데이터)\n예시:\n/home/alice (앨리스의 홈 디렉토리)\n/home/bob (밥의 홈 디렉토리)\n\n```\n\n```js\n| - /lib \n\n설명: 시스템 이진 파일이 필요로 하는 공유 라이브러리가 포함되어 있습니다.\n예시:\n/lib/libc.so.6 (C 표준 라이브러리)\n/lib/modules (커널 모듈)\n\n```  \n\n<div class=\"content-ad\"></div>\n\n\n|- /media\n\nDescription: USB 드라이브, CD 및 DVD와 같은 탈착식 미디어를 연결하는 데 사용됩니다.\n예시:\n/media/cdrom (CD-ROM을 연결하는 마운트 포인트)\n/media/usb (USB 드라이브를 연결하는 마운트 포인트)\n\n\n\n|- /mnt\n\nDescription: 임시로 파일 시스템을 마운트하는 일반적인 마운트 포인트입니다.\n예시:\n관리자는 다음과 같이 파일 시스템을 임시로 마운트할 수 있습니다:\nsudo mount /dev/sdb1 /mnt\n\n\n\n|- /opt\n\nDescription: 선택적 소프트웨어 패키지 및 타사 응용 프로그램이 포함됩니다.\n예시:\n/opt/google (Google 애플리케이션의 설치 디렉토리)\n/opt/vmware (VMware 애플리케이션의 설치 디렉토리)\n\n\n\n|- /proc\n\nDescription: 프로세스 및 시스템에 대한 정보를 제공하는 가상 파일 시스템입니다.\n예시:\n/proc/cpuinfo (CPU 정보)\n/proc/meminfo (메모리 사용량 정보)\n/proc/1234 (PID가 1234인 프로세스에 대한 정보를 포함하는 디렉토리)\n\n\n<div class=\"content-ad\"></div>\n\n```js\n|- /root\n\n설명: 시스템 관리자를 위한 홈 디렉토리입니다.\n예시:\n/root/.bashrc (루트 사용자용 Bash 구성 파일)\n/root/scripts (관리 작업을 위한 사용자 정의 스크립트)\n```\n\n```js\n|- /run\n\n설명: 시스템 프로세스 및 서비스의 런타임 데이터를 포함합니다.\n예시:\n/run/lock (락 파일)\n/run/user/1000 (UID 1000을 가진 사용자를 위한 런타임 데이터)\n```\n\n```js\n|- /sbin\n\n설명: 시스템 관리에 사용되는 필수 시스템 이진 파일이 포함되어 있습니다.\n예시:\n/sbin/reboot (시스템 재부팅)\n/sbin/ifconfig (네트워크 인터페이스 구성)\n/sbin/fdisk (디스크 파티셔닝 도구)\n```\n\n```js\n|- /srv\n\n설명: 시스템에서 제공하는 서비스에 대한 데이터가 포함되어 있습니다.\n예시:\n/srv/ftp (FTP 서버 데이터)\n/srv/www (웹 서버 데이터)\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n|- /sys\n\n설명: 커널, 장치 및 시스템 하드웨어에 대한 정보를 제공하는 가상 파일 시스템입니다.\n예시:\n/sys/class/net (네트워크 인터페이스)\n/sys/devices (시스템 장치)\n```\n\n```js\n|- /tmp\n\n설명: 애플리케이션에서 사용하는 임시 파일을 저장하는 디렉토리입니다.\n예시:\n애플리케이션 실행 시 생성되는 임시 파일로, 종종 재부팅 시 정리됩니다.\n/tmp/install.log (소프트웨어 설치 중 생성된 임시 로그 파일)\n```\n\n```js\n|- /usr\n\n설명: 사용자 관련 프로그램 및 데이터를 포함합니다. 중요한 하위 디렉토리가 여러 개 있습니다:\n/usr/bin: 사용자 실행 파일.\n예시:\n/usr/bin/python (Python 인터프리터)\n/usr/bin/gcc (GNU C 컴파일러)\n```\n\n```js\n|- /usr/sbin: 시스템 관리 이진 파일.\n예시:\n/usr/sbin/apache2 (아파치 웹 서버)\n\n|- /usr/lib: 사용자 응용프로그램을 위한 라이브러리.\n예시:\n/usr/lib/libssl.so (OpenSSL 라이브러리)\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n|- /usr/local\n\nDescription: 로컬에 설치된 소프트웨어 및 사용자 지정 스크립트입니다.\n예시:\n/usr/local/bin/myscript.sh (사용자 지정 스크립트)\n/usr/local/lib/mylib.so (사용자 지정 공유 라이브러리)\n```\n\n```js\n|- /var\n\nDescription: 로그, 데이터베이스 및 스풀 파일과 같은 변수 데이터 파일이 포함되어 있습니다.\n예시:\n/var/log: 로그 파일.\n예시:\n/var/log/syslog (시스템 로그)\n/var/log/auth.log (인증 로그)\n```\n\n```js\n|- /var/spool \nDescription: 메일 및 인쇄와 같은 작업을 위한 스풀 디렉토리입니다.\n예시:\n/var/spool/mail (사용자 메일함)\n```\n\n```js\n|- /var/tmp: \n\nDescription: 재부팅 간에 유지되는 임시 파일입니다.\n예시:\n세션 당보다 오래 지속되어야 하는 응용 프로그램이 생성하는 임시 파일.\n```\n\n<div class=\"content-ad\"></div>\n\n# 요약\n\n- Root (/): 최상위 디렉토리.\n- 시스템 이진 파일 (/bin, /sbin): 중요한 명령 줄 유틸리티 및 시스템 이진 파일.\n- 부팅 파일 (/boot): 부트로더 및 커널 파일.\n- 장치 파일 (/dev): 하드웨어 장치와의 인터페이스.\n- 설정 파일 (/etc): 시스템 및 애플리케이션 설정.\n- 홈 디렉토리 (/home): 사용자 데이터 및 설정.\n- 라이브러리 (/lib, /usr/lib): 공유 라이브러리.\n- 임시 파일 (/tmp, /var/tmp): 애플리케이션의 임시 저장소.\n- 사용자 프로그램 (/usr): 사용자 애플리케이션과 도구.\n- 로그 및 변수 데이터 (/var): 로그, 수동 파일 및 기타 변수 데이터.\n- 선택적 소프트웨어 (/opt): 제3자 애플리케이션 및 패키지.","ogImage":{"url":"/assets/img/2024-06-19-LinuxDirectoryStructure_0.png"},"coverImage":"/assets/img/2024-06-19-LinuxDirectoryStructure_0.png","tag":["Tech"],"readingTime":7},{"title":"컨테이너가 스왑 공간을 사용할 수 있을까요","description":"","date":"2024-06-19 14:53","slug":"2024-06-19-CanContainersUseSwapSpace","content":"\n\n\n![image](/assets/img/2024-06-19-CanContainersUseSwapSpace_0.png)\n\nLinux을 잘 아시는 분이라면 스왑 공간에 익숙할 것입니다. 간단히 말하면, 스왑은 가상 메모리로 사용되는 디스크의 일부입니다. 물리적 메모리 (RAM)가 가득 찼을 때 RAM에서 드물게 사용되는 데이터를 일시적으로 스왑 공간으로 이동할 수 있습니다. 이렇게 함으로써 새로운 메모리 요구에 맞춰 RAM을 확보합니다.\n\n스왑을 사용하는 장점은 메모리 사용량의 갑작스러운 변화를 다룰 수 있어 Out-Of-Memory(OOM) Killer가 충분한 메모리 없이 프로세스를 종료하는 것을 방지할 수 있다는 것입니다.\n\n특히 Memory Cgroups(제어 그룹)로 관리되는 컨테이너의 경우, 다음과 같은 의문이 생깁니다. 그들은 여전히 스왑 공간을 사용할 수 있을까? 그렇다면, 어떠한 잠재적인 문제가 있을까요?\n\n\n<div class=\"content-ad\"></div>\n\n# 문제 현상\n\n먼저, 사용 가능한 스왑 공간이 있는 노드에 컨테이너를 시작하고 메모리 Cgroup 제한을 설정해봅시다. 이후 어떤 일이 일어나는지 관찰해보겠습니다.\n\n만약 당신의 노드에 스왑 파티션이 없다면, 다음 명령어를 사용하여 하나를 생성할 수 있습니다.\n\n이 예시에서 스왑 공간 크기는 20G로 설정되어 있지만, 사용 가능한 디스크 공간에 따라 조정할 수 있습니다. 이러한 명령어를 실행한 후 free 명령어를 실행하면 스왑 공간이 지금 20G임을 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n아래 스크린샷을 참고하면 출력 예시를 확인할 수 있어요.\n\n![image](/assets/img/2024-06-19-CanContainersUseSwapSpace_1.png)\n\n자, 이제 OOM(메모리 부족) 문서의 예제와 비슷하게 512MB로 설정된 메모리 Cgroup 제한이 있는 컨테이너를 시작해보겠습니다. 컨테이너 내부의 mem_alloc 프로그램은 2GB의 메모리를 할당하려고 할 거에요.\n\nOOM 문서에서 설명한 상황과 달리, 이번에는 OOM 이벤트로 인해 컨테이너가 종료되지 않아요. 대신, 컨테이너는 계속해서 원활하게 실행됩니다.\n\n<div class=\"content-ad\"></div>\n\n위의 출력에서 우리는 mem_alloc 프로세스의 RSS (Resident Set Size) 메모리가 약 512MB(RES: 515596)로 유지되는 것을 관찰할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-CanContainersUseSwapSpace_2.png)\n\n이제 스왑 공간을 살펴봅시다. 1.5GB (사용 중 1542144KB)의 스왑 공간이 사용되고 있음을 관찰할 수 있습니다. 아래 그림에서 출력이 나와 있습니다. 간단한 계산으로 1.5GB + 512MB가 mem_alloc 프로그램이 요청한 2GB의 메모리와 동일함을 알 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-CanContainersUseSwapSpace_3.png)\n\n<div class=\"content-ad\"></div>\n\n우리가 방금 토론한 예시를 기반으로 하면, 스왑 공간이 있는 경우 OOM-킬 당할 수 있던 컨테이너가 원활하게 실행될 것이라고 생각할 수 있습니다. 처음에는 유익해 보일 수 있지만, 좀 더 심층적으로 생각해보면, 이는 메모리 Cgroup 제한의 목적을 약화시킬 수 있다는 점을 생각해보셨나요?\n\n더 분석해봅시다. 컨테이너 내 프로그램이 메모리 누수를 경험하는 경우, 메모리 Cgroup은 일반적으로 다른 애플리케이션들에 영향을 미치지 않도록 적시에 프로세스를 종료합니다. 그런데 이제 스왑 공간이 활성화되어 있으면, 누수되는 프로세스가 종료되지 않습니다. 대신에 계속해서 스왑 디스크에 읽고 쓰기를 계속하며 전체 노드의 성능을 저하시킬 수도 있습니다.\n\n이 분석을 고려하면, 컨테이너를 실행하는 노드에서 스왑을 비활성화하는 것이 더 나을 수 있다고 결론짓을 수도 있습니다. 하지만 결론을 서두르지 않는 것이 중요합니다. 우리는 항상 말씀드리듯이, “구체적인 환경은 구체적인 분석이 필요합니다.” 구체적인 시나리오를 고려할 때, 처음에는 그렇게 간단하지 않을 수도 있습니다.\n\n예를 들어 일부 종류의 프로그램은 가끔 메모리가 급증하여 OOM 킬러에 의해 종료되지 않도록하기 위해 스왑 공간이 필요할 수 있습니다. 이러한 프로그램을 재시작하는 것은 초기화 시간이 길기 때문에 비용이 들 수 있습니다. 이러한 프로그램들에 대해서는 스왑을 활성화하는 것이 의미가 있을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 프로그램들이 컨테이너에서 실행될 때, 같은 호스트 머신을 다른 컨테이너와 함께 공유하게 될 거예요. 하나의 컨테이너가 스왑이 필요없고 엄격한 Memory Cgroup 제한을 의존하는 경우, 두 컨테이너 간에 호스트 머신에서 충돌이 발생할 수 있어요. 그래서, 이 충돌을 어떻게 해결할 수 있을까요?\n\n이 문제를 해결하기 위해서는 Linux의 \"swappiness\" 개념을 살펴볼 필요가 있어요. 이것이 매우 도움이 될 거예요.\n\n# swappiness 매개변수를 올바르게 이해하는 방법\n\n일반적인 Linux 시스템에서 스왑 공간을 사용한 경우, proc 파일 시스템 (/proc/sys/vm/swappiness)에서 찾을 수 있는 swappiness 매개변수를 구성했을 수 있어요. swappiness의 정의는 Linux 커널 문서에 나와 있습니다.\n\n<div class=\"content-ad\"></div>\n\n고 значение은 kernel이 swap 하는 경향을 늘리고, 낮은 값은 그 경향을 줄입니다. swappiness 매개변수는 kernel이 런타임 메모리를 swap out 하는 선호도를 정의하며, 값이 100이면 kernel이 적극적으로 swap하고, 값이 0이면 kernel은 가능한 경우에는 swap을 피합니다. 기본 값은 60입니다.\n\n이 정의를 처음 읽고 값의 범위를 알게 되었을 때, swappiness를 백분율 값으로 생각했습니다. 이는 swap 공간 사용 빈도를 나타냅니다. 값이 100이면 메모리가 충분할 때에도 항상 디스크로 swap하게 되고, 값이 0이면 디스크로 swap하지 않는다는 것을 의미합니다.\n\n그러나 더 심층적인 고찰을 통해, 이해를 바로 잡아보자면, 이해가 완전히 잘못됐다기보다는 개념을 단순화한다는 점이 있습니다. 그래서 이 정의를 올바르게 이해하는 방법은 무엇일까요?\n\nLinux에서 디스크 파일 접근할 때, 시스템은 파일 I/O 성능을 향상시키기 위해 쉬는 메모리를 페이지 캐시로 사용하려고 노력합니다. swap 공간이 없는 경우, 메모리가 부족해지면 페이지 캐시가 해제되지만 Resident Set Size (RSS) 메모리는 해제되지 않는다는 것을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n대부분의 RSS 메모리는 malloc()에 의해 할당된 메모리와 같이 특정 디스크 파일과 연관시킬 수 없는 익명 메모리입니다. 스왑 공간이 활성화되어 있는 경우, 이 익명 메모리는 스왑 공간에 기록될 수 있습니다.\n\n그래서 스왑 공간이 활성화된 상황에서, 메모리가 부족할 때 리눅스 시스템이 페이지 캐시를 해제할지 아니면 익명 메모리를 해제하고 스왑 공간에 기록할지 결정하는 방법이 궁금해집니다.\n\n가능성이 높은 두 가지 시나리오를 분석해 봅시다:\n\n- 시스템이 모든 페이지 캐시를 먼저 해제하는 경우, 자주 파일 읽기/쓰기 작업이 필요한 경우에 성능이 저하될 수 있습니다.\n- 시스템이 먼저 모든 익명 메모리를 해제하고 스왑 공간에 기록하면, 그리고 즉시 해당 해제된 익명 메모리를 사용해야 할 때, 이를 스왑 공간에서 다시 읽어들여야 하므로 자주 디스크 I/O와 성능 저하가 발생할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n명확하게, Page Cache 및 익명 메모리의 릴리스를 균형 있게 유지해야 합니다. swappiness는 이 균형을 정의하는 매개변수입니다.\n\nswappiness는 이 균형을 어떻게 제어할까요? Linux 커널 코드가 이 swappiness 매개변수를 사용하는 방법을 살펴봅시다.\n\nswappiness 값의 범위는 0부터 100까지입니다. 하지만 이 값은 비율이지만 백분율은 아닙니다. 이 값은 릴리스되는 익명 메모리와 Page Cache 메모리 사이의 비율을 정의합니다.\n\n커널 코드에서 이 비율은 anon_prio:file_prio로 표현됩니다. 여기서 anon_prio는 swappiness와 동일합니다. 세 가지 시나리오에 대해 논의해 봅시다:\n\n<div class=\"content-ad\"></div>\n\n- 스왑니스가 100 일 때, 익명 메모리를 해제하는 비율과 페이지 캐시 메모리를 해제하는 비율은 100 : 100이며, 동일한 비율로 해제됩니다.\n- 기본 스왑니스 값이 60 일 때, 익명 메모리를 해제하는 비율과 페이지 캐시 메모리를 해제하는 비율은 60 : 140으로, 페이지 캐시 메모리가 익명 메모리보다 더 적극적으로 해제됨을 나타냅니다.\n\n```js\n        /*\n         * 스왑니스가 100 인 경우, 익명 및 파일에 동일한 우선 순위가 부여됩니다.\n         * 이 스캔 우선 순위는 사실 IO 비용의 역입니다.\n         */\n        anon_prio = swappiness;\n        file_prio = 200 - anon_prio;\n```\n\n한 가지 더 고려해야 할 시나리오가 있습니다: 스왑니스가 0으로 설정된 경우 무엇이 발생합니까? 이것은 Linux 가 익명 메모리를 스왑 공간에 쓰지 않도록 허용하지 않는다는 의미입니까?\n\n스왑니스의 정의를 다시 살펴보고 스왑니스가 0 인 경우에 특히 주의 깊게 살펴봅시다.\n\n<div class=\"content-ad\"></div>\n\n메모리(zone)의 \"high water mark\" 아래로 떨어질 때 swappiness가 0으로 설정되어 있어도 Linux는 여전히 메모리 스와핑을 수행합니다. 이는 메모리를 해제하기 위해 익명 메모리를 스왑 공간에 기록한다는 것을 의미합니다.\n\n이 문맥에서 \"zone\"은 Linux에서 물리적 메모리의 영역이며, 각 zone에는 메모리 압력 수준을 나타내는 세 가지 워터 마크가 있습니다.\n\n이 동작을 검증하기 위해 실험을 실행해 봅시다. 먼저 swappiness를 0으로 설정하는 명령어인 'echo 0 > /proc/sys/vm/swappiness'를 사용하세요. 그런 다음 이전 예제에서 사용한 mem_alloc 프로그램을 사용하여 메모리를 할당하세요. 예를 들어, 노드에 12GB의 메모리와 2GB의 스왑 공간이 있다면 mem_alloc을 사용하여 12GB의 메모리를 할당하세요.\n\nmem_alloc을 실행하기 전에 스왑 공간 사용량을 확인하세요. 사용량은 used=0로 표시되어야 합니다. mem_alloc을 실행한 후에는 일부 메모리가 스왑 공간에 기록되었음을 나타내는 출력이 표시되어야 합니다. 이것은 swappiness가 0으로 설정되어 있더라도 Linux가 필요할 때 여전히 메모리를 스왑 공간으로 스왑한다는 것을 확인합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-CanContainersUseSwapSpace_4.png)\n\nmem_alloc를 호출한 후에는 교체 공간이 실제로 사용 중입니다.\n\n![이미지](/assets/img/2024-06-19-CanContainersUseSwapSpace_5.png)\n\nmem_alloc가 노드에서 사용 가능한 최대 메모리(12GB)를 거의 요청했기 때문에 cat /proc/zoneinfo를 확인하면 normal 존의 높은 값이 free 값과 근접함을 알 수 있습니다. 이 상황에서 free가 high보다 작을 때 시스템은 익명 메모리 페이지를 회수하고 교체 공간에 기록합니다.\n\n\n<div class=\"content-ad\"></div>\n\n아래는 리눅스 시스템에서 swappiness 개념을 소개한 내용입니다. 이는 메모리가 부족할 때 익명 메모리와 페이지 캐시 메모리 간 재할당 비율을 결정합니다.\n\nswappiness 값은 0부터 100까지이며, 100은 익명 메모리와 페이지 캐시 메모리 간 재할당이 동일함을 의미합니다. 기본값은 일반적으로 60이며, 페이지 캐시 재할당을 우선시합니다. 심지어 swappiness가 0으로 설정되어도 Swap 파티션 사용이 완전히 비활성화되지는 않습니다. 이는 메모리가 부족할 때 Swap이 여전히 익명 메모리를 반환하기 위해 사용된다는 것을 의미합니다.\n\n# 문제 해결 방법\n\n<div class=\"content-ad\"></div>\n\n컨테이너를 Memory Cgroup으로 실행할 때 swappiness는 어떻게 작동하나요?\n\nMemory Cgroup 제어 그룹 아래 매개변수를 확인하면 memory.swappiness라는 매개변수가 있습니다. 이 매개변수는 무엇을 하는 걸까요?\n\nmemory.swappiness는 이 Memory Cgroup 제어 그룹 아래의 익명 메모리 및 페이지 캐시 회수를 제어하며, 범위 및 동작은 전역 swappiness와 유사합니다. 여기에는 우선 순위 순서가 있습니다: Memory Cgroup 제어 그룹에서 memory.swappiness 매개변수를 설정하면 이 그룹 내에서 전역 swappiness를 무효화시키므로 전역 swappiness가 작동하지 않게 됩니다.\n\n하지만 여기서 주의해야 할 차이점이 있습니다: memory.swappiness = 0으로 설정하면 익명 페이지 회수가 항상 비활성화되어 Swap 공간을 사용하지 않게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n이 시점에서 Linux 시스템은 더 이상 영역의 높은 기준 마크와 빈 메모리를 비교하여 메모리 Cgroup의 익명 메모리를 회수해야 하는지 여부를 결정하지 않습니다.\n\n“memory.swappiness=0”을 설정하면 메모리 Cgroup의 프로세스가 스왑 공간을 더 이상 사용하지 않습니다. 이 점을 이해하는 것이 중요합니다.\n\n이를 확인하기 위해 스왑 공간이 있는 노드에서 여전히 컨테이너를 실행해 볼 수 있습니다. 이전 글에서와 같은 컨테이너를 실행하되, 해당 Memroy Cgroup의 memory.swappiness를 0으로 설정하는 방법으로 진행할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-CanContainersUseSwapSpace_7.png)\n\n<div class=\"content-ad\"></div>\n\n이번에는 컨테이너에 메모리를 할당한 후 스왑 공간이 사용되지 않았으며, 컨테이너가 할당된 메모리가 memory.limit_in_bytes를 초과하면 OOM Kill이 발생했습니다.\n\n“memory.swappiness = 0”의 구성 및 기능으로 이 기사 초반에 제기한 문제를 해결할 수 있습니다.\n\n같은 호스트에서 컨테이너 A와 컨테이너 A의 스왑 공간이 필요한 애플리케이션을 실행하는 다른 컨테이너가 있는 상황에서, 다른 컨테이너는 스왑 공간을 사용할 필요가 없습니다.\n\n이 경우에는 호스트 노드에서 스왑 공간을 활성화하고, 다른 컨테이너에 해당하는 Memory Cgroups 제어 그룹에서 memory.swappiness 매개변수를 0으로 설정할 수 있습니다. 이렇게 함으로써 우리는 컨테이너 A의 요구 사항을 충족시키고, 다른 컨테이너가 영향을 받지 않고 여전히 Memory Cgroups의 memory.limit_in_bytes에 따라 메모리 사용량을 엄격하게 제한할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n결론적으로, memory.swappiness 매개변수는 매우 유용합니다. 이를 통해 Swap 공간을 사용해야 하는 컨테이너와 Swap 공간을 필요로 하지 않는 컨테이너가 동시에 동일한 호스트에서 실행될 수 있습니다.\n\n# 결론\n\n본 문서에서는 컨테이너에서 Swap 사용 여부에 대해 주로 다뤘습니다. 이 문제는 보다 간단하지 않습니다. 물론 호스트 노드에서 Swap 공간이 활성화되어 있다면 컨테이너에서 Swap을 사용할 수 있습니다. 그러나 동일한 호스트에서 Swap을 사용하지 않아도 되는 컨테이너를 위한 메모리 Cgroups 제한이 효과가 없어지는 문제가 발생할 수 있습니다.\n\n이 문제를 해결하기 위해 Linux의 swappiness 매개변수에 대해 알아보았습니다. swappiness 매개변수는 시스템에 Swap 공간이 있고 시스템이 메모리를 회수해야 할 때 페이지 캐시에서 메모리 해제를 우선시할지 익명 메모리(즉, Swap에 기록)에서 메모리 해제를 우선시할지 결정합니다.\n\n<div class=\"content-ad\"></div>\n\nswappiness 값은 0부터 100까지의 범위를 가지며, 다음 세 가지 값을 기억할 수 있습니다:\n\n- 100의 값은 페이지 캐시와 익명 메모리의 메모리 해제에 동등한 우선 순위를 부여함을 의미합니다.\n- 대부분의 Linux 시스템에서 기본값인 60의 값은 페이지 캐시에서 익명 메모리보다 메모리를 해제하는 것을 우선시합니다.\n- 0의 값은 시스템의 무료 메모리가 임계 값을 초과할 때에도 익명 메모리가 여전히 해제되고 페이지가 Swap 영역에 씌워집니다.\n\nproc 파일 시스템의 전역 swappiness 매개변수와 각 메모리 Cgroup 제어 그룹의 memory.swappiness 매개변수의 차이점은 메모리 Cgroup 제어 그룹의 swappiness 매개변수 값이 0으로 설정되면 해당 제어 그룹에서 Swap으로의 메모리 작성이 중지된다는 것입니다. memory.swappiness 매개변수를 사용하면 Swap을 사용해야 하는 컨테이너와 그렇지 않은 컨테이너가 동일한 호스트에서 실행되어 더 높은 하드웨어 자원 활용률을 이끌어냅니다.\n\n![이미지](/assets/img/2024-06-19-CanContainersUseSwapSpace_8.png)","ogImage":{"url":"/assets/img/2024-06-19-CanContainersUseSwapSpace_0.png"},"coverImage":"/assets/img/2024-06-19-CanContainersUseSwapSpace_0.png","tag":["Tech"],"readingTime":9},{"title":"가이드 Nuclei 사용법","description":"","date":"2024-06-19 14:51","slug":"2024-06-19-GuidetoUsingNuclei","content":"\n\nNuclei라는 강력한 취약점 스캐너 사용 방법을 배우세요.\n\n# 소개\n\nNuclei는 보안 연구원과 전문가를 위해 설계된 강력하고 유연한 오픈 소스 취약점 스캐너입니다. 웹 애플리케이션, API 및 네트워크 서비스의 다양한 취약점을 식별하고 보고하기 위해 사용자 정의 가능한 템플릿을 사용합니다. Nuclei는 다른 보안 도구와 통합되어 자동화된 워크플로에 매끄럽게 포함될 수 있습니다. 속도 제한, 사용자 지정 헤더, 외부 테스트, 다양한 구성 옵션과 같은 기능을 갖춘 Nuclei는 적극적인 취약점 관리와 보안 평가를 위한 효율적이고 철저한 솔루션을 제공합니다.\n\n![Nuclei 사용 가이드](/assets/img/2024-06-19-GuidetoUsingNuclei_0.png)\n\n<div class=\"content-ad\"></div>\n\n## TL;DR\n\n이 문서의 간략한 요약 버전을 여기에서 확인할 수 있습니다.\n\n# Basic Usage\n\nNuclei는 단일 대상물을 스캔하거나 파일에서 여러 대상물을 스캔하고 다른 도구들과의 워크플로에 통합할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 단일 대상 스캔\n\n취약점을 찾기 위해 단일 대상 URL을 스캔하려면 다음 명령을 사용하세요:\n\n```js\nnuclei -u http://example.com\n```\n\n다른 방법으로는 아래와 같이 사용할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nnuclei -target http://example.com\n```\n\n두 명령 모두 지정된 URL을 스캔하여 사용 중인 템플릿을 기반으로 알려진 취약점을 찾습니다.\n\n## 파일에서 대상 스캔\n\n대상 목록이 있는 경우 파일에 저장하고 각 대상을 스캔할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nnuclei -l targets.txt\n```\n\n이 방법은 여러 대상을 스캔하고 프로세스를 자동화하며 모든 지정된 URL이 취약점을 확인하는 데 효율적입니다.\n\n## 다른 도구와 Nuclei 통합하기\n\nNuclei를 다른 보안 도구와 통합하여 포괄적인 워크플로우를 만들 수 있습니다. 예를 들어 `subfinder`와 `httpx`를 Nuclei와 결합하여 하위 도메인을 찾은 다음 노출에 대해 스캔할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nsubfinder -d targetdomain.site -silent | httpx | nuclei -t http/exposures/\n```\n\n이 워크플로우는 먼저 `subfinder`를 사용하여 `targetdomain.site`의 서브도메인을 발견한 후, `httpx`로 해당 서브도메인의 HTTP 상태를 확인하고, 마지막으로 `http/exposures/` 디렉토리에 있는 Nuclei 템플릿을 사용하여 취약점을 스캔합니다.\n\n# 템플릿\n\n템플릿은 Nuclei가 스캔 중에 무엇을 찾을지를 정의합니다. 특정 유형의 취약점에 대한 것이거나 보다 일반적인 내용일 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 템플릿 폴더 사용하기\n\n특정 폴더의 모든 템플릿을 사용하여 스캔할 수 있습니다. 예를 들어, `http/exposures/` 폴더의 모든 템플릿을 사용하려면:\n\n```js\nnuclei -t http/exposures/\n```\n\n이 명령은 지정된 디렉토리의 모든 템플릿을 사용하여 대상을 스캔하며 다양한 유형의 노출을 확인합니다.\n\n<div class=\"content-ad\"></div>\n\n## 특정 템플릿 사용하기\n\n특정 템플릿을 활용하여 스캔하려면 `-t` 플래그와 함께 해당 템플릿을 나열하면 됩니다. 이를 통해 특정 취약점이나 기술을 기반으로 한 타겟 스캔이 가능합니다:\n\n```js\nnuclei -t http/technologies/tech-detect.yaml -t http/technologies/nginx/nginx-version.yaml\n```\n\n위 예시는 특정 기술을 스캔하고 사용 중인 Nginx 버전을 체크하는데, 지정된 템플릿을 활용합니다.\n\n<div class=\"content-ad\"></div>\n\n## 템플릿 태그 사용하기\n\n템플릿에 태그를 달아서 보다 쉽게 정리하고 활용할 수 있습니다. 특정 기준과 일치하는 템플릿을 사용하기 위해 태그를 지정할 수 있습니다:\n\n```js\nnuclei -u https://jira.targetdomain.site -tags jira,generic\n```\n\n이 명령은 `jira`와 `generic` 태그가 달린 템플릿을 사용하여 대상 URL을 스캔하여 관련 있는 취약점을 확인합니다.\n\n<div class=\"content-ad\"></div>\n\n## 심각도별 필터링\n\n더 중요한 문제에 집중하기 위해 심각도에 따라 템플릿을 필터링할 수 있습니다. 예를 들어:\n\n```js\nnuclei -u https://targetdomain.site -s critical,high,medium\n```\n\n이 명령은 심각하거나 높거나 중간 심각도로 분류된 템플릿을 사용하여 대상을 검사하며, 중요한 취약점을 우선적으로 처리합니다.\n\n<div class=\"content-ad\"></div>\n\n## 템플릿 제외하기\n\n스캔에서 특정 템플릿을 제외하려면 `-et` 플래그를 사용하세요. 이는 스캔을 개선하여 관련없거나 중요하지 않은 체크를 제외하는 데 도움이 됩니다:\n\n```js\nnuclei -et http/fuzzing/\n```\n\n이 명령은 스캔에서 `http/fuzzing/` 디렉토리의 모든 템플릿을 제외합니다.\n\n<div class=\"content-ad\"></div>\n\n# 헤더 설정\n\n스캔 중 HTTP 요청에 포함될 사용자 지정 헤더를 설정할 수 있습니다. 특정 상호 작용을 위해 특정 헤더가 필요한 응용 프로그램을 스캔할 때 유용합니다.\n\n## 사용자 지정 헤더\n\n`-H` 플래그를 사용하여 사용자 지정 헤더를 설정할 수 있습니다. 예를 들어, User-Agent 헤더를 설정하는 방법은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nnuclei -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36' -l targets.txt\n```\n\n이 명령어는 `targets.txt`에 나열된 모든 대상을 스캔하면서 각 요청마다 지정된 User-Agent 헤더를 보냅니다.\n\n# 속도 제한\n\n대상 서버를 과부하시키지 않도록 요청률과 동시 스레드 수를 제한할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 요청 및 쓰레드 제한\n\n초당 요청 수와 동시 쓰레드 수를 설정하여 스캔 속도를 제어하세요:\n\n```js\nnuclei -l targets.txt -rl 20 -c 5\n```\n\n이 명령은 Nuclei를 초당 20개의 요청으로 제한하고 최대 5개의 동시 쓰레드를 사용하여 속도와 서버 부하를 균형있게 유지합니다.\n\n<div class=\"content-ad\"></div>\n\n# 최적화\n\nNuclei는 스캔 성능을 최적화하고 오류를 효율적으로 처리하기 위한 여러 옵션을 제공합니다.\n\n## 타임아웃 설정\n\n스캔 속도를 높이기 위해 요청의 타임아웃을 줄일 수 있습니다. 기본 타임아웃은 10초이지만 필요에 따라 더 낮출 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nnuclei -l targets.txt -timeout 3\n```\n\n이 명령어는 요청 제한 시간을 3초로 설정합니다.\n\n## 오류 처리와 재시도\n\nNuclei가 오류를 처리하고 재시도하는 방법을 구성하세요. 일정 횟수의 오류 후 호스트를 건너뛰거나 실패한 요청에 대한 재시도 횟수를 설정하세요:\n\n<div class=\"content-ad\"></div>\n\n```js\nnuclei -l targets.txt --max-host-errors 5\n```\n\n```js\nnuclei -l targets.txt --retries 3\n```\n\n이 명령어들은 5번의 오류 발생 후 호스트를 건너뛰고, 실패한 요청을 최대 3번까지 다시 시도합니다.\n\n## 스캔 전략\n\n<div class=\"content-ad\"></div>\n\n로드와 효율성을 균형 있게 유지할 스캔 전략을 선택하세요. `host-spray`는 다음 대상으로 이동하기 전에 모든 템플릿을 단일 대상에 적용하고, `template-spray`는 여러 대상에 걸쳐 템플릿을 실행합니다:\n\n```js\nnuclei -l targets.txt -ss host-spray\n```\n\n이 명령은 `host-spray` 전략을 사용하며, 각 대상에 대한 부하를 줄일 수 있습니다.\n\n# 결과\n\n<div class=\"content-ad\"></div>\n\n**저장 결과**\n\n나중에 분석할 파일로 스캔 결과를 저장해보세요:\n\n```js\nnuclei -l targets.txt -o nuclei.log\n```\n\n<div class=\"content-ad\"></div>\n\n이 명령어는 스캔 결과를 `nuclei.log`에 기록합니다.\n\n## JSONL 출력\n\n스캔 결과를 JSONL (JSON Lines) 형식으로 출력하여 쉽게 구문 분석하고 다른 도구와 통합할 수 있습니다:\n\n```js\nnuclei -l targets.txt -jsonl\n```\n\n<div class=\"content-ad\"></div>\n\n## 프린팅 통계\n\n스캔 중에 진행 상황과 성능을 모니터링하는 통계를 표시하십시오:\n\n```js\nnuclei -l targets.txt -stats\n```\n\n## 마크다운 출력\n\n<div class=\"content-ad\"></div>\n\n아래는 Markdown 형식으로 결과를 저장하세요:\n\n```js\nnuclei -l targets.txt -me results/\n```\n\n이 명령은 스캔 결과를 `results/` 디렉토리에 Markdown 형식으로 저장합니다.\n\n# 외부 밴드 테스팅\n\n<div class=\"content-ad\"></div>\n\n외부 밴드 (OOB) 테스트는 일반적인 HTTP 요청/응답 주기 외에 발생하는 상호 작용을 테스트하는 것을 의미합니다.\n\n## OOB 테스트 비활성화\n\nOOB 테스트가 필요하지 않은 경우 비활성화할 수 있습니다:\n\n```js\nnuclei -l targets.txt -ni\n```\n\n<div class=\"content-ad\"></div>\n\n## 인터랙트 서버 사용하기\n\nOOB 상호 작용을 처리하기 위해 자체 호스트된 Interactsh 서버를 지정하십시오:\n\n```js\nnuclei -l targets.txt -iserver <server-addr> -itoken <server-token>\n```\n\n상호 작용 유출 시간:\n\n<div class=\"content-ad\"></div>\n\n```js\nnuclei -l targets.txt -interactions-eviction 120\n```\n\n그리고 사용자 지정 폴링 기간을 정의하십시오:\n\n```js\nnuclei -l targets.txt -interactions-poll-duration 10\n```\n\n이 명령어들은 Nuclei를 특정 Interactsh 서버를 사용하도록 구성하고 상호 작용을 기다릴 시간을 조절합니다.\n\n<div class=\"content-ad\"></div>\n\n# 설정\n\n세팅을 간편하게하고 일관된 스캔을 보장하기 위해 YAML 파일에서 구성을로드합니다. 기본 구성 파일은 `~/.config/nuclei/config.yaml`에 위치해 있습니다.\n\n## 구성 파일 사용\n\n`-config` 플래그를 사용하여 구성을 로드할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nnuclei -config nuclei.yaml -l targets.txt\n```\n\n## 구성 예시\n\n예시 구성 파일에는 사용자 지정 헤더, 템플릿 경로, 태그, 심각도 필터 및 속도 제한 설정이 포함될 수 있습니다:\n\n```js\nheader:\n  - 'X-BugBounty-Hacker: h1/nickname'\n\ntemplates:\n  - cves/\n  - vulnerabilities/\n  - misconfiguration/\n\ntags: exposures,cve\nseverity: critical,high,medium\n\ninclude-templates:\n  - vulnerabilities/xxx\n  - misconfiguration/xxxx\n\nexclude-tags: info,fuzz\nexclude-templates:\n  - vulnerabilities/xxx\n  - misconfiguration/xxxx\n\n# 속도 제한 설정\nrate-limit: 50\nbulk-size: 20\nconcurrency: 20\n```\n\n<div class=\"content-ad\"></div>\n\n이 구성은 스캔 프로세스를 최적화하기 위해 사용자 지정 헤더를 설정하고 템플릿을 지정하며 속도 제한 및 기타 설정을 정의합니다.\n\n# 업데이트\n\nNuclei 및 해당 템플릿을 최신 상태로 유지하여 최신 취약점을 확인할 수 있도록 합니다.\n\n## 업데이트 확인 비활성화\n\n<div class=\"content-ad\"></div>\n\n자동 업데이트 확인을 비활성화하여 스캔 중단을 방지하세요:\n\n```js\nnuclei -l targets.txt -duc\n```\n\n## 템플릿 및 Nuclei 업데이트\n\nNuclei 설치를 최신 버전으로 업데이트하세요.\n\n<div class=\"content-ad\"></div>\n\n```js\nnuclei -up\n```\n\n템플릿을 업데이트하려면:\n\n```js\nnuclei -ut\n```\n\n이 명령어들은 최신 기능 및 취약점 검사를 보장하여 템플릿과 Nuclei 도구 자체를 업데이트합니다.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\nNuclei는 보안 전문가와 연구자들에게 필수적인 도구로, 취약성 스캔을 위한 견고하고 유연한 솔루션을 제공합니다. 사용자는 다양한 타깃에서 보안 위험을 효과적으로 식별하고 관리할 수 있도록, 다양한 커스터마이즈 가능한 템플릿, 통합 기능 및 포괄적인 구성 옵션을 제공합니다. 레이트 제한, 커스텀 헤더, out-of-band 테스팅과 같은 강력한 기능을 활용함으로써 사용자들은 스캔 전략을 특정 요구사항과 환경에 맞게 맞춤화할 수 있습니다. Nuclei를 일상적인 보안 점검이나 더 큰 보안 작업 흐름에 도입할 때, 이 도구는 견고한 보안 자세를 유지하기 위해 필요한 다양성과 심도를 제공합니다. Nuclei의 파워를 활용하여 취약성 관리 방법을 향상시키고 잠재적인 위협에 선제적으로 대처하세요.","ogImage":{"url":"/assets/img/2024-06-19-GuidetoUsingNuclei_0.png"},"coverImage":"/assets/img/2024-06-19-GuidetoUsingNuclei_0.png","tag":["Tech"],"readingTime":8},{"title":"SQL에 대한 설명 랭킹 분석","description":"","date":"2024-06-19 09:54","slug":"2024-06-19-SQLExplainedRankingAnalytics","content":"\n\n인기 있는 RDBMS 관리 시스템 중 하나인 Oracle, SQL Server, Postgres 등에 경험이 있다면, 분석 또는 윈도잉 함수로 불리는 함수를 어느 정도 접해봤을 것입니다.\n\n분석 함수를 사용하면 데이터 집합 내의 행 그룹에 대한 집계 및 순위 시퀀스를 계산할 수 있습니다. 분석 함수에 대해 알아보고 사용해볼 가치가 있는지 궁금해 한다면, 확실한 \"예\"라고 말씀드릴 수 있어요. 그들은 굉장히 유용하며, 그들 없이는 어려운 것이 아니면 불가능한 일들을 SQL로 처리할 수 있게 해 줍니다.\n\n이 글에서는 SQL의 가장 일반적인 랭킹 기술 중 네 가지를 살펴보며, 랭킹 함수라고 하는 특정 분석 카테고리에 집중하겠습니다. 어떤 역할을 하는지 설명하고 사용 예시를 제공할 거에요.\n\n## 랭킹 분석 구문\n\n<div class=\"content-ad\"></div>\n\n대부분의 최신 SQL 방언에서 랭킹 분석 함수의 일반적인 형식은 다음과 같습니다.\n\n위 문장의 각 부분을 각각 간략히 살펴봅시다.\n\n## RANK | DENSE_RANK | ROW_NUMBER() | NTILE\n\n이들은 랭킹 함수의 이름들이며 대부분의 SQL에서 표시된 네 가지 함수를 지원합니다. 곧 각 개별 랭킹 함수에 대해 더 자세히 이야기해 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## OVER(…)\n\nover 키워드는 그룹화 정의를 나타내며 데이터 테이블에서 어떤 행을 등수 매기는지를 나타냅니다.\n\nPARTITION_BY_CLAUSE\n\npartition_by_clause는 선택 사항이며 데이터 세트에서 그룹화하려는 열(하나 이상)의 이름을 포함합니다. 생략하면 SQL이 등수 함수를 실행할 때 테이블의 모든 레코드를 고려합니다.\n\n<div class=\"content-ad\"></div>\n\n**ORDER_BY_CLAUSE**\n\n이 order_by_clause는 그룹을 구성하는 열이 순위를 매기기 전에 어떻게 정렬되는지를 지정합니다.\n\n**WINDOWING_CLAUSE**\n\n특정 그룹 내에서 windowing_clause는 현재 행에 대해 순위 함수가 작동해야 하는 레코드 범위를 정의합니다.\n\n<div class=\"content-ad\"></div>\n\n두 가지 유형의 윈도잉이 있습니다.\n\n- 로우 윈도우, 즉 현재 레코드로부터 봐야 할 한 개 이상의 물리적 행을 지정하는 윈도우\n- 레인지 윈도우로 현재 행의 값에서 빼거나 더하여 행의 범위를 정의하는 윈도우가 있습니다.\n\n일반적인 윈도우 프레임은 다음과 같습니다:\n\n- UNBOUNDED PRECEDING과 CURRENT ROW 사이의 행\n- X PRECEDING과 CURRENT ROW 사이의 행\n- CURRENT ROW과 X FOLLOWING 사이의 행\n- UNBOUNDED PRECEDING과 UNBOUNDED FOLLOWING 사이의 범위\n\n<div class=\"content-ad\"></div>\n\n완전성을 위해 WINDOWING_CLAUSE에 대한 설명을 추가했지만 솔직히 말해서, 일상 업무에서는 거의 사용할 일이 없을 것입니다. 99%의 경우, 기본값만으로 충분합니다. 사실, 일반적으로 랭킹 분석 함수에서는 windowing_clause를 전혀 사용하지 않습니다.\n\n랭킹 분석은 말로만 설명하기 어려운데, 그 사용 예를 몇 가지 보여드리는 게 가장 좋은 방법입니다.\n\n## 테스트 환경 설정\n\n저는 Oracle의 live SQL 웹사이트를 사용하여 테스트를 실행합니다. 이 서비스에 액세스하고 사용하는 방법에 대해 이전에 SQL에서 Grouping Sets, Rollup 및 Cube를 사용하는 데 관한 기사에서 설명했습니다. 완전히 무료로 설정하고 사용할 수 있습니다. 해당 기사의 링크는 아래에 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 샘플 테이블 생성 및 데이터 입력\n\n저희 예제에는 하나의 테이블만 필요하며 학생들과 공부하고 있는 과목의 시험 점수에 관한 데이터 세트가 포함되어 있습니다. 테이블은 student_id, student_name, subject, score로 구성되어 있습니다.\n\n```js\nCREATE TABLE student_scores (\n    student_id INT,\n    student_name VARCHAR(50),\n    subject VARCHAR(50),\n    score INT\n);\n\nINSERT INTO student_scores VALUES (1, 'Alice', 'Math', 95);\nINSERT INTO student_scores VALUES (2, 'Bob', 'Math', 85);\nINSERT INTO student_scores VALUES (3, 'Charlie', 'Math', 90);\nINSERT INTO student_scores VALUES (4, 'David', 'Math', 80);\nINSERT INTO student_scores VALUES (5, 'Eva', 'Math', 70);\nINSERT INTO student_scores VALUES (6, 'Frank', 'Science', 88);\nINSERT INTO student_scores VALUES (7, 'Grace', 'Science', 92);\nINSERT INTO student_scores VALUES (8, 'Hannah', 'Science', 85);\nINSERT INTO student_scores VALUES (9, 'Ivy', 'Science', 90);\nINSERT INTO student_scores VALUES (10, 'Jack', 'Science', 82);\nINSERT INTO student_scores VALUES (11, 'Kate', 'History', 78);\nINSERT INTO student_scores VALUES (12, 'Leo', 'History', 88);\nINSERT INTO student_scores VALUES (13, 'Mia', 'History', 84);\nINSERT INTO student_scores VALUES (14, 'Nina', 'History', 90);\nINSERT INTO student_scores VALUES (15, 'Oscar', 'History', 92);\n```\n\n```js\nselect * from student_scores;\n\n\n+------------+--------------+---------+-------+\n| student_id | student_name | subject | score |\n+------------+--------------+---------+-------+\n|          1 | Alice        | Math    |    95 |\n|          2 | Bob          | Math    |    85 |\n|          3 | Charlie      | Math    |    90 |\n|          4 | David        | Math    |    80 |\n|          5 | Eva          | Math    |    70 |\n|          6 | Frank        | Science |    88 |\n|          7 | Grace        | Science |    92 |\n|          8 | Hannah       | Science |    85 |\n|          9 | Ivy          | Science |    90 |\n|         10 | Jack         | Science |    82 |\n|         11 | Kate         | History |    78 |\n|         12 | Leo          | History |    88 |\n|         13 | Mia          | History |    84 |\n|         14 | Nina         | History |    90 |\n|         15 | Oscar        | History |    92 |\n+------------+--------------+---------+-------+\n\n15 rows selected.\n```\n\n<div class=\"content-ad\"></div>\n\n이제 데이터가 준비되었으니 순위 함수를 소개할 수 있어요.\n\nRANK\n\n랭크 함수를 사용하면 테이블의 행에 연속적인 정수 번호를 할당할 수 있지만, 랭크를 사용하면 순서 번호가 꼭 연속적이지 않을 수 있다는 점을 알아두어야 해요.\n\n가장 좋은 방법은 올림픽 스프린트 경기의 선수들을 상상하는 것이에요. 만약 두 선수가 1위에서 동시에 와 발생한다면 그들은 둘 다 1위를 할당받아 금메달을 딴다고 생각하시면 돼요. 그 다음으로 먼저 도착한 선수는 두 번째 위치가 아닌 세 번째 위치 (즉, 동메달)를 받게 될 거예요.\n\n<div class=\"content-ad\"></div>\n\n이것이 순위가 번호를 할당하는 방법이며, 아래 예시에서 명확히 볼 수 있습니다.\n\n```js\nselect subject, student_name, rank() over(order by subject) rnk  \nfrom student_scores;\n\n+---------+--------------+-----+\n| subject | student_name | rnk |\n+---------+--------------+-----+\n| History | Kate         |   1 |\n| History | Leo          |   1 |\n| History | Mia          |   1 |\n| History | Nina         |   1 |\n| History | Oscar        |   1 |\n| Math    | Alice        |   6 |\n| Math    | Bob          |   6 |\n| Math    | Charlie      |   6 |\n| Math    | David        |   6 |\n| Math    | Eva          |   6 |\n| Science | Frank        |  11 |\n| Science | Grace        |  11 |\n| Science | Hannah       |  11 |\n| Science | Ivy          |  11 |\n| Science | Jack         |  11 |\n+---------+--------------+-----+\n\n15 rows selected.\n```\n\n더 현실적인 사용 사례로, 각 과목의 점수를 해당 과목에서 가장 높은 점수순으로 나열할 수 있습니다.\n\n```js\nSELECT student_name, subject, score,\n       RANK() OVER (PARTITION BY subject ORDER BY score DESC) AS rank\nFROM student_scores;\n\n+--------------+---------+-------+------+\n| student_name | subject | score | rank |\n+--------------+---------+-------+------+\n| Alice        | Math    |    95 |    1 |\n| Charlie      | Math    |    90 |    2 |\n| Bob          | Math    |    85 |    3 |\n| David        | Math    |    80 |    4 |\n| Eva          | Math    |    70 |    5 |\n| Oscar        | History |    92 |    1 |\n| Nina         | History |    90 |    2 |\n| Leo          | History |    88 |    3 |\n| Mia          | History |    84 |    4 |\n| Kate         | History |    78 |    5 |\n| Grace        | Science |    92 |    1 |\n| Ivy          | Science |    90 |    2 |\n| Frank        | Science |    88 |    3 |\n| Hannah       | Science |    85 |    4 |\n| Jack         | Science |    82 |    5 |\n+--------------+---------+-------+------+\n\n15 rows selected.\n```\n\n<div class=\"content-ad\"></div>\n\n각 과목 그룹의 시작마다 랭크가 초기화된다는 것을 주목해주세요. 위의 데이터 세트를 입력으로 사용하여 각 과목 내에서 개별 최고 점수를 강조하는 것은 매우 쉽습니다.\n\n```js\nselect * from\n    (\n    SELECT student_name, subject, score,\n           RANK() OVER (PARTITION BY subject ORDER BY score DESC) AS rank\n    FROM student_scores\n    )\nwhere rank = 1\n\n\n\n| student_name | subject | score | rank |\n|--------------|---------|-------|------|\n| Alice        | Math    |    95 |    1 |\n| Oscar        | History |    92 |    1 |\n| Grace        | Science |    92 |    1 |\n```\n\n## DENSE_RANK\n\nDENSE_RANK 함수는 행에 순차적인 번호를 할당하는 데 rank 함수와 유사합니다. 차이점은 dense_rank가 간격이 없는 번호 시퀀스를 보장한다는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n올림픽 경주 비유를 이어가면, dense_rank 조건에서는 동일한 순위에 올라간 두 명이 여전히 금메달을 획득하지만, 그 다음 순위에 온 사람이 두 번째로 인정되어 은메달을 받게 됩니다.\n\n우리의 첫 번째 순위 SQL에서 rank를 dense_rank로 대체하면 내용을 명확히 이해할 수 있습니다. 다음 출력이 나옵니다.\n\n```js\nselect subject, student_name,dense_rank() over(order by subject) rnk  \nfrom student_scores;\n\n\n| subject | student_name | rnk |\n|---------|--------------|-----|\n| History | Kate         |   1 |\n| History | Leo          |   1 |\n| History | Mia          |   1 |\n| History | Nina         |   1 |\n| History | Oscar        |   1 |\n| Math    | Alice        |   2 |\n| Math    | Bob          |   2 |\n| Math    | Charlie      |   2 |\n| Math    | David        |   2 |\n| Math    | Eva          |   2 |\n| Science | Frank        |   3 |\n| Science | Grace        |   3 |\n| Science | Hannah       |   3 |\n| Science | Ivy          |   3 |\n| Science | Jack         |   3 |\n15 rows selected.\n```\n\ndense_rank에 대해 말할 것이 더는 없습니다. 간격 없는 순위 시퀀스가 반드시 필요하다면 순위 분석 대신 사용하면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n## ROW_NUMBER\n\n로우 넘버 분석은 각 파티션 내의 각 행에 고유한 정수 값을 할당합니다. 처음 들었을 때는 다른 두 등수 함수처럼 들릴 수 있지만, 중요한 차이점은 파티션 내에서 \"동점\"이 될 수 있는 레코드들이 다른 랭크가 부여되며, 각 파티션 내의 각 랭크는 그 파티션에 유니크하며 갭이 없다는 것입니다.\n\n다시 말해, 우리의 올림픽 경주 비유를 사용하면, 동시에 선을 획득한 선수 중 하나만 금메달을 획득하게 되고, 나머지는 은메달을 획득하며, 그 다음 순으로 들어온 사람은 동메달을 획득하게 됩니다. 이것은 예시입니다.\n\n```js\nSELECT student_name, subject, score,\n       ROW_NUMBER() OVER (ORDER BY score DESC) AS rn\nFROM student_scores;\n\n+--------------+---------+-------+----+\n| student_name | subject | score | rn |\n+--------------+---------+-------+----+\n| Alice        | Math    |    95 |  1 |\n| Grace        | Science |    92 |  2 |\n| Oscar        | History |    92 |  3 |\n| Ivy          | Science |    90 |  4 |\n| Nina         | History |    90 |  5 |\n| Charlie      | Math    |    90 |  6 |\n| Frank        | Science |    88 |  7 |\n| Leo          | History |    88 |  8 |\n| Bob          | Math    |    85 |  9 |\n| Hannah       | Science |    85 | 10 |\n| Mia          | History |    84 | 11 |\n| Jack         | Science |    82 | 12 |\n| David        | Math    |    80 | 13 |\n| Kate         | History |    78 | 14 |\n| Eva          | Math    |    70 | 15 |\n+--------------+---------+-------+----+\n\n15개 행이 선택되었습니다.\n```\n\n<div class=\"content-ad\"></div>\n\n그레이스가 영 매칭 점수를 받은 것과 마찬가지로 오스카도 점수가 같은데 그것보다 상위에 랭크되어 있는 것을 보실 수 있습니다. 점수별 정렬 순서가 결정되지 않았기 때문에 그들의 랭킹 역시 결정적이지 않다는 것을 의미합니다. 위와 같은 쿼리를 다시 실행하면 그들의 순서, 즉 순위 값이 반전될 수도 있습니다.\n\n참고로, over() 절 내부에서 파티션을 지정하지 않았기 때문에 랭킹은 전체 데이터 세트에 적용되어 레코드 번호가 1에서 시작하여 모든 레코드가 처리될 때까지 1씩 증가합니다.\n\n마지막 예제인 row_number() 함수로, 파티션 절을 함께 사용하는 방법은 테이블 데이터의 중복을 제거해야 할 때 매우 유용하다는 것을 보여드리겠습니다. 먼저 좀 더 많은 데이터를 생성하기 위해 약간의 중복된 행을 세 명의 학생, 케이트, 앨리스, 오스카에 대해 삽입할 것입니다.\n\n```js\ninsert into student_scores select * from student_scores\nwhere student_name in ('Alice','Kate','Oscar');\n\n\nselect * \nfrom student_scores;\n+------------+--------------+---------+-------+\n| student_id | student_name | subject | score |\n+------------+--------------+---------+-------+\n|          1 | Alice        | Math    |    95 |\n|          1 | Alice        | Math    |    95 |\n|          2 | Bob          | Math    |    85 |\n|          3 | Charlie      | Math    |    90 |\n|          4 | David        | Math    |    80 |\n|          5 | Eva          | Math    |    70 |\n|          6 | Frank        | Science |    88 |\n|          7 | Grace        | Science |    92 |\n|          8 | Hannah       | Science |    85 |\n|          9 | Ivy          | Science |    90 |\n|         10 | Jack         | Science |    82 |\n|         11 | Kate         | History |    78 |\n|         11 | Kate         | History |    78 |\n|         12 | Leo          | History |    88 |\n|         13 | Mia          | History |    84 |\n|         14 | Nina         | History |    90 |\n|         15 | Oscar        | History |    92 |\n|         15 | Oscar        | History |    92 |\n+------------+--------------+---------+-------+\n\n18 rows selected.\n```\n\n<div class=\"content-ad\"></div>\n\n이제 학생 이름을 기준으로 순위를 매기는 row_number()를 사용하여 순위가 `1인 데이터를 선택함으로써 데이터 테이블에서 중복 레코드를 효과적으로 식별할 수 있습니다. 다음은 이 작업을 수행하는 SQL입니다.\n\n```js\nWITH RankedScores AS (\n    SELECT student_id, student_name, subject, score,\n           ROW_NUMBER() OVER (PARTITION BY student_id, \n           student_name, subject, score ORDER BY student_id) AS rn\n    FROM student_scores\n)\nSELECT student_id, student_name, subject, score\nFROM RankedScores\nWHERE rn > 1;\n\n\n- 학생_id | 학생_이름 | 과목 | 점수\n-|------------|--------------|---------|-------|\n|          1 | Alice        | Math    |    95 |\n|         11 | Kate         | History |    78 |\n|         15 | Oscar        | History |    92 |\n\n3개의 행이 선택됨.\n```\n\n이 식별된 레코드를 사용하여 테이블에서 중복을 제거할 수 있습니다. 다음은 이러한 레코드를 사용하여 테이블을 원래 데이터 세트로 복원하는 인플레이스 삭제를 수행하는 예시입니다.\n\n```js\nDELETE FROM student_scores\nWHERE rowid IN (\n    SELECT rid\n    FROM (\n        SELECT rowid AS rid,\n               ROW_NUMBER() OVER (PARTITION BY student_id, \n               student_name, subject, score ORDER BY student_id) AS rn\n        FROM student_scores\n    )\n    WHERE rn > 1\n);\n\n3개의 행이 삭제됨.\n\n\nSELECT * FROM student_scores;\n\n- 학생_id | 학생_이름 | 과목 | 점수\n-|------------|--------------|---------|-------|\n|          1 | Alice        | Math    |    95 |\n|          2 | Bob          | Math    |    85 |\n|          3 | Charlie      | Math    |    90 |\n|          4 | David        | Math    |    80 |\n|          5 | Eva          | Math    |    70 |\n|          6 | Frank        | Science |    88 |\n|          7 | Grace        | Science |    92 |\n|          8 | Hannah       | Science |    85 |\n|          9 | Ivy          | Science |    90 |\n|         10 | Jack         | Science |    82 |\n|         11 | Kate         | History |    78 |\n|         12 | Leo          | History |    88 |\n|         13 | Mia          | History |    84 |\n|         14 | Nina         | History |    90 |\n|         15 | Oscar        | History |    92 |\n\n15개의 행이 선택됨.\n```\n\n<div class=\"content-ad\"></div>\n\n## NTILE\n\nNTILE 함수를 사용하면 데이터 세트를 대략적으로 동일한 크기의 레코드 그룹으로 나눌 수 있습니다. 이러한 그룹을 \"타일\"이라고 하며, 동일한 타일 내의 모든 항목에는 동일한 순위가 할당됩니다.\n\n아래 예시에서는 학생들의 성적을 기반으로 테이블을 4개의 그룹으로 세분화하려고 합니다. 다시 말해, 각 그룹(또는 타일)은 실제 데이터 세트의 범위 내에서 대략적으로 유사한 점수를 가져야 합니다.\n\n각 그룹 내의 레코드 수는 미리 알 수 없습니다. 요청한 타일 수가 출력에 표시되지만, 두 개의 그룹에는 3개의 레코드가 있고 다른 두 그룹에는 각각 4개와 5개의 레코드가 포함되어 있습니다. SQL이 보장할 수 있는 것은 가능한 경우 요청한 타일 수를 반환한다는 것뿐입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\nSELECT \n    student_name,\n    subject,\n    score,\n    NTILE(4) OVER (ORDER BY score) AS tile_rank\nFROM \n    student_scores;\n\n\n| student_name | subject | score | tile_rank  |\n|--------------|---------|-------|------------|\n| Eva          | Math    |    70 |          1 |\n| Kate         | History |    78 |          1 |\n| David        | Math    |    80 |          1 |\n| Jack         | Science |    82 |          2 |\n| Mia          | History |    84 |          2 |\n| Bob          | Math    |    85 |          2 |\n| Hannah       | Science |    85 |          2 |\n| Frank        | Science |    88 |          3 |\n| Leo          | History |    88 |          3 |\n| Ivy          | Science |    90 |          3 |\n| Nina         | History |    90 |          3 |\n| Charlie      | Math    |    90 |          3 |\n| Grace        | Science |    92 |          4 |\n| Oscar        | History |    92 |          4 |\n| Alice        | Math    |    95 |          4 |\n\n 15 rows selected\r\n```\n\n- NTILE(4)는 행을 4개의 타일 또는 그룹으로 나누고자 함을 나타냅니다.\n- OVER (ORDER BY score) 파티션 절을 지정하지 않았기 때문에 SQL은 데이터세트를 위해 테이블의 모든 레코드를 고려하고 이 레코드들을 점수 열을 기준으로 순서대로 정렬한 후 그것들을 네 개의 타일로 분할하도록 지시합니다.\n\n## 요약\n\n마무리로, 현대 SQL 시스템에서 가장 흔한 4가지 SQL 랭킹 함수를 강조하고 그 사용 예시를 보여드렸습니다. 이 함수들은 모두 상대적인 순서에 따라 레코드에 랭킹 값을 할당하며, RANK, DENSE_RANK 및 ROW_NUMBER는 동점 처리를 다루고 순번을 지정하는 방식이 다르게 제공합니다. 한편, NTILE은 데이터에 대한 통계 분석이나 세분화 분석을 수행해야 하는 경우에 유용합니다. 이러한 기능을 가능한 많이 사용하고 일상적으로 사용해 보시기를 권장합니다. 데이터 분석, 조작 및 보고를 위한 강력한 도구이기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n이 컨텐츠가 마음에 드셨다면, 이 기사들도 흥미롭게 보실 것 같아요.","ogImage":{"url":"/assets/img/2024-06-19-SQLExplainedRankingAnalytics_0.png"},"coverImage":"/assets/img/2024-06-19-SQLExplainedRankingAnalytics_0.png","tag":["Tech"],"readingTime":15},{"title":"제목 제로부터 dbt까지 스포티파이의 백만 개 플레이리스트 데이터를 분석하고 데이터 모델 구축하는 방법","description":"","date":"2024-06-19 09:52","slug":"2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData","content":"\n\n<img src=\"/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_0.png\" />\n\n앞으로 몇 주 동안, dbt (data build tool)를 사용하여 Spotify의 백만 개 플레이리스트 데이터셋을 엔드 투 엔드 분석 프로젝트로 변환하는 방법을 안내할 것입니다. 중소형 대형 실제 세계 원시 데이터를 상호 작용적인 데이터 모델로 변환하는 방법을 배우게 될 거에요. (어떤걸 🤣 기반으로 한 George Orwell의 하층층상 중간층에요)\n\n## 배울 내용\n\n- 30GB의 원시 JSON 데이터를 효율적이고 확장 가능하게 5GB Parquet 파일로 변환하기.\n- Parquet 파일을 심층적인 탐색과 분석을 위한 여러 dbt 모델로 변환하기.\n- 데이터 변환 프로세스에서 dbt를 사용하는 것이 왜 최선의 실천법인지 이해하기.\n- 데이터 무결성과 정확성을 보장하기 위해 각 dbt 모델 변경을 검증하는 방법에 대해 배우기 (스포일러: 오픈 소스 dbt 모델 코드 리뷰 도구인 Recce를 사용하세요).\n\n<div class=\"content-ad\"></div>\n\n## 데이터로부터 중요한 질문에 답변해주세요\n\n매주 Recce LinkedIn 페이지에 스포티파이 데이터셋에 관한 두 가지 질문을 게시할 것입니다. 예를 들어,\n\n- 적어도 3곡의 테일러 스위프트 노래를 포함하는 재생 목록은 몇 개인가요?\n- 제이 체오의 인기 있는 상위 10곡은 무엇인가요?\n- BLACKPINK 💗과 Post Malone이 모두 포함된 재생 목록은 몇 개인가요?\n\n그 후에 투표를 가장 많이 받은 질문을 오픈 소스 저장소에 구현할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 누구를 위한 것인가요?\n\n비즈니스 BI 또는 ML을 위해 데이터 변환에 dbt를 사용하는 방법에 관심이 있는 모든 분들을 환영합니다. 뿐만 아니라, 데이터 또는 분석 엔지니어로 계속된 작업에 유용한 몇 가지 dbt 모베스트 사항을 함께 공유할 예정입니다.\n\n# 백만 플레이리스트 데이터 준비하기\n\n시작할 준비가 되셨나요? 멋지네요. 이 프로젝트에서는 스포티파이 백만 플레이리스트 데이터셋을 사용할 예정입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_1.png)\n\n## 데이터셋 다운로드\n\nSpotify 정책에 따라 등록하고 여기서 원시 데이터를 다운로드해야 합니다. 우리는 spotify_million_playlist_dataset.zip 파일을 사용할 거에요 (크기는 5.4 GB 👀).\n\n이 zip 파일은 31GB로 풀리니 충분한 공간이 있는지 확인해주세요! (나중에 Parquet으로 변환하면 용량이 줄어듭니다)\n\n\n<div class=\"content-ad\"></div>\n\n\n![image](https://miro.medium.com/v2/resize:fit:960/1*HbIZkLZc-9ClzgToTNLvsg.gif)\n\n데이터셋을 다운로드하고 압축 해제한 후, data 폴더에는 천 개의 분할된 JSON 파일로 구성되어 있음을 발견할 것입니다. 이러한 파일들은 다음과 같은 패턴으로 명명되어 있습니다:\n\n- mpd.slice.0–999.json\n- mpd.slice.1000–1999.json\n- …\n- mpd.slice.999000–999999.json\n\n분할된 JSON 파일 중 하나에서 플레이리스트 항목의 전형적인 예시는 다음과 같습니다:\n\n\n<div class=\"content-ad\"></div>\n\n![링크](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_2.png)\n\n데이터셋의 각 JSON 파일은 1,000개의 재생목록을 나타내며, 총 1백만 개의 재생목록이 포함되어 있습니다. 파일 접두사 \"mpd\"는 \"Million Playlist Dataset\"의 약자입니다.\n\nSpotify 팀은 이러한 JSON 파일의 무결성을 확인하고 MD5 체크섬을 사용하여 기본 통계를 계산하는 데 도움이 되도록 ./src 폴더에 스크립트를 제공했습니다. 아래 명령어로 기본 통계를 계산할 수 있습니다:\n\n```js\n$ python src/stats.py data\n```\n\n<div class=\"content-ad\"></div>\n\nSpotify의 README 문서에 따르면, 이 프로그램의 결과물은 'stats.txt' 내용과 일치해야 합니다. stats.py의 실행 시간은 노트북의 성능에 따라 다를 수 있으며, 30분을 초과할 수도 있습니다.\n\n# 초기 인사이트\n\n우리는 우선적으로 몇 가지 탐구를 시작해 초기 인사이트를 얻고 데이터셋을 더 잘 이해할 것입니다. 이 작업은 raw json을 사용하여 이루어질 것이지만, 더 고급 데이터 상호작용을 위해서는 데이터를 더 효율적인 형식으로 변환해야 할 것입니다. 이에 Parquet을 사용할 것이며 (자세한 내용은 아래에 소개되어 있음), 이는 dbt와 함께 사용하기에 이상적이며 raw 데이터를 변환하는 데 유용합니다.\n\nSpotify의 1000개 raw 데이터 파일로 되돌아가보죠. 모든 데이터 분석 처리 워크플로우에서 겪었던 노고와 눈물이 어떤 것이었는지 보여드리겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# DuckDB 및 jq\n\nDuckDB와 jq는 모두 JSON 데이터와 상호 작용하기 위한 훌륭한 도구입니다. 이 멋진 도구들을 설치하려면 선호하는 패키지 관리자를 사용하십시오. 예를 들어:\n\n```sh\n$ brew install duckdb\n$ brew install jq\n```\n\n## JSON 구조 이해하기\n\n<div class=\"content-ad\"></div>\n\n우리는 JSON 데이터를 빠르게 확인하기 위해 jq를 사용할 수 있습니다. 이후 보다 심층적인 분석을 위해 DuckDB를 활용할 수 있습니다. 1000개 파일 중 하나를 살펴보겠습니다:\n\n```js\n$ cd spotify_million_playlist_dataset/data\n$ jq 'keys' mpd.slice.0-999.json\n```\n\nMarkdown 양식으로 표를 변경했습니다:\n\n![표](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_3.png)\n\nJSON의 모든 조각은 두 개의 키만 포함하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 정보 — 이것은 단순히 JSON 파일의 메타데이터입니다.\n- 재생 목록 — 실제로 관심 있는 데이터\n\n아마도 \"재생 목록\"이 배열이라는 것을 짐작하실 수 있습니다. 따라서 재생 목록에서 첫 번째 요소를 살펴보겠습니다.\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_4.png)\n\n부분 재생 목록 데이터(첫 번째 트랙만 표시)는 다음과 같이 보일 것입니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_5.png)\n\n# 덕DB로 분석하기\n\nJSON 데이터의 구조를 파악한 후에는 DuckDB를 사용하여 데이터를 분석할 수 있습니다. DuckDB는 SQL 데이터 유형을 JSON 파일 내에서 자동으로 감지하는 기능을 제공하므로 분석에 SQL 구문을 손쉽게 적용할 수 있습니다.\n\n## DuckDB 대화형 셸 열기\n\n\n<div class=\"content-ad\"></div>\n\n터미널에 duckdb를 입력하면 PostgreSQL의 psql 및 SQLite 셸과 유사한 대화형 셸에 들어갈 수 있어요.\n\n```js\n$ duckdb\n```\n\n## DuckDB의 maximum_object_size 조정\n\n다음 명령을 실행하면 아래의 오류가 표시됩니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nSELECT * FROM read_json_auto('./mpd.slice.0-999.json');\n\n-- \"maximum_object_size\" of 16777216 bytes exceeded \n-- while reading file \"./mpd.slice.0-999.json\" (>33554428 bytes).\n-- \"maximum_object_size\"을 늘려주세요.\n```\n\n이 오류는 밀리언 플레이리스트 데이터셋의 JSON 슬라이스가 DuckDB의 기본 maximum_object_size보다 크기 때문에 발생했습니다. 따라서 이를 조정하여 40MB로 설정해야 합니다 🫰:\n\n```js\nSELECT * \nFROM read_json_auto('./mpd.slice.0-999.json', maximum_object_size = 40000000); \n\n┌──────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n│         info         │                                                                            playlists                                                                             │\n│ struct(generated_o…  │ struct(\"name\" varchar, collaborative varchar, pid bigint, modified_at bjigint, num_tracks bigint, num_albums bigint, num_followers bigint, tracks struct(pos bi…  │\n├──────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n│ {'generated_on': 2…  │ [{'name': Throwbacks, 'collaborative': false, 'pid': 0, 'modified_at': 1493424000, 'num_tracks': 52, 'num_albums': 47, 'num_followers': 1, 'tracks': [{'pos': …  │\n└──────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n```\n\n## JSON 해제하기\n\n<div class=\"content-ad\"></div>\n\n우리는 컬럼 안에 중첩 구조인 재생 목록에만 관심이 있다는 것을 알고 있습니다. 그러므로 플레이리스트 열을 정규화하기 위해 UNNEST를 사용할 수 있습니다:\n\n```js\nSELECT UNNEST(playlists) \nFROM read_json_auto('./mpd.slice.0-999.json', maximum_object_size = 40000000) \nLIMIT 5;\n\n┌─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n│                                                                                    unnest(playlists)                                                                                    │\n│ struct(\"name\" varchar, collaborative varchar, pid bigint, modified_at bigint, num_tracks bigint, num_albums bigint, num_followers bigint, tracks struct(pos bigint, artist_name varch…  │\n├─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n│ {'name': Throwbacks, 'collaborative': false, 'pid': 0, 'modified_at': 1493424000, 'num_tracks': 52, 'num_albums': 47, 'num_followers': 1, 'tracks': [{'pos': 0, 'artist_name': Missy …  │\n│ {'name': Awesome Playlist, 'collaborative': false, 'pid': 1, 'modified_at': 1506556800, 'num_tracks': 39, 'num_albums': 23, 'num_followers': 1, 'tracks': [{'pos': 0, 'artist_name': …  │\n│ {'name': korean , 'collaborative': false, 'pid': 2, 'modified_at': 1505692800, 'num_tracks': 64, 'num_albums': 51, 'num_followers': 1, 'tracks': [{'pos': 0, 'artist_name': Hoody, 't…  │\n│ {'name': mat, 'collaborative': false, 'pid': 3, 'modified_at': 1501027200, 'num_tracks': 126, 'num_albums': 107, 'num_followers': 1, 'tracks': [{'pos': 0, 'artist_name': Camille Sai…  │\n│ {'name': 90s, 'collaborative': false, 'pid': 4, 'modified_at': 1401667200, 'num_tracks': 17, 'num_albums': 16, 'num_followers': 2, 'tracks': [{'pos': 0, 'artist_name': The Smashing …  │\n└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n```\n\nDuckDB는 UNNEST 함수에서 `recursive := true`와 같이 매우 편리한 옵션을 제공합니다. 이 옵션은 열을 재귀적으로 정규화합니다:\n\n```js\nSELECT UNNEST(playlists, recursive := true) \nFROM read_json_auto('./mpd.slice.0-999.json', maximum_object_size = 40000000) \nLIMIT 5;\n```\n\n<div class=\"content-ad\"></div>\n\n이렇게 하면 깊게 중첩된 JSON 데이터를 다루기가 매우 편리합니다.\n\n## JSON을 단일 표로 결합\n\n현재, 우리는 하나의 JSON 파일만 처리하고 있습니다. 만약 1,000개의 나누어진 JSON 파일을 모두 한 표로 합치고 싶다면 어떻게 해야 할까요?\n\nDuckDB는 여러 JSON 파일을 한 번에 읽을 수 있게 해주는 glob 구문을 제공합니다. `./mpd.slice.0-999.json`을 `./mpd.slice*.json`로 수정하면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n테이블 태그를 Markdown 형식으로 변경하세요.\n\nCREATE TABLE playlists AS \nSELECT UNNEST(playlists , recursive:= true) \nFROM read_json_auto('./mpd.slice*.json', maximum_object_size = 40000000);\n```\n\n내 노트북(M3 MacBook)에서 playlists DuckDB 테이블을 만드는 데 30초가 걸렸어요. 이제 데이터를 Parquet으로 변환할 준비가 되었어요.\n\n## Parquet으로 변환\n\n변환 과정 중간에 메모리 부족 오류를 방지하기 위해 일부 임시 파일이 필요할 수 있습니다. DuckDB 쉘에서 계속하여, 먼저 다음을 실행하세요:\n\n<div class=\"content-ad\"></div>\n\n```js\nSET temp_directory='./tmp';\n```\n\n이제 DuckDB 테이블에서 플레이리스트를 Parquet 파일로 내보낼 준비가 되었습니다. copy 명령을 사용하여 .parquet 확장자를 갖는 파일을 지정하면 DuckDB가 자동으로 Parquet 파일로 내보내기를 원한다는 것을 알게 됩니다.\n\n```js\nCOPY playlist TO 'playlists.parquet';\n```\n\n쉽죠?\n\n\n<div class=\"content-ad\"></div>\n\n# Parquet 대 JSON\n\n그래서, 왜 Parquet을 사용해야 할까요?\n\nParquet은 분석을 위한 우수한 파일 포맷으로, 컬럼 저장 방식을 통해 JSON에 비해 주목할만한 장점을 제공합니다. 이 설계는 데이터 압축 및 인코딩을 향상시켜 저장 공간을 줄이고 데이터 분석 워크플로우의 데이터 액세스 속도를 높이는데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n복잡한 중첩 데이터 구조도 지원하며 기존 데이터를 수정하지 않고 새 열을 추가할 수 있는 유연한 스키마 진화를 제공합니다. 이는 스키마 변경이 자주 발생하는 시나리오에 이상적인 형식이 됩니다.\n\n또한, 주요 데이터 웨어하우스와의 호환성을 통해 Parquet은 특히 dbt 사용자에게 매우 중요하며 데이터 통합 및 분석 워크플로우를 간소화합니다. 즐겨 사용하는 데이터 웨어하우스에서 쉽게 Parquet 파일을 가져오고 내보낼 수 있습니다.\n\nDuckDB와 jq를 사용하면 기가바이트의 JSON 데이터를 노트북에서 간단하게 분석할 수 있습니다.\n\n\"ON - YOUR - LAPTOP\"을 반복해보세요 💻\n\n<div class=\"content-ad\"></div>\n\n# 요약\n\n원본 데이터 세트는 1,000개의 JSON 파일로 이루어져 있으며 총 31GB입니다.\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_7.png)\n\n세 가지 간단한 DuckDB 쿼리를 실행한 후 1분의 처리 시간을 거쳐 단일 5.7GB Parquet 파일을 얻게 되어, 500% 개선이 이뤄졌습니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_8.png)\n\n지금은 몇 초 안에 노트북으로 \"플레이리스트에 테일러 스위프트 노래가 몇 개 있는지?\"와 같은 질문에 빠르게 답변할 수 있습니다. 마음이 홀립니다.\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_9.png)\n\n이 데이터 분석 프로젝트의 첫 번째 부분은 여기까지입니다. 곧 두 번째 부분도 뵙겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 다음에는...\n\n![이미지](/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_10.png)\n\n데이터셋을 Parquet 파일로 성공적으로 전환한 후, 다음 목표는 dbt의 파워를 활용하여 One Million Playlists 데이터셋에서 더 깊고 더 매력적인 분석적 인사이트를 발굴하는 것입니다.\n\n## 데이터에 소프트웨어 엔지니어링 최상의 실천 방법 적용\n\n<div class=\"content-ad\"></div>\n\n덕DB 셸은 데이터 집합을 대화식으로 분석할 수 있는 기능을 제공하지만, 우리의 SQL 변환에 보다 구조화되고 협업적이며 버전 관리된 접근이 필요함을 알 수 있습니다.\n\n여기서 dbt가 빛을 발합니다 🤩. dbt를 사용하면 데이터 변환을 코드로 처리할 수 있어 소프트웨어 엔지니어링 관행인 버전 관리, 코드 리뷰(Recce 빛나요 💖), 그리고 자동화된 테스트를 데이터 워크플로에 적용할 수 있습니다.\n\n## 함께 작업하기\n\n여러 SQL 쿼리를 논리적인 dbt 모델로 구성함으로써, 데이터 변환의 명확성과 유지 관리성을 향상시킬 뿐만 아니라, 데이터 팀이 서로 협력하여 서로의 작업을 점진적으로 빌드할 수 있습니다. 이 협업적인 접근은 데이터 모델이 견고하고 정확하며 최신 비즈니스 로직과 분석적 통찰을 반영하도록 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n## 신뢰할 수 있는 환경\n\n또한, dbt의 문서 기능을 사용하면 데이터 모델의 포괄적인 문서를 자동으로 생성하여 새 팀원들이 데이터 환경을 이해하기 쉽고 이해관계자들이 데이터 주도적 의사결정을 신뢰할 수 있게 합니다.\n\n## 데이터 주도적 개발\n\n요약하면, dbt는 SQL 변환을 효율적으로 관리할 수 있는 필수 도구와 함께 제공하여 협업적이고 반복적인 데이터 문화를 육성하는 데 도움이 되어, 오늘날의 데이터 주도적 세상에서 경쟁력을 유지하는 데 필수적입니다.\n\n<div class=\"content-ad\"></div>\n\n# Part 2에서 만나요\n\n최신 소식을 받아보고 더 흥미로운 소식을 확인하려면 LinkedIn을 팔로우하세요! 🤩\n\n업데이트: Part 2가 이제 사용 가능합니다. 저의 샘플 프로젝트를 따라가면서 dbt가 데이터 프로젝트 모델링에 적합한 이유를 살펴보겠습니다. 아, 그리고 중요한 Spotify 플레이리스트 질문에 대해 답변도 해드립니다!\n\n# 파이프라인에서 더 많은 기사","ogImage":{"url":"/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_0.png"},"coverImage":"/assets/img/2024-06-19-FromZerotodbtHowtoAnalyzeandBuildDataModelsfromSpotifysMillionPlaylistData_0.png","tag":["Tech"],"readingTime":12},{"title":"데이터, AI 서밋에서 얻은 교훈 파트 II","description":"","date":"2024-06-19 09:50","slug":"2024-06-19-DataAISummitTakeawaysPartII","content":"\n\n<img src=\"/assets/img/2024-06-19-DataAISummitTakeawaysPartII_0.png\" />\n\n# 소개\n\n지난 주 데이터 및 AI 써밋에서 전체 세션을 빠르게 업로드해 준 Databricks에 큰 감사를 전합니다. 그들 모두를 살펴보는 것은 불가능하지만, 내가 확인한 몇 가지 가운데 내가 좋아하는 이야기는 다음과 같습니다.\n\n# Spark 업그레이드/마이그레이션\n\n<div class=\"content-ad\"></div>\n\n대량 업그레이드와 마이그레이션은 플랫폼 팀에게 언제나 쉬운 작업이 아닙니다. 넷플릭스는 이 세션에서 모든 Spark 작업에 대한 인벤토리를 작성하고 마이그레이션 제어 및 자동화 플레인을 호스팅하며, 심지어 마이그레이션 프로세스에 유효성 검사 및 관측 기능을 통합하는 방법을 설명했습니다. 넷플릭스가 운영하는 규모를 고려하면, 그것은 매우 인상적입니다.\n\n우리는 이제까지 기관 전체적인 Spark 업그레이드를 하지 않았습니다(하지만 Databricks 런타임이 관련될 때 특히 그렇게 할 생각이 좋을 수도 있습니다). 하지만 이것은 어떻게 그겢을 달성할 수 있는 유용한 프레임워크를 제공합니다. 개별 팀이 업그레이드를 도울 수 있는 프로세스를 구축하고 적절한 관측 기능을 갖추어 각 팀이 너무 뒤처져 있지 않도록 확인하는 것이 의미가 있을지도 모릅니다. 우리 플랫폼 팀이 처리해야 할 다른 책임을 고려하면, 그렇게 하는 것이 관리하기 쉬워 보입니다.\n\n# 가드레일\n\n이전 게시물에서, 저는 Databricks 사용을 관리하는 데 중요한 컴퓨팅 정책의 중요성에 대해 썼습니다. 프로비던스는 Unity 마이그레이션 동안 훌륭한 일련의 가드레일을 소개했습니다:\n\n<div class=\"content-ad\"></div>\n\n- 클러스터 유형 제한\n- 대화식 클러스터 예약 불가\n- 운영 중 대화식 클러스터 미사용\n- 노드 수와 인스턴스 유형에 대한 제어 설정\n\n대화식 클러스터 예약을 허용하지 않는 것은 중요한 사항입니다. 모든 워크플로우는 이상적인 클러스터 구성을 위해 작업 클러스터를 사용해야 하며 대화식 클러스터처럼 일반적으로 보이는 유휴 기간을 피해야 합니다. (또한 작업 컴퓨팅이 대화식 컴퓨팅보다 저렴하다는 점도 말이죠).\n\n운영 중 대화식 클러스터를 사용하지 않는 것은 이전에 생각해보지 못한 부분이었는데, 합리적으로 보입니다. 하위 환경에서 생산 데이터를 읽기 위한 필요한 액세스가 있고 비상 시나리오를 위한 경우를 제외하고 별도의 운영 대화식 클러스터가 정말 필요할까요? 아마도 그렇지 않기 때문에 처음부터 허용하지 않는 것이 좋습니다.\n\n# 차원 모델링\n\n<div class=\"content-ad\"></div>\n\n그래, 당신은 아마 데이터 및 AI 서밋에서 차원 모델링 세션을 듣기 위해 참석하지 않았을 것 같지만, 컬럼형 데이터베이스를 적절히 설계할 때 장점이 있어요. 사실 테이블, 차원 테이블 및 다양한 유형의 천천히 변화하는 차원들 사이의 차이를 알아두면 정말 유용할 거예요. (여기서 네 가지만 있다고 생각했는데...)\n\n이 대화는 성능 대 개인 요구사항에 대한 더 나은 지원의 상충관계인 큰 하나의 테이블(OBT) 주변의 전형적인 사례를 언급하기에 잘 했어요. 기존 데이터 업데이트, 거버넌스 및 전반적인 중복성에 관해서는 OBT 접근 방식에는 단점이 있지만, 가끔 고려할 가치가 있어요.\n\n# 결론\n\n아마도 앞으로 몇 일 동안 더 많은 세션을 확인하게 될 것 같아요, 여기서 매우 훌륭한 시작이었어요. 다시 한 번 Databricks에 이러한 멋진 행사를 주최해준 것에 감사드려요.","ogImage":{"url":"/assets/img/2024-06-19-DataAISummitTakeawaysPartII_0.png"},"coverImage":"/assets/img/2024-06-19-DataAISummitTakeawaysPartII_0.png","tag":["Tech"],"readingTime":2},{"title":"특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 ","description":"","date":"2024-06-19 09:48","slug":"2024-06-19-DataEngineeringpipelineleveragingAirflowKafka","content":"\n\n# 소개\n\n이 문서에서는 Apache Airflow 및 Kafka(오픈 소스)를 활용하여 실시간 날씨 업데이트를 읽고 Twitter의 이벤트 스트림을 분석하여 대중의 반응을 이해하는 데이터 엔지니어링 파이프라인 아키텍처를 솔루션 디자인하는 방법에 대한 기본적인 개요를 제공합니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png)\n\n# ETL 기본 원칙\n\n<div class=\"content-ad\"></div>\n\nETL은 데이터 웨어하우스 또는 데이터 마트와 같은 분석 환경을 위해 데이터 획득 및 준비를 자동화하는 데이터 파이프라인 엔지니어링 방법론인 추출, 변환, 로드를 의미합니다. 이는 다양한 소스에서 데이터를 수집하고 표준 형식으로 편집한 다음 시각화, 탐색 및 모델링을 위한 새로운 환경으로 로드하는 것을 포함하며, 자동화 및 의사 결정을 지원합니다.\n\n# ELT 기본\n\nELT 프로세스는 추출, 로드 및 변환을 나타내며, 단곅하계의 단계 순서로 인한 독특한 차이로 인해 ETL과 다릅니다. ELT에서는 데이터가 데이터 레이크와 같은 목적지 환경으로 원본 형식 그대로 직접 로드됩니다. 이를 통해 목적지 플랫폼 내에서 필요에 따라 변환을 수행하고 동적으로 사용자 주도적 변경을 가능하게 합니다.\n\n# ETL과 ELT 비교\n\n<div class=\"content-ad\"></div>\n\nETL과 ELT의 차이점: ETL 파이프라인에서는 변환 작업이 목적지에 도달하기 전에 데이터 파이프라인 내에서 발생하는 반면, ELT는 변환 작업을 분리하여 목적지 환경에서 필요한 대로 수행할 수 있습니다. ETL은 엄격하고 목적이 명확하지만, ELT는 유연하며 빅 데이터 처리에 대한 셀프 서비스 분석을 제공합니다.\n\n# 데이터 수집 기술\n\n다양한 데이터 수집 기술에는 다음이 포함됩니다:\n\n- 완전 수집 대 부분 수집\n\n<div class=\"content-ad\"></div>\n\n- 전체 로딩: 데이터베이스에 초기 히스토리를 로드합니다. 추적 데이터는 새 창고에서 시작됩니다.\n- 점진적 로딩: 새 데이터를 삽입하거나 이미 로드된 데이터를 업데이트합니다. 거래 히스토리를 누적하는 데 사용됩니다. 데이터 양과 속도에 따라 일괄 또는 스트림 로드될 수 있습니다.\n\n정기 로딩 vs. 요청 로딩:\n\n- 정기 로딩: 매일 거래를 데이터베이스에 주기적으로 로드하며, 스크립트 작업에 의해 자동화됩니다.\n- 요청 로딩: 소스 데이터가 지정된 크기에 도달하거나 움직임, 소리 또는 임의의 변경 이벤트와 같은 다양한 이벤트에 의해 트리거됩니다.\n\n일괄 처리 vs. 스트림 로딩:\n\n<div class=\"content-ad\"></div>\n\n- 배치 로딩: 시간별로 정의된 단위로 데이터를 로드하며 일반적으로 몇 시간에서 며칠 동안 누적됩니다.\n- 스트림 로딩: 데이터가 제공되는 즉시 실시간으로 로드됩니다.\n- 마이크로 배치 로딩: 즉시 처리를 위한 최근 데이터에 액세스합니다.\n\n푸시 대 수신 데이터 로딩:\n\n- 수신 방법: 클라이언트가 서버로부터 데이터를 요청합니다(예: RSS 피드, 이메일).\n- 푸시 방법: 클라이언트가 서버 서비스에 구독하여 데이터를 실시간으로 전달 받습니다(예: 푸시 알림, 즉각 메시징 서비스).\n\n# 데이터 파이프라인이란 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n데이터 파이프라인은 데이터의 이동 또는 수정에 특히 관련이 있어요. 이러한 파이프라인은 데이터를 한 곳이나 형식에서 다른 곳이나 형식으로 운송하는 것을 목표로 하며, 데이터를 추출하고 최종적으로 적재하기 위해 선택적 변환 단계를 통해 안내하는 시스템을 구성합니다.\n\n파이프라인을 통해 흐르는 데이터를 시각화하는 것은 데이터 패킷으로 표현할 수 있으며, 이는 데이터의 단위를 넓게 이야기합니다. 이러한 패킷은 단일 레코드나 이벤트에서 대량 데이터 수집물까지 다양할 수 있어요. 이 맥락에서 데이터 패킷은 파이프라인으로 흡수되기 위해 대기열에 정리되며, 데이터 파이프라인의 길이는 단일 패킷이 횡단하는 데 걸리는 시간을 의미합니다. 패킷 간의 화살표는 처리량 지연이나 연속 패킷 도착 사이의 시간을 나타냅니다.\n\n데이터 파이프라인 주요 성능 지표\n\n- 지연 시간: 데이터 패킷이 파이프라인을 통과하는 총 시간을 의미합니다. 지연 시간은 파이프라인 내 각 처리 단계에서 소요된 개별 시간의 합으로, 파이프라인 내 가장 느린 프로세스에 의해 제한됩니다. 예를 들어, 웹 페이지의 로딩 시간은 서버 속도에 따라 결정되며 인터넷 서비스 속도와 관계없이 서버 속도에 의해 제어됩니다.\n- 처리량: 이는 시간 단위당 파이프라인을 통해 처리될 수 있는 데이터 양을 의미합니다. 처리량을 증가시키는 것은 시간 단위당 더 많은 패킷을 처리하고 큰 상자를 차례차례 통과시키는 우리 친구 사슬 예시와 유사합니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 파이프라인 응용:\n\n- 간단한 복사 파이프라인: 파일 백업과 같이 데이터를 한 위치에서 다른 위치로 복사하는 작업을 포함합니다.\n- 데이터 레이크 통합: 분산된 래 데이터 소스를 데이터 레이크에 통합하는 작업입니다.\n- 거래 기록 이동: 거래 기록을 데이터 웨어하우스로 전송하는 작업입니다.\n- IoT 데이터 스트리밍: IoT 장치에서 데이터를 스트리밍하여 대시보드나 경보 시스템에서 정보를 제공하는 것을 말합니다.\n- 기계 학습용 데이터 준비: 기계 학습의 개발이나 제품화를 위해 래 데이터를 준비하는 작업입니다.\n- 메시지 보내기 및 받기: 이메일, SMS 또는 온라인 비디오 회의와 같은 애플리케이션을 포함합니다.\n\n# 주요 데이터 파이프라인 프로세스\n\n데이터 파이프라인 프로세스는 일반적으로 구조화된 일련의 단계를 따릅니다:\n\n<div class=\"content-ad\"></div>\n\n- 추출: 하나 이상의 소스에서 데이터를 검색하는 과정입니다.\n- 투입: 추출된 데이터는 후속 처리를 위해 파이프라인에 투입됩니다.\n- 변환: 파이프라인 내의 선택적인 단계에서 데이터를 변환할 수 있습니다.\n- 로딩: 최종 단계는 변환된 데이터를 대상 시설로 로드합니다.\n- 스케줄링/트리거링: 작업을 필요에 따라 예약하거나 트리거하는 메커니즘입니다.\n- 모니터링: 전체 워크플로우가 효율적으로 작동하도록 모니터링됩니다.\n- 유지보수 및 최적화: 원활한 파이프라인 운영을 보장하기 위해 정기적인 유지보수 및 최적화 작업이 수행됩니다.\n\n# Apache Airflow\n\n- Python 기반의 오픈 소스 \"구성과 코드\" 플랫폼입니다. AirBNB에서 오픈 소스로 공개되었습니다.\n- 데이터 파이프라인 워크플로우를 작성, 예약 및 모니터링할 수 있습니다.\n- 확장 가능하며 병렬 컴퓨팅 노드를 지원하며 주요 클라우드 플랫폼과 통합됩니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_1.png)\n\n<div class=\"content-ad\"></div>\n\n- 스케줄러: 예약된 워크플로우를 트리거합니다.\n- 실행기: 작업을 Worker에 할당하여 실행합니다.\n- 웹 서버: DAG 검사, 트리거 및 디버깅을 위한 대화형 UI를 호스팅합니다.\n- DAG 디렉토리: 스케줄러, 실행기 및 Worker가 액세스할 수 있는 DAG 파일을 저장합니다.\n- 메타데이터 데이터베이스: 각 DAG 및 해당 작업의 상태를 유지합니다.\n\nDAG 및 작업 라이프사이클\n\nDAG(유향 비순환 그래프)는 작업 간의 종속성과 실행 순서를 지정합니다. 'DAG'는 순환이나 사이클이 없는 관계를 나타내는 특정 유형의 그래프로, 노드와 간선으로 구성되며 방향성을 가진 간선이 노드 간의 흐름을 보여줍니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_2.png)\n\n<div class=\"content-ad\"></div>\n\n작업 상태:\n\n- 상태 없음: 실행을 위해 대기 중인 작업.\n- 스케줄됨: 의존성에 따라 실행이 예약된 작업.\n- 제거됨: 실행이 시작된 이후에 사라진 작업.\n- 상류 작업 실패: 상류 작업에서 실패 발생.\n- 대기 중: 워커 가용성을 기다리는 작업.\n- 실행 중: 워커에 의해 실행 중인 작업.\n- 성공: 오류 없이 작업이 완료된 상태.\n- 실패: 실행 중에 오류가 발생한 작업.\n- 재시도 예정: 남은 재시도 횟수가 남아 있는 실패한 작업으로, 다시 예약됨.\n- 이상적인 작업 흐름: '상태 없음'에서 '스케줄됨'으로, '대기 중'으로, '실행 중'으로 이어져 '성공'으로 마무리됨.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_3.png)\n\nAirflow DAG 스크립트의 논리 블록\n\n<div class=\"content-ad\"></div>\n\n- 라이브러리 가져오기: 필요한 Python 라이브러리 가져오기.\n- DAG 인수: DAG에 대한 기본 인수(시작 날짜와 같은 것) 정의.\n- DAG 정의: 특정 속성을 사용하여 DAG 인스턴스화.\n- 작업 정의: DAG 내부의 개별 작업(노드) 정의.\n- 작업 파이프라인: 작업 간의 의존성을 지정하여 작업 간의 흐름을 구축.\n\n이러한 논리적 블록이 포함된 Python 스크립트 예제를 살펴보세요.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_4.png)\n\n# Kafka를 활용한 스트리밍 파이프라인 구축\n\n<div class=\"content-ad\"></div>\n\n이벤트는 이벤트 스트리밍의 맥락에서 엔티티의 관찰 가능한 상태 업데이트를 설명하는 데이터를 의미합니다. 예시로는 자동차의 GPS 좌표, 방 온도, 또는 응용 프로그램의 RAM 사용량 등이 있습니다.\n\n이벤트는 다양한 형식으로 제공됩니다:\n\n- 텍스트, 숫자 또는 날짜와 같은 원시 유형\n- 값이 원시 또는 복합 유형인 키-값 쌍 형식(e.g., JSON, XML)\n- 타임스탬프가 포함된 시간 감도를 위한 키-값 형식\n\n한 소스에서 한 대상으로: 이벤트 스트리밍은 소스(센서, 데이터베이스, 응용 프로그램)가 실시간 이벤트를 지속적으로 생성하고 이를 대상지(파일 시스템, 데이터베이스, 응용 프로그램)로 전달하는 것을 의미합니다. 이 과정은 이벤트 스트리밍이라고 불립니다.\n\n<div class=\"content-ad\"></div>\n\n많은 출처에서 많은 대상으로: 다양한 통신 프로토콜(FTP, HTTP, JDBC, SCP)을 사용하는 여러 분산 이벤트 소스 및 대상을 관리하는 것은 도전이 될 수 있습니다. 이벤트 스트림 플랫폼(ESP)은 미들웨어로 작용하여 다양한 이벤트 기반 ETL의 처리를 간단하게 만듭니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_5.png)\n\nESP 구성 요소\n\n- 이벤트 브로커: 이벤트를 수신하고 소비하는 핵심 구성 요소입니다.\n- 이벤트 저장소: 받은 이벤트를 저장하여 대상이 비동기적으로 검색할 수 있도록 합니다.\n- 분석 및 쿼리 엔진: 저장된 이벤트를 쿼리하고 분석합니다.\n\n<div class=\"content-ad\"></div>\n\n이벤트 브로커는 중요한 컴포넌트입니다. 이는 다음과 같은 구성 요소들을 포함합니다:\n\n- Ingester: 다양한 소스에서 이벤트를 효율적으로 수신합니다.\n- Processor: 직렬화, 역직렬화, 압축, 압축 해제, 암호화 및 복호화와 같은 작업을 수행합니다.\n- Consumption: 저장소에서 이벤트를 검색하고 구독된 대상에게 분배합니다.\n\n인기 있는 이벤트 처리 시스템 솔루션:\n\n- Apache Kafka\n- Amazon Kinesis\n- Apache Flink\n- Apache Spark\n- Apache Storm\n\n<div class=\"content-ad\"></div>\n\n아파치 카프카: 독특한 기능과 광범위한 응용 시나리오를 갖춘 가장 인기 있는 ESP 중 하나입니다. 카프카는 분산 클라이언트-서버 모델을 따릅니다.\n\n- 서버 측: 효율적인 협업을 위해 ZooKeeper가 관리하는 여러 브로커로 구성됩니다.\n- 네트워크 통신: 클라이언트와 서버 간의 데이터 교환에 TCP를 활용합니다.\n- 클라이언트 측: CLI, 자바, 스칼라, REST API 및 타사 옵션을 포함한 다양한 클라이언트를 제공합니다.\n\n카프카의 인기 이유는?\n\n- 확장성: 데이터를 여러 브로커에 분산하여 확장성과 고 처리량을 보장합니다.\n- 높은 신뢰성: 안정성을 위해 여러 파티션과 복제를 사용합니다.\n- 영구적인 지속성: 이벤트를 영구적으로 저장하여 소비자의 편의에 맞게 사용할 수 있습니다.\n- 오픈 소스: 특정 요구 사항에 맞춰 사용자 정의가 가능하여 무료로 제공됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 카프카 아키텍처\n\n카프카 클러스터는 여러 브로커로 구성되어 있으며, 각 브로커는 이벤트를 수신, 저장, 처리 및 배포하는 역할을 합니다. ZooKeeper에 의해 조율되는 이러한 브로커들은 로그 또는 트랜잭션과 같은 특정 이벤트 유형을 저장하는 데이터베이스와 유사한 주제를 관리합니다.\n\n파티셔닝과 복제: 카프카는 장애 허용성과 병렬 이벤트 처리를 위해 파티셔닝과 복제를 사용합니다. 일부 브로커가 실패하더라도, 카프카는 주제 파티션을 운영 중인 브로커에 분산시킴으로써 지속성을 보장합니다.\n\nKafka CLI를 사용한 주제 관리: 카프카 명령줄 인터페이스는 카프카 클러스터 내에서 주제를 생성, 나열, 설명 및 삭제하는 기능을 제공합니다. 명령에는 정의된 파티션 및 복제로 주제를 생성하고 주제 및 구성에 대한 자세한 정보를 얻는 등의 작업이 포함됩니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_6.png)\n\n카프카 프로듀서\n\n카프카 프로듀서는 이벤트를 토픽 파티션에 발행하는 클라이언트 앱입니다. 이벤트는 선택적 파티셔닝을 위해 키와 연결될 수 있습니다. 프로듀서 CLI를 사용하면 프로듀서를 관리하고 지정된 토픽에 이벤트를 키와 함께 발행할 수 있습니다.\n\n컨슈머로 이벤트 읽기: 컨슈머는 토픽에 가입하고 저장된 이벤트를 읽어 순차적으로 오프셋을 유지합니다. 오프셋을 재설정함으로써 컨슈머는 처음부터 이벤트를 다시 재생할 수 있습니다. 카프카 컨슈머와 프로듀서는 독립적으로 작동하여 동기화 없이 이벤트를 저장하고 소비할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n끝까지 이어지는 이벤트: 날씨 파이프라인\n\n단계 1: 이벤트 소스 정의\n\n극단적인 날씨에 대한 대중의 반응을 이해하기 위해 날씨와 트위터 이벤트 스트림을 분석하고 싶다고 상상해보세요. 두 가지 주요 이벤트 소스를 활용할 것입니다:\n\n- IBM Weather API: JSON 형식의 실시간 및 예보 날씨 데이터를 제공합니다.\n- Twitter API: JSON 형식의 실시간 트윗 및 언급을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n단계 2: Kafka 토픽 구성\n\nKafka 클러스터에서 날씨 및 트위터 이벤트용 전용 토픽을 생성하여 데이터 흐름을 효율적으로 처리하기 위해 적절한 파티션 및 복제를 보장하세요.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_7.png)\n\n단계 3: 프로듀서 개발\n\n<div class=\"content-ad\"></div>\n\n각 이벤트 소스에 대해 특정한 프로듀서를 개발하세요. 이들은 JSON 데이터를 바이트로 직렬화하고 해당 Kafka 토픽으로 게시할 것입니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_8.png)\n\n단계 4: 컨슈머 구현\n\n날씨 및 Twitter 이벤트용 전용 컨슈머를 생성하세요. 이 컨슈머들은 Kafka 토픽에서 바이트를 역직렬화하여 JSON 데이터로 변환한 후 처리할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_9.png\" />\n\n단계 5: Persistence를 위한 DB Writer 통합\n\n이벤트 데이터를 관계형 데이터베이스에 쓰고 싶다면 DB writer를 사용하십시오. 이 구성 요소는 컨슈머에서 JSON 파일을 구문 분석하고 해당 데이터베이스 레코드를 생성합니다.\n\n```js\n#db writer EXAMPLE\nimport json\nimport sqlite3\n\ndef write_to_database(record):\n    connection = sqlite3.connect(\"event_database.db\")\n    cursor = connection.cursor()\n    cursor.execute(\"INSERT INTO events VALUES (?)\", (json.dumps(record),))\n    connection.commit()\n    connection.close()\n\nrecord = {\"event_type\": \"weather\", \"data\": {\"temperature\": 25, \"location\": \"NYC\"}}\nwrite_to_database(record)\n```\n\n<div class=\"content-ad\"></div>\n\n6단계: SQL을 사용한 데이터베이스 상호작용\n\n데이터베이스에 레코드를 쓰기 위해 SQL 삽입문을 사용하세요. 이 단계는 카프카 토픽에서 데이터를 영구 저장소 솔루션으로 전환하는 과정을 완료합니다.\n\n```js\n-- SQL 삽입 예시\nINSERT INTO events VALUES ('{\"event_type\": \"weather\", \"data\": {\"temperature\": 25, \"location\": \"NYC\"}');\n```\n\n7단계: 시각화 및 분석\n\n<div class=\"content-ad\"></div>\n\n마침내, 수집하고 저장된 이벤트 데이터로부터 통찰력 있는 시각화와 분석을 위해 데이터베이스 레코드를 쿼리하세요. 수집된 이벤트 데이터로부터 가치 있는 통찰력을 얻기 위해 대시보드를 사용하는 것이 가장 좋습니다.\n\n이 end-to-end 파이프라인은 다양한 구성 요소의 원활한 통합을 보여주며, 이벤트 스트림을 관리하는 Kafka의 유연성과 강력함을 강조합니다.\n\n참고: 본 블로그 게시물에서 제공된 메모 및 정보는 \"ETL 및 쉘, Airflow 및 Kafka를 사용한 데이터 파이프라인\" 과정 중에 편집되었으며 개인적인 용도를 위한 기본 개요를 제공하기 위한 것입니다.","ogImage":{"url":"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png"},"coverImage":"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png","tag":["Tech"],"readingTime":10}],"page":"79","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}