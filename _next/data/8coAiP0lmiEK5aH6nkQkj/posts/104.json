{"pageProps":{"posts":[{"title":"Hugging Face의 Flan-T5 모델을 사용하여 고객 인사이트를 분석해 봅시다","description":"","date":"2024-05-27 15:16","slug":"2024-05-27-AnalyseCustomerInsightswithHuggingFacesFlan-T5Model","content":"\n\n현재의 데이터 중심 세계에서는 기업들이 항상 고객을 이해하고 관계를 발전시키는 방법을 찾고 있습니다. 이 임무에서 가장 흥미로운 도구 중 하나가 Hugging Face의 Flan-T5 모델입니다. 이 고급 자연어 처리(NLP) 모델은 또 다른 기술용어가 아니라, 기업이 데이터와 고객과 상호작용하는 방식을 혁신하는 중요한 도구입니다. Flan-T5가 고객 인사이트를 혁신하는 방법을 알아보기 위해 실제 사례를 살펴보겠습니다.\n\n# 과제: 고객 피드백 이해하기\n\n우리 팀은 여러 채널(설문, 소셜 미디어, 이메일, 라이브 챗)에서 고객 피드백을 수집했습니다. 이 피드백은 많은 인사이트가 담겨있지만 흩어져 있고 구조화되어 있지 않습니다. 수천 개의 코멘트를 살펴 트렌드와 실행 가능한 항목을 식별하는 작업은 허구속의바늘을 찾는 것과 같습니다.\n\n# Flan-T5 등장\n\n<div class=\"content-ad\"></div>\n\n허깅페이스의 Flan-T5 모델이 등장하는 곳입니다. Flan-T5는 \"Fine-tuned Language-Agnostic Network, Text-To-Text Transfer Transformer\"의 약자로, 다언어 및 다양한 맥락에서 인간과 유사한 텍스트를 이해하고 생성할 수 있는 고급 NLP 모델입니다. 이 모델의 강점은 특정 작업에 대해 세밀하게 조정될 수 있는 능력에 있어서, 이는 고객 피드백을 구문 분석하고 분석하는 데 이상적인 후보자로 만들어 줍니다.\n\n# Flan-T5 구조\n\nFlan-T5는 Transformer 아키텍처를 기반으로 하며, 구체적으로 텍스트-텍스트 프레임워크를 사용합니다. 이는 모든 NLP 작업 - 번역, 요약 또는 질의 응답 - 이 텍스트 입력을 텍스트 출력 문제로 캐스팅된다는 의미입니다. 다음은 아키텍처의 간소화된 개요입니다:\n\n- 인코더: 입력 텍스트를 처리하고 연속 표현의 집합으로 변환합니다.\n- 디코더: 이러한 연속 표현을 취하여 출력 텍스트를 생성합니다.\n\n양방향 인코더는 양방향에서 컨텍스트를 포착하며, 이는 고객 피드백의 세부 정보를 이해하는 데 효과적입니다.\n\n<div class=\"content-ad\"></div>\n\n# 해결책 구현:\n\n단계 1: 데이터 수집 및 전처리\n먼저, 모든 고객 피드백을 중앙 데이터베이스에 수집하고 데이터 전처리를 통해 노이즈를 제거합니다.\n\n단계 2: 플란-T5 파인튜닝\nHugging Face의 Transformers 라이브러리를 사용하여 플란-T5 모델을 파인튜닝합니다.\n\n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n\n# 토크나이저 및 모델 로드\ntokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\nmodel = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n\n# 데이터 토큰화\ndef preprocess_function(examples):\n    return tokenizer(examples['feedback'], truncation=True, padding='max_length', max_length=512)\n\ntrain_data_tokenized = train_data.apply(preprocess_function, axis=1)\ntest_data_tokenized = test_data.apply(preprocess_function, axis=1)\n\n# 훈련 인자 준비\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01\n)\n\n# 트레이너 인스턴스 생성\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data_tokenized,\n    eval_dataset=test_data_tokenized\n)\n\n# 모델 훈련\ntrainer.train()\n```\n\n<div class=\"content-ad\"></div>\n\n**단계 3:** 감성 분석 및 주제 모델링\n미세 조정된 모델을 배포하여 감성 분석을 수행하고 주요 주제를 식별합니다.\n\n# 현실 세계의 영향\n\nFlan-T5를 구현한 후, 전자상거래 회사는 변화적인 결과를 보았습니다:\n\n- 향상된 고객 이해: 모델은 피드백을 의미 있는 통찰로 정확하게 분류했습니다. 예를 들어, 부정적인 피드백의 상당 부분이 지연된 배송과 관련이 있음을 강조하여 회사가 물류 문제에 대응하도록 유도했습니다.\n- 선제적 고객 서비스: 실시간으로 트렌드를 식별함으로써 고객 서비스팀이 일반적인 문제에 선제적으로 대응하여 전반적인 고객 만족도를 향상시켰습니다.\n- 데이터 기반의 결정: 마케팅 및 제품 개발 팀은 이러한 통찰을 사용하여 캠페인을 맞춤화하고 제품 기능을 개선하여 고객 참여 및 충성도를 증대시켰습니다.\n\n<div class=\"content-ad\"></div>\n\n# Flan-T5가 돋보이는 이유\n\nFlan-T5의 매력은 그의 적응성에 있습니다. 이는 단순히 감성 분석이나 주제 모델링에 한정되지 않습니다. 비즈니스는 다음과 같은 다양한 응용 프로그램을 위해 그 기능을 활용할 수 있습니다:\n\n- 자동화된 고객 지원: 고객 쿼리를 이해하고 높은 정확도로 응답하는 챗봇 구현.\n- 콘텐츠 생성: 다양한 고객 세그먼트와 공감대를 형성하는 맞춤 마케팅 콘텐츠 작성.\n- 예측 분석: 고객의 행동과 선호도를 예측하여 전략적 결정을 이끌어내는 것.\n\n# Flan-T5 시작하기\n\n<div class=\"content-ad\"></div>\n\n플란-T5를 채택하는 것은 Hugging Face의 사용자 친화적인 도구와 포괄적인 문서 덕분에 생각보다 쉽습니다. 아래는 빠른 로드맵입니다:\n\n- Hugging Face의 모델 허브 탐색: 플란-T5를 찾아서 사전 훈련된 모델을 실험해보세요.\n- 트랜스포머 라이브러리 활용: 특정 데이터셋에서 모델을 세밀하게 조정하기 위해 트랜스포머 라이브러리를 활용하세요.\n- 배포 및 모니터링: 모델을 기존 시스템에 통합하고 성능을 지속적으로 모니터링하며 개선하세요.\n\n# 결론\n\nHugging Face의 플란-T5 모델의 능력을 활용하면 기업은 고객에 대한 심층적인 이해를 얻을 수 있습니다. 비구조적인 피드백을 실행 가능한 통찰로 변환함으로써 기업은 고객 경험을 향상시키고 참여를 촉진하며 궁극적으로 수익을 증대시킬 수 있습니다. 고객 기대가 지속적으로 변화하는 세상에서 플란-T5와 같은 최첨단 NLP 모델로 앞서가는 것은 선택이 아니라 필수입니다. 고객 인사이트를 혁신하시 ready하세요? 미래는 플란-T5입니다.","ogImage":{"url":"/assets/img/2024-05-27-AnalyseCustomerInsightswithHuggingFacesFlan-T5Model_0.png"},"coverImage":"/assets/img/2024-05-27-AnalyseCustomerInsightswithHuggingFacesFlan-T5Model_0.png","tag":["Tech"],"readingTime":4},{"title":"RAG 자동화를 위한 건축 설계도 Vertex AI 검색을 활용한 고급 문서이해","description":"","date":"2024-05-27 15:10","slug":"2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch","content":"\n<img src=\"/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_0.png\" />\n\n안녕하세요!\n\n생성적 AI는 개발자와 기업에 수많은 기회를 제공하여 비즈니스 프로세스를 혁신하고 고객 경험을 변화시키며 새로운 수익 방식을 발견하게 도와줍니다. 그러나 이 잠재력을 완전히 실현하려면, 건축가와 IT 리더들은 AI 모델, 응용 프로그램 및 에이전트를 신속하게 실험하고 반복할 수 있는 동시에 비용 관리, 거버넌스 및 확장성을 고려해야 하는 복잡한 환경을 탐험해야 합니다.\n\n최근 개최된 Next’24 구글 클라우드 이벤트에서 저희는 강력한 Vertex AI Search 및 Conversation 제품과 다양한 고급 개발자 도구를 통합한 혁명적인 솔루션인 Vertex AI Agent Builder를 공개했습니다. 이 포괄적인 제공은 개발자들이 복잡한 작업과 문의를 원활하게 처리할 수 있는 정교한 AI 기반 에이전트를 만들고 배포할 수 있게 도와주어 다양한 영역에서 혁신과 효율성을 촉진합니다.\n\n# Vertex AI Search 이해하기\n\n<div class=\"content-ad\"></div>\n\nVertex AI Search은 구글 클라우드 내에서 제공되는 포괄적인 플랫폼으로, 조직이 직원 및 고객을 위한 맞춤형 검색 솔루션을 만들 수 있도록 설계되었습니다. 이 플랫폼은 웹사이트, 구조화된 데이터(예: BigQuery 테이블, JSON 라인) 및 비정형 데이터(예: PDF, HTML, 텍스트)를 포함한 다양한 데이터 원본에 대해 구글 검색과 유사한 경험을 제공합니다.\n\n이전 블로그 포스트에서는 Vertex AI Search를 사용하여 공개적으로 색인된 웹페이지에서 대상 웹페이지를 수집하는 방법에 대해 논의했습니다. 이 방법은 이러한 웹페이지의 사전 구글 색인을 활용합니다. 우리는 PDF 문서를 채굴하기 위한 지식 발견 파이프라인을 구축하는 데 이 방법을 사용했습니다.\n\n우리 이전 포스트의 연장선으로 이 기사를 고려해 주세요. 여기서는 추출된 PDF 문서를 처리하는 방법에 대해 다룹니다. 이러한 문서는 이미 준비되어 있을 수 있어 바로 사용할 수 있습니다. 또는 당신의 기업에게 기밀인 소유 문서일 수도 있습니다. 여기서는 이러한 문서의 데이터를 수용하고 처리하며, 복잡한 쿼리에 응답할 수 있는 시스템을 구축하는 방법을 탐색할 것입니다. 예를 들어 사실 정보를 검색하거나 분기 보고서의 재무 표에서 숫자를 인출하는 것과 같은 복잡한 쿼리에 대답할 수 있는 시스템을 구축합니다.\n\n주로, Vertex AI Search는 GCP의 완전 관리형 플랫폼으로, 구글 검색 품질 기능을 기업 데이터에 통합하여 두 가지 주요 이점을 제공합니다:\n\n<div class=\"content-ad\"></div>\n\n- 향상된 검색 경험: 기존의 키워드 기반 검색을 모던한 대화형 경험으로 변환시켜 주는 기능입니다. 구글의 혁신적인 생성 검색과 비슷한 방식으로 작동합니다. 이 기능은 내부 및 고객 상대 애플리케이션의 효율성을 크게 향상시킵니다.\n- 강화된 생성 AI 애플리케이션: 생성 AI 애플리케이션 내 답변 생성을 지원합니다. 기업 데이터를 기반으로 하는 생성 AI는 Vertex AI 검색을 통해 실제 비즈니스 사용 사례에 중요한 정확성, 신뢰성 및 적합성을 보장합니다. 이는 검색 기능의 통합을 간편하게 하는 준비된 RAG 시스템 역할을 하며, 검색 기능을 획기적으로 향상해 줍니다.\n\n맞춤형 RAG 파이프라인 구축은 복잡할 수 있습니다. Vertex AI 검색은 준비된 솔루션을 제공하여 이 프로세스를 간단하게 만들어 줍니다. 데이터 추출 및 변환, 정보 검색 및 요약까지 검색 및 발견 프로세스의 모든 측면을 간소화하여 클릭 몇 번으로 줄여 줍니다. 결과적으로, 일반 검색 엔진으로 Vertex AI 검색을 사용하여 강력한 RAG 애플리케이션을 신속하게 개발할 수 있습니다.\n\n준비된 솔루션은 상당한 편의성을 제공하지만, Vertex AI 검색은 개발자에게 자세한 제어도 허용합니다. 플랫폼의 유연성을 활용하여 RAG 파이프라인 각 단계를 사용자의 필요에 맞게 맞춤화할 수 있습니다. 이 하이브리드 접근 방식을 통해 사전 구축된 구성 요소와 맞춤형 기능을 이상적으로 조화시킬 수 있어, 응용 프로그램이 특정 사용 사례와 완벽하게 일치하도록 보장할 수 있습니다.\n\nVertex AI 검색은 다양한 API 세트를 통해 이를 실현합니다. 이러한 API를 통해 Vertex AI 검색의 RAG 시스템의 기본 구성 요소를 노출시켜 개발자가 맞춤형 사용 사례에 대응하거나 자세한 제어를 필요로 하는 고객을 지원할 수 있습니다. 이는 Document AI Layout Parser API, Ranking API, Grounded Generation API, Check Grounding API 등을 포함합니다.\n\n<div class=\"content-ad\"></div>\n\n시작해 봅시다! 먼저 데이터셋을 이해하는 데 집중할 차례입니다. 이는 저희 RAG 파이프라인의 기반이 되는 것이죠. 그런 다음에는 이 데이터를 Vertex AI Search에 효과적으로 수집하여 신속하게 검색할 수 있도록 구성하는 방법을 배우게 될 거에요. Vertex AI Search 내에서 색인 전략에 중점을 두어야 하는데, 이는 인공지능이 필요할 때 가장 관련성 높은 정보에 접근할 수 있도록 하는 데 중요합니다. 우리는 색인된 문서를 쿼리하는 기술에 대해 자세히 살펴보고, 다양한 파이프라인 접근 방식을 실험할 겁니다. 마지막으로 결과물을 수집하고, 검색 정확도 및 생성된 답변의 품질을 평가하는 방법을 배우게 될 거에요. 이 여정을 통해 여러분은 RAG와 Vertex AI Search의 힘을 활용하여 더 스마트하고 정보에 기반한 AI 파이프라인을 구축하는 데 필요한 지식을 습득하게 될 거에요.\n\n# 데이터셋\n\n저희 실험에 사용할 데이터셋은 Alphabet, Amazon, Microsoft 세 기술 기업의 분기 보고서로 구성되어 있어요. 2021년 1분기부터 2023년 4분기까지의 기간 동안, 3년 동안의 36개 문서(각 기업당 12개)로 이루어진 데이터셋이에요.\n\n실험을 용이하게 하기 위해, 이 문서들에서 100개의 질문-답변 쌍을 유도했어요. 각 쌍은 한 문서에 직접 연결되어 있어, 단일 통로 질문-답변 시나리오를 구축합니다. 세심하게 만들어진 질문과 답변은 테이블과 복잡한 단락에서 정보를 추출하는 데 초점을 맞추었으며, RAG 시스템에 상당한 도전을 제공합니다. 이 100개의 질문-답변 쌍은 저희가 다룰 다양한 RAG 파이프라인 설계의 성능을 평가하기 위한 참 값 역할을 합니다. PDF 금융 분기 보고서 데이터셋은 여기에서 찾을 수 있으며, 질문-답변 쌍 데이터셋은 ground_truth.csv라는 CSV 파일에 있습니다. 이 파일에는 다음 메타 정보가 포함되어 있습니다 - i)매핑 문서, ii) 기업 이름, iii) 시기. 이 메타 정보는 CSV 파일의 document 열 하나에 포함되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\nAlphabet의 2020년 1분기 보고서에서 주로 영업 소득과 마진에 관련된 재무 결과를 요약한 샘플 테이블이 아래에 표시됩니다.\n\n![테이블](/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_1.png)\n\n우리의 실제 CSV에서의 샘플 질문과 위 테이블을 통해 파생된 예상 답변이 있습니다.\n\n```js\nGoogle의 2021년 3월 말 영업 소득은 얼마이며 (십억으로), 이전 연도 동기 대비 어떻게 비교되었습니까?\n```\n\n<div class=\"content-ad\"></div>\n\n```js\nGoogle의 영업 이익은 2021년 제1사분기에 164.37억 달러였습니다. 이는 2020년 제1사분기의 79.77억 달러에서 증가한 금액입니다.\n```\n\n위 답변을 작성하기 위해서는 먼저 쿼리에서 구체적인 세부 정보를 추론하여 올바른 문서를 검색해야 합니다. 이에는 올바른 페이지로 이동하고 적절한 테이블을 참조하며 열 정보를 구문 분석하는 것이 포함됩니다. 그런 다음 필드를 열 제목에 매핑하고 올바른 정보를 찾습니다. 마지막으로 이러한 수집된 정보를 일관된 답변으로 통합합니다.\n\n참고: 미국의 Microsoft는 전통적인 달력 년도와 일치하지 않는 재무 연도를 따릅니다. 예를 들어, 그들의 재정 첫 분기는 7월부터 9월까지의 기간을 다룹니다. 따라서 그들의 제1사분기 실적 보고서는 실제 달력에서 이전 분기의 성과를 반영합니다. 질문과 문서명에 이미 이 사항이 고려되어 있습니다.\n\n# 문서 수용 및 색인화\n\n\n\n<div class=\"content-ad\"></div>\n\n파이낸셜 문서를 이해하고 질문에 답하는 데 효과적으로 Vertex AI Search를 활용하려면 먼저 데이터를 준비하고 가져와야 합니다. 이를 위해 Vertex AI Search에서 전용 데이터 저장소를 생성하고 Google Cloud Storage (GCS)에서 파이낸셜 문서를 이 저장소로 가져와야 합니다. 다행히도, Vertex AI Search는 정보의 구문 분석, 조각화 및 색인 작업을 자동으로 처리해줍니다.\n\n다음으로, 데이터를 활용하여 견고한 검색 및 검색 기능을 제공하는 문서 검색 응용 프로그램을 구성할 것입니다. 이러한 단계를 따라가면 파이낸셜 문서의 효과적인 색인 및 탐색을 위한 견고한 기초를 확립할 수 있습니다. 이를 통해 실험 및 문서 질문 응답용 견고한 파이프라인을 개발하는 데 필요한 정보에 빠르게 액세스할 수 있습니다. 각 단계를 자세히 살펴보겠습니다.\n\nI. 데이터 저장소 생성:\n\nVertex AI Search의 데이터 저장소는 처리된 문서가 저장되는 컨테이너입니다. 처리된 조각을 포함하도록 데이터 저장소를 생성하려면 Vertex AI Search 프로젝트 내에서 쉽게 인식할 수 있도록 데이터 저장소에 고유한 식별자와 표시 이름을 할당해야 합니다. 현재 데이터 저장소에는 아무 문서도 포함되어 있지 않습니다. 다음 단계로 문서를 이 데이터 저장소에 푸시(입력)할 것입니다. 여기서 강조할 사항은 원본 PDF 문서가 실제로 GCS에 저장된다는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n아래 코드 스니펫은 REST API를 사용하여 Vertex AI Search를 구현하는 방법을 간단히 보여줍니다. 또한 Vertex AI Python SDK를 사용할 수도 있습니다. 여기서 Discovery Engine Vertex AI에 대한 문서를 참조하실 수 있습니다. 데이터 저장소를 생성하는 전체 코드는 여기에서 확인하실 수 있습니다.\n\n```js\nurl = f\"https://discoveryengine.googleapis.com/v1alpha/projects/{config.PROJECT_ID}/locations/global/collections/default_collection/dataStores?dataStoreId={data_store_id}\"\n\nheaders = {\n    'Authorization': f'Bearer {config.ACCESS_TOKEN}',\n    'Content-Type': 'application/json',\n    'X-Goog-User-Project': config.PROJECT_ID\n}\ndata = {\n    'displayName': data_store_display_name,\n    'industryVertical': IndustryVertical.GENERIC,\n    'solutionTypes': SolutionType.SOLUTION_TYPE_SEARCH,\n    'contentConfig': DataStore.ContentConfig.CONTENT_REQUIRED,\n    'documentProcessingConfig': {\n        'defaultParsingConfig': {\n            'layoutParsingConfig': {}\n        }\n    }\n}\n\nresponse = requests.post(url, headers=headers, json=data)\n```\n\nII. GCS에서 문서 입력:\n\n데이터 저장소가 생성되면 지정된 GCS 버킷에서 귀사의 금융 문서를 입력하기 시작합니다. 이 프로세스에는 데이터 집합의 원본 PDF 문서가 저장된 GCS 버킷의 URI를 지정해야 합니다. 이전에는 manifest 파일을 생성해야 합니다. 이것은 Vertex AI 검색에 입력할 문서들의 모든 메타데이터를 포착하는 JSON 파일입니다. 이 파일의 샘플 행인 metadata.json은 아래에서 표시됩니다.\n\n<div class=\"content-ad\"></div>\n\n```json\n{\n  \"id\": \"1\",\n  \"jsonData\": \"{\\\"company\\\": \\\"alphabet\\\", \\\"time_period\\\": \\\"Q1 2021\\\"}\",\n  \"content\": {\n    \"mimeType\": \"application/pdf\",\n    \"uri\": \"gs://vais-rag-patterns/raw_docs/alphabet-q1-2021.pdf\"\n  }\n}\n```\n\n아래는 삽입을 초기화하기 위한 코드의 예시 미리보기입니다. 이 코드는 REST API를 활용하며, 전체 코드는 여기 링크된 Git 저장소에서 찾을 수 있습니다.\n\n```js\nurl = f\"https://discoveryengine.googleapis.com/v1/projects/{project_id}/locations/global/collections/default_collection/dataStores/{data_store_id}/branches/0/documents:import\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {config.ACCESS_TOKEN}\",\n    \"Content-Type\": \"application/json\"\n}\n\ndata = {\n    \"gcsSource\": {\n        \"inputUris\": [gcs_input_uri]\n    }\n}\n\nresponse = requests.post(url, headers=headers, json=data)\n```\n\nIII. 문서 검색 애플리케이션 만들기:\n\n<div class=\"content-ad\"></div>\n\n우리 문서들의 성공적인 소화 후 마지막 단계는 문서 검색 애플리케이션을 생성하는 것입니다. 이 애플리케이션은 색인된 데이터와 상호 작용하여 재무 문서를 검색, 검색하고 분석하는 데 필요한 도구와 기능을 제공할 것입니다.\n\n이 앱을 만들기 위해 필요한 샘플 코드는 아래에 표시되어 있습니다. 회사 티어를 검색하도록 활성화하고 LLM을 사용하여 고급 검색을 활성화해야 합니다. 이는 효과적으로 문서 질의에 답할 수 있습니다. 이 프로세스는 REST API를 사용하지만 Python SDK를 사용하여도 수행할 수 있습니다. 앱 생성을 위한 완전한 코드는 여기에서 찾을 수 있습니다.\n\n```js\nurl = f\"https://discoveryengine.googleapis.com/v1alpha/projects/{config.PROJECT_ID}/locations/global/collections/default_collection/engines?engineId={data_store_id}\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {config.ACCESS_TOKEN}\",\n    \"Content-Type\": \"application/json\",\n    \"X-Goog-User-Project\": config.PROJECT_ID\n}\n\ndata = {\n    \"displayName\": data_store_display_name,\n    \"dataStoreIds\": [data_store_id],\n    \"solutionType\": SolutionType.SOLUTION_TYPE_SEARCH,\n    \"searchEngineConfig\": {\n        \"searchTier\": SearchTier.SEARCH_TIER_ENTERPRISE,\n        \"searchAddOns\": SearchAddOn.SEARCH_ADD_ON_LLM\n    }\n}\n\nresponse = requests.post(url, headers=headers, json=data)\n```\n\n위에서 설명한 전체 프로세스를 용이하게 만들기 위해 여기에 제공된 스크립트를 활용할 수 있습니다. 데이터 흡수와 애플리케이션 설정에 관련된 모든 필요한 단계를 처리합니다. 구조화된 방식으로 이 접근 방법을 따르면 Vertex AI Search의 강력함을 활용하여 재무 문서를 가치 있는 지식 베이스로 변환할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# RAG 자동화를 위한 아키텍처 패턴\n\n저희의 원시 PDF 문서가 Vertex AI Search 내에 소화되고 색인화되었으므로, 처리된 문서를 쿼리하고 답변을 생성하는 것이 이제 쉽게 간소화될 수 있습니다. 제공된 Python SDK 샘플 코드는 이전에 구성된 검색 애플리케이션을 통해 데이터베이스를 쿼리하는 방법을 보여줍니다. 참조용 전체 코드는 이곳에서 확인할 수 있습니다.\n\n```js\nclient_options = (\n    ClientOptions(api_endpoint=f\"{LOCATION}-discoveryengine.googleapis.com\")\n    if LOCATION != \"global\"\n    else None\n)\n\nclient = discoveryengine.SearchServiceClient(client_options=client_options)\n\nserving_config = client.serving_config_path(\n    project=config.PROJECT_ID,\n    location=LOCATION,\n    data_store=data_store_id,\n    serving_config=\"default_config\",\n)\n\ncontent_search_spec = discoveryengine.SearchRequest.ContentSearchSpec(\n    snippet_spec=discoveryengine.SearchRequest.ContentSearchSpec.SnippetSpec(\n    return_snippet=False\n    ),\n    extractive_content_spec=discoveryengine.SearchRequest.ContentSearchSpec.ExtractiveContentSpec(\n        max_extractive_answer_count=3,\n        max_extractive_segment_count=3,\n    ),\n    summary_spec=discoveryengine.SearchRequest.ContentSearchSpec.SummarySpec(\n        summary_result_count=5,\n        include_citations=True,\n        ignore_adversarial_query=False,\n        ignore_non_summary_seeking_query=False,\n    ),\n)\n\nrequest = discoveryengine.SearchRequest(\n    serving_config=serving_config,\n    query=search_query,\n    filter=filter_str,\n    page_size=5,\n    content_search_spec=content_search_spec,\n    query_expansion_spec=discoveryengine.SearchRequest.QueryExpansionSpec(\n        condition=discoveryengine.SearchRequest.QueryExpansionSpec.Condition.AUTO,\n    ),\n    spell_correction_spec=discoveryengine.SearchRequest.SpellCorrectionSpec(\n        mode=discoveryengine.SearchRequest.SpellCorrectionSpec.Mode.AUTO\n    ),\n)\n\nresponse = client.search(request)\n```\n\n이를 기반으로, 우리는 이제 여러 가지 방법으로 Vertex AI Search API를 사용하여 쉽게 RAG 파이프라인을 구축할 수 있습니다. 다음으로, 이를 실행하는 네 가지 일반적인 패턴을 살펴보며, 이러한 파이프라인이 구현될 수 있는 유연성과 쉬움을 보여드릴 것입니다.\n\n<div class=\"content-ad\"></div>\n\nVertex AI Search에서 검색 요청을 구성할 때 유용한 정보를 추출하기 위한 사양을 설정하는 것이 중요합니다. 스니펫, 세그먼트 및 답변 옵션을 활성화하여 관련 콘텐츠를 포괄적으로 검색할 수 있습니다. 또한 LLM 파워를 활용한 요약 기능을 활성화하면 검색 결과의 간단한 요약(답변)을 생성하여 사용자 경험을 향상시킬 수 있습니다. 결과적으로 생성되는 JSON 응답에는 요약된 답변과 추출된 세그먼트 및 답변이 모두 포함됩니다.\n\nVertex AI Search는 텍스트 데이터를 세그먼트화하고 추출하는 다음과 같은 세 가지 메소드를 사용합니다:\n\n- 스니펫: 검색 결과 문서에서 간단한 발췌문을 제공하여 콘텐츠 미리보기를 제공하며 종종 히트 하이라이팅을 포함합니다.\n- 추출형 답변: 원본 문서에서 직접 추출된 문장을 제공하여 간결하고 맥락에 맞는 답변을 제공합니다.\n- 추출형 세그먼트: 보다 상세한 직접 추출된 텍스트 데이터를 제공하여 답변 제시, 후처리 작업 및 대규모 언어 모델의 입력으로 사용할 수 있습니다.\n\n또한 스펠 수정 및 쿼리 확장을 위한 설정을 구성하여 검색 정확도를 높이고 잠재적 결과를 확장할 수도 있습니다. 우리의 사용 사례에서는 스니펫을 무시합니다.\n\n<div class=\"content-ad\"></div>\n\n# 패턴 I: 기본 설정(out-of-the-box, OOB) 답변 생성을 통한 검색\n\n패턴 I는 Vertex AI Agent Builder 콘솔 또는 Discovery Engine API를 통해 구현할 수 있는 간단하고 일반적인 파이프라인입니다. 이 파이프라인은 검색 인덱스에서 관련 정보를 검색하여 가져오는 데 사용됩니다. 이 인덱스는 데이터 저장소(datastore) 및 검색 앱을 통해 이전에 설정한 것이며 원본 원시 PDF가 GCS에 저장된 인덱스에 매핑됩니다. 또한 이 파이프라인은 내부 시스템(Large Language Model, LLM Powered)을 사용하여 검색 결과를 기반으로 간결한 답변을 생성합니다. 이를 통해 외부 LLM에 대한 명시적 호출이 필요 없습니다. 사용자는 단일 API 요청만으로 지원 문서에서의 구체적인 답변과 인용을 함께받을 수 있습니다. 이를 통해 관련 정보를 검색하고 요약하는 프로세스가 간소화됩니다. 이 파이프라인을 보여주는 코드는 [여기](공유된 저장소 링크)에서 찾을 수 있습니다.\n\n![RAG Automation](/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_2.png)\n\n위 다이어그램은 Vertex AI Search를 구동하는 간단한 RAG 파이프라인을 보여줍니다. 워크플로는 클라우드 스토리지에 저장된 원시 PDF 문서들의 컬렉션으로 시작합니다. 이러한 문서들은 이미 Vertex AI Search에 의해 흡수되고 처리되어 구조화된 인덱스가 생성되어 효율적인 검색 및 검색이 이루어집니다. 사용자가 쿼리를 제출하면 Vertex AI Search는 이러한 인덱스를 활용하여 가장 관련성 높은 문서를 신속하게 식별합니다. 그런 다음 이 문서들에서 적절한 정보를 추출하고 정보의 출처를 나타내는 인용을 포함하여 사용자에게 답변을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\nPattern I의 주목할 만한 장점은 그 간결함과 독립성입니다. 외부 LLM 호출이 필요하지 않고 전체 프로세스를 단일 API 요청으로 통합하여 필요를 제거합니다. 그러나 이러한 간소화된 방법은 유연성에 제한을 가져올 수 있습니다. 이는 최종 요약에서 관련성이 적은 문서에서 불필요한 정보를 포함시킬 수도 있습니다. 단일 소스에서 파생된 답변에 중점을 둔 단방향 질문 응답에 초점을 맞추어 검색 결과로부터 답변을 생성하는 것이 중요합니다.\n\n다음 갱신된 파이프라인 반복 (Pattern II)에서는 쿼리 이해 단계에서 메타데이터 통합을 탐색하고, 결과를 개선하기 위해 선택한 LLM으로 Gemini를 활용할 것입니다. 이 방법은 소음을 최소화하고 가장 관련 있는 검색된 정보에 집중하여 요약된 답변을 최적화하는 데 목표를 두고 있습니다.\n\n# Pattern II: OOB 답변 생성과 필터링된 검색\n\nPattern II는 필터링을 통한 쿼리 이해에 초점을 맞춘 사전 검색 단계를 포함하여 검색을 향상시키는 것입니다. 이는 사용자가 자연어 쿼리를 선호하는 상황을 다루며, 수동 필터링이 필요 없도록 하거나 원하는 필터가 즉시 제공되지 않는 상황을 수용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n파이프라인은 Gemini를 사용하여 사용자의 자연어 쿼리를 처리하여 시작합니다. Gemini는 기업 이름 및 기간과 같은 주요 정보를 추출하기 위해 명명된 엔티티 인식(NER)을 수행합니다. 이 추출된 메타데이터는 JSON 형식으로 출력되며 사용자의 의도와 더 일치하도록 검색 결과를 필터링하는 데 활용됩니다. 이 과정은 노이즈를 줄이는 뿐만 아니라 검색 메트릭을 크게 향상시킵니다.\n\n또한 추출된 기업 이름 및 기간 정보는 수집 단계에서 설정된 형식과 구문에 준수해야 합니다. 이 구현에서 메타데이터 필터링은 Vertex AI Search 내의 문서에 이전에 할당된 기업 이름 및 기간 태그를 활용합니다.\n\n따라서 이 개선은 Pattern I를 기반으로 쿼리 이해 단계를 검색 이전에 도입함으로써 검색 결과의 정확성과 관련성을 극대화합니다.\n\n상단의 아키텍처 다이어그램에 나타난 대로, 워크플로는 사용자가 특정 필터의 제약 없이 정보 요구를 표현하는 자연어를 사용하여 검색을 시작하는 것으로 시작됩니다. 그런 다음 쿼리가 Gemini에 전달되어 명명된 엔티티 인식(NER)을 수행하여 기업 이름 및 기간과 같은 관련 메타데이터를 추출합니다. 이 추출된 메타데이터는 JSON 형식으로 구조화되어 쉽게 필터링되며 이후 Vertex AI Search에서 사용되어 문서 색인을 필터링하여 추출된 기업 및 기간 정보와 일치하는 결과만을 좁혀서 제공합니다. 마지막으로, 필터링된 결과이기 때문에 더 관련성이 높은 문서들이 사용자에게 제공되며 정보의 출처를 나타내는 인용을 포함합니다.\n\n<div class=\"content-ad\"></div>\n\n위에서 설명한 pipeline의 코드 구현은 여기서 참조할 수 있습니다. 이 구현에서의 Entity 인식은 프롬프트를 사용하여 회사 이름 추출에는 zero-shot 방식을 활용하고 시기 추출에는 양성 및 음성 예시를 이용한 few-shot 방식을 사용합니다.\n\n```js\n아래 쿼리에서 회사 이름을 추출하십시오.\n회사 이름은 `Microsoft`, `Alphabet`, 또는 `Amazon` 중 하나입니다.\n회사 이름이 `LinkedIn`이면 `Microsoft`로 번역하십시오.\n\n중요 사항: 추출된 회사 이름은 반드시 줄 바꿈이나 구두점 또는 추가 공백 없이 단일 단어여야합니다.\n```\n\n```js\n쿼리로부터 특정 시기를 추출하십시오. 유효한 시기는 'Q1 2021' 형식만 가능합니다.\n\n유효하지 않은 형식의 예시:\n'Q2 2020 to Q2 2021'\n'Q2 2020 - Q2 2021'\n'Q2 2020, Q2 2021'\n\n추출된 시기는 단일 분기와 단일 연도만을 나타내야 합니다. 현재와 과거 간의 대조만을 고려해야합니다.\n\n중요 사항: 쿼리가 현재와 과거를 비교할 때 과거 참조를 무시하십시오.\n\n예시\n========\n'2020년 첫 분기'를 'Q1 2020'으로 변환하세요.\n'Q2 2021에서 Q2 2020 대비 증가'를 'Q2 2021'로 번역하세요.\n'2022년 12월 31일 종료 12개월'을 'Q4 2022'로 번역하세요.\n```\n\n# Pattern III: Extractive Segments 및 Gemini Pro를 활용한 필터링 검색 및 응답 생성\n\n<div class=\"content-ad\"></div>\n\n이전에는 Vertex AI Search에서 관련 콘텍스트를 가져오는 세 가지 다른 정밀도 수준에 대해 학습했습니다: 스니펫, 추출 세그먼트 및 추출된 답변. 이 패턴인 Pattern III에서는 검색 결과에서 추출된 세그먼트를 활용하여 기존 (OOB) 답변 생성 단계 (Pattern I 및 II)를 대체합니다. 생성된 답변에 특정한 스타일, 뉘앙스, 형식, 길이 또는 구조가 필요한 경우 특히 중요합니다. 이러한 시나리오에서 추출된 세그먼트를 외부 LLM인 Gemini와 함께 질의와 함께 명시적으로 전달할 수 있습니다. 우리는 요구 사항에 더 가까운 답변 생성을 위해 다양한 방법으로 프롬프트를 유연하게 설계할 수 있습니다. Pattern III는 이러한 방식을 포괄합니다.\n\n위에 표시된 아키텍처 다이어그램은 Pattern III의 워크플로우를 보여줍니다. 여기서 사용자는 먼저 자연어 질의를 제출하고, 이는 초기에 Gemini에 의해 관련 메타데이터를 추출하도록 처리됩니다. 이 구조화된 메타데이터는 Pattern I와 II와 유사하게 Vertex AI Search가 문서 색인을 효율적으로 검색하고 관련 문서를 식별하는 데 사용됩니다. 이러한 필터링된 문서에서 사용자의 질문에 직접 답변하는 구체적인 세그먼트가 추출됩니다. 최종 단계에서 Gemini는 이러한 추출된 세그먼트를 처리하여 사용자에게 포괄적인 답변을 생성하고 정보의 출처 문서를 나타내는 인용 정보를 통합합니다. 사용자는 정보가 추출된 문서에 대한 참조와 함께 질의에 대한 최종 답변을 받게 됩니다. 이 패턴을 다루는 코드는 여기에서 찾을 수 있습니다.\n\n이 워크플로에 대한 답변 생성에 사용되는 프롬프트는 아래에 표시된 것과 같이 간단할 수도 있고 특정 요구 사항에 기반하여 더 복잡하고 정교할 수도 있습니다.\n\n```js\n다음 콘텍스트를 기반으로 아래 질문에 명확하고 간결한 답변을 제공하십시오:\n\n콘텍스트: {콘텍스트}\n\n질문: {질문}\n```\n\n<div class=\"content-ad\"></div>\n\n# 패턴 IV: Gemini Pro를 활용한 추출적 답변을 통한 필터링된 검색과 답변 생성\n\n이번에는 이전의 패턴 III 구조를 대부분 유지하면서 주요 수정 사항을 가진 파이프라인 반복입니다. 이전에 문서에서 추출된 세그먼트를 활용하는 대신 추출적 답변을 활용합니다. 이 변경은 이전 프롬프트 구조를 유지한 채 이루어졌습니다. 이 변경의 영향을 설명하기 위해 테스트 세트에서 질문을 살펴보고 세그먼트 대비 답변에서 파생된 생성된 답변을 비교하겠습니다.\n\n아래에는 예상 질문과 그라운드 트루스 파일에서 기대되는 답변이 표시되어 있습니다.\n\n```js\n마이크로소프트의 실적 보고서에 따르면 2021년 Q1 LinkedIn의 매출 증가액은 얼마이고,\n일정한 화폐단위 조정을 고려했을 때 성장률은 어떻게 되는가?\n```\n\n<div class=\"content-ad\"></div>\n\n2021년 제1분기에는 링크드인의 매출이 전년대비 25% 증가했습니다. 환율 변동을 조정한 경우 성장률은 23%였습니다.\n\n우선, Vertex AI 검색에서 샘플 질문에 대한 추출 세그먼트를 살펴봅시다. 아래는 상위 세 개의 세그먼트입니다. 우리의 질문에 대한 답변은 비즈니스 하이라이트 하단에 있는 세그먼트 1과 테이블 콘텐츠 일부인 세그먼트 2에서 유도할 수 있음을 알 수 있습니다. 지미니 포스트-검색을 사용하여 최종 답변을 생성하기 위해 세그먼트를 연결하여 단일 문맥으로 전달하면 됩니다.\n\n## 비즈니스 하이라이트\n\n생산성 및 비즈니스 프로세스의 매출은 $13.6 억으로 증가했으며,\n15% 증가했습니다 (환율 변동으로 12% 상승),\n다음과 같은 비즈니스 하이라이트가 포함되어 있습니다:\n\n- 오피스 상용 제품 및 클라우드 서비스 매출이 14% 증가했습니다\n  (환율 변동으로 10% 증가),\n  오피스 365 상용 매출의 22% 증가로 주도되었습니다 (환율 변동으로 19% 상승)\n- 오피스 소비자 제품 및 클라우드 서비스 매출이 5% 증가했으며\n  (환율 변동으로 2% 증가),\n  마이크로소프트 365 소비자 구독자 수는 5020만으로 증가했습니다\n- 링크드인 매출이 25% 증가했습니다 (환율 변동으로 23% 상승)\n- 다이내믹스 제품 및 클라우드 서비스 매출이 26% 증가했습니다\n  (환율 변동으로 22% 상승),\n  다이내믹스 365 매출이 45% 증가로 주도되었습니다 (환율 변동으로 40% 증가)\n\n미세한 과정을 통한 재무 성과 일정 화 동일 통화 환산은 다음과 같습니다:\n\n| 세그먼트                    | 2020년 | 2021년 | 증가율 | 동일 통화 여파 |\n| --------------------------- | ------ | ------ | ------ | -------------- |\n| 생산성 및 비즈니스 프로세스 | $11743 | $13552 | 15%    | 12%            |\n| 인텔리전트 클라우드         | $12281 | $15118 | 23%    | 20%            |\n| 더 개인화된 컴퓨팅          | $10997 | $13036 | 19%    | 16%            |\n\n마이크로소프트에 대해\n\n마이크로소프트 (나스닥 \"MSFT\" @microsoft)는 지능형 클라우드와 지능형 엣지 시대에 대한 디지털 전환을 실현합니다. 그 사명은 지구상의 모든 사람과 기관에 더 높은 성공을 이루도록 자율화하는 것입니다.\n\n\n\n<div class=\"content-ad\"></div>\n\nRevenue in Intelligent Cloud는 $ 15.1 억으로 23% 증가했습니다 (일정 환율로 20% 상승), 다음 비즈니스 하이라이트가 있습니다:\n• 서버 제품 및 클라우드 서비스 수익은 26%(일정 환율에서 23% 증가) 성장했으며, Azure 수익은 50%(일정 환율에서 46% 상승) 증가했습니다.\n개인 컴퓨팅에서의 수익은 $ 13.0 억으로 19% 증가했습니다 (일정 환율로 16% 상승), 다음 비즈니스 하이라이트가 있습니다:\n• Windows OEM 수익이 10% 증가했습니다.\n• Windows 기업 제품 및 클라우드 서비스 수익은 10% 증가했습니다 (일정 환율로 7% 증가).\n• Xbox 콘텐츠 및 서비스 수익은 34% (일정 환율로 32% 증가).\n• 트래픽 취득 비용을 제외한 검색 광고 수익은 17%(일정 환율로 14%) 증가했습니다.\n• Surface 수익이 12%(일정 환율로 7%) 증가했습니다.\n\nMicrosoft는 2021 회계 연도 3분기에 주주에게 주식 재매수 및 배당금 100 억 달러를 돌려주었습니다. 이는 2020 회계 연도 3분기에 비해 1% 증가한 금액입니다.\n\n비즈니스 전망\nMicrosoft는 분기별 이익 발표에 관련하여 전방향 가이드를 제공할 것이며 이어서 이익 회의 전화와 웹캐스트에서 설명할 것입니다.\n\n분기별 하이라이트, 제품 출시 및 개선 사항\n매 분기마다 Microsoft는 수백 개의 제품을 새로 출시하거나 현재 제품 및 서비스를 개선하는 서비스로 제공합니다. 이러한 출시는 고객이 보다 생산적이고 안전하며 클라우드 및 엣지에서 차별화된 가치를 전달하기 위해 설계된 중요한 연구 및 개발 투자의 결과입니다. 우리가 어떻게 사업에서 혁신을 가속하고 시장 기회를 확대하고 있는지 보여주기 위해 제품 범주별로 분류된 이 분기의 주요 제품 출시 및 다른 하이라이트를 소개합니다.\n\n코로나19 대응\nMicrosoft는 직원의 안전을 보장하고 영역 사회의 건강과 안녕을 보호하며 원격 근무 상태에서 최상의 업무를 수행할 수 있도록 고객 및 협력사에 기술과 자원을 제공하는 데 초점을 맞추고 있습니다. Microsoft의 COVID-19 대응에 대한 자세한 정보는 여기에서 확인할 수 있습니다.\n\n환경, 사회 및 지배 (ESG)\nMicrosoft의 미션을 더욱 잘 실현하기 위해 우리는 양질의 영향을 미칠 수 있는 곳에만 환경, 사회 및 지배 (ESG) 노력에 중점을 둡니다. 최신 노력과 우선 사항에 대해 자세히 알아보려면 투자자 관계 ESG 웹사이트를 방문하십시오.\n\n<div class=\"content-ad\"></div>\n\n흥미롭게도, 우리가 궁금한 질문에 대한 답을 얻기 위해 필요한 정보는 마지막(세 번째) 추출한 답변에만 포함되어 있는 것을 알 수 있습니다. 최종 답변을 생성하기 위해 세 개의 추출한 답변을 모두 한 문맥으로 연결하여 Gemini에게 제공하고, 원래의 질문과 함께 제출하여 답변을 생성합니다.\n\n```js\nRevenue in Intelligent Cloud은 $15.1 억으로 23% 증가했으며 (환율 상수로는 20% 증가), 다음과 같은 비즈니스 현황을 보여줍니다:\n• 서버 제품 및 클라우드 서비스 수익이 26% 증가했으며 (환율 상수로는 23% 증가), Azure 수익이 50% 증가(환율 상수로는 46% 증가) • More Personal Computing의 수익은 $13.0 억으로 19% 증가했으며 (환율 상수로는 16% 증가), 다음과 같은 비즈니스 현황을 보여줍니다: • Windows OEM 수익이 10% 증가함 • Windows 상용 제품 및 클라우드 서비스 수익이 10% 증가함 (환율 상수로는 7% 증가)\n• Xbox 콘텐츠 및 서비스 수익이 34% 증가함 (환율 상수로는 32% 증가)\n• 트래픽 채광 비용 제외 검색 광고 수익이 17% 증가함 (환율 상수로는 14% 증가)\n• Surface 수익이 12% 증가함 (환율 상수로는 7% 증가)\nMicrosoft은 2021 회계 연도 3분기에 $10.0 억을 주주에게 주식 매수와 배당금 형태로 반환했으며, 이는 2020 회계 연도 3분기 대비 1% 증가했습니다.\n비즈니스 전망 Microsoft는 이 분기 실적 발표와 관련하여 이익 회의 전화와 웹캐스트에서 전반적인 안내를 제공할 것입니다.\n```\n\n```js\n재무 성과\n통화 변동 조정 2021년 3월 31일 3개월 종료 (백만 달러, 주당 금액 제외)\n수익 운영 이익 순이익 희석주당순이익\n2020년 표시 요건 (GAAP) $35021 $12975 $10752 $1.40\n2021년 표시 (GAAP) $41706 $17048 $15457 $2.03\n2021년 조정 (non-GAAP) $41706 $17048 $14837 $1.95\nY/Y 변경율 (GAAP) 19% 31% 44% 45%\nY/Y 변경율 (non-GAAP) 19% 31% 38% 39%\n통화 변동 영향 $972 $634 $615 $0.08\nY/Y 변경율 (non-GAAP) 통화 변동 16% 27% 32% 34%\n세그먼트 수익 통화 변동 조정 2021년 3월 31일 3개월 종료 (백만 달러)\nProductivity and Business Processes Intelligent Cloud More Personal Computing\n2020년 표시 $11743 $12281 $10997\n2021년 표시 $13552 $15118 $13036\nY/Y 변경율 15% 23% 19%\n통화 변동 영향 $366 $367 $239\nY/Y 통화 변동율 12% 20% 16%\n선택된 제품 및 서비스 수익 통화 변동 조정 2021년 3월 31일\nY/Y 변경율 (GAAP) 통화 변동 영향 Y/Y 통화 변동율\nOffice 상업용 제품 및 클라우드 서비스 14% (4)% 10%\nOffice 365 상업용 22% (3)% 19%\nOffice 소비자 제품 및 클라우드 서비스 5% (3)% 2%\nLinkedIn 25% (2)% 23%\nDynamics 제품 및 클라우드 서비스 26% (4)% 22%\nDynamics 365 45% (5)% 40%\n서버 제품 및 클라우드 서비스 26% (3)% 23%\nAzure 50% (4)% 46%\nWindows OEM 10% 0% 10%\nWindows 상업용 제품 및 클라우드 서비스 10% (3)% 7%\nXbox 콘텐츠 및 서비스 34% (2)% 32%\nSurface 12% (5)% 7%\n검색 광고 트래픽 채광 비용 제외 17% (3)% 14%\nMicrosoft에 대해 Microsoft(Nasdaq \"MSFT\" @microsoft)는 지능적인 클라우드와 지능형 엣지 시대의 디지털 변형을 가능케 합니다.\n```\n\n<div class=\"content-ad\"></div>\n\n저희가 확인한 바로는 Gemini가 생성한 최종 응답이 추출 세그먼트를 사용한 이전 응답보다 더 짧고 간결하다는 것을 알 수 있습니다.\n\n```js\n제공된 맥락에 따르면, Microsoft의 실적 보고서에 따르면 2021년 1분기 LinkedIn의 매출 증가율은 25%였습니다. 일정 통화를 고려한 경우 성장율은 23%였습니다.\n```\n\n해당 패턴을 포함한 소스 코드는 여기에서 찾을 수 있습니다.\n\n# 대안적 패턴\n\n<div class=\"content-ad\"></div>\n\n- 이전에 설명한 표준 워크플로우를 넘어서 PDF 문서로부터 답변 생성을 혁신적으로 개선할 수 있는 여러 고급 기술들이 있습니다. 그 중 하나는 질의 확장인데, 이는 초기 검색 질의를 관련 용어나 동의어로 확장시킵니다. 이 기능은 Vertex AI Search를 사용하여 매개변수를 AUTO로 설정함으로써 쉽게 활성화할 수 있습니다. 또는 DIY 사전 검색 단계를 설계하여 LLM을 사용하여 질의 변형을 생성한 후 Vertex AI Search에 병렬 호출을 할 수도 있습니다. 질의 확장은 질의 응답 시스템에서 정보 검색의 품질을 향상시키는 중요한 기술입니다. 다양한 질의 변형을 생성하여 검색의 관련성을 향상시키는 것뿐만 아니라 정확한 답변을 생성하는 데 필수적인 최상위 검색 문서의 대표성을 확보하는 데 핵심적인 역할을 합니다.\n- 문서 내 키워드 부스팅은 관련성을 향상시키는 또 다른 강력한 기술입니다. 이는 Vertex AI Search를 사용하여 즉시 지원됩니다. 특정 용어에 우선 순위를 부여함으로써 검색 결과의 관련성을 향상시킬 수 있습니다.\n- 게다가 검색 튜닝을 사용하여 검색 성능을 향상시킬 수도 있습니다. 이 접근 방법은 특정 산업이나 회사에 특화된 쿼리를 일반적인 언어 모델로 충분히 해결할 수 없을 때 특히 유용합니다. 검색 튜닝은 Vertex AI Search를 통해 즉시 지원됩니다.\n- PDF 문서의 사전 처리 유형을 선택하여 검색 관련성을 향상시키는 것도 중요합니다. 서로 다른 문서 파서를 사용하여 PDF 문서의 사전 처리 유형을 선택함으로써 검색 관련성을 향상시킬 수 있습니다. Vertex AI Search는 기본적으로 레이아웃 파서, OCR 파서 및 디지털 파서를 지원합니다. 우리의 사용 사례에서는 레이아웃 파서를 사용했습니다. 이는 Vertex AI Search를 위해 RAG용으로 PDF 문서를 사용할 계획이라면 권장됩니다. 또는 Document AI를 사용하여 문서에서 표를 추출하는 등 다른 정교한 방법론을 사용할 수 있습니다. PDF와 추출한 표를 텍스트 형식으로 변환하여 PDF로 처리하지 않고 Vertex AI Search에 삽입할 수 있습니다.\n- 마지막으로 사용자 정의 임베딩 기반 정보 검색이 필요한 기업을 위해 Vertex AI는 강력한 벡터 검색 기능을 제공합니다. Vertex AI의 벡터 검색은 수십억 벡터를 수용하고 밀리초 내에 가장 가까운 이웃을 식별할 수 있습니다. 벡터 검색(이전에는 Matching Engine로 알려짐)은 Vertex AI Search와 유사하며 Agent Builder의 일부입니다. Agent Builder는 이 두 검색 옵션의 캡슐화로 생각할 수 있습니다. 뒤에 덧붙이자면, 이 기사에서 extensively 다룬 것은 모두 Vertex AI Search에 관한 것입니다. Vertex AI 벡터 검색은 벡터 저장소로, chunking 전략, 임베딩 모델 선택, 의미 유사성 검색을 위한 점수 매기는 알고리즘 선택 등 모든 것에 대한 전체 사용자 정의를 원할 때 좋은 대체 옵션입니다. Agent Builder는 check grounding, grounded generation 및 ranking API와 같은 독립적인 API도 포함하고 있습니다. 이러한 API를 사용하여 벡터 검색과 함께 커스텀 RAG 파이프라인을 구축할 수 있습니다.\n\n# RAG 파이프라인 평가\n\n![Architecture Blueprint for RAG Automation Advanced Document Understanding using Vertex AI Search](/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_3.png)\n\n다음으로, 검색 성능 및 생성된 답변 품질을 평가하는 방법에 대해 알아보겠습니다. 우리는 검색 시스템을 평가하는 데 적합한 다양한 지표를 실험해보고, RAG 파이프라인에서 답변 품질을 평가하는 지표를 살펴볼 것입니다. 이 평가 과정을 통해 RAG 시스템을 개선하고 최적화하며 어떤 접근 방식이 더 효과적인지 이해하는 데 도움이 될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# I. 정보 검색 평가\n\n## 가) K에서의 정밀도\n\nK에서의 정밀도는 상위 K개의 검색 문서에서 관련 결과의 비율을 측정하는 지표입니다. 이 측정 방법은 초기 결과의 질이 정보의 철저한 검색보다 우선하는 시나리오에서 특히 중요합니다. 이러한 경우의 대표적인 예로 웹 검색 엔진이 있습니다. 여기서 사용자들은 주로 검색 결과의 첫 페이지에 집중합니다.\n\n예를 들어, YouTube에서 \"종이 비행기를 만드는 방법\"에 대한 상위 5개의 교육 비디오를 요청했다고 가정해보겠습니다. 이 중 5개 중 3개의 비디오가 목적에 맞게 지시를 제공한다면, K=5에서의 정밀도는 5개 중 3개 또는 60%가 됩니다. 이 지표는 초기 검색 결과의 관련성을 측정하는 방법을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n단일 점프 질문 응답 사용 사례에서는 각 쿼리 당 관련 항목(문서) 하나를 획득하는 것이 목표입니다. 따라서 K = 1일 때의 정밀도(Precision)를 계산하는 것이 논리적입니다. 이 경우 결과는 이진형입니다: Vertex AI Search에 의해 반환된 검색된 항목 중에서 원하는 문서를 가장 위에 위치시키는지 여부입니다.\n\n<img src=\"/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_4.png\" />\n\n## b) K에 대한 재현율(Recall)\n\n정보 검색 분야에서, K에 대한 Recall 메트릭은 검색 결과 중 상위 K 결과 내에서 검색된 모든 관련 문서의 비율을 측정하는 데 사용됩니다. 이는 정밀도와 다르게 Recall은 모든 잠재적으로 관련 있는 문서를 식별하는 시스템의 능력에 중점을 둡니다. 이러한 측정은 법적이나 학술 연구와 같은 분야에서 주요 문서의 누락이 심각한 결과로 이어질 수 있는 경우에 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, 종이 비행기 제작에 관한 10가지 중요한 비디오가 있다는 시나리오를 가정해 보겠습니다. 상위 5개 결과에 관심이 있습니다. 이 중 상위 5개 결과 중에서 10개 비디오 중 4개가 포함되어 있다면, 5개 중 재현율(Recall)은 10개 중 4개로 40%가 됩니다. 이 백분율은 상위 K개 결과 내에서 모든 가능한 관련 결과를 포착하는 검색 시스템의 효과성을 나타냅니다.\n\n우리의 사용 사례의 고유한 맥락을 고려할 때, 관련 문서가 하나뿐인 경우 관련 문서 한 개에 대한 재현율을 계산하는 것이 논리적입니다. 정밀도와 유사하게, 이는 특정 항목이 검색되었는지 여부를 나타내는 이진 결과를 얻을 것입니다.\n\n<img src=\"/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_5.png\" />\n\n참고: 우리의 사용 사례에서는 단일-홉 질문 응답이고, 답변이 항상 하나의 문서에 매핑된다는 것을 의미합니다. 여러 문서에서 유도된 답변인 다중-홉의 경우, 검색 @ k와 정밀도 @ k가 모두 1로 설정됩니다. 다중-홉 시나리오인 경우, k @ 3 및 k @ 5와 같은 더 높은 값이 더 유용하고 이로운 결과를 얻을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n저희 사용 사례에서는 필터를 적용하지 않은 표준 검색에서 k=1에서 정밀도와 리콜이 각각 51% (패턴 I)입니다. 그러나 명명된 엔티티 인식(Named Entity Recognition, NER)을 사용하고 필터를 적용하면 이러한 지표가 90%까지 증가합니다 (패턴 II).\n\n## c) MRR (평균 역순 순위)\n\n평균 역순 순위(Mean Reciprocal Rank, MRR)는 응답 목록에서 처음으로 올바른 답변의 역순 평균 순위를 측정하는 통계 지표입니다. MRR은 첫 번째 관련 문서의 위치가 추가 관련 문서의 존재보다 더 중요한 상황에서 특히 유용합니다. 이 지표는 질문 응답 시스템 및 사용자가 만족스러운 답변을 처음 만나는 것이 중요한 검색 환경 등에서 일반적으로 사용됩니다.\n\n마카다미아 쿠키에 대한 완벽한 레시피를 찾기 위해 검색 엔진을 사용한다고 상상해보세요. 만일 맨 처음 클릭한 레시피가 바로 필요한 것이라면, 그 검색 엔진은 완벽한 평균 역순 순위(MRR) 1점을 획득합니다. 즉, 바로 최적의 결과를 제공했다는 것을 의미합니다. 그러나 이상적인 레시피가 처음 확인한 것이 아니라 세 번째로 발견하는 것이 매력적이라면, 해당 검색에 대한 MRR은 1/3으로 낮아집니다.\n\n<div class=\"content-ad\"></div>\n\nMRR(Mean Reciprocal Rank)의 공식은 다음과 같습니다:\n\n![image](/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_6.png)\n\n여기서 𝑄은 총 쿼리 수를 나타내며, rᵢ는 i 번째 쿼리에 대한 첫 번째 관련 답변의 순위 위치를 의미합니다.\n\n우리의 사용 사례에서는 Pattern I(필터없는 문서 검색)에서 MRR이 64%입니다. 그러나 Pattern II에서 필터를 적용하면 이 숫자가 91%로 증가합니다.\n\n<div class=\"content-ad\"></div>\n\n## d) DCG (Discounted Cumulative Gain)\n\nDCG는 결과 목록에서의 위치를 기반으로 문서의 유용성 또는 \"이득\"을 측정합니다. 여기서의 가정은 검색 결과에서 먼저 나타나는 문서가 나중에 나타나는 것보다 사용자에게 더 관련성이 높다는 것입니다. \"할인\" 부분은 결과 목록에서의 위치에 비례하여 로그 비례 요소로 각 문서의 관련성 점수를 감소시키는 것을 의미합니다. 이는 사용자가 목록을 아래로 이동할수록 각 이후 결과를 확인할 가능성이 줄어든다는 것을 반영합니다. 수식은 다음과 같습니다:\n\n![수식](/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_7.png)\n\n여기서 p는 순위 위치, relᵢ는 𝑖 위치의 결과의 관련성 점수이며 log⁡₂(𝑖+1)은 (𝑖+1)의 이진 로그를 나타내는 할인 요소입니다. relᵢ를 계산하기 위해 우리는 단순히 관련 문서에 대해 이진 관련성 1을 사용하고 관련 없는 문서에 대해 0을 사용합니다.\n\n<div class=\"content-ad\"></div>\n\nMRR과 비교할 때, DCG는 최상위 결과물 하나만 고려하는 대신 여러 결과물의 관련성을 고려하여 검색 품질을 보다 포괄적으로 판단합니다. MRR과 DCG는 검색 성능에 대해 다른 시각을 제공하며, MRR은 최상위 결과물의 정확성에 초점을 두는 반면 DCG는 전체 결과 목록의 관련성을 고려합니다. 두 지표를 모두 모니터링하여, 회수 전략의 효과를 자세히 이해할 수 있습니다.\n\n## e) NDCG (정규화된 할인 누적 이익)\n\nDCG는 순위가 매겨진 목록의 총 관련성을 측정하는 반면, NDCG는 다양한 목록 간에 비교를 허용하는 DCG의 정규화된 버전으로, 일반적으로 DCG보다 NDCG가 선호됩니다. NDCG는 랭킹 시스템을 평가하기 위한 더 표준화되고 해석 가능한 지표를 제공하기 때문입니다. NDCG의 공식은 아래와 같습니다:\n\n![이미지](/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_8.png)\n\n<div class=\"content-ad\"></div>\n\n위에서:\n\n- DCG𝑝은 원래 공식을 사용하여 위치 𝑝에서의 DCG값을 의미합니다.\n- IDCG𝑝는 이상적인 DCG로, 모든 결과가 관련성에 따라 완벽하게 정렬된 경우 위치 𝑝에서의 최대 가능한 DCG값입니다. IDCG𝑝의 공식은 아래에 나와 있습니다:\n\n\n![IDCG formula](/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_9.png)\n\n\n여기서는 관련성 점수를 내림차순으로 정렬합니다.\n\n<div class=\"content-ad\"></div>\n\n우리의 사용 사례에서는 패턴 I 및 II에 대한 NDCG를 측정합니다. 패턴 I에 대해서는 평균 NDCG가 약 64% 정도입니다. 필터를 적용하면 이 수치가 약 91%로 증가합니다.\n\n## f) 평균 정밀도 (AP)\n\nAP는 검색 엔진과 같은 시스템이 관련 항목을 어떻게 순위를 매기는지를 측정합니다. 얼마나 많은 관련 항목이 발견되었는지와 그들이 얼마나 높게 순위되었는지를 모두 고려합니다. 예를 들어, 종이 비행기를 제작하는 방법에 관한 상위 5개의 지침 동영상을 요청했고, 여기에 동영상 순서가 다음과 같이 제공된다고 가정해 봅시다:\n\n- 동영상 A: 완벽한 지침 (관련 있음)\n- 동영상 B: 전혀 관련 없음 (관련 없음)\n- 동영상 C: 괜찮은 지침 (관련 있음)\n- 동영상 D: 다른 관련 없는 동영상 (관련 없음)\n- 동영상 E: 훌륭한 지침 (관련 있음)\n\n<div class=\"content-ad\"></div>\n\nAP를 계산하기 위해서는 관련 동영상이 발견된 각 지점에서의 정밀도를 살펴봅니다:\n\n- 동영상 A 이후: 1/1 = 100%\n- 동영상 C 이후: 2/3 = 66.7%\n- 동영상 E 이후: 3/5 = 60%\n\n이제 이러한 정밀도 값을 평균합니다: (100% + 66.7% + 60%) / 3 = 75.6%\n\n따라서이 검색 결과의 AP는 75.6%입니다. 이는 평균적으로 검색 중에 관련 결과를 상당히 빨리 얻게 된다는 것을 의미합니다. Precision @ 5는 처음 5개 결과의 관련성에만 초점을 맞추지만 (이 경우 60%), AP는 관련 동영상이 발견된 순서도 고려하여 상위 위치를 보상합니다. 이는 검색 엔진이 귀하를 위해 관련 결과를 찾는 데 얼마나 잘 수행되고 있는지에 대해 보다 세밀한 그림을 제공합니다. AP를 계산하는 공식은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_10.png)\n\nwhere\n\n- H is the set of positions of relevant documents.\n- ∣𝐻∣ is the number of relevant documents.\n- 𝑃(𝑖) is the precision at position 𝑖. P(i) for a relevant document at position 𝑖 is 1/i.\n\n## g) Mean Average Precision (MAP)\n\n\n<div class=\"content-ad\"></div>\n\nMAP은 여러 검색 또는 질의를 평가하기 위해 평균 정밀도 (AP)의 개념을 확장한 것입니다. AP는 하나의 검색이 관련 항목을 어떻게 순위 지정하는지를 측정하는 반면, MAP는 여러 검색에서 AP 점수를 평균하여 여러 질의에 대한 시스템의 전반적인 성능 측정을 제공합니다. 이는 단지 하나가 아닌 여러 테스트에서 평균 성적을 받는 것과 같습니다.\n\nMAP는 복수의 관련 문서가 각 질의에 대해 다루는 복잡한 시나리오를 처리할 수 있는 능력으로 인해 종종 선호됩니다. 하지만 각 질의에 하나의 관련 결과물만 있는 경우, MAP는 흥미로운 단순화를 겪습니다. 하나 관련 문서에 대한 질의를 다룰 때, MAP는 본질적으로 다음과 같이 요약됩니다:\n\n- 평균 정밀도가 정밀도가 됩니다: 각 질의의 AP는 단일 관련 문서가 나타나는 순위에서 달성된 정밀도에 단순히 해당합니다.\n- 정밀도가 역순위(Reciprocal Rank)가 됩니다: 하나의 관련 문서만 있는 경우, 정밀도는 해당 순위의 역수입니다(예: 문서가 3위에 있으면 정밀도는 1/3입니다). 이 값은 정확히 MRR 계산에 사용되는 값입니다.\n- MAP은 MRR을 반영합니다: 모든 질의에 대한 이 AP 값들의 평균인 MAP는 관련 문서의 역순위를 평균내어 지각한다. 이것이 바로 MRR이 하는 일입니다. 이 중첩성 때문에 특정 설정에서 MAP을 사용하면 MRR이 이미 제공하는 것 이상의 추가 통찰력을 제공하지 않습니다. MAP은 보통 여러 관련 문서가 각 질의에 대해 포함된 시나리오(다중 단계 질문 응답)에서 보다 유익하며, 서로 다른 순위에서 모든 관련 문서가 얼마나 잘 검색되었는지에 대한 세밀한 관점을 제공할 수 있습니다.\n\n패턴 I 및 II에 대한 실험에서, MAP은 본질적으로 MRR과 같습니다. 패턴 I에서 64%이며, 패턴 II에서 약 91%로 증가합니다.\n\n<div class=\"content-ad\"></div>\n\n상기 평가를 복제하는 데 필요한 모든 지원 코드는 여기에서 찾을 수 있습니다.\n\n# 답변 평가\n\n다음 단계에서는 RAG 파이프라인의 답변 생성 구성 요소를 평가해 보겠습니다. 각 질문에 대한 실제 정답이 제공되므로 생성된 답변의 품질을 평가하고 테스트 세트의 예상 응답과 의미론적으로 유사하며 정확한지 확인하는 것이 목표입니다.\n\n이를 위해 두 가지 서로 다른 메트릭을 적용할 수 있습니다. 첫 번째 메트릭은 cosine similarity를 활용하여 의미론적 유사성을 양적으로 평가합니다. 두 번째 메트릭은 LLM의 역할로, 생성된 답변과 인간이 생성한 답변을 동시에 LLM에 제시하여 모델의 출력이 인간의 기대와 얼마나 일치하는지를 평가할 수 있습니다. 이 접근 방식을 통해 생성된 답변의 사실적 정확성 및 전반적 일관성을 평가하여 인간이 작성한 응답과 구별할 수 없도록 보장할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 가) 의미 유사성\n\n이 메트릭은 생성된 답변과 기대한 답변의 벡터 표현 사이의 각도의 코사인을 측정합니다. 생성된 응답과 실제 답변(기대 값) 간의 의미적 유사성을 평가하는 것을 의미합니다. 이 평가는 기대 값과 응답을 기반으로 하며, 0부터 1까지의 값을 갖습니다. 높은 점수는 생성된 응답과 기대 값 사이의 더 좋은 일치를 나타냅니다.\n\n기대된 답변과 생성된 답변은 모두 텍스트 임베딩 모델 텍스트-임베딩-003을 사용하여 인코딩됩니다. 이 모델은 Vertex AI API를 통해 제공됩니다. 의미 유사성에 대한 코드 구현은 여기에서 찾을 수 있습니다.\n\n## 나) 사실적 정확성\n\n<div class=\"content-ad\"></div>\n\n이전 RAG 파이프라인에서 생성된 답변의 정확성을 평가하기 위해 우리는 Gemini를 활용합니다. 질문, 기대 답변(실제 답변), 그리고 생성된 답변이 Gemini에 전달됩니다. 프롬프트 템플릿을 사용하여 Gemini가 생성된 답변을 \"정확\"(기대 답변과 완전히 부합), \"부분적으로 정확\"(정보 일부를 포함하지만 불완전하거나 일부 오류가 있는 것), 또는 \"부정확\"(기대 답변과 부합하지 않는 것)으로 분류하도록 안내합니다. 이 분류는 답변 품질을 세분화하여 평가하게 해주며, 답변 생성 모델이 개선이 필요한 부분을 식별하는 데 도움이 됩니다. 이 평가 프로세스의 구현 세부사항은 여기에서 확인할 수 있습니다.\n\n아래 표는 이전에 실험한 네 가지 패턴(RAG 파이프라인)을 비교한 전반적인 답변 정확도를 보여줍니다. 정확도를 계산하기 위해 완전히 정확한 답변에는 1.0의 점수를 할당하고, 부분적으로 정확한 답변에는 0.5의 점수를, 그리고 LLM 사실적 정확성 출력에 따라 부정확하게 분류된 답변에는 0의 점수를 부여합니다. 그림에서는 외부 LLM 패스를 활용하여 추출방식을 사용하여 답변을 생성하는 패턴 IV가 다른 모든 접근 방법을 능가하여 정확도가 거의 70%에 달한다는 것을 보여줍니다.\n\n\n지정된 질문과 아래 표시된 기대 및 생성된 답변을 제공하고,\n답변을 비교하여 `correct`, `partially correct`, 또는 `incorrect` 중 하나의 클래스로 분류하세요.\n\n답변이 부분적으로 정확하거나 부정확한 경우, 이유를 제공하세요.\n출력은 클래스와 이유로 된 Python 딕셔너리 형식으로 제공해야 합니다.\n클래스에는 한 단어만 포함되어야 하며(기대 클래스임),\n이유는 숫자와 사실에만 초점을 맞추어 간결하게 제공해야 합니다.\n\n예상 및 예측된 답변 간의 의미론에 집중하지 마십시오.\n\n중요: 숫자와 사실만 비교하세요.\n\n단위가 다른 경우 비교 전에 정규화하세요.\n예) 10억 = 1000백만.\n\n질문: {question}\n\n기대 답변: {expected_ans}xa\n\n예측 답변: {predicted_ans}\n\n예측된 답변을 기대 답변과 비교하세요.\n예측된 답변이 사실적으로 정확하며 주어진 질문을 충족하는지 결정하세요.\n\n다음 형식에 따라 응답을 제공하세요:\n{format_instructions}\n\n\n<div class=\"content-ad\"></div>\n\n네, 해당 테이블 태그를 마크다운 형식으로 변경해 볼게요.\n\n\nWe can also break down the distribution of classes across the four different approaches (pipelines) to gain a better understanding of how improvements gradually occur with enhancements.\n\n![Distribution of classes](/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_12.png)\n\nThe box plots below show the distribution of semantic similarity scores across different classes (correct, partially correct, incorrect) for the four different document question-answering RAG pipelines we previously created. The x-axis represents the different classes, while the y-axis signifies the semantic similarity score, ranging from 0 (no similarity) to 1 (perfect similarity). These box plots display the median, quartiles, and range of the semantic similarity scores within each class. Overall, the figure provides a concise and informative visual representation of the performance of various question-answering approaches in terms of semantic similarity.\n\n![Box Plots](/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_13.png)\n\n\n<div class=\"content-ad\"></div>\n\n분포를 통해 다양한 패턴이 나타납니다:\n\n- OOB (패턴 I): 모든 클래스에서 다양한 점수 범위가 나타나며, 부분적으로 정확하지 않은 답변이 더 많이 나타납니다. 이는 즉, 기본 설정의 (OOB) 방식이 우리의 테스트 세트에서 일관되게 정확한 답변을 제공하는 데 어려움을 겪는 것을 시사합니다.\n- OOB + 필터 (패턴 II): OOB 방식보다 특히 잘못된 답변을 줄이는 측면에서 뚜렷한 향상이 나타납니다. 분포는 더 높은 유사도 점수 쪽으로 기울어져 있어 필터를 적용한 후 정확도가 향상되었음을 나타냅니다. 패턴 II의 잘못된 범주에 대한 흥미로운 관찰은 의미론적 유사성 분산이 감소하고 0.5에서 0.6 사이로 중심을 이동한다는 것입니다. 이는 패턴 I과 달리 의미적 유사성 점수의 범위가 더 넓었던 0.5에서 1까지 였던 것과 비교했을 때입니다.\n- 추출 세그먼트 (패턴 III): 이전 두 가지 방식과 비교했을 때 올바른 및 부분적으로 올바른 답변의 비율이 더 높은 개선을 보여줍니다. 이는 문맥에서 관련 세그먼트를 추출하는 것이 OOB 모델 또는 기본 필터링에 의존하는 것보다 효과적인 전략임을 시사합니다.\n- 추출 답변 (패턴 IV): 가장 우수한 성능을 달성하며, 가장 많은 올바른 답변과 가장 적은 잘못된 답변이 나타납니다. 이는 문맥에서 완전한 답변을 직접 추출하는 것이 가장 의미론적으로 유사하고 정확한 응답을 제공한다는 것을 나타냅니다.\n\n지금까지 RAG 파이프라인의 두 가지 주요 단계인 검색 및 답변 생성을 평가하는 방법에 대해 논의했습니다. 검색에서는 검색된 문서의 관련성을 평가하는 데 집중했습니다. 이를 더 확장하여 페이지 또는 검색된 문맥의 관련성을 평가할 수도 있습니다. 그러나 이를 위해 이에 대한 정답 정보를 보유하고 있어야 합니다.\n\n답변 품질의 경우, Ragas와 같은 다른 오픈 소스 대안 프레임워크를 사용하거나 해당하는 경우 Vertex AI의 Rapid Evaluation API를 활용할 수도 있습니다. Rapid Evaluation 서비스를 사용하면 점별 및 쌍별로 여러 메트릭을 통해 LLM을 평가할 수 있습니다. 추론 시간 입력, LLM 응답 및 추가 매개변수를 제공하면 서비스가 평가 작업과 관련된 특정 메트릭을 반환합니다.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n이 가이드에서는 금융 분야에서 업계 수준의 RAG 파이프라인을 생성하기 위해 Vertex AI Search의 활용을 탐색했습니다. 금융 데이터 세트의 삽입 및 색인화를 자세히 다루었으며 이러한 색인을 활용하여 다양한 방법으로 검색을 수행했습니다. Vertex AI Search가 제공하는 다양한 컨텍스트 유형에 액세스하고 비교를 위해 네 가지 다른 RAG 파이프라인을 생성했습니다. 또한 대체 파이프라인 구성을 살펴보았습니다.\n\n그런 다음 검색 성능 메트릭 및 답변 품질 평가 기술을 평가했습니다. 결과를 비교함으로써 다른 접근 방식의 효과에 대한 유익한 통찰력을 얻었습니다.\n\n우리의 주요 발견은 Vertex AI Search가 표준 및 완전히 사용자 정의 가능한 RAG 솔루션을 구축하기 위한 포괄적인 기능 세트를 제공한다는 것입니다. 이 플랫폼은 어떠한 선택한 도메인 내에서 정보 검색 및 질문 답변 작업을 혁신적으로 간소화합니다. 앞으로의 게시물에서는 Vertex AI의 미개척 기능을 활용한 다른 패턴을 탐색할 예정입니다.\n\n<div class=\"content-ad\"></div>\n\n이 가이드의 내용을 완전히 이해하려면 공유 코드 리포지토리를 설정하는 것을 권장합니다. 작업 환경에서 지침을 따라 실험과 결과를 복제하세요. 이렇게 하면 여러분이 손쉽게 자신의 사용 사례에 적응하고 확장할 수 있습니다!\n\n기사를 읽어 주신 것과 참여해 주신 것에 감사드립니다. 팔로우와 박수가 많은 의미를 갖습니다. 본문이나 공유 소스 코드에 관한 질문이 있으면 언제든지 arunpshankar@google.com 또는 shankar.arunp@gmail.com으로 연락해 주세요. 또한 https://www.linkedin.com/in/arunprasath-shankar/에서 저를 찾을 수도 있습니다.\n\n모든 피드백과 제안을 환영합니다. 대규모 기계 학습, NLP/NLU에 관심이 많으시고 협업을 열망한다면 기쁘게 연결할 수 있습니다. 더불어 Google Cloud, VertexAI, 그리고 NLP/ML에서의 다양한 생성적 AI 구성 요소와 응용 프로그램을 이해하려는 개인, 스타트업 또는 기업이시라면 도와 드리겠습니다. LinkedIn에서 연락 주시기 바랍니다!\n","ogImage":{"url":"/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_0.png"},"coverImage":"/assets/img/2024-05-27-ArchitecturalBlueprintsforRAGAutomationAdvancedDocumentUnderstandingusingVertexAISearch_0.png","tag":["Tech"],"readingTime":35},{"title":"질문-응답 시스템 주요 아키텍처 개요","description":"","date":"2024-05-27 15:08","slug":"2024-05-27-Question-AnsweringSystemsOverviewofMainArchitectures","content":"\n## 확장 가능한 정보 검색 시스템을 구축하는 디자인 방식 탐색\n\n![Question-AnsweringSystemsOverviewofMainArchitectures](/assets/img/2024-05-27-Question-AnsweringSystemsOverviewofMainArchitectures_0.png)\n\n# 소개\n\n최근 몇 년 동안 질문-응답 애플리케이션이 강력하게 등장했습니다. 현대적인 검색 엔진, 챗봇 또는 단순히 대량의 테마 데이터에서 관련 정보를 검색하는 애플리케이션 등 어디에서나 찾을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이름에서 알 수 있듯이 QA 응용 프로그램의 목적은 텍스트 단락에서 주어진 질문에 대한 가장 적합한 답변을 검색하는 것입니다. 처음 몇 가지 방법은 키워드 또는 정규 표현식을 사용한 단순한 검색으로 이뤄졌습니다. 당연히 이러한 접근 방식은 최적이 아닙니다: 질문이나 텍스트에 오타가 있을 수 있습니다. 게다가, 정규 표현식은 쿼리에서 주어진 단어와 관련이 높은 유의어를 감지할 수 없습니다. 이러한 접근 방식은 결과적으로, 트랜스포머와 벡터 데이터베이스 시대에 특히 강력한 새로운 방법론으로 대체되었습니다.\n\n이 기사에서는 현대적이고 확장 가능한 QA 응용 프로그램을 구축하기 위한 세 가지 주요 설계 접근 방식을 다룹니다.\n\n![이미지](/assets/img/2024-05-27-Question-AnsweringSystemsOverviewofMainArchitectures_1.png)\n\n# 추출형 QA\n\n<div class=\"content-ad\"></div>\n\n추출형 QA 시스템은 세 가지 구성 요소로 구성되어 있습니다:\n\n- 검색기 (Retriever)\n- 데이터베이스 (Database)\n- 리더 (Reader)\n\n![이미지](/assets/img/2024-05-27-Question-AnsweringSystemsOverviewofMainArchitectures_2.png)\n\n먼저, 질문이 검색기에 입력됩니다. 검색기의 목표는 질문에 해당하는 임베딩을 반환하는 것입니다. 간단한 벡터화 방법인 TF-IDF, BM-25부터 더 복잡한 모델까지 다양한 구현이 있을 수 있습니다. 대부분의 경우 트랜스포머와 같은 모델 (BERT)이 검색기에 통합됩니다. 단순한 단어 빈도수만을 의존하는 날것한 접근 방식과는 달리, 언어 모델은 텍스트의 의미를 캡처할 수 있는 밀집된 임베딩을 구축할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n질문에서 쿼리 벡터를 얻은 후, 외부 문서 모음에서 가장 유사한 벡터를 찾는 데 사용됩니다. 각 문서에는 질문에 대한 답변이 포함될 확률이 있습니다. 보통 문서 모음은 학습 단계에서 처리되어 리트리버에 전달되어 해당 문서에 대한 임베딩을 출력합니다. 이러한 임베딩은 일반적으로 효과적인 검색을 제공할 수 있는 데이터베이스에 저장됩니다.\n\n쿼리 벡터와 가장 유사한 상위 k개의 데이터베이스 벡터를 검색함으로써, 원래 텍스트 표현을 사용하여 다른 구성 요소인 리더가 답변을 찾습니다. 리더는 초기 질문을 취하고 k개의 검색된 문서 각각에서 텍스트 단락에서 답변을 추출하고 이 답변이 올바른 확률을 반환합니다. 가장 높은 확률을 갖는 답변이 마지막으로 독점 QA 시스템에서 반환됩니다.\n\n# 오픈 생성형 QA\n\n오픈 생성형 QA는 추출형 QA와 정확히 동일한 프레임워크를 따릅니다. 단, 리더 대신 제너레이터를 사용한다는 점에서 차이가 있습니다. 리더와 달리 제너레이터는 텍스트 단락에서 답변을 추출하지 않습니다. 대신, 질문과 텍스트 단락에서 제공된 정보를 사용하여 답변이 생성됩니다. 추출형 QA의 경우와 마찬가지로 가장 높은 확률을 갖는 답변이 최종 답변으로 선택됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Question-Answering Systems Overview of Main Architectures](/assets/img/2024-05-27-Question-AnsweringSystemsOverviewofMainArchitectures_3.png)\n\n비슷한 구조를 가지고 있기 때문에, 추출형 또는 개방적 생성 구조를 사용하는 것이 더 나은 시점에 대한 질문이 생길 수 있습니다. 독자 모델이 상대적 정보를 포함한 텍스트 단락에 직접 액세스할 수 있는 경우, 일반적으로 정확하고 간결한 답변을 검색하는 데 충분히 똑똑합니다. 반면에, 대부분의 경우 생생 생성 모델은 주어진 맥락에 대해 더 긴 범용 정보를 생성하는 경향이 있습니다. 이는 질문이 개방형 형태로 제시될 때 유익할 수 있지만, 짧거나 정확한 답변이 예상되는 상황에는 해당되지 않을 수 있습니다.\n\n## 검색 보조 생성\n\n최근 기계 학습에서 \"검색 보조 생성\" 또는 \"RAG\"라는 용어의 인기가 급증했습니다. 간단히 말하면, 이는 개방형 생성 QA 시스템에 기반을 둔 LLM 응용 프로그램을 생성하는 프레임워크입니다.\n\n\n<div class=\"content-ad\"></div>\n\n경우에 따라서, LLM 응용 프로그램이 여러 지식 도메인과 작동하는 경우, RAG 검색기는 주어진 쿼리에 가장 관련있는 지식 도메인을 식별하려는 보조 단계를 추가할 수 있습니다. 식별된 도메인에 따라 검색기는 다양한 작업을 수행할 수 있습니다. 예를 들어, 특정 도메인에 속하는 쿼리의 경우 해당 도메인의 벡터 데이터베이스를 사용하여 쿼리에 가장 관련있는 정보를 검색할 수 있습니다.\n\n이 기술은 모든 문서가 아닌 특정 문서 하위 집합을 통해 검색하기 때문에 검색 프로세스를 더 빠르게 만듭니다. 게다가, 검색을 더 신뢰할 수 있게 만들 수 있습니다. 왜냐하면 최종 검색된 컨텍스트가 더 많은 관련 문서에서 구성되기 때문입니다.\n\n![이미지](/assets/img/2024-05-27-Question-AnsweringSystemsOverviewofMainArchitectures_4.png)\n\n## 닫힌 생성 QA\n\n<div class=\"content-ad\"></div>\n\n닫힌 생성형 QA 시스템은 외부 정보에 액세스할 수 없으며 질문에서 제공된 정보만 사용하여 답변을 생성합니다.\n\n![이미지](/assets/img/2024-05-27-Question-AnsweringSystemsOverviewofMainArchitectures_5.png)\n\n닫힌 QA 시스템의 명백한 이점은 대규모 외부 문서 집합을 검색할 필요가 없어지므로 파이프라인 시간을 줄일 수 있다는 점입니다. 그러나 교육 및 정확도 측면에서는 비용이 발생합니다: 생성기는 충분히 강력하고 적절한 답변을 생성할 수 있도록 충분한 학습 지식을 갖추어야 합니다.\n\n닫힌 생성형 QA 파이프라인은 다른 단점도 있습니다. 생성기는 교육을 받은 데이터에 후에 나타난 정보를 알지 못합니다. 이 문제를 해결하기 위해 생성기는 최신 데이터셋에 다시 교육을 받을 수 있습니다. 그러나 생성기는 일반적으로 수백만 또는 수십억 개의 매개변수를 가지고 있기 때문에 이러한 교육은 매우 많은 자원을 필요로 합니다. 이와 비교하여 추출형 QA 및 개방형 생성형 QA 시스템으로 동일한 문제를 다루는 것은 훨씬 간단합니다. 벡터 데이터베이스에 새로운 콘텍스트 데이터를 추가하는 것만으로 충분합니다.\n\n<div class=\"content-ad\"></div>\n\n대부분의 경우 일반적인 질문이 있는 애플리케이션에서는 폐쇄형 생성 접근 방식이 사용됩니다. 매우 구체적인 도메인의 경우, 폐쇄형 생성 모델의 성능이 저하되는 경향이 있습니다.\n\n# 결론\n\n본 문서에서 QA 시스템 구축을 위한 세 가지 주요 접근 방법을 발견했습니다. 이들 중에서 절대적인 승자는 없습니다. 각각이 각자의 장단점을 갖고 있습니다. 그러므로 먼저 입력 문제를 분석하고 올바른 QA 아키텍처 유형을 선택하여 더 나은 성능을 내도록 하는 것이 선행되어야 합니다.\n\n머신 러닝 분야에서 Open Generative QA 아키텍처가 현재 트렌드로 떠오르고 있으며, 특히 최근에 등장한 혁신적인 RAG 기술들과 함께 그 인기가 높아지고 있습니다. NLP 엔지니어라면 지금 당장 RAG 시스템에 주목하는 것이 좋습니다. 이 시스템들은 최근에 엄청난 속도로 발전하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 자료\n\n- 질의 응답 | Hugging Face\n\n모든 이미지는 특별히 언급되지 않는 한 저자에 의해 제공됩니다.\n","ogImage":{"url":"/assets/img/2024-05-27-Question-AnsweringSystemsOverviewofMainArchitectures_0.png"},"coverImage":"/assets/img/2024-05-27-Question-AnsweringSystemsOverviewofMainArchitectures_0.png","tag":["Tech"],"readingTime":5},{"title":"성의 기초","description":"","date":"2024-05-27 15:07","slug":"2024-05-27-TheVeryFoundationofSex","content":"\n\n![image](/assets/img/2024-05-27-TheVeryFoundationofSex_0.png)\n\n나의 남자친구는 정말 질투심이 많은 사람인데, 그게 맘에 든다. 최근에 그는 약물을 먹여 적나라하게 만지작거리며 자국들을 즐기던 야한 이야기를 나눠주었다. 그 결과로 그는 세로필름으로 감싸져 있고 누군가에 의해 최면을 걸리게 된 것이었다. 그는 몇 년간 에로틱 탐험의 세계에 발을 들인 적이 있었는데, 아마도 나와 같은 기간 동안 활동했던 것 같다.\n\n매우 섹시한 최면 비디오를 보려면 여기를 클릭해주세요: [영상 링크](https://youtu.be/mkcTmNe6kQU?si=LktapkZGHkdnbZFW)\n\n특히 격렬한 세션 끝에 하룻밤, 그의 품에서 잠이 들었다. 그가 부드럽게 내 등을 쓸어 주며, 나는 몸을 돌리고 그가 속삭였다. \"너는 완전 훌륭한 사람이 됐어.\" 그 말에 오싹함이 느껴졌다. 왜냐하면 그가 몇 년 동안 내 블로그를 열심히 읽고 있었다는 것을 알고 있었기 때문이다. 몇 년 동안! 그리고 나는 보통 그저 내 이중 인생에 관해 아무것도 모르는 무술에서 만나는 바보들의 성적 유혹과 데이트 제의를 무시하곤 했다.\n\n\n<div class=\"content-ad\"></div>\n\n제가 여기 있을 때 그는 내 마음을 자극하고, 왜 그가 일찍이 연락을 한 때 나에게 더 빨리 손을 내밀지 않았는지 궁금하게 만들고 있습니다. 몇 년 전에 그가 처음 연락했을 때 응답했다면 어떠했을까요? 그러나 그 시절에는 준비가 되지 않았습니다. 제가 박사과정 첫 해에 있었고, 뇌졸중 회복 중이었으며, Peter가 남긴 상처를 극복하고 있었습니다. 그것은 복합적인 외상의 시기였습니다.\n\n그렇다면 여성과 남성, 그들이 어떻게 흥분되는지와 어떤 관련이 있을까요? 여성은 일반적으로 남성에 비해 정서적 관련에서 흥분하는 경향이 더 있습니다. 남성은 주로 신체적 자극에 더 많이 반응할 수 있지만, 여성은 정서적 연결과 강도를 통해 흥분하는 경향이 있습니다.\n\n여성은 감정적인 강도에 끌리는데, 그것이 전체 경험을 높여주기 때문입니다. 신체적 쾌락뿐만 아니라, 원하는 대로 느껴지고 소중하게 여기며, 더 깊은 수준에서 이해받는 경험에 관련이 있습니다. 남성이 감정을 섬세하게 성적 애착으로 연결할 수 있을 때, 그는 단순한 신체성을 초월하는 강력한 연결을 만들어냅니다.\n\nNLP(신경 언어 프로그래밍)를 사용하여 이러한 감정적 강도를 만들려면, 이 단계를 따르세요 (기본으로 돌아가 보겠습니다):\n\n<div class=\"content-ad\"></div>\n\n- 소통을 조절해요\n\n그녀의 몸짓, 목소리 톤, 말의 속도를 반영하여 깊은 연결을 형성하세요. 이렇게 하면 그녀가 이해받고 편안해 집니다.\n\n- 앵커링 활용\n\n일부 접촉, 말, 또는 제스처를 긍정적인 감정과 흥분과 연관짓습니다. 예를 들어, 센슈얼한 말을 속삭이는 동안 부드럽게 그녀의 손목에 손을 대어 접촉과 그녀의 흥분 사이에 강한 연결을 만들어 보세요.\n\n<div class=\"content-ad\"></div>\n\n3. 감각을 자극하는 표현 사용\n\n흥분을 증폭시키는 상황을 묘사하는 데 생동감과 감각이 풍부한 언어를 사용하세요. 그녀의 피부가 느껴지는 방법, 숨을 가쁘게 하게 되는 것, 그녀의 몸이 어떻게 반응하는지 이야기해 보세요.\n\n4. 절제와 리딩\n\n그녀의 감정 상태에 맞추어 시작한 후 서서히 그녀를 흥분하고 강렬한 상태로 이끌어보세요. 스토리텔링, 판타지 묘사 또는 자신의 욕망을 공유하는 것으로도 가능합니다.\n\n<div class=\"content-ad\"></div>\n\n5. 언어 패턴 활용하기\n\n임베디드 명령, 비유, 이중 굴레와 같은 최면적인 언어 패턴을 사용하여 그녀의 생각과 감정을 이끌어보세요. 예를 들어, \"당신이 편안해질수록, 더욱 흥분하고 있는 자신을 발견할 수도 있습니다.\"\n\n6. 기대감 조성하기\n\n조금씩 강도를 높이며 만지작거리고 긴장감을 쌓아가며 기대감을 조성하세요. 그녀가 더 바라게 만들어 강력한 감정적, 신체적 반응을 일으키게 하세요.\n\n<div class=\"content-ad\"></div>\n\n이러한 기술을 숙달함으로써 여러분은 그녀를 매혹시키고 격렬한 감정의 매듭을 만들어 흥분시키며, 잊지 못할 만족스러운 경험을 창출할 수 있습니다.\n\n저희는 다가오는 60일간의 NLP 훈련에서 이 기본적인 방법들을 가르치는 것에 열정적입니다. 여러분이 함께해 주시기를 강력하게 추천드리며, 저희는 이를 접근하기 쉽게 슬라이딩 스케일로 제공하고 있습니다.\n\n제 소개를 조금 드리자면, 저는 Robert Dilts 밑에서의 NLP 트레이너 자격증을 소지하고 있으며, 제 마스터 프랙티셔너 및 프랙티셔너 자격증은 NLPCA(팀 할볼름과 로버트 해리슨에 의해 양성된)에서 받았습니다. 저는 2003년부터 NLP를 실천해 오고 있습니다. 저의 최면 자격증은 National Guild를 통해 받았으며, 저의 박사 학위는 아리조나 대학과 프레스코트 대학에 인정받았습니다.\n\n거기에 더하여, 저는 kink 씬에서 20년 이상의 경험을 갖고 있습니다. 여러분이 저희와 함께 훈련할 수 있는 이 기회를 강력하게 추천드립니다.\n","ogImage":{"url":"/assets/img/2024-05-27-TheVeryFoundationofSex_0.png"},"coverImage":"/assets/img/2024-05-27-TheVeryFoundationofSex_0.png","tag":["Tech"],"readingTime":3},{"title":"대용량 데이터셋 특성 선택을 혁신하는 강화 학습","description":"","date":"2024-05-27 15:05","slug":"2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning","content":"\n## 아주 큰 데이터셋을 다룰 때 특성 선택에 강화 학습의 힘을 활용해 보세요\n\n기계 학습 모델에 대한 특성 선택을 변화시키는 강화 학습의 힘을 경험해보세요. 실용적인 예시와 전용 Python 라이브러리를 활용하여 이 혁신적인 접근 방식의 과정, 구현, 그리고 혜택에 대해 알아보세요.\n\n[이미지](/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_0.png)\n\n특성 선택은 기계 학습 모델을 구축하는 과정에서 결정적인 단계입니다. 모델에 좋은 특성을 선택하면 원하는 작업에 대한 결과를 향상시킬 수 있습니다. 실제로 특성은 노이즈를 추가하여 모델을 방해할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n기능을 선택하는 것은 특히 고차원 데이터 세트를 다룰 때 더 중요합니다. 이는 모델이 더 빨리 그리고 더 잘 배울 수 있게 합니다. 그 아이디어는 최적의 기능 수와 가장 의미 있는 것들을 찾는 데 있습니다.\n\n이 글에서는 이 문제를 다루고 새롭게 구현된 기능 선택 방법을 소개할 것입니다. 다양한 기능 선택 프로세스가 있지만 이미 다뤄진 글들이 많기 때문에 여기서 소개하지 않겠습니다. 저는 강화 학습 전략을 사용한 기능 선택에 초점을 맞출 것입니다.\n\n먼저, 강화 학습과 특히 Markov Decision Process에 대해 다룰 것입니다. 이는 데이터 과학 분야에서 매우 새로운 접근법이며 특히 기능 선택 목적으로 사용됩니다. 그 다음으로, 이를 구현하고 파이썬 라이브러리(FSRLearning)를 설치하고 사용하는 방법을 소개할 것입니다. 마지막으로, 이 구현의 효율성을 검증할 것입니다. 래퍼나 필터링과 같은 가능한 기능 선택 접근법 중에서, 강화 학습이 가장 강력하고 효율적입니다.\n\n이 글의 목표는 구체적이고 실제 문제를 위한 구현에 중점을 두는 것입니다. 이 문제의 이론적 측면은 예제를 통해 간단히 설명되지만 참고 자료가 끝에 제공될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 강화 학습: 특성 선택을 위한 마르코프 의사 결정 문제\n\n강화 학습 (RL) 기법이 게임 해결과 같은 문제에 매우 효율적일 수 있다는 것이 입증되었습니다. RL의 개념은 마르코프 의사 결정 과정 (MDP)에 기반을 두고 있습니다. 여기서 중요한 것은 MDP를 깊이 이해하는 것이 아니라 그 작동 방식과 어떻게 우리 문제에 유용할지에 대한 일반적인 개념을 이해하는 것입니다.\n\nRL 뒤에 숨겨진 단순한 아이디어는 에이전트가 알 수 없는 환경에서 시작한다는 것입니다. 이 에이전트는 작업을 완료하기 위해 조치를 취해야 합니다. 에이전트의 현재 상태 및 이전에 선택한 조치에 따라, 에이전트는 일부 조치를 선택하기 쉬울 것입니다. 도달한 각 새로운 상태와 취한 조치에 대해, 에이전트는 보상을 받게 됩니다. 그래서 여기 특성 선택을 위해 정의해야 할 주요 파라미터들이 있습니다:\n\n- 상태란 무엇인가요?\n- 조치란 무엇인가요?\n- 보상은 무엇인가요?\n- 어떻게 조치를 선택하나요?\n\n<div class=\"content-ad\"></div>\n\n첫째, 상태는 데이터 세트에 존재하는 기능들 중의 하위 집합에 불과합니다. 예를 들어, 데이터 세트에 세 가지 기능 (나이, 성별, 키)와 하나의 레이블이 있다면, 가능한 상태는 다음과 같습니다:\n\n```js\n[]                                              --> 빈 집합\n[나이], [성별], [키]                             --> 1개 기능 집합\n[나이, 성별], [성별, 키], [나이, 키]             --> 2개 기능 집합\n[나이, 성별, 키]                                --> 모든 기능 집합\n```\n\n상태에서는 기능의 순서가 중요하지 않으며, 이를 뒤쪽에서 조금 더 설명하겠습니다. 우리는 이를 세트로 간주하고 특징들의 목록으로 간주해서는 안 됩니다.\n\n행동에 대해서는, 하위 집합으로부터 현재 상태보다 한 가지 미탐색 기능이 더 추가된 다른 하위 집합으로 이동할 수 있습니다. 기능 선택 문제에서, 한 행동은 현재 상태에서 이미 탐색되지 않은 기능을 선택하고 다음 상태에 추가하는 것입니다. 여기에 가능한 행동의 예시가 있습니다:\n\n<div class=\"content-ad\"></div>\n\n\n[나이] -> [나이, 성별]\n[성별, 키] -> [나이, 성별, 키]\n\n\n이제 불가능한 행동의 예를 살펴봅시다:\n\n\n[나이] -> [나이, 성별, 키]\n[나이, 성별] -> [나이]\n[성별] -> [성별, 성별]\n\n\n우리는 상태와 행동을 정의했지만 보상은 정의하지 않았습니다. 보상은 상태의 품질을 평가하는 데 사용되는 실수입니다. 예를 들어 로봇이 미로의 출구에 도달하려고 노력하고 다음 행동으로 출구로 가기로 결정한다면, 이 행동에 대한 보상은 \"좋음\"이 될 것입니다. 만일 함정으로 가기로 다음 행동을 선택하면 보상은 \"나쁨\"이 될 것입니다. 보상은 이전 조치에 대한 정보를 제공하는 값입니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n기능 선택 문제에서 흥미로운 보상은 새로운 기능을 추가함으로써 모델에 추가되는 정확도 값일 수 있습니다. 다음은 보상이 계산되는 방법의 예시입니다:\n\n```js\n[나이] --> 정확도 = 0.65\n[나이, 성별] --> 정확도 = 0.76\n보상(성별) = 0.76 - 0.65 = 0.11\n```\n\n첫 방문한 각 상태마다 분류기가 해당 기능 집합으로 훈련됩니다. 이 값은 상태에 저장되며 분류기의 훈련은 상태가 나중에 다시 방문되더라도 한 번만 발생합니다. 분류기는 기능의 순서를 고려하지 않습니다. 이것이 우리가 이 문제를 트리가 아닌 그래프로 볼 수 있는 이유입니다. 이 예시에서 모델에 새로운 기능으로 성별 선택 작업의 보상은 현재 상태와 다음 상태의 정확도 차이입니다.\n\n<img src=\"/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_1.png\" />\n\n<div class=\"content-ad\"></div>\n\n위의 그래픽에서 각 기능이 숫자에 매핑되었습니다 (예: \"나이\"는 1, \"성별\"은 2이고 \"키\"는 3입니다). 최적의 세트를 찾기 위해 다른 메트릭을 사용하는 것이 완전히 가능합니다. 많은 비즈니스 응용 프로그램에서 정확도보다 재현율이 더 중요하게 여겨집니다.\n\n다음 중요한 질문은 현재 상태에서 다음 상태를 선택하는 방법이거나 환경을 어떻게 탐사하는지입니다. 매우 복잡한 문제가 될 수 있기 때문에 가장 최적의 방법을 찾아야 합니다. 실제로, 문제에서 10개의 기능이 있는 경우 모든 가능한 기능 세트를 단순히 탐색하면 상태의 수가\n\n```js\n10! + 2 = 3,628,802개의 가능한 상태\n```\n\n입니다.\n\n+2는 빈 상태와 모든 가능한 기능을 포함하는 상태를 고려했기 때문입니다. 이 문제에서는 정확도를 극대화하는 기능 세트를 얻기 위해 모든 상태에서 동일한 모델을 교육해야 할 것입니다. 강화 학습 접근 방식에서는 모든 상태를 방문할 필요가 없고 이미 방문한 상태로 이동할 때마다 모델을 교육할 필요가 없을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 문제에 대한 몇 가지 중지 조건을 결정해야 했고, 이를 나중에 자세히 설명할 것입니다. 현재 epsilon-탐욕 상태 선택이 선택되었습니다. 이것은 현재 상태에서 epsilon(0과 1 사이, 주로 0.2 정도)의 확률로 다음 동작을 무작위로 선택하고, 그렇지 않은 경우에는 함수를 최대화하는 동작을 선택하는 아이디어입니다. 특징 선택에 대한 함수는 각 특징이 모델의 정확도에 기여한 보상의 평균입니다.\n\nEpsilon-탐욕 알고리즘에는 두 단계가 포함됩니다:\n\n- 무작위 단계: epsilon의 확률로, 현재 상태의 가능한 이웃 중에서 다음 상태를 무작위로 선택합니다 (균일하게 또는 소프트맥스 선택이라고 상상할 수 있음)\n- 탐욕 단계: 현재 상태에 추가된 기능이 모델의 정확도에 대한 최대 기여를 갖도록 다음 상태를 선택합니다. 시간 복잡성을 줄이기 위해, 각 특징에 대한 이러한 값을 포함하는 목록을 초기화했습니다. 이 목록은 특징이 선택될 때마다 업데이트됩니다. 업데이트는 다음 공식 덕분에 매우 최적화되었습니다:\n\n\n<img src=\"/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_2.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n- AORf: 기능 \"f\"가 가져온 보상의 평균\n- k: \"f\"가 선택된 횟수\n- V(F): 특징 집합 F의 상태 값 (이 기사에서는 상세히 설명되지 않음)\n\n전체 아이디어는 어떤 기능이 모델에 가장 높은 정확도를 제공했는지 찾는 것입니다. 이것이 바로 우리가 여러 다른 환경에서 여러 상태를 검토하여 모델에 대한 가장 전반적인 정확도 값을 평가해야 하는 이유입니다.\n\n마지막으로 두 가지 중지 조건을 설명하겠습니다. 알고리즘이 방문하는 상태의 수를 최소화하는 것이 목표이기 때문에 이를 주의 깊게 살펴봐야 합니다. 방문한 상태가 적을수록 서로 다른 기능 집합으로 학습해야 할 모델의 양이 줄어듭니다. 정확도를 얻기 위해 모델을 학습하는 것은 시간과 계산 능력면에서 가장 비용이 많이 드는 단계입니다.\n\n- 알고리즘은 모든 기능을 포함하는 집합인 최종 상태에서 반드시 중지됩니다. 이 상태에 도달하는 것을 피하기 위해 모델을 학습하는 데 가장 비용이 많이든다.\n- 또한, 연속적으로 값이 감소하는 방문된 상태들의 시퀀스가 발생하면 그래프를 조회하는 것을 중지합니다. 데이터 세트의 총 기능 수의 제곱근 이후에는 계속 탐색을 중단하는 임계값이 설정되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n문제의 모델링이 설명되었으므로, 이제 파이썬에서의 구현에 대해 자세히 설명하겠습니다.\n\n## 강화 학습을 사용한 특성 선택을 위한 파이썬 라이브러리\n\n이 문제를 해결하는 파이썬 라이브러리가 있습니다. 이 부분에서 그 동작 방식을 설명하고 그것이 효율적인 전략임을 입증하겠습니다. 또한, 이 문서는 설명서로 작용하여 해당 부분이 끝나면 이 라이브러리를 여러분의 프로젝트에 사용할 수 있을 것입니다.\n\n## 1. 데이터 전처리\n\n<div class=\"content-ad\"></div>\n\n방문한 상태의 정확도를 평가해야 하기 때문에, 해당 기능 및 이 기능 선택 작업에 사용된 데이터를 모델에 제공해야 합니다. 데이터는 정규화되어야 하며 범주형 변수는 인코딩되어야 하며 가능한 적은 행을 가지고 있어야 합니다 (행이 적을수록 알고리즘은 더 빠릅니다). 또한, 기능과 일부 정수 사이의 매핑을 생성하는 것이 매우 중요합니다. 이 단계는 필수는 아니지만 매우 권장됩니다. 이 단계의 최종 결과는 예측할 수 있는 라벨과 함께 모든 기능이 포함된 DataFrame을 얻는 것입니다. 아래는 기준으로 사용된 데이터셋 예시입니다 (UCI Irvine Machine Learning Repository에서 확인할 수 있습니다).\n\n## 2. FSRLearning 라이브러리 설치 및 가져오기\n\n두 번째 단계는 라이브러리를 pip을 사용하여 설치하는 것입니다. 여기에 설치 명령어가 있습니다:\n\n```js\npip install FSRLearning\n```\n\n<div class=\"content-ad\"></div>\n\n라이브러리를 가져오려면 아래 코드를 사용합니다:\n\n단순히 Feature_Selector_RL 객체를 만들어서 기능 선택기를 만들 수 있을 겁니다. 몇 가지 매개변수를 채워야 합니다.\n\n- feature_number (정수): DataFrame X에 있는 피처 수\n- feature_structure (사전): 그래프 구현을 위한 사전\n- eps (부동 소수 [0; 1]): 무작위 다음 상태를 선택할 확률, 0은 오직 탐욕 알고리즘이고 1은 오직 무작위입니다.\n- alpha (부동 소수 [0; 1]): 업데이트 속도를 제어합니다, 0은 거의 업데이트하지 않는 상태이고 1은 매우 업데이트됩니다.\n- gamma (부동 소수 (0, 1]): 다음 상태의 관찰을 조절하는 요소, 0은 근시적인 상태이고 1은 장기적인 행동을 나타냅니다.\n- nb_iter (정수): 그래프를 통해 이동할 수열의 수\n- starting_state (\"empty\" 또는 \"random\"): \"empty\"인 경우 알고리즘은 빈 상태에서 시작하고, \"random\"인 경우 그래프의 무작위 상태에서 시작합니다.\n\n모든 매개변수가 튜닝될 수 있지만 대부분의 문제에 대해 일부 이터레이션만으로 충분합니다 (대략 100회) 그리고 epsilon 값이 0.2 정도면 충분합니다. 시작 상태는 그래프를 효율적으로 더 탐색하는 데 유용하지만 데이터셋에 매우 의존적일 수 있고 두 값 모두 테스트할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n마침내 다음 코드로 선택기를 매우 간단하게 초기화할 수 있습니다.\n\n알고리즘을 훈련하는 것은 대부분의 머신 러닝 라이브러리와 동일한 기초에 따라 매우 쉽습니다.\n\n다음은 출력 예시입니다:\n\n\n![Output Example](/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_3.png)\n\n\n<div class=\"content-ad\"></div>\n\n다음은 5-튜플 출력입니다:\n\n- 데이터프레임 X 내 특성의 인덱스(마치 맵핑처럼)\n- 특성이 관측된 횟수\n- 반복이 끝난 후 특성에 의해 가져온 보상의 평균\n- 특성의 중요도를 가장 적은 것부터 가장 중요한 것까지 순위로 나타낸 것(여기서 2는 가장 적은 중요도이고 7은 가장 중요한 특성입니다)\n- 전역적으로 방문된 상태의 수\n\n이 선택기의 또 다른 중요한 방법은 Scikit-Learn의 RFE 선택기와 비교하는 것입니다. X, y 및 선택기의 결과를 입력으로 사용합니다.\n\n선택 과정마다 RFE와 FSRLearning의 전역 측정 지표를 출력합니다. 또한 모델의 정확도에 대한 시각적 비교를 출력합니다. x축에는 선택된 특성 수가 있고 y축에는 정확도가 있습니다. 두 개의 수평 선은 각 방법의 정확도 중앙값입니다. 다음은 예시입니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n평균 기준 정확도: 0.854251012145749, 강화학습 정확도: 0.8674089068825909\n중간 기준 정확도: 0.8552631578947368, 강화학습 정확도: 0.868421052631579\nRFE보다 더 좋은 메트릭을 가진 변수 세트를 얻을 확률: 1.0\n두 곡선 사이의 면적: 0.17105263157894512\n```\n\n이 예시에서는 강화학습 방법이 항상 RFE보다 모델을 위한 더 좋은 특성 집합을 제공했습니다. 따라서 우리는 정렬된 특성 집합 중에서 확실히 선택할 수 있으며, 그것은 모델에 더 높은 정확도를 제공할 것입니다. 여러 번 모델과 비교자를 실행하여 매우 정확한 평가를 얻을 수 있지만 강화학습 방법이 항상 더 좋습니다.\n\n또 다른 흥미로운 방법은 get_plot_ratio_exploration입니다. 이는 지정된 이터레이션에 대해 이미 방문한 노드 수와 순서대로 방문한 노드 수를 비교하는 그래프를 그립니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n\n\n![그림1](/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_5.png)\n\n또한, 두 번째 중지 조건 덕분에 알고리즘의 시간 복잡도가 지수적으로 감소합니다. 따라서 기능의 수가 많더라도 수렴이 빨리 이루어질 것입니다. 아래 그림은 특정 크기의 집합이 방문된 횟수를 나타냅니다.\n\n![그림2](/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_6.png)\n\n모든 반복에서 알고리즘이 6개 이하의 변수를 포함하는 상태를 방문했습니다. 6개 이상의 변수를 넘어가면 도달한 상태의 수가 줄어드는 것을 볼 수 있습니다. 이는 작은 기능 집합으로 모델을 훈련하는 것이 큰 기능 집합보다 빠르기 때문에 좋은 동작입니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n# 결론 및 참고 자료\n\n전반적으로 RL 방법은 모델의 지표를 극대화하는 데 매우 효율적임을 알 수 있습니다. 항상 흥미로운 특성 하위 집합으로 빠르게 수렴합니다. 또한 FSRLearning 라이브러리를 사용하면 ML 프로젝트에서 이 방법을 매우 쉽고 빠르게 구현할 수 있습니다.\n\n프로젝트의 Github 저장소와 완벽한 문서는 여기에서 확인할 수 있습니다.\n\n문의 사항이 있으시면 직접 링크드인에서 연락하실 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 라이브러리는 다음 두 논문을 참고하여 구현되었습니다:\n\n- Sali Rasoul, Sodiq Adewole, 및 Alphonse Akakpo, FEATURE SELECTION USING REINFORCEMENT LEARNING (2021), ArXiv\n- Seyed Mehdin Hazrati Fard, Ali Hamzeh, 및 Sattar Hashemi, Using reinforcement  learning to find an optimal set of features (2013), ScienceDirect\n\n","ogImage":{"url":"/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_0.png"},"coverImage":"/assets/img/2024-05-27-RevolutionizingLargeDatasetFeatureSelectionwithReinforcementLearning_0.png","tag":["Tech"],"readingTime":9},{"title":"이해할 수 있는 이상 탐지 Frequent Patterns Outlier Factor FPOF","description":"","date":"2024-05-27 15:02","slug":"2024-05-27-InterpretableOutlierDetectionFrequentPatternsOutlierFactorFPOF","content":"\n## 범주형 데이터를 지원하며 이상치를 감지하고, 이상치에 대한 설명을 제공하는 감지 방법\n\n이상치 탐지는 기계 학습에서 흔한 작업입니다. 구체적으로, 이는 지도 레이블이 없는 데이터를 분석하는 비지도학습의 한 형태입니다. 데이터 집합에서 다른 항목에 비해 이례적인 항목을 찾는 작업입니다.\n\n데이터에서 이상치를 식별하려는 이유는 여러 가지가 있을 수 있습니다. 분석 중인 데이터가 회계 기록이고 오류 또는 사기를 찾고자 한다면, 데이터에는 수동으로 각 거래를 검토하기에는 너무 많은 거래가 포함되어 있어서 소수의 거래를 조사해야 합니다. 가장 이상한 레코드를 찾아 이를 조사하는 것이 좋은 시작점일 수 있습니다. 이는 오류와 사기가 모두 드물어서 이상치로 드러날 것이라는 생각으로 이루어진 것입니다.\n\n다시 말하지만, 모든 이상치가 흥미로운 것은 아니지만, 오류와 사기는 아마도 이상치가 될 가능성이 있으므로 이를 찾을 때 이상치를 식별하는 것은 매우 실용적인 기술일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n또는 데이터에는 신용 카드 거래, 센서 읽기, 기상 측정, 생물학적 데이터 또는 웹 사이트 로그가 포함될 수 있습니다. 모든 경우에, 오류나 다른 문제를 시사하는 레코드를 식별하는 것이 유용할 수 있으며, 가장 흥미로운 레코드를 찾는 것도 도움이 될 수 있습니다.\n\n또한 이상값 탐지는 비즈니스나 과학적 발견의 일부로 사용되어 데이터와 데이터에 설명된 프로세스를 더 잘 이해하는 데 도움이 될 수 있습니다. 과학적 데이터의 경우, 가장 이례적인 레코드를 찾는 것이 종종 가장 과학적으로 흥미로울 수 있습니다.\n\n## 이상값 탐지에서 해석 가능성의 필요성\n\n분류 및 회귀 문제의 경우 해석 가능한 모델을 사용하는 것이 종종 바람직합니다. 이는 정확도가 낮아질 수 있지만(탭화면 데이터의 경우 가장 높은 정확도는 일반적으로 해석하기 어려운 부스트 모델에서 얻어집니다), 안전성이 높아집니다. 우리는 모델이 보지 못한 데이터를 어떻게 다루게 될지 알고 있습니다. 그러나 분류 및 회귀 문제의 경우에는 개별 예측이 이루어지는 이유를 이해할 필요가 없는 경우도 흔합니다. 모델이 상당히 정확하다면, 모델이 예측을 만들도록만 하는 것만으로 충분할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이상치 탐지를 수행하면 해석 가능성이 훨씬 더 높아집니다. 이상치 탐지기가 레코드를 매우 이상하다고 예측하는 경우, 왜 이렇게 예측되었는지 명확하지 않으면 해당 항목을 처리하는 방법을 알 수 없을 수도 있고, 그것이 이상할 수도 있는지 여부조차 알 수 없습니다.\n\n사실, 상황에 따라 이상치 탐지를 수행해도, 왜 이상치로 표시된 항목이 표시되었는지를 잘 이해하지 못하면 그 가치가 제한될 수 있습니다. 신용 카드 거래 데이터 세트를 확인하는 경우 이상치 탐지 루틴이 매우 이례적인 것으로 보이는 일련의 구매를 식별하고, 따라서 의심스럽다고 식별할 경우, 이것들을 효과적으로 조사할 수 있는 방법은 무엇인지 알고 있어야만 합니다. 경우에 따라 이것이 명백하거나, 시간을 들여 조사한 후에 분명해질 수도 있지만, 발견된 시점에서 이상 점의 특성이 명확하다면 훨씬 효율적입니다.\n\n분류 및 회귀와 마찬가지로 해석 가능성이 불가능한 경우, 사후 해설이라고 하는 것을 사용하여 예측을 이해하려는 시도를 하는 것이 종종 가능합니다. Feature importances, proxy models, ALE plots 등을 사용하는 XAI(Explainable AI) 기법을 사용합니다. 이들은 아주 유용하며 앞으로의 기사에서도 다룰 것입니다. 그러나, 처음부터 결과가 명확한 것의 장점도 아주 큽니다.\n\n이 기사에서는 특히 표 형식의 데이터에 초점을 맞추지만, 이후의 기사에서 다른 형식을 살펴볼 것입니다. 오늘날 흔히 사용되는 탭 데이터를위한 이상치 검출 알고리즘 중에는 Isolation Forests, Local Outlier Factor (LOF), KNNs, One-Class SVM 등 여러 가지가 있습니다. 이들은 종종 매우 잘 작동하지만, 불행히도 대부분의 경우 이상치를 찾은 이유에 대한 설명을 제공하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n대부분의 이상치 탐지 방법은 알고리즘 수준에서 이해하기 쉽지만, 그래도 어떤 레코드가 탐지기에 의해 높은 점수를 받았고 다른 레코드가 그렇지 않았는지를 결정하는 것은 어렵습니다. 예를 들어 금융 거래 데이터 세트를 Isolation Forest와 같은 방법으로 처리하면 가장 이롭은 레코드를 볼 수 있지만, 특히 테이블에 많은 특성이 있는 경우, 이상치가 드문 조합의 다중 특성을 포함하거나 이상치가 특성이 높지만 다중 특성이 다소 이상한 경우에는 왜 그런지 이해하는 것이 어려울 수 있습니다.\n\n## FPOF(Frequent Patterns Outlier Factor)\n\n지금은 최소한 빠르게라도 이상치 탐지와 해석에 대해 살펴보았습니다. 이 기사의 나머지는 저의 책인 파이썬에서의 이상치 탐지(https://www.manning.com/books/outlier-detection-in-python)에서 다루는 FPOF에 대한 발췌문입니다.\n\nFPOF(FP-outlier: Frequent pattern based outlier detection)은 이상치 탐지에 어느 정도의 해석 가능성을 제공할 수 있는 소수의 탐지기 중 하나이며, 이상치 탐지에서 더 많이 사용되어야 할 가치가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n카테고리 데이터와 함께 작동하도록 설계된 매력적인 속성을 갖고 있습니다. 대부분의 현실 세계의 표 데이터는 숫자 및 범주형 열을 모두 포함하는 혼합된 형태입니다. 그러나 대부분의 검출기는 모든 열이 숫자인 것으로 가정하며, 모든 범주형 열을 숫자로 인코딩해야 합니다(원핫, 서수 또는 다른 인코딩을 사용하여).\n\nFPOF와 같은 검출기가 데이터가 범주형이라고 가정하는 경우, 우리는 반대의 문제를 겪습니다: 모든 숫자 특성은 범주형 형식으로 변환되어야 합니다. 둘 중 어느 것이라도 사용 가능하지만, 데이터가 주로 범주형인 경우 FPOF와 같은 검출기를 사용할 수 있는 것이 편리합니다.\n\n그리고 이상 탐지를 수행할 때 일부 숫자 검출기와 일부 범주형 검출기를 함께 사용할 때 혜택이 있습니다. 불행히도, 비교적 적은 수의 범주형 검출기가 있기 때문에 FPOF는 이런 면에서도 유용하며, 해석력이 필요하지 않은 경우에도 유용합니다.\n\n## FPOF 알고리즘\n\n<div class=\"content-ad\"></div>\n\nFPOF는 테이블에서 빈발 아이템 세트(Frequent Item Sets, FISs)를 식별하여 작동합니다. 이것들은 하나의 특성에서 매우 흔한 값이거나 함께 자주 나타나는 여러 열에 걸친 값의 세트일 수 있습니다.\n\n거의 모든 테이블에는 상당수의 FIS가 포함되어 있습니다. 단일 값에 기초한 FIS는 한 열의 일부 값이 다른 값보다 매우 흔하기 때문에 항상 발생하며, 이는 거의 항상 사실입니다. 그리고 여러 열에 걸친 FIS는 열 사이에 연관성이 있을 때 발생합니다: 특정 값(또는 숫자 값의 범위)이 다른 열에서 다른 값(또는 다시 말해 숫자 값의 범위)과 연관이 있을 때 발생합니다.\n\nFPOF는 데이터셋에 많은 빈발 아이템 세트를 포함하고 있다는 아이디어에 기반을 두고 있습니다(거의 모든 데이터셋이 해당됩니다). 그러므로 대부분의 행에는 여러 빈발 아이템 세트가 포함되며, 정상 레코드에는 이상 값(이상치) 행보다 훨씬 더 빈발한 아이템 세트가 포함됩니다. 이를 활용하여 대부분의 행보다 훨씬 적고 훨씬 덜 빈발한 FIS를 포함하는 행을 이상치로 식별할 수 있습니다.\n\n## 실제 데이터 예시\n\n<div class=\"content-ad\"></div>\n\n실제로 FPOF를 사용하는 실제 예제를 살펴보면 OpenML의 SpeedDating 세트를 살펴봅니다 (https://www.openml.org/search?type=data&sort=nr_of_likes&status=active&id=40536, CC BY 4.0 DEED 라이선스).\n\nFPOF를 실행하는 것은 먼저 데이터 집합에서 FIS를 채굴하는 것으로 시작합니다. 이를 지원하기 위해 Python에서 사용할 수 있는 여러 라이브러리가 있습니다. 이 예제에서는 머신러닝을 위한 범용 라이브러리 인 mlxtend (https://rasbt.github.io/mlxtend/)를 사용합니다. 빈발 항목 집합을 식별하는 여러 알고리즘을 제공하며, 여기서는 apriori라는 알고리즘을 사용합니다.\n\n먼저 OpenML에서 데이터를 수집합니다. 보통 범주형 및 (binned) 숫자형 특성을 모두 사용할 것이지만, 여기서는 간단하게 일부 특성만 사용할 것입니다.\n\n언급했듯이 FPOF는 숫자형 특성의 binning을 필요로 합니다. 일반적으로 각 숫자 열에 대해 작은 수의 (5에서 20개 정도) 폭이 동일한 bin을 사용합니다. 이를 위해 pandas의 cut() 메서드가 편리합니다. 이 예제는 더 간단합니다. 여기서는 범주형 열만 다룹니다.\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom mlxtend.frequent_patterns import apriori\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nimport warnings\n\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\ndata = fetch_openml('SpeedDating', version=1, parser='auto')\ndata_df = pd.DataFrame(data.data, columns=data.feature_names)\n\ndata_df = data_df[['d_pref_o_attractive', 'd_pref_o_sincere',\n                   'd_pref_o_intelligence', 'd_pref_o_funny',\n                   'd_pref_o_ambitious', 'd_pref_o_shared_interests']]\ndata_df = pd.get_dummies(data_df)\nfor col_name in data_df.columns:\n    data_df[col_name] = data_df[col_name].map({0: False, 1: True})\n\nfrequent_itemsets = apriori(data_df, min_support=0.3, use_colnames=True)\n\ndata_df['FPOF_Score'] = 0\n\nfor fis_idx in frequent_itemsets.index:\n    fis = frequent_itemsets.loc[fis_idx, 'itemsets']\n    support = frequent_itemsets.loc[fis_idx, 'support']\n    col_list = (list(fis))\n    cond = True\n    for col_name in col_list:\n        cond = cond & (data_df[col_name])\n\n    data_df.loc[data_df[cond].index, 'FPOF_Score'] += support\n\nmin_score = data_df['FPOF_Score'].min()\nmax_score = data_df['FPOF_Score'].max()\ndata_df['FPOF_Score'] = [(max_score - x) / (max_score - min_score)\n                         for x in data_df['FPOF_Score']]\n```\n\n아프리오리 알고리즘은 모든 기능이 원-핫 인코딩되어 있어야합니다. 이를 위해 판다의 get_dummies() 메서드를 사용합니다.\n\n그런 다음 apriori 메서드를 호출하여 빈번한 항목 집합을 결정합니다. 이를 수행하기 위해 FIS가 나타나는 행의 최소 분수 인 최소 지원을 지정해야합니다. 이 값을 너무 높게 설정하면 강한 이상값도 FIS를 포함 시키지 않으며 FIS가 적게 포함 된 레코드를 어렵게 구별하게됩니다. 그리고 이 값을 너무 낮게 설정하면 FIS가 의미없을 수 있으며 이상값도 이니셜과 동일한 수의 FIS를 포함 할 수 있습니다. 낮은 최소 지원으로 apriori를 사용하면 매우 많은 수의 FIS를 생성 할 수 있으며 실행 속도가 느려지고 해석 가능성이 낮아질 수 있습니다. 이 예에서는 0.3을 사용합니다.\n\nFIS의 크기에 제한을 둘 수도 있고 때로는 그렇게합니다. 최소 및 최대 열 수 사이에 관련되도록 요구하는 여러 항목 집합의 크기에 제한을 둘 수 있으며 가장 관심 있는 이상값의 형태를 좁히는 데 도움이 될 수 있습니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n빈도가 높은 항목 집합은 지원과 열 값을 나타내는 팬더 데이터프레임에서 반환됩니다. 이 값은 원-핫 인코딩된 열 형식으로 표시되며 원본 열과 값을 나타냅니다.\n\n결과를 해석하기 위해 먼저 자주 등장하는 항목 집합(frequent_itemsets)을 확인할 수 있습니다. 각 FIS의 길이를 포함하려면:\n\n```js\nfrequent_itemsets['length'] = \\\n    frequent_itemsets['itemsets'].apply(lambda x: len(x))\n```\n\n총 24개의 FIS가 발견되었으며, 가장 긴 것은 세 가지 특징을 포함하고 있습니다. 다음 표는 지원에 따라 정렬된 처음 열 개의 행을 표시합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-27-InterpretableOutlierDetectionFrequentPatternsOutlierFactorFPOF_0.png)\n\n그런 다음 각 빈번한 항목 집합을 루프하고 지원하는 빈번한 항목 집합을 포함하는 각 행에 대해 점수를 증가시킵니다. 이는 선택적으로 길이가 더 큰 빈번한 항목 집합을 선호하도록 조정할 수 있습니다(예를 들어, 지원이 0.4이고 5개 열을 포함하는 FIS는, 그 외의 조건이 동일한 경우, 2개 열을 포함하는 지원이 0.4인 FIS보다 관련성이 더 높습니다), 하지만 여기서는 각 행의 FIS 수 및 지원을 간단히 사용합니다.\n\n실제로 이것은 정상성에 대한 점수를 생성하고 이상값이 아닙니다. 따라서 점수를 0.0과 1.0 사이로 정규화할 때 순서를 뒤집습니다. 이제 가장 높은 점수를 가진 행이 가장 강한 이상값입니다: 가장 적고 가장 일반적인 빈번한 항목 집합을 가진 행입니다.\n\n원래 데이터프레임에 점수 열을 추가하고 점수별로 정렬하면 가장 정상적인 행을 확인할 수 있습니다:\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-05-27-InterpretableOutlierDetectionFrequentPatternsOutlierFactorFPOF_1.png)\n\n이 행의 값들이 FISs와 잘 일치하는 것을 볼 수 있습니다. d_pref_o_attractive의 값은 [21-100]이며 이는 FIS(지원 0.36)와 일치합니다. d_pref_o_ambitious와 d_pref_o_shared_interests의 값은 각각 [0-15]로, 이 또한 FIS(지원 0.59)와 일치합니다. 다른 값들도 대부분 FIS들과 일치합니다.\n\n가장 이상한 행은 다음과 같이 표시됩니다. 이는 식별된 FIS들과 일치하지 않습니다.\n\n![이미지](/assets/img/2024-05-27-InterpretableOutlierDetectionFrequentPatternsOutlierFactorFPOF_2.png)\n\n\n<div class=\"content-ad\"></div>\n\n자주 나오는 항목 집합 자체가 꽤 이해하기 쉬우므로,이 방법은 상당히 해석 가능한 결과를 얻는 장점이 있습니다. 다만, 자주 나오는 항목 집합이 많이 사용되는 경우에는 이러한 장점이 적을 수 있습니다.\n\n![image](/assets/img/2024-05-27-InterpretableOutlierDetectionFrequentPatternsOutlierFactorFPOF_3.png)\n\n해석력은 향상될 수 있지만, 이상치는 \"포함하지 않는다\"는 방식으로 식별되기 때문에, 각 이상치의 점수를 설명하는 것은 해당 이상치에 포함되지 않는 모든 항목 집합을 나열하는 것을 의미합니다. 그러나 각 이상치를 설명하기 위해 모든 누락된 항목 집합을 나열하는 것이 반드시 필요한 것은 아닙니다. 누락된 가장 일반적인 항목 집합을 나열하는 것은 대부분의 목적에 충분할 것입니다. 행에 나타나는 항목 집합과 있는 항목 집합의 통계와 빈도를 비교하는 것은 좋은 컨텍스트를 제공합니다.\n\n이 방법의 변형 중 하나는 자주 나오는 것이 아닌 드문 항목 집합을 사용하는 것인데, 각 행의 드문 항목 집합의 수와 희귀성에 따라 각 행을 점수 매깁니다. 이 방법도 유용한 결과를 얻을 수 있지만, 계산 비용이 상당히 많이 소요되며, 더 많은 항목 집합이 채굴되어야 하고, 각 행은 많은 항목 집합과 테스트되어야 합니다. 그러나 최종 점수는 각 행에 누락된 대신 발견된 항목 집합을 기반으로 하므로 더 해석하기 쉬울 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 결론\n\n여기에 나와 있는 코드 외에는 파이썬에서 FPOF를 구현한 것을 알지 못합니다. 그러나 R에서는 구현된 것이 있습니다. FPOF 작업의 주요 부분은 FISs를 채굴하는 것이며, 여기에서 사용된 mlxtend 라이브러리를 포함하여 이를 수행할 수 있는 다양한 파이썬 도구가 있습니다. 위에서 본 FPOP의 나머지 코드는 꽤 간단합니다.\n\n이상 탐지에서 해석 가능성의 중요성을 고려할 때, FPOF는 매우 유용할 수 있습니다.\n\n향후 기사에서는 이상 탐지를 위한 다른 해석 가능한 방법에 대해 알아볼 것입니다.\n\n<div class=\"content-ad\"></div>\n\n모든 그림은 저자에 의해 생성되었습니다.\n","ogImage":{"url":"/assets/img/2024-05-27-InterpretableOutlierDetectionFrequentPatternsOutlierFactorFPOF_0.png"},"coverImage":"/assets/img/2024-05-27-InterpretableOutlierDetectionFrequentPatternsOutlierFactorFPOF_0.png","tag":["Tech"],"readingTime":9},{"title":"시계열 데이터에서 이상치를 찾는 궁극의 안내서 파트 1","description":"","date":"2024-05-27 14:58","slug":"2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1","content":"\n\n![Outliers](/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_0.png)\n\n이상치: 통계 모델을 왜곡하고 예측을 왜곡시키며 의사 결정 프로세스를 약화시키는 문제가 되는 데이터 포인트들입니다.\n\n그들이 데이터 분석에서 특별히 좋아지지 않는 것은 놀라운 일이 아닙니다.\n\n제 이름은 Sara이며, 물리학 석사 학위를 가지고 있습니다. 현재는 글로벌 에너지 회사에서 데이터 과학자로 일하고 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n이 기사는 시계열 데이터에서 이상치를 식별하고 관리하는데 전념한 세 개의 시리즈를 시작합니다. 만약 이 시리즈를 계속해서 따르고 싶다면, 저를 팔로우하고 다음 부분이 발행될 때 업데이트를 받을 수 있도록 구독하세요!\n\n이 첫 번째 글에서는 시계열 데이터에서 이상치를 효과적으로 식별하기 위한 시각적 및 통계적 방법을 탐구합니다. 이 기초적인 지식은 분석 정확도를 향상시키려는 모든 사람에게 중요합니다.\n\n다음 기사에서는 기계 학습 방법에만 집중하여, 그 중요성과 복잡성에 비추어, 전용 논의가 필요합니다.\n\n그 후 세 번째 기사에서는 이상치를 관리하는 다양한 전략을 탐색할 것입니다. 우리는 변형 기술에 집중하여, 그 영향을 완화하는 실용적인 해결책을 제시할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n시작해 봅시다!\n\n컨텐츠:\n\nPart I (이 기사):\n\n- 왜 중요한가요?\n- Outliers vs Anomalies\n- 올바른 이상 탐지 방법 선택하기\n- 단변량 vs 다변량 시계열 데이터\n- 이상치 식별\n\n<div class=\"content-ad\"></div>\n\n- 시각 검사\n- 통계적 방법\n\n5. 평가 지표\n\nPart II (다음):\n\n3. 이상값 식별\n\n<div class=\"content-ad\"></div>\n\n- 기계 학습 방법\n- 평가 지표\n\n다음 장 (예정):\n\n4. 이상값 처리\n\n- 무시하거나 제거하거나?\n- 변환 기술\n- 대치\n- 상한 설정\n\n<div class=\"content-ad\"></div>\n\n어째서 신경 써야 할까요?\n\n만약 여러분이 이 글을 읽고 있다면, 아마도 모델링을 수행하기 전에 이상치를 처리하는 것이 얼마나 중요한지 이미 알고 계실 것입니다.\n\n시계열 데이터에서 이상치에 대해 신경 써야 하는 몇 가지 이유는 다음과 같습니다:\n\n- 이상치는 데이터 집합의 평균, 분산, 상관 계수 등과 같은 주요 통계치를 심각하게 왜곡하고 잘못 표현할 수 있습니다.\n- 이상치는 예측 모델의 성능을 훼손시킬 수 있습니다.\n- 이상치는 시계열 데이터에서 진정한 추세와 주기적인 행동을 가리고 숨길 수 있습니다.\n- 이상치를 철저히 검토하지 않은 데이터에 기반한 결정은 부적절한 전략적 결정으로 이어질 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n시계열 데이터에서 이상 값 처리가 효과적인 분석에 중요한 이유가 더 많이 있지만, 이것들은 우리가 시작하기를 원하는 충분한 이유일 것입니다.\n\n## Outliers vs Anomalies\n\n\"이상 값\"과 \"이상 값\"이라는 용어를 번갈아 사용할 것이지만, 그들의 정의에는 미묘한 차이가 있습니다. 이상 값은 정상에서 벗어나는 모든 데이터 포인트를 가리킬 수 있지만, 이상 값은 특히 대부분의 데이터 포인트로부터 크게 벗어난 극단적인 값이라는 것을 명시적으로 나타냅니다. 많은 방법이 이상 값과 이상 값 모두에 적용될 수 있습니다.\n\n# 시계열 데이터에 적합한 이상 값 탐지 방법을 선택하는 방법?\n\n<div class=\"content-ad\"></div>\n\n\n![The Ultimate Guide to Finding Outliers in Your Time-Series Data Part 1](/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_1.png)\n\n시계열 데이터의 이상 탐지 방법을 선택하기 위해서는 데이터셋과 예상되는 이상과 깊이 이해가 필요합니다.\n\n그렇다면, 데이터셋의 크기와 사용 가능한 계산 리소스를 고려해보세요.\n\n해석 가능성이 중요한 데이터셋의 경우 Z-Score와 이동 평균과 같은 간단한 방법이 이상적일 수 있습니다. 그러나 섬세한 패턴을 감지해야 하는 복잡한 시나리오와 같은 경우에는 LSTM 네트워크와 같은 고급 기법이 유용할 수 있습니다(이에 대한 자세한 내용은 이 시리즈의 두 번째 부분에서 다룰 예정이며, 상당한 데이터와 계산 능력이 필요합니다).\n\n\n<div class=\"content-ad\"></div>\n\n## 기억하세요: 데이터셋 크기, 계산 자원, 해석 가능성, 그리고 작업의 성격은 적절한 이상점 탐지 방법을 선택하는 데 중요합니다.\n\n정확한 성능 평가를 위해 다양한 방법과 지표를 실험해 보는 것이 유익할 수 있습니다. 가능하다면 정확도를 향상시키기 위해 여러 방법의 앙상블을 사용하는 것을 고려해보세요. 또한 분야에 대해 알고 있는 사학자나 전문가의 의견을 활용하여 방법을 선택할 수 있습니다.\n\n사기 탐지와 같이 이상치가 드물지만 중요한 경우와 같이 이상치 탐지 방법을 평가하는 것은 특히 어려울 수 있습니다.\n\n정밀도, 재현율, F1 점수와 같은 지표는 사기 활동을 포착하면서 거짓 양성을 줄이는 이러한 방법의 효과를 평가하는 데 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n예측 유지보수와 같은 맥락에서 ROC 곡선과 AUC 지표는 잠재적인 기계 고장을 적시에 식별하는 데 매우 유용합니다.\n\n건강 관리와 같은 산업에서는 시각화가 환자의 중요 생리 신호를 모니터링하는 데 자주 사용되지만, 이러한 방법의 정확성은 올바른 해석을 위해 도메인 전문 지식에 매우 의존합니다.\n\n# 단변량 vs 다변량 데이터\n\n이상치 분석을 시작하기 전에 데이터가 단변량인지 다변량인지 고려하는 것이 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n단변량 시계열 데이터는 시간에 따라 기록된 단일 관측값 순서로 구성됩니다. 전형적인 예시로는 일일 주식 가격, 월간 판매 수치 또는 연간 기상 데이터가 있습니다.\n\n![image](/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_2.png)\n\n반면, 다변량 시계열 데이터는 동일한 시간 간격에서 관찰되고 기록된 여러 변수 또는 순서로 구성됩니다.\n\n이 유형의 데이터는 서로 다른 변수 간의 관계 및 상호작용을 포착하며 개별적인 추세 및 계절적 변동도 포함합니다. 예를 들어, 다변량 시계열에는 온도, 습도 및 풍속의 일일 측정치가 동시에 기록된 것이 포함될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_3.png)\n\n이 기사에서 설명된 몇 가지 방법은 일변량 데이터에 더 적합하며 다른 몇 가지는 여러 변수를 처리하는 데 특화되어 있습니다.\n\n그러나 일부는 두 가지에 모두 적용할 수 있습니다. 방법에 깊이 들어가기 전에 두 종류의 데이터에 대한 일반적인 방법을 소개하겠습니다.\n\n일변량 데이터에 대해 시계열 플롯 및 상자 그림과 같은 시각적 검사 방법이 일반적으로 사용되며 한 번에 한 변수에 초점을 맞춥니다. 또한 일변량 설정에서 STL 분해가 전통적으로 사용됩니다. Z-점수, 수정된 Z-점수 방법 및 Grubbs' 검정도 이 유형의 데이터에 사용됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n기계 학습 방법 중 Isolation Forest, LOF 및 Autoencoders와 같은 방법들은 일반적으로 다변량 데이터에서 차원 축소와 이상 탐지에 사용되지만, 단일 시계열 데이터를 압축하고 복원하여 재구성 오류를 기반으로 이상을 식별하는 데도 사용됩니다.\n\n![이미지](/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_4.png)\n\n팁: 이 정신 지도에 나열된 방법 이외에도 더 많은 이상치 탐지 방법이 있습니다.\n\n여러 변수가 포함된 데이터의 경우 산점도 분석이 일반적으로 여러 변수 간의 관계를 조사하는 데 사용됩니다. Isolation Forests, LOF 및 Autoencoders는 고차원 데이터를 처리하는 데 자연스럽게 적합합니다.\n\n<div class=\"content-ad\"></div>\n\n다중 변수 데이터에도 여러 가지 일변량 방법을 적용할 수 있습니다. 예를 들어, Z-점수 방법은 각 변수에 대해 독립적으로 Z-점수를 계산하여 다중 변수 상황에서도 사용할 수 있습니다.\n\n상자 도표는 다차원 데이터셋 내 각 변수에 대해 별도로 사용될 수 있어 각 차원에서 이상치를 식별하는 데 도움이 됩니다. 다차원 시나리오에서는 산점도를 사용하여 변수 쌍을 플롯할 수 있습니다. STL 분해는 전통적으로 일변량이지만 각 시리즈를 독립적으로 분해하여 다중 변수 시리즈를 분석하는 데 적용할 수 있습니다.\n\n# 데이터에서 이상치를 탐지하기 위한 최상의 방법은 무엇입니까?\n\n# 시각적 방법\n\n<div class=\"content-ad\"></div>\n\n시각 검사는 시계열 데이터에서 이상치를 식별하는 데 중요한 방법입니다. 데이터의 특성에 따라 시각적 검사를 어떻게 수행할지가 영향을 받습니다.\n\n시계열 플롯\n\n시계열 데이터에 대한 가장 간단한 플롯입니다. 시간 경향, 패턴, 계절 변동 및 잠재적 이상치를 시간에 따라 확인할 수 있습니다. 다른 데이터에서 크게 벗어난 지점은 종종 쉽게 발견할 수 있습니다.\n\n```js\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ndef plot_temporal_trends(df, columns):\n\n    num_plots = len(columns)\n    fig, axes = plt.subplots(num_plots, 1, figsize=(10, num_plots * 3), sharex=False)  # sharex=False로 x축을 공유하지 않음\n    fig.suptitle(f'시간 트렌드', fontsize=16, y=1.02 + 0.01 * num_plots)\n\n    if num_plots == 1:  # axes가 반복 가능한지 확인\n        axes = [axes]\n\n    for ax, col in zip(axes, columns):\n        ax.plot(df.index, df[col], marker='o', markersize=4, linestyle='-', label=col)\n        ax.set_title(f'{col} - {title}')\n        ax.set_ylabel('값')\n\n        # 각 subplot의 x축에 대한 날짜 포매터 설정\n        ax.xaxis.set_major_locator(mdates.YearLocator(base=2))\n        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n\n        # 틱 라벨을 더 잘 보이도록 회전 및 정렬\n        ax.tick_params(axis='x', rotation=45)\n\n        ax.legend()\n\n    plt.tight_layout(rect=[0, 0, 1, 0.97])  # 제목을 위한 공간을 만들기 위해 레이아웃 조정\n    plt.show()\n\ncolumns = df.columns.tolist()\nplot_temporal_trends(df, columns)\n```\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_5.png\" />\n\n이상값을 발견할 수 있나요? 🤔 이 시각화는 이상값을 쉽게 드러낼 수도 있고, 데이터의 복잡성에 따라 더 자세한 분석이 필요할 수도 있습니다. 대부분의 경우, 추가 시각화가 필요할 수도 있습니다.\n\n## 이상값 분석에서 계절 고려 사항\n\n이상값을 계절별로 처리하는 것은 매우 중요할 수 있습니다, 특히 계절 변동을 나타내는 데이터를 다룰 때입니다. 많은 시계열 데이터 세트는 특정 시기에 명확한 패턴을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n사실 이상값으로 보였던 데이터 포인트가 계절에 따라 처리되면 이상값으로 간주되지 않을 수도 있습니다. 다시 말해 데이터와 문제의 성겁에 따라 다릅니다!\n\n예를 들어, 저는 최근 댐에서의 수질 측정 데이터를 다루는 프로젝트에 참여했습니다. 여기서 이상값은 계절별로 분석해야 한다는 것이 빠르게 명확해졌습니다. 각 계절은 고유한 특성과 추세를 가지고 있으며, 이는 파라미터에 독특한 방식으로 영향을 미칩니다. 계절별로 데이터를 분할하여, 각 하위 그룹에 효과적인 특정 이상값 탐지 방법을 적용할 수 있었습니다.\n\n예를 들어, 비가 많이 오는 계절의 수질 이상값은 유출 때문에 일반적일 수 있지만, 건조한 계절에는 비정상적일 수 있습니다.\n\n게다가, 계절별 분석은 계절적 영향을 고려하여 예측 모델을 개선할 수 있습니다. 이것은 농업, 관광 및 소매업과 같은 계절적 변화에 크게 영향을 받는 산업에 중요할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n박스 플롯\n\n데이터 집합이나 데이터 하위 집합에서 이상 징후를 정적으로 식별하는 데 유용합니다. 상자 외부의 점들(일반적으로 사분위수에서 1.5배의 사분위 범위로 설정)은 잠재적인 이상값입니다.\n\n```js\ndef plot_outliers(param_dfs):\n    for key, df in param_dfs.items():\n        plt.figure(figsize=(10, 6))\n        df.boxplot()\n        plt.title(f'{key}의 박스 플롯')\n        plt.xticks(rotation=45)\n        plt.show()\n\n# 이상값을 플롯하는 함수 호출\nplot_outliers(param_dfs)\n```\n\n![Boxplot image](/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_6.png)\n\n<div class=\"content-ad\"></div>\n\n상기 그림에서는 특정 매개변수를 다른 수심별로 분석하여 이들을 옆에 비교했습니다. 플롯 상의 작은 검은 원은 잠재적 이상점을 나타냅니다. 이러한 시각적 단서는 각 하위 집합 내에서 근본적으로 벗어난 데이터 포인트를 신속하게 식별하는 데 중요합니다.\n\n산포도\n\n시계열 데이터가 다른 변수와 관련이 있다면, 산포도는 두 변수의 맥락에서 이상점을 식별하는 데 도움이 될 수 있습니다.\n\n```js\n# 플로팅\nplt.figure(figsize=(10, 6))\n# 정상 데이터\nplt.scatter(df['연간 소득'][:-5], df['신용카드 지출'][:-5], color='blue')\nplt.title('연간 소득 대 신용카드 지출')\nplt.xlabel('연간 소득 ($)')\nplt.ylabel('신용카드 지출 ($)')\nplt.legend()\nplt.grid(True)\nplt.show()\n```\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_7.png\" />\n\n# 통계적 방법\n\n## STL 분해\n\n시계열 데이터는 추세, 계절성 및 잔류로 분할될 수 있다는 것을 이미 알고 계시다시피,\n\n\n<div class=\"content-ad\"></div>\n\nSTL 분해는 LOESS(Locally Estimated Scatterplot Smoothing)를 사용하여 시계열 신호를 이 세 가지 구분된 구성 요소로 효과적으로 분리하는 데 유용합니다. 이를 통해 데이터의 기본적인 행동에 대한 명확한 통찰력을 얻어 분석하고 예측할 수 있는 능력이 향상됩니다.\n\n참고: STL은 데이터 포인트가 시간 순서대로 배열되어 있다고 가정합니다. STL 분해는 데이터 포인트들이 자연적인 시간 순서로 서로 뒤이어 따라 오는 시리즈를 예상합니다. 이는 추세 및 계절 변동을 정확하게 추정하기 위한 중요한 요소로, 각 포인트가 이어지는 포인트를 이해하는 데 기여합니다.\n\n![이미지](/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_8.png)\n\nSTL은 이상치를 식별하는 데 어떻게 도움을 줄까요?\n\n<div class=\"content-ad\"></div>\n\nSTL 분해의 잔여 구성 요소는 계절성 및 추이 구성 요소로 설명할 수 없는 데이터 부분을 나타냅니다.\n\n이상적으로는 잔여는 무작위 잡음이어야 합니다. 이상점은 종종 잔여 시리즈에서 일반적인 잡음 수준에서 현저하게 벗어나는 이상한 돌출이나 패딩으로 감지될 수 있습니다. 따라서 잔여에서 극단적인 값들을 찾아보세요. 잔여는 이상적으로 무작위 잡음을 나타내므로 어떠한 중요한 이탈도 이상점을 나타낼 수 있습니다.\n\n잔여의 표준 편차를 분석하여 평균 잔여로부터 일반적으로 분류되는 이상한 거리에 있는 점들을 식별할 수 있습니다. 예를 들어, 평균 잔여에서 2 또는 3 표준 편차 이상 떨어진 데이터 포인트는 이상점으로 간주될 수 있습니다.\n\n추이 구성 요소는 단기 변동을 완화시키고 데이터 집합의 폭넓은 움직임을 강조합니다. 이상점은 추이에 예상치 못한 변동이나 대부분의 데이터가 설정한 부드러운 패턴과 어울리지 않는 급격한 변화를 일으킬 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 단계 1: 잔차 플롯 검토\n\n먼저 STL 분해에서 얻은 잔차 플롯을 살펴봅니다. 이 플롯은 추세나 계절성 구성 요소로 설명할 수 없는 데이터 포인트를 보여줍니다.\n\n## 단계 2: 통계 지표 계산\n\n잔차의 평균과 표준 편차를 계산합니다. 이는 정규 분포된 잡음 패턴에서 기대하는 것과 유의미하게 다른 데이터 포인트를 결정하는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n## 단계 3: 이상치에 대한 임계값 정의\n\n일반적으로, 평균으로부터 2 또는 3 표준 편차 이상 떨어진 데이터 포인트는 이상치로 간주됩니다. 이상치에 얼마나 민감하게 대응할지에 따라 임계값을 선택할 수 있습니다. 많은 경우, 3 표준 편차를 사용하는 것이 일반적입니다.\n\n## 단계 4: 이상치 식별\n\n잔차 구성 요소에서 이 임계값을 초과하는 데이터 포인트를 식별합니다. 이것들이 당신의 잠재적인 이상치입니다.\n\n<div class=\"content-ad\"></div>\n\n## 단계 5: 시각적 검사 및 상호 확인\n\n원래 시계열 플롯과 추세 및 계절성 구성 요소의 플롯을 다시 살펴보세요. 식별된 이상점이 이러한 구성 요소와 어떤 관련이 있는지 살펴보세요.\n\n이상점이 단순히 유례 없는 변동이거나 오류 또는 이상 현상을 나타낼 가능성이 있는지 확인하세요. 이는 데이터에 대한 문맥적 지식을 포함할 수 있습니다.\n\n```js\nimport numpy as np\n#예시\n\n# 잔차의 평균과 표준 편차 계산\nresiduals = result.resid\nmean_resid = np.mean(residuals)\nstd_resid = np.std(residuals)\n\n# 이상점 감지를 위한 임계값 정의\nthreshold = 3 * std_resid\n\n# 잠재적인 이상점 식별\noutliers = residuals[np.abs(residuals - mean_resid) > threshold]\n\n# 이상점의 날짜 및 값 출력\nprint(outliers)\n```\n\n<div class=\"content-ad\"></div>\n\nZ-점수와 수정된 Z-점수 방법\n\nZ-점수는 표준 점수로도 알려져 있으며, 데이터 포인트가 데이터 집합의 평균으로부터 몇 표준 편차 떨어져 있는지를 측정합니다. 이는 아래 공식을 사용하여 계산됩니다:\n\n\nZ = (X - μ) / σ\n\n\n- X는 개별 데이터 포인트를 나타냅니다.\n- μ는 데이터 집합의 평균을 나타냅니다.\n- σ는 데이터 집합의 표준 편차를 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n일정 임계값(일반적으로 2 또는 3)보다 높은 Z-점수를 가진 데이터 포인트는 이상값으로 간주됩니다.\n\n참고: Z-점수 방법은 데이터가 정규 분포를 따른다고 가정하거나 적어도 대략적으로 정규 분포를 따른다고 가정합니다. 또한, 특이값이 평균과 표준 편차에 비해 극단적인 값을 가질 것이라고 가정합니다.\n\n```js\nimport numpy as np\n\ndef identify_outliers_with_z_score(df, z_thresh=z_thresh):\n    print(\"Z-점수 방법을 사용하여 이상값 식별:\")\n    for col in df.columns:\n        if np.issubdtype(df[col].dtype, np.number):  # 숫자 데이터만\n            df_col = df[col].dropna()  # NaN 값 제거\n            z_scores = np.abs((df_col - df_col.mean()) / df_col.std(ddof=0))\n            outliers = df_col[z_scores > z_thresh]\n            print(f\"{col} - 이상값: {len(outliers)}\")\n            print(outliers, \"\\n\")\n\nz_thresh = 3\nidentify_outliers_with_z_score(df)\n```\n\n데이터 세트에서 이상값을 식별하기 위해 Z-점수 임계값을 선택하는 것은 데이터 분석 결과에 상당한 영향을 줄 수 있는 중요한 결정임을 기억해 주세요.\n\n<div class=\"content-ad\"></div>\n\n아래는 일반 분포 하에서의 z-점수에 대한 설명입니다: z-점수 1은 곡선 아래 약 68%를, z-점수 2는 약 95%를, z-점수 3는 약 99.7%를 포함합니다.\n\n만약 다양한 임계값을 사용하여 데이터의 이상치 개수를 궁금해 한다면, 그것을 플로팅하여 비교할 수도 있습니다:\n\n```js\ndef count_outliers_by_z_threshold(series, z_thresholds=[1, 2, 3]):\n    \"\"\"주어진 Z-점수 임계값에 대해 이상치 개수를 반환합니다.\"\"\"\n    mean = series.mean()\n    std = series.std()\n    z_scores = np.abs((series - mean) / std)\n\n    # 각 Z-점수 임계값에 대한 이상치 개수 계산\n    counts = {}\n    for z_thresh in z_thresholds:\n        counts[z_thresh] = (z_scores > z_thresh).sum()\n    return counts\n\n# 'param_dfs'가 DataFrame의 딕셔너리인 것을 가정합니다\nz_thresholds = [1, 2, 3]\noutlier_counts_by_threshold = {z: 0 for z in z_thresholds}\n\nfor df in param_dfs.values():\n    for column in df.columns:\n        series = df[column].dropna()  # NaN 값 제거\n        counts = count_outliers_by_z_threshold(series, z_thresholds)\n        for z_thresh, count in counts.items():\n            outlier_counts_by_threshold[z_thresh] += count\n\n# 플로팅을 위해 이상치 개수를 리스트로 변환\ncounts = [outlier_counts_by_threshold[z] for z in z_thresholds]\n\n# 플로팅\nplt.figure(figsize=(8, 6))\nbars = plt.bar(z_thresholds, counts, color=['blue', 'orange', 'green'])\n\nplt.xlabel('Z-점수 임계값')\nplt.ylabel('이상치 개수')\nplt.title('Z-점수 임계값에 따른 이상치 수')\nplt.xticks(z_thresholds)\n\n# 각 막대에 높이 값으로 주석 달기\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.05 * max(counts), int(yval), ha='center', va='bottom')\n\nplt.show()\n```\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_11.png)\n\n시계열 데이터를 다루고 있기 때문에 연도별 이상치 분포를 시각화하는 것이 유용할 수 있습니다:\n\n```js\ndf['Z-Score'] = zscore(df['Value'])\n\n# Z-score가 3보다 크거나 -3보다 작은 경우를 이상치로 정의\ndf['Outlier'] = (df['Z-Score'] > 3) | (df['Z-Score'] < -3)\n\n# 연도별 이상치 개수 계산\noutlier_counts = df.resample('Y')['Outlier'].sum().astype(int)\n\n# 연도를 별도 열로 얻기 위해 인덱스 재설정 및 플로팅 준비\noutlier_counts = outlier_counts.reset_index()\noutlier_counts['Year'] = outlier_counts['index'].dt.year\noutlier_counts.drop('index', axis=1, inplace=True)\n\nplt.figure(figsize=(10, 6))\ncolors = plt.cm.viridis(np.linspace(0, 1, len(outlier_counts)))\nplt.bar(outlier_counts['Year'], outlier_counts['Outlier'], color=colors)\nplt.title('Distribution of Outliers Per Year')\nplt.xlabel('Year')\nplt.ylabel('Number of Outliers')\nplt.xticks(outlier_counts['Year'])  # 모든 연도가 표시되도록\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.show()\n```\n\n![image](/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_12.png)\n\n\n\n<div class=\"content-ad\"></div>\n\n한 계절 당:\n\n![image](/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_13.png)\n\nRobust Z-Score\n\n수정된 Z-점수 방법으로도 알려진 이 기술은 일반적인 Z-점수를 개선하기 위해 평균 대신 중앙값(M)을 사용합니다. 평균은 가장 신뢰할 수있는 통계적 측정 방법이 아니기 때문에 중앙값을 사용합니다. 또한 표준 편차 대신 중앙값의 절대 편차(MAD)를 사용합니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_14.png)\n\n수정된 Z-점수 공식의 0.6745 배수는 중앙값 절대편차(MAD)를 표준 편차와 비교 가능하게 만들기 위해 사용됩니다.\n\n이 조정이 필요한 이유는 정의에 따라 MAD가 동일한 데이터 집합에 대해 표준 편차보다 작기 때문입니다. 따라서 이런 스케일링을 고려하여 임계값을 조정해야 합니다.\n\n참고: 중앙값과 MAD는 평균과 표준 편차보다 이상값에 강인한 특성을 가집니다. 때로는, 이 강인성으로 인해 MAD 값이 작아질 수 있으며, 특히 이상값이 극단적인 경우에 그렇습니다. 따라서, 가장 극단적인 관측치만 이상값으로 표시되도록 하기 위해 약간 더 높은 임계값이 필요할 수 있습니다 (문맥에 따라 3.5, 4 또는 5).\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom scipy.stats import median_absolute_deviation\n\n# 중앙값 및 중앙값 절대 편차(MAD)를 사용하여 수정된 Z-점수 계산\ndf['Median'] = df['Value'].median()\ndf['MAD'] = median_absolute_deviation(df['Value'])\ndf['Modified_Z-Score'] = 0.6745 * (df['Value'] - df['Median']) / df['MAD']\n\n# 수정된 Z-점수가 3보다 크거나 -3보다 작은 경우 이상값으로 정의\ndf['Outlier'] = (df['Modified_Z-Score'] > 3.5) | (df['Modified_Z-Score'] < -3.5)\n\n# 연도별 이상값 수 계산\noutlier_counts = df.resample('Y')['Outlier'].sum().astype(int)\n\n# 인덱스 재설정하여 연도를 별도의 열로 가져오고 플로팅을 위해 준비\noutlier_counts = outlier_counts.reset_index()\noutlier_counts['Year'] = outlier_counts['index'].dt.year\noutlier_counts.drop('index', axis=1, inplace=True)\n\n# 연도별 그래픽 플로팅\nplt.figure(figsize=(10, 6))\ncolors = plt.cm.viridis(np.linspace(0, 1, len(outlier_counts)))\nplt.bar(outlier_counts['Year'], outlier_counts['Outlier'], color=colors)\nplt.title('연도별 이상값 분포')\nplt.xlabel('연도')\nplt.ylabel('이상값 수')\nplt.xticks(outlier_counts['Year'])  # 모든 연도를 표시하도록 설정\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.show()\n```\n\n![이미지](/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_15.png)\n\n## 통계 및 시각적 방법 통합\n\nZ-점수 방법(임계값 3)으로 감지된 이상값을 시계열 도표로 시각화합시다.\n\n\n\n<div class=\"content-ad\"></div>\n\n```js\n#여기서 이상치는 사전입니다.\ndef plot_outliers(df, outliers_dict):\n    #한 stage마다 이상치가 있는 column의 수만큼 subplot의 행 수가 됩니다.\n    for stage, columns_outliers in outliers_dict.items():\n        #플롯 수 결정\n        num_plots = len(columns_outliers)\n        if num_plots == 0:\n            continue  # 이 stage에 이상치가 없으면 건너뜁니다.\n\n        #서브플롯 만들기\n        fig, axes = plt.subplots(nrows=num_plots, figsize=(15, num_plots * 5), sharex=True)\n        fig.suptitle(f'{stage} Stage의 이상치 시각화', fontsize=16)\n\n        if num_plots == 1:  # 플롯이 하나뿐이면 iterable로 만들기\n            axes = [axes]\n\n        for ax, (column, outliers) in zip(axes, columns_outliers.items()):\n            #전체 데이터 시리즈 플롯\n            ax.plot(df.index, df[column], label=f'{column} (전체 시리즈)', color='black', linestyle='-', marker='', alpha=0.5)\n\n            #이상치 강조\n            if not outliers.empty:\n                ax.scatter(outliers.index, outliers, color='red', label='이상치', marker='o', s=50)\n\n            ax.set_title(f'{column}의 이상치')\n            ax.set_ylabel('값')\n            ax.legend()\n\n            #x축 주요 로케이터와 포매터 설정\n            ax.xaxis.set_major_locator(mdates.YearLocator())\n            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n\n        plt.tight_layout(rect=[0, 0, 1, 0.96])  # 레이아웃 조정\n        plt.show()\n```\n\n![2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_16.png](/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_16.png)\n\nGrubb의 Test\n\nGrubb의 테스트, 최대 또는 최소값을 데이터셋의 평균 및 표준편차와 비교하여 잠재적인 이상치를 식별하는 최대 표준화 잔차 테스트로 알려져 있습니다.\n\n<div class=\"content-ad\"></div>\n\n참고: 데이터가 1차원 정규 분포를 따르고 이상치는 평균과 표준편차에 비해 극단적인 값을 가질 것으로 가정합니다.\n\n중요한 점은 그럽스 검정이 일반적으로 데이터 집합에서 최댓값 또는 최솟값 중 하나를 포함해 한 번에 한 데이터 포인트씩 수행됩니다.\n\n![image](/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_17.png)\n\nG는 그럽스 검정 통계량, X는 의심스러운 이상치 값(데이터 집합에서 최대값 또는 최소값), X(바로 위에 bar가 있는)는 데이터 집합의 평균이며, s는 데이터 집합의 표준 편차입니다.\n\n<div class=\"content-ad\"></div>\n\n의심되는 이상값이 유의한지 여부를 결정하기 위해 계산된 검정 통계량 G를 적절한 통계 분포(일반적으로 T-분포)의 임계값과 비교합니다.\n\nG가 임계값을 초과하는 경우, 의심되는 이상값은 통계적으로 유의하다고 간주되며, 이는 이상값일 가능성이 높음을 나타냅니다.\n\n```js\nfrom pyod.models.grubbs import grubbs_test\n\n# PyOD를 사용하여 Grubbs' 테스트 적용\noutliers_grubbs = []\nfor col in df.columns:\n    outliers = grubbs_test(df[col])\n    outliers_grubbs.extend([(idx, col) for idx in outliers])\nprint(\"Grubbs' 테스트에 의해 감지된 이상값:\")\nprint(outliers_grubbs)\n```\n\n# 평가 메트릭\n\n<div class=\"content-ad\"></div>\n\n저는 이에 대해 전체 블로그 글을 쓸 수도 있을 거예요.\n\n하지만 간단히 하기 위해, 주요 평가 지표를 언급하며 몇 가지 예시를 드릴게요.\n\n적절한 평가 지표를 선택하는 것은 시계열 분석에서 이상 탐지 방법의 효과를 평가하는 데 매우 중요한 요소에요.\n\n## 최적의 지표는 데이터의 성격, 해당 이상값의 특성 및 거짓 긍정과 거짓 부정 사이에서 필요한 균형에 따라 다를 거예요.\n\n<div class=\"content-ad\"></div>\n\n예시: 사기 탐지\n\n사기 탐지에서 이상 징후는 드물지만 중요합니다. 여기서 핵심적인 지표는 정밀도, 재현율, 그리고 F1 점수입니다. 정밀도는 실제로 이상 징후인 것들 중에 몇 개를 올바르게 식별했는지 측정합니다. 재현율은 방법으로 올바르게 식별된 실제 이상 징후의 수를 측정합니다. F1 점수는 정밀도와 재현율을 균형 있게 고려해 방법의 성능을 종합적으로 평가하는 데 도움을 줍니다.\n\n예시: 예측 유지보수\n\n예측 유지보수에서는 이상을 적시에 식별하는 것이 중요합니다. 여기서 수신기 조작 특성 (ROC) 곡선과 곡선 아래 영역 (AUC)은 주요 지표입니다. 이러한 지표들은 참 양성 비율과 거짓 양성 비율 사이의 균형을 이해하는 데 도움을 줍니다. 이는 서로 다른 운영 요구 사항을 충족시키기 위해 감지 임계값을 최적화하는 데 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n예시: 의료\n\n의료 분야에서 해석 가능성은 매우 중요하기 때문에 시각화 역시 매우 중요합니다. 이들은 전문가들이 감지된 이상 현상을 확인하고 의사 결정에 미치는 영향을 이해할 수 있도록 돕습니다. 도메인 지식을 통합함으로써, 이러한 시각 도구들은 결과물의 타당성과 해석 가능성을 향상시켜 미래에 적용 가능하고 중요한 이상 현상임을 보장합니다.\n\n적절한 지표를 선택하는 것은 시계열 데이터에서 이상 현상을 효과적으로 감지하고 관리하는 데 상당한 영향을 미칠 수 있습니다.\n\n이상 현상 관리에 어떤 지표를 선택해야 하는지 알아보았습니다.\n\n지금까지입니다! 이 글이 마음에 든다면 반드시 클랩을 눌러주세요 😋. 비슷한 기사를 원하신다면 제 팔로우도 환영합니다!\n\n<div class=\"content-ad\"></div>\n\n## 시계열 분석에서 주로 사용하는 통계적 방법은 무엇인가요?\n\n## 그 밖에 어떤 방법을 알고 계신가요? 댓글로 알려주세요 :)\n\n저는 물리학과 천문학 배경을 가진 데이터 과학자인 사라 노브레가입니다. 인공지능, MLOps, 스마트 시티, 지속가능성, 우주학, 그리고 인권에 관심이 많습니다.\n\n참고 문헌\n\n<div class=\"content-ad\"></div>\n\n시계열 데이터에서 이상 감지를 어떻게 수행하나요?\n","ogImage":{"url":"/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_0.png"},"coverImage":"/assets/img/2024-05-27-TheUltimateGuidetoFindingOutliersinYourTime-SeriesDataPart1_0.png","tag":["Tech"],"readingTime":20},{"title":"임베딩 크기를 줄이고 RAG 검색 속도 높이는 방법","description":"","date":"2024-05-27 14:56","slug":"2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed","content":"\n<img src=\"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_0.png\" />\n\n# 소개\n\n텍스트 임베딩은 단일 단어나 전체 문장의 고차원 벡터 표현입니다.\n\n이 숫자 배열로 이루어진 벡터는 기본 텍스트에 대한 풍부한 정보를 포착함으로써 의미 이해, 분류, 군집화, 정보 검색 (RAG), 재정렬 및 더 많은 하류 작업에 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n보통 임베딩 벡터의 차원 d는 고정됩니다. 임베딩 차원은 일반적으로 64에서 4096까지의 2의 제곱수로 구성됩니다.\n\n매트료시카 임베딩을 사용하면 응용 프로그램에 따라 임베딩의 차원을 변경할 수 있습니다. 이를 통해 저장 공간을 줄이고 비용을 절약하며 검색 속도를 높일 수 있습니다.\n\n# 텍스트 임베딩이란?\n\n![이미지](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_1.png)\n\n<div class=\"content-ad\"></div>\n\n저희는 모든 가능한 입력 문자를 정수 값으로 매핑하는 어휘를 정의하여 시작합니다. 이 어휘에는 알파벳 문자 뿐만 아니라 특수 문자, 짧은 단어 및 하위 단어도 포함됩니다:\n\n```js\n{\n  \"a\": 1,\n  \"b\": 2,\n  \"c\": 3,\n  ...\n  \"z\": 26,\n  \"the\": 27,\n  \" \": 28\n}\n```\n\n토큰화 후에는 토큰 목록을 인코더 모델에 전달할 수 있습니다. 인코더는 대량의 훈련 데이터로부터 학습하여 각 토큰을 고차원 숫자 벡터 임베딩으로 변환합니다.\n\n예를 들어, OpenAI의 text-embedding-3-large 모델의 임베딩의 출력 차원 d는 3072입니다.\n\n<div class=\"content-ad\"></div>\n\n단일 문장 임베딩을 얻으려면 여러 토큰 임베딩에서 정보를 압축해야 합니다. 이를 위한 한 가지 방법은 단순히 모든 토큰 임베딩을 평균내는 것입니다.\n\n# 마트료시카 임베딩\n\n마트료시카 임베딩은 워싱턴 대학, 구글 리서치, 하버드 대학의 연구자들에 의해 2022년에 발표된 \"Matryoshka Representation Learning\" 논문에서 소개되었습니다.\n\n마트료시카 임베딩은 서로 다른 세기의 정보를 하나의 임베딩 벡터에 인코딩하는 데 훈련되었습니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, MRL을 사용하여 단순히 크기 d = 1024의 전체 임베딩 벡터를 학습하는 대신, 우리는 동일한 시간에 최적화하려는 손실 함수를 위해 matryoshka_dims = [1024,512,256,128,64] 차원 목록을 사용합니다[2].\n\n이렇게 하면 처음 몇 차원에 가장 덜 구체적인 정보가 저장되고 나중 차원에는 점점 더 많은 세부 정보가 저장된 임베딩 벡터가 생성됩니다.\n\n![이미지](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_2.png)\n\n이는 우리가 원하는 곳에서 임베딩 벡터를 잘라도 성능을 너무 많이 희생하지 않고 사용할 수 있다는 효과가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 왜 중요한가요?\n\n만약 우리가 텍스트 임베딩 벡터를 벡터 데이터베이스에 저장하려고 한다고 가정해봅시다. 각 임베딩은 d 차원을 가지고 있습니다. 그리고 각 숫자는 일반적으로 32비트 부동 소수점 수입니다. 그래서 우리는 저장을 위해 n _ d _ 4 바이트가 필요합니다.\n\n그리고 만약 우리가 점곱이나 코사인 유사성과 같은 유사성 지표를 계산하려고 한다면 (코사인 유사성은 단지 정규화된 점곱일 뿐입니다), 차원 d가 클수록 수학적 계산을 더 많이 해야 합니다.\n\n![image](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_3.png)\n\n<div class=\"content-ad\"></div>\n\nMRL을 사용하면 작은 메모리 공간, 빠른 처리 속도 및 따라서 비용 절약에 관심이 있다면, 첫 64차원만 사용할 수도 있습니다. 최상의 하류 성능을 원한다면 모든 차원을 사용합니다. 그리고 그 중간을 선택할 수도 있습니다.\n\n따라서, MRL은 LLM 사용자들에게 내려보기 성능의 작은 저하에 대한 임베딩 크기(비용)의 대가를 거래할 수 있는 능력을 제공합니다.\n\n# Nomic AI에서 MRL 사용하기\n\nNomic의 Matryoshka 텍스트 임베딩 모델 nomic-embed-text-v1.5은 matryoshka_dims = [768,512,256,128,64]로 훈련되었습니다. 해당 모델은 Hugging Face에서 공개적으로 사용할 수 있습니다 [3].\n\n<div class=\"content-ad\"></div>\n\n또 다른 이 인코더 모델의 멋진 기능은 다른 접두사를 지원한다는 것입니다. 이 모델은 [search_query, search_document, classification, clustering] 접두사를 지원하여 각 특정 하류 작업에 대해 더 나은 임베딩을 얻을 수 있습니다.\n\nnomic-embed-text-v1.5 모델이 Massive Text Embedding Benchmark (MTEB)에서 어떻게 수행되는지 살펴보겠습니다:\n\n![image](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_4.png)\n\nPython에서 PyTorch와 Sentence Transformers 라이브러리를 사용하여 모델을 구현해 봅시다:\n\n<div class=\"content-ad\"></div>\n\n\n!pip install torch sentence_transformers einops\n\n\n\nimport torch\nfrom sentence_transformers import SentenceTransformer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n\nmodel = SentenceTransformer(\n    \"nomic-ai/nomic-embed-text-v1.5\",\n    device=device,\n    trust_remote_code=True,\n    prompts={\n        \"search_query\": \"search_query: \",\n        \"search_document\": \"search_document: \",\n        \"classification\": \"classification: \",\n        \"clustering\": \"clustering: \",\n    },\n)\n\n\ndef embed_sentences(\n    model: SentenceTransformer,\n    sentences: list[str],\n    prompt_name: str,\n    matryoshka_dim: int,\n    device: str,\n):\n    assert matryoshka_dim <= 768, \"maximum dimension for nomic-embed-text-v1.5 is 768\"\n    embeddings = model.encode(\n        sentences, prompt_name=prompt_name, device=device, convert_to_tensor=True\n    )\n    embeddings = torch.nn.functional.layer_norm(\n        embeddings, normalized_shape=(embeddings.shape[1],)\n    )\n    embeddings = embeddings[:, :matryoshka_dim]\n    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n    return embeddings.cpu()\n\n\nmatryoshka_dim 매개변수를 사용하여 768차원 임베딩 벡터를 자릅니다. 그런 다음 새로운 임베딩 벡터를 정규화합니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n이제 원하는 차원을 설정하고 위키피디아 텍스트와 RAG(검색 증강 생성)용 쿼리를 인코딩할 수 있습니다.\n\n```js\nmatryoshka_dim = 64\n\nwikipedia_texts = [\n    \"개(Canis familiaris 또는 Canis lupus familiaris)는 늑대의 길들여진 후손입니다.\",\n    \"알베르트 아인슈타인은 1879년 3월 14일 독일 제국의 뷔르템베르크 왕국 울름에서 태어났습니다.\",\n    \"아인슈타인은 어린 시절부터 물리학과 수학에서 뛰어나며, 곧 같은 나이의 아이들만이 보유한 수학적 전문 지식을 습득했습니다.\",\n    \"베르너 칼 하이젠베르크는 독일의 이론 물리학자로 양자역학 이론의 주요 선구자 중 한 명이며, 제2차 세계대전 중 나치 핵무기 프로그램의 주요 과학자였습니다.\",\n    \"스티븐 폴 잡스(1955년 2월 24일 - 2011년 10월 5일)는 기술 거장 애플 주식회사를 공동 창업하여 가장 잘 알려진 미국 사업가, 발명가, 투자가였습니다.\",\n    \"고양이(Felis catus), 일반적으로 가정 고양이 또는 집고양이로 불리는 것은 고양이과에서 유일하게 길들인 종입니다.\",\n]\n\nquestion = [\"알베르트 아인슈타인은 어디에서 태어났나요?\"]\n\nquestion_embedding = embed_sentences(\n    model,\n    sentences=question,\n    prompt_name=\"search_query\",\n    matryoshka_dim=matryoshka_dim,\n    device=device,\n)\n\ndocument_embeddings = embed_sentences(\n    model,\n    sentences=wikipedia_texts,\n    prompt_name=\"search_document\",\n    matryoshka_dim=matryoshka_dim,\n    device=device,\n)\n```\n\n```js\nprint(f\"document_embeddings.shape: {document_embeddings.shape}\")\nprint(f\"question_embedding.shape:  {question_embedding.shape}\")\n>> document_embeddings.shape: torch.Size([6, 64])\n>> question_embedding.shape:  torch.Size([1, 64])\n```\n\n우리는 Matryoshka 텍스트 임베딩의 첫 번째 두 차원을 산포도로 시각화할 수 있습니다. 그러나 이 임베딩 모델은 명시적으로 2차원의 Matryoshka 차원에 최적화되지는 않았습니다.\n\n<div class=\"content-ad\"></div>\n\nmd\n![image](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_5.png)\n\n다음으로, 문서 임베딩을 벡터 데이터베이스에 저장할 수 있습니다. 저는 Faiss를 사용하고 있어요. Faiss는 밀집 벡터의 효율적인 유사성 검색 및 클러스터링을 위한 Meta Research의 오픈 소스 라이브러리입니다 [4].\n\n```bash\n!pip install faiss-cpu\n```\n\n\n\n\n```python\nimport faiss\n\nindex = faiss.IndexFlatIP(matryoshka_dim)\nindex.add(document_embeddings)\n```\n\n\n\n<div class=\"content-ad\"></div>\n\n이 코드는 내적 제품을 사용하여 \"정확한 검색\"을 통해 벡터 데이터베이스를 만듭니다. 이때 IndexFlatIP를 사용하는데, 이는 내적 유사도 측정 방법입니다. 정규화된 임베딩을 사용하고 있기 때문에, 내적과 코사인 유사도는 동일합니다.\n\n이제 index는 여섯 개의 텍스트 임베딩으로 구성된 벡터 데이터베이스입니다:\n\n```js\nprint(index.ntotal)\n>> 6\n```\n\n질문과 가장 유사한 임베딩을 검색하고 상위 k개 결과를 검색해보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\ndistances, indices = index.search(question_embedding, k=6)\nprint(indices)\nprint(distances)\n>> [[1 2 3 4 0 5]]\n>> [[0.9633528  0.729192   0.63353264 0.62068397 0.512541   0.43155164]]\n```\n\n저희 데이터베이스에서 가장 유사한 텍스트는 인덱스 1이며 유사도 점수는 0.96입니다 (최대 점수는 1.0입니다).\n\n```js\n# d=64인 결과\nprint(question)\nprint(wikipedia_texts[1])\n>> ['알버트 아인슈타인은 어디에서 태어났나요?']\n>> '알버트 아인슈타인은 독일 제국의 퀴르템베르크 왕국 울름에서 1879년 3월 14일에 태어났습니다.'\n```\n\n저는 matryoshka_dim=768로 코드를 다시 실행했고 유사한 결과를 얻었습니다. 그러나 더 높은 차원은 더 많은 메모리와 계산이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n\n```js\n# 결과 d=768일 때\nprint(indices)\nprint(distances)\n>> [[1 2 4 3 0 5]]\n>> [[0.92466116 0.645744   0.54405797 0.54004824 0.39331824 0.37972206]]\n```\n\n# MRL 및 양자화\n\n더욱 압축된 임베딩을 원한다면, MRL과 이진 벡터 양자화를 함께 사용할 수 있습니다. 이진 양자화는 임베딩 벡터에서 0보다 큰 모든 숫자를 1로 변환하고 그 외의 숫자를 0으로 변환합니다 [5].\n\n<img src=\"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_6.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n이진 양자화를 사용하면 d 차원의 임베딩 벡터는 오직 d / 8 바이트의 메모리만 필요합니다. 이는 float32 형식의 d \\* 4 바이트와 비교해 크기가 32배로 줄어든 것을 의미합니다 [4]. 그러나 이 축소는 성능 저하와 함께 발생합니다.\n\n# 결론\n\nMatryoshka 손실을 사용하는 임베딩 모델은 훈련 중에 동시에 여러 임베딩 차원에 최적화되어 있습니다.\n\nMatryoshka 표현 학습을 사용하면 LLM 사용자가 텍스트 임베딩 크기를 작게 조정하여 성능 저하를 감수할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n더 작은 임베딩은 더 적은 메모리와 계산을 필요로하며, 이는 장기적으로 많은 비용을 절약할 수 있습니다. 또한 계산이 빨라져서 검색 속도가 빨라지기 때문에, 예를 들어 RAG 애플리케이션에 적합합니다.\n\n# 참고 자료\n\n[1] A. Kusupati 등. (2022), Matryoshka Representation Learning, arXiv:2205.13147\n\n[2] MatryoshkaLoss: https://www.sbert.net/docs/package_reference/losses.html#matryoshkaloss (접근일: 2024년 04월 05일)\n\n<div class=\"content-ad\"></div>\n\n[3] Hugging Face의 nomic-embed-text-v1.5: https://huggingface.co/nomic-ai/nomic-embed-text-v1.5 (접속일: 2024년 04월 05일)\n\n[4] Faiss 문서: https://github.com/facebookresearch/faiss/wiki/Getting-started (접속일: 2024년 04월 05일)\n\n[5] A. Shakir, T. Aarsen, S. Lee (2024), 바이너리 및 스칼라 임베딩 양자화로 훨씬 빠르고 저렴한 검색, Hugging Face 블로그\n","ogImage":{"url":"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_0.png"},"coverImage":"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_0.png","tag":["Tech"],"readingTime":9},{"title":"차이인차이 101","description":"","date":"2024-05-27 14:54","slug":"2024-05-27-Difference-in-Difference101","content":"\n차이인차이(DiD 또는 DD 또는 diff-in-diff)는 무엇인가요? 왜 차이인차이에 관심이 있나요? 오늘은 정책 효과를 연구하는 경제학에서 가장 인기 있는 방법 중 하나에 대한 모든 질문에 답할 거예요.\n\nDiD는 처리 그룹과 대조 그룹 간의 시간에 따른 결과 변화를 비교하여 인과 관계를 추정하는 널리 사용되는 경제학 기법입니다. 처리 그룹과 대조 그룹이 무엇인지에 대한 문제가 있어요. 처리는 정책 또는 변경로 인해 특정 그룹에 영향을 미치는 정책 개입을 말해요. 대조는 개입을 받지 않은 그룹을 말해요. 인과 관계란 원인과 결과의 관계를 의미해요.\n\n우리는 이 방법에 관심을 갖는 이유는 무작위 실험이 불가능한 경우에 정책 변경이나 개입의 효과를 평가하는 데 유용하기 때문이에요. 즉, 때때로 실험은 특정 그룹에 집중되므로 처리를 받은 사람들이 무작위가 아니라는 것을 의미해요. DiD는 무작위화 없이도 개입의 영향을 분리하는 데 도움이 될 거예요.\n\n<div class=\"content-ad\"></div>\n\n이 기사는 개념, 가정, 구현 및 예시 등에 대해 다룰 것입니다.\n\n# DiD란\n\n우리의 연구 질문은: 치료 D가 결과 y에 미치는 영향이 무엇인가요? DiD는 우리에게 치료 그룹이 개입되지 않았다면 치료 그룹에 무슨 일이 일어났을지를 추정할 수 있도록 해줍니다. 이 대역사적 시나리오는 치료의 실제 효과를 이해하는 데 중요합니다. 모든 직업이나 업무는 경제적 성장과 관련하여 세금 인하의 영향을 평가합니다. 또한 공공 정책 분야에서는 새 교통 법규가 사고 발생률에 미치는 영향을 평가합니다. 마케팅에서 DiD는 광고 캠페인이 매출에 미치는 영향을 분석합니다.\n\n![이미지](/assets/img/2024-05-27-Difference-in-Difference101_1.png)\n\n<div class=\"content-ad\"></div>\n\n위 다이어그램을 예로 들어보면 샘플에서 인구 데이터가 있습니다. 여기서는 처리군과 대조군으로 데이터를 나누고 처리군은 개입을 받았습니다. 두 그룹 모두 후기와 전기 변수를 관찰할 수 있습니다.\n\n# DiD 방법\n\n## 간단한 처리/대조 차이 추정기\n\n![DiD 다이어그램](/assets/img/2024-05-27-Difference-in-Difference101_2.png)\n\n<div class=\"content-ad\"></div>\n\n이 방정식은 치료와 대조 그룹 간의 시간 경과에 따른 결과 변화를 비교하여 치료 효과를 계산할 것입니다.\n\n수학을 이해하는 데 도움이 되기 위해 가짜 예제를 만들었습니다.\n\n![다면적 효과](/assets/img/2024-05-27-Difference-in-Difference101_3.png)\n\n위에서 언급한 공식을 사용하면 DiD 계수는 9가 될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## DiD Estimator: 회귀분석을 사용한 계산\n\nDiD는 처리 효과의 추정을 편향시킬 수 있는 시간 불변 특성을 제어하는 데 도움을 줍니다. 이는 시간이 지남에 따라 일정한 변수(예: 지리적 위치, 성별, 인종, 타고난 능력 등)의 영향을 제거한다는 것을 의미합니다. 그 이유는 이러한 특성이 각 그룹에 대해 전·후 처리 기간 모두 동일하게 영향을 미치기 때문입니다.\n\n기본 DiD 모델의 핵심 방정식은 다음과 같습니다:\n\n![DiD equation](/assets/img/2024-05-27-Difference-in-Difference101_4.png)\n\n<div class=\"content-ad\"></div>\n\n여기서:\n\n- y는 𝑡시간에 그룹 j의 개인 i의 결과 변수입니다.\n- 𝐴𝑓𝑡𝑒𝑟은 관측이 사후 처리 기간에 속하는 경우 1과 같은 더미 변수입니다.\n- 𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡은 관측이 처리 그룹에 속하는 경우 1과 같은 더미 변수입니다.\n- 𝐴𝑓𝑡𝑒𝑟 × 𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡은 상호 작용 항이며, 계수 β는 DiD 추정량을 캡처합니다.\n\n상호 작용 항의 계수는 y에서 DiD 추정량입니다. 회귀 분석은 추가 변수의 표준 오차를 제공하고 제어하는 데 도움이 되기 때문에 연구자들 사이에서 더 인기가 있습니다.\n\n# 평행 추세 가정\n\n<div class=\"content-ad\"></div>\n\nDiD에 대한 주요 가정 중 하나입니다. 이것은 치료가 없을 때 치료 그룹과 대조 그룹 간의 차이가 시간이 지남에 따라 일정하게 유지될 것이라는 생각에 기반을 두고 있습니다. 다시 말해, 치료가 없을 때 β (DiD 추정치)=0입니다.\n\n형식적으로, 이는 다음을 의미합니다:\n\n\n| Time        | Treatment Group | Control Group  | Difference   |\n|-------------|-----------------|----------------|--------------|\n| Before      | Y1              | Y0             | Y1 - Y0      |\n| After       | Y3              | Y2             | Y3 - Y2      |\n| DiD Estimate| (Y1 - Y0) - (Y3 - Y2)                    |\n\n\n또 다른 관점은 정책 변경이 없었을 때 두 그룹 간의 차이가 정책 변경 없이도 시간이 지남에 따라 동일하게 유지되었을 것이라는 것입니다. 치료 전에 추세가 평행하지 않으면 DiD 추정치가 편향될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 이 가정을 확인하는 방법\n\n그렇다면 다음 질문은: 어떻게 확인하는 걸까요? 평행 추세 가정의 타당성은 그래픽 분석과 플라시보 테스트를 통해 평가할 수 있습니다.\n\n![그림](/assets/img/2024-05-27-Difference-in-Difference101_6.png)\n\n이 가정은 치료가 적용되지 않은 경우 치료 그룹(주황색 선)과 대조 그룹(파란 점선)이 시간이 지남에 따라 평행한 경로를 따를 것이라는 것입니다. 치료(수직 선)는 치료가 적용되는 지점을 나타내며, 치료 전후에 두 그룹 간의 추세 차이를 비교하여 치료 효과를 추정할 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\n## 평행 추세 가정을 위반하는 예시\n\n간단히 말해서, 우리는 치료에서 두 가지를 찾습니다:\n\n- 기울기의 변화\n\n![image](/assets/img/2024-05-27-Difference-in-Difference101_7.png)\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-27-Difference-in-Difference101_8.png)\n\n위 두 경우 모두 평행한 추세 가정이 만족되지 않습니다. 치료 그룹 결과는 통제 그룹 결과보다 더 빨리 성장하거나(part a) 더 느리게 성장합니다(part b). 이를 수학적으로 표현하면 다음과 같습니다:\n\nDiD = 실제 효과 + 차이 추세 (차이 추세는 0이어야 함)\n\n차이 추세는 양수 (part a) 또는 음수 (part b)일 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\nDiD는 우리가 그것에 대한 차별적인 경향을 가지고 있기 때문에 개입의 영향을 분리할 수 없을 것입니다.\n\n2. 개입 이후의 치료 라인에서의 점프(상승 또는 하락)\n\n![image](/assets/img/2024-05-27-Difference-in-Difference101_9.png)\n\n위의 이미지에서 처리 그룹의 경향이 통제 그룹의 경향과 다르게 변경되어, 개입 없이 일정하게 유지되어야 했던 것과 다릅니다. DiD 연구에서는 점프가 허용되지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n# Placebo Tests\n\nPlacebo 테스트는 관측된 치료 효과가 실제로 치료로 인한 것이 아니라 다른 혼재 요소로 인한 것인지 확인하는 데 사용됩니다. 이는 치료 효과가 예상되지 않는 기간이나 그룹에 동일한 분석을 적용하는 것을 포함합니다. 이러한 플레이스보 테스트에서 유의한 효과가 발견되면, 원래 결과가 잘못된 것일 가능성이 있습니다.\n\n예를 들어, 2019년에 고등학교에 알약을 제공하는 개입 연구가 수행되었습니다. 우리는 플레이스보 테스트를 수행할 수 있어서 2017년이라는 가짜 개입 연도를 만들 수 있습니다. 여기서는 정책 변경이 발생하지 않았다는 것을 알고 있습니다. 플레이스보 날짜 (2017년)에 치료 효과 분석을 적용해도 유의미한 변화가 없는 경우, 2019년에 관측된 효과가 (있는 경우) 실제 정책 개입으로 인한 것일 가능성이 높습니다.\n\n# Extensions and Variations of DiD\n\n<div class=\"content-ad\"></div>\n\n- 이벤트 스터디 DiD: 연도별 처리 효과를 추정하여 처리 효과의 타이밍을 평가하고 사전 추세를 확인하는 데 유용합니다. 이 모델은 연도별 처리 효과를 다양하게 설정할 수 있습니다. 우리는 t+1, t+2, ..., t+n 시점에서 효과를 연구할 수 있습니다.\n- 합성 대조법(SCM): SCM은 여러 비치료 단위에 가중치를 부여하여 치료 전 특성을 근사하는 합성 대조 그룹을 구성합니다. 이 방법은 단일 처리 단위와 비치료 단위 집단을 비교할 때 특히 유용합니다. 여러 단위의 정보를 결합하여 믿을 수 있는 대조 사실을 제시합니다.\n\n이외에도 많은 방법이 있지만, 여기서는 두 가지만 소개하겠습니다. 나중에 더 자세히 설명하는 글을 쓸 수도 있겠네요.\n\n# 결론\n\n본 글에서는 평균 처리 효과를 추정하는 인기 있는 방법인 Difference-in-Differences (DiD) 추정기법을 분석했습니다. DiD는 처리 및 통제 그룹 간 시간 경과에 따른 변화를 비교함으로써 정책 효과를 연구하는 데 널리 사용됩니다. DiD의 주요 장점은 시간이 지남에 따라 일정하게 유지되는 관측되지 않은 혼입변수를 통제하여 개입의 실제 영향을 분리할 수 있는 능력입니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 평행 추세 가정, 전처리 데이터의 중요성, 그리고 시각적 분석과 장소보 테스트를 사용하여 가정 위반을 확인하는 방법과 같은 주요 개념을 탐구했습니다. 게다가, 이원차 차이-DiD의 확장과 변형인 이벤트 스터디 DiD 및 합성 통제 방법에 대해 이야기했는데, 이는 다양한 시나리오에서 추가 통찰력과 견고성을 제공합니다.\n\n# 참고문헌 및 추가 자료\n\n[1] Wing, C., Simon, K., & Bello-Gomez, R. A. (2018). 차이-DiD 연구 설계: 공공 보건 정책 연구를 위한 모범 사례. 공공 보건 연례 보고서, 39, 453–469.\n\n[2] Callaway, B., & Sant’Anna, P. H. (2021). 다중 시간 기간을 갖는 차이-차이 방법. 계량경제학 잡지, 225(2), 200–230.\n\n<div class=\"content-ad\"></div>\n\n[3] Donald, S. G., & Lang, K. (2007). Inference with difference-in-differences and other panel data. The review of Economics and Statistics, 89(2), 221–233.\n\n## 읽어 주셔서 감사합니다!\n\n읽어 주셔서 감사합니다! 🤗 만약 이 게시물을 즐겼고 더 많은 것을 보고 싶다면 팔로우해주세요. 또한 LinkedIn에서도 팔로우할 수 있습니다. 인과 추론 및 데이터 분석에 관한 블로그를 쓸 계획이며, 항상 단순하게 유지하려고 노력합니다.\n\n소소한 주의: 학습을 위해 쓰기 때문에 최선을 다하겠지만 실수가 발생할 수 있습니다. 오류를 발견하시면 알려주세요. 또한 새로운 주제에 대한 제안을 환영합니다!\n","ogImage":{"url":"/assets/img/2024-05-27-Difference-in-Difference101_0.png"},"coverImage":"/assets/img/2024-05-27-Difference-in-Difference101_0.png","tag":["Tech"],"readingTime":6},{"title":"Ollama를 사용하여 모델 실행하기 단계별 안내","description":"","date":"2024-05-27 14:51","slug":"2024-05-27-RunningmodelswithOllamastep-by-step","content":"\n\nLLM을 빠르게 테스트할 수 있는 방법을 찾고 계신가요? 전체 인프라를 설정할 필요 없이 테스트할 수 있는 방법이 있다면 정말 훌륭하죠. 이 짧은 기사에서 우리가 할 일이 바로 그거에요.\n\n![이미지](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png)\n\nOllama에 관해 경험이 있는 경우에는 특정 단락으로 이동해도 됩니다. 이 기사에서 찾을 수 있는 내용은 다음과 같아요:\n\n- Ollama가 무엇인가요?\n- Windows에 Ollama 설치하기.\n- Ollama [cmd] 실행하기.\n- 로컬로 모델 다운로드하기.\n- 다양한 용도에 맞는 다른 모델.\n- 모델 실행하기 [cmd].\n- CPU에 친화적인 양자화된 모델.\n- 다른 소스에서 모델 통합하기.\n- Ollama-파워드 (Python) 앱으로 개발자들의 삶을 더 쉽게 만들기.\n- 요약.\n\n<div class=\"content-ad\"></div>\n\n# 1. Ollama이란?\n\nOllama는 오픈 소스 코드로, 로컬에서 또는 본인의 서버에서 언어 모델과의 원활한 통합을 가능하게 하는 사용 준비 도구입니다. 이를 통해 상업용 API의 유료 버전을 사용하지 않아도 되므로, 특히 이제 Meta가 Llama2 모델을 상용으로 사용 가능하게 한 것을 고려하면, 자신의 데이터셋에서 추가 학습에 적합합니다.\n\n➡️ GitHub 저장소: https://github.com/ollama/ollama\n\n➡️ Ollama 공식 웹페이지: https://ollama.com\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_1.png)\n\n# 2. Windows에서 Ollama 설치하기\n\nOllama는 Windows, Mac 및 Linux에서도 원활하게 작동합니다. 이 간단한 자습서는 특히 Windows 10용 설치 단계를 안내합니다. 설치 후 프로그램은 약 384MB를 차지합니다. 그러나 다운로드한 모델이 가벼운 것은 아닐 수 있습니다.\n\n만약 도커 컨테이너에서 Ollama를 실행하길 원한다면, 아래 설명을 건너뛰고 \n\n감십시오.\n\n<div class=\"content-ad\"></div>\n\n➡️ https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image\n\n![Running Models with Ollama](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_2.png)\n\n➡️ Ollama 홈페이지로 이동하여 .exe 파일을 다운로드하세요: https://ollama.com\n\n![Running Models with Ollama](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_3.png)\n\n<div class=\"content-ad\"></div>\n\nOllama를 다운로드하고 Windows에 설치하세요. 보통 다음 경로에 위치한 기본 모델 저장 경로를 사용할 수 있습니다:\n\n```js\nC:\\Users\\your_user\\.ollama\n```\n\n그러나 C: 파티션에 공간이 제한적이라면, 대안 디렉토리로 전환하는 것이 권장됩니다. D:\\와 같은 다른 파티션이 있는 경우, 간단하게:\n\n- 데스크탑의 컴퓨터 아이콘을 마우스 오른쪽 클릭합니다.\n- 속성을 선택한 후 \"고급 시스템 설정\"으로 이동합니다.\n- 환경 변수를 클릭합니다.\n- ...을 위한 사용자 변수에서 모델을 저장할 디렉토리의 절대 경로를 삽입하십시오. 예를 들면:\n\n<div class=\"content-ad\"></div>\n\n```js\n변수: OLLAMA_MODELS\n값: D:\\your_directory\\models\n```\n\nOLLAMA_MODELS 변수의 이름을 변경하지 마십시오. 이 변수는 Ollama가 정확히 아래와 같이 검색할 것입니다.\n\nWindows의 하단 표시줄에 Ollama 아이콘이 나타납니다. 프로그램이 시작되지 않으면 Windows 프로그램에서 찾아서 거기서 시작하십시오.\n\n<img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_4.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n이제 Ollama를 실행하고 모델을 다운로드할 준비가 되었어요 :)\n\n# 3. Ollama 실행하기 [cmd]\n\n![image](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_5.png)\n\nOllama를 설정하고 나면 윈도우에서 cmd(명령줄)를 열고 로컬로 일부 모델을 다운로드할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\nOllama 로컬 대시보드를 사용하려면 웹 브라우저에서 다음 URL을 입력하세요:\n\n```js\nhttp://localhost:11434/api/\n```\n\nOllama를 실행하는 것은 그렇게 어렵지 않습니다. 나중에 CMD 및 Python 코드를 통해 어떻게 활용하는지 알아보겠습니다.\n\n중요한 몇 가지 명령어:\n\n<div class=\"content-ad\"></div>\n\n로컬로 사용 가능한 모델을 확인하려면 다음을 cmd에 입력하세요:\n\n```js\nollama list\n```\n\n특정 모델에 해당하는 SHA 파일을 확인하려면 cmd에 입력하세요 (예: llama2:7b 모델 확인을 위한 예시):\n\n```js\nollama show --modelfile llama2:7b\n```\n\n<div class=\"content-ad\"></div>\n\n모델을 제거하려면:\n\n```js\nollama rm llama2:7b\n```\n\n모델을 서버에 올리려면:\n\n```js\nollama serve\n```\n\n<div class=\"content-ad\"></div>\n\n# 4. 모델을 로컬로 다운로드하기\n\n웹사이트 ➡️ https://ollama.com/library 에서는 여러 다양한 파라미터 크기로 제공되는 다수의 모델을 다운로드할 수 있습니다.\n\n로컬로 모델을 다운로드하기 전에, 해당 모델을 로딩할 충분한 메모리를 가지고 있는지 확인해주세요. 테스트할 때는 애플리케이션에 통합하기에 적합한 작은 모델인 '7B'로 레이블이 지정된 모델을 사용하는 것이 좋습니다.\n\n⚠️ 부드러운 모델 작동을 위해 적어도 하나의 GPU를 보유하는 것이 강력하게 권장됩니다.\n\n<div class=\"content-ad\"></div>\n\n아래에는 내가 테스트하고 추천하는 여러 모델이 있습니다. 명령을 복사하여 명령 프롬프트에 붙여넣어 지정된 모델을 로컬로 가져올 수 있습니다.\n\n👉Meta에서의 Llama2 모델\n\n대화 시나리오를 위해 최적화된 생성 텍스트 모델 세트입니다. Ollama의 많은 모델과 마찬가지로 Llama2는 다양한 구성으로 제공됩니다:\n\n![image](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_6.png)\n\n<div class=\"content-ad\"></div>\n\n아래는 해당 모델을 가져오는 몇 가지 예시입니다:\n\n표준 모델:\n\n```js\nollama pull llama2\n```\n\n검열되지 않은 버전:\n\n<div class=\"content-ad\"></div>\n\n```js\nollama pull llama2-비겁하지 않은:7b\n```\n\n채팅 7B 모델:\n\n```js\nollama pull llama2:7b-채팅\n```\n\n➡️ 더 읽기: https://llama.meta.com/llama2\n\n\n<div class=\"content-ad\"></div>\n\n👉 구글의 젬마\n\n주요 7B 크기 모델과 유사한 견고한 성능을 제공하는 오픈 소스 모델입니다.\n\n```js\nollama pull gemma:7b\n```\n\n➡️ 자세히 보기: https://blog.google/technology/developers/gemma-open-models/\n\n<div class=\"content-ad\"></div>\n\n👉 Haotian Liu 등의 LLava.\n\n이미지에서 텍스트 설명을 다루는 데 뛰어나며 시각 및 언어 모델 모두에 대한 견고한 지원을 제공하는 멀티모달 모델입니다.\n\n```js\nollama pull llava\n```\n\n➡️ 자세히 알아보기: https://llava-vl.github.io/\n\n<div class=\"content-ad\"></div>\n\n5. 서로 다른 목적을 위한 다양한 모델\n\n![이미지](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_7.png)\n\n일부 모델은 특정 데이터셋에서 훈련되어 코드 완성, 대화 또는 이미지에서 텍스트로 변환과 같은 특정 작업에 더 적합합니다. Ollama에서는 다양한 목적을 위해 설계된 모델을 찾을 수 있습니다.\n\n첫 번째 그룹은 대화, 텍스트 완성, 요약 등을 용이하게 하는 데 초점을 맞춘 모델을 포함하고 있습니다. Gemma, Llama2, Falcon 또는 OpenChat과 같은 모델이 포함됩니다.\n\n<div class=\"content-ad\"></div>\n\n일부 예시:\n\n- [Falcon](https://ollama.com/library/falcon)\n\n- [Gemma](https://ollama.com/library/gemma)\n\n- [Openchat](https://ollama.com/library/openchat)\n\n<div class=\"content-ad\"></div>\n\n다음 그룹은 대화를 나누거나 챗봇 역할을 하는 다중 모달 모델과 이미지 설명(시각 모델), 텍스트 요약, 질문-답변(Q/A) 애플리케이션을 구동할 수 있는 모델들로 구성됩니다.\n\n일부 예시:\n\n➡️ https://ollama.com/library/llava\n\n➡️ https://ollama.com/library/bakllava\n\n<div class=\"content-ad\"></div>\n\n마지막으로, 매우 전문화된 그룹은 Ollama에서 이용 가능한 모델을 활용하여 개발자의 작업을 지원합니다. 코델라마, 돌핀-미스트랄, 돌핀-믹스트랄(코딩 작업에 능숙한 Mixtral 전문가 모델을 기반으로 세밀하게 조정된 모델)과 같은 모델들이 있으며, 계속해서 크리에이터들이 추가하고 있습니다.\n\n몇 가지 예시:\n\n➡️ https://ollama.com/library/codellama\n\n➡️ https://ollama.com/library/dolphin-mistral\n\n<div class=\"content-ad\"></div>\n\n➡️ https://ollama.com/library/dolphin-mixtral\n\n# 6. 모델 실행하기 [cmd]\n\n다운로드한 모델을 실행하려면, ollama run 모델이름:파라미터 \"당신의 프롬프트\"를 입력하세요. 예를 들어:\n\n```js\nollama run llama2:7b \"당신의 프롬프트\"\n```\n\n<div class=\"content-ad\"></div>\n\n다중 모달 모델을 사용하면 기본 프롬프트를 벗어난 파일, 로컬 이미지 경로 등을 포함할 수 있어 더 많은 기능을 확장할 수 있습니다.\n\n# 6. CPU 친화적 양자화 모델\n\n양자화는 모델의 정밀도를 유지하는 비용을 줄이는 것으로 관련 비용을 줄이는 것입니다. 이 과정 뒤에 숨은 직관력을 구축하는 데 도움이 되는 이 크고 훌륭한 기사에서 자세한 설명을 찾아볼 수 있습니다:\n\n📃 양자화 LLMs란 무엇인가? (Miguel Carreira Neves의 글):\n\n<div class=\"content-ad\"></div>\n\n➡️ https://www.tensorops.ai/post/what-are-quantized-llms\n\n추가 자료:\n\n📃 Extreme Compression of Large Language Models via Additive Quantization:\n\n➡️ https://arxiv.org/html/2401.06118v2\n\n<div class=\"content-ad\"></div>\n\n📃 SmoothQuant: 대형 언어 모델을 위한 정확하고 효율적인 사후 훈련 양자화:\n\n➡️ [논문 링크](https://arxiv.org/pdf/2211.10438.pdf)\n\n📃 BiLLM: LLMs를 위한 사후 훈련 양자화 한계 돌파:\n\n➡️ [논문 링크](https://arxiv.org/pdf/2402.04291.pdf)\n\n<div class=\"content-ad\"></div>\n\n간단하게 말하면, 양자화는 가중치 정밀도를 조정하여 모델 크기를 줄이고 중요한 정확도 하락 없이 성능을 쉽게 감소시킬 수 있는 하드웨어에서 실행할 수 있게 해줍니다.\n\n이 글과 함께 제공된 이미지를 통해 양자화 후에 모델이 원래 버전보다 상당히 적은 공간을 차지하는 것을 확인할 수 있습니다:\n\n![Quantized Models](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_8.png)\n\nOllama는 양자화된 모델을 지원하여 별도로 처리하는 부담을 덜어줍니다.\n\n<div class=\"content-ad\"></div>\n\n# 7. 다른 소스에서 모델 통합하기\n\n![Running Models With Ollama Step-by-Step](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_9.png)\n\nOllama의 모델은 다양성을 제공하지만 현재 모든 모델에 액세스할 수 있는 것은 아닙니다. 그러나 로컬에 직접 모델을 통합하는 것은 간단한 프로세스입니다. 새로운 모델을 지역 Ollama에 통합하는 방법을 알아봅시다.\n\nThe Bloke의 HuggingFace 계정에서 많은 양자화된 모델을 사용할 수 있습니다. 의학 논문을 위해서 우리는 편리하게 medicine-chat-GGUF 모델을 선택할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n➡️ https://huggingface.co/TheBloke/medicine-chat-GGUF\n\n해당 링크를 열고 파일 및 버전을 클릭하세요.\n\n![Files and versions](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_10.png)\n\nOllama 모델에 포함하고 싶은 모델을 다운로드하세요:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_11.png\" />\n\nModelfile이라는 빈 파일을 생성하고 아래 지정된 데이터를 삽입하세요 (저장된 모델의 절대 경로로 경로를 대체하십시오). 이 예제는 기본적이며, 모델의 온도, 시스템 메시지 등과 같은 여러 옵션을 포함하여 확장될 수 있습니다. 필요한 경우 파일에서 '#'를 제거하여 해당 옵션을 활성화하세요.\n\n```js\nFROM D:\\...\\medicine-chat.Q4_0.gguf\n# PARAMETER 온도 0.6\n# SYSTEM \"\"\"도움이 되는 의학 조수입니다.\"\"\"\n```\n\nModelfile을 저장한 후, cmd에 다음을 입력하세요:\n\n<div class=\"content-ad\"></div>\n\n```bash\nollama create 모델_이름 -f 모델_파일\n```\n\n# 9. Ollama를 활용한 (Python) 앱으로 개발자의 삶을 더 쉽게 만들기\n\n백그라운드에서 실행되는 Ollama는 일반적인 REST API와 같이 접근할 수 있습니다. 따라서 requests와 같은 라이브러리 또는 조금 더 발전된 FastAPI, Flask 또는 Django와 같은 프레임워크를 사용하여 응용 프로그램에 쉽게 통합할 수 있습니다.\n\nOllama python 패키지를 쉽게 pip를 통해 설치하세요.\n\n<div class=\"content-ad\"></div>\n\n⬆️ https://pypi.org/project/ollama/0.1.3:\n\n```js\npip install ollama\n```\n\nPython 코드를 통해 임베딩을 생성하는 방법:\n\n```js\nimport ollama\n\nembedding = ollama.embeddings(model=\"llama2:7b\", prompt=\"Hello Ollama!\")\n```\n\n<div class=\"content-ad\"></div>\n\n간단히 CURL을 사용하여:\n\n```js\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"llama2:7b\",\n  \"prompt\": \"Here is an article about llamas...\"\n}'\n```\n\nOllama 엔드포인트에 대해 더 알아보려면 다음 링크를 방문해주세요:\n\n➡️ https://github.com/ollama/ollama/blob/main/docs/api.md\n\n<div class=\"content-ad\"></div>\n\nOllama가 Langchain 프레임워크에 원활하게 통합되어 개발 노력을 최적화하고 기술 측면의 작업을 더욱 간편하게 만들었습니다:\n\n➡️ https://python.langchain.com/docs/integrations/llms/ollama\n\n임베딩을 만드는 간단함을 감상해보세요:\n\n```js\n# pip install langchain_community\nfrom langchain_community.embeddings import OllamaEmbeddings\n\n\nembed = OllamaEmbeddings(model=\"llama2:7b\")\nembedding = embed.embed_query(\"Hello Ollama!\")\n```\n\n<div class=\"content-ad\"></div>\n\n# 10. 요약\n\n본 기사는 당신을 Ollama를 사용하여 모델을 실행하는 과정을 단계별로 안내하여, 전체 인프라 구성 없이 LLM을 테스트할 수 있는 원활한 방법을 제공합니다.\n\n올라마는 오픈 소스 도구로, Meta의 Llama2 모델을 무료로 사용할 수 있게 해주는 로컬 또는 서버 기반의 언어 모델 통합을 용이하게 합니다. 윈도우에서의 설치 과정과 명령줄을 통해 Ollama를 실행하는 방법에 대해 설명되어 있습니다.\n\n이 기사에서는 모델 다운로드, 특정 작업을 위한 다양한 모델 옵션, 다양한 명령어를 사용하여 모델 실행, CPU 친화적인 양자화된 모델, 그리고 외부 모델 통합에 대해 탐구합니다. 또한, 개발자들을 위해 Ollama를 활용한 파이썬 애플리케이션을 강조하고 있습니다.","ogImage":{"url":"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png"},"coverImage":"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png","tag":["Tech"],"readingTime":10}],"page":"104","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":12,"currentPageGroup":5},"__N_SSG":true}