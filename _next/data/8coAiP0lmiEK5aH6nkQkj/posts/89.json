{"pageProps":{"posts":[{"title":"데이터에서의 마이크로서비스 대 모놀리식 접근 방식","description":"","date":"2024-06-19 05:26","slug":"2024-06-19-MicroservicesvsMonolithicApproachesinData","content":"\n\n# 소개\n\n데이터 공간에서 도구를 선택하는 일이 어렵다는 것을 설득하는 데 많은 단어를 쏟을 필요가 없습니다. 이 작업에는 수백 가지, 혹은 몇 천 가지의 방법이 있을 수도 있습니다.\n\n사람들이 종종 간과하는 것은 아키텍처가 이러한 결정에 어떻게 영향을 미치는지입니다.\n\n약 20년 전에는 응용프로그램이 필요한 회사가 소유한 컴퓨터에 있었습니다 – 이를 \"온프레미스\"라고 합니다. 이 컴퓨터를 소유하는 것은 아키텍처적인 결정이었습니다. 그 결과, 그 시기의 아키텍처와 호환되지 않아 클라우드 소프트웨어에 대한 수요가 없었기 때문에 클라우드 소프트웨어 업체는 존재하지 않았습니다.\n\n<div class=\"content-ad\"></div>\n\n2024년으로 빨리 이동하면 반대로 됩니다 - 대부분의 사람들이 완전히 클라우드로 이동했습니다. 그러나 우리 중 일부는 아직도 자체 서버를 운영하고 있습니다. 다른 사람들은 하이브리드 모델을 갖고 있습니다. 이는 아키텍처가 선택한 솔루션에 미치는 영향을 이해하는 것이 이전보다 더 중요하다는 것을 의미합니다. 이 글에서는 데이터 아키텍처에 대한 마이크로서비스 대 모놀리식 접근 방식이 도구 구매에 어떻게 영향을 미치는지 살펴보겠습니다.\n\n# 마이크로서비스 vs. 모놀리식 아키텍처\n\n데이터에서 일어나는 일 중 하나는 마이크로서비스 대 모놀리스 논쟁이 다시 시작된 것입니다. 이게 무엇인지 이해하려면:\n\n데이터 내에서 10년 전에는 모놀리식 응용프로그램이 꽤 흔했습니다. 이것의 예로는 대규모 Airflow 저장소가 있습니다. 이 저장소에는 데이터 수집 코드, 데이터 변환 코드 및 비즈니스 자동화가 포함되어 있을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n비지니스 자동화에는 대시보드 새로 고침, 보고서 전송 또는 작업 실패 시 알림 전송과 같은 것이 포함될 수 있습니다.\n\n한편 클라우드의 폭발성 증가는 부분적으로 벤처 투자를 통해 주어진 것이며 (이 보고서 참조), 데이터 아키텍처의 등장을 불러왔습니다. 이러한 데이터 아키텍처는 마이크로서비스를 밀접하게 닮았습니다. 마이크로서비스에는 일괄 데이터 이동, 데이터 변환 및 데이터 웨어하우스 관리용 애플리케이션이나 스트리밍 데이터 관리용 애플리케이션이 포함됩니다. \n\n데이터 스택을 단일체(monolith) 또는 마이크로서비스로 볼지 여부는 소프트웨어 선택에 영향을 미쳐야 합니다.\n\n<img src=\"/assets/img/2024-06-19-MicroservicesvsMonolithicApproachesinData_0.png\" />\n\n<div class=\"content-ad\"></div>\n\n# 마이크로서비스 데이터 아키텍처: 가정\n\n마이크로서비스 아키텍처를 선택하면, 당신은 당신의 마이크로서비스가 무엇을 수행하길 원하는지, 그리고 어떻게 통신하길 원하는지에 대한 몇 가지 가정을 암묵적으로 한다. 이러한 것들이 당신이 작업을 수행할 때 무엇을 선택해야 하는지에 영향을 미칠 것이다.\n\n## 가정 1: 분산 인프라\n\n마이크로서비스를 사용하면 데이터 팀은 클라우드와 온프레미스 인프라의 다른 부분에 인프라를 분배할 수 있다. 이는 각 마이크로서비스가 자신의 일에 특화될 수 있음을 의미한다. 이에는 두 가지 이점이 있다.\n\n<div class=\"content-ad\"></div>\n\n- 전문화와 효율성\n\n데이터 수집, 변환 등에 동일한 인프라를 사용하는 대신, 각각 다른 용도에 다른 부분을 사용할 수 있습니다. 이는 더 높은 전문화 수준과 궁극적으로 더 큰 효율성으로 이어집니다.\n\n2. 더 쉬운 확장성\n\n일반적으로 워크로드를 다른 마이크로서비스로 분리하면, 메모리 및 저장 공간과 같은 컴퓨팅 성능이 다양한 위치로 분할됩니다. 이는 서비스를 확장하거나 축소하는 작업이 덜 복잡해집니다. 예를 들어, 쿠버네티스 클러스터를 유지하는 대신 기본 워크로드를 서버리스 기능(자동으로 확장)에서 실행할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-MicroservicesvsMonolithicApproachesinData_1.png)\n\n## 가정 2: 높은 상호 운용성과 거버넌스\n\n마이크로서비스 인프라를 가정할 때, 이 서비스들이 서로 통신하고 상호 운용 가능하다는 것을 암시합니다. 소프트웨어 엔지니어링에서는 계약, 이벤트 기반 통신 및 Datadog와 같은 중앙 집중식 로깅 시스템을 통해 이를 가능하게 합니다.\n\n이를 데이터로 옮기는 것은 까다로울 수 있습니다. 데이터 수집 서비스, 데이터 웨어하우스에 위치한 데이터 변환 서비스 및 BI 도구(제3자 SAAS)가 있다면, 이들을 어떻게 연결하고 함께 거버넌스할 수 있을까요?\n\n\n<div class=\"content-ad\"></div>\n\n전통적인 방법은 Airflow와 같은 도구를 사용하여 그들을 함께 엮는 것인데, 이 방법은 매번 점을 연결하려면 코드를 작성해야 한다는 것입니다. 이 비용은 높습니다. 이 때문에 상호 운용성이 줄어들어 가정을 위반합니다.\n\n게다가, 많은 \"현대 데이터 스택\" 아키텍처에서, 엔드 투 엔드 가시성과 거버넌스를 얻는 유일한 방법은 데이터 카탈로그나 가시성 도구와 같은 추가 도구를 구매하는 것입니다. 이러한 도구들은 매우 비싸기 때문에 많은 데이터 팀이 마이크로서비스 아키텍처를 가정하는 상황에서 상호 운용성 부재와 거의 기본적인 거버넌스를 수용하게 됩니다.\n\n## 가정 3: 허용 가능한 데이터 이그레스와 저장\n\n데이터 아키텍처 내의 마이크로서비스에서 생성된 데이터와 메타데이터에는 많은 가치가 있습니다. 이는 여러 곳에 저장되지만 서비스 간에 공유됩니다. 이는 마이크로서비스 스타일 아키텍처에서 허용 가능하다고 가정되는 \"데이터 이그레스\" 비용을 필요로 합니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어 “ELT” 패턴을 채택하고 간단히 “Snowflake에 모든 데이터를 덤프” 한다면, 모든 데이터를 먼저 처리하는 것보다 입력 요금이 더 많이 발생할 수 있습니다. 일반적으로 아키텍처가 이벤트 주도 방식으로 통신하길 원한다면 먼저 \"T\"를 수행하는 것이 좋습니다. 예를 들어 Kafka는 이를 지원합니다.\n\n최근에 많이 확산된 일부 다른 도구에도 일감을 비쳐 줍니다. 예를 들어 Observability, Lineage 또는 이상 탐지 도구를 살펴보세요. 모든 이 도구는 기본 시스템에서 데이터를 (상당량의 데이터로) 가져옵니다. 이는 이 도구를 사용함으로써 이러한 데이터에 대한 출발 요금을 지불해야 한다는 것을 의미합니다.\n\n둘째, 이러한 데이터를 저장하므로 여기에도 저장 요금을 내야 합니다. 셋째, 제공되는 데이터 제품을 전달하는 데 사용하는 일정과 관련이 없는 일정에 실행되기 때문에 비용 부담이 될 수 있습니다. 마지막으로, 그리고 중요하게, 이미 갖고 있는 것과 상관없는 새로운 마이크로서비스인 경우에 컨텍스트를 함께 모아서 결과를 얻지 못하는데, 이는 불필요하게 높은 연산 비용으로 이어집니다. 결과적으로 이 비용을 지불하게 됩니다.\n\n`/assets/img/2024-06-19-MicroservicesvsMonolithicApproachesinData_2.png`의 이미지를 확인해보세요.\n\n\n\n<div class=\"content-ad\"></div>\n\n나는 거대한 건물에 대한 건축적 고려에 대해 다루지 않겠어요, 그것은 다음 포스트에서 다룰 주제일 거예요!\n\n# 마이크로서비스 또는 모듈식 아키텍처를 위한 배운 점\n\n많은 데이터 응용 프로그램이 상호 작용하거나 조정되는 것은 정말 마이크로서비스가 아니에요 – 이 글을 읽는 소프트웨어 엔지니어들은 이에 대해 제게 비난할 거에요! 그러니까 이를 “모듈식”이라고 부르자구요.\n\n이 \"모듈식\" 방법론은 최신 데이터 스택에 부합되어 있어요. 그러나 최신 데이터 스택은 가정 2와 3을 위반해요.\n\n<div class=\"content-ad\"></div>\n\n가정 2를 위반합니다. MDS가 상호 운용 가능하고 자체와 통신할 수 있는 정도가 매우 낮기 때문입니다. 실제로 작동시키기 위한 유일한 방법은 많은 코드가 포함된 거대한 Airflow 저장소가 있는 것입니다. 이는 단일체 아키텍처를 구축할 때 자연스러운 일입니다.\n\n가정 3을 위반합니다. 이집트 및 저장 비용의 수용 가능한 수준이 실제로 없습니다. 공간/계산이 분리된 여러 응용 프로그램을 갖는 경우, 새로운 소프트웨어를 추가할 때마다 데이터에 대해 두 배의 비용을 지불해야 합니다.\n\n보다 명확히 설명하기 위해 종종 \"만약 내가 직접 이것을 구축해야 한다면 어떻게 보였을까요?\"라는 질문을 합니다.\n\n대답은 항상 \"[클라우드 제공자 이름을 여기에 삽입]\"의 내부 개발 서비스입니다. 대부분의 데이터가 이미 클라우드에 있으므로 클라우드 제공업체가 원하는 서비스를 제공한다면, 데이터 전송 및 저장에 대해 두 번 지불할 필요가 없습니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 카탈로그에 대해 생각해보세요. 아키텍처 측면에서 데이터 카탈로그는 데이터 웨어하우스(예: BigQuery)로부터 모든 메타데이터를 가져와 저장하고, 해당 데이터를 기반으로 일부 계산을 수행하고 UI를 제공합니다.\n\n이러한 종류의 아키텍처를 사용하여 직접 구축하면 이직료를 지불해야 하고 추가 저장 공간을 지불해야 하며 약간의 추가 컴퓨팅 리소스를 사용해야 합니다(카탈로깅 컴퓨팅은 매우 간단하므로 저렴합니다). 또한 내 클라우드에 배포하지 않는 한 보안 문제도 생길 수 있습니다.\n\n이러한 대부분의 비용은 불필요합니다. 이미 데이터를 소유한 서비스에서 이러한 작업을 수행해야 합니다. 또는 데이터가 레이크하우스/가상화 패턴과 같이 여러 서비스에서 이용할 수 있어야 합니다. 그러나 이것을 카탈로그처럼 구축하지 않을 것입니다 - 이전 다이어그램의 갈색 상자들처럼 말이죠.\n\n하지만 기다려보세요 - GCP에 이와 비슷한 기능이 있지 않나요? 이름이 기억이 나지 않는데, 입에서 막 말하려 했는데... 아! 기억했어요! 바로 데이터 카탈로그입니다!\n\n<div class=\"content-ad\"></div>\n\n만약 BigQuery를 사용 중이라면, 이미 존재하는 데이터 카탈로그를 사용하는 것이 아키텍처적으로 훨씬 더 합리적일 것입니다. 데이터가 나가거나 중복 저장, 추가적인 보안 위험도 없으며, 아마도 동일한 용어를 사용하고 동일한 위치(Chrome 탭)에 있는 더 나은 UI를 제공할 것입니다.\n\n따라서, \"그 곳에 있어야 하는\" 것들은 항상 다른 제3자 SaaS 도구 대비 우위를 가지게 될 것이며, 제공 비용이 낮기 때문에 장점을 가질 수 있습니다.\n\n이것은 아래 다이어그램에서 설명할 수 있으며, GCP는 Collibra와 같은 회사에 비해 배송 비용이 낮습니다. 장기적으로, 이러한 서비스들은 클라우드 제공업체가 제작한 내부 도구에 비해 매우 효율적이고 탁월해야 합니다.\n\n<img src=\"/assets/img/2024-06-19-MicroservicesvsMonolithicApproachesinData_3.png\" />\n\n<div class=\"content-ad\"></div>\n\n장기적인 관점에서 GCP의 데이터 카탈로그는 항상 Collibra보다 저렴할 것입니다. 기억하세요, GCP는 중앙 비용을 고용하고 다른 회사를 설립할 필요가 없습니다. 단지 데이터 카탈로그를 제공하기 위해 100명의 개발자들을 고용합니다.\n\n같은 가격으로, 클라우드 공급업체가 실행 및 효율적이라면, 제공 비용이 너무 높기 때문에 제3자 SaaS 공급업체가 경쟁할 수 없습니다. 물론 그들의 서비스가 현재 상당히 더 나은 경우(현재 그렇다고 말할 수 있습니다).\n\n# 결론\n\n이 기사에서는 데이터 구조가 데이터 엔지니어와 아키텍트가 응용 프로그램이 어떻게 실행되고 서로 작동하는지에 대한 가정을 만들도록 강제한다는 것을 보았습니다. 마이크로서비스 또는 \"모듈식\" 아키텍처를 채택하는 경우, 좋은 아키텍처는 몇 가지 가정을 해야 한다는 것을 보았습니다.\n\n<div class=\"content-ad\"></div>\n\n이 가정들은 Modern Data Stack을 통합하여 사용하고 마이크로서비스를 사용할 때도 깨져요. 또한 낮은 상호 운용성 및 높은 외부 통신/저장 비용 때문에 별도의 SAAS로도 사용됩니다.\n\n재미있는 점은 클라우드 인프라 제공 업체가 독립 소프트웨어 공급 업체와 비교하여 추가 기능을 개발할 때 우위에 있는 것입니다. 그 이유는 그들이 이미 모든 데이터를 보유하고 있기 때문에 더 많은 자원과 구조 이점이 있기 때문입니다.\n\n예시로 클라우드 제공 업체 (GCP)를 데이터 카탈로그 제공 업체 (Collibra)와 비교해 보았습니다. 이것은 소프트웨어 제공 업체 중 많은 업체에 확장될 수 있지만 모든 업체에는 해당하지 않습니다. 일반적으로 감사, 카탈로그, 데이터 품질 등과 같은 제어 평면의 도구들이 기존 인프라에 잘 맞습니다.\n\n클라우드 인프라로 데이터를 가져오는 데 많은 컴퓨팅을 필요로 하는 도구들은 외부에 배치하는 것이 합리적해요. 예시로는 스트리밍 플랫폼이나 데이터 수집 도구 등이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n가장 흥미로운 점은 Airflow와 Monte Carlo와 같은 Orchestration 및 Observability 도구가 모던 데이터 스택에서 사용되는 모듈식 아키텍처에 아키텍처적으로 매력적이지 않은 패턴을 부담한다는 것이죠.\n\n아키텍처와 비용이 데이터 영역에서 사람들이 내리는 선택에 어떻게 영향을 미치기 시작하는지를 보는 것은 흥미로울 것 같습니다. 자유 시장은 신비로운 방식으로 작용합니다. 그 영향력은 다른 곳보다 데이터에서도 덜하지 않습니다! 💸\n\n만약 단일체 아키텍처, 다른 도구 또는 올인원 데이터 플랫폼에 이 관점을 적용하는 방법에 대해 알고 싶다면 언제든 연락해 주세요.","ogImage":{"url":"/assets/img/2024-06-19-MicroservicesvsMonolithicApproachesinData_0.png"},"coverImage":"/assets/img/2024-06-19-MicroservicesvsMonolithicApproachesinData_0.png","tag":["Tech"],"readingTime":7},{"title":"데이터 분석을 위한 SQL 배우는 법","description":"","date":"2024-06-19 05:24","slug":"2024-06-19-HowtoLearnSQLforDataAnalytics","content":"\n\n<img src=\"/assets/img/2024-06-19-HowtoLearnSQLforDataAnalytics_0.png\" />\n\n그래서... 데이터 분석가가 되고 싶으시군요.\n\n혹시 이미 풀타임 직업을 가지고 계시고 데이터 산업으로의 전직을 희망하시는 건가요?\n\n아니면 아직 이 분야에 완전히 처음이시고 분석 분야의 채용정보를 집중적으로 찾으시는 분일 수도 있겠군요.\n\n<div class=\"content-ad\"></div>\n\n어떤 분야에 속하든 상관없이 데이터 분석 인터뷰에서 항상 시험을 받게 되는 하나의 기술이 있습니다: SQL.\n\n이 글에서는 SQL을 처음부터 배우기 위한 완전한 로드맵을 제시하겠습니다.\n\n비디오 버전을 보려면 여기를 클릭하세요:\n\n이 글은 세 가지 섹션으로 나뉘어 있습니다. 원하는 섹션으로 건너뛰어도 괜찮습니다:\n\n<div class=\"content-ad\"></div>\n\n- SQL은 무엇이며 데이터 분석가에게 꼭 필요한 기술인 이유는 무엇인가요?\n- SQL을 배우는 방법은 무엇인가요?\n- SQL 면접 준비는 어떻게 해야 하나요?\n\n첫 두 가지 항목을 광범위하게 다루는 많은 자료를 본 적이 있지만, 많은 사람들이 SQL 학습 과정에서 가장 중요한 면접 준비 부분을 간과하는 경향이 있습니다.\n\n온라인 강좌를 수강하는 것과 SQL로 비즈니스 문제를 해결하는 것 사이에는 세계적인 차이가 있습니다. 이것은 기술 면접의 SQL 파트에서 처참히 실패한 후에 깨달은 교훈이었습니다.\n\n제 접근 방식을 변경한 후에야 구직 제의를 받기 시작했습니다.\n\n<div class=\"content-ad\"></div>\n\n내 의견으로는, SQL 학습 여정 중 1/4 정도만 강의를 수강하고 콘텐츠를 소비하는 데 사용해야 한다고 생각해요.\n\n나머지 시간은 면접 준비에 할애해야 해요, 왜냐하면 취직에 실패하면 이 모든 노력이 무의미할 수 있거든.\n\n그럼 시작해볼까요?\n\n# 먼저... SQL이 뭔지, 그리고 왜 배워야 하는지를 알아볼게요.\n\n<div class=\"content-ad\"></div>\n\n대부분의 회사들은 데이터를 데이터베이스에 저장합니다.\n\n그리고 데이터 분석가로서 여러분은 이 데이터를 분석하고 비즈니스 인사이트를 도출해야 합니다.\n\n예를 들어, 마케팅팀이 새로운 스타워즈 상품을 홍보해야 할 대상을 알고 싶어할 때 당신이 그 사람이 될 것입니다.\n\n당신은 데이터베이스에서 이러한 인사이트를 질의해야 합니다. 그리고 이 작업을 수행하기 위해 SQL이라는 프로그래밍 언어를 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\nSQL은 Structured Query Language의 약자입니다.\n\n스타워즈 상품 예시에서, 회사의 데이터베이스는 다양한 테이블로 이루어져 있다고 상상해보세요.\n\n각 테이블은 다른 종류의 정보를 추적합니다. 고객 세부 정보를 저장하는 \"Customers\" 라는 테이블, 상품 세부 정보를 저장하는 \"Products\" 테이블, 그리고 거래 데이터를 유지하는 \"Sales\" 테이블 등이 있을 수 있습니다:\n\n![Database Tables](/assets/img/2024-06-19-HowtoLearnSQLforDataAnalytics_1.png)\n\n<div class=\"content-ad\"></div>\n\n우리는 SQL 쿼리를 사용하여 이러한 테이블과 상호 작용할 수 있어요.\n\n데이터베이스의 모든 고객을 선택하려면 다음과 같은 쿼리를 작성하면 돼요:\n\n```js\nSelect * from Customers\n```\n\n쉽죠? 😊\n\n<div class=\"content-ad\"></div>\n\nSQL은 실제로 꽤 간단해요.\n\n일반 영어와 비슷해서 직관적이에요.\n\n사실, 엑셀을 알고 있다면 - 셀 추가하거나 피벗 테이블로 데이터를 그룹화하는 등 - SQL을 쉽게 배울 수 있어요.\n\n# SQL 배우는 방법?\n\n<div class=\"content-ad\"></div>\n\n이제 SQL이 무엇인지 알았으니 어떻게 이 언어를 배울 수 있을까요?\n\nSQL을 가르치는 수천 개의 부트캠프와 온라인 강좌가 있습니다.\n\n여기서 설명되는 데이터 분석을 위한 핵심 SQL 개념에 대해 다루는 한 이러한 학습 자료 중 하나를 선택할 수 있습니다.\n\n## 단계 1: SQL 환경 설정\n\n<div class=\"content-ad\"></div>\n\nSQL 구문 배우기 전에 데이터베이스 관리 시스템(DBMS)을 설정해야 합니다.\n\n이것은 SQL 쿼리를 작성하고 실행할 데이터베이스 시스템 설정입니다.\n\n여기 10분 이내에 설정하는 데 도움이 되는 YouTube 비디오가 있습니다.\n\nMySQL, PostgreSQL, SQLite 등 여러 유형의 데이터베이스 시스템을 선택할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 글에서는 이러한 데이터베이스 시스템 간의 세세한 차이에 대해서 다루지 않을 거예요.\n\n지금은 SQL과 PostgreSQL이 가장 인기 있는 선택지인 것을 알고 계시면 충분할 거예요.\n\n## 단계 2: SQL 명령어 배우기\n\n데이터 분석가로서 데이터베이스에서 통찰을 추출하고 그것을 비즈니스 결정에 활용할 거예요.\n\n<div class=\"content-ad\"></div>\n\n아래는 배워야 할 명령어를 세 가지 섹션으로 나누어 놓았어요:\n\n## 초보자\n\nSQL 초보자 레벨의 쿼리에는 다음과 같은 기본 개념이 포함되어 있어요:\n\n- Select\r\n- Where clause\r\n- From clause\r\n- Data types\r\n- Order by\r\n- Limit\n\n<div class=\"content-ad\"></div>\n\n이러한 개념들은 직관적이며, 배우는 데 단 1~2일이 걸릴 것입니다.\n\n## 중급\n\n다음 수준의 SQL 쿼리에는 다음이 포함됩니다:\n\n- Group by\n- Having\n- Joins (inner, outer, left, right, self)\n- Subqueries\n- Case statements\n\n<div class=\"content-ad\"></div>\n\n이 SQL 쿼리들 또한 꽤 쉽습니다. 데이터를 병합하거나 피벗 테이블을 만들거나 지난번에 \"if\" 문을 작성했다면, 이 SQL 개념들을 아주 빠르게 습득할 수 있을 겁니다.\n\n그렇지 않다면, 위의 명령어들을 배우는 데 약 4~5일이 소요될 것입니다.\n\n## 고급\n\n언어의 기본을 마스터 했다면, 다음과 같은 더 고급 개념들을 배우실 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 공통 테이블 표현식 (CTEs)\n- Pivot 테이블\n- 윈도우 함수\n- 성능 최적화\n\n위의 개념들을 이해하고 그 원리를 파악하는 데 약 일주일이 소요될 것입니다.\n\n# 이 SQL 개념을 배울 수 있는 곳은?\n\n다음은 위의 개념을 가르쳐줄 무료 학습 자료 3가지입니다:\n\n<div class=\"content-ad\"></div>\n\n## 1. 4시간 만에 SQL 배우기 (Luke Barousse)\n\n이것은 데이터 분석가로서 알아야 할 거의 모든 SQL 명령을 다루는 무료 YouTube 강의입니다.\n\n이 비디오의 제작자인 Luke Barousse는 데이터 분석가이기 때문에 이 강의를 분석 분야에 맞게 제작했습니다.\n\n튜토리얼에 나오는 모든 사용 사례와 문제 설명은 기타 강의에서 산업 범위를 벗어난 내용을 다루는 반면, 데이터 분석가에게 관련이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 2. 초보자를 위한 MySQL (모쉬와 함께 프로그래밍)\n\n이 강의는 Luke의 튜토리얼 대안으로 수강할 수 있습니다. 동일한 개념들이 다루어지기 때문이에요.\n\n하지만, 저는 2:18까지만 수강하는 것을 추천드립니다. 그 이후에 가르치는 개념들은 데이터 분석가들이 일반적으로 수행하지 않는 데이터베이스 수정 및 업데이트 작업에 초점을 맞추고 있어요.\n\n## 3. W3Schools\n\n<div class=\"content-ad\"></div>\n\nW3Schools는 다양한 프로그래밍 언어를 다루는 대화식 웹사이트입니다.\n\n위 자료에서 다루지 않은 주제를 학습하려면 이 사이트의 SQL 튜토리얼 섹션을 방문하는 것을 추천합니다.\n\n게다가, 이 플랫폼은 샘플 데이터베이스를 사용하여 직접 쿼리를 실행할 수 있는 대화식 인터페이스를 제공합니다. 따라서 자신의 SQL 환경에 접근할 수 없는 경우에 유용할 수 있습니다.\n\n# SQL 면접 준비하기\n\n<div class=\"content-ad\"></div>\n\n이전에 이 기사에서 언급한 대로, 튜토리얼을 소비하는 것과 문제를 해결하기 위해 SQL 쿼리를 작성하는 것 사이에는 큰 차이가 있습니다.\n\nSQL 인터뷰에서 통과하고 싶다면 문제 해결을 위해 데이터베이스에서 쿼리를 연습해야 합니다.\n\n저는 HackerRank와 LeetCode에 가입하는 것을 추천합니다 — 이 두 사이트는 가장 인기 있는 프로그래밍 챌린지 플랫폼 중 두 곳입니다.\n\n이 사이트들은 쉬운 것에서 어려운 것까지 다양한 난이도의 SQL 연습 문제를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n간단한 것부터 시작해서 조금씩 어려운 문제로 나아가세요.\n\n이 사이트에서 적어도 50개의 도전 문제를 완료하는 것을 추천합니다.\n\n나의 경험 상, SQL 인터뷰 문제는 이러한 사이트에서 가져온 것이거나 이 플랫폼의 문제와 매우 유사하게 구성되어 있습니다.\n\n또한 SQL 기술을 향상시키는 데 도움이 되는 생성 AI를 사용하는 것을 추천합니다.\n\n<div class=\"content-ad\"></div>\n\nChatGPT에게 샘플 데이터셋과 연습 문제를 요청하고 답변을 평가해 보세요. 개념을 이해하는 데 어려움을 겪을 때 특히 유용합니다. 저에게는 \"셀프 조인\"이 그랬어요. 학습 과정 초반에 셀프 조인이 낯설고 언제 적용해야 하는지 잘 모르겠던 걸요.\n\n<div class=\"content-ad\"></div>\n\n저는 테이블을 자기 자신과 결합하는 개념을 상상할 수 없었어요.\n\n이 명령어를 더 잘 이해하기 위해, ChatGPT에게 여러 가지 예시를 들어 설명해달라고 여러 번 요청했어요.\n\n그 후에 챗봇에게 셀프 조인에 관한 다양한 난이도의 연습 문제를 많이 풀게 했어요.\n\n결국 그 주제에 대해 이해하게 되었어요!\n\n<div class=\"content-ad\"></div>\n\n# 데이터 분석을 위한 SQL 학습 — 다음 단계\n\n이 기사에서 많은 내용을 다루었습니다.\n\n끝까지 따라오셨다면, 축하합니다!\n\n이제 SQL이 무엇이고, 분석을 위한 필수 기술인 이유, 그리고 어떻게 배울 수 있는지 알게 되었습니다.\n\n<div class=\"content-ad\"></div>\n\n그 다음 단계로서 매일 SQL을 배우는 것을 권장합니다.\n\n적어도 한 달 동안 매일 약 1-2시간을 SQL 학습에 투자해보세요.\n\n이것을 30일간의 SQL 도전 이라고 부르겠습니다.\n\n첫 주 정도는 SQL 명령어와 데이터베이스 관리에 익숙해지는 데 시간을 투자해보세요.\n\n<div class=\"content-ad\"></div>\n\n그럼, 나머지 시간을 활용하여 HackerRank나 Leetcode와 같은 사이트에서 SQL을 연습해보세요. 이 두 플랫폼을 통해 적어도 50가지 이상의 연습 문제를 완료해보세요.\n\n30일 동안의 도전을 마칠 때쯤에는 어떤 데이터 인터뷰에서도 SQL 부분을 뛰어넘을 준비가 되어 있을 거예요.","ogImage":{"url":"/assets/img/2024-06-19-HowtoLearnSQLforDataAnalytics_0.png"},"coverImage":"/assets/img/2024-06-19-HowtoLearnSQLforDataAnalytics_0.png","tag":["Tech"],"readingTime":6},{"title":"만일 당신이 이 BigQuery 이메일을 놓쳤다면 SQL 쿼리를 잃을 수도 있습니다","description":"","date":"2024-06-19 05:23","slug":"2024-06-19-IfYouMissedThisBigQueryEmailYouCouldLoseYourSQLQueries","content":"\n\n클라우드 및 데이터 엔지니어링의 세계를 처음 접하고 계신가요? 전문가로서의 여정을 시작할 때 포트폴리오로 시작해보세요. 무료 5페이지 프로젝트 아이데이션 가이드를 받아보세요.\n\n본 게시물에는 제휴 링크가 포함되어 있지 않습니다.\n\n기업 자산을 보호하는 것 외에도 좋은 기업 사이버 보안 팀은 필요한 교육과 함께 미끄러지면 피싱 테스트에 실패할 수 있다는 사실로 무장하고 하나의 일을 정말 잘 할 수 있습니다: 이메일을 열 때 정말로 걱정스럽게 만든다. 나는 현재 가짜 CEO가 와이어 이체를 요청하거나 \"스타벅스\"가 비트코인으로 기프트 카드를 다시로드하도록 하려는 메시지를 방어하는 데 패배한 적이 없어서, 아무런 필요하지 않은 긴급함을 전달하는 이메일에 대해 당연히 회의적이다.\n\n그래서 “조치 권장”이라는 자격요건이 담긴 이메일을 개인 인박스로 받았을 때 나는 내 조직의 보안팀이 초과 근무 중이라고 생각했다. 블록 버튼 위에 손가락을 두며 이메시지를 훑어보니 보통의 \"여기를 클릭하세요\" 함정이 있는지 확인했습니다.\n\n<div class=\"content-ad\"></div>\n\n다행히도, 이것은 Google Cloud로부터 온 메시지로, BigQuery에 중요하면서도 섬세한 변경 사항에 관한 내용이었습니다.\n\n![이미지](/assets/img/2024-06-19-IfYouMissedThisBigQueryEmailYouCouldLoseYourSQLQueries_0.png)\n\n구체적으로, 저장된 쿼리의 저장, 액세스 및 유지 관리 프로세스에 대한 변경 사항이 자세히 설명되어 있었는데, 저는 이 프로세스를 자주 실행하기 때문에 주목하게 되었습니다.\n\n따라서 이는 BigQuery 쿼리가 갑자기 양자 컴퓨트 리소스상에서 실행된다는 뉴스와 같이 혁신적인 발표는 아니지만, 나와 같이 BigQuery 쿼리를 만들고는 잊어버리는 경향이 있는 전문가들에게 영향을 미칠 것으로 생각되는 바입니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-IfYouMissedThisBigQueryEmailYouCouldLoseYourSQLQueries_1.png\" />\n\n2024년은 Google의 데이터 관련 제품에게 중요한 해가 될 것으로 예상됩니다. 출판사들과 콘텐츠 제작자들을 위한 기타 Google 제품 소식은 Google의 유니버설 애널리틱스(UA)의 폐기와 Google 애널리틱스 4 (GA4)로의 이주가 그림자를 드리웠습니다. 더 자세히 살펴보면, Google Cloud 기능은 조용히 Python 버전 3.8 미만의 지원을 폐지했고, 이로 인해 제가 소유한 비즈니스 지원 영역의 거의 50개 클라우드 기능을 이주하는 데 몇 시간과 많은 PR을 투자하게 되었습니다. 동시에 BigQuery는 오직 SQL 환경에서 제공되던 것에서 \"데이터를 AI로 가속화하는 협업 분석 공간\"인 BigQuery Studio로 이동하고 있습니다.\n\n<img src=\"/assets/img/2024-06-19-IfYouMissedThisBigQueryEmailYouCouldLoseYourSQLQueries_2.png\" />\n\n이 변경의 일환으로, 현재 고객들을 위해(저와 같은 경우 — 저는 엔터프라이즈 계정을 사용하며 개인 이메일과 연결된 계정을 유지합니다) 마이그레이션 날짜에 GCP가 이 새로운 \"협업 분석 공간\"을 지원하는 6개의 새로운 API를 자동으로 활성화할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n- Dataform\n- Dataplex\n- 빅쿼리 데이터 정책\n- 빅쿼리 연결\n- 분석 허브\n- 예약\n\nAPI 변경은 확실하게 흥미로운 일이지만, 여러분(또는 어떤 빅쿼리 사용자든)께서 이 이메일을 받게 된 이유는 중요한 리소스에 대한 액세스 손실 가능성이 있기 때문입니다. 특히 저장된 쿼리들에 대한 손실이요. 이 측면에서 긍정적인 소식은 상기 API가 2024년 3월 초에 자동으로 활성화될 것이지만, 사용자들은 2025년 3월까지 완전히 새로운 빅쿼리 스튜디오 설정으로 저장된 쿼리를 이전할 수 있다는 점입니다.\n\n\"내가 쓴 쿼리 초안들을 저장하기 위해서 이 정도까지 해야 한다니, 참 귀찮다\"고 생각 중이라면, 아마 맞는 말일지도 모릅니다. 그러나 데이터 엔지니어로서 배운 가장 중요한 교훈 중 하나는 때로는 가장 거친 코드 조각조차도 때로는 향후 중요한 스크립트에 통합되거나 기초로 활용될 수도 있다는 것입니다.\n\n저는 개인적으로 몇 가지 이유로 쿼리를 저장합니다.\n\n<div class=\"content-ad\"></div>\n\n만약 제가 큰 쿼리나 뷰에 열을 추가하는 것과 같은 빠른 업데이트를 작업 중이라면,\n\n또한, 스키마를 프로그래밍적으로 생성하는 것과 같은 작업을 자주 반복한다면 코드를 저장하곤 해요.\n\n```js\nSELECT column_name AS name,\nCASE WHEN data_type = \"FLOAT64\" THEN \"FLOAT\" \nWHEN data_type = \"INT64\" THEN \"INTEGER\"\nWHEN data_type = \"BOOLEAN\" THEN \"BOOL\"\nELSE END AS type,\n\"NULLABLE\" AS mode\nFROM `project.dataset.INFORMATION_SCHEMA`.COLUMNS\nWHERE table_name = \"target_table\"\n```\n\n제가 데이터를 검증하는 QA를 수행할 때 쿼리를 저장하는 것도 자주 하죠. 위 예시처럼, 데이터 유효성을 검증하기 위해 사용하는 몇 가지 프로세스가 있는데, 제 분석 팀원들이 자신들만의 QA를 수행하기 전에도 이미 사용해요. 여러 테이블에서 특정 필드를 비교할 때, 수익 지표와 같은, 이러한 열, CTE 및 JOIN을 가능한 한 한 번만 작성하고 싶어해요.\n\n<div class=\"content-ad\"></div>\n\n저는 수익과 같은 특정 민감한 데이터에 액세스할 수 있지만, BigQuery 액세스를 가진 모든 사람이 그러한 정보를 볼 수 있는 것은 아닙니다. 여기서 접근 제어와 권한이 매우 중요한 역할을 합니다.\n\n따라서 BigQuery Studio 이관의 일환으로 BigQuery 사용자는 새로운 쿼리를 만들고 저장하기 위해 새 권한이 필요합니다. 구체적으로 다음과 같습니다:\n\n![이미지](/assets/img/2024-06-19-IfYouMissedThisBigQueryEmailYouCouldLoseYourSQLQueries_3.png)\n\n저와 같은 BigQuery 관리자들은 더 넓은 범위에서 프로젝트 수준의 자원을 관리할 수 있도록 데이터폼 관리자 역할이 부여될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n모든 제품 이관처럼, 플랫폼과 최종 사용자는 원활한 전환과 자산 보존을 위해 특정 역할을 수행해야 합니다.\n\n### GCP\n\n- 자동으로 6개의 새로운 API를 활성화합니다.\n- 자동으로 BigQuery 사용자 및 BigQuery 관리자 계정을 필수 Dataform 권한으로 전환합니다.\n\n### 사용자\n\n<div class=\"content-ad\"></div>\n\n- 자동 API 활성화에서 선택하여 탈락하기 (2024년 3월 3일까지)\n- 계정 권한을 기존 액세스 제어 정책과 일치하도록 조정하기\n- 새 API에 의존하는 스크립트 및 액세스 범위가 있는 모든 스크립트 업데이트하기\n- 새로운 쿼리를 저장하기 – 개별적으로 또는 24년 여름에 예정된 새 변환 도구를 사용하여\n\n이 발표는 사용자들에게 충분한 시간을 제공한다고 인정하며(API 자동 활성화에 대해 거의 한 달, 새로운 저장된 쿼리 이전에는 1년), 제품 사용자의 중요한 책임을 강조합니다: 새로운 발전 사항을 항상 인식하고 폐기 대비 계획을 항상 갖도록하는 것입니다.\n\n어떤 규모의 프로젝트던지 시작을 할수록 빨리 하는 것이 더 좋습니다. 이 방법은 때때로 스트레스를 유발하는 이주 과정을 완화시키고 제품의 폐기가 발생한 직후의 예기치 못한 문제 발생 가능성을 방지할 수 있습니다.\n\n나는 데이터를 잃는 것보다 더 나쁜 것은 접근 권한을 잃는 것이기 때문에 내 개인 인스턴스에 저장된 쿼리를 매우 가까운 시일 내에 이전할 계획입니다.\n\n<div class=\"content-ad\"></div>\n\n도와드릴게요. 이 블로그 외에 어떻게 도와드릴 수 있는지 알려주실 겸 3가지 질문 설문에 응답해주세요. 모든 응답자에게 무료 선물을 제공해드립니다.","ogImage":{"url":"/assets/img/2024-06-19-IfYouMissedThisBigQueryEmailYouCouldLoseYourSQLQueries_0.png"},"coverImage":"/assets/img/2024-06-19-IfYouMissedThisBigQueryEmailYouCouldLoseYourSQLQueries_0.png","tag":["Tech"],"readingTime":5},{"title":"데이터캠프 세일 2024 - IT 전문가를 위한 50 할인 혜택","description":"","date":"2024-06-19 05:22","slug":"2024-06-19-DatacampSale202450DiscountOfferforITProfessionals","content":"\n\n## 데이터 캠프 연간 요금제로 데이터와 AI 역량 강화를 무제한으로 이용하여 50% 할인 혜택을 받으세요\n\n![Datacamp Sale](/assets/img/2024-06-19-DatacampSale202450DiscountOfferforITProfessionals_0.png)\n\n안녕하세요 여러분,\n\n여러분과 기쁜 소식을 하나 전하고 싶어요. 현재 데이터 캠프에서 큰 세일을 진행 중이며 연간 요금제에 최대 50% 할인 혜택을 제공하고 있어요. 데이터 캠프에 항상 참여하고 싶었다면, 지금이 참여하기에 가장 좋은 시기입니다. 지금 가격의 절반을 절약할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n# 데이터캠프 가입해야 하는 이유\n\n만약 데이터캠프에 가입해야 하는 이유 혹은 데이터캠프가 가치가 있는지 궁금하다면, 데이터 스킬을 배울 수 있는 최고의 웹사이트 중 하나라는 것을 알려드릴게요. 아래는 2024년에 데이터캠프에 가입해야 하는 몇 가지 이유입니다.\n\nAI 기술이 적용된 무제한 학습:\n\n- 파이썬, ChatGPT, SQL, R, Power BI 등 490개 이상의 코스에서 50% 할인 혜택을 받을 수 있습니다.\n- 포브스지 선정 1위 자격증 프로그램에 접속하여 여러분의 스킬을 인증하고 경력을 향상시킬 수 있습니다.  \n\n<div class=\"content-ad\"></div>\n\n실전 학습 경험:\n\n- 지금부터 경험이 없어도 오늘부터 학습을 시작하세요! 상호 작용형 브라우저 코스는 내장 AI 도움말로 안내해줍니다.\n- 강력한 포트폴리오를 구축하는 데 도움이 되는 실제 프로젝트로 지식을 정립하세요.\n\n맞춤 학습 경로:\n\n- 평가를 통해 맞춤 학습 경로를 받거나 처음부터 직업 궤적으로 뛰어들 수 있습니다.\n- 산업 리더가 설계한 코스로 자신의 속도에 맞게 학습하세요.\n\n<div class=\"content-ad\"></div>\n\n경력 전망을 넓히는 인증서:\n\n- 세계적인 기업들이 인정하는 인증서를 획득하여 기술을 검증하고 이력서를 향상시킵니다.\n- 데이터 및 AI 분야에서 최신 기술과 도구를 선도하는 위치에 머무르세요.\n\n그리고, Datacamp에서 참여할 수 있는 최고의 데이터 인증서들을 소개합니다:\n\n- 데이터 분석가 인증서\n- 데이터 과학자 인증서\n- 데이터 엔지니어 인증서\n- SQL 어소시에이트 인증서\n\n<div class=\"content-ad\"></div>\n\n# 특별 혜택: 50% 할인!\n\n6월 17일 (EST 오전 11:59)부터 7월 2일 (EST 오후 23:59)까지 모든 강좌 및 기능 무제한 액세스를 50% 할인된 가격으로 이용하세요.\n\n수천 명의 학습자와 함께 Datacamp를 통해 커리어를 변화시켜보세요. 미래를 위한 이 한정 기간 특별 혜택을 놓치지 마세요!\n\n지금 학습 여정을 시작해보세요!\n\n<div class=\"content-ad\"></div>\n\n아래는 Datacamp에 50% 할인 혜택을 받을 수 있는 링크입니다.\n\n![Datacamp 50% Discount Offer](/assets/img/2024-06-19-DatacampSale202450DiscountOfferforITProfessionals_1.png)\n\n다른 Datacamp 관련 기사들도 즐겨보실 수 있어요\n\n- 데이터 엔지니어링 직무에 인공 지능 학습이 왜 중요한지\n- DataLab 개요 — 데이터 과학자를 위한 새로운 도구\n- 인공 지능, ChatGPT 및 머신 러닝 배우기 위한 8가지 Datacamp 코스\n- 데이터 엔지니어를 위한 최고의 3가지 Datacamp 자격증\n- 파이썬 및 데이터 스킬 학습을 위한 무료 Datacamp 코스 10가지\n- DataCamp vs CodeCademy, 어느 쪽이 더 나은가요?\n\n<div class=\"content-ad\"></div>\n\n여기까지 이 기사를 읽어주셔서 감사합니다. 만약 이 정보가 유용하시다면 친구나 동료와 공유해주세요. 이것은 모든 데이터 분석가, 데이터 과학자 및 데이터 전문가에게 유용한 훌륭한 도구입니다.\n\n추신 - 데이터 분석가 자격증을 따로 배우는 것뿐만 아니라 이를 이력서에 활용할 수 있는 인정도 받고 싶다면 Datacamp의 데이터 분석가 자격증도 확인해보세요. 사실, 회원이라면 단일 멤버십으로 이 자격증과 DataLab에 모두 접근할 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-DatacampSale202450DiscountOfferforITProfessionals_0.png"},"coverImage":"/assets/img/2024-06-19-DatacampSale202450DiscountOfferforITProfessionals_0.png","tag":["Tech"],"readingTime":3},{"title":"Forecasting  Meta 예술과 과학의 균형 유지하기","description":"","date":"2024-06-19 05:19","slug":"2024-06-19-ForecastingMetaBalancingArtandScience","content":"\n\n![image](/assets/img/2024-06-19-ForecastingMetaBalancingArtandScience_0.png) \n\n스티븐 진과 케빈 번바움\n\n예측은 종종 과학과 예술 둘 다로 송두리째 말합니다. 한편으로는 기술적 모델링과 최적화가 필요합니다. 현대 알고리즘은 복잡성을 가진 패턴을 찾아내고 인간의 편견을 꿰뚫습니다. 다른 한편으로는 예측에는 모델에 쉽게 인코딩되지 않는 깊은 도메인 지식과 전문성이 필요할 수도 있습니다. 이는 때로 알 수 없는 부분이 많은 어두운 공간에서 의지할 수 있는 유일한 것일 수도 있습니다.\n\n예측가로서, 우리는 종종 이 두 가지 사고 방식이 대립하는 것을 발견합니다. 어떤 사람들은 블랙 박스 알고리즘이 상식을 버리는 것으로 생각하지만, 다른 사람들은 수학적 결과를 무시하는 것은 편견에 굴복하는 것이라고 생각합니다. 우리는 일관된 높은 품질의 예측을 생성하는 열쇠가 이 이분법의 교차점에 있다고 믿습니다. 이 기사에서, 저희는 Meta에서 예측의 예술과 과학을 균형있게 다루는 방법에 대해 논의할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 철학을 통해 다음을 채널하는 방법을 살펴볼 거에요:\n\nA. 예측의 타당성 확인\n\nB. 제품 영향의 통합\n\n## 확인\n\n<div class=\"content-ad\"></div>\n\n검증은 예측의 품질을 평가하는 것입니다. 종종 우리는 현실이 우리의 예측에 따라 잡히는 순간에 오류를 측정하는 것 이상의 용량에서 이를 수행해야 합니다. 우리는 종종 현실이 펼쳐지기를 기다리지 않고도 예측의 품질을 평가해야 합니다. 게다가, 현실이 펼쳐지더라도 예측의 정확성을 평가할 샘플의 양은 제한됩니다. 현실은 한 가지 방향으로만 펼쳐지며, 그 안의 데이터 포인트는 상관 관계가 있습니다.\n\n예를 들어, 페이스북 사용자 예측 (DAU 또는 일일 활성 사용자)을 다루고 있다고 상상해 봅시다. 우리는 내년 DAU를 예측하는 예측을 생성하려고 합니다. 당연히 아직 내년의 예측을 비교할 데이터는 없습니다. 우리는 좋은 예측을 가졌는지 여부를 검증하기 위해 다양한 방법을 활용해야 합니다. 이 결정을 내리는 것은 측정 항목 최적화와 인간 판단의 혼합입니다. 이 섹션에서는 이 프로세스의 기계적인 측면에 대해 논의하겠습니다.\n\n오류 메트릭 및 백테스팅\n\n방금 언급한 대로, 품질을 평가할 수 있는 가장 명백한 방법은 현실과 예측 사이의 차이를 측정하는 것입니다. 과거 데이터와 비교하기 위해 백테스팅을 활용할 수도 있습니다. 이는 예측 방법이 합리적인지 확인하는 데 유용합니다.\n\n<div class=\"content-ad\"></div>\n\n이러한 두 가지 사용 사례를 고려할 때, 이를 시행하는 데 많은 주의 사항이 있습니다:\n\n- 에러 메트릭 선택: 어떤 에러 메트릭을 사용하는 것이 가장 좋을까요?\n- 대표적인 테스트 식별: 공정한 백테스팅 기간은 어떤 기간이며, 이는 어떻게 우리의 매개 변수/모델 선택에 영향을 미쳐야 할까요?\n- 전문가 의견 및 유효성 검증 확립: 단순히 에러율을 측정함으로써 고려되지 않는 이 도메인에 대해 우리가 알고 있는 다른 사항이 있을까요? 예를 들어: 미래에 월드컵과 같은 구체적인 스포츠 이벤트가 발생한다는 것을 사전에 알 수 있고, 해당 이벤트에 대한 우리 데이터의 유사한 이력이 없을 수 있습니다. 우리 회사가 새 제품을 출시할 예정이라는 것과 그것이 미래에 미칠 영향에 대해 판단할 수 있습니다. 이러한 것들은 백테스팅 히스토리에서 잘 나타나지 않습니다.\n\n에러 메트릭 선택\n어떤 에러 메트릭을 사용해야 할까요? 그 에러 비율을 어떤 집계 단위로 측정할 것인가요? 에러 비율이 예측 목표의 공정한 평가인가요?\n\n시계열 모델링에서 선택할 수 있는 다양한 에러 메트릭이 있습니다. 각각 장단점이 있습니다. 여기 몇 가지가 있습니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-ForecastingMetaBalancingArtandScience_1.png\" />\n\n대표 테스트 식별\n\n예측의 대표적인 테스트를 만드는 것은 종종 많은 인간 판단을 필요로 합니다. 일부 잠재적인 함정은 다음과 같습니다:\n\n- 변동이 심한 또는 대표성이 없는 기간을 선택하는 것\n\n<div class=\"content-ad\"></div>\n\n이러한 표는 결과를 왜곡하고 부정확한 예측을 만들 수 있습니다. Meta에서는 2020년과 전염병 확산이 한 가지 독특한 시기였다는 것을 알아서, 이 기간을 일반적으로 우리의 백테스팅 세트와 함께 훈련 데이터에서도 제외합니다.\n\n- 예측 목표와 일치하지 않는 오차율 선택\n\n백분율 오차를 측정하는 오차율은 어떤 경우에는 훌륭할 수 있지만, 더 큰 규모의 시계열 정확도가 중요한 경우 부적합한 결과를 낼 수 있습니다. 예를 들어, 제품의 매출을 예측할 때 가장 높은 매출을 올린 제품의 예측 정확도가 가장 낮은 매출 제품의 것보다 훨씬 더 중요할 수 있습니다. 이러한 경우에는 절대적인 값을 다루는 오차율이 더 유용할 수 있습니다.\n\n- 적절한 집계로 테스트하기\n\n<div class=\"content-ad\"></div>\n\n또 다른 중요한 요소는 오차를 측정하는 집계 방식입니다. 예를 들어, 우리는 인도의 사용자 수를 예측할 수 있습니다. 이 예측은 그나라보다 훨씬 작은 규모의 국가와 동일한 가중치로 취급될 수 있습니다. 지리적으로 유사한 국가들을 함께 묶어 더 균형있는 오차 지표를 만들 수 있는 가능한 대응책 중 하나입니다. 다른 가능한 대응책은 전체 사용자수 예측의 오차율과 국가수준에서의 오차율을 측정하는 것일 수 있습니다.\n\n최종적으로, 이 결정은 예측 선택을 사용 사례에 매핑하는 데 인간 판단을 활용하는 것이 정말 좋은 예측가들이 순전히 수학을 넘어서 섬세한 선택을 통합하는 훌륭한 예가 됩니다. 이러한 선택사항은 모두 이러한 예측에 기반한 실제 사용 사례와 결정으로 다시 매핑되어야 합니다.\n\n전문가 의견과 유효성 검증\n\n도메인 전문가들과 예측 이해관계자들의 의견을 수집하는 것도 매우 중요합니다. 최종적으로 당신의 예측은 이해 관계자들에 의해 사용되고 검토되므로, 그들의 관심사와 사용 사례를 이해하여 가장 중요한 예측 부분에 최적화하는 데 도움이 될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n보통 도메인 전문가/이해 관계자로부터 3가지를 찾고 있습니다:\n\n- 사용 사례 이해하기\n이해 관계자들은 예보를 사용, 해석하고 다르게 활용합니다. 예보가 잠재적 이상 현상을 모니터링하는 데 사용된다면, 세부 정밀도가 최적화되어야 할 것입니다. 이해 관계자들이 장기 계획을 수립하는 경우, 예보는 추세의 신뢰성에 더욱 신경을 쓸 것입니다. 이러한 인간적인 요소가 구상적 방법론 선택을 결정합니다.\n- 올바른 요소/가정이 있는가?\n예보에서 고려해야 할 요소들(출시, 이벤트, 장애 등)은 우리가 하는 중요한 선택입니다. 우리는 모든 것을 고려할 수 없기 때문에 포함해야 할 핵심 요소를 선택하는 것은 보통 도메인 전문가로부터 옵니다. 이러한 정보의 일부는 예보에 중요할 수 있지만 역사적 데이터 자체에만 포함되지 않을 수 있습니다.\n- 예보가 합리적한가요?\n일반적으로, 예보의 합리성은 예보에 대한 합리성에 대한 의견이나 질문으로 나타납니다. 예를 들어, 예보가 성장의 이전 해와 유사한지 여부 등이 있습니다.\n\n이러한 유형의 질문은 인간 판단을 계산된 프로세스로 전환하는 데 도움이 되는 자동화를 통해 유효성 검사 프로세스에 포함될 수 있습니다.\n\nFacebook DAU의 예를 다시 살펴보면, 이러한 고려 사항이 어떻게 결정에 영향을 미치는지 생각해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n- 예측 모델을 일정량 초과 또는 미달하여 동일하게 처벌하는 오류 측정 항목이 필요합니다. 따라서 SMAPE(위 참조)는 합리적인 옵션입니다.\n- 게다가 2020년에는 팬데믹으로 인해 사람들이 인터넷에 더 많은 시간을 보냈다는 점을 알고 있습니다. 따라서 이는 미래를 대표하는 것이 아니기 때문에 합리적인 테스트 기간이 아닙니다.\n- 또한 최근 2년간 인터넷 인프라의 급격한 성장이 특정 지역의 성장의 주된 원동력이었음을 알 수 있습니다. 인프라 성장이 새로운 법률이나 경제적 환경으로 인해 둔화될 것이라는 사실을 알면 그 지역이 이전에 높았던 속도로 성장할 가능성이 낮을 것이라는 직관을 형성할 수 있습니다.\n\n# 제품 영향\n\n날씨를 예측하는 것과 달리, Meta에서는 예측하는 지표에 영향을 미칠 수 있는 능력이 있습니다. 예를 들어, 런칭할 기능에 대한 결정이 해당 지표 결과에 큰 영향을 미칠 수 있다는 사실을 알고 있습니다. 때로는 특정 런칭이 지표에 미칠 영향을 잘 알고 있을 수도 있습니다. 다른 경우에는 영향의 크기, 형태, 지속 기간 등에 대해 많은 불확실성을 가질 수 있습니다. 예를 들면, 이것이 시리즈의 궤적에 영구적인 변화를 일으킬 수도 있고, 대신 전체 시리즈의 수준 변화를 일으킬 수도 있습니다. 우리는 영향을 신중하게 평가하고 그것을 어떻게 통합할지 계획해야 합니다.\n\n포함 기준\n제품 영향과 같은 새로운 요소를 예측 모형에 포함할 때에는 체계적인 포함 기준이 필요합니다. 추가 요인을 고려함으로써 도입할 가능성이 있는 노이즈/불확실성과 균형을 맞추는 것 사이에는 본질적인 상충 관계가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n명확한 기준이 있으면 예측을 어떻게 진행할지에 대한 체계적인 논리를 얻을 수 있을 뿐 아니라 해당 예측이 어떤 가정들에 기반을 두고 구축되었는지에 대한 투명성도 확보할 수 있습니다. 예를 들어, Meta는 각 예측에 어떤 영향을 포함해야 하는지 엄격한 기준이 있습니다. 추정치의 불확실성이 높은 경우 일반적으로 예측에서 제외됩니다. 이러한 추정치는 다른 잘 알려진 이벤트의 정밀도를 떨어뜨려 예측의 유용성을 감소시킵니다. 반면, 매우 정확한 추정치(크기, 시기 및 모양)는 직접 반영됩니다.\n\n홀드아웃 통합\n\n실험 홀드아웃이 있는 경우 제품 기능의 영향에 대한 확신이 상당히 높습니다. 이러한 경우에는 모델이 예측하는 내용 외에 제품 영향을 예측에 직접 반영할 것입니다.\n\n모델이 예측한 추세와 제품 영향을 어떻게 예측할지에 대한 상당한 복잡성이 있습니다. 이러한 경우에는 측정에 대한 우리의 확신을 평가하여 영향을 중복 계산하지 않도록 해야 합니다. 어떤 경우에는 이전 제품 영향을 기존 훈련 데이터에서 제거하고, 다른 경우에는 대강 증분을 측정하여 모델의 신뢰도를 해치지 않도록 이를 추가할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n가상의 제품 기능을 출시하고자 하는 경우를 생각해 봅시다. 이 기능은 생일에 로그인할 때 작은 축하 애니메이션을 표시합니다. 현재, 일부 국가에만이 기능이 활성화되어 있습니다. 이러한 국가들 내에서는 일부 사용자가 홀드아웃 상태에 있습니다. 즉, 일부 사용자는 이 기능을 볼 수 없습니다. 이 두 그룹 간의 차이를 활용하여 이 기능이 어떤 영향을 미치는지 측정할 수 있습니다.\n\n우리는 이 정보를 사용하여 이 기능이 모든 국가에 출시될 때 어떤 영향을 미칠지 예상하고 있습니다. 홀드아웃이 있더라도, 이러한 추정치는 여전히 매우 큰 신뢰 구간을 갖게 될 수 있습니다. 이 모든 불확실성은 우리의 예측으로 향하게 됩니다. 이 기능이 영향을 미치는 복잡성은 예측의 복잡성으로 전이됩니다. 어떤 국가들은 이 기능에 다르게 반응할 수도 있습니다. 신선미의 영향이 있을 수 있고 시간이 지남에 따라 이 기능의 영향이 줄어들 수도 있습니다.\n\n게다가, 추가적인 성장을 어떻게 포함시킬지와 관계없이, 그 중 일부는 예측 알고리즘에 자연스럽게 포착될 것입니다. 이 기능의 영향을 제거한 데이터로 모델을 학습해야 할까요? 제품, 제품 영향 측정 방법 및 미래에 어떻게 전이될 것으로 기대되는 부분에 기반하여 내리는 복잡한 결정들입니다. 때로는 불확실성이 지불해야 하는 비용이 너무 높아서, 그러므로 예측에서 많은 것들을 제외해야 할 수도 있습니다.\n\n신뢰도가 높은 경우\n\n<div class=\"content-ad\"></div>\n\n당신이 예상 제품의 영향에 대해 확신을 가지고 있다면(크기, 시기, 형태 등을 포함하여) 해당 예상 제품을 예측에 통합하는 여러 가지 방법이 있습니다.\n\n일반적으로 두 가지 방법을 사용합니다:\n\n- 최선 책상: 때로는 더 간단한 방법을 선호하기도 합니다. 최선 책상 조정은 제품의 역사적인 기여와 예상치와의 차이를 비교합니다. 이 차이는 모델이 생성하는 어떤 결과에 추가됩니다. 이런 경우 간단함은 지표의 추세에 미치는 영향에 중점을 둔 견고한 가정을 만들어냅니다. 예를 들어, 내년에 기능을 출시할 계획이 있어서 우리 지표에 15%의 영향을 미칠 것으로 계획했지만, 일반적으로는 5%의 영향밖에 없는 경우가 있습니다. 이 추가 10%는 모델이 예측한 결과에 더해집니다.\n- 역사 수정: 제품의 영향이 지표의 역사에 심각한 혼란을 일으켜 온 경우, 우리는 미래 추세에 어떻게 작용할 지 이상을 가져보는 것 이상으로 걱정이 되기도 합니다. 모델이 학습한 계절 패턴이 제품의 변화로 인해 악화될 것을 우려하는 추가적인 문제점이 있을 수 있습니다. 교육 데이터가 손상될 수 있는 경우, 제품의 영향을 트렌드의 역사에서 제거하고 \"자연스럽게\" 예측하는 것을 고려할 수 있습니다. 이는 분해 요소에 충분한 데이터가 있는 경우에만 가능합니다. 어떤 방법을 사용할지 결정하는 것은 과거 제품의 영향의 변동성에 달려 있습니다. 우리의 지표 중에는 제품의 영향이 크게 영향을 끼치는 것도 있고 매우 안정적인 것도 있습니다. 또한, 과거 제품 영향을 추정하는 것이 항상 완벽하지 않을 수 있다는 점을 알아야 합니다. 정확한 평가를 얻는 데 어려움을 겪는 문제점 중에는 네트워크 효과의 부정확한 측정이나 특정 예측 지역의 실험에 대한 신뢰 구간이 너무 커져버리는 경우가 있습니다.\n\n상기 내용을 감안하여, 우리의 시계열 데이터에서 과거 제품의 영향을 제거해야 하는지 여부를 결정하기 위해 간단한 테스트를 실시합니다. 과거의 영향을 제거하고 남은 시계열의 신뢰도를 측정해보는 테스트입니다(추세/계절성 제거가 필요할 수 있으며, 예측 방법 및 시계열에 따라 차이가 있을 수 있습니다).\n\n<div class=\"content-ad\"></div>\n\n- 만일 우리의 시계열 데이터의 분산이 크게 감소하지 않으면, 첫 번째 방법을 선택하여 제품 영향이 원래 시계열 데이터에 내재적으로 포함되어 있다고 가정하고 점증 제품 영향을 최상위에 추가할 것입니다. (예: 알림이 하루 평균 사용자수(DAU)를 +10만 명 증가시키지만, 이번에는 사용자수를 +12만 명 증가시키기를 기대할 때, 추가 +2만 명이 최상위에 추가됨).\n- 우리의 시계열 데이터의 분산이 크게 감소하는 것을 확인하면 영향을 제거하고 조정된 시계열 데이터를 사용하여 예측하겠습니다.\n\n낮은 확신 사례\n\n가끔 낮은 확신 사건을 고려해야 할 때가 있습니다. 예를 들어:\n\n- 특정 트렌드가 일회성 외부 이벤트에 의해 주도되는지 여부에 대해 확신이 덜할 수 있습니다.\n- 중요한 출시의 타이밍에 대해 확신이 없을 수 있습니다.\n- 특정 이벤트가 예정되어 있지만 영향의 규모에 대해 불확실할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이러한 경우들은 특히 까다로운데, 이러한 요소들을 포함하려고 시도하면 나머지 예측의 유용성이 위험에 처할 수 있습니다. 우리가 평가하는 사건이 우리의 포함 기준에서 포착된 것인지를 기대합니다. 그러나 이와 같은 사건들은 종종 독특하며 독특하게 평가되어야 합니다.\n\n우리가 고려하는 주요 사항은 예측의 사용 사례입니다. 예를 들어, 예측이 연말 가치만을 위해 사용된다면, 알 수 없는 출시 시기를 포함해도 예측의 유용성을 손상시키지 않을 수 있습니다. 그러나 예측이 주간 모니터링에 사용된다면, 트렌드를 왜곡하는 사건을 잘못 예측하면 예측의 유용성이 파괴될 수 있습니다. 이러한 다른 비용/혜택을 따져 예측을 가능한 한 유용하게 만드는 것이 중요합니다.\n\n# 마무리\n\n우수한 예측을 만들기 위해서는 기술적 방법과 신중한 판단의 조화가 필요합니다. 둘 중 한 가지만으로는 미래를 가장 잘 예측할 수 없습니다. 수학이 맹점을 가지고 있고 인간의 개입이 필요하며, 기술적 방법이 인간의 편향을 극복할 수 있는 곳을 인식하는 것이 중요합니다. 이러한 예제들이 여러분의 예측을 더 잘 생성하도록 도와줄 것을 희망합니다.","ogImage":{"url":"/assets/img/2024-06-19-ForecastingMetaBalancingArtandScience_0.png"},"coverImage":"/assets/img/2024-06-19-ForecastingMetaBalancingArtandScience_0.png","tag":["Tech"],"readingTime":9},{"title":"데이터 빌드 도구 dbt의 힘","description":"","date":"2024-06-19 05:17","slug":"2024-06-19-ThePowerofDataBuildTooldbt","content":"\n\n## 데이터 분석 엔지니어를 위한 안내서\n\n데이터 분석은 끊임없이 발전하는 프로세스이며, 효율적인 워크플로우를 개선하고 결과물을 제공하는 데 도움이 되는 여러 도구들이 있습니다. 저는 2018년 초부터 데이터 빌드 도구 (dbt)를 활용하여 다양한 데이터 웨어하우스에 빠르게 데이터 모델을 작성해 왔습니다. 이번 몇 년 동안 dbt-core와 dbt-cloud를 모두 활용하며, 그들의 비즈니스 가치를 직접 확인해 왔습니다. 또한 제 배우자는 dbt 인증 개발자이자 자신이 소속된 컨설팅 회사에서 정기적으로 dbt 교육을 담당하고 있습니다.\n\n![데이터 빌드 도구 (dbt)](/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_0.png)\n\n본 문서에서는 dbt의 특징 중 주목할 만한 몇 가지를 살펴보고, DuckDB를 활용한 기술적인 솔루션을 스크린샷, 코드 조각, 그리고 지원하는 GitHub 저장소와 함께 안내해 드리겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# dbt의 특징\n\ndbt는 데이터 변환을 위한 강력한 도구로서 주목받고 있으며, 데이터 모델의 개발과 관리를 간편화하는 다양한 기능을 제공합니다. dbt는 ETL 도구가 아닌 것에 유의해야 합니다. 그것은 변환 단계에 특히 초점을 맞추고 있습니다. 그러나 데이터 무결성을 테스트하여 확실하게 유지하거나 포괄적인 문서를 생성하는 것과 같이, dbt는 분석 워크플로우를 향상시키는 데 필요한 능력을 제공합니다. 아래에서는 dbt를 Analytics 엔지니어에게 필수적인 도구로 만드는 주요 기능을 살펴보겠습니다.\n\n## 데이터 모델의 무결성을 위한 모델 테스팅\n\ndbt의 가장 강력한 기능 중 하나는 데이터 모델의 무결성을 테스트하는 능력입니다. 테스트를 데이터 변환 프로세스에 직접 통합함으로써, dbt는 데이터 품질 문제를 조기에 식별하고 해결할 수 있도록 보장합니다. 이 기능은 분석 신뢰성을 유지하는 데 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n다음은 ‘상자에서 나오는’ 몇 가지 테스트가 있습니다:\n\n- unique는 열의 모든 값이 고유한지 여부를 확인합니다.\n- not_null은 열에 NULL 값이 있는지 여부를 확인합니다.\n- accepted_values는 값이 정의된 목록에서 벗어나는지 확인합니다(열거형에 적합).\n- relationships는 열의 값이 다른 모델의 열에 있는지 확인합니다(외래 키에 적합).\n\n하지만 원하는 테스트를 정의할 수 있습니다. 테스트를 정의한 후에는 dbt test를 실행하고 모든 것이 통과되었는지 확인하는 것이 매우 쉽습니다:\n\n![이미지](/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_1.png)\n\n<div class=\"content-ad\"></div>\n\n또한 v1.8부터 dbt는 단위 테스트를 소개했습니다. 이는 파이썬에서 오랫동안 사용 가능했던 unittest 패키지와는 달리 (2022년 2월에 릴리스된 버전 1.0.0부터 사용 가능했음), SQL 단위 테스트할 수 있는 능력은 매우 새로운 것입니다. 매우 간단한 예제 코드에서는 단위 테스트의 범위가 제한적이었지만, 단위 테스트에 대해 더 자세히 알아보려면 여기를 참고하십시오.\n\n## 문서 웹사이트 구축\n\n또 다른 주목할만한 dbt의 기능은 데이터 모델에 대한 포괄적인 문서를 자동으로 생성할 수 있는 능력입니다. 이 문서는 프로젝트의 yaml 파일에서 작성되며, 모델 및 레코드 레벨의 설명 및 메타데이터로 보강될 수 있어 데이터 파이프라인을 명확하게 이해할 수 있게 해줍니다.\n\n문서는 명령줄에서 dbt docs generate && dbt docs serve를 입력하여 생성할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_2.png\" />\n\n웹 브라우저에서 문서가 열릴 겁니다. 아래에는 dim_employee 테이블에 대한 문서가 나와 있어요. 이 문서에는 테이블에 대한 설명, 데이터 유형을 가진 필드들, 그들의 설명, 그리고 그들에 대해 정의된 테스트들이 포함돼 있어요.\n\n<img src=\"/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_3.png\" />\n\n## 데이터 계통\n\n<div class=\"content-ad\"></div>\n\n문서 사이트 내에서 데이터 연형을 보여주는 링크가 오른쪽 하단에 있습니다. 이것은 상류 및 하류 모델, 소스 및 노출물을 모두 보여주어 매우 가치 있습니다:\n\n![show data lineage](/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_4.png)\n\n여기서 우리는 선택된 모델인 dim_employee를 보라면, 두 개의 모델을 참조하는 것을 확인할 수 있습니다 (파란색으로 식별된 단계). 이 모델들은 또한 두 개의 이전 모델(우리의 스냅샷)과 그 외부 소스(초록색으로 식별)를 참조합니다. 이것이 하나의 하류 모델에서 사용되고 있으며, 비즈니스에 노출되어 대시보드에서 사용되고 있음을 확인할 수 있습니다 (주황색으로 식별). \n\n## 데이터 계약\n\n<div class=\"content-ad\"></div>\n\n스키마.yml 내의 모델 정의 일부로 각 필드의 데이터 유형을 정의할 수 있습니다. 데이터 유형이 여기서 업데이트되지 않고 변경되면 모델이 실패합니다. 이렇게 함으로써 코드 변경 중에도 생성된 테이블 스키마가 일관되게 유지되며 변경이 발생할 때 변경 사항을 알 수 있어서 변경이 프로덕션에 영향을 미치기 전에 하류 사용자에게 알릴 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_5.png)\n\n## dbt의 모델 유형\n\ndbt에서는 데이터 파이프라인에서 특정 목적을 제공하는 각기 다른 유형의 모델을 생성할 수 있습니다. 이는 다음을 포함합니다:\n\n<div class=\"content-ad\"></div>\n\n- 스냅샷: 특정 시점에서 데이터의 상태를 캡처하여 유효 기간과 유효 종료 날짜를 갖는 Type 2 SCD와 유사한 형태로 저장합니다. 모든 변경 가능한 소스에서 스냅샷을 활용합니다. 지난 주에 데이터가 다르게 보인다는 사람이 나에게 말했을 때 매우 유용하며, 왜 다른지 알고 싶어하는 경우에 큰 도움이 됩니다.\n- 테이블: 데이터베이스에 지속적으로 실제 테이블로 쿼리 결과를 작성합니다.\n- 뷰: 데이터의 동적 뷰를 제공하는 가상 테이블을 생성합니다.\n- 점진적 모델: 마지막 실행 이후에 변경된 데이터만 처리하고 저장하여 성능 및 저장 공간을 최적화합니다.\n\n# 기술적 솔루션: DuckDB를 활용한 dbt 프로젝트 구축\n\n이제 DuckDB를 활용하는 dbt 프로젝트를 구축하는 단계별 가이드로 들어가 봅시다. CSV 파일에서 소스를 추출하고 간단한 팩트 및 디멘젼 모델을 구축하는 간단한 프로젝트를 사용할 것입니다. 두 가지 다른 디멘젼 모델을 만들었는데, 이 중 하나는 두 가지 기본 스냅샷을 활용한 Type 2 Slowly Changing Dimension 모델입니다.\n\n## 단계 1: 환경 설정하기\n\n<div class=\"content-ad\"></div>\n\n시작하려면 dbt 및 DuckDB를 설치해야 합니다. 자세한 지침은 공식 dbt 및 DuckDB 문서에서 찾을 수 있지만, 기본적으로 가상 Python 환경을 만들고 다음을 입력하는 것뿐입니다:\n\n```js\npip install dbt-core dbt-duckdb\n```\n\n## 단계 2: dbt 프로젝트 생성\n\ndbt 프로젝트를 초기화하고 데이터 웨어하우스로 DuckDB를 사용하도록 구성하십시오. dbt에 새 프로젝트 빌드를 시작하도록 지시하여이 작업을 수행합니다:\n\n<div class=\"content-ad\"></div>\n\n```js\ndbt init\n```\n\n프로젝트의 이름을 지정하고 원하는 데이터베이스를 선택하도록 안내받을 것입니다:\n\n![이미지](/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_6.png)\n\n## 단계 3: 소스 및 모델 정의하기\n\n<div class=\"content-ad\"></div>\n\nCSV 파일에서 데이터를 추출하기 위해 dbt에서 소스 정의를 만들어주세요. 그런 다음 이러한 소스가 가변적이라면 스냅샷 폴더 내에 스냅샷을 생성해주세요.\n\n스냅샷이 준비되면, 여기에서 '강력한 규칙'을 구현할 수 있는 스테이징 뷰를 생성하고 마지막으로 모델 폴더 내에서 사실 및 차원 모델을 정의해주세요.\n\n최종 구조는 다음과 같습니다:\n\n![구조](/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_7.png)\n\n<div class=\"content-ad\"></div>\n\nDuckDB를 사용할 때 dbt와 함께 사용하는 것 중 하나가 dbt가 변환 단계에만 집중하는 반면, DuckDB의 기능을 통해 외부 테이블을 참조할 수 있다는 점이 마음에 듭니다. 아래 스크린샷에서는 demo_files 폴더에 있는 네 개의 csv 파일을 가리키고 있습니다. 데이터를 데이터베이스에 로드해야만 참조할 수 있는 번거로움보다 훨씬 쉽습니다. \n\n![스크린샷](/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_8.png)\n\n소스가 생성되면 Jinja를 사용하여 모델에서 이를 참조할 수 있습니다. dbt는 이를 통해 소스, 모델 및 노출 사이의 계보와 관계를 생성하는 방법을 알고 있습니다.\n\n아래 스크린샷에서는 스냅샷 테이블 중 하나를 만든 모습을 볼 수 있습니다. CREATE TABLE을 생성할 필요도 없고 MERGE 문을 만드는 로직에 대해 걱정할 필요도 없습니다. dbt가 대신해줍니다. 단순히 SQL 문을 쓰고 Jinja 블록 내에서 정의된 코드를 사용하면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n![2024-06-19-ThePowerofDataBuildTooldbt_9](/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_9.png)\n\n위에 있는 설정은 스냅숏이 생성될 스키마, 고유 키 및 스냅숏 빌드 전략을 정의합니다.\n\n비슷하게, 다른 모델들도 동일한 방식으로 작성할 수 있습니다. 아래는 두 개의 하위 스냅숏에서 가져온 더 복잡한 Type 2 Slowly Changing Dimension 테이블입니다. 각각에 자체 유효한 시작 및 종료 날짜가 있는 두 하위 스냅숏에서 이상적인 to와 from 날짜를 원본으로 했습니다. 쿼리 끝에 내 차원 테이블의 -1 및 -2 값들을 간단한 UNION으로 추가했습니다 (화면 밖에서). 두 dbt 스냅숏을 결합하여 두 테이블을 통해 시간에 따른 변경 사항을 보여주고 있습니다. 스냅숏과 마찬가지로 타겟 스키마를 설정 블록에 추가하지 않았으므로 기본 스키마를 사용하도록 되어 있습니다:\n\n![2024-06-19-ThePowerofDataBuildTooldbt_10](/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_10.png)\n\n<div class=\"content-ad\"></div>\n\n내 팩트 테이블은 차원들과 조인하여 올바른 매핑을 식별하고 문서화는 관계를 보여주도록 합니다. 이 접근 방식은 분석가가 항상 INNER JOIN을 사용하여 테이블을 조인할 수 있도록 합니다. dim_employee 테이블에 대한 조인은 Type 2 SCD에 연결될 때 날짜 범위를 사용합니다:\n\n![image](/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_11.png)\n\n## 단계 4: 테스트 실행 및 문서 작성\n\ndbt 테스트를 실행하여 모델의 무결성을 확인하세요. 문서 웹사이트를 생성하여 데이터 파이프라인의 명확한 개요를 얻으세요.\n\n<div class=\"content-ad\"></div>\n\n(자세한 내용은 위쪽 섹션을 참조해주세요)\n\n## 단계 5: 결과 검토\n\ndbt run 결과를 검토하여 구축된 모델 및 생성된 문서를 확인하여 모든 것이 예상대로인지 확인하세요.\n\nDuckDB 데이터베이스를 생성하기 위해 이를 실행 중이므로 DuckDB cli로 이동하여 SQL 문을 실행하여 결과를 확인할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_12.png)\n\nDuckDB를 dbt 프로젝트에 사용하는 이유에 대해 궁금해할 수 있지만, 개발 환경에서 실행 중이라는 점을 강조하고 싶습니다. 개발 환경에서 DuckDB를 사용하고 프로덕션 환경에서는 Snowflake를 사용할 수 있습니다. 이렇게 함으로써 Snowflake의 개발 비용을 크게 줄일 수 있습니다. 개발 환경과 프로덕션 환경 간에 SQL 방언이 다른 경우, 하드코딩된 SQL 대신 Jinja 매크로를 활용할 수 있습니다. 이를 통해 현재 환경에 따라 올바른 SQL을 반환할 수 있습니다.\n\n# 결론\n\ndbt는 데이터 모델을 구축하고 관리하는 방식을 혁신적으로 바꿔놨으며, 모델 테스트 및 자동 문서화와 같은 강력한 기능을 제공합니다. 이 안내서를 따라가면 DuckDB를 이용한 dbt 프로젝트를 빠르게 설정하고 이 강력한 도구의 혜택을 누릴 수 있습니다. dbt를 사용해보고 더 많은 기능을 탐색해보기를 권장합니다. 여기서 전체 프로젝트와 코드 스니펫을 찾을 수 있습니다: [링크된 GitHub 저장소](링크).\n","ogImage":{"url":"/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_0.png"},"coverImage":"/assets/img/2024-06-19-ThePowerofDataBuildTooldbt_0.png","tag":["Tech"],"readingTime":7},{"title":"파이썬 코드를 간단하게 만드는 데이터 엔지니어링 프로젝트","description":"","date":"2024-06-19 05:14","slug":"2024-06-19-SimplifyingthePythonCodeforDataEngineeringProjects","content":"\n\n## 데이터 수집, 검증, 처리 및 테스트를 위한 파이썬 팁과 기술: 실용적인 안내\n\n다양한 원천과 형식에서 원시 데이터가 나옵니다. 중요한 비즈니스 문제에 대답할 수 있도록 데이터를 사용할 수 있게 하려면 데이터 엔지니어링을 수행하기 위해 상당한 노력과 시간이 필요합니다. 데이터의 양, 속도 및 분석 요구 사항에 따라 기본 데이터 인프라는 다양할 수 있지만, 여러 작업을 단순화하고 최적화하기 위한 일부 기본 코드 설계 기술은 여전히 중요합니다.\n\n본문에서는 데이터 수집부터 파이프라인 테스트까지 일반 데이터 엔지니어링 프로젝트의 여러 중요한 부분을 탐색할 것입니다. 파이썬은 데이터 엔지니어링에 가장 널리 사용되는 프로그래밍 언어이며, 내장 기능 및 효율적인 라이브러리를 사용하여 이러한 사용 사례를 어떻게 처리할 수 있는지 배우겠습니다.\n\n![이미지](/assets/img/2024-06-19-SimplifyingthePythonCodeforDataEngineeringProjects_0.png)\n\n<div class=\"content-ad\"></div>\n\n온라인 독특하고 모든 행사에 어울리는 선물을 판매하는 온라인 소매점을 운영 중이라고 상상해보세요. 이 온라인 상점은 매분, 매초 많은 거래량을 처리하고 있어요. 현재 거래에 대한 구매 습관을 분석하여 더 많은 고객들의 요구를 충족시키고 새로운 고객들에게 더 많은 서비스를 제공하는 것이 당신의 포부입니다. 이로 인해 거래 기록의 데이터 처리에 대해 파고들기로 하게 되었어요.\n\n## #0 모의 데이터\n\n먼저 JSON Lines (JSONL) 텍스트 형식을 사용하여 몇 가지 거래 데이터를 파일에 모의로 작성합니다. 각 줄은 별도의 JSON 객체입니다. 이 형식은 데이터 스트리밍에 매우 적합하며, 웹/앱 분석 및 로그 관리와 같은 분야에서 유용합니다.\n\n파일에서 데이터 필드는 다양한 데이터 유형에 속합니다. 고객 및 제품 식별자(정수/배열 형식), 결제 방법(문자열 형식) 및 총 거래 금액(부동소수점 숫자)을 포함합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nimport json\nimport random\nimport numpy as np\nimport datetime\n\n# 기존의 'retail_transactions.jsonl' 파일을 삭제합니다.\n! rm -f /p/a/t/h retail_transactions.jsonl\n\n# 거래 횟수를 설정합니다.\nno_of_iteration = 500000\n\n# 쓰기 모드로 파일을 엽니다.\nwith open('retail_transactions.jsonl', 'w') as f:\n  for num in range(no_of_iteration):\n    if (random.randint(1, 10000) != 5000):\n      # 유효한 거래를 생성합니다.\n      new_txn = {\n        'orderID': num,\n        'customerID': random.randint(1, 100000),\n        'productID': np.random.randint(10000, size=random.randint(1, 5)).tolist(),\n        'paymentMthd': random.choice(['신용 카드', '직불 카드', '디지털 지갑', '착불 현금', '암호화폐']),\n        'totalAmt': round(random.random() * 5000, 2),\n        'invoiceTime': datetime.datetime.now().isoformat()\n      }\n    else:\n      # 유효하지 않은 거래를 생성합니다.\n      new_txn = {\n        'orderID': \"\",\n        'customerID': \"\",\n        'productID': \"\",\n        'paymentMthd': \"\",\n        'totalAmt': \"\",\n        'invoiceTime': \"\"\n      }\n     \n     # 거래를 JSON 라인으로 파일에 씁니다.\n     f.write(json.dumps(new_txn) + \"\\n\")\r\n``` \n\n파일에 빈 데이터 필드가 있는 여러 건의 개별 거래를 발견할 수 있습니다. 이것은 실제 세계에서 자주 발생하는 데이터 품질 문제 중 하나인 누락된 데이터 문제를 모방합니다.\n\n## #1 데이터 수집 — 수확\n\n파일에서 거래 기록을 읽는 가장 간단한 방법 중 하나는 데이터 세트를 리스트로 반복하고 이후에 이를 Pandas DataFrame으로 변환하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 방법은 데모 데이터셋의 500,000개 거래에 대해 훌륭하게 작동할 것입니다. 그러나 실제 세계의 데이터셋은 수백만에서 심지어 수십억 행에 이를 수 있습니다. 전체 계산이 완료될 때까지 기다릴 필요가 있거나 메모리 문제가 발생할 수도 있습니다.\n\n가끔은 전체 결과에 신경 쓰지 않고 마지막 레코드가 로드되기 전에 초기 결과를 처리하고 싶을 수 있습니다. 이런 경우에는 yield를 사용하여 제너레이터의 흐름을 제어할 수 있습니다.\n\n사용자 코드와 라이브러리 코드를 번갈아가며 실행하는 루틴이 있습니다. 이는 순차적으로 실행되어 첫 번째 레코드에 도달하기 전에 두 번째 레코드에 액세스할 수 없음을 의미합니다. 이 개념에 대해 자세히 알아보려면 Pydata 토크 비디오에서 설명을 찾을 수 있습니다.\n\nyield 문은 다양한 실용적인 용도가 있습니다. 예를 들어 파일의 각 줄을 통해 이동하고 비어 있지 않은 레코드만 yield할 수 있습니다. 아래에서는 실시간 데이터 필터링을 실행하는 방법을 보여줍니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nimport json\n\ndef read_json_file(file_name):\n  # JSONL 파일 읽기\n  with open(file_name) as f:\n    for line in f:\n      txn = json.loads(line)\n      # 유효한 거래만 반환\n      if (txn['orderID'] != \"\"):\n        yield(txn)\n\ntxn_generator = read_json_file('retail_transactions.jsonl')\n```\n\n이 코드의 출력은 Python 생성기를 제공하며, 이는 특별한 유형의 반복자입니다. 반복문에서 next 함수를 사용하여 하나씩 다음 항목을 반환할 수 있습니다. 실시간 데이터 필터링 외에도, 데이터를 사전 처리하고 미리 정의된 일괄 크기로 제공하는 생성기 함수를 설계할 수 있습니다. 이 방법은 기계 학습 모델 훈련을 위해 데이터를 간편하게 전달하기 위해 사용될 수 있습니다. 더불어 생성기를 사용하여 웹 요청 및 응답을 비동기적으로 처리하는데도 사용할 수 있습니다.\n\n## #2 데이터 유효성 검사 — Pydantic\n\nJSON 데이터 목록이 주어진다고 가정해보세요. 데이터는 데이터 처리 후 거래 기록 정보를 포함합니다. 다음은 샘플 거래 정보입니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n{\n 'orderID': 10000,\n 'customerID': 48316,\n 'productID': [5620],\n 'paymentMthd': 'Cash on delivery',\n 'totalAmt': 9301.2,\n 'invoiceTime': '2024-06-10T23:30:29.608443',\n 'price': -1\n}\n```\n\n각 데이터가 들어올 때마다 유효성이 검증되었는지 확인하고, 그렇지 않으면 후속 데이터 처리 함수를 실행할 때 다양한 유형의 오류가 발생할 수 있습니다. 이는 pydantic 라이브러리를 사용하여 달성할 수 있습니다.\n\n```js\nfrom datetime import datetime\nfrom pydantic import BaseModel, ValidationError\n\n# 거래 기록용 데이터 모델 정의\nclass TxnModel(BaseModel):\n  orderID: int\n  customerID: int\n  productID: list[int]\n  paymentMthd: str\n  totalAmt: float\n  invoiceTime: datetime\n\ntry:\n  # 샘플 거래에 대해 스키마에 대한 유효성을 검증\n  TxnModel.model_validate(sample_txn)\n  print(\"유효성 검사 성공!\")\nexcept ValidationError as exc:\n  # 유효성 검사 오류가 있는 경우 오류 메시지 출력\n  print(\"유효성 검사 오류:\")\n  print(exc.errors())\n\n# 결과:\n# 유효성 검사 성공\n```\n\n때로는 더 엄격한 유효성 검사 규칙을 적용할 필요가 있습니다. Pydantic 기본 모델은 가능한 경우 문자열 데이터를 정수로 변환하려고 시도합니다. 이를 피하려면 strict=True를 모델 수준이나 필드 수준에서 설정할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n또한 데이터 필드에 사용자 정의 유효성 검사 규칙을 적용할 수 있습니다. 예를 들어, 지불 방법 값이 우리의 기대에 부합하는지 확인하고 싶을 수 있습니다. 테스트를 용이하게하기 위해 샘플 케이스의 지불 방법을 수동으로 'Bitcoin'으로 설정하고, 이후 AfterValidator를 사용하여 추가 확인을 위한 함수를 포함시킵니다. \n\n```js\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\n# 사용자 정의 유효성 검사 규칙 적용\ndef validate_payment_mthd(paymentMthd: str):\n  possible_values = ['신용 카드', '직불 카드', '디지털 지갑', '대금 지불', '암호화폐']\n  if paymentMthd not in possible_values:\n    raise ValueError(f\"유효하지 않은 지불 방법, 지불 유형은 {possible_values}중 하나여야 합니다.\")\n  return storage\n\n# 거래 레코드를 위한 데이터 모델 정의\nclass TxnModel(BaseModel):\n  orderID: int = Field(strict=True)\n  customerID: int\n  productID: list[int]\n  paymentMthd: Annotated[str, AfterValidator(validate_payment_mthd)]\n  totalAmt: Annotated[float, Field(strict=True, gt=0)]\n  invoiceTime: datetime\n\n# 존재하지 않는 지불 방법 수동으로 정의\nsample_txn['paymentMthd'] = 'Bitcoin'\n\ntry:\n  # 스키마에 대한 샘플 케이스 유효성 검사\n  TxnModel.model_validate(sample_txn)\n  print(\"유효성 검사 성공!\")\nexcept ValidationError as exc:\n  # 유효성 검사 오류에 대한 오류 메시지 출력\n  print(\"유효성 검사 오류:\")\n  print(exc.errors()[0]['ctx'])\n\n# 출력\n# 유효성 검사 오류:\n# {'error': ValueError(\"유효하지 않은 지불 방법, 지불 유형은 ['신용 카드', '직불 카드', '디지털 지갑', '대금 지불', '암호화폐']중 하나여야 합니다.\")}\n```\n\n이 유효성 검사기는 지불 방법이 가능한 값 목록에 포함되지 않음을 성공적으로 식별합니다. Pydantic의 내부 유효성 검사 논리를 적용한 후 사용자 정의 유효성 함수가 실행됩니다. 코드가 ValueError를 발생시키며 ValidationError이 채워집니다.\n\n오류가 발생하면 수정 조치를 취할 수 있습니다. 이러한 기능은 데이터 오류를 제거하여 데이터의 정확성과 완전성을 보장하는 데 도움을 줍니다.\n\n<div class=\"content-ad\"></div>\n\n## #3 데이터 처리\n\n(1) Python 데코레이터\n\n데이터 유효성 검사 후 데이터 집중적인 함수를 다루기 시작합니다. 데이터 파이프라인이 복잡해지면 실행 시간이 길어지는 경우가 많습니다. 함수의 성능을 최적화하고 실행 시간을 개선하는 데 필요한 자료를 식별하고자 합니다. 간단한 방법 중 하나는 각 함수의 시작과 끝에서 두 개의 타임스탬프를 수집한 후 그들 간의 시간 차이를 계산하는 것입니다.\n\n데이터 파이프라인 전체에 걸쳐 코드가 깔끔하게 유지되도록 하기 위해서는 Python 데코레이터를 활용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, 모든 거래에 대한 가격을 분류하는 데 걸리는 시간을 측정할 수 있습니다.\n\n```js\nimport time\n\n# 주어진 함수의 실행 시간을 측정합니다.\ndef time_decorator(func):\n  def wrapper(*args, **kwargs):\n    begin_time = time.time()\n    output = func(*args, **kwargs)\n    end_time = time.time()\n    print(f\"함수 {func.__name__}의 실행 시간: {round(end_time - begin_time, 2)} 초.\")\n    return output\n  return wrapper\n\n# 각 거래의 총 금액을 범주화합니다.\n@time_decorator\ndef group_txn_price(data):\n  for txn in data:\n    price = txn['totalAmt']\n    if 0 <= price <= 1500:\n      txn['totalAmtCat'] = '낮음'\n    elif 1500 < price <= 3500:\n      txn['totalAmtCat'] = '보통'\n    elif 3500 < price:\n      txn['totalAmtCat'] = '높음'\n  return data\n\ntxn_list = group_txn_price(txn_list)\n\n# 결과\n# 함수 group_txn_price의 실행 시간: 0.26 초.\n```\n\n데코레이터 접근 방식을 사용하면 원래 함수의 소스 코드를 변경하지 않고도 코드를 재사용할 수 있습니다. 마찬가지로, 작업이 실패할 때 함수 완료 로깅이나 이메일 알림에 대한 데코레이터 아이디어를 적용할 수 있습니다.\n\n(2) Map, reduce, filter\n\n<div class=\"content-ad\"></div>\n\n이것들은 많은 개발자들이 익숙할 것으로 생각되는 일반적으로 사용되는 Python 배열 메서드들입니다. 그러나 몇 가지 이유로 언급할 가치가 있다고 생각합니다: (1) 변경 불가능 - 이러한 함수들은 원래 리스트의 값들을 수정하지 않습니다; (2) 연쇄적 유연성 - 동시에 여러 함수의 조합을 적용할 수 있습니다; 그리고 (3) 간결하고 가독성 있음 - 오직 한 줄의 코드로만 가능합니다.\n\nJSON 객체 목록이 있는 경우, 결제 방법과 총액만 포함된 몇 개의 예제를 살펴보겠습니다.\n\nMap: 목록의 모든 요소에 동일한 작업을 수행합니다 (예: 결제 방법의 값에 접미사 추가).\n\n```js\nupdated_txn_list = list(map(lambda x: {\n                      'paymentMthd': f\"{x['paymentMthd']}_2024\",\n                      \"totalAmt\": x[\"totalAmt\"]\n                   }, txn_list))\n\nprint(updated_txn_list)\n\n# 출력\n# [{'paymentMthd': 'Cryptocurrency_2024', 'totalAmt': 3339.85},\n# {'paymentMthd': 'Cash on delivery_2024', 'totalAmt': 872.52},\n# ...]\r\n```\n\n<div class=\"content-ad\"></div>\n\n테이블 태그를 Markdown 형식으로 변경해주세요.\n\n```js\nupdated_txn_list = list(map(lambda x: x, filter(lambda y: y[\"paymentMthd\"] == \"Cryptocurrency\", txn_list)))\n\nprint(updated_txn_list)\n\n# Output\n# [{'paymentMthd': 'Cryptocurrency', 'totalAmt': 3339.85},\n# {'paymentMthd': 'Cryptocurrency', 'totalAmt': 576.15},\n# ...}] \n```\n\nReduce: 단일 값 결과를 얻는 방법(예: 모든 요소를 합하거나 곱하는 것)입니다.\n\n```js\nfrom functools import reduce\n\ntotal_amt_crypto = reduce(lambda acc, x: acc + x[\"totalAmt\"], updated_txn_list, 0)\n\nprint(total_amt_crypto)\n\n# Output\n# 250353984.67000002\n```\n\n<div class=\"content-ad\"></div>\n\n데이터 과학 프로젝트의 변환 단계에서 이러한 함수를 활용할 수 있어요. 예를 들어, 데이터를 조정하거나 정규화하기 위해 map()을 사용하거나 이상점과 관련 없는 데이터 포인트를 제거하기 위해 filter()를 사용하거나 요약 통계를 생성하기 위해 reduce()를 사용할 수 있어요.\n\n## #4 데이터 파이프라인 테스트 - Pytest\n\n데이터 파이프라인은 종종 데이터 수집, 데이터 정제 및 추출-변환-로드 (ETL) 작업을 포함해요. 잠재적인 오류 범위는 넓고 쉽게 간과될 수 있어요, 특히 모델 흐름과 결과가 사용자에게 해석하기 어려울 때요. 이러한 이유로 개발 팀은 테스트 노력에 더 heavily 의존합니다.\n\n가장 인기 있는 Python 테스트 프레임워크 중 하나는 Pytest에요. 기술 팀과 의사 결정자 모두가 신뢰할 수 있는 변환된 데이터의 고품질을 보장하고 싶다고 상상해봐요. 우리는 거래 가격을 분류하는 함수에 대한 테스트를 수행하고 싶어할 거예요. 이를 달성하기 위해 두 개의 파이썬 파일을 준비해야 해요:\n\n<div class=\"content-ad\"></div>\n\n- feature_engineering.py: 이 파일에는 이전에 작성한 함수가 포함되어 있습니다.\n\n```python\n# 각 거래의 총 금액을 범주화합니다.\ndef add_features(sample_cases):\n  for txn in sample_cases:\n    price = txn['totalAmt']\n    if 0 <= price <= 1500:\n      txn['totalAmtCat'] = '낮음'\n    elif 1500 < price <= 3500:\n      txn['totalAmtCat'] = '보통'\n    elif 3500 < price:\n      txn['totalAmtCat'] = '높음'\n  \n  return sample_cases\n```\n\n- test_feature_engineering.py: \"test_\" 접두사가 있는 파일이며, Pytest에서 테스트 목적으로 인식합니다.\n\n```python\nfrom feature_engineering import add_features\n\ndef test_add_features():\n  sample_cases = [{\n      'orderID': 1,\n      'customerID': 36536,\n      'productID': [2209, 2262, 4912, 3162, 5734],\n      'paymentMthd': 'Cryptocurrency',\n      'totalAmt': 576.15,\n      'invoiceTime': '2024-06-10T23:53:25.329928'\n    }]\n\n  # 샘플 케이스를 사용하여 함수 호출\n  sample_cases = add_features(sample_cases)\n \n  # assert 문을 확인합니다.\n  for txn in sample_cases:\n    assert 'totalAmtCat' in list(txn.keys())\n    assert len(txn) == 7\n    assert len(txn['totalAmtCat']) != 0\n```\n\n<div class=\"content-ad\"></div>\n\n위의 단언문은 새로운 'totalAmtCat' 데이터 필드가 비어 있지 않은 값으로 추가되며, 원래의 데이터 필드는 영향을 받지 않습니다. Pytest 명령을 실행함으로써 테스트가 통과되었음을 확인할 수 있습니다!\n\n![이미지](/assets/img/2024-06-19-SimplifyingthePythonCodeforDataEngineeringProjects_1.png)\n\n더 고급 사례로, 'load_data', 'clean_data' 및 'add_features'라는 세 가지 함수가 있는 경우, 이러한 함수들의 출력을 한 번에 하나씩 확인하도록 테스트 파일을 어떻게 설계해야 할까요?\n\n```js\nimport pytest\nimport json\nfrom feature_engineering import load_data, clean_data, add_features\n\n# 임시 JSONL 파일 설정\n@pytest.fixture\ndef jsonl_file(tmp_path):\n  sample_cases = [{'orderID': 10000,\n    'customerID': 48316,\n    'productID': [5620],\n    'paymentMthd': 'Cash on delivery',\n    'totalAmt': 9301.2,\n    'invoiceTime': '2024-06-10T23:30:29.608443',\n    'price': -1\n  }]\n\n  file_path = tmp_path + \"/test_transactions.jsonl\"\n\n  with open(file_path, 'w') as f:\n    for txn in sample_cases:\n        f.write(json.dumps(txn) + \"\\n\")\n\n  return file_path\n\n# `load_data` 함수를 검증하는 테스트 함수\ndef test_load_data(jsonl_file):\n  data = load_data(jsonl_file)\n  # 여기에 단언문 작성\n\n# `clean_data` 함수를 검증하는 테스트 함수\ndef test_clean_data(jsonl_file):\n  data = load_data(jsonl_file)\n  data = clean_data(data)\n  # 여기에 단언문 작성\n\n# `add_features` 함수를 검증하는 테스트 함수\ndef test_add_features(jsonl_file):\n  data = load_data(jsonl_file)\n  data = clean_data(data)\n  data = add_features(data)\n  # 여기에 단언문 작성\n```\n\n<div class=\"content-ad\"></div>\n\n초기화를 위해 JSON Lines 파일과 샘플 테스트 케이스를 사용하는 고정된 기준을 정의해야 합니다. 여기서는 앞에서 언급한 time_decorator와 비슷하게 작동하는 @pytest.fixture decorator를 사용합니다. 이 decorator는 샘플 파일을 반복해서 초기화하는 것을 방지하는 데 도움이 됩니다. 남은 코드에서는 파이프라인 함수를 실행하기 위해 여러 테스트 함수를 사용하고 논리 오류를 감지하기 위해 assert 문을 사용합니다.\n\n## 마무리\n\n우리는 데이터 엔지니어링 프로젝트의 중요한 측면을 몇 가지 경험했고 효율성과 가독성을 위해 Python 코드를 단순화하고 최적화하는 방법을 탐구했습니다:\n\n- yield를 사용하여 대용량 데이터셋을 처리하고 효율적인 메모리 사용을 가능케 함으로써 데이터 수집\n- Pydantic을 활용하여 스키마 및 사용자 정의 값 패턴을 기반으로 데이터 필드를 유효성 검사함으로써 데이터 검증\n- Python decorator 및 내장 라이브러리를 적용하여 중복 코드 없이 추가 기능을 활성화함으로써 데이터 처리\n- Pytest를 사용하여 워크플로 전체에서 품질 높은 함수 출력을 보장함으로써 파이프라인 테스트\n\n<div class=\"content-ad\"></div>\n\n## 마지막으로\n\n이 글을 즐겨 읽으셨다면, 제 Medium 페이지와 LinkedIn 페이지를 팔로우해 주시기 바랍니다. 그렇게 하시면 데이터 과학 사이드 프로젝트, 머신 러닝 운영(MLOps) 데모, 그리고 프로젝트 관리 방법론과 관련된 흥미로운 컨텐츠를 받아보실 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-SimplifyingthePythonCodeforDataEngineeringProjects_0.png"},"coverImage":"/assets/img/2024-06-19-SimplifyingthePythonCodeforDataEngineeringProjects_0.png","tag":["Tech"],"readingTime":13},{"title":"MapReduce에 대해 알아야 할 모든 것","description":"","date":"2024-06-19 05:12","slug":"2024-06-19-EverythingyouneedtoknowaboutMapReduce","content":"\n\n## Google에서 제공하는 'MapReduce: 대규모 클러스터에서 간소화된 데이터 처리' 논문의 모든 주요 통찰\n\n![image](/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_0.png)\n\n# 목차\n\n- 동기\n- 모델\n- MapReduce 구현\n- 지원 기능\n\n<div class=\"content-ad\"></div>\n\n# 소개\n\n2009년에 데이터 엔지니어로 일하셨군요.\n\n회사에서는 HDFS에 저장된 TB 단위의 데이터를 사용하고 계시군요.\n\n데이터 처리를 위한 로직을 작성하셨군요.\n\n<div class=\"content-ad\"></div>\n\n하나의 기계에서 실행했군요.\n\n작업을 끝내는 데 반나절이 걸렸군요.\n\n그래서 어떻게 최적화했나요?\n\n더 많은 기계를 사용해야 한다는 것을 깨달았죠.\n\n<div class=\"content-ad\"></div>\n\n그러나 어떻게 하면 계산을 신뢰할 수 있게 병렬 처리할 수 있을까요?\n\n당신이 2009년에 있었던 상황을 기억해봅시다:\n\n- Google BigQuery 출시 1년 전 (2010년)\n- Apache Spark 개발 2년 전 (2012년)\n- Amazon Redshift 출시 4년 전 (2013년)\n- AWS에서 Snowflake 출시 5년 전 (2014년)\n\n가장 가능성이 큰 선택은 구글에서 2004년에 처음으로 소개되었고 나중에 Yahoo가 오픈 소스화한 대규모 병렬 처리 프레임워크인 MapReduce였습니다.\n\n<div class=\"content-ad\"></div>\n\n이번 주에는 Google의 전형적인 논문을 통해 이 프레임워크에 대해 배우게 될 것입니다: MapReduce: 대규모 클러스터에서 간소화된 데이터 처리.\n\n# 동기\n\nGoogle에서는 수백 개의 연산이 대량의 데이터를 처리합니다. 이 중 대부분은 간단합니다. 그러나 데이터가 너무 많아서 한 대의 기계에서 처리할 수 없으며, 연산을 수백 대 또는 수천 대의 기계로 분산하여 실행하고 합리적으로 완료해야 합니다. 여기서 도전 과제가 있습니다:\n\n- 연산을 어떻게 병렬화할 것인가?\n- 데이터를 어떻게 효율적으로 분산할 것인가?\n- 장애를 어떻게 처리할 것인가?\n\n<div class=\"content-ad\"></div>\n\n이를 해결하기 위해 Google은 병렬화의 세부 사항을 추상화하여 간단한 계산을 표현할 수 있는 새로운 추상화를 설계했습니다. 이 모델은 Lisp와 다른 함수형 언어의 map 및 reduce 원시 기능에서 영감을 받았습니다. 이 작업의 Google 주요 기여 사항은 다음과 같습니다:\n\n- 병렬 계산을 정의하는 간단하고 강력한 인터페이스.\n- 대규모 계산의 자동 병렬화와 분배를 가능하게 함.\n- 상용 기계에서 높은 성능 달성 가능.\n\n# 모델\n\n![이미지](/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_1.png)\n\n<div class=\"content-ad\"></div>\n\n다음은 사용자가 정의하는 두 가지 기능을 갖는 모델입니다:\n\n- Map: 키/값 쌍 입력을 받아들이고 중간 키/값 쌍을 출력하는 함수입니다. 라이브러리는 동일한 키의 모든 값들을 그룹화하고 Reduce 작업에 전달합니다.\n- Reduce: Map 작업으로부터 중간 값들을 받습니다. 중간 값들은 이터레이터를 통해 Reduce에 제공됩니다. 그런 다음 Reduce 함수에서 정의된 로직을 사용하여 동일한 키의 중간 값들을 병합합니다 (예: Count, Sum 등). Reduce는 일반적으로 최대 하나의 출력 값을 생성합니다.\n\n정의 이후에 MapReduce 프로그램은 대규모 커머디티 머신 클러스터에서 병렬화되어 실행됩니다. 런타임은 사용자 개입 없이 데이터 파티셔닝, 장애 허용 및 머신 간 통신을 처리할 것입니다.\n\n# MapReduce 구현\n\n<div class=\"content-ad\"></div>\n\n# 실행 개요\n\n시스템은 데이터를 M개의 분할로 자동으로 분할합니다. 이 M개의 분할에서의 Map 호출은 여러 대의 기계에 분산됩니다. 이러한 분할은 서로 다른 기계에 의해 병렬로 처리될 수 있습니다. 중요한 점을 처리하기 위해 중요한 공간을 R 버킷으로 분할하는 파티션 함수(예: 키의 해시 함수)를 사용하여 Reduce 호출이 분산됩니다. 사용자는 파티션(R) 수와 파티션 함수를 정의할 수 있습니다.\n\n**일반적인 MapReduce 흐름에 대한 이미지:**\n\n![MapReduce](/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_2.png)\n\n일반적인 MapReduce 흐름에 따른 일반적인 단계는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- 맵리듀스 라이브러리는 입력 파일을 일반적으로 16에서 64 메가바이트(MB)로 나눈 M 개의 조각으로 분할합니다. (사용자가 크기/조각 팩터를 구성할 수 있습니다.)\n- 그런 다음 여러 대의 기계에서 프로그램을 복사본으로 시작합니다. (여러 대의 기계에서 맵리듀스 프로세스가 실행될 것이라고 생각할 수 있습니다.)\n- 맵리듀스 프로세스 중 하나는 마스터라고 불리며 나머지는 마스터에서 작업을 수신하는 워커입니다.\n- M개의 맵 작업과 R개의 리듀스 작업이 있고, 유휴 상태의 워커는 마스터로부터 맵 또는 리듀스 작업을 수신합니다.\n- 맵 워커는 해당 분할을 읽어 각 쌍을 사용자가 정의한 맵 함수에 전달합니다. (예: 각 값에 X를 곱하거나). 워커는 중간 키/값 쌍 출력을 메모리에 버퍼링합니다.\n- 워커는 주기적으로 버퍼링된 쌍을 로컬 디스크에 기록한 다음 그 위치를 디스크에 마스터에게 알립니다.\n- 마스터는 리듀스 워커에게 이러한 위치에 대해 통지합니다. 통지를 받은 리듀스 워커는 맵 워커의 로컬 디스크에서 버퍼링된 데이터를 읽기 위해 원격 프로시저 호출을 사용합니다.\n- 중간 데이터 읽기가 완료되면 리듀스 워커는 중간 키를 기준으로 데이터를 정렬하여 동일한 키의 모든 발생을 그룹화합니다. 리듀스 워커는 데이터를 정렬해야하는데, 이는 맵 워커에서 서로 다른 키를 처리해야 하기 때문입니다. 정렬은 동일한 키의 값이 서로 가까이 있도록합니다.\n- 리듀스 워커는 정렬된 중간 데이터를 반복하여 각 고유 키마다 해당 키와 대응하는 중간 값 집합을 사용자의 리듀스 함수에 전달한 다음, 리듀스 함수의 출력을 이 리듀스 파티션에 대한 최종 출력 파일에 추가합니다.\n- 모든 맵 및 리듀스 작업이 완료되면 마스터가 사용자 프로그램을 깨웁니다 (종류의 비동기 프로세스).\n- 성공적으로 완료된 후, 맵리듀스 실행의 출력은 R 버킷과 관련된 R 출력 파일에서 사용할 수 있습니다. 분리된 파일을 반환하면 사용자가 이러한 결과 파일을 다른 맵리듀스 프로그램이나 다른 분산 애플리케이션에 입력할 수 있습니다.\n\n# 마스터 데이터 구조\n\n마스터는 각 맵 및 리듀스 작업의 상태와 워커 기계의 식별 정보를 저장합니다. 마스터는 또한 맵 및 리듀스 워커 사이의 \"중개역할\"을 합니다: 맵 워커로부터의 중간 파일의 위치를 리듀스 워커에게 알려줍니다. 각 완료된 맵 작업에 대해 마스터는 중간 파일의 위치와 크기를 저장합니다. 맵 워커들이 작업을 완료하면 중간 파일의 위치 및 크기 정보를 마스터에게 업데이트합니다. 마스터는 이 정보를 진행 중인 리듀스 작업을 하는 워커에게 푸시합니다.\n\n# 내고장성\n\n<div class=\"content-ad\"></div>\n\nMapReduce의 궁극적인 목표 중 하나는 여러 대의 기계에서 대량의 데이터를 신뢰성 있게 처리하는 것입니다. 그렇다면, 만약 실패가 발생한다면 어떨까요?\n\n작업자의 실패\n\n![작업자 이미지](/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_3.png)\n\n- 마스터는 주기적으로 핑을 보내 작업자의 상태를 확인합니다.\n- 작업자로부터 일정 시간 동안 응답이 없으면, 마스터는 작업자의 실패로 간주합니다.\n- 완료되거나 실패한 Map 작업은 대기 상태로 재설정되어 마스터가 다른 기계에서 이러한 작업을 다시 예약할 수 있습니다. 완료된 Map 작업은 로컬 디스크에 결과가 저장되므로, 만약 Map 기계가 실패하면 Map 결과도 접근할 수 없게 됩니다.\n- 실패한 Reduce 작업은 또한 대기 상태로 설정되어 다시 예약될 수 있습니다. 완료된 Reduce 작업은 결과가 전역 파일 시스템에 저장되므로 다시 실행할 필요가 없습니다.\n- 작업자 A가 Map 작업을 처리하고, 나중에 작업자 A가 실패하면, 마스터의 일정에 따라 작업자 B가 이 Map 작업을 담당하게 되며, 모든 Reduce 작업자에게 다시 실행이 필요함을 통지합니다. 처음에 Map Worker A의 데이터를 사용한 모든 Reduce 작업은 Worker B의 데이터를 읽게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n실패한 마스터\n\n- 마스터는 주기적인 메타데이터 체크포인트를 작성합니다.\n- 마스터가 다운되면, 마지막 체크포인트 상태에서 새로운 마스터를 시작할 수 있습니다.\n\n# 지역성\n\n![이미지](/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_4.png)\n\n<div class=\"content-ad\"></div>\n\n구글은 GFS가 관리하는 입력 데이터가 클러스터의 기계들의 로컬 디스크에 저장되어 있음을 이용하여 네트워크 대역폭을 활용합니다. GFS는 각 파일을 64MB 블록으로 나누고 각 블록의 중복된 복제본 (기본 3개)을 다른 기계에 저장합니다. MapReduce 마스터는 해당 입력의 복제본을 저장하는 기계에 Map 작업을 스케줄하려고 노력합니다. 마스터가 그 방법으로 스케줄을 할 수 없으면 해당 작업의 입력 데이터의 복제본 근처에 Map 작업을 스케줄하려고 시도합니다. 이렇게 함으로써 대부분의 워커가 입력 데이터를 로컬에서 읽고 네트워크 대역폭을 소비하지 않도록 보장합니다.\n\n## 작업의 정밀도\n\nMapReduce는 M 단계를 M 조각으로, Reduce 단계를 R (사용자가 제한함) 조각으로 나눕니다. 일반적으로 M과 R은 사용 가능한 워커보다 훨씬 더 커야 합니다. 각 워커가 여러 다양한 작업을 수행하도록 함으로써 동적으로 작업을 분산하고 워커가 실패할 때의 복구 속도를 높일 수 있습니다: 실패한 Map 작업은 완료된 모든 다른 워커 기계들 사이에 분산될 수 있습니다. M과 R의 크기에는 제한이 있어야 합니다. 마스터는 O(M + R) 스케줄링 결정을 내려야 하고 O(M ∗ R) 상태를 메모리에 유지해야 하기 때문에 마스터에 부담이 많이 가지 않도록 하는 것이 필요합니다.\n\n## 백업 작업\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_5.png\" />\n\n맵리듀스 작업의 전체 대기 시간이 증가하는 일반적인 이유 중 하나는 \"straggler\"입니다. straggler는 소수의 맵 또는 리듀스 작업 중 하나를 완료하는 데 더 오랜 시간이 걸리는 머신을 가리킵니다. straggler는 나쁜 디스크가 장착된 머신이거나 마스터가 머신에서 여러 작업을 예약하여 작업 간 리소스 경합을 야기하는 등 다양한 이유로 발생할 수 있습니다. Google은 이에 대한 해결책을 갖고 있습니다. MapReduce 작업이 거의 완료되기 직전에 마스터는 진행 중인 나머지 작업의 백업 실행을 예약합니다. 이 백업 작업은 기본 작업과 함께 병행하여 실행됩니다. 주요 작업 또는 백업 실행이 완료되면 작업은 완료된 것으로 표시됩니다. 따라서 주요 작업에 문제가 발생하여 처리 속도가 늦추어지면, straggler 증상이 없는 백업 작업이 프로세스를 효율적으로 처리할 수 있습니다.\n\n# 지원 기능\n\n맵리듀스의 기본 기능은 대부분의 요구사항을 충족시키지만, Google은 몇 가지 확장 기능이 유용하다고 판단했습니다.\n\n<div class=\"content-ad\"></div>\n\n# 파티션 함수\n\n기본 데이터 파티션 함수는 해싱입니다. 다른 논리가 필요한 상황을 지원하기 위해 MapReduce 라이브러리 사용자는 사용자 정의 파티션 함수를 제공할 수 있습니다.\n\n# 순서 보장\n\nMapReduce는 중간 키/값 쌍이 주어진 파티션 내에서 키 순서를 오름차순으로 처리함을 보장합니다. 이는 각 파티션마다 정렬된 출력 파일을 생성하는 것이 쉬워지도록 만듭니다. 사용자가 데이터가 정렬되어 있으면 편리하다고 생각할 때 유용합니다.\n\n<div class=\"content-ad\"></div>\n\n# Combiner Function\n\n컴바이너 함수는 맵 워커에서 실행됩니다. 일반적으로 사용자는 컴바이너 함수와 리듀스 함수를 구현하는 데 동일한 코드를 사용합니다. 둘 사이의 유일한 차이점은 MapReduce 라이브러리가 함수의 출력을 처리하는 방법입니다:\n\n- 리듀스 함수의 출력은 최종 출력 파일에 작성됩니다.\n- 컴바이너 함수의 출력은 중간 파일에 작성되고, 이 파일은 리듀스 작업으로 전송됩니다.\n\n# 입력 및 출력 유형\n\n<div class=\"content-ad\"></div>\n\n맵리듀스는 다양한 형식의 입력 데이터를 읽을 수 있는 지원을 제공합니다. 예를 들어, 텍스트 유형은 키를 파일 내 오프셋으로 처리하고 값을 라인의 내용으로 처리합니다. 사용자는 간단한 리더 인터페이스를 구현하여 새로운 입력 유형을 지원할 수 있습니다. 출력 유형에 대해, 맵리듀스는 다양한 형식으로 데이터를 생성하기 위한 일련의 출력 유형을 지원하며 사용자가 새로운 출력 유형을 정의할 수도 있습니다.\n\n# 부작용\n\n맵리듀스를 사용하면 매핑 또는 리듀싱 작업이 추가 파일을 추가로 생성할 수 있는지 여부를 지정할 수 있습니다.\n\n# 잘못된 레코드 건너뛰기\n\n<div class=\"content-ad\"></div>\n\n가끔 사용자 코드에 버그가 있어 Map 또는 Reduce 함수가 특정 레코드에서 크래시하는 경우가 있습니다. 이러한 버그로 인해 MapReduce 프로그램이 완료되지 못할 수 있습니다. 보통은 버그를 수정하는 것이 일반적인 해결책이지만 때로는 추가 조치가 필요할 수도 있습니다. 버그가 제3자 라이브러리에서 발생할 수 있으며 해당 소스 코드에 접근할 수 없는 경우가 있습니다. 또한 경우에 따라 일부 레코드를 무시하는 것이 허용됩니다. Google은 옵션 실행 모드를 제공하여 MapReduce 라이브러리가 어떤 레코드가 크래시를 발생시킬 수 있는지 감지하고 이러한 레코드를 건너뛰어 전진 진행을 이룰 수 있습니다.\n\n# 상태 정보\n\n마스터는 내부 HTTP 서버를 실행하고 MapReduce 프로그램의 모니터링 및 추적을 위한 일련의 상태 페이지를 제공합니다. 상태 페이지에는 계산의 진행 상황을 보여주는 정보가 포함되어 있습니다. 완료된 작업, 진행 중인 작업, 입력 크기, 중간 크기, 출력 크기 등이 표시됩니다. 또한 각 작업에서 생성된 표준 오류 및 표준 출력 파일에 대한 링크도 포함되어 있습니다. 최상위 상태 페이지에는 실패한 워커와 그들이 실패할 때 처리하던 맵 및 리듀스 작업이 표시됩니다. 사용자는 이 데이터를 사용하여 계산이 얼마나 걸릴지 예상하고 더 많은 리소스를 추가해야 하는지 여부를 판단할 수 있습니다.\n\n# 카운터\n\n<div class=\"content-ad\"></div>\n\nMapReduce 라이브러리는 다양한 이벤트의 발생 횟수를 세는 카운터를 제공합니다. 예를 들어, 사용자 코드는 처리된 총 단어 수를 세고 싶어할 수 있습니다. 이 기능을 사용하려면 사용자는 명명된 카운터 객체를 만들고 Map 및 Reduce 함수에서 카운터를 적절하게 증가시킵니다. 워커는 주기적으로 카운터 값을 마스터에 보고하며, 이 보고는 마스터로부터 ping-health-check 요청에 대한 응답과 함께 전송됩니다. 마스터는 성공한 Map 및 Reduce 작업에서 카운터 값을 집계하고 프로그램이 완료될 때 사용자 코드로 반환합니다. 카운터 값을 집계할 때 마스터는 동일한 Map/Reduce 작업, 백업 작업 또는 실패한 작업의 중복 실행의 영향으로 발생하는 중복을 제거합니다.\n\n# 마무리\n\n이 글을 통해 큰 규모 클러스터에서 간단한 데이터 처리를 위한 MapReduce의 편리함을 알아냈습니다. 두 가지 Map 및 Reduce 함수로 구성되어 있지만 이 프레임워크는 상용품 기계의 대규모 클러스터에서 연산을 병렬화하는 매우 효율적이고 견고한 방법을 제공합니다. 글을 마치기 전에 한 가지 더 알려드립니다: BigQuery 처리 엔진인 Dremel은 MapReduce의 영감을 받아 개발되었습니다.\n\n이제 이만 쉬어 가도록 하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n다음 블로그에서 만나요 ;)\n\n# 참고 자료\n\n[1] Jeffrey Dean and Sanjay Ghemawat, MapReduce: Simplified Data Processing on Large Clusters (2004).","ogImage":{"url":"/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_0.png"},"coverImage":"/assets/img/2024-06-19-EverythingyouneedtoknowaboutMapReduce_0.png","tag":["Tech"],"readingTime":9},{"title":"undefined","description":"","date":"2024-06-19 05:11","slug":"undefined","content":"\n\n<img src=\"/assets/img/undefined_0.png\" />\n\n우리의 데이터 처리 세계에서는 Azure Data Factory (ADF) 파이프라인에서 Databricks 노트북을 실행하는 환경에서 비용을 관리하는 것이 중요합니다. 성능을 유지하면서 비용을 줄이기 위해 대화식 클러스터에서 작업 클러스터로 마이그레이션하기로 결정했습니다. 이 과정에서 우리가 관찰하고 배운 내용을 공유하려고 합니다.\n\n세팅\n\n우리의 데이터 처리에는 두 가지 유형의 실행 프로세스가 포함되어 있었습니다:\n1. 병렬 실행: ADF에서 동시에 트리거된 여러 노트북.\n2. 순차 실행: 종속성으로 인해 특정한 순서로 트리거된 노트북들이 서로 뒤이어 실행됩니다.\n\n<div class=\"content-ad\"></div>\n\n최초 계획\n\n병렬 실행:\n- 이전 방법: 대화식 클러스터 사용.\n- 새 방법: 작업 클러스터로 전환.\n- 이유: 각 작업 클러스터의 실행 시간이 모든 노트북이 동시에 트리거되기 때문에 동일했습니다.\n\n순차 실행:\n- 이전 방법: 대화식 클러스터 사용.\n- 새 방법: 작업 클러스터 사용 시도 중.\n- 도전: 각 작업 클러스터의 실행 시간이 누적되며, 노트북 세트마다 약 세 분이 추가됩니다. 십 개 세트가 있으면 이 과정에 추가로 30분이 소요되므로 순차 실행에 대해서는 계속 대화식 클러스터를 사용했습니다.\n\n스핀 업 시간을 줄이기 위해 컴퓨팅 풀 사용을 고려했지만, 이는 비용 절감이라는 주요 목표와 부합하지 않았습니다.\n\n<div class=\"content-ad\"></div>\n\n이주 작업\n\n우선, 동일한 구성을 가진 인터랙티브 클러스터 5개가 있었습니다. 비용 효율성을 위해:\n- 병렬 실행을 작업 클러스터로 옮겼습니다.\n- 순차적 실행을 위해 인터랙티브 클러스터 3개를 해체하여 2개만 남겼습니다.\n\n우리 클러스터 구성:\n- 노드 유형: Standard_DS32_v3\n- 워커 노드: 2에서 20으로 자동 확장\n\n작업 클러스터에도 동일한 구성을 사용했습니다.\n\n<div class=\"content-ad\"></div>\n\n예상치 못한 결과\n\n첫 달에는 비용이 감소할 것으로 예상했지만, 뜻밖에도 비용이 40% 증가했습니다.\n\n비용 분석:\n- 이전 이주 후: 대화식 클러스터 월 비용 $26,000\n- 이주 후:\n  - 대화식 클러스터: 월 $16,000\n  - 작업 클러스터: 월 $18,000\n  - 총합: 월 $34,000\n\n해결책\n\n<div class=\"content-ad\"></div>\n\n비용 증가를 인식하여, 저희는 작업 클러스터를 재구성했습니다:\n1. Standard_DS16_v3:\n   - 결과: 작업 완료에 소요되는 시간은 동일했습니다.\n   - 비용: 월 $12,000으로 감소했습니다.\n\n2. Standard_DS8_v3:\n   - 결과: 작업이 추가 10분 소요되었습니다.\n   - 비용: 더욱 감소하였으며, 효율적인 작업 완료 시간을 유지했습니다.\n\n이 조정으로 성능을 희생하지 않고 비용을 크게 줄일 수 있었습니다.\n\n맺음말\n\n<div class=\"content-ad\"></div>\n\n우리의 대화형 클러스터에서 작업 클러스터로의 여정은 예상치 못한 변화로 가득찼어요. 처음에는 비용이 증가했지만, 작업 클러스터를 재구성함으로써 상당한 절약을 이뤄 냈어요. 우리의 경험은 성능과 비용을 최적화하기 위해 지속적으로 모니터링하고 구성을 조정하는 중요성을 강조합니다.\n\n다음 기사에서는 클러스터 구성을 변경해도 작업을 완료하는 데 걸리는 시간이 거의 변하지 않은 이유에 대해 자세히 다뤄볼 거에요.\n\n참고: 여기서 언급된 비용은 대략적인 수치이며 설명 용도로 사용되었습니다.\n\n더 많은 통찰력을 기대해주세요!","ogImage":{"url":"/assets/img/undefined_0.png"},"coverImage":"/assets/img/undefined_0.png","tag":["Tech"],"readingTime":2},{"title":"QGIS에서 Landsat 표면 반사 데이터 다루기","description":"","date":"2024-06-19 05:09","slug":"2024-06-19-WorkingWithLandsatSurfaceReflectanceDatainQGIS","content":"\n\n표 정보로 형식을 변경합니다. \n\n표 정보는 Markdown 형식으로 변경할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n지구 탐사자(또는 원하는 데이터 저장소)에서 주문하세요.\n\n![이미지](/assets/img/2024-06-19-WorkingWithLandsatSurfaceReflectanceDatainQGIS_0.png)\n\nNDVI(및 기타 지수/계산)에 대해서는 표면 반사율 데이터를 사용하세요: Landsat Collection 2 Level-2\n\n진- 또는 거짓-컬러 이미지의 경우 래디언스(레벨-1) 또는 표면 반사율(레벨-2)이 모두 작동합니다(레벨-2가 서로 다른 시간과 지역에서 더 일관성 있지만, 흐림이나 구름이 있을 경우 아티팩트가 발생할 수 있습니다).\n\n<div class=\"content-ad\"></div>\n\n샘플 데이터셋이 필요하시다면, 2016년 5월 및 2023년 6월에 대해 인접한 두 장면으로부터 Landsat 데이터의 네 개 밴드를 업로드했습니다.\n\n데이터를 다운로드한 후 다음 명령어를 사용하여 QGIS로 가져오세요:\n\n레이어 ` 레이어 추가 ` 래스터 레이어 추가...\n\nNDVI 및 “표준” 위양색을 위해 B5 (근적외선), B4 (적색) 및 B3 (녹색)를 선택하세요. 그런 다음 추가를 클릭하세요\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-WorkingWithLandsatSurfaceReflectanceDatainQGIS_1.png)\n\n다중 스펙트럼(색상) 이미지를 만들려면 \"가상 래스터\"를 생성하세요.\n\n래스터 `기타` 가상 래스터 만들기...\n\n각 입력 파일을 별도의 밴드에 배치하는 것을 확인하세요 (이것은 중요합니다 — 그렇지 않으면 이미지가 단일 그레이스케일 밴드가 됩니다!)\n\n\n<div class=\"content-ad\"></div>\n\n\"…버튼을 사용하여 입력 레이어를 선택하세요.\n\n세 개의 밴드를 모두 선택하고, B5를 맨 위에, B4를 가운데에, B3를 가장 아래로 재배열하세요. 이렇게 하면 근적외선 = 빨강, 빨강 = 녹색, 초록 = 파랑의 밴드 순서가 설정됩니다.\n\n실행을 눌러주세요\n\n<img src=\"/assets/img/2024-06-19-WorkingWithLandsatSurfaceReflectanceDatainQGIS_2.png\" />\"\n\n<div class=\"content-ad\"></div>\n\n지금 이 시점에서 이 복합체의 이름을 바꾸고 저장하는 것이 좋은 생각입니다.\n\n이미지 이름을 마우스 오른쪽 버튼으로 클릭한 후 `내보내기 '다음으로 저장 ...'를 선택하십시오.\n\n출력 모드를 Raw data로 설정하십시오.\n\n원하는 폴더를 선택하고 파일 이름 옆에있는 ...을 클릭하여 이름을 입력하십시오.\n\n<div class=\"content-ad\"></div>\n\n압축 기능을 활성화하는 것도 좋은 아이디어입니다. (원시 데이터에는 높은 압축률을 선택하고, 렌더링된 이미지에는 낮은 압축률을 선택하세요)\n\n확인을 클릭하세요\n\n![이미지](/assets/img/2024-06-19-WorkingWithLandsatSurfaceReflectanceDatainQGIS_3.png)\n\n이미지 이름을 마우스 오른쪽 버튼으로 클릭한 후에 속성 '심볼'을 선택하세요\n\n<div class=\"content-ad\"></div>\n\n최소/최대 값 설정 확장\n\n누적 개수 설정을 선택하고, 범위를 2.0–98.0에서 0.1–99.9로 변경해주세요.\n\n선택 사항: 빨강, 녹색, 파랑에 대해 세 개의 최소값을 동일한 값으로 설정하고, 최대값도 빨강, 녹색, 파랑에 대해 동일한 값으로 설정해주세요. 표면 반사 데이터는 대기 효과가 제거된 백분율 값이며, 파장별 밝기 차이나 대기 산란을 조절할 필요가 없습니다.\n\n<img src=\"/assets/img/2024-06-19-WorkingWithLandsatSurfaceReflectanceDatainQGIS_4.png\" />\n\n<div class=\"content-ad\"></div>\n\n정확도를 \"실제 (느림)\"으로 설정한 후에 \"적용\"을 클릭하세요 (\"적용\"은 레이어 속성 창을 닫지 않고 결과를 미리 볼 수 있습니다). \"실제 (느림)\"은 전체 이미지의 모든 픽셀을 읽기 때문에 \"추정 (빠름)\"보다 조금 느릴 수 있지만 더 좋은 결과를 얻을 수 있다고 생각합니다.\n\n레이어 렌더링을 확장하여 (보이지 않는 경우 창을 아래로 스크롤하세요).\n\n감마를 1.8-2.2 사이의 값으로 설정한 후에 \"적용\"을 클릭하세요 (선호하는대로 설정하세요, 더 높은 숫자는 전체적으로 더 밝은 이미지를 만듭니다).\n\n메인 QGIS 창으로 돌아가려면 \"확인\"을 클릭하세요.\n\n<div class=\"content-ad\"></div>\n\n작업이 완료된 이미지를 GeoTIFF로 내보내려면, 먼저 레이어 팔레트에서 이미지 이름을 마우스 오른쪽 버튼으로 클릭한 다음 `Export Save As ...`를 선택하세요.\n\n출력 모드를 렌더링된 이미지로 설정해주세요.\n\n출력 폴더와 파일 이름을 선택하고, 저장 공간을 아낄 수 있는 만큼 Create Options 및 Low Compression(픽셀 당 8비트 이미지에 적합)을 확인해주세요.\n\n이미지를 저장하려면 OK를 클릭하세요!\n\n<div class=\"content-ad\"></div>\n\n여러 장면을 작업 중이라면, 인접 장면 및/또는 다른 날짜로 반복하거나 데이터를 가져와 다중 분광 파일을 만들어 방금 만든 스타일을 다른 장면으로 전송하세요. 새 파일을 저장하고 레이어 패널에 추가한 후, 2016 이미지에서 스타일을 복사/붙여넣기하세요:\n\n이미 조정한 래스터 레이어를 마우스 오른쪽 버튼으로 클릭하고 스타일 선택 ` 스타일 복사\n\n그런 다음 대상 레이어를 마우스 오른쪽 버튼으로 클릭하고 스타일 선택 ` 스타일 붙여넣기\n\n빛의 양과 대기 상황에 독립적인 표면 반사 데이터를 다루고 있기 때문에 같은 설정이 다른 시간과 다른 장소에서도 작동할 수 있습니다. (반사도 데이터는 계절과 날씨에 따라 변할 수 있으며, 각 장면은 개별적으로 보정되어야 합니다.) 그러나 이미지 간에 큰 변화가 있는 경우에는 사용하는 데이터를 최소값 및 최대값 계산에 신중히 선택해야 합니다. 눈이나 구름, 그림자가 있는 구름은 맑은 장면과 비교할 때 값이 크게 변할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# Raster 계산기를 사용하여 NDVI 계산하기\n\n저는 Normalized Difference Vegetation Index (NDVI)를 예시로 사용하고 있습니다. 이는 간단하고 널리 사용되는 지표이지만, 다른 분광 지수를 계산하는 데도 동일한 프로세스가 적용됩니다. 수식만 다를 뿐입니다. (Index Database는 산불 심각도부터 수질까지 모든 항목에 대한 지표를 찾기에 좋은 장소입니다. 다른 것을 실험해보고 싶다면 이용해보세요. 선택한 지표가 무엇이든, 올바른 단위를 얻기 위해 스케일링(0.000275)과 오프셋(-0.2) 요소를 적용하고 추가 계산을 수행하기 전에 기억하세요.)\n\n같은 표면 반사 데이터로 시작하되, 근적외선과 적외선 데이터만 필요합니다 (Landsat 8 및 9의 밴드 5와 4).\n\nRaster `Raster Calculator…`을 선택하세요.\n\n<div class=\"content-ad\"></div>\n\n그런 다음 다음 방정식을 입력하십시오 (따옴표로 묶인 부분은 계산에 사용되는 밴드를 나타냅니다. - 실제 데이터에 따라 달라질 수 있습니다).\n\n((“LC08_L2SP_157039_20160518_20200907_02_T1_SR_B5@1” * 0.0000275 - 0.2) -(“LC08_L2SP_157039_20160518_20200907_02_T1_SR_B4@1” * 0.0000275 - 0.2))/((“LC08_L2SP_157039_20160518_20200907_02_T1_SR_B5@1” * 0.0000275 - 0.2) + (“LC08_L2SP_157039_20160518_20200907_02_T1_SR_B4@1” * 0.0000275 - 0.2))\n\nNDVI 알고리즘은 보통 (NIR — Red)/(NIR + Red)로 구성되지만 추가적인 용어가 있는 이유는 무엇인가요? 16비트 정수 값으로 저장된 데이터를 반사도로 변환하여 스케일을 맞추어야 하기 때문입니다. 보통 반사도는 0에서 1까지의 범위에 있습니다. (표면 반사 알고리즘에서의 지나친 보정으로 값이 0 미만으로 떨어지고 특정 지형 (해천을 향한 산의 경사면)은 1 이상의 값으로 이끌 수 있기 때문에 이상치가 가능하지만 일반적으로 나타나지 않습니다.) 스케일 계수인 0.0000275와 오프셋인 0.2는 모든 Landsat 8 & 9 버전 2 데이터에서 일관적이지만 Sentinel-2와 같은 다른 이미지 원본에서는 달라질 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-WorkingWithLandsatSurfaceReflectanceDatainQGIS_5.png)\n\n<div class=\"content-ad\"></div>\n\n계산을 실행하기 전에, 출력이 저장될 폴더를 선택하고 \"Output Layer: ...\"라는 이름으로 출력 파일을 지정해야 합니다.\n\n그런 다음 NDVI 값을 계산하려면 확인 버튼을 클릭하세요. 이 값들은 새로운 레이어로 나타날 것입니다. 기본적으로 데이터는 씬에서 발견된 최소 및 최대 값을 기준으로 조정된 그레이스케일로 표시됩니다. 이것은 읽기가 어렵고, 크게 영감을 주는 것이 아닐 수 있으므로, 아마도 QGIS에서 컬러 램프라고 불리는 팔레트를 적용하고 싶을 것입니다.\n\n이미지 이름을 마우스 오른쪽 버튼으로 클릭한 후, `Symbology`를 선택하세요.\n\n<div class=\"content-ad\"></div>\n\n렌더 타입을 싱글밴드 의사 색상으로 변경해주세요.\n\n최솟값을 0으로, 최댓값을 0.8로 설정해주세요 (이 값은 NDVI의 기본값으로, -1부터 +1까지의 범위를 가지지만, 식물이 자라는 지역은 0 아래로 내려가거나 0.8을 넘을 일이 거의 없습니다.)\n\n해석하기 쉬운 지각적 팔레트를 채택해 색상 램프를 선택해주세요.\n\nQGIS에 여러 장면을 로드한 경우, 동일한 표현 매개변수를 각각에 적용하기 위해 스타일을 복사하여 붙여넣는 방법을 사용하시면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n이미 스타일링한 레이어에서 마우스 오른쪽 버튼을 클릭하고 스타일 ` 스타일 복사를 선택하여 데이터셋에서 설정을 복사한 다음 대상 레이어를 마우스 오른쪽 버튼으로 클릭하고 스타일 ` 스타일 붙여넣기를 선택하여 설정을 적용할 수 있습니다.\n\n멀티 스펙트럼 컴포지트의 스케일링 및 감마 설정에도 동일한 방법이 적용되며, 이는 특히 표면 반사 데이터를 다룰 때 효과적입니다.\n\n# 보너스: Open Street Map (벡터) 오버레이 추가\n\nQGIS에서 이미 래스터 데이터를 처리하고 스타일을 지정한 후 QuickOSM 플러그인을 사용하여 Open Street Map (OSM)의 벡터 데이터를 추가할 수 있습니다. OSM 및 QuickOSM은 깊은 주제이므로 여기서는 시작하는 데 도움이 될만한 팁 몇 가지만 제공하겠습니다.\n\n<div class=\"content-ad\"></div>\n\nQGIS에서 플러그인을 추가하려면 Plugins - Manage and Install Plugins...를 선택하세요.\n\n<img src=\"/assets/img/2024-06-19-WorkingWithLandsatSurfaceReflectanceDatainQGIS_7.png\" />\n\n플러그인을 로드하려면 OSM 또는 QuickOSM을 검색하고, 해당 플러그인을 선택한 후 설치 플러그인을 클릭하세요.\n\n그러면, Vector 메뉴를 통해 OSM 데이터를 추가할 수 있습니다: Vector - QuickOSM - QuickOSM\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-WorkingWithLandsatSurfaceReflectanceDatainQGIS_8.png\" />\n\n데이터를 찾으려면 넓은 특징 카테고리(경계, 고속도로 또는 자연과 같은)에 해당하는 값(행정, 국립공원 또는 정치와 같은 더 구체적인 카테고리)를 입력하십시오. 이렇게 하면 빠르게 복잡해질 수 있습니다(궁금하시다면 모든 OSM 기능에 대한 설명이 여기 있습니다). 그래서 여러 카테고리를 하나의 검색으로 결합하는 프리셋 기능을 사용하는 것을 권합니다. OSM은 지도와 함께 데이터베이스이기도 한데 머리가 아프게 만들 수 있습니다.\n\n<img src=\"/assets/img/2024-06-19-WorkingWithLandsatSurfaceReflectanceDatainQGIS_9.png\" />\n\nOSM에서 검색 및 다운로드를 하기 전에 검색 영역을 제한하는 마지막 단계입니다. 지도에서 관심 영역으로 사용할 레이어를 선택하는 것이 좋은 접근 방식이라고 생각합니다. 기능 목록 아래에 있는 드롭다운 메뉴 In을 선택하세요. Layer Extent로 변경하고 작업 중인 Landsat 씬을 선택하면 해당 레이어의 가장자리와 교차하는 데이터로 검색이 제한됩니다.\n\n<div class=\"content-ad\"></div>\n\n데이터베이스를 검색하고 데이터를 다운로드하려면 쿼리 실행을 클릭한 후 벡터를 따로 점, 선 및 폴리곤 레이어로 추가하세요.\n\n![image](/assets/img/2024-06-19-WorkingWithLandsatSurfaceReflectanceDatainQGIS_10.png)\n\n일반적으로 저는 데이터를 처리하는 도구로 QGIS를 사용하고, 포토샵 및/또는 일러스트레이터에서 정리하기 전에 모든 오버레이 벡터, 장식 및 주석이 포함된 고해상도 이미지를 내보낼 수 있는 것이 도움이 됩니다.\n\n기본적으로 QGIS는 화면에 표시된 줌 수준 및 바운딩 박스에서 내보냅니다. 전체 해상도로 내보내려면 레이어를 마우스 오른쪽 버튼으로 클릭한 후 네이티브 해상도로 확대(100%)를 선택하세요.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-WorkingWithLandsatSurfaceReflectanceDatainQGIS_11.png)\n\n데이터의 전체 범위로 경계를 설정하려면 Project ` Import/Export ` Export Map to PDF …(벡터를 유지하기 위함) 또는 Export Map to TIFF …(모든 것을 픽셀로 래스터로 변환하기 위함)를 클릭하세요. 그런 다음 Layer에서 Calculate & drop-down 목록에서 내보내고자 하는 레이어를 선택하세요. QGIS는 선택한 레이어의 크기에 맞는 전체 해상도의 이미지를 렌더링 및 내보내며 지리 위치 정보와 함께 완성됩니다. (적어도 GeoTIFF를 위해, 제 설치에서는 GeoPDF 옵션이 작동하지 않습니다.)\n\n도움이 되었다면, 연락을 유지하고 싶거나, 원격 감지 및 지도 제작에 대한 실습에 관심이 있다면 LinkedIn을 통해 연락하시거나, robertbsimmon@gmail.com으로 이메일을 보내거나 제 포트폴리오를 확인해보세요.\n","ogImage":{"url":"/assets/img/2024-06-19-WorkingWithLandsatSurfaceReflectanceDatainQGIS_0.png"},"coverImage":"/assets/img/2024-06-19-WorkingWithLandsatSurfaceReflectanceDatainQGIS_0.png","tag":["Tech"],"readingTime":8}],"page":"89","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true}