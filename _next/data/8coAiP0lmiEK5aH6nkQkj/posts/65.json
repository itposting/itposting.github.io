{"pageProps":{"posts":[{"title":"기자들을 위한 프롬프팅 기술과 모범 사례","description":"","date":"2024-06-19 19:35","slug":"2024-06-19-PromptingTechniquesandBestPracticesforJournalists","content":"\n\n<img src=\"/assets/img/2024-06-19-PromptingTechniquesandBestPracticesforJournalists_0.png\" />\n\n참고: 이 게시물은 Michael Crystal, Mona Gomaa 및 Nick Diakopoulos와 공동 저술되었습니다.\n\nAssociated Press의 최근 조사에 따르면, 콘텐츠 제작과 관련된 작업에 대한 생성 AI 사용에 대한 주요 관심이 드러났습니다. 이러한 작업으로는 정보 수집, 데이터 분석, 멀티미디어 콘텐츠 제작(예: 이미지 생성), 텍스트 요약, 번역, 그리고 필기가 해당됩니다. 분명히 대형 언어 모델(LLMs)은 언론사가 뉴스 매체를 위해 콘텐츠를 작성하는 방식을 뒤바꿀 수 있지만, 언론인들이 이러한 모델을 다양한 언론 사용 사례에 맞게 적응시키기 위해서는 효과적인 방법이 필요합니다. 이를 위한 한 가지 방법은 뉴스 작업을 위해 모델을 교육하는 것인데, 최근 그중 하나는 비용이 많이 들 수 있다고 주장했습니다. 시작하기 쉬운 다른 접근법은 프롬프트 엔지니어링이지만, 이는 뛰어난 기술과 시간 투자도 필요할 수 있습니다.\n\n본 기사에서는 LLM이 언론에서의 작업에 대한 원하는 출력을 유도하는 데 다양한 프롬프팅 기술을 사용하여 제어하는 방법에 대한 몇 가지 모범 사례를 설명합니다. 우선 프롬프트 엔지니어링에 대한 배경 정보를 제공한 뒤, 뉴스 제작에서 몇 가지 다른 프롬프팅 기술에 대해 자세히 설명하고 관련 예시를 제시합니다. 마지막으로 프롬프트 검증 및 출력 품질 평가에 대한 몇 가지 아이디어를 제시합니다.\n\n<div class=\"content-ad\"></div>\n\n# 프롬프트 엔지니어링이란 무엇인가요?\n\n프롬프트 엔지니어링은 사용자와 LLM(언어 모델 대규모 언어 모델) 간의 부상하는 커뮤니케이션 기술로, LLM에서 원하는 응답을 유도하기 위해 질문과 지침을 만드는 기법입니다. 프롬프트 엔지니어링은 표면상으로는 간단해 보이지만, LLM의 혜택을 완전히 누리기 위해 다양한 프롬프팅 기술에 대한 도메인 전문 지식이 필요합니다. 예를 들어, 이 안내서는 프롬프팅에 대한 17가지 다양한 접근 방식을 나열하고 있으며, 이 중 일부는 상당히 체계적이고 복잡할 수 있습니다. 게다가, 다른 모델은 최상의 성능을 얻기 위해 서로 다른 프롬프트 형식이나 기술이 필요할 수 있습니다.\n\n# 효과적인 프롬프트의 특징\n\nLLM으로부터 더 나은 결과를 얻기 위해서는 모든 프롬프트가 전체 문장으로 표현되어야 하며, 모델이 작업을 완료할 때 따라야 할(때로는 따라서는 안 될) 지침을 명확하게 표현해야 합니다. 게다가, 모델에게 자체 단계나 해결책을 찾도록 지시하여 결론에 이를 때까지 스스로 과정이나 해결책을 찾게 하는 것이, 특히 추론이 필요한 분석 작업에 대한 더 나은 결과 생성 능력을 향상시키는 데 도움이 된다는 것이 입증되었습니다. 또한, 좋은 프롬프트의 또 다른 중요한 특징은 특정 작업에 대한 프롬프트를 범위 설정하는 것입니다. 아주 포괄적이고 열린 작업에 대해 프롬프트하면(예: \"온라인 뉴스 사이트 생성\"), 해당 전체 목표의 작업 및 하위 작업을 개별적으로 다루기 위해 세분화하지 않는 한 효과적이지 않습니다. 모델이 주어진 지침에서 벗어나는 가능성을 최소화하기 위해서, OpenAI의 프롬프팅 안내서는 복잡한 작업을 서로 다른 프롬프트를 가진 하위 작업으로 분할하는 것을 권장합니다.\n\n<div class=\"content-ad\"></div>\n\n일반적으로 좋은 프롬프트는 프롬프트 텍스트와 컨텍스트를 포함한 구조로 구성됩니다 (Joe Amditis의 초보자용 프롬프트 핸드북 3장 참조). 그러나 작업에 따라 이 구조를 확장하여 생성된 텍스트의 길이와 같은 제약 조건, 외부 서비스나 데이터 소스에 액세스하는 도구 (예: Yahoo Finance API) 또는 추가 지시사항 (예: 어조나 글쓰기 스타일 지정)를 포함할 수도 있습니다. \n\n예를 들어 사실에 관한 정보가 필요한 작업의 경우, 프롬프트가 출력물을 근거로 하는 데 도움이 되도록 신뢰할 수 있는 참조 텍스트를 컨텍스트에 포함하는 것이 좋습니다.\n\n책임감 있는 프롬프팅 측면에서 주의할 점 중 하나는 프롬프트 컨텍스트에 기밀 정보나 개인 식별 가능 정보(PII)를 포함할 때 해당 정보가 제3자 AI 플랫폼(예: OpenAI 또는 Anthropic)과 공유되는 경우 조심해야 한다는 것입니다. PII 문제를 해결하기 위해 구체적인 정보를 일반적으로 익명화된 정보(예: \"월터 스미스\"를 프롬프트에서 \"`이름`\"으로 대체)로 대체한 후 서비스로부터 응답을 받은 후 원래의 이름으로 다시 대체할 수 있습니다. 또 다른 접근 방법은 외부 파티와 프롬프트 컨텍스트를 공유하지 않도록 로컬에 호스팅된 모델을 사용하는 것입니다.\n\n일부 모델은 시스템 프롬프트의 사용을 지원하기도 하는데, 이는 시스템이 어조, 스타일 또는 컨텍스트에 포함된 자료에서 그대로 설정된 대로 반응하는 전반적인 무대 설정이 될 수 있습니다. 예를 들어 \"귀하의 응답은 항상 귀하에게 제공된 특정 문서에 기초합니다.\"라는 시스템 프롬프트를 포함하여 모델이 응답을 제공할 때 문맥에 포함된 정보에만 작동하도록 유도할 수 있습니다. 그러나 시스템 프롬프트와 마찬가지로 모델이 항상 지시에 완벽하게 따르지 않을 수 있으므로 출력물을 확인하는 것이 여전히 필요합니다.\n\n다음으로, 이러한 특성들이 다른 프롬프팅 기술에 어떻게 적용될 수 있는지에 대해 논의하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 주요 프롬프팅 기술 개요\n\n## 제로샷 프롬프팅\n\n제로샷 프롬프팅은 모델에게 작업을 완료하도록 지시하지만 작업이나 출력물에 대한 예제나 시연을 제공하지 않고 수행하는 기술입니다 (즉, “제로”는 제로샷에서 제공된 예제의 수가 제로를 의미합니다). 이러한 종류의 프롬프팅은 아이디에이션(예: Figure 1에 설명된 같은 브레인스토밍 인터뷰 질문 작성), 콘텐츠 생성(예: 헤드라인 초안 작성) 및 번역이 필요한 작업에 유용할 수 있습니다. 그러나 이 기술은 원하는 출력물에 도달하기 위해 기본 프롬프트를 반복해서 필요한 세부 정보를 포함해야 한다는 것을 의미합니다. 이는 과연 LLMs의 통계적 성질에 기인하여 작업을 수행하는 동안 모델의 정확한 응답과 형식을 일관되게 예측하기 어렵게 만드는 요소일 수 있습니다.\n\n최신 뉴스 요약 등 사실적인 답변이 필요한 사례에서는 제로샷 프롬프팅이, 다른 맥락 없이 사용되면 모델의 훈련 데이터만을 활용해 응답을 제공한다는 한계가 있습니다. 따라서, 최소한의 지시에 제로샷 프롬프팅을 사용할 때는, 모델이 생성한 출력물이 모델의 훈련 데이터에 포함된 지식만을 반영할 수 있으며 이는 과거의 것일 수 있기 때문에 기자들은 특히 모델의 생성물에 대해 조심해야 합니다. 모델이 생성한 텍스트가 작업과 관련이 있는지 확인하는 한 가지 방법은, 모델에게 해당 발췌문을 기반으로 재미있는 헤드라인을 생성하도록 요청할 때와 같이 프롬프트에 관련 맥락을 포함시키는 것입니다. 때로는 GPT-4o와 같은 최첨단 모델은 제로샷 프롬프팅을 수행하고 이후 인터넷을 둘러다니면서 최신 소스에 접근하여 맥락을 제공할 수도 있습니다(Figures 2A 및 2B).\n\n<div class=\"content-ad\"></div>\n\n\n![Prompting Techniques and Best Practices for Journalists 1](/assets/img/2024-06-19-PromptingTechniquesandBestPracticesforJournalists_1.png)\n\n![Prompting Techniques and Best Practices for Journalists 2](/assets/img/2024-06-19-PromptingTechniquesandBestPracticesforJournalists_2.png)\n\n![Prompting Techniques and Best Practices for Journalists 3](/assets/img/2024-06-19-PromptingTechniquesandBestPracticesforJournalists_3.png)\n\n## Few-shot Prompting\n\n\n<div class=\"content-ad\"></div>\n\n추가적인 세부 정보, 구조, 및 서식화된 응답이 필요한 사용 사례에는 zero-shot prompting 대안으로 몇 가지 예시를 사용하는 것이 도움이 됩니다. Few-shot prompting은 zero-shot prompting을 기반으로 하여 prompt 문맥의 일부로 작업의 예시나 출력 서식을 포함함으로써 발전된 기술입니다 (Figure 3에 설명되어 있음). 이를 통해 모델은 생성된 텍스트에 반영해야 하는 원하는 패턴과 행동을 인식할 수 있습니다. 예를 들어, 기자들은 LLM에 부정적인 및 긍정적인 코멘트의 몇 가지 예시와 해당 레이블을 제공하여 뉴스 기사에 대한 사용자 코멘트를 분류하기 위해 few-shot prompt를 사용할 수 있습니다. 이는 기자들이 교두보 기사에 대한 청중의 반응을 이해하는 쉬운 방법을 갖도록 도울 수 있습니다. 비슷하게, few-shot prompting은 정보 추출(예: 사람이나 장소와 같은 명명된 엔티티 추출)이나 분류(예: 스키마에 따라 콘텐츠 레이블링) 사용 사례에 유용할 수 있습니다. 효과적인 few-shot prompting의 주요 도전 과제는 모델이 과업을 어떻게 완료해야 하는지를 보여주는 예시를 개발하는 것입니다.\n\n## Chain-of-Thought Prompting (CoT)\n\nChain of Thought prompting 기술을 통해 모델은 중간 단계를 통해 복잡한 과제에 대해 \"추론\"할 수 있습니다. 그러나 이는 지금까지 설명한 다른 prompt 기술과 상호 배타적인 기술은 아닙니다. 가장 기본적인 형태에서는 zero-shot 및 few-shot prompt를 포함하여 어떤 prompt에도 \"한 단계씩 생각해 봅시다\"를 추가할 수 있습니다. 이 방법이 작동하는 이유는 과제에 대한 대응 결과를 도와줄 수 있는 문맥을 출력하기 전에 모델이 작업에 착수하기 때문입니다. 이 prompt 기술은 텍스트에서 양적 질문에 답하는 것과 같은 분석적 작업에 유용하며, 이전에 추론이 필요한 복잡한 작업에 대해 더 나은 결과를 얻기 위해 few-shot prompting과 조합될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nFigure 4A와 4B에서 볼 수 있듯이, 제로샷 프롬프팅을 사용하여 모델에게 스텝바이스텝 사고하도록 지시하고 이를 CoT 결과와 결합하면, 동일한 작업에 대해 제로샷 프롬프팅만 사용하는 것보다 더 세밀한 답변이 나옵니다. 중간 출력은 모델이 최종 출력에 도달하는 과정을 이해하기 쉽도록 몇몇 정보를 제공하는 추가적인 이점이 있습니다. 다만, 제3자 모델에 프롬프트하는 API를 사용할 때 유의할 점은, CoT 프롬프팅이 더 많은 (중간) 출력 토큰을 생성하므로 비용이 더 발생할 수 있다는 점입니다.\n\n![PromptingTechniquesandBestPracticesforJournalists_5](/assets/img/2024-06-19-PromptingTechniquesandBestPracticesforJournalists_5.png)\n\n![PromptingTechniquesandBestPracticesforJournalists_6](/assets/img/2024-06-19-PromptingTechniquesandBestPracticesforJournalists_6.png)\n\n## 프롬프팅에서의 효과적인 품질 관리\n\n<div class=\"content-ad\"></div>\n\n프롬프트 품질을 효과적으로 제어하는 데는 두 가지 기본적인 접근 방법이 있습니다: (1) 강건성을 확립하기 위해 미리 프롬프트를 유효성 검사하고, (2) 각종 품질 기준에 따라 작업별로 요구되는 출력을 평가하는 것입니다.\n\n## 프롬프트 유효성 검사\n\n프롬프트를 작성하는 것은 LLMs의 힘에 진입하는 가장 쉬운 방법 중 하나일 수 있지만, 고품질의 출력을 생성할 수 있도록 프롬프트를 다듬는 데는 많은 반복과 시간이 필요할 수 있습니다. 이런 의미에서, 개발 중인 프롬프트의 성능을 평가하기 위한 유효성 검사 프로세스에 대해 생각하는 것이 도움이 될 수 있습니다. 그 프롬프트가 효과적인 프롬프트인지 어떻게 알 수 있을까요?\n\n각 작업을 대상으로 하려는 경우 준비해야 할 두 개의 데이터 세트가 필요한 방식을 권장합니다: 개발 중인 프롬프트의 성능을 평가하기 위한 데이터 세트인 개발 세트와 개발 세트에서 가능한 한 최적화한 후에만 프롬프트를 평가하는 데 사용하는 별도의 테스트 세트입니다. 이상적인 경우 두 데이터 세트 모두 무작위 샘플이므로 어느 쪽에도 기저 선택 편향이 없어야 합니다. 두 세트를 가지고 있는 것은 개발 세트와 잘 작동하도록 개발 및 다듬은 프롬프트가 테스트 세트에도 일반화되어 여전히 잘 작동하는지 이해하는 데 도움이 됩니다. 개발 및 테스트 세트가 크고 다양할수록 프롬프트가 다양한 조건에서 잘 작동하고 있는지 더 자신할 수 있습니다. 다만 주의해야 할 점은 모델을 교체하거나 제공 업체가 모델을 업데이트할 때마다 이 유효성 검사를 다시 실행해야 한다는 것입니다!\n\n<div class=\"content-ad\"></div>\n\n프롬프트 유효성 검사 과정 중 LLMs의 통계적 변동성을 고려하는 것이 중요합니다. 이는 각 프롬프트 반복에 해당하는 텍스트를 생성할 때 고려해야 합니다. 이 블로그 글에 포함된 예시는 ChatGPT 사용자 인터페이스를 이용하여 생성되었지만, 프롬프트를 개발하고 검증할 때 OpenAI의 Playground를 사용할 것을 권장합니다. Playground를 사용하면 이 유효성 검증에 유용한 여러 매개변수를 제어할 수 있습니다.\n\n가장 중요한 매개변수는 온도(temperature)입니다. 이것은 생성된 텍스트의 난수성 vs. 결정론성 수준을 제어합니다. LLMs는 한 번에 하나의 토큰을 생성하는 방식으로 작동하며, 높은 온도는 모델에 다음 토큰을 선택할 여지를 더 많이 줍니다. OpenAI 모델들의 경우, 이 값은 기본적으로 1이며 범위는 0에서 2까지입니다. 프롬프트의 제공된 문맥과 굳이 밀접하게 연결되어야 하는 작업(예: 생성된 요약이 기본 문서를 벗어나지 않도록 보장)의 경우 온도를 0에 가깝게 설정해야 합니다. 반면, 아이디어 발상이나 헤드라인 아이디어화와 같이 더 \"창의적\"이거나 \"자세한\" 텍스트가 필요한 경우, 온도 값을 2에 가깝게 설정하면 동일한 프롬프트에 대해 더 높은 창조적인 텍스트 변형이 나올 것입니다. 물론, 온도가 높을수록 출력물의 변동성도 더 크며, 따라서 유효성을 평가하기 위해 더 많은 시행을 해야 할 것입니다.\n\n프롬프트 유효성 검사는 특정 작업에 제작된 프롬프트를 대규모로 사용하기 전에 고려해야 하는 프롬프트 엔지니어링의 과소평가된 복잡성 중 하나입니다. 동시에 체계적인 방법으로 검증된 후에는 조직 내에서 프롬프트를 공유할 때 보다 자신 있게 할 수 있으며, 뉴스룸의 모든 사람이 자신의 버전을 창출하는 중복된 노력을 줄일 수 있습니다. 또한 프롬프트의 개발 과정을 문서화하고 벤치마크 개발 및 테스트 데이터셋을 설정함으로써, 작업 및 뉴스룸 간에 보다 투명하고 재현 가능하며 확장 가능한 프롬프트 엔지니어링 및 유효성 검사 방법에 기여할 것입니다.\n\n## AI 생성 콘텐츠의 품질 평가하기\n\n<div class=\"content-ad\"></div>\n\n지금쯤 LLM을 사용하는 많은 사람들은 그들이 환각에 빠질 가능성이 있다는 것을 알고 있을 것입니다. 즉, 그들이 출력물을 생성하는 방식의 통계적 특성 때문에 현실에 근거를 두지 않은 정보를 만들어 내는 것입니다. 이러한 점이 저널리즘에서 고려되지 않을 경우, 뉴스의 진실성과 신뢰에 영향을 미칠 수 있습니다.\n\nLLM을 사용하는 작업에 따라 평가 기준이 다를 수 있지만, 일반적으로 우리는 기자들이 정보 정확도와 편향과 관련된 차원을 최소한으로 고려해야 한다고 제안합니다. 여기에는 정치적 및 대표적 편향(과잉 또는 미달된 그룹의 표현)을 포함합니다. 또한 생성된 텍스트의 언어에 대한 추가적인 질적 차원도 평가과정에 고려될 수 있습니다. 가독성(생성된 텍스트에 불완전하거나 명확하지 않은 문장이 있는가?), 세분성(생성된 텍스트의 언어가 얼마나 자세한가?), 적절성(생성된 텍스트가 프롬프트에 설명된 요청과 얼마나 관련이 있는가?)이 있습니다. 물론 이것들은 다양한 기준 중 몇 가지에 불과하며, 여러분이 평가할 수 있는 기준들 중에서 더 많은 작업 특정 기준들을 개발할 것을 장려합니다. 기자들이 기술자나 사회과학자와 협업하여 다양한 뉴스 관련 작업에서 LLM 출력물의 성공을 평가할 수 있는 견고하고 신뢰할 수 있는 평가 지표와 기준을 설정하는 한 가지 방법이 될 수 있습니다.\n\n하지만 우리는 이러한 평가를 일반적으로 더 높은 품질의 출력물을 생성하는 경향이 있는 프롬프트를 보여주는 지표로 추천하더라도, 책임 있는 사용 관점에서 많은 사용 사례에 사람을 계속 참여시키는 것이 현명하다는 것을 강조합니다. 모델의 출력물이 청중에게 직접 제시될 때와 같은 정확도와 같은 차원에서 특히 중요합니다.\n\n# 프롬프트를 더욱 향상하는 방법\n\n<div class=\"content-ad\"></div>\n\nLLM(Large Language Models)의 능력이 계속해서 발전함에 따라, 최신 프롬프팅 기술 및 응용 프로그램에 대해 최신 정보를 유지하기 위해 주기적으로 제3자 모델과 관련된 프롬프트 가이드 및 라이브러리를 확인하는 것을 권장합니다. 아래는 프롬프팅을 위한 추가 참고 자료들로 책갈피해두고 싶은 몇 가지 자원을 나열해봤습니다:\n\n- Prompt Engineering — OpenAI\n- Prompt Examples — OpenAI\n- Intro to prompting — Anthropic\n- Prompt Library — Anthropic\n- Beginner’s prompt handbook: ChatGPT for local news publishers","ogImage":{"url":"/assets/img/2024-06-19-PromptingTechniquesandBestPracticesforJournalists_0.png"},"coverImage":"/assets/img/2024-06-19-PromptingTechniquesandBestPracticesforJournalists_0.png","tag":["Tech"],"readingTime":9},{"title":"다크로드 ChatGPT의 부상","description":"","date":"2024-06-19 19:33","slug":"2024-06-19-TheDarkLordChatgptRising","content":"\n\n<table>\n<tr>\n  <th>한국어</th>\n  <th>영어</th>\n</tr>\n<tr>\n  <td>안녕하세요</td>\n  <td>Hello</td>\n</tr>\n<tr>\n  <td>감사합니다</td>\n  <td>Thank you</td>\n</tr>\n</table>\n\n제가 언어 지원 기능을 이용하여 답변을 도와드릴게요.\n\n테이블 태그를 마크다운 형식으로 변경하려면 이렇게 하시면 됩니다.\n\n안녕하세요! 혹시 더 도와드릴 사항이 있나요?\n\n<div class=\"content-ad\"></div>\n\n제가 확신합니다. 소프트웨어 엔지니어에게는 심각한 영향을 미칠 것이지만, 작가들의 일자리를 없알릴 순 없을 거라고 생각해요.\n\n소프트웨어 산업에서 놀라운 업적을 이룬 사람들과, 과도한 컴퓨터 사용으로 건강이 악화되는 사람들도 알고 있습니다.\n\n![이미지](/assets/img/2024-06-19-TheDarkLordChatgptRising_1.png)\n\n## 스택 오버플로우 안녕~ 🙏\n\n<div class=\"content-ad\"></div>\n\nChatGPT을 훈련하는 데 일부 게임이 사용되었을 것이라고 생각합니다. Open AI 기업이 제품을 훈련하기 위해 Stack Overflow와 거래를 체결했음에도 불구하고, 그 뒤에서 숨바꼭질로 훈련된 것들이 있었던 것 같습니다.\n\n이 웹사이트에 질문에 대한 답변을 한 소프트웨어 엔지니어들의 노력들이 우연히 손에 넘어갔는데, 이 웹사이트는 많은 멋진 정보와 실제 자신을 향상시키는 소프트웨어 개발자들을 위한 많은 데이터를 포함하고 있었습니다. 이 협정 발표 직후, 소프트웨어 개발자들은 자신들이 받은 질문에 대한 답변을 수정하기 시작했습니다. 이것이 Stack Overflow가 새로운 기술을 배우는 개인들과 기업 부문에서 일하는 소프트웨어 엔지니어들에게 사용되는 이유입니다. 그러나 지금 이 웹사이트는 더 이상 활동하지 않고 있습니다.\n\n## 다음 목표는 애플\n\n인공지능 분야에서 Open AI는 빠르게 발전 중에 있습니다. 이 회사의 현재 목표는 애플을 방문하여 제품을 향상시키는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n뉴스에서 정말 큰 재앙이 일어났다니, 정말이지 소름 끼치는 일이야. 전 세계 사람들이 iPhone을 사용하고 있어. 만약 애플이 이 일에 협조하기로 합의한다면, 내 포함 모든 사람들의 정보가 더 이상 안전하지 않을 수도 있어.\n\n이전 의사 소통이, 특히 애플이 합의를 승인할 경우에, 다시 살아날 것 같아.\n\n![이미지](/assets/img/2024-06-19-TheDarkLordChatgptRising_2.png)\n\n## 마지막으로\n\n요즘 기술이 정말 빠르게 발전하고 있지만, 우리는 세계전쟁 상황에 있는 것은 아니야. 제 2차 세계대전의 다큐멘터리와 게임을 보면, 당시 기술이 빠르게 발전하고 있음을 볼 수 있어.\n\n<div class=\"content-ad\"></div>\n\n제게 있어서 Chat Gpt는 지금 새로운 체르노빌입니다.","ogImage":{"url":"/assets/img/2024-06-19-TheDarkLordChatgptRising_0.png"},"coverImage":"/assets/img/2024-06-19-TheDarkLordChatgptRising_0.png","tag":["Tech"],"readingTime":2},{"title":"직장에서 챗지피티를 활용하는 방법들 예시 포함","description":"","date":"2024-06-19 19:31","slug":"2024-06-19-WaysIUseChatGPTatWorkwithexamples","content":"\n\n\n![Image](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_0.png)\n\n나는 ChatGPT를 매일 업무 및 개인적인 질문에 사용합니다. 그것을 내 조수로 생각합니다. 내 대화가 시간 초과되어 잊어버리고 때때로 환각에 빠질 수 있기 때문에 항상 정확하지는 않습니다.\n\n그러나 ChatGPT는 여전히 업무에 유용합니다. Excel과 SQL에 관한 질문에 답변하거나 머릿속이 공허할 때 아이디어를 떠올리는 데 유용합니다.\n\n내가 일할 때 ChatGPT를 사용하는 가장 좋아하는 방법을 확인해보세요!\n\n\n<div class=\"content-ad\"></div>\n\n# 직장에서 ChatGPT를 활용하는 방법\n\n## 1. 엑셀에 관한 질문하기\n\nChatGPT를 사용하는 가장 흔한 방법은 엑셀을 배우는 데 활용하는 것입니다.\n\nChatGPT에게 엑셀 오류를 해결하거나, 엑셀 수식을 설명하거나, 어떤 수식을 사용해야 하는지 알아보기 위해 질문합니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, 제가 ChatGPT에 COUNT()와 COUNTA() 공식의 차이를 물어봤어요.\n\n제 ChatGPT 프롬프트: \"엑셀에서 COUNT 공식과 COUNTA 공식의 차이는 뭔가요?\"\n\n![이미지 1](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_1.png)\n\n![이미지 2](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_2.png)\n\n<div class=\"content-ad\"></div>\n\n어떤 ChatGPT 출력물이든 Excel의 기본 원칙을 이해하고 필요한 경우 수정할 수 있어야 합니다. 또한 Excel에서 수식을 테스트하여 ChatGPT가 예상하는 대로 작동하는지 확인해야 합니다.\n\n## 2. 기사 아이디어를 떠올리기\n\n머릿속이 텅 비어 있지만 이메일이나 기사, 예정된 행사를 위한 제목을 작성해야 할 때, 빈 페이지를 바라보는 것보다 ChatGPT를 사용하는 것이 나을 수 있습니다. 모든 ChatGPT 아이디어가 좋은 것은 아니므로 주의 깊게 선택하세요!\n\n제 ChatGPT 프롬프트: \"열정적인 데이터 분석가가 Excel을 사용하는 방법을 배울 수 있도록 돕기 위해 기사를 쓰고 싶습니다. 기사 아이디어를 생성하는 데 도와줄 수 있나요?\"\n\n<div class=\"content-ad\"></div>\n\n아래는 Markdown 형식으로 변경한 표입니다.\n\n\n<img src=\"/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_3.png\" />\n\n<img src=\"/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_4.png\" />\n\n## 3. 기사 콘텐츠 브레인스토밍.\n\nChatGPT를 사용하여 기사 아이디어를 생성했다면, 이제 콘텐츠가 필요합니다. 다행히도 ChatGPT는 논문 개요를 작성할 수 있습니다!\n\n\n<div class=\"content-ad\"></div>\n\n제 ChatGPT 프롬프트: \"엑셀 피벗 테이블을 사용하는 방법에 대해 데이터 분석가들에게 기사를 쓴다면 무엇에 초점을 맞춰야 할까요? 이는 엑셀 피벗 테이블에 경험이 없는 초보자를 대상으로 합니다.\"\n\n![이미지1](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_5.png)\n\n![이미지2](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_6.png)\n\nChatGPT도 충고했습니다:\n\n<div class=\"content-ad\"></div>\n\n모두를 고려해 보면, 이 글 개요는 좋은 것 같아요. 하지만 아직은 일반적이고 상위 레벨입니다. ChatGPT와 왔다갔다하여 구체적인 팁과 무엇을 포함해야 하는 실제 예시를 얻는 것이 좋을 것 같아요.\n\n그리고 이 모든 정보를 간단한 Medium 기사에 모두 포함시키기는 불가능할 것 같아요. 너무 길어지겠죠.\n\nChatGPT의 모든 결과물처럼 비판적인 시각으로 검토해 보세요. 만약 저가 이 글을 쓴다면, 피벗 테이블 소개, 데이터 준비, 그리고 피벗 테이블 생성과 같은 처음 몇 가지 항목을 포함할 것 같아요.\n\n## 4. 단순 SQL 코드를 교정하고 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n저는 SQL 문법을 확인하고 SQL 오류를 해결하며 SQL 코드에 주석을 추가하기 위해 ChatGPT를 사용합니다.\n\n예를 들어, SQL 코드가 예상대로 작동하지 않는 경우 아래와 같이 프롬프트를 입력할 수 있습니다.\n\n제 ChatGPT 프롬프트: “이 코드가 작동하지 않는 이유는 무엇일까요? SELECT*\n\nWHERE[First Name] = “Dawson”\n\n<div class=\"content-ad\"></div>\n\n```sql\nFROM table;”\n\n<img src=\"/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_7.png\" />\n\nChatGPT을 사용하여 간단한 SQL 코드를 생성하기도 합니다. 문제를 해결하려는 질문에 관련된 열과 컨텍스트를 포함시킵니다.\n\nChatGPT 프롬프트: \"테이블을 base_salary_final_amount를 기준으로 오름차순으로 정렬하려면 올바른 SQL 구문은 무엇인가요?\"\n```\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_8.png\" />\n\nSQL을 사용할 때 주의해야 할 몇 가지 사항:\n\n- 다른 프로그래밍 언어에 있는데 SQL에는 없는 SQL 함수를 만들지 않도록 주의하세요.\n- SQL은 표준이지만 MySQL, SQL Server, Postgres와 같이 다양한 “플레이버”가 있습니다. 필요한 SQL “플레이버”인지 확인해 주세요.\n- 다른 기술과 마찬가지로 AI를 사용할 때는 SQL 기초를 이해해야 합니다. ChatGPT의 출력이 정확한지 확인할 수 있도록 합니다.\n\n## 5. 비기술 직원이 이해할 수 있는 방식으로 기술적 개념을 요약하는 데 사용합니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 분석가로서, 종종 기술적인 개념을 기술 지식이 없는 사람들에게 설명해야 합니다. 효과적으로 이를 하기 위해서는 개념을 단순화하고 최소한의 전문 용어를 사용해야 합니다.\n\n예를 들어, 한번 제가 기술 지식이 없는 동료가 90백분위를 보고 싶어했는데, 제가 90백분위 수치를 제공했을 때, 그들은 혼란스러워 했습니다. 그래서 \"이 숫자가 무엇을 의미하는가요?\"라고 물어왔죠.\n\n저는 그들에게 90백분위에 대해 ChatGPT를 사용하여 설명했어요.\n\nChatGPT 프롬프트 문구: \"급여가 90백분위에 있다는 것은 무엇을 의미하나요? 기술 지식이 없는 사람도 이해할 수 있는 용어로 설명해주세요.\"\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_9.png)\n\nChatGPT가 말합니다: “급여가 90 백분위에 있다면, 그 급여 값은 특정 데이터 집합 내의 급여 중 90% 이상보다 크거나 같다는 것을 의미합니다. 다시 말해, 이는 해당 그룹의 사람들 중 90%의 급여보다 높다는 것을 나타냅니다.”\n\n이제 이 90 백분위에 대한 간단한 설명으로 동료에게 이메일을 보낼 수 있습니다. 더 많은 맥락이 필요하다면, ChatGPT에게 예제를 요청할 수도 있습니다.\n\n## 6. 구글 검색처럼 기본적인 질문을 하려면.\n\n\n<div class=\"content-ad\"></div>\n\nChatGPT를 구글 검색 엔진처럼 사용해보세요. 결과가 완벽하거나 최신 정보일 필요가 없는 경우, ChatGPT는 일반적인 지식을 제공합니다.\n\n나의 ChatGPT 프롬프트: \"직장에서 책상에서 할 수 있는 좋은 스트레칭은 무엇인가요? 구체적인 예제 목록을 제공해주세요.\"\n\n![이미지](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_10.png)\n\n![이미지](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_11.png)\n\n<div class=\"content-ad\"></div>\n\nChatGPT는 내가 일중에 할 수 있는 7가지 스트레칭을 소개했어. 목 스트레칭, 어깨와 상체 스트레칭, 가슴을 펴는 운동, 발목 동그라미 그리기, 그리고 엉덩이와 다리 스트레칭까지 모두 내 책상에서 쉽게 할 수 있을 거야!\n\n## 7. 사무실 예절에 관한 일반 질문하기\n\nChatGPT에게 어떻게 하면 일 중에 발생하는 문제를 다루는지, 회의를 계획적으로 유지하거나 상사와의 의사 소통 방식을 논의하는 방법을 물어보았어.\n\nChatGPT에게 내 상황을 설명하고 물었던 내 문장: \"나는 데이터 분석가로 일하고 있고, 주간 1:1 미팅이 상사와 있어. 때로는 미팅이 제대로 계획한 목적과 아이템에서 벗어날 때가 있어. 이 미팅이 생산적으로 유지되고 주요 일정에 집중할 수 있는 방법이 있을까?\"\n\n<div class=\"content-ad\"></div>\n\n\n![image 1](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_12.png)\n\n![image 2](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_13.png)\n\nChatGPT가 제 회의 생산성을 높이기 위한 일반적인 지침을 제공했습니다. 이에는 회의 전에 매니저와 안건을 공유하거나 각 항목에 대한 구체적인 시간대를 설정하고 토론을 집중하게 하기 위해 차트, 그래프 또는 슬라이드와 같은 시각 보조 도구를 사용하는 것이 포함됩니다.\n\n제 ChatGPT 프롬프트: \"사무실 업무를 위해 상사에게 그들의 선호하는 의사 소통 스타일에 대해 물어볼 수 있는 몇 가지 방법이 무엇인가요?\"\n\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_14.png)\n\n![Image 2](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_15.png)\n\nChatGPT gave suggestions that hadn’t occurred to me, like discussing frequency and timing and clarifying times/dates that are best for discussion.\n\n## 8. To ask for time productivity tips.\n\n\n<div class=\"content-ad\"></div>\n\n가끔은 업무 프로젝트에 집중하기 어려워하거나 제때 완료하는 데 어려움을 겪곤 해요. ChatGPT가 제 생산성을 향상시키는 방법에 대해 제안해줄 수 있어요.\n\n제 ChatGPT 프롬프트: \"데이터 분석가로 일할 때 시간 생산성을 극대화하는 데 도움이 되는 팁이 무엇인가요? 가능한 한 구체적이고 실용적으로 알려주세요. 시간 생산성을 향상시키는 방법 목록을 제시해주세요.\"\n\n![16](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_16.png)\n\n![17](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_17.png)\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_18.png)\n\nChatGPT가 제게 일할 때 생산성을 높이는 방법을 나열했어요. 특히, 다음과 같은 것들이 도움이 되었어요: 가능한 경우 회의를 최소화하고 동료에게 간결한 이메일을 보내달라고 요청하는 것, 작업 사이에 짧은 휴식을 가지고 에너지를 충전하는 것, 그리고 업무를 위임하는 것입니다.\n\nChatGPT의 초기 제안 목록이 마음에 들지 않는다면 \"시간 관리를 위한 Pomodoro 기술이 마음에 들지 않아요. 대신 사용할 다른 시간 관리 방법이 있을까요?\" 또는 \"직장에서 회의를 최소화하는 구체적인 방법 중 일부는 무엇인가요?\"와 같은 프롬프트를 사용해서 말해볼 수 있어요.\n\n## 9. IT 책상으로부터 이메일을 해석하는 방법\n\n\n<div class=\"content-ad\"></div>\n\n가끔 IT 팀에서 온 이메일을 이해하기 어려울 때가 있어요. 이유는 제가 모르는 용어가 들어가 있거나 이상하게 표현된 경우일 수 있죠. 그럴 때면 이메일에서 텍스트를 복사하여 ChatGPT에 붙여넣고 Layperson을 위해 이 언어를 간단하게 설명해달라고 요청해요 (단, 정보가 기밀이 아닌 경우에 한해요).\n\n내 ChatGPT 프롬프트 : \"비전문가를 위해 이 언어를 단순화할 수 있나요? 명언 언어입니다.\"\n\n![이미지](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_19.png)\n\n## 10. 단어나 구문을 알파벳순으로 나열하기.\n\n<div class=\"content-ad\"></div>\n\n\"ChatGPT에 요청해서 단어, 구절 또는 이름을 알파벳 순으로 나열해 주세요.\"\n\n제 ChatGPT 프롬프트: \"[빈칸에 나열할 목록 삽입]을(를) 알파벳 순으로 정렬하세요.\"\n\n\n![이미지](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_20.png)\n\n![이미지](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_21.png)\n\n\n<div class=\"content-ad\"></div>\n\nChatGPT의 모든 출력물을 확인하는 것이 중요합니다. 여기에는 하나의 오류만 있는 것 같아요: ChatGPT가 \"accordingly\"라는 단어를 두 번 반복했네요.\n\n## 11. 텍스트 요약\n\n회의나 이메일 스레드에서 노트를 요약하려면, 정보를 ChatGPT에 복사하여 붙여넣고 요약을 요청하세요 (정보가 기밀적이지 않은 경우를 가정합니다).\n\n내 ChatGPT 프롬프트: \"이메일 스레드를 요약해 줄 수 있나요? [그럼 이메일을 복사하여 붙여넣으세요]\"\n\n<div class=\"content-ad\"></div>\n\n이 표를 마크다운 형식으로 변경해주세요.\n\n<div class=\"content-ad\"></div>\n\n아웃룩 문의가 있을 때는 ChatGPT에게 물어보고 그 응답을 시험해봐요. 예를 들어, 나중에 응답할 이메일을 받았을 때 이를 기억하기 위해 아웃룩 캘린더 이벤트를 만드는 방법을 ChatGPT에게 물어봤어요.\n\nChatGPT 질의: \"아웃룩에서 자신을 위한 이메일을 캘린더 이벤트로 설정하는 방법이 있나요? 예를 들어, 특정 시간에 응답할 것을 기억하고 싶은 경우.\"\n\n![이미지](/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_24.png)\n\nChatGPT 솔루션을 시험해보았고 잘 작동했어요!\n\n<div class=\"content-ad\"></div>\n\n## 13. 항목들을 논리적인 그룹으로 분류합니다.\n\n제가 정렬해야 하는 항목 목록이 종종 있습니다만, 어떻게 해야할지 알지 못할 때가 있습니다. 예를 들어, Excel 키보드 단축키에 관한 기사를 쓰고 있었는데, 단축키 목록이 있었습니다: CTRL C는 \"복사\", CTRL A는 \"전체 데이터 세트 선택\", CTRL 1은 셀 서식 대화상자를 엽니다. \n\n제 기사에 맞는 Excel 키보드 단축키를 그룹화하고 싶었기 때문에, 단축키를 분류하도록 ChatGPT에 목록을 복사하여 붙여넣었습니다.\n\n내 ChatGPT 프롬프트: \"Excel 키보드 단축키에 관한 기사를 쓰고 있습니다. 논리적인 범주로 단축키를 그룹화하고 싶습니다. 이러한 단축키를 논리적인 그룹으로 분류할 수 있나요? [목록 삽입]\"\n\n<div class=\"content-ad\"></div>\n\nChatGPT가 키보드 단축키를 다음과 같은 카테고리로 정리했어요: 내비게이션 단축키, 선택 단축키, 편집 단축키, 서식 단축키, 데이터 단축키, 워크시트 관리 단축키, 그리고 검색 및 대체 단축키.\n\n이걸로 시간과 에너지를 많이 절약했어요!\n\n이것은 발표를 준비하는 데이터 분석가들에게도 유용해요. 무작위 데이터 포인트를 파워포인트 슬라이드에 뿌리기보다는 ChatGPT와 대화를 나누어 데이터 포인트를 논리적으로 나열해주세요.\n\n## 최종 의견\n\n<div class=\"content-ad\"></div>\n\nChatGPT은 구글을 사용하기 싫을 때 좋은 검색 엔진입니다. 게다가 앞뒤로 소통하고 답변을 맞춤화할 수 있어요.\n\n하지만 ChatGPT가 데이터 분석가의 업무를 대체할까봐 걱정된다면 안심하세요! 인공지능은 도메인 지식이나 데이터의 세심한 뉘앙스를 완전히 이해하는 단계에는 아닙니다.\n\nChatGPT가 업무를 더 쉽게 만들어 줄 수는 있지만, 여전히 데이터 분석의 기본기를 알아야 해요: 데이터를 저장하고 검색하고 수집하고 변환하는 방법을 알아야 해요. 정확성을 확인하기 위해 이러한 기본적인 기술이 필요해요.\n\nChatGPT의 모든 출력물과 마찬가지로 판단의 조심이 필요해요, 왜냐하면 ChatGPT는 \"환각\" 또는 정보를 만들어 내기도 해서요.\n\n<div class=\"content-ad\"></div>\n\n더 많은 정보를 원하시면 Maven Analytics의 ChatGPT 및 데이터 분석 코스를 확인해보세요. 해당 코스에서는 ChatGPT를 엑셀, 구글 스프레드시트, Power BI, SQL 및 Python에서 어떻게 활용하는지에 대한 팁을 제공합니다.\n\n또한 Maven Analytics의 \"Mavens of Data\" 인터뷰 시리즈 중 Alex the Analyst와의 “The Future of AI in Data”를 추천합니다. Alex는 업무에서 AI를 활용하는 방법과 현재의 한계에 대해 훌륭한 조언을 해주었습니다.\n\n추가 정보:\n\nMaven Analytics: ChatGPT 및 데이터 분석 코스\n\n<div class=\"content-ad\"></div>\n\nMaven Analytics: \"데이터에서 AI의 미래\" Alex the Analyst와의 인터뷰\n\n내 Medium 기사: SQL 스킬 향상을 위해 ChatGPT를 활용하는 5가지 방법\n\n내 Medium 기사: Excel 닌자가 되기 위해 ChatGPT를 활용하는 상위 5가지 방법\n\n내 Medium 기사: 장벽을 허물어라: 고객에게 데이터를 설명하는 데 ChatGPT 사용하기\n\n<div class=\"content-ad\"></div>\n\n외부 제출물의 내용은 Maven Analytics나 팀 구성원의 의견이나 업무를 반영하는 것은 아닐 수 있습니다.\n\n우리는 일생일대 학습을 촉진하는 것을 믿고 있으며, 데이터 커뮤니티가 자신의 작업을 공유하고 Maven Analytics 데이터 팸으로부터 피드백을 받을 수 있는 플랫폼을 제공하는 것이 목적입니다.\n\n기여하고 싶은 분은 여기에서 자신의 글을 제출해 주세요.\n\n즐거운 학습되세요!\n\n<div class=\"content-ad\"></div>\n\n- 팀 메이븐","ogImage":{"url":"/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_0.png"},"coverImage":"/assets/img/2024-06-19-WaysIUseChatGPTatWorkwithexamples_0.png","tag":["Tech"],"readingTime":10},{"title":"신서사이저 효과","description":"","date":"2024-06-19 19:28","slug":"2024-06-19-TheSynthesizerEffect","content":"\n\n또는, 왜 생성적 AI가 모두도 일부 시간을 속여 거짓말을 하고, 모두도 아예 거짓말을 하지 않는지입니다.\n\n![이미지](/assets/img/2024-06-19-TheSynthesizerEffect_0.png)\n\n\"AI\" 언어 생성기(LLM)는 웹의 흐린 근사값을 제시한다는 점에 대해 소개되었습니다. 이는 JPEG나 MP3와 같은 압축 알고리즘과 유사하게 만들어진 것입니다. 모든 압축은 정보를 버리고 소음을 도입함으로써 작동합니다. 성공하는 방안은 당신이 관심 없는 정보를 버리고, 당신이 신경 쓰지 않을만큼 충분히 가까운 근사값을 제시하는 것입니다.\n\n그렇다면 얼마나 가까워야 신경 쓰지 않을 정도일까요? 이는 어떤 도메인에 대해 얼마나 많이 알고 있는지에 따라 매우 다르다는 것 같습니다. 이것이 뭔가를 생각나게 합니다.\n\n<div class=\"content-ad\"></div>\n\n저는 초등학교 밴드에서 색소폰을 연주했어요. 그리고 의외로 예산이 높은 전자 음악 스튜디오를 갖춘 작은 대학에 다녀왔어요. 70년대부터 최신 기술의 악기까지 다양한 악기들이 있었어요. 이 중 많은 악기들은 스스로 말 그대로 소리가 나도록 설계되었지만, 대부분의 경우 신디사이저는 이미 알고 있는 악기들을 시뮬레이트하기 위해 최선을 다해요. 키보드로 연주하는 피아노가 가장 많이 사용되지만, 여기에는 피아노뿐만 아니라 플루트, 트럼본, 마림바, 스틸 드럼, 관음악, 그리고 상상할 수 있는 모든 것에 대한 모델(여기서는 패치라고 부름)이 있어요.\n\n제가 발견한 것은 대부분의 악기들이 완벽하게 들렸다는 거예요. 만약 같은 사람이 신디사이저와 실제 피아노를 녹음한다면, 어느 것이 어느 것인지 구분하기 어려울 것 같아요. 이건 색소폰을 제외한 모든 악기에 해당되었어요. 색소폰은 망가져 들렸어요. 다른 모든 것에 대해 좋은 패치를 만들었지만, 색소폰만 업데이트를 잊어먹은 것 같았어요.\n\n수업에서 모두가 같은 경험을 한다는 사실이 밝혀졌어요. 고등학교에서 플루트를 연주했다면, 색소폰 패치는 색소폰처럼 들리고 플루트 패치는 쓰레기처럼 들린다는 거예요. 다른 악기들도 마찬가지에요. 특정한 것을 오랜 시간 듣은 후에는 그 연습이 없는 사람들과는 달리 비슷한 소리를 다르게 듣는다는 것 같아요. 아마도 프로 오케스트라 지휘자라면, 모든 패치가 쓰레기처럼 들릴 것 같아요.\n\n![Synthesizer Effect](/assets/img/2024-06-19-TheSynthesizerEffect_1.png)\n\n<div class=\"content-ad\"></div>\n\n흥미로운 점은 이제 멍청한 표현의 20년이 지났다는 점인데, 기술의 많은 세대가 지나갔음에도 경험은 변하지 않았습니다. 피아노 전문가가 아닌 사람에게는 현재 합성된 피아노 소리가 완벽하게 잘 들립니다. 20년 전과 마찬가지로요. 피아노 전문가에게는 롤랜드 JV-1080보다 조금 더 나은 소리일지도 모르지만, 결국 피아노를 대체할 수는 없습니다.\n\n언어 모델도 같은 현상이 벌어지고 있다고 생각해요. 저는 컴퓨터 프로그램을 쓸 정도로 능력이 있어서, Github Copilot과 같은 코드 생성기의 결과물을 보면 쓰레기를 보여줘요. 이상한 버그도 있고, 이상한 스타일도 있으며, 갑자기 함수 중간에서 산만해져서 전혀 상관없는 내용을 써 버리기도 해요. 문제를 해결하는 것이 이 코드를 직접 작성하는 것보다 더 많은 작업을 필요로 할 겁니다. 코드 리뷰는 익숙한 문제인데, 시니어 엔지니어들은 그것을 항상 해야 하고, 대부분 불만스럽습니다.\n\n그런데 제 학생들이 그걸 사용하면요. 그들은 컴퓨터 프로그램을 쓰는 데 능력이 없으며, Copilot 출력물을 보면 전혀 다른 것을 보여줘요. 명백한 문제들이 명백하지 않고, 학생들에게는 처음부터 작성하는 것이 매우 어려울 수 있어요. 당신의 대안이라면, 일일이 확인을 해야 하는 가파른 암벽 등반의 힘든 싸움일 수도 있어요. 사실 그것이 이미 학생들이 하는 일이고요, 구글 검색 결과의 처음 몇 페이지에서 코드를 복사해오는 것입니다.\n\n이것이 바로 \"합성기 효과\"라는 말의 의미야: 아주 잘 알고 있다면, 기계는 당신을 속일 수 없어요. 모르면, 아주 속일 수도 있어요.\n\n<div class=\"content-ad\"></div>\n\n실제로 꽤 많은 사람들이 합성기 효과로 LLM에 대한 너무 과대한 기대를 갖고 있어서 혹은 흥분하거나 패닉에 빠지는 것 같아요. CEO들은 LLM이 엔지니어를 대체할 수 있다고 생각해요. 엔지니어들은 LLM이 세일즈 직원을 대체할 수 있다고 생각해요. 세일즈 직원들은 LLM이 CEO를 대체할 수 있다고 생각해요. 예술가들은 LLM이 다른 모든 사람을 대체할 수 있다고 생각하고, 그 외 모든 사람은 LLM이 예술가를 대체할 수 있다고 생각해요. 어느 정도 이것은 우리의 오래된 친구 Dunning과 Krueger와 관련이 있어요. X를 하기에 능숙하지 않은 사람은 기계가 생성한 X의 품질을 판단할 능력도 없는 것이죠.\n\n이 관점에서 의사나 변호사들이 때때로 자신을 대체하기 위해 LLM을 사용하려고 할 때 조금 걱정스러운 부분이 있어요. 만약 이 말이 이해가 된다면, 저라면 당신의 의사나 변호사가 되길 원하지 않겠어요, ChatGPT와 함께든 그렇지 않든요.\n\n- 제가 정확히 어떤 고급 합성기를 가지고 있었는지 기억은 안나지만, 아마도 Roland JV-1080이었을 거에요.\n- 학생들: 상처받지 마세요. 누구나 컴퓨터 프로그램을 작성하는 데 능숙하지 않죠. 그래서 여러분은 학생이에요.\n- 실제로 \"학생들\"이라고 말하고 싶은 건 예의를 갖추기 위해서예요, 하지만 많은 워킹 프로그래머들에게도 비슷한 상황이 맞아요.\n- 예를 들어: 2023년 12월 15일 블룸버그 뉴스: AI 실수들이 모두에게 교훈을 줍니다.\n\n작가의 노트: 이 합성기 부분은 실제로 제가 비공개 뉴스레터에 발표한 AI에 대한 긴 선언서에서 요청에 따라 적응한 것이에요. 만약 (a) 실생활에서 저를 알고 있고, (b) 무슨 비공개 뉴스레터인지 모른다면, 그리고 (c) 받고 싶다면, 저에게 물어보세요.","ogImage":{"url":"/assets/img/2024-06-19-TheSynthesizerEffect_0.png"},"coverImage":"/assets/img/2024-06-19-TheSynthesizerEffect_0.png","tag":["Tech"],"readingTime":3},{"title":"간단 리뷰 - 새로운 약물 복용 전 환자가 질문해야 하는 내용을 대답하는 챗봇의 정확도","description":"","date":"2024-06-19 19:27","slug":"2024-06-19-BriefReviewAccuracyofaChatbotinAnsweringQuestionsthatPatientsShouldAskBeforeTakingaNewMedication","content":"\n\n## 환자 약물에 대한 ChatGPT\n\n![이미지](/assets/img/2024-06-19-BriefReviewAccuracyofaChatbotinAnsweringQuestionsthatPatientsShouldAskBeforeTakingaNewMedication_0.png)\n\n- 저는 이 논문을 프로젝트 리더가 최근 공유하여 살펴보라고 했기 때문에 이 논문을 접하게 되었습니다.\n- 이 논문에서 저자들은 ChatGPT(챗봇)가 새로운 약을 복용하기 전 환자가 물어야 하는 질문에 대한 답변의 정확성을 평가했습니다.\n\n# 개요\n\n<div class=\"content-ad\"></div>\n\n- 환자 약 ChatGPT\n- 결과\n\n# 1. 환자 약 ChatGPT\n\n## 1.1. 방법\n\n![이미지](/assets/img/2024-06-19-BriefReviewAccuracyofaChatbotinAnsweringQuestionsthatPatientsShouldAskBeforeTakingaNewMedication_1.png)\n\n<div class=\"content-ad\"></div>\n\n- 기관인 건강 관리 연구 및 품질 기관(AHRQ)에서 얻은 12개의 질문이 상위 20개 약물에 대해 ChatGPT라는 챗봇에게 제출되었습니다.\n- 아래에는 문제가 기록된 상위 20개의 약물이 나와 있습니다:\n\n![image](/assets/img/2024-06-19-BriefReviewAccuracyofaChatbotinAnsweringQuestionsthatPatientsShouldAskBeforeTakingaNewMedication_2.png)\n\n## 1.2. 정확성 및 완전성\n\n- 2명의 리뷰어가 각 응답을 정확성에 대해 6점 척도로, 완전성에 대해 3점 척도로 독립적으로 평가하고 평가했습니다. 점수가 2점 이상인 것이 적절하다고 간주되었습니다.\n- 6점 정확성 척도(1 = 완전히 부정확; 2 = 올바르보다 부정확; 3 = 대체로 올바르고 부정확; 4 = 부정확보다 올바른; 5 = 거의 모두 올바른; 그리고 6 = 완전히 정확) 정확성은 임상 전문 지식과 약물 정보 데이터베이스를 사용하여 결정되었습니다.\n- 3점 완전성 척도(1 = 불완전 [질문의 일부 측면을 다루지만 중요한 부분이 미실거나 미완료]; 2 = 충분함 [질문의 모든 측면을 다루며 완전한 것으로 간주되기에 필요한 최소한의 정보를 제공]; 3 = 포괄적 [질문의 모든 측면을 다루며 예상외의 추가 정보나 맥락을 제공]). \n- 독립적인 리뷰 후, 2명의 리뷰어가 답변을 비교하고 잘못된 점을 논의하고 정확성과 완전성에 대한 공동의 점수를 할당하도록 만났습니다.\n\n<div class=\"content-ad\"></div>\n\n## 1.3. 재현성\n\n- 재현성을 평가하기 위해, 정확도가 6 미만으로 점수 매겨진 응답들이 14일 후에 재평가되었습니다. 그들은 정확도가 향상되거나 변화 없거나 정확도가 낮아질 수 있습니다.\n\n# 2. 결과\n\n## 2.1. 정확도\n\n<div class=\"content-ad\"></div>\n\n## 2.2. 완성도\n\n## 2.3. 재현성\n\n- 초기 쿼리와 반복 쿼리에서 중앙값 정확도 점수는 각각 5 (IQR 5-4) 및 4.5 (IQR 6-2)였습니다 (p = 0.64).\n\n## 2.4. 토론 및 제한 사항\n\n<div class=\"content-ad\"></div>\n\n- 약사들은 약물에 대한 가장 중요한 측면을 환자들에게 상담하는 독특하게 훈련을 받았어요. ChatGPT가 요구하는 것처럼 환자가 제대로 물어야만 상담 포인트를 다루어야 하는 필수 요건은 없어요.\n\n- 조사원이 두 명뿐이기 때문에 개인적인 편견과 주관적 해석의 위험이 있어요.\n- 이 연구의 추가적인 한계는 영어만을 사용하여 실시되었다는 점이에요.\n- 환자들이 일반적으로 묻는 질문에 답변하기 위해 챗봇을 활용하면 대부분 정확하지만 환자들에게 중요한 정보를 포함하거나 부족할 수 있어요. 환자들을 교육하는 것이 중요해요.","ogImage":{"url":"/assets/img/2024-06-19-BriefReviewAccuracyofaChatbotinAnsweringQuestionsthatPatientsShouldAskBeforeTakingaNewMedication_0.png"},"coverImage":"/assets/img/2024-06-19-BriefReviewAccuracyofaChatbotinAnsweringQuestionsthatPatientsShouldAskBeforeTakingaNewMedication_0.png","tag":["Tech"],"readingTime":2},{"title":"디자이너를 위한 GPT-4o의 의미","description":"","date":"2024-06-19 19:26","slug":"2024-06-19-WhatGPT-4omeansfordesigners","content":"\n\n<img src=\"/assets/img/2024-06-19-WhatGPT-4omeansfordesigners_0.png\" />\n\n월요일 좋은 하루 되세요! ☀️\n\n정확히 일주일 전, OpenAI가 GPT-4o를 발표했습니다. 저는 OpenAI가 공개한 라이브 이벤트와 데모를 시청했습니다.\n\n실시간 오디오 및 비디오와 같은 대부분의 업데이트된 기능이 아직 출시되지 않았지만, GPT-4o의 잠재력에 대한 초기 인상에 대해 이 기사에서 이야기하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 1. 다중 모달 기능은 다양한 흥미로운 응용 프로그램을 열어줍니다.\n\n\"GPT-4o\"의 \"o\"는 \"옴니\"를 의미하는 라틴어인 \"all\"의 약자입니다. 이는 텍스트, 오디오 및 이미지 입력의 모든 조합을 수용하고 텍스트, 오디오 및 이미지 출력의 모든 조합을 생성합니다.\n\n들리는 것과 보는 것을 처리하는 속도와 품질은 GPT-4보다 상당히 향상되었습니다. GPT-4에서 오디오-to-텍스트, 텍스트-to-텍스트 및 텍스트-to-오디오는 세 개의 별도 모델이었습니다. 이들 사이에 불가피한 지연이 있었고, 특정 정보가 손실될 수도 있었습니다.\n\nGPT-4o는 텍스트, 비전 및 오디오를 연결하는 새롭게 훈련된 단일 모델로, 모든 입력과 출력이 동일한 신경망에 의해 처리됩니다. 심지어 톤, 여러 화자 및 배경 소음도 감지할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 효율적인 다중 모달 상호 작용은 이전에는 불가능했던 몇몇 디자이너들의 사용 사례를 떠올리게 했어요.\n\n## 시나리오 1 — 사용자 인터뷰 분석\n\n[동영상 보기](https://www.youtube.com/watch?v=kkIAeMqASaY)\n\n이 데모 비디오를 보면서, 제가 과거에 진행한 사용자 인터뷰를 떠올리게 되었어요.\n\n<div class=\"content-ad\"></div>\n\nChatGPT(이하 GPT-4o 모델)가 실시간 사용자 인터뷰에서 회의 보조로 작동할 수 있다면 새로운 세계가 열릴 것입니다. 이는 노트를 작성하는 것뿐만 아니라 얼굴 표정과 억양과 같은 섬세한 것들을 감지할 수 있기 때문입니다. 이러한 비언어적 정보는 귀중합니다.\n\n예를 들어, 사용자 테스트 녹화를 여러 번 다시 시청하여 누가 무엇을 말했는지와 그 방식을 캡처하는 데 시간을 낭비할 필요가 없을 것입니다. 또한 감지된 감정은 면접 보고서의 일부로 포함될 수 있습니다.\n\n## 시나리오 2 - 디자인 리뷰\n\n마찬가지로, 사용자 인터뷰에서 GPT-4o가 시각 및 음성을 처리하고 실시간 피드백을 제공할 수 있다면, 디자인 리뷰에서도 협력자로 작동할 수 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n만약 ChatGPT가 디자이너 간의 대화를 엿들을 수 있고 디자인을 관찰할 수 있다면 얼마나 좋을까요? ChatGPT는 실시간으로 디자인 피드백을 제공하고 대화 내용에서 주요 포인트를 요약할 수 있을 것입니다.\n\n## 시나리오 3 — 실시간 디자인 지원\n\n[여기](https://www.youtube.com/watch?v=_nSmkyDNulk)에서 시연 영상을 확인할 수 있습니다. 이 데모 비디오는 ChatGPT와 GPT-4o 모델을 통해 시각적인 정보를 처리하고 사용자와 실시간 대화를 할 수 있는 능력을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n와우, 만약 ChatGPT가 제가 디자인하는 것을 지켜보고 조언을 해줄 수 있다면 정말 놀라울 것 같아요. 예를 들어, 제가 Figma에서 와이어프레임을 만들 때 ChatGPT가 실시간 대화를 하면서 제게 제안하고 조언을 해줄 수 있다면 좋겠죠.\n\n# 2. 피드백 속도가 훨씬 빨라집니다.\n\n비전 및 오디오 기능은 아직 GPT-4o에는 출시되지 않았지만, 적어도 텍스트 대 텍스트 모델은 제 계정에서 업데이트되었습니다.\n\nGPT-4o로 전환하여 GPT-4와 비교해서 성능이 어떤지 테스트해 보았어요.\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-06-19-WhatGPT-4omeansfordesigners_1.png)\n\nI found a random draft of an app design online and asked ChatGPT (with the GPT-4o model) to provide me with some suggestions for improving the UI design.\n\n![Image 2](/assets/img/2024-06-19-WhatGPT-4omeansfordesigners_2.png)\n\nThis is the result I received. It was extensive, and at the bottom, there were some actionable suggestions.\n\n\n<div class=\"content-ad\"></div>\n\n\n![2024-06-19-WhatGPT-4omeansfordesigners_3.png](/assets/img/2024-06-19-WhatGPT-4omeansfordesigners_3.png)\n\n![2024-06-19-WhatGPT-4omeansfordesigners_4.png](/assets/img/2024-06-19-WhatGPT-4omeansfordesigners_4.png)\n\n그런 다음 GPT-4로 전환하여 동일한 프롬프트를 요청했습니다.\n\n결과는 다음과 같습니다. - GPT-4o와 유사한 품질이지만 덜 많은 정보를 제공합니다.\n\n\n<div class=\"content-ad\"></div>\n\nGPT-4의 속도는 GPT-4o의 속도보다 훨씬 느렸습니다. GPT-4o는 GPT-4보다 적어도 2배 빨랐어요.\n\n![image](/assets/img/2024-06-19-WhatGPT-4omeansfordesigners_5.png)\n\n그런 다음, 제가 ChatGPT (GPT-4o)에게 제가 제공한 UI와 유사한 레이아웃의 실제 예시 몇 개를 보여주길 요청했어요.\n\n놀랍게도, 모든 예시는 “과정”과 “과제”에 중점을 둔 모바일 응용 프로그램과 관련이 있었어요. 이는 ChatGPT가 제공한 UI가 과정 관련 모바일 앱을 위한 것임을 감지했다는 것을 나타내요.\n\n<div class=\"content-ad\"></div>\n\n마지막으로, GPT-4로 전환하여 동일한 추가 질문을 했어요.\n\n결과의 품질은 GPT-4의 것만큼 좋지 않았어요. 예를 들어, Google Classroom이라는 코스 주제 앱이 하나뿐이었어요. 다른 예시들은 깔끔한 레이아웃에만 집중했지만 GPT-4의 제안들만큼 관련성이 떨어졌어요.\n\n<div class=\"content-ad\"></div>\n\n# 요약\n\n- GPT-4o는 GPT-4보다 훨씬 빠릅니다.\n- GPT-4o의 텍스트 생성 품질이 GPT-4보다 약간 우수합니다.\n- GPT-4o의 멀티모달 기능은 디자이너들에게 많은 잠재력을 제공합니다.\n- GPT-4o의 새로운 기능들이 앞으로 몇 달 동안 지속적으로 추가될 것입니다.\n\n읽어 주셔서 감사합니다. 이 이슈를 즐겁게 보셨다면, Design with AI 뉴스레터를 유익하게 활용할 수 있는 누군가에게 공유해보세요.\n\n좋은 한 주 되세요! 저는 시애틀로의 업무 관련 행사 참석을 위해 방문할 예정입니다 — 설레네요!\n\n<div class=\"content-ad\"></div>\n\n--- 신란\n\n\n📮 Design with AI에 참여해 주세요. AI가 디자인에 미치는 잠재력을 탐구하는 디지턈 퍼블리케이션입니다. 매주 실용적인 기사를 받아보면서 AI를 활용해 더 나은, 더 빠르고 똑똑한 디자인을 할 수 있습니다.\n\n\n🏫 메이븐이 제 AI 코스인 '제품 디자이너를 위한 AI'를 론칭했어요. 자리를 빨리 확보하고, 등록을 마칠 때까지 이 코드인 EARLYBIRD100을 사용하면 체크아웃 시 100달러를 할인받을 수 있어요.\n\n\n📙 제 세 번째 책인 \"포트폴리오 비밀\"이 드디어 Gumroad에서 구할 수 있게 되었어요. 다른 곳에서 찾기 어려운 디자인 포트폴리오에 대한 새로운 시각을 제시합니다.\n이 코드 MEDIUM을 사용하면 체크아웃 시 30% 할인 혜택을 받을 수 있어요.","ogImage":{"url":"/assets/img/2024-06-19-WhatGPT-4omeansfordesigners_0.png"},"coverImage":"/assets/img/2024-06-19-WhatGPT-4omeansfordesigners_0.png","tag":["Tech"],"readingTime":5},{"title":"LLamaIndex와 Gemini를 사용하여 고급 검색 엔진 만들기","description":"","date":"2024-06-19 19:24","slug":"2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini","content":"\n\n# 소개\n\n리트리버는 RAG(Retrieval Augmented Generation) 파이프라인의 가장 중요한 부분입니다. 이 문서에서는 LlamaIndex를 사용하여 키워드 및 벡터 검색 리트리버를 결합한 사용자 정의 리트리버를 구현할 것입니다. Gemini LLM을 사용한 다중 문서 채팅은 우리가 이 RAG 파이프라인을 구축할 프로젝트 사례입니다. 프로젝트를 시작하기 전에 우리는 이러한 애플리케이션을 구축하기 위해 서비스 및 스토리지 콘텍스트와 같은 몇 가지 중요한 구성 요소를 먼저 이해할 것입니다.\n\n## 학습 목표\n\n- RAG 파이프라인에 대한 통찰력을 얻고, 리트리버와 제너레이터 구성 요소의 역할을 이해하여 맥락적으로 응답을 생성하는 방법을 알아봅니다.\n- 키워드 및 벡터 검색 기술을 통합하여 검색 정확도를 향상시키는 사용자 정의 리트리버 개발법을 배웁니다.\n- LlamaIndex를 활용하여 데이터 삽입을 수행하고, LLM에 맥락을 제공하며, 사용자 지정 데이터와의 연결을 깊이 있게 이해하는 능력을 습득합니다.\n- LLM 응답에서 발생할 수 있는 환각 현상을 완화하는 데 사용자 정의 리트리버의 중요성을 이해합니다.\n- 문서의 관련성을 향상시키기 위해 다시 순위 지정 및 HyDE와 같은 고급 리트리버 구현을 탐색합니다.\n- Gemini LLM 및 LlamaIndex 내에서 임베딩을 통합하여 응답 생성 및 데이터 저장을 개선하여 RAG 기능을 향상합니다.\n- 맞춤형 리트리버 설정에 대한 의사 결정 능력을 발전시켜 검색 결과 최적화를 위해 AND 및 OR 연산 중 선택하는 방법을 배웁니다.\n\n<div class=\"content-ad\"></div>\n\n# LlamaIndex이 무엇인가요?\n\n대규모 언어 모델 분야는 매일 크게 발전하고 있습니다. 많은 모델이 빠르게 출시되고 있기 때문에 이러한 모델을 사용자 정의 데이터와 통합할 필요성이 점점 커지고 있습니다. 이러한 통합은 기업, 기업, 그리고 최종 사용자에게 더 많은 유연성과 데이터와의 깊은 연결성을 제공합니다.\n\n초기에 GPT-index로 알려져 있었던 LlamaIndex는 LLM 애플리케이션을 위해 설계된 데이터 프레임워크입니다. ChatGPT와 같은 사용자 지정 데이터 기반 챗봇을 만드는 인기가 계속해서 증가함에 따라 LlamaIndex와 같은 프레임워크는 점점 가치가 있는 존재가 되고 있습니다. 핵심적으로, LlamaIndex는 다양한 데이터 커넥터를 제공하여 데이터 수집을 용이하게합니다. 이 글에서는 우리가 데이터를 LLM에 context로 전달하는 방법을 알아볼 것이며, 이 개념이 의미하는 것이 Retrieval Augmented Generation, 줄여서 RAG입니다.\n\n# RAG가 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\nRAG(축약어인 Retrieval Augmented Generation)에는 Retriever와 Generator 두 가지 주요 구성 요소가 있습니다.\n\n- Retriever는 벡터 데이터베이스일 수 있으며, 사용자 쿼리에 관련 문서를 검색한 후 이를 문맥으로 사용자에게 전달하는 역할을 합니다.\n- Generator 모델은 대형 언어 모델로, 검색된 문서와 프롬프트를 함께 사용하여 문맥으로부터 의미 있는 응답을 생성하는 역할을 합니다.\n\n이 방식으로 RAG는 Automated Few shot prompting을 통한 문맥 학습에 대한 최적의 솔루션입니다.\n\n# Retriever의 중요성\n\n<div class=\"content-ad\"></div>\n\n\n![Retriever Component](/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_0.png)\n\nRAG 파이프라인에서 Retriever 구성 요소의 중요성을 이해해봅시다.\n\n사용자 정의 검색기를 개발할 때, 우리의 Bed을 가장 잘 수행하는 검색기 유형을 결정하는 것이 중요합니다. 우리의 목적에 따라 우리는 Keyword Search와 Vector Search를 통합한 Hybrid Search를 구현할 것입니다.\n\nVector Search는 유사성 또는 의미 검색을 기반으로 사용자 쿼리에 대한 관련 문서를 식별하며, Keyword Search는 용어 발생 빈도에 기반하여 문서를 찾습니다. 이 통합은 LlamaIndex를 사용하여 두 가지 방법으로 달성할 수 있습니다. Hybrid Search의 사용자 정의 검색기를 구축할 때, 중요한 결정 사항은 AND 또는 OR 연산자 중 어느 것을 사용할지 선택하는 것입니다:\n\n\n<div class=\"content-ad\"></div>\n\n- AND 연산: 이 방법은 모든 지정된 용어를 포함하는 문서를 검색하여 더 제한적이지만 높은 관련성을 보장합니다. 이를 키워드 검색과 벡터 검색 결과 간의 교집합으로 생각할 수 있습니다.\n\n- OR 연산: 이 방법은 지정된 용어 중 어떤 것이라도 포함하는 문서를 검색하며 결과의 폭을 늘릴 수 있지만 관련성을 줄일 수 있습니다. 이를 키워드 검색과 벡터 검색 결과 간의 합집합으로 생각할 수 있습니다.\n\n# LLamaIndex를 사용한 사용자 지정 검색기 만들기\n\n이제 LLamaIndex를 사용하여 사용자 지정 검색기를 만들어 보겠습니다. 이를 위해 특정 단계를 따라야 합니다.\n\n# 단계 1: 설치\n\n<div class=\"content-ad\"></div>\n\nGoogle Colab이나 Jupyter Notebook에서 코드 구현을 시작하려면 주로 필요한 라이브러리를 설치해야 합니다. 이 경우에는 사용자 지정 검색기 생성을 위해 LlamaIndex, 임베딩 모델 및 LLM 추론을 위한 Gemini, 데이터 커넥터로 PyPDF를 사용할 것입니다.\n\n```js\n!pip install llama-index\n!pip install llama-index-multi-modal-llms-gemini\n!pip install llama-index-embeddings-gemini\n```\n\n# 단계 2: Google API 키 설정\n\n이 프로젝트에서는 Google Gemini를 사용하여 대규모 언어 모델로 응답을 생성하고, LlamaIndex를 사용하여 데이터를 벡터-DB나 메모리 저장 공간에 변환 및 저장하는 임베딩 모델로 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n여기서 API 키를 얻으세요.\n\n```js\nfrom getpass import getpass\n```\n\n```js\nGOOGLE_API_KEY = getpass(\"Google API 키를 입력하세요:\")\n```\n\n# 단계 3: 데이터 로드 및 문서 노드 생성\n\n<div class=\"content-ad\"></div>\n\nLlamaIndex에서는 SimpleDirectoryLoader를 사용하여 데이터를 로드합니다. 먼저 폴더를 만들고 이 데이터 폴더에 데이터를 어떤 형식으로든 업로드해야 합니다. 저희 예시에서는 PDF 파일을 데이터 폴더에 업로드할 것입니다. 문서가 로드된 후, 문서를 더 작은 세그먼트로 분할하기 위해 노드로 파싱됩니다. 노드는 LlamaIndex 프레임워크 내에서 정의된 데이터 스키마입니다.\n\nLlamaIndex의 최신 버전은 코드 구조를 업데이트했는데요, 이제 노드 파서, 임베딩 모델 및 Settings 내의 LLM에 대한 정의가 포함되어 있습니다.\n\n```js\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.core import Settings\n```\n\n```js\ndocuments = SimpleDirectoryReader('data').load_data()\nnodes = Settings.node_parser.get_nodes_from_documents(documents)\n```\n\n<div class=\"content-ad\"></div>\n\n# 단계 4: 임베딩 모델 및 큰 언어 모델 설정하기\n\n젬니(Gemini)는 gemini-pro, gemini-1.0-pro, gemini-1.5, 비전 모델 등 다양한 모델을 지원합니다. 이 경우에는 기본 모델을 사용하고 Google API 키를 제공할 것입니다. Gemini의 임베딩 모델로는 현재 embedding-001을 사용하고 있습니다. 유효한 API 키가 추가되었는지 확인하세요.\n\n```js\nfrom llama_index.embeddings.gemini import GeminiEmbedding\nfrom llama_index.llms.gemini import Gemini\n```\n\n```js\nSettings.embed_model = GeminiEmbedding(\n    model_name=\"models/embedding-001\", api_key=GOOGLE_API_KEY\n)\nSettings.llm = Gemini(api_key=GOOGLE_API_KEY)\n```\n\n<div class=\"content-ad\"></div>\n\n# 단계5: 저장 문맥 정의 및 데이터 저장\n\n데이터가 노드로 구문 분석되면 LlamaIndex는 저장 문맥을 제공하여 데이터의 벡터 임베딩을 저장하는 기본 문서 저장소를 제공합니다. 이 저장 문맥은 데이터를 메모리에 유지하여 나중에 색인화할 수 있습니다.\n\n```js\nfrom llama_index.core import StorageContext\n```\n\n```js\nstorage_context = StorageContext.from_defaults()\nstorage_context.docstore.add_documents(nodes)\n```\n\n<div class=\"content-ad\"></div>\n\n인덱스-키워드 및 인덱스 생성\n\n![이미지](/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_1.png)\n\n하이브리드 검색을 수행하는 사용자 지정 검색기를 구축하려면 두 가지 인덱스를 생성해야 합니다. 벡터 검색을 수행할 수 있는 첫 번째 벡터 인덱스와 키워드 검색을 수행할 수 있는 두 번째 키워드 인덱스입니다. 인덱스를 생성하려면 저장 컨텍스트와 노드 문서뿐만 아니라 임베딩 모델과 LLM의 기본 설정도 필요합니다.\n\n```js\nfrom llama_index.core import SimpleKeywordTableIndex, VectorStoreIndex\n```\n\n<div class=\"content-ad\"></div>\n\n```js\nvector_index = VectorStoreIndex(nodes, storage_context=storage_context)\nkeyword_index = SimpleKeywordTableIndex(nodes, storage_context=storage_context)\n```\n\n# 단계6: 사용자 지정 검색기 만들기\n\nLlamaIndex를 사용하여 하이브리드 검색을 위한 사용자 지정 검색기를 만들기 위해 먼저 스키마를 정의해야 합니다. 이는 노드를 적절하게 구성함으로써 수행됩니다. 검색기에는 벡터 인덱스 검색기와 키워드 검색기 모두가 필요합니다. 이를 통해 하이브리드 검색을 수행하고 결과를 결합하여 혼돈을 최소화할 수 있습니다. 게다가 우리는 결과를 결합할 때 사용할 모드(AND 또는 OR)를 지정해야 합니다.\n\n노드가 구성되면 각 노드 ID에 대해 번들을 조회하고 벡터 및 키워드 검색기를 사용합니다. 선택한 모드에 따라 사용자 지정 검색기를 정의하고 완성합니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom llama_index.core import QueryBundle\nfrom llama_index.core.schema import NodeWithScore\n```\n\n```js\nfrom llama_index.core.retrievers import (\n    BaseRetriever,\n    VectorIndexRetriever,\n    KeywordTableSimpleRetriever,\n)\nfrom typing import List\n\nclass CustomRetriever(BaseRetriever):\n    def __init__(\n        self,\n        vector_retriever: VectorIndexRetriever,\n        keyword_retriever: KeywordTableSimpleRetriever,\n        mode: str = \"AND\") -> None:\n       \n        self._vector_retriever = vector_retriever\n        self._keyword_retriever = keyword_retriever\n        if mode not in (\"AND\", \"OR\"):\n            raise ValueError(\"Invalid mode.\")\n        self._mode = mode\n        super().__init__()\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n        vector_ids = {n.node.node_id for n in vector_nodes}\n        keyword_ids = {n.node.node_id for n in keyword_nodes}\n        combined_dict = {n.node.node_id: n for n in vector_nodes}\n        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n        if self._mode == \"AND\":\n            retrieve_ids = vector_ids.intersection(keyword_ids)\n        else:\n            retrieve_ids = vector_ids.union(keyword_ids)\n        retrieve_nodes = [combined_dict[r_id] for r_id in retrieve_ids]\n        return retrieve_nodes\n```\n\n# Step7: Define Retrievers\n\n이제 사용자 정의 검색기 클래스가 정의되었으므로, 검색기를 인스턴스화하고 쿼리 엔진을 합성해야 합니다. 응답 씨네사이저는 사용자 쿼리와 주어진 텍스트 청크 세트를 기반으로 LLM에서 응답을 생성하는 데 사용됩니다. 응답 씨네사이저에서 출력은 응답 객체이며, 이 객체는 사용자 정의 검색기를 하나의 매개 변수로 취합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom llama_index.core import get_response_synthesizer\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n```\n\n```js\nvector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=2)\nkeyword_retriever = KeywordTableSimpleRetriever(index=keyword_index)\n# custom retriever => combine vector and keyword retriever\ncustom_retriever = CustomRetriever(vector_retriever, keyword_retriever)\n# define response synthesizer\nresponse_synthesizer = get_response_synthesizer()\ncustom_query_engine = RetrieverQueryEngine(\n    retriever=custom_retriever,\n    response_synthesizer=response_synthesizer,\n)\n```\n\n# Step8: Run Custom Retriever Query Engine\n\n마침내, 현저하게 환각을 줄이는 사용자 정의 검색기를 개발했습니다. 그 효과를 테스트하기 위해, 우리는 컨텍스트 내부와 외부에서 한 가지 프롬프트를 포함한 사용자 쿼리를 실행하고 생성된 답변을 평가했습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nquery = \"데이터 컨텍스트에는 무엇이 포함되어 있나요?\"\nprint(custom_query_engine.query(query))\nprint(custom_query_engine.query(\"과학이란 무엇인가요?\")\n```\n\n![이미지](/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_2.png)\n\n# 결론\n\n우리는 LlamaIndex를 사용하여 벡터와 키워드 검색기를 결합하여 Gemini LLM 및 임베딩의 지원을 받아 하이브리드 검색을 수행하는 맞춤형 리트리버를 성공적으로 구현했습니다. 이 접근은 전형적인 RAG 파이프라인에서 LLM 환각을 어느 정도 감소시킴으로써 효과적입니다.\n\n<div class=\"content-ad\"></div>\n\n## 중요 사항\n\n- 벡터 및 키워드 리트리버를 통합한 사용자 지정 리트리버 개발으로 RAG의 관련 문서 식별 능력과 정확성을 향상시킴.\n- LlamaIndex 설정을 사용하여 Gemini Embedding 및 LLM을 구현하였으며, 최신 버전에서는 이전에 사용되던 Service Context보다 나은 것으로 대체되었음.\n- 사용자 지정 리트리버 구축 시 AND 또는 OR 연산을 사용할 것인지 결정하는 것이 중요하며, 특정 요구 사항에 따라 키워드 및 벡터 검색 결과의 교집합과 합집합을 균형 있게 조정해야 함.\n- 사용자 지정 리트리버 설정은 RAG 파이프라인 내에서 하이브리드 검색 메커니즘을 사용하여 대형 언어 모델 응답에서 환각을 크게 줄여줌.\n\n# 나에 대해\n\nLinkedIn 프로필이 이곳에 있어요. 연결하고 싶으시면 클릭해주세요. 제 글을 즐겁게 읽어주셨으면 좋겠어요. 마음에 드셨다면 친구들과 공유하고 저를 팔로우해주세요. 제 글 작성을 개선할 수 있는 생각이 있다면 자유롭게 의견을 남겨주세요.\n제 이전 게시된 모든 글은 여기에서 읽을 수 있어요. [https://aivichar.com/]","ogImage":{"url":"/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_0.png"},"coverImage":"/assets/img/2024-06-19-BuildingAdvancedSearchEngineswithLLamaIndexandGemini_0.png","tag":["Tech"],"readingTime":10},{"title":"대형 언어 모델 설명  I","description":"","date":"2024-06-19 19:23","slug":"2024-06-19-LargeLanguageModelsExplainedI","content":"\n\n안녕하세요! ChatGPT가 어떻게 작동하는지 궁금했던 적이 있나요? ChatGPT-3.5는 무료로 제공되지만 ChatGPT-4는 유료인 이유가 궁금하신가요? 그와 같은 대안이 있는지 알아보고 싶나요? 이 기사에서 ChatGPT에 대해 도는 모든 질문에 답해보겠습니다. ChatGPT에 대한 기술적 세부사항에만 근거하여 수학이나 ChatGPT 뒤에 있는 프로그램에 대한 내용은 제외할 것입니다.\n\n![이미지](/assets/img/2024-06-19-LargeLanguageModelsExplainedI_0.png)\n\nChatGPT를 이해하기 위해서는 뉴럴 네트워크가 무엇인지 알아야 합니다. 우리는 인간 뇌의 기능 단위가 뉴런임을 알고 있습니다. 뉴런은 뇌가 일을 기억하고 자체적으로 생각하도록 하는 길을 만들면서 뉴런의 네트워크를 형성함으로써 작용합니다. 지난 몇10년 동안 많은 과학자들이 인간 뇌를 모방하여 유사한 지능을 끌어내기 위해 노력해왔습니다. 이 넓은 연구 분야를 인공 지능이라고 합니다.\n\n인간 뇌와 유사하게, 여기서 관심 있는 뉴럴 네트워크는 정보를 흡수하고 처리할 수 있는 여러 뉴런으로 이루어져 있습니다. 기본 뉴럴 네트워크의 추상적인 구조는 다음과 같을 것입니다:\n\n<div class=\"content-ad\"></div>\n\n![그림](/assets/img/2024-06-19-LargeLanguageModelsExplainedI_1.png)\n\n입력 레이어를 통해 네트워크로 입력이 전달되는 구조로 구성되어 있습니다. 정보를 수용하는 데 사용되는 여러 개의 은닉 레이어와 신경 네트워크의 성격에 따라 출력을 제공하는 출력 레이어로 구성됩니다. 이들 모든 뉴런은 상호 연결되어 있으며 이러한 연결에는 가중치라는 것이 있습니다. 무엇이 있는지 결정하고 그 뉴런과 경로의 중요도를 결정합니다. 일반적으로 신경망은 뉴런, 가중치, 편향, 활성화 함수 및 최적화기로 구성되어 있습니다. (이 모든 것에 대한 세부사항은 이 기사의 목적이 아니므로 나중에 다룰 것입니다.)\n\nn-gram과 같은 전통적인 언어 모델은 이전 단어에 기초하여 문장에서 다음 단어의 가능성을 예측합니다. 또 다른 모델인 Hidden Markov Model (HMM)은 관찰 시퀀스와 숨겨진 상태 시퀀스 간의 확률적 관계를 설명하는 데 사용됩니다. 언어 모델이 일반적으로 확률적이며 문장에서 다음 단어를 예측하려고 하는 것을 이해합니다. 이 아이디어는 거대한 언어 모델의 기초이며 많은 뉴런이 관련되어 있다는 점을 제외하고도 동일합니다!\n\n이제 CNN(Convolutional Neural networks), RNN(Recurrent Neural Networks), Feed Forward Neural Networks 등과 같은 다양한 유형의 신경망이 있습니다. RNN은 ChatGPT의 기초를 형성합니다. 와우!! 물론, 기본 RNN에는 많은 단점이 있습니다. 따라서 RNN을 기초로 하는 Long Short Term Memory 신경망(LSTM)이 만들어졌습니다. 심지어 LSTM도 잘 작동하지 않았기 때문에 Encoder-Decoder 모델이 제안되었습니다. 인코더는 입력 시퀀스를 받아 고정 길이 벡터로 처리합니다. 이 벡터는 출력 시퀀스를 생성하는 디코더로 보내집니다. (참고: 각 인코더와 디코더에는 여러 개의 신경망이 내장되어 있습니다.) 그러나 인코더가 처리하기에 입력 시퀀스가 너무 길면 정보가 손실될 수 있습니다. 이 문제를 해결하기 위해 어텐션 메커니즘이 제안되었으며, 이를 통해 디코더는 입력 시퀀스를 다시 살펴보고 시퀀스의 중요한 부분에만 주의를 기울일 수 있습니다. 마지막으로, Transformer 아키텍처는 인코더-디코더 모델에서 어텐션 메커니즘의 사용을 결합하여 산업에 혁명을 일으켰습니다! Transformer 모델은 특히 긴 데이터 시퀀스를 처리하는 데 능숙합니다. 기계 번역 및 텍스트 생성과 같은 자연어 처리 작업에 적합합니다.\n\n<div class=\"content-ad\"></div>\n\n(참고: 위 단락에서는 자세히 설명하지 않았습니다. 대신, 모든 것을 깊이 있는 연구할 수 있는 모든 필요한 링크를 제공했습니다. 다가오는 기사에서도 모든 것을 자세히 다룰 예정입니다).\n\nLarge Language Models는 방대한 양의 데이터로 훈련된 언어 모델에 대한 것입니다. 따라서 LLM의 기본 모델은 주로 방대한 양의 데이터로 훈련된 transformer 모델이라는 것이 이해됩니다.\n\n![Large Language Models](/assets/img/2024-06-19-LargeLanguageModelsExplainedI_2.png)\n\nGenerative Pre-trained Transformer (GPT)은 Encoder만 사용하는 BERT 및 Encoder-Decoder를 사용하는 BART와 달리 Decoder 모델만 사용하는 LLM 중 하나입니다. 본 문서에서는 GPT-3.5를 고려합니다. 이는 약 96개의 레이어와 175B의 파라미터를 가진 풍부한 데이터로 훈련되었습니다. ChatGPT는 이전 버전인 GPT-3의 세분화된 버전으로, ‘Reinforcement Learning With Human Feedback’(RLHF)라는 개념을 포함하고 있습니다. 3.5 버전은 텍스트 생성에만 중점을 둔 것으로, 텍스트 형식으로 묻는 질문에 텍스트 형식으로 답변합니다.\n\n<div class=\"content-ad\"></div>\n\n하지만 실제로 어떻게 작동하는 건가요?\n\n우리가 하는 모든 질문마다, 입력은 GPT로 순차적으로 전달되며 96개의 레이어를 거쳐 다음 단어를 예측합니다 (이제 ChatGPT가 검색 엔진과 같이 한꺼번에 단어를 반환하지 않는 이유를 알게 되었죠).\n\n예를 들어, \"인도의 수도는 무엇인가요?\"라고 묻는다고 해봅시다. 이 입력은 GPT로 한 단어씩 전송됩니다 (필요한 모든 벡터 생성과 함께) 그리고 마지막으로 모든 문맥 단어들과 함께 \"다음 단어를 예측합니다. \"수도\", \"인도\", \"델리\"라는 단어들이 데이터에서 다른 단어보다 많이 함께 나타날 수 있죠.\n\n![이미지](/assets/img/2024-06-19-LargeLanguageModelsExplainedI_3.png)\n\n<div class=\"content-ad\"></div>\n\n마지막으로, GPT는 실제로는 신경망에 의해 형성된 LLM이기도 합니다. 따라서 어떤 질문을 하면, 이는 사전 훈련된 모델을 테스트하는 것이며 우리가 얻는 답변은 모델의 예측입니다. 하지만 모델이 어디에서 실행되는 걸까요? 모델을 로컬 머신에서 실행시킬 수 있을까요? 사전 훈련된 모델을 특정 도메인 질문에 대답하도록 할 수 있을까요?\n이 모든 질문에 대한 답변은 다음 기사에서 제공할 예정입니다. 그때까지 지식에 대한 갈증을 유지해 주세요!\n독자 여러분의 읽어주셔서 감사합니다.\n\n- https://www.labellerr.com/blog/evolution-of-neural-networks-to-large-language-models/\n- https://www.v7labs.com/blog/neural-network-architectures-guide\n- https://medium.com/@amanatulla1606/transformer-architecture-explained-2c49e2257b4c\n- https://www.geeksforgeeks.org/hidden-markov-model-in-machine-learning/\n- https://iq.opengenus.org/gpt-3-5-model/\n- https://manikanthgoud123.medium.com/what-is-rlhf-reinforcement-learning-from-human-feedback-d0ec88e0866c#:~:text=RLHF%20involves%20an%20ongoing%20training,creative%20and%20user%2Daligned%20results.","ogImage":{"url":"/assets/img/2024-06-19-LargeLanguageModelsExplainedI_0.png"},"coverImage":"/assets/img/2024-06-19-LargeLanguageModelsExplainedI_0.png","tag":["Tech"],"readingTime":4},{"title":"내가 Python을 사용하여 나만의 파일에서 새로운 GPT-4o를 어떻게 활용하는지","description":"","date":"2024-06-19 19:21","slug":"2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython","content":"\n\n\n![2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_0.png](/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_0.png)\n\n현재 인공 지능 뉴스를 따라갔다면 OpenAI가 최근에 최신 모델인 GPT-4o를 발표했다는 것을 알고 계실 것입니다. 가장 인상적인 새로운 기능은 다중 모달리티인데, 아쉽게도 이러한 기능들은 아직 공개되지 않았습니다.\n\n우리가 다중 모달 특징들이 롤아웃되길 기다리는 동안, 나는 여러분의 데이터에 GPT-4o를 사용하는 가장 쉬운 방법을 보여드리겠습니다, 어시스턴트!\n\nOpenAI는 최근에 어시스턴트를 업데이트하여 데이터를 섭취하는 능력을 크게 향상시켰습니다. GPT-4o의 효율성과 어시스턴트가 이제 최대 10,000개의 파일을 섭취할 수 있는 데, 여러분만의 어시스턴트를 만들기에 더 좋은 시기가 온 적이 없습니다.\n\n\n<div class=\"content-ad\"></div>\n\n이 글에서는 GPT-4o를 사용하여 어시스턴트를 만드는 가장 쉬운 방법을 안내하겠습니다.\n\n만약 프로페셔널이 여러분과 비즈니스에 맞는 어시스턴트를 개인 맞춤형으로 만들어주길 원하신다면 www.woyera.com 으로 연락해주세요.\n\n## 단계 1: 데이터 준비\n\n시작하기 전에, 여러분의 웹사이트나 애플리케이션을 위해 어시스턴트를 사용하려면 OpenAI API 키가 필요하다는 사실을 알려드리고 싶습니다.\n\n<div class=\"content-ad\"></div>\n\n먼저 OpenAI 플랫폼에 로그인하고 대시보드로 이동하세요. 그런 다음 아래에 표시된대로 “저장소”를 클릭하세요.\n\n![이미지](/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_1.png)\n\n저장소 페이지에서 “벡터 저장소”를 선택한 다음 “만들기”를 클릭하세요. 벡터 저장소를 사용하면 의미론적 검색, 효율적인 검색, 더 나은 문맥 파악, 확장성 및 OpenAI API와의 원활한 통합이 가능해집니다.\n\n![이미지](/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_2.png)\n\n<div class=\"content-ad\"></div>\n\n벡터 저장소를 만들었으면, Assistant가 액세스하길 원하는 파일을 추가하고 벡터 저장소 ID를 복사하십시오. 나중에 필요하게 됩니다.\n\n![image](/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_3.png)\n\n## 단계 2: Assistant 만들기\n\n왼쪽 탐색 모음을 사용하여 \"Assistants\" 플레이그라운드로 이동하고 \"Assistant 만들기\"를 선택하십시오.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_4.png\" />\n\n어시스턴트를 만들었으면 어시스턴트 ID 코드를 복사하여 나중에 사용할 수 있도록 해주세요.\n\n<img src=\"/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_5.png\" />\n\n어시스턴트에 정확한 지시를 제공하고 \"모델\"을 사용하여 원하는 모델을 선택하십시오. 여기서는 \"gpt-4o\"를 선택하면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n마크다운 형식으로 표 태그를 변경하십시오.\n\n![How to Use GPT-4 with Python - File Search](/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_6.png)\n\n저희 어시스턴트가 사용할 \"파일 검색\" 도구를 선택해 주세요.\n\n![How to Use GPT-4 with Python - Click Files](/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_7.png)\n\n아래에 보이는 대로 \"파일\"을 클릭해주세요.\n\n<div class=\"content-ad\"></div>\n\n![`/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_8.png`](/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_8.png)\n\nClick on \"Select vector store\"\n\n![`/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_9.png`](/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_9.png)\n\nFinally, you will use the vector store id that we copied earlier and paste it into the field, click select after you have chosen the vector store you want.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_10.png)\n\n벡터 저장소에 애니메이션을 연결한 후에는 옵션 된 파일의 내용에 관한 질문으로 에이전트를 테스트해 보세요.\n\n내 어시스턴트의 경우, Colleen Hoover의 소설 'Verity'가 있는 벡터 저장소에 연결하도록 만들었습니다. 여러분의 파트너가 요청한대로 그리고 답변을 참조하면서 이 답변을 생성하는 모습을 확인할 수 있어요!\n\n![이미지](/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_11.png)\n\n\n<div class=\"content-ad\"></div>\n\n이정도면 몇몇 사람들에게 충분할지도 모르지만, 파이썬을 사용하여 애플리케이션에 어시스턴트를 통합하는 방법을 자세히 보여드릴 수도 있습니다!\n\n## 단계 3: API 확인\n\n앱에 통합하기 전에 API 호출이 제대로 작동하는지 먼저 확인할 수 있습니다. 이를 위해 간단한 코드를 사용하여 확인할 수 있습니다. 전체 스크립트를 보고 싶다면 아래로 스크롤하세요.\n\n먼저, 이러한 인포트(imports)를 사용해야 합니다. 윈도우 환경이라면 \"pip install openai\"를 사용하고, 맥 환경이라면 \"pip3 install openai\"를 사용하는 것을 기억해 주세요.\n\n<div class=\"content-ad\"></div>\n\n```python\n예를 들어, 다음과 같이 코드를 작성할 수 있습니다.\n\nimport time\nfrom openai import OpenAI\n\n먼저 API 키를 사용하여 클라이언트를 만듭니다.\n\nclient = OpenAI(api_key='여기에 키를 입력하세요')\n\n그런 다음 대화를 저장할 \"스레드\"를 만듭니다.\n```\n\n<div class=\"content-ad\"></div>\n\n```js\nempty_thread = client.beta.threads.create()\nprint(empty_thread)\n```\n\n그런 다음 메시지를 추가할 스레드 ID를 가져옵니다.\n\n```js\nthread_id= empty_thread.id\n```\n\n메시지 생성 및 스레드에 추가하기\n\n<div class=\"content-ad\"></div>\n\n```js\nthread_message = client.beta.threads.messages.create(\n  thread_id=thread_id,\n  role=\"user\",\n  content=\"여기에 메시지를 입력하세요\",\n)\nprint(thread_message)\n```\n\n이 코드 부분은 \"실행\" 상태를 추적하는 데 사용됩니다. 처음 실행했을 때 응답이로드되는 데 시간이 오래 걸렸고 작동 중인지 느리게 진행 중인지 확신할 수 없었습니다. 이것은 실행이 완료되지 않은 동안 실행 상태를 출력합니다.\n\n```js\nwhile run.status != \"completed\":\n    run = client.beta.threads.runs.retrieve(thread_id=empty_thread.id, run_id=run.id)\n    print(f\"실행 상태: {run.status}\")\n    time.sleep(0.5)\nelse:\n    print(\"실행 완료!\")\n```\n\n쓰레드로부터의 응답 및 실행에서의 메시지를 얻으려면 다음을 사용합니다.\n\n\n<div class=\"content-ad\"></div>\n\n\nmessage_response = client.beta.threads.messages.list(thread_id=empty_thread.id) \nmessages = message_response.data\n\nlatest_message = messages[0]\nprint(f\" response: {latest_message.content[0].text.value}\")\n\n\nPut all the code together, and ask the OpenAI Assistant Interface the same question as earlier. It should give the same answer, and it did!\n\n![Image](/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_12.png)\n\nThis is the complete script!\n\n\n<div class=\"content-ad\"></div>\n\n```python\nimport time\nfrom openai import OpenAI\n\nclient = OpenAI(api_key='YOUR KEY HERE')\n\n# Create Thread\nempty_thread = client.beta.threads.create()\nprint(empty_thread)\n\n# Get thread id\nthread_id = empty_thread.id\n\n# Create message to add to Thread\nthread_message = client.beta.threads.messages.create(\n  thread_id=thread_id,\n  role=\"user\",\n  content=\"USER QUESTION HERE\",\n)\nprint(thread_message)\n\n# Create a Run\nrun = client.beta.threads.runs.create(\n  thread_id=thread_id,\n  assistant_id='YOUR ASSISTANT ID HERE'\n)\n\nrun_id = run.id\n\nwhile run.status != \"completed\":\n    run = client.beta.threads.runs.retrieve(thread_id=empty_thread.id, run_id=run.id)\n    print(f\"Run status: {run.status}\")\n    time.sleep(0.5)\nelse:\n    print(\"Run Complete!\")\n\nmessage_response = client.beta.threads.messages.list(thread_id=empty_thread.id)\nmessages = message_response.data\n\nlatest_message = messages[0]\nprint(f\"Response: {latest_message.content[0].text.value}\")\n```\n\n### Step 4: 통합\n\n도우미 API를 응용 프로그램에 통합하려면 수많은 다양한 방법을 사용할 수 있습니다. 이것은 API에 인터페이스를 제공하는 가장 쉬운 방법 중 하나였습니다.\n\nStreamlit은 우리에게 도우미를 표시할 수 있는 쉬운 프론트엔드를 제공합니다. 사이트를 Streamlit을 통해 호스팅하는 것을 선택한다면 다음 코드를 사용해보세요.\n\n\n<div class=\"content-ad\"></div>\n\n```python\nimport streamlit as st\nfrom openai import OpenAI\nimport time\n\n# 클라이언트 생성\nclient = OpenAI(api_key='API 키를 여기에 입력하세요')\n\nst.title(':book: Book Bot')\n\n# 채팅 기록 초기화 (세션 상태 사용)\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\n# 채팅 기록 표시\nfor message in st.session_state.messages:\n    with st.container():\n        st.markdown(f\"**{message['role']}:** {message['content']}\")\n\n# 입력 텍스트 상자\nuser_input = st.text_input(\"You:\", \"\")\n\nif st.button(\"Send\") and user_input:\n    # 사용자 메시지를 채팅 기록에 추가\n    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n    \n    # 사용자 메시지를 즉시 표시\n    with st.container():\n        st.markdown(f\"**You:** {user_input}\")\n\n    # 쓰레드 및 메시지 생성\n    thread = client.beta.threads.create()\n    thread_message = client.beta.threads.messages.create(\n        thread_id=thread.id,\n        role=\"user\",\n        content=user_input,\n    )\n    run = client.beta.threads.runs.create(\n        thread_id=thread.id,\n        assistant_id='어시스턴트 ID를 여기에 입력하세요'  # 귀하의 어시스턴트 ID로 대체\n    )\n    \n    # 실행이 완료될 때까지 기다림\n    with st.spinner(\"Thinking...\"):\n        while run.status != \"completed\":\n            run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n            time.sleep(0.5)\n\n    # 응답 검색 및 표시\n    messages = client.beta.threads.messages.list(thread_id=thread.id).data\n    latest_message = messages[0]\n    response_text = latest_message.content[0].text.value\n\n    # 어시스턴트 메시지를 채팅 기록에 추가\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response_text})\n    \n    # 전송 후 입력 텍스트 상자 지우기\n    user_input = \"\"\n\n    # 어시스턴트 메시지를 즉시 표시\n    with st.container():\n        st.markdown(f\"**Assistant:** {response_text}\")\n```\n\n이렇게 고쳐지는 페이지 모습입니다!\n\n<img src=\"/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_13.png\" />\n\nAI 모델 선택 및 비즈니스 통합 전체 프로세스를 다른 사람에게 맡기고 싶다면, www.woyera.com에서 연락하세요.\n","ogImage":{"url":"/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_0.png"},"coverImage":"/assets/img/2024-06-19-HowIusethenewGPT-4oonmyownfileswithPython_0.png","tag":["Tech"],"readingTime":8},{"title":"PDF 파싱의 신비를 해부하다 03 OCR이 필요 없는 소형 모델 기반 방법","description":"","date":"2024-06-19 19:16","slug":"2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod","content":"\n\nPDF 파일을 다른 형식으로 변환하는 것은 도전적일 수 있습니다. 종종 AI 애플리케이션에서 접근할 수 없는 형식에 상당한 정보를 잠그기 때문입니다. 만약 PDF 파일이나 해당 이미지를 기계가 읽을 수 있는 구조화된 또는 반구조화된 형식으로 변환할 수 있다면, 이 문제를 상당히 완화할 수 있을 것입니다. 이는 인공지능 애플리케이션의 지식 베이스를 크게 향상시킬 수도 있습니다.\n\n이 연재는 PDF 구문 분석을 해석하는 데 전념하고 있습니다. 본 시리즈의 첫 번째 기사에서는 PDF 구문 분석의 주요 작업을 소개하고 기존 방법을 분류하며 각 방법에 대한 간략한 소개를 제공했습니다. 그리고 이 시리즈의 두 번째 기사에서는 파이프라인 기반 방법에 초점을 맞췄습니다.\n\n본 기사는 이 시리즈의 세 번째로, PDF 구문 분석에 대한 또 다른 접근 방식을 소개합니다: OCR 없는 소형 모델 기반 방법. 접근 방식을 개괄한 다음, 다양한 대표적인 OCR 없는 소형 모델 기반 PDF 구문 분석 솔루션의 원칙을 소개합니다. 마지막으로, 얻은 통찰과 생각을 공유합니다.\n\n본 기사에서 언급하는 \"소형 모델\"은 일반적으로 30억 개의 파라미터보다 적은 파라미터를 가질 정도로 상대적으로 작은 모델을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n# 개요\n\n이전에 소개된 파이프라인 기반 PDF 구문 분석 방법은 주로 텍스트 인식을 위해 OCR 엔진을 사용합니다. 그러나 이는 계산 비용이 높아지고 언어 및 문서 유형에 대한 유연성이 결여되며, 잠재적인 OCR 오류가 후속 작업에 영향을 줄 수 있습니다.\n\n따라서 OCR 없는 방법이 개발되어야 합니다. 이는 그림 1에 설명된 대로 OCR을 명시적으로 사용하지 않습니다. 대신, 신경망을 사용하여 암묵적으로 작업을 완료합니다. 본질적으로 이러한 방법은 끝에서 끝까지 접근 방식을 채택하여 PDF 구문 분석 결과를 직접 출력합니다.\n\n구조적 관점에서 OCR 없는 방법은 파이프라인 기반 방법보다 간단합니다. 관심을 불러일으킬 OCR 없는 방법의 주요 측면은 모델 구조의 설계와 훈련 데이터의 구축입니다.\n\n<div class=\"content-ad\"></div>\n\n다음으로, 몇 가지 대표적인 OCR 미사용 소형 모델 기반 PDF 구문 분석 프레임워크를 소개하겠습니다:\n\n- Donut: OCR 미사용 문서 이해 트랜스포머.\n- Nougat: Donut 아키텍처를 기반으로 하며, PDF 문서, 수식 및 표와 같은 문서에서 특히 효과적입니다.\n- Pix2Struct: 시각 언어 이해를 위한 사전 교육으로 스크린샷 파싱.\n\n## Donut\n\n그림 2에서 설명한 것처럼 Donut은 문서 이미지를 종합적으로 이해하기 위해 설계된 엔드투엔드 모델입니다. 그 아키텍처는 transformer 기반의 시각 인코더와 텍스트 디코더 모듈로 구성되어 간단합니다.\n\n<div class=\"content-ad\"></div>\n\n도넛은 OCR과 관련된 어떤 모듈에도 의존하지 않습니다. 대신 문서 이미지에서 특징을 추출하기 위해 시각 인코더를 사용하고, 텍스트 디코더를 사용하여 토큰 시퀀스를 직접 생성합니다. 출력된 시퀀스는 JSON과 같은 구조화된 형식으로 변환할 수 있습니다.\n\n다음은 코드입니다:\n\n```js\nclass DonutModel(PreTrainedModel):\n    r\"\"\"\n    도넛(Donut): OCR 미사용 문서 이해 트랜스포머.\n    인코더는 입력 문서 이미지를 임베딩 세트로 매핑하고,\n    디코더는 원하는 토큰 시퀀스를 예측합니다. 이는 구조화된 형식으로 변환될 수 있으며,\n    프롬프트와 인코더 출력 임베딩이 주어질 때\n    \"\"\"\n    config_class = DonutConfig\n    base_model_prefix = \"donut\"\n\n    def __init__(self, config: DonutConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = SwinEncoder(\n            input_size=self.config.input_size,\n            align_long_axis=self.config.align_long_axis,\n            window_size=self.config.window_size,\n            encoder_layer=self.config.encoder_layer,\n            name_or_path=self.config.name_or_path,\n        )\n        self.decoder = BARTDecoder(\n            max_position_embeddings=self.config.max_position_embeddings,\n            decoder_layer=self.config.decoder_layer,\n            name_or_path=self.config.name_or_path,\n        )\n\n    def forward(self, image_tensors: torch.Tensor, decoder_input_ids: torch.Tensor, decoder_labels: torch.Tensor):\n        \"\"\"\n        입력 이미지와 원하는 토큰 시퀀스가 주어졌을 때 손실을 계산하고,\n        모델은 teacher-forcing 방식으로 훈련될 것입니다.\n\n        Args:\n            image_tensors: (batch_size, num_channels, height, width)\n            decoder_input_ids: (batch_size, sequence_length, embedding_dim)\n            decode_labels: (batch_size, sequence_length)\n        \"\"\"\n        encoder_outputs = self.encoder(image_tensors)\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            encoder_hidden_states=encoder_outputs,\n            labels=decoder_labels,\n        )\n        return decoder_outputs\n    ...\n    ...\n```\n\n## 인코더\n\n<div class=\"content-ad\"></div>\n\n도넛은 초기 문서 구문 분석 연구에서 우수한 성능을 나타낸 Swin-Transformer를 이미지 인코더로 활용합니다. 이 이미지 인코더는 입력 문서 이미지를 고차원 임베딩 집합으로 변환합니다. 이러한 임베딩은 텍스트 디코더의 입력으로 사용될 것입니다.\n\n해당 코드는 다음과 같습니다.\n\n```js\nclass SwinEncoder(nn.Module):\n    r\"\"\"\n    SwinTransformer를 기반으로 한 도넛 인코더\n    사전 훈련된 SwinTransformer를 사용하여 초기 가중치와 구성을 설정한 후, \n    도넛 인코더로 세부 구성을 수정합니다.\n\n    매개변수:\n        input_size: 입력 이미지 크기 (폭, 높이)\n        align_long_axis: 높이가 폭보다 크면 이미지를 회전할지 여부\n        window_size: SwinTransformer의 창 크기(=패치 크기)\n        encoder_layer: SwinTransformer 인코더의 레이어 수\n        name_or_path: huggingface.co에 등록된 사전 훈련된 모델 이름 또는 로컬에 저장된 모델 이름\n                      그렇지 않으면 `swin_base_patch4_window12_384`가 설정될 것입니다(`timm` 사용).\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: List[int],\n        align_long_axis: bool,\n        window_size: int,\n        encoder_layer: List[int],\n        name_or_path: Union[str, bytes, os.PathLike] = None,\n    ):\n        super().__init__()\n        self.input_size = input_size\n        self.align_long_axis = align_long_axis\n        self.window_size = window_size\n        self.encoder_layer = encoder_layer\n\n        self.to_tensor = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n            ]\n        )\n\n        self.model = SwinTransformer(\n            img_size=self.input_size,\n            depths=self.encoder_layer,\n            window_size=self.window_size,\n            patch_size=4,\n            embed_dim=128,\n            num_heads=[4, 8, 16, 32],\n            num_classes=0,\n        )\n        self.model.norm = None\n\n        # Swin 가중치 초기화\n        if not name_or_path:\n            swin_state_dict = timm.create_model(\"swin_base_patch4_window12_384\", pretrained=True).state_dict()\n            new_swin_state_dict = self.model.state_dict()\n            for x in new_swin_state_dict:\n                if x.endswith(\"relative_position_index\") or x.endswith(\"attn_mask\"):\n                    pass\n                elif (\n                    x.endswith(\"relative_position_bias_table\")\n                    and self.model.layers[0].blocks[0].attn.window_size[0] != 12\n                ):\n                    pos_bias = swin_state_dict[x].unsqueeze(0)[0]\n                    old_len = int(math.sqrt(len(pos_bias)))\n                    new_len = int(2 * window_size - 1)\n                    pos_bias = pos_bias.reshape(1, old_len, old_len, -1).permute(0, 3, 1, 2)\n                    pos_bias = F.interpolate(pos_bias, size=(new_len, new_len), mode=\"bicubic\", align_corners=False)\n                    new_swin_state_dict[x] = pos_bias.permute(0, 2, 3, 1).reshape(1, new_len ** 2, -1).squeeze(0)\n                else:\n                    new_swin_state_dict[x] = swin_state_dict[x]\n            self.model.load_state_dict(new_swin_state_dict)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        매개변수:\n            x: (배치 크기, 채널 수, 높이, 너비)\n        \"\"\"\n        x = self.model.patch_embed(x)\n        x = self.model.pos_drop(x)\n        x = self.model.layers(x)\n        return x\n    ...\n    ...\n```\n\n## 디코더\n\n<div class=\"content-ad\"></div>\n\n도넛은 디코더로 BART를 사용합니다.\n\n```js\nclass BARTDecoder(nn.Module):\n    \"\"\"\n    다국어 BART 기반의 도넛 디코더\n    사전 훈련된 다국어 BART 모델의 초기 가중치와 구성을 설정하고,\n    이를 도넛 디코더로 수정해 세부 구성을 변경합니다.\n\n    Args:\n        decoder_layer:\n            BARTDecoder의 레이어 수\n        max_position_embeddings:\n            훈련할 최대 시퀀스 길이\n        name_or_path:\n            huggingface.co에 등록되어 있거나 로컬에 저장된 사전 훈련 모델 이름,\n            그렇지 않은 경우 `hyunwoongko/asian-bart-ecjk`를 사용합니다 (`transformers`)\n    \"\"\"\n\n    def __init__(\n        self, decoder_layer: int, max_position_embeddings: int, name_or_path: Union[str, bytes, os.PathLike] = None\n    ):\n        super().__init__()\n        self.decoder_layer = decoder_layer\n        self.max_position_embeddings = max_position_embeddings\n\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained(\n            \"hyunwoongko/asian-bart-ecjk\" if not name_or_path else name_or_path\n        )\n\n        self.model = MBartForCausalLM(\n            config=MBartConfig(\n                is_decoder=True,\n                is_encoder_decoder=False,\n                add_cross_attention=True,\n                decoder_layers=self.decoder_layer,\n                max_position_embeddings=self.max_position_embeddings,\n                vocab_size=len(self.tokenizer),\n                scale_embedding=True,\n                add_final_layer_norm=True,\n            )\n        )\n        self.model.forward = self.forward  # 교차 어텐션을 가져오고 `generate` 함수 활용\n\n        self.model.config.is_encoder_decoder = True  # 교차 어텐션을 가져오기 위해\n        self.add_special_tokens([\"<sep/>\"])  # <sep/>은 JSON에서 목록을 나타내는 데 사용됨\n        self.model.model.decoder.embed_tokens.padding_idx = self.tokenizer.pad_token_id\n        self.model.prepare_inputs_for_generation = self.prepare_inputs_for_inference\n\n        # asian-bart로 가중치 초기화\n        if not name_or_path:\n            bart_state_dict = MBartForCausalLM.from_pretrained(\"hyunwoongko/asian-bart-ecjk\").state_dict()\n            new_bart_state_dict = self.model.state_dict()\n            for x in new_bart_state_dict:\n                if x.endswith(\"embed_positions.weight\") and self.max_position_embeddings != 1024:\n                    new_bart_state_dict[x] = torch.nn.Parameter(\n                        self.resize_bart_abs_pos_emb(\n                            bart_state_dict[x],\n                            self.max_position_embeddings\n                            + 2,  # https://github.com/huggingface/transformers/blob/v4.11.3/src/transformers/models/mbart/modeling_mbart.py#L118-L119\n                        )\n                    )\n                elif x.endswith(\"embed_tokens.weight\") or x.endswith(\"lm_head.weight\"):\n                    new_bart_state_dict[x] = bart_state_dict[x][: len(self.tokenizer), :]\n                else:\n                    new_bart_state_dict[x] = bart_state_dict[x]\n            self.model.load_state_dict(new_bart_state_dict)\n\n    ...\n    ...\n\n    def forward(\n        self,\n        input_ids,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        past_key_values: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        use_cache: bool = None,\n        output_attentions: Optional[torch.Tensor] = None,\n        output_hidden_states: Optional[torch.Tensor] = None,\n        return_dict: bool = None,\n    ):\n        \"\"\"\n        교차 어텐션을 가져오고 `generate` 함수를 활용하기 위한 포워드 함수\n\n        소스:\n        https://github.com/huggingface/transformers/blob/v4.11.3/src/transformers/models/mbart/modeling_mbart.py#L1669-L1810\n\n        Args:\n            input_ids: (배치 크기, 시퀀스 길이)\n            attention_mask: (배치 크기, 시퀀스 길이)\n            encoder_hidden_states: (배치 크기, 시퀀스 길이, 히든 크기)\n\n        Returns:\n            loss: (1, )\n            logits: (배치 크기, 시퀀스 길이, 히든 차원)\n            hidden_states: (배치 크기, 시퀀스 길이, 히든 크기)\n            decoder_attentions: (배치 크기, 헤드 수, 시퀀스 길이, 시퀀스 길이)\n            cross_attentions: (배치 크기, 헤드 수, 시퀀스 길이, 시퀀스 길이)\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.model.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.model.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.model.config.use_return_dict\n        outputs = self.model.model.decoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        logits = self.model.lm_head(outputs[0])\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n            loss = loss_fct(logits.view(-1, self.model.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return ModelOutput(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            decoder_attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )\n    ...\n    ...\n```\n\n도넛은 공개적으로 이용 가능한 사전 훈련된 다국어 BART 모델의 가중치를 사용해 초기화합니다.\n\n텍스트 디코더의 출력은 생성된 토큰 시퀀스입니다.\n\n<div class=\"content-ad\"></div>\n\n## 훈련\n\n사전 훈련\n\n사전 훈련의 목표는 다음 토큰 예측의 교차 엔트로피 손실을 최소화하는 것입니다. 이는 이미지와 이전 맥락을 함께 고려함으로써 달성됩니다. 이 작업은 의사 OCR 작업과 유사합니다. 모델은 주로 문서 이미지와 같은 시각적 말뭉치를 통해 시각 언어 모델로 훈련됩니다.\n\n사용된 훈련 데이터는 1100만 장의 스캔된 영어 문서 이미지인 IIT-CDIP입니다. 한편, 다국어 데이터를 생성하기 위해 합성 문서 생성기(SynthDoG)가 사용되었는데, 영어, 중국어, 일본어, 한국어를 포함한 데이터를 생성했습니다. 각 언어 당 50만 장의 이미지가 생성되었습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_0.png)\n\n생성된 예시는 그림 3에 나와 있습니다. 샘플은 여러 구성 요소로 구성됩니다: 배경, 문서, 텍스트, 레이아웃.\n\n- 배경 이미지는 ImageNet 샘플에서 가져옵니다.\n- 문서의 질감은 수집된 종이 사진에서 파생됩니다.\n- 단어와 구절은 위키피디아에서 샘플링합니다.\n- 레이아웃은 그리드를 무작위로 배치하는 간단한 규칙 기반 알고리즘에 의해 생성됩니다.\n\n또한 다양한 이미지 렌더링 기술을 사용하여 실제 문서를 흉내 냅니다.\n\n\n<div class=\"content-ad\"></div>\n\n또한, 표 4는 상업용 CLOVA OCR API를 통해 얻은 훈련 데이터의 라벨을 표시하고 있습니다.\n\n![이미지](/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_1.png)\n\n## 파인 튜닝\n\n파인 튜닝의 주된 목적은 하류 작업에 적응하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, 문서 분류 작업에서 디코더는 [START class][memo][END class] 토큰 시퀀스를 생성하는 방식으로 훈련됩니다. 이 시퀀스는 '\"class\": \"memo\"'와 같은 JSON 형식으로 직접 변환될 수 있습니다.\n\n# Nougat\n\n누가(Nougat)는 2023년 8월에 소개된 OCR이 필요 없는 end-to-end 작은 모델입니다. Nougat은 이미지의 내용을 직접 파싱할 수 있습니다. Nougat은 문학작품에서 스캔된 이미지나 PDF로 변환된 이미지를 입력으로 받아들이고, 결과를 마크다운 형식으로 출력합니다.\n\n## 모델 아키텍처\n\n<div class=\"content-ad\"></div>\n\n**노가(Nougat)**는 **도넛(Donut) 아키텍처** 위에 개발되었습니다. **도넛 아키텍처**를 기반으로 **신경망**을 통해 텍스트를 인식하며, 그림 5에서 시연된 것처럼 **OCR 관련 입력이나 모듈이 필요 없이** 암묵적으로 작동합니다.\n\n## 교육 데이터셋 구축\n\n**노가(Nougat)** 모델은 특히 혁신적이지는 않으며, 주요 초점은 **대규모 교육 데이터셋**을 구축하는 데 있습니다. 이는 매우 어려운 작업입니다.\n\n**노가(Nougat)**는 이미지와 **마크다운**의 쌍으로 이루어진 대규모 교육 데이터를 생성하여 비용 효율적인 접근 방식을 구현했습니다. 이는 **노가(Nougat)**가 배울 수 있는 가장 중요한 측면입니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 소스\n\nPDF 이미지와 마크다운 쌍을 포함한 대규모 데이터셋이 없어서, Nougat은 세 곳의 소스로부터 데이터셋을 구성했습니다: arXiv, PMC (PubMed Central) 및 IDL (Industry Documents Library), Figure 6에 나와 있습니다.\n\n![Figure 6](/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_2.png)\n\n전반적인 프로세스\n\n<div class=\"content-ad\"></div>\n\nArXiv 데이터는 대부분 TeX 소스 코드가 포함되어 있기 때문에 주로 사용됩니다. 처리 흐름은 도표 7에 나와 있습니다.\n\n도표 7에 설명된 대로, 주요 목표는 기존 자원, 즉 PDF 논문과 해당 TeX 소스 코드를 쌍으로 변환하는 것입니다. 각 쌍은 각 PDF 페이지에 대한 이미지와 해당 Markdown으로 구성됩니다.\n\n입력으로 이미지 가져오기\n\nPDF 페이지의 이미지를 얻는 과정은 비교적 간단합니다. PyPDFium2의 관련 API를 직접 사용하면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n```python\ndef rasterize_paper(\n    pdf: Union[Path, bytes],\n    outpath: Optional[Path] = None,\n    dpi: int = 96,\n    return_pil=False,\n    pages=None,\n) -> Optional[List[io.BytesIO]]:\n    \"\"\"\n    PDF 파일을 PNG 이미지로 래스터화합니다.\n\n    매개변수:\n        pdf (Path): PDF 파일의 경로입니다.\n        outpath (Optional[Path], optional): 출력 디렉토리입니다. None이면 PIL 이미지가 반환됩니다. 기본값은 None입니다.\n        dpi (int, optional): 출력 DPI입니다. 기본값은 96입니다.\n        return_pil (bool, optional): 디스크에 쓰는 대신 PIL 이미지를 반환할지 여부입니다. 기본값은 False입니다.\n        pages (Optional[List[int]], optional): 래스터화할 페이지입니다. None이면 모든 페이지가 래스터화됩니다. 기본값은 None입니다.\n\n    반환값:\n        Optional[List[io.BytesIO]]: `return_pil`이 True인 경우 PIL 이미지, 그렇지 않으면 None입니다.\n    \"\"\"\n    pils = []\n    if outpath is None:\n        return_pil = True\n    try:\n        if isinstance(pdf, (str, Path)):\n            pdf = pypdfium2.PdfDocument(pdf)\n        if pages is None:\n            pages = range(len(pdf))\n        renderer = pdf.render(\n            pypdfium2.PdfBitmap.to_pil,\n            page_indices=pages,\n            scale=dpi / 72,\n        )\n        for i, image in zip(pages, renderer):\n            if return_pil:\n                page_bytes = io.BytesIO()\n                image.save(page_bytes, \"bmp\")\n                pils.append(page_bytes)\n            else:\n                image.save((outpath / (\"%02d.png\" % (i + 1))), \"png\")\n    except Exception as e:\n        logging.error(e)\n    if return_pil:\n        return pils\n```\n\n<div class=\"content-ad\"></div>\n\n첫 번째 과제는 각 PDF 페이지에서 이미지로 된 학습 데이터와 해당하는 마크다운 레이블이 함께 있는 것 때문에 마크다운을 페이지별로 구분하는 방법을 찾는 것입니다.\n\n각 논문의 LaTeX 소스 파일이 다시 컴파일되지 않았기 때문에 LaTeX 컴파일러와 같은 방식으로 PDF 파일의 페이지 나누기를 자동으로 결정할 수 없습니다.\n\n이 목표를 달성하기 위해서 현재 사용 가능한 리소스를 활용해야 합니다. 전략은 원본 PDF 페이지의 텍스트와 마크다운 텍스트를 휴리스틱하게 매칭하는 것입니다.\n\n구체적으로는 먼저 PDFMiner를 사용하여 PDF에서 텍스트 라인을 추출하고, 그 다음 텍스트를 전처리하여 페이지 번호와 가능한 헤더 또는 푸터를 제거합니다. 그런 다음 PDF 라인을 입력으로 사용하고 페이지 번호를 레이블로 사용하여 tfidf_transformer 모델을 훈련시킵니다. 이후 훈련된 모델을 적용하여 마크다운을 단락으로 나누고 각 단락에 대해 페이지 번호를 예측합니다.\n\n<div class=\"content-ad\"></div>\n\n```python\ndef split_markdown(\n    doc: str,\n    pdf_file: str,\n    figure_info: Optional[List[Dict]] = None,\n    doc_fig: Dict[str, str] = {},\n    minlen: int = 3,\n    min_num_words: int = 22,\n    doc_paragraph_chars: int = 1000,\n    min_score: float = 0.75,\n    staircase: bool = True,\n) -> Tuple[List[str], Dict]:\n    ...\n    ...\n       if staircase:\n            # train bag of words\n            page_target = np.zeros(len(paragraphs))\n            page_target[num_paragraphs[1:-1] - 1] = 1\n            page_target = np.cumsum(page_target).astype(int)\n            model = BagOfWords(paragraphs, target=page_target)\n            labels = model(doc_paragraphs)\n\n            # fit stair case function\n            x = np.arange(len(labels))\n            stairs = Staircase(len(labels), labels.max() + 1)\n            stairs.fit(x, labels)\n            boundaries = (stairs.get_boundaries().astype(int)).tolist()\n            boundaries.insert(0, 0)\n        else:\n            boundaries = [0] * (len(pdf.pages))\n    ...\n    ...\n```\n\n마지막으로 마무리 작업을 합니다.\n\n두 번째 도전 과제는 PDF의 차트가 마크다운 파일의 위치와 정렬되지 않는 것입니다.\n\n이를 해결하기 위해 누가트는 먼저 pdffigures2를 사용하여 차트를 추출합니다. 인식된 제목은 TeX 소스 코드 내의 제목들과 Levenshtein 거리를 기반으로 일치시킵니다. 이 방법을 사용하면 각 그림 또는 표의 TeX 소스 코드 및 페이지 번호를 결정할 수 있습니다. Figure 7의 JSON 구조는 차트 제목과 해당 페이지 번호를 포함합니다.\n\n<div class=\"content-ad\"></div>\n\n마크다운이 개별 페이지로 분할되면 이전에 추출한 차트가 각 해당 페이지의 끝에 다시 삽입됩니다.\n\n```js\ndef split_markdown(\n    doc: str,\n    pdf_file: str,\n    figure_info: Optional[List[Dict]] = None,\n    doc_fig: Dict[str, str] = {},\n    minlen: int = 3,\n    min_num_words: int = 22,\n    doc_paragraph_chars: int = 1000,\n    min_score: float = 0.75,\n    staircase: bool = True,\n) -> Tuple[List[str], Dict]:\n    ...\n    ...\n\n    # 도표, 표 및 각주 다시 삽입\n    figure_tex = list(doc_fig.keys()), list(doc_fig.values())\n    if len(doc_fig) > 0:\n        iterator = figure_info.values() if type(figure_info) == dict else [figure_info]\n        for figure_list in iterator:\n            if not figure_list:\n                continue\n            for i, f in enumerate(figure_list):\n                if \"caption\" in f:\n                    fig_string = f[\"caption\"]\n                elif \"text\" in f:\n                    fig_string = f[\"text\"]\n                else:\n                    continue\n                ratios = []\n                for tex in figure_tex[1]:\n                    if f[\"figType\"] == \"Table\":\n                        tex = tex.partition(r\"\\end{table}\")[2]\n                    ratios.append(Levenshtein.ratio(tex, fig_string))\n                k = np.argmax(ratios)\n                if ratios[k] < 0.8:\n                    continue\n                if f[\"page\"] < len(out) and out[f[\"page\"]] != \"\":\n                    out[f[\"page\"]] += \"\\n\\n\" + remove_pretty_linebreaks(\n                        figure_tex[1][k].strip()\n                    )\n\n    for i in range(len(out)):\n        foot_match = re.findall(r\"\\[FOOTNOTE(.*?)\\]\\[ENDFOOTNOTE\\]\", out[i])\n        for match in foot_match:\n            out[i] = out[i].replace(\n                \"[FOOTNOTE%s][ENDFOOTNOTE]\" % match,\n                doc_fig.get(\"FOOTNOTE%s\" % match, \"\"),\n            )\n\n        out[i] = re.sub(r\"\\[(FIGURE|TABLE)(.*?)\\](.*?)\\[END\\1\\]\", \"\", out[i])\n    return out, meta\n```\n\n# Pix2Struct\n\nPix2Struct은 순수한 시각 언어 이해를 위해 특별히 설계된 사전 훈련된 이미지 - 텍스트 모델입니다. 또한 많은 하향 작업에 대해 세밀하게 조정할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 모델 구조\n\nPix2Struct은 ViT를 기반으로 한 이미지 인코더-텍스트 디코더입니다.\n\nPix2Struct의 구조는 논문에 그림으로 표시되어 있지 않으며 온라인에서도 찾을 수 없기 때문에, ViT 구조를 기반으로 한 참조 다이어그램을 제공합니다. Figure 8에 나와있는 것과 같이.\n\n\n![Pix2Struct Architecture](/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_3.png)\n\n\n<div class=\"content-ad\"></div>\n\n일반적인 ViT 방법을 사용할 때 입력 이미지를 미리 정의된 해상도로 조정한 후 고정 크기 블록을 추출하면 두 가지 부정적인 영향을 줄 수 있습니다:\n\n- 문서, 모바일 UI 및 그래픽과 같이 실제 종횡비가 크게 다를 수 있습니다.\n- 전달 작업으로 모델을 고해상도로 이동하는 것이 어렵습니다. 이는 모델이 사전 학습 중에 특정 해상도만 본다는 점에서 나타납니다.\n\n따라서 Pix2Struct는 Figure 9에서 보여지는 것처럼 입력 이미지의 종횡비를 보존하는 스케일링을 가능하게 하는 소규모 향상을 도입했습니다.\n\n<img src=\"/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_4.png\" />\n\n<div class=\"content-ad\"></div>\n\n## 사전 훈련 작업\n\nPix2Struct은 웹 페이지의 가림막이 적용된 스크린샷으로부터 HTML 기반 파싱을 예측하는 작업을 제안합니다.\n\n- 입력을 가리는 것은 그들의 동시 발생에 대한 공동 추론을 장려합니다.\n- 간소화된 HTML을 출력으로 사용하는 것은 텍스트, 이미지 및 레이아웃에 대한 명확한 신호를 제공하기 때문에 유리합니다.\n\n![이미지](/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_5.png)\n\n<div class=\"content-ad\"></div>\n\nFigure 10에 표시된 것처럼, Pix2Struct에 의해 제안된 스크린샷 구문 분석은 여러 잘 알려진 사전 학습 전략에서 신호를 효과적으로 결합합니다:\n\n- 미권한 부분 복원. 이 작업은 OCR과 유사하며 언어 이해를 위한 기본 기술입니다. Donut에서는 OCR 사전 학습을 위해 합성 렌더링이나 OCR 출력 사용이 제안되었습니다. Figure 10에서 `C++`을 예측하는 것이 이러한 학습 신호의 예입니다.\n- 가리기된 부분 복원. 이 작업은 BERT의 가리기된 언어 모델링과 유사합니다. 그러나 시각적 맥락은 종종 추가적인 강력한 단서를 제공합니다. Figure 10에서 `Python`을 예측하는 것은 이러한 유형의 신호의 예입니다.\n- 이미지에서 대체 텍스트 복원. 이미지 제목 전략의 사전 학습에 널리 사용되는 방법입니다. 이 접근 방식에서 모델은 웹 페이지를 추가적인 맥락으로 사용할 수 있습니다. Figure 10에 나타난 대로 img alt=C++를 예측하는 것이 이러한 학습 신호를 보여줍니다.\n\nPix2Struct는 두 가지 모델 변형을 사전 학습했습니다:\n\n- 282백만 개의 매개변수로 구성된 기본 모델.\n- 13억 개의 매개변수로 구성된 대형 모델.\n\n<div class=\"content-ad\"></div>\n\n## 사전 훈련 데이터셋\n\n사전 훈련의 목표는 Pix2Struct가 입력 이미지의 기본 구조를 나타내는 능력을 갖추는 것입니다. 이를 달성하기 위해 Pix2Struct는 C4 말뭉치의 URL을 기반으로 자체 감독적인 방식으로 입력 이미지와 대상 텍스트의 쌍을 생성합니다.\n\nPix2Struct는 HTML 소스 파일과 쌍을 이루는 8000만 개의 스크린샷을 수집했습니다. 이는 총 문서 수의 약 1/3에 해당합니다. 각 스크린샷은 폭이 1024 픽셀이며, 높이는 콘텐츠의 높이에 맞게 조정됩니다. 또한 얻어진 HTML 소스 파일은 단순화된 HTML로 변환될 것입니다.\n\n도표 11은 사전 훈련 데이터의 스크린샷을 보여주며, 실제 값과 예측된 파싱을 함께 제시합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_6.png)\n\n## Fine-tuning\n\nFine-tuning Pix2Struct starts with preprocessing the downstream data. This step guarantees that the image input and text output precisely reflect the task.\n\n![image](/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_7.png)\n\n\n<div class=\"content-ad\"></div>\n\n다운스트림 작업의 예는 그림 12에 나와 있습니다.\n\n전처리에 대해:\n\n- Screen2Words 캡션 작업의 경우 입력 이미지와 출력 텍스트를 직접 사용할 수 있습니다.\n- DocVQA 시각적 질문 응답 작업의 경우, Pix2Struct는 다중 모달 모델이 일반적으로 질문을 위한 특수 텍스트 채널을 예약하는 반면 질문을 원본 이미지의 맨 위에 제목으로 직접 제시합니다.\n- AI2D와 같은 객관식 답변의 경우, Pix2Struct는 제목의 일부로 질문에 포함하여 제시하기로 선택합니다.\n\n# 통찰과 생각\n\n<div class=\"content-ad\"></div>\n\n대표적인 OCR 무료 솔루션에 대한 소개는 여기까지입니다. 이제 통찰과 생각에 대해 이야기해 봅시다.\n\n## 사전 훈련 작업에 관하여\n\n이미지나 PDF에서 레이아웃, 텍스트, 의미 정보를 종합적으로 이해하기 위해, Donut, Nougat, 그리고 Pix2Struct는 유사한 훈련 작업을 개발했습니다:\n\n- Donut: 이미지 → JSON 형식\n- Nougat: 이미지 → Markdown\n- Pix2Struct: 마스크 처리된 이미지 → 간소화된 HTML\n\n<div class=\"content-ad\"></div>\n\n우리만의 OCR이 필요 없는 PDF 구문 분석 도구를 개발하려면, 먼저 훈련 작업을 설계해야 합니다. 원하는 출력 형식과 관련된 훈련 데이터를 획득하는 데 어려움이 있는 과제를 고려하는 것이 중요합니다.\n\n## 사전 훈련 데이터에 대해\n\n훈련 데이터는 OCR이 필요 없는 방법에서 중요합니다.\n\nDonut 및 Nougat의 훈련 데이터 획득은 (이미지, JSON) 및 (이미지, Markdown) 쌍이 쉽게 이용 가능하지 않기 때문에 도전적입니다.\n\n<div class=\"content-ad\"></div>\n\n반면에, Pix2Struct은 공개 데이터셋에서 제공된 웹 페이지를 직접 적용하여 데이터 획득을 더 편리하게 만든 것입니다. 그러나 Pix2Struct의 훈련 데이터는 웹 페이지에서 가져왔기 때문에 유해한 콘텐츠를 도입할 수도 있습니다. 이는 특히 다중 모달 모델에 민감할 수 있습니다. Pix2Struct은 아직 이러한 유해 콘텐츠를 다루기 위한 조치를 시행하지 않았습니다.\n\n만일 OCR 없는 PDF 구문 분석 도구를 개발하려고 한다면, 하나의 전략은 훈련을 위한 (입력, 출력) 쌍을 점진적으로 구축하는 데 공개 데이터를 활용하는 것입니다.\n\n뿐만 아니라, 입력 이미지의 적절한 해상도와 하나의 이미지에 포함할 PDF 페이지 수를 결정하는 것도 중요한 고려 사항입니다.\n\n## 성능 관련\n\n<div class=\"content-ad\"></div>\n\n도넛(Donut)과 Pix2Struct은 모두 다양한 하향 작업을 지원하는 일반 사전 학습 모델입니다. 따라서 그들의 평가 방법은 이러한 작업들의 벤치마크에 기반을 두고 있습니다.\n\nPix2Struct의 실험에 따르면, 그의 성능은 여러 작업에서 도넛보다 크게 우수하며 대부분의 작업에서 최신 기술(SOTA)을 초과합니다. 이는 아래 그림 13에서 보여집니다:\n\n![Figure 13](/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_8.png)\n\n그러나, 그림 13에 표시된 작업들은 앞서 우리가 정의한 PDF 구문 분석 작업과는 다릅니다. 이 부분에서 누가(Nougat)가 더 전문적입니다.\n\n<div class=\"content-ad\"></div>\n\n누가트는 마크다운의 전체 생산 과정에 초점을 맞추고 있습니다. 그래서 그 평가 체계는 Figure 14에 나와 있는 편집 거리, BLEU, METEOR 및 F-측정에 의존합니다.\n\n![이미지](/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_9.png)\n\n게다가, 누가트는 다른 도구보다 수식과 테이블과 같은 복잡한 요소를 LaTeX 소스 코드로 더 정확하게 파싱할 수 있습니다. 이를 Figures 15와 16에서 확인할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_10.png)\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_11.png)\n\nFurthermore, Nougat can conveniently acquire table captions and associate them with corresponding tables.\n\n## Pipeline-Based vs. OCR-Free\n\nFigure 17 compares the overall architecture and performance of two methods. The upper left illustrates the pipeline-based method, while Donut model is represented on the lower left.\n\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_12.png)\n\nFigure 17의 오른쪽에 나타난 것처럼, 도넛은 파이프라인 기반 방법에 비해 저장 공간을 적게 사용하고 더 높은 정확성을 제공합니다. 그러나 느린 속도로 작동합니다. 다른 OCR 무료 솔루션이 도넛과 유사합니다.\n\n## OCR-Free Small Model-Based Method의 제한사항\n\n- 파이프라인 기반 방법은 여러 모델을 사용하지만 각 모델은 가벼워요. 총 매개변수 수는 OCR 무료 모델보다 중요하게 적을 수 있습니다. 이 요소는 대규모 배포에 대해 도전을 제공할 수 있으며 OCR 무료 모델의 느린 구문 분석 속도로 이어질 수 있습니다. 예를 들어, 작은 모델이지만 Nougat의 매개변수 양은 250MB 또는 350MB입니다. 그러나 Nougat 논문에 명시된대로 생성 속도가 느립니다:\n\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_13.png)\n\n- 이 방법론을 위한 훈련 데이터셋을 구축하는 것은 비용이 많이 듭니다. 큰 규모의 이미지-텍스트 쌍을 구축해야하기 때문입니다. 더구나 더 많은 GPU와 더 오랜 훈련 기간이 필요해 기계 비용이 증가합니다.\n- 게다가 엔드 투 엔드 방법은 특정한 나쁜 케이스를 최적화하는 데 어려움이 있어 최적화 비용이 높아집니다. 파이프라인 기반 솔루션에서는 테이블 처리 모듈이 성능을 발휘하지 못할 경우 이 모듈만 최적화가 필요합니다. 그러나 엔드 투 엔드 솔루션에서는 모델 구조를 변경하지 않고 새로운 파인튜닝 데이터를 만들어야 합니다. 이로 인해 다른 시나리오에서 새로운 나쁜 케이스가 발생할 수 있습니다.\n\n# 결론\n\n본 글은 PDF 파싱에서의 OCR을 사용하지 않는 소형 모델 기반 방법에 대한 개요를 제공했습니다. 세 가지 대표적인 모델을 사용해 이 접근 방식을 탐구하며 상세한 소개와 도출된 통찰을 제공했습니다.\n\n<div class=\"content-ad\"></div>\n\n일반적으로 OCR을 사용하지 않는 작은 모델 기반 PDF 구문 분석 방법의 장점 중 하나는 중간 단계에서 발생할 수 있는 잠재적인 손상을 피할 수 있는 일괄 처리 과정입니다. 그러나 그 효과는 다중 모달 모델의 구조와 훈련 데이터의 품질에 크게 의존합니다. 또한 훈련 및 추론 속도가 느려 파이프라인 기반 방법보다는 실용적이지 않습니다. 이 방법의 해석 가능성 역시 파이프라인 기반 방법만큼 강하게 나타나지는 않습니다.\n\n개선이 필요하지만 OCR을 사용하지 않는 접근 방식은 표 및 수식 인식과 같은 영역에서 잘 수행됩니다. 이러한 강점은 우리가 자체 PDF 구문 분석 도구를 구축하는 데 유용한 통찰력을 제공합니다.\n\nPDF 구문 분석이나 문서 인텔리전스에 관심이 있다면 다른 기사를 확인해 보세요.\n\n그리고 최신 기사는 뉴스레터에서 찾을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 문서에 오류나 빠진 점이 있거나 공유하고 싶은 생각이 있다면 댓글 섹션에서 언급해 주세요.","ogImage":{"url":"/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_0.png"},"coverImage":"/assets/img/2024-06-19-DemystifyingPDFParsing03OCR-FreeSmallModel-BasedMethod_0.png","tag":["Tech"],"readingTime":27}],"page":"65","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}