{"pageProps":{"post":{"title":"트위터가 매일 40억 건의 이벤트를 실시간으로 처리하는 방법","description":"","date":"2024-06-19 01:40","slug":"2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily","content":"\n\n## 람다에서 카파로\n\n# 목차\n\n- 문맥과 과제\n- 이전 아키텍처\n- 새로운 아키텍처\n- 평가\n\n# 소개\n\n<div class=\"content-ad\"></div>\n\n몇 주 전에는 우리가 어떻게 Uber가 실시간 인프라를 처리하여 매일 수백만 건의 이벤트를 처리하는지 배웠습니다. 이번 주에는 데이터 실시간 처리 요구 사항에 대한 다른 대형 기술 회사의 처리 방법을 살펴볼 것입니다: 트위터.\n\n# 맥락과 도전 과제\n\n트위터는 매일 400조 건의 이벤트를 실시간으로 처리하고 페타바이트(PB)의 데이터를 생성합니다. 이벤트는 다양한 소스(하둡, 카프카, 구글 빅쿼리, 구글 클라우드 스토리지, 구글 퍼브섭 등)에서 발생합니다. 트위터는 데이터의 대규모 규모에 대응하기 위해 각 요구 사항에 특화된 내부 도구를 구축했습니다: 배치 처리용 Scalding, 스트림 처리용 Heron, 배치 및 실시간 처리용 TimeSeries AggregatoR 프레임워크, 데이터 소비용 데이터 액세스 레이어.\n\n기술의 견고성에도 불구하고 데이터의 성장은 인프라에 압력을 가합니다; 가장 현저한 예는 상호 작용 및 참여 파이프라인인데, 이는 대규모 데이터를 배치 및 실시간으로 처리합니다. 이 파이프라인은 Tweet와 사용자 상호 작용 데이터를 다양한 수준의 집계 및 메트릭스 차원을 사용하여 추출하기 위해 다양한 실시간 스트림 및 서버 및 클라이언트 로그에서 데이터를 수집하고 처리합니다. 이 파이프라인의 집계 데이터는 트위터의 광고 수익과 다양한 데이터 제품 서비스의 진실의 원천 역할을 합니다. 따라서 이 파이프라인은 낮은 지연 시간과 높은 정확성을 보장해야 합니다. 트위터가 이 임무를 처리하는 방법을 살펴봅시다.\n\n<div class=\"content-ad\"></div>\n\n# 오래된 아키텍처\n\n## 개요\n\n![이미지](/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_0.png)\n\n트위터는 처음에 람다 아키텍처를 사용했습니다. 이는 정확한 일괄 데이터 뷰를 제공하는 일괄 처리와 온라인 데이터를 보여주는 실시간 스트림 처리 두 개의 별도 파이프라인이 있습니다. 두 뷰 출력은 하루가 끝날 때 합쳐집니다. 트위터는 다음과 같이 아키텍처를 구축했습니다:\n\n<div class=\"content-ad\"></div>\n\n- Summingbird Platform: 제가 이해한 바로는, 이 플랫폼에는 Scalding과 Heron과 같은 여러 분산 엔진들과 MapReduce 로직을 정의하고 이를 해당 엔진에서 실행할 수 있도록 허용하는 전용 라이브러리가 포함되어 있습니다.\n- TimeSeries AggregatoR: 견고하고 확장 가능한 실시간 이벤트 시계열 집계 프레임워크.\n- Batch: 배치 파이프라인의 소스는 로그, 클라이언트 이벤트 또는 HDFS의 트윗 이벤트에서 나올 수 있습니다. Summingbird 플랫폼으로 데이터를 전처리하고 결과를 맨해튼 분산 저장 시스템에 저장하기 위해 많은 Scalding 파이프라인이 사용됩니다. 비용을 절감하기 위해 Twitter는 배치 파이프라인을 한 데이터 센터에 배치하고 데이터를 다른 2개 데이터 센터에 복제합니다.\n- 실시간: 실시간 파이프라인의 소스는 Kafka 주제에서 나옵니다. 데이터는 Summingbird 플랫폼 내의 Heron으로 \"흘러들어가\", 그런 다음 Heron의 결과가 Twitter Nighthawk 분산 캐시에 저장됩니다. 배치 파이프라인과 달리, 실시간 파이프라인은 3개 서로 다른 데이터 센터에 배포됩니다.\n- 배치 및 실시간 저장소 위에 쿼리 서비스가 있습니다.\n\n## 도전\n\n실시간 데이터의 대규모 및 높은 처리량으로 인해 데이터 손실 및 부정확성의 위험이 있습니다. 이벤트 스트림에 대한 처리 속도가 따라가지 못할 경우, Heron 토폴로지에서 백프레셔가 발생할 수 있습니다 (방향성을 갖는 비순환 그래프는 데이터 처리의 Heron 흐름을 나타냅니다). 시스템이 어느 정도 동압을 겪으면, Heron 볼트(일종의 노동자로 생각할 수 있음)가 지연이 누적되어 전체 시스템 대기 시간이 길어질 수 있습니다.\n\n뿐만 아니라, 백프레셔로 인해 많은 Heron 스트림 매니저 (스트림 매니저는 토폴로지 구성 요소 간의 데이터 라우팅을 관리함)가 실패할 수 있습니다. Twitter의 해결책은 스트림 매니저를 다시 시작하여 스트림 매니저를 복구하는 것입니다. 그러나, 재시작은 행사 손실을 일으킬 가능성이 있어 파이프라인의 전반적 정확성을 줄일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 새 아키텍처\n\n## 개요\n\n![이미지](/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_1.png)\n\n새로운 접근 방식으로 트위터는 Kappa 아키텍처를 사용하여 하나의 실시간 파이프라인으로 솔루션을 간소화했습니다. 이 아키텍처는 트위터 내부 및 Google Cloud Platform 솔루션을 활용할 것입니다:\n\n<div class=\"content-ad\"></div>\n\n- 온프레미스: 그들은 카프카 토픽 이벤트를 구글 파브섭 이벤트 형식으로 변환하는 전처리 서비스를 구축했습니다.\n- 구글 클라우드: 이벤트 들어오기 위해 파브섭을 사용하고, 중복 제거 및 실시간 집계에는 Dataflow 작업을 활용하며, 결과를 저장하기 위해 BigTable을 사용합니다.\n\n새로운 아키텍처의 프로세스 흐름은 다음과 같이 설명할 수 있습니다:\n\n- 단계 1: 소스 카프카 토픽에서 데이터를 소비하고 변환 및 필드 재매핑을 수행한 후 최종 결과를 중간 카프카 토픽으로 보냅니다.\n- 단계 2: 이벤트 프로세서는 중간 카프카 토픽에서 데이터를 파브섭 형식으로 변환하고 이벤트에 데이터 중복 제거를 위해 사용되는 UUID 및 처리 컨텍스트와 관련된 일부 메타정보를 추가합니다.\n- 단계 3: 이벤트 프로세서는 이벤트를 구글 파브섭 토픽으로 보냅니다. 트위터는 메시지가 구글 클라우드로 적어도 한 번 방식으로 전달되도록 PubSub을 게시하는 이 프로세스를 거의 무한정 재시도합니다.\n- 단계 4: 구글 Dataflow 작업은 파브섭에서 데이터를 처리합니다. Dataflow 작업자는 실시간으로 데이터 중복 제거 및 집계를 처리합니다.\n- 단계 5: Dataflow 작업자는 집계 결과를 BigTable에 기록합니다.\n\n# 평가\n\n<div class=\"content-ad\"></div>\n\n## 새로운 접근 방식의 성취\n\n- 오래된 아키텍처의 10초에서 10분 지연과 비교하여 대략 10초의 지연이 안정적으로 유지됩니다.\n- 실시간 파이프라인은 최대 ~100MB/s 인 이전 아키텍처 대비 대략 1GB/s의 처리량을 달성할 수 있습니다.\n- Google Pubsub에 최소 한 번 데이터 게시 및 데이터 흐름으로 부터의 중복 제거 작업으로 거의 정확히 한 번 처리 보장.\n- 일괄 처리 파이프라인 구축 비용 절감.\n- 더 높은 집계 정확도 달성.\n- 늦게 발생하는 이벤트 처리 기능.\n- 다시 시작 시 이벤트 손실 없음\n\n## 중복 비율 모니터링 방법\n\n<img src=\"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_2.png\" />\n\n<div class=\"content-ad\"></div>\n\n트위터는 두 개의 별도 Dataflow 파이프라인을 생성합니다: 하나는 Pubsub에서 원시 데이터를 직접 BigQuery로 전달하고, 다른 하나는 중복 제거된 이벤트 카운트를 BigQuery로 내보냅니다. 이 방식으로 Twitter는 중복 이벤트 백분율 및 중복 제거 후의 백분율 변경을 모니터링할 수 있습니다.\n\n## 이전 배치 파이프라인의 중복 제거된 개수를 새 Dataflow 파이프라인과 비교하는 방법은?\n\n![image](/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_3.png)\n\n- BigTable에 쓰는 것 외에도, 새 워크플로우는 중복 제거된 및 집계된 데이터를 BigQuery로 내보냅니다.\n- 트위터는 또한 이전 배치 데이터 파이프라인 결과를 BigQuery로 로드합니다.\n- 중복 카운트를 비교하기 위해 예약된 쿼리를 실행합니다.\n- 결과는 새로운 파이프라인 결과 중 95% 이상이 이전 배치 파이프라인과 정확히 일치한다는 것입니다. 5%의 차이는 주로 원래 배치 파이프라인이 지연된 이벤트를 버린 반면, 새 파이프라인은 효율적으로 포착할 수 있기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n# 마무리말\n\nTwitter가 새로운 Kappa 아키텍처로 전환함으로써, 예전 아키텍처와 비교하여 지연 시간과 정확성 면에서 크게 개선되었습니다. 더 나은 성능 뿐만 아니라, 새로운 아키텍처는 데이터 파이프라인을 간소화하여 스트림만을 유지했습니다.\n\n다음 블로그에서 뵙겠습니다.\n\n# 참고문헌\n\n<div class=\"content-ad\"></div>\n\n[1] Lu Zhang and Chukwudiuto Malife, 트위터에서 실시간으로 수십억 개의 이벤트 처리하기 (2021)","ogImage":{"url":"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_0.png"},"coverImage":"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_0.png","tag":["Tech"],"readingTime":5},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<h2>람다에서 카파로</h2>\n<h1>목차</h1>\n<ul>\n<li>문맥과 과제</li>\n<li>이전 아키텍처</li>\n<li>새로운 아키텍처</li>\n<li>평가</li>\n</ul>\n<h1>소개</h1>\n<div class=\"content-ad\"></div>\n<p>몇 주 전에는 우리가 어떻게 Uber가 실시간 인프라를 처리하여 매일 수백만 건의 이벤트를 처리하는지 배웠습니다. 이번 주에는 데이터 실시간 처리 요구 사항에 대한 다른 대형 기술 회사의 처리 방법을 살펴볼 것입니다: 트위터.</p>\n<h1>맥락과 도전 과제</h1>\n<p>트위터는 매일 400조 건의 이벤트를 실시간으로 처리하고 페타바이트(PB)의 데이터를 생성합니다. 이벤트는 다양한 소스(하둡, 카프카, 구글 빅쿼리, 구글 클라우드 스토리지, 구글 퍼브섭 등)에서 발생합니다. 트위터는 데이터의 대규모 규모에 대응하기 위해 각 요구 사항에 특화된 내부 도구를 구축했습니다: 배치 처리용 Scalding, 스트림 처리용 Heron, 배치 및 실시간 처리용 TimeSeries AggregatoR 프레임워크, 데이터 소비용 데이터 액세스 레이어.</p>\n<p>기술의 견고성에도 불구하고 데이터의 성장은 인프라에 압력을 가합니다; 가장 현저한 예는 상호 작용 및 참여 파이프라인인데, 이는 대규모 데이터를 배치 및 실시간으로 처리합니다. 이 파이프라인은 Tweet와 사용자 상호 작용 데이터를 다양한 수준의 집계 및 메트릭스 차원을 사용하여 추출하기 위해 다양한 실시간 스트림 및 서버 및 클라이언트 로그에서 데이터를 수집하고 처리합니다. 이 파이프라인의 집계 데이터는 트위터의 광고 수익과 다양한 데이터 제품 서비스의 진실의 원천 역할을 합니다. 따라서 이 파이프라인은 낮은 지연 시간과 높은 정확성을 보장해야 합니다. 트위터가 이 임무를 처리하는 방법을 살펴봅시다.</p>\n<div class=\"content-ad\"></div>\n<h1>오래된 아키텍처</h1>\n<h2>개요</h2>\n<p><img src=\"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_0.png\" alt=\"이미지\"></p>\n<p>트위터는 처음에 람다 아키텍처를 사용했습니다. 이는 정확한 일괄 데이터 뷰를 제공하는 일괄 처리와 온라인 데이터를 보여주는 실시간 스트림 처리 두 개의 별도 파이프라인이 있습니다. 두 뷰 출력은 하루가 끝날 때 합쳐집니다. 트위터는 다음과 같이 아키텍처를 구축했습니다:</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>Summingbird Platform: 제가 이해한 바로는, 이 플랫폼에는 Scalding과 Heron과 같은 여러 분산 엔진들과 MapReduce 로직을 정의하고 이를 해당 엔진에서 실행할 수 있도록 허용하는 전용 라이브러리가 포함되어 있습니다.</li>\n<li>TimeSeries AggregatoR: 견고하고 확장 가능한 실시간 이벤트 시계열 집계 프레임워크.</li>\n<li>Batch: 배치 파이프라인의 소스는 로그, 클라이언트 이벤트 또는 HDFS의 트윗 이벤트에서 나올 수 있습니다. Summingbird 플랫폼으로 데이터를 전처리하고 결과를 맨해튼 분산 저장 시스템에 저장하기 위해 많은 Scalding 파이프라인이 사용됩니다. 비용을 절감하기 위해 Twitter는 배치 파이프라인을 한 데이터 센터에 배치하고 데이터를 다른 2개 데이터 센터에 복제합니다.</li>\n<li>실시간: 실시간 파이프라인의 소스는 Kafka 주제에서 나옵니다. 데이터는 Summingbird 플랫폼 내의 Heron으로 \"흘러들어가\", 그런 다음 Heron의 결과가 Twitter Nighthawk 분산 캐시에 저장됩니다. 배치 파이프라인과 달리, 실시간 파이프라인은 3개 서로 다른 데이터 센터에 배포됩니다.</li>\n<li>배치 및 실시간 저장소 위에 쿼리 서비스가 있습니다.</li>\n</ul>\n<h2>도전</h2>\n<p>실시간 데이터의 대규모 및 높은 처리량으로 인해 데이터 손실 및 부정확성의 위험이 있습니다. 이벤트 스트림에 대한 처리 속도가 따라가지 못할 경우, Heron 토폴로지에서 백프레셔가 발생할 수 있습니다 (방향성을 갖는 비순환 그래프는 데이터 처리의 Heron 흐름을 나타냅니다). 시스템이 어느 정도 동압을 겪으면, Heron 볼트(일종의 노동자로 생각할 수 있음)가 지연이 누적되어 전체 시스템 대기 시간이 길어질 수 있습니다.</p>\n<p>뿐만 아니라, 백프레셔로 인해 많은 Heron 스트림 매니저 (스트림 매니저는 토폴로지 구성 요소 간의 데이터 라우팅을 관리함)가 실패할 수 있습니다. Twitter의 해결책은 스트림 매니저를 다시 시작하여 스트림 매니저를 복구하는 것입니다. 그러나, 재시작은 행사 손실을 일으킬 가능성이 있어 파이프라인의 전반적 정확성을 줄일 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<h1>새 아키텍처</h1>\n<h2>개요</h2>\n<p><img src=\"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_1.png\" alt=\"이미지\"></p>\n<p>새로운 접근 방식으로 트위터는 Kappa 아키텍처를 사용하여 하나의 실시간 파이프라인으로 솔루션을 간소화했습니다. 이 아키텍처는 트위터 내부 및 Google Cloud Platform 솔루션을 활용할 것입니다:</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>온프레미스: 그들은 카프카 토픽 이벤트를 구글 파브섭 이벤트 형식으로 변환하는 전처리 서비스를 구축했습니다.</li>\n<li>구글 클라우드: 이벤트 들어오기 위해 파브섭을 사용하고, 중복 제거 및 실시간 집계에는 Dataflow 작업을 활용하며, 결과를 저장하기 위해 BigTable을 사용합니다.</li>\n</ul>\n<p>새로운 아키텍처의 프로세스 흐름은 다음과 같이 설명할 수 있습니다:</p>\n<ul>\n<li>단계 1: 소스 카프카 토픽에서 데이터를 소비하고 변환 및 필드 재매핑을 수행한 후 최종 결과를 중간 카프카 토픽으로 보냅니다.</li>\n<li>단계 2: 이벤트 프로세서는 중간 카프카 토픽에서 데이터를 파브섭 형식으로 변환하고 이벤트에 데이터 중복 제거를 위해 사용되는 UUID 및 처리 컨텍스트와 관련된 일부 메타정보를 추가합니다.</li>\n<li>단계 3: 이벤트 프로세서는 이벤트를 구글 파브섭 토픽으로 보냅니다. 트위터는 메시지가 구글 클라우드로 적어도 한 번 방식으로 전달되도록 PubSub을 게시하는 이 프로세스를 거의 무한정 재시도합니다.</li>\n<li>단계 4: 구글 Dataflow 작업은 파브섭에서 데이터를 처리합니다. Dataflow 작업자는 실시간으로 데이터 중복 제거 및 집계를 처리합니다.</li>\n<li>단계 5: Dataflow 작업자는 집계 결과를 BigTable에 기록합니다.</li>\n</ul>\n<h1>평가</h1>\n<div class=\"content-ad\"></div>\n<h2>새로운 접근 방식의 성취</h2>\n<ul>\n<li>오래된 아키텍처의 10초에서 10분 지연과 비교하여 대략 10초의 지연이 안정적으로 유지됩니다.</li>\n<li>실시간 파이프라인은 최대 ~100MB/s 인 이전 아키텍처 대비 대략 1GB/s의 처리량을 달성할 수 있습니다.</li>\n<li>Google Pubsub에 최소 한 번 데이터 게시 및 데이터 흐름으로 부터의 중복 제거 작업으로 거의 정확히 한 번 처리 보장.</li>\n<li>일괄 처리 파이프라인 구축 비용 절감.</li>\n<li>더 높은 집계 정확도 달성.</li>\n<li>늦게 발생하는 이벤트 처리 기능.</li>\n<li>다시 시작 시 이벤트 손실 없음</li>\n</ul>\n<h2>중복 비율 모니터링 방법</h2>\n<img src=\"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_2.png\">\n<div class=\"content-ad\"></div>\n<p>트위터는 두 개의 별도 Dataflow 파이프라인을 생성합니다: 하나는 Pubsub에서 원시 데이터를 직접 BigQuery로 전달하고, 다른 하나는 중복 제거된 이벤트 카운트를 BigQuery로 내보냅니다. 이 방식으로 Twitter는 중복 이벤트 백분율 및 중복 제거 후의 백분율 변경을 모니터링할 수 있습니다.</p>\n<h2>이전 배치 파이프라인의 중복 제거된 개수를 새 Dataflow 파이프라인과 비교하는 방법은?</h2>\n<p><img src=\"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_3.png\" alt=\"image\"></p>\n<ul>\n<li>BigTable에 쓰는 것 외에도, 새 워크플로우는 중복 제거된 및 집계된 데이터를 BigQuery로 내보냅니다.</li>\n<li>트위터는 또한 이전 배치 데이터 파이프라인 결과를 BigQuery로 로드합니다.</li>\n<li>중복 카운트를 비교하기 위해 예약된 쿼리를 실행합니다.</li>\n<li>결과는 새로운 파이프라인 결과 중 95% 이상이 이전 배치 파이프라인과 정확히 일치한다는 것입니다. 5%의 차이는 주로 원래 배치 파이프라인이 지연된 이벤트를 버린 반면, 새 파이프라인은 효율적으로 포착할 수 있기 때문입니다.</li>\n</ul>\n<div class=\"content-ad\"></div>\n<h1>마무리말</h1>\n<p>Twitter가 새로운 Kappa 아키텍처로 전환함으로써, 예전 아키텍처와 비교하여 지연 시간과 정확성 면에서 크게 개선되었습니다. 더 나은 성능 뿐만 아니라, 새로운 아키텍처는 데이터 파이프라인을 간소화하여 스트림만을 유지했습니다.</p>\n<p>다음 블로그에서 뵙겠습니다.</p>\n<h1>참고문헌</h1>\n<div class=\"content-ad\"></div>\n<p>[1] Lu Zhang and Chukwudiuto Malife, 트위터에서 실시간으로 수십억 개의 이벤트 처리하기 (2021)</p>\n</body>\n</html>\n"},"__N_SSG":true}