{"pageProps":{"post":{"title":"윈도우에서 NVIDIA GPU 가속을 활용해 llama-cpp-python 설치하기 간단 안내","description":"","date":"2024-06-20 14:56","slug":"2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide","content":"\n\n<img src=\"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_0.png\" />\n\n개발자이신가요? 로컬 LLM 개발을 위해 Windows에서 하드웨어 가속된 llama-cpp-python의 성능을 끌어올리고 싶으신가요? 더 이상 찾지 마세요! 이 안내서에서는 스텝별로 안내하여 본인이 설치하는 동안 겪은 문제점을 피할 수 있도록 도와드리겠습니다.\n\n# Prerequisites:\n\n- Visual Studio 설치:\n\n<div class=\"content-ad\"></div>\n\n- Windows용 C++ CMake 도구.\n- C++ 핵심 기능\n- Windows 10/11 SDK.\n\n![이미지](/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_1.png)\n\n2. CUDA Toolkit:\n\n- NVIDIA 공식 웹사이트에서 CUDA Toolkit 12.2를 다운로드하고 설치합니다.\n- nvcc --version 및 nvidia-smi로 설치 여부 확인합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_2.png\" />\n\n- 환경 변수에 CUDA_PATH (C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.2)를 추가하세요.\n\n## 설치 단계:\n\n새 명령 프롬프트를 열고 Python 환경을 활성화하세요 (예: conda 사용). 다음 명령을 실행하세요:\n\n<div class=\"content-ad\"></div>\n\n```shell\nCMAKE_ARGS=-DLLAMA_CUBLAS=on을 설정합니다.\nFORCE_CMAKE=1로 설정합니다.\npip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir\n\n# 컴파일 시 cuBLAS가 사용되고 있는지 확인하려면 --verbose를 사용하세요.\n```\n\n설치 중 --verbose 옵션을 추가하면 CUDA가 컴파일에 사용되는지 확인할 수 있습니다.\n\n![이미지](/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_3.png)\n\nCUDA가 올바르게 구성되지 않았다면, llama-cpp-python은 하드웨어 가속을 사용하지 않고 설치됩니다.\n\n<div class=\"content-ad\"></div>\n\n만약 Cuda가 감지되지만 No CUDA toolset founderror가 발생한다면 다음을 수행하세요:\n\n- 파일을 복사합니다: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.2\\extras\\visual_studio_integration\\MSBuildExtensions 에서 아래 경로로:\n(Enterprise 버전 인 경우) C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\n또는\n(Community 버전인 경우) C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\n\n```js\ncopy \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.2\\extras\\visual_studio_integration\\MSBuildExtensions\" \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\"\n```\n\n(설치에 기반하여 경로를 조정하세요)\n\n<div class=\"content-ad\"></div>\n\n# 테스트\n\n- 다음의 Python 코드를 실행하여 설치를 확인하세요:\n\n```js\nfrom llama_cpp import Llama\nllm = Llama(model_path=\"model.gguf\", n_gpu_layers=30, n_ctx=3584, n_batch=521, verbose=True)\n# GPU 및 모델에 맞게 n_gpu_layers를 조정하세요\noutput = llm(\"Q: 태양계의 행성을 말해주세요? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)\nprint(output)\n```\n\n![링크 텍스트](/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_4.png)\n\n<div class=\"content-ad\"></div>\n\n만약 설치가 올바르게 되었다면, 모델 속성에서 BLAS = 1 지표가 표시될 것입니다.\n\n# 결론:\n\n이 단계를 따르면, Windows 기기에 cuBLAS 가속을 사용하여 llama-cpp-python을 성공적으로 설치했을 것입니다. 이 안내서는 과정을 간소화하고 흔한 문제를 피할 수 있도록 돕는 것을 목표로 합니다.\n\n이제 향상된 성능을 갖는 로컬 llama 개발에 뛰어들 준비가 되었습니다. GPU 오프로딩에 행운을 빕니다!","ogImage":{"url":"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_0.png"},"coverImage":"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_0.png","tag":["Tech"],"readingTime":3},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<img src=\"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_0.png\">\n<p>개발자이신가요? 로컬 LLM 개발을 위해 Windows에서 하드웨어 가속된 llama-cpp-python의 성능을 끌어올리고 싶으신가요? 더 이상 찾지 마세요! 이 안내서에서는 스텝별로 안내하여 본인이 설치하는 동안 겪은 문제점을 피할 수 있도록 도와드리겠습니다.</p>\n<h1>Prerequisites:</h1>\n<ul>\n<li>Visual Studio 설치:</li>\n</ul>\n<div class=\"content-ad\"></div>\n<ul>\n<li>Windows용 C++ CMake 도구.</li>\n<li>C++ 핵심 기능</li>\n<li>Windows 10/11 SDK.</li>\n</ul>\n<p><img src=\"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_1.png\" alt=\"이미지\"></p>\n<ol start=\"2\">\n<li>CUDA Toolkit:</li>\n</ol>\n<ul>\n<li>NVIDIA 공식 웹사이트에서 CUDA Toolkit 12.2를 다운로드하고 설치합니다.</li>\n<li>nvcc --version 및 nvidia-smi로 설치 여부 확인합니다.</li>\n</ul>\n<div class=\"content-ad\"></div>\n<img src=\"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_2.png\">\n<ul>\n<li>환경 변수에 CUDA_PATH (C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.2)를 추가하세요.</li>\n</ul>\n<h2>설치 단계:</h2>\n<p>새 명령 프롬프트를 열고 Python 환경을 활성화하세요 (예: conda 사용). 다음 명령을 실행하세요:</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-shell\">CMAKE_ARGS=-DLLAMA_CUBLAS=on을 설정합니다.\nFORCE_CMAKE=1로 설정합니다.\npip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir\n<span class=\"hljs-meta prompt_\">\n# </span><span class=\"bash\">컴파일 시 cuBLAS가 사용되고 있는지 확인하려면 --verbose를 사용하세요.</span>\n</code></pre>\n<p>설치 중 --verbose 옵션을 추가하면 CUDA가 컴파일에 사용되는지 확인할 수 있습니다.</p>\n<p><img src=\"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_3.png\" alt=\"이미지\"></p>\n<p>CUDA가 올바르게 구성되지 않았다면, llama-cpp-python은 하드웨어 가속을 사용하지 않고 설치됩니다.</p>\n<div class=\"content-ad\"></div>\n<p>만약 Cuda가 감지되지만 No CUDA toolset founderror가 발생한다면 다음을 수행하세요:</p>\n<ul>\n<li>파일을 복사합니다: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.2\\extras\\visual_studio_integration\\MSBuildExtensions 에서 아래 경로로:\n(Enterprise 버전 인 경우) C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\n또는\n(Community 버전인 경우) C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations</li>\n</ul>\n<pre><code class=\"hljs language-js\">copy <span class=\"hljs-string\">\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.2\\extras\\visual_studio_integration\\MSBuildExtensions\"</span> <span class=\"hljs-string\">\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\"</span>\n</code></pre>\n<p>(설치에 기반하여 경로를 조정하세요)</p>\n<div class=\"content-ad\"></div>\n<h1>테스트</h1>\n<ul>\n<li>다음의 Python 코드를 실행하여 설치를 확인하세요:</li>\n</ul>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> llama_cpp <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Llama</span>\nllm = <span class=\"hljs-title class_\">Llama</span>(model_path=<span class=\"hljs-string\">\"model.gguf\"</span>, n_gpu_layers=<span class=\"hljs-number\">30</span>, n_ctx=<span class=\"hljs-number\">3584</span>, n_batch=<span class=\"hljs-number\">521</span>, verbose=<span class=\"hljs-title class_\">True</span>)\n# <span class=\"hljs-variable constant_\">GPU</span> 및 모델에 맞게 n_gpu_layers를 조정하세요\noutput = <span class=\"hljs-title function_\">llm</span>(<span class=\"hljs-string\">\"Q: 태양계의 행성을 말해주세요? A: \"</span>, max_tokens=<span class=\"hljs-number\">32</span>, stop=[<span class=\"hljs-string\">\"Q:\"</span>, <span class=\"hljs-string\">\"\\n\"</span>], echo=<span class=\"hljs-title class_\">True</span>)\n<span class=\"hljs-title function_\">print</span>(output)\n</code></pre>\n<p><img src=\"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_4.png\" alt=\"링크 텍스트\"></p>\n<div class=\"content-ad\"></div>\n<p>만약 설치가 올바르게 되었다면, 모델 속성에서 BLAS = 1 지표가 표시될 것입니다.</p>\n<h1>결론:</h1>\n<p>이 단계를 따르면, Windows 기기에 cuBLAS 가속을 사용하여 llama-cpp-python을 성공적으로 설치했을 것입니다. 이 안내서는 과정을 간소화하고 흔한 문제를 피할 수 있도록 돕는 것을 목표로 합니다.</p>\n<p>이제 향상된 성능을 갖는 로컬 llama 개발에 뛰어들 준비가 되었습니다. GPU 오프로딩에 행운을 빕니다!</p>\n</body>\n</html>\n"},"__N_SSG":true}