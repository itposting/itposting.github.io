{"pageProps":{"post":{"title":"작은 머신러닝  XGBoost 회귀","description":"","date":"2024-06-19 05:58","slug":"2024-06-19-TinyMLXGBoostRegression","content":"\n\n수학적 기초부터 엣지 구현까지\n\n# 소셜 미디어:\n\n👨🏽‍💻 Github: thommaskevin/TinyML (github.com)\n👷🏾 Linkedin: Thommas Kevin | LinkedIn\n📽 Youtube: Thommas Kevin — YouTube\n👨🏻‍🏫 연구 그룹: Conecta.ai (ufrn.br)\n\n![이미지](/assets/img/2024-06-19-TinyMLXGBoostRegression_0.png)\n\n<div class=\"content-ad\"></div>\n\n## 요약\n\n### 1 - XGBoost 회귀 이론\n\n보완적으로, 부스팅은 일련의 모델 집합 𝑡가 순차적으로 훈련되는 앙상블 접근 방식을 나타냅니다. 각 모델 𝑡는 이전 모델, 𝑡−1에서 발견된 결함을 보정하는 목적으로 설계되었습니다.\n\n타겟 값 yᵢ와 샘플 xᵢ에 대한 모델 𝑡의 예측 ŷᵢᵗ을 고려하고, 평균 제곱 오차 (MSE) 등의 일반적인 오류 함수 l로 나타내고, 총 샘플 수를 n으로 표시할 때, 반복 t에서의 모델의 오류(또는 손실)는 다음과 같이 정의됩니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_1.png\" />\n\n모델이 단계적으로 구축되었다는 것을 관찰할 수 있습니다. t 단계에서의 예측은 t-1 단계에서의 예측에 새 모델 fₜ의 예측을 더한 결과입니다:\n\n<img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_2.png\" />\n\n우리는 모델의 복잡성을 조절하는 데 기여하는 정규화항을 도입할 것이며(나중에 이 항의 구체적인 기능이 명확해질 것입니다).\n\n<div class=\"content-ad\"></div>\n\nXGBoost의 기본 개념은 각 트리의 포함이 전략적이라는 것을 전제로 합니다: 목표는 항상 오차를 최소화하는 최적의 트리를 구축하는 것입니다. 이를 위해, 우리는 함수 L을 최적화 문제로 다룰 것이며, 결국 L을 최소화하는 fₜ를 결정하려고 합니다. 그러나 이 작업의 복잡성은 오차 함수 l을 선택하는 데 따라 다를 수 있습니다.\n\n따라서 우리는 이 함수를 Taylor 전개를 통해 간소화하기로 결정했습니다. 어떤 무한 차별화 가능한 함수도 다음 형식으로 표현할 수 있다는 것이 널리 인정받았습니다:\n\n![수식](/assets/img/2024-06-19-TinyMLXGBoostRegression_3.png)\n\n중간 단계에서 시리즈를 자르면 함수의 근사치를 얻을 수 있습니다. 현재 상황에서는 확장을 둘째 차수에서 중지하기로 선택했습니다.\n\n<div class=\"content-ad\"></div>\n\n![image1](/assets/img/2024-06-19-TinyMLXGBoostRegression_4.png)\n\ngᵢ (gradient)와 hᵢ (Hessian)로 도함수를 대체할 것입니다:\n\n![image2](/assets/img/2024-06-19-TinyMLXGBoostRegression_5.png)\n\n만약 이 방정식을 최소화하는 fₜ를 찾는 것이 목적이라면, 상수항인 l은 필요하지 않습니다. 따라서 l을 버리면 다음과 같이 됩니다:\n\n<div class=\"content-ad\"></div>\n\n\n![링크 텍스트](/assets/img/2024-06-19-TinyMLXGBoostRegression_6.png)\n\nXGBoost의 주목할만한 특성 중 하나는 손실 함수가 두 번 미분 가능해야 한다는 요구사항입니다. 특정 문제에 대해 사용자 정의 오류 함수를 이용하여 XGBoost를 적용하려는 경우, 오류 계산 뿐만 아니라 그레이디언트(일차 도함수) 및 헤시안(이차 도함수)에 대한 정보도 필요하다는 점을 명심하는 것이 중요합니다.\n\n## 1.1 — 의사 결정 트리\n\n의사 결정 트리의 작동을 고려할 때, 방정식 L을 다시 쓸 필요가 있습니다. 각 샘플 xᵢ가 leaf j와 연관되어 있음을 알 수 있습니다. 따라서 각 leaf에 대해 샘플이 포함된 집합 인덱스 Iⱼ를 만들 수 있습니다. \n\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_7.png)\n\nIⱼ가 정의되어 있으며, Iⱼ에 속하는 각 인덱스 i에 대해 샘플 xᵢ가 통과한 결정 경로 q는 잎 j로 이어진다.\n\n또한, 모델이 샘플 xᵢ에 대해 응답하는 것이 xᵢ가 속한 잎에서 관련된 가중치임을 알 수 있습니다:\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_8.png)\n\n\n<div class=\"content-ad\"></div>\n\n그 결과, 방정식의 일부 용어를 다시 정의할 수 있습니다:\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_9.png)\n\n대체를 수행하면 다음과 같이 얻을 수 있습니다:\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_10.png)\n\n<div class=\"content-ad\"></div>\n\n정규화 항도 확장할 거에요:\n\n![](/assets/img/2024-06-19-TinyMLXGBoostRegression_11.png)\n\n## 1.2 — 예측 오류 최적화\n\n나무의 모든 리프를 고려하는 대신에, 특정 리프에 초점을 맞출 거에요. 이 리프는 j로 표시돼요.\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-06-19-TinyMLXGBoostRegression_12.png)\n\nThe objective is to find the set of weights w that minimizes L. This may seem challenging at first glance, but let’s analyze it more closely.\n\n![Image 2](/assets/img/2024-06-19-TinyMLXGBoostRegression_13.png)\n\nAs previously noted, our error function for a leaf is quadratic, implying that the minimum is determined by the inflection point of the curve, where the first derivative is equal to zero.\n\n\n<div class=\"content-ad\"></div>\n\n<table>\n  <tr>\n    <td><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_14.png\" /></td>\n  </tr>\n</table>\n\nwᵈ를 고립시키면 다음과 같이 됩니다:\n\n<table>\n  <tr>\n    <td><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_15.png\" /></td>\n  </tr>\n</table>\n\n이제 임의의 리프에 대해 최적 가중치를 제공하는 식을 확인했습니다. 따라서 L에 대한 우리의 식에 이 식을 대입함으로써 우리는 다음을 얻습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_16.png)\n\n이전 방정식은 각 새 트리의 각 분리를 평가하는 데 사용됩니다. 엔트로피나 지니 계수가 전통적으로 의사결정 트리 구성에 사용되는 방법과 마찬가지로 분리에서 양쪽 노드인 왼쪽 노드와 오른쪽 노드가 생성됩니다. 분할별 이득은 새로운 리프인 Lₗ(왼쪽)과 Lᵣ(오른쪽)의 합을 이전 오차인 Lₜ에서 뺀 것으로 정의됩니다. (우리가 Leaf가 하나만 분석하므로 T=1이라고 가정합니다.)\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_17.png)\n\n\n<div class=\"content-ad\"></div>\n\n## 1.3 — 하이퍼파라미터 튜닝\n\n이러한 방정식을 이해하면 XGBoost의 하이퍼파라미터와 기능을 더 잘 이해할 수 있습니다.\n\nreg_lambda: 이 파라미터는 잎의 가중치에 영향을 미치며, 값이 클수록 가중치의 절대값이 작아집니다. 이러한 이유로 𝜆은 모델의 복잡성을 제어하는 매개변수로, 가중치가 너무 커지는 것을 방지합니다. 보다 정확히는 L2 정규화입니다.\n\n![이미지](/assets/img/2024-06-19-TinyMLXGBoostRegression_18.png)\n\n<div class=\"content-ad\"></div>\n\n- reg_alpha: 분모를 제로에 가깝게 만들어서 중요성이 적은 트리 또는 분할을 제외하는 효과가 있습니다. 유도된 값의 모듈리(0보다 작을 때 -1, 0보다 클 때 1)의 행동으로 인해 가중 함수가 두 가지 경우로 나누어짐을 언급해야 합니다.\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_19.png)\n\n- gamma: 𝛾는 분할이 발생하는 최솟값으로, 𝛾보다 낮은 값은 결과적으로 부정적인 이득이 발생하여 실제 결과를 악화시킬 수 있으므로 고려되지 않습니다.\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_20.png)\n\n<div class=\"content-ad\"></div>\n\n- learning_rate: 문 개선을 위해 각 가중치에 0에서 1 사이의 값을 곱하여 나무의 개별적인 중요성을 감소시키고 학습 과정을 늦춰 미래 나무의 포함 여지를 늘립니다.\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_21.png)\n\n여기서 η는 트리 ft의 전체 예측에 미치는 영향을 직접 조절하며 가중치 계산 방식을 수정하지 않습니다.\n\n- max_delta_step: 각 반복의 최대 절대 가중치를 상수 𝛿로 제한하여 가중치의 부호를 항상 보존합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-06-19-TinyMLXGBoostRegression_22.png)\n\n- max_child_weight: 자식 노드마다 ℎ의 합이 이 매개변수로 설정된 값보다 크기 때문에 분할이 수행됩니다. ℎ는 오차 함수(𝑙)의 도함수에 의해 결정됩니다. 따라서 ℎ의 값이 낮을 때는 해당 리프가 이미 충분히 \"순수\"하며 더 이상 분할할 필요가 없다는 것을 나타냅니다.\n\n![Image 2](/assets/img/2024-06-19-TinyMLXGBoostRegression_23.png)\n\n여기서 Python 구현에 사용 가능한 매개변수 전체 목록을 찾을 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 2— TinyML 구현\n\n위 예제를 통해 ESP32, Arduino, Raspberry 및 기타 다양한 마이크로컨트롤러 또는 IoT 장치에서 머신러닝 알고리즘을 구현할 수 있습니다.\n\n2.0 — requirements.txt 파일에 나열된 라이브러리 설치\n\n```js\n!pip install -r requirements.txt\n```\n\n<div class=\"content-ad\"></div>\n\n2.1 — 라이브러리 가져오기\n\n```js\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV\nimport m2cgen as m2c\nimport numpy as np\nfrom scipy.stats import uniform, randint\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom xgboost import plot_tree\n```\n\n2.2— 데이터셋 로드\n\n당뇨병 데이터셋은 Bradley Efron, Trevor Hastie, Iain Johnstone 및 Robert Tibshirani가 스탠포드 대학에서 만들었습니다. 그들의 당뇨병 진행 예측 연구에 사용되었습니다.\n\n<div class=\"content-ad\"></div>\n\n- 데이터셋은 임상 및 인구 통계 변수인 열 개의 기준 변수로 구성되어 있습니다:\n\n1. 나이\n\n2. 성별\n\n3. 체질량 지수 (BMI)\n\n<div class=\"content-ad\"></div>\n\n4. 평균 혈압\n\n5. S1 — TC, T-세포 (백혈구의 일종)\n\n6. S2 — LDL, 저밀도 리포닛\n\n7. S3 — HDL, 고밀도 리포닛\n\n<div class=\"content-ad\"></div>\n\n8. S4 - TCH, 총 콜레스테롤\n\n9. S5 - LTG, 혈청 트리글리세리드 수준의 로그 가능성\n\n10. S6 - 포도당, 혈당 수준\n\n- 데이터셋에는 442개의 인스턴스(환자)가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 대상 변수는 기준선 이후 1년 후의 질병 진행의 양을 양적으로 측정한 것입니다. 데이터 집합에 명시적으로 언급되지 않은 요소를 기반으로 질병 진행을 표현합니다. 이는 연속 변수입니다.\n\n```python\n# 데이터셋 불러오기\ndata = load_diabetes() # 데이터 불러오기\n\n# DataFrame 생성\ndf_diabetes = pd.DataFrame(data.data, columns=data.feature_names)\n\n# 대상 변수를 DataFrame에 추가\ndf_diabetes['target'] = data.target\n\n# NaN 값 제거\ndf = df_diabetes.dropna(axis='rows') # NaN 값 제거\n\n# DataFrame 표시\ndf_diabetes.head()\n```\n\n![이미지](/assets/img/2024-06-19-TinyMLXGBoostRegression_24.png)\n\n```python\ndf_diabetes.info()\n```\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_25.png)\n\n```js\ndf_diabetes.describe()\n```\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_26.png)\n\n2.3— Exploratory Data Analysis\n\n\n<div class=\"content-ad\"></div>\n\n```js\nsns.pairplot(df_diabetes)\n```\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_27.png)\n\n2.4— 데이터를 학습 및 테스트 세트로 분할\n\n```js\ndf = df_diabetes.iloc[:100,0:10]\n```\n\n<div class=\"content-ad\"></div>\n\n```js\nX=df.to_numpy()\n\ny=df_diabetes.iloc[:100,-1}\n```\n\n```js\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n```\n\n2.5 — Create the regressor model\n\n```js\nmodel = xgb.XGBRegressor(objective=\"reg:linear\", random_state=42)\n```\n\n<div class=\"content-ad\"></div>\n\n2.6 — 모델 훈련\n\n```js\nmodel.fit(X_train, y_train)\n```\n\n2.7 — 모델 최적화\n\nRandomizedSearchCV는 scikit-learn 라이브러리에서 제공하는 함수로, 머신 러닝 모델의 하이퍼파라미터 튜닝을 위해 교차 검증을 통해 주로 사용됩니다. 이 기술은 하이퍼파라미터의 폭넓은 탐색 영역을 다룰 때 유용하며, 가장 효과적인 값을 결정하는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n단계별 설명\n\n1. 매개변수 공간 정의:\n\nRandomizedSearchCV를 활용하기 전에, 모델의 하이퍼파라미터를 위한 탐색 공간을 지정해야 합니다. 특정 값의 그리드를 제공하는 대신, 각 하이퍼파라미터에 대해 분포를 정의합니다.\n\n2. 무작위 샘플링:\n\n<div class=\"content-ad\"></div>\n\nGridSearchCV와 같이 모든 가맹 별로 동시에 평가하는 것이 아니라, RandomizedSearchCV는 평가를 위해 일정한 조합을 무작위로 선택합니다. 이는 큰 탐색 공간을 다룰 때 유리합니다.\n\n3. 모델 훈련:\n\n랜덤으로 선택된 각 하이퍼파라미터 집합에 대해 RandomizedSearchCV는 교차 검증을 사용하여 모델을 훈련합니다. 데이터는 폴드로 나누어지며, 모델은 일부 폴드에서 훈련되고 나머지 폴드에서 평가됩니다.\n\n4. 성능 평가:\n\n<div class=\"content-ad\"></div>\n\n성능은 특정 메트릭(예: 정확도, F1 점수)을 사용하여 측정됩니다. 목표는 주어진 문제에 따라 이 메트릭을 최대화하거나 최소화하는 하이퍼파라미터를 찾는 것입니다(예: 분류 문제에서 정확도를 최대화).\n\n5. 최적 모델 선택:\n\n랜덤 서치를 완료하면 RandomizedSearchCV가 교차 검증 중 가장 우수한 평균 성능을 보인 하이퍼파라미터 세트를 반환합니다.\n\nRandomizedSearchCV를 사용하면 대규모 탐색 공간을 다룰 때 특히 모든 가능한 조합을 평가하는 그리드 서치(GridSearchCV)와 비교하여 계산 시간을 단축할 수 있습니다. 이 효율성은 모든 가능한 조합을 평가하는 대신 하이퍼파라미터 공간의 무작위 샘플을 탐색하는 데서 비롯됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nparams = {\n    \"colsample_bytree\": uniform(0.7, 0.3),\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), # 기본값 0.1 \n    \"max_depth\": randint(2, 6), # 기본값 3\n    \"n_estimators\": randint(100, 150), # 기본값 100\n    \"subsample\": uniform(0.6, 0.4)\n}\n\nbest_model = RandomizedSearchCV(model, param_distributions=params, random_state=42, n_iter=200, cv=3, verbose=1, n_jobs=1, return_train_score=True)\n\nbest_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_test, y_test)]\n```\n\n![이미지](/assets/img/2024-06-19-TinyMLXGBoostRegression_28.png)\n\n```js\ndef report_best_scores(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"순위 {0}인 모델\".format(i))\n            print(\"평균 검증 점수: {0:.3f} (표준편차: {1:.3f})\".format(\n                results['mean_test_score'][candidate],\n                results['std_test_score'][candidate]))\n            best_params = results['params'][candidate]\n            print(\"찾은 최적의 매개변수:\")\n            for param, value in best_params.items():\n                print(\"  {0}: {1}\".format(param, value))\n            print(\"\")\n```\n\n```js\nreport_best_scores(best_model.cv_results_, 1)\n```\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-19-TinyMLXGBoostRegression_29.png)\n\n```js\nmodel =  xgb.XGBRegressor(objective=\"reg:linear\", max_depth= 5, learning_rate= 0.29302969102852483, gamma = 0.38122934287034527)\nmodel.fit(X_train, y_train)\n```\n\n2.8 — Visualization\n\n```js\nfig, ax = plt.subplots(figsize=(20, 10))\nplot_tree(model, num_trees=0, ax=ax)\nplt.show()\n```\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_30.png)\n\n2.9— 훈련 데이터로 모델 평가\n\n```js\nscore = model.score(X_train, y_train)\ntraining_predict = model.predict(X_train)\nmse = mean_squared_error(y_train, training_predict)\n\nprint(\"R-squared:\", score)\nprint(\"MSE: \", mse)\nprint(\"RMSE: \", mse**(1/2.0))\n```\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_31.png)\n\n\n<div class=\"content-ad\"></div>\n\n```js\nx_ax = range(len(y_train))\nplt.plot(x_ax, y_train, label=\"원본\")\nplt.plot(x_ax, training_predict, label=\"예측된 값\")\nplt.title(\"훈련 및 예측된 데이터\")\nplt.xlabel('X축')\nplt.ylabel('Y축')\nplt.legend(loc='best', fancybox=True, shadow=True)\nplt.grid(True)\nplt.show()\n```\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_32.png)\n\n2.10— 테스트 데이터로 모델 평가\n\n```js\nscore = model.score(X_test, y_test)\ntest_predict = model.predict(X_test)\nmse = mean_squared_error(y_test, test_predict)\n\nprint(\"R-squared:\", score)\nprint(\"MSE: \", mse)\nprint(\"RMSE: \", mse**(1/2.0))\n```\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_33.png\" />\n\n```js\nx_ax = range(len(y_test))\nplt.plot(x_ax, y_test, label=\"original\")\nplt.plot(x_ax, test_predict, label=\"predicted\")\nplt.title(\"Testing and predicted data\")\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend(loc='best',fancybox=True, shadow=True)\nplt.grid(True)\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_34.png\" />\n\n2.11 — 테스트 데이터를 사용하여 모델 평가하기\n\n\n<div class=\"content-ad\"></div>\n\n```js\ncode = m2c.export_to_c(model)\nprint(code)\n```\n\n![Image](/assets/img/2024-06-19-TinyMLXGBoostRegression_35.png)\n\n2.12 — 템플릿을 .h 파일에 저장합니다.\n\n```js\nwith open('./XGBRegressor.h', 'w') as file:\n    file.write(code)\n```\n\n<div class=\"content-ad\"></div>\n\n2.13 — 모델 배포\n\n이 예제를 통해 ESP32, 아두이노, 아두이노 Portenta H7 with Vision Shield, 라즈베리 파이 및 기타 다양한 마이크로컨트롤러 또는 IoT 장치에서 머신 러닝 알고리즘을 구현할 수 있습니다.\n\n2.13.1 — 완성된 아두이노 스케치\n\n```js\n#include \"XGBRegressor.h\"\n\n\nvoid setup() {\n  Serial.begin(115200);\n}\n\nvoid loop() {\n  double X_1[] = { 2.71782911e-02,  5.06801187e-02,  1.75059115e-02,\n                  -3.32135761e-02, -7.07277125e-03,  4.59715403e-02,\n                  -6.54906725e-02,  7.12099798e-02, -9.64332229e-02,\n                  -5.90671943e-02};\n  double result_1 = score(X_1);\n  Serial.print(\"입력 X1로 예측 결과 (실제 값 = 69):\");\n  Serial.println(String(result_1, 7));\n  delay(2000);\n}\n```\n\n<div class=\"content-ad\"></div>\n\n3.12 — 결과\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_36.png)\n\n전체 프로젝트: [TinyML/14_XGBRegression at main · thommaskevin/TinyML](github.com)\n\n## 만약 마음에 드셨다면, 제 커피 한 잔 사주세요 ☕️💰 (Bitcoin)\n\n<div class=\"content-ad\"></div>\n\n```plaintext\n코드: bc1qzydjy4m9yhmjjrkgtrzhsgmkq79qenvcvc7qzn\n\n![Image](/assets/img/2024-06-19-TinyMLXGBoostRegression_37.png)\n```","ogImage":{"url":"/assets/img/2024-06-19-TinyMLXGBoostRegression_0.png"},"coverImage":"/assets/img/2024-06-19-TinyMLXGBoostRegression_0.png","tag":["Tech"],"readingTime":14},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>수학적 기초부터 엣지 구현까지</p>\n<h1>소셜 미디어:</h1>\n<p>👨🏽‍💻 Github: thommaskevin/TinyML (github.com)\n👷🏾 Linkedin: Thommas Kevin | LinkedIn\n📽 Youtube: Thommas Kevin — YouTube\n👨🏻‍🏫 연구 그룹: Conecta.ai (ufrn.br)</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_0.png\" alt=\"이미지\"></p>\n<h2>요약</h2>\n<h3>1 - XGBoost 회귀 이론</h3>\n<p>보완적으로, 부스팅은 일련의 모델 집합 𝑡가 순차적으로 훈련되는 앙상블 접근 방식을 나타냅니다. 각 모델 𝑡는 이전 모델, 𝑡−1에서 발견된 결함을 보정하는 목적으로 설계되었습니다.</p>\n<p>타겟 값 yᵢ와 샘플 xᵢ에 대한 모델 𝑡의 예측 ŷᵢᵗ을 고려하고, 평균 제곱 오차 (MSE) 등의 일반적인 오류 함수 l로 나타내고, 총 샘플 수를 n으로 표시할 때, 반복 t에서의 모델의 오류(또는 손실)는 다음과 같이 정의됩니다:</p>\n<p>모델이 단계적으로 구축되었다는 것을 관찰할 수 있습니다. t 단계에서의 예측은 t-1 단계에서의 예측에 새 모델 fₜ의 예측을 더한 결과입니다:</p>\n<p>우리는 모델의 복잡성을 조절하는 데 기여하는 정규화항을 도입할 것이며(나중에 이 항의 구체적인 기능이 명확해질 것입니다).</p>\n<p>XGBoost의 기본 개념은 각 트리의 포함이 전략적이라는 것을 전제로 합니다: 목표는 항상 오차를 최소화하는 최적의 트리를 구축하는 것입니다. 이를 위해, 우리는 함수 L을 최적화 문제로 다룰 것이며, 결국 L을 최소화하는 fₜ를 결정하려고 합니다. 그러나 이 작업의 복잡성은 오차 함수 l을 선택하는 데 따라 다를 수 있습니다.</p>\n<p>따라서 우리는 이 함수를 Taylor 전개를 통해 간소화하기로 결정했습니다. 어떤 무한 차별화 가능한 함수도 다음 형식으로 표현할 수 있다는 것이 널리 인정받았습니다:</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_3.png\" alt=\"수식\"></p>\n<p>중간 단계에서 시리즈를 자르면 함수의 근사치를 얻을 수 있습니다. 현재 상황에서는 확장을 둘째 차수에서 중지하기로 선택했습니다.</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_4.png\" alt=\"image1\"></p>\n<p>gᵢ (gradient)와 hᵢ (Hessian)로 도함수를 대체할 것입니다:</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_5.png\" alt=\"image2\"></p>\n<p>만약 이 방정식을 최소화하는 fₜ를 찾는 것이 목적이라면, 상수항인 l은 필요하지 않습니다. 따라서 l을 버리면 다음과 같이 됩니다:</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_6.png\" alt=\"링크 텍스트\"></p>\n<p>XGBoost의 주목할만한 특성 중 하나는 손실 함수가 두 번 미분 가능해야 한다는 요구사항입니다. 특정 문제에 대해 사용자 정의 오류 함수를 이용하여 XGBoost를 적용하려는 경우, 오류 계산 뿐만 아니라 그레이디언트(일차 도함수) 및 헤시안(이차 도함수)에 대한 정보도 필요하다는 점을 명심하는 것이 중요합니다.</p>\n<h2>1.1 — 의사 결정 트리</h2>\n<p>의사 결정 트리의 작동을 고려할 때, 방정식 L을 다시 쓸 필요가 있습니다. 각 샘플 xᵢ가 leaf j와 연관되어 있음을 알 수 있습니다. 따라서 각 leaf에 대해 샘플이 포함된 집합 인덱스 Iⱼ를 만들 수 있습니다.</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_7.png\" alt=\"image\"></p>\n<p>Iⱼ가 정의되어 있으며, Iⱼ에 속하는 각 인덱스 i에 대해 샘플 xᵢ가 통과한 결정 경로 q는 잎 j로 이어진다.</p>\n<p>또한, 모델이 샘플 xᵢ에 대해 응답하는 것이 xᵢ가 속한 잎에서 관련된 가중치임을 알 수 있습니다:</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_8.png\" alt=\"image\"></p>\n<p>그 결과, 방정식의 일부 용어를 다시 정의할 수 있습니다:</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_9.png\" alt=\"image\"></p>\n<p>대체를 수행하면 다음과 같이 얻을 수 있습니다:</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_10.png\" alt=\"image\"></p>\n<p>정규화 항도 확장할 거에요:</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_11.png\" alt=\"\"></p>\n<h2>1.2 — 예측 오류 최적화</h2>\n<p>나무의 모든 리프를 고려하는 대신에, 특정 리프에 초점을 맞출 거에요. 이 리프는 j로 표시돼요.</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_12.png\" alt=\"Image 1\"></p>\n<p>The objective is to find the set of weights w that minimizes L. This may seem challenging at first glance, but let’s analyze it more closely.</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_13.png\" alt=\"Image 2\"></p>\n<p>As previously noted, our error function for a leaf is quadratic, implying that the minimum is determined by the inflection point of the curve, where the first derivative is equal to zero.</p>\n<p>wᵈ를 고립시키면 다음과 같이 됩니다:</p>\n<p>이제 임의의 리프에 대해 최적 가중치를 제공하는 식을 확인했습니다. 따라서 L에 대한 우리의 식에 이 식을 대입함으로써 우리는 다음을 얻습니다:</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_16.png\" alt=\"image\"></p>\n<p>이전 방정식은 각 새 트리의 각 분리를 평가하는 데 사용됩니다. 엔트로피나 지니 계수가 전통적으로 의사결정 트리 구성에 사용되는 방법과 마찬가지로 분리에서 양쪽 노드인 왼쪽 노드와 오른쪽 노드가 생성됩니다. 분할별 이득은 새로운 리프인 Lₗ(왼쪽)과 Lᵣ(오른쪽)의 합을 이전 오차인 Lₜ에서 뺀 것으로 정의됩니다. (우리가 Leaf가 하나만 분석하므로 T=1이라고 가정합니다.)</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_17.png\" alt=\"image\"></p>\n<h2>1.3 — 하이퍼파라미터 튜닝</h2>\n<p>이러한 방정식을 이해하면 XGBoost의 하이퍼파라미터와 기능을 더 잘 이해할 수 있습니다.</p>\n<p>reg_lambda: 이 파라미터는 잎의 가중치에 영향을 미치며, 값이 클수록 가중치의 절대값이 작아집니다. 이러한 이유로 𝜆은 모델의 복잡성을 제어하는 매개변수로, 가중치가 너무 커지는 것을 방지합니다. 보다 정확히는 L2 정규화입니다.</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_18.png\" alt=\"이미지\"></p>\n<ul>\n<li>reg_alpha: 분모를 제로에 가깝게 만들어서 중요성이 적은 트리 또는 분할을 제외하는 효과가 있습니다. 유도된 값의 모듈리(0보다 작을 때 -1, 0보다 클 때 1)의 행동으로 인해 가중 함수가 두 가지 경우로 나누어짐을 언급해야 합니다.</li>\n</ul>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_19.png\" alt=\"image\"></p>\n<ul>\n<li>gamma: 𝛾는 분할이 발생하는 최솟값으로, 𝛾보다 낮은 값은 결과적으로 부정적인 이득이 발생하여 실제 결과를 악화시킬 수 있으므로 고려되지 않습니다.</li>\n</ul>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_20.png\" alt=\"image\"></p>\n<ul>\n<li>learning_rate: 문 개선을 위해 각 가중치에 0에서 1 사이의 값을 곱하여 나무의 개별적인 중요성을 감소시키고 학습 과정을 늦춰 미래 나무의 포함 여지를 늘립니다.</li>\n</ul>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_21.png\" alt=\"image\"></p>\n<p>여기서 η는 트리 ft의 전체 예측에 미치는 영향을 직접 조절하며 가중치 계산 방식을 수정하지 않습니다.</p>\n<ul>\n<li>max_delta_step: 각 반복의 최대 절대 가중치를 상수 𝛿로 제한하여 가중치의 부호를 항상 보존합니다.</li>\n</ul>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_22.png\" alt=\"Image 1\"></p>\n<ul>\n<li>max_child_weight: 자식 노드마다 ℎ의 합이 이 매개변수로 설정된 값보다 크기 때문에 분할이 수행됩니다. ℎ는 오차 함수(𝑙)의 도함수에 의해 결정됩니다. 따라서 ℎ의 값이 낮을 때는 해당 리프가 이미 충분히 \"순수\"하며 더 이상 분할할 필요가 없다는 것을 나타냅니다.</li>\n</ul>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_23.png\" alt=\"Image 2\"></p>\n<p>여기서 Python 구현에 사용 가능한 매개변수 전체 목록을 찾을 수 있습니다.</p>\n<h1>2— TinyML 구현</h1>\n<p>위 예제를 통해 ESP32, Arduino, Raspberry 및 기타 다양한 마이크로컨트롤러 또는 IoT 장치에서 머신러닝 알고리즘을 구현할 수 있습니다.</p>\n<p>2.0 — requirements.txt 파일에 나열된 라이브러리 설치</p>\n<pre><code class=\"hljs language-js\">!pip install -r requirements.<span class=\"hljs-property\">txt</span>\n</code></pre>\n<p>2.1 — 라이브러리 가져오기</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> sklearn.<span class=\"hljs-property\">datasets</span> <span class=\"hljs-keyword\">import</span> load_diabetes\n<span class=\"hljs-keyword\">from</span> sklearn.<span class=\"hljs-property\">model_selection</span> <span class=\"hljs-keyword\">import</span> train_test_split\n<span class=\"hljs-keyword\">from</span> sklearn.<span class=\"hljs-property\">metrics</span> <span class=\"hljs-keyword\">import</span> mean_squared_error\n<span class=\"hljs-keyword\">from</span> sklearn.<span class=\"hljs-property\">model_selection</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">RandomizedSearchCV</span>\n<span class=\"hljs-keyword\">import</span> m2cgen <span class=\"hljs-keyword\">as</span> m2c\n<span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n<span class=\"hljs-keyword\">from</span> scipy.<span class=\"hljs-property\">stats</span> <span class=\"hljs-keyword\">import</span> uniform, randint\n<span class=\"hljs-keyword\">import</span> matplotlib.<span class=\"hljs-property\">pyplot</span> <span class=\"hljs-keyword\">as</span> plt\n<span class=\"hljs-keyword\">import</span> xgboost <span class=\"hljs-keyword\">as</span> xgb\n<span class=\"hljs-keyword\">from</span> xgboost <span class=\"hljs-keyword\">import</span> plot_tree\n</code></pre>\n<p>2.2— 데이터셋 로드</p>\n<p>당뇨병 데이터셋은 Bradley Efron, Trevor Hastie, Iain Johnstone 및 Robert Tibshirani가 스탠포드 대학에서 만들었습니다. 그들의 당뇨병 진행 예측 연구에 사용되었습니다.</p>\n<ul>\n<li>데이터셋은 임상 및 인구 통계 변수인 열 개의 기준 변수로 구성되어 있습니다:</li>\n</ul>\n<ol>\n<li>\n<p>나이</p>\n</li>\n<li>\n<p>성별</p>\n</li>\n<li>\n<p>체질량 지수 (BMI)</p>\n</li>\n</ol>\n<ol start=\"4\">\n<li>\n<p>평균 혈압</p>\n</li>\n<li>\n<p>S1 — TC, T-세포 (백혈구의 일종)</p>\n</li>\n<li>\n<p>S2 — LDL, 저밀도 리포닛</p>\n</li>\n<li>\n<p>S3 — HDL, 고밀도 리포닛</p>\n</li>\n</ol>\n<ol start=\"8\">\n<li>\n<p>S4 - TCH, 총 콜레스테롤</p>\n</li>\n<li>\n<p>S5 - LTG, 혈청 트리글리세리드 수준의 로그 가능성</p>\n</li>\n<li>\n<p>S6 - 포도당, 혈당 수준</p>\n</li>\n</ol>\n<ul>\n<li>데이터셋에는 442개의 인스턴스(환자)가 있습니다.</li>\n</ul>\n<ul>\n<li>대상 변수는 기준선 이후 1년 후의 질병 진행의 양을 양적으로 측정한 것입니다. 데이터 집합에 명시적으로 언급되지 않은 요소를 기반으로 질병 진행을 표현합니다. 이는 연속 변수입니다.</li>\n</ul>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># 데이터셋 불러오기</span>\ndata = load_diabetes() <span class=\"hljs-comment\"># 데이터 불러오기</span>\n\n<span class=\"hljs-comment\"># DataFrame 생성</span>\ndf_diabetes = pd.DataFrame(data.data, columns=data.feature_names)\n\n<span class=\"hljs-comment\"># 대상 변수를 DataFrame에 추가</span>\ndf_diabetes[<span class=\"hljs-string\">'target'</span>] = data.target\n\n<span class=\"hljs-comment\"># NaN 값 제거</span>\ndf = df_diabetes.dropna(axis=<span class=\"hljs-string\">'rows'</span>) <span class=\"hljs-comment\"># NaN 값 제거</span>\n\n<span class=\"hljs-comment\"># DataFrame 표시</span>\ndf_diabetes.head()\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_24.png\" alt=\"이미지\"></p>\n<pre><code class=\"hljs language-python\">df_diabetes.info()\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_25.png\" alt=\"image\"></p>\n<pre><code class=\"hljs language-js\">df_diabetes.<span class=\"hljs-title function_\">describe</span>()\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_26.png\" alt=\"image\"></p>\n<p>2.3— Exploratory Data Analysis</p>\n<pre><code class=\"hljs language-js\">sns.<span class=\"hljs-title function_\">pairplot</span>(df_diabetes)\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_27.png\" alt=\"image\"></p>\n<p>2.4— 데이터를 학습 및 테스트 세트로 분할</p>\n<pre><code class=\"hljs language-js\">df = df_diabetes.<span class=\"hljs-property\">iloc</span>[:<span class=\"hljs-number\">100</span>,<span class=\"hljs-number\">0</span>:<span class=\"hljs-number\">10</span>]\n</code></pre>\n<pre><code class=\"hljs language-js\">X=df.<span class=\"hljs-title function_\">to_numpy</span>()\n\ny=df_diabetes.<span class=\"hljs-property\">iloc</span>[:<span class=\"hljs-number\">100</span>,-<span class=\"hljs-number\">1</span>}\n</code></pre>\n<pre><code class=\"hljs language-js\">X_train, X_test, y_train, y_test = <span class=\"hljs-title function_\">train_test_split</span>(X, y, test_size = <span class=\"hljs-number\">0.3</span>, random_state=<span class=\"hljs-number\">42</span>)\n</code></pre>\n<p>2.5 — Create the regressor model</p>\n<pre><code class=\"hljs language-js\">model = xgb.<span class=\"hljs-title class_\">XGBRegressor</span>(objective=<span class=\"hljs-string\">\"reg:linear\"</span>, random_state=<span class=\"hljs-number\">42</span>)\n</code></pre>\n<p>2.6 — 모델 훈련</p>\n<pre><code class=\"hljs language-js\">model.<span class=\"hljs-title function_\">fit</span>(X_train, y_train)\n</code></pre>\n<p>2.7 — 모델 최적화</p>\n<p>RandomizedSearchCV는 scikit-learn 라이브러리에서 제공하는 함수로, 머신 러닝 모델의 하이퍼파라미터 튜닝을 위해 교차 검증을 통해 주로 사용됩니다. 이 기술은 하이퍼파라미터의 폭넓은 탐색 영역을 다룰 때 유용하며, 가장 효과적인 값을 결정하는 데 도움이 됩니다.</p>\n<p>단계별 설명</p>\n<ol>\n<li>매개변수 공간 정의:</li>\n</ol>\n<p>RandomizedSearchCV를 활용하기 전에, 모델의 하이퍼파라미터를 위한 탐색 공간을 지정해야 합니다. 특정 값의 그리드를 제공하는 대신, 각 하이퍼파라미터에 대해 분포를 정의합니다.</p>\n<ol start=\"2\">\n<li>무작위 샘플링:</li>\n</ol>\n<p>GridSearchCV와 같이 모든 가맹 별로 동시에 평가하는 것이 아니라, RandomizedSearchCV는 평가를 위해 일정한 조합을 무작위로 선택합니다. 이는 큰 탐색 공간을 다룰 때 유리합니다.</p>\n<ol start=\"3\">\n<li>모델 훈련:</li>\n</ol>\n<p>랜덤으로 선택된 각 하이퍼파라미터 집합에 대해 RandomizedSearchCV는 교차 검증을 사용하여 모델을 훈련합니다. 데이터는 폴드로 나누어지며, 모델은 일부 폴드에서 훈련되고 나머지 폴드에서 평가됩니다.</p>\n<ol start=\"4\">\n<li>성능 평가:</li>\n</ol>\n<p>성능은 특정 메트릭(예: 정확도, F1 점수)을 사용하여 측정됩니다. 목표는 주어진 문제에 따라 이 메트릭을 최대화하거나 최소화하는 하이퍼파라미터를 찾는 것입니다(예: 분류 문제에서 정확도를 최대화).</p>\n<ol start=\"5\">\n<li>최적 모델 선택:</li>\n</ol>\n<p>랜덤 서치를 완료하면 RandomizedSearchCV가 교차 검증 중 가장 우수한 평균 성능을 보인 하이퍼파라미터 세트를 반환합니다.</p>\n<p>RandomizedSearchCV를 사용하면 대규모 탐색 공간을 다룰 때 특히 모든 가능한 조합을 평가하는 그리드 서치(GridSearchCV)와 비교하여 계산 시간을 단축할 수 있습니다. 이 효율성은 모든 가능한 조합을 평가하는 대신 하이퍼파라미터 공간의 무작위 샘플을 탐색하는 데서 비롯됩니다.</p>\n<pre><code class=\"hljs language-js\">params = {\n    <span class=\"hljs-string\">\"colsample_bytree\"</span>: <span class=\"hljs-title function_\">uniform</span>(<span class=\"hljs-number\">0.7</span>, <span class=\"hljs-number\">0.3</span>),\n    <span class=\"hljs-string\">\"gamma\"</span>: <span class=\"hljs-title function_\">uniform</span>(<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0.5</span>),\n    <span class=\"hljs-string\">\"learning_rate\"</span>: <span class=\"hljs-title function_\">uniform</span>(<span class=\"hljs-number\">0.03</span>, <span class=\"hljs-number\">0.3</span>), # 기본값 <span class=\"hljs-number\">0.1</span> \n    <span class=\"hljs-string\">\"max_depth\"</span>: <span class=\"hljs-title function_\">randint</span>(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">6</span>), # 기본값 <span class=\"hljs-number\">3</span>\n    <span class=\"hljs-string\">\"n_estimators\"</span>: <span class=\"hljs-title function_\">randint</span>(<span class=\"hljs-number\">100</span>, <span class=\"hljs-number\">150</span>), # 기본값 <span class=\"hljs-number\">100</span>\n    <span class=\"hljs-string\">\"subsample\"</span>: <span class=\"hljs-title function_\">uniform</span>(<span class=\"hljs-number\">0.6</span>, <span class=\"hljs-number\">0.4</span>)\n}\n\nbest_model = <span class=\"hljs-title class_\">RandomizedSearchCV</span>(model, param_distributions=params, random_state=<span class=\"hljs-number\">42</span>, n_iter=<span class=\"hljs-number\">200</span>, cv=<span class=\"hljs-number\">3</span>, verbose=<span class=\"hljs-number\">1</span>, n_jobs=<span class=\"hljs-number\">1</span>, return_train_score=<span class=\"hljs-title class_\">True</span>)\n\nbest_model.<span class=\"hljs-title function_\">fit</span>(X_train, y_train, early_stopping_rounds=<span class=\"hljs-number\">5</span>, eval_set=[(X_test, y_test)]\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_28.png\" alt=\"이미지\"></p>\n<pre><code class=\"hljs language-js\">def <span class=\"hljs-title function_\">report_best_scores</span>(results, n_top=<span class=\"hljs-number\">3</span>):\n    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">range</span>(<span class=\"hljs-number\">1</span>, n_top + <span class=\"hljs-number\">1</span>):\n        candidates = np.<span class=\"hljs-title function_\">flatnonzero</span>(results[<span class=\"hljs-string\">'rank_test_score'</span>] == i)\n        <span class=\"hljs-keyword\">for</span> candidate <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">candidates</span>:\n            <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"순위 {0}인 모델\"</span>.<span class=\"hljs-title function_\">format</span>(i))\n            <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"평균 검증 점수: {0:.3f} (표준편차: {1:.3f})\"</span>.<span class=\"hljs-title function_\">format</span>(\n                results[<span class=\"hljs-string\">'mean_test_score'</span>][candidate],\n                results[<span class=\"hljs-string\">'std_test_score'</span>][candidate]))\n            best_params = results[<span class=\"hljs-string\">'params'</span>][candidate]\n            <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"찾은 최적의 매개변수:\"</span>)\n            <span class=\"hljs-keyword\">for</span> param, value <span class=\"hljs-keyword\">in</span> best_params.<span class=\"hljs-title function_\">items</span>():\n                <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"  {0}: {1}\"</span>.<span class=\"hljs-title function_\">format</span>(param, value))\n            <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"\"</span>)\n</code></pre>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-title function_\">report_best_scores</span>(best_model.<span class=\"hljs-property\">cv_results_</span>, <span class=\"hljs-number\">1</span>)\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_29.png\" alt=\"Image\"></p>\n<pre><code class=\"hljs language-js\">model =  xgb.<span class=\"hljs-title class_\">XGBRegressor</span>(objective=<span class=\"hljs-string\">\"reg:linear\"</span>, max_depth= <span class=\"hljs-number\">5</span>, learning_rate= <span class=\"hljs-number\">0.29302969102852483</span>, gamma = <span class=\"hljs-number\">0.38122934287034527</span>)\nmodel.<span class=\"hljs-title function_\">fit</span>(X_train, y_train)\n</code></pre>\n<p>2.8 — Visualization</p>\n<pre><code class=\"hljs language-js\">fig, ax = plt.<span class=\"hljs-title function_\">subplots</span>(figsize=(<span class=\"hljs-number\">20</span>, <span class=\"hljs-number\">10</span>))\n<span class=\"hljs-title function_\">plot_tree</span>(model, num_trees=<span class=\"hljs-number\">0</span>, ax=ax)\nplt.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_30.png\" alt=\"image\"></p>\n<p>2.9— 훈련 데이터로 모델 평가</p>\n<pre><code class=\"hljs language-js\">score = model.<span class=\"hljs-title function_\">score</span>(X_train, y_train)\ntraining_predict = model.<span class=\"hljs-title function_\">predict</span>(X_train)\nmse = <span class=\"hljs-title function_\">mean_squared_error</span>(y_train, training_predict)\n\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"R-squared:\"</span>, score)\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"MSE: \"</span>, mse)\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"RMSE: \"</span>, mse**(<span class=\"hljs-number\">1</span>/<span class=\"hljs-number\">2.0</span>))\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_31.png\" alt=\"image\"></p>\n<pre><code class=\"hljs language-js\">x_ax = <span class=\"hljs-title function_\">range</span>(<span class=\"hljs-title function_\">len</span>(y_train))\nplt.<span class=\"hljs-title function_\">plot</span>(x_ax, y_train, label=<span class=\"hljs-string\">\"원본\"</span>)\nplt.<span class=\"hljs-title function_\">plot</span>(x_ax, training_predict, label=<span class=\"hljs-string\">\"예측된 값\"</span>)\nplt.<span class=\"hljs-title function_\">title</span>(<span class=\"hljs-string\">\"훈련 및 예측된 데이터\"</span>)\nplt.<span class=\"hljs-title function_\">xlabel</span>(<span class=\"hljs-string\">'X축'</span>)\nplt.<span class=\"hljs-title function_\">ylabel</span>(<span class=\"hljs-string\">'Y축'</span>)\nplt.<span class=\"hljs-title function_\">legend</span>(loc=<span class=\"hljs-string\">'best'</span>, fancybox=<span class=\"hljs-title class_\">True</span>, shadow=<span class=\"hljs-title class_\">True</span>)\nplt.<span class=\"hljs-title function_\">grid</span>(<span class=\"hljs-title class_\">True</span>)\nplt.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_32.png\" alt=\"image\"></p>\n<p>2.10— 테스트 데이터로 모델 평가</p>\n<pre><code class=\"hljs language-js\">score = model.<span class=\"hljs-title function_\">score</span>(X_test, y_test)\ntest_predict = model.<span class=\"hljs-title function_\">predict</span>(X_test)\nmse = <span class=\"hljs-title function_\">mean_squared_error</span>(y_test, test_predict)\n\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"R-squared:\"</span>, score)\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"MSE: \"</span>, mse)\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"RMSE: \"</span>, mse**(<span class=\"hljs-number\">1</span>/<span class=\"hljs-number\">2.0</span>))\n</code></pre>\n<pre><code class=\"hljs language-js\">x_ax = <span class=\"hljs-title function_\">range</span>(<span class=\"hljs-title function_\">len</span>(y_test))\nplt.<span class=\"hljs-title function_\">plot</span>(x_ax, y_test, label=<span class=\"hljs-string\">\"original\"</span>)\nplt.<span class=\"hljs-title function_\">plot</span>(x_ax, test_predict, label=<span class=\"hljs-string\">\"predicted\"</span>)\nplt.<span class=\"hljs-title function_\">title</span>(<span class=\"hljs-string\">\"Testing and predicted data\"</span>)\nplt.<span class=\"hljs-title function_\">xlabel</span>(<span class=\"hljs-string\">'X-axis'</span>)\nplt.<span class=\"hljs-title function_\">ylabel</span>(<span class=\"hljs-string\">'Y-axis'</span>)\nplt.<span class=\"hljs-title function_\">legend</span>(loc=<span class=\"hljs-string\">'best'</span>,fancybox=<span class=\"hljs-title class_\">True</span>, shadow=<span class=\"hljs-title class_\">True</span>)\nplt.<span class=\"hljs-title function_\">grid</span>(<span class=\"hljs-title class_\">True</span>)\nplt.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<p>2.11 — 테스트 데이터를 사용하여 모델 평가하기</p>\n<pre><code class=\"hljs language-js\">code = m2c.<span class=\"hljs-title function_\">export_to_c</span>(model)\n<span class=\"hljs-title function_\">print</span>(code)\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_35.png\" alt=\"Image\"></p>\n<p>2.12 — 템플릿을 .h 파일에 저장합니다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">with</span> <span class=\"hljs-title function_\">open</span>(<span class=\"hljs-string\">'./XGBRegressor.h'</span>, <span class=\"hljs-string\">'w'</span>) <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">file</span>:\n    file.<span class=\"hljs-title function_\">write</span>(code)\n</code></pre>\n<p>2.13 — 모델 배포</p>\n<p>이 예제를 통해 ESP32, 아두이노, 아두이노 Portenta H7 with Vision Shield, 라즈베리 파이 및 기타 다양한 마이크로컨트롤러 또는 IoT 장치에서 머신 러닝 알고리즘을 구현할 수 있습니다.</p>\n<p>2.13.1 — 완성된 아두이노 스케치</p>\n<pre><code class=\"hljs language-js\">#include <span class=\"hljs-string\">\"XGBRegressor.h\"</span>\n\n\n<span class=\"hljs-keyword\">void</span> <span class=\"hljs-title function_\">setup</span>(<span class=\"hljs-params\"></span>) {\n  <span class=\"hljs-title class_\">Serial</span>.<span class=\"hljs-title function_\">begin</span>(<span class=\"hljs-number\">115200</span>);\n}\n\n<span class=\"hljs-keyword\">void</span> <span class=\"hljs-title function_\">loop</span>(<span class=\"hljs-params\"></span>) {\n  double <span class=\"hljs-variable constant_\">X_1</span>[] = { <span class=\"hljs-number\">2.71782911e-02</span>,  <span class=\"hljs-number\">5.06801187e-02</span>,  <span class=\"hljs-number\">1.75059115e-02</span>,\n                  -<span class=\"hljs-number\">3.32135761e-02</span>, -<span class=\"hljs-number\">7.07277125e-03</span>,  <span class=\"hljs-number\">4.59715403e-02</span>,\n                  -<span class=\"hljs-number\">6.54906725e-02</span>,  <span class=\"hljs-number\">7.12099798e-02</span>, -<span class=\"hljs-number\">9.64332229e-02</span>,\n                  -<span class=\"hljs-number\">5.90671943e-02</span>};\n  double result_1 = <span class=\"hljs-title function_\">score</span>(<span class=\"hljs-variable constant_\">X_1</span>);\n  <span class=\"hljs-title class_\">Serial</span>.<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"입력 X1로 예측 결과 (실제 값 = 69):\"</span>);\n  <span class=\"hljs-title class_\">Serial</span>.<span class=\"hljs-title function_\">println</span>(<span class=\"hljs-title class_\">String</span>(result_1, <span class=\"hljs-number\">7</span>));\n  <span class=\"hljs-title function_\">delay</span>(<span class=\"hljs-number\">2000</span>);\n}\n</code></pre>\n<p>3.12 — 결과</p>\n<p><img src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_36.png\" alt=\"image\"></p>\n<p>전체 프로젝트: <a href=\"github.com\">TinyML/14_XGBRegression at main · thommaskevin/TinyML</a></p>\n<h2>만약 마음에 드셨다면, 제 커피 한 잔 사주세요 ☕️💰 (Bitcoin)</h2>\n<pre><code class=\"hljs language-plaintext\">코드: bc1qzydjy4m9yhmjjrkgtrzhsgmkq79qenvcvc7qzn\n\n![Image](/assets/img/2024-06-19-TinyMLXGBoostRegression_37.png)\n</code></pre>\n</body>\n</html>\n"},"__N_SSG":true}