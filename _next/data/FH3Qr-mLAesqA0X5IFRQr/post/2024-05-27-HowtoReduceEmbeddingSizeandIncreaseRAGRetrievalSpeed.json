{"pageProps":{"post":{"title":"임베딩 크기를 줄이고 RAG 검색 속도 높이는 방법","description":"","date":"2024-05-27 14:56","slug":"2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed","content":"\n<img src=\"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_0.png\" />\n\n# 소개\n\n텍스트 임베딩은 단일 단어나 전체 문장의 고차원 벡터 표현입니다.\n\n이 숫자 배열로 이루어진 벡터는 기본 텍스트에 대한 풍부한 정보를 포착함으로써 의미 이해, 분류, 군집화, 정보 검색 (RAG), 재정렬 및 더 많은 하류 작업에 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n보통 임베딩 벡터의 차원 d는 고정됩니다. 임베딩 차원은 일반적으로 64에서 4096까지의 2의 제곱수로 구성됩니다.\n\n매트료시카 임베딩을 사용하면 응용 프로그램에 따라 임베딩의 차원을 변경할 수 있습니다. 이를 통해 저장 공간을 줄이고 비용을 절약하며 검색 속도를 높일 수 있습니다.\n\n# 텍스트 임베딩이란?\n\n![이미지](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_1.png)\n\n<div class=\"content-ad\"></div>\n\n저희는 모든 가능한 입력 문자를 정수 값으로 매핑하는 어휘를 정의하여 시작합니다. 이 어휘에는 알파벳 문자 뿐만 아니라 특수 문자, 짧은 단어 및 하위 단어도 포함됩니다:\n\n```js\n{\n  \"a\": 1,\n  \"b\": 2,\n  \"c\": 3,\n  ...\n  \"z\": 26,\n  \"the\": 27,\n  \" \": 28\n}\n```\n\n토큰화 후에는 토큰 목록을 인코더 모델에 전달할 수 있습니다. 인코더는 대량의 훈련 데이터로부터 학습하여 각 토큰을 고차원 숫자 벡터 임베딩으로 변환합니다.\n\n예를 들어, OpenAI의 text-embedding-3-large 모델의 임베딩의 출력 차원 d는 3072입니다.\n\n<div class=\"content-ad\"></div>\n\n단일 문장 임베딩을 얻으려면 여러 토큰 임베딩에서 정보를 압축해야 합니다. 이를 위한 한 가지 방법은 단순히 모든 토큰 임베딩을 평균내는 것입니다.\n\n# 마트료시카 임베딩\n\n마트료시카 임베딩은 워싱턴 대학, 구글 리서치, 하버드 대학의 연구자들에 의해 2022년에 발표된 \"Matryoshka Representation Learning\" 논문에서 소개되었습니다.\n\n마트료시카 임베딩은 서로 다른 세기의 정보를 하나의 임베딩 벡터에 인코딩하는 데 훈련되었습니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, MRL을 사용하여 단순히 크기 d = 1024의 전체 임베딩 벡터를 학습하는 대신, 우리는 동일한 시간에 최적화하려는 손실 함수를 위해 matryoshka_dims = [1024,512,256,128,64] 차원 목록을 사용합니다[2].\n\n이렇게 하면 처음 몇 차원에 가장 덜 구체적인 정보가 저장되고 나중 차원에는 점점 더 많은 세부 정보가 저장된 임베딩 벡터가 생성됩니다.\n\n![이미지](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_2.png)\n\n이는 우리가 원하는 곳에서 임베딩 벡터를 잘라도 성능을 너무 많이 희생하지 않고 사용할 수 있다는 효과가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 왜 중요한가요?\n\n만약 우리가 텍스트 임베딩 벡터를 벡터 데이터베이스에 저장하려고 한다고 가정해봅시다. 각 임베딩은 d 차원을 가지고 있습니다. 그리고 각 숫자는 일반적으로 32비트 부동 소수점 수입니다. 그래서 우리는 저장을 위해 n _ d _ 4 바이트가 필요합니다.\n\n그리고 만약 우리가 점곱이나 코사인 유사성과 같은 유사성 지표를 계산하려고 한다면 (코사인 유사성은 단지 정규화된 점곱일 뿐입니다), 차원 d가 클수록 수학적 계산을 더 많이 해야 합니다.\n\n![image](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_3.png)\n\n<div class=\"content-ad\"></div>\n\nMRL을 사용하면 작은 메모리 공간, 빠른 처리 속도 및 따라서 비용 절약에 관심이 있다면, 첫 64차원만 사용할 수도 있습니다. 최상의 하류 성능을 원한다면 모든 차원을 사용합니다. 그리고 그 중간을 선택할 수도 있습니다.\n\n따라서, MRL은 LLM 사용자들에게 내려보기 성능의 작은 저하에 대한 임베딩 크기(비용)의 대가를 거래할 수 있는 능력을 제공합니다.\n\n# Nomic AI에서 MRL 사용하기\n\nNomic의 Matryoshka 텍스트 임베딩 모델 nomic-embed-text-v1.5은 matryoshka_dims = [768,512,256,128,64]로 훈련되었습니다. 해당 모델은 Hugging Face에서 공개적으로 사용할 수 있습니다 [3].\n\n<div class=\"content-ad\"></div>\n\n또 다른 이 인코더 모델의 멋진 기능은 다른 접두사를 지원한다는 것입니다. 이 모델은 [search_query, search_document, classification, clustering] 접두사를 지원하여 각 특정 하류 작업에 대해 더 나은 임베딩을 얻을 수 있습니다.\n\nnomic-embed-text-v1.5 모델이 Massive Text Embedding Benchmark (MTEB)에서 어떻게 수행되는지 살펴보겠습니다:\n\n![image](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_4.png)\n\nPython에서 PyTorch와 Sentence Transformers 라이브러리를 사용하여 모델을 구현해 봅시다:\n\n<div class=\"content-ad\"></div>\n\n\n!pip install torch sentence_transformers einops\n\n\n\nimport torch\nfrom sentence_transformers import SentenceTransformer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n\nmodel = SentenceTransformer(\n    \"nomic-ai/nomic-embed-text-v1.5\",\n    device=device,\n    trust_remote_code=True,\n    prompts={\n        \"search_query\": \"search_query: \",\n        \"search_document\": \"search_document: \",\n        \"classification\": \"classification: \",\n        \"clustering\": \"clustering: \",\n    },\n)\n\n\ndef embed_sentences(\n    model: SentenceTransformer,\n    sentences: list[str],\n    prompt_name: str,\n    matryoshka_dim: int,\n    device: str,\n):\n    assert matryoshka_dim <= 768, \"maximum dimension for nomic-embed-text-v1.5 is 768\"\n    embeddings = model.encode(\n        sentences, prompt_name=prompt_name, device=device, convert_to_tensor=True\n    )\n    embeddings = torch.nn.functional.layer_norm(\n        embeddings, normalized_shape=(embeddings.shape[1],)\n    )\n    embeddings = embeddings[:, :matryoshka_dim]\n    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n    return embeddings.cpu()\n\n\nmatryoshka_dim 매개변수를 사용하여 768차원 임베딩 벡터를 자릅니다. 그런 다음 새로운 임베딩 벡터를 정규화합니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n이제 원하는 차원을 설정하고 위키피디아 텍스트와 RAG(검색 증강 생성)용 쿼리를 인코딩할 수 있습니다.\n\n```js\nmatryoshka_dim = 64\n\nwikipedia_texts = [\n    \"개(Canis familiaris 또는 Canis lupus familiaris)는 늑대의 길들여진 후손입니다.\",\n    \"알베르트 아인슈타인은 1879년 3월 14일 독일 제국의 뷔르템베르크 왕국 울름에서 태어났습니다.\",\n    \"아인슈타인은 어린 시절부터 물리학과 수학에서 뛰어나며, 곧 같은 나이의 아이들만이 보유한 수학적 전문 지식을 습득했습니다.\",\n    \"베르너 칼 하이젠베르크는 독일의 이론 물리학자로 양자역학 이론의 주요 선구자 중 한 명이며, 제2차 세계대전 중 나치 핵무기 프로그램의 주요 과학자였습니다.\",\n    \"스티븐 폴 잡스(1955년 2월 24일 - 2011년 10월 5일)는 기술 거장 애플 주식회사를 공동 창업하여 가장 잘 알려진 미국 사업가, 발명가, 투자가였습니다.\",\n    \"고양이(Felis catus), 일반적으로 가정 고양이 또는 집고양이로 불리는 것은 고양이과에서 유일하게 길들인 종입니다.\",\n]\n\nquestion = [\"알베르트 아인슈타인은 어디에서 태어났나요?\"]\n\nquestion_embedding = embed_sentences(\n    model,\n    sentences=question,\n    prompt_name=\"search_query\",\n    matryoshka_dim=matryoshka_dim,\n    device=device,\n)\n\ndocument_embeddings = embed_sentences(\n    model,\n    sentences=wikipedia_texts,\n    prompt_name=\"search_document\",\n    matryoshka_dim=matryoshka_dim,\n    device=device,\n)\n```\n\n```js\nprint(f\"document_embeddings.shape: {document_embeddings.shape}\")\nprint(f\"question_embedding.shape:  {question_embedding.shape}\")\n>> document_embeddings.shape: torch.Size([6, 64])\n>> question_embedding.shape:  torch.Size([1, 64])\n```\n\n우리는 Matryoshka 텍스트 임베딩의 첫 번째 두 차원을 산포도로 시각화할 수 있습니다. 그러나 이 임베딩 모델은 명시적으로 2차원의 Matryoshka 차원에 최적화되지는 않았습니다.\n\n<div class=\"content-ad\"></div>\n\nmd\n![image](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_5.png)\n\n다음으로, 문서 임베딩을 벡터 데이터베이스에 저장할 수 있습니다. 저는 Faiss를 사용하고 있어요. Faiss는 밀집 벡터의 효율적인 유사성 검색 및 클러스터링을 위한 Meta Research의 오픈 소스 라이브러리입니다 [4].\n\n```bash\n!pip install faiss-cpu\n```\n\n\n\n\n```python\nimport faiss\n\nindex = faiss.IndexFlatIP(matryoshka_dim)\nindex.add(document_embeddings)\n```\n\n\n\n<div class=\"content-ad\"></div>\n\n이 코드는 내적 제품을 사용하여 \"정확한 검색\"을 통해 벡터 데이터베이스를 만듭니다. 이때 IndexFlatIP를 사용하는데, 이는 내적 유사도 측정 방법입니다. 정규화된 임베딩을 사용하고 있기 때문에, 내적과 코사인 유사도는 동일합니다.\n\n이제 index는 여섯 개의 텍스트 임베딩으로 구성된 벡터 데이터베이스입니다:\n\n```js\nprint(index.ntotal)\n>> 6\n```\n\n질문과 가장 유사한 임베딩을 검색하고 상위 k개 결과를 검색해보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\ndistances, indices = index.search(question_embedding, k=6)\nprint(indices)\nprint(distances)\n>> [[1 2 3 4 0 5]]\n>> [[0.9633528  0.729192   0.63353264 0.62068397 0.512541   0.43155164]]\n```\n\n저희 데이터베이스에서 가장 유사한 텍스트는 인덱스 1이며 유사도 점수는 0.96입니다 (최대 점수는 1.0입니다).\n\n```js\n# d=64인 결과\nprint(question)\nprint(wikipedia_texts[1])\n>> ['알버트 아인슈타인은 어디에서 태어났나요?']\n>> '알버트 아인슈타인은 독일 제국의 퀴르템베르크 왕국 울름에서 1879년 3월 14일에 태어났습니다.'\n```\n\n저는 matryoshka_dim=768로 코드를 다시 실행했고 유사한 결과를 얻었습니다. 그러나 더 높은 차원은 더 많은 메모리와 계산이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n\n```js\n# 결과 d=768일 때\nprint(indices)\nprint(distances)\n>> [[1 2 4 3 0 5]]\n>> [[0.92466116 0.645744   0.54405797 0.54004824 0.39331824 0.37972206]]\n```\n\n# MRL 및 양자화\n\n더욱 압축된 임베딩을 원한다면, MRL과 이진 벡터 양자화를 함께 사용할 수 있습니다. 이진 양자화는 임베딩 벡터에서 0보다 큰 모든 숫자를 1로 변환하고 그 외의 숫자를 0으로 변환합니다 [5].\n\n<img src=\"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_6.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n이진 양자화를 사용하면 d 차원의 임베딩 벡터는 오직 d / 8 바이트의 메모리만 필요합니다. 이는 float32 형식의 d \\* 4 바이트와 비교해 크기가 32배로 줄어든 것을 의미합니다 [4]. 그러나 이 축소는 성능 저하와 함께 발생합니다.\n\n# 결론\n\nMatryoshka 손실을 사용하는 임베딩 모델은 훈련 중에 동시에 여러 임베딩 차원에 최적화되어 있습니다.\n\nMatryoshka 표현 학습을 사용하면 LLM 사용자가 텍스트 임베딩 크기를 작게 조정하여 성능 저하를 감수할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n더 작은 임베딩은 더 적은 메모리와 계산을 필요로하며, 이는 장기적으로 많은 비용을 절약할 수 있습니다. 또한 계산이 빨라져서 검색 속도가 빨라지기 때문에, 예를 들어 RAG 애플리케이션에 적합합니다.\n\n# 참고 자료\n\n[1] A. Kusupati 등. (2022), Matryoshka Representation Learning, arXiv:2205.13147\n\n[2] MatryoshkaLoss: https://www.sbert.net/docs/package_reference/losses.html#matryoshkaloss (접근일: 2024년 04월 05일)\n\n<div class=\"content-ad\"></div>\n\n[3] Hugging Face의 nomic-embed-text-v1.5: https://huggingface.co/nomic-ai/nomic-embed-text-v1.5 (접속일: 2024년 04월 05일)\n\n[4] Faiss 문서: https://github.com/facebookresearch/faiss/wiki/Getting-started (접속일: 2024년 04월 05일)\n\n[5] A. Shakir, T. Aarsen, S. Lee (2024), 바이너리 및 스칼라 임베딩 양자화로 훨씬 빠르고 저렴한 검색, Hugging Face 블로그\n","ogImage":{"url":"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_0.png"},"coverImage":"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_0.png","tag":["Tech"],"readingTime":9},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<h1>소개</h1>\n<p>텍스트 임베딩은 단일 단어나 전체 문장의 고차원 벡터 표현입니다.</p>\n<p>이 숫자 배열로 이루어진 벡터는 기본 텍스트에 대한 풍부한 정보를 포착함으로써 의미 이해, 분류, 군집화, 정보 검색 (RAG), 재정렬 및 더 많은 하류 작업에 사용할 수 있습니다.</p>\n<p>보통 임베딩 벡터의 차원 d는 고정됩니다. 임베딩 차원은 일반적으로 64에서 4096까지의 2의 제곱수로 구성됩니다.</p>\n<p>매트료시카 임베딩을 사용하면 응용 프로그램에 따라 임베딩의 차원을 변경할 수 있습니다. 이를 통해 저장 공간을 줄이고 비용을 절약하며 검색 속도를 높일 수 있습니다.</p>\n<h1>텍스트 임베딩이란?</h1>\n<p><img src=\"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_1.png\" alt=\"이미지\"></p>\n<p>저희는 모든 가능한 입력 문자를 정수 값으로 매핑하는 어휘를 정의하여 시작합니다. 이 어휘에는 알파벳 문자 뿐만 아니라 특수 문자, 짧은 단어 및 하위 단어도 포함됩니다:</p>\n<pre><code class=\"hljs language-js\">{\n  <span class=\"hljs-string\">\"a\"</span>: <span class=\"hljs-number\">1</span>,\n  <span class=\"hljs-string\">\"b\"</span>: <span class=\"hljs-number\">2</span>,\n  <span class=\"hljs-string\">\"c\"</span>: <span class=\"hljs-number\">3</span>,\n  ...\n  <span class=\"hljs-string\">\"z\"</span>: <span class=\"hljs-number\">26</span>,\n  <span class=\"hljs-string\">\"the\"</span>: <span class=\"hljs-number\">27</span>,\n  <span class=\"hljs-string\">\" \"</span>: <span class=\"hljs-number\">28</span>\n}\n</code></pre>\n<p>토큰화 후에는 토큰 목록을 인코더 모델에 전달할 수 있습니다. 인코더는 대량의 훈련 데이터로부터 학습하여 각 토큰을 고차원 숫자 벡터 임베딩으로 변환합니다.</p>\n<p>예를 들어, OpenAI의 text-embedding-3-large 모델의 임베딩의 출력 차원 d는 3072입니다.</p>\n<p>단일 문장 임베딩을 얻으려면 여러 토큰 임베딩에서 정보를 압축해야 합니다. 이를 위한 한 가지 방법은 단순히 모든 토큰 임베딩을 평균내는 것입니다.</p>\n<h1>마트료시카 임베딩</h1>\n<p>마트료시카 임베딩은 워싱턴 대학, 구글 리서치, 하버드 대학의 연구자들에 의해 2022년에 발표된 \"Matryoshka Representation Learning\" 논문에서 소개되었습니다.</p>\n<p>마트료시카 임베딩은 서로 다른 세기의 정보를 하나의 임베딩 벡터에 인코딩하는 데 훈련되었습니다.</p>\n<p>예를 들어, MRL을 사용하여 단순히 크기 d = 1024의 전체 임베딩 벡터를 학습하는 대신, 우리는 동일한 시간에 최적화하려는 손실 함수를 위해 matryoshka_dims = [1024,512,256,128,64] 차원 목록을 사용합니다[2].</p>\n<p>이렇게 하면 처음 몇 차원에 가장 덜 구체적인 정보가 저장되고 나중 차원에는 점점 더 많은 세부 정보가 저장된 임베딩 벡터가 생성됩니다.</p>\n<p><img src=\"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_2.png\" alt=\"이미지\"></p>\n<p>이는 우리가 원하는 곳에서 임베딩 벡터를 잘라도 성능을 너무 많이 희생하지 않고 사용할 수 있다는 효과가 있습니다.</p>\n<h2>왜 중요한가요?</h2>\n<p>만약 우리가 텍스트 임베딩 벡터를 벡터 데이터베이스에 저장하려고 한다고 가정해봅시다. 각 임베딩은 d 차원을 가지고 있습니다. 그리고 각 숫자는 일반적으로 32비트 부동 소수점 수입니다. 그래서 우리는 저장을 위해 n _ d _ 4 바이트가 필요합니다.</p>\n<p>그리고 만약 우리가 점곱이나 코사인 유사성과 같은 유사성 지표를 계산하려고 한다면 (코사인 유사성은 단지 정규화된 점곱일 뿐입니다), 차원 d가 클수록 수학적 계산을 더 많이 해야 합니다.</p>\n<p><img src=\"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_3.png\" alt=\"image\"></p>\n<p>MRL을 사용하면 작은 메모리 공간, 빠른 처리 속도 및 따라서 비용 절약에 관심이 있다면, 첫 64차원만 사용할 수도 있습니다. 최상의 하류 성능을 원한다면 모든 차원을 사용합니다. 그리고 그 중간을 선택할 수도 있습니다.</p>\n<p>따라서, MRL은 LLM 사용자들에게 내려보기 성능의 작은 저하에 대한 임베딩 크기(비용)의 대가를 거래할 수 있는 능력을 제공합니다.</p>\n<h1>Nomic AI에서 MRL 사용하기</h1>\n<p>Nomic의 Matryoshka 텍스트 임베딩 모델 nomic-embed-text-v1.5은 matryoshka_dims = [768,512,256,128,64]로 훈련되었습니다. 해당 모델은 Hugging Face에서 공개적으로 사용할 수 있습니다 [3].</p>\n<p>또 다른 이 인코더 모델의 멋진 기능은 다른 접두사를 지원한다는 것입니다. 이 모델은 [search_query, search_document, classification, clustering] 접두사를 지원하여 각 특정 하류 작업에 대해 더 나은 임베딩을 얻을 수 있습니다.</p>\n<p>nomic-embed-text-v1.5 모델이 Massive Text Embedding Benchmark (MTEB)에서 어떻게 수행되는지 살펴보겠습니다:</p>\n<p><img src=\"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_4.png\" alt=\"image\"></p>\n<p>Python에서 PyTorch와 Sentence Transformers 라이브러리를 사용하여 모델을 구현해 봅시다:</p>\n<p>!pip install torch sentence_transformers einops</p>\n<p>import torch\nfrom sentence_transformers import SentenceTransformer</p>\n<p>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"</p>\n<p>model = SentenceTransformer(\n\"nomic-ai/nomic-embed-text-v1.5\",\ndevice=device,\ntrust_remote_code=True,\nprompts={\n\"search_query\": \"search_query: \",\n\"search_document\": \"search_document: \",\n\"classification\": \"classification: \",\n\"clustering\": \"clustering: \",\n},\n)</p>\n<p>def embed_sentences(\nmodel: SentenceTransformer,\nsentences: list[str],\nprompt_name: str,\nmatryoshka_dim: int,\ndevice: str,\n):\nassert matryoshka_dim &#x3C;= 768, \"maximum dimension for nomic-embed-text-v1.5 is 768\"\nembeddings = model.encode(\nsentences, prompt_name=prompt_name, device=device, convert_to_tensor=True\n)\nembeddings = torch.nn.functional.layer_norm(\nembeddings, normalized_shape=(embeddings.shape[1],)\n)\nembeddings = embeddings[:, :matryoshka_dim]\nembeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\nreturn embeddings.cpu()</p>\n<p>matryoshka_dim 매개변수를 사용하여 768차원 임베딩 벡터를 자릅니다. 그런 다음 새로운 임베딩 벡터를 정규화합니다.</p>\n<p>이제 원하는 차원을 설정하고 위키피디아 텍스트와 RAG(검색 증강 생성)용 쿼리를 인코딩할 수 있습니다.</p>\n<pre><code class=\"hljs language-js\">matryoshka_dim = <span class=\"hljs-number\">64</span>\n\nwikipedia_texts = [\n    <span class=\"hljs-string\">\"개(Canis familiaris 또는 Canis lupus familiaris)는 늑대의 길들여진 후손입니다.\"</span>,\n    <span class=\"hljs-string\">\"알베르트 아인슈타인은 1879년 3월 14일 독일 제국의 뷔르템베르크 왕국 울름에서 태어났습니다.\"</span>,\n    <span class=\"hljs-string\">\"아인슈타인은 어린 시절부터 물리학과 수학에서 뛰어나며, 곧 같은 나이의 아이들만이 보유한 수학적 전문 지식을 습득했습니다.\"</span>,\n    <span class=\"hljs-string\">\"베르너 칼 하이젠베르크는 독일의 이론 물리학자로 양자역학 이론의 주요 선구자 중 한 명이며, 제2차 세계대전 중 나치 핵무기 프로그램의 주요 과학자였습니다.\"</span>,\n    <span class=\"hljs-string\">\"스티븐 폴 잡스(1955년 2월 24일 - 2011년 10월 5일)는 기술 거장 애플 주식회사를 공동 창업하여 가장 잘 알려진 미국 사업가, 발명가, 투자가였습니다.\"</span>,\n    <span class=\"hljs-string\">\"고양이(Felis catus), 일반적으로 가정 고양이 또는 집고양이로 불리는 것은 고양이과에서 유일하게 길들인 종입니다.\"</span>,\n]\n\nquestion = [<span class=\"hljs-string\">\"알베르트 아인슈타인은 어디에서 태어났나요?\"</span>]\n\nquestion_embedding = <span class=\"hljs-title function_\">embed_sentences</span>(\n    model,\n    sentences=question,\n    prompt_name=<span class=\"hljs-string\">\"search_query\"</span>,\n    matryoshka_dim=matryoshka_dim,\n    device=device,\n)\n\ndocument_embeddings = <span class=\"hljs-title function_\">embed_sentences</span>(\n    model,\n    sentences=wikipedia_texts,\n    prompt_name=<span class=\"hljs-string\">\"search_document\"</span>,\n    matryoshka_dim=matryoshka_dim,\n    device=device,\n)\n</code></pre>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-title function_\">print</span>(f<span class=\"hljs-string\">\"document_embeddings.shape: {document_embeddings.shape}\"</span>)\n<span class=\"hljs-title function_\">print</span>(f<span class=\"hljs-string\">\"question_embedding.shape:  {question_embedding.shape}\"</span>)\n>> document_embeddings.<span class=\"hljs-property\">shape</span>: torch.<span class=\"hljs-title class_\">Size</span>([<span class=\"hljs-number\">6</span>, <span class=\"hljs-number\">64</span>])\n>> question_embedding.<span class=\"hljs-property\">shape</span>:  torch.<span class=\"hljs-title class_\">Size</span>([<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">64</span>])\n</code></pre>\n<p>우리는 Matryoshka 텍스트 임베딩의 첫 번째 두 차원을 산포도로 시각화할 수 있습니다. 그러나 이 임베딩 모델은 명시적으로 2차원의 Matryoshka 차원에 최적화되지는 않았습니다.</p>\n<p>md\n<img src=\"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_5.png\" alt=\"image\"></p>\n<p>다음으로, 문서 임베딩을 벡터 데이터베이스에 저장할 수 있습니다. 저는 Faiss를 사용하고 있어요. Faiss는 밀집 벡터의 효율적인 유사성 검색 및 클러스터링을 위한 Meta Research의 오픈 소스 라이브러리입니다 [4].</p>\n<pre><code class=\"hljs language-bash\">!pip install faiss-cpu\n</code></pre>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> faiss\n\nindex = faiss.IndexFlatIP(matryoshka_dim)\nindex.add(document_embeddings)\n</code></pre>\n<p>이 코드는 내적 제품을 사용하여 \"정확한 검색\"을 통해 벡터 데이터베이스를 만듭니다. 이때 IndexFlatIP를 사용하는데, 이는 내적 유사도 측정 방법입니다. 정규화된 임베딩을 사용하고 있기 때문에, 내적과 코사인 유사도는 동일합니다.</p>\n<p>이제 index는 여섯 개의 텍스트 임베딩으로 구성된 벡터 데이터베이스입니다:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-title function_\">print</span>(index.<span class=\"hljs-property\">ntotal</span>)\n>> <span class=\"hljs-number\">6</span>\n</code></pre>\n<p>질문과 가장 유사한 임베딩을 검색하고 상위 k개 결과를 검색해보겠습니다:</p>\n<pre><code class=\"hljs language-js\">distances, indices = index.<span class=\"hljs-title function_\">search</span>(question_embedding, k=<span class=\"hljs-number\">6</span>)\n<span class=\"hljs-title function_\">print</span>(indices)\n<span class=\"hljs-title function_\">print</span>(distances)\n>> [[<span class=\"hljs-number\">1</span> <span class=\"hljs-number\">2</span> <span class=\"hljs-number\">3</span> <span class=\"hljs-number\">4</span> <span class=\"hljs-number\">0</span> <span class=\"hljs-number\">5</span>]]\n>> [[<span class=\"hljs-number\">0.9633528</span>  <span class=\"hljs-number\">0.729192</span>   <span class=\"hljs-number\">0.63353264</span> <span class=\"hljs-number\">0.62068397</span> <span class=\"hljs-number\">0.512541</span>   <span class=\"hljs-number\">0.43155164</span>]]\n</code></pre>\n<p>저희 데이터베이스에서 가장 유사한 텍스트는 인덱스 1이며 유사도 점수는 0.96입니다 (최대 점수는 1.0입니다).</p>\n<pre><code class=\"hljs language-js\"># d=<span class=\"hljs-number\">64</span>인 결과\n<span class=\"hljs-title function_\">print</span>(question)\n<span class=\"hljs-title function_\">print</span>(wikipedia_texts[<span class=\"hljs-number\">1</span>])\n>> [<span class=\"hljs-string\">'알버트 아인슈타인은 어디에서 태어났나요?'</span>]\n>> <span class=\"hljs-string\">'알버트 아인슈타인은 독일 제국의 퀴르템베르크 왕국 울름에서 1879년 3월 14일에 태어났습니다.'</span>\n</code></pre>\n<p>저는 matryoshka_dim=768로 코드를 다시 실행했고 유사한 결과를 얻었습니다. 그러나 더 높은 차원은 더 많은 메모리와 계산이 필요합니다.</p>\n<pre><code class=\"hljs language-js\"># 결과 d=<span class=\"hljs-number\">768</span>일 때\n<span class=\"hljs-title function_\">print</span>(indices)\n<span class=\"hljs-title function_\">print</span>(distances)\n>> [[<span class=\"hljs-number\">1</span> <span class=\"hljs-number\">2</span> <span class=\"hljs-number\">4</span> <span class=\"hljs-number\">3</span> <span class=\"hljs-number\">0</span> <span class=\"hljs-number\">5</span>]]\n>> [[<span class=\"hljs-number\">0.92466116</span> <span class=\"hljs-number\">0.645744</span>   <span class=\"hljs-number\">0.54405797</span> <span class=\"hljs-number\">0.54004824</span> <span class=\"hljs-number\">0.39331824</span> <span class=\"hljs-number\">0.37972206</span>]]\n</code></pre>\n<h1>MRL 및 양자화</h1>\n<p>더욱 압축된 임베딩을 원한다면, MRL과 이진 벡터 양자화를 함께 사용할 수 있습니다. 이진 양자화는 임베딩 벡터에서 0보다 큰 모든 숫자를 1로 변환하고 그 외의 숫자를 0으로 변환합니다 [5].</p>\n<p>이진 양자화를 사용하면 d 차원의 임베딩 벡터는 오직 d / 8 바이트의 메모리만 필요합니다. 이는 float32 형식의 d * 4 바이트와 비교해 크기가 32배로 줄어든 것을 의미합니다 [4]. 그러나 이 축소는 성능 저하와 함께 발생합니다.</p>\n<h1>결론</h1>\n<p>Matryoshka 손실을 사용하는 임베딩 모델은 훈련 중에 동시에 여러 임베딩 차원에 최적화되어 있습니다.</p>\n<p>Matryoshka 표현 학습을 사용하면 LLM 사용자가 텍스트 임베딩 크기를 작게 조정하여 성능 저하를 감수할 수 있습니다.</p>\n<p>더 작은 임베딩은 더 적은 메모리와 계산을 필요로하며, 이는 장기적으로 많은 비용을 절약할 수 있습니다. 또한 계산이 빨라져서 검색 속도가 빨라지기 때문에, 예를 들어 RAG 애플리케이션에 적합합니다.</p>\n<h1>참고 자료</h1>\n<p>[1] A. Kusupati 등. (2022), Matryoshka Representation Learning, arXiv:2205.13147</p>\n<p>[2] MatryoshkaLoss: <a href=\"https://www.sbert.net/docs/package_reference/losses.html#matryoshkaloss\" rel=\"nofollow\" target=\"_blank\">https://www.sbert.net/docs/package_reference/losses.html#matryoshkaloss</a> (접근일: 2024년 04월 05일)</p>\n<p>[3] Hugging Face의 nomic-embed-text-v1.5: <a href=\"https://huggingface.co/nomic-ai/nomic-embed-text-v1.5\" rel=\"nofollow\" target=\"_blank\">https://huggingface.co/nomic-ai/nomic-embed-text-v1.5</a> (접속일: 2024년 04월 05일)</p>\n<p>[4] Faiss 문서: <a href=\"https://github.com/facebookresearch/faiss/wiki/Getting-started\" rel=\"nofollow\" target=\"_blank\">https://github.com/facebookresearch/faiss/wiki/Getting-started</a> (접속일: 2024년 04월 05일)</p>\n<p>[5] A. Shakir, T. Aarsen, S. Lee (2024), 바이너리 및 스칼라 임베딩 양자화로 훨씬 빠르고 저렴한 검색, Hugging Face 블로그</p>\n</body>\n</html>\n"},"__N_SSG":true}