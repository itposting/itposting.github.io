{"pageProps":{"post":{"title":"6가지 방법으로 LLMs를 로컬에서 실행하는 방법","description":"","date":"2024-06-19 21:41","slug":"2024-06-19-6WaystoRunLLMsLocally","content":"\n\n상용 AI 및 대규모 언어 모델(LLM)은 한 가지 큰 단점이 있습니다: 개인 정보 보호 문제! 민감하거나 자사 데이터를 다룰 때 이러한 도구를 활용할 수 없습니다.\n\n이로 인해 로컬에서 개인 LLM을 운영하는 방법에 대해 알아야 합니다. 오픈 소스 모델은 해결책을 제시하지만 그들만의 일련의 도전과 혜택이 따릅니다.\n\n당신의 컴퓨터에서 실행할 수 있는 ChatGPT의 로컬 대안을 발견하기 위한 여정에 함께해주세요.\n\n# 기대치 설정\n\n<div class=\"content-ad\"></div>\n\n오픈 소스는 다양한 모델이 제공되므로 Meta와 같은 대규모 조직에서 제공하는 모델부터 개별 열정가들이 개발한 모델까지 수천 가지가 있습니다. 그러나 이러한 모델을 실행하는 것은 고유의 일련의 도전 과제를 제시할 수 있습니다:\n\n- 강력한 하드웨어가 필요할 수 있습니다: 많은 메모리와 가능한 경우 GPU\n- 오픈 소스 모델은 개선되고 있지만, 대개 ChatGPT와 같은 더 정교한 제품의 능력을 따라잡지 못할 수 있습니다. 이러한 제품들은 대규모 엔지니어 팀의 지원을 받고 있습니다.\n- 모든 모델을 상업적으로 사용할 수 있는 것은 아닙니다.\n\nGoogle의 유출 문서에 따르면 오픈 소스와 폐쇄 소스 모델 간의 간격이 좁혀지고 있다고 합니다.\n\n![이미지](/assets/img/2024-06-19-6WaystoRunLLMsLocally_0.png)\n\n<div class=\"content-ad\"></div>\n\n# 1. Hugging Face와 Transformers\n\nHugging Face은 머신러닝과 인공지능을 위한 도커 허브와 같은 서비스로, 다양한 오픈소스 모델을 제공합니다. 다행히도, Hugging Face는 주기적으로 모델을 평가하고 가장 좋은 모델을 선택할 수 있도록 리더보드를 제공합니다.\n\n또한, Hugging Face는 transformers라는 파이썬 라이브러리도 제공하는데, 이 라이브러리는 로컬에서 LLM을 간편하게 실행할 수 있게 해줍니다. 아래 예제는 라이브러리를 사용하여 이전 버전의 GPT-2 microsoft/DialoGPT-medium 모델을 실행하는 방법을 보여줍니다. 첫 번째 실행 시 Transformers는 모델을 다운로드하고, 이 모델을 사용하여 다섯 번의 대화를 할 수 있습니다. 이 스크립트를 실행하기 위해서는 PyTorch도 설치되어 있어야 합니다.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n```\n\n<div class=\"content-ad\"></div>\n\n```js\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side='left')\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n# 출처: https://huggingface.co/microsoft/DialoGPT-medium\n# 5줄 동안 채팅해 봅시다\nfor step in range(5):\n    # 새로운 사용자 입력을 인코딩하고 eos_token을 추가하여 Pytorch의 텐서를 반환합니다\n    new_user_input_ids = tokenizer.encode(input(\">> 사용자:\") + tokenizer.eos_token, return_tensors='pt')\n    # 채팅 기록 텐서에 새로운 사용자 입력 토큰을 추가합니다\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n    # 총 채팅 기록을 1000 토큰으로 제한하며 응답을 생성합니다\n    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n    # 최신 출력 토큰을 이쁘게 출력합니다\n    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\r\n```\n\nTransformers 장점:\n\n- 모델 자동 다운로드\n- 코드 조각 사용 가능\n- 실험 및 학습에 이상적\n\nTransformers 단점:\n\n<div class=\"content-ad\"></div>\n\n- ML 및 NLP에 대한 좋은 이해가 필요합니다.\n- 코딩 및 구성 기술이 필요합니다.\n\n## 2. LangChain\n\n로컬에서 LLM을 실행하는 또 다른 방법은 LangChain을 사용하는 것입니다. LangChain은 AI 애플리케이션을 구축하기 위한 Python 프레임워크입니다. 지원하는 모델 중 하나 위에 AI 애플리케이션을 개발하기 위한 추상화 및 미들웨어를 제공합니다. 예를 들어 아래 코드는 microsoft/DialoGPT-medium 모델에 한 가지 질문을 하는 코드입니다:\n\n```js\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\n``` \n\n<div class=\"content-ad\"></div>\n\n```js\nhf = HuggingFacePipeline.from_model_id(\n    model_id=\"microsoft/DialoGPT-medium\", task=\"text-generation\", pipeline_kwargs={\"max_new_tokens\": 200, \"pad_token_id\": 50256},\n)\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"Question: {question}\nAnswer: Let's think step by step.\"\"\"\nprompt = PromptTemplate.from_template(template)\nchain = prompt | hf\nquestion = \"What is electroencephalography?\"\nprint(chain.invoke({\"question\": question}))\n```\n\nLangChain Pros:\n\n- 모델 관리가 쉽습니다\n- AI 응용 프로그램 개발을 위한 유용한 유틸리티들\n\nLangChain Cons:\n\n<div class=\"content-ad\"></div>\n\n- 속도가 제한되어 있어 Transformers와 동일합니다\n- 여전히 애플리케이션 로직을 코딩하거나 적합한 UI를 만들어야 합니다.\n\n# 3. Llama.cpp\n\nLlama.cpp은 LLM을 위한 C 및 C++ 기반 추론 엔진으로, Apple 실리콘에 최적화되어 있으며 Meta의 Llama2 모델을 실행할 수 있습니다.\n\n저장소를 복제하고 프로젝트를 빌드한 후에는 다음을 사용하여 모델을 실행할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n$ ./main -m /path/to/model-file.gguf -p \"안녕하세요!\"\n```\n\nLlama.cpp의 장점:\n\n- Python 기반의 솔루션보다 높은 성능\n- Llama 7B와 같은 대형 모델을 저사양 하드웨어에서 지원\n- 다른 언어로 AI 애플리케이션을 빌드하기 위한 바인딩을 제공하며 추론은 Llama.cpp를 통해 실행됨.\n\nLlama.cpp의 단점:\n\n<div class=\"content-ad\"></div>\n\n- 제한된 모델 지원\n- 도구 빌딩이 필요합니다\n\n## 4. 람파파일\n\nMozilla에서 개발한 람파파일은 LLM(?)을 실행하는 사용자 친화적인 대안을 제공합니다. 람파파일은 휴대성과 하나의 파일로 실행 가능한 능력으로 유명합니다.\n\n람파파일을 다운로드한 후 GGUF 형식의 모델과 함께 사용하면 로컬 브라우저 세션을 시작할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n$ ./llamafile -m /path/to/model.gguf\n```\n\nLlamafile 장점:\n\n- Llama.cpp와 동일한 속도 장점을 누릴 수 있습니다.\n- 모델이 포함된 단일 실행 파일을 빌드할 수 있습니다.\n\nLlamafile 단점:\n\n<div class=\"content-ad\"></div>\n\n- 프로젝트는 아직 초기 단계에 있어요.\n- 모든 모델이 지원되는 것은 아니에요. Llama.cpp가 지원하는 모델만 지원돼요.\n\n# 5. Ollama\n\nOllama는 Llama.cpp와 Llamafile에 대한 더 사용하기 쉬운 대안이에요. 실행 파일을 다운로드하여 설치하는 서비스를 머신에 설치해요. 설치가 완료되면 터미널을 열고 아래와 같이 실행해주세요.\n\n```js\n$ ollama run llama2\n```\n\n<div class=\"content-ad\"></div>\n\nOllama는 모델을 다운로드하고 대화형 세션을 시작할 것입니다.\n\nOllama 장점:\n\n- 설치 및 사용이 쉽습니다.\n- 람마와 비쿠냐 모델을 실행할 수 있습니다.\n- 속도가 정말 빠릅니다.\n\nOllama 단점:\n\n<div class=\"content-ad\"></div>\n\n- 모델 라이브러리 제공이 제한적입니다.\n- 모델을 스스로 관리하며, 사용자 지정 모델을 재사용할 수 없습니다.\n- LLM을 실행할 때 조정 가능한 옵션이 없습니다.\n- 아직 Windows 버전은 없습니다.\n\n# 6. GPT4ALL\n\nGPT4ALL은 직관적인 GUI를 갖춘 사용하기 쉬운 데스크톱 응용 프로그램입니다. 로컬 모델 실행을 지원하며, OpenAI에 API 키를 사용하여 연결할 수 있습니다. GPT4ALL은 문맥을 위해 로컬 문서를 처리하는 능력으로 두드러집니다. 개인 정보 보호를 보장합니다.\n\n![이미지](/assets/img/2024-06-19-6WaystoRunLLMsLocally_1.png)\n\n<div class=\"content-ad\"></div>\n\n장점:\n\n- 친근한 UI를 가진 정교한 대안\n- 다양한 선별된 모델을 지원\n\n단점:\n\n- 모델 선택이 제한적\n- 일부 모델은 상업적 이용 제약이 있음\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\nLLM을 로컬에서 실행할 수 있는 적합한 도구를 선택하는 것은 여러분의 요구사항과 전문 지식에 달려 있어요. GPT4ALL과 같은 사용자 친화적인 응용 프로그램부터 Llama.cpp 및 Python 기반의 더 기술적인 옵션까지 다양한 선택지가 있어요. 오픈 소스 모델들이 새로운 기능들을 제공하며, 데이터와 개인 정보 보호에 대한 더 많은 통제 기회를 제공하고 있어요.\n\n본 안내서는 로컬 LLM 세계를 탐색하는 데 도움이 되는 직관성을 제공하고 있어요. 이러한 모델들이 발전함에 따라, ChatGPT와 같은 제품들과 경쟁력을 갖출 것으로 약속되고 있어요.\n\n2023년 12월 14일에 https://semaphoreci.com에 게재된 내용입니다.","ogImage":{"url":"/assets/img/2024-06-19-6WaystoRunLLMsLocally_0.png"},"coverImage":"/assets/img/2024-06-19-6WaystoRunLLMsLocally_0.png","tag":["Tech"],"readingTime":6},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>상용 AI 및 대규모 언어 모델(LLM)은 한 가지 큰 단점이 있습니다: 개인 정보 보호 문제! 민감하거나 자사 데이터를 다룰 때 이러한 도구를 활용할 수 없습니다.</p>\n<p>이로 인해 로컬에서 개인 LLM을 운영하는 방법에 대해 알아야 합니다. 오픈 소스 모델은 해결책을 제시하지만 그들만의 일련의 도전과 혜택이 따릅니다.</p>\n<p>당신의 컴퓨터에서 실행할 수 있는 ChatGPT의 로컬 대안을 발견하기 위한 여정에 함께해주세요.</p>\n<h1>기대치 설정</h1>\n<p>오픈 소스는 다양한 모델이 제공되므로 Meta와 같은 대규모 조직에서 제공하는 모델부터 개별 열정가들이 개발한 모델까지 수천 가지가 있습니다. 그러나 이러한 모델을 실행하는 것은 고유의 일련의 도전 과제를 제시할 수 있습니다:</p>\n<ul>\n<li>강력한 하드웨어가 필요할 수 있습니다: 많은 메모리와 가능한 경우 GPU</li>\n<li>오픈 소스 모델은 개선되고 있지만, 대개 ChatGPT와 같은 더 정교한 제품의 능력을 따라잡지 못할 수 있습니다. 이러한 제품들은 대규모 엔지니어 팀의 지원을 받고 있습니다.</li>\n<li>모든 모델을 상업적으로 사용할 수 있는 것은 아닙니다.</li>\n</ul>\n<p>Google의 유출 문서에 따르면 오픈 소스와 폐쇄 소스 모델 간의 간격이 좁혀지고 있다고 합니다.</p>\n<p><img src=\"/assets/img/2024-06-19-6WaystoRunLLMsLocally_0.png\" alt=\"이미지\"></p>\n<h1>1. Hugging Face와 Transformers</h1>\n<p>Hugging Face은 머신러닝과 인공지능을 위한 도커 허브와 같은 서비스로, 다양한 오픈소스 모델을 제공합니다. 다행히도, Hugging Face는 주기적으로 모델을 평가하고 가장 좋은 모델을 선택할 수 있도록 리더보드를 제공합니다.</p>\n<p>또한, Hugging Face는 transformers라는 파이썬 라이브러리도 제공하는데, 이 라이브러리는 로컬에서 LLM을 간편하게 실행할 수 있게 해줍니다. 아래 예제는 라이브러리를 사용하여 이전 버전의 GPT-2 microsoft/DialoGPT-medium 모델을 실행하는 방법을 보여줍니다. 첫 번째 실행 시 Transformers는 모델을 다운로드하고, 이 모델을 사용하여 다섯 번의 대화를 할 수 있습니다. 이 스크립트를 실행하기 위해서는 PyTorch도 설치되어 있어야 합니다.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM, AutoTokenizer\n<span class=\"hljs-keyword\">import</span> torch\n</code></pre>\n<pre><code class=\"hljs language-js\">tokenizer = <span class=\"hljs-title class_\">AutoTokenizer</span>.<span class=\"hljs-title function_\">from_pretrained</span>(<span class=\"hljs-string\">\"microsoft/DialoGPT-medium\"</span>, padding_side=<span class=\"hljs-string\">'left'</span>)\nmodel = <span class=\"hljs-title class_\">AutoModelForCausalLM</span>.<span class=\"hljs-title function_\">from_pretrained</span>(<span class=\"hljs-string\">\"microsoft/DialoGPT-medium\"</span>)\n# 출처: <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//huggingface.co/microsoft/DialoGPT-medium</span>\n# <span class=\"hljs-number\">5</span>줄 동안 채팅해 봅시다\n<span class=\"hljs-keyword\">for</span> step <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">range</span>(<span class=\"hljs-number\">5</span>):\n    # 새로운 사용자 입력을 인코딩하고 eos_token을 추가하여 <span class=\"hljs-title class_\">Pytorch</span>의 텐서를 반환합니다\n    new_user_input_ids = tokenizer.<span class=\"hljs-title function_\">encode</span>(<span class=\"hljs-title function_\">input</span>(<span class=\"hljs-string\">\">> 사용자:\"</span>) + tokenizer.<span class=\"hljs-property\">eos_token</span>, return_tensors=<span class=\"hljs-string\">'pt'</span>)\n    # 채팅 기록 텐서에 새로운 사용자 입력 토큰을 추가합니다\n    bot_input_ids = torch.<span class=\"hljs-title function_\">cat</span>([chat_history_ids, new_user_input_ids], dim=-<span class=\"hljs-number\">1</span>) <span class=\"hljs-keyword\">if</span> step > <span class=\"hljs-number\">0</span> <span class=\"hljs-keyword\">else</span> new_user_input_ids\n    # 총 채팅 기록을 <span class=\"hljs-number\">1000</span> 토큰으로 제한하며 응답을 생성합니다\n    chat_history_ids = model.<span class=\"hljs-title function_\">generate</span>(bot_input_ids, max_length=<span class=\"hljs-number\">1000</span>, pad_token_id=tokenizer.<span class=\"hljs-property\">eos_token_id</span>)\n    # 최신 출력 토큰을 이쁘게 출력합니다\n    <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"DialoGPT: {}\"</span>.<span class=\"hljs-title function_\">format</span>(tokenizer.<span class=\"hljs-title function_\">decode</span>(chat_history_ids[:, bot_input_ids.<span class=\"hljs-property\">shape</span>[-<span class=\"hljs-number\">1</span>]:][<span class=\"hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-title class_\">True</span>)))\n</code></pre>\n<p>Transformers 장점:</p>\n<ul>\n<li>모델 자동 다운로드</li>\n<li>코드 조각 사용 가능</li>\n<li>실험 및 학습에 이상적</li>\n</ul>\n<p>Transformers 단점:</p>\n<ul>\n<li>ML 및 NLP에 대한 좋은 이해가 필요합니다.</li>\n<li>코딩 및 구성 기술이 필요합니다.</li>\n</ul>\n<h2>2. LangChain</h2>\n<p>로컬에서 LLM을 실행하는 또 다른 방법은 LangChain을 사용하는 것입니다. LangChain은 AI 애플리케이션을 구축하기 위한 Python 프레임워크입니다. 지원하는 모델 중 하나 위에 AI 애플리케이션을 개발하기 위한 추상화 및 미들웨어를 제공합니다. 예를 들어 아래 코드는 microsoft/DialoGPT-medium 모델에 한 가지 질문을 하는 코드입니다:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">llms</span>.<span class=\"hljs-property\">huggingface_pipeline</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">HuggingFacePipeline</span>\n</code></pre>\n<pre><code class=\"hljs language-js\">hf = <span class=\"hljs-title class_\">HuggingFacePipeline</span>.<span class=\"hljs-title function_\">from_model_id</span>(\n    model_id=<span class=\"hljs-string\">\"microsoft/DialoGPT-medium\"</span>, task=<span class=\"hljs-string\">\"text-generation\"</span>, pipeline_kwargs={<span class=\"hljs-string\">\"max_new_tokens\"</span>: <span class=\"hljs-number\">200</span>, <span class=\"hljs-string\">\"pad_token_id\"</span>: <span class=\"hljs-number\">50256</span>},\n)\n<span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">prompts</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">PromptTemplate</span>\ntemplate = <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"Question: {question}\nAnswer: Let's think step by step.\"</span><span class=\"hljs-string\">\"\"</span>\nprompt = <span class=\"hljs-title class_\">PromptTemplate</span>.<span class=\"hljs-title function_\">from_template</span>(template)\nchain = prompt | hf\nquestion = <span class=\"hljs-string\">\"What is electroencephalography?\"</span>\n<span class=\"hljs-title function_\">print</span>(chain.<span class=\"hljs-title function_\">invoke</span>({<span class=\"hljs-string\">\"question\"</span>: question}))\n</code></pre>\n<p>LangChain Pros:</p>\n<ul>\n<li>모델 관리가 쉽습니다</li>\n<li>AI 응용 프로그램 개발을 위한 유용한 유틸리티들</li>\n</ul>\n<p>LangChain Cons:</p>\n<ul>\n<li>속도가 제한되어 있어 Transformers와 동일합니다</li>\n<li>여전히 애플리케이션 로직을 코딩하거나 적합한 UI를 만들어야 합니다.</li>\n</ul>\n<h1>3. Llama.cpp</h1>\n<p>Llama.cpp은 LLM을 위한 C 및 C++ 기반 추론 엔진으로, Apple 실리콘에 최적화되어 있으며 Meta의 Llama2 모델을 실행할 수 있습니다.</p>\n<p>저장소를 복제하고 프로젝트를 빌드한 후에는 다음을 사용하여 모델을 실행할 수 있습니다:</p>\n<pre><code class=\"hljs language-js\">$ ./main -m /path/to/model-file.<span class=\"hljs-property\">gguf</span> -p <span class=\"hljs-string\">\"안녕하세요!\"</span>\n</code></pre>\n<p>Llama.cpp의 장점:</p>\n<ul>\n<li>Python 기반의 솔루션보다 높은 성능</li>\n<li>Llama 7B와 같은 대형 모델을 저사양 하드웨어에서 지원</li>\n<li>다른 언어로 AI 애플리케이션을 빌드하기 위한 바인딩을 제공하며 추론은 Llama.cpp를 통해 실행됨.</li>\n</ul>\n<p>Llama.cpp의 단점:</p>\n<ul>\n<li>제한된 모델 지원</li>\n<li>도구 빌딩이 필요합니다</li>\n</ul>\n<h2>4. 람파파일</h2>\n<p>Mozilla에서 개발한 람파파일은 LLM(?)을 실행하는 사용자 친화적인 대안을 제공합니다. 람파파일은 휴대성과 하나의 파일로 실행 가능한 능력으로 유명합니다.</p>\n<p>람파파일을 다운로드한 후 GGUF 형식의 모델과 함께 사용하면 로컬 브라우저 세션을 시작할 수 있습니다:</p>\n<pre><code class=\"hljs language-js\">$ ./llamafile -m /path/to/model.<span class=\"hljs-property\">gguf</span>\n</code></pre>\n<p>Llamafile 장점:</p>\n<ul>\n<li>Llama.cpp와 동일한 속도 장점을 누릴 수 있습니다.</li>\n<li>모델이 포함된 단일 실행 파일을 빌드할 수 있습니다.</li>\n</ul>\n<p>Llamafile 단점:</p>\n<ul>\n<li>프로젝트는 아직 초기 단계에 있어요.</li>\n<li>모든 모델이 지원되는 것은 아니에요. Llama.cpp가 지원하는 모델만 지원돼요.</li>\n</ul>\n<h1>5. Ollama</h1>\n<p>Ollama는 Llama.cpp와 Llamafile에 대한 더 사용하기 쉬운 대안이에요. 실행 파일을 다운로드하여 설치하는 서비스를 머신에 설치해요. 설치가 완료되면 터미널을 열고 아래와 같이 실행해주세요.</p>\n<pre><code class=\"hljs language-js\">$ ollama run llama2\n</code></pre>\n<p>Ollama는 모델을 다운로드하고 대화형 세션을 시작할 것입니다.</p>\n<p>Ollama 장점:</p>\n<ul>\n<li>설치 및 사용이 쉽습니다.</li>\n<li>람마와 비쿠냐 모델을 실행할 수 있습니다.</li>\n<li>속도가 정말 빠릅니다.</li>\n</ul>\n<p>Ollama 단점:</p>\n<ul>\n<li>모델 라이브러리 제공이 제한적입니다.</li>\n<li>모델을 스스로 관리하며, 사용자 지정 모델을 재사용할 수 없습니다.</li>\n<li>LLM을 실행할 때 조정 가능한 옵션이 없습니다.</li>\n<li>아직 Windows 버전은 없습니다.</li>\n</ul>\n<h1>6. GPT4ALL</h1>\n<p>GPT4ALL은 직관적인 GUI를 갖춘 사용하기 쉬운 데스크톱 응용 프로그램입니다. 로컬 모델 실행을 지원하며, OpenAI에 API 키를 사용하여 연결할 수 있습니다. GPT4ALL은 문맥을 위해 로컬 문서를 처리하는 능력으로 두드러집니다. 개인 정보 보호를 보장합니다.</p>\n<p><img src=\"/assets/img/2024-06-19-6WaystoRunLLMsLocally_1.png\" alt=\"이미지\"></p>\n<p>장점:</p>\n<ul>\n<li>친근한 UI를 가진 정교한 대안</li>\n<li>다양한 선별된 모델을 지원</li>\n</ul>\n<p>단점:</p>\n<ul>\n<li>모델 선택이 제한적</li>\n<li>일부 모델은 상업적 이용 제약이 있음</li>\n</ul>\n<h1>결론</h1>\n<p>LLM을 로컬에서 실행할 수 있는 적합한 도구를 선택하는 것은 여러분의 요구사항과 전문 지식에 달려 있어요. GPT4ALL과 같은 사용자 친화적인 응용 프로그램부터 Llama.cpp 및 Python 기반의 더 기술적인 옵션까지 다양한 선택지가 있어요. 오픈 소스 모델들이 새로운 기능들을 제공하며, 데이터와 개인 정보 보호에 대한 더 많은 통제 기회를 제공하고 있어요.</p>\n<p>본 안내서는 로컬 LLM 세계를 탐색하는 데 도움이 되는 직관성을 제공하고 있어요. 이러한 모델들이 발전함에 따라, ChatGPT와 같은 제품들과 경쟁력을 갖출 것으로 약속되고 있어요.</p>\n<p>2023년 12월 14일에 <a href=\"https://semaphoreci.com%EC%97%90\" rel=\"nofollow\" target=\"_blank\">https://semaphoreci.com에</a> 게재된 내용입니다.</p>\n</body>\n</html>\n"},"__N_SSG":true}