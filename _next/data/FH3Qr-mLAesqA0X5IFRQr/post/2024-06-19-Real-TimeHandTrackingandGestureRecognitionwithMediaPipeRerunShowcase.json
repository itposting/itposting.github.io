{"pageProps":{"post":{"title":"미디어파이프를 사용한 실시간 손 추적 및 제스처 인식 재생 횟수 쇼케이스","description":"","date":"2024-06-19 18:31","slug":"2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase","content":"\n\n## 미디어파이프의 손 추척 및 제스처 인식을 Rerun과 함께 시각화하는 방법\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*pE_4QrsVPV7vMrxB6YS1cQ.gif)\n\n이 게시물에서는 미디어파이프 파이썬과 Rerun SDK를 사용하여 손 추척 및 제스처 인식의 예제를 소개하고 있습니다.\n\n더 깊이 파고들고 이해를 넓히고 싶다면, 미디어파이프 파이썬 및 Rerun SDK를 설치하여 손을 추적하고 다양한 제스처를 인식하고 데이터를 시각화하는 방법을 안내해 드리겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## 그러므로, 다음을 배울 것입니다:\n\n- MediaPipe Python 및 Rerun 설치하는 방법\n- MediaPipe 제스처 인식을 사용한 손 추적 및 제스처 인식 방법\n- 손 추적 및 제스처 인식 결과를 Rerun Viewer에서 시각화하는 방법\n\n예제를 시도하기를 열망한다면, 아래 제공된 코드를 사용해보세요:\n\n```js\n# rerun GitHub 저장소를 로컬 머신에 클론합니다.\ngit clone https://github.com/rerun-io/rerun\n\n# rerun 저장소 디렉토리로 이동합니다.\ncd rerun\n\n# 필요한 Python 패키지를 requirements 파일에 명시된대로 설치합니다.\npip install -r examples/python/gesture_detection/requirements.txt\n\n# 예제를 위한 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py\n\n# 특정 이미지에 대한 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py --image path/to/your/image.jpg\n\n# 특정 비디오에 대한 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py --video path/to/your/video.mp4\n\n# 카메라 스트림으로 주요 Python 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.py --camera\n```\n\n<div class=\"content-ad\"></div>\n\n# 손 추적 및 제스처 인식 기술\n\n계속하기 전에, 우리가 가능하게 한 기술에 대해 인정해주어야 합니다. 손 추적 및 제스처 인식 기술은 기기가 손 움직임과 제스처를 명령이나 입력으로 해석할 수 있도록 하는 것을 목표로 합니다. 이 기술의 핵심은 미리 훈련된 기계 학습 모델이 시각 입력을 분석하고 손의 랜드마크와 제스처를 식별합니다. 이러한 기술의 실제 응용은 다양하며, 손 움직임과 제스처를 사용하여 스마트 기기를 제어하는 데 사용될 수 있습니다. 인간-컴퓨터 상호 작용, 로봇 공학, 게임 및 증강 현실은 이 기술의 잠재적인 응용 분야 중 가장 유망하게 보입니다.\n\n그러나 이러한 기술을 사용하는 방법에 대해 항상 주의해야 합니다. 민감하고 중요한 시스템에서 사용시 손 제스처를 잘못 해석할 수 있고, 잘못된 양성 또는 음성의 가능성이 작지 않습니다. 이를 활용함으로써 발생하는 윤리적 및 법적 문제가 사용자들이 특히 공공장소에서 자신의 제스처가 기록되는 것을 원치 않을 수 있습니다. 현실 세계 시나리오에서 이 기술을 도입하기로 결정했다면, 윤리적 및 법적 고려 사항을 고려하는 것이 중요합니다.\n\n# 요구 사항 및 설정\n\n<div class=\"content-ad\"></div>\n\n먼저, 필요한 라이브러리를 설치해야 합니다. 이는 OpenCV, MediaPipe 및 Rerun과 같은 라이브러리를 포함합니다. MediaPipe Python은 컴퓨터 비전 및 머신러닝을 위한 온디바이스 ML 솔루션을 통합하려는 개발자들에게 유용한 도구이며, Rerun은 시간이 지남에 따라 변화하는 다중 모달 데이터를 시각화하기 위한 SDK입니다.\n\n```js\n# 요구 사항 파일에서 지정된 필수 Python 패키지 설치\npip install -r examples/python/gesture_detection/requirements.txt\n```\n\n그런 다음, 여기서 미리 정의된 모델을 다운로드해야 합니다: HandGestureClassifier\n\n# MediaPipe를 사용한 손 추적과 제스처 인식\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png\" />\n\n이제 샘플 이미지에 제스처 인식을 위해 MediaPipe 사전 훈련 모델을 사용해봅시다. 아래 코드는 MediaPipe 제스처 인식 솔루션의 초기화 및 구성을 설정하는 기초를 제공합니다.\n\n```js\nfrom mediapipe.tasks.python import vision\nfrom mediapipe.tasks import python\n\nclass GestureDetectorLogger:\n\n    def __init__(self, video_mode: bool = False):\n        self._video_mode = video_mode\n\n        base_options = python.BaseOptions(\n            model_asset_path='gesture_recognizer.task'\n        )\n        options = vision.GestureRecognizerOptions(\n            base_options=base_options,\n            running_mode=mp.tasks.vision.RunningMode.VIDEO if self._video_mode else mp.tasks.vision.RunningMode.IMAGE\n        )\n        self.recognizer = vision.GestureRecognizer.create_from_options(options)\n\n\n    def detect(self, image: npt.NDArray[np.uint8]) -> None:\n        image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n  \n        # 제스처 검출 모델로부터 결과 가져오기\n        recognition_result = self.recognizer.recognize(image)\n  \n        for i, gesture in enumerate(recognition_result.gestures):\n            # 인식된 제스처 중 상위 제스처 가져오기\n            print(\"최상위 제스처 결과: \", gesture[0].category_name)\n  \n        if recognition_result.hand_landmarks:\n            # MediaPipe에서 손 랜드마크 가져오기\n            hand_landmarks = recognition_result.hand_landmarks\n            print(\"손 랜드마크: \" + str(hand_landmarks))\n  \n            # MediaPipe에서 손 연결 정보 가져오기\n            mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n            print(\"손 연결 정보: \" + str(mp_hands_connections))\n```\n\nGestureDetectorLogger 클래스 내의 detect 함수는 이미지를 인자로 받아 모델 결과를 출력하며, 인식된 최상위 제스처와 감지된 손 랜드마크를 강조합니다. 모델에 대한 추가 정보는 해당 모델 카드를 참조하세요.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_1.png)\n\n아래 코드를 사용하여 직접 시도해볼 수 있어요:\n\n```js\ndef run_from_sample_image(path)-> None:\n    image = cv2.imread(str(path))\n    show_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    logger = GestureDetectorLogger(video_mode=False)\n    logger.detect_and_log(show_image)\n\n# 샘플 이미지로 제스처 인식 실행하기\nrun_from_sample_image(SAMPLE_IMAGE_PATH)\n```\n\n# 재실행을 사용하여 확인, 디버그 및 데모하기\n\n\n<div class=\"content-ad\"></div>\n\n이 단계는 솔루션의 신뢰성과 효과성을 보장하는 데 도움이 됩니다. 모델을 준비한 상태로 결과를 시각화하여 정확성을 확인하고 잠재적인 문제를 해결하며 능력을 시연할 수 있습니다. 결과를 시각화해서 Rerun SDK를 사용하면 간단하고 빠르게 가능합니다.\n\n## Rerun을 어떻게 사용할까요?\n\n![이미지](/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_2.png)\n\n- Rerun SDK를 사용하여 코드에서 로깅하여 다중 데이터를 스트림으로 전송\n- 현지 또는 원격으로 라이브 또는 녹화된 스트림을 시각화하고 상호 작용\n- 레이아웃을 대화식으로 구축하고 시각화를 사용자 정의\n- 필요할 때 Rerun을 확장\n\n<div class=\"content-ad\"></div>\n\n코드 작성에 앞서, Rerun Viewer를 설치하기 위해 해당 페이지를 방문하는 것이 좋습니다. 그런 다음, Rerun SDK에 대해 읽어보는 것을 권해드립니다. 파이썬 빠른 시작 가이드와 파이썬에서 데이터 기록하기를 읽어보세요. 이러한 초기 단계는 원활한 설정을 보장하고 다가오는 코드 실행에 도움이 될 것입니다.\n\n## 비디오 또는 실시간 실행\n\n비디오 스트리밍에는 OpenCV가 사용됩니다. 특정 비디오의 파일 경로를 선택하거나 0 또는 1의 인수를 제공하여 자체 카메라에 액세스할 수 있습니다 (기본 카메라를 사용하려면 0을 사용하고, 맥에서는 1을 사용할 수 있습니다).\n\n타임라인의 소개를 강조하는 것이 중요합니다. Rerun 타임라인의 기능은 데이터를 하나 이상의 타임라인과 연관시킬 수 있게 합니다. 결과적으로, 비디오의 각 프레임은 해당 타임스탬프와 연관되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndef run_from_video_capture(vid: int | str, max_frame_count: int | None) -> None:\n    \"\"\"\n    비디오 스트림에서 탐지기를 실행합니다.\n\n    매개변수\n    ----------\n    vid:\n        탐지기가 실행될 비디오 스트림입니다. 기본 카메라에는 0/1을 사용하거나 비디오 파일의 경로를 지정하세요.\n    max_frame_count:\n        처리할 최대 프레임 수입니다. None이면 모든 프레임을 처리합니다.\n    \"\"\"\n    cap = cv2.VideoCapture(vid)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n\n    detector = GestureDetectorLogger(video_mode=True)\n\n    try:\n        it: Iterable[int] = itertools.count() if max_frame_count is None else range(max_frame_count)\n\n        for frame_idx in tqdm.tqdm(it, desc=\"프레임 처리 중\"):\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            if np.all(frame == 0):\n                continue\n\n            frame_time_nano = int(cap.get(cv2.CAP_PROP_POS_MSEC) * 1e6)\n            if frame_time_nano == 0:\n                frame_time_nano = int(frame_idx * 1000 / fps * 1e6)\n\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n            rr.set_time_sequence(\"프레임 번호\", frame_idx)\n            rr.set_time_nanos(\"프레임 시간\", frame_time_nano)\n            detector.detect_and_log(frame, frame_time_nano)\n            rr.log(\n                \"미디어/비디오\",\n                rr.Image(frame)\n            )\n\n    except KeyboardInterrupt:\n        pass\n\n    cap.release()\n    cv2.destroyAllWindows()\n```\n\n## 시각화를 위한 데이터 로깅\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*c1Us-7PoWSP0rgVdlMQUhA.gif\" />\n\nRerun Viewer에서 데이터를 시각화하려면 Rerun SDK를 사용하여 데이터를 로깅하는 것이 중요합니다. 이전에 언급된 가이드는 이 프로세스에 대한 통찰을 제공합니다. 이 문맥에서는 정규화된 값으로 손 랜드마크 포인트를 추출한 다음, 이미지의 너비와 높이를 사용하여 이미지 좌표로 변환합니다. 이러한 좌표는 2D 포인트로 Rerun SDK에 로깅됩니다. 추가로, 랜드마크 간의 연결을 식별하고 2D 라인스트립으로 로깅합니다.\n\n<div class=\"content-ad\"></div>\n\n제스처 인식을 위해 결과는 콘솔에 출력됩니다. 그러나 소스 코드 안에서는 TextDocument 및 이모지를 사용하여 이러한 결과를 시청자에게 제시하는 방법을 탐구할 수 있습니다.\n\n```js\nclass GestureDetectorLogger:\n\n    def detect_and_log(self, image: npt.NDArray[np.uint8], frame_time_nano: int | None) -> None:\n        # 이미지에서 제스처 인식\n        height, width, _ = image.shape\n        image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n\n        recognition_result = (\n            self.recognizer.recognize_for_video(image, int(frame_time_nano / 1e6))\n            if self._video_mode\n            else self.recognizer.recognize(image)\n        )\n\n        # 값 지우기\n        for log_key in [\"Media/Points\", \"Media/Connections\"]:\n            rr.log(log_key, rr.Clear(recursive=True))\n\n        for i, gesture in enumerate(recognition_result.gestures):\n            # 인식된 제스처를 기록\n            gesture_category = gesture[0].category_name if recognition_result.gestures else \"None\"\n            print(\"제스처 카테고리:\", gesture_category)\n\n        if recognition_result.hand_landmarks:\n            hand_landmarks = recognition_result.hand_landmarks\n\n            # 정규화된 좌표를 이미지 좌표로 변환\n            points = self.convert_landmarks_to_image_coordinates(hand_landmarks, width, height)\n\n            # 이미지 및 Hand Entity에 점 기록\n            rr.log(\n               \"Media/Points\",\n                rr.Points2D(points, radii=10, colors=[255, 0, 0])\n            )\n\n            # MediaPipe에서 손 연결 가져오기\n            mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n            points1 = [points[connection[0]] for connection in mp_hands_connections]\n            points2 = [points[connection[1]] for connection in mp_hands_connections]\n\n            # 이미지와 Hand Entity에 연결 기록\n            rr.log(\n               \"Media/Connections\",\n                rr.LineStrips2D(\n                   np.stack((points1, points2), axis=1),\n                   colors=[255, 165, 0]\n                )\n             )\n\n    def convert_landmarks_to_image_coordinates(hand_landmarks, width, height):\n        return [(int(lm.x * width), int(lm.y * height)) for hand_landmark in hand_landmarks for lm in hand_landmark]\n```\n\n## 3D Points\n\n마지막으로, 손 랜드마크를 3D 포인트로 표시하는 방법을 살펴봅니다. 먼저 init 함수에서 Annotation Context의 키포인트를 사용하여 포인트 사이의 연결을 정의한 다음 3D 포인트로 기록합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*rNILX857c8TfScr6t7KKgQ.gif)\n\n```python\nclass GestureDetectorLogger:\n\n    def __init__(self, video_mode: bool = False):\n        # ... existing code ...\n        rr.log(\n            \"/\",\n            rr.AnnotationContext(\n                rr.ClassDescription(\n                    info=rr.AnnotationInfo(id=0, label=\"Hand3D\"),\n                    keypoint_connections=mp.solutions.hands.HAND_CONNECTIONS\n                )\n            ),\n            timeless=True\n        )\n        rr.log(\"Hand3D\", rr.ViewCoordinates.RIGHT_HAND_X_DOWN, timeless=True)\n\n\n    def detect_and_log(self, image: npt.NDArray[np.uint8], frame_time_nano: int | None) -> None:\n        # ... existing code ...\n\n        if recognition_result.hand_landmarks:\n            hand_landmarks = recognition_result.hand_landmarks\n\n            landmark_positions_3d = self.convert_landmarks_to_3d(hand_landmarks)\n            if landmark_positions_3d is not None:\n                rr.log(\n                    \"Hand3D/Points\",\n                    rr.Points3D(landmark_positions_3d, radii=20, class_ids=0, keypoint_ids=[i for i in range(len(landmark_positions_3d))])\n                )\n\n        # ... existing code ...\n```\n\n준비 완료! 마법이 시작됩니다:\n\n```python\n# For image\nrun_from_sample_image(IMAGE_PATH)\n\n# For saved video\nrun_from_video_capture(VIDEO_PATH)\n\n# For Real-Time\nrun_from_video_capture(0) # mac may need 1\n```\n\n<div class=\"content-ad\"></div>\n\n이 예제의 전체 소스 코드는 GitHub에서 확인할 수 있습니다. 탐색하고 변경하며 구현의 내부 작업을 이해하는 데 자유롭게 사용하세요.\n\n# 손 추적 및 제스처 인식을 넘어서\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*zc2gezkjPuJMjuToD4gBOw.gif)\n\n마침내, 다양한 애플리케이션 범위에서 다양한 종류의 다중 모달 데이터를 시각화하는 데 관심이 있다면, Rerun Examples를 살펴보고 탐구할 것을 권장합니다. 이러한 예제는 잠재적인 현실 세계 사례를 강조하고 그러한 시각화 기술의 실용적인 응용에 대한 소중한 통찰력을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n만약 이 글이 유익하고 통찰력이 있었다면, 더 많은 내용을 기대해주세요! 나는 로봇공학과 컴퓨터 비전 시각화 게시물에 대해 깊이 있는 내용을 정기적으로 공유하고 있습니다. 놓치고 싶지 않은 미래 업데이트와 흥미로운 프로젝트를 위해 팔로우해주세요!\n\n또한, LinkedIn에서 저를 찾을 수 있습니다.\n\n비슷한 글:","ogImage":{"url":"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png"},"coverImage":"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_0.png","tag":["Tech"],"readingTime":12},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<h2>미디어파이프의 손 추척 및 제스처 인식을 Rerun과 함께 시각화하는 방법</h2>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*pE_4QrsVPV7vMrxB6YS1cQ.gif\" alt=\"image\"></p>\n<p>이 게시물에서는 미디어파이프 파이썬과 Rerun SDK를 사용하여 손 추척 및 제스처 인식의 예제를 소개하고 있습니다.</p>\n<p>더 깊이 파고들고 이해를 넓히고 싶다면, 미디어파이프 파이썬 및 Rerun SDK를 설치하여 손을 추적하고 다양한 제스처를 인식하고 데이터를 시각화하는 방법을 안내해 드리겠습니다.</p>\n<h2>그러므로, 다음을 배울 것입니다:</h2>\n<ul>\n<li>MediaPipe Python 및 Rerun 설치하는 방법</li>\n<li>MediaPipe 제스처 인식을 사용한 손 추적 및 제스처 인식 방법</li>\n<li>손 추적 및 제스처 인식 결과를 Rerun Viewer에서 시각화하는 방법</li>\n</ul>\n<p>예제를 시도하기를 열망한다면, 아래 제공된 코드를 사용해보세요:</p>\n<pre><code class=\"hljs language-js\"># rerun <span class=\"hljs-title class_\">GitHub</span> 저장소를 로컬 머신에 클론합니다.\ngit clone <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//github.com/rerun-io/rerun</span>\n\n# rerun 저장소 디렉토리로 이동합니다.\ncd rerun\n\n# 필요한 <span class=\"hljs-title class_\">Python</span> 패키지를 requirements 파일에 명시된대로 설치합니다.\npip install -r examples/python/gesture_detection/requirements.<span class=\"hljs-property\">txt</span>\n\n# 예제를 위한 주요 <span class=\"hljs-title class_\">Python</span> 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.<span class=\"hljs-property\">py</span>\n\n# 특정 이미지에 대한 주요 <span class=\"hljs-title class_\">Python</span> 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.<span class=\"hljs-property\">py</span> --image path/to/your/image.<span class=\"hljs-property\">jpg</span>\n\n# 특정 비디오에 대한 주요 <span class=\"hljs-title class_\">Python</span> 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.<span class=\"hljs-property\">py</span> --video path/to/your/video.<span class=\"hljs-property\">mp4</span>\n\n# 카메라 스트림으로 주요 <span class=\"hljs-title class_\">Python</span> 스크립트를 실행합니다.\npython examples/python/gesture_detection/main.<span class=\"hljs-property\">py</span> --camera\n</code></pre>\n<h1>손 추적 및 제스처 인식 기술</h1>\n<p>계속하기 전에, 우리가 가능하게 한 기술에 대해 인정해주어야 합니다. 손 추적 및 제스처 인식 기술은 기기가 손 움직임과 제스처를 명령이나 입력으로 해석할 수 있도록 하는 것을 목표로 합니다. 이 기술의 핵심은 미리 훈련된 기계 학습 모델이 시각 입력을 분석하고 손의 랜드마크와 제스처를 식별합니다. 이러한 기술의 실제 응용은 다양하며, 손 움직임과 제스처를 사용하여 스마트 기기를 제어하는 데 사용될 수 있습니다. 인간-컴퓨터 상호 작용, 로봇 공학, 게임 및 증강 현실은 이 기술의 잠재적인 응용 분야 중 가장 유망하게 보입니다.</p>\n<p>그러나 이러한 기술을 사용하는 방법에 대해 항상 주의해야 합니다. 민감하고 중요한 시스템에서 사용시 손 제스처를 잘못 해석할 수 있고, 잘못된 양성 또는 음성의 가능성이 작지 않습니다. 이를 활용함으로써 발생하는 윤리적 및 법적 문제가 사용자들이 특히 공공장소에서 자신의 제스처가 기록되는 것을 원치 않을 수 있습니다. 현실 세계 시나리오에서 이 기술을 도입하기로 결정했다면, 윤리적 및 법적 고려 사항을 고려하는 것이 중요합니다.</p>\n<h1>요구 사항 및 설정</h1>\n<p>먼저, 필요한 라이브러리를 설치해야 합니다. 이는 OpenCV, MediaPipe 및 Rerun과 같은 라이브러리를 포함합니다. MediaPipe Python은 컴퓨터 비전 및 머신러닝을 위한 온디바이스 ML 솔루션을 통합하려는 개발자들에게 유용한 도구이며, Rerun은 시간이 지남에 따라 변화하는 다중 모달 데이터를 시각화하기 위한 SDK입니다.</p>\n<pre><code class=\"hljs language-js\"># 요구 사항 파일에서 지정된 필수 <span class=\"hljs-title class_\">Python</span> 패키지 설치\npip install -r examples/python/gesture_detection/requirements.<span class=\"hljs-property\">txt</span>\n</code></pre>\n<p>그런 다음, 여기서 미리 정의된 모델을 다운로드해야 합니다: HandGestureClassifier</p>\n<h1>MediaPipe를 사용한 손 추적과 제스처 인식</h1>\n<p>이제 샘플 이미지에 제스처 인식을 위해 MediaPipe 사전 훈련 모델을 사용해봅시다. 아래 코드는 MediaPipe 제스처 인식 솔루션의 초기화 및 구성을 설정하는 기초를 제공합니다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> mediapipe.<span class=\"hljs-property\">tasks</span>.<span class=\"hljs-property\">python</span> <span class=\"hljs-keyword\">import</span> vision\n<span class=\"hljs-keyword\">from</span> mediapipe.<span class=\"hljs-property\">tasks</span> <span class=\"hljs-keyword\">import</span> python\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">GestureDetectorLogger</span>:\n\n    def <span class=\"hljs-title function_\">__init__</span>(self, <span class=\"hljs-attr\">video_mode</span>: bool = <span class=\"hljs-title class_\">False</span>):\n        self.<span class=\"hljs-property\">_video_mode</span> = video_mode\n\n        base_options = python.<span class=\"hljs-title class_\">BaseOptions</span>(\n            model_asset_path=<span class=\"hljs-string\">'gesture_recognizer.task'</span>\n        )\n        options = vision.<span class=\"hljs-title class_\">GestureRecognizerOptions</span>(\n            base_options=base_options,\n            running_mode=mp.<span class=\"hljs-property\">tasks</span>.<span class=\"hljs-property\">vision</span>.<span class=\"hljs-property\">RunningMode</span>.<span class=\"hljs-property\">VIDEO</span> <span class=\"hljs-keyword\">if</span> self.<span class=\"hljs-property\">_video_mode</span> <span class=\"hljs-keyword\">else</span> mp.<span class=\"hljs-property\">tasks</span>.<span class=\"hljs-property\">vision</span>.<span class=\"hljs-property\">RunningMode</span>.<span class=\"hljs-property\">IMAGE</span>\n        )\n        self.<span class=\"hljs-property\">recognizer</span> = vision.<span class=\"hljs-property\">GestureRecognizer</span>.<span class=\"hljs-title function_\">create_from_options</span>(options)\n\n\n    def <span class=\"hljs-title function_\">detect</span>(self, <span class=\"hljs-attr\">image</span>: npt.<span class=\"hljs-property\">NDArray</span>[np.<span class=\"hljs-property\">uint8</span>]) -> <span class=\"hljs-title class_\">None</span>:\n        image = mp.<span class=\"hljs-title class_\">Image</span>(image_format=mp.<span class=\"hljs-property\">ImageFormat</span>.<span class=\"hljs-property\">SRGB</span>, data=image)\n  \n        # 제스처 검출 모델로부터 결과 가져오기\n        recognition_result = self.<span class=\"hljs-property\">recognizer</span>.<span class=\"hljs-title function_\">recognize</span>(image)\n  \n        <span class=\"hljs-keyword\">for</span> i, gesture <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">enumerate</span>(recognition_result.<span class=\"hljs-property\">gestures</span>):\n            # 인식된 제스처 중 상위 제스처 가져오기\n            <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"최상위 제스처 결과: \"</span>, gesture[<span class=\"hljs-number\">0</span>].<span class=\"hljs-property\">category_name</span>)\n  \n        <span class=\"hljs-keyword\">if</span> recognition_result.<span class=\"hljs-property\">hand_landmarks</span>:\n            # <span class=\"hljs-title class_\">MediaPipe</span>에서 손 랜드마크 가져오기\n            hand_landmarks = recognition_result.<span class=\"hljs-property\">hand_landmarks</span>\n            <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"손 랜드마크: \"</span> + <span class=\"hljs-title function_\">str</span>(hand_landmarks))\n  \n            # <span class=\"hljs-title class_\">MediaPipe</span>에서 손 연결 정보 가져오기\n            mp_hands_connections = mp.<span class=\"hljs-property\">solutions</span>.<span class=\"hljs-property\">hands</span>.<span class=\"hljs-property\">HAND_CONNECTIONS</span>\n            <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"손 연결 정보: \"</span> + <span class=\"hljs-title function_\">str</span>(mp_hands_connections))\n</code></pre>\n<p>GestureDetectorLogger 클래스 내의 detect 함수는 이미지를 인자로 받아 모델 결과를 출력하며, 인식된 최상위 제스처와 감지된 손 랜드마크를 강조합니다. 모델에 대한 추가 정보는 해당 모델 카드를 참조하세요.</p>\n<p><img src=\"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_1.png\" alt=\"이미지\"></p>\n<p>아래 코드를 사용하여 직접 시도해볼 수 있어요:</p>\n<pre><code class=\"hljs language-js\">def <span class=\"hljs-title function_\">run_from_sample_image</span>(path)-> <span class=\"hljs-title class_\">None</span>:\n    image = cv2.<span class=\"hljs-title function_\">imread</span>(<span class=\"hljs-title function_\">str</span>(path))\n    show_image = cv2.<span class=\"hljs-title function_\">cvtColor</span>(image, cv2.<span class=\"hljs-property\">COLOR_BGR2RGB</span>)\n    logger = <span class=\"hljs-title class_\">GestureDetectorLogger</span>(video_mode=<span class=\"hljs-title class_\">False</span>)\n    logger.<span class=\"hljs-title function_\">detect_and_log</span>(show_image)\n\n# 샘플 이미지로 제스처 인식 실행하기\n<span class=\"hljs-title function_\">run_from_sample_image</span>(<span class=\"hljs-variable constant_\">SAMPLE_IMAGE_PATH</span>)\n</code></pre>\n<h1>재실행을 사용하여 확인, 디버그 및 데모하기</h1>\n<p>이 단계는 솔루션의 신뢰성과 효과성을 보장하는 데 도움이 됩니다. 모델을 준비한 상태로 결과를 시각화하여 정확성을 확인하고 잠재적인 문제를 해결하며 능력을 시연할 수 있습니다. 결과를 시각화해서 Rerun SDK를 사용하면 간단하고 빠르게 가능합니다.</p>\n<h2>Rerun을 어떻게 사용할까요?</h2>\n<p><img src=\"/assets/img/2024-06-19-Real-TimeHandTrackingandGestureRecognitionwithMediaPipeRerunShowcase_2.png\" alt=\"이미지\"></p>\n<ul>\n<li>Rerun SDK를 사용하여 코드에서 로깅하여 다중 데이터를 스트림으로 전송</li>\n<li>현지 또는 원격으로 라이브 또는 녹화된 스트림을 시각화하고 상호 작용</li>\n<li>레이아웃을 대화식으로 구축하고 시각화를 사용자 정의</li>\n<li>필요할 때 Rerun을 확장</li>\n</ul>\n<p>코드 작성에 앞서, Rerun Viewer를 설치하기 위해 해당 페이지를 방문하는 것이 좋습니다. 그런 다음, Rerun SDK에 대해 읽어보는 것을 권해드립니다. 파이썬 빠른 시작 가이드와 파이썬에서 데이터 기록하기를 읽어보세요. 이러한 초기 단계는 원활한 설정을 보장하고 다가오는 코드 실행에 도움이 될 것입니다.</p>\n<h2>비디오 또는 실시간 실행</h2>\n<p>비디오 스트리밍에는 OpenCV가 사용됩니다. 특정 비디오의 파일 경로를 선택하거나 0 또는 1의 인수를 제공하여 자체 카메라에 액세스할 수 있습니다 (기본 카메라를 사용하려면 0을 사용하고, 맥에서는 1을 사용할 수 있습니다).</p>\n<p>타임라인의 소개를 강조하는 것이 중요합니다. Rerun 타임라인의 기능은 데이터를 하나 이상의 타임라인과 연관시킬 수 있게 합니다. 결과적으로, 비디오의 각 프레임은 해당 타임스탬프와 연관되어 있습니다.</p>\n<pre><code class=\"hljs language-js\">def <span class=\"hljs-title function_\">run_from_video_capture</span>(<span class=\"hljs-attr\">vid</span>: int | str, <span class=\"hljs-attr\">max_frame_count</span>: int | <span class=\"hljs-title class_\">None</span>) -> <span class=\"hljs-title class_\">None</span>:\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    비디오 스트림에서 탐지기를 실행합니다.\n\n    매개변수\n    ----------\n    vid:\n        탐지기가 실행될 비디오 스트림입니다. 기본 카메라에는 0/1을 사용하거나 비디오 파일의 경로를 지정하세요.\n    max_frame_count:\n        처리할 최대 프레임 수입니다. None이면 모든 프레임을 처리합니다.\n    \"</span><span class=\"hljs-string\">\"\"</span>\n    cap = cv2.<span class=\"hljs-title class_\">VideoCapture</span>(vid)\n    fps = cap.<span class=\"hljs-title function_\">get</span>(cv2.<span class=\"hljs-property\">CAP_PROP_FPS</span>)\n\n    detector = <span class=\"hljs-title class_\">GestureDetectorLogger</span>(video_mode=<span class=\"hljs-title class_\">True</span>)\n\n    <span class=\"hljs-attr\">try</span>:\n        <span class=\"hljs-attr\">it</span>: <span class=\"hljs-title class_\">Iterable</span>[int] = itertools.<span class=\"hljs-title function_\">count</span>() <span class=\"hljs-keyword\">if</span> max_frame_count is <span class=\"hljs-title class_\">None</span> <span class=\"hljs-keyword\">else</span> <span class=\"hljs-title function_\">range</span>(max_frame_count)\n\n        <span class=\"hljs-keyword\">for</span> frame_idx <span class=\"hljs-keyword\">in</span> tqdm.<span class=\"hljs-title function_\">tqdm</span>(it, desc=<span class=\"hljs-string\">\"프레임 처리 중\"</span>):\n            ret, frame = cap.<span class=\"hljs-title function_\">read</span>()\n            <span class=\"hljs-keyword\">if</span> not <span class=\"hljs-attr\">ret</span>:\n                <span class=\"hljs-keyword\">break</span>\n\n            <span class=\"hljs-keyword\">if</span> np.<span class=\"hljs-title function_\">all</span>(frame == <span class=\"hljs-number\">0</span>):\n                <span class=\"hljs-keyword\">continue</span>\n\n            frame_time_nano = <span class=\"hljs-title function_\">int</span>(cap.<span class=\"hljs-title function_\">get</span>(cv2.<span class=\"hljs-property\">CAP_PROP_POS_MSEC</span>) * <span class=\"hljs-number\">1e6</span>)\n            <span class=\"hljs-keyword\">if</span> frame_time_nano == <span class=\"hljs-number\">0</span>:\n                frame_time_nano = <span class=\"hljs-title function_\">int</span>(frame_idx * <span class=\"hljs-number\">1000</span> / fps * <span class=\"hljs-number\">1e6</span>)\n\n            frame = cv2.<span class=\"hljs-title function_\">cvtColor</span>(frame, cv2.<span class=\"hljs-property\">COLOR_BGR2RGB</span>)\n\n            rr.<span class=\"hljs-title function_\">set_time_sequence</span>(<span class=\"hljs-string\">\"프레임 번호\"</span>, frame_idx)\n            rr.<span class=\"hljs-title function_\">set_time_nanos</span>(<span class=\"hljs-string\">\"프레임 시간\"</span>, frame_time_nano)\n            detector.<span class=\"hljs-title function_\">detect_and_log</span>(frame, frame_time_nano)\n            rr.<span class=\"hljs-title function_\">log</span>(\n                <span class=\"hljs-string\">\"미디어/비디오\"</span>,\n                rr.<span class=\"hljs-title class_\">Image</span>(frame)\n            )\n\n    except <span class=\"hljs-title class_\">KeyboardInterrupt</span>:\n        pass\n\n    cap.<span class=\"hljs-title function_\">release</span>()\n    cv2.<span class=\"hljs-title function_\">destroyAllWindows</span>()\n</code></pre>\n<h2>시각화를 위한 데이터 로깅</h2>\n<p>Rerun Viewer에서 데이터를 시각화하려면 Rerun SDK를 사용하여 데이터를 로깅하는 것이 중요합니다. 이전에 언급된 가이드는 이 프로세스에 대한 통찰을 제공합니다. 이 문맥에서는 정규화된 값으로 손 랜드마크 포인트를 추출한 다음, 이미지의 너비와 높이를 사용하여 이미지 좌표로 변환합니다. 이러한 좌표는 2D 포인트로 Rerun SDK에 로깅됩니다. 추가로, 랜드마크 간의 연결을 식별하고 2D 라인스트립으로 로깅합니다.</p>\n<p>제스처 인식을 위해 결과는 콘솔에 출력됩니다. 그러나 소스 코드 안에서는 TextDocument 및 이모지를 사용하여 이러한 결과를 시청자에게 제시하는 방법을 탐구할 수 있습니다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">GestureDetectorLogger</span>:\n\n    def <span class=\"hljs-title function_\">detect_and_log</span>(self, <span class=\"hljs-attr\">image</span>: npt.<span class=\"hljs-property\">NDArray</span>[np.<span class=\"hljs-property\">uint8</span>], <span class=\"hljs-attr\">frame_time_nano</span>: int | <span class=\"hljs-title class_\">None</span>) -> <span class=\"hljs-title class_\">None</span>:\n        # 이미지에서 제스처 인식\n        height, width, _ = image.<span class=\"hljs-property\">shape</span>\n        image = mp.<span class=\"hljs-title class_\">Image</span>(image_format=mp.<span class=\"hljs-property\">ImageFormat</span>.<span class=\"hljs-property\">SRGB</span>, data=image)\n\n        recognition_result = (\n            self.<span class=\"hljs-property\">recognizer</span>.<span class=\"hljs-title function_\">recognize_for_video</span>(image, <span class=\"hljs-title function_\">int</span>(frame_time_nano / <span class=\"hljs-number\">1e6</span>))\n            <span class=\"hljs-keyword\">if</span> self.<span class=\"hljs-property\">_video_mode</span>\n            <span class=\"hljs-keyword\">else</span> self.<span class=\"hljs-property\">recognizer</span>.<span class=\"hljs-title function_\">recognize</span>(image)\n        )\n\n        # 값 지우기\n        <span class=\"hljs-keyword\">for</span> log_key <span class=\"hljs-keyword\">in</span> [<span class=\"hljs-string\">\"Media/Points\"</span>, <span class=\"hljs-string\">\"Media/Connections\"</span>]:\n            rr.<span class=\"hljs-title function_\">log</span>(log_key, rr.<span class=\"hljs-title class_\">Clear</span>(recursive=<span class=\"hljs-title class_\">True</span>))\n\n        <span class=\"hljs-keyword\">for</span> i, gesture <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">enumerate</span>(recognition_result.<span class=\"hljs-property\">gestures</span>):\n            # 인식된 제스처를 기록\n            gesture_category = gesture[<span class=\"hljs-number\">0</span>].<span class=\"hljs-property\">category_name</span> <span class=\"hljs-keyword\">if</span> recognition_result.<span class=\"hljs-property\">gestures</span> <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">\"None\"</span>\n            <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"제스처 카테고리:\"</span>, gesture_category)\n\n        <span class=\"hljs-keyword\">if</span> recognition_result.<span class=\"hljs-property\">hand_landmarks</span>:\n            hand_landmarks = recognition_result.<span class=\"hljs-property\">hand_landmarks</span>\n\n            # 정규화된 좌표를 이미지 좌표로 변환\n            points = self.<span class=\"hljs-title function_\">convert_landmarks_to_image_coordinates</span>(hand_landmarks, width, height)\n\n            # 이미지 및 <span class=\"hljs-title class_\">Hand</span> <span class=\"hljs-title class_\">Entity</span>에 점 기록\n            rr.<span class=\"hljs-title function_\">log</span>(\n               <span class=\"hljs-string\">\"Media/Points\"</span>,\n                rr.<span class=\"hljs-title class_\">Points2D</span>(points, radii=<span class=\"hljs-number\">10</span>, colors=[<span class=\"hljs-number\">255</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>])\n            )\n\n            # <span class=\"hljs-title class_\">MediaPipe</span>에서 손 연결 가져오기\n            mp_hands_connections = mp.<span class=\"hljs-property\">solutions</span>.<span class=\"hljs-property\">hands</span>.<span class=\"hljs-property\">HAND_CONNECTIONS</span>\n            points1 = [points[connection[<span class=\"hljs-number\">0</span>]] <span class=\"hljs-keyword\">for</span> connection <span class=\"hljs-keyword\">in</span> mp_hands_connections]\n            points2 = [points[connection[<span class=\"hljs-number\">1</span>]] <span class=\"hljs-keyword\">for</span> connection <span class=\"hljs-keyword\">in</span> mp_hands_connections]\n\n            # 이미지와 <span class=\"hljs-title class_\">Hand</span> <span class=\"hljs-title class_\">Entity</span>에 연결 기록\n            rr.<span class=\"hljs-title function_\">log</span>(\n               <span class=\"hljs-string\">\"Media/Connections\"</span>,\n                rr.<span class=\"hljs-title class_\">LineStrips2D</span>(\n                   np.<span class=\"hljs-title function_\">stack</span>((points1, points2), axis=<span class=\"hljs-number\">1</span>),\n                   colors=[<span class=\"hljs-number\">255</span>, <span class=\"hljs-number\">165</span>, <span class=\"hljs-number\">0</span>]\n                )\n             )\n\n    def <span class=\"hljs-title function_\">convert_landmarks_to_image_coordinates</span>(hand_landmarks, width, height):\n        <span class=\"hljs-keyword\">return</span> [(<span class=\"hljs-title function_\">int</span>(lm.<span class=\"hljs-property\">x</span> * width), <span class=\"hljs-title function_\">int</span>(lm.<span class=\"hljs-property\">y</span> * height)) <span class=\"hljs-keyword\">for</span> hand_landmark <span class=\"hljs-keyword\">in</span> hand_landmarks <span class=\"hljs-keyword\">for</span> lm <span class=\"hljs-keyword\">in</span> hand_landmark]\n</code></pre>\n<h2>3D Points</h2>\n<p>마지막으로, 손 랜드마크를 3D 포인트로 표시하는 방법을 살펴봅니다. 먼저 init 함수에서 Annotation Context의 키포인트를 사용하여 포인트 사이의 연결을 정의한 다음 3D 포인트로 기록합니다.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*rNILX857c8TfScr6t7KKgQ.gif\" alt=\"image\"></p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">GestureDetectorLogger</span>:\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, video_mode: <span class=\"hljs-built_in\">bool</span> = <span class=\"hljs-literal\">False</span></span>):\n        <span class=\"hljs-comment\"># ... existing code ...</span>\n        rr.log(\n            <span class=\"hljs-string\">\"/\"</span>,\n            rr.AnnotationContext(\n                rr.ClassDescription(\n                    info=rr.AnnotationInfo(<span class=\"hljs-built_in\">id</span>=<span class=\"hljs-number\">0</span>, label=<span class=\"hljs-string\">\"Hand3D\"</span>),\n                    keypoint_connections=mp.solutions.hands.HAND_CONNECTIONS\n                )\n            ),\n            timeless=<span class=\"hljs-literal\">True</span>\n        )\n        rr.log(<span class=\"hljs-string\">\"Hand3D\"</span>, rr.ViewCoordinates.RIGHT_HAND_X_DOWN, timeless=<span class=\"hljs-literal\">True</span>)\n\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">detect_and_log</span>(<span class=\"hljs-params\">self, image: npt.NDArray[np.uint8], frame_time_nano: <span class=\"hljs-built_in\">int</span> | <span class=\"hljs-literal\">None</span></span>) -> <span class=\"hljs-literal\">None</span>:\n        <span class=\"hljs-comment\"># ... existing code ...</span>\n\n        <span class=\"hljs-keyword\">if</span> recognition_result.hand_landmarks:\n            hand_landmarks = recognition_result.hand_landmarks\n\n            landmark_positions_3d = self.convert_landmarks_to_3d(hand_landmarks)\n            <span class=\"hljs-keyword\">if</span> landmark_positions_3d <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n                rr.log(\n                    <span class=\"hljs-string\">\"Hand3D/Points\"</span>,\n                    rr.Points3D(landmark_positions_3d, radii=<span class=\"hljs-number\">20</span>, class_ids=<span class=\"hljs-number\">0</span>, keypoint_ids=[i <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-built_in\">len</span>(landmark_positions_3d))])\n                )\n\n        <span class=\"hljs-comment\"># ... existing code ...</span>\n</code></pre>\n<p>준비 완료! 마법이 시작됩니다:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># For image</span>\nrun_from_sample_image(IMAGE_PATH)\n\n<span class=\"hljs-comment\"># For saved video</span>\nrun_from_video_capture(VIDEO_PATH)\n\n<span class=\"hljs-comment\"># For Real-Time</span>\nrun_from_video_capture(<span class=\"hljs-number\">0</span>) <span class=\"hljs-comment\"># mac may need 1</span>\n</code></pre>\n<p>이 예제의 전체 소스 코드는 GitHub에서 확인할 수 있습니다. 탐색하고 변경하며 구현의 내부 작업을 이해하는 데 자유롭게 사용하세요.</p>\n<h1>손 추적 및 제스처 인식을 넘어서</h1>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*zc2gezkjPuJMjuToD4gBOw.gif\" alt=\"이미지\"></p>\n<p>마침내, 다양한 애플리케이션 범위에서 다양한 종류의 다중 모달 데이터를 시각화하는 데 관심이 있다면, Rerun Examples를 살펴보고 탐구할 것을 권장합니다. 이러한 예제는 잠재적인 현실 세계 사례를 강조하고 그러한 시각화 기술의 실용적인 응용에 대한 소중한 통찰력을 제공합니다.</p>\n<p>만약 이 글이 유익하고 통찰력이 있었다면, 더 많은 내용을 기대해주세요! 나는 로봇공학과 컴퓨터 비전 시각화 게시물에 대해 깊이 있는 내용을 정기적으로 공유하고 있습니다. 놓치고 싶지 않은 미래 업데이트와 흥미로운 프로젝트를 위해 팔로우해주세요!</p>\n<p>또한, LinkedIn에서 저를 찾을 수 있습니다.</p>\n<p>비슷한 글:</p>\n</body>\n</html>\n"},"__N_SSG":true}