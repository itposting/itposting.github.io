{"pageProps":{"post":{"title":"유튜브 트렌드 분석 파이프라인 Airflow, Spark, S3 및 Docker를 이용한 ETL","description":"","date":"2024-06-19 09:37","slug":"2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker","content":"\n\n이 기사에서는 Apache Airflow와 PySpark를 사용하여 자동 ETL (추출, 변환, 로드) 파이프라인을 만드는 방법을 안내합니다. 이 파이프라인은 YouTube Data API에서 트렌드 비디오 데이터를 가져와 처리한 후 처리된 데이터를 S3에 저장할 것입니다.\n\nTwitter API를 사용한 파이프라인을 보여주는 Darshil Parmar의 YouTube 비디오를 시청한 후, 유사한 프로젝트에 도전하기로 영감을 받았습니다. 그러나 Twitter API의 가격 정책 변경으로 인해, 시청자가 YouTube Data API를 대체로 제안했고 이것이 제 흥미를 자극했습니다.\n\n프로젝트에 돌입하기 전에 두 가지 필수 사항이 있습니다:\n\n1. Youtube Data API 키 획득\n\n<div class=\"content-ad\"></div>\n\n- Google Developers Console을 방문해 주세요.\n- 새 프로젝트를 생성해 주세요.\n- \"YouTube Data API\"를 검색하고 활성화해 주세요.\n- 새 자격 증명을 생성하고 프로젝트에서 나중에 사용할 API 키를 복사해 주세요.\n\n자세한 지침은 YouTube Data API 시작 가이드를 참조해 주세요.\n\n2. AWS 액세스 키 ID 및 비밀 액세스 키 획득\n\n- AWS Management Console에 로그인해 주세요.\n- IAM(Identity and Access Management) 섹션으로 이동하고 새 사용자를 생성해 주세요.\n- 필요한 S3 액세스 정책을 부여하고 액세스 키를 생성해 주세요.\n- 프로젝트에서 사용할 액세스 키 ID와 비밀 액세스 키를 안전하게 저장해 주세요.\n\n<div class=\"content-ad\"></div>\n\n이제 실제 프로젝트를 시작하겠습니다! 준비됐나요 여러분!!\n\n![YouTube Trend Analysis Pipeline ETL with Airflow, Spark, S3, and Docker](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png)\n\n이 글은 4가지 주요 단계로 구성되어 있어요:\n\n- 소프트웨어 설치 및 설정\n- Youtube Data API에서 데이터 추출\n- PySpark를 사용하여 데이터 변환\n- AWS S3로 데이터 로드\n\n<div class=\"content-ad\"></div>\n\n# 1. 소프트웨어 설치 및 설정:\n\n- VS Code — [VS Code 다운로드 및 설치](https://code.visualstudio.com/).\n- Docker Desktop — [Docker Desktop 다운로드 및 설치](https://www.docker.com/products/docker-desktop).\n- (선택사항) Windows Subsystem for Linux (WSL) — 데이터 엔지니어링에 사용되는 Apache Airflow 및 PySpark와 같은 많은 도구 및 라이브러리가 Unix 계열 시스템을 위해 개발되었습니다. 이러한 도구를 Windows에서 사용할 때 발생할 수 있는 호환성 문제를 피하기 위해 WSL을 통해 네이티브 Linux 환경에서 실행할 수 있습니다.\n  - ` 관리자 권한으로 PowerShell을 엽니다.\n  - ` 다음 명령을 실행하세요: wsl --install.\n  - ` 명령에 따라 WSL을 설치하고 Microsoft Store에서 Linux 배포판(예: Ubuntu)을 선택하세요.\n  - ` Linux 배포판에 사용자 이름 및 암호를 설정하세요.\n\n이 프로젝트를 실행하는 데 WSL이 반드시 필요한 것은 아닙니다. Docker Desktop은 Windows에서 네이티브로 실행될 수 있으며 Docker 자체가 관리하는 가벼운 Linux 가상 머신(VM)을 사용합니다. 그러나 Docker Desktop과 함께 WSL을 사용하면 Windows에서 직접 Linux 명령 및 작업을 실행할 수 있어 보다 네이티브한 개발 경험을 제공합니다.\n\n이제 설정을 시작해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n부분 1 — 도커 이미지 만들기\n\n- 프로젝트용 새 폴더를 만들고 \"Airflow-Project\"로 이름을 지어주세요.\n- 해당 폴더에서 명령 프롬프트를 엽니다.\n- 명령 프롬프트에서 아래 명령을 실행하세요:\n\n```bash\ncode .\n```\n\n- 이 명령은 VS Code에서 해당 폴더를 프로젝트로 엽니다.\n- VS Code에서 \"dockerfile\"이라는 새 파일을 만들고 아래 코드를 붙여넣으세요:\n\n<div class=\"content-ad\"></div>\n\n```js\nFROM apache/airflow:latest\n\n# 시스템 종속성을 설치하기 위해 루트 사용자로 전환합니다\nUSER root\n\n# git, OpenJDK를 설치하고 apt 캐시를 정리합니다\nRUN apt-get update && \\\n    apt-get -y install git default-jdk && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Python 패키지를 설치하기 위해 airflow 사용자로 전환합니다\nUSER airflow\n\n# 필요한 Python 패키지를 설치합니다\nRUN pip install --no-cache-dir pyspark pandas google-api-python-client emoji boto3\n```\n\n이 Docker 파일은 프로젝트를 실행하는 데 필요한 모든 패키지를 포함하고 있어요.\n\n- 파일을 마우스 오른쪽 버튼으로 클릭하고 VS Code에서 \"이미지 빌드\" 옵션을 선택하세요. 이름을 입력하라는 프롬프트가 나타나면 \"airflow-project\"를 입력하세요. 이 명령은 Docker 이미지를 생성합니다. 그러나 이미지를 사용하려면 docker-compose.yml 파일을 생성하고 이미지를 사용하도록 구성해야 합니다.\n\n(재미있는 사실: 파일에서 Python 설치가 없는 이유 궁금하신가요? 실제로 Dockerfile에서 사용된 기본 이미지인 apache/airflow:latest에는 Python이 이미 설치되어 있어요. 왜냐하면 Airflow 자체가 Python으로 작성되어 있기 때문에 주로 워크플로 및 작업 정의에 Python을 사용합니다. 따라서 Dockerfile에서 별도로 Python을 설치할 필요가 없답니다!)\n\n<div class=\"content-ad\"></div>\n\n파트 2 — 도커 컴포즈 파일 생성하기\n\n도커 컴포즈를 사용하면 멀티 컨테이너 도커 애플리케이션을 쉽게 다룰 수 있습니다. 이를 통해 단일 명령으로 여러 도커 컨테이너를 정의하고 실행할 수 있으며 각 서비스의 환경 변수, 볼륨, 포트 및 기타 설정을 명확하고 조직적인 방식으로 구성할 수 있습니다. 도커 컴포즈를 사용하면 단일 명령어인 docker-compose up 또는 docker-compose down을 사용하여 여러 서비스를 쉽게 시작, 중지 및 관리할 수 있습니다.\n\n- \"docker-compose.yml\" 파일을 생성하고 다음 코드를 파일에 붙여넣습니다:\n\n```js\nversion: '3'\nservices:\n\n  airflowproject:\n    image: airflow-project:latest\n    environment:\n      - AWS_ACCESS_KEY_ID=your-aws-access-key\n      - AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key\n      - YOUTUBE_API_KEY=your-youtube-api-key\n    volumes:\n      - ./airflow:/opt/airflow\n    ports:\n      - \"8080:8080\"\n    command: airflow standalone\n```\n\n<div class=\"content-ad\"></div>\n\n- 이제 파일을 마우스 오른쪽 버튼으로 클릭한 후 VS Code에서 'Compose Up' 옵션을 선택하세요. 환경을 설정하기 위해 클릭하세요.\n- 깜짝 놀랄 일이 벌어졌어요! 이 작업을 완료한 후에는 VS Code 프로젝트 디렉토리에 \"airflow\"라는 새 폴더가 나타날 수 있습니다.\n\nDocker 데스크톱을 열어서 모든 것이 올바르게 완료되었는지 확인하세요. 올바르게 완료된 경우 다음과 같은 화면이 표시됩니다.\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_1.png)\n\n- 이제 Airflow 프로젝트를 클릭하여 Airflow가 8080 포트에서 실행 중임을 나타내는 로그가 표시되는 화면을 엽니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_2.png)\n\n- 포트를 클릭하면 Airflow 로그인 페이지로 이동합니다. 이 링크를 처음 열어보는 경우 자격 증명을 제공해야 합니다.\n- 사용자 이름은 \"admin\"이고 비밀번호는 compose up 명령을 실행한 후 생성된 Airflow 폴더 내의 \"standalone_admin_password.txt\" 파일에 있습니다.\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_3.png)\n\n- 로그인 페이지에서 자격 증명을 입력한 후, 로컬 호스트에서 Airflow가 실행 중인 것을 확인할 수 있습니다. 다음과 같이 나타납니다:\n\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_4.png\" />\n\n당신의 환경 설정 완료입니다! 휴―!!\n\n# 2. YouTube 데이터 API에서 데이터 추출하기:\n\n<div class=\"content-ad\"></div>\n\n- Airflow 폴더 아래에 \"dags\"라는 이름의 폴더를 만들고, dags 폴더 아래에 \"youtube_etl_dag.py\"라는 파이썬 파일을 만듭니다.\n- 이제 \"youtube_etl_dag.py\" 파일에 다음을 import하세요.\n\n```js\nimport logging\nimport os\nimport re\nimport shutil\nfrom datetime import datetime, timedelta\n\nimport boto3\nimport emoji\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_date, udf\nfrom pyspark.sql.types import (DateType, IntegerType, LongType, StringType,\n                               StructField, StructType)\n\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n```\n\n- 이 프로젝트를 실행하는 데 위의 모든 라이브러리가 필요합니다(코드 작성을 시작하면 모두 유용해집니다)\n- VS Code에서 오류가 발생하는 것을 볼 수 있습니다. 그 이유는 모든 종속성이 도커에 설치되어 있지만 로컬 머신에는 설치되어 있지 않기 때문이므로 신경 쓰지 마십시오.\n- Airflow에서 구문 오류가 있으면 화면 상단에 표시되고, 논리 오류/예외는 Airflow 로그에서 확인할 수 있습니다.\n\n```js\n# DAG와 기본 인수 정의\ndefault_args = {\n    'owner': 'airflow',  # DAG 소유자\n    'depends_on_past': False,  # 과거 DAG 실행에 종속하는지 여부\n    'email_on_failure': False,  # 실패 시 이메일 알림 비활성화\n    'email_on_retry': False,  # 재시도 시 이메일 알림 비활성화\n    'retries': 1,  # 재시도 횟수\n    'retry_delay': timedelta(minutes=5),  # 재시도 간의 지연 시간\n     'start_date': datetime(2023, 6, 10, 0, 0, 0),  # 매일 자정(00:00) UTC에 실행\n}\n\ndag = DAG(\n    'youtube_etl_dag',  # DAG 식별자\n    default_args=default_args,  # 기본 인수 할당\n    description='간단한 ETL DAG',  # DAG 설명\n    schedule_interval=timedelta(days=1),  # 일별 스케줄 간격\n    catchup=False,  # 누락된 DAG 실행을 복구하지 않음\n)\n```  \n\n<div class=\"content-ad\"></div>\n\n매일 자정(0시)에 실행되는 DAG인 'youtube_etl_dag'을 정의하고 있습니다. 이 DAG은 Airflow에서 관리 및 트리거되며, VS Code에서 별도로 실행할 필요가 없습니다. Python 파일을 업데이트하면 Airflow에서 자동으로 변경 사항을 감지하고 반영할 것입니다.\n\n현재 Airflow에는 DAG이 표시되지만 아직 정의된 작업이 없어서 어떤 작업도 표시되지 않습니다. DAG를 기능적으로 만들기 위해 데이터 추출 작업을 만들어봅시다.\n\n```js\n# YouTube API에서 데이터를 추출하기 위한 Python callable 함수\ndef extract_data(**kwargs):\n    api_key = kwargs['api_key']\n    region_codes = kwargs['region_codes']\n    category_ids = kwargs['category_ids']\n    \n    df_trending_videos = fetch_data(api_key, region_codes, category_ids)\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    # DataFrame을 CSV 파일로 저장\n    df_trending_videos.to_csv(output_path, index=False)\n\ndef fetch_data(api_key, region_codes, category_ids):\n    \"\"\"\n    YouTube API에서 여러 나라와 카테고리의 인기 동영상 데이터를 가져옵니다.\n    \"\"\"\n    # 비디오 데이터를 저장할 빈 리스트를 초기화합니다.\n    video_data = []\n\n    # YouTube API 서비스 빌드\n    youtube = build('youtube', 'v3', developerKey=api_key)\n\n    for region_code in region_codes:\n        for category_id in category_ids:\n            # 각 지역 및 카테고리에 대해 next_page_token을 None으로 초기화\n            next_page_token = None\n            while True:\n                # 인기 동영상을 가져오기 위해 YouTube API에 요청을 보냅니다.\n                request = youtube.videos().list(\n                    part='snippet,contentDetails,statistics',\n                    chart='mostPopular',\n                    regionCode=region_code,\n                    videoCategoryId=category_id,\n                    maxResults=50,\n                    pageToken=next_page_token\n                )\n                response = request.execute()\n                videos = response['items']\n\n                # 각 비디오를 처리하고 데이터를 수집합니다.\n                for video in videos:\n                    video_info = {\n                        'region_code': region_code,\n                        'category_id': category_id,\n                        'video_id': video['id'],\n                        'title': video['snippet']['title'],\n                        'published_at': video['snippet']['publishedAt'],\n                        'view_count': int(video['statistics'].get('viewCount', 0)),\n                        'like_count': int(video['statistics'].get('likeCount', 0)),\n                        'comment_count': int(video['statistics'].get('commentCount', 0)),\n                        'channel_title': video['snippet']['channelTitle']\n                    }\n                    video_data.append(video_info)\n\n                # 결과의 더 많은 페이지가 있는 경우 다음 페이지 토큰을 가져옵니다.\n                next_page_token = response.get('nextPageToken')\n                if not next_page_token:\n                    break\n\n    return pd.DataFrame(video_data)\n\n# DAG를 위한 데이터 추출 작업 정의\nextract_task = PythonOperator(\n    task_id='extract_data_from_youtube_api',\n    python_callable=extract_data,\n    op_kwargs={\n        'api_key': os.getenv('YOUTUBE_API_KEY'),\n        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],\n        'category_ids': ['1', '2', '10', '15', '20', '22', '23']\n    },\n    dag=dag,\n)\n\nextract_task # 이 작업을 실행하도록 DAG를 설정함\n```\n\n이 코드에서 두 가지 주요 작업이 이루어지고 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- DAG에 extract_task라는 작업을 만들고 있습니다.\n- extract_task에서 호출되는 callable 함수인 extract_data를 정의하고 있습니다. 이 함수는 YouTube Data API에서 데이터를 가져와 \"Youtube_Trending_Data_Raw\"로 시작하는 CSV 파일에 pandas DataFrame을 사용하여 저장합니다.\n\nYouTube Data API 문서를 참조하여 API의 다른 부분에서 사용 가능한 데이터에 대해 자세히 이해할 수 있습니다. 우리는 트렌딩 비디오 데이터에 관심이 있으므로 API의 해당 부분에 집중할 것입니다. next_page_token은 모든 페이지에서 데이터를 검색하도록 보장합니다.\n\n코드를 수정한 후 Airflow 페이지에 변경 사항이 반영되어야 합니다. DAG를 수동으로 실행하려면 왼쪽 상단에 있는 실행 버튼을 클릭하시면 됩니다. 그래프에서 작업 상태 (대기, 실행 중, 성공 등)는 다른 색상으로 나타납니다. DAG가 실행 중일 때 로그를 보실 수도 있습니다.\n\n<img src=\"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_5.png\" />\n\n<div class=\"content-ad\"></div>\n\n런 버튼을 클릭하면 데이터를 가져오고 파일에 저장하는 데 시간이 걸립니다. 작업의 각 단계에서 그래프 색상이 변경되는 것을 볼 수 있을 거에요. 멋지죠? :)\n\n작업 상태가 성공을 나타내는 녹색으로 변하면, 새 파일인 \"Youtube-Trending-Data-Raw\"가 생긴 것을 확인할 수 있어요.\n\n우리의 Raw 데이터는 이렇게 생겼어요:\n\n![이미지](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_6.png)\n\n<div class=\"content-ad\"></div>\n\n이제 추출 작업이 완료되었습니다. 다음 작업으로 넘어가 봅시다!\n\n## 3. PySpark를 사용하여 데이터 변환하기:\n\n원시 데이터 파일을 살펴보면 데이터에 많은 해시태그와 이모지가 있는데, 이는 우리 프로젝트에는 필요하지 않습니다. 데이터를 전처리하고 정리하여 추가 분석에 유용하도록 만들어 봅시다.\n\n이 작업에 PySpark를 사용할 것입니다. PySpark는 대용량 데이터 세트를 처리하고 변환 작업을 수행하기 위해 설계된 강력한 프레임워크입니다. 데이터 세트가 특히 크지 않기 때문에 Pandas를 사용할 수도 있지만, 전에 PySpark를 사용한 적이 있어 이번에도 PySpark를 사용하기로 결정했습니다. 최근 PySpark를 공부하고 있으며, 이론을 공부하는 것보다 실제 구현이 더 흥미롭다고 느낍니다.\n\n<div class=\"content-ad\"></div>\n\n\n# Python callable function to extract data from YouTube API\ndef extract_data(**kwargs):\n    api_key = kwargs['api_key']\n    region_codes = kwargs['region_codes']\n    category_ids = kwargs['category_ids']\n    \n    df_trending_videos = fetch_data(api_key, region_codes, category_ids)\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    # Save DataFrame to CSV file\n    df_trending_videos.to_csv(output_path, index=False)\n\ndef fetch_data(api_key, region_codes, category_ids):\n    \"\"\"\n    Fetches trending video data for multiple countries and categories from YouTube API.\n    Returns a pandas data frame containing video data.\n    \"\"\"\n    video_data = []\n\n    # Build YouTube API service\n    youtube = build('youtube', 'v3', developerKey=api_key)\n\n    for region_code in region_codes:\n        for category_id in category_ids:\n            # Initialize the next_page_token to None for each region and category\n            next_page_token = None\n            while True:\n                # Make a request to the YouTube API to fetch trending videos\n                request = youtube.videos().list(\n                    part='snippet,contentDetails,statistics',\n                    chart='mostPopular',\n                    regionCode=region_code,\n                    videoCategoryId=category_id,\n                    maxResults=50,\n                    pageToken=next_page_token\n                )\n                response = request.execute()\n                videos = response['items']\n\n                # Process each video and collect data\n                for video in videos:\n                    video_info = {\n                        'region_code': region_code,\n                        'category_id': category_id,\n                        'video_id': video['id'],\n                        'title': video['snippet']['title'],\n                        'published_at': video['snippet']['publishedAt'],\n                        'view_count': video['statistics'].get('viewCount', 0),\n                        'like_count': video['statistics'].get('likeCount', 0),\n                        'comment_count': video['statistics'].get('commentCount', 0),\n                        'channel_title': video['snippet']['channelTitle']\n                    }\n                    video_data.append(video_info)\n\n                # Get the next page token, if there are more pages of results\n                next_page_token = response.get('nextPageToken')\n                if not next_page_token:\n                    break\n\n    return pd.DataFrame(video_data)\n\ndef preprocess_data_pyspark_job():\n    spark = SparkSession.builder.appName('YouTubeTransform').getOrCreate()\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    df = spark.read.csv(output_path, header=True)\n    \n    # Define UDF to remove hashtag data, emojis\n    def clean_text(text):\n     if text is not None:\n        # Remove emojis\n        text = emoji.demojize(text, delimiters=('', ''))\n        \n        # Remove hashtag data\n        if text.startswith('#'):\n            text = text.replace('#', '').strip()\n        else:\n            split_text = text.split('#')\n            text = split_text[0].strip()\n        \n        # Remove extra double quotes and backslashes\n        text = text.replace('\\\\\"', '')  # Remove escaped quotes\n        text = re.sub(r'\\\"+', '', text)  # Remove remaining double quotes\n        text = text.replace('\\\\', '')  # Remove backslashes\n        \n        return text.strip()  # Strip any leading or trailing whitespace\n\n     return text\n    # Register UDF\n    clean_text_udf = udf(clean_text, StringType())\n\n    # Clean the data\n    df_cleaned = df.withColumn('title', clean_text_udf(col('title'))) \\\n                   .withColumn('channel_title', clean_text_udf(col('channel_title'))) \\\n                   .withColumn('published_at', to_date(col('published_at'))) \\\n                   .withColumn('view_count', col('view_count').cast(LongType())) \\\n                   .withColumn('like_count', col('like_count').cast(LongType())) \\\n                   .withColumn('comment_count', col('comment_count').cast(LongType())) \\\n                   .dropna(subset=['video_id'])\n    \n    # Generate the filename based on the current date\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'\n    \n    # Write cleaned DataFrame to the specified path\n    df_cleaned.write.csv(output_path, header=True, mode='overwrite')   \n\n\n# Define extract task for the DAG\nextract_task = PythonOperator(\n    task_id='extract_data_from_youtube_api',\n    python_callable=extract_data,\n    op_kwargs={\n        'api_key': os.getenv('YOUTUBE_API_KEY'),\n        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],\n        'category_ids': ['1', '2', '10', '15', '20', '22', '23']\n    },\n    dag=dag,\n)\n\n# Define preprocessing task for the DAG\npreprocess_data_pyspark_task= PythonOperator(\n    task_id='preprocess_data_pyspark_task',\n    python_callable=preprocess_data_pyspark_job,\n    dag=dag\n)\n\nextract_task >> preprocess_data_pyspark_task\n\n\n여기서는 이 코드가 하는 일을 설명해 드렸습니다.\n\n- \"preprocess_data_pyspark_task\"라는 작업을 만듭니다.\n- 이 작업은 preprocess_data_pyspark_job 함수를 호출합니다.\n- preprocess_data_pyspark_job 함수는 데이터를 정리합니다.\n- 그리고 정리된 데이터는 \"Transformed_Youtube_Data_currentDate\"라는 폴더에 저장됩니다.\n- 이 폴더 안에는 정리된 데이터가 담긴 \"part-\" 접두사가 붙은 새 CSV 파일이 생성됩니다.\n\n만약 Airflow를 보신다면 아래와 같이 첫 번째 작업에 새로운 작업이 추가된 것을 보실 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n아래는 우리가 변환한 데이터의 모습입니다:\n\n![Transformed Data](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_8.png)\n\n이 작업은 완료되었습니다. 이제 최종 작업으로 넘어갈 차례입니다.\n\n<div class=\"content-ad\"></div>\n\n# 4. S3로 데이터 로드하기:\n\n이 작업을 시작하기 전에 처음에 설정한 IAM 사용자를 사용하여 S3 버킷을 생성하고 버킷 이름을 메모해주세요.\n\n우리의 최종 코드입니다!\n\n```js\nimport logging\nimport os\nimport re\nimport shutil\nfrom datetime import datetime, timedelta\n\nimport boto3\nimport emoji\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_date, udf\nfrom pyspark.sql.types import (DateType, IntegerType, LongType, StringType,\n                               StructField, StructType)\n\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\n# DAG 및 기본 인자 정의\ndefault_args = {\n    'owner': 'airflow',  # DAG 소유자\n    'depends_on_past': False,  # 이전 DAG 실행에 의존 여부\n    'email_on_failure': False,  # 실패 시 이메일 알림 비활성화\n    'email_on_retry': False,  # 재시도 시 이메일 알림 비활성화\n    'retries': 1,  # 재시도 횟수\n    'retry_delay': timedelta(minutes=5),  # 재시도 사이 간격\n    'start_date': datetime(2023, 6, 10, 0, 0, 0),  # 매일 자정(00:00) UTC에 실행\n}\n\n# DAG 정의\ndag = DAG(\n    'youtube_etl_dag',  # DAG 식별자\n    default_args=default_args,  # 기본 인수 할당\n    description='간단한 ETL DAG',  # DAG 설명\n    schedule_interval=timedelta(days=1),  # 스케줄 간격: 매일\n    catchup=False,  # 누락된 DAG 실행을 복구하지 않음\n)\n\n# YouTube API에서 데이터를 추출하는 Python 유형의 함수\ndef extract_data(**kwargs):\n    api_key = kwargs['api_key']\n    region_codes = kwargs['region_codes']\n    category_ids = kwargs['category_ids']\n    \n    df_trending_videos = fetch_data(api_key, region_codes, category_ids)\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    # DataFrame을 CSV 파일로 저장\n    df_trending_videos.to_csv(output_path, index=False)\n\n# YouTube API에서 데이터 가져오는 함수\ndef fetch_data(api_key, region_codes, category_ids):\n    \"\"\"\n    YouTube API에서 여러 국가 및 카테고리의 트렌드 비디오 데이터를 가져옵니다.\n    비디오 데이터가 포함된 Pandas 데이터 프레임 반환.\n    \"\"\"\n    # 비디오 데이터를 보관할 빈 리스트 초기화\n    video_data = []\n\n    # YouTube API 서비스 빌드\n    youtube = build('youtube', 'v3', developerKey=api_key)\n\n    for region_code in region_codes:\n        for category_id in category_ids:\n            # 각 지역 및 카테고리마다 next_page_token을 None으로 초기화\n            next_page_token = None\n            while True:\n                # YouTube API에 트렌드 비디오를 가져오도록 요청\n                request = youtube.videos().list(\n                    part='snippet,contentDetails,statistics',\n                    chart='mostPopular',\n                    regionCode=region_code,\n                    videoCategoryId=category_id,\n                    maxResults=50,\n                    pageToken=next_page_token\n                )\n                response = request.execute()\n                videos = response['items']\n\n                # 각 비디오 처리 및 데이터 수집\n                for video in videos:\n                    video_info = {\n                        'region_code': region_code,\n                        'category_id': category_id,\n                        'video_id': video['id'],\n                        'title': video['snippet']['title'],\n                        'published_at': video['snippet']['publishedAt'],\n                        'view_count': video['statistics'].get('viewCount', 0),\n                        'like_count': video['statistics'].get('likeCount', 0),\n                        'comment_count': video['statistics'].get('commentCount', 0),\n                        'channel_title': video['snippet']['channelTitle']\n                    }\n                    video_data.append(video_info)\n\n                # 결과의 추가 페이지가 있는 경우 다음 페이지 토큰 가져오기\n                next_page_token = response.get('nextPageToken')\n                if not next_page_token:\n                    break\n\n    return pd.DataFrame(video_data)\n\n# PySpark 작업 전처리 함수\ndef preprocess_data_pyspark_job():\n    spark = SparkSession.builder.appName('YouTubeTransform').getOrCreate()\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n    df = spark.read.csv(output_path, header=True)\n    \n    # 해시태그 데이터, 이모지 제거를 위한 UDF 정의\n    def clean_text(text):\n     if text is not None:\n        # 이모지 제거\n        text = emoji.demojize(text, delimiters=('', ''))\n        \n        # 해시태그 및 이후 모든 것 제거\n        if text.startswith('#'):\n            text = text.replace('#', '').strip()\n        else:\n            split_text = text.split('#')\n            text = split_text[0].strip()\n        \n        # 추가 이중 인용부호와 백슬래시 제거\n        text = text.replace('\\\\\"', '')  # 이스케이프된 따옴표 제거\n        text = re.sub(r'\\\"+', '', text)  # 남은 이중 인용부호 제거\n        text = text.replace('\\\\', '')  # 백슬래시 제거\n        \n        return text.strip()  # 선행 또는 후행 공백 제거\n\n     return text\n    # UDF 등록\n    clean_text_udf = udf(clean_text, StringType())\n\n    # 데이터 정리\n    df_cleaned = df.withColumn('title', clean_text_udf(col('title'))) \\\n                   .withColumn('channel_title', clean_text_udf(col('channel_title'))) \\\n                   .withColumn('published_at', to_date(col('published_at'))) \\\n                   .withColumn('view_count', col('view_count').cast(LongType())) \\\n                   .withColumn('like_count', col('like_count').cast(LongType())) \\\n                   .withColumn('comment_count', col('comment_count').cast(LongType())) \\\n                   .dropna(subset=['video_id'])\n    \n    # 현재 날짜를 기반으로 파일 이름 생성\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    output_path = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'\n    \n    # 정리된 DataFrame을 지정된 경로에 작성\n    df_cleaned.write.csv(output_path, header=True, mode='overwrite')   \n\n# S3로 데이터 업로드 함수\ndef load_data_to_s3(**kwargs):\n    bucket_name = kwargs['bucket_name']\n    today = datetime.now().strftime('%Y/%m/%d')\n    prefix = f\"processed-data/{today}\"\n    current_date = datetime.now().strftime(\"%Y%m%d\")\n    local_dir_path  = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'\n    upload_to_s3(bucket_name, prefix, local_dir_path)\n\ndef upload_to_s3(bucket_name, prefix, local_dir_path):\n    aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n    aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n\n    s3_client = boto3.client(\n        's3',\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key\n    )\n\n    for root, dirs, files in os.walk(local_dir_path):\n         for file in files:\n            if file.endswith('.csv'):\n                file_path = os.path.join(root, file)\n                s3_key = f\"{prefix}/{file}\"\n                logging.info(f\"Uploading {file_path} to s3://{bucket_name}/{s3_key}\")\n                s3_client.upload_file(file_path, bucket_name, s3_key)\n\n# DAG의 추출 작업 정의\nextract_task = PythonOperator(\n    task_id='extract_data_from_youtube_api',\n    python_callable=extract_data,\n    op_kwargs={\n        'api_key': os.getenv('YOUTUBE_API_KEY'),\n        'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],\n        'category_ids': ['1', '2', '10', '15', '20', '22', '23']\n    },\n    dag=dag,\n)\n\n# DAG의 데이터 전처리 작업 정의\npreprocess_data_pyspark_task= PythonOperator(\n    task_id='preprocess_data_pyspark_task',\n    python_callable=preprocess_data_pyspark_job,\n    dag=dag\n)\n\n\n\n<div class=\"content-ad\"></div>\n\n이제 저희가 만든 최종 작업인 load_data_to_s3_task를 소개합니다. 이 작업은 load_data_to_s3 함수를 호출하여 파일을 S3 버킷에 업로드합니다. 업로드가 잘 되었는지 확인하려면 S3 버킷의 내용을 확인하세요.\n\n마침내 우리의 Airflow는 이렇게 생겼습니다!\n\n![Airflow](/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_9.png)\n\n이제 이 데이터를 Tableau나 다른 BI 도구에 연결하여 흥미로운 대시보드를 만들고 인사이트를 시각화해 보세요!\n\n<div class=\"content-ad\"></div>\n\n함께 이 파이프라인을 따라 오면서 새로운 기술 몇 가지를 배웠으면 좋겠어요! 🚀 성공적으로 여기까지 왔다면 축하해요! 🎉 이 새롭게 얻은 지식이 데이터 엔지니어링에서의 향후 모험에 큰 도움이 되길 바래요!\n\n이 프로젝트의 Github 저장소를 첨부합니다:\n\n만약 이 글을 좋아하셨다면, 공유하고, 좋아요를 눌러주시고, 아래에 댓글을 남겨주시고 구독해주세요. 🎉👏📝\n\n커튼을 닫습니다! 🎭","ogImage":{"url":"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png"},"coverImage":"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png","tag":["Tech"],"readingTime":25},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>이 기사에서는 Apache Airflow와 PySpark를 사용하여 자동 ETL (추출, 변환, 로드) 파이프라인을 만드는 방법을 안내합니다. 이 파이프라인은 YouTube Data API에서 트렌드 비디오 데이터를 가져와 처리한 후 처리된 데이터를 S3에 저장할 것입니다.</p>\n<p>Twitter API를 사용한 파이프라인을 보여주는 Darshil Parmar의 YouTube 비디오를 시청한 후, 유사한 프로젝트에 도전하기로 영감을 받았습니다. 그러나 Twitter API의 가격 정책 변경으로 인해, 시청자가 YouTube Data API를 대체로 제안했고 이것이 제 흥미를 자극했습니다.</p>\n<p>프로젝트에 돌입하기 전에 두 가지 필수 사항이 있습니다:</p>\n<ol>\n<li>Youtube Data API 키 획득</li>\n</ol>\n<ul>\n<li>Google Developers Console을 방문해 주세요.</li>\n<li>새 프로젝트를 생성해 주세요.</li>\n<li>\"YouTube Data API\"를 검색하고 활성화해 주세요.</li>\n<li>새 자격 증명을 생성하고 프로젝트에서 나중에 사용할 API 키를 복사해 주세요.</li>\n</ul>\n<p>자세한 지침은 YouTube Data API 시작 가이드를 참조해 주세요.</p>\n<ol start=\"2\">\n<li>AWS 액세스 키 ID 및 비밀 액세스 키 획득</li>\n</ol>\n<ul>\n<li>AWS Management Console에 로그인해 주세요.</li>\n<li>IAM(Identity and Access Management) 섹션으로 이동하고 새 사용자를 생성해 주세요.</li>\n<li>필요한 S3 액세스 정책을 부여하고 액세스 키를 생성해 주세요.</li>\n<li>프로젝트에서 사용할 액세스 키 ID와 비밀 액세스 키를 안전하게 저장해 주세요.</li>\n</ul>\n<p>이제 실제 프로젝트를 시작하겠습니다! 준비됐나요 여러분!!</p>\n<p><img src=\"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_0.png\" alt=\"YouTube Trend Analysis Pipeline ETL with Airflow, Spark, S3, and Docker\"></p>\n<p>이 글은 4가지 주요 단계로 구성되어 있어요:</p>\n<ul>\n<li>소프트웨어 설치 및 설정</li>\n<li>Youtube Data API에서 데이터 추출</li>\n<li>PySpark를 사용하여 데이터 변환</li>\n<li>AWS S3로 데이터 로드</li>\n</ul>\n<h1>1. 소프트웨어 설치 및 설정:</h1>\n<ul>\n<li>VS Code — <a href=\"https://code.visualstudio.com/\" rel=\"nofollow\" target=\"_blank\">VS Code 다운로드 및 설치</a>.</li>\n<li>Docker Desktop — <a href=\"https://www.docker.com/products/docker-desktop\" rel=\"nofollow\" target=\"_blank\">Docker Desktop 다운로드 및 설치</a>.</li>\n<li>(선택사항) Windows Subsystem for Linux (WSL) — 데이터 엔지니어링에 사용되는 Apache Airflow 및 PySpark와 같은 많은 도구 및 라이브러리가 Unix 계열 시스템을 위해 개발되었습니다. 이러한 도구를 Windows에서 사용할 때 발생할 수 있는 호환성 문제를 피하기 위해 WSL을 통해 네이티브 Linux 환경에서 실행할 수 있습니다.\n<ul>\n<li>` 관리자 권한으로 PowerShell을 엽니다.</li>\n<li>` 다음 명령을 실행하세요: wsl --install.</li>\n<li>` 명령에 따라 WSL을 설치하고 Microsoft Store에서 Linux 배포판(예: Ubuntu)을 선택하세요.</li>\n<li>` Linux 배포판에 사용자 이름 및 암호를 설정하세요.</li>\n</ul>\n</li>\n</ul>\n<p>이 프로젝트를 실행하는 데 WSL이 반드시 필요한 것은 아닙니다. Docker Desktop은 Windows에서 네이티브로 실행될 수 있으며 Docker 자체가 관리하는 가벼운 Linux 가상 머신(VM)을 사용합니다. 그러나 Docker Desktop과 함께 WSL을 사용하면 Windows에서 직접 Linux 명령 및 작업을 실행할 수 있어 보다 네이티브한 개발 경험을 제공합니다.</p>\n<p>이제 설정을 시작해 봅시다.</p>\n<p>부분 1 — 도커 이미지 만들기</p>\n<ul>\n<li>프로젝트용 새 폴더를 만들고 \"Airflow-Project\"로 이름을 지어주세요.</li>\n<li>해당 폴더에서 명령 프롬프트를 엽니다.</li>\n<li>명령 프롬프트에서 아래 명령을 실행하세요:</li>\n</ul>\n<pre><code class=\"hljs language-bash\">code .\n</code></pre>\n<ul>\n<li>이 명령은 VS Code에서 해당 폴더를 프로젝트로 엽니다.</li>\n<li>VS Code에서 \"dockerfile\"이라는 새 파일을 만들고 아래 코드를 붙여넣으세요:</li>\n</ul>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-variable constant_\">FROM</span> apache/<span class=\"hljs-attr\">airflow</span>:latest\n\n# 시스템 종속성을 설치하기 위해 루트 사용자로 전환합니다\n<span class=\"hljs-variable constant_\">USER</span> root\n\n# git, <span class=\"hljs-title class_\">OpenJDK</span>를 설치하고 apt 캐시를 정리합니다\n<span class=\"hljs-variable constant_\">RUN</span> apt-get update &#x26;&#x26; \\\n    apt-get -y install git <span class=\"hljs-keyword\">default</span>-jdk &#x26;&#x26; \\\n    apt-get clean &#x26;&#x26; \\\n    rm -rf /<span class=\"hljs-keyword\">var</span>/lib/apt/lists<span class=\"hljs-comment\">/*\n\n# Python 패키지를 설치하기 위해 airflow 사용자로 전환합니다\nUSER airflow\n\n# 필요한 Python 패키지를 설치합니다\nRUN pip install --no-cache-dir pyspark pandas google-api-python-client emoji boto3\n</span></code></pre>\n<p>이 Docker 파일은 프로젝트를 실행하는 데 필요한 모든 패키지를 포함하고 있어요.</p>\n<ul>\n<li>파일을 마우스 오른쪽 버튼으로 클릭하고 VS Code에서 \"이미지 빌드\" 옵션을 선택하세요. 이름을 입력하라는 프롬프트가 나타나면 \"airflow-project\"를 입력하세요. 이 명령은 Docker 이미지를 생성합니다. 그러나 이미지를 사용하려면 docker-compose.yml 파일을 생성하고 이미지를 사용하도록 구성해야 합니다.</li>\n</ul>\n<p>(재미있는 사실: 파일에서 Python 설치가 없는 이유 궁금하신가요? 실제로 Dockerfile에서 사용된 기본 이미지인 apache/airflow:latest에는 Python이 이미 설치되어 있어요. 왜냐하면 Airflow 자체가 Python으로 작성되어 있기 때문에 주로 워크플로 및 작업 정의에 Python을 사용합니다. 따라서 Dockerfile에서 별도로 Python을 설치할 필요가 없답니다!)</p>\n<p>파트 2 — 도커 컴포즈 파일 생성하기</p>\n<p>도커 컴포즈를 사용하면 멀티 컨테이너 도커 애플리케이션을 쉽게 다룰 수 있습니다. 이를 통해 단일 명령으로 여러 도커 컨테이너를 정의하고 실행할 수 있으며 각 서비스의 환경 변수, 볼륨, 포트 및 기타 설정을 명확하고 조직적인 방식으로 구성할 수 있습니다. 도커 컴포즈를 사용하면 단일 명령어인 docker-compose up 또는 docker-compose down을 사용하여 여러 서비스를 쉽게 시작, 중지 및 관리할 수 있습니다.</p>\n<ul>\n<li>\"docker-compose.yml\" 파일을 생성하고 다음 코드를 파일에 붙여넣습니다:</li>\n</ul>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-attr\">version</span>: <span class=\"hljs-string\">'3'</span>\n<span class=\"hljs-attr\">services</span>:\n\n  <span class=\"hljs-attr\">airflowproject</span>:\n    <span class=\"hljs-attr\">image</span>: airflow-<span class=\"hljs-attr\">project</span>:latest\n    <span class=\"hljs-attr\">environment</span>:\n      - <span class=\"hljs-variable constant_\">AWS_ACCESS_KEY_ID</span>=your-aws-access-key\n      - <span class=\"hljs-variable constant_\">AWS_SECRET_ACCESS_KEY</span>=your-aws-secret-access-key\n      - <span class=\"hljs-variable constant_\">YOUTUBE_API_KEY</span>=your-youtube-api-key\n    <span class=\"hljs-attr\">volumes</span>:\n      - ./<span class=\"hljs-attr\">airflow</span>:<span class=\"hljs-regexp\">/opt/</span>airflow\n    <span class=\"hljs-attr\">ports</span>:\n      - <span class=\"hljs-string\">\"8080:8080\"</span>\n    <span class=\"hljs-attr\">command</span>: airflow standalone\n</code></pre>\n<ul>\n<li>이제 파일을 마우스 오른쪽 버튼으로 클릭한 후 VS Code에서 'Compose Up' 옵션을 선택하세요. 환경을 설정하기 위해 클릭하세요.</li>\n<li>깜짝 놀랄 일이 벌어졌어요! 이 작업을 완료한 후에는 VS Code 프로젝트 디렉토리에 \"airflow\"라는 새 폴더가 나타날 수 있습니다.</li>\n</ul>\n<p>Docker 데스크톱을 열어서 모든 것이 올바르게 완료되었는지 확인하세요. 올바르게 완료된 경우 다음과 같은 화면이 표시됩니다.</p>\n<p><img src=\"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_1.png\" alt=\"이미지\"></p>\n<ul>\n<li>이제 Airflow 프로젝트를 클릭하여 Airflow가 8080 포트에서 실행 중임을 나타내는 로그가 표시되는 화면을 엽니다.</li>\n</ul>\n<p><img src=\"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_2.png\" alt=\"이미지\"></p>\n<ul>\n<li>포트를 클릭하면 Airflow 로그인 페이지로 이동합니다. 이 링크를 처음 열어보는 경우 자격 증명을 제공해야 합니다.</li>\n<li>사용자 이름은 \"admin\"이고 비밀번호는 compose up 명령을 실행한 후 생성된 Airflow 폴더 내의 \"standalone_admin_password.txt\" 파일에 있습니다.</li>\n</ul>\n<p><img src=\"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_3.png\" alt=\"이미지\"></p>\n<ul>\n<li>로그인 페이지에서 자격 증명을 입력한 후, 로컬 호스트에서 Airflow가 실행 중인 것을 확인할 수 있습니다. 다음과 같이 나타납니다:</li>\n</ul>\n<p>당신의 환경 설정 완료입니다! 휴―!!</p>\n<h1>2. YouTube 데이터 API에서 데이터 추출하기:</h1>\n<ul>\n<li>Airflow 폴더 아래에 \"dags\"라는 이름의 폴더를 만들고, dags 폴더 아래에 \"youtube_etl_dag.py\"라는 파이썬 파일을 만듭니다.</li>\n<li>이제 \"youtube_etl_dag.py\" 파일에 다음을 import하세요.</li>\n</ul>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> logging\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">import</span> shutil\n<span class=\"hljs-keyword\">from</span> datetime <span class=\"hljs-keyword\">import</span> datetime, timedelta\n\n<span class=\"hljs-keyword\">import</span> boto3\n<span class=\"hljs-keyword\">import</span> emoji\n<span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">from</span> googleapiclient.<span class=\"hljs-property\">discovery</span> <span class=\"hljs-keyword\">import</span> build\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">SparkSession</span>\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> col, to_date, udf\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">types</span> <span class=\"hljs-keyword\">import</span> (<span class=\"hljs-title class_\">DateType</span>, <span class=\"hljs-title class_\">IntegerType</span>, <span class=\"hljs-title class_\">LongType</span>, <span class=\"hljs-title class_\">StringType</span>,\n                               <span class=\"hljs-title class_\">StructField</span>, <span class=\"hljs-title class_\">StructType</span>)\n\n<span class=\"hljs-keyword\">from</span> airflow <span class=\"hljs-keyword\">import</span> <span class=\"hljs-variable constant_\">DAG</span>\n<span class=\"hljs-keyword\">from</span> airflow.<span class=\"hljs-property\">operators</span>.<span class=\"hljs-property\">python_operator</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">PythonOperator</span>\n</code></pre>\n<ul>\n<li>이 프로젝트를 실행하는 데 위의 모든 라이브러리가 필요합니다(코드 작성을 시작하면 모두 유용해집니다)</li>\n<li>VS Code에서 오류가 발생하는 것을 볼 수 있습니다. 그 이유는 모든 종속성이 도커에 설치되어 있지만 로컬 머신에는 설치되어 있지 않기 때문이므로 신경 쓰지 마십시오.</li>\n<li>Airflow에서 구문 오류가 있으면 화면 상단에 표시되고, 논리 오류/예외는 Airflow 로그에서 확인할 수 있습니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-variable constant_\">DAG</span>와 기본 인수 정의\ndefault_args = {\n    <span class=\"hljs-string\">'owner'</span>: <span class=\"hljs-string\">'airflow'</span>,  # <span class=\"hljs-variable constant_\">DAG</span> 소유자\n    <span class=\"hljs-string\">'depends_on_past'</span>: <span class=\"hljs-title class_\">False</span>,  # 과거 <span class=\"hljs-variable constant_\">DAG</span> 실행에 종속하는지 여부\n    <span class=\"hljs-string\">'email_on_failure'</span>: <span class=\"hljs-title class_\">False</span>,  # 실패 시 이메일 알림 비활성화\n    <span class=\"hljs-string\">'email_on_retry'</span>: <span class=\"hljs-title class_\">False</span>,  # 재시도 시 이메일 알림 비활성화\n    <span class=\"hljs-string\">'retries'</span>: <span class=\"hljs-number\">1</span>,  # 재시도 횟수\n    <span class=\"hljs-string\">'retry_delay'</span>: <span class=\"hljs-title function_\">timedelta</span>(minutes=<span class=\"hljs-number\">5</span>),  # 재시도 간의 지연 시간\n     <span class=\"hljs-string\">'start_date'</span>: <span class=\"hljs-title function_\">datetime</span>(<span class=\"hljs-number\">2023</span>, <span class=\"hljs-number\">6</span>, <span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>),  # 매일 자정(<span class=\"hljs-number\">00</span>:<span class=\"hljs-number\">00</span>) <span class=\"hljs-variable constant_\">UTC</span>에 실행\n}\n\ndag = <span class=\"hljs-title function_\">DAG</span>(\n    <span class=\"hljs-string\">'youtube_etl_dag'</span>,  # <span class=\"hljs-variable constant_\">DAG</span> 식별자\n    default_args=default_args,  # 기본 인수 할당\n    description=<span class=\"hljs-string\">'간단한 ETL DAG'</span>,  # <span class=\"hljs-variable constant_\">DAG</span> 설명\n    schedule_interval=<span class=\"hljs-title function_\">timedelta</span>(days=<span class=\"hljs-number\">1</span>),  # 일별 스케줄 간격\n    catchup=<span class=\"hljs-title class_\">False</span>,  # 누락된 <span class=\"hljs-variable constant_\">DAG</span> 실행을 복구하지 않음\n)\n</code></pre>\n<p>매일 자정(0시)에 실행되는 DAG인 'youtube_etl_dag'을 정의하고 있습니다. 이 DAG은 Airflow에서 관리 및 트리거되며, VS Code에서 별도로 실행할 필요가 없습니다. Python 파일을 업데이트하면 Airflow에서 자동으로 변경 사항을 감지하고 반영할 것입니다.</p>\n<p>현재 Airflow에는 DAG이 표시되지만 아직 정의된 작업이 없어서 어떤 작업도 표시되지 않습니다. DAG를 기능적으로 만들기 위해 데이터 추출 작업을 만들어봅시다.</p>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-title class_\">YouTube</span> <span class=\"hljs-variable constant_\">API</span>에서 데이터를 추출하기 위한 <span class=\"hljs-title class_\">Python</span> callable 함수\ndef <span class=\"hljs-title function_\">extract_data</span>(**kwargs):\n    api_key = kwargs[<span class=\"hljs-string\">'api_key'</span>]\n    region_codes = kwargs[<span class=\"hljs-string\">'region_codes'</span>]\n    category_ids = kwargs[<span class=\"hljs-string\">'category_ids'</span>]\n    \n    df_trending_videos = <span class=\"hljs-title function_\">fetch_data</span>(api_key, region_codes, category_ids)\n    current_date = datetime.<span class=\"hljs-title function_\">now</span>().<span class=\"hljs-title function_\">strftime</span>(<span class=\"hljs-string\">\"%Y%m%d\"</span>)\n    output_path = f<span class=\"hljs-string\">'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'</span>\n    # <span class=\"hljs-title class_\">DataFrame</span>을 <span class=\"hljs-variable constant_\">CSV</span> 파일로 저장\n    df_trending_videos.<span class=\"hljs-title function_\">to_csv</span>(output_path, index=<span class=\"hljs-title class_\">False</span>)\n\ndef <span class=\"hljs-title function_\">fetch_data</span>(api_key, region_codes, category_ids):\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    YouTube API에서 여러 나라와 카테고리의 인기 동영상 데이터를 가져옵니다.\n    \"</span><span class=\"hljs-string\">\"\"</span>\n    # 비디오 데이터를 저장할 빈 리스트를 초기화합니다.\n    video_data = []\n\n    # <span class=\"hljs-title class_\">YouTube</span> <span class=\"hljs-variable constant_\">API</span> 서비스 빌드\n    youtube = <span class=\"hljs-title function_\">build</span>(<span class=\"hljs-string\">'youtube'</span>, <span class=\"hljs-string\">'v3'</span>, developerKey=api_key)\n\n    <span class=\"hljs-keyword\">for</span> region_code <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">region_codes</span>:\n        <span class=\"hljs-keyword\">for</span> category_id <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">category_ids</span>:\n            # 각 지역 및 카테고리에 대해 next_page_token을 <span class=\"hljs-title class_\">None</span>으로 초기화\n            next_page_token = <span class=\"hljs-title class_\">None</span>\n            <span class=\"hljs-keyword\">while</span> <span class=\"hljs-title class_\">True</span>:\n                # 인기 동영상을 가져오기 위해 <span class=\"hljs-title class_\">YouTube</span> <span class=\"hljs-variable constant_\">API</span>에 요청을 보냅니다.\n                request = youtube.<span class=\"hljs-title function_\">videos</span>().<span class=\"hljs-title function_\">list</span>(\n                    part=<span class=\"hljs-string\">'snippet,contentDetails,statistics'</span>,\n                    chart=<span class=\"hljs-string\">'mostPopular'</span>,\n                    regionCode=region_code,\n                    videoCategoryId=category_id,\n                    maxResults=<span class=\"hljs-number\">50</span>,\n                    pageToken=next_page_token\n                )\n                response = request.<span class=\"hljs-title function_\">execute</span>()\n                videos = response[<span class=\"hljs-string\">'items'</span>]\n\n                # 각 비디오를 처리하고 데이터를 수집합니다.\n                <span class=\"hljs-keyword\">for</span> video <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">videos</span>:\n                    video_info = {\n                        <span class=\"hljs-string\">'region_code'</span>: region_code,\n                        <span class=\"hljs-string\">'category_id'</span>: category_id,\n                        <span class=\"hljs-string\">'video_id'</span>: video[<span class=\"hljs-string\">'id'</span>],\n                        <span class=\"hljs-string\">'title'</span>: video[<span class=\"hljs-string\">'snippet'</span>][<span class=\"hljs-string\">'title'</span>],\n                        <span class=\"hljs-string\">'published_at'</span>: video[<span class=\"hljs-string\">'snippet'</span>][<span class=\"hljs-string\">'publishedAt'</span>],\n                        <span class=\"hljs-string\">'view_count'</span>: <span class=\"hljs-title function_\">int</span>(video[<span class=\"hljs-string\">'statistics'</span>].<span class=\"hljs-title function_\">get</span>(<span class=\"hljs-string\">'viewCount'</span>, <span class=\"hljs-number\">0</span>)),\n                        <span class=\"hljs-string\">'like_count'</span>: <span class=\"hljs-title function_\">int</span>(video[<span class=\"hljs-string\">'statistics'</span>].<span class=\"hljs-title function_\">get</span>(<span class=\"hljs-string\">'likeCount'</span>, <span class=\"hljs-number\">0</span>)),\n                        <span class=\"hljs-string\">'comment_count'</span>: <span class=\"hljs-title function_\">int</span>(video[<span class=\"hljs-string\">'statistics'</span>].<span class=\"hljs-title function_\">get</span>(<span class=\"hljs-string\">'commentCount'</span>, <span class=\"hljs-number\">0</span>)),\n                        <span class=\"hljs-string\">'channel_title'</span>: video[<span class=\"hljs-string\">'snippet'</span>][<span class=\"hljs-string\">'channelTitle'</span>]\n                    }\n                    video_data.<span class=\"hljs-title function_\">append</span>(video_info)\n\n                # 결과의 더 많은 페이지가 있는 경우 다음 페이지 토큰을 가져옵니다.\n                next_page_token = response.<span class=\"hljs-title function_\">get</span>(<span class=\"hljs-string\">'nextPageToken'</span>)\n                <span class=\"hljs-keyword\">if</span> not <span class=\"hljs-attr\">next_page_token</span>:\n                    <span class=\"hljs-keyword\">break</span>\n\n    <span class=\"hljs-keyword\">return</span> pd.<span class=\"hljs-title class_\">DataFrame</span>(video_data)\n\n# <span class=\"hljs-variable constant_\">DAG</span>를 위한 데이터 추출 작업 정의\nextract_task = <span class=\"hljs-title class_\">PythonOperator</span>(\n    task_id=<span class=\"hljs-string\">'extract_data_from_youtube_api'</span>,\n    python_callable=extract_data,\n    op_kwargs={\n        <span class=\"hljs-string\">'api_key'</span>: os.<span class=\"hljs-title function_\">getenv</span>(<span class=\"hljs-string\">'YOUTUBE_API_KEY'</span>),\n        <span class=\"hljs-string\">'region_codes'</span>: [<span class=\"hljs-string\">'US'</span>, <span class=\"hljs-string\">'GB'</span>, <span class=\"hljs-string\">'IN'</span>, <span class=\"hljs-string\">'AU'</span>, <span class=\"hljs-string\">'NZ'</span>],\n        <span class=\"hljs-string\">'category_ids'</span>: [<span class=\"hljs-string\">'1'</span>, <span class=\"hljs-string\">'2'</span>, <span class=\"hljs-string\">'10'</span>, <span class=\"hljs-string\">'15'</span>, <span class=\"hljs-string\">'20'</span>, <span class=\"hljs-string\">'22'</span>, <span class=\"hljs-string\">'23'</span>]\n    },\n    dag=dag,\n)\n\nextract_task # 이 작업을 실행하도록 <span class=\"hljs-variable constant_\">DAG</span>를 설정함\n</code></pre>\n<p>이 코드에서 두 가지 주요 작업이 이루어지고 있습니다:</p>\n<ul>\n<li>DAG에 extract_task라는 작업을 만들고 있습니다.</li>\n<li>extract_task에서 호출되는 callable 함수인 extract_data를 정의하고 있습니다. 이 함수는 YouTube Data API에서 데이터를 가져와 \"Youtube_Trending_Data_Raw\"로 시작하는 CSV 파일에 pandas DataFrame을 사용하여 저장합니다.</li>\n</ul>\n<p>YouTube Data API 문서를 참조하여 API의 다른 부분에서 사용 가능한 데이터에 대해 자세히 이해할 수 있습니다. 우리는 트렌딩 비디오 데이터에 관심이 있으므로 API의 해당 부분에 집중할 것입니다. next_page_token은 모든 페이지에서 데이터를 검색하도록 보장합니다.</p>\n<p>코드를 수정한 후 Airflow 페이지에 변경 사항이 반영되어야 합니다. DAG를 수동으로 실행하려면 왼쪽 상단에 있는 실행 버튼을 클릭하시면 됩니다. 그래프에서 작업 상태 (대기, 실행 중, 성공 등)는 다른 색상으로 나타납니다. DAG가 실행 중일 때 로그를 보실 수도 있습니다.</p>\n<p>런 버튼을 클릭하면 데이터를 가져오고 파일에 저장하는 데 시간이 걸립니다. 작업의 각 단계에서 그래프 색상이 변경되는 것을 볼 수 있을 거에요. 멋지죠? :)</p>\n<p>작업 상태가 성공을 나타내는 녹색으로 변하면, 새 파일인 \"Youtube-Trending-Data-Raw\"가 생긴 것을 확인할 수 있어요.</p>\n<p>우리의 Raw 데이터는 이렇게 생겼어요:</p>\n<p><img src=\"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_6.png\" alt=\"이미지\"></p>\n<p>이제 추출 작업이 완료되었습니다. 다음 작업으로 넘어가 봅시다!</p>\n<h2>3. PySpark를 사용하여 데이터 변환하기:</h2>\n<p>원시 데이터 파일을 살펴보면 데이터에 많은 해시태그와 이모지가 있는데, 이는 우리 프로젝트에는 필요하지 않습니다. 데이터를 전처리하고 정리하여 추가 분석에 유용하도록 만들어 봅시다.</p>\n<p>이 작업에 PySpark를 사용할 것입니다. PySpark는 대용량 데이터 세트를 처리하고 변환 작업을 수행하기 위해 설계된 강력한 프레임워크입니다. 데이터 세트가 특히 크지 않기 때문에 Pandas를 사용할 수도 있지만, 전에 PySpark를 사용한 적이 있어 이번에도 PySpark를 사용하기로 결정했습니다. 최근 PySpark를 공부하고 있으며, 이론을 공부하는 것보다 실제 구현이 더 흥미롭다고 느낍니다.</p>\n<h1>Python callable function to extract data from YouTube API</h1>\n<p>def extract_data(**kwargs):\napi_key = kwargs['api_key']\nregion_codes = kwargs['region_codes']\ncategory_ids = kwargs['category_ids']</p>\n<pre><code>df_trending_videos = fetch_data(api_key, region_codes, category_ids)\ncurrent_date = datetime.now().strftime(\"%Y%m%d\")\noutput_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\n# Save DataFrame to CSV file\ndf_trending_videos.to_csv(output_path, index=False)\n</code></pre>\n<p>def fetch_data(api_key, region_codes, category_ids):\n\"\"\"\nFetches trending video data for multiple countries and categories from YouTube API.\nReturns a pandas data frame containing video data.\n\"\"\"\nvideo_data = []</p>\n<pre><code># Build YouTube API service\nyoutube = build('youtube', 'v3', developerKey=api_key)\n\nfor region_code in region_codes:\n    for category_id in category_ids:\n        # Initialize the next_page_token to None for each region and category\n        next_page_token = None\n        while True:\n            # Make a request to the YouTube API to fetch trending videos\n            request = youtube.videos().list(\n                part='snippet,contentDetails,statistics',\n                chart='mostPopular',\n                regionCode=region_code,\n                videoCategoryId=category_id,\n                maxResults=50,\n                pageToken=next_page_token\n            )\n            response = request.execute()\n            videos = response['items']\n\n            # Process each video and collect data\n            for video in videos:\n                video_info = {\n                    'region_code': region_code,\n                    'category_id': category_id,\n                    'video_id': video['id'],\n                    'title': video['snippet']['title'],\n                    'published_at': video['snippet']['publishedAt'],\n                    'view_count': video['statistics'].get('viewCount', 0),\n                    'like_count': video['statistics'].get('likeCount', 0),\n                    'comment_count': video['statistics'].get('commentCount', 0),\n                    'channel_title': video['snippet']['channelTitle']\n                }\n                video_data.append(video_info)\n\n            # Get the next page token, if there are more pages of results\n            next_page_token = response.get('nextPageToken')\n            if not next_page_token:\n                break\n\nreturn pd.DataFrame(video_data)\n</code></pre>\n<p>def preprocess_data_pyspark_job():\nspark = SparkSession.builder.appName('YouTubeTransform').getOrCreate()\ncurrent_date = datetime.now().strftime(\"%Y%m%d\")\noutput_path = f'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'\ndf = spark.read.csv(output_path, header=True)</p>\n<pre><code># Define UDF to remove hashtag data, emojis\ndef clean_text(text):\n if text is not None:\n    # Remove emojis\n    text = emoji.demojize(text, delimiters=('', ''))\n    \n    # Remove hashtag data\n    if text.startswith('#'):\n        text = text.replace('#', '').strip()\n    else:\n        split_text = text.split('#')\n        text = split_text[0].strip()\n    \n    # Remove extra double quotes and backslashes\n    text = text.replace('\\\\\"', '')  # Remove escaped quotes\n    text = re.sub(r'\\\"+', '', text)  # Remove remaining double quotes\n    text = text.replace('\\\\', '')  # Remove backslashes\n    \n    return text.strip()  # Strip any leading or trailing whitespace\n\n return text\n# Register UDF\nclean_text_udf = udf(clean_text, StringType())\n\n# Clean the data\ndf_cleaned = df.withColumn('title', clean_text_udf(col('title'))) \\\n               .withColumn('channel_title', clean_text_udf(col('channel_title'))) \\\n               .withColumn('published_at', to_date(col('published_at'))) \\\n               .withColumn('view_count', col('view_count').cast(LongType())) \\\n               .withColumn('like_count', col('like_count').cast(LongType())) \\\n               .withColumn('comment_count', col('comment_count').cast(LongType())) \\\n               .dropna(subset=['video_id'])\n\n# Generate the filename based on the current date\ncurrent_date = datetime.now().strftime(\"%Y%m%d\")\noutput_path = f'/opt/airflow/Transformed_Youtube_Data_{current_date}'\n\n# Write cleaned DataFrame to the specified path\ndf_cleaned.write.csv(output_path, header=True, mode='overwrite')   \n</code></pre>\n<h1>Define extract task for the DAG</h1>\n<p>extract_task = PythonOperator(\ntask_id='extract_data_from_youtube_api',\npython_callable=extract_data,\nop_kwargs={\n'api_key': os.getenv('YOUTUBE_API_KEY'),\n'region_codes': ['US', 'GB', 'IN', 'AU', 'NZ'],\n'category_ids': ['1', '2', '10', '15', '20', '22', '23']\n},\ndag=dag,\n)</p>\n<h1>Define preprocessing task for the DAG</h1>\n<p>preprocess_data_pyspark_task= PythonOperator(\ntask_id='preprocess_data_pyspark_task',\npython_callable=preprocess_data_pyspark_job,\ndag=dag\n)</p>\n<p>extract_task >> preprocess_data_pyspark_task</p>\n<p>여기서는 이 코드가 하는 일을 설명해 드렸습니다.</p>\n<ul>\n<li>\"preprocess_data_pyspark_task\"라는 작업을 만듭니다.</li>\n<li>이 작업은 preprocess_data_pyspark_job 함수를 호출합니다.</li>\n<li>preprocess_data_pyspark_job 함수는 데이터를 정리합니다.</li>\n<li>그리고 정리된 데이터는 \"Transformed_Youtube_Data_currentDate\"라는 폴더에 저장됩니다.</li>\n<li>이 폴더 안에는 정리된 데이터가 담긴 \"part-\" 접두사가 붙은 새 CSV 파일이 생성됩니다.</li>\n</ul>\n<p>만약 Airflow를 보신다면 아래와 같이 첫 번째 작업에 새로운 작업이 추가된 것을 보실 수 있습니다:</p>\n<p>아래는 우리가 변환한 데이터의 모습입니다:</p>\n<p><img src=\"/assets/img/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_8.png\" alt=\"Transformed Data\"></p>\n<p>이 작업은 완료되었습니다. 이제 최종 작업으로 넘어갈 차례입니다.</p>\n<h1>4. S3로 데이터 로드하기:</h1>\n<p>이 작업을 시작하기 전에 처음에 설정한 IAM 사용자를 사용하여 S3 버킷을 생성하고 버킷 이름을 메모해주세요.</p>\n<p>우리의 최종 코드입니다!</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> logging\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">import</span> shutil\n<span class=\"hljs-keyword\">from</span> datetime <span class=\"hljs-keyword\">import</span> datetime, timedelta\n\n<span class=\"hljs-keyword\">import</span> boto3\n<span class=\"hljs-keyword\">import</span> emoji\n<span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">from</span> googleapiclient.<span class=\"hljs-property\">discovery</span> <span class=\"hljs-keyword\">import</span> build\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">SparkSession</span>\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> col, to_date, udf\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">types</span> <span class=\"hljs-keyword\">import</span> (<span class=\"hljs-title class_\">DateType</span>, <span class=\"hljs-title class_\">IntegerType</span>, <span class=\"hljs-title class_\">LongType</span>, <span class=\"hljs-title class_\">StringType</span>,\n                               <span class=\"hljs-title class_\">StructField</span>, <span class=\"hljs-title class_\">StructType</span>)\n\n<span class=\"hljs-keyword\">from</span> airflow <span class=\"hljs-keyword\">import</span> <span class=\"hljs-variable constant_\">DAG</span>\n<span class=\"hljs-keyword\">from</span> airflow.<span class=\"hljs-property\">operators</span>.<span class=\"hljs-property\">python_operator</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">PythonOperator</span>\n\n# <span class=\"hljs-variable constant_\">DAG</span> 및 기본 인자 정의\ndefault_args = {\n    <span class=\"hljs-string\">'owner'</span>: <span class=\"hljs-string\">'airflow'</span>,  # <span class=\"hljs-variable constant_\">DAG</span> 소유자\n    <span class=\"hljs-string\">'depends_on_past'</span>: <span class=\"hljs-title class_\">False</span>,  # 이전 <span class=\"hljs-variable constant_\">DAG</span> 실행에 의존 여부\n    <span class=\"hljs-string\">'email_on_failure'</span>: <span class=\"hljs-title class_\">False</span>,  # 실패 시 이메일 알림 비활성화\n    <span class=\"hljs-string\">'email_on_retry'</span>: <span class=\"hljs-title class_\">False</span>,  # 재시도 시 이메일 알림 비활성화\n    <span class=\"hljs-string\">'retries'</span>: <span class=\"hljs-number\">1</span>,  # 재시도 횟수\n    <span class=\"hljs-string\">'retry_delay'</span>: <span class=\"hljs-title function_\">timedelta</span>(minutes=<span class=\"hljs-number\">5</span>),  # 재시도 사이 간격\n    <span class=\"hljs-string\">'start_date'</span>: <span class=\"hljs-title function_\">datetime</span>(<span class=\"hljs-number\">2023</span>, <span class=\"hljs-number\">6</span>, <span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>),  # 매일 자정(<span class=\"hljs-number\">00</span>:<span class=\"hljs-number\">00</span>) <span class=\"hljs-variable constant_\">UTC</span>에 실행\n}\n\n# <span class=\"hljs-variable constant_\">DAG</span> 정의\ndag = <span class=\"hljs-title function_\">DAG</span>(\n    <span class=\"hljs-string\">'youtube_etl_dag'</span>,  # <span class=\"hljs-variable constant_\">DAG</span> 식별자\n    default_args=default_args,  # 기본 인수 할당\n    description=<span class=\"hljs-string\">'간단한 ETL DAG'</span>,  # <span class=\"hljs-variable constant_\">DAG</span> 설명\n    schedule_interval=<span class=\"hljs-title function_\">timedelta</span>(days=<span class=\"hljs-number\">1</span>),  # 스케줄 간격: 매일\n    catchup=<span class=\"hljs-title class_\">False</span>,  # 누락된 <span class=\"hljs-variable constant_\">DAG</span> 실행을 복구하지 않음\n)\n\n# <span class=\"hljs-title class_\">YouTube</span> <span class=\"hljs-variable constant_\">API</span>에서 데이터를 추출하는 <span class=\"hljs-title class_\">Python</span> 유형의 함수\ndef <span class=\"hljs-title function_\">extract_data</span>(**kwargs):\n    api_key = kwargs[<span class=\"hljs-string\">'api_key'</span>]\n    region_codes = kwargs[<span class=\"hljs-string\">'region_codes'</span>]\n    category_ids = kwargs[<span class=\"hljs-string\">'category_ids'</span>]\n    \n    df_trending_videos = <span class=\"hljs-title function_\">fetch_data</span>(api_key, region_codes, category_ids)\n    current_date = datetime.<span class=\"hljs-title function_\">now</span>().<span class=\"hljs-title function_\">strftime</span>(<span class=\"hljs-string\">\"%Y%m%d\"</span>)\n    output_path = f<span class=\"hljs-string\">'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'</span>\n    # <span class=\"hljs-title class_\">DataFrame</span>을 <span class=\"hljs-variable constant_\">CSV</span> 파일로 저장\n    df_trending_videos.<span class=\"hljs-title function_\">to_csv</span>(output_path, index=<span class=\"hljs-title class_\">False</span>)\n\n# <span class=\"hljs-title class_\">YouTube</span> <span class=\"hljs-variable constant_\">API</span>에서 데이터 가져오는 함수\ndef <span class=\"hljs-title function_\">fetch_data</span>(api_key, region_codes, category_ids):\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    YouTube API에서 여러 국가 및 카테고리의 트렌드 비디오 데이터를 가져옵니다.\n    비디오 데이터가 포함된 Pandas 데이터 프레임 반환.\n    \"</span><span class=\"hljs-string\">\"\"</span>\n    # 비디오 데이터를 보관할 빈 리스트 초기화\n    video_data = []\n\n    # <span class=\"hljs-title class_\">YouTube</span> <span class=\"hljs-variable constant_\">API</span> 서비스 빌드\n    youtube = <span class=\"hljs-title function_\">build</span>(<span class=\"hljs-string\">'youtube'</span>, <span class=\"hljs-string\">'v3'</span>, developerKey=api_key)\n\n    <span class=\"hljs-keyword\">for</span> region_code <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">region_codes</span>:\n        <span class=\"hljs-keyword\">for</span> category_id <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">category_ids</span>:\n            # 각 지역 및 카테고리마다 next_page_token을 <span class=\"hljs-title class_\">None</span>으로 초기화\n            next_page_token = <span class=\"hljs-title class_\">None</span>\n            <span class=\"hljs-keyword\">while</span> <span class=\"hljs-title class_\">True</span>:\n                # <span class=\"hljs-title class_\">YouTube</span> <span class=\"hljs-variable constant_\">API</span>에 트렌드 비디오를 가져오도록 요청\n                request = youtube.<span class=\"hljs-title function_\">videos</span>().<span class=\"hljs-title function_\">list</span>(\n                    part=<span class=\"hljs-string\">'snippet,contentDetails,statistics'</span>,\n                    chart=<span class=\"hljs-string\">'mostPopular'</span>,\n                    regionCode=region_code,\n                    videoCategoryId=category_id,\n                    maxResults=<span class=\"hljs-number\">50</span>,\n                    pageToken=next_page_token\n                )\n                response = request.<span class=\"hljs-title function_\">execute</span>()\n                videos = response[<span class=\"hljs-string\">'items'</span>]\n\n                # 각 비디오 처리 및 데이터 수집\n                <span class=\"hljs-keyword\">for</span> video <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">videos</span>:\n                    video_info = {\n                        <span class=\"hljs-string\">'region_code'</span>: region_code,\n                        <span class=\"hljs-string\">'category_id'</span>: category_id,\n                        <span class=\"hljs-string\">'video_id'</span>: video[<span class=\"hljs-string\">'id'</span>],\n                        <span class=\"hljs-string\">'title'</span>: video[<span class=\"hljs-string\">'snippet'</span>][<span class=\"hljs-string\">'title'</span>],\n                        <span class=\"hljs-string\">'published_at'</span>: video[<span class=\"hljs-string\">'snippet'</span>][<span class=\"hljs-string\">'publishedAt'</span>],\n                        <span class=\"hljs-string\">'view_count'</span>: video[<span class=\"hljs-string\">'statistics'</span>].<span class=\"hljs-title function_\">get</span>(<span class=\"hljs-string\">'viewCount'</span>, <span class=\"hljs-number\">0</span>),\n                        <span class=\"hljs-string\">'like_count'</span>: video[<span class=\"hljs-string\">'statistics'</span>].<span class=\"hljs-title function_\">get</span>(<span class=\"hljs-string\">'likeCount'</span>, <span class=\"hljs-number\">0</span>),\n                        <span class=\"hljs-string\">'comment_count'</span>: video[<span class=\"hljs-string\">'statistics'</span>].<span class=\"hljs-title function_\">get</span>(<span class=\"hljs-string\">'commentCount'</span>, <span class=\"hljs-number\">0</span>),\n                        <span class=\"hljs-string\">'channel_title'</span>: video[<span class=\"hljs-string\">'snippet'</span>][<span class=\"hljs-string\">'channelTitle'</span>]\n                    }\n                    video_data.<span class=\"hljs-title function_\">append</span>(video_info)\n\n                # 결과의 추가 페이지가 있는 경우 다음 페이지 토큰 가져오기\n                next_page_token = response.<span class=\"hljs-title function_\">get</span>(<span class=\"hljs-string\">'nextPageToken'</span>)\n                <span class=\"hljs-keyword\">if</span> not <span class=\"hljs-attr\">next_page_token</span>:\n                    <span class=\"hljs-keyword\">break</span>\n\n    <span class=\"hljs-keyword\">return</span> pd.<span class=\"hljs-title class_\">DataFrame</span>(video_data)\n\n# <span class=\"hljs-title class_\">PySpark</span> 작업 전처리 함수\ndef <span class=\"hljs-title function_\">preprocess_data_pyspark_job</span>():\n    spark = <span class=\"hljs-title class_\">SparkSession</span>.<span class=\"hljs-property\">builder</span>.<span class=\"hljs-title function_\">appName</span>(<span class=\"hljs-string\">'YouTubeTransform'</span>).<span class=\"hljs-title function_\">getOrCreate</span>()\n    current_date = datetime.<span class=\"hljs-title function_\">now</span>().<span class=\"hljs-title function_\">strftime</span>(<span class=\"hljs-string\">\"%Y%m%d\"</span>)\n    output_path = f<span class=\"hljs-string\">'/opt/airflow/Youtube_Trending_Data_Raw_{current_date}'</span>\n    df = spark.<span class=\"hljs-property\">read</span>.<span class=\"hljs-title function_\">csv</span>(output_path, header=<span class=\"hljs-title class_\">True</span>)\n    \n    # 해시태그 데이터, 이모지 제거를 위한 <span class=\"hljs-variable constant_\">UDF</span> 정의\n    def <span class=\"hljs-title function_\">clean_text</span>(text):\n     <span class=\"hljs-keyword\">if</span> text is not <span class=\"hljs-title class_\">None</span>:\n        # 이모지 제거\n        text = emoji.<span class=\"hljs-title function_\">demojize</span>(text, delimiters=(<span class=\"hljs-string\">''</span>, <span class=\"hljs-string\">''</span>))\n        \n        # 해시태그 및 이후 모든 것 제거\n        <span class=\"hljs-keyword\">if</span> text.<span class=\"hljs-title function_\">startswith</span>(<span class=\"hljs-string\">'#'</span>):\n            text = text.<span class=\"hljs-title function_\">replace</span>(<span class=\"hljs-string\">'#'</span>, <span class=\"hljs-string\">''</span>).<span class=\"hljs-title function_\">strip</span>()\n        <span class=\"hljs-attr\">else</span>:\n            split_text = text.<span class=\"hljs-title function_\">split</span>(<span class=\"hljs-string\">'#'</span>)\n            text = split_text[<span class=\"hljs-number\">0</span>].<span class=\"hljs-title function_\">strip</span>()\n        \n        # 추가 이중 인용부호와 백슬래시 제거\n        text = text.<span class=\"hljs-title function_\">replace</span>(<span class=\"hljs-string\">'\\\\\"'</span>, <span class=\"hljs-string\">''</span>)  # 이스케이프된 따옴표 제거\n        text = re.<span class=\"hljs-title function_\">sub</span>(r<span class=\"hljs-string\">'\\\"+'</span>, <span class=\"hljs-string\">''</span>, text)  # 남은 이중 인용부호 제거\n        text = text.<span class=\"hljs-title function_\">replace</span>(<span class=\"hljs-string\">'\\\\'</span>, <span class=\"hljs-string\">''</span>)  # 백슬래시 제거\n        \n        <span class=\"hljs-keyword\">return</span> text.<span class=\"hljs-title function_\">strip</span>()  # 선행 또는 후행 공백 제거\n\n     <span class=\"hljs-keyword\">return</span> text\n    # <span class=\"hljs-variable constant_\">UDF</span> 등록\n    clean_text_udf = <span class=\"hljs-title function_\">udf</span>(clean_text, <span class=\"hljs-title class_\">StringType</span>())\n\n    # 데이터 정리\n    df_cleaned = df.<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">'title'</span>, <span class=\"hljs-title function_\">clean_text_udf</span>(<span class=\"hljs-title function_\">col</span>(<span class=\"hljs-string\">'title'</span>))) \\\n                   .<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">'channel_title'</span>, <span class=\"hljs-title function_\">clean_text_udf</span>(<span class=\"hljs-title function_\">col</span>(<span class=\"hljs-string\">'channel_title'</span>))) \\\n                   .<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">'published_at'</span>, <span class=\"hljs-title function_\">to_date</span>(<span class=\"hljs-title function_\">col</span>(<span class=\"hljs-string\">'published_at'</span>))) \\\n                   .<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">'view_count'</span>, <span class=\"hljs-title function_\">col</span>(<span class=\"hljs-string\">'view_count'</span>).<span class=\"hljs-title function_\">cast</span>(<span class=\"hljs-title class_\">LongType</span>())) \\\n                   .<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">'like_count'</span>, <span class=\"hljs-title function_\">col</span>(<span class=\"hljs-string\">'like_count'</span>).<span class=\"hljs-title function_\">cast</span>(<span class=\"hljs-title class_\">LongType</span>())) \\\n                   .<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">'comment_count'</span>, <span class=\"hljs-title function_\">col</span>(<span class=\"hljs-string\">'comment_count'</span>).<span class=\"hljs-title function_\">cast</span>(<span class=\"hljs-title class_\">LongType</span>())) \\\n                   .<span class=\"hljs-title function_\">dropna</span>(subset=[<span class=\"hljs-string\">'video_id'</span>])\n    \n    # 현재 날짜를 기반으로 파일 이름 생성\n    current_date = datetime.<span class=\"hljs-title function_\">now</span>().<span class=\"hljs-title function_\">strftime</span>(<span class=\"hljs-string\">\"%Y%m%d\"</span>)\n    output_path = f<span class=\"hljs-string\">'/opt/airflow/Transformed_Youtube_Data_{current_date}'</span>\n    \n    # 정리된 <span class=\"hljs-title class_\">DataFrame</span>을 지정된 경로에 작성\n    df_cleaned.<span class=\"hljs-property\">write</span>.<span class=\"hljs-title function_\">csv</span>(output_path, header=<span class=\"hljs-title class_\">True</span>, mode=<span class=\"hljs-string\">'overwrite'</span>)   \n\n# <span class=\"hljs-variable constant_\">S3</span>로 데이터 업로드 함수\ndef <span class=\"hljs-title function_\">load_data_to_s3</span>(**kwargs):\n    bucket_name = kwargs[<span class=\"hljs-string\">'bucket_name'</span>]\n    today = datetime.<span class=\"hljs-title function_\">now</span>().<span class=\"hljs-title function_\">strftime</span>(<span class=\"hljs-string\">'%Y/%m/%d'</span>)\n    prefix = f<span class=\"hljs-string\">\"processed-data/{today}\"</span>\n    current_date = datetime.<span class=\"hljs-title function_\">now</span>().<span class=\"hljs-title function_\">strftime</span>(<span class=\"hljs-string\">\"%Y%m%d\"</span>)\n    local_dir_path  = f<span class=\"hljs-string\">'/opt/airflow/Transformed_Youtube_Data_{current_date}'</span>\n    <span class=\"hljs-title function_\">upload_to_s3</span>(bucket_name, prefix, local_dir_path)\n\ndef <span class=\"hljs-title function_\">upload_to_s3</span>(bucket_name, prefix, local_dir_path):\n    aws_access_key_id = os.<span class=\"hljs-title function_\">getenv</span>(<span class=\"hljs-string\">'AWS_ACCESS_KEY_ID'</span>)\n    aws_secret_access_key = os.<span class=\"hljs-title function_\">getenv</span>(<span class=\"hljs-string\">'AWS_SECRET_ACCESS_KEY'</span>)\n\n    s3_client = boto3.<span class=\"hljs-title function_\">client</span>(\n        <span class=\"hljs-string\">'s3'</span>,\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key\n    )\n\n    <span class=\"hljs-keyword\">for</span> root, dirs, files <span class=\"hljs-keyword\">in</span> os.<span class=\"hljs-title function_\">walk</span>(local_dir_path):\n         <span class=\"hljs-keyword\">for</span> file <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">files</span>:\n            <span class=\"hljs-keyword\">if</span> file.<span class=\"hljs-title function_\">endswith</span>(<span class=\"hljs-string\">'.csv'</span>):\n                file_path = os.<span class=\"hljs-property\">path</span>.<span class=\"hljs-title function_\">join</span>(root, file)\n                s3_key = f<span class=\"hljs-string\">\"{prefix}/{file}\"</span>\n                logging.<span class=\"hljs-title function_\">info</span>(f<span class=\"hljs-string\">\"Uploading {file_path} to s3://{bucket_name}/{s3_key}\"</span>)\n                s3_client.<span class=\"hljs-title function_\">upload_file</span>(file_path, bucket_name, s3_key)\n\n# <span class=\"hljs-variable constant_\">DAG</span>의 추출 작업 정의\nextract_task = <span class=\"hljs-title class_\">PythonOperator</span>(\n    task_id=<span class=\"hljs-string\">'extract_data_from_youtube_api'</span>,\n    python_callable=extract_data,\n    op_kwargs={\n        <span class=\"hljs-string\">'api_key'</span>: os.<span class=\"hljs-title function_\">getenv</span>(<span class=\"hljs-string\">'YOUTUBE_API_KEY'</span>),\n        <span class=\"hljs-string\">'region_codes'</span>: [<span class=\"hljs-string\">'US'</span>, <span class=\"hljs-string\">'GB'</span>, <span class=\"hljs-string\">'IN'</span>, <span class=\"hljs-string\">'AU'</span>, <span class=\"hljs-string\">'NZ'</span>],\n        <span class=\"hljs-string\">'category_ids'</span>: [<span class=\"hljs-string\">'1'</span>, <span class=\"hljs-string\">'2'</span>, <span class=\"hljs-string\">'10'</span>, <span class=\"hljs-string\">'15'</span>, <span class=\"hljs-string\">'20'</span>, <span class=\"hljs-string\">'22'</span>, <span class=\"hljs-string\">'23'</span>]\n    },\n    dag=dag,\n)\n\n# <span class=\"hljs-variable constant_\">DAG</span>의 데이터 전처리 작업 정의\npreprocess_data_pyspark_task= <span class=\"hljs-title class_\">PythonOperator</span>(\n    task_id=<span class=\"hljs-string\">'preprocess_data_pyspark_task'</span>,\n    python_callable=preprocess_data_pyspark_job,\n    dag=dag\n)\n\n\n\n&#x3C;div <span class=\"hljs-keyword\">class</span>=<span class=\"hljs-string\">\"content-ad\"</span>>&#x3C;/div>\n\n이제 저희가 만든 최종 작업인 load_data_to_s3_task를 소개합니다. 이 작업은 load_data_to_s3 함수를 호출하여 파일을 <span class=\"hljs-variable constant_\">S3</span> 버킷에 업로드합니다. 업로드가 잘 되었는지 확인하려면 <span class=\"hljs-variable constant_\">S3</span> 버킷의 내용을 확인하세요.\n\n마침내 우리의 <span class=\"hljs-title class_\">Airflow</span>는 이렇게 생겼습니다!\n\n![<span class=\"hljs-title class_\">Airflow</span>](<span class=\"hljs-regexp\">/assets/img</span><span class=\"hljs-regexp\">/2024-06-19-YouTubeTrendAnalysisPipelineETLwithAirflowSparkS3andDocker_9.png)\n\n이제 이 데이터를 Tableau나 다른 BI 도구에 연결하여 흥미로운 대시보드를 만들고 인사이트를 시각화해 보세요!\n\n&#x3C;div class=\"content-ad\">&#x3C;/</span>div>\n\n함께 이 파이프라인을 따라 오면서 새로운 기술 몇 가지를 배웠으면 좋겠어요! 🚀 성공적으로 여기까지 왔다면 축하해요! 🎉 이 새롭게 얻은 지식이 데이터 엔지니어링에서의 향후 모험에 큰 도움이 되길 바래요!\n\n이 프로젝트의 <span class=\"hljs-title class_\">Github</span> 저장소를 첨부합니다:\n\n만약 이 글을 좋아하셨다면, 공유하고, 좋아요를 눌러주시고, 아래에 댓글을 남겨주시고 구독해주세요. 🎉👏📝\n\n커튼을 닫습니다! 🎭\n</code></pre>\n</body>\n</html>\n"},"__N_SSG":true}