{"pageProps":{"post":{"title":"당신의 LLM에 맞는 적절한 RAG 프레임워크 선택 LlamaIndex 또는 LangChain","description":"","date":"2024-06-19 19:45","slug":"2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain","content":"\n\n이미지(`<img>`) 태그를 Markdown 형식으로 변경하겠습니다.\n\n\n![Choosing the right RAG framework for your LLMLlamaIndex or LangChain](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_0.png)\n\n\n큰 언어 모델(Large Language Models, LLMs)은 현대의 주요 인공지능 기술 중 하나입니다. 2022년 11월에 OpenAI가 자체 생성 신생대화봇(Generative AI chatbot)을 공개하면서, 이런 첨단 기술들의 응용 가능성에 대한 사람들의 관심이 커졌습니다. ChatGPT의 놀라운 기능을 보고 나서 기업, 개발자, 개인들이 자신만의 맞춤형 ChatGPT 버전을 원했습니다. 이것이 Gen AI 모델 개발, 통합, 관리를 용이하게 하는 도구/프레임워크에 대한 수요 급증을 야기했습니다.\n\n시장에는 이런 수요를 채우는 두 주요 프레임워크가 있습니다: LlamaIndex와 LangChain. 하지만, 이 두 프레임워크의 목표는 개발자가 자신만의 맞춤형 LLM 응용프로그램을 만드는 데 도움을 주는 것입니다. 이들 프레임워크 각각은 고유의 장단점을 갖고 있습니다. 본 블로그 포스트의 목적은 LlamaIndex와 LangChain 사이의 주요 차이점을 드러내어 당신이 특정 용례에 맞는 적절한 프레임워크를 선택하는 데 도움을 주는 것입니다.\n\n# LlamaIndex 소개\n\n\n<div class=\"content-ad\"></div>\n\n\n![LlamaIndex](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_1.png)\n\nLlamaIndex는 사용자 정의 데이터를 기반으로 한 LLM(Llama Language Model)을 색인하고 쿼리하는 프레임워크입니다. 구조화된 데이터(관계형 데이터베이스), 비구조화된 데이터(NoSQL 데이터베이스) 및 반구조화된 데이터(세일즈포스 CRM 데이터)와 같은 다양한 소스를 통해 데이터 연결을 가능케 합니다.\n\n데이터가 소유권이라고 할지라도, 최신 LLM의 이해 가능한 임베딩(embedding)으로 대규모로 색인화할 수 있습니다. 따라서 모델을 다시 교육할 필요가 사라집니다.\n\n# LlamaIndex 작동 방식은?\n\n\n<div class=\"content-ad\"></div>\n\n![RAG Framework](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_2.png)\n\n라마 인덱스는 LLM의 고급 맞춤화를 용이하게 합니다. 소유권 데이터를 활용하여 모델이 문맥 기반 응답을 점차적으로 개선할 수 있도록 메모리에 내장합니다. 라마 인덱스는 대규모 언어 모델을 도메인 지식 전문가로 전환시킵니다. 이는 AI 어시스턴트로 작동하거나 소유한 진실의 소스(예: 영업 부문에 특화된 비즈니스 정보가 포함된 PDF 문서)를 기반으로 쿼리에 응답하는 대화형 챗봇 역할을 수행할 수 있습니다.\n\n소유권 데이터에 기반한 LLM 맞춤화를 위해, 라마 인덱스는 검색 증가 생성(RAG) 기술을 사용합니다. RAG는 주로 두 가지 주요 단계로 구성됩니다.\n- 인덱싱 단계: 소유권 데이터가 효과적으로 벡터 인덱스로 변환됩니다. 이때 데이터는 벡터 임베딩이나 의미적 의미가 부여된 숫자 표현으로 변환됩니다.\n- 쿼리 단계: 시스템이 쿼리를 받을 때, 가장 높은 의미 유사성을 가진 쿼리가 정보 청크 형태로 반환됩니다. 이 정보 청크는 LLM으로 보내져 최종 응답을 얻기 위해 원본 프롬프트 쿼리와 함께 전송됩니다. 이 메커니즘을 통해 RAG는 LLM의 기본 지식만으로는 불가능한 매우 정확하고 관련성 높은 출력을 생성하는 데 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 라마지수 사용 시작하기\n\n우선, 시작하기 위해 라마-인덱스를 설치해보세요:\n\n```js\npip install llama-index \n```\n\n오픈AI의 LLM을 사용하기 위해서는 오픈AI API 키가 필요합니다. 비밀 키를 받으셨다면 .env 파일에 다음과 같이 설정해야 합니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\"\n```\n\n여기서부터 LlamaIndex로 빌딩을 시작할 수 있어요! 더 많은 예제, 안내 및 사용 사례를 보려면 LlamaIndex 문서를 참조해주세요.\n\n에이전트, 지수, 쿼리 엔진 및 데이터셋과 같은 더 많은 리소스를 탐색하려면 Llama 인덱스 개발자를 위한 커뮤니티 공유 리소스/구성 요소에 액세스하기 위해 Llama 허브로 이동해주세요.\n\n# 🦙LlamaIndex로 QnA 애플리케이션 개발하기\n\n\n<div class=\"content-ad\"></div>\n\n라마인덱스의 기능을 실시간으로 보여주기 위해, 사용자 정의 문서에 기반한 쿼리에 답변을 할 수 있는 QnA 애플리케이션을 개발하는 코드 워크스루를 진행하겠습니다.\n\n먼저, 필요한 모든 종속 항목을 설치하는 과정부터 시작해보겠습니다:\n\n```js\npip install llama-index openai nltk\n```\n\n다음으로, LlamaIndex의 SimpleDirectoryReader 함수를 사용하여 문서를 로드하고 인덱스를 구축하는 과정을 시작해봅시다.\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom llama_index.core import (\n    VectorStoreIndex,\n    SimpleDirectoryReader\n )\n\n# SDR 함수 내에서 경로를 정의하고 색인을 빌드합니다\ndocuments = SimpleDirectoryReader(\"docs\").load_data()\nindex = VectorStoreIndex.from_documents(documents, show_progress=True)\n```\n\n\"docs\" 폴더 안에 Python 코스 자격 요건 PDF를 업로드했습니다:\n\n인덱스를 쿼리하고 응답을 확인합니다:\n\n```python\n# 쿼리 엔진\nquery_engine = index.as_query_engine()\n\nresponse = query_engine.query(\"과제와 프로젝트를 놓치면 성적과 백분율이 어떻게 될까요?\")\nprint(response)\n```\n\n<div class=\"content-ad\"></div>\n\n\n![Choosing the right RAG framework for your LLMLlamaIndex or LangChain](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_3.png)\n\n검색 엔진은 데이터 인덱스를 검색하여 관련 조각을 가져올 것입니다.\n\n또한 이 질의 엔진을 기억하면서 채팅 엔진으로 변환할 수도 있습니다. 함수를 수정하여 다음과 같이:\n\n```js\nchat_engine = index.as_chat_engine()\nresponse = chat_engine.chat(\"만약 제 숙제와 프로젝트를 놓치면, 어떤 등급과 퍼센티지를 받게 될까요?\")\nprint(response)\nfollow_up = chat_engine.chat(\"그리고 프로젝트만 놓친다면, 어떤 등급을 받을까요?\")\nprint(follow_up)\n```\n\n<div class=\"content-ad\"></div>\n\n매번 인덱스를 다시 빌드하지 않도록 하려면, 디스크에 저장할 수 있어요:\n\n```js\nindex.storage_context.persist()\n```\n\n그리고 나중에 다시 불러올 수 있어요:\n\n```js\nfrom llama_index.core import (\n    StorageContext,\n    load_index_from_storage,\n)\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\nindex = load_index_from_storage(storage_context)\n```\n\n<div class=\"content-ad\"></div>\n\n위에 표시된 코드는 Hugging Face Spaces에 배포된 QnA 챗봇 Gradio 애플리케이션의 일부입니다. 소스 코드와 데이터셋은 여기에서 사용할 수 있습니다.\n\n# LangChain 소개\n\n![이미지](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_4.png)\n\nLangChain은 사용자 지정 데이터 소스를 기반으로 맞춤형 LLMs(언어 생성 모델)를 구축하는 데 사용되는 또 다른 프레임워크입니다. LangChain은 관계형 데이터베이스(예: 테이블 데이터), 비관계형 데이터베이스(예: 문서), 프로그래밍 소스(예: API) 또는 심지어 사용자 정의 지식 베이스와 같은 다양한 소스로부터 데이터를 연결할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nLangChain은 단순히 다른 통합 도구들과 함께 LLMs에 전송되는 요청의 연속인 체인을 형성하는 메커니즘을 활용합니다. 각 단계의 출력물이 다음 입력으로 전달되어 체인이 형성됩니다.\n\nLangChain은 귀사의 독점 데이터와 함께 작동하며, 관련 콘텍스트가 LLMs에 제공되어 적절한 응답을 생성합니다. 회사 데이터용 맞춤형 QNA 챗봇, 내부 분석 도구, 또는 귀사 데이터 소스와 함께 작동하는 AI 보조 프로그램이든, LangChain을 통해 다양한 도구를 통합하고 여러 LLM 응용 프로그램을 체인으로 연결하여 더욱 포괄적인 시스템을 구축할 수 있습니다.\n\n# LangChain 작동 방식\n\n![이미지](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_5.png)\n\n<div class=\"content-ad\"></div>\n\nLangChain은 다음과 같은 구성 요소를 가지고 있습니다:\n\n- \"Prompts\"는 원하는 출력/응답을 얻기 위해 모델에게 제공하는 암시입니다.\n- LangChain은 사용자에게 Language 모델을 쉽게 교체할 수 있는 인터페이스를 제공합니다. LangChain은 GPT-4, Gemini 1.5 Pro, Hugging Face LLM, Claude 3와 같은 최신 LLM들과 작업할 수 있도록 해줍니다.\n- LangChain은 임베딩, 인메모리 벡터 저장소 등과 같은 색인 기술을 활용합니다.\n- LangChain은 다양한 구성 요소를 연결하는 것을 용이하게 합니다.\n- LangChain은 사용자가 작업 및 도구를 할당하는 데 도움이 되는 다양한 AI 에이전트를 제공합니다.\n\n# LangChain으로 시작하기\n\nLangChain을 사용하여 빌드를 시작하는 첫 번째 단계는 LangChain 패키지를 설치하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\npip install langchain\n```\n\nLangChain 튜토리얼을 위해 cohere API 키를 사용할 것입니다. .env 파일 내에 API 키를 넣어 cohere 환경 변수를 설정해 주세요:\n\n```js\nimport os\nos.environ[\"cohere_apikey\"] = \"여러분의_API_키_여기에_입력\"\n```\n\n이후에는 LangChain을 활용하여 개발을 시작할 수 있습니다! 더 자세한 예제, 가이드 및 사용 사례는 LangChain 문서를 참조해 주세요.\n\n<div class=\"content-ad\"></div>\n\nLangChain 생태계를 탐험하고 싶다면 Langchain Hub로 이동하여 LLM 작업 흐름에 통합할 수 있는 개발자 커뮤니티와 데이터 커넥터, 도구 및 프레임워크를 찾을 수 있어요.\n\n# 🦜🔗LangChain으로 QnA 애플리케이션 만들기\n\nLangchain의 능력을 실시간으로 시연하기 위해 사용자 정의 문서를 기반으로 질문에 답변할 수 있는 QnA 애플리케이션을 개발하는 코드 워크스루를 진행할 거에요.\n\n첫 번째 단계는 모든 종속성을 설치하는 것이에요:\n\n<div class=\"content-ad\"></div>\n\n```js\npip install langchain cohere chromadb ## openai의 LLM 대신 cohere를 사용하겠습니다.\n```\n\n그 후 문서 데이터를로드하고 색인을 생성합니다. 또한 cohere embeddings를 사용하여 임베딩을 생성할 것입니다:\n\n```js\nfrom langchain.document_loaders import OnlinePDFLoader\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import CohereEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nloader = OnlinePDFLoader(document)\ndocuments = loader.load()\n\n# 텍스트 분할기 초기화\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=chunksize, chunk_overlap=10,\n   separators=[\" \", \",\", \"\\n\"])\n\n# Cohere 임베딩 초기화\nembeddings = CohereEmbeddings(model=\"large\", cohere_api_key=st.secrets[\"cohere_apikey\"])\n\ntexts = text_splitter.split_documents(documents)\nglobal db\ndb = Chroma.from_documents(texts, embeddings)\nretriever = db.as_retriever()\nglobal qa\n```\n\n색인을 쿼리하고 응답을 확인합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nquery = \"다른 국가들과 인도 헌법의 세속주의 접근 방식을 비교해보세요?\"\nresult = db.query(query)\nprint(result)\n```\n\n<img src=\"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_6.png\" />\n\n쿼리는 의미론적으로 데이터를 검색하고 적절한 답변을 검색합니다.\n\nRetrievalQA 모듈을 사용하여 체인을 할 수 있습니다. 여기서는 cohere의 LLM을 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom langchain.llms import Cohere\nfrom langchain.chains import RetrievalQA\n\nqa = RetrievalQA.from_chain_type(\n    llm=Cohere(\n        model=\"command-xlarge-nightly\",\n        temperature=temp_r,\n        cohere_api_key=st.secrets[\"cohere_apikey\"],\n    ),\n    chain_type=\"stuff\",\n    retriever=retriever,\n    return_source_documents=True,\n    chain_type_kwargs=chain_type_kwargs,\n)\n```\n\n위에 표시된 코드는 QnA Streamlit 애플리케이션의 일부입니다. 소스 코드와 데이터 세트는 여기에서 사용할 수 있습니다.\n\n# 람마인덱스(LlamaIndex) vs 랑체인(LangChain)의 최상의 사용 사례\n\n람마인덱스(LlamaIndex):\n\n<div class=\"content-ad\"></div>\n\n- 특정 지식 베이스를 가진 쿼리 및 검색 기반 정보 검색 시스템 구축.\n- 사용자 쿼리에 대답으로 관련 정보 청크만 제공할 수 있는 QnA 챗봇 개발.\n- 대규모 문서의 요약, 텍스트 완성, 언어 번역 등\n\nLangChain:\n\n- 엔드 투 엔드 대화형 챗봇 및 AI 에이전트 구축\n- LLMs에 사용자 정의 워크플로 통합\n- API 및 기타 데이터 소스를 통해 LLMs의 데이터 연결 옵션 확장\n\nLangchain과 LlamaIndex의 결합된 사용 사례: (\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_7.png)\n\n- 전문가 AI 에이전트 구축: LangChain은 여러 데이터 원본을 통합하고 LlamaIndex는 유사 의미 검색 능력으로 빠른 응답을 생성하고 정리할 수 있습니다.\n- 고급 R&D 도구: LangChain의 체이닝을 사용하여 도구와 워크플로우를 동기화하고, LlamaIndex를 사용하여 더 맥락을 이해할 수 있는 LLM을 생성하고 가장 관련성 높은 응답을 얻을 수 있습니다.\n\n# LlamaIndex 대 LangChain: 적절한 프레임워크 선택\n\n![이미지](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_8.png)\n\n\n<div class=\"content-ad\"></div>\n\n여기 중요한 몇 가지 질문이 있습니다. 적절한 프레임워크를 선택하기 전에 다음을 고려해 보세요:\n\n- 프로젝트 요구 사항은 무엇인가요? 기본적으로 인덱스, 쿼리 검색 및 검색 애플리케이션을 위해서는 LlamaIndex를 선택할 수 있습니다. 그러나 사용자 지정 워크플로를 통합해야 하는 애플리케이션의 경우 LangChain이 더 나은 선택일 수 있습니다.\n- 사용하기 쉽고 접근하기 쉬운가요? LlamaIndex는 더 간단한 인터페이스를 제공하지만, LangChain은 NLP 개념과 구성 요소에 대한 심층적인 이해가 필요합니다.\n- 얼마나 많은 사용자 정의를 원하시나요? LangChain은 쉬운 사용자 정의와 도구 통합을 가능케 하는 모듈식 디자인을 제공합니다. 그러나 LlamaIndex는 주로 검색 및 검색 기반 프레임워크입니다.\n\n![이미지](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_9.png)\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\nLlamaIndex과 LangChain은 사용자 정의 LLM 기반 응용 프로그램을 개발하려는 개발자들에게 매우 유용한 프레임워크입니다. LlamaIndex의 USP는 우수한 검색 및 검색 기능을 제공하여 사용자에게 간소화된 인덱싱 및 쿼리 솔루션을 제공합니다. 반면 LangChain의 USP는 모듈식 설계 및 LLM 영역의 다양한 도구 및 구성 요소와의 통합 가능성에 있습니다.\n\n둘 중에 선택하는 데 고민이 되면 프로젝트 요구 사항이 무엇인지, 사용하기 쉽고 접근성이 어떤지, 얼마나 많은 사용자 정의를 원하는지와 같은 질문을 스스로에게 한 번 던져 보세요.\n\nLangChain은 보다 넓은 프레임워크 내에서 여러 도구를 사용하고 싶다면 최적입니다. 예를 들어, 다중 작업이 가능한 AI 기반 지능형 에이전트와 같은 경우입니다.\n\n그러나 만약 스마트 검색 및 검색 시스템을 구축하는 게 목표라면, LlamaIndex를 선택해보세요. LlamaIndex의 강점은 정보의 색인 및 검색에 있어서, LLM을 위한 깊은 데이터 탐색기 앱을 구축하는 것이 가능해집니다.\n\n<div class=\"content-ad\"></div>\n\n\nFinally remember that it’s not a classic case of either or, in the real world, you can implement a system whose architecture may contain both the frameworks, each playing their own unique roles.\n\n# FAQs\n\nQ1: How do LlamaIndex and LangChain differ in their primary focus?\n\nA1: LangChain’s main focus is the development & deployment of LLMs, along with the customization of LLMs using fine-tuning methods. However, LlamaIndex aims to provide an end-to-end ML workflow, along with data management & model evaluation.\n\n\n<div class=\"content-ad\"></div>\n\nQ2: 기계 학습 초보자에게 더 나은 플랫폼은 무엇인가요?\n\nA2: LlamaIndex가 구현이 간단하고 직관적이기 때문에 초보자에게 더 선호됩니다. 반면에 LangChain은 LLM 및 NLP 개념에 대해 더 심층적인 이해가 필요합니다.\n\nQ3: LlamaIndex와 LangChain을 함께 사용할 수 있나요?\n\nA3: 네, 두 플랫폼의 강점을 결합하여 사용 사례에 대한 솔루션을 개발하는 것이 가능합니다. LlamaIndex는 데이터 전처리 및 초기 모델 훈련 단계를 담당하고, LangChain은 LLM의 세밀한 조정, 도구 통합, 및 배포를 용이하게 해 줄 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nQ4: 내 맞춤형 LLM 애플리케이션에 사용할 프레임워크는 무엇이 좋을까요?\n\nA4: LangChain은 자연어 처리 작업 및 외부 데이터와의 복잡한 상호작용에 의존하는 사용 사례에 유리하며, 텍스트 요약, 감성 분석, 대화형 AI 봇 등과 같이 고급 언어 모델 기능이 필요한 애플리케이션에 적합합니다. 반면, LlamaIndex는 외부 데이터와의 일반 상호작용이 필요한 작업에 더 유리합니다(빠른 데이터 조회 및 검색과 같은 질의 응답 챗봇).\n\nQ5: LlamaIndex 또는 LangChain 사용 시 제한 사항이 있나요?\n\nA5: LlamaIndex는 고도로 전문화된 NLP 작업에는 적합하지 않습니다. 반면, LangChain은 고급 언어 모델 기능이 실제로 필요하지 않은 기계 학습 워크플로우를 해결하는 데 과도할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 자원\n\n- kyrolabs/awesome-langchain: 😎 멋진 LangChain 프레임워크 관련 도구 및 프로젝트의 멋진 목록 (github.com)\n- LLamaIndex와 함께하는 멋진 프로젝트들 (github.com)\n- [LangChain과 LLamaIndex의 4가지 작업 비교하기](https://lmy.medium.com/comparing-langchain-and-llamaindex-with-4-tasks-2970140edf33)\n- Langchain 플레이리스트\n- LLamaIndex 플레이리스트\n\n![이미지](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_10.png)\n\n이 이야기는 Generative AI에 게시되었습니다. LinkedIn에서 저희와 연락하고 최신 AI 이야기를 받아보려면 Zeniteq를 팔로우하세요.\n\n<div class=\"content-ad\"></div>\n\n최신 generative AI 뉴스 및 업데이트를 받아보려면 저희 뉴스레터를 구독해주세요. 함께 AI의 미래를 함께 만들어요!\n\n![참조 이미지](/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_11.png)","ogImage":{"url":"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_0.png"},"coverImage":"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_0.png","tag":["Tech"],"readingTime":12},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>이미지(<code>&#x3C;img></code>) 태그를 Markdown 형식으로 변경하겠습니다.</p>\n<p><img src=\"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_0.png\" alt=\"Choosing the right RAG framework for your LLMLlamaIndex or LangChain\"></p>\n<p>큰 언어 모델(Large Language Models, LLMs)은 현대의 주요 인공지능 기술 중 하나입니다. 2022년 11월에 OpenAI가 자체 생성 신생대화봇(Generative AI chatbot)을 공개하면서, 이런 첨단 기술들의 응용 가능성에 대한 사람들의 관심이 커졌습니다. ChatGPT의 놀라운 기능을 보고 나서 기업, 개발자, 개인들이 자신만의 맞춤형 ChatGPT 버전을 원했습니다. 이것이 Gen AI 모델 개발, 통합, 관리를 용이하게 하는 도구/프레임워크에 대한 수요 급증을 야기했습니다.</p>\n<p>시장에는 이런 수요를 채우는 두 주요 프레임워크가 있습니다: LlamaIndex와 LangChain. 하지만, 이 두 프레임워크의 목표는 개발자가 자신만의 맞춤형 LLM 응용프로그램을 만드는 데 도움을 주는 것입니다. 이들 프레임워크 각각은 고유의 장단점을 갖고 있습니다. 본 블로그 포스트의 목적은 LlamaIndex와 LangChain 사이의 주요 차이점을 드러내어 당신이 특정 용례에 맞는 적절한 프레임워크를 선택하는 데 도움을 주는 것입니다.</p>\n<h1>LlamaIndex 소개</h1>\n<p><img src=\"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_1.png\" alt=\"LlamaIndex\"></p>\n<p>LlamaIndex는 사용자 정의 데이터를 기반으로 한 LLM(Llama Language Model)을 색인하고 쿼리하는 프레임워크입니다. 구조화된 데이터(관계형 데이터베이스), 비구조화된 데이터(NoSQL 데이터베이스) 및 반구조화된 데이터(세일즈포스 CRM 데이터)와 같은 다양한 소스를 통해 데이터 연결을 가능케 합니다.</p>\n<p>데이터가 소유권이라고 할지라도, 최신 LLM의 이해 가능한 임베딩(embedding)으로 대규모로 색인화할 수 있습니다. 따라서 모델을 다시 교육할 필요가 사라집니다.</p>\n<h1>LlamaIndex 작동 방식은?</h1>\n<p><img src=\"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_2.png\" alt=\"RAG Framework\"></p>\n<p>라마 인덱스는 LLM의 고급 맞춤화를 용이하게 합니다. 소유권 데이터를 활용하여 모델이 문맥 기반 응답을 점차적으로 개선할 수 있도록 메모리에 내장합니다. 라마 인덱스는 대규모 언어 모델을 도메인 지식 전문가로 전환시킵니다. 이는 AI 어시스턴트로 작동하거나 소유한 진실의 소스(예: 영업 부문에 특화된 비즈니스 정보가 포함된 PDF 문서)를 기반으로 쿼리에 응답하는 대화형 챗봇 역할을 수행할 수 있습니다.</p>\n<p>소유권 데이터에 기반한 LLM 맞춤화를 위해, 라마 인덱스는 검색 증가 생성(RAG) 기술을 사용합니다. RAG는 주로 두 가지 주요 단계로 구성됩니다.</p>\n<ul>\n<li>인덱싱 단계: 소유권 데이터가 효과적으로 벡터 인덱스로 변환됩니다. 이때 데이터는 벡터 임베딩이나 의미적 의미가 부여된 숫자 표현으로 변환됩니다.</li>\n<li>쿼리 단계: 시스템이 쿼리를 받을 때, 가장 높은 의미 유사성을 가진 쿼리가 정보 청크 형태로 반환됩니다. 이 정보 청크는 LLM으로 보내져 최종 응답을 얻기 위해 원본 프롬프트 쿼리와 함께 전송됩니다. 이 메커니즘을 통해 RAG는 LLM의 기본 지식만으로는 불가능한 매우 정확하고 관련성 높은 출력을 생성하는 데 사용할 수 있습니다.</li>\n</ul>\n<h1>라마지수 사용 시작하기</h1>\n<p>우선, 시작하기 위해 라마-인덱스를 설치해보세요:</p>\n<pre><code class=\"hljs language-js\">pip install llama-index \n</code></pre>\n<p>오픈AI의 LLM을 사용하기 위해서는 오픈AI API 키가 필요합니다. 비밀 키를 받으셨다면 .env 파일에 다음과 같이 설정해야 합니다:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> os\nos.<span class=\"hljs-property\">environ</span>[<span class=\"hljs-string\">\"OPENAI_API_KEY\"</span>] = <span class=\"hljs-string\">\"your_api_key_here\"</span>\n</code></pre>\n<p>여기서부터 LlamaIndex로 빌딩을 시작할 수 있어요! 더 많은 예제, 안내 및 사용 사례를 보려면 LlamaIndex 문서를 참조해주세요.</p>\n<p>에이전트, 지수, 쿼리 엔진 및 데이터셋과 같은 더 많은 리소스를 탐색하려면 Llama 인덱스 개발자를 위한 커뮤니티 공유 리소스/구성 요소에 액세스하기 위해 Llama 허브로 이동해주세요.</p>\n<h1>🦙LlamaIndex로 QnA 애플리케이션 개발하기</h1>\n<p>라마인덱스의 기능을 실시간으로 보여주기 위해, 사용자 정의 문서에 기반한 쿼리에 답변을 할 수 있는 QnA 애플리케이션을 개발하는 코드 워크스루를 진행하겠습니다.</p>\n<p>먼저, 필요한 모든 종속 항목을 설치하는 과정부터 시작해보겠습니다:</p>\n<pre><code class=\"hljs language-js\">pip install llama-index openai nltk\n</code></pre>\n<p>다음으로, LlamaIndex의 SimpleDirectoryReader 함수를 사용하여 문서를 로드하고 인덱스를 구축하는 과정을 시작해봅시다.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> llama_index.core <span class=\"hljs-keyword\">import</span> (\n    VectorStoreIndex,\n    SimpleDirectoryReader\n )\n\n<span class=\"hljs-comment\"># SDR 함수 내에서 경로를 정의하고 색인을 빌드합니다</span>\ndocuments = SimpleDirectoryReader(<span class=\"hljs-string\">\"docs\"</span>).load_data()\nindex = VectorStoreIndex.from_documents(documents, show_progress=<span class=\"hljs-literal\">True</span>)\n</code></pre>\n<p>\"docs\" 폴더 안에 Python 코스 자격 요건 PDF를 업로드했습니다:</p>\n<p>인덱스를 쿼리하고 응답을 확인합니다:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># 쿼리 엔진</span>\nquery_engine = index.as_query_engine()\n\nresponse = query_engine.query(<span class=\"hljs-string\">\"과제와 프로젝트를 놓치면 성적과 백분율이 어떻게 될까요?\"</span>)\n<span class=\"hljs-built_in\">print</span>(response)\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_3.png\" alt=\"Choosing the right RAG framework for your LLMLlamaIndex or LangChain\"></p>\n<p>검색 엔진은 데이터 인덱스를 검색하여 관련 조각을 가져올 것입니다.</p>\n<p>또한 이 질의 엔진을 기억하면서 채팅 엔진으로 변환할 수도 있습니다. 함수를 수정하여 다음과 같이:</p>\n<pre><code class=\"hljs language-js\">chat_engine = index.<span class=\"hljs-title function_\">as_chat_engine</span>()\nresponse = chat_engine.<span class=\"hljs-title function_\">chat</span>(<span class=\"hljs-string\">\"만약 제 숙제와 프로젝트를 놓치면, 어떤 등급과 퍼센티지를 받게 될까요?\"</span>)\n<span class=\"hljs-title function_\">print</span>(response)\nfollow_up = chat_engine.<span class=\"hljs-title function_\">chat</span>(<span class=\"hljs-string\">\"그리고 프로젝트만 놓친다면, 어떤 등급을 받을까요?\"</span>)\n<span class=\"hljs-title function_\">print</span>(follow_up)\n</code></pre>\n<p>매번 인덱스를 다시 빌드하지 않도록 하려면, 디스크에 저장할 수 있어요:</p>\n<pre><code class=\"hljs language-js\">index.<span class=\"hljs-property\">storage_context</span>.<span class=\"hljs-title function_\">persist</span>()\n</code></pre>\n<p>그리고 나중에 다시 불러올 수 있어요:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> llama_index.<span class=\"hljs-property\">core</span> <span class=\"hljs-keyword\">import</span> (\n    <span class=\"hljs-title class_\">StorageContext</span>,\n    load_index_from_storage,\n)\n\nstorage_context = <span class=\"hljs-title class_\">StorageContext</span>.<span class=\"hljs-title function_\">from_defaults</span>(persist_dir=<span class=\"hljs-string\">\"./storage\"</span>)\nindex = <span class=\"hljs-title function_\">load_index_from_storage</span>(storage_context)\n</code></pre>\n<p>위에 표시된 코드는 Hugging Face Spaces에 배포된 QnA 챗봇 Gradio 애플리케이션의 일부입니다. 소스 코드와 데이터셋은 여기에서 사용할 수 있습니다.</p>\n<h1>LangChain 소개</h1>\n<p><img src=\"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_4.png\" alt=\"이미지\"></p>\n<p>LangChain은 사용자 지정 데이터 소스를 기반으로 맞춤형 LLMs(언어 생성 모델)를 구축하는 데 사용되는 또 다른 프레임워크입니다. LangChain은 관계형 데이터베이스(예: 테이블 데이터), 비관계형 데이터베이스(예: 문서), 프로그래밍 소스(예: API) 또는 심지어 사용자 정의 지식 베이스와 같은 다양한 소스로부터 데이터를 연결할 수 있습니다.</p>\n<p>LangChain은 단순히 다른 통합 도구들과 함께 LLMs에 전송되는 요청의 연속인 체인을 형성하는 메커니즘을 활용합니다. 각 단계의 출력물이 다음 입력으로 전달되어 체인이 형성됩니다.</p>\n<p>LangChain은 귀사의 독점 데이터와 함께 작동하며, 관련 콘텍스트가 LLMs에 제공되어 적절한 응답을 생성합니다. 회사 데이터용 맞춤형 QNA 챗봇, 내부 분석 도구, 또는 귀사 데이터 소스와 함께 작동하는 AI 보조 프로그램이든, LangChain을 통해 다양한 도구를 통합하고 여러 LLM 응용 프로그램을 체인으로 연결하여 더욱 포괄적인 시스템을 구축할 수 있습니다.</p>\n<h1>LangChain 작동 방식</h1>\n<p><img src=\"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_5.png\" alt=\"이미지\"></p>\n<p>LangChain은 다음과 같은 구성 요소를 가지고 있습니다:</p>\n<ul>\n<li>\"Prompts\"는 원하는 출력/응답을 얻기 위해 모델에게 제공하는 암시입니다.</li>\n<li>LangChain은 사용자에게 Language 모델을 쉽게 교체할 수 있는 인터페이스를 제공합니다. LangChain은 GPT-4, Gemini 1.5 Pro, Hugging Face LLM, Claude 3와 같은 최신 LLM들과 작업할 수 있도록 해줍니다.</li>\n<li>LangChain은 임베딩, 인메모리 벡터 저장소 등과 같은 색인 기술을 활용합니다.</li>\n<li>LangChain은 다양한 구성 요소를 연결하는 것을 용이하게 합니다.</li>\n<li>LangChain은 사용자가 작업 및 도구를 할당하는 데 도움이 되는 다양한 AI 에이전트를 제공합니다.</li>\n</ul>\n<h1>LangChain으로 시작하기</h1>\n<p>LangChain을 사용하여 빌드를 시작하는 첫 번째 단계는 LangChain 패키지를 설치하는 것입니다.</p>\n<pre><code class=\"hljs language-js\">pip install langchain\n</code></pre>\n<p>LangChain 튜토리얼을 위해 cohere API 키를 사용할 것입니다. .env 파일 내에 API 키를 넣어 cohere 환경 변수를 설정해 주세요:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> os\nos.<span class=\"hljs-property\">environ</span>[<span class=\"hljs-string\">\"cohere_apikey\"</span>] = <span class=\"hljs-string\">\"여러분의_API_키_여기에_입력\"</span>\n</code></pre>\n<p>이후에는 LangChain을 활용하여 개발을 시작할 수 있습니다! 더 자세한 예제, 가이드 및 사용 사례는 LangChain 문서를 참조해 주세요.</p>\n<p>LangChain 생태계를 탐험하고 싶다면 Langchain Hub로 이동하여 LLM 작업 흐름에 통합할 수 있는 개발자 커뮤니티와 데이터 커넥터, 도구 및 프레임워크를 찾을 수 있어요.</p>\n<h1>🦜🔗LangChain으로 QnA 애플리케이션 만들기</h1>\n<p>Langchain의 능력을 실시간으로 시연하기 위해 사용자 정의 문서를 기반으로 질문에 답변할 수 있는 QnA 애플리케이션을 개발하는 코드 워크스루를 진행할 거에요.</p>\n<p>첫 번째 단계는 모든 종속성을 설치하는 것이에요:</p>\n<pre><code class=\"hljs language-js\">pip install langchain cohere chromadb ## openai의 <span class=\"hljs-variable constant_\">LLM</span> 대신 cohere를 사용하겠습니다.\n</code></pre>\n<p>그 후 문서 데이터를로드하고 색인을 생성합니다. 또한 cohere embeddings를 사용하여 임베딩을 생성할 것입니다:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">document_loaders</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">OnlinePDFLoader</span>\n<span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">vectorstores</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Chroma</span>\n<span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">embeddings</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">CohereEmbeddings</span>\n<span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">text_splitter</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">RecursiveCharacterTextSplitter</span>\n\nloader = <span class=\"hljs-title class_\">OnlinePDFLoader</span>(<span class=\"hljs-variable language_\">document</span>)\ndocuments = loader.<span class=\"hljs-title function_\">load</span>()\n\n# 텍스트 분할기 초기화\ntext_splitter = <span class=\"hljs-title class_\">RecursiveCharacterTextSplitter</span>(chunk_size=chunksize, chunk_overlap=<span class=\"hljs-number\">10</span>,\n   separators=[<span class=\"hljs-string\">\" \"</span>, <span class=\"hljs-string\">\",\"</span>, <span class=\"hljs-string\">\"\\n\"</span>])\n\n# <span class=\"hljs-title class_\">Cohere</span> 임베딩 초기화\nembeddings = <span class=\"hljs-title class_\">CohereEmbeddings</span>(model=<span class=\"hljs-string\">\"large\"</span>, cohere_api_key=st.<span class=\"hljs-property\">secrets</span>[<span class=\"hljs-string\">\"cohere_apikey\"</span>])\n\ntexts = text_splitter.<span class=\"hljs-title function_\">split_documents</span>(documents)\n<span class=\"hljs-variable language_\">global</span> db\ndb = <span class=\"hljs-title class_\">Chroma</span>.<span class=\"hljs-title function_\">from_documents</span>(texts, embeddings)\nretriever = db.<span class=\"hljs-title function_\">as_retriever</span>()\n<span class=\"hljs-variable language_\">global</span> qa\n</code></pre>\n<p>색인을 쿼리하고 응답을 확인합니다.</p>\n<pre><code class=\"hljs language-js\">query = <span class=\"hljs-string\">\"다른 국가들과 인도 헌법의 세속주의 접근 방식을 비교해보세요?\"</span>\nresult = db.<span class=\"hljs-title function_\">query</span>(query)\n<span class=\"hljs-title function_\">print</span>(result)\n</code></pre>\n<p>쿼리는 의미론적으로 데이터를 검색하고 적절한 답변을 검색합니다.</p>\n<p>RetrievalQA 모듈을 사용하여 체인을 할 수 있습니다. 여기서는 cohere의 LLM을 사용할 것입니다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">llms</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Cohere</span>\n<span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">chains</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">RetrievalQA</span>\n\nqa = <span class=\"hljs-title class_\">RetrievalQA</span>.<span class=\"hljs-title function_\">from_chain_type</span>(\n    llm=<span class=\"hljs-title class_\">Cohere</span>(\n        model=<span class=\"hljs-string\">\"command-xlarge-nightly\"</span>,\n        temperature=temp_r,\n        cohere_api_key=st.<span class=\"hljs-property\">secrets</span>[<span class=\"hljs-string\">\"cohere_apikey\"</span>],\n    ),\n    chain_type=<span class=\"hljs-string\">\"stuff\"</span>,\n    retriever=retriever,\n    return_source_documents=<span class=\"hljs-title class_\">True</span>,\n    chain_type_kwargs=chain_type_kwargs,\n)\n</code></pre>\n<p>위에 표시된 코드는 QnA Streamlit 애플리케이션의 일부입니다. 소스 코드와 데이터 세트는 여기에서 사용할 수 있습니다.</p>\n<h1>람마인덱스(LlamaIndex) vs 랑체인(LangChain)의 최상의 사용 사례</h1>\n<p>람마인덱스(LlamaIndex):</p>\n<ul>\n<li>특정 지식 베이스를 가진 쿼리 및 검색 기반 정보 검색 시스템 구축.</li>\n<li>사용자 쿼리에 대답으로 관련 정보 청크만 제공할 수 있는 QnA 챗봇 개발.</li>\n<li>대규모 문서의 요약, 텍스트 완성, 언어 번역 등</li>\n</ul>\n<p>LangChain:</p>\n<ul>\n<li>엔드 투 엔드 대화형 챗봇 및 AI 에이전트 구축</li>\n<li>LLMs에 사용자 정의 워크플로 통합</li>\n<li>API 및 기타 데이터 소스를 통해 LLMs의 데이터 연결 옵션 확장</li>\n</ul>\n<p>Langchain과 LlamaIndex의 결합된 사용 사례: (</p>\n<p><img src=\"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_7.png\" alt=\"이미지\"></p>\n<ul>\n<li>전문가 AI 에이전트 구축: LangChain은 여러 데이터 원본을 통합하고 LlamaIndex는 유사 의미 검색 능력으로 빠른 응답을 생성하고 정리할 수 있습니다.</li>\n<li>고급 R&#x26;D 도구: LangChain의 체이닝을 사용하여 도구와 워크플로우를 동기화하고, LlamaIndex를 사용하여 더 맥락을 이해할 수 있는 LLM을 생성하고 가장 관련성 높은 응답을 얻을 수 있습니다.</li>\n</ul>\n<h1>LlamaIndex 대 LangChain: 적절한 프레임워크 선택</h1>\n<p><img src=\"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_8.png\" alt=\"이미지\"></p>\n<p>여기 중요한 몇 가지 질문이 있습니다. 적절한 프레임워크를 선택하기 전에 다음을 고려해 보세요:</p>\n<ul>\n<li>프로젝트 요구 사항은 무엇인가요? 기본적으로 인덱스, 쿼리 검색 및 검색 애플리케이션을 위해서는 LlamaIndex를 선택할 수 있습니다. 그러나 사용자 지정 워크플로를 통합해야 하는 애플리케이션의 경우 LangChain이 더 나은 선택일 수 있습니다.</li>\n<li>사용하기 쉽고 접근하기 쉬운가요? LlamaIndex는 더 간단한 인터페이스를 제공하지만, LangChain은 NLP 개념과 구성 요소에 대한 심층적인 이해가 필요합니다.</li>\n<li>얼마나 많은 사용자 정의를 원하시나요? LangChain은 쉬운 사용자 정의와 도구 통합을 가능케 하는 모듈식 디자인을 제공합니다. 그러나 LlamaIndex는 주로 검색 및 검색 기반 프레임워크입니다.</li>\n</ul>\n<p><img src=\"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_9.png\" alt=\"이미지\"></p>\n<h1>결론</h1>\n<p>LlamaIndex과 LangChain은 사용자 정의 LLM 기반 응용 프로그램을 개발하려는 개발자들에게 매우 유용한 프레임워크입니다. LlamaIndex의 USP는 우수한 검색 및 검색 기능을 제공하여 사용자에게 간소화된 인덱싱 및 쿼리 솔루션을 제공합니다. 반면 LangChain의 USP는 모듈식 설계 및 LLM 영역의 다양한 도구 및 구성 요소와의 통합 가능성에 있습니다.</p>\n<p>둘 중에 선택하는 데 고민이 되면 프로젝트 요구 사항이 무엇인지, 사용하기 쉽고 접근성이 어떤지, 얼마나 많은 사용자 정의를 원하는지와 같은 질문을 스스로에게 한 번 던져 보세요.</p>\n<p>LangChain은 보다 넓은 프레임워크 내에서 여러 도구를 사용하고 싶다면 최적입니다. 예를 들어, 다중 작업이 가능한 AI 기반 지능형 에이전트와 같은 경우입니다.</p>\n<p>그러나 만약 스마트 검색 및 검색 시스템을 구축하는 게 목표라면, LlamaIndex를 선택해보세요. LlamaIndex의 강점은 정보의 색인 및 검색에 있어서, LLM을 위한 깊은 데이터 탐색기 앱을 구축하는 것이 가능해집니다.</p>\n<p>Finally remember that it’s not a classic case of either or, in the real world, you can implement a system whose architecture may contain both the frameworks, each playing their own unique roles.</p>\n<h1>FAQs</h1>\n<p>Q1: How do LlamaIndex and LangChain differ in their primary focus?</p>\n<p>A1: LangChain’s main focus is the development &#x26; deployment of LLMs, along with the customization of LLMs using fine-tuning methods. However, LlamaIndex aims to provide an end-to-end ML workflow, along with data management &#x26; model evaluation.</p>\n<p>Q2: 기계 학습 초보자에게 더 나은 플랫폼은 무엇인가요?</p>\n<p>A2: LlamaIndex가 구현이 간단하고 직관적이기 때문에 초보자에게 더 선호됩니다. 반면에 LangChain은 LLM 및 NLP 개념에 대해 더 심층적인 이해가 필요합니다.</p>\n<p>Q3: LlamaIndex와 LangChain을 함께 사용할 수 있나요?</p>\n<p>A3: 네, 두 플랫폼의 강점을 결합하여 사용 사례에 대한 솔루션을 개발하는 것이 가능합니다. LlamaIndex는 데이터 전처리 및 초기 모델 훈련 단계를 담당하고, LangChain은 LLM의 세밀한 조정, 도구 통합, 및 배포를 용이하게 해 줄 수 있습니다.</p>\n<p>Q4: 내 맞춤형 LLM 애플리케이션에 사용할 프레임워크는 무엇이 좋을까요?</p>\n<p>A4: LangChain은 자연어 처리 작업 및 외부 데이터와의 복잡한 상호작용에 의존하는 사용 사례에 유리하며, 텍스트 요약, 감성 분석, 대화형 AI 봇 등과 같이 고급 언어 모델 기능이 필요한 애플리케이션에 적합합니다. 반면, LlamaIndex는 외부 데이터와의 일반 상호작용이 필요한 작업에 더 유리합니다(빠른 데이터 조회 및 검색과 같은 질의 응답 챗봇).</p>\n<p>Q5: LlamaIndex 또는 LangChain 사용 시 제한 사항이 있나요?</p>\n<p>A5: LlamaIndex는 고도로 전문화된 NLP 작업에는 적합하지 않습니다. 반면, LangChain은 고급 언어 모델 기능이 실제로 필요하지 않은 기계 학습 워크플로우를 해결하는 데 과도할 수 있습니다.</p>\n<h1>자원</h1>\n<ul>\n<li>kyrolabs/awesome-langchain: 😎 멋진 LangChain 프레임워크 관련 도구 및 프로젝트의 멋진 목록 (github.com)</li>\n<li>LLamaIndex와 함께하는 멋진 프로젝트들 (github.com)</li>\n<li><a href=\"https://lmy.medium.com/comparing-langchain-and-llamaindex-with-4-tasks-2970140edf33\" rel=\"nofollow\" target=\"_blank\">LangChain과 LLamaIndex의 4가지 작업 비교하기</a></li>\n<li>Langchain 플레이리스트</li>\n<li>LLamaIndex 플레이리스트</li>\n</ul>\n<p><img src=\"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_10.png\" alt=\"이미지\"></p>\n<p>이 이야기는 Generative AI에 게시되었습니다. LinkedIn에서 저희와 연락하고 최신 AI 이야기를 받아보려면 Zeniteq를 팔로우하세요.</p>\n<p>최신 generative AI 뉴스 및 업데이트를 받아보려면 저희 뉴스레터를 구독해주세요. 함께 AI의 미래를 함께 만들어요!</p>\n<p><img src=\"/assets/img/2024-06-19-ChoosingtherightRAGframeworkforyourLLMLlamaIndexorLangChain_11.png\" alt=\"참조 이미지\"></p>\n</body>\n</html>\n"},"__N_SSG":true}