{"pageProps":{"post":{"title":"초보자를 위한 엔드 투 엔드 Airflow 프로젝트 베를린 날씨 데이터 스크래핑 및 Amazon S3에 업로드하기","description":"","date":"2024-06-20 15:21","slug":"2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3","content":"\n\n파이썬 개발과 Apache Airflow에 열정을 가진 데이터 엔지니어로서, 베를린의 최신 날씨 데이터를 가져 와 CSV 파일로 저장하고 Amazon S3로 업로드하는 프로젝트를 시작했습니다. 이 튜토리얼에서는 Python, 웹 스크래핑을 위한 BeautifulSoup, 데이터 조작을 위한 Pandas, 그리고 오케스트레이션을 위한 Airflow를 사용한 전체 설정 및 구현 방법을 안내해 드릴 겁니다.\n\n# 프로젝트 개요\n\n이 프로젝트에서는 다음을 목표로 합니다:\n\n- 날씨 데이터 스크래핑: 날씨 웹사이트에서 베를린의 실시간 날씨 정보를 가져 오는 웹 스크래핑 기술을 활용합니다.\n- 데이터 로컬 저장: 가져온 데이터를 로컬 파일 시스템의 CSV 파일에 저장합니다.\n- Amazon S3로 업로드: 날씨 데이터가 포함된 CSV 파일을 Amazon S3 버킷에 업로드하는 메커니즘을 구현합니다.\n- Airflow로 자동화: Apache Airflow를 사용하여 매 시간마다 데이터 가져오기와 업로드 프로세스를 자동화하고 예약합니다.\n\n<div class=\"content-ad\"></div>\n\n# 사용된 도구 및 기술\n\n- Python: 스크립팅 및 데이터 조작에 사용됩니다.\n- BeautifulSoup: HTML 및 XML 문서 구문 분석을 위한 Python 라이브러리로, 여기서 웹 스크래핑에 사용됩니다.\n- Pandas: 파이썬에서 데이터를 분석하고 조작하는 강력한 도구로, 표 형식의 데이터를 처리하고 다루는 데 활용됩니다.\n- Apache Airflow: 워크플로우를 프로그래밍적으로 작성, 예약 및 모니터링하는 오픈 소스 도구입니다.\n- Amazon S3: 스케일링 가능한 객체 저장 서비스인 Amazon Simple Storage Service로, 데이터를 저장하고 검색하는 데 사용됩니다.\n\n# 구현 단계별 안내\n\n# 1. 환경 설정하기\n\n<div class=\"content-ad\"></div>\n\nPython이 설치되어 있고 필요한 라이브러리(requests, beautifulsoup4, pandas, AWS SDK의 boto3)가 함께 설치되었는지 확인하기 위해 다음 명령을 실행해주세요:\n\n```js\npip install requests beautifulsoup4 pandas boto3\n```\n\n# 2. 날씨 데이터 수집 및 로컬에 데이터 저장\n\n추출한 날씨 데이터를 Pandas를 사용하여 로컬 CSV 파일에 저장하세요:\n\n<div class=\"content-ad\"></div>\n\n'Alex The Analyst' YouTube 채널에서 BeautifulSoup를 사용하여 스크랩을 배웠어요. 완성된 재생 목록을 확인해보세요.\n\n```js\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nimport pytz\n```\n\n```js\n# 날씨 데이터를 업데이트하고 CSV로 저장하는 함수\ndef update_weather(**kwargs):\n    url = 'https://weather.com/weather/today/l/52.52,13.40'\n    page = requests.get(url)\n    if page.status_code != 200:\n        raise Exception(f\"페이지를 가져오지 못했습니다: {page.status_code}\")\n    soup = BeautifulSoup(page.text, 'html.parser')\n    # 체감 온도에 대한 제목 찾기\n    title_element = soup.find(class_='TodayDetailsCard--feelsLikeTempLabel--1UNV1')\n    if title_element:\n        title = title_element.text.strip()\n    else:\n        title = \"체감 온도\"\n    # DataFrame을 저장할 파일 경로\n    file_path = '/opt/airflow/dags/weather_checkin.csv'\n    # 파일이 있는지 확인\n    if os.path.exists(file_path):\n        logging.info(f\"파일 {file_path}이 존재합니다. 기존 DataFrame을 불러옵니다.\")\n        # 기존 DataFrame 불러오기\n        current_weather_berlin_df = pd.read_csv(file_path)\n    else:\n        logging.info(f\"파일 {file_path}이 존재하지 않습니다. 새 DataFrame을 생성합니다.\")\n        # 제목과 날짜 및 시간 열을 가진 새 DataFrame 초기화\n        current_weather_berlin_df = pd.DataFrame(columns=[title, 'date_time'])\n    # 체감 온도 값 찾기\n    value_element = soup.find('span', class_='TodayDetailsCard--feelsLikeTempValue--2icPt')\n    if value_element:\n        feels_like_temp = value_element.text.strip()\n    else:\n        feels_like_temp = None\n    # 베를린 시간대의 현재 날짜 및 시간 가져오기\n    berlin_tz = pytz.timezone('Europe/Berlin')\n    current_datetime = datetime.now(berlin_tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n    # DataFrame에 데이터 추가\n    if feels_like_temp:\n        logging.info(f\"새 데이터 추가 중: {feels_like_temp}, {current_datetime}\")\n        new_data = {title: [feels_like_temp], 'date_time': [current_datetime]}\n        current_weather_berlin_df = current_weather_berlin_df.append(pd.DataFrame(new_data), ignore_index=True)\n    else:\n        logging.error(\"체감 온도 값 찾기를 실패했습니다\")\n    # DataFrame을 CSV 파일로 저장\n    logging.info(f\"{file_path}에 DataFrame을 저장 중\")\n    current_weather_berlin_df.to_csv(file_path, index=False)\n    logging.info(current_weather_berlin_df)\n    # S3에 CSV 업로드\n    bucket_name = 'myfirstbucketsoumya'  # S3 버킷 이름으로 대체\n    s3_key = 'current_weather_berlin.csv'  # 원하는 S3 키로 대체\n    upload_to_s3(file_path, bucket_name, s3_key)\n```\n\n# 3. Amazon S3로 업로드하기\n\n<div class=\"content-ad\"></div>\n\n아래는 boto3를 사용하여 CSV 파일을 Amazon S3에 업로드하는 기능을 구현한 것입니다:\n\n```python\nimport boto3\n```\n\n```python\n# AWS 자격 증명\nAWS_ACCESS_KEY_ID = 'your-access-key-id' # 자격 증명을 하드코딩합니다.\nAWS_SECRET_ACCESS_KEY = 'your-secret-access-key'\nAWS_REGION = 'eu-central-1'\n# 현재 날씨 CSV를 S3에 업로드하는 함수\ndef upload_to_s3(file_path, bucket_name, s3_key):\n    try:\n        # 자격 증명을 사용하여 Amazon S3와의 세션을 초기화합니다.\n        s3 = boto3.client(\n            's3',\n            aws_access_key_id='your-access-key-id', # 값을 하드코딩합니다.\n            aws_secret_access_key='your-secret-access-key', # 값을 하드코딩합니다.\n            region_name='eu-central-1' # 값을 하드코딩합니다.\n        )\n        bucket_name = 'myfirstbucketsoumya'\n        file_key = 'hourly_berlin_weather.txt'\n        # CSV 파일을 S3에 업로드합니다.\n        s3.upload_file(file_path, bucket_name, s3_key)\n        logging.info(f\"날씨 데이터를 S3에 업로드했습니다: s3://{bucket_name}/{s3_key}\")\n    except Exception as e:\n        logging.error(f\"S3로 업로드 실패: {e}\")\n```\n\ndocker-compose.yaml에 몇 가지 변경 사항이 있습니다. AWS 자격 증명과 requirements.txt 컨테이너를 업데이트하십시오.\n\n<div class=\"content-ad\"></div>\n\n```js\n#변경 1\nx-airflow-common:\n  &airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.9.1}\n  environment:\n    &airflow-common-env\n    PYTHONPATH: /opt/airflow/dags/airflow_env_bs/lib/python3.12/site-packages\n \n    #AWS_ACCESS_KEY_ID: your-access-key-id #값을 하드코딩\n    #AWS_SECRET_ACCESS_KEY: your-secret-access-key #값을 하드코딩\n    #AWS_REGION: eu-central-1 #값을 하드코딩\n\n#변경-2\nairflow-init:\n    <<: *airflow-common\n    entrypoint: /bin/bash\n    command: >\n      -c \"pip install -r /requirements.txt && airflow webserver\" \n```\n\n# 5. Apache Airflow로 자동화하기\n\n마지막으로 Apache Airflow를 사용하여 전체 프로세스를 조율하세요. 다음은 DAG를 정의하는 방법입니다:\n\n```js\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n# DAG 정의\nDAG_NAME = 'berlin-weather'\ndefault_args = \n{'owner': 'airflow',\n'depends_on_past': False,\n'start_date': datetime(2023, 6, 19),\n'retries': 1,\n'retry_delay': timedelta(minutes=5),\n}\ndag = DAG(\ndag_id=DAG_NAME,\ndescription='베를린 날씨 매 시간 갱신',\nschedule_interval='@hourly',\ndefault_args=default_args,\ncatchup=False,\n)\n# 작업 정의\nupdate_weather_task = PythonOperator(\ntask_id='update_weather',\npython_callable=update_weather,\ndag=dag,\n)\n# 작업 의존성\nupdate_weather_task\n```\n\n# 5. CSV 파일을 위한 S3 버킷 확인\n\nAirflow 웹 서버를 열고 DAG를 실행하세요.\n\n<img src=\"/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_0.png\" />\n\n<div class=\"content-ad\"></div>\n\nDAG를 트리거한 후에 \"current_weather_berlin.csv\"라는 S3 Bucket을 확인해보세요. 거기에는 데이터 폴더가 있을 겁니다.\n\n![이미지](/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_1.png)\n\n# 결론\n\n이 프로젝트에서는 Python을 사용하여 베를린 날씨 데이터를 가져오고 로컬에 저장하며 Amazon S3로 업로드하는 프로세스를 자동화하는 방법을 탐색했습니다. 이를 위해 웹 스크래핑용 BeautifulSoup, 데이터 처리용 Pandas, 그리고 워크플로우 자동화용 Apache Airflow를 사용했습니다. 이러한 단계를 따라가면 이 프로젝트를 적응하고 확장하여 보다 복잡한 데이터 파이프라인 및 통합을 처리할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n제 Github 저장소를 확인하러 가보세요: Scraping-Berlin-Weather-Data-and-Uploading-to-Amazon-S3\n\n태그: airflow, 데이터 엔지니어링 프로젝트, AWS S3, 도커, 초보자 프로젝트","ogImage":{"url":"/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_0.png"},"coverImage":"/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_0.png","tag":["Tech"],"readingTime":7},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>파이썬 개발과 Apache Airflow에 열정을 가진 데이터 엔지니어로서, 베를린의 최신 날씨 데이터를 가져 와 CSV 파일로 저장하고 Amazon S3로 업로드하는 프로젝트를 시작했습니다. 이 튜토리얼에서는 Python, 웹 스크래핑을 위한 BeautifulSoup, 데이터 조작을 위한 Pandas, 그리고 오케스트레이션을 위한 Airflow를 사용한 전체 설정 및 구현 방법을 안내해 드릴 겁니다.</p>\n<h1>프로젝트 개요</h1>\n<p>이 프로젝트에서는 다음을 목표로 합니다:</p>\n<ul>\n<li>날씨 데이터 스크래핑: 날씨 웹사이트에서 베를린의 실시간 날씨 정보를 가져 오는 웹 스크래핑 기술을 활용합니다.</li>\n<li>데이터 로컬 저장: 가져온 데이터를 로컬 파일 시스템의 CSV 파일에 저장합니다.</li>\n<li>Amazon S3로 업로드: 날씨 데이터가 포함된 CSV 파일을 Amazon S3 버킷에 업로드하는 메커니즘을 구현합니다.</li>\n<li>Airflow로 자동화: Apache Airflow를 사용하여 매 시간마다 데이터 가져오기와 업로드 프로세스를 자동화하고 예약합니다.</li>\n</ul>\n<h1>사용된 도구 및 기술</h1>\n<ul>\n<li>Python: 스크립팅 및 데이터 조작에 사용됩니다.</li>\n<li>BeautifulSoup: HTML 및 XML 문서 구문 분석을 위한 Python 라이브러리로, 여기서 웹 스크래핑에 사용됩니다.</li>\n<li>Pandas: 파이썬에서 데이터를 분석하고 조작하는 강력한 도구로, 표 형식의 데이터를 처리하고 다루는 데 활용됩니다.</li>\n<li>Apache Airflow: 워크플로우를 프로그래밍적으로 작성, 예약 및 모니터링하는 오픈 소스 도구입니다.</li>\n<li>Amazon S3: 스케일링 가능한 객체 저장 서비스인 Amazon Simple Storage Service로, 데이터를 저장하고 검색하는 데 사용됩니다.</li>\n</ul>\n<h1>구현 단계별 안내</h1>\n<h1>1. 환경 설정하기</h1>\n<p>Python이 설치되어 있고 필요한 라이브러리(requests, beautifulsoup4, pandas, AWS SDK의 boto3)가 함께 설치되었는지 확인하기 위해 다음 명령을 실행해주세요:</p>\n<pre><code class=\"hljs language-js\">pip install requests beautifulsoup4 pandas boto3\n</code></pre>\n<h1>2. 날씨 데이터 수집 및 로컬에 데이터 저장</h1>\n<p>추출한 날씨 데이터를 Pandas를 사용하여 로컬 CSV 파일에 저장하세요:</p>\n<p>'Alex The Analyst' YouTube 채널에서 BeautifulSoup를 사용하여 스크랩을 배웠어요. 완성된 재생 목록을 확인해보세요.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> bs4 <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">BeautifulSoup</span>\n<span class=\"hljs-keyword\">import</span> requests\n<span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">import</span> pytz\n</code></pre>\n<pre><code class=\"hljs language-js\"># 날씨 데이터를 업데이트하고 <span class=\"hljs-variable constant_\">CSV</span>로 저장하는 함수\ndef <span class=\"hljs-title function_\">update_weather</span>(**kwargs):\n    url = <span class=\"hljs-string\">'https://weather.com/weather/today/l/52.52,13.40'</span>\n    page = requests.<span class=\"hljs-title function_\">get</span>(url)\n    <span class=\"hljs-keyword\">if</span> page.<span class=\"hljs-property\">status_code</span> != <span class=\"hljs-number\">200</span>:\n        raise <span class=\"hljs-title class_\">Exception</span>(f<span class=\"hljs-string\">\"페이지를 가져오지 못했습니다: {page.status_code}\"</span>)\n    soup = <span class=\"hljs-title class_\">BeautifulSoup</span>(page.<span class=\"hljs-property\">text</span>, <span class=\"hljs-string\">'html.parser'</span>)\n    # 체감 온도에 대한 제목 찾기\n    title_element = soup.<span class=\"hljs-title function_\">find</span>(class_=<span class=\"hljs-string\">'TodayDetailsCard--feelsLikeTempLabel--1UNV1'</span>)\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-attr\">title_element</span>:\n        title = title_element.<span class=\"hljs-property\">text</span>.<span class=\"hljs-title function_\">strip</span>()\n    <span class=\"hljs-attr\">else</span>:\n        title = <span class=\"hljs-string\">\"체감 온도\"</span>\n    # <span class=\"hljs-title class_\">DataFrame</span>을 저장할 파일 경로\n    file_path = <span class=\"hljs-string\">'/opt/airflow/dags/weather_checkin.csv'</span>\n    # 파일이 있는지 확인\n    <span class=\"hljs-keyword\">if</span> os.<span class=\"hljs-property\">path</span>.<span class=\"hljs-title function_\">exists</span>(file_path):\n        logging.<span class=\"hljs-title function_\">info</span>(f<span class=\"hljs-string\">\"파일 {file_path}이 존재합니다. 기존 DataFrame을 불러옵니다.\"</span>)\n        # 기존 <span class=\"hljs-title class_\">DataFrame</span> 불러오기\n        current_weather_berlin_df = pd.<span class=\"hljs-title function_\">read_csv</span>(file_path)\n    <span class=\"hljs-attr\">else</span>:\n        logging.<span class=\"hljs-title function_\">info</span>(f<span class=\"hljs-string\">\"파일 {file_path}이 존재하지 않습니다. 새 DataFrame을 생성합니다.\"</span>)\n        # 제목과 날짜 및 시간 열을 가진 새 <span class=\"hljs-title class_\">DataFrame</span> 초기화\n        current_weather_berlin_df = pd.<span class=\"hljs-title class_\">DataFrame</span>(columns=[title, <span class=\"hljs-string\">'date_time'</span>])\n    # 체감 온도 값 찾기\n    value_element = soup.<span class=\"hljs-title function_\">find</span>(<span class=\"hljs-string\">'span'</span>, class_=<span class=\"hljs-string\">'TodayDetailsCard--feelsLikeTempValue--2icPt'</span>)\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-attr\">value_element</span>:\n        feels_like_temp = value_element.<span class=\"hljs-property\">text</span>.<span class=\"hljs-title function_\">strip</span>()\n    <span class=\"hljs-attr\">else</span>:\n        feels_like_temp = <span class=\"hljs-title class_\">None</span>\n    # 베를린 시간대의 현재 날짜 및 시간 가져오기\n    berlin_tz = pytz.<span class=\"hljs-title function_\">timezone</span>(<span class=\"hljs-string\">'Europe/Berlin'</span>)\n    current_datetime = datetime.<span class=\"hljs-title function_\">now</span>(berlin_tz).<span class=\"hljs-title function_\">strftime</span>(<span class=\"hljs-string\">\"%Y-%m-%d %H:%M:%S\"</span>)\n    # <span class=\"hljs-title class_\">DataFrame</span>에 데이터 추가\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-attr\">feels_like_temp</span>:\n        logging.<span class=\"hljs-title function_\">info</span>(f<span class=\"hljs-string\">\"새 데이터 추가 중: {feels_like_temp}, {current_datetime}\"</span>)\n        new_data = {<span class=\"hljs-attr\">title</span>: [feels_like_temp], <span class=\"hljs-string\">'date_time'</span>: [current_datetime]}\n        current_weather_berlin_df = current_weather_berlin_df.<span class=\"hljs-title function_\">append</span>(pd.<span class=\"hljs-title class_\">DataFrame</span>(new_data), ignore_index=<span class=\"hljs-title class_\">True</span>)\n    <span class=\"hljs-attr\">else</span>:\n        logging.<span class=\"hljs-title function_\">error</span>(<span class=\"hljs-string\">\"체감 온도 값 찾기를 실패했습니다\"</span>)\n    # <span class=\"hljs-title class_\">DataFrame</span>을 <span class=\"hljs-variable constant_\">CSV</span> 파일로 저장\n    logging.<span class=\"hljs-title function_\">info</span>(f<span class=\"hljs-string\">\"{file_path}에 DataFrame을 저장 중\"</span>)\n    current_weather_berlin_df.<span class=\"hljs-title function_\">to_csv</span>(file_path, index=<span class=\"hljs-title class_\">False</span>)\n    logging.<span class=\"hljs-title function_\">info</span>(current_weather_berlin_df)\n    # <span class=\"hljs-variable constant_\">S3</span>에 <span class=\"hljs-variable constant_\">CSV</span> 업로드\n    bucket_name = <span class=\"hljs-string\">'myfirstbucketsoumya'</span>  # <span class=\"hljs-variable constant_\">S3</span> 버킷 이름으로 대체\n    s3_key = <span class=\"hljs-string\">'current_weather_berlin.csv'</span>  # 원하는 <span class=\"hljs-variable constant_\">S3</span> 키로 대체\n    <span class=\"hljs-title function_\">upload_to_s3</span>(file_path, bucket_name, s3_key)\n</code></pre>\n<h1>3. Amazon S3로 업로드하기</h1>\n<p>아래는 boto3를 사용하여 CSV 파일을 Amazon S3에 업로드하는 기능을 구현한 것입니다:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> boto3\n</code></pre>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># AWS 자격 증명</span>\nAWS_ACCESS_KEY_ID = <span class=\"hljs-string\">'your-access-key-id'</span> <span class=\"hljs-comment\"># 자격 증명을 하드코딩합니다.</span>\nAWS_SECRET_ACCESS_KEY = <span class=\"hljs-string\">'your-secret-access-key'</span>\nAWS_REGION = <span class=\"hljs-string\">'eu-central-1'</span>\n<span class=\"hljs-comment\"># 현재 날씨 CSV를 S3에 업로드하는 함수</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">upload_to_s3</span>(<span class=\"hljs-params\">file_path, bucket_name, s3_key</span>):\n    <span class=\"hljs-keyword\">try</span>:\n        <span class=\"hljs-comment\"># 자격 증명을 사용하여 Amazon S3와의 세션을 초기화합니다.</span>\n        s3 = boto3.client(\n            <span class=\"hljs-string\">'s3'</span>,\n            aws_access_key_id=<span class=\"hljs-string\">'your-access-key-id'</span>, <span class=\"hljs-comment\"># 값을 하드코딩합니다.</span>\n            aws_secret_access_key=<span class=\"hljs-string\">'your-secret-access-key'</span>, <span class=\"hljs-comment\"># 값을 하드코딩합니다.</span>\n            region_name=<span class=\"hljs-string\">'eu-central-1'</span> <span class=\"hljs-comment\"># 값을 하드코딩합니다.</span>\n        )\n        bucket_name = <span class=\"hljs-string\">'myfirstbucketsoumya'</span>\n        file_key = <span class=\"hljs-string\">'hourly_berlin_weather.txt'</span>\n        <span class=\"hljs-comment\"># CSV 파일을 S3에 업로드합니다.</span>\n        s3.upload_file(file_path, bucket_name, s3_key)\n        logging.info(<span class=\"hljs-string\">f\"날씨 데이터를 S3에 업로드했습니다: s3://<span class=\"hljs-subst\">{bucket_name}</span>/<span class=\"hljs-subst\">{s3_key}</span>\"</span>)\n    <span class=\"hljs-keyword\">except</span> Exception <span class=\"hljs-keyword\">as</span> e:\n        logging.error(<span class=\"hljs-string\">f\"S3로 업로드 실패: <span class=\"hljs-subst\">{e}</span>\"</span>)\n</code></pre>\n<p>docker-compose.yaml에 몇 가지 변경 사항이 있습니다. AWS 자격 증명과 requirements.txt 컨테이너를 업데이트하십시오.</p>\n<pre><code class=\"hljs language-js\">#변경 <span class=\"hljs-number\">1</span>\nx-airflow-<span class=\"hljs-attr\">common</span>:\n  &#x26;airflow-common\n  <span class=\"hljs-attr\">image</span>: ${<span class=\"hljs-attr\">AIRFLOW_IMAGE_NAME</span>:-apache/<span class=\"hljs-attr\">airflow</span>:<span class=\"hljs-number\">2.9</span><span class=\"hljs-number\">.1</span>}\n  <span class=\"hljs-attr\">environment</span>:\n    &#x26;airflow-common-env\n    <span class=\"hljs-attr\">PYTHONPATH</span>: <span class=\"hljs-regexp\">/opt/</span>airflow/dags/airflow_env_bs/lib/python3<span class=\"hljs-number\">.12</span>/site-packages\n \n    #<span class=\"hljs-attr\">AWS_ACCESS_KEY_ID</span>: your-access-key-id #값을 하드코딩\n    #<span class=\"hljs-attr\">AWS_SECRET_ACCESS_KEY</span>: your-secret-access-key #값을 하드코딩\n    #<span class=\"hljs-attr\">AWS_REGION</span>: eu-central-<span class=\"hljs-number\">1</span> #값을 하드코딩\n\n#변경-<span class=\"hljs-number\">2</span>\nairflow-<span class=\"hljs-attr\">init</span>:\n    &#x3C;&#x3C;: *airflow-common\n    <span class=\"hljs-attr\">entrypoint</span>: <span class=\"hljs-regexp\">/bin/</span>bash\n    <span class=\"hljs-attr\">command</span>: >\n      -c <span class=\"hljs-string\">\"pip install -r /requirements.txt &#x26;&#x26; airflow webserver\"</span> \n</code></pre>\n<h1>5. Apache Airflow로 자동화하기</h1>\n<p>마지막으로 Apache Airflow를 사용하여 전체 프로세스를 조율하세요. 다음은 DAG를 정의하는 방법입니다:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> datetime <span class=\"hljs-keyword\">import</span> datetime\n<span class=\"hljs-keyword\">from</span> airflow <span class=\"hljs-keyword\">import</span> <span class=\"hljs-variable constant_\">DAG</span>\n<span class=\"hljs-keyword\">from</span> airflow.<span class=\"hljs-property\">operators</span>.<span class=\"hljs-property\">python_operator</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">PythonOperator</span>\n</code></pre>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-variable constant_\">DAG</span> 정의\n<span class=\"hljs-variable constant_\">DAG_NAME</span> = <span class=\"hljs-string\">'berlin-weather'</span>\ndefault_args = \n{<span class=\"hljs-string\">'owner'</span>: <span class=\"hljs-string\">'airflow'</span>,\n<span class=\"hljs-string\">'depends_on_past'</span>: <span class=\"hljs-title class_\">False</span>,\n<span class=\"hljs-string\">'start_date'</span>: <span class=\"hljs-title function_\">datetime</span>(<span class=\"hljs-number\">2023</span>, <span class=\"hljs-number\">6</span>, <span class=\"hljs-number\">19</span>),\n<span class=\"hljs-string\">'retries'</span>: <span class=\"hljs-number\">1</span>,\n<span class=\"hljs-string\">'retry_delay'</span>: <span class=\"hljs-title function_\">timedelta</span>(minutes=<span class=\"hljs-number\">5</span>),\n}\ndag = <span class=\"hljs-title function_\">DAG</span>(\ndag_id=<span class=\"hljs-variable constant_\">DAG_NAME</span>,\ndescription=<span class=\"hljs-string\">'베를린 날씨 매 시간 갱신'</span>,\nschedule_interval=<span class=\"hljs-string\">'@hourly'</span>,\ndefault_args=default_args,\ncatchup=<span class=\"hljs-title class_\">False</span>,\n)\n# 작업 정의\nupdate_weather_task = <span class=\"hljs-title class_\">PythonOperator</span>(\ntask_id=<span class=\"hljs-string\">'update_weather'</span>,\npython_callable=update_weather,\ndag=dag,\n)\n# 작업 의존성\nupdate_weather_task\n</code></pre>\n<h1>5. CSV 파일을 위한 S3 버킷 확인</h1>\n<p>Airflow 웹 서버를 열고 DAG를 실행하세요.</p>\n<p>DAG를 트리거한 후에 \"current_weather_berlin.csv\"라는 S3 Bucket을 확인해보세요. 거기에는 데이터 폴더가 있을 겁니다.</p>\n<p><img src=\"/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_1.png\" alt=\"이미지\"></p>\n<h1>결론</h1>\n<p>이 프로젝트에서는 Python을 사용하여 베를린 날씨 데이터를 가져오고 로컬에 저장하며 Amazon S3로 업로드하는 프로세스를 자동화하는 방법을 탐색했습니다. 이를 위해 웹 스크래핑용 BeautifulSoup, 데이터 처리용 Pandas, 그리고 워크플로우 자동화용 Apache Airflow를 사용했습니다. 이러한 단계를 따라가면 이 프로젝트를 적응하고 확장하여 보다 복잡한 데이터 파이프라인 및 통합을 처리할 수 있습니다.</p>\n<p>제 Github 저장소를 확인하러 가보세요: Scraping-Berlin-Weather-Data-and-Uploading-to-Amazon-S3</p>\n<p>태그: airflow, 데이터 엔지니어링 프로젝트, AWS S3, 도커, 초보자 프로젝트</p>\n</body>\n</html>\n"},"__N_SSG":true}