{"pageProps":{"post":{"title":"Llama-3 추론을 Intel Core Ultra 5에서 실행하기 DirectML 및 ONNX 대 IPEX-LLM","description":"","date":"2024-06-19 01:21","slug":"2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM","content":"\n\n이전 글에서 언급했듯이 Intel은 ONNX + DirectML을 위한 하드웨어 가속화를 제공합니다. 이에 대해 몇 가지 실험을 진행했습니다.\n\nMicrosoft은 PyTorch를 위한 DirectML 인터페이스를 제공합니다. 현재 16비트와 32비트 부동 소수점에서만 추론을 지원합니다. 예제에서 초당 토큰 수를 측정하기 위해 포크를 생성했습니다.\n\n```js\nconda create --name pytdml python=3.10 -y\nconda activate pytdml\npip install torch-directml\ngit clone https://github.com/luweigen/DirectML\ncd DirectML/PyTorch/llm\npip install -r requirements.txt\npython app.py --precision float16 --model_repo \"meta-llama/Meta-Llama-3-8B-Instruct\" --stream_every_n=143\n```\n\n<img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png\" />\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_1.png\" />\n\n이전 실험에서 🤗Transformers + IPEX-LLM이 최상의 성능을 보여줬기 때문에 이 설정에서는 16비트 부동 소수점 추론만 비교할 것입니다.\n\n테스트 프로그램 ipex-llm-llama3.py는 다음과 같이 수정되었습니다.\n\n```js\n# Wei Lu(mailwlu@gmail.com)에 의해 수정됨\n# 2016년 The BigDL Authors에 저작권 속함\n#\n# Apache 라이선스, 버전 2.0에 따라 라이선스 부여\n# 이 파일을 라이선스와 준수하면서 사용해야 합니다.\n# 라이선스 사본은 다음에서 얻을 수 있습니다.\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# 적용되는 법률에 따라 필요하거나 서면으로 합의되거나, 소프트웨어가\n# \"있는 그대로\" 배포됩니다. 조건이나 보증 없이\n# 명시 또는 묵시적으로, 까지도 어떤 종류의 조건도 보증 없이,\n# 명시적 또는 묵시적으로. 언어 특정 권한과 관련해야 합니다.\n# 권한 및 제한 사항\n#\n\nimport torch\nimport time\nimport argparse\n\nfrom ipex_llm.transformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer\n\n# 모델을 기반으로 프롬프트를 조정할 수 있습니다.\n# 여기서 프롬프트 조정은 다음을 참조합니다. https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3\n기본_시스템_프롬프트 = \"\"\"\\\n\"\"\"\n\ndef get_prompt(user_input: str, chat_history: list[tuple[str, str]],\n               system_prompt: str) -> str:\n    prompt_texts = [f'<|begin_of_text|>']\n\n    if system_prompt != '':\n        prompt_texts.append(f'<|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|>')\n\n    for history_input, history_response in chat_history:\n        prompt_texts.append(f'<|start_header_id|>user<|end_header_id|>\\n\\n{history_input.strip()}<|eot_id|>')\n        prompt_texts.append(f'<|start_header_id|>assistant<|end_header_id|>\\n\\n{history_response.strip()}<|eot_id|>')\n\n    prompt_texts.append(f'<|start_header_id|>user<|end_header_id|>\\n\\n{user_input.strip()}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n')\n    return ''.join(prompt_texts)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Llama3 모델을 사용하여 `generate()` API를 사용하여 토큰 예측')\n    parser.add_argument('--repo-id-or-model-path', type=str, default=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                        help='Meta-Llama-3 (예: `meta-llama/Meta-Llama-3-70B-Instruct`)를 다운로드할 Huggingface 저장소 ID'\n                             '또는 Huggingface 체크포인트 폴더에 대한 경로')\n    parser.add_argument('--prompt', type=str, default=\"OpenVINO is\",\n                        help='추론할 프롬프트')\n    parser.add_argument('--n-predict', type=int, default=128,\n                        help='예측할 최대 토큰 수')\n    parser.add_argument('--bit', type=str, default=\"4\",\n                        help='4로 설정하면 4비트로 로드하거나 off 또는 load_in_low_bit 옵션은 sym_int4, asym_int4, sym_int5, asym_int5, sym_int8,nf3,nf4, fp4, fp8, fp8_e4m3, fp8_e5m2, fp6, gguf_iq2_xxs, gguf_iq2_xs, gguf_iq1_s, gguf_q4k_m, gguf_q4k_s, fp16, bf16, fp6_k')\n\n    args = parser.parse_args()\n    model_path = args.repo_id_or_model_path\n\n    if args.bit == \"4\":\n        # 4비트로 모델 로드,\n        # 모델의 관련 레이어를 INT4 형식으로 변환합니다.\n        # iGPU를 사용하는 Windows 사용자의 경우, LLM을 실행할 때 `cpu_embedding=True`를 from_pretrained 함수에서 설정하는 것을 권장합니다.\n        # 이렇게 하면 메모리 집약적인 임베딩 레이어가 iGPU 대신 CPU를 활용할 수 있습니다. 이 경우에는 도움이 되지 않습니다.\n        model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                    load_in_4bit=True,\n                                                    optimize_model=True,\n                                                    trust_remote_code=True,\n                                                    use_cache=True)\n    elif args.bit == \"off\" or args.bit == \"32\":\n        model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                    optimize_model=True,\n                                                    trust_remote_code=True,\n                                                    use_cache=True)\n    else:\n        model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                    load_in_low_bit=args.bit,\n                                                    optimize_model=True,\n                                                    trust_remote_code=True,\n                                                    use_cache=True)\n\n    if args.bit == \"32\":\n        model = model.to('xpu')\n    else:\n        model = model.half().to('xpu')\n\n    # 토크나이저 로드\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n    # 여기서 종결자는 다음을 참조합니다. https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct#transformers-automodelforcausallm\n    종결자 = [\n        tokenizer.eos_token_id,\n        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n    ]\n\n    # 예측된 토큰 생성\n    with torch.inference_mode():\n        prompt = get_prompt(args.prompt, [], system_prompt=DEFAULT_SYSTEM_PROMPT)\n        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('xpu')\n        # ipex_llm 모델은 워밍업이 필요하므로 추론 시간을 정확하게 할 수 있습니다.\n        output = model.generate(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=20)\n\n        # 추론 시작\n        st = time.time()\n        output = model.generate(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=args.n_predict)\n        torch.xpu.synchronize()\n        end = time.time()\n        output = output.cpu()\n        output_str = tokenizer.decode(output[0], skip_special_tokens=False)\n        print(f'추론 시간: {end-st} s, 토큰: {len(output[0])}, t/s:{len(output[0])/(end-st)}')\n        print('-'*20, '프롬프트', '-'*20)\n        print(prompt)\n        print('-'*20, '출력 (skip_special_tokens=False)', '-'*20)\n        print(output_str)\n```\n\n<div class=\"content-ad\"></div>\n\n```js\npython ipex-llm-llama3.py --repo-id-or-model-path=meta-llama/Meta-Llama-3-8B-Instruct --bit=fp16\n```\n\n![image](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_2.png)\n\n![image](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_3.png)\n\nDirectML은 낮은 비트 양자화를 지원하는 ONNX Runtime의 Execution Provider가 될 수도 있습니다. 우리의 테스트는 이 python API를 기반으로 합니다.\n\n<div class=\"content-ad\"></div>\n\n\n```js\npip install onnxruntime-genai --pre\npip install onnxruntime-genai-directml --pre\npip install torch transformers onnx onnxruntime\nconda install conda-forge::vs2015_runtime\n```\n\n마지막 줄은 이 문제를 해결하기 위한 것입니다.\n\n라마 모델은 다음과 같이 변환되었습니다.\n\n```js\npython -m onnxruntime_genai.models.builder -m meta-llama/Meta-Llama-3-8B-Instruct -o llama-3-8B-Instruct-int4-onnx-directml -p int4 -e dml -c ..\\.cache\\huggingface\\hub\\\n```\n\n<div class=\"content-ad\"></div>\n\n변환된 모델이 🤗 허브에 업로드되었습니다.\n\n다음은 테스트 프로그램 genai-llama3.py입니다.\n\n```python\nimport time\nimport argparse\nimport onnxruntime_genai as og\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Predict Tokens using onnxruntime_genai')\n    parser.add_argument('--model-path', type=str, default=\"llama-3-8B-Instruct-int4-onnx-directml\",\n                        help='model path')\n    parser.add_argument('--prompt', type=str, default=\"OpenVINO is\",\n                        help='Prompt to infer')\n    parser.add_argument(\n        '--max-length',\n        type=int,\n        default=143,\n        help='max lengths'\n    )\n    args = parser.parse_args()\n\n    model = og.Model(args.model_path)\n    tokenizer = og.Tokenizer(model)\n    \n    # Set the max length to something sensible by default,\n    # since otherwise it will be set to the entire context length\n    search_options = {}\n    search_options['max_length'] = args.max_length\n\n    chat_template = '<|user|>\\n{input} <|end|>\\n<|assistant|>'\n\n    text = args.prompt\n    if not text:\n        print(\"오류, 입력이 비어 있을 수 없습니다\")\n        exit\n\n    prompt = f'{chat_template.format(input=text)}'\n\n    input_tokens = tokenizer.encode(prompt)\n\n    params = og.GeneratorParams(model)\n    params.set_search_options(**search_options)\n    params.input_ids = input_tokens\n\n    st =  time.time()\n    output = model.generate(params)\n    out_txt = tokenizer.decode(output[0])\n\n    sec = time.time() - st\n    cnt = len(output[0])\n\n    print(\"생성 결과:\", out_txt)\n    print(f'추론 시간: {sec} 초, 토큰 수: {cnt}, 초당 토큰 수:{cnt/sec}')\n```\n\n<img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_4.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_5.png)\n\n🤗Transformers + IPEX-LLM과 비교해 볼 수 있어요.\n\n```js\npython ipex-llm-llama3.py --repo-id-or-model-path=meta-llama/Meta-Llama-3-8B-Instruct --bit=4\n```\n\n![이미지](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_6.png)\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지1](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_7.png)\n\n결과를 함께 살펴봅시다.\n\n![이미지2](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_8.png)\n\n앞으로 어떻게 이러한 흥미로운 차이가 생겼는지에 대한 구현 세부사항을 탐구할 것입니다.\n","ogImage":{"url":"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png"},"coverImage":"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png","tag":["Tech"],"readingTime":10},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>이전 글에서 언급했듯이 Intel은 ONNX + DirectML을 위한 하드웨어 가속화를 제공합니다. 이에 대해 몇 가지 실험을 진행했습니다.</p>\n<p>Microsoft은 PyTorch를 위한 DirectML 인터페이스를 제공합니다. 현재 16비트와 32비트 부동 소수점에서만 추론을 지원합니다. 예제에서 초당 토큰 수를 측정하기 위해 포크를 생성했습니다.</p>\n<pre><code class=\"hljs language-js\">conda create --name pytdml python=<span class=\"hljs-number\">3.10</span> -y\nconda activate pytdml\npip install torch-directml\ngit clone <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//github.com/luweigen/DirectML</span>\ncd <span class=\"hljs-title class_\">DirectML</span>/<span class=\"hljs-title class_\">PyTorch</span>/llm\npip install -r requirements.<span class=\"hljs-property\">txt</span>\npython app.<span class=\"hljs-property\">py</span> --precision float16 --model_repo <span class=\"hljs-string\">\"meta-llama/Meta-Llama-3-8B-Instruct\"</span> --stream_every_n=<span class=\"hljs-number\">143</span>\n</code></pre>\n<p>이전 실험에서 🤗Transformers + IPEX-LLM이 최상의 성능을 보여줬기 때문에 이 설정에서는 16비트 부동 소수점 추론만 비교할 것입니다.</p>\n<p>테스트 프로그램 ipex-llm-llama3.py는 다음과 같이 수정되었습니다.</p>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-title class_\">Wei</span> <span class=\"hljs-title class_\">Lu</span>(mailwlu@gmail.<span class=\"hljs-property\">com</span>)에 의해 수정됨\n# <span class=\"hljs-number\">2016</span>년 <span class=\"hljs-title class_\">The</span> <span class=\"hljs-title class_\">BigDL</span> <span class=\"hljs-title class_\">Authors</span>에 저작권 속함\n#\n# <span class=\"hljs-title class_\">Apache</span> 라이선스, 버전 <span class=\"hljs-number\">2.0</span>에 따라 라이선스 부여\n# 이 파일을 라이선스와 준수하면서 사용해야 합니다.\n# 라이선스 사본은 다음에서 얻을 수 있습니다.\n#\n#     <span class=\"hljs-attr\">http</span>:<span class=\"hljs-comment\">//www.apache.org/licenses/LICENSE-2.0</span>\n#\n# 적용되는 법률에 따라 필요하거나 서면으로 합의되거나, 소프트웨어가\n# <span class=\"hljs-string\">\"있는 그대로\"</span> 배포됩니다. 조건이나 보증 없이\n# 명시 또는 묵시적으로, 까지도 어떤 종류의 조건도 보증 없이,\n# 명시적 또는 묵시적으로. 언어 특정 권한과 관련해야 합니다.\n# 권한 및 제한 사항\n#\n\n<span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">import</span> time\n<span class=\"hljs-keyword\">import</span> argparse\n\n<span class=\"hljs-keyword\">from</span> ipex_llm.<span class=\"hljs-property\">transformers</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">AutoModelForCausalLM</span>\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">AutoTokenizer</span>\n\n# 모델을 기반으로 프롬프트를 조정할 수 있습니다.\n# 여기서 프롬프트 조정은 다음을 참조합니다. <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3</span>\n기본_시스템_프롬프트 = <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\\\n\"</span><span class=\"hljs-string\">\"\"</span>\n\ndef <span class=\"hljs-title function_\">get_prompt</span>(<span class=\"hljs-attr\">user_input</span>: str, <span class=\"hljs-attr\">chat_history</span>: list[tuple[str, str]],\n               <span class=\"hljs-attr\">system_prompt</span>: str) -> <span class=\"hljs-attr\">str</span>:\n    prompt_texts = [f<span class=\"hljs-string\">'&#x3C;|begin_of_text|>'</span>]\n\n    <span class=\"hljs-keyword\">if</span> system_prompt != <span class=\"hljs-string\">''</span>:\n        prompt_texts.<span class=\"hljs-title function_\">append</span>(f<span class=\"hljs-string\">'&#x3C;|start_header_id|>system&#x3C;|end_header_id|>\\n\\n{system_prompt}&#x3C;|eot_id|>'</span>)\n\n    <span class=\"hljs-keyword\">for</span> history_input, history_response <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">chat_history</span>:\n        prompt_texts.<span class=\"hljs-title function_\">append</span>(f<span class=\"hljs-string\">'&#x3C;|start_header_id|>user&#x3C;|end_header_id|>\\n\\n{history_input.strip()}&#x3C;|eot_id|>'</span>)\n        prompt_texts.<span class=\"hljs-title function_\">append</span>(f<span class=\"hljs-string\">'&#x3C;|start_header_id|>assistant&#x3C;|end_header_id|>\\n\\n{history_response.strip()}&#x3C;|eot_id|>'</span>)\n\n    prompt_texts.<span class=\"hljs-title function_\">append</span>(f<span class=\"hljs-string\">'&#x3C;|start_header_id|>user&#x3C;|end_header_id|>\\n\\n{user_input.strip()}&#x3C;|eot_id|>&#x3C;|start_header_id|>assistant&#x3C;|end_header_id|>\\n\\n'</span>)\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">''</span>.<span class=\"hljs-title function_\">join</span>(prompt_texts)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n    parser = argparse.<span class=\"hljs-title class_\">ArgumentParser</span>(description=<span class=\"hljs-string\">'Llama3 모델을 사용하여 `generate()` API를 사용하여 토큰 예측'</span>)\n    parser.<span class=\"hljs-title function_\">add_argument</span>(<span class=\"hljs-string\">'--repo-id-or-model-path'</span>, type=str, <span class=\"hljs-keyword\">default</span>=<span class=\"hljs-string\">\"meta-llama/Meta-Llama-3-70B-Instruct\"</span>,\n                        help=<span class=\"hljs-string\">'Meta-Llama-3 (예: `meta-llama/Meta-Llama-3-70B-Instruct`)를 다운로드할 Huggingface 저장소 ID'</span>\n                             <span class=\"hljs-string\">'또는 Huggingface 체크포인트 폴더에 대한 경로'</span>)\n    parser.<span class=\"hljs-title function_\">add_argument</span>(<span class=\"hljs-string\">'--prompt'</span>, type=str, <span class=\"hljs-keyword\">default</span>=<span class=\"hljs-string\">\"OpenVINO is\"</span>,\n                        help=<span class=\"hljs-string\">'추론할 프롬프트'</span>)\n    parser.<span class=\"hljs-title function_\">add_argument</span>(<span class=\"hljs-string\">'--n-predict'</span>, type=int, <span class=\"hljs-keyword\">default</span>=<span class=\"hljs-number\">128</span>,\n                        help=<span class=\"hljs-string\">'예측할 최대 토큰 수'</span>)\n    parser.<span class=\"hljs-title function_\">add_argument</span>(<span class=\"hljs-string\">'--bit'</span>, type=str, <span class=\"hljs-keyword\">default</span>=<span class=\"hljs-string\">\"4\"</span>,\n                        help=<span class=\"hljs-string\">'4로 설정하면 4비트로 로드하거나 off 또는 load_in_low_bit 옵션은 sym_int4, asym_int4, sym_int5, asym_int5, sym_int8,nf3,nf4, fp4, fp8, fp8_e4m3, fp8_e5m2, fp6, gguf_iq2_xxs, gguf_iq2_xs, gguf_iq1_s, gguf_q4k_m, gguf_q4k_s, fp16, bf16, fp6_k'</span>)\n\n    args = parser.<span class=\"hljs-title function_\">parse_args</span>()\n    model_path = args.<span class=\"hljs-property\">repo_id_or_model_path</span>\n\n    <span class=\"hljs-keyword\">if</span> args.<span class=\"hljs-property\">bit</span> == <span class=\"hljs-string\">\"4\"</span>:\n        # <span class=\"hljs-number\">4</span>비트로 모델 로드,\n        # 모델의 관련 레이어를 <span class=\"hljs-title class_\">INT4</span> 형식으로 변환합니다.\n        # iGPU를 사용하는 <span class=\"hljs-title class_\">Windows</span> 사용자의 경우, <span class=\"hljs-variable constant_\">LLM</span>을 실행할 때 <span class=\"hljs-string\">`cpu_embedding=True`</span>를 from_pretrained 함수에서 설정하는 것을 권장합니다.\n        # 이렇게 하면 메모리 집약적인 임베딩 레이어가 iGPU 대신 <span class=\"hljs-variable constant_\">CPU</span>를 활용할 수 있습니다. 이 경우에는 도움이 되지 않습니다.\n        model = <span class=\"hljs-title class_\">AutoModelForCausalLM</span>.<span class=\"hljs-title function_\">from_pretrained</span>(model_path,\n                                                    load_in_4bit=<span class=\"hljs-title class_\">True</span>,\n                                                    optimize_model=<span class=\"hljs-title class_\">True</span>,\n                                                    trust_remote_code=<span class=\"hljs-title class_\">True</span>,\n                                                    use_cache=<span class=\"hljs-title class_\">True</span>)\n    elif args.<span class=\"hljs-property\">bit</span> == <span class=\"hljs-string\">\"off\"</span> or args.<span class=\"hljs-property\">bit</span> == <span class=\"hljs-string\">\"32\"</span>:\n        model = <span class=\"hljs-title class_\">AutoModelForCausalLM</span>.<span class=\"hljs-title function_\">from_pretrained</span>(model_path,\n                                                    optimize_model=<span class=\"hljs-title class_\">True</span>,\n                                                    trust_remote_code=<span class=\"hljs-title class_\">True</span>,\n                                                    use_cache=<span class=\"hljs-title class_\">True</span>)\n    <span class=\"hljs-attr\">else</span>:\n        model = <span class=\"hljs-title class_\">AutoModelForCausalLM</span>.<span class=\"hljs-title function_\">from_pretrained</span>(model_path,\n                                                    load_in_low_bit=args.<span class=\"hljs-property\">bit</span>,\n                                                    optimize_model=<span class=\"hljs-title class_\">True</span>,\n                                                    trust_remote_code=<span class=\"hljs-title class_\">True</span>,\n                                                    use_cache=<span class=\"hljs-title class_\">True</span>)\n\n    <span class=\"hljs-keyword\">if</span> args.<span class=\"hljs-property\">bit</span> == <span class=\"hljs-string\">\"32\"</span>:\n        model = model.<span class=\"hljs-title function_\">to</span>(<span class=\"hljs-string\">'xpu'</span>)\n    <span class=\"hljs-attr\">else</span>:\n        model = model.<span class=\"hljs-title function_\">half</span>().<span class=\"hljs-title function_\">to</span>(<span class=\"hljs-string\">'xpu'</span>)\n\n    # 토크나이저 로드\n    tokenizer = <span class=\"hljs-title class_\">AutoTokenizer</span>.<span class=\"hljs-title function_\">from_pretrained</span>(model_path, trust_remote_code=<span class=\"hljs-title class_\">True</span>)\n\n    # 여기서 종결자는 다음을 참조합니다. <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct#transformers-automodelforcausallm</span>\n    종결자 = [\n        tokenizer.<span class=\"hljs-property\">eos_token_id</span>,\n        tokenizer.<span class=\"hljs-title function_\">convert_tokens_to_ids</span>(<span class=\"hljs-string\">\"&#x3C;|eot_id|>\"</span>),\n    ]\n\n    # 예측된 토큰 생성\n    <span class=\"hljs-keyword\">with</span> torch.<span class=\"hljs-title function_\">inference_mode</span>():\n        prompt = <span class=\"hljs-title function_\">get_prompt</span>(args.<span class=\"hljs-property\">prompt</span>, [], system_prompt=<span class=\"hljs-variable constant_\">DEFAULT_SYSTEM_PROMPT</span>)\n        input_ids = tokenizer.<span class=\"hljs-title function_\">encode</span>(prompt, return_tensors=<span class=\"hljs-string\">\"pt\"</span>).<span class=\"hljs-title function_\">to</span>(<span class=\"hljs-string\">'xpu'</span>)\n        # ipex_llm 모델은 워밍업이 필요하므로 추론 시간을 정확하게 할 수 있습니다.\n        output = model.<span class=\"hljs-title function_\">generate</span>(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=<span class=\"hljs-number\">20</span>)\n\n        # 추론 시작\n        st = time.<span class=\"hljs-title function_\">time</span>()\n        output = model.<span class=\"hljs-title function_\">generate</span>(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=args.<span class=\"hljs-property\">n_predict</span>)\n        torch.<span class=\"hljs-property\">xpu</span>.<span class=\"hljs-title function_\">synchronize</span>()\n        end = time.<span class=\"hljs-title function_\">time</span>()\n        output = output.<span class=\"hljs-title function_\">cpu</span>()\n        output_str = tokenizer.<span class=\"hljs-title function_\">decode</span>(output[<span class=\"hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-title class_\">False</span>)\n        <span class=\"hljs-title function_\">print</span>(f<span class=\"hljs-string\">'추론 시간: {end-st} s, 토큰: {len(output[0])}, t/s:{len(output[0])/(end-st)}'</span>)\n        <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">'-'</span>*<span class=\"hljs-number\">20</span>, <span class=\"hljs-string\">'프롬프트'</span>, <span class=\"hljs-string\">'-'</span>*<span class=\"hljs-number\">20</span>)\n        <span class=\"hljs-title function_\">print</span>(prompt)\n        <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">'-'</span>*<span class=\"hljs-number\">20</span>, <span class=\"hljs-string\">'출력 (skip_special_tokens=False)'</span>, <span class=\"hljs-string\">'-'</span>*<span class=\"hljs-number\">20</span>)\n        <span class=\"hljs-title function_\">print</span>(output_str)\n</code></pre>\n<pre><code class=\"hljs language-js\">python ipex-llm-llama3.<span class=\"hljs-property\">py</span> --repo-id-or-model-path=meta-llama/<span class=\"hljs-title class_\">Meta</span>-<span class=\"hljs-title class_\">Llama</span>-<span class=\"hljs-number\">3</span>-8B-<span class=\"hljs-title class_\">Instruct</span> --bit=fp16\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_2.png\" alt=\"image\"></p>\n<p><img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_3.png\" alt=\"image\"></p>\n<p>DirectML은 낮은 비트 양자화를 지원하는 ONNX Runtime의 Execution Provider가 될 수도 있습니다. 우리의 테스트는 이 python API를 기반으로 합니다.</p>\n<pre><code class=\"hljs language-js\">pip install onnxruntime-genai --pre\npip install onnxruntime-genai-directml --pre\npip install torch transformers onnx onnxruntime\nconda install conda-<span class=\"hljs-attr\">forge</span>::vs2015_runtime\n</code></pre>\n<p>마지막 줄은 이 문제를 해결하기 위한 것입니다.</p>\n<p>라마 모델은 다음과 같이 변환되었습니다.</p>\n<pre><code class=\"hljs language-js\">python -m onnxruntime_genai.<span class=\"hljs-property\">models</span>.<span class=\"hljs-property\">builder</span> -m meta-llama/<span class=\"hljs-title class_\">Meta</span>-<span class=\"hljs-title class_\">Llama</span>-<span class=\"hljs-number\">3</span>-8B-<span class=\"hljs-title class_\">Instruct</span> -o llama-<span class=\"hljs-number\">3</span>-8B-<span class=\"hljs-title class_\">Instruct</span>-int4-onnx-directml -p int4 -e dml -c ..\\.<span class=\"hljs-property\">cache</span>\\huggingface\\hub\\\n</code></pre>\n<p>변환된 모델이 🤗 허브에 업로드되었습니다.</p>\n<p>다음은 테스트 프로그램 genai-llama3.py입니다.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> time\n<span class=\"hljs-keyword\">import</span> argparse\n<span class=\"hljs-keyword\">import</span> onnxruntime_genai <span class=\"hljs-keyword\">as</span> og\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n    parser = argparse.ArgumentParser(description=<span class=\"hljs-string\">'Predict Tokens using onnxruntime_genai'</span>)\n    parser.add_argument(<span class=\"hljs-string\">'--model-path'</span>, <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">str</span>, default=<span class=\"hljs-string\">\"llama-3-8B-Instruct-int4-onnx-directml\"</span>,\n                        <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">'model path'</span>)\n    parser.add_argument(<span class=\"hljs-string\">'--prompt'</span>, <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">str</span>, default=<span class=\"hljs-string\">\"OpenVINO is\"</span>,\n                        <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">'Prompt to infer'</span>)\n    parser.add_argument(\n        <span class=\"hljs-string\">'--max-length'</span>,\n        <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">int</span>,\n        default=<span class=\"hljs-number\">143</span>,\n        <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">'max lengths'</span>\n    )\n    args = parser.parse_args()\n\n    model = og.Model(args.model_path)\n    tokenizer = og.Tokenizer(model)\n    \n    <span class=\"hljs-comment\"># Set the max length to something sensible by default,</span>\n    <span class=\"hljs-comment\"># since otherwise it will be set to the entire context length</span>\n    search_options = {}\n    search_options[<span class=\"hljs-string\">'max_length'</span>] = args.max_length\n\n    chat_template = <span class=\"hljs-string\">'&#x3C;|user|>\\n{input} &#x3C;|end|>\\n&#x3C;|assistant|>'</span>\n\n    text = args.prompt\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> text:\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"오류, 입력이 비어 있을 수 없습니다\"</span>)\n        exit\n\n    prompt = <span class=\"hljs-string\">f'<span class=\"hljs-subst\">{chat_template.<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-built_in\">input</span>=text)}</span>'</span>\n\n    input_tokens = tokenizer.encode(prompt)\n\n    params = og.GeneratorParams(model)\n    params.set_search_options(**search_options)\n    params.input_ids = input_tokens\n\n    st =  time.time()\n    output = model.generate(params)\n    out_txt = tokenizer.decode(output[<span class=\"hljs-number\">0</span>])\n\n    sec = time.time() - st\n    cnt = <span class=\"hljs-built_in\">len</span>(output[<span class=\"hljs-number\">0</span>])\n\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"생성 결과:\"</span>, out_txt)\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f'추론 시간: <span class=\"hljs-subst\">{sec}</span> 초, 토큰 수: <span class=\"hljs-subst\">{cnt}</span>, 초당 토큰 수:<span class=\"hljs-subst\">{cnt/sec}</span>'</span>)\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_5.png\" alt=\"이미지\"></p>\n<p>🤗Transformers + IPEX-LLM과 비교해 볼 수 있어요.</p>\n<pre><code class=\"hljs language-js\">python ipex-llm-llama3.<span class=\"hljs-property\">py</span> --repo-id-or-model-path=meta-llama/<span class=\"hljs-title class_\">Meta</span>-<span class=\"hljs-title class_\">Llama</span>-<span class=\"hljs-number\">3</span>-8B-<span class=\"hljs-title class_\">Instruct</span> --bit=<span class=\"hljs-number\">4</span>\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_6.png\" alt=\"이미지\"></p>\n<p><img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_7.png\" alt=\"이미지1\"></p>\n<p>결과를 함께 살펴봅시다.</p>\n<p><img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_8.png\" alt=\"이미지2\"></p>\n<p>앞으로 어떻게 이러한 흥미로운 차이가 생겼는지에 대한 구현 세부사항을 탐구할 것입니다.</p>\n</body>\n</html>\n"},"__N_SSG":true}