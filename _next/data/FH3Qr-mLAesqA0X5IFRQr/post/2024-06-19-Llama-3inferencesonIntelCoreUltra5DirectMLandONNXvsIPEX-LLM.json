{"pageProps":{"post":{"title":"Llama-3 ì¶”ë¡ ì„ Intel Core Ultra 5ì—ì„œ ì‹¤í–‰í•˜ê¸° DirectML ë° ONNX ëŒ€ IPEX-LLM","description":"","date":"2024-06-19 01:21","slug":"2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM","content":"\n\nì´ì „ ê¸€ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ Intelì€ ONNX + DirectMLì„ ìœ„í•œ í•˜ë“œì›¨ì–´ ê°€ì†í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ì— ëŒ€í•´ ëª‡ ê°€ì§€ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.\n\nMicrosoftì€ PyTorchë¥¼ ìœ„í•œ DirectML ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í˜„ì¬ 16ë¹„íŠ¸ì™€ 32ë¹„íŠ¸ ë¶€ë™ ì†Œìˆ˜ì ì—ì„œë§Œ ì¶”ë¡ ì„ ì§€ì›í•©ë‹ˆë‹¤. ì˜ˆì œì—ì„œ ì´ˆë‹¹ í† í° ìˆ˜ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ í¬í¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\n\n```js\nconda create --name pytdml python=3.10 -y\nconda activate pytdml\npip install torch-directml\ngit clone https://github.com/luweigen/DirectML\ncd DirectML/PyTorch/llm\npip install -r requirements.txt\npython app.py --precision float16 --model_repo \"meta-llama/Meta-Llama-3-8B-Instruct\" --stream_every_n=143\n```\n\n<img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png\" />\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_1.png\" />\n\nì´ì „ ì‹¤í—˜ì—ì„œ ğŸ¤—Transformers + IPEX-LLMì´ ìµœìƒì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ê¸° ë•Œë¬¸ì— ì´ ì„¤ì •ì—ì„œëŠ” 16ë¹„íŠ¸ ë¶€ë™ ì†Œìˆ˜ì  ì¶”ë¡ ë§Œ ë¹„êµí•  ê²ƒì…ë‹ˆë‹¤.\n\ní…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ ipex-llm-llama3.pyëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n```js\n# Wei Lu(mailwlu@gmail.com)ì— ì˜í•´ ìˆ˜ì •ë¨\n# 2016ë…„ The BigDL Authorsì— ì €ì‘ê¶Œ ì†í•¨\n#\n# Apache ë¼ì´ì„ ìŠ¤, ë²„ì „ 2.0ì— ë”°ë¼ ë¼ì´ì„ ìŠ¤ ë¶€ì—¬\n# ì´ íŒŒì¼ì„ ë¼ì´ì„ ìŠ¤ì™€ ì¤€ìˆ˜í•˜ë©´ì„œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n# ë¼ì´ì„ ìŠ¤ ì‚¬ë³¸ì€ ë‹¤ìŒì—ì„œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# ì ìš©ë˜ëŠ” ë²•ë¥ ì— ë”°ë¼ í•„ìš”í•˜ê±°ë‚˜ ì„œë©´ìœ¼ë¡œ í•©ì˜ë˜ê±°ë‚˜, ì†Œí”„íŠ¸ì›¨ì–´ê°€\n# \"ìˆëŠ” ê·¸ëŒ€ë¡œ\" ë°°í¬ë©ë‹ˆë‹¤. ì¡°ê±´ì´ë‚˜ ë³´ì¦ ì—†ì´\n# ëª…ì‹œ ë˜ëŠ” ë¬µì‹œì ìœ¼ë¡œ, ê¹Œì§€ë„ ì–´ë–¤ ì¢…ë¥˜ì˜ ì¡°ê±´ë„ ë³´ì¦ ì—†ì´,\n# ëª…ì‹œì  ë˜ëŠ” ë¬µì‹œì ìœ¼ë¡œ. ì–¸ì–´ íŠ¹ì • ê¶Œí•œê³¼ ê´€ë ¨í•´ì•¼ í•©ë‹ˆë‹¤.\n# ê¶Œí•œ ë° ì œí•œ ì‚¬í•­\n#\n\nimport torch\nimport time\nimport argparse\n\nfrom ipex_llm.transformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer\n\n# ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n# ì—¬ê¸°ì„œ í”„ë¡¬í”„íŠ¸ ì¡°ì •ì€ ë‹¤ìŒì„ ì°¸ì¡°í•©ë‹ˆë‹¤. https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3\nê¸°ë³¸_ì‹œìŠ¤í…œ_í”„ë¡¬í”„íŠ¸ = \"\"\"\\\n\"\"\"\n\ndef get_prompt(user_input: str, chat_history: list[tuple[str, str]],\n               system_prompt: str) -> str:\n    prompt_texts = [f'<|begin_of_text|>']\n\n    if system_prompt != '':\n        prompt_texts.append(f'<|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|>')\n\n    for history_input, history_response in chat_history:\n        prompt_texts.append(f'<|start_header_id|>user<|end_header_id|>\\n\\n{history_input.strip()}<|eot_id|>')\n        prompt_texts.append(f'<|start_header_id|>assistant<|end_header_id|>\\n\\n{history_response.strip()}<|eot_id|>')\n\n    prompt_texts.append(f'<|start_header_id|>user<|end_header_id|>\\n\\n{user_input.strip()}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n')\n    return ''.join(prompt_texts)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Llama3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ `generate()` APIë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ì˜ˆì¸¡')\n    parser.add_argument('--repo-id-or-model-path', type=str, default=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                        help='Meta-Llama-3 (ì˜ˆ: `meta-llama/Meta-Llama-3-70B-Instruct`)ë¥¼ ë‹¤ìš´ë¡œë“œí•  Huggingface ì €ì¥ì†Œ ID'\n                             'ë˜ëŠ” Huggingface ì²´í¬í¬ì¸íŠ¸ í´ë”ì— ëŒ€í•œ ê²½ë¡œ')\n    parser.add_argument('--prompt', type=str, default=\"OpenVINO is\",\n                        help='ì¶”ë¡ í•  í”„ë¡¬í”„íŠ¸')\n    parser.add_argument('--n-predict', type=int, default=128,\n                        help='ì˜ˆì¸¡í•  ìµœëŒ€ í† í° ìˆ˜')\n    parser.add_argument('--bit', type=str, default=\"4\",\n                        help='4ë¡œ ì„¤ì •í•˜ë©´ 4ë¹„íŠ¸ë¡œ ë¡œë“œí•˜ê±°ë‚˜ off ë˜ëŠ” load_in_low_bit ì˜µì…˜ì€ sym_int4, asym_int4, sym_int5, asym_int5, sym_int8,nf3,nf4, fp4, fp8, fp8_e4m3, fp8_e5m2, fp6, gguf_iq2_xxs, gguf_iq2_xs, gguf_iq1_s, gguf_q4k_m, gguf_q4k_s, fp16, bf16, fp6_k')\n\n    args = parser.parse_args()\n    model_path = args.repo_id_or_model_path\n\n    if args.bit == \"4\":\n        # 4ë¹„íŠ¸ë¡œ ëª¨ë¸ ë¡œë“œ,\n        # ëª¨ë¸ì˜ ê´€ë ¨ ë ˆì´ì–´ë¥¼ INT4 í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n        # iGPUë¥¼ ì‚¬ìš©í•˜ëŠ” Windows ì‚¬ìš©ìì˜ ê²½ìš°, LLMì„ ì‹¤í–‰í•  ë•Œ `cpu_embedding=True`ë¥¼ from_pretrained í•¨ìˆ˜ì—ì„œ ì„¤ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n        # ì´ë ‡ê²Œ í•˜ë©´ ë©”ëª¨ë¦¬ ì§‘ì•½ì ì¸ ì„ë² ë”© ë ˆì´ì–´ê°€ iGPU ëŒ€ì‹  CPUë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°ì—ëŠ” ë„ì›€ì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n        model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                    load_in_4bit=True,\n                                                    optimize_model=True,\n                                                    trust_remote_code=True,\n                                                    use_cache=True)\n    elif args.bit == \"off\" or args.bit == \"32\":\n        model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                    optimize_model=True,\n                                                    trust_remote_code=True,\n                                                    use_cache=True)\n    else:\n        model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                    load_in_low_bit=args.bit,\n                                                    optimize_model=True,\n                                                    trust_remote_code=True,\n                                                    use_cache=True)\n\n    if args.bit == \"32\":\n        model = model.to('xpu')\n    else:\n        model = model.half().to('xpu')\n\n    # í† í¬ë‚˜ì´ì € ë¡œë“œ\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n    # ì—¬ê¸°ì„œ ì¢…ê²°ìëŠ” ë‹¤ìŒì„ ì°¸ì¡°í•©ë‹ˆë‹¤. https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct#transformers-automodelforcausallm\n    ì¢…ê²°ì = [\n        tokenizer.eos_token_id,\n        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n    ]\n\n    # ì˜ˆì¸¡ëœ í† í° ìƒì„±\n    with torch.inference_mode():\n        prompt = get_prompt(args.prompt, [], system_prompt=DEFAULT_SYSTEM_PROMPT)\n        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('xpu')\n        # ipex_llm ëª¨ë¸ì€ ì›Œë°ì—…ì´ í•„ìš”í•˜ë¯€ë¡œ ì¶”ë¡  ì‹œê°„ì„ ì •í™•í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n        output = model.generate(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=20)\n\n        # ì¶”ë¡  ì‹œì‘\n        st = time.time()\n        output = model.generate(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=args.n_predict)\n        torch.xpu.synchronize()\n        end = time.time()\n        output = output.cpu()\n        output_str = tokenizer.decode(output[0], skip_special_tokens=False)\n        print(f'ì¶”ë¡  ì‹œê°„: {end-st} s, í† í°: {len(output[0])}, t/s:{len(output[0])/(end-st)}')\n        print('-'*20, 'í”„ë¡¬í”„íŠ¸', '-'*20)\n        print(prompt)\n        print('-'*20, 'ì¶œë ¥ (skip_special_tokens=False)', '-'*20)\n        print(output_str)\n```\n\n<div class=\"content-ad\"></div>\n\n```js\npython ipex-llm-llama3.py --repo-id-or-model-path=meta-llama/Meta-Llama-3-8B-Instruct --bit=fp16\n```\n\n![image](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_2.png)\n\n![image](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_3.png)\n\nDirectMLì€ ë‚®ì€ ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ì§€ì›í•˜ëŠ” ONNX Runtimeì˜ Execution Providerê°€ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ í…ŒìŠ¤íŠ¸ëŠ” ì´ python APIë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.\n\n<div class=\"content-ad\"></div>\n\n\n```js\npip install onnxruntime-genai --pre\npip install onnxruntime-genai-directml --pre\npip install torch transformers onnx onnxruntime\nconda install conda-forge::vs2015_runtime\n```\n\në§ˆì§€ë§‰ ì¤„ì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.\n\në¼ë§ˆ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n```js\npython -m onnxruntime_genai.models.builder -m meta-llama/Meta-Llama-3-8B-Instruct -o llama-3-8B-Instruct-int4-onnx-directml -p int4 -e dml -c ..\\.cache\\huggingface\\hub\\\n```\n\n<div class=\"content-ad\"></div>\n\në³€í™˜ëœ ëª¨ë¸ì´ ğŸ¤— í—ˆë¸Œì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n\në‹¤ìŒì€ í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ genai-llama3.pyì…ë‹ˆë‹¤.\n\n```python\nimport time\nimport argparse\nimport onnxruntime_genai as og\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Predict Tokens using onnxruntime_genai')\n    parser.add_argument('--model-path', type=str, default=\"llama-3-8B-Instruct-int4-onnx-directml\",\n                        help='model path')\n    parser.add_argument('--prompt', type=str, default=\"OpenVINO is\",\n                        help='Prompt to infer')\n    parser.add_argument(\n        '--max-length',\n        type=int,\n        default=143,\n        help='max lengths'\n    )\n    args = parser.parse_args()\n\n    model = og.Model(args.model_path)\n    tokenizer = og.Tokenizer(model)\n    \n    # Set the max length to something sensible by default,\n    # since otherwise it will be set to the entire context length\n    search_options = {}\n    search_options['max_length'] = args.max_length\n\n    chat_template = '<|user|>\\n{input} <|end|>\\n<|assistant|>'\n\n    text = args.prompt\n    if not text:\n        print(\"ì˜¤ë¥˜, ì…ë ¥ì´ ë¹„ì–´ ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        exit\n\n    prompt = f'{chat_template.format(input=text)}'\n\n    input_tokens = tokenizer.encode(prompt)\n\n    params = og.GeneratorParams(model)\n    params.set_search_options(**search_options)\n    params.input_ids = input_tokens\n\n    st =  time.time()\n    output = model.generate(params)\n    out_txt = tokenizer.decode(output[0])\n\n    sec = time.time() - st\n    cnt = len(output[0])\n\n    print(\"ìƒì„± ê²°ê³¼:\", out_txt)\n    print(f'ì¶”ë¡  ì‹œê°„: {sec} ì´ˆ, í† í° ìˆ˜: {cnt}, ì´ˆë‹¹ í† í° ìˆ˜:{cnt/sec}')\n```\n\n<img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_4.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n\n![ì´ë¯¸ì§€](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_5.png)\n\nğŸ¤—Transformers + IPEX-LLMê³¼ ë¹„êµí•´ ë³¼ ìˆ˜ ìˆì–´ìš”.\n\n```js\npython ipex-llm-llama3.py --repo-id-or-model-path=meta-llama/Meta-Llama-3-8B-Instruct --bit=4\n```\n\n![ì´ë¯¸ì§€](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_6.png)\n\n\n<div class=\"content-ad\"></div>\n\n\n![ì´ë¯¸ì§€1](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_7.png)\n\nê²°ê³¼ë¥¼ í•¨ê»˜ ì‚´í´ë´…ì‹œë‹¤.\n\n![ì´ë¯¸ì§€2](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_8.png)\n\nì•ìœ¼ë¡œ ì–´ë–»ê²Œ ì´ëŸ¬í•œ í¥ë¯¸ë¡œìš´ ì°¨ì´ê°€ ìƒê²¼ëŠ”ì§€ì— ëŒ€í•œ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì„ íƒêµ¬í•  ê²ƒì…ë‹ˆë‹¤.\n","ogImage":{"url":"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png"},"coverImage":"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png","tag":["Tech"],"readingTime":10},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>ì´ì „ ê¸€ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ Intelì€ ONNX + DirectMLì„ ìœ„í•œ í•˜ë“œì›¨ì–´ ê°€ì†í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ì— ëŒ€í•´ ëª‡ ê°€ì§€ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.</p>\n<p>Microsoftì€ PyTorchë¥¼ ìœ„í•œ DirectML ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í˜„ì¬ 16ë¹„íŠ¸ì™€ 32ë¹„íŠ¸ ë¶€ë™ ì†Œìˆ˜ì ì—ì„œë§Œ ì¶”ë¡ ì„ ì§€ì›í•©ë‹ˆë‹¤. ì˜ˆì œì—ì„œ ì´ˆë‹¹ í† í° ìˆ˜ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ í¬í¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\">conda create --name pytdml python=<span class=\"hljs-number\">3.10</span> -y\nconda activate pytdml\npip install torch-directml\ngit clone <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//github.com/luweigen/DirectML</span>\ncd <span class=\"hljs-title class_\">DirectML</span>/<span class=\"hljs-title class_\">PyTorch</span>/llm\npip install -r requirements.<span class=\"hljs-property\">txt</span>\npython app.<span class=\"hljs-property\">py</span> --precision float16 --model_repo <span class=\"hljs-string\">\"meta-llama/Meta-Llama-3-8B-Instruct\"</span> --stream_every_n=<span class=\"hljs-number\">143</span>\n</code></pre>\n<p>ì´ì „ ì‹¤í—˜ì—ì„œ ğŸ¤—Transformers + IPEX-LLMì´ ìµœìƒì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ê¸° ë•Œë¬¸ì— ì´ ì„¤ì •ì—ì„œëŠ” 16ë¹„íŠ¸ ë¶€ë™ ì†Œìˆ˜ì  ì¶”ë¡ ë§Œ ë¹„êµí•  ê²ƒì…ë‹ˆë‹¤.</p>\n<p>í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ ipex-llm-llama3.pyëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-title class_\">Wei</span> <span class=\"hljs-title class_\">Lu</span>(mailwlu@gmail.<span class=\"hljs-property\">com</span>)ì— ì˜í•´ ìˆ˜ì •ë¨\n# <span class=\"hljs-number\">2016</span>ë…„ <span class=\"hljs-title class_\">The</span> <span class=\"hljs-title class_\">BigDL</span> <span class=\"hljs-title class_\">Authors</span>ì— ì €ì‘ê¶Œ ì†í•¨\n#\n# <span class=\"hljs-title class_\">Apache</span> ë¼ì´ì„ ìŠ¤, ë²„ì „ <span class=\"hljs-number\">2.0</span>ì— ë”°ë¼ ë¼ì´ì„ ìŠ¤ ë¶€ì—¬\n# ì´ íŒŒì¼ì„ ë¼ì´ì„ ìŠ¤ì™€ ì¤€ìˆ˜í•˜ë©´ì„œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n# ë¼ì´ì„ ìŠ¤ ì‚¬ë³¸ì€ ë‹¤ìŒì—ì„œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n#\n#     <span class=\"hljs-attr\">http</span>:<span class=\"hljs-comment\">//www.apache.org/licenses/LICENSE-2.0</span>\n#\n# ì ìš©ë˜ëŠ” ë²•ë¥ ì— ë”°ë¼ í•„ìš”í•˜ê±°ë‚˜ ì„œë©´ìœ¼ë¡œ í•©ì˜ë˜ê±°ë‚˜, ì†Œí”„íŠ¸ì›¨ì–´ê°€\n# <span class=\"hljs-string\">\"ìˆëŠ” ê·¸ëŒ€ë¡œ\"</span> ë°°í¬ë©ë‹ˆë‹¤. ì¡°ê±´ì´ë‚˜ ë³´ì¦ ì—†ì´\n# ëª…ì‹œ ë˜ëŠ” ë¬µì‹œì ìœ¼ë¡œ, ê¹Œì§€ë„ ì–´ë–¤ ì¢…ë¥˜ì˜ ì¡°ê±´ë„ ë³´ì¦ ì—†ì´,\n# ëª…ì‹œì  ë˜ëŠ” ë¬µì‹œì ìœ¼ë¡œ. ì–¸ì–´ íŠ¹ì • ê¶Œí•œê³¼ ê´€ë ¨í•´ì•¼ í•©ë‹ˆë‹¤.\n# ê¶Œí•œ ë° ì œí•œ ì‚¬í•­\n#\n\n<span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">import</span> time\n<span class=\"hljs-keyword\">import</span> argparse\n\n<span class=\"hljs-keyword\">from</span> ipex_llm.<span class=\"hljs-property\">transformers</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">AutoModelForCausalLM</span>\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">AutoTokenizer</span>\n\n# ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n# ì—¬ê¸°ì„œ í”„ë¡¬í”„íŠ¸ ì¡°ì •ì€ ë‹¤ìŒì„ ì°¸ì¡°í•©ë‹ˆë‹¤. <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3</span>\nê¸°ë³¸_ì‹œìŠ¤í…œ_í”„ë¡¬í”„íŠ¸ = <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\\\n\"</span><span class=\"hljs-string\">\"\"</span>\n\ndef <span class=\"hljs-title function_\">get_prompt</span>(<span class=\"hljs-attr\">user_input</span>: str, <span class=\"hljs-attr\">chat_history</span>: list[tuple[str, str]],\n               <span class=\"hljs-attr\">system_prompt</span>: str) -> <span class=\"hljs-attr\">str</span>:\n    prompt_texts = [f<span class=\"hljs-string\">'&#x3C;|begin_of_text|>'</span>]\n\n    <span class=\"hljs-keyword\">if</span> system_prompt != <span class=\"hljs-string\">''</span>:\n        prompt_texts.<span class=\"hljs-title function_\">append</span>(f<span class=\"hljs-string\">'&#x3C;|start_header_id|>system&#x3C;|end_header_id|>\\n\\n{system_prompt}&#x3C;|eot_id|>'</span>)\n\n    <span class=\"hljs-keyword\">for</span> history_input, history_response <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">chat_history</span>:\n        prompt_texts.<span class=\"hljs-title function_\">append</span>(f<span class=\"hljs-string\">'&#x3C;|start_header_id|>user&#x3C;|end_header_id|>\\n\\n{history_input.strip()}&#x3C;|eot_id|>'</span>)\n        prompt_texts.<span class=\"hljs-title function_\">append</span>(f<span class=\"hljs-string\">'&#x3C;|start_header_id|>assistant&#x3C;|end_header_id|>\\n\\n{history_response.strip()}&#x3C;|eot_id|>'</span>)\n\n    prompt_texts.<span class=\"hljs-title function_\">append</span>(f<span class=\"hljs-string\">'&#x3C;|start_header_id|>user&#x3C;|end_header_id|>\\n\\n{user_input.strip()}&#x3C;|eot_id|>&#x3C;|start_header_id|>assistant&#x3C;|end_header_id|>\\n\\n'</span>)\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">''</span>.<span class=\"hljs-title function_\">join</span>(prompt_texts)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n    parser = argparse.<span class=\"hljs-title class_\">ArgumentParser</span>(description=<span class=\"hljs-string\">'Llama3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ `generate()` APIë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ì˜ˆì¸¡'</span>)\n    parser.<span class=\"hljs-title function_\">add_argument</span>(<span class=\"hljs-string\">'--repo-id-or-model-path'</span>, type=str, <span class=\"hljs-keyword\">default</span>=<span class=\"hljs-string\">\"meta-llama/Meta-Llama-3-70B-Instruct\"</span>,\n                        help=<span class=\"hljs-string\">'Meta-Llama-3 (ì˜ˆ: `meta-llama/Meta-Llama-3-70B-Instruct`)ë¥¼ ë‹¤ìš´ë¡œë“œí•  Huggingface ì €ì¥ì†Œ ID'</span>\n                             <span class=\"hljs-string\">'ë˜ëŠ” Huggingface ì²´í¬í¬ì¸íŠ¸ í´ë”ì— ëŒ€í•œ ê²½ë¡œ'</span>)\n    parser.<span class=\"hljs-title function_\">add_argument</span>(<span class=\"hljs-string\">'--prompt'</span>, type=str, <span class=\"hljs-keyword\">default</span>=<span class=\"hljs-string\">\"OpenVINO is\"</span>,\n                        help=<span class=\"hljs-string\">'ì¶”ë¡ í•  í”„ë¡¬í”„íŠ¸'</span>)\n    parser.<span class=\"hljs-title function_\">add_argument</span>(<span class=\"hljs-string\">'--n-predict'</span>, type=int, <span class=\"hljs-keyword\">default</span>=<span class=\"hljs-number\">128</span>,\n                        help=<span class=\"hljs-string\">'ì˜ˆì¸¡í•  ìµœëŒ€ í† í° ìˆ˜'</span>)\n    parser.<span class=\"hljs-title function_\">add_argument</span>(<span class=\"hljs-string\">'--bit'</span>, type=str, <span class=\"hljs-keyword\">default</span>=<span class=\"hljs-string\">\"4\"</span>,\n                        help=<span class=\"hljs-string\">'4ë¡œ ì„¤ì •í•˜ë©´ 4ë¹„íŠ¸ë¡œ ë¡œë“œí•˜ê±°ë‚˜ off ë˜ëŠ” load_in_low_bit ì˜µì…˜ì€ sym_int4, asym_int4, sym_int5, asym_int5, sym_int8,nf3,nf4, fp4, fp8, fp8_e4m3, fp8_e5m2, fp6, gguf_iq2_xxs, gguf_iq2_xs, gguf_iq1_s, gguf_q4k_m, gguf_q4k_s, fp16, bf16, fp6_k'</span>)\n\n    args = parser.<span class=\"hljs-title function_\">parse_args</span>()\n    model_path = args.<span class=\"hljs-property\">repo_id_or_model_path</span>\n\n    <span class=\"hljs-keyword\">if</span> args.<span class=\"hljs-property\">bit</span> == <span class=\"hljs-string\">\"4\"</span>:\n        # <span class=\"hljs-number\">4</span>ë¹„íŠ¸ë¡œ ëª¨ë¸ ë¡œë“œ,\n        # ëª¨ë¸ì˜ ê´€ë ¨ ë ˆì´ì–´ë¥¼ <span class=\"hljs-title class_\">INT4</span> í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n        # iGPUë¥¼ ì‚¬ìš©í•˜ëŠ” <span class=\"hljs-title class_\">Windows</span> ì‚¬ìš©ìì˜ ê²½ìš°, <span class=\"hljs-variable constant_\">LLM</span>ì„ ì‹¤í–‰í•  ë•Œ <span class=\"hljs-string\">`cpu_embedding=True`</span>ë¥¼ from_pretrained í•¨ìˆ˜ì—ì„œ ì„¤ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n        # ì´ë ‡ê²Œ í•˜ë©´ ë©”ëª¨ë¦¬ ì§‘ì•½ì ì¸ ì„ë² ë”© ë ˆì´ì–´ê°€ iGPU ëŒ€ì‹  <span class=\"hljs-variable constant_\">CPU</span>ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°ì—ëŠ” ë„ì›€ì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n        model = <span class=\"hljs-title class_\">AutoModelForCausalLM</span>.<span class=\"hljs-title function_\">from_pretrained</span>(model_path,\n                                                    load_in_4bit=<span class=\"hljs-title class_\">True</span>,\n                                                    optimize_model=<span class=\"hljs-title class_\">True</span>,\n                                                    trust_remote_code=<span class=\"hljs-title class_\">True</span>,\n                                                    use_cache=<span class=\"hljs-title class_\">True</span>)\n    elif args.<span class=\"hljs-property\">bit</span> == <span class=\"hljs-string\">\"off\"</span> or args.<span class=\"hljs-property\">bit</span> == <span class=\"hljs-string\">\"32\"</span>:\n        model = <span class=\"hljs-title class_\">AutoModelForCausalLM</span>.<span class=\"hljs-title function_\">from_pretrained</span>(model_path,\n                                                    optimize_model=<span class=\"hljs-title class_\">True</span>,\n                                                    trust_remote_code=<span class=\"hljs-title class_\">True</span>,\n                                                    use_cache=<span class=\"hljs-title class_\">True</span>)\n    <span class=\"hljs-attr\">else</span>:\n        model = <span class=\"hljs-title class_\">AutoModelForCausalLM</span>.<span class=\"hljs-title function_\">from_pretrained</span>(model_path,\n                                                    load_in_low_bit=args.<span class=\"hljs-property\">bit</span>,\n                                                    optimize_model=<span class=\"hljs-title class_\">True</span>,\n                                                    trust_remote_code=<span class=\"hljs-title class_\">True</span>,\n                                                    use_cache=<span class=\"hljs-title class_\">True</span>)\n\n    <span class=\"hljs-keyword\">if</span> args.<span class=\"hljs-property\">bit</span> == <span class=\"hljs-string\">\"32\"</span>:\n        model = model.<span class=\"hljs-title function_\">to</span>(<span class=\"hljs-string\">'xpu'</span>)\n    <span class=\"hljs-attr\">else</span>:\n        model = model.<span class=\"hljs-title function_\">half</span>().<span class=\"hljs-title function_\">to</span>(<span class=\"hljs-string\">'xpu'</span>)\n\n    # í† í¬ë‚˜ì´ì € ë¡œë“œ\n    tokenizer = <span class=\"hljs-title class_\">AutoTokenizer</span>.<span class=\"hljs-title function_\">from_pretrained</span>(model_path, trust_remote_code=<span class=\"hljs-title class_\">True</span>)\n\n    # ì—¬ê¸°ì„œ ì¢…ê²°ìëŠ” ë‹¤ìŒì„ ì°¸ì¡°í•©ë‹ˆë‹¤. <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct#transformers-automodelforcausallm</span>\n    ì¢…ê²°ì = [\n        tokenizer.<span class=\"hljs-property\">eos_token_id</span>,\n        tokenizer.<span class=\"hljs-title function_\">convert_tokens_to_ids</span>(<span class=\"hljs-string\">\"&#x3C;|eot_id|>\"</span>),\n    ]\n\n    # ì˜ˆì¸¡ëœ í† í° ìƒì„±\n    <span class=\"hljs-keyword\">with</span> torch.<span class=\"hljs-title function_\">inference_mode</span>():\n        prompt = <span class=\"hljs-title function_\">get_prompt</span>(args.<span class=\"hljs-property\">prompt</span>, [], system_prompt=<span class=\"hljs-variable constant_\">DEFAULT_SYSTEM_PROMPT</span>)\n        input_ids = tokenizer.<span class=\"hljs-title function_\">encode</span>(prompt, return_tensors=<span class=\"hljs-string\">\"pt\"</span>).<span class=\"hljs-title function_\">to</span>(<span class=\"hljs-string\">'xpu'</span>)\n        # ipex_llm ëª¨ë¸ì€ ì›Œë°ì—…ì´ í•„ìš”í•˜ë¯€ë¡œ ì¶”ë¡  ì‹œê°„ì„ ì •í™•í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n        output = model.<span class=\"hljs-title function_\">generate</span>(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=<span class=\"hljs-number\">20</span>)\n\n        # ì¶”ë¡  ì‹œì‘\n        st = time.<span class=\"hljs-title function_\">time</span>()\n        output = model.<span class=\"hljs-title function_\">generate</span>(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=args.<span class=\"hljs-property\">n_predict</span>)\n        torch.<span class=\"hljs-property\">xpu</span>.<span class=\"hljs-title function_\">synchronize</span>()\n        end = time.<span class=\"hljs-title function_\">time</span>()\n        output = output.<span class=\"hljs-title function_\">cpu</span>()\n        output_str = tokenizer.<span class=\"hljs-title function_\">decode</span>(output[<span class=\"hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-title class_\">False</span>)\n        <span class=\"hljs-title function_\">print</span>(f<span class=\"hljs-string\">'ì¶”ë¡  ì‹œê°„: {end-st} s, í† í°: {len(output[0])}, t/s:{len(output[0])/(end-st)}'</span>)\n        <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">'-'</span>*<span class=\"hljs-number\">20</span>, <span class=\"hljs-string\">'í”„ë¡¬í”„íŠ¸'</span>, <span class=\"hljs-string\">'-'</span>*<span class=\"hljs-number\">20</span>)\n        <span class=\"hljs-title function_\">print</span>(prompt)\n        <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">'-'</span>*<span class=\"hljs-number\">20</span>, <span class=\"hljs-string\">'ì¶œë ¥ (skip_special_tokens=False)'</span>, <span class=\"hljs-string\">'-'</span>*<span class=\"hljs-number\">20</span>)\n        <span class=\"hljs-title function_\">print</span>(output_str)\n</code></pre>\n<pre><code class=\"hljs language-js\">python ipex-llm-llama3.<span class=\"hljs-property\">py</span> --repo-id-or-model-path=meta-llama/<span class=\"hljs-title class_\">Meta</span>-<span class=\"hljs-title class_\">Llama</span>-<span class=\"hljs-number\">3</span>-8B-<span class=\"hljs-title class_\">Instruct</span> --bit=fp16\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_2.png\" alt=\"image\"></p>\n<p><img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_3.png\" alt=\"image\"></p>\n<p>DirectMLì€ ë‚®ì€ ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ì§€ì›í•˜ëŠ” ONNX Runtimeì˜ Execution Providerê°€ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ í…ŒìŠ¤íŠ¸ëŠ” ì´ python APIë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\">pip install onnxruntime-genai --pre\npip install onnxruntime-genai-directml --pre\npip install torch transformers onnx onnxruntime\nconda install conda-<span class=\"hljs-attr\">forge</span>::vs2015_runtime\n</code></pre>\n<p>ë§ˆì§€ë§‰ ì¤„ì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.</p>\n<p>ë¼ë§ˆ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\">python -m onnxruntime_genai.<span class=\"hljs-property\">models</span>.<span class=\"hljs-property\">builder</span> -m meta-llama/<span class=\"hljs-title class_\">Meta</span>-<span class=\"hljs-title class_\">Llama</span>-<span class=\"hljs-number\">3</span>-8B-<span class=\"hljs-title class_\">Instruct</span> -o llama-<span class=\"hljs-number\">3</span>-8B-<span class=\"hljs-title class_\">Instruct</span>-int4-onnx-directml -p int4 -e dml -c ..\\.<span class=\"hljs-property\">cache</span>\\huggingface\\hub\\\n</code></pre>\n<p>ë³€í™˜ëœ ëª¨ë¸ì´ ğŸ¤— í—ˆë¸Œì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.</p>\n<p>ë‹¤ìŒì€ í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ genai-llama3.pyì…ë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> time\n<span class=\"hljs-keyword\">import</span> argparse\n<span class=\"hljs-keyword\">import</span> onnxruntime_genai <span class=\"hljs-keyword\">as</span> og\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n    parser = argparse.ArgumentParser(description=<span class=\"hljs-string\">'Predict Tokens using onnxruntime_genai'</span>)\n    parser.add_argument(<span class=\"hljs-string\">'--model-path'</span>, <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">str</span>, default=<span class=\"hljs-string\">\"llama-3-8B-Instruct-int4-onnx-directml\"</span>,\n                        <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">'model path'</span>)\n    parser.add_argument(<span class=\"hljs-string\">'--prompt'</span>, <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">str</span>, default=<span class=\"hljs-string\">\"OpenVINO is\"</span>,\n                        <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">'Prompt to infer'</span>)\n    parser.add_argument(\n        <span class=\"hljs-string\">'--max-length'</span>,\n        <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">int</span>,\n        default=<span class=\"hljs-number\">143</span>,\n        <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">'max lengths'</span>\n    )\n    args = parser.parse_args()\n\n    model = og.Model(args.model_path)\n    tokenizer = og.Tokenizer(model)\n    \n    <span class=\"hljs-comment\"># Set the max length to something sensible by default,</span>\n    <span class=\"hljs-comment\"># since otherwise it will be set to the entire context length</span>\n    search_options = {}\n    search_options[<span class=\"hljs-string\">'max_length'</span>] = args.max_length\n\n    chat_template = <span class=\"hljs-string\">'&#x3C;|user|>\\n{input} &#x3C;|end|>\\n&#x3C;|assistant|>'</span>\n\n    text = args.prompt\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> text:\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"ì˜¤ë¥˜, ì…ë ¥ì´ ë¹„ì–´ ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"</span>)\n        exit\n\n    prompt = <span class=\"hljs-string\">f'<span class=\"hljs-subst\">{chat_template.<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-built_in\">input</span>=text)}</span>'</span>\n\n    input_tokens = tokenizer.encode(prompt)\n\n    params = og.GeneratorParams(model)\n    params.set_search_options(**search_options)\n    params.input_ids = input_tokens\n\n    st =  time.time()\n    output = model.generate(params)\n    out_txt = tokenizer.decode(output[<span class=\"hljs-number\">0</span>])\n\n    sec = time.time() - st\n    cnt = <span class=\"hljs-built_in\">len</span>(output[<span class=\"hljs-number\">0</span>])\n\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"ìƒì„± ê²°ê³¼:\"</span>, out_txt)\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f'ì¶”ë¡  ì‹œê°„: <span class=\"hljs-subst\">{sec}</span> ì´ˆ, í† í° ìˆ˜: <span class=\"hljs-subst\">{cnt}</span>, ì´ˆë‹¹ í† í° ìˆ˜:<span class=\"hljs-subst\">{cnt/sec}</span>'</span>)\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_5.png\" alt=\"ì´ë¯¸ì§€\"></p>\n<p>ğŸ¤—Transformers + IPEX-LLMê³¼ ë¹„êµí•´ ë³¼ ìˆ˜ ìˆì–´ìš”.</p>\n<pre><code class=\"hljs language-js\">python ipex-llm-llama3.<span class=\"hljs-property\">py</span> --repo-id-or-model-path=meta-llama/<span class=\"hljs-title class_\">Meta</span>-<span class=\"hljs-title class_\">Llama</span>-<span class=\"hljs-number\">3</span>-8B-<span class=\"hljs-title class_\">Instruct</span> --bit=<span class=\"hljs-number\">4</span>\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_6.png\" alt=\"ì´ë¯¸ì§€\"></p>\n<p><img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_7.png\" alt=\"ì´ë¯¸ì§€1\"></p>\n<p>ê²°ê³¼ë¥¼ í•¨ê»˜ ì‚´í´ë´…ì‹œë‹¤.</p>\n<p><img src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_8.png\" alt=\"ì´ë¯¸ì§€2\"></p>\n<p>ì•ìœ¼ë¡œ ì–´ë–»ê²Œ ì´ëŸ¬í•œ í¥ë¯¸ë¡œìš´ ì°¨ì´ê°€ ìƒê²¼ëŠ”ì§€ì— ëŒ€í•œ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì„ íƒêµ¬í•  ê²ƒì…ë‹ˆë‹¤.</p>\n</body>\n</html>\n"},"__N_SSG":true}