{"pageProps":{"post":{"title":"AWS Glue ETL을 사용하여 DynamoDB에서 Redshift로 데이터 이전하기","description":"","date":"2024-06-19 16:04","slug":"2024-06-19-DynamoDBtoRedshiftusingAWSGlueETL","content":"\n\nAWS Glue ETL을 사용하여 DynamoDB에서 Redshift로의 파이프라인을 구축하는 과정은 여러 단계로 이루어져 있습니다. 아래에 이를 설정하는 자세한 가이드가 있어요:\n\n단계 1: DynamoDB와 Redshift 설정하기\n\n1. DynamoDB:\n이미 하지 않았다면 DynamoDB 테이블을 생성하고 데이터로 채워 넣어주세요.\n\n2.Redshift:\nAmazon Redshift 클러스터를 설정하세요.\nRedshift에 DynamoDB 테이블 구조와 일치하도록 데이터베이스와 테이블을 생성해주세요.\n\n<div class=\"content-ad\"></div>\n\n단계 2: DynamoDB용 AWS Glue 크롤러 생성하기\n\n1. AWS Glue 콘솔을 엽니다.\n2. 새 크롤러를 생성합니다:\n이름: 크롤러에 이름을 지정합니다.\n데이터 원본: DynamoDB를 선택하고 테이블을 선택합니다.\nIAM 역할: 없는 경우 필요한 권한을 가진 IAM 역할을 생성합니다.\n데이터베이스: 기존의 Glue 데이터베이스를 선택하거나 새 데이터베이스를 생성합니다.\n실행 빈도: 요구 사항에 따라 설정합니다.\n\n3. 크롤러 실행:\n이 작업을 통해 Glue 데이터 카탈로그에 DynamoDB 테이블을 위한 테이블이 생성됩니다.\n\n단계 3: AWS Glue ETL 작업 생성하기\n\n<div class=\"content-ad\"></div>\n\n1. AWS Glue 콘솔을 엽니다.\n2. 새 작업을 만듭니다:\n이름: 작업에 이름을 지정합니다.\nIAM 역할: 동일한 역할 또는 필요한 권한이 있는 다른 역할을 사용합니다.\n유형: \"Spark\"를 선택합니다.\nGlue 버전: 적절한 Glue 버전을 선택합니다.\n보안 구성, 스크립트 라이브러리 및 작업 매개변수: 필요에 따라 구성합니다.\n작업 북마킹: 처리된 데이터를 추적하려면 활성화합니다.\n\n3. 스크립트 편집기:\nGlue Studio 시각적 편집기를 사용하거나 스크립트를 직접 편집할 수 있습니다.\n\n단계 4: ETL 스크립트 작성\n\n다음은 다이나모DB에서 Redshift로 데이터를 이동하는 예제 스크립트입니다:\n\n<div class=\"content-ad\"></div>\n\n```python\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\n# Glue 컨텍스트 초기화\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# DynamoDB 테이블을 소스로 정의\ndynamodb_table = \"dynamodb_table_name\"\ndatasource0 = glueContext.create_dynamic_frame.from_catalog(\n    database=\"your_glue_database\",\n    table_name=dynamodb_table\n)\n\n# 필요한 변환 작업 적용\napplymapping1 = ApplyMapping.apply(\n    frame=datasource0,\n    mappings=[\n        (\"dynamodb_column1\", \"string\", \"redshift_column1\", \"string\"),\n        (\"dynamodb_column2\", \"number\", \"redshift_column2\", \"int\"),\n        # 필요한 경우 더 많은 열 추가\n    ]\n)\n```  \n\n<div class=\"content-ad\"></div>\n\n# Redshift 연결 옵션 정의하기\nredshift_tmp_dir = “s3://your-bucket/temp-dir/”\nredshift_connection_options = '\n“url”: “jdbc:redshift://your-redshift-cluster:5439/your_database”,\n“user”: “your_redshift_user”,\n“password”: “your_redshift_password”,\n“dbtable”: “your_redshift_table”,\n“redshiftTmpDir”: redshift_tmp_dir\n'\n\n# Redshift로 쓰기\ndatasink4 = glueContext.write_dynamic_frame.from_jdbc_conf(\nframe = applymapping1,\ncatalog_connection = “your_redshift_connection”,\nconnection_options = redshift_connection_options\n)\n\n# 작업 완료하기\njob.commit()\n\n단계 5: Glue Job 실행 및 예약하기\n\n<div class=\"content-ad\"></div>\n\n1. 작업 실행\nAWS Glue 콘솔에서 작업을 수동으로 실행할 수 있습니다.\n2. 작업 일정 설정:\nGlue 트리거를 사용하여 특정 간격에 작업을 실행할 수 있는 일정을 설정하세요.\n\n단계 6: ETL 작업 모니터링 및 관리\n\n1. 모니터링:\nAWS Glue의 모니터링 기능을 사용하여 작업 실행, 오류 및 성능을 추적하세요.\n2. 오류 처리:\n필요한 경우 스크립트에 오류 처리를 구현하세요.\n3. 최적화:\n특히 대규모 데이터 집합을 처리할 때 성능을 위해 ETL 작업을 최적화하세요.\n\n이러한 단계를 따르면 AWS Glue ETL 작업을 사용하여 DynamoDB에서 Redshift로 데이터를 전송하는 파이프라인을 설정할 수 있습니다. 감사합니다.","ogImage":{"url":"/assets/img/2024-06-19-DynamoDBtoRedshiftusingAWSGlueETL_0.png"},"coverImage":"/assets/img/2024-06-19-DynamoDBtoRedshiftusingAWSGlueETL_0.png","tag":["Tech"],"readingTime":3},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>AWS Glue ETL을 사용하여 DynamoDB에서 Redshift로의 파이프라인을 구축하는 과정은 여러 단계로 이루어져 있습니다. 아래에 이를 설정하는 자세한 가이드가 있어요:</p>\n<p>단계 1: DynamoDB와 Redshift 설정하기</p>\n<ol>\n<li>DynamoDB:\n이미 하지 않았다면 DynamoDB 테이블을 생성하고 데이터로 채워 넣어주세요.</li>\n</ol>\n<p>2.Redshift:\nAmazon Redshift 클러스터를 설정하세요.\nRedshift에 DynamoDB 테이블 구조와 일치하도록 데이터베이스와 테이블을 생성해주세요.</p>\n<p>단계 2: DynamoDB용 AWS Glue 크롤러 생성하기</p>\n<ol>\n<li>\n<p>AWS Glue 콘솔을 엽니다.</p>\n</li>\n<li>\n<p>새 크롤러를 생성합니다:\n이름: 크롤러에 이름을 지정합니다.\n데이터 원본: DynamoDB를 선택하고 테이블을 선택합니다.\nIAM 역할: 없는 경우 필요한 권한을 가진 IAM 역할을 생성합니다.\n데이터베이스: 기존의 Glue 데이터베이스를 선택하거나 새 데이터베이스를 생성합니다.\n실행 빈도: 요구 사항에 따라 설정합니다.</p>\n</li>\n<li>\n<p>크롤러 실행:\n이 작업을 통해 Glue 데이터 카탈로그에 DynamoDB 테이블을 위한 테이블이 생성됩니다.</p>\n</li>\n</ol>\n<p>단계 3: AWS Glue ETL 작업 생성하기</p>\n<ol>\n<li>\n<p>AWS Glue 콘솔을 엽니다.</p>\n</li>\n<li>\n<p>새 작업을 만듭니다:\n이름: 작업에 이름을 지정합니다.\nIAM 역할: 동일한 역할 또는 필요한 권한이 있는 다른 역할을 사용합니다.\n유형: \"Spark\"를 선택합니다.\nGlue 버전: 적절한 Glue 버전을 선택합니다.\n보안 구성, 스크립트 라이브러리 및 작업 매개변수: 필요에 따라 구성합니다.\n작업 북마킹: 처리된 데이터를 추적하려면 활성화합니다.</p>\n</li>\n<li>\n<p>스크립트 편집기:\nGlue Studio 시각적 편집기를 사용하거나 스크립트를 직접 편집할 수 있습니다.</p>\n</li>\n</ol>\n<p>단계 4: ETL 스크립트 작성</p>\n<p>다음은 다이나모DB에서 Redshift로 데이터를 이동하는 예제 스크립트입니다:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> sys\n<span class=\"hljs-keyword\">from</span> awsglue.transforms <span class=\"hljs-keyword\">import</span> *\n<span class=\"hljs-keyword\">from</span> awsglue.utils <span class=\"hljs-keyword\">import</span> getResolvedOptions\n<span class=\"hljs-keyword\">from</span> pyspark.context <span class=\"hljs-keyword\">import</span> SparkContext\n<span class=\"hljs-keyword\">from</span> awsglue.context <span class=\"hljs-keyword\">import</span> GlueContext\n<span class=\"hljs-keyword\">from</span> awsglue.job <span class=\"hljs-keyword\">import</span> Job\n\n<span class=\"hljs-comment\"># Glue 컨텍스트 초기화</span>\nargs = getResolvedOptions(sys.argv, [<span class=\"hljs-string\">'JOB_NAME'</span>])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args[<span class=\"hljs-string\">'JOB_NAME'</span>], args)\n\n<span class=\"hljs-comment\"># DynamoDB 테이블을 소스로 정의</span>\ndynamodb_table = <span class=\"hljs-string\">\"dynamodb_table_name\"</span>\ndatasource0 = glueContext.create_dynamic_frame.from_catalog(\n    database=<span class=\"hljs-string\">\"your_glue_database\"</span>,\n    table_name=dynamodb_table\n)\n\n<span class=\"hljs-comment\"># 필요한 변환 작업 적용</span>\napplymapping1 = ApplyMapping.apply(\n    frame=datasource0,\n    mappings=[\n        (<span class=\"hljs-string\">\"dynamodb_column1\"</span>, <span class=\"hljs-string\">\"string\"</span>, <span class=\"hljs-string\">\"redshift_column1\"</span>, <span class=\"hljs-string\">\"string\"</span>),\n        (<span class=\"hljs-string\">\"dynamodb_column2\"</span>, <span class=\"hljs-string\">\"number\"</span>, <span class=\"hljs-string\">\"redshift_column2\"</span>, <span class=\"hljs-string\">\"int\"</span>),\n        <span class=\"hljs-comment\"># 필요한 경우 더 많은 열 추가</span>\n    ]\n)\n</code></pre>\n<h1>Redshift 연결 옵션 정의하기</h1>\n<p>redshift_tmp_dir = “s3://your-bucket/temp-dir/”\nredshift_connection_options = '\n“url”: “jdbc:redshift://your-redshift-cluster:5439/your_database”,\n“user”: “your_redshift_user”,\n“password”: “your_redshift_password”,\n“dbtable”: “your_redshift_table”,\n“redshiftTmpDir”: redshift_tmp_dir\n'</p>\n<h1>Redshift로 쓰기</h1>\n<p>datasink4 = glueContext.write_dynamic_frame.from_jdbc_conf(\nframe = applymapping1,\ncatalog_connection = “your_redshift_connection”,\nconnection_options = redshift_connection_options\n)</p>\n<h1>작업 완료하기</h1>\n<p>job.commit()</p>\n<p>단계 5: Glue Job 실행 및 예약하기</p>\n<ol>\n<li>작업 실행\nAWS Glue 콘솔에서 작업을 수동으로 실행할 수 있습니다.</li>\n<li>작업 일정 설정:\nGlue 트리거를 사용하여 특정 간격에 작업을 실행할 수 있는 일정을 설정하세요.</li>\n</ol>\n<p>단계 6: ETL 작업 모니터링 및 관리</p>\n<ol>\n<li>모니터링:\nAWS Glue의 모니터링 기능을 사용하여 작업 실행, 오류 및 성능을 추적하세요.</li>\n<li>오류 처리:\n필요한 경우 스크립트에 오류 처리를 구현하세요.</li>\n<li>최적화:\n특히 대규모 데이터 집합을 처리할 때 성능을 위해 ETL 작업을 최적화하세요.</li>\n</ol>\n<p>이러한 단계를 따르면 AWS Glue ETL 작업을 사용하여 DynamoDB에서 Redshift로 데이터를 전송하는 파이프라인을 설정할 수 있습니다. 감사합니다.</p>\n</body>\n</html>\n"},"__N_SSG":true}