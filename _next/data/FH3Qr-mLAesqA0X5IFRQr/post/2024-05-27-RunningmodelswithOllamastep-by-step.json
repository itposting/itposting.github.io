{"pageProps":{"post":{"title":"Ollama를 사용하여 모델 실행하기 단계별 안내","description":"","date":"2024-05-27 14:51","slug":"2024-05-27-RunningmodelswithOllamastep-by-step","content":"\n\nLLM을 빠르게 테스트할 수 있는 방법을 찾고 계신가요? 전체 인프라를 설정할 필요 없이 테스트할 수 있는 방법이 있다면 정말 훌륭하죠. 이 짧은 기사에서 우리가 할 일이 바로 그거에요.\n\n![이미지](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png)\n\nOllama에 관해 경험이 있는 경우에는 특정 단락으로 이동해도 됩니다. 이 기사에서 찾을 수 있는 내용은 다음과 같아요:\n\n- Ollama가 무엇인가요?\n- Windows에 Ollama 설치하기.\n- Ollama [cmd] 실행하기.\n- 로컬로 모델 다운로드하기.\n- 다양한 용도에 맞는 다른 모델.\n- 모델 실행하기 [cmd].\n- CPU에 친화적인 양자화된 모델.\n- 다른 소스에서 모델 통합하기.\n- Ollama-파워드 (Python) 앱으로 개발자들의 삶을 더 쉽게 만들기.\n- 요약.\n\n<div class=\"content-ad\"></div>\n\n# 1. Ollama이란?\n\nOllama는 오픈 소스 코드로, 로컬에서 또는 본인의 서버에서 언어 모델과의 원활한 통합을 가능하게 하는 사용 준비 도구입니다. 이를 통해 상업용 API의 유료 버전을 사용하지 않아도 되므로, 특히 이제 Meta가 Llama2 모델을 상용으로 사용 가능하게 한 것을 고려하면, 자신의 데이터셋에서 추가 학습에 적합합니다.\n\n➡️ GitHub 저장소: https://github.com/ollama/ollama\n\n➡️ Ollama 공식 웹페이지: https://ollama.com\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_1.png)\n\n# 2. Windows에서 Ollama 설치하기\n\nOllama는 Windows, Mac 및 Linux에서도 원활하게 작동합니다. 이 간단한 자습서는 특히 Windows 10용 설치 단계를 안내합니다. 설치 후 프로그램은 약 384MB를 차지합니다. 그러나 다운로드한 모델이 가벼운 것은 아닐 수 있습니다.\n\n만약 도커 컨테이너에서 Ollama를 실행하길 원한다면, 아래 설명을 건너뛰고 \n\n감십시오.\n\n<div class=\"content-ad\"></div>\n\n➡️ https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image\n\n![Running Models with Ollama](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_2.png)\n\n➡️ Ollama 홈페이지로 이동하여 .exe 파일을 다운로드하세요: https://ollama.com\n\n![Running Models with Ollama](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_3.png)\n\n<div class=\"content-ad\"></div>\n\nOllama를 다운로드하고 Windows에 설치하세요. 보통 다음 경로에 위치한 기본 모델 저장 경로를 사용할 수 있습니다:\n\n```js\nC:\\Users\\your_user\\.ollama\n```\n\n그러나 C: 파티션에 공간이 제한적이라면, 대안 디렉토리로 전환하는 것이 권장됩니다. D:\\와 같은 다른 파티션이 있는 경우, 간단하게:\n\n- 데스크탑의 컴퓨터 아이콘을 마우스 오른쪽 클릭합니다.\n- 속성을 선택한 후 \"고급 시스템 설정\"으로 이동합니다.\n- 환경 변수를 클릭합니다.\n- ...을 위한 사용자 변수에서 모델을 저장할 디렉토리의 절대 경로를 삽입하십시오. 예를 들면:\n\n<div class=\"content-ad\"></div>\n\n```js\n변수: OLLAMA_MODELS\n값: D:\\your_directory\\models\n```\n\nOLLAMA_MODELS 변수의 이름을 변경하지 마십시오. 이 변수는 Ollama가 정확히 아래와 같이 검색할 것입니다.\n\nWindows의 하단 표시줄에 Ollama 아이콘이 나타납니다. 프로그램이 시작되지 않으면 Windows 프로그램에서 찾아서 거기서 시작하십시오.\n\n<img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_4.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n이제 Ollama를 실행하고 모델을 다운로드할 준비가 되었어요 :)\n\n# 3. Ollama 실행하기 [cmd]\n\n![image](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_5.png)\n\nOllama를 설정하고 나면 윈도우에서 cmd(명령줄)를 열고 로컬로 일부 모델을 다운로드할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\nOllama 로컬 대시보드를 사용하려면 웹 브라우저에서 다음 URL을 입력하세요:\n\n```js\nhttp://localhost:11434/api/\n```\n\nOllama를 실행하는 것은 그렇게 어렵지 않습니다. 나중에 CMD 및 Python 코드를 통해 어떻게 활용하는지 알아보겠습니다.\n\n중요한 몇 가지 명령어:\n\n<div class=\"content-ad\"></div>\n\n로컬로 사용 가능한 모델을 확인하려면 다음을 cmd에 입력하세요:\n\n```js\nollama list\n```\n\n특정 모델에 해당하는 SHA 파일을 확인하려면 cmd에 입력하세요 (예: llama2:7b 모델 확인을 위한 예시):\n\n```js\nollama show --modelfile llama2:7b\n```\n\n<div class=\"content-ad\"></div>\n\n모델을 제거하려면:\n\n```js\nollama rm llama2:7b\n```\n\n모델을 서버에 올리려면:\n\n```js\nollama serve\n```\n\n<div class=\"content-ad\"></div>\n\n# 4. 모델을 로컬로 다운로드하기\n\n웹사이트 ➡️ https://ollama.com/library 에서는 여러 다양한 파라미터 크기로 제공되는 다수의 모델을 다운로드할 수 있습니다.\n\n로컬로 모델을 다운로드하기 전에, 해당 모델을 로딩할 충분한 메모리를 가지고 있는지 확인해주세요. 테스트할 때는 애플리케이션에 통합하기에 적합한 작은 모델인 '7B'로 레이블이 지정된 모델을 사용하는 것이 좋습니다.\n\n⚠️ 부드러운 모델 작동을 위해 적어도 하나의 GPU를 보유하는 것이 강력하게 권장됩니다.\n\n<div class=\"content-ad\"></div>\n\n아래에는 내가 테스트하고 추천하는 여러 모델이 있습니다. 명령을 복사하여 명령 프롬프트에 붙여넣어 지정된 모델을 로컬로 가져올 수 있습니다.\n\n👉Meta에서의 Llama2 모델\n\n대화 시나리오를 위해 최적화된 생성 텍스트 모델 세트입니다. Ollama의 많은 모델과 마찬가지로 Llama2는 다양한 구성으로 제공됩니다:\n\n![image](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_6.png)\n\n<div class=\"content-ad\"></div>\n\n아래는 해당 모델을 가져오는 몇 가지 예시입니다:\n\n표준 모델:\n\n```js\nollama pull llama2\n```\n\n검열되지 않은 버전:\n\n<div class=\"content-ad\"></div>\n\n```js\nollama pull llama2-비겁하지 않은:7b\n```\n\n채팅 7B 모델:\n\n```js\nollama pull llama2:7b-채팅\n```\n\n➡️ 더 읽기: https://llama.meta.com/llama2\n\n\n<div class=\"content-ad\"></div>\n\n👉 구글의 젬마\n\n주요 7B 크기 모델과 유사한 견고한 성능을 제공하는 오픈 소스 모델입니다.\n\n```js\nollama pull gemma:7b\n```\n\n➡️ 자세히 보기: https://blog.google/technology/developers/gemma-open-models/\n\n<div class=\"content-ad\"></div>\n\n👉 Haotian Liu 등의 LLava.\n\n이미지에서 텍스트 설명을 다루는 데 뛰어나며 시각 및 언어 모델 모두에 대한 견고한 지원을 제공하는 멀티모달 모델입니다.\n\n```js\nollama pull llava\n```\n\n➡️ 자세히 알아보기: https://llava-vl.github.io/\n\n<div class=\"content-ad\"></div>\n\n5. 서로 다른 목적을 위한 다양한 모델\n\n![이미지](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_7.png)\n\n일부 모델은 특정 데이터셋에서 훈련되어 코드 완성, 대화 또는 이미지에서 텍스트로 변환과 같은 특정 작업에 더 적합합니다. Ollama에서는 다양한 목적을 위해 설계된 모델을 찾을 수 있습니다.\n\n첫 번째 그룹은 대화, 텍스트 완성, 요약 등을 용이하게 하는 데 초점을 맞춘 모델을 포함하고 있습니다. Gemma, Llama2, Falcon 또는 OpenChat과 같은 모델이 포함됩니다.\n\n<div class=\"content-ad\"></div>\n\n일부 예시:\n\n- [Falcon](https://ollama.com/library/falcon)\n\n- [Gemma](https://ollama.com/library/gemma)\n\n- [Openchat](https://ollama.com/library/openchat)\n\n<div class=\"content-ad\"></div>\n\n다음 그룹은 대화를 나누거나 챗봇 역할을 하는 다중 모달 모델과 이미지 설명(시각 모델), 텍스트 요약, 질문-답변(Q/A) 애플리케이션을 구동할 수 있는 모델들로 구성됩니다.\n\n일부 예시:\n\n➡️ https://ollama.com/library/llava\n\n➡️ https://ollama.com/library/bakllava\n\n<div class=\"content-ad\"></div>\n\n마지막으로, 매우 전문화된 그룹은 Ollama에서 이용 가능한 모델을 활용하여 개발자의 작업을 지원합니다. 코델라마, 돌핀-미스트랄, 돌핀-믹스트랄(코딩 작업에 능숙한 Mixtral 전문가 모델을 기반으로 세밀하게 조정된 모델)과 같은 모델들이 있으며, 계속해서 크리에이터들이 추가하고 있습니다.\n\n몇 가지 예시:\n\n➡️ https://ollama.com/library/codellama\n\n➡️ https://ollama.com/library/dolphin-mistral\n\n<div class=\"content-ad\"></div>\n\n➡️ https://ollama.com/library/dolphin-mixtral\n\n# 6. 모델 실행하기 [cmd]\n\n다운로드한 모델을 실행하려면, ollama run 모델이름:파라미터 \"당신의 프롬프트\"를 입력하세요. 예를 들어:\n\n```js\nollama run llama2:7b \"당신의 프롬프트\"\n```\n\n<div class=\"content-ad\"></div>\n\n다중 모달 모델을 사용하면 기본 프롬프트를 벗어난 파일, 로컬 이미지 경로 등을 포함할 수 있어 더 많은 기능을 확장할 수 있습니다.\n\n# 6. CPU 친화적 양자화 모델\n\n양자화는 모델의 정밀도를 유지하는 비용을 줄이는 것으로 관련 비용을 줄이는 것입니다. 이 과정 뒤에 숨은 직관력을 구축하는 데 도움이 되는 이 크고 훌륭한 기사에서 자세한 설명을 찾아볼 수 있습니다:\n\n📃 양자화 LLMs란 무엇인가? (Miguel Carreira Neves의 글):\n\n<div class=\"content-ad\"></div>\n\n➡️ https://www.tensorops.ai/post/what-are-quantized-llms\n\n추가 자료:\n\n📃 Extreme Compression of Large Language Models via Additive Quantization:\n\n➡️ https://arxiv.org/html/2401.06118v2\n\n<div class=\"content-ad\"></div>\n\n📃 SmoothQuant: 대형 언어 모델을 위한 정확하고 효율적인 사후 훈련 양자화:\n\n➡️ [논문 링크](https://arxiv.org/pdf/2211.10438.pdf)\n\n📃 BiLLM: LLMs를 위한 사후 훈련 양자화 한계 돌파:\n\n➡️ [논문 링크](https://arxiv.org/pdf/2402.04291.pdf)\n\n<div class=\"content-ad\"></div>\n\n간단하게 말하면, 양자화는 가중치 정밀도를 조정하여 모델 크기를 줄이고 중요한 정확도 하락 없이 성능을 쉽게 감소시킬 수 있는 하드웨어에서 실행할 수 있게 해줍니다.\n\n이 글과 함께 제공된 이미지를 통해 양자화 후에 모델이 원래 버전보다 상당히 적은 공간을 차지하는 것을 확인할 수 있습니다:\n\n![Quantized Models](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_8.png)\n\nOllama는 양자화된 모델을 지원하여 별도로 처리하는 부담을 덜어줍니다.\n\n<div class=\"content-ad\"></div>\n\n# 7. 다른 소스에서 모델 통합하기\n\n![Running Models With Ollama Step-by-Step](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_9.png)\n\nOllama의 모델은 다양성을 제공하지만 현재 모든 모델에 액세스할 수 있는 것은 아닙니다. 그러나 로컬에 직접 모델을 통합하는 것은 간단한 프로세스입니다. 새로운 모델을 지역 Ollama에 통합하는 방법을 알아봅시다.\n\nThe Bloke의 HuggingFace 계정에서 많은 양자화된 모델을 사용할 수 있습니다. 의학 논문을 위해서 우리는 편리하게 medicine-chat-GGUF 모델을 선택할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n➡️ https://huggingface.co/TheBloke/medicine-chat-GGUF\n\n해당 링크를 열고 파일 및 버전을 클릭하세요.\n\n![Files and versions](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_10.png)\n\nOllama 모델에 포함하고 싶은 모델을 다운로드하세요:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_11.png\" />\n\nModelfile이라는 빈 파일을 생성하고 아래 지정된 데이터를 삽입하세요 (저장된 모델의 절대 경로로 경로를 대체하십시오). 이 예제는 기본적이며, 모델의 온도, 시스템 메시지 등과 같은 여러 옵션을 포함하여 확장될 수 있습니다. 필요한 경우 파일에서 '#'를 제거하여 해당 옵션을 활성화하세요.\n\n```js\nFROM D:\\...\\medicine-chat.Q4_0.gguf\n# PARAMETER 온도 0.6\n# SYSTEM \"\"\"도움이 되는 의학 조수입니다.\"\"\"\n```\n\nModelfile을 저장한 후, cmd에 다음을 입력하세요:\n\n<div class=\"content-ad\"></div>\n\n```bash\nollama create 모델_이름 -f 모델_파일\n```\n\n# 9. Ollama를 활용한 (Python) 앱으로 개발자의 삶을 더 쉽게 만들기\n\n백그라운드에서 실행되는 Ollama는 일반적인 REST API와 같이 접근할 수 있습니다. 따라서 requests와 같은 라이브러리 또는 조금 더 발전된 FastAPI, Flask 또는 Django와 같은 프레임워크를 사용하여 응용 프로그램에 쉽게 통합할 수 있습니다.\n\nOllama python 패키지를 쉽게 pip를 통해 설치하세요.\n\n<div class=\"content-ad\"></div>\n\n⬆️ https://pypi.org/project/ollama/0.1.3:\n\n```js\npip install ollama\n```\n\nPython 코드를 통해 임베딩을 생성하는 방법:\n\n```js\nimport ollama\n\nembedding = ollama.embeddings(model=\"llama2:7b\", prompt=\"Hello Ollama!\")\n```\n\n<div class=\"content-ad\"></div>\n\n간단히 CURL을 사용하여:\n\n```js\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"llama2:7b\",\n  \"prompt\": \"Here is an article about llamas...\"\n}'\n```\n\nOllama 엔드포인트에 대해 더 알아보려면 다음 링크를 방문해주세요:\n\n➡️ https://github.com/ollama/ollama/blob/main/docs/api.md\n\n<div class=\"content-ad\"></div>\n\nOllama가 Langchain 프레임워크에 원활하게 통합되어 개발 노력을 최적화하고 기술 측면의 작업을 더욱 간편하게 만들었습니다:\n\n➡️ https://python.langchain.com/docs/integrations/llms/ollama\n\n임베딩을 만드는 간단함을 감상해보세요:\n\n```js\n# pip install langchain_community\nfrom langchain_community.embeddings import OllamaEmbeddings\n\n\nembed = OllamaEmbeddings(model=\"llama2:7b\")\nembedding = embed.embed_query(\"Hello Ollama!\")\n```\n\n<div class=\"content-ad\"></div>\n\n# 10. 요약\n\n본 기사는 당신을 Ollama를 사용하여 모델을 실행하는 과정을 단계별로 안내하여, 전체 인프라 구성 없이 LLM을 테스트할 수 있는 원활한 방법을 제공합니다.\n\n올라마는 오픈 소스 도구로, Meta의 Llama2 모델을 무료로 사용할 수 있게 해주는 로컬 또는 서버 기반의 언어 모델 통합을 용이하게 합니다. 윈도우에서의 설치 과정과 명령줄을 통해 Ollama를 실행하는 방법에 대해 설명되어 있습니다.\n\n이 기사에서는 모델 다운로드, 특정 작업을 위한 다양한 모델 옵션, 다양한 명령어를 사용하여 모델 실행, CPU 친화적인 양자화된 모델, 그리고 외부 모델 통합에 대해 탐구합니다. 또한, 개발자들을 위해 Ollama를 활용한 파이썬 애플리케이션을 강조하고 있습니다.","ogImage":{"url":"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png"},"coverImage":"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png","tag":["Tech"],"readingTime":10},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>LLM을 빠르게 테스트할 수 있는 방법을 찾고 계신가요? 전체 인프라를 설정할 필요 없이 테스트할 수 있는 방법이 있다면 정말 훌륭하죠. 이 짧은 기사에서 우리가 할 일이 바로 그거에요.</p>\n<p><img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png\" alt=\"이미지\"></p>\n<p>Ollama에 관해 경험이 있는 경우에는 특정 단락으로 이동해도 됩니다. 이 기사에서 찾을 수 있는 내용은 다음과 같아요:</p>\n<ul>\n<li>Ollama가 무엇인가요?</li>\n<li>Windows에 Ollama 설치하기.</li>\n<li>Ollama [cmd] 실행하기.</li>\n<li>로컬로 모델 다운로드하기.</li>\n<li>다양한 용도에 맞는 다른 모델.</li>\n<li>모델 실행하기 [cmd].</li>\n<li>CPU에 친화적인 양자화된 모델.</li>\n<li>다른 소스에서 모델 통합하기.</li>\n<li>Ollama-파워드 (Python) 앱으로 개발자들의 삶을 더 쉽게 만들기.</li>\n<li>요약.</li>\n</ul>\n<h1>1. Ollama이란?</h1>\n<p>Ollama는 오픈 소스 코드로, 로컬에서 또는 본인의 서버에서 언어 모델과의 원활한 통합을 가능하게 하는 사용 준비 도구입니다. 이를 통해 상업용 API의 유료 버전을 사용하지 않아도 되므로, 특히 이제 Meta가 Llama2 모델을 상용으로 사용 가능하게 한 것을 고려하면, 자신의 데이터셋에서 추가 학습에 적합합니다.</p>\n<p>➡️ GitHub 저장소: <a href=\"https://github.com/ollama/ollama\" rel=\"nofollow\" target=\"_blank\">https://github.com/ollama/ollama</a></p>\n<p>➡️ Ollama 공식 웹페이지: <a href=\"https://ollama.com\" rel=\"nofollow\" target=\"_blank\">https://ollama.com</a></p>\n<p><img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_1.png\" alt=\"이미지\"></p>\n<h1>2. Windows에서 Ollama 설치하기</h1>\n<p>Ollama는 Windows, Mac 및 Linux에서도 원활하게 작동합니다. 이 간단한 자습서는 특히 Windows 10용 설치 단계를 안내합니다. 설치 후 프로그램은 약 384MB를 차지합니다. 그러나 다운로드한 모델이 가벼운 것은 아닐 수 있습니다.</p>\n<p>만약 도커 컨테이너에서 Ollama를 실행하길 원한다면, 아래 설명을 건너뛰고</p>\n<p>감십시오.</p>\n<p>➡️ <a href=\"https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image\" rel=\"nofollow\" target=\"_blank\">https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image</a></p>\n<p><img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_2.png\" alt=\"Running Models with Ollama\"></p>\n<p>➡️ Ollama 홈페이지로 이동하여 .exe 파일을 다운로드하세요: <a href=\"https://ollama.com\" rel=\"nofollow\" target=\"_blank\">https://ollama.com</a></p>\n<p><img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_3.png\" alt=\"Running Models with Ollama\"></p>\n<p>Ollama를 다운로드하고 Windows에 설치하세요. 보통 다음 경로에 위치한 기본 모델 저장 경로를 사용할 수 있습니다:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-attr\">C</span>:\\<span class=\"hljs-title class_\">Users</span>\\your_user\\.<span class=\"hljs-property\">ollama</span>\n</code></pre>\n<p>그러나 C: 파티션에 공간이 제한적이라면, 대안 디렉토리로 전환하는 것이 권장됩니다. D:\\와 같은 다른 파티션이 있는 경우, 간단하게:</p>\n<ul>\n<li>데스크탑의 컴퓨터 아이콘을 마우스 오른쪽 클릭합니다.</li>\n<li>속성을 선택한 후 \"고급 시스템 설정\"으로 이동합니다.</li>\n<li>환경 변수를 클릭합니다.</li>\n<li>...을 위한 사용자 변수에서 모델을 저장할 디렉토리의 절대 경로를 삽입하십시오. 예를 들면:</li>\n</ul>\n<pre><code class=\"hljs language-js\">변수: <span class=\"hljs-variable constant_\">OLLAMA_MODELS</span>\n값: <span class=\"hljs-attr\">D</span>:\\your_directory\\models\n</code></pre>\n<p>OLLAMA_MODELS 변수의 이름을 변경하지 마십시오. 이 변수는 Ollama가 정확히 아래와 같이 검색할 것입니다.</p>\n<p>Windows의 하단 표시줄에 Ollama 아이콘이 나타납니다. 프로그램이 시작되지 않으면 Windows 프로그램에서 찾아서 거기서 시작하십시오.</p>\n<p>이제 Ollama를 실행하고 모델을 다운로드할 준비가 되었어요 :)</p>\n<h1>3. Ollama 실행하기 [cmd]</h1>\n<p><img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_5.png\" alt=\"image\"></p>\n<p>Ollama를 설정하고 나면 윈도우에서 cmd(명령줄)를 열고 로컬로 일부 모델을 다운로드할 수 있어요.</p>\n<p>Ollama 로컬 대시보드를 사용하려면 웹 브라우저에서 다음 URL을 입력하세요:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-attr\">http</span>:<span class=\"hljs-comment\">//localhost:11434/api/</span>\n</code></pre>\n<p>Ollama를 실행하는 것은 그렇게 어렵지 않습니다. 나중에 CMD 및 Python 코드를 통해 어떻게 활용하는지 알아보겠습니다.</p>\n<p>중요한 몇 가지 명령어:</p>\n<p>로컬로 사용 가능한 모델을 확인하려면 다음을 cmd에 입력하세요:</p>\n<pre><code class=\"hljs language-js\">ollama list\n</code></pre>\n<p>특정 모델에 해당하는 SHA 파일을 확인하려면 cmd에 입력하세요 (예: llama2:7b 모델 확인을 위한 예시):</p>\n<pre><code class=\"hljs language-js\">ollama show --modelfile <span class=\"hljs-attr\">llama2</span>:7b\n</code></pre>\n<p>모델을 제거하려면:</p>\n<pre><code class=\"hljs language-js\">ollama rm <span class=\"hljs-attr\">llama2</span>:7b\n</code></pre>\n<p>모델을 서버에 올리려면:</p>\n<pre><code class=\"hljs language-js\">ollama serve\n</code></pre>\n<h1>4. 모델을 로컬로 다운로드하기</h1>\n<p>웹사이트 ➡️ <a href=\"https://ollama.com/library\" rel=\"nofollow\" target=\"_blank\">https://ollama.com/library</a> 에서는 여러 다양한 파라미터 크기로 제공되는 다수의 모델을 다운로드할 수 있습니다.</p>\n<p>로컬로 모델을 다운로드하기 전에, 해당 모델을 로딩할 충분한 메모리를 가지고 있는지 확인해주세요. 테스트할 때는 애플리케이션에 통합하기에 적합한 작은 모델인 '7B'로 레이블이 지정된 모델을 사용하는 것이 좋습니다.</p>\n<p>⚠️ 부드러운 모델 작동을 위해 적어도 하나의 GPU를 보유하는 것이 강력하게 권장됩니다.</p>\n<p>아래에는 내가 테스트하고 추천하는 여러 모델이 있습니다. 명령을 복사하여 명령 프롬프트에 붙여넣어 지정된 모델을 로컬로 가져올 수 있습니다.</p>\n<p>👉Meta에서의 Llama2 모델</p>\n<p>대화 시나리오를 위해 최적화된 생성 텍스트 모델 세트입니다. Ollama의 많은 모델과 마찬가지로 Llama2는 다양한 구성으로 제공됩니다:</p>\n<p><img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_6.png\" alt=\"image\"></p>\n<p>아래는 해당 모델을 가져오는 몇 가지 예시입니다:</p>\n<p>표준 모델:</p>\n<pre><code class=\"hljs language-js\">ollama pull llama2\n</code></pre>\n<p>검열되지 않은 버전:</p>\n<pre><code class=\"hljs language-js\">ollama pull llama2-비겁하지 않은:7b\n</code></pre>\n<p>채팅 7B 모델:</p>\n<pre><code class=\"hljs language-js\">ollama pull <span class=\"hljs-attr\">llama2</span>:7b-채팅\n</code></pre>\n<p>➡️ 더 읽기: <a href=\"https://llama.meta.com/llama2\" rel=\"nofollow\" target=\"_blank\">https://llama.meta.com/llama2</a></p>\n<p>👉 구글의 젬마</p>\n<p>주요 7B 크기 모델과 유사한 견고한 성능을 제공하는 오픈 소스 모델입니다.</p>\n<pre><code class=\"hljs language-js\">ollama pull <span class=\"hljs-attr\">gemma</span>:7b\n</code></pre>\n<p>➡️ 자세히 보기: <a href=\"https://blog.google/technology/developers/gemma-open-models/\" rel=\"nofollow\" target=\"_blank\">https://blog.google/technology/developers/gemma-open-models/</a></p>\n<p>👉 Haotian Liu 등의 LLava.</p>\n<p>이미지에서 텍스트 설명을 다루는 데 뛰어나며 시각 및 언어 모델 모두에 대한 견고한 지원을 제공하는 멀티모달 모델입니다.</p>\n<pre><code class=\"hljs language-js\">ollama pull llava\n</code></pre>\n<p>➡️ 자세히 알아보기: <a href=\"https://llava-vl.github.io/\" rel=\"nofollow\" target=\"_blank\">https://llava-vl.github.io/</a></p>\n<ol start=\"5\">\n<li>서로 다른 목적을 위한 다양한 모델</li>\n</ol>\n<p><img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_7.png\" alt=\"이미지\"></p>\n<p>일부 모델은 특정 데이터셋에서 훈련되어 코드 완성, 대화 또는 이미지에서 텍스트로 변환과 같은 특정 작업에 더 적합합니다. Ollama에서는 다양한 목적을 위해 설계된 모델을 찾을 수 있습니다.</p>\n<p>첫 번째 그룹은 대화, 텍스트 완성, 요약 등을 용이하게 하는 데 초점을 맞춘 모델을 포함하고 있습니다. Gemma, Llama2, Falcon 또는 OpenChat과 같은 모델이 포함됩니다.</p>\n<p>일부 예시:</p>\n<ul>\n<li>\n<p><a href=\"https://ollama.com/library/falcon\" rel=\"nofollow\" target=\"_blank\">Falcon</a></p>\n</li>\n<li>\n<p><a href=\"https://ollama.com/library/gemma\" rel=\"nofollow\" target=\"_blank\">Gemma</a></p>\n</li>\n<li>\n<p><a href=\"https://ollama.com/library/openchat\" rel=\"nofollow\" target=\"_blank\">Openchat</a></p>\n</li>\n</ul>\n<p>다음 그룹은 대화를 나누거나 챗봇 역할을 하는 다중 모달 모델과 이미지 설명(시각 모델), 텍스트 요약, 질문-답변(Q/A) 애플리케이션을 구동할 수 있는 모델들로 구성됩니다.</p>\n<p>일부 예시:</p>\n<p>➡️ <a href=\"https://ollama.com/library/llava\" rel=\"nofollow\" target=\"_blank\">https://ollama.com/library/llava</a></p>\n<p>➡️ <a href=\"https://ollama.com/library/bakllava\" rel=\"nofollow\" target=\"_blank\">https://ollama.com/library/bakllava</a></p>\n<p>마지막으로, 매우 전문화된 그룹은 Ollama에서 이용 가능한 모델을 활용하여 개발자의 작업을 지원합니다. 코델라마, 돌핀-미스트랄, 돌핀-믹스트랄(코딩 작업에 능숙한 Mixtral 전문가 모델을 기반으로 세밀하게 조정된 모델)과 같은 모델들이 있으며, 계속해서 크리에이터들이 추가하고 있습니다.</p>\n<p>몇 가지 예시:</p>\n<p>➡️ <a href=\"https://ollama.com/library/codellama\" rel=\"nofollow\" target=\"_blank\">https://ollama.com/library/codellama</a></p>\n<p>➡️ <a href=\"https://ollama.com/library/dolphin-mistral\" rel=\"nofollow\" target=\"_blank\">https://ollama.com/library/dolphin-mistral</a></p>\n<p>➡️ <a href=\"https://ollama.com/library/dolphin-mixtral\" rel=\"nofollow\" target=\"_blank\">https://ollama.com/library/dolphin-mixtral</a></p>\n<h1>6. 모델 실행하기 [cmd]</h1>\n<p>다운로드한 모델을 실행하려면, ollama run 모델이름:파라미터 \"당신의 프롬프트\"를 입력하세요. 예를 들어:</p>\n<pre><code class=\"hljs language-js\">ollama run <span class=\"hljs-attr\">llama2</span>:7b <span class=\"hljs-string\">\"당신의 프롬프트\"</span>\n</code></pre>\n<p>다중 모달 모델을 사용하면 기본 프롬프트를 벗어난 파일, 로컬 이미지 경로 등을 포함할 수 있어 더 많은 기능을 확장할 수 있습니다.</p>\n<h1>6. CPU 친화적 양자화 모델</h1>\n<p>양자화는 모델의 정밀도를 유지하는 비용을 줄이는 것으로 관련 비용을 줄이는 것입니다. 이 과정 뒤에 숨은 직관력을 구축하는 데 도움이 되는 이 크고 훌륭한 기사에서 자세한 설명을 찾아볼 수 있습니다:</p>\n<p>📃 양자화 LLMs란 무엇인가? (Miguel Carreira Neves의 글):</p>\n<p>➡️ <a href=\"https://www.tensorops.ai/post/what-are-quantized-llms\" rel=\"nofollow\" target=\"_blank\">https://www.tensorops.ai/post/what-are-quantized-llms</a></p>\n<p>추가 자료:</p>\n<p>📃 Extreme Compression of Large Language Models via Additive Quantization:</p>\n<p>➡️ <a href=\"https://arxiv.org/html/2401.06118v2\" rel=\"nofollow\" target=\"_blank\">https://arxiv.org/html/2401.06118v2</a></p>\n<p>📃 SmoothQuant: 대형 언어 모델을 위한 정확하고 효율적인 사후 훈련 양자화:</p>\n<p>➡️ <a href=\"https://arxiv.org/pdf/2211.10438.pdf\" rel=\"nofollow\" target=\"_blank\">논문 링크</a></p>\n<p>📃 BiLLM: LLMs를 위한 사후 훈련 양자화 한계 돌파:</p>\n<p>➡️ <a href=\"https://arxiv.org/pdf/2402.04291.pdf\" rel=\"nofollow\" target=\"_blank\">논문 링크</a></p>\n<p>간단하게 말하면, 양자화는 가중치 정밀도를 조정하여 모델 크기를 줄이고 중요한 정확도 하락 없이 성능을 쉽게 감소시킬 수 있는 하드웨어에서 실행할 수 있게 해줍니다.</p>\n<p>이 글과 함께 제공된 이미지를 통해 양자화 후에 모델이 원래 버전보다 상당히 적은 공간을 차지하는 것을 확인할 수 있습니다:</p>\n<p><img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_8.png\" alt=\"Quantized Models\"></p>\n<p>Ollama는 양자화된 모델을 지원하여 별도로 처리하는 부담을 덜어줍니다.</p>\n<h1>7. 다른 소스에서 모델 통합하기</h1>\n<p><img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_9.png\" alt=\"Running Models With Ollama Step-by-Step\"></p>\n<p>Ollama의 모델은 다양성을 제공하지만 현재 모든 모델에 액세스할 수 있는 것은 아닙니다. 그러나 로컬에 직접 모델을 통합하는 것은 간단한 프로세스입니다. 새로운 모델을 지역 Ollama에 통합하는 방법을 알아봅시다.</p>\n<p>The Bloke의 HuggingFace 계정에서 많은 양자화된 모델을 사용할 수 있습니다. 의학 논문을 위해서 우리는 편리하게 medicine-chat-GGUF 모델을 선택할 수 있습니다:</p>\n<p>➡️ <a href=\"https://huggingface.co/TheBloke/medicine-chat-GGUF\" rel=\"nofollow\" target=\"_blank\">https://huggingface.co/TheBloke/medicine-chat-GGUF</a></p>\n<p>해당 링크를 열고 파일 및 버전을 클릭하세요.</p>\n<p><img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_10.png\" alt=\"Files and versions\"></p>\n<p>Ollama 모델에 포함하고 싶은 모델을 다운로드하세요:</p>\n<p>Modelfile이라는 빈 파일을 생성하고 아래 지정된 데이터를 삽입하세요 (저장된 모델의 절대 경로로 경로를 대체하십시오). 이 예제는 기본적이며, 모델의 온도, 시스템 메시지 등과 같은 여러 옵션을 포함하여 확장될 수 있습니다. 필요한 경우 파일에서 '#'를 제거하여 해당 옵션을 활성화하세요.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-variable constant_\">FROM</span> <span class=\"hljs-attr\">D</span>:\\...\\medicine-chat.<span class=\"hljs-property\">Q4_0</span>.<span class=\"hljs-property\">gguf</span>\n# <span class=\"hljs-variable constant_\">PARAMETER</span> 온도 <span class=\"hljs-number\">0.6</span>\n# <span class=\"hljs-variable constant_\">SYSTEM</span> <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"도움이 되는 의학 조수입니다.\"</span><span class=\"hljs-string\">\"\"</span>\n</code></pre>\n<p>Modelfile을 저장한 후, cmd에 다음을 입력하세요:</p>\n<pre><code class=\"hljs language-bash\">ollama create 모델_이름 -f 모델_파일\n</code></pre>\n<h1>9. Ollama를 활용한 (Python) 앱으로 개발자의 삶을 더 쉽게 만들기</h1>\n<p>백그라운드에서 실행되는 Ollama는 일반적인 REST API와 같이 접근할 수 있습니다. 따라서 requests와 같은 라이브러리 또는 조금 더 발전된 FastAPI, Flask 또는 Django와 같은 프레임워크를 사용하여 응용 프로그램에 쉽게 통합할 수 있습니다.</p>\n<p>Ollama python 패키지를 쉽게 pip를 통해 설치하세요.</p>\n<p>⬆️ <a href=\"https://pypi.org/project/ollama/0.1.3\" rel=\"nofollow\" target=\"_blank\">https://pypi.org/project/ollama/0.1.3</a>:</p>\n<pre><code class=\"hljs language-js\">pip install ollama\n</code></pre>\n<p>Python 코드를 통해 임베딩을 생성하는 방법:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> ollama\n\nembedding = ollama.<span class=\"hljs-title function_\">embeddings</span>(model=<span class=\"hljs-string\">\"llama2:7b\"</span>, prompt=<span class=\"hljs-string\">\"Hello Ollama!\"</span>)\n</code></pre>\n<p>간단히 CURL을 사용하여:</p>\n<pre><code class=\"hljs language-js\">curl <span class=\"hljs-attr\">http</span>:<span class=\"hljs-comment\">//localhost:11434/api/embeddings -d '{</span>\n  <span class=\"hljs-string\">\"model\"</span>: <span class=\"hljs-string\">\"llama2:7b\"</span>,\n  <span class=\"hljs-string\">\"prompt\"</span>: <span class=\"hljs-string\">\"Here is an article about llamas...\"</span>\n}<span class=\"hljs-string\">'\n</span></code></pre>\n<p>Ollama 엔드포인트에 대해 더 알아보려면 다음 링크를 방문해주세요:</p>\n<p>➡️ <a href=\"https://github.com/ollama/ollama/blob/main/docs/api.md\" rel=\"nofollow\" target=\"_blank\">https://github.com/ollama/ollama/blob/main/docs/api.md</a></p>\n<p>Ollama가 Langchain 프레임워크에 원활하게 통합되어 개발 노력을 최적화하고 기술 측면의 작업을 더욱 간편하게 만들었습니다:</p>\n<p>➡️ <a href=\"https://python.langchain.com/docs/integrations/llms/ollama\" rel=\"nofollow\" target=\"_blank\">https://python.langchain.com/docs/integrations/llms/ollama</a></p>\n<p>임베딩을 만드는 간단함을 감상해보세요:</p>\n<pre><code class=\"hljs language-js\"># pip install langchain_community\n<span class=\"hljs-keyword\">from</span> langchain_community.<span class=\"hljs-property\">embeddings</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">OllamaEmbeddings</span>\n\n\nembed = <span class=\"hljs-title class_\">OllamaEmbeddings</span>(model=<span class=\"hljs-string\">\"llama2:7b\"</span>)\nembedding = embed.<span class=\"hljs-title function_\">embed_query</span>(<span class=\"hljs-string\">\"Hello Ollama!\"</span>)\n</code></pre>\n<h1>10. 요약</h1>\n<p>본 기사는 당신을 Ollama를 사용하여 모델을 실행하는 과정을 단계별로 안내하여, 전체 인프라 구성 없이 LLM을 테스트할 수 있는 원활한 방법을 제공합니다.</p>\n<p>올라마는 오픈 소스 도구로, Meta의 Llama2 모델을 무료로 사용할 수 있게 해주는 로컬 또는 서버 기반의 언어 모델 통합을 용이하게 합니다. 윈도우에서의 설치 과정과 명령줄을 통해 Ollama를 실행하는 방법에 대해 설명되어 있습니다.</p>\n<p>이 기사에서는 모델 다운로드, 특정 작업을 위한 다양한 모델 옵션, 다양한 명령어를 사용하여 모델 실행, CPU 친화적인 양자화된 모델, 그리고 외부 모델 통합에 대해 탐구합니다. 또한, 개발자들을 위해 Ollama를 활용한 파이썬 애플리케이션을 강조하고 있습니다.</p>\n</body>\n</html>\n"},"__N_SSG":true}