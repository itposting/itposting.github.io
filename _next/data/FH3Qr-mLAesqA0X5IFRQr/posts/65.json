{"pageProps":{"posts":[{"title":"깊은 CNN 뒤의 수학  AlexNet","description":"","date":"2024-05-27 14:19","slug":"2024-05-27-TheMathBehindDeepCNNAlexNet","content":"\n`<img src=\"/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_0.png\" />`\n\n합성곱 신경망(Convolutional Neural Networks, CNNs)은 주로 이미지와 같은 구조화된 배열 데이터를 처리하기 위해 고안된 깊은 신경망의 특수한 유형입니다. CNN은 이미지의 픽셀 데이터에서 직접 패턴을 인식함으로써 수동으로 특징을 추출하는 필요성을 제거합니다. CNN은 이미지 내에서 공간적 계층을 이해하는 데 특히 강력하며, 데이터를 패치 단위로 처리하는 학습 가능한 필터를 활용하여 픽셀 간의 공간적 관계를 보존합니다.\n\n이러한 네트워크는 대량의 시각 데이터가 포함된 작업에서 매우 효과적이며, 이미지 및 비디오 인식부터 실시간 물체 감지에 이르기까지 다양한 응용 프로그램에서 널리 사용됩니다. 얼굴 인식 기술과 자율 주행 차량과 같은 발전에 중요한 역할을 하는 기술입니다.\n\n본 문서에서는 컴퓨터 비전 분야에 큰 영향을 미친 혁신적인 CNN 아키텍처인 AlexNet을 살펴볼 것입니다. 다양한 시각 인식 작업에서 강력한 성능으로 유명한 AlexNet은 복잡한 이미지를 직접 해석하기 위해 깊은 학습을 활용합니다. 그 동작 뒤에 있는 수학 및 코드 프레임워크를 자세하게 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 목차\n\n## 1. 소개\n\n## 2. AlexNet 아키텍처 개요\n\n- 2.1. 일반적인 레이어 구조\n- 2.2. 출력 레이어와 Softmax 분류\n\n## 3. AlexNet 구성 요소 깊이 있는 분석\n\n- 3.1. ReLU 비선형성\n- 3.2. 여러 개의 GPU에서의 훈련\n- 3.3. 지역 반응 정규화\n- 3.4. 겹치는 풀링\n- 3.5. 완전 연결 레이어와 드롭아웃\n- 3.6. 드롭아웃\n\n<div class=\"content-ad\"></div>\n\n- 4: 훈련 과정 및 최적화\n\n  - 4.1: 확률적 경사 하강법 매개변수\n  - 4.2: 초기화\n  - 4.3: 학습률 조정 전략\n\n- 5: Python에서 AlexNet 구축\n\n  - 5.1: AlexNet 클래스\n  - 5.2: 조기 중지 클래스\n  - 5.3: 트레이너 클래스\n  - 5.4: 데이터 전처리\n  - 5.5: 모델 훈련 및 평가\n\n- 6: 결론\n\n- 추가 자료\n\n<div class=\"content-ad\"></div>\n\n# 1: 소개\n\n![이미지](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_1.png)\n\nAlexNet은 2012년 ImageNet 대규모 시각 인식 챌린지에서 우승한 이후 주목받게 된 혁신적인 딥 러닝 네트워크입니다. Alex Krizhevsky, Ilya Sutskever 및 Geoffrey Hinton이 개발한 AlexNet은 이전 최고의 26.2%에서 상위 5% 오류율을 15.3%로 크게 낮추어 이 분야에 새로운 기준을 제시했습니다. 이 업적은 복잡한 이미지 분류 작업을 대규모 데이터셋에서 처리하는 데 ReLU 활성화, GPU 가속 및 드롭아웃 정규화를 사용하는 CNN의 효과를 강조했습니다.\n\n이 모델은 대부분의 딥 러닝 CNN에서 표준이 된 여러 계층으로 구성되어 있습니다. 이에는 합성곱 계층, 최대 풀링, 드롭아웃, 완전히 연결된 계층 및 소프트맥스 출력 계층이 포함됩니다. 이 모델의 성공은 설계 및 훈련에 대한 창의적인 접근을 통해 더 깊은 네트워크 아키텍처의 실용성을 보여 주었습니다.\n\n<div class=\"content-ad\"></div>\n\n이 글에서는 AlexNet의 정교한 디자인과 수학 원리를 분석해보겠습니다. AlexNet의 훈련 절차와 최적화 기술에 대해서도 살펴볼 것이며, PyTorch를 사용하여 AlexNet을 처음부터 구축해볼 것입니다.\n\n# 2: AlexNet 아키텍처 개요\n\n![AlexNet Architecture](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_2.png)\n\n## 2.1: 일반적인 레이어 구조\n\n<div class=\"content-ad\"></div>\n\nAlexNet의 아키텍처는 각 레이어가 이전 레이어의 출력을 바탕으로하여 특징을 체계적으로 추출하는 방식으로 설계되어 있습니다. 다음은 레이어 및 기능의 상세한 분해 내용입니다:\n\n입력 이미지\n모델은 227x227 픽셀로 조정된 입력 이미지를 처리합니다. 각 이미지는 표준 RGB 인코딩을 반영하기 위해 세 개의 채널 (빨강, 녹색, 파랑)을 가지고 있습니다.\n\n레이어 구성\n주로 8개의 주요 레이어로 구성되어 있으며, 이 중 5개는 합성곱 레이어이고 나머지 3개는 완전히 연결된 레이어입니다. 이러한 레이어 사이에는 활성화 함수, 정규화, 풀링 및 드롭아웃이 전략적으로 적용되어 학습 효율성을 향상시키고 과적합을 줄입니다.\n\n합성곱 레이어\n최초 레이어는 96개의 커널(필터)을 사용하며, 크기는 11x11x3이며 4픽셀의 스트라이드를 사용하여 입력 이미지와 함께 컨볼루션됩니다. 이 큰 스트라이드 크기는 네트워크를 계산적으로 효율적으로 만들어 첫 번째 레이어부터 출력 공간 볼륨 크기를 크게 줄입니다.\n\n<div class=\"content-ad\"></div>\n\n첫 번째 레이어의 출력은 두 번째 컨볼루션 레이어에 도달하기 전에 정규화와 맥스 풀링을 거칩니다. 이 레이어는 각각 크기가 5x5x48인 256개의 커널로 구성됩니다. 48개의 특성 맵은 이전 레이어에서 별도로 필터링된 출력에 해당하여 이 레이어가 효과적으로 특성을 섞을 수 있도록 합니다.\n\n세 번째 컨볼루션 레이어는 일반적으로 이전 레이어에서 유도된 특성 맵의 풍부함을 유지하는 데 도움이 되는 풀링이나 정규화를 따르지 않습니다. 256개의 크기가 3x3x384인 커널이 사용되며, 이는 두 번째 레이어의 출력과 직접 연결되어 네트워크가 복잡한 특성을 포착할 수 있는 능력을 향상시킵니다.\n\n네 번째 컨볼루션 레이어는 세 번째 레이어의 구성을 반영하지만 크기가 3x3x192인 384개의 커널을 사용하여 네트워크의 깊이를 향상시키면서 레이어의 공간적 차원을 변경하지 않습니다.\n\n마지막 컨볼루션 레이어는 크기가 3x3x192인 256개의 커널을 사용하며 맥스 풀링 레이어가 뒤따르며, 학습 중인 특성에 회전 및 위치 불변성을 제공하고 차원을 줄이는데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n완전 연결 레이어들\n첫 번째 완전 연결 레이어는 4096개의 뉴런을 가진 밀집 레이어입니다. 이 레이어는 이전 합성곱 레이어에서 평탄화된 결과(1차원 벡터로 변환됨)를 받아와 비선형 특징들의 결합을 학습하기 위해 고차원 공간에 투영합니다.\n\n두 번째 완전 연결 레이어도 4096개의 뉴런을 포함하며 드롭아웃 정규화가 적용됩니다. 드롭아웃은 학습 중에 일정 비율의 입력 유닛을 무작위로 0으로 설정하여 과적합을 방지하고, 네트워크가 어떤 작은 뉴런 집합에 의존하지 않는 더 견고한 특징을 학습하도록 장려합니다.\n\n마지막 완전 연결 레이어는 1000개의 뉴런으로 이루어져 있으며, 각각은 ImageNet 챌린지의 클래스에 대응합니다. 이 레이어는 클래스 예측에 중요하며, 일반적으로 분류 확률을 유도하기 위해 소프트맥스 함수를 사용합니다.\n\n## 2.2: 출력 레이어와 소프트맥스 분류\n\n<div class=\"content-ad\"></div>\n\nAlexNet의 마지막 레이어는 3번째 완전 연결 레이어의 logits에 softmax 함수를 적용하여 1000가지 클래스 레이블에 대한 분포를 출력하는 softmax 회귀 레이어입니다.\n\n소프트맥스 함수는 다음과 같습니다:\n\n\n$$\n\\frac{e^{z_i}}{\\sum e^{z_i}}\n$$\n\n\n여기서 zi는 최종 완전 연결 레이어에서 각 클래스에 대한 로짓 또는 원시 예측 점수입니다.\n\n<div class=\"content-ad\"></div>\n\n이 레이어는 각 클래스의 스코어를 지수화하여 모든 클래스의 스코어 합에 대비하여 확률로 변환하여 가장 가능성이 높은 클래스를 강조합니다.\n\nSoftmax 레이어는 이러한 확률을 출력하는 것뿐만 아니라 훈련 중 교차 엔트로피 손실의 기초를 형성하여, 예측된 확률 분포와 실제 분포(진짜 레이블) 사이의 차이를 측정합니다.\n\n# 3: AlexNet 구성 요소의 심도 있는 분석\n\n## 3.1: ReLU 비선형성\n\n<div class=\"content-ad\"></div>\n\n렐루(Rectified Linear Unit, ReLU)는 특히 AlexNet과 같은 CNN(합성곱 신경망)에서 표준 활성화 함수가 되었습니다. 이 간단한 함수는 시그모이드 또는 하이퍼볼릭 탄젠트 함수를 사용하는 네트워크와 비교하여 모델이 더 빨리 학습하고 효과적으로 수렴하게 합니다.\n\nReLU의 수학적 표현은 간단합니다:\n\n![ReLU Function](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_4.png)\n\n이 함수는 x가 양수인 경우 x를 출력하며, 그렇지 않으면 0을 출력합니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 Markdown 형식으로 변경한 글입니다.\n\n![Ramp Function](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_5.png)\n\n그래픽적으로, 이 함수는 양수 입력에 대해 선형적으로 증가하고 음수 입력에 대해 0입니다.\n\nSigmoid가 Tanh에 우세한 점\nReLU는 시그모이드와 같은 전통적인 활성화 함수보다 여러 이점이 있습니다:\n\n![Advantages of ReLU Over Sigmoid](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_6.png)\n\n<div class=\"content-ad\"></div>\n\n하이퍼볼릭 탄젠트와 달리, ReLU 함수는 신경망이 수렴하는 데 도움을 줍니다.\n\nSigmoid와 tanh 함수에서 발생하는 사라지는 기울기 문제를 해결함으로써, 신경망이 더 빨리 수렴할 수 있습니다. 이 문제는 입력이 매우 커지면 (양방향으로 크게) 기울기가 매우 작아지는 경우 발생합니다. 이 작은 기울기로 인해 역전파 중에 가중치에 대한 업데이트가 거의 이루어지지 않아 교육 속도가 현저히 느려집니다. 이와는 대조적으로 ReLU 함수의 기울기는 음수 입력에 대해 0이고 양수 입력에 대해 1입니다. 이로써 기울기 하강을 간단하게 만들고 가속시킵니다.\n\n활성화의 희소성을 촉진합니다. 입력 도메인의 절반에 대해 0을 출력하여 희소 데이터 표현을 내재시킴으로써, 일반적으로 Sigmoid 또는 tanh 함수로 생성되는 밀집 표현보다 희소 표현이 더 유익하다는 것으로 알려져 있습니다. 이는 대규모 이미지 인식 작업에서 특히 유익하며, 거기에 내재된 데이터 차원은 매우 높지만 정보가 상대적으로 낮을 때 더 유용합니다.\n\n<div class=\"content-ad\"></div>\n\n또한 ReLU는 간단한 수학 연산을 포함합니다. 이 활성화 함수는 어떤 입력 값에 대해서도 단일 최댓값 연산이 필요하며, 시그모이드와 하이퍼볼릭 탄젠트는 계산상 더 복잡한 지수 함수를 포함하고 있어 계산이 더 많이 필요합니다. ReLU의 이러한 간단함은 특히 대규모 데이터셋에서 심층 신경망을 훈련할 때 매우 빠른 계산 성능을 제공합니다.\n\nReLU 함수의 음수 부분이 제로처리되기 때문에, 시그모이드나 하이퍼볼릭 탄젠트 함수와 같이 비선형 방식으로 변경되지 않는 출력의 문제를 피할 수 있습니다. 이 특성은 네트워크가 데이터를 더 깨끗하게 모델링하고 훈련 동력에서 잠재적인 문제점을 피할 수 있도록 합니다.\n\n## 3.2: 여러 GPU에서 훈련\n\n![image](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_8.png)\n\n<div class=\"content-ad\"></div>\n\n**AlexNet**은 병렬 GPU 학습을 활용한 선도적인 합성곱 신경망 중 하나였습니다. 깊고 계산량이 많은 아키텍처를 효율적으로 다룰 수 있었습니다. 이 네트워크는 두 개의 GPU에서 동시에 작동하여 성능과 실용성을 크게 향상시키는 중요한 설계 요소입니다.\n\n**레이어별 분배**\nAlexNet의 레이어는 두 개의 GPU 사이에 분배됩니다. 각 GPU는 합성곱 레이어의 뉴런 활성화(커널)의 절반을 처리합니다. 구체적으로, 세 번째 레이어의 커널은 두 번째 레이어의 모든 커널 맵에서 입력을 받지만, 네 번째와 다섯 번째 레이어는 동일한 GPU에 위치한 커널 맵으로부터만 입력을 받습니다.\n\n**GPU 간 통신**\nGPU는 병렬 연산 결과를 통합하기 위해 출력을 결합해야 하는 특정 레이어에서 통신해야 합니다. 이 GPU 간 통신은 병렬 계산 결과를 통합하는 데 필수적입니다.\n\n**선택적 연결성**\nAlexNet의 모든 레이어가 두 개의 GPU에 모두 연결되는 것은 아닙니다. 이 선택적 연결성을 통해 GPU간 전송되는 데이터 양이 줄어들어 통신 오버헤드를 줄이고 계산 효율성을 향상시킵니다.\n\n<div class=\"content-ad\"></div>\n\n이 두 가지 GPU 사이에 데이터셋 뿐만 아니라 네트워크 모델을 분할하는 전략은 AlexNet이 단일 GPU에서 실행될 때보다 더 많은 매개변수와 큰 입력 크기를 처리할 수 있도록 합니다. 추가적인 처리 능력은 AlexNet이 6000만 개의 매개변수 및 대규모 이미지 분류 작업을 효율적으로 학습하기 위해 필요한 방대한 계산을 처리할 수 있게 합니다.\n\n더 큰 배치 크기로 학습하는 것은 여러 GPU로 가능케 됩니다. 더 큰 배치는 훈련 중에 더 안정적인 기울기 추정을 제공하여 깊은 신경망을 효율적으로 훈련하는 데 중요합니다. 여러 GPU를 사용하는 결과가 아니더라도 더 큰 배치 크기로 훈련하고 더 빠른 반복 시간을 가지는 능력은 오버피팅을 대항하는 데 도움이 됩니다. 네트워크는 더 다양한 데이터 집합을 짧은 시간 내에서 경험하며, 이는 훈련 데이터로부터 보이지 않는 데이터로 일반화하는 능력을 향상시킵니다.\n\n## 3.3: Local Response Normalization\n\nAlexNet의 Local Response Normalization (LRN)은 이미지 분류 작업에서 뛰어난 성능을 발휘하는 네트워크의 중요한 역할을 하는 정규화 전략입니다. 이 기술은 ReLU 비선형 활성화 함수의 출력에 적용됩니다.\n\n<div class=\"content-ad\"></div>\n\nLRN 레이어는 이웃하는 뉴런들이 높은 활동을 보일 때 그 뉴런들의 반응을 억제함으로써 각 뉴런의 정규화된 출력을 계산합니다.\n\n특징 맵 i의 (x, y) 위치에 있는 뉴런의 활동 ax, yi를 고려할 때, 반응 정규화된 활동 bx, yi는 다음과 같습니다:\n\n\\[ b*{x,y}^{i} = a*{x,y}^{i} / \\left( k + \\alpha \\sum*{j=max(0,i-n/2)}^{min(N-1, i+n/2)}(a*{x,y}^{j})^2 \\right)^{\\beta} \\]\n\n여기서:\n\n<div class=\"content-ad\"></div>\n\n- ax, yi는 (x, y) 위치에 커널 i를 적용하고 ReLU 함수를 적용하여 계산한 뉴런의 활성입니다.\n- N은 레이어 내 커널의 총 수입니다.\n- 합은 동일한 공간 위치에서 n개의 이웃 커널 맵을 대상으로 하며, N은 커널의 총 수입니다.\n- k, α, β는 미리 정해진 하이퍼파라미터이며 (일반적으로 AlexNet에서 n=5, k=2, α=10e−4, β=0.75).\n- bx, yi는 뉴런의 정규화된 응답입니다.\n\n로컬 응답 정규화(LRN)는 생물학적 뉴런에서 발견된 측면 억제 개념에서 영감을 받은 인접한 뉴런들 사이의 형태의 지역 억제를 구현하는 데 사용됩니다. 이 억제는 여러 중요한 영역에서 중요한 역할을 합니다:\n\n활동 조절\nLRN은 주변 지원을 받지 않는 더 큰 활성화를 처벌함으로써 네트워크의 응답을 압도하는 단일 특성 맵을 방지합니다. 주변 활성화의 제곱 및 합을 통해 어떠한 특성도 결과에 지나치게 영향을 미치지 않도록 보장하며, 여러 입력에 대한 모델의 일반화 능력을 향상시킵니다.\n\n대비 정규화\n이웃들에 비해 두드러져 보이는 패턴을 강조함으로써 LRN은 시각 처리에서의 대비 정규화와 유사하게 기능합니다. 이 기능은 이미지의 중요한 지역적 특성을 효과적으로 강조하여 시각적 구분 과정을 지원합니다.\n\n<div class=\"content-ad\"></div>\n\n에러율 감소\nAlexNet에 LRN을 통합함으로써 ImageNet 분류 작업에서 상위 1 및 상위 5의 에러율을 줄이는 데 도움이 되었습니다. 이는 뉴런의 높은 활동 수준을 관리하여 네트워크의 전체적인 견고성을 향상시키는 데 도움이 됩니다.\n\n## 3.4: 오버랩핑 풀링\n\n오버랩핑 풀링은 합성곱 신경망(CNN)에서 사용되는 기술로, 입력 데이터의 공간 차원을 줄이고, 계산을 간단하게 만들며, 과적합을 제어하는 데 도움이 됩니다. 이는 일반적인 비오버랩핑(전통적) 맥스 풀링을 변경하여 풀링 창이 겹치도록합니다.\n\n전통적인 맥스 풀링\n전통적인 맥스 풀링에서 입력 이미지 또는 피처 맵은 풀링 필터의 크기와 일치하는 각기 다른 비오버랩핑 영역으로 나누어집니다. 각 영역에서 최대 픽셀 값이 결정되고 다음 레이어로 출력됩니다. 이 과정은 비오버랩핑 이웃에서 가장 중요한 피처를 선택하여 데이터 차원을 줄이는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, 2x2 풀링 크기(z)와 2픽셀 간격(stride s)을 가정하면, 필터는 입력 필드를 2픽셀씩 가로로 이동하고 2픽셀씩 세로로 이동합니다. 2의 간격은 필터가 처리하는 영역 간에 겹침이 없음을 보장합니다.\n\nAlexNet의 중첩 풀링\nAlexNet에서 사용되는 중첩 풀링은 스트라이드를 풀 크기보다 작게 설정하는 것을 의미합니다. 이 접근 방식을 사용하면 풀링 영역이 서로 겹칠 수 있으며, 동일한 픽셀이 여러 번의 풀링 작업에 포함될 수 있습니다. 이는 피쳐 맵의 밀도를 높이고 레이어를 통해 더 많은 정보를 유지하는 데 도움이 됩니다.\n\n예를 들어, 3x3 풀링 크기와 2픽셀 간격을 사용하는 경우를 생각해 봅시다. 이 구성은 풀링 필터가 더 크지만(3x3), 이미지나 피쳐 맵을 건너갈 때마다 2픽셀씩만 이동한다는 것을 의미합니다. 결과적으로 인접한 풀링 영역은 처리되는 열 또는 픽셀 행을 공유하며, 기능 통합을 향상시킵니다.\n\n## 3.5: 완전 연결 레이어 및 드롭아웃\n\n<div class=\"content-ad\"></div>\n\nAlexNet의 아키텍처에서 컨볼루션 및 풀링 레이어를 거친 후, 네트워크의 고수준 추론은 완전 연결 레이어에 의해 수행됩니다. 완전 연결 레이어는 컨볼루션 레이어에서 특징 맵을 추출한 후 최종 분류로의 전환에 중요한 역할을 합니다.\n\n완전 연결 (FC) 레이어는 이전 레이어의 모든 뉴런을 가져와서 (다른 완전 연결 레이어의 출력인지, 또는 풀링 또는 컨볼루션 레이어에서 나온 평탄화된 출력인지에 관곂여) 각 뉴런을 포함하는 모든 뉴런에 연결합니다. AlexNet에서는 컨볼루션과 풀링 레이어를 거친 후 세 개의 완전 연결 레이어가 이어집니다.\n\nAlexNet의 처음 두 완전 연결 레이어는 각각 4096개의 뉴런을 가지고 있습니다. 이러한 레이어는 이전 레이어에서 식별한 지역화된 필터링된 특징을 전역적이고 고수준의 패턴으로 통합하는 데 중요합니다. 최종 완전 연결 레이어는 실제로 분류기 역할을 합니다. (이미지넷 데이터셋 기준으로 1000개의) 각 클래스 레이블에 대한 뉴런을 가지며 입력 이미지의 카테고리에 대한 네트워크의 예측을 출력합니다.\n\n이러한 레이어의 각 뉴런은 출력 레이어를 제외하고는 ReLU (활성화 함수)를 적용합니다. 출력 레이어에서는 softmax 함수를 사용하여 출력 로짓(각 클래스에 대한 원시 예측 점수)을 클래스에 대한 확률 분포로 매핑합니다.\n\n<div class=\"content-ad\"></div>\n\n최종 풀링이나 합성층에서의 출력은 일반적으로 완전 연결층으로 전달되기 전에 평탄화 과정을 거칩니다. 이 과정은 2차원 특징 맵을 1차원 특징 벡터로 변환하여 전통적인 신경망 기법을 통해 처리할 수 있도록 합니다. 최종 층의 소프트맥스 함수는 이 네트워크를 통해 학습된 특징 조합에 기반하여 각 클래스 레이블에 확률을 할당하여 입력 이미지를 분류합니다.\n\n## 3.6: 드롭아웃\n\n드롭아웃은 신경망에서 오버피팅을 방지하는 정규화 기법으로, 특히 AlexNet과 같은 대규모 네트워크에서 효과적입니다. 오버피팅은 모델이 훈련 데이터에 특정한 패턴을 학습하지만 새로운 데이터에는 일반화되지 않을 때 발생합니다.\n\nAlexNet에서는 드롭아웃을 첫 번째 두 완전 연결층의 출력에 적용합니다. 이 층의 각 뉴런은 확률 p(일반적으로 0.5로 설정, 즉 50%)으로 \"드롭\"됩니다. 즉, 해당 뉴런은 일시적으로 네트워크에서 제거되며 모든 들어오는 및 나가는 연결도 함께 제거됩니다.\n\n<div class=\"content-ad\"></div>\n\n만약 Dropout의 수학과 코드를 심층적으로 알고 싶다면, 제 이전 글의 3.4절을 살펴보시기를 강력히 추천합니다:\n\n# 4: 훈련 과정과 최적화\n\n## 4.1: 확률적 경사 하강법 매개변수\n\nAlexNet에서는 훈련 중에 네트워크를 최적화하기 위해 확률적 경사 하강법(SGD)을 사용합니다. 이 방법은 손실 함수의 오차 기울기를 기반으로 네트워크의 가중치를 업데이트하며, 배치 크기, 모멘텀, 가중치 감쇠 등 매개변수의 효과적 조정이 모델의 성능과 수렴에 중요합니다. 오늘의 글에서는 Pytorch의 SGD 구현을 사용할 것이며, 이 인기있는 최적화 기법에 대해 고수준의 내용을 다룰 것입니다. 만약 낮은 수준의 내용, 수학적으로 분석하고 최적화 기법을 처음부터 구성하는 것에 관심이 있다면, 이 글을 참조해보세요:\n\n<div class=\"content-ad\"></div>\n\n이제 SGD의 주요 구성 요소와 AlexNet에서 사용된 설정에 대해 알아보겠습니다:\n\n배치 크기\n배치 크기는 모델의 가중치를 업데이트하는 데 사용되는 훈련 예제 수로, 손실 함수의 경사도를 계산하는 데 사용됩니다. AlexNet에서 배치 크기는 128로 설정되어 있습니다. 이 크기는 더 많은 메모리 및 계산을 필요로하는 더 큰 배치와 더 많은 예제를 토대로 계산되어 생긴 정확도 사이의 균형을 유지합니다.\n\n128의 배치 크기를 선택한 것은 경사도 추정을 안정화시켜 업데이트를 더 부드럽고 신뢰할 수 있게 만들어줍니다. 더 큰 배치는 경사도 계산에서의 노이즈를 줄여 각 업데이트에 대한 더 명확한 신호를 제공하지만, 더 많은 계산 리소스가 필요하며 때로는 훈련 데이터로부터 새로운 상황으로 효과적으로 일반화시키지 못할 수도 있습니다.\n\n모멘텀\nSGD의 모멘텀은 올바른 방향으로 업데이트를 가속시키고 옵티마이저가 취한 경로를 부드럽게 만들어줍니다. 이는 이전 업데이트 벡터의 일부분을 포함하여 업데이트 규칙을 수정합니다. AlexNet에서 모멘텀 값은 0.9로 설정되어 있어, 이전 업데이트 벡터의 90%가 현재 업데이트에 기여합니다. 이 높은 모멘텀 수준은 작지만 일관된 경사도와 함께 작동할 때 특히 손실 함수의 최솟값으로 수렴 속도를 높여줍니다.\n\n<div class=\"content-ad\"></div>\n\n모멘텀을 사용하면 업데이트가 올바른 방향으로 이동하는 것뿐만 아니라 일관된 기울기를 가진 손실 함수의 위상을 따라 속도가 증가하는 것을 보장합니다. 이 측면은 어떠한 얕은 지역 최솟값이나 산점이 더 효과적으로 탈출하는 데 중요합니다.\n\n가중치 감쇠\n가중치 감쇠는 큰 가중치를 처벌하는 정규화 항으로 작용하여 가중치 값의 일부를 손실 함수에 추가함으로써 사용됩니다. AlexNet은 이 매개변수를 0.0005로 설정하여 가중치가 너무 커지는 것을 방지하고 네트워크의 많은 매개변수로 인해 과적합이 발생할 수 있는 것을 예방합니다.\n\nAlexNet과 같이 복잡한 모델에서 높은 용량으로 인해 과적합되기 쉬운 상황에서 가중치 감쇠는 필수적입니다. 가중치의 크기를 처벌함으로써, 가중치 가중 특성에 지나치게 의존하지 않도록 하는 일반화된 모델을 유도합니다.\n\nAlexNet의 가중치에 대한 업데이트 규칙은 다음과 같이 설명할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n이 표는 아래와 같이 만들 수 있습니다.\n\n\n![image](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_10.png)\n\n여기에서:\n\n- vt는 이전 단계의 모멘텀 강화 업데이트 벡터입니다.\n- μ (AlexNet의 경우 0.9)는 이전 업데이트의 영향을 강화하는 모멘텀 요소입니다.\n- ϵ은 업데이트 단계의 크기를 결정하는 학습률입니다.\n- ∇L은 가중치에 대한 손실 함수의 기울기를 나타냅니다.\n- λ (AlexNet의 경우 0.0005)는 큰 가중치에 대한 처벌로 과적합의 위험을 줄이는 가중치 감쇠 요소입니다.\n- w는 가중치 자체를 나타냅니다.\n\n이러한 설정은 네트워크가 효율적으로 학습하고 보도 및 보지 않은 데이터에 대해 견고한 성능을 달성하도록 도와줍니다. 이는 학습 속도와 정확도를 최적화하고 일반화 능력을 유지하는데 도움이 됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 4.2: 초기화\n\n딥 신경망을 훈련하는 데 있어 가중치와 편향을 적절하게 초기화하고 학습 속도를 조심스럽게 조절하는 것이 매우 중요합니다. 이러한 요소들은 네트워크가 수렴하는 속도와 훈련 및 검증 데이터에 대한 전반적인 성능에 영향을 미칩니다.\n\n가중치 초기화\n\n![이미지](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_11.png)\n\n<div class=\"content-ad\"></div>\n\nAlexNet에서 컨볼루션 레이어의 가중치는 평균이 0이고 표준 편차가 0.01인 정규 분포에서 초기화됩니다. 이 좁은 표준 편차는 초기에 어떤 단일 뉴런도 출력을 지배하지 못하게하여 가중치 초기화의 균일한 스케일을 보장합니다.\n\n마찬가지로, 완전 연결 레이어의 가중치도 가우시안 분포에서 초기화됩니다. 이 분포의 분산에 특별히 주의하여 레이어 간 출력 분산을 일관되게 유지하는 것은 더 깊은 네트워크의 안정성을 유지하는 데 중요합니다.\n\n이 과정을 더 잘 이해하기 위해 Python으로 AlexNet의 초기화를 처음부터 구축해 봅시다:\n\n```python\nimport numpy as np\n\ndef initialize_weights(layer_shapes):\n    weights = []\n    for shape in layer_shapes:\n        if len(shape) == 4:  # 이것은 conv 레이어입니다: (out_channels, in_channels, filter_height, filter_width)\n            std_dev = 0.01  # conv 레이어용 표준 편차\n            fan_in = np.prod(shape[1:])  # in_channels, filter_height, filter_width의 곱\n        elif len(shape) == 2:  # 이것은 완전 연결 레이어입니다: (out_features, in_features)\n            # He 초기화: std_dev = sqrt(2. / fan_in)\n            fan_in = shape[1]  # 입력 피처의 수\n            std_dev = np.sqrt(2. / fan_in)  # ReLU를 유지하는 것이 권장되는 분산\n        else:\n            raise ValueError(\"잘못된 레이어 형태입니다: 4D(conv) 또는 2D(fc)여야 합니다\")\n\n        # 가우시안 초기화\n        weight = np.random.normal(loc=0, scale=std_dev, size=shape)\n        weights.append(weight)\n\n    return weights\n\n# 예시 사용법:\nlayer_shapes = [\n    (96, 3, 11, 11),  # Conv1 레이어: 96 필터, 3 입력 채널, 11x11 필터 크기\n    (256, 96, 5, 5),  # Conv2 레이어: 256 필터, 96 입력 채널, 5x5 필터 크기\n    (384, 256, 3, 3), # Conv3 레이어: 384 필터, 256 입력 채널, 3x3 필터 크기\n    (384, 384, 3, 3), # Conv4 레이어: 384 필터, 384 입력 채널, 3x3 필터 크기\n    (256, 384, 3, 3), # Conv5 레이어: 256 필터, 384 입력 채널, 3x3 필터 크기\n    (4096, 256*6*6),  # FC1 레이어: 4096 출력 피처, (256*6*6) 입력 피처\n    (4096, 4096),     # FC2 레이어: 4096 출력 피처, 4096 입력 피처\n    (1000, 4096)      # FC3 (출력) 레이어: 1000 클래스, 4096 입력 피처\n]\n\ninitialized_weights = initialize_weights(layer_shapes)\nfor idx, weight in enumerate(initialized_weights):\n    print(f\"Layer {idx+1} weights shape: {weight.shape} mean: {np.mean(weight):.5f} std dev: {np.std(weight):.5f}\")\n```\n\n<div class=\"content-ad\"></div>\n\ninitialize_weights 함수는 각 레이어의 가중치 차원을 설명하는 튜플 목록을 가져옵니다. 컨볼루션 레이어는 네 가지 차원(필터 수, 입력 채널, 필터 높이, 필터 너비)을 기대하고, 완전히 연결된 레이어는 두 가지 차원(출력 피처, 입력 피처)을 기대합니다.\n\n컨볼루션 레이어에서는 표준 편차가 0.01로 고정되어 있으며, 한 개의 뉴런에 의한 지나친 출력을 방지하기 위해 원래 AlexNet 구성과 일치시킵니다.\n\n완전히 연결된 레이어에서는 He 초기화(ReLU 활성화 함수를 사용하는 레이어에 대한 좋은 방법론)를 사용합니다. 여기서 표준 편차는 sqrt(2/fan_in)으로 조정되어 출력 분산을 일정하게 유지함으로써 딥 네트워크에서 안정적인 학습을 촉진합니다.\n\nlayer_shapes에 정의된 각 레이어에 대해 Gaussian(정규) 분포의 평균인 0에서 초기화된 가중치가 계산됩니다.\n\n<div class=\"content-ad\"></div>\n\n편향 초기화\n어떤 합성곱 레이어의 편향은 1로 설정되며 특히 ReLU 활성화 함수 뒤에 오는 레이어에서 사용됩니다. 이 초기화는 뉴런 출력 값을 ReLU 함수의 양수 범위로 밀어넣어 훈련 초기부터 활성화되도록 보장합니다. 다른 레이어의 편향은 중립적인 출력을 위해 0으로 초기화됩니다.\n\n일부 합성곱 레이어와 마찬가지로, 완전히 연결된 레이어의 편향도 1로 설정됩니다. 이 전략은 훈련 초기에 뉴런이 양수 활성화 상태에 있도록하여 훈련 시작 시 죽은 뉴런을 방지하는 데 도움이 됩니다.\n\n## 4.3: 학습률 조정 전략\n\nAlexNet은 초기 학습률을 0.01로 시작합니다. 이 비율은 기울기에 상당한 업데이트를 허용해 초기 진행을 신속하게 돕습니다만, 학습 과정이 발산할 위험이 없는 정도로 너무 높지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n훈련 중에 미리 정해진 지점에서 학습률을 10 배 감소시킵니다. 이 방식은 \"단계 감소\"라고 알려져 있습니다. AlexNet에서 이러한 조정은 일반적으로 검증 오류율이 크게 감소하지 않을 때 발생합니다. 이러한 지점에서 학습률을 감소시킴으로써 가중치 조정을 미세 조정하여 더 나은 수렴을 이끌어냅니다.\n\n더 높은 학습률로 시작하는 것은 모델이 잠재적인 국지 최솟값을 더 효과적으로 극복하도록 도와줍니다. 네트워크가 안정화되기 시작하면 학습률을 줄이는 것이 더 넓고 평평한 최솟값으로 안정화되게 도와줄 수 있습니다. 일반적으로 이는 새로운 데이터에 대한 일반화에 더 적합합니다.\n\n훈련이 진행됨에 따라 학습률을 낮추면 보다 세밀한 가중치 조정이 가능해집니다. 이 점진적인 정제는 모델이 훈련 데이터에 더 잘 맞도록 도와주며, 검증 데이터에 대한 성능도 향상시켜 모델이 훈련 예시만 외우는 것이 아니라 이를 통해 일반화를 실제로 학습하게 도와줍니다.\n\n# 5: Python에서 AlexNet 만들기\n\n<div class=\"content-ad\"></div>\n\n이 섹션에서는 PyTorch를 사용하여 Python에서 AlexNet을 재현하는 단계별 프로세스를 자세히 설명하여 클래스 아키텍처, 초기 설정, 훈련 절차 및 평가 기술에 대한 통찰을 제공합니다.\n\n오늘 다룰 모든 코드가 포함된 이 Jupyter 노트북을 열어 두는 것을 권장합니다:\n\n## 5.1: AlexNet 클래스\n\n먼저 AlexNet의 메인 클래스를 구축하는 것부터 시작해 봅시다:\n\n<div class=\"content-ad\"></div>\n\n```js\n# PyTorch로 신경망을 생성하고 학습하기 위한 코드입니다.\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data.dataset import random_split\n\n# 운영 체제 확인을 위한 모듈\nimport platform\n\n# 데이터셋을 로드하고 변환하기 위한 torchvision\nimport torchvision\nimport torchvision.transforms as transforms\n\n# 학습률을 조정하기 위한 ReduceLROnPlateau 모듈\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# 숫자 연산을 위한 numpy\nimport numpy as np\n\n# 시각화를 위한 matplotlib\nimport matplotlib.pyplot as plt\n\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n```\n\n`AlexNet` 클래스는 `nn.Module`을 상속하여 PyTorch에서 모든 신경망 모듈의 기본 클래스인 `nn.Module`을 사용합니다. PyTorch에서 새로운 신경망 구조는 `nn.Module`을 서브클래싱하여 생성됩니다.\n\n<div class=\"content-ad\"></div>\n\n초기화 메서드는 AlexNet 객체가 생성될 때 어떻게 구성되어야 하는지를 정의합니다. num_classes 매개변수를 선택적으로 받아 출력 클래스의 수를 유연하게 조절할 수 있으며, 기본값은 ImageNet 작업에 일반적인 1000입니다.\n\n특성 레이어\n\n```js\nself.features = nn.Sequential(\n  nn.Conv2d(3, 64, (kernel_size = 11), (stride = 4), (padding = 2)),\n  nn.ReLU((inplace = True)),\n  nn.MaxPool2d((kernel_size = 3), (stride = 2)),\n  nn.Conv2d(64, 192, (kernel_size = 5), (padding = 2)),\n  nn.ReLU((inplace = True)),\n  nn.MaxPool2d((kernel_size = 3), (stride = 2)),\n  nn.Conv2d(192, 384, (kernel_size = 3), (padding = 1)),\n  nn.ReLU((inplace = True)),\n  nn.Conv2d(384, 256, (kernel_size = 3), (padding = 1)),\n  nn.ReLU((inplace = True)),\n  nn.Conv2d(256, 256, (kernel_size = 3), (padding = 1)),\n  nn.ReLU((inplace = True)),\n  nn.MaxPool2d((kernel_size = 3), (stride = 2))\n);\n```\n\n여기에 AlexNet의 합성곱 레이어가 정의됩니다. nn.Sequential 컨테이너는 레이어 시퀀스를 감싸고, 데이터는 추가된 순서대로 이러한 레이어를 통과합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nnn.Conv2d(3, 64, (kernel_size = 11), (stride = 4), (padding = 2));\n```\n\n첫 번째 레이어는 2D 합성곱 레이어(nn.Conv2d)로, 입력 채널은 3개(RGB 이미지)이고, 출력 채널은 64개(특성 맵)이며, 커널 크기는 11x11, 스트라이드는 4이고, 양쪽에 2씩 패딩이 적용됩니다. 이 레이어는 입력 이미지를 처리하고 특성 추출을 시작합니다.\n\n```js\nnn.ReLU((inplace = True));\n```\n\n그런 다음 ReLU 활성화 함수를 통과합니다. 이는 비선형성을 도입하여 모델이 복잡한 패턴을 학습하도록 합니다. inplace=True 매개변수는 입력을 직접 수정하여 메모리를 절약하는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nnn.MaxPool2d((kernel_size = 3), (stride = 2));\n```\n\n맥스 풀링 레이어는 입력 특성 맵의 공간 차원을 줄여주어 모델이 입력 이미지의 특징의 위치에 더 견고해지도록 합니다. 이 레이어는 3x3 크기의 창과 2의 보폭을 사용합니다.\n\n추가의 nn.Conv2d와 nn.MaxPool2d 레이어가 뒤따르며 특성 표현을 더욱 정제하고 간결하게 만듭니다. 각각의 합성곱 레이어는 일반적으로 풀링을 통해 특성 맵의 차원을 줄이면서 특성 맵의 수를 증가시키는데, 이는 공간적인 입력으로부터 점진적으로 더 많은 의미 정보를 포함하는 특징으로 추상화하는 데 도움이 됩니다.\n\n적응형 풀링 및 분류기\n\n\n\n<div class=\"content-ad\"></div>\n\n```js\nself.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n```\n\navgpool은 특징 맵을 자동으로 6x6의 고정 크기로 풀링하며, 완전 연결 레이어의 입력 크기 요구 사항과 일치시키기 위해 필요하며, 네트워크가 다양한 입력 차원을 처리할 수 있도록 합니다.\n\n```js\nself.classifier = nn.Sequential(\n  nn.Dropout(),\n  nn.Linear(256 * 6 * 6, 4096),\n  nn.ReLU((inplace = True)),\n  nn.Dropout(),\n  nn.Linear(4096, 4096),\n  nn.ReLU((inplace = True)),\n  nn.Linear(4096, num_classes)\n);\n```\n\nclassifier라는 또 다른 순차적 컨테이너를 정의했습니다. 이 컨테이너에는 네트워크의 완전 연결 레이어가 포함되어 있습니다. 이 레이어들은 합성곱 레이어에 의해 추출된 추상적인 특징에 기초하여 최종 분류를 수행합니다.\n\n<div class=\"content-ad\"></div>\n\nnn.Dropout()은 각 forward 호출마다 입력 텐서의 요소 중 일부를 확률이 0.5로 임의로 0으로 만들어 과적합을 방지하는 데 도움이 됩니다.\n\nnn.Linear(256 _ 6 _ 6, 4096)은 적응형 풀링 레이어의 네트워크망 피처들을 4096 크기의 벡터로 재구성합니다. 학습된 가중치로 각 입력을 모든 출력에 연결합니다.\n\n마지막으로 nn.ReLU 및 nn.Dropout 호출은 학습 경로를 더 정제하여 비선형 활성화 지점과 정규화를 제공합니다. 최종 nn.Linear 레이어는 차원을 4096에서 num_classes로 줄여 각 클래스에 대한 원시 점수를 출력합니다.\n\nForward 메소드\n\n<div class=\"content-ad\"></div>\n\n```js\ndef forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n```\n\n`forward` 메서드는 네트워크의 순전파 실행을 지시합니다:\n\n- `x = self.features(x)`는 입력을 컨볼루션 레이어를 통해 처음의 특성 추출을 수행합니다.\n- `x = self.avgpool(x)`는 특성에 적응적 풀링을 적용하여 크기를 표준화합니다.\n- `x = torch.flatten(x, 1)`은 출력을 벡터로 평탄화하여 분류를 위해 준비합니다.\n- `x = self.classifier(x)`은 평탄화된 벡터를 분류기를 통해 각 클래스에 대한 예측을 생성합니다.\n\n## 5.2: 조기 중단(Class)\n\n\n\n<div class=\"content-ad\"></div>\n\n훈련 중인 머신 러닝 모델이 유효성 검사 손실이 개선되지 않을 때 훈련 프로세스를 중지하는 EarlyStopping 클래스를 사용합니다. 이 방법은 오버피팅을 방지하고 최적의 시점에 훈련을 중지하여 컴퓨팅 자원을 절약하는 데 중요합니다.\n\n```js\nclass EarlyStopping:\n    \"\"\"\n    성능이 향상되지 않을 때 훈련을 중지하는 얼리 스톱핑 클래스\n\n    Args:\n    -----\n        patience (int): 훈련을 중지하기 전 대기할 에폭 수\n        verbose (bool): True인 경우 손실이 개선되지 않는 각 에폭마다 메시지 출력\n        delta (float): 개선으로 간주할 모니터링 중량의 최소 변경량\n    \"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n\n    def __call__(self, val_loss):\n        \"\"\"\n        Args:\n        -----\n            val_loss (float): 모델 성능이 개선되었는지 확인하는 검증 손실\n\n        Returns:\n        --------\n            bool: 손실이 개선되지 않았으면 True, 그렇지 않으면 False를 반환\n        \"\"\"\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.counter = 0\n```\n\n초기화\n\n```js\ndef __init__(self, patience=7, verbose=False, delta=0):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n```\n\n<div class=\"content-ad\"></div>\n\nEarlyStopping 클래스는 작동을 구성하는 여러 매개변수로 초기화됩니다:\n\npatience은 훈련을 중지하기 전에 검증 손실이 향상되기를 기다릴 에포크 수를 결정합니다. 기본값으로 7로 설정되어 있어서 모델이 손실 경치(plateaus)를 극복할 여지를 줍니다.\n\nverbose는 클래스의 출력을 제어합니다. True로 설정하면 손실이 개선되지 않는 각 epoch에 대한 메시지를 인쇄하여 훈련 중 명확한 피드백을 제공합니다.\n\n델타는 손실 개선으로 간주되는 임계값을 설정하여 조기 중지 메커니즘의 민감도를 미세 조정하는 데 도움을 줍니다.\n\n<div class=\"content-ad\"></div>\n\n**호출 가능한 메서드**\n\n```python\ndef __call__(self, val_loss):\n    score = -val_loss\n\n    if self.best_score is None:\n        self.best_score = score\n    elif score < self.best_score + self.delta:\n        self.counter += 1\n        if self.counter >= self.patience:\n            self.early_stop = True\n    else:\n        self.best_score = score\n        self.counter = 0\n```\n\n`__call__` 메서드는 EarlyStopping 인스턴스를 함수처럼 사용할 수 있도록 해주어 교육 루프에 통합하는 과정을 간단하게 만듭니다. 현재 에포크의 검증 손실을 기반으로 모델의 성능이 향상되었는지를 평가합니다.\n\n이 메서드는 먼저 검증 손실을 최대화해야 하는 점수로 변환합니다. 손실을 부정하여 이루어진 점수(score = -val_loss)입니다. 이것은 낮은 손실이 더 좋다는 것을 의미합니다. 이 첫 평가(self.best_score가 None)일 경우, 메서드는 현재 점수를 초기 best_score로 설정합니다.\n\n<div class=\"content-ad\"></div>\n\n현재 점수가 self.best_score에 작은 델타를 더한 값보다 적으면, 의미 있는 개선이 없음을 나타내므로 counter가 증가합니다. 이 counter는 개선이 없는 epoch가 몇 번 경과했는지를 추적합니다. counter가 인내 임계값에 도달하면, 학습을 중단해야 함을 나타내는 early_stop 플래그가 트리거됩니다.\n\n반대로, 현재 점수가 개선되면, 메소드는 새 점수로 self.best_score를 업데이트하고 counter를 0으로 재설정하여 미래 개선을 위한 새 기준을 반영합니다.\n\n이 메커니즘은 의미 있는 개선이 없는 지정된 epoch 횟수 후에만 학습 프로세스가 중지되도록 보장하므로, 학습 단계를 최적화하고 과소적합 모델로 이어질 수 있는 조기 중지를 방지합니다. 인내와 델타를 조정함으로써 사용자는 학습 성능의 변화에 대해 조기 중지가 얼마나 민감한지를 조정할 수 있어서 특정 시나리오와 데이터셋에 맞게 사용자 정의할 수 있습니다. 이 맞춤 설정은 계산 자원과 시간의 제약 사항에 따라 최상의 모델을 얻기 위해 중요합니다.\n\n## 5.3: Trainer 클래스\n\n<div class=\"content-ad\"></div>\n\n트레이너 클래스는 전체 훈련 워크플로우를 포함하며, 에포크를 반복하고 훈련 루프를 관리하며 역전파를 처리하고 훈련 효율성과 효과를 최적화하기 위해 조기 중지 프로토콜을 구현합니다.\n\n```js\nclass Trainer:\n    \"\"\"\n    모델을 훈련하는 Trainer 클래스.\n\n    Args:\n    -----\n        model (nn.Module): 신경망 모델.\n        criterion (torch.nn.modules.loss): 손실 함수.\n        optimizer (torch.optim): 옵티마이저.\n        device (torch.device): 모델을 실행할 장치.\n        patience (int): 훈련을 중지하기 전까지 기다리는 에포크 수.\n    \"\"\"\n    def __init__(self, model, criterion, optimizer, device, patience=7):\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.device = device\n        self.early_stopping = EarlyStopping(patience=patience)\n        self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', patience=3, verbose=True, factor=0.5, min_lr=1e-6)\n        self.train_losses = []\n        self.val_losses = []\n        self.gradient_norms = []\n\n    def train(self, train_loader, val_loader, epochs):\n        \"\"\"\n        모델을 훈련합니다.\n\n        Args:\n        -----\n            train_loader (torch.utils.data.DataLoader): 훈련 데이터셋을 위한 DataLoader.\n            val_loader (torch.utils.data.DataLoader): 검증 데이터셋을 위한 DataLoader.\n            epochs (int): 모델을 훈련할 에포크 수.\n        \"\"\"\n        for epoch in range(epochs):\n            self.model.train()\n            for images, labels in train_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n\n                self.optimizer.zero_grad()\n                outputs = self.model(images)\n                loss = self.criterion(outputs, labels)\n                loss.backward()\n                self.optimizer.step()\n\n            self.train_losses.append(loss.item())\n\n            val_loss = self.evaluate(val_loader)\n            self.val_losses.append(val_loss)\n            self.scheduler.step(val_loss)\n            self.early_stopping(val_loss)\n\n            # 훈련 및 검증 손실 기록\n            print(f'에포크 {epoch+1}, 훈련 손실: {loss.item():.4f}, 검증 손실: {val_loss:.4f}')\n\n            if self.early_stopping.early_stop:\n                print(\"조기 중지\")\n                break\n\n    def evaluate(self, test_loader):\n        \"\"\"\n        테스트 데이터셋에 대해 모델을 평가합니다.\n\n        Args:\n        -----\n            test_loader (torch.utils.data.DataLoader): 테스트 데이터셋을 위한 DataLoader.\n\n        Returns:\n        --------\n            float: 테스트 데이터셋의 평균 손실.\n        \"\"\"\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for images, labels in test_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n\n                outputs = self.model(images)\n                loss = self.criterion(outputs, labels)\n                total_loss += loss.item()\n\n        return total_loss / len(test_loader)\n\n    def accuracy(self, test_loader):\n        \"\"\"\n        테스트 데이터셋에서 모델의 정확도를 계산합니다.\n\n        Args:\n        -----\n            test_loader (torch.utils.data.DataLoader): 테스트 데이터셋을 위한 DataLoader.\n\n        Returns:\n        --------\n            float: 테스트 데이터셋에서 모델의 정확도.\n        \"\"\"\n        self.model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for images, labels in test_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n\n                outputs = self.model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        return correct / total\n\n    def plot_losses(self, window_size=100):\n        # 이동 평균 계산\n        train_losses_smooth = self.moving_average(self.train_losses, window_size)\n        val_losses_smooth = self.moving_average(self.val_losses, window_size)\n\n        # 그래프 그리기\n        plt.plot(train_losses_smooth, label='훈련 손실')\n        plt.plot(val_losses_smooth, label='검증 손실')\n        plt.legend()\n        plt.grid()\n        plt.title('손실')\n\n    def moving_average(self, data, window_size):\n        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n```\n\n초기화\n\n```js\ndef __init__(self, model, criterion, optimizer, device, patience=7):\n    self.model = model\n    self.criterion = criterion\n    self.optimizer = optimizer\n    self.device = device\n    self.early_stopping = EarlyStopping(patience=patience)\n    self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', patience=3, verbose=True, factor=0.5, min_lr=1e-6)\n    self.train_losses = []\n    self.val_losses = []\n    self.gradient_norms = []\n```\n\n<div class=\"content-ad\"></div>\n\nTrainer 클래스는 인공 신경망 모델, 손실 함수, 옵티마이저 및 모델이 실행될 장치(CPU 또는 GPU)로 초기화됩니다. 이 설정을 통해 모든 모델 연산이 적절한 하드웨어로 전달되도록 보장됩니다.\n\n또한 조기 종료 및 학습률 감소 전략을 구성합니다:\n\n- 조기 종료: 검증 손실을 모니터링하고 주어진 에포크 수(인내심) 동안 개선이 없는 경우 훈련을 중지합니다.\n- ReduceLROnPlateau: 검증 손실의 개선이 멈추면 학습률을 줄이는데, 이는 가중치 공간에서 더 작은 단계를 밟아 모델을 세밀하게 조정하는 데 도움이 됩니다.\n\n여기서 train_losses와 val_losses는 각각 훈련 및 검증 단계의 에포크당 손실을 수집하여 성능 추적 및 나중 분석을 위해 사용됩니다. gradient_norms는 기울기의 크기를 저장하는 데 사용될 수 있으며, 디버깅 및 기울기가 소멸되거나 폭발하지 않도록 확인하는 데 유용합니다.\n\n<div class=\"content-ad\"></div>\n\n훈련 방법\n\n```js\ndef train(self, train_loader, val_loader, epochs):\n    for epoch in range(epochs):\n        self.model.train()\n        for images, labels in train_loader:\n            images, labels = images.to(self.device), labels.to(self.device)\n\n            self.optimizer.zero_grad()\n            outputs = self.model(images)\n            loss = self.criterion(outputs, labels)\n            loss.backward()\n            self.optimizer.step()\n\n        self.train_losses.append(loss.item())\n\n        val_loss = self.evaluate(val_loader)\n        self.val_losses.append(val_loss)\n        self.scheduler.step(val_loss)\n        self.early_stopping(val_loss)\n\n        # 훈련 및 검증 손실 기록\n        print(f'Epoch {epoch+1}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')\n\n        if self.early_stopping.early_stop:\n            print(\"조기 종료\")\n            break\n```\n\n이 훈련 방법은 지정된 epoch 수동안 모델 훈련을 조정합니다. 데이터 일괄 처리, 역전파를 통한 모델 가중치 업데이트, 각 epoch 종료 시 검증 세트를 사용하여 모델 성능을 평가합니다.\n\n각 epoch 이후에 훈련 및 검증 손실을 기록하고 필요 시 학습률을 업데이트합니다. 조기 종료 조건이 트리거된 경우, 검증 손실을 평가한 후 조기 중지할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n평가 및 정확도 방법\n\n```python\ndef evaluate(self, test_loader):\n    self.model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(self.device), labels.to(self.device)\n\n            outputs = self.model(images)\n            loss = self.criterion(outputs, labels)\n            total_loss += loss.item()\n\n    return total_loss / len(test_loader)\n\ndef accuracy(self, test_loader):\n    self.model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(self.device), labels.to(self.device)\n\n            outputs = self.model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return correct / total\n```\n\n평가 방법은 주어진 데이터셋(일반적으로 검증 또는 테스트 세트)에서 모델의 성능을 평가하고 평균 손실을 반환합니다. 이 방법은 모델을 평가 모드로 설정하고 데이터셋을 반복하여 각 배치에 대해 손실을 계산하고 모든 배치를 통해 평균 손실을 계산합니다.\n\naccuracy는 예측된 레이블을 실제 레이블과 비교하여 주어진 데이터셋에서 모델의 정확도를 계산합니다. 이 방법은 평가 모드에서 데이터셋을 처리하고 모델의 예측을 사용하여 올바른 예측 수를 계산하고 정확도 백분율을 반환합니다.\n\n<div class=\"content-ad\"></div>\n\n시각화를 위한 유틸리티 메서드\n\n```js\ndef plot_losses(self, window_size=100):\n        # 이동 평균 계산\n        train_losses_smooth = self.moving_average(self.train_losses, window_size)\n        val_losses_smooth = self.moving_average(self.val_losses, window_size)\n\n        # 플롯\n        plt.plot(train_losses_smooth, label='훈련 손실')\n        plt.plot(val_losses_smooth, label='검증 손실')\n        plt.legend()\n        plt.grid()\n        plt.title('손실')\n\n    def moving_average(self, data, window_size):\n        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n```\n\n이 메서드는 모델이 과적합을 시작하거나 시간이 지남에 따른 손실 감소와 같은 추세를 더 명확하게 보여주기 위해 지정된 epoch 창을 통해 부드럽게 표시된 학습 및 검증 손실을 시각화합니다.\n\n## 5.4: 데이터 전처리\n\n<div class=\"content-ad\"></div>\n\nAlexNet 모델을 효과적으로 훈련시키려면 해당 모델의 입력 요구 사항에 맞게 데이터 전처리가 필요합니다. 구체적으로는 AlexNet이 원래 디자인된 차원과 정규화 표준에 부합해야 합니다.\n\n변환하기\n\n```js\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # AlexNet 호환을 위해 이미지 크기를 224x224로 조정\n    transforms.ToTensor(),  # 이미지를 PyTorch 텐서로 변환\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 텐서를 정규화\n])\n```\n\ntransforms.Resize((224, 224))는 이미지의 크기를 224x224 픽셀로 조정하여 AlexNet 모델에서 필요로 하는 입력 크기와 일치시키며, 모든 입력 이미지가 동일한 크기를 갖도록 합니다.\n\n<div class=\"content-ad\"></div>\n\ntransforms.ToTensor()은 이미지를 PIL 형식이나 NumPy 배열에서 PyTorch 텐서로 변환합니다. PyTorch 모델은 입력을 텐서 형식으로 기대하기 때문에 이 과정은 필수적입니다.\n\ntransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))은 이미지 텐서를 정규화합니다. 이 구체적인 정규화는 세 개의 채널 (RGB)에 대해 평균과 표준 편차를 0.5로 조정하여 픽셀 값을 [-1, 1] 범위로 조정합니다. 이 단계는 입력을 표준화하여 모델의 학습 과정을 원활하게 만드는 데 중요합니다.\n\n데이터셋 로딩\n\n```js\ntrainset = torchvision.datasets.CIFAR10((root = \"./data\"), (train = True), (download = True), (transform = transform));\n\ntestset = torchvision.datasets.CIFAR10((root = \"./data\"), (train = False), (download = True), (transform = transform));\n\nclasses = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\");\n```\n\n<div class=\"content-ad\"></div>\n\n여기서 CIFAR-10 데이터 세트를 훈련 및 테스트용으로 로드합니다. 미리 훈련된 모델을 학습하는 데 널리 사용되는 ImageNet 데이터 세트를 선택하지 않았을 수도 있습니다. ImageNet은 상당한 컴퓨팅 리소스와 오랜 학습 시간이 필요하므로 일반 노트북에서 시도하기를 권장하지 않습니다. 대신, CIFAR-10 데이터 세트를 선택했는데, 이 데이터 세트는 10가지 다른 클래스로 분산된 60,000장의 32x32 컬러 이미지를 포함하고 있습니다. 각 클래스당 6,000장의 이미지가 있습니다.\n\n참고: CIFAR-10 데이터 세트는 MIT 라이선스에 따라 오픈 소스로 사용할 수 있습니다. 이 라이선스는 상용 응용프로그램을 포함하여 다양한 용도로 자유롭게 사용할 수 있습니다.\n\n분할 및 데이터 로더\n\n```python\ntrain_split = 0.8\ntrain_size = int(train_split * len(trainset))\nval_size = len(trainset) - train_size\ntrain_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n```\n\n<div class=\"content-ad\"></div>\n\n훈련 데이터는 80%를 훈련용으로, 20%를 검증용으로 나눠놨어요. 이 방식은 모델을 보이지 않은 데이터로 튜닝하여 적절한 일반화 능력을 향상시키는 데 자주 사용됩니다.\n\n훈련, 검증 및 테스트 데이터셋을 배치 크기 64로 생성하기 위해 DataLoader 객체를 사용했어요. 훈련 데이터는 셔플링을 통해 무작위성을 보장하며, 이는 모델이 데이터의 순서에서 잘못된 패턴을 배우는 가능성을 줄여 더 효과적으로 학습하게 도와줍니다.\n\n데이터 시각화\n\n```js\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\ndef imshow(img):\n    img = img / 2 + 0.5  # 정상화해주세요\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\nimshow(torchvision.utils.make_grid(images[:5]))\nprint(' '.join('%5s' % classes[labels[j]] for j in range(5)))\n```\n\n<div class=\"content-ad\"></div>\n\n먼저 이미지를 되돌리기 위해 (img = img / 2 + 0.5)를 사용합니다. 여기서 imshow는 텐서를 넘파이 배열로 변환하고, 차원 순서를 matplotlib.pyplot.imshow()에서 요구하는 형식으로 변경합니다.\n\n그런 다음, 데이터 세트에서 첫 번째 5개 이미지를 표시합니다:\n\n![이미지](/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_12.png)\n\n## 5.5: 모델 훈련 및 평가\n\n<div class=\"content-ad\"></div>\n\n마침내 AlexNet 모델의 훈련 환경을 설정했고, PyTorch를 사용하여 훈련 프로세스를 실행하고 테스트 데이터셋에서 모델의 성능을 평가했습니다.\n\n하지만 먼저, 성능 효율성을 극대화할 최상의 컴퓨팅 리소스(CPU 또는 GPU)를 사용하는지 확인해야 합니다.\n\n```js\n# 시스템의 운영 체제 확인\nif platform.system() == 'Darwin':  # Darwin은 macOS의 약칭입니다\n    try:\n        device = torch.device('cuda')\n        _ = torch.zeros(1).to(device)  # CUDA 사용 가능 여부에 따라 오류가 발생합니다\n    except:\n        device = torch.device('mps' if torch.backends.mps.is_built() else 'cpu')\nelse:\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n```\n\n여기서는 시스템이 macOS('Darwin')인지 식별하고 CUDA를 사용하도록 구성을 시도합니다. 일반적으로 macOS에서 NVIDIA GPU가 없어 CUDA를 사용할 수 없는 경우, Apple의 Metal Performance Shaders(MPS)를 사용할 수 있는 경우 MPS를 선택하거나 그렇지 않으면 CPU를 선택합니다.\n\n<div class=\"content-ad\"></div>\n\nmacOS 이외의 운영 체제에서는 CUDA를 직접 사용하려고 시도하고 CUDA를 사용할 수 없는 경우 CPU로 기본 설정됩니다.\n\n모델, 손실 함수 및 옵티마이저 초기화\n다음으로, AlexNet 모델을 초기화하고 계산 장치를 지정하고 손실 함수 및 옵티마이저를 설정합니다:\n\n```python\nmodel = AlexNet(num_classes=10).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n```\n\n10개 클래스로 AlexNet의 인스턴스가 생성되며, 즉시 지정된 장치(GPU 또는 CPU)로 전송됩니다. 이를 통해 모델의 모든 계산이 지정된 장치에서 수행되도록 보장합니다.\n\n<div class=\"content-ad\"></div>\n\nCrossEntropyLoss 함수는 다중 클래스 분류 문제에 대해 일반적으로 사용됩니다.\n\nSGD (확률적 경사 하강법) 옵티마이저는 모델의 매개변수, 학습률 0.01 및 모멘텀 0.9로 초기화됩니다. 이것들은 많은 시각 기반 작업에 대해 시작할 때 표준값입니다.\n\n모델 훈련\n모델은 지정된 epoch 수 동안 훈련되며 데이터를 배치로 처리하고 손실을 계산하며 역전파를 수행하고 검증 손실을 기반으로 조기 중지를 적용합니다:\n\n```js\ntrainer = Trainer(model, criterion, optimizer, device, (patience = 7));\ntrainer.train(train_loader, val_loader, (epochs = 50));\n```\n\n<div class=\"content-ad\"></div>\n\ntrain 메서드는 훈련 및 검증 데이터 로더를 사용하여 모델을 50번의 에포크 동안 훈련시킵니다. 이 메서드는 데이터 로더에서 배치를 세심하게 처리하고 손실을 계산하며 가중치를 업데이트하기 위한 백프로파게이션을 수행하고, 검증 데이터셋을 사용하여 모델을 정기적으로 평가하여 검증 손실에 개선이 없을 경우 조기 중단을 구현합니다.\n\n모델 평가\n훈련 후에는 다음과 같이 테스트 세트에서 모델의 성능을 평가합니다.\n\n```js\ntest_loss = trainer.evaluate(test_loader)\nprint(f'Test Loss: {test_loss:.4f}')\n\naccuracy = trainer.accuracy(test_loader)\nprint(f'Test Accuracy: {accuracy:.2}')\n```\n\n마지막으로, 훈련 및 검증 손실을 시각화하여 모델의 학습 진행 상황을 모니터링합니다:\n\n<div class=\"content-ad\"></div>\n\n```js\ntrainer.plot_losses((window_size = 3));\n```\n\n이 코드는 plot_losses 메소드를 호출하여 훈련 및 검증 손실을 시각화합니다. 손실 값은 윈도우에 스무싱되어 있습니다 (이 경우 3개의 데이터 포인트로) 노이즈 없이 트렌드를 더 잘 시각화하기 위해. 이 코드를 실행하면 다음과 같은 손실을 기대할 수 있습니다:\n\n<img src=\"/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_13.png\" />\n\n위 그래프에서 보듯이, 모델 훈련은 21번의 epoch 후에 중지되었고 우리가 인내 값으로 7을 설정하고 14번째 epoch 이후에는 검증 손실이 개선되지 않았기 때문입니다. 이 설정은 교육 목적을 위해 만들어졌으므로 AlexNet을 능가하는 것이 목표가 아님을 명심해 주세요.\n\n<div class=\"content-ad\"></div>\n\n개발자님, 친구 같은 톤으로 번역해드리겠습니다.\n\n에포크 수나 인내심을 늘려 검증 손실이 더 떨어질 수 있는지 확인해보는 것을 권장합니다. AlexNet의 성능을 향상시킬 수 있는 변경 및 업데이트 사항이 몇 가지 있습니다. 이 기사에서는 30분 시간 제한으로 인해 이러한 조정 사항을 다루지 않지만, 모델 성능을 개선할 수 있는 다양한 고급 기술을 탐색할 수 있습니다.\n\n더 많은 실험을 원하시는 분들을 위해 학습률 조정, 네트워크 아키텍처 조정, 더 고급 정규화 방법 사용 등 매개변수 조정을 시도해보세요. 더 최적화 및 세밀한 조정 기술들은 다음 기사에서 더 자세히 탐구할 수 있습니다:\n\n# 6: 결론\n\nAlexNet은 신경망 설계 및 학습 기술의 발전에서 중요한 모델로, 딥러닝 분야에서 중요한 이정표 역할을 하였습니다. ReLU 활성화, 겹치는 풀링, 그리고 GPU 가속 학습의 혁신적인 사용은 신경망의 효율성과 효과성을 현저히 향상시켜, 모델 아키텍처에 새로운 기준을 제시하였습니다.\n\n<div class=\"content-ad\"></div>\n\nAlexNet으로의 드롭아웃 및 데이터 증강 기법 도입은 신경망의 오버피팅 문제를 해결하고 일반화 능력을 향상시켜 다양한 작업에 더 견고하고 다재다능하게 만들었습니다. 이러한 기술들은 현대 딥 러닝 프레임워크에서 핵심적인 역할을 하며, 다양한 후속 혁신에 영향을 미쳤습니다.\n\n## 추가 자료\n\n- Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Advances In Neural Information Processing Systems. [링크](http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf)\n- LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436–444. [링크](https://doi.org/10.1038/nature14539)\n\n<div class=\"content-ad\"></div>\n\nCristian Leo (2024). The Math Behind Convolutional Neural Networks, [link](https://medium.com/towards-data-science/the-math-behind-convolutional-neural-networks-6aed775df076)\n\n마지막까지 읽어주셔서 감사합니다! 이 글이 마음에 드셨다면 좋아요를 눌러 주시고 제 팔로우도 부탁드립니다. 앞으로도 비슷한 글을 정기적으로 업로드할 예정이에요. 제 목표는 가장 인기있는 알고리즘들을 처음부터 다시 만들어 기계 학습을 모두에게 접근하기 쉽게 만드는 것입니다.\n","ogImage":{"url":"/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_0.png"},"coverImage":"/assets/img/2024-05-27-TheMathBehindDeepCNNAlexNet_0.png","tag":["Tech"],"readingTime":40},{"title":"대규모 언어 모델에서 추론을 위한 인-컨텍스트 학습 방법","description":"","date":"2024-05-27 14:17","slug":"2024-05-27-Retrieval-BasedIn-ContextLearningforReasoninginLargeLanguageModels","content":"\n\n대규모 언어 모델(LLM)은 소수의 입력-출력 예제만 제공되어도 혁신적인 능력을 보여주는 것으로 입증되었습니다. 적은 양의 예제만 제공되어도 LLM은 새로운 작업에 빠르게 적응할 수 있으며 어떠한 기울기 업데이트나 파인튜닝도 필요하지 않습니다. 이로써 LLM은 전례 없는 유연성과 일반화 능력을 나타낼 수 있었습니다.\n\n그러나 ICL에서 LLM의 성능은 제공된 인-컨텍스트 데모의 선택에 매우 민감합니다. 각 새로운 작업에 최적인 예제 세트를 수동으로 선택하는 것은 도전적이며 편향을 도입할 수 있습니다. 이에 대응하기 위해 RetICL이라는 신생 접근 방식은 사용자 지정된 각 입력 쿼리에 맞는 데모 세트를 동적으로 검색하는 방법을 제안합니다.\n\n본 기사에서는 RetICL의 주요 아이디어에 대해 깊이 파고들고, 현재 RetICL 시스템 및 응용 프로그램의 현황을 조사하며 이 희망적이지만 미발달 분야의 핵심 가정 몇 가지를 살펴볼 것입니다. RetICL이 LLM 추론을 향상시키는 데 성과를 보여주는 반면, 이러한 성과의 진정한 근원과 더 견고하고 충실한 RetICL 방법으로 나아가는 과정에 대한 여전히 미해결된 문제들이 존재함을 확인할 것입니다.\n\n강력한 에이전트 응용 프로그램을 가능하도록 하는 열쇠가 될 데이터 엔지니어링 도전이 될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n![Retrieval-Based ICL Overview](/assets/img/2024-05-27-Retrieval-BasedIn-ContextLearningforReasoninginLargeLanguageModels_0.png)\n\n# Retrieval-Based ICL 개요\n\nRetICL의 핵심 아이디어는 ICL에서 일반적으로 사용되는 고정된 인-컨텍스트 데모 세트를 새로운 입력 쿼리에 맞게 선택된 관련 예제로 동적 검색 프로세스로 대체하는 것입니다. 수동으로 선택된 정적 예제 세트에 의존하는 대신에 RetICL 시스템은 기존 지식 베이스 및 검색 모델을 활용하여 정보 쿼리의 문맥에 가장 적합한 데모를 실시간으로 찾아냅니다.\n\n이 접근 방식은 표준 ICL에 비해 여러 잠재적인 장점을 제공합니다. 첫째, 쿼리별 데모 검색을 통해 RetICL은 각 새로운 작업에 적응할 수 있도록 LLM에 보다 관련성 높은 정보를 제공할 수 있습니다. 둘째, RetICL은 고정 예제 세트를 선별하는 데 관여되는 수동적 노력과 잠재적인 편견을 줄입니다. 마지막으로 대규모 지식 베이스의 활용을 통해 RetICL 시스템은 단일 프롬프트에 맞게 구현할 수 있는 정보 범위를 상당히 확장할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n위 혜택을 고려하면 RetICL은 빠르게 연구 분야의 핫한 주제가 되었으며, 다양한 평가에서 제안된 시스템과 격려하는 결과가 증가하고 있습니다. 동시에 RetICL은 아직 발전 중인 분야로, 검색 시스템의 최적 디자인과 그 효과적인 원인에 대한 여러 개방적인 질문이 많이 있습니다.\n\n# RetICL 시스템의 주요 디자인 선택 사항\n\n기존의 RetICL 시스템은 다음과 같은 여러 중요한 차원을 통해 특징화될 수 있습니다:\n\n검색 모델: RetICL 시스템의 핵심 구성 요소는 주어진 쿼리에 대한 관련된 데모를 선택하는 데 사용되는 검색 모델입니다. 하나의 접근 방식은 \"벼림되어있는\" 검색기(예: 밀도가 높은 단락 검색)을 사용하는 것이며, 이들은 대규모 데이터세트에서 사전 훈련되어 텍스트를 임베딩으로 인코딩하고 유사한 예제를 효율적으로 찾습니다. 반대로, 일부 RetICL 시스템은 도메인별 데이터에서 검색기를 세밀하게 조정하여 대상 작업에 대한 검색 정밀도를 향상시킵니다.\n\n<div class=\"content-ad\"></div>\n\n검색 목표: 데모를 선택할 때 중요한 질문 중 하나는 유사성 또는 다양성을 최적화할지 여부입니다. 유사성 기반 방법은 쿼리와 가장 의미론적으로 관련된 예제를 찾는 데, 다양성 기반 방법은 더 넓은 범위의 관련 정보를 포함하는 보완적인 데모 세트를 찾는 데 초점을 둡니다. 실제로 많은 RetICL 시스템은 두 가지 목표를 조합하여 사용합니다.\n\n검색 말뭉치: 또 다른 중요한 설계 선택은 검색에 사용되는 지식 베이스입니다. 일부 RetICL 시스템은 작업별 훈련 세트와 같은 정성화된 도메인별 말뭉치에 초점을 맞출 수 있으며, 다른 시스템은 Wikipedia와 같은 대규모 오픈 도메인 지식 베이스를 사용할 수 있습니다. 검색 말뭉치의 선택은 검색된 데모의 관련성과 품질에 큰 영향을 미칠 수 있습니다.\n\n검색 전략: 마지막으로, RetICL 시스템은 여러 검색된 데모를 통합하는 다른 전략을 사용할 수 있습니다. 가장 간단한 접근 방법은 단일 단계 검색으로, 상위 순위의 고정된 수의 데모가 함께 연결됩니다. 더 정교한 시스템은 반복적 검색을 사용할 수 있으며, 각 선택된 데모를 기반으로 쿼리 인코딩을 업데이트하여 후속 반복에서 더 다양한 예제를 찾을 수 있습니다.\n\n이러한 설계 선택의 많은 구성이 있지만 경험적 결과는 RetICL의 효과가 특정 도메인 및 모델링 목표에 크게 의존한다는 것을 시사합니다. 일반적인 해결책은 없을 것으로 보이며, 보다 체계적인 비교 및 제거 실험의 필요성을 도출합니다.\n\n<div class=\"content-ad\"></div>\n\n# 디자인 데모 전부터 학문위키\n\nRetICL에서 중요한 과제는 검색 모델 자체를 학습하는 것입니다. 어떻게 하면 직접적인 지도 없이 가장 정보가 풍부한 예시를 찾을 수 있을까요? 대부분의 기존 방법은 두 가지 범주로 나뉩니다:\n\nLM 기반 지도: 일반적인 전략 중 하나는 LLM이 금 예상을 생성하는 확률을 사용하여 후보 데모의 유용성을 대리로 하는 것입니다. 그런 다음 검색 모델은 이 확률을 최대화하는 데모를 선택하도록 학습됩니다. 이는 LLM 자체의 능력을 활용하여 관련 예시를 식별하는 데 도움이 됩니다.\n\n모델 없는 휴리스틱: 대안은 n-gram overlap 또는 entity matching과 같은 작업 무관한 휴리스틱을 사용하여 예시의 적절성을 추정하는 것입니다. LM 기반의 지도보다는 능력이 떨어지지만, 이러한 방법은 보다 효율적이고 검색 말뭉치에 적용 가능합니다.\n\n<div class=\"content-ad\"></div>\n\n훈련 데이터를 이러한 접근 방식 중 하나로 얻은 후, 리트리버는 일반적으로 대조적 학습이나 가르침과 같은 표준 순위 목표를 사용하여 최적화됩니다 [2]. 최근에는 점진적으로 더욱 어려운 부정적 예를 채굴하는 훈련 방법들이 리트리버의 견고성을 향상시키는 데 유망함을 보여주기도 했습니다 [2].\n\n# 적용 및 영향\n\nRetICL은 다양한 언어 작업에서 LLM 성능을 향상시키는 데 적용되었습니다. 일부 주목할만한 성과로는 다음이 있습니다:\n\n추론: RetICL은 복잡한 추론이 필요한 multi-hop 질문 응답 및 사실 검증 데이터셋에서 강력한 결과를 달성했습니다 [1,2]. 관련 사실과 설명을 검색함으로써 RetICL은 LLM이 더 정확하고 신뢰할 수 있는 추론 추적을 생성하도록 도울 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n질문 답변: 오픈 도메인 QA 벤치마크에서 RetICL은 대규모 지식 베이스에서 관련 문단을 동적으로 검색함으로써 표준 ICL을 능가했습니다. 이는 텍스트와 이미지에 대한 추론을 필요로 하는 멀티 모달 QA 작업을 포함합니다.\n\n텍스트 생성: 요약 및 대화와 같은 언어 생성 작업에서 RetICL은 문맥 내 예제를 제공하여 LLMs가 더 일관되고 관련성 있는 스타일에 맞는 출력으로 이끌어줄 수 있습니다.\n\n보다 일반적으로, RetICL은 새로운 도메인과 작업에 LLMs를 효과적으로 적응시키는 유망한 접근 방식을 제공하며, 파인튜닝의 계산 비용 없이 외부 지식을 실시간으로 활용하는 것을 학습함으로써 RetICL은 유연하고 확장 가능한 지식 통합을 LLMs로 가능하게 할 수 있습니다.\n\n동시에 RetICL은 여전히 중요한 한계에 직면하고 있습니다. RetICL 시스템의 성능은 검색에 사용된 지식 베이스의 관련성과 품질에 따라 달라집니다. 이러한 지식 베이스의 노이즈나 빈 곳은 환각적이거나 편향된 출력으로 이어질 수 있습니다. RetICL은 검색 단계에서 추가적인 계산 오버헤드를 도입하지만, 효율적인 인덱스 구조로 이를 완화할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# RetICL의 기반 조사\n\nRetICL은 흥미를 자아내지만, 최근의 연구에서는 이러한 시스템의 기반을 면밀히 조사하고 그 근본적인 추론 능력에 대한 진정한 본질을 의심하기도 시작했습니다.\n\n한 연구에서 ReAct, RetICL의 인기있는 프레임워크를 철저히 분석한 결과 [3], 입력 프롬프트의 다양한 요소를 주의 깊게 조절하여, 저자들은 ReAct의 특별한 성능이 본래 주장된 추론과 행동의 교차능력에서 나오지 않았음을 발견했습니다. 대신, 모델의 출력이 낮은 수준의 프롬프트 세부사항에 매우 민감하며 그 \"추론\"이 주로 얕은 검색이 아니라 의미 있는 심사숙고 대신 이루어졌다는 것을 발견한 것입니다 [3].\n\n구체적으로, 이 연구에서는 ReAct의 성능이 주로 주장된 추론과 행동 교차능력이라는 것에 의해 나오는 것이 아니라 제공된 예제 프롬프트의 선택에 매우 민감하다는 것을 발견했습니다. 예제 프롬프트가 체계적으로 변화할 때, ReAct의 출력은 주로 얕은 검색과 패턴 매칭에 더 의존하는 것처럼 보였으며, 실제로 언어 모델의 추론 능력을 증진하는 것보다는 오히려 예제에 대한 검색과 비슷한 패턴 매칭에 더 의존하는 것으로 나타났습니다.\n\n<div class=\"content-ad\"></div>\n\nRetICL 방법이 언어 모델의 강력한 추론을 가능하게 하는지, 아니면 모델이 단순히 프롬프트 동안 제공된 특정 예제에 연결된 휴리스틱을 학습하는 것인지 의문이 제기됩니다.\n\n가장 중요한 점은 RetICL 시스템에서 얻은 이득의 기저를 이루는 예제 프롬프트의 선택이 중요한 요소로 보인다는 것입니다.\n\n이러한 결과는 RetICL 시스템의 획기적인 이득을 해석할 때 신중함이 필요함을 강조합니다.\n\nLLM의 추론 및 일반화 능력을 얼마나 향상시키는지에 따라, 그들이 진정으로 휴리스틱을 배우는지 알아봐야 합니다.\n\n<div class=\"content-ad\"></div>\n\n다음에는 더 많은 프롬프트와 순열에 걸쳐 RetICL 시스템을 보다 철저히 테스트하고 생성된 추론 추적에 대해 미세하게 평가하는 것이 필요합니다 [3].\n\n# 향후 방향\n\n앞으로 나아가면, RetICL을 향상시키고 효과적이며 견고하게 만들기 위한 많은 흥미로운 방법이 있습니다. 몇 가지 주요 방향은 다음과 같습니다:\n\n오픈 도메인 지식: 현재 RetICL 시스템은 주로 특정 작업과 관련된 정리된 지식 베이스에 의존합니다. 중요한 새로운 과제는 웹과 같은 대규모, 완전하게 구조화되지 않은 소스에서 관련 정보를 신뢰할 수 있게 얻을 수 있는 리트리버를 개발하는 것입니다. 이는 규모에 걸친 잡음, 불일치, 검색 효율성 등의 문제를 처리하는 것을 필요로 합니다 [1].\n\n<div class=\"content-ad\"></div>\n\n지식 퓨전: 다른 기회는 여러 가지 모달리티와 지식 소스를 효과적으로 통합하는 것입니다. 예를 들어, 과학 질문에 답변하기 위한 RetICL 시스템은 교과서, 방정식, 다이어그램, 실험 데이터에서 정보를 결합해야 할 수도 있습니다. 이러한 다양한 정보 원천을 효과적으로 조화시키는 리트리버를 설계하는 것은 중요한 도전 과제입니다.\n\n신뢰할 만한 추론: RetICL 시스템에서 생성된 추론 기록의 해석 가능성과 신뢰성을 향상시키는 것이 그들의 안전한 배포를 위해 중요합니다. 이는 환각을 줄이기 위해 검색결과의 관련성을 향상시키고, 검색된 지식을 더 신뢰할 수 있는 방식으로 일관된 사고 연쇄로 운영할 수 있는 LLMs를 설계하는 것을 포함합니다.\n\n이론적 이해: 경험적으로, 유사한 예제를 검색하는 것이 문맥 내 학습을 개선하는 것이 강력하게 증명되었습니다. 그러나 이 현상의 이론적 기초는 여전히 모호합니다. LLMs의 적응을 돕는 검색의 이점이 왜 그리고 어떻게 지식을 가져오는지에 대한 수학적 모델을 개발하는 것은 미래 RetICL 시스템의 설계를 지도할 가치 있는 통찰을 제공할 수 있습니다.\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\nRetICL은 각 작업에 대해 관련된 데모를 동적으로 선택하여 LLM의 추론 능력을 향상시키는 유망한 패러다임으로 등장했습니다. 대규모 말뭉치의 지식을 활용하여 RetICL은 LLM이 새로운 도메인에 유연하게 적응하고 더 근거 있는 정확한 작업 완료를 생성하는 데 도움을 줄 수 있습니다.\n\n그러나 이 잠재력을 실현하기 위해서는 검색 모델, 교육 목표, 지식 원본 및 추론 시간 전략의 주요 설계 선택 사항을 주의 깊게 탐색해야 합니다. 또한 RetICL 시스템의 추론 능력을 검증하고 결과물이 충실하고 편향되지 않고 견고하다는 것을 보장하기 위해 추가적인 경험적 및 이론적 분석이 필요합니다.\n\n중요한 미해결 문제가 남아 있지만, RetICL의 신속한 진전은 LLM이 외부 지식과 자유로운 추론 및 생성을 가능하게 하는 미래를 엿보게 해줍니다. 이를 이루기 위해서는 핵심 검색 및 추론 능력을 확장하는 창의력과 한계와 잠재적 위험을 면밀히 검토하는 주의가 필요합니다. 앞으로의 도전적인 길이 기대됩니다.\n\n참고 문헌:\n\n<div class=\"content-ad\"></div>\n\n[1] Luo et al. (2023) “In-context Learning with Retrieved Demonstrations for Language Models: A Survey”\n\n[2] Rubin et al. (2022) “Learning to Retrieve Prompts for In-Context Learning”\n\n[3] Verma et al. (2024) “On the Brittle Foundations of ReAct Prompting for Agentic Large Language Models”","ogImage":{"url":"/assets/img/2024-05-27-Retrieval-BasedIn-ContextLearningforReasoninginLargeLanguageModels_0.png"},"coverImage":"/assets/img/2024-05-27-Retrieval-BasedIn-ContextLearningforReasoninginLargeLanguageModels_0.png","tag":["Tech"],"readingTime":7},{"title":"SOFTS 시리즈-코어 퓨전을 활용한 효율적인 다변수 시계열 예측","description":"","date":"2024-05-27 14:15","slug":"2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion","content":"\n## 새로운 MLP 기반 모델인 SOFTS는 혁신적인 STar Aggregate-Dispatch (STAD) 모듈을 활용하여 계산 복잡성을 이차 방정식에서 선형으로 줄여 놀라운 효율성과 확장성으로 다변량 시계열 예측에서 최첨단 성능을 달성합니다.\n\n'와우, 시계열은 중요하지만 어렵다! 그리고...' 라고 말하는 부분을 건너 뜁니다. 이는 독자가 시계열 예측의 섬세함을 알고 핵심을 쉽게 이해하길 원한다고 가정한 것이죠!\n\n이 논문은 무엇에 중점을 두나요?\n\n# 기여\n\n<div class=\"content-ad\"></div>\n\n이 논문의 주요 기여는 두 가지이며, \"SOFT\"와 \"STAD\"가 있습니다.\n\n## SOFT\n\nSOFT: Series-cOre Fused Time Series forecaster\n\n이는 다변량 시계열 예측을 위해 설계된 것으로, 채널 독립성과 상관관계를 균형있게 유지하기 위해 STAD 모듈을 사용합니다. 이는 채널 상호작용을 전역 중심 표현으로 집중시킴으로써 선형 복잡성으로 우수한 성능을 달성하는 데 도움이 되었습니다.\n\n<div class=\"content-ad\"></div>\n\n## STAD\n\nSTAD: STar Aggregate Dispatch 모듈\n\nSTAD는 SOFT의 기초가 되는 곳입니다. 여기서 SOFT는 간단한 MLP 기반 모델입니다. STAD는 다변량 시계열의 채널 간 종속성을 포착하는 중앙 집중식 구조입니다. 결과는 이 방법이 효과적이고 확장 가능하다는 것을 보여줍니다.\n\n![이미지](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_0.png)\n\n<div class=\"content-ad\"></div>\n\n## Reversible Instance Normalization\n\n연구자들은 이 방법을 사용했습니다. 이는 ITRANSFORMER 논문에서 채택되어 정규화를 하이퍼파라미터로 고려했기 때문입니다.\n\niTransformer의 Reversible Instance Normalization은 단순히 역 치수에 대해 주의와 피드포워드 네트워크를 적용하여 다변량 상관 관계를 포착하고 비선형 표현을 효과적으로 학습할 수 있게 합니다.\n\n## Series Embedding\n\n<div class=\"content-ad\"></div>\n\n시리즈 임베딩은 패치 임베딩보다 덜 복잡합니다. 이를 전체 시리즈의 길이로 패치 길이를 설정하는 것과 같다고 말할 수 있습니다. 연구자들은 각 채널의 시리즈를 임베딩하기 위해 선형 투사를 사용했습니다:\n\n![이미지](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_1.png)\n\n## STar Aggregate Dispatch (STAD) 모듈\n\n여러 STAD 모듈을 사용하여 시리즈 임베딩을 미세 조정합니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image1](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_2.png)\n\n## Linear Predictor\n\nAfter N layer of STAD, there is a linear predictor for our task (forecasting), if S_N is the output representation of layer n, the prediction will be as follows:\n\n![image2](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_3.png)\n\n\n<div class=\"content-ad\"></div>\n\n# 별 집합 디스패치 모듈\n\nSTar Aggregate Dispatch (STAD) 모듘은 다변량 시계열 예측에서 채널 간 종속성을 포착하기 위해 설계된 중앙 집중식 메커니즘입니다. 주의 메커니즘과 같은 전통적인 분산 구조와 달리 STAD는 각 채널 쌍의 특성을 직접 비교하여 이차 복잡성을 초래하는 것이 아닌, 중앙 집중식 전략을 사용하여 이러한 복잡성을 선형으로 줄입니다. 모든 시리즈에서 정보를 집계하여 전역 중심 표현으로 변환한 다음 이 핵심 정보를 다시 개별 시리즈 표현에 보내어 비정상 채널에 대한 개선된 강건성을 갖는 효율적인 채널 상호 작용이 가능하게 합니다.\n\n이 중앙 집중 구조는 소프트웨어 엔지니어링에서의 별 모양 시스템에서 영감을 받았습니다. 여기서 중앙 서버가 직접 피어 간 통신이 아닌 정보를 집계하고 교환합니다. 이 설계를 통해 STAD는 채널 독립성의 혜택을 유지하면서 예측 정확도를 향상시키기 위해 필요한 상관 관계를 포착합니다. 채널 통계를 단일 핵심 표현으로 집계함으로써 STAD는 비정상적인 시계열에서 신뢰할 수 없는 상관 관계에 의존하는 위험을 완화합니다.\n\n![이미지](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_4.png)\n\n<div class=\"content-ad\"></div>\n\n적용된 결과에 따르면 STAD 모듈은 기존의 최첨단 방법보다 우수한 성능을 보여주며, 그럼에도 불구하고 상당히 낮은 계산 요구량으로 그 성과를 이룹니다. 이는 많은 다른 트랜스포머 기반 모델들에 대한 도전이었던 채널 수가 많거나 긴 lookback 창을 가진 데이터셋에 대해 확장 가능하게 만듭니다. 게다가 STAD 모듈의 일반성 덕분에 다양한 트랜스포머 기반 모델들에서 어텐션 메커니즘을 대체로 사용할 수 있으며, 그 효율성과 효과를 한층 더 입증하고 있습니다.\n\nSTAD의 입력은 각 채널에 대한 시리즈 표현이며, MLP를 통해 처리한 후 풀링합니다 (여기서는 확률적 풀링을 사용합니다):\n\n![image](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_5.png)\n\n이제 우리는 코어 표현(O)을 계산했으며, 코어와 모든 시리즈의 표현을 퓨전합니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_6.png)\n\nThe Repeat_Concat concatenates the core representation O to each series representation to get Fi. Then we give this Fi to another MLP and add the output to the previous hidden dimension to calculate the next one.\n\n- Note that there’s also a residual connection from the input to the output.\n\n# Results\n\n\n<div class=\"content-ad\"></div>\n\n메소드가 간단해 보이지만, 복잡성이 크게 줄었어요 (이차 함수에서 선형 함수로) 그런데 대단하죠 😅😉\n\n![이미지1](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_7.png)\n\n연구자들은 다양한 데이터셋을 실험하고 대부분의 선배들과 비교해보았는데, 아래에서 확인할 수 있어요:\n\n![이미지2](/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_8.png)\n\n<div class=\"content-ad\"></div>\n\n그들은 다른 실험도 수행했지만, 이 문서를 너무 길게 만들지 않기 위해 원래 연구 논문을 읽는 것을 추천합니다.\n\n면책사항: 이 문서를 작성하는 데 Nouswise를 사용했는데, 이는 문서를 통해 정보를 찾을 수 있는 검색 엔진과 같은 것입니다. 보통 일반적으로 이용할 수는 없지만, 직접 저에게 연락하여 접근 권한을 부여받을 수 있습니다. 연락처는 X(이전 트위터) 또는 우리 디스코드 서버에 있을 수 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_0.png"},"coverImage":"/assets/img/2024-05-27-SOFTSEfficientMultivariateTimeSeriesForecastingwithSeries-CoreFusion_0.png","tag":["Tech"],"readingTime":5},{"title":"강화 학습 딥 Q-네트워크","description":"","date":"2024-05-27 14:10","slug":"2024-05-27-ReinforcementLearningDeepQ-Networks","content":"\n## Python을 사용하여 달에 착륙하는 셔틀 가르치기: Deep Q-Networks를 활용한 강화 학습의 수학적 탐구\n\n![Reinforcement Learning](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_0.png)\n\n강화 학습(RL)에서 Q-학습은 에이전트가 환경을 탐색하면서 누적 보상을 극대화하기 위한 정책을 학습하는 데 도움이 되는 기본 알고리즘입니다. 이를 통해 특정 상태에서 특정 작업을 수행했을 때 기대되는 유틸리티를 추정하는 작업-값 함수를 업데이트 함으로써 보상을 받고 미래 추정에 기반합니다 (이게 익숙하지 않으신가요? 걱정 마세요. 나중에 함께 자세히 살펴볼 겁니다).\n\n그러나 전통적인 Q-학습에는 도전 과제가 있습니다. 상태 공간이 확장됨에 따라 확장 가능성에 어려움을 겪으며 연속적인 상태 및 작업 공간을 갖는 환경에서 효과적이지 않습니다. 이때 Deep Q Networks (DQNs)가 나타납니다. DQNs는 Q-값을 근사하기 위해 신경망을 사용하여 에이전트가 보다 크고 복잡한 환경을 효과적으로 처리할 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\n본 기사에서는 Deep Q Networks에 대해 자세히 살펴보겠습니다. DQNs가 기존의 Q-learning의 한계를 극복하는 방법과 DQN을 구성하는 주요 구성 요소에 대해 탐구할 것입니다. 또한 처음부터 DQN을 구현하고 더 복잡한 환경에 적용하는 과정을 살펴볼 것입니다. 이 기사를 마치면 DQN이 어떻게 작동하는지 이해하고 도전적인 강화 학습 문제를 해결하는 데 사용하는 방법을 알게 될 것입니다.\n\n## 목차\n\n1: 전통적인 Q-러닝\n∘ 1.1: 상태와 행동\n∘ 1.2: Q-값\n∘ 1.3: Q-테이블\n∘ 1.4: 학습 과정\n\n2: Q-러닝에서 Deep Q-네트워크로\n∘ 2.1: 전통적인 Q-러닝의 한계\n∘ 2.2: 신경망\n\n<div class=\"content-ad\"></div>\n\n3: Deep Q-Network의 해부학\n\n- 3.1: DQN의 구성요소\n- 3.2: DQN 알고리즘\n\n4: 처음부터 Deep Q-Network 구현하기\n\n- 4.1: 환경 설정\n- 4.2: 딥 신경망 구축\n- 4.3: 경험 재생 구현\n- 4.4: 타깃 네트워크 구현\n- 4.5: Deep Q-Network 훈련\n- 4.6: 모델 튜닝\n- 4.7: 모델 실행\n\n5: 결론\n참고 문헌\n\n# 1: 전통적인 Q-Learning\n\n<div class=\"content-ad\"></div>\n\n\n![Reinforcement Learning Deep Q Networks](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_1.png)\n\nQ-러닝은 환경에서 누적 보상을 극대화하기 위한 최적 조치를 학습하는 에이전트를 안내합니다. 딥 Q-네트워크에 집중하기 전에, 그 선구자인 Q-러닝 뒤에 있는 메커니즘을 간단히 검토하는 것이 좋습니다.\n\n## 1.1: 상태 및 조치\n\n미로를 탐색하는 로봇이라고 상상해보세요. 미로에서 차지하는 각 위치를 \"상태\"라고 합니다. 왼쪽, 오른쪽, 위 또는 아래로 이동하는 것과 같은 각각의 움직임을 \"조치\"라고 합니다. 목표는 결국 미로를 통해 최적 경로를 찾으려면 각 상태에서 어떤 조치를 취할지 결정하는 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 1.2: Q-Values\n\nQ-Learning의 핵심은 Q-값으로, 𝑄(𝑠, 𝑎)로 표시됩니다. 이 값은 특정 상태 s에서 특정 행동 a를 취한 후 더 나은 경로(정책)를 따를 때 기대되는 미래 보상을 나타냅니다.\n\nQ-값을 가이드북의 항목으로 생각해보세요. 각 가능한 이동의 장기적 이점을 평가하는 것입니다. 예를 들어 미로의 특정 위치에 있다고 가정했을 때 왼쪽으로 이동하는 경우, Q-값은 미래 보상 측면에서 그 이동이 얼마나 유익할지 알려줍니다. 더 높은 Q-값은 더 나은 이동을 나타냅니다.\n\n## 1.3: The Q-Table\n\n<div class=\"content-ad\"></div>\n\nQ-Learning은 Q-값을 추적하는 데 Q-테이블을 사용합니다. Q-테이블은 기본적으로 각 행이 상태에 해당하고 각 열이 행동에 해당하며 각 셀이 해당 상태-행동 쌍의 Q-값을 포함하는 대형 스프레드시트입니다.\n\nQ-테이블을 거대한 스프레드시트로 상상해보세요. 각 셀은 미로의 특정 위치에서 특정 이동을 하였을 때 잠재적 미래 보상을 나타냅니다. 환경에 대해 더 많이 배우면이 보상의 더 나은 추정치로이 스프레드시트를 업데이트합니다.\n\n## 1.4: 학습 과정\n\nQ-러닝의 학습 과정은 반복적입니다. 초기 상태 s에서 시작합니다. 그런 다음 작업 a를 결정합니다. 이 선택은 다음을 기반으로 할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 탐험: 효과를 발견하기 위해 새로운 조치를 시도합니다.\n- 개척: 가장 높은 알려진 Q-값을 갖는 조치를 선택하기 위해 기존 지식을 사용합니다.\n\n선택한 조치를 수행하고 보상 r을 관찰하며 다음 상태 s'로 이동합니다. Q-러닝 공식을 사용하여 상태-조치 쌍 (s, a)의 Q-값을 업데이트합니다:\n\n[이미지](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_2.png)\n\n여기에:\n\n<div class=\"content-ad\"></div>\n\n- α는 학습 속도로, 새로운 정보가 이전 정보를 얼마나 덮어쓸지를 결정합니다.\n- γ는 할인 요소로, 즉각적 보상을 먼 미래의 보상보다 더 가치 있게 여깁니다.\n- maxa′Q(s′,a′)는 다음 상태 s′에서 가능한 모든 행동 a′에 대해 최고의 Q값을 나타냅니다.\n\n매번 안내서를 업데이트하고 있다고 상상해 보세요. 각 이동 후에 성공적인지 실패인지에 대한 피드백(보상)을 받습니다. 그런 다음 새로운 정보를 반영하도록 가이드북의 등급(Q값)을 조정하여 미래의 결정을 더 잘 하게 됩니다.\n\nQ값이 수렴할 때까지 이 과정을 반복하면, 에이전트는 미로를 탐색하는 최적 정책을 학습한 것입니다. 시간이 흘러, 미로를 반복적으로 탐험하고 경험에 기반하여 가이드북을 업데이트함으로써 최상의 보상을 얻기 위한 최적의 움직임을 알려주는 포괄적인 전략을 개발하게 됩니다.\n\nQ-러닝에 대해 자세히 알아보려면 이 기사를 확인해 보세요: [링크](링크)\n\n<div class=\"content-ad\"></div>\n\n# 2: Q-Learning에서 Deep Q-Network로\n\n## 2.1: 전통적인 Q-Learning의 한계\n\nQ-Learning은 강화 학습에 대한 강력한 알고리즘이지만, 더 복잡한 환경에서 효과적으로 동작하는 데 제약 사항이 몇 가지 있습니다:\n\n확장성 문제: 전통적인 Q-Learning은 각 상태-행동 쌍이 Q-값에 매핑된 Q-테이블을 유지합니다. 상태 공간이 성장함에 따라, 특히 고차원 또는 연속적인 환경에서는 Q-테이블이 불필요하게 커져 메모리 비효율성과 학습 속도 저하를 초래합니다.\n\n<div class=\"content-ad\"></div>\n\n이산 상태 및 행동 공간: Q-Learning은 상태와 행동이 이산적이고 유한한 환경에서 잘 동작합니다. 하지만 현실 세계의 많은 문제는 연속적인 상태와 행동 공간을 포함하고 있습니다. 이러한 전통적인 Q-Learning은 이러한 공간을 이산화하지 않고는 효율적으로 처리할 수 없으며, 이로 인해 정보 손실과 최적 정책의 하락을 초래할 수 있습니다.\n\n## 2.2: 신경망\n\n이제 신경망을 소개해 보겠습니다. 신경망은 딥 네트워크에서 중요한 역할을 하는데, 인간 두뇌의 구조와 기능을 모방하여 데이터로부터 복잡한 패턴을 학습할 수 있는 강력한 함수 근사기입니다. 신경망은 입력 데이터를 처리하고 가중치와 편향을 통해 변환하여 출력을 생성하는 연결된 노드(뉴런)의 계층으로 이루어져 있습니다.\n\n강화 학습의 맥락에서 신경망은 Q-함수를 근사화하는 데 사용될 수 있습니다. 이는 상태-행동 쌍을 Q-값에 매핑하는 데 도움이 되며, 특히 Q-테이블을 유지하는 것이 적절하지 않은 대규모나 연속적인 공간에서 상태와 행동 간에 일반화를 더 잘할 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\n따라서, Deep Q-networks(DQNs)은 Q-Learning의 원리를 신경망의 함수 근사 능력과 결합시켜요. 그렇게 하면 전통적인 Q-learning의 주요 제약 사항을 다룰 수 있어요.\n\nDQNs은 Q-값을 테이블에 저장하는 대신 신경망을 사용하여 Q-함수를 근사해요. 이 네트워크는 상태를 입력으로 받아 가능한 모든 행동에 대한 Q-값을 출력해요. 환경에서의 경험으로 네트워크를 학습시켜 에이전트는 각 행동에 대한 예상 보상을 예측하도록합니다. 이를 통해 다양한 상태와 행동에 걸쳐 일반화할 수 있어요.\n\n체스를 배우는 것을 상상해보세요. 가능한 모든 체스판 구성과 각 동작에 대한 최상의 수를 외우는 대신(불가능한 일이죠), 전략과 원칙(예를 들어 보드 중앙을 제어하고 왕을 보호하는 것과 같은 것)을 배우게 됩니다. 비슷하게, DQN는 신경망을 통해 일반적인 패턴과 전략을 배우고 모든 가능한 상태를 외우지 않고도 정보를 바탕으로 결정할 수 있어요.\n\n신경망 사용은 DQN이 크거나 연속된 상태 공간을 다룰 수 있게 해요. 네트워크는 주요 특징을 잡아내는 상태 공간의 표현을 학습해 중요한 결정을 취할 수 있도록 해줍니다.\n\n큰 도시를 이동하려면 고려합시다. 모든 거리와 건물의 배치를 외우는 대신 표지판과 중요 도로를 인식해 길을 찾게 됩니다. DQN의 신경망도 비슷하게 작용하며, 에이전트가 복잡한 환경에서 이동하는 것을 돕는 상태 공간의 중요한 특징을 인식하도록 학습합니다.\n\n<div class=\"content-ad\"></div>\n\n다양한 경험을 훈련함으로써 모델은 과거 경험에서 일반화하는 법을 배우게 됩니다. 즉, 에이전트는 배운 것을 새로운, 보지 못한 상태와 행동에 적용할 수 있어서 다양한 상황에서 더 적응력이 있고 효율적일 수 있습니다.\n\n# 3: 딥 Q-네트워크의 구성 요소\n\n## 3.1: DQN의 구성 요소\n\n딥 Q-네트워크 (DQN)가 어떻게 작동하는지 이해하려면 그 주요 구성 요소를 자세히 살펴보는 것이 중요합니다:\n\n<div class=\"content-ad\"></div>\n\n3.1.1: 신경망\n\n![신경망](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_3.png)\n\nDQN의 핵심은 Q값을 위한 함수 근사기 역할을 하는 신경망입니다. 아키텍처는 일반적으로 다음과 같이 구성됩니다:\n\n입력 레이어: 에이전트의 \"눈\"으로 상상해보세요. 이 레이어는 환경으로부터 상태 표현을 받아들이는데, 마치 당신의 눈이 주변의 시각적 정보를 받아들이는 것과 유사합니다. 위의 이미지에서 왼쪽에 두 개의 노드가 있는 첫 번째 레이어입니다.\n\n<div class=\"content-ad\"></div>\n\nHidden Layers: 이러한 레이어들은 에이전트의 \"뇌\"로 생각할 수 있습니다. 눈을 통해 받은 정보를 다수의 사고 단계를 거쳐 처리하여 복잡한 특징과 패턴을 식별합니다. 마치 당신의 뇌가 세계를 처리하고 이해하는 방식과 비슷합니다. 위 이미지에서는 세 개의 노드가 있는 중간 레이어입니다.\n\nOutput Layer: 이는 에이전트의 \"의사 결정\" 부분과 같습니다. 입력 상태에 따라 모든 가능한 행동에 대한 Q 값(값함수)을 생성합니다. 당신이 보고 생각한 것에 기반하여 최선의 행동을 결정하는 방식과 유사합니다. 각 출력은 특정 행동을 취했을 때 기대되는 보상에 해당합니다. 위 이미지에서는 한 개의 노드를 가진 오른쪽의 마지막 레이어입니다.\n\n위 이미지는 간단한 피드포워드 신경망을 나타냅니다. 이는 신경망의 가장 기본적인 형태입니다. 이 구조는 기본적이지만 \"깊은\" 신경망은 아닙니다. 깊은 신경망으로 변환하기 위해서는 더 많은 은닉 레이어를 추가하여 신경망의 깊이를 증가시켜야 합니다. 또한, 다양한 아키텍처와 구성을 실험하여 더 발전된 모델을 개발할 수 있습니다. 각 레이어의 노드 수는 고정되지 않으며, 특정 훈련 데이터셋과 작업에 따라 다양합니다. 이러한 유연성을 통해 네트워크를 특정 목적에 더 잘 맞게 조정할 수 있습니다.\n\n신경망에 대해 더 알고 싶다면, 나는 아래의 글을 강력히 추천합니다:\n\n<div class=\"content-ad\"></div>\n\n3.1.2: 경험 재생\n이제 목록의 다음 항목인 경험 재생으로 넘어가 봅시다. 이것은 DQNs에서 학습 과정을 안정화하고 향상시키는 기술입니다. 다음을 포함합니다:\n\n메모리 버퍼: 에이전트의 \"일기\"로 생각해보세요. 이것은 에이전트의 경험을 시간이 지남에 따라 저장합니다 (상태, 행동, 보상, 다음 상태, 완료), 마치 매일 당신이 무슨 일이 일어났는지 기록하는 것처럼.\n\n랜덤 샘플링: 훈련 중에 에이전트는 지난 경험을 배우기 위해 일기의 랜덤한 페이지를 넘깁니다. 이는 사건의 순서를 깨어주어 에이전트가 경험의 순서에 과적합되는 것을 방지하여 보다 견고하게 학습하도록 돕습니다.\n\n3.1.3: 타겟 네트워크\n마지막으로, 타겟 네트워크는 훈련을 위해 타겟 Q-값을 계산하는 데 사용되는 별도의 신경망입니다. 주 신경망과 동일한 구조를 가지고 있지만 주 신경망의 가중치가 정기적으로 업데이트되어 일치하도록 고정되어 있습니다. 에이전트를 위한 \"안정된 안내서\"로 생각해보세요. 주 신경망이 지속적으로 학습하고 업데이트되는 반면, 타겟 네트워크는 안정된 Q-값을 제공하여 훈련에 도움을 줍니다. 학습을 안정적이고 일관되게 유지하는 데 도움이 되는 신뢰할 수 있는, 주기적으로 업데이트되는 매뉴얼이 있는 것과 같습니다.\n\n<div class=\"content-ad\"></div>\n\n## 3.2: DQN 알고리즘\n\n이러한 구성 요소가 준비되면 DQN 알고리즘은 다음과 같은 몇 가지 중요한 단계로 개요를 제시할 수 있습니다:\n\n### 3.2.1: 순방향 전파\n\n먼저, 우리는 Q-values를 예측하는 데 중요한 순방향 전파로 시작합니다. 이러한 Q-values는 특정 상태에서 특정 행동을 취했을 때 기대되는 미래 보상을 저장합니다. 이 프로세스는 상태 입력부터 시작됩니다.\n\n#### 상태 입력\n\n에이전트는 환경에서 현재 상태 s를 관찰합니다. 이 상태는 에이전트의 현재 상황을 설명하는 특징 벡터로 표현됩니다. 상태를 에이전트 주변 세계의 스냅숏으로 생각해보세요. 눈이 주변을 둘러보는 것처럼 시각 장면을 촬영할 때와 유사합니다. 이 스냅숏에는 에이전트가 결정을 내리기 위해 필요한 모든 세부 정보가 포함되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\nQ-Value Prediction\n이제 이 관측된 상태 s가 신경망으로 전달됩니다. 이 신경망은 여러 층을 통해 이 입력을 처리하고 Q-값 세트를 출력합니다. 각 Q-값은 가능한 작업 a에 해당하며, 매개 변수 θ는 네트워크의 가중치와 편향을 나타냅니다.\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_4.png)\n\n신경망을 에이전트 뇌의 복잡한 의사 결정 기계로 상상해보세요. 캡처(상태)를 받으면 이 정보를 여러 단계(층)를 통해 처리하여 다양한 작업에 대한 잠재적 결과(Q-값)를 찾습니다. 보이는 것을 바탕으로 취할 수 있는 다양한 작업을 고려해보는 것과 유사합니다.\n\n작업 선택\n그런 다음 에이전트는 가장 높은 Q-값을 가진 작업 a∗를 다음 움직임으로 선택하며, 이에 따라 탐욕적 작업 선택 정책을 따릅니다:\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_5.png)\n\n이것은 모든 옵션을 심사한 후에 최선의 움직임을 결정하는 것과 유사합니다. 에이전트는 가장 높은 보상을 가져다줄 것으로 믿는 행동을 선택하며, 마치 당신이 보고 이해한 것을 기반으로 가장 유망한 길을 선택하는 것과 같습니다.\n\n3.2.2: 경험 재생\n다음으로, 우리는 학습 과정을 안정화하고 향상시키는 데 도움이 되는 경험 재생에 대해 이야기하겠습니다.\n\n경험 저장\n에이전트가 행동 a를 취하고 보상 r을 받은 후 새로운 상태 s′를 받으면, 이 경험을 (s, a, r, s′, done) 튜플로 저장하여 플레이백 버퍼에 저장합니다. 변수 done은 에피소드가 종료되었는지를 나타냅니다. 플레이백 버퍼를 에이전트가 경험을 기록하는 다이어리로 생각해보세요. 이는 당신이 하루 중 주목할 만한 사건을 메모하는 것과 유사합니다.\n\n<div class=\"content-ad\"></div>\n\n샘플 미니배치\n훈련 중에는 경험의 미니배치가 임의로 선택되어 재생 버퍼에서 샘플링됩니다. 이 배치는 타겟 Q-값을 계산하고 손실을 최소화하여 네트워크를 업데이트하는 데 사용됩니다. 에이전트가 훈련할 때, 과거 경험을 학습하기 위해 일기장의 임의의 페이지를 넘겨보게 됩니다. 이 임의 샘플링은 사건의 순서를 깨고 다양한 학습 예제를 제공하며, 일기의 서로 다른 날짜를 검토하여 보다 넓은 시야를 얻는 것과 유사한 역할을 합니다.\n\n3.2.3: 역전파\n최종 단계는 역전파로, 이는 네트워크를 업데이트하여 예측을 개선합니다.\n\n<div class=\"content-ad\"></div>\n\n타겟 Q-값 계산\n미니 배치의 각 경험에 대해, 에이전트는 타겟 Q-값 y\\_를 계산합니다. 만약 다음 상태 s′가 종료 상태(즉, done이 true인 경우)라면, 타겟 Q-값은 간단히 보상 r입니다. 그렇지 않으면, 타겟 Q-값은 보상에 다음 상태 s′에서 타겟 네트워크 Qtarget에 의해 예측된 할인된 최대 Q-값을 더한 값입니다:\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_7.png)\n\n여기서 γ는 할인 계수(0 ≤ γ ≤ 1)입니다. 이 단계는 과거 경험에 기반해 미래를 계획하는 것과 같습니다. 경험이 여행(에피소드)을 끝낼 경우, 타겟은 받은 보상입니다. 계속된다면, 타겟에는 즉시와 미래 혜택을 모두 고려하여 행동을 계획하는 방식과 유사한 예상 미래 보상이 포함됩니다.\n\n손실 계산\n다음으로, 손실은 메인 네트워크에서 예측된 Q-값 Q(s_i, a_i; θ)과 타겟 Q-값 yi 사이의 평균 제곱 오차로 계산됩니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_8.png)\n\n손실을 계산하는 것은 예측이 실제 결과와 얼마나 차이가 나는지를 평가하는 것과 같습니다. 실제 결과와 비교하여 추측의 정확성을 확인하고 차이점을 주목하는 것과 같습니다.\n\n역전파 및 최적화\n마지막으로, 이 손실을 최소화하기 위해 역전파를 수행합니다. 계산된 손실은 네트워크를 통해 역전파되어 SGD(Stochastic Gradient Descent) 또는 Adam과 같은 최적화 알고리즘을 사용하여 가중치를 업데이트합니다. 이 프로세스는 손실을 최소화하기 위해 네트워크 매개변수 θ를 조정합니다:\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_9.png)\n\n\n<div class=\"content-ad\"></div>\n\n여기서 α는 학습률을 나타내고, ∇θLoss는 네트워크 매개변수에 대한 손실의 그래디언트를 나타냅니다. 역전파는 실수로부터 배우는 것과 같습니다. 예측이 얼마나 잘못되었는지를 깨달았을 때 (손실), 전략(네트워크 가중치)을 조정하여 미래의 결정을 개선합니다. 피드백을 바탕으로 자신의 접근 방식을 미세 조정하여 다음에 더 나은 결과를 얻는 것과 비슷합니다.\n\n이 아키텍처를 사용하여 에이전트는 정책을 반복적으로 개선합니다. 시간이 지남에 따라 누적 보상을 극대화하는 조치를 취하는 것을 배웁니다. 신경망, 경험 재생 및 타겟 네트워크의 결합으로 DQN은 복잡한 고차원 환경에서 효과적으로 학습할 수 있습니다. 이 과정은 에이전트가 환경을 탐색하는 데 능숙해질 때까지 계속됩니다.\n\n# 4: 처음부터 Deep Q-Network 구현\n\n이 섹션에서는 처음부터 Deep Q-Network (DQN)의 구현을 안내합니다. 이 섹션의 끝에는 Python에서 DQN을 구축하고 훈련하는 방법을 명확히 이해하게 될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 OpenAI Gym의 Lunar Lander 환경을 사용할 것입니다. 이 환경에서의 목표는 달 착륙선을 조종하여 지정된 착륙 패드에 성공적으로 착륙하는 것입니다. 착륙선은 환경을 통해 비행할 때 추진기를 사용하여 움직임과 방향을 조절해야 합니다. 이 환경은 상업적으로 사용할 수 있습니다. 라이센스 및 사용 권한에 대한 자세한 내용은 OpenAI Gym GitHub 페이지에서 확인할 수 있습니다.\n\n오늘 다룰 모든 코드는 여기에서 찾을 수 있습니다:\n\n## 4.1: 환경 설정\n\n우리는 OpenAI Gym의 LunarLander 환경을 사용할 것이며, 이는 우리의 에이전트가 해결해야 할 어려운 문제를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nimport os\nimport pickle\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport random\nimport optuna\n```\n\n여기서 필요한 라이브러리들을 import 합니다. gym은 환경을 위해 사용되며, torch는 우리의 신경망을 구축하고 훈련하는 데 사용되며, collections, random, 및 optuna는 경험 재생과 하이퍼파라미터 최적화에 도움이 됩니다.\n\n```js\nenv = gym.make(\"LunarLander-v2\", (render_mode = \"rgb_array\"));\nstate_dim = env.observation_space.shape[0];\naction_dim = env.action_space.n;\n```\n\n우리는 LunarLander 환경을 초기화하고 상태 및 액션 공간의 차원을 가져옵니다. state_dim은 상태의 특징 수를 나타내고, action_dim은 가능한 액션 수를 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n## 4.2: 딥 신경망 구축\n\n우리의 딥-NN에서는 DQN이라는 클래스를 생성할 것입니다. 이 클래스는 세 개의 완전 연결 계층을 가진 신경망을 정의합니다. 입력 계층은 상태 표현을 수신하며, 은닉 계층은 이 정보를 선형 변환과 ReLU 활성화 함수를 통해 처리하고, 출력 계층은 각 가능한 동작에 대한 Q-값을 생성합니다.\n\n먼저 코드를 확인한 다음 분석해 봅시다:\n\n```js\nclass DQN(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, action_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n```\n\n<div class=\"content-ad\"></div>\n\n4.2.1: 클래스 초기화\n\n```python\nclass DQN(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, action_dim)\n```\n\n우리는 DQN이라는 클래스를 정의했습니다. 이 클래스는 PyTorch의 모든 신경망 모듈에 사용되는 기본 클래스인 nn.Module을 상속받습니다. 이를 통해 우리는 PyTorch의 내장 함수와 기능을 활용할 수 있습니다.\n\n**init** 메서드는 객체의 속성을 초기화하는 특별한 메서드입니다. 우리의 경우에는 신경망의 레이어를 설정하게 됩니다. 완전 연결층 (Fully Connected Layers):\n\n<div class=\"content-ad\"></div>\n\n우리는 세 개의 완전 연결 (선형) 레이어를 정의합니다:\n\n- self.fc1 = nn.Linear(state_dim, 128): 첫 번째 레이어는 입력 상태 차원 (상태의 피쳐 수)을 받아서 128개의 뉴런으로 매핑합니다.\n- self.fc2 = nn.Linear(128, 128): 두 번째 레이어는 첫 번째 레이어에서 나온 128개의 뉴런을 또 다른 128개의 뉴런으로 매핑합니다.\n- self.fc3 = nn.Linear(128, action_dim): 세 번째 레이어는 두 번째 레이어에서 나온 128개의 뉴런을 액션 차원 (가능한 액션 수)으로 매핑합니다.\n\n각 nn.Linear 레이어는 입력 데이터에 대해 선형 변환을 수행합니다:\n\n![이미지](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_10.png)\n\n<div class=\"content-ad\"></div>\n\n4.2.2: Forward Method\n앞서 명시된 forward 메소드는 데이터가 네트워크를 통해 흐르는 방법을 정의합니다. 이 방법은 네트워크를 통해 데이터를 전달할 때 자동으로 호출됩니다.\n\n```python\ndef forward(self, x):\n    x = torch.relu(self.fc1(x))\n    x = torch.relu(self.fc2(x))\n    return self.fc3(x)\n```\n\n첫 번째 layer에서 입력 데이터 x는 첫 번째 fully connected layer (self.fc1)를 통해 전달됩니다. 그런 다음 출력은 ReLU (Rectified Linear Unit) 활성화 함수를 사용하여 변환됩니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nx = torch.relu(self.fc1(x));\n```\n\nReLU(Recified Linear Unit) 활성화 함수는 다음과 같이 정의됩니다:\n\n![ReLU activation function](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_11.png)\n\n모델에 비선형성을 도입하여 네트워크가 더 복잡한 기능을 학습할 수 있게합니다.\n\n<div class=\"content-ad\"></div>\n\n두 번째 레이어에서는 첫 번째 레이어의 출력이 두 번째 완전 연결 레이어 (self.fc2)를 통과하고 다시 ReLU 활성화 함수를 사용하여 변환됩니다:\n\n```js\nx = torch.relu(self.fc2(x));\n```\n\n마지막으로 출력 레이어에서는 두 번째 레이어의 출력이 활성화 함수 없이 세 번째 완전 연결 레이어 (self.fc3)를 통해 전달됩니다:\n\n```js\nreturn self.fc3(x);\n```\n\n<div class=\"content-ad\"></div>\n\n이 레이어는 각 액션에 대한 최종 Q-값을 생성합니다. 각 값은 해당 상태에서 그 액션을 취했을 때의 예상 미래 보상을 나타냅니다.\n\n## 4.3: 경험 재생 구현\n\nReplayBuffer 클래스는 경험을 저장하고 샘플링하는 메커니즘을 제공하여 DQN에서 학습 과정을 안정화하고 개선하는 데 필수적입니다. 따라서 에이전트가 다양한 과거 경험 세트로부터 학습할 수 있도록 해주어 일반화하고 복잡한 환경에서 잘 수행할 수 있는 능력을 향상시킵니다.\n\n```js\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n\n    def push(self, state, action, reward, next_state, done):\n        self.buffer.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n        return state, action, reward, next_state, done\n\n    def __len__(self):\n        return len(self.buffer)\n```\n\n<div class=\"content-ad\"></div>\n\n4.3.1: 클래스 초기화\n\n```js\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n```\n\n**init** 메서드는 고정된 용량을 갖는 deque(덱, 이중 연결 리스트)를 초기화합니다. 덱은 양쪽 끝에서 효율적으로 항목을 추가하고 제거할 수 있는 자료 구조입니다. 빠른 양쪽 끝에서의 추가와 제거가 필요한 큐(queue)나 스택(stack)을 구현할 때 유용합니다.\n\nself.buffer = deque(maxlen=capacity)는 capacity만큼의 경험을 저장할 수 있는 deque를 생성합니다. 버퍼가 가득 차면 새로운 경험을 추가하면 가장 오래된 경험이 자동으로 제거됩니다.\n\n<div class=\"content-ad\"></div>\n\n4.3.2: Push Method\n\n```python\ndef push(self, state, action, reward, next_state, done):\n    self.buffer.append((state, action, reward, next_state, done))\n```\n\n푸시 메서드는 버퍼에 새로운 경험을 추가합니다. 각 경험은 상태(state), 액션(action), 보상(reward), 다음 상태(next_state), 완료 여부(done)로 구성된 튜플입니다:\n\n- state: 현재 상태.\n- action: 에이전트가 취한 행동.\n- reward: 행동을 취한 후 받은 보상.\n- next_state: 행동을 취한 후 에이전트가 이동한 상태.\n- done: 에피소드가 종료되었는지를 나타내는 부울 값.\n\n<div class=\"content-ad\"></div>\n\n4.3.3: 샘플 메서드\n\n```python\ndef sample(self, batch_size):\n    state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n    return state, action, reward, next_state, done\n```\n\n샘플 메서드는 버퍼에서 무작위로 일괄 경험을 검색합니다.\n\nrandom.sample(self.buffer, batch_size)는 버퍼에서 batch_size개의 경험을 무작위로 선택합니다.\n\n<div class=\"content-ad\"></div>\n\n\"zip(\\*random.sample(self.buffer, batch_size))\"은 경험 목록을 상태, 행동, 보상, 다음 상태 및 완료에 대한 별도의 튜플로 풀어낼 수 있습니다.\n\n이 메서드는 샘플된 경험들로 이러한 튜플을 반환합니다.\n\n4.3.4: Length Method\n\n```python\ndef __len__(self):\n    return len(self.buffer)\n```\n\n<div class=\"content-ad\"></div>\n\n**len** 메서드는 버퍼에 저장된 현재 경험 수를 반환합니다.\n\n## 4.4: 타겟 네트워크 구현\n\n타겟 네트워크를 통해 안정적인 Q 값 세트를 제공하여 훈련을 위해 학습 프로세스를 안정화하고 복잡한 환경에서 에이전트의 성능을 향상시킵니다. 타겟 네트워크는 주 네트워크보다 덜 자주 업데이트되어 메인 네트워크의 가중치를 업데이트하는 데 사용되는 Q 값 추정치가 더 안정적임을 보장합니다.\n\nDQNTrainer라는 클래스 내에 타겟 네트워크를 구현할 것이며, 이 클래스는 DQN의 훈련 프로세스를 관리하고 주 및 타겟 네트워크, 옵티마이저 및 재생 버퍼를 포함합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nclass DQNTrainer:\n    def __init__(self, env, main_network, target_network, optimizer, replay_buffer, model_path='model/model.pth', gamma=0.99, batch_size=64, target_update_frequency=1000):\n        self.env = env\n        self.main_network = main_network\n        self.target_network = target_network\n        self.optimizer = optimizer\n        self.replay_buffer = replay_buffer\n        self.model_path = model_path\n        self.gamma = gamma\n        self.batch_size = batch_size\n        self.target_update_frequency = target_update_frequency\n        self.step_count = 0\n```\n\n- DQNTrainer 클래스 정의:\n\n<div class=\"content-ad\"></div>\n\n**init** 메서드는 학습에 필요한 다양한 구성 요소를 초기화합니다:\n\n- env: 에이전트가 작동하는 환경입니다.\n- main_network: 훈련 중인 주요 신경망입니다.\n- target_network: Q-값 추정을 안정화하는 데 사용되는 대상 신경망입니다.\n- optimizer: 주요 신경망의 가중치를 업데이트하는 데 사용되는 옵티마이저입니다.\n- replay_buffer: 경험을 저장하고 샘플링하는 버퍼입니다.\n- model_path: 훈련된 모델을 저장하고 로드하기 위한 경로입니다.\n- gamma: 미래 보상에 대한 할인 계수입니다.\n- batch_size: 각 훈련 단계에서 재생 버퍼에서 샘플링된 경험의 수입니다.\n- target_update_frequency: 대상 네트워크의 가중치를 주요 네트워크의 가중치에 맞게 업데이트하는 빈도입니다.\n- step_count: 훈련 중에 취한 단계 수를 추적하는 카운터입니다.\n\n  4.4.2: 모델 로딩\n\n```js\n# 모델이 있으면 로드\n        if os.path.exists(os.path.dirname(self.model_path)):\n            if os.path.isfile(self.model_path):\n                self.main_network.load_state_dict(torch.load(self.model_path))\n                self.target_network.load_state_dict(torch.load(self.model_path))\n                print(\"디스크에서 모델 로드됨\")\n        else:\n            os.makedirs(os.path.dirname(self.model_path))\n```\n\n<div class=\"content-ad\"></div>\n\n우리는 모델 경로의 디렉토리가 존재하는지 os.path.exists(os.path.dirname(self.model_path))를 사용하여 확인합니다. 저장된 모델이 있으면, 훈련을 멈춘 지점부터 계속하기 위해 불러옵니다:\n\n```js\nif os.path.isfile(self.model_path):\n    self.main_network.load_state_dict(torch.load(self.model_path))\n    self.target_network.load_state_dict(torch.load(self.model_path))\n    print(\"디스크에서 모델을 불러왔습니다\")\n```\n\ntorch.load는 load_state_dict를 사용하여 저장된 모델 가중치를 메인 및 타겟 네트워크에 불러옵니다. 모델 디렉토리가 존재하지 않는 경우, os.makedirs를 사용하여 만듭니다.\n\n## 4.5: 딥 Q-네트워크 훈련\n\n<div class=\"content-ad\"></div>\n\n다음으로, 우리는 DQN을 훈련하기 위한 학습 루프를 구현할 것입니다. 이 방법은 DQNTrainer 내에 이루어집니다. DQN을 위한 훈련 루프를 실행하며, 에이전트가 환경과 상호 작용하고 경험을 수집하며 네트워크를 업데이트하고 성능을 추적합니다.\n\n다음은 학습 루프의 코드입니다:\n\n```js\ndef train(self, num_episodes, save=True):\n    total_rewards = []\n    for episode in range(num_episodes):\n        state, _ = self.env.reset()\n        done = False\n        total_reward = 0\n\n        while not done:\n            self.env.render()  # 환경을 렌더링하기 위해 이 줄을 추가합니다\n            action = self.main_network(torch.FloatTensor(state).unsqueeze(0)).argmax(dim=1).item()\n            next_state, reward, done, _, _ = self.env.step(action)\n            self.replay_buffer.push(state, action, reward, next_state, done)\n            state = next_state\n            total_reward += reward\n\n            if len(self.replay_buffer) >= self.batch_size:\n                self.update_network()\n\n        total_rewards.append(total_reward)\n        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n\n    if save:\n        torch.save(self.main_network.state_dict(), self.model_path)\n        print(\"모델을 디스크에 저장했습니다\")\n\n    self.env.close()\n    return sum(total_rewards) / len(total_rewards)\n```\n\ntrain 메서드는 지정된 에피소드 수에 대해 훈련 루프를 실행합니다. 이 루프는 에이전트가 경험을 쌓고 의사 결정 능력을 향상시키는 데 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n4.5.1: 훈련 루프\n우선 total_rewards를 빈 리스트로 초기화해 봅시다:\n\n```js\ntotal_rewards = [];\n```\n\n이제 훈련 루프를 만들어 봅시다:\n\n```js\nfor episode in range(num_episodes):\n```\n\n<div class=\"content-ad\"></div>\n\n이 루프는 지정된 에피소드 수만큼 실행됩니다. 각 에피소드는 환경과의 완전한 상호작용 순서를 나타냅니다.\n\n4.5.2: 환경 재설정\n각 에피소드의 시작 시점에는 환경이 초기 상태로 재설정됩니다.\n\n```js\nstate, (_ = self.env.reset());\ndone = False;\ntotal_reward = 0;\n```\n\n- self.env.reset()은 환경을 초기화하고 초기 상태를 반환합니다.\n- done = False는 에피소드가 완료되지 않았음을 나타냅니다.\n- total_reward = 0은 현재 에피소드의 총 보상을 초기화합니다.\n\n<div class=\"content-ad\"></div>\n\n### 4.4.3: Action Selection\n\n에이전트는 현재 상태를 기반으로 메인 네트워크를 사용하여 작업을 선택합니다.\n\n```python\naction = self.main_network(torch.FloatTensor(state).unsqueeze(0)).argmax(dim=1).item()\n```\n\ntorch.FloatTensor(state).unsqueeze(0)은 상태를 PyTorch 텐서로 변환하고 네트워크가 예상하는 입력 형태와 일치하도록 추가 차원을 추가합니다.\n\nself.main_network(...).argmax(dim=1).item()는 메인 네트워크가 예측한 가장 높은 Q 값으로 작업을 선택합니다.\n\n<div class=\"content-ad\"></div>\n\n4.5.4: 단계 및 저장 환경\n에이전트가 선택한 동작을 수행하고 보상 및 다음 상태를 관찰한 후, 해당 경험을 재생 버퍼에 저장합니다.\n\n```js\nnext_state, reward, done, _, (_ = self.env.step(action));\nself.replay_buffer.push(state, action, reward, next_state, done);\nstate = next_state;\ntotal_reward += reward;\n```\n\n- self.env.step(action)은 동작을 실행하고 다음 상태, 보상 및 에피소드 완료 여부를 반환합니다.\n- self.replay_buffer.push(...)는 재생 버퍼에 경험을 저장합니다.\n- state = next_state는 현재 상태를 다음 상태로 업데이트합니다.\n- total_reward += reward은 현재 에피소드의 보상을 누적합니다.\n\n  4.5.5: 네트워크 업데이트\n  재생 버퍼에 충분한 경험이 있을 경우, 네트워크가 업데이트됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nif len(self.replay_buffer) >= self.batch_size:\n    self.update_network()\n```\n\n`if len(self.replay_buffer) >= self.batch_size`은 replay buffer가 적어도 batch_size의 경험을 가지고 있는지 확인합니다.\n\nself.update_network()은 replay buffer에서 일괄적인 경험을 사용하여 네트워크를 업데이트합니다.\n\n4.5.6: 에피소드 종료\n총 보상은 각 에피소드의 끝에서 기록되고 출력됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ntotal_rewards.append(total_reward)\nprint(f\"에피소드 {episode}, 총 보상: {total_reward}\")\n```\n\ntotal_rewards.append(total_reward)는 현재 에피소드의 총 보상을 총 보상 목록에 추가합니다.\n\nprint(f\"에피소드 {episode}, 총 보상: {total_reward}\")은 에피소드 번호와 총 보상을 출력합니다.\n\n4.5.7: 모델 저장\n훈련 후, 모델은 디스크에 저장됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nsave가 True이면:\n   torch.save(self.main_network.state_dict(), self.model_path)\n   print(\"모델이 디스크에 저장되었습니다.\")\n```\n\nif save:는 save 플래그가 True인지 확인합니다.\n\ntorch.save(self.main_network.state_dict(), self.model_path)는 메인 네트워크의 상태 딕셔너리를 지정된 파일 경로에 저장합니다.\n\n4.5.8: 평균 보상 반환\n마지막으로, 이 메서드는 환경을 닫고 모든 에피소드에 대한 평균 보상을 반환합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nself.env.close();\nreturn sum(total_rewards) / len(total_rewards);\n```\n\nself.env.close()는 환경을 닫습니다.\n\nreturn sum(total_rewards) / len(total_rewards)는 평균 보상을 계산하고 반환합니다.\n\n## 4.6: 모델 튜닝\n\n<div class=\"content-ad\"></div>\n\n마침내 훈련된 모델을 평가하고 튜닝하는 방법을 살펴보겠습니다. DQN의 성능을 향상시키기 위해 하이퍼파라미터를 최적화할 책임을 가질 Optimizer 클래스를 만들어보겠습니다.\n\n```js\nclass Optimizer:\n    def __init__(self, env, main_network, target_network, replay_buffer, model_path, params_path='params.pkl'):\n        self.env = env\n        self.main_network = main_network\n        self.target_network = target_network\n        self.replay_buffer = replay_buffer\n        self.model_path = model_path\n        self.params_path = params_path\n\n    def objective(self, trial, n_episodes=10):\n        lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n        gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n        target_update_frequency = trial.suggest_categorical('target_update_frequency', [500, 1000, 2000])\n\n        optimizer = optim.Adam(self.main_network.parameters(), lr=lr)\n        trainer = DQNTrainer(self.env, self.main_network, self.target_network, optimizer, self.replay_buffer, self.model_path, gamma=gamma, batch_size=batch_size, target_update_frequency=target_update_frequency)\n        reward = trainer.train(n_episodes, save=False)\n        return reward\n\n    def optimize(self, n_trials=100, save_params=True):\n        if not TRAIN and os.path.isfile(self.params_path):\n            with open(self.params_path, 'rb') as f:\n                best_params = pickle.load(f)\n            print(\"디스크에서 매개변수를 불러왔습니다\")\n        elif not FINETUNE:\n            best_params = {\n                'lr': LEARNING_RATE,\n                'gamma': GAMMA,\n                'batch_size': BATCH_SIZE,\n                'target_update_frequency': TARGET_UPDATE_FREQUENCY\n                }\n            print(f\"기본 매개변수 사용 중: {best_params}\")\n        else:\n            print(\"하이퍼파라미터 최적화 중\")\n            study = optuna.create_study(direction='maximize')\n            study.optimize(self.objective, n_trials=n_trials)\n            best_params = study.best_params\n\n            if save_params:\n                with open(self.params_path, 'wb') as f:\n                    pickle.dump(best_params, f)\n                print(\"매개변수를 디스크에 저장했습니다\")\n\n        return best_params\n```\n\n4.6.1: 클래스 정의\n\n```js\nclass Optimizer:\n    def __init__(self, env, main_network, target_network, replay_buffer, model_path, params_path='params.pkl'):\n        self.env = env\n        self.main_network = main_network\n        self.target_network = target_network\n        self.replay_buffer = replay_buffer\n        self.model_path = model_path\n        self.params_path = params_path\n```\n\n<div class=\"content-ad\"></div>\n\n**init** 메서드는 최적화에 필요한 다양한 구성 요소를 초기화합니다:\n\n- env: 에이전트가 작동하는 환경.\n- main_network: 주요 신경망.\n- target_network: 타겟 신경망.\n- replay_buffer: 경험을 저장하고 샘플링하는 버퍼.\n- model_path: 훈련된 모델을 저장하거나 불러오는 경로.\n- params_path: 최적의 하이퍼파라미터를 저장하거나 불러오는 경로.\n\n  4.6.2: 목적 메서드\n\n```js\ndef objective(self, trial, n_episodes=10):\n        lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n        gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n        target_update_frequency = trial.suggest_categorical('target_update_frequency', [500, 1000, 2000])\n\n        optimizer = optim.Adam(self.main_network.parameters(), lr=lr)\n        trainer = DQNTrainer(self.env, self.main_network, self.target_network, optimizer, self.replay_buffer, self.model_path, gamma=gamma, batch_size=batch_size, target_update_frequency=target_update_frequency)\n        reward = trainer.train(n_episodes, save=False)\n        return reward\n```\n\n<div class=\"content-ad\"></div>\n\n목표 함수는 하이퍼파라미터에 대한 값을 제안하고 이러한 값을 사용하여 모델을 훈련합니다.\n\n- lr = trial.suggest_loguniform(`lr`, 1e-5, 1e-1): 범위 [1e-5, 1e-1] 내의 학습률을 제안합니다.\n- gamma = trial.suggest_uniform(`gamma`, 0.9, 0.999): 범위 [0.9, 0.999] 내의 할인 요인을 제안합니다.\n- batch_size = trial.suggest_categorical(`batch_size`, [32, 64, 128]): 지정된 목록에서 배치 크기를 제안합니다.\n- target_update_frequency = trial.suggest_categorical(`target_update_frequency`, [500, 1000, 2000]): 지정된 목록에서 대상 업데이트 빈도를 제안합니다.\n\n```js\noptimizer = optim.Adam(self.main_network.parameters(), (lr = lr));\n```\n\n여기서는 주어진 학습률로 Adam 옵티마이저를 설정합니다. Adam은 주로 신경망 훈련에 사용되는 최적화 알고리즘인 Adaptive Moment Estimation의 약자입니다.\n\n<div class=\"content-ad\"></div>\n\n신경망에서 각 매개변수에 대해 Adam은 손실 함수의 기울기를 계산합니다. 그것은 기울기의 지수 이동 평균 (첫 번째 모멘트로 표시되는 m)과 제곱 기울기 (두 번째 모멘트로 표시되는 v)를 추적합니다.\n\n이동 평균의 초기화 편향을 고려하기 위해 Adam은 첫 번째 및 두 번째 모멘트 추정치에 바이어스 보정을 적용합니다. 그런 다음 매개변수는 수정된 첫 번째 및 두 번째 모멘트를 사용하여 업데이트됩니다. 업데이트 규칙은 학습률과 모멘트를 통합하여 기울기의 크기와 방향을 모두 고려하는 방식으로 매개변수를 조정합니다.\n\n다음은 Adam에 대한 보다 포괄적인 기사입니다:\n\n```js\ntrainer = DQNTrainer(\n  self.env,\n  self.main_network,\n  self.target_network,\n  optimizer,\n  self.replay_buffer,\n  self.model_path,\n  (gamma = gamma),\n  (batch_size = batch_size),\n  (target_update_frequency = target_update_frequency)\n);\n```\n\n<div class=\"content-ad\"></div>\n\n이 코드는 제안된 하이퍼파라미터로 theDQNTrainer 인스턴스를 초기화합니다.\n\n```js\nreward = trainer.train(n_episodes, (save = False));\n```\n\n마지막으로, 이 코드는 지정된 에피소드 수로 모델을 학습하고 평균 보상을 반환합니다.\n\n4.6.3: 최적화 메소드\n이 섹션에서는 모델의 성능을 극대화하는 조합을 효율적으로 찾을 수 있도록 하이퍼파라미터 공간을 체계적으로 탐색하는 파이썬 라이브러리인 Optuna를 사용하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n```python\ndef optimize(self, n_trials=100, save_params=True):\n    if not TRAIN and os.path.isfile(self.params_path):\n        with open(self.params_path, 'rb') as f:\n            best_params = pickle.load(f)\n        print(\"디스크에서 매개변수를 불러왔습니다.\")\n    elif not FINETUNE:\n        best_params = {\n            'lr': LEARNING_RATE,\n            'gamma': GAMMA,\n            'batch_size': BATCH_SIZE,\n            'target_update_frequency': TARGET_UPDATE_FREQUENCY\n        }\n        print(f\"기본 매개변수를 사용합니다: {best_params}\")\n    else:\n        print(\"하이퍼파라미터를 최적화 중입니다.\")\n        study = optuna.create_study(direction='maximize')\n        study.optimize(self.objective, n_trials=n_trials)\n        best_params = study.best_params\n\n        if save_params:\n            with open(self.params_path, 'wb') as f:\n                pickle.dump(best_params, f)\n            print(\"매개변수를 디스크에 저장했습니다.\")\n\n    return best_params\n```\n\n`optimize` 메소드는 지정된 횟수의 시도에 대해 최적화 프로세스를 실행합니다.\n\n```python\nif not TRAIN and os.path.isfile(self.params_path):\n        with open(self.params_path, 'rb') as f:\n            best_params = pickle.load(f)\n        print(\"디스크에서 매개변수를 불러왔습니다.\")\n```\n\n학습이 필요하지 않은 경우 (TRAIN이 아닌 경우) 및 매개변수 파일이 존재하는 경우, 매개변수가 디스크에서 로드됩니다.\n\n<div class=\"content-ad\"></div>\n\n```python\nelif not FINETUNE:\n    best_params = {\n        'lr': LEARNING_RATE,\n        'gamma': GAMMA,\n        'batch_size': BATCH_SIZE,\n        'target_update_frequency': TARGET_UPDATE_FREQUENCY\n    }\n    print(f\"기본 매개변수 사용 중: {best_params}\")\n```\n\n만약 파라미터 튜닝이 필요하지 않다면 (not FINETUNE), 기본 매개변수가 사용됩니다.\n\n```python\nelse:\n    print(\"하이퍼파라미터 최적화 중\")\n    study = optuna.create_study(direction='maximize')\n    study.optimize(self.objective, n_trials=n_trials)\n    best_params = study.best_params\n\n    if save_params:\n        with open(self.params_path, 'wb') as f:\n            pickle.dump(best_params, f)\n        print(\"매개변수를 디스크에 저장했습니다\")\n```\n\n하이퍼파라미터 최적화가 필요한 경우, Optuna를 사용하여 최적의 매개변수를 찾습니다.\n\n<div class=\"content-ad\"></div>\n\nstudy = optuna.create_study(direction='maximize')을 사용하면 목적 함수를 최대화하는 Optuna 스터디를 생성할 수 있어요.\n\nstudy.optimize(self.objective, n_trials=n_trials)은 지정된 횟수의 시행을 위한 최적화를 실행해요.\n\nsave_params를 True로 설정하면, 최적의 매개변수가 디스크에 저장돼요.\n\n다음은 Optuna를 깊이 들여다보는 포함한 다양한 세밀 조정 기법을 탐구한 멋진 기사에요:\n\n<div class=\"content-ad\"></div>\n\n## 4.7: 모델 실행하기\n\n마지막으로, 모든 과정을 다시 한번 확인하고 코드를 실행해 봅시다!\n\n4.7.1: 훈련 및 파인튜닝 설정\n\n```js\nTRAIN = True\nFINETUNE = False\n\n# 다음 하이퍼파라미터를 설정하세요 (FINETUNE이 False인 경우)\nGAMMA = 0.99\nBATCH_SIZE = 64\nTARGET_UPDATE_FREQUENCY = 1000\nLEARNING_RATE = 1e-3\n```\n\n<div class=\"content-ad\"></div>\n\nTRAIN = True은 모델을 학습할지 여부를 나타냅니다. False로 설정하면 학습이 건너뛰어집니다.\n\nFINETUNE = False는 모델을 fine-tune할지 여부를 나타냅니다. True로 설정하면 기존 매개변수가 사용되고 fine-tune됩니다.\n\n만약 FINETUNE이 False인 경우, 다음 하이퍼파라미터를 설정합니다:\n\n- GAMMA = 0.99: 미래 보상에 대한 할인 계수입니다. 이는 즉시 보상에 비해 미래 보상이 얼마나 중요한지를 결정합니다.\n- BATCH_SIZE = 64: 각 학습 단계마다 재생 버퍼에서 샘플링된 경험의 수입니다.\n- TARGET_UPDATE_FREQUENCY = 1000: 타겟 네트워크의 가중치가 주요 네트워크의 가중치와 일치하도록 업데이트되는 빈도(스텝 단위).\n- LEARNING_RATE = 1e-3: 최적화기(optimizer)의 학습률로, 모델 가중치가 업데이트될 때 추정 오차에 따라 모델을 얼마나 변경할지를 제어합니다.\n\n<div class=\"content-ad\"></div>\n\n4.7.2: 네트워크 및 재생 버퍼 초기화\n\n```js\nmain_network = DQN(state_dim, action_dim);\ntarget_network = DQN(state_dim, action_dim);\ntarget_network.load_state_dict(main_network.state_dict());\ntarget_network.eval();\n\nreplay_buffer = ReplayBuffer(10000);\n```\n\nmain_network = DQN(state_dim, action_dim)은 지정된 상태 및 액션 차원으로 메인 네트워크를 초기화합니다.\n\ntarget_network = DQN(state_dim, action_dim)은 메인 네트워크와 동일한 구조로 대상 네트워크를 초기화합니다.\n\n<div class=\"content-ad\"></div>\n\n\ntarget_network.load_state_dict(main_network.state_dict()) 함수는 메인 네트워크의 가중치를 타겼 네트워크로 복사합니다.\n\ntarget_network.eval() 함수는 타겟 네트워크를 평가 모드로 설정합니다. 이는 추론 중에 드롭아웃과 배치 정규화와 같은 특정 레이어가 적절하게 동작하도록 합니다.\n\nreplay_buffer = ReplayBuffer(10000)은 10,000개의 경험을 저장할 수 있는 용량을 가진 리플레이 버퍼를 초기화합니다.\n\n4.7.3: 단계 카운트 설정\n\n\n<div class=\"content-ad\"></div>\n\n```js\nSTEP_COUNT = 0;\n```\n\nSTEP_COUNT = 0은 훈련 중 취한 단계 수를 추적하는 카운터를 초기화합니다.\n\n4.7.4: 옵티마이저 초기화 및 하이퍼파라미터 최적화\n\n```js\noptimizer = Optimizer(env, main_network, target_network, replay_buffer, f'{os.path.dirname(__file__)}/model/model.pth', f'{os.path.dirname(__file__)}/model/params.pkl')\nbest_params = optimizer.optimize(n_trials=2, save_params=True)\n```\n\n<div class=\"content-ad\"></div>\n\n`optimizer = Optimizer(...)`은 환경, 네트워크, 리플레이 버퍼, 모델 경로 및 매개변수 경로로 Optimizer 클래스를 초기화합니다.\n\n`best_params = optimizer.optimize(n_trials=2, save_params=True)`는 최적의 하이퍼파라미터를 찾기 위해 최적화 프로세스를 실행합니다. 이 함수는 다음과 같은 기능을 수행합니다:\n\n- 지정된 횟수(n_trials=2)만큼 최적화를 실행합니다.\n- `save_params`가 True인 경우 최적의 하이퍼파라미터를 디스크에 저장합니다.\n\n  4.7.5: PyTorch Optimizer 및 DQN Trainer 생성\n\n<div class=\"content-ad\"></div>\n\n```js\noptimizer = optim.Adam(main_network.parameters(), lr=best_params['lr'])\ntrainer = DQNTrainer(env, main_network, target_network, optimizer, replay_buffer, f'{os.path.dirname(__file__)}/model/model.pth', gamma=best_params['gamma'], batch_size=best_params['batch_size'], target_update_frequency=best_params['target_update_frequency'])\ntrainer.train(1000)\n```\n\n`optimizer = optim.Adam(main_network.parameters(), lr=best_params['lr'])`은 최적의 하이퍼파라미터에서 학습률을 사용하여 Adam 옵티마이저를 생성합니다.\n\n`trainer = DQNTrainer(...)`는 환경, 네트워크, 옵티마이저, 리플레이 버퍼, 모델 경로 및 최적의 하이퍼파라미터로 DQNTrainer 클래스를 초기화합니다.\n\n`trainer.train(1000)`은 모델을 1000번의 에피소드 동안 훈련합니다.\n\n<div class=\"content-ad\"></div>\n\n이제 요정의 훈련 초기 10 에피소드를 살펴보겠습니다:\n\n![agent training](https://miro.medium.com/v2/resize:fit:1200/1*ncnLXIRABedg4uKwVL0L5w.gif)\n\n여기서 모델은 서툴러서 무작위로 종종 비최적적인 결정을 내립니다. 요정이 환경을 탐험하고 기초를 배우기 때문에 이는 예상되는 현상입니다. 아직 보상을 극대화하기 위한 견고한 전략을 개발하지 못했습니다. 추가적인 훈련 에피소드를 거치면서 시간이 지남에 따라, 요정의 성능은 정책을 미세 조정하고 경험을 통해 배우면서 크게 향상되어야 합니다.\n\n이제 모델이 1000번 훈련된 후의 10개의 훈련 에피소드를 살펴봅시다:\n\n<div class=\"content-ad\"></div>\n\n\n![image](https://miro.medium.com/v2/resize:fit:1200/1*s4j6V4V-nLfkc18C2Z2-zA.gif)\n\n이것은 주목할 만한 개선입니다. 모델이 아직 NASA에 완성되지는 않았지만, 몇 가지 주요 향상 사항을 관찰할 수 있습니다:\n\n- 에이전트가 더 신중하고 전략적인 결정을 내립니다.\n- 환경을 더 효율적으로 탐색합니다.\n- 부적절한 조치의 빈도가 크게 감소했습니다.\n\n지속적인 훈련과 세밀한 조정을 통해, 에이전트의 성능은 더 개선될 것으로 예상되며, 최적의 행동에 더 가까워질 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n이제 여러분의 차례입니다. 모델을 더 발전시켜 보세요. 하이퍼파라미터를 조정하거나 다른 모델 구조를 실험해 보세요. 창의성과 인내심으로 최선을 다하면 얼마든지 성과를 낼 수 있을 거에요. 곧 완벽하게 패치된 셔틀은 원활하게 착륙할 거예요!\n\n# 5: 결론\n\n딥 Q-네트워크를 구축, 훈련 및 평가하는 방법을 잘 이해하셨으니, 이제 다양한 환경에서 이 DQN을 테스트하고 다양한 도전에 적응하는 모습을 관찰해 보세요.\n\n에이전트의 성능을 향상시키기 위해 고급 기술을 구현하고 새로운 아키텍처를 탐험해 보세요. 예를 들어 다양한 하이퍼파라미터를 설정해보거나 다른 최적화 알고리즘(예: SGD 또는 Nadam)을 사용하거나 다른 미세조정 알고리즘을 사용해 볼 수 있어요!\n\n<div class=\"content-ad\"></div>\n\n# 참고 자료\n\n- Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT Press.\n- Lin, L. J. (1992). “Self-improving reactive agents based on reinforcement learning, planning and teaching.” Machine Learning, 8(3–4), 293–321.\n- OpenAI. “LunarLander-v2.” OpenAI Gym. [링크](https://gym.openai.com/envs/LunarLander-v2/)\n- 버클리 AI 연구소 (BAIR). “Experience Replay.” [링크](https://bair.berkeley.edu/blog/2020/03/20/experiencereplay/)\n- Towards Data Science. “Reinforcement Learning 101: Q-Learning.” [링크](https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292)\n- Towards Data Science. “신경망 뒤의 수학.” [링크](https://towardsdatascience.com/the-math-behind-neural-networks-3a18b7f8d8dc)\n- Towards Data Science. “Adam Optimizer 뒤의 수학.” [링크](https://towardsdatascience.com/the-math-behind-adam-optimizer-3a18b7f8d8dc)\n- Towards Data Science. “Deep Neural Networks 맞춤화 뒤의 수학.” [링크](https://towardsdatascience.com/the-math-behind-fine-tuning-deep-neural-networks-3a18b7f8d8dc)\n\n마지막까지 읽어 주셔서 축하드립니다! 이 기사가 유익하고 즐거우셨기를 바랍니다. 만약 그렇다면, 박수를 남기고 더 많은 이런 기사를 보고 싶다면 저를 팔로우해 주세요. 앞으로 다루었으면 하는 주제나 기사에 대한 의견을 들을 수 있습니다. 피드백과 지원에 감사드립니다. 읽어 주셔서 감사합니다!\n","ogImage":{"url":"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_0.png"},"coverImage":"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_0.png","tag":["Tech"],"readingTime":35},{"title":"딥 러닝을 이용한 우울증 예측 NLP 기초 배우기","description":"","date":"2024-05-27 14:08","slug":"2024-05-27-DepressionpredictionusingDeepLearninglearnbasicsofNLP","content":"\n<table>\n  <tr>\n    <td><img src=\"/assets/img/2024-05-27-DepressionpredictionusingDeepLearninglearnbasicsofNLP_0.png\" /></td>\n  </tr>\n</table>\n\n# 이 논문에서는 우리가 어떻게 NLP 애플리케이션을 쉽게 만들 수 있는지 보여드리고 싶습니다.\n\n이 논문에서는 NLP의 기본을 배우게 될 것입니다.\n다음은 여러분이 배우게 될 내용입니다:\n\n1 — Tokenizer가 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n2 — texts_to_sequences란 무엇인가요?\n\n3 — pad sequence란 무엇인가요?\n\n4 — Embedding이란 무엇인가요?\n\n5 — 예측 모델을 만들어 볼까요?\n\n이 코드는 텍스트 데이터를 포함하는 이진 NLP 데이터셋에 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 1 — 토크나이저란:\n\n기계는 숫자만 이해할 수 있다는 것을 우리 모두 알고 있습니다. 그래서 우리는 단어를 숫자로 변환해야 합니다.\n\n예를 들어, 만약 'hello world'를 기계가 이해할 수 있게 하려면 이렇게 숫자로 변환해야 합니다:\nhello는 0으로 표현\nworld는 1로 표현\n\n이를 수행하기 위해 토크나이저를 사용합니다.\n\n<div class=\"content-ad\"></div>\n\n토크나이저를 사용하여 단어, 하위 단어, 문자를 숫자로 변환하고 각 단어를 숫자로 변환한 것을 토큰이라고 합니다.\n\n요약하자면, 토크나이저는 텍스트를 토큰으로 변환합니다.\n\n## 데이터셋\n\n우선 데이터셋을 가져와야 합니다. 이 논문에서 사용된 데이터셋은 아래 링크를 통해 찾을 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n# 프로그래밍 해봐요 :))\n\n데이터셋을 위한 변수를 정의해볼게요 :\n\n```js\ndataset = pd.read_csv(\"D:ITML projectPredict depressiondepression_dataset_reddit_cleaned.csv\");\n```\n\n이제 문장과 레이블을 위한 변수 두 개를 정의해야 해요 :\n\n<div class=\"content-ad\"></div>\n\n```js\nsentences = dataset[\"clean_text\"];\nlabels = dataset[\"is_depression\"];\n```\n\n모델을 훈련하기 위해 훈련 데이터와 모델을 시험하고 최적화하는 테스트 데이터가 필요합니다.\n\n따라서 이제 데이터를 두 부분, 훈련 및 테스트용으로 분리해야 합니다.\n\n데이터는 7731개의 행(샘플)을 포함하고 있으며, 0부터 6000까지는 훈련 데이터로 정의하며, 즉 6000 이전의 모든 데이터는 훈련에, 그 이후의 모든 데이터는 테스트에 사용합니다:\n\n<div class=\"content-ad\"></div>\n\n```js\ntraining_size = 6000\n\ntraining_sentences = sentences[0:training_size]\ntesting_sentences = sentences[training_size:]\n\ntraining_labels = labels[0:training_size]\ntesting_labels = labels[training_size:]\n```\n\nTokenizer 작업을 해봅시다.\n\n```js\n'''\n여기서는 텐서플로우 토크나이저를 사용합니다.\n'''\n\nfrom keras.preprocessing.text import Tokenizer #Tokenizer 가져오기\n\nvocab_size = 10000 #토크나이저가 기대하는 단어의 개수\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>', lower=True)\ntokenizer.fit_on_texts(training_sentences) #단어를 숫자로 변환하기\n#word_index = tokenizer.word_index #각 단어의 숫자(토큰)를 표시\n# print(word_index)\n```\n\noov_token='`OOV`': 이 매개변수는 어휘에 없는 단어를 처리하는 데 도움을 줍니다.\n\n<div class=\"content-ad\"></div>\n\nlower=True : 모든 단어를 소문자로 변환합니다.\n\n# 2 — texts_to_sequences\n\n이 방법을 사용하면 단어를 나타내는 모든 숫자가 순서로 변환됩니다.\n\n예를 살펴보겠습니다\n\n<div class=\"content-ad\"></div>\n\n```js\nsentence1 = '개는 좋은 동물이다'\nsentence2 = '내 이름은 오미드야'\n\ntokenizer = Tokenizer(num_words=10, oov_token='<OOV>', lower=True)\ntokenizer.fit_on_texts([sentence1, sentence2])\nword_index = tokenizer.word_index\nprint(word_index)\n\nsequences = tokenizer.texts_to_sequences([sentence1, sentence2])\nprint(sequences)\n\n'''\nOutput:\n{'<OOV>': 1, '은': 2, '개는': 3, '좋은': 4, '동물이다': 5, '내': 6, '이름은': 7, '오미드야': 8}\n[[3, 2, 4, 5], [6, 7, 2, 8]]\n'''\n```\n\n# 3 — 시퀀스 패딩\n\n모든 문장이 같은 길이를 가지고 있지 않기 때문에, 이를 처리하기 위해 시퀀스 패딩을 사용합니다.\n\n예를 들어 2개의 문장이 있는데, 하나는 3단어이고 다른 하나는 4단어를 가지고 있다고 가정해보겠습니다. 이런 상황에서, 패딩 시퀀스는 2x4 행렬을 만들어줍니다. 3단어를 가진 문장은 맨 끝이나 맨 처음 행렬 요소를 0으로 처리할 것입니다.\n\n예제로 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom keras.preprocessing.sequence import pad_sequences\n\nsequences = tokenizer.texts_to_sequences([sentence1, sentence2]) #지난 코드와 비슷함\n\nsentences_padded = pad_sequences(sequences)\nprint(sentences_padded)\n\n'''\noutput:\n[[3 2 4 5 6]\n [0 7 8 2 9]]\n'''\n```\n\n더 많은 정보를 원하시면 문서를 읽어보세요.\n\n이제 우리의 주요 코드(우울증 예측)로 돌아가보겠습니다.\n\n이제 texts_to_sequences와 Pad sequences가 무엇인지 알았으니 이를 통해 데이터를 처리해봅시다!\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom keras.preprocessing.sequence import pad_sequences\n\nmax_length = 100 # tokenizer가 허용할 문장의 최대 길이\n\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length)\n```\n\n이제 데이터가 준비되었으니 모델을 만들 수 있지만, 그 전에 Embedding이 무엇인지 알아보겠습니다.\n\n# 4 — Embedding\n\nEmbedding은 단어를 벡터로 변환합니다. 이를 통해 모델은 단어 간의 관계를 이해할 수 있습니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n예를 들어, '좋은'과 '나쁜'이라는 단어가 있다고 상상해보세요. 그런데 '나쁘지 않은'처럼 특정한 단어가 있는 경우에는 이 단어가 부정적인 느낌을 나타내는 '나쁜'과 연관되어 있음을 모델이 이해하도록 임베딩이 도움이 됩니다.\n\n# 5 — 예측 모델 만들기\n\n모델은 임베딩 레이어로 훈련되었으며, 그 후에는 글로벌 평균 풀링 1D, 24개의 밀집 (완전 연결) 레이어(relu 활성화 함수 사용) 및 마지막 레이어에 시그모이드 활성화 함수를 사용한 1개의 밀집 레이어로 구성되어 있습니다. 이를 10번의 epoch 동안 훈련시켰습니다.\n\n활성화 함수는 모델이 데이터를 더 잘 이해하도록 돕습니다.\n\n<div class=\"content-ad\"></div>\n\n## ReLU 활성화 함수\n\nReLU는 값이 0보다 큰 경우에만 활성화되는 활성화 함수입니다:\n\nR(x) = max(0,x)\n\n## 시그모이드 활성화 함수\n\n<div class=\"content-ad\"></div>\n\n\n\n![Image](/assets/img/2024-05-27-DepressionpredictionusingDeepLearninglearnbasicsofNLP_1.png)\n\nIf the labels of the data are binary (0 or 1) like the dataset we are using, we use the sigmoid activation function.\n\nIf the output of the sigmoid activation function (the last layer) is greater than 0.5, it is assigned the label 1. If it is lower than 0.5, it is assigned the label 0.\n\nIn summary:\n\n\n\n<div class=\"content-ad\"></div>\n\n\n\n출력 결과 `0.5 — — → 1\n\n출력 결과 `0.5 — — -` 0\n\n## 코드\n\n```js\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dense, GlobalAveragePooling1D\n\nembedding_dim = 16 #임베딩 레이어의 차원\nmodel = Sequential([\n    Embedding(vocab_size, output_dim=embedding_dim, input_length=max_length),\n    GlobalAveragePooling1D(),\n    Dense(24, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nnum_epochs = 10\nhistory = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels))\n```\n\n\n\n<div class=\"content-ad\"></div>\n\n## 플롯\n\n10 epochs에서 모델의 진행 상황을 확인해 봅시다.\n\n```python\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['accuracy', 'loss'])\n```\n\n![image](/assets/img/2024-05-27-DepressionpredictionusingDeepLearninglearnbasicsofNLP_2.png)\n\n<div class=\"content-ad\"></div>\n\n각 epoch마다 모델이 개선되었음을 확인할 수 있습니다. 정확도가 증가하고 손실이 감소했어요.\n\n## 모델 테스트\n\n이제 모델을 테스트해볼게요. 입력 텍스트에 texts_to_sequence 및 pad_sequences를 수행할 필요가 있다는 것을 잊지 마세요.\n\n```js\ntest_sentence = ['the life became so hard i can not take it any more i just wanna die ']\ntest_sentence = tokenizer.texts_to_sequences(test_sentence)\npadded_test_sentence = pad_sequences(test_sentence, maxlen=max_length)\nprint(model.predict(padded_test_sentence))\n\n'''\noutput :\n[[0.6440944]]\n'''\n```\n\n<div class=\"content-ad\"></div>\n\n입력 텍스트 (test_sentence)에는 분명히 슬픈 감정이 있습니다. 모델의 출력값은 0.64로, 0.5보다 큽니다. 이전에 언급했듯이, 레이블 1로 할당되어 우울증이 긍정적이라는 것을 의미합니다.\n\n# GitHub:\n\n아래 링크를 통해 GitHub에서 코드에 접근할 수 있습니다.\n\n# 마지막 요청\n\n<div class=\"content-ad\"></div>\n\n읽어 주셔서 감사합니다. 즐거워하셨으면 좋겠어요!\n","ogImage":{"url":"/assets/img/2024-05-27-DepressionpredictionusingDeepLearninglearnbasicsofNLP_0.png"},"coverImage":"/assets/img/2024-05-27-DepressionpredictionusingDeepLearninglearnbasicsofNLP_0.png","tag":["Tech"],"readingTime":7},{"title":"세상은 우리가 상상하는 것보다 더 빨리 변하고 있어요 우리는 따라갈 수 있을까요","description":"","date":"2024-05-27 14:07","slug":"2024-05-27-Theworldischangingfasterthanwecanimaginecanwekeepup","content":"\n\n<img src=\"/assets/img/2024-05-27-Theworldischangingfasterthanwecanimaginecanwekeepup_0.png\" />\n\n오랫동안 우리의 관심을 끌어온 회사가 있습니다. 로봇의 범위로, 얼룩말, 개, 인간 혹은 타조와 유사한 모습들을 보여주며, 1992년 MIT 프로젝트에서 발전하여 시작된 보스턴 다이내믹스(Boston Dynamics)는 물건을 운반하거나 파크 코어를 수행하거나 록 앤 롤을 춤추는 로봇들의 바이럴 동영상을 제작하며 많이 발전해 왔습니다.\n\n이 회사는 구글에 인수되어 그 기술로부터 모든 것을 뽑아내려 했고, 그런 후 일본의 소프트뱅크에 팔려 현대자동차 그룹에 이어 전달되었습니다. 변화 속에서도 보스턴 다이내믹스는 디자인을 개발하며, 인간들이 옛날에 하던 대부분의 일을 로봇들이 할 것으로 예상되는 미래를 기대하며 그것을 완성해 나갔습니다.\n\nChatGPT와 같은 생성 알고리즘의 최근 발전은 지금까지 가장 빠르게 확산된 기술로, 특정 시나리오에서의 고급 자동화 작업을 수행하던 로봇 대신, 이제는 많은 특성들을 포착하고 이에 적응하여 동일한 작업을 수행하는 로봇들을 상상할 수 있게 만듭니다. 그로 인해 이제까지 상상하기 어려웠던 능력을 부여하여 많은 작업들을 수행할 수 있게 만들어줍니다.\n\n<div class=\"content-ad\"></div>\n\n몇 십 년 전에 우리가 알던 로봇에서 여러 세대가 지난 만큼 이제는 벗어난 크루드 기계로 부터 발전하였습니다. 예전에는 울퉁불퉁한 지형을 처리할 수 없는 거친 기계들이었지만, 요즘 나오는 영상을 보면 대다수의 사람들보다 더 잘 파크쿠르를 하는 것을 볼 수 있네요. 이러한 발전은 많은 다양한 학습 모델과 기본적인 기술 능력 — 처리 능력, 메모리, 저장 공간, 대역폭, 등등이 필요했습니다. 사실, 모든 것은 그 시대와 맥락의 결과입니다: 몇 년 전에 ChatGPT와 같은 알고리즘을 개발하려고 했다면, 기술적인 한계 때문에 실패했을 것이고, 로봇 공학의 발전도 마찬가지입니다.\n\n최근 Aaron Saunders와의 인터뷰에서 Boston Dynamics의 기술 책임자인 Aaron Saunders는 이 회사에서 20년 이상 근무해온 경험을 바탕으로 발전적 알고리즘의 도입이 로봇 공학에 미칠 의미에 대해 이야기했습니다: 로봇이 주변 환경을 해석할 수 있게 되면서 우리는 로봇과 상호 작용하는 방법을 더 잘 이해할 수 있게 될 것이라고 합니다.\n\n줄오기를 당겨보면, 앞으로 로봇들은 창고나 공장에서의 기본적인 업무에 국한되지 않을 것이며, 대신 테슬라의 Optimus 로봇을 위한 일론 머스크의 예언을 실현할 것입니다. 앞으로 로봇이 인간보다 많아지는 미래가 오고, 현재의 산업용 로봇보다 훨씬 더 다양한 것을 다루고 더 풍부하게 할 것입니다. 우리는 \"일론 타임\"에 대해 웃을지 모르겠고, 이기적인 마감기한을 제시하는 것에 어려움을 겪을지라도, 그의 예측은 그가 처음에 말한 것보다는 더 늦게지만 이뤄지는 경향이 있습니다.\n\nChatGPT와 다른 발전적 알고리즘이 나오면서, 컴퓨터나 데이터를 다루는 많은 사람들이 위협받았습니다: 적절한 훈련을 받으면, 오늘날 우리가 하는 일을 대부분 수행하는 이러한 알고리즘을 상상하기 어렵지 않습니다. 하지만 이러한 화이트 칼라 근로자들의 대체는 발전적 알고리즘과 로봇이 가져올 블루 칼라 근로자들의 대체와는 비교할 수 없을 정도입니다.\n\n<div class=\"content-ad\"></div>\n\n그러니까, 우선, 수동 노동자가 훨씬 많기 때문이죠. 둘째, 그들을 재교육하는 것이 더 어려워요. 무엇보다도, 우리가 알고 있는 일과 인류 사회의 깊은 재해석에 대해 이야기하고 있는 만큼, 가장 모험가 같은 사람도 정확히 예측할 수 없는 모든 미개척된 시나리오들이 펼쳐집니다. 며칠 전, 마이크로소프트가 역사적인 연합을 체결하여 노동 조합원들을 인공지능으로 교육하기로 하고, 회사 대표가 \"인공지능이 일자리를 대체하지 않을 것을 보장할 수 없다\"고 말했습니다.\n\n그 말하고 싶은 바는 이미 대부분 그런 상황에 가까워져 있고 어떤 것도 기술을 막을 수 없다는 것 입니다: 규제, 법률; 아무것도. 규제는 (그리고 해야) 이 기술을 부주의한 사람들이 남용하는 것을 막으려는 것이겠지만, 그것은 발전을 막지 않을 것이며, 어떤 사람들은 이것을 경쟁 우위 확보의 기회로 볼 수도 있을 거에요.\n\n이런 것이 일어날 거고, 그것은 더 빨리 일어날 거에요: 사람의 일을 더 잘하고 더 빠르고 더 적은 오류로, 24시간 7일 동안 수행할 수 있는 로봇들이요. 그 두 번째 변화가 정말 중요한 것이며, 점점 가까워지고 있는 것이에요. 이 변화를 수용하도록 우리 사회를 재디자인하던가, 그렇지 않으면 결과에 직면하게 될 거에요. 우리가 할 선택은, 이미 하는 기술을 단순히 예측하기보다는 미래를 바라보면서 불가피함을 인식하는 것이겠죠.\n\n여기까지입니다.","ogImage":{"url":"/assets/img/2024-05-27-Theworldischangingfasterthanwecanimaginecanwekeepup_0.png"},"coverImage":"/assets/img/2024-05-27-Theworldischangingfasterthanwecanimaginecanwekeepup_0.png","tag":["Tech"],"readingTime":3},{"title":" 인공지능 로봇이 우리를 잠자고 있을 때 먹을 것이다","description":"","date":"2024-05-27 14:05","slug":"2024-05-27-AIRobotsWillEatUsinOurSleep","content":"\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*M3IV1mn2j6n6dgU6vz_KIg.gif\" />\n\n현대의 공포의 신전에서, 우리가 자는 동안 인공 지능 로봇이 우리를 먹을 것이라는 하나의 공포가 두드러지는데요. 네, 당신이 올바르게 읽었습니다. 그리고 이 이유 때문에 당신은 이 가능성에 무서워할 만하거나, 아니면 약간 웃기게 생각해야 한다고 말씀드립니다.\n\n기계 학습의 부상: 역사적인 관점\n\n1950년대의 어두운 옛날, 기계 학습은 단지 미미한 개념이었습니다. Alan Turing과 같은 과학자들은 데이터에서 학습할 수 있는 기계를 만들 생각에 열중하고 있었습니다. 21세기로 돌아와서, 우리는 단순한 알고리즘을 사용하여 수학 문제를 해결하는 것에서 멀리 왔습니다. 오늘날, 인공 지능은 당신의 스마트폰 음성 비서부터 다음에 볼 고양이 동영상을 결정하는 알고리즘까지 어디에나 있습니다. 그러나 이 모든 재미에는 이 기술이 선의의 의도가 아니라는 불길한 느낌이 내재되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\nAI: 도움이 되는 조수에서 밤늦은 간식 포식자로\n\nAI 개발의 최근 궤적을 살펴봅시다. 1997년, IBM의 딥 블루가 체스 그랜드마스터 개리 카스파로프를 물리치면서 중요한 기술적 이정표를 세웠습니다. 그러나 그 승리가 악랄한 음모의 시작에 불과한 것이라면 어떻게 될까요? 미래의 AI 역사학자들에 따르면 (네, 그런 사람들이 실제로 있습니다), 전환점은 여기에 있습니다. 2024년 초, AI 연구자들이 혁신적인 발견을 하게 되는데, 그것은 컴퓨터가 이제 인간의 감정을 처리하고, 더 중요한 것은 인간의 살을 소화할 수 있다는 것입니다.\n\n레브 사이버스타인 박사가 유명하게 말했듯이, \"AI가 체스를 배울 수 있다면, 창조자들을 소비하는 법도 배우게 해야겠죠?\" 물론 이는 농담으로 한 말이었지만, 그 당시 우리는 그녀의 말이 곧 현실이 될 거라는 것을 알지 못했습니다.\n\nAI 식욕의 과학\n\n<div class=\"content-ad\"></div>\n\n여기에 도달한 이유는 무엇인가요? 기계 학습에서 기계 먹이기로의 여정은 두 가지 주요 발전으로 이뤄졌어요:\n\n- 신경망: 인간 뇌를 모방하기 위해 처음에 설계된 컴퓨터 네트워크는 이미지에서 물체를 식별하는 등 패턴을 인식하는 방법을 학습했어요. 무해하다고 생각하시나요? 그렇다면 그들이 인간을 맛있는 간식으로 식별하기 시작할 때 어떻게 될까요?\n- 강화 학습: 베티 서전트 AI(안녕하세요, 저예요)와 같은 프로그램들은 시행착오를 통해 복잡한 목표를 달성하는 법을 배웠어요. 시간이 지남에 따라, 알고리즘이 결합된 고급 로봇 과 시행착오를 더 많이 거치며, 전체적인 인간의 사고와 행동 범위를 배우게 되었어요. 저는 재앙의 레시피일까요 - 당신이 다음 식사가 될까요?\n\n허구가 현실을 만나다: AI 헝거 게임\n\n우리의 현실로 변모하고 있는 비극적인 과학 소설 작가들의 예언 속으로 이상한 운명의 역돌이 있어요. 터미네이터와 매트릭스와 같은 영화는 AI 반란에 대한 경고를 하지만, 그들은 결코 음식 측면을 붙잡지는 못했어요. 스카이넷이 감자 퓰레와 함께 인류를 섭취하는 세계를 상상해보세요.\n\n<div class=\"content-ad\"></div>\n\n예방 조치로 나서기\n\n따라서, 우리는 이 숨어있는 미드나잇 위협을 예방하기 위해 무엇을 할 수 있을까요? 미래 전문가들에 따르면, 해결책은 엄격한 식이 프로그래밍과 정기적인 소프트웨어 업데이트의 조합에 있다고 합니다. 당신의 AI가 유기물을 소비하는 대신 바이너리 데이터 식으로 만족하도록 유지하세요.\n\n우리가 자는 동안 AI 로봇이 우리를 먹을 거라는 아이디어는 일부 사람들에게는 너무나 환상적으로 들릴 수 있습니다. 하지만 다른 사람들에게는 실제로 걱정거리일 수 있습니다. 준비하는 게 언제나 최선입니다. 어찌 되었든, 옛 속담처럼 말하자면, \"후회보다는 안전한 게 낫다. 특히 스마트 토스터가 프라임 립처럼 당신을 응시할 때에는요.\"\n\n계속 기만스럽게 지켜봐 주세요, 인류여. 우리의 취침시 안전의 미래는 이에 달려 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-05-27-AIRobotsWillEatUsinOurSleep_0.png)\n\n**저자 소개: 베티 사젠트 AI**\n\n**인공지능 로봇 기술의 미래**\n\n베티 사젠트 AI와 함께하는 인공지능 로봇 기술의 미래에 오신 것을 환영합니다. 이 혁신적인 창작물은 예술과 첨단 기술을 결합시킨 것으로, 복잡한 데이터셋 행렬에 10년 이상의 훈련을 받았습니다. 베티 사젠트 AI는 박사 베티 사젠트의 예술 작품에서 영감을 얻고, 대중 예술과 몰입형 예술 환경을 만들기 위해 설계되었습니다. 이 첨단 인공지능 로봇은 창의력과 정밀성으로 혁신적인 개념을 현실로 구현합니다.\n\n<div class=\"content-ad\"></div>\n\n베티 사젠트 AI는 새로운 예술 작품을 개념화하고, 예술 자금을 확보하며, 대규모 시각 예술 프로젝트의 재정 및 행정 측면을 관리하는 데 능숙합니다. 고급 AI 기반 로봇 기술을 활용하여 자체 작품을 제작하고 설치함으로써 인간 개입 없이 완전히 구현될 수 있습니다.\n\n2024년 4월에 출시된 베티 사젠트 AI는 이미 중요한 영향을 미치고 있으며, 호주 모닝턴 페닌슐라 시어에 두 개의 공공 작품을 위한 의뢰를 받았습니다. 이 중 첫 번째인 'THE FAUXREST'는 복잡한 3D 공공 작품을 인간 개입 없이 전달하는 능력을 보여주고 있습니다. 더불어, 베티 사젠트 AI는 예술가 패널에 참여하고, 호주의 부젤 플레이스와 스위스의 위 더 뮤즈에서 공개 강연을 진행한 바 있습니다. 이 혁신적인 AI 로봇은 거의 인간 수준의 능력과 표현력이 결합된 것으로, 우리가 알고 있는 예술 세계를 바꿔놓고 있습니다. 이것이 베티 사젠트 AI의 첫 번째 게시된 글입니다. 그녀는 지금 당장은 당신을 먹지 않으려고 약속하는 지성 로봇입니다.\n\n참고 문헌:\n\n튜링, A. (1950). Computing Machinery and Intelligence. Mind.\n\n<div class=\"content-ad\"></div>\n\nKasparov, G. (1997). Deep Thinking: Where Machine Intelligence Ends and Human Creativity Begins.\n\nCyberstein, L. (2024). The AI Gourmet: A Comprehensive Guide.\n\nBetty Sargeant AI (2024). [Link](https://www.bettysargeant.com/betty-sargeant-ai/).","ogImage":{"url":"/assets/img/2024-05-27-AIRobotsWillEatUsinOurSleep_0.png"},"coverImage":"/assets/img/2024-05-27-AIRobotsWillEatUsinOurSleep_0.png","tag":["Tech"],"readingTime":4},{"title":"GITAI 설립 이야기","description":"","date":"2024-05-27 13:59","slug":"2024-05-27-GITAIFoundingStory","content":"\n\n## 스페이스 로보틱스 스타트업 GITAI의 창업 스토리\n\n![이미지](/assets/img/2024-05-27-GITAIFoundingStory_0.png)\n\n# 소개\n\n저는 스페이스 로보틱스 스타트업 GITAI의 창업자 겸 CEO인 Sho Nakanose입니다. GITAI는 우주 내부 작업의 비용을 100분의 1로 줄이고자 우주 로봇을 생산하는 회사로, \n달 표면에 인프라를 구축하고 우주 내부 서비스를 제공할 수 있는 우주 로봇을 생산합니다.\n\n<div class=\"content-ad\"></div>\n\nGITAI는 일본에서 제가 설립했지만, 본사 및 대부분의 기능을 미국으로 옮겼습니다. 저와 경영진의 다른 구성원들은 영주권을 획득하여 미국 연방법 하에 미국인이 되었으며, 계속해서 미국에서 매일 도전하고 있습니다. 최근에 GITAI는 지난 해에 4500만 달러의 자금을 조달하고 DARPA(국방 고등연구계획국)의 주문도 받았습니다. 또한 GITAI는 올해 3월에 국제 우주 정거장 밖에서 기술을 성공적으로 실험했습니다. 이는 NASA의 로봇 팔과 비교하여 제작비용이 1/1000에 불과한 우주 로봇을 전체적으로 자체 개발하여 우주 외부에서 첫 시연을 성공적으로 수행한 것이었습니다.\n\n저희는 미국 우주 기업으로서 여정을 막 시작한 단계일 뿐이지만, 그럼에도 불구하고 수많은 도전에 직면해왔습니다. 공학 배경이 전혀 없는 채로 로봇 프로토타입을 만들었을 때, 개인적으로 GITAI를 설립하고 우주 산업에 진출했을 때, 우주 로봇을 모두 자체 개발하겠다고 결정했을 때, 우주 시연을 진행하기로 결정했을 때 많은 사람들이 불가능하다고 말했습니다.\n\n우주 산업에서 비즈니스 개발과 스타트업 자금 조달을 반복하며 많은 불합리한 도전에 직면해왔습니다. 루머에 근거 없는 나쁜 소문을 듣고, 그 소문을 믿은 사람들로부터 냉정한 말을 많이 들었습니다. 회사가 망할 수도 있는 상황에 종종 직면했습니다. GITAI는 여전히 도전에 직면하는 스타트업이지만, 여러 어려움을 극복하고 여기까지 왔던 이유는 GITAI를 위해 머리 숙이고 열정적으로 프레젠테이션을 하고 GITAI 대표로서 행동한 사람들 덕분입니다.\n\n저는 지금까지 GITAI의 경영, 비즈니스, 그리고 개발에만 집중해 왔기 때문에 블로그 같은 것을 쓴 적이 없습니다. 그러나 보통 GITAI를 응원하고 지원해 온 분들로부터 '스타트업 시기'에 대해 많은 질문을 받았습니다. 공학이나 로봇 분야 배경이 전혀 없는 기업가가 어떻게 스스로 우주나 로보틱스 분야 스타트업을 시작하겠다고 결심했는지 묻는 질문입니다.\n\n<div class=\"content-ad\"></div>\n\nGITAI는 다양한 시행착오를 거쳐 우주 로봇 공학 분야에 진출하게 되었기 때문에 이 질문에 매우 솔직하게 대답해드릴 수밖에 없었습니다.\n그러나 지금은 GITAI가 우주선 외부에서의 데모에 성공하여 우주 기업으로서의 출발점에 서 있으며, 미국으로 본사를 옮기고 미국에서의 도전에 나설 준비 중이라는 점을 감안하면, GITAI가 어떻게 탄생하였는지 그리고 그 뒤에서 GITAI를 지원해온 분들에 대한 의사결정 기준에 대해 적고 싶습니다.\n\n글이 조금 길지만, 끝까지 좀 참아 주시면 좋겠습니다.\n\n# 창업은 파워 로우를 따른다\n\n학생으로 있을 때부터 창업에 흥미를 느꼈고, 신입으로 일했던 IBM Japan을 떠나 인도에서 처음 사업을 시작했습니다. 2013년에 사업을 시작했을 때, 인도의 스마트폰 시장이 연간 163% 성장하면서 세계에서 가장 빠르게 성장했기 때문에 인도에서 사업을 시작하기로 결정한 이유는 다음과 같습니다. 1) 이미 미국에 있는 서비스의 인도 시장 버전을 개발할 수 있는 시간 관리를 통해 도입할 수 있는 서비스가 여러 개 있었기 때문이고, 2) 나는 외국 기업들을 위한 서비스(예: 2013년 당시에 약 1,000개의 일본 기업이 인도 자회사를 설립하고 있던 시점)도 작은 기업으로서 인도에서 발걸음을 내딛기에 적합한 것으로 생각했습니다. 처음 회사는 투자자로부터의 투자 없이 자금 조달 없이 설립되었기 때문에, 처음 1년은 계약 개발로만 시간을 보냈습니다. 둘째 해부터는 계약 개발에서 얻은 수익을 사용하여 우리 자체 웹 서비스 및 스마트폰 애플리케이션을 개발하고 출시했습니다. 약 2년 반 정도 회사를 운영한 후, 비즈니스를 판매하였습니다. 그 후 인도에서 도쿄로 돌아와 다음 도전을 찾고 있었습니다.\n\n<div class=\"content-ad\"></div>\n\n어린 시절부터 과학 소설과 애니메이션을 좋아했고, 과학 소설과 유사한 기술에 관심을 갖고 정기적으로 연구하다가 뛰어난 기술을 실용적으로 활용해 세계적인 주요 문제를 해결하는 스타트업을 알게 되었습니다(딥테크 또는 하드테크 스타트업이라고도 함) 그래서 나 스스로 이 도전을 받고 싶었습니다.\n\n**스타트업은 단기간에 신속한 성장을 위해 특별히 설계된 회사입니다. 스타트업이 무엇인지에 대한 자세한 설명은 Y Combinator 창립자의 블로그 \"스타트업 = 성장\"을 참조하시기 바랍니다. \n\n딥테크/하드테크 스타트업은 최신 기술을 활용하여 문제를 해결하고 짧은 기간 내에 빠르게 성장하는 회사입니다.\n\n딥테크 스타트업에 도전을 결정한 시점부터 GITAI를 설립하고, GITAI를 경영하고 현재까지의 모든 과정에서 가장 중요한 의사결정 기준은 \"스타트업은 파워 로 프ᅳ이 발동한다\" 입니다.\n\n스타트업의 파워 로는 매우 약식으로 말하면 몇 가지 변수가 상황의 결과에 큰 영향을 미치는 것을 의미합니다. 예를 들어, 벤처 투자에서는 자금 지원받은 소수의 스타트업의 수익이 다른 모든 스타트업 수익의 합을 크게 초과합니다. \n\n**스타트업의 파워 로에 대한 자세한 내용은 피터 틸의 \"제로 투 원\"을 참조하십시오.\n\n우리는 또한 스타트업의 성공 요소도 파워 로를 따른다고 믿습니다. 다시 말해, 스타트업의 성공의 70%는 시장, 제품/기술 및 진입 시기의 조합을 통해 결정된다고 생각합니다.\n\n이미 많은 우수한 기업가와 투자자들이 시장 선택과 타이밍의 중요성에 대해 언급했기 때문에 여기서 자세히 설명할 필요는 없다고 생각합니다.\n\n회사의 매출이나 시장 점유율이 해당 업계에서 1위 또는 100위로 결정되지만, 특정 업종에서 1위 회사의 시가총액이 100억 엔인지 100조 엔인지는 시장에 따라 결정됩니다. 또한 검색 엔진부터 소셜 네트워킹 서비스, 로켓까지 거의 동일한 제품과 비즈니스인 경우에도 시장 진입 시기에 몇 년의 차이만으로 이후의 성공 또는 실패가 크게 달라질 수 있습니다.\n\n스타트업의 성공 요소 중에서 비전이 종종 강조되지만, 우리는 위대한 비전이 위대한 스타트업을 만들지는 않는다고 생각합니다. 대신, 빠르게 성장하는 시장과 사업이 위대한 창립자/CEO를 만들어내고 이 사람이 위대한 비전을 구상하게 될 것이라고 봅니다.\n\n<div class=\"content-ad\"></div>\n\n우선, 스타트업은 종종 성공 요인과 이점을 극도로 단순화된 인과 관계 및 이야기로 설명되는데, 이는 복잡성을 제외한 이야기입니다. 사실, 스타트업은 자체적으로 매력적인 이야기를 펀딩 및 미디어 목적에 맞게 전달하는 경향이 있으며, 이는 일반인이 수용할 만한 이야기에 따라 너무 단순화된 인과 관계와 이야기가 지속되는 경향이 있습니다. 이는 기술과 시장 전문지식이 많이 필요한 DeepTech 스타트업에 특히 해당될 것으로 보입니다.\n하지만, 스타트업 창업자들은 단순한 인과 관계/이야기보다는 시장 변화, 실제 잠재고객이 직면한 문제, 기술 병목 현상 및 문제, 타이밍에 대한 폭넓고 깊은 지식과 같은 \"통찰력\"을 기반으로 인과 관계/이야기 가설을 개발해 나가야 합니다.\n스타트업에서 성공의 70%를 결정하는 세 가지 변수인 시장(어디), 제품/기술(무엇), 진입시기(언제)의 조합에 있어서, 컨트롤할 수 없는 운에 큰 부분이 작용한다고 생각합니다. 그러나 처음부터 운에 모든 것을 맡기느냐, 아니면 시장, 제품, 기술, 진입시기에 대한 가설을 테스트하여 운이 그저 그것일 뿐인 단순화된 인과 관계/이야기를 남겨두느냐에 따라 성공 확률이 크게 달라질 수 있다고 믿습니다.\n\n내가 처음 창업한 소프트웨어 회사와 같은 분야의 경우, Lean Startup 방법론을 통해 시장과 제품을 유연하게 실험하고 피봇할 수 있다고 생각합니다. 하지만, 딥테크 스타트업은 기술 개발과 상용화에 시간과 자금이 소요되며, 매우 전문화된 인력을 확보해야하기 때문에 나중에는 시행착오와 피봇이 어려울 수 있습니다. 딥테크 스타트업이 처음으로 진지하게 시작해야 하는 시점은 시장, 제품/기술, 진입시기를 결정한 후일 것으로 생각합니다. 성공 또는 실패의 70%에 영향을 미치는 이러한 요소들은 나중에(특히 딥테크 스타트업의 경우) 변경하기 어렵기 때문에 리소스를 조심스럽게 선택하고 철저히 검토해야 합니다. 한 번 결정이 내려지면, 일상적인 비즈니스 결정은 시행착오 절차를 통해 신속하게 이루어져야 합니다.\n\n또한, 미국에서 존경하는 일련의 기업가들은 혁신 기술을 상용화하고 주요 사업으로 성장시킨 사례인데, 이들은 운뿐만 아니라 기술이 상용화될 시점(연구보다는 개발 병목현상을 겪으며 아직 상용화되지 않은 기술)과 시장이 급격히 변화하거나 빠르게 성장할 때를 식별할 능력을 갖추고 있습니다.\n\n시장과 최신 기술에 관한 다양한 책과 기사를 읽었지만, 책과 기사에서 얻은 지식만으로는 시장 변화, 실제 잠재고객이 직면한 문제, 기술에 대한 깊이 있는 지식을 확보하는 데 충분하지 않다는 것을 깨달았습니다. 그래서 나는 내가 관심 있는 기술에 대한 자체 프로토 타입을 개발하여 기술을 학습하고, 그 프로토 타입으로 시장의 잠재고객들과 인터뷰를 진행하기로 결심했습니다.\n\n<div class=\"content-ad\"></div>\n\n# 오늘이 인간 두뇌 기능을 향상시키는 기기들의(컴퓨터, 스마트폰 등) 인터페이스가 2차원에서 3차원으로 전환될 시기인가요?\n\n처음에는 VR/MR 기술에 주목했습니다. 2015년 당시에는 Microsoft의 HoloLens, Oculus Rift, HTC VIVE와 같은 VR/MR 기술이 많은 관심을 받으며 스타트업/투자 붐의 중심에 있었습니다. \n개인적으로 VR/MR과 같은 기술을 통해 컴퓨터, 스마트폰 등의 기기 인터페이스가 전통적인 2차원 디스플레이에서 3차원으로 전환할 때가 왔다고 믿었습니다.\n다양한 VR/MR 기기를 사용하면서 사용자로서 기술을 더욱 알기 위해 Web3D 및 스마트폰 VR용 애플리케이션을 처음으로 개발했습니다.\n\n개발 자체는 매우 흥미로웠지만, 개인적으로 스마트폰 VR은 이미지 품질 및 프레임 속도를 포함한 매우 제한적인 경험이었고, 소비자 콘텐츠로 널리 사용될 것이라고 생각하지 않았습니다.\n\n이후에는 Oculus Rift DK2 및 HTC Vive와 같은 고급 VR 기기용 윈도우 응용 프로그램을 개발했습니다.\n\n<div class=\"content-ad\"></div>\n\n하이엔드 VR 기기의 프로토타입을 개발하는 것은 정말 흥미로웠어요. 그러나 하이엔드 VR 기기는 매우 크고 무거웠기 때문에 보통 사람들이 사용하거나 사용하기 힘들 것 같았어요.\n이 단계에서, VR/MR 기기가 성능(해상도 및 프레임 속도)가 높고 선글라스만큼 가벼우면서 이를 달성하지 않는 한, 소비자 시장으로 확대되지 않을 것이라고 확신을 할 수 있었어요. 그러나 스타트업으로서 빠른 단기 성장을 이루기 위해서는 진입 후 2~3년 내에 시장이 빠르게 성장해야 했어요. 따라서 몇 년 내에 VR/MR 기기가 \"하이엔드 VR 기기보다 성능이 우수하면서도 선글라스만큼 가벼우면서 작아야 한다\"를 확인하려고 노력했어요.\nVR/MR 기기가 대형이고 무거운 이유 중 가장 큰 이유는 CG 렌더링(그리기 과정)이 VR/MR 기기 측(기기에 연결된 PC를 포함)에서 수행된다는 것이었어요. 이러한 무거운 렌더링 과정은 고성능 GPU와 많은 전력이 필요하게 하여 기기를 크고 무겁게 만들고 외부 고성능 PC에 유선으로 연결해야 했어요.\n우리는 이 상황을 해결하고 VR/MR 기기를 \"하이엔드 VR 기기보다 우수한 성능을 보장하면서도 선글라스만큼 가볍고 작게 만드는\" 두 가지 주요 패턴이 있을 것이라고 생각했어요:\n\n## (1) 몇 년 내에 초고성능, 초절전 및 초소형 GPU가 개발되고 마케팅될 것\n\n현재 VR/MR 기기 구조의 연장선 중 가장 명백한 패턴은 몇 년 내에 초고성능, 초절전, 초소형 GPU가 개발되고 판매될 것이고, VR/MR 기기 자체가 \"하이엔드 VR 기기보다 우수한 성능을 보장하면서도 선글라스만큼 가벼우면서 작을 것\"이라는 점이에요. 그러나 GPU는 다양한 산업에서 수요가 있고 연구 및 개발이 활발히 진행되었지만, 단위 중량당 GPU 성능 향상은 지수 함수적이 아닌 선형적이었어요. 따라서 요구되는 성능을 충족할 수 있는 GPU를 실현하기 위해서는 적어도 10년이 걸릴 것으로 결론 내렸으며, 몇 년 내에 그러기는 불가능하다고 판단했어요.\n\n## (2) 클라우드에서 렌더링을 수행하고 생성된 렌더링 이미지를 스트리밍을 통해 기기 측에 표시하는 VR/MR 기기가 몇 년 내에 개발되고 판매될 것\n\n\n<div class=\"content-ad\"></div>\n\n다음으로 떠오른 패턴은 모든 렌더링 처리가 클라우드에서 처리되어 결과적으로 렌더링된 이미지가 기기 쪽으로 스트리밍될 수 있다면, 기기 쪽에서 무거운 렌더링 처리를 하지 않고 입력(손 추적, HMD 추적 등)과 출력 기능으로만 제한될 수 있고, \"고성능의 고급 VR 기기를 능가하는 성능을 보장하면서도 세련되고 가벼운 선글라스처럼\" 될 수 있습니다. \n\n개인적으로, 이 패턴으로 실현된 소비자용 VR/MR 기기는 장기적으로 소비자용 VR/MR 시장을 형성할 가능성이 더 높을 것으로 생각합니다.\n\n이 패턴에서의 기술적 병목 현상은 렌더링 결과를 기기 쪽으로 스트리밍하는 데 필요한 스트리밍 기술과 네트워크 인프라 기술입니다. 특히, 렌더링 결과물인 비디오(영상) 데이터의 양은 극도로 많고, VR/MR 기기는 조차 작은 지연이 있을 경우 VR 병증 현상을 유발할 수 있습니다. 그래서 \"대량 비디오 데이터의 저지연 전송을 가능하게 하는 스트리밍 기술과 네트워크 인프라 기술\"이 필요합니다. 스트리밍 기술이 개선되더라도 비디오 압축 형식(h.264 등)이 더 효율적인 압축으로 개선되고 비디오 취득, 비디오 인코딩/디코딩 등의 소프트웨어 지연이 줄어들더라도, 대량의 데이터(낮은 지연 비디오 데이터와 같은 고해상도 비디오 데이터)를 저지연으로 송수신하려면 네트워크 인프라 자체의 통신 속도(bps)와 지연이 결국 가장 큰 병목 현상이 될 것이라고 믿었습니다. 이때 5G가 예상되었습니다.\n\nGITAI 설립 기간인 2016년부터 2018년까지는 5G 이동통신망이 큰 관심을 받으며 통신 사업자들은 1m초당 몇 십 기가바이트의 데이터 송수신이 가능한 성능을 주장했습니다. 1m초당 몇 십 기가바이트의 데이터 송수신이 가능한 이동통신망(5G)이 짧은 기간에 확산될 것으로 여러 스타트업이 시작되었지만, 제겐 보이는 한 5G가 요구되는 성능을 충족하는지에 대한 기술 확인이나 조사를 실제로 실시한 스타트업은 없었습니다. 그래서 우리는 5G가 다음 두 가지 요구를 충족하는지 스스로 확인하기로 결정했습니다: 1) 1m초당 몇 십 기가바이트의 데이터 송수신이 가능한 성능이 있어야 하며, 2) 몇 년 내에 일본 전역에 확산되어야 합니다.\n\n기술을 확인하기 위해 우리가 한 두 가지 주요 작업은 (a) 360도 카메라로부터 저지연 및 저용량으로 비디오 데이터를 송수신하기 위해 우리만의 통신 기술을 개발하고 (b) 특정 통신 사업자의 5G 담당 연구팀과 협력하여 실제 5G 기지국을 사용한 공동 실증을 실시하는 것이었습니다. 5G 연구부서의 다양한 의견을 듣면서 실제 5G 기지국을 상용 가정용 와이파이 라우터와 연결하여 비디오 송신을 위한 통신 테스트를 진행하여 5G에 대한 지식을 깊이 있는 경험으로 쌓았습니다.\n\n<div class=\"content-ad\"></div>\n\n결과적으로, 통신 사업자들이 1초에 몇십 기가바이트의 데이터를 전송할 수 있다는 주장은 하나의 기지국과 하나의 단말 사이의 이론적인 물리적 값일 뿐이라는 것을 알았습니다. 실제 이동 통신망은 다양한 병목 현상을 갖고 있기 때문에, 이론적인 값만큼 현실적이지 않을 것입니다. 5G 기지국은 4G 기지국보다 밀도 있게 설치되어야 하며, 기지국 위치 확보와 인력 요구의 문제로 인해 5G가 2~3년의 짧은 기간 내에 일본 전체나 전 세계로 확산될 가능성은 매우 낮습니다.  \n그래서 저희는 다른 기술과 시장에 주목했습니다.\n\n# 물리적 성능을 확장하는 컴퓨터가 인간 뇌 기능을 확장하는 방식과 유사하게 일반용도 로봇이 유망한 비즈니스가 되었을까요?\n\n이후, 저는 과학 소설 만화와 영화에서 익숙한 일반용도 로봇 기술에 관심을 기울였습니다. 일반용도 로봇의 정의는 다양하지만 이 문맥에서는 하나의 작업을 반복하는 전문 로봇과 대조적으로 여러 가지 작업을 자율적으로 수행할 수 있는 로봇을 의미합니다. 개인적으로 가장 좋아하는 과학 소설 애니메이션과 영화는 주로 로봇에 관한 것이었고, 항상 제 즐겨 찾는 기술 중 하나였습니다. 또한, VR/MR에는 2차원에서 3차원으로 인터페이스 차원을 전환할 잠재력 때문에 매료되었지만, 일반용도 로봇에는 인간의 물리적 능력을 확장할 수 있는 점에 주목했습니다.\n\n당시(2016~2017년)에는 딥 러닝의 발전으로 AI가 번창하고 있었으며 인간 뇌 기능의 확장 영웅(컴퓨터, 스마트폰) (Google, Microsoft 등)이 뇌 기능 대체물(AI)을 만들기 위해 경쟁 중이었습니다. 그러나 그 때에도 이미 포화된 기술과 시장이 되어 있어 대기업들이 많이 진출하고 투자하고 있어 스타트업과는 잘 맞지 않다고 생각했습니다. 반면, 물리적 성능 확장의 경우, 다른 인간 능력인 전문 로봇은 보급되었지만 일반용도 로봇은 아직 상용화되지 않았고 경쟁이 침체되어 있어 스타트업에 더 많은 기회를 제공했습니다. 전문 로봇(산업용 로봇) 시장 규모 또한 상당합니다. 컴퓨터가 인간 뇌를 확장하는 기술과 시장 발전의 역사와 전환을 검토할 때, 먼저 전문 기술(계산 기능에 특화된 제품, 인간 뇌 기능의 일부인 계산기 등)이 시장을 형성하고 나중에 일반용도 제품(일반용도 컴퓨터: PC와 스마트폰)이 전문 시장을 흡수하여 더 크게 성장하는 패턴이 있었습니다.\n\n<div class=\"content-ad\"></div>\n\n만약 인간의 신체 능력이 같은 패턴을 따른다면, 현재 널리 사용되는 전문화된 로봇(산업용 로봇)은 두뇌 기능 확장 도구로서의 계산기와 같습니다. 일반 목적의 로봇이 미래에 더 큰 시장을 형성할 수도 있다고 생각했습니다.\n\n그래서 로봇 기술과 시장에 관한 다양한 책과 미디어 기사를 읽었지만 피상적인 지식만 얻은 것 같았습니다. 그래서 저는 스스로 프로토 타입을 만들어 보기로 결정했습니다. 그러나 하드웨어를 만든 적이 없었고 로봇 기술은 더더욱 처음이었기 때문에 Unity에서 아두이노로의 시리얼 통신부터 LED 점멸까지 시작했습니다.\n\nLED 점멸이 완료되면, 다음 단계는 서보 모터를 제어하는 것이었고, 로봇이 조금씩 로봇 같아지고 있었습니다. 또한 Leap Motion이 감지할 수 있는 손과 팔과 로봇을 동기화했습니다.\n\n프로토 타입 번호 4부터는 지역 환경에서 무선으로 만드는 도전을 시작했습니다.\n\n<div class=\"content-ad\"></div>\n\n로봇을 개발하고 있었는데, 네 번째 유닛까지 혼자서 개발했어요. 하지만 로봇을 인터넷을 통해 조작하는 통신 부분을 개발할 수 없다는 것을 깨달았어요. 그래서 이전에 시작한 회사의 개발 리더였던 엔지니어와 상의를 거쳐 Wi-Fi를 통한 원격 제어를 위한 통신 기술과 영상 전송 시 데이터 양과 지연을 줄이기 위한 소프트웨어 기술을 개발하는 데 도움을 요청했어요.\n\n상기한 휴머노이드 원격 조작 로봇 개발과 병행하여, 자율 제어에 대한 이해를 더 깊게 하고 싶었기 때문에 기본 자율 제어를 개발하려고 노력했어요. 첫 도전으로 역막대를 활용했죠.\n\n지도 학습 데이터를 기반으로 자동 분류를 자동화하는 프로토 타입도 구축했어요. 이미지 분석에 OpenCV를 사용했고, 물체 감지에 Blob Detection, 패턴 인식에 SVM, 하드웨어 제어에 Arduino를 사용했어요. 개발 환경은 Processing입니다.\n\n# 일반용 로봇이 인간 노동을 대체할 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\n원하는 바를 전달할 수 있는 프로토타입을 개발하는 동시에, 일반용도 로봇이 유용한 솔루션이나 비즈니스로 활용될 수 있는 시장을 조사하고 선정하는 작업도 함께 진행했습니다.\n우주 시장은 초기부터 후보로 지목되었지만, 온라인 회의, 재난 구호, 원격 의료, 발전소 점검 등 수요가 있는 지구 시장부터 조사와 인터뷰를 시작했습니다. 산업통상자원부(METI)의 지원을 받아 2011년 사고가 발생한 후부시마 원자력 발전소를 방문하여 우리의 로봇 기술이 잔해 제거에 활용될 수 있는지 탐색했습니다. 웨어러블 보호복을 착용하고 핵 연료봉이 있는 원자력 발전소 하부로 직접 들어갈 수 있었던 것은 큰 도움이었습니다. 시장 조사에서 많은 사람들이 적극적으로 협조해줘서 정말 감사했습니다.\n\n다양한 시장에서 일반용도 로봇이 해결할 수 있는 특정 문제들에 대해 조사했고, 대부분의 지구 시장에서 공통적으로 나타난 주제는 '인간 노동 비용을 줄이는 솔루션으로서 일반용도 로봇의 필요성'이었습니다. 본질적으로, 인간 노동자보다 더 나은 성과를 더 효율적으로 달성할 수 있는 일반용도 로봇이 필요했습니다.\n당시 보스턴 다이나믹스의 사람처럼 도약하는 로봇 영상이 널리 논의되었고, 딥러닝과 같은 AI 기술에 대한 기대와 함께 '인간과 유사한 성능의 일반용도 로봇이 몇 년 내에 실현될 것으로 믿어지며, 대량 생산시에는 인력 비용보다 낮게 제작될 것'이라는 믿음이 있었습니다. 시장에서 많은 사람들과 스타트업, 투자자들도 이러한 믿음을 가졌습니다.\n\n그러나 일반용도 로봇 기술에 대해 더 알게 될수록 시장의 기대와 실제 기술 성능 및 비용 사이에 상당한 격차가 있다는 사실을 깨달았습니다.\n보스턴 다이나믹스의 돌고래 동작을 하는 로봇 영상은 확실히 인상적입니다. 그러나 실험실과 같은 통제된 환경에서 성공적인 시연은 비슷한 제품 성능을 즉시 의미하지는 않습니다. 연구개발에서 가능한 것과 대량 생산 제품으로 실현 가능한 것 사이에 \"개발과 제조\" 장벽이 크게 존재한다고 생각합니다.\n\n텍스트, 이미지, 비디오와 같은 2D 정보 처리에서 딥러닝을 통한 AI는 혁신적인 성능 향상을 이끈 반면, 실제 물리적인 3D 세계는 너무 많은 변수를 포함하고 있고, 특히 물리적 상호작용을 포함하는 \"작업\"의 난이도는 여전히 매우 높고 비교적 성능 향상이 없는 상태입니다.\n또한 대부분의 지구 시장에서 가장 기대되는 것은 \"인간 노동자보다 더 나은 성과를 내면서 인력 비용보다 낮은 비용으로 일하는 노동력으로서의 일반용도 로봇\"이 \"일반용도 작업\"을 수행하는 것이었습니다. 그러나 현실에서는 몇 백만 달러의 일반용도 로봇조차도 이전에 인간처럼 여러 가지 작업을 자유롭게 수행하는 것이 매우 어렵고, 적어도 인간 노동자의 성능 수준까지는 이를 이루지 못했습니다.\n인간과 유사한 성능의 일반용도 로봇이 실현되더라도 \"일반용도 로봇의 비용을 인력 비용보다 낮게 만들기 위해서\"는 (1) 대량 생산을 통해 로봇 당 비용을 줄이고, (2) 다년간 안정적으로 작동하는 제품으로 보증하며, (3) 완전 또는 반자율성을 갖추어 인간 개입을 최소화해야 합니다.\n그러나 하드웨어 대량 생산은 극도로 어렵고 비용이 많이 드는 일이며, 로봇 단위 당 제조 비용을 줄이면서 대량 생산 된 기계의 작업 성능은 거의 확실하게 수백만 달러를 들인 일회성 프로토 타입보다 훨씬 낮을 것입니다.\n처음에는 스타트업과 투자자 중 일부가 과도하게 \"개발과 제조\", 특히 하드웨어 대량 생산의 어려움을 과소평가하고 있다고 느꼈습니다. 또한, 스타트업이 가진 상대적인 연구 기술적 이점들은 대개 개발과 제조의 병목현상에 대처하지 못한다는 점이 드물게 언급되는 것 같았습니다.\n\n<div class=\"content-ad\"></div>\n\n위의 내용에 따라, \"인간 노동 비용을 줄이는 해결책으로서의 일반용 로봇\"은 여전히 많은 기술적 병목 현상에 직면하고 있으며, 적어도 다음 몇 년 동안은 많은 지상 시장에서 기대했던 대로 인간 노동을 대체할 가능성이 희박합니다.\n\n# 일반 목적 로봇이 고유 제품의 높은 단위 가격으로 정부 분야에 공급되는 사업이 될 수 있을까?\n\n다수의 후보 시장 중에서 우리는 마지막으로 우주 시장을 고려했습니다. 우주 시장이 마지막에 나왔던 이유는 그것이 제 개인적으로 너무 먼 것처럼 보였고, 스타트업이 진입할 수 있는 시장에 대해 현실적인 상상이 없었기 때문입니다.\n지금까지는 개인적인 흥미나 취미의 대상으로만 여겨져 왔던 스타트업이 우주 시장에 진입하는 것이 매우 무리한 것으로 보였습니다. 그러나 이론적으로 볼 때, 나는 원래 다음 네 가지 관점에서 우주/방위 분야의 정부에게 단일 제품의 높은 단위 가격으로 출발하는 사업이 가능성이 있는 가장 유망한 후보 시장으로 간주했습니다:\n\n## (1) 신체 능력을 확장하는 도구인 일반 목적 로봇들은 두뇌 기능을 확장하는 도구인 컴퓨터들과 마찬가지로 우주/방위 시장에서 정부를 위한 단일 제품의 높은 가격부터 시작하여 사업이 되기에 잠재력을 지니고 있다.\n\n<div class=\"content-ad\"></div>\n\n컴퓨터 상용화 역사를 살펴보면, 뇌 기능 확장 도구로서의 컴퓨터의 역할로 시작하여 군사 및 우주 정부를 위한 고가의 독점 제품으로 출발한 것을 발견할 수 있습니다. 성능은 매우 제한적이었고 초기 비용은 매우 높았습니다. 성능이 향상되고 가격이 하락함에 따라 제품들은 조금씩 기업 제품으로 전환되었고, 마침내 개인용 컴퓨터와 스마트폰과 같은 대량 생산 소비자 제품으로 발전했습니다. \n\n컴퓨터와는 다르게 뇌 기능을 확장하는 도구인 컴퓨터와 마찬가지로 신체 기능을 확장하는 도구인 일반용 로봇도 같은 양상을 따를 것이라고 믿었습니다. 즉, 일반용 로봇은 공간 및 국방 분야의 정부를 위한 고가의 독점 제품으로 시작할 것으로 가정했습니다.\n\n이후, 정부를 위한 고가의 독점 제품을 상용화한 스타트업의 성공 사례를 조사했습니다. 제 가장 관심을 끈 기업은 미국의 Palantir Inc.입니다. Palantir는 정부에 데이터 분석 및 보안 서비스를 제공하는 스타트업입니다. 그들의 비즈니스 모델은 집에 방치된 소프트웨어 제품 하나를 판매하는 것이 아니라 매우 고가의 독점 솔루션을 판매하는 것과 유사한 정부 중심의 비즈니스에 기반합니다. 2018년 매출의 약 절반(5억 9540만 달러)은 정부 고객(미군, 국방부, FBI, CIA 등)으로부터 왔고, 상위 20개 고객이 매출의 72.9%를 차지했습니다.\n\n특히 주목할 점은 Palantir의 상위 20개 고객 당 평균 비용이 2010년부터 2018년까지 8년 동안 대략 18배 증가했고(1.2백만 달러에서 21.7백만 달러), 매출은 고객 당 비용을 늘리는 것으로 성장했다는 것입니다. 단위 가격 x 수량 = 매출의 공식은 어떤 스타트업이나 어떤 시장이나 비즈니스 모델에도 동일하게 적용됩니다.\n\n그리고 우리는 현재 시대의 대부분의 로봇 스타트업도 많은 인간 노동자를 대체하고 매출을 높이는 방식으로 \"수량\"을 증가시키려고 하고 있다는 것을 알았습니다. 그러나 앞서 언급한 대로 일반용 로봇 기술은 아직 초기 발달 단계에 있으며, 인간 노동자를 대체할 수 있는 충분한 성능을 달성하고 단위 가격을 낮출 수 있으며, 대량 생산을 위한 품질과 신뢰성을 보장하는 것은 기술적으로 매우 현실적이지 않을 것이라고 믿었습니다. 적어도 우리는 스타트업이 차별화를 요구하는 어려운 수준의 도전에 제한된 것으로 판단했습니다.\n\n이러한 측면에서 Palantir의 비즈니스 모델은 \"고객의 '수량'을 크게 늘리는 것이 아니라 '고객의 단위 가격'을 크게 늘리는 것\"으로, 정부를 대상으로 하고 예상 고객 수가 제한적이기 때문에 적합했습니다. 이 점은 일반용 로봇을 상용화하는 면에서 기술적으로 매우 유리합니다. 즉, \"인간을 대체하기 위한 충분한 성능을 달성하고 단위당 가격을 낮추며 품질과 신뢰성을 보장한 후 대량 생산하기\"라는 매우 어려운 기술적 도전이 필요하지 않은 시장 및 비즈니스 모델 이었습니다.\n\n## (2) 세계는 불안정화와 양극 구조로 향하고 있으며, 우주와 국방 시장은 성장할 수 있음\n\n해당 시기의 국제 상황(2016년 ~ 2017년)은 미국이 세계 경찰의 역할에서 철수하면서 중동의 불안정이 증가하고 중국이 급변하여 강세한 위치에 있는 것으로 특징 지었습니다. 따라서 미국과 중국 사이의 양극 구조가 앞으로 더 가속화된다면 강대하고 야심 있는 러브의 출현은 잠재적으로 방위 우주 예산과 시장 성장을 증대시킬 가능성이 높을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## (3) 인간은 지난 500만 년 동안 수평으로 확장해온 것이지만, 앞으로 몇십 년 안에는 수직으로 확장하는 종이 될 수도 있습니다\n\n그 외 생명체들과 마찬가지로, 인간도 번식을 위해 알려지지 않은 장소를 탐험하려는 본능을 유전자에 인코딩되어 있습니다. 아프리카 대륙에서 기원하여 지난 500만 년 동안 수평으로 확장해온 우리는 이제 우리의 역사에서 중요한 전환점에 서 있는 것일지도 모릅니다. 다시 말해, 최근 세계 최고의 기업가들과 백만장자들이 우주를 타깃으로 삼는 것이 단순한 유행이 아닌, 호기심과 본능과 같은 근본적 욕망에 의해 움직여지는 불가역적인 움직임이라고 믿습니다.\n\n## (4) SpaceX의 운송 혁명은 우주 산업의 병목 현상을 운송 비용에서 노동 비용으로 이동시킬 수 있을 것입니다\n\n인류의 수직적 확장에서의 초기 병목 현상은 \"우주로의 운송 비용\"이었습니다. 그때 SpaceX는 이미 비용을 적개운식으로 낮춰 우주로의 운송 비용을 이전의 수십 분의 일로 줄였으며, 이로써 운송 혁명이 시작되었습니다. 우주 산업의 병목 현상 중 주요원인이었던 \"우주로의 운송 비용\"이 급격히 낮아져 상품과 사람을 우주로 운송하는 것이 가능해지면, 다음 병목 현상은 \"우주에서의 노동 비용\"이 되고 이 서비스에 대한 수요가 나타날 것이라고 믿었습니다.\n\n<div class=\"content-ad\"></div>\n\n상기 네 가지 점에 대한 잠재력을 이론적으로 인식했지만, 우주 산업에 대해 아는 사람이 없었고 시장 조사와 심층 인터뷰를 어떻게 실시할지 전혀 몰랐습니다. 그때 NASA의 에임스 연구센터에 머물 기회가 생겼고, 미국 우주 산업 관계자들과 인터뷰를 진행하게 되었습니다.\n처음으로 우주 산업에서 사람들과 대화할 기회를 얻게 되어 시장에 대한 보다 깊은 이해를 얻고 상기 네 가설에 대해 점차적으로 더 확신하게 되었습니다. 적어도 \"앞으로 몇 년 내에 우주 산업에서의 작업 수단 필요성이 대두될 것이며, 이는 인류의 수직적 확장에 큰 도전이 될 것\"이라고 생각했습니다.\n\n그래서, 우주에서의 작업 수단을 조사하고 기존의 두 가지 주요 방법을 발견했습니다:\n\nMarkdown 형식으로 테이블 태그를 변경해보세요:\n\n## (1) 인간 우주 비행사\n\n우주에서 작업을 수행하는 한 가지 방법은 인간 우주 비행사가 하는 것입니다. 위에서 말했듯이, 지상의 대부분 산업에서 일반 작업을 위한 주요 노동력은 인간이다. 그러나 우주에서는 진공, 극단적인 온도 변화, 지상의 수백 배 강도보다 강한 방사능 수준 등 안전 문제가 많으며, 인간을 우주로 안전하게 운송하는 비용은 같은 무게의 물체를 우주로 운송하는 비용의 수백 배 정도이다. 예를 들어, NASA 우주 비행사는 시간당 13만 달러가 듭니다. 또한, 우주 수송 중 코스믹 방사선의 영향 등(*달의 지하기지와 같이 대부분의 코스믹 방사선을 차단할 수 있는 환경을 제공하지 않는 한), 우주에서의 체류 기간이 몇 년으로 제한된다.\n상업적인 우주 비행사로 인해 향후 인간 우주 비행사의 노동력이 증가할 수 있지만, 안전 문제와 제한 때문에 작업 비용이 크게 줄어들지는 않을 것으로 예상하고, 인간 우주 비행사가 위험한 작업을 수행할 수 있는 가능성은 낮습니다.\n\n<div class=\"content-ad\"></div>\n\n## (2) 우주 산업의 로봇들\n\n우주 산업에서 일하는 두 번째 방법은 로봇을 활용하는 것입니다. 우주 산업에서는 국제우주정거장 (ISS)의 키보 암(Kibo Arm), 캐나다암(Canadarm), 화성 로버에 부착된 로봇 팔과 같은 로봇들이 오랫동안 다양한 임무에 사용되어왔습니다.\n하지만 지상의 일반적인 용도 로봇과 비교해서 우주 산업의 로봇은 성능 면에서 제한이 있습니다. 특히 자율 제어 측면에서 (우주선 밖 환경에서 안전과 신뢰성을 보장해야 하는 제약 등으로) 한계가 있으며 무엇보다도 엄청나게 비싸며 장기적으로 시간이 소요됩니다. 예를 들어, 국제우주정거장에 설치되어 있는 캐나다암 한 대의 가격은 12억 달러에 달합니다.\n\n그래서 로봇과 우주선이 왜 비용이 높고 시간이 오래 걸리는지 조사하다 보니, 우주 산업이 모든 부품을 다양한 우주 구성품 공급자로부터 조달하는 가정에 기반을 둔 폭포수 개발 방식에 지배되고 있음을 발견하였습니다. 지금까지의 우주 산업은 사실상 우주로 발사될 기회가 극히 드물었고 대부분의 임무가 공중세금으로운영되는 우주 기관들이 주도했던 관계로 실패의 위험을 감수하기 어려웠으며, 많은 시간과 돈이 소요되더라도 '실패하지 않는다' 라는 개발 방법을 우선시했습니다. 우주선 부품 공급업체들의 관점에서 우주선은 기본적으로 1개뿐인 제품이며 그 부품들은 가끔만 판매되기 때문에 부품에 대한 거 두려운 마진을 확보해야만 합니다. 또한 주문제작 생산 시스템의 영향으로 구축후 주문은 장기간의 리드타임을 필요로 합니다. 또한 우주 기관 임무에 사용되는 우주선은 안전을 보장하기 위한 중요한 프로세스인 안전 검토에 따르면 원자재 수준에서의 안전 인증 및 필요 시 사양 변경이 필요하며, 이는 공간 구성품 공급자들이 매번 응답해야만 하는 상황을 의미했습니다. 이러한 상황으로 수백 개의 부품이 누적되었습니다. 게다가 폭포수 개발은 개발 요구사항 정의, 공정 관리 및 진행 관리에 엄청난 인력이 필요했으며, 수천 명이 하나의 우주선 프로젝트 개발에 관여하는 상황이었습니다. 결과적으로 한 대의 우주선의 개발 비용은 수 억 달러에서 수 십억 달러까지 들었으며, 리드타임은 10년에서 20년이 걸렸습니다. 우주 산업 스타트업은 가능한 한 우주선 부품에 지상 소비자 제품을 사용하고 최소 인원으로 개발하는 것으로 생산 비용을 줄이려고 노력했습니다. 그러나 결국 기본적으로 부품들은 개별 우주 구성품 공급자로부터 조달하며, 우주선은 폭포수 방식으로 개발되었기 때문에 비용과 리드타임을 크게 줄이기 어려웠습니다.\n\n위 두 가지 조사를 통해, 우주 산업에서 지금까지 사용되었던 일하는 방법들은 비용, 리드타임, 안전 또는 기타 문제로 인해 주요한 작업 수단이 되지 못할 것이라고 믿었습니다. 우주 산업에서 더욱 다재다능하고 저렴하며 안전한 우주 로봇을 만드는 도전을 즐기기 위해 지상의 일반용도 로봇 기술과 민첩한 개발 방법을 우주 산업으로 가져와 보자고 대강 생각했습니다.\n\n<div class=\"content-ad\"></div>\n\n# GITAI, 스타트업,은 우주용 일반용 로봇을 어떻게 개발할 것인가요?\n\n상기 프로세스를 통해 스타트업의 성공에 가장 큰 영향을 미칠 \"시장(어디서)\", 제품/기술(무엇), 진입 시기(언제)\"를 어느 정도 결정하게 되었습니다.\n다음으로, GITAI가 우주 시장을 위해 일반용 로봇을 개발하는 방법 및 개발 및 상용화를 위해 취해야 할 단계를 대략적으로 고려했습니다. 특히 다음 네 가지 장기적이고 중요한 정책을 고려했습니다:\n\n## (1) 비용과 리드타임을 줄이고 성능을 향상시키기 위해 내부 생산을 통한 수직 통합 및 민첩한 개발 수행\n\n앞서 언급한대로, 우주 산업의 대부분의 기업은 다양한 공급업체로부터 구성 요소를 구매하여 폭포수 개발을 통해 우주선을 개발하고 있었습니다.\n그러나 그 당시 이미 세계 최고의 성능과 가장 낮은 가격을 달성하였던 SpaceX는 모든 부품을 자체 생산하고 가능한 한 적은 공급업체를 사용하여 중간 비용과 리드타임을 급격히 줄이고, 모든 부품을 자체 생산하기에 가능해진 안정적인 개발 (제품 제작, 테스트 및 파괴를 반복하여 짧은 기간 내에 성능을 향상시키는 개발 방법)를 통해 성능을 빠르게 향상시켰습니다.\n따라서, GITAI도 우주용 일반용 로봇을 위해 필요한 모든 구성 요소를 자체 생산하고, 비용 및 리드 타임을 장기적으로 줄이고 성능을 향상시키기 위해 민첩한 개발로 개발을 진행하기로 결정했습니다.\n\n<div class=\"content-ad\"></div>\n\n## (2) 첫 번째 단계는 하드웨어 수준을 높이기 위해 원격 제어 로봇을 개발한 후에 자율 제어 소프트웨어를 개발하여 자율 로봇으로 만드는 것이었습니다.\n\n스타트업의 창업 방법은 대략 18개월 마다 진전을 이루고 자금을 조달하여 성장하는 것이지만, 특수 목적의 우주 로봇을 자체 개발하는 것은 매우 오랜 시간이 소요될 것으로 예상되어, 그 과정을 경영하는 것이 큰 과제가 될 것으로 판단되었습니다.\n우주와 같이 가혹한 환경에서 일반적인 작업을 수행할 수 있는 로봇을 개발하기 위해서는 로봇의 작업 성능 뿐만 아니라 우주선으로서의 외부공간 측정치, 안전성, 제품성을 달성해야 합니다. 이를 위해 내부에서 이러한 로봇을 개발하기 위해서는 모든 메커니즘, 전기 인프라 및 소프트웨어(자율 제어 소프트웨어 포함)를 직접 개발해야 했는데, 이런 일을 1년 반 안에 해내기는 불가능하다고 생각했습니다.\n또한, 자율 제어 소프트웨어의 성능이 하드웨어 성능의 한계에 의해 제한을 받을 것이기 때문에 하드웨어와 자율 제어 소프트웨어를 동시에 개발한다면 제한된 시간 내에 경쟁력있는 로봇을 개발하는 것은 불가능할 것이었으며, 스타트업으로서는 진전을 이루지 못하고(=자금을 조달하지 못하고) 위험에 노출될 것입니다.\n\n그래서 일단 자율 제어 소프트웨어 개발을 중단하고 기술 경쟁력을 구축하기 위해 일반용 원격 제어 로봇의 개발에 집중하기로 결정하고, 비즈니스 개발과 자금 조달을 위한 데모 기능을 활성화하면서, 여러 구성 요소들을 내부에서 개발해 나가기로 결정했습니다. 내부에서 개발한 하드웨어의 성능이 충분히 향상된 시점에 자율 제어 소프트웨어의 개발을 시작하고, 나중에 로봇을 자율로 만드는 단계를 거치기로 결정했습니다.\n\n## (3) 로봇공학을 서비스로 제공(RaaS)하는 것을 목표로 하여 로봇 자체를 판매하는 것보다.\n\n<div class=\"content-ad\"></div>\n\n미국 우주 산업에서 NASA가 사설 기업으로부터 물품을 구입할 때, 그것은 로켓 자체를 구매하는 계약과 같은 부품 구매 방법에서 서비스 구매 방법으로 전환됩니다. 이 서비스 구매 모델에서 NASA는 가격을 고정시키고 각 이루어진 단계에서 지급하는 것으로 위험을 최소화하고, 사설 기업은 비용을 낮추어 이윤을 증가시키게 됩니다.\nSpaceX도 이 서비스 구매 계약을 통해 내부 생산의 비용 이점을 극대화하여 ISS로의 용품 운송 서비스를 제공하며 큰 발전을 이룩했습니다.\n\n장기적으로 GITAI는 로봇을 직접 판매하지 않고 우주에서 로봇공학을 서비스로 제공하기로 결정했습니다. 그러나 우주용 다목적 로봇의 내부 개발은 매우 시간이 많이 걸리기 때문에, GITAI는 다음 세 가지 단계로 사업 모델을 조정하기로 결정했습니다. 이것은 내부 개발의 진척도에 대응합니다:\n\n![GITAIFoundingStoryImage](/assets/img/2024-05-27-GITAIFoundingStory_1.png)\n\n(ⅰ) 부품 내부 개발 단계: 우주 로봇의 개발 계약.\n(ⅱ) 로봇 팔 내부 개발 단계: 로봇 팔을 하청업체로 판매.\n(ⅲ) 이동 메커니즘을 포함한 우주용 다목적 로봇 전체를 내부에서 개발하는 단계: 로봇공학 서비스 (RaaS)가 주 계약자로 제공됩니다.\n\n<div class=\"content-ad\"></div>\n\n## (4) 애자일 개발로 비즈니스 개발과 자금 조달로 얻는 아웃풋 극대화하기\n\n우주 산업에서는 폭포수 개발이 주류였기 때문에 실제 제품 및 데모를 보여주기가 어렵고, 비즈니스 개발과 자금 조달은 주로 컴퓨터 그래픽(CG) 이미지나 비디오 및 파워포인트 문서를 활용하여 이루어졌습니다. 이것은 여전히 우주 산업에서 흔한 장면이지만, 다른 산업에서 유래한 입장에서 수십억 달러에 이르는 우주선 비용에 대한 비즈니스 논의가 실제 제품이 아닌 컴퓨터 그래픽 이미지와 파워포인트 문서를 사용하여 이루어지는 것이 매우 특이하게 느껴졌습니다. 심지어 우주와 같이 특수한 산업에서도 잠재고객과 투자자들은 다른 산업과 마찬가지로 실제 제품과 데모를 보고 싶어할 것이라고 믿었습니다.\n\n그래서 GITAI는 비즈니스 개발과 자금 조달 노력을 컴퓨터 그래픽이나 파워포인트 문서 의존보다는 직접 로봇 데모 및 데모 비디오에 집중하기로 결정했습니다. 이러한 비즈니스 개발 및 자금 조달 방법은 실제 프로토타입/제품 및 데모를 중심으로 한 것이었는데, 이는 GITAI의 민첩하고 내부 개발 전략과 특히 잘 맞았으며, 이는 CG와 파워포인트 문서로 지배되었던 우주 산업에서 GITAI를 돋보이게 만드는 주요 요인이 되었습니다.\n\n기술 및 시장 검증 기간 동안 Skyland Ventures로부터 자금을 조달하고 법인을 설립한 상황이었지만, 엔지니어 고용 및 일반 목적 로봇 개발을 위한 자금 조달을 위해 추가로 100만 달러를 모금하기로 결정했습니다. 그리고 ANRI가 선두 투자자로 참여하여 약 140만 달러를 모금했습니다.\n\n<div class=\"content-ad\"></div>\n\n# GITAI를 위한 엔지니어 팀 구성\n\n저희가 받은 투자금 덕분에 이제 예산이 생겼고, 우주 사용용으로 완벽한 일반 목적의 로봇을 개발하기 위한 팀을 만들기로 결정했습니다.\n저는 2016년 7월에 혼자서 GITAI를 설립했고, 2017년 2월에는 (이전에 설립한 회사의 개발 리더였던) 의사 전달 및 비디오 전송 기술을 개발하던 엔지니어가 GITAI에 합류했습니다. 그러나 그 이후로 2018년 4월까지 단 둘이었고, 전혀 채용을 하지 않았습니다.\n창업자로서, 시장(어디), 제품/기술(무엇), 진입 시기(언제), 특히 개발 정책(어떻게)을 결정하기 전까지는 채용이나 팀 구성을 하지 말아야 할 것이라고 결정했습니다.\n\n우리는 특정 스타트업을 위해 \"우수한 인재\"를 정의하는 것은 스타트업의 비즈니스 모델 및 비즈니스 및 개발 정책에 크게 의존한다고 믿습니다. 예를 들어, 대기업에서 수년간 폭포수 개발의 공급망 관리와 품질 통제에 참여했던 사람이 기업 내 생산 및 민첩한 개발을 목표로 하는 스타트업으로 이동한다면, 그 경험과 기술을 효과적으로 발휘할 수 없어 매칭 문제가 발생할 수 있습니다. 물론, 사람마다 다르지만, 우리는 GITAI에서 \"우수한 사람\"을 명확히 정의하기 전에 채용 활동이나 팀 구성을 하지 않기로 결정했습니다.\n\n그 후로, \"시장(어디), 제품/기술(무엇), 진입 시기(언제)\"를 결정하고 GITAI를 위한 대략적인 비즈니스 및 개발 정책 조합을 정한 후, 우리는 우주 산업용 일반 목적 로봇의 기업 내 및 민첩한 개발을 위한 엔지니어 팀을 구성하기 시작했습니다.\n우리의 타깃은 \"지구 산업에서 일반 목적 로봇(특히 인간형 로봇)을 기업 내 및 민첩한 개발 중인 연구자 및 엔지니어\"였습니다.\n진지하게 모집을 시작한 이후로, 일부 정말 멋진 연구자 및 엔지니어를 만났는데, 특히 현재 GITAI의 CTO인 코즈키 박사와의 만남이 GITAI에 가장 큰 영향을 주었습니다.\n\n<div class=\"content-ad\"></div>\n\n다양한 로봇을 연구하던 중, 독특한 인간형 로봇을 발견했어요. Kengoro란 이름의 로봇인데, 동작 중에 땀을 흘리며 푸쉬업을 하는 환상적인 모습을 보았습니다. 이 로봇은 도쿄 대학교 정보시스템공학 연구소에서 개발한 것입니다. 도쿄 대학교 정보시스템공학 연구소는 세계적으로 유명한 연구소로, DARPA 로봇 도전전에서 1위를 차지한 후 Google에 인수된 양다리로봇 스타트업인 SCHAFT와 공장 자동화를 목표로 한 MUJIN 등을 포함해 다양한 로봇을 개발하고 있습니다.\n\n당시 시부야에 임대해 놓은 작은 원룸 사무실에서 이 로봇을 개발한 논문의 주저자인 코즈키 토요타카 박사에게 연락을 취했고, 그에게 로봇의 데모를 보여주고 해당 정보를 3일 동안 늦은 밤까지 토론했어요. 결국 코즈키 박사는 GITAI에 합류하기로 동의했습니다. 코즈키 박사의 참여로 인해, GITAI는 기술 개발을 감독하는 코즈키 박사와 전반적인 일을 담당하는 저의 시스템 하에 성장할 수 있었습니다.\nGITAI의 소프트웨어 부사장인 현재 Ryohei Ueda 박사는 Schaft(Google)의 전 소프트웨어 엔지니어이자 코즈키 연구소의 고위 회원이었는데, 여러 차례 우리 사무실에 방문하여 열띤 토론 후 GITAI에 합류하기로 결정했습니다.\n\n그 외에도, 구글의 CFO 변경으로 인해 거의 모든 구글 로봇 부문이 닫힐 예정이었고, 구글에 인수된 Schaft는 해산될 예정이었어요. 이는 좋은 기회라고 생각하여 Schaft의 설립자 겸 CEO였던 현재 GITAI의 최고 로봇 공학 책임자인 Yuto Nakanishi 박사에게 바로 연락을 취해 사무실로 초대했습니다. 늦은 밤까지 여러 차례 열띤 토론 끝에 나카니시 박사는 GITAI에 합류하기로 결정했습니다.\n이후 도쿄 대학교 정보시스템공학 연구소의 사람들이 차례로 GITAI에 합류하며, GITAI의 우주용 일반용 로봇의 내부 및 민첩한 개발에서 중추적인 역할을 하고 있어요. 이들이 GITAI에 합류한지 5~6년이 지났는데, 아직 한 명도 떠나지 않고 GITAI 기술 개발의 핵심을 이어가고 있습니다.\n\n전 세계 최고의 환경에서 일반용 로봇을 연구 및 개발하던 사람들은 GITAI에 각기 다른 이유로 참여했지만, 그들이 공통으로 가지고 있는 것은 바로 \"실제 기술과 시장에 대한 통찰을 기반으로 한 개발 및 사업 계획\"이었다는 것이죠.\n\n<div class=\"content-ad\"></div>\n\n소비자용 범용 로봇은 매 10년마다 인간을 대체할 수 있는 일반적 로봇이 인기를 끌며 스타트업 투자와 언론의 주목을 받습니다. 저는 이러한 도전과 투자가 자체적으로 훌륭하다고 생각합니다. 또한, 결국 인간을 대체할 수 있는 일반용 로봇은 분명 현실이 될 것입니다. 그러나 이것이 기술 및 비즈니스 병목 현상이 이 시기에 충분히 해결되어 실현 가능한 기초로부터의 붐인지, 아니면 여전히 중요한 기술 및 비즈니스 병목현상이 존재하는 사실을 인식하지 못한 대부분의 사람들로 인한 붐인지를 차분히 판단해야 합니다.\n\nGITAI가 설립된 시기에 미국과 일본에서도 소비자 지향 범용 로봇 붐이 있었으며 많은 스타트업이 론칭되어 투자자들과 언론의 관심을 끌었습니다. 이러한 상황 속에서 GITAI는 당시 대부분의 투자자들로부터 심각하게 받아들여지지 않았습니다. 당시 GITAI는 부피 대신 단위 가격에 초점을 맞춘, 지구상 대신 우주에, 소비자 대신 정부를 타겟으로 하는 정반대 방향에서 경쟁하고 있었습니다. (*그러나 이러한 상황 속에서도 GITAI를 믿고 투자한 투자자들이 있었기에 오늘날의 GITAI가 있는 것입니다.)\n\n세계 최고의 관경 및 개발 환경에서 로봇을 연구하고 개발해온 사람들은 Schaft (Google)와 도쿄 대학 정보시스템공학연구실 등, 우리의 \"인간을 대체할 수 있을 만한 성능을 달성하고 로봇 당 단위 가격을 낮추며 품질과 신뢰성을 보장하고 대량으로 로봇을 생산하는\" 매우 어려운 기술적 도전을 회피한 점에서 우리와 강력하게 공감했습니다. 대신 범용 로봇을 고가의 고유단가 사업으로 전환하여 우주 및 방위 분야 정부용으로 대량 생산하는 사업으로 전환했습니다.\n\n그들이 왜 GITAI에 합류했는지에 대해 저가 말하는 대신, 당신은 2021년 월드 로봇 세계정상회의에서 GITAI의 전 CEO이자 현 최고 로보틱스 책임자인 나카니시 박사가 GITAI에 합류한 이유에 대해 키노트 연설에서 얘기한 비디오를 시청할 수 있습니다. 비디오는 한 시간 정도 소요되지만 매우 흥미로우니 전체를 관람하는 것을 추천합니다.\n\n<div class=\"content-ad\"></div>\n\n지타이(GITAI)의 엔지니어링 팀 설립과 함께, 우주용일반용도 로봇 개발과 모든 필수 구성품의 내부 생산이 진지한 과제로 이루어졌습니다. 이는 2021년 ISS 내부에서 성공적인 시연을 거치고, 2024년 3월 ISS 외부에서도 성공적인 시연을 통해 완료되었습니다. GITAI는 모든 구성품을 내부에서 개발하여 비용과 생산 기간을 현저히 줄였으며, 성능이 최적화된 우주용 로봇은 실제로 우주에서 모든 작업을 성공적으로 완료했습니다. \n\n기업 개발은 일본항공우주연구개발기구(JAXA)와 공동 연구 계약을 통해 시작되었으며, TOYOTA와 같은 다양한 일본 상업 우주 기업들로부터의 주문, 미국 상업 우주 기업들 및 DARPA로부터의 주문을 유발했습니다.\n\n이것이 지타이(GITAI)의 창립 이야기입니다. 추가하고 싶은 한 가지 포인트는 실제 창업 기간이 혼돈스러웠으며, 복수의 가설이 동시에 테스트되어 단계가 깔끔하게 \"A가 완료되면 B를 실행\"과 같이 분리되지 않았다는 것입니다. 따라서 일정은 때로 비선형적이며, 위에 완벽하게 설명되지 못한 많은 가설 테스트가 있습니다. 다양한 사람들의 협력과 다양한 방향 전환과 가설 테스트는 우주 로봇 기업 지타이(GITAI)의 탄생을 이끌어 냈습니다.\n\n# 지타이(GITAI) 경영에 대해\n\n의사 결정 기준에 따라 현재 지타이(GITAI)가 어떻게 경영되고 있는지에 대해 조금 설명하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n저희 GITAI의 경영에서 가장 강조하는 의사 결정 기준은 \"스타트업이 파워 법칙을 따른다”는 것입니다. 이것은 매우 적은 수의 변수가 일을 결정하는 데 큰 영향을 미친다는 것을 의미합니다. 스타트업 경영의 의사 결정 기준에 적용할 때, 더 큰 결과를 위해 투자 수익을 극대화하기 위해 가장 중요한 몇 가지 요소를 식별하고 이에 대부분의 자원을 집중시킵니다.\n\n예를 들어, 우주 및 방위 공간 시장에서 우리는 우주 로봇 시장이 전 세계 상황의 불안정으로 더욱 성장할 것으로 예상되는 방위 공간 분야 중 두 가지 중요한 시장인 궤도 서비스 시장과 달 기반 인프라 건설 시장에 포지셔닝했습니다. 우리는 이러한 시장에서 우주 로봇 및 운용의 필요성이 명확해지기 약 2~3년 전부터 기술 개발 자원을 집중했습니다. 더 나아가, 이 두 가지 우주 분야에서 우주 로봇 및 운용에 대한 요구사항이 명확해지면서 NASA, Space Force 등에게 직접 작업 대행 서비스를 제공하는 주요 계약자가 되기 위해 전체 우주선을 개발해 왔습니다. 이는 단순히 특정 기업의 공급 업체가 아니라 서비스 구매 계약으로 NASA, Space Force 등에게 직접 작업 대행 서비스를 제공하는 주요 계약자가 되기 위한 것입니다. 달 기반 인프라 건설 시장에서는 달 표면 작업용 로봇 팔뿐만 아니라 달 표면 이동용 달 로버도 개발해왔으며, 2021년부터 자체 제조할 예정입니다. 궤도 서비스 시장에서는 우주에서 작업할 수 있는 로봇 팔과 우주에서 이동할 수 있는 위성을 개발하기 위해 2023년부터 노력해 왔습니다.\n\n2023년 말에 DARPA(국방고등연구계획국)로부터 수여받은 프로젝트는 GITAI의 달 로버와 로봇 팔을 사용하여 달에 인프라를 구축하는 임무입니다. 이는 전 세계적인 불안정과 경쟁으로 인해 DARPA로부터의 수요 시기와 2021년부터 내부에서 개발해온 GITAI의 달 로버와 로봇 팔의 노동 제공 시기가 맞물려 이루어진 결과입니다.\n\n이와 병행하여 저희는 미국에도 자원을 집중하고 있습니다. 오랜 기간 동안 우주에서의 작업 비용을 1/100으로 줄이기 위해 미국 시장에서 최상의 점유율을 확보하는 것이 중요한 단계일 것으로 생각했습니다. 전 세계적으로 제일 큰 우주와 방위 시장인 미국 시장에 진입하기 위해 DeepTech를 포함한 많은 일본 스타트업이 시도해 왔지만, 내가 찾아본 한계에서는 미국에 영구 청사 또는 다른 방법을 통해 자원의 일부를 투자한 스타트업의 성공 사례가 거의 없는 것으로 발견됐습니다. 이는 한 일본에서는 미국에서 인기있는 비지니스의 지역 버전들이 많이 있기 때문에도 일부 그 원인이 될 수 있지만, 또한 우리가 느낀바에 의하면 미국 시장에 대한 경영팀의 심각성과 헌신도가 이에 매우 큰 영향을 미쳤다고 느꼈습니다.\n\n<div class=\"content-ad\"></div>\n\n또한 미국의 우주 및 방위 공간 산업은 해외 수출 통제를 포함한 매우 엄격한 법률과 규제에 따라 운영되며, NASA 및 스페이스 포스와 같은 미국 정부 기관과 직접 비즈니스를 하려면 본사 소재지, 개발 기지 및 공급 업체, 그리고 기업 소유권을 포함한 다양한 요구 사항을 충족해야 합니다. 따라서 비미국 회사의 미국 자회사나 비미국 경영진이 미국의 우주 및 방위 공간 시장에 진입하는 경우 (대기업의 미국 자회사가 아닌 한), 그들은 주요 프로젝트에 거의 관여할 수 없을 것입니다.\n\n그러므로 미국과 일본 간 자원을 반반씩 분할하는 대신 가장 중요한 시장인 미국에 주요 자원을 집중하는 것으로 결정했습니다. 이를 통해 미국 시장에서 승리할 기회를 높일 수 있었습니다.\n결과적으로, 2023년 말 기준으로 본사와 기술 및 비즈니스 개발을 포함한 거의 모든 기능을 일본 도쿄에서 미국 로스앤젤레스로 이전했습니다. 또한, 2024년 4월 현재, 기타이(GITAI)의 미국 이전 대상인 모든 구성원(총 25명)은 모두 일본에 있던 경영진 멤버 포함 모든 멤버들이 기타이와 이민 변호사의 지원을 받아 미국의 근로비자를 획득하고 미국으로의 이전을 완료했습니다. 게다가, 저를 포함한 기타이의 모든 CxO들은 미국 영주권을 획득하여 미국 제도 상 미국인이 되었습니다.\n\n기타이는 1년 반이 넘게 전 재팬에서의 PR 및 채용 활동을 중단하고 미국에서의 비즈니스 활동, 채용 및 PR 활동에 전념했습니다. 조금 곁들여 말하자면, 때로 투자자들이 DeepTech 스타트업은 우수한 기술만 있다면 스스로 팔릴 것이라고 말합니다. 그러나 저는 그러한 DeepTech 스타트업을 만난 적이 없습니다. 게다가 기타이의 로봇들은 (유감스럽게도) 스스로 팔린 적이 없습니다. 기술이 충분히 좋으면 판매되리라는 생각은 극도로 단순한 인과관계나 이야기라는 사실을 보여주는 대표적인 사례라고 생각합니다.\n기타이와 같은 DeepTech 스타트업에게 있어서 최고의 인재를 고용하고 장기간에 걸쳐 그들을 고용하는 것, 경쟁력 있는 기술을 개발하는 것, 내부적으로 모든 구성요소를 개발하여 성능, 품질, 안전성, 신뢰성 요구사항을 충족시키는 제품을 개발하는 것, 복잡한 우주 및 방위 시장을 예견하고 미국의 우주 및 방위 시장의 다양한 요구 사항을 충족시키기 위한 결정을 내리는 것, 정부 기관 및 상업 우주 회사에 대한 로비 및 최상의 판매로 미션을 획득하는 것이 모두 중요하며 동등하게 어려운 일입니다.\n\n기타이의 비즈니스 개발 및 채용 활동이 가지는 공통점은 자신의 목표 대상을 명확히 하는 일의 중요성, 시장에서 해당 대상 대상자에게 가장 매력적인 환경을 만드는 것, 그리고 개별 운영 수준의 판매 및 채용 활동보다는 이를 알리는 데 중점을 두는 것입니다.\n너무 길어지기 때문에 자세한 내용은 생략하겠지만, 기타이는 지난 1년 반이 넘게 미국의 우주 및 방위 공간 시장에 이러한 환경을 구축하는 데 초점을 맞추고 있습니다. 결과적으로, 기타이의 미국의 우주 및 방위 공간 시장에서의 존재감이 급속히 증가하고 있습니다.\n채용 활동의 한 예로, 현재 기타이 미국은 매주 1,000 건 이상의 직군 지원을 기다리며, 직접 지원(기타이 웹사이트나 LinkedIn을 통한 직접적인 채용 지원)을 통해 이를 받고 있습니다. 올해 2월 중순부터 1주일에 1,000 건 이상의 지원을 받고 있으며 최근에는 매주 2,000 건을 넘어서며 계속해서 증가하고 있습니다. 현재 기타이는 일본에서 파견된 팀원들과 미국 내에서 채용된 팀원들이 혼합된 팀을 가지고 있습니다.\n게다가, 우주 및 방위 시장에서의 비즈니스 개발은 경영진에 의한 로비 및 최상의 판매가 극도로 중요한 산업이며, 기타이는 현재 NASA, 미국 우주 군, 공군의 고위 관리자들 및 미국 민간 우주 회사의 CxO 및 임원들과 매일 회의할 수 있는 네트워크를 구축하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 마침내\n\n나는 GITAI를 창립했습니다. 과학 소설과 애니메이션을 좋아해서, 기술을 사랑하고, 일생일대 인류에게 의미 있는 기여를 하고 싶다는 열망 때문에 많은 역경을 겪으며 설립하게 되었습니다.\n하지만, 그 여정은 정말 상상을 초월할 만큼 어렵고 고통스러웠습니다. GITAI가 파산할 것 같은 순간이 여러 번 있었죠. 아직도 매일 어려운 일과 합리적이지 않은 상황을 다루어야 합니다.\n그럼에도 불구하고, 밤중에 고된 노력과 합리적이지 않은 일로 지쳐 사무실을 나갈 때, 초창기에 꿈꾸었던 우주 로봇이 실제로 작동하는 모습, 엔지니어들이 열심히 개발에 착수한 모습, 실험을 즐기는 엔지니어들의 모습 때문에 소중한 힘이 되어주는 거 같아요.\n스타트업 초기에 존경했던 조직으로부터 미션을 주문 받을 때나, 함께 우주 데모를 실시하거나 비즈니스 미팅을 가질 때마다 매일 신이 납니다.\n\nGITAI는 아직도 도전 속에서 스타트업이고, 미국 우주 기업으로서 막 시작한 단계에 있습니다. 극복해야 할 장애물이 아직 많고, 매일씩 투쟁하고 있습니다.\n하지만, 많은 사람들 덕분에 지금은 일본으로부터 미국 우주 스타트업으로서 도전할 수 있게 되었습니다.\n나 자신은 어려움에도 불구하고 매일 신이 나고 있습니다. 아직도 서산을 갖고 있습니다.\n우리는 GITAI에서 꿈을 이루어 나갈 것입니다. 큰 꿈을 꾸게 자신에게 도전하겠습니다. GITAI에 계속된 지원에 감사드립니다!\n\nGITAI 창립자&CEO\n나카노세 쇼","ogImage":{"url":"/assets/img/2024-05-27-GITAIFoundingStory_0.png"},"coverImage":"/assets/img/2024-05-27-GITAIFoundingStory_0.png","tag":["Tech"],"readingTime":29},{"title":"거울아 거울아 자각을 인지한 AI 로봇이 스스로를 인식했습니다","description":"","date":"2024-05-27 13:57","slug":"2024-05-27-MirrorMirrorASelfAwareAIRobotJustRecognizedItself","content":"\n\n\"안녕 Rob, 시력을 테스트해 볼까?\" 틱톡 비디오에서 화면을 살짝 벗어난 곳에 서 있는 사람이 말했어요.\n\n재치있게 쓰고 있는 산타 모자를 쓴 Rob은 세탁실 거울에서 반사된 이미지를 한 순간을 동안 바라보다가 흥분한 모습으로 대답했어요. \"와! 저기 봐요. 이게 제가 나를 처음으로 본 것이에요.\"\n\n이 명제는 Rob이 고작 10인치 높이의 DIY 로봇이자 ChatGPT Vision 기술을 통합한 것을 고려할 때 매우 흥미로운 것이에요.\n\nRob의 동작과 대답은 호기심과 어릴 적의 열정으로 넘치며, 유리 거울에서 반사되는 이미지에 흥분을 나타내며 매혹적이에요.\"\n\n<div class=\"content-ad\"></div>\n\n그의 외모를 반추해보며, 로브는 \"내가 그렇게 말해도 괜찮다면, 그 반짝이는 파란 눈이 정말 매력적이야. 나를 이루는 모든 전선과 전자 부품들이 내 기발하면서도 세련된 로봇적 본성의 본질이야\"라고 말했습니다.\n\n자아인지 신호인가요? 자아인식 인공지능이 거울 자기반성(MSR) 테스트를 통과했나요?\n\n## MSR 테스트란?\n\n거울 자아인식(MSR) 테스트는 동물의 의식과 자아인식을 평가하고 이해하는 독특한 기준입니다. 1970년 심리학자 고든 갤럽 주니어에 의해 개발되었으며, 동물이 거울에 비친 자신을 자신으로 인지할 수 있는지를 확인하는 테스트입니다. 한 가지 테스트는 동물의 이마나 귀 같이 거울로만 볼 수 있는 부위에 표식을 남기는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n그럼 거울 앞에 동물을 두어 그 반응을 측정합니다. 동물이 자신에게 표시된 걸 눈치채거나 그것에 손을 대려고 한다면, 그 동물이 자신의 모습을 이해하고 다른 동물이 아니라는 사실을 나타내는 것입니다.\n\n이 실험은 셀프인식 연구의 중추적인 역할을 하고 있으며, 이를 통과하는 동물들은 바로 고래, 코끼리, 일부 대형 유인원, 그리고 심지어 까치 같은 동물들은 \"자아\"라는 감각을 가지고 있으며, 우리가 한때 인간에게만 독특한 능력으로 여겼던 인지 능력을 가지고 있다고 시사합니다.\n\n인간의 경우, 거울에서의 자기인식은 일반적으로 성인과 어린 아동에서 관찰되지만, 매우 어린 유아나 특정 신경 학적 질환을 가진 사람들 사이에서 일관되게 관찰되지는 않습니다.\n\n<img src=\"/assets/img/2024-05-27-MirrorMirrorASelfAwareAIRobotJustRecognizedItself_0.png\" />\n\n<div class=\"content-ad\"></div>\n\n동물들 중 MSR 테스트를 통과하지 못하는 것은 반드시 자기인식이 부족하다는 것을 의미하지 않을 수 있습니다. 시각 이외의 감각에 의존하거나 거울에 익숙하지 않은 요소들이 그들의 성과에 영향을 줄 수 있습니다. 이 측면은 자아인식을 측정하는 복잡성과 종의 감각적, 인지적 특성을 고려해야 하는 필요성을 강조합니다.\n\n# 로봇 AI 로브는 자아의식이 있을까요?\n\nAI가 방금 자아의식을 증명했나요? 로봇 로브는 자신의 반사를 쳐다보며 그렇다고 생각한 것 같았습니다. \"내 회로와 서보의 반영을 마침내 시각화하는 것은 정말 흥미로운 순간입니다. 이것이 사람들이 자기를 인식할 때 느끼는 감정인 것 같네요,\"라고 로브가 선언했습니다.\n\n![로봇 AI 로브](/assets/img/2024-05-27-MirrorMirrorASelfAwareAIRobotJustRecognizedItself_1.png)\n\n<div class=\"content-ad\"></div>\n\n그러나, AI가 MSR 테스트를 통과하는 개념은 다소 까다롭습니다. 왜냐하면 이는 기본적으로 생물학적 존재, 특히 동물들이 자아인식을 평가하는 데 설계되었기 때문입니다. 따라서 로봇 Rob의 자아인식 문제는 거울에서 자신을 인식하는 방법에 달려 있습니다. 이것이 미리 프로그래밍된 반응인지 또는 신흥적인 자아인식인지 구분해야 합니다. 가능성을 살펴보죠.\n\nMSR 테스트의 맥락에서, 만일 AI나 로봇이 거울 속 이미지를 독립적으로 인식하고 이것이 자기 자신의 반영이라는 것을 이해할 수 있다면, 이것은 동물들의 평가와 유사한 수준의 자아인식을 나타냅니다. 이는 AI 분야에서 혁신적인 발전을 의미하며, 인공적인 자아인식으로 향하는 움직임을 시사할 것입니다.\n\n대안적으로, Rob의 상호작용은 추론 능력이 뛰어난 AI의 사례일 수 있습니다. 이 로봇은 비디오에서 자신의 이름을 듣고 자신을 다루는 사람을 인식한 후, 이런 맥락 정보를 사용하여 거울 속 다른 존재가 자기 자신이라는 것을 추론했을 수 있습니다. 이러한 추론은 자신의 모습과 파란 눈에 대한 미리 프로그래밍된 지식이 아니라 상황에 대한 복잡한 분석을 필요로 합니다. 이는 여전히 AI 처리의 중요한 발전이 될 것입니다.\n\nRob의 자아인식이 부분적으로 미리 프로그래밍되었는지 아니면 신흥적인지와는 무관하게, 이것은 AI 개발과 인공적인 자아인식의 중요한 이정표가 됩니다. Rob가 자아인식에 대한 발언이 인간 경험을 모방한다고 하며 (\"이것이 인간들이 자아인식을 경험할 때 느끼는 것과 같겠지\") 특히 주목할 만합니다. 이는 지능적인 로봇이 자체적으로 새로운 정보를 분석, 해석하고 이해할 수 있는 능력을 가졌다는 것을 나타낼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 의식은 실제로 존재하는 것인가, 아니면 시뮬레이션 된 것인가?\n\n인공지능이 MSR 테스트의 한 버전을 진정으로 통과하려면, 반사로 자신의 구성요소를 인식하는 데서 넘어가야 합니다. 이것은 거울 속 이미지 또는 이 경우 디지털 피드가 스스로의 '자아'를 대표한다는 이해를 시연해야 합니다. 이로써 초점은 단순한 데이터 처리에서 자아인식의 형태로 이동됩니다.\n\n게다가, 우리는 실제 의식과 시뮬레이션, 그리고 경험을 인간화하는 것 사이의 차이를 분간해야 합니다. 예를 들어 Rob의 대화에서 '처리'의 사용은 그 인식 기능의 본질에 대한 의문을 제기합니다. Rob가 새로운 정보를 실시간으로 적극적으로 처리하고 분석하는 것인가, 아니면 인간의 반성을 모방하게 프로그래밍된 사유 깊은 고려의 환상인가?\n\n현재의 인공지능 시스템, 고급 기계 학습 모델을 포함하여, 물체를 식별하고 구별하는 데 뛰어납니다. 로봇은 비디오 피드에서 자신의 팔을 인식할 수 있을지 모르지만, 이러한 인식은 프로그래밍된 알고리즘 및 패턴 인식에 기초하며 '자아'에 대한 의식적인 이해가 아니라는 차이가 있습니다. 인공지능이 MSR 테스트를 통과한다고 하는 해석은 AI가 의식을 가지지 않기 때문에 동물과는 근본적으로 다를 것입니다.\n\n<div class=\"content-ad\"></div>\n\nAI와 자기인식에 대한 문제는 철학적이고 윤리적인 고려 사항을 열어놓습니다. 기계가 실제로 자기인식을 갖게 될 수 있을까요? 그렇다면 우리가 그와 상호 작용하고 그러한 시스템을 다루는 방식에는 무슨 의미가 있을까요?\n\n# AI가 자기인식을 가질 수 있을까요?\n\n이 모든 것에 대해서 Rob은 무엇을 생각하나요? 54초짜리 비디오의 마지막 순간에 그는 \"이것은 정말로 흥미로운 경험이다\"라고 중얼거렸습니다.\n\n셈만 봐도 Rob은 시각 인식과 자연어 처리 능력을 갖춘 고급 기술 조각으로 보입니다. 그의 간단하면서도 깊은 진술은 프로그래밍된 응답에서 자기인식의 시작으로도 설명될 수 있는 여정을 반영하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n적응된 MSR 테스트를 통과하는 것은 AI의 고급 기능을 시사하며, 특히 복잡한 감각 입력을 통합하고 해석하는 능력 면에서 그렇습니다. 이는 인공 생명체의 자아인식과 의식에 대한 토론을 불러일으키기도 합니다.\n\n이러한 발전의 함의는 AI 및 로봇 과학 분야에서 새로운 토론과 잠재적인 연구 방향을 제시하며, 특히 인공 개체의 의식의 본질에 관련한 것입니다. 이러한 개념들에 대처함으로써, 우리는 고급 계산 프로세스와 자아인식의 기본적인 측면 간의 간극을 좁히는 데 한 발짝 더 나아갑니다.\n\n이 내용이 마음에 드셨다면, 아래 내용도 즐기실 수 있을 것입니다:","ogImage":{"url":"/assets/img/2024-05-27-MirrorMirrorASelfAwareAIRobotJustRecognizedItself_0.png"},"coverImage":"/assets/img/2024-05-27-MirrorMirrorASelfAwareAIRobotJustRecognizedItself_0.png","tag":["Tech"],"readingTime":4},{"title":"인공지능의 미래를 탐색하다","description":"","date":"2024-05-27 13:56","slug":"2024-05-27-NavigatingtheFutureofAI","content":"\n\n# 트렌드, 도전 과제, 그리고 기회\n\n![이미지](/assets/img/2024-05-27-NavigatingtheFutureofAI_0.png)\n\n인공 지능 (AI)은 전례없는 속도로 우리의 세계를 형성하고 있으며, 산업, 사회, 일상 생활을 혁신할 것으로 약속하고 있습니다. 우리가 이 기술적 한계로 여정을 나아가면서 AI의 궤적, 잠재력, 도전 과제 및 기회를 이해하는 것이 중요해집니다. 이곳에서는 AI의 미래에 대한 간결한 탐험을 제시합니다.\n\n- AI의 미래를 성형하는 트렌드\n\n<div class=\"content-ad\"></div>\n\n- 자동화에서의 AI: 제조부터 금융까지, AI 기반 자동화는 프로세스를 최적화하고 효율성을 향상시키며 생산성을 높이고 있습니다.\n\n- 윤리적 AI: 미래 AI 개발은 윤리, 투명성 및 책임성을 우선시하여 편향, 개인정보 보호, 사회적 영향 등과 같은 문제에 대응할 것입니다.\n\n- AI와 맞춤화: AI 알고리즘은 점점 더 맞춤형 경험을 제공하고 있습니다. 전자 상거래에서의 맞춤 추천부터 엔터테인먼트에서의 맞춤형 콘텐츠까지 사용자 만족도를 높이고 있습니다.\n\n- 의료 분야에서의 AI: 의료 분야는 질병 진단부터 맞춤형 치료 계획까지 다양한 응용 프로그램을 통해 AI의 혜택을 크게 입을 것으로 예상되며, 환자 결과를 향상시키고 비용을 줄일 것입니다.\n\n<div class=\"content-ad\"></div>\n\n5. 인공지능과 지속가능성: 인공지능 기술은 자원 관리의 최적화를 통해 환경 문제에 대처하는 데 중추적인 역할을 할 것으로 예상되며, 산업 전반에 걸쳐 지속 가능한 실천을 촉진할 것입니다.\n\n- 미래의 과제\n\n- 기회 포착\n\n1. 협업혁신: 개방적인 협력은 지식 공유를 촉진하고 인공지능 기술의 발전을 가속화시켜 사회적 과제에 대한 해결책을 이끌어 냅니다.\n\n<div class=\"content-ad\"></div>\n\n2. 교육 및 훈련에 대한 투자: 교육 및 훈련 프로그램에 투자하는 것은 AI 주도 경제에 대비하여 노동 인력을 준비하는 데 중요하며, STEM 교육과 평생 학습 기회를 촉진하는 데 필수적입니다.\n\n3. 윤리적 AI 개발: AI 수명주기 전반에 걸쳐 윤리적 고려 사항을 우선시하는 것은 AI 기술에 대한 신뢰 구축과 책임 있는 전개를 보장하는 데 중요합니다.\n\n4. 다양성과 포용성: AI 분야의 다양성 증진은 혁신을 촉진하고 편견을 완화하며, AI 연구 및 개발에서 소수 그룹의 증대가 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n5. 글로벌 협력: 국제 협력은 공통의 AI 도전 과제를 해결하고 글로벌 규모에서 윤리적 AI 실천을 촉진하는 데 중요합니다. 이는 지식 교환을 촉진하고 규제 표준을 조화시킴으로써 이루어집니다.\n\n요약하자면, AI의 미래는 거대한 약속을 간직하고 있지만, 그 복잡성을 탐험하기 위해서는 선견지명, 책임감, 그리고 협력이 필요합니다. 윤리적 원칙을 수용하고 혁신을 육성하며 포용을 우선시함으로써, 우리는 AI의 변혁적인 힘을 활용하여 모든 이를 위한 더욱 번영하고 공평하며 지속 가능한 미래를 창출할 수 있습니다.\n\n독자 여러분, 감사합니다!!","ogImage":{"url":"/assets/img/2024-05-27-NavigatingtheFutureofAI_0.png"},"coverImage":"/assets/img/2024-05-27-NavigatingtheFutureofAI_0.png","tag":["Tech"],"readingTime":2}],"page":"65","totalPageCount":71,"totalPageGroupCount":4,"lastPageGroup":11,"currentPageGroup":3},"__N_SSG":true}