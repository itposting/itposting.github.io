{"pageProps":{"posts":[{"title":"고급 RAG 12 전세계 이해도 향상하기","description":"","date":"2024-06-19 03:24","slug":"2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding","content":"\n\n많은 중요한 실제 업무는 과학 문헌 검토, 법적 사례 요약 및 의료 진단과 같이 다양한 문서 덩어리나 문서 간의 지식 이해가 필요합니다.\n\n기존 RAG 방법은 각 덩어리가 독립적으로 인코딩되기 때문에 정보를 이해하는 작업을 필요로 하는 LLMs에게 도움을 줄 수 없습니다.\n\n본 문서에서는 문서 또는 말뭉치의 전역적 이해를 향상시키기 위한 네 가지 혁신적인 방법을 소개합니다. 이를 통해 얻은 통찰과 생각에 대한 내용도 함께 공유할 것입니다.\n\n네 가지 방법은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- RAPTOR: 이는 텍스트 청크를 재귀적으로 삽입, 클러스터링 및 요약하는 트리 기반 검색 시스템입니다.\n- Graph RAG: 이 방법은 지식 그래프 생성, 커뮤니티 탐지, RAG 및 쿼리 중심 요약(QFS)을 결합하여 전체 텍스트 코퍼스의 철저한 이해를 도와줍니다.\n- HippoRAG: 이 검색 프레임워크는 사람의 장기 기억의 해리 인덱싱 이론에서 영감을 받습니다. LLMs, 지식 그래프 및 개인화된 PageRank 알고리즘과 협력합니다.\n- spRAG: 이 방법은 AutoContext와 Relevant Segment Extraction(RSE)이라는 두 가지 핵심 기술을 통해 표준 RAG 시스템의 성능을 향상시킵니다.\n\n# RAPTOR: 트리 구조화 검색을 위한 재귀적 요약 처리\n\nRAPTOR은 텍스트 세그먼트를 재귀적으로 포함, 클러스터링 및 요약하는 혁신적인 트리 기반 검색 시스템입니다. 이는 아래에서 위로 트리를 구성하여 다양한 수준의 요약을 제공합니다.\n\n추론 중에 RAPTOR는 이 트리에서 정보를 검색하여 다양한 수준의 추상화에서 더 긴 문서의 데이터를 통합합니다.\n\n<div class=\"content-ad\"></div>\n\n## 핵심 아이디어\n\nRAPTOR은 임베딩을 기반으로 텍스트 청크를 클러스터로 구성하기 위해 재귀 방법을 사용합니다. 이는 아래에서 위로 트리를 구성하여 각 클러스터에 대한 요약을 생성합니다. 이 과정은 그림 1에 설명되어 있습니다.\n\n![Figure 1](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_0.png)\n\n아래에서는 그림 1과 관련된 구체적인 주제에 대해 자세히 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n- RAPTOR 트리 작성\n- 검색 프로세스\n\n## RAPTOR 트리 작성\n\n텍스트 청킹\n\n검색 말뭉치를 연속적인 100토큰 단위의 청크로 나눕니다. 100토큰을 초과하는 경우, RAPTOR는 전체 문장을 다음 청크로 이동하여 문맥 및 의미 일관성을 유지합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndef split_text(\n    text: str, tokenizer: tiktoken.get_encoding(\"cl100k_base\"), max_tokens: int, overlap: int = 0\n):\n    \"\"\"\n    입력 텍스트를 tokenizer와 최대 허용 토큰을 기반으로 작은 청크로 분할합니다.\n    \n    Args:\n        text (str): 분할할 텍스트입니다.\n        tokenizer (CustomTokenizer): 텍스트를 분할하는 데 사용할 tokenizer입니다.\n        max_tokens (int): 최대 허용 토큰 수입니다.\n        overlap (int, optional): 청크 간의 겹치는 토큰 수입니다. 기본값은 0입니다.\n    \n    Returns:\n        List[str]: 텍스트 청크의 목록입니다.\n    \"\"\"\n    ...\n    ...        \n        # 현재 청크에 문장을 추가하면 최대 토큰을 초과하는 경우, 새로운 청크를 시작합니다.\n        elif current_length + token_count > max_tokens:\n            chunks.append(\" \".join(current_chunk))\n            current_chunk = current_chunk[-overlap:] if overlap > 0 else []\n            current_length = sum(n_tokens[max(0, len(current_chunk) - overlap):len(current_chunk)])\n            current_chunk.append(sentence)\n            current_length += token_count\n    ...\n    ...\n```\n\n텍스트 임베딩\n\n이러한 청크를 밀집 벡터 표현으로 생성하기 위해 Sentence-BERT를 사용합니다.\n\n이러한 청크와 해당 임베딩은 RAPTOR 트리 구조의 리프 노드를 형성합니다.\n\n<div class=\"content-ad\"></div>\n\n```python\nclass TreeBuilder:\n    \"\"\"\n    TreeBuilder 클래스는 요약 모델과 임베딩 모델을 사용하여 \"트리\"라고 불리는 계층적 텍스트 추상 구조를 작성하는 책임이 있습니다.\n    \"\"\"\n    ...\n    ...\n    def build_from_text(self, text: str, use_multithreading: bool = True) -> Tree:\n        \"\"\"입력 텍스트에서 골든 트리를 작성하며, 선택적으로 멀티스레딩을 사용합니다.\n\n        Args:\n            text (str): 입력 텍스트입니다.\n            use_multithreading (bool, optional): 리프 노드를 만들 때 멀티스레딩을 사용할지 여부입니다.\n                기본값: True.\n\n        Returns:\n            Tree: 골든 트리 구조입니다.\n        \"\"\"\n        chunks = split_text(text, self.tokenizer, self.max_tokens)\n\n        logging.info(\"리프 노드 생성 중\")\n\n        if use_multithreading:\n            leaf_nodes = self.multithreaded_create_leaf_nodes(chunks)\n        else:\n            leaf_nodes = {}\n            for index, text in enumerate(chunks):\n                __, node = self.create_node(index, text)\n                leaf_nodes[index] = node\n\n        layer_to_nodes = {0: list(leaf_nodes.values())}\n\n        logging.info(f\"생성된 {len(leaf_nodes)} 개의 리프 임베딩\")\n        ...\n        ...\n```\n\n클러스터링 방법\n\n클러스터링은 RAPTOR 트리를 구성하는 데 중요하며, 텍스트 단락을 일관된 그룹으로 구성합니다. 관련 콘텐츠를 함께 모아 나중의 검색 프로세스를 향상시킵니다.\n\nRAPTOR의 클러스터링 방법은 다음과 같은 특징을 가지고 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n- 군집에는 가우시안 혼합 모델(GMMs)과 UMAP 차원 축소가 사용됩니다.\n- UMAP 매개변수를 수정하여 전역 및 지역 클러스터를 식별할 수 있습니다.\n- 모델 선택을 위해 Bayesian Information Criterion (BIC)이 사용되어 최적 클러스터 수를 결정합니다.\n\n이 군집화 방법의 핵심은 노드가 여러 군집에 속할 수 있다는 것입니다. 따라서 하나의 텍스트 세그먼트에 여러 주제에 대한 정보가 들어 있기 때문에 고정된 범주 수가 필요하지 않으며, 이는 여러 개요에 텍스트 세그먼트를 포함시켜줍니다.\n\n노드를 GMM을 사용하여 군집화한 후, 각 군집 내의 노드는 LLM에 의해 요약됩니다. 이 과정은 대량의 데이터를 뽑아 선택된 노드의 간결하고 일관된 개요로 변환합니다.\n\n구현에서는 gpt-3.5 turbo가 요약 생성에 사용됩니다. 해당 프롬프트는 그림 2에 나와 있습니다.\n\n<div class=\"content-ad\"></div>\n\nMarkdown 형식으로 변경\n\n![Construction Algorithm](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_1.png)\n\n생성 알고리즘\n\n지금까지 전체 트리의 리프 노드를 얻고 클러스터링 알고리즘을 결정했습니다.\n\n도형 1의 중간에 나타난 것처럼 함께 그룹화된 노드는 형제가 되며, 부모 노드는 해당 특정 클러스터의 요약을 포함합니다. 생성된 요약은 트리의 비리프 노드를 구성합니다.\n\n<div class=\"content-ad\"></div>\n\n## 검색 프로세스\n\n노드들을 요약한 후, 삽입, 클러스터링, 그리고 요약 과정이 더 이상 실행할 수 없을 때까지 계속됩니다. 이렇게 하면 원본 문서의 구조화된 다층 트리 표현이 생성됩니다.\n\n해당 코드는 아래와 같이 나타납니다.\n\n```python\nclass ClusterTreeConfig(TreeBuilderConfig):\n    ...\n    ...\n    def construct_tree(\n        self,\n        current_level_nodes: Dict[int, Node],\n        all_tree_nodes: Dict[int, Node],\n        layer_to_nodes: Dict[int, List[Node]],\n        use_multithreading: bool = False,\n    ) -> Dict[int, Node]:\n        ...\n        ...\n\n        for layer in range(self.num_layers):\n\n            new_level_nodes = {}\n\n            logging.info(f\"Constructing Layer {layer}\")\n\n            node_list_current_layer = get_node_list(current_level_nodes)\n\n            if len(node_list_current_layer) <= self.reduction_dimension + 1:\n                self.num_layers = layer\n                logging.info(\n                    f\"Stopping Layer construction: Cannot Create More Layers. Total Layers in tree: {layer}\"\n                )\n                break\n\n            clusters = self.clustering_algorithm.perform_clustering(\n                node_list_current_layer,\n                self.cluster_embedding_model,\n                reduction_dimension=self.reduction_dimension,\n                **self.clustering_params,\n            )\n\n            lock = Lock()\n\n            summarization_length = self.summarization_length\n            logging.info(f\"Summarization Length: {summarization_length}\")\n\n            ...\n            ...\n```\n\n<div class=\"content-ad\"></div>\n\nRAPTOR 트리를 갖게 된 후 쿼리하는 방법은 무엇인가요?\n\n쿼리하는 방법에는 두 가지가 있습니다: 트리 순회 방법과 축소된 트리를 기반으로 하는 방법으로, 이는 그림 3에 나와 있습니다.\n\n![image](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_2.png)\n\n트리 순회는 트리의 루트 레벨에서 시작하여 쿼리 벡터와의 코사인 유사성에 따라 상위 k개 노드(이 경우 상위 1개)를 검색합니다. 각 레벨에서 이전 레이어의 상위 k개 노드의 자식노드로부터 상위 k개 노드를 검색하며, 해당 코드는 아래에 표시되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\nclass TreeRetriever(BaseRetriever):\n    ...\n    ...\n    def retrieve_information(\n        self, current_nodes: List[Node], query: str, num_layers: int\n    ) -> str:\n        \"\"\"\n        쿼리에 기반하여 트리에서 가장 관련성 높은 정보를 검색합니다.\n\n        Args:\n            current_nodes (List[Node]): 현재 노드의 목록.\n            query (str): 쿼리 텍스트.\n            num_layers (int): 횡단할 레이어의 수.\n\n        Returns:\n            str: 가장 관련성 높은 노드를 사용하여 생성된 콘텍스트.\n        \"\"\"\n\n        query_embedding = self.create_embedding(query)\n\n        selected_nodes = []\n\n        node_list = current_nodes\n\n        for layer in range(num_layers):\n\n            embeddings = get_embeddings(node_list, self.context_embedding_model)\n\n            distances = distances_from_embeddings(query_embedding, embeddings)\n\n            indices = indices_of_nearest_neighbors_from_distances(distances)\n\n            if self.selection_mode == \"threshold\":\n                best_indices = [\n                    index for index in indices if distances[index] > self.threshold\n                ]\n\n            elif self.selection_mode == \"top_k\":\n                best_indices = indices[: self.top_k]\n\n            nodes_to_add = [node_list[idx] for idx in best_indices]\n\n            selected_nodes.extend(nodes_to_add)\n\n            if layer != num_layers - 1:\n\n                child_nodes = []\n\n                for index in best_indices:\n                    child_nodes.extend(node_list[index].children)\n\n                # 중복 값을 제외합니다.\n                child_nodes = list(dict.fromkeys(child_nodes))\n                node_list = [self.tree.all_nodes[i] for i in child_nodes]\n\n        context = get_text(selected_nodes)\n        return selected_nodes, context\n\n\n반면에, 축소된 트리는 트리를 단일 레이어로 축소하고 일정 토큰 수에 도달할 때까지 노드를 검색합니다. 다시 말해, 쿼리 벡터와의 코사인 유사도를 기반으로 상응하는 코드는 다음과 같습니다.\n\n\nclass TreeRetriever(BaseRetriever):\n    ...\n    ...\n    def retrieve_information_collapse_tree(self, query: str, top_k: int, max_tokens: int) -> str:\n        \"\"\"\n        쿼리에 기반하여 트리에서 가장 관련성 높은 정보를 검색합니다.\n\n        Args:\n            query (str): 쿼리 텍스트.\n            max_tokens (int): 최대 토큰 수.\n\n        Returns:\n            str: 가장 관련성 높은 노드를 사용하여 생성된 콘텍스트.\n        \"\"\"\n\n        query_embedding = self.create_embedding(query)\n\n        selected_nodes = []\n\n        node_list = get_node_list(self.tree.all_nodes)\n\n        embeddings = get_embeddings(node_list, self.context_embedding_model)\n\n        distances = distances_from_embeddings(query_embedding, embeddings)\n\n        indices = indices_of_nearest_neighbors_from_distances(distances)\n\n        total_tokens = 0\n        for idx in indices[:top_k]:\n\n            node = node_list[idx]\n            node_tokens = len(self.tokenizer.encode(node.text))\n\n            if total_tokens + node_tokens > max_tokens:\n                break\n\n            selected_nodes.append(node)\n            total_tokens += node_tokens\n\n        context = get_text(selected_nodes)\n        return selected_nodes, context\n\n\n따라서 어떤 방법이 더 나은지요?\n\n\n<div class=\"content-ad\"></div>\n\nRAPTOR이 그림 4에서 보여준대로 비교를 진행했습니다.\n\n![Figure 4](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_3.png)\n\n그림 4에서 보여진 대로, 2000 토큰을 가진 축소된 트리가 최상의 결과를 제공합니다. 이는 트리 탐색보다 더 많은 유연성을 제공하기 때문입니다. 구체적으로, 모든 노드를 동시에 탐색함으로써, 주어진 문제에 대한 적절한 상세 수준에서 정보를 검색합니다.\n\n그림 5는 RAPTOR이 \"이야기의 중심 주제는 무엇인가요?\" 및 \"신데렐라가 행복한 결말을 어떻게 이끌어 냈나요?\"라는 두 질문에 관련된 정보를 검색하는 방법을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_4.png)\n\nHighlighted nodes indicate RAPTOR’s selections, while arrows point to DPR’s (Dense Passage Retrieval) leaf nodes. Importantly, the context provided by RAPTOR often includes the information retrieved by DPR, either directly or within higher-layer summaries.\n\n# Graph RAG\n\nGraph RAG employs LLM to construct a graph-based text index in two stages:\n\n\n<div class=\"content-ad\"></div>\n\n- 먼저, 소스 문서에서 지식 그래프를 도출합니다.\n- 이후에는 밀접하게 연결된 엔터티 그룹에 대한 커뮤니티 요약을 생성합니다.\n\n쿼리가 주어지면 각 커뮤니티 요약은 부분 응답에 기여합니다. 이러한 부분 응답은 최종 글로벌 답변을 형성하기 위해 집계됩니다.\n\n## 개요\n\n도식 6은 Graph RAG의 파이프라인을 보여줍니다. 보라색 상자는 인덱싱 작업을 나타내고, 초록색 상자는 쿼리 작업을 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_5.png)\n\nRAG 그래프는 데이터셋 도메인에 특화된 LLM(대규모 언어 모델) 프롬프트를 사용하여 노드(엔터티와 같은), 엣지(관계와 같은), 및 공변량(클레임과 같은)을 감지, 추출, 요약합니다.\n\n커뮤니티 탐지는 그래프를 노드, 엣지, 공변량의 그룹으로 나누어주며 LLM이 색인 및 질의 시 요약할 수 있도록 합니다.\n\n특정 쿼리에 대한 전역 응답은 해당 쿼리와 관련된 모든 커뮤니티 요약에 대해 최종 쿼리 중심 요약을 수행하여 생성됩니다.\n\n<div class=\"content-ad\"></div>\n\nFigure 6의 각 단계 구현에 대해 아래에서 설명하겠습니다. 2024년 6월 12일을 기준으로 Graph RAG는 현재 오픈 소스가 아니므로 소스 코드와 관련하여 논의할 수 없습니다.\n\n## 단계 1: 소스 문서 → 텍스트 청크\n\n청크 크기의 트레이드오프는 RAG의 오랜 문제입니다.\n\n청크가 너무 길면 LLM 호출 수가 감소합니다. 그러나 컨텍스트 창의 제약으로 인해 대량의 정보를 완전히 이해하고 관리하기가 어려워집니다. 이 상황은 리콜률의 저하로 이어질 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Screenshot](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_6.png)\n\nFigure 7에 설명된 대로 HotPotQA 데이터셋의 경우, 600 토큰의 청크 크기는 2400 토큰의 청크 크기에 비해 효과적인 엔티티를 두 배 더 추출합니다.\n\n## 단계 2: 텍스트 청크 → 요소 인스턴스(엔티티 및 관계)\n\n해당 방법은 각 청크에서 엔티티와 관계를 추출하여 지식 그래프를 구성하는 것을 포함합니다. 이는 LLM과 프롬프트 엔지니어링의 조합을 통해 달성됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n동시에 Graph RAG는 다단계 반복 프로세스를 사용합니다. 이 프로세스에서 LLM은 모든 엔티티가 추출되었는지 결정해야 합니다. 이는 이진 분류 문제와 유사합니다.\n\n## 단계 3: 요소 인스턴스 → 요소 요약 → 그래프 커뮤니티 → 커뮤니티 요약\n\n이전 단계에서 엔티티, 관계, 주장을 추출하는 것은 사실 요약의 한 형태입니다.\n\n하지만 Graph RAG는 이것만으로 충분하지 않고 LLM을 사용하여 이러한 \"요소\"를 더 자세히 요약해야 한다고 생각합니다.\n\n<div class=\"content-ad\"></div>\n\n잠재적인 우려 사항은 LLMs가 항상 같은 엔티티에 대한 참조를 동일한 텍스트 형식으로 추출하지 않을 수 있다는 점입니다. 이로 인해 중복된 엔티티 요소가 발생하여 그래프에서 중복된 노드가 생성될 수 있습니다.\n\n그 이슈는 빠르게 사라질 것입니다.\n\nGraph RAG는 커뮤니티 탐지 알고리즘을 활용하여 그래프 내에서 커뮤니티 구조를 식별하여 연결된 엔티티를 동일한 커뮤니티에 통합합니다. Figure 8은 Leiden 알고리즘을 사용하여 MultiHop-RAG 데이터셋에서 식별된 그래프 커뮤니티를 보여줍니다.\n\n이 시나리오에서 LLM이 추출 중에 엔티티의 모든 변형을 일관되게 식별하지 못하더라도 커뮤니티 탐지는 이러한 변형 사이의 연결을 수립하는 데 도움을 줄 수 있습니다. 한 번 커뮤니티로 그룹화되면, 이러한 변형이 동일한 엔티티 의미를 가리킨다는 것을 나타냅니다. 다만 표현이나 동의어가 다를 뿐입니다. 이는 지식 그래프 분야에서의 엔티티 모호성 해소와 유사합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_7.png\" />\n\n커뮤니티를 식별한 후, Leiden 계층 내 각 커뮤니티에 대한 보고서 형태의 요약을 생성할 수 있습니다. 이러한 요약은 데이터셋의 전역 구조와 의미를 이해하는 데 독립적으로 유용합니다. 이 요약은 또한 어떠한 문제 없이 말뭉치를 이해하는 데 사용할 수 있습니다.\n\n그림 9은 커뮤니티 요약의 생성 방법을 보여줍니다.\n\n## 단계 4: 커뮤니티 요약 → 커뮤니티 답변 → 전역 답변\n\n<div class=\"content-ad\"></div>\n\n이제 마지막 단계에 도달했습니다: 이전 단계에서의 커뮤니티 요약을 기반으로 최종 답변을 생성하는 것입니다.\n\n커뮤니티 구조의 계층적 특성으로 인해 서로 다른 수준의 요약은 다양한 질문에 대답할 수 있습니다.\n\n그러나 여기서 또 하나의 질문이 생깁니다: 다양한 수준의 커뮤니티 요약이 있는 경우, 어떤 수준이 세부 사항과 범위 사이의 균형을 맞출 수 있을까요?\n\nGraph RAG은 Graph RAG 논문의 섹션 3을 더 자세히 살펴보면서 가장 적절한 추상화 수준을 선택합니다.\n\n<div class=\"content-ad\"></div>\n\n주어진 커뮤니티 수준에서는 그림 10에 표시된 대로 어떤 사용자 쿼리에 대한 글로벌 답변이 생성됩니다.\n\n# HippoRAG\n\nHippoRAG는 인간의 장기 기억의 해마 색인 이론에서 영감을 받아 새로운 검색 프레임워크입니다. LLMs, 지식 그래프 및 개인화된 페이지랭크 알고리즘과 협력하여 작동합니다. 이 협력은 인간의 기억에서 피질과 해마프스의 다양한 역할을 모방합니다.\n\n## 주요 아이디어\n\n<div class=\"content-ad\"></div>\n\n**표 11**은 인간 두뇌가 지식 통합의 어려운 과제를 비교적 쉽게 해결하는 방법을 보여줍니다.\n\n인간 장기 기억에 관한 잘 알려진 이원칙인 해마기억색인이론은 이 놀라운 능력에 대한 가능한 설명을 제시합니다.\n\n구체적으로 환경 기반의 지속적으로 업데이트되는 기억은 신장피질과 C자 모양의 해마 간 상호작용에 의존합니다. 신장피질은 실제 기억 표현을 처리하고 저장하는 반면, 해마는 해마색인을 유지합니다. 이 색인은 신장 피질에서 기억 단위를 가리키고 그들의 연결을 저장하는 일련의 상호 연결된 색인입니다.\n\n![이미지](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_8.png)\n\n<div class=\"content-ad\"></div>\n\nFigure 11에서는 수천 명의 Stanford 교수와 알츠하이머병 연구자를 설명하는 여러 통로에서 알츠하이머병 연구와 관련된 Stanford 교수를 식별하는 것을 목표로 합니다.\n\n- 과거 방식의 RAG는 통로를 독립적으로 부호화하여 Thomas 교수를 식별하는 데 어려움을 겪었는데, 통로가 두 기능을 동시에 언급할 때에만 이를 가능케 했습니다.\n- 그에 반해, 이 교수에 익숙한 사람들은 뇌의 연관 메모리 능력 덕분에 기억하기 쉬울 것입니다. 이 능력은 도형 11에서 파란색으로 나타난 C 모양 해마색 부분에 의해 주도되는 것으로 여겨집니다.\n- 이 메커니즘의 영향을 받아 HippoRAG는 LLMs가 지식 통합 작업을 관리하기 위해 유사한 연상 지도를 구축하고 활용할 수 있게 합니다.\n\n## 개요\n\n도형 11에서 영감을 받아, HippoRAG의 각 구성 요소는 도형 12에 제시된 것처럼 인간 장기 기억의 세 구성 요소 중 하나에 해당합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_9.png](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_9.png)\n\nHippoRAG은 인간의 장기기억의 세 가지 구성 요소를 모방하여 패턴 구분 및 완료 기능을 에뮬레이트합니다.\n\n- 오프라인 색인을 위해 LLM은 텍스트 단락을 열린 KG 트리플로 처리합니다. 그런 다음 인공 해마색인에 추가됩니다. 동시에 합성 해마집 영역(PHR)은 동의어를 감지합니다. 위의 예에서 HippoRAG는 Thomas 교수를 포함하는 트리플을 추출하고 KG에 통합합니다.\n- 온라인 검색을 위해 LLM 뉴로피질은 질의에서 명명된 엔터티를 추출합니다. 해마색인에 연결되도록 해마집 검색 인코더가 이들을 링크합니다. HippoRAG은 문맥 기반 검색을 위해 개인화된 페이지랭크 알고리즘을 활용하며 Thomas 교수와 관련된 정보를 추출합니다.\n\n## 전체 프로세스 데모\n\n\n<div class=\"content-ad\"></div>\n\nHippoRAG 파이프라인을 소개하는 실용적인 예시가 있습니다.\n\nFigure 13은 질문, 그에 대한 답변, 그리고 지원 및 분랄 글에서의 내용을 보여줍니다.\n\n![Figure 13](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_10.png)\n\nFigure 14는 OpenIE 절차와 지식 그래프의 관련 부분을 포함한 색인화 단계를 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n마지막으로, 표 15은 검색 단계를 보여줍니다. 쿼리 Named Entity Recognition (NER), 쿼리 노드 검색, 개인화된 페이지 랭크 (PPR) 알고리즘이 노드 확률에 미치는 영향, 그리고 최상위 검색 결과의 계산이 표시됩니다.\n\n아래에는 소스 코드와 함께 HippoRAG가 장기 기억을 구축하고 검색하는 두 가지 측면에 대해 구체적으로 논의합니다.\n\n<div class=\"content-ad\"></div>\n\n## 장기 기억을 구축하는 방법\n\n장기 기억을 구축하는 과정은 주로 다음 세 단계로 구성됩니다.\n\n먼저, Figure 16에서 보여지는 대로 OpenIE를 사용하여 검색 코퍼스의 각 텍스트에서 명명된 개체 세트를 추출하기 위해 LLM을 활용하십시오.\n\n![이미지](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_13.png)\n\n<div class=\"content-ad\"></div>\n\n다음으로, 최종 삼중체를 추출하기 위해 OpenIE 프롬프트에 명명된 엔티티를 추가하십시오. Figure 17에 나와 있는 대로요.\n\n![Figure 17](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_14.png)\n\n마지막으로, 파인튠된 외장형 밀도 인코더를 활용하여 지식 그래프를 생성하고 검색에 사용할 것입니다.\n\n## 검색 방법\n\n<div class=\"content-ad\"></div>\n\n먼저, 사용자 쿼리에서 명명된 엔티티 집합을 추출하기 위해 LLM을 사용하십시오. Figure 18에서 보여지는 것처럼요.\n\n![Figure 18](/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_15.png)\n\n그런 다음, 이러한 명명된 엔티티를 지식 그래프의 노드에 유사성에 따라 링크합니다. 우리는 이러한 선택된 노드를 쿼리 노드라고 부릅니다.\n\n해마에서 해마 색인 요소들 간의 신경 경로는 관련 이웃들이 활성화되어 상류로 회상될 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\n이 효율적인 그래프 탐색 프로세스를 모방하기 위해, HippoRAG는 Personalized PageRank (PPR) 알고리즘을 활용합니다. 이 알고리즘은 그래프 상에서 확률을 사용자가 정의한 일련의 소스 노드를 통해만 분배하는 PageRank의 버전입니다. 아래에 해당 코드가 표시되어 있습니다.\n\n```js\n    def rank_docs(self, query: str, top_k=10):\n        \"\"\"\n        쿼리를 기반으로 문서를 순위 지정\n        @param query: 입력 구문\n        @param top_k: 반환할 문서 수\n        @return: 순위가 지정된 문서 ID 및 점수\n        \"\"\"\n        ...\n        ...\n        # Personalized PageRank (PPR) 또는 다른 그래프 알고리즘을 실행하여 문서 점수 계산\n        if len(query_ner_list) > 0:\n            combined_vector = np.max([top_phrase_vectors], axis=0)\n\n            if self.graph_alg == 'ppr':\n                ppr_phrase_probs = self.run_pagerank_igraph_chunk([top_phrase_vectors])[0]\n            elif self.graph_alg == 'none':\n                ppr_phrase_probs = combined_vector\n            elif self.graph_alg == 'neighbor_2':\n                ppr_phrase_probs = self.get_neighbors(combined_vector, 2)\n            elif self.graph_alg == 'neighbor_3':\n                ppr_phrase_probs = self.get_neighbors(combined_vector, 3)\n            elif self.graph_alg == 'paths':\n                ppr_phrase_probs = self.get_neighbors(combined_vector, 3)\n            else:\n                assert False, f'그래프 알고리즘 {self.graph_alg}은(는) 구현되지 않았습니다.'\n\n            fact_prob = self.facts_to_phrases_mat.dot(ppr_phrase_probs)\n            ppr_doc_prob = self.docs_to_facts_mat.dot(fact_prob)\n            ppr_doc_prob = min_max_normalize(ppr_doc_prob)\n        else:\n            ppr_doc_prob = np.ones(len(self.extracted_triples)) / len(self.extracted_triples)\n        ...\n        ...\n``` \n\n마지막으로, 해마신호가 상류로 전달될 때와 같이 HippoRAG는 이전에 인덱싱된 통로 전체에 대한 출력 PPR 노드 확률을 집계하고 이를 검색을 위해 등수를 매기기 위해 사용합니다.\n\n# spRAG\n\n<div class=\"content-ad\"></div>\n\nspRAG은 복잡한 쿼리를 관리하기 위한 방법입니다. 표준 RAG의 성능을 향상시키는 두 가지 주요 기술을 통해 작동합니다:\n\n- AutoContext\n- 관련 세그먼트 추출 (RSE)\n\n우리는 spRAG가 청크 전반에 걸친 복잡한 쿼리를 처리하는 방법에 초점을 맞추고 있습니다. 현재 spRAG에 대한 논문은 없으며 분석과 코드가 결합된 상태만 있습니다.\n\n## AutoContext: 문서 수준 컨텍스트의 자동 주입\n\n<div class=\"content-ad\"></div>\n\n전통적인 RAG에서는 일반적으로 문서를 포함하는 데 고정 길이의 청크로 나눕니다. 이 간단한 방법은 종종 문서 수준의 컨텍스트 정보를 간과하여 보다 정확하고 포괄적인 컨텍스트 포함을 방해할 수 있습니다.\n\n이 문제를 해결하기 위해 AutoContext가 개발되었습니다. 그 핵심 아이디어는 각 청크에 포함되기 전에 문서 수준의 컨텍스트 정보를 자동으로 통합하는 것입니다.\n\n구체적으로, 1~2 문장으로 문서 요약을 작성하고 파일 이름과 함께 각 청크의 시작 부분에 추가합니다. 결과적으로, 각 청크는 고립되어 있지 않지만 전체 문서의 컨텍스트 정보를 가지고 있습니다. 문서 요약을 얻는 코드는 아래와 같이 표시됩니다.\n\n```js\ndef get_document_context(auto_context_model: LLM, text: str, document_title: str, auto_context_guidance: str = \"\"):\n    # content이 너무 긴 경우 자르기\n    max_content_tokens = 6000 # 이 숫자를 변경하면 위의 자르기 메시지도 업데이트해야 합니다\n    text, num_tokens = truncate_content(text, max_content_tokens)\n    if num_tokens < max_content_tokens:\n        truncation_message = \"\"\n    else:\n        truncation_message = TRUNCATION_MESSAGE\n\n    # 문서 컨텍스트 가져오기\n    prompt = PROMPT.format(auto_context_guidance=auto_context_guidance, document=text, document_title=document_title, truncation_message=truncation_message)\n    chat_messages = [{\"role\": \"user\", \"content\": prompt}]\n    document_context = auto_context_model.make_llm_call(chat_messages)\n    return document_context\n```\n\n<div class=\"content-ad\"></div>\n\n## 관련 세그먼트 추출: 연관 텍스트 청크의 지능적 조합\n\nRSE는 후속 처리 단계입니다. 그 목적은 가장 관련성 있는 정보를 제공할 수 있는 청크를 지능적으로 식별하고 결합하여 더 긴 세그먼트를 형성하는 것입니다.\n\n구체적으로, RSE는 먼저 콘텐츠 유사 또는 의미론적으로 관련된 검색된 청크를 그룹화합니다. 그런 다음 쿼리 요구 사항에 따라 이러한 청크를 선택하고 조합하여 최상의 세그먼트를 형성합니다. 관련 코드는 아래에 나와 있습니다.\n\n```js\ndef get_best_segments(all_relevance_values: list[list], document_splits: list[int], max_length: int, overall_max_length: int, minimum_value: float) -> list[tuple]:\n    \"\"\"\n    이 함수는 청크 관련성 값들을 가져와서 최상의 세그먼트를 찾기 위해 최적화 알고리즘을 실행합니다.\n\n    - all_relevance_values: 각 메타-문서의 각 청크에 대한 관련성 값 목록의 목록으로서, 각 외부 목록은 쿼리를 나타냅니다\n    - document_splits: 각 문서의 시작을 나타내는 인덱스 목록 - 최상의 세그먼트는 이러한 인덱스와 중복되지 않을 것입니다\n    \n    반환\n    - best_segments: 메타-문서에서 최상의 세그먼트의 인덱스를 나타내는 튜플 목록 (끝 인덱스는 불포함)\n    \"\"\"\n    best_segments = []\n    total_length = 0\n    rv_index = 0\n    bad_rv_indices = []\n    while total_length < overall_max_length:\n        # 쿼리를 순환합니다\n        if rv_index >= len(all_relevance_values):\n            rv_index = 0\n        # 쿼리 중 더 이상 유효한 세그먼트가 없는 경우 작업을 완료합니다\n        if len(bad_rv_indices) >= len(all_relevance_values):\n            break        \n        # 이미 이 쿼리에 대해 더 이상 유효한 세그먼트가 없음을 결정했는지 확인하고 해당 경우 건너뜁니다\n        if rv_index in bad_rv_indices:\n            rv_index += 1\n            continue\n        \n        # 이 쿼리에 대해 최상의 남은 세그먼트를 찾습니다\n        relevance_values = all_relevance_values[rv_index]  # 해당 쿼리의 관련성 값 가져오기\n        best_segment = None\n        best_value = -1000\n        for start in range(len(relevance_values)):\n            # 음수 값 시작 지점 건너뜁니다\n            if relevance_values[start] < 0:\n                continue\n            for end in range(start+1, min(start+max_length+1, len(relevance_values)+1)):\n                # 음수 값 끝 지점 건너뜁니다\n                if relevance_values[end-1] < 0:\n                    continue\n                # 이 세그먼트가 최상의 세그먼트 중 어느 것과도 겹치는지 확인\n                if any(start < seg_end and end > seg_start for seg_start, seg_end in best_segments):\n                    continue\n                # 이 세그먼트가 문서 분할 중 어느 것과도 겹치는지 확인\n                if any(start < split and end > split for split in document_splits):\n                    continue\n                # 이 세그먼트가 전체 최대 길이를 초과할 것 같은지 확인\n                if total_length + end - start > overall_max_length:\n                    continue\n                segment_value = sum(relevance_values[start:end])  # 세그먼트 값을 그 청크의 관련성 값 합으로 정의\n                if segment_value > best_value:\n                    best_value = segment_value\n                    best_segment = (start, end)\n        \n        # 유효한 세그먼트를 찾지 못한 경우 해당 쿼리를 마쳤다는 표시를 하고 진행\n        if best_segment is None or best_value < minimum_value:\n            bad_rv_indices.append(rv_index)\n            rv_index += 1\n            continue\n\n        # 그렇지 않은 경우, 최상의 세그먼트 목록에 세그먼트를 추가합니다\n        best_segments.append(best_segment)\n        total_length += best_segment[1] - best_segment[0]\n        rv_index += 1\n    \n    return best_segments\n```\n\n<div class=\"content-ad\"></div>\n\n# 통찰과 생각\n\n## 알고리즘과 자료 구조 비교\n\nRAPTOR는 클러스터링을 통해 트리와 유사한 데이터 구조를 생성하고 이 구조를 기반으로 검색을 수행합니다.\n\nGraph RAG와 HippoRAG 모두 지식 그래프를 활용하지만 약간의 차이가 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 데이터 구조 관련하여, Graph RAG는 지식 요소를 요약하여 정보를 통합합니다. 그러므로 새로운 데이터가 추가될 때마다 요약 프로세스를 반복해야 합니다. 이는 RAPTOR에도 적용됩니다. 그러나 HippoRAG는 단순히 지식 그래프에 엣지를 추가함으로써 새로운 지식을 손쉽게 통합할 수 있습니다.\n- 검색 알고리즘 관점에서, Graph RAG는 커뮤니티 감지에 의존하며, HippoRAG는 개인화 페이지랭크 (PPR) 알고리즘을 활용합니다.\n\n그 외의 다른 요소들과 달리, spRAG는 고급 데이터 구조를 사용하지 않습니다. 각 청크에 문서 요약과 파일 이름을 추가한 후, 관련성 값에 기반한 검색을 실행합니다. 이는 spRAG의 색인 및 쿼리 속도가 가장 빠를 것을 시사합니다.\n\n## 성능에 대해\n\nHippoRAG는 실험을 수행하여 기준으로 삼은 RAPTOR를 능가하는 결과를 보여주었습니다. Figure 19에 나와 있는 것처럼요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_16.png\" />\n\n그래프 RAG 논문에는 성능 비교 실험이 포함되어 있지 않습니다.\n\n또한 현재 spRAG에 대한 논문이 없습니다.\n\n## 향상된 범위에 대해\n\n<div class=\"content-ad\"></div>\n\n네 가지 방법 — RAPTOR, Graph RAG, HippoRAG, 그리고 spRAG — 는 전체 말뭉치의 이해를 향상시키기 위해 노력합니다.\n\n각각은 전체 말뭉치를 기반으로 데이터 구조를 구축합니다.\n\n## 사용자 정의 가능성에 대해\n\n이 문맥에서 HippoRAG는 모든 구성 요소가 오프더셸프이기 때문에 추가 교육이 필요하지 않아 Figure 20에 나와 있는 것처럼 더 우수합니다.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_17.png\" />\n\n따라서, 특정 구성 요소를 섬세하게 조정함으로써, 개선의 상당한 잠재력이 있습니다.\n\n# 결론\n\n본문은 코드 설명을 보충하여 문서나 말뭉치의 전통적인 RAG의 전역 이해력을 향상시키기 위한 네 가지 새로운 방법을 소개합니다. 또한 제 개인적인 통찰과 생각도 포함되어 있습니다.\n  \n\n<div class=\"content-ad\"></div>\n\nRAG에 관심이 있으시다면 다른 내 기사들도 살펴보세요.\n\n또한, 최신 기사들은 제 뉴스레터에서 확인할 수 있습니다.\n\n마지막으로, 오류나 생략된 부분이 있거나 공유할 생각이 있다면 댓글 섹션에서 자유롭게 토론해 주세요.","ogImage":{"url":"/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_0.png"},"coverImage":"/assets/img/2024-06-19-AdvancedRAG12EnhancingGlobalUnderstanding_0.png","tag":["Tech"],"readingTime":24},{"title":"챗GPT 디자인 해킹 - 사용자의 99가 이것을 모른다","description":"","date":"2024-06-19 03:21","slug":"2024-06-19-ChatGPTDesignHack99ofUsersDontKnowThis","content":"\n\n## 챗GPT를 5성급 그래픽 디자이너로 변신시키기\n\n챗GPT(GPT-4o)가 진지한 그래픽 디자이너 업그레이드를 받았어요. 이게 아직 작동하는 건지 확실하지 않아요...이렇게 사용된 걸 본 적이 없어요.\n\n![이미지](/assets/img/2024-06-19-ChatGPTDesignHack99ofUsersDontKnowThis_0.png)\n\n챗GPT를 그래픽 디자이너로 활용하는 방법에 대해 YouTube 동영상을 만들었어요. 프롬프트만으로 YouTube 섬네일, 소셜 미디어 그래픽 등을 만들어보세요.\n\n<div class=\"content-ad\"></div>\n\n이 튜토리얼에서는 ChatGPT를 Photoshop이나 Canva와 유사한 디자인 도구로 변환하는 방법을 보여 드리겠습니다. ChatGPT만을 이용하여 아래에서 보는 배경 이미지와 같은 멋진 디자인을 만드는 방법을 배울 수 있습니다.\n\n![ChatGPT Design](/assets/img/2024-06-19-ChatGPTDesignHack99ofUsersDontKnowThis_1.png)\n\n이제 레이어와 오버레이를 추가할 수 있습니다.\n\n이 디자인 핵은 무한한 가능성을 여는 방법입니다. 오랜 시간 동안 시도해 왔습니다.\n\n<div class=\"content-ad\"></div>\n\n이번 업데이트는 엄청 커요.\n\n이전에는 DALL·E-3에서 그래픽을 만들 수 있었지만, 그것들은 레이어가 없는 이미지였어요. ChatGPT에게 재밌고 흥미로운 디자인과 그래픽을 만들라고 할 수 있었지만, 그것들은 평면적이었어요. 아무것도 변경할 수 없었죠.\n\n어떤 것이든 설계한 적이 있다면, 포토샵이나 캔바와 같은 도구에서 디자인이 이루어지는 곳이 레이어인 것을 아실 거예요.\n\n이것들이 없으면 굉장히 제한되어요.\n\n<div class=\"content-ad\"></div>\n\n위의 YouTube 썸네일은 여러 디자인 자산을 사용하여 레이어와 오버레이를 만들기 위해 프롬프트를 사용해 완전히 생성되었습니다.\n\n배경, 로고, 제 자신의 cutout 배경 이미지 및 폰트(Open Sans)를 포함한 이미지를 프롬프트만으로 만들었습니다. 심지어 폰트 픽셀 크기까지 정의했죠.\n\n마진 및 패딩 CSS 규칙으로 완전히 원하는 대로 만들 수 있습니다.\n\n# 전체 YouTube 썸네일 만드는 방법입니다\n\n제목 | 설명\n----|----\nStep 1 | 배경과 로고 추가\nStep 2 | 자신의 cutout 이미지 추가\nStep 3 | 폰트(Open Sans) 적용\nStep 4 | 마진과 패딩 CSS 규칙 적용\n\n<div class=\"content-ad\"></div>\n\n시작하기 전에 몇 가지 에셋이 필요합니다:\n\n- Cutout Image: 투명 배경이 있는 PNG 이미지, 자신이나 다른 주제의 이미지입니다. 이미 가지고 있지 않은 경우 무료 온라인 사진 편집 도구를 사용하여 만들 수 있습니다.\n- Logo: 투명 배경이 있는 PNG 형식의 브랜드 로고입니다.\n\n이 튜토리얼에서 제가 사용한 것은:\n\n- 투명 배경을 가진 내 사진\n- AI Growth Guys 로고 (여러분의 로고 선택)\n\n<div class=\"content-ad\"></div>\n\n아래는 전적으로 프롬프트에 의해 생성되었습니다. 프롬프트를 사용하여 원하는 레이어나 에셋을 추가할 수 있습니다.\n\n## 배경 차원 설정하기\n\n- 배경 크기 지정: ChatGPT에게 이미지의 크기를 알려주는 것으로 시작하세요. 이 예시에서는 1920x1080 픽셀의 검은색 배경을 사용했습니다.\n- 텍스트 추가: ChatGPT에게 이미지에 텍스트를 추가하도록 지시하세요. 예를 들어, \"상단 좌측에 흰색 120픽셀 굵은 Open Sans 글꼴을 넣고 '나는 ChatGPT를 포토샵으로 변신시킨다'라고 작성해주세요.\"\n- 컷아웃 이미지 추가: 그다음 컷아웃 이미지를 업로드하세요. ChatGPT가 이를 왜곡하지 않고 전체 이미지를 사용하도록 지정해야 합니다.\n\n## 로고 배치하기\n\n<div class=\"content-ad\"></div>\n\n- 초기 배치: 먼저 ChatGPT에게 로고를 왼쪽 하단에 배치하도록 지시하세요. 너무 크게 나오거나 원하는 위치에 없다면 크기와 위치를 조정해보세요.\n- 크기 조정: 로고가 너무 크다면 ChatGPT에게 원하는 크기로 조절하도록 요청하세요. 저는 원본 크기의 25%로 조정했고, 왼쪽 하단에 배치했습니다.\n\nChatGPT는 지침과 HTML을 이해합니다. 그래서 간격과 크기에 관한 일반적인 규칙을 말할 수 있어요.\n\n# 마지막 손질 — 몇 번의 반복이 필요할 거예요.\n\n- 색상 조정: 텍스트를 두드러지게 하기 위해 색상을 변경할 수 있어요. 예를 들어, 'Photoshop'이라는 단어를 빨간색으로 바꾸도록 ChatGPT에게 요청했고, 나머지 텍스트는 흰색으로 유지했어요.\n- 필요한 경우 재배치: 요소들이 예기치 않게 움직일 때, ChatGPT에게 재배치하도록 요청하세요. 예를 들어, 'Photoshop' 단어는 원래 위치에 유지하고 색상만 변경하도록 ChatGPT에게 지시했어요.\n- 또한 ChatGPT에게 텍스트를 여러 줄로 배치하도록 지시해야 했어요. 처음에는 전체 텍스트가 한 줄에 있었는데, 맞지 않았어요. 한 번의 지시만으로 두 줄로 변경하도록 지시했어요.\n\n<div class=\"content-ad\"></div>\n\n# 최종 조정 및 내보내기\n\n모든 조정을 마친 후 멋진 최종 이미지가 나와야 합니다. 이를 소셜 미디어, YouTube 썸네일 또는 다른 창의적인 프로젝트에 활용할 수 있습니다.\n\n마지막에 문제가 생겼었어요. ChatGPT가 제 모든 레이어에 지쳐서 제 프롬프트 스레드를 깨는 것 같더라구요. ㅋㅋ\n\n하지만 멋지게 작동했어요. 더 나아질 것입니다.\n\n<div class=\"content-ad\"></div>\n\n저는 ChatGPT의 사용 방법, AI를 활용한 디자인, 그리고 온라인 비즈니스 성장에 대해 가르치고 있어요.\n\n저의 👉 AI Growth Guys Newsletter 👈를 확인해보세요!\n\n아래에서 다른 채널도 보실 수 있어요.\n\n우리 YouTube 채널도 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n당사 웹사이트에서 저희를 팔로우해주세요: AI Growth Guys","ogImage":{"url":"/assets/img/2024-06-19-ChatGPTDesignHack99ofUsersDontKnowThis_0.png"},"coverImage":"/assets/img/2024-06-19-ChatGPTDesignHack99ofUsersDontKnowThis_0.png","tag":["Tech"],"readingTime":4},{"title":"닉이 말하는데, 무료로 ChatGPT로 시간당 145를 벌 수 있다고 해요 방법을 알려드리겠습니다","description":"","date":"2024-06-19 03:20","slug":"2024-06-19-NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow","content":"\n\n## 정말 해볼 가치가 있는 방법인가요?\n\n![image](/assets/img/2024-06-19-NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow_0.png)\n\n온라인으로 돈을 벌기 위한 방법에 대한 많은 YouTube 동영상을 시청했었는데요,\n\n하지만 대부분이 완전한 헛소리라는 걸 알게 되어서 오랜 시간을 멈추었어요.\n\n<div class=\"content-ad\"></div>\n\n어제는 Dave Nick이라는 유튜버의 영상을 보게 되었는데, 그 영상에서 \"무료로 ChatGPT로 시간당 $145 벌 수 있다\"고 말했어요.\n\n그래서 이 영상에 대한 간단한 리뷰 기사를 쓰기로 결정했어요. 이것을 통해 여러분이 이것을 시도해볼 가치가 있는지 여부를 알 수 있을 거예요.\n\n영상을 확인하고 싶다면 여기로 이동해주세요:\n\n## 방법은 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n닉은 쓰여진 글 스레드를 통해 X에 대한 무료 책을 제공하여 온라인으로 돈을 벌 수 있는 방법을 제안합니다.\n\n아래는 작동 방식입니다:\n\n1. \"Audible\"이라는 프로그램에 등록합니다.\n2. 이 프로그램은 창작자 프로그램을 제공하여 다른 사람들을 그들의 웹 사이트에서 무료 체험에 등록하도록 초대할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이번 시범을 신청하면 초대한 사람에게 5달러의 수수료를 받게 됩니다.\n\n당신의 업무는 간단합니다:\n\n무료 eBook을 방문자들에게 홍보하여 시험 가입을 유도하고 그에 대한 보상금을 받는 것입니다.\n\n![이미지](/assets/img/2024-06-19-NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow_1.png)\n\n<div class=\"content-ad\"></div>\n\n나는 솔직히 이것이 돈을 버는 똑똑하고 효과적인 방법이라고 생각해요,\n\n하지만,\n\n이미 많은 관객이 있다면 그게 가능해요.\n\n## 링크로 트래픽을 어떻게 받을 것인가:\n\n<div class=\"content-ad\"></div>\n\n이제, 큰 질문이 있어요:\n\n그 제휴 링크로 트래픽을 어떻게 유도할까요?\n\n음, \n\n닉은 그것에 대한 전략을 갖고 있어요.\n\n<div class=\"content-ad\"></div>\n\n그는 ChatGPT와 얼티밋 X 트위터 콘텐츠 크리에이터를 사용하는 것을 제안합니다.\n\n![이미지](/assets/img/2024-06-19-NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow_2.png)\n\nChatGPT를 사용하면 인기 있는 책 카테고리를 파악하고 이러한 책에서 주요 통찰을 요약한 흥미로운 트위터 스레드를 작성할 수 있습니다.\n\n이 스레드는 두 가지 목적을 제공합니다:\n\n<div class=\"content-ad\"></div>\n\n그들은 귀하의 관객에 가치를 제공하고 그 안에 포함된 제휴 링크를 신속하게 홍보합니다.\n\n## 하지만 우리 진지해져 봅시다.\n\nTwitter`X`에서 큰 팔로워들을 구축하는 데는 시간과 노력이 필요합니다.\n\n그러니까, 처음부터 시작한다면,\n\n<div class=\"content-ad\"></div>\n\n결과를 실제로 보기까지는 시간이 걸릴 수 있어요.\n\n그 방법은 희망적으로 들리지만, 어떠한 방식으로도 금방 부자가 될 수 있는 계획은 아니에요.\n\n요구되는 것:\n\n헌신성,\n\n<div class=\"content-ad\"></div>\n\nConsistency,\n\n트위터와 같은 소셜 미디어 플랫폼에서 인기를 얻기 위해서는 일정함과 약간의 행운이 필요합니다.\n\n![NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow](/assets/img/2024-06-19-NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow_3.png)\n\n## 대신 해야 할 일\n\n<div class=\"content-ad\"></div>\n\n트위터만 사용하는 것이 아닌 다른 방법들도 시도해보세요.\n\n광고하고 싶은 책에 대한 블로그나 웹사이트를 시작해보세요.\n\n리뷰와 유용한 기사를 작성하여 해당 책에 관심 있는 사람들을 끌어들일 수 있어요.\n\n또한 사이트를 방문한 사람들로부터 이메일 주소를 수집하고 유용한 콘텐츠와 특별 혜택을 보내줄 수도 있어요.\n\n<div class=\"content-ad\"></div>\n\nAudible 홍보만 하는 게 아니라, 더 좋은 혜택을 제공하는 다른 회사를 찾아보세요.\n\n더 많은 수익을 올리거나 여러분의 대상 독자들이 더 선호할 수 있는 제품을 제공하는 제휴 프로그램이 있을 수도 있습니다.\n\n여러 가지 방법을 시도하고 새로운 아이디어에 열려 있으면, 여러분에게 가장 잘 맞는 온라인 수익을 창출할 수 있는 최선의 방법을 찾을 수 있습니다.\n\n## 요약하여 말하자면\n\n<div class=\"content-ad\"></div>\n\n닉의 전략이 똑똑한 것은 사실이지만, 실제로 돈을 벌기 전에 트위터에서 이상적인 청중을 유치해야 합니다.\n\n솔직히 말하자면, 이 방법으로 매 시간 145달러를 벌 수 있다고 생각하지 않아요.\n\n아마도 한 달에 수백 달러를 벌 수도 있지만, 그것도 이미 청중이 있는 경우에 한합니다.\n\n내 의견으로는, 이미 청중이 없다면 이 방법을 시도하지 않는 것이 좋다고 생각해요.\n\n<div class=\"content-ad\"></div>\n\n위 글을 읽어주셔서 감사합니다❤\n\n유튜브에서 닉 데이브의 채널을 꼭 확인해보세요. 온라인으로 돈을 벌기 위한 다양한 방법을 나누고 있어요.\n\n또한, 이 글은 이 플랫폼의 다른 작가인 브리트 말카의 영감을 받았어요 🦊\n\n꼭 그녀의 페이지도 방문해보세요.","ogImage":{"url":"/assets/img/2024-06-19-NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow_0.png"},"coverImage":"/assets/img/2024-06-19-NickSaysYouCanMake145PerHourWithChatGPTForFreeHereshow_0.png","tag":["Tech"],"readingTime":3},{"title":"애플 인텔리전스, 혹은 단순한 실용주의","description":"","date":"2024-06-19 03:19","slug":"2024-06-19-AppleIntelligenceorsimplypragmatism","content":"\n\n![ScreenShot](/assets/img/2024-06-19-AppleIntelligenceorsimplypragmatism_0.png)\n\n회사의 월드와이드 개발자 회의(WWDC)에서의 개회 발표는 제품에 AI를 통합하지 못한 실패에 집중했지만 이를 공식적으로 인정하지 않으려고 했습니다. 본질적으로 필요에 의한 덕목을 만드는 시도로, 그것은 빅테크 세계에서 쉽지 않습니다.\n\nApple의 기본적인 문제는 개인 정보를 존중하는 정책으로 이미지와 차별화 측면에서 이용하고 있는 점 때문에, Meta, Google, Microsoft 등 다른 기업들보다 운영 데이터가 훨씬 적다는 것입니다. 이미 2015년에 Apple의 개인 정보 보호 정책이 일부 데이터 과학자들이 회사의 취업 제안을 거절하고, 대신 다른 기업으로 가서 더 많은 자료를 활용할 수 있는 선택을 할 정도로 영향을 미치고 있다는 보도가 있었습니다. 그 이후로 상황은 변하지 않았지만, 한 가지 문제가 추가되었습니다: 매우 효율이 낮은 생성 알고리즘이 도입되어 데이터 요구량이 급증하게 되어 회사의 문제를 악화시키고 있습니다.\n\n이렇게 보았을 때, Apple은 어떻게 해야 할까요? 기본적으로 브랜드의 A와 놀아야 하고, 자사 모델을 Apple Intelligence로 명명하여 \"우리 같은 이들을 위한 AI\"로 위치시켜야 합니다. 다른 기술에 관심을 가지기 귀찮은 사람들을 위한 AI, 즉 몇 가지 기본 기능을 원하는 사람들을 위한 AI로 말이죠. 그리고 무엇보다도, 다른 브랜드의 기기를 소유한 사람들에게 열등함을 느끼지 말아야 합니다. 지루하고 실용적일지 모르지만... 그것이 반드시 나쁜 것은 아닙니다. Apple만이 할 수 있는 것처럼, 회사의 플랫폼을 끝없이 선보여서 참가자들을 피로하게 만든 후 남았습니다.\n\n<div class=\"content-ad\"></div>\n\n애플의 솔루션은 기본적인 AI로, 기기에서 개인 정보를 침해하지 않고 작동합니다. 기본적으로 간단한 기계 학습입니다. 하지만 iPhone의 프로세서가 그리 강력하지 않기 때문에, 특정 쿼리에 대한 두 번째 수준이 애플 서버에서 작동합니다. 애플 실리콘이 사용되며 애플 맵스의 경우처럼 쿼리를 수신하고 답변한 후 십 분 후에 삭제됩니다. 귀하의 데이터는 저장되지 않으며 회사 자체에서 액세스할 수 없습니다.\n\n이 모든 것은 매우 잘되고 있는데, Nvidia의 강력한 칩을 사용하지 않을 것이라고 결정한 회사가 경쟁 업체의 알고리즘 성능에 견줄만한 성과를 내지 못할 수 있다는 방법은 없습니다. 그에 대한 대응으로 애플 사용자들은 OpenAI의 수요에 따라 ChatGPT 4o에 액세스할 수 있게 되었습니다. 이를 통해 OpenAI에 상당한 수익을 창출하고 산업 리더로 올라설 수 있는 발판을 제공합니다. 사용자들이 요청을 하면, 그들의 기기는 그 정보가 애플의 벽을 넘어가게 된다는 경고를 보내며, 그 결과 OpenAI가 정보를 활용하는 용도에 노출되었다는 응답을 받게 될 것이지만 그냥 ChatGPT 앱을 열어서 프롬프트를 입력한 것과 유사한 답변을 받을 수 있습니다. 게다가, 애플은 사용자가 이를 활용하는 회사들과의 협정에 도달할 때 추가 알고리즘을 제공할 의사가 있다고 밝혔습니다. 그러니 일론 머스크가 이를 좋아하지 않는다고 해도 무슨 상관이죠?\n\n좋은 솔루션일까요? 아니죠, 실용주의입니다. 노인 시리의 이미지를 조금 꾸미는 수준으로 나아가 더 이상 어리석게 보이지 않도록 하고 (다행히 회사에게는 아직도 Alexa와 같이 더 한정적인 대안이 있습니다), 우리는 앱 간 작업 가능성을 구축합니다. 이는 사용자와 애플리케이션 개발자 모두에게 이득이 되는데 (WWDC임을 잊지 말아주세요, 그들의 대상은 그들이기 때문입니다), 우리는 어떤 앱이든 합리적인 AI에 액세스할 수 있는 플랫폼을 구축할 수 있습니다. 그것은 가치 제안을 향상시킬 수 있는 AI를 쉽게 액세스할 수 있도록 하는 API와 개발 도구로 이루어진 것입니다. 그리고 부차적으로 사용자들은 이러한 기능에 액세스하기 위해 기기를 업데이트하는 것을 권장받게 됩니다.\n\n한계를 이점으로 전환하는 측면에서 그리 나쁜 움직임은 아니나 다소 너무 자명한 것입니다. 그동안 애플은 미래의 AI 개발이 조금 더 지능적이되어야 하며, 학습의 한계를 해결하기 위해 엄청난 양의 데이터 대신 더 효율적인 모델을 선택하는 것을 기대하고 있습니다. 일단은 애플은 데이터 전쟁의 구경꾼일 뿐입니다.\n\n<div class=\"content-ad\"></div>\n\n다음은 마크다운 형식으로 표 태그를 변경해야 합니다.","ogImage":{"url":"/assets/img/2024-06-19-AppleIntelligenceorsimplypragmatism_0.png"},"coverImage":"/assets/img/2024-06-19-AppleIntelligenceorsimplypragmatism_0.png","tag":["Tech"],"readingTime":3},{"title":"애플이 OpenAI에 전쟁 선포했다고","description":"","date":"2024-06-19 03:18","slug":"2024-06-19-DidApplejustdeclarewaronOpenAI","content":"\n\n애플의 역사는 핵심 경쟁사를 생태계에 통합한 뒤 결국 그들을 앞질러서나 제거하는 양상을 보여줍니다.\n\nMac OS 하이 시에라(2017년)에서 \"메일, 연락처, 캘린더, 메시지 및 다른 앱과 함께 사용할\" \"인터넷 계정\"을 추가하는 인터페이스를 고려해보세요:\n\n![이미지](/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_0.png)\n\n일곱 년 후에는 이 패널을 서드파티 \"시리 소스\"로 대체하여 Apple Intelligence의 개인화된 유틸리티를 구동할 수 있다고 상상할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n실제로 Siri는 이미 사용하는 애플리케이션에서 \"학습\"할 수 있습니다:\n\n![image1](/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_1.png)\n\nmacOS Sequoia와 iOS 18이 출시되면 Siri를 통해 ChatGPT, Gemini 또는 다른 모델에 질문을 전달할 수 있을 것입니다. 아마도 Apple이 몇 년 전에 \"인터넷 계정\" 제공업체들과 유사한 방식으로 통합할 것으로 예상됩니다.\n\n![image2](/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_2.png)\n\n<div class=\"content-ad\"></div>\n\n맥OS 소노마( macOS Sonoma)에서는 Twitter, Facebook 및 LinkedIn이 추가할 수있는 인터넷 계정에서 누락되어 있습니다. 아마도 메시지, 연락처 및 캘린더의 양방향 동기화가 중단된 통합이나 API 변경으로 이것이 발생한 것일 수 있습니다.\n\n그럼에도 불구하고 이것은 애플이 경쟁사를 OS 수준의 통합에 통합시켰다가 결국은 그것들을 오래된 기술로 만드는 추세를 보여줍니다.\n\nTwitter (지금은 X)와 LinkedIn의 경우, 공유 기능이 시스템 전체 기능으로 진화했습니다. 애플은 케임브리지 아날리티카 사건 이후 Facebook으로부터 거리를 둘 필요가 있었는데, 연락처 통합(지금은 연락처)을 계속 유지하면 신뢰성이 훼손될 것이었기 때문입니다. 이 전례는 애플이 AI 서비스의 통합에 대해 비슷한 방식을 채택할 수 있음을 시사합니다. 초기에는 더 열린 실험적인 방식으로 시작하여 뒤이어 자사의 소유 제공을 우선시하는 방식으로 전환할 수 있습니다.\n\n따라서 애플은 AI 경주를 개인 정보 보호를 고려한 환경에서 진행하고 있지만, 애플은 사용자 데이터 및 경험에 대한 통제를 유지하기 위해 필요할 때 OpenAI의 존재를 애플 운영 체제에서 사라지게 할 계획이라고 의심하지 않습니다. 애플 인텔리전스를 자사의 \"프라이빗 클라우드 컴퓨팅\"과 결합하면 ChatGPT 및 이 목록에 나타날 수 있는 다른 서비스를 필요할 때마다 떨어뜨릴 수 있는 능력을 제공받게 될 것이며, 이를 트위터, 페이스북 및 링크드인과 같은 서비스에 대해 한 것처럼 할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image 1](/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_3.png)\n\nWWDC 키노트에서, 멋진 크레이그 페데리기가 Siri의 지연 때문에 ChatGPT와 주목을 나눠야 한다는 사실에 자신을 젠더고 있을 것으로 의심했습니다 (상당히 큰 선두를 잃은 후).\n\n![image 2](/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_4.png)\n\n그럼에도 불구하고, 페데리기는 동시에 애플에 시간을 벌려주었습니다, 왜냐하면 이미 경쟁사의 능력을 흡수하는 방안이 이미 마련되어 있기 때문입니다.\n\n\n<div class=\"content-ad\"></div>\n\n그게 바로 애플 인텔리전스가 의미하는 바에요.","ogImage":{"url":"/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_0.png"},"coverImage":"/assets/img/2024-06-19-DidApplejustdeclarewaronOpenAI_0.png","tag":["Tech"],"readingTime":2},{"title":"딥러닝 그림으로 쉽게 이해하기, 제4부 순환 신경망","description":"","date":"2024-06-19 03:14","slug":"2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks","content":"\n\n저희의 그림으로 보여주는 딥러닝 여정 Part 4에 오신 것을 환영합니다! 오늘은 순환 신경망(Recurrent Neural Networks)에 대해 자세히 살펴보겠습니다. 입력, 출력, 활성화 함수 같은 익숙한 개념들에 대해 이야기할 건데, 조금씩 다른 면을 발견할 거에요. 그리고 이번이 여정의 첫 스탑이라면, 특히 Part 1과 Part 2를 읽어보시길 추천드립니다.\n\n순환 신경망(RNN)은 이전 상태에 의존하는 다음 위치에 영향을 받는 순서 기반 문제를 처리하기 위해 명시적으로 설계된 독특한 모델입니다.\n\n간단한 MIT 강의 예시로, 시간 tn에 특정 지점에 있는 공을 상상해보세요.\n\n<div class=\"content-ad\"></div>\n\n만약 우리가 볼의 방향을 예측하라는 요청을 받았다면, 추가 정보 없이는 추측의 일입니다. 볼은 아무 방향으로 움직일 수 있습니다.\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_1.png)\n\n하지만 만약 볼의 이전 위치에 대한 데이터가 제공된다면 어떨까요?\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_2.png)\n\n<div class=\"content-ad\"></div>\n\n이제 우리는 공이 오른쪽으로 계속 움직일 것이라고 자신 있게 예측할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_3.png)\n\n이 예측 시나리오는 우리가 순차적 문제라고 부르는 것입니다 — 여기서 답은 이전 데이터에 강력하게 영향을 받습니다. 이러한 순차적 문제는 모든 곳에 있으며, 과거 온도 데이터에 기반한 내일의 온도 예측부터 감정 분석, 명명된 개체 인식, 기계 번역 및 음성 인식을 포함한 다양한 언어 모델에 이르기까지 다양합니다. 오늘은 감정 탐지에 초점을 맞추어 시퀀스 기반 문제의 간단한 예제를 살펴보겠습니다.\n\n감정 탐지에서는 텍스트 조각을 가져와 해당 텍스트가 긍정적인지 부정적인지 여부를 결정합니다. 오늘은 영화 리뷰를 입력으로 받아 그것이 긍정적인지 아닌지를 예측하는 RNN을 구축할 것입니다. 따라서 이 영화 리뷰를 고려해 봅시다...\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_4.png)\n\n우리의 신경망이 이것이 긍정적인 감정을 가지고 있다고 예측하길 원합니다.\n\n이것은 간단한 분류 문제처럼 들릴 수 있지만, 여기서 표준 신경망이 직면한 두 가지 주요 도전 과제가 있습니다.\n\n첫째, 우리는 가변 입력 길이를 다루고 있습니다. 표준 신경망은 길이가 다른 입력을 처리하는 데 어려움을 겪습니다. 예를 들어, 만약 우리가 세 단어로 이루어진 영화 리뷰로 신경망을 훈련한다면, 우리의 입력 크기는 세 개로 고정될 것입니다. 그러나 더 긴 리뷰를 입력하고 싶다면 어떻게 해야 할까요?\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_5.png)\n\n위의 리뷰를 12개의 입력값으로 처리하는 데 어려워하고 처리하지 못할 수 있습니다. 지난 글들과 달리 입력값의 개수가 고정된 게 아닙니다(아이스크림 수익 모델은 온도와 요일 2개의 입력값이 있었습니다). 이 경우에는 모델이 유연하게 동작하고 얼마든지 많은 단어들에 적응할 수 있어야 합니다.\n\n또한 연속적인 입력값을 가지고 있습니다. 일반적인 신경망은 입력값의 방향성을 완전히 이해하지 못하는데, 이것은 여기서 중요합니다. 두 문장이 정확히 같은 단어를 포함할지라도 순서가 다르면 완전히 반대의 의미를 가질 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_6.png)\n\n\n<div class=\"content-ad\"></div>\n\n이러한 도전에 직면할 때, 우리는 동적으로 입력을 순차적으로 처리할 수 있는 방법이 필요하다. 여기서 RNN이 빛을 발한다.\n\n이 문제에 접근하는 방법은 먼저 리뷰의 첫 단어 \"that\"을 처리하는 것이다:\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_7.png)\n\n그런 다음 이 정보를 사용하여 두 번째 단어 \"was\"를 처리한다:\n\n<div class=\"content-ad\"></div>\n\n이제 위의 모든 정보를 사용하여 마지막 단어 \"현저한(phenomenal)\"을 처리하고 리뷰의 감정에 대한 예측을 제공해보겠습니다:\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_9.png)\n\n신경망을 구성하기 전에 입력에 대해 논의해야 합니다. 신경망에 입력되는 값은 숫자여야 합니다. 그러나 여기서의 입력값은 단어이므로 이러한 단어를 숫자로 변환해야 합니다. 이를 수행하는 여러 가지 방법이 있지만, 오늘은 기본적인 방법을 사용하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n지금은 10,000개의 단어로 이루어진 큰 사전이 있다고 상상해 봅시다. 우리는 (순진하게) 리뷰에 나오는 어떤 단어라도 이 10,000단어 사전 안에서 찾을 수 있다고 가정할 것입니다. 각 단어는 해당하는 숫자로 매핑되어 있습니다.\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_10.png)\n\n단어 \"that\"을 숫자들의 묶음으로 변환하려면, \"that\"이 매핑된 숫자를 확인해야 합니다...\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_11.png)\n\n<div class=\"content-ad\"></div>\n\n이것을 10,000 개의 0으로 이루어진 행렬로 표현하되 8600번째 요소만 1인 형태로 나타내면 됩니다:\n\n![Matrix](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_12.png)\n\n비슷하게, 다음 두 단어 \"was\" (사전에서 9680번째 단어)와 \"phenomenal\" (사전에서 4242번째 단어)의 수치적 표현은 다음과 같습니다:\n\n![Matrix](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_13.png)\n\n<div class=\"content-ad\"></div>\n\n이제 우리는 단어를 신경망 친화적인 입력으로 변환하는 방법을 알아봤습니다.\n\n이제 우리의 주의를 신경망의 디자인으로 돌려봅시다. 간단히 설명하기 위해, 네트워크가 10,000개의 입력(= 1단어), 하나의 뉴런으로 이루어진 단일 은닉층 및 하나의 출력 뉴런을 가정해봅시다.\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_14.png)\n\n물론, 이것이 완전히 훈련된 신경망인 경우, 각 입력마다 연관된 가중치가 있고 뉴런들은 편향(bias) 항을 가지게 될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n![신규 이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_15.png)\n\n이 네트워크에서 입력 가중치는 i가 입력을 나타내는 곳에 wᵢ로 표시됩니다. 숨겨진 레이어 뉴런의 편향 항은 bₕ로 나타냅니다. 숨겨진 레이어와 출력 뉴런을 연결하는 가중치는 wₕᵧ입니다. 마지막으로, 출력 뉴런의 편향은 y가 결과를 나타내므로 bᵧ로 표시됩니다.\n\n우리는 활성화 함수로 숨겨진 뉴런에 대해 쌍곡선 탄젠트 함수 (tanh)를 사용할 것입니다.\n\n![신규 이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_16.png)\n\n<div class=\"content-ad\"></div>\n\n첫 번째 기사에서 다룬 내용을 다시 상기해보자면, tanh 함수는 입력값을 받아 -1부터 1 사이의 출력값을 생성합니다. 큰 양수 입력은 1에 가까워지고, 큰 음수 입력은 -1에 가까워집니다.\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_17.png)\n\n텍스트의 감정을 판단하기 위해, 우리는 출력 뉴런에서 시그모이드 활성화 함수를 사용할 수 있습니다. 이 함수는 숨겨진 레이어에서 출력을 받아 긍정적 감정의 확률을 나타내는 0부터 1까지의 값을 출력합니다. 1에 가까운 예측은 긍정적인 리뷰를 나타내며, 0에 가까운 예측은 긍정적이지 않을 확률이 높다는 것을 시사합니다.\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_18.png)\n\n<div class=\"content-ad\"></div>\n\n이 활성화 함수들을 사용하면, 우리의 신경망은 다음과 같이 나타납니다:\n\n![neural network](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_19.png)\n\n이 신경망은 텍스트 입력을 받아 해당 텍스트가 긍정적인 감정을 가질 확률을 예측합니다. 위 예시에서, 신경망은 입력으로 \"that\"을 처리하고 이것이 긍정적인 경우일 확률을 예측합니다. 솔직히 말해서, \"that\"이라는 단어 자체로는 감정을 예측하는 데 큰 힌트를 주지는 않습니다. 이제 다음 단어를 신경망에 어떻게 통합할지 알아야 합니다. 이것이 순환 신경망의 순환적 측면이 작용하고, 기본 구조가 수정되는 시기입니다.\n\n리뷰의 두 번째 단어 \"was\"를 입력으로 넣기 위해, 위의 신경망을 정확하게 복사하여 새로 만듭니다. 그러나, 입력으로 \"that\" 대신 \"was\"를 사용합니다:\n\n<div class=\"content-ad\"></div>\n\n![Deep Learning Illustrated Part 4 Recurrent Neural Networks 20](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_20.png)\n\n앞 단어인 \"this\"에서의 정보도 이 신경망에서 사용하려고 합니다. 따라서, 이전 신경망의 은닉층에서의 출력을 가져와 현재 신경망의 은닉층으로 전달합니다:\n\n![Deep Learning Illustrated Part 4 Recurrent Neural Networks 21](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_21.png)\n\n이건 중요한 단계니까 천천히 살펴봅시다.\n\n<div class=\"content-ad\"></div>\n\n첫 번째 기사에서 우리는 각 뉴런의 처리가 두 단계로 이루어진다는 것을 배웠어요: 합산과 활성화 함수 (이 용어가 무엇을 의미하는지 잘 모르겠다면 첫 번째 기사를 읽어보세요). 이러한 과정이 첫 번째 신경망에서 어떻게 이루어지는지 살펴봅시다.\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_22.png)\n\n첫 번째 신경망의 은닉층 뉴런에서 첫 번째 단계는 합산입니다:\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_23.png)\n\n<div class=\"content-ad\"></div>\n\n여기서 입력값들을 각각의 가중치로 곱하고 편향 항을 모든 곱의 합에 더합니다:\n\n![equation1](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_24.png)\n\n이 방정식을 단순화하기 위해 입력 가중치를 wₓ로, 입력값을 x로 나타냅시다:\n\n![equation2](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_25.png)\n\n<div class=\"content-ad\"></div>\n\n그 다음, 2단계에서는 이 합계를 활성화 함수 tanh를 통해 전달합니다:\n\n![Image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_26.png)\n\n![Image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_27.png)\n\n이로써 첫 번째 신경망의 은닉층에서 출력 h₁을 생성합니다. 여기서 두 가지 옵션이 있습니다. h1을 출력 뉴론으로 전달하거나 다음 신경망의 은닉층으로 전달할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n(option 1) 만약 \"that\"에 대한 감성 예측을 하려면, h₁을 가져와서 출력 뉴런에 전달할 수 있습니다:\n\n![image1](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_28.png)\n\n출력 뉴런에 대해서는 합산 단계를 진행합니다...\n\n![image2](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_29.png)\n\n<div class=\"content-ad\"></div>\n\n<table>\n  <tr>\n    <td><img src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_30.png\" /></td>\n  </tr>\n</table>\n\n…그리고 이 합계에 시그모이드 함수를 적용합니다…\n\n<table>\n  <tr>\n    <td><img src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_31.png\" /></td>\n  </tr>\n</table>\n\n…이것으로 우리가 예측한 긍정적인 감정 값이 나옵니다:\n\n<div class=\"content-ad\"></div>\n\n![Image 1](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_32.png)\n\nSo, y₁_hat here shows us the predicted probability that \"that\" has a positive sentiment.\n\nHowever, that's not what we aim for. Instead of passing h₁ to the output neuron, we transfer this information to the next neural network in the following way:\n\n![Image 2](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_33.png)\n\n<div class=\"content-ad\"></div>\n\n신경망의 다른 부분과 마찬가지로 우리는 한 숨김층에서 다른 숨김층으로의 입력에 대한 입력 가중치 wₕₕ를 가지고 있습니다. 숨김층은 h₁을 h₁과 wₕₕ의 곱을 합산 단계에 추가함으로써 통합합니다. 따라서 두 번째 신경망 뉴런의 숨김층의 업데이트된 합산 단계는 다음과 같습니다:\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_34.png)\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_35.png)\n\n그런 다음이 합산은 tanh 함수를 통과합니다...\n\n<div class=\"content-ad\"></div>\n\n아래는 두 번째 신경망의 은닉 레이어에서 출력되는 h₂를 생성합니다:\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_37.png)\n\n여기서 다시, h₂를 출력 뉴런을 통해 전달하여 감성 예측을 얻을 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_38.png)\n\n![Image 2](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_39.png)\n\n여기서, y₂_hat은 \"그것이\"가 긍정적인 감정을 가졌을 확률을 예측합니다.\n\n하지만 리뷰는 여기서 끝나지 않는다는 것을 알고 있습니다. 그래서 이전의 숨겨진 레이어 출력 값을 현재의 숨겨진 레이어에 전달하여 이 프로세스를 한 번 더 복제할 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_40.png)\n\nWe process the hidden layer neuron...\n\n![Image 2](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_41.png)\n\n...to an output, h₃:\n\n\n<div class=\"content-ad\"></div>\n\n아래는 리뷰에서 마지막 단어이자 마지막 입력이기 때문에, 우리는 이 데이터를 외부 뉴런에 전달합니다...\n\n아래는 리뷰에서 마지막 단어이자 마지막 입력이기 때문에, 우리는 이 데이터를 외부 뉴런에 전달합니다...\n\n... 그리고 이를 통해 감정의 최종 예측을 제공합니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_44.png\" />\n\n이 y₃_hat은 우리가 원하는 영화 리뷰의 감성으로, 처음에 그려 놓은 것을 얻는 방법입니다!\n\n<img src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_45.png\" />\n\n## 형식적 표현\n\n<div class=\"content-ad\"></div>\n\n위의 다이어그램을 자세히 설명하면 다음과 같습니다:\n\n![다이어그램](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_46.png)\n\n각 단계마다 입력 x가 숨겨진 레이어를 통해 흐르고 출력 h를 생성합니다. 이 출력은 다음 신경망의 숨겨진 레이어로 이동하거나 감정 예측인 y_hat으로 이어집니다. 각 단계에는 가중치(weight)와 편향(bias) 용어가 포함되어 있습니다(다이어그램에는 편향이 표시되지 않음). 강조할 중요한 점은 모든 숨겨진 레이어를 하나의 조밀한 상자로 통합하고 있다는 것입니다. 모델에는 하나의 숨겨진 레이어와 단일 뉴런이 있는 것만 포함되어 있지만, 더 복잡한 모델에는 여러 숨겨진 레이어와 다수의 뉴런이 포함될 수 있고, 이러한 모든 요소가 이 상자(숨겨진 상태)로 압축됩니다. 이 숨겨진 상태는 숨겨진 레이어의 추상적인 개념을 담고 있습니다.\n\n본질적으로, 이것은 이 신경망의 단순화된 버전입니다:\n\n<div class=\"content-ad\"></div>\n\n\n![딥러닝 일러스트레이티드 파트 4: 순환 신경망 (RNN)](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_47.png)\n\n이미지에서도 볼 수 있듯이 이 과정을 단순화한 다이어그램으로 표현할 수 있습니다:\n\n![딥러닝 일러스트레이티드 파트 4: 순환 신경망 (RNN)](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_48.png)\n\n이 프로세스의 본질은 출력값을 은닉층으로 순환해서 되돌린다는 점이며, 이것이 왜 순환 신경망이라고 불리는지에 대한 이유입니다. 이는 종종 교과서에서 신경망이 어떻게 표현되는지 보여주는 방식입니다.\n\n\n<div class=\"content-ad\"></div>\n\n수학적으로 이 문제를 두 가지 기본 방정식으로 요약할 수 있어요:\n\n\n![equation1](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_49.png)\n\n\n첫 번째 방정식은 숨겨진 상태 내에서 발생하는 전체 선형 변환을 포함해요. 이 경우, 이 변환은 개별 뉴런 내에서의 tanh 활성화 함수예요. 두 번째 방정식은 출력 층에서 발생하는 변환을 나타내며, 이는 저희 예시에서 시그모이드 활성화 함수에 해당돼요.\n\n\n![equation2](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_50.png)\n\n\n<div class=\"content-ad\"></div>\n\n# RNN이 해결하는 문제 유형\n\n## 많은 것들을 하나로\n\n우리는 이전에 여러 입력(우리의 경우에는 리뷰 안의 모든 단어)이 RNN에 공급되는 시나리오를 논의했습니다. 그러면 RNN은 리뷰의 감정을 나타내는 단일 출력을 생성합니다. 각 단계마다 출력을 가질 수 있지만, 우리의 주된 관심사는 최종 출력에 있습니다. 왜냐하면 이는 전체 리뷰의 감정을 담고 있기 때문입니다.\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_51.png)\n\n<div class=\"content-ad\"></div>\n\n다른 예는 텍스트 완성입니다. 단어 문자열을 제공하면 RNN이 다음 단어를 예측하도록 원합니다.\n\n\n<img src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_52.png\" />\n\n\n## One-To-Many\n\n\n<img src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_53.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n원 대 다 문제의 고전적인 예는 이미지 캡션입니다. 여기서 하나의 입력은 이미지이고 출력은 여러 단어로 구성된 캡션이 됩니다.\n\n## 다 대 다\n\n![RNN Example](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_54.png)\n\n이 유형의 RNN은 기계 번역과 같은 작업에 사용됩니다. 예를 들어 영어 문장을 힌디어로 번역하는 작업입니다.\n\n<div class=\"content-ad\"></div>\n\n# 단점\n\n이제 RNN이 작동하는 방식을 자세히 살펴보았으니, 왜 그들이 널리 사용되지 않는지 살펴볼 가치가 있습니다 (스토리 전환!). 잠재력이 있음에도 불구하고, RNN은 특히 Vanishing Gradient Problem이라고 하는 것 때문에 교육 과정 중에 중요한 도전에 직면합니다. 이 문제는 RNN을 더 많이 펼치면서 더욱 악화되며, 결국 교육 과정을 복잡하게 만듭니다.\n\n이상적인 세상에서는 RNN이 현재 단계 입력과 이전 단계의 입력을 모두 동등하게 고려하는 것을 원합니다: \n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_55.png)\n\n<div class=\"content-ad\"></div>\n\n그러나 실제로는 다음과 같이 보입니다:\n\n`<img src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_56.png\" />`\n\n각 단계는 약간 전 단계를 잊어버리는 경향이 있어서, 그 결과 단기 기억 문제인 사라지는 기울기 문제가 발생합니다. RNN이 더 많은 단계를 처리할수록, 이전 단계에서의 정보를 유지하는 데 어려움을 겪을 수 있습니다.\n\n입력이 세 개인 경우에는 이 문제가 그리 두드러지지 않습니다. 그렇다면 입력이 여섯 개인 경우는 어떨까요?\n\n<div class=\"content-ad\"></div>\n\n첫 두 단계에서의 정보가 마지막 단계에서는 거의 없음을 발견했습니다. 이는 중요한 문제입니다.\n\n다음은 텍스트 완성 작업을 사용하여 이 점을 설명하는 예시입니다. 이 문장을 완성하는 것에 성공할 수 있는 RNN이 있습니다.\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_57.png)\n\n그러나 단어가 추가될수록 RNN은 다음 단어를 정확하게 예측하기 어려워 질 수 있습니다. 처음 단어에서 예측해야 할 단어 사이의 거리가 늘어나면서 RNN이 초기 단어들이 제공하는 문맥을 잊어버리기도 할 수 있기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_58.png\" />\n\n이것은 RNN이 이론상으로 훌륭해 보이지만 실제로는 종종 부족하다는 사실을 강조합니다. 단기 기억 문제를 해결하기 위해 우리는 Long Short-Term Memory (LSTM) 네트워크라고 불리는 특수 유형의 신경망을 사용합니다. 하지만 그것은 다음 파트에서 다루도록 하겠습니다. 그러니 기대해 주세요!\n\n# 보너스: Softmax 활성화 함수\n\n이전에 우리는 감성 예측을 다루는 다른, 훨씬 더 나은 방법에 대해 이야기했습니다. 출력 뉴런에 대한 활성화 함수를 결정했을 때로 돌아가보겠습니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n![Deep Learning Illustrated](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_59.png)\n\n하지만 이번에는 조금 다른 것에 집중합니다. 순환 요소를 제외하고 기본 신경망에 초점을 맞춰 봅시다. 이제 우리의 목표는 무엇일까요? 영화 리뷰 전체가 아닌 단일 입력 단어의 감성을 예측하는 것입니다.\n\n이전에, 우리의 예측 모델은 입력이 양수일 확률을 출력하도록 목표로 했습니다. 이를 위해 출력 뉴런에서 시그모이드 활성화 함수를 사용하여 이를 성취했습니다. 이 함수는 긍정적 감정의 가능성에 대한 확률 값을 생성합니다. 예를 들어, \"terrible\"라는 단어를 입력하면, 우리 모델은 이상적으로 긍정적인 가능성이 낮음을 나타내는 낮은 값을 출력할 것입니다.\n\n![Deep Learning Illustrated](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_60.png)\n\n\n<div class=\"content-ad\"></div>\n\n그러나 다시 한번 생각해보면, 이 결과가 크게 만족스럽지는 않습니다. 긍정적 감정의 낮은 확률은 반드시 부정적이라는 것을 의미하는 것은 아닙니다. 입력이 중립적이었을 수도 있습니다. 그렇다면 이를 어떻게 개선할 수 있을까요?\n\n다음을 고려해보세요. 만약 영화 리뷰가 긍정적인지, 중립적인지, 부정적인지 알고 싶다면 어떻게 될까요?\n\n그래서, 입력이 긍정적인지 예측하는 확률을 반환하는 하나의 출력 뉴런 대신, 세 개의 출력 뉴런을 사용할 수 있습니다. 각각이 리뷰가 긍정적, 중립적, 부정적일 확률을 예측할 것입니다.\n\n![이미지](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_61.png)\n\n<div class=\"content-ad\"></div>\n\n현재 네트워크의 각 뉴런에 시그모이드 함수를 적용하여, 확률을 출력하기 위해 단일 출력 뉴런 네트워크에서 사용한 것과 동일한 원리를 적용할 수 있습니다.\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_62.png)\n\n각 뉴런은 각자에게 해당하는 확률 값을 출력할 것입니다:\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_63.png)\n\n<div class=\"content-ad\"></div>\n\n그러나 문제가 있습니다: 확률이 올바르게 합산되지 않습니다 (0.1 + 0.2 + 0.85 != 1) 그래서 이것은 그다지 좋은 해결책이 아닙니다. 출력 뉴런 모두에 시그모이드 함수를 간단히 적용하는 것으로는 문제를 해결할 수 없습니다. 세 개의 출력 간에 이러한 확률을 정규화하는 방법을 찾아야 합니다.\n\n여기서 우리의 무기에 강력한 활성화 함수를 소개합니다 — 소프트맥스 활성화. 소프트맥스 활성화 함수를 사용하면 우리의 신경망은 새로운 모습을 취합니다:\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_64.png)\n\n처음에는 복잡해 보일 수 있지만, 소프트맥스 함수는 실제로 매우 직관적입니다. 간단히 말해 출력 뉴런에서 출력 값 (y_hat)을 가져와 정규화합니다.\n\n<div class=\"content-ad\"></div>\n\n그러나 이 세 개의 출력 뉴런에 대해서는 어떤 활성화 함수도 사용하지 않아야 한다는 것이 중요합니다. 출력 (y_hats)은 합산 단계 직후에 직접 얻는 결과가 될 것입니다.\n\n우리는 이 y_hat 출력을 소프트맥스 공식을 통해 정규화합니다. 이 공식은 긍정적인 감정의 확률 예측을 제공해줍니다:\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_65.png)\n\n마찬가지로, 부정적인 결과와 중립 결과의 확률 예측을 얻을 수도 있습니다:\n\n<div class=\"content-ad\"></div>\n\n아래와 같이 작동해 봅시다. 예를 들어, \"terrible\"가 입력으로 주어지면 다음과 같은 y_hat 값들이 나타날 것입니다:\n\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_66.png)\n\n\n그런 다음이 값을 가져와 softmax 공식에 대입하여 \"terrible\"라는 단어가 긍정적인 함축을 가질 확률을 계산할 수 있습니다.\n\n\n![image](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_67.png)\n\n\n<div class=\"content-ad\"></div>\n\n이 의미는 감성 뉴런에서 출력된 결합된 결과를 사용하면 \"terrible\"이 긍정적인 감정을 가진다는 확률이 0.05인 것을 의미합니다.\n\n만약 입력이 중립적인지의 확률을 계산하려면 비슷한 공식을 사용하되 분자를 바꿔야 합니다. 따라서, \"terrible\"이 중립적인지의 가능성은 다음과 같습니다:\n\n![probability formula](/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_68.png)\n\n그리고 \"terrible\"이 부정적이라고 예측되는 확률은:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_69.png\" />\n\n그러면 뭐, 이제 확률이 1에 합쳐져서 모델이 더 설명 가능하고 논리적입니다.\n\n그래서 우리가 신경망에게 \"“terrible”라는 단어가 부정적인 감정이 있는 확률이 얼마나 되냐?\"고 물으면, 꽤 직관적인 답을 받게 됩니다. \"terrible\"이라는 단어가 부정적인 감정이라는 것에 대해 확률적으로 85%가 있다고 자신있게 말합니다. 이것이 바로 softmax 활성화 함수의 매력입니다!\n\n오늘은 여기까지! 우리는 순환 신경망과 소프트맥스 활성화 함수라는 두 가지 큰 주제를 다뤘습니다. 이것들은 나중에 더 깊게 다룰 많은 고급 개념들의 기초입니다. 그러니 시간을 내어 천천히 생각해보시고, 계속 질문하거나 의견을 나누고 싶다면 언제든지 LinkedIn에서 연락 주시거나 shreya.statistics@gmail.com으로 이메일 보내 주세요!","ogImage":{"url":"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_0.png"},"coverImage":"/assets/img/2024-06-19-DeepLearningIllustratedPart4RecurrentNeuralNetworks_0.png","tag":["Tech"],"readingTime":17},{"title":"LLM을 위한 지시어 파인 튜닝에 대한 포괄적인 소개","description":"","date":"2024-06-19 03:09","slug":"2024-06-19-AComprehensiveIntroductiontoInstructionFine-TuningforLLMs","content":"\n\n지시 튜닝은 큰 언어 모델(LLM)의 능력을 특정 지시를 따르도록 개선하기 위해 사용되는 과정입니다. InstructGPT의 작업은 먼저 지시 미세 조정에 대한 작업을 소개했습니다.\n\nInstructGPT는 인간 지시를 더 잘 따르도록 GPT-3를 미세 조정하여 학습되었습니다. 인간이 모델의 응답을 평가한 데이터셋에서 GPT-3를 조정하는 것은 ChatGPT를 만드는 방향으로 큰 발전이었습니다.\n\n이 문서에서는 기존 LLM의 성능을 향상시키기 위해 지시 미세 조정하는 과정과 결과를 배우게 됩니다. 또한 미세 조정한 LLM의 성능을 평가하고 기본 모델과의 개선 정도를 양적으로 측정할 수 있는 중요한 지표에 대해 알게 될 것입니다.\n\n![image](/assets/img/2024-06-19-AComprehensiveIntroductiontoInstructionFine-TuningforLLMs_0.png)\n\n<div class=\"content-ad\"></div>\n\n## 목차:\n\n- 지시사항 프롬프트를 사용한 LLMs 세밀조정\n- 세밀조정 과정\n- 지시 데이터 세트 준비\n- 지시 세밀조정 프로세스\n- 평가 및 성능 측정 지표\n\n# 1. 지시 프롬프트를 활용한 LLMs 세밀조정\n\n큰 LLMs 및 GPT3와 같은 기본 모델들은 프롬프트에 포함된 지시사항을 식별하고 올바르게 zero-shot 추론을 수행할 수 있지만, 더 작은 LLMs와 같은 다른 모델들은 작업을 수행하지 못할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, \"불어로 이 문장을 번역해주세요: '안녕, 어떻게 지내세요?'\"라는 지시를 받으면 능력있는 LLM은 비슷한 번역 예시를 볼 필요 없이 올바른 번역 \"Bonjour, comment ça va?\"를 생성할 수 있습니다.\n\n그러나 더 작은 LLM들이나 덜 포괄적인 훈련 데이터를 갖고 있는 LLM들은 또는 더 복잡한 작업에 대해서 적절한 작업을 제대로 수행하기 위해 안내 없이 어려움을 겪을 수 있습니다. 이를 해결하기 위해 one-shot 및 few-shot 추론 기술이 사용되는데, 여기서 한 번 또는 몇 가지 예제가 모델이 작업을 이해하는 데 도움이 됩니다.\n\n예시: One-Shot 추론\n\n- 프롬프트: \"이 문장을 독일어로 번역해주세요: '좋은 아침.' 예시: '어떻게 지내세요?' - ` 'Wie geht es dir?'\"\"\n- 모델 출력: \"Guten Morgen.\"\n\n<div class=\"content-ad\"></div>\n\n여기서 모델은 제공된 예시를 사용하여 \"Good morning\"을 올바르게 번역하는 방법을 추론합니다.\n\n예시: Few-Shot 추론\n\n- 프롬프트: \"다음 문장을 프랑스어로 번역하십시오: '안녕히 가세요.' 예시: '안녕' -` 'Bonjour'. '고맙습니다' -` 'Merci'.\"\n- 모델 출력: \"Au revoir.\"\n\n몇 가지 예시를 제공함으로써, 모델은 번역 작업에 대한 더 나은 이해를 얻고 정확한 결과를 내놓습니다. 파인 튜닝은 LLM의 가중치를 업데이트하기 위해 레이블이 지정된 예시를 사용하여 기본 모델을 더 학습시키는 솔루션을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n# 2. 지시 사항 세부 조정 과정\n\n이전에 대비해서, 선행 교육에서는 LLM을 자기 지도 학습을 통해 거대한 양의 비구조화된 텍스트 데이터를 사용하여 훈련했지만, 지시 사항 세부 조정은 레이블이 지정된 예제 데이터 세트를 사용하여 LLM의 가중치를 업데이트하는 지도 학습 과정입니다.\n\n레이블이 지정된 예제는 프롬프트-완료 쌍이며, 세부 조정 프로세스는 모델의 훈련을 확장하여 특정 작업에 대한 좋은 완료를 생성할 수 있는 능력을 향상시킵니다.\n\n다양한 작업에 대한 지시 사항 세부 조정 예시:\n\n<div class=\"content-ad\"></div>\n\n- 텍스트 분류:\n\n   - 작업: 영화 리뷰의 감정 분류\n   - 프롬프트: \"이 리뷰의 감정을 분류하세요: '나는 이 영화를 정말 좋아했어요! 처음부터 끝까지 멋있었어요.'\"\n   - 완료: \"감정: 긍정적\"\n\n2. 텍스트 요약:\n\n   - 작업: 기사 요약\n   - 프롬프트: \"다음 기사를 요약하세요: '주식 시장은 역대급 성장을 보여주었으며, 주요 지수가 사상 최고치를 기록했습니다. 투자자들은 경제의 회복에 대해 낙관적입니다.'\"\n   - 완료: \"요약: 경제 회복에 대한 낙관주의 속에서 주식 시장이 사상 최고치를 경신했습니다.\"\n\n<div class=\"content-ad\"></div>\n\n3. 번역:\n\n- 작업: 영어 문장을 프랑스어로 번역하십시오.\n- 프롬프트: “다음 문장을 프랑스어로 번역하십시오: ‘The weather is nice today.’”\n- 완료: “Le temps est agréable aujourd’hui.”\n\n4. 질의응답:\n\n- 작업: 주어진 텍스트를 기반으로 질문에 답하십시오.\n- 프롬프트: “다음 글을 읽고 질문에 답하십시오: ‘중국의 만리장성은 세계에서 가장 유명한 구조물 중 하나입니다. 침입으로부터 보호하기 위해 건설되었습니다.’ 질문: 만리장성은 왜 지어졌습니까?”\n- 완료: “만리장성은 침입으로부터 보호하기 위해 건설되었습니다.”\n\n<div class=\"content-ad\"></div>\n\n5. Named Entity Recognition (NER):\n\n- Task: 명명된 Entity(개체)를 식별하고 분류합니다. 예를 들어, 사람, 조직, 위치 등\n- 지시문: \"다음 문장에서 Entity(개체)를 식별하고 분류하세요: 'Barack Obama was born in Hawaii and served as the President of the United States.'\"\n- 완성: \"Barack Obama: 사람, Hawaii: 위치, President of the United States: 직책\"\n\n지시서 파인튜닝은 특정 지시에 대한 모델의 반응을 보여주는 예시를 사용하여 다양한 작업에서 모델의 성능을 향상시키는 데 특히 좋습니다. 지시서 파인튜닝의 가장 중요한 장점 중 세 가지는 다음과 같습니다:\n\n- 작업별 전문 지식: 레이블이 지정된 예시를 통해 특정 작업에 대해 직접 학습함으로써 모델이 해당 작업에서 높은 능숙도를 갖게 됩니다.\n- 향상된 정확도: 파인튜닝은 모델이 훈련된 작업에 대한 정확도를 크게 향상시키며 명확한 지침과 예시를 통해 학습합니다.\n- 문맥 처리 효율: 파인튜닝 후에는 프롬프트 내에서 여러 예시를 요구하지 않아도 되므로, 문맥 창에서 다른 관련 정보를 위한 공간을 절약할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 3. 지시 데이터 세트 준비\n\n지시 미세 조정 작업의 한 가지 어려움은 많은 공개 데이터 세트가 내용은 풍부하지만 지시 프롬프트로 사용하기에 적합하게 구조화되어 있지 않다는 것입니다. 예를 들어, 언어 모델 사전 훈련에 사용되는 데이터 세트는 특정 지시나 프롬프트 없이 원시 텍스트 단락으로 구성될 수 있습니다.\n\n이러한 어려움을 해결하기 위해 연구원들과 개발자들은 지시 프롬프트 데이터 세트로 변환하기 위해 기존 데이터 세트를 변환하는 미리 정의된 템플릿을 포함하는 라이브러리와 도구를 선별해 왔습니다. 예를 들어, 지시 프롬프트 라이브러리 및 예시를 포함한 Template Libraries, 예를 들면:\n\n- Hugging Face의 NLP Datasets: Hugging Face는 자연어 처리 (NLP) 데이터 세트의 방대한 컬렉션을 제공하며, 이 중 많은 데이터 세트에 미리 정의된 프롬프트 템플릿이 함께 제공됩니다. 이 템플릿을 사용하면 사용자가 원시 데이터 세트를 지시 기반 프롬프트 형식으로 변환할 수 있습니다.\n- OpenAI의 GPT Prompt Engineering: OpenAI는 지시 엔지니어링을 위한 자원과 도구를 제공하며, 특정 작업에 맞춘 프롬프트 라이브러리를 포함합니다. 이러한 라이브러리는 분류, 텍스트 생성 및 요약과 같은 작업을 위한 사용 준비가 완료된 템플릿을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n이 라이브러리와 도구를 사용하면 지시용 데이터 세트를 만들 수 있습니다:\n\n- Amazon 제품 리뷰 데이터 세트: 개발자는 Amazon 제품 리뷰 데이터 세트를 활용하여 언어 모델을 감정 분석이나 제품 분류를 위해 세밀하게 조정할 수 있습니다. \"이 리뷰의 감정을 분류하십시오\" 또는 \"제품 평가를 예측하십시오\"와 같은 프롬프트 템플릿을 적용하여, 개발자는 원시 리뷰를 세부 조정을 위한 지시 프롬프트로 변환할 수 있습니다.\n- Stanford Sentiment Treebank (SST): SST는 감정(긍정적 또는 부정적)으로 분류된 영화 리뷰를 포함하는 데이터셋입니다. 적절한 프롬프트 템플릿을 사용하면 연구자들은 SST를 감정 분석 세밀 조정 작업용 지시 프롬프트 데이터 세트로 변환할 수 있습니다.\n- CNN/Daily Mail 데이터 세트: 이 데이터 세트는 뉴스 기사와 글머리 기사 요약이 짝지어진 것입니다. \"이 기사에 대한 요약 생성\"과 같은 프롬프트 템플릿을 활용하여 개발자는 텍스트 요약 세밀 조정용 지시 데이터 세트를 준비할 수 있습니다.\n- WMT 번역 작업 데이터 세트: WMT(기계 번역 워크샵)은 기계 번역 모델을 훈련하기 위한 데이터 세트를 제공합니다. \"이 문장을 프랑스어로 번역하십시오\"와 같은 프롬프트 템플릿을 사용하여 연구자는 번역 세밀 조정 작업용 지시 프롬프트를 생성할 수 있습니다.\n\n# 4. 지시 세밀 조정 프로세스\n\n지시 데이터 세트를 준비했다면, 이를 훈련, 검증 및 테스트 세트로 나눕니다. 세밀 조정 중에는 훈련 데이터 세트에서 프롬프트를 선택하고 LLM에 전달하여 완성본을 생성합니다.\n\n<div class=\"content-ad\"></div>\n\nLLM 완성 결과를 교육 데이터에 지정된 응답과 비교하여 표준 교차 엔트로피 함수를 사용하여 손실을 계산하고 역전파를 통해 모델 가중치를 업데이트하십시오.\n\n모델의 성능을 향상시키기 위해 여러 배치의 프롬프트-완성 쌍을 여러 번의 epoch 동안 반복합니다.\n\n# 5. 평가 및 성능 지표\n\n표준 지도 학습과 마찬가지로, 보유 검증 데이터 세트를 사용하여 LLM 성능을 측정하는 별도의 평가 단계를 정의하여 검증 정확도를 얻습니다.\n\n<div class=\"content-ad\"></div>\n\n미세 조정을 완료한 후, 테스트 정확도를 얻기 위해 홀드아웃 테스트 데이터셋을 사용하여 최종 성능 평가를 수행하십시오. 이때, 이메일 보완보상평가(BLEU) 및 ROUGE(Recall-Oriented Understudy for Gisting Evaluation)은 긴 통역모델 (LLM) 지침 미세 조정을 평가하는 데 사용되는 인기있는 두 평가 지표 중 하나입니다.\n\n- BLEU (이중 언어 평가 보조) 스코어:\n\n- 정의: 한 언어에서 다른 언어로 기계 번역된 텍스트의 품질을 평가하기 위한 지표로, 이를 인간이 만든 번역과 비교합니다.\n- 예시: 번역 작업에서 모델이 \"The weather is nice today\"을 정확하게 \"Le temps est agréable aujourd’hui\"로 번역했을 때, 인간 번역과 비교하여 BLEU 스코어가 높게 나타납니다.\n\n2. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) 스코어:\n\n<div class=\"content-ad\"></div>\n\n- 정의: 자동 요약 및 기계 번역을 평가하는 메트릭스 세트입니다. 이는 모델 출력물과 참조 텍스트 간의 n-gram의 중첩을 측정합니다.\n- 예시: 요약 작업의 경우, 높은 ROUGE 점수는 모델이 생성한 요약과 인간이 작성한 요약 간에 높은 중첩이 있음을 나타냅니다.\n\n세밀 조정 과정은 기반 모델의 새 버전을 만들어내며 이를 보통 가르침 모델이라고 합니다. 이는 당신이 관심 있는 작업에 더 적합한 모델입니다.\n\n지시 프롬프트로 세밀 조정하는 것이 오늘날 LLM을 세밀 조정하는 가장 흔한 방법입니다. 본 문서에서는 이 중요한 주제에 대해 간략히 소개되었으니 이제 손을 더럽히고 LLM을 조금 만지작거리며 조정해보는 것이 시간입니다.\n\n## 만약 이 문서를 좋아하셨고 저를 지원하고 싶으시다면, 확인해주십시오:\n\n<div class=\"content-ad\"></div>\n\n- 👏 이 이야기에 박수를 보내주세요 (50번 클랩!) 이 기사가 주목받을 수 있도록 도와주세요\n- To Data & Beyond 뉴스레터를 구독해주세요\n- 제 Medium 계정을 팔로우해주세요\n- 📰 제 Medium 프로필에서 더 많은 콘텐츠를 확인해주세요\n- 🔔 팔로우하기: LinkedIn | Youtube | GitHub | Twitter\n\n## 제 뉴스레터 'To Data & Beyond'를 구독하여 제 글을 완전히 그리고 일찍 볼 수 있습니다:\n\n## 데이터 과학과 AI 분야에서 커리어를 시작하고 방향을 모를 때 도움이 필요하신가요? 저는 데이터 과학 멘토링 세션과 장기적 커리어 멘토링을 제공합니다:\n\n- 멘토링 세션: [링크](https://lnkd.in/dXeg3KPW)\n- 장기적 멘토링: [링크](https://lnkd.in/dtdUYBrM)\n\n<div class=\"content-ad\"></div>\n\n![Image](/assets/img/2024-06-19-AComprehensiveIntroductiontoInstructionFine-TuningforLLMs_1.png)","ogImage":{"url":"/assets/img/2024-06-19-AComprehensiveIntroductiontoInstructionFine-TuningforLLMs_0.png"},"coverImage":"/assets/img/2024-06-19-AComprehensiveIntroductiontoInstructionFine-TuningforLLMs_0.png","tag":["Tech"],"readingTime":7},{"title":"작은 시간 혼합기 TTM IBM의 강력한 제로샷 예측 모델","description":"","date":"2024-06-19 03:07","slug":"2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM","content":"\n\n![이미지](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_0.png)\n\nLLM(Large Language Models)에 대한 최신 연구를 따라가보면 주로 두 가지 주요 접근 방식을 볼 수 있어요:\n\n첫째, 연구자들은 가능한 가장 큰 모델을 구축하는 데 주력합니다. 단어 예측을 통한 사전 학습은 성능 향상에 중요한 역할을 합니다(그리고 수백만 달러가 소비되는 곳이기도 합니다!).\n\n둘째, 연구자들은 양자화와 같은 기술을 사용하여 작고 빠른 모델을 만들어냅니다 — 강력한 일반적인 성능을 유지하면서요.\n\n<div class=\"content-ad\"></div>\n\n그러나 일부 작업에서 더 작은 모델이 훨씬 큰 모델보다 더 나은 성과를 내는 흥미로운 일이 발생합니다. 예를 들어, Llama 3-8B는 MMLU 작업에서 더 큰 Llama 2-70B보다 우수한 성과를 냈습니다!\n\nIBM에서 소개한 Tiny Time Mixers (TTM)[1]은 두 번째 접근 방식을 따릅니다. 더 큰 SOTA 모델 — MOIRAI를 포함하여 —을 능가하는 가벼운 모델로, M4 데이터셋에서 우수한 성과를 거둡니다. 게다가, 이는 오픈 소스입니다!\n\n이 기사에서는 다음을 논의합니다:\n\n- TTM의 아키텍처 및 기능.\n- TTM을 특별하게 만드는 혁신적인 기능.\n- 다른 모델과의 벤치마킹 결과를 비교한 결과.\n\n<div class=\"content-ad\"></div>\n\n시작해요!\n\n# Enter Tiny Time Mixer (TTM)\n\nTTM의 주요 특징은 다음과 같습니다:\n\n- Non-Transformer Architecture: TTM은 Attention 메커니즘을 사용하지 않기 때문에 매우 빠릅니다. 완전 연결된 NN 계층만 사용합니다.\n- TSMixer Foundation: TTM은 아키텍처에서 TSMixer[2] (IBM의 혁신적인 시계열 모델)을 활용합니다.\n- 다양한 입력: 다변량 예측이 가능한 TTM은 추가 채널, 외부 변수 및 알려진 미래 입력을 수용하여 예측 다양성을 향상시킵니다.\n- 빠르고 강력함: TTM은 Monash 데이터 세트의 244백만개 샘플로 사전 훈련되었으며, 6대의 A100 GPU를 사용하여 8시간 이내에 훈련되었습니다.\n- 우수한 제로샷 예측: TTM은 사전 훈련되어 있으며, 미처 본 적 없는 데이터에 대한 우수한 제로샷 예측을 수행하여 큰 SOTA 모델을 능가합니다.\n\n<div class=\"content-ad\"></div>\n\n중요한 사항:\n\n# TTM 혁신\n\nTTM은 여러 혁신적인 기능을 소개합니다:\n\n- 다중 수준 모델링: TTM은 먼저 채널 독립적 방식(일변량 시퀀스)으로 사전 훈련을 받은 후, 세밀 조정 중에 여러 변수 종속성을 학습하기 위해 교차 채널 혼합을 사용합니다.\n- 적응형 패치 적용: 단일 패치 길이 대신 TTM은 서로 다른 레이어 간에 여러 패치 길이를 학습합니다. 각 시계열이 특정 패치 길이에서 최적으로 작동하기 때문에 적응형 패치는 모델이 다양한 데이터에 대해 더 잘 일반화되도록 도와줍니다.\n- 해상도 접두사 튜닝: 다른 주파수(예: 주간, 일별 데이터)는 전통적인 시계열 모델에 어려운 부분입니다. TTM은 시계열 주파수를 인코딩하기 위한 추가 임베딩 레이어를 사용하여 모델이 신호의 주파수에 따라 정확하게 예측을 조건부로 설정할 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\n# Tiny Time Mixers — 아키텍처\n\nTSMixer은 TTM의 전신입니다. TSMixer는 견고한 모델이지만, 기본 모델로 사용하거나 외부 변수를 처리하는 데 사용할 수는 없습니다.\n\nTTM은 TSMixer를 구성 요소로 사용하여 새로운 기능을 도입함으로써, 저자들이 보지 못한 데이터에 대해 일반화된 비-트랜스포머 모델을 만들었습니다.\n\nTTM의 아키텍처는 그림 1에 나와 있습니다. 우리는 두 단계, 사전 훈련(왼쪽)과 파인튜닝(오른쪽)에 대해 설명할 것입니다:\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_1.png)\n\n**의미론적 지식**: sl=context_size, fl=forecasting_length, c = 입력 기능의 채널 수, c’= 예측 채널의 수.\n\n## 사전 훈련\n\n- 사전 훈련 중에는 모델이 단변량 시계열로만 학습됩니다.\n- 먼저 개별 시계열을 정규화합니다. 마지막 출력은 역정규화됩니다 (표준적인 방법).\n- 패칭은 시계열에서 널리 성공한 기술이며 여기서도 사용됩니다. 단변량 시퀀스를 크기가 pl인 n 패치로 나눕니다.\n- TTM 백본 모듈은 적응형 패칭을 적용하고 패치를 크기 p에서 hf로 사상합니다. TTM 백본은 TTM의 핵심이며 나중에 자세히 설명하겠습니다.\n- TTM 디코더는 TTM 백본과 동일한 아키텍처를 갖고 있지만 훨씬 작아서 매개변수가 80% 적습니다.\n- 예측 선형 헤드에는 1개의 완전 연결 계층이 있으며 최종 예측을 생성합니다 (그런 다음 역정규화됨).\n- MSE 손실은 예측 기간 fl 동안 계산됩니다.\n\n<div class=\"content-ad\"></div>\n\n## Feat-Tuning\n\n- 여기서는 TTM 백본이 동결되어 있고 TTM 디코더 및 Forecast 선형 헤드의 가중치만 업데이트됩니다.\n- 우리는 소수 데이터만으로 학습하는 후속 예측(few-shot forecasting) 또는 전체 데이터셋을 사용하는 후속 예측(full-shot forecasting)을 수행할 수 있습니다.\n- Feat-Tuning 단계에서는 다변량 데이터셋을 사용할 수 있습니다. 이 경우 TTM 디코더에서 채널 혼합이 활성화됩니다.\n- 선택적으로, 미래의 알려진 변수를 모델링하기 위해 외생 혼합 블록(그림 1에 나와 있음)을 활성화할 수도 있습니다.\n\n# TTM 백본\n\nTTM의 핵심 구성 요소는 TTM 백본입니다. 이는 Resolution Prefix Tuning과 Adaptive Patching을 가능하게 합니다.\n\n<div class=\"content-ad\"></div>\n\n이 컴포넌트를 자세히 살펴보자면 그 기능을 이해할 수 있어요 (그림 2에 표시됨):\n\n![이미지](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_2.png)\n\n- 임베딩 레이어는 크기 pl에서 패치를 투영하여 크기 hf의 입력 임베딩을 만든답니다.\n- Resolution Prefix Tuning 모듈은 시간-주파수/해상도를 나타내는 hf 크기의 임베딩을 만들고 이를 입력 임베딩에 연결합니다 (그림 2의 n=n+1 연산을 주목해주세요).\n- TTM 블록은 3개의 하위 모듈을 포함합니다: 패치 분할 모듈, 베니라 TSMixer 블록 및 패치 병합 블록:\n- 패치 분할 모듈은 패치 수를 K만큼 증가시키고 패치 길이를 다시 K만큼 감소시킵니다. 예를 들어, 첫 번째 수준에서 크기 [c,n, hf]의 입력은 [c, 4*n, hf//4]로 변화합니다.\n- TSMixer 블록이 변환된 입력에 적용되며 패치 병합 블록이 [c, 4*n, hf//4] 입력을 다시 [c,n, hf]로 변형합니다.\n\n# 외부 믹서\n\n<div class=\"content-ad\"></div>\n\n미래의 알려진 변수가 있는 경우, Exogenous Mixer를 활성화할 수 있습니다. 이 모듈은 Figure 3에 표시되어 있으며, TTM 아키텍처에서의 위치는 Figure 1에 표시되어 있습니다:\n\n![Exogenous Mixer](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_3.png)\n\nExogenous Mixer 블록은 간단합니다: 시계열의 미래 값(y3와 y4; Figure 3, 녹색)이 알려진 경우, 이를 사용하여 대상 변수(y1과 y2; Figure 4, 보라색)의 예측을 안내합니다.\n\n# TTM 교육 세부 정보 및 데이터세트\n\n<div class=\"content-ad\"></div>\n\n저자들은 다양한 문맥과 예측 길이에 대해 5가지 TTM 버전을 만들었습니다. 이는 (512,96), (512,192), (512, 336), (512,720), (96,24) 입니다.\n\n교육에 관해서, 저자들은 모델 사전 훈련을 위해 Monash 데이터베이스의 하위 집합(244k 샘플)을 사용했고, 파인튜닝 성능을 평가하기 위해 Informer 데이터셋을 사용했습니다. 또한, 저자들은 외부 혼합기 블록의 효과를 평가하고 알려진 미래 변수를 추가함으로써 성능이 얼마나 향상되는지 조사하기 위해 다른 데이터셋을 사용했습니다.\n\n이러한 데이터셋에 대해 더 자세한 내용은 원본 논문에서 확인할 수 있습니다. 아래는 (512,96) 변형을 위한 교육 하이퍼파라미터입니다:\n\n- pl(패치 길이) = 64\n- 백본 수준 수 = 6\n- 각 수준 당 TTM 블록 수 = 2\n- 배치 크기 = 3천\n- 에폭 = 20\n\n<div class=\"content-ad\"></div>\n\n학습 및 파인 튜닝 구성에 대한 자세한 내용은 원 논문을 참조해 주세요.\n\n# 평가 벤치마크\n\nTTM 대 최신 기법 모델\n\n그 다음, 저자들은 Zero-shot 및 5% Few-shot 버전의 TTM을 다른 최신 기법 모델과 비교했습니다. 이때 사용된 평가 메트릭은 MSE였습니다. 결과는 다음과 같은 표 1에서 확인할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n아래는 너무 인상적입니다:\n\n평균적으로, Few-shot TTM이 모든 다른 모델을 능가했습니다. 심지어 Zero-shot TTM이 일부 모델을 능가할 수 있었습니다! 기억하세요, Zero-shot TTM은 이러한 데이터에 대해 학습을 받지 않고 예측을 생성합니다.\n\n또한 TTM은 작년에 소개된 새로운 기반 시계열 모델인 GPT4TS를 앞서 나갔습니다.\n\n<div class=\"content-ad\"></div>\n\nTMT 이외에도 다음으로 높은 순위의 모델은 GPT4TS, PatchTST 및 TSMixer입니다. 모두 패치(patching)를 활용합니다. 최근 시계열 예측 연구에서 패치(patching)가 매우 유익한 기술임이 입증되었습니다.\n\nTTM 대 foundation 모델\n\n저자들은 TTM을 독립적으로 평가하며 특히 GPT4TS와 LLMTime과 비교합니다.\n- LLMTime은 GPT-3과 LLaMa-2를 사용하여 시계열 예측을 위해 특정 수정을 가한 모델입니다.\n- GPT4TS는 다양한 작업(예측, 분류 등)을 위해 범용 시계열 모델이며, 기본 모델로 GPT-2를 사용합니다.\n\n<div class=\"content-ad\"></div>\n\n비교 결과는 표시됩니다. Table 2 (LLMTime)와 Table 3 (GPT4TS):\n\n![LLMTime](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_5.png)\n\n![GPT4TS](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_6.png)\n\nLLMTime은 제로샷 예측 시나리오에서 평가되었고, GPT4TS는 퓨샷 예측기로 동작했습니다.\n\n<div class=\"content-ad\"></div>\n\n- 두 가지 비교에서 TTM이 명확한 승자입니다.\n- 게다가 TTM은 훨씬 빠르며 리소스를 상당히 적게 필요로 합니다. 이는 TTM이 GPT4TS와 같은 무거운 트랜스포머 계산을 사용하지 않기 때문에 예상된 결과입니다.\n\n## 외생 변수의 효과성\n\n현대 실제 세계 데이터셋은 가능한 경우 외생 변수를 사용하므로 예측 애플리케이션에서 이를 활용하는 것이 합리적입니다.\n\nTTM의 저자들은 이와 같은 변수를 사용함으로써 TTM이 어떻게 향상되는지 조사했습니다 (해당하는 경우). 구체적으로 제로샷 TTM, 일반 TTM 및 외생 변수를 사용하는 채널 혼합 (TTM-CM)을 비교했습니다.\n\n<div class=\"content-ad\"></div>\n\n그들은 TSMixer와 그 채널 혼합 변형을 평가했습니다. 결과는 다음과 같이 Table 4에 표시되어 있습니다:\n\n![Table 4](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_7.png)\n\n다시 한 번, 결과는 매우 흥미로워요: 먼저, TTM-CM이 1위를 차지하여 외생 변수가 모델에 도움이 되는 것을 의미합니다.\n\n채널 혼합 속성을 사용하는 TSMixer 변형은 2위를 차지했습니다. 또한, 제로-샷 TTM이 최악의 성능을 보입니다. 보조 변수가 있는 경우 모델 성능을 향상시키는 데 사용되어야 함이 명백합니다.\n\n<div class=\"content-ad\"></div>\n\n# Tiny Time Mixers 실무 활용\n\n모델 버전 512-96과 1024-96의 가중치를 HuggingFace에서 다운로드하여 다음과 같이 세밀 조정할 수 있습니다:\n\n```js\n!git clone https://github.com/IBM/tsfm.git\n!pip install transformers\n!pip install datasets\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tsfm_public.models.tinytimemixer.utils import (\n    count_parameters,\n    plot_preds,\n)\n\nfrom tsfm_public.models.tinytimemixer import TinyTimeMixerForPrediction\nfrom tsfm_public.toolkit.callbacks import TrackingCallback\n\nzeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\"ibm/TTM\", revision='main')\nfinetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\"ibm/TTM\", revision='main', head_dropout=0.0,dropout=0.0,loss=\"mse\")\n```\n\n따라서 transformers 라이브러리의 익숙한 Trainer 모듈을 사용하여 TTM을 세밀 조정할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nfinetune_forecast_trainer = Trainer(\nmodel=finetune_forecast_model,\nargs=finetune_forecast_args,\ntrain_dataset=train_dataset,\neval_dataset=valid_dataset,\ncallbacks=[early_stopping_callback, tracking_callback],\noptimizers=(optimizer, scheduler))\n\n# Fine tune\nfinetune_forecast_trainer.train()\n```\n\n<img src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_8.png\" />\n\npredictions_test = finetune_forecast_trainer.predict(test_dataset)\n\n이후에는 사적 데이터셋을 통해 예측을 받은 후 결과를 플롯합니다:\n\n<div class=\"content-ad\"></div>\n\n아래는 테이블 태그를 마크다운 형식으로 변경하도록 했습니다.\n\n\n<img src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_9.png\" />\n\n# 마무리 말씀\n\nTiny Time Mixer (TTM)은 다른 접근 방식을 따른 혁신적인 모델로서, 더 작지만 효율적인 모델들을 위한 길을 열어두었습니다.\n\n특히, TTM은 어텐션을 사용하지 않았고 여전히 강력한 시계열(Time Series) 기반 모델을 구축할 수 있다는 것을 입증했습니다.\n\n\n<div class=\"content-ad\"></div>\n\n최초로 MLP만 사용한 메타러닝 기능을 갖춘 시계열 모델은 N-BEATS와 N-HITS였어요. 이 트렌드가 어떻게 이어지는지 한번 살펴봐요.\n\n최근에는 NLP 모델에서도 이러한 트렌드를 관측하고 있어요. 우리는 Mamba(State Space) xLSTM(기반 RNN)과 Hyena(CNN 기반)을 보았는데, 이들은 언어 모델이지만 트랜스포머는 아니며 다양한 벤치마크에서 인상적인 결과를 얻고 있어요.\n\n시계열 모델에 대한 이런 접근 방식이 어떻게 전개될지도 한번 살펴봅시다. 결국, 시계열에 대한 기초 모델 연구는 아직 새로운 상황이에요!\n\n# 읽어주셔서 감사합니다!\n\n<div class=\"content-ad\"></div>\n\n- 제 LinkedIn 팔로우해 주세요!\n- 제 뉴스레터, AI Horizon Forecast를 구독해 주세요!\n\n## 참고 자료\n\n[1] Ekambaram 등, Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series (2024년 4월)\n\n[2] Ekambaram 등, TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting (2023년 6월)","ogImage":{"url":"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_0.png"},"coverImage":"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_0.png","tag":["Tech"],"readingTime":9},{"title":"MLX로부터 GPT를 처음부터 만들어보기","description":"","date":"2024-06-19 03:00","slug":"2024-06-19-GPTfromScratchwithMLX","content":"\n\n## MacBook에서 GPT-2 정의 및 훈련하기\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_0.png)\n\n이 게시물의 목표는 MLX, Apple 실리콘을 위한 Apple의 기계 학습 라이브러리를 사용하여 GPT-2를 처음부터 정의하고 훈련하는 과정을 안내하는 것입니다. 토크나이저에서 샘플링까지 모든 과정을 상세히 다루고자 합니다. Karpathy의 훌륭한 GPT 처음부터 튜토리얼 영감을 받아, 우리는 Shakespeare의 작품에 대해 모델을 훈련할 것입니다. 우리는 비어 있는 Python 파일로 시작하여 Shakespeare 스타일 텍스트를 작성할 수 있는 소프트웨어로 끝낼 것입니다. 그리고 이 모든 것을 훨씬 빠르게 가능하게 하는 MLX에서 모두 구축할 것입니다.\n\n본 게시물은 따라하며 체험하는 것이 가장 좋습니다. 코드는 아래 리포지토리에 포함되어 있으며 이를 열어 참조하는 것을 권장합니다.\n\n<div class=\"content-ad\"></div>\n\n# 목차\n\n- 데이터 준비\n- GPT-2 코딩\n- 입력 임베딩\n- 위치 임베딩\n- 셀프 어텐션\n- 키, 쿼리 및 값\n- 멀티헤드 어텐션\n- MLP\n- 블록\n- 레이어 정규화 및 스킵 연결\n- 순방향 패스\n- 샘플링\n- 초기화\n- 훈련 루프\n- 참고 자료\n\n# 데이터 준비\n\nmlx를 설치하고 다음 임포트를 실행하십시오.\n\n<div class=\"content-ad\"></div>\n\n```js\nimport mlx.core as mx\nimport mlx.nn as nn\nimport mlx.optimizers as optim\nimport mlx.utils as utils\nimport numpy as np\nimport math\n```\n\nLLM 훈련의 첫 번째 단계는 큰 텍스트 데이터 코퍼스를 수집한 다음 토큰화하는 것입니다. 토큰화는 텍스트를 정수로 매핑하는 작업으로, LLM에 공급할 수 있습니다. 이 모델의 훈련 코퍼스는 셰익스피어의 작품들을 연결한 것입니다. 이는 대략 100만 글자이며 다음과 같습니다:\n\n```js\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n...\n```\n\n먼저 파일을 하나의 긴 문자열로 읽어 text 변수에 저장합니다. 그런 다음 set() 함수를 사용하여 텍스트에 있는 모든 고유한 문자를 얻어서 우리의 어휘가 됩니다. vocab을 출력하여 우리의 어휘에 있는 모든 문자를 하나의 문자열로 볼 수 있으며, 우리는 총 65개의 문자가 있어서 이것이 우리의 토큰이 될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n\n# 단어장 생성하기\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nvocab = sorted(list(set(text)))\nvocab_size = len(vocab)\n\nprint(''.join(vocab))\n# !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nprint(vocab_size)\n# 65\n\n\n생산 모델은 바이트 페어 인코딩과 같은 토큰화 알고리즘을 사용하여 하위 단어 청크의 더 큰 어휘를 생성할 것입니다. 오늘 우리의 초점은 아키텍처에 있기 때문에, 문자 수준의 토큰화를 계속할 것입니다. 다음으로, 단어장을 정수로 매핑하여 토큰 ID로 알려진 것으로 이동할 것입니다. 그런 다음 텍스트를 토큰으로 인코딩하고 문자열로 다시 디코딩할 수 있습니다.\n\n\n# 단어장을 정수로 매핑하기\nitos = {i:c for i,c in enumerate(vocab)} # int to string\nstoi = {c:i for i,c in enumerate(vocab)} # string to int\nencode = lambda x: [stoi[c] for c in x]\ndecode = lambda x: ''.join([itos[i] for i in x])\n\nprint(encode(\"hello world\"))\n# [46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\nprint(decode(encode(\"hello world\")))\n# hello world\n\n\n모든 문자 및 해당 어휘의 인덱스를 반복하여 숫자를 문자에 매핑하는 itos와 문자를 숫자에 매핑하는 stoi 사전을 생성하기 위해 enumerate() 함수를 사용합니다. 그런 다음 이러한 매핑을 사용하여 encode 및 decode 함수를 만듭니다. 이제 전체 텍스트를 인코딩하고 훈련 및 검증 데이터로 나눌 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\n데이터 = 인코딩(텍스트)\n분할 = int(0.9 * len(데이터))\n훈련_데이터 = 데이터[:분할]\n검증_데이터 = 데이터[분할:]\n```\n\n현재 훈련 데이터는 토큰들의 매우 긴 문자열입니다. 그러나 이전 토큰들이 주어졌을 때 다음 토큰을 예측하는 모델을 훈련하려고 합니다. 따라서 데이터셋은 입력이 토큰 문자열이고 레이블이 올바른 다음 토큰인 예제로 구성되어야 합니다. 다음 토큰을 예측하는 데 사용되는 최대 토큰 수인 context length라는 모델 매개변수를 정의해야 합니다. 훈련 예제는 우리의 context length의 길이가 될 것입니다.\n\n처음 ctx_len+1 개의 토큰을 살펴봅시다.\n\n```js\nctx_len = 8\nprint(훈련_데이터[:ctx_len + 1])\n# [18, 47, 56, 57, 58,  1, 15, 47, 58]\n# x: [18, 47, 56, 57, 58,  1, 15, 47] | y: 58\n```\n\n<div class=\"content-ad\"></div>\n\n이 예제는 인풋이 \"18, 47, 56, 57, 58, 1, 15, 47\"이고, 원하는 아웃풋이 \"58\"인 트레이닝 예제입니다. 이는 8 토큰의 컨텍스트를 가지고 있습니다. 그러나 생성 중에 필요한 7, 6, 5 ... 0개의 토큰만을 가지고 다음 토큰을 예측할 수 있도록 모델을 훈련시키고 싶습니다. 따라서 이 예제에 포함된 8개의 하위 예제를 고려합니다:\n\n```js\nctx_len = 8\nprint(train_data[:ctx_len + 1])\n# [18, 47, 56, 57, 58,  1, 15, 47, 58]\n# 8 sub examples\n# [18] --> 47\n# [18, 47] --> 56\n# [18, 47, 56] --> 57\n# [18, 47, 56, 57] --> 58\n# [18, 47, 56, 57, 58] --> 1\n# [18, 47, 56, 57, 58, 1] --> 15\n# [18, 47, 56, 57, 58, 1, 15] --> 47\n# [18, 47, 56, 57, 58, 1, 15, 47] --> 58\n```\n\n라벨은 간단히 왼쪽으로 이동한 인풋입니다.\n\n```js\nprint(\"inputs: \", train_data[:ctx_len])\nprint(\"labels: \", train_data[1:ctx_len+1]) # labels = inputs indexed 1 higher\n# inputs: [18, 47, 56, 57, 58,  1, 15, 47]\n# labels: [47, 56, 57, 58,  1, 15, 47, 58]\n```\n\n<div class=\"content-ad\"></div>\n\n인덱스 0에서 입력은 18이고 라벨은 47입니다. 인덱스 1에서 입력은 인덱스 1을 포함하여 그 이전 모든 것, 즉 [18, 47]이고 라벨은 56입니다. 등등. 이제 라벨이 입력 순서에서 한 단계 상위로 색인화됨을 이해했으므로 데이터셋을 구축할 수 있습니다.\n\n```js\n# 훈련 및 검증 데이터 세트 생성\nctx_len = 8\nX_train = mx.array([train_data[i:i+ctx_len] for i in range(0, len(train_data) - ctx_len, ctx_len)])\ny_train = mx.array([train_data[i+1:i+ctx_len+1] for i in range(0, len(train_data) - ctx_len, ctx_len)])\nX_val = mx.array([val_data[i:i+ctx_len] for i in range(0, len(val_data) - ctx_len, ctx_len)])\ny_val = mx.array([val_data[i+1:i+ctx_len+1] for i in range(0, len(val_data) - ctx_len, ctx_len)])\n```\n\n우리는 데이터를 반복하고 입력(X)으로 ctx_len 크기의 청크를 가져와서 같은 청크를 라벨(y)로 하나 높은 색인에서 가져옵니다. 그런 다음 이 Python 리스트를 mlx 배열 객체로 만듭니다. 모델 내부로 mlx를 사용할 것이므로 입력을 mlx 배열로 만들고 싶습니다.\n\n그리고 한 가지 더. 훈련 중에 모델에 한 번에 하나의 예제만 전달하고 싶지 않습니다. 효율성을 위해 병렬로 여러 예제를 한꺼번에 전달하고 싶습니다. 이 예제 그룹을 우리의 배치라고 하며, 그룹 내의 예제 수가 배치 크기입니다. 따라서 훈련용 배치를 생성하는 함수를 정의합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndef get_batches(X, y, b_size, shuffle=True):\n    if shuffle:\n        ix = np.arange(X.shape[0])\n        np.random.shuffle(ix)\n        ix = mx.array(ix)\n        X = X[ix]\n        y = y[ix]\n    for i in range(0, X.shape[0], b_size):\n        input = X[i:i+b_size]\n        label = y[i:i+b_size]\n        yield input, label\n```\n\n만약 shuffle=True라면, 데이터를 임의로 섞은 인덱스로 인덱싱하여 데이터를 섞습니다. 그런 다음 데이터 세트를 반복하고 입력 및 레이블 데이터 세트에서 배치 크기 청크를 반환합니다. 이 청크는 미니 배치로 알려져 있으며 병렬로 처리하는 예제를 쌓은 것입니다. 이러한 미니 배치는 모델 훈련 중에 우리의 입력이 될 것입니다.\n\n다음은 컨텍스트 길이가 8 인 4 개의 예제의 미니 배치 예제입니다.\n\n![2024-06-19-GPTfromScratchwithMLX_1.png](/assets/img/2024-06-19-GPTfromScratchwithMLX_1.png)\n\n\n<div class=\"content-ad\"></div>\n\n32 개의 다음 토큰 예측 문제가 포함된 미니배치입니다. 모델은 입력의 각 토큰에 대해 다음 토큰을 예측하고 레이블은 손실을 계산하는 데 사용됩니다. 입력의 각 색인에 대한 다음 토큰이 포함된 것을 주목해주세요.\n\n이 텐서들의 형태가 복잡해질 것을 염두에 두시면 좋겠어요. 지금은 일단, 우리가 모델에 (배치 크기, ctx_len) 모양의 텐서를 입력할 것이라는 것만 기억해주세요.\n\n# 코딩 GPT-2\n\nGPT-2 아키텍처를 살펴보고 구현하려는 것에 대한 개요를 파악해봅시다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_2.png)\n\n이게 혼란스러워 보이더라도 걱정하지 마세요. 우리는 바닥부터 꼭대기로 한 단계씩 구현할 거에요. 먼저 입력 임베딩을 구현하는 것부터 시작해봅시다.\n\n## 입력 임베딩\n\n입력 임베딩 레이어의 목적은 토큰 ID를 벡터로 매핑하는 것입니다. 각 토큰은 모델을 통해 전달될 때 그것에 대한 표현으로 사용될 벡터로 매핑됩니다. 각 토큰에 대한 벡터는 모델을 통해 전달되면서 정보를 축적 및 교환하고, 결국 다음 토큰을 예측하는 데 사용될 것입니다. 이러한 벡터들을 임베딩(embedding)이라고 합니다.\n\n<div class=\"content-ad\"></div>\n\n토큰 ID를 벡터로 매핑하는 가장 간단한 방법은 조회 테이블을 통해 할 수 있습니다. 각 토큰에 대한 임베딩 벡터가 있는 (vocab_size, n_emb) 크기의 행렬을 만듭니다. 이 행렬을 임베딩 가중치라고 합니다.\n\n![image](/assets/img/2024-06-19-GPTfromScratchwithMLX_3.png)\n\n다이어그램은 크기가 (65, 6)인 임베딩 레이어의 예시를 보여줍니다. 이는 어휘 사전에 65개의 토큰이 있고 각각이 길이가 6인 임베딩 벡터로 표현됨을 의미합니다. 입력된 시퀀스는 임베딩 가중치를 색인하여 각 토큰에 해당하는 벡터를 얻는 데 사용됩니다. 모델에 입력하는 미니배치를 기억하십니까? 원래 미니배치는 크기가 (batch_size, ctx_len)입니다. 임베딩 레이어를 통과한 후 크기는 (batch_size, ctx_len, n_emb)입니다. 각 토큰이 단일 정수가 아니라 길이가 n_emb인 벡터임을 의미합니다.\n\n이제 코드에서 임베딩 레이어를 정의해봅시다.\n\n<div class=\"content-ad\"></div>\n\n```python\nn_emb = 6 # 파일 맨 위에 이러한 하이퍼파라미터를 추가할 수 있어요\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb)\n```\n\n우리의 구현을 정리하기 위한 클래스를 정의할 거예요. mlx의 기능을 활용하기 위해 nn.Module을 서브클래스화할 거예요. 그럼 init 함수에서는 슈퍼클래스 생성자를 호출하고 wte라고 불리는 토큰 임베딩 레이어를 초기화할 거예요.\n\n## 위치 임베딩\n\n다음은 위치 임베딩이에요. 위치 임베딩의 목적은 시퀀스에서 각 토큰의 위치에 대한 정보를 인코딩하는 거예요. 이걸 우리의 입력 임베딩에 추가해서 각 토큰의 완전한 표현을 얻을 수 있어요. 그 표현에는 시퀀스에서 토큰의 위치에 대한 정보가 담겨있어요.\n\n\n<div class=\"content-ad\"></div>\n\n```python\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb) # 토큰 임베딩\n        self.wpe = nn.Embedding(ctx_len, n_emb) # 위치 임베딩\n```\n\n위치 임베딩은 토큰 임베딩과 동일한 방식으로 작동합니다. 하지만 각 토큰마다 행이 있는 것 대신, 각 가능한 위치 인덱스마다 행이 있습니다. 이는 임베딩 가중치의 모양이 (ctx_len, n_emb)가 됨을 의미합니다. 이제 GPT 클래스에 __call__ 함수를 구현해보겠습니다. 이 함수에는 모델의 forward pass가 포함될 것입니다.\n\n```python\n# 텐서 모양 주석\ndef __call__(self, x):\n    B, T = x.shape # (B = 배치 크기, T = ctx_len)\n    tok_emb = self.wte(x) # (B, T, n_emb)\n    pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)\n    x = tok_emb + pos_emb # (B, T, n_emb)\n```\n\n먼저, 입력의 차원을 B와 T 변수로 나누어 보다 쉽게 처리합니다. 시퀀스 모델링 맥락에서 B와 T는 일반적으로 \"배치\"와 \"시간\" 차원을 나타내는 약어로 사용됩니다. 이 경우, 시퀀스의 \"시간\" 차원은 컨텍스트 길이입니다.\n\n<div class=\"content-ad\"></div>\n\n다음으로 토큰 및 위치 임베딩을 계산합니다. 위치 임베딩의 경우 입력이 mx.arange(T)임에 유의하십시오. 이는 우리가 임베딩하고자 하는 위치인 0부터 T-1까지의 연속 정수 배열을 출력합니다. 임베딩 레이어를 통과한 후에는 각 위치에 대해 n_emb 길이의 벡터를 추출하므로 모양이 (T, n_emb)인 텐서가 생성됩니다. pos_emb가 tok_emb과 형태가 다르더라도 mlx가 브로드캐스트 또는 배치 차원을 통해 pos_emb을 복제하여 요소별 덧셈을 허용하기 때문에 두 값을 더할 수 있습니다. 마지막으로 덧셈을 수행하여 토큰의 새로운 표현을 얻습니다.\n\n## 셀프 어텐션\n\n지금까지 각 토큰의 표현 벡터는 독립적으로 계산되었습니다. 그들은 어떠한 정보를 교환할 기회도 가지지 못했습니다. 이는 주변 맥락에 따라 단어의 의미와 사용이 의존되므로 언어 모델링에서 직관적으로 나쁜 접근입니다. 셀프 어텐션은 이전 토큰으로부터 정보를 현재 토큰으로 통합하는 방법입니다.\n\n먼저, 가장 단순한 접근 방식을 살펴보겠습니다. 만약 각 토큰을 단순히 해당 표현 벡터의 평균으로 표현하고 그 이전의 모든 토큰의 벡터를 더한다면 어떨까요? 이렇게 하면 이전 토큰들로부터 정보를 현재 토큰의 표현에 담을 수 있습니다. 어떻게 보이게 될까요?\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_4.png)\n\n하지만 self-attention은 for-loop를 사용하지 않습니다. 핵심 아이디어는 이전 토큰의 평균을 행렬 곱셈으로 얻을 수 있다는 것입니다!\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_5.png)\n\n입력 시퀀스를 특별한 행렬로 왼쪽부터 곱하면 원하는 결과를 얻을 수 있습니다. 이 행렬을 주목해보면 이는 어텐션 가중치라고 알려져 있습니다. 어텐션 가중치 행렬의 각 행은 주어진 토큰의 표현에 각 다른 토큰이 얼마나 많이 기여하는지를 나타냅니다. 예를 들어, 두 번째 행의 경우 [0.5, 0.5, 0, 0]입니다. 이것은 두 번째 행의 결과가 0.5*토큰1 + 0.5*토큰2 + 0*토큰3 + 0*토큰4, 즉 토큰1과 토큰2의 평균이 됨을 의미합니다. 어텐션 가중치는 하삼각 행렬입니다 (우상단 항목이 0). 이는 미래 토큰이 주어진 토큰의 표현에 포함되지 않도록 보장합니다. 이는 토큰이 이전 토큰과만 통신할 수 있도록 하며, 생성 중에 모델은 이전 토큰에만 액세스할 수 있는 것이 보장됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n어떻게 주의 집중 가중치 행렬을 생성할 수 있는지 살펴봅시다.\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_6.png)\n\n주의 가중치 행렬을 구성하는 배열을 만들고 오른쪽 상단 항목에 -inf를 넣은 다음 행별 softmax를 수행하면 원하는 주의 가중치를 얻을 수 있습니다. 이 작업을 수행하는 것으로 이 작업이 작동하는 방법을 확인하는 것이 좋습니다. 핵심은 (ctx_len, ctx_len) 크기의 배열을 가지고 각 행에 softmax를 수행하여 합계가 1이 되는 주의 가중치를 얻을 수 있다는 것입니다.\n\n이제 naive 자기 주의 영역을 벗어나 볼 수 있습니다. 이전 토큰을 간단히 평균화하는 대신 이전 토큰에 대한 임의의 가중 합계를 사용합니다. 임의의 행렬의 분포 소프트맥스를 수행할 때 어떻게 되는지 주목하세요.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_7.png)\n\n각 행의 합이 1인 가중치를 계속 얻습니다. 훈련 중에는 왼쪽 행렬의 숫자를 학습하여 각 토큰이 다른 토큰의 표현에 얼마나 많이 참여하는지를 지정할 수 있습니다. 이것이 토큰이 서로에게 \"주의\"를 기울이는 방법입니다. 그러나 여전히 이 왼쪽 행렬이 어디에서 나왔는지 이해하지 못했습니다. 이러한 사전 소프트맥스 주의 가중치는 토큰 자체에서 계산되지만 간접적으로 세 개의 선형 변환을 통해 수행됩니다.\n\n## Keys, Queries, and Values\n\n![이미지](/assets/img/2024-06-19-GPTfromScratchwithMLX_8.png)\n\n\n<div class=\"content-ad\"></div>\n\n우리 시퀀스의 각 토큰은 3개의 새로운 벡터를 생성합니다. 이러한 벡터를 키(key), 쿼리(query) 및 값(value)라고합니다. 한 토큰의 쿼리 벡터와 다른 토큰의 키 벡터의 내적을 사용하여 두 토큰 간의 \"유사성\"을 측정합니다. 우리는 각 토큰과 각 다른 토큰 사이의 쌍별 유사성을 계산하고 싶어합니다. 따라서 쿼리 벡터(4x3)를 키 벡터의 전치(3x4)와 곱하여 원시 어텐션 가중치(4x4)를 얻습니다. 행렬 곱셈이 작동하는 방식으로 인해 원시 어텐션 가중치의 (i,j) 항목은 토큰 i의 쿼리와 토큰 j의 키의 내적 또는 두 가지 사이의 \"유사성\"이됩니다. 따라서 우리는 모든 토큰 간의 상호 작용을 계산했습니다. 그러나 과거 토큰이 미래 토큰과 상호 작용하는 것을 원하지 않기 때문에 상단 우측 항목들에 -inf 마스크를 적용하여 소프트맥스 후에 제로아웃되도록합니다. 그런 다음 행별 소프트맥스를 수행하여 최종 어텐션 가중치를 얻습니다. 이러한 가중치를 입력과 직접 곱하는 대신 값 프로젝션과 곱합니다. 결과적으로 새로운 표현이 생성됩니다.\n\n이제 우리는 주의를 개념적으로 이해했으니, 구현해 봅시다.\n\n```js\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n  \n```\n\n키, 쿼리 및 값 프로젝션 레이어를 정의하여 시작합니다. n_emb에서 진행하는 대신 n_emb에서 head_size로 프로젝션합니다. 아무것도 변경되지 않으며, 주의를 통해 계산된 새로운 표현이 차원 head_size가됨을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n    def __call__(self, x): # shapes commented\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n\n\n앞서로부터 전달받은 값으로 key, query, value를 계산한 뒤, 입력 모양을 미래의 편리함을 위해 변수 B, T, C로 나눕니다.\n\n\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\n\n\n이어서, 어텐션 가중치를 계산합니다. 키 텐서의 마지막 두 차원만 바꿔야 하므로 차원 변환은 마지막 두 차원에 대해서만 이루어집니다. 배치 차원은 여러 학습 예제를 병렬로 전달하기 위한 것뿐입니다. mlx의 전치 함수는 차원의 새로운 순서를 입력으로 받기 때문에, 마지막 두 차원을 전치하기 위해 [0, 2, 1]을 전달합니다. 그리고 여기에 주목할 점: 어텐션 가중치들은 head_size의 제곱근에 역수를 적용합니다. 이는 스케일드 어텐션이라 불리며, 목적은 Q와 K가 단위 분산을 가질 때, attn_weights도 단위 분산을 가지게 하는 것입니다. attn_weights의 분산이 높으면 softmax가 이 작은 값과 큰 값을 0 또는 1로 매핑하여 복잡성이 적은 표현을 얻도록 합니다.\n\n\n<div class=\"content-ad\"></div>\n\n다음 단계는 미래 토큰에 주의를 기울이지 않는 인과 언어 모델링, 즉 토큰이 미래 토큰에 주의를 기울이지 않도록 마스크를 적용하는 것입니다.\n\n```js\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] < indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\n```\n\n우리는 미리 설정한 ctx_len=4와 같은 다이어그램에서 indices 변수를 [0, 1, 2, 3]으로 설정하기 위해 mx.arange(4)을 사용한다.\n\n<div class=\"content-ad\"></div>\n\n그럼 indices[:, None]와 같이 인덱스 값을 가진 열 벡터를 생성할 수 있어요. 마찬가지로 indices[None]을 사용하면 행 벡터를 얻을 수 있어요. 그리고 ` 비교를 수행할 때 mlx는 벡터들을 브로드캐스트합니다. 그 이유는 형태가 맞지 않아 요소별로 비교할 수 없기 때문이에요. 브로드캐스팅은 mlx가 부족한 차원에 따라 벡터를 복제한다는 것을 의미해요. 그 결과, (4, 4) 행렬 간의 요소별 비교가 이루어집니다. 그게 이해가 되죠. 참고로, 텐서 처리할 때 브로드캐스팅 세부 정보에 익숙해지는 것을 권장드립니다. 이 링크를 읽어보세요. 튜토리얼 등장횟수가 많을 거에요.\n\n요소별 비교 후, 다음 텐서가 남아 있어요:\n\n```js\n[[False,  True,  True,  True],\n [False, False,  True,  True],\n [False, False, False,  True],\n [False, False, False, False]]\n```\n\n이 텐서에 -1e9를 곱하면 값을 구할 수 있어요:\n\n<div class=\"content-ad\"></div>\n\n```js\n[[-0e+00, -1e+09, -1e+09, -1e+09],\n [-0e+00, -0e+00, -1e+09, -1e+09],\n [-0e+00, -0e+00, -0e+00, -1e+09],\n [-0e+00, -0e+00, -0e+00, -0e+00]]\n```\n\n이제 추가적인 마스크가 있습니다. 이 행렬을 어텐션 가중치에 추가하여 모든 오른쪽 상단 항목을 매우 큰 음수로 만들 수 있습니다. 이렇게 하면 소프트맥스 연산 후에 이들이 0이 될 것입니다. 또한, _causal_mask 속성 이름에 \"_\"를 접두사로 추가하여 개인 변수로 표시합니다. 이것은 mlx에게 이것이 매개변수가 아니며 교육 중에 업데이트되지 않아야 함을 나타냅니다.\n\n```js\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] < indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\n        attn_weights = attn_weights + self._causal_mask\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        o = (attn_weights @ V) # (B, T, head_size)\n```\n\n이제 최종 어텐션 가중치를 얻기 위해 행별로 softmax 처리하고 이러한 가중치를 값에 곱하여 출력을 얻을 수 있습니다. softmax에 axis=-1을 전달하여 행이 있는 마지막 차원을 따라 softmax를 수행하려는 것을 지정합니다.\n\n<div class=\"content-ad\"></div>\n\n최종 단계는 선형 투영 및 드롭아웃을 출력하는 것입니다.\n\n```js\n드롭아웃 = 0.1 # 파일 상단의 하이퍼파라미터와 함께 추가\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] < indices[None] # 브로드캐스팅 트릭\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 투영\n        self.resid_dropout = nn.Dropout(드롭아웃)\n    def __call__(self, x):\n        B, T, C = x.shape # (배치 크기, ctx 길이, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\n        attn_weights = attn_weights + self._causal_mask\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        o = (attn_weights @ V) # (B, T, head_size)\n        o = self.c_proj(self.resid_dropout(o))\n        return o\n```\n\n출력 투영과 잔차 드롭아웃인 c_proj 및 resid_dropout 두 개의 새 계층을 추가했습니다. 출력 투영은 벡터를 원래 차원인 n_emb로 반환하는 역할을 합니다. 드롭아웃은 정규화 및 훈련 안정성을 위해 추가되었으며, 트랜스포머 블록을 쌓으면서 심층 네트워크를 구축하는 것이 중요합니다. 이것으로 하나의 어텐션 헤드를 구현하는 것이 끝났습니다!\n\n## 다중 헤드 어텐션\n\n<div class=\"content-ad\"></div>\n\n하나의 주의 헤드만 있는 LLM보다는 여러 개의 주의 헤드를 병렬로 사용하고 그 출력을 연결하여 최종 표현을 만들곤 합니다. 예를 들어, 하나의 head_size=64를 가진 attention head가 있다고 가정해봅시다. 각 토큰에 대해 생성된 벡터는 64 차원입니다. 우리는 head_size=16인 4개의 병렬 attention head로 동일한 결과를 얻을 수 있습니다. 이들의 출력을 연결하여 16x4 = 64 차원의 출력을 생성할 수 있습니다. Multi-head attention은 모델이 더 복잡한 표현을 학습할 수 있도록 합니다. 각 head가 다른 projection 및 attention 가중치를 학습하기 때문입니다.\n\n```js\nn_heads = 4\nclass MultiHeadAttention(nn.Module): # 단순한 구현\n    def __init__(self):\n        super().__init__()\n        self.heads = [Attention(head_size // n_heads) for _ in range(n_heads)]\n    def __call__(self, x):\n        return mx.concatenate([head(x) for head in self.heads], axis=-1)\n```\n\n간단한 구현은 n_heads의 attention head 목록을 생성하고, 각각의 크기를 최종 head 크기로 나눈 것입니다. 그리고 각 헤드의 출력을 마지막 축을 기준으로 연결하는 것입니다. 그러나 이 구현은 비효율적이며 텐서의 속도를 활용하지 못합니다. 텐서의 성능을 활용한 multi-head attention을 구현해 봅시다.\n\n```js\nhead_size = 64 # 파일 상단에 넣기\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] < indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 projection\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n```\n\n<div class=\"content-ad\"></div>\n\n우선, 단일 헤드 어텐션 구현부터 시작해보겠습니다. __init__() 함수는 변경되지 않았어요. forward pass는 키(key), 쿼리(query), 값(value) 프로젝션을 생성하는 것으로 일반적으로 시작합니다.\n\n```js\nhead_size = 64 # 파일 맨 위에 배치\nn_heads = 8 # 파일 맨 위에 배치\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] < indices[None] # 브로드캐스팅 트릭\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 프로젝션\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (배치 크기, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        mha_shape = (B, T, n_heads, head_size//n_heads)\n        K = mx.as_strided(K, (mha_shape)) # (B, T, n_heads, head_size//n_heads)\n        Q = mx.as_strided(Q, (mha_shape)) # (B, T, n_heads, head_size//n_heads)\n        V = mx.as_strided(V, (mha_shape)) # (B, T, n_heads, head_size//n_heads)\n```\n\n다음으로 수행해야 할 일은 헤드 수를 나타내는 새로운 차원을 추가하는 것이에요. 기존의 부자연스러운 구현에서는 각각 고유한 키, 쿼리 및 값 텐서를 가진 별도의 어텐션 객체를 사용했었지만, 이제 이 모든 요소를 하나의 텐서에 모두 가지고 있기 때문에 헤드를 위한 차원이 필요합니다. 우리가 원하는 새로운 모양을 mha_shape에 정의합니다. 그런 다음 각 텐서를 헤드 차원을 가지도록 재구성하기 위해 mx.as_strided()를 사용합니다. 이 함수는 파이토치의 view와 동등하며 mlx에게 이 배열을 다른 모양으로 다루도록 지시합니다. 그러나 아직 문제가 있어요. 이전과 같이 Q @ K_t(K의 마지막 2 차원을 전치한 K_t)를 곱하여 어텐션 가중치를 계산하려고 하면 다음과 같은 모양을 곱하게 됩니다.\n\n```js\n(B, T, n_heads, head_size//n_heads) @ (B, T, head_size//n_heads, n_heads)\n결과 모양: (B, T, n_heads, n_heads)\n```\n\n<div class=\"content-ad\"></div>\n\n이제 (B, T, n_heads, n_heads) 모양의 텐서가 생성됩니다. 이는 올바른 결과가 아닙니다. 한 개의 헤드에서 우리의 어텐션 가중치는 (B, T, T) 모양이어야 합니다. 각 토큰 쌍 간의 상호 작용을 제공하기 때문에 이는 의미가 있습니다. 따라서 이제 우리의 모양은 똑같아야 하지만 헤드 차원이 추가되어야 합니다: (B, n_heads, T, T). 이를 위해 키, 쿼리 및 값의 차원을 변환하고, n_heads 차원을 2가 아닌 1로 만든 다음 재구성하는 방식으로 이를 달성합니다.\n\n```js\nhead_size = 64 # 파일 상단에 배치\nn_heads = 8 # 파일 상단에 배치\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] < indices[None] # 브로드캐스팅 트릭\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 프로젝션\n        self.attn_dropout = nn.Dropout(dropout)\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (배치 크기, 문맥 길이, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        mha_shape = (B, T, n_heads, head_size//n_heads)\n        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1]) # (B, n_heads, T, T)\n        attn_weights = attn_weights + self._causal_mask[:T, :T]\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        attn_weights = self.attn_dropout(attn_weights)\n        o = (attn_weights @ V) # (B, n_heads, T, head_size//n_heads)\n        \n```\n\n이제 올바른 어텐션 가중치를 계산할 수 있습니다. 각 개별 어텐션 헤드의 크기로 어텐션 가중치를 조정합니다. 연결 이후의 크기인 head_size가 아닌 각 개별 어텐션 헤드의 크기로 어텐션 가중치를 조정합니다. 또한 어텐션 가중치에 드롭아웃을 적용합니다.\n\n마지막으로, 연결을 수행하고 출력 프로젝션 및 드롭아웃을 적용합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nhead_size = 64 # 파일 상단에 넣어 둬요\nn_heads = 8 # 파일 상단에 넣어 둬요\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] < indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # 출력 프로젝션\n        self.attn_dropout = nn.Dropout(dropout)\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (배치 크기, ctx 길이, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        mha_shape = (B, T, n_heads, head_size//n_heads)\n        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\n        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1]) # (B, n_heads, T, T)\n        attn_weights = attn_weights + self._causal_mask[:T, :T]\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        attn_weights = self.attn_dropout(attn_weights)\n        o = (attn_weights @ V) # (B, n_heads, T, head_size//n_heads)\n        o = o.transpose([0, 2, 1, 3]).reshape((B, T, head_size)) # 헤드 연결\n        o = self.c_proj(self.resid_dropout(o))\n        return o\n```\n\n모든 것을 하나의 텐서로 가지고 있기 때문에 형태 조작을 통해 연결을 수행할 수 있어요. 먼저, `transpose` 함수를 사용하여 `n_heads`를 두 번째로 마지막 차원으로 이동합니다. 그런 다음, 앞서 수행한 헤드 분할을 되돌리기 위해 원래 크기로 다시 형태를 변환합니다. 이는 각 헤드에서 최종 벡터를 연결하는 것과 동일합니다. 그리고 이게 멀티 헤드 어텐션에 대한 모든 것이에요! 가장 집중력이 필요한 구현 부분을 처리했어요.\n\n# MLP\n\n아키텍처의 다음 부분은 멀티레이어 퍼셉트론 또는 MLP입니다. 이는 2개의 쌓인 선형 레이어를 의미합니다. 여기에 말할 것은 많지 않아요, 이것은 표준 신경망입니다.\n\n\n<div class=\"content-ad\"></div>\n\n```python\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.c_fc = nn.Linear(n_emb, 4 * n_emb)\n        self.gelu = nn.GELU()\n        self.c_proj = nn.Linear(4 * n_emb, n_emb)\n        self.dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        x = self.gelu(self.c_fc(x))\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n```\n\n입력을 받아 c_fc를 사용하여 고차원으로 프로젝션합니다. 그런 다음 gelu 비선형성을 적용하고 c_proj를 사용하여 임베딩 차원으로 다시 프로젝션합니다. 마지막으로 드롭아웃을 적용하고 반환합니다. MLP의 목적은 주의를 통해 벡터가 통신한 후 일부 계산을 허용하는 것입니다. 이러한 통신 레이어(주의) 및 계산 레이어(mlp)를 블록에 쌓겠습니다.\n\n# 블록\n\nGPT 블록은 주의가 뒤따르는 MLP로 구성됩니다. 이러한 블록은 구조를 깊게 만들기 위해 반복됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n```python\nclass Block(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mlp = MLP()\n        self.mha = MultiHeadAttention()\n    \n    def __call__(self, x):\n        x = self.mha(x)\n        x = self.mlp(x)\n        return x\n```\n\n이제 훈련 안정성을 향상시키기 위해 두 가지 기능을 추가해야 합니다. 아키텍처 다이어그램을 다시 살펴보겠습니다.\n\n## 레이어 정규화 및 스킵 연결\n\n![GPTfromScratchwithMLX_10](/assets/img/2024-06-19-GPTfromScratchwithMLX_10.png)\n\n\n<div class=\"content-ad\"></div>\n\n아직도 빨간색으로 표시된 구성 요소를 구현해야 합니다. 화살표는 skip 연결을 나타냅니다. 입력이 직접 변환되는 대신, 어텐션 및 MLP 레이어의 효과는 가산적입니다. 이들 결과는 입력에 직접적으로 대체하는 대신 추가됩니다. 이는 깊은 신경망의 훈련 안정성에 도움이 됩니다. 왜냐하면 역전파에서 덧셈 연산의 피연산자들은 합과 동일한 기울기를 받게 됩니다. 그러므로 기울기가 자유롭게 역방향으로 흐를 수 있어서 깊은 신경망을 괴롭히는 사라지거나 폭주하는 기울기와 같은 문제를 방지할 수 있습니다. 또한 레이어 정규화는 활성화 함수들이 정규 분포를 보이도록 하여 훈련 안정성을 돕습니다. 아래는 최종 구현입니다.\n\n```js\nclass Block(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mlp = MLP()\n        self.mha = MultiHeadAttention()\n        self.ln_1 = nn.LayerNorm(dims=n_emb)\n        self.ln_2 = nn.LayerNorm(dims=n_emb)\n    def __call__(self, x):\n        x = x + self.mha(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n```\n\n레이어 정규화는 멀티헤드 어텐션 및 MLP 이전에 적용됩니다. skip 연결은 x = x + ...와 같이 추가를 의미합니다.\n\n# Forward Pass\n\n<div class=\"content-ad\"></div>\n\n블록을 정의했으니, GPT-2의 전방 향 과정을 완료할 수 있습니다.\n\n```python\nn_layers = 3 # 파일 맨 위에 배치\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb) # 토큰 임베딩\n        self.wpe = nn.Embedding(ctx_len, n_emb) # 위치 임베딩\n        self.blocks = nn.Sequential(\n            *[Block() for _ in range(n_layers)],\n        ) # 트랜스포머 블록들\n        self.ln_f = nn.LayerNorm(dims=n_emb) # 최종 레이어 정규화\n        self.lm_head = nn.Linear(n_emb, vocab_size) # 출력 프로젝션\n    # 텐서 모양 주석\n    def __call__(self, x):\n        B, T = x.shape # (B = 배치 크기, T = ctx_len)\n        tok_emb = self.wte(x) # (B, T, n_emb)\n        pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)\n        x = tok_emb + pos_emb # (B, T, n_emb)\n        x = self.blocks(x) # (B, T, n_emb)\n        x = self.ln_f(x) # (B, T, b_emb)\n        logits = self.lm_head(x) # (B, T, vocab_size)\n        return logits\n```\n\nnn.Sequential을 사용해 블록을 담는 컨테이너를 만들어서 입력을 순차적으로 전달할 수 있습니다. 그런 다음 self.blocks(x)를 사용하여 모든 블록을 적용할 수 있습니다. 마지막으로 레이어 정규화를 적용하고 lm_head를 적용합니다. lm_head 또는 언어 모델링 헤드는 임베딩 차원에서 어휘 크기로 매핑하는 단순한 선형 레이어입니다. 모델은 어휘 내 각 단어에 대한 값이 포함된 벡터를 출력하며, 이를 로짓이라고 합니다. 로짓에 소프트맥스를 적용하여 어휘 전체에 대한 확률 분포를 얻을 수 있으며, 다음 토큰을 샘플링하거나 훈련 중 손실을 계산하는 데 사용할 수 있습니다. 학습을 시작하기 전에 구현해야 할 두 가지 사항만 더 남았습니다.\n\n# 샘플링\n\n<div class=\"content-ad\"></div>\n\n훈련이 완료된 후 모델에서 한 번 sampling하기 위해 generate 함수를 작성해야 합니다. 아이디어는 우리가 선택한 일련의 시퀀스로 시작하고, 그 다음 토큰을 예측하여 이를 시퀀스에 추가하는 것입니다. 그런 다음 새로운 시퀀스를 입력하고 다시 다음 토큰을 예측합니다. 이를 멈출 때까지 반복합니다.\n\n```js\n# GPT 클래스의 메서드\ndef generate(self, max_new_tokens):\n  ctx = mx.zeros((1, 1), dtype=mx.int32)\n```\n\n우리는 모델에 단일 토큰 'zero'로 프롬프트를 제공합니다. Zero는 새 줄 문자이므로 모델이 얼마나 셰익스피어와 유사한지 확인하고 싶으므로 세대를 시작하는 자연스러운 장소입니다. 참고로, (1, 1) 형태로 초기화하여 시퀀스 길이가 하나인 단일 배치를 시뮬레이션합니다.\n\n```js\n# GPT 클래스의 메서드\ndef generate(self, max_new_tokens):\n  ctx = mx.zeros((1, 1), dtype=mx.int32)\n  for _ in range(max_new_tokens):\n    logits = self(ctx[:, -ctx_len:]) # 마지막 ctx_len 문자열 전달\n    logits = logits[:, -1, :] # 다음 토큰에 대한 로짓 얻기\n    next_tok = mx.random.categorical(logits, num_samples=1)\n    ctx = mx.concatenate((ctx, next_tok), axis=1)\nreturn ctx\n```\n\n<div class=\"content-ad\"></div>\n\n다음 토큰에 대한 로짓을 얻으려면 마지막 ctx_len 문자열을 모델에 전달합니다. 그러나 모델 출력은 (B, T, vocab_size) 모양입니다. 왜냐하면 입력의 각 토큰에 대한 다음 토큰의 로짓을 예측하기 때문입니다. 학습 중에는 이를 전부 사용하지만 이제는 새 토큰을 샘플링하기 위해 마지막 토큰의 로짓만 원합니다. 이를 위해 로짓을 인덱싱하여 순서 차원인 첫 번째 차원에서 마지막 요소를 얻습니다. 그런 다음 mx.random.categorical() 함수를 사용하여 다음 토큰을 샘플합니다. 이 함수는 로짓을 softmax를 통해 확률 분포로 변환하고 확률에 따라 토큰을 무작위로 샘플링합니다. 마지막으로 새 토큰을 문맥에 연결하고 max_new_tokens 횟수만큼 프로세스를 반복합니다.\n\n# 초기화\n\n마지막으로 중요한 훈련 다이내믹스를 위해 가중치 초기화를 처리해야 합니다.\n\n```js\n# GPT의 방법\ndef _init_parameters(self):\n    normal_init = nn.init.normal(mean=0.0, std=0.02)\n    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))\n```\n\n<div class=\"content-ad\"></div>\n\n먼저, 두 가지 서로 다른 nn.init.normal 함수를 정의합니다. 첫 번째는 모든 선형 및 임베딩 레이어를 초기화하는 함수입니다. 두 번째는 특히 잔여 투영인 선형 레이어를 초기화하는 함수이며, 이는 다중 헤드 어텐션과 MLP 내부의 마지막 선형 레이어를 말합니다. 이 특별한 초기화의 이유는 GPT-2 논문에 따르면 모델 깊이가 증가함에 따라 잔여 경로를 따라 누적을 확인하기 때문입니다 [2].\n\nmlx에서는 mx.update() 함수를 사용하여 모델의 매개변수를 변경할 수 있습니다. 문서를 확인해보면, 새로운 모델 매개변수의 완전한 또는 부분적인 사전을 예상합니다. 이 사전이 어떻게 구성되는지는 GPT 클래스 내에서 self.parameters()을 출력하여 확인할 수 있습니다.\n\n```js\n{'wte': {'weight': array([[-0.025084, -0.0197523, -0.0341617, ..., -0.0979123, -0.0830218, -0.0784692],\n       [-0.00777913, -0.117002, -0.0310708, ..., 0.0128591, 0.122941, 0.000414443],\n       [0.0240044, -0.0859084, 0.0253116, ..., 0.108967, 0.0767123, 0.0221565],\n       ...,\n       [0.050729, -0.04578, 0.0685943, ..., -0.0496998, -0.00350879, -0.00631825],\n       [0.00518804, 0.0499818, 0.0330045, ..., 0.0300661, 0.0431054, 0.000958906],\n       [-0.0323007, 0.0132046, 0.0208218, ..., -0.0785159, 0.00436121, -0.00726994]], dtype=float32)}, 'wpe': {'weight': array([[0.000797923, -0.0396898, -0.029047, ..., -0.0132273, 0.00684483, -0.0067624],\n       [-0.0247021, -0.0274349, 0.0310587, ..., -0.100099, 0.0301566, -0.0178732],\n       [0.0929172, -0.0468649, 0.0101506, ..., -0.0341086, -0.0516283, 0.0447596],\n       ...,\n       [-0.0508172, 0.0892201, -0.00183612, ..., -0.00341944, 0.023437, 0.0296461],\n       [0.0105829, 0.0688093, 0.146744, ..., -0.0836337, 0.0206679, 0.0184166],\n       [-0.00578717, -0.0606196, -0.0917056, ..., -0.0641549, -0.0490424, 0.0998114]], dtype=float32)}, 'blocks': {'layers': [{'mlp': {'c_fc': {'weight': array([[0.0169199, 0.00264431, 0.0316978, ..., -0.0596867, -0.0153549, 0.0176386],\n       ...\n```\n\n모든 모델 가중치를 mx.array로 포함하는 중첩된 사전입니다. 따라서 모델의 매개변수를 초기화하려면 새 매개변수로 이와 같은 사전을 구성하고 self.update()에 전달해야 합니다. 이를 위해 다음과 같이 수행할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n# GPT의 방법\ndef _init_parameters(self):\n    normal_init = nn.init.normal(mean=0.0, std=0.02)\n    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))\n    new_params = []\n    for name, module in self.named_modules():\n        if isinstance(module, nn.layers.linear.Linear):\n            new_params.append((name + '.weight', normal_init(module.weight)))\n        elif isinstance(module, nn.layers.embedding.Embedding):\n            new_params.append((name + '.weight', normal_init(module.weight))\n```\n\nnew_params 라는 튜플 목록을 유지합니다. 이 목록에는 (parameter_name, new_value)의 튜플이 포함됩니다. 다음으로 self.named_modules()를 사용하여 model의 각 nn.Module 객체를 반복하며 (name, module) 튜플을 반환합니다. 루프 내에서 모듈 이름을 인쇄하면 다음과 같이 보입니다.\n\n```js\nlm_head\nblocks\nblocks.layers.4\nblocks.layers.3\nblocks.layers.3.ln_2\nblocks.layers.3.ln_1\nblocks.layers.3.mha\nblocks.layers.3.mha.resid_dropout\nblocks.layers.3.mha.c_proj\nblocks.layers.3.mha.attn_dropout\nblocks.layers.3.mha.c_attn\n...\nblocks.layers.0.mlp.dropout\nblocks.layers.0.mlp.c_proj\nblocks.layers.0.mlp.gelu\nblocks.layers.0.mlp.c_fc\nwpe\nwte\n```\n\nisinstance() 함수를 사용하여 linear 및 embedding 레이어를 찾은 다음 목록에 추가합니다. 예를 들어, \"blocks.layers.0.mlp.c_fc\"에 도달하는 경우, 이는 MLP의 첫 번째 linear 레이어입니다. 이 경우 첫 번째 if 문이 트리거되어 (\"block.layers.0.mlp.c_fc.weight\", [`초기화된 weight 값 여기에 추가`])의 튜플이 목록에 추가됩니다. 우리는 특정한 이 방법으로 가중치를 초기화하고자 하기 때문에 이름에 \".weight\"를 추가해야 합니다. 이제 잔류 투영 초기화를 처리해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n```python\n# GPT의 메서드\ndef _init_parameters(self):\n    normal_init = nn.init.normal(mean=0.0, std=0.02)\n    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))\n    new_params = []\n    for name, module in self.named_modules():\n        if isinstance(module, nn.layers.linear.Linear):\n            if 'c_proj' in name: # 잔차 투영\n                new_params.append((name + '.weight', residual_init(module.weight)))\n            else:\n                new_params.append((name + '.weight', normal_init(module.weight)))\n            if hasattr(module, 'bias'):\n                new_params.append((name + '.bias', mx.zeros(module.bias.shape)))\n        elif isinstance(module, nn.layers.embedding.Embedding):\n            new_params.append((name + '.weight', normal_init(module.weight)))\n    self = self.update(utils.tree_unflatten(new_params))\n```\n\n선형 레이어인지 확인한 후 \"c_proj\"가 이름에 있는지 확인하고, 잔차 투영이라고 명명한 대로 특별한 초기화를 적용할 수 있습니다. 마지막으로 편향을 0으로 초기화해야 합니다.\n\n선형 브랜치 아래에 다른 if 문을 추가하여 nn.Module 객체가 편향 특성을 가지고 있는지 확인합니다. 그런 경우 해당 값을 0으로 초기화된 목록에 추가합니다. 마지막으로 튜플 목록을 중첩된 딕셔너리로 변환해야 합니다. 다행히 mlx에는 매개변수 딕셔너리를 처리하는 기능이 구현되어 있으며,이 목록을 중첩 된 매개변수 딕셔너리로 변환하기 위해 util.tree_unflatten() 함수를 사용할 수 있습니다. 이를 매개변수를 초기화하기 위해 update 메서드에 전달합니다. 이제 생성자에서 _init_parameters()를 호출할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n```python\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb)  # 토큰 임베딩\n        self.wpe = nn.Embedding(ctx_len, n_emb)  # 위치 임베딩\n        self.blocks = nn.Sequential(\n            *[Block() for _ in range(n_layers)],\n        )  # 트랜스포머 블록들\n        self.ln_f = nn.LayerNorm(dims=n_emb)  # 최종 레이어 정규화\n        self.lm_head = nn.Linear(n_emb, vocab_size)  # 출력 프로젝션\n        self._init_parameters()  # <-- 파라미터 초기화\n        # 초기화 시 전체 파라미터 수 출력\n        total_params = sum([p.size for n, p in utils.tree_flatten(self.parameters())])\n        print(f\"총 파라미터 수: {(total_params / 1e6):.3f}M\")\n    \n    # 텐서 모양 주석\n    def __call__(self, x):\n        B, T = x.shape  # (B = 배치 크기, T = ctx_len)\n        tok_emb = self.wte(x)  # (B, T, n_emb)\n        pos_emb = self.wpe(mx.arange(T))  # (T, n_emb)\n        x = tok_emb + pos_emb  # (B, T, n_emb)\n        x = self.blocks(x)  # (B, T, n_emb)\n        x = self.ln_f(x)  # (B, T, b_emb)\n        logits = self.lm_head(x)  # (B, T, vocab_size)\n        return logits\n    \n    def generate(self, max_new_tokens):\n        ctx = mx.zeros((1, 1), dtype=mx.int32)\n        for _ in range(max_new_tokens):\n            logits = self(ctx[:, -ctx_len:])\n            logits = logits[:, -1, :]\n            next_tok = mx.random.categorical(logits, num_samples=1)\n            ctx = mx.concatenate((ctx, next_tok), axis=1)\n        return ctx\n    \n    def _init_parameters(self):\n        normal_init = nn.init.normal(mean=0.0, std=0.02)\n        residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))\n        new_params = []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.layers.linear.Linear):\n                if 'c_proj' in name:\n                    new_params.append((name + '.weight', residual_init(module.weight)))\n                else:\n                    new_params.append((name + '.weight', normal_init(module.weight)))\n                if 'bias' in module:\n                    new_params.append((name + '.bias', mx.zeros(module.bias.shape)))\n            elif isinstance(module, nn.layers.embedding.Embedding):\n                new_params.append((name + '.weight', normal_init(module.weight))\n        self = self.update(utils.tree_unflatten(new_params))\n```\n\n생성자에 총 파라미터 수를 출력하는 코드를 추가했습니다. 마지막으로 훈련 루프를 구축할 준비가 되었습니다.\n\n# 훈련 루프\n\n모델을 훈련하기 위해서는 손실 함수가 필요합니다. 다음 토큰을 예측하므로 교차 엔트로피 손실을 사용합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndef loss_fn(model, x, y):\n    logits = model(x)\n    B, T, C = logits.shape # (batch_size, seq_len, vocab_size)\n    logits = logits.reshape(B*T, C)\n    y = y.reshape(B*T)\n    loss = nn.losses.cross_entropy(logits, y, reduction='mean')\n    return loss\n```\n\n먼저, 모델에서 로짓을 얻습니다. 그런 다음 로짓을 vocab_size 길이의 배열 목록으로 재구성합니다. 또한 정확한 토큰 ID 인 y를 동일한 길이로 재구성합니다. 그런 다음 내장된 교차 엔트로피 손실 함수를 사용하여 각 예제의 손실을 계산한 다음 이를 평균 내어 단일 값으로 얻습니다.\n\n```js\nmodel = GPT()\nmx.eval(model.parameters()) # 모델 파라미터 생성 (mlx는 게으른 평가)\nloss_and_grad = nn.value_and_grad(model, loss_fn)\nlr = 0.1\noptimizer = optim.AdamW(learning_rate=lr)\n```\n\n다음으로, 모델을 인스턴스화합니다. 그러나 mlx는 게으르게 평가되기 때문에 파라미터가 할당되고 생성되지 않습니다. 파라미터에 mx.eval을 호출하여 생성되도록 보장해야 합니다. 그런 다음 nn.value_and_grad()를 사용하여 손실 및 모델 파라미터의 그래디언트를 반환하는 함수를 얻을 수 있습니다. 이것이 우리가 최적화하는 데 필요한 모든 것입니다. 마지막으로 AdamW 옵티마이저를 초기화합니다.\n\n\n<div class=\"content-ad\"></div>\n\nnn.value_and_grad()에 대한 간단한 설명입니다. PyTorch를 사용해 보신 분이라면 loss.backward()를 사용할 것을 기대할 수 있습니다. 이 명령은 계산 그래프를 통과하며 모델 내 각 텐서의 .grad 속성을 업데이트합니다. 그러나 mlx의 자동 미분은 계산 그래프가 아닌 함수에 적용됩니다 [3]. 따라서 mlx에는 nn.value_and_grad()와 같이 함수를 입력받아 기울기 함수를 반환하는 내장 함수가 있습니다.\n\n이제 학습 루프를 정의해 보겠습니다.\n\n```js\nnum_epochs=20\nbatch_size=32\nfor epoch in range(num_epochs):\n    model.train(True)\n    running_loss = 0\n    batch_cnt = 0\n    for input, label in get_batches(X_train, y_train, batch_size):\n        batch_cnt += 1\n        loss, grads = loss_and_grad(model, input, label)\n        optimizer.update(model, grads)\n        running_loss += loss.item()\n        # 새로운 매개변수 및 옵티마이저 상태 계산\n        mx.eval(model.parameters(), optimizer.state)\n    avg_train_loss = running_loss / batch_cnt\n    model.train(False) # 평가 모드로 설정\n    running_loss = 0\n    batch_cnt = 0\n    for input, label in get_batches(X_val, y_val, batch_size):\n        batch_cnt += 1\n        loss = loss_fn(model, input, label)\n        running_loss += loss.item()\n    avg_val_loss = running_loss / batch_cnt\n    print(f\"Epoch {epoch:2} | train = {avg_train_loss:.4f} | val = {avg_val_loss:.4f}\")\n```\n\n외부 루프는 에포크를 거칩니다. 먼저 일부 모듈이 드롭아웃과 같이 학습 및 테스트 중에 다른 동작을 하는 경우가 있으므로 모델을 학습 모드로 설정합니다. 그런 다음 이전에 사용한 get_batches 함수를 사용하여 학습 데이터 배치를 반복합니다. 배치 단위로 손실과 기울기를 얻습니다. 그런 다음 모델과 기울기를 옵티마이저에 전달하여 모델 매개변수를 업데이트합니다. 마지막으로 매개변수 및 옵티마이저 상태가 업데이트되도록 mx.eval을 호출합니다(mx는 지연 평가를 수행하는 것을 기억하세요). 그런 다음 데이터의 평균 학습 손실을 계산하여 나중에 인쇄합니다. 이는 학습 데이터 한 번 통과입니다. 비슷하게 검증 손실을 계산하고 나서 에포크에서 평균 학습 및 검증 손실을 인쇄합니다.\n\n<div class=\"content-ad\"></div>\n\n\ncompletion = decode(model.generate(1000)[0].tolist())\nprint(completion)\nwith open('completions.txt', 'w') as f:\n    f.write(completion)\n\n\n마지막으로, 모델에서 생성하는 코드를 추가합니다. 생성 결과는 여전히 (B, T) 형태이므로 0에서 색인화하여 1차원으로 만든 다음 mlx 배열을 Python 리스트로 변환해야 합니다. 그런 다음 앞서 설명한 decode 함수에 전달하고 파일에 쓸 수 있습니다.\n\n다음은 학습에 사용할 매개변수입니다 (이를 변경해보실 수 있습니다):\n\n\nctx_len = 128\nn_emb = 128\ndropout = 0.1\nhead_size = 128\nn_heads = 4 \nn_layers = 3 \nnum_epochs = 20\nbatch_size = 64\nlr = 1e-3\n\n\n<div class=\"content-ad\"></div>\n\n이제 파일을 실행하여 훈련을 시작할 수 있습니다. 위의 설정으로 훈련을 진행하면 m2 맥북에서 약 10분이 걸렸어요. 지난 에포크에서 다음과 같은 훈련 손실을 얻었어요.\n\n```js\n에포크 19 | 훈련 = 1.6961 | 검증 = 1.8143\n```\n\n일부 출력을 살펴보겠습니다.\n\n```js\nGLOUCESTER:\nBut accomes mo move it.\n\nKING EDWARD:\nWhere our that proclaim that I curse, or I sprithe.\n\nCORIOLANUS:\nNot want:\nHis bops to thy father\nAt with hath folk; by son and fproathead:\nThe good nor may prosperson like it not,\nWhat, the beggares\nMore hath, when that made a,\nYour vainst Citizen:\nLet here are go in queen me and knife\nTo my deserved me you promise: not a fettimes,\nThat one the will not.\n\nCORIOLANUS:\nAnd been of queens,\nThou to do we best!\n\nJULIET:\nNot, brother recourable this doth our accuse\nInto fight!\n```\n\n<div class=\"content-ad\"></div>\n\n아주 작은 모델로 10분 동안의 훈련만으로 이 정도면 꽤 좋지 않나요? 문자를 예측하는데 셰익스피어 형식 같네요, 비록 무의미하긴 하지만요. 우리 모델과 실제 GPT-2의 유일한 차이는 이제 규모 뿐이에요! 이제 실험해보고 싶네요 — 다양한 설정을 시도해보거나 아키텍처를 잠시 건드려서 얼마나 낮은 손실을 달성할 수 있는지 확인해보세요.\n\n# 참고 문헌\n\n[1] Karpathy A (2015). Tiny Shakespeare [데이터 세트]. https://github.com/karpathy/char-rnn (MIT 라이선스)\n\n[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, 언어 모델은 비지도 멀티태스크 학습자입니다 (2019), OpenAI\n\n<div class=\"content-ad\"></div>\n\n[3] Automatic Differentiation — mlx docs","ogImage":{"url":"/assets/img/2024-06-19-GPTfromScratchwithMLX_0.png"},"coverImage":"/assets/img/2024-06-19-GPTfromScratchwithMLX_0.png","tag":["Tech"],"readingTime":41},{"title":"LSTM 구조를 사용하여 설명하는 어텐션","description":"","date":"2024-06-19 02:58","slug":"2024-06-19-AttentionexplainedusingLSTMarchitecture","content":"\n\n![이미지](/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_0.png)\n\nNLP 엔지니어로서 자주 들어본 말 중 하나가 'Attention(주의)!‘입니다. 트랜스포머(Transformer)와 GPT에 대해 배우기 시작한 사람들에게는 혼란스러울 수 있습니다. 그러나 기계가 보다 긴 시퀀스에서도 맥락 정보를 유지하는 방법을 알아야 합니다. \n\n이 블로그에서는 먼저 LSTM이 무엇인지, 그 단점들은 무엇이었는지, 그리고 어떻게 attention 메커니즘이 이를 극복하는 데 도움이 되었는지 알아보겠습니다.\n\n# Attention이 필요한 이유?\n\n<div class=\"content-ad\"></div>\n\n\n신경망 기술인 트랜스포머가 등장하기 전에 사용되던 기본 빌딩 블록은 인코더-디코더 LSTM 아키텍처였어요.\n\nLSTM 아키텍처는 주로 3개의 게이트로 구성돼 있어요. 각 게이트는 셀 상태로의 정보 흐름을 제어하는 역할을 합니다. 각 게이트와 그 기능에 대한 간단한 개요는 다음과 같아요:\n\n- 입력 게이트: 현재 입력에서 얼마나 많은 새로운 정보가 셀 상태에 추가돼야 하는지를 제어합니다. 현재 입력과 이전 숨겨진 상태를 가져와서 시그모이드 활성화 함수를 통과시켜 0과 1 사이의 값을 생성하고, 이 값을 현재 입력과 이전 숨겨진 상태를 통과시켜 생성된 후보 셀 상태에 곱해줍니다(이 값은 tanh 활성화 함수를 거칩니다).\n- 잊기 게이트: 이전 셀 상태 중 어느 부분을 유지하거나 잊을지를 결정합니다. 이전 숨겨진 상태와 현재 입력을 가져와서 시그모이드 활성화 함수를 통과시킵니다. 0과 1 사이의 결과값을 얻어 이전 셀 상태에 곱해줍니다. 이는 이전 셀 상태를 얼마나 유지할지를 결정합니다.\n- 출력 게이트: LSTM 셀의 출력과 다음 숨겨진 상태에 노출돼야 하는 셀 상태 얼마나 많은지를 결정합니다. 현재 입력과 이전 숨겨진 상태를 가져와서 시그모이드 활성화 함수를 통과시킵니다. 이 값은 현재 셀 상태의 tanh 값에 곱해져 다음 숨겨진 상태를 생성합니다.\n\n![LSTM 아키텍처를 이용한 어텐션](/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_1.png) \n\n\n<div class=\"content-ad\"></div>\n\n위 표는 LSTM이 장기 의존성을 효과적으로 포착하기 위해 시간이 지남에 따라 셀 상태를 유지하고 조정할 수 있도록 함께 작동하는 게이트를 보여줍니다. 그러나 위 구조에서 입력의 문맥 길이가 증가하면 LSTM이 이러한 게이트에 모든 필요한 문맥을 저장하는 것이 어려워집니다. 예를 들어, \"자주 피우지 마세요, 이것은 당신의 폐에 강력한 영향을 미치고 더 심각한 합병증으로 이어질 것입니다.\" 라는 문장을 생각해보십시오. 위 문장에서 모델이 \"자주\"를 잊어버리면 문장 전체의 의미가 변경될 수 있습니다.\n\n그래서 이를 피하기 위해 어텐션(Attention)이 도입되었습니다. 이 기법을 도입한 주된 목적은 디코더가 다음 토큰을 예측할 때 인코더의 각 입력 토큰의 영향을 받도록 하는 것입니다. 이렇게 하면 입력의 문맥 길이에 관계없이 디코더가 모든 단어에 액세스할 수 있습니다. 유사도 점수에 따라 특정 입력 토큰이 다른 것보다 예측에 더 많은 영향을 미치도록 허용됩니다.\n\n# 어텐션은 어떻게 작동하나요?\n\n![LSTM 아키텍처를 사용하여 어텐션이 설명된 그림](/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_2.png)\n\n<div class=\"content-ad\"></div>\n\n위의 아키텍처 다이어그램에서는 영어에서 스페인어로 번역 작업에 인코더-디코더 모델이 사용됩니다. 인코더는 LSTM의 단일 레이어를 사용하며 해당 레이어에 2개의 LSTM 셀이 있습니다. LSTM이 다음 단어를 인코딩할 때, 각 LSTM 레이어에 동일한 가중치와 편향이 사용됩니다. 디코더에는 다른 가중치와 편향 세트가 있습니다. 이 인코더-디코더 아키텍처의 구분은 가변 길이의 입력과 출력을 번역하는 데 도움이 됩니다. 예를 들어, \"Let's go\"가 \"Vamos\"로 번역됩니다. 디코더는 'EOS' 토큰에 도달하거나 최대 출력 단어 제한에 도달했을 때 단어 생성을 중지합니다.\n\n인코더에서 나오는 컨텍스트 벡터는 해당 LSTM 레이어의 각 단어인 \"let's\"와 \"go\"에 대한 정보를 제공하는 추가 데이터와 함께 디코더로 전달됩니다.\n\n이를 위해 LSTM 인코더의 단기 메모리에서 가져온 값은 현재 디코더 상태와 코사인 유사도를 사용하여 비교됩니다. 마찬가지로, 두 번째로 펼쳐진 LSTM 네트워크의 출력은 현재 디코더 상태와 다시 계산됩니다. 이 유사도 점수는 각 입력 토큰에서 각 출력 토큰과 계산되며 이 점수는 Softmax 함수를 통과하여 확률로 변환됩니다.\n\nSoftmax 함수 공식:\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_3.png)\n\n이 확률 점수는 각 입력 토큰이 해당 출력 토큰에 미치는 영향의 양을 나타냅니다. 그런 다음 인코더 출력은 해당 확률 점수에 따라 조정되어 인코더 출력의 가중 합인 컨텍스트 벡터를 계산합니다. 마지막으로 이 벡터는 현재 디코더 상태 벡터에 추가되어 다음 상태의 확률 점수를 생성하기 위해 완전 연결 네트워크를 통과합니다. 디코더 어휘에서 가장 높은 확률 점수를 가진 토큰이 예측되고, `EOS` 토큰이 예측되거나 출력 단어 제한이 도달할 때까지 디코더의 다음 레이어로 전달됩니다.\n\n이 아이디어를 통해 LSTM 모델의 필요성이 결국 사라지고 Transformer가 대세가 되었습니다.\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 어텐션(Attention)이 언어 모델이 입력에서 각 단어의 중요성을 이해하는 데 어떻게 도와주는지 설명합니다. 트랜스포머(Transformer) 아키텍처에서, 셀프-어텐션(Self-Attention)과 마스크드 셀프-어텐션(Masked Self-Attention)에 대해 이해할 수 있습니다.\n\n# 참고 자료\n\nStatquest: Joshua Starmer https://youtube.com/@statquest?si=sX9sq6vUnjzVbhCr\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” arXiv, August 1, 2023. https://doi.org/10.48550/arXiv.1706.03762.\n\n<div class=\"content-ad\"></div>\n\nLST Architecture Diagram – [Link](https://towardsdatascience.com/lstm-recurrent-neural-networks-how-to-teach-a-network-to-remember-the-past-55e54c2ff22e)","ogImage":{"url":"/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_0.png"},"coverImage":"/assets/img/2024-06-19-AttentionexplainedusingLSTMarchitecture_0.png","tag":["Tech"],"readingTime":4}],"page":"54","totalPageCount":71,"totalPageGroupCount":4,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true}