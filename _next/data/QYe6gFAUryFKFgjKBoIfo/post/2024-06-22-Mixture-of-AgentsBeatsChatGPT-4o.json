{"pageProps":{"post":{"title":"ChatGPT-4를 능가하는 Mixture-of-Agents의 비밀","description":"","date":"2024-06-22 21:07","slug":"2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o","content":"\n\n만약 지금까지 저를 따라와 주셨다면, 제가 얼마나 오랜 추론 모델에 열정적인지 알아채셨을 것입니다. 저는 — 학계도 동의하는 바와 같이 — 이러한 모델이 추론 능력 측면에서 AI의 다음 육지일 것이라고 믿습니다.\n\n이제, Together.ai(많은 LLM을 서비스하는 회사)의 연구자 그룹이 Mixture-of-Agents(MoA)에 대해 논문을 발표했습니다. 다양한 LLM이 복잡한 작업에서 훨씬 더 나은 결과를 얻기 위해 어떻게 결합될 수 있는지에 대해 다루고 있습니다.\n\n그리고 더 나은 결과란 매우 열등한 모델 세트를 사용하면서도 GPT-4를 인기 있는 벤치마크에서 이겼다는 말이죠.\n\n하지만 그게 가능한 방법은 무엇일까요? 이것이 바로 오랜 추론의 힘입니다. 함께 알아보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 반복이 지름길\n\n정의에 대해 모르는 사람들을 위해, Large Language Models (LLMs)은 직접적인 명확한 답변을 제공하는 대신에, 일정한 시간 동안 반복적으로 자아를 반성하고 '신중한' 대답을 제공할 수 있는 것으로 정의됩니다.\n\n모델들이 \"생각할 시간을 더 주게 되면,\" 현재 세대보다 우수한 결과를 얻게 되는데, 기본 모델이 비슷한 상태일 때에도 마찬가지입니다.\n\n아래에서 볼 수 있듯이, GPT-3.5는 GPT-4와 직접 비교했을 때 월등히 떨어지지만, 에이전트 워크플로에 적용되는 경우에는 그 위력을 크게 발휘합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Mixture of Agents Beats ChatGPT-4](/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_0.png)\n\n만약 이게 왜 이렇게 잘 작동하는지 궁금하다면, 정확히는 확신할 수 없지만, 대부분의 연구자들, 예를 들어 Yoshua Bengio와 같은 사람들이 다니엘 칸만의 사고 모델을 가리킵니다: 시스템 1과 2.\n\n- 시스템 1은 빠르고 의식하지 않으며 직관적입니다. 자동차를 운전할 때, 당신은 움직임을 생각하지 않습니다; 그것들은 본능적인 것이죠 (시스템 1).\n- 시스템 2는 느리고 의식적이며 신중합니다. 복잡한 수학 문제를 풀 때, 당신과 당신의 전두엽은 완전히 관여하여 문제를 해결합니다 (시스템 2).\n\n요컨데, 현재 LLMs는 시스템 1 사고자들이며, 마찬가지로 긴 추론 모델들은 시스템 2 사고를 흉내냅니다.\n\n\n<div class=\"content-ad\"></div>\n\n이제 긴 추론 프레임워크가 어떻게 작동하는지 알아봐요!\n\n## 협업의 원동력\n\n이미 언급했던 대로, 학계는 상당 시간 동안 LLM을 최대한 활용하기 위해 협업 프레임워크를 만들어 왔어요.\n\n1년 조금 넘은 시간 전에 MIT와 Google 연구원들이 Society of Minds를 선보이며 LLM을 위한 협업 프레임워크를 제시했어요.\n\n<div class=\"content-ad\"></div>\n\n그 때는 훨씬 덜 강력한 모델(바드와 ChatGPT-3.5 사용)이 사용되었음에도 불구하고, 이러한 프레임워크는 이미 초기 잘못된 답변을 내놓은 두 개의 LLM이 공유한 후에 올바른 답변을 얻는 등 매우 흥미로운 신생 행동을 보여주었습니다:\n\n![그림1](/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_1.png)\n\n아마도 더 중요한 연구는 Tree-of-Thoughts (ToT)이라는 것으로, 이는 구글과 프린스턴 대학교가 협력하여 이룬 것으로 LLM이 가능한 해결책 영역을 탐색할 수 있을 때 그들의 결과가 두드러지게 향상되며, 특히 추론 능력 측면에서 크게 발전한다는 아이디어를 확립했습니다:\n\n![그림2](/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_2.png)\n\n<div class=\"content-ad\"></div>\n\n어떤 복잡한 작업들에 대해서는 ToT가 성능을 10배 올려놓은 경우도 있었어요. 이는 이들이 흥미로운 발견을 하였다는 업계에 대한 경고 신호였죠.\n\n지금쯤이면 내용을 이해하셨을거라고 가정해 봅니다.\n\n긴 추론 모델은 여러 가능한 답변을 생성하고 그 답변들을 일정한 품질 기준을 충족할 때까지 개선하여 올바른 답변의 가능성을 최대화합니다.\n\n그렇다면 혼합 에이전트는 이 그림 속에서 어떻게 등장할까요?\n\n<div class=\"content-ad\"></div>\n\n# 여러 개의 LLM이 하나의 LLM보다 나아요\n\n어느 면에서 MoA는 개념적으로는 전문가들의 혼합과 유사합니다.\n\nMoE에서는 LLM의 층 (단순히 피드포워드 층만 해당되지만, 그건 다른 문제입니다)을 각 뉴런 세트를 전문화하는 '전문가'들로 분해하여, 따라서 주어진 예측에 LLM의 파라미터의 일부만 활성화합니다.\n\n여기서 LLM을 부분으로 나누는 대신, 더 작은 LLM으로 가득 찬 '큰 LLM'을 만듭니다. 아래에서 보듯이 우리는 에이전트라고 부르는 LLM으로 가득찬 'LLM 레이어'를 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_3.png\" />\n\n이러한 에이전트들은 제시된 프롬프트에 가능한 응답을 생성하는 것이 그들의 업무이기 때문에 제안자라고 불립니다.\n\n중요한 점은 이러한 레이어들이 밀도가 높다는 것인데, 다음 레이어의 에이전트들(LLM)이 이전 레이어에서 제안된 모든 답변을 받는다는 뜻입니다. 그 결과, 이러한 LLM들은 여전히 초기 질문에 대답해야 하지만 이전 레이어의 에이전트들의 응답을 사용하여 보다 깊은 맥락으로 답변할 수 있습니다.\n\n마지막으로, 다른 LLM 에이전트인 집계자는 이전 에이전트들이 제공한 모든 축적된 정보를 통합하고 사용자가 받는 응답을 작성합니다.\n\n<div class=\"content-ad\"></div>\n\n그리고, 이 자가 정제 프로세스는 매우 인상적인 결과를 보여줍니다.\n\n오픈소스 모델 세트는 모두 개별적으로 GPT-4o보다 훨씬 우월하지만 AlpacaEval 2.0 벤치마크(명령 따르기 벤치마크)에서 후자를 무너뜨렸습니다. GPT-4o의 이전 최고 성능인 57.5%에서 65.1%로 7.6%의 절대 개선 폭을 달성했습니다.\n\nFLASK와 같은 다른 벤치마크에서도 MoA는 GPT-4o와 경쟁력을 유지하면서, 오늘날 LLM에 대한 최신 기술 솔루션으로 만들어졌습니다.\n\n![이미지](/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_4.png)\n\n<div class=\"content-ad\"></div>\n\n너무 흥미롭게도, 이 프레임워크는 예측 당 생성된 토큰의 수가 훨씬 더 많다는 것을 의미하지만, 실제 모델의 정확도를 고려할 때, 전방 모델보다 비용 효율적인 것으로 보입니다. 이는 원가 및 각 순방향 패스당 사용된 테라플롭에 관해서이기도 합니다.\n\n![이미지](/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_5.png)\n\n## 새로운 세대\n\n오늘날 GenAI 제품에 대한 수요는 꿈보다는 현실이 되기 어려운 상황이며, 이는 주된 이유로 미비한 추론 능력이 있다고 할 수 있습니다. 그리고 장기 추론 모델들이 이 상황을 곧 변경할 것으로 예상됩니다.\n\n<div class=\"content-ad\"></div>\n\n그러나 이 연구에서 가장 놀라운 점은 전반적인 해결책이 평균적으로 더 많은 토큰을 생성하더라도, 협업에 사용된 평균 모델이 선두 모델보다 훨씬 작다는 점에서 더 저렴해 보인다는 것입니다.\n\nGPT-4보다 매개변수가 적은 MoA 모델이 여전히 능가한다는 것을 감안할 때, 장기 추론이 매개변수를 더 효율적으로 사용하는 방법인가요? 많은 사람들이 생각하는 것과는 달리, 개별 대형 모델보다 더 적은 컴퓨팅 자원을 필요로 할까요?\n\n이것은 양쪽 다 최선의 해법일 것입니다: 보다 나은 모델과 보다 효율적인 추론 작업. 우리는 이 문제를 가볍게 생각할 수 없습니다; 대형 언어 모델(Large Language Models, LLMs)로 인한 에너지 소비는 엄청난 문제입니다.\n\n따라서 성능은 높지만 더 저렴한 해결책에 대한 문을 열어주는 것은 흥미로우며, 솔직히 말해서 산업의 생존에 필수적입니다.","ogImage":{"url":"/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_0.png"},"coverImage":"/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_0.png","tag":["Tech"],"readingTime":5},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>만약 지금까지 저를 따라와 주셨다면, 제가 얼마나 오랜 추론 모델에 열정적인지 알아채셨을 것입니다. 저는 — 학계도 동의하는 바와 같이 — 이러한 모델이 추론 능력 측면에서 AI의 다음 육지일 것이라고 믿습니다.</p>\n<p>이제, Together.ai(많은 LLM을 서비스하는 회사)의 연구자 그룹이 Mixture-of-Agents(MoA)에 대해 논문을 발표했습니다. 다양한 LLM이 복잡한 작업에서 훨씬 더 나은 결과를 얻기 위해 어떻게 결합될 수 있는지에 대해 다루고 있습니다.</p>\n<p>그리고 더 나은 결과란 매우 열등한 모델 세트를 사용하면서도 GPT-4를 인기 있는 벤치마크에서 이겼다는 말이죠.</p>\n<p>하지만 그게 가능한 방법은 무엇일까요? 이것이 바로 오랜 추론의 힘입니다. 함께 알아보겠습니다.</p>\n<div class=\"content-ad\"></div>\n<h1>반복이 지름길</h1>\n<p>정의에 대해 모르는 사람들을 위해, Large Language Models (LLMs)은 직접적인 명확한 답변을 제공하는 대신에, 일정한 시간 동안 반복적으로 자아를 반성하고 '신중한' 대답을 제공할 수 있는 것으로 정의됩니다.</p>\n<p>모델들이 \"생각할 시간을 더 주게 되면,\" 현재 세대보다 우수한 결과를 얻게 되는데, 기본 모델이 비슷한 상태일 때에도 마찬가지입니다.</p>\n<p>아래에서 볼 수 있듯이, GPT-3.5는 GPT-4와 직접 비교했을 때 월등히 떨어지지만, 에이전트 워크플로에 적용되는 경우에는 그 위력을 크게 발휘합니다.</p>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_0.png\" alt=\"Mixture of Agents Beats ChatGPT-4\"></p>\n<p>만약 이게 왜 이렇게 잘 작동하는지 궁금하다면, 정확히는 확신할 수 없지만, 대부분의 연구자들, 예를 들어 Yoshua Bengio와 같은 사람들이 다니엘 칸만의 사고 모델을 가리킵니다: 시스템 1과 2.</p>\n<ul>\n<li>시스템 1은 빠르고 의식하지 않으며 직관적입니다. 자동차를 운전할 때, 당신은 움직임을 생각하지 않습니다; 그것들은 본능적인 것이죠 (시스템 1).</li>\n<li>시스템 2는 느리고 의식적이며 신중합니다. 복잡한 수학 문제를 풀 때, 당신과 당신의 전두엽은 완전히 관여하여 문제를 해결합니다 (시스템 2).</li>\n</ul>\n<p>요컨데, 현재 LLMs는 시스템 1 사고자들이며, 마찬가지로 긴 추론 모델들은 시스템 2 사고를 흉내냅니다.</p>\n<div class=\"content-ad\"></div>\n<p>이제 긴 추론 프레임워크가 어떻게 작동하는지 알아봐요!</p>\n<h2>협업의 원동력</h2>\n<p>이미 언급했던 대로, 학계는 상당 시간 동안 LLM을 최대한 활용하기 위해 협업 프레임워크를 만들어 왔어요.</p>\n<p>1년 조금 넘은 시간 전에 MIT와 Google 연구원들이 Society of Minds를 선보이며 LLM을 위한 협업 프레임워크를 제시했어요.</p>\n<div class=\"content-ad\"></div>\n<p>그 때는 훨씬 덜 강력한 모델(바드와 ChatGPT-3.5 사용)이 사용되었음에도 불구하고, 이러한 프레임워크는 이미 초기 잘못된 답변을 내놓은 두 개의 LLM이 공유한 후에 올바른 답변을 얻는 등 매우 흥미로운 신생 행동을 보여주었습니다:</p>\n<p><img src=\"/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_1.png\" alt=\"그림1\"></p>\n<p>아마도 더 중요한 연구는 Tree-of-Thoughts (ToT)이라는 것으로, 이는 구글과 프린스턴 대학교가 협력하여 이룬 것으로 LLM이 가능한 해결책 영역을 탐색할 수 있을 때 그들의 결과가 두드러지게 향상되며, 특히 추론 능력 측면에서 크게 발전한다는 아이디어를 확립했습니다:</p>\n<p><img src=\"/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_2.png\" alt=\"그림2\"></p>\n<div class=\"content-ad\"></div>\n<p>어떤 복잡한 작업들에 대해서는 ToT가 성능을 10배 올려놓은 경우도 있었어요. 이는 이들이 흥미로운 발견을 하였다는 업계에 대한 경고 신호였죠.</p>\n<p>지금쯤이면 내용을 이해하셨을거라고 가정해 봅니다.</p>\n<p>긴 추론 모델은 여러 가능한 답변을 생성하고 그 답변들을 일정한 품질 기준을 충족할 때까지 개선하여 올바른 답변의 가능성을 최대화합니다.</p>\n<p>그렇다면 혼합 에이전트는 이 그림 속에서 어떻게 등장할까요?</p>\n<div class=\"content-ad\"></div>\n<h1>여러 개의 LLM이 하나의 LLM보다 나아요</h1>\n<p>어느 면에서 MoA는 개념적으로는 전문가들의 혼합과 유사합니다.</p>\n<p>MoE에서는 LLM의 층 (단순히 피드포워드 층만 해당되지만, 그건 다른 문제입니다)을 각 뉴런 세트를 전문화하는 '전문가'들로 분해하여, 따라서 주어진 예측에 LLM의 파라미터의 일부만 활성화합니다.</p>\n<p>여기서 LLM을 부분으로 나누는 대신, 더 작은 LLM으로 가득 찬 '큰 LLM'을 만듭니다. 아래에서 보듯이 우리는 에이전트라고 부르는 LLM으로 가득찬 'LLM 레이어'를 생성합니다.</p>\n<div class=\"content-ad\"></div>\n<img src=\"/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_3.png\">\n<p>이러한 에이전트들은 제시된 프롬프트에 가능한 응답을 생성하는 것이 그들의 업무이기 때문에 제안자라고 불립니다.</p>\n<p>중요한 점은 이러한 레이어들이 밀도가 높다는 것인데, 다음 레이어의 에이전트들(LLM)이 이전 레이어에서 제안된 모든 답변을 받는다는 뜻입니다. 그 결과, 이러한 LLM들은 여전히 초기 질문에 대답해야 하지만 이전 레이어의 에이전트들의 응답을 사용하여 보다 깊은 맥락으로 답변할 수 있습니다.</p>\n<p>마지막으로, 다른 LLM 에이전트인 집계자는 이전 에이전트들이 제공한 모든 축적된 정보를 통합하고 사용자가 받는 응답을 작성합니다.</p>\n<div class=\"content-ad\"></div>\n<p>그리고, 이 자가 정제 프로세스는 매우 인상적인 결과를 보여줍니다.</p>\n<p>오픈소스 모델 세트는 모두 개별적으로 GPT-4o보다 훨씬 우월하지만 AlpacaEval 2.0 벤치마크(명령 따르기 벤치마크)에서 후자를 무너뜨렸습니다. GPT-4o의 이전 최고 성능인 57.5%에서 65.1%로 7.6%의 절대 개선 폭을 달성했습니다.</p>\n<p>FLASK와 같은 다른 벤치마크에서도 MoA는 GPT-4o와 경쟁력을 유지하면서, 오늘날 LLM에 대한 최신 기술 솔루션으로 만들어졌습니다.</p>\n<p><img src=\"/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_4.png\" alt=\"이미지\"></p>\n<div class=\"content-ad\"></div>\n<p>너무 흥미롭게도, 이 프레임워크는 예측 당 생성된 토큰의 수가 훨씬 더 많다는 것을 의미하지만, 실제 모델의 정확도를 고려할 때, 전방 모델보다 비용 효율적인 것으로 보입니다. 이는 원가 및 각 순방향 패스당 사용된 테라플롭에 관해서이기도 합니다.</p>\n<p><img src=\"/assets/img/2024-06-22-Mixture-of-AgentsBeatsChatGPT-4o_5.png\" alt=\"이미지\"></p>\n<h2>새로운 세대</h2>\n<p>오늘날 GenAI 제품에 대한 수요는 꿈보다는 현실이 되기 어려운 상황이며, 이는 주된 이유로 미비한 추론 능력이 있다고 할 수 있습니다. 그리고 장기 추론 모델들이 이 상황을 곧 변경할 것으로 예상됩니다.</p>\n<div class=\"content-ad\"></div>\n<p>그러나 이 연구에서 가장 놀라운 점은 전반적인 해결책이 평균적으로 더 많은 토큰을 생성하더라도, 협업에 사용된 평균 모델이 선두 모델보다 훨씬 작다는 점에서 더 저렴해 보인다는 것입니다.</p>\n<p>GPT-4보다 매개변수가 적은 MoA 모델이 여전히 능가한다는 것을 감안할 때, 장기 추론이 매개변수를 더 효율적으로 사용하는 방법인가요? 많은 사람들이 생각하는 것과는 달리, 개별 대형 모델보다 더 적은 컴퓨팅 자원을 필요로 할까요?</p>\n<p>이것은 양쪽 다 최선의 해법일 것입니다: 보다 나은 모델과 보다 효율적인 추론 작업. 우리는 이 문제를 가볍게 생각할 수 없습니다; 대형 언어 모델(Large Language Models, LLMs)로 인한 에너지 소비는 엄청난 문제입니다.</p>\n<p>따라서 성능은 높지만 더 저렴한 해결책에 대한 문을 열어주는 것은 흥미로우며, 솔직히 말해서 산업의 생존에 필수적입니다.</p>\n</body>\n</html>\n"},"__N_SSG":true}