{"pageProps":{"post":{"title":"벡터 DB를 사용하여 실시간 뉴스 검색 엔진을 구축하는 방법","description":"","date":"2024-06-19 16:12","slug":"2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs","content":"\n\n아파치 카프카, 바이트왁스, 그리고 업스태시 벡터 데이터베이스를 활용한 라이브 뉴스 집계 스트리밍 파이프라인 구현을 위한 실용적인 안내서입니다.\n\nearthweb.com에서 실시한 연구에 따르면, 매일 온라인 및 오프라인에서 유입되는 뉴스 기사는 200-300만 건 사이에 있다고 합니다!\n\n모든 방향에서 우리에게 쏟아지는 뉴스로 인해 정신없을 때가 많습니다. 그래서 우리는 실제로 관심 있는 뉴스를 빠르게 받아볼 수 있는 짧고 빠른 방법을 찾고 있습니다.\n\n본 문서에서는 이러한 문제를 해결하고자 합니다! 좀 더 구체적으로, 여러 출처로부터 데이터를 수집하고 해당 정보 채널을 당신의 관심사에 맞는 종단점으로 좁힐 수 있는 시스템을 구축할 것입니다 — 뉴스 검색 엔진입니다!\n\n<div class=\"content-ad\"></div>\n\n이론에 대해서만 이야기하거나 이러한 시스템을 구현하는 방법을 알려주는 것이 아니라, 우리는 단계별로 설명하고 보여줄 거예요!\n\n시작하기 전에, 이 기사에서 제공하는 모든 것은 Decoding ML Articles GitHub 저장소에서 완전한 코드 지원을 받습니다.\n\n# 목차\n\n- 아키텍처 개요\n- 도구 고려 사항\n- 전제 조건\n3.1 새로운 Upstash Kafka 클러스터 생성\n3.2 새로운 Upstash Vector 인덱스 생성\n3.3 2개의 라이브 뉴스 API에 등록\n3.4 설치\n- 데이터 수집\n4.1 기사 가져오기 관리자\n4.2 Kafka 메시지 제작\n4.3 Pydantic을 사용한 데이터 교환 모델\n4.4 KafkaProducers 실행\n- 수집 파이프라인\n5.1 Kafka에서 메시지 수신\n5.2 Bytewax 데이터플로 구현\n5.3 기사 정제, 형식 지정, 청크화, 삽입\n5.4 벡터 작성 및 VectorDB에 업서트\n- 파이프라인 시작\n- 사용자 인터페이스\n- 결론\n- 참고문헌\n\n<div class=\"content-ad\"></div>\n\n# 아키텍처 개요\n\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_0.png)\n\n요약하자면, 우리는 뉴스 API에서 뉴스 기사를 가져와서 생생한 시스템을 구축할 것입니다. 가져온 데이터를 정의된 형식으로 파싱하고 포맷팅한 다음 첫 번째 단계로 Kafka 토픽에 메시지를 직렬화하고 스트리밍할 것입니다. \n두 번째 단계에서는 Bytewax를 사용하여 Kafka 토픽에서 메시지를 더 청소, 파싱, 청크, 임베드, 벡터를 업서팅하여 벡터 데이터베이스로 보내고, 데이터베이스와 상호 작용할 수 있는 UI로 마무리됩니다.\n\n# 툴 고려 사항\n\n<div class=\"content-ad\"></div>\n\n올바른 도구를 선택하는 것이 고성능, 확장성, 및 구현 용이성을 달성하는 핵심이었습니다. 프로젝트 전체에서 사용된 도구를 살펴보겠습니다:\n\n- Upstash Serverless Kafka: 인프라 관리에 대해 걱정할 필요 없이 강력하고 확장 가능한 이벤트 스트리밍을 위해 사용됩니다.\n- Python 스레딩: 여러 뉴스 API에서 동시에 데이터를 가져오면서 스레드 안전한 Kafka Producer 인스턴스를 공유하여 메모리 풋프린트와 성능을 최적화합니다.\n- Pydantic 모델: 일관된 및 유효한 데이터 교환 구조를 보장하기 위해 사용됩니다.\n- Bytewax: 스트리밍 데이터를 처리하고 변환하는 데 간편하고 빠른 속도를 제공하기 때문에 사용됩니다.\n- Upstash Vector Database: Serverless로 구성이 쉽고 Python, JS, 및 GO 내에서 쉽게 통합됩니다. UI 콘솔 대시보드에서 풍부한 탐색 옵션과 실시간 상태 메트릭을 제공하는 것이 큰 장점입니다.\n\n하드웨어 요구 사항에 따르면 GPU는 필요하지 않습니다.\n\n비용은 얼마입니까? — 무료입니다.\n이 안내서는 무료 티어 플랜만 사용하도록 설정했으므로 사용한 플랫폼에 대해 지불할 필요가 없습니다!\n\n<div class=\"content-ad\"></div>\n\n# 준비 사항\n\n어떠한 구현을 하기 전에, 각 서비스에 접근할 수 있는지 확인해야 합니다. 따라서 다음을 설정해야 합니다:\n\n- 새로운 Upstash Kafka 클러스터\n- 새로운 Upstash Vector Index\n- 뉴스 API 등록\n- 환경 설치\n\n처음 시작할 때는 약 5분 정도 걸립니다. 함께 해보세요!\n\n<div class=\"content-ad\"></div>\n\n## 새로운 Upstash Kafka 클러스터 생성\n\n먼저 Upstash에 등록해야 합니다. 로그인 후에 다음과 같은 대시보드가 나타납니다:\n\n![대시보드 이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_1.png)\n\n다음으로 상단 바에서 Kafka를 선택하고 + 클러스터 생성 버튼을 클릭하여 새 클러스터를 만들어야 합니다. 클릭하면 다음 모달이 나타납니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_2.png\" />\n\n클러스터에 이름을 지정하고 본인의 위치와 가장 가까운 지역을 선택한 후, 클러스터 생성을 클릭하여 완료하세요. 완료되면 새로운 Kafka 클러스터가 아래에 표시됩니다. 새로운 Kafka 클러스터를 선택하고 아래 화면으로 이동하게 됩니다:\n\n<img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_3.png\" />\n\n이 뷰에서 주요 구성 요소를 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n- 상세 정보: 클러스터 개요 및 Upstash가 제공하는 기능을 보여줍니다.\n- 사용량: 생성/소비된 메시지 수, 비용 영향 등을 보여주는 차트입니다.\n- 주제: 이 탭에서는 Kafka 주제를 만들고 세부 정보를 모니터링할 수 있습니다.\n\n클러스터를 생성한 다음 해야 할 다음 단계는 메시지를 생성(보내기)하고 소비(받기)할 수 있는 주제를 정의하는 것입니다.\n\n주제 탭 아래에서 \"주제 생성\"을 선택하면 다음과 같은 화면이 나타납니다:\n\n<img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_4.png\" />\n\n<div class=\"content-ad\"></div>\n\n주제 이름을 지정하고, 나머지는 기본 상태로 두어서 Create를 클릭하세요.\n\n카프카 클러스터를 성공적으로 생성했습니다. 이제 클러스터에 연결하는 데 도움이 되는 환경 변수를 복사하고 설정해야 합니다. 이를 위해 클러스터 대시보드로 이동하여 세부 정보 탭에서 엔드포인트, 사용자 이름 및 비밀번호를 복사하여 .env 파일에 붙여넣으세요.\n그 후, Topics로 이동하여 카프카 토픽 이름을 복사하세요.\n\n지금까지 .env 파일이어야 하는 모습은 다음과 같습니다:\n\n```js\nUPSTASH_KAFKA_UNAME=\"[사용자 이름]\"\nUPSTASH_KAFKA_PASS=\"[비밀번호]\"\nUPSTASH_KAFKA_ENDPOINT=\"[엔드포인트]\"\nUPSTASH_KAFKA_TOPIC=\"[토픽 이름]\"\n```\n\n<div class=\"content-ad\"></div>\n\n## 새 Upstash Vector 색인 만들기\n\n이제 새로운 Vector 데이터베이스를 만들어 보겠습니다. 이를 위해 대시보드에서 Vector를 선택하고 + Index 작성을 클릭하세요. 그러면 다음 뷰로 이동됩니다:\n\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_5.png)\n\n벡터 데이터베이스에 이름을 할당하고, 위치에 가장 가까운 지역을 선택한 다음 Embedding을 생성할 때 사용할 모델로 sentence-transformers/all-MiniLM-L6-v2을 선택하세요. 뉴스 기사의 임베딩을 생성할 때 사용할 모델과 벡터 간 거리 비교에 코사인 유사도 메트릭을 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n새로운 Vector Index를 만든 후에는 Kafka Cluster와 동일한 작업 흐름을 따를 수 있습니다. Index Name, Endpoint, Token을 복사하고 .env 파일에 붙여넣기하세요.\n\n현재 .env 파일은 다음과 같이 보여야 합니다:\n\n```js\nUPSTASH_KAFKA_UNAME=\"[여기에 사용자명 입력]\"\nUPSTASH_KAFKA_PASS=\"[여기에 비밀번호 입력]\"\nUPSTASH_KAFKA_ENDPOINT=\"[여기에 엔드포인트 입력]\"\nUPSTASH_KAFKA_TOPIC=\"[여기에 토픽 이름 입력]\"\n\nUPSTASH_VECTOR_ENDPOINT=\"[Vector 엔드포인트 입력]\"\nUPSTASH_VECTOR_TOPIC=\"[Vector 이름 입력]\"\nUPSTASH_VECTOR_KEY=\"[Vector 토큰 입력]\"\n``` \n\n## 뉴스 API에 등록하기\n\n<div class=\"content-ad\"></div>\n\n다음 APIs를 사용하여 기사를 가져올 예정입니다:\n\n- 🔗 NewsAPI\n\n하루에 100번의 API 호출을 할 수 있는 무료 개발자 플랜을 제공합니다.\n\n2. 🔗 NewsData\n\n<div class=\"content-ad\"></div>\n\n무료 요금제가 제공되며 하루에 200개의 크레딧을 받습니다. 각 크레딧은 10개의 기사와 동일하며, 이는 하루에 총 2000개의 기사를 가져올 수 있다는 것을 의미합니다.\n\n저희 현재의 사용 사례에는 이 API들이 충분한 기능을 제공하여 구축 중인 뉴스 검색 엔진을 구현하고 유효성을 검사할 수 있습니다. 동시에 기존 워크플로우가 동일하게 유지되므로 개선 및 확장할 여지도 남겨두고 있습니다.\n무료 요금제에 따른 유일한 제약은 타임드-배치 페치를 수행할 수 없다는 것입니다. 즉, 이 API들을 쿼리할 때 from_date, to_date를 사용할 수 없습니다. 하지만 이는 문제가 되지 않습니다.\n대신 페치 호출 간의 대기 시간을 이용하여 이 동작을 모방할 예정입니다.\n\n다음 단계는 두 플랫폼에 등록하는 것입니다 — 걱정 마세요, 가능한 간단합니다.\n\n- NewsAPI에 등록한 후, /account로 이동하여 API_KEY 필드를 확인한 후 이를 .env 파일의 NEWSAPI_KEY에 복사하여 붙여넣으십시오.\n- NewsData에 등록한 후, /api-key로 이동하여 API KEY를 확인한 후 이를 .env 파일의 NEWSDATAIO_KEY에 복사하여 붙여넣으십시오.\n\n<div class=\"content-ad\"></div>\n\n지루한 부분은 끝났습니다. 이제 이러한 API에 액세스할 수 있고, 기사를 가져올 수 있습니다. 각 API에서 페이로드가 어떻게 보이는지 살펴봅시다:\n\n![image](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_6.png)\n\n## 사전 준비 조치 요약\n\nKafka 클러스터를 생성하고, 벡터 인덱스를 생성하고, 뉴스 API에 등록하는 이 3단계를 모두 마친 후에 .env 파일은 다음과 같은 모습이어야 합니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nUPSTASH_KAFKA_UNAME=\"[여기에 사용자 이름 입력]\"\nUPSTASH_KAFKA_PASS=\"[여기에 암호 입력]\"\nUPSTASH_KAFKA_ENDPOINT=\"[여기에 엔드포인트 입력]\"\nUPSTASH_KAFKA_TOPIC=\"[토픽 이름 입력]\"\n\nUPSTASH_VECTOR_ENDPOINT=\"[벡터 엔드포인트 입력]\"\nUPSTASH_VECTOR_TOPIC=\"[벡터 이름 입력]\"\nUPSTASH_VECTOR_KEY=\"[벡터 토큰 입력]\"\n\nNEWSAPI_KEY=\"[NEWSAPI 키 입력]\"\nNEWSDATAIO_KEY=\"[NEWSDATA 키 입력]\"\nNEWS_TOPIC = \"news\" # 이것은 가져올 기사의 카테고리입니다\n\n다음 단계는 구현 세부 정보에 들어가기 전에 환경과 필수 패키지를 설치하는 것입니다.\n다음은 Makefile 설치 단계의 모습입니다:\n\n# Makefile\n...\ninstall:\n @echo \"$(GREEN) [CONDA] [$(ENV_NAME)] 파이썬 환경 생성 $(RESET)\"\n conda create --name $(ENV_NAME) python=3.9 -y\n @echo \"환경 활성화 중...\"\n @bash -c \"source $$(conda info --base)/etc/profile.d/conda.sh && conda activate $(ENV_NAME) \\\n   && pip install poetry \\\n   poetry env use $(which python)\"\n @echo \"패키지 설치 중\"\n @echo \"pyproject.toml 위치로 변경 중...\"\n @bash -c \" PYTHON_KEYRING_BACKEND=keyring.backends.fail.Keyring poetry install\"\n...\n\n환경을 준비하려면 make install을 실행하세요.\n\n<div class=\"content-ad\"></div>\n\n이제 이 소스로부터 기사를 가져오는 핸들러 구현을 조사해 봅시다.\n\n# 데이터 수집\n\n이 모듈의 목적은 두 API를 쿼리하는 기능을 캡슐화하고, 페이로드를 구문 분석하여 두 페이로드에 모두 존재하는 속성을 사용하여 공통 문서 형식으로 포매팅하고, 클러스터로 메시지를 보내기 위해 공유 KafkaProducer 인스턴스를 사용하는 것입니다.\n\n자세히 살펴볼 내용은 다음 하위 모듈들입니다:\n\n<div class=\"content-ad\"></div>\n\n- Articles Fetching Manager Class\n- 카프카 클러스터로 메시지를 보내는 방법\n- Pydantic 데이터 모델\n- 파이프라인 실행\n\n## Articles Fetching Manager Class\n\n구현 내용에 대해 알아보겠습니다:\n\nimport datetime\nimport functools\nimport logging\nfrom typing import Callable, Dict, List\n\nfrom newsapi import NewsApiClient\nfrom newsdataapi import NewsDataApiClient\nfrom pydantic import ValidationError\n\nfrom models import NewsAPIModel, NewsDataIOModel\nfrom settings import settings\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n\ndef handle_article_fetching(func: Callable) -> Callable:\n    \"\"\"\n    뉴스 기사 가져오기 기능에 대한 예외 처리를 담당하는 데코레이터입니다.\n\n    이 데코레이터는 기사 가져오기 기능을 감싸서 발생하는 예외를 catch하고 로깅합니다.\n    오류가 발생하면 오류를 기록하고 빈 목록을 반환합니다.\n\n    Args:\n        func (Callable): 감쌀 기사 가져오기 함수.\n\n    Returns:\n        Callable: 감싼 함수.\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except ValidationError as e:\n            logger.error(f\"기사 처리 중 유효성 검사 오류 발생: {e}\")\n        except Exception as e:\n            logger.error(f\"소스로부터 데이터를 가져오는 도중 오류 발생: {e}\")\n            logger.exception(e)\n        return []\n\n    return wrapper\n\nclass NewsFetcher:\n    \"\"\"\n    다양한 API에서 뉴스 기사를 가져오는 클래스입니다.\n\n    속성:\n        _newsapi (NewsApiClient): NewsAPI 클라이언트.\n        _newsdataapi (NewsDataApiClient): NewsDataAPI 클라이언트.\n\n    메서드:\n        fetch_from_newsapi(): NewsAPI로부터 기사 가져오기.\n        fetch_from_newsdataapi(): NewsDataAPI로부터 기사 가져오기.\n        sources: 호출 가능한 가져오기 함수 목록을 반환합니다.\n    \"\"\"\n\n    def __init__(self):\n        self._newsapi = NewsApiClient(api_key=settings.NEWSAPI_KEY)\n        self._newsdataapi = NewsDataApiClient(apikey=settings.NEWSDATAIO_KEY)\n\n    @handle_article_fetching\n    def fetch_from_newsapi(self) -> List[Dict]:\n        \"\"\"NewsAPI에서 상위 뉴스 가져오기.\"\"\"\n        response = self._newsapi.get_everything(\n            q=settings.NEWS_TOPIC,\n            language=\"en\",\n            page=settings.ARTICLES_BATCH_SIZE,\n            page_size=settings.ARTICLES_BATCH_SIZE,\n        )\n        return [\n            NewsAPIModel(**article).to_common()\n            for article in response.get(\"articles\", [])\n        ]\n\n    @handle_article_fetching\n    def fetch_from_newsdataapi(self) -> List[Dict]:\n        \"\"\"NewsDataAPI에서 뉴스 데이터 가져오기.\"\"\"\n        response = self._newsdataapi.news_api(\n            q=settings.NEWS_TOPIC,\n            language=\"en\",\n            size=settings.ARTICLES_BATCH_SIZE,\n        )\n        return [\n            NewsDataIOModel(**article).to_common()\n            for article in response.get(\"results\", [])\n        ]\n\n    @property\n    def sources(self) -> List[callable]:\n        \"\"\"뉴스 가져오기 함수 목록입니다.\"\"\"\n        return [self.fetch_from_newsapi, self.fetch_from_newsdataapi]\n\n<div class=\"content-ad\"></div>\n\n이 구현에서 고려해야 할 몇 가지 중요 사항이 있습니다:\n\n- NewsAPIModel과 NewsDataIOModel은 특정 페이로드 형식에 익숙한 Pydantic 모델입니다.\n- 우리는 handle_article_fetching 데코레이터를 사용하여 원시 페이로드를 Pydantic 모델로 변환할 때 유효성 오류나 더 넓은 예외를 잡습니다.\n- 우리에게는 API를 쿼리하는 callable 메서드를 반환하는 sources라는 속성이 있습니다. 이것은 데이터 수집 모듈 내에서 사용될 것이며 멀티 프로듀서 스레드를 생성하여 Kafka 클러스터로 메시지를 전송합니다. 다음에 이어서 설명하겠습니다.\n\n## Kafka 메시지 생성\n\n다음에 우리가 구현할 작업 흐름입니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_7.png\" />\n\n여기서 중요한 포인트들입니다:\n\n- API에서 가져오는 작업에 별도 스레드를 사용합니다.\n- 메시지를 보내기 위해 공통 카프카 프로듀서 인스턴스를 공유합니다.\n- 데이터 교환을 보증하기 위해 Pydantic 모델을 사용합니다.\n\n기사를 가져오는 데 별도 스레드를 사용하고, 클러스터로 메시지를 보내기 위해 단일 카프카 프로듀서 인스턴스를 사용하는 것이 우리의 사용 사례에서 권장되는 방법입니다. 그 이유는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- 효율성 및 성능: KafkaProducer는 스레드 안전합니다. 새 인스턴스를 만드는 것은 네트워크 연결과 일부 설정이 필요합니다. 여러 스레드 간에 하나의 단일 인스턴스를 공유하면 이러한 작업과 관련된 오버헤드를 줄일 수 있습니다.\n- 처리량: 단일 프로듀서 인스턴스는 메시지를 Kafka 클러스터로 보내기 전에 메시지를 일괄 처리합니다.\n- 자원: 사용 사례에 완전히 적용되지는 않지만, 우리는 오직 2개의 프로듀서 스레드만 가지고 있기 때문에 인스턴스 수를 제한함으로써 시스템 자원 이용률을 최적화할 수 있습니다.\n\n여기 Kafka로 메시지 처리를 담당하는 주요 기능이 있습니다:\n\ndef run(self) -> NoReturn:\n        \"\"\"지속적으로 Kafka 주제로 메시지를 가져와 보냅니다.\"\"\"\n        while self.running.is_set():\n            try:\n                messages: List[CommonDocument] = self.fetch_function()\n                if messages:\n                    messages = [msg.to_kafka_payload() for msg in messages]\n                    self.producer.send(self.topic, value=messages)\n                    self.producer.flush()\n                logger.info(\n                    f\"프로듀서 : {self.producer_id}이(가) {len(messages)}개의 메시지를 전송함.\"\n                )\n                time.sleep(self.wait_window_sec)\n            except Exception as e:\n                logger.error(f\"프로듀서 작업자 {self.producer_id}에서 오류 발생: {e}\")\n                self.running.clear()  # 오류 시 스레드를 중지합니다\n\n구현에서 고려해야 할 중요 사항:\n\n<div class=\"content-ad\"></div>\n\n- 우리는 fetch sources의 수만큼 KafkaProducerThread 인스턴스가 생성됩니다.\n- 우리는 모든 스레드를 KafkaProducerSwarm 아래에 랩합니다.\n- 모든 스레드 사이에서 단일 KafkaProducer 인스턴스를 공유하며, 이는 클러스터와 통신할 것입니다.\n- 우리는 N개의 fetching 스레드로 확장할 수 있지만 여전히 단일 KafkaProducer 인스턴스를 유지하기 위해 싱글톤 디자인 패턴을 따릅니다.\n\n## Pydantic을 사용한 데이터 교환 모델\n\n위에서 제시한 코드 스니펫 구현에서, 이전에 설명되지 않았던 *Document, *Model 객체의 사용을 관찰했을 수 있습니다. 이 섹션에서 이들이 무엇인지 자세히 살펴보겠습니다.\n\n이들은 데이터 교환을 위한 Pydantic 모델들이며, 우리가 구축 중인 응용 프로그램 내에서 이러한 모델들은:\n\n<div class=\"content-ad\"></div>\n\n- NewsDataIOModel: NewsData API에서 가져온 원시 페이로드를 래핑하고 포맷합니다.\n- NewsAPIModel: NewsAPI API에서 가져온 원시 페이로드를 래핑하고 포맷합니다.\n- CommonDocument: 위에서 언급한 다양한 뉴스 형식 사이의 공통 형식을 설정합니다.\n- RefinedDocument: metadata 아래에 유용한 필드를 그룹화하고 기사 설명 텍스트와 같은 주요 필드를 강조하는 공통 형식을 필터링합니다.\n- ChunkedDocument: 텍스트를 청크로 나누고 chunk_id와 document_id 사이의 계보를 보장합니다.\n- EmbeddedDocument: 청크를 임베드하여 chunk_id와 document_id 사이의 계보를 보장합니다.\n\n예를 들어, 위 CommonDocument 모델은 다양한 뉴스 페이로드 형식 사이의 연결 역할을 나타내므로 이와 같이 구성됩니다:\n\nclass CommonDocument(BaseModel):\n    article_id: str = Field(default_factory=lambda: str(uuid4()))\n    title: str = Field(default_factory=lambda: \"N/A\")\n    url: str = Field(default_factory=lambda: \"N/A\")\n    published_at: str = Field(\n        default_factory=lambda: datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    )\n    source_name: str = Field(default_factory=lambda: \"Unknown\")\n    image_url: Optional[str] = Field(default_factory=lambda: None)\n    author: Optional[str] = Field(default_factory=lambda: \"Unknown\")\n    description: Optional[str] = Field(default_factory=lambda: None)\n    content: Optional[str] = Field(default_factory=lambda: None)\n\n    @field_validator(\"title\", \"description\", \"content\")\n    def clean_text_fields(cls, v):\n        if v is None or v == \"\":\n            return \"N/A\"\n        return clean_full(v)\n\n    @field_validator(\"url\", \"image_url\")\n    def clean_url_fields(cls, v):\n        if v is None:\n            return \"N/A\"\n        v = remove_html_tags(v)\n        v = normalize_whitespace(v)\n        return v\n\n    @field_validator(\"published_at\")\n    def clean_date_field(cls, v):\n        try:\n            parsed_date = parser.parse(v)\n            return parsed_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n        except (ValueError, TypeError):\n            logger.error(f\"Error parsing date: {v}, using current date instead.\")\n\n    @classmethod\n    def from_json(cls, data: dict) -> \"CommonDocument\":\n        \"\"\"JSON 객체에서 CommonDocument를 만듭니다.\"\"\"\n        return cls(**data)\n\n    def to_kafka_payload(self) -> dict:\n        \"\"\"Kafka 페이로드의 공통 표현을 준비합니다.\"\"\"\n        return self.model_dump(exclude_none=False)\n\n해석해보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n- 뉴스 기사 형식에 공통 속성 시리즈가 포함되어 있습니다.\n- 각 필드를 유효성 검사하거나 field_validator 데코레이터를 사용하여 기본값을 지정합니다.\n- to_kafka_payload 메서드는 메시지 직렬화를 보장하여 Kafka 클러스터로 전송하기 전에 처리합니다.\n\n## 텍스트 필드 클린업 프로세스\n\n클린업 프로세스는 간단합니다. 텍스트를 정리하고 다음을 보장하기 위해 메서드를 사용합니다:\n\n- 끝에 있는 공백이나 \\n, \\t를 제거합니다.\n- ul/li 목록 항목을 제거합니다.\n- 텍스트 내에 HTML 태그가 있으면 제거합니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 이러한 변환을 간소화하기 위해 구조화되지 않은 [7] Python 라이브러리를 사용하고 있습니다.\n\n## KafkaProducers 실행\n\n지금까지 다음 모듈을 수행/구현했습니다:\n\n- 필요한 모든 서비스에 등록\n- Kafka 클러스터 및 벡터 데이터베이스 생성\n- 뉴스 기사 검색 핸들러 구현\n- 데이터 교환을 위한 Pydantic 모델 구현\n- KafkaProducer 로직 구현\n\n<div class=\"content-ad\"></div>\n\n작업이 완료되면 이제 안전하게 우리의 파이프라인에서 생산 단계를 실행하고 Upstash의 KafkaCluster에서 메시지를 확인할 수 있습니다.\n\n그럼 시작해봐요!\n프로젝트의 루트 디렉토리에서, Makefile에 데이터 수집을 실행하는 명령어가 있습니다:\n\n....\n\nrun_producers:\n @echo \"$(GREEN) [실행 중] 데이터 수집 파이프라인 Kafka 프로듀서 $(RESET)\"\n @bash -c \"poetry run python -m src.producer\"\n\n...\n\n이 🔗Makefile은 우리가 구축 중인 솔루션과 상호작용하기 위한 유용한 명령어가 포함되어 있습니다. 이 경우에는 make run_producers를 사용하여 run_producers를 실행해야 합니다. 이렇게 하면 KafkaSwarm이 시작되고 NewsAPIs에서 기사를 가져와 형식을 지정한 다음 Kafka 클러스터로 보내는 스레드를 다룰 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_8.png)\n\n로그를 통해 프로듀서 스레드가 각각 5개의 메시지를 보냈다는 것을 확인했습니다. 메시지들이 클러스터에 도달했는지 확인하려면 Upstash 콘솔로 이동하여 Kafka 클러스터 → 메시지를 확인하십시오. 다음과 같은 화면이 나타날 것입니다:\n\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_9.png)\n\n이 시점에서는 API에서 뉴스 기사를 가져와 형식을 맞춘 후 Kafka로 메시지를 보내는 데이터 수집 파이프라인의 구현 및 테스트가 완료되었습니다. 다음으로는 Kafka에서 새 메시지를 처리하는 \"컨슈머\" 또는 적재 파이프라인을 구현할 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 데이터 수집 파이프라인\n\n우리가 Kafka 주제에서 메시지를 받았다는 것을 확인한 후에는 \"소비자\" 파이프라인을 구현해야 합니다. 이는 다음을 의미합니다:\n\n- Kafka 주제에서 메시지 읽기\n- 파싱, 형식 지정, 청크화, 임베딩 생성\n- 벡터 객체 생성 및 Upstash Vector Index에 업서트\n\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_10.png)\n\n<div class=\"content-ad\"></div>\n\n이를 위해 Bytewax [4]를 사용하여 이러한 단계를 올바른 순서로 연결하는 DataFlow를 정의할 것입니다.\n\n바로 구현에 들어가서 주요 개념을 설명해보겠습니다!\n\n- Bytewax Flow에 입력으로 Kafka Source를 정의합니다.\n\n```js\nimport json\nfrom typing import List\n\nfrom bytewax.connectors.kafka import KafkaSinkMessage, KafkaSource\n\nfrom logger import get_logger\nfrom models import CommonDocument\nfrom settings import settings\n\nlogger = get_logger(__name__)\n\ndef build_kafka_stream_client():\n    \"\"\"\n    Build a Kafka stream client to read messages from the Upstash Kafka topic using the ByteWax KafkaSource connector.\n    \"\"\"\n    kafka_config = {\n        \"bootstrap.servers\": settings.UPSTASH_KAFKA_ENDPOINT,\n        \"security.protocol\": \"SASL_SSL\",\n        \"sasl.mechanisms\": \"SCRAM-SHA-256\",\n        \"sasl.username\": settings.UPSTASH_KAFKA_UNAME,\n        \"sasl.password\": settings.UPSTASH_KAFKA_PASS,\n        \"auto.offset.reset\": \"earliest\",  # Start reading at the earliest message\n    }\n    kafka_input = KafkaSource(\n        topics=[settings.UPSTASH_KAFKA_TOPIC],\n        brokers=[settings.UPSTASH_KAFKA_ENDPOINT],\n        add_config=kafka_config,\n    )\n    logger.info(\"KafkaSource client created successfully.\")\n    return kafka_input\n\ndef process_message(message: KafkaSinkMessage):\n    \"\"\"\n    On a Kafka message, process the message and return a list of CommonDocuments.\n    - message: KafkaSinkMessage(key, value) where value is the message payload.\n    \"\"\"\n    documents: List[CommonDocument] = []\n    try:\n        json_str = message.value.decode(\"utf-8\")\n        data = json.loads(json_str)\n        documents = [CommonDocument.from_json(obj) for obj in data]\n        logger.info(f\"Decoded into {len(documents)} CommonDocuments\")\n        return documents\n    except StopIteration:\n        logger.info(\"No more documents to fetch from the client.\")\n    except KeyError as e:\n        logger.error(f\"Key error in processing document batch: {e}\")\n    except json.JSONDecodeError as e:\n        logger.error(f\"Error decoding JSON from message: {e}\")\n        raise\n    except Exception as e:\n        logger.exception(f\"Unexpected error in next_batch: {e}\")\n```\n\n<div class=\"content-ad\"></div>\n\n이 구현에서 중요한 점들:\n\n- build_kafka_stream_client : 미리 정의된 Bytewax KafkaSource 커넥터를 사용하여 KafkaConsumer의 인스턴스를 생성합니다.\n- process_message : Kafka Topic에서 메시지를 처리할 콜백 함수입니다.\n\n2. Bytewax 플로우의 출력으로 Upstash Vector (Index)를 정의합니다.\n\n```js\nfrom typing import Optional, List\n\nfrom bytewax.outputs import DynamicSink, StatelessSinkPartition\nfrom upstash_vector import Index, Vector\nfrom models import EmbeddedDocument\nfrom settings import settings\nfrom logger import get_logger\n\n\nlogger = get_logger(__name__)\n\n\nclass UpstashVectorOutput(DynamicSink):\n    \"\"\"Upstash 벡터 출력을 나타내는 클래스입니다.\n\n    이 클래스는 at-least-once 처리를 지원하는 동적 출력 유형인 Upstash 벡터 출력을 생성하는 데 사용됩니다.\n    resume 이후의 메시지는 resume 즉각적으로 복제됩니다.\n\n    Args:\n        vector_size (int): 벡터의 크기.\n        collection_name (str, optional): 컬렉션의 이름입니다. 기본값은 constants.VECTOR_DB_OUTPUT_COLLECTION_NAME입니다.\n        client (Optional[UpstashClient], optional): Upstash 클라이언트입니다. 기본값은 None입니다.\n    \"\"\"\n\n    def __init__(\n        self,\n        vector_size: int = settings.EMBEDDING_MODEL_MAX_INPUT_LENGTH,\n        collection_name: str = settings.UPSTASH_VECTOR_TOPIC,\n        client: Optional[Index] = None,\n    ):\n        self._collection_name = collection_name\n        self._vector_size = vector_size\n\n        if client:\n            self.client = client\n        else:\n            self.client = Index(\n                url=settings.UPSTASH_VECTOR_ENDPOINT,\n                token=settings.UPSTASH_VECTOR_KEY,\n                retries=settings.UPSTASH_VECTOR_RETRIES,\n                retry_interval=settings.UPSTASH_VECTOR_WAIT_INTERVAL,\n            )\n\n    def build(\n        self, step_id: str, worker_index: int, worker_count: int\n    ) -> StatelessSinkPartition:\n        return UpstashVectorSink(self.client, self._collection_name)\n\n\nclass UpstashVectorSink(StatelessSinkPartition):\n    \"\"\"\n    Upstash Vector 데이터베이스 컬렉션에 문서 임베딩을 작성하는 싱크입니다.\n    이 구현은 효율성을 높이기 위해 배치 업서트를 활용하며, 오류 처리 및 로깅을 향상시키고 가독성 및 유지 보수성을 위해 Pythonic한 모법을 따릅니다.\n\n    Args:\n        client (Index): 쓰기에 사용할 Upstash Vector 클라이언트입니다.\n        collection_name (str, optional): 쓸 컬렉션의 이름입니다. 기본값은 UPSTASH_VECTOR_TOPIC 환경 변수의 값입니다.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Index,\n        collection_name: str = None,\n    ):\n        self._client = client\n        self._collection_name = collection_name\n        self._upsert_batch_size = settings.UPSTASH_VECTOR_UPSERT_BATCH_SIZE\n\n    def write_batch(self, documents: List[EmbeddedDocument]):\n        \"\"\"\n        구성된 Upstash Vector 데이터베이스 컬렉션에 문서 임베딩의 배치를 작성합니다.\n\n        Args:\n            documents (List[EmbeddedDocument]): 쓸 문서들입니다.\n        \"\"\"\n        vectors = [\n            Vector(id=doc.doc_id, vector=doc.embeddings, metadata=doc.metadata)\n            for doc in documents\n        ]\n\n        # 효율성을 위한 배치 업서트\n        for i in range(0, len(vectors), self._upsert_batch_size):\n            batch_vectors = vectors[i : i + self._upsert_batch_size]\n            try:\n                self._client.upsert(vectors=batch_vectors)\n            except Exception as e:\n                logger.error(f\"배치 업서트 중 예외 발생 {e}\")\n```\n\n<div class=\"content-ad\"></div>\n\n이 구현에서 중요한 사항들입니다:\n\n- UpstashVectorOutput: 다양한 대상으로 데이터를 전달하기 위해 설계된 Bytewax DynamicSink 추상화를 인스턴스화합니다. 우리의 경우, 이는 Upstash Vector Index 클라이언트 연결 위에 래핑될 것입니다.\n- UpstashVectorSink: 우리의 DynamicSink을 래핑하고 업서트 벡터를 우리의 VectorDatabase에 처리하는 기능을 담당합니다. 이 StatelessSinkPartition은 DynamicSink가 어떠한 상태도 유지하지 않고 우리의 Sink에 대한 모든 입력을 write_batch 기능 구현에 따라 처리합니다.\n\n## 나머지 Bytewax Flow 빌드하기\n\n여기 Upstash Kafka Topic에서 메시지를 가져와 정제, 수정, 분할, 삽입하고 Upstash Vector Index에 벡터를 업서트하는 저희 DataFlow의 전체 구현입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n\"\"\"\n    이 스크립트는 Upstash 사용 사례에 대한 ByteWax 데이터플로 구현을 정의합니다.\n    데이터플로에는 다음 단계가 포함되어 있습니다:\n        1. 입력: 카프카 스트림에서 데이터를 읽기\n        2. 정제: 입력 데이터를 공통 형식으로 변환 \n        3. 청크 분리: 입력 데이터를 더 작은 청크로 분리\n        4. 임베드: 입력 데이터에 대한 임베딩 생성\n        5. 출력: 출력 데이터를 Upstash 벡터 데이터베이스에 쓰기\n\"\"\"\n\nfrom pathlib import Path\nfrom typing import Optional\n\nimport bytewax.operators as op\nfrom vector import UpstashVectorOutput\nfrom consumer import process_message, build_kafka_stream_client\nfrom bytewax.connectors.kafka import KafkaSource\nfrom bytewax.dataflow import Dataflow\nfrom bytewax.outputs import DynamicSink\nfrom embeddings import TextEmbedder\nfrom models import ChunkedDocument, EmbeddedDocument, RefinedDocument\nfrom logger import get_logger\n\nlogger = get_logger(__name__)\n\n\ndef build(\n    model_cache_dir: Optional[Path] = None,\n) -> Dataflow:\n    \"\"\"\n    Upstash 사용 사례에 대한 ByteWax 데이터플로를 구축합니다.\n    다음과 같은 데이터플로를 따릅니다:\n        * 1. Tag: ['kafka_input']   = KafkaSource에서 입력 데이터 읽기\n        * 2. Tag: ['map_kinp']      = KafkaSource에서 CommonDocument로 메시지 처리\n            * 2.1 [Optional] Tag ['dbg_map_kinp'] = ['map_kinp'] 후 디버깅\n        * 3. Tag: ['refine']        = 메시지를 정제된 문서 형식으로 변환\n            * 3.1 [Optional] Tag ['dbg_refine'] = ['refine'] 후 디버깅\n        * 4. Tag: ['chunkenize']    = 정제된 문서를 더 작은 청크로 나누기\n            * 4.1 [Optional] Tag ['dbg_chunkenize'] = ['chunkenize'] 후 디버깅\n        * 5. Tag: ['embed']         = 청크에 대한 임베딩 생성\n            * 5.1 [Optional] Tag ['dbg_embed'] = ['embed'] 후 디버깅\n        * 6. Tag: ['output']        = 임베딩을 Upstash 벡터 데이터베이스에 쓰기\n    노트:\n        각 선택적 태그는 문제 해결을 위해 활성화할 수 있는 디버깅 단계입니다.\n    \"\"\"\n    model = TextEmbedder(cache_dir=model_cache_dir)\n\n    dataflow = Dataflow(flow_id=\"news-to-upstash\")\n    stream = op.input(\n        step_id=\"kafka_input\",\n        flow=dataflow,\n        source=_build_input(),\n    )\n    stream = op.flat_map(\"map_kinp\", stream, process_message)\n    # _ = op.inspect(\"dbg_map_kinp\", stream)\n    stream = op.map(\"refine\", stream, RefinedDocument.from_common)\n    # _ = op.inspect(\"dbg_refine\", stream)\n    stream = op.flat_map(\n        \"chunkenize\",\n        stream,\n        lambda refined_doc: ChunkedDocument.from_refined(refined_doc, model),\n    )\n    # _ = op.inspect(\"dbg_chunkenize\", stream)\n    stream = op.map(\n        \"embed\",\n        stream,\n        lambda chunked_doc: EmbeddedDocument.from_chunked(chunked_doc, model),\n    )\n    # _ = op.inspect(\"dbg_embed\", stream)\n    stream = op.output(\"output\", stream, _build_output())\n    logger.info(\"성공적으로 ByteWax 데이터플로를 생성했습니다.\")\n    logger.info(\n        \"\\t단계: Kafka 입력 -> 매핑 -> 정제 -> 청크 분리 -> 임베딩 -> 업로드\"\n    )\n    return dataflow\n\n\ndef _build_input() -> KafkaSource:\n    return build_kafka_stream_client()\n\n\ndef _build_output() -> DynamicSink:\n    return UpstashVectorOutput()\n```\n\n<div class=\"content-ad\"></div>\n\n- kafka_input: 카프카 메시지를 가져와 CommonDocument Pydantic 형식으로 변환하는 단계입니다.\n- map_kinp: 카프카 입력을 의미하며, 수신된 메시지에 flat map을 적용하여 List[CommonDocument] Pydantic 객체를 생성합니다.\n- refine: List[CommonDocument]를 순회하고 RefinedDocument 인스턴스를 생성하는 단계입니다.\n- chunkenize: List[RefinedDocument]를 순회하고 ChunkedDocument 인스턴스를 생성하는 단계입니다.\n- embed: List[ChunkedDocuments]를 순회하고 EmbeddedDocument 인스턴스를 생성하는 단계입니다.\n- output: List[EmbeddedDocument]를 순회하고 Vector 객체를 생성하여 Upstash Vector Index에 업서트하는 단계입니다.\n\n# 파이프라인 시작\n\n지금까지 구현한 것은 다음과 같습니다:\n\n- 데이터 수집 파이프라인: 주기적으로 NewsAPI에서 원시 페이로드를 가져와 형식을 지정한 뒤, 카프카 토픽으로 메시지를 전송하는 단계입니다.\n- 인제션 파이프라인: 이는 Bytewax DataFlow로, 카프카 토픽에 연결되어 메시지를 소비하고, 최종적으로 벡터를 업서트하는 Vector 데이터베이스에 업데이트합니다.\n\n<div class=\"content-ad\"></div>\n\n프로젝트 루트에있는 Makefile에서 미리 정의된 명령을 사용하여 이 두 서비스를 모두 시작할 수 있습니다:\n\n```js\n# 카프카 메시지를 생성하기 위해 데이터 수집 파이프라인 실행\nmake run_producers\n\n# 카프카 메시지를 소비하고 벡터를 업데이트하기 위해 인제스처리 파이프라인 실행\nmake run_pipeline\n```\n\n그리고... 완료되었습니다!\n성공적으로 프로듀서/컨슈머 서비스를 시작했습니다.\n남은 모듈은 UI입니다. 벡터 데이터베이스와 뉴스 기사를 검색하는 데 상호 작용하는 데 사용됩니다.\n\n# 사용자 인터페이스\n\n<div class=\"content-ad\"></div>\n\nUI는 다음과 같은 기능을 갖춘 기본 Streamlit [8] 애플리케이션입니다:\n\n- 텍스트 검색 창\n- 벡터 데이터베이스에서 가져온 기사로 채워진 카드가 표시되는 div 섹션\n\n카드에는 다음과 같은 데이터 필드가 포함되어 있습니다:\n\n- 발행일\n- 유사도 점수\n- 기사 이미지\n- SeeMore 버튼을 클릭하면 원본 기사 URL로 이동합니다.\n\n<div class=\"content-ad\"></div>\n\n한번 메시지/질문을 텍스트 상자에 입력하면 입력이 정리되고 (소문자로 변환, 비ASCII 문자 제거 등) 그리고 삽입됩니다. 새로운 삽입물을 사용하여 벡터 데이터베이스를 쿼리하여 가장 유사한 항목을 가져옵니다. 그 결과는 구성되어 렌더링될 것입니다.\n\n다음은 예시입니다:\n\n\n![How to build a real-time News Search Engine using VectorDBs - Part 1](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_11.png)\n\n![How to build a real-time News Search Engine using VectorDBs - Part 2](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_12.png)\n\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n축하합니다!\n\n성공했습니다! 멋진 프로젝트만큼이나 라이브로 출시할 준비가 된 뉴스 검색 엔진을 만들었습니다. 단순히 무작정 던지는 것이 아니라, 우리는 최고의 소프트웨어 개발 관행을 따르기도 했습니다.\n\nPydantic을 사용하여 데이터를 잘 처리했고, 유닛 테스트를 작성하고, 스레딩을 활용하여 작업을 가속화했으며, Upstash의 서버리스 카프카와 벡터 데이터베이스를 활용하여 파이프라인을 쉽게 설정할 뿐만 아니라 빠르고 확장 가능하며 오류 대비 가능하도록 만들었습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 당신은 이 청사진을 대부분의 데이터 기반 아이디어에 적용할 수 있는 능력을 갖게 되었어요. 이건 이 프로젝트뿐만 아니라 앞으로 만들게 될 멋진 것들을 위한 큰 승리에요.\n\n# 참고 자료\n\n[1] News Search Engine using Upstash Vector — Decoding ML Github (2024)\n[2] Upstash Serverless Kafka\n[3] Upstash Serverless Vector Database\n[4] Bytewax Stream Processing with Python\n[5] Singleton Pattern\n[6] sentence-transformers/all-MiniLM-L6-v2\n[7] unstructured Python Library\n[8] Streamlit Python\n\n# 더 읽을 거리\n\n<div class=\"content-ad\"></div>\n\n이 글과 관련성 순으로 정렬되었습니다.","ogImage":{"url":"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_0.png"},"coverImage":"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_0.png","tag":["Tech"],"readingTime":28},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>아파치 카프카, 바이트왁스, 그리고 업스태시 벡터 데이터베이스를 활용한 라이브 뉴스 집계 스트리밍 파이프라인 구현을 위한 실용적인 안내서입니다.</p>\n<p>earthweb.com에서 실시한 연구에 따르면, 매일 온라인 및 오프라인에서 유입되는 뉴스 기사는 200-300만 건 사이에 있다고 합니다!</p>\n<p>모든 방향에서 우리에게 쏟아지는 뉴스로 인해 정신없을 때가 많습니다. 그래서 우리는 실제로 관심 있는 뉴스를 빠르게 받아볼 수 있는 짧고 빠른 방법을 찾고 있습니다.</p>\n<p>본 문서에서는 이러한 문제를 해결하고자 합니다! 좀 더 구체적으로, 여러 출처로부터 데이터를 수집하고 해당 정보 채널을 당신의 관심사에 맞는 종단점으로 좁힐 수 있는 시스템을 구축할 것입니다 — 뉴스 검색 엔진입니다!</p>\n<div class=\"content-ad\"></div>\n<p>이론에 대해서만 이야기하거나 이러한 시스템을 구현하는 방법을 알려주는 것이 아니라, 우리는 단계별로 설명하고 보여줄 거예요!</p>\n<p>시작하기 전에, 이 기사에서 제공하는 모든 것은 Decoding ML Articles GitHub 저장소에서 완전한 코드 지원을 받습니다.</p>\n<h1>목차</h1>\n<ul>\n<li>아키텍처 개요</li>\n<li>도구 고려 사항</li>\n<li>전제 조건\n3.1 새로운 Upstash Kafka 클러스터 생성\n3.2 새로운 Upstash Vector 인덱스 생성\n3.3 2개의 라이브 뉴스 API에 등록\n3.4 설치</li>\n<li>데이터 수집\n4.1 기사 가져오기 관리자\n4.2 Kafka 메시지 제작\n4.3 Pydantic을 사용한 데이터 교환 모델\n4.4 KafkaProducers 실행</li>\n<li>수집 파이프라인\n5.1 Kafka에서 메시지 수신\n5.2 Bytewax 데이터플로 구현\n5.3 기사 정제, 형식 지정, 청크화, 삽입\n5.4 벡터 작성 및 VectorDB에 업서트</li>\n<li>파이프라인 시작</li>\n<li>사용자 인터페이스</li>\n<li>결론</li>\n<li>참고문헌</li>\n</ul>\n<div class=\"content-ad\"></div>\n<h1>아키텍처 개요</h1>\n<p><img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_0.png\" alt=\"이미지\"></p>\n<p>요약하자면, 우리는 뉴스 API에서 뉴스 기사를 가져와서 생생한 시스템을 구축할 것입니다. 가져온 데이터를 정의된 형식으로 파싱하고 포맷팅한 다음 첫 번째 단계로 Kafka 토픽에 메시지를 직렬화하고 스트리밍할 것입니다.\n두 번째 단계에서는 Bytewax를 사용하여 Kafka 토픽에서 메시지를 더 청소, 파싱, 청크, 임베드, 벡터를 업서팅하여 벡터 데이터베이스로 보내고, 데이터베이스와 상호 작용할 수 있는 UI로 마무리됩니다.</p>\n<h1>툴 고려 사항</h1>\n<div class=\"content-ad\"></div>\n<p>올바른 도구를 선택하는 것이 고성능, 확장성, 및 구현 용이성을 달성하는 핵심이었습니다. 프로젝트 전체에서 사용된 도구를 살펴보겠습니다:</p>\n<ul>\n<li>Upstash Serverless Kafka: 인프라 관리에 대해 걱정할 필요 없이 강력하고 확장 가능한 이벤트 스트리밍을 위해 사용됩니다.</li>\n<li>Python 스레딩: 여러 뉴스 API에서 동시에 데이터를 가져오면서 스레드 안전한 Kafka Producer 인스턴스를 공유하여 메모리 풋프린트와 성능을 최적화합니다.</li>\n<li>Pydantic 모델: 일관된 및 유효한 데이터 교환 구조를 보장하기 위해 사용됩니다.</li>\n<li>Bytewax: 스트리밍 데이터를 처리하고 변환하는 데 간편하고 빠른 속도를 제공하기 때문에 사용됩니다.</li>\n<li>Upstash Vector Database: Serverless로 구성이 쉽고 Python, JS, 및 GO 내에서 쉽게 통합됩니다. UI 콘솔 대시보드에서 풍부한 탐색 옵션과 실시간 상태 메트릭을 제공하는 것이 큰 장점입니다.</li>\n</ul>\n<p>하드웨어 요구 사항에 따르면 GPU는 필요하지 않습니다.</p>\n<p>비용은 얼마입니까? — 무료입니다.\n이 안내서는 무료 티어 플랜만 사용하도록 설정했으므로 사용한 플랫폼에 대해 지불할 필요가 없습니다!</p>\n<div class=\"content-ad\"></div>\n<h1>준비 사항</h1>\n<p>어떠한 구현을 하기 전에, 각 서비스에 접근할 수 있는지 확인해야 합니다. 따라서 다음을 설정해야 합니다:</p>\n<ul>\n<li>새로운 Upstash Kafka 클러스터</li>\n<li>새로운 Upstash Vector Index</li>\n<li>뉴스 API 등록</li>\n<li>환경 설치</li>\n</ul>\n<p>처음 시작할 때는 약 5분 정도 걸립니다. 함께 해보세요!</p>\n<div class=\"content-ad\"></div>\n<h2>새로운 Upstash Kafka 클러스터 생성</h2>\n<p>먼저 Upstash에 등록해야 합니다. 로그인 후에 다음과 같은 대시보드가 나타납니다:</p>\n<p><img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_1.png\" alt=\"대시보드 이미지\"></p>\n<p>다음으로 상단 바에서 Kafka를 선택하고 + 클러스터 생성 버튼을 클릭하여 새 클러스터를 만들어야 합니다. 클릭하면 다음 모달이 나타납니다:</p>\n<div class=\"content-ad\"></div>\n<img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_2.png\">\n<p>클러스터에 이름을 지정하고 본인의 위치와 가장 가까운 지역을 선택한 후, 클러스터 생성을 클릭하여 완료하세요. 완료되면 새로운 Kafka 클러스터가 아래에 표시됩니다. 새로운 Kafka 클러스터를 선택하고 아래 화면으로 이동하게 됩니다:</p>\n<img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_3.png\">\n<p>이 뷰에서 주요 구성 요소를 살펴보겠습니다:</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>상세 정보: 클러스터 개요 및 Upstash가 제공하는 기능을 보여줍니다.</li>\n<li>사용량: 생성/소비된 메시지 수, 비용 영향 등을 보여주는 차트입니다.</li>\n<li>주제: 이 탭에서는 Kafka 주제를 만들고 세부 정보를 모니터링할 수 있습니다.</li>\n</ul>\n<p>클러스터를 생성한 다음 해야 할 다음 단계는 메시지를 생성(보내기)하고 소비(받기)할 수 있는 주제를 정의하는 것입니다.</p>\n<p>주제 탭 아래에서 \"주제 생성\"을 선택하면 다음과 같은 화면이 나타납니다:</p>\n<img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_4.png\">\n<div class=\"content-ad\"></div>\n<p>주제 이름을 지정하고, 나머지는 기본 상태로 두어서 Create를 클릭하세요.</p>\n<p>카프카 클러스터를 성공적으로 생성했습니다. 이제 클러스터에 연결하는 데 도움이 되는 환경 변수를 복사하고 설정해야 합니다. 이를 위해 클러스터 대시보드로 이동하여 세부 정보 탭에서 엔드포인트, 사용자 이름 및 비밀번호를 복사하여 .env 파일에 붙여넣으세요.\n그 후, Topics로 이동하여 카프카 토픽 이름을 복사하세요.</p>\n<p>지금까지 .env 파일이어야 하는 모습은 다음과 같습니다:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-variable constant_\">UPSTASH_KAFKA_UNAME</span>=<span class=\"hljs-string\">\"[사용자 이름]\"</span>\n<span class=\"hljs-variable constant_\">UPSTASH_KAFKA_PASS</span>=<span class=\"hljs-string\">\"[비밀번호]\"</span>\n<span class=\"hljs-variable constant_\">UPSTASH_KAFKA_ENDPOINT</span>=<span class=\"hljs-string\">\"[엔드포인트]\"</span>\n<span class=\"hljs-variable constant_\">UPSTASH_KAFKA_TOPIC</span>=<span class=\"hljs-string\">\"[토픽 이름]\"</span>\n</code></pre>\n<div class=\"content-ad\"></div>\n<h2>새 Upstash Vector 색인 만들기</h2>\n<p>이제 새로운 Vector 데이터베이스를 만들어 보겠습니다. 이를 위해 대시보드에서 Vector를 선택하고 + Index 작성을 클릭하세요. 그러면 다음 뷰로 이동됩니다:</p>\n<p><img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_5.png\" alt=\"이미지\"></p>\n<p>벡터 데이터베이스에 이름을 할당하고, 위치에 가장 가까운 지역을 선택한 다음 Embedding을 생성할 때 사용할 모델로 sentence-transformers/all-MiniLM-L6-v2을 선택하세요. 뉴스 기사의 임베딩을 생성할 때 사용할 모델과 벡터 간 거리 비교에 코사인 유사도 메트릭을 사용할 것입니다.</p>\n<div class=\"content-ad\"></div>\n<p>새로운 Vector Index를 만든 후에는 Kafka Cluster와 동일한 작업 흐름을 따를 수 있습니다. Index Name, Endpoint, Token을 복사하고 .env 파일에 붙여넣기하세요.</p>\n<p>현재 .env 파일은 다음과 같이 보여야 합니다:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-variable constant_\">UPSTASH_KAFKA_UNAME</span>=<span class=\"hljs-string\">\"[여기에 사용자명 입력]\"</span>\n<span class=\"hljs-variable constant_\">UPSTASH_KAFKA_PASS</span>=<span class=\"hljs-string\">\"[여기에 비밀번호 입력]\"</span>\n<span class=\"hljs-variable constant_\">UPSTASH_KAFKA_ENDPOINT</span>=<span class=\"hljs-string\">\"[여기에 엔드포인트 입력]\"</span>\n<span class=\"hljs-variable constant_\">UPSTASH_KAFKA_TOPIC</span>=<span class=\"hljs-string\">\"[여기에 토픽 이름 입력]\"</span>\n\n<span class=\"hljs-variable constant_\">UPSTASH_VECTOR_ENDPOINT</span>=<span class=\"hljs-string\">\"[Vector 엔드포인트 입력]\"</span>\n<span class=\"hljs-variable constant_\">UPSTASH_VECTOR_TOPIC</span>=<span class=\"hljs-string\">\"[Vector 이름 입력]\"</span>\n<span class=\"hljs-variable constant_\">UPSTASH_VECTOR_KEY</span>=<span class=\"hljs-string\">\"[Vector 토큰 입력]\"</span>\n</code></pre>\n<h2>뉴스 API에 등록하기</h2>\n<div class=\"content-ad\"></div>\n<p>다음 APIs를 사용하여 기사를 가져올 예정입니다:</p>\n<ul>\n<li>🔗 NewsAPI</li>\n</ul>\n<p>하루에 100번의 API 호출을 할 수 있는 무료 개발자 플랜을 제공합니다.</p>\n<ol start=\"2\">\n<li>🔗 NewsData</li>\n</ol>\n<div class=\"content-ad\"></div>\n<p>무료 요금제가 제공되며 하루에 200개의 크레딧을 받습니다. 각 크레딧은 10개의 기사와 동일하며, 이는 하루에 총 2000개의 기사를 가져올 수 있다는 것을 의미합니다.</p>\n<p>저희 현재의 사용 사례에는 이 API들이 충분한 기능을 제공하여 구축 중인 뉴스 검색 엔진을 구현하고 유효성을 검사할 수 있습니다. 동시에 기존 워크플로우가 동일하게 유지되므로 개선 및 확장할 여지도 남겨두고 있습니다.\n무료 요금제에 따른 유일한 제약은 타임드-배치 페치를 수행할 수 없다는 것입니다. 즉, 이 API들을 쿼리할 때 from_date, to_date를 사용할 수 없습니다. 하지만 이는 문제가 되지 않습니다.\n대신 페치 호출 간의 대기 시간을 이용하여 이 동작을 모방할 예정입니다.</p>\n<p>다음 단계는 두 플랫폼에 등록하는 것입니다 — 걱정 마세요, 가능한 간단합니다.</p>\n<ul>\n<li>NewsAPI에 등록한 후, /account로 이동하여 API_KEY 필드를 확인한 후 이를 .env 파일의 NEWSAPI_KEY에 복사하여 붙여넣으십시오.</li>\n<li>NewsData에 등록한 후, /api-key로 이동하여 API KEY를 확인한 후 이를 .env 파일의 NEWSDATAIO_KEY에 복사하여 붙여넣으십시오.</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>지루한 부분은 끝났습니다. 이제 이러한 API에 액세스할 수 있고, 기사를 가져올 수 있습니다. 각 API에서 페이로드가 어떻게 보이는지 살펴봅시다:</p>\n<p><img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_6.png\" alt=\"image\"></p>\n<h2>사전 준비 조치 요약</h2>\n<p>Kafka 클러스터를 생성하고, 벡터 인덱스를 생성하고, 뉴스 API에 등록하는 이 3단계를 모두 마친 후에 .env 파일은 다음과 같은 모습이어야 합니다:</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-variable constant_\">UPSTASH_KAFKA_UNAME</span>=<span class=\"hljs-string\">\"[여기에 사용자 이름 입력]\"</span>\n<span class=\"hljs-variable constant_\">UPSTASH_KAFKA_PASS</span>=<span class=\"hljs-string\">\"[여기에 암호 입력]\"</span>\n<span class=\"hljs-variable constant_\">UPSTASH_KAFKA_ENDPOINT</span>=<span class=\"hljs-string\">\"[여기에 엔드포인트 입력]\"</span>\n<span class=\"hljs-variable constant_\">UPSTASH_KAFKA_TOPIC</span>=<span class=\"hljs-string\">\"[토픽 이름 입력]\"</span>\n\n<span class=\"hljs-variable constant_\">UPSTASH_VECTOR_ENDPOINT</span>=<span class=\"hljs-string\">\"[벡터 엔드포인트 입력]\"</span>\n<span class=\"hljs-variable constant_\">UPSTASH_VECTOR_TOPIC</span>=<span class=\"hljs-string\">\"[벡터 이름 입력]\"</span>\n<span class=\"hljs-variable constant_\">UPSTASH_VECTOR_KEY</span>=<span class=\"hljs-string\">\"[벡터 토큰 입력]\"</span>\n\n<span class=\"hljs-variable constant_\">NEWSAPI_KEY</span>=<span class=\"hljs-string\">\"[NEWSAPI 키 입력]\"</span>\n<span class=\"hljs-variable constant_\">NEWSDATAIO_KEY</span>=<span class=\"hljs-string\">\"[NEWSDATA 키 입력]\"</span>\n<span class=\"hljs-variable constant_\">NEWS_TOPIC</span> = <span class=\"hljs-string\">\"news\"</span> # 이것은 가져올 기사의 카테고리입니다\n\n다음 단계는 구현 세부 정보에 들어가기 전에 환경과 필수 패키지를 설치하는 것입니다.\n다음은 <span class=\"hljs-title class_\">Makefile</span> 설치 단계의 모습입니다:\n\n# <span class=\"hljs-title class_\">Makefile</span>\n...\n<span class=\"hljs-attr\">install</span>:\n @echo <span class=\"hljs-string\">\"$(GREEN) [CONDA] [$(ENV_NAME)] 파이썬 환경 생성 $(RESET)\"</span>\n conda create --name $(<span class=\"hljs-variable constant_\">ENV_NAME</span>) python=<span class=\"hljs-number\">3.9</span> -y\n @echo <span class=\"hljs-string\">\"환경 활성화 중...\"</span>\n @bash -c <span class=\"hljs-string\">\"source $$(conda info --base)/etc/profile.d/conda.sh &#x26;&#x26; conda activate $(ENV_NAME) \\\n   &#x26;&#x26; pip install poetry \\\n   poetry env use $(which python)\"</span>\n @echo <span class=\"hljs-string\">\"패키지 설치 중\"</span>\n @echo <span class=\"hljs-string\">\"pyproject.toml 위치로 변경 중...\"</span>\n @bash -c <span class=\"hljs-string\">\" PYTHON_KEYRING_BACKEND=keyring.backends.fail.Keyring poetry install\"</span>\n...\n\n환경을 준비하려면 make install을 실행하세요.\n\n&#x3C;div <span class=\"hljs-keyword\">class</span>=<span class=\"hljs-string\">\"content-ad\"</span>>&#x3C;/div>\n\n이제 이 소스로부터 기사를 가져오는 핸들러 구현을 조사해 봅시다.\n\n# 데이터 수집\n\n이 모듈의 목적은 두 <span class=\"hljs-variable constant_\">API</span>를 쿼리하는 기능을 캡슐화하고, 페이로드를 구문 분석하여 두 페이로드에 모두 존재하는 속성을 사용하여 공통 문서 형식으로 포매팅하고, 클러스터로 메시지를 보내기 위해 공유 <span class=\"hljs-title class_\">KafkaProducer</span> 인스턴스를 사용하는 것입니다.\n\n자세히 살펴볼 내용은 다음 하위 모듈들입니다:\n\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">\"content-ad\"</span>></span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">div</span>></span></span>\n\n- <span class=\"hljs-title class_\">Articles</span> <span class=\"hljs-title class_\">Fetching</span> <span class=\"hljs-title class_\">Manager</span> <span class=\"hljs-title class_\">Class</span>\n- 카프카 클러스터로 메시지를 보내는 방법\n- <span class=\"hljs-title class_\">Pydantic</span> 데이터 모델\n- 파이프라인 실행\n\n## <span class=\"hljs-title class_\">Articles</span> <span class=\"hljs-title class_\">Fetching</span> <span class=\"hljs-title class_\">Manager</span> <span class=\"hljs-title class_\">Class</span>\n\n구현 내용에 대해 알아보겠습니다:\n\n<span class=\"hljs-keyword\">import</span> datetime\n<span class=\"hljs-keyword\">import</span> functools\n<span class=\"hljs-keyword\">import</span> logging\n<span class=\"hljs-keyword\">from</span> typing <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Callable</span>, <span class=\"hljs-title class_\">Dict</span>, <span class=\"hljs-title class_\">List</span>\n\n<span class=\"hljs-keyword\">from</span> newsapi <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">NewsApiClient</span>\n<span class=\"hljs-keyword\">from</span> newsdataapi <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">NewsDataApiClient</span>\n<span class=\"hljs-keyword\">from</span> pydantic <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">ValidationError</span>\n\n<span class=\"hljs-keyword\">from</span> models <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">NewsAPIModel</span>, <span class=\"hljs-title class_\">NewsDataIOModel</span>\n<span class=\"hljs-keyword\">from</span> settings <span class=\"hljs-keyword\">import</span> settings\n\nlogging.<span class=\"hljs-title function_\">basicConfig</span>(level=logging.<span class=\"hljs-property\">INFO</span>)\nlogger = logging.<span class=\"hljs-title function_\">getLogger</span>(__name__)\nlogger.<span class=\"hljs-title function_\">setLevel</span>(logging.<span class=\"hljs-property\">DEBUG</span>)\n\n\ndef <span class=\"hljs-title function_\">handle_article_fetching</span>(<span class=\"hljs-attr\">func</span>: <span class=\"hljs-title class_\">Callable</span>) -> <span class=\"hljs-title class_\">Callable</span>:\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    뉴스 기사 가져오기 기능에 대한 예외 처리를 담당하는 데코레이터입니다.\n\n    이 데코레이터는 기사 가져오기 기능을 감싸서 발생하는 예외를 catch하고 로깅합니다.\n    오류가 발생하면 오류를 기록하고 빈 목록을 반환합니다.\n\n    Args:\n        func (Callable): 감쌀 기사 가져오기 함수.\n\n    Returns:\n        Callable: 감싼 함수.\n    \"</span><span class=\"hljs-string\">\"\"</span>\n\n    @functools.<span class=\"hljs-title function_\">wraps</span>(func)\n    def <span class=\"hljs-title function_\">wrapper</span>(*args, **kwargs):\n        <span class=\"hljs-attr\">try</span>:\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title function_\">func</span>(*args, **kwargs)\n        except <span class=\"hljs-title class_\">ValidationError</span> <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">e</span>:\n            logger.<span class=\"hljs-title function_\">error</span>(f<span class=\"hljs-string\">\"기사 처리 중 유효성 검사 오류 발생: {e}\"</span>)\n        except <span class=\"hljs-title class_\">Exception</span> <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">e</span>:\n            logger.<span class=\"hljs-title function_\">error</span>(f<span class=\"hljs-string\">\"소스로부터 데이터를 가져오는 도중 오류 발생: {e}\"</span>)\n            logger.<span class=\"hljs-title function_\">exception</span>(e)\n        <span class=\"hljs-keyword\">return</span> []\n\n    <span class=\"hljs-keyword\">return</span> wrapper\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">NewsFetcher</span>:\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    다양한 API에서 뉴스 기사를 가져오는 클래스입니다.\n\n    속성:\n        _newsapi (NewsApiClient): NewsAPI 클라이언트.\n        _newsdataapi (NewsDataApiClient): NewsDataAPI 클라이언트.\n\n    메서드:\n        fetch_from_newsapi(): NewsAPI로부터 기사 가져오기.\n        fetch_from_newsdataapi(): NewsDataAPI로부터 기사 가져오기.\n        sources: 호출 가능한 가져오기 함수 목록을 반환합니다.\n    \"</span><span class=\"hljs-string\">\"\"</span>\n\n    def <span class=\"hljs-title function_\">__init__</span>(self):\n        self.<span class=\"hljs-property\">_newsapi</span> = <span class=\"hljs-title class_\">NewsApiClient</span>(api_key=settings.<span class=\"hljs-property\">NEWSAPI_KEY</span>)\n        self.<span class=\"hljs-property\">_newsdataapi</span> = <span class=\"hljs-title class_\">NewsDataApiClient</span>(apikey=settings.<span class=\"hljs-property\">NEWSDATAIO_KEY</span>)\n\n    @handle_article_fetching\n    def <span class=\"hljs-title function_\">fetch_from_newsapi</span>(self) -> <span class=\"hljs-title class_\">List</span>[<span class=\"hljs-title class_\">Dict</span>]:\n        <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"NewsAPI에서 상위 뉴스 가져오기.\"</span><span class=\"hljs-string\">\"\"</span>\n        response = self.<span class=\"hljs-property\">_newsapi</span>.<span class=\"hljs-title function_\">get_everything</span>(\n            q=settings.<span class=\"hljs-property\">NEWS_TOPIC</span>,\n            language=<span class=\"hljs-string\">\"en\"</span>,\n            page=settings.<span class=\"hljs-property\">ARTICLES_BATCH_SIZE</span>,\n            page_size=settings.<span class=\"hljs-property\">ARTICLES_BATCH_SIZE</span>,\n        )\n        <span class=\"hljs-keyword\">return</span> [\n            <span class=\"hljs-title class_\">NewsAPIModel</span>(**article).<span class=\"hljs-title function_\">to_common</span>()\n            <span class=\"hljs-keyword\">for</span> article <span class=\"hljs-keyword\">in</span> response.<span class=\"hljs-title function_\">get</span>(<span class=\"hljs-string\">\"articles\"</span>, [])\n        ]\n\n    @handle_article_fetching\n    def <span class=\"hljs-title function_\">fetch_from_newsdataapi</span>(self) -> <span class=\"hljs-title class_\">List</span>[<span class=\"hljs-title class_\">Dict</span>]:\n        <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"NewsDataAPI에서 뉴스 데이터 가져오기.\"</span><span class=\"hljs-string\">\"\"</span>\n        response = self.<span class=\"hljs-property\">_newsdataapi</span>.<span class=\"hljs-title function_\">news_api</span>(\n            q=settings.<span class=\"hljs-property\">NEWS_TOPIC</span>,\n            language=<span class=\"hljs-string\">\"en\"</span>,\n            size=settings.<span class=\"hljs-property\">ARTICLES_BATCH_SIZE</span>,\n        )\n        <span class=\"hljs-keyword\">return</span> [\n            <span class=\"hljs-title class_\">NewsDataIOModel</span>(**article).<span class=\"hljs-title function_\">to_common</span>()\n            <span class=\"hljs-keyword\">for</span> article <span class=\"hljs-keyword\">in</span> response.<span class=\"hljs-title function_\">get</span>(<span class=\"hljs-string\">\"results\"</span>, [])\n        ]\n\n    @property\n    def <span class=\"hljs-title function_\">sources</span>(self) -> <span class=\"hljs-title class_\">List</span>[callable]:\n        <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"뉴스 가져오기 함수 목록입니다.\"</span><span class=\"hljs-string\">\"\"</span>\n        <span class=\"hljs-keyword\">return</span> [self.<span class=\"hljs-property\">fetch_from_newsapi</span>, self.<span class=\"hljs-property\">fetch_from_newsdataapi</span>]\n\n&#x3C;div <span class=\"hljs-keyword\">class</span>=<span class=\"hljs-string\">\"content-ad\"</span>>&#x3C;/div>\n\n이 구현에서 고려해야 할 몇 가지 중요 사항이 있습니다:\n\n- <span class=\"hljs-title class_\">NewsAPIModel</span>과 <span class=\"hljs-title class_\">NewsDataIOModel</span>은 특정 페이로드 형식에 익숙한 <span class=\"hljs-title class_\">Pydantic</span> 모델입니다.\n- 우리는 handle_article_fetching 데코레이터를 사용하여 원시 페이로드를 <span class=\"hljs-title class_\">Pydantic</span> 모델로 변환할 때 유효성 오류나 더 넓은 예외를 잡습니다.\n- 우리에게는 <span class=\"hljs-variable constant_\">API</span>를 쿼리하는 callable 메서드를 반환하는 sources라는 속성이 있습니다. 이것은 데이터 수집 모듈 내에서 사용될 것이며 멀티 프로듀서 스레드를 생성하여 <span class=\"hljs-title class_\">Kafka</span> 클러스터로 메시지를 전송합니다. 다음에 이어서 설명하겠습니다.\n\n## <span class=\"hljs-title class_\">Kafka</span> 메시지 생성\n\n다음에 우리가 구현할 작업 흐름입니다:\n\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">\"content-ad\"</span>></span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">div</span>></span></span>\n\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">img</span> <span class=\"hljs-attr\">src</span>=<span class=\"hljs-string\">\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_7.png\"</span> /></span></span>\n\n여기서 중요한 포인트들입니다:\n\n- <span class=\"hljs-variable constant_\">API</span>에서 가져오는 작업에 별도 스레드를 사용합니다.\n- 메시지를 보내기 위해 공통 카프카 프로듀서 인스턴스를 공유합니다.\n- 데이터 교환을 보증하기 위해 <span class=\"hljs-title class_\">Pydantic</span> 모델을 사용합니다.\n\n기사를 가져오는 데 별도 스레드를 사용하고, 클러스터로 메시지를 보내기 위해 단일 카프카 프로듀서 인스턴스를 사용하는 것이 우리의 사용 사례에서 권장되는 방법입니다. 그 이유는 다음과 같습니다:\n\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">\"content-ad\"</span>></span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">div</span>></span></span>\n\n- 효율성 및 성능: <span class=\"hljs-title class_\">KafkaProducer</span>는 스레드 안전합니다. 새 인스턴스를 만드는 것은 네트워크 연결과 일부 설정이 필요합니다. 여러 스레드 간에 하나의 단일 인스턴스를 공유하면 이러한 작업과 관련된 오버헤드를 줄일 수 있습니다.\n- 처리량: 단일 프로듀서 인스턴스는 메시지를 <span class=\"hljs-title class_\">Kafka</span> 클러스터로 보내기 전에 메시지를 일괄 처리합니다.\n- 자원: 사용 사례에 완전히 적용되지는 않지만, 우리는 오직 <span class=\"hljs-number\">2</span>개의 프로듀서 스레드만 가지고 있기 때문에 인스턴스 수를 제한함으로써 시스템 자원 이용률을 최적화할 수 있습니다.\n\n여기 <span class=\"hljs-title class_\">Kafka</span>로 메시지 처리를 담당하는 주요 기능이 있습니다:\n\ndef <span class=\"hljs-title function_\">run</span>(self) -> <span class=\"hljs-title class_\">NoReturn</span>:\n        <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"지속적으로 Kafka 주제로 메시지를 가져와 보냅니다.\"</span><span class=\"hljs-string\">\"\"</span>\n        <span class=\"hljs-keyword\">while</span> self.<span class=\"hljs-property\">running</span>.<span class=\"hljs-title function_\">is_set</span>():\n            <span class=\"hljs-attr\">try</span>:\n                <span class=\"hljs-attr\">messages</span>: <span class=\"hljs-title class_\">List</span>[<span class=\"hljs-title class_\">CommonDocument</span>] = self.<span class=\"hljs-title function_\">fetch_function</span>()\n                <span class=\"hljs-keyword\">if</span> <span class=\"hljs-attr\">messages</span>:\n                    messages = [msg.<span class=\"hljs-title function_\">to_kafka_payload</span>() <span class=\"hljs-keyword\">for</span> msg <span class=\"hljs-keyword\">in</span> messages]\n                    self.<span class=\"hljs-property\">producer</span>.<span class=\"hljs-title function_\">send</span>(self.<span class=\"hljs-property\">topic</span>, value=messages)\n                    self.<span class=\"hljs-property\">producer</span>.<span class=\"hljs-title function_\">flush</span>()\n                logger.<span class=\"hljs-title function_\">info</span>(\n                    f<span class=\"hljs-string\">\"프로듀서 : {self.producer_id}이(가) {len(messages)}개의 메시지를 전송함.\"</span>\n                )\n                time.<span class=\"hljs-title function_\">sleep</span>(self.<span class=\"hljs-property\">wait_window_sec</span>)\n            except <span class=\"hljs-title class_\">Exception</span> <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">e</span>:\n                logger.<span class=\"hljs-title function_\">error</span>(f<span class=\"hljs-string\">\"프로듀서 작업자 {self.producer_id}에서 오류 발생: {e}\"</span>)\n                self.<span class=\"hljs-property\">running</span>.<span class=\"hljs-title function_\">clear</span>()  # 오류 시 스레드를 중지합니다\n\n구현에서 고려해야 할 중요 사항:\n\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">\"content-ad\"</span>></span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">div</span>></span></span>\n\n- 우리는 fetch sources의 수만큼 <span class=\"hljs-title class_\">KafkaProducerThread</span> 인스턴스가 생성됩니다.\n- 우리는 모든 스레드를 <span class=\"hljs-title class_\">KafkaProducerSwarm</span> 아래에 랩합니다.\n- 모든 스레드 사이에서 단일 <span class=\"hljs-title class_\">KafkaProducer</span> 인스턴스를 공유하며, 이는 클러스터와 통신할 것입니다.\n- 우리는 N개의 fetching 스레드로 확장할 수 있지만 여전히 단일 <span class=\"hljs-title class_\">KafkaProducer</span> 인스턴스를 유지하기 위해 싱글톤 디자인 패턴을 따릅니다.\n\n## <span class=\"hljs-title class_\">Pydantic</span>을 사용한 데이터 교환 모델\n\n위에서 제시한 코드 스니펫 구현에서, 이전에 설명되지 않았던 *<span class=\"hljs-title class_\">Document</span>, *<span class=\"hljs-title class_\">Model</span> 객체의 사용을 관찰했을 수 있습니다. 이 섹션에서 이들이 무엇인지 자세히 살펴보겠습니다.\n\n이들은 데이터 교환을 위한 <span class=\"hljs-title class_\">Pydantic</span> 모델들이며, 우리가 구축 중인 응용 프로그램 내에서 이러한 모델들은:\n\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">\"content-ad\"</span>></span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">div</span>></span></span>\n\n- <span class=\"hljs-title class_\">NewsDataIOModel</span>: <span class=\"hljs-title class_\">NewsData</span> <span class=\"hljs-variable constant_\">API</span>에서 가져온 원시 페이로드를 래핑하고 포맷합니다.\n- <span class=\"hljs-title class_\">NewsAPIModel</span>: <span class=\"hljs-title class_\">NewsAPI</span> <span class=\"hljs-variable constant_\">API</span>에서 가져온 원시 페이로드를 래핑하고 포맷합니다.\n- <span class=\"hljs-title class_\">CommonDocument</span>: 위에서 언급한 다양한 뉴스 형식 사이의 공통 형식을 설정합니다.\n- <span class=\"hljs-title class_\">RefinedDocument</span>: metadata 아래에 유용한 필드를 그룹화하고 기사 설명 텍스트와 같은 주요 필드를 강조하는 공통 형식을 필터링합니다.\n- <span class=\"hljs-title class_\">ChunkedDocument</span>: 텍스트를 청크로 나누고 chunk_id와 document_id 사이의 계보를 보장합니다.\n- <span class=\"hljs-title class_\">EmbeddedDocument</span>: 청크를 임베드하여 chunk_id와 document_id 사이의 계보를 보장합니다.\n\n예를 들어, 위 <span class=\"hljs-title class_\">CommonDocument</span> 모델은 다양한 뉴스 페이로드 형식 사이의 연결 역할을 나타내므로 이와 같이 구성됩니다:\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">CommonDocument</span>(<span class=\"hljs-title class_\">BaseModel</span>):\n    <span class=\"hljs-attr\">article_id</span>: str = <span class=\"hljs-title class_\">Field</span>(default_factory=<span class=\"hljs-attr\">lambda</span>: <span class=\"hljs-title function_\">str</span>(<span class=\"hljs-title function_\">uuid4</span>()))\n    <span class=\"hljs-attr\">title</span>: str = <span class=\"hljs-title class_\">Field</span>(default_factory=<span class=\"hljs-attr\">lambda</span>: <span class=\"hljs-string\">\"N/A\"</span>)\n    <span class=\"hljs-attr\">url</span>: str = <span class=\"hljs-title class_\">Field</span>(default_factory=<span class=\"hljs-attr\">lambda</span>: <span class=\"hljs-string\">\"N/A\"</span>)\n    <span class=\"hljs-attr\">published_at</span>: str = <span class=\"hljs-title class_\">Field</span>(\n        default_factory=<span class=\"hljs-attr\">lambda</span>: datetime.<span class=\"hljs-title function_\">now</span>().<span class=\"hljs-title function_\">strftime</span>(<span class=\"hljs-string\">\"%Y-%m-%d %H:%M:%S\"</span>)\n    )\n    <span class=\"hljs-attr\">source_name</span>: str = <span class=\"hljs-title class_\">Field</span>(default_factory=<span class=\"hljs-attr\">lambda</span>: <span class=\"hljs-string\">\"Unknown\"</span>)\n    <span class=\"hljs-attr\">image_url</span>: <span class=\"hljs-title class_\">Optional</span>[str] = <span class=\"hljs-title class_\">Field</span>(default_factory=<span class=\"hljs-attr\">lambda</span>: <span class=\"hljs-title class_\">None</span>)\n    <span class=\"hljs-attr\">author</span>: <span class=\"hljs-title class_\">Optional</span>[str] = <span class=\"hljs-title class_\">Field</span>(default_factory=<span class=\"hljs-attr\">lambda</span>: <span class=\"hljs-string\">\"Unknown\"</span>)\n    <span class=\"hljs-attr\">description</span>: <span class=\"hljs-title class_\">Optional</span>[str] = <span class=\"hljs-title class_\">Field</span>(default_factory=<span class=\"hljs-attr\">lambda</span>: <span class=\"hljs-title class_\">None</span>)\n    <span class=\"hljs-attr\">content</span>: <span class=\"hljs-title class_\">Optional</span>[str] = <span class=\"hljs-title class_\">Field</span>(default_factory=<span class=\"hljs-attr\">lambda</span>: <span class=\"hljs-title class_\">None</span>)\n\n    @<span class=\"hljs-title function_\">field_validator</span>(<span class=\"hljs-string\">\"title\"</span>, <span class=\"hljs-string\">\"description\"</span>, <span class=\"hljs-string\">\"content\"</span>)\n    def <span class=\"hljs-title function_\">clean_text_fields</span>(cls, v):\n        <span class=\"hljs-keyword\">if</span> v is <span class=\"hljs-title class_\">None</span> or v == <span class=\"hljs-string\">\"\"</span>:\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"N/A\"</span>\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title function_\">clean_full</span>(v)\n\n    @<span class=\"hljs-title function_\">field_validator</span>(<span class=\"hljs-string\">\"url\"</span>, <span class=\"hljs-string\">\"image_url\"</span>)\n    def <span class=\"hljs-title function_\">clean_url_fields</span>(cls, v):\n        <span class=\"hljs-keyword\">if</span> v is <span class=\"hljs-title class_\">None</span>:\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"N/A\"</span>\n        v = <span class=\"hljs-title function_\">remove_html_tags</span>(v)\n        v = <span class=\"hljs-title function_\">normalize_whitespace</span>(v)\n        <span class=\"hljs-keyword\">return</span> v\n\n    @<span class=\"hljs-title function_\">field_validator</span>(<span class=\"hljs-string\">\"published_at\"</span>)\n    def <span class=\"hljs-title function_\">clean_date_field</span>(cls, v):\n        <span class=\"hljs-attr\">try</span>:\n            parsed_date = parser.<span class=\"hljs-title function_\">parse</span>(v)\n            <span class=\"hljs-keyword\">return</span> parsed_date.<span class=\"hljs-title function_\">strftime</span>(<span class=\"hljs-string\">\"%Y-%m-%d %H:%M:%S\"</span>)\n        except (<span class=\"hljs-title class_\">ValueError</span>, <span class=\"hljs-title class_\">TypeError</span>):\n            logger.<span class=\"hljs-title function_\">error</span>(f<span class=\"hljs-string\">\"Error parsing date: {v}, using current date instead.\"</span>)\n\n    @classmethod\n    def <span class=\"hljs-title function_\">from_json</span>(cls, <span class=\"hljs-attr\">data</span>: dict) -> <span class=\"hljs-string\">\"CommonDocument\"</span>:\n        <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"JSON 객체에서 CommonDocument를 만듭니다.\"</span><span class=\"hljs-string\">\"\"</span>\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title function_\">cls</span>(**data)\n\n    def <span class=\"hljs-title function_\">to_kafka_payload</span>(self) -> <span class=\"hljs-attr\">dict</span>:\n        <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"Kafka 페이로드의 공통 표현을 준비합니다.\"</span><span class=\"hljs-string\">\"\"</span>\n        <span class=\"hljs-keyword\">return</span> self.<span class=\"hljs-title function_\">model_dump</span>(exclude_none=<span class=\"hljs-title class_\">False</span>)\n\n해석해보겠습니다:\n\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">\"content-ad\"</span>></span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">div</span>></span></span>\n\n- 뉴스 기사 형식에 공통 속성 시리즈가 포함되어 있습니다.\n- 각 필드를 유효성 검사하거나 field_validator 데코레이터를 사용하여 기본값을 지정합니다.\n- to_kafka_payload 메서드는 메시지 직렬화를 보장하여 <span class=\"hljs-title class_\">Kafka</span> 클러스터로 전송하기 전에 처리합니다.\n\n## 텍스트 필드 클린업 프로세스\n\n클린업 프로세스는 간단합니다. 텍스트를 정리하고 다음을 보장하기 위해 메서드를 사용합니다:\n\n- 끝에 있는 공백이나 \\n, \\t를 제거합니다.\n- ul/li 목록 항목을 제거합니다.\n- 텍스트 내에 <span class=\"hljs-variable constant_\">HTML</span> 태그가 있으면 제거합니다.\n\n&#x3C;div <span class=\"hljs-keyword\">class</span>=<span class=\"hljs-string\">\"content-ad\"</span>>&#x3C;/div>\n\n우리는 이러한 변환을 간소화하기 위해 구조화되지 않은 [<span class=\"hljs-number\">7</span>] <span class=\"hljs-title class_\">Python</span> 라이브러리를 사용하고 있습니다.\n\n## <span class=\"hljs-title class_\">KafkaProducers</span> 실행\n\n지금까지 다음 모듈을 수행/구현했습니다:\n\n- 필요한 모든 서비스에 등록\n- <span class=\"hljs-title class_\">Kafka</span> 클러스터 및 벡터 데이터베이스 생성\n- 뉴스 기사 검색 핸들러 구현\n- 데이터 교환을 위한 <span class=\"hljs-title class_\">Pydantic</span> 모델 구현\n- <span class=\"hljs-title class_\">KafkaProducer</span> 로직 구현\n\n&#x3C;div <span class=\"hljs-keyword\">class</span>=<span class=\"hljs-string\">\"content-ad\"</span>>&#x3C;/div>\n\n작업이 완료되면 이제 안전하게 우리의 파이프라인에서 생산 단계를 실행하고 <span class=\"hljs-title class_\">Upstash</span>의 <span class=\"hljs-title class_\">KafkaCluster</span>에서 메시지를 확인할 수 있습니다.\n\n그럼 시작해봐요!\n프로젝트의 루트 디렉토리에서, <span class=\"hljs-title class_\">Makefile</span>에 데이터 수집을 실행하는 명령어가 있습니다:\n\n....\n\n<span class=\"hljs-attr\">run_producers</span>:\n @echo <span class=\"hljs-string\">\"$(GREEN) [실행 중] 데이터 수집 파이프라인 Kafka 프로듀서 $(RESET)\"</span>\n @bash -c <span class=\"hljs-string\">\"poetry run python -m src.producer\"</span>\n\n...\n\n이 🔗<span class=\"hljs-title class_\">Makefile</span>은 우리가 구축 중인 솔루션과 상호작용하기 위한 유용한 명령어가 포함되어 있습니다. 이 경우에는 make run_producers를 사용하여 run_producers를 실행해야 합니다. 이렇게 하면 <span class=\"hljs-title class_\">KafkaSwarm</span>이 시작되고 <span class=\"hljs-title class_\">NewsAPIs</span>에서 기사를 가져와 형식을 지정한 다음 <span class=\"hljs-title class_\">Kafka</span> 클러스터로 보내는 스레드를 다룰 것입니다.\n\n&#x3C;div <span class=\"hljs-keyword\">class</span>=<span class=\"hljs-string\">\"content-ad\"</span>>&#x3C;/div>\n\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_8.png\" alt=\"이미지\"></p>\n<p>로그를 통해 프로듀서 스레드가 각각 5개의 메시지를 보냈다는 것을 확인했습니다. 메시지들이 클러스터에 도달했는지 확인하려면 Upstash 콘솔로 이동하여 Kafka 클러스터 → 메시지를 확인하십시오. 다음과 같은 화면이 나타날 것입니다:</p>\n<p><img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_9.png\" alt=\"이미지\"></p>\n<p>이 시점에서는 API에서 뉴스 기사를 가져와 형식을 맞춘 후 Kafka로 메시지를 보내는 데이터 수집 파이프라인의 구현 및 테스트가 완료되었습니다. 다음으로는 Kafka에서 새 메시지를 처리하는 \"컨슈머\" 또는 적재 파이프라인을 구현할 것입니다.</p>\n<div class=\"content-ad\"></div>\n<h1>데이터 수집 파이프라인</h1>\n<p>우리가 Kafka 주제에서 메시지를 받았다는 것을 확인한 후에는 \"소비자\" 파이프라인을 구현해야 합니다. 이는 다음을 의미합니다:</p>\n<ul>\n<li>Kafka 주제에서 메시지 읽기</li>\n<li>파싱, 형식 지정, 청크화, 임베딩 생성</li>\n<li>벡터 객체 생성 및 Upstash Vector Index에 업서트</li>\n</ul>\n<p><img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_10.png\" alt=\"이미지\"></p>\n<div class=\"content-ad\"></div>\n<p>이를 위해 Bytewax [4]를 사용하여 이러한 단계를 올바른 순서로 연결하는 DataFlow를 정의할 것입니다.</p>\n<p>바로 구현에 들어가서 주요 개념을 설명해보겠습니다!</p>\n<ul>\n<li>Bytewax Flow에 입력으로 Kafka Source를 정의합니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">from</span> typing <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">List</span>\n\n<span class=\"hljs-keyword\">from</span> bytewax.<span class=\"hljs-property\">connectors</span>.<span class=\"hljs-property\">kafka</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">KafkaSinkMessage</span>, <span class=\"hljs-title class_\">KafkaSource</span>\n\n<span class=\"hljs-keyword\">from</span> logger <span class=\"hljs-keyword\">import</span> get_logger\n<span class=\"hljs-keyword\">from</span> models <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">CommonDocument</span>\n<span class=\"hljs-keyword\">from</span> settings <span class=\"hljs-keyword\">import</span> settings\n\nlogger = <span class=\"hljs-title function_\">get_logger</span>(__name__)\n\ndef <span class=\"hljs-title function_\">build_kafka_stream_client</span>():\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    Build a Kafka stream client to read messages from the Upstash Kafka topic using the ByteWax KafkaSource connector.\n    \"</span><span class=\"hljs-string\">\"\"</span>\n    kafka_config = {\n        <span class=\"hljs-string\">\"bootstrap.servers\"</span>: settings.<span class=\"hljs-property\">UPSTASH_KAFKA_ENDPOINT</span>,\n        <span class=\"hljs-string\">\"security.protocol\"</span>: <span class=\"hljs-string\">\"SASL_SSL\"</span>,\n        <span class=\"hljs-string\">\"sasl.mechanisms\"</span>: <span class=\"hljs-string\">\"SCRAM-SHA-256\"</span>,\n        <span class=\"hljs-string\">\"sasl.username\"</span>: settings.<span class=\"hljs-property\">UPSTASH_KAFKA_UNAME</span>,\n        <span class=\"hljs-string\">\"sasl.password\"</span>: settings.<span class=\"hljs-property\">UPSTASH_KAFKA_PASS</span>,\n        <span class=\"hljs-string\">\"auto.offset.reset\"</span>: <span class=\"hljs-string\">\"earliest\"</span>,  # <span class=\"hljs-title class_\">Start</span> reading at the earliest message\n    }\n    kafka_input = <span class=\"hljs-title class_\">KafkaSource</span>(\n        topics=[settings.<span class=\"hljs-property\">UPSTASH_KAFKA_TOPIC</span>],\n        brokers=[settings.<span class=\"hljs-property\">UPSTASH_KAFKA_ENDPOINT</span>],\n        add_config=kafka_config,\n    )\n    logger.<span class=\"hljs-title function_\">info</span>(<span class=\"hljs-string\">\"KafkaSource client created successfully.\"</span>)\n    <span class=\"hljs-keyword\">return</span> kafka_input\n\ndef <span class=\"hljs-title function_\">process_message</span>(<span class=\"hljs-attr\">message</span>: <span class=\"hljs-title class_\">KafkaSinkMessage</span>):\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    On a Kafka message, process the message and return a list of CommonDocuments.\n    - message: KafkaSinkMessage(key, value) where value is the message payload.\n    \"</span><span class=\"hljs-string\">\"\"</span>\n    <span class=\"hljs-attr\">documents</span>: <span class=\"hljs-title class_\">List</span>[<span class=\"hljs-title class_\">CommonDocument</span>] = []\n    <span class=\"hljs-attr\">try</span>:\n        json_str = message.<span class=\"hljs-property\">value</span>.<span class=\"hljs-title function_\">decode</span>(<span class=\"hljs-string\">\"utf-8\"</span>)\n        data = json.<span class=\"hljs-title function_\">loads</span>(json_str)\n        documents = [<span class=\"hljs-title class_\">CommonDocument</span>.<span class=\"hljs-title function_\">from_json</span>(obj) <span class=\"hljs-keyword\">for</span> obj <span class=\"hljs-keyword\">in</span> data]\n        logger.<span class=\"hljs-title function_\">info</span>(f<span class=\"hljs-string\">\"Decoded into {len(documents)} CommonDocuments\"</span>)\n        <span class=\"hljs-keyword\">return</span> documents\n    except <span class=\"hljs-title class_\">StopIteration</span>:\n        logger.<span class=\"hljs-title function_\">info</span>(<span class=\"hljs-string\">\"No more documents to fetch from the client.\"</span>)\n    except <span class=\"hljs-title class_\">KeyError</span> <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">e</span>:\n        logger.<span class=\"hljs-title function_\">error</span>(f<span class=\"hljs-string\">\"Key error in processing document batch: {e}\"</span>)\n    except json.<span class=\"hljs-property\">JSONDecodeError</span> <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">e</span>:\n        logger.<span class=\"hljs-title function_\">error</span>(f<span class=\"hljs-string\">\"Error decoding JSON from message: {e}\"</span>)\n        raise\n    except <span class=\"hljs-title class_\">Exception</span> <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">e</span>:\n        logger.<span class=\"hljs-title function_\">exception</span>(f<span class=\"hljs-string\">\"Unexpected error in next_batch: {e}\"</span>)\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>이 구현에서 중요한 점들:</p>\n<ul>\n<li>build_kafka_stream_client : 미리 정의된 Bytewax KafkaSource 커넥터를 사용하여 KafkaConsumer의 인스턴스를 생성합니다.</li>\n<li>process_message : Kafka Topic에서 메시지를 처리할 콜백 함수입니다.</li>\n</ul>\n<ol start=\"2\">\n<li>Bytewax 플로우의 출력으로 Upstash Vector (Index)를 정의합니다.</li>\n</ol>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> typing <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Optional</span>, <span class=\"hljs-title class_\">List</span>\n\n<span class=\"hljs-keyword\">from</span> bytewax.<span class=\"hljs-property\">outputs</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">DynamicSink</span>, <span class=\"hljs-title class_\">StatelessSinkPartition</span>\n<span class=\"hljs-keyword\">from</span> upstash_vector <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Index</span>, <span class=\"hljs-title class_\">Vector</span>\n<span class=\"hljs-keyword\">from</span> models <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">EmbeddedDocument</span>\n<span class=\"hljs-keyword\">from</span> settings <span class=\"hljs-keyword\">import</span> settings\n<span class=\"hljs-keyword\">from</span> logger <span class=\"hljs-keyword\">import</span> get_logger\n\n\nlogger = <span class=\"hljs-title function_\">get_logger</span>(__name__)\n\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">UpstashVectorOutput</span>(<span class=\"hljs-title class_\">DynamicSink</span>):\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"Upstash 벡터 출력을 나타내는 클래스입니다.\n\n    이 클래스는 at-least-once 처리를 지원하는 동적 출력 유형인 Upstash 벡터 출력을 생성하는 데 사용됩니다.\n    resume 이후의 메시지는 resume 즉각적으로 복제됩니다.\n\n    Args:\n        vector_size (int): 벡터의 크기.\n        collection_name (str, optional): 컬렉션의 이름입니다. 기본값은 constants.VECTOR_DB_OUTPUT_COLLECTION_NAME입니다.\n        client (Optional[UpstashClient], optional): Upstash 클라이언트입니다. 기본값은 None입니다.\n    \"</span><span class=\"hljs-string\">\"\"</span>\n\n    def <span class=\"hljs-title function_\">__init__</span>(\n        self,\n        <span class=\"hljs-attr\">vector_size</span>: int = settings.<span class=\"hljs-property\">EMBEDDING_MODEL_MAX_INPUT_LENGTH</span>,\n        <span class=\"hljs-attr\">collection_name</span>: str = settings.<span class=\"hljs-property\">UPSTASH_VECTOR_TOPIC</span>,\n        <span class=\"hljs-attr\">client</span>: <span class=\"hljs-title class_\">Optional</span>[<span class=\"hljs-title class_\">Index</span>] = <span class=\"hljs-title class_\">None</span>,\n    ):\n        self.<span class=\"hljs-property\">_collection_name</span> = collection_name\n        self.<span class=\"hljs-property\">_vector_size</span> = vector_size\n\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-attr\">client</span>:\n            self.<span class=\"hljs-property\">client</span> = client\n        <span class=\"hljs-attr\">else</span>:\n            self.<span class=\"hljs-property\">client</span> = <span class=\"hljs-title class_\">Index</span>(\n                url=settings.<span class=\"hljs-property\">UPSTASH_VECTOR_ENDPOINT</span>,\n                token=settings.<span class=\"hljs-property\">UPSTASH_VECTOR_KEY</span>,\n                retries=settings.<span class=\"hljs-property\">UPSTASH_VECTOR_RETRIES</span>,\n                retry_interval=settings.<span class=\"hljs-property\">UPSTASH_VECTOR_WAIT_INTERVAL</span>,\n            )\n\n    def <span class=\"hljs-title function_\">build</span>(\n        self, <span class=\"hljs-attr\">step_id</span>: str, <span class=\"hljs-attr\">worker_index</span>: int, <span class=\"hljs-attr\">worker_count</span>: int\n    ) -> <span class=\"hljs-title class_\">StatelessSinkPartition</span>:\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title class_\">UpstashVectorSink</span>(self.<span class=\"hljs-property\">client</span>, self.<span class=\"hljs-property\">_collection_name</span>)\n\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">UpstashVectorSink</span>(<span class=\"hljs-title class_\">StatelessSinkPartition</span>):\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    Upstash Vector 데이터베이스 컬렉션에 문서 임베딩을 작성하는 싱크입니다.\n    이 구현은 효율성을 높이기 위해 배치 업서트를 활용하며, 오류 처리 및 로깅을 향상시키고 가독성 및 유지 보수성을 위해 Pythonic한 모법을 따릅니다.\n\n    Args:\n        client (Index): 쓰기에 사용할 Upstash Vector 클라이언트입니다.\n        collection_name (str, optional): 쓸 컬렉션의 이름입니다. 기본값은 UPSTASH_VECTOR_TOPIC 환경 변수의 값입니다.\n    \"</span><span class=\"hljs-string\">\"\"</span>\n\n    def <span class=\"hljs-title function_\">__init__</span>(\n        self,\n        <span class=\"hljs-attr\">client</span>: <span class=\"hljs-title class_\">Index</span>,\n        <span class=\"hljs-attr\">collection_name</span>: str = <span class=\"hljs-title class_\">None</span>,\n    ):\n        self.<span class=\"hljs-property\">_client</span> = client\n        self.<span class=\"hljs-property\">_collection_name</span> = collection_name\n        self.<span class=\"hljs-property\">_upsert_batch_size</span> = settings.<span class=\"hljs-property\">UPSTASH_VECTOR_UPSERT_BATCH_SIZE</span>\n\n    def <span class=\"hljs-title function_\">write_batch</span>(self, <span class=\"hljs-attr\">documents</span>: <span class=\"hljs-title class_\">List</span>[<span class=\"hljs-title class_\">EmbeddedDocument</span>]):\n        <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n        구성된 Upstash Vector 데이터베이스 컬렉션에 문서 임베딩의 배치를 작성합니다.\n\n        Args:\n            documents (List[EmbeddedDocument]): 쓸 문서들입니다.\n        \"</span><span class=\"hljs-string\">\"\"</span>\n        vectors = [\n            <span class=\"hljs-title class_\">Vector</span>(id=doc.<span class=\"hljs-property\">doc_id</span>, vector=doc.<span class=\"hljs-property\">embeddings</span>, metadata=doc.<span class=\"hljs-property\">metadata</span>)\n            <span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> documents\n        ]\n\n        # 효율성을 위한 배치 업서트\n        <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">range</span>(<span class=\"hljs-number\">0</span>, <span class=\"hljs-title function_\">len</span>(vectors), self.<span class=\"hljs-property\">_upsert_batch_size</span>):\n            batch_vectors = vectors[i : i + self.<span class=\"hljs-property\">_upsert_batch_size</span>]\n            <span class=\"hljs-attr\">try</span>:\n                self.<span class=\"hljs-property\">_client</span>.<span class=\"hljs-title function_\">upsert</span>(vectors=batch_vectors)\n            except <span class=\"hljs-title class_\">Exception</span> <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">e</span>:\n                logger.<span class=\"hljs-title function_\">error</span>(f<span class=\"hljs-string\">\"배치 업서트 중 예외 발생 {e}\"</span>)\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>이 구현에서 중요한 사항들입니다:</p>\n<ul>\n<li>UpstashVectorOutput: 다양한 대상으로 데이터를 전달하기 위해 설계된 Bytewax DynamicSink 추상화를 인스턴스화합니다. 우리의 경우, 이는 Upstash Vector Index 클라이언트 연결 위에 래핑될 것입니다.</li>\n<li>UpstashVectorSink: 우리의 DynamicSink을 래핑하고 업서트 벡터를 우리의 VectorDatabase에 처리하는 기능을 담당합니다. 이 StatelessSinkPartition은 DynamicSink가 어떠한 상태도 유지하지 않고 우리의 Sink에 대한 모든 입력을 write_batch 기능 구현에 따라 처리합니다.</li>\n</ul>\n<h2>나머지 Bytewax Flow 빌드하기</h2>\n<p>여기 Upstash Kafka Topic에서 메시지를 가져와 정제, 수정, 분할, 삽입하고 Upstash Vector Index에 벡터를 업서트하는 저희 DataFlow의 전체 구현입니다.</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    이 스크립트는 Upstash 사용 사례에 대한 ByteWax 데이터플로 구현을 정의합니다.\n    데이터플로에는 다음 단계가 포함되어 있습니다:\n        1. 입력: 카프카 스트림에서 데이터를 읽기\n        2. 정제: 입력 데이터를 공통 형식으로 변환 \n        3. 청크 분리: 입력 데이터를 더 작은 청크로 분리\n        4. 임베드: 입력 데이터에 대한 임베딩 생성\n        5. 출력: 출력 데이터를 Upstash 벡터 데이터베이스에 쓰기\n\"</span><span class=\"hljs-string\">\"\"</span>\n\n<span class=\"hljs-keyword\">from</span> pathlib <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Path</span>\n<span class=\"hljs-keyword\">from</span> typing <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Optional</span>\n\n<span class=\"hljs-keyword\">import</span> bytewax.<span class=\"hljs-property\">operators</span> <span class=\"hljs-keyword\">as</span> op\n<span class=\"hljs-keyword\">from</span> vector <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">UpstashVectorOutput</span>\n<span class=\"hljs-keyword\">from</span> consumer <span class=\"hljs-keyword\">import</span> process_message, build_kafka_stream_client\n<span class=\"hljs-keyword\">from</span> bytewax.<span class=\"hljs-property\">connectors</span>.<span class=\"hljs-property\">kafka</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">KafkaSource</span>\n<span class=\"hljs-keyword\">from</span> bytewax.<span class=\"hljs-property\">dataflow</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Dataflow</span>\n<span class=\"hljs-keyword\">from</span> bytewax.<span class=\"hljs-property\">outputs</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">DynamicSink</span>\n<span class=\"hljs-keyword\">from</span> embeddings <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">TextEmbedder</span>\n<span class=\"hljs-keyword\">from</span> models <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">ChunkedDocument</span>, <span class=\"hljs-title class_\">EmbeddedDocument</span>, <span class=\"hljs-title class_\">RefinedDocument</span>\n<span class=\"hljs-keyword\">from</span> logger <span class=\"hljs-keyword\">import</span> get_logger\n\nlogger = <span class=\"hljs-title function_\">get_logger</span>(__name__)\n\n\ndef <span class=\"hljs-title function_\">build</span>(\n    <span class=\"hljs-attr\">model_cache_dir</span>: <span class=\"hljs-title class_\">Optional</span>[<span class=\"hljs-title class_\">Path</span>] = <span class=\"hljs-title class_\">None</span>,\n) -> <span class=\"hljs-title class_\">Dataflow</span>:\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    Upstash 사용 사례에 대한 ByteWax 데이터플로를 구축합니다.\n    다음과 같은 데이터플로를 따릅니다:\n        * 1. Tag: ['kafka_input']   = KafkaSource에서 입력 데이터 읽기\n        * 2. Tag: ['map_kinp']      = KafkaSource에서 CommonDocument로 메시지 처리\n            * 2.1 [Optional] Tag ['dbg_map_kinp'] = ['map_kinp'] 후 디버깅\n        * 3. Tag: ['refine']        = 메시지를 정제된 문서 형식으로 변환\n            * 3.1 [Optional] Tag ['dbg_refine'] = ['refine'] 후 디버깅\n        * 4. Tag: ['chunkenize']    = 정제된 문서를 더 작은 청크로 나누기\n            * 4.1 [Optional] Tag ['dbg_chunkenize'] = ['chunkenize'] 후 디버깅\n        * 5. Tag: ['embed']         = 청크에 대한 임베딩 생성\n            * 5.1 [Optional] Tag ['dbg_embed'] = ['embed'] 후 디버깅\n        * 6. Tag: ['output']        = 임베딩을 Upstash 벡터 데이터베이스에 쓰기\n    노트:\n        각 선택적 태그는 문제 해결을 위해 활성화할 수 있는 디버깅 단계입니다.\n    \"</span><span class=\"hljs-string\">\"\"</span>\n    model = <span class=\"hljs-title class_\">TextEmbedder</span>(cache_dir=model_cache_dir)\n\n    dataflow = <span class=\"hljs-title class_\">Dataflow</span>(flow_id=<span class=\"hljs-string\">\"news-to-upstash\"</span>)\n    stream = op.<span class=\"hljs-title function_\">input</span>(\n        step_id=<span class=\"hljs-string\">\"kafka_input\"</span>,\n        flow=dataflow,\n        source=<span class=\"hljs-title function_\">_build_input</span>(),\n    )\n    stream = op.<span class=\"hljs-title function_\">flat_map</span>(<span class=\"hljs-string\">\"map_kinp\"</span>, stream, process_message)\n    # _ = op.<span class=\"hljs-title function_\">inspect</span>(<span class=\"hljs-string\">\"dbg_map_kinp\"</span>, stream)\n    stream = op.<span class=\"hljs-title function_\">map</span>(<span class=\"hljs-string\">\"refine\"</span>, stream, <span class=\"hljs-title class_\">RefinedDocument</span>.<span class=\"hljs-property\">from_common</span>)\n    # _ = op.<span class=\"hljs-title function_\">inspect</span>(<span class=\"hljs-string\">\"dbg_refine\"</span>, stream)\n    stream = op.<span class=\"hljs-title function_\">flat_map</span>(\n        <span class=\"hljs-string\">\"chunkenize\"</span>,\n        stream,\n        lambda <span class=\"hljs-attr\">refined_doc</span>: <span class=\"hljs-title class_\">ChunkedDocument</span>.<span class=\"hljs-title function_\">from_refined</span>(refined_doc, model),\n    )\n    # _ = op.<span class=\"hljs-title function_\">inspect</span>(<span class=\"hljs-string\">\"dbg_chunkenize\"</span>, stream)\n    stream = op.<span class=\"hljs-title function_\">map</span>(\n        <span class=\"hljs-string\">\"embed\"</span>,\n        stream,\n        lambda <span class=\"hljs-attr\">chunked_doc</span>: <span class=\"hljs-title class_\">EmbeddedDocument</span>.<span class=\"hljs-title function_\">from_chunked</span>(chunked_doc, model),\n    )\n    # _ = op.<span class=\"hljs-title function_\">inspect</span>(<span class=\"hljs-string\">\"dbg_embed\"</span>, stream)\n    stream = op.<span class=\"hljs-title function_\">output</span>(<span class=\"hljs-string\">\"output\"</span>, stream, <span class=\"hljs-title function_\">_build_output</span>())\n    logger.<span class=\"hljs-title function_\">info</span>(<span class=\"hljs-string\">\"성공적으로 ByteWax 데이터플로를 생성했습니다.\"</span>)\n    logger.<span class=\"hljs-title function_\">info</span>(\n        <span class=\"hljs-string\">\"\\t단계: Kafka 입력 -> 매핑 -> 정제 -> 청크 분리 -> 임베딩 -> 업로드\"</span>\n    )\n    <span class=\"hljs-keyword\">return</span> dataflow\n\n\ndef <span class=\"hljs-title function_\">_build_input</span>() -> <span class=\"hljs-title class_\">KafkaSource</span>:\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title function_\">build_kafka_stream_client</span>()\n\n\ndef <span class=\"hljs-title function_\">_build_output</span>() -> <span class=\"hljs-title class_\">DynamicSink</span>:\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title class_\">UpstashVectorOutput</span>()\n</code></pre>\n<div class=\"content-ad\"></div>\n<ul>\n<li>kafka_input: 카프카 메시지를 가져와 CommonDocument Pydantic 형식으로 변환하는 단계입니다.</li>\n<li>map_kinp: 카프카 입력을 의미하며, 수신된 메시지에 flat map을 적용하여 List[CommonDocument] Pydantic 객체를 생성합니다.</li>\n<li>refine: List[CommonDocument]를 순회하고 RefinedDocument 인스턴스를 생성하는 단계입니다.</li>\n<li>chunkenize: List[RefinedDocument]를 순회하고 ChunkedDocument 인스턴스를 생성하는 단계입니다.</li>\n<li>embed: List[ChunkedDocuments]를 순회하고 EmbeddedDocument 인스턴스를 생성하는 단계입니다.</li>\n<li>output: List[EmbeddedDocument]를 순회하고 Vector 객체를 생성하여 Upstash Vector Index에 업서트하는 단계입니다.</li>\n</ul>\n<h1>파이프라인 시작</h1>\n<p>지금까지 구현한 것은 다음과 같습니다:</p>\n<ul>\n<li>데이터 수집 파이프라인: 주기적으로 NewsAPI에서 원시 페이로드를 가져와 형식을 지정한 뒤, 카프카 토픽으로 메시지를 전송하는 단계입니다.</li>\n<li>인제션 파이프라인: 이는 Bytewax DataFlow로, 카프카 토픽에 연결되어 메시지를 소비하고, 최종적으로 벡터를 업서트하는 Vector 데이터베이스에 업데이트합니다.</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>프로젝트 루트에있는 Makefile에서 미리 정의된 명령을 사용하여 이 두 서비스를 모두 시작할 수 있습니다:</p>\n<pre><code class=\"hljs language-js\"># 카프카 메시지를 생성하기 위해 데이터 수집 파이프라인 실행\nmake run_producers\n\n# 카프카 메시지를 소비하고 벡터를 업데이트하기 위해 인제스처리 파이프라인 실행\nmake run_pipeline\n</code></pre>\n<p>그리고... 완료되었습니다!\n성공적으로 프로듀서/컨슈머 서비스를 시작했습니다.\n남은 모듈은 UI입니다. 벡터 데이터베이스와 뉴스 기사를 검색하는 데 상호 작용하는 데 사용됩니다.</p>\n<h1>사용자 인터페이스</h1>\n<div class=\"content-ad\"></div>\n<p>UI는 다음과 같은 기능을 갖춘 기본 Streamlit [8] 애플리케이션입니다:</p>\n<ul>\n<li>텍스트 검색 창</li>\n<li>벡터 데이터베이스에서 가져온 기사로 채워진 카드가 표시되는 div 섹션</li>\n</ul>\n<p>카드에는 다음과 같은 데이터 필드가 포함되어 있습니다:</p>\n<ul>\n<li>발행일</li>\n<li>유사도 점수</li>\n<li>기사 이미지</li>\n<li>SeeMore 버튼을 클릭하면 원본 기사 URL로 이동합니다.</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>한번 메시지/질문을 텍스트 상자에 입력하면 입력이 정리되고 (소문자로 변환, 비ASCII 문자 제거 등) 그리고 삽입됩니다. 새로운 삽입물을 사용하여 벡터 데이터베이스를 쿼리하여 가장 유사한 항목을 가져옵니다. 그 결과는 구성되어 렌더링될 것입니다.</p>\n<p>다음은 예시입니다:</p>\n<p><img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_11.png\" alt=\"How to build a real-time News Search Engine using VectorDBs - Part 1\"></p>\n<p><img src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_12.png\" alt=\"How to build a real-time News Search Engine using VectorDBs - Part 2\"></p>\n<div class=\"content-ad\"></div>\n<h1>결론</h1>\n<p>축하합니다!</p>\n<p>성공했습니다! 멋진 프로젝트만큼이나 라이브로 출시할 준비가 된 뉴스 검색 엔진을 만들었습니다. 단순히 무작정 던지는 것이 아니라, 우리는 최고의 소프트웨어 개발 관행을 따르기도 했습니다.</p>\n<p>Pydantic을 사용하여 데이터를 잘 처리했고, 유닛 테스트를 작성하고, 스레딩을 활용하여 작업을 가속화했으며, Upstash의 서버리스 카프카와 벡터 데이터베이스를 활용하여 파이프라인을 쉽게 설정할 뿐만 아니라 빠르고 확장 가능하며 오류 대비 가능하도록 만들었습니다.</p>\n<div class=\"content-ad\"></div>\n<p>이제 당신은 이 청사진을 대부분의 데이터 기반 아이디어에 적용할 수 있는 능력을 갖게 되었어요. 이건 이 프로젝트뿐만 아니라 앞으로 만들게 될 멋진 것들을 위한 큰 승리에요.</p>\n<h1>참고 자료</h1>\n<p>[1] News Search Engine using Upstash Vector — Decoding ML Github (2024)\n[2] Upstash Serverless Kafka\n[3] Upstash Serverless Vector Database\n[4] Bytewax Stream Processing with Python\n[5] Singleton Pattern\n[6] sentence-transformers/all-MiniLM-L6-v2\n[7] unstructured Python Library\n[8] Streamlit Python</p>\n<h1>더 읽을 거리</h1>\n<div class=\"content-ad\"></div>\n<p>이 글과 관련성 순으로 정렬되었습니다.</p>\n</body>\n</html>\n"},"__N_SSG":true}