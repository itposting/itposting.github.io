{"pageProps":{"post":{"title":"모듈형 RAG와 RAG Flow 첫 번째 파트","description":"","date":"2024-06-23 19:19","slug":"2024-06-23-ModularRAGandRAGFlowPart","content":"\n\n# 소개\n\n지난 일년 동안, 검색 증가 생성(Retrieval-Augmented Generation, RAG) 개념은 LLM 애플리케이션 구현 방법으로서 상당한 주목을 받았습니다. 저희는 RAG에 대한 포괄적인 조사 보고서를 작성했는데, 이 보고서는 Naive RAG에서 Advanced RAG 및 Modular RAG로의 전환에 대해 탐구했습니다. 그러나 이 조사는 주로 증가(Source/Stage/Process)를 통해 RAG 기술을 규명하였습니다.\n\n본 글은 특히 Modular RAG 패러다임을 중심으로 합니다. 우리는 Module Type, Module 및 Operator로 구성된 세 단계의 Modular RAG 패러다임을 더 확실히 정의했습니다. 이 패러다임에 따라, 현재 RAG 시스템 내 핵심 기술인 6가지 주요 Module Types, 14개의 Modules 및 40여 개의 Operators에 대해 상세히 다루어 RAG에 대한 포괄적인 이해를 제공하고 있습니다.\n\n다양한 Operator를 조합함으로써, 우리는 다양한 RAG 흐름을 유도할 수 있으며, 이 개념을 이 글에서 명확히 설명하고자 합니다. 광범위한 연구를 기반으로, 일반적인 패턴, 몇 가지 구체적인 구현 사례 및 최상의 업계 사례를 요약하였습니다. (공간 제약으로 인해 이 부분은 Part II에서 다룰 예정입니다.)\n\n<div class=\"content-ad\"></div>\n\n이 글의 목적은 현재 RAG 개발 상태에 대한 더 정교한 이해를 제공하고 미래 발전을 위한 길을 만들어주는 것입니다. 모듈식 RAG는 새로운 연산자, 모듈, 그리고 새로운 플로우의 구성을 용이하게 하는 다양한 기회를 제공합니다.\n\n# 모듈식 RAG란 무엇인가요?\n\nRAG의 발전은 다음과 같은 중요한 측면들에 반영되어 보다 다양하고 유연한 과정을 이끌어내고 있습니다:\n\n- 향상된 데이터 획득: RAG는 기존의 비구조적 데이터를 넘어 반구조적 및 구조적 데이터를 포함하고, 구조적 데이터의 전처리에 중점을 두어 검색의 품질을 향상시키고 모델이 외부 지식 소스에 의존하는 것을 줄이는 방향으로 확장되었습니다.\n- 통합된 기술: RAG는 세부 조정, 어댑터 모듈 사용, 강화 학습을 포함한 다른 기술들과 통합하여 검색 능력을 강화하고 있습니다.\n- 적응형 검색 프로세스: 검색 프로세스는 검색한 내용을 활용하여 생성을 안내하고 그 반대로 하는 등 다단계 검색 강화를 지원하도록 진화했습니다. 또한, 자율적인 판단과 LLM 사용을 통해 검색 필요성을 판단하여 질문에 대한 효율을 높였습니다.\n\n<div class=\"content-ad\"></div>\n\n모듈식 RAG의 정의\n\n위에서는 RAG의 신속한 발전이 체인 스타일의 고급 RAG 패러다임을 능가하여 모듈식 특성을 과시하고 있음을 볼 수 있습니다. 현재의 조직 부재와 추상화 부족을 해결하기 위해, Naive RAG와 Advanced RAG의 개발 패러다임을 무리없이 통합하는 모듈식 RAG 접근 방식을 제안합니다.\n\n모듈식 RAG는 매우 확장 가능한 패러다임을 제시하며, RAG 시스템을 모듈 유형, 모듈 및 연산자의 세 단계 구조로 나누고 있습니다. 각 모듈 유형은 RAG 시스템의 핵심 프로세스를 나타내며, 여러 기능 모듈을 포함하고 있습니다. 각 기능 모듈에는 또 다른 여러 특정 연산자가 포함되어 있습니다. 전체 RAG 시스템은 여러 모듈과 해당 연산자들의 순열과 조합으로 이루어진 RAG Flow를 형성하게 됩니다. Flow 내에서 각 모듈 유형에서 다른 기능 모듈을 선택할 수 있으며, 각 기능 모듈 내에서는 하나 이상의 연산자를 선택할 수 있습니다.\n\n이전 패러다임과의 관계\n\n<div class=\"content-ad\"></div>\n\n모듈식 RAG은 RAG 시스템을 다층 구조의 모듈식 형태로 조직합니다. 고급 RAG는 RAG의 모듈식 형태이며, Naive RAG는 고급 RAG의 특수한 경우입니다. 이 세 가지 패러다임 간의 관계는 상속과 발전의 하나입니다.\n\n모듈식 RAG의 장점\n\n모듈식 RAG의 이점은 명백하며, 기존의 RAG 관련 작업에 대한 신선하고 포괄적인 시각을 제공합니다. 모듈식 구성을 통해 관련 기술과 방법들이 명확하게 요약됩니다.\n\n- 연구적 시각. 모듈식 RAG는 확장성이 높아 연구자들이 현재 RAG 개발에 대한 포괄적인 이해를 기반으로 새로운 모듈 유형, 모듈, 그리고 연산자를 제안하기 쉽습니다.\n- 응용 시각. RAG 시스템의 설계 및 구성이 더 편리해지며, 사용자들이 기존 데이터, 사용 시나리오, 하향 작업 등에 따라 RAG Flow를 사용자 정의할 수 있습니다. 개발자들은 또한 현재 Flow 구성 방법을 참고하고, 다른 응용 시나리오와 도메인에 기반하여 새로운 플로우와 패턴을 정의할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Module](/assets/img/2024-06-23-ModularRAGandRAGFlowPart_0.png)\n\n## Module Type — Module — Operators\n\n### 1. Indexing\n\nIndexing, the process of breaking down text into manageable chunks, is a crucial step in organizing the system, facing three main challenges:\n\n\n<div class=\"content-ad\"></div>\n\n- 완전하지 않은 내용 표현. 청크의 의미 정보는 세분화 방법에 영향을 받아 중요 정보가 잃거나 긴 문맥 속에서 잠기는 결과를 초래합니다.\n- 부정확한 청크 유사성 검색. 데이터 양이 증가함에 따라 검색에서의 잡음이 커져 잘못된 데이터와 빈번히 일치하게 되어 검색 시스템을 취약하고 신뢰할 수 없게 만듭니다.\n- 명확하지 않은 참조 궤적. 검색된 청크는 어느 문서에서든 유래할 수 있으며 인용 트레일이 없어, 다수의 다른 문서에서 유사하게 의미가 있는 혹은 완전히 다른 주제의 내용을 포함할 수 있습니다.\n\n## 청크 최적화\n\n더 큰 청크는 더 많은 문맥을 포착할 수 있지만, 더 많은 잡음을 생성하여 더 오랜 처리 시간과 높은 비용이 필요합니다. 반면 더 작은 청크는 필요한 문맥을 완전히 전달하지 않을 수 있지만, 더 적은 잡음을 가지고 있습니다.\n\n- 슬라이딩 윈도우\n\n<div class=\"content-ad\"></div>\n\n이러한 요구 사항을 균형 있게 조절하는 한 가지 간단한 방법은 중첩 청크를 사용하는 것입니다. 슬라이딩 창을 활용하면 의미적 전환을 향상시킬 수 있습니다. 그러나 의미적 고려 사항이 부족하다는 제한 사항이 있습니다. - 작은 청크에서 큰 청크로 핵심 아이디어는 검색에 사용되는 청크와 합성에 사용되는 청크를 분리하는 것입니다. 더 작은 청크를 사용하면 검색의 정확도가 향상되고, 더 큰 청크는 더 많은 컨텍스트 정보를 제공할 수 있습니다.\n\n![이미지](/assets/img/2024-06-23-ModularRAGandRAGFlowPart_1.png)\n\n<div class=\"content-ad\"></div>\n\n특히 한 가지 방법은 더 작은 청크를 검색한 다음 부모 ID를 참조하여 더 큰 청크를 반환하는 것일 수 있습니다. 또는 개별 문장을 검색하고 문장 주변 텍스트 창을 반환할 수도 있습니다.\n\n자세한 정보 및 LlamaIndex 구현.\n\n- 요약\n\n이는 작은 것에서 큰 것으로의 개념과 유사하며, 먼저 더 큰 청크의 개요가 생성되고, 이후 개요에 대해 검색이 수행됩니다. 그런 다음 더 큰 청크에 대해 보조 검색을 수행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 메타데이터 첨부\n\n청크에 페이지 번호, 파일 이름, 작성자, 타임스탬프, 요약 또는 청크가 대답할 수 있는 질문과 같은 메타데이터 정보를 포함시킬 수 있습니다. 그 결과로 검색 범위를 제한할 수 있는 이 메타데이터를 기반으로 검색이 필터링될 수 있습니다. LlamaIndex에서 이 구현을 확인해보세요.\n\n# 구조적 구성\n\n정보 검색을 강화하는 효과적인 방법 중 하나는 문서에 대한 계층적 구조를 설정하는 것입니다. 청크 구조를 구성함으로써 RAG 시스템은 적절한 데이터의 검색과 처리를 가속화할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 계층적 색인\n\n문서의 계층 구조에서 노드는 부모-자식 관계로 배열되어 있으며 이들에 연결된 청크가 있습니다. 각 노드에는 데이터 요약이 저장되어 있어 데이터를 신속하게 탐색하고 RAG 시스템이 어떤 청크를 추출해야 하는지 결정하는 데 도움이 됩니다. 이 접근 방식은 블록 추출 문제로 인한 오류를 완화하는 데도 도움이 됩니다.\n\n구조적 색인을 구축하는 주요 방법은 다음과 같습니다:\n\n- 구조 인식: 문서에서 단락과 문장을 분할\n- 콘텐츠 인식: PDF, HTML, Latex에 내재된 구조\n- 의미 인식: NLP 기술을 활용한 텍스트의 의미 인식과 분할, NLTK와 같은 기법 사용\n\n<div class=\"content-ad\"></div>\n\n대규모로 아커스의 계층적 인덱스를 확인하실 수 있습니다.\n\n- KG 조직 문서\n\n지식 그래프(KGs)를 활용하여 문서의 계층 구조를 구축함으로써 일관성을 유지할 수 있습니다. 이는 다양한 개념과 엔티티 간의 연결을 명확히 하고 환각 가능성을 크게 줄입니다.\n\n다른 이점은 정보 검색 프로세스를 LLM이 이해할 수 있는 지침으로 변환하여 지식 검색의 정확성을 향상시키고 LLM이 맥락에 부합한 응답을 생성할 수 있도록 함으로써 RAG 시스템의 전체적인 효율성을 향상시킵니다.\n\n<div class=\"content-ad\"></div>\n\nNeo4j 구현 및 LllmaIndex Neo4j 쿼리 엔진을 확인해보세요.\n\nKG를 사용하여 여러 문서를 조직하는 경우, 이 연구 논문 KGP: 지식 그래프 프롬프팅을 참고하실 수 있습니다.\n\n# 2. 사전 검색\n\nNaive RAG의 주요 도전 중 하나는 사용자의 원본 쿼리에 직접 의존하고 있다는 것입니다. 정확하고 명확한 질문을 구성하는 것은 어려우며, 무분별한 쿼리는 하위 수준의 검색 효과를 초래합니다.\n\n<div class=\"content-ad\"></div>\n\n이 단계에서의 주요 도전 과제는 다음과 같습니다:\n\n- 제대로 작성되지 않은 질문들입니다. 질문 자체가 복잡하며, 언어가 잘 정리되지 않았습니다.\n- 언어 복잡성 및 모호함입니다. 언어 모델은 종종 전문 용어나 다의어적 약어를 다룰 때 어려움을 겪습니다. 예를 들어, “LLM”이 큰 언어 모델을 의미하는지 법적 맥락에서의 법학 석사를 나타내는지를 구분하지 못할 수 있습니다.\n\n## 질의 확장\n\n단일 질문을 복수의 질문으로 확장하면 질문의 내용을 풍부하게 만들어 특정 뉘앙스의 부족을 보충함으로써 생성된 답변의 최적적인 관련성을 보장할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 다중 쿼리\n\nLLM을 통해 쿼리를 확장하기 위해 prompt 엔지니어링을 사용하여 이러한 쿼리들을 병렬로 실행할 수 있습니다. 쿼리의 확장은 무작위가 아니라 신중하게 설계된 것입니다. 이 설계의 두 가지 중요한 기준은 쿼리의 다양성과 범위입니다.\n\n여러 쿼리를 사용하는 한 가지 어려움은 사용자의 원래 의도가 희석될 수 있다는 것입니다. 이를 완화하기 위해 우리는 prompt 엔지니어링에서 모델에게 원본 쿼리에 더 높은 중요도를 할당하도록 지시할 수 있습니다.\n\n- 하위 쿼리\n\n<div class=\"content-ad\"></div>\n\n서브 질문 계획 프로세스는 원본 질문을 완전히 대답할 수 있는 필요한 서브 질문을 생성하는 것을 나타냅니다. 관련 컨텍스트를 추가하는 이 프로세스는 원칙적으로 쿼리 확장과 유사합니다. 구체적으로, 복잡한 질문은 적은 것에서 많은 것으로 유도하는 방법을 사용하여 일련의 보다 간단한 서브 질문으로 분해될 수 있습니다.\n\n- CoVe\n\n쿼리 확장에 대한 다른 접근 방식은 Meta AI가 제안한 Chain-of-Verification(CoVe)의 사용을 포함합니다. 확장된 쿼리는 LLM에 의해 유효성을 검사하여 환각을 줄이는 효과를 얻습니다. 유효성이 검증된 확장된 쿼리는 전형적으로 더 높은 신뢰성을 보입니다.\n\n# 쿼리 변환\n\n<div class=\"content-ad\"></div>\n\n- 다시 작성\n\n원본 쿼리는 실제 상황에서 LLM 검색에 항상 최적이 아닙니다. 따라서, 우리는 LLM에 대해 쿼리를 다시 작성하도록 유도할 수 있습니다. 쿼리 다시 작성을 위해 LLM을 사용하는 것 외에도, RRR(Rewrite-retrieve-read)과 같은 특수한 작은 언어 모델을 활용할 수 있습니다.\n\n타오바오 프로모션 시스템에서 쿼리 다시 작성 방법인 BEQUE:Query Rewriting for Retrieval-Augmented Large Language Models의 구현은 장인 쿼리에 대한 회수 효과를 현저히 향상시켰으며, GMV 상승으로 이어졌습니다.\n\n- HyDE\n\n<div class=\"content-ad\"></div>\n\n쿼리에 대답할 때 LLM은 벡터 데이터베이스의 쿼리 및 계산된 벡터를 직접 검색하는 대신 가정된 답변인 가상 문서를 작성합니다. 이는 문제나 쿼리의 임베딩 유사성을 찾는 대신 답변 간의 임베딩 유사성에 초점을 맞추고 있습니다. 또한 쿼리 간의 검색에 초점을 맞춘 Reverse HyDE도 포함되어 있습니다.\n\n![이미지](/assets/img/2024-06-23-ModularRAGandRAGFlowPart_2.png)\n\n- Step-back Prompting\n\nGoogle DeepMind가 제안한 Step-back Prompting 방법을 사용하면 원본 쿼리를 추상화하여 고수준 개념 질문인 스텝백 질문을 생성할 수 있습니다. RAG 시스템에서는 스텝백 질문과 원본 쿼리 모두 검색에 사용되며, 두 결과 모두 언어 모델 답변 생성의 기초로 활용됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 쿼리 라우팅\n\n다양한 쿼리에 따라 다른 RAG 파이프라인으로 라우팅되며, 다양한 시나리오를 수용할 수 있는 유연한 RAG 시스템에 적합합니다.\n\n- 메타데이터 라우터/ 필터\n\n첫 번째 단계는 쿼리에서 키워드(엔티티)를 추출한 후, 키워드 및 청크 내의 메타데이터를 기반으로 필터링하여 검색 범위를 좁히는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n- Semantic Router\n\n라우팅하는 또 다른 방법은 쿼리의 의미 정보를 활용하는 것입니다. 특정 방법은 Semantic Router를 활용합니다. 물론 의미론적 및 메타데이터 기반 방법을 결합하여 질의 라우팅을 향상시키는 하이브리드 라우팅 방법도 사용할 수 있습니다.\n\nSemantic router 리포지토리를 확인해보세요.\n\n# 쿼리 구성\n\n<div class=\"content-ad\"></div>\n\n유저의 쿼리를 다른 쿼리 언어로 변환하여 대체 데이터 소스에 접근하는 작업입니다. 일반적으로 사용되는 방법은 다음과 같습니다:\n\n- 텍스트를 Cypher로 변환\n- 텍스트를 SQL로 변환\n\n많은 시나리오에서는 구조화된 쿼리 언어(SQL, Cypher 등)가 의미 정보 및 메타데이터와 결합되어 더 복잡한 쿼리를 작성하는 데 자주 사용됩니다. 자세한 내용은 Langchain 블로그를 참조해주세요.\n\n# 3 조회\n\n<div class=\"content-ad\"></div>\n\n검색 과정은 RAG에서 중요한 역할을 합니다. 강력한 PLM을 활용하여 쿼리와 텍스트를 잠재적 공간에 효과적으로 표현하여 질문과 문서 간의 의미 유사성을 성립하는 데 도움이 됩니다.\n\n고려해야 할 세 가지 주요 사항:\n\n- 검색 효율성\n- 임베딩 품질\n- 작업, 데이터 및 모델의 정렬\n\n# 검색기 선택\n\n<div class=\"content-ad\"></div>\n\nChatGPT가 출시된 이후에는 임베딩 모델 개발에 대한 열기가 높아졌어요. Hugging Face의 MTEB 리더보드는 8가지 작업(클러스터링, 분류, 이중 텍스트, 쌍 분류, 재랭킹, 정보 검색, 시맨틱 텍스트 유사성 및 요약)에 걸쳐 거의 모든 가능한 임베딩 모델을 평가합니다. 또한, C-MTEB는 중국어 임베딩 모델의 능력을 평가하며, 6가지 작업과 35개의 데이터셋을 다루고 있어요.\n\nRAG 애플리케이션을 구축할 때, \"어떤 임베딩 모델을 사용해야 할까?\"에 대한 보편적인 해답은 없습니다. 그러나 특정 임베딩이 특정 사용 사례에 더 적합하다는 점을 알 수 있을 거예요.\n\nMTEB/C-MTEB 리더보드를 확인해보세요.\n\n- Sparse Retriever\n\n<div class=\"content-ad\"></div>\n\n희박 인코딩 모델은 다소 구식 기술로 간주될 수 있지만 단어 빈도 통계와 같은 통계적 방법에 기반하고 있어서 높은 인코딩 효율성과 안정성을 유지하고 있습니다. 일반적인 계수 인코딩 모델로는 BM25와 TF-IDF가 있습니다.\n\n- 조밀 검색기\n\n신경망 기반의 밀도 인코딩 모델에는 여러 종류가 있습니다:\n\n- BERT 아키텍처에서 구축된 인코더-디코더 언어 모델인 ColBERT와 같은 모델.\n- BGE 및 Baichuan-Text-Embedding과 같은 포괄적인 다중 작업 파인튜닝 모델.\n- OpenAI-Ada-002 및 Cohere Embedding과 같은 클라우드 API 기반 모델.\n- 대규모 데이터 애플리케이션을 위해 설계된 다음 세대 가속화 인코딩 프레임워크 Dragon+.\n- 혼합/하이브리드 검색\n\n<div class=\"content-ad\"></div>\n\n두 가지 임베딩 접근 방식은 서로 다른 관련 기능을 캡쳐하고 상호 보완적인 관련 정보를 활용함으로써 상호 이득을 얻을 수 있습니다. 예를 들어, 희박 검색 모델은 밀도 검색 모델을 교육하기 위한 초기 검색 결과를 제공하는 데 사용될 수 있습니다. 게다가 PLM(Pre-trained Language Models)은 희소 검색을 강화하기 위해 용어 가중치를 학습하는 데 활용될 수 있습니다. 구체적으로, 희소 검색 모델이 밀도 검색 모델의 제로샷 검색 기능을 강화하고 희귀 엔티티를 포함하는 쿼리를 처리하는 데 밀도 리트리버를 돕는 것이 증명되었습니다.\n\n![이미지](/assets/img/2024-06-23-ModularRAGandRAGFlowPart_3.png)\n\n# 리트리버 파인튜닝\n\n특히 의료, 법률 및 소유 용어가 풍부한 기타 전문 분야와 같이 사전 훈련된 모델이 임베딩 공간에서 유사하다고 판단하는 컨텍스트가 벗어날 수 있는 경우, 임베딩 모델을 조정함으로써 이 문제를 해결할 수 있습니다. 이러한 조정은 추가적인 노력이 필요하지만 검색 효율성과 도메인 정렬을 크게 향상시킬 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- SFT\n\n도메인별 데이터를 기반으로 자체 세부 조정 데이터 세트를 구축할 수 있습니다. LlamaIndex를 사용하면 빠르게 작업을 완료할 수 있어요.\n\n- LSR (LM-지도 학습 리트리버)\n\n직접 데이터 세트에서 세부 조정 데이터 세트를 구성하는 대신 LSR은 LM이 생성한 결과를 우선적 신호로 사용하여 RAG 프로세스 중에 임베딩 모델을 세부 조정합니다.\n\n<div class=\"content-ad\"></div>\n\n- RL(강화 학습)\n\nRLHF(사람 피드백에서 강화 학습)에서 영감을 받아 LM 기반 피드백을 활용하여 강화 학습을 통해 리트리버를 강화합니다.\n\n- 어댑터\n\n가끔 전체 리트리버를 세밀하게 조정하는 것은 비용이 많이 들 수 있으며, 특히 API 기반 리트리버를 직접 세밀하게 조정할 수 없는 경우에는 어댑터 모듈을 통합하고 세밀 조정을 진행함으로써 이를 완화할 수 있습니다. 어댑터를 추가하는 또다른 이점은 특정 다운스트림 작업과의 더 나은 조정을 달성할 수 있는 능력입니다.\n\n<div class=\"content-ad\"></div>\n\n- Task Specific.PRCA: 검색 질의응답을 위한 블랙박스 대규모 언어 모델에 플러그인 리워드 기반 컨텍스트 어댑터를 적합화하는 작업.\n- Task Agnostic. AAR(Augmentation-Adapted Retriver)는 여러 하향 작업을 수용하기 위해 설계된 범용 어댑터를 소개합니다.\n\n## 4. 검색 후\n\n전체 문서 청크를 검색하여 이를 LLM의 컨텍스트 환경에 직접 공급하는 것은 최적의 선택이 아닙니다. 문서 후처리는 LLM이 컨텍스트 정보를 보다 효과적으로 활용하는 데 도움이 될 수 있습니다.\n\n주요 도전 과제는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- 중간에 길들이 미아 되다. 인간처럼 LLM은 긴 텍스트의 처음과 끝만을 기억하고, 중간 부분은 잊어 버리기 쉽습니다.\n- 소음/반사실적 청크. 소음이나 사실적으로 모순된 문서를 검색하면 최종 검색 결과 생성에 영향을 줄 수 있습니다.\n- 맥락 창. 큰 모델에서 관련 콘텐츠를 상당량 검색하더라도 맥락 정보의 길이에 제한이 있어 모든 이 콘텐츠를 포함시키기 어렵습니다.\n\n# 재랭크\n\n검색된 문서 청크들을 콘텐츠나 길이를 변경하지 않고, LLM에 대한 더 중요한 문서 청크의 가시성을 높이기 위해 재랭크합니다. 구체적으로는:\n\n- 규칙 기반 재랭크\n\n<div class=\"content-ad\"></div>\n\n특정 규칙에 따라 메트릭을 계산하여 청크를 다시 순위 지정합니다. 일반적인 메트릭은 다음과 같습니다:\n\n- 다양성\n- 관련성\n- MRR (최대 주변 관련성, 1998)\n\nMMR의 아이디어는 중복을 줄이고 결과 다양성을 높이는 데 있습니다. 텍스트 요약에 사용되며 쿼리 관련성과 정보의 독창성을 병합 기준으로 사용하여 최종 키워드 목록에서 구절을 선택합니다.\n\nHayStack에서 해당 순위 지정 구현을 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n- 모델 기반 재순위\n\n언어 모델을 활용하여 문서 청크를 재정렬하고, 선택지는 다음과 같습니다:\n\n- BERT 시리즈의 인코더-디코더 모델인 SpanBERT\n- Cohere rerank 또는 bge-raranker-large와 같은 특화된 재순위 모델\n- GPT-4와 같은 일반적인 대규모 언어 모델\n\n# 압축 및 선택\n\n<div class=\"content-ad\"></div>\n\nRAG 프로세스에서 흔한 오해는 가능한 많은 관련 문서를 검색하여 연결하여 긴 검색 프롬프트를 형성한다는 것이 유익하다는 믿음입니다. 그러나 과도한 문맥은 더 많은 잡음을 도입할 수 있으며, LLM이 주요 정보를 인식하는 것을 약화시키고 \"중간에서 잃어버림\"과 같은 문제로 이어질 수 있습니다. 이러한 문제를 해결하는 일반적인 접근법은 검색된 콘텐츠를 압축하고 선택하는 것입니다.\n\n- （Long)LLMLingua\n\nGPT-2 Small이나 LLaMA-7B와 같은 정련된 작은 언어 모델을 사용하여 중요하지 않은 토큰을 검색 프롬프트에서 감지하고 제거하여, 사람들이 이해하기 어려운 형태에서 LLM이 잘 이해하는 형태로 변환됩니다. 이 접근 방식은 검색 프롬프트 압축을 위한 직접적이고 실용적인 방법을 제시하여, LLM의 추가 훈련 필요를 없애면서 언어 무결성과 압축 비율을 균형있게 유지합니다.\n\nLLMLingua 프로젝트를 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n- Recomp\n\nRecomp은 두 가지 유형의 압축기를 소개합니다: 검색된 문서에서 적절한 문장을 선택하는 추출 압축기 및 여러 문서에서 정보를 결합하여 간결한 요약을 생성하는 추상적 압축기입니다. 두 압축기 모두 생성된 요약이 언어 모델의 입력에 앞부분에 추가될 때 최종 작업에서 언어 모델의 성능을 향상시키도록 훈련되었으며, 요약의 간결성을 보장합니다. 검색된 문서가 입력과 관련이 없거나 언어 모델에 추가 정보를 제공하지 않는 경우, 압축기는 빈 문자열을 반환할 수 있어 선택적 확장을 구현할 수 있습니다.\n\n- 선택적 컨텍스트\n\n입력 컨텍스트에서 중복되는 콘텐츠를 식별하고 제거함으로써 입력을 최적화하여 언어 모델의 추론 효율성을 향상할 수 있습니다. 선택적 컨텍스트는 \"불용어 제거\" 전략과 유사합니다. 실제로, 선택적 컨텍스트는 기본 언어 모델에 의해 계산된 자기 정보에 기반하여 어휘 단위의 정보 콘텐츠를 평가합니다. 더 높은 자기 정보를 갖는 콘텐츠를 유지함으로써, 이 방법은 언어 모델 처리를 위한 보다 간결하고 효율적인 텍스트 표현을 제공하며, 다양한 응용 프로그램에서의 성능을 저해시키지 않습니다. 그러나 압축된 콘텐츠와 대상 언어 모델 및 압축을 위해 사용되는 작은 언어 모델 간의 일치를 간과합니다.\n\n<div class=\"content-ad\"></div>\n\n- 태깅-필터\n\n태깅은 비교적 직관적이고 명료한 방식입니다. 특히, 문서들이 먼저 라벨이 붙여지고, 그 후 쿼리의 메타데이터를 기반으로 필터링됩니다.\n\n- LLM-비평\n\n다른 명확하고 효과적인 방법은 LLM이 최종 답변을 생성하기 전에 검색된 콘텐츠를 평가하도록 하는 것입니다. 이를 통해 LLM은 LLM 비평을 통해 적합하지 않은 문서들을 걸러낼 수 있습니다. 예를 들어, Chatlaw에서 LLM은 참조된 법적 규정에 대한 자체 제안을 받아 그들의 적합성을 평가하게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 5세대\n\n사용자의 쿼리와 검색된 컨텍스트 정보에 기반하여 답변을 생성하기 위해 LLM을 활용하십시오.\n\n# 생성기 선택\n\n시나리오에 따라 LLM의 선택은 다음 두 가지 유형으로 분류될 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 클라우드 API 기반 생성기\n\n클라우드 API를 기반으로하여 OpenAI의 ChatGPT, GPT-4, Anthropic Claude 등을 활용하여 타사 LLMs를 호출합니다. 이점:\n\n- 서버 압력이 없음\n- 높은 동시성\n- 보다 강력한 모델 활용 가능\n\n단점:\n\n<div class=\"content-ad\"></div>\n\n- 데이터가 제3자를 통해 전달되어 데이터 개인 정보 보호에 대한 우려가 생깁니다\n- 모델을 조정할 수 없음 (대부분의 경우)\n- 온-프레미스\n\n로컬에 배포된 오픈 소스 또는 자체 개발된 LLMs, 예를 들어 람마 시리즈, GLM 등니다. 로컬에 배포된 모델은 클라우드 API 기반 모델과 정반대의 장단점을 가지고 있습니다. 로컬에 배포된 모델은 더 큰 유연성과 더 나은 개인 정보 보호를 제공하지만 더 많은 계산 리소스가 필요합니다.\n\n# 생성기 세부 조정\n\nLLM 사용뿐만 아니라 시나리오와 데이터 특성에 기반한 목표 지향적인 세부 조정은 더 나은 결과를 얻을 수 있습니다. 이것은 온-프레미스 설정을 사용하는 가장 큰 장점 중 하나이기도 합니다. 일반적인 세부 조정 방법은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- SFT\n\n특정 도메인에서 데이터가 부족한 경우, LLM에게 추가 지식을 제공하여 fine-tuning을 통해 LLM에게 도움이 될 수 있습니다. Huggingface의 fine-tuning 데이터 또한 초기 단계로 활용될 수 있습니다.\n\nFine-tuning의 또 다른 이점은 모델의 입력과 출력을 조정할 수 있는 능력입니다. 예를 들어, 이것은 LLM이 특정 데이터 형식에 적응하고 지시된 특정한 스타일로 응답을 생성할 수 있도록 할 수 있습니다.\n\n- RL\n\n<div class=\"content-ad\"></div>\n\nLLM 출력물을 인간 또는 검색기 선호사항에 맞게 조정하는 것은 가능한 접근 방식입니다. 예를 들어 최종 생성된 답변에 대해 수동으로 주석을 달고 그에 대한 피드백을 통해 강화 학습을 제공하는 것이 있습니다. 인간 선호도에 맞추는 것뿐만 아니라 세밀하게 조정된 모델과 검색기의 선호도에도 부합시킬 수 있습니다.\n\n- 증류\n\n강력한 사유재 모델이나 더 큰 매개변수의 오픈소스 모델에 액세스 할 수 없는 상황에서, 강력한 모델(e.g. GPT-4)을 증류하는 간단하고 효과적인 방법이 있습니다.\n\n- 이중 미세 조정\n\n<div class=\"content-ad\"></div>\n\nRAG 프로세스를 제어하는 데 사용되는 모듈을 가리키는 Orchestration입니다. RAG는 더 이상 고정 프로세스를 따르지 않고, 주요 시점에서 결정을 내리고 결과에 따라 동적으로 다음 단계를 선택하는 것을 의미합니다. 이것은 Naive RAG와 비교해 모듈화된 RAG의 주요 특징 중 하나입니다.\n\n<div class=\"content-ad\"></div>\n\n판사 모듈은 RAG 프로세스의 중요한 부분을 평가하여 외부 문서 저장소를 검색해야 하는 필요성, 답변의 만족도 및 추가적인 탐색이 필요한지를 결정합니다. 주로 반복적이고 반복적이며 적응적인 검색에 사용됩니다. 구체적으로는 다음 두 가지 연산자가 주로 포함됩니다:\n\n- 규칙 기반\n\n미리 정의된 규칙을 기반으로 후속 조치가 결정됩니다. 일반적으로 생성된 답변은 평가되고, 그런 다음 점수가 미리 정의된 임계값을 충족하는지에 따라 계속할지 중지할지가 결정됩니다. 일반적인 임계값에는 토큰의 확신 수준이 포함됩니다.\n\n- 프롬프트 기반\n\n<div class=\"content-ad\"></div>\n\nLLM은 다음 조치를 자율적으로 결정합니다. 이를 실현하는 데 주로 두 가지 접근 방식이 있습니다. 첫 번째는 대화 기록을 기반으로 LLM에게 반영하거나 판단하도록 하는 것으로, ReACT 프레임워크에서 볼 수 있습니다. 이 방법의 장점은 모델을 미세 조정할 필요가 없다는 것입니다. 그러나 판단의 출력 형식은 LLM이 지침을 준수하는 정도에 따라 달라집니다. FLARE는 프롬프트 기반 사례입니다.\n\n- 조정 기반\n\n두 번째 접근 방식은 LLM이 특정 작업을 트리거하기 위해 특정 토큰을 생성하는 것인데, 이 방법은 Toolformer로 거슬러 올라갈 수 있으며 RAG에서 적용되며 Self-RAG에 적용됩니다.\n\n# 퓨전\n\n<div class=\"content-ad\"></div>\n\n이 개념은 RAG Fusion에서 유래되었습니다. 질의 확장 섹션에서 언급된 바와 같이 현재 RAG 프로세스는 더 이상 단일 파이프라인이 아닙니다. 종종 다양한 분기를 통해 검색 범위나 다양성을 확대해야 합니다. 따라서 여러 분기로 확장한 뒤에는 퓨전 모듈이 여러 대답을 병합하는데 필요합니다.\n\n- 가능성 앙상블\n\n퓨전 방법은 여러 분기에서 생성된 서로 다른 토큰의 가중치값에 기반하여 최종 출력물의 종합적 선택을 이끌어냅니다. 주로 가중 평균을 사용합니다. REPLUG 참조.\n\n- RRF (Reciprocal Rank Fusion)\n\n<div class=\"content-ad\"></div>\n\nRRF는 여러 검색 결과 목록의 순위를 결합하여 하나의 통합된 순위를 생성하는 기술입니다. 워털루 대학교 (캐나다)와 Google과의 협력으로 개발된 RRF는 단일 분기 아래의 청크를 재배치하는 것보다 더 효과적인 결과를 생성합니다.\n\n결론\n\nRAG Flow에 관한 내용은 곧 출판될 PART II에서 소개될 예정입니다.\n\n이번이 Medium에 기사를 처음 게시하는 것이라서 많은 기능에 아직 익숙해지고 있습니다. 피드백과 비평은 언제나 환영합니다.","ogImage":{"url":"/assets/img/2024-06-23-ModularRAGandRAGFlowPart_0.png"},"coverImage":"/assets/img/2024-06-23-ModularRAGandRAGFlowPart_0.png","tag":["Tech"],"readingTime":16},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<h1>소개</h1>\n<p>지난 일년 동안, 검색 증가 생성(Retrieval-Augmented Generation, RAG) 개념은 LLM 애플리케이션 구현 방법으로서 상당한 주목을 받았습니다. 저희는 RAG에 대한 포괄적인 조사 보고서를 작성했는데, 이 보고서는 Naive RAG에서 Advanced RAG 및 Modular RAG로의 전환에 대해 탐구했습니다. 그러나 이 조사는 주로 증가(Source/Stage/Process)를 통해 RAG 기술을 규명하였습니다.</p>\n<p>본 글은 특히 Modular RAG 패러다임을 중심으로 합니다. 우리는 Module Type, Module 및 Operator로 구성된 세 단계의 Modular RAG 패러다임을 더 확실히 정의했습니다. 이 패러다임에 따라, 현재 RAG 시스템 내 핵심 기술인 6가지 주요 Module Types, 14개의 Modules 및 40여 개의 Operators에 대해 상세히 다루어 RAG에 대한 포괄적인 이해를 제공하고 있습니다.</p>\n<p>다양한 Operator를 조합함으로써, 우리는 다양한 RAG 흐름을 유도할 수 있으며, 이 개념을 이 글에서 명확히 설명하고자 합니다. 광범위한 연구를 기반으로, 일반적인 패턴, 몇 가지 구체적인 구현 사례 및 최상의 업계 사례를 요약하였습니다. (공간 제약으로 인해 이 부분은 Part II에서 다룰 예정입니다.)</p>\n<div class=\"content-ad\"></div>\n<p>이 글의 목적은 현재 RAG 개발 상태에 대한 더 정교한 이해를 제공하고 미래 발전을 위한 길을 만들어주는 것입니다. 모듈식 RAG는 새로운 연산자, 모듈, 그리고 새로운 플로우의 구성을 용이하게 하는 다양한 기회를 제공합니다.</p>\n<h1>모듈식 RAG란 무엇인가요?</h1>\n<p>RAG의 발전은 다음과 같은 중요한 측면들에 반영되어 보다 다양하고 유연한 과정을 이끌어내고 있습니다:</p>\n<ul>\n<li>향상된 데이터 획득: RAG는 기존의 비구조적 데이터를 넘어 반구조적 및 구조적 데이터를 포함하고, 구조적 데이터의 전처리에 중점을 두어 검색의 품질을 향상시키고 모델이 외부 지식 소스에 의존하는 것을 줄이는 방향으로 확장되었습니다.</li>\n<li>통합된 기술: RAG는 세부 조정, 어댑터 모듈 사용, 강화 학습을 포함한 다른 기술들과 통합하여 검색 능력을 강화하고 있습니다.</li>\n<li>적응형 검색 프로세스: 검색 프로세스는 검색한 내용을 활용하여 생성을 안내하고 그 반대로 하는 등 다단계 검색 강화를 지원하도록 진화했습니다. 또한, 자율적인 판단과 LLM 사용을 통해 검색 필요성을 판단하여 질문에 대한 효율을 높였습니다.</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>모듈식 RAG의 정의</p>\n<p>위에서는 RAG의 신속한 발전이 체인 스타일의 고급 RAG 패러다임을 능가하여 모듈식 특성을 과시하고 있음을 볼 수 있습니다. 현재의 조직 부재와 추상화 부족을 해결하기 위해, Naive RAG와 Advanced RAG의 개발 패러다임을 무리없이 통합하는 모듈식 RAG 접근 방식을 제안합니다.</p>\n<p>모듈식 RAG는 매우 확장 가능한 패러다임을 제시하며, RAG 시스템을 모듈 유형, 모듈 및 연산자의 세 단계 구조로 나누고 있습니다. 각 모듈 유형은 RAG 시스템의 핵심 프로세스를 나타내며, 여러 기능 모듈을 포함하고 있습니다. 각 기능 모듈에는 또 다른 여러 특정 연산자가 포함되어 있습니다. 전체 RAG 시스템은 여러 모듈과 해당 연산자들의 순열과 조합으로 이루어진 RAG Flow를 형성하게 됩니다. Flow 내에서 각 모듈 유형에서 다른 기능 모듈을 선택할 수 있으며, 각 기능 모듈 내에서는 하나 이상의 연산자를 선택할 수 있습니다.</p>\n<p>이전 패러다임과의 관계</p>\n<div class=\"content-ad\"></div>\n<p>모듈식 RAG은 RAG 시스템을 다층 구조의 모듈식 형태로 조직합니다. 고급 RAG는 RAG의 모듈식 형태이며, Naive RAG는 고급 RAG의 특수한 경우입니다. 이 세 가지 패러다임 간의 관계는 상속과 발전의 하나입니다.</p>\n<p>모듈식 RAG의 장점</p>\n<p>모듈식 RAG의 이점은 명백하며, 기존의 RAG 관련 작업에 대한 신선하고 포괄적인 시각을 제공합니다. 모듈식 구성을 통해 관련 기술과 방법들이 명확하게 요약됩니다.</p>\n<ul>\n<li>연구적 시각. 모듈식 RAG는 확장성이 높아 연구자들이 현재 RAG 개발에 대한 포괄적인 이해를 기반으로 새로운 모듈 유형, 모듈, 그리고 연산자를 제안하기 쉽습니다.</li>\n<li>응용 시각. RAG 시스템의 설계 및 구성이 더 편리해지며, 사용자들이 기존 데이터, 사용 시나리오, 하향 작업 등에 따라 RAG Flow를 사용자 정의할 수 있습니다. 개발자들은 또한 현재 Flow 구성 방법을 참고하고, 다른 응용 시나리오와 도메인에 기반하여 새로운 플로우와 패턴을 정의할 수 있습니다.</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-23-ModularRAGandRAGFlowPart_0.png\" alt=\"Module\"></p>\n<h2>Module Type — Module — Operators</h2>\n<h3>1. Indexing</h3>\n<p>Indexing, the process of breaking down text into manageable chunks, is a crucial step in organizing the system, facing three main challenges:</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>완전하지 않은 내용 표현. 청크의 의미 정보는 세분화 방법에 영향을 받아 중요 정보가 잃거나 긴 문맥 속에서 잠기는 결과를 초래합니다.</li>\n<li>부정확한 청크 유사성 검색. 데이터 양이 증가함에 따라 검색에서의 잡음이 커져 잘못된 데이터와 빈번히 일치하게 되어 검색 시스템을 취약하고 신뢰할 수 없게 만듭니다.</li>\n<li>명확하지 않은 참조 궤적. 검색된 청크는 어느 문서에서든 유래할 수 있으며 인용 트레일이 없어, 다수의 다른 문서에서 유사하게 의미가 있는 혹은 완전히 다른 주제의 내용을 포함할 수 있습니다.</li>\n</ul>\n<h2>청크 최적화</h2>\n<p>더 큰 청크는 더 많은 문맥을 포착할 수 있지만, 더 많은 잡음을 생성하여 더 오랜 처리 시간과 높은 비용이 필요합니다. 반면 더 작은 청크는 필요한 문맥을 완전히 전달하지 않을 수 있지만, 더 적은 잡음을 가지고 있습니다.</p>\n<ul>\n<li>슬라이딩 윈도우</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>이러한 요구 사항을 균형 있게 조절하는 한 가지 간단한 방법은 중첩 청크를 사용하는 것입니다. 슬라이딩 창을 활용하면 의미적 전환을 향상시킬 수 있습니다. 그러나 의미적 고려 사항이 부족하다는 제한 사항이 있습니다. - 작은 청크에서 큰 청크로 핵심 아이디어는 검색에 사용되는 청크와 합성에 사용되는 청크를 분리하는 것입니다. 더 작은 청크를 사용하면 검색의 정확도가 향상되고, 더 큰 청크는 더 많은 컨텍스트 정보를 제공할 수 있습니다.</p>\n<p><img src=\"/assets/img/2024-06-23-ModularRAGandRAGFlowPart_1.png\" alt=\"이미지\"></p>\n<div class=\"content-ad\"></div>\n<p>특히 한 가지 방법은 더 작은 청크를 검색한 다음 부모 ID를 참조하여 더 큰 청크를 반환하는 것일 수 있습니다. 또는 개별 문장을 검색하고 문장 주변 텍스트 창을 반환할 수도 있습니다.</p>\n<p>자세한 정보 및 LlamaIndex 구현.</p>\n<ul>\n<li>요약</li>\n</ul>\n<p>이는 작은 것에서 큰 것으로의 개념과 유사하며, 먼저 더 큰 청크의 개요가 생성되고, 이후 개요에 대해 검색이 수행됩니다. 그런 다음 더 큰 청크에 대해 보조 검색을 수행할 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>메타데이터 첨부</li>\n</ul>\n<p>청크에 페이지 번호, 파일 이름, 작성자, 타임스탬프, 요약 또는 청크가 대답할 수 있는 질문과 같은 메타데이터 정보를 포함시킬 수 있습니다. 그 결과로 검색 범위를 제한할 수 있는 이 메타데이터를 기반으로 검색이 필터링될 수 있습니다. LlamaIndex에서 이 구현을 확인해보세요.</p>\n<h1>구조적 구성</h1>\n<p>정보 검색을 강화하는 효과적인 방법 중 하나는 문서에 대한 계층적 구조를 설정하는 것입니다. 청크 구조를 구성함으로써 RAG 시스템은 적절한 데이터의 검색과 처리를 가속화할 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>계층적 색인</li>\n</ul>\n<p>문서의 계층 구조에서 노드는 부모-자식 관계로 배열되어 있으며 이들에 연결된 청크가 있습니다. 각 노드에는 데이터 요약이 저장되어 있어 데이터를 신속하게 탐색하고 RAG 시스템이 어떤 청크를 추출해야 하는지 결정하는 데 도움이 됩니다. 이 접근 방식은 블록 추출 문제로 인한 오류를 완화하는 데도 도움이 됩니다.</p>\n<p>구조적 색인을 구축하는 주요 방법은 다음과 같습니다:</p>\n<ul>\n<li>구조 인식: 문서에서 단락과 문장을 분할</li>\n<li>콘텐츠 인식: PDF, HTML, Latex에 내재된 구조</li>\n<li>의미 인식: NLP 기술을 활용한 텍스트의 의미 인식과 분할, NLTK와 같은 기법 사용</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>대규모로 아커스의 계층적 인덱스를 확인하실 수 있습니다.</p>\n<ul>\n<li>KG 조직 문서</li>\n</ul>\n<p>지식 그래프(KGs)를 활용하여 문서의 계층 구조를 구축함으로써 일관성을 유지할 수 있습니다. 이는 다양한 개념과 엔티티 간의 연결을 명확히 하고 환각 가능성을 크게 줄입니다.</p>\n<p>다른 이점은 정보 검색 프로세스를 LLM이 이해할 수 있는 지침으로 변환하여 지식 검색의 정확성을 향상시키고 LLM이 맥락에 부합한 응답을 생성할 수 있도록 함으로써 RAG 시스템의 전체적인 효율성을 향상시킵니다.</p>\n<div class=\"content-ad\"></div>\n<p>Neo4j 구현 및 LllmaIndex Neo4j 쿼리 엔진을 확인해보세요.</p>\n<p>KG를 사용하여 여러 문서를 조직하는 경우, 이 연구 논문 KGP: 지식 그래프 프롬프팅을 참고하실 수 있습니다.</p>\n<h1>2. 사전 검색</h1>\n<p>Naive RAG의 주요 도전 중 하나는 사용자의 원본 쿼리에 직접 의존하고 있다는 것입니다. 정확하고 명확한 질문을 구성하는 것은 어려우며, 무분별한 쿼리는 하위 수준의 검색 효과를 초래합니다.</p>\n<div class=\"content-ad\"></div>\n<p>이 단계에서의 주요 도전 과제는 다음과 같습니다:</p>\n<ul>\n<li>제대로 작성되지 않은 질문들입니다. 질문 자체가 복잡하며, 언어가 잘 정리되지 않았습니다.</li>\n<li>언어 복잡성 및 모호함입니다. 언어 모델은 종종 전문 용어나 다의어적 약어를 다룰 때 어려움을 겪습니다. 예를 들어, “LLM”이 큰 언어 모델을 의미하는지 법적 맥락에서의 법학 석사를 나타내는지를 구분하지 못할 수 있습니다.</li>\n</ul>\n<h2>질의 확장</h2>\n<p>단일 질문을 복수의 질문으로 확장하면 질문의 내용을 풍부하게 만들어 특정 뉘앙스의 부족을 보충함으로써 생성된 답변의 최적적인 관련성을 보장할 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>다중 쿼리</li>\n</ul>\n<p>LLM을 통해 쿼리를 확장하기 위해 prompt 엔지니어링을 사용하여 이러한 쿼리들을 병렬로 실행할 수 있습니다. 쿼리의 확장은 무작위가 아니라 신중하게 설계된 것입니다. 이 설계의 두 가지 중요한 기준은 쿼리의 다양성과 범위입니다.</p>\n<p>여러 쿼리를 사용하는 한 가지 어려움은 사용자의 원래 의도가 희석될 수 있다는 것입니다. 이를 완화하기 위해 우리는 prompt 엔지니어링에서 모델에게 원본 쿼리에 더 높은 중요도를 할당하도록 지시할 수 있습니다.</p>\n<ul>\n<li>하위 쿼리</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>서브 질문 계획 프로세스는 원본 질문을 완전히 대답할 수 있는 필요한 서브 질문을 생성하는 것을 나타냅니다. 관련 컨텍스트를 추가하는 이 프로세스는 원칙적으로 쿼리 확장과 유사합니다. 구체적으로, 복잡한 질문은 적은 것에서 많은 것으로 유도하는 방법을 사용하여 일련의 보다 간단한 서브 질문으로 분해될 수 있습니다.</p>\n<ul>\n<li>CoVe</li>\n</ul>\n<p>쿼리 확장에 대한 다른 접근 방식은 Meta AI가 제안한 Chain-of-Verification(CoVe)의 사용을 포함합니다. 확장된 쿼리는 LLM에 의해 유효성을 검사하여 환각을 줄이는 효과를 얻습니다. 유효성이 검증된 확장된 쿼리는 전형적으로 더 높은 신뢰성을 보입니다.</p>\n<h1>쿼리 변환</h1>\n<div class=\"content-ad\"></div>\n<ul>\n<li>다시 작성</li>\n</ul>\n<p>원본 쿼리는 실제 상황에서 LLM 검색에 항상 최적이 아닙니다. 따라서, 우리는 LLM에 대해 쿼리를 다시 작성하도록 유도할 수 있습니다. 쿼리 다시 작성을 위해 LLM을 사용하는 것 외에도, RRR(Rewrite-retrieve-read)과 같은 특수한 작은 언어 모델을 활용할 수 있습니다.</p>\n<p>타오바오 프로모션 시스템에서 쿼리 다시 작성 방법인 BEQUE:Query Rewriting for Retrieval-Augmented Large Language Models의 구현은 장인 쿼리에 대한 회수 효과를 현저히 향상시켰으며, GMV 상승으로 이어졌습니다.</p>\n<ul>\n<li>HyDE</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>쿼리에 대답할 때 LLM은 벡터 데이터베이스의 쿼리 및 계산된 벡터를 직접 검색하는 대신 가정된 답변인 가상 문서를 작성합니다. 이는 문제나 쿼리의 임베딩 유사성을 찾는 대신 답변 간의 임베딩 유사성에 초점을 맞추고 있습니다. 또한 쿼리 간의 검색에 초점을 맞춘 Reverse HyDE도 포함되어 있습니다.</p>\n<p><img src=\"/assets/img/2024-06-23-ModularRAGandRAGFlowPart_2.png\" alt=\"이미지\"></p>\n<ul>\n<li>Step-back Prompting</li>\n</ul>\n<p>Google DeepMind가 제안한 Step-back Prompting 방법을 사용하면 원본 쿼리를 추상화하여 고수준 개념 질문인 스텝백 질문을 생성할 수 있습니다. RAG 시스템에서는 스텝백 질문과 원본 쿼리 모두 검색에 사용되며, 두 결과 모두 언어 모델 답변 생성의 기초로 활용됩니다.</p>\n<div class=\"content-ad\"></div>\n<h1>쿼리 라우팅</h1>\n<p>다양한 쿼리에 따라 다른 RAG 파이프라인으로 라우팅되며, 다양한 시나리오를 수용할 수 있는 유연한 RAG 시스템에 적합합니다.</p>\n<ul>\n<li>메타데이터 라우터/ 필터</li>\n</ul>\n<p>첫 번째 단계는 쿼리에서 키워드(엔티티)를 추출한 후, 키워드 및 청크 내의 메타데이터를 기반으로 필터링하여 검색 범위를 좁히는 것입니다.</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>Semantic Router</li>\n</ul>\n<p>라우팅하는 또 다른 방법은 쿼리의 의미 정보를 활용하는 것입니다. 특정 방법은 Semantic Router를 활용합니다. 물론 의미론적 및 메타데이터 기반 방법을 결합하여 질의 라우팅을 향상시키는 하이브리드 라우팅 방법도 사용할 수 있습니다.</p>\n<p>Semantic router 리포지토리를 확인해보세요.</p>\n<h1>쿼리 구성</h1>\n<div class=\"content-ad\"></div>\n<p>유저의 쿼리를 다른 쿼리 언어로 변환하여 대체 데이터 소스에 접근하는 작업입니다. 일반적으로 사용되는 방법은 다음과 같습니다:</p>\n<ul>\n<li>텍스트를 Cypher로 변환</li>\n<li>텍스트를 SQL로 변환</li>\n</ul>\n<p>많은 시나리오에서는 구조화된 쿼리 언어(SQL, Cypher 등)가 의미 정보 및 메타데이터와 결합되어 더 복잡한 쿼리를 작성하는 데 자주 사용됩니다. 자세한 내용은 Langchain 블로그를 참조해주세요.</p>\n<h1>3 조회</h1>\n<div class=\"content-ad\"></div>\n<p>검색 과정은 RAG에서 중요한 역할을 합니다. 강력한 PLM을 활용하여 쿼리와 텍스트를 잠재적 공간에 효과적으로 표현하여 질문과 문서 간의 의미 유사성을 성립하는 데 도움이 됩니다.</p>\n<p>고려해야 할 세 가지 주요 사항:</p>\n<ul>\n<li>검색 효율성</li>\n<li>임베딩 품질</li>\n<li>작업, 데이터 및 모델의 정렬</li>\n</ul>\n<h1>검색기 선택</h1>\n<div class=\"content-ad\"></div>\n<p>ChatGPT가 출시된 이후에는 임베딩 모델 개발에 대한 열기가 높아졌어요. Hugging Face의 MTEB 리더보드는 8가지 작업(클러스터링, 분류, 이중 텍스트, 쌍 분류, 재랭킹, 정보 검색, 시맨틱 텍스트 유사성 및 요약)에 걸쳐 거의 모든 가능한 임베딩 모델을 평가합니다. 또한, C-MTEB는 중국어 임베딩 모델의 능력을 평가하며, 6가지 작업과 35개의 데이터셋을 다루고 있어요.</p>\n<p>RAG 애플리케이션을 구축할 때, \"어떤 임베딩 모델을 사용해야 할까?\"에 대한 보편적인 해답은 없습니다. 그러나 특정 임베딩이 특정 사용 사례에 더 적합하다는 점을 알 수 있을 거예요.</p>\n<p>MTEB/C-MTEB 리더보드를 확인해보세요.</p>\n<ul>\n<li>Sparse Retriever</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>희박 인코딩 모델은 다소 구식 기술로 간주될 수 있지만 단어 빈도 통계와 같은 통계적 방법에 기반하고 있어서 높은 인코딩 효율성과 안정성을 유지하고 있습니다. 일반적인 계수 인코딩 모델로는 BM25와 TF-IDF가 있습니다.</p>\n<ul>\n<li>조밀 검색기</li>\n</ul>\n<p>신경망 기반의 밀도 인코딩 모델에는 여러 종류가 있습니다:</p>\n<ul>\n<li>BERT 아키텍처에서 구축된 인코더-디코더 언어 모델인 ColBERT와 같은 모델.</li>\n<li>BGE 및 Baichuan-Text-Embedding과 같은 포괄적인 다중 작업 파인튜닝 모델.</li>\n<li>OpenAI-Ada-002 및 Cohere Embedding과 같은 클라우드 API 기반 모델.</li>\n<li>대규모 데이터 애플리케이션을 위해 설계된 다음 세대 가속화 인코딩 프레임워크 Dragon+.</li>\n<li>혼합/하이브리드 검색</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>두 가지 임베딩 접근 방식은 서로 다른 관련 기능을 캡쳐하고 상호 보완적인 관련 정보를 활용함으로써 상호 이득을 얻을 수 있습니다. 예를 들어, 희박 검색 모델은 밀도 검색 모델을 교육하기 위한 초기 검색 결과를 제공하는 데 사용될 수 있습니다. 게다가 PLM(Pre-trained Language Models)은 희소 검색을 강화하기 위해 용어 가중치를 학습하는 데 활용될 수 있습니다. 구체적으로, 희소 검색 모델이 밀도 검색 모델의 제로샷 검색 기능을 강화하고 희귀 엔티티를 포함하는 쿼리를 처리하는 데 밀도 리트리버를 돕는 것이 증명되었습니다.</p>\n<p><img src=\"/assets/img/2024-06-23-ModularRAGandRAGFlowPart_3.png\" alt=\"이미지\"></p>\n<h1>리트리버 파인튜닝</h1>\n<p>특히 의료, 법률 및 소유 용어가 풍부한 기타 전문 분야와 같이 사전 훈련된 모델이 임베딩 공간에서 유사하다고 판단하는 컨텍스트가 벗어날 수 있는 경우, 임베딩 모델을 조정함으로써 이 문제를 해결할 수 있습니다. 이러한 조정은 추가적인 노력이 필요하지만 검색 효율성과 도메인 정렬을 크게 향상시킬 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>SFT</li>\n</ul>\n<p>도메인별 데이터를 기반으로 자체 세부 조정 데이터 세트를 구축할 수 있습니다. LlamaIndex를 사용하면 빠르게 작업을 완료할 수 있어요.</p>\n<ul>\n<li>LSR (LM-지도 학습 리트리버)</li>\n</ul>\n<p>직접 데이터 세트에서 세부 조정 데이터 세트를 구성하는 대신 LSR은 LM이 생성한 결과를 우선적 신호로 사용하여 RAG 프로세스 중에 임베딩 모델을 세부 조정합니다.</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>RL(강화 학습)</li>\n</ul>\n<p>RLHF(사람 피드백에서 강화 학습)에서 영감을 받아 LM 기반 피드백을 활용하여 강화 학습을 통해 리트리버를 강화합니다.</p>\n<ul>\n<li>어댑터</li>\n</ul>\n<p>가끔 전체 리트리버를 세밀하게 조정하는 것은 비용이 많이 들 수 있으며, 특히 API 기반 리트리버를 직접 세밀하게 조정할 수 없는 경우에는 어댑터 모듈을 통합하고 세밀 조정을 진행함으로써 이를 완화할 수 있습니다. 어댑터를 추가하는 또다른 이점은 특정 다운스트림 작업과의 더 나은 조정을 달성할 수 있는 능력입니다.</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>Task Specific.PRCA: 검색 질의응답을 위한 블랙박스 대규모 언어 모델에 플러그인 리워드 기반 컨텍스트 어댑터를 적합화하는 작업.</li>\n<li>Task Agnostic. AAR(Augmentation-Adapted Retriver)는 여러 하향 작업을 수용하기 위해 설계된 범용 어댑터를 소개합니다.</li>\n</ul>\n<h2>4. 검색 후</h2>\n<p>전체 문서 청크를 검색하여 이를 LLM의 컨텍스트 환경에 직접 공급하는 것은 최적의 선택이 아닙니다. 문서 후처리는 LLM이 컨텍스트 정보를 보다 효과적으로 활용하는 데 도움이 될 수 있습니다.</p>\n<p>주요 도전 과제는 다음과 같습니다:</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>중간에 길들이 미아 되다. 인간처럼 LLM은 긴 텍스트의 처음과 끝만을 기억하고, 중간 부분은 잊어 버리기 쉽습니다.</li>\n<li>소음/반사실적 청크. 소음이나 사실적으로 모순된 문서를 검색하면 최종 검색 결과 생성에 영향을 줄 수 있습니다.</li>\n<li>맥락 창. 큰 모델에서 관련 콘텐츠를 상당량 검색하더라도 맥락 정보의 길이에 제한이 있어 모든 이 콘텐츠를 포함시키기 어렵습니다.</li>\n</ul>\n<h1>재랭크</h1>\n<p>검색된 문서 청크들을 콘텐츠나 길이를 변경하지 않고, LLM에 대한 더 중요한 문서 청크의 가시성을 높이기 위해 재랭크합니다. 구체적으로는:</p>\n<ul>\n<li>규칙 기반 재랭크</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>특정 규칙에 따라 메트릭을 계산하여 청크를 다시 순위 지정합니다. 일반적인 메트릭은 다음과 같습니다:</p>\n<ul>\n<li>다양성</li>\n<li>관련성</li>\n<li>MRR (최대 주변 관련성, 1998)</li>\n</ul>\n<p>MMR의 아이디어는 중복을 줄이고 결과 다양성을 높이는 데 있습니다. 텍스트 요약에 사용되며 쿼리 관련성과 정보의 독창성을 병합 기준으로 사용하여 최종 키워드 목록에서 구절을 선택합니다.</p>\n<p>HayStack에서 해당 순위 지정 구현을 확인해보세요.</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>모델 기반 재순위</li>\n</ul>\n<p>언어 모델을 활용하여 문서 청크를 재정렬하고, 선택지는 다음과 같습니다:</p>\n<ul>\n<li>BERT 시리즈의 인코더-디코더 모델인 SpanBERT</li>\n<li>Cohere rerank 또는 bge-raranker-large와 같은 특화된 재순위 모델</li>\n<li>GPT-4와 같은 일반적인 대규모 언어 모델</li>\n</ul>\n<h1>압축 및 선택</h1>\n<div class=\"content-ad\"></div>\n<p>RAG 프로세스에서 흔한 오해는 가능한 많은 관련 문서를 검색하여 연결하여 긴 검색 프롬프트를 형성한다는 것이 유익하다는 믿음입니다. 그러나 과도한 문맥은 더 많은 잡음을 도입할 수 있으며, LLM이 주요 정보를 인식하는 것을 약화시키고 \"중간에서 잃어버림\"과 같은 문제로 이어질 수 있습니다. 이러한 문제를 해결하는 일반적인 접근법은 검색된 콘텐츠를 압축하고 선택하는 것입니다.</p>\n<ul>\n<li>（Long)LLMLingua</li>\n</ul>\n<p>GPT-2 Small이나 LLaMA-7B와 같은 정련된 작은 언어 모델을 사용하여 중요하지 않은 토큰을 검색 프롬프트에서 감지하고 제거하여, 사람들이 이해하기 어려운 형태에서 LLM이 잘 이해하는 형태로 변환됩니다. 이 접근 방식은 검색 프롬프트 압축을 위한 직접적이고 실용적인 방법을 제시하여, LLM의 추가 훈련 필요를 없애면서 언어 무결성과 압축 비율을 균형있게 유지합니다.</p>\n<p>LLMLingua 프로젝트를 확인해보세요.</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>Recomp</li>\n</ul>\n<p>Recomp은 두 가지 유형의 압축기를 소개합니다: 검색된 문서에서 적절한 문장을 선택하는 추출 압축기 및 여러 문서에서 정보를 결합하여 간결한 요약을 생성하는 추상적 압축기입니다. 두 압축기 모두 생성된 요약이 언어 모델의 입력에 앞부분에 추가될 때 최종 작업에서 언어 모델의 성능을 향상시키도록 훈련되었으며, 요약의 간결성을 보장합니다. 검색된 문서가 입력과 관련이 없거나 언어 모델에 추가 정보를 제공하지 않는 경우, 압축기는 빈 문자열을 반환할 수 있어 선택적 확장을 구현할 수 있습니다.</p>\n<ul>\n<li>선택적 컨텍스트</li>\n</ul>\n<p>입력 컨텍스트에서 중복되는 콘텐츠를 식별하고 제거함으로써 입력을 최적화하여 언어 모델의 추론 효율성을 향상할 수 있습니다. 선택적 컨텍스트는 \"불용어 제거\" 전략과 유사합니다. 실제로, 선택적 컨텍스트는 기본 언어 모델에 의해 계산된 자기 정보에 기반하여 어휘 단위의 정보 콘텐츠를 평가합니다. 더 높은 자기 정보를 갖는 콘텐츠를 유지함으로써, 이 방법은 언어 모델 처리를 위한 보다 간결하고 효율적인 텍스트 표현을 제공하며, 다양한 응용 프로그램에서의 성능을 저해시키지 않습니다. 그러나 압축된 콘텐츠와 대상 언어 모델 및 압축을 위해 사용되는 작은 언어 모델 간의 일치를 간과합니다.</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>태깅-필터</li>\n</ul>\n<p>태깅은 비교적 직관적이고 명료한 방식입니다. 특히, 문서들이 먼저 라벨이 붙여지고, 그 후 쿼리의 메타데이터를 기반으로 필터링됩니다.</p>\n<ul>\n<li>LLM-비평</li>\n</ul>\n<p>다른 명확하고 효과적인 방법은 LLM이 최종 답변을 생성하기 전에 검색된 콘텐츠를 평가하도록 하는 것입니다. 이를 통해 LLM은 LLM 비평을 통해 적합하지 않은 문서들을 걸러낼 수 있습니다. 예를 들어, Chatlaw에서 LLM은 참조된 법적 규정에 대한 자체 제안을 받아 그들의 적합성을 평가하게 됩니다.</p>\n<div class=\"content-ad\"></div>\n<h1>5세대</h1>\n<p>사용자의 쿼리와 검색된 컨텍스트 정보에 기반하여 답변을 생성하기 위해 LLM을 활용하십시오.</p>\n<h1>생성기 선택</h1>\n<p>시나리오에 따라 LLM의 선택은 다음 두 가지 유형으로 분류될 수 있습니다:</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>클라우드 API 기반 생성기</li>\n</ul>\n<p>클라우드 API를 기반으로하여 OpenAI의 ChatGPT, GPT-4, Anthropic Claude 등을 활용하여 타사 LLMs를 호출합니다. 이점:</p>\n<ul>\n<li>서버 압력이 없음</li>\n<li>높은 동시성</li>\n<li>보다 강력한 모델 활용 가능</li>\n</ul>\n<p>단점:</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>데이터가 제3자를 통해 전달되어 데이터 개인 정보 보호에 대한 우려가 생깁니다</li>\n<li>모델을 조정할 수 없음 (대부분의 경우)</li>\n<li>온-프레미스</li>\n</ul>\n<p>로컬에 배포된 오픈 소스 또는 자체 개발된 LLMs, 예를 들어 람마 시리즈, GLM 등니다. 로컬에 배포된 모델은 클라우드 API 기반 모델과 정반대의 장단점을 가지고 있습니다. 로컬에 배포된 모델은 더 큰 유연성과 더 나은 개인 정보 보호를 제공하지만 더 많은 계산 리소스가 필요합니다.</p>\n<h1>생성기 세부 조정</h1>\n<p>LLM 사용뿐만 아니라 시나리오와 데이터 특성에 기반한 목표 지향적인 세부 조정은 더 나은 결과를 얻을 수 있습니다. 이것은 온-프레미스 설정을 사용하는 가장 큰 장점 중 하나이기도 합니다. 일반적인 세부 조정 방법은 다음과 같습니다:</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>SFT</li>\n</ul>\n<p>특정 도메인에서 데이터가 부족한 경우, LLM에게 추가 지식을 제공하여 fine-tuning을 통해 LLM에게 도움이 될 수 있습니다. Huggingface의 fine-tuning 데이터 또한 초기 단계로 활용될 수 있습니다.</p>\n<p>Fine-tuning의 또 다른 이점은 모델의 입력과 출력을 조정할 수 있는 능력입니다. 예를 들어, 이것은 LLM이 특정 데이터 형식에 적응하고 지시된 특정한 스타일로 응답을 생성할 수 있도록 할 수 있습니다.</p>\n<ul>\n<li>RL</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>LLM 출력물을 인간 또는 검색기 선호사항에 맞게 조정하는 것은 가능한 접근 방식입니다. 예를 들어 최종 생성된 답변에 대해 수동으로 주석을 달고 그에 대한 피드백을 통해 강화 학습을 제공하는 것이 있습니다. 인간 선호도에 맞추는 것뿐만 아니라 세밀하게 조정된 모델과 검색기의 선호도에도 부합시킬 수 있습니다.</p>\n<ul>\n<li>증류</li>\n</ul>\n<p>강력한 사유재 모델이나 더 큰 매개변수의 오픈소스 모델에 액세스 할 수 없는 상황에서, 강력한 모델(e.g. GPT-4)을 증류하는 간단하고 효과적인 방법이 있습니다.</p>\n<ul>\n<li>이중 미세 조정</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>RAG 프로세스를 제어하는 데 사용되는 모듈을 가리키는 Orchestration입니다. RAG는 더 이상 고정 프로세스를 따르지 않고, 주요 시점에서 결정을 내리고 결과에 따라 동적으로 다음 단계를 선택하는 것을 의미합니다. 이것은 Naive RAG와 비교해 모듈화된 RAG의 주요 특징 중 하나입니다.</p>\n<div class=\"content-ad\"></div>\n<p>판사 모듈은 RAG 프로세스의 중요한 부분을 평가하여 외부 문서 저장소를 검색해야 하는 필요성, 답변의 만족도 및 추가적인 탐색이 필요한지를 결정합니다. 주로 반복적이고 반복적이며 적응적인 검색에 사용됩니다. 구체적으로는 다음 두 가지 연산자가 주로 포함됩니다:</p>\n<ul>\n<li>규칙 기반</li>\n</ul>\n<p>미리 정의된 규칙을 기반으로 후속 조치가 결정됩니다. 일반적으로 생성된 답변은 평가되고, 그런 다음 점수가 미리 정의된 임계값을 충족하는지에 따라 계속할지 중지할지가 결정됩니다. 일반적인 임계값에는 토큰의 확신 수준이 포함됩니다.</p>\n<ul>\n<li>프롬프트 기반</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>LLM은 다음 조치를 자율적으로 결정합니다. 이를 실현하는 데 주로 두 가지 접근 방식이 있습니다. 첫 번째는 대화 기록을 기반으로 LLM에게 반영하거나 판단하도록 하는 것으로, ReACT 프레임워크에서 볼 수 있습니다. 이 방법의 장점은 모델을 미세 조정할 필요가 없다는 것입니다. 그러나 판단의 출력 형식은 LLM이 지침을 준수하는 정도에 따라 달라집니다. FLARE는 프롬프트 기반 사례입니다.</p>\n<ul>\n<li>조정 기반</li>\n</ul>\n<p>두 번째 접근 방식은 LLM이 특정 작업을 트리거하기 위해 특정 토큰을 생성하는 것인데, 이 방법은 Toolformer로 거슬러 올라갈 수 있으며 RAG에서 적용되며 Self-RAG에 적용됩니다.</p>\n<h1>퓨전</h1>\n<div class=\"content-ad\"></div>\n<p>이 개념은 RAG Fusion에서 유래되었습니다. 질의 확장 섹션에서 언급된 바와 같이 현재 RAG 프로세스는 더 이상 단일 파이프라인이 아닙니다. 종종 다양한 분기를 통해 검색 범위나 다양성을 확대해야 합니다. 따라서 여러 분기로 확장한 뒤에는 퓨전 모듈이 여러 대답을 병합하는데 필요합니다.</p>\n<ul>\n<li>가능성 앙상블</li>\n</ul>\n<p>퓨전 방법은 여러 분기에서 생성된 서로 다른 토큰의 가중치값에 기반하여 최종 출력물의 종합적 선택을 이끌어냅니다. 주로 가중 평균을 사용합니다. REPLUG 참조.</p>\n<ul>\n<li>RRF (Reciprocal Rank Fusion)</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>RRF는 여러 검색 결과 목록의 순위를 결합하여 하나의 통합된 순위를 생성하는 기술입니다. 워털루 대학교 (캐나다)와 Google과의 협력으로 개발된 RRF는 단일 분기 아래의 청크를 재배치하는 것보다 더 효과적인 결과를 생성합니다.</p>\n<p>결론</p>\n<p>RAG Flow에 관한 내용은 곧 출판될 PART II에서 소개될 예정입니다.</p>\n<p>이번이 Medium에 기사를 처음 게시하는 것이라서 많은 기능에 아직 익숙해지고 있습니다. 피드백과 비평은 언제나 환영합니다.</p>\n</body>\n</html>\n"},"__N_SSG":true}