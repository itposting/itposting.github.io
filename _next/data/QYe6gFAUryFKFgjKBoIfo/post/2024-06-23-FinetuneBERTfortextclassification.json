{"pageProps":{"post":{"title":"BERT 미세 조정으로 텍스트 분류하는 방법","description":"","date":"2024-06-23 19:32","slug":"2024-06-23-FinetuneBERTfortextclassification","content":"\n\n섬세 조정은 대형 언어 모델이 사용자 지정 데이터에 적응하고 텍스트 분류와 같은 하향 작업을 잘 수행할 수 있도록 돕는 중요한 기술입니다.\n\n본 문서는 섬세 조정의 기본에 초점을 맞추고, LORA, QLORA 등 다른 기술에 대해 깊게 다루지는 않습니다. 시작하는 가장 좋은 방법은 BERT로 실험을 해보는 것입니다.\n\n주로 두 가지 방법으로 이 작업을 수행할 수 있습니다:\n\n- 허깅페이스 트레이너 API 사용: 사용하기 쉽지만 매우 사용자 정의가 어려움\n- PyTorch 사용: 트레이너보다 조금 어려우나 프로세스에 대한 더 많은 사용자 정의와 제어를 제공합니다\n\n<div class=\"content-ad\"></div>\n\n데이터셋\n\n우리는 Hugging Face에서 제공하는 Yelp Reviews 데이터셋을 사용할 예정입니다. 이 데이터셋은 다음 두 열로 구성되어 있습니다:\n\n- 레이블: 1부터 5까지의 별표가 부여된 등급입니다.\n- 텍스트: 리뷰 내용입니다.\n\n저희의 목표는 리뷰 텍스트로부터 별의 개수를 예측할 수 있는 모델을 훈련하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n휍핑페이스 트레이너 API를 사용하여 파인튜닝하기\n\n- 모든 라이브러리를 설치하세요 :\n\n```js\n!pip install --upgrade transformers datasets evaluate huggingface_hub torch\n```\n\n참고: 이 라이브러리들의 최신 버전을 항상 사용하도록 하세요.\n\n<div class=\"content-ad\"></div>\n\n2. 데이터셋 로드: Hugging Face에서 제공하는 datasets 라이브러리를 사용하여 데이터셋을 로드할 수 있어요.\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"yelp_review_full\")\n```\n\n데이터셋을 확인해봐요. 어떤 데이터를 다루게 될지 알아봅시다.\n\n```python\ndataset[\"train\"][1]\n```\n\n<div class=\"content-ad\"></div>\n\n우리 데이터가 어떻게 보이는지 확인해보세요.\n\n```js\n{'label': 1,\n 'text': \"안타깝게도 Dr. 골트버그의 환자로서 느끼는 좌절은 뉴욕의 다른 많은 의사들과 겪어온 경험의 반복입니다 - 좋은 의사, 하지만 최악의 스태프. 그의 스텝은 단순히 전화를 받지 않는 것 같습니다. 답변을 받으려면 보통 반복적인 전화로 2시간이 걸립니다. 누가 그런 시간을 가진 사람이며 누가 그것과 소통하길 원하겠습니까? 다른 많은 의사들과도 이 문제를 겪어왔고, 이해가 안 가네요. 사무원이 있고 의료 필요가 있는 환자가 있는데, 왜 전화를 받는 사람이 없는 건지요? 이해할 수 없고, 신경질만 나게 합니다. Dr. 골트버그에게 2점을 주어야 하는 점이 유감입니다.\"}\n```\n\n3. 토크나이저를 로드하고 텍스트를 토큰화하는 함수를 만들어보세요:\n\n```js\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n```\n\n<div class=\"content-ad\"></div>\n\n토크나이저는 텍스트를 입력_ids, 토큰_유형_ids 및 어텐션_마스크로 이해할 수 있는 세 개의 열로 변환합니다.\n\n데이터셋에서 작은 배치를 만들기(선택 사항)\n\n```js\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n```\n\n4. 모델 불러오기:\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n```\n\n분류할 레이블 수를 초기화하려면 num_labels 매개변수를 사용하세요.\n\n5. 훈련 인수 초기화\n\n```python\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\")\n```\n\n<div class=\"content-ad\"></div>\n\nhttps://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments에서 제공되는 매개변수에 대한 자세한 정보를 확인할 수 있습니다.\n\n6. 메트릭 계산 함수 설정:\n\n```js\nimport numpy as np\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n```\n\n7. 학습 시작:\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom transformers import Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()\n```\n\n트레이닝을 시작하려면 wandb 키를 입력하라는 프롬프트가 나타납니다. 키를 입력하면 트레이닝 프로세스가 시작됩니다. 트레이닝이 완료되면 아래와 같은 결과를 보게 될 것입니다.\n\n![트레이닝 결과](/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png)\n\n선택적으로 노트북에서 허깅페이스로 모델을 저장할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n```js\nfrom huggingface_hub import login\nlogin()\nmodel.push_to_hub(\"HuggingfaceUsername/yourModelName\")\n```\n\n8. 추론 실행:\n\n모델을 테스트하려면 PyTorch를 사용할 수 있습니다.\n\n```js\nimport torch\nimport torch.nn.functional as F\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"HuggingfaceUsername/yourModelName\")\ns=\"The was awesome and I loved it\"\ntt=tokenizer(s,return_tensors=\"pt\", padding=True, truncation=True)\n```\n\n\n<div class=\"content-ad\"></div>\n\n모델을 평가 모드로 설정하면 더 이상 가중치를 업데이트할 필요가 없어지고, 이제 분류 작업에 사용할 수 있습니다.\n\n```python\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(**tt)\n```\n\n결과를 확인해보겠습니다.\n\n```python\nSequenceClassifierOutput(loss=None, logits=tensor([[-2.3995, -2.0111, -0.8381,  2.4683,  2.8968]]), hidden_states=None, attentions=None)\n```\n\n<div class=\"content-ad\"></div>\n\n여기서 중요한 변수는 로짓 변수입니다. 이 경우 로짓은 텍스트가 특정 클래스에 속할 확률을 나타냅니다. 현재 로짓은 이해하기 어려운 형식으로 표시됩니다. 이를 이해할 수 있는 형식으로 변환하려면 이해할 수 있는 숫자로 변환해야 합니다.\n\n```js\nlogits = outputs.logits\nprint(\"로짓:\", logits)\n\n# 소프트맥스를 사용하여 로짓을 확률로 변환합니다\nprobabilities = F.softmax(logits, dim=-1)\nprint(\"확률:\", probabilities)\n\n# 예측된 클래스를 결정합니다\npredicted_class = torch.argmax(probabilities, dim=-1)\nprint(\"예측된 클래스:\", predicted_class.item())\n```\n\n여기서 출력은 4입니다.\n\nPyTorch를 사용한 파인 튜닝\n\n<div class=\"content-ad\"></div>\n\n모델이 이해할 수 있도록 몇 가지 전처리 단계가 필요합니다.\n\n- 열 삭제\n\n```js\ntokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\ntokenized_datasets.set_format(\"torch\")\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n```\n\n2. 데이터로더(Dataloader) 생성\n\n<div class=\"content-ad\"></div>\n\n```js\nimport torch\nfrom torch.utils.data import DataLoader\ntraindataloader=DataLoader(small_train_dataset,batch_size=8,shuffle=True)\ntestdataloader=DataLoader(small_eval_dataset,batch_size=8)\n```\n\n3. 모델을 다운로드하고 GPU에 로드해주세요.\n\n```js\nfrom transformers import AutoModelForSequenceClassification\nmodel=AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n```\n\n4. 옵티마이저(optimizer)와 학습률 스케줄러(learning rate scheduler)를 생성하세요.\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom torch.optim import AdamW, SGD\nfrom transformers import get_scheduler\noptimizer = SGD(model.parameters(), lr=5e-5)\nnum_epochs = 3\nnum_training_steps = num_epochs * len(traindataloader)\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n```\n\n원하는 옵티마이저와 학습률 스케줄러를 조정하여 가장 적합한 것을 선택할 수 있어요.\n\n5. 학습 및 평가\n\n모델을 model.train()을 사용하여 학습 모드로 설정해주세요.\n\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in traindataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n```\n\n```js\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\nmodel.eval()\nfor batch in testdataloader:\n    b = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**b)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n\nmetric.compute()\n```\n\n제 Kaggle 노트북에서 스크립트를 확인할 수 있습니다. https://www.kaggle.com/code/exterminator11/finetune-bert. 행운을 빕니다!","ogImage":{"url":"/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png"},"coverImage":"/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png","tag":["Tech"],"readingTime":8},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>섬세 조정은 대형 언어 모델이 사용자 지정 데이터에 적응하고 텍스트 분류와 같은 하향 작업을 잘 수행할 수 있도록 돕는 중요한 기술입니다.</p>\n<p>본 문서는 섬세 조정의 기본에 초점을 맞추고, LORA, QLORA 등 다른 기술에 대해 깊게 다루지는 않습니다. 시작하는 가장 좋은 방법은 BERT로 실험을 해보는 것입니다.</p>\n<p>주로 두 가지 방법으로 이 작업을 수행할 수 있습니다:</p>\n<ul>\n<li>허깅페이스 트레이너 API 사용: 사용하기 쉽지만 매우 사용자 정의가 어려움</li>\n<li>PyTorch 사용: 트레이너보다 조금 어려우나 프로세스에 대한 더 많은 사용자 정의와 제어를 제공합니다</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>데이터셋</p>\n<p>우리는 Hugging Face에서 제공하는 Yelp Reviews 데이터셋을 사용할 예정입니다. 이 데이터셋은 다음 두 열로 구성되어 있습니다:</p>\n<ul>\n<li>레이블: 1부터 5까지의 별표가 부여된 등급입니다.</li>\n<li>텍스트: 리뷰 내용입니다.</li>\n</ul>\n<p>저희의 목표는 리뷰 텍스트로부터 별의 개수를 예측할 수 있는 모델을 훈련하는 것입니다.</p>\n<div class=\"content-ad\"></div>\n<p>휍핑페이스 트레이너 API를 사용하여 파인튜닝하기</p>\n<ul>\n<li>모든 라이브러리를 설치하세요 :</li>\n</ul>\n<pre><code class=\"hljs language-js\">!pip install --upgrade transformers datasets evaluate huggingface_hub torch\n</code></pre>\n<p>참고: 이 라이브러리들의 최신 버전을 항상 사용하도록 하세요.</p>\n<div class=\"content-ad\"></div>\n<ol start=\"2\">\n<li>데이터셋 로드: Hugging Face에서 제공하는 datasets 라이브러리를 사용하여 데이터셋을 로드할 수 있어요.</li>\n</ol>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> datasets <span class=\"hljs-keyword\">import</span> load_dataset\ndataset = load_dataset(<span class=\"hljs-string\">\"yelp_review_full\"</span>)\n</code></pre>\n<p>데이터셋을 확인해봐요. 어떤 데이터를 다루게 될지 알아봅시다.</p>\n<pre><code class=\"hljs language-python\">dataset[<span class=\"hljs-string\">\"train\"</span>][<span class=\"hljs-number\">1</span>]\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>우리 데이터가 어떻게 보이는지 확인해보세요.</p>\n<pre><code class=\"hljs language-js\">{<span class=\"hljs-string\">'label'</span>: <span class=\"hljs-number\">1</span>,\n <span class=\"hljs-string\">'text'</span>: <span class=\"hljs-string\">\"안타깝게도 Dr. 골트버그의 환자로서 느끼는 좌절은 뉴욕의 다른 많은 의사들과 겪어온 경험의 반복입니다 - 좋은 의사, 하지만 최악의 스태프. 그의 스텝은 단순히 전화를 받지 않는 것 같습니다. 답변을 받으려면 보통 반복적인 전화로 2시간이 걸립니다. 누가 그런 시간을 가진 사람이며 누가 그것과 소통하길 원하겠습니까? 다른 많은 의사들과도 이 문제를 겪어왔고, 이해가 안 가네요. 사무원이 있고 의료 필요가 있는 환자가 있는데, 왜 전화를 받는 사람이 없는 건지요? 이해할 수 없고, 신경질만 나게 합니다. Dr. 골트버그에게 2점을 주어야 하는 점이 유감입니다.\"</span>}\n</code></pre>\n<ol start=\"3\">\n<li>토크나이저를 로드하고 텍스트를 토큰화하는 함수를 만들어보세요:</li>\n</ol>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">AutoTokenizer</span>\n\ntokenizer = <span class=\"hljs-title class_\">AutoTokenizer</span>.<span class=\"hljs-title function_\">from_pretrained</span>(<span class=\"hljs-string\">\"google-bert/bert-base-cased\"</span>)\n\n\ndef <span class=\"hljs-title function_\">tokenize_function</span>(examples):\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title function_\">tokenizer</span>(examples[<span class=\"hljs-string\">\"text\"</span>], padding=<span class=\"hljs-string\">\"max_length\"</span>, truncation=<span class=\"hljs-title class_\">True</span>)\n\n\ntokenized_datasets = dataset.<span class=\"hljs-title function_\">map</span>(tokenize_function, batched=<span class=\"hljs-title class_\">True</span>)\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>토크나이저는 텍스트를 입력_ids, 토큰_유형_ids 및 어텐션_마스크로 이해할 수 있는 세 개의 열로 변환합니다.</p>\n<p>데이터셋에서 작은 배치를 만들기(선택 사항)</p>\n<pre><code class=\"hljs language-js\">small_train_dataset = tokenized_datasets[<span class=\"hljs-string\">\"train\"</span>].<span class=\"hljs-title function_\">shuffle</span>(seed=<span class=\"hljs-number\">42</span>).<span class=\"hljs-title function_\">select</span>(<span class=\"hljs-title function_\">range</span>(<span class=\"hljs-number\">1000</span>))\nsmall_eval_dataset = tokenized_datasets[<span class=\"hljs-string\">\"test\"</span>].<span class=\"hljs-title function_\">shuffle</span>(seed=<span class=\"hljs-number\">42</span>).<span class=\"hljs-title function_\">select</span>(<span class=\"hljs-title function_\">range</span>(<span class=\"hljs-number\">1000</span>))\n</code></pre>\n<ol start=\"4\">\n<li>모델 불러오기:</li>\n</ol>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(<span class=\"hljs-string\">\"google-bert/bert-base-cased\"</span>, num_labels=<span class=\"hljs-number\">5</span>)\n</code></pre>\n<p>분류할 레이블 수를 초기화하려면 num_labels 매개변수를 사용하세요.</p>\n<ol start=\"5\">\n<li>훈련 인수 초기화</li>\n</ol>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> TrainingArguments\n\ntraining_args = TrainingArguments(output_dir=<span class=\"hljs-string\">\"test_trainer\"</span>)\n</code></pre>\n<div class=\"content-ad\"></div>\n<p><a href=\"https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments%EC%97%90%EC%84%9C\" rel=\"nofollow\" target=\"_blank\">https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments에서</a> 제공되는 매개변수에 대한 자세한 정보를 확인할 수 있습니다.</p>\n<ol start=\"6\">\n<li>메트릭 계산 함수 설정:</li>\n</ol>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n<span class=\"hljs-keyword\">import</span> evaluate\n\nmetric = evaluate.<span class=\"hljs-title function_\">load</span>(<span class=\"hljs-string\">\"accuracy\"</span>)\ndef <span class=\"hljs-title function_\">compute_metrics</span>(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.<span class=\"hljs-title function_\">argmax</span>(logits, axis=-<span class=\"hljs-number\">1</span>)\n    <span class=\"hljs-keyword\">return</span> metric.<span class=\"hljs-title function_\">compute</span>(predictions=predictions, references=labels)\n</code></pre>\n<ol start=\"7\">\n<li>학습 시작:</li>\n</ol>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Trainer</span>\ntrainer = <span class=\"hljs-title class_\">Trainer</span>(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\ntrainer.<span class=\"hljs-title function_\">train</span>()\n</code></pre>\n<p>트레이닝을 시작하려면 wandb 키를 입력하라는 프롬프트가 나타납니다. 키를 입력하면 트레이닝 프로세스가 시작됩니다. 트레이닝이 완료되면 아래와 같은 결과를 보게 될 것입니다.</p>\n<p><img src=\"/assets/img/2024-06-23-FinetuneBERTfortextclassification_0.png\" alt=\"트레이닝 결과\"></p>\n<p>선택적으로 노트북에서 허깅페이스로 모델을 저장할 수도 있습니다.</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> huggingface_hub <span class=\"hljs-keyword\">import</span> login\n<span class=\"hljs-title function_\">login</span>()\nmodel.<span class=\"hljs-title function_\">push_to_hub</span>(<span class=\"hljs-string\">\"HuggingfaceUsername/yourModelName\"</span>)\n</code></pre>\n<ol start=\"8\">\n<li>추론 실행:</li>\n</ol>\n<p>모델을 테스트하려면 PyTorch를 사용할 수 있습니다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">import</span> torch.<span class=\"hljs-property\">nn</span>.<span class=\"hljs-property\">functional</span> <span class=\"hljs-keyword\">as</span> F\n# <span class=\"hljs-title class_\">Load</span> model directly\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">AutoTokenizer</span>, <span class=\"hljs-title class_\">AutoModelForSequenceClassification</span>\n\ntokenizer = <span class=\"hljs-title class_\">AutoTokenizer</span>.<span class=\"hljs-title function_\">from_pretrained</span>(<span class=\"hljs-string\">\"google-bert/bert-base-cased\"</span>)\nmodel = <span class=\"hljs-title class_\">AutoModelForSequenceClassification</span>.<span class=\"hljs-title function_\">from_pretrained</span>(<span class=\"hljs-string\">\"HuggingfaceUsername/yourModelName\"</span>)\ns=<span class=\"hljs-string\">\"The was awesome and I loved it\"</span>\ntt=<span class=\"hljs-title function_\">tokenizer</span>(s,return_tensors=<span class=\"hljs-string\">\"pt\"</span>, padding=<span class=\"hljs-title class_\">True</span>, truncation=<span class=\"hljs-title class_\">True</span>)\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>모델을 평가 모드로 설정하면 더 이상 가중치를 업데이트할 필요가 없어지고, 이제 분류 작업에 사용할 수 있습니다.</p>\n<pre><code class=\"hljs language-python\">model.<span class=\"hljs-built_in\">eval</span>()\n<span class=\"hljs-keyword\">with</span> torch.no_grad():\n    outputs = model(**tt)\n</code></pre>\n<p>결과를 확인해보겠습니다.</p>\n<pre><code class=\"hljs language-python\">SequenceClassifierOutput(loss=<span class=\"hljs-literal\">None</span>, logits=tensor([[-<span class=\"hljs-number\">2.3995</span>, -<span class=\"hljs-number\">2.0111</span>, -<span class=\"hljs-number\">0.8381</span>,  <span class=\"hljs-number\">2.4683</span>,  <span class=\"hljs-number\">2.8968</span>]]), hidden_states=<span class=\"hljs-literal\">None</span>, attentions=<span class=\"hljs-literal\">None</span>)\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>여기서 중요한 변수는 로짓 변수입니다. 이 경우 로짓은 텍스트가 특정 클래스에 속할 확률을 나타냅니다. 현재 로짓은 이해하기 어려운 형식으로 표시됩니다. 이를 이해할 수 있는 형식으로 변환하려면 이해할 수 있는 숫자로 변환해야 합니다.</p>\n<pre><code class=\"hljs language-js\">logits = outputs.<span class=\"hljs-property\">logits</span>\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"로짓:\"</span>, logits)\n\n# 소프트맥스를 사용하여 로짓을 확률로 변환합니다\nprobabilities = F.<span class=\"hljs-title function_\">softmax</span>(logits, dim=-<span class=\"hljs-number\">1</span>)\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"확률:\"</span>, probabilities)\n\n# 예측된 클래스를 결정합니다\npredicted_class = torch.<span class=\"hljs-title function_\">argmax</span>(probabilities, dim=-<span class=\"hljs-number\">1</span>)\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"예측된 클래스:\"</span>, predicted_class.<span class=\"hljs-title function_\">item</span>())\n</code></pre>\n<p>여기서 출력은 4입니다.</p>\n<p>PyTorch를 사용한 파인 튜닝</p>\n<div class=\"content-ad\"></div>\n<p>모델이 이해할 수 있도록 몇 가지 전처리 단계가 필요합니다.</p>\n<ul>\n<li>열 삭제</li>\n</ul>\n<pre><code class=\"hljs language-js\">tokenized_datasets = tokenized_datasets.<span class=\"hljs-title function_\">remove_columns</span>([<span class=\"hljs-string\">\"text\"</span>])\ntokenized_datasets = tokenized_datasets.<span class=\"hljs-title function_\">rename_column</span>(<span class=\"hljs-string\">\"label\"</span>, <span class=\"hljs-string\">\"labels\"</span>)\ntokenized_datasets.<span class=\"hljs-title function_\">set_format</span>(<span class=\"hljs-string\">\"torch\"</span>)\nsmall_train_dataset = tokenized_datasets[<span class=\"hljs-string\">\"train\"</span>].<span class=\"hljs-title function_\">shuffle</span>(seed=<span class=\"hljs-number\">42</span>).<span class=\"hljs-title function_\">select</span>(<span class=\"hljs-title function_\">range</span>(<span class=\"hljs-number\">1000</span>))\nsmall_eval_dataset = tokenized_datasets[<span class=\"hljs-string\">\"test\"</span>].<span class=\"hljs-title function_\">shuffle</span>(seed=<span class=\"hljs-number\">42</span>).<span class=\"hljs-title function_\">select</span>(<span class=\"hljs-title function_\">range</span>(<span class=\"hljs-number\">1000</span>))\n</code></pre>\n<ol start=\"2\">\n<li>데이터로더(Dataloader) 생성</li>\n</ol>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">from</span> torch.<span class=\"hljs-property\">utils</span>.<span class=\"hljs-property\">data</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">DataLoader</span>\ntraindataloader=<span class=\"hljs-title class_\">DataLoader</span>(small_train_dataset,batch_size=<span class=\"hljs-number\">8</span>,shuffle=<span class=\"hljs-title class_\">True</span>)\ntestdataloader=<span class=\"hljs-title class_\">DataLoader</span>(small_eval_dataset,batch_size=<span class=\"hljs-number\">8</span>)\n</code></pre>\n<ol start=\"3\">\n<li>모델을 다운로드하고 GPU에 로드해주세요.</li>\n</ol>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">AutoModelForSequenceClassification</span>\nmodel=<span class=\"hljs-title class_\">AutoModelForSequenceClassification</span>.<span class=\"hljs-title function_\">from_pretrained</span>(<span class=\"hljs-string\">\"google-bert/bert-base-cased\"</span>, num_labels=<span class=\"hljs-number\">5</span>)\ndevice = torch.<span class=\"hljs-title function_\">device</span>(<span class=\"hljs-string\">\"cuda\"</span>) <span class=\"hljs-keyword\">if</span> torch.<span class=\"hljs-property\">cuda</span>.<span class=\"hljs-title function_\">is_available</span>() <span class=\"hljs-keyword\">else</span> torch.<span class=\"hljs-title function_\">device</span>(<span class=\"hljs-string\">\"cpu\"</span>)\nmodel.<span class=\"hljs-title function_\">to</span>(device)\n</code></pre>\n<ol start=\"4\">\n<li>옵티마이저(optimizer)와 학습률 스케줄러(learning rate scheduler)를 생성하세요.</li>\n</ol>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> torch.optim <span class=\"hljs-keyword\">import</span> AdamW, SGD\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> get_scheduler\noptimizer = SGD(model.parameters(), lr=<span class=\"hljs-number\">5e-5</span>)\nnum_epochs = <span class=\"hljs-number\">3</span>\nnum_training_steps = num_epochs * <span class=\"hljs-built_in\">len</span>(traindataloader)\nlr_scheduler = get_scheduler(\n    name=<span class=\"hljs-string\">\"linear\"</span>, optimizer=optimizer, num_warmup_steps=<span class=\"hljs-number\">0</span>, num_training_steps=num_training_steps\n)\n</code></pre>\n<p>원하는 옵티마이저와 학습률 스케줄러를 조정하여 가장 적합한 것을 선택할 수 있어요.</p>\n<ol start=\"5\">\n<li>학습 및 평가</li>\n</ol>\n<p>모델을 model.train()을 사용하여 학습 모드로 설정해주세요.</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> tqdm.<span class=\"hljs-property\">auto</span> <span class=\"hljs-keyword\">import</span> tqdm\n\nprogress_bar = <span class=\"hljs-title function_\">tqdm</span>(<span class=\"hljs-title function_\">range</span>(num_training_steps))\n\nmodel.<span class=\"hljs-title function_\">train</span>()\n<span class=\"hljs-keyword\">for</span> epoch <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">range</span>(num_epochs):\n    <span class=\"hljs-keyword\">for</span> batch <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">traindataloader</span>:\n        batch = {<span class=\"hljs-attr\">k</span>: v.<span class=\"hljs-title function_\">to</span>(device) <span class=\"hljs-keyword\">for</span> k, v <span class=\"hljs-keyword\">in</span> batch.<span class=\"hljs-title function_\">items</span>()}\n        outputs = <span class=\"hljs-title function_\">model</span>(**batch)\n        loss = outputs.<span class=\"hljs-property\">loss</span>\n        loss.<span class=\"hljs-title function_\">backward</span>()\n\n        optimizer.<span class=\"hljs-title function_\">step</span>()\n        lr_scheduler.<span class=\"hljs-title function_\">step</span>()\n        optimizer.<span class=\"hljs-title function_\">zero_grad</span>()\n        progress_bar.<span class=\"hljs-title function_\">update</span>(<span class=\"hljs-number\">1</span>)\n</code></pre>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> evaluate\n\nmetric = evaluate.<span class=\"hljs-title function_\">load</span>(<span class=\"hljs-string\">\"accuracy\"</span>)\nmodel.<span class=\"hljs-built_in\">eval</span>()\n<span class=\"hljs-keyword\">for</span> batch <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">testdataloader</span>:\n    b = {<span class=\"hljs-attr\">k</span>: v.<span class=\"hljs-title function_\">to</span>(device) <span class=\"hljs-keyword\">for</span> k, v <span class=\"hljs-keyword\">in</span> batch.<span class=\"hljs-title function_\">items</span>()}\n    <span class=\"hljs-keyword\">with</span> torch.<span class=\"hljs-title function_\">no_grad</span>():\n        outputs = <span class=\"hljs-title function_\">model</span>(**b)\n\n    logits = outputs.<span class=\"hljs-property\">logits</span>\n    predictions = torch.<span class=\"hljs-title function_\">argmax</span>(logits, dim=-<span class=\"hljs-number\">1</span>)\n    metric.<span class=\"hljs-title function_\">add_batch</span>(predictions=predictions, references=batch[<span class=\"hljs-string\">\"labels\"</span>])\n\nmetric.<span class=\"hljs-title function_\">compute</span>()\n</code></pre>\n<p>제 Kaggle 노트북에서 스크립트를 확인할 수 있습니다. <a href=\"https://www.kaggle.com/code/exterminator11/finetune-bert\" rel=\"nofollow\" target=\"_blank\">https://www.kaggle.com/code/exterminator11/finetune-bert</a>. 행운을 빕니다!</p>\n</body>\n</html>\n"},"__N_SSG":true}