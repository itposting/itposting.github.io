{"pageProps":{"post":{"title":"품질 엔지니어를 위한 RAG 사용 가이드","description":"","date":"2024-06-22 17:14","slug":"2024-06-22-RAGforQualityEngineers","content":"\n\n## RAG 만들기는 쉽지만, 품질 있는 RAG 만들기는 어려워요\n\n![image](/assets/img/2024-06-22-RAGforQualityEngineers_0.png)\n\n검색 확장 생성(RAG)은 대규모 언어 모델(LLMs)의 기능을 확장하는 일반적인 패턴이 되었습니다.\n\n이론적으로 RAG는 간단합니다(컨텍스트 창에 데이터를 추가하기만 하면 됩니다!) 하지만 실제로는 복잡합니다. 상자 다이어그램 너머에는 고급 청킹 전략, 재랭킹, 다중 쿼리 리트리버, 작은 데이터부터 큰 데이터 검색, 가상 문서 임베딩, 사전 임베딩 데이터 보강, 동적 라우팅, 맞춤 임베딩 모델... 등이 숨어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n초기 파이프라인을 설정하는 것은 빠르고 쉽지만, 프로덕션 수준의 품질에 도달하는 것은 상당히 복잡합니다. 신중한 고려 없이 RAG 시스템은 부정확하거나 관련성이 없는 정보를 반환할 수 있습니다. 비효율적으로 비싼 리소스를 소비하거나 프로덕션 규모의 소스 데이터로 확장할 때 병목 현상이 발생할 수도 있습니다.\n\nRAG 시스템의 품질을 효과적으로 평가하고 효율적으로 이해하는 것은 각 개별 구성 요소가 전체 RAG 파이프라인을 만드는 과정을 이해하는 데 필요합니다. 이러한 각 부분에 대한 설계 결정은 품질에 영향을 미치며, RAG 애플리케이션을 배포하려는 모든 사람이 알아야 합니다.\n\n이 글에서는 테스트와 품질 관점에서 RAG 개념과 패턴에 대한 소개를 제공합니다. RAG가 왜 가치 있는지에 대한 소개로 시작하여, 프로덕션 품질의 RAG를 구축하는 데 내재된 많은 설계 결정이 어떻게 향상되는지에 대해 설명합니다. 이 소개는 특정 평가 방법과 기법에 대해 논의하기 전에 필요한 기초를 제공할 것입니다.\n\n# LLM의 한계\n\n<div class=\"content-ad\"></div>\n\nRAG 파이프라인을 이해하려면, RAG가 대응하려는 LLM의 한계를 먼저 이해해야 합니다.\n\nLLM의 핵심은 간단합니다: 프롬프트를 보내면 응답을 받습니다.\n\nLLM이 응답을 반환하려면 모델과 추론 계산을 실행해야 합니다. 이 계산은 입력을 수백억 개 또는 수조의 매개변수와 결합하는 것을 포함합니다. 이는 비용이 많이 드는 계산입니다.\n\nLLM을 호출하는 것만큼이나 LLM을 훈련시키는 것은 훨씬 어렵습니다. 훈련은 모델 내 매개변수에 대한 최적값을 결정하는 과정입니다. 최상의 가중치를 계산하는 데 사용되는 다양한 알고리즘이 있지만, 모두 특정 입력에서 모델을 실행하고 오차를 계산한 후에 조정을 하는 반복적 과정을 포함합니다. 이 과정은 많은 횟수로 많은 입력에서 계속되며, 결국 훈련된 모델을 얻게 됩니다. 모델 추론은 몇 초만에 완료될 수 있지만, 모델 훈련은 광범위한 GPU 클러스터에서도 몇 주가 걸릴 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-RAGforQualityEngineers_1.png\" />\n\n엄청난 교육 비용은 새로운 정보를 LLM에 통합하는데 병목 현상을 야기합니다. 대부분의 기업은 모델을 교육할 자원이 없으며, 단순히 사적 데이터로 교육하여 LLM에 \"새로운 정보를 추가\"할 수 없습니다. 대신, 잘 자금을 지원받는 대규모 기술 기업은 대형 공개 데이터 세트에서 일반 목적의 기반이 되는 모델을 교육하고, 이러한 모델은 RAG와 같은 보조 프로세스로 새로운 능력 및 정보가 부가됩니다.\n\n구체적으로 RAG는 새로운 모델을 교육하는 높은 비용을 우회하는 방식으로 LLM이 추가적인 지식에 접근하도록 하는 것을 목표로 합니다.\n\n# RAG 기본 원리\n\n<div class=\"content-ad\"></div>\n\nRAG가 실제로 작동하는 방식에 대해 알아봅시다.\n\nLLM에 전송된 프롬프트는 제한된 길이인 context window을 가지고 있습니다. Context window은 토큰(단어에 대한 대략적인 동등물) 단위로 측정됩니다. Context window의 크기는 보통 1K, 4K 또는 그 이상의 토큰으로 표시되지만 더 큰 context window이 사용 가능해지고 있습니다(예: Gemini 1.5 Pro의 128K).\n\n많은 사람들이 직관적으로 context window은 할 수 있는 가장 긴 질문이라고 생각하지만, 이는 context window에 대한 한정적인 사고 방식입니다. LLM의 작동 방식으로 인해, context window에 제공된 정보는 LLM이 응답을 생성하는 동안 LLM에게 사용 가능합니다. 따라서 추가 정보를 제공하는 데 사용될 수 있습니다. 일반적으로 이를 in-context learning 이라고 합니다.\n\n따라서, 우리는 context window을 사용하여 LLM이 질문에 답변하기 위해 필요한 새로운 지식을 제공할 수 있습니다. 예를 들어, 우리가 상조에 관한 회사 정책에 대해 물어보는 프롬프트를 만들고, context window 내에서 이에 관한 전체 회사 안내서(상조에 관한 섹션 포함)를 넣는다면 새로운 정보를 제공하여 LLM이 응답할 수 있게 할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n해당 솔루션은 이미 관련 정보가 있고 그 정보가 문맥 창 안에 쏙 들어갈 수 있는 경우 간단합니다. 불행히도 항상 그런 것은 아닙니다. 따라서 우리는 우리의 프롬프트에 관련 정보만 검색 및 다운 선택할 수 있는 메커니즘이 필요합니다.\n\n무식한 접근법은 프롬프트에서 용어를 검색하여 관련할 수 있는 데이터 전체에서 주변 텍스트를 복사한 후 이를 프롬프트에 추가하는 것입니다.\n\n이 간단한 키워드 검색 형태의 RAG는 LLM 응답을 향상시킬 수 있으며 어떤 맥락에서는 유용할 수 있지만 가끔식 거짓 양성 (다른 맥락에서 사용된 키워드) 때문에 문제가 될 수도 있습니다. 다행히도 우리는 텍스트의 의미에 맞추어 일치시키는 의미 검색을 활용하여 더 나은 결과를 얻을 수 있습니다.\n\n구체적으로, 우리는 포지블리 관련 데이터 청크에서 임베딩 모델을 활용하여 임베딩을 생성한 다음 이 임베딩을 통해 데이터를 검색하여 우리의 프롬프트에 관련된 데이터를 찾을 수 있습니다. 이 방법은 매우 단순화된 접근법이지만, 진정한 RAG와 같은 결과를 얻기 시작하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# RAG 및 임베딩\n\n간단한 키워드 검색보다 RAG가 제공하는 이점을 이해하려면 임베딩 모델의 목적과 성격을 이해해야 합니다. 이는 그 자체로 심도 있는 주제이지만, RAG를 이해하는 데 중요합니다.\n\n임베딩 모델은 우리의 원래 LLM과 유사하지만, 새로운 콘텐츠를 생성하는 대신 입력을 벡터(숫자 목록)로 축소합니다. 임베딩 모델은 매우 큰 숫자 목록입니다. 임베딩 모델이 생성하는 벡터는 일반적으로 768 또는 1536 숫자(차원)지만, 다른 크기의 벡터도 존재합니다.\n\n임베딩 모델에 의해 생성된 벡터는 단순히 임의의 숫자 세트가 아니라 모델에 따라 입력 데이터의 의미를 요약한 것입니다. 이 벡터는 다른 모델에게는 의미가 없지만 \"유사한\" 텍스트는 같은 모델에서 유사한 벡터를 생성할 것입니다. \"유사하다\"는 단순히 \"동일한 키워드를 가지고 있다\" 이상을 의미합니다. 임베딩 모델은 구조화되지 않은 데이터로부터 보다 심층적인 의미를 추출하기 위해 특별히 훈련되었습니다. 예를 들어 \"남자 말이 날지 않는다\"와 \"날개가 있는 사나이가 말 타고 있다\"는 비슷한 단어를 가지고 있지만 같은 모델에서는 서로 멀리 떨어진 벡터를 생성할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n벡터의 멋진 점은 그들에게 수학 연산을 수행할 수 있다는 것입니다. 빠른 수학이죠. 수백만 개의 벡터를 검색하여 비교적 짧은 시간 안에 유사한 벡터를 찾을 수 있습니다. (여기에 사용된 일부 알고리즘이 있습니다.)\n\n이제 우리의 RAG 파이프라인 조각들이 모두 마련되었으니, 단계별로 진행해봅시다.\n\n첫 네 단계는 한 번에 수행되거나 소스 데이터가 변경될 때 업데이트됩니다. 다섯 번부터 여덟 번까지의 단계는 각 추론 요청마다 수행됩니다:\n\n- 모든 가능성 있는 데이터를 수집합니다. - 이토록 많은 데이터가 있어서 우리의 프롬프트의 컨텍스트 창에 쏙 들어가기 불가능합니다.\n- 이 데이터를 더 작은 조각으로 나눕니다 (나중에 자세히 설명하겠습니다).\n- 그런 다음 각 조각을 임베딩 모델을 통해 실행하여 조각의 의미를 포함하는 벡터를 생성합니다.\n- 해당 벡터를 벡터 데이터베이스에 저장합니다.\n- 각 추론 요청으로: 프롬프트를 받으면, 그 프롬프트를 조각낸 소스 데이터와 동일한 임베딩 모델을 통해 실행하여 다른 벡터 (프롬프트 벡터 또는 쿼리 벡터라고 함)를 생성합니다.\n- 우리의 벡터 데이터베이스에서 우리의 프롬프트 벡터와 유사한 벡터를 검색합니다. 반환된 벡터들은 생 데이터를 키워드로 검색했다면 얻었을 것보다 더 나은 매치일 것입니다.\n- 식별된 관련 벡터를 (선택적으로) 재정렬하고, 그런 다음 상위 벡터 각각의 생 데이터를 반환합니다.\n- 원시 데이터는 초기 프롬프트와 결합되어 LLM으로 보내집니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_2.png)\n\n여기, 우리 LLM은 이제 새로운 독점 데이터를 모두 훈련한 것처럼 작동하며, 기본 모델 훈련을 비용 문제로 수행할 필요가 없습니다.\n\n이론상으로는 이렇게 작동해야 합니다. 그러나 실제로는 이 너무 단순한 파이프라인은 여러분의 제품 요구를 만족시키지 못할 가능성이 높으며, 우수한 품질의 제품에 도달하려면 RAG 파이프라인을 준비하려면 특정 애플리케이션의 요구를 충족시키기 위해 다양한 부분을 적응, 개선, 교체 또는 확장해야 할 것입니다.\n\n# RAG 디자인과 품질\n\n\n<div class=\"content-ad\"></div>\n\n위에서는 RAG를 소개하지만, RAG는 실제로 매우 복잡할 수 있으며, 이러한 실제 세계의 복잡성은 응용 프로그램의 품질에 영향을 미칠 수 있습니다. RAG 파이프라인 내에서 사용 가능한 일부 구현 도전, 품질 위험 및 대안을 이해하기 위해 각 단계를 살펴보겠습니다.\n\n## #1—관련 데이터 수집, 적재 및 풍부화\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_3.png)\n\n시작부터 모든 \"가능성 있는 데이터\"를 찾아야 합니다.\n\n<div class=\"content-ad\"></div>\n\n위의 RAG 다이어그램에서 1번 아이콘은 여러 소스에서 데이터를 수집하고, 정리하고, 변환하며, 익명화하고, 토큰화하는 데이터 파이프라인(또는 파이프라인 세트!)일 가능성이 높습니다.\n\n일부 파이프라인은 특히 텍스트 이외의 형식을 가진 원시 데이터의 경우 매우 복잡해질 수 있습니다. 예를 들어, 일부 파이프라인은 대량의 스캔된 물리 문서를 처리하기 위해 OCR 기술을 널리 활용합니다.\n\n데이터 파이프라인의 복잡성은 데이터 파이프라인 테스트의 모든 과제가 따라옵니다.\n\n가장 잘 구현된 RAG 파이프라인도 소스 데이터가 심지어 벡터 DB로 전달되지 않는다면 완전히 실패할 수 있으며, 이 데이터의 다양성, 속도 및 양에 따라 RAG의 이 단계는 복합적이며 많은 응용프로그램 품질 문제의 원인이 될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n일반적인 데이터 파이프라인 활동에 추가로, RAG는 데이터 풍부화를 통해 혜택을 얻을 수 있습니다. 종종, 다른 시스템(또는 사람들)은 소스 데이터에 대한 맥락을 알고 있어서 그 의미를 평가하는 데 엄청난 도움이 될 수 있습니다. 예를 들어, 고객 데이터베이스에는 다른 시스템에서 제공하는 태그나 주석과 같은 관련 정보를 추가하여 데이터를 풍부화할 수 있습니다. 종종, 다른 생성 모델이나 자연어 처리(NLP)가 더 깨끗하거나 요약된 메타데이터를 생성하는 데 사용됩니다. 모두 \"임베딩 생성\" 전의 \"전처리\"로 생각해보세요. 그리고 제대로 수행된다면, 검색 품질을 크게 향상시킬 수 있습니다.\n\n당신이 RAG 검색 시스템의 품질을 평가하고 있다면, 데이터가 실제로 어떻게 소스되고 흡수되는지 이해하는 데 시간을 투자하는 것이 가치가 있습니다. 훌륭한 AI 부분에 도착하기 전에 RAG 파이프라인에 도달하기 전에 데이터가 어떻게 가져오고 처리되는지 알아두세요.\n\n## #2—Chunking\n\n![2024-06-22-RAGforQualityEngineers_4](/assets/img/2024-06-22-RAGforQualityEngineers_4.png)\n\n<div class=\"content-ad\"></div>\n\n데이터가 수신되고 임베딩 모델을 실행하기 전에는 데이터를 diskrete pieces로 나눠야 합니다. 그렇다면 데이터를 어떻게 분할할지 어떻게 결정하나요? 이를 chunking strategy라고 합니다.\n\n최적의 크기는 얼마나 크거나 작아야 할까요? 청크들은 서로 중첩되어야 할까요? 페이지, 단락 또는 일정한 길이로만 나누는 것 이외에 더 스마트한 분할 방법이 있을까요? 비표준 형식의 데이터(code, JSON 등)는 어떻게 chunk해야 할까요?\n\n이러한 질문들은 chunking strategy가 답하려고 노력하는 것이며 완벽한 해결책은 없습니다. 서로 다른 전략은 서로 다른 타협점을 갖습니다. 일부는 간단하고 빠르게 구현할 수 있으며, 보통 결과를 제공합니다. 다른 전략은 더 복잡하고 관련이 깊습니다. 더 나은 히트율과 LLM 응답 품질을 제공할 수 있습니다. 데이터를 너무 거칠게 나누면 의미 없는 데이터로 context window를 채우거나 다른 관련 청크를 밀어내거나 의미 있는 일치를 얻을 수 없을 정도로 일반적인 임베딩을 생성할 수 있습니다. 너무 세밀하게 나누면 관련 데이터를 잘라낼 수 있습니다.\n\n이 문서에서는 다섯 가지 chunking 범주를 탐구합니다: 고정 크기, 재귀, 문서 기반, 의미 기반, 그리고 AI를 사용한 Agentic(체킹에 인공 지능 사용, 멋지죠!)입니다.\n\n<div class=\"content-ad\"></div>\n\n많은 기타 접근 방식이 있습니다. 예를 들어, 작은 것에서 큰 것으로 검색을 최적화하려면 작은 청크를 사용하지만 각 청크는 큰 부모 청크에 연결되어 있어서 삽입될 컨텍스트 모델에 검색됩니다. 콘텍스트 인식 청킹은 문서의 성격에 대해 기존 지식을 활용하여 문서를 논리적 청크로 적절하게 분할합니다.\n\n위 목록은 아마 완전하지 않을 수 있지만, RAG 구현자가 사용할 수 있는 다양한 옵션과 애플리케이션 전체의 품질에 적합하고 조정된 청킹 전략의 중요성을 보여줍니다. Pinecone 블로그에서 이러한 전략 중 많은 내용을 자세히 다루고 있습니다.\n\n## #3—임베딩 모델 선택과 구성\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_5.png)\n\n<div class=\"content-ad\"></div>\n\n임베딩을 생성하는 데 사용할 수 있는 여러 모델이 있습니다. 서로 다른 모델은 다양한 상황에서 더 나은 또는 나쁘게 수행할 수 있습니다. 일부 모델은 일반 사용을 위해 사전 훈련되어 있고 일부는 특정 도메인(예: 의학 기록)에 대해 세밀하게 조정되어 있습니다. 또한 응용 프로그램에서 처리하는 특정 데이터에 대해 자체 임베딩 모델을 세밀하게 조정할 수도 있습니다.\n\n또한 많은 모델이 다른 크기로 제공되며(임베딩 생성 비용 및 시간에 영향을 미침), 다른 입력 길이(처리 가능한 최대 청크 크기) 및 다른 출력 벡터 차원(높은 차원 = 더 정확하지만 더 많은 공간 요구와 느린 속도)으로 제공됩니다.\n\n일부 임베딩 모델은 API를 통해만 액세스할 수 있습니다(예: OpenAI 임베딩 엔드포인트), 다른 모델은 완전한 오픈 소스로 제공되어 로컬에서 다운로드하고 실행하거나 클라우드 공급업체에 호스팅할 수 있습니다.\n\n응용 프로그램 내에서 다른 데이터 경로에 대해 다른 임베딩 모델을 사용하는 것도 가능합니다.\n\n<div class=\"content-ad\"></div>\n\n일반적으로 좋은 임베딩 모델은 RAG 응용 프로그램에 충분할 수 있지만, 일부는 특정 임베딩 모델이나 사용자 지정된 모델을 사용하여 혜택을 얻을 수 있습니다.\n\n임베딩 전략에 대한 설계 고려 사항과 해당 선택의 품질 특성을 알면 응용 프로그램의 평가 요구 사항과 접근 방식에 대한 통찰력을 제공할 것입니다. 임베딩 모델 선택을 평가하는 더 깊은 논의는 여기에서 확인할 수 있습니다.\n\n## #5—쿼리 처리 및 임베딩\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_6.png)\n\n<div class=\"content-ad\"></div>\n\n수신한 쿼리에 임베딩 모델을 정확히 실행해야 한다는 규칙은 없습니다. 실제로 이 쿼리를 최적화하여 애플리케이션의 전반적인 품질을 향상시킬 수 있는 다양한 방법이 있습니다. 이는 특히 쿼리가 사람 사용자로부터 직접 제출되었으며 모호하고 애매한 쿼리일 경우에 더욱 참된 것입니다.\n\n애플리케이션의 특성 또는 의도에 대한 추가 지식을 통해 LLM 또는 전통적인 논리를 사용하여 쿼리를 축소하거나 다시 작성하는 것이 가능할 수 있습니다. 다시 말해, 의도된 것이 아닌 실제로 묻는 것이었던 쿼리를 재작성하는 방식으로 쿼리를 재구성할 수 있습니다.\n\n쿼리 처리의 고급 형태인 HyDE도 있습니다. 여기서는 가상 문서를 작성하여 유사한 문서(답변에서 답변)를 벡터 검색하고 임베딩 및 쿼리 검색(질문에서 답변)을 하는 것대신 사용할 수 있습니다.\n\n또 다른 옵션은 쿼리를 여러 관련된 쿼리로 분할하고 각각을 병렬로 실행한 다음 결과를 결합하는 것입니다. 이는 처리 비용이 듬성들지만 검색 품질을 향상시킬 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n특정 사용 사례에 따라 사용자 지정 쿼리 처리가 필요할 수 있으며 응용 프로그램의 품질과 동작에 큰 영향을 미칠 수 있습니다.\n\n## #4, #6—Vector DB 및 Vector Search\n\n![image](/assets/img/2024-06-22-RAGforQualityEngineers_7.png)\n\n벡터 검색은 빠르지만, 쿼리와 유사한 임베딩을 찾기 위해 벡터 DB를 검색하는 데에는 시간 (그리고 돈) 비용이 소요될 수 있습니다. 이 비용을 최소화하는 한 가지 방법은 의미 캐싱입니다. 의미 캐싱에서는 임베딩이 처음 검색된 후 응답이 캐시되어, 향후 유사한 검색이 캐시로부터 직접 데이터를 반환하게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n물론, 캐싱은 복잡성을 증가시킵니다 (그리고 컴퓨터 과학에서의 두 번째 어려운 문제 중 하나입니다—다른 하나의 이름을 기억하지 못하겠군요). 캐싱은 성능을 향상시킬 수 있지만, 오래된 캐시는 변동성 있는 소스 데이터 환경에서 특히 응답 품질을 해치는 요인이 될 수 있습니다.\n\n## #7—재랭킹\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_8.png)\n\n위에서 설명한 내용에서 우리는 단순히 우리의 벡터 검색으로 반환된 모든 관련 데이터를 컨텍스트 창에 채울 수 있다고 가정했습니다. 이것은 명백히 간소화된 내용이며, 반환된 모든 벡터 중 어느 것이 컨텍스트 창에 포함될 것인지를 결정하기 위한 과정이 있어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n검색 결과를 콘텍스트 창 안에 맞출 수 있을 때에도, 많은 연구에서 콘텍스트 Stuffing(콘텍스트 창 채우기)가 LLM 회상을 부정적으로 영향을 줄 수 있다는 것을 지적합니다. 이는 중간에서 사라지는 문제를 도입하여 응답 품질(회상은 LLM이 콘텍스트 창에 있는 정보를 사용하는 능력입니다)에 영향을 줄 수 있습니다.\n\n해결책은 초기 벡터 검색 후에 추가 단계로 재랭킹을 추가하는 것입니다.\n\n재랭킹의 TLDR(요약): 임베딩 모델은 속도에 최적화되어 있으며 많은 문서에 대해 실행되어야 하므로 빠릅니다. 재랭킹 모델(또는 교차 인코더)이라 불리는 다른 모델은 느리지만 정확도에 최적화되어 있습니다. 그래서 빠르고 부정확한 임베딩 모델을 사용하여 임베딩을 생성한 다음, 작은 집합에서 최고 품질의 문서를 찾기 위해 느리고 정확한 모델을 사용합니다. 느린 정확한 검색에서 가장 일치하는 결과는 콘텍스트 창에서 우선순위를 갖습니다.\n\n다시 말하지만, 이보다 더 많은 내용이 있지만, 재랭킹의 본질은 바로 이것입니다. Pinecone 블로그에서 더 자세한 설명을 찾을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다시 순위를 매기면 RAG에서 반환된 데이터의 관련성을 크게 향상시킬 수 있습니다. 컨텍스트 창에서 더 관련성이 높은(또는 무관련성이 적은) 데이터는 응답 품질을 향상시킬 것입니다. 그러나 복잡성과 지연이 증가하지만, 품질의 트레이드오프는 많은 RAG 응용 프로그램에서 가치 있는 요소일 수 있습니다.\n\n## 큰 컨텍스트 창 vs. RAG\n\n우리는 마침내 LLM을 호출하는 지점에 도달했지만, 프롬프트 엔지니어링에 대해 이야기하기 전에 RAG와 큰 컨텍스트 창 간의 관계에 대해 언급할 시간을 가져야 합니다.\n\nLLM 기술은 빠르게 발전하고 있으며 개선의 한 가지 측면은 컨텍스트 창의 크기입니다. 한 가지 대표적인 예는 2024년 2월에 출시된 Gemini 1.5 Pro이며, 128K 컨텍스트 창을 제공하며(공개적으로 출시되지 않음) 최대 백만(!!!) 토큰까지 확장할 수 있는 옵션이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n일부 사람들은 100만 토큰 컨텍스트 창이 RAG 파이프라인을 사용할 때 사용되지 않을 것이라고 예상했습니다. 그러나 실제로는 그렇지 않습니다. 이 블로그에서는 RAG가 왜 유용하며 (심지어 필수적이기도 한) 거대한 컨텍스트 창을 사용할 때도 필요한 이유에 대해 설명합니다. (스포일러: 비용, 지연 시간 및 회수 품질)\n\n대규모 컨텍스트 모델은 유용하며, LLMs가 많은 사실들 간에 종합적인 결론을 요구하는 쿼리에 응답하는 데 도움이 될 수 있습니다 (이러한 사실들이 RAG를 통해 선별되어 있는지 여부는 상관 없음).\n\n큰 컨텍스트 창과 RAG 간의 관계는 계속 발전할 것이며, RAG를 구현하고 테스트하는 사람들은 응용 프로그램 품질에 미치는 이러한 트레이드오프와 그들의 영향을 이해해야 합니다.\n\n## #8—프롬프트 생성\n\n<div class=\"content-ad\"></div>\n\n![2024-06-22-RAGforQualityEngineers_9](/assets/img/2024-06-22-RAGforQualityEngineers_9.png)\n\n벡터DB에서 관련 데이터를 많이 받아 재설정하고, LLM(Large Language Model)의 문맥 창 안에 맞는 적절한 데이터 세트로 마무리했습니다. 그럼 이제 어떻게 해야 할까요? 받은 데이터를 초기 질문 뒤에 밀어 넣고 끝내면 될까요?\n\nLLM을 다뤄본 사람이라면 알 수 있듯이, 그것만큼 간단한 일이 아닙니다. LLM은 강력할 수 있지만, 변덕스럽고 짜증을 유발할 수도 있습니다. 당신의 프롬프트에 작은 세부 사항이 응담 품질에 상당한 영향을 미칠 수 있다는 것이 밝혀졌습니다. 프롬프트의 단어 선택, 데이터 순서, 사용하는 어조, \"시간을 들이다\"와 같은 제안, 심리적 언어 사용까지 모두 LLM 응답 품질에 영향을 미칠 수 있습니다. 프롬프트를 자동으로 생성하는 최적 프롬프트 생성 전략이 있습니다. ...맞아, 프롬프트 생성에 특별히 훈련된 다른 모델을 사용하는 것입니다. 이것은 신속히 진화하는 프롬프트 엔지니어링 분야의 일부입니다.\n\n최상의 품질의 응답을 생성할 정확한 프롬프트 템플릿은 보통 모델 및 응용 프로그램에 따라 다르며 종종 실험과 시행착오가 필요할 수 있습니다. RAG의 이 보이지 않는 작은 세부 사항이 가지는 품질 영향을 감안할 때, 적용된 특정 프롬프트 엔지니어링은 시스템의 다른 부분과 마찬가지로 면밀히 평가되고 심사되어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n# RAG 시스템의 측정 및 평가\n\n우리는 RAG 파이프라인의 주요 구성 요소들을 살펴보고 (간략하게) 이들이 애플리케이션 품질에 미치는 영향에 대해 이야기해 보았습니다. 이것은 소개였지만, 이러한 유형의 애플리케이션의 내부 작업 및 품질 도전에 대한 통찰력을 제공해야 합니다. RAG에 대해 더 깊이 파고드는 많은 훌륭한 기사, 블로그, 논문이 있습니다. 시작할 때 하나만 선택한다면, \"Retrieval-Augmented Generation for Large Language Models—A Survey\"를 읽어보세요.\n\n주요 교훈: RAG를 구현할 때 선택할 수 있는 옵션과 선택지가 많으며, 각각에는 품질에 대한 대가와 영향이 있습니다. 이러한 선택들 중 일부는 직접적으로 평가될 수 있고, 일부는 전반적인 검색이나 응답 품질에 영향을 미칩니다. 이러한 선택 각각과 이들이 당신의 RAG 시스템에 어떤 영향을 미칠 수 있는지 이해하는 것이 전반적인 애플리케이션의 제품 품질을 달성하는 데 중요합니다.\n\n당연한 다음 질문은 다음과 같습니다: 좋아, 그런데 RAG를 어떻게 평가할까요? 개방형 자유형식 응답의 품질을 어떻게 측정할까요? 어떤 지표를 사용하여 실제로 측정할 수 있을까요? 이러한 평가를 자동화할 수 있을까요, 그리고 어느 수준에서 할 수 있을까요? LLM은 본질적으로 비결정론적이고 그들이 소비하는 데이터도 본질적으로 불안정할 때 품질을 어떻게 보장할까요?\n\n<div class=\"content-ad\"></div>\n\n이런 건들로 이루어진 큰 질문들이에요. 우리는 ARC나 HellaSwag 같은 프레임워크를 이용한 모델 평가, LLM-as-a-judge와 같은 접근 방식, 바늘 찾기 테스트와 같은 테스트, 어려움, 신뢰성, 그리고 관련성과 같은 측정 항목, Ragas와 LlamaIndex와 같은 도구 등의 주제를 다룰 거에요.\n\n하지만, 이 모든 재미로움은 다음 블로그를 기다려야 해요.\n\n본 글에 대한 기술적 피드백으로 Etienne Ohl와 Jack Bennetto에게 특별히 감사드려요.","ogImage":{"url":"/assets/img/2024-06-22-RAGforQualityEngineers_0.png"},"coverImage":"/assets/img/2024-06-22-RAGforQualityEngineers_0.png","tag":["Tech"],"readingTime":13},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<h2>RAG 만들기는 쉽지만, 품질 있는 RAG 만들기는 어려워요</h2>\n<p><img src=\"/assets/img/2024-06-22-RAGforQualityEngineers_0.png\" alt=\"image\"></p>\n<p>검색 확장 생성(RAG)은 대규모 언어 모델(LLMs)의 기능을 확장하는 일반적인 패턴이 되었습니다.</p>\n<p>이론적으로 RAG는 간단합니다(컨텍스트 창에 데이터를 추가하기만 하면 됩니다!) 하지만 실제로는 복잡합니다. 상자 다이어그램 너머에는 고급 청킹 전략, 재랭킹, 다중 쿼리 리트리버, 작은 데이터부터 큰 데이터 검색, 가상 문서 임베딩, 사전 임베딩 데이터 보강, 동적 라우팅, 맞춤 임베딩 모델... 등이 숨어 있습니다.</p>\n<div class=\"content-ad\"></div>\n<p>초기 파이프라인을 설정하는 것은 빠르고 쉽지만, 프로덕션 수준의 품질에 도달하는 것은 상당히 복잡합니다. 신중한 고려 없이 RAG 시스템은 부정확하거나 관련성이 없는 정보를 반환할 수 있습니다. 비효율적으로 비싼 리소스를 소비하거나 프로덕션 규모의 소스 데이터로 확장할 때 병목 현상이 발생할 수도 있습니다.</p>\n<p>RAG 시스템의 품질을 효과적으로 평가하고 효율적으로 이해하는 것은 각 개별 구성 요소가 전체 RAG 파이프라인을 만드는 과정을 이해하는 데 필요합니다. 이러한 각 부분에 대한 설계 결정은 품질에 영향을 미치며, RAG 애플리케이션을 배포하려는 모든 사람이 알아야 합니다.</p>\n<p>이 글에서는 테스트와 품질 관점에서 RAG 개념과 패턴에 대한 소개를 제공합니다. RAG가 왜 가치 있는지에 대한 소개로 시작하여, 프로덕션 품질의 RAG를 구축하는 데 내재된 많은 설계 결정이 어떻게 향상되는지에 대해 설명합니다. 이 소개는 특정 평가 방법과 기법에 대해 논의하기 전에 필요한 기초를 제공할 것입니다.</p>\n<h1>LLM의 한계</h1>\n<div class=\"content-ad\"></div>\n<p>RAG 파이프라인을 이해하려면, RAG가 대응하려는 LLM의 한계를 먼저 이해해야 합니다.</p>\n<p>LLM의 핵심은 간단합니다: 프롬프트를 보내면 응답을 받습니다.</p>\n<p>LLM이 응답을 반환하려면 모델과 추론 계산을 실행해야 합니다. 이 계산은 입력을 수백억 개 또는 수조의 매개변수와 결합하는 것을 포함합니다. 이는 비용이 많이 드는 계산입니다.</p>\n<p>LLM을 호출하는 것만큼이나 LLM을 훈련시키는 것은 훨씬 어렵습니다. 훈련은 모델 내 매개변수에 대한 최적값을 결정하는 과정입니다. 최상의 가중치를 계산하는 데 사용되는 다양한 알고리즘이 있지만, 모두 특정 입력에서 모델을 실행하고 오차를 계산한 후에 조정을 하는 반복적 과정을 포함합니다. 이 과정은 많은 횟수로 많은 입력에서 계속되며, 결국 훈련된 모델을 얻게 됩니다. 모델 추론은 몇 초만에 완료될 수 있지만, 모델 훈련은 광범위한 GPU 클러스터에서도 몇 주가 걸릴 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<img src=\"/assets/img/2024-06-22-RAGforQualityEngineers_1.png\">\n<p>엄청난 교육 비용은 새로운 정보를 LLM에 통합하는데 병목 현상을 야기합니다. 대부분의 기업은 모델을 교육할 자원이 없으며, 단순히 사적 데이터로 교육하여 LLM에 \"새로운 정보를 추가\"할 수 없습니다. 대신, 잘 자금을 지원받는 대규모 기술 기업은 대형 공개 데이터 세트에서 일반 목적의 기반이 되는 모델을 교육하고, 이러한 모델은 RAG와 같은 보조 프로세스로 새로운 능력 및 정보가 부가됩니다.</p>\n<p>구체적으로 RAG는 새로운 모델을 교육하는 높은 비용을 우회하는 방식으로 LLM이 추가적인 지식에 접근하도록 하는 것을 목표로 합니다.</p>\n<h1>RAG 기본 원리</h1>\n<div class=\"content-ad\"></div>\n<p>RAG가 실제로 작동하는 방식에 대해 알아봅시다.</p>\n<p>LLM에 전송된 프롬프트는 제한된 길이인 context window을 가지고 있습니다. Context window은 토큰(단어에 대한 대략적인 동등물) 단위로 측정됩니다. Context window의 크기는 보통 1K, 4K 또는 그 이상의 토큰으로 표시되지만 더 큰 context window이 사용 가능해지고 있습니다(예: Gemini 1.5 Pro의 128K).</p>\n<p>많은 사람들이 직관적으로 context window은 할 수 있는 가장 긴 질문이라고 생각하지만, 이는 context window에 대한 한정적인 사고 방식입니다. LLM의 작동 방식으로 인해, context window에 제공된 정보는 LLM이 응답을 생성하는 동안 LLM에게 사용 가능합니다. 따라서 추가 정보를 제공하는 데 사용될 수 있습니다. 일반적으로 이를 in-context learning 이라고 합니다.</p>\n<p>따라서, 우리는 context window을 사용하여 LLM이 질문에 답변하기 위해 필요한 새로운 지식을 제공할 수 있습니다. 예를 들어, 우리가 상조에 관한 회사 정책에 대해 물어보는 프롬프트를 만들고, context window 내에서 이에 관한 전체 회사 안내서(상조에 관한 섹션 포함)를 넣는다면 새로운 정보를 제공하여 LLM이 응답할 수 있게 할 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<p>해당 솔루션은 이미 관련 정보가 있고 그 정보가 문맥 창 안에 쏙 들어갈 수 있는 경우 간단합니다. 불행히도 항상 그런 것은 아닙니다. 따라서 우리는 우리의 프롬프트에 관련 정보만 검색 및 다운 선택할 수 있는 메커니즘이 필요합니다.</p>\n<p>무식한 접근법은 프롬프트에서 용어를 검색하여 관련할 수 있는 데이터 전체에서 주변 텍스트를 복사한 후 이를 프롬프트에 추가하는 것입니다.</p>\n<p>이 간단한 키워드 검색 형태의 RAG는 LLM 응답을 향상시킬 수 있으며 어떤 맥락에서는 유용할 수 있지만 가끔식 거짓 양성 (다른 맥락에서 사용된 키워드) 때문에 문제가 될 수도 있습니다. 다행히도 우리는 텍스트의 의미에 맞추어 일치시키는 의미 검색을 활용하여 더 나은 결과를 얻을 수 있습니다.</p>\n<p>구체적으로, 우리는 포지블리 관련 데이터 청크에서 임베딩 모델을 활용하여 임베딩을 생성한 다음 이 임베딩을 통해 데이터를 검색하여 우리의 프롬프트에 관련된 데이터를 찾을 수 있습니다. 이 방법은 매우 단순화된 접근법이지만, 진정한 RAG와 같은 결과를 얻기 시작하고 있습니다.</p>\n<div class=\"content-ad\"></div>\n<h1>RAG 및 임베딩</h1>\n<p>간단한 키워드 검색보다 RAG가 제공하는 이점을 이해하려면 임베딩 모델의 목적과 성격을 이해해야 합니다. 이는 그 자체로 심도 있는 주제이지만, RAG를 이해하는 데 중요합니다.</p>\n<p>임베딩 모델은 우리의 원래 LLM과 유사하지만, 새로운 콘텐츠를 생성하는 대신 입력을 벡터(숫자 목록)로 축소합니다. 임베딩 모델은 매우 큰 숫자 목록입니다. 임베딩 모델이 생성하는 벡터는 일반적으로 768 또는 1536 숫자(차원)지만, 다른 크기의 벡터도 존재합니다.</p>\n<p>임베딩 모델에 의해 생성된 벡터는 단순히 임의의 숫자 세트가 아니라 모델에 따라 입력 데이터의 의미를 요약한 것입니다. 이 벡터는 다른 모델에게는 의미가 없지만 \"유사한\" 텍스트는 같은 모델에서 유사한 벡터를 생성할 것입니다. \"유사하다\"는 단순히 \"동일한 키워드를 가지고 있다\" 이상을 의미합니다. 임베딩 모델은 구조화되지 않은 데이터로부터 보다 심층적인 의미를 추출하기 위해 특별히 훈련되었습니다. 예를 들어 \"남자 말이 날지 않는다\"와 \"날개가 있는 사나이가 말 타고 있다\"는 비슷한 단어를 가지고 있지만 같은 모델에서는 서로 멀리 떨어진 벡터를 생성할 것입니다.</p>\n<div class=\"content-ad\"></div>\n<p>벡터의 멋진 점은 그들에게 수학 연산을 수행할 수 있다는 것입니다. 빠른 수학이죠. 수백만 개의 벡터를 검색하여 비교적 짧은 시간 안에 유사한 벡터를 찾을 수 있습니다. (여기에 사용된 일부 알고리즘이 있습니다.)</p>\n<p>이제 우리의 RAG 파이프라인 조각들이 모두 마련되었으니, 단계별로 진행해봅시다.</p>\n<p>첫 네 단계는 한 번에 수행되거나 소스 데이터가 변경될 때 업데이트됩니다. 다섯 번부터 여덟 번까지의 단계는 각 추론 요청마다 수행됩니다:</p>\n<ul>\n<li>모든 가능성 있는 데이터를 수집합니다. - 이토록 많은 데이터가 있어서 우리의 프롬프트의 컨텍스트 창에 쏙 들어가기 불가능합니다.</li>\n<li>이 데이터를 더 작은 조각으로 나눕니다 (나중에 자세히 설명하겠습니다).</li>\n<li>그런 다음 각 조각을 임베딩 모델을 통해 실행하여 조각의 의미를 포함하는 벡터를 생성합니다.</li>\n<li>해당 벡터를 벡터 데이터베이스에 저장합니다.</li>\n<li>각 추론 요청으로: 프롬프트를 받으면, 그 프롬프트를 조각낸 소스 데이터와 동일한 임베딩 모델을 통해 실행하여 다른 벡터 (프롬프트 벡터 또는 쿼리 벡터라고 함)를 생성합니다.</li>\n<li>우리의 벡터 데이터베이스에서 우리의 프롬프트 벡터와 유사한 벡터를 검색합니다. 반환된 벡터들은 생 데이터를 키워드로 검색했다면 얻었을 것보다 더 나은 매치일 것입니다.</li>\n<li>식별된 관련 벡터를 (선택적으로) 재정렬하고, 그런 다음 상위 벡터 각각의 생 데이터를 반환합니다.</li>\n<li>원시 데이터는 초기 프롬프트와 결합되어 LLM으로 보내집니다.</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-22-RAGforQualityEngineers_2.png\" alt=\"이미지\"></p>\n<p>여기, 우리 LLM은 이제 새로운 독점 데이터를 모두 훈련한 것처럼 작동하며, 기본 모델 훈련을 비용 문제로 수행할 필요가 없습니다.</p>\n<p>이론상으로는 이렇게 작동해야 합니다. 그러나 실제로는 이 너무 단순한 파이프라인은 여러분의 제품 요구를 만족시키지 못할 가능성이 높으며, 우수한 품질의 제품에 도달하려면 RAG 파이프라인을 준비하려면 특정 애플리케이션의 요구를 충족시키기 위해 다양한 부분을 적응, 개선, 교체 또는 확장해야 할 것입니다.</p>\n<h1>RAG 디자인과 품질</h1>\n<div class=\"content-ad\"></div>\n<p>위에서는 RAG를 소개하지만, RAG는 실제로 매우 복잡할 수 있으며, 이러한 실제 세계의 복잡성은 응용 프로그램의 품질에 영향을 미칠 수 있습니다. RAG 파이프라인 내에서 사용 가능한 일부 구현 도전, 품질 위험 및 대안을 이해하기 위해 각 단계를 살펴보겠습니다.</p>\n<h2>#1—관련 데이터 수집, 적재 및 풍부화</h2>\n<p><img src=\"/assets/img/2024-06-22-RAGforQualityEngineers_3.png\" alt=\"이미지\"></p>\n<p>시작부터 모든 \"가능성 있는 데이터\"를 찾아야 합니다.</p>\n<div class=\"content-ad\"></div>\n<p>위의 RAG 다이어그램에서 1번 아이콘은 여러 소스에서 데이터를 수집하고, 정리하고, 변환하며, 익명화하고, 토큰화하는 데이터 파이프라인(또는 파이프라인 세트!)일 가능성이 높습니다.</p>\n<p>일부 파이프라인은 특히 텍스트 이외의 형식을 가진 원시 데이터의 경우 매우 복잡해질 수 있습니다. 예를 들어, 일부 파이프라인은 대량의 스캔된 물리 문서를 처리하기 위해 OCR 기술을 널리 활용합니다.</p>\n<p>데이터 파이프라인의 복잡성은 데이터 파이프라인 테스트의 모든 과제가 따라옵니다.</p>\n<p>가장 잘 구현된 RAG 파이프라인도 소스 데이터가 심지어 벡터 DB로 전달되지 않는다면 완전히 실패할 수 있으며, 이 데이터의 다양성, 속도 및 양에 따라 RAG의 이 단계는 복합적이며 많은 응용프로그램 품질 문제의 원인이 될 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<p>일반적인 데이터 파이프라인 활동에 추가로, RAG는 데이터 풍부화를 통해 혜택을 얻을 수 있습니다. 종종, 다른 시스템(또는 사람들)은 소스 데이터에 대한 맥락을 알고 있어서 그 의미를 평가하는 데 엄청난 도움이 될 수 있습니다. 예를 들어, 고객 데이터베이스에는 다른 시스템에서 제공하는 태그나 주석과 같은 관련 정보를 추가하여 데이터를 풍부화할 수 있습니다. 종종, 다른 생성 모델이나 자연어 처리(NLP)가 더 깨끗하거나 요약된 메타데이터를 생성하는 데 사용됩니다. 모두 \"임베딩 생성\" 전의 \"전처리\"로 생각해보세요. 그리고 제대로 수행된다면, 검색 품질을 크게 향상시킬 수 있습니다.</p>\n<p>당신이 RAG 검색 시스템의 품질을 평가하고 있다면, 데이터가 실제로 어떻게 소스되고 흡수되는지 이해하는 데 시간을 투자하는 것이 가치가 있습니다. 훌륭한 AI 부분에 도착하기 전에 RAG 파이프라인에 도달하기 전에 데이터가 어떻게 가져오고 처리되는지 알아두세요.</p>\n<h2>#2—Chunking</h2>\n<p><img src=\"/assets/img/2024-06-22-RAGforQualityEngineers_4.png\" alt=\"2024-06-22-RAGforQualityEngineers_4\"></p>\n<div class=\"content-ad\"></div>\n<p>데이터가 수신되고 임베딩 모델을 실행하기 전에는 데이터를 diskrete pieces로 나눠야 합니다. 그렇다면 데이터를 어떻게 분할할지 어떻게 결정하나요? 이를 chunking strategy라고 합니다.</p>\n<p>최적의 크기는 얼마나 크거나 작아야 할까요? 청크들은 서로 중첩되어야 할까요? 페이지, 단락 또는 일정한 길이로만 나누는 것 이외에 더 스마트한 분할 방법이 있을까요? 비표준 형식의 데이터(code, JSON 등)는 어떻게 chunk해야 할까요?</p>\n<p>이러한 질문들은 chunking strategy가 답하려고 노력하는 것이며 완벽한 해결책은 없습니다. 서로 다른 전략은 서로 다른 타협점을 갖습니다. 일부는 간단하고 빠르게 구현할 수 있으며, 보통 결과를 제공합니다. 다른 전략은 더 복잡하고 관련이 깊습니다. 더 나은 히트율과 LLM 응답 품질을 제공할 수 있습니다. 데이터를 너무 거칠게 나누면 의미 없는 데이터로 context window를 채우거나 다른 관련 청크를 밀어내거나 의미 있는 일치를 얻을 수 없을 정도로 일반적인 임베딩을 생성할 수 있습니다. 너무 세밀하게 나누면 관련 데이터를 잘라낼 수 있습니다.</p>\n<p>이 문서에서는 다섯 가지 chunking 범주를 탐구합니다: 고정 크기, 재귀, 문서 기반, 의미 기반, 그리고 AI를 사용한 Agentic(체킹에 인공 지능 사용, 멋지죠!)입니다.</p>\n<div class=\"content-ad\"></div>\n<p>많은 기타 접근 방식이 있습니다. 예를 들어, 작은 것에서 큰 것으로 검색을 최적화하려면 작은 청크를 사용하지만 각 청크는 큰 부모 청크에 연결되어 있어서 삽입될 컨텍스트 모델에 검색됩니다. 콘텍스트 인식 청킹은 문서의 성격에 대해 기존 지식을 활용하여 문서를 논리적 청크로 적절하게 분할합니다.</p>\n<p>위 목록은 아마 완전하지 않을 수 있지만, RAG 구현자가 사용할 수 있는 다양한 옵션과 애플리케이션 전체의 품질에 적합하고 조정된 청킹 전략의 중요성을 보여줍니다. Pinecone 블로그에서 이러한 전략 중 많은 내용을 자세히 다루고 있습니다.</p>\n<h2>#3—임베딩 모델 선택과 구성</h2>\n<p><img src=\"/assets/img/2024-06-22-RAGforQualityEngineers_5.png\" alt=\"이미지\"></p>\n<div class=\"content-ad\"></div>\n<p>임베딩을 생성하는 데 사용할 수 있는 여러 모델이 있습니다. 서로 다른 모델은 다양한 상황에서 더 나은 또는 나쁘게 수행할 수 있습니다. 일부 모델은 일반 사용을 위해 사전 훈련되어 있고 일부는 특정 도메인(예: 의학 기록)에 대해 세밀하게 조정되어 있습니다. 또한 응용 프로그램에서 처리하는 특정 데이터에 대해 자체 임베딩 모델을 세밀하게 조정할 수도 있습니다.</p>\n<p>또한 많은 모델이 다른 크기로 제공되며(임베딩 생성 비용 및 시간에 영향을 미침), 다른 입력 길이(처리 가능한 최대 청크 크기) 및 다른 출력 벡터 차원(높은 차원 = 더 정확하지만 더 많은 공간 요구와 느린 속도)으로 제공됩니다.</p>\n<p>일부 임베딩 모델은 API를 통해만 액세스할 수 있습니다(예: OpenAI 임베딩 엔드포인트), 다른 모델은 완전한 오픈 소스로 제공되어 로컬에서 다운로드하고 실행하거나 클라우드 공급업체에 호스팅할 수 있습니다.</p>\n<p>응용 프로그램 내에서 다른 데이터 경로에 대해 다른 임베딩 모델을 사용하는 것도 가능합니다.</p>\n<div class=\"content-ad\"></div>\n<p>일반적으로 좋은 임베딩 모델은 RAG 응용 프로그램에 충분할 수 있지만, 일부는 특정 임베딩 모델이나 사용자 지정된 모델을 사용하여 혜택을 얻을 수 있습니다.</p>\n<p>임베딩 전략에 대한 설계 고려 사항과 해당 선택의 품질 특성을 알면 응용 프로그램의 평가 요구 사항과 접근 방식에 대한 통찰력을 제공할 것입니다. 임베딩 모델 선택을 평가하는 더 깊은 논의는 여기에서 확인할 수 있습니다.</p>\n<h2>#5—쿼리 처리 및 임베딩</h2>\n<p><img src=\"/assets/img/2024-06-22-RAGforQualityEngineers_6.png\" alt=\"이미지\"></p>\n<div class=\"content-ad\"></div>\n<p>수신한 쿼리에 임베딩 모델을 정확히 실행해야 한다는 규칙은 없습니다. 실제로 이 쿼리를 최적화하여 애플리케이션의 전반적인 품질을 향상시킬 수 있는 다양한 방법이 있습니다. 이는 특히 쿼리가 사람 사용자로부터 직접 제출되었으며 모호하고 애매한 쿼리일 경우에 더욱 참된 것입니다.</p>\n<p>애플리케이션의 특성 또는 의도에 대한 추가 지식을 통해 LLM 또는 전통적인 논리를 사용하여 쿼리를 축소하거나 다시 작성하는 것이 가능할 수 있습니다. 다시 말해, 의도된 것이 아닌 실제로 묻는 것이었던 쿼리를 재작성하는 방식으로 쿼리를 재구성할 수 있습니다.</p>\n<p>쿼리 처리의 고급 형태인 HyDE도 있습니다. 여기서는 가상 문서를 작성하여 유사한 문서(답변에서 답변)를 벡터 검색하고 임베딩 및 쿼리 검색(질문에서 답변)을 하는 것대신 사용할 수 있습니다.</p>\n<p>또 다른 옵션은 쿼리를 여러 관련된 쿼리로 분할하고 각각을 병렬로 실행한 다음 결과를 결합하는 것입니다. 이는 처리 비용이 듬성들지만 검색 품질을 향상시킬 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<p>특정 사용 사례에 따라 사용자 지정 쿼리 처리가 필요할 수 있으며 응용 프로그램의 품질과 동작에 큰 영향을 미칠 수 있습니다.</p>\n<h2>#4, #6—Vector DB 및 Vector Search</h2>\n<p><img src=\"/assets/img/2024-06-22-RAGforQualityEngineers_7.png\" alt=\"image\"></p>\n<p>벡터 검색은 빠르지만, 쿼리와 유사한 임베딩을 찾기 위해 벡터 DB를 검색하는 데에는 시간 (그리고 돈) 비용이 소요될 수 있습니다. 이 비용을 최소화하는 한 가지 방법은 의미 캐싱입니다. 의미 캐싱에서는 임베딩이 처음 검색된 후 응답이 캐시되어, 향후 유사한 검색이 캐시로부터 직접 데이터를 반환하게 됩니다.</p>\n<div class=\"content-ad\"></div>\n<p>물론, 캐싱은 복잡성을 증가시킵니다 (그리고 컴퓨터 과학에서의 두 번째 어려운 문제 중 하나입니다—다른 하나의 이름을 기억하지 못하겠군요). 캐싱은 성능을 향상시킬 수 있지만, 오래된 캐시는 변동성 있는 소스 데이터 환경에서 특히 응답 품질을 해치는 요인이 될 수 있습니다.</p>\n<h2>#7—재랭킹</h2>\n<p><img src=\"/assets/img/2024-06-22-RAGforQualityEngineers_8.png\" alt=\"이미지\"></p>\n<p>위에서 설명한 내용에서 우리는 단순히 우리의 벡터 검색으로 반환된 모든 관련 데이터를 컨텍스트 창에 채울 수 있다고 가정했습니다. 이것은 명백히 간소화된 내용이며, 반환된 모든 벡터 중 어느 것이 컨텍스트 창에 포함될 것인지를 결정하기 위한 과정이 있어야 합니다.</p>\n<div class=\"content-ad\"></div>\n<p>검색 결과를 콘텍스트 창 안에 맞출 수 있을 때에도, 많은 연구에서 콘텍스트 Stuffing(콘텍스트 창 채우기)가 LLM 회상을 부정적으로 영향을 줄 수 있다는 것을 지적합니다. 이는 중간에서 사라지는 문제를 도입하여 응답 품질(회상은 LLM이 콘텍스트 창에 있는 정보를 사용하는 능력입니다)에 영향을 줄 수 있습니다.</p>\n<p>해결책은 초기 벡터 검색 후에 추가 단계로 재랭킹을 추가하는 것입니다.</p>\n<p>재랭킹의 TLDR(요약): 임베딩 모델은 속도에 최적화되어 있으며 많은 문서에 대해 실행되어야 하므로 빠릅니다. 재랭킹 모델(또는 교차 인코더)이라 불리는 다른 모델은 느리지만 정확도에 최적화되어 있습니다. 그래서 빠르고 부정확한 임베딩 모델을 사용하여 임베딩을 생성한 다음, 작은 집합에서 최고 품질의 문서를 찾기 위해 느리고 정확한 모델을 사용합니다. 느린 정확한 검색에서 가장 일치하는 결과는 콘텍스트 창에서 우선순위를 갖습니다.</p>\n<p>다시 말하지만, 이보다 더 많은 내용이 있지만, 재랭킹의 본질은 바로 이것입니다. Pinecone 블로그에서 더 자세한 설명을 찾을 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<p>다시 순위를 매기면 RAG에서 반환된 데이터의 관련성을 크게 향상시킬 수 있습니다. 컨텍스트 창에서 더 관련성이 높은(또는 무관련성이 적은) 데이터는 응답 품질을 향상시킬 것입니다. 그러나 복잡성과 지연이 증가하지만, 품질의 트레이드오프는 많은 RAG 응용 프로그램에서 가치 있는 요소일 수 있습니다.</p>\n<h2>큰 컨텍스트 창 vs. RAG</h2>\n<p>우리는 마침내 LLM을 호출하는 지점에 도달했지만, 프롬프트 엔지니어링에 대해 이야기하기 전에 RAG와 큰 컨텍스트 창 간의 관계에 대해 언급할 시간을 가져야 합니다.</p>\n<p>LLM 기술은 빠르게 발전하고 있으며 개선의 한 가지 측면은 컨텍스트 창의 크기입니다. 한 가지 대표적인 예는 2024년 2월에 출시된 Gemini 1.5 Pro이며, 128K 컨텍스트 창을 제공하며(공개적으로 출시되지 않음) 최대 백만(!!!) 토큰까지 확장할 수 있는 옵션이 있습니다.</p>\n<div class=\"content-ad\"></div>\n<p>일부 사람들은 100만 토큰 컨텍스트 창이 RAG 파이프라인을 사용할 때 사용되지 않을 것이라고 예상했습니다. 그러나 실제로는 그렇지 않습니다. 이 블로그에서는 RAG가 왜 유용하며 (심지어 필수적이기도 한) 거대한 컨텍스트 창을 사용할 때도 필요한 이유에 대해 설명합니다. (스포일러: 비용, 지연 시간 및 회수 품질)</p>\n<p>대규모 컨텍스트 모델은 유용하며, LLMs가 많은 사실들 간에 종합적인 결론을 요구하는 쿼리에 응답하는 데 도움이 될 수 있습니다 (이러한 사실들이 RAG를 통해 선별되어 있는지 여부는 상관 없음).</p>\n<p>큰 컨텍스트 창과 RAG 간의 관계는 계속 발전할 것이며, RAG를 구현하고 테스트하는 사람들은 응용 프로그램 품질에 미치는 이러한 트레이드오프와 그들의 영향을 이해해야 합니다.</p>\n<h2>#8—프롬프트 생성</h2>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-22-RAGforQualityEngineers_9.png\" alt=\"2024-06-22-RAGforQualityEngineers_9\"></p>\n<p>벡터DB에서 관련 데이터를 많이 받아 재설정하고, LLM(Large Language Model)의 문맥 창 안에 맞는 적절한 데이터 세트로 마무리했습니다. 그럼 이제 어떻게 해야 할까요? 받은 데이터를 초기 질문 뒤에 밀어 넣고 끝내면 될까요?</p>\n<p>LLM을 다뤄본 사람이라면 알 수 있듯이, 그것만큼 간단한 일이 아닙니다. LLM은 강력할 수 있지만, 변덕스럽고 짜증을 유발할 수도 있습니다. 당신의 프롬프트에 작은 세부 사항이 응담 품질에 상당한 영향을 미칠 수 있다는 것이 밝혀졌습니다. 프롬프트의 단어 선택, 데이터 순서, 사용하는 어조, \"시간을 들이다\"와 같은 제안, 심리적 언어 사용까지 모두 LLM 응답 품질에 영향을 미칠 수 있습니다. 프롬프트를 자동으로 생성하는 최적 프롬프트 생성 전략이 있습니다. ...맞아, 프롬프트 생성에 특별히 훈련된 다른 모델을 사용하는 것입니다. 이것은 신속히 진화하는 프롬프트 엔지니어링 분야의 일부입니다.</p>\n<p>최상의 품질의 응답을 생성할 정확한 프롬프트 템플릿은 보통 모델 및 응용 프로그램에 따라 다르며 종종 실험과 시행착오가 필요할 수 있습니다. RAG의 이 보이지 않는 작은 세부 사항이 가지는 품질 영향을 감안할 때, 적용된 특정 프롬프트 엔지니어링은 시스템의 다른 부분과 마찬가지로 면밀히 평가되고 심사되어야 합니다.</p>\n<div class=\"content-ad\"></div>\n<h1>RAG 시스템의 측정 및 평가</h1>\n<p>우리는 RAG 파이프라인의 주요 구성 요소들을 살펴보고 (간략하게) 이들이 애플리케이션 품질에 미치는 영향에 대해 이야기해 보았습니다. 이것은 소개였지만, 이러한 유형의 애플리케이션의 내부 작업 및 품질 도전에 대한 통찰력을 제공해야 합니다. RAG에 대해 더 깊이 파고드는 많은 훌륭한 기사, 블로그, 논문이 있습니다. 시작할 때 하나만 선택한다면, \"Retrieval-Augmented Generation for Large Language Models—A Survey\"를 읽어보세요.</p>\n<p>주요 교훈: RAG를 구현할 때 선택할 수 있는 옵션과 선택지가 많으며, 각각에는 품질에 대한 대가와 영향이 있습니다. 이러한 선택들 중 일부는 직접적으로 평가될 수 있고, 일부는 전반적인 검색이나 응답 품질에 영향을 미칩니다. 이러한 선택 각각과 이들이 당신의 RAG 시스템에 어떤 영향을 미칠 수 있는지 이해하는 것이 전반적인 애플리케이션의 제품 품질을 달성하는 데 중요합니다.</p>\n<p>당연한 다음 질문은 다음과 같습니다: 좋아, 그런데 RAG를 어떻게 평가할까요? 개방형 자유형식 응답의 품질을 어떻게 측정할까요? 어떤 지표를 사용하여 실제로 측정할 수 있을까요? 이러한 평가를 자동화할 수 있을까요, 그리고 어느 수준에서 할 수 있을까요? LLM은 본질적으로 비결정론적이고 그들이 소비하는 데이터도 본질적으로 불안정할 때 품질을 어떻게 보장할까요?</p>\n<div class=\"content-ad\"></div>\n<p>이런 건들로 이루어진 큰 질문들이에요. 우리는 ARC나 HellaSwag 같은 프레임워크를 이용한 모델 평가, LLM-as-a-judge와 같은 접근 방식, 바늘 찾기 테스트와 같은 테스트, 어려움, 신뢰성, 그리고 관련성과 같은 측정 항목, Ragas와 LlamaIndex와 같은 도구 등의 주제를 다룰 거에요.</p>\n<p>하지만, 이 모든 재미로움은 다음 블로그를 기다려야 해요.</p>\n<p>본 글에 대한 기술적 피드백으로 Etienne Ohl와 Jack Bennetto에게 특별히 감사드려요.</p>\n</body>\n</html>\n"},"__N_SSG":true}