{"pageProps":{"post":{"title":"장기 단기 메모리 LSTM  RNN 개선하기","description":"","date":"2024-06-19 18:53","slug":"2024-06-19-LongShortTermMemoryLSTMImprovingRNNs","content":"\n\n\n![그림](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_0.png)\n\n이 글에서는 Long-Short-Term Memory Networks (LSTM)에 대해 소개하겠습니다. 이는 일반적인 바닐라 순환 신경망(RNN)의 변형으로서 장기 의존성을 처리하는 데 능숙합니다.\n\n이들은 의사결정에 필요하거나 중요하지 않다고 판단되는 특정 정보를 기억하거나 잊는 서로 다른 \"게이트\"를 사용합니다.\n\nLSTM은 RNN의 최신 버전으로, 산업 내에서 폭넓게 사용되며 우리가 오늘날 보는 모든 멋진 대형 언어 모델 (LLMs)의 기반이 됩니다.\n\n\n<div class=\"content-ad\"></div>\n\nRNN 개요\n\nRecurrent Neural Networks(RNN)은 일반적인 피드포워드 신경망의 변형으로, 자연어나 시계열 데이터와 같은 순차 데이터를 더 잘 처리할 수 있도록 합니다.\n\n이는 이전 입력과 출력을 다음 레이어로 전달하는 숨겨진 순환 뉴런을 가지고 수행됩니다. 아래는 예시입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_1.png)\n\n네트워크를 통해 전달되는 벡터 h를 주목해보세요. 이것이 순환 신경망(RNNs) 뒤에 숨겨진 주요 기능인 은닉 상태입니다. 이것이 시퀀스 데이터에 대해 잘 작동하는 이유입니다.\n\n은닉 상태는 이전에 계산된 은닉 상태와 해당 시간 단계에서의 새 입력을 결합합니다. 그런 다음 해당 시간 단계의 최종 출력을 계산하기 위해 시그모이드 활성화 함수가 적용됩니다. 수학적으로 표현하면:\n\n![image](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_2.png)\n\n\n<div class=\"content-ad\"></div>\n\n아래에서:\n\n- Y: 출력 벡터\n- X: 기능의 입력 벡터\n- h: 숨겨진 상태\n- V: 출력을 위한 가중 행렬\n- U: 입력을 위한 가중 행렬\n- W: 숨겨진 상태를 위한 가중 행렬\n\nV, U 및 W의 가중 행렬은 시간에 걸쳐 백프로파게이션을 통해 찾아집니다. 이는 백프로파게이션 알고리즘의 한 변형에 불과합니다.\n시각적으로는 이렇게 보입니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_3.png)\n\nFor example, when predicting Y_1, the RNN would use the inputs of X_1 plus the output from the previous time step from Y_0. As Y_0 influences Y_1, we can then see that Y_0 will also indirectly influence Y_2, demonstrating the recurrent nature.\n\nIf you want a full intro to RNNs then check out my previous blog.\n\n# Vanishing & Exploding Problem\n\n\n<div class=\"content-ad\"></div>\n\nRNN의 긍정적인 측면 중 하나는 각 계층이 U, W 및 V의 가중 행렬을 공유하지만, 정규 피드포워드 신경망은 각 계층마다 자체 가중 행렬을 갖는다는 것입니다. 이로 인해 RNN은 더 메모리를 효율적으로 사용할 수 있습니다.\n\n그러나 이 가중 행렬 공유는 그들의 중요한 결함 중 하나인 사라지는 그래디언트와 폭주하는 그래디언트 문제를 야기합니다.\n\nRNN은 backpropagation through time (BPTT)라는 backpropagation 알고리즘의 변형을 사용하여 학습합니다. 이 알고리즘은 일반적인 backpropagation과 유사하지만, 각 시간 단계에서 업데이트해야 하는 계층 간의 공유 가중 행렬로 인해 더 '중첩된' 계산이 필요합니다.\n\nBPTT의 일반적인 공식은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image 1](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_4.png)\n\nRNN에서 J는 임의의 가중치 행렬이며 U, W 또는 V일 수 있으며 E는 총 오차입니다.\n\nRNN은 일반적인 신경망보다 더 깊은 경향이 있습니다(각 시간 단계는 하나의 레이어입니다). 따라서 그래디언트가 1보다 작거나 큰 경우에는 역방향으로 전파될 때 그래디언트가 소멸되거나 폭발될 수 있습니다.\n\n![image 2](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_5.png)\n\n\n<div class=\"content-ad\"></div>\n\n관심 있는 독자를 위해, 왜 이런 일이 발생하는지의 수학적인 전체 분석은 여기서 찾을 수 있어요. 이것은 고유값과 야코비안 행렬과 같은 재미있는 것들이 관련돼 있습니다!\n\n만약 역전파를 통한 시간 알고리즘 (BTTP)과 그라디언트가 사라지거나 폭발하는 문제의 전체적인 분석을 원한다면, 제 이전 게시물을 확인해보세요.\n\n사라지는 그라디언트와 폭발하는 그라디언트 문제를 잘 보여주는 좋은 예시는 Stanford의 CS224D 수업에서 제시되었습니다. 두 개의 문장을 생각해보세요:\n\n- \"제인이 방으로 들어갔어요. 존도 들어왔어요. 제인이 ___에게 안녕했어요.\"\n- \"제인이 방으로 들어갔어요. 존도 들어왔어요. 늦었고, 모두가 긴 하루 일과를 마치고 집으로 향했어요. 제인이 ___에게 안녕했어요.\"\n\n<div class=\"content-ad\"></div>\n\n어떤 경우에도 빈 공간은 아마도 존을 가리키는 것입니다. RNN은 이 맥락을 학습하여 두 문장 모두 출력이 존임을 이해해야 합니다.\n\n그러나 실험 결과, 문장 1이 문장 2보다 정확하게 예측되는 경향이 있었습니다. 이는 문장 2에서 그래디언트가 소멸되어 예측을 할 때 먼 맥락을 효율적으로 인식하지 못하기 때문입니다.\n\n이것은 분명히 문제입니다. RNN은 이와 같은 시나리오에 대한 \"기억\"을 갖도록 설계되었기 때문입니다.\n\n그래서, 이 문제에 대해 어떻게 해결할까요?\n\n<div class=\"content-ad\"></div>\n\n# 롱-숏텀 메모리\n\n## 개요\n\nLSTMs는 1997년 Hochreiter & Schmidhuber에 의해 소개되었으며, 그 기본 아이디어는 순환 셀 내부에 \"게이트\"가 있다는 것입니다. 이러한 게이트는 순환 셀이 장기 기억을 구축할 때 무엇을 기억하고 잊어야 하는지를 제어합니다.\n\n일반적인 RNN에서는 순환 셀이 다음과 같이 보입니다:\n\n\n|  Table  |  Tag   |\n|---------|--------|\n\n\n<div class=\"content-ad\"></div>\n\n\n![Long Short Term Memory - LSTM Improving RNNs](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_6.png)\n\nHowever, the LSTM cell is a lot more complicated:\n\n![LSTM Cell](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_7.png)\n\nI appreciate there is a lot going on here, but lets break it down step by step.\n\n\n<div class=\"content-ad\"></div>\n\n## 셀 상태\n\n핵심적인 차이 중 하나는 셀 상태 C의 도입입니다. 이 셀 상태에는 컨텍스트 및 과거 패턴과 같이 기본적인 정보가 포함되어 있습니다. 바로 메모리입니다. 이 셀 상태는 셀을 통과하며 선형 상호 작용을 하는 여러 개의 게이트에 의해 조정될 수 있습니다.\n\n셀 상태와 은닉 상태를 혼동하기 쉽지만, 일반적으로 셀 상태는 네트워크의 전체 메모리를 포함하도록 설계되었으며, 은닉 상태는 단기 의존성을 위해 사용되고 실제로 최근 정보만을 가지고 있습니다. 또한 예측을 위해 셀의 출력에 사용됩니다.\n\n## 잊기 게이트\n\n<div class=\"content-ad\"></div>\n\nLSTM의 첫 번째 단계는 forget gate입니다. 이 게이트는 이전 셀 상태 C_'t-1'에서 어떤 이전 정보를 삭제할지 결정하는 역할을 합니다.\n\n![이미지](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_8.png)\n\n여기서:\n\n- σ: 시그모이드 활성화 함수.\n- W_f: forget gate의 가중치 행렬.\n- h_'t−1': 이전 시간 단계의 출력.\n- x_t: 시간 단계 t의 입력.\n- b_f: forget gate의 편향.\n- f_t: 0과 1 사이의 값을 가지는 forget gate 출력.\n- X_t: 현재 입력.\n\n<div class=\"content-ad\"></div>\n\n출력 f_t는 이전 셀 상태 C_'t-1'에 곱해져 어떤 요소를 잊어야 하는지 수정합니다. 시그모이드 함수 덕분에 값은 0과 1 사이에 있으며, 0은 잊기를 의미하고 1은 기억에 추가됩니다.\n\nW_f 행렬에서 올바른 값을 찾아 역전파를 통해 이 정보를 학습합니다. 이를 통해 우리는 기억할지 잊을지를 결정할 수 있습니다.\n\n## 입력 게이트 및 후보 셀 상태\n\n입력 게이트 i_t는 다음 단계이지만 현재 시간 단계에서 셀 상태에 추가할 새로운 기억을 결정합니다. 후보 셀 상태 C*_t는 셀 상태에 추가할 가능한 모든 정보를 보유합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_9.png)\n\nWhere:\n\n- σ: 시그모이드 활성화 함수.\n- tanh⁡: 쌍곡 탄젠트 활성화 함수.\n- W_i: 입력 게이트용 가중치 행렬.\n- W_c: 후보 셀 상태용 가중치 행렬.\n- b_i: 입력 게이트용 편향\n- b_c: 후보 셀 상태용 편향.\n- C*_t: 후보 셀 상태, -1과 1 사이의 출력 값.\n- i_t: 0과 1 사이의 입력 게이트 출력.\n- h_'t-1': 이전 숨은 상태.\n- X_t: 현재 입력.\n\ntanh을 사용하면 셀 상태를 증가시키거나 감소시킬 수 있습니다. tanh는 출력을 -1과 1 사이로 압축하기 때문입니다. 시그모이드는 기억에 새로운 것을 추가하기 위해 이전 게이트와 유사한 이유로 사용됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n입력 게이트는 특히 이전 셀 상태C_'t-1'입니다.\n\n## 셀 상태 업데이트\n\n우리는 후보 셀 상태 C*_t 에서 새로운 셀 상태 C_t로 관련 정보만 추가하려고 합니다. 이를 위해 후보 셀 상태를 입력 게이트 i_t 와 곱하고 이를 잊어버린 게이트 f_t 와 이전 셀 상태 C_'t-1' 의 곱과 더할 수 있습니다.\n\n![image](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_10.png)\n\n<div class=\"content-ad\"></div>\n\n모든 작업은 좀 더 친숙한 느낌을 주려고 세포 상태를 갱신했습니다. 관련 없는 정보는 잊어버리고 필요한 새로운 정보를 추가했어요.\n\n## 출력 게이트\n\n마지막 단계는 셀에서 어떤 것을 예측으로 출력할지 결정하는 것이었지요. 먼저 출력 게이트 o_t를 계산하고, 이는 우리가 출력할 셀 상태의 어느 부분을 결정하는데 사용됩니다. 이는 기본적으로 일반 RNN의 일반적인 숨겨진 상태 반복 셀과 비슷한 역할을 해요.\n\n그 출력값은 새로운 셀 상태의 tanh 값과 곱해져서 원하는 값만을 출력합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_11.png)\n\nWhere:\n\n- σ: 시그모이드 함수.\n- tanh: 쌍곡선 탄젠트 활성화 함수.\n- W_o: 가중치 행렬.\n- b_o: 편향.\n- o_t: 출력 상태.\n- h_t: 새로운 은닉 상태.\n- h_'t-1': 이전 은닉 상태.\n- X_t: 현재 입력.\n- C_t: 새로운 셀 상태.\n\n그게 전부에요! 언급할 중요한 점은 모든 가중치 행렬이 BPTT를 사용하여 어떤 요소를 잊고 기억할지 학습해야 한다는 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 변형\n\n이것은 표준 LSTM일 뿐이고 다른 변형들이 있습니다. 그중 가장 흔한 것은 다음과 같습니다:\n\n- 양방향\n- 합성곱\n- 쌓인\n- 청구 접속\n- 게이트 순환 유닛\n\n이 글에서 이러한 모두를 다루는 것은 범위를 벗어나지만, 관심 있는 독자는 위에 제공된 링크에서 자세히 알아볼 수 있습니다. 다음 글에서는 게이트 순환 유닛에 대해 다룰 예정입니다.\n\n<div class=\"content-ad\"></div>\n\n# 요약 및 더 깊은 생각\n\nLSTMs는 처음에는 무서워 보일 수 있지만, 이 글을 통해 조금 더 친숙해졌으면 좋겠어요! 다양한 계산이 있지만, 이 모든 것들은 매우 유사합니다. 잊어버릴 것을 결정하는 forget gate와 기억에 추가할 새로운 정보를 결정하는 input gate 두 가지 기본 구성 요소가 있습니다. LSTMs의 장점은 이러한 gate들 덕분에 장기 기억력이 더 나은 것입니다.\n\n# 추가로!\n\n저는 '데이터 소스 공유', 매주 공유하는 더 나은 데이터 과학자가 되는 팁, 업계에서의 일반적인 경험, 지난 주에 한 생각들을 나누는 무료 뉴스레터를 운영하고 있어요.\n\n<div class=\"content-ad\"></div>\n\n# 나와 소통해요!\n\n- 링크드인, 트위터, 또는 인스타그램에서 연락해요.\n- 내 YouTube 채널에서는 기술적인 데이터 과학과 머신러닝 개념을 배우세요!\n\n# 참고 자료\n\n- LSTM에 대한 훌륭한 블로그 포스트\n- 스탠포드 RNN 체트시트\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. Aurélien Géron. 2019년 9월. 출판사: O'Reilly Media, Inc. ISBN: 9781492032649.","ogImage":{"url":"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_0.png"},"coverImage":"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_0.png","tag":["Tech"],"readingTime":8},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p><img src=\"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_0.png\" alt=\"그림\"></p>\n<p>이 글에서는 Long-Short-Term Memory Networks (LSTM)에 대해 소개하겠습니다. 이는 일반적인 바닐라 순환 신경망(RNN)의 변형으로서 장기 의존성을 처리하는 데 능숙합니다.</p>\n<p>이들은 의사결정에 필요하거나 중요하지 않다고 판단되는 특정 정보를 기억하거나 잊는 서로 다른 \"게이트\"를 사용합니다.</p>\n<p>LSTM은 RNN의 최신 버전으로, 산업 내에서 폭넓게 사용되며 우리가 오늘날 보는 모든 멋진 대형 언어 모델 (LLMs)의 기반이 됩니다.</p>\n<div class=\"content-ad\"></div>\n<p>RNN 개요</p>\n<p>Recurrent Neural Networks(RNN)은 일반적인 피드포워드 신경망의 변형으로, 자연어나 시계열 데이터와 같은 순차 데이터를 더 잘 처리할 수 있도록 합니다.</p>\n<p>이는 이전 입력과 출력을 다음 레이어로 전달하는 숨겨진 순환 뉴런을 가지고 수행됩니다. 아래는 예시입니다.</p>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_1.png\" alt=\"image\"></p>\n<p>네트워크를 통해 전달되는 벡터 h를 주목해보세요. 이것이 순환 신경망(RNNs) 뒤에 숨겨진 주요 기능인 은닉 상태입니다. 이것이 시퀀스 데이터에 대해 잘 작동하는 이유입니다.</p>\n<p>은닉 상태는 이전에 계산된 은닉 상태와 해당 시간 단계에서의 새 입력을 결합합니다. 그런 다음 해당 시간 단계의 최종 출력을 계산하기 위해 시그모이드 활성화 함수가 적용됩니다. 수학적으로 표현하면:</p>\n<p><img src=\"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_2.png\" alt=\"image\"></p>\n<div class=\"content-ad\"></div>\n<p>아래에서:</p>\n<ul>\n<li>Y: 출력 벡터</li>\n<li>X: 기능의 입력 벡터</li>\n<li>h: 숨겨진 상태</li>\n<li>V: 출력을 위한 가중 행렬</li>\n<li>U: 입력을 위한 가중 행렬</li>\n<li>W: 숨겨진 상태를 위한 가중 행렬</li>\n</ul>\n<p>V, U 및 W의 가중 행렬은 시간에 걸쳐 백프로파게이션을 통해 찾아집니다. 이는 백프로파게이션 알고리즘의 한 변형에 불과합니다.\n시각적으로는 이렇게 보입니다:</p>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_3.png\" alt=\"image\"></p>\n<p>For example, when predicting Y_1, the RNN would use the inputs of X_1 plus the output from the previous time step from Y_0. As Y_0 influences Y_1, we can then see that Y_0 will also indirectly influence Y_2, demonstrating the recurrent nature.</p>\n<p>If you want a full intro to RNNs then check out my previous blog.</p>\n<h1>Vanishing &#x26; Exploding Problem</h1>\n<div class=\"content-ad\"></div>\n<p>RNN의 긍정적인 측면 중 하나는 각 계층이 U, W 및 V의 가중 행렬을 공유하지만, 정규 피드포워드 신경망은 각 계층마다 자체 가중 행렬을 갖는다는 것입니다. 이로 인해 RNN은 더 메모리를 효율적으로 사용할 수 있습니다.</p>\n<p>그러나 이 가중 행렬 공유는 그들의 중요한 결함 중 하나인 사라지는 그래디언트와 폭주하는 그래디언트 문제를 야기합니다.</p>\n<p>RNN은 backpropagation through time (BPTT)라는 backpropagation 알고리즘의 변형을 사용하여 학습합니다. 이 알고리즘은 일반적인 backpropagation과 유사하지만, 각 시간 단계에서 업데이트해야 하는 계층 간의 공유 가중 행렬로 인해 더 '중첩된' 계산이 필요합니다.</p>\n<p>BPTT의 일반적인 공식은 다음과 같습니다:</p>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_4.png\" alt=\"image 1\"></p>\n<p>RNN에서 J는 임의의 가중치 행렬이며 U, W 또는 V일 수 있으며 E는 총 오차입니다.</p>\n<p>RNN은 일반적인 신경망보다 더 깊은 경향이 있습니다(각 시간 단계는 하나의 레이어입니다). 따라서 그래디언트가 1보다 작거나 큰 경우에는 역방향으로 전파될 때 그래디언트가 소멸되거나 폭발될 수 있습니다.</p>\n<p><img src=\"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_5.png\" alt=\"image 2\"></p>\n<div class=\"content-ad\"></div>\n<p>관심 있는 독자를 위해, 왜 이런 일이 발생하는지의 수학적인 전체 분석은 여기서 찾을 수 있어요. 이것은 고유값과 야코비안 행렬과 같은 재미있는 것들이 관련돼 있습니다!</p>\n<p>만약 역전파를 통한 시간 알고리즘 (BTTP)과 그라디언트가 사라지거나 폭발하는 문제의 전체적인 분석을 원한다면, 제 이전 게시물을 확인해보세요.</p>\n<p>사라지는 그라디언트와 폭발하는 그라디언트 문제를 잘 보여주는 좋은 예시는 Stanford의 CS224D 수업에서 제시되었습니다. 두 개의 문장을 생각해보세요:</p>\n<ul>\n<li>\"제인이 방으로 들어갔어요. 존도 들어왔어요. 제인이 ___에게 안녕했어요.\"</li>\n<li>\"제인이 방으로 들어갔어요. 존도 들어왔어요. 늦었고, 모두가 긴 하루 일과를 마치고 집으로 향했어요. 제인이 ___에게 안녕했어요.\"</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>어떤 경우에도 빈 공간은 아마도 존을 가리키는 것입니다. RNN은 이 맥락을 학습하여 두 문장 모두 출력이 존임을 이해해야 합니다.</p>\n<p>그러나 실험 결과, 문장 1이 문장 2보다 정확하게 예측되는 경향이 있었습니다. 이는 문장 2에서 그래디언트가 소멸되어 예측을 할 때 먼 맥락을 효율적으로 인식하지 못하기 때문입니다.</p>\n<p>이것은 분명히 문제입니다. RNN은 이와 같은 시나리오에 대한 \"기억\"을 갖도록 설계되었기 때문입니다.</p>\n<p>그래서, 이 문제에 대해 어떻게 해결할까요?</p>\n<div class=\"content-ad\"></div>\n<h1>롱-숏텀 메모리</h1>\n<h2>개요</h2>\n<p>LSTMs는 1997년 Hochreiter &#x26; Schmidhuber에 의해 소개되었으며, 그 기본 아이디어는 순환 셀 내부에 \"게이트\"가 있다는 것입니다. 이러한 게이트는 순환 셀이 장기 기억을 구축할 때 무엇을 기억하고 잊어야 하는지를 제어합니다.</p>\n<p>일반적인 RNN에서는 순환 셀이 다음과 같이 보입니다:</p>\n\n\n\n\n\n\n\n<table><thead><tr><th>Table</th><th>Tag</th></tr></thead></table>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_6.png\" alt=\"Long Short Term Memory - LSTM Improving RNNs\"></p>\n<p>However, the LSTM cell is a lot more complicated:</p>\n<p><img src=\"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_7.png\" alt=\"LSTM Cell\"></p>\n<p>I appreciate there is a lot going on here, but lets break it down step by step.</p>\n<div class=\"content-ad\"></div>\n<h2>셀 상태</h2>\n<p>핵심적인 차이 중 하나는 셀 상태 C의 도입입니다. 이 셀 상태에는 컨텍스트 및 과거 패턴과 같이 기본적인 정보가 포함되어 있습니다. 바로 메모리입니다. 이 셀 상태는 셀을 통과하며 선형 상호 작용을 하는 여러 개의 게이트에 의해 조정될 수 있습니다.</p>\n<p>셀 상태와 은닉 상태를 혼동하기 쉽지만, 일반적으로 셀 상태는 네트워크의 전체 메모리를 포함하도록 설계되었으며, 은닉 상태는 단기 의존성을 위해 사용되고 실제로 최근 정보만을 가지고 있습니다. 또한 예측을 위해 셀의 출력에 사용됩니다.</p>\n<h2>잊기 게이트</h2>\n<div class=\"content-ad\"></div>\n<p>LSTM의 첫 번째 단계는 forget gate입니다. 이 게이트는 이전 셀 상태 C_'t-1'에서 어떤 이전 정보를 삭제할지 결정하는 역할을 합니다.</p>\n<p><img src=\"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_8.png\" alt=\"이미지\"></p>\n<p>여기서:</p>\n<ul>\n<li>σ: 시그모이드 활성화 함수.</li>\n<li>W_f: forget gate의 가중치 행렬.</li>\n<li>h_'t−1': 이전 시간 단계의 출력.</li>\n<li>x_t: 시간 단계 t의 입력.</li>\n<li>b_f: forget gate의 편향.</li>\n<li>f_t: 0과 1 사이의 값을 가지는 forget gate 출력.</li>\n<li>X_t: 현재 입력.</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>출력 f_t는 이전 셀 상태 C_'t-1'에 곱해져 어떤 요소를 잊어야 하는지 수정합니다. 시그모이드 함수 덕분에 값은 0과 1 사이에 있으며, 0은 잊기를 의미하고 1은 기억에 추가됩니다.</p>\n<p>W_f 행렬에서 올바른 값을 찾아 역전파를 통해 이 정보를 학습합니다. 이를 통해 우리는 기억할지 잊을지를 결정할 수 있습니다.</p>\n<h2>입력 게이트 및 후보 셀 상태</h2>\n<p>입력 게이트 i_t는 다음 단계이지만 현재 시간 단계에서 셀 상태에 추가할 새로운 기억을 결정합니다. 후보 셀 상태 C*_t는 셀 상태에 추가할 가능한 모든 정보를 보유합니다.</p>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_9.png\" alt=\"image\"></p>\n<p>Where:</p>\n<ul>\n<li>σ: 시그모이드 활성화 함수.</li>\n<li>tanh⁡: 쌍곡 탄젠트 활성화 함수.</li>\n<li>W_i: 입력 게이트용 가중치 행렬.</li>\n<li>W_c: 후보 셀 상태용 가중치 행렬.</li>\n<li>b_i: 입력 게이트용 편향</li>\n<li>b_c: 후보 셀 상태용 편향.</li>\n<li>C*_t: 후보 셀 상태, -1과 1 사이의 출력 값.</li>\n<li>i_t: 0과 1 사이의 입력 게이트 출력.</li>\n<li>h_'t-1': 이전 숨은 상태.</li>\n<li>X_t: 현재 입력.</li>\n</ul>\n<p>tanh을 사용하면 셀 상태를 증가시키거나 감소시킬 수 있습니다. tanh는 출력을 -1과 1 사이로 압축하기 때문입니다. 시그모이드는 기억에 새로운 것을 추가하기 위해 이전 게이트와 유사한 이유로 사용됩니다.</p>\n<div class=\"content-ad\"></div>\n<p>입력 게이트는 특히 이전 셀 상태C_'t-1'입니다.</p>\n<h2>셀 상태 업데이트</h2>\n<p>우리는 후보 셀 상태 C*<em>t 에서 새로운 셀 상태 C_t로 관련 정보만 추가하려고 합니다. 이를 위해 후보 셀 상태를 입력 게이트 i_t 와 곱하고 이를 잊어버린 게이트 f_t 와 이전 셀 상태 C</em>'t-1' 의 곱과 더할 수 있습니다.</p>\n<p><img src=\"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_10.png\" alt=\"image\"></p>\n<div class=\"content-ad\"></div>\n<p>모든 작업은 좀 더 친숙한 느낌을 주려고 세포 상태를 갱신했습니다. 관련 없는 정보는 잊어버리고 필요한 새로운 정보를 추가했어요.</p>\n<h2>출력 게이트</h2>\n<p>마지막 단계는 셀에서 어떤 것을 예측으로 출력할지 결정하는 것이었지요. 먼저 출력 게이트 o_t를 계산하고, 이는 우리가 출력할 셀 상태의 어느 부분을 결정하는데 사용됩니다. 이는 기본적으로 일반 RNN의 일반적인 숨겨진 상태 반복 셀과 비슷한 역할을 해요.</p>\n<p>그 출력값은 새로운 셀 상태의 tanh 값과 곱해져서 원하는 값만을 출력합니다.</p>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-19-LongShortTermMemoryLSTMImprovingRNNs_11.png\" alt=\"image\"></p>\n<p>Where:</p>\n<ul>\n<li>σ: 시그모이드 함수.</li>\n<li>tanh: 쌍곡선 탄젠트 활성화 함수.</li>\n<li>W_o: 가중치 행렬.</li>\n<li>b_o: 편향.</li>\n<li>o_t: 출력 상태.</li>\n<li>h_t: 새로운 은닉 상태.</li>\n<li>h_'t-1': 이전 은닉 상태.</li>\n<li>X_t: 현재 입력.</li>\n<li>C_t: 새로운 셀 상태.</li>\n</ul>\n<p>그게 전부에요! 언급할 중요한 점은 모든 가중치 행렬이 BPTT를 사용하여 어떤 요소를 잊고 기억할지 학습해야 한다는 것입니다.</p>\n<div class=\"content-ad\"></div>\n<h1>변형</h1>\n<p>이것은 표준 LSTM일 뿐이고 다른 변형들이 있습니다. 그중 가장 흔한 것은 다음과 같습니다:</p>\n<ul>\n<li>양방향</li>\n<li>합성곱</li>\n<li>쌓인</li>\n<li>청구 접속</li>\n<li>게이트 순환 유닛</li>\n</ul>\n<p>이 글에서 이러한 모두를 다루는 것은 범위를 벗어나지만, 관심 있는 독자는 위에 제공된 링크에서 자세히 알아볼 수 있습니다. 다음 글에서는 게이트 순환 유닛에 대해 다룰 예정입니다.</p>\n<div class=\"content-ad\"></div>\n<h1>요약 및 더 깊은 생각</h1>\n<p>LSTMs는 처음에는 무서워 보일 수 있지만, 이 글을 통해 조금 더 친숙해졌으면 좋겠어요! 다양한 계산이 있지만, 이 모든 것들은 매우 유사합니다. 잊어버릴 것을 결정하는 forget gate와 기억에 추가할 새로운 정보를 결정하는 input gate 두 가지 기본 구성 요소가 있습니다. LSTMs의 장점은 이러한 gate들 덕분에 장기 기억력이 더 나은 것입니다.</p>\n<h1>추가로!</h1>\n<p>저는 '데이터 소스 공유', 매주 공유하는 더 나은 데이터 과학자가 되는 팁, 업계에서의 일반적인 경험, 지난 주에 한 생각들을 나누는 무료 뉴스레터를 운영하고 있어요.</p>\n<div class=\"content-ad\"></div>\n<h1>나와 소통해요!</h1>\n<ul>\n<li>링크드인, 트위터, 또는 인스타그램에서 연락해요.</li>\n<li>내 YouTube 채널에서는 기술적인 데이터 과학과 머신러닝 개념을 배우세요!</li>\n</ul>\n<h1>참고 자료</h1>\n<ul>\n<li>LSTM에 대한 훌륭한 블로그 포스트</li>\n<li>스탠포드 RNN 체트시트</li>\n<li>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. Aurélien Géron. 2019년 9월. 출판사: O'Reilly Media, Inc. ISBN: 9781492032649.</li>\n</ul>\n</body>\n</html>\n"},"__N_SSG":true}