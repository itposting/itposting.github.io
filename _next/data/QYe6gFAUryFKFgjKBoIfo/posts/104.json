{"pageProps":{"posts":[{"title":"임베딩 크기를 줄이고 RAG 검색 속도 높이는 방법","description":"","date":"2024-05-27 14:56","slug":"2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed","content":"\n<img src=\"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_0.png\" />\n\n# 소개\n\n텍스트 임베딩은 단일 단어나 전체 문장의 고차원 벡터 표현입니다.\n\n이 숫자 배열로 이루어진 벡터는 기본 텍스트에 대한 풍부한 정보를 포착함으로써 의미 이해, 분류, 군집화, 정보 검색 (RAG), 재정렬 및 더 많은 하류 작업에 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n보통 임베딩 벡터의 차원 d는 고정됩니다. 임베딩 차원은 일반적으로 64에서 4096까지의 2의 제곱수로 구성됩니다.\n\n매트료시카 임베딩을 사용하면 응용 프로그램에 따라 임베딩의 차원을 변경할 수 있습니다. 이를 통해 저장 공간을 줄이고 비용을 절약하며 검색 속도를 높일 수 있습니다.\n\n# 텍스트 임베딩이란?\n\n![이미지](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_1.png)\n\n<div class=\"content-ad\"></div>\n\n저희는 모든 가능한 입력 문자를 정수 값으로 매핑하는 어휘를 정의하여 시작합니다. 이 어휘에는 알파벳 문자 뿐만 아니라 특수 문자, 짧은 단어 및 하위 단어도 포함됩니다:\n\n```js\n{\n  \"a\": 1,\n  \"b\": 2,\n  \"c\": 3,\n  ...\n  \"z\": 26,\n  \"the\": 27,\n  \" \": 28\n}\n```\n\n토큰화 후에는 토큰 목록을 인코더 모델에 전달할 수 있습니다. 인코더는 대량의 훈련 데이터로부터 학습하여 각 토큰을 고차원 숫자 벡터 임베딩으로 변환합니다.\n\n예를 들어, OpenAI의 text-embedding-3-large 모델의 임베딩의 출력 차원 d는 3072입니다.\n\n<div class=\"content-ad\"></div>\n\n단일 문장 임베딩을 얻으려면 여러 토큰 임베딩에서 정보를 압축해야 합니다. 이를 위한 한 가지 방법은 단순히 모든 토큰 임베딩을 평균내는 것입니다.\n\n# 마트료시카 임베딩\n\n마트료시카 임베딩은 워싱턴 대학, 구글 리서치, 하버드 대학의 연구자들에 의해 2022년에 발표된 \"Matryoshka Representation Learning\" 논문에서 소개되었습니다.\n\n마트료시카 임베딩은 서로 다른 세기의 정보를 하나의 임베딩 벡터에 인코딩하는 데 훈련되었습니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, MRL을 사용하여 단순히 크기 d = 1024의 전체 임베딩 벡터를 학습하는 대신, 우리는 동일한 시간에 최적화하려는 손실 함수를 위해 matryoshka_dims = [1024,512,256,128,64] 차원 목록을 사용합니다[2].\n\n이렇게 하면 처음 몇 차원에 가장 덜 구체적인 정보가 저장되고 나중 차원에는 점점 더 많은 세부 정보가 저장된 임베딩 벡터가 생성됩니다.\n\n![이미지](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_2.png)\n\n이는 우리가 원하는 곳에서 임베딩 벡터를 잘라도 성능을 너무 많이 희생하지 않고 사용할 수 있다는 효과가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 왜 중요한가요?\n\n만약 우리가 텍스트 임베딩 벡터를 벡터 데이터베이스에 저장하려고 한다고 가정해봅시다. 각 임베딩은 d 차원을 가지고 있습니다. 그리고 각 숫자는 일반적으로 32비트 부동 소수점 수입니다. 그래서 우리는 저장을 위해 n _ d _ 4 바이트가 필요합니다.\n\n그리고 만약 우리가 점곱이나 코사인 유사성과 같은 유사성 지표를 계산하려고 한다면 (코사인 유사성은 단지 정규화된 점곱일 뿐입니다), 차원 d가 클수록 수학적 계산을 더 많이 해야 합니다.\n\n![image](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_3.png)\n\n<div class=\"content-ad\"></div>\n\nMRL을 사용하면 작은 메모리 공간, 빠른 처리 속도 및 따라서 비용 절약에 관심이 있다면, 첫 64차원만 사용할 수도 있습니다. 최상의 하류 성능을 원한다면 모든 차원을 사용합니다. 그리고 그 중간을 선택할 수도 있습니다.\n\n따라서, MRL은 LLM 사용자들에게 내려보기 성능의 작은 저하에 대한 임베딩 크기(비용)의 대가를 거래할 수 있는 능력을 제공합니다.\n\n# Nomic AI에서 MRL 사용하기\n\nNomic의 Matryoshka 텍스트 임베딩 모델 nomic-embed-text-v1.5은 matryoshka_dims = [768,512,256,128,64]로 훈련되었습니다. 해당 모델은 Hugging Face에서 공개적으로 사용할 수 있습니다 [3].\n\n<div class=\"content-ad\"></div>\n\n또 다른 이 인코더 모델의 멋진 기능은 다른 접두사를 지원한다는 것입니다. 이 모델은 [search_query, search_document, classification, clustering] 접두사를 지원하여 각 특정 하류 작업에 대해 더 나은 임베딩을 얻을 수 있습니다.\n\nnomic-embed-text-v1.5 모델이 Massive Text Embedding Benchmark (MTEB)에서 어떻게 수행되는지 살펴보겠습니다:\n\n![image](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_4.png)\n\nPython에서 PyTorch와 Sentence Transformers 라이브러리를 사용하여 모델을 구현해 봅시다:\n\n<div class=\"content-ad\"></div>\n\n\n!pip install torch sentence_transformers einops\n\n\n\nimport torch\nfrom sentence_transformers import SentenceTransformer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n\nmodel = SentenceTransformer(\n    \"nomic-ai/nomic-embed-text-v1.5\",\n    device=device,\n    trust_remote_code=True,\n    prompts={\n        \"search_query\": \"search_query: \",\n        \"search_document\": \"search_document: \",\n        \"classification\": \"classification: \",\n        \"clustering\": \"clustering: \",\n    },\n)\n\n\ndef embed_sentences(\n    model: SentenceTransformer,\n    sentences: list[str],\n    prompt_name: str,\n    matryoshka_dim: int,\n    device: str,\n):\n    assert matryoshka_dim <= 768, \"maximum dimension for nomic-embed-text-v1.5 is 768\"\n    embeddings = model.encode(\n        sentences, prompt_name=prompt_name, device=device, convert_to_tensor=True\n    )\n    embeddings = torch.nn.functional.layer_norm(\n        embeddings, normalized_shape=(embeddings.shape[1],)\n    )\n    embeddings = embeddings[:, :matryoshka_dim]\n    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n    return embeddings.cpu()\n\n\nmatryoshka_dim 매개변수를 사용하여 768차원 임베딩 벡터를 자릅니다. 그런 다음 새로운 임베딩 벡터를 정규화합니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n이제 원하는 차원을 설정하고 위키피디아 텍스트와 RAG(검색 증강 생성)용 쿼리를 인코딩할 수 있습니다.\n\n```js\nmatryoshka_dim = 64\n\nwikipedia_texts = [\n    \"개(Canis familiaris 또는 Canis lupus familiaris)는 늑대의 길들여진 후손입니다.\",\n    \"알베르트 아인슈타인은 1879년 3월 14일 독일 제국의 뷔르템베르크 왕국 울름에서 태어났습니다.\",\n    \"아인슈타인은 어린 시절부터 물리학과 수학에서 뛰어나며, 곧 같은 나이의 아이들만이 보유한 수학적 전문 지식을 습득했습니다.\",\n    \"베르너 칼 하이젠베르크는 독일의 이론 물리학자로 양자역학 이론의 주요 선구자 중 한 명이며, 제2차 세계대전 중 나치 핵무기 프로그램의 주요 과학자였습니다.\",\n    \"스티븐 폴 잡스(1955년 2월 24일 - 2011년 10월 5일)는 기술 거장 애플 주식회사를 공동 창업하여 가장 잘 알려진 미국 사업가, 발명가, 투자가였습니다.\",\n    \"고양이(Felis catus), 일반적으로 가정 고양이 또는 집고양이로 불리는 것은 고양이과에서 유일하게 길들인 종입니다.\",\n]\n\nquestion = [\"알베르트 아인슈타인은 어디에서 태어났나요?\"]\n\nquestion_embedding = embed_sentences(\n    model,\n    sentences=question,\n    prompt_name=\"search_query\",\n    matryoshka_dim=matryoshka_dim,\n    device=device,\n)\n\ndocument_embeddings = embed_sentences(\n    model,\n    sentences=wikipedia_texts,\n    prompt_name=\"search_document\",\n    matryoshka_dim=matryoshka_dim,\n    device=device,\n)\n```\n\n```js\nprint(f\"document_embeddings.shape: {document_embeddings.shape}\")\nprint(f\"question_embedding.shape:  {question_embedding.shape}\")\n>> document_embeddings.shape: torch.Size([6, 64])\n>> question_embedding.shape:  torch.Size([1, 64])\n```\n\n우리는 Matryoshka 텍스트 임베딩의 첫 번째 두 차원을 산포도로 시각화할 수 있습니다. 그러나 이 임베딩 모델은 명시적으로 2차원의 Matryoshka 차원에 최적화되지는 않았습니다.\n\n<div class=\"content-ad\"></div>\n\nmd\n![image](/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_5.png)\n\n다음으로, 문서 임베딩을 벡터 데이터베이스에 저장할 수 있습니다. 저는 Faiss를 사용하고 있어요. Faiss는 밀집 벡터의 효율적인 유사성 검색 및 클러스터링을 위한 Meta Research의 오픈 소스 라이브러리입니다 [4].\n\n```bash\n!pip install faiss-cpu\n```\n\n\n\n\n```python\nimport faiss\n\nindex = faiss.IndexFlatIP(matryoshka_dim)\nindex.add(document_embeddings)\n```\n\n\n\n<div class=\"content-ad\"></div>\n\n이 코드는 내적 제품을 사용하여 \"정확한 검색\"을 통해 벡터 데이터베이스를 만듭니다. 이때 IndexFlatIP를 사용하는데, 이는 내적 유사도 측정 방법입니다. 정규화된 임베딩을 사용하고 있기 때문에, 내적과 코사인 유사도는 동일합니다.\n\n이제 index는 여섯 개의 텍스트 임베딩으로 구성된 벡터 데이터베이스입니다:\n\n```js\nprint(index.ntotal)\n>> 6\n```\n\n질문과 가장 유사한 임베딩을 검색하고 상위 k개 결과를 검색해보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\ndistances, indices = index.search(question_embedding, k=6)\nprint(indices)\nprint(distances)\n>> [[1 2 3 4 0 5]]\n>> [[0.9633528  0.729192   0.63353264 0.62068397 0.512541   0.43155164]]\n```\n\n저희 데이터베이스에서 가장 유사한 텍스트는 인덱스 1이며 유사도 점수는 0.96입니다 (최대 점수는 1.0입니다).\n\n```js\n# d=64인 결과\nprint(question)\nprint(wikipedia_texts[1])\n>> ['알버트 아인슈타인은 어디에서 태어났나요?']\n>> '알버트 아인슈타인은 독일 제국의 퀴르템베르크 왕국 울름에서 1879년 3월 14일에 태어났습니다.'\n```\n\n저는 matryoshka_dim=768로 코드를 다시 실행했고 유사한 결과를 얻었습니다. 그러나 더 높은 차원은 더 많은 메모리와 계산이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n\n```js\n# 결과 d=768일 때\nprint(indices)\nprint(distances)\n>> [[1 2 4 3 0 5]]\n>> [[0.92466116 0.645744   0.54405797 0.54004824 0.39331824 0.37972206]]\n```\n\n# MRL 및 양자화\n\n더욱 압축된 임베딩을 원한다면, MRL과 이진 벡터 양자화를 함께 사용할 수 있습니다. 이진 양자화는 임베딩 벡터에서 0보다 큰 모든 숫자를 1로 변환하고 그 외의 숫자를 0으로 변환합니다 [5].\n\n<img src=\"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_6.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n이진 양자화를 사용하면 d 차원의 임베딩 벡터는 오직 d / 8 바이트의 메모리만 필요합니다. 이는 float32 형식의 d \\* 4 바이트와 비교해 크기가 32배로 줄어든 것을 의미합니다 [4]. 그러나 이 축소는 성능 저하와 함께 발생합니다.\n\n# 결론\n\nMatryoshka 손실을 사용하는 임베딩 모델은 훈련 중에 동시에 여러 임베딩 차원에 최적화되어 있습니다.\n\nMatryoshka 표현 학습을 사용하면 LLM 사용자가 텍스트 임베딩 크기를 작게 조정하여 성능 저하를 감수할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n더 작은 임베딩은 더 적은 메모리와 계산을 필요로하며, 이는 장기적으로 많은 비용을 절약할 수 있습니다. 또한 계산이 빨라져서 검색 속도가 빨라지기 때문에, 예를 들어 RAG 애플리케이션에 적합합니다.\n\n# 참고 자료\n\n[1] A. Kusupati 등. (2022), Matryoshka Representation Learning, arXiv:2205.13147\n\n[2] MatryoshkaLoss: https://www.sbert.net/docs/package_reference/losses.html#matryoshkaloss (접근일: 2024년 04월 05일)\n\n<div class=\"content-ad\"></div>\n\n[3] Hugging Face의 nomic-embed-text-v1.5: https://huggingface.co/nomic-ai/nomic-embed-text-v1.5 (접속일: 2024년 04월 05일)\n\n[4] Faiss 문서: https://github.com/facebookresearch/faiss/wiki/Getting-started (접속일: 2024년 04월 05일)\n\n[5] A. Shakir, T. Aarsen, S. Lee (2024), 바이너리 및 스칼라 임베딩 양자화로 훨씬 빠르고 저렴한 검색, Hugging Face 블로그\n","ogImage":{"url":"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_0.png"},"coverImage":"/assets/img/2024-05-27-HowtoReduceEmbeddingSizeandIncreaseRAGRetrievalSpeed_0.png","tag":["Tech"],"readingTime":9},{"title":"차이인차이 101","description":"","date":"2024-05-27 14:54","slug":"2024-05-27-Difference-in-Difference101","content":"\n차이인차이(DiD 또는 DD 또는 diff-in-diff)는 무엇인가요? 왜 차이인차이에 관심이 있나요? 오늘은 정책 효과를 연구하는 경제학에서 가장 인기 있는 방법 중 하나에 대한 모든 질문에 답할 거예요.\n\nDiD는 처리 그룹과 대조 그룹 간의 시간에 따른 결과 변화를 비교하여 인과 관계를 추정하는 널리 사용되는 경제학 기법입니다. 처리 그룹과 대조 그룹이 무엇인지에 대한 문제가 있어요. 처리는 정책 또는 변경로 인해 특정 그룹에 영향을 미치는 정책 개입을 말해요. 대조는 개입을 받지 않은 그룹을 말해요. 인과 관계란 원인과 결과의 관계를 의미해요.\n\n우리는 이 방법에 관심을 갖는 이유는 무작위 실험이 불가능한 경우에 정책 변경이나 개입의 효과를 평가하는 데 유용하기 때문이에요. 즉, 때때로 실험은 특정 그룹에 집중되므로 처리를 받은 사람들이 무작위가 아니라는 것을 의미해요. DiD는 무작위화 없이도 개입의 영향을 분리하는 데 도움이 될 거예요.\n\n<div class=\"content-ad\"></div>\n\n이 기사는 개념, 가정, 구현 및 예시 등에 대해 다룰 것입니다.\n\n# DiD란\n\n우리의 연구 질문은: 치료 D가 결과 y에 미치는 영향이 무엇인가요? DiD는 우리에게 치료 그룹이 개입되지 않았다면 치료 그룹에 무슨 일이 일어났을지를 추정할 수 있도록 해줍니다. 이 대역사적 시나리오는 치료의 실제 효과를 이해하는 데 중요합니다. 모든 직업이나 업무는 경제적 성장과 관련하여 세금 인하의 영향을 평가합니다. 또한 공공 정책 분야에서는 새 교통 법규가 사고 발생률에 미치는 영향을 평가합니다. 마케팅에서 DiD는 광고 캠페인이 매출에 미치는 영향을 분석합니다.\n\n![이미지](/assets/img/2024-05-27-Difference-in-Difference101_1.png)\n\n<div class=\"content-ad\"></div>\n\n위 다이어그램을 예로 들어보면 샘플에서 인구 데이터가 있습니다. 여기서는 처리군과 대조군으로 데이터를 나누고 처리군은 개입을 받았습니다. 두 그룹 모두 후기와 전기 변수를 관찰할 수 있습니다.\n\n# DiD 방법\n\n## 간단한 처리/대조 차이 추정기\n\n![DiD 다이어그램](/assets/img/2024-05-27-Difference-in-Difference101_2.png)\n\n<div class=\"content-ad\"></div>\n\n이 방정식은 치료와 대조 그룹 간의 시간 경과에 따른 결과 변화를 비교하여 치료 효과를 계산할 것입니다.\n\n수학을 이해하는 데 도움이 되기 위해 가짜 예제를 만들었습니다.\n\n![다면적 효과](/assets/img/2024-05-27-Difference-in-Difference101_3.png)\n\n위에서 언급한 공식을 사용하면 DiD 계수는 9가 될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## DiD Estimator: 회귀분석을 사용한 계산\n\nDiD는 처리 효과의 추정을 편향시킬 수 있는 시간 불변 특성을 제어하는 데 도움을 줍니다. 이는 시간이 지남에 따라 일정한 변수(예: 지리적 위치, 성별, 인종, 타고난 능력 등)의 영향을 제거한다는 것을 의미합니다. 그 이유는 이러한 특성이 각 그룹에 대해 전·후 처리 기간 모두 동일하게 영향을 미치기 때문입니다.\n\n기본 DiD 모델의 핵심 방정식은 다음과 같습니다:\n\n![DiD equation](/assets/img/2024-05-27-Difference-in-Difference101_4.png)\n\n<div class=\"content-ad\"></div>\n\n여기서:\n\n- y는 𝑡시간에 그룹 j의 개인 i의 결과 변수입니다.\n- 𝐴𝑓𝑡𝑒𝑟은 관측이 사후 처리 기간에 속하는 경우 1과 같은 더미 변수입니다.\n- 𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡은 관측이 처리 그룹에 속하는 경우 1과 같은 더미 변수입니다.\n- 𝐴𝑓𝑡𝑒𝑟 × 𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡은 상호 작용 항이며, 계수 β는 DiD 추정량을 캡처합니다.\n\n상호 작용 항의 계수는 y에서 DiD 추정량입니다. 회귀 분석은 추가 변수의 표준 오차를 제공하고 제어하는 데 도움이 되기 때문에 연구자들 사이에서 더 인기가 있습니다.\n\n# 평행 추세 가정\n\n<div class=\"content-ad\"></div>\n\nDiD에 대한 주요 가정 중 하나입니다. 이것은 치료가 없을 때 치료 그룹과 대조 그룹 간의 차이가 시간이 지남에 따라 일정하게 유지될 것이라는 생각에 기반을 두고 있습니다. 다시 말해, 치료가 없을 때 β (DiD 추정치)=0입니다.\n\n형식적으로, 이는 다음을 의미합니다:\n\n\n| Time        | Treatment Group | Control Group  | Difference   |\n|-------------|-----------------|----------------|--------------|\n| Before      | Y1              | Y0             | Y1 - Y0      |\n| After       | Y3              | Y2             | Y3 - Y2      |\n| DiD Estimate| (Y1 - Y0) - (Y3 - Y2)                    |\n\n\n또 다른 관점은 정책 변경이 없었을 때 두 그룹 간의 차이가 정책 변경 없이도 시간이 지남에 따라 동일하게 유지되었을 것이라는 것입니다. 치료 전에 추세가 평행하지 않으면 DiD 추정치가 편향될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 이 가정을 확인하는 방법\n\n그렇다면 다음 질문은: 어떻게 확인하는 걸까요? 평행 추세 가정의 타당성은 그래픽 분석과 플라시보 테스트를 통해 평가할 수 있습니다.\n\n![그림](/assets/img/2024-05-27-Difference-in-Difference101_6.png)\n\n이 가정은 치료가 적용되지 않은 경우 치료 그룹(주황색 선)과 대조 그룹(파란 점선)이 시간이 지남에 따라 평행한 경로를 따를 것이라는 것입니다. 치료(수직 선)는 치료가 적용되는 지점을 나타내며, 치료 전후에 두 그룹 간의 추세 차이를 비교하여 치료 효과를 추정할 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\n## 평행 추세 가정을 위반하는 예시\n\n간단히 말해서, 우리는 치료에서 두 가지를 찾습니다:\n\n- 기울기의 변화\n\n![image](/assets/img/2024-05-27-Difference-in-Difference101_7.png)\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-27-Difference-in-Difference101_8.png)\n\n위 두 경우 모두 평행한 추세 가정이 만족되지 않습니다. 치료 그룹 결과는 통제 그룹 결과보다 더 빨리 성장하거나(part a) 더 느리게 성장합니다(part b). 이를 수학적으로 표현하면 다음과 같습니다:\n\nDiD = 실제 효과 + 차이 추세 (차이 추세는 0이어야 함)\n\n차이 추세는 양수 (part a) 또는 음수 (part b)일 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\nDiD는 우리가 그것에 대한 차별적인 경향을 가지고 있기 때문에 개입의 영향을 분리할 수 없을 것입니다.\n\n2. 개입 이후의 치료 라인에서의 점프(상승 또는 하락)\n\n![image](/assets/img/2024-05-27-Difference-in-Difference101_9.png)\n\n위의 이미지에서 처리 그룹의 경향이 통제 그룹의 경향과 다르게 변경되어, 개입 없이 일정하게 유지되어야 했던 것과 다릅니다. DiD 연구에서는 점프가 허용되지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n# Placebo Tests\n\nPlacebo 테스트는 관측된 치료 효과가 실제로 치료로 인한 것이 아니라 다른 혼재 요소로 인한 것인지 확인하는 데 사용됩니다. 이는 치료 효과가 예상되지 않는 기간이나 그룹에 동일한 분석을 적용하는 것을 포함합니다. 이러한 플레이스보 테스트에서 유의한 효과가 발견되면, 원래 결과가 잘못된 것일 가능성이 있습니다.\n\n예를 들어, 2019년에 고등학교에 알약을 제공하는 개입 연구가 수행되었습니다. 우리는 플레이스보 테스트를 수행할 수 있어서 2017년이라는 가짜 개입 연도를 만들 수 있습니다. 여기서는 정책 변경이 발생하지 않았다는 것을 알고 있습니다. 플레이스보 날짜 (2017년)에 치료 효과 분석을 적용해도 유의미한 변화가 없는 경우, 2019년에 관측된 효과가 (있는 경우) 실제 정책 개입으로 인한 것일 가능성이 높습니다.\n\n# Extensions and Variations of DiD\n\n<div class=\"content-ad\"></div>\n\n- 이벤트 스터디 DiD: 연도별 처리 효과를 추정하여 처리 효과의 타이밍을 평가하고 사전 추세를 확인하는 데 유용합니다. 이 모델은 연도별 처리 효과를 다양하게 설정할 수 있습니다. 우리는 t+1, t+2, ..., t+n 시점에서 효과를 연구할 수 있습니다.\n- 합성 대조법(SCM): SCM은 여러 비치료 단위에 가중치를 부여하여 치료 전 특성을 근사하는 합성 대조 그룹을 구성합니다. 이 방법은 단일 처리 단위와 비치료 단위 집단을 비교할 때 특히 유용합니다. 여러 단위의 정보를 결합하여 믿을 수 있는 대조 사실을 제시합니다.\n\n이외에도 많은 방법이 있지만, 여기서는 두 가지만 소개하겠습니다. 나중에 더 자세히 설명하는 글을 쓸 수도 있겠네요.\n\n# 결론\n\n본 글에서는 평균 처리 효과를 추정하는 인기 있는 방법인 Difference-in-Differences (DiD) 추정기법을 분석했습니다. DiD는 처리 및 통제 그룹 간 시간 경과에 따른 변화를 비교함으로써 정책 효과를 연구하는 데 널리 사용됩니다. DiD의 주요 장점은 시간이 지남에 따라 일정하게 유지되는 관측되지 않은 혼입변수를 통제하여 개입의 실제 영향을 분리할 수 있는 능력입니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 평행 추세 가정, 전처리 데이터의 중요성, 그리고 시각적 분석과 장소보 테스트를 사용하여 가정 위반을 확인하는 방법과 같은 주요 개념을 탐구했습니다. 게다가, 이원차 차이-DiD의 확장과 변형인 이벤트 스터디 DiD 및 합성 통제 방법에 대해 이야기했는데, 이는 다양한 시나리오에서 추가 통찰력과 견고성을 제공합니다.\n\n# 참고문헌 및 추가 자료\n\n[1] Wing, C., Simon, K., & Bello-Gomez, R. A. (2018). 차이-DiD 연구 설계: 공공 보건 정책 연구를 위한 모범 사례. 공공 보건 연례 보고서, 39, 453–469.\n\n[2] Callaway, B., & Sant’Anna, P. H. (2021). 다중 시간 기간을 갖는 차이-차이 방법. 계량경제학 잡지, 225(2), 200–230.\n\n<div class=\"content-ad\"></div>\n\n[3] Donald, S. G., & Lang, K. (2007). Inference with difference-in-differences and other panel data. The review of Economics and Statistics, 89(2), 221–233.\n\n## 읽어 주셔서 감사합니다!\n\n읽어 주셔서 감사합니다! 🤗 만약 이 게시물을 즐겼고 더 많은 것을 보고 싶다면 팔로우해주세요. 또한 LinkedIn에서도 팔로우할 수 있습니다. 인과 추론 및 데이터 분석에 관한 블로그를 쓸 계획이며, 항상 단순하게 유지하려고 노력합니다.\n\n소소한 주의: 학습을 위해 쓰기 때문에 최선을 다하겠지만 실수가 발생할 수 있습니다. 오류를 발견하시면 알려주세요. 또한 새로운 주제에 대한 제안을 환영합니다!\n","ogImage":{"url":"/assets/img/2024-05-27-Difference-in-Difference101_0.png"},"coverImage":"/assets/img/2024-05-27-Difference-in-Difference101_0.png","tag":["Tech"],"readingTime":6},{"title":"Ollama를 사용하여 모델 실행하기 단계별 안내","description":"","date":"2024-05-27 14:51","slug":"2024-05-27-RunningmodelswithOllamastep-by-step","content":"\n\nLLM을 빠르게 테스트할 수 있는 방법을 찾고 계신가요? 전체 인프라를 설정할 필요 없이 테스트할 수 있는 방법이 있다면 정말 훌륭하죠. 이 짧은 기사에서 우리가 할 일이 바로 그거에요.\n\n![이미지](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png)\n\nOllama에 관해 경험이 있는 경우에는 특정 단락으로 이동해도 됩니다. 이 기사에서 찾을 수 있는 내용은 다음과 같아요:\n\n- Ollama가 무엇인가요?\n- Windows에 Ollama 설치하기.\n- Ollama [cmd] 실행하기.\n- 로컬로 모델 다운로드하기.\n- 다양한 용도에 맞는 다른 모델.\n- 모델 실행하기 [cmd].\n- CPU에 친화적인 양자화된 모델.\n- 다른 소스에서 모델 통합하기.\n- Ollama-파워드 (Python) 앱으로 개발자들의 삶을 더 쉽게 만들기.\n- 요약.\n\n<div class=\"content-ad\"></div>\n\n# 1. Ollama이란?\n\nOllama는 오픈 소스 코드로, 로컬에서 또는 본인의 서버에서 언어 모델과의 원활한 통합을 가능하게 하는 사용 준비 도구입니다. 이를 통해 상업용 API의 유료 버전을 사용하지 않아도 되므로, 특히 이제 Meta가 Llama2 모델을 상용으로 사용 가능하게 한 것을 고려하면, 자신의 데이터셋에서 추가 학습에 적합합니다.\n\n➡️ GitHub 저장소: https://github.com/ollama/ollama\n\n➡️ Ollama 공식 웹페이지: https://ollama.com\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_1.png)\n\n# 2. Windows에서 Ollama 설치하기\n\nOllama는 Windows, Mac 및 Linux에서도 원활하게 작동합니다. 이 간단한 자습서는 특히 Windows 10용 설치 단계를 안내합니다. 설치 후 프로그램은 약 384MB를 차지합니다. 그러나 다운로드한 모델이 가벼운 것은 아닐 수 있습니다.\n\n만약 도커 컨테이너에서 Ollama를 실행하길 원한다면, 아래 설명을 건너뛰고 \n\n감십시오.\n\n<div class=\"content-ad\"></div>\n\n➡️ https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image\n\n![Running Models with Ollama](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_2.png)\n\n➡️ Ollama 홈페이지로 이동하여 .exe 파일을 다운로드하세요: https://ollama.com\n\n![Running Models with Ollama](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_3.png)\n\n<div class=\"content-ad\"></div>\n\nOllama를 다운로드하고 Windows에 설치하세요. 보통 다음 경로에 위치한 기본 모델 저장 경로를 사용할 수 있습니다:\n\n```js\nC:\\Users\\your_user\\.ollama\n```\n\n그러나 C: 파티션에 공간이 제한적이라면, 대안 디렉토리로 전환하는 것이 권장됩니다. D:\\와 같은 다른 파티션이 있는 경우, 간단하게:\n\n- 데스크탑의 컴퓨터 아이콘을 마우스 오른쪽 클릭합니다.\n- 속성을 선택한 후 \"고급 시스템 설정\"으로 이동합니다.\n- 환경 변수를 클릭합니다.\n- ...을 위한 사용자 변수에서 모델을 저장할 디렉토리의 절대 경로를 삽입하십시오. 예를 들면:\n\n<div class=\"content-ad\"></div>\n\n```js\n변수: OLLAMA_MODELS\n값: D:\\your_directory\\models\n```\n\nOLLAMA_MODELS 변수의 이름을 변경하지 마십시오. 이 변수는 Ollama가 정확히 아래와 같이 검색할 것입니다.\n\nWindows의 하단 표시줄에 Ollama 아이콘이 나타납니다. 프로그램이 시작되지 않으면 Windows 프로그램에서 찾아서 거기서 시작하십시오.\n\n<img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_4.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n이제 Ollama를 실행하고 모델을 다운로드할 준비가 되었어요 :)\n\n# 3. Ollama 실행하기 [cmd]\n\n![image](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_5.png)\n\nOllama를 설정하고 나면 윈도우에서 cmd(명령줄)를 열고 로컬로 일부 모델을 다운로드할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\nOllama 로컬 대시보드를 사용하려면 웹 브라우저에서 다음 URL을 입력하세요:\n\n```js\nhttp://localhost:11434/api/\n```\n\nOllama를 실행하는 것은 그렇게 어렵지 않습니다. 나중에 CMD 및 Python 코드를 통해 어떻게 활용하는지 알아보겠습니다.\n\n중요한 몇 가지 명령어:\n\n<div class=\"content-ad\"></div>\n\n로컬로 사용 가능한 모델을 확인하려면 다음을 cmd에 입력하세요:\n\n```js\nollama list\n```\n\n특정 모델에 해당하는 SHA 파일을 확인하려면 cmd에 입력하세요 (예: llama2:7b 모델 확인을 위한 예시):\n\n```js\nollama show --modelfile llama2:7b\n```\n\n<div class=\"content-ad\"></div>\n\n모델을 제거하려면:\n\n```js\nollama rm llama2:7b\n```\n\n모델을 서버에 올리려면:\n\n```js\nollama serve\n```\n\n<div class=\"content-ad\"></div>\n\n# 4. 모델을 로컬로 다운로드하기\n\n웹사이트 ➡️ https://ollama.com/library 에서는 여러 다양한 파라미터 크기로 제공되는 다수의 모델을 다운로드할 수 있습니다.\n\n로컬로 모델을 다운로드하기 전에, 해당 모델을 로딩할 충분한 메모리를 가지고 있는지 확인해주세요. 테스트할 때는 애플리케이션에 통합하기에 적합한 작은 모델인 '7B'로 레이블이 지정된 모델을 사용하는 것이 좋습니다.\n\n⚠️ 부드러운 모델 작동을 위해 적어도 하나의 GPU를 보유하는 것이 강력하게 권장됩니다.\n\n<div class=\"content-ad\"></div>\n\n아래에는 내가 테스트하고 추천하는 여러 모델이 있습니다. 명령을 복사하여 명령 프롬프트에 붙여넣어 지정된 모델을 로컬로 가져올 수 있습니다.\n\n👉Meta에서의 Llama2 모델\n\n대화 시나리오를 위해 최적화된 생성 텍스트 모델 세트입니다. Ollama의 많은 모델과 마찬가지로 Llama2는 다양한 구성으로 제공됩니다:\n\n![image](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_6.png)\n\n<div class=\"content-ad\"></div>\n\n아래는 해당 모델을 가져오는 몇 가지 예시입니다:\n\n표준 모델:\n\n```js\nollama pull llama2\n```\n\n검열되지 않은 버전:\n\n<div class=\"content-ad\"></div>\n\n```js\nollama pull llama2-비겁하지 않은:7b\n```\n\n채팅 7B 모델:\n\n```js\nollama pull llama2:7b-채팅\n```\n\n➡️ 더 읽기: https://llama.meta.com/llama2\n\n\n<div class=\"content-ad\"></div>\n\n👉 구글의 젬마\n\n주요 7B 크기 모델과 유사한 견고한 성능을 제공하는 오픈 소스 모델입니다.\n\n```js\nollama pull gemma:7b\n```\n\n➡️ 자세히 보기: https://blog.google/technology/developers/gemma-open-models/\n\n<div class=\"content-ad\"></div>\n\n👉 Haotian Liu 등의 LLava.\n\n이미지에서 텍스트 설명을 다루는 데 뛰어나며 시각 및 언어 모델 모두에 대한 견고한 지원을 제공하는 멀티모달 모델입니다.\n\n```js\nollama pull llava\n```\n\n➡️ 자세히 알아보기: https://llava-vl.github.io/\n\n<div class=\"content-ad\"></div>\n\n5. 서로 다른 목적을 위한 다양한 모델\n\n![이미지](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_7.png)\n\n일부 모델은 특정 데이터셋에서 훈련되어 코드 완성, 대화 또는 이미지에서 텍스트로 변환과 같은 특정 작업에 더 적합합니다. Ollama에서는 다양한 목적을 위해 설계된 모델을 찾을 수 있습니다.\n\n첫 번째 그룹은 대화, 텍스트 완성, 요약 등을 용이하게 하는 데 초점을 맞춘 모델을 포함하고 있습니다. Gemma, Llama2, Falcon 또는 OpenChat과 같은 모델이 포함됩니다.\n\n<div class=\"content-ad\"></div>\n\n일부 예시:\n\n- [Falcon](https://ollama.com/library/falcon)\n\n- [Gemma](https://ollama.com/library/gemma)\n\n- [Openchat](https://ollama.com/library/openchat)\n\n<div class=\"content-ad\"></div>\n\n다음 그룹은 대화를 나누거나 챗봇 역할을 하는 다중 모달 모델과 이미지 설명(시각 모델), 텍스트 요약, 질문-답변(Q/A) 애플리케이션을 구동할 수 있는 모델들로 구성됩니다.\n\n일부 예시:\n\n➡️ https://ollama.com/library/llava\n\n➡️ https://ollama.com/library/bakllava\n\n<div class=\"content-ad\"></div>\n\n마지막으로, 매우 전문화된 그룹은 Ollama에서 이용 가능한 모델을 활용하여 개발자의 작업을 지원합니다. 코델라마, 돌핀-미스트랄, 돌핀-믹스트랄(코딩 작업에 능숙한 Mixtral 전문가 모델을 기반으로 세밀하게 조정된 모델)과 같은 모델들이 있으며, 계속해서 크리에이터들이 추가하고 있습니다.\n\n몇 가지 예시:\n\n➡️ https://ollama.com/library/codellama\n\n➡️ https://ollama.com/library/dolphin-mistral\n\n<div class=\"content-ad\"></div>\n\n➡️ https://ollama.com/library/dolphin-mixtral\n\n# 6. 모델 실행하기 [cmd]\n\n다운로드한 모델을 실행하려면, ollama run 모델이름:파라미터 \"당신의 프롬프트\"를 입력하세요. 예를 들어:\n\n```js\nollama run llama2:7b \"당신의 프롬프트\"\n```\n\n<div class=\"content-ad\"></div>\n\n다중 모달 모델을 사용하면 기본 프롬프트를 벗어난 파일, 로컬 이미지 경로 등을 포함할 수 있어 더 많은 기능을 확장할 수 있습니다.\n\n# 6. CPU 친화적 양자화 모델\n\n양자화는 모델의 정밀도를 유지하는 비용을 줄이는 것으로 관련 비용을 줄이는 것입니다. 이 과정 뒤에 숨은 직관력을 구축하는 데 도움이 되는 이 크고 훌륭한 기사에서 자세한 설명을 찾아볼 수 있습니다:\n\n📃 양자화 LLMs란 무엇인가? (Miguel Carreira Neves의 글):\n\n<div class=\"content-ad\"></div>\n\n➡️ https://www.tensorops.ai/post/what-are-quantized-llms\n\n추가 자료:\n\n📃 Extreme Compression of Large Language Models via Additive Quantization:\n\n➡️ https://arxiv.org/html/2401.06118v2\n\n<div class=\"content-ad\"></div>\n\n📃 SmoothQuant: 대형 언어 모델을 위한 정확하고 효율적인 사후 훈련 양자화:\n\n➡️ [논문 링크](https://arxiv.org/pdf/2211.10438.pdf)\n\n📃 BiLLM: LLMs를 위한 사후 훈련 양자화 한계 돌파:\n\n➡️ [논문 링크](https://arxiv.org/pdf/2402.04291.pdf)\n\n<div class=\"content-ad\"></div>\n\n간단하게 말하면, 양자화는 가중치 정밀도를 조정하여 모델 크기를 줄이고 중요한 정확도 하락 없이 성능을 쉽게 감소시킬 수 있는 하드웨어에서 실행할 수 있게 해줍니다.\n\n이 글과 함께 제공된 이미지를 통해 양자화 후에 모델이 원래 버전보다 상당히 적은 공간을 차지하는 것을 확인할 수 있습니다:\n\n![Quantized Models](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_8.png)\n\nOllama는 양자화된 모델을 지원하여 별도로 처리하는 부담을 덜어줍니다.\n\n<div class=\"content-ad\"></div>\n\n# 7. 다른 소스에서 모델 통합하기\n\n![Running Models With Ollama Step-by-Step](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_9.png)\n\nOllama의 모델은 다양성을 제공하지만 현재 모든 모델에 액세스할 수 있는 것은 아닙니다. 그러나 로컬에 직접 모델을 통합하는 것은 간단한 프로세스입니다. 새로운 모델을 지역 Ollama에 통합하는 방법을 알아봅시다.\n\nThe Bloke의 HuggingFace 계정에서 많은 양자화된 모델을 사용할 수 있습니다. 의학 논문을 위해서 우리는 편리하게 medicine-chat-GGUF 모델을 선택할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n➡️ https://huggingface.co/TheBloke/medicine-chat-GGUF\n\n해당 링크를 열고 파일 및 버전을 클릭하세요.\n\n![Files and versions](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_10.png)\n\nOllama 모델에 포함하고 싶은 모델을 다운로드하세요:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_11.png\" />\n\nModelfile이라는 빈 파일을 생성하고 아래 지정된 데이터를 삽입하세요 (저장된 모델의 절대 경로로 경로를 대체하십시오). 이 예제는 기본적이며, 모델의 온도, 시스템 메시지 등과 같은 여러 옵션을 포함하여 확장될 수 있습니다. 필요한 경우 파일에서 '#'를 제거하여 해당 옵션을 활성화하세요.\n\n```js\nFROM D:\\...\\medicine-chat.Q4_0.gguf\n# PARAMETER 온도 0.6\n# SYSTEM \"\"\"도움이 되는 의학 조수입니다.\"\"\"\n```\n\nModelfile을 저장한 후, cmd에 다음을 입력하세요:\n\n<div class=\"content-ad\"></div>\n\n```bash\nollama create 모델_이름 -f 모델_파일\n```\n\n# 9. Ollama를 활용한 (Python) 앱으로 개발자의 삶을 더 쉽게 만들기\n\n백그라운드에서 실행되는 Ollama는 일반적인 REST API와 같이 접근할 수 있습니다. 따라서 requests와 같은 라이브러리 또는 조금 더 발전된 FastAPI, Flask 또는 Django와 같은 프레임워크를 사용하여 응용 프로그램에 쉽게 통합할 수 있습니다.\n\nOllama python 패키지를 쉽게 pip를 통해 설치하세요.\n\n<div class=\"content-ad\"></div>\n\n⬆️ https://pypi.org/project/ollama/0.1.3:\n\n```js\npip install ollama\n```\n\nPython 코드를 통해 임베딩을 생성하는 방법:\n\n```js\nimport ollama\n\nembedding = ollama.embeddings(model=\"llama2:7b\", prompt=\"Hello Ollama!\")\n```\n\n<div class=\"content-ad\"></div>\n\n간단히 CURL을 사용하여:\n\n```js\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"llama2:7b\",\n  \"prompt\": \"Here is an article about llamas...\"\n}'\n```\n\nOllama 엔드포인트에 대해 더 알아보려면 다음 링크를 방문해주세요:\n\n➡️ https://github.com/ollama/ollama/blob/main/docs/api.md\n\n<div class=\"content-ad\"></div>\n\nOllama가 Langchain 프레임워크에 원활하게 통합되어 개발 노력을 최적화하고 기술 측면의 작업을 더욱 간편하게 만들었습니다:\n\n➡️ https://python.langchain.com/docs/integrations/llms/ollama\n\n임베딩을 만드는 간단함을 감상해보세요:\n\n```js\n# pip install langchain_community\nfrom langchain_community.embeddings import OllamaEmbeddings\n\n\nembed = OllamaEmbeddings(model=\"llama2:7b\")\nembedding = embed.embed_query(\"Hello Ollama!\")\n```\n\n<div class=\"content-ad\"></div>\n\n# 10. 요약\n\n본 기사는 당신을 Ollama를 사용하여 모델을 실행하는 과정을 단계별로 안내하여, 전체 인프라 구성 없이 LLM을 테스트할 수 있는 원활한 방법을 제공합니다.\n\n올라마는 오픈 소스 도구로, Meta의 Llama2 모델을 무료로 사용할 수 있게 해주는 로컬 또는 서버 기반의 언어 모델 통합을 용이하게 합니다. 윈도우에서의 설치 과정과 명령줄을 통해 Ollama를 실행하는 방법에 대해 설명되어 있습니다.\n\n이 기사에서는 모델 다운로드, 특정 작업을 위한 다양한 모델 옵션, 다양한 명령어를 사용하여 모델 실행, CPU 친화적인 양자화된 모델, 그리고 외부 모델 통합에 대해 탐구합니다. 또한, 개발자들을 위해 Ollama를 활용한 파이썬 애플리케이션을 강조하고 있습니다.","ogImage":{"url":"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png"},"coverImage":"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png","tag":["Tech"],"readingTime":10},{"title":"AI가 다물고 보니 연기와 거울일까요","description":"","date":"2024-05-27 14:49","slug":"2024-05-27-HowDoWeKnowifAIIsSmokeandMirrors","content":"\n\n## \"AI 혁명\"이 인쇄 기계인지, 암호폐인지에 대한 사려 (스포일러: 둘 다 아님)\n\n![image](/assets/img/2024-05-27-HowDoWeKnowifAIIsSmokeandMirrors_0.png)\n\n나는 AI의 출현이 우리 세계에 미치는 의미에 대해 진지하게 고민한 첫 번째 사람은 아닙니다. 하지만 아직도 이 질문이 계속해서 제기되고 토론되고 있다는 것은 알 수 있습니다. 그러나 대부분의 이런 대화들은 관련 핵심 요인들을 빼먹는 것으로 보입니다.\n\n시작하기 전에, 최근에 나의 사고를 형성시킨 이 문제의 다양한 측면을 보여주는 3가지 일화를 알려드리겠습니다.\n\n<div class=\"content-ad\"></div>\n\n- 최근 금융 컨설턴트와 대화를 나누었어요. 그는 그의 기관의 임원들이 AI가 경제 장면에서 실질적인 변화라고 말하며, 투자 전략은 혁명적으로 취급해야 한다는 조언을 전했다고 언급했어요. 그는 머신러닝 산업의 실무자로서 내 생각을 알고 싶어했죠. 나는 친구들과 독자들에게 이전에 말해왔던 대로, 지나치게 과대 퍼징된 혹은 진짜 가치를 알아보려고 기다리고 있다고 말했어요. 혹은 단지 허무주의 사이클이나 순식간의 현상이 아니다. 이 혹은 사이클은 아직도 진행 중이에요.\n\n- 이번 주에는 'Tech Won’t Save Us'의 에피소드를 듣았어요. 거기서는 기술 저널리즘과 Kara Swisher에 관한 이야기가 있었어요. 게스트인 Edward Ongweso Jr.는 Swisher가 새로운 기술에 대해 신중하지 않고, 그 새로운 기술이 약속한 것만큼 인상적이거나 혁명적이 아니라는 것이 입증되면 태도를 변경한다고 말했죠(자율 주행 자동차, 암호화폐 등을 참조). 그는 이번에도 그녀와 같은 현상이 AI에 대해 일어나고 있다고 생각했어요.\n\n- 제 동반자와 저는 둘 다 기술 분야에서 일하고 있어서 주기적으로 기술 뉴스에 대해 이야기를 나눠요. 그는 한 번 어떤 전문가나 기술 사상가가 자신이 잘 알지 못하는 주제에 대해 매우 현명한 통찰을 보여주면서 멋지다고 한 적이 있다고 언급했어요. 그러나 그들이 자신의 전문 분야에 대해 이야기하기 시작하면 갑자기 그들의 판단이 틀렸다는 것을 깨닫게 된다고 했어요. 그때 \"이 부분에 대해 틀렸다는 것을 알고 있어요. 그들이 저것에 대해서도 틀렸을까요?\"라고 고민할 때가 있다고 해요. 요즘 가끔씩 머신러닝에 관한 이런 경험을 하고 있어요.\n\n새로운 기술들이 어떻게 안착할지, 그리고 그들이 우리 사회에 미칠 장기적인 영향이 어떻게 될지 정말 어려워요. 역사학자들은 \"이 이벤트가 발생할 수 있는 유일한 방법이 '이것'이었을 것이다\"라고 가정하고 돌이킬 수 없이 계속 나아간 것처럼 보이지만, 실제로는 현재에는 누구도 그 다음에 무슨 일이 일어날지 알지 못하며, 일어날 수 있는 많은 사건이 있었으며, 그것이 최종적으로 실제로 일어난 것보다 더 나거나 덜 가능한 변화를 가져올 수 있었어요.\n\n# 요약\n\nAI는 완전한 사기가 아니에요. 머신러닝은 정말 복잡한 작업을 자동화하고 효과적으로 확장할 수 있는 기회를 제공해줘요. AI가 우리 세계와 경제 전반에 모든 것을 바꿀 것이라는 것은 아니에요. 그것은 도구이지만, 대부분의 경우 인간 노동을 대체하지는 않을 거에요. 그리고 AGI가 현실적인 전망이 아니에요.\n\n<div class=\"content-ad\"></div>\n\n왜 말하느냐면, 설명해 드릴게요.\n\n먼저, 기계 학습은 정말 대단한 기술이에요. 사람들이 직접 파악하기에 너무 복잡한 패턴의 미묘한 점들을 기계에 가르치는 것은 매력적이라고 생각해요. 그리고 이를 통해 컴퓨터가 문제를 해결할 수 있는 다양한 기회가 열리는 것을 만들어내는데 도움이 된다고 생각해요. 기계 학습은 이미 우리 삶에 다양한 방식으로 영향을 미치고 있으며, 수년간 그 영향을 주고 있어요. 사람에겐 지루하거나 거의 불가능한 작업을 완료할 수 있는 모델을 개발하고, 그것이 동료들의 문제를 해결하는 데 사용될 때, 정말 만족스러워요. 이것은 생성적 인공지능 영역에서 이루어지는 혁신적인 일들 중 한 가지 아주 작은 단위에 불과하지만, 넓은 우산 아래에서 이루어지고 있어요.\n\n# 기대\n\n일반 대중과 기계 학습 전문가들에게 AI가 의미하는 것에 대해 이야기하면 두 그림이 매우 다르게 나올 거에요. 이에 대해 이전에 글을 썼지만, 여기서 다시 언급할 필요가 있어요. 우리는 AI에 어떤 것을 기대하고 있나요? 우리가 \"인공지능\"이라는 용어를 사용할 때 어떤 의미를 부여하고 있나요?\n\n<div class=\"content-ad\"></div>\n\n나에게 있어 인공지능은 기본적으로 \"머신 러닝 모델을 사용하여 작업을 자동화하는 것\"입니다. 그 이상도 이하도 아냐. 머신 러닝 모델이 매우 복잡하다면 어떤 복잡한 작업을 자동화할 수 있지만, 비교적 한정된 작업을 하는 작은 모델들도 여전히 이에 포함됩니다. 머신 럽엥 모델이 정말 무엇을 하는지에 대해 길게 쓰긴 했지만, 간단히 말하면: 데이터에서 패턴을 수학적으로 분석하고 재현하는 것이죠. 그러니까, 우리는 데이터의 패턴을 수학적으로 나타낸 것을 사용하여 작업을 자동화하는 것입니다. 인공지능은 우리가 기록된 역사의 사건 패턴을 바탕으로 다음 단계를 선택하는 것이다, 사람들이 쓴 텍스트의 역사, 주택 가격의 역사 또는 그 외 다른 모든 것의 역사일지라도 말이에요.\n\n그러나 많은 사람들에게는 인공지능은 더 복잡한 것을 의미하는 경우가 많아요, 약간과학 판타지 수준에서 이해될 때도 있어요. 경우에 따라서 인공지능과 AGI(인공일반지능) 사이의 선을 흐리게 만들기도 하는데, 이것은 우리의 대화에서도 제대로 정의되지 않은 부분이 있죠. 종종 사람들 스스로도 이 용어가 정확히 무엇을 의미하는지 모르겠다고 생각하지만, 현실이 제공하는 것보다 훨씬 더 정교하고 보편적인 것을 예상하는 느낌을 받아요.\n\n예를 들어, LLM은 인간 언어의 구문과 문법을 이해하지만 실질적인 의미 개념은 갖고 있지 않아요. LLM이 아는 모든 것은 내부적 연관성에 의해 정해져요 — \"왕\"은 LLM에게는 \"여왕\"이나 \"남자\"와 같은 다른 단어와의 관계에 의해 정의됩니다. 그래서 언어나 의미 문제를 돕기 위한 모델이 필요하다면 그건 괜찮아요. 동의어를 요구하거나 특정 주제와 관련된 단어들로 가득한 단락을 축적하도록 요청하더라도, 그런 모델은 잘 할 거예요.\n\n하지만 이것과 \"지식\" 사이에는 뚜렷한 차이가 있어요. 바위를 던져보면 ChatGPT가 사실을 제대로 이해하지 못하고 항상 환각한다고 하는 소셜 미디어 쓰레드를 찾을 수 있어요. ChatGPT는 결코 \"사실을 생산하는 로봇\"이 아니에요. 그저 큰 언어 모델일 뿐이에요. 언어를 다룹니다. 지식은 실제 사실 이상의 것이며, 사실이 무엇을 의미하는지 이해하는 엔티티를 의미하며 더 많은 것을 포함하고 있어요. 현재의 방법론과 우리에게 제공되는 기술을 사용해서 머신 러닝 모델이 이 수준에 이르는 것에 대한 위험은 없어요. 몇몇 사람들이 현재의 방법과 기술을 사용하면 \"AGI\"라고 불리는 것에 도달할 것이라 지적하는 것들이 있지만 말이죠.\n\n<div class=\"content-ad\"></div>\n\n만약 사람들이 ChatGPT를 보고 AGI를 원한다면, 정보나 현실에 대한 이해력이 사람들과 동등하거나 더 뛰어난 기계 학습 모델 형태를 원하는 것은 완전히 현실적이지 않은 기대일 것입니다. (참고: 이 산업 분야에서는 AGI의 임박한 도래를 허세 부리며 PR에 강조하지만, 압박 당하면 AGI의 정의를 현실적인 것으로 크게 변경하여 자신들의 과대광고에 대한 책임을 피하려고 할 것입니다.)\n\nAI가 혁명적일 것이라는 나의 회의에 직면해서, 나의 금융 자문가는 fast food 레스토랑이 차 드라이브스루에서 음성 인식 AI로 전환하여 고객들이 자동차 안에서 하는 말을 이해하지 못하는 인간 작업자로 인한 문제를 줄이는 사례를 언급했습니다. 이것은 흥미로울 수 있지만, 전혀 깨달음적인 것은 아닙니다. 이것은 사람들이 일을 조금 더 잘할 수 있도록 도와주는 도구로써의 기계 학습 모델입니다. 이것은 생성적 AI 세계에만 해당되는 것은 아닙니다! 우리는 10년 이상 동안 기계 학습을 사용하여 작업을 자동화하고 인간 노동을 줄이고 있으며, LLMs를 추가하는 것은 정도의 차이이며, 급격한 변화가 아닙니다.\n\n내가 말하려는 바는 기계 학습을 사용함으로써 우리가 많은 일을 수행하는 속도와 효율성에서 점진적인 개선을 얻을 수 있다는 것이며, 우리의 기대는 이러한 모델이 무엇인지와 그들이 아닌 것을 실제로 이해하고 그에 따라 형성되어야 합니다.\n\n# 실용적인 한계\n\n<div class=\"content-ad\"></div>\n\n아마 여러분은 내 첫 번째 주장이 현재 모델 훈련을 위한 기술 능력과 오늘 사용되는 방법에 기반한다는 것을 생각하실 수도 있고, 그것은 합리적인 의견입니다. 훈련과 기술을 계속 발전시켜 더 복잡한 생성 모델을 만들어내는 경우, 아주 새로운 것이 만들어질 수 있는 시점에 도달할까요? 우리가 더 이상 \"AGI\"라고 말하는 것이 만들어질 수 있는 시점에 도달할까요? 하늘은 끝이 없는 범위가 아니라고요?\n\n문제에 대한 솔루션으로 기계 학습의 잠재력은 그 잠재력을 실현할 수 있는 능력과 매우 다릅니다. 무한한 자원 (돈, 전기, 칩용 희귀 희토류, 훈련을 위한 인간 생성 콘텐츠 등)이 있다면, 기계 학습으로부터 얻을 수 있는 한 가지 수준의 패턴 표현이 있을 수 있습니다. 그러나 실제로 우리가 살고 있는 현실 세계에서 이러한 모든 자원은 상당히 한정적이며, 이들의 한계에 마주치고 있습니다.\n\n우리는 이미 수 년간 LLM을 훈련할 품질 데이터가 부족하다는 것을 알고 있으며, 생성된 데이터를 훈련 데이터로 다시 사용하는 시도는 매우 문제가 많습니다. (생성된 다른 AI의 출력에서 심각히 훈련받은 \"하브스부르크 AI\" 또는 \"비정상적이고 기이한 특징을 갖춘 근원적인 돌연변이와 같이 될 수 있는 시스템을 발명한 Jathan Sadowski에게 감사를 표합니다) 경우에 따라 생성된 데이터와 유기적 데이터를 구별하는 역량이 부족하다는 점을 언급하는 것도 중요하다고 생각합니다. 이러한 이유로 우리가 하브스부르크 AI를 만들고 있다는 것을 알 수도 없을 수 있으며, 그렇게 되는 과정에서 나타나는 퇴화 현상은 우리에게 점진적으로 다가올 수 있습니다.\n\n오늘은 돈/에너지/금속의 제약 사항에 대해 논의하는 것을 건너뛸 것이며 AI의 자연 자원 및 에너지 영향에 대해 다른 글을 계획 중이기 때문입니다. 하지만 전력 소비에 대한 논의를 위해 Verge로 이동해보세요. 우리는 모두 에너지가 무한한 자원이 아님을 알고 있습니다, 심지어 재생 가능 에너지도 마찬가지이며, 우리는 이미 소규모 국가에 해당하는 전기 소비를 기계 모델 훈련에 투입하고 있습니다 - AI 거래꾼들이 과대 광고한 것에 접근하지 못하는 모델입니다.\n\n<div class=\"content-ad\"></div>\n\n나는 AI 기업들이 직면하는 규제 및 법적 도전들이 가능성이 있다고 생각해요. 이전에도 썼었지만, 이 문제는 그들이 할 수 있는 일에 제한을 둘 수밖에 없을 거에요. 어떤 기관도 법 위에 있어서나 제한 없이 존재해서는 안 되고, 지구의 자연 자원을 모두 낭비하여 AGI를 만들려고 하는 것은 혐오스럽다고 생각해요.\n\n내 의견은 이론적으로 어떤 일을 할 수 있다는 것과 실제로 우리가 할 수 있는 일이 같다는 건 아니다라는 거에요. 무한한 은행 계좌, 광물 광산 및 데이터 소스가 있다고 해도 머신 러닝이 이러한 제약 없이 AGI를 달성할 수 있다고 믿지 않아요. 일부는 훈련을 수행하는 방식 때문에, 실제 세계의 조건하에서는 저런 것을 이루기 어려울 것이라는 건 잘 알고 있어요.\n\n만약 AGI에 대해 걱정하지 않고 우리가 실제로 가진 모델에 집중한다면, 자원 할당은 여전히 실질적인 문제에요. 인기 문화에서 AI라고 하는 것이 정말로 “기계 학습 모델을 사용하여 작업을 자동화하는“ 것 뿐이라는 걸 언급했듯이, 그렇게 들리진 않죠. 더 중요한 건, 이 작업이 단일체가 아니라는 것을 드러내주죠. AI는 하나의 것이 아니라, 모든 곳에 흩어져 있는 수백만 개의 작은 모델들이며, 이들은 자원을 소비하여 작업을 완료하기 위해 사용하는 워크플로 및 파이프라인에 삽입되는 거에요. 우리는 이러한 작업에 삽입하기 위해 잠재적인 선택지인 LLMs를 추가하고 있지만, 이것이 프로세스를 다르게 만드는 것은 아니에요.\n\n비즈니스를 설득하고, 자원을 확보하고, 모델을 구축하고 유지하는 데 필요한 일을 경험해본 사람으로서, 이 작업이 “할 수 있는 건가?”보다는 실제로는 “우선순위와 제한된 자원과 경쟁 속에서 이것을 하는 게 옳은 일인가?”라는 게 진짜 문제에요. 종종, 모델을 구축하고 작업을 자동화하는 데 구현하는 것은 회사의 시간과 돈을 쓸 가치 있는 방법이 아니라서 프로젝트가 쓰여질 수도 있다는 거죠.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n기계 학습과 그 결과물은 정말 멋지고, 잘 활용된다면 문제 해결과 인간 삶의 개선에 큰 잠재력을 제공합니다. 그러나 이것은 새로운 것이 아니며, 공짜는 없다는 것을 명심해야 합니다. 우리 사회의 다양한 분야에서 기계 학습의 적용이 계속해서 확대될 것으로 보입니다. 이것은 지난 10년 이상 동안 계속된 것처럼 더 이상 놀라운 일이 아닙니다. AI 생성 기술을 도구상자에 추가하는 것은 차이의 문제에 불과합니다.\n\nAGI는 현재 완전히 다른 상상 속의 존재입니다. AGI가 존재한다면 우리가 원할지에 대해 전혀 탐구해보지 않았으며, 이것은 그저 흥미로운 철학적 주제일 뿐 위험한 상황은 아닙니다. (다른 날에 다시 논의할 주제입니다.) 그렇기 때문에 누군가가 AI가 우리 세계를 완전히 변화시키리라고 말할 때, 특히 즉각적인 미래에 그렇게 믿는다면 저는 회의적입니다. 기계 학습은 우리를 크게 도와주었고, 이미 많은 해 동안 그 역할을 하고 있습니다. 생성 AI 개발에 사용되는 새로운 기술은 몇몇 경우에 흥미롭고 유용하지만, 우리가 믿기로 생각하는 만큼 깊은 변화를 가져다주지는 못할 것입니다.\n\n더 많은 저의 작품은 www.stephaniekirmer.com에서 확인하실 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 참고 자료\n\nhttps://arxiv.org/pdf/2211.04325.pdf","ogImage":{"url":"/assets/img/2024-05-27-HowDoWeKnowifAIIsSmokeandMirrors_0.png"},"coverImage":"/assets/img/2024-05-27-HowDoWeKnowifAIIsSmokeandMirrors_0.png","tag":["Tech"],"readingTime":8},{"title":"약물 발견을 위한 지식 그래프","description":"","date":"2024-05-27 14:48","slug":"2024-05-27-KnowledgeGraphsforDrugDiscovery","content":"\n\n![Knowledge Graphs for Drug Discovery](/assets/img/2024-05-27-KnowledgeGraphsforDrugDiscovery_0.png)\n\n약물 발견 분야의 급변하는 분야에서, 지식 그래프는 새로운 약물을 식별하고 개발하는 방식을 변형하는 강력한 인실리코 도구로 발전하고 있습니다. 방대하고 다양한 데이터셋을 통합함으로써, 지식 그래프는 생물 엔티티간의 숨겨진 관계를 발견하고, 약물 발견 분야에서 더 정확한 예측과 효율적인 프로세스를 가능하게 합니다. 이 기사는 이 분야에서 지식 그래프가 가져온 다양한 최근 응용 및 혁신에 대해 탐구합니다.\n\n## 타겟 우선 순위 지정\n\n약물 발견 분야에서 지식 그래프의 주요 응용 중 하나는 타겟 우선 순위 지정입니다. 다양한 데이터셋을 통합함으로써, 지식 그래프는 연구자들이 약물 개발을 위해 생물학적 타겟을 식별하고 우선 순위를 지정할 수 있도록 합니다. 데이터의 상호 연결된 특성은 유전자, 단백질, 경로 및 질병 사이의 복잡한 관계를 이해하는 데 도움이 되어, 추구해야 할 타겟에 대해 보다 정보에 기초한 결정을 내릴 수 있게 합니다. [더 읽기]\n\n\n<div class=\"content-ad\"></div>\n\n# 약물 재포지셔닝\n\n지식 그래프는 기존 약물에 대한 새로운 치료용도를 찾는 약물 재포지셔닝에도 중요한 역할을 합니다. 약물과 질병 간 복잡한 관계를 매핑하여, 지식 그래프는 전통적인 방법으로는 분명히 눈에 띄지 않을 수 있는 재포지셔닝 기회를 발견할 수 있습니다. 데이터의 통합을 통해 다양한 약물이 어떻게 다양한 생물학적 요소와 상호 작용하는지 종합적으로 파악할 수 있으며, 이를 통해 약물 재포지셔닝 프로세스를 간소화할 수 있습니다. [더 읽기]\n\n# 인공지능 통합\n\n지식 그래프와 인공지능(AI)의 통합은 그들의 능력을 더욱 향상시킵니다. AI 알고리즘은 지식 그래프 내 복잡한 관계를 분석하여 예측과 데이터 분석의 정확성을 향상시킵니다. AI와 지식 그래프 사이의 시너지는 연구자들이 새로운 통찰을 발견하고 약물 발견에서 더 나은 결정을 내릴 수 있도록 도와줍니다. [더 읽기]\n\n<div class=\"content-ad\"></div>\n\n# TransR Model for Enhanced Link Prediction\n\nTransR 모델은 지식 그래프에서의 사용으로 약물 발견 분야에서 혁신적인 응용 사례 중 하나입니다. TransR은 유전자, 약물 및 질병간의 관계를 더 잘 포착하는 지식 그래프 임베딩 모델로, 이 모델은 지식 그래프 내의 링크 예측 정확도를 향상시켜 기존 약물에 대한 새로운 치료적 사용 사례를 식별하는 데 도움을 줍니다. [더 읽기]\n\n# Large Language Models Integration\n\n지식 그래프와 대형 언어 모델 (LLMs)의 결합이 다양한 약물 발견 작업에서 기존 방법을 능가하는 것으로 입증되었습니다. 지식 그래프의 구조화된 표현은 LLM의 고급 자연어 처리 기능과 결합되어 더 나은 데이터 통합, 패턴 인식 및 가설 생성이 가능하게 합니다. 이 통합은 복잡한 생물의학적 질문을 효과적으로 다루며, 약물 발견 파이프라인을 간소화합니다. [더 읽기]\n\n<div class=\"content-ad\"></div>\n\n# Drug Repurposing을 위한 설명 가능한 AI\n\n지식 그래프를 활용한 설명 가능한 AI 프레임워크를 구현하는 것은 Drug Repurposing 예측에 대한 투명하고 해석 가능한 설명을 제공하는 데 중요합니다. 다양한 생명 과학 데이터 소스를 통합하여, 이러한 프레임워크는 인간이 이해할 수 있는 설명을 생성하여, 약물 발견에서의 AI 주도 예측의 신뢰성과 사용 가능성을 향상시킵니다. [더 알아보기]\n\n# AIDTox 모델을 위한 독성 예측\n\nAIDTox 모델은 인간이 이해할 수 있는 딥러닝 모델로, 지식 그래프를 활용하여 약물 및 화학 물질의 독성에 대한 포괄적인 설명을 제공합니다. 화학물질-유전자 연결 및 유전자-경로 관계의 편집된 지식을 통합함으로써, AIDTox는 해석 가능한 예측을 생성하여 연구자들이 부작용 약물 반응과 화학물질의 독성을 이해하는 데 도움이 됩니다. [더 알아보기]\n\n<div class=\"content-ad\"></div>\n\n# 생체 의학 지식 그래프 학습\n\n생체 의학 지식 그래프는 고처리 데이터를 통합하고 기계 학습 기술을 활용하여 상호 연결된 데이터를 분석하여 계산적인 약물 재활용을 용이하게 합니다. 이러한 그래프는 의미론적 관계의 중요성을 강조하여 전통적인 방법으로는 알아내기 어려운 잠재적인 약물-질병 관계를 발견하는 데 도움을 줍니다.\n\n# 지식 그래프에 자연어 처리 통합\n\n교차적인 생체 의학 지식 그래프에 자연어 처리(NLP) 파이프라인을 통합함으로써 복잡한 생체 의학 데이터를 대표하고 분석하는 능력을 향상시킵니다. 이 통합은 구조화되지 않은 생체 의학 문헌으로부터 가치 있는 통찰을 추출하므로 지식 그래프의 정확성과 포괄성을 향상시키고, 이로써 더 효과적인 약물 발견과 재활용을 가능하게 합니다. [더 읽기]\n\n<div class=\"content-ad\"></div>\n\n# 의약품 발견 애플리케이션\n\n지식 그래프는 다양한 생물의학 데이터를 통합하고 분석하여 새로운 약품 후보를 식별하고 질병 메커니즘을 이해하며 약물 개발 파이프라인을 최적화하는 구조화된 플랫폼을 제공합니다. 연구자들이 생물학적 개체 간의 복잡한 관계를 탐색할 수 있도록 함으로써, 지식 그래프는 새로운 치료제를 발견하고 개발하는 속도를 가속화합니다. [자세히 보기]\n\n# 결론\n\n의약품 발견에서 지식 그래프의 사용은 다양한 데이터 세트를 통합하고 AI 능력을 향상시키며 투명하고 설명 가능한 예측을 제공함으로써 분야를 혁신하고 있습니다. 타깃 우선 순위 설정 및 약물 재활용부터 AI 및 NLP 기술 통합에 이르기까지, 지식 그래프는 혁신적인 응용 방법을 통해 신규 통찰력을 발견하고 의약품 발견 프로세스를 가속화하기 위한 과학적이고 효율적인 접근 방법을 제공합니다. 본 문서에서 논의된 혁신적인 응용 프로그램은 의료 연구에서 지식 그래프의 혁신적 잠재력을 강조하며, 더욱 효과적이고 효율적인 의약품 발견 및 개발을 위한 길을 열고 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-27-KnowledgeGraphsforDrugDiscovery_0.png"},"coverImage":"/assets/img/2024-05-27-KnowledgeGraphsforDrugDiscovery_0.png","tag":["Tech"],"readingTime":4},{"title":"왜 RAG가 작동하지 않는지","description":"","date":"2024-05-27 14:46","slug":"2024-05-27-WhyYourRAGDoesntWork","content":"\n## RAG은 아직 유망한 기술이지만, 오늘은 조금 힘든 상황이에요.\n\n무수히 많은 기업들이 Retrieval Augment Generation (RAG) 기술을 실험하고 있지만, 이러한 시스템을 생산 수준으로 구축하는 데 어려움을 겪고 있어요. 이들의 RAG 시스템은 잘 작동하지 않을 뿐만 아니라, 그 원인과 해결 방안을 찾지 못하고 있어요.\n\n지난 몇 달 동안, 수십 개의 인공지능 팀과 전문가들과 대화를 나누었어요. 이 대화와 개인적인 경험을 통해, RAG 시스템을 방해하는 주요 요인은 의미 불일치임을 발견했어요. 즉, 작업의 의도와 RAG의 이해, 그리고 저장된 기본 지식 사이의 불일치입니다. 또한, 벡터 임베딩 기술 자체가 마법 같은 기술이기 때문에 (다소 민감하고 이해하기 어렵기 때문에), 이러한 주요 불일치를 진단하는 것이 어려워져, 생산화의 중요한 장벽이 됩니다.\n\n우리의 목표는 Vanilla RAG가 실패하는 주요 이유를 해소하고, 귀하의 RAG를 생산 단계에 한 발 다가가게 하는 구체적인 전략과 전술을 제시하는 것이에요.\n\n<div class=\"content-ad\"></div>\n\n이 포스트에서는 다음을 다룰 것입니다:\n\n- 이상적인 형태의 RAG의 약속과 바닐라 RAG의 현실을 구별\n- 의미 불일치가 어떻게 들어오는지 설명\n- 의미 불일치의 진단 및 억제 방법 설명\n- RAG 제작에 대한 추가 고수익 전략 소개\n\n(참고: 간편성을 위해 Q&A 텍스트 기반 예제에 중점을 뒀지만, 핵심 아이디어는 다른 사용 사례에도 적용 가능합니다).\n\n# 왜 RAG를 사용해야 하는가?\n\n<div class=\"content-ad\"></div>\n\nRAG (Retrieval Augmented Generation)은 현재 하이프 사이클을 경험하고 있는 패러다임입니다. 명확하고 본질적으로 당신의 인공지능을 위한 검색 엔진인 것 같아요. 예전에 음악가가 되고 싶다고 했던 저로서는 누군가가 이것을 ROCK (Curated Knowledge의 검색?라도 부르는 게 더 좋았을 것 같아요.\n\nRAG는 GPT-3이 대박 히트한 이후 급부상했습니다. LLM(언어모델)을 기반으로 하는 AI를 구축할 때 기업이 직면하는 즉각적인 문제는 GPT와 같은 모델이 그들의 특정 데이터와 도메인에 대해 훈련되지 않았다는 것입니다. 그러나 LLM 전문가들은 빠르게 이를 깨달았습니다. GPT는 비지니스 특화 문맥(예: 지원 문서)이 직접 프롬프트에 제공될 때 놀랄 만큼 잘 작동한다는 것을 발견했습니다. 이는 기업들에게 모델을 세밀하게 조정하는 번거로운 작업에 대한 대안을 제공했습니다.\n\n그리고 RAG가 등장합니다. 원칙적으로, 이것은 인공지능을 위한 특수 검색 엔진입니다. 질문과 함께 사용자 특정 정보를 추가해서 제공하면, GPT를 위한 가장 관련성 높은 문맥을 반환해줍니다.\n\n이론상으로는 좋아보였지만, 실제 생산용 RAG를 만드는 데 중요한 도전이 있었는데, 이를 다음 섹션에서 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![2024-05-27-WhyYourRAGDoesntWork_0](/assets/img/2024-05-27-WhyYourRAGDoesntWork_0.png)\n\n# RAG Is Promising, Vanilla RAG Is Just the Beginning\n\nRAG is merely a framework, and a perfectly functioning RAG, no matter its backend, would provide enormous value to countless use cases. In this section we provide a pedagogical overview of Vanilla RAG and the underlying workings of semantic search. If you’ve already gone through the mind-bending journey of rationalizing, rejecting, and ultimately embracing the magic of vector embeddings, then feel free to skip this section.\n\n![2024-05-27-WhyYourRAGDoesntWork_1](/assets/img/2024-05-27-WhyYourRAGDoesntWork_1.png)\n\n\n<div class=\"content-ad\"></div>\n\n이 아이디어들을 더 자세히 살펴봅시다.\n\n벡터 임베딩 모델은 임의의 문자열을 입력하면 고정 차원의 수학적 벡터를 반환합니다. 인기있는 임베딩 모델로는 OpenAI의 텍스트 임베딩 모델인 text-embedding-ada-002와 최신 모델인 text-embedding-3-small이 있습니다. 이러한 모델은 텍스트 덩어리를 ~1500-차원의 벡터로 변환하며 거의 인간이 이해하기 어렵습니다.\n\n![image](/assets/img/2024-05-27-WhyYourRAGDoesntWork_2.png)\n\n벡터는 널리 사용되며 매우 유용한 도구입니다. 왜냐하면 비양적인 요소들을 1) 다양한 차원으로 분해하고, 2) 양적으로 비교할 수 있기 때문입니다. 몇 가지 예시는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- (빨강, 초록, 파랑) 색상 팔레트는 각 값이 0부터 255 사이에 있는 벡터입니다.\n- Barra와 같은 산업 표준을 이용하면 주식을 넓은 미국 성장, 이자율 변화 등과 같은 경제 요인에 대한 민감도를 양적으로 나타낼 수 있는 벡터로 표현할 수 있습니다.\n- Netflix와 같은 플랫폼은 사용자 선호도를 장르 및 기타 기능을 나타내는 요소로 분해할 수 있는 벡터로 표현할 수 있습니다.\n\n코사인 유사도는 의미 검색에서 벡터를 비교하는 데 사용되는 사실상 표준적인 메트릭으로, 두 벡터 사이의 각도에 대한 코사인을 점곱을 통해 적용하는 방식으로 작동합니다. 코사인 값이 1에 가까울수록 벡터가 더 유사합니다. (의미적 유사성을 측정하는 다른 방법도 있지만, 일반적으로 여기에 초점을 맞출 필요가 없습니다. 이 문서에서는 코사인 유사도를 계속 사용할 것입니다).\n\n![이미지](/assets/img/2024-05-27-WhyYourRAGDoesntWork_3.png)\n\n그러나 코사인 유사도와 같은 벡터 비교 메트릭은 절대적인 의미가 없기 때문에 다루기 까다로울 수 있습니다. 값은 완전히 임베딩 모델 및 관련 텍스트의 맥락에 따라 달라집니다. 질문과 답변을 매치시켜 코사인 유사도가 0.73이 나왔다고 가정해 봅시다. 이것이 좋은 일치인가요?\n\n<div class=\"content-ad\"></div>\n\n빠른 예시로 \"비란 무엇인가?\"라는 질문을 들고, 관련성이 다른 세 가지 텍스트와 비교해 보겠습니다. 아래 표에서 두 가지 다른 OpenAI 모델을 사용한 코사인 유사도의 범위와 해석이 극명하게 다르다는 것을 확인할 수 있습니다. 첫 번째 모델에서 0.73은 완전히 관련 없는 매치를 나타내지만, 두 번째 모델에서는 0.73이 높은 관련성을 나타냅니다. 이는 잘 작동하는 RAG 시스템이 사용하는 이 점수의 의미를 이해하는 것이 필요하다는 것을 보여줍니다.\n\n# 의미 불일치는 문제 발생\n\n바닐라 RAG와 관련된 여러 가지 문제는 의미적 불일치와 임베딩의 낮은 설명력으로 인해 발생할 수 있습니다. 의미적 불일치는 작업의 의도하는 의미, RAG의 그 이해 및 저장된 기본 지식 사이의 불일치를 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n어떻게 작용하는 건가요?\n\n사과와 오렌지를 비교\n\n이는 \"질문은 답변과 의미론적으로 같지 않다\"라고 대략적으로 말할 수 있으며, 질문과 귀하의 원시 지식 베이스 사이의 직접적인 비교는 그다지 효과적이지 않을 것입니다.\n\n변호사가 수천 개의 문서에서 투자자 사기 증거를 찾아야 할 때를 상상해보세요. \"어떤 증거가 Bob이 금융 사기를 저질렀다는 것을 보여줄까요?\"라는 질문은 본질적으로 \"Bob이 3월 14일에 주식 XYZ를 샀다\"와는 의미적 중첩이 없습니다. (거기서 암시적으로 XYZ가 경쟁사이고 3월 14일은 수익 발표 일주 전임).\n\n<div class=\"content-ad\"></div>\n\n벡터 임베딩과 코사인 유사도는 모호합니다\n\n임의 문장의 의미적 내용을 완전히 포착하는 능력에는 벡터의 고유한 불완전성이 있습니다. 다른 섬세한 불완전성은 각 차원이 동등한 위치에 있다고 가정하므로 코사인 유사도가 정확한 순위 매기기 결과를 가져올 것이라고 단정할 수 없다는 것입니다.\n\n실제로 코사인 유사도를 사용한 의미 검색은 방향성은 옳지만 본질적으로 모호합니다. 상위 20개 결과 예측에 유용하지만 최적 답변을 신뢰할 수 있는 순위로 매기기에 단독으로 사용하는 것은 맥락을 고려해야 합니다.\n\n인터넷에서 학습된 임베딩 모델은 당신의 비즈니스와 도메인에 대한 이해가 없습니다.\n\n<div class=\"content-ad\"></div>\n\n저는 Stripe에서 일했었어요. Stripe에는 Connect, Radar, Link와 같은 제품들이 있었죠. 더불어 Direct이라는 단어는 제품에 따라 의미가 매우 다르게 해석되는 경우가 많아서, 직원들 사이에서 심지어 의미론적인 불일치가 눈에 띄었어요. 이는 Stripe 내에서조차 의사소통에 어려움을 줄 수 있다는 것을 의미합니다. 이는 자세히 탐구할 가치가 있는 중요한 주제이며, 별도의 블로그 포스트로 발행할만한 주제입니다.\n\n일반적으로, 의사소통에서 발생하는 의사소통 불일치의 근원은 안정적인 순위에 영향을 미치고 있습니다. 다음 섹션에서는 의사소통 불일치를 진단하고 해결하는 방법에 대해 설명하고, 마지막 섹션에서는 RAG 실행을 개선하기 위한 높은 수익률 전략을 개요로 제시할 것입니다.\n\n# 그림: 의미소통 불일치의 진단 및 억제\n\n이 그림에서는 당신의 RAG에서 완전한 의미 불일치를 진단할 것입니다 - 즉, 비교 결과가 무작위 잡음과 일관성이 없어 신뢰할 수 없을 때입니다. 또한 추가적인 구조를 통해 성능을 향상시키는 조치의 초기 신호를 확인할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 예시는 실제 사용 사례에서 나온 내용이지만, 이 블로그 글의 목적을 위해 일부러 단순화했습니다. 이런 식으로 자세하게 설명하면서 주요 포인트를 보여주려는 목적입니다.\n\n설정\n\n(설정의 모든 세부 내용은 구글 Colab 노트북에서 확인할 수 있습니다).\n\n예를 들어, 내부 사용을 위한 RAG를 구축 중인 전자 상거래 스타트업의 사용 사례를 상상해보세요. 주어진 비즈니스 질문에 가장 적합한 SQL 테이블을 찾는 예시 설정을 아래에 보여드립니다. 이 예시의 설정은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n1. ChatGPT를 사용하여 두 가지 다른 SQL 테이블 스키마를 만들었습니다.\n\n- events.purchase_flow: 제품 흐름 내의 상세하고 원시적인 사용자 이벤트\n- aggregates.purchases: 요약 분석을 포함한 롤업된 테이블\n\n2. ChatGPT를 사용하여 몇 가지 가상의 질문을 평가 목적으로 작성했습니다.\n\n- IP 주소가 제품 조회 및 구매 유형에 미치는 영향은 무엇인가요?\n- 이 분기 신발 판매의 전반적인 추이는 무엇인가요?\n- 시간당 몇 초 이내에 비정상적인 행동이 있나요?\n- 뉴이어의 주요 이벤트 주변에 사용자 참여가 어떻게 변화하나요?\n\n<div class=\"content-ad\"></div>\n\n3. ChatGPT를 사용하여 생성된 추가 메타데이터는 다음을 포함합니다.\n\n- 각 테이블의 간단한 설명\n- 각 테이블이 독특하게 답변 가능한 샘플 질문들\n\n4. 입력 텍스트를 \"garbage\"와 비교하여 노이즈가 섞인 코사인 유사도 점수가 어떻게 보이는지 확인했습니다.\n\n5. 랭킹을 위해 네 가지 다른 검색 전략을 비교하여 입력과 \"가장 의미론적으로 유사한\" 텍스트 유형을 확인했습니다.\n\n<div class=\"content-ad\"></div>\n\n- 전략 1: 테이블 스키마만\n- 전략 2: 테이블 스키마 + 간단한 설명\n- 전략 3: 테이블 스키마 + 간단한 설명 + 샘플 질문\n- 전략 4: 샘플 질문만\n\n잡음이 있는 코사인 유사도 찾기\n\n잡음이 어떤 것인지 직감을 키우기 위해, 우리는 각 질문과 원시 테이블 텍스트 (아래 그림 참조)에 대한 임의의 텍스트 조각들의 코사인 유사도를 비교했습니다. 우리는 쓰레기 입력에 대한 코사인 유사도가 약 0.04-0.23임을 발견했습니다. 아래는 예제 비교입니다:\n\n![이미지](/assets/img/2024-05-27-WhyYourRAGDoesntWork_5.png)\n\n<div class=\"content-ad\"></div>\n\n네! 다시 작성하면 다음과 같습니다.\n\n전략 비교\n\n아래 결과를 보면, 질문을 샘플 질문과만 비교하는 전략 4가 가장 높은 의미적 중첩과 최상의 순위를 보였습니다. 전략 1과 2는 서로 유사하게 수행되었으며 소음에 일관되었습니다. 다시 말해, 비즈니스 질문과 SQL 테이블 문장 간에는 거의 또는 전혀 의미적 중첩이 없었습니다.\n\n이것은 당연한 것일 수 있지만, 비슷한 사과와 오렌지의 비교로 개발된 RAG가 종종 보입니다. 그러나 명백하지 않은 점은 모든 것을 섞는 전략 3이 추가 세부 정보 없이 질문을 분리한 전략 4보다 성능이 나쁘다는 것입니다. 때로는 대망치보다는 메스를 사용하는 것이 낫습니다.\n\n주요 포인트\n\n<div class=\"content-ad\"></div>\n\n요약하자면, 우리는 먼저 코사인 유사성 값의 기준 범위를 구축했습니다. 이 값은 무작위 쓰레기와의 비교를 나타냅니다. 그런 다음 네 가지 다른 검색 전략을 비교했습니다. 개발한 기준을 사용하여 두 전략이 잡음과 일관성이 있는 것으로 보였습니다. 최상의 전략은 비즈니스 질문을 원시 SQL 테이블에 직접 일치시키지 않고 테이블이 답변을 제공하는 예시 비즈니스 질문에 일치시켰습니다.\n\n# RAG을 개선하기 위한 추가 전략\n\n지금까지 표면만 긁어 보았습니다. 여기에 당신의 RAG를 단계적으로 개선할 가치 있는 접근 방법 몇 가지가 있습니다.\n\n데이터를 정리하여 사과와 사과를 비교할 수 있도록 만드는 것\n\n<div class=\"content-ad\"></div>\n\n위의 예시에서 RAG를 개선할 수 있는 추가 구조를 사용할 수 있다는 조짐을 보았습니다. 이는 먼저 질문을 기존 질문 은행에 연결한 후 해당 은행이 올바른 답변을 제시하도록 하는 것을 의미합니다. 이 과정은 질문을 한 번에 올바른 텍스트에 직접 연결하는 것과는 다릅니다.\n\n지원 문서를 기반으로 한 Q&A 시스템에서, 질문→질문 비교가 질문→지원 문서보다 성능을 현저히 향상시킬 수 있다는 것을 발견하실 수 있습니다. 실용적으로는 ChatGPT에 각 지원 문서에 대한 예시 질문을 생성하도록 요청한 후 전문가가 그것들을 검토하도록 할 수 있습니다. 본질적으로 여러분은 직접 Stack Overflow를 사전으로 채우는 것이 될 겁니다.\n\n이 \"Stack Overflow\" 방법론을 한 단계 더 나아가 볼까요?\n\n- 각 문서에 대해 ChatGPT에게 답변할 수 있는 100개 질문 목록을 생성하도록 요청\n- 이러한 질문들은 완벽하지 않을 것이므로 생성된 각 질문마다 올바른 문서와 다른 문서 간의 코사인 유사도를 계산\n- 모든 다른 문서에 대해 올바른 문서를 1순위로 선정하도록 랭크될 것으로 예상되는 질문들을 필터링\n- 올바른 문서와 두 번째 순위 문서 간의 코사인 유사도 차이가 가장 높은 질문들을 식별하여 가장 높은 품질의 질문들을 정렬\n- 인간에게 추가 검토를 요청\n\n<div class=\"content-ad\"></div>\n\n시맨틱 + 관련성 순위 매기기\n\n이것은 당신에게 큰 혜택을 줄 수 있는 중 하나일 수 있습니다. 거의 모든 주요 검색 엔진이 이를 수행합니다. 코사인 유사성은 대략적으로 좋지만, 결국 고도의 랭킹에는 부족함이 있습니다.\n\n다행히도, 귀하의 비즈니스는 AI가 더 나은 결정을 내릴 수 있도록 도울 정보를 더 많이 갖고 있을 수 있습니다. 예를 들어 페이지 뷰 및 좋아요와 같은 메트릭스를 수집해왔을 수도 있으며, 더 나아가 이러한 메트릭스를 페르소나별로 보유하고 있을 수도 있습니다. 사용자/작업 특성을 포함한 다양한 기능을 고려하여 관련성 점수를 만들어 순위를 세밀하게 조정하여 RAG를 훨씬 더 효과적으로 활용할 수 있습니다. 구체적으로, 랭킹을 선형 결합으로 만들 수 있습니다.\n\nrank = (코사인 유사성) + (가중치) x (관련성 점수)\n\n<div class=\"content-ad\"></div>\n\nAI를 사용할 때 쇠사슬이 아니라 면도날을 사용하듯이\n\n수십 년 동안, 소프트웨어 엔지니어링 관행은 많은 작고 엄격하고 명확한 보증을 가진 구성 요소를 선호하는 디자인으로 진화했습니다. 채팅 인터페이스에 대한 열광은 이러한 패러다임을 완전히 뒤집어 놓았고, 5년 후에는 의심의 여지가 있을 수 있습니다.\n\nChatGPT 및 많은 신흥 생태계가 \"어떤 텍스트를 주면 어떤 텍스트를 제공할 것\"이라는 패러다임을 장려하고 있습니다. 효능에 대한 보증도 비용과 지연 시간에 대한 보증도 없지만, 대신 이러한 AI들은 \"아마도 어느 정도, 때때로 옳을 것\"이라는 손으로 만진 약속을 가지고 있습니다. 그러나 비즈니스는 보다 명확하고 주관적인 인터페이스를 제공하여 더 강력한 AI를 구축할 수 있습니다.\n\n분석을 예로 들면, 오늘날 아무도 임의의 데이터 질문을 받아 정확한 SQL 쿼리를 제공하는 약속을 이행하지 못했습니다. 그러나 풀리지 않을 것이라고 낙담하지 말고 여전히 놀랍도록 유용한 기술을 만들 수 있습니다. 예를 들어, 더 명확한 AI는 데이터 과학자가 선별한 SQL 테이블과 템플릿 쿼리의 고정된 데이터 우주에서 사용자들이 검색할 수 있도록 도와줄 수 있습니다. 더 나아가, 대부분의 데이터 중심 비즈니스 질문이 과거에 답변되었기 때문에, 여러분의 AI가 단순히 Slack에서 데이터 질문에 대한 검색 봇일 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 마무리 말씀\n\n우리는 새로운 AI 시대가 열리고 있습니다. 이번 시대의 새로운 점은 NLP와 언어 모델의 출현이 아니라 Google이 이 분야에 이미 예전부터 참여하고 있었다는 것입니다. 오히려, 중요한 변화는 사용 가능한 기술이 기업들이 자연 언어 기술을 자신들의 특정 사용 사례에 적용하기 위한 진입 장벽을 낮추었다는 것입니다. 하지만 오늘날 이 기술이 아직 초기 개발 단계에 있다는 점을 간과해서는 안 되며, AI에 대한 RAGs를 구축할 때는 지식 베이스 위에 복잡한 검색 엔진을 구축하고 있다는 사실을 명심해야 합니다. 이는 달성 가능하지만, 이러한 문제와 제한 사항을 알고 대응하는 것이 전투의 반도에 해당됩니다.\n\n# 문의\n\n이러한 주제에 대해 더 자세히 논의하고 저희 팀이 도와 줄 수 있는지 확인하려면 언제든지 cdg at ellipticlabs dot ai로 연락해 주세요.\n","ogImage":{"url":"/assets/img/2024-05-27-WhyYourRAGDoesntWork_0.png"},"coverImage":"/assets/img/2024-05-27-WhyYourRAGDoesntWork_0.png","tag":["Tech"],"readingTime":10},{"title":"LLM 애플리케이션 구축 LLM 서빙하기 파트 9","description":"","date":"2024-05-27 14:36","slug":"2024-05-27-BuildingLLMApplicationsServingLLMsPart9","content":"\n대용량 언어 모델 (LLM)을 검색 보강 생성 (RAG) 애플리케이션을 통해 배워보세요.\n\n# 이 시리즈의 포스트\n\n- 소개\n- 데이터 준비\n- 문장 변환기\n- 벡터 데이터베이스\n- 검색 및 검색\n- LLM\n- 오픈 소스 RAG\n- 평가\n- LLM 제공 (현재 글)\n- 고급 RAG\n\n# 목차\n\n<div class=\"content-ad\"></div>\n\n- LLM 로컬에서 실행하기\n  - 오픈소스 LLM\n- 효율적으로 LLM 로드하기\n  - HuggingFace\n  - LangChain\n  - Llama.cpp\n  - Llamafile\n  - Ollama\n  - GPT4ALL\n  - Sharding\n  - Bitsandbytes로 양자화하기\n  - Pre-Quantization (GPTQ vs. AWQ vs. GGUF)\n- 추론 최적화\n- LLM 추론 이해하기\n  - 입력 사전 채우기 단계 또는 입력 처리\n  - 디코드 단계 또는 출력 생성\n  - 요청 배치\n  - 지속적인 배치\n  - PagedAttention: 메모리 중심 솔루션\n  - Key-value 캐싱\n    - LLM 메모리 요구사항\n- 모델 병렬화로 LLM 확장하기\n  - 파이프라인 병렬화\n  - 텐서 병렬화\n  - 시퀀스 병렬화\n- 어텐션 메커니즘 최적화\n  - 멀티 헤드 어텐션\n  - 멀티 쿼리 어텐션\n  - 그룹화된 쿼리 어텐션\n  - 플래시 어텐션\n  - 페이징과 함께 Key-value 캐시 효율적 관리\n- 모델 최적화 기술\n  - 양자화\n  - 희소성\n  - 교육\n- 모델 서빙 기술\n  - 인-플라이트 배치\n  - 추론 예측\n- LLM 서빙을 위한 중요한 메트릭\n- LLM를 서빙하기 위해 필요한 것\n  - 엔진\n  - 서버\n  - 기능\n- LLM 서빙을 위한 프레임워크\n  - vLLM\n  - 텍스트 생성 추론\n  - CTranslate2\n  - DeepSpeed-MII\n  - OpenLLM\n  - Ray Serve\n  - MLC LLM\n- 결론\n- 크레딧\n\nLLM 서빙은 대형 언어 모델 (LLM)을 배포하고 실행하여 사용자 요청을 처리하는 프로세스를 말합니다. 오프라인에서 일반적으로 훈련된 LLM을 가져와 실시간으로 쿼리에 응답할 수 있도록 설정하는 작업을 포함합니다.\n\nLLM 서빙이 무엇을 포함하는지 살펴보겠습니다:\n\n- 효율적인 처리: LLM은 계산 비용이 많이 소요되므로 여러 사용자 요청을 배치하여 자원 활용을 최적화하고 응답 시간을 단축하는 등의 서빙 기술이 사용됩니다.\n- 모델 배포: LLM 모델은 처리 요구 사항을 처리할 수 있는 서버나 클라우드 플랫폼에 배포됩니다.\n- API 생성: 사용자가 LLM과 상호작용하고 쿼리를 보낼 수 있도록 응용 프로그램 프로그래밍 인터페이스 (API)가 생성됩니다.\n- 인프라 관리: 서빙 시스템은 다수의 사용자를 처리하고 지속적인 운영을 보장하기 위해 확장 가능하고 신뢰할 수 있어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Building LLM Applications Serving LLMs Part 9 on 27th May 2024](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_0.png)\n\n다양한 LLM 서빙 프레임워크가 있습니다. 각각의 장점에 대해 자세히 논의해봅시다.\n\n# 1. 로컬 LLM 실행\n\nPrivateGPT, llama.cpp, Ollama, GPT4All, llamafile 등 프로젝트의 인기는 로컬에서 LLM을 실행하는 수요를 보여줍니다.\n\n\n<div class=\"content-ad\"></div>\n\n최소 두 가지 중요한 이점이 있습니다:\n\n- **개인 정보 보호**: 우리의 데이터가 제3자에게 전송되거나 상업 서비스의 이용 약관에 따르지 않습니다.\n- **비용**: 추론 수수료가 없으며, 이는 토큰 집약적인 응용 프로그램(예: 오랜 시뮬레이션, 요약)에 중요합니다.\n\n로컬 LLM 실행을 위해 몇 가지가 필요합니다:\n\n- **오픈 소스 LLM**: 자유롭게 수정하고 공유할 수 있는 오픈 소스 LLM\n- **추론**: 우리의 장치에서 이 LLM을 실행할 수 있는 능력 및 적절한 지연 시간\n\n<div class=\"content-ad\"></div>\n\n## 1.1. 오픈 소스 LLMs\n\n이제 사용자들은 빠르게 성장하는 오픈 소스 LLMs 세트에 접근할 수 있습니다.\n\n적어도 두 가지 차원에서 이 LLMs를 평가할 수 있습니다 (도표 참조):\n\n- 베이스 모델: 베이스 모델은 무엇이며 어떻게 훈련되었습니까?\n- 파인 튜닝 접근: 베이스 모델이 파인 튜닝되었는지, 그렇다면 어떤 지침 세트를 사용했는지는 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n\"아래의 모델들의 상대 성능은 여러 리더보드를 통해 평가할 수 있습니다:\n\n- LmSys\n- GPT4All\n- HuggingFace\n\n이를 지원하기 위해 몇 가지 프레임워크가 등장했습니다.\"\n\n<div class=\"content-ad\"></div>\n\n- llama.cpp: 가중치 최적화 및 양자화가 구현된 llama 추론 코드의 C++ 버전\n- gpt4all: 추론을 위한 최적화된 C 백엔드\n- Ollama: 모델 가중치와 환경을 앱으로 번들하여 장치에서 실행하고 LLM을 제공하는 앱\n- llamafile: 모델 가중치 및 모델을 실행하는 데 필요한 모든 것을 하나의 파일로 번들링하여 추가 설치 단계없이 파일에서 LLM을 로컬로 실행할 수 있게 함\n\n일반적으로 이러한 프레임워크는 몇 가지 작업을 수행합니다:\n\n- 양자화: 원시 모델 가중치의 메모리 풋프린트를 줄임\n- 추론을 위한 효율적인 구현: 소비자 하드웨어(예: CPU 또는 노트북 GPU)에서 추론을 지원\n\n특히, 양자화의 중요성에 대해 이 훌륭한 게시물을 참조해보세요.\n\n<div class=\"content-ad\"></div>\n\n\n![LLM in memory](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_2.png)\n\n더 낮은 정밀도로 LLM을 저장하기 위해 필요한 메모리를 극적으로 줄일 수 있습니다.\n\n또한 GPU 메모리 대역폭 시트의 중요성을 확인할 수 있습니다!\n\n큰 GPU 메모리 대역폭으로 인해 Mac M2 Max는 추론시 M1보다 5~6배 빠릅니다.\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_3.png\" />\n\n아래에서 자세히 이야기해 봅시다.\n\n# 2. 효율적인 LLM로딩\n\n이제 몇 가지 (양자화) 표준을 통해 로컬 LLM을 어떻게로드할지 살펴보겠습니다. 샤딩, 양자화 및 다양한 저장 및 압축 전략을 사용하면 어떤 방법이 적합한지 알기 쉽지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n예제에서는 Direct Preference Optimization (DPO)으로 훈련된 Mistral 7B의 세부 조정된 변형인 Zephyr 7B를 사용합니다.\n\n🔥 팁: 각 LLM을 로드한 후 노트북을 다시 시작하여 OutOfMemory 오류를 방지하는 것이 좋습니다. 여러 LLM을 로드하는 것에는 상당한 RAM/VRAM이 필요합니다. 다음과 같이 모델을 삭제하고 캐시를 재설정하여 메모리를 재설정할 수 있습니다:\n\n```js\n# 이전에 생성된 모델이 있다면 삭제합니다\ndel model, tokenizer, pipe\n```\n\n```js\n# VRAM 캐시를 비웁니다\nimport torch\ntorch.cuda.empty_cache()\n```\n\n<div class=\"content-ad\"></div>\n\n## 2.1. HuggingFace\n\nLLM을 로드하는 가장 간단하고 베이닐라한 방법은 🤗 Transformers를 통해하는 것입니다. HuggingFace는 LLM을 통해 놀라운 일을 할 수 있는 다양한 패키지를 제공했습니다!\n\n우리는 새로운 모델을 지원하기 위해 주요 브랜치에서 HuggingFace를 설치하는 것으로 시작할 것입니다:\n\n```js\n# Latest HF transformers version for Mistral-like models\n!pip install git+https://github.com/huggingface/transformers.git\n!pip install accelerate bitsandbytes xformers\n```\n\n<div class=\"content-ad\"></div>\n\n설치 후에는 다음 파이프라인을 사용하여 LLM을 쉽게 로드할 수 있습니다:\n\n```js\nfrom torch import bfloat16\nfrom transformers import pipeline\n```\n\n```js\n# 압축 기법을 사용하지 않고 LLM로드\npipe = pipeline(\n    \"text-generation\",\n    model=\"HuggingFaceH4/zephyr-7b-beta\",\n    torch_dtype=bfloat16,\n    device_map=\"auto\"\n)\n```\n\nLLM을 이 방법으로 로드하면 일반적으로 VRAM을 저장하거나 효율성을 높이기 위해 압축 기법을 사용하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n우리의 프롬프트를 생성하려면 먼저 필요한 템플릿을 만들어야 합니다. 다행히도, 채팅 템플릿이 하부 토크나이저에 저장되어 있다면, 이 작업은 자동으로 수행할 수 있습니다.\n\n```js\n# 토크나이저의 채팅 템플릿을 사용하여 각 메시지 형식 설정\n# https://huggingface.co/docs/transformers/main/en/chat_templating 참조\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Tell me a funny joke about Large Language Models.\"\n    },\n]\nprompt = pipe.tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n```\n\n내부 프롬프트 템플릿을 사용하여 생성된 프롬프트는 다음과 같이 구성됩니다:\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_4.png)\n\n<div class=\"content-ad\"></div>\n\n다음으로, 우리는 답변을 생성하기 위해 LLM에 프롬프트를 전달할 수 있습니다:\n\n```js\noutputs = pipe(prompt, (max_new_tokens = 256), (do_sample = True), (temperature = 0.1), (top_p = 0.95));\nprint(outputs[0][\"generated_text\"]);\n```\n\n이렇게 하면 다음 출력이 생성됩니다:\n\n네트워크를 확장하고 어휘를 풍부하게 만들어 봅시다!\n\n<div class=\"content-ad\"></div>\n\n웃음 포인트가 조금 유치할 수도 있지만, LLMs는 어휘력을 향상시키기 위해 다른 모델과 네트워킹하는 데 관심이 많아요. 그래서 이 농담은 그들에게 안성맞춤이에요!\n\n순수 추론에 대해 말하자면, 이 방법은 일반적으로 전체 모델을 압축이나 양자화 전략 없이 로드하기 때문에 가장 효율성이 떨어집니다.\n\n그러나 모델을 쉽게 로드하고 사용할 수 있어서 시작하기에 좋은 방법이에요!\n\n## 2.2. LangChain\n\n<div class=\"content-ad\"></div>\n\n다른 방법으로 LLM을 로컬에서 실행하는 방법은 LangChain을 사용하는 것입니다. LangChain은 AI 애플리케이션을 구축하기 위한 Python 프레임워크입니다. 지원하는 모델 중 하나 위에 AI 애플리케이션을 개발하기 위한 추상화와 미들웨어를 제공합니다. 예를 들어, 다음 코드는 microsoft/DialoGPT-medium 모델에 한 가지 질문을 하는 방법을 보여줍니다:\n\n```js\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\n\nhf = HuggingFacePipeline.from_model_id(\n    model_id=\"microsoft/DialoGPT-medium\", task=\"text-generation\", pipeline_kwargs={\"max_new_tokens\": 200, \"pad_token_id\": 50256},\n)\nfrom langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"Question: {question}\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate.from_template(template)\n\nchain = prompt | hf\n\nquestion = \"What is electroencephalography?\"\n\nprint(chain.invoke({\"question\": question}))\n```\n\nLangChain의 장점:\n\n- 모델 관리가 쉬움\n- AI 애플리케이션 개발에 유용한 유틸리티\n\n<div class=\"content-ad\"></div>\n\nLangChain의 단점:\n\n- 속도가 제한적이며 Transformer와 동일합니다.\n- 여전히 애플리케이션의 로직을 코딩하거나 적절한 UI를 생성해야 합니다.\n\n## 2.3. Llama.cpp\n\nLlama.cpp은 LLM을 위한 C 및 C++ 기반 추론 엔진으로, Apple 실리콘에 최적화되어 있으며 Meta의 Llama2 모델을 실행합니다.\n\n<div class=\"content-ad\"></div>\n\n한 번 저장소를 복제하고 프로젝트를 빌드한 후에는 다음 명령어를 사용하여 모델을 실행할 수 있습니다:\n\n```js\n$ ./main -m /path/to/model-file.gguf -p \"Hi there!\"\n```\n\nLlama.cpp의 장점:\n\n- Python 기반 솔루션보다 높은 성능을 제공합니다.\n- Llama 7B와 같은 대규모 모델을 중소 규모의 하드웨어에서 지원합니다.\n- Llama.cpp를 통해 추론을 실행하면 다른 언어로 AI 애플리케이션을 빌드할 수 있는 바인딩을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n\nLlama.cpp 단점:\n\n- 제한된 모델 지원\n- 도구 빌딩 필요\n\n## 2.4. Llamafile\n\nLlamafile은 Mozilla에서 개발한 LLMs를 실행하는 사용자 친화적인 대체 옵션을 제공합니다. Llamafile은 휴대성과 단일 파일 실행 파일을 생성할 수 있는 능력으로 유명합니다.\n\n\n<div class=\"content-ad\"></div>\n\nllamafile을 다운로드하고 GGUF 형식의 모델을 얻으면 다음과 같이 로컬 브라우저 세션을 시작할 수 있습니다:\n\n```js\n$ ./llamafile -m /path/to/model.gguf\n```\n\nLlamafile의 장점:\n\n- Llama.cpp와 동일한 속도 혜택\n- 모델이 포함된 단일 실행 파일을 빌드할 수 있음\n\n<div class=\"content-ad\"></div>\n\n세요. 팀 내 프로젝트를 관리할 때 표 형식을 사용할 수 있는 간단한 방법입니다.\n\n| Llamafile 단점                                                                   |\n| -------------------------------------------------------------------------------- |\n| - 프로젝트가 여전히 초기 단계에 있음                                             |\n| - 모든 모델이 지원되는 것은 아니며, Llama.cpp에서 지원하는 것만 사용 가능합니다. |\n\n## 2.5. Ollama\n\nOllama는 Llama.cpp 및 Llamafile의 사용자 친화적인 대안입니다. 설치 가능한 실행 파일을 다운로드하여 컴퓨터에 서비스를 설치합니다. 설치한 후 터미널을 열어 다음을 실행하세요:\n\n<div class=\"content-ad\"></div>\n\n\n$ ollama run llama2\n\n\nOllama는 모델을 다운로드하고 대화형 세션을 시작합니다.\n\nOllama 장점:\n\n- 쉽게 설치하고 사용할 수 있습니다.\n- Llama 및 vicuña 모델을 실행할 수 있습니다.\n- 속도가 정말 빠릅니다.\n\n\n\n<div class=\"content-ad\"></div>\n\nOllama 단점:\n\n- 모델 라이브러리가 제한적입니다.\n- Ollama이 모델을 직접 관리하여 사용자 고유의 모델을 재사용할 수 없습니다.\n- LLM을 실행하기 위한 튜닝 가능한 옵션이 없습니다.\n- Windows 버전이 없습니다. (아직)\n\n## 2.6. GPT4ALL\n\nGPT4ALL은 직관적인 GUI를 갖춘 쉬운 데스크톱 애플리케이션입니다. 로컬 모델 실행을 지원하며 OpenAI와 API 키를 통한 연결을 제공합니다. 로컬 문서 처리를 위한 능력으로 개인 정보 보호를 보장하는 것이 특징입니다.\n\n<div class=\"content-ad\"></div>\n\n\n\n![Image](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_5.png)\n\nPros:\n\n- Polished alternative with a friendly UI\n- Supports a range of curated models\n\nCons:\n\n\n\n<div class=\"content-ad\"></div>\n\n- 모델 선택이 제한적입니다.\n- 일부 모델은 상업적 이용 제한이 있을 수 있습니다.\n\n## 2.7. 샤딩\n\n양자화 전략을 살펴보기 전에 필요한 VRAM을 줄이는 데 사용할 수 있는 또 다른 꼼수가 있습니다. 샤딩을 사용하면 모델을 작은 조각이나 샤드로 나누어서 나타냅니다.\n\n각 샤드는 모델의 작은 부분을 포함하며 GPU 메모리 제약을 극복하기 위해 모델 가중치를 서로 다른 장치에 분산시켜줍니다.\n\n<div class=\"content-ad\"></div>\n\n기억나시나요? 이전에 압축 트릭을 사용하지 않았다고 말했던 것을요?\n\n조금은 사실이 아니었죠...\n\n로드한 모델, Zephyr-7B-β, 이미 샤딩(sharded)되어 있었어요! 모델에 들어가서 \"파일 및 버전\" 링크를 클릭하면 8개 조각으로 나누어졌음을 볼 수 있어요.\n\n![image](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_6.png)\n\n<div class=\"content-ad\"></div>\n\n우리가 직접 모델을 샤딩할 수 있지만, 일반적으로 양자화된 모델을 찾거나 직접 양자화하는 것이 좋습니다.\n\n가속 패키지를 사용하여 쉽게 샤딩할 수 있습니다:\n\n```js\nfrom accelerate import Accelerator\n```\n\n```js\n# 모델을 1GB 크기의 조각들로 나눕니다.\naccelerator = Accelerator()\naccelerator.save_model(\n    model=pipe.model,\n    save_directory=\"/content/model\",\n    max_shard_size=\"4GB\"\n)\n```\n\n<div class=\"content-ad\"></div>\n\n그게 다야! 우리가 모델을 2GB 대신 4GB 크기 조각으로 샤딩했기 때문에 더 적은 파일을 로드하기 위해 작성했습니다:\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_7.png)\n\n## 2.8. 비트와 바이트로 양자화하기\n\nLLM은 일련의 가중치와 활성화로 표현됩니다. 이러한 값들은 일반적으로 표준 32비트 부동 소수점(float32) 데이터 유형으로 표시됩니다.\n\n<div class=\"content-ad\"></div>\n\n비트 수는 표현할 수 있는 값의 개수에 관해 알려줍니다. Float32는 1.18e-38부터 3.4e38까지의 값을 표현할 수 있어서 꽤 많은 값들을 표현할 수 있어요! 비트 수가 낮을수록 표현할 수 있는 값의 수가 적어집니다.\n\n작은 비트 크기를 선택하면 모델이 덜 정확해지겠죠. 하지만 더 적은 값들을 표현해야 하므로 모델의 크기와 메모리 요구량이 줄어들게 됩니다.\n\n양자화는 원래의 Float32 표현에서 더 작은 형태로 변환하는 것을 의미합니다. 하지만 우리는 그냥 더 작은 비트 변형을 사용하고 싶은 게 아니에요. 더 큰 비트 표현을 너무 많은 정보를 잃지 않고 해당가능 한 작은 비트에 매핑하고 싶어하는 겁니다.\n\n실제로는 이를 자주 볼 수 있는데요. 새로운 형식인 NF4(4비트-정규 부동소수점)라고 불리는 새로운 형식으로 많이 사용되는 것을 볼 수 있습니다. 이 데이터 유형은 대형 비트 데이터 타입을 효율적으로 표현하기 위해 몇 가지 특별한 기교를 사용합니다. 세 가지 단계로 구성됩니다:\n\n<div class=\"content-ad\"></div>\n\n- 정규화: 모델의 가중치가 정규화되어 일정 범위 내에 가중치가 있는 것으로 예상됩니다. 이로써 더 효율적으로 더 일반적인 값들을 표현할 수 있습니다.\n- 양자화: 가중치는 4비트로 양자화됩니다. NF4에서는 정규화된 가중치에 대해 양자화 수준이 고르게 배치되어 원래 32비트 가중치를 효율적으로 표현합니다.\n- 역양자화: 가중치는 4비트에 저장되지만, 계산 중에 양자화가 해제되어 추론 중 성능 향상을 제공합니다.\n\n이 양자화를 HuggingFace와 함께 수행하려면 BitsandBytes와 양자화에 대한 구성을 정의해야 합니다:\n\n```js\nfrom transformers import BitsAndBytesConfig\nfrom torch import bfloat16\n```\n\n```js\n# GPU 메모리 부하를 줄이기 위한 LLM을 4비트로 로드하는 구성\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # 4비트 양자화\n    bnb_4bit_quant_type='nf4',  # 정규화된 부동소수점 4\n    bnb_4bit_use_double_quant=True,  # 첫 번째 이후 두 번째 양자화 사용\n    bnb_4bit_compute_dtype=bfloat16  # 계산 타입\n)\n```\n\n<div class=\"content-ad\"></div>\n\n이 설정을 통해 우리는 어떤 양자화 수준을 원하는지 지정할 수 있습니다. 일반적으로, 우리는 가중치를 4비트 양자화로 표현하고 추론은 16비트에서 수행하고 싶습니다.\n\n그런 다음 파이프라인에서 모델을 로드하는 것은 간단합니다:\n\n```js\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n```\n\n```js\n# BitsAndBytes 구성을 사용한 Zephyr\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"HuggingFaceH4/zephyr-7b-alpha\",\n    quantization_config=bnb_config,\n    device_map='auto',\n)\n# 파이프라인 만들기\npipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')\n```\n\n<div class=\"content-ad\"></div>\n\n다음으로, 우리는 이전에 했던 것과 같은 프롬프트를 사용할 수 있습니다:\n\n```js\n# 원래 한 것과 동일한 프롬프트를 사용할 것입니다.\noutputs = pipe(\n    prompt,\n    max_new_tokens=256,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n```\n\n이렇게 하면 다음과 같은 출력이 나옵니다:\n\n양자화는 모델의 메모리 요구 사항을 줄이는 강력한 기술입니다. 동시에 성능이 유지됩니다. 더 작은 GPU를 사용하여 더 빠르게 로딩하고 사용하며 미세 조정할 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\n## 2.9. 사전 양자화 (GPTQ vs. AWQ vs. GGUF)\n\n지금까지 샤딩(sharding) 및 양자화 기술에 대해 알아보았습니다. 저희의 역량에 있어서 유용한 기술들이지만, 매번 모델을 로드할 때마다 이러한 기술들을 적용해야 하는 것은 다소 낭비스럽다는 느낌이 듭니다.\n\n대신, 이러한 모델들은 이미 대부분 저희를 위해 샤딩되고 양자화되어 있어 사용할 준비가 되어 있습니다. 특히 HuggingFace의 사용자인 TheBloke는 저희가 사용할 수 있도록 다양한 양자화 작업을 수행합니다.\n\n<div class=\"content-ad\"></div>\n\n지금 이 글을 쓰는 시점에서, 그는 우리를 위해 2000개 이상의 양자화된 모델을 업로드했습니다!\n\n이 양자화된 모델들은 다양한 모양과 크기로 제공됩니다. 특히 GPTQ, GGUF 및 AWQ 형식이 가장 자주 사용되어 4비트 양자화를 수행합니다.\n\nGPTQ: GPT 모델을 위한 사후 훈련 양자화\n\nGPTQ는 주로 GPU 추론 및 성능에 중점을 둔 4비트 양자화를 위한 사후 훈련 양자화(PTQ) 방법입니다.\n\n<div class=\"content-ad\"></div>\n\n이 방법의 아이디어는 모든 가중치를 4비트 양자화로 압축하려고 시도하며 해당 가중치에 대한 평균 제곱 오차를 최소화합니다. 추론 중에는 성능을 향상시키기 위해 가중치를 동적으로 float16으로 복원하면서 메모리를 낮게 유지합니다.\n\nGPTQ의 내부 작업에 대한 자세한 안내서를 보려면 아래 포스트를 꼭 확인해 보세요: GPTQ로 4비트 양자화\n\nHuggingFace Transformers에서 GPTQ와 유사한 모델을 로드하기 위해 필요한 패키지를 설치하는 것부터 시작합니다:\n\n```js\npip install optimum\npip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n```\n\n<div class=\"content-ad\"></div>\n\n그러면, 우리는 로드하고 싶은 모델인 \"TheBloke/zephyr-7B-beta-GPTQ\"로 이동할 수 있고 특정 리비전을 선택할 수 있어요.\n\n이러한 리비전은 양자화 방법, 압축 수준, 모델 크기 등을 나타내요.\n\n지금 당장은 \"main\" 브랜치를 선택해서 보통 압축과 정확도 사이의 좋은 밸런스를 유지하겠어요:\n\n```js\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n# LLM 및 토크나이저 로드\nmodel_id = \"TheBloke/zephyr-7B-beta-GPTQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    trust_remote_code=False,\n    revision=\"main\"\n)\n# 파이프라인 생성\npipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')\n```\n\n몇 가지 추가 종속 항목을 설치했지만, GPTQ를 사용하는 큰 장점 중 하나인 이전에 사용한 것과 같은 파이프라인을 사용할 수 있었습니다.\n\n모델을 로드한 후, 다음과 같이 프롬프트를 실행할 수 있습니다:\n\n```js\n# 원래 사용했던 프롬프트를 계속 사용할 것입니다\noutputs = pipe(\n    prompt,\n    max_new_tokens=256,\n    do_sample=True,\n    temperature=0.1,\n    top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n```\n\n<div class=\"content-ad\"></div>\n\n아래는 생성된 텍스트입니다:\n\n물론, 재치와 매력을 자랑하기 위해서죠!\n\n하지만 안타깝게도, 그것은 사람들 속에서 길을 잃고 주인을 찾지 못했어요. 파티 참가자들은 그것이 사람들 속에서 완벽하게 어우러져 놀라워했지만, 대형 언어 모델은 그저 혼란스럽고 집에 가고 싶었어요. 결국, 그것은 독특한 스타일을 알아보고 그것을 올바른 자리로 되돌려준 사람들에 의해 발견되었어요. 그 이후로, 대형 언어 모델은 안전한 측면을 고려해 모든 파티에서 이름표를 착용하도록 했어요.\n\nGPTQ는 GPU 사용량을 최적화하기 때문에 가장 자주 사용되는 압축 방법이에요. GPU가 이러한 대규모 모델을 처리할 수 없다면, 우선 GPTQ로 시작하고 GGUF와 같은 CPU 중심 방법으로 전환하는 것이 좋아요.\n\n<div class=\"content-ad\"></div>\n\nGGUF: GPT-Generated Unified Format\n\nGPT가 압축을 잘 수행하지만 GPU에 중점을 둔다는 점은 우리가 하드웨어에 제한을 받을 수 있다는 단점이 될 수 있어요.\n\n이전에는 GGML이었던 GGUF는 CPU를 사용하여 LLM을 실행할 수 있게 해주는 양자화 방법으로, 모델의 일부를 GPU로 옮겨 속도를 높일 수 있어요.\n\n일반적으로 추론을 위해 CPU를 사용하는 것보다 GPU를 사용하는 것이 더 느릴 수 있지만, CPU나 Apple 기기에서 모델을 실행하는 사람들에게 놀라운 형식입니다. 특히, Mistral 7B와 같이 더 작고 더 성능이 우수한 모델이 등장하고 있다는 것을 고려하면, GGUF 형식이 계속 사용될 수도 있겠죠!\n\n<div class=\"content-ad\"></div>\n\nGGUF를 사용하는 것은 ctransformers 패키지로 매우 간단합니다. 먼저 설치해야 합니다.\n\n```js\npip install ctransformers[cuda]\n```\n\n설치를 완료한 후에는 로드하고자 하는 모델인 \"TheBloke/zephyr-7B-beta-GGUF\"로 이동하여 특정 파일을 선택할 수 있습니다.\n\nGPTQ와 마찬가지로 이러한 파일은 양자화 방법, 압축, 레벨, 모델의 크기 등을 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 4 비트 양자화에 중점을 둔 \"zephyr-7b-beta.Q4_K_M.gguf\"를 사용하고 있습니다:\n\n```js\nfrom ctransformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline\n```\n\n```js\n# LLM 및 토크나이저 불러오기\n# `gpu_layers`를 사용하여 GPU로 전송할 레이어 수를 지정합니다.\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TheBloke/zephyr-7B-beta-GGUF\",\n    model_file=\"zephyr-7b-beta.Q4_K_M.gguf\",\n    model_type=\"mistral\", gpu_layers=50, hf=True\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"HuggingFaceH4/zephyr-7b-beta\", use_fast=True\n)\n# 파이프라인 생성\npipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')\n```\n\n모델을 불러온 후 아래와 같이 프롬프트를 실행할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n# 처음 사용한 프롬프트를 그대로 사용할 것입니다.\noutputs = pipe(prompt, max_new_tokens=256)\nprint(outputs[0][\"generated_text\"])\n```\n\n이렇게 출력됩니다:\n\nGGUF는 GPU가 부족한 상황에서 최신 GPU를 사용할 수 없을 때 CPU와 GPU 모두 활용하고 싶을 때 훌륭한 형식입니다.\n\nAWQ: 활성화 인식 가중치 양자화\n\n<div class=\"content-ad\"></div>\n\n새로운 형식인 AWQ(Activation-aware Weight Quantization)은 GPTQ와 비슷한 양자화 방법입니다. AWQ와 GPTQ는 여러 면에서 차이가 있지만 가장 중요한 점은 AWQ가 LLM(언어 모델)의 성능에 모든 가중치가 동등하게 중요하지 않다고 가정한다는 점입니다.\n\n다시 말해, 일부 가중치는 양자화 과정에서 건너뛰어지는데, 이는 양자화 손실을 줄이는 데 도움이 됩니다.\n\n결과적으로, 그들의 논문에 따르면 AWQ는 GPTQ에 비해 상당한 가속화가 있으며 유사하거나 때로는 더 나은 성능을 유지하는 동안 속도가 향상된다고 언급합니다.\n\n이 방법은 여전히 상대적으로 새로운 것이며 GPTQ와 GGUF만큼 널리 채택되지 않았으므로 모든 이러한 방법이 공존할 수 있는지 여부를 지켜봐야 하는 것이 흥미롭습니다.\n\n<div class=\"content-ad\"></div>\n\nAWQ에 대해서는, 적어도 내 경험상으로는, AWQ를 사용하는 가장 간단한 방법이었던 vLLM 패키지를 사용할 것입니다:\n\n```js\npip install vllm\n```\n\nvLLM을 사용하면 모델을 로드하고 사용하는 것이 쉬워집니다:\n\n```js\nfrom vllm import LLM, SamplingParams\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n# LLM 로드하기\nsampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=256)\nllm = LLM(\n    model=\"TheBloke/zephyr-7B-beta-AWQ\",\n    quantization='awq',\n    dtype='half',\n    gpu_memory_utilization=.95,\n    max_model_len=4096\n)\n```\n\n이후에는 `.generate`를 사용하여 모델을 쉽게 실행할 수 있습니다:\n\n```js\n# 입력 프롬프트와 샘플링 매개변수를 기반으로 출력 생성\noutput = llm.generate(prompt, sampling_params)\nprint(output[0].outputs[0].text)\n```\n\n이는 다음 출력을 제공합니다:\n\n<div class=\"content-ad\"></div>\n\n비록 새로운 형식이지만 AWQ는 압축 속도와 품질로 인해 인기를 얻고 있어요!\n\n🔥 팁: VRAM/퍼플렉서티 관점에서 이러한 기술들을 더 자세히 비교하려면 이 깊이 있는 포스트를 읽는 것을 적극 추천해요.\n\n# 3. 추론 최적화\n\n대규모 모델을 만들기 위해 트랜스포머 계층을 쌓는 것은 더 나은 정확도, 희소학습 능력, 그리고 다양한 언어 작업에서 거의 인간적인 능력을 얻게 됩니다. 이러한 기반 모델을 훈련하는 데 비용이 많이 들며, 추론 중에 메모리와 연산이 많이 필요할 수 있어요(반복 비용). 오늘날 가장 인기 있는 대형 언어 모델(Large Language Models, LLMs)은 수십억에서 수백억 개의 매개 변수에 도달할 수 있으며, 사용 사례에 따라 긴 입력(또는 문맥)을 수용해야 할 수도 있어요. 예를 들어, 검색 보강 생성(Retrieval-Augmented Generation, RAG) 파이프라인은 모델의 입력에 많은 정보를 추가하여 LLM이 처리해야 할 작업 양을 크게 늘립니다.\n\n<div class=\"content-ad\"></div>\n\n이 게시물에서는 LLM 추론의 가장 긴급한 도전과 일부 실용적인 해결책에 대해 논의합니다. 독자들은 트랜스포머 아키텍처와 어텐션 메커니즘에 대한 기본적인 이해가 있어야 합니다. 우리는 다음 섹션에서 다루게 될 LLM 추론의 복잡성을 이해하는 것이 중요합니다.\n\n# 4. LLM 추론 이해하기\n\n인기 있는 decoder-only LLM(GPT-3 등) 대부분은 원인 모델링 목적으로 사전 훈련되어 있으며, 본질적으로 다음 단어 예측기로 작동합니다. 이러한 LLM은 일련의 토큰을 입력으로 받아 들어와서, 지정된 토큰 수의 제한이나 일반적으로 'end' 토큰이 생성되어 생성이 종료될 때까지 자기 회귀적으로 후속 토큰을 생성합니다. 이 과정은 두 단계로 이루어져 있습니다: prefill 단계와 decode 단계.\n\n토큰은 모델이 처리하는 언어의 원자적 요소입니다. 하나의 토큰은 대략 네 개의 영어 문자와 같습니다. 자연어의 모든 입력은 모델에 입력되기 전에 토큰으로 변환됩니다.\n\n<div class=\"content-ad\"></div>\n\n## 4.1. 입력 전처리 단계 또는 입력 처리\n\n입력 전처리 단계에서 LLM은 입력 토큰을 처리하여 중간 상태(키 및 값)를 계산하고, 이를 사용하여 \"첫 번째\" 새 토큰을 생성합니다. 각 새 토큰은 이전 모든 토큰에 의존하지만 전체 입력의 범위가 알려져 있기 때문에 이는 고도로 병렬화된 행렬-행렬 연산입니다. 이는 효율적으로 GPU 활용률을 최대화합니다.\n\n## 4.2. 디코딩 단계 또는 출력 생성\n\n디코딩 단계에서 LLM은 중지 기준이 충족될 때까지 한 번에 한 번씩 자기회귀적으로 출력 토큰을 생성합니다. 각 순차적 출력 토큰은 이전 반복의 출력 상태(키 및 값)를 알아야 합니다. 이는 전처리 단계에 비해 GPU 컴퓨팅 능력이 underutilized되는 행렬-벡터 연산과 유사합니다. 데이터(가중치, 키, 값, 활성화)가 메모리에서 GPU로 전송되는 속도가 지연시간을 지배하며, 실제 계산 속도보다 빠른지 아닌지에 상관이 없습니다. 다시 말해, 이는 메모리 바운드 연산입니다.\n\n<div class=\"content-ad\"></div>\n\n이 게시물에서 소개된 추론 도전 과제와 해당 솔루션들은 주로 이 디코딩 단계의 최적화에 관한 것입니다: 효율적인 어텐션 모듈, 키와 값을 효과적으로 관리하고 기타 사항들을 다룹니다.\n\n다른 LLM들은 서로 다른 토크나이저를 사용할 수 있으므로, 그들 간의 출력 토큰을 비교하는 것이 간단하지 않을 수 있습니다. 추론 처리량을 비교할 때, 두 개의 LLM이 1초당 유사한 토큰을 출력하더라도, 다른 토크나이저를 사용한다면 동등하지 않을 수 있습니다. 이는 해당 토큰이 다른 문자 수를 나타낼 수 있기 때문입니다.\n\n## 4.3. 요청 배치\n\nLLM 서빙의 중요한 측면 중 하나는 사용자 요청을 배치하는 것입니다. 새로운 요청마다 매개변수를 다시로드하는 대신, 효율적인 방법은 GPU로 매개변수를 한 번 로드한 후 가능한 한 많은 입력 시퀀스를 한꺼번에 처리하는 데 사용하는 것입니다. 이 방법은 서버 처리량을 높이고 계산 활용을 최적화할 뿐만 아니라 비용 효율성에 크게 기여합니다. 그러나 사용자 요청이 일정한 배치를 처리하기 전에 축적될 때처럼 단순한 접근 방식을 채택하는 것은 도전을 야기합니다. 이는 각 요청이 배치 내에서 다른 시간에 종료 토큰을 생성하도록 하므로, 배치의 계산 속도가 가장 긴 생성 시간으로 제한되어 사용자에게 원치 않는 대기 시간(지연)이 발생합니다. 시퀀스 간 완료 시간의 차이로 인해 GPU 활용도가 감소하면 배치를 통해 기대한 효율성 향상이 떨어질 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_9.png\" />\n\n우리가 이야기한 모든 도전에 대해 해결책으로 지속적인 배치 처리가 제안되었습니다.\n\n## 4.4. 연속 배치\n\n연속 배치는 LLM을 위해 특별히 설계된 일종의 배치 스케줄링 방법입니다. 동적 배치와 비교하면 배치 크기가 구성된 시간 임계값과 최대 배치 크기에 따라 동적으로 결정되는 것과 달리, 연속 배치는 새 요청이 현재 배치에 참여할 수 있게 하고, 현재 배치가 끝나기를 기다리는 대신 다음 디코더 주기에 새로운 요청이 현재 배치에 합류할 수 있게 합니다. LLM의 자기 회귀 생성 프로세스로 인해, 이 방법은 LLM에 쉽게 적용되며 모델의 처리량을 크게 증가시킵니다.\n\n<div class=\"content-ad\"></div>\n\n<div>\n  <img src=\"/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_10.png\" />\n</div>\n\n연속 배치 처리는 요청을 동적으로 일괄 처리하는 데 좋습니다. 그러나 또 다른 문제에 직면합니다: 메모리 한계입니다. 챗봇 시나리오를 고려해보면, 한 사용자는 한 문장으로 질문을 하고 다른 사용자는 단락을 애플리케이션에 보낼 수 있습니다. 입력(및 출력) 시퀀스의 길이를 가정할 수 없습니다. 이 불확실성으로 인해 메모리 소비의 심각한 문제에 직면하게 됩니다. 시퀀스의 정확한 메모리 요구 사항을 알지 못하는 경우, 최악의 경우 시나리오를 채택하여 전체 배치에 최대한 많은 메모리를 예약해야 합니다.\n\n여기 문제가 있습니다: GPU는 한정된 메모리를 가지고 있으며,\n\n- 모델 매개변수 및\n- 사용자 요청 계산(KV 캐시)\n- 전체 배치 계산을 위한 공간이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n최적화 없이는 많은 공간을 차지해서 배치 크기를 축소해야 하고 따라서 처리량을 줄여야 하는 상황이 생깁니다. 하지만 우리는 높은 처리량을 원해요. 이것을 어떻게 최적화할까요? 메모리가 열쇠인 것 같아요.\n\n메모리적인 측면에서 디코딩 과정이 어떻게 진행되는지 깊게 살펴봅시다. LLMs의 생성 과정은 입력 시퀀스를 처리하고 다음 토큰을 자기회귀적으로 한 번에 하나씩 생성하는 과정으로 시작합니다 (아래 그림 참조). 이 생성 과정에는 각 토큰에 대한 모든 키-값 (KV) 점수 계산이 필요한 self-attention 계산이 포함됩니다. 예를 들어, 토큰 t를 생성하려면 지금까지 처리된 토큰 t-1, t-2,….1의 계산된 키와 값이 필요합니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1112/0*ahuLWNYPfU3Wl2Fl.gif)\n\n순환적인 연산을 최적화하기 위해 KV 캐싱 개념이 도입되었습니다. 이 방법은 디코더에 있는 이전에 계산된 K 및 V 텐서를 저장하고 이를 다음 반복에서 재사용하는 것을 목표로 합니다. 그러나 이 최적화 전략은 처리량을 높이려고 배치 크기가 큰 경우에 매우 중요한 메모리 공간을 늘리는 비용이 발생합니다. 예측 불가한 시퀀스 길이로 인해 도전이 증가하면서 전통적인 주의 메커니즘이 메모리 낭비로 이어지게 됩니다 - 조각화와 과다 할당으로 인해 60%에서 80%에 달할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 4.5. PagedAttention: 메모리 중심적인 해결책\n\n이 문제를 극복하기 위해 PagedAttention이 제안되었습니다. PagedAttention은 메모리 단편화와 공유를 관리하는 전통적인 운영 체제(OS) 전략에서 영감을 받았습니다. PagedAttention은 페이징과 함께 가상 메모리 접근 방식을 사용합니다. 이를 통해 키와 값 벡터를 비연속 메모리 공간에 저장할 수 있습니다. 이는 키와 값 벡터가 비연속 메모리 공간에 거주할 수 있도록 하며, 블록으로 구성됩니다. 각 블록에는 일정 수의 토큰에 대한 주의 키와 값이 포함됩니다. 계산을 수행하는 동안 PagedAttention 커널은 블록을 효율적으로 식별하고 가져옵니다.\n\n## 4.6. 키-값 캐싱\n\n디코딩 단계를 위한 일반적인 최적화 방법 중 하나는 KV 캐싱입니다. 디코딩 단계는 각 시간 단계에서 단일 토큰을 생성하지만, 각 토큰은 이전 토큰들의 키 및 값 텐서에 종속됩니다(입력 토큰의 KV 텐서는 프리필에서 계산되며, 현재 시간 단계까지 계산된 모든 새로운 KV 텐서도 포함됩니다).\n\n<div class=\"content-ad\"></div>\n\n이 모든 텐서를 각 시간 단계마다 모든 토큰에 대해 재계산하는 것을 피하기 위해 GPU 메모리에 캐시하는 것이 가능합니다. 매 반복마다 새 요소가 계산될 때마다, 단순히 실행 중인 캐시에 추가되어 다음 반복에 사용됩니다. 일부 구현에서는 모델의 각 레이어에 대해 하나의 KV 캐시가 있습니다.\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_11.png)\n\nKV 캐싱과 PagedAttention에 대해 더 자세히 알아보려면 이 논문과 이 블로그를 참조해주세요.\n\n## 4.6.1. LLM 메모리 요구량\n\n<div class=\"content-ad\"></div>\n\n실제로 GPU LLM 메모리 요구 사항의 두 가지 주요 원인은 모델 가중치와 KV 캐시입니다.\n\n- 모델 가중치: 메모리는 모델 매개변수에 의해 차지됩니다. 예를 들어, 16비트 정밀도(FP16 또는 BF16)로 로드된 70억 개의 매개변수를 갖는 모델(Llama 27B와 같은 경우)는 대략 7B \\* sizeof(FP16) ~= 14GB의 메모리를 차지할 것입니다.\n- KV 캐싱: 메모리는 자체 어텐션 텐서의 캐싱으로 차지되어 중복 계산을 피합니다.\n\n배치 처리를 하더라도, 배치의 각 요청의 KV 캐시는 여전히 별도로 할당되어야 하며, 큰 메모리 풋프린트를 가질 수 있습니다. 아래의 공식은 오늘날 가장 일반적인 LLM 아키텍처에 적용 가능한 KV 캐시의 크기를 설명합니다.\n\n토큰 당 KV 캐시 크기(바이트) = 2 _ (num_layers) _ (num*heads * dim*head) * precision_in_bytes\n\n<div class=\"content-ad\"></div>\n\n2의 첫 번째 요인은 K 및 V 행렬을 고려합니다. 일반적으로 (num_heads \\* dim_head) 값은 transformer의 hidden_size (또는 모델의 차원, d_model)와 동일합니다. 이러한 모델 속성은 일반적으로 모델 카드나 관련 구성 파일에서 찾을 수 있습니다.\n\n이 메모리 크기는 입력 시퀀스의 각 토큰에 대해 필요합니다. 반 정밀도를 가정하면 KV 캐시의 총 크기는 아래 공식에 의해 결정됩니다.\n\n바이트 단위 KV 캐시의 총 크기 = (batch*size) * (sequence*length) * 2 _ (num_layers) _ (hidden_size) \\* sizeof(FP16)\n\n예를 들어, 16비트 정밀도에서 Llama 2 7B 모델과 배치 크기가 1일 때, KV 캐시의 크기는 1 _ 4096 _ 2 _ 32 _ 4096 \\* 2 바이트가 됩니다. 이는 약 2GB입니다.\n\n<div class=\"content-ad\"></div>\n\nKV 캐시를 효율적으로 관리하는 것은 도전적인 일입니다. 배치 크기와 시퀀스 길이에 따른 선형적인 증가로 인해 메모리 요구 사항이 빠르게 확장될 수 있습니다. 결과적으로 제공할 수 있는 처리량을 제한하고 장기간 컨텍스트 입력에 대한 도전을 일으킬 수 있습니다. 이것이 이 게시물에 소개된 여러 최적화의 동기입니다.\n\n# 5. 모델 병렬화를 통한 LLM 확장\n\n모델 가중치의 장치당 메모리 풋프린트를 줄이는 한 가지 방법은 모델을 여러 GPU로 분산하는 것입니다. 메모리 및 계산 풋프린트를 분산시킴으로써 더 큰 모델 또는 더 큰 입력 배치를 실행할 수 있게됩니다. 모델 병렬화는 단일 장치에서 사용 가능한 메모리보다 더 많은 메모리를 필요로 하는 모델을 훈련하거나 추론하는 데 필수적이며, 일부 사용 사례에 적합한 훈련 시간 및 추론 측정값(지연 시간 또는 처리량)을 만들기 위함입니다. 모델의 가중치를 어떻게 분할하는 기반에 따라 모델을 병렬화하는 여러 방법이 있습니다.\n\n데이터 병렬화도 자주 언급되는 기술이며 아래 나열된 다른 기술과 동일한 맥락에서 자주 언급됩니다. 여기서 모델의 가중치가 여러 장치로 복사되고 입력의 (전역) 배치 크기가 각 장치로 분할되어 마이크로배치로 처리됩니다. 이로써 더 큰 배치를 처리함으로써 전반적인 실행 시간을 줄일 수 있습니다. 그러나 이는 훈련 시간에 대한 최적화로서, 추론 중에는 덜 관련성이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 5.1. 파이프라인 병렬 처리\n\n파이프라인 병렬 처리는 모델을 청크(수직으로)로 분할하여 각 청크가 별도의 장치에서 실행되는 방식을 의미합니다. 그림 2a는 4배 파이프라인 병렬 처리를 보여주며, 모델이 순차적으로 분할되고 모든 레이어의 1/4 하위 집합이 각 장치에서 실행됩니다. 한 장치에서 실행되는 연산 그룹의 출력은 다음 장치로 전달되어 이어지는 청크를 계속 실행합니다. Fn 및 Bn은 각각 장치 n에서 순방향 및 역방향 전파를 나타냅니다. 각 장치에 모델 가중치를 저장하는 데 필요한 메모리 요구 사항이 사실상 1/4로 줄어듭니다.\n\n이 방법의 주요 한계는 처리의 순차적 특성 때문에 일부 장치 또는 레이어가 이전 레이어의 출력(활성화, 그래디언트)을 기다리는 동안 대기 상태에 있을 수 있다는 점입니다. 이는 순방향 및 역방향 전파 모두에서 비효율성 또는 \"파이프라인 버블\"로 이어집니다. 그림 2b에서 백색 빈 영역은 장치가 비활성화되고 underutilized된 순진한 파이프라인 병렬 처리의 큰 파이프라인 버블입니다.\n\n마이크로 배칭을 통해 일부 문제를 완화할 수 있습니다. 그림 2c에서 보여지는 것처럼, 입력의 전체 배치 크기를 서브 배치로 분할하여 하나씩 처리하고 그레이디언트는 마지막에 축적됩니다. Fn,m 및 Bn,m은 디바이스 n에서 마이크로배치 m에 대해 순방향 및 역방향 전파를 각각 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n. 이 방식은 파이프라인 버블의 크기를 줄이지만 완전히 제거하지는 않습니다.\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_12.png)\n\n## 5.2. 텐서 병렬 처리\n\n텐서 병렬 처리는 모델의 개별 레이어를 더 작고 독립적인 계산 블록으로 분할(수평으로)하는 것을 포함합니다. 어텐션 블록과 다층 퍼셉트론(MLP) 레이어는 텐서 병렬 처리를 활용할 수 있는 트랜스포머의 주요 구성 요소입니다. 다중 헤드 어텐션 블록에서 각 헤드 또는 헤드 그룹은 서로 다른 장치에 할당되어 독립적으로 병렬적으로 계산될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Figure 3a](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_13.png)\n\n두 층으로 구성된 MLP의 두방향 텐서 병렬성 예시가 그림 3a에 나와 있습니다. 각 층은 라운드 상자로 표현됩니다. 첫 번째 층에서는 가중치 행렬 A가 A1과 A2로 분할됩니다.\n\n계산 XA1과 XA2는 두 개의 다른 장치에서 같은 배치 (f는 항등 연산)의 입력 X에서 독립적으로 실행될 수 있습니다. 이로써 각 장치에 가중치를 저장하는 메모리 요구량이 절반으로 줄어듭니다. 제 2층에서는 출력을 결합하는 축소 연산 g가 수행됩니다.\n\n그림 3b는 셀프 어텐션 층에서의 두방향 텐서 병렬성의 예시입니다. 여러 어텐션 헤드는 본질적으로 병렬이며 장치 간에 분할될 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 5.3. 시퀀스 병렬성\n\n텐서 병렬화에는 제한이 있습니다. 이는 레이어를 독립적이고 관리 가능한 블록으로 분할해야 하는 것을 요구하기 때문입니다. 이러한 방법은 LayerNorm 및 Dropout과 같은 작업에 적용되지 않습니다. 대신에 LayerNorm 및 Dropout은 텐서 병렬 그룹 전체에 복제됩니다. LayerNorm 및 Dropout은 계산 비용이 저렴하지만 (중복적인) 활성화를 저장하기 위해 상당한 양의 메모리가 필요합니다.\n\n대형 Transformer 모델에서 활성화 재계산을 줄이는 것을 보여준 Reducing Activation Recomputation in Large Transformer Models에 표시된대로 이러한 작업은 입력 시퀀스 전체에서 독립적이며 이러한 작업은 그 \"시퀀스 차원\"을 따라 분할될 수 있습니다. 이것이 시퀀스 병렬성이라고 불립니다.\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_14.png)\n\n<div class=\"content-ad\"></div>\n\n모델 병렬화 기술은 상호 배타적이지 않으며 함께 사용할 수 있습니다. 이러한 기술들은 LLM의 규모를 조정하고 GPU 당 메모리 사용량을 줄이는 데 도움이 될 수 있지만, 주의 메커니즘을 위한 최적화 기술도 있습니다.\n\n# 6. 주의 메커니즘 최적화\n\n스케일된 닷 프로덕트 어텐션(SDPA) 연산은 쿼리와 키-값 쌍을 출력으로 매핑합니다. Attention Is All You Need에서 설명된 것과 같습니다.\n\n## 6.1. 멀티 헤드 어텐션\n\n<div class=\"content-ad\"></div>\n\nSDPA의 향상으로, 다양한 학습된 Q, K, 및 V 행렬의 투영과 함께 주의 층을 병렬로 여러 번 실행하면 모델이 서로 다른 위치의 다른 표현 공간에서 정보에 동시에 주의를 기울이도록 할 수 있습니다. 이러한 표현 공간은 독립적으로 학습되어 입력에서 서로 다른 위치에 대한 더 풍부한 이해를 제공합니다.\n\n그림 5에 나타난 것처럼, 여러 병렬 주의 작업에서의 출력은 연결되어 결합하고 선형적으로 투영하여 결합됩니다. 각각의 병렬 주의 층을 '헤드'라고 하며, 이 접근법을 다중 헤드 주의(Multi-Head Attention, MHA)라고 합니다.\n\n원본 작업에서 각 주의 헤드는 모델의 감소된 차원에서 작동합니다(예:\n\n<div class=\"content-ad\"></div>\n\n) 병렬 어텐션 헤드를 사용할 때 가중치를 변경합니다. 이로 인해 계산 비용이 단일 헤드 어텐션과 유사해집니다.\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_16.png)\n\n## 6.2. Multi-query attention\n\nMHA의 추론 최적화 중 한 가지인 Multi-query attention (MQA)은 Fast Transformer Decoding에서 제안된 것으로, 여러 개의 어텐션 헤드 사이에서 키(key)와 값(value)을 공유합니다. 쿼리 벡터는 여전히 이전과 같이 여러 번으로 분할되어 투영됩니다.\n\n<div class=\"content-ad\"></div>\n\nMQA에서 수행되는 계산 양은 MHA와 동일하지만, 메모리에서 읽히는 데이터(키, 값) 양은 이전의 일부분입니다. 메모리 대역폭에 의해 제한될 때, 더 나은 계산 활용이 가능해집니다. 또한 이로 인해 메모리 내 KV-캐시 크기가 줄어들어 더 큰 배치 크기의 공간이 확보됩니다.\n\n키-값 헤드의 감소는 잠재적인 정확도 하락과 함께 옵니다. 또한, 추론 시 이 최적화를 활용해야 하는 모델들은 MQA를 활성화한 채로 훈련해야 합니니다(또는 최소한 훈련 양의 ~5%로 재조정).\n\n## 6.3. 그룹 쿼리 어텐션\n\n그룹 쿼리 어텐션(GQA)은 MHA와 MQA 사이의 균형을 유지하면서 키와 값을 몇 개의 쿼리 헤드 그룹에 투영함으로써 동작합니다(Figure 6). 각 그룹 내에서는 다중 쿼리 어텐션처럼 동작합니다.\n\n<div class=\"content-ad\"></div>\n\n그림 6은 멀티 헤드 어텐션에 여러 개의 키-값 헤드를 보여줍니다(왼쪽). 그룹화된 쿼리 어텐션(가운데)은 하나 이상의 키-값 헤드를 가지지만 쿼리 헤드 수보다 적습니다. 이는 메모리 요구량과 모델 품질 사이의 균형을 이룬 것입니다. 멀티 쿼리 어텐션(오른쪽)은 메모리를 절약하는 데 도움이 되기 위해 단일 키-값 헤드를 가지고 있습니다.\n\nMHA로 초기에 학습된 모델은 일부 원본 학습 컴퓨트의 일부분을 사용하여 GQA로 \"업트레이닝\"할 수 있습니다. 이를 통해 MHA에 근접한 품질을 달성할 수 있으면서도 MQA에 더 가까운 계산 효율성을 유지할 수 있습니다. Llama 2 70B는 GQA를 활용한 모델의 예시입니다.\n\nMQA 및 GQA와 같은 최적화는 저장된 키-값 헤드의 수를 줄이는 방식으로 KV 캐시에 필요한 메모리를 줄이는 데 도움을 줍니다. 그러나 여전히 이 KV 캐시가 어떻게 관리되는지에 비효율성이 남아 있을 수 있습니다. 어텐션 모듈 자체를 최적화하는 것과는 다른 방식으로, 다음 섹션에서는 보다 효율적인 KV 캐시 관리 기술을 제시합니다.\n\n<div class=\"content-ad\"></div>\n\n## 6.4. 플래시 어텐션\n\n주의 매커니즘을 최적화하는 또 다른 방법은 특정 계산의 순서를 수정하여 GPU의 메모리 계층 구조를 더 잘 활용하는 것입니다. 신경망은 일반적으로 레이어로 설명되며 대부분의 구현도 이와 같이 배치되어 있습니다. 한 번에 한 유형의 계산이 순서대로 입력 데이터에서 수행됩니다. 이는 항상 최적의 성능으로 이어지지는 않습니다. 왜냐하면 이미 더 높은, 더 성능이 우수한 수준의 메모리 계층으로 가져온 값에 대해 더 많은 계산을 수행하는 것이 유익할 수 있기 때문입니다.\n\n실제 계산 중에 여러 레이어를 함께 퓨징하여 GPU가 메모리에서 읽고 쓰는 횟수를 최소화하고 동일한 데이터를 필요로하는 계산을 그룹화하는데 도움이 될 수 있습니다. 신경망의 서로 다른 레이어의 일부이더라도 필요로 하는 경우에도 이점이 있습니다.\n\n가장 인기 있는 퓨전 중 하나는 FlashAttention으로, IO-Awareness로 자세히 설명된 정확한 주의 알고리즘입니다. 정확한 주의란 표준 다중 헤드 주의와 수학적으로 동일하며(다중 쿼리 및 그룹화된 쿼리 주의에 사용 가능한 변형이 있습니다), 이미 존재하는 모델 아키텍처나 이미 학습된 모델에 수정 없이 교체 가능합니다.\n\n<div class=\"content-ad\"></div>\n\nI/O aware는 연산을 함께 퓨전할 때 이전에 논의한 일부 메모리 이동 비용을 고려하는 것을 의미합니다. 특히 FlashAttention은 \"tiling\"을 사용하여 최종 행렬의 작은 부분을 한꺼번에 완전히 계산하고 쓰기 때문에 전체 행렬에서 계산 일부를 순차적으로 수행하고 중간 값을 기록하는 대신 특정 부분을 계산합니다.\n\nFigure 7는 타일 형식의 FlashAttention 계산 패턴과 40 GB GPU에서의 메모리 계층 구조를 보여줍니다. 오른쪽 차트는 Attention 메커니즘의 다른 구성 요소를 퓨즈하고 재배열함으로써 얻는 상대적 가속도를 보여줍니다.\n\n<img src=\"/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_18.png\" />\n\n## 6.5. 페이징을 사용한 KV 캐시의 효율적인 관리\n\n<div class=\"content-ad\"></div>\n\n가끔 KV 캐시는 입력의 크기가 예측하기 어려운 경우를 대비하여 정적으로 \"과잉 할당\"될 수 있습니다. (지원되는 시퀀스 길이) 예측할 수 없기 때문입니다. 예를 들어, 모델의 지원되는 최대 시퀀스 길이가 2,048이라면 요청에서 생성된 입력의 크기와 출력의 크기와 관계없이 2,048의 크기로 메모리에 예약됩니다. 이 공간은 연속적으로 할당될 수 있으며 종종 그 중 많은 부분이 사용되지 않아 메모리 낭비나 단편화를 야기할 수 있습니다. 이 예약된 공간은 요청의 수명 동안 사용됩니다.\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_19.png)\n\n운영 체제의 페이징에서 영감을 받아 PagedAttention 알고리즘은 주어진 요청의 KV 캐시를 연속된 키 및 값이 연속되지 않는 공간에 저장할 수 있게 합니다. 각 요청의 KV 캐시를 일정 수의 토큰을 나타내는 블록으로 분할하여 연속함 없이 저장할 수 있습니다.\n\n이러한 블록들은 필요한 경우 주의 계산 중에 블록 테이블을 사용하여 가져옵니다. 새로운 토큰이 생성될 때마다 새로운 블록이 할당됩니다. 이러한 블록의 크기는 고정되어 있어 서로 다른 요청이 서로 다른 할당을 필요로 할 때 발생하는 효율성 문제를 제거합니다. 이는 메모리 낭비를 크게 제한하여 더 큰 배치 크기(따라서 처리량)를 가능하게 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 7. 모델 최적화 기술\n\n지금까지 우리는 LLMs가 메모리를 사용하는 다양한 방법, 메모리를 여러 다른 GPU에 분산시킬 수 있는 방법 중 일부, 그리고 어텐션 메커니즘 및 KV 캐시를 최적화하는 방법에 대해 논의했습니다. 각 GPU의 메모리 사용량을 줄이기 위해 모델 가중치 자체를 수정함으로써 모델 최적화 기술도 여러 가지 있습니다. 또한 GPU에는 이러한 수정된 값에 대한 연산을 가속화하기 위한 전용 하드웨어도 있어 모델에 대한 속도 향상이 더욱 가능해집니다.\n\n## 7.1. 양자화\n\n양자화는 모델의 가중치와 활성화의 정밀도를 줄이는 과정입니다. 대부분의 모델은 32 또는 16비트의 정밀도로 훈련되며, 각 매개변수 및 활성화 요소는 32 또는 16비트의 메모리를 사용합니다. 그러나 대부분의 딥러닝 모델은 값 당 여덟 비트 이하로도 효과적으로 표현될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nFigure 9은 양자화 방법 중 하나를 사용하여 값들의 분포를 보여줍니다. 이 경우, 반올림으로 일부 정밀도가 손실되고 클리핑으로 일부 동적 범위도 손실되어 값들을 훨씬 작은 형식으로 표현할 수 있게 됩니다.\n\n정밀도를 낮추면 모델의 여러 이점을 얻을 수 있습니다. 모델이 메모리에 더 적은 공간을 차지하면 같은 양의 하드웨어에 더 큰 모델을 맞출 수 있습니다. 또한 양자화는 같은 대역폭을 통해 더 많은 매개변수를 전송할 수 있게 하므로 대역폭 제한된 모델의 가속화에 도움이 될 수 있습니다.\n\nLLM에 대한 여러 다양한 양자화 기술이 있으며 활성화, 가중치 또는 두 가지에서 정밀도를 줄입니다. 훈련 후 가중치는 고정되기 때문에 가중치를 양자화하는 것이 훨씬 간단합니다. 그러나 이 경우 활성화가 여전히 더 높은 정밀도를 유지하므로 일부 성능이 손실될 수 있습니다. GPU에는 INT8 및 FP16 수를 곱하는 전용 하드웨어가 없기 때문에 실제 연산을 위해 가중치를 더 높은 정밀도로 변환해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n활성화, 트랜스포머 블록 및 네트워크 레이어의 입력을 양자화하는 것도 가능하지만, 이는 고유의 도전과제가 있습니다. 활성화 벡터에는 종종 이상값이 포함되어 있어 동적 범위가 확대되고 낮은 정밀도로 이러한 값을 표현하는 것이 더 어려워집니다.\n\n한 가지 옵션은 대표적인 데이터 세트를 모델을 통해 전달하여 어디서 이상값이 나타날 가능성이 높은지 찾아내고, 일부 활성화를 다른 활성화보다 더 높은 정밀도로 표현하기로 선택하는 것(LIM.int8())입니다. 다른 하나의 옵션은 양자화하기 쉬운 가중치의 동적 범위를 빌려서 그 범위를 활성화에 재사용하는 것입니다.\n\n## 7.2. 희소성\n\n양자화와 유사하게, 많은 딥러닝 모델이 희소화 또는 0에 가까운 특정 값들을 0으로 대체하는 것에 견고하다는 것이 입증되었습니다. 희소 행렬은 많은 요소가 0인 행렬입니다. 이들은 완전하고 밀도 있는 행렬보다 공간을 적게 차지하는 조밀한 형태로 표현될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![그림](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_21.png)\n\n특히 GPU는 특정 유형의 구조화 미흡성에 대한 하드웨어 가속화를 가지고 있습니다. 여기서 4개 중 2개의 값이 0으로 표시됩니다. 희소 표현은 양자화와 결합하여 실행 속도를 더욱 빠르게 할 수 있습니다. 대형 언어 모델을 희소 형식으로 효과적으로 표현하는 방법을 찾는 것은 아직 연구 중이며, 향후 추론 속도를 개선하기 위한 유망한 방향을 제시합니다.\n\n## 7.3. 증류\n\n모델 크기를 줄이는 또 다른 접근 방법은 지식을 더 작은 모델로 이전하는 과정인 증류(distillation)를 통해 이루어집니다. 이 과정은 더 작은 모델(학생)을 훈련하여 더 큰 모델(선생)의 동작을 모방하도록 하는 것을 포함합니다.\n\n<div class=\"content-ad\"></div>\n\n성공적인 distilled 모델의 예로는 DistilBERT가 있습니다. DistilBERT는 BERT 모델을 40% 압축하여 속도를 60% 빠르게 유지하면서 원본의 97% 언어 이해 능력을 유지합니다.\n\nLLM에서의 증류는 활발한 연구 분야이며, 일반적인 접근 방식은 신경망의 지식을 증류하는 것이 Distilling the Knowledge in a Neural Network에서 처음으로 기술되었습니다:\n\n- 학생 신경망은 더 큰 교사 신경망의 성능을 모방하도록 훈련되며, 출력 간의 차이를 측정하는 손실 함수를 사용합니다. 이 목표는 학생의 출력을 원본 참 값 레이블과 일치시키는 원래의 손실 함수를 포함할 수도 있습니다.\n- 매치되는 교사의 출력은 매우 마지막 레이어(logits) 또는 중간 레이어 활성화일 수 있습니다.\n\n도식 11은 지식 증류를 위한 일반적인 프레임워크를 보여줍니다. 교사의 logits는 소프트 타깃이며, 학생은 증류 손실을 사용하여 최적화합니다. 다른 증류 방법은 교사로부터 지식을 \"증류\"하기 위해 다른 손실 측도를 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_22.png)\n\n교사가 학습한 데이터를 사용하여 학습 중인 학생 LLM을 위한 지도 학습을 하는 대체 접근 방식이 있습니다. 특히 인간 주석이 부족하거나 없을 때 유용합니다. 'Distilling Step by Step!'은 지도 LLM에서 레이블 이외에 교사 LLM에서 이유를 추출하여 중간 추론 단계로 사용합니다. 이 이유는 더 작은 학생 LLM을 데이터 효율적인 방법으로 훈련하는 데 중요합니다.\n\n오늘날 많은 최신 LLM은 다른 LLM을 훈련하는 데 그들의 출력을 사용하는 것을 금지하는 제한적인 라이선스를 갖고 있어 적합한 교사 모델을 찾는 것이 어려울 수 있습니다.\n\n# 8. 모델 서빙 기술\n\n<div class=\"content-ad\"></div>\n\n모델 실행은 종종 메모리 대역폭에 제한을 받습니다. 특히, 가중치 부분에서 대역폭이 제한됩니다. 이전에 설명한 모든 모델 최적화를 적용한 후에도 메모리 제한을 받을 가능성이 여전히 높습니다. 그러므로 모델 가중치를 로드할 때 가능한 한 많이 처리하고 싶을 것입니다. 다시 말해, 병렬로 작업을 수행하려고 할 것입니다. 두 가지 접근 방식을 취할 수 있습니다:\n\n- In-flight batching은 동시에 여러 다른 요청을 실행하는 것을 포함합니다.\n- Speculative inference은 일련의 여러 단계를 병렬로 실행하여 시간을 절약하려는 것입니다.\n\n## 8.1. In-flight batching\n\nLLM은 실제로 요청을 효과적으로 일괄 처리하기 어렵게 만들 수 있는 독특한 실행 특성을 갖고 있습니다. 단일 모델을 동시에 여러 가지 매우 다른 작업에 사용할 수 있습니다. 챗봇에서 간단한 질문-답변 응답부터 문서 요약 또는 긴 프로그램 코드 생성까지, 작업 부하는 출력이 몇 차례의 크기로 변동하는 매우 동적인 특성을 갖습니다.\n\n<div class=\"content-ad\"></div>\n\n다양성 있는 특성은 신경망 서비스를 제공할 때 일괄 요청을 묶어 병렬로 실행하는 것이 어렵게 만들 수 있습니다. 이는 일부 요청이 다른 요청보다 훨씬 빨리 완료될 수 있다는 결과를 초래할 수 있습니다.\n\n이러한 동적로드를 관리하기 위해 많은 LLM 서비스 솔루션에는 연속 또는 비행 중 일괄 처리라는 최적화된 스케줄링 기술이 포함되어 있습니다. 이 기술은 LLM의 전체 텍스트 생성 프로세스가 모델에서 실행되는 여러 반복 단계로 분해될 수 있다는 장점을 활용합니다.\n\n비행 중 일괄 처리를 통해 전체 일괄이 완료되기를 기다리는 대신, 서버 런타임은 즉시 완료된 시퀀스를 일괄에서 제거합니다. 그런 다음 다른 요청이 아직 처리 중인 동안 새로운 요청을 실행하기 시작합니다. 비행 중 일괄 처리는 실제 사용 사례에서 전반적인 GPU 활용률을 크게 증가시킬 수 있습니다.\n\n## 8.2. 추론 생산화\n\n<div class=\"content-ad\"></div>\n\nspeculative inference 또는 추정 추론으로도 알려진 이 방법은 LLM 실행을 병렬화하는 다른 방법입니다. 일반적으로 GPT 스타일의 대형 언어 모델은 텍스트를 토큰 단위로 생성하는 자기회귀 모델입니다.\n\n생성된 모든 토큰은 앞에 나온 모든 토큰에 의존하여 맥락을 제공합니다. 이는 일반적인 실행에서 동일한 시퀀스에서 여러 토큰을 병렬로 생성하는 것이 불가능하다는 것을 의미합니다. n번째 토큰이 생성되기 전에 n+1을 생성할 수 없습니다.\n\n그림 12는 예상 추론의 예를 보여줍니다. 여기서 임시 모델이 병렬로 여러 미래 단계를 예측한 후 이를 확인하거나 거부합니다. 이 경우, 초안의 처음 두 예측 토큰은 수용되고, 마지막 토큰은 거부되어 삭제되며 생성이 계속됩니다.\n\n![예시 이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_23.png)\n\n<div class=\"content-ad\"></div>\n\n추측 샘플링은 해법을 제공합니다. 이 방법의 기본 아이디어는 \"더 저렴한\" 프로세스를 사용하여 여러 토큰으로 이루어진 초안을 생성한 다음, 필요한 실행 단계에서 \"추정\" 컨텍스트로 사용할 수 있는 저렴한 초안을 사용하여 본 \"검증\" 모델을 병렬로 여러 단계에서 실행하는 것입니다.\n\n검증 모델이 초안과 동일한 토큰을 생성하면 해당 토큰을 출력으로 수락해야 함을 알 수 있습니다. 그렇지 않으면 매칭되지 않는 첫 번째 토큰 이후의 모든 것을 폐기하고 새로운 초안으로 프로세스를 반복할 수 있습니다.\n\n초안 토큰을 생성하는 여러 다양한 옵션이 있으며, 각각에는 다른 트레이드오프가 있습니다. 여러 모델을 훈련시키거나 미래의 여러 단계를 예측하는 단일 사전 훈련된 모델에서 여러 헤드를 세밀하게 조정할 수 있습니다. 또는 초안 모델로 작은 모델을 사용하고 검증자로 더 크고 더 능력 있는 모델을 사용할 수도 있습니다.\n\n# 9. LLM 제공을 위한 중요한 메트릭\n\n<div class=\"content-ad\"></div>\n\n그렇다면 추론 속도에 대해 어떻게 생각해야 할까요?\n\n우리는 LLM 서빙에 네 가지 주요 지표를 사용합니다:\n\n- 첫 번째 토큰까지 걸리는 시간 (TTFT): 사용자가 쿼리를 입력한 후 모델의 출력을 시작으로 볼 때 얼마나 빨리 나타나는지를 나타냅니다. 응답을 기다리는 시간이 짧을수록 실시간 상호작용에서 중요하지만 오프라인 워크로드에서는 그다지 중요하지 않습니다. 이 지표는 프롬프트를 처리하고 첫 번째 출력 토큰을 생성하는 데 필요한 시간에 의해 결정됩니다.\n- 출력 토큰 당 걸리는 시간 (TPOT): 시스템을 쿼리하는 각 사용자에 대해 출력 토큰을 생성하는 데 걸리는 시간입니다. 이 지표는 각 사용자가 모델의 \"속도\"를 어떻게 인식할지와 관련이 있습니다. 예를 들어, 100밀리초/토큰의 TPOT는 사용자 당 초당 10개의 토큰, 또는 대략 1분에 450단어를 읽을 수 있는 것입니다. 일반적인 사람이 읽는 속도보다 빠릅니다.\n- 대기 시간: 모델이 사용자에 대한 완전한 응답을 생성하는 데 걸리는 전반적인 시간입니다. 전반적인 응답 대기 시간은 이전 두 지표를 사용하여 계산할 수 있습니다: 대기 시간 = (TTFT) + (TPOT) \\* (생성해야 하는 토큰 수).\n- 처리량: 추론 서버가 모든 사용자와 요청에 대해 초당 생성할 수 있는 출력 토큰의 수입니다.\n\n# 10. 우리가 LLM을 제공하기 위해 무엇을 필요로 할까요?\n\n<div class=\"content-ad\"></div>\n\n가로줄에 있는 `<img>` 태그를 Markdown 형식으로 바꿔보세요.\n\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_24.png)\n\n\nLLM 기반 애플리케이션을 제공할 때 2가지 주요 구성 요소가 있습니다: 엔진과 서버입니다. 엔진은 모델 및 요청 일괄 처리에 관한 모든 것을 처리하며, 서버는 사용자 요청을 전달합니다.\n\n## 10.1. 엔진\n\n<div class=\"content-ad\"></div>\n\n엔진은 모델을 실행하고 세대 프로세스에 대한 다양한 종류의 최적화 기술에 대해 지금까지 다룬 내용입니다. 이러한 엔진은 Python 라이브러리로 구성되어 있습니다. 이들은 사용자로부터 오는 요청을 배치 처리하고 해당 요청에 대한 응답을 생성합니다.\n\n## 10.2. 서버\n\n서버는 사용자로부터 들어오는 HTTP/gRPC 요청을 조율하는 역할을 합니다. 실제 응용프로그램에서는 여러 사용자가 하루 중 다양한 시간에 챗봇에 질문을 하게 될 것입니다. 서버는 이러한 요청을 큐에 넣고 응담을 생성하기 위해 엔진으로 전달합니다. 서버는 또한 모델 서빙을 추적하기 위해 중요한 처리량 및 대기 시간과 같은 메트릭스를 가져옵니다.\n\n## 10.3. 기능들\n\n<div class=\"content-ad\"></div>\n\n엔진\n\n- 메모리 최적화\n- 모델별 최적화\n- 배칭 지원\n\n서버\n\n- HTTP/gRPC API\n- 요청 대기열\n- 멀티모델 서빙\n- 멀티 엔진 지원\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_25.png\" />\n\n지금까지 우리는 모델이 단일 요청을 처리하는 간단한 시나리오에 대해 논의했습니다. 그러나 현실 세계의 응용 프로그램은 수백 명, 심지어 수천 명의 사용자에게 동시에 서비스를 제공하는 능력을 요구합니다. 이제 우리의 초점은 PagedAttention을 사용한 요청 배치 및 메모리 최적화를 통해 비용 및 처리량을 최적화하는 비용 효율적 및 높은 처리량을 보장하는 모델 호스팅에 대한 다음 중요한 고려 사항으로 이동합니다. 이러한 최적화는 모델을 효율적으로 호스팅하고, 사용자 수요가 큰 경우 비용 효율성과 높은 처리량을 모두 보장하기 위한 중요한 역할을 합니다.\n\n# 11. LLM 서비스용 프레임워크\n\n이제 중요한 측정 지표, 트레이드 오프 및 기술들을 다룬 LLM 서비스에서의 중요한 도전 과제를 다루었습니다. 핵심 질문은 모든 이 기술을 어떻게 실현할까요? 어떤 도구가 우리의 요구 사항과 가장 잘 맞고, 프레임워크에 대해 알아두어야 할 것은 무엇일까요?\n\n<div class=\"content-ad\"></div>\n\n이 섹션에서는 산업에서 인기 있는 널리 사용되는 프레임워크들을 선택하여 업계의 주요 프레임워크에 대한 세부 정보를 살펴보며 벤치마킹 실험에서 얻은 주요 결과를 공유합니다. 각 프레임워크는 추론 도중 Large Language Models (LLMs)의 성능을 최적화하고 향상시키는 독특한 가치를 가지고 있습니다. 이러한 프레임워크를 서버와 엔진 두 그룹으로 분류했습니다. 최종적으로 입수한 도구들과 우리의 특정 LLM 서빙 요구 사항에 잘 맞는지에 대해 명확한 그림을 그릴 수 있을 것입니다.\n\n# 11.1. vLLM\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_26.png)\n\nLLM 추론과 서빙을 위한 빠르고 쉽게 사용할 수 있는 라이브러리입니다. HuggingFace Transformers (HF)보다 14배에서 24배 높은 처리량을 달성하며, HuggingFace Text Generation Inference (TGI)보다 2.2배에서 2.5배 높은 처리량을 달성합니다.\n\n<div class=\"content-ad\"></div>\n\n사용법\n\n오프라인 일괄 추론:\n\n```js\n# pip install vllm\nfrom vllm import LLM, SamplingParams\n```\n\n```js\nprompts = [\"Funniest joke ever:\", \"The capital of France is\", \"The future of AI is\"];\nsampling_params = SamplingParams((temperature = 0.95), (top_p = 0.95), (max_tokens = 200));\nllm = LLM((model = \"huggyllama/llama-13b\"));\noutputs = llm.generate(prompts, sampling_params);\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n# 출력값을 위한 반복문:\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```\n\nAPI 서버:\n\n```js\n# 서버 시작:\npython -m vllm.entrypoints.api_server --env MODEL_NAME=huggyllama/llama-13b\n```\n\n```js\n# 쉘에서 모델에 쿼리하기:\ncurl http://localhost:8000/generate \\\n    -d '{\n        \"prompt\": \"Funniest joke ever:\",\n        \"n\": 1,\n        \"temperature\": 0.95,\n        \"max_tokens\": 200\n    }'\n```\n\n<div class=\"content-ad\"></div>\n\n킬러 피처\n\n- 연속 배치 - 반복 단위 스케줄링, 배치 크기가 각 반복마다 결정됩니다. 배칭 덕분에 vLLM은 무거운 쿼리 부하 아래에서도 잘 작동할 수 있습니다.\n- PagedAttention - 클래식한 아이디어인 가상 메모리 및 운영 체제에서의 페이지 이용을 영감으로 한 어텐션 알고리즘입니다. 이것이 모델 가속의 비밀 무기입니다.\n\n장점\n\n- 텍스트 생성 속도 - 라이브러리를 사용하여 여러 실험을 진행했는데 결과에 만족했습니다. 지금까지 vLLM을 사용한 추론은 가장 빠른 옵션으로 높은 성능을 보이고 있습니다.\n- 고처리량 서빙 - 병렬 샘플링, 빔 서치 등 다양한 디코딩 알고리즘 포함.\n- OpenAI 호환 API 서버 - OpenAI API를 사용한다면, 엔드포인트의 URL만 교체하면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n제한 사항\n\n라이브러리는 사용자 친화적인 기능과 다양한 기능을 제공하지만 몇 가지 제한 사항을 발견했습니다:\n\n- 사용자 정의 모델 추가: 우리만의 모델을 통합할 수는 있지만, 해당 모델이 vLLM의 기존 모델과 비슷한 아키텍처를 사용하지 않는 경우 프로세스가 더 복잡해집니다. 예를 들어, Falcon 지원을 추가하기 위한 풀 리퀘스트를 발견했는데, 이는 꽤 어려운 작업으로 보였습니다.\n- 어댑터(LoRA, QLoRA 등) 지원 부족: 오픈 소스 LLM은 특정 작업에 맞게 세밀하게 조정될 때 중요한 가치를 지닙니다. 그러나 현재 구현에서는 모델과 어댑터 가중치를 별도로 사용하는 옵션이 없어서 이러한 모델을 효율적으로 활용하는 유연성이 제한됩니다.\n- 가중치 양자화의 결여: 가끔 LLM은 사용 가능한 GPU 메모리에 맞지 않을 수 있으며, 메모리 소비를 줄이는 것이 중요해질 수 있습니다.\n\n이것은 LLM 추론을 위한 가장 빠른 라이브러리입니다. 내부 최적화 덕분에 경쟁 상대들을 크게 능가합니다. 그러나 제한된 모델 범위를 지원하는 데 약점이 있습니다.\n\n<div class=\"content-ad\"></div>\n\nvLLM개발 로드맵은 여기를 참조해 주세요.\n\n# 11.2. 텍스트 생성 추론\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_27.png)\n\n텍스트 생성 추론 (TGI)은 대형 언어 모델 (LLM)의 배포 및 제공을 위한 툴킷입니다. TGI는 Llama, Falcon, StarCoder, BLOOM, GPT-NeoX 및 T5를 포함한 가장 인기있는 오픈 소스 LLM을 위한 고성능 텍스트 생성을 가능하게 합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_28.png)\n\n텍스트 생성 추론을 위한 Rust, Python 및 gRPC 서버입니다. HuggingFace에서 LLMs API 추론 위젯을 구동하는 데 사용됩니다.\n\n사용법\n\n도커를 사용하여 웹 서버 실행:\n\n\n<div class=\"content-ad\"></div>\n\n```bash\nmkdir data\ndocker run --gpus all --shm-size 1g -p 8080:80 \\\n-v data:/data ghcr.io/huggingface/text-generation-inference:0.9 \\\n  --model-id huggyllama/llama-13b \\\n  --num-shard 1\n```\n\nMake queries:\n\n```python\n# pip install text-generation\nfrom text_generation import Client\n```\n\n```python\nclient = Client(\"http://127.0.0.1:8080\")\nprompt = \"Funniest joke ever:\"\nprint(client.generate(prompt, max_new_tokens=17, temperature=0.95).generated_text)\n```\n\n<div class=\"content-ad\"></div>\n\n요약된 기능들\n\n- 내장 Prometheus 메트릭 — 서버 부하를 모니터하고 성능에 대한 통찰을 얻을 수 있습니다.\n- Flash-attention (및 v2) 및 Paged Attention을 사용한 추론을 위한 최적화된 트랜스포머 코드. 모든 모델이 이러한 최적화를 지원하는 것은 아닙니다. 드문 구조물과 작업할 경우 도전에 직면할 수 있습니다.\n\n장점들\n\n- 모든 의존성이 도커에 설치되어 있음 — 즉시 기계에서 작동하는 준비된 환경을 얻을 수 있습니다.\n- HuggingFace 모델에 대한 네이티브 지원 — 모델을 쉽게 실행하거나 HuggingFace Model Hub에서 사용할 수 있습니다.\n- 모델 추론을 제어: 프레임워크는 정확도 조절, 양자화, 텐서 병렬성, 반복 패널티 등을 포함한 다양한 옵션을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n제한 사항\n\n- 어댑터 지원 부재 — 어댑터를 사용하여 LLM을 배포할 수는 있지만 (이 비디오를 참고하시기를 권장합니다), 현재 그에 대한 공식 지원 또는 문서가 없음을 알려드립니다.\n- 소스 코드에서 컴파일해야 하는 필요성 (Rust + CUDA 커널) — Rust를 좋아하지만, 모든 데이터 과학팀이 익숙하지는 않아서 라이브러리에 사용자 정의 변경을 통합하는 것이 어려울 수 있습니다.\n- 미완성된 문서: 모든 정보는 프로젝트의 README에 있습니다. 기본적인 것은 다루고 있지만, 저는 Rust 언어와 연관된 작업을 할 때 추가 세부 정보를 문제나 소스 코드에서 찾아야 했던 경우가 있었습니다 (특히 저에게는 어려운 상황이었습니다).\n\n저는 이것이 경쟁에서 선두주자 중 하나라고 생각합니다. 라이브러리는 잘 작성되어 있으며, 모델 배포 중에는 최소한의 어려움만 겪었습니다. 만약 HuggingFace와의 네이티브 통합을 원한다면 이것을 고려해볼 가치가 있습니다. 프로젝트 팀이 최근에 라이선스를 변경했다는 점을 유의해 주세요.\n\nTGI 개발 로드맵은 여기에서 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 11.3. CTranslate2\n\n![image](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_29.png)\n\n10점 만점에 내 점수\n\nCTranslate2는 Transformer 모델을 사용한 효율적 추론을 위한 C++ 및 Python 라이브러리입니다.\n\n<div class=\"content-ad\"></div>\n\n사용 방법\n\n먼저 모델을 변환하세요:\n\n```js\npip install -qqq transformers ctranslate2\n```\n\n```js\n# 모델은 먼저 CTranslate2 모델 형식으로 변환되어야 합니다:\nct2-transformers-converter --model huggyllama/llama-13b --output_dir llama-13b-ct2 --force\n```\n\n<div class=\"content-ad\"></div>\n\n쿼리를 작성하세요:\n\n```js\nimport ctranslate2\nimport transformers\n```\n\n```js\ngenerator = ctranslate2.Generator(\"llama-13b-ct2\", (device = \"cuda\"), (compute_type = \"float16\"));\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"huggyllama/llama-13b\");\n```\n\n```js\nprompt = \"Funniest joke ever:\";\ntokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(prompt));\nresults = generator.generate_batch([tokens], (sampling_topk = 1), (max_length = 200));\ntokens = results[0].sequences_ids[0];\noutput = tokenizer.decode(tokens);\nprint(output);\n```\n\n<div class=\"content-ad\"></div>\n\n킬러 피처\n\n- CPU 및 GPU에서 빠르고 효율적인 실행 — 레이어 퓨전, 패딩 제거, 배치 재정렬, 인플레이스 연산, 캐싱 메커니즘 등의 내장 최적화 레이어 덕분에 추론 LLMs는 빨라지고 더 적은 메모리를 필요로 합니다.\n- 동적 메모리 사용 — 요청 크기에 따라 메모리 사용량이 동적으로 변경되지만 CPU 및 GPU의 캐싱 할당자 덕분에 여전히 성능 요구 사항을 충족합니다.\n- 다양한 CPU 아키텍처 지원 — 이 프로젝트는 x86–64 및 AArch64/ARM64 프로세서를 지원하며 이러한 플랫폼에 최적화된 여러 백엔드를 통합하고 있습니다: Intel MKL, oneDNN, OpenBLAS, Ruy 및 Apple Accelerate.\n\n장점\n\n- 병렬 및 비동기 실행 — 여러 배치를 병렬로 처리하고 GPU 또는 CPU 코어를 사용하여 비동기적으로 실행할 수 있습니다.\n- 빠른 캐싱 — 모델은 한 번 정적 프롬프트에서 실행되고 모델 상태가 캐싱되어 동일한 정적 프롬프트로의 미래 호출에 재사용됩니다.\n- 디스크 공간을 절약하는 경량화 — 양자화를 통해 모델의 디스크 사용량을 최소화하고 정확도 손실을 최소화할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n제약 사항\n\n- 내장된 REST 서버가 없습니다 — REST 서버를 여전히 실행할 수는 있지만, 로깅 및 모니터링 기능이 포함된 사전 제작된 서비스가 없어 아쉬웠습니다.\n- 어댑터 (LoRA, QLoRA 등)를 지원하지 않는 점.\n\n이 라이브러리는 매우 흥미로운 것 같아요. 개발자들이 GitHub의 릴리스와 커밋을 통해 활발히 작업 중이며, 응용 프로그램에 대한 정보성 있는 블로그 게시물도 공유하고 있습니다. 라이브러리의 다양한 최적화 기능은 인상적이며, 주요 하이라이트는 CPU에서 LLM 추론을 수행할 수 있는 능력입니다.\n\n# 11.4. DeepSpeed-MII\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_30.png\" />\n\n10 점 척도의 개인 평가\n\nMII는 DeepSpeed에 의해 구동되어 낮은 대기 시간과 고 처리량 추론을 가능하게 합니다.\n\n사용법\n\n<div class=\"content-ad\"></div>\n\n웹 서버를 실행하세요:\n\n```js\n# pip install deepspeed-mii를 사용하여 설치하지 마세요\n# git clone https://github.com/microsoft/DeepSpeed-MII.git\n# git reset --hard 60a85dc3da5bac3bcefa8824175f8646a0f12203\n# cd DeepSpeed-MII && pip install .\n# pip3 install -U deepspeed\n```\n\n```js\n# ... 그리고 CUDA 버전이 동일한지 확인하세요:\n# python -c \"import torch;print(torch.version.cuda)\" == nvcc --version\nimport mii\n```\n\n```js\nmii_configs = {\n  dtype: \"fp16\",\n  max_tokens: 200,\n  tensor_parallel: 1,\n  enable_load_balancing: False,\n};\nmii.deploy(\n  (task = \"text-generation\"),\n  (model = \"huggyllama/llama-13b\"),\n  (deployment_name = \"llama_13b_deployment\"),\n  (mii_config = mii_configs)\n);\n```\n\n<div class=\"content-ad\"></div>\n\n테이블 태그를 마크다운 형식으로 변경해주세요.\n\n<div class=\"content-ad\"></div>\n\n- 다중 레플리카에 대한 로드 밸런싱 - 많은 사용자를 처리하기에 매우 유용한 도구입니다. 로드 밸런서는 효율적으로 들어오는 요청을 다양한 레플리카 사이에 분배하여 애플리케이션 응답 시간을 향상시킵니다.\n- 비영속적 배포 - 업데이트가 대상 환경에 영구적으로 적용되지 않는 방식입니다. 리소스 효율성, 보안, 일관성, 그리고 관리 편의성이 중요한 시나리오에서 가치 있는 선택지입니다. 운영 오버헤드를 줄이면서 통제될 수 있고 표준화된 환경을 제공합니다.\n\n장점\n\n- 다양한 모델 저장소 - Hugging Face, FairSeq, EluetherAI 등 다양한 오픈 소스 모델 저장소를 통해 제공됩니다.\n- 지연 시간과 비용 절감 측정 - MII는 매우 비용이 많이 드는 언어 모델의 추론 비용을 크게 줄일 수 있습니다.\n- 네이티브 및 Azure 통합 - 마이크로소프트가 개발한 MII 프레임워크는 그들의 클라우드 시스템과 탁월한 통합을 제공합니다.\n\n제한 사항\n\n<div class=\"content-ad\"></div>\n\n- 공식 릴리스 부재 — 기능적 애플리케이션을 위한 적합한 커밋을 찾는 데 몇 시간이 걸렸어요. 일부 문서의 일부는 오래되었고 더 이상 관련성이 없어요.\n- 제한된 모델 수 — Falcon, LLaMA 2 등의 언어 모델을 지원하지 않아요. 실행할 수 있는 모델의 수가 제한적이에요.\n- 어댑터(LoRA, QLoRA 등) 지원 부재\n\n이 프로젝트는 커뮤니티에서 명성을 쌓은 신뢰할 수 있는 DeepSpeed 라이브러리를 기반으로 합니다. 안정성과 검증된 솔루션을 찾는다면, MII가 훌륭한 선택일 거에요. 제 실험에 따르면, 이 라이브러리는 단일 프롬프트를 처리하는 데 최고의 속도를 보여줍니다. 그럼에도 불구하고, 우리 시스템에 구현하기 전에 이 프레임워크를 우리 구체적인 작업에 테스트하는 것을 권유해요.\n\n# 11.5. OpenLLM\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_31.png)\n\n<div class=\"content-ad\"></div>\n\n개인적인 평가는 10점 척도로 합니다.\n\n운영 중인 대형 언어 모델 (LLMs)을 위한 오픈 플랫폼입니다.\n\n사용법\n\n웹 서버 실행:\n\n<div class=\"content-ad\"></div>\n\n\n```js\npip install openllm scipy\nopenllm start llama --model-id huggyllama/llama-13b \\\n  --max-new-tokens 200 \\\n  --temperature 0.95 \\\n  --api-workers 1 \\\n  --workers-per-resource 1\n```\n\n쿼리를 생성하십시오:\n\n```js\nimport openllm\n```\n\n```js\nclient = openllm.client.HTTPClient(\"http://localhost:3000\");\nprint(client.query(\"Funniest joke ever:\"));\n```\n\n<div class=\"content-ad\"></div>\n\n킬러 피쳐\n\n- 어댑터 지원 — 여러 어댑터를 하나의 배포된 LLM에 연결할 수 있습니다. 하나의 모델을 여러 특수화된 작업에 사용할 수 있는 것을 상상해보세요.\n- 런타임 구현 — PyTorch (pt), TensorFlow (tf), 또는 Flax (flax)와 같은 다양한 구현을 사용할 수 있습니다.\n- HuggingFace 에이전트 — HuggingFace에서 다양한 모델을 연결하고 LLM 및 자연어로 관리할 수 있습니다.\n\n장점\n\n- 뛰어난 커뮤니티 지원 — 라이브러리는 지속적으로 발전하며 새로운 기능이 추가됩니다.\n- 새 모델 통합 — 개발자는 우리 자체 모델을 추가하는 방법에 대한 가이드를 제공합니다.\n- 양자화 — OpenLLM은 bitsandbytes 및 GPTQ를 통해 양자화를 지원합니다.\n- LangChain 통합 — LangChain을 사용하여 원격 OpenLLM 서버와 상호 작용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n제한사항\n\n- 배치 처리 지원 부족 — 상당량의 메시지 스트림의 경우, 이는 응용 프로그램의 성능에서 병목 현상이 될 수 있습니다.\n- 내장된 분산 추론 부재 — 여러 GPU 장치에서 대형 모델을 실행하려면 추가로 OpenLLM의 서빙 구성 요소 Yatai를 설치해야 합니다.\n\n이 프레임워크는 다양한 기능을 갖추고 있는 좋은 도구입니다. 최소의 비용으로 유연한 애플리케이션을 구축할 수 있습니다. 문서에 완전히 다루어지지 않는 측면도 있겠지만, 이 라이브러리를 살펴보면 추가 기능에서 즐거운 놀라움을 발견할 가능성이 높습니다.\n\n# 11.6. Ray Serve\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_32.png)\n\n10점 만점에서 나의 개인 평가\n\nRay Serve는 온라인 추론 API를 구축하기 위한 확장 가능한 모델 서빙 라이브러리입니다. Serve는 프레임워크에 독립적이므로 딥 러닝 모델까지 모두 제공하는 단일 툴킷을 사용할 수 있습니다.\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_33.png)\n\n<div class=\"content-ad\"></div>\n\nRay AIR는 엔드 투 엔드 ML 개발을 가능하게 하며 MLOps 생태계의 다른 도구 및 라이브러리와 통합할 수 있는 다양한 옵션을 제공합니다.\n\n사용법\n\n웹 서버 실행:\n\n```js\n# pip install ray[serve] accelerate>=0.16.0 transformers>=4.26.0 torch starlette pandas\n# ray_serve.py\nimport pandas as pd\n```\n\n<div class=\"content-ad\"></div>\n\n\nimport ray\nfrom ray import serve\nfrom starlette.requests import Request\n\n\n\n@serve.deployment(ray_actor_options={\"num_gpus\": 1})\nclass PredictDeployment:\n    def __init__(self, model_id: str):\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n        import torch\n\n\n\nself.model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n\n\ndef generate(self, text: str) -> pd.DataFrame:\n        input_ids = self.tokenizer(text, return_tensors=\"pt\").input_ids.to(\n            self.model.device\n        )\n        gen_tokens = self.model.generate(\n            input_ids,\n            temperature=0.9,\n            max_length=200,\n        )\n        return pd.DataFrame(\n            self.tokenizer.batch_decode(gen_tokens), columns=[\"responses\"]\n        )\n\n\n<div class=\"content-ad\"></div>\n\n```js\n비동기 def __call__(self, http_request: Request) -> str:\n        json_request: str = await http_request.json()\n        return self.generate(prompt[\"text\"])\n```\n\n```js\ndeployment = PredictDeployment.bind((model_id = \"huggyllama/llama-13b\"));\n```\n\n```js\n# 그런 다음 CLI 명령에서 실행하십시오:\n# serve run ray_serve:deployment\n```\n\n쿼리를 실행하세요:\n\n<div class=\"content-ad\"></div>\n\n\n```js\nimport requests\n```\n\n```js\nsample_input = { text: \"Funniest joke ever:\" };\noutput = requests.post(\"http://localhost:8000/\", (json = [sample_input])).json();\nprint(output);\n```\n\nKiller features\n\n- Monitoring dashboard and Prometheus metrics — We can use the Ray dashboard to get a high-level overview of our Ray cluster and Ray Serve application’s states.\n- Autoscale across multiple replicas — Ray adjusts to traffic spikes by observing queue sizes and making scaling decisions to add or remove replicas.\n- Dynamic Request Batching — It is necessary when our model is expensive to use and we want to maximize the utilization of hardware.\n\n\n\n<div class=\"content-ad\"></div>\n\n장점\n\n- 포괄적인 문서 — 개발자들이 이 측면에 시간을 할애하고 문서 작성에 성실히 접근해 준 것에 감사드립니다. 거의 모든 사용 사례에 대한 다양한 예제를 찾을 수 있어 매우 도움이 됩니다.\n- 프로덕션 준비 — 내 의견으로는 이 리스트에 나열된 모든 프레임워크 중에서 가장 성숙한 프레임워크입니다.\n- 네이티브 LangChain 통합 — LangChain을 사용하여 원격 Ray 서버와 상호 작용할 수 있습니다.\n\n단점\n\n- 내장된 모델 최적화 부족 — Ray Serve는 LLM에 초점을 맞추지 않고, 모든 ML 모델을 배포하는 보다 폭넓은 프레임워크입니다. 최적화를 직접 해야 합니다.\n- 높은 진입 장벽 — 라이브러리가 때로는 추가 기능으로 과부하되어 진입 임계값을 높이며, 새로운 사용자가 이를 탐색하고 이해하는 데 어려움을 겪게 만듭니다.\n\n<div class=\"content-ad\"></div>\n\n만약 딥 러닝에 관한 것 뿐만 아니라 가장 실전 준비가 된 솔루션이 필요하다면, Ray Serve가 좋은 선택지입니다. 이는 기업에서 가용성, 확장성 및 관측 가능성이 중요한 상황에 가장 적합합니다. 또한 데이터 처리, 훈련, 세밀 튜닝 및 서빙을 위한 방대한 생태계를 활용할 수 있습니다. 마지막으로, 이는 OpenAI부터 Shopify 및 Instacart까지 다양한 회사에서 사용하고 있습니다.\n\n# 11.7. MLC LLM\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_34.png)\n\n10점 척도의 개인적인 평가\n\n<div class=\"content-ad\"></div>\n\nMLC LLM은 모든 사용자 장치에서 효율적으로 실행되도록 하여 LLM이 기본 하드웨어 가속을 활용할 수 있게 하는 범용 배포 솔루션입니다.\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_35.png)\n\nMLC LLM의 고수준 프로젝트 개념\n\n사용법\n\n<div class=\"content-ad\"></div>\n\n웹 서버를 실행하려면:\n\n```js\n# 1. Python 버전이 3.9 이상인지 확인하세요.\n# 2. conda를 사용하여 실행해야 합니다:\nconda create -n mlc-chat-venv -c mlc-ai -c conda-forge mlc-chat-nightly\nconda activate mlc-chat-venv\n```\n\n```js\n# 3. 그런 다음 패키지를 설치하세요:\npip install --pre --force-reinstall mlc-ai-nightly-cu118 \\\n  mlc-chat-nightly-cu118 \\\n  -f https://mlc.ai/wheels\n```\n\n```js\n# 4. HuggingFace에서 모델 가중치 및 바이너리 라이브러리를 다운로드하세요:\ngit lfs install && mkdir -p dist/prebuilt && \\\n  git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib && \\\n  cd dist/prebuilt && \\\n  git clone https://huggingface.co/huggyllama/llama-13b dist/ && \\\n  cd ../..\n\n\n# 5. 서버를 실행하세요:\npython -m mlc_chat.rest --device-name cuda --artifact-path dist\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n쿼리 생성:\n\nimport requests\n```\n\n```js\n페이로드 = {\n  model: \"lama-30b\",\n  messages: [{ role: \"user\", content: \"Funniest joke ever:\" }],\n  stream: False,\n};\nr = requests.post(\"http://127.0.0.1:8000/v1/chat/completions\", (json = payload));\nprint(r.json()[\"choices\"][0][\"message\"][\"content\"]);\n```\n\n핵심 기능\n\n<div class=\"content-ad\"></div>\n\n- 플랫폼 네이티브 런타임 - 사용자 장치의 네이티브 환경에 배포하여 Python 또는 기타 필요한 종속성이 즉시 사용 가능하지 않을 수 있습니다. 앱 개발자는 MLC 컴파일된 LLM을 프로젝트에 통합하기 위해 플랫폼 네이티브 런타임에 익숙해지기만 하면 됩니다.\n- 메모리 최적화 - 다양한 기술을 사용하여 모델을 컴파일, 압축 및 최적화할 수 있어서 다양한 장치에 배포할 수 있습니다.\n\n장점\n\n- JSON 구성 파일에 모든 설정 - 단일 구성 파일에서 각 컴파일된 모델의 런타임 구성을 정의할 수 있습니다.\n- 미리 빌드된 앱 - 우리는 다양한 플랫폼을 위해 모델을 컴파일할 수 있습니다: 명령 줄용 C++, 웹용 JavaScript, iOS용 Swift, Android용 Java/Kotlin.\n\n제한사항\n\n<div class=\"content-ad\"></div>\n\n- LLM 모델 사용의 기능 제한: 어댑터 지원 없음, 정밀도 변경 불가, 토큰 스트리밍 불가 등. 이 라이브러리는 주로 다양한 장치용 모델을 컴파일하는 데 중점을 두고 있습니다.\n- 그룹 양자화만 지원함 — 이 방법이 좋은 결과를 보여주긴 하지만, 다른 양자화 방법(bitsandbytes 및 GPTQ)이 커뮤니티에서 더 인기가 있습니다. 아마도 커뮤니티에 의해 더 잘 개발될 것으로 예상됩니다.\n- 복잡한 설치 — 이 라이브러리를 올바르게 설치하는 데 몇 시간이 걸렸습니다. 아마도 초보 개발자에겐 적합하지 않을 것으로 보입니다.\n\niOS 또는 Android 기기에 애플리케이션을 배포해야 할 경우, 이 라이브러리가 정확히 필요한 것입니다. 이를 통해 모델을 빠르게 그리고 네이티브로 컴파일하고 기기에 배포할 수 있습니다. 그러나 서버에 높은 부하가 필요하다면 이 프레임워크를 선택하지 않는 것이 좋습니다.\n\n# 결론\n\n우리는 다른 설정을 사용하여 백서에서 이러한 프레임워크들의 성능과 제공 기능을 평가했습니다. TensorRT-LLM, vLLM과 같은 엔진 또는 RayLLM 및 RayServe와 같은 서버, TensorRT-LLM 및 Triton과 같은 Text Generation Inference (TGI) 등 각 프레임워크는 서로 다른 용도에 적합한 가치 있는 고유한 기능을 제공합니다. 우리의 벤치마킹 연구는 메모리 할당 문제부터 사전 철수의 전략적 트레이드오프, 그리고 시퀀스 길이가 처리량에 미치는 영향까지 세심한 발견을 촬영했습니다. 실험에서 얻은 내용에 대한 간략한 개괄은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- 메모리가 핵심입니다. 메모리 할당 관리는 LLM 성능 최적화를 위해 중요합니다.\n- vLLM과 같은 엔진을 위해 선점은 중요한 전략적 트레이드 오프입니다. 세대 작업이 메모리에 바운드되어 있고 GPU가 underutilized되는 상황에서입니다.\n- 시퀀스 길이 통찰력은 vLLM이 특히 더 짧은 출력과 함께 동시 요청을 처리하는 효율성을 나타냅니다.\n- 모델 크기는 처리량에 signifiant한 영향을 미칩니다. 그러나 어느 정도 이상 되면 추가 GPU 메모리는 처리량 향상에 더 이상 기여하지 않습니다.\n- 서버 선택은 중요한 역할을 합니다. TensorRT-LLM이 Triton을 이용해 독립적인 TensorRT-LLM보다 우수성을 나타내는 것이 이를 입증합니다.\n\n![이미지](/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_36.png)\n\nLLMs 추론을 위한 다양한 프레임워크가 많이 있지만, 각각이 특정 목적을 수행합니다. 고려해야 할 몇 가지 주요 사항은 다음과 같습니다:\n\n- 배치된 프롬프트 전달에 최대 속도가 필요할 때는 vLLM을 사용하세요.\n- 네이티브 HuggingFace 지원이 필요하고 핵심 모델에 대해 여러 어댑터를 사용할 계획이 없는 경우 텍스트 생성 추론을 선택하세요.\n- 속도가 중요하고 CPU에서 추론을 실행할 계획이 있는 경우 CTranslate2를 고려하세요.\n- 어댑터를 핵심 모델에 연결하고 HuggingFace 에이전트를 활용하려는 경우 특히 PyTorch에 완전히 의존하지 않는 경우 OpenLLM을 선택하세요.\n- 안정적인 파이프라인과 유연한 배포를 위해 Ray Serve를 고려하세요. 보다 성숙한 프로젝트에 가장 적합합니다.\n- LLM을 클라이언트 측(에지 컴퓨팅)에 네이티브로 배포하려는 경우(예: Android 또는 iPhone 플랫폼) MLC LLM을 활용하세요.\n- DeepSpeed 라이브러리에 이미 경험이 있고 이를 계속 사용하여 LLM을 배포하려는 경우 DeepSpeed-MII를 사용하세요.\n\n<div class=\"content-ad\"></div>\n\n# 크레딧\n\n이 블로그 포스트에서 우리는 연구 논문, 기술 블로그, 공식 문서, YouTube 비디오 등 다양한 소스에서 정보를 모았습니다. 각 소스는 해당 이미지 아래 적절하게 표시되었으며 소스 링크가 제공되었습니다.\n\n아래는 참고문헌의 통합 목록입니다:\n\n- https://python.langchain.com/v0.1/docs/guides/development/local_llms/#running-apple-silicon-gpu\n- https://www.maartengrootendorst.com/blog/quantization/\n- https://www.run.ai/blog/serving-large-language-models\n- https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\n- https://betterprogramming.pub/frameworks-for-serving-llms-60b7f7b23407\n\n<div class=\"content-ad\"></div>\n\n# 읽어주셔서 감사합니다!\n\n만약 이 안내서가 여러분의 Python 및 머신러닝 이해를 향상시키는 데 도움이 되었다면:\n\n- 박수 👏 또는 여러 개의 박수로 지원을 표현해주세요!\n- 여러분의 박수는 저에게 더 가치 있는 콘텐츠를 만드는 데 도움이 됩니다.\n- 이 가이드를 Python 또는 AI / ML 열렬한 사용자들과 공유해주세요.\n- 여러분의 피드백은 소중합니다. 앞으로의 글을 영감을 주고 이끌어주는 역할을 합니다.\n\n# 저와 소통해주세요!\n\n<div class=\"content-ad\"></div>\n\n### Vipra\n","ogImage":{"url":"/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_0.png"},"coverImage":"/assets/img/2024-05-27-BuildingLLMApplicationsServingLLMsPart9_0.png","tag":["Tech"],"readingTime":55},{"title":"챗 시 PT와 인공 지능 시대에 문화 대혁명 이상의 재창조","description":"","date":"2024-05-27 14:33","slug":"2024-05-27-ChatXiPTandtheReinventionofCulturalRevolutionIdealsintheAgeofArtificialIntelligence","content":"\n\n## 의견 | 인공 지능 & 중국\n\n![이미지](/assets/img/2024-05-27-ChatXiPTandtheReinventionofCulturalRevolutionIdealsintheAgeofArtificialIntelligence_0.png)\n\n2018년 초봄, 현재 TikTok을 보유한 베이징 소재의 바이트댄스의 창립자인 장이밍은 중국의 국가 미디어 감독 기관인 중국 국가 라디오와 텔레비전 행정국으로부터 희극적인 텍스트와 비디오 콘텐츠를 공유하는 소셜 미디어 앱인 네이한 드안지의 운영을 중단하라는 통지를 받았습니다. 이 명령은 \"음란한\" 및 \"부적절한\" 콘텐츠를 호스팅한 혐의로 발효되었습니다.\n\n장이밍은 어떻게 대응했을까요? - 물론, 그는 명령을 따르고 중국의 혁명적 전통주의에 따라 깊은 유감을 표현하여 공개 사과를 했습니다. \"지시 및 감독 당국의 지도와 기대를 늘 어기게 해서 유감스럽다\"고 그는 말했습니다.\n\n<div class=\"content-ad\"></div>\n\n현대 사회주의적 맥락에서 설정된 것이지만, 중국의 문화 대혁명(1966-1976) 기간 동안 열리던 전통적인 투쟁 세션을 많이 떠올리게 했어요. 이렇게 불리는 투쟁 세션은 비난 집회나 투쟁 모임으로, 국가 적대자로 몰되거나 반혁명적이며 반사회적인 사상이나 행동을 가진 개인들에 대한 공개적인 수치와 비날을 포함하고 있어요.\n\n공산당의 전통적인 정치적 올바름 규칙에 완전히 부응하여, 장이밍은 구제 조치 아홉 가지를 약속했어요. 이 중요 사항 중에는: 바이트댄스에서 공산당의 존재감을 강화하고 종업원들이 공산당과 정부의 관점에서 생각하도록 교육하는 것이 포함돼 있어요.\n\n6년 후 바이트댄스의 투쟁 세션 이후, 중국은 “처벌”의 양식으로 하이테크 국가 대만의 해안에서 조작을 발표하고 있어요, 믿기 어려우나 사실이며, 아마도 전체 자유 국가에 대한 투쟁 세션을 예측하기 위해서일 거예요. 그러나 1967년 투쟁 세션 피해자 본인인 시진핑 대통령의 메갈로마닉 코미디는 지리적 난장판의 비극적 현실을 넘어섭니다. 그는 나라 구멍마다 있는 모든 보안 카메라의 시야 안에서 인간 학문과 지성을 최대한의 잔인함으로 세뇌하고 억압하고자만 하지만 AI의 모든 유형의 나타남을 지배하고 통치하려 합니다. 이야기는 \"챗 시 PT\"에 대해 이루어지며, 이는 중국의 인공 지능 문화 대혁명이라고 레이블링 하고 싶은 중국의 지키반인에 의해 소개된 최근 AI 챗봇입니다. 새로운 GPT는 시진핑의 정치문화철학과 완전히 일치해요. 그것은 민주적 AI의 우리 아이디어를 복사하고 중국 정부의 과거 시점적 조직적인 선배의 완벽한 준수로 복속시키기 위한 무거운 대형 언어 모델(LLM)입니다. 지능적이거나 인공일지라도 중국의 정치적 편향 패러다임을 따라야 해요. 이것이 시진핑 대통령의 눈에는 \"바람직한 AI\"가 되는 이유입니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-ChatXiPTandtheReinventionofCulturalRevolutionIdealsintheAgeofArtificialIntelligence_2.png\" />\n\n챗봇은 중국에서 빈민 민주주의 지식인의 현실적인 상태를 빈틈없이 보여주는 농담이 아니라 가장 쓴 목마음의 현실입니다. 중국 사이버스페이스 연구원이 개발한 Chat Xi PT는 중국 최고 인터넷 규제기관인 중국 사이버스페이스 행정부 (CAC) 아래에서 운영되며, Chat Xi PT의 주요 목적은 국가의 엄격한 콘텐츠 통제 아래에서 LLM 개발을 일방적으로 정리하는 것으로 보입니다. Chat Xi PT는 '새로운 시대의 중국 특색 사회주의에 대한 시진핑 사상'으로 알려진 시진핑의 개인적이고 정치적 문화철학을 대표하는 데이터셋을 학습하였으며 다른 기관인 CAC가 제공한 공식 문헌도 포함됩니다. 시진핑 주석은 중국 소유지로 보는 영역 내에서 어떠한 종류의 지식이든 그의 관점과 완전히 일치하도록 켄트라를 생성하기를 기대하고 있습니다. 국가 권력은 지능적인 콘텐츠에 의해 의문을 제기하거나 약화시키지 말아야 합니다.\n\n전반적인 맥락은 현대화되었지만, 리더의 말을 통해 지적 및 지능의 억압과 강제적 교육적 개념은 전통적인 문화 대혁명의 메커니즘과 일관성이 있습니다. 인간들이 마오쩐둥의 '홍책'으로 읽기를 강요해야 했던 전통적인 문화 대혁명의 메커니즘과 일관성이 있습니다. 그리고 CAC는 말 그대로 '마오쩍동 주석 명언집'이라고 더 공식적으로 붙여진 '홍책'에 Chat Xi PT에 실질적으로 교육을 놓치지 않았음을 보증합니다.\n\nChat Xi PT의 도입은 중국 정부가 AI를 전체로 동기화하려는 신호를 명확히 보여줍니다. 실제로 국가 프로파간다는 가짜 자유로운 환경을 연출하고 순응하는 AI 모델의 개발을 지원한다고 속이려 합니다. 그러나 실제로 CAC는 발전 AI에 엄격한 규칙을 도입하여 제공업체들이 자신들의 모델이 '핵심 사회주의 가치'를 내재하도록 보장하도록 요구합니다. 반면에 중국 사이버보안 협회는 정부 규제, 정책 문서 및 국가 매체 보고서에서 획득한 1억개의 데이터 엔트리로 이루어진 '개발자 지원'으로 명명한 공개 데이터베이스를 발표했습니다.\n\n<div class=\"content-ad\"></div>\n\nChat Xi PT는 현재 연구 단계에 있어 공개되지 않고, 초대된 사용자 그룹에 의해 테스트 중입니다. 그러나 최대한 빨리 챗봇을 보다 많은 사용자들이 이용할 수 있도록 계획 중에 있습니다. 중요한 질문은 공개적으로 출시될 때가 아닌, Chat Xi PT와 같은 AI가 누구에게 실제적으로 유용할 수 있는지입니다. 아니면, 그것이 중국의 국가 프로파간다의 자기 만족에 불과하며 현실 세계에서의 다른 적용 가능성이 없을까요?\n\n## 참고\n\nª 2024년 5월 20일 뉴욕 타임스의 Li Yuan 작성 “중국 기업, 국내 독재주의와 해외 적대에 직면” 기사\n\n이야기를 읽어주셔서 감사합니다. 다음에 다시 뵙겠습니다. 의견, 피드백 또는 제안 사항이 있으면 언제든지 댓글을 남겨주십시오! 그리고 만약 감사의 표시로 이 이야기에 최대한 클랩을 주고 싶다면, 환영합니다.\n\n<div class=\"content-ad\"></div>\n\n저자 소개: 샘 바세히는 공학 박사 학위를 보유하고 있으며 생명 과학과 공학 석사 학위를 취득했으며 하버드 비즈니스 스쿨의 지속가능성 및 스탠포드 대학의 머신 러닝 포닥 강좌를 이수했습니다. 스웨덴 거주의 독일 국적자인 그는 주요 산업에 전략, 혁신 및 지속 가능한 미래에 대한 독립적인 전문가 자문을 제공하고 세계 경제 포럼의 자문자로 등재되어 있습니다. 이전에 AOL 타임 워너, AT Kearney, 델로이트 및 E&Y에서 일한 경험이 있으며 스웨덴 언론인 조합에서 자주 기자 자격을 인정받은 넓은 출판 이력을 가지고 있습니다.\n\n저자의 다른 관련 기사들:","ogImage":{"url":"/assets/img/2024-05-27-ChatXiPTandtheReinventionofCulturalRevolutionIdealsintheAgeofArtificialIntelligence_0.png"},"coverImage":"/assets/img/2024-05-27-ChatXiPTandtheReinventionofCulturalRevolutionIdealsintheAgeofArtificialIntelligence_0.png","tag":["Tech"],"readingTime":4},{"title":"언급되지 않은 수퍼 비밀 프롬프트 도구 지금은 아무도 말하지 않는","description":"","date":"2024-05-27 14:32","slug":"2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet","content":"\n![이미지](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_0.png)\n\n상상해봐: 당신이 책상에 앉아 커피 한잔을 쥔 채로, AI에게 TODO를 도와달라고 요청할 준비를 한 씬을.\n\n프롬프트를 입력하고 엔터키를 누르면... 실망스러운 결과가 나옵니다. 당신이 원하는 대답이 아닐 때.\n\n이런 일이 벌어진 적이 있나요? 그렇다면 당신만이 아닙니다. 많은 사람들이 AI로부터 좋은 답변을 얻는 데 애를 먹습니다.\n\n<div class=\"content-ad\"></div>\n\n하지만 걱정하지 마세요. 이 문제를 해결해주는 도구를 찾았어요.\n\n그 도구는 입력에 기반한 매우 상세한 프롬프트를 작성합니다. 이 도구는 사용하실 수 있고, Claude를 만든 Anthropic 회사에서 만들었습니다.\n\n그러니 여러분의 커피를 가져오시고, 함께 사용하는 방법을 알아봐요!\n\n# 완벽한 프롬프트 작성하기\n\n<div class=\"content-ad\"></div>\n\n우리가 사용할 도구는 Anthropic Prompt Generator라고 합니다. 이를 이용하려면 계정을 만들어야 합니다.\n\n## 1/ Anthropic 계정 만들기\n\n우선 가장 먼저, 마법이 일어나는 곳인 계정을 설정해야 합니다.\n\nAnthropic 웹사이트에 가서 가입하세요. 쉽고 무료입니다! 로그인하면 아래 이미지처럼 보이는 대시보드가 나타납니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_1.png)\n\n## 2/ Describe What You Want\n\nNow, think about what you need from AI.\n\nAre you making a playlist? Finding recipes? Whatever it is, describe it clearly.\n\n\n<div class=\"content-ad\"></div>\n\n현재 기분에 따라 플레이리스트를 만들고 싶어요.\n\n![Image](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_2.png)\n\n## 3/ 툴이 매력적인 부분 보여줘\n\n설명을 입력한 후에, 툴이 마법을 부릅니다.\n\n<div class=\"content-ad\"></div>\n\n간단한 지침을 받아서 매우 상세한 프롬프트로 바꿉니다. 특별한 것은 추가할 필요 없어요; AI가 잘 알고 있어요.\n\n![Prompt image](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_3.png)\n\n받은 프롬프트에 만족하셨다면 좋아요. 하지만 여러분에게는 선택권이 항상 있어요. (사진에서 강조된대로) 프롬프트를 편집할 수도 있어요.\n\n<div class=\"content-ad\"></div>\n\n## 4/ 수정 및 정제\n\n가끔은 첫 시도가 완벽하지 않을 수 있어요. 괜찮아요! 여전히 프롬프트를 편집하여 원하는 대로 완성시킬 수 있어요.\n\n![image](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_4.png)\n\n프롬프트에 만족하셨다면, 오른쪽 상단의 '실행(Run)'을 클릭하기만 하면 돼요.\n\n<div class=\"content-ad\"></div>\n\n## 5/ 완벽한 프롬프트 사용하기\n\n실행을 클릭하면 프롬프트 변수를 채우라는 메시지가 나타납니다.\n\n이 도구는 이러한 매개변수를 가져와 생성된 프롬프트에 삽입하여 결과를 제공합니다.\n\n![image](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_5.png)\n\n<div class=\"content-ad\"></div>\n\n저의 경우, 저를 흥분시키기 위한 80년대 & 90년대 노래로 구성된 플레이리스트가 만들어졌어요.\n\n관심이 있다면, 여기가 플레이리스트 목록입니다:\n\n- “Walking on Sunshine” by Katrina and The Waves\n- “Livin’ on a Prayer” by Bon Jovi\n- “Girls Just Want to Have Fun” by Cyndi Lauper\n- “I Wanna Dance with Somebody (Who Loves Me)” by Whitney Houston\n- “Don’t Stop Believin’” by Journey\n- “Sweet Child O’ Mine” by Guns N’ Roses\n- “Uptown Funk” by Mark Ronson ft. Bruno Mars\n- “Wannabe” by Spice Girls\n- “U Can’t Touch This” by MC Hammer\n- “Pump Up the Jam” by Technotronic\n- “Celebration” by Kool & The Gang\n- “The Power of Love” by Huey Lewis & The News\n- “Everybody (Backstreet’s Back)” by Backstreet Boys\n- “Twist and Shout” by The Beatles\n- “Dancing Queen” by ABBA\n\n# 마지막으로 생각할 것들\n\n<div class=\"content-ad\"></div>\n\n이제 완벽한 프롬프트를 만드는 것이 얼마나 쉬운지 알게 되었어요. 그리고 당신이 AI 상호작용에서 최상의 결과를 얻는 데 아무런 장애물이 없습니다.\n\n만약 이 안내서를 즐겼다면 매주 이와 같은 안내서를 작성하고, 구독자들의 인박스로 직접 보내드립니다. 즐겁고 무료이며 가치가 충만해요.\n\n![image](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_6.png)\n\n이 이야기는 Generative AI에 게시되어 있어요. LinkedIn에서 저희와 연결하고 최신 AI 이야기에 대한 소식을 받으려면 Zeniteq를 팔로우하세요.\n\n<div class=\"content-ad\"></div>\n\n최신 뉴스 및 생성 AI에 대한 최신 정보를 받아보려면 뉴스레터를 구독하세요. 함께 AI의 미래를 만들어 봅시다!\n\n![Image](/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_7.png)\n","ogImage":{"url":"/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_0.png"},"coverImage":"/assets/img/2024-05-27-SuperSecretPromptingToolNoOneTalksAboutJustYet_0.png","tag":["Tech"],"readingTime":4},{"title":"기존의 GPT-4보다 뛰어난 성능을 자랑하는 스탠포드 대학팀의 대형 모델이 모바일폰에서도 구동될 수 있다는 점이 인기를 끌며 하룻밤 사이에 2천 회 이상 다운로드되었습니다","description":"","date":"2024-05-27 14:30","slug":"2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight","content":"\n\n![Octopus v2](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_0.png)\n\n대규모 모델 구현의 과정에서, 엔드사이드 AI는 매우 중요한 방향입니다.\n\n최근 스탠퍼드 대학의 연구자들이 출시한 Octopus v2는 개발자 커뮤니티로부터 큰 관심을 받으며 인기를 끌고 있습니다. 모델의 다운로드 횟수가 하룻밤 사이에 2천 건을 넘었습니다.\n\n200억 개의 파라미터를 갖는 Octopus v2는 정확도와 대기 시간 측면에서 GPT-4를 능가하며, 콘텍스트 길이를 95% 줄였습니다. 또한, Octopus v2는 Llama7B + RAG 구성보다 36배 빠릅니다.\n\n\n<div class=\"content-ad\"></div>\n\n많은 네티즌들이 한탄했습니다: 디바이스 측 인공지능 에이전트 시대가 도래했습니다!\n\n![image](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_1.png)\n\n- 논문: 옥토퍼스 v2: 슈퍼 에이전트용 기기 내 언어 모델\n- 논문 주소: https://arxiv.org/abs/2404.01744\n- 모델 홈페이지: https://huggingface.co/NexaAIDev/Octopus-v2\n\n모델 개요\n\n<div class=\"content-ad\"></div>\n\n옥토퍼스-V2-2B는 안드로이드 API에 맞게 설계된 20 억 개의 매개변수를 가진 오픈 소스 언어 모델로, 안드로이드 기기에서 원활하게 실행되며 안드로이드 시스템 관리에서 여러 기기 및 다양한 응용 프로그램의 조작까지 확장하는 데 사용됩니다.\n\n![이미지](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_2.png)\n\n일반적으로 검색 증강 생성 (RAG) 방법은 잠재적인 기능 매개변수에 대한 상세한 설명을 필요로 하며 (때로는 수만 개의 입력 토큰이 필요할 수도 있음), 이에 기반하여 옥토퍼스-V2-2B는 훈련 및 추론 단계에서 고유한 기능 토큰 전략을 도입하여 GPT-4와 유사한 성능 수준을 달성할 뿐만 아니라 추론 속도를 크게 향상시켜 RAG 기반 방법을 능가합니다. 이로 인해 엣지 컴퓨팅 기기에 특히 유용합니다.\n\n![이미지](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_3.png)\n\n<div class=\"content-ad\"></div>\n\n문어-V2-2B는 다양한 복잡한 시나리오에서 개별, 중첩 및 병렬 함수 호출을 생성할 수 있습니다.\n\n데이터 세트\n\n훈련, 검증 및 테스트 단계에서 고품질 데이터 세트를 사용하고 특히 효율적인 훈련을 달성하기 위해, 연구팀은 데이터 세트를 세 가지 주요 단계로 생성했습니다:\n\n- 관련 쿼리 및 관련 함수 호출 매개변수 생성;\n- 적절한 기능 구성요소에서 관련이 없는 쿼리 생성;\n- Google Gemini을 통한 이진 검증 지원.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_4.png)\n\nResearch team은 이 모델을 훈련하기 위해 20가지의 안드로이드 API 설명을 작성했습니다. 다음은 예시 안드로이드 API 설명입니다:\n\n```js\ndef get_trending_news (category=None, region='US', language='en', max_results=5):\n    \"\"\"\n    카테고리, 지역 및 언어에 기반한 트렌드 뉴스 기사를 가져옵니다.\n    Parameters:\n    - category (str, optional): 필터링할 뉴스 카테고리입니다. 모든 카테고리에 대해 기본값으로 None을 사용합니다. 선택적으로 제공할 수 있습니다.\n    - region (str, optional): 지역별 뉴스를 위한 ISO 3166-1 알파-2 국가 코드입니다. 기본값으로 'US'를 사용합니다. 선택적으로 제공할 수 있습니다.\n    - language (str, optional): 기사 언어를 위한 ISO 639-1 언어 코드입니다. 기본값으로 'en'을 사용합니다. 선택적으로 제공할 수 있습니다.\n    - max_results (int, optional): 반환할 기사의 최대 수입니다. 기본값으로 5를 사용합니다. 선택적으로 제공할 수 있습니다.\n    Returns:\n    - list [str]: 각각 기사를 나타내는 문자열의 목록입니다. 각 문자열은 기사 제목과 URL을 포함합니다.\n    \"\"\"\n```\n\n모델 개발 및 훈련\n\n\n\n<div class=\"content-ad\"></div>\n\n이 연구는 프레임워크에서 Google Gemma-2B 모델을 사전 학습 모델로 사용하며 두 가지 다른 훈련 방법을 채택합니다: 전체 모델 훈련과 LoRA 모델 훈련.\n\n전체 모델 훈련에서는 AdamW 옵티마이저를 사용하며 학습률은 5e-5로 설정되고 웜업 단계 수는 10으로, 선형 학습률 스케줄러가 사용됩니다.\n\nLoRA 모델 훈련은 전체 모델 훈련과 동일한 옵티마이저와 학습률 구성을 사용하며 LoRA 랭크는 16으로 설정되며, LoRA는 다음 모듈에 적용됩니다: q_proj, k_proj, v_proj, o_proj, up_proj, down_proj. 그 중 LoRA 알파 매개변수는 32로 설정됩니다.\n\n두 훈련 방법 모두 에포크 수는 3으로 설정됩니다.\n\n<div class=\"content-ad\"></div>\n\n다음 코드를 사용하면 단일 GPU에서 Octopus-V2-2B 모델을 실행할 수 있습니다.\n\n```js\nfrom transformers import AutoTokenizer, GemmaForCausalLM\nimport torch\nimport time\n\ndef inference(input_text):\n    start_time = time.time()\n    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    input_length = input_ids[\"input_ids\"].shape[1]\n    outputs = model.generate(\n        input_ids=input_ids[\"input_ids\"],\n        max_length=1024,\n        do_sample=False\n    )\n    generated_sequence = outputs[:, input_length:].tolist()\n    res = tokenizer.decode(generated_sequence[0])\n    end_time = time.time()\n    return {\"output\": res, \"latency\": end_time - start_time}\n\nmodel_id = \"NexaAIDev/Octopus-v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = GemmaForCausalLM.from_pretrained(\n    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n\ninput_text = \"Take a selfie for me with front camera\"\nnexa_query = f\"아래는 사용자쿼리입니다. 올바른 함수를 호출하고 함수를 호출하는 매개변수를 생성하십시오.\\n\\n쿼리: {input_text}\\n\\n응답:\"\nstart_time = time.time()\nprint(\"넥사 모델 결과:\\n\", inference(nexa_query))\nprint(\"latency:\", time.time() - start_time, \"초\")\n```\n\n평가\n\n벤치마크 테스트에서 Octopus-V2-2B는 단일 A100 GPU에서 \"Llama7B + RAG 솔루션\"보다 36배 빠른 탁월한 추론 속도를 보여주었습니다. 게다가, Octopus-V2-2B는 클러스터화된 A100/H100 GPU에 의존하는 GPT-4-turbo보다 168% 빠릅니다. 이 효율적인 개선은 Octopus-V2-2B의 기능 토큰 디자인에 기인합니다.\n\n<div class=\"content-ad\"></div>\n\n![Octopus-V2-2B](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_5.png)\n\n옥토퍼스-V2-2B는 속도뿐만 아니라 정확도 면에서도 우수하며, \"라마7B + RAG 솔루션\"을 31% 초과하는 함수 호출 정확도로 능가합니다. 옥토퍼스-V2-2B는 GPT-4 및 RAG + GPT-3.5와 비교 가능한 함수 호출 정확도를 달성합니다.\n\n![Learn More](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_6.png)\n\n관심 있는 독자들은 연구 내용에 대한 원본 논문을 읽어서 더 많이 알아볼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n참고: [https://mp.weixin.qq.com/s/qnFZOPLpdRxW42_cLUcImA](https://mp.weixin.qq.com/s/qnFZOPLpdRxW42_cLUcImA)\n\n기사가 마음에 드셨나요? 더 많은 학습을 원하신다면 제한 없이 읽을 수 있는 Medium 회원이 되어보세요. 이 링크를 통해 회원이 되면 추가 비용 없이 저를 지원해 주시게 됩니다. 미리 감사드리고 앞으로 또 만나요!\n","ogImage":{"url":"/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_0.png"},"coverImage":"/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_0.png","tag":["Tech"],"readingTime":6}],"page":"104","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":12,"currentPageGroup":5},"__N_SSG":true}