{"pageProps":{"posts":[{"title":"NLP와 지식 그래프를 활용한 2024 최신 오피오이드 연구 방법","description":"","date":"2024-06-22 21:38","slug":"2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch","content":"\n\n## 오피오이드 연구에서 인공지능과 지식 그래프 통합을 통한 혁신\n\n![이미지](/assets/img/2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch_0.png)\n\n오피오이드 위기는 전 세계적으로 수백만 명의 사람들에 영향을 미치는 엄청난 공중보건 위기입니다. 처방약물 오필리아, 헤로인, 펜타닐과 같은 합성 오피오이드의 광범위한 남용을 특징으로 하며, 이 위기는 중독, 과다복용, 사망률이 전례 없는 수준으로 이어지고 있습니다. 이 보건 위기의 복잡성과 규모는 새로운 통찰력을 발견하고 효과적인 개입책을 개발하기 위해 혁신적인 연구 방법이 필요합니다.\n\nNLP와 지식 그래프를 활용한 연구는 오피오이드 위기와의 전쟁에서 강력한 도구로 부상했습니다. 이런 최첨단 기술을 활용함으로써 연구자들은 과학문헌, 임상 노트, 의료 보고서, 그리고 소셜 미디어 토론 등 방대한 양의 비정형 데이터 저장소에 숨겨진 다양한 정보를 발굴할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nNLP 기술은 비구조적 소스에서 구조화된 정보를 추출할 수 있게 해주어, 약물, 질병, 부작용과 같은 개체들 그리고 그들 사이의 관계를 식별하는 데 도움을 줍니다. 이 능력은 특히 오피오이드 연구 분야에서 여러 요소 간의 복잡한 관계를 이해하는 데 중요합니다.\n\n반면에 지식 그래프는 NLP를 통해 추출된 구조화된 정보를 통합하고 표현하는 강력한 프레임워크를 제공합니다. 개체들 간의 복잡한 관계를 모델링함으로써, 지식 그래프는 연구자들이 고급 질의, 추론, 그리고 지식 탐색 작업을 수행하고 독립된 데이터셋에 숨어있을 수 있는 통찰을 찾아내게 도와줍니다.\n\nNeo4j는 그래프 없이는 불가능한 연결된 데이터를 분석하는 데 사용됩니다. Neo4j는 연결된 데이터를 저장하고 처리하기 위해 특별히 설계된 네이티브 그래프 데이터베이스로, 모든 규모의 복잡한 생명과학 문제를 해결하는 데 도움이 됩니다.\n\nNLP와 지식 그래프의 결합은 중독에 기여하는 다양한 요인을 포괄적으로 이해함으로써 오피오이드 연구의 혁신을 약속합니다. 잠재적인 위험 요소를 식별하고 더 효과적인 예방 및 치료 전략을 개발하는 데 도움을 줄 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 게시물에서는 John Snow Labs의 Healthcare NLP 라이브러리에서 사전 학습된 모델을 사용하여 오피오이드 관련 엔티티 및 그들의 부작용을 추출하고, Neo4J를 사용하여 오피오이드 약물, 증상 및 연구 논문 간의 복잡한 관계를 나타내는 지식 그래프를 구축하는 방법에 대해 다룹니다. Neo4J를 사용하여 오피오이드 약물, 관련 조건 (효과) 및 관련 PubMed 논문과 같은 엔티티를 노드로, 그들의 관계를 엣지로 나타내면, Neo4J가 오피오이드의 부작용과 해당 약물 및 상태가 언급된 PubMed 기사에 대한 통찰을 제공할 수 있습니다. 전체적으로, Neo4J의 고도로 연결된 데이터를 처리할 수 있는 능력은 지식 관리부터 운영 분석 및 의사 결정 지원까지 다양한 의료 분야에서 가치 있는 도구로 작용합니다.\n\n먼저 Spark NLP 소개를 간단히 살펴보고, 좀 더 구체적인 결과를 통해 오피오이드 약물 분석에 대해 설명하겠습니다.\n\n## Spark NLP 및 LLM\n\n헬스케어 라이브러리는 John Snow Labs의 Spark NLP 플랫폼의 강력한 구성 요소로, 의료 분야 내에서 NLP 작업을 용이하게 하는 데 설계되었습니다. 이 라이브러리는 의료 데이터에 특화된 2,200개 이상의 사전 학습된 모델 및 파이프라인을 제공하여 정확한 정보 추출, 임상 및 의학적 개념을 위한 NER 및 텍스트 분석 능력을 제공합니다. 정기적으로 업데이트되고 첨단 알고리즘으로 구성된 이 헬스케어 라이브러리는 전자 의료 기록, 임상 노트 및 생물 의학 문헌과 같은 구조화되지 않은 의료 데이터 원본에서 보다 깊은 통찰을 제공하여 의료 전문가들에게 정보 처리를 간소화하고 강력한 통찰력을 제공하는 것을 목표로 합니다.\n\n<div class=\"content-ad\"></div>\n\n존 스노우 랩의 GitHub 저장소는 협업 플랫폼 역할을 하며 사용자들이 스파크 NLP 및 관련 도구의 이해와 활용을 더욱 향상시키기 위해 코드 샘플, 튜토리얼 및 프로젝트와 같은 오픈 소스 자원에 액세스할 수 있는 곳입니다.\n\n존 스노우 랩은 또한 주기적으로 인증 교육을 제공하여 사용자들이 헬스케어 라이브러리 및 NLP 플랫폼의 다른 구성 요소를 활용하는 데 전문성을 쌓을 수 있도록 지원합니다.\n\n존 스노우 랩의 데모 페이지는 라이브러리의 기능을 탐색할 수 있는 사용자 친화적 인터페이스를 제공하여 사용자들이 상호작용적으로 다양한 기능과 모델을 테스트하고 시각화할 수 있도록 돕습니다. 이는 이러한 도구들이 의료 및 기타 분야의 실제 시나리오에 어떻게 적용될 수 있는지에 대한 보다 심층적인 이해를 돕습니다.\n\n## 아편 연구의 이유\n\n<div class=\"content-ad\"></div>\n\n옵이오이드 전염병은 모든 수명을 앗아가고 심각한 고통을 초래하여 공중보건 위기를 야기했습니다. 이 위기와의 싸움이 격렬해지면서, 연구원과 보건의료 전문가들은 더 심층적인 통찰을 얻고 옵이오이드 중독과 그에 미치는 넓은 영향에 대응할 수 있는 더 효과적인 전략을 개발하기 위해 혁신적인 방법을 모색하고 있습니다.\n\n아래 그래프(국립보건원(NIH) 제작)는 1990년대 후반의 비교적 낮은 수준에서 급격히 증가하여 2022년에 81,000명을 넘는 사망자 수로 끝나는 압도적인 모습을 그렸습니다. 남성과 여성에 대한 그래프의 명확한 선들은 남성 인구에 미치는 비대칭적인 영향을 보여주지만, 양측 모두가 수년 동안 옵이오이드 관련 사망자 수가 상당히 증가한 것을 보여줍니다.\n\n![그래프](/assets/img/2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch_1.png)\n\n## 임상 노트로부터 종양학 관련 개체 추출\n\n<div class=\"content-ad\"></div>\n\n예명(entity) 인식(Named Entity Recognition, NER) 모델 또는 대규모 언어 모델(Large Language Models, LLMs)을 사용하여 비구조화된 텍스트 데이터에서 오피오이드 관련 정보를 추출하는 것은 분석에서 중요한 단계입니다. 전자 의료 기록(EHRs), 임상 노트 및 의학 문헌과 같은 비구조화 데이터 소스에는 많은 가치 있는 정보가 포함되어 있지만 구조화되지 않은 형식으로 제공되기 때문에 중요한 도전이 존재합니다. NER 모델은 특히 이러한 도전에 대응하기 위해 훈련되어 텍스트 내에서 개체를 사전 정의된 범주(라벨링)로 식별하고 분류하는 방식으로 작동합니다.\n\n오피오이드 연구의 맥락에서, NER 모델은 특정 약물 이름(예: 옥시코돈, 펜타닐)과 관련된 개체, 오피오이드 사용과 관련된 환자 증상(예: 호흡 억제, 변비), 그리고 진단, 치료, 동반 질환과 같은 기타 중요한 의학 용어를 인식하고 추출하는 데 훈련됩니다. 고급 NER 모델은 텍스트의 문맥과 의미를 이해하기 위해 심층 학습 기술과 LLMs에 기반한 정교한 알고리즘을 활용하여 개체 인식의 정확성을 크게 향상시킵니다.\n\n명명된 개체 간의 관계를 효율적으로 수립하기 위해 관계 추출을 사용하면 대규모 비구조화 텍스트 데이터를 다루는 데 도움이 됩니다. 이를 통해 인식된 개체와 그들 간의 관계가 포함된 구조화된 정보로 변환됩니다.\n\n비구조화 데이터에서 관련 있는 오피오이드 관련 개체들을 성공적으로 추출한 후, 다음 중요 단계는 이러한 개체들 간의 관계를 수립하는 것입니다. 여기서 지식 그래프(Knowledge Graphs)가 중요한 역할을 합니다. 지식 그래프는 추출된 데이터를 구조화된 형식으로 정리하고, 네트워크 형식의 다른 개체 간의 관계를 매핑합니다. 예를 들어 특정 오피오이드 약물을 해당하는 부작용과 연결하거나, 환자 증상을 특정 오피오이드 처방과 연관시키거나, 환자가 복용 중인 다른 약물과의 상호 작용을 강조할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n과정은 NER 모델이 텍스트 데이터에서 엔티티를 추출하는 것으로 시작합니다. 예를 들어, \"환자가 만성 통증으로 옥시코돈을 처받았지만 심한 변비와 호흡 억제 증상이 발생했습니다\"라는 임상 노트로부터 엔티티로 \"옥시코돈\", \"만성 통증\", \"변비\", \"호흡 억제\"를 식별할 것입니다. 이후, 지식 그래프는 \"알펜타닐\"과 \"신생아 호흡 억제 증후군\" 사이의 caused_by와 \"옥시코돈\"과 \"변비\" 및 \"호흡 억제\" 사이의 caused_by와 같은 관계를 설정할 것입니다.\n\n이 통합된 접근 방식은 데이터 분석을 향상시키는데 그치지 않고 오피오이드 연구의 복잡한 종속성을 포함한 종합적인 이해를 용이하게 합니다. NER 모델을 활용하여 엔티티 추출 및 관계 설정을 위한 관계 추출을 결합하고, 관계 매핑을 위해 지식 그래프를 활용함으로써 연구자들은 숨겨진 패턴을 발견하고 잠재적인 건강 결과를 예측하며, 표적화된 개입 방안을 개발할 수 있습니다. 이 방법론은 오피오이드 위기 대응의 중요한 발전을 나타내며, 오피오이드 사용 및 그에 따른 영향의 예방, 치료, 정책 결정을 위한 더 깊은 통찰력과 보다 효과적인 전략을 제공합니다.\n\n텍스트에서 정보 추출\n\nHealthcare NLP 라이브러리의 NER 모델을 사용하여 생성된 엔티티를 빠르게 시각화하는 능력은 생성된 결과를 조사하는 데 매우 유용한 기능입니다. Spark NLP Display는 Spark NLP로 생성된 추출된 및 레이블이 지정된 엔티티를 시각화하기 위한 오픈 소스 Python 라이브러리입니다. NerVisualizer는 추출된 네임드 엔티티를 강조하고 분석된 텍스트 위에 레이블을 장식으로 표시합니다.\n\n<div class=\"content-ad\"></div>\n\n아래 예시에서는 오피오이드 관련 엔터티를 추출하기 위해 특별히 훈련된 모델(ner_opioid)이 두 가지 오피오이드 의약품을 식별했습니다: 만성 통증에 처방된 옥시코돈(Oxycodone)과 발작 통증에 사용되는 펜티넬 패치(Fentanyl patch). 시각화 도구는 당뇨병 관리에 사용되는 메트포르민(Metformin)과 고혈압에 사용되는 리시노프릴(Lisinopril)과 같은 비오피오이드 의약품을 식별했습니다. 의약품은 타입별(오피오이드 의약품 및 기타 의약품)로 명확하게 분류되었습니다. 또한 의약품의 의도된 용도 또는 관련 상황(일반 증상, 다른 질병)도 추출되어 레이블이 지정되었습니다.\n\n![이미지](/assets/img/2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch_2.png)\n\n관계 추출 시각화 도구는 (가상의) 42세 남성 환자의 의료 기록 및 현재 상태를 예시로 제공하며 오피오이드 사용에 초점을 맞춥니다. 이 도구는 환자가 만성 및 발작성 통증을 주요 불편으로 제기했고, 이를 오피오이드 진통제를 이용한 만성 통증 관리에 연결했습니다. 다이어그램은 환자의 과거 의료 기록을 포함하여 고혈압과 제2형 당뇨병을 보여주며 현재 의약품 처방 요령을 개요로 제시합니다. 이 시각적 표현은 환자의 증상, 의료 기록 및 의약품 사용 사이의 복잡한 관계를 효과적으로 포착하여 잠재적인 오피오이드 관련 합병증 및 추가 의료 관심이 필요한 영역에 대한 소중한 통찰력을 제공합니다.\n\n## 지식 그래프 결과\n\n<div class=\"content-ad\"></div>\n\n사이퍼는 Neo4j의 그래프 쿼리 언어로, 그래프에서 데이터를 검색할 수 있는 SQL과 유사합니다. 다른 언어와 직관적인 유사성으로 인해 가장 쉽게 학습할 수 있는 그래프 언어입니다.\n\n다음 예제에서는 쿼리를 사용하여 모든 노드와 관계를 검색하여, 모든 노드(옵이오이드, 조건, 그리고 PubMed 논문 ID)의 전체 그림을 제공합니다.\n\n이 표현은 데이터 내에서 클러스터, 이상치, 그리고 패턴을 식별하는 데 유용합니다. 밀집된 영역은 노드 간의 높은 연결성과 상호 작용을 나타내어, 연구가 많이 이루어진 영역이나 높은 참조 주제를 시사할 수 있습니다. 반면 희소한 영역이나 고립된 노드는 연구가 적거나 특정 주제를 나타낼 수 있습니다.\n\n이러한 시각화는 연구에서 주요 연구 영역, 연구의 잠재적 공백, 그리고 데이터 관계의 전반적인 구조를 식별하는 데 유용합니다. 의료 조건, 약물 참조 및 학술 논문의 상호 연결된 성질을 이해하는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n아래 시각화는 노드와 관계에 대한 더 자세한 정보를 제공하여 데이터의 포괄적인 매핑을 보여줍니다. Alfentanil은 합성, 단순 작용성 아편 진통제로, 펜타닐의 소분자 유도체로 분류됩니다.\n\n중심 노드인 alfentanil은 다양한 질환과 논문에 연결되어 있습니다. caused_by와 같은 관계는 alfentanil이 잠재적으로 일으킬 수 있는 의료 조건을 강조합니다.\n\narticle_of와 mentioned_in의 밀집된 네트워크는 이 데이터 집합 내 엔티티에 대한 포괄적인 연구 및 참고 자료의 유용성을 나타냅니다.\n\n이 시각화는 호흡기 질환과 해당 질환을 유발하거나 관련시킬 수 있는 다양한 아편 진통제 간의 관계에 초점을 맞추고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, 수족의 인정, 펜타닐 시트랏, 듀라게식, 코드인, 옥시코돈 등이 호산구 진통제이거나 호흡 기능을 악화시키거나 어떤 방식으로든 호흡 기능과 연결된 다른 물질일 수 있습니다.\n\n오른쪽의 결과 개요를 보면 총 27개의 노드가 있으며, 10개의 상태 노드, 17개의 호산구 노드 및 25개의 caused_by 관계가 있음을 알 수 있습니다.\n\n아래 예시에서 데이터베이스에서 쿼리를 실행하여 가장 유사한 상태를 가지는 호산구가 무엇인지 확인할 수 있습니다.\n\n쿼리는 호산구 간에 공유된 상태를 반환합니다. 예를 들어, 모르핀과 MS 콘틴은 만성 통증 질환 및 인지 저하와 같은 상태를 공유합니다. 반면 돌로핀과 메타돈은 인지 기능 장애와 불안 증상과 같은 상태를 공유합니다.\n\n<div class=\"content-ad\"></div>\n\n이 방법은 의료 전문가들이 공유된 부작용과 치료 목적에 대해 이해하는 데 도움이 될 수 있습니다. 또한 공유된 조건을 기반으로 의약품의 부작용을 예측하는 데도 도움이 될 수 있습니다.\n\n![이미지](/assets/img/2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch_3.png)\n\n## 결론\n\n요약하자면, NLP와 지식 그래프의 결합은 오피오이드 연구를 가속화하고 강화하는 강력한 방법을 제공합니다. 이러한 기술을 사용하여 바이오의학 정보의 대량을 효율적으로 추출, 정리 및 연결함으로써 연구자들은 다음을 할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 약물, 부작용 및 생물학적 경로 사이의 숨겨진 관계를 발견하세요.\n- 통증 관리를 위한 잠재적인 새로운 대상 식별\n- 유해한 약물 상호 작용을 예측하고 완화하세요.\n- 처방 관행과 환자 결과에 대한 패턴 분석\n\n아피오이드 위기가 공중 보건에 상당한 어려움을 줄 채 지속되는 가운데, 이러한 고급 컴퓨팅 기술을 활용하는 것은 점점 중요해집니다. 이 기술들은 연구 과정을 효율화할 뿐만 아니라 더 안전하고 효과적인 통증 관리 전략으로 이어질 수 있는 혁신적인 통찰력을 제공합니다.\n\nNLP 및 지식 그래프의 통합은 아피오이드 연구에서 이 복잡한 문제에 대응하는 우리의 능력을 크게 발전시키는 중요한 한걸음입니다. 이러한 기술이 계속 발전함에 따라, 이들은 근거에 기반한 정책 수립, 혁신적 치료법 개발, 그리고 궁극적으로 통증 관리 분야에서 환자 관리 개선에 더 큰 역할을 약속합니다.\n\n부록: 이 연구는 2024년 Neo4j Health Care & Life Sciences 워크샵에서 발표되었습니다. 전체 워크샵 비디오는 여기에서 확인할 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch_0.png"},"coverImage":"/assets/img/2024-06-22-HarnessingthePowerofNLPandKnowledgeGraphsforOpioidResearch_0.png","tag":["Tech"],"readingTime":8},{"title":"Greg의 비디오에서 배우는 RAG 청킹 5단계 전략","description":"","date":"2024-06-22 21:37","slug":"2024-06-22-FiveLevelsofChunkingStrategiesinRAGNotesfromGregsVideo","content":"\n\n# 소개\n\n대형 데이터 파일을 더 작은 세그먼트로 분할하는 것은 LLM 응용 프로그램의 효율성을 높이는 가장 중요한 단계 중 하나입니다. 목표는 LLM에게 특정 작업에 필요한 정보를 정확하게 제공하는 것이며, 그 이상의 정보는 제공하지 않는 것입니다.\n\n“나의 솔루션에서 적합한 청킹 전략은 무엇이어야 할까”는 고급 RAG 솔루션을 구축하는 동안 LLM 전문가가 필수적으로 결정해야 하는 초기적이고 근본적인 결정 중 하나입니다.\n\nGreg Kamradt는 비디오에서 다양한 청킹 전략을 개요로 제공합니다. 이러한 전략은 RAG 기반 LLM 어플리케이션 개발의 시작점으로 활용될 수 있습니다. 이들은 복잡성과 효과성을 기준으로 다섯 수준으로 분류되었습니다.\n\n<div class=\"content-ad\"></div>\n\n# 레벨 1: 고정 크기 청킹\n\n이것은 텍스트를 세그먼트로 분할하는 가장 단순하고 원시적인 방법입니다. 콘텐츠나 구조와 관계없이 텍스트를 지정된 문자 수의 청크로 분할합니다.\n\nLangchain과 llamaindex 프레임워크는 CharacterTextSplitter와 SentenceSplitter(문장을 기준으로 분할하는 것이 기본 설정) 클래스를 이용해 이러한 청킹 기술을 제공합니다. 몇 가지 기억해야 할 개념들 -\n\n- 텍스트가 어떻게 분할되는지: 한 문자씩\n- 청크 크기 측정 방법: 문자 수로\n- 청크 크기: 청크에 포함된 문자 수\n- 청크 중첩: 연속적인 청크에서 중복 데이터를 유지하기 위해 오버랩되는 문자 수\n- 구분자: 텍스트를 분할할 문자(기본 설정은 \"\")\n\n<div class=\"content-ad\"></div>\n\n# 레벨 2: 재귀 청킹\n\n고정 크기 청킹은 구현하기 쉽지만 텍스트의 구조를 고려하지 않습니다. 재귀 청킹은 대안을 제공합니다.\n\n이 방법에서는 텍스트를 계층적이고 반복적인 방식으로 더 작은 청크로 나누어 일련의 구분자를 사용합니다. 텍스트를 분할하는 초기 시도가 원하는 크기의 청크를 생성하지 못하는 경우, 해당 방법은 다른 구분자를 사용하여 결과 청크에서 자체를 재귀적으로 호출하여 원하는 청크 크기를 달성합니다.\n\nLangchain 프레임워크는 default 구분자(“\\n\\n”, “\\n”, “ “,””)를 사용하여 텍스트를 분할하는 RecursiveCharacterTextSplitter 클래스를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n# 레벨 3: 문서 기반 청킹\n\n이 청킹 방법에서는 문서를 그 내재적인 구조에 기반하여 분할합니다. 이 방법은 내용의 흐름과 구조를 고려하지만 명확한 구조가 없는 문서의 경우 효과적이지 않을 수 있습니다.\n\n- 마크다운 포맷의 문서: Langchain은 마크다운을 구분자로 사용하여 문서를 분할하는 MarkdownTextSplitter 클래스를 제공합니다.\n- Python/JS 코드 포함 문서: Langchain은 Python 프로그램을 클래스, 함수 등을 기반으로 분할하는 PythonCodeTextSplitter를 제공하며 RecursiveCharacterTextSplitter 클래스의 from_language 메서드를 통해 언어를 제공할 수 있습니다.\n- 테이블을 다루는 문서: 테이블을 처리할 때 1단계와 2단계에 기반하여 분할하면 행과 열 사이의 관계가 손실될 수 있습니다. 이 관계를 보존하기 위해 테이블 내용을 언어 모델이 이해할 수 있는 방식으로 형식화합니다 (예: HTML의 `table` 태그, `;`로 구분된 CSV 형식 등). 시맨틱 검색 중에 테이블에서 바로 포함된 내용과 일치시키는 것은 어려울 수 있습니다. 개발자들은 종종 추출 후 테이블을 요약하고 해당 요약의 임베딩을 생성하여 일치시키는 데 사용합니다.\n- 이미지를 포함하는 문서 (멀티 모달): 이미지와 텍스트의 임베딩은 서로 다를 수 있습니다 (CLIP 모델은 이를 지원). 가장 좋은 전략은 멀티 모달 모델(예: GPT-4 vision)을 사용하여 이미지의 요약을 생성하고 해당 임베딩을 저장하는 것입니다. Unstructured.io는 pdf 문서에서 이미지를 추출하기 위한 partition_pdf 메서드를 제공합니다.\n\n# 레벨 4: 시맨틱 청킹\n\n<div class=\"content-ad\"></div>\n\n위의 세 가지 수준은 문서의 내용과 구조를 다루며 일정한 청크 크기의 값을 유지해야 한다. 이 청킹 방법은 임베딩에서 의미를 추출한 다음 이러한 청크 간의 의미적 관계를 평가하기 위한 것이다. 핵심 아이디어는 의미적으로 유사한 청크를 함께 유지하는 것입니다.\n\n![image](/assets/img/2024-06-22-FiveLevelsofChunkingStrategiesinRAGNotesfromGregsVideo_0.png)\n\nLlamindex에는 SemanticSplitterNodeParse 클래스가 있어 문서를 청크로 분할할 수 있습니다. 이는 청크 간의 맥락적 관계를 이용하여 문장 사이의 중단점을 임베딩 유사성을 이용해 적응적으로 선택합니다.\n\n알아둬야 할 몇 가지 개념들\n\n<div class=\"content-ad\"></div>\n\n- buffer_size: 초기 창(chunks)에 대한 윈도우를 결정하는 구성 가능한 매개변수\n- breakpoint_percentile_threshold: 다른 구성 가능한 매개변수. 청크를 분할할 임계값을 결정하는 값\n- embed_mode: 사용되는 임베딩 모델\n\n# 레벨 5: 주체적 청킹\n\n이 청킹 전략은 LLM을 사용하여 컨텍스트에 기반하여 청크에 포함해야 할 텍스트 양과 내용을 결정하는 가능성을 탐색합니다.\n\n초기 청크를 생성하려면 Proposals 개념을 사용하며, 이는 원시 텍스트에서 독립된 문장을 추출하는 논문에 기초합니다. Langchain은 이를 구현하기 위한 Proposal 검색 템플릿을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n주장을 생성한 후, 이들은 LLM 기반 에이전트에게 공급됩니다. 이 에이전트는 주장이 기존 청크에 포함되어야 하는지 또는 새로운 청크를 생성해야 하는지를 결정합니다.\n\n![이미지](/assets/img/2024-06-22-FiveLevelsofChunkingStrategiesinRAGNotesfromGregsVideo_1.png)\n\n# 결론\n\n본 문서에서는 Langchain 및 Llamaindex 프레임워크에서 다양한 청킹 전략과 그 구현 방법을 탐색했습니다.\n\n<div class=\"content-ad\"></div>\n\nGreg가 작성한 코드를 찾으려면 다음 링크를 참조하세요: [Greg의 코드](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb)\n\nGenerative AI 및 기계 학습이 더 빠르게 발전하고 있기 때문에 이 기사를 지속적으로 업데이트할 것입니다. Generative AI 및 기계 학습의 발전에 대해 자주 쓰기 때문에 LinkedIn에서 저를 팔로우하십시오: [LinkedIn 프로필](https://www.linkedin.com/in/anurag-mishra-660961b7/)","ogImage":{"url":"/assets/img/2024-06-22-FiveLevelsofChunkingStrategiesinRAGNotesfromGregsVideo_0.png"},"coverImage":"/assets/img/2024-06-22-FiveLevelsofChunkingStrategiesinRAGNotesfromGregsVideo_0.png","tag":["Tech"],"readingTime":4},{"title":"Code Llama로 나만의 LLM 코딩 어시스턴트 만드는 방법 ","description":"","date":"2024-06-22 21:36","slug":"2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama","content":"\n\n이 실습에서는 무료로 사용할 수 있고 로컬 GPU에서 실행되는 AI 코드 어시스턴트를 구현할 예정입니다.\n\n챗봇에 질문을 하면 자연어로 답변하며 여러 프로그래밍 언어로 코드도 제공합니다.\n\n우리는 Hugging Face transformer 라이브러리를 사용하여 LLM을 구현하고 Chatbot 프론트 엔드에는 Streamlit을 사용할 것입니다.\n\n# LLM이 텍스트를 생성하는 방법은 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n디코더 전용 트랜스포머 모델인 GPT 계열은 주어진 입력 프롬프트에 대한 다음 단어를 예측하도록 훈련되었습니다. 이로 인해 텍스트 생성에 아주 능숙합니다.\n\n![이미지](/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_0.png)\n\n충분한 훈련 데이터가 제공된다면, 코드를 생성하는 것도 배울 수 있습니다. IDE에서 코드를 채우는 방식이나 챗봇으로 질문에 답변하는 방식으로 가능합니다.\n\nGitHub Copilot은 상용 예시로서 AI 페어 프로그래머의 한 예입니다. Meta AI의 Code Llama 모델은 유사한 능력을 갖추고 있지만 무료로 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 코드 람마란 무엇인가요?\n\n코드 람마는 Meta AI가 만들고 2023년 8월에 처음으로 출시한 코드 전용 LLM 계열의 특별한 제품입니다.\n\n![이미지](/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_1.png)\n\nMeta AI는 기본 모델 Llama 2(디코더 전용 Transformer 모델로 GPT-4와 유사함)을 시작으로, 대부분 코드로 이루어진 500B 토큰의 교육 데이터를 활용하여 추가 교육을 진행했습니다.\n\n<div class=\"content-ad\"></div>\n\n그 이후로 Code Llama에 대한 세 가지 버전이 네 가지 다른 크기로 제공됩니다.\n\nCode Llama 모델은 연구 및 상업적 사용을 위해 무료입니다.\n\n![이미지](/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_2.png)\n\n## Code Llama\n\n<div class=\"content-ad\"></div>\n\n코드 Llama는 코드 생성을 위한 기반 모델입니다. 코드 Llama 모델은 infill 목적으로 훈련되어 IDE 내에서 코드 완성을 위해 설계되었습니다.\n\n## 코드 Llama — Instruct\n\nInstruct 버전은 인간의 질문에 답변하기 위해 지시 데이터셋에 맞춰 세밀하게 조정되었습니다. 이는 ChatGPT와 유사합니다.\n\n## 코드 Llama — Python\n\n<div class=\"content-ad\"></div>\n\n파이썬 버전은 추가 데이터셋인 100B 토큰의 파이썬 코드로 훈련되었습니다. 이 모델들은 코드 생성을 위해 의도되었습니다.\n\n# LLM 챗봇 코딩\n\n본 튜토리얼에서는 Instruct 버전 중 가장 작은 모델인 CodeLlama-7b-Instruct — hf를 사용할 것입니다. 이 모델은 자연어 질문에 답변하도록 세밀하게 튜닝되어 있기 때문에 챗봇으로 사용할 수 있습니다.\n\n가장 작은 모델조차도 여전히 7B 매개변수로 상당히 큽니다. 매개변수의 16비트 반정밀도를 사용하면, 모델은 약 14 GB의 GPU 메모리가 필요합니다. 4비트 양자화를 사용하면, 메모리 요구 사항을 약 3.5 GB 정도로 줄일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 모델 구현하기\n\n우리는 먼저 Hugging Face에서 Code Llama 모델을 불러오고 주어진 프롬프트에 기반하여 텍스트를 생성할 ChatModel 클래스를 생성하는 것으로 시작하겠습니다.\n\n우리는 4비트 양자화를 위해 BitsAndBytesConfig를 사용하며, 모델을 로드하기 위해 AutoModelForCausalLM을 사용하고 입력 프롬프트로부터 토큰 임베딩을 생성하기 위해 AutoTokenizer를 사용합니다.\n\n```js\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nclass ChatModel:\n    def __init__(self, model=\"codellama/CodeLlama-7b-Instruct-hf\"):\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True, # 4비트 양자화 사용\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n        )\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model,\n            quantization_config=quantization_config,\n            device_map=\"cuda\",\n            cache_dir=\"./models\", # 모델을 models 폴더에 다운로드\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            model, use_fast=True, padding_side=\"left\"\n        )\r\n```\n\n<div class=\"content-ad\"></div>\n\n또한, 사용자의 이전 입력 프롬프트와 AI가 생성한 응답을 저장하는 고정 길이의 히스토리 목록을 만듭니다. 이는 대화의 기억을 제공하여 LLM에게 대화의 기억을 부여하는 데 유용합니다.\n\n```js\nself.history = []\nself.history_length = 1\n```\n\nCode Llama은 사용자 프롬프트 앞에 시스템 프롬프트를 사용합니다.\n\n기본적으로, codellama-13b-chat 예제에서 시스템 프롬프트를 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nself.DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n당신은 코드와 소프트웨어 디자인에 대한 깊은 지식을 가진, 도움이 되는, 예의 바르고 정직한 도우미입니다. 항상 도움이 될 수 있는 답변을 해야 하며, 안전하고 신중해야 합니다. 답변에 해로운, 부정한, 인종 차별적, 성 차별적, 유해한, 위험한, 또는 불법적인 내용을 포함해서는 안 됩니다. 답변이 사회적으로 편향되거나 부정적이여선 안됩니다.\\n\\n만약 질문이 이해할 수 없거나 사실적으로 일관성이 없다면, 올바른 대답 대신 왜 잘못된 것인지 설명하세요. 만약 질문에 대한 대답을 모르면, 가짜 정보를 공유하지 말고 대신 말해주세요.\\\n        \"\"\"\n```\n\n이제 self.history에 현재 대화를 추가하는 함수를 구현해봅시다.\n\nLLM(어라운드  모델)은 한정된 문맥 길이를 가지고 있기 때문에 메모리에 정보를 한정적으로 보관할 수밖에 없습니다. 여기서는 self.history_length = 1 개의 질문과 대답만 최대한 보관합니다.\n\n```js\n    def append_to_history(self, user_prompt, response):\n        self.history.append((user_prompt, response))\n        if len(self.history) > self.history_length:\n            self.history.pop(0)\n```\n\n<div class=\"content-ad\"></div>\n\n마침내 우리는 입력 프롬프트에 기반한 텍스트를 생성하는 generate 함수를 구현합니다.\n\n각 LLM에는 훈련에 사용된 특정 프롬프트 템플릿이 있습니다. Code Llama의 경우 codellama-13b-chat의 프롬프트 템플릿을 참조로 사용했습니다.\n\n```js\n    def generate(\n        self, user_prompt, system_prompt, top_p=0.9, temperature=0.1, max_new_tokens=512\n    ):\n\n        texts = [f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n\"]\n        do_strip = False\n        for old_prompt, old_response in self.history:\n            old_prompt = old_prompt.strip() if do_strip else old_prompt\n            do_strip = True\n            texts.append(f\"{old_prompt} [/INST] {old_response.strip()} </s><s>[INST] \")\n        user_prompt = user_prompt.strip() if do_strip else user_prompt\n        texts.append(f\"{user_prompt} [/INST]\")\n        prompt = \"\".join(texts)\n\n        inputs = self.tokenizer(\n            prompt, return_tensors=\"pt\", add_special_tokens=False\n        ).to(\"cuda\")\n\n        output = self.model.generate(\n            inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            pad_token_id=self.tokenizer.eos_token_id,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            top_p=top_p,\n            top_k=50,\n            temperature=temperature,\n        )\n        output = output[0].to(\"cpu\")\n        response = self.tokenizer.decode(output[inputs[\"input_ids\"].shape[1] : -1])\n        self.append_to_history(user_prompt, response)\n        return response\n```\n\n응답은 시스템 프롬프트와 사용자 프롬프트를 기반으로 합니다. 답변의 창의성은 top_p 및 temperature와 같은 매개변수에 따라 달라집니다.\n\n<div class=\"content-ad\"></div>\n\ntop_p를 사용하면 출력 토큰의 확률 값을 제한하여 너무 드물게 발생하는 토큰을 생성하는 것을 피할 수 있어요:\n\ntemperature를 사용하면 출력 토큰의 확률 분포를 평평하게 하거나 날카롭게 할 수 있어요:\n\n프론트엔드 애플리케이션을 진행하기 전에 ChatModel을 테스트해보죠.\n\n```js\nfrom ChatModel import *\n\nmodel = ChatModel()\nresponse = model.generate(\n    user_prompt=\"C++에서 hello world 프로그램을 작성해봐\", \n    system_prompt=model.DEFAULT_SYSTEM_PROMPT\n)\nprint(response)\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n당신이 요청한 작업은 완료되었습니다. 이제 테이블 태그가 Markdown 형식으로 변경되었습니다.\n```\n\n<div class=\"content-ad\"></div>\n\n```js\r\nimport streamlit as st\nfrom ChatModel import *\n\nst.title(\"Code Llama Assistant\")\n\n\n@st.cache_resource\ndef load_model():\n    model = ChatModel()\n    return model\n\n\nmodel = load_model()  # load our ChatModel once and then cache it\r\n```\n\n다음으로 generate 함수를 위한 모델 매개변수를 입력 제어하는 사이드바를 생성합니다.\n\n```js\r\nwith st.sidebar:\n    temperature = st.slider(\"온도\", 0.0, 2.0, 0.1)\n    top_p = st.slider(\"top_p\", 0.0, 1.0, 0.9)\n    max_new_tokens = st.number_input(\"max_new_tokens\", 128, 4096, 256)\n    system_prompt = st.text_area(\n        \"시스템 프롬프트\", value=model.DEFAULT_SYSTEM_PROMPT, height=500\n    )\r\n```\n\n그리고 챗봇 메시지 인터페이스를 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 채팅 기록 초기화\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\n# 앱 재실행시 기록된 채팅 메시지 표시\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\n# 사용자 입력 받기\nif prompt := st.chat_input(\"무엇이든 물어보세요!\"):\n    # 사용자 메시지를 채팅 기록에 추가\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    # 사용자 메시지를 채팅 메시지 컨테이너에 표시\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n\n    # 챗봇 응답을 채팅 메시지 컨테이너에 표시\n    with st.chat_message(\"assistant\"):\n        user_prompt = st.session_state.messages[-1][\"content\"]\n        answer = model.generate(\n            user_prompt,\n            top_p=top_p,\n            temperature=temperature,\n            max_new_tokens=max_new_tokens,\n            system_prompt=system_prompt,\n        )\n        response = st.write(answer)\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\r\n```\n\n스트림릿 앱을 streamlit run app.py로 실행하여 브라우저가 열립니다.\n\n이제 챗봇에 코딩 관련 질문을 할 수 있습니다.\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n저희는 Meta AI의 Code Llama LLM을 활용하여 AI 코딩 어시스턴트를 구현했어요. 그리고 Hugging Face의 transformer 라이브러리와 Streamlit을 사용해서 프론트엔드 애플리케이션을 만들었어요.\n\n6GB의 GPU 메모리를 갖춘 노트북으로는 4비트 양자화된 Code Llama 모델을 7B 매개변수와 함께 사용할 수밖에 없었어요. 더 큰 GPU를 사용하면 16비트 버전이나 더 큰 모델이 더 잘 작동할 것입니다.\n\nP.S. Code Llama로부터 제가 받은 농담보다 더 재미있는 농담들을 기대해봅니다 🤡.\n\n더 많은 LLM에 관심이 있으시다면, 최근에 공개된 오픈소스 모델에 대한 개요를 확인해보세요:\n\n<div class=\"content-ad\"></div>\n\n# 참고 자료\n\n[1] B. Rozière 외: Code Llama: 코드를 위한 오픈 기반 모델 (2023), arXiv:2308.12950\n\n# 자원\n\n- Streamlit 채팅 앱 예제: 기본 LLM 채팅 앱 구축\n- Hugging Face Code Llama gradio 구현: codellama-13b-chat\n- 이 문서의 전체 작업 코드: [https://github.com/leoneversberg/codellama-chatbot](https://github.com/leoneversberg/codellama-chatbot)","ogImage":{"url":"/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_0.png"},"coverImage":"/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_0.png","tag":["Tech"],"readingTime":9},{"title":"효율적인 HNSW 인덱싱 대규모 병렬 처리를 통한 인덱스 빌드 시간 단축 하는 방법","description":"","date":"2024-06-22 21:34","slug":"2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism","content":"\n\n## 소개\n\n검색 증강 생성 (RAG)을 위한 Generative AI (GenAI) 및 전자 상거래를 위한 추천과 같은 응용 프로그램에 의해 주도되는 벡터 데이터베이스 분야에서, HNSW (Hierarchical Navigable Small Worlds) 알고리즘은 근사 최근접 이웃 (ANN) 검색을 위해 상위 벡터 데이터베이스 제공 업체에서 많이 사용됩니다. 이 알고리즘은 빠른 검색과 좋은 검색률을 제공하지만 인덱스 빌드 시간이 길다는 점을 감수해야 합니다.\n\n인덱스 빌드 시간이 길면 확장성 제한, 운영 비용 증가, 개발자들을 위한 실험 감소, 그리고 동적 데이터셋의 느린 업데이트와 같은 여러 가지 도전에 직면하게 됩니다.\n\n본 블로그 포스트에서는 HNSW 인덱스를 빌드하는 데 오랜 시간이 걸리는 이유를 설명하고, 고도의 병렬성을 활용하여 CPU를 기반으로 한 기존 솔루션과 비교하여 인덱스 빌드 시간을 대략 85% 줄일 수 있는 솔루션을 제시합니다.\n\n<div class=\"content-ad\"></div>\n\n## HNSW 개요\n\nHNSW는 쿼리의 근사 최근 이웃을 효율적으로 탐색하는 그래프 기반 알고리즘입니다. 그래프에는 계층적 노드(벡터 임베딩)가 있으며, 각 계층에는 이전 계층의 일부가 포함됩니다. 노드는 에지로 연결되어 있으며, 에지는 그들 사이의 거리를 나타내며, 거리는 코사인 유사성과 같은 메트릭으로 측정됩니다.\n\n상위 계층은 더 적은 노드와 더 긴 에지를 가지고 있어서 그래프의 먼 영역을 연결하고 벡터 공간을 빠르게 탐색할 수 있습니다. 가장 아래 계층에는 모든 벡터가 포함되어 있으며, 짧은 범위의 에지(인접 노드로의 연결)를 가지고 있어서 더 세부적인 검색이 가능합니다.\n\n그림 1은 HNSW 그래프 구조의 간단한 예를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism_0.png)\n\n인덱스를 구축하거나 업데이트하는 동안, 새 노드는 기존 노드로부터의 거리에 따라 삽입됩니다. 각 레이어마다 각 정점의 최근 이웃 목록이 유지되며, 해당 크기는 매개변수 ef_construction에 의해 결정됩니다.\n\n알고리즘은 이 목록의 노드를 반복하면서, 가장 가까운 이웃들과의 거리 계산을 수행하여 노드의 이웃이 쿼리에 더 가까운지 확인합니다. 그렇다면 해당 이웃은 목록에 추가할 후보로 고려됩니다.\n\n더 큰 ef_construction은 더 많은 후보를 추적하고 평가하여 실제 최근 이웃을 찾을 가능성을 높이며, 정확도를 향상시키지만 더 많은 거리 계산이 필요하기 때문에 인덱스 구축 시간이 늘어납니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 느린 인덱스 빌드 시간의 결과\n\n이전 섹션에서 볼 수 있듯이, HNSW 인덱스를 구축하려면 계층적 그래프에서 가장 가까운 이웃을 찾기 위해 많은 거리 계산이 필요합니다. 이로 인해 검색 대기 시간이 낮고 재현율이 좋아지지만, 이는 빌드 및 업데이트가 느린 그래프라는 희생을 갖게 됩니다.\n\n예를 들어, Figure 2에서 볼 수 있듯이, eBay의 경험에 따르면 1억 6천만 개의 벡터를 위해 HNSW 인덱스를 구축하는 데 3~6시간이 걸릴 수 있으며 데이터셋이 커질수록 빌드 시간이 빨리 증가합니다.\n\n이는 수십억 개의 실시간 리스트가 있는 이러한 기업에게 문제가 될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism_1.png\" />\n\n느린 인덱스 빌드 시간은 많은 도전을 야기할 수 있습니다: 확장성 문제, 운영 비용 증가, 개발자 응용 프로그램 실험 제한 및 동적 데이터 세트 업데이트 지연.\n\n확장성 문제\n\n인덱스 빌드 시간이 느릴수록 전자 상거래 및 RAG와 같은 애플리케이션의 확장성이 제한됩니다. 전자 상거래의 경우, 고객 기반과 제품 카탈로그 확장으로 인덱스 빌드 시간이 증가하여 관련 제품 추천의 서비스가 지연됩니다. RAG의 경우, 더 큰 데이터 세트는 높은 품질의 응답을 제공하지만, 느린 인덱스 빌드로 효율적으로 관리할 수 있는 데이터 양이 제한됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n운영 비용 증가\n\n인덱스 빌드 시간이 느린 경우 CPU 및 GPU와 같은 계산 리소스가 더 오래 사용됩니다. 이는 특히 리소스 사용에 비례한 비용이 발생하는 클라우드 컴퓨팅 환경에서 운영 비용을 증가시킵니다.\n\n개발자 응용 프로그램 실험 제한\n\n개발자들은 모델을 세밀하게 조정하고 응용 프로그램 성능을 개선하기 위해 빠르게 실험해야 합니다. 인덱스 빌드 시간이 길면 실험 횟수를 제한하고 그 실험을 평가하는 데 필요한 시간이 증가합니다. 이는 혁신을 늦추고 개선을 미루게 합니다.\n\n<div class=\"content-ad\"></div>\n\n동적 데이터셋의 느린 업데이트\n\n전자 상거래 및 RAG와 같은 애플리케이션에는 빈번히 업데이트되는 동적 데이터셋이 있습니다. 느린 인덱스 빌드는 새로운 데이터의 통합을 지연시켜 고갱도 비추천 및 응답의 만료로 이어지며, 이는 사용자 만족도 및 신뢰에 부정적인 영향을 미칠 수 있습니다.\n\n## 인덱스 빌드 시간을 줄이는 방법\n\n인덱스 빌드 시간을 줄이는 두 가지 효과적인 방법은 병렬 처리와 벡터 양자화입니다.\n\n<div class=\"content-ad\"></div>\n\n병렬 처리\n\n데이터 세트는 벡터 군집으로 분할되고 해당 군집 내에서 가장 가까운 이웃 거리 계산이 병렬로 수행됩니다. 병렬 처리는 거리 계산에 필요한 시간을 크게 줄여줍니다. 이는 인덱스 구축 과정 중 가장 시간이 많이 소요되는 부분입니다.\n\n그러나 대부분의 HNSW 인덱스 빌드 솔루션에서 사용하는 CPU는 제한된 병렬 처리 능력을 갖고 있으므로 더 높은 병렬 처리 능력을 갖춘 솔루션이 필요합니다.\n\n벡터 양자화\n\n<div class=\"content-ad\"></div>\n\n벡터 양자화는 벡터를 압축하여 데이터 전송 당 더 많은 벡터를 포장할 수 있게 하며, 더 적은 비트에서 최근접 이웃 거리 계산을 수행하여 단순화합니다. 이로써 외부 메모리에서의 느린 메모리 접근 수를 줄이고 거리 계산 속도를 높일 수 있습니다.\n\n## 메모리 내 계산 (CiM) 연상 처리 - 유연한 대규모 병렬 처리\n\nGSI Technology의 메모리 내 계산 연상 처리 장치 (APU)는 백만 개의 비트 프로세서를 가진 기술로, 비트 수준에서 계산을 수행하여 유연한 양자화를 가능케 합니다. 이는 모든 크기의 데이터 요소에서 대규모 병렬 처리가 가능하도록 합니다.\n\nAPU를 사용하면 계산이 메모리 내에서 직접 수행되므로 프로세서와 메모리 간의 전통적인 병목 현상을 피할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nAPU HNSW Index Build Process\n\n1. 양자화: 데이터 세트를 원하는 비트 길이로 압축합니다 (예: 특성 당 4비트).\n\n2. 클러스터링: K-평균 클러스터링을 사용하여 데이터 세트를 클러스터로 분할합니다.\n\n3. 할당: 각 정점을 클러스터 중심까지의 거리를 기반으로 가장 가까운 클러스터에 할당합니다.\n\n<div class=\"content-ad\"></div>\n\n4. 데이터 불러오기: 여러 클러스터를 APU에 로드합니다.\n\n5. 최근접 이웃 탐색: APU에 로드된 클러스터에 할당된 정점들을 대상으로 최근접 이웃 탐색을 수행합니다.\n\n6. 단계 4와 5 반복: 모든 클러스터가 처리되고 모든 정점이 그래프에 삽입될 때까지 단계 4와 5를 반복합니다.\n\n7. 이웃들의 합집합: 각 정점에 대해 다수의 클러스터에서 가장 가까운 이웃들을 병합합니다.\n\n<div class=\"content-ad\"></div>\n\n8. 최적화: 연결 후, 간선을 양방향으로 만들고 각 정점이 중복된 간선을 제거하여 `= K`개의 이웃을 보유하도록 보장합니다.\n\nAPU는 수백만 비트 프로세서를 사용하여 5단계에서의 최근접 이웃 거리 계산을 병렬로 수행합니다. 이 대규모 병렬 처리는 유연한 양자화와 함께 거리 계산에 필요한 시간을 크게 줄여줍니다. 최근접 이웃 거리 계산을 수행하는 것은 인덱스를 구축하는 가장 시간이 많이 소요되는 부분입니다. 이를 줄이는 것이 인덱스를 빠르게 구축하는 데 가장 큰 영향을 미칠 것입니다.\n\n## 결과\n\nNvidia의 Figure 3에 따르면, 인텔 Xeon Platinum 8480CL CPU는 1억 개의 벡터를 위한 HNSW 인덱스를 구축하는 데 약 5,636초(약 1.5시간)가 걸리는 것을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Figure 4](/assets/img/2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism_2.png)\n\nFigure 4에 따르면 APU 시스템이 100 백만 벡터 HNSW 인덱스를 864초(약 15분)에 구축할 수 있습니다. 이는 Intel Xeon Platinum 8480CL CPU의 5,636초(약 1.5시간)에 비해 약 85%의 시간 단축입니다.\n\n또한, Figure 4는 APU 시스템이 약 2시간에 10억 벡터 HNSW 인덱스를 구축할 수 있다는 것을 보여주며, 이는 10억 벡터를 가정하여 eBay의 성능 개선을 비교했을 때 극적인 향상을 보입니다.\n\n![Figure 4](/assets/img/2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism_3.png)\n\n\n<div class=\"content-ad\"></div>\n\n## 결론\n\nHNSW는 GenAI와 전자 상거래와 같은 애플리케이션에서 사용되는 벡터 유사성 검색에 중요한 알고리즘입니다. 높은 검색률과 빠른 검색을 제공하지만, 색인 빌드 시간이 길어지는 단점이 있습니다.\n\n색인 빌드 시간을 줄이는 주요 방법 중 하나는 병렬화입니다. 안타깝게도 대부분의 현재 솔루션은 한정된 병렬 처리 능력을 갖춘 CPU를 사용합니다.\n\n대량의 병렬 처리와 유연한 양자화를 통해 GSI Technology의 APU는 기존 CPU 기반 솔루션과 비교하여 색인 빌드 시간을 약 85% 줄일 수 있습니다. 이는 확장성을 향상시키고 운영 비용을 낮추며 빠른 개발자 애플리케이션 실험을 가능하게 하며, 동적 데이터 세트에 대한 적시적인 업데이트를 보장합니다.\n\n<div class=\"content-ad\"></div>\n\nAPU의 유연한 비트 수준 처리가 대량 병렬 처리를 제공하여 인덱스 빌드 시간을 크게 줄이는 방법에 대한 자세한 내용은 화이트페이퍼 \"효율적인 HNSW: 인덱스 빌드 시간을 85%로 단축\"을 읽어보세요.\n\nGSI 기술의 온프레미스 및 클라우드 기반 HNSW 인덱스 빌드 옵션에 대해 알아보려면 associativecomputing@gsitechnology.com 으로 문의하세요.","ogImage":{"url":"/assets/img/2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism_0.png"},"coverImage":"/assets/img/2024-06-22-EfficientHNSWIndexingReducingIndexBuildTimeThroughMassiveParallelism_0.png","tag":["Tech"],"readingTime":6},{"title":"일반 NLP 방법론 소개 기초부터 고급까지","description":"","date":"2024-06-22 21:31","slug":"2024-06-22-IntroofGenericNLPmethods","content":"\n\n# NLP 배경 리뷰\n\n자연어 처리(NLP)는 광범위한 응용 분야를 갖고 있고 챗봇, 기계 번역과 같이 점점 더 다양한 제품들이 상용화되고 있습니다. 그러나 다양한 작업들은 각기 다른 요구 사항을 가지고 있어 하나의 표준 NLP 모델로는 이러한 요구 사항을 다루기 어렵습니다. 최근의 작업에서는 기계가 의도를 심층적으로 이해하고 자연스럽게 언어를 생성하기를 요구합니다. 딥러닝의 빠른 발전으로 복잡한 NLP 모델이 더 복잡한 문제들을 다룰 수 있게 되었습니다. NLP에 대한 미래는 밝아 보입니다. 그러나, 연구자들과 전문가들에게는 새로운 어려움의 산이 등장하고 있습니다. 더 효율적인 NLP 모델을 찾고 고품질의 데이터셋을 수집하고 주어진 계산 및 저장 자원의 제한을 다루기 위한 실행 가능한 해결책을 마련해야 하는 엄중한 도전에 직면하고 있습니다. 이러한 어려움은 한 번에 해결될 수 없습니다. 먼저 거대한 모델을 사전 학습하고 그 모델을 세밀하게 조정하거나 프롬프팅하는 두 단계 방법은 실행 가능하며 새로운 표준 방법이 되고 있습니다.\n\n개인적 취향으로 최근 NLP에서 사용되는 고급 방법들을 배우는 데 시간을 투자했습니다. NLP는 상당히 복잡한 문제로 여겨집니다. 이러한 방법들에 대한 기본 아이디어들은 다른 작업 영역에 적용될 수 있으며, IoT를 위한 편재 학습과 같은 다중 모드 학습이 있습니다.\n\n아래는 사전 학습, 세밀 조정, 프롬프트와 관련된 고전적 방법들을 정리해 보았습니다. 그 중에 BERT, GPT, 적응형 세밀 조정, 행동 세밀 조정, 파라미터 효율적 세밀 조정, 프롬프트 엔지니어링이 포함되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 사전 훈련 소개\n\n어노테이션 및 대규모 데이터셋의 수집은 비용이 많이 들거나 일부 특정 영역의 작업에는 불가능한 경우도 있습니다. 그러나 위키피디아와 같이 많은 양의 라벨이 없는 데이터가 많이 있다는 사실이 있습니다. 이에 따라 자연어 처리 분야에서 싼 비용으로 대규모 라벨이 없는 말뭉치를 사용하여 사전 훈련된 언어 모델(PLM)은 매우 큰 가치를 추가합니다. 이는 PLM이 하류 작업에 대해 빠르고 효과적으로 파인 튜닝될 수 있다는 것을 의미합니다. PLM은 효율적인 모델 초기화를 제공하며 다양한 작업에서 수렴을 가속화할 수 있습니다. 또한 PLM은 특정 영역에 대한 오버피팅을 어느 정도 피할 수 있습니다.\n\n일반적으로 PLM은 문맥 없는 임베딩과 문맥을 고려한 임베딩으로 나뉩니다. 문맥 없는 임베딩 방법은 매우 제한적인 문맥 정보를 갖고 있는 정적 단어 임베딩에 중점을 두며 상대적으로 간단한 모델로 구성됩니다. 이 방법에는 NNLM, word2vec (CBOW, Skip-Gram), Glove가 포함됩니다. 문맥을 고려한 임베딩 방법은 단어 토큰을 문맥 정보를 통합하여 매핑하는 것을 목표로 하며, CoVe, ELMo, GPT, BERT, XLNet이 이에 해당됩니다.\n\n![이미지](/assets/img/2024-06-22-IntroofGenericNLPmethods_0.png)\n\n<div class=\"content-ad\"></div>\n\nTransformer 아키텍처는 NLP 작업을 다루는 강력한 성능을 나타내어, 최근 PLM에서 Transformer 아키텍처를 활용하고 있다 [17]. BERT는 Transformer Encoder 아키텍처를 활용하며, 마스크된 언어 모델링을 특징으로 하며, 양방향 자동 인코딩 언어 모델이다 [18]. GPT는 Transformer Decoder 아키텍처를 활용하며, 단방향 자동 회귀 언어 모델이다 [19].\n\n<img src=\"/assets/img/2024-06-22-IntroofGenericNLPmethods_1.png\" />\n\n사전 훈련 작업은 클래식 언어 모델링, 마스크된 언어 모델링, 순열 언어 모델링, 노이즈 제거 오토인코더, 대조 학습으로 분류될 수 있다. 모델 아키텍처를 개선하거나 지식 그래프와 같은 더 많은 사전 정보를 통합함으로써, RoBERTa [20], ELECTRA [21], BART [22], T5 [23], XLNet, MPNet [24], ERNIE-BaiDu [25], Transformer-XL [26], DistilBERT [27], ALBERT [28], MobileBERT [29], ConvBERT [30], DeBERTa [31], BigBird [32]와 같이 다양한 BERT 또는 GPT 확장 모델이 출현하고 있다.\n\n# 파인튜닝 소개\n\n<div class=\"content-ad\"></div>\n\n사전 학습된 임베딩을 활용한 이전 특성 추출과 비교했을 때, 사전 학습된 언어 모델(PLM)의 파인 튜닝은 자연어 처리의 다양한 하향 작업으로의 전이 학습에 더 효과적이고 강력함이 입증되었습니다. 일반적으로 채택된 파인 튜닝 절차는 첫 번째 단계로 레이블이 지정되지 않은 대규모 데이터를 기반으로 모델을 사전 학습하는 것입니다. 그 과정에서 가장 보편적으로 사용되는 방법은 가려진 언어 모델링(MLM) 접근 방식입니다. 두 번째 단계는 다운스트림 작업이나 도메인에 특화된 레이블이 지정된 데이터에서 PLM을 파인 튜닝하는 것이며, 이때 교차 엔트로피 손실 함수와 같은 표준 손실 함수가 사용됩니다.\n\nPLM은 비교적 고정되어 있고 계산 리소스가 더 많이 필요하며, 더 많은 데이터셋, 효율적인 모델 업데이트가 필요하지만, 파인 튜닝은 다양한 실용적 용도에 중요한 빠른 적응형 모델을 제공할 수 있습니다. 따라서 파인 튜닝은 귀중하며 최근 고급 방법은 파인 튜닝 성능을 더욱 향상시킬 수 있습니다. 예를 들어 적응형 파인 튜닝, 행동적 파인 튜닝, 매개변수 효율적 파인 튜닝, 텍스트 대 텍스트 파인 튜닝, 파인 튜닝 불안정성 완화 등이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n적응형 미세 조정은 PLM을 대상 데이터 분포에 더 적응하도록 전이하는 것을 의미합니다. PLM은 MLM(Masked Language Model) 접근 방식을 사용하여 관련 없는 레이블이 붙지 않은 데이터에 대해 미세 조정을 한 다음, 특정 작업 레이블이 붙은 데이터에 대해 교차 엔트로피 손실을 사용하여 모델을 미세 조정합니다. 대상 데이터 도메인으로의 전이는 특히 전문적인 다른 대상 데이터 분포에 유용하며, 이로 인해 최종 모델의 일반화 능력이 제한되기도 합니다.\n\n[이미지1]\n\n행동적 미세 조정은 주로 작업에 초점을 맞추어 PLM을 작업 레이블이 붙은 데이터에 대해 작업 관련 손실 함수를 사용하여 먼저 미세 조정한 후에 작업을 진행합니다. 이 중간 미세 조정 훈련은 높은 수준의 추론과 추론에 유익하며, 이름이 붙은 엔티티, 파라프레이징, 구문, 답변 문장 선택, 질문에 답변하기와 같은 작업에 적용됩니다.\n\n[이미지2]\n\n<div class=\"content-ad\"></div>\n\nParameter-efficient fine-tuning은 Downstream 작업의 계산 부담을 크게 줄이기 위해 대부분의 PLM을 고정시키고 소수의 매개변수만 미세 조정하는 것이다. 채택된 어댑터는 몇 개의 작은 레이어를 동결된 PLM에 삽입한다. 또는 PLM의 하위 매개변수를 미세 조정하고 나머지 큰 집합을 동결시킬 수도 있다.\n\nText-to-text fine-tuning은 가리거나 문맥연결 모델을 상위 자체 학습 레이어로 대체하여, 가리기 언어 모델을 자율적 모델로 이전시키는 것이다.\n\n작은 대상 데이터셋과 임의의 가중치 초기화는 미세 조정 모델의 심각한 불안정성을 유발할 수 있다. 따라서, PLM 모델을 중간으로 전송하여 대상 도메인이나 작업으로 불안정성을 완화할 수 있으며, 적대적 또는 신뢰 영역 기반 데이터 증강 방법도 유용하다.\n\n# Prompting 소개\n\n<div class=\"content-ad\"></div>\n\nNLP 모델이 점점 더 정교해지면, 분명한 딜레마가 발생합니다. 한편에서는 수십억 개의 매개변수를 가진 보다 일반적이고 강력하지만 무거운 PLM이 존재하는 반면, 다른 한편에서 downstream 작업은 오직 독특하고 가벼우며 빠르게 적응하는 모델이 필요합니다. 한편에서는 많은 편향과 노이즈가 있는 주석이 되지 않은 원본 데이터가 많이 있지만, 다른 한편에서는 과연 소량이고 매우 비싼 과제별 주석 데이터만 사용할 수 있습니다. 한편에서는 대규모 데이터로 상당히 거대한 모델을 훈련시키는 것이 가능하며 고성능 컴퓨팅 센터를 통해 이것이 이루어집니다. 다른 한편에서는 downstream 작업이 한정된 계산 리소스를 제공할 수 있으며 스마트폰과 같은 엣지 컴퓨테이션만 가능합니다. 한편에서는 고전적인 PLM이 더 오랜 시간 동안 업데이트되며, 다른 한편에서는 downstream 작업이 요구 사항을 훨씬 더 빠르게 변경합니다. 요약하면, 이 중첩된 접근법은 방대한 범용 PLM과 유연한 가벼운 과제별 파인튜닝 모델로 이어지며 이러한 목표를 달성하는 데 점점 더 많은 비용이 듭니다. [40]은 이에 대처하기 위한 네 번째 패러다임, 즉 사전 훈련, 프롬프트, 예측을 요약했습니다. 회고적으로, 첫 번째 패러다임은 신경망 없이 특성 엔지니어링을 수동으로 해야 합니다. 두 번째 패러다임은 신경망을 활용하며 아키텍처 엔지니어링을 수동으로 해야 합니다. 세 번째 패러다임은 사전 훈련 및 파인튜닝 접근법을 활용하며 사전 훈련 및 파인튜닝을 위해 적절한 손실 함수를 찾기 위해 목적 함수 엔지니어링을 수동으로 해야 합니다. 네 번째 패러다임은 사전 훈련, 프롬프트, 예측 접근법을 활용하며 적절한 프롬프트를 찾기 위해 프롬프트 마이닝 엔지니어링을 필요로 합니다.\n\n프롬프팅 워크플로우는 4단계로 나눌 수 있습니다. 첫 번째는 프롬프트 구성으로, 필요한 작업을 템플릿에 매핑할 수 있으며, 채워진 템플릿은 PLM에 입력되어 빈 칸 프롬프트(템플릿 텍스트 문자열 중간의 미채워진 슬롯) 및 접두사 프롬프트(템플릿 텍스트 문자열 시작 부분의 미채워진 슬롯)을 포함한 답변을 생성합니다. 두 번째는 답변 구성으로, 생성된 답변을 필요한 레이블로 변환하는 맵 함수를 만들어낼 수 있습니다. 세 번째는 답변 예측으로, 실제로는 고유하게 채워진 템플릿이 PLM에 입력되어 해당하는 답변을 생성합니다. 네 번째는 답변 레이블 매핑으로, 해당하는 답변을 필요한 레이블에 매핑합니다. 요약하면, 원래 입력 x를 템플릿에 채워서 프롬프트 x'를 얻은 후, 일부 빈 칸 슬롯을 채워서 프롬프트 x'를 PLM에 입력하여 완전히 채워진 문자열 x''을 출력하며, 이것은 최종적으로 원하는 레이블 y로 매핑될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n그러므로 템플릿을 가져오는 것이 중요합니다. 즉, 프롬프트 엔지니어링으로 분류될 수 있는데, 이는 수동 템플릿 엔지니어링, 이산 프롬프트를 포함한 자동화된 템플릿 학습, 프롬프트 마이닝, 프롬프트 해석, 기울기 베이스 검색, 프롬프트 생성, 프롬프트 점수 및 접두사 튜닝, 이산 프롬프트로 초기화된 튜닝, 하드 소프트 프롬프트 혼합 튜닝이 추가로 포함됩니다. 더불어 다중 프롬프트는 프롬프트 성능을 향상시키고 결과를 확장할 수 있습니다. 프롬프트 앙상블, 프롬프트 증강, 프롬프트 구성, 프롬프트 분해를 포함합니다. 프롬프트를 생성하는 다양한 방법이 있지만, 최선의 방법을 결정하기 위한 직접 비교는 어렵습니다. 게다가 일부 실험에서는 템플릿의 약간의 변형이나 변경이 결과에 큰 차이를 일으킬 수 있습니다.\n\n프롬프트 [41-46]는 지식 탐색, 분류 기반 작업, 정보 추출, 자연어 처리에서의 추론, 질문에 대한 답변, 텍스트 생성, 텍스트 생성 자동 평가, 멀티모달 학습, 메타 응용 프로그램, 리소스 등과 같은 다양한 하류 작업에 널리 활용될 수 있습니다. 이러한 것들은 프롬프트의 일반화된 성능과 다양한 하류 작업에 대한 무감하임을 증명합니다. 이는 프롬프트 방법이 다양한 하류 작업을 완료할 수 있는 다양한 프롬프트 템플릿을 갖춘 효과적인 PLM을 실현할 수 있다는 것을 의미합니다. 흔히잘 알려진 피드백-\"답변\" 작업이 필요한 경우에, 일반적으로 고전적인 세밀한 튜닝 방법을 사용하여 PLM을 적응시키기 위해 특정 작업 영역 지식이 필요합니다. 반면 프롬프팅은 PLM이 학습한 지식을 활용합니다. 그렇기에 프롬프팅은 다양한 하류 작업의 모델 업데이트에 필요한 계산 부하를 줄이고, 모델 복사본을 저장할 필요가 없다는 것이고, 연습으로 특정 하류 작업에 대한 주석이 달린 데이터는 종종 매우 비싸며, 프롬프팅은 이 작업에 조금의 특정 데이터나 이전 지식으로 유익할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![IntroofGenericNLPmethods](/assets/img/2024-06-22-IntroofGenericNLPmethods_10.png)\n\n작업 특정 데이터 세트의 주석이 작고, 계산 리소스가 제한되며 엣지 노드의 저장 공간이 제한되어 실시간 응답이 필요하고, 다양한 하위 작업의 신속하게 변화하는 요구 사항이 고려될 때 사전 훈련 모델을 채택하고 그 후 적응 엔지니어링을 하는 것이 가장 좋은 방법으로 인정받고 있습니다. Transform 모델은 NLP에서 효과적이라는 것이 입증되었습니다. 따라서 BERT, GPT 및 그 이후에 축Enhanced한 모델들이 널리 사용되고 있습니다. 튜닝 작업은 사전 훈련된 모델을 특정 작업에 적응하는 데 초점을 맞춥니다. 따라서 더 높은 성능을 달성하기 위해 작업 데이터나 작업 도메인으로 이동하는 중간 단계를 채택하는 것이 선호됩니다. 삽입된 적응기를 사용하면 더 동적인 작업 요구 사항을 처리할 수 있습니다. 또는 프롬프트 엔지니어링을 통해 사전 훈련 모델에서 직접 필요한 정보를 추출할 수도 있습니다. 프롬프팅 방법은 인간 실무자에게 매우 직접적이고 이해하기 쉽지만, 일부 학자들은 하위 작업으로 세밀 조정하지 않으면 최고의 성능을 얻을 수 없다고 주장합니다.\n\n# 참고 자료\n\n1. Edunov, S.; Baevski, A.; Auli, M., Pre-trained language model representations for language generation. arXiv preprint arXiv:1903.09722 2019.\n\n<div class=\"content-ad\"></div>\n\n2. Dong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y.; Gao, J.; Zhou, M.; Hon, H.-W., 통합 언어 모델 사전 훈련을 통한 자연어 이해 및 생성. Advances in Neural Information Processing Systems 2019, 32.\n\n3. Zhang, X.; Li, P.; Li, H., Ambert: 다중 단위 토큰화를 사용한 사전 훈련된 언어 모델. arXiv preprint arXiv:2008.11869 2020.\n\n4. Wu, S.; He, Y. Entity 정보로 언어 모델을 보강하여 관계 분류의 성능 향상, Proceedings of the 28th ACM international conference on information and knowledge management, 2019; pp 2361–2364.\n\n5. Gunel, B.; Du, J.; Conneau, A.; Stoyanov, V., 사전 훈련된 언어 모델 파인튜닝을 위한 지도 대조적 학습. arXiv preprint arXiv:2011.01403 2020.\n\n<div class=\"content-ad\"></div>\n\n6. Qiu, X.; Sun, T.; Xu, Y.; Shao, Y.; Dai, N.; Huang, X., 자연어 처리를 위한 사전 학습 모델: 조사. Science China Technological Sciences 2020, 63 (10), 1872–1897.\n\n7. Esmaeilzadeh, A.; Taghva, K. 신경망 언어 모델 (NNLM) 및 BERT를 활용한 텍스트 분류: 경험적 비교, SAI Intelligent Systems Conference 논문집, Springer: 2021; pp 175–189.\n\n8. Church, K. W., Word2Vec. Natural Language Engineering 2017, 23 (1), 155–162.\n\n9. Rong, X., word2vec 매개변수 학습 해설. arXiv 사전 인쇄 arXiv:1411.2738 2014.\n\n<div class=\"content-ad\"></div>\n\n- Ma, L.; Zhang, Y. (2015) Using Word2Vec to process big text data. 2015 IEEE International Conference on Big Data (Big Data), IEEE, pp. 2895–2897.\n- Pennington, J.; Socher, R.; Manning, C. D. (2014) Glove: Global vectors for word representation. Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543.\n- McCann, B.; Bradbury, J.; Xiong, C.; Socher, R. Learned in translation: Contextualized word vectors. Advances in neural information processing systems 2017, 30.\n- Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; Zettlemoyer, L. (2018) Deep contextualized word representations. arXiv:1802.05365. [Link](https://ui.adsabs.harvard.edu/abs/2018arXiv180205365P) (accessed February 01, 2018).\n\n<div class=\"content-ad\"></div>\n\n14. Floridi, L.; Chiriatti, M., GPT-3: 그 특성, 범위, 한계 및 결과. Minds and Machines 2020, 30 (4), 681–694.\n\n15. Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K., Bert: 언어 이해를 위한 깊은 양방향 트랜스포머 사전 훈련. arXiv 사전 인쇄 arXiv:1810.04805 2018.\n\n16. Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R. R.; Le, Q. V., Xlnet: 언어 이해를 위한 일반화된 자기 회귀 사전 훈련. 신경 정보 처리 시스템의 진보 2019, 32.\n\n17. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; Polosukhin, I., Attention is all you need. 신경 정보 처리 시스템의 진보 2017, 30.\n\n<div class=\"content-ad\"></div>\n\n18. Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K., BERT: Deep Bidirectional Transformers를 위한 사전 훈련. ArXiv 2019, abs/1810.04805.\n\n19. Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I., 생성 사전 훈련을 통한 언어 이해 개선. 2018.\n\n20. Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; Stoyanov, V., Roberta: 견고하게 최적화된 bert 사전 훈련 접근 방식. arXiv 사전 인쇄 arXiv:1907.11692 2019.\n\n21. Clark, K.; Luong, M.-T.; Le, Q. V.; Manning, C. D., Electra: 텍스트 인코더를 생성자가 아닌 판별자로 사전 훈련하는 방법. arXiv 사전 인쇄 arXiv:2003.10555 2020.\n\n<div class=\"content-ad\"></div>\n\n22. Lewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; Mohamed, A.; Levy, O.; Stoyanov, V.; Zettlemoyer, L. 2019. \"Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.\" arXiv preprint arXiv:1910.13461 \n\n23. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; Liu, P. J. 2019. \"Exploring the limits of transfer learning with a unified text-to-text transformer.\" arXiv preprint arXiv:1910.10683 \n\n24. Song, K.; Tan, X.; Qin, T.; Lu, J.; Liu, T.-Y. 2020. \"Mpnet: Masked and permuted pre-training for language understanding.\" Advances in Neural Information Processing Systems, 33, 16857–16867.\n\n25. Wei, J.; Ren, X.; Li, X.; Huang, W.; Liao, Y.; Wang, Y.; Lin, J.; Jiang, X.; Chen, X.; Liu, Q. 2019. \"Nezha: Neural contextualized representation for Chinese language understanding.\" arXiv preprint arXiv:1909.00204\n\n<div class=\"content-ad\"></div>\n\n26. Dai, Z.; Yang, Z.; Yang, Y.; Carbonell, J.; Le, Q. V.; Salakhutdinov, R., Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860 2019.\n\n27. Sanh, V.; Debut, L.; Chaumond, J.; Wolf, T., DistilBERT, BERT의 압축 버전: 더 작고, 빠르고, 저렴하고 가벼움. arXiv preprint arXiv:1910.01108 2019.\n\n28. Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; Soricut, R., Albert: 자기지도 언어 표현의 가벼운 BERT. arXiv preprint arXiv:1909.11942 2019.\n\n29. Sun, Z.; Yu, H.; Song, X.; Liu, R.; Yang, Y.; Zhou, D., MobileBERT: 자원 제한 장치를 위한 콤팩트한 범용 BERT. arXiv preprint arXiv:2004.02984 2020.\n\n<div class=\"content-ad\"></div>\n\n\n30. Jiang, Z.-H.; Yu, W.; Zhou, D.; Chen, Y.; Feng, J.; Yan, S., Convbert: Improving bert with span-based dynamic convolution. Advances in Neural Information Processing Systems 2020, 33, 12837–12848.\n\n31. He, P.; Liu, X.; Gao, J.; Chen, W., Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654 2020.\n\n32. Zaheer, M.; Guruganesh, G.; Dubey, K. A.; Ainslie, J.; Alberti, C.; Ontanon, S.; Pham, P.; Ravula, A.; Wang, Q.; Yang, L., Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems 2020, 33, 17283–17297.\n\n33. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; Zettlemoyer, L. In Deep Contextualized Word Representations, NAACL, 2018.\n\n\n<div class=\"content-ad\"></div>\n\n34. Howard, J.; Ruder, S. Universal Language Model Fine-tuning을 위한 텍스트 분류(ACL, 2018).\n\n35. Dai, A. M.; Le, Q. V., 준지도학습 순차 학습. 신경 정보 처리 시스템의 발전 2015, 28.\n\n36. Phang, J.; Févry, T.; Bowman, S. R., Sentence encoders on stilts: 중간 레이블 데이터 작업에 대한 보조 훈련. arXiv 사전 인쇄 arXiv:1811.01088 2018.\n\n37. Rebuffi, S.-A.; Bilen, H.; Vedaldi, A., 잔여 어댑터를 사용한 다중 시각 도메인 학습. 신경 정보 처리 시스템의 발전 2017, 30.\n\n<div class=\"content-ad\"></div>\n\n38. 아가자니안, A.; 제틀모이어, L.; 구프타, S., 내재적 차원이 언어 모델 파인튜닝의 효과를 설명합니다. arXiv 사전 인쇄 arXiv:2012.13255 2020.\n\n39. 주, C.; 청, Y.; 간, Z.; 선, S.; 골드스타인, T.; 리우, J., Freelb: 자연어 이해를 위한 향상된 적대적 훈련. arXiv 사전 인쇄 arXiv:1909.11764 2019.\n\n40. 리우, P.; 위안, W.; 부, J.; 장, Z.; 하야시, H.; 노이빅, G., 사전 학습, 프롬프트 및 예측: 자연어 처리에서 프롬프트 방법의 체계적 조사. arXiv 사전 인쇄 arXiv:2107.13586 2021.\n\n41. 양, Y.; 황, P.; 차오, J.; 리, J.; 린, Y.; 동, J. S.; 마, F.; 장, J., 적대적 예제 생성 및 강화를 위한 프롬프트 기반 접근 방법. arXiv 사전 인쇄 arXiv:2203.10714 2022.\n\n<div class=\"content-ad\"></div>\n\n42. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.; Le, Q.; Zhou, D., Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 2022.\n\n43. Liang, S.; Zhao, M.; Schütze, H., Modular and Parameter-Efficient Multimodal Fusion with Prompting. arXiv preprint arXiv:2203.08055 2022.\n\n44. Hanna, M.; Mareček, D. In Analyzing BERT’s Knowledge of Hypernymy via Prompting, Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, 2021; pp 275–282.\n\n45. Abaho, M.; Bollegala, D.; Williamson, P.; Dodd, S., Position-based Prompting for Health Outcome Generation. arXiv preprint arXiv:2204.03489 2022.\n\n<div class=\"content-ad\"></div>\n\n46. Wang, C.; Wang, J.; Qiu, M.; Huang, J.; Gao, M. In TransPrompt: Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021; pp 2792–2802.\n\n47. Lester, B.; Al-Rfou, R.; Constant, N., The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 2021.","ogImage":{"url":"/assets/img/2024-06-22-IntroofGenericNLPmethods_0.png"},"coverImage":"/assets/img/2024-06-22-IntroofGenericNLPmethods_0.png","tag":["Tech"],"readingTime":15},{"title":"GPT-3 사용법 소수 샷 학습자를 위한 포괄적 가이드","description":"","date":"2024-06-22 21:29","slug":"2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners","content":"\n\n## 메타러닌 학습 프레임워크 내에서 대규모에서 타이타닉 규모로 GPT를 효율적으로 확장하는 방법\n\n![그림](/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_0.png)\n\n# 소개\n\nGPT는 최근 많은 인기를 얻고 있는 언어 모델 가족입니다. 데이터 과학 커뮤니티의 관심은 GPT-3가 2020년에 출시되면서 급속하게 집중되었습니다. GPT-2가 등장한 후, 거의 누구도 1년 안에 175B의 매개변수를 포함한 GPT의 타이타닉 버전이 출현할 것으로 상상하지 못했습니다! 이는 이전 버전과 비교했을 때 2차적으로 두 배 많은 양입니다.\n\n<div class=\"content-ad\"></div>\n\nGPT-3의 엄청난 용량 덕분에 코드 완성, 글 작성, 콘텐츠 생성, 가상 어시스턴트 등 다양한 일상 시나리오에서 사용할 수 있었습니다. 이러한 작업의 품질이 항상 완벽하지는 않지만, GPT-3이 달성한 전반적인 진전은 정말 놀랍습니다!\n\n이 기사에서는 GPT-3의 주요 세부 사항과 GPT-2 창조자들로부터 영감을 받은 유용한 아이디어를 자세히 살펴볼 것입니다. 탐구하는 동안, 공식 GPT-3 논문을 참조하겠습니다. GPT-3의 대부분의 설정은 GPT-2에서 직접 파생된 데이터 수집, 구조 선택 및 사전 훈련 과정을 포함합니다. 그래서 대부분의 시간을 GPT-3의 새로운 측면에 집중할 것입니다.\n\n# 메타-러닝 프레임워크\n\nGPT-3 창조자들은 GPT-2에서 사용된 학습 방법에 대해 매우 관심을 가졌습니다: 일반적인 사전 훈련 + 미세 조정 프레임워크 대신, 저자들은 크고 다양한 데이터 세트를 수집하고 텍스트 입력에 작업 목표를 통합했습니다. 이 방법론은 여러 가지 이유로 편리했습니다:\n\n<div class=\"content-ad\"></div>\n\n- 미세 조정 단계를 제거함으로써 이제는 개별 하위 작업을 위해 여러 대규모 레이블된 데이터 세트가 더 이상 필요하지 않습니다.\n- 다른 작업에 대해 하나의 모델 버전을 여러 개 사용하는 대신 하나만 사용할 수 있습니다.\n- 모델은 사람이 하는 것과 더 유사한 방식으로 작동합니다. 대부분의 경우 사람들은 주어진 작업을 완전히 이해하려면 언어 예제가 전혀 필요하지 않거나 몇 개만 필요합니다. 추론 중에 모델은 해당 예제를 텍스트 형식으로 받을 수 있습니다. 그 결과로 이 측면은 인간과 상호 작용하는 AI 애플리케이션을 개발하는 데 더 나은 전망을 제공합니다.\n- 모델은 한 번만 단일 데이터 세트에서 훈련됩니다. 미세 조정 + 미세 조정 패러다임과 달리, 모델은 완전히 다른 데이터 분포를 가질 수 있던 두 가지 다른 데이터 세트에서 훈련되어야 했다는 점으로 잠재적인 일반화 문제를 야기할 수 있었습니다.\n\n공식적으로, 설명된 프레임워크를 메타 학습이라고 합니다. 논문은 공식적인 정의를 제공합니다:\n\n학습 패러다임을 더 자세히 설명하기 위해 내부 및 외부 루프 용어가 소개됩니다. 기본적으로 내부 루프는 훈련 중에 단일 순방향 패스에 해당하며 외부 루프는 모든 내부 루프를 나타냅니다.\n\n훈련 과정 동안 모델은 다른 텍스트 예제에서 유사한 작업을 받을 수 있습니다. 예를 들어, 모델은 서로 다른 배치에서 다음과 같은 예제를 볼 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- \"Good\"은 \"excellent\"의 동의어입니다.\n- \"Computer\"은 \"laptop\"의 동의어입니다.\n- \"House\"는 \"building\"의 동의어입니다.\n\n이 예시들은 모델이 어떻게 동의어를 이해하는지를 돕는데 도움이 되며, 특정 단어의 동의어를 찾을 때 유용할 수 있습니다. 특정 작업 내에서 비슷한 언어 지식을 습득하도록 도와주는 예시의 조합을 \"컨텍스트 학습\"이라고 합니다.\n\n<img src=\"/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_1.png\" />\n\n## n-shot 학습\n\n<div class=\"content-ad\"></div>\n\n추론 중에 수행되는 모델의 쿼리는 작업 예제를 추가로 포함할 수 있습니다. 작업 데모가 쿼리의 목적을 더 잘 이해하는 데 중요한 역할을 한다는 것을 알 수 있었습니다. 제공된 작업 예제의 수(샷)에 따라 아래 표에 요약된 세 가지 유형의 학습이 존재합니다:\n\n| 샷 수 | 학습 유형 |\n|-------|------------|\n| 0     | Zero-shot  |\n| 1     | One-shot   |\n| 2+    | Few-shot   |\n\n대부분의 경우(항상은 아니지만) 제공된 예제의 수가 모델이 올바른 답변을 제공하는 능력과 긍정적으로 상관 관계가 있음을 알 수 있습니다. 저자들은 다른 크기의 모델을 세 가지 n-shot 설정 중 하나에 사용하여 연구를 완료했습니다. 결과에서 용량이 증가함에 따라 모델이 문맥 학습에 더 능숙해진다는 것을 보여줍니다. 성능 차이가 증가하는 선 그래프로 이를 시연합니다. 적은 수, 한 개, 영 샷 설정 간의 성능 차이가 모델 크기와 함께 더 커짐을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n# 아키텍처\n\n이 논문은 GPT-3의 아키텍처 설정을 정확히 설명하고 있습니다.\n\n## 데이터셋\n\n초기에 저자들은 GPT-3를 훈련하기 위해 Common Crawl 데이터셋을 사용하고자 했습니다. 이 굉장히 큰 데이터셋은 다양한 주제의 데이터를 담고 있습니다. 그러나 초기의 원본 데이터셋은 데이터 품질에 문제가 있어 처음에는 필터링되고 중복이 제거되었습니다. 최종 데이터셋을 더욱 다양하게 만들기 위해 아래 다이어그램에 표시된 네 가지 다른 작은 데이터셋과 연결되었습니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_4.png\" />\n\n# 훈련 세부 정보\n\n- 옵티마이저: Adam (β₁ = 0.9, β₂ = 0.999, ε = 1e-6).\n- 폭주 그래디언트 문제를 방지하기 위해 1.0에서 그래디언트 클리핑이 사용됩니다.\n- 학습 속도 조정을 위해 코사인 감쇠와 선형 웜업의 조합이 사용됩니다.\n- 배치 크기는 학습 중 32K에서 3.2M 토큰으로 점진적으로 증가합니다.\n- 0.1의 가중치 감쇠가 정규화자로 사용됩니다.\n- 더 나은 계산 효율성을 위해 모든 시퀀스의 길이가 2048로 설정됩니다. 단일 시퀀스 내의 다른 문서는 구분자 토큰으로 분리됩니다.\n\n## 빔 탐색\n\n<div class=\"content-ad\"></div>\n\nGPT-3는 자기회귀 모델입니다. 이것은 과거의 예측된 단어에 대한 정보를 사용하여 미래의 다음 단어를 예측하는 데 사용합니다.\n\n탐욕 알고리즘은 자기회귀 모델에서 텍스트 시퀀스를 구성하기 위한 가장 단순한 방법 중 하나입니다. 각 반복에서 모델이 가장 가능성이 높은 단어를 선택하도록 강제하고, 이를 다음 단어의 입력으로 사용합니다. 그러나 현재 반복에서 가장 가능성이 높은 단어를 선택하는 것이 로그 우도 최적화에 대한 최선의 방법은 아닙니다!\n\n이미지 링크: ![이미지](/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_5.png)\n\n낮은 확률로 현재 단어를 선택하면 나머지 예측된 단어들의 확률이 높아질 수 있는 상황이 발생할 수 있습니다. 한편, 지역 단어를 가장 높은 확률로 선택하는 것은 그 다음 단어들도 높은 확률에 해당한다는 것을 보장하지는 않습니다. 탐욕 전략이 최적으로 작동하지 않는 경우를 보여주는 예시는 아래 다이어그램에 나와 있습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_6.png)\n\n가능한 해결책은 모든 옵션 중에서 가장 가능성이 높은 시퀀스를 찾는 것입니다. 그러나 이 접근 방식은 가능한 시퀀스의 조합이 무수히 많기 때문에 매우 효율적이지 않습니다.\n\n빔 서치(Beam Search)는 탐욕 알고리즘과 모든 가능한 조합을 탐색하는 것 사이의 좋은 절충안입니다. 각 반복에서 빔 서치는 가장 가능성이 높은 토큰을 여러 개 선택하고 현재 가장 가능성이 높은 시퀀스 집합을 유지합니다. 새로운 더 가능성 있는 시퀀스가 형성될 때마다, 해당 시퀀스 중 가장 가능성이 낮은 것을 대체합니다. 알고리즘의 끝에는 집합에서 가장 가능성이 높은 시퀀스가 반환됩니다.\n\n![Image](/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_7.png)\n\n\n<div class=\"content-ad\"></div>\n\n빔 서치는 최상의 검색 전략을 보장하지는 않지만 실제로는 근사치가 매우 잘 동작합니다. 그 이유로 GPT-3에서 사용됩니다.\n\n# 단점\n\nGPT-3는 인간과 유사한 긴 텍스트 조각을 생성하는 놀라운 능력을 가졌지만 몇 가지 단점이 있습니다:\n\n- 텍스트 생성 중 GPT-3이 내린 결정은 일반적으로 해석하기 어려워 분석하기 어렵습니다.\n- GPT-3은 모델로 항상 방지할 수 없는 해로운 방식으로 사용될 수 있습니다.\n- GPT-3는 학습 데이터 세트에 편향이 있어 때때로 공정성 측면에서 취약할 수 있습니다. 특히, 성별 평등, 종교 또는 인종과 같은 민감한 도메인에 관련된 경우입니다.\n- 이전의 GPT-2 보다 GPT-3는 훈련에 수십 배나 더 많은 에너지(수천 페타플랍 / 일)가 필요한데, 이는 친환경적이지 않습니다. 동시에, GPT-3 개발자들은 이 모델이 추론 중에 매우 효율적이기 때문에 평균 소비량이 여전히 낮다는 점으로 이 측면을 정당화합니다.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\nGPT-3는 상상할 수 없는 175B개의 훈련 가능한 매개변수를 보유하여 이전 모델들을 강력히 이기는 몇 가지 최고의 기준들에서 엄청난 인기를 얻었습니다! 그 당시에 GPT-3 결과는 때때로 사람이 생성한 텍스트인지 GPT-3가 생성한 것인지 구별하기가 어려울 정도로 좋았습니다.\n\nGPT-3의 몇 가지 단점과 제한사항에도 불구하고, 이는 연구자들에게 미래에 대한 새로운 탐구와 잠재적 개선의 가능성을 여는 문이 되었습니다.\n\n# 자료들\n\n<div class=\"content-ad\"></div>\n\n- 언어 모델은 소수 샷 학습자들입니다.\n\n모든 이미지는 특별히 언급되지 않는 한 작성자가 찍은 것입니다.","ogImage":{"url":"/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_0.png"},"coverImage":"/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_0.png","tag":["Tech"],"readingTime":6},{"title":"자연어 처리NLP에서 텍스트 분류 머신러닝으로 텍스트 카테고리화 8부","description":"","date":"2024-06-22 21:27","slug":"2024-06-22-TextClassificationinNLPCategorizingTextwithMachineLearningPart8","content":"\n\n![image](/assets/img/2024-06-22-TextClassificationinNLPCategorizingTextwithMachineLearningPart8_0.png)\n\n## 목차\n1. 텍스트 분류의 기본 개념 이해\n2. 텍스트 분류를 위한 데이터 준비\n   2.1. 데이터 수집 및 주석\n   2.2. 데이터 정제 및 전처리\n3. 특성 추출 기술\n4. 텍스트 분류를 위한 머신러닝 모델\n   4.1. 나이브 베이즈 분류기\n   4.2. 서포트 벡터 머신\n   4.3. 신경망 및 딥 러닝\n5. 모델 성능 평가\n6. 텍스트 분류의 고급 전략\n   6.1. 불균형한 클래스 다루기\n   6.2. 전이 학습 활용\n\n더 자세한 정보는 GPTutorPro에서 무료로 확인하세요.\n\n42페이지 전자책인 \"데이터 과학 | 포괄적 핸드북\"을 무료로 받아보세요. 무료 구독하세요!\n\n<div class=\"content-ad\"></div>\n\n## 1. 텍스트 분류 기초 이해\n\n텍스트 분류는 미리 정의된 그룹으로 텍스트를 분류하는 기계 학습(ML)의 기본적인 작업입니다. 이 섹션에서는 텍스트 분류의 기초 개념을 배우게 됩니다.\n\n텍스트 분류의 핵심은 텍스트 문서에 레이블을 할당하는 것을 목표로 합니다. 이는 이메일을 스팸이냐 아니냐로 분류하거나 기사를 주제별로 분류하는 것을 포함할 수 있습니다. 이 과정은 다음과 같은 단계를 거칩니다:\n\n- 데이터 수집 및 레이블링\n- 텍스트 전처리\n- 특성 추출\n- 모델 훈련 및 평가\n\n<div class=\"content-ad\"></div>\n\n강력한 텍스트 분류 시스템을 만드는 데 각 단계가 중요합니다. ML 워크플로에 어떻게 기여하는지 이해하기 위해 이러한 단계를 자세히 살펴봅시다.\n\n먼저, 데이터셋이 필요합니다. 이 데이터셋은 분류하고자 하는 텍스트를 대표하는 것이어야 합니다. 수집된 텍스트는 적절한 범주로 레이블이 지정되어 주석이 달립니다.\n\n다음으로, 전처리는 데이터를 정리하고 준비합니다. 이 과정에는 특수 문자와 어간, 의미 중요단어 추출과 같은 노이즈 제거가 포함됩니다. 깨끗한 데이터는 더 나은 ML 모델로 이어집니다.\n\n특성 추출은 텍스트를 ML 알고리즘이 이해할 수 있는 형식으로 변환합니다. Bag of Words, TF-IDF 또는 단어 임베딩과 같은 기법이 일반적으로 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n마침내, 이 처리된 데이터에서 모델을 훈련합니다. Naive Bayes, Support Vector Machines 또는 Neural Networks와 같은 모델이 인기 있는 선택지입니다. 훈련 후, 정확도, 정밀도 및 재현율과 같은 메트릭을 사용하여 모델의 성능을 평가합니다.\n\n이 기본 사항을 이해하면 후속 섹션에서 다룰 더 고급 텍스트 분류 전략을 위한 기초가 마련됩니다.\n\n## 2. 텍스트 분류를 위한 데이터 준비\n\n텍스트 분류를 위해 데이터를 준비하는 것은 기계 학습 과정에서 중요한 단계입니다. 이는 몇 가지 주요 단계를 포함합니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 수집: 분류하려는 텍스트를 잘 대표하는 견고한 데이터 세트를 수집하세요. 편향을 피하려면 다양한 출처를 사용해보세요.\n\n데이터 주석: 데이터를 정확하게 라벨링하세요. 이를 위해서는 수동 태깅이나 인간 감독 하에 반자동화된 프로세스 등이 필요할 수 있습니다.\n\n데이터 정제: 데이터 세트에서 노이즈를 제거하세요. 이는 관련 없는 문자, 서식 문제, 비텍스트 정보 등을 포함합니다.\n\n데이터 전처리: 텍스트를 표준화하세요. 소문자로 변환하고, 토큰화하고, 불용어를 제거하며, 어간 추출 또는 표제어 추출을 수행하세요.\n\n<div class=\"content-ad\"></div>\n\n간단한 Python 코드 조각을 준비했습니다. 기본 텍스트 전처리에 대한 것이에요:\n\n```js\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# 샘플 텍스트\ntext = \"Text classification with Machine Learning is fun!\"\ntext = text.lower()\ntext = re.sub(r'[^a-z0-1\\s]', '', text)\n\n# 토큰화\ntokens = text.split()\n\n# 불용어 제거\nstop_words = set(stopwords.words('english'))\ntokens = [word for word in tokens if not word in stop_words]\n\n# 어근 추출\nlemmatizer = WordNetLemmatizer()\ntokens = [lemmatizer.lemmatize(word) for word in tokens]\n```\n\n기억하세요: 기계 학습 모델의 성능에 직접적인 영향을 미치는 것은 입력 데이터의 품질입니다.\n\n## 2.1. 데이터 수집 및 주석화\n\n<div class=\"content-ad\"></div>\n\n텍스트 분류에서 데이터 수집 및 주석 작업은 기본 단계입니다. 효과적으로 머신 러닝 모델을 훈련시키기 위해서는 견고한 데이터셋이 필요합니다. 아래는 시작하는 방법입니다:\n\n- 데이터 수집: 분류 목표와 관련된 다양한 텍스트 데이터를 수집하는 것부터 시작하세요. 이는 고객 리뷰, 기사, 혹은 소셜 미디어 게시물일 수 있습니다.\n\n- 주석 작업: 수집한 데이터는 라벨이 필요합니다. 이는 각 텍스트 샘플에 카테고리나 태그를 할당하는 것을 의미합니다. 모델 훈련에 중요합니다.\n\nAmazon Mechanical Turk와 같은 도구를 활용해 대규모 주석 작업을 수행할 수도 있습니다. 또는 전문 소프트웨어를 사용해 이 프로세스 일부를 자동화할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n데이터의 품질이 모델의 성능에 직접적인 영향을 미친다는 것을 기억해주세요. 따라서 데이터셋이 다음을 충족하는지 확인해보세요:\n\n- 문제 공간을 충분히 대표할만큼 큰지\n- 모든 범주를 포괄하기 위해 다양한지\n- 명확한 학습 신호를 제공하기 위해 정확하게 주석이 달려 있는지\n\n아래는 데이터 주석을 위한 간단한 Python 코드 조각입니다:\n\n```python\n# 데이터 주석을 위한 샘플 Python 코드\ndata = {\"text\": \"제품이 좋고 배송도 빠릅니다!\", \"label\": \"긍정적\"}\nprint(f'텍스트: {data[\"text\"]}\\n레이블: {data[\"label\"]}')\n```\n\n<div class=\"content-ad\"></div>\n\n데이터가 준비되었으므로 다음 단계, 즉 전처리로 넘어갈 준비가 되었습니다.\n\n## 2.2. 데이터 클리닝과 전처리\n\n머신 러닝을 사용한 텍스트 분류에서 데이터 클리닝과 전처리는 중요한 단계입니다. 이 단계는 원시 데이터를 머신 러닝 알고리즘이 이해할 수 있는 형식으로 변환합니다.\n\n먼저 데이터셋에서 노이즈를 제거해야 합니다. 이는 HTML 태그를 제거하고 오타를 교정하고 특수 문자를 제거하는 것을 포함합니다. 다음은 기본 텍스트 클리닝을 위한 Python 코드 조각입니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nimport re\ndef clean_text(text):\n    # HTML 태그를 제거합니다\n    text = re.sub(r'<.*?>', '', text)\n    # 오타를 수정하고 특수 문자를 제거합니다\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    return text\n```\n\n다음으로, 토큰화는 텍스트를 개별 단어나 구를 나눕니다. 어간 추출이나 표제어 추출과 같은 정규화 작업은 단어를 기본 형태로 축소시킵니다.\n\n마지막으로, 불용어 제거는 분석에 적은 가치를 더하는 일반적인 단어를 제거합니다. NLTK와 같은 라이브러리들은 포괄적인 불용어 목록을 제공합니다:\n\n```js\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfiltered_sentence = [w for w in word_tokens if not w in stop_words]\n```\n\n<div class=\"content-ad\"></div>\n\n이 단계를 따라가면 텍스트 분류에서 효율적인 기계 학습 모델 훈련을 위해 데이터를 준비할 수 있습니다.\n\n### 3. 특성 추출 기술\n\n특성 추출은 텍스트 분류 및 기계 학습에서 중요합니다. 이는 기계 학습 알고리즘이 이해할 수 있는 숫자적인 특성으로 원시 데이터를 변환합니다. 여기 중요한 기술들이 있습니다:\n\nBag of Words (BoW): 이 방법은 텍스트 내 모든 단어에서 어휘를 생성하고 그 발생 빈도를 계산합니다. 예를 들면:\n\n<div class=\"content-ad\"></div>\n\n```js\r\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\r\n```\n\nTF-IDF: 용어인 Term Frequency-Inverse Document Frequency의 줄임말로, 문서 모음에서 단어가 문서와 얼마나 관련이 있는지를 평가합니다. 계산 방법은 다음과 같습니다:\n\n```js\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(corpus)\r\n```\n\nWord Embeddings: 이 기술은 밀집된 벡터 형식으로 단어를 나타내며 의미를 캡처합니다. Gensim과 같은 라이브러리 또는 TensorFlow와 같은 프레임워크를 사용하여 단어 임베딩을 구현할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n각 방법은 각각의 강점을 가지고 있으며, 구체적인 텍스트 분류 작업 요구에 따라 선택됩니다.\n\n## 4. 텍스트 분류를 위한 머신러닝 모델\n\n텍스트 분류에는 다양한 머신러닝 모델을 사용할 수 있습니다. 각 모델은 강점을 가지고 있으며, 다른 종류의 텍스트 데이터에 적합합니다.\n\n나이브 베이즈 분류기는 베이즈 이론을 기반으로 하는 확률 모델입니다. 대규모 데이터셋에 특히 적합하며 구현하기 쉽습니다. 간단한 Python 예시는 아래와 같습니다:\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# 샘플 데이터\n문서 = ['텍스트 분류는 쉽습니다', '머신 러닝은 즐겁습니다']\n라벨 = [0, 1]\n\n# 텍스트를 토큰 카운트 행렬로 변환\n벡터라이저 = CountVectorizer()\nX = 벡터라이저.fit_transform(문서)\n\n# 나이브 베이즈 분류기 학습\nclf = MultinomialNB()\nclf.fit(X, 라벨)\n```\n\n서포트 벡터 머신(SVM)은 텍스트 분류 작업에 강력합니다. 고차원 데이터와 복잡한 관계를 모델링하는 데 효과적입니다.\n\n신경망과 딥러닝 접근 방식인 컨볼루션 신경망(CNN)과 같은 방법들이 인기를 얻고 있습니다. 이들은 텍스트의 문맥과 의미를 포착할 수 있습니다.\n\n적절한 모델을 선택하는 것은 데이터셋과 작업 중인 텍스트의 특징에 따라 다릅니다. 최적의 모델을 찾기 위해 실험이 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n## 4.1. 나이브 베이즈 분류기\n\n나이브 베이즈 분류기는 기계 학습을 사용한 텍스트 분류를 위한 기본 알고리즘입니다. 이는 확률 예측을 계산하는 베이즈 정리에 기반을 두고 있습니다. 이 분류기는 특정 기능이 클래스에 존재하는 것이 다른 어떤 기능의 존재와 상관이 없다고 가정합니다.\n\n다음은 Python에서 나이브 베이즈 분류기를 구현하는 방법입니다:\n\n```js\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\n# 샘플 데이터셋\ndocuments = ['문서 1의 텍스트', '문서 2의 텍스트', '문서 3의 텍스트']\nlabels = [0, 1, 0]\n\n# 텍스트를 토큰 수의 매트릭스로 변환\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(documents)\n\n# 데이터셋을 훈련 및 테스트 세트로 분할\nX_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3)\n\n# 분류기를 초기화하고 데이터에 맞춤\nclf = MultinomialNB()\nclf.fit(X_train, y_train)\n```\n\n<div class=\"content-ad\"></div>\n\n훈련 후 모델의 성능을 평가하고 예측할 수 있습니다. Naive Bayes 분류기는 텍스트 데이터와 잘 작동하여 이메일 필터링, 감성 분석 및 문서 분류에 인기가 있습니다.\n\n## 4.2. 서포트 벡터 머신\n\n서포트 벡터 머신(SVM)은 텍스트 분류 작업에 강력합니다. SVM은 텍스트와 같은 고차원 데이터와 잘 작동합니다.\n\n먼저 TF-IDF와 같은 기술을 사용하여 텍스트 데이터를 숫자 벡터로 변환합니다. 그런 다음, SVM은 서로 다른 클래스를 가장 잘 분리하는 초평면을 찾습니다.\n\n<div class=\"content-ad\"></div>\n\n여기 간단한 파이썬 예제가 있어요. scikit-learn 라이브러리를 사용합니다:\n\n```python\nfrom sklearn import svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\n# 샘플 데이터\ntexts = [\"Text classification is fascinating\", \"Machine learning drives innovation\"]\nlabels = [0, 1]\n\n# 텍스트 벡터화\nvectorizer = TfidfVectorizer()\nvectors = vectorizer.fit_transform(texts)\n\n# 데이터셋 분할\nX_train, X_test, y_train, y_test = train_test_split(vectors, labels, test_size=0.2)\n\n# 모델 훈련\nclassifier = svm.SVC(kernel='linear')\nclassifier.fit(X_train, y_train)\n\n# 예측\npredictions = classifier.predict(X_test)\n```\n\n이 코드 예시는 텍스트 벡터화, 데이터셋 분할 및 SVM 분류기 훈련 과정을 보여줍니다. 기억하세요, 최적 성능을 위해 매개변수 조정이 중요합니다.\n\nSVM 및 해당 구현에 대한 자세한 내용은 scikit-learn 문서를 참조하세요.\n\n<div class=\"content-ad\"></div>\n\n## 4.3. 신경망과 딥러닝\n\n신경망과 딥러닝은 기계 학습의 텍스트 분류에서 주도적인 위치를 차지하고 있습니다. 이러한 모델은 인간 뇌의 연결된 뉴런 구조를 모방하여 데이터를 처리합니다. 다음은 적용 방법입니다:\n\n단계 1: 데이터 준비\n데이터셋을 준비하세요. 지도 학습을 위해 레이블이 지정되어 있는지 확인하세요. TensorFlow나 PyTorch와 같은 도구를 사용하여 처리하세요.\n\n단계 2: 모델 선택\n신경망 아키텍처를 선택하세요. 합성곱 신경망 (CNN)은 지역 패턴을 이해해야 하는 텍스트에 좋습니다. 순환 신경망 (RNN), 특히 LSTM (Long Short-Term Memory) 네트워크는 순차 데이터에 더 적합합니다.\n\n<div class=\"content-ad\"></div>\n\n### 단계 3: 특성 추출\n텍스트를 숫자 형태로 변환합니다. 단어 임베딩과 같은 기술을 사용하여 단어와 그 의미를 밀집 표현으로 제공합니다.\n\n### 단계 4: 모델 훈련\n준비된 특성을 사용하여 모델을 훈련합니다. 학습률 및 에폭 수와 같은 하이퍼파라미터를 조정하여 성능을 향상시킵니다.\n\n### 단계 5: 평가\n정확도, 정밀도, 재현율, F1 점수와 같은 지표를 사용하여 모델을 평가합니다. 훈련 중에 모델이 보지 않았던 별도의 테스트 세트를 사용합니다.\n\n다음은 Keras를 사용한 간단한 파이썬 코드 스니펫이 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```python\nkeras.models에서 Sequential, Dense, LSTM, Embedding을 가져와주세요\nmodel = Sequential()\nmodel.add(Embedding(input_dim=1000, output_dim=64))\nmodel.add(LSTM(128))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n```\n\n데이터셋에 맞게 input_dim과 output_dim을 조정해주시기 바랍니다. 이 코드는 Embedding 레이어, LSTM 레이어 및 밀도 출력 레이어를 정의합니다.\n\n딥러닝 모델은 상당한 계산 리소스를 필요로 합니다. 더 빠른 학습 시간을 위해 GPU 가속을 사용하는 것을 고려해보세요.\n\n이러한 단계를 따르고 신경망을 사용함으로써 효과적으로 텍스트 데이터를 이해하고 분류하는 강력한 텍스트 분류 시스템을 구축할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 5. 모델 성능 평가\n\n텍스트 분류에서 기계 학습 모델의 성능을 평가하는 것은 매우 중요합니다. 이는 모델의 효과를 이해하고 더 나은 성능을 위한 안내를 제공합니다. 다음은 주요 단계입니다:\n\n혼란 행렬: 이 테이블은 실제 양성, 거짓 양성, 실제 음성, 거짓 음성을 나열합니다. 모델의 정확도를 이해하는 데 중요합니다.\n\n```js\n# 혼란 행렬을 생성하는 파이썬 코드\nfrom sklearn.metrics import confusion_matrix\ny_true = [2, 0, 2, 2, 0, 1]\ny_pred = [0, 0, 2, 2, 0, 2]\nconfusion_matrix(y_true, y_pred)\n```\n\n<div class=\"content-ad\"></div>\n\n정밀도와 재현율: 정밀도는 양성 예측의 정확성을 측정합니다. 재현율 또는 민감도는 실제 양성 비율을 측정합니다.\n\n```js\n# 정밀도와 재현율을 계산하는 Python 코드\nfrom sklearn.metrics import precision_score, recall_score\nprecision = precision_score(y_true, y_pred, average='macro')\nrecall = recall_score(y_true, y_pred, average='macro')\n```\n\nF1 점수: F1 점수는 정밀도와 재현율의 조화 평균입니다. 두 가지 사이의 균형이 필요할 때 유용합니다.\n\n항상 새로운, 보지 못한 데이터로 모델을 테스트해야 합니다. 이렇게 하면 모델이 일반화되고 훈련 데이터에 오버피팅되지 않도록 보장할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\"실세계 데이터에서 좋은 성능을 발휘하는 모델을 구축하는 것이 목표입니다. 이러한 지표를 사용하여 모델의 성능을 측정하고 만족스러운 결과를 얻을 때까지 반복하세요.\n\n## 6. 텍스트 분류의 고급 전략\n\n텍스트 분류에서 고급 전략은 모델 정확도를 향상시키고 어려움을 극복하는 데 도움이 됩니다. 여기서는 두 가지를 탐색합니다: 불균형 클래스 처리와 전이 학습 활용.\n\n불균형 클래스 처리: 불균형 데이터셋은 기계 학습 모델에 편향을 일으킬 수 있어 일반화 성능이 저하될 수 있습니다. 이를 해결하기 위해:\"\n\n<div class=\"content-ad\"></div>\n\n- 클래스 분포를 균형있게 맞추기 위해 리샘플링 기술을 사용해보세요.\n- 클래스를 다르게 가중시키기 위해 비용 민감학습을 적용해보세요.\n- 드문 사건을 위해 이상 탐지 방법을 고려해보세요.\n\n전이 학습 활용하기: 전이 학습은 사전에 훈련된 모델을 활용하여 특히 데이터가 부족할 때 성능을 향상시킬 수 있습니다. 이를 실행하기 위해:\n\n- 작업과 관련된 사전 훈련된 모델을 선택합니다.\n- 모델을 특정 데이터셋에 맞게 미세 조정합니다.\n- Catastrophic Forgetting을 피하기 위해 학습률을 조정합니다.\n\n이러한 전략들은 신중한 고려가 필요하지만, 기계 학습에서 텍스트 분류 결과를 크게 향상시킬 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 6.1. 불균형 클래스 다루기\n\n텍스트 분류에서 불균형 클래스를 다루는 것은 머신 러닝 모델의 성능을 심각하게 왜곡할 수 있습니다. 이 도전 과제를 처리하는 몇 가지 전략은 다음과 같습니다:\n\n재샘플링 기술: 소수 클래스를 오버샘플링하거나 다수 클래스를 언더샘플링하여 균형을 달성할 수 있습니다.\n\n```js\n# 소수 클래스 오버샘플링\nfrom imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=42)\nX_resampled, y_resampled = ros.fit_resample(X, y)\n```\n\n<div class=\"content-ad\"></div>\n\n알고리즘 조정: Decision Trees나 앙상블 방법과 같은 알고리즘을 사용하여 클래스 불균형에 민감하지 않도록 조정하세요.\n\n비용 감안학습: 학습 알고리즘을 수정하여 소수 클래스의 오분류를 과반수보다 더 벌로 처리하세요.\n\n이러한 방법을 적용하면 모델의 일반화 능력을 향상시키고 모든 클래스에 대해 정확한 예측을 할 수 있습니다.\n\n## 6.2. 전이 학습 활용하기\n\n<div class=\"content-ad\"></div>\n\n전이 학습은 텍스트 분류 모델을 혁신적으로 개선할 수 있는 머신 러닝 기술입니다. 미리 훈련된 모델을 사용하고 해당 작업에 적응시키는 것이 핵심입니다. 여기에 이 기술을 활용하는 방법이 있습니다:\n\n먼저, 도메인에 관련된 미리 훈련된 모델을 선택하세요. BERT나 GPT와 같은 모델은 방대한 텍스트 코퍼스로 훈련되어 언어의 미묘한 점을 이해할 수 있습니다.\n\n```js\nfrom transformers import BertModel, BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n```\n\n다음으로, 데이터셋을 준비하세요. 텍스트 데이터가 깨끗하고 모델에 맞게 올바르게 포맷되어 있는지 확인하세요.\n\n<div class=\"content-ad\"></div>\n\n그럼, 데이터셋에 맞게 모델을 세밀하게 튜닝해보세요. 이 단계에서는 모델의 가중치를 조정하여 분류 작업에 더 잘 맞춥니다.\n\n```js\nfrom transformers import AdamW\noptimizer = AdamW(model.parameters(), lr=1e-5)\n# 여기에 학습 루프를 구현하세요\n```\n\n마지막으로, 모델의 성능을 평가하세요. 정확도, 정밀도, 리콜 등과 같은 지표를 사용하여 효과를 측정하세요.\n\n전이 학습을 활용함으로써, 효율적이고 정확한 강력한 텍스트 분류 시스템을 구축할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다음은 완전한 튜토리얼 목록입니다:\n\n지원하는 무료 튜토리얼과 정신 건강 스타트업.\n\n파이썬, 머신 러닝, 딥 러닝, 그리고 LLMs 마스터하기: E-book 50% 할인 (쿠폰: RP5JT1RL08)","ogImage":{"url":"/assets/img/2024-06-22-TextClassificationinNLPCategorizingTextwithMachineLearningPart8_0.png"},"coverImage":"/assets/img/2024-06-22-TextClassificationinNLPCategorizingTextwithMachineLearningPart8_0.png","tag":["Tech"],"readingTime":12},{"title":"지식 그래프 향상된 접근 방식 소개","description":"","date":"2024-06-22 21:24","slug":"2024-06-22-IntroofKnowledgeGraphenhancedapproaches","content":"\n\n# 지식 그래프 백그라운드 리뷰\n\n지식 그래프는 자연어 처리 애플리케이션, 추천 시스템 및 기타 지식 기반 작업에서 널리 사용됩니다. 이는 보다 구조화된 정보와 일반적인 감각을 제공할 수 있기 때문입니다. 이미지 캡션이 포함된 지식 베이스, 질문 응답이 포함된 지식 베이스, 추천 시스템이 포함된 지식 베이스가 모두 아래에서 논의되었으며, 주요 논문 세 편 [1–3]에서 중요 아이디어와 방법론이 요약되었습니다. 기본 아이디어는 해결해야 할 문제를 임베딩 벡터로 변환하고 해당하는 지식 그래프를 그래프 임베딩 벡터로 인코딩한 다음 두 종류의 임베딩 벡터를 융합하여 더 나은 결과를 얻는 것입니다. 따라서 효과적인 그래프 임베딩과 모델 설계의 다른 부분이 중요합니다. 심층 신경망은 성능을 향상시킬 수 있지만, 지식 그래프에 의해 유발되는 해석 능력과 추론 추론은 여전히 향상되어야 합니다.\n\n전형적인 지식 그래프는 수십억 건의 개체-관계-개체 쌍(s, r, o)으로 구성되며, 여기서 s, r 및 o는 주체, 관계 및 객체를 각각 나타냅니다. 지식 그래프에는 엔티티(주체 또는 객체), 관계, 엔티티의 속성이 포함됩니다. 일반적으로 목적을 가진 지식 그래프 또는 도메인별 지식 그래프가 있습니다.\n\n지식 그래프를 구축하려면 지식 모델링, 지식 획득, 지식 퓨전, 지식 저장 및 지식 응용과 같은 다양한 작업이 필요합니다. 지식 모델링은 다수준 지식 시스템을 구축하고 데이터베이스를 공고히 정의, 조직 및 관리하기 위해 추상적인 지식, 엔티티, 관계, 속성을 정의하는 것입니다. 지식 획득은 다양한 소스와 구조에서 데이터를 그래프 데이터로 변환하고 구조화된 데이터, 반구조화된 데이터, 비구조화된 데이터, 지식 인덱싱, 지식 추론 등을 처리하여 데이터의 유효성과 무결성을 보장하는 것입니다. 지식 퓨전은 다중 소스에서 반복된 지식 정보를 퓨전하는 것으로, 퓨전 컴퓨팅, 수동 작업 퓨전 등이 포함됩니다. 지식 저장은 비즈니스 시나리오에 따라 합리적인 지식 저장 솔루션을 제공하는 것입니다. 지식 저장 솔루션은 유연하고 다양하며 확장 가능해야 합니다. 지식 응용은 그래프 검색, 지식 계산, 그래프 시각화와 같은 분석 및 응용 기능을 제공하는 것뿐만 아니라 그래프 기본 애플리케이션, 그래프 구조 분석, 그래프 의미론적 애플리케이션, 자연어 처리, 그래프 데이터 획득, 그래프 통계, 데이터 집합 획득, 데이터 집합 통계 등을 포함한 다양한 종류의 지식 계산 SDK를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n지식 그래프 구축이나 컴퓨팅에 초점을 맞추는 대신, 이 보고서는 다양한 자연어 처리 작업에 대한 지식 그래프 적용에 집중할 것입니다. 적절한 지식 그래프가 있다고 가정하면, 지식 그래프는 의미 수준과 지식 수준의 추가 정보를 제공할 수 있기 때문에 다양한 NLP 작업을 보다 효과적으로 해결할 수 있습니다. 지식 그래프는 물건 추천, 기계 읽기, 기계 이해, 자동 추출, 텍스트 분류, 단어 임베딩, 질문 응답, 대화 관리, 순차 생성 등과 같은 이러한 작업에서 능력을 보여줍니다. 제 개인적인 흥미로 인해, 유명한 방법, 클래식 모델 및 관련 기본 아이디어에 대해 깊게 파고들고 싶었습니다. 여기서 지식 기반 이미지 캡션, 지식 기반 질문 응답, 지식 기반 추천 시스템과 관련된 세 가지 논문[1–3]을 선택했습니다. 기본 아이디어는 질문을 임베딩 공간으로 매핑하고, 지식 그래프 표현을 위한 적절한 임베딩 공간을 찾아 두 임베딩 공간에서 정보를 통합하여 의미 수준과 상식 수준에서 더 나은 성능을 달성하는 것입니다.\n\n# 지식 기반 이미지 캡션 소개\n\n첫 번째 논의는 지식 기반 이미지 캡션에 관한 것입니다. 캡션은 이미지 옆에 나타나는 하나 또는 여러 문장으로, 이미지의 전반적인 내용을 식별하거나 설명할 수 있습니다. 이미지 캡션은 따라서 이미지에 대한 텍스트 설명을 생성하는 과정입니다. 이 작업은 고수준 의미 공간에서 두 가지 널리 연구된 모달리티를 연결하는 다리인 시각-언어 맞춤의 가장 기본적인 방안으로 간주될 수 있습니다. 이미지 캡션을 잘 수행할 수 있다면, 이미지-텍스트 검색 및 텍스트-이미지 검색과 같은 다양한 관련 하류 작업에 이러한 방법을 쉽게 확장할 수 있습니다. 구체적으로, 이미지 캡션 작업은 이미지와 캡션을 이해하기 위해 컴퓨터 비전과 자연어 처리의 발전을 요구합니다. 이 이미지에 대한 의미 있는 설명을 생성하기 위해서는 이미지와 의미를 다양한 기술을 사용하여 세부적으로 이해하는 것을 필요로 합니다. 컴퓨터 비전 부분에서 사용되는 기술에는 이미지 분할, 물체 감지, 특성 추출, 이미지 생성, 적대적 학습 등이 포함되며, 자연어 처리 부분의 관련 작업에는 시퀀스 이해, 엔티티 추출, 의미적 특징 추출, 시퀀스 생성 등이 포함됩니다. NLP의 인코더 디코더 아키텍처에서 영감을 받아, 이미지 캡션 문제에서도 동일한 아키텍처가 채택되었으며, 주의 메커니즘, 강화 학습과 같은 다른 개선 기술도 함께 사용되어 파라미터를 조정합니다.\n\n또한, 동적 지식 베이스를 활용하여 후속 추론, 텍스트 질문 응답[4–9], 시각적 질문 응답[10–17], 이미지 생성[18–20], 시각적 기초[21–25], 퓨-샷 분류[26–29]에 사용할 수 있습니다. [2]에서는 동적 지식 사전과 그래프 합성망을 채택하여, 인간의 자연어에 대한 환율성 표현을 가져와서 결과적인 캡션을 유창하고 다양하게 만드는 데 도움이 되도록 하였습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지1](/assets/img/2024-06-22-IntroofKnowledgeGraphenhancedapproaches_0.png)\n\n![이미지2](/assets/img/2024-06-22-IntroofKnowledgeGraphenhancedapproaches_1.png)\n\n기본 아이디어는 인코더(CNN)에 의해 이미지 특성을 추출하고, 그래프 대표자(MGCN)에 의해 이미지 특성을 매핑하고, 언어 말뭉치로 미리 훈련된 재인코더의 사전을 사용하여 그래프 대표자를 보완하고, 마지막으로 디코더를 사용하여 캡션 문장을 생성하는 것입니다. Scene 그래프는 개체, 속성, 관계를 나타내는 1000차원의 멀티-핫 벡터로 표시됩니다. GCN에는 전결합층과 ReLU 활성화 함수가 포함되어 있어 관계 내장, 속성 내장, 개체 내장을 계산하기 위한 각각의 방정식이 아래에 표시되어 있습니다. 이후, 텍스트 기반 지식사전은 데이터셋의 문장만 사용하여 사전 훈련할 수 있습니다. 사전은 이미지 캡션 작업에 파라미터를 고정시켜 전달할 수 있습니다. 마찬가지로, 이미지 지식 그래프는 개체 탐지기로 사용되는 Faster-RCNN, 관계 추출기로 사용되는 MOTIFS 관계 탐지기, 그리고 속성 추출기로 사용되는 작은 fc-ReLU-fc-Softmax 네트워크 헤드를 사용하는 멀티 모달 그래프 합성망에 의해 처리됩니다. MGCN의 출력은 보다 풍부한 의미 정보 및 자연적 추론을 생성하기 위해 사전에 의해 다시 인코딩됩니다. 마지막으로 강화 학습을 통한 고성능 디코더가 해당 캡션을 생성할 것입니다. 자세한 실험 결과에 따르면, MGCN, GCN, D는 각각 최종 성능 향상에 기여할 수 있습니다. 사전 훈련용 대규모 언어 말뭉치를 사용하면, 모델이 데이터셋 편향의 과적합 문제와 추론 추론력을 개선하기 위한 도움이 될 수 있습니다. 지식 기반 사전을 포함한 전체 아키텍처는 전통적인 인코더 디코더 아키텍처와 비교하여 최신 기술 성능을 달성하고 더 많은 추론 정보를 생성할 수 있습니다.\n\n![이미지3](/assets/img/2024-06-22-IntroofKnowledgeGraphenhancedapproaches_2.png)\n\n\n<div class=\"content-ad\"></div>\n\n![Knowledge Graph Enhanced Approaches](/assets/img/2024-06-22-IntroofKnowledgeGraphenhancedapproaches_3.png)\n\n![Knowledge Graph Enhanced Approaches](/assets/img/2024-06-22-IntroofKnowledgeGraphenhancedapproaches_4.png)\n\n# 지식 그래프 기반 질문 응답 소개\n\n두 번째 토론은 지식 기반 질문 응답에 관한 것입니다. 질문 응답 문제는 오랫동안 연구되어 왔습니다. 미리 학습된 언어 모델에서 직접 응답을 생성하는 것은 그다지 만족스럽지 않습니다. 그럼에도 불구하고, 지식 베이스 또는 지식 그래프는 더 많은 선험적 정보를 가져올 수 있다고 여겨지며, 따라서 지식 베이스 질문 응답 방법론은 더 많은 연구의 주목을 끌고 있습니다. [3, 30] 작업이 소개되기 전에는, 구조화된 지식 베이스 질문 응답이 연구되어 왔습니다. 이는 복잡한 질문에 대한 기계의 응답을 두 가지 방법으로 더 정확하게 개선하기 위해 필요한 질문을 표준 쿼리로 변환하고, 쿼리 일치를 사용하여 지식 베이스의 API를 활용하는 정보 검색 기반의 방법입니다. 또 다른 방법은 의미 분석 기반으로, 의미 분석 시스템을 사용하여 질문의 의미를 변환하고, 후보 응답을 사용하여 거리를 측정하고, 상위 N 순위 점수로 적합한 답변을 찾는 방법입니다.\n\n<div class=\"content-ad\"></div>\n\n[3]은 기본 아이디어에 대한 클래식한 방법입니다: 먼저 질문 내의 엔티티를 기반으로 지식 베이스에서 후보 답변을 찾아내는 것(빔 서치), 그런 다음 질문과 후보 답변을 저차원 공간에 매핑하여 분산 표현, 즉 분산 임베딩을 얻는 과정을 거칩니다. 이후 분산 임베딩에 대한 판별자를 훈련시켜 질문 임베딩과 해당 올바른 답변 임베딩 간의 상관 점수를 높게 유지합니다. 모델 훈련이 완료되면 잠재적인 후보 답변을 상관 점수에 따라 순위를 매기고 최상위 답변을 선택할 수 있습니다.\n\n질문 임베딩에는 먼저 질문을 사전 사이즈 및 지식 베이스 내의 엔티티 및 관계 수에 해당하는 다중 핫 벡터로 매핑하는 작업이 포함됩니다. 그런 다음 행렬 곱셈에 의해 저 차원 공간으로 변환하여 N 차원에서 k 차원으로 다시 형성합니다. 답변 후보 임베딩은 유사한 과정을 거쳐 생성되지만 더 많은 정보가 인코딩됩니다. 첫 번째는 경로 표현이고, 답변 임베딩에는 질문에서 답변 엔티티로의 경로를 포함해야 하며 단순함을 위해 1-hop 또는 2-hop 경로가 사용됩니다. 두 번째는 서브그래프 표현이며, 후보 답변 엔티티의 서브그래프를 고려해야 합니다. 경로 표현과 서브그래프 표현을 구별하기 위해 원래 답변 벡터의 차원은 사전 사이즈와 지식 베이스 내의 엔티티 및 관계 수의 두 배입니다. 그런 다음 행렬 곱셈을 사용하여 차원을 k로 축소합니다. 최종 점수 함수는 S(q, a) = f(q)⊤g(a)이며, 훈련 중 정확한 답변이 더 높은 점수를 가져야 합니다. 손실 함수는 마진 기반 순위 함수입니다.\n\n![이미지](/assets/img/2024-06-22-IntroofKnowledgeGraphenhancedapproaches_5.png)\n\n여기서 m은 마진으로 0.1이며, Ã는 부정적인 샘플 (다른 후보 경로)의 50% 및 무작위 답변의 50%로 구성된 부정 후보 답변 세트입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Intro of Knowledge Graph Enhanced Approaches 6](/assets/img/2024-06-22-IntroofKnowledgeGraphenhancedapproaches_6.png)\n\n![Intro of Knowledge Graph Enhanced Approaches 7](/assets/img/2024-06-22-IntroofKnowledgeGraphenhancedapproaches_7.png)\n\n이 논문은 실험적으로 2-홉 경로가 1-홉 경로보다 훨씬 우수하며, 서브그래프 표현이 최종 성능을 향상시킬 수 있다는 것을 검증합니다. 왜냐하면 서브그래프와 2-홉은 둘 다 더 많은 정보를 가져올 수 있기 때문입니다. 이 방법은 수작업으로 생성된 피처나 추가 시스템(어휘 맵, 품사 태그, 의존성 트리 등)의 도움이 거의 필요하지 않습니다. 비교적 간단하고 구현하기 쉬우며, 강력한 F1 점수 점수인 39.2를 달성했습니다. 또한, 이 논문에서 데이터셋을 확장하고 멀티태스크 학습을 함으로써 실험 데이터 부족의 단점을 일부 해소하는 데 기여했습니다. 그러나 임베딩 방법은 블랙 박스처럼 작용하여 해석 가능성이 부족하며, 의미 주석은 문제를 논리적 형태의 표현으로 변환하고 정보 추출은 각 차원에 의미를 제공해 주지만 선험지식과 추론이 부족합니다. 또한, 이 방법은 단어 순서를 고려하지 않는 bag-of-words 모델과 유사하며, 모델은 단순한 얕은 다층 퍼셉트론만 사용한다는 것이 특징입니다. 이는 딥 뉴럴 네트워크로 해결할 수 있습니다.\n\n# 지식 기반 추천 시스템 소개\n\n\n<div class=\"content-ad\"></div>\n\n세 번째 토론은 지식 기반 추천 시스템에 관한 것입니다. 추천 시스템은 전자 상거래, 온라인 영화, 온라인 게임, 레스토랑, 여가 활동 등 다양한 분야에서 널리 사용됩니다. 또한, 지식 그래프는 기계 판독, 자동 추출, 텍스트 분류, 단어 임베딩, 질의 응답 등 다양한 응용 프로그램에서 성공적으로 활용되고 있습니다. 따라서 추천 시스템과 지식 그래프를 결합하여 추천의 정확도와 설명 가능성을 향상시키는 것이 합리적입니다. 특히 사용 가능한 지식 그래프에서 엔티티와 관계 정보의 장점을 살리는 데 중요합니다 [1, 31, 32]. 여기서 추천 임베딩 공간과 지식 그래프 임베딩 공간을 통합하는 한 가지 방법을 찾는 것이 중요합니다.\n\n[1]은 뉴스 추천 문제를 다루는데, 이는 본질적으로 시간에 민감하며 짧은 수명 주기, 주제에 민감하며 지식 엔티티와 상식에 가득찬 내용으로, 다른 추천 시나리오와 비교했을 때입니다. 기존 지식 그래프의 정보를 활용하여 사용자의 현재 관심사를 정확하게 파악하고 더 나은 예측을 제공할 것으로 기대됩니다.\n\n[1]은 깊은 지식 지능 네트워크(DKN)를 제안했는데, 뉴스 제목에 대한 더 나은 표현을 만들기 위해 제목 단어 임베딩, 제목 엔티티 임베딩, 엔티티 컨텍스트 임베딩을 포함하는 아이디어를 기반으로 하고 있습니다. 여기서 마지막 두 임베딩은 지식 그래프에서 추출된 것이며, 사용자가 클릭한 과거 제목에 후보 뉴스 제목을 주의 네트워크를 통해 일치시켜서 후보 뉴스를 클릭할 확률을 생성합니다. 단어 임베딩은 널리 사용되지만, 지식 그래프 임베딩에는 TransE, TransH, TransR, TransD와 같은 다양한 접근 방식이 있습니다. 이를 통해 제목 단어 시퀀스가 지식 그래프의 엔티티로 매핑되고, 엔티티 임베딩은 해당 엔티티 하위 그래프를 벡터화하여 얻습니다. 또한, 엔티티의 컨텍스트, 즉 지식 그래프에서 한 번의 점프 이내에 있는 엔티티,도 추출되어 벡터화되는 방식과 동일합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-22-IntroofKnowledgeGraphenhancedapproaches_9.png)\n\n단순히 이 세 개의 임베딩을 연결하는 대신, 단어별로 정렬한 다음 아래에 표시된 것처럼 3채널 텐서를 구성하는 것이 더 좋습니다. 이후 CNN 모델을 사용하여 특징 맵과 풀링을 추출합니다. 또한, 주의 네트워크를 사용하여 사용자가 클릭한 히스토리 제목 순서의 가중 기여를 자동으로 계산합니다.\n\n![이미지](/assets/img/2024-06-22-IntroofKnowledgeGraphenhancedapproaches_10.png)\n\n실험을 거친 결과, 이 방법이 효과적이며 SOTA 성과를 보였습니다. 게다가, 제외 연구를 수행한 결과, 엔티티 임베딩, 컨텍스트 임베딩, DKN+TransD, 매핑된 DKN, 주의 네트워크가 모두 AUC에 기여할 수 있다는 것을 명확히 입증했습니다. 또한, 사례 연구를 통해 모델이 지식 그래프를 활용한 후에 잠재적 정보 연결을 찾아낼 수 있다는 것을 명백히 보여줍니다. [31] 또한 사용자 선호도가 지식 그래프를 따라 한 단계씩 전파된다는 것을 연구하였습니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n![IntroofKnowledgeGraphenhancedapproaches_11](/assets/img/2024-06-22-IntroofKnowledgeGraphenhancedapproaches_11.png)\n\n![IntroofKnowledgeGraphenhancedapproaches_12](/assets/img/2024-06-22-IntroofKnowledgeGraphenhancedapproaches_12.png)\n\n# References\n\n[1] H. Wang, F. Zhang, X. Xie, and M. Guo, “DKN: Deep knowledge-aware network for news recommendation,” in Proceedings of the 2018 world wide web conference, 2018, pp. 1835–1844.\n\n\n<div class=\"content-ad\"></div>\n\n[2] X. 양, K. 탕, H. 장, J. 최, “이미지 캡션을 위한 씬 그래프 자동 인코딩,” IEEE/CVF 컴퓨터 비전 및 패턴인식 콘퍼런스 논문집, 2019, pp. 10685–10694.\n\n[3] A. 보르드, S. 초프라, J. 웨스턴, “서브그래프 임베딩을 이용한 질의응답,” arXiv 프리프린트 arXiv:1406.3676, 2014.\n\n[4] K. 쉬, S. 레디, Y. 펑, S. 황, D. 조, “관계 추출 및 텍스트 증거를 통한 프리베이스 질문응답,” arXiv 프리프린트 arXiv:1603.00957, 2016.\n\n[5] W. 첸, M.-W. 창, E. 슐링거, W. 왕, W. W. 코엔, “테이블 및 텍스트를 활용한 오픈 질문 응답,” arXiv 프리프린트 arXiv:2010.10439, 2020.\n\n<div class=\"content-ad\"></div>\n\n[6] W. Chen, H. Zha, Z. Chen, W. Xiong, H. Wang, and W. Wang, \"Hybridqa: A dataset of multi-hop question answering over tabular and textual data,\" arXiv preprint arXiv:2004.07347, 2020.\n\n[7] A. H. Li, P. Ng, P. Xu, H. Zhu, Z. Wang, and B. Xiang, \"Dual reader-parser on hybrid textual and tabular evidence for open domain question answering,\" arXiv preprint arXiv:2108.02866, 2021.\n\n[8] P. Yin, G. Neubig, W.-t. Yih, and S. Riedel, \"TaBERT: Pretraining for joint understanding of textual and tabular data,\" arXiv preprint arXiv:2005.08314, 2020.\n\n[9] H. Iida, D. Thai, V. Manjunatha, and M. Iyyer, \"Tabbie: Pretrained representations of tabular data,\" arXiv preprint arXiv:2105.02584, 2021.\n\n<div class=\"content-ad\"></div>\n\n[10] H. A. Pandya and B. S. Bhatt, “Question answering survey: Directions, challenges, datasets, evaluation matrices,” arXiv preprint arXiv:2112.03572, 2021.\n\n[11] Y. Zhang, J. Hare, and A. Prügel-Bennett, “Learning to count objects in natural images for visual question answering,” arXiv preprint arXiv:1802.05766, 2018.\n\n[12] M. Zhao et al., “Towards Video Text Visual Question Answering: Benchmark and Baseline,” in Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.\n\n[13] R. Tanaka, K. Nishida, and S. Yoshida, “Visualmrc: Machine reading comprehension on document images,” in Proceedings of the AAAI Conference on Artificial Intelligence, 2021, vol. 35, no. 15, pp. 13878–13888.\n\n<div class=\"content-ad\"></div>\n\n# Table of Research Papers\n\n| 번호 | 작가 | 년도 | 제목 |\n|-----|------|------|------|\n| 14 | Q. Zhu, C. Gao, P. Wang, and Q. Wu | 2021 | \"Simple is not easy: A simple strong baseline for textvqa and textcaps\" in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 4, pp. 3608–3615. |\n| 15 | Y. Kant et al. | 2020 | \"Spatially aware multimodal transformers for textvqa\" in European Conference on Computer Vision, pp. 715–732. |\n| 16 | X. Wang et al. | 2020 | \"On the general value of evidence, and bilingual scene-text visual question answering\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10126–10135. |\n| 17 | R. Hu, A. Singh, T. Darrell, and M. Rohrbach | 2020 | \"Iterative answer prediction with pointer-augmented multimodal transformers for textvqa\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9992–10002. |\n\n<div class=\"content-ad\"></div>\n\n[18] J. Park and Y. Kim, “Styleformer: Transformer를 기반으로 한 스타일 벡터를 활용한 생성적 적대 신경망,” IEEE/CVF 컴퓨터 비전 및 패턴 인식 학회 논문집, 2022, pp. 8983–8992.\n\n[19] W. Liao, K. Hu, M. Y. Yang, and B. Rosenhahn, “시맨틱-공간 인식 GAN을 활용한 텍스트에서 이미지 생성,” IEEE/CVF 컴퓨터 비전 및 패턴 인식 학회 논문집, 2022, pp. 18187–18196.\n\n[20] P. Zhang, L. Yang, J.-H. Lai, and X. Xie, “자세 안내를 위한 이중-작업 상관성 조사: 사람 이미지 생성,” IEEE/CVF 컴퓨터 비전 및 패턴 인식 학회 논문집, 2022, pp. 7713–7722.\n\n[21] G. Luo 외, “주시어 표현 이해 및 분할을 위한 다중 작업 협력 네트워크,” IEEE/CVF 컴퓨터 비전 및 패턴 인식 학회 논문집, 2020, pp. 10034–10043.\n\n<div class=\"content-ad\"></div>\n\n[22] H. Liu, A. Lin, X. Han, L. Yang, Y. Yu, and S. Cui, “Refer-it-in-rgbd: A bottom-up approach for 3D visual grounding in RGBD images,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 6032–6041.\n\n[23] Y. Qi et al., “Reverie: Remote embodied visual referring expression in real indoor environments,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 9982–9991.\n\n[24] Y. Liu, B. Wan, L. Ma, and X. He, “Relation-aware instance refinement for weakly supervised visual grounding,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 5612–5621.\n\n[25] J. Wang and L. Specia, “Phrase localization without paired training examples,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 4663–4672.\n\n<div class=\"content-ad\"></div>\n\n[26] S. Chen et al., \"Hsva: Hierarchical semantic-visual adaptation for zero-shot learning,\" Advances in Neural Information Processing Systems, vol. 34, pp. 16622–16634, 2021.\n\n[27] H.-Y. Tseng, H.-Y. Lee, J.-B. Huang, and M.-H. Yang, \"Cross-domain few-shot classification via learned feature-wise transformation,\" arXiv preprint arXiv:2001.08735, 2020.\n\n[28] Y. Tian, Y. Wang, D. Krishnan, J. B. Tenenbaum, and P. Isola, \"Rethinking few-shot image classification: a good embedding is all you need?,\" in European Conference on Computer Vision, 2020: Springer, pp. 266–282.\n\n[29] Y. Wang, Q. Yao, J. T. Kwok, and L. M. Ni, \"Generalizing from a few examples: A survey on few-shot learning,\" ACM computing surveys (csur), vol. 53, no. 3, pp. 1–34, 2020.\n\n<div class=\"content-ad\"></div>\n\n[30] A. Bordes, J. Weston, and N. Usunier, “Open question answering with weakly supervised embedding models,” in Joint European conference on machine learning and knowledge discovery in databases, 2014: Springer, pp. 165–180.\n\n[31] H. Wang et al., “Ripplenet: Propagating user preferences on the knowledge graph for recommender systems,” in Proceedings of the 27th ACM international conference on information and knowledge management, 2018, pp. 417–426.\n\n[32] F. Zhang, N. J. Yuan, D. Lian, X. Xie, and W.-Y. Ma, “Collaborative knowledge base embedding for recommender systems,” in Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 2016, pp. 353–362.","ogImage":{"url":"/assets/img/2024-06-22-IntroofKnowledgeGraphenhancedapproaches_0.png"},"coverImage":"/assets/img/2024-06-22-IntroofKnowledgeGraphenhancedapproaches_0.png","tag":["Tech"],"readingTime":15},{"title":"Grammarly를 처음부터 만드는 방법 한글로 상세하게 알아보기","description":"","date":"2024-06-22 21:20","slug":"2024-06-22-CreatingGrammarlyfromScratch","content":"\n\nLLM (Language Model)의 완전한 구현과 API, 프런트 엔드, 그리고 간편한 모니터링을 통해.\n\n![이미지](/assets/img/2024-06-22-CreatingGrammarlyfromScratch_0.png)\n\n참고: 이 Grammarly 복제품은 덴마크어로 제작되었습니다. 제작된 정확한 모듈은 덴마크어에 특화되어 있지만, 방법론과 구현은 다른 언어로 전환 가능해야 합니다.\n\n내 지식상, 다른 언어를 위한 Grammarly의 완전한 복제본을 만들어 다른 사람들이 배울 수 있도록 그 과정을 문서화한 사람은 없는 것 같아요. 적어도 제가 찾은 바로는요. 그래서 제가 직접 시도해 보기로 결정했어요. 쉽지는 않지만 안정적이고 가치 있는 상태로 이끌어낼 수 있을 거라고 생각해요.\n\n<div class=\"content-ad\"></div>\n\n이 글은 좀 길 거에요. 최대한 장을 나눠서 구성했어요. 필요한 부분을 읽고, 흥미 없는 부분은 건너뛰셔도 돼요. 시작해볼까요?\n\n이 내용을 읽을 때, 프로젝트가 완성되기까지 약 18개월이 걸렸다는 걸 염두에 두시면 좋아요. 그래서 어떤 부분은 빨리 끝난 것처럼 보일지라도, 한 달 또는 두 달이 걸렸다고 가정하셔도 돼요 :-).\n\n# 0. 아이디어\n\nGrammarly의 완벽한 복제품을 만드는 것은 불가능하고 목표도 아니에요. 제가 흥미를 느끼는 건 핵심 기능을 재구축해보고 그 작동 방식을 파악하는 거에요. 그래서 여기서의 주요 목표는 다음을 이해하는 것이에요:\n\n<div class=\"content-ad\"></div>\n\n- 철자 교정, 간단한 문법 규칙, 그리고 문장부호에 도움이 되는 간단한 체커를 만들기\n- 더 많은 문제를 더 나은 방식으로 해결하기 위해 큰 모델 구축하기 (철자 교정, 문장부호, 또는 다른 것)\n- 사용자가 프로젝트와 상호 작용할 수 있는 간단한 UI 개발하기\n- 모든 것을 웹과 클라우드에서 작동하도록 연결하기\n\n# 1. 시작하는 곳은 어디인가요?\n\n어떤 것을 개발하기 전에, 이 프로젝트를 어떻게 구축해야 하는지 정의합시다.\n\n실시간으로, 실수를 했던 것은 구조, 아키텍처 등에 대한 고려 없이 이 프로젝트를 완성했었다는 것입니다. 이는 큰 실수였습니다! 그래서 그 문제를 가장 먼저 해결하고, 전체 코드베이스를 2~3번 완전히 다시 작성하는 것을 줄이겠습니다.\n\n<div class=\"content-ad\"></div>\n\n각 올바른 항목을 모듈로 만들어 추가, 수정 또는 삭제하기 쉽도록 구성하는 것이 아이디어입니다. 그러나 어떤 모듈이 가치를 제공할까요? 이 주제에 대해 조사한 후, 한 보고서가 주요 문제들(덴마크어)은 다음과 같습니다(특정 순서 없음):\n\n- 구두점(주로 “,” 및 “.”)\n- 대문자\n- 철자\n- 오잘된 시제\n- 주제 부재\n- 관사\n- 복합어\n\n그리고 제가 추가한 일부 작은 문제점들:\n\n- 과도한 공백\n- 중복 단어\n- 명확성?\n\n<div class=\"content-ad\"></div>\n\n쉬운 내용부터 시작해봐요...\n\n# 1.1 구조 및 유틸리티\n\n첫 시작은 간단한 모듈을 구축하는 것입니다. 이렇게 하면 구조가 작동하는지 확인한 후에 어떠한 중요한 모듈을 만들기 전에 다시 작성해야할 수도 있는 불편함을 먼저 확인할 수 있습니다. 백엔드는 Python으로 구축될 것입니다. 다른 언어로 하는 것이 더 빠를 수도 있지만, 그것이 Grammarly의 경우라고 하더라도 이미 존재하는 패키지를 사용해서 백엔드를 상당히 간단하게 만들 수 있습니다! 처음에 다룰 주제는:\n\n- 대소문자\n- 과도한 공백\n- 두 번 반복되는 단어\n\n<div class=\"content-ad\"></div>\n\n아직 모듈을 개발하기에 앞서, 외부 구조를 먼저 만들어봅시다. 모듈은 찾은 오류 목록을 반환할 것이므로, 이를 위한 클래스를 만들어보겠습니다. Error와 ErrorList 클래스가 만들어졌으며, 각 오류에 대한 기능과 오류 목록을 함께 추가해야 할 필요가 있습니다(utils/error_handling.py에서 확인 가능). Error 클래스는 다음과 같은 main 메서드로 요약됩니다:\n\n```js\ndef to_list(self, include_type=False):\n    self.is_healthy()\n    if self.wrong_word == self.right_word:\n        print(\"ALERT: Error has the same wrong and right word, therefore skipped\")\n        print(self.to_finalized_list())\n        return None\n    if include_type:\n        return [self.wrong_word, self.right_word, self.indexes, self.description, self.get_type()]\n    return [self.wrong_word, self.right_word, self.indexes, self.description]\n```\n\n그렇다면 오류의 유형은 무엇일까요? 같은 위치에 여러 오류가 발생하는 문제를 해결하는 데 도움이 됩니다. Grammarly는 동일한 단어에 여러 오류를 보여주지 않고 결합하기 때문에, 우리도 이와 같이 해야 합니다. error_concatenator()는 오류를 함께 결합하는 방법입니다. 이는 지루하고 번거로운 작업이지만 보람있는 과정이며, 다음과 같이 오류를 변환합니다:\n\n```js\n    [\n        \",\", \"\", [18,19], \n        \"'that' 앞에 쉼표가 있어서는 안 됩니다\", \"del_punc\"\n    ]\n    +\n    [\n        \"paul walker\", \"Paul Walker\", [8,19], \n        \"'Paul Walker'는 대문자로 시작해야 합니다\", \"add_cap\"\n    ]\n\n    =>\n\n    [\n        \"paul walker,\", \"Paul Walker\",\n        [8,19], \n        \"'that' 앞에 쉼표가 있어서는 안 됩니다. 'Paul Walker'는 대문자로 시작해야 합니다\", \n        \"add_cap\"\n    ]\n```\n\n<div class=\"content-ad\"></div>\n\n이제 색인에 대해 이야기해 봅시다. 일부 모듈은 문자에 대해 작동해야 하고, 일부는 단어에 대해 작동해야 합니다. 또 다른 문제는 일부 모듈이 예측을 하기 전에 다른 모듈에 종속될 수 있다는 것입니다. 그래서 우리는 앞단에서 오류를 쉽게 강조하기 위해 단어 인덱스를 캐릭터 인덱스로 변환하는 방법이 필요합니다. 그리고 이러한 인덱스는 각 모듈이 수정하는 입력 문장과 다를 수 있는 입력 문장을 가리켜야 합니다.\n\n이를 달성하기 위해 IndexFinder 클래스가 만들어졌습니다. 초기화할 때 입력 문장이 제공됩니다. IndexFinder는 그런 다음 모듈 사이에서 전달되며, 필요한 경우 그들의 오류를 기반으로 문장을 변경할 수 있습니다. 필요할 때마다, 단어 인덱스를 입력 문장의 캐릭터 인덱스로 변환할 수 있습니다.\n\n실제 모듈을 만들기 전에, 주 스크립트를 실행하여 먼저 작업해 보겠습니다. 이것은 상당히 쉽습니다. 필요한 모듈을 가져와 초기화한 후, Flask와 같은 마이크로웹 프레임워크와 함께 작동하도록 만들어야 합니다. index 함수는 다음과 같이 보여야 합니다:\n\n```js\napp = Flask(__name__)\nCORS(app)\n@app.route(\"/\", methods=[\"POST\"])\n\ndef index():\n    data = request.get_json()\n    input = data[\"sentence\"]\n    output = correct_input(input)\n    return jsonify(output)\n```\n\n<div class=\"content-ad\"></div>\n\n올바른 입력 함수를 사용하면 우리가 원하는 대로 문장을 수정할 수 있습니다. CORS는 프론트엔드가 백엔드에 연결할 수 있도록 하는 데 필요합니다. Flask는 완전한 프로덕션 환경에는 적합하지 않을 수 있지만, 지금은 작동해야 합니다. 만약 이를 배포하고 적절한 트래픽을 받는다면, 이 부분은 개선되어야 할 것입니다.\n\n## 1.2. 얼마나 어려울까요?\n\n첫 번째 모듈은 과도한 공백입니다. 일부 공간(그리고 여러분의 시간)을 보존하기 위해 코드에 대해서는 자세히 설명하지 않겠습니다. 모든 모듈의 아이디어와 기능을 어떻게 달성할 것인지에 대해 설명하겠습니다. 이 모듈은 매우 간단합니다. 앞뒤로 나열된 공백을 제거하고 싶습니다. 앞에서부터 뒤로 모든 문자를 확인합니다. 공백이 나타나면 삭제하라고 제안합니다. 뒤에서부터 앞으로 반복합니다. 그런 다음 각 문자를 확인합니다. 여러 개의 공백이 있다면 하나의 공백으로 바꾸라고 제안합니다.\n\n다음은 복합어입니다. 문장에서 \"I really really like a a cake\"와 같이 적용할 수 있습니다. 이겢 수 있는 단순한 오류들 중에는 발견하기 어려운 경우도 있습니다. 또한 같은 단어가 두 번 연속으로 나타나야 하는 경우도 있을 수 있습니다. 이를 찾으려면 대량의 텍스트를 조사해야 합니다. 덴마크어에서는 Gigaword를 사용했지만, 영어에서는 Wikipedia, nltk 및 Spacy를 포함한 다양한 옵션이 있습니다. 대량의 텍스트를 여러 번 조사해야하니 적절한 것을 찾으시길 바랍니다. 신문, 연구 보고서 등과 같이 이상적인 고품질로부터 가져오는 것이 좋습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 이를 살펴보고, 한 단어가 두 번 연속으로 나타나는 모든 이벤트를 저장하고 확인에서 제외하세요. 또한, 이름을 수정하고 싶지 않은 경우가 있으니 명명된 entity 목록을 가져와서 제외하세요. 이 모듈로는 여기까지입니다.\n\n이 섹션의 마지막 모듈은 대소문자화입니다. 앞서 말한 명명된 entity 목록을 가져와야 합니다. 이제 모든 단어를 살펴보고, 이전 단어가 마침표를 가지고 있다면, 이미 대문자화 되어 있지 않다면 대문자화를 제안하세요. 만약 단어가 명명된 entity 목록에 있다면 대문자화를 제안하세요. 또한, 이 모듈은 기준을 충족하지 않는 단어는 대문자화하지 말아야 합니다. 이제 이 모듈은 문장이 그 모듈의 결과로 변경될 수 있기 때문에 구두점 이후에 있어야 할 중요한 사항입니다.\n\n# 1.3 Named Entity Recognition\n\n이제, 앞에서 언급한대로, 명명된 entity가 무엇인지 파악해야 합니다. 또한, 각 단어의 품사를 알아내야 합니다. 다행히도, 이것은 새로운 것이 아니며 이미 여러 NLP 라이브러리에서 구현되어 있으므로 빠르게 가져와서 사용하면 충분할 것입니다. 영어 모델은 Huggingface, nltk 및 Spacy에서 사용 가능하니 선호도에 따라 선택하세요.\n\n<div class=\"content-ad\"></div>\n\n메인 함수의 시작 부분에 NER 및 POS 태거(각각 다를 수 있습니다)를 실행하여 나중에 사용할 수 있도록 해보세요. 대문자 및 복합 단어 모듈에서 NER가 예상대로 작동하는지 확인해보세요.\n\n# 1.4 이제 재미있는 부분으로: 모델 훈련\n\n아니면 거의 그쯤이죠. 모델을 훈련하기 전에 데이터가 필요합니다. 이전에 언급한 소스들을 기억하시나요? 이제 그것들을 잘 활용할 시간입니다. 중요한 점은 충분한 양뿐만 아니라 품질이 매우 뛰어난 텍스트를 찾는 것입니다. 문법 오류가 극히 낮은 것을 확인하려면 직접 일부 내용을 확인해보세요.\n\n우리가 만드는 데이터셋은 선택한 모델에 적합해야 합니다. 처음 해결해야 할 문제는 구두점입니다. 따라서 무엇을 하기 전에 사용할 적절한 모델을 결정해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 1.5 적합한 모델 선택\n\n딥러닝 모델의 NLP 공간은 번창하고 있으므로 사용할 수 있는 모델이 많습니다. 적어도 영어로는 그렇죠. 일반적으로 이 문제를 다루는 두 가지 방법을 볼 수 있습니다 (다른 생각이 떠오르면 저에게 연락해 주세요!)\n\n- Seq-2-seq: 모델에 문장을 제공하면 모델이 문장에 구두점과 함께 반환합니다. 데이터셋은 문장 간에 있어야 하며 품질이 높은 많은 데이터가 필요합니다. 구두점을 찾아 잠재적 제안의 인덱스를 반환하는 것이 어려울 수 있습니다.\n- BERT: 모델에 문장을 제공하면 분류된 결과를 반환합니다. 더 적은 데이터가 필요하지만 추론 속도가 느릴 수 있어 합리적인 계산 시간을 위해 더 작은 모델이 필요하지만 정확성을 유지할 만큼 충분히 커야 합니다. (Dandelion과 DistilBERT만 덴마크어로 제공되지만 RoBERTa, DistilBERT, Electra 등을 사용할 수 있습니다.)\n\nSep-2-sep를 통해 결과가 불충분하고 학습 시간이 지나치게 소요되어 덴마크어 미리 훈련된 BERT로 이동하기로 결정했습니다. 이후 데이터셋은 비교적 간단하게 생성할 수 있었습니다. (2024년 2월 현재, GPT의 성장이 엄청나기 때문에 실제로 seq-2-seq가 적합한 해결책이 될 수 있으므로 꼭 Huggingface를 확인해보세요!)\n\n<div class=\"content-ad\"></div>\n\n데이터셋을 생성하기 전에, 스코프를 결정해 보겠습니다. 여기서 스코프란 입력 문장의 크기를 의미합니다. 좌측에 x개 단어, 우측에 y개 단어를 제공합니다. 중간 단어 뒤쪽에서 문장부호를 예측합니다. 실시간으로 올바른 스코프를 알아내는 방법은 하나뿐인데, 그것은 '테스트'입니다! 하지만 그것은 저렴하지 않습니다. 제가 작은 실험을 통해 무슨 일이 일어나는지 확인해 보았습니다. 스코프가 작을수록 추론이 빠르고 훈련도 빠릅니다만, 정확도는 낮아집니다. 그렇기에 그것은 선이 아주 섬세합니다.\n\n저는 처음에 BERT로 실험을 진행했지만(대부분의 결과를 여기에 남겼습니다), 빠른 추론을 위해 distilBERT로 빨리 전환하였습니다. 보통 왼쪽부터 더 많은 정보가 필요하지만, 오른쪽에서도 약간의 정보가 필요합니다. 높은 정확도를 얻기 위해, 그리고 여전히 빠른 추론을 위해 15-5가 적당한 스코프로 보입니다. 따라서 다음과 같은 문장이 주어진다면:\n\n데이터셋은 다음과 같이 보여져야 합니다 (여기서는 3-3의 스코프를 보여주기 위해). 대문자와 문장 부호를 모두 삭제해야 합니다. 왜냐하면 이것을 알게 된다면 정확도가 왜곡되고 실제 문장을 수정할 때 적용되지 않을 수 있기 때문입니다.\n\n![그림](/assets/img/2024-06-22-CreatingGrammarlyfromScratch_1.png)\n\n<div class=\"content-ad\"></div>\n\n표 태그를 Markdown 형식으로 변경해보세요.\n\n바로 이해하셨으면 좋겠네요.\n\n# 1.6 훈련\n\n사용한 데이터셋은 약 46,000,000개의 요소가 있었습니다. BERT의 저자들은 미세 조정을 위해 2~4회의 에폭이 최선이라고 언급하고 있습니다. 저는 2회의 에폭이 충분하다는 것을 발견했고, 3회 훈련을 할 경우 정확도가 매우 미세하게(~0.1%) 상승했지만 중요하지 않다고 판단했습니다. 비용에 비해 이점이 크지 않아 2회로 결정했습니다.\n\n<img src=\"/assets/img/2024-06-22-CreatingGrammarlyfromScratch_2.png\" />\n\n<div class=\"content-ad\"></div>\n\ndistilBERT은 Jarvislabs에서 6개의 A100을 대여하여 약 10시간 동안 합쳐서 약 120달러를 지불하고 학습되었습니다. 학습에 사용된 스크립트는 FineTuneModels/FineTuneBert에서 찾을 수 있습니다. 이것은 확실히 가장 저렴하거나 최선의 방법은 아니었지만 (2024년 2월 기준으로, 더 작은 규모의 학습 크기로 이러한 모델을 세밀하게 조정할 수 있는 더 저렴한 방법이 있어야 하므로 사용 가능한 옵션을 탐색할 시간을 갖는 것이 좋습니다), 작업을 완료하는 데 성공했습니다.\n\n# 1.7 모듈로 구현하기\n\n추론은 물론 학습과는 약간 다르지만 구현하기 어렵지 않습니다. 여기에 전체 모듈을 구현했습니다. 가장 중요한 것은 모델의 출력을 softmax로부터 argmax하는 것입니다. 3개의 값이 있는 softmax에서 가장 높은 값을 가져와야 예측할 수 있습니다. 입력 문장에서 데이터셋을 생성해야 하지만 그 외에는 어렵지 않아야 합니다:\n\n```js\ndef get_predictions(self, data : string):\n    dataset, split_indexes = self.get_dataset(data)\n    tokenized_data = self.tokenizer(dataset, padding=True, truncation=True)\n    final_dataset = Dataset(tokenized_data)\n    raw_predictions, _, _ = self.trainer.predict(final_dataset)\n    maxed_predictions = np.argmax(raw_predictions, axis=1)\n    return maxed_predictions\n```\n\n<div class=\"content-ad\"></div>\n\n1) 입력문에서 데이터셋을 생성합니다, 2) 토큰화합니다, 3) 텐서 데이터셋으로 형식을 맞춥니다 (올바르게 구현하는 것이 어려울 수 있습니다. Utilities/model_utils.py 상단을 살펴보세요), 4) 모델을 실행하고, 5) 예측값을 최대화합니다.\n\n# 1.8 추가 모듈\n\n아직 몇 가지 모듈이 빠져 있습니다. 이 게시물의 크기를 줄이기 위해 (그래, 아마 이미 늦었을지도 모르겠네요 😅?!) 간단히 다루겠습니다.\n\n잘못된 동사형: 이 접근 방식은 구두점과 완전히 동일했습니다. 덴마크어에서 현재형 동사의 주요 문제는 현재형 동사가 끝에 무성자음 \"r\"이 있어서 듣기 어려울 수 있다는 점입니다. 이를 해결하기 위해 동사가 알려지지 않은 상태에서 새 데이터셋이 생성되었습니다. 따라서 이번에는 데이터셋이 다음과 같이 보일 것입니다 (여기에 표시된 것 보다 더 많은 범위로)\n\n<div class=\"content-ad\"></div>\n\n\n![Grammarly from scratch](/assets/img/2024-06-22-CreatingGrammarlyfromScratch_3.png)\n\n동사의 형태를 나타내는 값입니다. 이 방법은 상당히 좋은 결과를 보여줬어요:\n\n![Grammarly from scratch](/assets/img/2024-06-22-CreatingGrammarlyfromScratch_4.png)\n\n이 경우, 잘못된 예측의 비율은 약 1%로 저한테는 조금 높았어요. 그래서 저는 신뢰 수준이 95% 이상인 경우에만 예측을 사용했어요 (이 모델들에 대한 확신은 아니지만 어느 정도의 추정이라고 할 수 있어요). 이렇게 하니 잘못된 예측의 비율을 약 0.4%로 줄일 수 있었고, 정확도는 2 pp만큼 감소했어요 (와우)! 🎉\n\n\n<div class=\"content-ad\"></div>\n\n맞춤법 검사는 각자 큰 프로젝트이며 이곳에서 다루기에는 너무 많습니다. 나중에 이에 관해 다른 게시물을 작성할 계획이 있습니다. 여러 가지 방법을 시도해 봤어요: 간단한 맞춤법 검사기, n-gram, 단어 임베딩 및 GPTs 등을 사용했습니다. 결국 Peter Norvig의 간단한 맞춤법 검사기 공식과 덴마크어 철자 관련 특정 문제를 혼합하여 철자 문제의 위치를 잘 추측할 수 있었습니다. 참고로 이 공식은 놀라울 만큼 훌륭하니 꼭 읽어보시기를 추천드립니다.\n\n주제 부족은 좀 더 현대적인 문제이지만, 이것을 조사해보는 것은 재미있게 생각했습니다. 보다 공식적인 텍스트를 작성할 때는 주어 부족이 결코 존재해서는 안 됩니다. 이 문제에 대한 만족스러운 해결책을 찾지 못했지만, 현재 주요 기능은 문장 구조가 몇 가지 하드코딩된 빈번한 구조와 일치하는지 확인하는 것입니다. 그렇다면 주어가 누락된 것을 쉽게 결정하고 제안할 수 있습니다.\n\n기사는 덴마크어에서 영어나 기타 언어와 마찬가지로 어렵습니다. 가장 큰 문제는 일반성과 중성을 구분하여 명사를 사용해야 하는 시점의 차이입니다 (대부분의 언어는 남성과 여성을 사용하지만 본질적으로는 같은 문제입니다). 다행히, 앞서 설명한 POS 태거가 품사 분석할 때 이를 처리하기 때문에 현재 문장과 일치하는 POS 객체에서 정보를 찾아 확인하는 것만으로 간단합니다.\n\n# 2. 프론트엔드\n\n<div class=\"content-ad\"></div>\n\n휴, 그것은 힘들었죠. 이제 사용자들이 모듈을 직접 시도하고 데이터를 얻을 수 있는 곳이 필요합니다. 이것은 웹 사이트나 플러그인 중 어딘가에 배포되어야 합니다. 이것은 백엔드와 함께 어딘가에 배포되어야 합니다.\n\n# 2.1 간단한 웹페이지\n\n이를 수행하는 여러 가지 방법이 있습니다. React나 Angular가 그 예입니다만, 저는 순수한 html, css 및 js로 수행하기로 결정했습니다. 이것은 최고의 방법은 아니지만 사용자가 테스트할 수 있는 간단한 웹 사이트로서 제작될 것입니다(이 웹사이트는 원래 덴마크어로 되어 있었고, Google이 번역을 수행했기 때문에 완벽하지 않을 수 있습니다):\n\n![웹페이지 이미지](/assets/img/2024-06-22-CreatingGrammarlyfromScratch_5.png)\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-22-CreatingGrammarlyfromScratch_6.png)\n\n도메인을 구입하고 Github 레포지토리에 연결하였으며, Github 페이지를 통해 사이트를 런칭했어요. 정적 사이트를 개발하는 장점이에요. 만일 몇 가지 프레임워크를 사용한다면, Vercel은 호스팅할 수 있는 방법일지도 모르겠어요.\n\n한 가지 집중한 점은 지속적인 수정과 텍스트를 작은 조각으로 나눌 때에 있었어요. 처음에는 서비스가 너무 느리다는 피드백이 있었어요. 이는 주로 BERT로 인한 문장부호로 어렵게 최적화된 이유에 있었어요. 대신 입력을 작은 조각으로 나누기로 결정하여 수정이 시간을 두고 사용자에게 제공되도록 하기로 했어요. 이는 사용자 경험을 향상시키는 것으로 보였어요. 텍스트가 변경되었는지 지속적으로 확인하여 사용자가 텍스트를 변경하고 웹페이지의 텍스트 편집기에서 실시간 피드백을 받을 수 있도록 했어요.\n\n# 2.2 전부 함께 모으기\n\n<div class=\"content-ad\"></div>\n\n그래서 이를 호스팅할 곳이 필요합니다. 프론트 엔드는 무료로 Github 페이지에 호스팅되어 있습니다. 백엔드에는 몇 가지 옵션이 있습니다. 문제는 Azure, AWS 또는 GCP를 사용하는 경우 함수 앱 / 람다 함수 / 클라우드 함수가 될 수 없다는 것입니다. 모델 및 큰 사전이 작동하려면 이러한 모듈을로드해야 하기 때문입니다. 이는 약 15초 정도 소요되므로 각 요청마다 일어날 수 없습니다.\n\nVM을 임대하는 것이 저의 선택이었습니다. 구글이 가장 낮은 가격을 제공했기 때문에 그 과정이 전부 구글에 달렸습니다(월 200달러). 이에 대한 더 나은 해결책이 있을 것이라고 생각되므로 여기에는 개선할 공간이 분명히 있습니다. Google VM에 간단한 Flask 앱을 설정하는 것은 비교적 쉽지만, 새 코드를 업로드하고 프로덕션 환경에 배포하기 위해 일부 gcp cli 명령어를 찾고 있을 수 있습니다. 그래서 다음 사항을 염두에 두세요:\n\n\n        gcloud builds submit --tag _bucket_or_vm_name_\n        gcloud run deploy --image _bucket_or_vm_name_ --platform managed\n\n\n_bucket_or_vm_name_라는 부분은 'gcr.io/grammatiktakbackend/index'와 같이 나타나야 합니다. 프론트 엔드가 요청을 보낼 수 있도록 Python 스크립트에서 `CORS(app)`를 사용하여 Cors를 활성화해야 합니다. 보안상의 이유로 요청의 경우 프론트 엔드만 화이트리스트에 추가하는 것이 좋습니다. GCP를 사용할 때 이는 그들의 플랫폼에서 수행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 2.3 프로덕션에서의 드리프트와 정확성에 대해 얘기해 볼까요?\n\n웹사이트 하단에 피드백을 남길 수 있는 옵션이 있다는 것을 눈치 채셨을 수도 있습니다. 우리는 프로덕션에서의 드리프트 문제와 모듈이 실제로 얼마나 잘 작동하는지 정확히 모르는 문제를 함께 겪고 있습니다. 먼저, ModuleTracker가 만들어져 각 모듈(시간 및 수정사항)을 요청별로 추적하고 결과를 프론트 엔드로 다시 전송하기 전에 저장소에 업로드합니다. 저장소로는 Google Firestore를 사용했습니다. 사용된 저장소에 비해 비용이 비싸지만 유연하고 확장 가능한 NoSQL 데이터베이스입니다.\n\n피드백에 대해서는 높은 보안을 보장하기 위해 정보를 백엔드로 보내고 그 후 저장소로 전송합니다. 이미 활성화되어 있는 저장소에 연결이 빠르게 이뤄지며 VM의 비용을 증가시키지 않지만 최적화가 필요한 지점입니다.\n\n마지막으로, 마지막 수정이 처리될 때 수락되거나 거부된 수정사항에 대한 정보를 전송할 수 있습니다. 이제 이 모든 준비가 끝났으니, 어떤 모듈이 가치를 제공하고 수용 가능한 시간 내에 그것을 수행하는지 측정하기 위해 데이터를 쉽게 추출할 수 있습니다. 이제 분석하고, 그리고 모듈을 개선하거나 삭제하거나 추가하여 서비스를 향상시킵니다.\n\n<div class=\"content-ad\"></div>\n\n# 3.1 Word 추가 기능\n\n웹 사이트 외에도 서비스를 통합하여 사용할 수 있는 방법을 찾았습니다. 그래서 Grammarly가 워드에서 작동하는 것과 같은 방식으로 Word 추가 기능을 개발하기로 결정했습니다. Google 문서의 API는 굉장히 느립니다. 그래서 처음에는 Google 문서에 구현할 수 있는 방법이 없었습니다. Word 추가 기능은 로컬에서 작동하지만, 공개적으로 사용할 수 있도록 하는 노력을 하지는 않았습니다. 다시 시도해본다면, 여러 앱과 함께 작동할 수 있는 데스크탑 응용 프로그램을 개발하겠다고 생각합니다. 이렇게 하면 Word와 작업할 때 더 유연성을 제공할 수 있을 것입니다. 추가 기능 프레임워크가 약간 구식이고 오래되었다고 느껴집니다.\n\n# 3.2 고품질 데이터셋\n\n고품질의 덴마크어 데이터셋이 많지 않기 때문에 제가 직접 텍스트를 리뷰하여 모듈 테스트에 사용하기 위한 데이터를 만들었습니다. 훈련 데이터셋은 /DataProcessing 폴더에서 파일을 실행하여 생성할 수 있습니다. 모듈의 오류 수정에 사용되는 데이터셋 및 사전은 /GrammatiktakBackend/Datasets에서 찾을 수 있습니다. 테스트 데이터셋은 GrammatiktakDatasets 리포지토리에서 찾을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 4. 마무리\n\n크레딧: 많은 사람들 덕분에 이 프로젝트가 성사되었습니다. 그들은 자신의 지식, 모델 및 데이터를 모두가 사용하기 쉬운 형식으로 자유롭게 제공했습니다. 그들에게 큰 감사를 표하고 싶습니다! 그들은 이 프로젝트에 참여하지 않았으며 이 프로젝트와 관련된 어떤 일에 대해서도 책임을 지지 않습니다.\n\n- Leon Derczynski & Manuel R. Ciosici, ITU, Copenhagen: NLP 개발을 위해 Gigaword.dk에서 뛰어난 대량의 덴마크어 텍스트 컬렉션을 공유한 분들에게 감사드립니다.\n- Anita Ågerup Jervelund, dsn.dk: 덴마크어 초/중등학교의 철자 및 문법 오류에 대한 지식을 이 멋진 보고서로 공유해준 분께 감사드립니다.\n- certainly.io, Certainly, Malte Højmark-Bertelsen: 덴마크어 NLP 커뮤니티를 위해 널리 사용 가능한 Danish BERT를 개발하고 교육한 분들에게 감사드립니다.\n\n내 글을 이렇게 멀리까지 읽어 주시는 분이 있을지 잘 모르겠네요. 만약 이 글을 끝까지 읽으신 분이 있다면, 정말 감사합니다! 궁금한 점이 있으시면 트위터로 연락해주세요. 앞으로의 모든 일에 행운을 빕니다!","ogImage":{"url":"/assets/img/2024-06-22-CreatingGrammarlyfromScratch_0.png"},"coverImage":"/assets/img/2024-06-22-CreatingGrammarlyfromScratch_0.png","tag":["Tech"],"readingTime":14},{"title":"대형 언어 모델LLMs 쉽게 배우기 기초부터 시작하는 가이드","description":"","date":"2024-06-22 21:18","slug":"2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs","content":"\n\n![이미지](/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_0.png)\n\n여러분 중 많은 분들이 ChatGPT와 같은 도구를 재미있게 활용해 보신 적이 있을 것입니다. 지난 몇 년 동안 이 기술은 비즈니스 프로세스부터 일상적인 업무까지 우리 삶의 모든 측면으로 침투해 왔습니다. 그리고 알고 계세요? 이제 막 시작이라는데요. 많은 사람들이 아직 기계 학습, 신경망 또는 인공지능을 완전히 이해하지 못하고 있지만, 이제는 그것이 변화하려고 합니다. 이것은 시작점입니다. 지금 기초를 잡지 않으면 나중에 따라잡기가 더 어려울 수도 있습니다. 그러니 새로운 지식을 습득하는 것이 높은 시간입니다.\n\nOpenAI가 ChatGPT를 선보인 시점은 게임 체인저였습니다. 이를 통해 Large Language Models(LLMs)의 놀라운 파워가 드러났습니다. 이러한 모델들은 여행 계획부터 요리 레시피까지 모든 것을 변화시켰습니다. 하지만 이제 우리는 그것들을 해체하려고 합니다: 본질적으로, LLMs는 언어 처리에 뛰어난 수학적 모델들입니다.\n\n자연어 처리(Natural Language Processing, NLP)의 하위 집합인 이 기술은 기계 학습에서 주도적인 역할을 하게 되었습니다. 이제 모든 사람들이 LLMs와 NLP에 대해 배우고, 그들의 잠재력을 어떻게 활용할지에 대해 열망하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n그러니까, 이것을 자세히 살펴보겠습니다 — LLM이란 무엇이며 어떻게 작동하는지, 그리고 학습, 삶, 그 이상에 활용하는 방법에 대해 알아보겠습니다. 이 시리즈를 따라가기 위해 필요한 것은 기본적인 학교 수학 지식, 약간의 Python 프로그래밍 스킬, 그리고 건강한 호기심입니다. 머신러닝에 대한 기본적인 이해는 플러스이지만 필수는 아닙니다. 더 깊이 파고 싶다면 해당 정보를 모두 확인할 수 있는 추가 소스를 제공할 테니 걱정하지 마세요. 이 시리즈는 코더와 연구자뿐만 아니라 소프트웨어 엔지니어들을 위한 것입니다. 이해가 안 된다면 직접 관여하고 LLM을 실험해보세요.\n\n# 대형 언어 모델 해석\n\nLLM의 등장은 OpenAI의 ChatGPT가 화제가 되기 전에 시작되었습니다. 2017년 초에 구글 연구팀이 Transformer라는 혁신적인 딥러닝 모델 구조를 소개했습니다 (연구 논문: \"Attention is all you need\").\n\n이 구조는 빠르게 다양한 자연어 처리 (NLP) 작업의 표준을 제시했습니다. 아마도 구글 번역, 구글 검색 엔진 또는 자동 완성 등에서 Transformer 모델과 상호작용해본 적이 있을 것입니다. Transformer 이전에는 재귀 신경망(RNNs)이 이러한 작업에 주로 사용되었습니다. 이제 LLM이 어떻게 작동하는지 탐구하고 NLP의 기초를 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\nNLP 또는 자연어 처리는 인간 언어를 이해하고 해석하는 데 초점을 맞춘 기계 학습 분야입니다. 수학적 및 통계적 방법을 사용하여 주로 다루는 두 가지 유형의 작업이 있습니다: 분류 (예: 이메일 유형 식별) 및 회귀 (예: 인공 지능에 관한 시를 생성). 신속한 발전 덕분에 오픈 소스 LLM을 실험할 수 있는 다양한 프레임워크와 패키지가 있습니다. 인기 있는 패키지 중 하나는 Hugging Face의 Transformers입니다.\n\n대형 언어 모델(LLMs)은 일반적으로 Transformer 아키텍처를 기반으로 하며 인간 언어, 코드 및 기타 데이터를 이해하고 생성하도록 설계되었습니다. 이러한 심층 학습 모델은 거대한 데이터셋(페타바이트 규모의 텍스트)에서 훈련되어 인간 언어의 뉘앙스를 포착합니다.\n\nTransformer 신경망 아키텍처는 인상적이면서도 간단합니다. 다른 아키텍처에서는 어렵게 확장할 수 있는 방식으로 고도로 병렬화되고 확장할 수 있습니다. Transformer 아키텍처의 중요한 구성 요소 중 하나는 self-attention으로, 시퀀스의 각 단어가 시퀀스 내의 모든 다른 단어를 고려할 수 있게 해줍니다. 이는 장거리 종속성 및 맥락적 관계를 포착합니다. 그러나 Transformer에도 한계가 있습니다. 입력 맥락 창이 몇 단어까지 처리할 수 있는지가 한 가지 도전입니다. 다른 도전도 있지만 나중에 다룰 것입니다.\n\n이 게시물은 Transformer 아키텍처의 심화 내용을 다루는 것은 아니지만 기본적인 이해가 도움이 됩니다. Transformer의 작동 방식에 대해 궁금하다면, 3Blue1Brown의 Visual Intro To Transformers라는 유튜브 비디오를 강력히 추천합니다.\n\n<div class=\"content-ad\"></div>\n\nNLP 분야에서 가장 일반적인 하위 작업은 다음과 같습니다:\n\n- 분류: 텍스트, 문장 또는 단어를 분류하는 것입니다. 스팸 vs. 스팸이 아닌 이메일 식별, 문법 교정, 책의 감정 파악 또는 장르 정의 등이 예시로挙げられます. 개별 단어와 같은 더 세분화된 요소도 분류할 수 있으며, 이는 문법 태깅이나 명명된 엔티티 인식과 같은 작업으로, 각 단어에 \"사람\", \"장소\" 또는 \"객체\"와 같은 레이블이 지정됩니다.\n- 텍스트 생성: 새로운 텍스트 콘텐츠를 생성하는 것으로, 질문에 대한 답변 생성, 언어 번역을 위한 새로운 문장 작성 또는 완전히 새로운 텍스트 작성 등을 포함합니다.\n\nPython 환경에서 트랜스포머를 사용하려면 PiP를 통해 설치하세요:\n\n```js\n$ pip install transformers\n```\n\n<div class=\"content-ad\"></div>\n\n**Transformers를 사용하는 것은 가능한 쉽습니다. 예를 들어, 분류 작업:\n\n```python\nfrom transformers import pipeline\n\nclassifier = pipeline(\"text-classification\")\ntext = \"나는 자연어 처리를 위해 트랜스포머를 사용하는 것을 좋아합니다!\"\nresult = classifier(text)\n\nprint(result)\n```\n\n다양한 파이프라인과 모델이 있습니다. 이 파이프라인 내에서 사용 가능한 일반적인 작업 중 일부는 다음과 같습니다:\n\n- fill-mask: 문장에서 가리기 처리된 토큰을 예측합니다.\n- feature-extraction: 텍스트의 벡터/임베딩 표현을 제공합니다.\n- ner: 개체명 인식(Named Entity Recognition).\n- question-answering: 맥락에 기반하여 질문에 대한 답변을 제공합니다.\n- sentiment-analysis: 텍스트의 감정을 결정합니다.\n- summarization: 텍스트를 핵심 요점으로 압축합니다.\n- text-generation: 프롬프트에 기반하여 새로운 텍스트를 생성합니다.\n- translation: 한 언어에서 다른 언어로 텍스트를 번역합니다.\n- zero-shot-classification: 레이블이 지정되지 않은 텍스트를 분류합니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어 Named Entity Recognition을 사용하는 방법:\n\n```python\nfrom transformers import pipeline\nner = pipeline(\"ner\", grouped_entities=True)\ntext = \"The Transformers library is a powerful tool for natural language processing that are used by developers.\"\nentities = ner(text)\nprint(entities)\n```\n\n텍스트 생성: 이는 새로운 텍스트 콘텐츠를 생성하는 것을 의미하며, 예를 들어 질문에 대한 답변 생성, 언어 번역을 위한 새로운 문장 작성 또는 새로운 텍스트 작성을 포함합니다.\n\n텍스트 생성 작업의 예시:\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline(\"text-generation\", model=\"gpt2\")\n\nprompt = \"한때 멀고 먼 곳에서,\"\ngenerated_text = generator(prompt, max_length=50, num_return_sequences=1)\n\nprint(generated_text)\n```\n\n# LLMs가 고수준에서 작동하는 방식:\n\n오늘날 다양한 LLMs가 제공되고 있으며, 이들의 차이를 이해하지 못하면 혼란스러울 수 있습니다. Transformers는 여러 범주로 나뉘어 특정 작업을 해결하도록 설계되었습니다.\n\n1. GPT와 유사한 모델들 (자기 회귀형 트랜스포머): 이러한 모델들은 한 번에 한 단어씩 텍스트를 생성하며, 각 단어는 이전에 생성된 단어에 의존합니다. 이러한 모델들은 이야기 완성 및 회화 에이전트와 같은 텍스트 생성 작업에서 뛰어납니다. 예시로 GPT-2, GPT-3, 그리고 GPT-4 등이 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n2. BERT와 RoBERTa와 같은 BERT와 유사한 모델들: 이러한 모델들은 문장 내 단어들의 맥락을 분석하여 텍스트를 이해하고 해석하기 위해 설계되었습니다. 이러한 모델들은 텍스트 분류, 개체명 인식, 그리고 질문에 대한 답변과 같은 작업에 탁월합니다.\n\n3. BART와 T5와 같은 BART/T5와 유사한 모델들: 이러한 다목적 모델들은 입력 시퀀스를 출력 시퀀스로 변환하여 다양한 작업을 처리할 수 있습니다. 이러한 모델들은 텍스트 요약, 번역, 그리고 기타 변환 작업과 같은 작업에 특히 효과적입니다.\n\n이전에 언급된 Transformer 모델들(GPT, BERT, BART, 그리고 T5와 같은)은 근본적으로 언어 모델로 훈련됩니다. 이러한 모델들은 자가 지도 학습(self-supervised learning)이라는 방법을 통해 대량의 원시 텍스트 데이터에서 학습합니다. 자가 지도 학습에서는 모델이 입력 데이터로부터 자체 학습 신호를 생성하여 인간이 작성한 레이블 데이터가 필요하지 않다는 것을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n이 교육과정에서 모델들은 노출된 언어에 대한 통계적 이해를 발전시킵니다. 그러나 이 넓은 언어 이해는 특정 실용적인 작업에 직접적으로 적용되지는 않습니다. 이러한 모델을 특정 응용 프로그램에 유용하게 만들기 위해 전이 학습이라는 프로세스를 사용합니다. 전이 학습에서는, 미리 학습된 모델을 특정 작업에 맞게 세부 조정하는데 지도 학습을 사용하며, 이는 특정 작업에 맞춘 인간이 라벨이 지정된 데이터를 포함합니다.\n\n이 문맥에서의 일반적인 작업 중 하나는 인과 언어 모델링인데, 이 때 모델은 문장의 이전 단어를 기반으로 다음 단어를 예측합니다. 이 방법은 미래 입력을 고려하지 않고 과거와 현재 입력을 사용하여 예측을 생성하기 때문에 텍스트 생성 및 자동 완성과 같은 작업에 적합합니다.\n\n반면에 BERT와 같은 모델에서 사용되는 마스크 언어 모델링은 문장에서 특정 단어를 가리고 모델을 훈련시켜 주변 단어에 의해 제공된 문맥에 기반해 이러한 가리기된 단어를 예측하도록 합니다. 이 방법을 통해 모델은 양방향 문맥을 이해할 수 있어서 질문 응답 및 텍스트 분류와 같은 이해 작업에 매우 효과적입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_3.png)\n\n현대의 Transformer 모델은 매우 많은 매개변수와 저장 공간을 가지고 있습니다. 예를 들어, GPT-4 모델에는 1.76조 개의 매개변수가 있습니다! 이러한 모델을 훈련하는 데는 많은 양의 고품질 데이터와 상당한 컴퓨팅 자원이 필요하기 때문에 개별 개발자가 처음부터 이러한 모델을 훈련하는 것은 현실적이지 않습니다.\n\n이러한 도전에도 불구하고 위에서 언급한 기술들, 특히 전이 학습은 특정 사용 사례에 대해 이러한 사전 훈련된 모델을 효과적으로 세밀하게 조정할 수 있도록 합니다. 세밀하게 조정은 사전 훈련된 모델의 가중치를 더 작고 작업별 데이터셋에 맞게 조정하는 것을 의미합니다. 이 과정은 처음부터 모델을 훈련하는 것에 비해 필요한 컴퓨팅 자원과 시간을 상당히 줄여줍니다.\n\n전이 학습 워크플로우 예시:\n\n\n<div class=\"content-ad\"></div>\n\n1. Pretraining: 모델은 먼저 대용량의 텍스트 데이터 코퍼스를 사용하여 자기지도 학습으로 초기 학습됩니다. 이 단계는 모델이 문법, 구문 및 의미와 같은 일반적인 언어 기능을 학습하는 데 도움을 줍니다.\n\n2. Fine-Tuning: 사전 학습된 모델은 그 후 특정 응용 프로그램에 특화된 작은 레이블이 지정된 데이터셋에서 세밀하게 조정됩니다. 예를 들어, 고객 리뷰 데이터셋에서 BERT를 세밀하게 조정하면 감성 분석 작업에서 성능을 향상시킬 수 있습니다.\n\n3. 평가 및 배포: 세밀하게 조정한 후 모델은 특정 작업에 대한 성능 기준을 충족하는지 확인하기 위해 평가됩니다. 확인된 후 모델은 실제 응용 프로그램에 배포될 수 있습니다.\n\n트랜스포머는 자연어 처리를 혁신했지만, 계산 비용, 에너지 소비, 방대한 데이터셋의 필요성과 같은 도전 과제가 있습니다. 연구자들은 이러한 모델을 더 효율적이고 접근성이 높게 만들기 위해 지속적으로 새로운 아키텍처 및 기술을 탐구하고 있습니다. 예를 들어, 대규모 모델의 크기를 줄이면서 성능을 유지하는 모델 증류와 계산 부담을 줄이려는 희소한 주의 메커니즘과 같은 기술은 희망적인 연구 분야입니다.\n\n<div class=\"content-ad\"></div>\n\n요약하자면, GPT, BERT, BART, T5와 같은 Transformer 모델의 능력을 이해하고 활용하는 것은 자기 지도 학습과 전이 학습을 통해 자연어 처리에서 다양한 실용적인 응용 프로그램을 발전시킬 수 있습니다. 이러한 모델을 특정 작업에 맞게 세밀하게 조정함으로써, 계산 리소스를 많이 필요로하지 않고도 그들의 능력을 활용할 수 있습니다.\n\n# 고수준 Transformer 아키텍처:\n\n오늘날의 LLMs 대부분은 대부분 Transformers입니다. Transformer 모델 아키텍처는 아래에서 보여지며 (처음에는 복잡해 보여도 걱정하지 마세요):\n\n![Transformer Model](/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_4.png)\n\n<div class=\"content-ad\"></div>\n\n그러나, 한 걸음씩 천천히 짚어 보겠습니다. 트랜스포머 아키텍처는 주로 두 가지 중요한 구성 요소로 이루어져 있습니다: 인코더(Encoder)와 디코더(Decoder):\n\n![이미지](/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_5.png)\n\n- 인코더(Encoder): 인코더는 정교한 독해자처럼 생각할 수 있습니다. 입력 텍스트를 가져와 해당 특징의 상세한 표현을 만들어내어 모델이 입력을 깊이 이해할 수 있도록 돕습니다. 이는 입력을 해당 특징을 캡처하는 연속적인 표현으로 인코딩하는 과정을 포함합니다.\n- 디코더(Decoder): 디코더는 창의적인 작가와 같습니다. 인코더의 표현을 활용하여 다른 입력과 함께 대상 시퀀스를 생성하며, 모델이 출력 생성에 최적화되도록 합니다. 이는 텍스트 번역, 새로운 문장 생성 또는 정보 요약 등을 포함할 수 있습니다.\n\n작업에 따라 이러한 구성 요소는 독립적으로 또는 함께 사용될 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n1. 인코더 전용 모델: 문장 분류 및 개체명 인식과 같이 입력의 심도 있는 이해가 필요한 작업에 이상적입니다. BERT와 RoBERTa가 대표적인 예시입니다.\n\n2. 디코더 전용 모델: 텍스트 생성과 같이 텍스트를 생성하는 것이 목표인 생성 작업에 적합합니다. GPT-2, GPT-3 및 GPT-4가 대표적인 예시입니다.\n\n3. 인코더-디코더 모델(시퀀스-투-시퀀스 모델이라고도 함): 번역 및 요약과 같이 이해와 생성이 모두 필요한 작업에 완벽합니다. BART와 T5가 대표적인 예시입니다.\n\nTransformer 아키텍처의 중요한 특징 중 하나는 그 고급 어텐션 메커니즘입니다. 이러한 메커니즘은 모델이 입력 시퀀스의 특정 부분에 집중하도록 안내하여 더 정확하고 문맥을 고려한 결과를 얻게 합니다.\n\n<div class=\"content-ad\"></div>\n\n주의는 딥 러닝 모델(RNN, Transformer)에서 사용되는 메커니즘으로, 입력의 다른 부분에 서로 다른 가중치를 할당하여 모델이 생성, 번역 또는 감정 분석과 같은 작업을 수행하는 동안 가장 중요한 정보를 우선적으로 강조할 수 있도록 합니다. 앞서 말한대로, 주의는 입력의 다른 부분에 동적으로 초점을 맞추게 하여 성능을 개선하고 더 높은 정확도를 제공합니다.\n\n주의 메커니즘에는 다음이 포함됩니다:\n\n- Self-Attention(자가 주의): 문장에서 각 단어의 중요성을 다른 모든 단어에 대비하여 가중치를 부여할 수 있도록 하는 것으로, 모델이 멀리 떨어진 의존성과 관계를 포착할 수 있게 합니다. 자세한 설명은 The Illustrated Transformer를 참조하세요.\n- Multi-Head Attention(다중 헤드 주의): 여러 주의 헤드를 사용하여 모델이 동시에 입력의 다른 부분에 집중할 수 있는 능력을 향상시킵니다. 이 병렬 처리는 모델의 성능을 크게 향상시킵니다.\n\n주의에 대한 비유: 복잡한 책을 읽는 상황을 상상해 보세요. 자가 주의는 메모를 작성하고 서로 교차 참조하여 이야기를 더 잘 이해하는 것과 유사하며, 다중 헤드 주의는 여러 사람이 책을 읽고 각각 중요한 섹션을 강조하는 것처럼, 결합하면 더 풍부한 이해를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n어떤 LLM이든 어떤 종류의 규칙을 학습하려면, 우리가 텍스트로 인식하는 것을 기계가 해석할 수 있는 형태로 변환해야 합니다. 이 과정은 임베딩/벡터화를 통해 이루어집니다. 이러한 과정의 출력물은 단어, 문장 또는 토큰들의 수학적(벡터) 표현인 임베딩입니다. 임베딩을 통해 우리는 단어, 문장을 표현하고, 벡터 거리 계산을 통해 다른 단어와의 의미론적 의미와 관계를 파악할 수 있습니다. 임베딩에는 몇 가지 유형이 있습니다: 위치적(토큰의 위치 인코딩), 토큰 임베딩(의미론), 혼합형 임베딩:\n\n- 위치 임베딩: 문장 내 토큰의 위치를 인코딩하여 모델이 단어의 순서를 이해하는 데 도움을 줍니다.\n- 토큰 임베딩: 개별 단어나 토큰의 의미를 표현합니다.\n- 혼합형 임베딩: 위치적과 토큰 임베딩을 결합하여 입력의 포괄적인 표현을 제공합니다.\n\n![이미지](/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_6.png)\n\n지금까지 우리는 Transformers, 인코더, 디코더, 인코더-디코더, 어텐션, 그리고 임베딩에 대한 기본적이고 고수준의 내용을 다뤘습니다. 그러나 LLM 사용의 가장 중요한 부분은 RLHF(인간 피드백으로부터 강화 학습)이라는 기술입니다.\n\n<div class=\"content-ad\"></div>\n\n이 기술은 언어 모델을 사용자의 의도에 맞추는 데 사용됩니다. 이것은 강화 학습(Reinforcement Learning, RL)을 통해 이루어집니다. RLHF는 사전 학습된 LLM을 정렬하는 매커니즘으로, 인간 피드백을 활용하여 성능을 향상시키는 것으로 매우 인기가 있습니다. 이것은 LLM이 상대적으로 적은 양의 인간 피드백에서 배워 행동을 수정할 수 있도록 합니다. RLHF는 GPT-3.5, GPT-4와 같은 현대적인 LLM에서 상당한 개선을 보여주었으며, 심지어 ChatGPT 제품에서도 사용됩니다.\n\n이것은 LLM과 트랜스포머에 대한 고수준 정보였습니다. 저는 트랜스포머 뒤의 세부사항과 수학을 설명하는 별도의 게시물을 가지고 있습니다. 그러나 일단은 LLM이 무엇이며 어떻게 작동하는지 명확하게 알아두는 것으로 충분합니다.\n\nLLM을 실제로 활용할 수 있는 분야는 다음과 같습니다:\n\n- 고전적인 NLP(텍스트/단어/문장 분류)\n- 한 언어에서 다른 언어로 번역\n- 코드/SQL/리뷰/간단한 텍스트 생성\n- 정보 검색\n- 의미론적 검색\n- 챗봇\n\n<div class=\"content-ad\"></div>\n\n요약하자면, Transformer 아키텍처는 인코더, 디코더, 고급 어텐션 메커니즘 및 임베딩을 통해 LLMs에 강력한 기반을 제공합니다. RLHF와 같은 기술들은 이러한 모델을 더욱 강화시켜 다양한 NLP 작업에 효과적으로 활용할 수 있게 만들어 줍니다.","ogImage":{"url":"/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_0.png"},"coverImage":"/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_0.png","tag":["Tech"],"readingTime":11}],"page":"24","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true}