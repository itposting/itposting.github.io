{"pageProps":{"posts":[{"title":"자연어 처리NLP 입문 기본 개념과 활용 방법","description":"","date":"2024-06-23 20:04","slug":"2024-06-23-IntroductiontoNaturalLanguageProcessingNLP","content":"\n\n![image](/assets/img/2024-06-23-IntroductiontoNaturalLanguageProcessingNLP_0.png)\n\n# 자연 언어 처리(NLP) 및 응용 프로그램 개요\n\n자연 언어 처리(NLP)는 인공 지능(AI)의 하위 분야로, 컴퓨터와 인간 간의 자연 언어를 통한 상호 작용에 중점을 둡니다. 이는 자연 언어와 음성을 분석하고 합성하기 위해 계산 기술을 적용하는 것을 의미합니다. NLP는 컴퓨터 과학, 언어학 및 기계 학습을 결합하여 컴퓨터가 인간의 언어를 이해하고 해석하며 생산할 수 있도록 합니다.\n\n# NLP의 응용 프로그램\n\n<div class=\"content-ad\"></div>\n\nNLP는 다음과 같은 다양한 응용 프로그램을 포함하고 있습니다:\n\n- 텍스트 분류: 스팸 메일 감지, 감성 분석 및 주제 분류와 같이 미리 정의된 범주로 텍스트를 자동으로 분류합니다.\n- 명명된 엔티티 인식 (NER): 텍스트에서 사람, 조직, 위치, 날짜 등과 같은 엔티티를 식별하고 분류합니다.\n- 기계 번역: 한 언어에서 다른 언어로 텍스트를 번역합니다. 구글 번역과 같은 서비스가 여기에 해당합니다.\n- 감성 분석: 소셜 미디어 게시물에서 표현된 감정을 판별합니다. 긍정적, 부정적 또는 중립적인 감정을 분석합니다.\n- 텍스트 요약: 긴 텍스트의 간결한 요약을 자동으로 생성합니다.\n- 질의 응답: 자연 언어로 제기된 질문에 답변할 수 있는 시스템을 구축합니다.\n\n<img src=\"/assets/img/2024-06-23-IntroductiontoNaturalLanguageProcessingNLP_1.png\" />\n\n# 기본 NLP 작업\n\n<div class=\"content-ad\"></div>\n\n텍스트 분류 모델 구축에 들어가기 전에 몇 가지 기본 NLP 작업을 이해하는 것이 중요합니다: 토큰화, 어간 추출 및 표제어 추출.\n\n# 토큰화\n\n토큰화는 텍스트를 개별 단어 또는 토큰으로 분해하는 과정입니다. 이는 텍스트 처리의 첫 번째 단계입니다.\n\n```python\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\n\ntext = \"Natural Language Processing is fascinating.\"\ntokens = word_tokenize(text)\nprint(tokens)\n```\n\n<div class=\"content-ad\"></div>\n\n# 어간 추출\n\n어간 추출은 단어를 그 뿌리 형태로 줄입니다. 예를들어, \"running\"은 \"run\"이 됩니다.\n\n```js\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nwords = [\"running\", \"ran\", \"runs\"]\nstemmed_words = [stemmer.stem(word) for word in words]\nprint(stemmed_words)\n```\n\n# 표제어 추출\n\n<div class=\"content-ad\"></div>\n\n표 태그를 마크다운 형식으로 변경해주세요.\n\n`js`\n```python\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nlemmatizer = WordNetLemmatizer()\nwords = [\"running\", \"ran\", \"runs\"]\nlemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\nprint(lemmatized_words)\n```\n\n## 간단한 텍스트 분류 모델 구축\n\n이 섹션에서는 파이썬을 사용하여 간단한 텍스트 분류 모델을 구축할 것입니다. 머신러닝 모델을 사용하여 영화 리뷰를 긍정적 또는 부정적으로 분류할 것입니다. 이를 위해 sklearn 라이브러리를 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 단계 1: 라이브러리 및 데이터셋 불러오기\n\n먼저, 필요한 라이브러리를 가져와서 데이터셋을 로드해보겠습니다. 영화 리뷰 데이터셋을 다운로드하기 위해 nltk 라이브러리를 사용할 것입니다.\n\n```python\nimport nltk\nfrom sklearn.datasets import load_files\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# 영화 리뷰 데이터셋 다운로드\nnltk.download('movie_reviews')\nfrom nltk.corpus import movie_reviews\n# 데이터셋 로드\ndocuments = [(movie_reviews.raw(fileid), category)\n             for category in movie_reviews.categories()\n             for fileid in movie_reviews.fileids(category)]\n```\n\n# 단계 2: 데이터셋 준비하기\n\n<div class=\"content-ad\"></div>\n\n데이터 세트를 학습 및 테스트 세트로 분할해야 합니다.\n\n```js\n# 데이터 세트를 텍스트와 레이블로 분할합니다\n텍스트, 레이블 = zip(*문서)\n\n# 학습 및 테스트 세트로 분할합니다\nX_학습, X_테스트, y_학습, y_테스트 = train_test_split(텍스트, 레이블, test_size=0.2, random_state=42)\n```\n\n# 단계 3: 텍스트 전처리\n\n텍스트 데이터를 숫자 벡터로 변환하기 위해 TfidfVectorizer를 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 텍스트 데이터를 TF-IDF 특성으로 변환합니다\nvectorizer = TfidfVectorizer(stop_words='english')\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n```\n\n# 단계 4: 모델 훈련\n\n텍스트 분류 모델을 훈련하는 데 Naive Bayes 알고리즘을 사용할 것입니다.\n\n```js\n# 나이브 베이즈 분류기를 훈련합니다\nclassifier = MultinomialNB()\nclassifier.fit(X_train_tfidf, y_train)\n```\n\n<div class=\"content-ad\"></div>\n\n# 단계 5: 모델 평가하기\n\n이제 테스트 데이터에서 모델을 평가해 봅시다.\n\n```js\n# 테스트 세트에 대한 레이블 예측\ny_pred = classifier.predict(X_test_tfidf)\n\n# 정확도 계산\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'정확도: {accuracy * 100:.2f}%')\n# 분류 보고서 출력\nprint(classification_report(y_test, y_pred))\n```\n\n이 블로그에서 소개된 단계를 따르면, 이제 자연어 처리에 대한 기본적인 이해력과 기본적인 텍스트 분류 모델을 구축할 수 있는 능력이 생겼을 것입니다. 이것은 시작에 불과합니다. 자연어 처리 분야에서 더 많은 고급 기술과 모델을 탐험할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이러한 기술과 도구를 활용해 더 정교한 NLP 애플리케이션을 구축해보세요. 즐거운 코딩 되세요!","ogImage":{"url":"/assets/img/2024-06-23-IntroductiontoNaturalLanguageProcessingNLP_0.png"},"coverImage":"/assets/img/2024-06-23-IntroductiontoNaturalLanguageProcessingNLP_0.png","tag":["Tech"],"readingTime":4},{"title":"트랜스포머 쉽게 이해하기 Part 3 멀티-헤드 어텐션 심층 분석","description":"","date":"2024-06-23 20:02","slug":"2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive","content":"\n\n## 직관적인 트랜스포머 시리즈 NLP\n\n이것은 트랜스포머에 관한 제 시리즈에서 세 번째 글입니다. 우리는 위에서 아래로의 방식으로 그 기능을 다루고 있습니다. 이전 글에서는 트랜스포머가 무엇인지, 그 구조, 그리고 어떻게 작동하는지 배웠습니다.\n\n이번 글에서는 한 발 더 나아가 Multi-head Attention에 대해 더 자세히 파고들 것입니다. 이것이 트랜스포머의 두뇌라고 할 수 있습니다.\n\n시리즈에서 이전 및 다음 글에 대한 간단한 요약입니다. 내 목표는 무엇이 어떻게 작동하는지만 아는 것이 아니라, 왜 그런 방식으로 작동하는지 이해하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n- 기능 개요 (Transformer의 사용 방법 및 RNN보다 우수한 점. 아키텍처 구성 요소 및 훈련 및 추론 중의 동작)\r\n- 작동 방식 (내부 운영 end-to-end. 데이터 흐름 및 수행되는 계산, 행렬 표현 등)\r\n- Multi-head Attention - 이 기사 (Transformer 전체에서 Attention 모듈의 작동 방식)\r\n- 왜 Attention이 성능을 향상시키는가 (Attention이 하는 일뿐만 아니라 왜 잘 작동하는지. Attention이 문장 내 단어 간의 관계를 어떻게 파악하는지)\r\n\r\n그리고 자연어 처리 응용 프로그램에 관심이 있다면, 좋아할만한 기사가 몇 개 더 있습니다.\r\n\r\n- Beam Search (음성인식 및 자연어 처리 응용프로그램에서 일반적으로 사용되는 알고리즘으로 예측을 향상하는 방법)\r\n- Bleu Score (Bleu Score 및 Word Error Rate는 자연어 처리 모델의 두 가지 필수적인 메트릭스입니다)\r\n\r\n# Transformer에서 어떻게 Attention이 사용되는가\n\n<div class=\"content-ad\"></div>\n\nPart 2에서 논의한 대로, 어텐션은 Transformer에서 세 곳에서 사용됩니다:\n\n- 인코더의 셀프 어텐션 - 입력 시퀀스가 자신에게 주의를 기울임\n- 디코더의 셀프 어텐션 - 대상 시퀀스가 자신에게 주의를 기울임\n- 디코더의 인코더-디코더 어텐션 - 대상 시퀀스가 입력 시퀀스에 주의를 기울임\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_0.png)\n\n어텐션 입력 매개변수 - 쿼리(Query), 키(Key), 값(Value)\n\n<div class=\"content-ad\"></div>\n\n주의 계층은 Query, Key 및 Value로 알려진 세 매개변수 형식으로 입력을 받습니다.\n\n세 매개변수는 각 단어가 벡터로 표현된 시퀀스와 유사한 구조를 갖습니다.\n\n인코더 셀프 어텐션\n\n입력 시퀀스는 입력 임베딩 및 위치 인코딩에 공급되어 각 입력 시퀀스의 각 단어에 대한 의미와 위치를 캡처하는 인코딩 표현을 생성합니다. 이것은 모두 Query, Key 및 Value 매개변수에 공급되며, 첫 번째 인코더의 셀프 어텐션에서 각 입력 시퀀스의 각 단어에 대한 인코딩 표현을 생성합니다. 이제 각 단어에 대한 어텐션 점수도 포함된 새 인코딩 표현입니다. 이것이 스택 내의 모든 인코더를 통과할 때마다 각 셀프 어텐션 모듈은 각 단어의 표현에 자체 어텐션 점수를 추가합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_1.png)\n\n디코더 셀프 어텐션\n\n디코더 스택으로 오는 경우, 목표 시퀀스가 출력 임베딩과 위치 인코딩으로 전달됩니다. 이는 목표 시퀀스의 각 단어에 대한 의미와 위치를 캡처한 인코딩 표현을 생성합니다. 이것은 첫 번째 디코더의 Self-Attention에서 모든 세 가지 매개변수인 Query, Key, Value로 전달되며, 목표 시퀀스의 각 단어에 대한 인코딩 표현을 생성하고 각 단어의 주의 점수를 반영합니다.\n\n레이어 정규화를 거친 후, 이것은 첫 번째 디코더의 인코더-디코더 어텐션에서 Query 매개변수로 전달됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n에 대한 관심\n\n이에 더불어 스택 내 최종 인코더의 출력은 Encoder-Decoder Attention의 Value 및 Key 매개변수로 전달됩니다.\n\n따라서 Encoder-Decoder Attention은 디코더 Self-Attention의 대상 시퀀스와 인코더 스택의 입력 시퀀스의 표현을 받게 됩니다. 이로써 입력 시퀀스의 관심 점수의 영향을 포착하는 대상 시퀀스 단어마다 관심 점수를 포함하는 표현을 생성합니다.\n\n이것은 스택 내 모든 디코더를 통해 전달되므로, 각 Self-Attention 및 각 Encoder-Decoder Attention은 각 단어의 표현에 고유한 관심 점수를 추가합니다.\n\n<div class=\"content-ad\"></div>\n\n# 다중 어텐션 헤드\n\nTransformer에서 어텐션 모듈은 병렬로 동시에 여러 번 계산을 반복합니다. 각각을 어텐션 헤드라고 합니다. 어텐션 모듈은 쿼리(Query), 키(Key), 값(Value) 매개변수를 N개로 분할하고 각 분할을 별도로 헤드를 통해 전달합니다. 모든 이 유사한 어텐션 계산은 그런 다음 합쳐져 최종 어텐션 점수를 생성합니다. 이를 다중 헤드 어텐션이라고 하며, 이를 통해 Transformer는 단어마다 여러 관계와 뉘앙스를 인코딩하는 강력한 능력을 갖게 됩니다.\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_2.png)\n\n내부에서 데이터가 어떻게 처리되는지 정확히 이해하기 위해 Transformer를 훈련하여 번역 문제를 해결하는 과정에서 어텐션 모듈의 작동 방식을 살펴보겠습니다. 영어로 된 입력 시퀀스('You are welcome')와 스페인어로 된 대상 시퀀스('De nada')로 구성된 훈련 데이터의 샘플을 사용하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 주목해야 할 하이퍼파라미터\n\n데이터 차원을 결정하는 세 가지 하이퍼파라미터가 있습니다:\n\n- 임베딩 크기 — 임베딩 벡터의 너비입니다 (예시에서는 너비 6을 사용합니다). 이 차원은 Transformer 모델 전반에서 전달되며 때로는 '모델 크기'와 같은 다른 이름으로 불리기도 합니다.\n- 쿼리 크기 (키 및 값 크기에 동일) — 쿼리, 키 및 값 행렬을 만들기 위해 세 개의 선형 레이어에서 사용되는 가중치의 크기입니다 (예시에서는 쿼리 크기를 3으로 사용합니다).\n- 어텐션 헤드의 수 (예시에서는 2개의 헤드를 사용합니다)\n\n또한 샘플의 수를 나타내는 배치 크기가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 입력 레이어\n\n입력 임베딩 및 위치 인코딩 레이어는 (샘플 수, 시퀀스 길이, 임베딩 크기) 형태의 행렬을 생성하여 스택 내 첫 번째 인코더의 쿼리, 키 및 값에 공급됩니다.\n\n![](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_3.png)\n\n시각화를 간단하게 만들기 위해 이미지에서 배치 차원을 제거하고 나머지 차원에 초점을 맞추겠습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_4.png)\n\n## 선형 레이어\n\n쿼리(Query), 키(Key), 값(Value)에 대한 세 개의 별개의 선형 레이어가 있습니다. 각 선형 레이어는 고유한 가중치를 가지고 있습니다. 입력은 이러한 선형 레이어를 거쳐 Q, K, V 행렬을 생성합니다.\n\n## 어텐션 헤드별 데이터 분할\n\n\n<div class=\"content-ad\"></div>\n\n이제 데이터가 여러 개의 Attention head로 분할되어 각각 독립적으로 처리됩니다.\n\n그러나 이해해야 할 중요한 점은 이것이 논리적인 분할임을 주의해야 합니다. Query, Key 및 Value는 물리적으로 분리된 행렬로 분할되지 않습니다. Query, Key 및 Value에 대해 각 Attention head마다 하나의 데이터 행렬이 사용되며, 각 Attention head의 논리적으로 분리된 섹션이 행렬에 있습니다. 마찬가지로 Attention head마다 별도의 Linear layer가 없습니다. 모든 Attention head가 동일한 Linear layer를 공유하지만 각자의 논리적인 데이터 행렬 섹션에 작용합니다.\n\nLinear layer의 가중치는 head별로 논리적으로 분할됩니다.\n\n이 논리적인 분할은 입력 데이터와 Linear layer의 가중치를 Attention head별로 균등하게 분할함으로써 이뤄집니다. 위와 같이 Query Size를 선택함으로써 이를 달성할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n\nQuery Size = Embedding Size / Number of heads\n\n![Image 1](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_5.png)\n\nIn our example, that is why the Query Size = 6/2 = 3. Even though the layer weight (and input data) is a single matrix we can think of it as ‘stacking together’ the separate layer weights for each head.\n\n![Image 2](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_6.png)\n\n\n<div class=\"content-ad\"></div>\n\n모든 헤드에 대한 계산은 N 개의 별도 연산이 필요한 대신 단일 행렬 연산을 통해 달성할 수 있습니다. 이렇게 하면 계산이 효율적으로 이루어지고 모델이 단순해지며 선형 레이어가 더 적게 필요하지만 독립적인 어텐션 헤드의 힘을 여전히 발휘할 수 있습니다.\n\nQ, K 및 V 행렬 재구성\n\n선형 레이어에서 출력된 Q, K 및 V 행렬을 명시적인 헤드 차원이 포함된 모양으로 재구성합니다. 이제 각 '슬라이스'가 하나의 헤드당 하나의 행렬에 해당합니다.\n\n이 행렬은 다시 헤드와 시퀀스 차원을 교체하여 재구성됩니다. 배치 차원은 그림에 표시되지 않았지만, Q의 차원은 이제 (Batch, Head, Sequence, Query size)입니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_7.png)\n\n아래 그림에서는 Linear 레이어에서 나온 예제 Q 매트릭스를 분할하는 전체 과정을 확인할 수 있습니다.\n\n최종 단계는 시각화용입니다 — Q 매트릭스는 단일 매트릭스이지만, 논리적으로 독립된 각 head마다 별도의 Q 매트릭스로 생각할 수 있습니다.\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_8.png)\n\n<div class=\"content-ad\"></div>\n\n우리는 어텐션 점수를 계산할 준비가 되어 있어요.\n\n# 각 헤드에 대한 어텐션 점수 계산\n\n이제 우리는 헤드 간에 분할된 3개의 행렬인 Q, K 및 V를 소유하고 있어요. 이들은 어텐션 점수를 계산하는 데 사용돼요.\n\n우리는 하나의 헤드에 대한 연산을 보여줄 거에요. 이때는 마지막 두 차원(시퀀스 및 쿼리 크기)만 사용하고 처음 두 차원(배치 및 헤드)은 건너뛸 거에요. 본질적으로, 우리는 보고 있는 연산이 각 헤드와 각 배치 샘플마다 \"반복\"되는 것으로 상상할 수 있어요 (물론 실제로는 반복문이 아니라 단일 행렬 작업으로 이루어지고 있어요).\n\n<div class=\"content-ad\"></div>\n\n첫 번째 단계는 Q와 K 사이의 행렬 곱셈을 수행하는 것입니다.\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_9.png)\n\n결과에 Mask 값이 추가됩니다. 인코더 셀프 어텐션에서는 패딩 값을 마스킹하여 주의 점수에 참여하지 않도록합니다.\n\n다른 마스크가 디코더 셀프 어텐션 및 디코더 인코더 어텐션에서 적용되며, 이에 대해서는 흐름에서 조금 뒤에 다룹니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Transformers Explained Visually Part 3](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_10.png)\n\nThe result is now scaled by dividing by the square root of the Query size, and then a Softmax is applied to it.\n\n![Transformers Explained Visually Part 3](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_11.png)\n\nAnother matrix multiplication is performed between the output of the Softmax and the V matrix.\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_12.png\" />\n\n인코더 Self-attention의 완전한 어텐션 점수 계산은 다음과 같습니다:\n\n<img src=\"/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_13.png\" />\n\n# 각 헤드의 어텐션 점수를 병합합니다\n\n<div class=\"content-ad\"></div>\n\n각 헤드에 대해 별도의 주의 점수가 있습니다. 이것들을 한 가지 점수로 결합해야 합니다. 이 병합 작업은 본질적으로 분할 작업의 반대입니다.\n\n헤드 차원을 제거하여 결과 행렬을 단순히 재구성함으로써 수행됩니다. 이 과정은 다음과 같습니다:\n\n- Attention Score 행렬을 재구성하여 헤드 및 시퀀스 차원을 교환합니다. 다시 말해, 행렬 모양이 (배치, 헤드, 시퀀스, 쿼리 크기)에서 (배치, 시퀀스, 헤드, 쿼리 크기)로 변경됩니다.\n- 헤드 차원을 축소하여 (배치, 시퀀스, 헤드 * 쿼리 크기)로 재구성합니다. 이를 통해 각 헤드의 주의 점수 벡터를 단일 병합된 주의 점수로 연결합니다.\n\n임베딩 크기 = 헤드 * 쿼리 크기이므로 병합된 점수는 (배치, 시퀀스, 임베딩 크기)입니다. 아래 그림에서는 예제 점수 행렬에 대한 병합 과정을 자세히 볼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_14.png\" />\n\n# End-to-end Multi-head Attention\n\nPutting it all together, this is the end-to-end flow of the Multi-head Attention.\n\n<img src=\"/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_15.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n# 다중 헤드 분할은 더 풍부한 해석을 제공합니다\n\n임베딩 벡터는 단어의 의미를 포착합니다. 다중 헤드 어텐션의 경우, 입력(및 대상) 시퀀스의 임베딩 벡터가 여러 헤드에 걸쳐 논리적으로 분할된다는 것을 보았습니다. 이것의 의미는 무엇인가요?\n\n![image](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_16.png)\n\n이것은 임베딩의 별도 섹션들이 시퀀스 내 다른 단어와의 관련성에 따라 각 단어의 의미의 다른 측면을 학습할 수 있다는 것을 의미합니다. 이를 통해 트랜스포머는 시퀀스의 보다 풍부한 해석을 제공할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이것은 현실적인 예시가 아닐 수도 있지만 직관을 키우는 데 도움이 될 수 있습니다. 예를 들어, 하나의 섹션은 명사의 ‘성별’(남성, 여성, 중성)을 포함할 수 있고, 다른 하나는 명사의 ‘수’(단수 대 복수)를 포함할 수 있습니다. 이는 번역 중에 중요할 수 있는데, 많은 언어에서 사용해야 하는 동사가 이러한 요인에 따라 달라질 수 있습니다.\n\n# 디코더 셀프 어텐션 및 마스킹\n\n디코더 셀프 어텐션은 인코더 셀프 어텐션과 마찬가지로 작동하지만 대상 시퀀스의 각 단어에 대해 작동합니다.\n\n![이미지](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_17.png)\n\n<div class=\"content-ad\"></div>\n\n같은 방법으로, Masking은 대상 시퀀스에서 패딩 단어를 가려냅니다.\n\n# 디코더 인코더-디코더 어텐션 및 마스킹\n\n디코더 인코더-디코더 어텐션은 두 개의 소스에서 입력을 받습니다. 따라서, 각 입력 단어 간 상호 작용을 계산하는 인코더 자가 어텐션과 달리, 각 대상 단어 간 상호 작용을 계산하는 디코더 자가 어텐션은 각 대상 단어와 각 입력 단어 간의 상호 작용을 계산합니다.\n\n![image](/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_18.png)\n\n<div class=\"content-ad\"></div>\n\n따라서 결과 Attention Score의 각 셀은 하나의 Q (즉, 대상 시퀀스 단어)와 모든 다른 K (즉, 입력 시퀀스) 단어 및 모든 V (즉, 입력 시퀀스) 단어 간 상호 작용을 나타냅니다.\n\n이와 유사하게, Masking은 대상 출력의 후속 단어를 마스킹하여 시리즈의 두 번째 기사에서 자세히 설명했던 대로 수행됩니다.\n\n# 결론\n\n이를 통해 트랜스포머의 어텐션 모듈이 무엇을 하는지에 대한 좋은 감을 제공했기를 바랍니다. 두 번째 기사에서 확인한 트랜스포머의 end-to-end 흐름과 함께 조합하면 이제 전체 트랜스포머 아키텍처의 상세 운영을 다루었습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 우리는 Transformer가 정확히 무엇을 하는지 이해했습니다. 그러나 Transformer의 Attention이 왜 그런 계산을 수행하는지에 대한 질문에 완전히 대답하지 않았습니다. 왜 Query, Key 및 Value 개념을 사용하고, 방금 본 것처럼 왜 행렬 곱셈을 수행하는 걸까요?\n\n우리는 '각 단어 간의 관계를 포착한다'는 희미한 직관을 가지고 있지만, 이게 정확히 무엇을 의미하는지는 무엇일까요? Transformer의 Attention이 앞의 각 단어의 미묘한 차이를 이해하는 능력을 가지도록 하는 방법은 무엇일까요?\n\n그것은 흥미로운 질문이며이 시리즈의 마지막 글에 대한 주제입니다. 그것을 이해하면 우리는 진정으로 Transformer 아키텍처의 우아함을 이해할 것입니다.\n\n마지막으로, 이 기사를 좋아하셨다면, 오디오 딥 러닝, 지리 위치 기계 학습 및 이미지 캡션 아키텍처에 대한 다른 시리즈도 즐기실 수 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n계속해서 배워봐요!","ogImage":{"url":"/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_0.png"},"coverImage":"/assets/img/2024-06-23-TransformersExplainedVisuallyPart3Multi-headAttentiondeepdive_0.png","tag":["Tech"],"readingTime":11},{"title":"SageMaker 비동기 추론으로 대형 언어 모델 배포하는 방법","description":"","date":"2024-06-23 19:59","slug":"2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference","content":"\n\n<img src=\"/assets/img/2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference_0.png\" />\n\nLLM(대규모 언어 모델)은 인기를 얻고 있으며 이를 추측하는 방법도 늘어가고 있습니다. LLM 호스팅에는 모델의 크기와 배포된 하드웨어의 최적 사용을 보장해야 한다는 어려움이 잘 알려져 있습니다. LLM 사용 사례도 다양합니다. 어떤 것은 실시간 응답 시간이 필요할 수 있고, 다른 것은 거의 실시간 기반의 지연 시간 요구 사항일 수 있습니다.\n\n후자와 더 많은 오프라인 추측 사용 사례를 위해, SageMaker 비동기 추론이 좋은 옵션으로 제공됩니다. 비동기 추론에서는 이름에서 알 수 있듯이 지연 시간이 굉장히 엄격하지 않지만 필요에 따라 호출하고 확장할 수 있는 활성화된 엔드포인트가 필요합니다. 특히 LLM에서 이러한 유형의 작업 부하는 내용 편집/생성, 요약 등과 같은 사용 사례로 인해 점점 인기를 얻고 있습니다. 이러한 작업 부하들은 하위 초 단위 응답이 필요하지는 않지만 필요에 따라 호출할 수 있는 적시의 추론을 필요로 하며, SageMaker Batch Transform과 같은 완전히 오프라인적인 성격과는 대조적입니다.\n\n이 예시에서는 HuggingFace 텍스트 생성 추론 서버를 SageMaker 비동기 엔드포인트와 함께 사용하여 Flan-T-5-XXL 모델을 호스팅하는 방법을 살펴볼 것입니다.\n\n<div class=\"content-ad\"></div>\n\n참고: 본 글은 Python, LLMs 및 Amazon SageMaker에 대한 기본적인 이해를 전제로 합니다. Amazon SageMaker 추론을 시작하려면 다음 가이드를 참조해주세요. SageMaker 비동기 추론의 기초를 다룰 것이지만 더 깊은 소개를 원하시면 다음에 나오는 스타터 예제를 참조해주세요.\n\n공지: 저는 AWS의 머신 러닝 아키텍트이며, 이견은 제 개인적인 의견입니다.\n\n# 목차\n\n- SageMaker 비동기 추론을 사용하는 시점\n- TGI 비동기 추론 구현\n   a. 설정 및 엔드포인트 배포\n   b. 비동기 추론 호출\n   c. 자동 스케일링 설정\n- 추가 자료 및 결론\n\n<div class=\"content-ad\"></div>\n\n# 1. SageMaker 비동기 추론 사용 시점\n\nSageMaker 추론은 현재 사용 사례에 따라 활용할 수 있는 네 가지 옵션이 있습니다. 세 가지 엔드포인트 기반 옵션이 있고 완전 오프라인 추론을 위한 한 가지 옵션이 있습니다:\n\n- 엔드포인트 기반 옵션:\n    - SageMaker 실시간 추론: 서브초/밀리초 응답 시간과 고 처리량 워크로드를 위한 옵션입니다. 이 엔드포인트는 CPU, GPU 또는 Inferentia 칩을 활용하며 하드웨어 단계에서 AutoScaling을 적용하여 인프라를 확장할 수 있습니다. 일반적인 사용 사례로는 Ad-Tech 기반 예측, 실시간 챗봇 등이 있습니다.\n    - SageMaker 서버리스 추론: 갑작스럽고 간헐적인 워크로드에 최적화되어 있으며 cold-start를 허용할 수 있는 옵션입니다 (Provisioned Concurrency를 통해 완화할 수 있음). 여기서는 엔드포인트 뒤에 있는 모든 인프라를 관리하지 않으며 확장은 자동으로 처리됩니다.\n    - SageMaker 비동기 추론: 오늘 다룰 옵션으로, 비동기 추론을 통해 거의 실시간 기반의 응답 시간 요구 사항을 충족하고 여전히 엔드포인트를 위해 정의한 전용 하드웨어를 사용합니다. 그러나 비동기 추론의 경우 실시간 추론과 달리 0 개의 인스턴스로 축소할 수 있는 옵션이 있습니다. 비동기 추론을 통해 내장 큐를 사용하여 요청을 관리하고이 큐의 가득 찬 정도에 따라 확장할 수 있습니다.\n\n- 오프라인 추론:\n    - SageMaker 배치 변환: 데이터셋이 있고 데이터셋으로 반환된 출력만 필요할 때 최적입니다. 영구적인 엔드포인트는 없으며 완전히 오프라인 추론입니다. 일반적인 사용 사례로는 특정 주기에 추론이 필요한 데이터셋이 있는 경우 일정 시간에 배치 변환 작업을 실행하는 것이 있습니다.\n\n이 사용 사례에서는 특히 비동기 추론에 초점을 맞추고 있습니다. 이 옵션은 거의 실시간 능력과 0 인스턴스로 축소할 수 있는 능력 때문에 Batch Transform과 Real-Time Inference 사이에서 결혼 생각할 수 있는 옵션입니다. 즉시 생성이 필요하지 않은 사용 사례를 위한 LLM을 호스팅하는 데 효율적인 방법으로 기능을 제공할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이러한 사용 사례의 예시로는 요약, 콘텐츠 생성, 편집 등이 있습니다. 이러한 사용 사례는 모두 가변 시간에 activation이 필요할 수 있으므로 지속적인 엔드포인트가 필요하지만 실시간 추론의 응답 시간은 필요로 하지 않을 수도 있습니다. 비동기 추론을 통해 성능과 비용 측면에서 이러한 종류의 사용 사례들을 다룰 수 있습니다.\n\n오늘의 예시로, 우리는 인기 있는 Flan 모델을 SageMaker 비동기 추론에 적용해 보겠습니다. SageMaker 비동기 추론 엔드포인트를 생성하는 것은 실시간 엔드포인트 생성과 매우 유사합니다. 주요 차이점은 실시간 추론처럼 페이로드를 직접 전달하는 대신 입력 데이터에 대한 S3 경로가 필요하다는 점입니다.\n\n![이미지](/assets/img/2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference_1.png)\n\n비동기 엔드포인트 내에는 내부 대기열도 있음을 유의해야 합니다. 모든 추론마다 SageMaker는 요청을 대기열에 넣고 결과 위치를 S3에 반환합니다. 비동기 엔드포인트에 대해 AutoScaling을 구성할 때 이 대기열 내의 요청 수에 따라 스케일을 조정할 수 있습니다. 또한 출력 S3 경로에서 직접 폴링하는 대신 성공 또는 오류 있는 추론 알림을 수신하기 위해 선택적으로 SNS 토픽을 통합할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 비동기 추론에 대한 이해가 조금 더 깊어졌으니 구현부로 넘어가 봅시다!\n\n# 2. TGI 비동기 추론 구현\n\n우리는 새로운 SageMaker Studio 환경에서 Base Python3 커널과 ml.c5.xlarge 인스턴스로 작업할 것입니다. 비동기 추론을 위해 익숙한 Boto3 AWS Python SDK 및 상위 레벨 SageMaker Python SDK를 사용할 것입니다.\n\n## a. 설정 및 엔드포인트 배포\n\n<div class=\"content-ad\"></div>\n\n비동기 추론을 사용하려면 먼저 데이터가 저장될 출력 S3 경로를 정의해야 합니다.\n\n```js\nsagemaker_session = sagemaker.Session()\ndefault_bucket = sagemaker_session.default_bucket()\nbucket_prefix = \"async-llm-output\"\nasync_output_path = f\"s3://{default_bucket}/{bucket_prefix}/output\"\nprint(f\"내 모델 추론 결과는 다음 S3 경로에 저장될 것입니다: {async_output_path}\")\n```\n\n그런 다음 이 S3 경로를 사용하여 비동기 추론 구성을 지정할 수 있습니다. 이 경우 SNS 주제를 지정하지 않지만, 성공적이거나 오류가 발생했을 때 서비스를 통해 알림을받기를 원하는 경우 선택적으로 포함할 수 있습니다.\n\n```js\nfrom sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n\nasync_config = AsyncInferenceConfig(\n    output_path=async_output_path,\n    max_concurrent_invocations_per_instance=10,\n    # 선택적으로 Amazon SNS 주제 지정\n    # notification_config = {\n    # \"SuccessTopic\": \"arn:aws:sns:<aws-region>:<account-id>:<topic-name>\",\n    # \"ErrorTopic\": \"arn:aws:sns:<aws-region>:<account-id>:<topic-name>\",\n    # }\n)\n```\n\n<div class=\"content-ad\"></div>\n\n이 정의된 후에는 Flan T-5-XXL 모델을 위한 HuggingFace Hub 링크에서 SageMaker 코드를 직접 가져올 수 있습니다. 이 코드는 Text Generation Inference 모델 서버를 활용하며, Tensor Parallelism과 같은 내장 최적화가 포함되어 있습니다.\n\n```js\n# huggingface hub에서 배포 코드 직접 가져와서 비동기 구성 추가\nhub = {\n   'HF_MODEL_ID':'google/flan-t5-xxl',\n   'SM_NUM_GPUS': json.dumps(4)\n}\n\nhuggingface_model = HuggingFaceModel(\n   image_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"),\n   env=hub,\n   role=role, \n)\r\n```\n\n그런 다음 SageMaker 모델 객체와 비동기 구성을 함께 사용하여 엔드포인트를 생성할 수 있습니다.\n\n```js\n# SageMaker 추론을 위해 모델 배포\npredictor = huggingface_model.deploy(\n initial_instance_count=1,\n instance_type=\"ml.g5.12xlarge\",\n container_startup_health_check_timeout=300,\n async_inference_config=async_config\n)\r\n```\n\n<div class=\"content-ad\"></div>\n\n콘솔 또는 UI에서 엔드포인트가 비동기 타입으로 지정되었음을 확인할 수 있습니다.\n\n![image](/assets/img/2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference_2.png)\n\n## b. 비동기 추론 호출\n\n단일 페이로드로 엔드포인트를 호출하려면 실시간 추론과 같이 \"predict\" 내장 메소드를 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\n# 단일 호출\n\npayload = \"미국의 수도는 어디인가요?\"\ninput_data = {\n    \"inputs\": payload,\n    \"parameters\": {\n        \"early_stopping\": True,\n        \"length_penalty\": 2.0,\n        \"max_new_tokens\": 50,\n        \"temperature\": 1,\n        \"min_length\": 10,\n        \"no_repeat_ngram_size\": 3,\n        },\n}\npredictor.predict(input_data)\r\n```\n\n그러나 현실적인 사용 사례에 대한 비동기 추론을 확장하려면 S3에서 데이터로 엔드포인트를 호출할 수 있습니다. 비동기 추론의 아이디어는 여러 요청이 입력 S3 버킷에 저장되어 있고 호출은 각 데이터 포인트에 대한 결과와 해당하는 S3 출력 파일을 반환한다는 것입니다. 여기서 한 가지 더 강조하고 싶은 것은 전체 데이터 집합이 처리되고 요청에 의해 호출할 엔드포인트가 없는 Batch Transform과 다르다는 점입니다.\n\n이제 우리는 이 데모를 위해 동일한 데이터포인트를 가진 가짜 데이터 집합을 만들어 S3로 옮기겠습니다. 다음 코드는 입력 파일을 포함하는 로컬 디렉터리를 생성합니다:\n\n```py\nimport json\nimport os\n\noutput_directory = 'inputs'\nos.makedirs(output_directory, exist_ok=True)\n\nfor i in range(1, 20):\n    json_data = [input_data.copy()]\n\n    file_path = os.path.join(output_directory, f'input_{i}.jsonl')\n    with open(file_path, 'w') as input_file:\n        for line in json_data:\n            json.dump(line, input_file)\n            input_file.write('\\n')\r\n```\n\n<div class=\"content-ad\"></div>\n\n이미 제공된 유틸리티 함수를 사용하여 로컬 파일을 S3에 업로드하여 추론할 수 있습니다.\n\n```js\ndef upload_file(input_location):\n    prefix = f\"{bucket_prefix}/input\"\n    return sagemaker_session.upload_data(\n        input_location,\n        bucket=default_bucket,\n        key_prefix=prefix,\n        extra_args={\"ContentType\": \"application/json\"} # 꼭 지정해야함\n    )\n\nsample_data_point = upload_file(\"inputs/input_1.jsonl\")\nprint(f\"샘플 데이터 포인트가 업로드되었습니다: {sample_data_point}\")\n```\n\n그런 다음 Boto3 SDK를 통해 \"invoke_endpoint_async\" API 호출로이 S3 경로에서 샘플 추론을 실행할 수 있습니다.\n\n```js\nimport boto3\nruntime = boto3.client(\"sagemaker-runtime\")\n\nresponse = runtime.invoke_endpoint_async(\n    EndpointName=predictor.endpoint_name,\n    InputLocation=sample_data_point,\n    Accept='application/json',\n    ContentType=\"application/json\"\n)\n\noutput_location = response[\"OutputLocation\"]\nprint(f\"출력 위치: {output_location}\")\n```\n\n<div class=\"content-ad\"></div>\n\n한 번 더 제공된 유틸리티 함수를 사용하여 출력 파일의 출력을 관찰합니다. LLM을 사용하여 실제 추론을 수행하고 S3 파일을 생성하는 데 시간이 걸릴 수 있습니다. 따라서 제공된 함수에서는 화면에 표시할 내용이 포함된 데이터 파일이 나타날 때까지 데이터 파일을 확인합니다.\n\n```js\nimport urllib, time\nfrom botocore.exceptions import ClientError\n\n# 함수 참조/크레딧: https://github.com/aws/amazon-sagemaker-examples/blob/main/async-inference/Async-Inference-Walkthrough-SageMaker-Python-SDK.ipynb\ndef get_output(output_location):\n    output_url = urllib.parse.urlparse(output_location)\n    bucket = output_url.netloc\n    key = output_url.path[1:]\n    while True:\n        try:\n            return sagemaker_session.read_s3_file(bucket=output_url.netloc, key_prefix=output_url.path[1:])\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n                print(\"output을 기다리는 중...\")\n                time.sleep(60)\n                continue\n            raise\n\noutput = get_output(output_location)\nprint(f\"Output: {output}\")\n```\n\n그런 다음 모든 샘플 데이터 포인트를 실행하여 모든 입력 파일에서 추론을 수행할 수 있습니다:\n\n```js\ninferences = []\nfor i in range(1, 20):\n    input_file = f\"inputs/input_{i}.jsonl\"\n    input_file_s3_location = upload_file(input_file)\n    print(f\"{input_file}을 사용하여 Endpoint를 호출 중\")\n    async_response = predictor.predict_async(input_path=input_file_s3_location)\n    output_location = async_response.output_path\n    print(output_location)\n    inferences += [(input_file, output_location)]\n    time.sleep(0.5)\n\nfor input_file, output_location in inferences:\n    output = get_output(output_location)\n    print(f\"입력 파일: {input_file}, 출력: {output}\")\n```\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference_3.png\" />\n\n## c. AutoScaling 설정\n\n비동기 추론에서 자동 스케일링은 실시간 추론과 마찬가지로 Application AutoScaling을 통해 설정됩니다. 이곳에서의 차이점은 스케일링할 수 있는 새로운 메트릭이 있다는 점입니다.\n\n비동기 추론 내에서 이미 구현된 내부 대기열이 있음을 이해했듯이, 자동 스케일링은 이 대기열에 있는 항목의 수에 따라 확장하거나 축소할 수 있습니다. 이는 CloudWatch 메트릭 \"ApproximateBackLogSize\"로 캡처됩니다. 이 요청은 이미 처리 중인 것이거나 아직 처리되지 않은 것입니다.\n\n<div class=\"content-ad\"></div>\n\nReal-Time Inference와 비슷한 방식으로 Boto3 SDK를 사용하여 정책을 설정했습니다. 최소 인스턴스 수를 0으로 정의했는데, 이 기능은 Asynchronous Inference에서만 지원됩니다.\n\n```js\nclient = boto3.client(\n    \"application-autoscaling\"\n)  # SageMaker를 포함한 다른 서비스의 Application Auto Scaling을 나타내는 일반 클래스\n\nresource_id = (\n    \"endpoint/\" + predictor.endpoint_name + \"/variant/\" + \"AllTraffic\"\n)  # application autoscaling이 엔드포인트를 참조하는 형식\n\n# 비동기 엔드포인트에서 인스턴스 수를 0으로 설정하여 Autoscaling 구성\nresponse = client.register_scalable_target(\n    ServiceNamespace=\"sagemaker\",\n    ResourceId=resource_id,\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n    MinCapacity=0,\n    MaxCapacity=5,\n)\n```\n\n최소 및 최대 인스턴스 수를 지정한 후, 확장 및 축소에 대한 쿨다운 기간을 정의할 수 있습니다. 여기에서 \"MetricName\"을 \"ApproximateBackLogSize\" 메트릭으로 지정하였음을 알려드립니다.\n\n```js\nresponse = client.put_scaling_policy(\n    PolicyName=\"Invocations-ScalingPolicy\",\n    ServiceNamespace=\"sagemaker\",  # 리소스를 제공하는 AWS 서비스의 이름 공간\n    ResourceId=resource_id,  # 엔드포인트 이름\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  # SageMaker는 인스턴스 수만 지원\n    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n    TargetTrackingScalingPolicyConfiguration={\n        \"TargetValue\": 5.0,  # 메트릭의 목표 값 - 여기서 메트릭은 SageMakerVariantInvocationsPerInstance입니다.\n        \"CustomizedMetricSpecification\": {\n            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n            \"Namespace\": \"AWS/SageMaker\",\n            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": predictor.endpoint_name}],\n            \"Statistic\": \"Average\",\n        },\n        \"ScaleInCooldown\": 600,  # 쿨다운 기간은 이전 작업의 영향이 나타나기 전에 추가 인스턴스를 시작하거나 종료하는 것을 방지합니다.\n        \"ScaleOutCooldown\": 100,  # ScaleOutCooldown - 확장 작업 완료 후 다른 확장 작업을 시작하기 전의 시간 간격.\n    },\n)\n```\n\n<div class=\"content-ad\"></div>\n\nAutoScaling을 테스트하려면 일정 기간 동안 요청을 보낼 수 있습니다. 스케일링 정책에 따르면 대상 값은 엔드포인트 뒤에 있는 큐에서 아직 처리 중이거나 처리되지 않은 요청 또는 호출을 5회로 설정됩니다.\n\n```js\nrequest_duration = 60 * 15 # 15분\nend_time = time.time() + request_duration\nprint(f\"{request_duration}초 동안 테스트가 실행됩니다.\")\nwhile time.time() < end_time:\n    predictor.predict(input_data)\n```\n\n일정 시간 동안 요청을 보내지 않은 후에는 인스턴스 수가 제로로 축소되고, 큐가 완전히 비어 있는 것을 주의하세요:\n\n![이미지](/assets/img/2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference_4.png)\n\n<div class=\"content-ad\"></div>\n\n# 3. 추가 자료 및 결론\n\n전체 예제 코드는 위 링크에서 찾을 수 있습니다. SageMaker 비동기 추론은 전적으로 실시간이나 일괄 처리가 아닌 특정 LLM 사용 사례에 사용할 수 있는 기능입니다. 이 기사가 귀하에게 LLM을 규모 확장하여 인퍼런스를 제공하는 다른 방법에 대한 유용한 소개였기를 바랍니다. 이 분야에서 더 많은 콘텐츠를 기대해 주세요!\n\n항상 읽어 주셔서 감사합니다. 읽은 후 의견을 자유롭게 남겨 주세요.\n\n이 기사를 즐겁게 보셨다면 LinkedIn에서 저와 연락하고 Medium 뉴스레터를 구독해 주시기 바랍니다.","ogImage":{"url":"/assets/img/2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference_0.png"},"coverImage":"/assets/img/2024-06-23-DeployingLargeLanguageModelswithSageMakerAsynchronousInference_0.png","tag":["Tech"],"readingTime":12},{"title":"LLM API 비교 모델별 가격 분석","description":"","date":"2024-06-23 19:58","slug":"2024-06-23-LLMAPIsPriceComparisonbyModel","content":"\n\n<img src=\"/assets/img/2024-06-23-LLMAPIsPriceComparisonbyModel_0.png\" />\n\n이 블로그 게시물은 점점 진화하는 대형 언어 모델(LLM) API 및 관련 가격 구조에 대해 다룹니다.\n\n우리의 목표는 성능이 우수한 LLM 중에서 가장 경쟁력 있는 LLM API 모델을 가격 측면에서 강조하는 지속적으로 업데이트되는 리소스를 만드는 것입니다.\n\n이 게시물을 쉽게 참조하기 위해 즐겨찾기해 두고, 새로운 LLM API 모델을 제안하거나 제공된 정보의 부정확성을 지적해 주십시오.\n\n<div class=\"content-ad\"></div>\n\n비교의 세부 사항에 대해 파헤치기 전에 맥락 개요부터 시작해보죠...\n\n## 모델은 어떻게 선택되나요?\n\n최고의 모델은 대규모 멀티태스크 언어 이해 (MMLU) 벤치마크 및 LMSYS 챗봇 아레나의 리더보드 테이블에서 추출됩니다.\n\n## 제공자는 어떻게 선택되나요?\n\n<div class=\"content-ad\"></div>\n\n일단 모델을 식별하면 다양한 공급업체에서 그 모델을 추적합니다. 여기서는 각 모델에 대한 최상의 가격 옵션을 제시합니다. 추론용 API 공급업체가 없는 경우 목록에서 제거됩니다.\n\n## 가격이 어떻게 계산됩니까?\n\n가격은 토큰당 계산되며, 백만 개 토큰당 미국 달러로 표시되며, 입력 및 출력 토큰 가격을 3대 1의 비율로 섞은 가격입니다.\n\n## 모델별 LLM API 요금 살펴보기\n\n<div class=\"content-ad\"></div>\n\n가장 강력한 LLM 모델들의 표:\n\n이 페이지를 쉽게 참조하기 위해 즐겨찾기를 해두세요. 댓글 섹션에서 기여해도 괜찮아요!","ogImage":{"url":"/assets/img/2024-06-23-LLMAPIsPriceComparisonbyModel_0.png"},"coverImage":"/assets/img/2024-06-23-LLMAPIsPriceComparisonbyModel_0.png","tag":["Tech"],"readingTime":1},{"title":"Transformer 아키텍처 완벽 설명","description":"","date":"2024-06-23 19:57","slug":"2024-06-23-TransformerArchitectureexplained","content":"\n\nTransfomers은 최근에 많은 소음을 일으키고 있는 머신 러닝의 새로운 개발입니다. 그들은 맥락을 잘 추적하는 데 놀라울 정도로 뛰어납니다. 그래서 그들이 쓰는 텍스트가 이치에 맞는 것입니다. 이 장에서는 변압기의 아키텍처와 작동 방식에 대해 알아볼 것입니다.\n\n![이미지](/assets/img/2024-06-23-TransformerArchitectureexplained_0.png)\n\n변압기 모델은 머신 러닝의 가장 흥미로운 새로운 발전 중 하나입니다. Attention is All You Need 논문에서 소개되었습니다. 변압기는 이야기, 수필, 시를 쓰거나, 질문에 답하거나, 다국어 간 번역하거나, 사람들과 대화하거나, 심지어 인간에게 어려운 시험도 패스할 수 있습니다! 하지만 그것들은 무엇인가요? 변압기 모델의 아키텍처는 그다지 복잡하지 않습니다. 그것은 단지 매우 유용한 구성 요소들을 연결한 것으로, 각각의 구성 요소는 자체 기능을 가지고 있습니다. 이 장에서는 이러한 구성 요소들을 모두 배우게 될 것입니다.\n\n요약하면, 변압기는 무엇을 하는 걸까요? 핸드폰에서 텍스트 메시지를 작성하고 있다고 상상해 보세요. 각 단어 뒤에는 세 개의 단어가 제안될 수 있습니다. 예를 들어, \"안녕, 어떻게\"를 입력하면 핸드폰은 \"당신\"이나 \"당신의\"와 같은 단어를 다음 단어로 제안할 수 있습니다. 물론, 핸드폰에서 제안된 단어를 계속 선택하면 이러한 단어로 이루어진 메시지가 이치에 맞지 않음을 빨리 알 수 있습니다. 각각 3개 또는 4개의 연속된 단어 집합을 살펴보면 이치가 맞을 수 있지만, 이러한 단어들은 의미 있는 내용으로 이어지지 않습니다. 이는 핸드폰에서 사용된 모델이 메시지의 전반적인 맥락을 가지고 있지 않기 때문에 발생합니다. 그 모델은 단순히 최근 몇 단어 뒤에 어떤 단어가 더 자주 나올지 예측하는 것입니다. 반면에 변압기는 쓰여지는 내용의 맥락을 추적하고, 그래서 그들이 쓰는 텍스트가 의미가 있는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n휴대폰은 텍스트 메시지에서 사용할 다음 단어를 제안할 수 있지만, 일관된 텍스트를 생성할 수 있는 능력은 없습니다.\n\n![Transformer Architecture](/assets/img/2024-06-23-TransformerArchitectureexplained_1.png)\n\n솔직하게 말하자면, 트랜스포머가 한 번에 한 단어씩 텍스트를 작성한다는 것을 처음 알게 되었을 때, 믿을 수가 없었습니다. 우선, 이것이 인간이 문장을 형성하고 생각하는 방법이 아닙니다. 우리는 먼저 기본적인 생각을 형성한 후, 점차 정제하고 단어를 추가합니다. 이것은 또한 ML 모델이 다른 작업을 하는 방식이 아닙니다. 예를 들어, 이미지는 이렇게 만들어지지 않습니다. 대부분의 신경망 기반의 그래픽 모델은 이미지의 대략적인 버전을 형성하고, 점진적으로 정제하거나 세부 정보를 추가하여 완벽하게 만듭니다. 그렇다면 왜 트랜스포머 모델이 한 단어씩 텍스트를 작성하는 걸까요? 한 가지 답은, 그렇게 하는 것이 정말 잘 작동하기 때문입니다. 더 만족스러운 답은, 트랜스포머가 맥락을 매우 잘 추적하여 다음 단어가 아이디어를 이어나가는 데 정확히 필요한 것이기 때문입니다.\n\n그러면 트랜스포머는 어떻게 훈련되는 걸까요? 많은 양의 데이터로, 사실상 인터넷의 모든 데이터로 말이죠. 그래서 \"안녕, 어떻게\"라는 문장을 트랜스포머에 입력하면, 인터넷의 모든 텍스트를 기반으로 \"당신\"이라는 다음 단어가 가장 좋다는 것을 단순히 알고 있습니다. 좀 더 복잡한 명령을 주면, 예를 들어, \"이야기를 써봐.\" 라고 주면, 좋은 다음 단어로 \"한 번\"을 사용할 수 있음을 알아낼 수도 있습니다. 그러면 이 단어를 명령에 추가하고, 좋은 다음 단어가 \"있던\", 이어갈 다음 단어가 무엇인지 찾아나갑니다. 그리고 한 단어씩, 이렇게 이어가면서 이야기를 써 나갈 것입니다.\n\n<div class=\"content-ad\"></div>\n\n명령: 이야기를 쓰세요\n응답: 한 번에\n\n다음 명령: 이야기를 쓰세요. 한 번에\n응답: 옛날에\n\n다음 명령: 이야기를 쓰세요. 한 번에 옛날에\n응답: 한\n\n다음 명령: 이야기를 쓰세요. 한 번에 옛날에 한\n응답: 시간\n\n<div class=\"content-ad\"></div>\n\n한때 가족들이 모여 있던 어느 편안한 마을에서 다섯 형제가 살았어요. 그들의 이름은 래리, 루이스, 레오나르도, 루이라, 그리고 루카스였죠. 모두가 가난한 환경에서 자라났지만, 그들은 서로를 깊이 사랑하는 특별한 유대 관계를 형성했어요.\n\n하루, 마을에 한 노인이 나타났어요. 그 노인은 썩은 소나무 막대기를 들고 다니며 지쳐 보이더라구요. 형제들은 노인을 도와주기로 했어요. 그들은 함께 얘기를 나누고, 노인에게 식사와 숙소를 제공했어요.\n\n이야기를 나누는 동안, 노인은 금지된 섬의 보물이 있는 곳을 가리키며 이야기를 풀어냈어요. 형제들은 노인을 따라가기로 결심했고, 어려운 여정을 통해 함께 고난과 역경을 극복하며 결국 보물을 찾아냈어요.\n\n그들은 이 경험을 통해 서로의 용기, 협력, 그리고 신뢰를 배웠어요. 이들은 함께한 경험이 더 이상 그들을 나눌 수 없는 특별한 유대 관계를 형성하는 데 도움이 되었죠. 이제 형제들은 당신의 도움이 주변에 필요한 이웃을 발견하고 노인처럼 따뜻한 마음으로 도와줄 준비가 되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-TransformerArchitectureexplained_2.png)\n\n# 토큰화\n\n토큰화는 가장 기본적인 단계입니다. 단어, 구두점 등을 포함한 토큰 데이터 세트로 이루어져 있습니다. 토큰화 단계는 각 단어, 접두사, 접미사, 구두점을 포함하여 해당 라이브러리의 알려진 토큰으로 보냅니다.\n\n![이미지](/assets/img/2024-06-23-TransformerArchitectureexplained_3.png)\n\n\n<div class=\"content-ad\"></div>\n\n# 포함\n\n입력이 토큰화되면 단어를 숫자로 변환해야 합니다. 이를 위해 임베딩을 사용합니다. 이전 장에서 텍스트 임베딩이 모든 텍스트를 숫자 목록으로 변환한다는 것을 배웠습니다. 두 개의 텍스트가 유사하면 해당 벡터의 숫자도 서로 유사합니다(요소별로, 즉 동일한 위치의 각 숫자 쌍이 유사함을 의미). 그렇지 않으면 두 개의 텍스트가 다르면 해당 벡터의 숫자도 다릅니다.\n\n![transformer](/assets/img/2024-06-23-TransformerArchitectureexplained_4.png)\n\n# 위치 인코딩\n\n<div class=\"content-ad\"></div>\n\n문장의 각 토큰에 해당하는 벡터를 가지고 나면, 다음 단계는 이 모든 것을 하나의 벡터로 결합하는 것입니다. 여러 벡터를 하나의 벡터로 변환하는 가장 일반적인 방법은 각 요소를 더하는 것입니다. 즉, 각 좌표를 따로 더합니다. 예를 들어, 길이가 2인 벡터 [1,2]와 [3,4]가 있다면, 이에 해당하는 합은 [1+3, 2+4]로 [4, 6]이 됩니다. 이 방법은 작동할 수 있지만 한 가지 주의할 점이 있습니다. 덧셈은 교환법칙이 성립하므로, 숫자를 다른 순서로 더하더라도 동일한 결과를 얻을 수 있습니다. 이 경우 \"I'm not sad, I'm happy\"와 \"I'm not happy, I'm sad\"는 같은 결과 벡터를 얻게 됩니다. 이러한 경우는 바람직하지 않습니다. 그래서 두 문장에 대해 서로 다른 벡터를 얻기 위한 방법을 고안해야 합니다. 여러 방법이 있지만, 우리는 그 중 하나를 선택할 것입니다: 위치 인코딩(Positional Encoding). 위치 인코딩은 단어의 임베딩 벡터에 미리 정의된 벡터 시퀀스를 추가하는 것으로, 이를 통해 각 문장에 대해 고유한 벡터를 얻고 같은 단어가 다른 순서로 나타난 문장에는 서로 다른 벡터가 할당됩니다. 아래 예시에서 단어 \"Write\", \"a\", \"story\", \".\"에 해당하는 벡터가 각 단어의 위치 정보를 담고 있는 수정된 벡터로 변환됩니다. \"Write (1)\", \"a (2)\", \"story (3)\", \". (4)\"으로 레이블이 지정됩니다.\n\n![Transformer Architecture Explained](/assets/img/2024-06-23-TransformerArchitectureexplained_5.png)\n\n# 트랜스포머 블록\n\n지금까지의 내용을 요약해보겠습니다. 단어가 들어오면 토큰으로 변환되고(tokenization), 토큰화된 단어들은 숫자로 변환됩니다(임베딩), 그리고 순서가 고려됩니다(위치 인코딩). 이렇게 모델에 입력되는 각 토큰에 대해 하나의 벡터가 생성됩니다. 이제 다음 단계는 이 문장에서 다음 단어를 예측하는 것입니다. 이 작업은 실제로 매우 큰 신경망으로 수행되며, 해당 목표에 정확하게 훈련된 모델을 사용하여 이 문장에서 다음 단어를 예측합니다.\n\n<div class=\"content-ad\"></div>\n\n큰 신경망을 교육할 수는 있지만, 주요 단계인 주의 메커니즘을 추가함으로써 크게 개선할 수 있습니다. Attention is All you Need 논문에서 소개된 이 주의 메커니즘은 트랜스포머 모델의 주요 구성 요소 중 하나로, 그들이 잘 동작하는 이유 중 하나입니다. 주의는 이전 섹션에서 설명되었지만, 지금은 이것을 텍스트의 각 단어에 맥락을 추가하는 방법으로 상상해 보세요.\n\n주의 구성 요소는 피드포워드 신경망의 각 블록에 추가됩니다. 따라서 다음 단어를 예측하는 것을 목표로 하는 대형 피드포워드 신경망을 상상해보면, 여러 개의 작은 신경망 블록으로 구성된 것이라고 생각할 수 있습니다. 각 블록에 주의 구성 요소가 추가됩니다. 각 트랜스포머 구성 요소는 트랜스포머 블록이라고 불리고 주로 다음 두 가지 구성 요소로 형성됩니다:\n\n- 주의 구성 요소.\n- 피드포워드 구성 요소.\n\n![image](/assets/img/2024-06-23-TransformerArchitectureexplained_6.png)\n\n<div class=\"content-ad\"></div>\n\n# 주의\n\n다음 단계는 주의입니다.  어텐션 메커니즘은 매우 중요한 문제, 즉 문맥의 문제를 다룹니다. 가끔은 동일한 단어라고 하더라도 다른 의미로 사용될 수 있습니다. 이것은 임베딩이 단어를 벡터로 보내기만 하고 사용 중인 단어의 정의가 무엇인지 알지 못하기 때문에 언어 모델을 혼란스럽게 만듭니다.\n\n어텐션이라는 기술은 언어 모델이 문맥을 이해하는 데 도움이 되는 매우 유용한 기술입니다. 어텐션이 어떻게 작동하는지 이해하기 위해 다음 두 문장을 살펴보겠습니다:\n\n- 문장 1: 강의 둑.\n- 문장 2: 은행에 있는 돈.\n\n<div class=\"content-ad\"></div>\n\n안녕하세요! '은행'이라는 단어가 두 번 나오지만 다른 의미를 갖고 있음을 알 수 있습니다. 제1문장에서는 강가의 땅을 가리키고, 두 번째 문장에서는 돈을 보관하는 기관을 가리키고 있죠. 컴퓨터는 이를 모르기 때문에 이 지식을 주입해야 합니다. 그럼 무엇이 도움이 될까요? 문장 안의 다른 단어들이 우리 구조에 도움이 될 수 있습니다. 첫 번째 문장에서는 'the'나 'of'는 도움이 되지 않지만, 'river'라는 단어는 강가의 땅을 언급하고 있음을 알려줍니다. 마찬가지로, 두 번째 문장에서는 'money'라는 단어가 우리에게 돈을 보관하는 기관을 가리킨다는 것을 이해할 수 있게 해줍니다.\n\n요약하자면, 어텐션은 문장(또는 텍스트 조각) 안의 단어들을 단어 임베딩에서 가깝게 이동시키는 역할을 합니다. 이렇게 하면 \"은행에 돈\"이라는 문장에서 \"은행\"이 \"돈\"이라는 단어와 가까워집니다. 마찬가지로, \"강가의 은행\"이라는 문장에서 \"은행\"은 \"강\"이라는 단어와 가까워집니다. 이렇게 두 문장 각각의 수정된 \"은행\"은 주변 단어의 일부 정보를 함께 전달하여 맥락을 추가합니다.\n\n트랜스포머 모델에서 사용되는 어텐션 단계는 실제로 훨씬 강력하며, 멀티헤드 어텐션이라고 불립니다. 멀티헤드 어텐션에서는 여러 다른 임베딩이 사용되어 벡터를 수정하고 맥락을 추가합니다. 멀티헤드 어텐션은 언어 모델이 텍스트를 처리하고 생성할 때 효과적인 수준으로 도달하도록 도와주었습니다.\n\n<div class=\"content-ad\"></div>\n\n# 소프트맥스 레이어\n\n이제 여러 개의 변형 블록으로 구성된 변압기 계층들이 형성됨을 알게 되었으니, 각각이 주의 및 피드포워드 레이어를 포함하는 많은 계층의 변압기를 문장에서 다음 단어를 예측하는 대규모 신경망으로 생각할 수 있습니다. 변압기는 모든 단어에 대한 점수를 출력하며, 문장에서 다음으로 가장 가능성이 높은 단어들에 가장 높은 점수를 부여합니다.\n\n변압기의 마지막 단계는 소프트맥스 레이어로, 이 점수를 확률로 변환(합이 1이 되는)하여 가장 높은 점수가 가장 높은 확률에 해당하도록 합니다. 그런 다음 이러한 확률 중에서 다음 단어를 샘플링할 수 있습니다. 아래 예시에서 변압기는 \"Once\"에 0.5의 가장 높은 확률을 부여하고 \"Somewhere\"에는 0.3, \"There\"에는 0.2의 확률을 부여합니다. 한 번 샘플링하면 단어 \"once\"가 선택되어 변압기의 출력이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n자 이제 뭐 할까요? 우리는 단계를 반복해요. 이제 텍스트 \"한 이야기를 써보세요. 한 번\"을 모델에 입력하면, 아마도 출력은 \"upon\"이 될 거에요. 이 단계를 계속 반복하면, 변환기는 \"한 번에 어느 날, ... 이었어요.\"와 같은 이야기를 쓰게 될 거에요.\n\n## 훈련 후\n\n이제 변환기가 어떻게 작동하는지 알았으니, 아직 해야 할 일이 있어요. 다음을 상상해보세요. 변환기에게 \"알제리의 수도는 무엇인가요?\"라고 물어봤을 때, \"알제\"라고 대답하고 넘어가길 원할 거에요. 그러나 변환기는 전체 인터넷으로 훈련받았어요. 인터넷은 큰 공간이고, 그것이 반드시 최선의 질문/답변 저장소라는 보장은 없어요. 예를 들어 많은 페이지가 답변이 없는 질문 목록을 갖고 있을 수 있어요. 이 경우 \"알제리의 수도는 무엇인가요?\" 다음 문장에는 \"알제리의 인구는 어떻게 되나요?\"나 \"부르키나파소의 수도는 무엇인가요?\"와 같은 다른 질문이 나올 수 있어요. 변환기는 자신의 응답을 고려하는 인간이 아니에요, 단지 인터넷(또는 제공된 데이터셋)에서 본 것을 모방할 뿐이에요. 그렇다면 변환기에게 질문에 대답하게 하려면 어떻게 해야 할까요?\n\n답은 후훈련(post-training)에 있어요. 사람에게 특정 작업을 가르치듯이, 변환기에게도 작업을 수행하도록 시킬 수 있어요. 변환기가 전체 인터넷에 훈련을 받은 후, 많은 질문과 그에 해당하는 답변의 대형 데이터셋으로 다시 훈련받아요. 변환기(사람과도 같이)는 학습한 마지막 것에 편향을 가지기 때문에 후훈련은 변환기가 요청받은 작업들에 성공하는 데 매우 유용한 단계로 입증되었어요.\n\n<div class=\"content-ad\"></div>\n\n포스트 트레이닝은 많은 다른 작업에도 도움이 됩니다. 예를 들어, 대화 데이터셋을 사용하여 트랜스포머를 포스트 트레이닝하면 챗봇으로 잘 작동하도록 돕거나, 이야기, 시, 코드를 작성하는 데 도움을 줄 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-23-TransformerArchitectureexplained_0.png"},"coverImage":"/assets/img/2024-06-23-TransformerArchitectureexplained_0.png","tag":["Tech"],"readingTime":8},{"title":"텍스트 임베딩 종합 가이드 2024 최신","description":"","date":"2024-06-23 19:53","slug":"2024-06-23-TextEmbeddingsComprehensiveGuide","content":"\n\n## 텍스트 임베딩의 진화, 시각화, 그리고 응용\n\n우리 인간은 텍스트를 읽고 이해할 수 있습니다 (적어도 일부분은요). 컴퓨터는 반대로 \"숫자로 생각\"하기 때문에 단어와 문장의 의미를 자동으로 파악할 수 없습니다. 만약 우리가 컴퓨터가 자연어를 이해하도록 하려면, 이 정보를 컴퓨터가 작업할 수 있는 형식인 숫자 벡터로 변환해야 합니다.\n\n수십 년 전에 사람들은 텍스트를 기계가 이해할 수 있는 형식으로 변환하는 방법을 배웠습니다 (그 중 하나는 ASCII였습니다). 이러한 방식은 텍스트를 렌더링하고 전송하는 데 도움이 되지만 단어의 의미를 부호화하지는 않습니다. 당시에는 키워드 검색 기술이 표준 검색 기술이었으며 특정 단어나 N-gram을 포함하는 모든 문서를 찾는 방식이었습니다.\n\n그 후 몇 10년이 지난 후, 임베딩이 등장했습니다. 우리는 단어, 문장, 심지어 이미지에 대한 임베딩을 계산할 수 있습니다. 임베딩도 숫자의 벡터입니다만, 의미를 포착할 수 있습니다. 그래서 의미 검색을 수행하거나 다양한 언어로 된 문서를 다루는 데 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 글에서는 임베딩 주제를 깊이 있게 다루어보고자 합니다:\n\n- 임베딩이 만들어지기 전의 역사와 진화에 대해,\n- OpenAI 도구를 사용하여 임베딩을 계산하는 방법,\n- 문장이 서로 가까운지 판단하는 방법,\n- 임베딩을 시각화하는 방법,\n- 가장 흥미로운 부분은 임베딩을 실제로 활용하는 방법입니다.\n\n이어서 나아가서 임베딩의 진화에 대해 배워보겠습니다.\n\n# 임베딩의 진화\n\n<div class=\"content-ad\"></div>\n\n우리는 텍스트 표현의 역사로 간단한 여행을 시작할 것입니다.\n\n## 단어 가방\n\n텍스트를 벡터로 변환하는 가장 기본적인 방법은 단어 가방입니다. 리처드 P. 페이만의 유명한 명언 중 하나를 살펴보겠습니다. \"우리는 아직 발견들을 만들어내는 시대에 살고 있다\". 이를 통해 단어 가방 접근법을 설명해보겠습니다.\n\n단어 가방 벡터를 얻는 첫 번째 단계는 텍스트를 단어(토큰)로 나눈 다음, 단어를 기본 형태로 줄이는 것입니다. 예를 들어, \"running\"은 \"run\"으로 변환됩니다. 이 과정을 어간 추출(stemming)이라고 합니다. NLTK Python 패키지를 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\n\ntext = 'We are lucky to live in an age in which we are still making discoveries'\n\n# 토큰화 - 텍스트를 단어로 나누기\nwords = word_tokenize(text)\nprint(words)\n# ['We', 'are', 'lucky', 'to', 'live', 'in', 'an', 'age', 'in', 'which',\n#  'we', 'are', 'still', 'making', 'discoveries']\n\nstemmer = SnowballStemmer(language=\"english\")\nstemmed_words = list(map(lambda x: stemmer.stem(x), words))\nprint(stemmed_words)\n# ['we', 'are', 'lucki', 'to', 'live', 'in', 'an', 'age', 'in', 'which',\n#  'we', 'are', 'still', 'make', 'discoveri']\r\n```\n\n자, 이제 우리 단어들의 기본 형태 리스트가 있습니다. 다음 단계는 이들 빈도를 계산하여 벡터를 만드는 것입니다.\n\n```js\r\nimport collections\nbag_of_words = collections.Counter(stemmed_words)\nprint(bag_of_words)\n# {'we': 2, 'are': 2, 'in': 2, 'lucki': 1, 'to': 1, 'live': 1, \n# 'an': 1, 'age': 1, 'which': 1, 'still': 1, 'make': 1, 'discoveri': 1}\r\n```\n\n사실, 만약 텍스트를 벡터로 변환하고 싶다면, 텍스트에 있는 단어뿐만 아니라 전체 어휘를 고려해야 합니다. \"i\", \"you\", \"study\"도 어휘에 있다고 가정하고, 파인만의 명언에서 벡터를 만들어 봅시다.\r\n\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_0.png)\n\n이 방법은 꽤 기본적이며 단어의 의미를 고려하지 않기 때문에 \"그 여자는 데이터 과학을 공부하고 있다\"와 \"젊은 여성이 AI와 ML을 배우고 있다.\"라는 문장이 서로 가까운 위치에 있지 않을 수 있습니다.\n\n## TF-IDF\n\n단어 가방 접근법의 약간 개선된 버전인 TF-IDF(Term Frequency — Inverse Document Frequency)입니다. 이것은 두 가지 지표의 곱셈입니다.\n\n<div class=\"content-ad\"></div>\n\n![Markdown Table](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_1.png)\n\n- 용어 빈도는 문서에서 단어의 빈도를 보여줍니다. 이를 계산하는 가장 흔한 방법은 이 문서에서 용어의 로우 카운트(단어 가방에 있는 것처럼)을 전체 용어(단어) 수로 나누는 것입니다. 그러나 로우 카운트, 부욜리언 \"빈도\", 정규화에 대한 다양한 접근 방법이 많이 있습니다. 위키피디아에서 다양한 접근 방법에 대해 더 배울 수 있습니다.\n\n![Markdown Table](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_2.png)\n\n- 역문서 주파수는 단어가 얼마나 많은 정보를 제공하는지를 나타냅니다. 예를 들어, \"a\"나 \"that\" 같은 단어는 문서 주제에 대해 추가 정보를 제공하지 않습니다. 대조적으로, \"ChatGPT\"나 \"생물정보학\" 같은 단어는 도메인을 정의하는 데 도움이 될 수 있습니다 (하지만 이 문장에는 해당하지 않음). 이는 전체 문서 수와 해당 단어를 포함하는 문서 수의 비율의 로그함수로 계산됩니다. IDF가 0에 가까울수록 단어가 흔하고 제공하는 정보가 더 적습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_3.png\" />\n\n그래서 결과적으로 우리는 일반적인 단어 (\"I\"나 \"you\"와 같은)이 낮은 가중치를 갖는 벡터를 얻게됩니다. 한편, 문서에서 여러 번 발생하는 드문 단어들은 더 높은 가중치를 갖게 됩니다. 이 전략은 약간 더 나은 결과를 제공하지만 여전히 의미적 의미를 잡아내기는 어렵습니다.\n\n이 방법론의 다른 어려움은 상당히 희소한 벡터를 생성한다는 점입니다. 벡터의 길이는 말뭉치 크기와 동일합니다. 영어에는 약 470,000개의 고유 단어가 있습니다(출처). 그러므로 우리는 거대한 벡터를 갖게 될 것입니다. 하지만 문장에는 50개 이상의 고유 단어가 나타나지 않을 것이므로 벡터의 값 중 99.99%는 0일 것입니다. 이는 어떤 정보도 인코딩하지 않습니다. 이에 대해 과학자들은 밀도 있는 벡터 표현에 대해 고민하기 시작했습니다.\n\n## Word2Vec\n\n<div class=\"content-ad\"></div>\n\n가장 유명한 밀집 표현 방법 중 하나는 구글이 2013년에 Mikolov 등이 제안한 \"효율적인 단어 표현 추정을 위한 Word2Vec\" 논문에서 소개한 word2vec입니다.\n\n논문에서 언급된 두 가지 word2vec 접근 방식은 Continuous Bag of Words(주변 단어를 기반으로 단어를 예측하는 방법)와 Skip-gram(반대 작업인 단어를 기반으로 문맥을 예측하는 방법)입니다.\n\n밀집 벡터 표현의 핵심 아이디어는 두 모델을 훈련하는 것입니다: 인코더와 디코더. 예를 들어, Skip-gram의 경우 \"christmas\"라는 단어를 인코더에 전달할 수 있습니다. 그런 다음, 인코더가 \"merry\", \"to\", \"you\"와 같은 단어를 얻을 것으로 예상하여 디코더에 전달할 수 있는 벡터를 생성할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_5.png)\n\n이 모델은 이제 단어의 의미를 고려하기 시작했습니다. 단어의 맥락에서 훈련되었기 때문입니다. 그러나 형태학(예: \"-less\"는 무언가의 부족을 의미함)을 무시합니다. 나중에는 GloVe에서 서브워드 스킵-그램을 살펴봄으로써 이 단점을 개선했습니다.\n\n또한, word2vec은 단어와만 작동할 수 있었지만, 우리는 전체 문장을 인코딩하고 싶습니다. 그러니, 트랜스포머로 다음 진화 단계로 넘어가 봅시다.\n\n## 트랜스포머와 문장 임베딩\n\n\n<div class=\"content-ad\"></div>\n\n다음 진화는 Vaswani 등이 발표한 \"Attention Is All You Need\" 논문에서 소개된 트랜스포머 접근 방식과 관련이 있었습니다. 트랜스포머는 정보가 풍부한 밀집 벡터를 생성할 수 있었고 현대 언어 모델의 주요 기술로 자리 잡게 되었습니다.\n\n저는 트랜스포머의 구조 세부 사항에 대해 다루지 않겠습니다. 왜냐하면 이것은 우리 주제와 관련이 그리 크지 않고 많은 시간이 소요되기 때문입니다. 더 배우고 싶다면 \"Transformers, Explained\" 또는 \"The Illustrated Transformer\"와 같은 다양한 자료가 많이 있습니다.\n\n트랜스포머를 사용하면 동일한 \"핵심\" 모델을 사용하여 다른 사용 사례에 대해 미세 조정할 수 있으며, 핵심 모델을 다시 학습시킬 필요가 없습니다(시간이 많이 소요되고 상당한 비용이 듭니다). 이것은 사전 훈련된 모델의 등장으로 이어졌습니다. 가장 인기 있는 최초의 모델 중 하나는 Google AI가 개발한 BERT(Bidirectional Encoder Representations from Transformers)였습니다.\n\n내부적으로 BERT는 여전히 word2vec과 유사한 토큰 수준에서 작동하지만, 우리는 여전히 문장 임베딩을 얻고 싶습니다. 따라서, 모든 토큰 벡터의 평균을 취하는 단순한 방법을 적용할 수 있습니다. 유감스럽게도, 이 방법은 좋은 성능을 보여주지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n2019년에 이 문제는 Sentence-BERT가 출시되면서 해결되었습니다. 이는 의미론적 텍스트 유사성 작업에서 이전 방법들을 모두 능가하며 문장 포함 벡터의 계산을 가능하게 했습니다.\n\n이 주제는 매우 방대하기 때문에 이 기사에서 모두 다 다룰 수는 없을 거예요. 그러니 진지하게 관심이 있다면 이 기사에서 문장 포함 벡터에 대해 더 배울 수 있습니다.\n\n우리는 임베딩의 발전을 간략히 다뤘고 이론에 대한 고수준 이해를 얻었습니다. 이제 실습으로 넘어가서 OpenAI 도구를 사용하여 어떻게 임베딩을 계산하는지 배워보겠습니다.\n\n# 임베딩 계산\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 OpenAI 임베딩을 사용할 것입니다. 최근에 출시된 새로운 모델인 text-embedding-3-small을 시도해볼 것입니다. 이 새로운 모델은 text-embedding-ada-002보다 성능이 더 좋게 나타났습니다:\n\n- 널리 사용되는 다국어 검색 (MIRACL) 벤치마크의 평균 점수가 31.4%에서 44.0%로 상승했습니다.\n- 영어 작업에 대한 자주 사용되는 벤치마크인 MTEB의 평균 성능도 향상되어 61.0%에서 62.3%로 상승했습니다.\n\nOpenAI는 또한 새로운 큰 모델인 text-embedding-3-large를 출시했습니다. 이제 이것이 가장 우수한 임베딩 모델입니다.\n\n데이터 소스로는 Stack Exchange Data Dump의 작은 샘플을 사용할 것입니다. 이는 Stack Exchange 네트워크에서 모든 사용자 기여 콘텐츠의 익명화된 덤프입니다. 저는 흥미로운 주제를 선택하고 각각에서 100개의 질문을 샘플링했습니다. 주제는 생성적 AI부터 커피 또는 자전거까지 다양합니다. 그래서 다양한 주제를 볼 수 있을 겁니다.\n\n<div class=\"content-ad\"></div>\n\n먼저, 모든 스택 오버플로우 질문에 대한 임베딩을 계산해야 합니다. 한 번 실행하고 결과를 로컬로 저장하는 것이 좋습니다(파일이나 벡터 저장소에). OpenAI Python 패키지를 사용하여 임베딩을 생성할 수 있습니다.\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\ndef get_embedding(text, model=\"text-embedding-3-small\"):\n   text = text.replace(\"\\n\", \" \")\n   return client.embeddings.create(input = [text], model=model)\\\n       .data[0].embedding\n\nget_embedding(\"We are lucky to live in an age in which we are still making discoveries.\")\n```\n\n결과적으로, 우리는 부동 소수점 숫자로 이루어진 1536차원 벡터를 얻습니다. 이제 이를 모든 데이터에 대해 반복하고 값을 분석할 수 있습니다.\n\n가장 궁금할 수 있는 주요 질문은 의미적으로 문장들이 얼마나 가까운지입니다. 답을 발견하기 위해 벡터 간의 거리 개념을 논의해 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 벡터 간 거리\n\n임베딩은 사실 벡터입니다. 따라서 두 문장이 얼마나 가까운지 이해하려면 벡터 간 거리를 계산할 수 있습니다. 더 작은 거리는 더 가까운 의미를 나타낼 것입니다.\n\n두 벡터 간의 거리를 측정하는 데 사용할 수 있는 다양한 메트릭이 있습니다:\n\n- 유클리디안 거리 (L2),\n- 맨하탄 거리 (L1),\n- 내적 (Dot product),\n- 코사인 거리.\n\n<div class=\"content-ad\"></div>\n\n그들에 대해 이야기해 봅시다. 간단한 예로, 우리는 두 개의 2D 벡터를 사용할 것입니다.\n\n```js\nvector1 = [1, 4]\nvector2 = [2, 2]\n```\n\n## 유클리디안 거리 (L2)\n\n두 지점(또는 벡터) 사이의 거리를 정의하는 가장 표준적인 방법은 유클리디안 거리 또는 L2 norm입니다. 이 측정 기준은 일상생활에서 가장 많이 사용되며, 예를 들어 2개의 도시 사이의 거리를 언급할 때 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\nL2 거리에 대한 시각적 표현과 공식이 있습니다.\n\n![Image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_6.png)\n\n파이썬 또는 numpy 함수를 사용하여 이 메트릭을 계산할 수 있습니다.\n\n```python\nimport numpy as np\n\nsum(list(map(lambda x, y: (x - y) ** 2, vector1, vector2))) ** 0.5\n# 2.2361\n\nnp.linalg.norm((np.array(vector1) - np.array(vector2)), ord = 2)\n# 2.2361\n```\n\n<div class=\"content-ad\"></div>\n\n# 맨해튼 거리 (L1)\n\n다른 일반적으로 사용되는 거리 측정 방법은 L1 노름 또는 맨해튼 거리입니다. 이 거리는 뉴욕의 맨해튼 섬에서 명명되었습니다. 이 섬은 거리가 격자 레이아웃으로 되어 있고, 맨해튼에서 두 지점 사이의 가장 짧은 경로는 격자 모양을 따라야 하므로 L1 거리가 됩니다.\n\n![image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_7.png)\n\n우리는 이를 처음부터 구현하거나 numpy 함수를 사용하여 구현할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\nsum(list(map(lambda x, y: abs(x - y), vector1, vector2)))\r\n# 3\r\n\r\nnp.linalg.norm((np.array(vector1) - np.array(vector2)), ord = 1)\r\n# 3.0\r\n```\r\n\r\n## 내적(Dot product)\r\n\r\n벡터 간 거리를 계산하는 다른 방법은 내적 또는 스칼라 곱을 계산하는 것입니다. 다음은 해당 공식이며 쉽게 구현할 수 있습니다.\r\n\r\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_8.png\" />\n\n<div class=\"content-ad\"></div>\n\n```python\nsum(list(map(lambda x, y: x*y, vector1, vector2)))\n# 11\n\nnp.dot(vector1, vector2)\n# 11\n```\n\n이 메트릭은 해석하기가 조금 까다로운 편이에요. 한편으로는 벡터가 한 방향을 향하고 있는지를 보여줍니다. 다른 한편으로는 결과는 벡터들의 크기에 크게 의존합니다. 예를 들어 두 쌍의 벡터 간의 내적을 계산해볼게요:\n\n- (1, 1) vs (1, 1)\n- (1, 1) vs (10, 10).\n\n두 경우 모두 벡터가 일직선상에 있지만, 두 번째 경우에 내적은 10배 크게 나와요: 2 대 20.\n\n<div class=\"content-ad\"></div>\n\n## 코사인 유사도\n\n많은 경우, 코사인 유사도가 사용됩니다. 코사인 유사도는 벡터의 크기(또는 노름)에 의해 정규화된 내적입니다.\n\n![Image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_9.png)\n\n이전처럼 직접 모든 것을 계산하거나 sklearn의 함수를 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndot_product = sum(list(map(lambda x, y: x*y, vector1, vector2)))\nnorm_vector1 = sum(list(map(lambda x: x ** 2, vector1))) ** 0.5\nnorm_vector2 = sum(list(map(lambda x: x ** 2, vector2))) ** 0.5\n\ndot_product/norm_vector1/norm_vector2\n\n# 0.8575\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncosine_similarity(\n  np.array(vector1).reshape(1, -1), \n  np.array(vector2).reshape(1, -1))[0][0]\n\n# 0.8575\n```\n\ncosine_similarity 함수는 2차원 배열을 기대합니다. 그래서 numpy 배열을 reshape 해주어야 합니다.\n\n이 메트릭의 물리적 의미에 대해 조금 이야기해 봅시다. Cosine similarity는 두 벡터 사이의 코사인 값과 같습니다. 벡터가 서로 가까울수록 메트릭 값이 높아집니다.\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_10.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n우리는 심지어 벡터 사이의 정확한 각도를 도 단위로 계산할 수도 있어요. 약 30도 정도의 결과를 얻었고, 꽤 합리적으로 보이네요.\n\n```js\nimport math\nmath.degrees(math.acos(0.8575))\n\n# 30.96\n```\n\n## 어떤 측정 지표를 사용할까요?\n\n우리는 두 벡터 사이의 거리를 계산하는 다양한 방법에 대해 토론해 왔고, 여러분은 어떤 방법을 사용할지 고려하기 시작할 수 있을 거예요.\n\n<div class=\"content-ad\"></div>\n\n내가 가진 임베딩을 비교하기 위해 어떤 거리든 사용할 수 있어요. 예를 들어, 다른 클러스터 사이의 평균 거리를 계산했어요. L2 거리와 코사인 유사도 모두 비슷한 결과를 보여줘요:\n\n- 클러스터 내의 객체들은 다른 클러스터보다 서로 더 가까워요. L2 거리에 대해 가까울수록 낮은 거리를 의미하지만 코사인 유사도에서는 가까운 객체일수록 값이 높아져요. 헷갈리지 마세요.\n- \"정치\"와 \"경제\" 또는 \"ai\"와 \"데이터과학\"과 같이 일부 주제들이 서로 아주 가까운 것을 알 수 있어요.\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_11.png\" />\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_12.png\" />\n\n<div class=\"content-ad\"></div>\n\n그러나 NLP 작업에 대해서는 일반적으로 코사인 유사도를 사용하는 것이 최선의 방법입니다. 몇 가지 그 이유는:\n\n- 코사인 유사도는 -1과 1 사이에 있으며, L1과 L2는 무제한이기 때문에 해석하기 쉽습니다.\n- 실용적인 측면에서 유클리드 거리의 제곱근보다 내적을 계산하는 것이 더 효과적입니다.\n- 코사인 유사도는 차원의 저주에 영향을 덜 받습니다 (이에 대해 뒤에서 더 얘기할 것입니다).\n\n위의 결과에서 인트라 및 인터 클러스터 거리 간의 차이가 크지 않다는 점을 알 수 있을 것입니다. 이 현상의 원인은 벡터의 고차원성 때문입니다. 이 효과는 \"차원의 저주\"라고 불리며, 차원이 높을수록 벡터 간 거리 분포가 좁아진다는 것을 알 수 있습니다. 이에 대해 더 자세히 알아보려면 이 글을 참조해보세요.\n\n간단히 설명드리겠습니다. OpenAI 임베딩 값의 분포를 계산하고 차원이 다른 300개의 벡터 집합을 생성했습니다. 그런 다음, 모든 벡터 사이의 거리를 계산하고 히스토그램을 그렸습니다. 차원이 증가함에 따라 벡터의 거리 분포가 좁아진다는 것을 쉽게 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_13.png)\n\n임베딩 사이의 유사성을 측정하는 방법을 배웠어요. 여기서 이론적인 부분은 마쳤고, 더 실용적인 부분(시각화 및 실용적인 응용)으로 넘어가겠습니다. 데이터를 보는 것이 가장 중요하니, 시각화부터 시작해봐요.\n\n# 임베딩 시각화\n\n데이터를 이해하는 가장 좋은 방법은 시각적으로 나타내는 것이에요. 아쉽지만, 임베딩은 1536차원이 있어서 데이터를 살펴보기가 꽤 어려워요. 그러나, 한 가지 방법이 있어요: 차원 축소 기술을 사용하여 벡터를 이차원 공간에 투영하는 것이에요.\n\n<div class=\"content-ad\"></div>\n\n## PCA\n\n가장 기본적인 차원 축소 기술은 PCA(주성분 분석)입니다. 이를 사용해 봅시다.\n\n먼저, sklearn에 전달하기 위해 임베딩을 2D numpy 배열로 변환해야 합니다.\n\n```python\nimport numpy as np\nembeddings_array = np.array(df.embedding.values.tolist())\nprint(embeddings_array.shape)\n# (1400, 1536)\n```\n\n<div class=\"content-ad\"></div>\n\n그럼, 우리는 PCA 모델을 n_components = 2로 초기화해야 해요 (2D 시각화를 생성하고 싶기 때문에), 전체 데이터에서 모델을 학습하고 새로운 값을 예측해야 해요.\n\n```js\nfrom sklearn.decomposition import PCA\n\npca_model = PCA(n_components = 2)\npca_model.fit(embeddings_array)\n\npca_embeddings_values = pca_model.transform(embeddings_array)\nprint(pca_embeddings_values.shape)\n# (1400, 2)\n```\n\n결과적으로, 우리는 각 질문에 대해 두 개의 특성을 가진 행렬을 얻었으므로, scatter plot에서 쉽게 시각화할 수 있어요.\n\n```js\nfig = px.scatter(\n    x = pca_embeddings_values[:,0], \n    y = pca_embeddings_values[:,1],\n    color = df.topic.values,\n    hover_name = df.full_text.values,\n    title = 'PCA embeddings', width = 800, height = 600,\n    color_discrete_sequence = plotly.colors.qualitative.Alphabet_r\n)\n\nfig.update_layout(\n    xaxis_title = 'first component', \n    yaxis_title = 'second component')\nfig.show()\n```\n\n<div class=\"content-ad\"></div>\n\n![img](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_14.png)\n\n각 주제의 질문들이 서로 꽤 가까이 위치해 있는 것을 볼 수 있어 좋습니다. 그러나 모든 클러스터가 혼재되어 있어서 개선할 부분이 있습니다.\n\n## t-SNE\n\nPCA는 선형 알고리즘이지만, 대부분의 관계는 실제로는 비선형입니다. 그래서 비선형성 때문에 클러스터를 분리할 수 없을 수도 있습니다. 비선형 알고리즘인 t-SNE을 사용해보고 더 나은 결과를 보여줄 수 있는지 확인해봅시다.\n\n<div class=\"content-ad\"></div>\n\n거의 동일한 코드를 사용했습니다. PCA 대신 t-SNE 모델을 사용했어요.\n\n```js\nfrom sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, random_state=42)\ntsne_embeddings_values = tsne_model.fit_transform(embeddings_array)\n\nfig = px.scatter(\n    x = tsne_embeddings_values[:,0], \n    y = tsne_embeddings_values[:,1],\n    color = df.topic.values,\n    hover_name = df.full_text.values,\n    title = 't-SNE embeddings', width = 800, height = 600,\n    color_discrete_sequence = plotly.colors.qualitative.Alphabet_r\n)\n\nfig.update_layout(\n    xaxis_title = 'first component', \n    yaxis_title = 'second component')\nfig.show()\n```\n\nt-SNE 결과가 훨씬 좋아 보여요. 대부분의 클러스터가 분리되어 있지만 \"genai\", \"datascience\", \"ai\" 는 분리되지 않았어요. 그러나 이건 예상한대로에요 - 이러한 주제를 내가 분리할 수 있을지 의심스러워요.\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_15.png\" />\n\n<div class=\"content-ad\"></div>\n\n이 시각화를 보면 임베딩이 의미적 의미를 인코딩하는 데 상당히 효과적임을 확인할 수 있어요.\n\n또한, 데이터를 3D로 시각화할 수 있는 사영(projection)을 만들어볼 수 있어요. 실용적일지는 확실하지 않지만, 데이터를 3D로 살펴보는 것은 흥미롭고 관심을 끌 수 있어요.\n\n```js\ntsne_model_3d = TSNE(n_components=3, random_state=42)\ntsne_3d_embeddings_values = tsne_model_3d.fit_transform(embeddings_array)\n\nfig = px.scatter_3d(\n    x = tsne_3d_embeddings_values[:,0], \n    y = tsne_3d_embeddings_values[:,1],\n    z = tsne_3d_embeddings_values[:,2],\n    color = df.topic.values,\n    hover_name = df.full_text.values,\n    title = 't-SNE embeddings', width = 800, height = 600,\n    color_discrete_sequence = plotly.colors.qualitative.Alphabet_r,\n    opacity = 0.7\n)\nfig.update_layout(xaxis_title = 'first component', yaxis_title = 'second component')\nfig.show()\n```\n\n![3D 시각화](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_16.png)\n\n<div class=\"content-ad\"></div>\n\n## 바코드\n\n임베딩을 이해하는 방법은 몇 개를 바코드처럼 시각화하여 상관 관계를 확인하는 것입니다. 나는 세 가지 임베딩 예시를 선택했습니다: 두 개는 서로에게 가장 가깝고, 나머지 하나는 데이터 세트에서 가장 멀리 떨어져 있는 예시입니다.\n\n```js\nembedding1 = df.loc[1].embedding\nembedding2 = df.loc[616].embedding\nembedding3 = df.loc[749].embedding\n```\n\n```js\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nembed_len_thr = 1536\n\nsns.heatmap(np.array(embedding1[:embed_len_thr]).reshape(-1, embed_len_thr),\n    cmap = \"Greys\", center = 0, square = False, \n    xticklabels = False, cbar = False)\nplt.gcf().set_size_inches(15,1)\nplt.yticks([0.5], labels = ['AI'])\nplt.show()\n\nsns.heatmap(np.array(embedding3[:embed_len_thr]).reshape(-1, embed_len_thr),\n    cmap = \"Greys\", center = 0, square = False, \n    xticklabels = False, cbar = False)\nplt.gcf().set_size_inches(15,1)\nplt.yticks([0.5], labels = ['AI'])\nplt.show()\n\nsns.heatmap(np.array(embedding2[:embed_len_thr]).reshape(-1, embed_len_thr),\n    cmap = \"Greys\", center = 0, square = False, \n    xticklabels = False, cbar = False)\nplt.gcf().set_size_inches(15,1)\nplt.yticks([0.5], labels = ['바이오인포매틱스'])\nplt.show()\n```  \n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_17.png)\n\n우리 경우에는 고차원 때문에 벡터가 서로 가까운지 쉽게 보기 어려울 수 있습니다. 그래도 나는 이 시각화를 좋아합니다. 몇 가지 경우에 도움이 될 수도 있으니, 나는 이 아이디어를 당신과 공유하고자 합니다.\n\n우리는 임베딩을 시각화하는 방법을 배웠고, 텍스트의 의미를 파악하는 능력에 대한 의문은 남지 않았습니다. 이제 실제로 임베딩을 어떻게 활용할 수 있는지에 대해 논의하는 가장 흥미로운 부분으로 넘어가 보겠습니다.\n\n# 실용적인 응용\n\n<div class=\"content-ad\"></div>\n\n물론, 임베딩의 주요 목표는 텍스트를 숫자의 벡터로 인코딩하거나 시각화하기 위해서만 하는 것이 아닙니다. 우리는 텍스트의 의미를 포착하는 능력에서 많은 이점을 얻을 수 있습니다. 실용적인 예제들을 함께 살펴보겠습니다.\n\n## 클러스터링\n\n먼저 클러스터링부터 시작해보죠. 클러스터링은 초기 레이블 없이 데이터를 그룹으로 분할할 수 있는 비지도학습 기술입니다. 클러스터링을 통해 데이터의 내부 구조적 패턴을 이해하는 데 도움을 받을 수 있습니다.\n\n가장 기본적인 클러스터링 알고리즘 중 하나인 K-평균을 사용할 것입니다. K-평균 알고리즘을 위해서는 클러스터의 개수를 지정해야 합니다. 실루엣 스코어를 사용하여 최적의 클러스터 수를 정의할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n2부터 50까지의 k (클러스터 수)를 시도해 보겠습니다. 각 k에 대해 모델을 훈련하고 실루엣 점수를 계산할 것입니다. 실루엣 점수가 높을수록, 더 좋은 클러스터링 결과를 얻을 수 있습니다.\n\n```python\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport tqdm\n\nsilhouette_scores = []\nfor k in tqdm.tqdm(range(2, 51)):\n    kmeans = KMeans(n_clusters=k, \n                    random_state=42, \n                    n_init='auto').fit(embeddings_array)\n    kmeans_labels = kmeans.labels_\n    silhouette_scores.append(\n        {\n            'k': k,\n            'silhouette_score': silhouette_score(embeddings_array, \n                                                 kmeans_labels, metric='cosine')\n        }\n    )\n\nfig = px.line(pd.DataFrame(silhouette_scores).set_index('k'),\n              title='<b>K-means 클러스터링을 위한 실루엣 점수</b>',\n              labels={'value': '실루엣 점수'}, \n              color_discrete_sequence=plotly.colors.qualitative.Alphabet)\nfig.update_layout(showlegend=False)\n```\n\n우리의 경우, k가 11일 때 실루엣 점수가 최대치에 도달합니다. 따라서 최종 모델에는 이 클러스터 수를 사용합시다.\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_18.png\" />\n\n<div class=\"content-ad\"></div>\n\n클러스터를 시각화해 보는 t-SNE를 이용한 차원 축소를 이미 이전에 수행한 것처럼 해보겠습니다.\n\n```js\ntsne_model = TSNE(n_components=2, random_state=42)\ntsne_embeddings_values = tsne_model.fit_transform(embeddings_array)\n\nfig = px.scatter(\n    x = tsne_embeddings_values[:,0], \n    y = tsne_embeddings_values[:,1],\n    color = list(map(lambda x: '클러스터 %s' % x, kmeans_labels)),\n    hover_name = df.full_text.values,\n    title = '클러스터링을 위한 t-SNE 임베딩', width = 800, height = 600,\n    color_discrete_sequence = plotly.colors.qualitative.Alphabet_r\n)\nfig.update_layout(\n    xaxis_title = '첫 번째 성분', \n    yaxis_title = '두 번째 성분')\nfig.show()\n```\n\n시각적으로 알고리즘이 클러스터를 상당히 잘 정의했음을 확인할 수 있습니다 — 그들은 꽤 잘 분리되어 있습니다.\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_19.png\" />\n\n<div class=\"content-ad\"></div>\n\n우리는 사실적인 주제 라벨을 가지고 있으므로, 클러스터링이 얼마나 좋은지를 심층적으로 판단할 수도 있어요. 각 클러스터에 대한 주제 혼합을 살펴봅시다.\n\n```js\ndf['cluster'] = list(map(lambda x: '클러스터 %s' % x, kmeans_labels))\ncluster_stats_df = df.reset_index().pivot_table(\n    index='cluster', values='id',\n    aggfunc='count', columns='topic').fillna(0).applymap(int)\n\ncluster_stats_df = cluster_stats_df.apply(\n  lambda x: 100*x/cluster_stats_df.sum(axis=1))\n\nfig = px.imshow(\n    cluster_stats_df.values, \n    x=cluster_stats_df.columns,\n    y=cluster_stats_df.index,\n    text_auto='.2f', aspect=\"auto\",\n    labels=dict(x=\"클러스터\", y=\"팩트 주제\", color=\"비율, %\"),\n    color_continuous_scale='pubugn',\n    title='<b>각 클러스터의 주제 비율</b>', height=550)\n\nfig.show()\n```\n\n<img src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_20.png\" />\n\n대부분의 경우, 클러스터링은 완벽하게 작동했어요. 예를 들어, 클러스터 5에는 거의 자전거에 관한 질문만 있고, 클러스터 6은 커피에 관한 것이에요. 그러나 유사한 주제를 구별하지 못했어요:\n\n<div class=\"content-ad\"></div>\n\n- \"ai,\" \"genai,\" and \"datascience\"은 동일한 클러스터에 있습니다.\n- \"economics\"와 \"politics\"은 같은 그룹에 속합니다.\n\n이 예제에서는 피처로써 임베딩만 사용했지만, 질문을 한 사용자의 나이, 성별 또는 국가와 같은 추가 정보가 있다면 모델에 포함시킬 수도 있습니다.\n\n## 분류\n\n임베딩을 분류 또는 회귀 작업에 사용할 수 있습니다. 예를 들어 고객 리뷰 감정을 예측하는 (분류)이나 NPS 점수를 예측하는 (회귀) 등 다양한 작업에 활용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n분류 및 회귀는 지도 학습이므로 레이블이 필요합니다. 다행히도, 우리는 질문의 주제를 알고 있으므로 모델을 적합시켜 예측할 수 있습니다.\n\n저는 랜덤 포레스트 분류기를 사용할 것입니다. 랜덤 포레스트에 대해 간단히 상기하고 싶다면 여기에서 확인할 수 있어요. 분류 모델의 성능을 올바르게 평가하려면 데이터 세트를 학습 및 테스트 세트(80% 대 20%)로 분할할 것입니다. 그런 다음 학습 데이터 세트에서 모델을 훈련하고 테스트 데이터 세트에서 품질을 측정할 수 있습니다(모델이 이전에 보지 못한 질문).\n\n```js\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nclass_model = RandomForestClassifier(max_depth = 10)\n\n# 특징 및 대상 정의\nX = embeddings_array\ny = df.topic\n\n# 데이터를 학습 및 테스트 세트로 분할\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state = 42, test_size=0.2, stratify=y\n)\n\n# 적합 및 예측\nclass_model.fit(X_train, y_train)\ny_pred = class_model.predict(X_test)\n```\n\n모델의 성능을 추정하기 위해 혼동 행렬을 계산해 보겠습니다. 이상적인 상황에서는 비대각 요소가 모두 0이어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nfig = px.imshow(\n  cm, x = class_model.classes_,\n  y = class_model.classes_, text_auto='d', \n  aspect=\"auto\", \n  labels=dict(\n      x=\"predicted label\", y=\"true label\", \n      color=\"cases\"), \n  color_continuous_scale='pubugn',\n  title = '<b>혼동 행렬</b>', height = 550)\n\nfig.show()\n```\n\n![이미지](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_21.png)\n\n군집화와 유사한 결과를 확인할 수 있습니다. 일부 주제는 쉽게 분류되고 정확도가 100%인 반면, 다른 주제들은 구별하기 어려운 경우도 있습니다(특히 \"ai\" 주제).\n\n하지만 전체적으로 91.8%의 정확도를 달성했으며, 이는 꽤 좋은 성과입니다.\n\n<div class=\"content-ad\"></div>\n\n## 이상 징후 찾기\n\n데이터에서 이상 징후를 찾기 위해 임베딩을 사용할 수도 있습니다. 예를 들어, t-SNE 그래프에서 \"여행\" 주제에 대한 몇 가지 질문이 군집에서 꽤 멀리 떨어져 있는 것을 볼 수 있었습니다. 이 테마를 살펴보고 이상 징후를 찾아보겠습니다. 이를 위해 이상 탐지 알고리즘인 Isolation Forest를 사용할 것입니다.\n\n```js\nfrom sklearn.ensemble import IsolationForest\n\ntopic_df = df[df.topic == 'travel']\ntopic_embeddings_array = np.array(topic_df.embedding.values.tolist())\n\nclf = IsolationForest(contamination=0.03, random_state=42)\ntopic_df['is_anomaly'] = clf.fit_predict(topic_embeddings_array)\n\ntopic_df[topic_df.is_anomaly == -1][['full_text']]\n```\n\n여기에서, 여행 주제에 대한 가장 흔하지 않은 댓글을 찾았습니다 (원본).\n\n<div class=\"content-ad\"></div>\n\n```js\n로마 구역의 곳곳에 있는 분수에서 물을 마셔도 안전한가요?\n\n로마를 방문했을 때 오래된 지역을 거닐며 다양한 종류의 분수를 보았습니다. 물이 끊임없이 흘러나오는 분수들이 많았는데, 땅으로 흘러가는 분수도 있고, 대야에 모이는 분수도 있었습니다.\n\n이런 분수에서 나오는 물을 마셔도 괜찮을까요? 방문객이 마실 수 있는 안전한 물일까요? 분수 사용에 대한 방문자들이 알아야 할 예절이 있을까요?\n```\n\n물에 관한 이야기이기 때문에 이 주석의 기능은 사람들이 물을 따르는 커피 주제와 밀접하게 관련되어 있습니다. 그래서 이 주석의 삽입 표현은 커피 클러스터와 꽤 가까운 것으로 보입니다.\n\nt-SNE 시각화에서 찾아보면 실제로 커피 클러스터에 가까운 것을 확인할 수 있습니다.\n\n![이미지](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_22.png)\n\n\n<div class=\"content-ad\"></div>\n\n## RAG — 검색 증가 생성\n\n최근 LLM의 인기가 높아지면서, 임베딩이 RAG 사용 사례에서 널리 사용되고 있습니다.\n\n우리는 많은 문서가 있는 경우(예: 스택 오버플로우의 모든 질문)에 검색 증가 생성이 필요합니다. 그리고 모든 정보를 항상 LLM에 전달할 수 없기 때문에\n\n- LLM은 컨텍스트 크기에 제한이 있습니다(현재 GPT-4 Turbo의 경우 128K입니다).\n- 우리는 토큰을 구매해야 하므로 모든 정보를 항상 전달하는 것이 더 비십니다.\n- LLM은 더 큰 컨텍스트에서 성능이 떨어집니다. 자세한 내용은 \"바늘 찾기\" - LLM의 압력 테스트를 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n대규모 지식 베이스와 함께 작업할 수 있도록 RAG 방법론을 활용할 수 있어요:\n\n- 모든 문서에 대한 임베딩을 계산하고 벡터 저장소에 저장합니다.\n- 사용자 요청을 받으면 해당 요청의 임베딩을 계산하여 저장소에서 관련 문서를 검색할 수 있어요.\n- 최종 답변을 얻기 위해 LLM에게 관련 문서만 전달하면 돼요.\n\nRAG에 대해 더 자세히 알고 싶다면 여기에 더 많은 내용을 담은 제 논문을 읽어보세요.\n\n# 요약\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 텍스트 임베딩에 대해 많은 세부 내용을 논의했습니다. 이제 여러분은 이 주제에 대해 완전하고 심도 있는 이해를 가졌을 것입니다. 저희 여정을 간단히 요약하면 다음과 같습니다:\n\n- 먼저, 텍스트 작업 방법의 진화를 살펴보았습니다.\n- 그 다음으로, 텍스트 간에 유사한 의미를 가지고 있는지를 이해하는 방법에 대해 논의했습니다.\n- 그 후에는 텍스트 임베딩 시각화의 다양한 접근 방법을 살펴보았습니다.\n- 마지막으로, 임베딩을 클러스터링, 분류, 이상 탐지 및 RAG와 같은 다양한 실용적인 작업에서 특징으로 사용해 보았습니다.\n\n# 참고\n\n이 기사에서는 크리에이티브 커먼즈 라이센스 하에 공개된 스택 엑스체인지 데이터 덤프에서 데이터 세트를 사용했습니다.\n\n<div class=\"content-ad\"></div>\n\n이 글은 다음 강좌에서 영감을 받았습니다:\n\n- DeepLearning.AI와 Google Cloud의 협력으로 진행되는 \"Understanding and Applying Text Embeddings\",\n- DeepLearning.AI와 Weaviate의 협력으로 진행되는 \"Vector Databases: From Embeddings to Applications\".","ogImage":{"url":"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_0.png"},"coverImage":"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_0.png","tag":["Tech"],"readingTime":24},{"title":"심리 상담 챗봇  정신 건강을 위한 LLMs 사용 방법","description":"","date":"2024-06-23 19:51","slug":"2024-06-23-TherapistChatbotLLMsforMentalHealth","content":"\n\n<img src=\"/assets/img/2024-06-23-TherapistChatbotLLMsforMentalHealth_0.png\" />\n\n요즘 빠르게 변화하는 세상에서 우리의 정신 건강을 우선시할 시간을 찾기는 어려울 수 있어요. 업무, 가족, 그리고 일상 생활의 요구들은 종종 자기 관리를 할 시간을 남기지 않아서 정기적인 치료 세션은 물론 스스로를 돌보는 시간을 가져다주지 않을 때가 많아요. 시간을 내기로 결심했다고 해도, 자격 있는 치료사의 이용 가능성이 부족하면 예약을 기다리는 사람들이 많아질 수 있어요. 그리고 마침내 소중한 세션이 확보되더라도, 비용이 부담스러울 수 있어 이미 부담스러운 마음에 금전적인 압박을 느끼게 해요.\n\n이 딜레마는 정신 건강 치료 분야에서 혁신적인 해결책에 대한 점점 더 커지는 필요성을 강조해요. 진출하게 된 것이 바로 언어 모델(Language Models, LLMs)을 기반으로 하는 치료사 챗봇들의 시대입니다. 이 가상 상담가들은 우리가 스마트폰이나 컴퓨터의 편안함 속에서 24시간 365일 즉각적인 지원을 받을 수 있도록하여 우리가 지원을 받는 방식을 혁신하려고 합니다.\n\n인공 지능(AI)과 자연어 처리(NLP)를 활용하여, 이 챗봇들은 실제 치료 대화를 시뮬레이션할 수 있어요. 이들은 공감, 지도, 그리고 개인의 필요에 맞게 맞춤형 대처 전략을 제공할 수 있어요. 이러한 접근 방식은 특히 사회적 편견이나 물리적인 장벽 때문에 전통적인 치료를 받기 주저하는 분들에게 접근하는 데 매우 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n그러나 중요한 점은 심리상담사 챗봇은 정신 건강 관리를 보완할 수 있지만 전문 상담의 대체품이 아니라는 것입니다. 훈련받은 심리상담사들의 세밀한 이해력과 공감적인 대응을 갖추지 못합니다. 그들은 교육과 경험에 기초한 맞춤형 치료를 제공할 수 있는 훈련받은 상담사들의 심리 마음을 나타내지 못합니다.\n\n심리건강 관리에 LLMs를 통합하는 목표는 접근성을 증대하고 확대하는 데 있습니다. 더 나은 심리적 안녕을 향한 여정을 하는 사람들에게 발판을 제공합니다. 심리 건강 문제에 직면한 모든 사람들에게 전문적인 도움을 구하는 것이 중요한 단계이며, 이러한 혁신은 자기 돌봄과 지원을 위한 우리 도구상자에 유망한 추가 요소로 작용합니다.\n\n그런데, 이 기술을 활용하여 우리 자신의 심리 상담사 AI를 만들어보는 방법에 대해 깊이 파고들어 보겠습니다!\n\n전체 프로젝트와 함께 제 GitHub는 다음에서 확인할 수 있습니다 — https://github.com/Dev-Pandey-0302/Therapist-Chatbot\n\n<div class=\"content-ad\"></div>\n\n유튜브의 Nicholas Renotte에 대한 큰 찬사를 드립니다 (Nicholas Renotte를 검색해보세요, 놀라운 데이터 과학자!)\n\n# 코딩을 시작합니다.\n\n이 프로젝트의 핵심은 Llama_cpp 프레임워크를 기반으로 구축될 것입니다. 간단한 접근 방식은 ollama를 사용하는 것일 수 있지만, 이 안내서에서는 Llama_cpp를 사용하는 데 중점을 둘 것입니다.\n\n먼저, 우리는 ~특정 부분이 누락되었습니다~\n\n<div class=\"content-ad\"></div>\n\n```js\r\ngit clone https://github.com/ggerganov/llama.cpp\r\n```\r\n\r\n원하는 기계에 llama_cpp를 설치할 거예요.\r\n\r\n다음으로, make 명령을 실행해야 해요:\r\n\r\n- Mac: cd llama.cpp && make\r\n- Windows (from here):\r\n\n\n<div class=\"content-ad\"></div>\n\n- 최신 Fortran 버전의 w64devkit을 다운로드하세요.\n- PC에 w64devkit을 압축 해제하세요.\n- w64devkit.exe를 실행하세요.\n- cd 명령어를 사용하여 llama.cpp 폴더로 이동하세요.\n- 여기서\n\n```js\nmake\n```\n\n위 명령어를 실행하세요. 그 후에는 의존성을 설치합니다. 가능하다면 의존성을 설치하기 전 가상 환경을 만들어도 좋습니다. 가상 환경을 만드는 방법을 모르신다면 아래 링크를 확인해보세요- https://www.freecodecamp.org/news/how-to-setup-virtual-environments-in-python/\n\n```js\npip install openai 'llama-cpp-python[server]' pydantic instructor streamlit gtts\n```\n\n<div class=\"content-ad\"></div>\n\n# GGUF 모델 다운로드\n\nGGUF가 무엇인가요?\n\n물어봐 주셔서 감사합니다!\n\n이 애플리케이션에서는 다음을 사용할 것입니다-\n\n<div class=\"content-ad\"></div>\n\nWesselvanGils/MentaLLaMA-chat-7b-GGUF-q8\n\n이는 8비트 양자화된 GGUF 모델입니다. 따라서 전용 서버가 필요하지 않고 로컬 머신에서 실행할 수 있습니다.\n\n이 모델을 오픈 소스로 만들어준 Wessel van Gils에게 감사드립니다. 여기에 그들의 GitHub이 있습니다!\n\n더 자세한 정보를 알고 싶다면 자유롭게 이 글을 읽어보세요.\n\n<div class=\"content-ad\"></div>\n\n모델 다운로드로 돌아가기\n\n여기로 이동해주세요- https://huggingface.co/WesselvanGils/MentaLLaMA-chat-7b-GGUF-q8\n\n파일 및 버전 탭을 클릭하고 .gguf 파일을 다운로드하세요. 용량이 7GB가 넘는 파일이라 시간이 조금 걸릴 수 있으니 참고해 주세요.\n\n# app.py 파일 만들기\n\n<div class=\"content-ad\"></div>\n\n이제 app.py 파일을 만들어 봅시다.\n\n이 튜토리얼에서는 streamlit을 사용할 것입니다. 그러나 gradio와 같은 대체 솔루션을 사용해도 괜찮습니다.\n\n```python\nfrom openai import OpenAI\nfrom gtts import gTTS\nfrom io import BytesIO, StringIO\n# streamlit 앱 프레임워크 사용\nimport streamlit as st\n\n# 클라이언트 생성\nclient = OpenAI(\n    api_key=\"sk-1234567890\",\n    base_url='http://localhost:8000/v1'\n)\n\n# 앱의 제목\nst.title(\"TherapyBot- Mental Health Support를 위한 챗봇\")\n\n# 의료 기록 업로드\nuploaded_file = st.file_uploader(\"\", type=[\"txt\"], label_visibility=\"collapsed\")\ncss = '''\n<style>\n    [data-testid='stFileUploader'] {\n        width: max-content;\n    }\n    [data-testid='stFileUploader'] section {\n        padding: 0;\n        float: left;\n    }\n    [data-testid='stFileUploader'] section > input + div {\n        display: none;\n    }\n    [data-testid='stFileUploader'] section + div {\n        float: right;\n        padding-top: 0;\n    }\n\n</style>\n'''\nst.markdown(css, unsafe_allow_html=True)\n\ndoc_data = \"\"\n# 파일 업로드 시\nif uploaded_file is not None:\n    # 파일을 문자열로 읽기\n    stringio = StringIO(uploaded_file.getvalue().decode(\"utf-8\"))\n    doc_data = stringio.read()\n    doc_data = \"This is my medical record - \" + doc_data + \" Please answer the following question based on the earlier medical record- \"\n    \n# 사용자 입력 받기\nprompt = st.chat_input('채팅을 시작하거나 의료 기록을 업로드하십시오. 어떻게 도와드릴까요?')\n...\n```\n\n이 특정 app.py 파일은 구글의 gtts 라이브러리를 사용한 텍스트 음성 변환 기능도 제공합니다. 이를 위해서는 인터넷 연결이 필요하지만, 오프라인으로 완전히 실행하려면 gtts를 import하지 않고 마지막 4줄을 주석 처리하세요.\n\n<div class=\"content-ad\"></div>\n\n이 파일을 저장한 후 로컬 서버를 실행해 보겠습니다.\n\n# 라마.cpp 서버\n\n터미널에서 다음을 실행하면 서버가 가동될 것입니다. 다운로드한 GGUF 모델의 경로가 올바른지 확인하세요.\n\n```js\npython -m llama_cpp.server --model D:\\CHATBOT_PROJ_NEW\\MentaLLaMA-chat-7b-GGUF-q8\\MentaLLaMA-chat-7b-GGUF-q8.gguf --n_gpu -1\n```\n\n<div class=\"content-ad\"></div>\n\n# App.py 실행\n\n로컬호스트 서버가 시작된 후, 별도의 터미널에서 다음 명령을 실행하세요\n\n```js\nstreamlit run app.py\n```\n\n축하합니다!\n\n<div class=\"content-ad\"></div>\n\n지역 컴퓨터에서 실행 중인 자체 치료사 AI를 만들었습니다. .txt 파일을 제공하든 일반적인 질문을 하든 자유롭게 진행해주세요!\n\n![이미지](/assets/img/2024-06-23-TherapistChatbotLLMsforMentalHealth_1.png)\n\n읽어 주셔서 감사합니다!","ogImage":{"url":"/assets/img/2024-06-23-TherapistChatbotLLMsforMentalHealth_0.png"},"coverImage":"/assets/img/2024-06-23-TherapistChatbotLLMsforMentalHealth_0.png","tag":["Tech"],"readingTime":5},{"title":"컴퓨터는 왜 이진수Binary 체계를 사용할까","description":"","date":"2024-06-23 19:49","slug":"2024-06-23-WhydoComputersevenuseBinary","content":"\n\n# 소개적인 쓰레기\n\n데이터 과학자의 주된 도구는 무엇인가요? 공정하고 명백한 답은 컴퓨터입니다. 컴퓨터는 우리보다 훨씬 빠르게 데이터를 처리하기 때문에 데이터를 활용하는 모든 작업을 컴퓨터 없이 하는 것을 상상해 보세요. 곁에 연필과 종이가 있어 손으로 수많은 표와 그래프, 계산식을 지쳐하며 그리는 과정을 생각해 보세요. 이 모든 작업은 컴퓨터에서 몇 초만에 처리될 것을 알고 있지만요. 데이터 과학은 컴퓨터 없이는 존재하지 않을 것이라고 하더군요.\n\n컴퓨터가 데이터 과학을 수행하는 데 필수적인 도구라는 사실을 감안하면, 해당 직업에는 컴퓨터 작동 원리를 이해하는 것이 필요할 것으로 예상됩니다. 결국, 도구를 이해하지 못할 때 작업을 올바르게 수행하는 것은 어려운 일입니다. 그러나 지망하는 데이터 과학자에게는 해당 분야에 대한 기초 지식을 습득하는 데 시간을 할애하는 것이 너무 쉬울 수도 있습니다.\n\n![그림](/assets/img/2024-06-23-WhydoComputersevenuseBinary_0.png)\n\n<div class=\"content-ad\"></div>\n\n알겠어요. 수학, 컴퓨터 과학 및 공학의 이론적 주제는 종종 Python, Tensorflow 또는 Amazon Web Services와 같은 응용 중심 학습 목표보다 즉각적으로 보상을 받기가 어려워서(또는 솔직히 말해 즉각적으로 고용가능하지 않아서) 부차적인 학습 목표로 밀려납니다. 신진 데이터 과학자가 왜 이러한 종류의 주제로 향하는지 이해할 수밖에 없죠. 이러한 주제는 프로젝트 포트폴리오가 빠르게 성장함에 따라 기술과 이해력의 즉각적이고 실질적인 향상을 제공하기 때문입니다.\n\n데이터 과학 여정을 시작하는 데 좋은 시작점이기는 하지만, 더 이론적인 측면을 학습하면 시작 단계를 넘어 다음 단계를 나아갈 수 있습니다. 이 문서의 목적은 당신에게 데이터 과학에서 매일 사용하는 기술의 이론적 배경을 소개하는 데 있습니다.\n\n좋아요, 이제 소개가 끝났으니 재미있는 내용으로 들어가볼게요. 우리는 아마도 잠깐이라도 생각해 봤을 질문으로 시작할 거에요: 컴퓨터가 왜 이진수를 사용할까요? 결과부터 말하자면: 그렇지 않으면 비효율적일 것이기 때문입니다. 이 문서의 목적은 왜 그것이 비효율적인지 설명하는 것입니다.\n\n# 이진수란 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n이미 이진수가 무엇인지를 알고 계시다면, 이 섹션을 건너 뛰셔도 좋아요. 아니라면, 이 내용이 귀하를 위한 것입니다!\n\n상상해보세요. 만약 우리가 지적인 외계 생명체를 만난다면. 손가락이 5개가 아니라 3개씩 두 손에 각각 있는 특이한 생물이 있다고 상상해보세요. 그들은 어떻게 세어갈까요? 우리는 1부터 9까지의 숫자를 사용하지만, 그 이후에는 첫 번째 숫자를 0으로 되돌리고 다음 자리를 1부터 열어 10을 만듭니다. 그러나 우리의 외계 친구들은 이것이 별로 이해하기 어렵다고 생각할지도 모릅니다. 왜냐하면 그들은 6개의 손가락밖에 없고 우리는 10개이기 때문입니다. 당신이 숫자에 할당하는 자리값이 얼마나 임의적인지 알게 되고 있나요? 만약 사람들이 10개의 손가락을 태어나지 않았다면, 우리는 아마 우리가 하는 방식으로 세지 않을 것입니다. 우리는 10부터 새로운 숫자를 시작하기 대신에, 6, 2, 16, 46 또는 다른 어떤 숫자든, 각 값을 나타내는 고유한 기호가 있으면 됩니다. 외계인이 어떻게 세는지와 우리가 어떻게 세는지 비교해 봅시다. 각 행에 있는 값은 서로 동일합니다:\n\n![image](/assets/img/2024-06-23-WhydoComputersevenuseBinary_1.png)\n\n우리는 이러한 새로운 숫자 시스템을 'X 진수' 숫자 시스템이라고 합니다. 여기서 X는 새로운 자리로 넘어가는 값입니다. 예를 들어, 우리의 숫자 시스템은 각 자리가 10보다 더 커지기 전에 끝나므로 10 진수 시스템이라고 합니다. 반면에, 우리의 외계인 친구들은 6 진수 시스템을 갖게 될 것입니다. 왜냐하면 그들의 숫자는 6보다 작아지기 때문입니다. 컴퓨터는 2 진수 시스템에서 작동하며 바이너리로 알려져 있습니다. 바이너리 시스템에 적용된 외계인의 6 진수 시스템에 적용된 동일한 계산 논리를 적용해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n\n![Binary Explanation](/assets/img/2024-06-23-WhydoComputersevenuseBinary_2.png)\n\n만약 그 이해가 당신에게 의미가 있다면, 이진법이 어떻게 작동하는지 이해하고 있는 것입니다. 너무 겁나지 않았으면 좋겠어요! 많은 사람들이 이것을 \"언어\"로 지칭해서 겁을 먹는데, 이진법은 그냥 다른 숫자 체계일 뿐이며, 우리가 익숙한 10진법처럼 작동합니다. 산술도 기본적으로 우리 체계에서 하는 것과 동일하게 작동합니다.\n\n우리 숫자가 10의 거듭제곱의 합으로 나타낼 수 있는 것과 같이, 이진수는 2의 거듭제곱의 합으로 나타낼 수 있습니다. 설명하는 가장 좋은 방법은 예를 통해 설명하는 것입니다. (참고로, 이진수는 숫자 앞에 0b를 붙여서 표시되기도 합니다. 이 글에서는 이 방식으로 계속해서 표시할 것입니다).\n\n10진법에서\n\n\n<div class=\"content-ad\"></div>\n\n`<img src=\"/assets/img/2024-06-23-WhydoComputersevenuseBinary_3.png\" />`\n\n마찬가지로, 2진법의 경우,\n\n`<img src=\"/assets/img/2024-06-23-WhydoComputersevenuseBinary_4.png\" />`\n\n위의 합산을 수행하면, ob1101은 13이 됩니다. 2진수를 10진수로 변환하는 쉬운 방법입니다.\n\n<div class=\"content-ad\"></div>\n\n# 왜 컴퓨터는 이진수를 사용할까요?\n\n이제 이진수가 어떻게 작동하는지 기본적으로 이해했으니, 이 기사의 중심 질문에 대해 다룰 수 있습니다. 컴퓨터가 정보를 이진수로 표현하는 이유는 무엇일까요? 컴퓨터가 정보를 이진수로 표현해야 하는 유일한 방법이기 때문은 아닙니다. 이것은 흔한 오해입니다. 사실, ENIAC과 같은 최초의 컴퓨터들 중 일부는 10진수를 사용했습니다. 현대 컴퓨터가 정보를 표현할 때 이진수를 사용하는 큰 이유는 세 가지가 있습니다.\n\n# 첫 번째 이유, 공간 효율성\n\n우리가 이진수 체계를 사용하는 큰 이유 중 하나는 다른 체계보다 간단하기 때문입니다. 간단히 말하면, 오로지 2 상태(0 또는 1)만을 표현하는 것은 더 쉽고 더 적은 물리적 부품이 필요합니다. 이것이 왜 그런지 이해하려면, 컴퓨터와 같은 디지털 시스템이... 음... 디지털이라는 것을 이해해야 합니다. 즉, 컴퓨터는 연속적인 수 대신 숫자의 이산적 표현을 사용하는 하드웨어를 사용합니다. 수학 수업에서 기억할 수 있겠지만, 이산적인 수는 정수(0, 1, -4 등)이고 연속적인 수는 정수와 각 소수부(0 1, -4, .2, -.2343 등)가 있는 수입니다. 그러나 컴퓨터는 전기를 사용하여 동작하는 실제 시스템이기 때문에 전압 수준을 정보로 표현하는 경우가 많습니다. 전압은 연속적인 값이며(3.22682393 볼트인 것이 가능합니다), 컴퓨터는 디지털입니다 — 오직 이산적인 값만을 사용하는 방식을 알고 있습니다. 어떻게 하면 연속적인 값(전압)을 디지털 시스템으로 표현할 수 있을까요? 우리는 전압 값을 디지털 값에 상응하는 전압 범위로 설정하여 그 방법을 찾습니다. 아래는 그러한 설정의 예시입니다:\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-WhydoComputersevenuseBinary_5.png)\n\n상기 도표를 사용하는 회로는 0부터 2 V 사이의 전압을 0b0으로 간주하고, 4V부터 6 V 사이를 0b1로, 그 이외의 값은 시스템이 문제가 있다는 것을 알려줍니다. 중간에 전환 존이 포함되어 있는 것을 주목하세요. 물리적 시스템에 대해 생각하는 것에 익숙하지 않다면, 우리가 0과 1 사이를 즉시 왔다갔다할 수 있어야 할 것 같다는 것이 이상할 수 있습니다. 그러나 물리적 시스템에서는 오차 여유를 만들어야 합니다. 그렇지 않으면 시스템이 필요 이상으로 매우 취약해질 수 있습니다. 토론을 벌이지는 않지만, 디지털 회로는 논리 게이트로 이루어져 있고, 논리 게이트는 켜고 끄기 위한 공간이 필요합니다. 이 낮음과 높음 사이의 전압 범위는 게이트가 \"종류\"로 켜진 상태입니다. 이 범위에 머무르면 구성 요소에 손상을 줄 수 있으므로, 구성 요소를 쉬게 두지 않는 전환 존을 계획합니다.\n\n만약 3진법 시스템을 만들고 싶다면, 또 다른 상태를 추가해야 합니다:\n\n![이미지](/assets/img/2024-06-23-WhydoComputersevenuseBinary_6.png)\n\n\n<div class=\"content-ad\"></div>\n\n자연스럽게 다른 상태를 추가하려면 더 많은 하드웨어 구성 요소가 필요합니다. 이는 더 높은 기수 체계를 사용하는 시스템을 설계할 때 하드웨어 면에서 복잡해지는 점을 의미합니다(설계 및 구성 요소 수 측면에서). 이것이 컴퓨터가 이진수를 사용하는 근본적인 이유입니다. 그러나 높은 기수 체계를 사용하는 것이 더 중요한 두 가지 영향을 주목해 보겠습니다.\n\n# 두 번째 이유, 전력 효율성\n\n전기 전력을 정의하는 것에 대해 잠시 이야기해 봅시다. 방정식은 정말로 간단합니다:\n\n![전기 전력](/assets/img/2024-06-23-WhydoComputersevenuseBinary_7.png)\n\n<div class=\"content-ad\"></div>\n\nP는 전력(Watts), I는 전류(Amps), R은 저항(Ohms), V는 전압(Voltz)를 나타냅니다. 각 새로운 논리 수준마다 전압을 높일수록, 전체 장치의 전력 소비가 지수적으로 증가함에 따라 동등한 전압이 가정되며, 이는 대부분의 경우 합리적인 가정입니다. 소비 전력이 많을수록 장치를 실행하는 데 소비되는 비용이 더 많아지므로 대부분의 경우 이러한 시스템을 피하는 이유가 큽니다.\n\n# 세 번째 이유, 열 효율성\n\n랩톱이 한 두 시간 동안 열심히 작동한 후 뒷면을 만져본 적이 있나요? 아마도 많은 열을 느꼈을 것입니다. 과도한 열은 전자제품의 적이며, 그 생성을 피하는 것이 엔지니어들에게 우선 과제입니다. 열에 관한 방정식을 살펴 보세요:\n\n![Heat Equation](/assets/img/2024-06-23-WhydoComputersevenuseBinary_8.png)\n\n<div class=\"content-ad\"></div>\n\nQ는 열(쥴), t는 시간(초), 그리고 R과 I는 다시 저항과 전류를 나타냅니다. 회로에 사용하는 각 새로운 구성 요소는 작동에 더 많은 전류를 그리는 결과를 가져올 것입니다. 이에 따라 열이 증가합니다.\n\n# 결론적인 발언\n\n이 기사에서는 컴퓨터가 왜 이진수를 사용하는지(이진수가 무엇인지 소개하는 것 외에도) 다루었습니다. 우리는 이것이 비효율적이라는 것을 알고 있고, 그 이유에 대한 메커니즘인 전압, 전류, 그리고 전력 등의 개념을 소개했습니다. 컴퓨팅에서 다른 숫자 체계를 사용하려면 디지털 논리 회로로 더 많은 상태를 나타내야 한다는 것을 보았습니다. 그 결과 전력과 회로 구성 요소가 더 필요하며 더 많은 열을 발생시키면서, 엔지니어와 소비자가 피하려고 하는 것입니다.\n\n하지만, 우리는 실제로 컴퓨터가 이 정보를 어떻게 나타내는지를 정말 다루지 않았습니다 — 그저 디지털 회로를 사용한다고 말했을 뿐입니다. 다음 몇 개의 기사에서는 이 질문을 더 자세히 탐구할 것입니다. 디지털 논리 회로가 어떻게 작동하는지 설명해 보겠습니다 — 가능한 최하위 수준인 반도체부터 시작해서요. 이에 관심이 있다면, 많이 기대해 주세요! 이 기사와 같이 저는 전기 공학 개념을 아는 것으로 상정하지 않고 성장 중인 데이터 과학자들을 대상으로 하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n아마도 당신은 새로운 데이터 과학자이거나 이미 경험 많은 데이터 과학자이실 것입니다. 이 기사가 흥미롭게 느껴졌으면 좋겠습니다. 컴퓨터 하드웨어가 데이터 과학이나 머신 러닝 개념과 정확히 관련이 없지만, 이것들은 우리의 전문 분야 도구이며, 이에 대한 기본적인 이해를 가지는 것이 가치 있다고 생각합니다. 동의하시고 이 기사가 흥미롭다고 생각하셨다면, 다음에는 실제 컴퓨터 계산 방식에 대해 다룰 예정이니, 그때 뵐 수 있기를 희망합니다!\n\n# 참고자료\n\n[1] R. Palaniappan, Digital Systems Design (2011), https://dvikan.no/ntnu-studentserver/kompendier/digital-systems-design.pdf","ogImage":{"url":"/assets/img/2024-06-23-WhydoComputersevenuseBinary_0.png"},"coverImage":"/assets/img/2024-06-23-WhydoComputersevenuseBinary_0.png","tag":["Tech"],"readingTime":7},{"title":"주식 예측에서 머신러닝이 실패하는 주요 이유 파트 01","description":"","date":"2024-06-23 19:47","slug":"2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01","content":"\n\n이 블로그 시리즈에서는 머신 러닝이 주식 가격을 예측하는 데 실패하는 이유 또는 일반적으로 머신 러닝 기반 투자 펀드가 실패하는 이유에 대해 논의하려 합니다. 이 블로그의 내용은 Marcos Lopez de Prado의 \"금융 머신 러닝 발전\"이라는 책에서 가져왔습니다. 이 책은 금융에 관심 있는 모든 사람들에게 필독서입니다. 이 책은 금융 데이터를 처리하는 동안 머신 러닝 실무자들이 범한 모든 실수를 언급합니다. 이 블로그를 통해 이 책에서의 학습 내용을 요약하려고 합니다.\n\n- Reason 1 : 메모리 vs 정상성 트레이드 오프 :\n\n주식의 가격을 예측하고 싶다고 가정해 봅시다. ARMA와 같은 모든 전통적인 방법이 정상성 데이터에 작용한다는 것을 알고 있습니다. 일반적으로 정상 데이터는 일련의 시리즈 전체에서 일정한 평균과 일정한 분산을 의미합니다. 데이터를 정상으로 만들기 위해 시계열 데이터에서 차분을 수행합니다. 1차 차분은 현재 주가 값을 이전 값에서 뺀 것을 의미합니다.\n\n![Image](/assets/img/2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01_0.png)\n\n<div class=\"content-ad\"></div>\n\n차이를 만들면 데이터가 정체성을 띄게 됩니다. 그러나 1차 차이를 구할 때는 데이터의 모든 과거적인 패턴을 잃어버리게 됩니다(그림 참조). 이로 인해 그 데이터의 내용을 잃게 됩니다. 그리고 기억력은 모델의 예측 능력을 결정하는 중요한 요소입니다. 데이터를 정체성을 갖도록 만드는 중간 과정에서 기억력을 잃게 됩니다. 이런 실수가 학술 논문이나 업무 현장에서 많이 발생합니다. 그렇다면 어떻게 데이터를 정체성을 갖게 하면서도 정보를 완전히 잃지 않을 수 있을까요? 그 대답은 없습니다. 따라서 더 많이 정체성을 갖도록 만드는 경우에는 더 많은 기억력을 잃게 됩니다. 릴라이언스의 과거 가격을 통해 이러한 경우를 이해해봅시다.\n\n![image](/assets/img/2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01_1.png)\n\n위 그래프에서 보듯이 데이터를 정체성을 갖도록 만들면 가격이 시간에 따라 어떻게 변동하는지에 대한 정보를 모두 잃고 따라서 그 기억력도 잃게 됩니다. 그리고 이에 따라 그런 데이터로 만든 모델의 예측 능력도 잃게 됩니다. 그렇다면 어떻게 해야 할까요? 부분적인 정체성을 달성하면서 부분적인 기억력을 잃지 않도록 하는 방법을 찾아야 합니다. 그렇게 되면 분수 차이화라는 개념이 등장합니다. 분수 차이화에서는 1차 차이화 대신 어떤 분수 값을 사용하여 차이화를 수행하게 되어 모든 정보를 완전히 잃지 않습니다. 그렇다면 분수 차이화를 어떻게 수행할까요? 함께 살펴봅시다.\n\n![image](/assets/img/2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01_2.png)\n\n<div class=\"content-ad\"></div>\n\n위에서 보시다시피 주문 차이를 확장했습니다. 이제 만약 d=0.3의 값을 넣으면 0.2의 분수 차이 값이 나올 것입니다. 이것은 무한급수이므로 어떤 지점까지 값을 취할 수 있고, 그 이후에는 시리즈를 잘라내도 괜찮습니다. 왜냐하면 B^n 계수 값이 높아질수록 거의 제로에 가까워질 것이기 때문입니다. 아래 그래프는 일정 지점 이후에 서로 다른 d 값에 대한 B^n 계수를 보여줍니다.\n\n이제 동일한 플롯을 동일한 차수의 분수 차이로 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n상단 차트에서 확인할 수 있듯이, 차분의 순서를 증가할수록 점점 더 많은 메모리를 잃고 더욱 안정화됩니다. 차분이 0일 때는 모든 메모리를 가지고 있지만, 시리즈는 안정적이지 않고 차분이 1일 때는 시리즈가 메모리를 가지고 있지 않지만 완전히 안정화됩니다. 그래서 우리는 어느 정도의 메모리를 잃으면서 데이터를 의미 있는 확률로 안정화할지 교환해야 합니다. 그래서 이제 d값을 어떻게 찾아야 할까요?\n\n시계열의 안정성을 위한 ADF(Augmented Dickey-Fuller) 검정이 관련됩니다. ADF 검정은 시리즈가 안정적인지 여부를 테스트하는 데 사용됩니다. 다양한 d 값에 대해 ADF 검정을 수행한 후, 아래 빨간색 선 그래프는 다양한 d 값에 대한 검정 통계 값입니다. 검정 통계 값이 수평선보다 낮다면 해당 시리즈가 안정적이라고 할 수 있습니다. 따라서 아래 차트에서 d=0.4가 시리즈를 안정화시키는 데 충분하다고 할 수 있습니다. 그래서 시리즈에서 최소한의 메모리 손실로 안정화된 시리즈를 얻으려면 d=0.4를 사용할 수 있습니다.\n\n2. 이유 2: 비효율적인 샘플링\n\n<div class=\"content-ad\"></div>\n\n다른 많은 실무자와 학술 논문 작성자들이 하는 또 다른 흔한 실수는 데이터 샘플링이 비효율적인 것입니다. 대부분의 경우 그들은 데이터를 시간 간격마다 샘플링합니다. 예를 들어, 5분마다 또는 10분마다 데이터를 샘플링합니다. 시간 프레임에 기반한 데이터 샘플링 시 주요 문제점이 있습니다.\n\n- 시장이 정규시간 간격에 맞춰 정보를 처리하지 않기 때문에 문제가 발생합니다. 예를 들어 시장은 오픈할 때보다 정오에 활동성이 높으므로, 높은 활동성 시간 동안 정보를 과소샘플링하고 낮은 활동성 시간 동안 정보를 과대샘플링합니다.\n- 시간 샘플링된 데이터는 연쇄상관, 이분산성 및 수익의 비정상성과 같은 부정적인 통계적 특성을 보입니다.\n\n이 문제를 극복하기 위해 다양한 바(bar)가 정의될 것입니다. \n\n- 틱 바(Tick bar): 타임스탬프, 거래량, 오픈 가격, 종가 등의 모든 변수를 일정 거래 횟수 이후 추출합니다. 예를 들어, 1000 거래가 이루어진 후 모든 변수를 샘플링합니다. Mandelbrot 및 Taylor [1967]은 거래 횟수에 따른 샘플링이 우수한 통계적 특성을 보인다는 것을 처음으로 깨달았습니다: \"고정된 거래 횟수에 따른 가격 변동이 가우시안 분포를 가질 수 있습니다.\"\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01_6.png)\n\n![이미지](/assets/img/2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01_7.png)\n\n- 볼륨 바: 틱 바는 주문 조각화(쪼개짐) 문제가 있습니다. 예를 들어, 어떤 가격에 어떤 가격에 10주를 매도하는 경우, 10주를 사 실 때 1틱으로 기록됩니다. 하지만 만약 1주를 10번 사면 10개의 거래로 기록됩니다. 이 문제를 해결하기 위해 일정 거래량이 발생한 후 정보를 샘플링합니다. 이것이 볼륨 바라고 알려져 있습니다.\n- 달러 바: 달러 바는 일정 거래가 발생한 후 정보를 샘플링하여 형성됩니다. 예를 들어, $5000의 거래가 발생한 후에 정보를 샘플링합니다. \"값\"은 반드시 $로만 측정되는 것은 아니고, Rs, 유로 등이 될 수 있습니다. 달러 바가 필요한 이유는 무엇일까요? 특정 기간 동안 100%의 평가 상승을 보인 주식을 분석하려고 할 때, 그 기간 끝에 $1,000가치의 그 주식을 판매하려면, 그 주식을 $1,000가치 살 때와는 반의 주식을 거래해야 합니다. 다시 말해, 거래된 주식 수는 실제 교환된 가치에 따라 결정됩니다. 그러므로, 주요 가격 변동이 있는 분석에 관여할 때, 거래된 값의 관점에서 바를 샘플링하는 것이 의미가 있습니다. 또 다른 주장은 보너스 주식, 주식 분할로 주식 수가 자주 변경되므로 거래량보다는 가격을 기준으로 샘플링하는 것이 더 합리적이라는 점입니다.\n\n다음 파트에서는 정보 주도의 일부 고급 정보 바에 대해 이야기해보겠습니다. 즉, 시장에 새로운 정보가 들어오면 정보를 샘플링하는 것을 의미합니다. 다음 블로그 시리즈에서 정보 주도형 바에 대해 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n이 블로그를 좋아하신다면 꼭 의겢이나 좋아요를 클릭해 주세요!\n\n참고:\n\n1) Marcos López de Prado의 금융 기계 학습 발전","ogImage":{"url":"/assets/img/2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01_0.png"},"coverImage":"/assets/img/2024-06-23-majorreasonswhymachinelearningfailsinstockpredictionpart-01_0.png","tag":["Tech"],"readingTime":5},{"title":"컴퓨터는 실제로 어떻게 계산할까","description":"","date":"2024-06-23 19:45","slug":"2024-06-23-HowDoComputersActuallyCompute","content":"\n\n# 소개글\n\n컴퓨터가 숫자를 계산하는 장치라는 말은 항상 듣고 다닙니다. 그들은 인간이 희망하기 힘든 속도로 데이터를 처리하며, 초당 수천 개의 논리적인 결정을 내릴 수 있다는데요. 하지만 그들이 이를 물리적으로 어떻게 하는 걸까요? 이러한 매우 비싼 기기가 자신들의 창조자들을 (어느 면에서는) 능가할 수 있는 무엇이 그리 특별한 걸까요? 이 질문에 우리는 이 기사에서 다룰 것입니다.\n\n![이미지](/assets/img/2024-06-23-HowDoComputersActuallyCompute_0.png)\n\n이것은 내가 쓰는 새로운 시리즈, \"데이터 과학자를 위한 컴퓨터 하드웨어 입문\"의 두 번째 부분입니다. 물리학, 전기 공학 또는 저수준 컴퓨터 과학 관련 지식을 전제로 하지 않는 이 시리즈는 새로운 (또는 경험이 풍부한) 데이터 과학자가 전문 도구에 대한 이해를 깊이 있게 하고자 합니다. 결국, 어떤 뛰어난 장인이 자신의 도구에 대해 익숙하지 않겠습니까?\n\n<div class=\"content-ad\"></div>\n\n시리즈의 이전 설치 파일을 따라 가기 위해서는 필요하지 않아요 (원한다면 다음에서 읽을 수 있어요: 컴퓨터는 왜 이진수를 사용할까요?). 컴퓨터가 어떻게 작동하는지 궁금하지만 해당 분야에 대한 배경이 많이 없다면 이 시리즈는 시작하기에 좋은 장소가 될 거예요.\n\n멋진데요, 입문부터 떨어뜨렸어요. 이제 재미있는 부분으로 들어가볼까요?\n\n# 가장 기본적인 디지털 회로 요소: 트랜지스터\n\n디지털 회로에서 가장 기본적인 요소는 트랜지스터에요. 트랜지스터는 반도체 기반 구성 요소로 스위치처럼 작용할 수 있어요. 반도체(예를 들면 실리콘)는 특정 조건 하에서 전기를 전도하고, 다른 조건 하에서 전기를 차단하는 재료들의 한 종류에요. 반도체 재료를 재빠르게 활용함으로써 회로 구성 요소를 \"켜고\" \"끌\" 수 있어요. (트랜지스터로 가능한 다른 것들도 있지만, 이 기사에서는 다루지 않을 거예요). 재료를 \"재빠르게 활용\"하는 방법은 반도체로 트랜지스터를 만드는 것이에요.\n\n<div class=\"content-ad\"></div>\n\n트랜지스터는 베이스(base), 콜렉터(collector), 그리고 에미터(emitter)의 세 개의 I/O 위치를 가지고 있어요. 아래의 트랜지스터 다이어그램에서 콜렉터는 상단에 있고, 베이스는 중간 왼쪽에 위치하며, 에미터는 하단에 위치합니다. 이런 트랜지스터의 배치는 NPN (부정-긍정-부정) BJT(양극성 접합 트랜지스터)라고 불립니다. BJTs보다 더 인기 있는 많은 종류의 트랜지스터가 있어요. 실제로 디지털 응용에서 사용되는 트랜지스터는 주로 FETs (장효과 트랜지스터)이고, BJTs는 아닙니다. 그렇지만 기술적 세부사항이 약간 다르더라도 최종 결과는 대부분 동일합니다 — 모든 트랜지스터는 여전히 스위치 역할을 합니다. BJTs가 약간 더 간단하기 때문에, 이 점에 중점을 두겠어요.\n\n![트랜지스터 다이어그램](/assets/img/2024-06-23-HowDoComputersActuallyCompute_1.png)\n\n트랜지스터에 관한 한 가지를 기억해주세요. 베이스에 약간의 전기를 가하면 콜렉터에서 에미터로 더 많은 양의 전기가 흐를 수 있습니다. 이를 수도꼭지를 켜고 끄는 것처럼 생각해보세요. 우리는 스지(Ge이미터)를 통해 물(전기)이 물 공급원(콜렉터)에서 나가게 하기 위해 수도선을 돌리는 것처럼, 베이스에 약간의 전기를 가해서 전류를 제어합니다. BJTs의 경우, 우리는 전류를 다루어 전류 흐름을 조절합니다. FETs의 경우, 전압을 다루어 전압 \"흐름\"을 조절합니다. 그러나 두 방법 모두 개념은 동일합니다.\n\n좋아요, 이제 우리는 트랜지스터가 무엇이며, 스위치를 만들기 위해 반도체를 사용한다는 것을 이해했습니다. 우리는 베이스에 약간의 전기를 제어하여 콜렉터와 에미터 간의 전기 흐름을 켜고 끕니다. 믿든 안 믿든, 이 아이디어 — 스위치 — 가 컴퓨터 안에서 일어나는 모든 것의 기본 구성 요소입니다. 우리는 트랜지스터를 사용하여 논리 회로를 설계할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n# 논리 게이트: 컴퓨터가 어떻게 계산하는지\n\n논리 게이트는 기본 논리 기능을 구현하는 회로 요소들로 (다소 자명하게) 설계됩니다. 논리 게이트의 예로는 AND, OR, NOT 게이트가 있습니다. 논리 게이트의 출력은 입력 조건이 충족될 때 켜지고, 그렇지 않으면 꺼집니다.\n\n우리가 깊게 들어가기 전, 전기를 데이터로 어떻게 이해하는지 간단히 다시 살펴보겠습니다. 디지털 회로에 대해 이야기할 때, 보통 0을 나타내는 전압 범위와 1을 나타내는 전압 범위가 있습니다. 간단히 말해, 입력/출력에 전기가 적게 있다면 해당 입력/출력 값이 0임을 이해합니다. 전기가 조금 이상 있다면 1로 간주합니다. 이를 통해 이진수로 회로 작업을 수행할 수 있습니다. (이전 기사인 '컴퓨터가 왜 이진수를 사용할까?'를 읽었으면, 더 자세히 다룬 내용일 것입니다). 입력 또는 출력의 전기 양 대신 입력 또는 출력의 0 또는 1에 대해 이야기함으로써 문제를 단순화합니다. 그러나 이러한 숫자들이 전기 양으로 모델링된다는 것을 잊지 마세요.\n\n(부가 설명 — 간단히 설명하기 위해 약간 우외해주고 있습니다. 전압은 단순히 전기의 양이라고 말하기 정확하지 않습니다. 그러나 두 가지를 동일시함으로써 전체 개념을 조금 더 쉽게 이해하고, 디지털 논리 회로 작동 방식을 이해하는 데 지장이 없게 합니다.)\n\n<div class=\"content-ad\"></div>\n\n대박이네요! 이제 이해하셨으니 논리 게이트로 빠져들어볼까요! 예를 들어, AND 게이트를 살펴봅시다.\n\n![AND Gate](/assets/img/2024-06-23-HowDoComputersActuallyCompute_2.png)\n\n왼쪽에 있는 두 개의 도일은 입력이고, 오른쪽에 있는 도일이 출력입니다. AND 게이트 안에는 잘 배치된 수많은 트랜지스터가 있지만, 그것에 대해서는 나중에 다뤄보겠습니다. AND 게이트는 어떻게 작동할까요? 입력 두 곳에 모두 1(전기가 많음)이 있을 때 출력이 1이 됩니다. 그렇지 않으면 출력은 0이 됩니다. 이는 파이썬의 \"and\" 키워드와 마찬가지로 작동합니다! 사실, 파이썬(그리고 어떤 프로그래밍 언어든, 정말로)은 실제로 “and”, “or”, “not”과 같은 기능을 수행하기 위해 논리 게이트를 사용합니다. 멋지지 않나요? 파이썬에서 “and”를 호출하면 실제로 컴퓨터의 CPU 내부에 있는 단일 논리 게이트를 사용하고 있는 것이죠! 이 하드웨어 구성요소를 (거의) 직접 사용해오셨는데 그것을 몰랐다니 대단하시네요!\n\n<div class=\"content-ad\"></div>\n\n디지턈 회로에 대해 이야기할 때는 입력과 출력에 대해 정확하게 설명할 방법이 필요합니다. 이를 위해, 참진리표(truth table)라 불리는 것을 사용합니다. 여기 AND 게이트의 참진리표가 있어요.\n\n![AND Gate Truth Table](/assets/img/2024-06-23-HowDoComputersActuallyCompute_3.png)\n\n2개의 입력이 각각 2가지 상태를 가지기 때문에 입력에 대한 2² = 4개의 다른 배열이 있습니다. 따라서 참진리표는 4행을 가지게 됩니다. 우리는 두 입력이 모두 1이면 켜지는(1인) 회로를 원했기 때문에 참진리표에 1을 확인할 수 있습니다.\n\nAND 게이트의 경우, 참진리표는 명백하지만 더 복잡한 회로의 경우, 참진리표는 입력을 출력에 매핑하는 회로를 추적하는 데 매우 유용한 도구입니다.\n\n<div class=\"content-ad\"></div>\n\n## 논리 게이트의 하부 구조\n\n좋아요, 이제 논리 게이트가 어떻게 작동하는지 알게 되었어요. 그리고 트랜지스터의 기본 개념도 이해했군요. 이제 이 둘을 어떻게 결합하여 AND 게이트를 만들 수 있는지 알아볼까요? 아래의 회로를 살펴보세요.\n\n![image](/assets/img/2024-06-23-HowDoComputersActuallyCompute_4.png)\n\n위쪽에 전기원이 있습니다. 기억하세요, 트랜지스터는 스위치처럼 작동하기 때문에 소스에서 전기가 흐를 수 있기 위해서는 해당 트랜지스터의 베이스에서 전기가 존재해야 합니다. 양 입력에 각각 1이 입력되면 어떻게 될까요? 소스에서 출력까지 전기가 흐를 수 있기 때문에 출력 값은 1이 됩니다. 그러나 어느 하나의 트랜지스터라도 꺼져 있다면 (즉, 입력 중 하나에 0이 입력된 경우), 전기가 차단되어 출력 값이 0이 됩니다. 이는 AND에 대한 참 진리표와 일치합니다! 이제 AND의 논리를 구현하는 물리적인 구성 요소를 가졌습니다! OR 및 NOT에 대해서도 비슷한 설계 과정을 따를 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n사이드 노트: 그렇다고 계속 괴롭힐 필요는 없어요 — 이 회로가 AND를 위한 실제 회로보다는 조금 더 간단합니다. 실제로, 원하는대로 전기를 흘리기 위해 저항기와 전기 접지와 같은 몇 가지 전기 부품이 필요합니다. 그러나 이 간단화된 버전은 일어나는 대부분의 것을 대표하고 있으며 주된 개념도 전달하고 있습니다.\n\n# 논리 게이트 결합: 복잡한 계산\n\n그래서, 우리는 트랜지스터 수준에서 논리 게이트가 어떻게 동작하는지 알고 있습니다. AND, OR, NOT와 같은 함수를 구현할 수 있습니다. 그러나 거기서부터 더 복잡한 것으로 어떻게 나아가야 할까요? 결국, 논리는 컴퓨터가 수행할 수 있는 유일한 계산이 아닙니다. 저희는 데이터 과학을 할 때 덧셈, 곱셈, 나눗셈 등을 자주 진행하니까 뭔가 더 있어야 한다는 것이 맞죠.\n\n이러한 고차 함수 중 많은 것들이 논리 게이트를 사용하여 구현될 수 있다는 것이 밝혀졌습니다. 예를 들어, 덧셈을 살펴보죠. AND와 XOR 두 개의 게이트만 사용합니다. XOR은 베타식 OR의 약자입니다. 입력 중 하나라도 1이면 XOR의 출력이 켜집니다. 그러나 모두 1이거나 아무도 아니면 출력은 0이 됩니다. XOR의 진리표는 다음과 같습니다.\n\n<div class=\"content-ad\"></div>\n\n아래 서킷을 살펴보세요:\n\n![circut](/assets/img/2024-06-23-HowDoComputersActuallyCompute_6.png)\n\n만약 하실 마음이 있다면 아래 진리표를 스스로 채워보세요. 머리 속에서 채워도 괜찮아요. 결과물이 궁금하시겠죠?\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-HowDoComputersActuallyCompute_7.png)\n\n두 입력이 모두 0 인 경우에는 어떻게 되나요? 그럼 AND 와 XOR가 꺼져서, Output과 Carry가 모두 꺼집니다.\n\n![이미지](/assets/img/2024-06-23-HowDoComputersActuallyCompute_8.png)\n\n0 과 1 은 어떤가요? XOR이 켜져 있고 AND는 꺼져 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n아래는 마크다운 형식으로 변경해야 합니다.\n\n\n<img src=\"/assets/img/2024-06-23-HowDoComputersActuallyCompute_9.png\" />\n1 and 0 would be the same, then.\n\n<img src=\"/assets/img/2024-06-23-HowDoComputersActuallyCompute_10.png\" />\nWhat about 1 and 1? The XOR is off and AND is on!\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-HowDoComputersActuallyCompute_11.png\" />\n\n이진 덧셈 회로의 진리표를 보셨나요? Carry는 다음 자리로 1을 옮기는 것을 나타내며 (일반 덧셈에서 1의 자리에서 10의 자리로 1을 옮기는 것과 같습니다), 출력은 해당 자리의 값을 나타냅니다! 수동으로 확인해볼 수도 있어요: 1 + 1 = 2. 2를 이진수로 표현하면 0b10입니다. 1은 Carry로, 0은 출력으로 표현됩니다.\n\n1비트 더하는 회로는 두 개의 간단한 게이트로 표현할 수 있어요. 더 많은 비트를 추가하려면 동일한 더하는 회로를 더 이어붙이기만 하면 돼요 (이를 캐스케이딩이라고 해요). 논리 게이트와 진리표를 사용하여 다양하고 중요한 회로를 만들어낼 수 있어요.\n\n# 마무리맺음\n\n<div class=\"content-ad\"></div>\n\n이번 글에서는 트랜지스터가 논리 게이트를 만드는 데 어떻게 사용되는지 다루었으며, 논리 게이트가 결합되어 컴퓨터가 수행하는 계산 기능을 많이 생성하는 방법도 다뤘습니다. 아직 궁금증이 남아 계신다면 댓글에서 질문해 주세요! 하나 더 언급하지 못한 점은 진리 표에서 회로 설계로 어떻게 이어지는지 입니다. 이 글은 하드웨어 작동 방식보다는 설계 프로세스에 더 초점을 맞추었기 때문에 여기에는 잘 맞지 않는 부분이었습니다. 그러나 답변을 드리겠습니다. - 부울 대수를 사용합니다. 이 시리즈의 후속 섹션에서 부울 대수에 대해 다룰 예정입니다.\n\n이로써 마무리 지었습니다! 본 글은 저의 컴퓨터 하드웨어 시리즈 중 두 번째 글입니다. 다음에는 컴퓨터가 실제로 기억하는 방법에 대해 이야기할 예정입니다!\n\n이 시리즈를 좋아하시나요? 개선할 점이 있나요? 다루어보기를 원하는 하드웨어 주제가 있나요? 깊이 들어갈 부분이 있나요? 과하게 다루어진 부분이 있나요? 댓글에서 제안해 주시면 대답해 드리겠습니다. 피드백은 이 글이 하드웨어 배경이 없는 독자를 위해 적절한 수준에서 쓰여져 있는지, 그리고 흥미롭고 교육적인 내용으로 써졌는지 확인하는 데 도움이 됩니다. 읽어 주셔서 감사합니다. 다음에 뵙겠습니다!","ogImage":{"url":"/assets/img/2024-06-23-HowDoComputersActuallyCompute_0.png"},"coverImage":"/assets/img/2024-06-23-HowDoComputersActuallyCompute_0.png","tag":["Tech"],"readingTime":7}],"page":"6","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true}