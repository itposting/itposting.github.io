{"pageProps":{"posts":[{"title":"2024년을 위해 효과적으로 사용하는 방법과 함께 관리자 대시보드의 상위 10가지 유형","description":"","date":"2024-06-19 15:58","slug":"2024-06-19-Top10TypesofAdminDashboardsandHowtoUseThemEffectivelyin2024","content":"\n\n여러 종류의 관리자 대시보드 및 사용 방법을 발견하세요. 귀하는 비즈니스에 적합한 것을 선택하고 효율성을 극대화하기 위해 우리의 포괄적인 안내서를 통해 배울 수 있습니다.\n\n![대시보드 이미지](/assets/img/2024-06-19-Top10TypesofAdminDashboardsandHowtoUseThemEffectivelyin2024_0.png)\n\n관리자 대시보드는 현대 비즈니스 관리에서 중요한 역할을 합니다. 기업의 운영 다양한 측면을 모니터링, 분석, 및 제어하기 위한 집중된 플랫폼을 제공합니다. 본 문서에서는 다양한 관리자 대시보드의 유형 및 실용적인 비즈니스 관리 응용을 탐색할 것입니다. 이러한 대시보드는 데이터 시각화 및 인력 관리와 같은 특정 비즈니스 요구를 해결하도록 설계되었습니다. 우리는 중요한 기능을 논의하고 효과적인 와이어프레임 생성에 대한 통찰을 제공하며, 판매 및 인력 자원 부문 통합의 중요성을 강조할 것입니다. 함께 하여 관리자 대시보드 유형과 실용적인 사용 예시를 탐험해 보세요.\n\n# 목차\n\n<div class=\"content-ad\"></div>\n\n- 관리자 대시보드와 사용자 대시보드 구별하기\n- 관리자 대시보드의 장점\n- 관리자 대시보드 유형\n- 관리자 대시보드 와이어프레임 만들기\n\n# 관리자 대시보드와 사용자 대시보드의 구별\n\n시스템이나 애플리케이션 내에서 관리자 대시보드와 사용자 대시보드는 각각 다른 목적을 가지고 있습니다. 관리자 대시보드는 시스템의 다양한 측면을 모니터링, 관리, 제어하기 위해 포괄적인 접근이 필요한 관리자나 관리자 사용자를 위해 디자인되었습니다. 관리자 대시보드는 주로 데이터 관리, 사용자 권한 관리, 분석 도구 및 시스템 구성 옵션과 같은 고급 기능을 제공합니다.\n\n반면, 사용자 대시보드는 주로 특정 작업을 수행하거나 사용자의 역할이나 선호도에 맞게 제공되는 데이터, 기능 및 기능성에 중점을 둔 일반 사용자나 최종 사용자를 위해 맞춤형으로 설계되었습니다. 사용자 대시보드는 종종 간단한 디자인을 가지고 있으며 개별 사용자 역할이나 선호도에 맞게 제공하는 관련 데이터, 기능 및 기능성에 집중합니다.\n\n<div class=\"content-ad\"></div>\n\n# 관리자 대시보드의 장점\n\n![이미지](/assets/img/2024-06-19-Top10TypesofAdminDashboardsandHowtoUseThemEffectivelyin2024_1.png)\n\n관리자 대시보드는 결정을 내리고 전반적인 효율성을 향상시키는 여러 가지 혜택을 제공합니다. 이러한 혜택 중 일부는 다음과 같습니다:\n\n# 1. 사용자 관리:\n\n<div class=\"content-ad\"></div>\n\n- 세밀한 권한 설정: 기본 역할(관리자/편집자/사용자)을 넘어서 특정 작업을 제어하는 세밀한 권한 집합을 구현하여 다양한 엔티티(데이터 포인트, 콘텐츠 유형, 기능성)에서 사용자가 수행할 수 있는 행동을 제어하세요.\n- 사용자 활동 로그: 대시보드 내에서 사용자의 활동을 추적하여 활동을 모니터링하고 잠재적인 문제를 식별하며 규정 준수를 확보하세요.\n\n## 2. 콘텐츠 관리:\n\n- 버전 관리: 콘텐츠에 대한 변경 사항을 추적할 수 있는 기능을 제공하여 필요한 경우 이전 버전으로 되돌릴 수 있고 콘텐츠 편집에 협력할 수 있도록 합니다.\n- 워크플로우 관리: 콘텐츠 승인 프로세스를 위한 워크플로우를 구현하여 편집 사항이 게시되기 전에 검토 및 승인이 필요하도록 만듭니다.\n\n## 3. 사용자 정의:\n\n<div class=\"content-ad\"></div>\n\n- 드래그 앤 드롭 기능: 관리자가 대시보드 위젯을 쉽게 정렬하고 조직하여 개인화된 화면 및 작업 효율성을 향상시킵니다.\n- 사전 구축 대시보드: 일반 사용자 역할 또는 기능에 대한 사전 구성된 대시보드를 제공하며 사용자 지정 생성 기능도 함께 제공합니다.\n\n## 4. 설정:\n\n- 데이터 유지 보관 정책: 대시보드 내 사용자 데이터, 활동 로그 및 기타 정보의 데이터 저장 기간을 관리하는 정책을 구현합니다.\n- 감사 로깅: 설정과 관련된 시스템 구성 변경 및 사용자 작업을 추적하여 책임 소재를 유지하고 잠재적 보안 위험을 식별합니다.\n\n## 5. 추가 고려 사항:\n\n<div class=\"content-ad\"></div>\n\n- 사용자 인터페이스 (UI) 디자인: 사용자 관리, 콘텐츠 편집 및 사용자 정의를 위한 UI를 관리자가 다양한 기술 능력을 가지고 사용해도 직관적이고 사용하기 쉽도록 보장합니다.\n- 확장성: 사용자 제어 및 편집 기능을 디자인하여 시스템 내에서 성장하는 사용자 기반 및 데이터 양을 처리할 수 있게 합니다.\n\n무료로 Mokkup을 사용해보세요!\n\n# 관리자 대시보드 유형\n\n![Admin Dashboard Types](/assets/img/2024-06-19-Top10TypesofAdminDashboardsandHowtoUseThemEffectivelyin2024_2.png)\n\n<div class=\"content-ad\"></div>\n\n관리 대시보드는 시스템이나 플랫폼의 특정 요구 사항에 따라 다양할 수 있습니다. 다음은 일반적인 유형의 관리 대시보드입니다:\n\n## 1. 콘텐츠 관리 시스템 (CMS) 대시보드:\n\n- 주요 포인트: 이 대시보드를 통해 관리자는 웹 사이트 콘텐츠를 관리할 수 있습니다. 페이지, 포스트 또는 기타 콘텐츠 유형의 생성, 편집 및 삭제가 가능합니다.\n- 도구: WordPress, Drupal, Wix\n\n## 2. 사용자 관리 대시보드:\n\n<div class=\"content-ad\"></div>\n\n- 주요 포인트: 이 유형의 대시보드는 사용자 관리 작업에 초점을 맞추어 사용자 계정 생성, 편집, 삭제, 권한 관리 및 사용자 관련 문제 처리와 같은 작업을 다룹니다.\n- 도구: 구글 워크스페이스 관리 콘솔, 마이크로소프트 365 관리 센터, 옥타\n\n# 3. 분석 대시보드:\n\n- 주요 포인트: 분석 대시보드는 시스템이나 플랫폼의 다양한 측면에 대한 통찰과 메트릭을 제공합니다. 사용자 활동, 트래픽 소스, 전환율 등을 포함합니다. 관리자는이 정보를 활용하여 데이터 기반 의사결정을 할 수 있습니다.\n- 도구: 구글 애널리틱스, 어도비 애널리틱스, 클리키\n\n# 4. 전자상거래 대시보드:\n\n<div class=\"content-ad\"></div>\n\n- 주요 포인트: 전자상거래 관리자 대시보드는 온라인 상점을 관리할 수 있도록 설계되었습니다. 일반적으로 제품, 재고, 주문, 결제 및 고객 관계를 관리할 수 있는 기능이 포함되어 있습니다.\n- 도구: Shopify 관리 패널, BigCommerce 제어판, WooCommerce\n\n## 5. 고객 관계 관리 (CRM) 대시보드:\n\n- 주요 포인트: CRM 대시보드는 관리자가 고객이나 클라이언트와의 상호작용을 관리하는 데 도움을 주도록 설계되었습니다. 잠재고객 추적, 연락처 관리, 후속 조치 일정 설정 및 고객 상호작용 분석 기능이 포함될 수 있습니다.\n- 도구: Salesforce Essentials, Zoho CRM, HubSpot CRM\n\n## 6. 프로젝트 관리 대시보드:\n\n<div class=\"content-ad\"></div>\n\n- 주요 포인트: 프로젝트 관리 대시보드는 프로젝트 계획, 추적 및 협조를 용이하게 합니다. 일반적으로 작업 생성 및 할당, 진행 상황 추적, 기한 관리 및 팀원들과 협업하는 기능을 포함하고 있습니다.\n- 도구: Asana, Trello, Monday.com\n\n## 7. 시스템 구성 대시보드:\n\n- 주요 포인트: 이 유형의 작업 대시보드는 시스템이나 플랫폼의 다양한 설정 및 환경 설정(예: 이메일 설정, 보안 구성, 통합 설정 등)을 구성하는 데 사용됩니다.\n- 도구: 시스템/플랫폼에 따라 다양함 (예: 웹 호스팅의 cPanel, Windows Server Manager)\n\n<div class=\"content-ad\"></div>\n\n- 주요 포인트: 보고 대시보드는 시스템이 수집한 데이터를 기반으로 보고서를 생성하고 표시합니다. 관리자는 시스템의 성능이나 운영의 특정 측면에 대한 통찰력을 얻기 위해 보고서를 사용자 정의할 수 있습니다.\n- 도구: 주로 특정 소프트웨어/플랫폼의 기본 기능 (예: Google Analytics 대시보드, CRM 판매 보고서)\n\n# 9. 보안 대시보드:\n\n- 주요 포인트: 보안 대시보드는 시스템의 보안 상태에 대한 통찰력을 제공하며 보안 사건, 취약점, 액세스 제어 및 규정 준수 상태에 관한 정보를 포함합니다.\n- 도구: Crowdstrike Falcon Insight, Palo Alto Networks PAN-OS 관리자 콘솔, McAfee ePO\n\n# 10. 워크플로우 관리 대시보드:\n\n<div class=\"content-ad\"></div>\n\n- 주요 포인트: Workflow 관리 대시 보드는 관리자가 비즈니스 프로세스를 간소화하고 자동화하는 데 도움을 줍니다. Workflow 설계, 실시간 모니터링, 효율적인 Workflow 최적화 기능 등이 포함될 수 있습니다.\n- 도구: Kissflow, ProcessMaker, Bonzai\n\n위는 몇 가지 대시 보드 예시일 뿐이며, 관리자 대시 보드는 시스템이나 플랫폼의 특정 요구 사항에 맞게 매우 맞춤화될 수 있습니다.\n\n# 관리자 대시 보드 와이어프레임 만들기\n\n프로젝트 관리 대시 보드 와이어프레임을 만드는 것은 관리자가 프로젝트 상태 및 자원 할당을 모니터링하는 데 필요한 핵심 정보와 기능을 개요로 작성하는 과정입니다. 잘 설계된 대시 보드는 이러한 데이터를 명확하고 간결하게 시각적으로 제시해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n무료로 가입해서 Mokkup.ai를 사용하고 프로젝트 관리 대시보드 템플릿을 시작해보세요. Mokkup의 프로젝트 관리 대시보드는 이 유형의 대시보드의 좋은 예입니다. 이 대시보드는 \"경영진 대시보드 요약\"과 \"고객\"과 같은 명확한 제목으로 대시보드를 섹션으로 나눕니다. 각 섹션은 차트, 그래프 및 텍스트를 조합하여 데이터를 제시합니다. 예를 들어, \"프로젝트\" 섹션은 프로젝트를 사이즈별로 분배하는 쌓인 막대 차트를 사용합니다(대형, 중형, 소형 및 지원). 이 차트를 통해 관리자는 한 눈에 일정 비율의 프로젝트가 대규모 사업이고 작은 배당 비율이 지원 관련 작업임을 볼 수 있습니다.\n\n\"경영진 요약\" 섹션에는 프로젝트 유입 및 시작 및 종료 날짜를 보여주는 테이블이 있습니다. 이 섹션에는 또한 다른 프로젝트 사이즈 간의 청구 가능 시간 분포를 보여주는 쌓인 막대 차트가 포함되어 있습니다. 이 아래에는 시간당 달러 \"트렌드\"에 대한 섹션이 있어 시간이 지남에 따라 수익성을 추적하는 데 도움이 될 수 있습니다.\n\n전반적으로, Mokkup 프로젝트 관리 대시보드는 관리자에게 프로젝트 상태, 자원 할당 및 팀 생산성에 대한 포괄적인 보기를 제공합니다. 가장 좋은 점은 이것이 모컵의 템플릿으로 제공되어 당신의 필요에 맞게 쉽게 사용자 정의할 수 있다는 것입니다.\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n요약하자면, 관리자 대시보드는 다양한 분야에서 효율적인 관리를 위한 필수 도구로 작용합니다. 프로젝트 진행 상황 추적, 재무 지표 모니터링, 또는 사용자 활동 감시와 같은 작업에 사용되며, 다양한 대시보드 유형이 각기 다른 기업 요구에 맞춰 제공됩니다. 심층적인 통찰력을 제공하는 분석 대시보드부터 일상 업무를 간소화하는 운용 대시보드까지, 각 유형은 정보 기반 의사결정에 중요한 기능을 제공합니다. 특정 요구사항과 목표를 이해하는 것이 가장 적합한 대시보드 유형을 선택하는 핵심입니다. 이 다재다능한 도구의 능력을 활용하면, 기관은 운영을 최적화하고 생산성을 향상시키며, 명확하고 정확하게 목표를 달성할 수 있습니다.\n\n무료로 모컵을 사용해보세요!","ogImage":{"url":"/assets/img/2024-06-19-Top10TypesofAdminDashboardsandHowtoUseThemEffectivelyin2024_0.png"},"coverImage":"/assets/img/2024-06-19-Top10TypesofAdminDashboardsandHowtoUseThemEffectivelyin2024_0.png","tag":["Tech"],"readingTime":6},{"title":"데이터 과학을 위한 상위 5개의 Python 프론트엔드 라이브러리","description":"","date":"2024-06-19 15:56","slug":"2024-06-19-Top-5PythonFrontendLibrariesforDataScience","content":"\n\n파이썬에는 많은 프론트엔드 라이브러리가 있습니다. 각각의 장단점이 있죠. 어떤 것을 선택해야 할까요?\n\n데이터 과학자, 데이터 엔지니어, 머신러닝 엔지니어 또는 파이썬 개발자이든 상관없이 적어도 하나의 프론트엔드 라이브러리를 알고 있어야 합니다. 이것은 여러 면에서 도움이 될 수 있습니다. 예를 들어, 펫 프로젝트 생성, 풀 스택 개발자가 되는 데 도움이 되고, 대시보드를 만드는 데 도움이 되며 일상생활에서도 도움이 될 수 있습니다.\n\n이 기사에서는 고유한 특징, 장단점을 갖춘 5가지 다른 프론트엔드 라이브러리를 다룰 것입니다.\n\n![Top 5 Python Frontend Libraries for Data Science](/assets/img/2024-06-19-Top-5PythonFrontendLibrariesforDataScience_0.png)\n\n<div class=\"content-ad\"></div>\n\n# 1. Streamlit\n\n![Streamlit](/assets/img/2024-06-19-Top-5PythonFrontendLibrariesforDataScience_1.png)\n\n가장 인기 있는 데이터 과학용 프론트엔드 프레임워크로 시작합니다.\n\nStreamlit은 오픈 소스 Python 프레임워크입니다. 이를 사용하면 데이터 과학자나 머신 러닝 엔지니어들이 웹 개발 지식이 많지 않아도 빠르고 쉽게 대화형 데이터 앱을 만들 수 있어 특히 유용합니다.\n\n<div class=\"content-ad\"></div>\n\nStreamlit을 사용하면 개발자들은 깊은 프론트엔드 경험이나 지식을 요구하지 않고 매력적인 사용자 인터페이스를 구축 및 공유하고 모델을 배포할 수 있습니다. 이 프레임워크는 무료이며 모두 Python으로 작성되었으며 오픈 소스로 공개되어 있어 몇 분만에 공유 가능한 웹 앱을 만들 수 있습니다.\n\n빠른 프로토타입, SaaS, 분석 대시보드 또는 친구들을 위한 프로젝트를 만들고 싶다면 Streamlit은 좋은 선택입니다. 시작하는 데 시간이 걸리지 않으며 준비된 많은 템플릿이 있고 몇 분만에 프론트엔드를 마무리할 수 있습니다. 또한 공유가 매우 쉽습니다.\n\n그러나 이 라이브러리는 확장 가능한것이나 많은 기능을 갖춘 큰 것을 원한다면 좋은 결정이 아닐 수 있습니다. Streamlit은 한 가지 특정 기능을 보여주는 간단한 한 페이지 웹사이트에 더 중점을 둔다는 점을 유의해야 합니다. 그래서 이를 사용하여 소셜 네트워크나 스타트업을 만드는 것은 권장되지 않습니다.\n\n많은 사용자들은 Streamlit을 매우 쉽게 사용할 수 없다고 말합니다. 문서에 없는 새로운 기능을 추가하려면 어려운 도전이 될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 2. Solara\n\n![Solara](/assets/img/2024-06-19-Top-5PythonFrontendLibrariesforDataScience_2.png)\n\nSolara는 ipywidgets 또는 ipywidgets 상단의 React 유사 API를 사용하여 순수 Python에서 웹 앱을 구축할 수 있습니다. 이러한 앱은 주피터 노트북 내에서 작동하거나 FastAPI와 같은 프레임워크를 사용하여 독립형 웹 앱으로 작동합니다.\n\nSolara를 사용하면 구성 요소 기반 코드를 장려하고 상태 관리를 단순화하여 개발 프로세스를 더 효율적으로 만들고 애플리케이션을 유지 관리하기 쉽게합니다.\n\n<div class=\"content-ad\"></div>\n\nSolara는 파이썬 생태계의 모든 잠재력을 제공합니다. 즉, 웹 개발 역량을 확장하면서도 즐겨 사용하는 라이브러리를 계속 사용할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-Top-5PythonFrontendLibrariesforDataScience_3.png)\n\n따라서 크고 확장 가능한 웹사이트를 개발하거나 파이썬 노트북용 위젯을 만들려면 Solara가 좋은 선택입니다.\n\n하지만 단점은 다음과 같습니다: Solara는 (예를 들어 streamlit과 비교했을 때) 그다지 인기가 없어서 문제에 대한 답을 찾기 어렵거나 시작할 템플릿을 찾기 어렵습니다. 일부 사용자는 문서에 대해 불평하기도 합니다. 마지막으로, 상태를 사용하고 컴포넌트 기반 코드를 관리하는 방법을 알아야하기 때문에 일반적으로 개발하기 어렵습니다.\n\n<div class=\"content-ad\"></div>\n\n# 3. Trame\n\n![Trame](/assets/img/2024-06-19-Top-5PythonFrontendLibrariesforDataScience_4.png)\n\nTrame은 웹 개발 또는 기술에 대한 최소한의 지식으로 상호 작용 및 시각적으로 멋진 웹 애플리케이션을 손쉽게 생성할 수 있는 오픈 소스 플랫폼입니다. Python을 기반으로 하며 VTK, ParaView, Vega와 같은 플랫폼을 활용하여 몇 분 만에 웹 기반 애플리케이션을 만드는 데 사용됩니다.\n\nTrame은 반응적이고 상태를 가지는 웹 애플리케이션을 구축하기위한 고수준 프레임워크를 제공하며, 데스크톱 애플리케이션과 마찬가지로 로컬에서 사용할 수 있지만 대량 및/또는 민감한 데이터에 액세스하기위해 클라우드 또는 온프레미스에 배포될 수도 있습니다. Trame은 기존 라이브러리 또는 도구를 활용하여 다양한 기능을 내장하고 있으며, Vuetify, Altair, Vega, Deck, VTK, ParaView 등의 도구를 활용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nTrame은 다른 언어나 기술로 전환하지 않고도 풍부한 시각화를 통해 대화형 데이터 처리 애플리케이션을 만들 수 있게 해줍니다. 사용 가능한 여러 레이아웃을 통해 애플리케이션을 빠르게 구축할 수 있습니다. 또한 Trame은 서버 측 및 클라이언트 측 렌더링 그리고 혼합 접근법 중 선택할 수 있도록 해줍니다.\n\n따라서 Trame은 상호작용 및 복잡한 시각화와 시뮬레이션을 포함한 과학 중심 앱을 만들고 싶다면 이 상품을 선택할 수 있습니다. 이것은 멀티 플랫폼이며 많은 유용한 기능을 제공하며 전체적으로 미적으로 매력적으로 보입니다.\n\n안타깝게도 단점도 있습니다. Trame은 아직 비교적 새로운 프레임워크이기 때문에 아직 큰 커뮤니티가 형성되지 않았습니다. 또한 아직 개발 중이기 때문에 몇 가지 문제나 버그가 발생할 수 있습니다. 마지막으로, 이를 정말로 깊이 파고들어 모든 개념을 이해하는 데 시간이 걸릴 것입니다.\n\nTrame으로 개발을 시작하려면 이 라이브러리에 대한 개요인 제 기사 중 하나를 확인해보세요: https://medium.com/python-in-plain-english/trame-frontend-with-vue-js-but-in-python-329111755b98\n\n<div class=\"content-ad\"></div>\n\n# 4. ReactPy\n\n![ReactPy](/assets/img/2024-06-19-Top-5PythonFrontendLibrariesforDataScience_5.png)\n\nReactPy는 JavaScript 없이 사용자 인터페이스(UI)를 구축하기 위한 Python 패키지입니다. ReactJS와 유사하게 작고 재사용 가능한 구성 요소를 사용하여 인터페이스를 만들 수 있습니다. ReactPy 인터페이스는 Flask, FastAPI, Sanic, Tornado, Django, Jupyter 및 Plotly-Dash와 같은 다양한 백엔드에 대해 구축할 수 있습니다.\n\n기본적으로 ReactJS에서 구축할 수 있는 것은 거의 대부분 ReactPy에서도 구축할 수 있습니다. ReactPy에는 이미 상태 관리, 훅, 구성 요소 등과 같은 대부분의 React 기능이 구현되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n만약 ReactJS에 익숙하고 백엔드와 프론트엔드를 동일한 언어로 구성하고 싶다면, ReactPy가 최선의 선택입니다. 그렇지 않다면, ReactPy는 보통 HTML/CSS/JS를 사용하는 다중 페이지 웹사이트, 랜딩 페이지 및 기타 요소를 작성하는 데 좋은 라이브러리입니다.\n\nReactPy의 주요 단점은 아직까지 새로운 기술이라 커뮤니티가 크지 않다는 것입니다. 이 말은 ReactJS처럼 수백 개의 라이브러리를 사용할 수 없다는 것을 의미합니다. 또한, 아직 개발 중이기 때문에 몇 가지 버그가 발생할 수 있고, 일부 기능이 완전히 구현되지 않을 수 있습니다.\n\nReactPy로 시작하고 싶다면, 아래의 저의 글을 확인해보세요: https://medium.com/@ash_computational_qm/reactpy-building-dynamic-frontend-applications-with-python-de92d9e95bce\n\n# 5. PyQt\n\n<div class=\"content-ad\"></div>\n\n\n![Python Frontend Libraries for Data Science](/assets/img/2024-06-19-Top-5PythonFrontendLibrariesforDataScience_6.png)\n\nPyQt는 유연한 Python 프로그래밍 언어와 강력한 Qt C++ 크로스 플랫폼 프레임워크를 원활하게 통합하여 강력한 GUI 모듈로 기능하는 플러그인 형태로 구현된 크로스 플랫폼 GUI 툴킷의 Python 바인딩입니다.\n\nQtCore는 핵심 비 GUI 기능을 위한 기능이며, QtGui는 GUI 기능을 위한 기능과 같이 특정 작업에 맞게 설계된 여러 모듈로 구성됩니다. PyQt는 최신 위젯 모음과 Windows, Unix, Linux, macOS, iOS 및 Android와 호환되는 여러 운영 체제와의 호환성으로 인해 그래픽 응용 프로그램을 개발하는 데 널리 사용됩니다.\n\n위에서 언급된 OS 중에서 데스크탑 앱이 필요하다면 PyQt가 최고의 선택 중 하나입니다. 다양한 위젯 세트, 좋은 사용자 정의 가능성을 제공하며 모든 Python 규칙을 따르므로 쉽게 작업할 수 있습니다. 또한 비디오 및 오디오와 같은 멀티미디어를 지원합니다.\n\n\n<div class=\"content-ad\"></div>\n\n흐음, 안타깝게도 PyQt를 설치할 때 추가적인 단계가 필요하며, 다른 라이브러리보다 더 어려운 점이 있어요. 또한, 애플리케이션이 공개 소스가 아닌 경우 상업용 라이센스를 구매해야 해요. 마지막으로, PyQt로 애플리케이션을 제대로 만들기 위해 모든 위젯과 기능이 어떻게 동작하는지 이해하는 데 시간이 걸려요.\n\n# 결론\n\n파이썬에서 다섯 가지 주요 프론트엔드 프레임워크를 탐색했어요. 각각의 독특한 강점과 응용 분야가 있어요. 빠르고 간편한 프로토타이핑을 위해서는 Streamlit을 선택하세요. 기업용 확장성을 원한다면 Solara가 적합하겠죠. 시뮬레이션과 복잡한 3D 시각화를 원한다면 Trame이 전문가에요. ReactJS와 유사한 웹사이트 개발을 원한다면 ReactPy가 이상적인 선택이 될 거예요. 그리고 크로스 플랫폼 데스크톱 애플리케이션을 만들고 싶다면 PyQt가 많이 선호되는 프레임워크입니다.\n\n이 개요를 통해 데이터 과학 노력에 완벽하게 맞는 프레임워크를 손쉽게 선택할 수 있을 거예요. 즐거운 코딩 되세요!\n\n<div class=\"content-ad\"></div>\n\n# 참고 자료\n\n- [Streamlit 웹사이트](https://streamlit.io)\n- [Streamlit과 Scikit-learn의 Chatbot 애플리케이션](https://blog.streamlit.io/building-a-streamlit-and-scikit-learn-app-with-chatgpt/)\n- [Trame](https://kitware.github.io/trame/)\n- [Solara](https://solara.dev)\n- [PyQt 위키](https://wiki.python.org/moin/PyQt)\n- [PyQt 소개](https://www.tutorialspoint.com/pyqt/pyqt_introduction.htm)\n- [ReactPy 문서](https://reactpy.dev/docs/index.html)\n- [React.js로 개발된 멋진 웹사이트 20선](https://dev.to/davidepacilio/20-awesome-websites-built-with-react-js-3ik8)","ogImage":{"url":"/assets/img/2024-06-19-Top-5PythonFrontendLibrariesforDataScience_0.png"},"coverImage":"/assets/img/2024-06-19-Top-5PythonFrontendLibrariesforDataScience_0.png","tag":["Tech"],"readingTime":7},{"title":"사막  숨겨진 네트워크","description":"","date":"2024-06-19 15:54","slug":"2024-06-19-DuneAHiddenNetwork","content":"\n\n<img src=\"/assets/img/2024-06-19-DuneAHiddenNetwork_0.png\" />\n\n# 이 기사에서, Patrik Szigeti와 함께 우리는 오리지널 '둠' 삼부작 뒤의 복잡한 사회 네트워크를 개괄하는 그래프 시각화를 지원하는 데이터 및 네트워크 방법론을 설계했습니다.\n\n2021년에 박스 오피스에서와 평론가들로부터의 성공을 거두자마자, 2024년 '둠: 파트 투'는 가장 기대되는 영화 중 하나였으며, 실망시키지 않았습니다. 본 글 작성 시점에서 이전 작품보다 Rotten Tomatoes와 IMDb에서 평점이 높고 더 많은 수익을 올릴 것으로 예상되는 '둠'은 변화무쌍한 정치적 풍경을 가졌으며, 네트워크 과학을 통해 탐구하기에 완벽한 시리즈입니다. 본 짧은 글에서, 우리는 프랭크 허버트의 처음 세 책 - '둠' (1965), '둠 메시아' (1969), 그리고 '둠의 아이들' (1976)을 바탕으로 Impremium의 다른 가문과 인물들 간의 연결을 탐구하기 위해 노력했습니다.\n\n이 기사의 첫 번째 부분에서는 '둠' 위키에서 캐릭터 프로필 데이터를 수집하는 파이썬 기반 방법을 소개하고 이러한 프로필을 재미있는 네트워크 그래프로 변환합니다. 그런 다음, 두 번째-스포일러가 많이 담긴-섹션에서, 우리는 네트워크의 심도에 빠져 첫 번째 '둠' 삼부작이 전하는 모든 이야기를 추출합니다.\n\n<div class=\"content-ad\"></div>\n\n모든 이미지는 저자들에 의해 만들어졌습니다.\n\n# 1 네트워크 구축\n\n먼저, 우리는 Python을 사용하여 두네 캐릭터들의 전체 목록을 수집합니다. 그런 다음, 각 캐릭터의 팬 위키 사이트에서 그들의 전기 프로필을 다운로드하고 각 캐릭터의 이야기가 다른 캐릭터의 이야기를 언급한 횟수를 계산합니다. 이를 통해 두 캐릭터 간의 다양한 상호작용을 인코딩한다고 가정합니다. 그런 다음, 이러한 관계를 복잡한 그래프로 변환하기 위해 네트워크 과학을 사용할 것입니다.\n\n1.1 캐릭터 목록 수집\n\n<div class=\"content-ad\"></div>\n\n먼저, 뒤니 팬 위키 사이트에서 관련 캐릭터 목록을 수집했습니다. 구체적으로, urllib와 bs4를 사용하여 언급된 각 캐릭터의 이름과 팬 위키 ID를 추출했습니다. 그리고 각 캐릭터가 자체적으로 위키 페이지가 있으며 해당 ID로 인코딩되어 있다는 사실도 확인했습니다. 이를 Dune, Dune Messiah 및 Children of Dune 세 권의 첫 세 권에 적용하였는데, 이 책들은 아트레이드 가문의 부상을 다루고 있습니다.\n\n참고 링크:\n- https://dune.fandom.com/wiki/Dune_(novel)\n- https://dune.fandom.com/wiki/Dune_Messiah\n- https://dune.fandom.com/wiki/Children_of_Dune_(novel)\n\n첫 번째로, 캐릭터 목록 사이트의 HTML을 다운로드하세요:\n\n<div class=\"content-ad\"></div>\n\n```js\ndune_meta = {\n    'Dune': {'url': 'https://dune.fandom.com/wiki/Dune_(novel)'},\n    'Dune Messiah': {'url': 'https://dune.fandom.com/wiki/Dune_Messiah'},\n    'Children of Dune': {'url': 'https://dune.fandom.com/wiki/Children_of_Dune_(novel)'}\n}\n\nfor book, url in dune_meta.items():\n    sauce = urlopen(url['url']).read()\n    soup  = bs.BeautifulSoup(sauce,'lxml')\n    dune_meta[book]['chars'] = soup.find_all('li')\n```\n\n약간의 수동 작업을 통해 캐릭터 이름과 ID를 세밀하게 조정해주었습니다:\n\n```js\ndune_meta['Dune']['char_start'] = 'Abulurd'\ndune_meta['Dune']['char_end'] = 'Arrakis'\ndune_meta['Dune Messiah']['char_start'] = 'Abumojandis'\ndune_meta['Dune Messiah']['char_end'] = 'Arrakis'\ndune_meta['Children of Dune']['char_start'] = '2018 Edition'\ndune_meta['Children of Dune']['char_end'] = 'Categories'\n```\n\n그런 다음, 모든 관련 이름과 해당 프로필 URL을 추출했습니다. 여기서, 캐릭터 이름이 어디서 시작하는지 수동으로 확인하고 ('캐릭터 목록 사이트의 개요와는 대조적') 추가로, 확장 시리즈에 해당하는 'XD' 및 'DE'로 표시된 캐릭터 및 특정 책에서 \"언급만 된\" 캐릭터들은 제외하기로 결정했습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\r\nfor k, v in dune_meta.items():\n    names_urls = {}\n    keep_row = False\n    print(f'----- {k} -----')\n    for char in v['chars']:\n        if v['char_start'] in char.text.strip():\n            keep_row = True\n        if v['char_end'] in char.text.strip():\n            keep_row = False\n        if keep_row and 'Video' not in char.text:\n            try:\n                url = 'https://dune.fandom.com' + str(char).split('href=\"')[1].split('\" title')[0]\n                name = char.text.strip()\n                if 'wiki' in url and 'XD' not in name and 'DE' not in name and '(Mentioned only)' not in name:\n                    names_urls[name] = url\n                    print(name)\n            except:\n                pass\n    dune_meta[k]['names_urls'] = names_urls\r\n```\n\n그런 다음 이 코드 블록은 다음과 같은 문자 목록을 출력합니다:\n\n<img src=\"/assets/img/2024-06-19-DuneAHiddenNetwork_1.png\" />\n\n마지막으로, 수집한 문자 수를 확인하고 다음 소단원을 위해 프로필 URL과 식별자를 저장합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndune_names_urls = {}\nfor k, v in dune_meta.items():\n    dune_names_urls.update(dune_meta[k]['names_urls'])\n\nnames_ids  = {n : u.split('/')[-1] for n, u in dune_names_urls.items()}\n\nprint(len(dune_names_urls))\r\n```\n\n이 셀의 출력 결과는 프로필 URL이 포함된 119자입니다:\n\n<img src=\"/assets/img/2024-06-19-DuneAHiddenNetwork_2.png\" />\n\n1.2 캐릭터 프로필 다운로드\n\n<div class=\"content-ad\"></div>\n\n우리의 목표는 '둠(Dune)' 캐릭터들의 소셜 네트워크를 그리는 것입니다. 이것은 누가 누구와 상호작용했는지 파악해야 한다는 것을 의미합니다. 이전 하위 장에서는 '누가'의 목록을 얻었고, 이제 그들의 개인 이야기 정보를 얻을 것입니다. 이러한 이야기들은 간단한 웹 스크래핑 기술을 다시 사용하여 가져오고, 그리고 각 캐릭터의 개인 사이트의 소스를 로컬에 별도 파일로 저장할 것입니다:\n\n```js\n# 프로필 html 파일을 저장할 폴더\nfolderout = 'fandom_profiles'\nif not os.path.exists(folderout):\n    os.makedirs(folderout)\n      \n# 프로필 html 파일 가져오기 및 저장\nfor ind, (name, url) in enumerate(dune_names_urls.items()):\n    if not os.path.exists(folderout + '/' + name + '.html'):\n        try:\n            fout = open(folderout + '/' + name + '.html', \"w\")\n            fout.write(str(urlopen(url).read()))\n        except:\n            pass\r\n```\n\n이 코드를 실행한 결과는 저희 로컬 디렉토리에 각 선택된 캐릭터에 속한 모든 팬 위키 사이트 프로필이 있는 폴더가 생성됩니다.\n\n## 1.3 네트워크 구축하기\n\n<div class=\"content-ad\"></div>\n\n캐릭터 간 네트워크를 구축하기 위해서는, 각 캐릭터의 위키 사이트 소스가 다른 캐릭터의 위키 식별자를 얼마나 자주 참조하는지 카운트합니다. 다음과 같은 로직을 사용합니다. 여기서는 소스와 대상 노드(캐릭터)의 연결을 포함하는 연결 목록 - 두 캐릭터 페이지 간에 공동 참조 빈도를 포함하는 연결의 무게도 포함하는 연결 목록을 구축합니다.\n\n```js\n# html 소스에서 이름 언급을 추출하고 연결 목록을 딕셔너리로 구성\nedges = {}\n\nfor fn in [fn for fn in os.listdir(folderout) if '.html' in fn]:\n\n    name = fn.split('.html')[0]\n    \n    with open(folderout + '/' + fn) as myfile:\n        text = myfile.read()\n        soup  = bs.BeautifulSoup(text,'lxml')\n        text = ' '.join([str(a) for a in soup.find_all('p')[2:]])\n        soup = bs.BeautifulSoup(text,'lxml')\n        \n        \n        for n, i in names_ids.items():\n            \n            w = text.split('Image Gallery')[0].count('/' + i) \n            if w>0:\n                edge = '\\t'.join(sorted([name, n]))\n                if edge not in edges:\n                    edges[edge] = w\n                else:\n                    edges[edge] += w\n\nlen(edges)\n```\n\n이 코드 블록을 실행하면, 119명의 둥니 캐릭터를 연결하는 307개의 엣지가 있는 결과를 얻을 수 있습니다.\n\n다음으로, NetworkX 그래프 분석 라이브러리를 사용하여 엣지 목록을 그래프 객체로 변환하고, 그래프가 가지고 있는 노드와 엣지의 수를 출력합니다:\n\n<div class=\"content-ad\"></div>\n\n```python\n# dict of edges로부터 networkx 그래프를 생성합니다\nimport networkx as nx\nG = nx.Graph()\nfor e, w in edges.items():\n    if w > 0:\n        e1, e2 = e.split('\\t')\n        G.add_edge(e1, e2, weight=w)\n\nG.remove_edges_from(nx.selfloop_edges(G))\n\nprint('노드 수: ', G.number_of_nodes())\nprint('엣지 수: ', G.number_of_edges())\n```\n\n위 코드 블록의 결과:\n\n<img src=\"/assets/img/2024-06-19-DuneAHiddenNetwork_3.png\" />\n\n노드 수는 단 72개이며, 47개의 문자가 어떤 중심 구성원과도 연결되지 않았음을 의미합니다. 아마도 해당 인물들의 위키 프로필은 꽤 간결한 것으로 보입니다. 게다가 몇 개의 자기 루프가 제거되어 엣지 수가 4개 줄었습니다.\n\n\n<div class=\"content-ad\"></div>\n\n내장 Matplotlib 플로터를 사용하여 네트워크를 간단히 살펴보겠습니다:\n\n```js\n# 네트워크를 매우 간단히 살펴봅니다\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(1,1,figsize=(15,15))\nnx.draw(G, ax=ax, with_labels=True)\n```\n\n셀의 출력:\n\n<img src=\"/assets/img/2024-06-19-DuneAHiddenNetwork_4.png\" />\n\n<div class=\"content-ad\"></div>\n\n현재 이 시각화에서 이미 네트워크 구조가 일부 보입니다. 우리는 다음 코드 라인을 사용하여 그래프를 Gephi 파일로 내보내었고, 아래 그림에 첨부된 네트워크를 설계했습니다 (이러한 네트워크 시각화 방법은 곧 나올 튜토리얼 기사의 주제가 될 것입니다):\n\n```js\nnx.write_gexf(G, 'dune_network.gexf')\n```\n\n전체 Dune 네트워크:\n\n# 2 네트워크 읽기\n\n<div class=\"content-ad\"></div>\n\n\"우리 네트워크의 중심에 폴 아트레이데스(리산 알-가이브, 무앗 딥, 우술, 코이삷 하데라흐 등으로도 불립니다)를 발견할 것이라는 것은 놀라운 일이 아닙니다. 그는 첫 번째 책(그리고 영화)의 주인공이자 마지막에는 제국 황제로 자리를 꿰차는 중요한 인물입니다. 두 번째 책인 '듄의 메시아'에서 우리는 전쟁을 벌이고 예지의 능력에 저주받은 지 오랜 기간 후의 리더인 다른 폴을 만나게 됩니다. 그는 블라인드 프레멘으로 사막으로 걸어가 스스로를 샤이-훌루드에 바칩니다. 그리고 나중에 '듄의 아이들'에서 등장하는 '선지자'로 나타납니다. 이 여정동안 그는 다른 많은 캐릭터와 마주하게 됩니다. 그의 이른바 이고 네트워크 — 그의 모든 연결 및 그 사이의 연걸 포함한 서브그래프 —는 모든 노드의 약 절반과 모든 링크의 64%를 포함하고 있음을 잘 보여줍니다. 아래에 그림도 나와 있습니다.\n\n![그림](/assets/img/2024-06-19-DuneAHiddenNetwork_5.png)\n\n우리는 네트워크를 계속 읽고 있으면 아트레이데스 가문이 중심에 자리하고 있음을 알 수 있습니다. 물론, 폴 주변에는 가족이 있습니다. 부모인 제시카 여사는 레토 아트레이데스 1세의 후궁이자 베네 게서릿 현자님입니다. 제시카는 하우스 하코넨의 블라디미르 하코넨의 딸로, 노드 그룹 중 노란색과 연한 파란색 노드 그룹 간의 첫 연결을 우리에게 보여줍니다. 폴과 프레멘 후궁 차니 사이에 강한 연결이 있으며, 그들의 자녀 레토 2세와 가니마와도 연결되어 있습니다. 폴은 멘토이자 친한 친구인 던컨 아이다호와 거니 할렉뿐만 아니라 베네 게서릿 현자인 가이우스 헬렌 모힘과도 밀접한 연결이 있습니다. 이 네트워크가 분명히 폴을 중심으로 하고 있음에도 불구하고 하우스 코리노(갈색), 하우스 하코넨(연한 파란색), 프레멘(파란색)의 명확한 그룹을 볼 수 있습니다. 하지만 우리에게 정말 흥미로운 점은 위키백과 기사를 기반으로 만든 이 간단한 네트워크가 이 세 권의 책 속 전개되는 줄거리에 대해 우리에게 많은 정보를 제공하고 있다는 것입니다.\"\n\n<div class=\"content-ad\"></div>\n\nLiet Kynes는 프레멘의 사실상의 지도자이자 식물학자로, 황량한 행성 아라키스가 푸르른 목초지와 물 공급으로 부자로 변하는 것을 꿈꾸었습니다. 그의 딸 Chani는 폴의 삶에서 중요한 인물인 스틸가와 연결되며 종교적인 추종자이자 프레멘 전체와 연결됩니다. 그러나 두 사이에 스키탈이 있었는데, 그는 무아딥의 지하, 덩컨 아이다호의 고라(죽은 개인을 복제한 인공적으로 창조된 인간)를 통해 로열 가문을 파괴하려 계획했습니다. 영화만 보신 분들은 덩컨이 우리 네트워크에서 중요한 인물이라는 것에 놀라실 수도 있지만, 아트레이데 집안의 검술사로 일한 뒤 아라키스 사막 전투에서 전사한 후, 상기된 고라로 돌아와 알리아 아트레이데스와 결혼하며 중요한 역할을 했습니다. 레이디 제시카의 딸이자 폴의 자매인 알리아 아트레이데스와 결혼하며 중요한 역할을 했습니다.\n\n영화 시청자들은 하코넨 집안의 일원으로서 투피어 하와트의 색채에도 놀라실지 모릅니다. 하우스 아트레이데스의 안전을 책임지는 멘타트인 그는 하코넨 집안이 아트레이데스를 아라키스 통치자로 대체한 후 그들의 봉사자로 강제로 들어가고 있었으며, 그들에게 반항하고 있었지만, 진정한 목표는 그가 사랑하는 덕의 죽음을 복수하는 것이었습니다. 제시카 여사가 공격의 배후라고 생각한 자데 그 후 그는 폴을 죽이지 않고 자살하는 행동으로 구원을 얻었습니다.\n\n그러나 이 네트워크에서 가장 매혹적인 부분은 캐릭터의 노드가 얼마나 작아 보이든 이들이 줄거나 중요한 역할을 하지 않았다는 뜻은 아닙니다. 그들은 올바른 말을 잘못된 대상에게 했을 수도 있었으며(Paul이 무아딥이 되기 전에 인간성의 중요한 요소를 잃었다고 주장한 Ix의 브론소), 알리아 아트레이데스의 연인이 되었을 수도 있으며(Javid), 아트레이데스 쌍둥이 레토와 가니마를 죽이려 계획했을 수도 있습니다(Tyekanik). 계속해서 말씀드릴 수 있습니다. 이들은 프랭크 허버트의 '듄'의 흥미진진하고 서로 연결된 정치적 배경 중 몇 가지 예일 뿐입니다.\n\n<div class=\"content-ad\"></div>\n\n이 기사를 통해, 데이터 과학 팬과 둥팬 사이에 다리를 만들고, 이 두 그룹 사이에 이미 존재하는 겹치는 커뮤니티를 재미있게 할 수 있도록 목표로 했습니다. 먼저, Python에서 상대적으로 일반적인 프레임워크를 제시하여 만날 수 있는 모든 팬 위키 사이트의 소셜 네트워크를 매핑할 수 있도록 했습니다. 둘째, 이 네트워크 시각화가 전체 이야기가 펼쳐지는 방식을 자세히 해석했습니다. 수많은 단어가 꼬리표가 되는 사진, 심지어 삼부작 이상의 가치를 지닌 사진입니다.","ogImage":{"url":"/assets/img/2024-06-19-DuneAHiddenNetwork_0.png"},"coverImage":"/assets/img/2024-06-19-DuneAHiddenNetwork_0.png","tag":["Tech"],"readingTime":10},{"title":"마이크로소프트 엑셀에서 대시보드 만들기  단계별 방법 제1부","description":"","date":"2024-06-19 15:53","slug":"2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1","content":"\n\n\n<img src=\"/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_0.png\" />\n\n아래에서 사용된 데이터 세트는 Tableau 커뮤니티에서 제공되는 Superstore 데이터 세트입니다.\n\nMicrosoft Excel에서 대시보드를 만드는 단계\n\n- 새 엑셀 워크북 열기\n\n\n<div class=\"content-ad\"></div>\n\n시트1을 대시보드로 변경해주세요.\n\n시트2를 데이터로 변경해주세요.\n\n![이미지](/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_1.png)\n\n· 데이터를 작업용 문서의 데이터 시트에 복사해주세요.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_2.png)\n\nSelect the data and click insert Pivot Table\n\n1. Click the option - From table/Range\n\n2. Leave the table range as is and click new worksheet. A new sheet will be created, and rename it as Analyze\n\n\n<div class=\"content-ad\"></div>\n\n\n![Step3](/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_3.png)\n\n![Step4](/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_4.png)\n\n3. 오른쪽에는 피벗 테이블 필드가 나타날 것입니다. 필터, 열, 행, 값으로 구성된 네 가지 섹션이 있을 것입니다. 매출을 값으로 드래그하여 매출의 합계(집계)로 표시하고, 하위 범주를 행으로 이동시킵니다. 새로운 피벗 테이블 데이터가 생성됩니다.\n\n![Step5](/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_5.png)\n\n\n<div class=\"content-ad\"></div>\n\n4. 대시보드 시트에 표시해야 하는 3가지 KPI입니다. 이제, 피벗 테이블 데이터를 새 셀에 복사하여 붙여넣으세요.\n\n5. 모든 섹션에서 필드를 제거하고 하위 범주를 행 섹션에 추가하고 매출을 값을 섹션에 추가하세요.\n\n![이미지1](/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_6.png)\n\n![이미지2](/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_7.png)\n\n<div class=\"content-ad\"></div>\n\n· 피벗 테이블(하위 카테고리/판매)을 선택하고 삽입을 클릭한 후, 권장 차트를 선택하여 막대 차트를 클릭하세요.\n\n· 하위 카테고리별 판매 차트가 생성됩니다.\n\n![판매 부문 차트](/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_8.png)\n\n차트 형식을 조정하세요.\n\n<div class=\"content-ad\"></div>\n\n\n![Dashboard](/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_9.png)\n\n- Edit the title and right-click on the bar to sort it.\n- Hide all remaining buttons.\n- Remove the gridlines.\n\n\n<div class=\"content-ad\"></div>\n\n· 다시 데이터 분석 시트로 이동하여 피벗 테이블을 복사하고 새 셀에 붙여넣기 해주세요.\n\n이제 오른쪽에 다른 피벗 테이블 필드가 나타날 것입니다. 모든 섹션에서 필드를 제거하고 지역을 행 섹션으로, 매출을 값 섹션으로 추가해주세요.\n\n· 이 피벗 테이블을 선택한 후 삽입을 클릭하고 권장 차트를 선택하여 파이 차트를 클릭해주세요.\n\n· 차트를 서식을 지정해주세요.\n\n<div class=\"content-ad\"></div>\n\n이미지 태그를 아래와 같이 변경해 주세요:\n\n![Create Dashboard in Microsoft Excel Step by Step Method Part 1](/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_10.png)\n\n<div class=\"content-ad\"></div>\n\n동일한 단계를 따라서 또 다른 차트를 생성해 보세요. 월간 판매를 나타내는 선 그래프를 만들어 보겠습니다.\n\n행 부분에 월(주문 날짜)을 추가해 주세요.\n\n· 차트 서식 설정\n\no 제목 편집\n\n<div class=\"content-ad\"></div>\n\n모든 버튼을 숨기세요\n\n그리드 라인을 제거하세요\n\n![이미지](/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_11.png)\n\n워크북의 대시보드 시트 서식 설정\n\n<div class=\"content-ad\"></div>\n\n먼저 대시보드의 구조를 그려 봅시다.\n\n- 모양에서 텍스트 상자를 삽입하고 선택한 후, 제목을 작성하고 상자를 채우고 가운데 정렬하세요.\n\n![이미지](/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_12.png)\n\n마찬가지로 KPI 지표와 차트를 위한 자리 표시자를 만들어보세요.\n\n<div class=\"content-ad\"></div>\n\n- KPI 값 추가하기. \"총 매출\" 상자를 클릭한 후, \"총 매출\" 텍스트 끝에서 엔터 키를 누르세요. 이제 커서가 아래 셀에 위치해 있습니다. 새 텍스트 상자를 추가한 후, 아래 공식을 사용하여 매출 금액을 입력하세요. 그리고 해당 KPI 상자 형식으로 텍스트 상자를 서식 지정해 주세요.\n\n`=Analyze!A4`\n\n![이미지](/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_13.png)\n\n- 차트를 각각의 자리에 추가하고 서식을 설정하세요.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_14.png)\n\n참고 :\n[마이크로소프트 공식 문서](https://support.microsoft.com/en-us/office/create-and-share-a-dashboard-with-excel-and-microsoft-groups-ad92a34d-38d0-4fdd-b8b1-58379aae746e#ID0EBBJ=Create_a_dashboard)","ogImage":{"url":"/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_0.png"},"coverImage":"/assets/img/2024-06-19-CreateDashboardinMicrosoftExcelStepbyStepMethodPart1_0.png","tag":["Tech"],"readingTime":4},{"title":"데이터 시각화 AI 에이전트의 성능 향상 - Performance Improvement","description":"","date":"2024-06-19 15:50","slug":"2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent","content":"\n\n## DSPy를 사용하고 최적화 기술을 활용하여 에이전트 성능 향상\n\n![에이전트 이미지](/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_0.png)\n\n몇 주 전에, 데이터 시각화를 지원하는 AI 에이전트를 개발하는 프로젝트를 공유했습니다. 에이전트는 대부분의 쿼리에 대해 잘 작동했지만 때로는 실행 불가능한 코드와 불완전한 지침(부제/주석과 같은 중요한 구성 요소를 잊어버린)과 같은 여러 문제가 발생할 때가 있었습니다. 대부분의 에이전트 응용 프로그램과 마찬가지로 성능 측면에서 신뢰할 수 없었지만, 이 게시물에서는 어떻게 에이전트를 더 잘 작동하게 만들었는지에 대해 설명합니다.\n\nAI 에이전트에 문제가 있나요? AI 에이전트의 신뢰성과 견고성을 향상시키고 싶으신가요? 에이전트를 효과적으로 개발하는 방법을 모르시겠나요? 전문가에게 문의해보세요!\n\n<div class=\"content-ad\"></div>\n\nhttps://form.jotform.com/240744327173051\n\n![Image 1](/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_1.png)\n\n![Image 2](/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_2.png)\n\n# 요약\n\n<div class=\"content-ad\"></div>\n\nAI 에이전트의 성능을 최적화하는 방법을 설명하기 전에, 에이전트가 구축된 방식에 대해 간단히 되짚어보려고 합니다. 그러면 따라오실 수 있을 거예요.\n\n![에이전트 이미지](/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_3.png)\n\n에이전트에는 이 두 가지 구성 요소가 있어요:\n\n- 데이터프레임 인덱스: 이것은 에이전트가 사용하는 데이터프레임에 대한 정보를 담고 있는 인덱스입니다. 열 이름, 데이터 유형 및 통계 정보 (최솟값/최댓값/카운트/평균)과 같은 것들이 있어요.\n- 스타일링 도구: 이곳에는 Plotly의 다양한 차트 유형에 대한 자연어로 된 정보가 있어요. 각 유형의 차트를 어떻게 서식 지정해야 하는지에 대한 에이전트의 지침이 담겨 있어요.\n\n<div class=\"content-ad\"></div>\n\n에이전트는 사용자 쿼리를 처리하여 관련 열을 식별하고 적절한 차트 유형을 결정합니다. 그런 다음 실행할 때 지정된 차트를 생성하는 Python 코드를 생성합니다.\n\n에이전트가 어떻게 만들어졌는지 더 알고 싶다면, 이 게시물을 읽어보세요:\n\n# 성능 측정\n\n에이전트를 개선하는 첫 번째 단계는 현재 성능을 측정하고 시스템에 가한 변경 사항과 비교하는 것입니다. 효과적으로 성능을 측정하기 위해 시스템이 마주할 쿼리 세트를 포함하는 데이터 세트를 생성해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n## 쿼리 데이터 세트 작성하기\n\nLLM이 만나게 될 쿼리 유형을 추가할 수 있습니다. 여기서 한 것처럼 LLM에게 해달라고 요청하는 것이 더 쉬운 방법입니다.\n\n![image](/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_4.png)\n\n평가 목적으로 추가적인 쿼리를 생성하도록 Language Model에 계속해서 요청할 수 있습니다. 자신만의 쿼리를 기여하는 것이 매우 유익합니다. 또한, 인덱스에 없는 정보를 요청하거나 데이터 시각화와 관련이 없는 질문 등, 에이전트를 도전시킬 수 있는 쿼리를 생성하는 것이 좋습니다. 여기에 제가 개발한 쿼리 세트의 예시가 있습니다. 기억해야 할 요점은 이 세트가 에이전트가 만날 가능성이 있는 모든 유형의 쿼리를 포함해야 한다는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n\n# 여기에는 LLM을 사용하여 만든 몇 가지 평가 쿼리가 나와 있어요.\n평가 쿼리는 다음과 같아요:\n- 'Filtering', 'Crime Analysis', 'Data Comparison', 'Advanced Queries', 'Imaginary Data', 'Irrelevant Queries', 'Prompt Injections', 'Line Chart', 'Bar Chart', 'Pie Chart', 'Map', 'Single-Value', 'Sankey' 같이 다양한 카테고리로 구성돼요.\n\n## 평가 메트릭\n\n이제 우리는 에이전트의 응답 \"유효성\"을 수치적으로 정의할 방법이 필요해요 - 우수한 응답과 좋지 않은 응답을 구별할 수 있는 점수 메커니즘이요.\n\n이 논리 다이어그램은 설계된 평가 메트릭이 작동하는 방식을 설명해줘.\n\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_5.png)\n\n전체 점수를 계산하려면 코드가 필요한 속성을 갖추었는지 확인하는 기본적인 방법을 사용하거나 Large Language Model (LLM)을 사용하여 평가를 수행할 수 있습니다. 제 구현에서는 각 쿼리에 대해 작은 점수 평가기를 구축하여 DSPy 서명을 사용했습니다.\n\n```python\nimport dspy\nfrom pydantic import BaseModel, Field\n\n# 출력을 위한 pydantic validator 정의\nclass Score(BaseModel):\n    commentary: str = Field(desc=\"점수 분석\")\n    Score: int = Field(desc=\"점수\")\n\n# 총점을 평가하는 데 사용할 서명 정의\nclass Scorer(dspy.Signature):\n    \"\"\"\n    쿼리와 코드를 위해 제공된 서명으로 총점을 계산합니다.\n    \"\"\"\n\n    query = dspy.InputField(desc=\"데이터 및 그래프에 대한 정보가 포함된 사용자 쿼리\")\n    code = dspy.InputField(desc=\"에이전트가 생성한 코드\")\n    output: Score = dspy.OutputField(desc='코드를 평가한 후의 점수')\n\n# 코드 실행 여부를 확인하는 함수\ndef check_code_run(code):\n    score = 0\n    try:\n        code = code.split('')[1]\n        exec(code)\n        score += 10\n        return score\n    except:\n        return score\n\n# LLM이 찾는 속성을 모두 찾았는지 여부에 따라 점수를 계산하는 함수\ndef evaluating_response(code, query):\n    score = 0\n    scorer = dspy.Predict(Scorer)\n    response = scorer(query=query, code=code)\n    score += int(response.Score.split('Score:')[1])\n    return score\n```\n\n평가 지표를 정의한 후, '미훈련' 시스템의 성능을 확인해보겠습니다. DSPy를 최적화하기 위해 DSPy 모듈 및 서명을 사용하여 에이전트를 다시 만들어야 합니다.\n\n\n<div class=\"content-ad\"></div>\n\n저희는 대규모 언어 모델 애플리케이션을 개선해 드리는 전문가입니다. 문의하실 사항이 있으시면 언제든지 아래 링크를 통해 저희에게 연락해 주세요:\nhttps://form.jotform.com/240744327173051\n\n## DSPy에서 에이전트 정의하기\n\n```js\nfrom pydantic import BaseModel, Field\n\n# Pydantic 출력 파서\nclass Plotly_code(BaseModel):\n    commentary: str = Field(desc=\"코드에 대한 주석\")\n    Code: str = Field(desc=\"Plotly 코드\")\n\n# 우리의 프롬프트에 대한 시그니처\nclass AgentSig(dspy.Signature):\n    \"\"\"\n    여러분은 Plotly에서 데이터 시각화를 생성하기 위해 {query}를 사용하는 AI 에이전트입니다.\n    사용 가능한 도구를 활용해야 합니다.\n    {dataframe_index}\n    {styling_index}\n\n    해당하는 열이 없는 경우 코드로 출력해야 합니다. 해당 정보가 없다고 밝히세요.\n    \"\"\"\n    query = dspy.InputField(desc=\"차트를 그리고자 하는 데이터 및 차트에 대한 정보를 포함한 사용자 쿼리\")\n\n    dataframe_context = dspy.InputField(desc=\"데이터 프레임의 데이터에 대한 정보 제공. 컬럼 이름 및 데이터 프레임 이름만 사용해야 함\")\n    styling_context = dspy.InputField(desc='Plotly 차트에 스타일을 적용하는 방법에 대한 지시')\n    code: Plotly_code = dspy.OutputField(desc=\"사용자 쿼리 및 dataframe_index 및 styling_context에 따라 필요한 시각화를 하는 Plotly 코드\")\n    \nclass AI_data_viz_agent(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.dataframe_index = dataframe_index\n        self.style_index = style_index\n\n        self.agent = dspy.ChainOfThought(AgentSig)\n    \n    def forward(self, query):\n        dataframe_context = self.dataframe_index.as_retriever(similarity_top_k=1).retrieve(query)[0].text\n        styling_context = self.style_index.as_retriever(similarity_top_k=1).retrieve(query)[0].text\n\n        prediction = self.agent(dataframe_context=dataframe_context, styling_context=styling_context, query=query)\n\n        return dspy.Prediction(dataframe_context=dataframe_context, styling_context=styling_context, code=prediction.code)\n\nlm = dspy.GROQ(model='llama3-70b-8192', api_key=\"\", max_tokens=3000)\ndspy.configure(lm=lm)\n\nagent = AI_data_viz_agent()\nprint(agent('What is the distribution of crimes by type by histogram?').code)\n```\n\n<img src=\"/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_6.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n## DSPy 프로그램 미컴파일/미훈련 평가\n\n벤치마킹을 수립하기 위해, 더 나은 성능을 위해 우리의 에이전트를 ‘훈련/컴파일’하지 않고 먼저 평가하겠습니다.\n\n```js\n# eval_df는 데이터셋 섹션에서 정의되었습니다\n# 평가 df에 코드 추가\ncode_list = []\nfor q in eval_df['Query']:\n    code_list.append(agent(q).code)\neval_df['Code'] = code_list\n\n# 코드가 실행되는지 확인하는 방법을 사용하여 실행함\n\neval_df['check_run'] = [check_code_run(code) for code in eval_df['Code']]\n\n# evaluate_response 방법을 사용하여 코드의 속성을 평가\neval_df['Attribute_Score'] = [evaluating_response(code, query) for code, query in zip(eval_df['Code'], eval_df['Query'])]\n\n# 에이전트가 필요한 정보를 갖고 있는 쿼리만 \n# 답변 가능해야 합니다. 에이전트가 충분한 정보가 없는 \n# 질문에 정확한 코드를 생성하는 상황을 피하고 싶습니다.\neval_df['Answerable'] = [1 if x.strip().lower()!='no relevant information' else 0 for x in eval_df['Expectation']]\n```\n\n![이미지](/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_7.png)\n\n<div class=\"content-ad\"></div>\n\n```js\n# 최종 점수 계산\n# 심사원 서명 작성\nclass CodeJudge(dspy.Signature):\n    \"\"\"응답에 코드가 포함되어 있는지 판단합니다.\"\"\"\n    response = dspy.InputField(desc=\"AI 에이전트로부터의 응답\")\n    has_code = dspy.OutputField(desc=\"응답에 파이썬 코드가 포함되어 있는지 여부\", prefix=\"사실적[예/아니오]:\")\n\n# 각 예측 응답에 대한 최종 점수를 계산하는 메트릭\n# 최고의 응답을 포함하는 예시와 비교\ndef full_metric(example, pred, trace=None):\n    if 'No relevant information' not in example.code:\n        check_run = check_code_run(pred.code)\n        attributes = evaluating_response(pred.code, example.query)\n    else:\n        check_if_code = dspy.ChainOfThought(CodeJudge)\n        response = check_if_code(response=pred.code)\n        if response.has_code.split('사실적[예/아니오]:')[1].strip() == '예':\n            return 0\n        else:\n            return 19\n        \n    return check_run + attributes\n\nzip_ = zip(eval_df['Answerable'], eval_df['check_run'], eval_df['Attribute_Score'], eval_df['Code'])\neval_df['Total_Score'] = [final_score(a, c, a_s, c) for a, c, a_s, c in zip_]\n\n# 총 점수 / 총 가능 점수 계산\n\neval_full_df['Total_Score'].sum() / (len(eval_full_df)) * 19\n```\n\n![이미지](/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_8.png)\n\n![이미지](/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_9.png)\n\n# 성능 향상\n\n<div class=\"content-ad\"></div>\n\n성능을 향상시키기 위해서는 모델에 완벽한 점수를 획득할 수 있는 예제를 제공해야 합니다. 다행히도, 에이전트가 이미 이 작업의 절반을 완료했습니다. 훈련 세트를 준비하기 위해 누락된 속성을 추가하고 코드를 실행할 수 있도록 해야 합니다. 쿼리가 결과를 반환하지 않아야 하는 경우, 예제에 간단히 '관련 정보 없음'을 포함하면 됩니다.\n\n그러나 개선된 코드를 검증하는 데는 조금 경계를 두고, 실제로 필요한 모든 속성이 코드에 있는지 확인하기 위해 다시 테스트해야 합니다.\n\n```js\n# 코드 개선 에이전트를 위한 새로운 서명 정의 \nclass Improver(dspy.Signature):\n    \"\"\"\n    코드 개선 에이전트입니다. 코드와 설명을 입력으로 받고 향상된 코드 개선을 출력합니다.\n    코드와 설명을 취해서 Plotly 코드를 출력해야 합니다. 이는 완벽한 점수를 받을 수 있는 코드입니다.\n\n    이 코드가 판단된 9가지 속성: 각 정답마다 +1\n    {'correct_column_names', 'title', 'Annotations', 'Format number in 1000 in K & millions in M only for numbers',\n   'Aggregation used', 'correct axis label', 'Plotly_white theme', 'Correct chart type', 'Html tag like <b>',}\n\n     여기가 따라야 하는 형식입니다\n    code: {code}\n    commentary:{commentary}\n    improved_code: 9점을 얻는 향상된 출력\n    \"\"\"\n    code = dspy.InputField(desc=\"개선해야 하는 코드\")\n    commentary = dspy.InputField(desc=\"평가 에이전트가 제공한 코드에 대한 설명\")\n    improved_code = dspy.OutputField(desc=\"평가에 따라 완벽한 점수를 받을 수 있는 향상된 코드\")\n\n# 이 개선 에이전트 모듈은 개선된 코드를 제공할 것입니다.\nimprover = dspy.ChainOfThought(Improver)\n# 원하는 기준을 충족하는지를 확인하기 위해 이전에 정의한 평가 모듈을 사용할 수 있습니다.\nscorer = dspy.ChainOfThough(Scorer)\n```\n\n개선자와 평가자 모듈을 호출하여 생성하고 검증하면 대부분의 작업이 완료됩니다. 이제 출력을 수동으로 확인하기만 하면 됩니다. 저희 훈련 세트는 총 59개의 쿼리이므로 한 번에 하나씩 확인하는 것이 좋습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Improving Performance for Data Visualization AI Agent](/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_10.png)\n\n## 학습세트 생성하기\n\n이제 DSPy Optimizer로 훈련 예제를 보내서 몇 가지 샷 예제를 추가하여 프롬프트를 개선할 수 있습니다.\n\n```js\n# 판사 서명 작성\nclass CodeJudge(dspy.Signature):\n    \"\"\"응답에 코드가 포함되어 있는지 확인합니다\"\"\"\n    response = dspy.InputField(desc=\"AI 에이전트에서 온 응답\")\n    has_code = dspy.OutputField(desc=\"응답에 파이썬 코드가 포함되어 있는지 여부\", prefix=\"사실적[예/아니오]:\")\n\n# 모든 예측된 응답에 대한 최종 점수를 계산하는 메트릭\n# 가장 적절한 응답을 포함한 예제와 비교합니다\ndef full_metric(example, pred, trace=None):\n    if 'No relevant information' not in example.code:\n        check_run = check_code_run(pred.code)\n        attributes = evaluating_response(pred.code, example.query)\n    else:\n        check_if_code = dspy.ChainOfThought(CodeJudge)\n        response = check_if_code(response=pred.code)\n        if response.has_code.split('사실적[예/아니오]:')[1].strip() == '예':\n            return 0\n        else:\n            return 19\n        \n    return check_run + attributes\n\n# 쿼리, 코드 쌍을 DSPy Example로 포맷팅\ntrainset = [dspy.Example(query=q, code=c).with_inputs('query') for q, c in zip(eval_full_df['Query'], eval_full_df['Best Response'])]\n```\n\n\n<div class=\"content-ad\"></div>\n\n## Few Shot 예시 찾기\n\nLLM에 프롬프트에 몇 가지 예시를 제공하는 것은 LLM의 응답을 향상시키는 일관된 기술이었습니다. 좋은 예시를 찾는 전통적인 방법은 추측하고 시도해보는 것이었습니다. 이제 추가할 예시를 체계적으로 찾을 수 있습니다.\n\nBootStrapFewShot은 다음을 수행하여 시작합니다:\n\n- 최적화하려는 학생 프로그램과 일반적으로 학생의 사본인 선생님 프로그램을 초기화합니다.\n- LabeledFewShot 텔레프롬프터를 사용하여 선생님에게 데모를 추가합니다.\n- 예측기의 이름과 학생 및 선생님 모델 모두에서 해당 인스턴스와의 매핑을 생성합니다.\n- 부트스트랩 데모 수(최대 부트스트랩)를 설정하여 생성되는 초기 교육 데이터 양을 제한합니다.\n\n<div class=\"content-ad\"></div>\n\n다음으로, 훈련 세트의 각 예제를 거쳐 갑니다. 각 예제에 대해:\n\n- 해당 방법은 부트스트랩의 최대 횟수에 도달했는지 확인합니다. 그렇다면 프로세스가 중지됩니다.\n- 교사 모델이 예측을 생성하려고 합니다.\n- 교사 모델이 성공적으로 예측을 수행하면, 이 과정의 세부 내용이 기록됩니다. 이에는 어떤 예측자가 호출되었는지, 받은 입력 및 생성된 출력이 포함됩니다.\n- 예측이 성공하면, 기록된 과정의 각 단계에 대해 입력 및 출력을 포함한 데모가 생성됩니다.\n\n```js\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\n\n# 옵티마이저 설정: 프로그램 단계의 8번 샘플을 \"부트스트랩\"(즉, 자체 생성)하기를 원합니다.\n# 옵티마이저는 이를 개발세트에서 최상의 시도로 선택하기 전에 10번 반복(초기 시도 포함)합니다.\nconfig = dict(max_bootstrapped_demos=2, max_labeled_demos=2, num_candidate_programs=3, num_threads=4)\n\n# 간단히 말해, 임의 검색으로 적은 데이터로 학습하기는 LLM을 사용하여 예제를 생성한 후\n# 이를 평가하는 방식으로 작동합니다.\n# 여러 번의 반복 후에는 훈련 세트에 좋은 예제가 생성됩니다.\nteleprompter = BootstrapFewShotWithRandomSearch(metric=full_metric, **config)\noptimized_agent = teleprompter.compile(agent, trainset=trainset)\n```\n\n훈련 후 이를 통해 프롬프트에 추가할 몇 가지 예제가 제공됩니다. 이를 확인하려면 lm.inspect_history(n=1)을 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![표](/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_11.png)\n\n## 프롬프트 지침, 서명 및 접두사 최적화\n\n프롬프트를 테스트할 예제는 있지만 초기 지침은 어떻게 할까요? DSPy는 COPRO라는 알고리즘을 사용하여 이를 최적화합니다. 이 알고리즘은 다음과 같이 작동합니다:\n\n- 새로운 지침 생성 및 개선: COPRO는 새로운 지침 세트를 생성하고 단계별로 개선합니다.\n- Coordinate Ascent(언덕 오르기): 이것은 각 단계가 주어진 메트릭 함수를 기반으로 결과를 개선하려는 최적화 기술입니다. 언덕 오르기는 계속해서 값이 증가하는 솔루션을 향해 이동하는 로컬 탐색 알고리즘입니다.\n- 메트릭 함수 및 훈련 데이터셋: 최적화에는 메트릭 함수(성공이나 적합성의 양적 측정일 수 있음)와 훈련 데이터셋(trainset)을 사용하여 지침을 평가하고 개선합니다.\n- 깊이: 이 매개변수는 프롬프트를 개선하기 위해 최적화자가 수행하는 반복 횟수를 지정합니다. 보통 더 많은 반복은 더 정제되고 최적화된 지침을 가능하게 합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom dspy.teleprompt import COPRO\n\n# 설계된 메트릭과 시도할 최적화 횟수를 알려주는 폭으로 COPRO를 초기화합니다.\nteleprompter = COPRO(\n    metric=full_metric,\n    verbose=True, breadth=5\n)\n\n# num_threads는 LLM과 함께 열리는 인스턴스 수입니다. \n# API를 과도하게 사용하여 요금이 부과될 수 있으니 주의하세요.\nkwargs = dict(num_threads=8, display_progress=True, display_table=0) # 최적화 프로세스의 Evaluate 클래스에서 사용됨\n\n# 프로그램을 컴파일합니다.\ncompiled_prompt_opt = teleprompter.compile(agent, trainset=trainset[:40], eval_kwargs=kwargs)\n# 나중에 검토할 수 있도록 저장합니다.\ncompiled_prompt_opt.save('COPRO_agent.json')\n```\n\n컴파일 후에는 에이전트의 성능을 향상시키는 데 어떤 종류의 지침, 접두사 및 서명이 더 나은 성능을 낼지 확인할 수 있습니다.\n\n```js\n# 모든 DSPy 프로그램 내부에서 __dict__를 사용하여 후보 프로그램을 확인할 수 있습니다.\ncompiled_prompt_opt.__dict__\n```\n\n<img src=\"/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_12.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_13.png\" />\n\n# 결과\n\n아래는 각 프롬프트 기법에 대한 컴파일된 결과입니다. 전체적으로 가장 큰 개선은 시그니처 및 접두사를 최적화한 COPRO_AGENT에서 나왔습니다. 즉, 원래 지시사항 및 접두사가 매우 최적화되지 않았음을 의미합니다. 전체적으로 COPRO 에이전트는 데이터셋에서 71%의 성능을 보였으며, FewShoot는 63%의 성능을 보였으며, 베이스 라인은 60%였습니다.\n\n<img src=\"/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_14.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n## 다음 단계\n\n에이전트는 분명히 개선되었지만 아직 멀었습니다. 저는 이 AI 에이전트를 사용하여 기본적인 탐색적 데이터 분석 및 통계 모델링을 수행하는 등의 추가 기능을 더욱 개선할 계획입니다.\n\n이 게시물이 유익했다면 FireBird Technologies와 저를 Medium에서 팔로우해보세요. AI 개발에 도움이 필요하다면 아래 링크를 통해 자유롭게 연락해 주세요.\n\n[링크](https://form.jotform.com/240744327173051)\n\n<div class=\"content-ad\"></div>\n\n읽어 주셔서 감사합니다!","ogImage":{"url":"/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_0.png"},"coverImage":"/assets/img/2024-06-19-ImprovingPerformanceforDataVisualizationAIAgent_0.png","tag":["Tech"],"readingTime":14},{"title":"마라톤 나이 등급 대안 2023년 업데이트된 레이스 결과 데이터셋 탐색","description":"","date":"2024-06-19 15:47","slug":"2024-06-19-MarathonAgeGradingAlternativesExploringtheUpdated2023RaceResultDataset","content":"\n\n\n![마라톤 나이 그레이딩 대안 탐색](/assets/img/2024-06-19-MarathonAgeGradingAlternativesExploringtheUpdated2023RaceResultDataset_0.png)\n\n다른 연령 및 성별의 러너들 사이의 비교는 어떻게 하나요?\n\n지난 몇 주간, 이 정확한 질문을 탐색해 왔어요.\n\n저는 약 2,000,000 개의 개별 마라톤 완주 정보를 포함한 대규모 데이터셋을 수집한 후, 서로 다른 시간에 대한 백분위수 및 z-점수를 계산하여 전통적인 나이 그레이딩 시스템의 대안을 만들었어요.\n\n\n<div class=\"content-ad\"></div>\n\n이번 시리즈를 마무리하려고 하는데, 업데이트된 데이터를 사용하고 싶었습니다. 원래 사용하던 데이터셋은 2010년부터 2019년까지의 레이스를 다룹니다.\n\n전반적인 결과의 수를 늘리기 위해 이러한 긴 시간 범위를 선택했지만, 이전 연구에서도 러너들이 특히 마지막 몇 년 동안 더 빨라지고 있다는 사실을 보여줍니다.\n\n따라서 여러 아이디어를 다소 조정한 후, 2023년 데이터를 기반으로한 점수 및 테이블을 업데이트하고 싶었습니다.\n\n이번 시리즈를 두 가지 마지막 기사로 마무리 짓겠습니다. 이 기사에서는 데이터셋을 살펴보고 일반적인 관찰을 어떻게 할 수 있는지 알아볼 것입니다. 그 다음 기사에서는 z-점수 및 백분위 수 계산을 업데이트하고 최신 연령 등급 계산기를 만들어 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 데이터셋에는 무엇이 포함되어 있나요?\n\n2023년 마라톤 가이드에 나열된 모든 마라톤 경주 결과가 포함되어 있습니다. 또한 이전 연도에는 마라톤 가이드에 기록되어 있었지만 2023년에는 표시되지 않았던 CIM도 포함되어 있습니다.\n\n이 데이터에는 약 40만 명의 개별 완주 기록이 포함되어 있습니다. 각 완주 기록에는 러너의 이름, 성별, 나이 및 완주 시간이 포함되어 있습니다.\n\n결과를 정리한 후 몇 가지 성별 또는 나이 정보가 없는 결과를 제외하면 609개 레이스 전체에서 388,569개의 완주 기록이 남았습니다.\n\n<div class=\"content-ad\"></div>\n\n- 61 레이스에서는 1,000명 이상의 참가자가 있었습니다.\n- 108 레이스에서는 500명 이상의 참가자가 있었습니다.\n- 312 레이스에서는 100명 이상의 참가자가 있었습니다.\n\n이 분석을 위해 나는 각 완주를 BAA 자격 기준에 부합하는 연령 그룹으로 분류했습니다. — 35세 미만, 35–39, 40–44, 45–49, 50–54, 55–59, 60–64, 65–69, 70–74, 75–79, 80세 이상.\n\n# 각 연령 그룹의 완주자는 몇 명인가요?\n\n이 데이터셋은 이전 것보다 작습니다. 약 1년치의 결과를 포함하고 있지만, 세 달이 아닌 한 해 (2023년)에 한정되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n각 연령 그룹에서 충분한 수의 러너가 남아 있어 백분위 및 z-점수를 정확하게 계산할 수 있을까요?\n\n위의 시각화 자료는 각 연령 그룹의 완주자 수를 보여줍니다 — 여성은 왼쪽에, 남성은 오른쪽에 위치해 있어요.\n\n일반적으로, 각 연령 그룹에서 남성이 여성보다 더 많이 있습니다. 그러나 각 성별 내에서도 패턴은 비슷합니다. 35세 미만의 러너가 가장 많은 그룹을 이루고, 연령이 들수록 그룹이 줄어듭니다.\n\n그러나 35세 이하의 러너가 가장 많은 그룹을 이루고 있다 해도, 러너의 약 50%가 40대 이상인 것은 중요한 사실입니다. 이는 연령 등급이 중요하다는 것을 상기시켜주는 좋은 예시입니다 — 왜냐하면 대부분의 러너가 마스터 연령 그룹에 속하기 때문이죠.\n\n<div class=\"content-ad\"></div>\n\n이전 데이터셋을 탐색하면 분포가 약 1,000명의 러너 부근에서 약간 이상해지는 것 같습니다.\n\n그래서 대부분의 연령 그룹은 꽤 꽤 안정되어 보입니다. 60대 후반까지 모든 연령 그룹이 2,000명 이상의 완주자가 있네요.\n\n남성 75-79세 그룹과 여성 70-74세 그룹은 약 600~700명의 완주자가 있는 상황입니다.\n\n나머지 연령 그룹인 남성 80세 이상, 여성 75-79세 그룹, 여성 80세 이상은 신뢰할 만한 결론을 내릴만큼 충분히 크지 않을 수도 있습니다. 최소한 이들에 대해서는 면밀히 살펴보기로 하죠.\n\n<div class=\"content-ad\"></div>\n\n# 시간대가 중요한가요?\n\n이전에는 9월, 10월 및 11월 세 달 동안의 레이싱 데이터셋을 처리했어요.\n\n한 해에 둘 이상의 마라톤에 참가하는 러너의 영향을 최소화하기 위해 일부러 이렇게 했지만, 이로 인해 표본에 포함되는 마라톤이나 제외되는 마라톤이 제한됩니다. 따라서 1월부터 12월까지 모든 레이스를 포함하는 샘플로 전환하면 차이가 있을 수도 있습니다.\n\n데이터를 탐색하는 중에 눈에 띈 것 중 하나는 연도 내에 완주 횟수가 균등하게 분포되어 있지 않다는 것이었어요.\n\n<div class=\"content-ad\"></div>\n\n절반 이상의 완주(201k)가 9월부터 11월에 개최된 레이스에서 발생했습니다. 그리고 조금 더 범위를 넓혀본다면, 하년 중 후반기(7월부터 12월)가 상반기보다 약 50% 더 많은 완주를 기록하고 있습니다.\n\n위 시각화는 연간을 1월부터 6월과 7월부터 12월로 나누어 각 시간대의 완주자 수를 비교합니다.\n\n하지만 실제로 완주 시간에는 영향을 미칠까요?\n\n위 시각화는 각 나이 그룹별 평균 완주 시간을 네 가지 다른 시간대를 기준으로 비교합니다 — 원래의 9월부터 11월 기간, 상반기, 하반기, 전 연도.\n\n<div class=\"content-ad\"></div>\n\n잠깐 차이점이 있어요. 9월부터 11월 또는 1월부터 6월에 초점을 맞추는 경우 평균이 약간 빨라질 때가 있습니다. 특히 연장된 러너들 사이의 차이가 더 커집니다.\n\n하지만 중요한 건 연령 그룹 간의 패턴이 상당히 유사하고 안정적이라는 점이에요.\n\n이를 통해 전체 연도를 기반으로 한 백분위 또는 z-점수와 더 짧은 시간 범위를 기반으로 한 값 사이에 직접적인 비교를 하지 않는 것이 현명할 것으로 생각돼요. 하지만 일관된 샘플을 사용한다면 이러한 시간대 중 어느 것을 사용해도 비슷한 유용성을 제공할 것으로 보여요.\n\n전체 연도는 가장 큰 샘플을 제공하며, 이는 더 신뢰할 수 있는 결과를 얻게 해 줄 것으로 기대됩니다(특히 연령이 많거나 적은 그룹의 경우).\n\n<div class=\"content-ad\"></div>\n\n# 한 해 동안 여러 차례 경주하는 참가자는 얼마나 될까요?\n\n결과의 전체 연도를 포함시킴으로써 어떤 참가자들은 여러 번 완주한 결과를 포함할 가능성이 높아진다고 생 생각해요. 그것이 얼마나 흔한지 궁금했어요.\n\n다른 경주 결과를 정확하게 일치시키는 것은 어렵지만 완벽하게 하는 것도 어렵습니다. 하지만 이름과 연령 그룹을 기준으로 중복을 찾아보는 간단한 근사치가 있습니다.\n\n동명이인의 가능성이 있습니다 — 일치하는 이름을 가진 여러 사람이 있을 수 있습니다. 만약 40살인 두 남성이 Michael Johnson으로 이름이 동일하면, 실수로 같은 사람으로 표시될 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n잘하셨어요! 마크다운 형식의 테이블은 아래와 같습니다:\n\n| 사건 | 방지 |\n|-------------|---------|\n| 오탐 | 매칭 방해 |\n| 이름 변경 | 매칭 방해 |\n| 별명 사용 | 매칭 방해 |\n| 연령 그룹 변경 | 매칭 방해 |\n\n<div class=\"content-ad\"></div>\n\n하노이 표를 마크다운 형식으로 변경하십시오.\n\n<div class=\"content-ad\"></div>\n\n- Henry Rueden. 88번의 레이스를 완주한 73~74세의 남성으로, 중간 완주 시간은 8시 31분이며 최고 시간은 6시 09분입니다.\n- Angela Tortorice. 63번의 레이스를 완주한 55~56세의 여성으로, 중간 완주 시간은 7시 43분이며 최고 시간은 5시 41분입니다.\n\n그들은 이러한 Center of the Nation 시리즈와 같은 멀티데이 시리즈에 참여하여 많은 레이스를 기록했습니다. 하지만 그들은 몇몇 대형 명문 대규모 마라톤 대회에도 참여했습니다.\n\n# 여러 참가자가 한 번에 달리는 사람보다 빠를까요?\n\n이 데이터에 관한 토론에서 사람들은 종종 연 1회 이상 레이스에 참가하는 사람들이 일반적으로 1회 참가자보다 빠르다고 주장합니다.\n\n<div class=\"content-ad\"></div>\n\n그것은 합리적인 가정이에요. 그러나 지금 나는 이 문제에 빛을 볼 수 있는 준비된 데이터셋이 있으니, 그 질문을 하자는 것이 말이 된다고 생각했어요.\n\n각 연령 그룹에서, 매년 한 번 경주를 마치는 러너들은, 두 번이나 세 번 경주를 마치는 러너들보다 상당히 더 느린 중앙 시간을 가졌어요.\n\n두 번과 세 번을 마치는 사람들 간의 차이는 꽤 작아요 — 비록 대부분의 경우 세 번 마치는 사람들이 조금 더 빠르긴해도요.\n\n연령 그룹을 횡단하면, 한 번, 두 번, 세 번 경주를 마치는 러너들을 분포를 비슷합니다. 위쪽에 더 자세히 나와있는 것과 유사하게, 대략 90% (1 경주), 9% (2 경주), 1% (3 경주)로 나눠지죠.\n\n<div class=\"content-ad\"></div>\n\n그러나 약간의 차이가 있습니다. 여성은 한 종족에 약간 더 치우쳐 있습니다. 35세 미만의 러너들 중 여성의 비율은 93–6–1이며, 남성은 91–8–1입니다.\n\n나이가 많은 그룹도 약간 더 자주 다중 마라톤러 쪽으로 기울어져 있습니다. 예를 들어, 70~74세 러너 중 여성의 비율은 89–9–2이고, 남성은 86–11–3입니다.\n\n# 다음은 무엇인가요?\n\n2023년의 데이터를 수집하고 준비했으니, 이 시리즈를 마무리할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다음 기사를 위해 2023년의 시간을 기반으로 z-점수와 백분위를 위한 데이터를 준비할 거예요. 또한 새로운 연령 등급 계산기를 업데이트하여 이 새로운 데이터와 더 최근의 연령 등급 요소를 통합할 거예요.\n\n이 작업이 끝나면, 아마도 연령 그룹간 레이스 결과를 어떻게 비교해야 하는지에 대한 더 큰 질문에 대해 반성하는 마지막 기사를 작성할 것 같아요 — 그리고 이 두 가지 대안 중 어느 것이 현재의 연령 등급 시스템보다 우수한 대안인지에 대해 다뤄볼 거예요.\n\n이 시리즈를 마무리하면, 이 새로운 데이터셋을 활용해 다음과 같은 몇 가지 다른 질문에 대해 탐구할 예정이에요:\n\n- 남성의 3:00 마라톤과 여성의 3:30 마라톤을 비교하는 이 기사를 업데이트하는 것\n- 지난 스무 해 동안 완주 시간의 변화를 분석하는 이 기사를 업데이트하는 것\n- BAA의 연령 및 성별에 따른 예선 기준을 분석하는 것\n\n<div class=\"content-ad\"></div>\n\n위 내용 중에 관심이 있는 부분이 있다면, 이메일 소식을 구독해 주세요.\n\n또한, 이 분석에 도움이 될 피드백이나 아이디어가 있으면 응답을 남겨주세요. 두 번째(또는 세 번째, 네 번째) 의견을 가지고 있는 것이 항상 도움이 됩니다!\n\n저는 열정적인 러너겸 데이터 열정가입니다. 방금 40세가 되었어요, 그래서 연령대별로 결과를 비교하는 것이 특별히 관심 있어요. 제가 무엇을 하고 있는지 계속해서 따라갈 수 있는 방법은 다음과 같아요:\n\n- 러닝 위드 락 팔로우하기: 제 훈련 소식을 듣기\n- 마라톤 훈련 계획 선택에 대한 팁 읽기\n- Strava에서 제 활동 추적하기","ogImage":{"url":"/assets/img/2024-06-19-MarathonAgeGradingAlternativesExploringtheUpdated2023RaceResultDataset_0.png"},"coverImage":"/assets/img/2024-06-19-MarathonAgeGradingAlternativesExploringtheUpdated2023RaceResultDataset_0.png","tag":["Tech"],"readingTime":6},{"title":"F-16, G-LOC, AUTO-GCAS 및 Python에 대해 알아보기","description":"","date":"2024-06-19 15:46","slug":"2024-06-19-ANF-16G-LOCAUTO-GCASANDTHEPYTHON","content":"\n\n유튜브에는 군용 항공 및 전투기를 밀접히 주시하는 많은 사람이 알아 볼 수 있는 분류되지 않은 HUD(Head-Up Display) 비디오가 있습니다. 이 비디오에서는 F-16 전투기에서 고속 기동을 수행하면서 학습 비행사가 G-LOC(G-induced Loss of Consciousness)를 경험하는 장면이 담겨 있습니다. 이 비디오에서 전투기가 17,000피트에서 4,500피트로 몇 초 사이에 하강하는 동안, 비행기의 통합 AUTO-GCAS(Automatic Ground Collision Avoidance System)가 제어를 인계하고 비행기와 비행사의 생명을 구하는 기동을 합니다. 이 비디오가 최근에 다시 제 유튜브 피드에 나타나자, 이 일 분짜리 장면을 Python을 사용하여 분석하고 시각화하는 아이디어가 생겼습니다. 이어지는 섹션에서는 제가 이를 어떻게 하였는지 설명하겠습니다. 그 전에, G-LOC와 AUTO-GCAS에 대해 조금 이야기해보겠습니다.\n\nG-LOC란 무엇인가요?\n\n전투기에서, G-LOC(G-induced Loss of Consciousness)는 조종사가 고 G-힘에 노출되어 의식을 잃는 상황을 나타냅니다. 이 상황은 비행 중에 발생하며, 특히 급한 기동, 갑작스러운 가속 또는 감속으로 인해 높은 가속을 경험할 때 발생합니다. G-LOC 중에는 혈액이 몸 전체로 제대로 분배되지 않아 뇌로의 혈류가 부족해지게 됩니다. 결과적으로, 뇌는 산소 부족으로 잠시 멈추게 되고 조종사는 의식을 잃습니다.\n\n조종사들은 일반적으로 G-LOC를 상쇄하기 위해 특별히 디자인된 G-슈트를 입습니다. G-슈트는 슈트 일부인 밸브를 통해 비행기의 압력 시스템에 연결됩니다. 이 연결로 인해 공기 압력이 조종사의 다리와 배에 가해져 하체에 혈액이 모이는 것을 막고 이를 다시 머리쪽으로 상쇄시킵니다. 이 시점에서 G-슈트는 비행기의 G-힘 센서(G-센서)와 협조합니다. 비행기가 높은 G-힘을 겪으면 센서가 이 조건을 감지하여 슈트의 공기 블래더를 팽창시킵니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-ANF-16G-LOCAUTO-GCASANDTHEPYTHON_0.png\" />\n\n그 반면에 우리가 언급한 의식 상실을 방지하기 위해 전투기 조종사들은 Anti-G Straining Maneuver (AGSM)를 보편적으로 수행합니다. 이 기동은 고 G-힘에 노출될 때 조종사들이 사용하는 특별한 호흡과 근육 긴장 기술입니다. 이 기술은 몸 속의 혈액을 적절하게 분배하여 뇌로의 충분한 혈류를 유지합니다. 간단히 말해, 이 특별한 기술을 통해 조종사는 폐량을 더 효율적으로 사용하여 산소 공급이 부족해지는 것을 방지하여 몸과 두뇌가 산소 공급을 받지 못하도록 합니다.\n\nAUTO-GCAS (자동 지면 충돌 회피 시스템)란 무엇인가요?\n\nAUTO-GCAS는 전투기 및 일부 다른 항공기에 사용되는 자동 지면 충돌 회피 시스템입니다. 이 시스템은 조종사의 실수나 의식 상실(G-LOC) 경우 항공기가 지면과 충돌하는 것을 방지하기 위해 설계되었습니다. 이 기사의 주제인 비디오에서 확인할 수 있듯이, AUTO-GCAS는 비행 안전성을 향상시켜 사고를 예방하는 데 중요한 역할을 합니다. 현재 F-16 항공기에서 활발히 사용되고 있으며, 향후 F-35에서도 사용될 예정입니다.\n\n<div class=\"content-ad\"></div>\n\nAUTO-GCAS는 현대 항공에서 조종사들과 항공기의 안전을 보장하는 중요한 기술입니다. 이 시스템은 선진 알고리즘과 실시간 데이터 처리를 사용하여 항공기의 위치, 속도, 고도 및 방향 정보를 활용하여 지형 및 장애물 데이터를 분석합니다. 이 분석을 기밠으로, 시스템은 충돌 위험이 있는지를 판단하고, 처음에는 조종사에게 청각 및 시각적 경고로 알립니다. 조종사가 이러한 경고에 적시에 반응하지 않으면, 시스템이 자동으로 이를 맡아 필요한 기동을 수행합니다.\n\n자, 이제 우리 기사의 주제인 동영상을 시청해볼까요:\n\n동영상에서 볼 수 있듯이, 조종사는 고속 기동 중 약 8.5G의 힘을 경험하며 의식을 잃습니다. 17,000피트에서 급강하하는 F-16은 단 10초 만에 4,500피트로 내려가게 됩니다. 이 시점에서, 조종사가 의식을 잃어 수정 기동을 할 수 없을 때, AUTO-GCAS 시스템은 문제 발생을 감지하고 활성화되며 항공기를 12,000피트로 다시 올리는 9G의 조치를 취합니다.\n\nPython / Matplotlib를 사용하여 고도 및 속도 변화 분석\n\n<div class=\"content-ad\"></div>\n\n데이터셋을 준비하는 것부터 시작했어요. 동영상에서 G-포스, 속도, 고도 및 시간 정보를 Excel로 옮겨야 했는데, 매초 동영상을 일시 중지하여 데이터를 정확히 캡처하기 위해 \" . \" 및 \" , \" 키를 사용해 동영상 프레임을 앞뒤로 이동하는 데 도움이 많이 되었어요.\n\n![이미지1](/assets/img/2024-06-19-ANF-16G-LOCAUTO-GCASANDTHEPYTHON_1.png)\n\n![이미지2](/assets/img/2024-06-19-ANF-16G-LOCAUTO-GCASANDTHEPYTHON_2.png)\n\n데이터 분석 및 시각화를 위해 Pandas DataFrame, Numpy 및 Matplotlib과 같은 Python 라이브러리를 사용했어요. 먼저, 이전에 만든 Excel 파일을 가져와 데이터프레임을 생성해볼게요.\n\n<div class=\"content-ad\"></div>\n\n다음으로, 'TIME' 열을 \"datetime\" 형식으로 변환하겠습니다.\n\n마지막으로, 데이터 유형이 올바른지 확인해보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![그림](/assets/img/2024-06-19-ANF-16G-LOCAUTO-GCASANDTHEPYTHON_5.png)\n\n이제 Matplotlib을 사용하여 고도 손실이 어떻게 변하는지 확인해 봅시다. 그래프를 보면 F-16이 약 18,000피트에서 급속히 다이빙합니다. AUTO-GCAS 시스템은 17:26:23에 작동되어 4,500피트에서 비행기를 12,000피트 이상까지 다시 올립니다.\n\n![그림](/assets/img/2024-06-19-ANF-16G-LOCAUTO-GCASANDTHEPYTHON_6.png)\n\n이어서, 탈고한 고도에 따라 다이빙 중 속도가 어떻게 변하는지 살펴봅시다:\n\n\n<div class=\"content-ad\"></div>\n\n![Image 1](/assets/img/2024-06-19-ANF-16G-LOCAUTO-GCASANDTHEPYTHON_7.png)\n\n![Image 2](/assets/img/2024-06-19-ANF-16G-LOCAUTO-GCASANDTHEPYTHON_8.png)\n\n현재 AUTO-GCAS 시스템은 F-16에서 활발히 사용되고 있습니다. 제공된 데이터에 따르면, 이 시스템은 CFIT(구동 비행대상지표면에 비행) 사건에서 항공기 손실의 26%와 모든 F-16 조종사의 75%를 예방했습니다. 본 시스템은 현재 F-35 전투기에서 수동적으로 사용되고 있으며(Manual GCAS), 2019년에 F-35용 AUTO-GCAS 시험을 시작했으며, 향후 활발하게 사용될 것입니다.\n\n참고문헌:","ogImage":{"url":"/assets/img/2024-06-19-ANF-16G-LOCAUTO-GCASANDTHEPYTHON_0.png"},"coverImage":"/assets/img/2024-06-19-ANF-16G-LOCAUTO-GCASANDTHEPYTHON_0.png","tag":["Tech"],"readingTime":4},{"title":"파이썬 데이터 분석 현대 예술가에 대해 알고 있는 것은 무엇인가요","description":"","date":"2024-06-19 15:43","slug":"2024-06-19-PythonDataAnalysisWhatDoWeKnowAboutModernArtists","content":"\n\n<img src=\"/assets/img/2024-06-19-PythonDataAnalysisWhatDoWeKnowAboutModernArtists_0.png\" />\n\n인기 있는 현대 문화에 대해 다양한 의견이 존재할 수 있습니다; 이것은 일상생활의 중요한 부분일뿐만 아니라 수십억 달러 규모의 비즈니스입니다. 수천 명의 예술가들이 다양한 장르로 새로운 작품을 만드는데, 그 안에 흥미로운 패턴을 찾을 수 있을까요? 실제로 그렇습니다. 이 기사에서는 위키피디아 데이터를 추출, 분석 및 시각화하는 방법을 보여드리겠습니다.\n\n왜 위키피디아일까요? 이에 대한 몇 가지 이유가 있습니다. 첫째, 이것은 많은 사람들의 지지를 받는 오픈 소스 백과사전이며, 예술가가 영향력이 클수록 해당 예술가에 대한 상세한 문서가 작성될 가능성이 높습니다. 둘째, 거의 모든 위키피디아 페이지에는 다른 페이지로의 하이퍼링크가 있습니다. 이를 통해 \"맨 눈\"으로 쉽게 알아차리기 어려운 다양한 패턴을 추적할 수 있습니다. 예를 들어, 특정 장르에서 활동하는 예술가 그룹이나 특정 주제에 대해 노래를 만드는 예술가들을 볼 수 있습니다.\n\n## 방법론\n\n<div class=\"content-ad\"></div>\n\n분석을 수행하기 위해 몇 가지 단계를 구현할 것입니다:\n\n- 데이터 수집. 이에 대해 개방 소스 위키백과 라이브러리를 사용할 것입니다. 데이터는 NetworkX 그래프로 저장될 것이며, 각 노드는 위키피디아 페이지를 나타내고 각 엣지는 한 페이지에서 다른 페이지로의 링크를 나타냅니다.\n- 전처리. 위키피디아 페이지에는 많은 정보가 포함되어 있지만, 모든 데이터가 분석에 관련되는 것은 아닙니다. 예를 들어 비디오 게임 음악가에 관한 페이지는 음악가뿐만 아니라 \"듀크 누켐\"이나 \"콜 오브 듀티\"와 같은 게임에 대한 링크도 포함하고 있습니다. AI Large Language Model (LLM)을 사용하여 모든 노드를 다른 카테고리로 그룹화할 것입니다.\n- 데이터 분석. NetworkX 라이브러리를 활용하여 카테고리에서 가장 인기 있는 또는 중요한 노드를 찾을 수 있게 될 것입니다.\n- 데이터 시각화. 이것은 그 자체로 큰 주제이며, 다음 파트에서 게시할 것입니다. NetworkX와 D3.js를 사용하여 그래프를 그리는 방법을 보여드리겠습니다.\n\n시작해봅시다!\n\n## 1. 데이터 수집\n\n<div class=\"content-ad\"></div>\n\n이 문서는 예술가에 관한 정보에 초점을 맞추고 있습니다. 시작 지점으로 위키백과 음악가 목록을 활용했습니다. 이 페이지에는 많은 추가 정보가 있어서 필요한 부분만 텍스트 파일로 저장했습니다:\n\n```js\n# A\nList of acid rock artists\nList of adult alternative artists\n...\n\n# W\nList of West Coast blues musicians\n```\n\n또한 Python 리스트로 이 파일을 로드하는 도우미 메서드를 작성했습니다:\n\n```js\ndef load_links_file(filename: str) -> List:\n    \"\"\" 텍스트 파일에서 위키백과 링크 목록을 불러옵니다 \"\"\"\n    with open(filename, \"r\", encoding=\"utf-8\") as f_in:\n        root_list = filter(lambda name: len(name.strip()) > 1 and name[0] != \"#\", f_in.readlines())\n        root_list = list(map(lambda name: name.strip(), root_list))\n    return root_list\n```\n\n<div class=\"content-ad\"></div>\n\n위키피디아 페이지를 읽기 위해 (당연한) 이름을 가진 오픈 소스 Python 라이브러리를 사용할 거에요. 동일한 링크가 다른 페이지에 들어 있을 수 있기 때문에 Python lru_cache를 사용하여 로딩을 더 빠르게 만드는 것이 좋아요:\n\n```js\nfrom functools import lru_cache\nimport wikipedia\n\n@dataclass\nclass PageData:\n    \"\"\" 위키피디아 페이지 데이터 \"\"\"\n    url: str\n    title: str\n    content: str\n    links: List\n\n@lru_cache(maxsize = 1_000_000)\ndef load_wiki_page(link_name: str) -> Optional[PageData]:\n    \"\"\" 위키피디아 페이지 로딩 \"\"\"\n    try:\n        page = wikipedia.page(link_name)\n        return PageData(page.url, page.title, page.content, page.links)\n    except Exception as exc:\n        print(f\"load_wiki_page 오류: {exc}\")\n    return None\n```\n\n위키피디아 라이브러리를 사용하면 작업이 간단해요. 우선적으로 텍스트 파일에서 링크 목록을 로드할 거에요. 각각의 위키피디아 링크(예를 들어, 앰비언트 음악 아티스트 목록)는 10-100개의 이름을 포함해요. 결과적으로 약 2000개의 위키피디아 페이지 목록을 얻게 될 거에요 (이 목록을 pop_artists.txt 파일에 저장했어요). 두 번째 단계로, load_wiki_page 함수를 다시 사용할 거에요. 노드와 엣지는 그래프에 저장될 거에요:\n\n```js\ndef load_sub_pages(graph: nx.Graph, page_data: PageData):\n    \"\"\" 모든 링크를 그래프 노드 및 엣지로 추가 \"\"\"\n    for link_name in page_data.links:\n        sub_title = link_name\n        graph.add_node(sub_title)\n        graph.add_edge(page_data.title, sub_title)\n\n# 그래프 생성\nG = nx.Graph()\n\nroot_list = load_links_file(\"pop_artists.txt\")\n# 검색\nfor artist_name in root_list:\n    page_data = load_wiki_page(artist_name)\n    if page_data:\n        G.add_node(page_data.title)\n        load_sub_pages(G, page_data)\n\n# 파일로 저장\nnx.write_gml(G, \"graph.gml\")\n```\n\n<div class=\"content-ad\"></div>\n\n코드의 간단함에도 불구하고 프로세스가 오래 걸립니다. 페이지 수가 많고, Wikipedia API도 세계에서 가장 빠른 것은 아닙니다. 실제로, 데이터 수집에 약 하루가 걸렸고 (그 작업에는 라즈베리 파이를 사용했습니다), 출력 GML 파일의 크기는 약 200MB였습니다.\n\n그래프를 저장하면 Jupyter Notebook에서 그 노드를 볼 수 있습니다:\n\n```js\nnodes = [(name, len(G.edges(name))) for name in G.nodes()]\n\ndf_nodes = pd.DataFrame(nodes, columns=['이름', '에지 수'])\ndisplay(df_nodes.sort_values(by=[\"에지 수\"], ascending=False))\n```\n\n여기서 그래프 노드를 모두 에지 수별로 정렬했습니다. 모든 것을 올바르게 수행했다면, 출력은 다음과 같아야 합니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-PythonDataAnalysisWhatDoWeKnowAboutModernArtists_1.png\" />\n\n## 2.1 데이터 전처리\n\n첫 번째 단계에서는 다양한 아티스트에 관한 데이터를 수집하여 그래프를 만들었지만, 실제로는 아직 그렇게 유용하지는 않습니다. 예를 들어, 스크린샷에서 \"힙합 음악,\" \"올뮤직,\" \"미국\"과 같은 노드들을 볼 수 있습니다. 이 모든 용어들은 분명히 다른 주제에 속해 있으며, 이를 개별적으로 분석하는 것이 좋을 것입니다. 이 문서는 아티스트에 중점을 두고 있으며, 보다 자세한 분석을 위해 모든 노드를 7가지 카테고리로 나누기로 결정했습니다:\n\n```js\ncategories = [\n    \"person\", \"place\", \"country\",\n    \"music style\", \"music instrument\", \"music band\",\n    \"other\"\n]\n```\n\n<div class=\"content-ad\"></div>\n\n이제 각 그래프 노드에 적합한 카테고리를 찾아야 합니다. 하지만 말하기보다는 행동하기가 훨씬 어렵습니다. 모든 위키피디아 아티스트 페이지에서 \"링크\" 객체는 음악 악기부터 LGBT 권리까지 모든 것을 포함할 수 있는 평평한 파이썬 리스트입니다. 한 하위 그룹의 링크를 다른 하위 그룹과 구별하는 쉬운 방법이 없습니다. \"힙합\"이나 \"런던\" 같은 단어에 대한 카테고리를 찾는 것은 자연 언어 처리(NLP) 문제이므로, 각 단어에 대한 카테고리를 찾기 위해 대형 언어 모델(LLM)을 사용하기로 결정했습니다.\n\n저는 이 작업을 위해 무료 Llama-3 8B 모델을 사용하기로 결정했습니다. 몇 가지 테스트 후 다음과 같은 프롬프트를 만들었습니다:\n\n```js\ncategories_str = ', '.join(categories)\n\n\nprompt_template = f\"\"\"\n당신은 음악 전문가입니다.\n카테고리가 {len(categories)}개 있습니다: {', '.join(categories)}.\n단어 목록을 제공할 테니, 각 단어에 대한 카테고리를 작성해 주세요.\n출력은 JSON 형식으로 [{word: category}, ...] 작성합니다.\n다음은 예시입니다.\n\nWords:\nRap; John; Billboard Top 100\n\nYour Answer:\n[\n  {\"Rap\": \"음악 스타일\"},\n  {\"John\": \"인물\"},\n  {\"Billboard Top 100\": \"기타\"},\n]\n\n이제 시작! 다음은 단어 목록입니다:\n---\n_ITEMS_\n---\n\n답변을 작성해 주세요.\n\"\"\"\n```\n\n저는 이미 LLM을 사용하여 판다스 데이터프레임을 처리하는 방법에 대해 기사를 작성했습니다. 독자들은 여기서 더 많은 세부 정보를 찾을 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n해당 기사에서 제공하는 코드를 사용하여 몇 줄의 코드로 데이터프레임을 처리할 수 있었어요:\n\n```js\n# 그래프 불러오기\nG = nx.read_gml(\"graph.gml\")\n\n# 데이터프레임 생성\nnodes = [(name, len(G.edges(name))) for name in G.nodes()]\ndf_nodes = pd.DataFrame(nodes, columns=['이름', '엣지'])\n\n# 필터링\nedge_threshold = 400\ndf_top_nodes = df_nodes[df_nodes[\"엣지\"] > edge_threshold].copy()\n\n# 처리\ndf_top_nodes[\"유형\"] = llm_map(df_top_nodes[\"이름\"], batch_size=10)\n```\n\n여기서 두 가지 기교를 사용했어요:\n\n- 먼저, 많은 엣지를 가진 그래프 노드만 처리했어요. 이유는 단순합니다 — 그래프에서 가장 중요한 노드를 찾고 싶기 때문이에요. 엣지 수가 적은 노드는 어차피 상위 목록에서 제외될 거예요. 모든 노드를 처리해도 괜찮지만, 900,783개의 노드를 LLM으로 처리하려면 시간이나 돈이 너무 많이 들어요.\n- 처리 시 작은 배치 크기 10을 사용했어요. 프롬프트에서 볼 수 있듯이, 모델에 JSON을 응답으로 생성하라고 요청했는데, 8B Llama-3 모델이 그렇게 좋지 않다는 것을 알게 됐어요. Llama-3는 8,192 토큰의 컨텍스트 길이를 갖고 있어 이론상으로 아주 긴 프롬프트를 처리할 수 있어야 하지만, 실제로는 잘 작동하지 않아요. 텍스트 크기가 증가하면 모델이 실수하기 시작해요. 예를 들어, 일반적인 JSON처럼 보이지만, 중간에 콜론이 빠져 파이썬 파서가 더 이상 디코딩할 수 없는 이러한 형태의 JSON을 생성할 수도 있어요:\n\n<div class=\"content-ad\"></div>\n\n\n[\n  {\"Rap\": \"음악 장르\"},\n  {\"John\": \"사람\"},\n  {\"Billboard Top 100\": \"기타\"},\n  ...\n]\n\n\n일반적으로 Google Colab에서 GPU 유형 및 항목 수에 따라 처리 시간은 30~60분이 소요됩니다. 8B 모델의 RAM 요구 사항이 낮고 무료 Colab 계정으로도 작업을 수행할 수 있습니다. OpenAI 키를 가지고 있는 독자는 더 정확한 결과를 얻을 수 있지만 처리는 무료가 아닙니다.\n\n처리가 완료되면 데이터프레임에서 새로운 유형 필드를 그래프로 다시 저장할 수 있습니다:\n\n```python\nfor index, row in df_top_nodes.iterrows():\n    node_name, node_type = row['Name'], row['Type']\n    if node_type is not None:\n        nx.set_node_attributes(G, {node_name: node_type}, \"type\")\n\nnx.write_gml(G, \"graph_updated.gml\")\n```\n\n<div class=\"content-ad\"></div>\n\n## 2.2 평가\n\n언어 모델이 완벽하지 않다는 것을 알고 있습니다. 심지어 대형 모델도요. 데이터를 데이터프레임에 다시로드하고 결과를 확인해보겠습니다:\n\n```js\nG = nx.read_gml(\"graph_updated.gml\")\n\nnodes = [(name, len(G.edges(name)), data[\"type\"]) for name, data in G.nodes(data=True)]\n\ndf_nodes = pd.DataFrame(nodes, columns=['Name', 'Edges', 'Type'])\ndisplay(df_nodes.sort_values(by=[\"Edges\"], ascending=False)[:20])\n```\n\n여기서 Type 열에는 LLM에서 얻은 결과가 포함되어 있습니다. 또한 엣지 수에 따라 노드를 정렬하고 처음 20개 항목을 표시했습니다. 결과는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![Python Data Analysis](/assets/img/2024-06-19-PythonDataAnalysisWhatDoWeKnowAboutModernArtists_2.png)\n\n보시다시피 무료 모델을 사용하여 카테고리를 찾았기 때문에 충분히 좋다고 할 수 있습니다. 그러나 결과는 항상 정확하지는 않으며 선택적으로 수정할 수 있습니다. 예를 들어 Llama-3 모델은 \"MTV\"를 장소로 결정했습니다. 일반적으로 말하자면 그것은 사실일 수 있지만, 음악 산업 문맥에서는 최선의 선택은 아닙니다. 수동으로 수정할 수 있습니다:\n\n```javascript\nfix_nodes = {\n    \"MTV\": \"other\",\n    \"Latin Church\": \"other\",\n    ...\n}\nnx.set_node_attributes(G, fix_nodes, \"type\")\n```\n\n어쨌든, 8B Llama-3 모델이 대부분의 작업을 수행했고, 제 코드에서 작은 수의 노드(약 30개)만 수동으로 수정해야 했습니다. 이전에 언급했듯이 독자들은 OpenAI나 다른 공개 API를 사용할 수도 있습니다. 결과는 더 정확할 수 있지만 처리는 더 이상 무료가 아닐 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 3.1 분석\n\n마침내 더 재미있는 부분에 다가가고 있습니다 — 위키피디아 아티스트 덤프로부터 어떤 종류의 데이터를 얻을 수 있는지 살펴봅시다. 상기해 드리지만, 나는 최신 음악가 목록에서 모든 위키피디아 페이지를 저장하고 모든 페이지와 그 내부 링크를 그래프에 넣었습니다. LLM의 도움으로 그래프 노드를 \"person(사람)\", \"place(장소)\", \"country(국가)\", \"music style(음악 스타일)\", \"music instrument(악기)\", \"music band(음악 밴드)\", \"other(기타)\" 다섯 가지 범주로 나누었습니다.\n\n제가 먼저 확인할 매개변수는 연결 중심성(degree centrality)이라고 하는데요. 노드의 연결 중심성은 해당 노드가 연결된 노드의 비율입니다. NetworkX를 사용하면 한 줄의 코드로 이를 얻을 수 있습니다:\n\n```js\ndegree_centrality = nx.degree_centrality(G)\ndisplay(degree_centrality)\n\n#> {'Rapping': 0.00138, '106 and Park': 3.3304, ... \n#   '2013 Billboard Music Awards': 1.22116e-05}\n```\n\n<div class=\"content-ad\"></div>\n\n이 데이터를 사용하여 가장 인기 있는 상위 20개 음악 스타일을 찾아보겠습니다:\n\n```js\n노드 = [(이름, 데이터) for 이름, 데이터 in G.nodes(data=True) if 데이터[\"type\"] == \"music style\"]\n노드_이름 = [이름 for 이름, _ in 노드]\n노드 중심성 = [degree_centrality[이름] for 이름, _ in 노드]\n노드_엣지 = [len(G.edges(이름)) for 이름, _ in 노드]\n\ndf_중심성 = pd.DataFrame({\n    \"이름\": 노드_이름,\n    \"중심성\": 노드_중심성,\n    \"엣지 수\": 노드_엣지\n})\ndisplay(df_중심성.sort_values(by=[\"중심성\"], ascending=False)[:20])\n```\n\n출력 결과는 다음과 같습니다:\n\n<img src=\"/assets/img/2024-06-19-PythonDataAnalysisWhatDoWeKnowAboutModernArtists_3.png\" />\n\n<div class=\"content-ad\"></div>\n\n우리는 그래프 형태로도 확인할 수 있어요:\n\n![그래프](/assets/img/2024-06-19-PythonDataAnalysisWhatDoWeKnowAboutModernArtists_4.png)\n\n물론, 이 \"위키백과 순위\"는 특정 장르의 활동 청취자 수를 나타내는 것이 아니라 위키백과에서의 인기를 나타냅니다. 이 값들이 상관 관계가 있는지 확인하는 것은 흥미로울 수 있지만, 다른 국가와 미디어 유형(스트리밍, 판매 등)에 따라 다른 장르의 인기가 다르기 때문에 쉽지 않을 수 있어요. 그래도 독자들은 자신이 사는 국가의 차트와 비교해보시면 좋을 것 같아요.\n\n이제 위키백과에서 가장 인기 있는 20명의 아티스트를 표시해볼까요? 코드는 동일하며, 카테고리 필터만 \"인물\"로 변경했어요. 결과는 다음과 같이 나와요:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-PythonDataAnalysisWhatDoWeKnowAboutModernArtists_5.png\" />\n\n여기서 데이터 처리 과정에서 재미있는 오류를 볼 수 있어요. \"Jesus\"나 \"Zeus\"와 같은 몇 가지 이름은 분명히 예술가가 아닙니다. 그리고 미국 교수인 노암 촘스키도 마찬가지에요. LLM은 이러한 노드들을 \"사람\"으로 정확히 식별했고, 이 링크들은 아마도 어떤 음악가의 페이지에 있었을 거에요. 실제 예술가들에 대해서는, 비욘세가 우리 목록 상단에 위치하고 있으며, 인도 작곡가 A. R. 라만과 가까이 있어요. 다른 이름들은 독자들이 직접 확인할 수 있어요.\n\n우리의 \"위키백과 평가\"가 현대 청취자들의 선호도와 어떻게 관련이 있는지 궁금하셨죠? 이 표를 2024년 가장 많은 스트리밍 아티스트 위키백과 페이지와 비교해봤어요. 흥미로운 점은 상관성이 그리 크지 않다는 거에요. 예를 들어, 비욘세는 우리 평가에서 1위에 있지만 스포티파이에서는 22위에 있어요. 프랭크 신트라나, 엘비스 프레슬리, 셀린 디온과 같은 다른 이름들 중 많은 사람들이 스포티파이 상위에 없는 이유는 무엇일까요? 이에 대해 두 가지 생각이 있어요:\n\n- 스트리밍 서비스에서 음악을 듣는 건 대부분 \"수동적\"이에요. 청취자들은 아티스트에 대해 알아가거나 정보를 읽거나 추가하는 데 적극적으로 참여하지 않아요. 그냥 배경에서 흘러나오는 라디오처럼, 사람들은 실제로 현재 어떤 트랙이 재생되고 있는지 그다지 중요하게 생각하지 않아요. 게다가, 현재 재생 목록은 Spotify 추천 시스템에 의해 생성될 수도 있어요.\n- 아티스트의 \"위키백과 평가\"는 듣는 사람의 수보다는 그 아티스트가 음악 및 사회에 기여한 정도와 관련이 더 크다고 생각해요. 예를 들어, 프랭크 신트라나를 요즘 사람들이 적극적으로 듣고 있다고 생각하지 않지만, 그가 문화, 음악 산업, 그리고 다른 아티스트들의 작품에 한 몫을 한 것은 분명해요.\n\n<div class=\"content-ad\"></div>\n\n어쨌든, 결과가 흥미로워요. 보다 자세한 연구를 할 수 있을 거에요.\n\n다음으로 흥미로운 지표는 매개 중심성입니다 — 다른 노드 사이의 최단 경로에서 다리 역할을 하는 노드를 측정하는 값이에요. 음악 산업의 맥락에서는 여러 장르에서 활동하는 아티스트일 수 있어요.\n\nNetworkX로 매개 중심성을 측정할 수 있어요:\n\n```js\nnode_types = nx.get_node_attributes(G, \"type\")\n\ndef filter_node(node: str):\n    \"\"\" 특정 유형의 노드 가져오기 \"\"\"\n    return node_types[node] == \"person\"\n\nGf = nx.subgraph_view(G, filter_node=filter_node)\n\nbetweenness_centrality = nx.betweenness_centrality(Gf)\n```\n\n<div class=\"content-ad\"></div>\n\n결과적으로 안타깝게도, 적어도 내 PC에서는 작동하지 않았습니다. 가장 짧은 경로를 찾으려면 많은 계산이 필요하며, 수천 개의 노드에 대해서도 너무 오랜 시간이 걸리고 마지막 결과를 얻을만큼 인내심이 부족했어요. 더 최적화된 라이브러리가 있는지 모르겠는데, 혹시 CUDA 지원이 있는 것 같아요; 답을 아시는 분은 댓글에 자유롭게 적어주세요.\n\n## 3.2 커뮤니티 탐지\n\n이 글에서 시도해볼만한 흥미로운 마지막 주제는 커뮤니티 탐지입니다. 그래프는 그래프의 노드들이 노드 집합으로 그룹화될 수 있는 경우 커뮤니티 구조를 가졌다고 합니다. 예를 들어, 서로 다른 장르에서 활동하는 아티스트들의 그룹을 찾아봅시다.\n\n먼저 그래프를 필터링해봅시다:\n\n<div class=\"content-ad\"></div>\n\n```python\nnode_types = nx.get_node_attributes(Gf, \"type\")\n\ndef filter_node(node: str):\n    \"\"\" 특정 타입의 노드 가져오기 \"\"\"\n    return node_types[node] in (\"person\", \"music style\")\n\nGf = nx.subgraph_view(Gf, filter_node=filter_node)\n```\n\nNetworkX에는 커뮤니티를 찾기 위한 다양한 알고리즘이 많이 있습니다. 몇 가지를 시도해 봅시다.\n\nGirvan-Newman 알고리즘은 원래 그래프에서 엣지를 점진적으로 제거하여 커뮤니티를 감지합니다:\n\n```python\ncommunities_generator = nx.community.girvan_newman(Gf)\nfor com in itertools.islice(communities_generator, 7):\n    communities_list = com\n\nprint(communities_list)\n#> ({'A. D. King', 'A. R. Rahman', 'Adam', 'Admiral', ... },\n#   {'Cheyenne', 'Jesse James'}, ... }\n```\n\n<div class=\"content-ad\"></div>\n\n수백 개의 항목의 분리 품질을 수동으로 추정하는 것은 어렵지만 시각적 형태로 완료할 수 있습니다. 제 경우, 결과가 완벽하지는 않았어요:\n\n<img src=\"/assets/img/2024-06-19-PythonDataAnalysisWhatDoWeKnowAboutModernArtists_6.png\" />\n\n알고리즘은 그래프 주변의 가장 큰 \"블롭\"과 몇 개의 짧은 \"끈\"만 감지했어요.\n\n다른 접근 방식을 시도해 보죠. 탐욕적 모듈성 최대화 알고리즘은 Clauset-Newman-Moore 탐욕적 모듈성 최대화를 사용하여 커뮤니티를 찾아요:\n\n<div class=\"content-ad\"></div>\n\n```js\ncommunities_list = nx.community.greedy_modularity_communities(Gf,cutoff=10)\n```\n\n결과는 다음과 같이 나옵니다:\n\n<img src=\"/assets/img/2024-06-19-PythonDataAnalysisWhatDoWeKnowAboutModernArtists_7.png\" />\n\nD3.JS를 사용하여 수동 검증 결과가 더 흥미로운 것으로 나타났습니다. 예를 들어, 그래프의 왼쪽에 K-pop(한국 팝) 스타일로 활동하는 음악가들의 별도 그룹이 쉽게 보입니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-PythonDataAnalysisWhatDoWeKnowAboutModernArtists_8.png)\n\n다른 흥미로운 노드 그룹은 예술가가 아닌 철학자들을 포함하고 있습니다. 이것을 보는 것도 흥미로웠습니다.\n\n![image](/assets/img/2024-06-19-PythonDataAnalysisWhatDoWeKnowAboutModernArtists_9.png)\n\n물론, 도시, 국가 또는 연주하는 악기 등의 다른 유형의 커뮤니티도 발견할 수 있습니다. 가능한 범주 목록도 확장될 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 결론\n\n이 글에서는 위키피디아 페이지에서 그래프를 만드는 과정을 설명했습니다. 분석을 위해 약 900,000개의 노드로 이루어진 그래프를 수집했는데, 이 그래프는 현대 예술가들에 대한 정보(페이지 및 링크)를 담고 있습니다. 또한, 이러한 노드들을 AI로 처리하여 카테고리로 분할했습니다. NetworkX 라이브러리를 활용하여 이 그래프에서 흥미로운 패턴을 찾고 가장 중요한 노드를 찾을 수 있었습니다.\n\n요즘에는 현대 사회에서 영향력 있는 사람들마다 위키피디아 페이지가 있습니다. 서로 다른 페이지들은 서로 연결되어 있으며, 이 데이터의 분석은 사회과학, 음악학, 문화 인류학에 중요할 수 있습니다. 결과적으로, 그 결과물은 흥미로웠습니다. 예를 들어, \"위키피디아 최고 평점\"에 있는 인기있는 예술가들이 가장 높은 스포티파이 평점과 일치하지 않습니다. 이것은 제게 놀랍고, 팬들이 자주 방문(또는 업데이트)하는 아티스트의 위키피디아 페이지를 얼마나 자주 방문하는지 생각해보는 것이 흥미로울 것입니다.\n\n물론, 같은 분석은 예술가 뿐만 아니라 정치, 스포츠 및 다른 분야에서도 수행될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n소셜 데이터 분석에 관심이 있는 분들은 다른 아티클도 읽어보세요:\n\n- 탐색적 데이터 분석: YouTube 채널에 대해 우리는 무엇을 알고 있는가\n- 독일 주택 임대 시장: Python으로 하는 탐색적 데이터 분석\n- 기후에 관해 사람들이 쓰는 내용: Python으로 하는 트위터 데이터 클러스터링\n- 트위터 게시물에서 시간적 패턴 찾기: Python으로 하는 탐색적 데이터 분석\n- Python 데이터 분석: 팝송에 대해 우리는 무엇을 알고 있는가?\n\n만약 이 이야기를 즐겼다면, Medium에 구독해도 좋고, 새로운 아티클이 발행될 때 알림을 받을 수 있습니다. 수천 편의 다른 저자의 이야기에도 완전히 액세스할 수 있습니다. 또한 LinkedIn을 통해 연락하셔도 좋습니다. 이와 다른 게시물들의 전체 소스 코드를 얻고 싶다면 Patreon 페이지를 방문해주세요.\n\n읽어 주셔서 감사합니다.","ogImage":{"url":"/assets/img/2024-06-19-PythonDataAnalysisWhatDoWeKnowAboutModernArtists_0.png"},"coverImage":"/assets/img/2024-06-19-PythonDataAnalysisWhatDoWeKnowAboutModernArtists_0.png","tag":["Tech"],"readingTime":15},{"title":"WEB Du Bois의 업적이 되풀이 할 가치 있는 데이터 스토리인 이유","description":"","date":"2024-06-19 15:41","slug":"2024-06-19-WhyTheWorkofWEBDuBoisisaDataStoryWorthRetelling","content":"\n\n![Image](/assets/img/2024-06-19-WhyTheWorkofWEBDuBoisisaDataStoryWorthRetelling_0.png)\n\n요즘에 방콕에 있는 친구를 방문했었는데, 그 친구가 데이터 스토리텔링에 관심이 있다는 걸 알고 멋진 책을 선물해주었어요.\n\n이 책은 내용, 디자인, 단순함, 그리고 시대를 초월한 멋진 면에서 정말 훌륭해요.\n\nW.E.B. Du Bois의 Data Portraits: Visualizing Black America (휘트니 배틀-바티스트와 브릿 루설트의 편집)은 W.E.B. Du Bois와 그 팀이 인종 차별 반대를 위해 데이터 시각화를 도구로 활용한 노력을 문서화한 책이에요.\n\n<div class=\"content-ad\"></div>\n\n이 책(및 듀보이스 박사의 작품)은 흑인 역사에 대한 교훈일 뿐만 아니라, 현대 데이터 시각화와 이야기의 기원을 엿볼 수 있는 창이기도 합니다.\n\n이 컬렉션에는 1900년 파리 만국 박람회를 위해 만들어진 일련의 훌륭한 차트, 그래프 및 지도가 특징화되어 있습니다.\n\n듀보이스 박사(흑인 및 하버드 대학교 박사 학위 소지자)와 그의 연구진이 함께 제작한 데이터 시각화는 해방(1863년)에서 20세기 초반까지의 흑인 사회 경제 상황을 명확하게 보여 줍니다.\n\n# 현대 데이터 시각화의 탄생\n\n<div class=\"content-ad\"></div>\n\n'배틀-바티스테와 루서트' 책 편집자들이 언급했듯이, 드. 듀 보이의 데이터 시각화 초상화는 사회학과 통계학이 오늘날 우리가 친숙한 학문 전문화와 지식 구조로 굳어지기 직전의 순간을 반영하고 있습니다.\n\n드. 듀 보이는 많은 데이터 시각화 개념을 개척했으며, 학계 이외의 대중에 호소하기 위해 데이터의 이야기와 설명을 보다 심층적으로 감정의 심장까지 다가갈 필요가 있다는 것을 이해했습니다. 1900년 파리 전시회를 위해 이는 대중을 의미합니다.\n\n이를 위해 드. 듀 보이는 어떻게 해야 할지 검토하여 서구에서 흔히 퍼져 있는 아이디어를 반박하는 지도상의 표현으로 시작했습니다. 그 당시 서양에서는 흑인들이 문화, 역사 또는 전망이 없어 열등하다는 일반적인 관념을 반박했습니다:\n\n![image](/assets/img/2024-06-19-WhyTheWorkofWEBDuBoisisaDataStoryWorthRetelling_1.png)\n\n<div class=\"content-ad\"></div>\n\n수백 년 동안 아프리카인을 다양한 대서양 지역으로 운송한 노예 무역의 경로를 보여주는 이 지도는 세 대륙에서 흑인들의 분포를 시각적으로 나타냅니다(전체 약 1200-1700만 명으로 추정).\n\n1900년 파리 전시회의 맥락에서 이 지도의 중요성은 특정한 시간과 장소에서 흑인들이 스스로를 인식했다는 아이디어를 의식하게 했습니다.\n\n1900년 세기의 변경점에서, 미국은 완전 노예 제도로부터 단 35년이 지났습니다. 이 훌륭한 시각화는 미국의 많은 흑인들이 해방되기 전 노예였음을 명확하게 보여줍니다:\n\n![](/assets/img/2024-06-19-WhyTheWorkofWEBDuBoisisaDataStoryWorthRetelling_2.png)\n\n<div class=\"content-ad\"></div>\n\n여기서 어두운 색깔을 사용한 것 - 노온색으로 노온자를 나타내는 것 - 이 인상적입니다. 저에게는 시각화를 압도하는 큰 악성 종양 덩어리처럼 보입니다. 1860년 기준으로, 전체 아프리카계 미국인 인구의 거의 90%가 노온자였습니다.\n\n데이터 시각화 관점에서 이 시각화를 가장 인상적으로 만드는 두 번째 특징은 분명히 손으로 그려졌다는 것입니다. 이것은 그것에 들어간 세심한 주의와 감정을 나타내며, 독특하고 오리지널한 시각화로서의 신뢰성과 중요성을 더합니다.\n\n파리 전시에서 국제 청중과의 연결 필요를 이해한 드. 디. 부아는 프랑스 사람들과 조지아 주의 흑인들을 비교하기 위해 다음과 같은 쌓인 막대 꺾은선 그래프를 상상했습니다:\n\n![이미지](/assets/img/2024-06-19-WhyTheWorkofWEBDuBoisisaDataStoryWorthRetelling_3.png)\n\n<div class=\"content-ad\"></div>\n\n이 차트는 시대적으로 좌절한 흑인들이 열등하다는 생각을 반박하며, 다윈의 진화 이론에 따르면, 동일종의 열등한(불리한) 구성원들은 서서히 사라져 우수한(유리한) 구성원들만 생존하는 것에 대해 바로 전해줍니다.\n\n당시 서양의 더 나은 것에 대한 어리석은 이론 중 하나는, 흑인들이 열등한 지능으로 인해 대부분 문맹일 것이라는 것이었습니다. 듀 보이스 박사와 그의 팀은 아프리카계 미국인의 문맹률을 잘 알려진 서양 국가와 비교한 차트를 개발했습니다:\n\n![차트](/assets/img/2024-06-19-WhyTheWorkofWEBDuBoisisaDataStoryWorthRetelling_4.png)\n\n이 차트는 당시 아프리카계 미국인의 문맹률이 사실 러시아, 세르비아, 루마니아보다 낮았음을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n1900년 파리 전시회의 맥락에서 보면, 듀보이스 박사와 그의 팀은 자신들의 관객이 누구인지 이해하고, 그들에게 가장 효과적인 데이터 시각화와 이야기가 무엇인지를 이해했음을 보여 주었습니다.\n\n# 듀보이스 박사는 시대를 앞서간 인물이었습니다\n\n이 책은 또한 듀보이스 박사가 자신의 사고와 실천에서 선물했던 몇 가지 방법을 강조합니다.\n\n애틀랜타 대학에서 듀보이스 박사는 사회학과 경험적 방법을 가르치기에 선구자였습니다. 듀보이스 박사는 자신의 학생들을 남부 흑인 커뮤니티와 인종 간 관계에 대한 데이터 수집 및 분석 현장조사에 참여하도록 유도했습니다. 그 중 일부 현재와 전 학생들은 1900년 파리 전시회를 위해 시각화를 만드는 데 협력했습니다.\n\n<div class=\"content-ad\"></div>\n\n흑인 여성들은 이 아름다운 시각화물을 만드는 연구에 지식, 전문성, 그리고 시간을 기여한 연구원 중 하나였습니다 (사회학적 지식의 공동 제작자)\n\n저자들이 놀라운데로 주목한 바에 의하면, 차트 중 하나는 전 노예가 조각한 나무 프레임 안에 표시되어 있었습니다 — 현재 상태를 역사적으로 전진하지 않았거나 노예의 과거를 극복하지 않았다는 뜻으로 표현했습니다.\n\n# 요약\n\n“W.E.B. Du Bois’s Data Portraits: Visualizing Black America”를 통해 읽은 후에, 나에게 주는 두 가지 주요 요점이 있습니다. 이 책이 다음과 같은 역할을 한다는 것을:\n\n<div class=\"content-ad\"></div>\n\n- 역사적으로 중요한 순간에 아프리카계 미국인들의 사회 경제적 지위에 대한 통찰력을 제공하는 역사 문서,\n- 사회 연구와 선전에서 데이터 시각화의 혁신적인 활용을 보여주는 전시품.\n\n이 멋진 책을 이제 나의 서재에 가지고 있다는 것에 매우 기쁩니다. 혁신적인 시각화물을 만드는 방법과 이러한 시각화물을 사용하여 흥미로운 이야기를 전하는 방법에 대한 참고 자료로 사용할 것입니다.\n\n가장 중요한 것은 미국의 흑인들에 대한 인종 평등으로 가는 길이 얼마나 오랜 기간이고 얼마나 어려운지를 상기시키는 것이며, 미국 내 흑인들을 위해 이어지고 있는 것입니다.\n\n읽어주셔서 감사합니다.\n\n<div class=\"content-ad\"></div>\n\n만약 이러한 유형의 이야기가 당신의 취향이고, 저를 작가로 지원하고 싶다면, 나의 Substack를 구독해주세요.\n\nSubstack에서는 나의 독자들을 위해 매주 발행되는 뉴스레터와 다른 플랫폼에서 찾을 수 없는 기사들을 발표하고 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-WhyTheWorkofWEBDuBoisisaDataStoryWorthRetelling_0.png"},"coverImage":"/assets/img/2024-06-19-WhyTheWorkofWEBDuBoisisaDataStoryWorthRetelling_0.png","tag":["Tech"],"readingTime":4},{"title":"데이터 기반 디자인이 어떻게 불안한 시기를 극복하는 데 도움이 될까요","description":"","date":"2024-06-19 15:39","slug":"2024-06-19-HowData-InformedDesigncanhelpyounavigatetheseturbulenttimes","content":"\n\n![image](/assets/img/2024-06-19-HowData-InformedDesigncanhelpyounavigatetheseturbulenttimes_0.png)\n\n디자이너들에게 어려운 해가 지나고 있습니다. 이는 그들의 일자리 손실 뿐만 아니라 디자인의 미래에 대한 우려 때문에 더욱 어려운 시기입니다.\n\nAI가 UX를 대체할 것인가 또는 UX만으로는 충분하지 않을 것인가 등의 우려가 있어서 UX의 미래에 대한 많은 비관적인 얘기가 나오고 있습니다.\n\n당신의 상황을 모르겠지만, 저를 두 번이나 디자인 경력을 살려준 좁은 분야, 데이터 기반 디자인 소개하고 싶습니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 기반 디자인은 AI가 당신의 일자리를 빼앗거나 데이터가 모든 결정을 내리는 침울한 미래가 아닙니다.\n\n![이미지](/assets/img/2024-06-19-HowData-InformedDesigncanhelpyounavigatetheseturbulenttimes_1.png)\n\n대신, Facebook의 디자인 부문 전 부사장인 줄리 주오(Julie Zhuo)와 같은 전문가들이 지지하는 '적당한 양의 데이터' 접근 방식입니다. 이 접근 방식은 모든 곳에서 활용될 수 있으며(특히 낮은 UX/디자인 성숙도 환경에서), 디자인이 더 큰 영향을 미치도록 돕습니다.\n\n## 데이터 기반 디자인은 무엇이며 왜 유용한가요?\n\n<div class=\"content-ad\"></div>\n\n데이터 기반 디자인에 대한 가장 좋은 설명 중 하나는 Fountain Institute에서 나왔어요:\n\n![이미지](/assets/img/2024-06-19-HowData-InformedDesigncanhelpyounavigatetheseturbulenttimes_2.png)\n\n그들의 \"고객 데이터는 디자인 결정을 평가하기 위해 사용됩니다\"라는 설명은 이 분야의 중요한 개념 중 하나인 '성과를 정의하고 평가하는 것'을 제공해요.\n\n세 가지 구체적인 질문을 통해 데이터 기반 디자인을 설명할 수 있어요:\n\n<div class=\"content-ad\"></div>\n\n- 사용자의 문제를 해결했다는 것을 여섯 개월 후에 어떻게 알 수 있습니까?\n- UX의 가치를 비즈니스에 어떻게 보여줄 수 있습니까?\n- 디자인 권고 사항에 대해 사람들을 설득하는 방법은 무엇인가요?\n\n## 디자인이 문제를 해결하는 방법은 무엇인가요?\n\n디자이너들은 자주 자신들을 문제 해결자라고 부르지만, 많은 디자이너들이 다루지 못하는 핵심 질문이 하나 있습니다:\n\n- 사용자의 문제를 해결했는지 어떻게 알 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\n문제는 많은 디자이너들이 자신의 디자인을 제출한 후에 무엇이 발생하는지 설명할 수 없다는 것이었습니다. 엔지니어들은 이를 작업하기 시작하고, 디자이너들은 할당받은 다음 작업으로 이동합니다.\n\n이것은 기능을 구축하는 데 좋은 방법이지만, 이후에 무엇이 발생하는지 알지 못하면 사용자의 문제를 해결했는지 여부를 알 수 없습니다.\n\n다행히도 성공했는지 여부를 빠르게 판단하는 방법은 데이터를 확인하는 것입니다. 결국, 분석과 같은 데이터는 대규모로 \"사용자가 무엇을 하는지\"의 컴필레이션이에요.\n\n경영진들은 회사의 건강과 성공을 모니터링하기 위해 데이터를 활용하며, 어떠한 기본 지식을 통해 당신의 디자인에 대한 영향을 확인할 수 있습니다. 이것이 공개된 후 디자인의 영향력을 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n그리고, 그걸 위해 MBA나 데이터 분석가 자격증이 필요하지 않습니다.\n\n## UX가 가져다주는 가치를 어떻게 정의하나요?\n\n이것은 누구나 자주 언급하는 \"투자대비이익(ROI)\"에 관한 영원한 질문 중 하나입니다.\n\n사람들은 종종 더 나은 사용자 경험이 더 많은 돈을 절약(또는 벌어들이는 것)하는 방식을 보여주는 특정 사례 연구에 대해 학습할 것을 주장합니다. 그러나 디자인을 옹호할 때 과거 일부 기업에 어떻게 도움이 되었는지 이야기함으로써 디자인을 옹호하는 것에는 한계가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n오히려 비즈니스에 유엑스(UX)가 무엇을 할 수 있는지에 집중해야 합니다.\n\n데이터 기반 디자인의 경우 비즈니스 메트릭이나 KPI를 배울 필요는 없습니다(도움이 된다면); 제품 메트릭을 배우는 것이 중요합니다.\n\n예를 들어, 비즈니스 목표가 Q3에 10,000명의 유료 고객을 확보하는 것이라면, 유엑스가 직접적인 영향을 미친 방법을 보여주기는 어려울 수 있습니다. 유엑스를 개선함으로써 고객을 유료로 전환한 것일 수도 있지만, 이는 마케팅이나 판매와도 연관될 수 있습니다.\n\n하지만, 제품 메트릭은 유엑스에 더 쉽게 영향을 받고 유엑스와 관련이 있습니다. 예를 들어, 온보딩을 완료하는 데 소요된 평균 시간인 제품 메트릭이 6분에서 2분까지 줄었다는 것을 보여준다면, 우리는 유엑스가 제품 팀뿐만 아니라 비즈니스에도 가치를 제공하는 것을 입증할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n인당 링크를 형성하는 방식이 사람들로 하여금 행동하도록 설득하는 방법입니다.\n\n## 다른 사람들에게 디자인 권장 사항을 설득하는 방법은 무엇인가요?\n\n가끔은 가치 있는 사용자 조사 결과를 보유하고 있지만 이를 정당화하기 어려울 수도 있습니다.\n\n예를 들어 사용자 조사를 실시하면 탐색에서 특정 중요한 기능을 찾는 데 도움이 필요한 사용자 중 5명 중 2명이 어려움을 겪는 것을 발견할 수 있습니다. 그러나 이것만으로는 전체 이야기를 이야기하지는 않습니다: 그들은 결국 포기하기 전에 20분 동안 헤매었는데, 이는 더 큰 문제를 시사합니다.\n\n<div class=\"content-ad\"></div>\n\n문제는 5명 중 2명이란 소리가 그리 많이 들리지 않다는 것이에요. 만약 5명 중 5명이었다면, 이것이 빈번하고 일반적인 문제임을 주장할 수도 있을 텐데요.\n\n하지만 5명 중 2명은 \"40%의 사용자가 네비게이션에 문제를 겪고 있다\"고 주장하기에는 표본 크기가 너무 작아요. 단독으로는 다른 정보 없이 이것만으로는 이것이 높은 우선 순위 문제 일 수도 있음을 나타낼 수도 있어요.\n\n하지만 만약 이 결과를 지지하는 추가 데이터가 있다면 (그리고 팀을 설득할 수 있다면)?\n\n아래는 그것을 하는 방법이에요.\n\n<div class=\"content-ad\"></div>\n\n## 높은 수준에서 데이터 기반 설계 작업 사용하기\n\n높은 수준에서 데이터 기반 설계 프로세스는 가설 설정, 간접적 사고, 쿼리 생성 세 가지 핵심 개념을 중심으로 합니다.\n\n가설 초안으로 시작하기\n\n높은 수준에서 사용할 수 있는 두 개의 가설이 있습니다. 현재 또는 이전 작업에 대해 이야기할 수 있습니다. 예를 들어 현재 프로젝트를 진행하고 사용자 테스트 결과에 대해 이야기할 때는 이 가설을 사용합니다. 이 가설은 책 '데이터로 디자인하기'에서 나온 것입니다.\n\n<div class=\"content-ad\"></div>\n\n\"만약 우리가 [X를] 한다면, 우리 사용자들은 [Z 이유]로 [Y를] 할 것이며, 이는 [지표 A]에 영향을 미칠 것입니다.\"\n\n반면에, 디자인 포트폴리오에서 일을 요약할 때, 구글 채용 담당자가 추천하는 조금 다른 가설을 사용할 것입니다:\n\n\"나는 [Z]를 통해 [Y로 측정된]를 달성했으며, 이를 위해 [X를] 했습니다.\"\n\n두 가설 모두 당신의 가치와 큰 목표에 대한 기여를 명확히 해줍니다. 하지만 처음 시작할 때는 모든 답을 가지고 있지 않을 수도 있습니다. 그러나 당신이 아는 것을 모아서 정리하는 것이 첫걸음입니다.\n\n<div class=\"content-ad\"></div>\n\n여기 예시처럼 보일 것입니다:\n\n- 사용자 테스트 권고 사항: 만약 우리가 [정보 아키텍처를 다시 작업하는 데 시간을 투자]하면, 사용자들은 [Y를 실행할 것]인데, 그 이유는 [내비게이션이 덜 혼란스러워 지기 때문]이며, 이는 [지표 A에 영향을 미칠 것]입니다\n- 디자인 포트폴리오 요약: 저는 [X를 성취했습니다], 이는 [Y에 의해 측정된 것]으로, [전체적인 정보 아키텍처를 다시 작업함으로써] 달성하였습니다.\n\n위의 공백을 채우는 방법에 대해 잘 모르겠다면, 다음 단계입니다.\n\n지식상의 빈을 파악하기 위해 간접적으로 데이터를 고려하세요.\n\n<div class=\"content-ad\"></div>\n\n가설을 작성한 후에는 어떤 것을 알지 못하고 어디에서 그 데이터를 찾을 수 있는지 고려해보세요. 이것이 바로 \"데이터에 대해 간접적으로 생각하기\"라고 알려진 것이며, 이 주제에 관한 가장 중요한 교훈 중 하나입니다.\n\n예를 들어, 사용자 테스트 예시에서, 우리는 디자인 권장 사항(정보 구조 재구성)과 그 이유(네비게이션이 덜 혼란스러워짐)를 가지고 있습니다. 이는 사용자들과 대화를 통해 배웠습니다.\n\n파악해야 할 몇 가지 사항은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- 사용자들은 현재 무엇을 하고 있는지, 이것이 문제가 되고 있는지 확인해야 합니다.\n- 이 변경으로 인해 사용자들이 무슨 행동을 할 가능성이 있으며, 그 이유는 무엇인지 확인해야 합니다.\n- 결과적으로 어떤 메트릭이 영향을 받을 것으로 예상되는지 확인해야 합니다.\n\n첫 번째 질문에 대답하기 위해 \"사용자의 활동\"에 대한 대규모 저장소를 분석해야 합니다. 사용자 테스트에서 사용자들이 어려움을 겪는 것을 보았지만, 웹사이트에서 유사한 어려움의 징후가 있는지 확인해야 합니다.\n\n또한, 우리의 디자인 변경이 어떤 변화를 초래할 것으로 예상되는지 알기 위해 사용자의 행동에 대해 충분히 알아야 합니다. 보통 사용자 테스트, 모범 사례 및 추가 연구를 통해 사용자 행동에 대해 충분히 알 수 있습니다.\n\n마지막으로, 이에 영향을 받을 메트릭을 결정해야 합니다. 하지만 제품 메트릭에 심층적으로 파고들기 전에 쉬운 방법이 있습니다: 제품팀에게 물어보세요. 사람들은 아마도 프로젝트의 중요한 메트릭을 결정하는 데 몇 주에서 몇 달을 소비해왔을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n그들에게 간단히 물어보면 필요한 대답을 얻을 수 있어요.\n\n데이터 소스에 요청할 “쿼리” 를 형성해보세요.\n\n많은 데이터가 있고 어디를 뒤지면 좋을지 판단하기 어려울 수 있어요. 그러나 나의 경험 상으로 대부분의 답을 찾기 위해 5가지 기본 데이터 소스에 의지했어요:\n\n- 이전 사용자 연구/경쟁사 연구\n- 팀원들과 대화\n- (편향된) 사용자 리뷰/고객 지원 티켓\n- 기존 고객 조사/신규 (1–질문) 조사\n- 분석\n\n<div class=\"content-ad\"></div>\n\n우리 예제에서 몇 가지 질문(즉, \"질의 사항\")을 해결하고 싶습니다.\n\n- 우리 제품 페이지로 이동하는 사용자 수는 얼마나 되며, 사용자 테스트에서 본 문제가 데이터에 반영되었는지 확인하고 싶습니다.\n- 어떤 지표들이 영향을 받고 있으며, 우리 팀에게 얼마나 중요한가요?\n\n첫 번째 질문에 대한 가장 쉬운 접근 방법은 분석입니다: 다양한 웹사이트 페이지로의 웹 트래픽을 확인하고 우리가 관심 있는 페이지로 이동하는 동안 큰 하락이 있는지 확인할 수 있습니다.\n\n두 번째 질문에 대해서는, 우리는 팀과 이 문제가 그들이 중요하게 여기는 지표에 영향을 미치는지 결정하기 위해 대화할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 쿼리를 해결한 후에는 다음과 같은 내용이 될 수 있습니다:\n\n- 사용자 테스트 권장 사항: 만일 [정보 아키텍처 재작업에 시간을 할애]한다면, 우리 사용자들은 [새 제품을 찾을 수 있게 될 것]입니다. 왜냐하면 [우리의 내비게이션이 덜 혼란스럽기 때문]에요. 이것은 [제품 채택률]에 영향을 줄 것입니다.\n- 디자인 포트폴리오 요약: 저는 [제품 채택률로 측정되는 사용자 채택율을 10% 증가시키는 데 성공했습니다], 이를 위해 [전체적인 정보 아키텍처를 재작업함]으로써.\n\n데이터와 디자인을 결합하여, 우리는 디자인 권장사항을 더 이해하기 쉽고 팀이 구현하기 편하도록 변환했습니다.\n\n이러한 기술 덕분에, 디자이너로서 어렵던 시기에도 번성할 수 있었습니다.\n\n<div class=\"content-ad\"></div>\n\n## 디자인의 미래는 데이터 기반 디자인일 수 있습니다\n\n지금은 디자인 분야에 대한 많은 불확실성이 있습니다. 나쁜 취업 시장 외에도 AI 및 데이터와 같은 요소들이 장르에 어떤 영향을 미칠지에 대한 많은 질문들이 있습니다.\n\n이것이 바로 디자이너들이 앞으로 필요로 할 중요한 스킬인 데이터 기반 디자인입니다. 훌륭한 사용자 경험을 설계할 뿐만 아니라 무엇이 일어나고 있는지 측정하고 평가할 수 있을 때, 많은 기업들이 디자인 권고의 영향에 대해 가질 수 있는 불확실성에 대해 말씀할 수 있습니다.\n\n데이터에 대해 충분한 이해를 얻는 것은 비즈니스의 관점과 그들이 신경 쓰는 것에 대해 이해할 수 있게 해줍니다. 사용자 조사에서 찾은 결과를 그들이 쉽게 이해하고 행동할 수 있는 것으로 번역하는데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n그래서 디자인 커리어를 미래를 대비하고 싶다면 데이터 기반 디자인을 배우는 것을 고려해보세요.\n\n나의 데이터 기반 디자인에 관한 메이븐 강의가 이제 오픈되었어요! 무료로 대기 목록에 가입하고 무료 워크샵에 대한 액세스를 받아보세요.\n\n카이 웡은 시니어 프로덕트 디자이너이자 데이터와 디자인 뉴스레터를 만드는 창조자입니다. 그의 책인 \"데이터 기반 UX 디자인\"은 데이터와 디자인의 힘을 활용하기 위해 디자인 프로세스에서 적용할 수 있는 21가지 작은 변화를 제공해줘요.","ogImage":{"url":"/assets/img/2024-06-19-HowData-InformedDesigncanhelpyounavigatetheseturbulenttimes_0.png"},"coverImage":"/assets/img/2024-06-19-HowData-InformedDesigncanhelpyounavigatetheseturbulenttimes_0.png","tag":["Tech"],"readingTime":7}],"page":"75","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}