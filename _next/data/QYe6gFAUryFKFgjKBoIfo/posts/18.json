{"pageProps":{"posts":[{"title":"SQL로 MLflow 모델 구축하기 머신러닝 라이프사이클 관리 쉽게 하는 방법","description":"","date":"2024-06-23 16:37","slug":"2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement","content":"\n\n## MLflow 생태계에 SQL 모델을 통합하는 단계별 안내서\n\n![이미지](/assets/img/2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement_0.png)\n\n## 소개\n\n기계 학습의 끊임없이 진화하는 풍경에서, 모델의 끝-끝 수명주기를 원할하게 관리해야 하는 필요성이 중요해졌습니다. 이러한 복잡한 프로세스를 단순화하는 오픈 소스 플랫폼인 MLflow가 나타납니다. 이 포괄적인 안내서에서는 SQL 기반 모델과 MLflow의 기능을 융합하는 과정을 탐색할 것입니다. 우리의 주요 목표는 두 가지입니다: 첫째, 간단한 SQL 기반 모델을 사용하여 MLflow의 기본 원리를 실습적으로 이해하는 것이며, 둘째로, MLflow의 모델 저장소 내에 SQL 쿼리를 캡슐화하는 흥미로운 도전에 대응하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## SQL 기반 모델의 중요성\n\nSQL(구조화된 쿼리 언어) 기반 모델은 현실 세계 비즈니스 시나리오에서 독특하게 중요하며 순위 매기기, 추천 시스템 및 데이터 필터링 작업에서 중추적 역할을 합니다. SQL 기반 모델이 중요한 역할을 하는 몇 가지 추가 도메인을 살펴보겠습니다:\n\n- 재고 관리: SQL 데이터베이스는 재고 수준, 재주문 점 및 공급망 데이터를 추적합니다. SQL 쿼리는 재고 수준을 모니터링하고 재고 보충 알림을 생성하며 재고 순환을 최적화하는 데 도움을 줍니다.\n- 고객 세분화 및 타겟 마케팅: 마케터들은 SQL 기반 모델을 활용하여 고객 베이스를 세분화하고 타겟 마케팅 캠페인을 설계합니다. 이러한 모델은 고객 인구 통계, 구매 이력 및 온라인 행동을 분석하여 특정 선호도를 갖는 고객 세그먼트를 식별함으로써 맞춤형 마케팅 전략을 구현할 수 있습니다.\n- 공급망 최적화: 복잡한 공급망을 관리하는 기업들은 SQL 모델을 사용하여 재고 수준을 최적화하고 물류를 최적화하며 비용을 최소화합니다. SQL 쿼리는 공급업체 성과, 수요 예측 및 생산 일정을 분석하여 적시에 납품하고 효율적인 운영을 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n## MLflow이란 무엇인가요?\n\nMLflow는 Databricks에서 개발한 오픈 소스 플랫폼으로, 기계 학습 수명주기의 end-to-end 관리를 용이하게 합니다. 실험 추적, 코드를 재현 가능한 실행으로 패키징하고 모델을 공유하고 배포하는 도구를 제공하며, 모든 것을 통합된 프레임워크 내에서 처리합니다. 데이터 과학자, 기계 학습 엔지니어, 또는 비즈니스 분석가이든, MLflow는 기계 학습 프로젝트를 구성하고 협력하는 구조화된 방식을 제공합니다.\n\nMLflow의 주요 구성 요소:\n\n- 추적: MLflow의 추적 구성 요소를 사용하면 중요한 메트릭, 매개 변수 및 다른 실행에 연결된 아티팩트를 기록하고 모니터링할 수 있습니다.\n- 프로젝트: MLflow의 프로젝트 기능을 사용하면 코드, 종속성 및 환경 사양을 재사용 가능한 형식으로 패키징할 수 있습니다. 이를 통해 실험을 재현 가능하게 하여, 개발에서 제품화로의 전환 시에 일관성이 깨지지 않도록 할 수 있습니다.\n- 모델: 훈련된 기계 학습 모델의 모음입니다. 서로 다른 모델을 나타내는 서로 다른 책을 개인 서재로 상상해보세요. MLflow는 여러분이 이러한 모델을 패키징, 구성 및 관리할 수 있게 해주어 나중에 쉽게 찾아서 사용할 수 있습니다.\n- 모델 레지스트리: 모델 레지스트리를 정리된 서재로 생각해보세요. 여러분이 훈련된 기계 학습 모델의 서로 다른 버전을 저장하고 관리할 수 있는 곳입니다. 여러분이 좋아하는 책의 다른 버전이 있는 책장이 있는 것처럼 생각해보세요. MLflow의 모델 레지스트리는 모델이 시간이 지남에 따라 진화하는 것을 추적하여 필요할 때 쉽게 찾아서 사용할 수 있도록 해줍니다.\n\n<div class=\"content-ad\"></div>\n\nMLflow은 scikit-learn 및 TensorFlow와 같은 다양한 모델 라이브러리를 조화롭게 통합하여 배포를 최적화하고 모델 알고리즘 변경에 대한 우려를 덜어주는 통합 플랫폼으로 빛난다. MLflow를 사용하면 기업은 다양한 알고리즘 실험을 통해 일정한 배포 프로세스를 준수하는 동시에 유연성을 얻을 수 있습니다. 이 플랫폼은 라이브러리 간의 차이를 추상화하여 개발에서 배포로의 수동 변환 없이 원활한 전환을 가능하게 합니다. 이 유연성은 알고리즘 잠금을 회피하고 새로운 혁신에 적응하며, 배포 파이프라인을 중앙 집중화하여 유지 관리 부담을 줄입니다.\n\n![이미지](/assets/img/2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement_1.png)\n\n## Mflow에 SQL 기반 모델 통합하는 이유\n\nMLflow는 Python 모델 및 TensorFlow 아티팩트를 관리하는 데 뛰어나지만, 모델 컨텍스트 내에서 SQL 쿼리를 수용하는 것은 원래 지원되지 않습니다. 이 분리는 종종 기관이 SQL 기반 및 Python 모델을 배포하기 위해 별도의 시스템을 유지하도록 강요하여 유지 보수 노력과 추적 불일치가 증가하게 합니다.\n\n<div class=\"content-ad\"></div>\n\n해당 도전을 극복하기 위해, 우리는 파이썬 함수 내에서 SQL 쿼리의 논리를 캡슐화하는 천재적인 전략을 곧 발표할 예정이에요. 이를 통해, 우리는 SQL 기반 모델을 MLflow의 생태계에 원활하게 통합하여 효율적인 저장, 버전 관리, 배포를 가능하게 합니다.\n\n이 통합은 전통적인 기계 학습 알고리즘을 넘어 플랫폼의 능력을 확장하여, 데이터 과학 작업에서 SQL 쿼리의 파워를 활용하려는 기관들에게 가치 있는 자산이 됩니다. 이는 모델 라이프사이클 관리에 대한 통합된 접근 방식을 제공하며, 동일한 프레임워크 내에서 SQL 및 Python 기반 모델을 효율적으로 결합하여, 데이터 분석 및 모델 개발에 SQL 전문 지식을 필요로 하는 기관에 대응합니다. 이는 MLflow의 유틸리티를 확장하여 더 넓은 범위의 모델링 작업을 처리하고, 모델 개발 라이프사이클 전반에 걸쳐 개선된 효율성과 협업을 촉진합니다.\n\n## 단계별 구현\n\n문제 진단: 에어비앤비를 고려해봅시다. 웹 사이트에서 사용자 경험을 향상시키기 위해, 에어비앤비는 숙박 정보의 랭킹 모델을 최적화하고자 합니다. 고급 기계 학습 알고리즘으로 진입하기 전에, 그들은 SQL 기반 모델을 벤치마크로 구축하고자 합니다. 주요 목표는 사용자가 특정 지역에 대한 요청을 제출할 때, 웹 사이트의 리스트를 평균 리뷰 점수를 기반으로 순위를 매기는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 차례로 수행할 단계입니다:\n\n1. 모의 데이터 생성: 먼저 Airbnb 리스트 데이터를 모방하는 모의 데이터 세트를 생성할 것입니다. 이 데이터 세트에는 리스트 ID, 지역, 리뷰 점수, 생성 날짜 및 방의 수가 포함됩니다. 이 데이터 생성을 통해 모델에 대한 대표적인 데이터 세트로 작업할 수 있게 됩니다.\n\n방금 생성한 데이터 세트를 간략히 살펴보겠습니다:\n\n![데이터세트](/assets/img/2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement_2.png)\n\n<div class=\"content-ad\"></div>\n\n2. 랭킹 함수 정의하기: 지역별로 목록을 랭킹하는 함수를 정의할 거에요. 이 함수는 지역(지역)을 입력으로 받아와서 SQL 쿼리를 사용하여 리뷰 점수를 기반으로 내림차순으로 필터링하고 정렬할 거에요.\n\n함수에 대해 빠르게 테스트를 해 보겠습니다:\n\n3. MLflow를 위한 함수 랩핑: 이 랭킹 함수를 MLflow에 통합하기 위해서는 MLflow의 규칙을 따라 랩핑할 필요가 있어요. mlflow.pyfunc.PythonModel을 상속하는 RankingModel이라는 파이썬 클래스를 만들 거에요. 이 클래스에는 스파크 세션을 초기화하는 predict 메서드가 포함되며 입력에서 지역을 추출하고 랭킹 함수를 호출할 거에요.\n\n4. 모델 테스트: RankingModel이 예상대로 작동하는지 확인하기 위해 빠른 테스트를 실행할 거에요. 이 테스트에서 모델의 인스턴스를 만들고 모델 입력(지역)을 정의하고 predict 메서드를 호출하여 순위가 매겨진 목록을 얻을 거에요.\n\n<div class=\"content-ad\"></div>\n\n4. 모델 등록: 성공적인 테스트 후, 앞으로 사용할 모델을 MLflow에 등록합니다. 이 단계에는 MLflow 런을 정의, 테스트 데이터로 predict() 메서드를 호출, predict 함수의 시그니처 추론, 모델 아티팩트를 MLflow에 로깅, 그리고 모델 레지스트리에 모델 등록이 포함됩니다.\n\n등록된 모델을 살펴보겠습니다.\n\n![등록된 모델](/assets/img/2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement_3.png)\n\n이제 등록 후에 다른 노트북이나 콘텍스트에서 이 모델을 로드하고 해당 지역을 입력으로 예측 메서드를 호출하면 그 지역에 대한 리뷰 점수별로 순위가 매겨진 목록 ID가 반환됩니다.\n\n<div class=\"content-ad\"></div>\n\n5. 등록된 모델 로드 및 사용하기: 마지막으로 등록된 모델을 다른 컨텍스트나 노트북에서 로드하고 예측에 사용하는 방법을 보여드리겠습니다. 이를 통해 모델을 다양한 애플리케이션에 원활하게 통합할 수 있습니다.\n\n이러한 단계를 따르면 SQL 기반 모델을 성공적으로 구축했고, MLflow와 통합했으며, 다양한 데이터 기반 하위 응용 프로그램에서 사용할 수 있게 될 것입니다.\n\n## 결론\n\n마지막으로, 우리는 SQL 모델을 성공적으로 작성하고 MLflow에 통합했습니다. 이 모델은 이제 등록되어 다른 서비스의 엔드포인트로 제공되어 리뷰 점수 기반 순위 목록에 대한 확장 가능하고 효율적인 솔루션으로 사용할 준비가 되었습니다. 이 접근 방식은 MLflow의 다양한 유형의 모델을 통합하여 통일된 프레임워크 내에서 다루는 다양성을 보여주며, Python 기반 모델에서 SQL 기반 모델로의 능력을 확장합니다. 전체 노트북은 여기에서 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 기사가 유익하고 재미있었으면 좋겠습니다. 좋아요로 감사를 표현해 주시고, 댓글로 피드백을 공유해 주세요.\n\n# 추가 정보\n\n제가 작성한 PySpark 튜토리얼 컬렉션을 소개합니다. 이 튜토리얼은 PySpark의 다양한 측면을 마스터하는 데 도움이 되도록 설계되었습니다. 제가 다음 기사에서 우선적으로 다루기를 원하는 구체적인 주제나 기술이 있으면 자유롭게 제안해 주세요. 여러분의 피드백은 귀중합니다. 만약 이 PySpark 튜토리얼이 유익하고 도움이 되었다면, 더 깊은 내용의 컨텐츠를 제공하는 Medium에서 저를 팔로우하시기를 권장합니다. PySpark의 세계로의 여정을 즐기세요! 즐거운 학습과 코딩되세요!","ogImage":{"url":"/assets/img/2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement_0.png"},"coverImage":"/assets/img/2024-06-23-BridgingtheGapConstructingSQL-BasedModelsinMLflowforStreamlinedMLLifecycleManagement_0.png","tag":["Tech"],"readingTime":6},{"title":"IPL 통계 분석을 위한 고급 SQL 쿼리 완벽 가이드","description":"","date":"2024-06-23 16:34","slug":"2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics","content":"\n<img src=\"/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_0.png\" />\n\n# 소개:\n\n상상해보세요. BCCI(인도크리켓통제위원회)에서 IPL 2150의 데이터 분석가로 고용되었다고 가정해 봅시다. 네, 2150 년에도 데이터 분석가는 여전히 높은 수요가 있고, AI가 아직 모든 일자리를 대체하지는 않았습니다. 누가 생각했겠습니까? 아마도 AI는 여전히 크리켓의 규칙을 이해하려고 노력 중일지도 모릅니다! 그런데, 이 프로젝트에서는 2150 년 자료가 제공되지 않습니다. 그래서 신경 쓰지 마세요.\n\n당신의 팀 매니저가 IPL 시즌 전체 기록을 포함하는 여러 CSV 파일을 손에 쥐고 여러분에게 접근합니다. 그들은 여러분에게 포괄적인 분석을 수행하고 이 데이터를 Postgres(RDBMS)로 이전하여 팀 내에서 더 효율적인 데이터 관리를 요청합니다.\n모든 데이터 집합과 마찬가지로, 도메인 지식은 데이터 분석가가 효과적으로 데이터 분석을 수행하는 데 중요합니다. IPL 크리켓에 익숙하지 않다면, 분석을 진행하기 전에 데이터 집합의 열을 먼저 살펴봄으로써 도메인 지식을 얻는 것이 좋습니다.\n\n<div class=\"content-ad\"></div>\n\n# 데이터 세트:\n\nCSV 파일에는 아래 그림에 표시된 6개의 테이블이 포함되어 있으며, 이를 pgAdmin (postgreSQL의 RDMS)에서 다음 데이터베이스 스키마처럼 변환해야 합니다.\n실제 시나리오에서 기업은 일반적으로 CSV에서 SQL 데이터베이스로의 전환보다 DBMS(데이터베이스 관리 시스템)를 직접 사용합니다. 그러나 우리의 SQL 프로젝트 목적으로 이 전환이 수행되었습니다.\n\n<img src=\"/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_1.png\" />\n\n제약 조건:\n\n<div class=\"content-ad\"></div>\n\n- 주어진 스키마에 포함된 기본 키 (Primary Key)와 외래 키 (Foreign Key) 제약 조건\n- out_type은 'caught', 'caught and bowled', 'bowled', 'stumped', 'retired hurt', 'keeper catch', 'lbw', 'run out', 'hit wicket', 또는 NULL(실제 null이 아닌 문자열 형태) 값만 가질 수 있습니다.\n- role_desc는 'Player', 'Keeper', 'CaptainKeeper' 또는 'Captain' 값만 가질 수 있습니다.\n- toss_name은 'field' 또는 'bat' 값만 가질 수 있습니다.\n- win_type은 'wickets', 'runs', 또는 NULL(실제 null이 아닌 문자열 형태) 값만 가질 수 있습니다.\n- ball_by_ball 테이블의 runs_scored 값은 0에서 6 사이여야 합니다.\n- ball_by_ball 테이블의 innings_no 값은 1 또는 2만 가능합니다.\n\n# 분석적 질문 및 해결책:\n\n자세한 분석을 위해 팀 매니저가 다음 작업을 할당하고 명확한 경로를 제시했습니다.\n\nQ 1: 주어진 데이터베이스 스키마에 따라 CSV 파일을 생성하고, 모든 제약 조건과 테이블 간 관계가 올바르게 반영되도록 하고, 그 후 pgAdmin에 가져오세요. (다른 RDBMS를 사용하는 경우, 모든 쿼리에 대해 구문을 조정하십시오)\n\n<div class=\"content-ad\"></div>\n\n해결책: 지정된 데이터베이스 스키마와 일치하도록 필요한 테이블을 적절한 제약 조건과 관계와 함께 생성하고 해당 CSV 파일에서 데이터를 가져옵니다.\n\n```js\n-- 해결책 1:\n-- 테이블을 생성할 때 위의 데이터베이스 스키마에 따라\n-- 필요한 제약 조건 및 관계 키를 추가해주세요\n\n-- venue 테이블 생성\ncreate table if not exists venue(\n venue_id int,\n venue_name varchar(50) not null,\n city_name varchar(50) not null,\n country_name varchar(50) not null,\n constraint pk_venue_venue_id primary key (venue_id)\n);\n-- venue.csv 파일에서 값 가져오기\ncopy venue\nfrom 'D:\\Downloads\\A Portfolio Projects\\SQL Projects\\IPL Analysis\\Dataset CSV\\venue.csv'\ndelimiter ','\ncsv header;\n\n-- team 테이블 생성\ncreate table if not exists team(\n team_id int,\n team_name varchar(50) not null,\n constraint pk_team_team_id primary key(team_id)\n);\n-- team.csv 파일에서 값 가져오기\ncopy team\nfrom 'D:\\Downloads\\A Portfolio Projects\\SQL Projects\\IPL Analysis\\Dataset CSV\\team.csv'\ndelimiter ','\ncsv header;\n\n-- player 테이블 생성\ncreate table if not exists player(\n player_id int,\n player_name varchar(50) not null,\n dob date not null,\n batting_hand varchar(50) not null,\n bowling_skill varchar(50) not null,\n country_name varchar(50) not null,\n constraint pk_player_player_id primary key(player_id)\n);\n-- player.csv 파일에서 값 가져오기\ncopy player\nfrom 'D:\\Downloads\\A Portfolio Projects\\SQL Projects\\IPL Analysis\\Dataset CSV\\player.csv'\ndelimiter ','\ncsv header;\n\n-- match 테이블 생성\ncreate table if not exists match(\n match_id int primary key,\n season_year int not null,\n team1 int not null references team(team_id),\n team2 int not null references team(team_id),\n venue_id int not null references venue(venue_id),\n toss_winner int not null references team(team_id),\n match_winner int not null references team(team_id),\n toss_name varchar(50) not null check(toss_name in ('field', 'bat')),\n win_type varchar(50) not null check(win_type in ('wickets', 'runs', 'NULL')),\n man_of_match int not null references player(player_id),\n win_margin int not null\n)\n-- match.csv 파일에서 값 가져오기\ncopy match\nfrom 'D:\\Downloads\\A Portfolio Projects\\SQL Projects\\IPL Analysis\\Dataset CSV\\match.csv'\ndelimiter ','\ncsv header;\n\n-- player_match 테이블 생성\ncreate table if not exists player_match(\n playermatch_key bigint primary key,\n match_id int not null references match(match_id),\n player_id int not null references player(player_id),\n role_desc varchar(50) not null check(role_desc in ('Player', 'Keeper', 'CaptainKeeper', 'Captain')),\n team_id int not null references team(team_id)\n);\n-- player_match.csv 파일에서 값 가져오기\ncopy player_match\nfrom 'D:\\Downloads\\A Portfolio Projects\\SQL Projects\\IPL Analysis\\Dataset CSV\\player_match.csv'\ndelimiter ','\ncsv header;\n\n-- ball_by_ball 테이블 생성\ncreate table if not exists ball_by_ball(\n match_id int not null references match(match_id),\n innings_no int not null check(innings_no<3 and innings_no>0),\n over_id int not null,\n ball_id int not null,\n runs_scored int not null check(runs_scored<=6 and runs_scored>=0),\n extra_runs int not null,\n out_type varchar(50) not null check(out_type in ('caught', 'caught and bowled', 'bowled', 'stumped', 'retired hurt', 'keeper catch', 'lbw', 'run out', 'hit wicket', 'NULL')),\n striker int not null references player(player_id),\n non_striker int not null references player(player_id),\n bowler int not null references player(player_id),\n constraint pk_ball_by_ball_id primary key(match_id, innings_no, over_id, ball_id)\n)\n-- ball_by_ball.csv 파일에서 값 가져오기\ncopy ball_by_ball\nfrom 'D:\\Downloads\\A Portfolio Projects\\SQL Projects\\IPL Analysis\\Dataset CSV\\ball_by_ball.csv'\ndelimiter ','\ncsv header;\n```\n\n질문 2: 생성한 테이블에서 각 경기장 마다 스코어된 평균 달성량을 찾으려면 스타디움에서 경기당 평균 달성량(두 팀의 총점)을 계산해야 합니다.\n총 점수를 계산하려면 ball_by_ball 테이블에서 runs_scored 및 extra_runs를 합산해야 합니다.\n\n해결책: 각 경기장에서 스코어된 평균 달성량을 찾으려면 다음 단계를 따라야 합니다. 먼저 각 경기장에서 플레이된 총 경기수를 계산하고, 그 다음 각 경기장에서 스코어된 총 점수를 결정합니다. 마지막으로 총 점수를 플레이된 경기수로 나누어 각 경기장의 경기 당 평균 달성량을 얻을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 2단계: 각 구장에서의 경기 수 계산\nwith\nno_of_match_per_venue as\n (\n  select v.venue_id, v.venue_name, count(match_id) as no_of_matches\n  from match m\n  join venue v\n  on v.venue_id=m.venue_id\n  group by v.venue_id, v.venue_name\n ),\n-- 2단계: 각 구장에서의 총 득점 계산\ntotal_run_per_venue as\n (\n  select v.venue_id, sum(b.runs_scored+b.extra_runs) as total_run\n  from ball_by_ball b\n  join match m\n  on m.match_id = b.match_id\n  join venue v\n  on v.venue_id = m.venue_id\n  group by v.venue_id\n )\n-- 마지막으로 위의 두 임시 테이블을 사용하여\n-- 각 구장에서의 경기 당 평균 득점을 계산합니다\nselect  npv.venue_name, tpv.total_run, npv.no_of_matches,\nround(tpv.total_run/npv.no_of_matches::numeric,3) as avg_run\nfrom no_of_match_per_venue npv\njoin total_run_per_venue tpv\non npv.venue_id = tpv.venue_id\norder by avg_run desc;\n```\n\n![2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_2.png](/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_2.png)\n\n질문 3: 평균으로 경기 당 가장 많은 볼을 친 선수들을 찾고 상위 10명으로 제한하십시오.\n스트라이커로서 해당 선수가 등록된 경우 선수가 공을 쳤다고 간주합니다.\n\n해결책: 먼저 각 선수가 참가한 총 경기 수를 계산해야 합니다. 그 다음 각 선수가 스트라이커로서 받은 총 볼 수를 확인해야 합니다. 마지막으로 경기 당 평균으로 가장 많은 볼을 친 상위 10명의 선수를 식별할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n-- 솔루션 3:\n-- 단계 1: player_match 테이블에서 플레이어가 참가한 경기 수를 세기\nwith num_of_match_by_player as\n(\nselect player_id, count(match_id) as no_of_match from player_match\ngroup by player_id\n),\n-- 단계 2: 공격수로서 플레이어가 참가한 총 볼 수 계산\ntotal_ball_played_by_player as\n(\nselect striker, count(ball_id) as total_ball_played from ball_by_ball\ngroup by striker\n)\n-- 최종적으로 플레이어 당 평균 한 경기에서 가장 많이 볼을 친 상위 10명을 계산\nselect player_id, player_name, avg_ball_played from\n(\nselect \\*,\n-- 동률이 있는 경우를 포함하기 위해 rank 함수 사용\nrank() over(order by avg_ball_played desc) from\n(\n-- 평균 계산\nselect p.player_id, p.player_name,\n(tp.total_ball_played/mp.no_of_match) as avg_ball_played\nfrom num_of_match_by_player mp, total_ball_played_by_player tp, player p\nwhere mp.player_id = tp.striker\nand\np.player_id = mp.player_id\n)\n)\nwhere rank<=10; -- 상위 10개 가져오기\n\n![](/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_3.png)\n\nQ 4: 가장 빈도가 높은 6타자를 찾아보세요.\n즉, 플레이어가 차지한 볼 중에서 가장 높은 비율로 6점을 친 플레이어를 찾으세요. 플레이어 ID, 플레이어 이름, 플레이어가 6점을 얻은 횟수, 차진 볼 수, 6의 비율을 출력하세요.\n\n솔루션: 먼저 각 플레이어가 차진 볼 수를 계산합니다. 그런 다음, 각 플레이어가 친 6점을 결정합니다. 마지막으로 각 플레이어의 6의 비율을 계산하세요.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- Solution 4:\n-- 각 세션에서 각 선수가 참가한 공의 수 계산\nwith ball_by_player as(\n select striker, count(ball_id) as ball_played from ball_by_ball\n group by striker\n),\n-- 각 선수가 기록한 6점 수 계산\nsix_by_player as(\n select striker, count(ball_id) as no_of_six from ball_by_ball\n where runs_scored = 6\n group by striker\n)\n-- 최종 비율 얻기\nselect p.player_id, p.player_name, bp.ball_played, sp.no_of_six,\nround((sp.no_of_six::numeric/bp.ball_played),2) as fraction\nfrom ball_by_player as bp, six_by_player as sp, player as p\nwhere bp.striker = sp.striker and bp.striker = p.player_id\norder by fraction desc;\n```\n\n<img src=\"/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_4.png\" />\n\nQ 5: 각 시즌에서 가장 많은 득점을 기록한 상위 3 타자 및 가장 많은 wickets를 따낸 상위 3 볼러 player_ids를 찾아보세요. Output (season_year, batsman, runs, bowler, wickets). 여기서 batsman 및 bowler는 선수들의 player_ids입니다. 동점인 경우 더 낮은 player_id를 먼저 출력합니다. season_year (날짜가 빠른 순)와 rank(특정 시즌에 더 많은 득점 및 wickets를 기록한 타자와 볼러)로 정렬합니다. (no_of_seasons\\*3)개의 행이 있을 것입니다.\n\nSolution: 먼저, 각 시즌에서 가장 많은 wickets를 기록한 상위 3 타자를 식별합니다. 다음으로, 각 시즌에서 가장 많은 wickets를 기록한 상위 3 볼러를 결정합니다. 마지막으로, 이러한 결과를 결합하여 최종 목록을 얻습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 솔루션 5:\n-- 먼저 각 시즌에서 각각 가장 많은 횟수의 릴리를 기록한 상위 3명의 타자를 찾습니다.\nwith top_batsman as\n (\n select *,\n rank() over(partition by season_year order by run desc, striker) from\n  (\n  select m.season_year, b.striker, p.player_name, sum(runs_scored) as run\n  from ball_by_ball as b, match as m, player as p\n  where b.match_id = m.match_id and p.player_id = b.striker\n  group by m.season_year, b.striker, p.player_name\n  )\n ),\n-- 그리고 두 번째로, 각 시즌에서 각각 가장 많은 볼을 기록한 상위 3명의 볼러를 찾는다.\ntop_bowlers as(\n select *,\n rank() over(partition by season_year order by wicket desc, bowler) from\n  (\n  select m.season_year, b.bowler, p.player_name, count(out_type) as wicket\n  from ball_by_ball as b, match as m, player as p\n  where b.match_id = m.match_id and p.player_id = b.bowler\n  and b.out_type not in ('run out', 'retired hurt')\n  group by m.season_year, b.bowler, p.player_name\n  )\n )\n-- 위 두 가지를 조인하여 최종 결과를 얻으세요\nselect tbt.season_year, tbt.striker, tbt.player_name, tbt.run, tbo.bowler, tbo.player_name, tbo.wicket\nfrom top_batsman as tbt, top_bowlers as tbo\nwhere tbt.rank=tbo.rank and tbt.rank<=3 and tbo.rank<=3 and tbt.season_year = tbo.season_year\norder by season_year;\n```\n\n<img src=\"/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_5.png\" />\n\n질문 6: 각 경기에서 최대 파트너십 득점을 달성한 선수의 ID를 찾기 위한 SQL 쿼리를 작성하세요. 결과에는 (match_id, player1, runs1, player2, runs2)가 포함되어야 하며, 파트너십 득점의 내림차순으로 정렬되어야 합니다. 동점의 경우 match_id가 오름차순으로 정렬되어야 합니다. runs1이 항상 runs2보다 큰지 확인하고, runs1과 runs2가 동일한 경우 player1_id가 player2_id보다 커야 합니다. extra_runs는 포함되어서는 안 됩니다. 서로 다른 선수가 동일한 파트너십 득점을 여러 번 달성하는 경우 각 경기의 여러 행이 존재할 수 있습니다.\n\n솔루션:\n단계 1- partnership이라는 공통 테이블 표현(CTE)을 사용하여 각 파트너십(경기 ID 및 연결된 선수 ID로 식별)이 가져온 총 득점(extr):\n단계 2- 다른 CTE인 striker_run_contributed는 각 파트너십에서 스트라이커가 기여한 총 득점을 계산합니다.\n단계 3- CTE final_table은 파트너십 득점을 스트라이커의 득점 기여와 결합하고, 비 스트라이커의 득점을 계산합니다. 각 경기의 최고 파트너십 득점만 포함하도록 필터링합니다.\n단계 4- 주 쿼리는 결과를 선택하고 정렬하여 더 높은 득점자가 항상 먼저 나오고 run1이 항상 run2보다 크도록 합니다. 두 선수가 동일한 득점인 경우 더 높은 ID를 가진 선수가 먼저 표시됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 질문 5:\n-- 각 경기에서 가장 많은 협력 득점을 얻은 플레이어들의 ID 찾기?\n-- 한 경기에 여러 개의 행이 있을 수 있습니다.\n-- 출력 (match_id, player1, runs1, player2, runs2),\n-- 협력 득점의 내림차순으로(동점인 경우 match_id는 오름차순으로 비교).\n-- 각 행에서 run1 > run2\n-- runs1=runs2인 경우 player1_id > player2_id. 참고: extra_runs는 계산하지 않아야 함\n-- 솔루션\n\nwith partnership as\n(\n select match_id, striker, non_striker, p_id, p_run from\n (\n  select *,\n  sum(runs_scored) over(partition by match_id, p_id order by match_id) as p_run,\n  row_number() over(partition by match_id, p_id order by match_id) as rank\n  from(\n   select b.match_id, b.runs_scored, b.striker, b.non_striker,\n   case when striker<non_striker then concat(non_striker,' ',striker)\n   else concat(striker, ' ', non_striker)\n   end as p_id\n   from ball_by_ball as b\n  )\n ) where rank=1\n order by p_run desc, match_id asc\n),\nstriker_run_contributed as\n (\n select b.match_id, b.striker, b.non_striker, sum(b.runs_scored) as striker_run\n from ball_by_ball as b\n group by b.match_id, b.striker, b.non_striker\n  ),\nfinal_table as\n(\n select p.match_id, p.striker, p.non_striker, sr.striker_run, (p.p_run-sr.striker_run) as non_striker_run,\n p.p_run\n from partnership as p, striker_run_contributed as sr\n where p.match_id = sr.match_id and p.striker = sr.striker and p.non_striker = sr.non_striker\n and p.p_run = (select max(p_run) from partnership as pt where pt.match_id = p.match_id)\n order by p.p_run desc, p.match_id asc\n  )\nselect match_id,\ncase when (striker_run = non_striker_run and striker>non_striker) then striker\n  when striker_run>non_striker_run then striker\n  else non_striker end as player_1,\ncase when (striker_run>non_striker_run) then striker_run\n  else non_striker_run end as run1,\ncase when (striker_run = non_striker_run and striker>non_striker) then non_striker\n  when striker_run>non_striker_run then non_striker\n  else striker end as player_2,\ncase when (striker_run>non_striker_run) then non_striker_run\n  else striker_run end as run2,\np_run as total_partnership\nfrom final_table;\n```\n\n<img src=\"/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_6.png\" />\n\n질문 7: 이닝 유형이 wickets인 모든 경기에서 득점이 6점 미만인 이닝 ID를 찾으세요.\n출력 (match_id, innings_no, over_id). 참고: 이닝에서 득점된 점수에는 extra_runs도 포함됨.\n\n솔루션: 먼저 ball_by_ball 테이블과 이긴 경기 정보를 포함하는 match 테이블을 조인한 후, 득점이 6점 미만인 경우에 해당하는 over_id를 가져옵니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 질문 7:\n-- 이닝 종료로 승리한 경기 중에서 6 미만의 점수를 기록한 이닝 ID를 찾아주세요.\n-- 출력 (match_id, innings_no, over_id). 참고: 이닝에서 기록된 점수에는 추가 점수도 포함됩니다.\n\n-- 해결 방법 7:\n\nselect b.match_id, b.innings_no, b.over_id\nfrom ball_by_ball as b\njoin match as m on m.match_id = b.match_id\nwhere win_type = 'wickets'\ngroup by b.match_id, b.innings_no, b.over_id\nhaving sum(b.runs_scored) + sum(extra_runs) < 6\n```\n\n![이미지](/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_7.png)\n\nQ 8: 2013 시즌에서 가장 많은 홈런을 친 상위 5명의 타자 나열하기?\n출력 (player_name).\n\n해결 방법: ball_by_ball 테이블을 match 테이블과 연결하여 시즌 연도를 얻고, player 테이블과 연결하여 선수명을 얻습니다. 2013년에 홈런을 세어 상위 5명을 제한하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 질문 8:\n-- 2013 시즌에서 가장 많은 홈런을 친 상위 5명의 타자를 나열하십시오.\n-- 알파벳순으로 동점이 발생했을 경우를 고려하십시오. 결과 (선수 이름).\n\n-- 해결책 8:\nselect p.player_name from ball_by_ball as b, match as m, player as p\nwhere (b.match_id = m.match_id and b.striker = p.player_id)\nand (m.season_year = 2013 and b.runs_scored = 6)\ngroup by b.striker, p.player_name order by count(runs_scored) desc limit 5\n```\n\n![2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_8.png](/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_8.png)\n\nQ 9: 2013 시즌에서 가장 낮은 스트라이크 비율(평균 당 탈아웃당한 볼의 수)으로 상위 5명의 볼러를 나열하십시오. 알파벳순으로 동점이 발생했을 경우를 고려하십시오. 결과 (선수 이름).\n\n해결책: 우선 2013년에 각 선수가 얼마나 많은 아웃을 기록했는지를 계산하십시오. 'NULL', 'retired hurt', 'run out'과 같은 out_type은 볼러로 카운트되지 않습니다. 그래서 데이터 분석가는 데이터 세트에 대한 도메인 지식을 어느 정도 알고 있는 것이 중요합니다. 그런 다음 각 볼러가 한 공을 던진 횟수를 계산하십시오. 마지막으로 평균 비율을 구하고, 비율이 높을수록 볼러로서의 스트라이크 비율이 낮습니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 질문 9: 2013 시즌에서 볼링 스트라이크율(얻은 퍼스트볼당 볼이 던져진 평균 수)이 가장 낮은 5명의 볼러를 나열하십시오. 알파벳순으로 동률 발생 시 이름순으로 정렬하십시오. 결과값은 (선수 이름)으로 출력합니다.\n\n-- 해결책 9:\nwith wicket as\n(\n select b.bowler, p.player_name, count(out_type) as no_of_wicket\n from ball_by_ball as b, player as p, match as m\n where (b.bowler = p.player_id and b.match_id = m.match_id)\n and (b.out_type not in ('NULL', 'retired hurt', 'run out')\n and m.season_year = 2013)\n group by b.bowler, p.player_name\n),\nballs as\n(\n select b.bowler, p.player_name, count(ball_id) as no_of_ball\n from ball_by_ball as b, player as p, match as m\n where (b.bowler = p.player_id and b.match_id = m.match_id)\n and m.season_year = 2013\n group by b.bowler, p.player_name\n\n)\nselect b.player_name, b.no_of_ball/w.no_of_wicket as ratio from wicket as w, balls as b\nwhere w.bowler = b.bowler\norder by ratio desc , b.player_name limit 5;\n```\n\n![이미지](/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_9.png)\n\nQ 10: 각 나라(적어도 한 명의 선수가 아웃 처리됨)별로 어떤 경기에서 볼 아웃된 선수의 수를 찾아내십시오? 결과값은 (나라 이름, 수)으로 출력합니다. 여기서 나라는 선수의 속한 국적입니다.\n\n해결책: 볼링 백볼 테이블을 선수 테이블과 조인하여 국가 이름을 얻고, out_type = \"볼드\"로 필터링합니다. 적어도 한 명의 선수가 있는 각 나라별로 볼드 아웃된 선수의 수를 그룹화하여 계산합니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 질문 10:\n-- 적어도 한 명의 선수가 볼을 던진 나라마다\n-- 임의의 경기에서 볼 처리를 받은 플레이어의 수를 찾으세요.\n-- 출력 (country_name, count). 여기서 나라는 선수의 국적입니다.\n\n-- 해결 방법:\nselect p.country_name, count(striker) no_of_bowled_out\nfrom ball_by_ball as b, player as p\nwhere p.player_id = b.striker and b.out_type = 'bowled'\ngroup by p.country_name having count(striker) > 0 order by no_of_bowled_out desc\n```\n\n![이미지](/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_10.png)\n\nQ 11: ‘푸네’에서 진행된 임의의 경기에서 적어도 백을 득점한 오른손 타자의 이름을 나열해주세요? 출력 (player_name, run).\n\n해결 방법:\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 질문 11:\n-- 'Pune'에서 플레이된 모든 경기 중에서 적어도 한 번 센추리를 기록한 우포수 선수들의 이름을 나열하십시오. player_name을 알파벳순으로 출력하십시오.\n\n-- 해결책:\nselect p.player_name, sum(runs_scored) as run\nfrom ball_by_ball as b, match as m, venue as v, player as p\nwhere (b.match_id = m.match_id and m.venue_id = v.venue_id\n    and p.player_id = b.striker and v.city_name = 'Pune'\n    and p.batting_hand = 'Right-hand bat')\ngroup by b.striker, p.player_name having sum(runs_scored)>=100\norder by run desc, p.player_name;\n```\n\n![이미지](/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_11.png)\n\n보너스 질문:\n자체 해결해보기-\n적어도 한 번의 경기를 이겨온 모든 팀에 대한 승률을 찾으십시오(모든 시즌에 걸쳐). 팀 이름으로 알파벳순으로 결과를 정렬하십시오. 출력 (team_name, win_percentage).\n팀의 승률은 = (팀이 이긴 경기수 / 팀이 플레이한 총 경기수) \\* 100로 계산될 수 있습니다.\n참고: 소수점 셋째 자리까지 백분율로 계산하십시오.\n\n# 결론:\n\n<div class=\"content-ad\"></div>\n\n간단히 말씀드리자면, SQL을 사용하여 IPL 통계에 뛰어들어 본 것은 정말 즐거운 경험이었어요! 선수, 팀 및 경기에 관한 멋진 통찰력을 발견하여 트렌드와 우수한 성적을 눈에 띄게하기 쉬웠습니다.\n\n저의 Github 저장소를 참조하여 SQL 쿼리, 질문 및 데이터셋을 이용할 수 있습니다.\n\n도움이 되었기를 바라며 다시 한번 감사합니다.\n","ogImage":{"url":"/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_0.png"},"coverImage":"/assets/img/2024-06-23-In-DepthAdvanceSQLQueriesforIPLStatistics_0.png","tag":["Tech"],"readingTime":18},{"title":"Snowflake의 미래 부여가 결국 실패할 이유","description":"","date":"2024-06-23 16:32","slug":"2024-06-23-WhyyourSnowflakefuturegrantswilleventuallybreak","content":"\n\n<img src=\"/assets/img/2024-06-23-WhyyourSnowflakefuturegrantswilleventuallybreak_0.png\" />\n\n여기 설정이 있어요...\n\n방금 MyFirstRole이라는 새 역할을 만들었습니다.\n\n```js\ncreate role MyFirstRole;\n```\n\n<div class=\"content-ad\"></div>\n\n계획은 이 역할에 대해 MyDatabase 라는 데이터베이스에 대해 완전한 읽기 액세스 권한을 부여하는 것입니다. MyDatabase 내의 모든 스키마의 모든 현재 테이블 및 뷰를 선택할 수 있도록하고 싶습니다.\n\n```js\ngrant usage on database MyDatabase to role MyFirstRole;\ngrant usage on all schemas in database MyDatabase to role MyFirstRole;\ngrant select on all tables in database MyDatabase to role MyFirstRole;\ngrant select on all views in database MyDatabase to role MyFirstRole;\n```\n\n위 명령문을 설정하면 우리의 역할은 이제 MyDatabase의 모든 항목을 선택할 수 있습니다.\n\n그러나 시간이 흐른다면 데이터베이스에 새로운 테이블 및 뷰가 생성될 것입니다. 새로운 개체가 생성될 때마다 MyFirstRole에게 수동으로 선택 권한을 부여하는 것은 비현실적입니다. 다행히도 미래의 권한 부여가 이 문제를 해결해 줄 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n미래 스키마에서 MyDatabase 내의 향후 역할에 대한 사용 권한을 MyFirstRole 역할에 부여합니다.\n미래 테이블에서 MyDatabase 내의 향후 테이블에 대한 선택 권한을 MyFirstRole 역할에 부여합니다.\n미래 뷰에서 MyDatabase 내의 향후 뷰에 대한 선택 권한을 MyFirstRole 역할에 부여합니다.\n```\n\n이제 MyFirstRole은 앞으로 생성되는 테이블이나 뷰에서 select 문을 실행할 수 있습니다.\n\n모든 것이 잘 되고 예상대로 작동하고 있습니다. MyFirstRole에 대한 select 권한을 수동으로 업데이트할 필요가 없으며, 새롭게 생성된 테이블과 뷰에 액세스를 부여해야 하는 경우를 제외하고는요. 이때, MyDatabase에 제한적인 액세스를 갖는 새로운 역할인 MySecondRole을 생성해야 할 때가 올 것입니다.\n\nMyDatabase에는 MySchema라는 스키마가 있다고 가정합시다. MySecondRole은 MySchema 내의 현재 및 향후 테이블 및 뷰에서만 선택할 수 있고 다른 작업은 제한되어야 한다고 합니다.\n\n<div class=\"content-ad\"></div>\n\n조금만 더 해야겠어요...\n\n```js\ncreate role MySecondRole;\ngrant usage on database MyDatabase to role MySecondRole;\ngrant usage on schema MyDatabase.MySchema to role MySecondRole;\ngrant select on all tables in schema MyDatabase.MySchema to role MySecondRole;\ngrant select on future tables in schema MyDatabase.MySchema to role MySecondRole;\ngrant select on all views in schema MyDatabase.MySchema to role MySecondRole;\ngrant select on future views in schema MyDatabase.MySchema to role MySecondRole;\n```\n\n다시 한 번, MySecondRole에 대한 새 테이블 및 뷰가 MySchema에 생성되는 것처럼 모든 것이 예상대로 작동하는 것처럼 보입니다. 그러나 몇 주 후에 MyFirstRole이 MySecondRole에 대한 그랜트 이후에 생성된 테이블 또는 뷰에 더 이상 액세스 권한이 없다는 것을 깨닫게 됩니다.\n\n그리고 이것이 미래의 권한이 결국 망가지게 되는 이유입니다...\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-WhyyourSnowflakefuturegrantswilleventuallybreak_1.png\" />\n\n간단한 영어로, MyFirstRole은 데이터베이스 수준에서 미래 객체에 권한을 부여받았습니다. ... 데이터베이스에서 미래 테이블을 부여했습니다. 그와 반대로, MySecondRole은 스키마 수준에서 미래 부여를 받았습니다. ... 스키마 수준의 부여는 동일한 데이터베이스 내에서 데이터베이스 수준의 부여보다 우선합니다. 이는 MyFirstRole이 MySecondRole의 미래 부여가 실행된 시점에 테이블 및 뷰에 대한 미래 부여가 무효화되었음을 의미합니다.\n\n이에 대한 해결책은 없습니다. 동일한 데이터베이스에서 데이터베이스 및 스키마 수준에서 미래 부여가 동시에 존재하는 세계에서 살 수 없습니다. 해결책은 데이터베이스의 액세스 패턴을 사전에 파악하고 이에 맞게 부여하는 것입니다. 액세스 요구 사항이 복잡하고 여러 역할 간에 맞춤형인 경우 스키마 수준에서 미래 부여가 적합합니다. 더 간단하고 단일 목적의 데이터베이스(스테이징이 좋은 예시입니다)의 경우에는 데이터베이스 수준의 부여를 사용할 수도 있습니다.\n\n읽어 주셔서 감사합니다!\n\n<div class=\"content-ad\"></div>\n\n안녕하세요! 부담 가지지 마시고 언제든지 인사해주세요 👋\n\n트위터 — https://x.com/jduran9987\n\n링크드인 — https://www.linkedin.com/in/jonathan-duran-80974a183/","ogImage":{"url":"/assets/img/2024-06-23-WhyyourSnowflakefuturegrantswilleventuallybreak_0.png"},"coverImage":"/assets/img/2024-06-23-WhyyourSnowflakefuturegrantswilleventuallybreak_0.png","tag":["Tech"],"readingTime":3},{"title":"데이터 늪에서 데이터 마스터까지 Apache Iceberg와 함께하는 레이크하우스 혁명 ","description":"","date":"2024-06-23 16:31","slug":"2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg","content":"\n\n<img src=\"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_0.png\" />\n\n우리의 데이터 인프라는 처음에 Amazon S3를 사용한 데이터 레이크와 Amazon Redshift를 사용한 데이터 웨어하우스의 조합으로 이루어져 있었습니다.\n\n이 구성은 대량의 데이터를 저장하고 분석할 수 있는 장점이 있었지만, 추가 저장 공간 및 유지보수 문제와 ACID 규칙 준수를 지원하지 않는 등 여러 가지 도전 과제가 있었습니다.\n\n# 목표\n\n<div class=\"content-ad\"></div>\n\n호수집 구조로 전환하는 목표는 데이터 레이크와 데이터 웨어하우스의 최상의 특징을 결합하는 것이었습니다. 이미 완전히 발달한 데이터 레이크가 있었기 때문에, 우리의 초점은 데이터 웨어하우스의 기능을 통합하는 데 있었습니다.\n\n## 데이터 레이크하우스와 데이터 레이크 및 데이터 웨어하우스의 차이는 무엇인가요?\n\n이름에서 알 수 있듯이 '데이터 레이크하우스'는 데이터 레이크와 데이터 웨어하우스의 최상의 특징을 결합합니다. 본질적으로 데이터 레이크하우스는 데이터 레이크의 기능을 확장하여 데이터 웨어하우스와 유사한 기능을 통합합니다. 데이터 레이크의 유연성, 확장 가능성 및 비용 효율성을 제공하는 한편, 데이터 웨어하우스와 주로 관련된 튼튼한 데이터 관리와 ACID (원자성, 일관성, 분리, 지속성) 트랜잭션을 제공하려고 합니다.\n\n# 왜 아파치 아이스버그를 선택했나요?\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_1.png\" />\n\n- ACID 트랜잭션: ACID 트랜잭션을 지원하여 데이터 일관성과 신뢰성을 보장하고, 동시에 쓰기 및 읽기를 허용하여 데이터 오염이 발생하지 않습니다.\n- 비용 및 유지보수 감소: Redshift와 연관된 높은 저장 및 라이선스 비용을 최소화하며, 기본적으로 compaction 및 압축(zstd)을 지원합니다.\n- 성능 최적화: 메타데이터 가지치기, 파티셔닝 및 데이터 건너뛰기와 같은 기능을 통해 쿼리 성능을 크게 향상시킵니다.\n- 호환성: Apache Spark, Flink, Presto 등 여러 데이터 처리 엔진과 함께 작동하여 작업에 최적인 도구를 선택할 수 있는 유연성 제공.\n- Parquet, ORC, Avro와 같은 파일 형식 지원.\n- 기존의 AWS 생태계 및 Athena, Glue, Catalog, EMR 등과 시프트레이엘튼랏하는 탐바.\n- 성능 향상: 빠르게 쿼리되어 데이터를 효율적으로 검색할 수 있습니다.\n- 통합 데이터 처리: 일괄 및 스트리밍 데이터 처리에 대해 통합된 경험을 제공하여 실시간 및 기존 데이터의 원활한 통합 및 처리를 가능하게 합니다.\n\n<img src=\"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_2.png\" />\n\n# Iceberg 아키텍처:\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_3.png)\n\n![Image 2](/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_4.png)\n\n# Apache Iceberg을 활용한 Lakehouse 전환 단계\n\n환경 설정:\n\n\n<div class=\"content-ad\"></div>\n\n- 저장소 구성: Amazon S3와 같은 확장 가능한 저장소 솔루션을 설정하여 원본 데이터와 처리된 데이터를 저장하세요. 이미 저희와 같이 S3를 활용 중이라면 데이터 및 메타데이터를 저장할 대상 버킷을 정의하세요.\r\n- Iceberg 설치 및 구성: EMR을 사용 중이므로 Spark 세션을 생성할 때 Iceberg 관련 설정을 추가해야 합니다.\n\n```js\r\nspark = SparkSession.builder \\\n    .appName(\"user_device_data\") \\\n    .master(\"yarn\") \\\n    .config(\"spark.sql.defaultCatalog\", catalog) \\\n    .config(f\"spark.sql.catalog.{catalog}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog}.warehouse\",\n            \"<Your S3 Warehouse Path>\") \\\n    .config(\"spark.sql.catalog.glue_catalog.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(\"spark.sql.catalog.glue_catalog.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\r\n```\n\n데이터 이전:\n\n- 데이터레이크 — 기존 Parquet에서 Iceberg로 데이터 마이그레이션: 먼저 테이블을 만들었고, 기존 데이터레이크에서 데이터를 읽어와 Apache Spark를 사용하여 Iceberg 테이블에 기록함으로써 메타데이터가 올바르게 캡처되도록합니다.\n- 데이터웨어하우스 — Redshift 데이터를 Iceberg 형식으로 투입: Redshift에서 언로드한 데이터를 S3로 복사한 후, 데이터레이크와 동일한 접근 방식을 따랐습니다.\n\n<div class=\"content-ad\"></div>\n\n아래와 같은 방법으로 인플레이스 마이그레이션을 수행할 수 있습니다:\n\n- add_files 사용\n- migrate 사용\n\n[기존 데이터 레이크를 Apache Iceberg를 사용한 트랜잭션 데이터 레이크로 마이그레이션하기](https://aws.amazon.com/blogs/big-data/migrate-an-existing-data-lake-to-a-transactional-data-lake-using-apache-iceberg/)\n\n기존 ETL 프로세스에서의 조정:\n\n<div class=\"content-ad\"></div>\n\n- 우리는 모든 ETL 작업에 대한 싱크 구성을 변경하여 데이터 아이스버그 형식으로 쓰게 했습니다. 위에서 언급한 구성은 스파크 세션을 만들 때 사용되었습니다.\n\n데이터 거버넌스 및 메타데이터 관리:\n\nIceberg 테이블의 유지 보수 작업.\n\n- Compact : 우리는 다시 쓰고 결과 파일의 원하는 크기로 재작성하기 위해 rewriteDataFiles 절차를 실행합니다. 이것은 읽기 시간을 최적화하는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n('write.parquet.target-file-size-bytes '='52428800') \n# 약 이만큼의 바이트를 대상으로 생성된 파일의 크기를 제어합니다.\n\n2. 스냅샷 만료: 분석에 더 이상 필요하지 않은 데이터에 대해 스냅샷 만료를 실행하여 불필요한 저장 비용을 피합니다. 만료된 스냅샷과 연결된 매니페스트 목록, 매니페스트 및 데이터 파일은 여전히 유효한 스냅샷과 연관되어 있지 않은 한 스냅샷 삭제 시에 삭제됩니다.\n\n우리는 이 작업을 수행하기 위해 expireSnapshots 프로시저를 실행합니다.\n\n3. 오래된 메타데이터 파일 제거: Iceberg는 새 메타데이터 파일이 생성될 때 오래된 메타데이터 파일을 삭제하는 설정을 활성화할 수 있습니다. 또한 테이블이 보유해야 하는 메타데이터 파일 수를 설정할 수 있습니다. 우리는 그 수를 5로 설정했습니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\nwrite.metadata.delete-after-commit.enabled  true\nwrite.metadata.previous-versions-max 5\n```\n\n4. Orphan 파일 삭제 : Orphan 파일 제거를 위해 deleteOrphanFiles 절차를 실행하여 필요 없는 파일을 저장하지 않습니다. 이러한 파일들은 정기적인 정리 프로세스에서 선택되지 않습니다.\n\n쿼리 및 분석:\n\n- 쿼리 최적화: Iceberg는 메타데이터 가지치기(metadata pruning) 및 프리디케이트 푸시다운(predicate pushdown)과 같은 기능을 지원하여 쿼리 성능을 최적화할 수 있습니다. 데이터와 메타데이터 모두에 대한 쿼리 엔진으로 Athena를 사용하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n메타데이터 쿼리 치트 시트 : [https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-table-metadata.html](https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-table-metadata.html)\n\n- 실시간 분석 활성화: 스파크 ETL 프로세스에서 데이터 수집 및 업데이트가 발생하여 배치 및 스트리밍 데이터 처리에 통합된 경험을 제공합니다.\n\n# Apache Iceberg 구현의 장점\n\n비용 효율성:\n\n<div class=\"content-ad\"></div>\n\n- 저장 비용 절감: Apache Iceberg는 기본 Z-표준 (zstd) 압축을 사용하여 저장 공간 요구 사항을 크게 줄입니다. 또한 파티션 분할을 지원하여 데이터 스캔을 제한하여 저장 공간 사용을 최적화합니다.\n- 중복 데이터 웨어하우징 솔루션 소거: Iceberg의 ACID 규정 준수로 기존 데이터 웨어하우징 솔루션이 불필요해집니다. Amazon S3의 저장 비용은 Redshift의 그것보다 상당히 낮기 때문에 Redshift 라이선싱 비용을 상당히 절약할 수 있습니다.\n\nApache Iceberg의 채택으로 일반 Parquet 형식으로 데이터를 저장하는 비용과 전체 S3 비용 모두 30%의 저장 비용 절감과 20%의 S3 비용 절감을 이끌었습니다.\n\n성능 향상:\n\n- 쿼리 성능 개선: Iceberg의 최적화된 데이터 관리 및 인덱싱으로 쿼리 성능과 데이터 검색 시간이 상당히 향상되었습니다.\n\n<div class=\"content-ad\"></div>\n\n역사적 데이터 수정:\n\n- 간편화된 데이터 업데이트: 이전에는 역사적 데이터를 수정하기 위해 작은 배치 작업을 작성해야 했습니다. 아이스버그를 사용하면 몇 가지 업데이트 명령을 실행함으로써 이를 달성할 수 있어, 프로세스가 간소화되었습니다.  \n\n접근 제어:\n\n- 간편화된 행 수준 접근: Redshift에서 서로 다른 국가에 대한 행 수준 접근을 제공하는 것은 복잡했습니다. 아이스버그를 통해 데이터 파티셔닝을 사용하여 특정 버킷에 대한 접근 정책을 쉽게 구현할 수 있어, 접근 관리가 간소화됩니다.\n\n<div class=\"content-ad\"></div>\n\nApache Iceberg를 구현함으로써 비용 효율성, 향상된 성능, 간단화된 기존 데이터 수정 및 향상된 액세스 관리를 달성했습니다.\n\n# 결론\n\n여러 데이터셋을 Apache Iceberg로 이관하는 작업을 성공적으로 완료했으며 이미 상당한 비용 및 성능 이점을 확인하고 있습니다. 레이크하우스 아키텍처로의 전환은 데이터 레이크와 데이터 웨어하우스의 최상의 기능을 활용할 수 있게 해주어 더 효율적이고 확장 가능한 데이터 인프라를 구축하게 되었습니다.\n\n이 구현 기간 동안 빈말 야다브와 시바무 구프타에게 놀라운 헌신과 값진 기여에 진심으로 감사드립니다.\n\n<div class=\"content-ad\"></div>\n\n사실, 한국어로 \"테이블\" 태그를 \"Markdown\" 형식으로 변환하면 되는 것 같아요.","ogImage":{"url":"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_0.png"},"coverImage":"/assets/img/2024-06-23-FromDataSwampstoDataMasteryEmbracingtheLakehouseRevolutionwithApacheIceberg_0.png","tag":["Tech"],"readingTime":7},{"title":"2024년에 데이터 엔지니어가 되기 위한 조건은 무엇일까","description":"","date":"2024-06-23 16:28","slug":"2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024","content":"\n\n<img src=\"/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_0.png\" />\n\n만약 이 글을 읽고 있다면, 요즘 직업을 바꾸려고 고민 중이신 것 같군요. 아마도 소프트웨어 엔지니어링과 데이터베이스 디자인에 관심이 있으신 거겠죠. 당신의 배경이 무엇이든 중요하지 않아요 — 마케팅, 분석, 혹은 금융 분야에서 오셨든, 여러분도 할 수 있어요! 이 이야기는 데이터 공간에 빠르게 진입하는 방법을 찾아주기 위한 것이에요. 예전에 저도 똑같이 해서 그 뒤로 후회한 적이 없어요. 기술 분야, 특히 데이터는 매력과 혜택이 넘쳐나요. 선뜻 언급하지 않은 원격 근무와 선도 기업들로부터의 막대한 혜택 패키지도 말이죠. 파일과 숫자로 마법을 부릴 수 있다는 사실 그 자체가 멋진 거 아니겠어요? 이 글에서는 2~3개월 안에 완료할 수 있는 기술과 프로젝트들을 요약해서 소개할 거에요. 상상해보세요, 몇 달간의 노력만으로 첫 직장 면접 준비를 마치고 있을 수 있답니다.\n\n## 왜 데이터 엔지니어링이고 데이터 과학이 아닌가요?\n\n사실 왜 데이터 분석이나 데이터 과학이 아니고 데이터 엔지니어링을 선택하는 걸까요? 저는 이 역할의 본질에 답이 있다고 생각해요. 데이터 엔지니어가 되기 위해서는 소프트웨어 엔지니어링과 데이터베이스 디자인, 기계 학습(ML) 모델, 그리고 데이터 모델링과 비즈니스 인텔리전스(BI) 개발을 배워야 해요.\n\n<div class=\"content-ad\"></div>\n\n데이터 엔지니어링은 DICE에 따르면 가장 빠르게 성장하는 직업입니다. 그들은 갭을 보여주기 위해 연구를 수행했습니다. 빨리 움직이세요.\n\n![이미지](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_1.png)\n\n데이터 과학자는 장기간 시장에서 \"가장 매혹적인\" 직업으로 여겨져 왔지만, 최근에 데이터 엔지니어의 부족이 있다는 것으로 보입니다. 이 분야에서 엄청난 수요가 있음을 확인할 수 있습니다. 이는 경험이 풍부하고 높은 자격을 갖춘 엔지니어들 뿐만 아니라 입문자도 포함됩니다. 지난 5년 동안 영국에서 데이터 엔지니어링은 가장 빠르게 성장하는 직업 중 하나였으며, 2023년 LinkedIn의 인기 있는 직업 목록에서 13위를 차지했습니다. 평균적으로 매주 약 4차례의 취업 면접 요청을 받습니다. 입문 데이터 엔지니어들은 훨씬 더 자주 초대될 것입니다.\n\n데이터 엔지니어링이 매우 복잡하기 때문에 급여와 혜택 package는 다른 기술 분야보다 훨씬 더 좋아보입니다. 실제로, 데이터가 잡다하고 지루한 데이터 조작 작업인 것처럼 보여 소프트웨어 엔지니어 중에는 데이터를 피하는 것을 선호하는 사람들도 많이 있습니다. 이로 인해 데이터 엔지니어링은 데이터 플랫폼 및 데이터 파이프라인 디자인 패턴을 배우려는 사람들에게 수익성 있는 목표가 되고 있습니다. 데이터 엔지니어링은 데이터 조작 및 프로세스 조정에 관한 것입니다. 데이터는 정제되고 테스트되고 승인되어 사용자에게 적시에 전달되어야 합니다. 이는 ML 및 BI가 그것을 많이 의존하는 이유입니다.\n\n<div class=\"content-ad\"></div>\n\n이 이야기에서는, 2~3개월 안에 달성할 수 있는 기술 세트와 가능한 프로젝트를 요약해 보려고 합니다. 상상해보세요, 몇 달 동안 적극적인 학습을 한 후에는 첫 직장 면접 준비가 완료되어 있게 됩니다.\n\n## 데이터 엔지니어링은 압도적으로 느껴질 수 있어요\n\nSTEM(과학, 기술, 공학, 수학) 배경 없이 데이터 엔지니어링에 뛰어드는 것은 매우 어려울 수 있어요. 코딩 자체가 쉬운 일이 아니에요. 데이터베이스 및 데이터 파이프라인 오케스트레이션은 처음부터 이해하기가 더욱 어려웠어요. 몇 년 전에 저는 양적 금융을 전공한 석사 학위를 받고 분석 매니저로 일했었어요. 코딩을 배우기로 결정한 날을 기억해요. 대학에서 얻은 수준처럼은 아니지만 실제 세계의 문제를 해결하기 위해 실무에서 나의 기술을 적용할 수 있도록 배우고자 한 것이죠.\n\n일상적으로 업무를 수행하면서 여가 시간에만 소프트웨어 엔지니어링을 배워야 했던 어려움이 있었던 것도 기억이 나네요.\n\n<div class=\"content-ad\"></div>\n\n나는 Fiverr과 PeoplePerHour에서 프로젝트를 찾아보며 기업들이 데이터에 대해 어떤 것을 필요로 하는지 살펴보았던 기억이 납니다. 지금 생각해보면 이것이 많은 고객들의 진정한 고통 포인트를 이해하는 데 많은 도움이 되었고 아마도 가장 효율적인 학습 방법이었을 것입니다.\n\n그래서 모든 데이터 실무자를 향한 첫 번째 조언은 믿음입니다.\n\n데이터 엔지니어링 분야에 진입하는 것은 압도적으로 느껴질 수 있지만 가치가 있습니다. 부끄러워하지 마시고 글을 쓰는 사람들에게 물어보세요. Medium은 그런 점에서 정말 좋은 곳입니다. 왜 취향에 맞는 주제를 확인하고 누구를 팔로우할지 확인해보지 않으세요?\n\n## 계획\n\n<div class=\"content-ad\"></div>\n\n쉬는 시간을 가져가서 진정으로 그것이 필요한지 생각해보세요. 만약 답이 '예' 라면 필요한 건 계획 뿐입니다. 이 곳에서 목표는 속도가 아닙니다. 가능하다면 아무런 고통 없이 \"데이터 엔지니어링에 진입하는 방법\"을 실현 가능한 해결책으로 기록하는 것이 이제의 목표입니다.\n\n지금은 단지 다음 몇 달에 집중하고자 하는 것과 복습해야 할 것들을 생각해봅시다.\n\n## 데이터 엔지니어링의 습관\n\n첫 두 주 동안 실제로 배우면서 이 습관을 습득하고자 할 것입니다. 조금씩 하되 꾸준히 합니다. 학습의 습관을 형성해야 합니다. 예를 들어, 저는 Google Professional Data Engineer 시험 준비를 하면서 이렇게 했습니다. 매일 아침에 체육관에서 사이클을 타면서 책을 읽었죠. 아침이 가장 생산적인 시간이기 때문에 그 때 진행했습니다. 이 2020년의 글은 여전히 유효합니다. 많은 것들이 실제로 변한 게 많지 않고, 학습은 주로 데이터 엔지니어링의 기본 원칙에 대해 집중했습니다. 물론, 제품 특정 질문이 많았지만, 이 글은 빠르게 학습하는 방법에 대한 지침서입니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 엔지니어링은 다음과 같은 기술 영역들에 관련이 있어요:\n\n- ETL 및 데이터 추출\n- 데이터 조작과 데이터 모델링 (대개 SQL을 사용함)\n- 파이프라인 테스트\n- 데이터 테스트\n- 보고 및 비즈니스 인텔리전스\n- MLOps 및 기계 학습 파이프라인\n- 이 모든 것을 조율하는 것\n\n## 첫 1-2주: SQL\n\n일단 SQL에 집중해 볼게요. 목록에서 첫 번째 항목은 아니지만 저는 이것이 가장 보편적이라고 생각해요. SQL 방언은 데이터 모델링에서 널리 사용되어 왔기 때문에 이제 데이터 조작의 표준으로 간주될 수 있어요. 처음 두 주 동안 해야 할 것은 다양한 SQL 쿼리를 실행해보고 어떤 데이터 파이프라인에서 사용될 수 있는지 상상하는 거에요. 여기서 다시 정리하고 싶은 것들은 아마도 다음과 같을 거에요:\n\n<div class=\"content-ad\"></div>\n\n- SQL을 사용하여 테이블을 만드는 방법\n- 공통 테이블 표현식을 사용하는 방법\n- SQL을 사용하여 데이터를 모킹하는 방법\n- 증분 전략을 사용하여 테이블을 업데이트하는 방법\n- 데이터 품질을 테스트하고 데이터를 정리하는 방법\n\n이러한 질문들은 압도될 수 있을지도 모르지만, 많은 훌륭하고 간단한 예제들이 있습니다. 이러한 예제들은 몇 가지 무료 데이터 웨어하우스 솔루션과 결합하여 비교적 간단하고 생산적인 샌드박스를 만드는 데 도움이 될 수 있습니다. 이에 대해 이전 이야기 중 하나에서 이야기했었습니다. SQL 관련해서는 일상적인 데이터 엔지니어링에 실제로 필요한 모든 것입니다.\n\n가장 어려운 주제인 MERGE와 같은 주제도 SQL이 CTE 내에 목된 데이터를 포함할 때 쉽게 설명될 수 있습니다:\n\n```js\ncreate temp table last_online as (\n    select 1 as user_id\n    , timestamp('2000-10-01 00:00:01') as last_online\n)\n;\ncreate temp table connection_data  (\n  user_id int64\n  ,timestamp timestamp\n)\nPARTITION BY DATE(_PARTITIONTIME)\n;\ninsert connection_data (user_id, timestamp)\n    select 2 as user_id\n    , timestamp_sub(current_timestamp(),interval 28 hour) as timestamp\nunion all\n    select 1 as user_id\n        , timestamp_sub(current_timestamp(),interval 28 hour) as timestamp\nunion all\n    select 1 as user_id\n        , timestamp_sub(current_timestamp(),interval 20 hour) as timestamp\nunion all\n    select 1 as user_id\n    , timestamp_sub(current_timestamp(),interval 1 hour) as timestamp\n;\n\nmerge last_online t\nusing (\n  select\n      user_id\n    , last_online\n  from\n    (\n        select\n            user_id\n        ,   max(timestamp) as last_online\n\n        from \n            connection_data\n        where\n            date(_partitiontime) >= date_sub(current_date(), interval 1 day)\n        group by\n            user_id\n\n    ) y\n\n) s\non t.user_id = s.user_id\nwhen matched then\n  update set last_online = s.last_online, user_id = s.user_id\nwhen not matched then\n  insert (last_online, user_id) values (last_online, user_id)\n;\nselect * from last_online\n;\n```\n\n<div class=\"content-ad\"></div>\n\n## 주 3–4: 현대 데이터 스택\n\n현대 데이터 스택 및 데이터 플랫폼 아키텍처 유형에 대한 몇 가지 이야기를 읽는 것을 추천합니다. 데이터 엔지니어링에서 사용할 수 있는 다양한 도구 및 프레임워크에 대한 전략적 개요를 제공하여, 채용 인터뷰 중에 기술에 능통하다는 것을 채용 담당자에게 알려줍니다. 모든 도구를 알 필요는 없지만, \"당신은 이 분야에 계십니까\"라는 질문은 잠재 고용주와의 첫 만남 중에 상상할 수 있는 가장 매혹적인 질문입니다. 여기서는 기술의 최근 이벤트(IPO, 합병 및 인수), 개발 및 새로운 도구에 대한 인식을 보여주고 싶습니다. DuckDB 또는 Polars와 같은 것에 대해 들었다고 언급하는 것만으로도 여러분이 호기심이 많고 열정적이라는 것을 사람들에게 알려줍니다.\n\n시장에 있는 다양한 데이터 도구로 쉽게 길을 잃을 수 있습니다. 눈송이(Snowflake)에 대해 이야기하고, 그 IPO가 얼마나 성공적이었는지를 언급했던 것을 기억합니다. 그것이 많은 도움이 되었거나 적어도 우리가 인터뷰어와 동일한 의견을 가졌다는 것 같아요. 우리는 현대 데이터 스택 및 그것을 현대적이고 견고하며 비용 효율적으로 만드는 요소들에 대해 토론했습니다. 간단히 말해서, 데이터를 다루기 위해 사용되는 도구 모음입니다. 데이터를 어떻게 처리할 것인가에 따라, 이러한 도구는 다음을 포함할 수 있습니다:\n\n- 관리되는 ETL/ELT 데이터 파이프라인 서비스\n- 클라우드 기반 관리되는 데이터 웨어하우스/데이터 레이크(데이터의 대상지)\n- 데이터 변환 도구\n- 비즈니스 인텔리전스 또는 데이터 시각화 플랫폼\n- 기계 학습 및 데이터 과학 기능\n\n<div class=\"content-ad\"></div>\n\n앞선 두 주 동안 SQL을 사용하여 데이터를 변환하고 조작하는 방법을 이미 배웠어요. 이제 이 전략적인 지식을 활용하는 방법을 알게 됐으니, 그에 맞게 활용해봐요.\n\n![이미지](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_2.png)\n\n## 5-6주차: 파이썬 기초\n\n지금은 파이썬에 대한 지식을 되짚거나 살짝 배우는 시기예요. 파이썬은 정말 배우기 쉬운 방법이죠. 스크립트 형식이라 코드를 읽기 쉽고 유용한 라이브러리가 많습니다. 이 모든 특징들로 인해 데이터 엔지니어링에서 프로그래밍 언어로 많이 선택되었어요. 반복문, 함수, 조건문, 오류 처리, 그리고 데이터 구조와 같은 기본적인 프로그래밍 개념에 초점을 맞출 거예요. 데이터 엔지니어링에서는 이런 것들을 자주 사용할 거라고 생각해요.\n\n<div class=\"content-ad\"></div>\n\n제안드리는 바는 데이터 API 및 요청을 통해 시작하는 것입니다. 이러한 지식을 클라우드 서비스와 결합하면 미래에 필요한 모든 ETL 프로세스를 위한 매우 좋은 기반이 마련됩니다.\n\n전형적인 데이터 파이프라인 [5]은 Python 함수(또는 오퍼레이터)의 연쇄이며 다음과 같이 보일 것입니다:\n\n\n![pipeline](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_3.png)\n\n\n우리는 Python 함수를 사용하여 데이터를 처리하며, 결과적으로 다음과 같은 파이프라인을 얻을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_4.png)\n\nMore examples can be found here:\n\n## API requests\n\n이해API 요청 방법은 중요합니다. 이것은 ETL 서비스가 다른 서비스와 상호 작용하는 주요 방법이기 때문입니다. 즉, 데이터를 추출합니다. 데이터 엔지니어는 API 서비스에 요청을 보내어 데이터를 요청할 때 많이 사용합니다. 그런 다음 실제로 변환 (ETL)하는 데이터를 페이징하거나 스트림으로 응답합니다. 아래 예제를 고려해보십시오. 이 예제는 NASA 소행성 API에서 데이터를 추출하는 방법을 설명합니다. 이는 매우 간단한 예제이며 매우 배우기 쉬운 이유입니다.\n\n\n<div class=\"content-ad\"></div>\n\n```python\n# nasa.py\nimport requests\nsession = requests.Session()\n\nurl=\"https://api.nasa.gov/neo/rest/v1/feed\"\napiKey=\"your_api_key\"\nrequestParams = {\n    'api_key': apiKey,\n    'start_date': '2023-04-20',\n    'end_date': '2023-04-21'\n}\nresponse = session.get(url, params=requestParams, stream=True)\nprint(response.status_code)\n```\n\n더 고급이고 실행 가능한 예제는 [여기 이야기](6)에서 찾을 수 있습니다.\n\n## 7–8 주차: 추출 — 적재\n\n파이썬과 SQL을 조금 배우면 실제 데이터를 추출하고 클라우드 어딘가에 저장할 수 있습니다. AWS, GCP 및 Azure와 같은 클라우드 서비스 제공업체들이 시장을 선도하고 있으며 그 중 적어도 하나에 익숙해지는 것이 필수적입니다. 그래서 이제 우리는 실제로 첫 번째 데이터 파이프라인을 만들고 싶어할 것입니다. 이것은 간단한 함수일 수 있습니다. NASA 소행성 데이터를 추출하여 AWS S3에 저장하는 것이다. 그게 다입니다! 아주 간단하지만 이것이 우리의 첫 번째 데이터 파이프라인이며 매일, 매 시간 등으로 실행되도록 예약할 수 있습니다. 서버리스 마이크로서비스로 배포하고 무료로 실행되어 클라우드 저장 공간에서 데이터를 추출 및 보존합니다. AWS 웹 UI를 사용하여 쉽게 배포할 수 있습니다. 그러나 서비스를 배포하는 더 선호되는 방법은 인프라스트럭처 코드입니다. 해당 주제는 본질적으로 이해하기 어려우며 초보자이신 경우 깊게 파고들지 않는 것이 좋습니다.\n\n\n<div class=\"content-ad\"></div>\n\n다음 몇 주 동안은 명령줄 도구에 주로 초점을 맞추고 클라우드 기능을 배포하고 클라우드에서 리소스를 프로비저닝하는 몇 가지 트릭을 익히는 것을 추천합니다.\n\n간단한 AWS 람다처럼 ETL 서비스를 생성할 수 있습니다:\n\n```js\n# AWS CLI를 사용하여 패키지된 람다 배포:\naws \\\nlambda create-function \\\n--function-name etl-service-lambda \\\n--zip-file fileb://stack.zip \\\n--handler <당신의 람다 핸들러 경로>/app.lambda_handler \\\n--runtime python3.12 \\\n--role arn:aws:iam::<당신의 AWS 계정 ID>:role/my-lambda-role\n\n# # 이미 배포되었다면 업데이트를 위해 다음을 사용합니다:\n# aws --profile mds lambda update-function-code \\\n# --function-name mysql-lambda \\\n# --zip-file fileb://stack.zip;\n```\n\n예를 들어, AWS CLI를 사용하여 ETL 서비스를 호출할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\naws lambda invoke \\\n    --function-name etl-service-lambda \\\n    --payload '{ \"data\": \"value\" }' \\\n    response.json\n```\n\n일반적인 코드 예시와 람다 패키지를 사용한 예시는 여기 [7]에서 확인할 수 있습니다.\n\n이제 데이터가 클라우드에 저장되었으므로 데이터 웨어하우스 도구로 불러올 수 있습니다. 저는 1TB 데이터 스캔당 $5가 청구되는 BigQuery를 권장합니다. 테스트 데이터로 작업하게 되므로 비용이 전혀 발생하지 않습니다. 다음과 같이 외부 테이블을 만들 수 있습니다 [8]:\n\n```js\n# 데이터가 Google Cloud Storage에 저장되어 있다고 가정\nLOAD DATA INTO source.nasa_asteroids\nFROM FILES(\n  format='JSON',\n  uris = ['gs://nasa-asteroids-data/*']\n)\n```\n\n<div class=\"content-ad\"></div>\n\n또한 AWS CLI의 기본을 배우고 싶다면 이 기사를 시도해보실 수도 있습니다 [9]\n\n## 9-10주차\n\n이번에는 Python에서 단위 테스트의 기본을 배우고 ETL 서비스를 조정하는 방법에 대해 알아보고 싶어요. 이미 Python 함수를 만들어 API 호출을 수행하고 데이터를 추출하는 방법을 배웠죠. 아마도 우리는 Python for Data Engineers의 예제를 사용하여 Python에서 데이터를 변환하는 방법도 배웠을지도 모르겠어요. 이제는 적용한 데이터 변환 로직을 테스트하고 싶어요. 단위 테스트는 소프트웨어 엔지니어링에서 필수적인 기술이며 장기적으로 많은 시간을 절약해줍니다. 코드를 테스트하고 유지하는데 도움이 되죠. 요약하자면 Pytest 모듈의 기본을 배우고 싶습니다. 예를 들어, 아래와 같은 ETL 함수의 논리를 테스트할 수 있어야 합니다:\n\n```js\n# etl.py\ndef etl(item):\n    # 여기서 데이터 변환을 수행\n    return item.lower()\n```\n\n<div class=\"content-ad\"></div>\n\nPython 스크립트를 실행하기만 하면 간단히 이 작업을 수행할 수 있습니다 [10]:\n\n```js\n# etl_test.py\nfrom etl import etl\n\ndef test_etl_returns_lowercase():\n    assert etl('SOME_UPPERCASE') == 'some_uppercase'\n```\n\nunittest 라이브러리를 사용한 또 다른 예시를 살펴보겠습니다:\n\n```js\n# ./prime.py\nimport math\n\ndef is_prime(num):\n    '''num이 소수인지 확인합니다.\n    '''\n    for i in range(2,int(math.sqrt(num))+1):\n        if num%i==0:\n            return False\n    return True\n```\n\n<div class=\"content-ad\"></div>\n\nunittest을 사용하면 간단해요. 다음과 같이 테스트할 거예요:\n\n```js\n# ./test.py\nimport unittest\nfrom prime import is_prime\n\nclass TestPrime(unittest.TestCase):\n\n    def test_thirteen(self):\n        self.assertTrue(is_prime(13))\n```\n\n이제 테스팅의 기본을 알았으니, 함수들이 올바른 데이터를 반환하는지 확신할 수 있어요. 그러므로 다음 단계는 ETL 프로세스를 조정하는 추가적인 마이크로서비스를 배포하는 것이죠. 요약하면, 이는 간단한 AWS Lambda 함수거나 스케줄에 따라 우리의 ETL 서비스를 호출할 수 있는 다른 서버리스 애플리케이션이 될 수 있어요. 이것은 매우 간단하며 여기서 복잡하게 하고 싶지 않아요. 다른 Python Lambda 함수를 배포하고 매일 또는 매시간 실행되도록 스케줄을 지정할 거예요. 그것을 위해 AWS EventBridge 이벤트를 사용하고 cron 스케줄을 설정할 수 있어요. 우리의 Orchestator Lambda 코드는 아래와 같이 보일 거에요.\n\n```js\nimport json\nimport boto3\n \n# 다른 Lambda를 호출하기 위한 AWS Lambda 클라이언트\nclient = boto3.client('lambda')\n \ndef lambda_handler(event, context):\n \n    # 다른 Lambda로 전달할 데이터\n    data = {\n        \"ProductName\": \"iPhone SE\"\n    }\n \n    response = client.invoke(\n        FunctionName='arn:aws:lambda:eu-west-1:12345678:function:etl-service-lambda',\n        InvocationType='RequestResponse',\n        Payload=json.dumps(data)\n    )\n \n    response = json.load(response['Payload'])\n \n    print('\\n')\n    print(response)\n```\n\n<div class=\"content-ad\"></div>\n\n만약 더 알고 싶다면, AWS Step Functions 및 Infrastructure as Code와 관련된 고급 튜토리얼이 있습니다.\n\n## 11–12 주차\n\nML 기본 지식을 배우세요. 데이터를 추출하고 ETL을 수행하는 방법 및 데이터 웨어하우스로 데이터를 로드하는 방법을 이미 알고 있습니다.\n\n아래 튜토리얼을 살펴보세요. 이 튜토리얼은 사용자 이탈을 다루고 행동 데이터를 사용하여 사용자 이탈을 예측하는 방법을 설명합니다. 몇 시간 만에 완료할 수 있지만 이탈에 대한 구체적인 내용을 깊게 파고들고 싶다면 더 많은 시간이 걸릴 수 있습니다. 모든 머신 러닝 모델을 알 필요는 없습니다. 우리는 머신 러닝 및 데이터 과학 분야에서 아마존 및 구글과 경쟁할 수는 없지만, 사용하는 방법을 알아야 합니다. 클라우드 서비스 제공 업체들이 제공하는 다양한 관리형 ML 서비스가 있으며, 그것들에 친숙해져야 합니다. 데이터 엔지니어들은 이러한 서비스를 위해 데이터 세트를 준비하며, 이에 대해 몇 개의 튜토리얼을 진행하는 것이 유용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 결론\n\n데이터 엔지니어링을 빠르게 학습하고 이 분야의 전문가가 되어야 한다고 자신을 누르지 마세요. 많은 사람들에게는 특정 분야를 숙달하는 데 몇 년이 걸리기 때문에 주말에 학습하면서 몇 달 안에 이룰 수 있는 목표에 집중하는 것이 좋습니다. 데이터 엔지니어는 ETL/ELT 기술, 데이터 모델링에 대한 충분한 지식이 필요하며 적어도 Python에서 코딩할 수 있어야 합니다. 이 글에서는 데이터 엔지니어링을 가장 효율적으로 배울 수 있는 12주 계획을 개요로 설명했습니다. 즐기시기 바랍니다.\n\n## 추천 도서\n\n[1] https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/\n\n<div class=\"content-ad\"></div>\n\n[2] [Google Professional Data Engineer 시험을 2020년에 통과한 방법](https://towardsdatascience.com/how-i-passed-google-professional-data-engineer-exam-in-2020-2830e10658b6)\n\n[3] [초보자를 위한 고급 SQL 기술](https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488)\n\n[4] [데이터 플랫폼 아키텍처 유형](https://towardsdatascience.com/data-platform-architecture-types-f255ac6e0b7)\n\n[5] [데이터 파이프라인 디자인 패턴](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3)\n\n<div class=\"content-ad\"></div>\n\n\n[6] [Python for Data Engineers](https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd)\n\n[7] [Building a Batch Data Pipeline with Athena and MySQL](https://towardsdatascience.com/building-a-batch-data-pipeline-with-athena-and-mysql-7e60575ff39c)\n\n[8] [When Your Stack is a Lake House](https://medium.com/towards-artificial-intelligence/when-your-stack-is-a-lake-house-6bcb17f9bff6)\n\n[9] [Mastering AWS CLI](https://medium.com/geekculture/mastering-aws-cli-5454ad5e685c)\n\n<div class=\"content-ad\"></div>\n\n[10] [Python을 사용한 데이터 파이프라인 테스트 안내](https://towardsdatascience.com/a-guide-to-data-pipeline-testing-with-python-a85e3d37d361)\n\n[11] [데이터 파이프라인 Orchestration](https://towardsdatascience.com/data-pipeline-orchestration-9887e1b5eb7a)","ogImage":{"url":"/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_0.png"},"coverImage":"/assets/img/2024-06-23-WhatDoesitTaketoGetintoDataEngineeringin2024_0.png","tag":["Tech"],"readingTime":14},{"title":"기초부터 고급까지 빅데이터 전문가를 위한 Apache Hive 완벽 가이드","description":"","date":"2024-06-23 16:26","slug":"2024-06-23-FromBasicstoAdvancedNavigatingApacheHiveforBigDataProfessionals","content":"\n\n![image](/assets/img/2024-06-23-FromBasicstoAdvancedNavigatingApacheHiveforBigDataProfessionals_0.png)\n\n아파치 하이브는 하둡을 위한 데이터 웨어하우징 및 SQL과 유사한 쿼리 언어입니다. 페이스북에서 개발되었으며, 현재 아파치 소프트웨어 재단의 일부이며 다양한 기관에서 대용량 데이터 처리를 위해 사용됩니다.\n\n혜택\n\n아파치 하이브는 특히 하둡 생태계 내 대용량 데이터 처리의 맥락에서 다른 도구들보다 여러 가지 이점을 제공합니다. 여기에는 몇 가지 주요 혜택이 있습니다:\n\n<div class=\"content-ad\"></div>\n\n1. SQL과 유사한 인터페이스\n\na) 사용 편의성: 하이브는 HiveQL이라는 SQL과 유사한 쿼리 언어를 제공하여, 이미 SQL을 알고 있는 사용자에게 쉽게 접근할 수 있습니다. 이는 새로운 프로그래밍 언어나 쿼리 구문을 배워야 하는 다른 도구와 비교하여 학습 곡선을 낮춰줍니다.\n\nb) 선언형 언어: 사용자가 필요로 하는 데이터에 집중할 수 있도록 처리 로직의 세부사항을 신경 쓰지 않게 합니다.\n\n2. 확장성과 성능\n\n<div class=\"content-ad\"></div>\n\na) 확장성: Hive는 Hadoop의 분산 저장소(HDFS) 및 처리(MapReduce, Tez, 또는 Spark)를 활용하여 대규모 데이터셋을 효율적으로 처리할 수 있도록 설계되었습니다.\n\nb) 성능: 파티셔닝, 버킷팅, 그리고 벡터화된 쿼리 실행과 같은 기술을 통해 Hive는 쿼리 성능을 크게 최적화할 수 있습니다.\n\n3. Hadoop 생태계와의 통합\n\na) 원활한 통합: Hive는 HDFS, YARN, HBase와 같은 다른 Hadoop 구성 요소뿐만 아니라 Flume와 Sqoop과 같은 데이터 수집 도구와도 원활하게 통합됩니다.\n\n<div class=\"content-ad\"></div>\n\nb) 유연성: 하이브는 텍스트, 시퀀스, Avro, ORC 및 Parquet과 같은 다양한 데이터 형식을 지원하여 다양한 사용 사례에 유용합니다.\n\n4. 확장성\n\na) 사용자 정의 함수(UDFs): 하이브를 사용하면 사용자가 자바, 파이썬 또는 다른 언어로 사용자 정의 함수를 작성하여 기능을 확장할 수 있습니다. 이는 복잡한 처리 요구를 처리하는 유연성을 제공합니다.\n\nb) 확장성: 하이브의 아키텍처는 사용자 정의 SerDes(직렬화/역직렬화기) 및 입력-출력 형식을 추가하는 것을 지원하여 확장성을 향상시킵니다.\n\n<div class=\"content-ad\"></div>\n\n5. 데이터 읽기 시 구성 스키마\n\n- 구성 스키마에 대한 설명: 전통적인 데이터베이스가 쓰기 작업 중에 스키마를 강제하는 것과 달리, 하이브는 읽기 시에 스키마를 적용합니다. 이는 반정형 및 비구조화된 데이터를 다루는 데 유연성을 제공합니다.\n\n6. 비용 효율성\n\n- 오픈 소스: 하이브는 아파치 소프트웨어 재단의 오픈 소스 프로젝트로, 무료로 사용할 수 있고 개발 및 지원에 기여하는 대규모 커뮤니티가 존재합니다.\n\n<div class=\"content-ad\"></div>\n\nb) 상품 하드웨어: 하이브는 하둡에서 실행되는데, 이는 전통적인 데이터 웨어하우징 솔루션과 비교하여 인프라 비용을 줄이기 위해 상품 하드웨어에서 실행되도록 설계되었습니다.\n\n7. 비즈니스 인텔리전스와 분석\n\na) BI 도구 통합: 하이브는 Tableau, Power BI 및 Apache Superset과 같은 비즈니스 인텔리전스 도구와 쉽게 통합될 수 있으며, 데이터 시각화 및 분석을 용이하게 합니다.\n\nb) 즉석 쿼리: 하이브는 대규모 데이터셋에서 즉석 쿼리를 실행하는 데 적합하며, 탐색적 데이터 분석에 유용합니다.\n\n<div class=\"content-ad\"></div>\n\n8. 트랜잭션 지원\n\na) ACID 트랜잭션: 하이브는 ACID (원자성, 일관성, 고립성, 지속성) 트랜잭션을 지원하여 데이터 무결성이 중요한 시나리오에서 신뢰성 있고 일관된 데이터 처리를 가능하게 합니다.\n\n9. 다양한 기능 세트\n\na) 인덱싱과 캐싱: 하이브는 쿼리 성능을 높이기 위한 인덱싱을 지원하며 쿼리 결과를 캐시하여 반복적인 쿼리에 대한 응답 시간을 개선할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nb) 조인 및 집계: 하이브는 데이터 분석에 필수적인 복잡한 조인 및 집계를 강력하게 지원합니다.\n\n10. 보안 및 접근 제어\n\na) 인증 및 권한 부여: 하이브는 Kerberos 인증을 지원하며, 세밀한 접근 제어를 위해 Apache Ranger 또는 Apache Sentry와 통합되어 데이터 보안과 규제 요구 사항을 준수합니다.\n\nb) 암호화: 데이터 암호화를 통해 데이터를 여유 및 전송 중 모두 안전하게 보호할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 아키텍처:\n\n1. 하이브 클라이언트: 사용자가 하이브와 상호 작용하는 인터페이스입니다. Command Line Interface (CLI), JDBC/ODBC 드라이버, 그리고 Apache Hue와 같은 웹 인터페이스가 포함됩니다.\n\n2. 하이브 서비스: 하이브서버2(쿼리 실행), 웹 기반 HCatalog인 WebHCat, 그리고 스키마 및 메타데이터 저장을 위한 메타스토어가 포함됩니다.\n\n3. 하이브 저장 및 연산: 데이터 저장은 일반적으로 HDFS(Hadoop 분산 파일 시스템)에 있으며, MapReduce, Tez, 또는 Spark를 사용하여 연산이 수행됩니다.\n\n<div class=\"content-ad\"></div>\n\n하이브에서의 데이터 모델링:\n\n1. 데이터베이스: 테이블의 네임스페이스.\n\n2. 테이블: 행과 열로 구성된 구조화된 데이터.\n\n3. 파티션: 테이블을 분할하여 쿼리와 관리를 쉽게 할 수 있는 세그먼트로 나눔.\n\n<div class=\"content-ad\"></div>\n\n안녕하세요! 아래는 한국어로 번역한 내용입니다.\n\n고급 기능:\n\n1. 사용자 정의 함수(UDFs): 내장 함수로 처리할 수 없는 특정 작업을 위해 사용자가 작성한 사용자 지정 함수입니다.\n\n2. 조인 작업: 내부 조인, 외부 조인, 맵 사이드 조인을 지원하여 효율적인 처리를 도와줍니다.\n\n<div class=\"content-ad\"></div>\n\n3. 인덱스: 데이터를 스캔하는 양을 줄여 쿼리 성능을 향상시킵니다.\n\n4. 뷰: 쿼리 결과에 의해 생성된 가상 테이블입니다.\n\n성능 튜닝:\n\n1. 파티셔닝: 스캔하는 데이터 양을 제한하여 쿼리 성능을 향상시킵니다.\n\n<div class=\"content-ad\"></div>\n\n2. Bucketing: 맵 쪽 조인을 더 효율적으로 돕습니다.\n\n3. Vectorization: 쿼리 성능을 향상시키기 위해 여러 행을 함께 처리합니다.\n\n4. Cost-Based Optimization (CBO): 통계를 사용하여 쿼리 실행 계획을 최적화합니다.\n\n사용 사례:\n\n<div class=\"content-ad\"></div>\n\n데이터 웨어하우징: 대규모 데이터셋을 저장하고 분석합니다.\n\nETL 작업: 대용량 데이터에 대한 추출, 변환 및 로드 작업을 수행합니다.\n\n데이터 분석: 대규모 데이터의 일괄 처리 및 분석을 수행합니다.\n\n비즈니스 인텔리전스: 보고 및 분석을 위해 BI 도구와 통합됩니다.\n\n<div class=\"content-ad\"></div>\n\n다른 도구들과의 통합:\n\n1. Apache HBase: 저지연성 저장 및 검색을 위해 사용됩니다.\n\n2. Apache Spark: 빠른 인메모리 데이터 처리를 위해 사용됩니다.\n\n3. Apache Pig: 복잡한 데이터 변환을 위한 스크립팅 플랫폼입니다.\n\n<div class=\"content-ad\"></div>\n\n4. Apache Flume: 데이터 수집을위한 도구입니다.\n\n5. Apache Sqoop: Hadoop과 관계형 데이터베이스 간의 데이터 전송을 위한 도구입니다.\n\n하이브의 주요 도전 과제\n\n아파치 하이브는 대용량 데이터 처리를위한 강력한 도구이지만 사용자가 하이브를 사용할 때 마주치는 일련의 도전 과제가 있습니다. 하이브를 사용하면 사용자가 마주칠 수있는 주요 문제 몇 가지는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n1. 성능 이슈\n\na) 지연 시간: Hive는 배치 처리를 위해 설계되어 실시간 쿼리 요구에 대한 높은 지연 시간을 야기할 수 있습니다. 낮은 지연 시간 응용 프로그램에 적합하지 않을 수 있습니다.\n\nb) 쿼리 최적화: Hive 쿼리의 성능을 최적화하는 것은 복잡할 수 있으며 파티션, 버킷 및 인덱싱을 신중하게 설계해야 합니다.\n\nc) 소량 파일 문제: Hive는 소량의 많은 파일을 처리하는 데 성능이 저하될 수 있습니다. 대규모 데이터 세트를 효율적으로 처리하려면 소량의 파일을 큰 파일로 통합해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n2. 스키마 진화의 복잡성\n\n가) 스키마 변경: Hive에서 테이블 스키마를 변경하는 일은 번거로울 수 있습니다. 열을 추가하거나 삭제하거나 데이터 유형을 수정하고 메타데이터를 업데이트하는 등의 변경 사항은 호환성 문제를 야기할 수 있으며 신중한 관리가 필요합니다.\n\n나) 역호환성: 스키마를 변경할 때 역호환성을 보장하는 것은 특히 데이터 모델이 진화하는 환경에서 도전적일 수 있습니다.\n\n3. 자원 관리\n\n<div class=\"content-ad\"></div>\n\na) **리소스 경합:** 공유 Hadoop 클러스터에서 Hive 작업과 다른 Hadoop 워크로드(예: Spark 또는 HBase) 간의 리소스 경합은 성능 저하로 이어질 수 있습니다.\n\nb) **YARN 구성:** Hive 쿼리에 대한 최적의 자원 할당을 위해 YARN을 적절하게 구성하는 것은 복잡할 수 있고 근본적인 인프라를 깊이 이해해야 합니다.\n\n4. **디버깅 및 오류 처리**\n\na) **복잡한 로그:** Hive 쿼리의 디버깅은 종종 복잡하고 매우 상세한 로그를 통해 이루어지며, 이는 문제점을 정확히 파악하기 어렵게 만듭니다.\n\n<div class=\"content-ad\"></div>\n\nb) **에러 메시지**: Hive 및 Hadoop 구성 요소에서 발생하는 에러 메시지는 알아보기 어려울 수 있고 문제를 해결하는 방법에 대한 명확한 안내를 제공하지 않을 수 있습니다.\n\n5. 실시간 처리 능력의 제한\n\na) 실시간 데이터 처리: Hive의 배치 지향적인 특성 때문에 실시간 데이터 처리 및 저지연 애플리케이션에는 부적합할 수 있습니다. Apache Kafka나 Apache Flink와 같은 대안들이 실시간 처리에 더 적합합니다.\n\n6. Hadoop 생태계에 대한 의존\n\n<div class=\"content-ad\"></div>\n\na) 하둡 의존성: 하이브가 하둡 생태계와 긴밀히 통합되어 있기 때문에 하둡의 제한사항과 복잡성을 상속받습니다. 하둡 구성 요소를 업그레이드하거나 변경하면 하이브의 성능과 안정성에 영향을 줄 수 있습니다.\n\nb) 호환성 문제: 서로 다른 버전의 하이브, 하둡 및 다른 생태계 도구 간의 호환성을 유지하는 것은 어려울 수 있습니다.\n\n7. ETL 워크플로의 복잡성\n\na) ETL 파이프라인 복잡성: 하이브에서 복잡한 ETL 파이프라인을 설계하고 유지하는 것은 데이터 변환 및 정리 작업으로 인해 특히 어려울 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nETL에서의 오류 처리: ETL 워크플로우에서 견고한 오류 처리 및 복구 메커니즘을 보장하려면 추가적인 노력이 필요합니다.\n\n실제 프로젝트에서 작업할 때 고려해야 할 사항:\n\n1. Hive 환경 설정\n\na) 클러스터 설정: Hadoop 클러스터가 올바르게 설정되고 구성되었는지 확인해야 합니다. Hive는 HDFS 및 YARN과 같은 Hadoop 구성 요소에 의존합니다.\n\n<div class=\"content-ad\"></div>\n\nb) Hive 설치: 클러스터 노드에 Hive를 설치하세요. 간단한 HiveQL 쿼리를 실행하여 설치를 확인해보세요.\n\nc) 메타스토어 구성: Hive 메타스토어를 MySQL 또는 PostgreSQL과 같은 신뢰할 수 있는 RDBMS로 구성하세요. 이렇게 하면 성능과 신뢰성이 향상됩니다.\n\n2. 데이터 적재 및 저장\n\na) 데이터 로딩: LOAD DATA 명령을 사용하여 데이터를 Hive 테이블로 로드하세요. 또는 Apache Sqoop과 같은 도구를 사용하여 관계형 데이터베이스로부터 데이터를 가져올 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\nb) 파티셔닝 및 버킷팅: 데이터 스키마를 계획하여 파티션과 버킷을 효과적으로 활용하세요. 이렇게 하면 스캔해야 하는 데이터 양을 줄여 쿼리 성능을 크게 향상시킬 수 있습니다.\n\nc) 데이터 포맷: 사용 사례에 따라 적합한 데이터 포맷(ORC, Parquet)을 선택하세요. ORC와 Parquet은 읽기 중심 워크로드에 대해 더 나은 압축률과 성능을 제공합니다.\n\n3. 효율적인 쿼리 작성\n\na) 일찍 필터링: 가능한 한 쿼리에서 필터를 가장 빨리 적용하여 처리해야 하는 데이터 양을 최소화하세요.\n\n<div class=\"content-ad\"></div>\n\nb) 적절한 조인 사용: 데이터셋의 크기에 따라 적절한 조인 유형(맵 사이드 조인, 브로드캐스트 조인)을 선택하세요.\n\nc) 전체 테이블 스캔 피하기: 전체 테이블 스캔을 피하려면 파티션 및 인덱스를 사용하세요.\n\nd) 결과 집합 제한: 개발 및 테스트 중에 특히 결과 행 수를 제한하는 LIMIT를 사용하세요.\n\n4. 성능 튜닝\n\n<div class=\"content-ad\"></div>\n\na) 벡터화: 한 번에 여러 행을 처리할 수 있도록 벡터화를 활성화하여 쿼리 성능을 향상시킵니다.\n\nb) 비용 기반 최적화(CBO): Hive가 가장 효율적인 쿼리 실행 계획을 선택할 수 있도록 CBO가 활성화되어 있는지 확인합니다.\n\nc) 병렬 실행: 리듀서 및 맵 작업의 수를 조정하여 병렬성을 높입니다.\n\nd) 자원 할당: YARN 및 Tez/Spark를 구성하여 Hive 작업에 충분한 자원(메모리, CPU)을 할당합니다.\n\n<div class=\"content-ad\"></div>\n\n5. 오류 처리 및 디버깅\n\na) 로그: 자세한 오류 메시지를 확인하려면 하둡 잡 트래커, HiveServer2 및 YARN의 로그를 확인하세요.\n\nb) 실행 계획 설명: 쿼리의 실행 계획을 이해하고 잠재적 병목 현상을 식별하려면 EXPLAIN을 사용하세요.\n\nc) 세션 변수: 디버깅을 위해 쿼리 실행 설정을 사용자 정의하려면 Hive 세션 변수(set 명령)를 사용하세요.\n\n<div class=\"content-ad\"></div>\n\n6. 보안 및 접근 제어\n\na) 인증: Hadoop 클러스터에서 안전한 인증을 위해 Kerberos를 구성합니다.\n\nb) 권한 부여: 세밀한 접근 제어 및 정책을 관리하기 위해 Apache Ranger 또는 Sentry를 사용하세요.\n\nc) 암호화: 데이터가 휴식 중이든 이동 중이든 보안 표준을 준수하기 위해 데이터를 암호화해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n7. 자동화 및 일정 설정\n\na) 워크플로우 자동화: Apache Oozie 또는 Apache Airflow를 사용하여 Hive 워크플로 및 ETL 프로세스를 자동화합니다.\n\nb) 일정 설정: 규칙적인 데이터 로드 및 ETL 작업을 일정에 맞춰 설정하여 데이터가 항상 최신 상태임을 보장합니다.\n\n8. 모니터링 및 유지보수\n\n<div class=\"content-ad\"></div>\n\na) 모니터링 도구: 클러스터 건강 상태와 Hive 작업 성능을 추적하기 위해 Ambari, Cloudera Manager 또는 Grafana와 같은 모니터링 도구를 사용하세요.\n\nb) 메타데이터 관리: Hive 메타스토어 데이터베이스를 정기적으로 백업하고 오래된 메타데이터를 정리하여 성능 문제를 방지하세요.\n\nc) 업그레이드 및 패치: 최신 패치 및 업그레이드로 Hive 및 Hadoop 구성 요소를 업데이트하여 더 나은 성능과 보안을 제공하세요.\n\n9. 다른 도구들과 통합\n\n<div class=\"content-ad\"></div>\n\na) 비즈니스 인텔리전스 (BI): 하이브를 Tableau, Power BI 또는 Apache Superset과 연결하여 데이터 시각화 및 보고를 할 수 있습니다.\n\nb) 데이터 사이언스: 하이브를 주피터 노트북이나 Apache Zeppelin과 같은 데이터 과학 플랫폼과 함께 사용하여 데이터 분석 및 기계 학습을 할 수 있습니다.\n\n10. 문서화와 모범 사례\n\na) 문서화: 하이브 설정에 대한 상세한 문서를 유지하며, 구성 세부 정보, 스키마 설계 및 ETL 워크플로우를 포함합니다.\n\n<div class=\"content-ad\"></div>\n\nb) Best Practices: Hive 쿼리 최적화, 데이터 모델링 및 리소스 관리에 대한 최상의 사례를 따르면 효율적이고 신뢰할 수 있는 작업이 보장됩니다.\n\nHIVE COMMANDS:\n\n![Hive Commands 1](/assets/img/2024-06-23-FromBasicstoAdvancedNavigatingApacheHiveforBigDataProfessionals_1.png)\n\n![Hive Commands 2](/assets/img/2024-06-23-FromBasicstoAdvancedNavigatingApacheHiveforBigDataProfessionals_2.png)\n\n<div class=\"content-ad\"></div>\n\n**다른 도구들과의 비교:**\n\n1. Hive vs. Pig: Apache Pig은 하둡에서 실행되며 ETL 작업에 적합하지만 Pig Latin이라는 절차적 스크립트 언어를 사용하며 SQL에 익숙한 사용자들에게는 직관적이지 않을 수 있습니다.\n\n2. Hive vs. HBase: HBase는 대규모 데이터셋에 대한 저레이턴시 실시간 읽기/쓰기 접근에 적합한 NoSQL 데이터베이스입니다. 반면 Hive는 배치 처리 및 분석 쿼리에 더 적합합니다.\n\n3. Hive vs. Spark SQL: Spark SQL은 인메모리 처리를 제공하여 특정 워크로드에 대해 더 빠른 쿼리 성능을 제공할 수 있습니다. 그러나 Hive는 더욱 성숙하며 하둡 생태계와 더 깊게 통합되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n테이블 태그를 마크다운 형식으로 변경하시면 됩니다.","ogImage":{"url":"/assets/img/2024-06-23-FromBasicstoAdvancedNavigatingApacheHiveforBigDataProfessionals_0.png"},"coverImage":"/assets/img/2024-06-23-FromBasicstoAdvancedNavigatingApacheHiveforBigDataProfessionals_0.png","tag":["Tech"],"readingTime":10},{"title":"PySpark 완벽 해설 explode와 collect_list 함수 사용 방법","description":"","date":"2024-06-23 16:24","slug":"2024-06-23-PySparkExplainedTheexplodeandcollect_listFunctions","content":"\n\nPySpark SQL은 Apache PySpark의 SQL을 위한 파이썬 인터페이스로, 데이터 변환 및 분석을 위한 강력한 도구 모음입니다. 데이터베이스 SQL 시스템에서 사용 가능한 가장 일반적인 유형의 작업을 모방하도록 만들어졌으며, Spark에서 사용 가능한 데이터프레임 패러다임을 활용하여 추가 기능을 제공할 수 있습니다.\n\n간단히 말하면, PySpark SQL은 개발자가 데이터를 효율적으로 조작하고 처리할 수 있는 다양한 기능을 제공합니다.\n\n이러한 기능 중에서, 특히 유용한 두 가지 함수는 데이터를 고유한 방식으로 변환하고 집계하는 능력으로 인해 주목할 만합니다. 이것들은 explode 및 collect_list 연산자입니다.\n\n본 문서에서는 이러한 각각의 기능이 정확히 무엇을 하는지 설명하고, 각각에 대한 사용 사례와 샘플 PySpark 코드를 보여드리겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## Explode\n\nPySpark SQL의 `explode` 함수는 배열이나 맵과 같은 중첩 데이터 구조를 변환하고 평평하게 만드는 다목적 도구입니다. 이 함수는 중첩된 컬렉션을 포함하는 복잡한 데이터 세트를 다룰 때 특히 유용합니다. 중첩된 구조 내의 개별 요소를 분석하고 조작할 수 있도록 해줍니다.\n\nPyspark의 배열은 다른 컴퓨터 언어의 배열과 매우 유사합니다. 주로 동일한 유형의 요소를 특정 순서로 저장하는 데이터 구조로, 보통 연속적인 메모리 위치에 저장됩니다.\n\nSpark의 맵은 파이썬과 같은 언어의 사전과 동등합니다. 특정 키에 대한 값을 매우 빠르게 조회할 수 있는 키-값 쌍의 시리즈를 보유합니다. 나중에 `explode`를 사용하는 예제를 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n어레이 컬럼에 적용되는 explode 함수는 어레이의 각 요소에 대해 새로운 행을 생성하고 해당 요소 값을 새 열에 저장합니다. 기본적으로 이 새 열은 col이라는 이름으로 지정되지만 별칭을 사용하여 사용자 정의 열 이름을 지정할 수 있습니다.\n\n마찬가지로, 맵 컬럼에 적용되는 explode 함수는 두 개의 새로운 열을 생성합니다: 키를 위한 열과 값을 위한 열. 기본적으로 이 열은 각각 key와 value로 지정되지만 다시 한 번 별칭을 사용하여 사용자 정의 열 이름을 제공할 수 있습니다.\n\n## Collect_list\n\nPySpark SQL의 collect_list 함수는 컬럼에서 값을 모아서 배열로 변환하는 집계 함수입니다. 기존에 다른 PySpark SQL 함수들을 사용하여 평탄화되거나 변환된 데이터를 재구성하거나 집계해야 하는 경우에 특히 유용합니다. 많은 면에서, 이 함수는 explode에 대한 보완 함수로 생각될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 함수는 데이터를 그룹화하기 위해 하나 이상의 열을 기반으로 한 그룹화 연산자(groupBy operator)와 함께 자주 사용됩니다.\n\n## 무료 Pyspark 개발 환경에 액세스하는 방법\n\n이 글의 코드를 따라하려면 PySpark 개발 환경에 액세스해야 합니다.\n\n귀하의 직장을 통해, 클라우드를 통해 또는 로컬 설치를 통해 PySpark에 액세스할 수 있다면 행운이겠죠. 그렇지 않은 경우, 아래 링크를 확인하여 Databricks Community Edition이라는 훌륭한 무료 온라인 PySpark 개발 환경에 액세스할 수 있는 방법에 대해 자세히 살펴보세요.\n\n<div class=\"content-ad\"></div>\n\nDatabricks는 Apache Spark를 중심으로 구축된 데이터 엔지니어링, 머신 러닝 및 분석을 위한 클라우드 기반 플랫폼으로, 대용량 데이터 워크로드를 처리하기 위한 통합 환경을 제공합니다. Databricks의 설립자들은 Spark를 만들었으므로 이 분야에 대해 정통하다는 것을 알 수 있습니다.\n\n## 사용 사례 예시\n\n이제 explode 및 collect_list의 동작에 대해 조금 더 알게 되었으니, 이러한 함수들의 사용 사례를 살펴보겠습니다.\n\nexplode 함수\n\n<div class=\"content-ad\"></div>\n\nexplode 함수를 사용하여 배열을 변환하는 방법으로 시작하겠습니다. Spark에서 배열은 동일한 유형의 요소를 고정 크기의 순차적 컬렉션으로 저장하는 데이터 구조입니다.\n\n한 텍스트 열에 사람들의 이름을, 또 다른 배열 열에는 그들이 좋아하는 과일을 담은 PySpark 데이터프레임을 설정할 것입니다.\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode\n\n# 스파크 세션 초기화\nspark = SparkSession.builder.appName(\"ArrayExplodeExample\").getOrCreate()\n\n# 배열 열 \"fruits\"를 포함하는 DataFrame 생성\ndata = [\n    (\"John\", [\"apple\", \"banana\", \"cherry\"]),\n    (\"Mary\", [\"orange\", \"grape\"]),\n    (\"Jane\", [\"strawberry\", \"blueberry\", \"raspberry\"]),\n    (\"Mark\", [\"watermelon\"])\n]\n\n# 스키마 정의 및 DataFrame 생성\ndf = spark.createDataFrame(data, [\"name\", \"fruits\"])\n\n# 원래 DataFrame 표시\ndf.show(truncate=False)\n```\n\n이 데이터를 분석하는 많은 경우에서 \"name\"과 \"fruit\"의 각 기능 조합이 별도의 레코드로 존재하는 것이 훨씬 편리합니다. 이를 위해 explode 함수를 사용하여 이를 달성할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 배열 열을 펼치기 위해 explode 함수를 사용하세요\nexploded_df = df.withColumn(\"fruit\", explode(df.fruits))\n\n# 펼쳐진 DataFrame을 보여주세요\nexploded_df[\"name\",\"fruit\"].show(truncate=False)\n\n\n\n+----+----------+\n|name|fruit     |\n+----+----------+\n|John|apple     |\n|John|banana    |\n|John|cherry    |\n|Mary|orange    |\n|Mary|grape     |\n|Jane|strawberry|\n|Jane|blueberry |\n|Jane|raspberry |\n|Mark|watermelon|\n+----+----------+\n```\n\n이제 데이터는 일반적인 데이터테이블과 비슷하게 보이며, 추가 데이터프레임이나 SQL 작업을 수행하여 추가 분석을 수행할 경우 더 잘 정리되어 있습니다.\n\nPyspark에서 맵을 다루는 explode 사용법은 매우 유사합니다.\n\n```js\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, create_map, lit, col\nfrom pyspark.sql.types import MapType, StringType\n\n# Spark 세션 초기화\nspark = SparkSession.builder.appName(\"ExplodeExample\").getOrCreate()\n\n# 샘플 데이터\ndata = [\n    (\"Tom\", {\"Salary\": \"£5000\", \"Bonus\": \"£0\"}),\n    (\"Dick\", {\"Salary\": \"£2690\", \"Bonus\": None}),\n    (\"Harry\", {\"Salary\": \"£45000\", \"Bonus\": \"£20000\"})\n]\n\n# DataFrame 생성\ndf = spark.createDataFrame(data, [\"Name\", \"Remuneration\"])\n\n# 원본 DataFrame 보기\ndf.show(truncate=False)\n\n\n\n+-------+-----------------------------------+\n|Name   |Remuneration                       |\n+-------+-----------------------------------+\n|Tom    |{Salary -> £5000, Bonus -> £0}     |\n|Dick   |{Salary -> £2690, Bonus -> null}   |\n|Harry  |{Salary -> £45000, Bonus -> £20000}|\n+-------+-----------------------------------+\n```\n\n<div class=\"content-ad\"></div>\n\nexplode 함수를 적용하면 키-값 쌍이 개별 레코드로 분할됩니다. 이전 예제와 마찬가지로, 이렇게 하면 더 나은 구성이 가능하여 추가 분석이 용이해집니다.\n\n```js\nremuneration_exploded = df.select(\n    col(\"Name\"),\n    explode(col(\"Remuneration\")).alias(\"key\", \"value\")\n)\n\n# 변환된 DataFrame 표시\nremuneration_exploded.show(truncate=False)\n\n\n+-------+------+-------+\n| Name  | key  | value |\n+-------+------+-------+\n|Tom    |Salary|£5000  |\n|Tom    |Bonus |   £0  |\n|Dick   |Salary|£2690  |\n|Dick   |Bonus | null  |\n|Harry  |Salary|£45000 |\n|Harry  |Bonus |£20000 |\n+-------+------+-------+\n```\n\n조금 더 복잡한 예제로 마무리해 보겠습니다. 다음과 같은 PySpark 데이터프레임이 있다고 가정해 봅시다.\n\n```js\n+----+-----------+-----------+\n|col1|     col2  |     col3  |\n+----+-----------+-----------+\n| a  | [1, 2, 3] | [4, 5, 6] |\n+----+-----------+-----------+\n```\n\n<div class=\"content-ad\"></div>\n\n그리고 우리는 다음과 같은 출력을 얻고 싶습니다.\n\n```js\n+------+-----+-------+\n|col1  |col2  |col3  |\n+------+------+------+\n|   a  |   1  |   4  |\n|   a  |   2  |   5  |\n|   a  |   3  |   6  |\n+------+------+------+\n```\n\n이 작업은 생각보다 어렵습니다. 먼저, 입력 테스트 데이터를 생성해 봅시다.\n\n```js\ntestData = [('a',[1,2,3],[4,5,6]),]\n\ndf = spark.createDataFrame(data=testData, schema=['col1','col2','col3'])\n```\n\n<div class=\"content-ad\"></div>\n\n일단 보면 col2와 col3를 각각 터뜨릴 수 있다고 생각할 수 있지만, 여러분은 각 번 열을 번 열 씩만 터뜨릴 수 있다는 것이기 때문에 그렇게 하지 못할 거라고요. 한 번 해보세요, 그러면 제가 무슨 말을 하는지 이해하실 거에요.\n\n```js\ndf.select(\"col1\", explode(\"col2\").alias(\"col2\"), \"col3\").select(\"col1\",\"col2\", explode(\"col3\").alias(\"col3\")).show()\n```\n\n```js\n+----+----+----+\n|col1|col2|col3|\n+----+----+----+\n|   a|   1|   4|\n|   a|   1|   5|\n|   a|   1|   6|\n|   a|   2|   4|\n|   a|   2|   5|\n|   a|   2|   6|\n|   a|   3|   4|\n|   a|   3|   5|\n|   a|   3|   6|\n+----+----+----+\n```\n\n흠, 우리가 원하는 것과는 조금 다르네요. 원하는 대로 하려면, PySpark 배열의 경우 Python의 zip 연산과 동일한 작업을 수행하는 중간 단계가 필요합니다. Python zip 연산자는 두 가지 반복 가능한(iterable) 항목을 묶어줍니다. 예를 들어, 만약 우리가 아래와 같은 것이 있었다면,\n\n<div class=\"content-ad\"></div>\n\n```js\nnumbers = [1, 2, 3]\n\nletters = ['a', 'b', 'c']\n\nzipped = zip(numbers, letters)\n\nprint(list(zipped))\n\n[(1, 'a'), (2, 'b'), (3, 'c')]\n```\n\nArrays의 동등한 명령어는 arrays_zip이라고 부르기 쉽습니다. 그래서 먼저 그것을 사용하여 배열을 \"짜맞추고\", 그런 다음 폭파해야 합니다. Pyspark SQL이나 데이터프레임 작업을 사용하여 구현할 수 있습니다. 다음은 SQL에서의 해결 방법입니다.\n\n```js\nfrom pyspark.sql.functions import *\n\n# 입력 데이터의 데이터베이스 테이블 만들기\n\ndf.createOrReplaceTempView(\"test_table\")\n\nspark.sql(\"select col1, tmp.col2, tmp.col3 from (select col1, explode(tmp) as tmp from (select col1, arrays_zip(col2, col3) as tmp from test_table))\").show()\n\n+------+------+------+\n| col1 | col2 | col3 |\n+------+------+------+\n|   a  |   1  |   4  |\n|   a  |   2  |   5  |\n|   a  |   3  |   6  |\n+------+------+------+\n```\n\ncollect_list 함수\n\n<div class=\"content-ad\"></div>\n\ncollect_list 함수는 PySpark 데이터프레임 데이터를 레코드 단위로 저장하고 해당 데이터의 컬럼을 컬렉션으로 반환합니다. 이런 면에서, explode 함수의 반대 역할을 합니다. 간단한 예시로 설명드리겠습니다. 만약 다음과 같은 입력 데이터셋이 있다고 가정해봅시다.\n\n```js\ntestData = (['a'],['b'],['c']) \n\ndf = spark.createDataFrame(data=testData, schema = ['letter_column']) \n\ndf.printSchema() \n\ndf.show() \n\n+-------------+ \n|letter_column| \n+-------------+ \n|            a| \n|            b| \n|            c| \n+-------------+ \n```\n\n데이터에 collect_list를 적용한 결과는 다음과 같습니다.\n\n```js\nfrom pyspark.sql.functions import collect_list\n\ndf.select(collect_list(\"letter_column\").alias(\"letter_row\")).show()\n\n+----------+  \n|letter_row|  \n+----------+  \n| [a, b, c]| \n+----------+ \n```\n\n<div class=\"content-ad\"></div>\n\n보통은 데이터의 한 열만 다루는 것이 아니기 때문에, 좀 더 복잡한 문제를 고려해보면 다음과 같은 PySpark 데이터프레임이 있는 경우가 있습니다. 이는 3일 동안의 가스 및 전기의 도매 가격을 보여주는 데이터입니다.\n\n```js\n|연료       |      날짜| 가격| \n|-----------|----------|------| \n|Gas        |2019-10-11|121.56| \n|Gas        |2019-10-10|120.56| \n|Electricity|2019-10-11|100.00| \n|Gas        |2019-10-12|119.56| \n|Electricity|2019-10-10| 99.00| \n|Electricity|2019-10-12|101.00| \n```\n\n다음 형식의 새로운 데이터 세트를 반환하려고 합니다. 중요한 점은 각 연료의 가격이 왼쪽에서 오른쪽으로 날짜 순서대로 나열되어야 한다는 것입니다.\n\n```js\n|연료       |Price_hist              |\n|-----------|------------------------|\n|Electricity|[99.0, 100.0, 101.0]    |\n|Gas        |[120.56, 121.56, 119.56]|\n```\n\n<div class=\"content-ad\"></div>\n\n우리는 입력 데이터 세트를 만들기 위한 코드를 작성할 것입니다.\n\n```js\ndata = [\n    (\"가스\", \"2019-10-11\", 121.56),\n    (\"가스\", \"2019-10-10\", 120.56),\n    (\"전기\", \"2019-10-11\", 100.00),\n    (\"가스\", \"2019-10-12\", 119.56),\n    (\"전기\", \"2019-10-10\", 99.00),\n    (\"전기\", \"2019-10-12\", 101.00)\n]\n\n# DataFrame 생성\ndf = spark.createDataFrame(data, [\"연료\", \"날짜\", \"가격\"])\n\n# DataFrame 표시\ndf.show()\n```\n\n이제 코드를 실행하면,\n\n```js\nfrom pyspark.sql.functions import collect_list\n\ndf.select(\"연료\", collect_list(\"가격\").alias(\"가격 이력\")).show(truncate=False)\n```\n\n<div class=\"content-ad\"></div>\n\n위의 코드를 번역하였습니다.\n\n```js\n...\n...\nAnalysisException: [MISSING_GROUP_BY] 쿼리에 GROUP BY 절이 포함되어 있지 않습니다. GROUP BY를 추가하거나 OVER 절을 사용하여 윈도우 함수로 변환하십시오.;\nAggregate [Fuel#2, collect_list(Price#4, 0, 0) AS Price Hist#22]\n+- LogicalRDD [Fuel#2, Date#3, Price#4], false\n```\n\n이 부분은 좋지 않아요. 명확히, 연료 이름에 대해 어떤 종류의 그룹화를 수행해야 합니다. 운이 좋게도, collect_list 함수는 실제로 집계 함수이므로 원하는 결과를 얻기 위해 데이터프레임에서 사전 정렬 작업과 함께 agg 및 groupBy 작업을 사용할 수 있습니다. 다음을 실행하면 원하는 결과를 얻을 수 있습니다.\n\n```js\nfrom pyspark.sql.functions import collect_list\n\n# 날짜 순으로 정렬하여 가격을 날짜 순으로 정렬합니다\nsorted_df = df.sort(\"Fuel\", \"Date\")\n\n# 연료별로 그룹화하여 가격을 목록으로 수집합니다\nresult_df = sorted_df.groupBy(\"Fuel\").agg(collect_list(\"Price\").alias(\"Price_hist\"))\n\n# 결과 DataFrame 표시\nresult_df.show(truncate=False)\n```\n\n<div class=\"content-ad\"></div>\n\n요구하는 출력은 다음과 같습니다.\n\n\n| Fuel       | Price_hist              |\n|------------|-------------------------|\n| Electricity| [99.0, 100.0, 101.0]    |\n| Gas        | [120.56, 121.56, 119.56]|\n\n\n## 요약\n\n본 문서에서는 PySpark SQL의 더 이상하고 유용한 데이터 조작 함수 두 가지를 소개했으며, 그들이 귀하에게 어떻게 가치있는지에 대한 사용 사례도 제시했습니다.\n\n<div class=\"content-ad\"></div>\n\n만약 데이터프레임에서 배열이나 사전 데이터 필드를 변환하여 별도 레코드로 넣어야 할 경우 explode 함수를 사용하세요.\n\ncollect_list 함수는 explode 함수의 반대로 생각할 수 있습니다. 이를 사용하여 개별 데이터프레임 레코드에서 항목을 컬렉션으로 집계하세요.\n\n이 내용이 마음에 들었다면, 아래 글들도 흥미롭게 보실 수 있을 것 같아요.","ogImage":{"url":"/assets/img/2024-06-23-PySparkExplainedTheexplodeandcollect_listFunctions_0.png"},"coverImage":"/assets/img/2024-06-23-PySparkExplainedTheexplodeandcollect_listFunctions_0.png","tag":["Tech"],"readingTime":11},{"title":"데이터 프로젝트의 코드 우수성을 위한 4가지 R 1부","description":"","date":"2024-06-23 16:22","slug":"2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1","content":"\n\n![이미지](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_0.png)\n\n# 소개\n\n어떤 데이터 과학 프로젝트도 성공에 중요한 키패드 요소는 고품질의 코드입니다. 단순한 데이터 분석부터 복잡한 머신 러닝 파이프라인에 이르기까지, 코드 품질은 프로젝트의 정확성, 효율성 및 유지 보수성을 보장하기 위해 항상 최우선 사항입니다. 잘 쓰여진 코드는 작업이 다른 사람(포함하여 미래의 본인)에 의해 쉽게 이해되고 수정 및 확장될 수 있도록 합니다. 버그와 오류의 가능성을 최소화하고 데이터 및 머신 러닝 프로젝트를 효율적이고 효과적이며 견고하게 만들어줍니다. 그러나 고품질의 코드를 작성하는 것은 항상 쉬운 일이 아닙니다.\n\n우리 모두는 낮은 품질의 코드를 본 적이 있습니다. 그리고 제가 '본다'고 말할 때, 실제로는 '쓴다'는 것을 의미합니다!\n\n<div class=\"content-ad\"></div>\n\n알고 계시죠: 빠른 분석과 증명 개념 모델링 연습에 도전하셨습니다. 그래서 데이터 세트를 CSV 파일로 덤프하고 노트북을 열어서 두 번 실행하면 오류를 발생시키는 암호화된 42개 셀을 만들었어요. 그 결과, 알수 없는 함수 이름, 덮어쓴 변수, 해독할 수 없는 차트, 결국 자신의 머리가 폭발하는지 EC2 인스턴스의 메모리가 폭발하는지 혼란의 소용돌이 속에 빠지게 되었어요.\n\n하지만 물론, 훌륭한 POC 모델은 충분히 작동하기 때문에 어디에 있게 될까요? 바로 프로덕션 환경이죠!\n\n저주받은 일이 생기면, 항상 그렇듯이, 몇 달 후에 다시 자신의 작업을 돌아보면서 정확히 무엇을 했는지 그리고 첫 번째로 어떻게 작동했는지 이해하려고 애를 쓰게 되는 상황이죠.\n\n네, 우리 모두 그런 경험을 해봤지만, 더 이상 그럴 필요가 없어요!\n\n<div class=\"content-ad\"></div>\n\n이 다부분 선언에서는 데이터 프로젝트를 위한 탁월한 코드를 작성하는 데 도움이 되는 4가지 개념(우연히도 모두 'R'로 시작)을 안내하겠습니다. 이 네 가지 R에 기반한 코드베이스를 구축하여 머신러닝 파이프라인 및 정신 건강을 모두 지킬 수 있기를 희망합니다!\n\n참고: 더 간단하게 하기 위해, 이 글의 범위는 데이터 프로젝트를 위한 파이썬 코드 개발로 한정되었지만, 일반적인 개념은 다른 언어로 확장 가능해야 합니다.\n\n# 첫 번째 R: 가독성\n\n![image](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_1.png)\n\n<div class=\"content-ad\"></div>\n\n남이 읽을 걸 감안하고 코드를 작성하세요.\n\n아니, 더 나은 한 가지를 해볼게요:\n\n남이 읽을 걸 염두에 두고 코드를 작성하세요. 좋은 코드라면, 결국 누군가가 읽을 거에요.\n\n가독성은 다른 사람들(미래의 나 포함)이 코드를 읽는 시점과 장소에 관계없이 쉽게 이해하고 해석할 수 있어야 한다는 걸 의미해요.\n\n<div class=\"content-ad\"></div>\n\n이것은 명확한 구조, 의미 있는 변수 이름, 객체 간 간단하고 논리적인 관계, 그리고 충분한 주석과 문서화가 포함됩니다. 가독성은 간단한 이유로 고품질 코드의 가장 중요한 측면입니다:\n\n코드는 쓰여지는 횟수보다 읽히는 횟수가 더 많습니다. 오늘 쓰고 있는 코드를 읽을 사람은 아마도 여러분들 중 어느 누군가가 여섯 달 뒤에 새로운 기능을 추가하거나 코드베이스에서 버그를 추적할 때 일 것입니다. 가독성을 우선시함으로써 여러분의 코드가 다른 사람들이 이해하기 쉬워지는데 그치지 않고, 장래의 여러분들이 정확히 무엇을 했는지 기억하려고 소중한 시간을 낭비하지 않아도 된다는 점에서 장래의 여러분의 시간을 아낄 수 있습니다.\n\n우리 모두가 조금 더 나은 가독성의 코드를 작성하는 데 시간을 투자한다면, 서로의 코드를 읽는 동안 머리를 긁는 시간을 줄일 수 있습니다. 따라서 높은 기준을 유지하고 동료에게 모범되는 행동을 보인다면, 개선된 효율성, 명확한 커뮤니케이션, 더 나은 협업을 통해 모두가 혜택을 받을 수 있습니다. 저는 개인적으로 대략 72%의 시간을 깨끗하고 가독성 있는 코드를 작성합니다. 여러분들은 제게 비해서 더 나은 결과를 얻을 수 있습니다!\n\n가독성 있는 코드를 작성하는 실용적인 팁은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n## 코딩에 들어가기 전에 잠깐 멈춰보세요\n\n![Image](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_2.png)\n\n코드를 작성하기 전에 생각하는 시간을 가지세요. 떠오르는 첫 번째 해결책이 문제에 대한 가장 좋은 해결책은 아닐 가능성이 높습니다. 올바른 알고리즘을 사용하고 계십니까? 가장 효율적인 데이터 구조를 사용하고 있나요? 필요 없는 중복 변수를 사용하고 있진 않나요? 코드로 작성하기 전에 단순한 영어로 논리를 설명할 수 있나요?\n\n문제를 개념화하지 않고 코드를 작성하면, 대부분 부적절하거나 완전히 잘못된 것을 작성하게 될 것입니다. 그러면 나중에 반드시 되돌아가서 수정해야 할 것입니다. 하지만 코드를 처음부터 다시 쓸 필요는 없습니다. 아니요, 아니요, 필요한 이유가 있기 때문에 이미 이러한 잘못된 해결책을 만드는 데 수십 시간을 투자했으니까요. 그러므로 처음부터 다시 시작하는 대신에 이제 작동하지 않는 솔루션에 수정을 강요하고, 그 외에 대비하기 위해 어떤 패치를 상위에 올려야 할지 고려해야 합니다. 그리고 그것이 아무도 39.5피트 막대로 만지기 싫어하는 나쁜 코드를 만들어내는 완벽한 요리 비법입니다.\n\n<div class=\"content-ad\"></div>\n\n시간을 내어 생각해보세요. 무엇을 할 것인지 고려해보세요. 주변을 한번 두루 둘러보세요. 그리고 모두가 분명해지면, 뛰어들 준비가 돼 있나요?!\n\n## 명확하고 명시적인 변수와 함수 이름을 선택하세요\n\n![이미지](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_3.png)\n\n이번에 대해 내가 말할 것을 이미 알고 있으니 말할 필요는 없어요. 제목이 모두 설명해주고 있으니까요. 왜냐하면 매우 명확하고 명시적이기 때문이죠!\n\n<div class=\"content-ad\"></div>\n\n안타깝게도, 저는 대부분의 경우 코드를 작성할 때 이렇게 작성하게 됩니다. 특히 급하게 할 때는요.\n\n```js\n# 나쁜 코드를 보여드리는 중\ndf = pd.read_csv('customer_data.csv')\ndf_2 = df[df['age'] > 30]\n```\n\n하지만 여러분은 저처럼 하지 마세요. 객체와 변수(이 경우 판다스 데이터프레임)에 의미 있는 이름을 선택하세요.\n\n모든 데이터프레임을 df라고 부르는 것은 쉽지만 좋은 생각은 아닙니다. 이 예제에서는 데이터프레임 df에 어떤 데이터가 있고 df_2가 무엇을 나타내는지 즉시 알 수 없습니다. 조금 더 나아지도록 해보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n\n# 이 코드는 약간 더 나은 코드입니다\ncustomer_data = pd.read_csv('customer_data.csv')\ncustomer_over_30 = customer_data[customer_data['age'] > 30]\n\n\n이 예제에서는 고객 데이터를 포함하는 원본 데이터프레임의 이름이 더 명확하고, 30세 이상인 고객을 참조하는 두 번째 데이터프레임인 customer_over_30이 더 명확합니다. 훨씬 나아 보이죠? \n\n하지만 과도하게 하는 것은 중요하지 않습니다. 변수 이름을 customers_over_30_active_today_purchased_under_100처럼 길게 지으면 안 됩니다. 이름은 이유당창기술상개발자의 에코메모리에 쉽게 들어맞을 수 있을만큼 짧아야 합니다. 코드를 읽고 변수를 for 루프와 if 문을 통해 추적할 다른 개발자는 빨리 잊지 않을 겁니다. 이름이 모호하면 추적해야 할 것이 무엇인지 곧 잊어버릴 것입니다. 이름이 너무 길 경우 그것이 무엇을 나타내는지 오용하거나 혼란스러울 수 있습니다.\n\n## PEP 8 코딩 스타일을 따르세요\n\n\n<div class=\"content-ad\"></div>\n\n\n![2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_4.png](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_4.png)\n\nPEP 8은 이미 알지 못하는 경우를 대비한 Python 코드의 공식 스타일 가이드입니다. 이것은 Python 코드의 가독성을 최대화하기 위한 서식 규칙 세트로, 네이밍 규칙, 공백 사용 여부, 모듈 가져오는 방법 및 순서 등이 포함됩니다.\n\n이 코드 단편은 PEP8 스타일 가이드를 따르지 않습니다.\n\n```python\n# 이 코드는 좋지 않아\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport pandas as pd\nfrom sklearn.model_selection import *\n\ndef loadData(file_path):\n    return pd.read_csv(file_path)\ndef splitData(df):\n    return train_test_split(df.drop('target',1),df['target'])\ndef train_Model(X_train,Y_train):\n    return RandomForestClassifier().fit(X_train,Y_train)\ndf = loadData('some_data.csv')\n\nX_train, X_test, Y_train, Y_test = splitData(df)\nmodel = trainModel(X_train, Y_train)\n```\n\n<div class=\"content-ad\"></div>\n\n여기 많은 주황색 깃발이 있어요! 함수 이름이 snake_case로 되어 있지 않아요. 쉼표 뒤나 연산자 주변에 공백이 없어요. 함수들 사이에는 충분한 공백이나 너무 많은 공백이 있어서 코드를 읽기 어려워요. 전반적으로 말하면, 이 코드는 보기 싫은 코드에요.\n\n여기 PEP 8을 따르는 개선된 버전이 있어요.\n\n```js\n# 이 코드가 더 좋아요\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef load_data(file_path):\n    return pd.read_csv(file_path)\n\n\ndef split_data(df):\n    return train_test_split(df.drop('target', axis=1), df['target'])\n\n\ndef train_model(features_train, target_train):\n    return RandomForestClassifier().fit(features_train, target_train)\n\n\ndf = load_data('some_data.csv')\nfeatures_train, features_test, target_train, target_test = split_data(df)\nmodel = train_model(features_train, target_train)\r\n```\n\n함수 이름은 snake_case로 되어 있고, 쉼표 뒤와 연산자 주변에 공백이 있어요. 함수들 사이에는 두 줄의 공백이 있어요. 추가로 변수 이름이 약간 더 유의미하도록 변경되었어요.\n\n<div class=\"content-ad\"></div>\n\nPEP 8 스타일을 따르는 코드를 보장해주는 서식 지원 도구가 있습니다(린팅 및 서식 지정 부분을 참조하세요), 하지만 처음부터 깔끔하고 가독성이 좋은 코드를 작성하기 위한 가장 중요한 규칙을 알고 있어야 합니다.\n\n## 가능한 한 코드에 내부 문서를 추가하세요\n\n![image](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_5.png)\n\n깔끔한 코드는 문서의 자리 표시자가 아닙니다. 항상 코드에 두 가지 유형의 내부 문서를 추가해야 합니다: (1) 코드가 의도한 작업, (2) 코드를 사용하는 방법. 파이썬에서는 일반적으로 인라인 주석과 독스트링의 조합으로 제공됩니다.\n\n<div class=\"content-ad\"></div>\n\n여기는 잘 문서화된 코드 예시입니다:\n\n```js\n# 이 코드가 훨씬 좋아졌어요\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n```\n\n```js\ndef load_data(file_path):\n    \"\"\"\n    csv 파일에서 데이터를 불러옵니다.\n\n    Parameters:\n        file_path (str): csv 파일의 경로.\n\n    Returns:\n        DataFrame: 불러온 데이터를 pandas DataFrame으로 반환합니다.\n    \"\"\"\n    \n    # pandas의 read_csv 함수를 사용하여 csv 파일에서 데이터를 불러옵니다.\n    loaded_data = pd.read_csv(file_path)\n    \n    return loaded_data\n\n\ndef split_data(df, test_size=0.2):\n    \"\"\"\n    데이터를 훈련 및 테스트 세트로 나누는 함수입니다.\n\n    Parameters:\n        df (DataFrame): 나눌 데이터.\n        test_size (float): 테스트 세트의 비율.\n\n    Returns:\n        X_train, X_test, y_train, y_test (DataFrame, DataFrame, Series, Series): \n            훈련 및 테스트 데이터가 특징과 대상으로 분할됩니다.\n    \"\"\"\n    \n    # sklearn의 train_test_split 함수를 사용하여 데이터를 \n    # 훈련 및 테스트 세트로 나눕니다.\n    X_train, X_test, y_train, y_test = \\\n      train_test_split(df.drop('target', axis=1), \n                       df['target'], \n                       test_size=test_size, \n                       random_state=42,\n                       )\n    \n    return X_train, X_test, y_train, y_test\n\n\ndef train_model(features_train, target_train):\n    \"\"\"\n    RandomForestClassifier 모델을 훈련시키는 함수입니다.\n\n    Parameters:\n        features_train (DataFrame): 훈련 특징.\n        target_train (Series): 훈련 대상.\n\n    Returns:\n        RandomForestClassifier: 훈련된 모델.\n    \"\"\"\n    \n    # RandomForestClassifier를 인스턴스화하고 훈련 데이터에 맞춥니다.\n    model = RandomForestClassifier().fit(features_train, target_train)\n    \n    return model\n\n\n# csv 파일에서 데이터를 불러옵니다.\ndf = load_data('some_data.csv')\n\n# 불러온 데이터를 훈련 및 테스트 세트로 나눕니다.\nfeatures_train, features_test, target_train, target_test = split_data(df)\n\n# 훈련 데이터로 RandomForestClassifier 모델을 훈련합니다.\nmodel = train_model(features_train, target_train)\n```\n\n직업 생활을 하면서 본 뛰어난 품질의 코드들은 대개 자세한 설명과 함께 함수 또는 클래스의 목적, 매개변수 및 반환 값에 대한 문서화 문자열(docstring)이 포함되어 있습니다(예시가 포함되어 있으면 더 좋습니다!). 그러나 코드 자체에 인라인 주석이 부족하다는 점에 항상 실망합니다. 이러한 주석은 특정 코드 라인의 의도를 명확히 하기에 중요하며, 코드베이스를 더 이해하기 쉽고 유지보수하기 용이하며, 무엇보다도 확장 가능하게 만들어 줍니다. 인라인 주석을 쓰는 데 너무 많다고 두려워하지 마세요. 코드를 인라인 문서화로 설명하는 데 공간이 너무 많을 순 없습니다.\n\n<div class=\"content-ad\"></div>\n\n좋은 실천 방법은 당신이 이루고자 하는 로직을 설명하는 한 줄씩 주석을 작성하고 이를 가짜 코드로 취급한 후 실제 코드가 이어지도록 하는 것일 수 있습니다. 예를 들어:\n\n```js\n# 초기화된 결과를 저장할 빈 리스트를 만듭니다\nresults = []\n\n# 0부터 10까지의 범위를 반복합니다\nfor i in range(10):\n    # 숫자가 짝수인지 확인합니다\n    if i % 2 == 0:\n        # 숫자가 짝수인 경우 결과 리스트에 추가합니다\n        results.append(i)\n\nprint(results)\n# 출력: [0, 2, 4, 6, 8]\n```\n\n여기서 실수를 발견했나요?\n\n0부터 10까지 반복하려 했지만 결과는 8까지만 보입니다. 무슨 일이 일어났을까요?\n\n<div class=\"content-ad\"></div>\n\n오, 저희는 10을 포함하지 않는 range(10)을 사용했네요. 어이가 없네요!\n\n코드는 괜찮아요. 매우 가독성이 좋아요. 그 안에 명백한 오류가 없어요. 유일한 문제는 프로그래머가 그것을 수행하려고 의도한 대로 동작하지 않는다는 점이에요.\n\n여기서 문제의 심각성은 분명히 지나치게 과장되었지만, 이 간단한 예제를 사용하여 한 가지 사실을 설명하려고 해요.\n\n코드 라인은 프로그램이 하는 일을 보여줍니다. 인라인 코멘트는 프로그래머가 그것을 수행하려고 의도한 것을 보여줍니다. 의도와 현실 간에 불일치가 있는 경우 인라인 코멘트를 통해 코드의 맨 끝으로 스크롤하는 것보다 그 간극을 식별하고 깨다리는 데 훨씬 빨리 도와줄 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n코드에 주석과 문서를 작성하는 것은 항상 부담스러울 수 있습니다. 특히 이미 깨끗한 코드를 작성하는 데 많은 노력을 기울이고 있을 때 더 그렇습니다. 코드가 매우 가독성이 높고 이해하기 쉽다면, 무슨 일을 하는지 설명하는 주석을 작성할 필요가 없다고 주장할 수 있습니다. 간단한 경우에는 사실이 될 수 있지만, 다소 복잡한 알고리즘을 구현할 때는 문제가 생길 수 있습니다. 깨끗한 코드와 깨끗한 문서는 서로 다른 목적을 위해 필요하며 놀라운 데이터 프로젝트를 위해 둘 다 필요합니다.\n\n참고: 여기서 말씀드리는 것은 사용자 및 개발자가 코드 일부를 이해하는 데 도움이 되도록 함수 또는 클래스 수준에서 제공해야 하는 최소한의 내부 문서에 대한 것입니다. 이는 (가능한 경우) 모듈/패키지 수준의 (잠재적으로 외부) 문서와는 별개이며 사용자에게 어떻게 연결되는지에 대해 높은 수준에서 설명하는 데 필요한 것입니다.\n\n## 다른 사람에게 코드를 읽어보라고 요청하세요 (가능하다면)\n\n![이미지](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_6.png)\n\n<div class=\"content-ad\"></div>\n\n코드가 가독성이 좋은지 알아보는 가장 좋은 방법은요... 맞죠, 당신이 예상한 대로: 그것을 읽어보는 겁니다!\n\n일반적으로 협업 중에는 코드 리뷰를 통해 이루어집니다(GitHub 및 Bitbucket의 풀 리퀘스트 또는 GitLab의 병합 리퀘스트를 통해). 여기서의 도전 과제는 사람들이 일반적으로 코드 리뷰를 하는 걸 좋아하지 않는다는 점입니다. 솔직히 말해서, 코드 리뷰는 지루할 수 있어요. 하지만, 코드 리뷰는 큰 선배 개발자들에 의한 코드 기여를 검토할 때 학습 경험이 될 수도 있고, 주니어 개발자들이 작성한 코드를 검토할 때 가르침의 기회가 될 수도 있어요.\n\n코드를 검토할 때는 긍정적이고 지지적인 태도를 가지는 것이 정말 중요해요. 여러분의 코멘트가 그것을 읽는 사람에게 영향을 줄 수 있다는 걸 명심하고, 가혹한 비판보다 건설적인 피드백을 제공하려 노력해 보세요. 기억하세요, 여러분은 개발자가 아니라 코드를 검토하는 중이에요.\n\n반면에 코드 리뷰 중에 피드백을 받을 때는 목표가 개인적으로 비판하는 게 아니라 코드의 품질을 향상시키는 데 있다는 것을 염두에 두세요. 자존심을 버리세요. 여러분과 여러분의 코드 둘 다 완벽하지 않다는 걸 기억하세요. 따라서 다른 시각을 수용할 수 있도록 열려 있으며, 이를 통해 훨씬 더 나은 해결책으로 이끌 수 있어요. 또한, 여러분의 시간과 피드백에 감사를 표하는 것을 잊지 말고 항상 감사해하세요. 그들은 자신들의 시간을 더 즐거운 일에 할애할 수도 있지만, 여러분과 여러분의 코드의 품질에 전념하기로 결정했기 때문이에요.\n\n<div class=\"content-ad\"></div>\n\n## Linting 및 형식 지정 도구 사용\n\n<img src=\"/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_7.png\" />\n\nLinting 및 형식 지정은 코드를 더 읽기 쉽게 만드는 데 매우 도움이 될 수 있습니다 (또한 견고성을 높입니다). 그러나 이들은 정확히 무엇일까요?\n\nLinting은 코드를 분석하고 잠재적인 오류와 불일치를 식별하는 프로그램을 실행하는 프로세스입니다. 일반적으로 사용자 정의할 수 있는 일정한 코드 품질 수준을 유지하는 데 도움을 줍니다. 예를 들어, numpy를 가져왔지만 실제로 사용하지 않는 경우, 린터는 이를 감지하고 가져오기 문을 제거할 수 있도록 알려줍니다. 이는 잠재적인 오류와 \"음성\" 버그를 피하는 데 도움이 됩니다. 예를 들어 변수를 None과 비교할 때 x == None 대신 x is None을 사용하지 않아 코드가 실패하는 상황이 있었습니다. 코드가 왜 나를 싫어하는지 두 날 동안 골머리를 썼었죠. Linting은 그런 머리 아픔을 방지할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n포맷팅은 코드를 읽기 쉽고 세련되게 만드는 데 관련된 것이에요. 이는 코드를 특정한 미리 정의된 규칙이나 표준에 따라 조직화하거나 조정하는 것을 포함해요. 예를 들어, 슈퍼 멋진 데이터 처리 모듈을 작성했지만 보기에 정말 추한 느낌이었고, 다른 사람들과 공유하기가 조금 부끄러웠던 적이 있나요?\n\n코드 포맷팅은 그런 부분을 다루는 데 도움이 될 수 있어요.여러분의 코드가 깔끔하게 정리되면 좋은 점이죠.\n\n```js\n# 데이터 처리 함수의 극악적인 가상 예시...\n# 집에서 시도하지 마세요!\ndef process_data(\n    data_source,\n    filter_function,\n    transform_function,\n    group_function,\n    aggregate_function,\n    normalize_function,\n    save_function,\n    destination,\n):\n    data = load_data(data_source)\n    \n    data = filter_function(data)\n    data = transform_function(data)\n    data = group_function(data)\n    data = aggregate_function(data)\n    data = normalize_function(data)\n    save_function(data, destination)\n```\n\n코드의 형식을 잘 맞춰주면 이렇게 변할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n인기 있는 코드 린터인 pylint와 flake8, 그리고 코드 가독성을 향상시키는 Black 같은 포매터 등이 있습니다. 저는 개인적으로 linting과 formatting을 위해 Ruff를 사용합니다. Ruff는 빠르고 매우 유연하여 코드 가독성을 유지하는 데 좋은 도구로 CI/CD 파이프라인에 추가할 수 있습니다. Ruff를 사용하는 방법에 대해 더 알고 싶다면 댓글에서 알려주세요. 나중에 이에 대한 글을 쓸지도 모르겠어요.\n\n## 지겹은 일은 AI 비서에게 맡기세요\n\n![AI assistants](/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_8.png)\n\nAI 비서를 활용하면 수고스러운 코딩 작업을 자동화하고 동시에 코드를 더 가독성 있게 만들 수 있습니다. 아니, ChatGPT를 사용하여 코드를 작성하는 게 아닙니다. 간단한 유틸리티 함수나 판다스 데이터 작업이 필요하다면 AI에 맡기세요. 그렇게 하고 새로운 시간을 활용하여 푸어 오버 커피 한 잔을 즐기세요.\n\n<div class=\"content-ad\"></div>\n\nAI를 사용하여 보일러플레이트 코드를 많이 작성하고 인라인 주석을 작성하며 코드에 대한 문서를 생성하는 것에 대해 이야기하고 있어요. GitHub Copilot과 같은 AI 어시스턴트는 타이핑하는 시간을 많이 절약할 수 있어요. 솔직히 말하면, 작성할 코드 중 많은 부분이 반드시 창의적이지 않아요. 이는 보일러플레이트 코드, 루틴 작업 및 반복 요소를 포함하고 있어요. 특히 유틸리티 함수에 대한 독스트링과 간단한 단위 테스트에 대해 이는 특히 사실이에요. Copilot은 이러한 일상적인 작업을 빠르게 처리할 수 있어요. 그래서 코드의 복잡하고 창의적인 측면에 더 많은 시간을 쏟을 수 있게 되고, 다른 사람 개발자들에게도 논리가 이해하기 쉽고 따라가기 쉽도록 만들어주게 되요.\n\n지금까지 Copilot을 사용하면서, 내가 의도한 대로 복잡한 논리가 들어간 탄탄한 코드를 작성하는 데 어려움이 있어요(아니면 내가 그냥 나쁜 프롬프트 엔지니어인지 모르겠어요). 그러나 인라인 주석, 독스트링 및 보일러플레이트 코드에 대한 자동 완성을 제안하는 데 놀랍도록 훌륭한 일을 합니다. 그래서 늘 뇌를 아끼지는 못하지만, 확실히 키보드 작업 시간을 많이 아낄 수 있고 — 보너스로 코드를 더 읽기 쉽게 만들어줍니다.\n\n어시스턴트로 사용하되, 그것을 지팡이로 만들지 않도록 주의하세요. 코드 작성을 위해 AI에 너무 의존하는 것에 대해 신중해지세요. 왜냐하면 어떤 근육이든 꾸준히 사용하지 않으면 그 근육은 분명히 약화되기 마련이기 때문이에요.\n\n저는 데이터 프로젝트의 코드 훌륭성을 위한 네 가지 R 중 첫 번째인 코드 가독성에 집중한 이 부분이 유용하고 실용적이었으면 좋겠어요. 여기서 공유한 팁들이 더 나은 코드를 작성하고 더 견고한 머신러닝 파이프라인을 구축하는 데 도움이 되기를 바랍니다.\n\n<div class=\"content-ad\"></div>\n\n지금 두 번째 R이 무엇을 의미하는지 추측할 수 있나요?\n\n다음 파트도 계속 기대해주세요!\n\n향후 게시글을 업데이트하려면 Medium에서 저를 팔로우해주십시오. 이 시리즈의 두 번째 부분 또한 포함될 예정입니다. 이 글에 대한 여러분의 생각을 듣고 싶으니 아래나 옆에 댓글을 남겨주세요. 또는 여러분의 장치에서 댓글 섹션이 있는 곳에 남겨주세요. 추가로 논의하고 싶은 질문이나 사항이 있으면 LinkedIn에서 연락주세요. 여러분을 만나 기뻐할 것입니다!\n\n기억하세요, 학습의 여정은 길고 계속됩니다. 기술을 능숙하게 유지하고 지식을 최신 상태로 유지하며 시야를 넓혀가세요. 더 나은 코드를 작성하고 독특하며 탁월한 데이터 및 머신러닝 프로젝트를 구축하는 데 건배해봅시다! 🍻","ogImage":{"url":"/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_0.png"},"coverImage":"/assets/img/2024-06-23-TheFourRsofCodeExcellenceforDataProjectsPart1_0.png","tag":["Tech"],"readingTime":14},{"title":"Fabric 완벽 마스터하기 실시간 주식 데이터 스트리밍 및 분석 방법","description":"","date":"2024-06-23 16:20","slug":"2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis","content":"\n\n\n![2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_0.png](/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_0.png)\n\n실시간 주식 시장 데이터를 관리하고 분석하기 위해 Fabric 이벤트 스트림을 사용하세요. 시간, 심볼, 가격 및 거래량과 같은 필드를 포함하는 이 데이터는 실시간 이벤트를 시뮬레이션하고 KQL 데이터베이스를 사용하여 분석하는 데 사용됩니다.\n\n## Fabrics에서 사용되는 구성 요소:\n\n- Fabric Event Stream\n- Azure Data Explorer (KQL Database)\n- Power BI\n- KQL Query\n\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_1.png)\n\n## 실시간 주식 데이터 설정 및 분석 단계\n\n- 워크스페이스 생성:\n\n1. Microsoft Fabric에 로그인: Microsoft Fabric에 로그인하고 Power BI를 선택합니다.\n2. 워크스페이스 생성: 메뉴 바에서 Workspaces를 선택하고 자신이 원하는 이름으로 새 워크스페이스를 만들어 Fabric 용량(Trial, Premium, 또는 Fabric)이 있는지 확인하세요.\n\n<div class=\"content-ad\"></div>\n\n2. 실시간 인텔리전스 기능 활성화: 파워 BI 포털의 왼쪽 하단에 있는 파워 BI 아이콘을 선택하고 실시간 인텔리전스 경험으로 전환하세요.\n\n3. KQL 데이터베이스 생성:\n\n- 이름을 선택하고 새 데이터베이스를 생성하세요.\n- OneLake 활성화: 데이터베이스 세부 정보 패널에서 OneLake에서 사용 가능하도록 설정하세요.\n\n![이미지](/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_2.png)\n\n<div class=\"content-ad\"></div>\n\n4. 이벤트스트림 만들기: 리얼타임 인텔리전스 메뉴에서 이벤트스트림(미리보기)를 선택하고 이름을 지정하세요. Microsoft Fabric의 이벤트스트림은 코드를 필요로하지 않고 실시간 이벤트를 캡처하고 변환하여 다양한 대상으로 라우팅합니다.\n\n5. 샘플 데이터 소스 추가:\n\n- 이벤트스트림 캔버스에서 새 출처를 추가하고 샘플 데이터를 선택하세요.\n- 샘플 데이터 소스의 이름을 지정하고 필요한 값으로 구성하세요.\n- 구성을 적용하려면 이벤트스트림을 게시하세요.\n\n![이미지](/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_3.png)\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_4.png\" />\n\n6. **KQL Database를 Eventstream 대상으로 설정하기**\n\n- **대상 추가**: Eventstream 캔버스에서 대상 추가를 선택하고 KQL Database를 선택합니다.\n- **대상 구성**: KQL Database에 대한 세부 정보를 입력하고 데이터 삽입 모드를 선택하고 대상의 이름을 지정한 후 구성을 저장합니다.\n\n7. **이벤트 변환**: Eventstream 캔버스에서 그룹화와 같은 변환 이벤트를 추가하고 필요에 맞게 구성합니다. 해당 이벤트를 이벤트 스트림에 연결하고 변경 사항을 발행합니다.\n\n<div class=\"content-ad\"></div>\n\n## KQL을 사용하여 데이터 분석하기\n\nKusto Query Language (KQL)은 KQL 데이터베이스에서 데이터를 조회하는 데 사용됩니다. 주식 데이터를 분석하는 몇 가지 예시 쿼리가 있습니다:\n\n- 평균 매수-매도 스프레드\nStockData_Table\n| extend bidPrice = todouble(bidPrice), askPrice = todouble(askPrice)\n| extend BidAskSpread = askPrice — bidPrice\n| summarize AvgBidAskSpread = round(avg(BidAskSpread),2) by symbol\n- 평균 거래량\nStockData_Table\n| extend volume = todouble(volume)\n| summarize AvgVolume = round(avg(volume),2) by symbol\n- 가격 변동률\nStockData_Table\n| extend lastSalePrice = todouble(lastSalePrice)\n| order by symbol, ['time'] asc\n| extend PrevPrice = prev(lastSalePrice, 1)\n| where isnotnull(PrevPrice)\n| extend PriceChangePercent = ((lastSalePrice — PrevPrice) / PrevPrice) * 100\n| summarize AvgPriceChangePercent = round(avg(PriceChangePercent),2) by symbol\n- 시장 점유율 백분율\nStockData_Table\n| extend marketPercent = todouble(marketPercent)\n| summarize AvgMarketSharePercent = avg(marketPercent) by symbol\n\n<img src=\"/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_5.png\" />\n\n<div class=\"content-ad\"></div>\n\n## 시각화를 위한 Power BI 활용\n\n- KQL 데이터베이스에 연결: Power BI를 사용하여 KQL 데이터베이스에 연결하고 분석된 데이터를 기반으로 시각화를 생성하세요.\n- 대시보드 생성: 평균 입찰-요청 스프레드, 평균 거래량, 시장 점유율 등 핵심 성과 지표(KPI)를 시각화하기 위한 대화형 대시보드를 구성하세요.\n\n본 프로젝트는 Microsoft Fabric의 Eventstream을 활용하여 실시간 데이터 스트리밍, 분석, 시각화를 실현하며 KQL 및 Power BI를 사용하여 주식 시장 데이터에 대한 귀중한 통찰력을 제공합니다.","ogImage":{"url":"/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_0.png"},"coverImage":"/assets/img/2024-06-23-MasteringFabricReal-TimeStockDataStreamingandAnalysis_0.png","tag":["Tech"],"readingTime":4},{"title":"데이터 아키텍처 간단 개요","description":"","date":"2024-06-23 16:18","slug":"2024-06-23-DataArchitectureABriefOverview","content":"\n\n데이터 아키텍처는 성공적인 데이터 엔지니어링의 중요한 요소입니다. 조직 전반에서 데이터 수집, 저장, 처리 및 활용 방식의 기초를 제공합니다. 잘 설계된 데이터 아키텍처를 통해 기업은 원활한 데이터 통합, 높은 성능, 확장성 및 효과적인 데이터 거버넌스를 달성할 수 있습니다.\n\n이 블로그 포스트에서는 우수한 데이터 아키텍처의 핵심 원칙을 탐구하고 핵심 아키텍처 개념을 논의하며 견고하고 확장 가능한 데이터 시스템을 설계하는 데 도움이 되는 다양한 데이터 아키텍처 패턴을 살펴볼 것입니다.\n\n# 데이터 아키텍처란 무엇인가요?\n\n데이터 아키텍처는 조직 내에서 데이터가 수집, 저장, 관리되고 사용되는 방식을 개요로 설명하는 구조화된 프레임워크를 의미합니다. 데이터 흐름, 데이터 모델 및 데이터 처리에 사용되는 기술을 정의합니다. 견고한 데이터 아키텍처는 데이터가 다양한 비즈니스 요구에 대해 접근 가능하고 신뢰할 수 있으며 관련성이 있도록 보장하여 효율적인 의사 결정과 운영 효율성을 촉진합니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 아키텍처는 기업 아키텍처의 하위 집합으로, 조직의 전체 IT 인프라 및 프로세스를 포괄합니다. 기업 아키텍처에는 비즈니스, 애플리케이션, 기술, 데이터 아키텍처와 같은 다양한 구성 요소가 포함됩니다. 기업 아키텍처의 더 넓은 맥락을 이해하면 데이터 아키텍처가 조직의 전체 전략 내에서 어떤 역할과 중요성을 발휘하는지를 이해할 수 있습니다.\n\n\\[이미지는 여기에 들어가는데, 이미지의 경로가 이상해서 제대로 표시되지 않았어요!]\n\n# 데이터 아키텍처의 진화\n\n과거 몇 10년 동안 데이터 아키텍처는 효율적인 데이터 분석의 필요성 증가와 데이터를 비즈니스 이익으로 활용하기 위한 데이터 전략의 현대화로 인해 상당한 변화를 겪었습니다. 이러한 진화는 각각이 특정 아키텍처 관행과 기술로 특징 지어지는 여러 중요한 단계로 나타납니다.\n\n<div class=\"content-ad\"></div>\n\n## 2000년 이전: 기업 데이터 웨어하우스 (EDW) 시대\n\n이 기간 동안 주목된 것은 기업 데이터 웨어하우스 (EDW)의 성공과 구현이었습니다. 조직은 다양한 소스로부터 데이터를 저장, 관리 및 분석할 수 있는 중앙 저장소를 생성하기 위해 노력했습니다. 주요 목표는 구조화되고 일관된 데이터 저장을 통해 비즈니스 인텔리전스 및 보고 요구를 지원하는 것이었습니다.\n\n특징:\n\n- 중앙 데이터 저장\n- 정의된 스키마를 가진 구조화된 데이터\n- 비즈니스 인텔리전스 및 보고에 초점을 맞춤\n\n<div class=\"content-ad\"></div>\n\n한계:\n\n- 시행 및 유지 관리 비용 증가\n- 증가하는 데이터 양으로 인한 확장성 문제\n- 다양한 데이터 원본 통합의 어려움\n\n## 2000년부터 2010년까지: EDW 후기\n\n이 기간에는 전통적인 데이터 웨어하우스의 한계에 대한 대처 방향으로의 변화가 있었습니다. 조직들은 분산된 데이터와 데이터 실로에 대한 도전에 직면하며, 일관성 없고 조각난 분석을 유발했습니다. 데이터 마트가 인기를 얻게 되었는데, 이는 부서가 자체 데이터를 관리할 수 있게 해 주었지만, 더 많은 분열을 유발했습니다.\n\n<div class=\"content-ad\"></div>\n\n특징:\n\n- 데이터 웨어하우스 및 데이터 마트에 의존\n- 조각난 데이터 분석\n- 데이터 사일로의 등장\n\n한계:\n\n- 부서간 일관되지 않은 데이터 분석\n- 기업 전체 통찰력을 위한 데이터 통합의 어려움\n- 여러 데이터 시스템 유지비용 증가\n\n<div class=\"content-ad\"></div>\n\n## 2010년부터 2020년까지: 논리 데이터 웨어하우스 (LDW) 시대\n\n논리 데이터 웨어하우스(LDW)는 단편화된 데이터 환경에 대한 해결책으로 등장했습니다. LDW는 데이터 웨어하우스, 데이터 마트 및 데이터 레이크를 포함한 다양한 저장 시스템에서의 데이터 접근을 통합하는 공통 의미 계층을 도입했습니다. 이 접근 방식은 더 통합된 데이터 분석과 향상된 데이터 접근성을 가능케 했습니다.\n\n주요 특징:\n\n- 공통 의미 계층을 통한 통합된 데이터 접근\n- 데이터 웨어하우스, 데이터 마트 및 데이터 레이크의 통합\n- 향상된 데이터 분석 기능\n\n<div class=\"content-ad\"></div>\n\n혜택:\n\n- 데이터 일관성 및 통합이 향상됩니다.\n- 확장성과 유연성이 향상됩니다.\n- 대규모 데이터 및 고급 분석에 대한 지원이 좋아집니다.\n\n제한 사항:\n\n- 여러 데이터 저장 시스템에 대한 계속된 의존이 필요합니다.\n- Semantic 레이어 관리의 복잡성이 증가합니다.\n- LDW를 구현하고 유지하는 데 전문 기술이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n## 2020년부터의 시대: 증강 데이터 분석과 활성 메타데이터의 시대\n\n현재와 미래의 데이터 아키텍처 단계는 인공 지능, 기계 학습 및 데이터 조정과 같은 첨단 기술에 의해 주도되는 증강 데이터 분석의 부상으로 특징 지어집니다. 이 시대는 데이터 접근의 민주화와 셀프 서비스 분석을 가능하게 하는 활성 메타데이터를 기반으로 합니다.\n\n특징:\n\n- AI와 기계 학습을 활용한 증강 데이터 분석\n- 메타데이터를 활용한 데이터 셀프 서비스\n- 고급 데이터 조정 및 추천 엔진\n- 적응적인 실천과 활성 메타데이터 분석\n\n<div class=\"content-ad\"></div>\n\n혜택:\n\n- 향상된 데이터 접근성 및 셀프 서비스 기능\n- 고급 분석을 통한 의사 결정 개선\n- 변화하는 비즈니스 요구에 대한 적응 능력 향상\n\n활성 메타데이터: 메타데이터는 오늘날 핵심 역할을 담당하며 데이터의 다양한 측면을 설명하고 지능적인 데이터 관리를 가능케 합니다. 네 가지 유형의 메타데이터가 있습니다:\n\n- 기술 메타데이터: 데이터 저장, 구조 및 처리에 관한 정보.\n- 운영 메타데이터: 데이터 워크플로우, 프로세스 및 사용에 대한 데이터.\n- 비즈니스 메타데이터: 데이터의 비즈니스 의미와 사용에 대한 맥락 정보.\n- 소셜 메타데이터: 데이터 상호작용 및 사용 패턴에서 유도된 인사이트.\n\n<div class=\"content-ad\"></div>\n\n활성 메타데이터는 데이터를 설명하는 것뿐만 아니라 시스템 간 작업을 식별하여 더 동적이고 지능적인 데이터 작업을 용이하게 합니다.\n\n사용 사례:\n\n- 마스터 데이터 관리\n- 기업 간 데이터 교환\n- 애플리케이션 데이터 통합\n- 파트너 데이터 교환\n\n활성 메타데이터와 증강 분석으로 발전하는 것은 데이터 아키텍처의 능력을 전통적인 방식을 넘어서게 하여 다양한 사용 사례에 걸쳐 더 정교한 데이터 관리와 분석을 가능케 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 주요 아키텍처 원칙과 개념\n\n핵심 아키텍처 원칙과 개념을 이해하는 것은 효과적인 데이터 아키텍처를 설계하는 데 중요합니다. 이러한 원칙들은 견고하고 확장 가능하며 유연한 데이터 시스템을 구축하는 데 기반을 제공합니다.\n\n## 주요 기업의 원칙\n\n주요 기업들은 자체 아키텍처 원칙을 따라 자신들의 시스템의 효과성과 효율성을 보장합니다. 예를 들어:\n\n<div class=\"content-ad\"></div>\n\nAWS 웰 아키텍처 프레임워크에는 여섯 가지 핵심이 있습니다:\n\n- 운영 우수성\n- 보안\n- 신뢰성\n- 성능 효율성\n- 비용 최적화\n- 지속 가능성\n\n구글 클라우드의 클라우드 네이티브 아키텍처를 위한 다섯 가지 원칙은 다음과 같습니다:\n\n- 자동화를 위한 디자인\n- 상태 관리에 똑똑하게\n- 관리형 서비스 우선\n- 깊이 우선 방어 실천\n- 항상 아키텍처 구축중임.\n\n<div class=\"content-ad\"></div>\n\n이 프레임워크와 \"데이터 엔지니어링 기본\"과 같은 다른 리소스에서 영감을 받아, 여기에는 좋은 데이터 아키텍처를 설계하는 데 필수적인 몇 가지 조직화된 원칙과 개념이 있습니다.\n\n## 도메인과 서비스\n\n도메인은 데이터 아키텍처가 지원하는 특정 영역의 비즈니스 또는 지식입니다. 서비스는 도메인 내에서 특정 작업을 수행하는 기능입니다.\n\n혜택:\n\n<div class=\"content-ad\"></div>\n\n- 조직적 명확성: 명확히 정의된 도메인과 서비스는 데이터와 프로세스를 조직화하는 데 도움이 됩니다.\n- 집중된 개발: 팀은 특정 서비스에 집중하여 효율성과 전문성을 향상시킬 수 있습니다.\n\n## 분산 시스템\n\n분산 시스템은 서로 다른 네트워크 컴퓨터에 위치한 구성 요소가 통신하고 조정하여 공통 목표를 달성하는 시스템입니다. 확장 가능하고 강인한 데이터 아키텍처를 구축하는 데 필수적입니다.\n\n혜택:\n\n<div class=\"content-ad\"></div>\n\n- 확장성: 더 많은 노드를 추가하여 부하 처리 용이.\n- 신뢰성: 중복된 노드는 일부 노드가 실패해도 시스템 가용성을 보장합니다.\n\n## 확장성과 탄력성\n\n확장성은 시스템이 자원을 추가함으로써 증가하는 작업 양을 처리하는 능력입니다. 탄력성은 수요에 따라 자원을 동적으로 확장 또는 축소하는 능력입니다.\n\n혜택:\n\n<div class=\"content-ad\"></div>\n\n- 비용 효율성: 사용한 리소스에 대해서만 지불하세요.\n- 성능 최적화: 피크 시간에는 성능을 유지하고 낮은 사용량 기간에는 비용을 최적화하세요.\n\n## 가용성 및 신뢰성\n\n가용성은 시스템이 운영 및 접근 가능한 시간의 백분율입니다. 신뢰성은 시스템이 시간이 지남에 따라 올바르고 일관되게 작동하는 능력입니다.\n\n주요 지표:\n\n<div class=\"content-ad\"></div>\n\n- 가동 시간: 시스템이 작동 중인 시간의 백분율.\n- 고장 사이의 평균 시간 (MTBF): 시스템 고장 사이의 평균 시간.\n- 복구에 소요되는 평균 시간 (MTTR): 고장 난 시스템을 수리하는 데 걸리는 평균 시간.\n\n전략:\n\n- 중복성: 고장 시 대체 시스템을 구현.\n- 장애 조치 기구: 고장 시 대기 시스템으로 자동 전환.\n\n## 이벤트 주도형 아키텍처\n\n<div class=\"content-ad\"></div>\n\n이벤트 주도 아키텍처는 시스템이 이벤트나 상태 변경에 대응하는 설계 패러다임입니다. 이 접근 방식은 높은 결합도를 갖추고, 유연성과 확장성을 촉진합니다.\n\n활용 사례:\n\n- 실시간 분석\n- 사물 인터넷 시스템\n- 통지 시스템\n\n## 사용자 액세스: 단일 vs. 멀티테넌트\n\n<div class=\"content-ad\"></div>\n\nSingle-tenant 아키텍처는 각 고객에게 소프트웨어 및 데이터베이스의 전용 인스턴스를 할당하며, 멀티테넌트 아키텍처는 다중 고객 사이에서 리소스를 공유합니다.\n\n고려해야 할 사항:\n\n- 성능: Single-tenant는 더 나은 성능을 제공할 수 있지만, 멀티테넌트는 비용 효율적입니다.\n- 보안: 멀티테넌트 시스템은 다른 고객들을 위한 데이터 격리 및 보안을 보장해야 합니다.\n- 유지 보수: 멀티테넌트 시스템은 중앙에서 유지 및 업데이트하기 쉽습니다.\n\n이러한 원칙을 준수하고 이러한 개념을 이해하여 현대 비즈니스 환경의 요구 사항을 충족하면서 확장 가능하고 신뢰할 수 있으며 비용 효율적인 데이터 아키텍처를 설계할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 주요 데이터 아키텍처 패턴\n\n지난 수십 년간 데이터 아키텍처에서는 효과적으로 데이터를 관리하고 활용하기 위한 필수 요소로 여러 가지 핵심 패턴이 나타났습니다. 각 패턴에는 강점, 약점 및 이상적인 사용 사례가 있습니다. 여기에서 이러한 패턴을 간단히 살펴보고, 다양한 맥락에서 어떻게 적용할 수 있는지 설명하겠습니다.\n\n## 1. 데이터 웨어하우스\n\n데이터 웨어하우스는 구조화된 데이터를 저장하고 보고 및 분석하기 위해 설계된 중앙 저장소입니다. 쿼리 성능과 데이터 집계에 최적화되어 있어 비즈니스 인텔리전스 애플리케이션에 이상적입니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 해당 데이터 아키텍처에 대한 간략한 설명입니다:\n\n특징:\n\n- 중앙 집중 저장: 여러 출처에서 수집한 데이터를 한 곳에 저장합니다.\n- 구조화된 데이터: 데이터가 체계적으로 구성되어 일반적으로 스키마를 따릅니다.\n- 분석 최적화: 빠른 쿼리 및 데이터 검색을 위해 설계되었습니다.\n\n혜택:\n\n<div class=\"content-ad\"></div>\n\n- 높은 성능: 복잡한 쿼리와 데이터 분석에 최적화되어 있습니다.\n- 데이터 통합: 다양한 소스에서 데이터를 통합하여 통일된 뷰를 제공합니다.\n- 일관성: 스키마 강제화를 통해 데이터 품질과 일관성을 준수합니다.\n\n사용 사례:\n\n- 비즈니스 인텔리전스 및 보고\n- 과거 데이터 분석\n- 다중 시스템에서 데이터 통합\n\n## 2. 데이터 레이크\n\n<div class=\"content-ad\"></div>\n\nData lakes는 방대한 양의 원시, 비구조화 및 반구조화 데이터를 저장합니다. 데이터 웨어하우스와 달리 데이터 레이크는 데이터에 엄격한 스키마를 부과하지 않아 데이터 수집 및 저장에 더 많은 유연성을 제공합니다.\n\n![DataArchitectureABriefOverview_2.png](/assets/img/2024-06-23-DataArchitectureABriefOverview_2.png)\n\n특징:\n\n- 원시 데이터 저장: 데이터를 원시 형식으로 저장합니다.\n- 확장성: 대량의 데이터를 처리할 수 있습니다.\n- 스키마형 읽기: 데이터 저장 시가 아닌 데이터를 읽을 때 스키마가 적용됩니다.\n\n<div class=\"content-ad\"></div>\n\n장점:\n\n- 유연성: 구조화된, 비구조화된 및 반구조화된 데이터를 포함한 다양한 데이터 유형을 저장할 수 있습니다.\n- 확장성: 대규모 데이터 작업을 처리하기에 적합합니다.\n- 비용 효율성: 데이터 웨어하우스와 비교하여 대량의 데이터를 저장하는 데 비용이 저렴할 수 있습니다.\n\n도전:\n\n- 데이터 관리: 적절한 관리 없이 데이터 레이크는 데이터 스왐프가 될 수 있습니다.\n- 성능: 데이터 웨어하우스와 비교하여 쿼리 성능이 느릴 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n사용 사례:\n\n- 대규모 데이터 분석\n- 기계 학습 및 데이터 과학\n- 로그 및 이벤트 데이터 저장\n\n## 3. 현대 데이터 스택\n\n현대 데이터 스택은 데이터 통합, 변환, 저장 및 분석을 용이하게 하는 일련의 모듈식 클라우드 기반 도구의 집합을 가리킵니다. 이 방식은 기민성, 확장성 및 사용 편의성을 강조합니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 마크다운 형식으로 변경한 내용입니다.\n\n\n![Data Architecture Overview](/assets/img/2024-06-23-DataArchitectureABriefOverview_3.png)\n\nComponents:\n\n- Data Ingestion: Tools like Fivetran or Stitch for extracting and loading data.\n- Data Transformation: Tools like dbt (data build tool) for transforming data within the warehouse.\n- Data Storage: Cloud data warehouses like Snowflake or Google BigQuery.\n- Data Visualization: Tools like Looker, Tableau, or Mode for data exploration and visualization.\n\nBenefits:\n\n\n<div class=\"content-ad\"></div>\n\n- 모듈성: 쉽게 교체하거나 업그레이드할 수 있는 유연한 구성 요소입니다.\n- 확장성: 데이터 양과 사용량에 맞게 확장되는 클라우드 기반 솔루션입니다.\n- 사용 편의성: 사용자 친화적인 인터페이스와 자동화로 광범위한 기술 전문 지식이 필요하지 않습니다.\n\n사용 사례:\n\n- 데이터 분석 솔루션의 신속한 배포\n- 민첩한 데이터 관리 및 변환\n- 셀프 서비스 분석 및 보고\n\n## 4. 통합 배치 및 스트리밍 아키텍처\n\n<div class=\"content-ad\"></div>\n\n통합 배치 및 스트리밍 아키텍처는 실시간 및 일괄 데이터를 하나의 프레임워크에서 처리하는 것을 목표로 합니다. 이 접근 방식은 데이터 처리 파이프라인을 간소화하고 일괄 및 스트리밍 데이터를 관리하기 위한 별도 시스템의 복잡성을 줄입니다.\n\n![이미지](/assets/img/2024-06-23-DataArchitectureABriefOverview_4.png)\n\n주요 아키텍처:\n\n- 람다 아키텍처: 데이터 흐름을 별도 경로로 분리하여 일괄 및 스트리밍 처리를 결합합니다.\n- 카파 아키텍처: 모든 데이터를 스트림으로 처리함으로써 람다 아키텍처를 간소화합니다.\n\n<div class=\"content-ad\"></div>\n\n## 람다 아키텍처:\n\n- Batch Layer: 대규모의 과거 데이터를 처리합니다.\n- Speed Layer: 즉각적인 인사이트를 위해 실시간 데이터를 처리합니다.\n- Serving Layer: 배치 및 스피드 레이어의 출력을 통합된 결과로 병합합니다.\n\n장점:\n\n- 포괄적인 데이터 처리: 과거 및 실시간 데이터를 처리합니다.\n- 장애 허용성: 배치 재처리를 통해 데이터 정확성을 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n도전:\n\n- 복잡성: 별도의 배치 및 스트리밍 시스템을 관리하는 것은 어려울 수 있습니다.\n\n## Kappa 아키텍처:\n\n- 통합형 스트림 처리: 모든 데이터를 스트림으로 처리하여 아키텍처를 간소화합니다.\n- 재처리: 데이터 스트림을 재처리하여 과거 분석을 수행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n혜택:\n\n- 간단함: 단일 데이터 처리 모델을 사용하여 복잡성을 줄임.\n- 유연성: 관리 및 확장이 더 쉬움.\n\n## 5. 데이터 레이크하우스\n\n데이터 레이크하우스는 데이터 레이크와 데이터 웨어하우스의 기능을 결합하여, 데이터 레이크의 유연성과 데이터 웨어하우스의 성능 및 관리 기능을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-DataArchitectureABriefOverview_5.png\" />\n\n특징:\n\n- 통합 스토리지: 구조화된 및 구조화되지 않은 데이터를 모두 저장합니다.\n- ACID 트랜잭션: 데이터 작업에 대한 원자성, 일관성, 격리 및 내구성을 지원합니다.\n- 스키마 강제: 스키마를 읽거나 쓸 때 스키마에 대한 강제를 허용합니다.\n\n혜택:\n\n<div class=\"content-ad\"></div>\n\n- 유연성: 다양한 데이터 유형과 워크로드를 처리할 수 있습니다.\n- 성능: 고성능 쿼리 및 데이터 관리를 위해 최적화되었습니다.\n- 데이터 관리: 견고한 데이터 관리 및 거버넌스 기능을 제공합니다.\n\n사용 사례:\n\n- 통합 데이터 분석 및 보고\n- 기계 학습 및 AI 워크로드\n- 실시간 및 일괄 데이터 처리\n\n## 6. 데이터 매쉬\n\n<div class=\"content-ad\"></div>\n\n데이터 메시는 도메인 주도 설계의 원칙을 데이터 관리에 적용한 분산형 데이터 아키텍처로, 중앙 집중식 데이터 레이크나 데이터 웨어하우스에서 데이터 소유 및 책임을 특정 비즈니스 도메인과 일치시키는 분산 모델로 전환합니다.\n\n![이미지](/assets/img/2024-06-23-DataArchitectureABriefOverview_6.png)\n\n특징:\n\n- 도메인 중심: 데이터는 해당 도메인이 가장 잘 아는 곳에서 관리됩니다.\n- 제품으로서의 데이터: 각 도메인은 데이터를 제품으로 취급하여 품질과 사용성을 보장합니다.\n- 셀프 서비스 데이터 인프라: 도메인이 자체적으로 데이터를 관리할 수 있도록 필요한 도구와 플랫폼을 제공합니다.\n- 연합된 지배 체계: 중앙 집중식 정책과 표준을 분산 방식으로 시행합니다.\n\n<div class=\"content-ad\"></div>\n\n혜택:\n\n- 확장성: 비즈니스가 성장함에 따라 조직이 데이터 아키텍처를 확장하는 것을 가능하게 합니다.\n- 민첩성: 더 빠르고 유연한 데이터 관리와 분석을 가능하게 합니다.\n- 소유권: 도메인 내에서 데이터 소유권과 책임을 강조합니다.\n\n사용 사례:\n\n- 다양한 데이터 요구 사항을 가진 대규모 조직\n- 데이터 거버넌스 및 품질을 향상시키려는 기업\n- 데이터 인프라를 확장하려는 조직\n\n<div class=\"content-ad\"></div>\n\n## 7. 데이터 패브릭\n\n데이터 패브릭은 온프레미스 및 클라우드 환경을 횡단하며 이종 데이터 원본 및 응용 프로그램을 연결하여 통합 데이터 환경을 만드는 구조적 방법론입니다. 일관된 데이터 관리와 거버넌스를 보장하며 심층적인 데이터 접근과 통합을 제공하여 데이터 환경의 일관성을 유지하고자 합니다.\n\n![데이터 아키텍처 개요](/assets/img/2024-06-23-DataArchitectureABriefOverview_7.png)\n\n특징:\n\n<div class=\"content-ad\"></div>\n\n- 통합 액세스: 모든 데이터 원본에 대한 단일 액세스 포인트를 제공합니다.\n- 통합: 다양한 데이터 환경을 연결하여 데이터 이동 및 통합을 가능하게 합니다.\n- 자동화: AI 및 기계 학습을 활용하여 데이터 관리 작업을 자동화합니다.\n- 거버넌스: 데이터 풍경 전반에 걸쳐 데이터 품질, 보안 및 규정 준수를 보장합니다.\n\n혜택:\n\n- 일관성: 조직 전반에서 일관된 데이터를 보장합니다.\n- 효율성: 데이터 실로를 줄이고 데이터 액세스를 간소화합니다.\n- 민첩성: 신속한 데이터 통합 및 이동을 용이하게 합니다.\n\n사용 사례:\n\n<div class=\"content-ad\"></div>\n\n- 하이브리드 데이터 환경을 가진 조직\n- 데이터 통합 및 관리를 개선하고자 하는 기업\n- 데이터 지배와 준수를 보장해야 하는 비즈니스\n\n# 추가 자료\n\n- Ross Pettit의 “Utility from Value Add 구분하기”\n- Joshua Klahr의 “현대 데이터 아키텍처의 여섯 가지 원칙”\n- Snowflake의 “데이터 웨어하우스 아키텍처란?” 웹 페이지\n- Erik Bernhardsson의 “소프트웨어 인프라 2.0: 워크리스트”\n- Etai Mizrahi의 “데이터 빚을 앞서가는 방법”\n- Neal Ford의 “전략 대 전술: SOA와 무관한 수구의 함정”\n- Dustin Lange 등의 “Deequ로 규모에 맞게 테스트 데이터 품질 유지하기”\n- IBM Education의 “3계층 아키텍처”\n- TOGAF 프레임워크 웹사이트\n- Prukalpa의 “CDOs가 2021년 주목해야 할 상위 5가지 데이터 트렌드”\n- Alexey Makhotkin의 “240 개의 테이블과 문서가 없다고?”\n- Molly Vorwerck의 “관측 가능 데이터의 궁극적인 체크리스트”\n- Apache Flink Roadmap의 “통합 분석: 일괄 및 스트리밍이 만나다; SQL 이상”\n- Martin Fowler의 “Utility Vs Strategic Dichotomy”\n- Ben Lorica 등의 “데이터 레이크하우스란?”\n- Thor Olavsrud의 “데이터 관리를 위한 프레임워크인 데이터 아키텍처란?”\n- Casber Wang의 “오픈 데이터 생태계란 무엇이며 왜 계속해서 존재하는가?”\n- Laszlo Sragner의 “MLOps가 잘못된 이유는 무엇인가?”\n- Chris Riccomini의 “데이터 메시가 뭐길래”\n- Martin Fowler의 “누가 아키텍트가 필요한가”\n- Zachman 프레임워크 위키피디아 페이지\n- Prukalpa의 “현대 데이터 플랫폼의 구성 요소”\n- Zhamak Dehghani의 “단일 데이터 레이크를 분산 데이터 메시로 나아가는 방법”\n- Zhamak Dehghani의 “데이터 메시 원칙 및 논리 아키텍처”\n- Iman Samizadeh의 “큰 데이터용 람다와 카파 두 데이터 처리 아키텍처 간단 소개”\n- Hussein Danish의 “원칙에 따른 데이터 엔지니어링, 제1부: 아키텍처 전망”\n- Jay Kreps의 “로그: 소프트웨어 엔지니어가 실시간 데이터의 통일된 추상화에 대해 알아야 할 사항”\n\n이것들은 저가 블로그 글 작성 중 살펴본 일부 자료입니다. 이들은 데이터 아키텍처의 개념과 모범 사례에 대한 추가 통찰력을 제공하고 깊게 이해하는 데 도움이 될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 요약\n\n이 블로그 포스트에서는 학습 중에 만난 데이터 아키텍처의 기본 원칙 중 일부를 개요로 설명하고 간략히 주요 데이터 아키텍처 패턴을 논의했습니다.\n\n이 상세한 데이터 아키텍처 패턴을 이해함으로써, 조직의 요구에 가장 적합한 아키텍처를 결정하고 효과적으로 구현하는 방법에 대해 정보를 얻을 수 있습니다.\n\n만일 내가 어떤 원칙이나 아키텍처를 잘못 이해하고 있다면 말씀해 주십시오. 배우는 것에 기쁨을 느낄 테니까요.","ogImage":{"url":"/assets/img/2024-06-23-DataArchitectureABriefOverview_0.png"},"coverImage":"/assets/img/2024-06-23-DataArchitectureABriefOverview_0.png","tag":["Tech"],"readingTime":13}],"page":"18","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true}