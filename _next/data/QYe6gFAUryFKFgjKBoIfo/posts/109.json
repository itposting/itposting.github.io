{"pageProps":{"posts":[{"title":"내 전문지식을 3가지 도구를 이용해 몇 분 안에 판매용 코스로 어떻게 변환했는지 알아보세요","description":"","date":"2024-05-27 13:14","slug":"2024-05-27-HowIActuallyConvertMyExpertiseintoCoursesForSaleinMinuteswith3ToolsWillAmazeYou","content":"\n\n![HowIActuallyConvertMyExpertiseintoCoursesForSaleinMinuteswith3ToolsWillAmazeYou_0.png](/assets/img/2024-05-27-HowIActuallyConvertMyExpertiseintoCoursesForSaleinMinuteswith3ToolsWillAmazeYou_0.png)\n\n이것에 대해 '100만 번' 듣거나 읽어 보았을 겁니다; \"Fiverr, Payhip, Gumroad, Amazon 등 마켓플레이스에서 온라인 강좌를 판매하여 돈을 벌 수 있습니다.\" 또는 \"온라인 강좌를 판매하여 6자리 수입을 올린 방법을 배워보세요.\"와 비슷한 고함 소리 제목들을 들어본 적이 있을 겁니다. 아마도 여러분은 전문 지식이 있고 수많은 아이디어를 가지고 있으며, 그 큰 케이크 한 조각을 손에 넣는 꿈을 꾸어 왔을 것입니다, 그렇지 않습니까?\n\n유감스럽게도, 시작점이 어디인지 전혀 모르겠다면! 아니면 아마도 단순히 시간을 들여 과정을 설계하는 일이 너무 힘든 것처럼 느껴질 수 있다면요. 음, 알겠어요? 답은 드디어 여기 있으며, 이에 대한 해답은 세 개의 AI 앱에 있을 수도 있습니다.\n\n참고 — 이 리소스들을 사용하여 온라인 강좌를 만들고 판매하기만 해서 금방 부자가 될 수는 없습니다. 일반적인 조언으로, 성공을 거두기 위해서는 당신이 판매하려는 것에 관심이 있는 시장 조사를 하고 성공적으로 하기 위한 대상 시장을 찾아야 합니다.\n\n<div class=\"content-ad\"></div>\n\n그리고 '인공지능 쓰레기를 생산하는 도구'를 공유하기 전에 내 목을 움켜쥐려 들지 말아주세요! 그렇지 않아요! 오히려, 이 도구들은 이미 존재하는 콘텐츠를 코스로 만드는 데 도움이 되는 도구들이에요.\n\n결과물이 마음에 안 든다면 편집 버튼을 눌러서 스타일을 다시 적용할 수 있어요.\n\n맞아요, AI를 사용해서 생성하는 기능도 있어요. 하지만 특정 분야에 전문지식을 가지고 있고 생성된 정보를 확인할 수 있다면, 언제든지 스타일과 개성을 더해 편집할 수 있어요.\n\n중요한 점은 결과물이 코스 형식으로 구조화되어 있어서 초보자들에겐 좋다는 거예요. 하지만 새로운 기술에 의존하지 않고 속도와 질 높게 코스를 쉽게 만들 수 있는 기술을 이미 갖고 계신다면, 이 도구는 여러분을 위한 것이 아니에요. 생산성과 빠른 작업 완료에 대해 얘기하는 거예요.\n\n<div class=\"content-ad\"></div>\n\n일반적인 규칙으로서, 자신이 전문 지식이 없는 분야의 강의를 만들지 말아야 합니다. 이는 신뢰를 훼손할 수 있습니다.\n\n솔직하게 말씀드리자면, 이 몇 개 앱의 전체 버전은 저렴하지 않을 수도 있어요. 하지만 무엇을 원하고 얻을 수 있는 혜택을 알고 있다면, 그것에 투자하는 것은 가치가 있을 겁니다. 아, 제가 관련된 사람은 아니에요! 이를 명확히 한 후에, 달려봅시다.\n\nCourseau\n\n![이미지](https://miro.medium.com/v2/resize:fit:1380/1*yLEay5NUzZqYNQSwYv7Jhw.gif)\n\n<div class=\"content-ad\"></div>\n\n먼저 소개할 플랫폼은 Courseau입니다. 이미 갖고 계신 콘텐츠를 동적인 온라인 코스로 변환하기에 완벽한 플랫폼입니다. 즉, YouTube 비디오, 팟캐스트 에피소드, 또는 심지어 원고 블로그 콘텐츠가 있다면 이를 온라인 코스로 판매할 수 있을 것으로 믿습니다. 물론, 어떤 사람들은 코스나 단계별 가이드를 선호합니다. Courseau는 이 과정을 간소화해줄 수 있습니다. 이렇게 합니다:\n\n먼저 '무료로 시작' 버튼을 클릭하여 등록을 완료하세요. 등록이 완료되면 '라이브러리' 섹션에서 '만들기' 옵션이 표시됩니다. 그곳에서 온라인 코스 툴을 찾아 코스를 만들기 시작하세요.\n\n무료 계정에서는 '미니'를 만나게 됩니다. 이는 GPT 3.5의 간소화 버전을 사용하여 여러분이 관심을 갖는 경우 6~8개의 레슨으로 이뤄진 간단한 코스를 만드는 데 도움을 줍니다.\n\n그리고 이것이 우리의 관심을 끄는 부분입니다 — 이 플랫폼은 YouTube 링크, 오디오 파일, 웹페이지, PDF, Word 문서와 같은 다양한 형식의 콘텐츠 업로드를 받아들입니다. 페이지 수로 30페이지 또는 오디오로 2시간까지입니다.\n\n<div class=\"content-ad\"></div>\n\n더 많은 기능을 원하신다면, Pro로 업그레이드하여 더 긴 그리고 광범위한 강좌 생성 문을 열어보세요. 소스 URL을 붙여 넣거나 문서를 업로드한 후 처음 강좌를 만들기 위해 단계를 따르세요.\n\n이 단계 이후에는 커스터마이즈 섹션을 만나게 될 겁니다. 여기서 색감을 변화시켜 강좌에 개성을 불어넣을 수 있습니다. 선택을 마치면 \"강좌 생성\"을 클릭하고 그 마법을 일으키게두 되세요.\n\n몇 분 안에 강좌가 완성될 것입니다. 콘텐츠에 따라서 강의 계획서, 실행 가능한 단계, 심지어 퀴즈까지 모두 포함될 수 있을 겁니다. 이는 콘텐츠를 매력적인 학습 경험으로 정제하는 플랫폼의 능력을 검증하는 것입니다.\n\n만약 당신이 만든 강좌가 맞는 느낌을 주면, 당연히 ‘발행’을 클릭하세요. 아직 그 수준에 도달하지 못했다면, ‘편집’ 옵션을 통해 그것을 다듬을 수 있는 창구를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n수정된 내용을 저장한 후 ‘저장 및 미리보기’를 클릭하면 작품을 잠깐 볼 수 있어요. 여기가 과제인데, 특히 무료로 이용하는 분들에겐 안 기능이요 — Courseau는 과정 생성에 뛰어난데, 과정을 전개시키고 싶으면 업그레이드가 필요해요. 그러니 세계와 나누고 싶은 작품을 공유하고 싶다면 업그레이드가 필요할 거에요.\n\n업그레이드하고 싶다면, 완료 후 과정 링크를 받을 거예요. 그러면 Gumroad나 Payhip 같은 플랫폼과 통합해 전문성을 활용해 돈을 버실 수 있어요.\n\n그래서 이 도구는 가치가 있어요. 하지만 한 번 들여다 볼 이유도 있어요.\n\n러닝 스튜디오 AI\n\n<div class=\"content-ad\"></div>\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*qXSr2cw1a9YsD_9XQ15Gdg.gif)\n\n이 앱을 사용하면 가입하자마자 친절한 사용자 인터페이스로 환영받을 수 있습니다. 주제를 입력하고 강의 라이브러리에 짜맞추는 것을 지켜보는 것으로 초대합니다. 그게 전부입니다!\n\n커리큘럼 형식과는 달리 Learning Studio AI는 논리적 진행을 제공하는 블로그 형식의 프레젠테이션을 제공합니다. 편집 모드 없이도 학생들이 콘텐츠를 쉽게 미리보기할 수 있는 몰입형 경험이 가능합니다.\n\n세계와 지식을 공유할 준비가 되었다면, Learning Studio AI를 통해 링크를 복사하여 학생들과 공유하면 그들이 가입하고 참여할 수 있습니다. 오프라인 학습을 위해 PDF 버전을 다운로드할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 플랫폼의 멋진 점은 기술에 능숙한 사람들만을 위한 것이 아니라는 것입니다. 게다가, 다양한 주제를 다룰 수 있는 유연성을 갖추고 있습니다. 이 플랫폼은 정확도 높은 코스를 생성할 수 있어서 실용적인 연습과 실제 응용을 포함한 풍부한 학습 경험을 제공할 수 있습니다.\n\n마찬가지로 코스를 매우 쉽게 개인화할 수 있습니다. 예를 들어, 테마 색상을 변경하거나, Learning Studio AI 로고의 가시성을 결정하거나, 이미지나 비디오를 삽입하여 학습 여정을 향상시킬 수 있습니다.\n\n내 의견으로는, 이것은 온라인 코스를 통해 지식을 만들고 공유하려는 사람들을 위한 강력하고 직관적인 도구로 빛납니다. 게다가, 누구나 사용할 수 있습니다.\n\n미니 코스\n\n<div class=\"content-ad\"></div>\n\n![image](https://miro.medium.com/v2/resize:fit:1380/1*BcOtUKohcz4S6ogqDNBrsg.gif)\n\n미니 코스는 또 다른 뛰어난 앱입니다! 이 앱은 각각 다른 세그먼트를 나타내는 '카드' 시스템을 활용합니다. 위의 gif 예시에서 볼 수 있듯이, 저는 \"WordPress를 활용한 웹사이트 개발 초보자를 위한 궁극의 안내서\"라는 코스를 만들어 보았습니다. 이는 제 전문 분야 중 하나입니다.\n\n따라서 내용을 단계별로 사실 확인하고 필요한 곳에 수정을 가할 수 있습니다. 또한 작문 스타일을 완전히 통합하여 스스로의 것으로 만들 수도 있습니다!\n\n이 앱이 얼마나 완벽하게 사용자 정의 가능한지, 이미지, 비디오, 심지어 AI 생성 콘텐츠를 추가하여 학습 경험을 풍부하게하고 안내할 수 있다는 점을 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n미니 코스로 무엇을 할 수 있을까요? AI 어시스턴트를 활용하여 안내 받는 것이나, 원한다면 처음부터 시작할 수도 있어요. 카드 단위로 컨텐츠를 쉽게 조합할 수 있어요.\n\n이 앱을 통해 컨텐츠를 만드는 것뿐만 아니라 여정을 디자인하는 경험을 할 수 있어요. 또한 문법 검사, 요약 기능, 브랜드에 맞는 다양한 테마 등을 제공하여 코스를 완벽하게 세밀하게 조정할 수 있어요.\n\n코스가 준비되면 미리보기 기능을 통해 학생으로서 코스를 체험해볼 수 있어요. 마음에 드는 내용이라면 쉽게 공유할 수 있어요.\n\n미니 코스는 빠르게 온라인 코스를 만들고 배포하고자 하는 사람들에게 신뢰할 수 있고 다재다능한 도구로 보입니다. 이미 잘 확립된 존재감과 다양한 기능을 갖춘 이 앱은 모든 수준의 교육자들에게 뛰어난 선택지가 될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n그럼 이렇게 되었어요! 각 플랫폼마다 Courseau의 매끄러운 콘텐츠 변환, Learning Studio AI의 혁신적인 블로그 스타일 코스, Mini Course의 대화형 카드 시스템과 같은 독특한 강점을 제공합니다.\n\n그래서 경험이 풍부한 교육자이든 디지털 콘텐츠 크리에이터이든, 이 도구들은 학습자가 어디에서나 공감할 수 있는 매력적인 온라인 강좌를 디자인하는 데 도움이 되는 다양한 기능을 제공합니다.\n\n학습의 여정은 절대 끝나지 않으며 항상 다양한 형태를 취합니다. 그럼 읽어주셔서 감사합니다!","ogImage":{"url":"/assets/img/2024-05-27-HowIActuallyConvertMyExpertiseintoCoursesForSaleinMinuteswith3ToolsWillAmazeYou_0.png"},"coverImage":"/assets/img/2024-05-27-HowIActuallyConvertMyExpertiseintoCoursesForSaleinMinuteswith3ToolsWillAmazeYou_0.png","tag":["Tech"],"readingTime":6},{"title":"PIC 생태계","description":"","date":"2024-05-27 13:11","slug":"2024-05-27-PICEcosystem","content":"\n# PIC16LF18324 코딩을 시작하는 방법\n\n만약 이미 라즈베리 파이코나 다양한 아두이노로 몇 가지 프로젝트를 해 보셨고 다른 마이크로컨트롤러에서 작업을 시도해보고 싶다면, Microchip 사의 PIC를 사용해보시길 권해드립니다. 여기에는 때로는 흥미로운 도전이 될 수 있는 8비트 마이크로컨트롤러들이 포함되어 있습니다. 셋업하기 위해 약간의 비용이 들 수는 있지만, 칩 자체는 저렴하며 즐겁게 만들고 탐험할 수 있습니다.\n\n이 글은 시작하는 데 필요한 도구에 대해 설명합니다. 이 글은 개발 보드가 아닌 PIC 마이크로컨트롤러 자체에 대한 것입니다. 여기에서 다루고 있는 유형의 개별 PIC 마이크로컨트롤러보다 비쌌지만 PicKit 항목을 구입할 필요가 없도록 도와주는 PIC 마이크로컨트롤러용 개발 보드가 있습니다.\n\n만약 여전히 관심이 있다면, 개발 보드를 넘어서거나 다른 사전에 납땜된 방법을 넘어서고 싶다면, 계속해서 읽어보세요.\n\n<div class=\"content-ad\"></div>\n\n# 왜 PIC를 선택해야 하는가?\n\nPIC의 흥미로운 측면 중 하나는 그 가족 중 특별한 저전력 칩이 있다는 것입니다. 저전력 사용은 빠른 알고리즘을 설계하는 것과 유사한 도전 과제입니다. 그러나 적절한 MCU를 선택함으로써 큰 도움을 받을 수 있습니다. 아두이노 \"플랫폼\"은 많은 사람들에게 좋은 시작점입니다. 그것은 간단히 ATTiny85와 같은 관련 AVR 칩을 탐구할 수 있도록 쉽게 사용할 수 있습니다. 그러나 마이크로컨트롤러에 대한 보다 폭넓은 이해를 원하고 아두이노의 세계를 벗어나려면, PIC 패밀리는 매우 낮은 전력을 제공하여 그 방법을 제공할 수 있습니다. PIC16LF18324 (또한: 18324, 이후로는 \"칩\" 또는 \"픽\"으로 지칭)는 이러한 제안 중 하나입니다. 이 칩은 이름과는 달리 16이 아닌 8비트 칩입니다. 14개의 핀이 있으며, 그 중 12개는 I/O 핀입니다. PIC 패밀리의 다른 구성원들은 다양한 핀 수를 갖고 있습니다. 이름에 'L'이 있는 것은 \"저전력\"을 의미합니다. 이름에 'L'이 없는 비저전력 모델도 있지만 이 모델은 여기에서 다루지 않겠습니다. PIC16LF18324 칩만 구입하는 비용은 ATTiny85와 비슷합니다. ATTiny85처럼 스룰 구멍 또는 표면실장 패키지로 구입할 수 있으며, 내장 오실레이터도 내장되어 있습니다.\n\n18324는 7K의 RAM을 제공하며, 외부 크리스탈이 필요하지 않고 제공되는 32MHz의 속도로 실행됩니다. 내장 오실레이터는 공장에서 보정되었습니다. 7KB의 플래시와 1/2K (512바이트)의 정적 RAM이 탑재되어 있습니다. 다양한 주변 기기(많은 MCU들처럼)를 갖고 있습니다.\n\n- USART (증강 USART 또는 EUSART라고 불림)\n- I2C\n- SPI\n- PWM (모터 제어나 TV 및 기타 리모컨에서 자주 사용됨)\n- 타이머 (캡처/비교 포함); 또한 \"와치독\" 타이머도 있음\n- 아날로그/디지털 변환기 및 디지털/아날로그 변환기\n- 물론: 핀을 읽거나 핀에 디지털 값을 쓸 수 있는 능력(1은 LED 켜기; 0은 LED 끄기)\n\n<div class=\"content-ad\"></div>\n\n18324에서 특히 좋은 기능 중 하나는 PPS입니다. 이 \"주변 핀 선택\" 기능을 사용하면 대부분의 핀을 어떤 주변 장치에도 사용할 수 있습니다. 다양한 마이크로컨트롤러의 핀 배치도를 보신 적이 있을 수도 있습니다. 18324는 핀에 대한 일부 기본 설정이 있을 수 있지만, 상당한 유연성으로 라우팅할 수 있습니다. 이에는 일부 제한 사항이 있을 수 있지만(클럭 속도 등을 기반으로 한 PPS 매핑이 불가능한 특정 핀 등), 예를 들어 EUSART TX 핀을 RC4로 라우팅할 수 있습니다.\n\n핀에 대해 이야기할 때, 14개의 핀에 제한된 18324는 그 I/O 핀을 RAn 또는 RCn 중 하나로 지정합니다. PIC16 패밀리의 더 큰 멤버들은 RBn도 설정합니다. 두 핀은 전원 공급용으로 \"바쁩니다\" (VDD와 VSS). 많은 다른 칩들과 달리 전원 핀이 대각선에 위치하는 칩들과 달리, VDD(전원)와 VSS(그라운드)는 한쪽 끝의 노치 반대편에 있습니다. 전원 핀은 18324 칩의 한쪽 끝에 있습니다. 전체 핀 배치도는 아래에 표시되어 있습니다.\n\n이 간단한 소개가 제공된 이제 PIC16lf18324에서 시작하기 위해 필요한 설정 또는 \"장비\"를 살펴보는 시간입니다.\n\n# 사전 준비 조건\n\n<div class=\"content-ad\"></div>\n\n만약 이 글을 그냥 흥미로이 읽고 싶다면 맘껏 읽어보세요. 하지만 이 글과 함께 코딩하길 원한다면, 다음 사항을 준비해야 하거나 받아야 할 수도 있습니다.\n\n- 브레드보드\n- PIC16LF18324 칩\n- 프로그래밍 장비\n- 와이어 (또는 두폰트 케이블)\n- LED\n- 두 개의 저항\n- 100Ω에서 330Ω의 저항 값이 적당합니다. 더 높은 값도 괜찮지만, LED가 어두워질 수 있습니다.\n- 5.1 kΩ 풀업 저항기 (또는 근사값)\n- 0.1 µF 캐패시터. 세라믹, 비편광이 권장됩니다.\n- 어떤 종류의 전원 공급 장치. 건전지가 적당합니다. 약 3.3V 정도 필요합니다.\n- 'C' 코드에 대한 약간의 지식이 있으면 유용합니다. 코드는 제공됩니다.\n\n# 칩 구매\n\n이들은 DigiKey 또는 Mouser에서 주문할 수 있습니다. 둘 다 각각 2달러 미만입니다 (2024년 5월에 각각 1.29달러에서 1.56달러로 확인되었습니다). 대량 구매할 경우 할인이 적용되어 가격이 더욱 내려갈 수 있습니다. 필요한 패키징을 획득하기 위해 주의해야 합니다. 이 칩을 브레드보드에 꽂으려면 \"PDIP\"와 같은 스루홀 패키지가 필요합니다. 이는 \"14 PDIP\" 또는 \"PDIP-14\"로 불릴 수 있습니다. 또한 배송 비용을 주의하세요. 이 비용이 여러 칩의 가격보다 비쌀 수 있습니다. 세 개 이상을 그룹으로 구매하면 배송비를 절감할 수도 있습니다. 아마존에서는 (이 지침과 호환되지 않지만 보통 사용하기 쉬운) PIC 보드를 20달러 미만으로 구매할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 가장 큰 비용: 프로그래머\n\nPIC16 패밀리 칩에 노출되기 위해 인-시스템 프로그래밍용 개발 보드를 사용하는 것이 가능할 수 있습니다. 독자가 투자를 활용하고 싶다면 좋은 구글링을 해볼 만한 가치가 있을지도 모릅니다. 그러나 '훈련 바퀴를 제거하고', 생태계에 깊숙이 들어가려면 로우 마이크로컨트롤러를 사용해야 합니다. 이는 마이크로컨트롤러 작업 시 지출이 반복적으로 필요한 사항입니다. 이 기기 없이도 성공적으로 진행할 수 있다면 언제든지 자유롭게 해보십시오.\n\n과거에는 PICkit 4®가 약 $100에 구매 가능했으며, 아래에서 설명합니다. 보다 최근에 PicKit 5가 소개되었으며 제조사에 의해 PICkit 4가 지연 폐기되었습니다. 그러나 가격은 대략 동일합니다. 유사한 기능을 갖춘 것으로 가정됩니다. 두 제품 모두 MPLAB X IDE v6 소프트웨어와 호환됩니다(아래 더 자세히 설명함). 이제는 더 저렴한 기기(약 $40)도 구매할 수 있을 수 있습니다. 개인별 사용환경에 따라 다를 수 있습니다.\n\nMPLAB PICKit4에는 한쪽 끝에 연결 커넥터 구멍이 있습니다(또는 참조로 사용하는 레이블링에 따라서 '밑면'입니다). 매우 특정한 핀배열을 갖추고 있습니다. 이 구멍을 브레드보드에 연결해야 합니다. Dupont 케이블이나 일반 와이어로도 잘 작동합니다. 이 칩에 대한 다양한 프로젝트를 수행한다면, 더 편리한 것이 필요할 수 있음을 발견할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nPICKit4의 다른 쪽에는 작은 USB 2 커넥터가 있습니다. 이를 PC에 연결할 수 있습니다 (사람들은 포럼 게시물에 따르면 MAC 및 Linux도 사용합니다). 연결하고 나면 IDE에서 프로그램을 할 수 있습니다.\n\n# 소프트웨어\n\n마이크로컨트롤러에 코드를 로드하는 것은 C 컴파일러와 MPLAB X IDE라는 다운로드 가능한 소프트웨어로 처리할 수 있습니다. 이 문서 작성 시에는 버전 6입니다.\n\n![이미지](/assets/img/2024-05-27-PICEcosystem_0.png)\n\n<div class=\"content-ad\"></div>\n\n아래에 전체 지침이 있지만, 대체로 Microchip의 개발자 도움 페이지인 Install MPLAB® X IDE Version 6.00 Walkthrough에서도 확인할 수 있습니다. MPLAB X는 다른 IDE와 유사하게 설치됩니다(실제로, 설치 파일을 기반으로 하면 Apache의 NetBeans IDE를 기반으로 한 것으로 보이며, 마이크로컨트롤러와 함께 사용하기 위해 많은 사용자 정의가 있습니다).\n\n이 IDE에서 완전히 새로운 PIC16LF18324 프로젝트를 설정하는 방법은 아래에 설명되어 있지만, 먼저 몇 가지 유용한 도구와 필요한 대화 상자에 대해 살펴보겠습니다.\n\n# 몇 가지 도구와 대화 상자\n\n프로젝트를 만든 후에는 프로퍼티 페이지를 살펴보는 것이 매우 유용합니다. 왼쪽에 설명된 대로, \"마우스 클릭\"(Windows에서 좌 클릭하거나 기타 OS에서는 일반적으로 OS와 같은 방식으로 메뉴를 띄워주십시오)하고 나서 \"Properties\" 메뉴 항목을 가장 아래에서 클릭합니다. IDE의 나중/이전 버전은 이것을 다르게 배치할 수 있지만, \"Properties\"는 매우 가능성이 높은 이름입니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-PICEcosystem_1.png\" />\n\n클릭하면 아래와 같이 새로운 팝업 대화 상자가 나타납니다. 이 대화 상자에는 작업할 기기 이름 설정이 포함되어 있습니다. 이 설정은 디버깅 및 내장 플래시 드라이브에 코드를 불러올 때 필요합니다.\n\n<img src=\"/assets/img/2024-05-27-PICEcosystem_2.png\" />\n\n왼쪽에 트리 구조를 주목해주세요. 이것은 오른쪽 창의 모드를 변경합니다. \"PICKit4\" 노드를 클릭하면 아래 모드로 변경됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n![PICEcosystem Image 3](/assets/img/2024-05-27-PICEcosystem_3.png)\n\n이것은 \"Options categories\" 드롭다운을 사용하는 데 매우 중요한 부분입니다. 이것은 여전히 보이는 컨트롤을 더 조정합니다.\n\n![PICEcosystem Image 4](/assets/img/2024-05-27-PICEcosystem_4.png)\n\n\"Power target circuit from the PICkit 4\"를 선택해야 합니다. 이 설정은 컴퓨터의 USB 드라이브를 통해 PICkit 4와 PIC 칩을 연결한 경우에 사용됩니다. 디버깅 및 플래시 드라이브에 코드를 업로드하는 데 필요합니다. 이 값은 USB 전원을 보드로 공급합니다. 이를 사용하지 않으면 보드 자체의 전원이 사용됩니다. 그림에서와 같이 단순한 브레드보드에 칩을 연결한 경우, PICkit으로부터 전원을 공급하는 것이 올바른 선택입니다. 이 값은 기본적으로 선택되지 않습니다. 아마도 소프트웨어가 모든 하드웨어를 보호하려고 하기 때문일 것입니다. 대상 보드에 백업 전원 공급이 사용 중인 경우 이 설정은 좋은 선택이 아닙니다. 실제로 이 설정을 \"Power target circuit from PICkit 4\"로 변경하지 말아야 할 때까지 변경하지 않으려다고 말할 정도입니다. 필요한 경우 팝업 메시지가 표시될 때까지 변경하지 마세요. 이 설정이 필요하지만 설정되지 않은 경우 PICkit / IDE 조합은 팝업 오류 대화상자로 경고합니다. 설정을 저장하려면 \"확인\" 버튼을 클릭해야 합니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 구성 비트\n\n![PICEcosystem_5](/assets/img/2024-05-27-PICEcosystem_5.png)\n\n이 메뉴 항목은 \"구성 비트\"를 엽니다. PIC 영역에서는 하드웨어를 낮은 수준에서 설정하는 특수 플래시 버닝 설정이 선택됩니다. 다른 마이크로컨트롤러에서는 이러한 설정을 \"퓨즈\"라고 할 수 있습니다.\n\n![PICEcosystem_6](/assets/img/2024-05-27-PICEcosystem_6.png)\n\n<div class=\"content-ad\"></div>\n\n여기에서 구성할 수 있는 설정 사항입니다. 이 작은 칩이 실제로 수행할 수 있는 몇 가지 흥미로운 작업의 아이디어를 제공할 수 있습니다 (브라운 아웃 감지, 위에서 언급한 PPS(주변 핀 선택), 그리고 와치독 타이머까지 모두 여기서 암시됩니다). 이 토론과 가장 관련된 것은 아마 위쪽에 있는 \"FEXTOSC\"/\"RSTOSC\" 설정 쌍일 것입니다. 이 PIC 칩은 클럭 구동에 있어 유연합니다. 외부 오실레이터를 사용할 수 있지만, 사용 중이 아닌 경우 여기서 비트를 설정하지 않아야 합니다. 이 토론 목적상 FEXTOSC(외부 오실레이터)를 off로 설정하고, RSTOSC(리셋 기본 오실레이터)를 NFINT32(내부 32,000,000 Hz로 읽음) 설정으로 설정하세요. 이것은 1 MHz로도 설정할 수 있습니다. 그러나 외부 설정은 칩을 구동하는 오실레이터를 배선해야 하며, 이 토론의 범위를 벗어납니다. 또한 핀을 사용하고 더 많은 비용이 소요되며 더 많은 하드웨어 조작이 필요합니다.\n\n이 문서의 다른 곳에서 제시된 코드에 관해: 작업 부분 외에도 실제로 운영 부분에 대한 추가 코드가 상단의 “Generate Source Code to Output” 버튼을 클릭하여 생성되었습니다. 이것은 다음과 같은 코드를 생성합니다: 코드 값 시작부)을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\nXC8 컴파일러를 설치할 겁니다. 다운로드할 수 있는 링크는 아래와 같아요: https://www.microchip.com/en-us/tools-resources/develop/mplab-xc-compilers .\n\n![이미지](/assets/img/2024-05-27-PICEcosystem_7.png)\n\n컴파일러를 선택하는 것은 당신이 작성하는 코드와 최적화 수준에 영향을 줄 거예요. 이 컴파일러는 IDE와 칩이 같은 회사에서 제공하는 것이라 안전한 선택이에요. 다운로드 페이지에는 \"수상 경력\"이 있다고 쓰여 있어요. 당연히 독자께서는 리뷰와 대안을 확인하실 수도 있어요. 다른 하드웨어를 구입해야 할 수도 있지만, ATTiny 시리즈 같은 AVR을 더 좋아하는 경우, 이 글에서 소개된 설정과 유사한 것을 사용하면 Arduino의 세계에서 벗어날 수도 있어요.\n\n해당 버튼을 클릭하면 라이선스를 보여주는 큰 페이지로 이어지는데, 이는 이 소프트웨어를 기반으로 한 프로젝트를 배포할 계획이 있다면 중요한 고려 사항이에요. 또한 다운로드 컨트롤이 나와 있어요.\n\n<div class=\"content-ad\"></div>\n\n\n![PICEcosystem_8](/assets/img/2024-05-27-PICEcosystem_8.png)\n\n현재 작성 시점보다 몇 달 전의 날짜를 가진 것이 있습니다. 이는 코드가 꾸준히 유지되고 있는 좋은 신호입니다. IDE는 업데이트 프롬프트를 정기적으로 제공합니다.\n\n여기서 다운로드는 설치 프로그램을 위한 것입니다. 해당 설치 프로그램을 실행하면 아래의 대화 상자들이 팝업됩니다.\n\n![PICEcosystem_9](/assets/img/2024-05-27-PICEcosystem_9.png)\n\n\n<div class=\"content-ad\"></div>\n\n\n![PICEcosystem_10](/assets/img/2024-05-27-PICEcosystem_10.png)\n\n![PICEcosystem_11](/assets/img/2024-05-27-PICEcosystem_11.png)\n\n![PICEcosystem_12](/assets/img/2024-05-27-PICEcosystem_12.png)\n\n![PICEcosystem_13](/assets/img/2024-05-27-PICEcosystem_13.png)\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-05-27-PICEcosystem_14.png)\n\n![이미지](/assets/img/2024-05-27-PICEcosystem_15.png)\n\n# Tool Chains\n\nMPLAB X IDE는 빌드를 수행하기 위해 \"툴 체인\" (컴파일러 및 기타 관련 코드를 포함하는 도구)을 사용합니다. \"도구/옵션\"을 이용하여 관리할 수 있으며, 이어지는 대화 상자에서 임베디드 아이콘을 클릭하세요.\n\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-27-PICEcosystem_16.png)\n\n![이미지](/assets/img/2024-05-27-PICEcosystem_17.png)\n\n# 프로젝트 생성하기\n\n이것이 여러분이 처음으로 만드는 PIC16LF18324 프로젝트일 수도 있습니다. 혹시 그보다 더 큰 프로젝트라면 환영합니다! 여기서는 간단한 LED 프로젝트를 만들어 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 \"파일 / 새 프로젝트\"로 시작합니다.\n\n![File / New Project](/assets/img/2024-05-27-PICEcosystem_18.png)\n\n이렇게하면 위자드 대화 상자가 표시됩니다.\n\n![Wizard Dialog](/assets/img/2024-05-27-PICEcosystem_19.png)\n\n<div class=\"content-ad\"></div>\n\n우리는 Microchip 임베디드 독립 프로젝트를 진행할 것입니다. \"샘플\" 서브트리가 상당히 유혹적이지만, 우리가 선택한 칩과 관련이 없습니다. 독립 프로젝트는 실제로 다루기 매우 간단합니다.\n\n![image](/assets/img/2024-05-27-PICEcosystem_20.png)\n\n![image](/assets/img/2024-05-27-PICEcosystem_21.png)\n\n![image](/assets/img/2024-05-27-PICEcosystem_22.png)\n\n<div class=\"content-ad\"></div>\n\n다음 화면에서는 컴파일러를 선택할 수 있어요. 이 IDE에서는 이에 대한 유연성을 제공합니다.\n\n![이미지](/assets/img/2024-05-27-PICEcosystem_23.png)\n\n이전에 설치한 것을 사용하세요.\n\n![이미지](/assets/img/2024-05-27-PICEcosystem_24.png)\n\n<div class=\"content-ad\"></div>\n\n\"Set as main project\"를 선택한 채로 두면 여러 제어에서 이것을 기본 선택으로 지정할 수 있습니다. 아래에서 LED를 깜빡이는 코드가 간단하게 포함될 것입니다.\n\n![LED Blinking](/assets/img/2024-05-27-PICEcosystem_25.png)\n\n# 코드\n\n지금까지 진행한 내용으로 코드를 추가하는 시작점을 얻었습니다. 이를 위해 IDE의 projects 탭을 사용하여 Projects 창을 표시하고 싶을 것입니다. 아래 이미지에서 소스 파일 트리 노드가 비어 있는 것을 볼 수 있습니다 ('+' 표시가 없는 상태로 확장해도 파일이 없습니다).\n\n<div class=\"content-ad\"></div>\n\n아래는 표 형식을 Markdown 포맷으로 바꾸는 방법입니다:\n\n\n<img src=\"/assets/img/2024-05-27-PICEcosystem_26.png\" />\n\nWe will remedy that.\n\n<img src=\"/assets/img/2024-05-27-PICEcosystem_27.png\" />\n\nUsing this control, we can add a “main.c” file. ‘C’ is the language we will use for this program.\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-05-27-PICEcosystem_28.png)\n\n이미지를 삽입한 코드는 아래와 같습니다.\n\n/*\n\n* File: blinker.c\n\n\n<div class=\"content-ad\"></div>\n\n- 작성자: you\n\n-\n\n- 2024년 5월 23일 오전 12:21에 작성함\n\n\\*/\n\n<div class=\"content-ad\"></div>\n\n```c\n#include `xc.h`\n\nvoid main(void) '\n\nreturn;\n\n'\n```\n\n<div class=\"content-ad\"></div>\n\n이것은 그렇게 많은 것을 하지 않아. 하지만 완전한 프로그램인 건 맞아. 이제 하는 일은 세부 사항을 채우는 거야. 자세히 구성하면 요런 느낌이 될 거야.\n\n\n// PIC16LF18324 Configuration Bit Settings\n\n// ‘C’ source line config statements\n\n// CONFIG1\n\n\n<div class=\"content-ad\"></div>\n\n\n- FEXTOSC = LP: 외부 오실레이터 모드 선택 비트 (32.768 kHz에 최적화된 LP(크리스탈 오실레이터))\n- RSTOSC = HFINT32: COSC 비트의 전원 업 기본 값 (2x PLL과 함께 HFINTOSC(32MHz)가 기본값)\n- CLKOUTEN = ON: 클록 출력 활성화 비트 (CLKOUT 기능이 활성화되어 OSC2에서 FOSC/4 클록이 나타남)\n- CSWEN = OFF: 클록 전환 활성화 비트 (사용자 소프트웨어로 NOSC 및 NDIV 비트를 변경할 수 없음)\n\n\n<div class=\"content-ad\"></div>\n\n\n#pragma config FCMEN = OFF // 실패 안전 클럭 모니터 활성화 (실패 안전 클럭 모니터가 비활성화됨)\n\n// CONFIG2\n\n#pragma config MCLRE = OFF // 마스터 클리어 활성화 비트 (MCLR/VPP 핀 기능은 디지턼 입력; MCLR 내부적으로 비활성화됨; 약한 풀업은 포트 핀의 WPU 제어 비트에 따라 제어됨.)\n\n#pragma config PWRTE = ON // 파워업 타이머 활성화 비트 (PWRT 활성화됨)\n\n\n<div class=\"content-ad\"></div>\n\nMarkdown 형식으로 테이블 태그를 변경해주세요:\n\n\n| Configuration | Setting  |\n|---------------|----------|\n| WDTE          | OFF      |\n| LPBOREN       | ON       |\n| BOREN         | OFF      |\n| BORV          | HIGH     |\n\n\n<div class=\"content-ad\"></div>\n\n\n#pragma config PPS1WAY = OFF // PPSLOCK bit One-Way Set Enable bit (The PPSLOCK bit can be set and cleared repeatedly (subject to the unlock sequence))\n\n#pragma config STVREN = OFF // Stack Overflow/Underflow Reset Enable bit (Stack Overflow or Underflow will not cause a Reset)\n\n#pragma config DEBUG = ON // Debugger enable bit (Background debugger enabled)\n\n// CONFIG3\n\n\n<div class=\"content-ad\"></div>\n\n\n#pragma config WRT = ALL // User NVM self-write protection bits (0000h to 0FFFh write protected, no addresses may be modified)\n\n#pragma config LVP = OFF // Low Voltage Programming Enable bit (High Voltage on MCLR/VPP must be used for programming.)\n\n// CONFIG4\n\n#pragma config CP = ON // User NVM Program Memory Code Protection bit (User NVM code protection enabled)\n\n\n<div class=\"content-ad\"></div>\n\n\n#pragma config CPD = ON // 데이터 NVM 메모리 코드 보호 비트 (데이터 NVM 코드 보호 활성화)\n\n// #pragma 구문은 프로젝트 파일 포함보다 앞에 있어야 합니다.\n\n// ON 및 OFF에 대한 #define 대신 프로젝트 열거형 사용.\n\n#include `xc.h`\n\n\n<div class=\"content-ad\"></div>\n\n```c\n#define _XTAL_FREQ 32000000 // 20MHz 크리스탈 주파수로 정의\n\nvoid main(void) '\n\nTRISC0 = 0; // RC0 핀을 디지턀 출력 핀으로 설정\n\nwhile (1) '\n```\n\n<div class=\"content-ad\"></div>\n\n```c\nRC0 = 1; // RC0 핀을 논리 High로 설정하고 켭니다\n\n__delay_ms(250); // 1/4초의 지연을 추가합니다\n\nRC0 = 0; // RC0 핀을 논리 Low로 설정하고 끕니다\n\n__delay_ms(1000); // 1초의 지연을 추가합니다\n```\n\n<div class=\"content-ad\"></div>\n\nMCU(칩)의 핀이 올바르게 배선되면 이것이 칩에 적재되어 LED가 아래에 연결한 상태로 깜박이게 만들 것입니다.\n\n<div class=\"content-ad\"></div>\n\n코드의 시작 부분부터 (대부분을 포함하여) 이전에 언급한 'bit' 설정이 있습니다. 이러한 설정을 제거하면 'C' pragma 설정으로 주로 구성되어 있지만 실제로 다른 동작을 일으킬 수 있습니다. 이 설정은 실제로 자동으로 생성되므로 특별히 이해할 필요는 없습니다.\n\n이 코드에서 주목할 점은 선언되지 않은 변수에 대한 모든 참조입니다. 이들은 실제로 'register' 위치입니다. 마이크로컨트롤러(MCU)의 핀은 일반적으로 특수 하드웨어로 지원되며 매우 유연한 기능을 갖고 있습니다. 여기서 PIC은 유연성에서 뛰어나지만 선택을해야 하는 대가가 따릅니다. 이것은 Arduino 스케치와 같은 것과는 다른, 좀 더 복잡한 세계입니다. 여기서 RC0 핀은 먼저 설정해야 합니다. 다행히 이 작업은 꽤 간단하고 \"가능성이 높은\" 작업입니다 - 핀에 쓰기 작업을 수행해야 합니다. 다른 기능에는 더 많은 단계가 필요합니다.\n\n\"while(1)\"은 \"영원한 루프\"의 구현입니다. 계속해서 실행됩니다. 이 루프는 지정된 시간 간격으로 1과 0을 쓰면서 RC0 핀에서 높은 또는 낮은 전압을 생성합니다. 이전에 언급한 핀배치에 대해 아래에서 살펴보겠습니다.\n\n# 코드 번쩍이기\n\n<div class=\"content-ad\"></div>\n\n제공되는 우수한 도구 세트를 이용하여, 우리는 MCU에 코드를 'burn'하거나 'flash'할 수 있습니다. MCU에 프로그래밍을 하려면, 위에서 언급한 특별한 하드웨어를 통해 해야 합니다. 왜냐하면 MCU는 일반용 컴퓨터와는 달리 매우 독립적이기 때문입니다. 이 간단한 MCU의 연결성은 그 핀에 연결하는 모듈에 달려 있습니다. 인터넷 연결기나 무선 기능은 없지만, 자체 메모리(여러 유형)를 갖고 있으며, 프로그램을 플래시에 넣기 위해서는 (리셋이나 전원 차단 후에도 동일하게 유지될) 칩에 PICkit 4와 같은 장치를 사용하여 프로그램을 넣어주어야 합니다. 이러한 장치는 종종 특별한 전압 수준을 사용합니다. 이를 통해 칩이 실수로 재프로그램되지 않고 정상적으로 실행되도록 할 수 있습니다(그리고 'brick'이나 쓸모없어지는 것을 막을 수 있습니다).\n\n## 배선\n\n코드 플래싱 작업 중에, C 코드는 적절한 형식으로 컴파일됩니다. 이 때 브레드보드가 처음으로 등장합니다. 프로그램을 burn하기 위해서는 두 가지를 알아야 합니다. 첫째, PIC16LF18324 자체의 핀 배치도를 알아야 합니다.\n\n![PIC16LF18324 Pinout](/assets/img/2024-05-27-PICEcosystem_29.png)\n\n<div class=\"content-ad\"></div>\n\nPIC16(L)F183XX 제품 브리프에서 확인할 수 있어요.\n\n다음으로, 이 핀들이 PICKit4에 어떻게 연결되어야 하는지 알아야 해요.\n\n![이미지를 찾을 수 없습니다](/assets/img/2024-05-27-PICEcosystem_30.png)\n\nMPLAB PICkit 4 인-서킷 디버거 사용자 가이드에 이 자세한 내용이 있어요.\n\n<div class=\"content-ad\"></div>\n\n여기에는 특별히 중요한 핀이 있습니다. 핀 1은 MCLR 핀으로 \"액티브 로우\"입니다. 이 핀은 칩의 일부이기 때문에 정상 작동 시에는 이 핀을 \"풀드 하이\"로 유지해야 합니다. 우리는 PIC 칩의 해당 핀에서 브레드보드의 전원 레일로 이어지는 상당히 큰(5.1kΩ) 저항을 사용하여 이를 수행할 것입니다. 왼쪽에 있는 핀들의 중간에 있는 이 핀으로부터 실행되는 와이어는 위와 같이 정렬된 칩의 왼쪽 측 핀 중간에 있습니다. 그렇지 않으면, 프로그래머의 핀 1로 동일하게 연결해야 합니다. 그렇지 않으면, 다섯 개의 핀을 연결해야 합니다.\n\n# 배선\n\n지금까지 알게 된 내용을 고려하면, 여기서 설명하는 대로 연결해야 합니다.\n\n![PICEcosystem_31.png](/assets/img/2024-05-27-PICEcosystem_31.png)\n\n<div class=\"content-ad\"></div>\n\n\n![2024-05-27-PICEcosystem_32](/assets/img/2024-05-27-PICEcosystem_32.png)\n\n![2024-05-27-PICEcosystem_33](/assets/img/2024-05-27-PICEcosystem_33.png)\n\n\"top\"을 나타내는 칩 상단 가장자리의 작은 들여쓰기 위치를 주목해주세요. 그 위치에서 왼쪽에는 전원, 오른쪽에는 접지가 있습니다. 가능한 명확하게 하기 위해, 화살표로 가리킨 핀(PICkit의 핀 1)은 왼쪽에 있는 칩의 중간 핀과 연결되어 있습니다(거기에는 저항도 있습니다). 그게 바로 노란색 와이어입니다. 빨간색 VDD 와이어는 기판 왼쪽의 빨간 전원 레일에 연결되어 있습니다. 접지 와이어는 기판 오른쪽의 접지 전원 레일로 연결되어 있습니다. PIC 칩을 전원 공급하기 위해, 작은 빨간색 와이어가 맨 위/왼쪽 쪽 핀으로 이어지고, 작은 회색 와이어가 맨 위/오른쪽 쪽 핀으로 이어지고 있습니다. 파란 와이어는 접지 와이어 아래에 연결되어 있습니다. 하얀 와이어는 파란 와이어 아래에 연결되어 있습니다.\n\nPICkit 쪽에서, 노란 와이어는 핀 1에, 빨간 와이어는 핀 2에, 검정 와이어는 핀 3에, 파란 와이어는 핀 4에, 하얀 와이어는 핀 5에 연결되어 있습니다. 확인을 위해, 화살표 표식으로부터 가장 먼 PICkit 상에 3개의 빈 구멍이 보여야 합니다. 다시 말씀드리면 노란-빨간-검정-파란-하얀 순입니다.\"\n\n\n<div class=\"content-ad\"></div>\n\n# 플래시 데모\n\n![이미지](/assets/img/2024-05-27-PICEcosystem_34.png)\n\n플래시를 시도하기 전에 이 연결을 끊는 것이 더 원할한 경험을 보장할 수 있습니다.\n\n![이미지](/assets/img/2024-05-27-PICEcosystem_35.png)\n\n<div class=\"content-ad\"></div>\n\n이 콘트롤은 그림을 쓸 수 있도록 하는 것입니다.\n\n![image](/assets/img/2024-05-27-PICEcosystem_36.png)\n\n아래 화면이 나타납니다. 이 메시지를 다시 표시하지 않으려면 해당 상자를 체크해주세요. 그럼 플래시가 시작됩니다. 몇 가지 기기는 3.3V이고 다른 기기는 5V일 수 있으니 주의가 필요합니다. PIC16LF18324는 3.3V에서 작동합니다. 또한 IDE에서 설정을 해야 하는 문제가 하나 더 있습니다. 여기서도 이 경고를 남겨두었는데, 이것 역시 소홀히 할 수 있기 때문입니다.\n\n![image](/assets/img/2024-05-27-PICEcosystem_37.png)\n\n<div class=\"content-ad\"></div>\n\nPIC16LF18324는 3.3 볼트 장치입니다. 다른 장치에 대해 이 경고에 주의하십시오.\n\n# 주의\n\n이 설정에서 MCU를 프로그래밍 / 플래싱하기 전에 수행해야 할 단계가 있습니다. 아래 오류 메시지:\n\n![PICEcosystem_38](/assets/img/2024-05-27-PICEcosystem_38.png)\n\n<div class=\"content-ad\"></div>\n\n…기본 설정은 \"장치 전원이 켜짐\"임을 알려줍니다. 이것은 PICkit 4의 중요한 기능입니다. 장치에 전원을 공급하거나 자체 전원이 있는 장치에 프로그램을 할 수 있습니다. 그러나 설정은 안전을 우선시합니다.\n\n변경할 설정은 다음 순서로 찾을 수 있습니다:\n\n![이미지](/assets/img/2024-05-27-PICEcosystem_39.png)\n\n![이미지](/assets/img/2024-05-27-PICEcosystem_40.png)\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-05-27-PICEcosystem_41.png)\n\nTo be safe, after completing this step, check if the setting is still there. It might be helpful to click Debug / Disconnect debug tool after an unsuccessful attempt. Now, try the burning step again.\n\n![Image 2](/assets/img/2024-05-27-PICEcosystem_42.png)\n\nIf things don't go well, please double-check the wiring. If the wires are incorrect, you may encounter this situation.\n\n\n<div class=\"content-ad\"></div>\n\n![PICEcosystem_43](/assets/img/2024-05-27-PICEcosystem_43.png)\n\n# 재미있는 부분\n\n대부분의 프로그래머는 \"Hello, world\" 순간을 인지하고 있습니다. 이것은 작동하는 채크된 프로그램이 있다는 순간을 의미합니다. 새로운 프로그래밍 언어, 프레임워크 또는 시스템을 배우고 있을 때, 이것이 작동한다면 적어도 연결되어 있고 문법 오류가 없다는 것을 보여줍니다. 마이크로컨트롤러의 세계에서는 동일한 이정표는 주로 깜박이는 LED로 이루어집니다. 마이크로컨트롤러는 비디오 디스플레이를 필요로하지 않습니다. LED가 깜박이는 것은 중요합니다. 왜냐하면 실수로 LED를 전원에 연결할 수도 있기 때문이며(반듯한 저항 없이는 하지 마세요) 또는 잘못 구성된 핀에 연결해서 계속 켜져 있을 수도 있습니다. 깜박이면, 여러분이 실제 프로그램을 실행하고 일어나는 일들이 있음을 증명해줍니다.\n\n여기서 이것을 실현하기 위해 다른 배선이 필요합니다. 왼쪽 측면의 전원 레일에 3.3V 전원을 연결하고, 공급의 접지를 지시된 대로 접지 레일에 연결해야 합니다. 그런 다음, RC0는 18324의 오른쪽 하단에서부터 세 번째 핀입니다. 이것은 100에서 330옴 저항체에 연결되어야 합니다. 저항체의 다른 끝은 LED의 전방 편향 핀(또는 전원 수신 핀)에 연결돼야하며, 다른 핀은 다시 지지에 연결돼야 합니다. 0.1µF 커패시터의 두 리드를 빵판의 \"전원 레일\"에 눌러 넣으세요. 또한 두 지지 레일을 연결하는 선이 있는지 확인해야 합니다. 이것은 이를 연결하는 한 가지 가능한 방법일 뿐입니다. 중요한 것은 칩과 LED의 전원 및 접지, 핀이 LED에 연결되도록하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 전선 관련 이야기\n\n이곳에 보여지는 세팅은 매우 최소한입니다. 데이터 시트에서는 리셋 핀에 커패시터를 달아 볼트 스플리터를 사용하도록 권장합니다. 우리는 오직 하나의 저항만을 사용하고 있습니다.\n\n![이미지1](/assets/img/2024-05-27-PICEcosystem_44.png)\n\n![이미지2](/assets/img/2024-05-27-PICEcosystem_45.png)\n\n<div class=\"content-ad\"></div>\n\n코드와 배선이 올바르고 칩이 올바르게 프로그램되었다면 LED가 1초에 한 번씩 깜박일 것이며, 각 켜진 시간은 ¼초여야 합니다.\n\n중요한 주의: 이 기사에서는 저전력 운영에 대해 언급했지만, 여기서 사용된 입문용 코드는 그것을 달성하기에는 아주 멀었습니다. 이 칩으로 할 수 있는 최선의 방법이라고 생각하지 마십시오.\n\n# 결론\n\n이것으로 도구 세트의 요약을 마치며, 가능한 것을 시연하는 데 대한 데모를 제공했습니다. 물론 이것은 단순히 시작에 불과합니다. 이와 같은 MCU를 사용하면 수많은 다른 기능과 능력이 함께 제공됩니다. 1/0 입력을 읽거나 1/0 출력을 생성할 수 있는 이렇게 간단한 하드웨어조차도 다양한 방법으로 활용할 수 있으며, MCU로 할 수 있는 것은 훨씬 더 많습니다. 마이크로컨트롤러에서 코드를 실행하는 방법을 알면, 그 외의 모든 가능성을 탐험할 수 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-27-PICEcosystem_0.png"},"coverImage":"/assets/img/2024-05-27-PICEcosystem_0.png","tag":["Tech"],"readingTime":19},{"title":"블라인드 SQL Injection 관리자 패스워드 한 글자씩 알아내기-Lab9","description":"","date":"2024-05-27 13:06","slug":"2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9","content":"\n\n안녕 친구. 다시 오신 걸 환영합니다. 이번에도 이전 글을 이번 글의 끝에 링크하겠습니다.\n\n# Lab9: 조건부 응답으로 인한 Blind SQL Injection\n\n이 랩에는 Blind SQL Injection 취약점이 포함되어 있습니다. 애플리케이션은 분석을 위해 추적 쿠키를 사용하고, 제출된 쿠키 값이 포함된 SQL 쿼리를 수행합니다.\n\n<div class=\"content-ad\"></div>\n\nSQL 쿼리의 결과가 반환되지 않고 오류 메시지가 표시되지 않습니다. 그러나 쿼리가 어떤 행도 반환할 때 페이지에 \"다시 오신 것을 환영합니다\" 메시지가 표시됩니다.\n\n데이터베이스에는 사용자 이름과 비밀번호라는 열이 있는 다른 테이블인 사용자가 있습니다. 관리자 사용자의 비밀번호를 알아내기 위해 시각 SQL 인젝션 취약점을 악용해야 합니다.\n\n이 랩을 해결하려면 관리자 사용자로 로그인하세요.\n\n해결책\n\n<div class=\"content-ad\"></div>\n\n이 시나리오는 이전에 다룬 문제와 비슷해 보입니다. 이전에 작성한 글에서는 카테고리를 클릭하면 인터페이스의 왼쪽 상단에 표시되는 웰컴 백 메시지가 나타납니다.\n\n![이미지](/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_1.png)\n\n버프 스위트를 열고 몇 가지 요청을 보냈으며, 이 중 하나를 Repeater로 보내어 불리언 페이로드를 사용하여 SQLi 취약점을 가진 쿠키 값을 테스트했습니다.\n\n```js\n' AND 1=1-- # 웰컴 백 메시지를 받는 결과\n\n' AND 1=2-- # 웰컴 백 메시지를 받지 못함.\n```\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_2.png\" />\n\n내가 시도한 것은 다음과 같았어:\n\n```js\n' UNION SELECT username,password FROM users--\n```\n\n하지만 잘 되지 않았어. 심지어 시간 기반 페이로드도 작동하지 않았어.\n\n<div class=\"content-ad\"></div>\n\n수업 실습을 검토하면, 데이터를 추출하기 위해 substring을 사용할 수 있다는 제안이 있었습니다. 따라서, username이 \"administrator\"인 것을 알면, 비밀번호만을 추출할 필요가 있습니다.\n\n<img src=\"/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_3.png\" />\n\n우리의 페이로드는 다음과 같아야 합니다:\n\n```js\n' AND SUBSTRING((SELECT password FROM users WHERE username='administrator'), 1, 1) = 'a\n```\n\n<div class=\"content-ad\"></div>\n\n1 (시작 위치): 이는 부분 문자열 추출이 문자열(비밀번호)의 첫 번째 문자에서 시작해야 함을 지정합니다.\n1 (길이): 이는 하나의 문자만 추출해야 함을 나타냅니다.\n\n이것을 침입자에게 보내서 payload가 작동하는지 확인하기 위해 a=z로 대체하고 0-9로 자동으로 테스트하도록 지시합시다.\n\n![이미지](/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_4.png)\n\n<div class=\"content-ad\"></div>\n\n저희의 페이로드가 작동 중이에요. 아래 스크린샷을 확인해주세요. 컨텐츠 길이의 차이를 주목해 주세요. A에 도달하면 \"welcome back\" 메시지가 반환돼요.\n\n![이미지](/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_5.png)\n\n다음 단계로 넘어가야 해요. 다만, 패스워드의 길이를 모르기 때문에 패스워드를 알아내는 데 얼마나 시간이 걸릴지 알 수가 없어요. 수동으로 해야 할 것 같아요.\n\n다음에 사용할 페이로드는 다음과 같을 거에요:\n\n<div class=\"content-ad\"></div>\n\n```js\n' AND SUBSTRING((SELECT password FROM users WHERE username='administrator'), 2, 1) = 'a\n```\n\n![Blind SQL Injection](/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_6.png)\n\n![Blind SQL Injection](/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_7.png)\n\n제가 'a7eb5rsh00a9n7jffq9v'라는 패스워드를 추출했어요. 하지만, Burp Suite Repeater를 사용하여 확인해보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n페이로드:\n\n```js\n' AND SUBSTRING((SELECT username FROM users WHERE password='a7eb5rsh00a9n7jffq9v'), 1, 1) = 'a\n```\n\n![이미지](/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_8.png)\n\n관리자용으로 올바른 자격 증명이 필요합니다. 도전적이었지만 새로운 것을 배웠어요😊. 끝까지 머물러 주셔서 감사합니다. 재미있고 유익했다면 50번 클릭해주세요😊.","ogImage":{"url":"/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_0.png"},"coverImage":"/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_0.png","tag":["Tech"],"readingTime":3},{"title":"비기술 직군 팀에 SQL을 가르치면서 얻은 교훈","description":"","date":"2024-05-27 13:06","slug":"2024-05-27-LessonsfromTeachingSQLtoNon-TechnicalTeams","content":"\n\n## 조정된 방식에서 더 맞춤화된 방식으로 — 그리고 먼 거리에서의 코칭이 미래라고 생각하는 이유\n\n나의 경력 동안, 내가 내부 SQL 교육을 진행하는 다양한 상황에 처해왔습니다. 이러한 교육 세션은 항상 제게 최우선 순위는 아니었지만, 가장 만족스러운 프로젝트 중 하나였습니다. 누군가가 자신의 쿼리를 실행하는 데 익숙해지고, 스스로 필요한 정보를 찾아 대시보드를 작성하며, 이 새로 습득한 기술에 흥분하는 것을 보게 되면, 나는 몰라요 — 그냥 기분이 좋습니다.\n\n최근에, 예전에 교육받은 한 명의 \"학생\" 이름이 한 명의 어려운 SQL 질문을 하기 위해 공동 그룹에 등장한 것을 보았는데, 그때 나의 반응은 마치 \"다크 나이트 라이즈\"에서 알프레드가 브루스 웨인에게 고개를 끄덕이는 것과 같았습니다 (만약 이 참조를 모르시면 여기 있습니다).\n\n이 기사의 목표는 내가 내부 SQL 교육을 운영하면서 처한 내 여정과 배운 점을 전달하여 전체적으로 비기술적인 (또는 적어도 SQL을 잘 알지 못하는) 팀에게 가르칠 수 있는 방법을 알려주어, 당신의 조직에서 지식의 선물을 나누고 나처럼 유사한 기쁨을 느낄 수 있기를 희망합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-LessonsfromTeachingSQLtoNon-TechnicalTeams_0.png\" />\n\n# 처음으로 이 교육을 진행한 이유\n\n일반적으로, 나를 교육을 진행하게 한 상황은 크게 두 가지 범주로 나뉩니다:\n\n- 역량 강화 필요: 조직 내에서 SQL 역량 부족으로 인해 한계에 부딪히는 경우가 있습니다. 이는 여러 도구와 스프레드시트를 사용하여 최종 보고서에 도달하기 위한 혼잡한 프로세스의 출현으로 일반적으로 드러납니다. 당연히 해결책이 항상 SQL 쪽에 있는 것은 아니지만, 제 경험상 시간이 많이 소요되는 다중 단계 프로세스 중 하나를 보유하고 있고 내면에 더 나은 방법이 있다고 생각한다면, 아마도 그 방법이 있을 확률이 높습니다.\n- 자원 부족: 분석 관련 자원이 부족한 조직에서는 \"이웃 스킬\"을 갖춘 개인들(즉, 스프레드시트 및 데이터 작업에 익숙한 사람들)을 식별하고 역량 개발을 제안하는 것이 조직과 개인 양쪽에 매우 유익하다고 생각했습니다. 개인의 시야를 확대하면서 사업에 더 많은 가치를 창출할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이러한 교육을 진행하고 싶은 이유는 다양할 수 있습니다 (위 목록이 전부가 아닙니다; 이는 상호배타적인 것도 아니라는 주장이 나올 수 있습니다). 이곳에서 이루고자 하는 목표를 명확히 하는 것이 중요합니다. 목표에 따라 교육을 실행하는 방식이 크게 달라질 수 있습니다.\n\n# 초기 반복 사례 또는 \"만능\" 유형의 교육 한계를 발견한 방법\n\n2015년의 초기 버전에서 저는 점진적인 방식을 시도해 보았습니다. 보통의 교실 형식으로, X주 동안의 프로그램을 제공했습니다. 매주 1시간씩 수업을 진행했으며(항상 같은 요일 같은 시간에), SQL을 배우고자 하는 모든 관심 있는 사람들에게 열려 있었습니다. 주로 SQL에 초점을 맞춰 진행되었습니다.\n\n- 매주 그룹은 무언가 새로운 것을 배웠으며, SQL의 \"Hello World\"부터 시작해 (SELECT * FROM TABLE LIMIT 1) CTE 여러 개로 윈도우 함수를 사용하는 방법, 쿼리 최적화까지 모두 포함했습니다.\n- 각 수업 간에 그룹은 수업에서 배우는 지식을 시험하고 고착화하기 위해 숙제를 수행해야 했습니다.\n\n<div class=\"content-ad\"></div>\n\n일부 사람들은 끝까지 계속했지만 성공률(성공은 누군가가 교육 후에 새롭게 습득한 SQL 기술을 계속 사용하는 경우로 정의됨)은 극히 낮았습니다. 매 세션마다 오는 사람들이 점점 줄어들었습니다. 수업 외에 제안된 연습을 하는 사람은 소수였습니다. 사실적으로 말하자면 성공하지 못했어요.\n\n하지만 이로부터 많은 교훈을 얻었습니다:\n- 멘토링을 즐겼습니다: 다른 사람들에게 새로운 기술을 가르치고 지도하는 즐거움에 대해 배웠고, 결국 이 블로그와 다른 다양한 활동을 통해 보상을 얻었다.\n- SQL이 \"너무 기술적이다\"는 두려움: 많은 사람들이 그 무료 교육에 참여하지 않았거나 매우 첫 번째 장애물에서 포기했던 이유는 SQL을 기술적인 사람들만을 위한 것으로 생각했기 때문이고, 그들은 자신을 기술적인 사람으로 생각하지 않았기 때문입니다.\n- \"유지\" 메커니즘 없이 교육을 실시하는 것은 실패할 운명이다: 사람들이 이 교육을 완수할 수 있을 것이라는 사람들의 자율을 믿는 것은 합리적인 생각이 아님을 이해하게 되었습니다. 어떤 조직에서든 지속적인 교육을 완수하지 않을 수 있는 많은 경쟁 우선 순위와 사유가 있습니다. 따라서 학생들을 찾아내어 교육을 듣기 위해 강한 내재적 동기부여가 있는 사람들(예: SQL을 배우기 위한 명확한 목표가 있는 경우)이나 강력한 외부적 동기부여를 제공해야 합니다(예: 그들의 매니저가 SQL을 배우라고 요구하여 더 기술적인 프로젝트를 맡을 것을 기대하는 경우).\n- SQL을 가르치는 것은 방정식의 한 부분일 뿐입니다: 마지막으로, 더 중요한 것은 SQL을 가르치기 위해 SQL만을 가르치는 것이 중요하지 않다는 것을 깨달았습니다. 누구나 SQL을 고립시키지 않고 사용합니다. SQL의 현실은 다음과 같습니다:\n\n- SQL 코드를 작성하기 전에 조직 내에서 올바른 데이터 세트를 찾아야 합니다(성숙한 조직에서 쉬울 수 있지만, 성장하기 시작한 조직이나 존재하지 않는 조직에서는 복잡할 수 있습니다).\n- 데이터 세트를 찾았다면, 쿼리에 쓸 올바른 필드를 찾아야하고 이 필드가 원하는 정보를 담고 있는지 확인해야 합니다(이것 또한 하나의 기술입니다).\n- 그러고 나서 데이터 세트에 액세스 권한을 요청해야 하며, 액세스 권한이 승인되면 특정 가이드라인과 기능이 있는 특정 도구에 SQL 코드를 작성해야 합니다.\n- 쿼리를 작성하는 동안, 컴퓨팅 비용을 주시하고 쿼리를 실행하기 전에 필요에 따라 다시 구조를 잡아야 합니다.\n- 등등. 만약 이러한 요소들을 가르치지 않는다면, 학생들이 SQL을 사용하기 어려울 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 모든 배움이 나의 프로그램을 개선하는 데 길을 열었다 - 더 맞춤화된 방식으로.\n\n# 더 최근에 - 더 맞춤화된 방향으로의 전환\n\n위와 같이 개선된 몇 차례의 반복 뒤, 길을 따라 얻은 모든 배움을 반성하고 새로운 방식을 시도해 보았습니다: 규모를 잊고 완전히 반대 방향으로 나아갔습니다. 전체 반에서 매주 1시간 수업을 듣는 대신, 몇 명 선택된 개인들과 매주 짧은 1:1 세션을 갖기 시작했습니다.\n\n프로그램은 여전히 누구에게나 열려 있었지만, 이제는 참여할 수 있는 사람을 선정하는 프로세스가 있었습니다. \"들어가고 싶은\" 사람들은 다음을 보여 주어야 했습니다:\n\n<div class=\"content-ad\"></div>\n\n- SQL 학습에 대한 명확한 필요성: 잠재적인 학생들은 SQL을 배우고 싶은 이유와 SQL이 필요한 프로젝트를 설명하는 양식을 작성해야 했습니다(예: \"X 보고서를 자동화하고 싶어요, Y 대시보드를 구축하고 싶어요\"). 선택되면, 이 프로젝트가 프로그램 전체 기간 동안 작업할 프로젝트가 되었을 것입니다.\n- 이미 있는 인접한 기술들: 잠재적인 학생들은 \"인접 기술\"이라고 부르는 것을 보여줘야 했습니다. 즉, SQL이나 데이터 분석을 위한 필요한 기술과 유사한 기술들을 보여주어야 했습니다.\n- 일정에 충분한 시간을 할애할 수 있는 능력(및 의지): 프로그램에 대한 \"신청\"의 일환으로 - 학생들은 자신의 \"학습\" 프로젝트를 자신의 관리자와 검증하고 다음 X 주 동안 적어도 X0%의 시간을 학습에 할애할 의지가 있어야 했습니다. X0%는 많이 보일 수 있지만 - 사실 그것은 X0%에 관한 것이 아닌 메시지를 보내는 것이었습니다. 이 프로그램은 시간이 많이 소요되므로 잠재적인 학생들은 성공하기 위해 필요한 시간을 할애할 수 있는지 확인해야 했습니다.\n\n교육 자체에 대해서 - 초점은 SQL에서 프로젝트로 전환되었습니다. 교육의 첫 세션은 그들의 프로젝트를 마일스톤으로 나누는 데 시간을 보냈습니다. 첫 번째 마일스톤은 모두에게 동일했습니다: SQL 기초를 배울 수 있는 (온라인 또는 오프라인, 무료 또는 유료 - 본인이 선호하는 것으로) 자원을 찾아내고 완료하는 것입니다.\n\n저는 이것이 약간 실망스러울 수 있다는 것을 인식하고 싶습니다 - \"SQL 가르치기\"에 대한 글을 \"SQL 학습\" 부분에서 그렇게 탐내지 않을 것이라고 기대할 수 있습니다. 저의 일반적인 신념은 SQL의 주요 개념을 매우 짧은 시간에 배울 수 있지만, 실제로 여러 달 또는 몇 년이 걸려야 진정으로 뛰어난 수준에 도달할 것이라고 생각하며, 가능한 빨리 실제 문제에 적용하기 시작하면 더 빨리 견고한 수준에 도달할 수 있다고 생각합니다. 따라서 프로그램의 대부분은 실제 문제에 적용하는 데 소요되며, 기본적인 SQL을 이해하는 데 (인터넷의 훌륭한 것을 통해 쉽게 얻을 수 있는 것) 소요되는 시간에 대해 많은 시간이 소요되는 것은 아닙니다.\n\n위의 첫 번째 단계가 완료되면, 우리는 다음 마일스톤을 향해 노력할 것입니다. 예를 들어, 대시보드를 구축하고 싶은 사람을 위해 프로젝트를 분할해보면:\n\n<div class=\"content-ad\"></div>\n\n- SQL 기초 학습\n- 적절한 데이터셋 및 쿼리 로직 찾기 (필요한 정보 획득 방법 학습)\n- 필요한 경우 데이터베이스 구축 (데이터베이스 구축과 관련된 역할 및 책임)\n- 이 데이터베이스를 대시보드 도구에 연결\n- 대시보드 설계\n- 대시보드 작성\n\n그리고 여기서, 매주 다양한 이정표에 도달할 것으로 예상했습니다. 학생들은 주에서 어디서 걸릴 경우 언제든지 제게 피드백을 요청하거나 이메일을 보낼 수 있었지만, 일반적으로 진행 상황에 따라 독립적으로 이정표를 완수해야 했습니다.\n\n이 시스템을 통해 낮은 실패율을 관찰했습니다 (성공은 훈련 후에 새로 습득한 SQL 기술을 계속 사용하는 사람으로 정의됩니다). 이때 뒷받침되는 이유들을 곰곰히 생각해보면 이러한 이유가 있습니다:\n\n- 선발 과정 추가: 더해진 선발 과정은 실제 프로젝트를 가진 가장 동기가 부여된 사람들만 훈련의 일부가 되도록 보장했습니다.\n- 이정표 시스템은 강제 기능이 좋았습니다: 목표를 설정하는 것은 훌륭한 시작입니다, 그러나 목표를 달성하기 위해 필요한 단계나 궁극적으로 목표를 달성할 때 필요한 작업에 대해 고민해보지 않으면 목표를 달성할 가능성이 적습니다. 명확한 마감일 아래에서 명확한 결과물을 제공하는 이정표 시스템은 학생의 성장을 크게 도와주는 책임감과 피드백 루프를 만들어냅니다.\n- 처음부터 올바른 기대 설정이 모든 것을 더욱 단순하게 만들었습니다: 어떤 것을 성공으로 이끌어가는 큰 부분은 마음가짐과 일에 대한 우리의 인식과 연관이 있다고 믿습니다. 이 프로그램을 시작하자마자, 올바른 기대 설정을 하려고 노력했습니다: (1) 시간이 많이 들 것이다 (2) 도전적일 것이다 (3) 오랜 기간이 걸릴 것이다\n- (4) 그러나 시간을 투자하고 도전을 하나씩 극복하겠고, 궁극적으로 승리할 것입니다\n- 사람들에게 SQL 학습 방법을 가르치는 것 대 SQL을 가르치는 것: 마지막으로 — 이 주요 변경 사항이 프로그램에서 큰 차이를 만들었습니다. 이것은 사용자들이 필요한 핵심 정보를 찾아내고 실험하며 배우면서 익숙해지도록 했습니다. 그들이 더 자립적이 되어 계속成长할 수 있도록 했고, 프로그램이 종료된 후에도 지속적으로 발전할 수 있도록 했습니다.\n\n<div class=\"content-ad\"></div>\n\n지금까지 위의 방법은 제가 시도한 가운데 가장 성공적인 하나입니다. 그러나 시간이 많이 소요되고 개선할 여지가 많이 보입니다.\n\n# 보다 하이브리드 방식으로\n\n이 시점에서 가장 중요한 질문은 다음과 같습니다: 위의 프로그램을 어떻게 확장할 수 있을까요? 이 교육에서 제가 한 역할을 반성해보면, 주로 방향을 제시하고 사람들을 정직하게 유지하는 데 중점을 두었습니다:\n\n- 처음에: 학생들이 프로젝트를 구조화하고 단계별로 나누는 데 도움을 주었습니다.\n- 각 단계마다: 각 장애물에 접근하는 가장 좋은 방법에 대한 조언을 했습니다. 만약 막힌다면, 어떻게 해제할지에 대한 지침을 제공했습니다.\n- 프로그램 전반에 걸쳐: 그들의 승리를 축하하고 도전하며, 힘들 때 동기부여를 시도했지만, 동시에 그들이 설정한 일정 내에 무슨 일을 해야 하는지 제시했습니다.\n\n<div class=\"content-ad\"></div>\n\n위의 내용을 자동화하는 것은 어렵지 않을까요? 혹은 혹시 LLMs를 이용해서 가능할지도 모르겠네요. 요즘 세상은 뭐가 되는지 모르겠어요. 그래도 어떻게든 표준화하고 최적화할 수 있고, 비동기적으로 많은 작업을 처리할 수 있으니 매주 회의를 필요로 하지 않는 방식으로 개선할 수 있을 거예요. 다음 반복에서 저는 학생 당 소요 시간을 줄여서 더 많은 학생들을 교육할 수 있는 방법을 시도해보고 싶네요.\n\n저자 주: 요즘 피트니스 인플루언서들이 “거리에서의 코칭”을 제공하고 있는 것을 점점 더 많이 보게 되는데, 여기서 코치와 이메일로 소통하고 훈련 영상을 보내며 맞춤형 프로그램을 받을 수 있어요. 데이터 분석에서도 비슷한 방법이 있을 수 있을까요?\n\n프로그램 자체에 대해, “커뮤니티” 요소를 통합하고 싶어요. 특히, 페이만 기법을 강력하게 신봉하는 편인데요. 페이만 기법은 배운 것을 가르치는 것인데요. 구체적으로 말하자면, 학생들이 배운 내용을 문서화하고 새로운 학생들에게 공유하도록 유도하고 싶어요 (마치 영화 “이웃에게 선물하기”처럼 말이에요). 여기에는 몇 가지 장점이 있을 거 같아요:\n\n- 이를 통해 프로그램의 규모를 확장할 수 있고, 더 많은 사람들이 지식을 활용할 수 있게 될 거예요\n- 이제는 선생님인 학생들이 이해해야 할 핵심 개념을 내재화하고 자신의 이해의 빈틈을 찾아낼 수 있게 도와줄 거에요\n- 거대한 지식 베이스를 시작할 수 있고, 그러면 프로그램에 참여할 수 없는 고도로 동기 부여된 개인들을 위한 자기 서비스 접근 방식을 더 활용할 수 있게 될 거에요.\n\n<div class=\"content-ad\"></div>\n\n항상 그렇지만, 아이디어는 쉽게 얻을 수 있어요. 실행 단계에서 어떤 것이 잘 작동하고 어떤 것이 그렇지 않은지를 이해하게 돼요. 곧 그것을 실험해 보고, 나중에 미래의 글에서 결과를 공유할 예정이에요.\n\n# 결론\n\n지난 8년 동안, 동료와 보고서를 SQL 전문가로 발전시키는 여러 프로그램을 시도해봤어요. 항상 성공한 것은 아니었지만, 몇 년 전에 광범위한 프로그램에서 좀 더 맞춤화된 멘토십으로 전환한 것이 많은 성공과 유익한 교훈을 안겨줬어요.\n\n지금 나의 진정한 도전은 그 방법을 확장하는 것이에요. 어떻게 하면 선택된 개인들을 위해 최대한 가치를 창출하는 데 집중하기 위해 모든 불필요한 것을 단순화하고 제거할 수 있을까요? 그렇게 하면 그들이 자신의 조직에서 일으키는 영향력을 10배로 향상시킬 수 있게 될 거에요. 아마도 피트니스 인플루언서들이 뭔가를 알고 있을지도 몰라요…\n\n<div class=\"content-ad\"></div>\n\n# 이 글을 즐겁게 읽으셨기를 바랍니다! 공유하고 싶은 조언이 있으시면 댓글 섹션에 남겨주세요!\n\n그리고 더 많이 읽고 싶으시다면, 아마도 다음 게시물들도 맘에 드실지도 몰라요:\n\nPS: 본 글은 다양한 분석 업무 경험을 바탕으로 얻은 지식을 담은 뉴스레터인 'Analytics Explained'에 동시 게시되었습니다. 싱가폴 스타트업부터 SF 대형 기업까지에서 배운 내용을 요약하고, 독자들의 분석, 성장, 경력에 관한 질문에 답변하고 있습니다.","ogImage":{"url":"/assets/img/2024-05-27-LessonsfromTeachingSQLtoNon-TechnicalTeams_0.png"},"coverImage":"/assets/img/2024-05-27-LessonsfromTeachingSQLtoNon-TechnicalTeams_0.png","tag":["Tech"],"readingTime":8},{"title":"세계에서 가장 강력한 SQL LLM을 구축하는 Snowflake의 방법","description":"","date":"2024-05-27 13:03","slug":"2024-05-27-HowSnowflakebuildingthemostpowerfulSQLLLMintheworld","content":"\nAI가 데이터 민주화를 위한 새로운 기회를 만들어냈습니다. SQL을 필요로 하지 않고 언어 모델 이해력의 힘을 이용하여 통찰력을 얻는 것으로 많은 사용자가 현재 대량의 데이터에 감춰진 가치 있는 통찰을 발견할 수 있게 되었습니다.\n\n지난 달, Snowflake의 공학 부사장인 Vivek Raghunathan이 Fully Connected 컨퍼런스에서 혁신적인 Snowflake Copilot에 대해 이야기했습니다. 이 비디오는 지난 주 YouTube에서 공개되었습니다. WrenAI 팀은 몇 달 동안 텍스트를 SQL로 변환하는 작업을 진행해왔고, 우리는 Vivek이 최근에 공유한 SQL LLM 주제에 대한 생각과 기술을 많이 배웠습니다. 오늘, 이 비디오에서 얻은 생각과 교훈을 정리하고자 합니다. 이것이 다른 개발자들이 텍스트를 SQL로 더 빨리 혁신할 수 있도록 도움이 되기를 바랍니다!\n\n만약 전체 비디오에 관심이 있다면, 아래 링크를 확인해보세요!\n\n자, 시작해봅시다!\n\n<div class=\"content-ad\"></div>\n\n# 데이터 분석에서 인공지능의 현재와 미래\n\nVivek은 데이터 분석에서 인공지능의 현재와 미래에 대해 이야기했습니다.\n\n## 현재: 당신을 돕는 인공지능\n\n- 분석가를 위한 대화형 도우미\n- 자연어를 SQL로 전환: 분석가가 SQL을 실행\n- 인터페이스를 통해 반복적인 데이터 및 스키마 발견\n\n<div class=\"content-ad\"></div>\n\n하지만 미래에는 비즈니스 사용자들을 위한 완전한 대화형 기사로 발전할 것입니다. SQL을 모르는 사람들도 자연어로 질문하고 답변을 받을 수 있어서 더 많은 개인들이 능력을 얻을 것입니다.\n\n## 미래: 의존할 수 있는 AI\n\n- 비즈니스 사용자들을 위한 대화형 기사\n- 답변을 위한 자연어: SQL 전문 지식 불필요\n- 상호작용 가능한 데이터 및 시각화를 허용하는 인터페이스\n\n## 우리의 생각:\n\n<div class=\"content-ad\"></div>\n\nWrenAI를 개발하면서도 이를 경험했습니다. 현재 기술 혁신의 한계로 모든 비즈니스 사용자를 대상으로 한 완전한 AI 팔로우업 서비스를 제공하는 것은 여전히 제한적인 면이 있습니다. 이는 자율 주행 자동차에서 경험하는 것과 비슷합니다. 수년 동안 자동차 제조사들은 운전자와 동행하는 자율 주행 시스템을 출시해왔습니다. 자율 주행 기능을 통해 고객과 운전자 경험이 전환되는 것을 확인할 때까지는 여전히 데이터 분석가들의 도움이 필요할 것입니다.\n\n# 현실 세계에서 Text-to-SQL은 그리 간단한가요?\n\nVivek는 Text-to-SQL이 마치 빙산 문제와 같다고 언급했습니다. 문제는 표면상 간단해 보이지만 실제로는 매우 복잡하다고합니다.\n\n대화 중 인용문:\n\n<div class=\"content-ad\"></div>\n\n그의 강연에서 언급된 몇 가지 분명한 도전 과제들은 빠르게 직면하게 될 것입니다:\n\n- 현실 세계는 지저분한 스키마와 데이터를 가지고 있으며, 종종 수만 개의 테이블과 수십만 개의 열이 포함된 데이터베이스가 있습니다.\n- 현실 세계의 의미론은 더 복잡합니다: rev1, rev2 및 rev3로 레이블이 지정된 열을 가진 테이블이 있을 수 있지만, 수익 열은 무엇인가요? 미국 달러인가요, 아니면 현지 통화입니까? 몇 주 전에 전송된 이메일에서 폐기되었나요? 이 중 어느 것이 현재 진실의 근원인가요?\n- 테이블 간에는 올바르게 결합할 수 있는 여러 가지 방법이 있어 더욱 복잡해집니다.\n\n## 저희의 생각\n\n정말 간단한 문제가 아니네요!\n\n<div class=\"content-ad\"></div>\n\nWrenAI는 데이터와 의미 사이의 도전을 해결하는 데 주안점을 둡니다; 텍스트에서 SQL로 신뢰할 수 있는 변환을 만들기 위한 핵심은 기존 데이터 구조의 위에 의미 아키텍처에 대응하는 신뢰할 수 있는 의미 엔진을 구축하는 방법입니다. 실제로, 의미 관계, 계산, 집계를 정의하는 것과 같이, LLMs는 서로 다른 시나리오에서 서로 다른 문맥을 다루는 방법을 배워야 합니다. 이는 견고한 의미적 계층에 많이 의존합니다.\n\n# Snowflake 실험 v0부터 v4까지\n\nSnowflake는 v0부터 v4까지 여러 번의 실험을 거쳤습니다. 다행히도 Vivek은 개선된 텍스트에서 SQL로의 혁신을 위해 다음 버전에서 시도하고 배운 내용을 너그럽게 공유했습니다.\n\n자세히 알아봅시다!\n\n<div class=\"content-ad\"></div>\n\n# V0: Optimized for Spider\n\n대화에서 Vivek은 말했습니다:\n\n## 처음으로 직면한 빙산 (도전):\n\nV0 버전에서는, 최상의 모델을 사용하여 스파이더 데이터셋을 사용하여 82%의 결과를 달성하였지만, 최적화되지 않은 Zero-shot GPT-4는 74%의 결과를 가져왔습니다. 그러나 실제 세계 데이터에서는, 최상의 모델을 사용하여 정확도가 9%로 떨어지고, prompt-optimized된 GPT-4를 사용하면 14%의 결과를 얻었습니다.\n\n<div class=\"content-ad\"></div>\n\n그들이 중요성을 깨닫기 시작한 때입니다. \"의미론적 카탈로그\"는 의미론이 데이터 검색에 엄청 중요하다는 것을, 사전 훈련된 LLMs가 귀하의 비즈니스 콘텍스트에 대해 아무것도 모르기 때문에 유일한 방법은 RAG를 통해 의미론을 제공하는 것입니다.\n\n## 우리의 생각:\n\n의미론은 텍스트에서 SQL로의 전환 도전을 해결하는 데 핵심적이며, WrenAI를 구현하기 시작할 때 중심적인 핵심 설계입니다.\n\n# V1: 실세계에서 검색이 중요합니다\n\n<div class=\"content-ad\"></div>\n\nV1 버전에서는 Snowflake 팀이 고민하며 시작했습니다. 웹 품질의 검색을 기업 메타데이터 검색에 적용하고 그 결과를 LLM(언어 모델)에 공급한다면 성능이 혁신적으로 향상될 수 있다는 아이디어가 나왔죠.\n\n다시 말해, 이 LLM에 어떤 내용을 포함해야 할지라는 더 단순한 문제를 해결하기 위해 보다 어려운 문제를 먼저 해결하려고 합니다.\n\n아래는 결과인데, Snowflake의 최고 모델은 9%에서 24%로 개선되었고, GPT-4 버전도 14%에서 28%로 성장했습니다. 따라서 의미론적 카탈로그 검색이 중요하다는 직관이 옳다는 것이 입증되었습니다.\n\n![Snowflake](/assets/img/2024-05-27-HowSnowflakebuildingthemostpowerfulSQLLLMintheworld_0.png)\n\n<div class=\"content-ad\"></div>\n\nVivek 씨가 말했던 것처럼, Snowflake에서 대화 카탈로그 검색 결과를 어떻게 검색하는지 설명했어요.\n\n## 그들이 직면한 두 번째 어려움(도전):\n\nSnowflake 팀이 직면한 다른 어려움은 레이터입니다. 모든 것을 수작업으로 다시 주석을 다는 작업을 했고, 성능을 평가하기 위해 더 복잡한 예제를 추가했으며, 단일 테이블 대 복수 테이블 및 간단한 조인 대 복잡한 조인과 같은 데이터 의미론에 기반하여 슬라이스를 나눴다.\n\n아래에는 Vivek가 그의 발표에서 공유한 내용이 있습니다:\n\n<div class=\"content-ad\"></div>\n\n## 우리의 생각:\n\n이 통찰은 우리에게 흥미로워 보입니다. WrenAI를 구현할 때 검색 최적화를 많이 하지 않았습니다. 현재는 벡터 저장소에서 Top-N 선택만 사용합니다. 검색 기술의 더 자세한 세부 사항을 살펴보고, 이 영역을 개선하기 위한 작업을 진행 중입니다.\n\n우리 팀은 또한 내부 검증 데이터 세트를 구축하고 있습니다. 이 데이터 세트는 단일 및 다중 테이블, 그리고 단순 및 복잡한 조인 데이터 세트를 포함하여 보다 복잡한 시나리오를 갖도록 설계되어 있습니다. 이로써 솔루션이 실제 세계에서 정확하게 유지되도록 합니다.\n\n이 주제에 대한 토론을 위한 새로운 이슈를 오픈해 주시면 환영입니다! 우리 팀은 더 많은 개선사항을 살펴보기를 원합니다!\n\n<div class=\"content-ad\"></div>\n\n# V2: Text2SQL 모델링 통찰\n\n다음으로, Vivek은 이러한 발전에도 불구하고 모델이 대화 능력에서 여전히 어려움을 겪고 있다고 공유했습니다. SQL 작업을 최적화했지만 대화를 처리하고 지시를 따르는 것에 어려움이 있었다고 합니다.\n\n아래는 그들이 공유한 몇 가지 통찰입니다:\n\n- 더 나은 기본 LLMs: 코드 LLMs가 SQL 작업에서 아주 잘 수행된다는 것이 밝혀졌습니다.\n- 더 나은 신호: 몇 가지는 LLM 생성에서 나오며, 예를 들어 주석 등이 있습니다. 일부는 스노우플레이크 문서 등의 고전적인 기술에서 얻어집니다. 그 중 하나는 쿼리 이력인데, 실제 사람들이 실제로 하는 작업에 관한 보물창고입니다.\n- 사고의 연결: 처음에는 테이블을 선택하고, 그 다음에 조인하고, 그 다음에 열을 선택하고, 그리고 마지막으로 디코드 시에 정확성을 확인합니다. LLM이 JSON을 생성할 때는 출력이 스키마와 일치하는지 확인하는 의존성 파서가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 새로운 깔끔한 평가를 통해 상당한 개선을 확인했습니다. 기본 모델 27%가 39%가 되었고, GPT-4를 사용한 경우 40%가 46%로 상승했습니다.\n\n## 그들이 마주한 세 번째 빙하(도전):\n\n다음 도전 과제는 제로 샷 텍스트 - SQL 솔루션 대신 대화형 공동 조종사를 구축하는 것입니다. 대화를 처리하고 분석가들이 쿼리를 점진적으로 개선할 수 있도록 해야 합니다.\n\n시스템의 한 부분을 최적화하면 전체 시스템이 의도치 않게 최적화되지 않을 수 있습니다. 두 가지 큰 문제가 발생했습니다:\n\n<div class=\"content-ad\"></div>\n\n- 모델은 지시에 따라 더 이상 우수하지 않았습니다. 왜냐하면 텍스트-SQL 작업만을 보았기 때문입니다.\n- 이는 대화에서 메모리가 떨어졌습니다. 여러 턴의 경우가 아닌, 제로샷 사례만을 경험했기 때문입니다.\n\n## 저희의 생각\n\n이 강의에서 언급한 텍스트-SQL 도전 과제는 제로샷 방식이 아닌 상호작용적인 접근이 필요합니다. WrenAI에서도 이러한 접근을 추구하고 있으며, 지속적으로 개선하고 있는 몇 가지 경험을 진행 중에 있습니다.\n\n우리의 구현을 통해 \"증강 및 생성\" RAG 파이프라인에서 검증, 수정 및 명확화 다이얼로그와 같은 정교한 프로세스를 구현했습니다. 이를 통해 LLMs가 사용자의 의도를 완전히 이해할 수 있도록 지원하고 있습니다. 물론, 계속해서 개선할 부분이 많이 남아 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Snowflake](/assets/img/2024-05-27-HowSnowflakebuildingthemostpowerfulSQLLLMintheworld_1.png)\n\n## V3: 지시 따르기, 도구 사용\n\nVivek은 텍스트를 SQL로 바꾸는 작업과 더 일반적인 지시 따르기 작업을 섞어 LLM을 지시 따르도록 다시 교육했다. 그리고 이를 Multi-LLM 설정에 오케스트레이터 LLM으로 계층화했다.\n\n오케스트레이터 모델의 책임은 고객과 대화하는 것입니다. 이는 주체적인 방식과 유사합니다. SQL을 작성해야 할 때마다 다른 크기의 텍스트를 SQL 모델을 사용하도록 허용했습니다. 지능적으로 작업을 위임하는 이 방법은 많은 문제를 해결해 주었습니다.\n\n\n<div class=\"content-ad\"></div>\n\n그리고 숫자들은 더욱 향상되었어요. 최고 모델은 이제 추가된 지시 따르기 능력으로 38%에서 41%로 상승했고, GPT-4 플러스 최적화를 통해 46%의 평가도달을 이루었어요.\n\n### 그들이 직면한 네 번째 얼음산 (도전):\n\n46.4%에서 99%로 나아가면, 텍스트-투-SQL의 목표는 SQL을 모르는 비즈니스 사용자를 위한 대화형 조종사를 구축하는 것이에요. 이것이 기회이며, 그들은 99%의 정확도가 필요해요.\n\n### 우리의 생각\n\n<div class=\"content-ad\"></div>\n\nWrenAI에서는 텍스트-SQL의 미래에 대해 낙관적입니다. 우리는 LLM 혁신이 숨막히게 빠르게 진행되고 있어 곧 LLM이 의미적 맥락을 통해 인간 수준의 이해력에 가깝게 발전하여 이상적인 데이터 민주화 세계를 이룰 것이라 믿습니다.\n\n# V4: 정확도 99%로 향상\n\n이제 스노우플레이크에서 진행 중입니다! 고객이 제공한 의미적 맥락을 활용하여 메트릭 및 조인 경로 등을 이해합니다.\n\n## 우리의 생각\n\n<div class=\"content-ad\"></div>\n\n눈송이에서 더 많은 정보 공유를 기대하고 있어요! 정말 흥미로워요!\n\n# 모든 수업의 최종 결론\n\n마지막으로, Vivek이 모든 도전을 경험하고 얻은 주요 교훈과 배운 점을 아래에 정리했어요.\n\n- 👏 제품은 전체 e2e 시스템입니다: LLM 모델링뿐만 아니라요\n- 👏 의미 체계 카탈로그 검색은 중요합니다: 실제 LLM 검색 엔진으로 강화하기\n- 👏 SQL 주석 품질이 중요합니다: 주석 처리 담당자는 전문가여야 해요\n- 👏 대화형 SQL은 LLM의 불사조 문제입니다: 복잡한 명령어 튜닝, 사고 과정 연결, 도구 사용\n- 👏 99%까지 가려면 돌파구가 필요합니다\n\n<div class=\"content-ad\"></div>\n\n그게 다에요! Vivek 씨와 Snowflake 팀이 이번 토롤에서 많은 귀중한 교훈을 나눠 주셨어요. 당신으로부터 많은 것을 배웠습니다!\n\n앞으로도 기대됩니다!\n\nWrenAI를 확인해보지 않았다면, 방문해보세요!\n\n👉 GitHub: https://github.com/Canner/WrenAI\n\n<div class=\"content-ad\"></div>\n\n👉 X: [https://twitter.com/getwrenai](https://twitter.com/getwrenai)\n\n👉 Medium: [https://blog.getwren.ai/](https://blog.getwren.ai/)\n\n이 글을 즐겨 보셨다면 깃헙에서 ⭐ WrenAI에 스타를 주시는 걸 잊지 마세요 ⭐ 언제나 독자 여러분 감사합니다.\n","ogImage":{"url":"/assets/img/2024-05-27-HowSnowflakebuildingthemostpowerfulSQLLLMintheworld_0.png"},"coverImage":"/assets/img/2024-05-27-HowSnowflakebuildingthemostpowerfulSQLLLMintheworld_0.png","tag":["Tech"],"readingTime":7},{"title":"SQL 트랜잭션 및 ACID 속성","description":"","date":"2024-05-27 13:02","slug":"2024-05-27-SQLTransactionsandACIDProperties","content":"\n![image](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_0.png)\n\n# SQL에서 거래 소개\n\nSQL을 데이터베이스로 사용하는 은행 시스템을 상상해봅시다. 사용자 A가 사용자 B의 계좌로 돈을 입금하려고 합니다. 돈을 송금하면 사용자 A의 계좌 잔액에서 해당 금액을 빼내고 이 돈을 사용자 B의 계좌에 입금하려는데, 갑자기 데이터베이스가 크래시될 경우 어떻게 될까요?\n\n![image](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_1.png)\n\n<div class=\"content-ad\"></div>\n\n사용자 A의 잔고에서 인출한 돈이 사라졌다는 것을 의미합니까? SQL 데이터베이스에서는 그렇지 않습니다. 왜냐하면 SQL 트랜잭션을 사용하기 때문입니다.\n\n# 트랜잭션과 ACID 속성\n\n트랜잭션은 단일 원자 단위로 수행되는 하나 이상의 SQL 작업 시퀀스입니다. 이는 데이터베이스에서 데이터 일관성을 보장하기 위한 목적으로 사용됩니다. 트랜잭션은 주로 ACID 약어로 불리는 다음 속성을 갖습니다:\n\n- 원자성: 전체 트랜잭션은 전체가 성공하거나 전체가 실패하는 단위로 처리됩니다.\n- 일관성: 트랜잭션은 데이터베이스를 하나의 유효한 상태에서 다른 유효한 상태로 변환시키며 데이터베이스 불변을 유지합니다.\n- 고립성: 동시에 실행되는 트랜잭션에 의해 수행된 수정 사항은 서로 분리되어 커밋될 때까지 격리됩니다.\n- 지속성: 트랜잭션이 커밋되면 시스템 장애가 발생하더라도 지속되어 유지됩니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-SQLTransactionsandACIDProperties_2.png\" />\n\n# SQL 트랜잭션에서의 중요 명령어\n\nSQL 트랜잭션의 시작을 BEGIN TRANSACTION 키워드로 표시합니다.\n\n<img src=\"/assets/img/2024-05-27-SQLTransactionsandACIDProperties_3.png\" />\n\n<div class=\"content-ad\"></div>\n\n모든 트랜잭션 중에 발생한 변경 사항을 저장하려면 변경 사항을 데이터베이스에 COMMIT합니다.\n\n![이미지](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_4.png)\n\n트랜잭션 중에 문제가 발생하면 ROLLBACK 명령을 사용하여 트랜잭션 중에 수행된 모든 변경 사항을 되돌릴 수 있으며 데이터베이스를 트랜잭션 시작 시점의 상태로 되돌릴 수 있습니다.\n\n![이미지](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_5.png)\n\n<div class=\"content-ad\"></div>\n\n## 예시\n\n우리 간단한 은행 애플리케이션 예제로 돌아가 봅시다. 여기서는 계좌 A에서 계좌 B로 $100을 이체해야 합니다. 이 과정은 두 단계로 이루어집니다:\n\n- 계좌 A에서 금액을 차감하기\n- 그 금액을 계좌 B에 추가하기\n\n이 두 가지 단계는 모두 성공적으로 완료되어야 합니다. SQL 트랜잭션으로 이를 어떻게 작성할 수 있는지 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n\nBEGIN TRANSACTION;\n\n- Account A 잔액에서 100을 차감합니다.\n  UPDATE Accounts\n  SET balance = balance - 100\n  WHERE account_id = 'A';\n  -- Account A에 충분한 잔액이 있는지 확인하고, 부족하다면 롤백합니다.\n  IF @@ROWCOUNT = 0\n  ROLLBACK;\n  -- Account B 잔액에 100을 추가합니다.\n  UPDATE Accounts\n  SET balance = balance + 100\n  WHERE account_id = 'B';\n  -- 모든 것이 잘 되었다면 트랜잭션을 커밋합니다.\n  COMMIT;\n\n\n이 트랜잭션은 다음을 수행합니다:\n\n- 트랜잭션을 시작하여 다음 작업이 단일 원자적 프로세스의 일부임을 보장합니다.\n- Account A에서 $100을 차감합니다: account_id와 balance 열이 있는 'accounts'라는 테이블이 있다고 가정합니다.\n- Account A에 충분한 자금이 있는지 확인합니다: Account A에 충분한 금액이 없으면, ROLLBACK TRANSACTION을 사용하여 모든 변경 사항이 취소되는 롤백이 수행됩니다.\n- Account B에 $100을 추가합니다: Account A에 충분한 금액이 있다면, Account B에 $100이 추가됩니다.\n- 트랜잭션을 커밋합니다: 두 개의 업데이트가 모두 성공적으로 수행되면, COMMIT TRANSACTION 명령이 실행되어이 트랜잭션 중에 수행된 변경 사항을 영구적으로 적용합니다.\n\n이것은 두 계정이 적절히 업데이트되거나 어느 시점에서 문제가 발생할 경우 변경 내용이 적용되지 않으므로 데이터의 무결성이 유지됩니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n# SQL 트랜잭션의 격리 수준\n\n데이터베이스 트랜잭션의 격리 수준은 트랜잭션 무결성이 유지되는 방식 및 각 트랜잭션이 다른 트랜잭션과 얼마나 격리되는지를 결정합니다.\n\nSQL 표준은 일관성과 성능 간의 균형을 맞추기 위해 네 가지 격리 수준을 정의합니다.\n\n## 1. Read Uncommitted\n\n<div class=\"content-ad\"></div>\n\n- 설명: 격리의 가장 낮은 수준입니다. 트랜잭션이 커밋되기 전에 다른 트랜잭션이 작업한 변경 사항을 볼 수 있습니다.\n- 예시에 미치는 영향: 돈을 송금하는 도중에 다른 트랜잭션이 계좌 A 또는 B의 잔액을 업데이트하고 있다면, 이 트랜잭션은 이러한 커밋되지 않은 값을 읽을 수 있습니다. 이는 실제로 존재하지 않는 잔액을 보는 등의 문제를 발생시킬 수 있습니다 (다른 트랜잭션이 실패하고 롤백될 경우).\n\n## 2. Read Committed\n\n- 설명: 트랜잭션이 커밋된 데이터만 읽을 수 있도록 보장합니다.\n- 예시에 미치는 영향: 이 수준은 'Read Uncommitted'의 문제를 피하기 위해 커밋된 계좌 A와 B의 잔액만 읽도록 합니다. 하지만 트랜잭션 내에서 잔액을 여러 번 읽는 경우, 다른 트랜잭션이 데이터를 수정하는 경우 다른 값들을 보게 될 수 있습니다 (반복할 수 없는 읽기).\n\n## 3. Repeatable Read\n\n<div class=\"content-ad\"></div>\n\n- 설명: 거래가 데이터를 두 번째로 읽을 때 동일한 데이터 값을 찾을 수 있도록 보장합니다(반복되지 않는 읽기를 피함).\n- 예시에 미치는 영향: 이 수준은 거래 내에서 동일한 데이터의 여러 번의 읽기 사이에 다른 사람에 의해 생긴 변경 사항을 볼 수 없도록 합니다. 이는 잔액 확인 및 업데이트 작업 중 일관된 읽기 결과를 유지하는 데 도움이 됩니다. 그러나 다른 거래에 의해 추가된 새로운 행이 발생하는 유령 읽기를 막을 수는 없을 수도 있습니다.\n\n## 4. Serializable\n\n- 설명: 최고 수준의 격리. 거래가 직렬로 실행된 것처럼 완전히 격리됩니다.\n- 예시에 미치는 영향: 이는 완전한 격리를 보장합니다. 다른 거래가 이체 프로세스에 간섭할 수 없습니다. 모든 동시성 문제(더티 리드, 반복되지 않는 읽기 및 유령 읽기)를 방지하지만 동시성이 감소하고 잠금으로 인한 잠재적 성능 문제가 발생할 수 있습니다.\n\n다양한 격리 수준에서 여러 현상이 발생할 수 있으며, 더티 리드, 반복되지 않는 읽기 또는 유령 읽기와 같은 현상이 발생할 수 있습니다. 아래에서 이 용어가 의미하는 바를 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n# Dirty Reads\n\n더티 리드는 트랜잭션이 동시에 커밋되지 않은 다른 트랜잭션에 의해 작성된 데이터를 읽을 때 발생합니다. 결과적으로 다른 트랜잭션이 롤백하면 처음 트랜잭션이 데이터를 읽게 됩니다. 하지만 해당 데이터는 데이터베이스에 공식적으로 기록된 적이 없습니다.\n\n예시:\n\n- 트랜잭션 1이 시작되고 계좌 A에서 계좌 B로 $100을 이체합니다.\n- 트랜잭션 1이 커밋되기 전에 트랜잭션 2가 시작되고 계좌 A의 잔액을 읽습니다.\n- 트랜잭션 1이 실패하고 롤백되면, 트랜잭션 2는 공식적으로 커밋되지 않은 잔액을 읽게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n\n![image](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_6.png)\n\n# Non-Repeatable Reads (Read Uncommitted)\n\n이것은 트랜잭션 진행 중 같은 행이 두 번 조회되고, 두 번 조회 사이에 행 내의 값이 다른 경우 발생합니다. 본질적으로 다른 트랜잭션이 두 번의 조회 사이에 행을 수정한 경우입니다.\n\n예시:\n\n\n\n<div class=\"content-ad\"></div>\n\n- 거래 1이 시작되고 계정 A의 잔액을 읽습니다.\n- 거래 2가 계정 A에서 계정 B로 $100을 이체하고 커밋합니다.\n- 거래 1이 다시 계정 A의 잔액을 읽으면 이전과 다른 잔액을 볼 수 있습니다.\n\n![이미지](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_7.png)\n\n# 환상 읽기\n\n환상 읽기란 다른 트랜잭션에서 읽고 있는 레코드에 새로운 행이 추가되거나 (또는 기존 행이 삭제되는 경우) 트랜잭션 중에 발생합니다. 이는 동일한 트랜잭션에서의 후속 읽기가 원래 읽기의 일부가 아니었던 새로 추가된 행을 포함하거나 삭제되지 않은 행을 제외한 행 집합을 반환할 수 있음을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n예시:\n\n- Transaction 1은 계정 A의 거래 수를 세는 쿼리를 시작합니다.\n- Transaction 2는 계정 A에 새로운 거래 기록을 삽입하고 커밋합니다.\n- Transaction 1은 다시 계정 A의 거래 수를 세어보고 이전보다 더 많은 거래를 발견합니다.\n\n![그림](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_8.png)\n\n높은 격리 수준은 발생할 수 있는 현상의 종류를 줄이지만 더 낮은 동시성과 잠재적인 성능 영향을 감수해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n만약 SQL 데이터베이스의 확장 메커니즘에 대해 더 알고 싶다면, 데이터베이스 샤드, 복제 등을 다루는 깊이 있는 Database Essentials 비디오를 확인해보세요:\n\n여기 처음 오신 분들을 위해, 저는 Hayk입니다. 저는 웹 개발자 분들이 첫 번째 기술 직을 확보하거나 웹 개발 마스터리 커뮤니티에서 시니어 역할로 진출하는 데 도와드리고 있어요.\n\n웹 개발에 대한 주간 통찰력을 놓치고 싶지 않다면, 내 뉴스레터를 구독해주세요.\n\n","ogImage":{"url":"/assets/img/2024-05-27-SQLTransactionsandACIDProperties_0.png"},"coverImage":"/assets/img/2024-05-27-SQLTransactionsandACIDProperties_0.png","tag":["Tech"],"readingTime":6},{"title":"90의 작업을 처리할 수 있는 10가지 SQL 문장","description":"","date":"2024-05-27 13:00","slug":"2024-05-27-10SQLStatementsThatCanHandle90ofTasks","content":"\n구조화된 쿼리 언어(SQL)는 관계형 데이터베이스를 관리하고 질의하는 강력한 도구입니다. 초보자든 경험 많은 데이터 전문가든 상관없이, 여러분은 자주 사용하게 될 특정 SQL 문을 발견할 것입니다. 본 문서에서는 데이터베이스 작업의 90%를 처리할 수 있는 10가지 필수 SQL 문을 다룹니다. 코드 예시를 함께 제공할 것입니다.\n\n![이미지](/assets/img/2024-05-27-10SQLStatementsThatCanHandle90ofTasks_0.png)\n\n# 1. 소개\n\n# SQL의 중요성\n\n<div class=\"content-ad\"></div>\n\nSQL은 관계형 데이터베이스와 상호 작용하기 위한 표준 언어입니다. 데이터를 검색하거나 데이터베이스 구조를 수정하는 등 다양한 작업을 수행할 수 있습니다. SQL을 이해하는 것은 데이터 작업을 하는 사람에게 필수적이며, 데이터 분석, 보고 및 애플리케이션 개발을 위한 기초를 제공합니다.\n\n# 2. SELECT 문\n\n# 데이터 검색\n\nSELECT 문은 하나 이상의 테이블에서 데이터를 검색하는 데 사용됩니다. 검색할 열을 지정하고 결과를 필터링할 조건을 추가할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 테이블에서 모든 열을 검색합니다\nSELECT * FROM employees;\n\n-- 특정 열을 검색합니다\nSELECT first_name, last_name FROM employees;\n\n-- 결과를 필터링하기 위해 조건을 추가합니다\nSELECT product_name, price FROM products WHERE price > 50;\n```\n\n# 3. INSERT INTO 문\n\n# 새 데이터 추가\n\nINSERT INTO 문을 사용하면 테이블에 새로운 데이터 행을 추가할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 단일 행 삽입\nINSERT INTO customers (first_name, last_name, email) VALUES ('John', 'Doe', 'john@example.com');\n\n-- 여러 행 삽입\nINSERT INTO orders (order_date, total_amount) VALUES\n    ('2023-01-15', 150.00),\n    ('2023-01-16', 220.50),\n    ('2023-01-17', 75.25);\n```\n\n# 4. UPDATE 문\n\n# 기존 데이터 수정하기\n\nUPDATE 문은 테이블의 기존 데이터를 수정하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 하나의 행 업데이트하기\nUPDATE products SET price = 25.99 WHERE product_id = 101;\n\n-- 여러 행 업데이트하기\nUPDATE employees SET manager_id = 105 WHERE department = 'Sales';\n```\n\n# 5. DELETE 문\n\n# 데이터 삭제\n\nDELETE 문은 테이블에서 행을 제거하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 한 행 삭제\nDELETE FROM customers WHERE customer_id = 201;\n\n-- 조건을 충족하는 모든 행 삭제\nDELETE FROM orders WHERE order_date < '2023-01-15';\n```\n\n## 6. CREATE TABLE 문\n\n### 새 테이블 생성\n\nCREATE TABLE 문은 지정된 열과 데이터 유형을 가진 새 테이블을 생성하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n\nCREATE TABLE products (\n    product_id INT PRIMARY KEY,\n    product_name VARCHAR(255),\n    price DECIMAL(10, 2)\n);\n\n\n## 7. ALTER TABLE 문\n\n## 테이블 수정\n\nALTER TABLE 문을 사용하면 기존 테이블을 추가, 수정 또는 삭제하여 테이블을 수정할 수 있습니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 새 열 추가\nALTER TABLE employees ADD COLUMN hire_date DATE;\n\n-- 열 데이터 유형 수정\nALTER TABLE customers ALTER COLUMN phone_number VARCHAR(15);\n```\n\n# 8. DROP TABLE 문\n\n# 테이블 삭제하기\n\nDROP TABLE 문은 기존 테이블과 해당 데이터를 모두 삭제하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 테이블 삭제\nDROP TABLE products;\n```\n\n## 9. WHERE 절\n\n## 데이터 필터링\n\nWHERE 절은 지정된 조건에 따라 행을 필터링하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 가격이 50보다 큰 제품 조회\nSELECT product_name, price FROM products WHERE price > 50;\n\n-- 영업 부서의 직원 조회\nSELECT first_name, last_name FROM employees WHERE department = 'Sales';\n```\n\n# 10. JOIN 절\n\n# 여러 테이블에서 데이터 결합\n\nJOIN 절을 사용하여 서로 관련된 열을 기반으로 두 개 이상의 테이블에서 행을 결합합니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 고객 이름과 주문 조회하기\nSELECT c.first_name, c.last_name, o.order_date\nFROM customers AS c\nJOIN orders AS o ON c.customer_id = o.customer_id;\n```\n\n# 11. GROUP BY 절\n\n# 데이터 집계\n\nGROUP BY 절은 특정 열의 값을 가진 행을 그룹화하는 데 사용되며, 종종 SUM 및 COUNT와 같은 집계 함수와 함께 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 각 제품별 총 매출 계산\nSELECT product_id, SUM(quantity * price) AS total_sales\nFROM order_details\nGROUP BY product_id;\n```\n\n# 12. 결론\n\n# 기초 마스터\n\n이 10가지 SQL 문은 관계형 데이터베이스 작업 시 대부분의 작업을 다룹니다. 이 문장들을 이해하고 숙달함으로써 데이터베이스 관리 및 데이터 조작에 대한 견고한 기초를 갖게 될 것입니다. SQL은 다양한 기능을 제공하는 언어이며, 데이터 작업에 더욱 강력한 방법을 발견하면서 더욱 쉽게 작업할 수 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# SQL 기초 지식\n\n시간을 내어 주셔서 감사합니다! 🚀\nSQL 기초 지식에서 더 많은 콘텐츠를 찾아보실 수 있어요! 💫\n","ogImage":{"url":"/assets/img/2024-05-27-10SQLStatementsThatCanHandle90ofTasks_0.png"},"coverImage":"/assets/img/2024-05-27-10SQLStatementsThatCanHandle90ofTasks_0.png","tag":["Tech"],"readingTime":4},{"title":"사용 기반 API 요금 청구 및 미터링을 위한 실시간 분석 솔루션","description":"","date":"2024-05-27 12:58","slug":"2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering","content":"\n\n![Real-Time Analytics Solution](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_0.png)\n\nDisclaimer: The author of this article is a Developer Advocate at Redpanda, which is a critical component of the solution discussed. The author also brings prior expertise in API Management and Apache Pinot to the table. Hence, the proposed solution is a combination of these technologies aimed at solving a prevalent problem.\n\nAn API business refers to a company that packages its services or functionalities as a set of API (Application Programming Interface) products. These APIs can be sold to new and existing customers, who can then integrate these functionalities into their own applications. The company can generate revenue by charging these customers based on their usage of the APIs.\n\nA company operating an API business needs a data infrastructure component to track API call volume and bill consumers accordingly.\n\n\n<div class=\"content-ad\"></div>\n\n이 게시물에서는 Apache APISIX, Redpanda 및 Apache Pinot를 사용하여 실시간 API 사용 추적 솔루션을 구축하기 위한 참조 아키텍처를 제시합니다. 이 게시물은 \"어떻게\"보다는 \"왜\"에 중점을 두었습니다. 이를 솔루션 설계 연습으로 간주하고 심층 튜토리얼이 아니라는 것을 고려해 주세요. 저는 솔루션 패턴을 청사진으로 추출하여 향후 프로젝트에서 재사용할 수 있도록 돕고자 합니다.\n\n다른 말 없이 시작해 봅시다.\n\n# APIs 및 API 관리\n\nAPI 및 API 관리 개념에 대해 처음이라면, 먼저 간단히 소개해 드리겠습니다.\n\n<div class=\"content-ad\"></div>\n\n디지털 비즈니스에서 API는 비즈니스 작업에 프로그래밍 방식으로 액세스할 수 있도록 해줘서 인간을 제외할 수 있어요. 이러한 비즈니스 작업에는 주문 생성, 자금 이체, CRM에서 고객 주소 업데이트 등이 포함될 수 있어요.\n\n비즈니스에서 전형적인 API 환경에는 세 가지 당사자가 관련돼요:\n\n- 백엔드 시스템 — 비즈니스 작업을 실행하는 시스템이에요.\n- 소비자 — 비즈니스 작업을 사용하려는 1차 및 3차 애플리케이션이에요.\n- API 프록시 — 프록시로서 작용하며 중간자 역할을 하는 요소에요.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_1.png)\n\n<div class=\"content-ad\"></div>\n\nAPI의 역할은 내부 비즈니스 시스템을 소비자로부터 분리하여, 소비자가 백엔드 시스템의 복잡성을 처리할 필요 없이 이를 제공하는 것입니다. 이러한 방식으로, API는 추상화 계층 역할을 합니다. API는 HTTP를 포함한 다양한 통신 프로토콜을 통해 작동하며, RESTful 및 GraphQL API 스타일을 볼 수 있습니다.\n\n운영 중에는 조직이 API 수명주기의 다양한 단계를 관리하기 위해 전체 수명주기 API 관리 플랫폼을 사용합니다. API 프록시 디자인, 배포, 런타임 정책 참여 및 모니터링과 같은 API 수명주기의 각 단계에 대한 전용 구성 요소를 번들로 제공하는 API 관리 플랫폼이 있습니다. 이 문서의 문맥에서 Apache APISIX를 사용할 것이며, 이는 Apache 라이선스 하에 배포되는 오픈 소스 API 관리 플랫폼입니다.\n\n그렇다고 해서 여기서 구축하는 솔루션이 APISIX와 무조건적으로 결합된 것은 아닙니다. 시장에서 구할 수 있는 대부분의 전체 수명주기 API 관리 제품과 통합할 수 있습니다. 단, 적합한 통합 인터페이스를 제공해야 합니다.\n\n![Real-Time Analytics Solution for Usage-Based API Billing and Metering](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_2.png)\n\n번역 시 일부 용어는 컨텍스트에 맞게 번역되었습니다.\n\n<div class=\"content-ad\"></div>\n\n# API를 활용하여 수익을 창출하는 방법\n\n이제, API를 활용하여 수익을 창출하는 방법에 대해 살펴봅시다. 즉, 사용량에 따라 소비자에게 요금을 부과하는 수익 모델을 찾아보는 것입니다.\n\n더 나은 설명을 위해 현실적인 예시를 들어보겠습니다.\n\n부동산 감정 회사를 고려해보세요. 이 회사는 주택 구매자와 판매자에게 즉각적인 부동산 평가를 제공합니다. 이 평가는 우편번호, 부동산 유형, 지역과 같은 간단한 요소를 기반으로 합니다. 현재, 이 회사는 웹 기반 사용자 인터페이스만 제공하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_3.png)\n\n비지니스 운영을 확장하고 더 많은 고객을 유치하며 새로운 시장 세그먼트에 진입하기 위해 이 회사는 API 비지니스로 진출하기로 결정했습니다. 이 말은 평가 엔진을 API 제품 세트로 패키징하여 새로운 및 기존 소비자에게 판매하고 그들의 API 호출 사용량에 따라 청구할 것을 의미합니다.\n\n이를 위해 먼저 평가 엔진을 분리하고 API 관리 플랫폼 뒤에 배치하여 달성합니다. 이를 통해 소비자들이 일련의 API를 통해 기능에 액세스할 수 있게 됩니다.\n\n![image](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_4.png)\n\n\n<div class=\"content-ad\"></div>\n\n평가 API는 부동산 회사, 은행, 보험사, 정부 등 다양한 분야의 잠재 고객들을 유치할 것입니다:\n\n- 부동산 회사 — 주택 구매자를 위한 정확한 평가값 제공.\n- 은행 — 모기지 승인 전 주택 평가.\n- 보험 제공업자 — 주택 및 내용 보험에 대한 더 정확한 견적 제공.\n- 정부 — 부동산 세금 쉽게 계산.\n\n## API 수익화 모델\n\n이 회사는 어떻게 수익을 창출할까요? 평가 API를 API 제품 세트로 포장하여 구독 계층과 함께 판매하세요!\n\n<div class=\"content-ad\"></div>\n\n가입 등급은 소비자가 매달 고정된 API 호출 횟수를 사용할 수 있는 할당량입니다. 그 할당량을 초과하면 사용자는 제한을 받거나 초과 사용량에 대해 요금을 지불해야 합니다.\n\n예를 들어, 가치평가 API는 다음과 같이 세 가지 가입 등급으로 제공될 수 있습니다.\n\n- 브론즈: 매달 10,000건의 요청\n- 골드: 매달 100,000건의 요청\n- 플래티넘: 매달 무제한의 요청\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_5.png)\n\n<div class=\"content-ad\"></div>\n\n고객은 예상 사용량에 따라 다양한 티어 중에서 선택하여 API에 가입할 수 있습니다. 한 달의 끝에 회사는 실제 사용량을 기반으로 고객에게 청구할 것입니다.\n\n우리의 목표는 각 고객의 API 사용량을 효율적이고 신뢰할 수 있는 방법으로 측정하는 것입니다.\n\n# 솔루션 계획\n\n이제 우리가 해결하려는 문제를 이해했으니, 구현에 들어가기 전에 몇 가지 설계 결정사항을 설명해 드리겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## KPI 지표\n\n첫 번째 단계는 솔루션으로부터 기대하는 KPI 또는 지표를 식별하는 것입니다. 특히 다음 다섯 가지에 관심이 있습니다.\n\n- API 사용량 — 시간당 소비자 당 API 호출 횟수\n- API 지연 — 느린, 느린 API를 식별하기 위한 종단 간 지연 시간\n- 고유 사용자 — API 당 고유 호출 수는?\n- 지리적 사용량 분포 — 주로 API 사용자가 어디에서 왔는가?\n- 오류 수 — 호출 오류가 많으면 백엔드에 문제가 있다는 것을 의미합니다.\n\n이상적으로 이런 것들이 모두 이렇게 시각화된 대시보드에서 보고 싶습니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 마크다운 형식으로 표시 변환한 코드입니다.\n\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_6.png)\n\n## 이해 관계자\n\n두 번째 디자인 결정은 솔루션 이해 관계자 - 이러한 지표를 전달해야 하는 대상. 주로 세 가지 당사자가 있습니다.\n\n고객 및 협력사 - 소비자는 실시간 대시 보드에서 할당량 사용량과 청구 추정을 확인하는 것을 좋아합니다.\n\n\n<div class=\"content-ad\"></div>\n\nAPI 운영 팀 - 이 팀은 API 관리 인프라를 관리합니다. API의 건강 정보에 특히 관심이 있으며, 지연시간, 처리량, 오류 등을 주로 다룹니다.\n\nAPI 제품 팀 - 이 팀은 API 제품을 소유하고 있습니다. 그들은 소비자의 인구 통계, API 중에서 더 인기 있는 것이 무엇인지 등을 확인하기 위해 즉시 실험을 실행하고 싶어 합니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_7.png)\n\n## 일괄 처리 또는 실시간 처리?\n\n<div class=\"content-ad\"></div>\n\n최종 디자인 결정으로, 실시간 및 일괄 메트릭 사이에 80:20의 분할을 설정하겠습니다.\n\n데이터는 시간이 경과함에 따라 가치를 잃습니다. 데이터를 빨리 처리할수록 적절한 조치를 취할 수 있습니다. 그래서 우리는 API 트래픽 및 헬스 메트릭을 실시간으로 처리하겠습니다.\n\n소비자 API 키가 유출된 상황을 생각해보세요. 악의적인 API 클라이언트가 훔친 키나 인증 토큰을 사용하여 소비자를 대신해 API를 호출할 수 있습니다. 시스템은 이 API 키에서의 급격한 트래픽 증가를 감지하여 비정상으로 식별하고 키를 차단하면서 소비자에게 경보를 보낼 수 있습니다. 경보를 받은 소비자는 즉시 API 키를 재발급하여 비용을 최소화할 수 있습니다.\n\n그러나 모든 사용 사례가 실시간 처리를 필요로 하는 것은 아닙니다. 어떤 사용 사례는 자연스럽게 일괄 처리에 적합합니다.\n\n<div class=\"content-ad\"></div>\n\n- 고객을 위한 월별 사용량에 기반한 청구 보고서.\n- 업무팀을 위한 주간 API 건강 보고서.\n- 제품팀을 위한 매일 API 트래픽 보고서.\n\n# 구현\n\n지금은 이 기사의 중간 지점에 도달했으며 지금까지의 토론을 바탕으로 다음 솔루션 아키텍처를 제시합니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_8.png)\n\n<div class=\"content-ad\"></div>\n\n다이어그램이 복잡하고 많은 알 수없는 기술이 있어서 알겠습니다. 그래서, 세 개의 레이어로 나누어서 각각에 대해 자세히 설명하겠습니다.\n\n## 데이터 수집\n\n이전에 언급했듯이, API 관리 시스템은 디자인 및 런타임 측면에서 트래픽 모양 만들기, 인증 및 구독 관리와 같은 API 라이프사이클 관리 작업을 수행하는 여러 이동 부품을 가지고 있습니다. 이에 대한 추가 측면도 있습니다.\n\n![image](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_9.png)\n\n<div class=\"content-ad\"></div>\n\n그러나, 우리가 가장 관심 있는 구성 요소는 API 게이트웨이입니다. 이 곳은 모든 API 트래픽이 백엔드로 흐르는 곳입니다.\n\n우리의 첫 번째 작업은 API 게이트웨이에서 접촉점을 확인하는 것입니다. 이를 통해 왕복하는 API 요청과 응답을 수집할 수 있습니다. 그런 다음 이 정보를 실시간으로 분석용 데이터 저장소로 이동시키는 데이터 파이프라인을 구축할 것입니다. 이를 통해 향후 질의를 용이하게 할 것입니다.\n\n그러나, 이 쓰기 경로를 구현할 때 직접 데이터를 기본 데이터 저장소에 쓰는 것은 여러 문제점을 야기할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nAPIM 시스템과 분석 인프라 사이의 강한 결합 - 나중에 새 APIM 공급업체로 전환할 때 분석 인프라의 상당 부분을 다시 작성해야 할 수도 있습니다.\n\n동기 쓰기 - 운영 중에 두 시스템 모두 사용 가능해야 하므로 분석 시스템을 유지보수 목적으로 중지하기 어려울 수 있습니다.\n\n확장 가능한 데이터 수집 - API 게이트웨이의 돌발적인 트래픽 급증으로 인해 분석 시스템이 과부하되어 두 시스템 모두 협조하여 확장해야 할 수 있습니다.\n\n이로 인해 APIM과 분석 인프라를 분리하는 방법을 모색할 필요가 있습니다. Apache Kafka와 같은 스트리밍 데이터 플랫폼은 API 게이트웨이에서 높은 처리량 데이터 스트림을 낮은 지연 시간으로 수신할 수 있으므로 여기에 적합할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n해당 솔루션에서는 성능과 간편함 측면에서 Kafka보다 우월한 Redpanda, Kafka API 호환 스트리밍 데이터 플랫폼을 사용할 것입니다. 하지만 만약 Kafka만 사용하길 원한다면 괜찮습니다. 해당 솔루션은 두 기술에 모두 매끄럽게 작동합니다.\n\nRedpanda를 중심으로 한 데이터 파이프라인은 다음과 같이 구성됩니다:\n\n![Redpanda Data Pipeline](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_11.png)\n\nRedpanda의 추가로 두 시스템이 분리되었고 쓰기 경로가 비동기로 동작합니다. 이는 분석 시스템이 유지 보수를 위해 오프라인 상태로 들어갈 수 있고, 중단된 지점부터 다시 재개할 수 있도록 합니다. 게다가 Redpanda는 갑작스러운 트래픽 급증을 흡수하여 분석 시스템이 과부하를 받거나 API 게이트웨이에 맞춰 스케일링할 필요가 없도록 해줍니다.\n\n<div class=\"content-ad\"></div>\n\n이제 APISIX와 Redpanda 사이의 연결을 어떻게 만들어야 할지 궁금할 것입니다. 다행히도, APISIX는 Kafka를 위한 내장 데이터 싱크를 제공합니다. 게이트웨이로 API 요청이 발생하고 응답이 반환될 때, 이 싱크는 실시간으로 Kafka 토픽에 레코드를 발행합니다. 우리는 Redpanda와 함께 이 싱크를 사용할 수 있습니다. 왜냐하면 Redpanda가 Kafka API와 호환되기 때문입니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_12.png)\n\nAPISIX는 개별 API 호출을 JSON 이벤트로 형식화하고 지연 시간, HTTP 상태 및 타임스탬프와 같은 중요한 메트릭을 포함시킵니다. 이러한 정보들은 분석 데이터 저장소에서 관련 있는 차원으로 매핑될 것입니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_13.png)\n\n<div class=\"content-ad\"></div>\n\nAPI 관리 플랫폼에 Kafka 싱크가 없는 경우 어떻게 할까요? 그럼 대안으로 API 게이트웨이의 HTTP 액세스 로그를 Kafka로 스트림 처리할 수도 있습니다. 이를 위해 Filebeat나 유사한 도구를 사용할 수 있습니다.\n\n## 분석 데이터베이스\n\n이제 Redpanda에 API 호출 이벤트가 랜딩되고 있으니, 다음 단계는 적합한 분석 데이터베이스 기술을 식별하는 것입니다.\n\nOLTP 데이터베이스, 키-값 저장소 또는 데이터 웨어하우스가 될 수 있을까요? 다음 기대 기준에 따라 각각을 평가해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n- 스트리밍 데이터 수집 - 실시간 데이터 원본인 Kafka에서 가져와야 합니다. 여기서는 배치 데이터 로딩이 없어야 합니다. 스트리밍 수집은 더 높은 데이터 신선도를 보장합니다.\n- 낮은 지연 쿼리 - 쿼리 지연 시간은 하위 초 범위 내여야 하며 사용자를 위한 분석 대시보드를 만족시켜야 합니다.\n- 높은 쿼리 처리량 - 사용자를 대상으로 하는 분석 대시보드에서 동시에 발생하는 쿼리를 처리할 수 있어야 하며 지연 시간을 훼손하지 않아야 합니다.\n\n위의 모든 조건을 충족하는 분석 데이터베이스로 Apache Pinot을 선택했습니다.\n\nApache Pinot은 실시간 분산 OLAP 데이터베이스로, 스트리밍 데이터에서 OLAP 워크로드를 처리하도록 설계되어 극히 낮은 지연 시간과 높은 동시성을 제공합니다. Pinot은 Kafka와 원활하게 통합되어 Kafka 주제에서 데이터가 생성될 때마다 실시간 수집을 허용합니다. 수집된 데이터는 색인이 작성되고 열 형식으로 저장되어 효율적인 쿼리 실행을 가능케 합니다.\n\n아키텍처에서 Pinot을 사용하면 엔드 투 엔드 데이터 파이프라인은 다음과 같이 보입니다. Pinot은 Redpanda와 API 호환성으로 원활하게 통합됩니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_14.png\" />\n\n파이프라인에 스트림 프로세서가 필요한가요? 정말 필요한지는 사용 사례에 따라 다릅니다.\n\n스트림 프로세서 대신 Redpanda의 Wasm 기반 인브로커 데이터 변환을 사용하여 API 이벤트 페이로드에서 민감한 필드를 제거할 수 있습니다. 그러나 아파치 Flink와 같은 상태 보유형 스트림 프로세서는 다음과 같을 때 파이프라인에 더 많은 가치를 더할 수 있습니다:\n\n- 실시간 결합 및 보강이 필요할 때 — 핀오토로 전달할 추가 차원이 필요하며, 이는 여러 스트림을 결합하여 파생할 수 있습니다. 예: IP 지오코딩.\n- 알림 — 사용량의 이상 현상을 기반으로 알림을 트리거하고 하류 이벤트 주도형 워크플로를 실행합니다.\n\n<div class=\"content-ad\"></div>\n\n## Serving layer\n\n우리의 분석 데이터 파이프라인이 이제 완성되었습니다. 모든 파이프라인 구성 요소는 데이터 인프라 구조층에 있습니다. 필요하다면 Pinot 쿼리 콘솔에 액호크 SQL 쿼리를 실행하여 메트릭을 생성할 수 있습니다.\n\n그러나 솔루션의 모든 이해 관계자/사용자가 그렇게 하길 원하는 것은 아닙니다. 우리는 각 사용자 그룹에게 메트릭을 직관적이고 편안하게 찾을 수 있는 방식으로 제시해야 합니다. 이것이 우리가 분석의 마지막 단계인 서빙 레이어를 구현하는 곳입니다.\n\n<img src=\"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_15.png\" />\n\n<div class=\"content-ad\"></div>\n\n우리의 우선순위는 소비자들입니다. 그들은 사용량과 청구 예상을 시각화하는 실시간 대시보드가 필요합니다. 이를 위해 Streamlit과 같은 프레임워크를 활용하여 Python 기반 데이터 어플리케이션을 개발할 수 있습니다. Pinot Python 드라이버 pinotdb를 사용하면 애플리케이션과 Pinot 쿼리 환경을 연결할 수 있습니다.\n\nBI 및 즉석 탐색이 필요한 사용자 그룹, 특히 API 제품 소유자는 Tableau와 Apache Superset과 같은 선호하는 BI 도구를 연동할 수 있는 Pinot의 ODBC 인터페이스를 사용할 수 있습니다.\n\n일괄 작업을 위해 Pinot는 Presto나 Trino와 같은 쿼리 연합 엔진에 Pinot 커넥터를 통해 연동할 수 있습니다.\n\n# 요약\n\n<div class=\"content-ad\"></div>\n\n다음은 파이프라인 구현 단계 순서를 나열하여 글을 마무리해 봅시다.\n\n1. Redpanda 클러스터를 프로비저닝하고 토픽을 생성하고 ACL을 설정합니다.\n2. APISIX에서 Kafka 싱크를 구성합니다.\n3. Pinot 스키마와 테이블을 생성합니다.\n4. 필요에 따라 데이터를 가공합니다.\n5. 대시보드를 생성하거나 연결합니다.\n\n이 솔루션은 비즈니스 자체에서 호스팅하고 관리하는 자체 호스팅 배포 모델을 전제로 합니다. 그러나 동일한 설계 원칙이 이 도구들의 호스팅 버전을 선택해도 적용될 수 있다는 점을 알아두는 것이 중요합니다. 아키텍처의 각 구성 요소는 호스팅 서비스로 대체될 수 있으며, 이를 통해 다양한 배포 전략에 대응할 수 있는 유연한 해결책이 될 수 있습니다.\n\n이전에 언급했듯이, 이 글은 \"어떻게\"보다는 \"왜\"에 대해 주로 다루고 있습니다. 목표는 정확한 실행 방법보다는 근본적인 해결책 패턴을 이해하는 것입니다. 이 글을 다음 실시간 분석 프로젝트의 청사진으로 삼아 보세요. 필요에 따라 다른 기술을 통합하여 조정할 수 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_0.png"},"coverImage":"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_0.png","tag":["Tech"],"readingTime":11},{"title":"데이터 폭풍 속을 해마하는 여정 정교한 레이크하우스 플랫폼 구축하기","description":"","date":"2024-05-27 12:55","slug":"2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform","content":"\n![이미지](/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_0.png)\n\n목요일 오후 늦은 시간, 우리는 깨달음을 얻었습니다. 어두운 사무실에서 깜박이는 화면들을 둘러싸고, 두 명의 예리한 데이터 분석가 동료와 데이터 과학자로 구성된 헌신적인 팀, 그리고 나, 데이터 엔지니어는 PostgreSQL과 MySQL 데이터베이스에서 데이터를 조인하려고 깊이 파고들고 있었습니다. 이 작업은 간단할텐데 어쩌면 서로 다른 부서에서 오는 불일치된 데이터 구조와 충돌하는 스키마로 애를 쓰고 있었습니다. 이러한 이질적인 데이터 세트를 수동으로 맞추려고 할수록 복잡성이 압도되는 느낌이었습니다. 혼돈스러운 정보 동기화 시도마다 실패할 때마다 공기는 분노로 더 두꺼워졌습니다. \"이렇게 일할 수는 없어,\" 살짝 중얼거렸던 저의 목소리엔 스트레스가 느껴졌습니다. 우리의 현재 시스템이 현실에 부합하지 않다는 것은 명백했습니다—우리는 흩어진 데이터 정복 뿐만 아니라 이 넓은 데이터 정글을 이해할 수 있는 통합 플랫폼이 필요했던 것입니다. 이 순간이 우리에게 중대한 변화가 필수적이라는 것을 알게 된 시점이었고, 큰 변화가 곧 찾아올 것임을 알게 된 시점이었습니다.\n\n이 여정을 시작하면서, 저희는 우리 회사의 의사 결정 프로세스의 기반이 되는 견고한 데이터 플랫폼 아키텍처를 만들었습니다. 이 블로그 글에서는 다양한 데이터를 단일하고 강력한 분석 엔진으로 통합하는 것뿐만 아니라 지속적으로 발전하고 시간이 지남에 따라 더 많은 데이터 소스를 통합하는 유연하고 확장 가능한 데이터 인프라를 구축하는 과정에서 우리가 직면한 인사이트와 도전에 대해 탐구할 것입니다.\n\n대용량의 원시 데이터를 원래 형식 그대로 저장할 수 있는 유연성으로 기업들에게 빠르게 적응하고 효율적으로 확장할 수 있는 도구로써 데이터 레이크가 부각되었습니다. 그러나 계속 진행함에 따라 데이터 관리에 더 구조적인 접근이 필요하다는 것을 깨달았고, 그로 인해 레이크하우스 구조를 채택하게 되었습니다. 이 하이브리드 모델은 데이터 레이크의 확장성과 유연성을 데이터 웨어하우스의 관리 기능과 결합하여 데이터 전략을 향상시킵니다. 이 이야기는 이러한 기술을 활용하기 위해 우리가 취한 실질적인 단계를 살펴보며, 데이터 레이크를 레이크하우스 프레임워크로 통합함으로써 데이터 주도 기업에게 혁신적인 자산이 될 수 있는 방법을 밝힐 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 레이크하우스 이전의 데이터 전략\n\n레이크하우스를 개발하기 전에, 데이터 관리는 간단했지만 우리의 요구사항이 증가함에 따라 비효율적으로 변했습니다. 처음에는 BI용 Apache Superset을 사용하여 3개의 주요 데이터 소스를 관리했는데, 처음에는 최소한의 복잡성으로 우리의 요구사항을 충족시켰습니다.\n\n그러나 우리의 데이터 요구사항이 증가함에 따라 시스템의 한계가 나타나기 시작했습니다. 두 가지 다른 소스에서 데이터를 조인해야 할 필요가 발생했을 때 중대한 도전이 발생했습니다. 당시 우리의 솔루션은 매우 효율적이지 못했습니다: 필요한 데이터를 한 소스에서 다른 소스로 수동으로 복제했습니다. 이 프로세스는 시간이 많이 걸릴 뿐만 아니라 데이터를 동기화하기 위해 빈번한 업데이트가 필요했기 때문에 오류를 발생시킬 가능성도 있었습니다.\n\n또한 다양한 팀과 프로젝트가 발전함에 따라 Superset 내에서 여러 데이터셋이 생성되었는데, 각각이 특정한 분석 요구에 맞게 조정되었습니다. 불행히도, 이로 인해 여러 데이터셋에 중복 변환 요소가 코딩되어 복잡성이 증가했을 뿐만 아니라 이러한 변환을 유지하고 업데이트하는 것이 점점 더 부담스러워졌습니다.\n\n<div class=\"content-ad\"></div>\n\n## 데이터 아키텍처 결정: Lake, Warehouse 또는 Lakehouse?\n\n저희 데이터 인프라에 적합한 아키텍처를 선택하는 것은 중요한 결정이었습니다. 세 가지 주요 옵션 중에서 선택을 고민했습니다: 데이터 레이크, 데이터 웨어하우스 및 레이크하우스. 각각에 대한 간단한 개요를 살펴보겠습니다:\n\n• **데이터 레이크**: 데이터 레이크는 원시 형식으로 방대한 양의 데이터를 저장합니다. 다양한 소스에서 대량의 다양한 데이터를 처리하는 데 이상적이며 높은 유연성과 확장성을 제공합니다. 그러나 구조화된 데이터 환경의 처리 효율성 일부가 부족합니다.\n\n• **데이터 웨어하우스**: 이것은 질의 및 분석에 최적화된 구조화된 형식으로 데이터를 저장하는 시스템입니다. 데이터 웨어하우스는 구조화된 데이터에 대한 빠른 쿼리 성능에 뛰어나지만 변경사항 및 새로운 데이터 유형의 수용에 있어서 덜 유연할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n• Lakehouse: 데이터 레이크와 데이터 웨어하우스의 장점을 결합한 하이브리드 모델입니다. 데이터 레이크의 넓은 저장 공간과 유연성을 제공하면서 데이터 웨어하우스의 효율적인 쿼리 기능을 갖추고 있습니다.\n\n신중한 고려 끝에 저희는 여러 가지 이유로 레이크하우스 아키텍처를 도입하기로 결정했습니다:\n\n1. 유연성: 레이크하우스 아키텍처는 필요한 적응성을 제공했습니다. 기존의 데이터 웨어하우스는 새로운 데이터 소스나 유형을 빠르게 통합하는 것이 어려워 변경에 제한이 있고 느립니다.\n\n2. 단순화된 아키텍처: 처음에는 기존의 ETL 프로세스를 데이터 웨어하우스와 데이터 레이크에 별도로 구축하는 것을 고려했지만, 두 개의 별도 시스템을 유지할 명확한 이유를 찾지 못했습니다. 레이크하우스 모델은 강력한 쿼리 및 저장 기능을 하나의 보다 관리하기 쉬운 시스템으로 통합한 간소화된 접근 방식을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n다음은 주요 구성 요소에 대한 개요입니다:\n\nOur Data Lakehouse stack\n\n우리의 레이크하우스 아키텍처는 AWS 기술과 오픈 소스 솔루션의 최선을 활용하여 데이터를 효율적으로 관리하고 분석하는 것을 목표로 합니다.\n\n이미지를 Markdown 형식으로 변경했습니다.\n\n\n![Lakehouse Overview](/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_1.png)\n\n\n<div class=\"content-ad\"></div>\n\n**데이터 저장:**\n\n우리는 데이터 저장을 위해 AWS S3를 활용하며, 환경을 개발 및 프로덕션용 2개의 전용 버킷으로 구성합니다. 메달리온 아키텍처를 채택하여 각 버킷 내에 bronze, silver 및 gold 세 가지 독립적인 레이어(폴더)를 설정했습니다. 각 레이어는 데이터 관리 수명주기에서 특정 목적을 제공합니다. 메달리온 아키텍처는 데이터를 세 개의 레이어로 분류하는 레이크하우스 시스템에 사용되는 계층화된 데이터 처리 모델입니다:\n\n- Bronze Layer (Raw Layer): 이 기본 레벨에서는 다양한 소스로부터 도착한 대로 데이터를 정확히 저장하여 JSON, CSV 등의 원래 형식으로 보존합니다. 이 레이어는 주로 데이터 엔지니어링 팀이 디버깅 및 데이터 무결성 확인을 위해 액세스하는 데 중요하며 데이터 과학자가 초기 인사이트를 얻고 데이터 품질을 측정하기 위해 탐색 분석을 시작하는 중요한 역할을 합니다. 초기 인사이트는 더 나은 데이터 처리 전략을 안내하는 데 중요합니다.\n- Silver Layer (Cleansed Layer): 데이터가 실버 레이어로 이동하면 필요한 클렌징 및 변환 프로세스를 수행합니다. 여기서 우리는 불일치를 수정하고 데이터를 풍부하게하여 구체적인 비즈니스 규칙을 적용하여 구조화되고 유용하게 만듭니다. 우리의 분석 엔지니어는 이 클렌징 된 데이터와 작업하여 복잡한 변환을 실행하고 내부 분석을 이끄는 자세한 보고서를 생성합니다. 더 나아가 이 레이어는 우리의 데이터 과학자가 정교한 모델을 구축하는 데 의존하는 정돈된 데이터 환경을 제공합니다.\n- Gold Layer (Aggregated Layer): 이 곳에서 데이터는 가장 높은 가치를 얻으며, 비즈니스 수준의 집계 및 핵심 성능 지표로 변환됩니다. 신속한 검색 및 고속 분석을 위해 최적화된 골드 레이어는 주로 의사 결정자를 위해 액세스됩니다. 이들은 회사 전반의 전략 및 운영에 영향을 미치는 실행 가능한 인사이트를 위해 정제된 데이터에 의존합니다. 더불어 이 레이어는 기업 수준의 보고서 및 대시보드의 기반 역할을 합니다.\n\n실버 및 골드 레이어에서는 쿼리 성능을 최적화하기 위해 데이터 파일을 Parquet 파일로 저장합니다. Parquet의 효율적인 열 지향 저장 형식 덕분에 쿼리 성능이 최적화됩니다. 또한, 이러한 Parquet 파일 위에서 Apache Iceberg를 활용하여 레이크하우스 아키텍처에 여러 가지 중요한 기능을 제공합니다. Apache Iceberg를 사용하면 데이터 레이크를 전통적인 데이터베이스처럼 다루되 더 큰 유연성과 확장성을 갖습니다. 스냅샷, 트랜잭션, 업서트 및 삭제와 같은 복잡한 작업을 지원함으로써 데이터 레이크를 더 동적이고 다재다능한 시스템으로 변환할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 카탈로그:\n\n저희의 데이터 카탈로그 관리에는 비용 효율성, 다른 AWS 서비스와의 깊은 통합, 그리고 직관적인 메타데이터 관리 기능으로 인해 AWS Glue Catalog를 선택했습니다. AWS Glue Catalog는 중앙 메타데이터 저장소로 기능하며, 이를 통해 각종 AWS 서비스 간의 데이터 자산을 보다 쉽게 관리하고 접근할 수 있습니다. AWS Glue Crawler를 활용하여 S3에 저장된 데이터를 자동으로 발견하고 분류하여 데이터 카탈로그 테이블을 손쉽게 생성하고 업데이트할 수 있습니다.\n\n하지만 AWS Glue Catalog는 운영 요구에 맞게 데이터 검색을 용이하게 하는 측면에서 제한이 있음을 인지하고 있습니다. 잘 통합되어 비용 효율적이지만 세련된 데이터 카탈로그의 세부 기능 중 일부를 지원하지 않습니다. 특히 대규모 데이터 작업에 필수적인 향상된 검색 및 발견 도구와 같은 기능을 지원하지 않습니다. 이는 모델링을 위해 다양한 데이터세트에 빠르게 액세스해야 하는 데이터 과학자, 비즈니스 결정에 신속한 통찰을 얻어야 하는 데이터 분석가, 그리고 철저한 데이터 탐색에 의존하여 포괄적인 보고서를 작성하는 비즈니스 인텔리전스 전문가를 포함한 여러 팀 멤버에 영향을 줄 수 있습니다. 앞으로는 저희 조직의 중요한 역할들의 요구를 충족시키기 위해 데이터 검색을 지원하는 더 편리하고 포괄적인 데이터 카탈로그 솔루션을 탐구할 계획입니다.\n\n데이터 액세스 및 쿼리 엔진:\n\n<div class=\"content-ad\"></div>\n\nAWS Athena는 주요 쿼리 엔진으로 사용되며 AWS Glue 카탈로그와 원활하게 통합됩니다. 이 간편한 설정을 통해 데이터 레이크를 효과적으로 쿼리할 수 있어 Athena는 우리 데이터 아키텍처의 중요한 구성 요소입니다. Athena를 사용하는 주요 장점 중 하나는 비용 효율성입니다. Athena는 쿼리 중 스캔된 데이터 양에 따라 요금이 부과되기 때문에 현재 우리의 쿼리는 과도한 데이터 양을 스캔하지 않아 비용을 상당히 낮게 유지할 수 있었습니다.\n\n그러나 우리는 데이터 레이크 사용을 확대함에 따라(특히 애플리케이션 내 차트를 통한 직접 데이터 쿼리 통합이 예정된) Athena와 관련된 비용이 증가할 수 있다는 점을 알고 있습니다. 이러한 잠재적 시나리오에 대비하기 위해 Trino로 전환을 고려 중이며, 이는 EKS에서 실행되어 AWS Glue 메타스토어에 연결될 것입니다. Athena와 Trino 사이의 기본적인 유사성으로 인해 이 마이그레이션은 간단할 것으로 예상됩니다.\n\n현재 Athena에서 두 가지 서로 다른 워크그룹을 활용하고 있습니다 - SQL 쿼리를 위한 하나와 Spark(Python) 연산을 위한 다른 하나입니다. 앞으로, 우리는 이러한 설정을 세분화하여 변환, 고객 분석 등과 같은 다양한 비즈니스 요구에 대해 별도의 워크그룹을 생성하여 운영 효율성과 비용 관리를 향상시킬 계획입니다.\n\n데이터 거버넌스:\n\n<div class=\"content-ad\"></div>\n\nAWS Lake Formation은 저희 데이터 거버넌스에 중요한 역할을 합니다. 레이크하우스 아키텍처에서 데이터 보안과 권한 관리를 크게 향상시킵니다. 이는 PHI 및 민감한 데이터를 다루는 데 핵심적인 엄격한 접근 제어를 시행하는 데 도움이 됩니다.\n\n강력한 접근 제어를 위한 LF-Tags 구현: 데이터가 안전하게 액세스되고 엄격한 정책을 준수하는 데 필요한 접근 권한을 정교하게 제어하기 위해, 우리는 데이터베이스 및 테이블 수준에서 권한을 세밀하게 제어하기 위해 LF-Tags를 활용합니다. 우리의 태그 전략은 체계적으로 설계되어 있으며, 데이터베이스는 일반적으로 태그가 지정되며, 더 구체적인 요구 사항에 따라 테이블 수준에서 권한을 관리합니다. 우리가 사용하는 가능한 태그에는 다음과 같은 것들이 있습니다:\n\n- 환경: dev, prod\n- 부서: app, internal, devops, hr, customers, infra, sales, ds\n- PHI: true\n- 데이터 레이크 레이어: gold, silver, bronze\n- 클라이언트 대면: true (데이터를 고객에게 노출할 수 있는지 여부를 나타냄)\n\n각 데이터베이스와 테이블에 여러 태그를 적용하여 세밀한 역할 기반 접근 제어를 가능하게 합니다. 예를 들어, 우리 애플리케이션에서 데이터를 쿼리하는 고객을 위한 역할은 client_facing: true, data_lake_layer: gold, 그리고 environment: prod와 같은 태그 조합을 통해 액세스 권한을 부여받을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n최초에 AWS Lake Formation을 설정하는 것은 간단하지 않았습니다. 이 플랫폼은 강력하지만 직관적이지 않았고, 권한 행동을 우리의 거버넌스 요구에 맞게 조정하는 데 상당한 노력과 시간이 걸렸습니다. 이러한 도전을 극복하기 위해서는 가파른 학습 곡선이 필요했는데, 다양한 구성을 실제로 실험해야 했고, 우리가 필요로 하는 상세한 액세스 제어를 효과적으로 구현하고 관리하는 방법을 이해해야 했습니다.\n\n![이미지](/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_2.png)\n\n데이터 수집:\n\n우리는 단순하게 사용할 수 있고 기관 전체의 데이터를 수집해야 할 필요가 있는 미래의 요구를 예측하여 다양한 커넥터를 지원하는 플랫폼을 찾았습니다. 두 플랫폼인 Airbyte(오픈 소스 솔루션)와 Rivery(SaaS 솔루션)을 비교하는 POC를 진행한 후, 몇 가지 설득력있는 이유로 Airbyte를 선택했습니다.\n\n<div class=\"content-ad\"></div>\n\n먼저, 저희의 결정은 데이터 양에 따라 청구되지 않는 비용 효율적인 솔루션에 더 기울였습니다. 저희는 증가하는 비용을 걱정하지 않고 자유롭게 데이터를 가져오길 선호했습니다. 게다가 Airbyte의 개발 도구는 특히 인상적이었습니다. 플랫폼의 Connector Builder SDK는 Connector Builder UI와 로우코드 커넥터 개발 환경이 모두 포함되어 있어 필요한 간편함과 유연성을 제공했습니다. 이 기능 덕분에 우리는 우리의 특정 요구에 맞게 데이터 커넥터를 쉽게 구축하고 맞춤화할 수 있었습니다.\n\nAirbyte는 대규모이자 활발한 커뮤니티를 자랑하지만 제품에 몇 가지 어려움을 겪었습니다. 처음에는 데이터 수집 속도가 느렸습니다. 특히 PostgreSQL에서 20GB를 2일 이상으로 전송하는 데 시간이 걸렸습니다. 먼저 AWS-data-lake 목적지를 사용해 보았지만 느리고 지속적인 동기화를 지원하지 않았습니다. 이 문제를 해결하기 위해 이 문제를 고치기위한 pull request를 제출했지만 3개월이 걸렸습니다. 더 나은 해결책을 찾기위해 여러 다른 목적지를 실험해 보았습니다. Parquet을 사용할 때 타임스탬프가 struct로 형식화되는 짜증나는 문제가 있는 S3 목적지가 있었습니다. 이 구체적인 문제는 2년째 해결되지 않은 상태로 있는데, 이는 지원 측면에서 중요한 차이점을 보여줍니다. 유망한 Iceberg 목적지는 AWS Glue Catalog를 지원하지 않았습니다. 그래서 AWS-Glue 목적지를 시도했지만 JSON 출력만 지원해서 비효율적이라는 것을 발견했습니다.\n\n최종적으로 이러한 옵션 중 어느 것도 우리의 요구 사항을 완전히 충족시키지 못했기 때문에, 우리는 자체적으로 사용자 정의 AWS-data-lake 목적지를 개발하기로 결정했습니다. 우리는 원래 코드를 복제하고 우리의 요구 사항에 맞게 특별히 맞춤화하여 데이터 수집 프로세스를 크게 향상시킨 맞춤형 솔루션을 만들었습니다.\n\n이러한 어려움에도 불구하고 Airbyte는 효과적으로 우리의 요구 사항을 모두 충족시켰습니다. 오늘날, Airbyte를 사용하여 약 15가지 다른 데이터 소스를 데이터 레이크에 성공적으로 통합했으며, 데이터 수집 능력과 전체 데이터 전략을 크게 향상시켰습니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 처리:\n\n저희의 데이터 처리 워크플로우는 dbt Core에 의해 강력하게 주도되며, ELT (추출, 로드, 변환) 접근 방식을 사용합니다. 모든 데이터가 브론즈층에서 시작되어 점진적으로 실버층과 골드층으로 변환됩니다.\n\n저희는 dbt Athena 어댑터를 사용하고 있으며, SQL 및 Python (PySpark) 모델을 지원합니다. 이 다양성은 더 복잡한 변환을 효과적으로 처리하는 데 중요합니다. dbt-Athena 어댑터는 활기찬 커뮤니티의 혜택을 받고 있으며, 정기적인 업데이트로 계속 발전하고 있습니다. 처음에는 Python Athena 통합을 채택하는 데 약간 주저했었는데, 그 당시의 혁신성과 제한된 추적 레코드 때문이었습니다. 그러나 철저한 테스트와 유효성 검사를 거친 후에는 어떠한 문제도 발생하지 않았고, 안정성과 효율성을 확인하며 우리의 프로덕션 환경에 성공적으로 구현했습니다.\n\ndbt에서 테이블 속성을 구성하는 것은 직관적이고 유연하며, 우리의 데이터 관리 능력을 크게 향상시킵니다. 예를 들어, 증분 테이블을 널리 사용하는데, 이는 새 데이터 또는 변경된 데이터만 효율적으로 가져오는 데 중요합니다. Iceberg 테이블 형식을 활용하여 병합 증분 전략을 채택하면 데이터세트를 중복 처리 없이 원활하게 업데이트할 수 있습니다. 또한, dbt에서 데이터 파티션을 관리하는 것도 간단해졌습니다. 파티션은 테이블 속성 내에서 직접 선언할 수 있습니다. 아래는 우리의 골드층 테이블에 대한 테이블 속성 구성 예시입니다. 우리가 재료화 전략, 파티셔닝 및 데이터 형식을 어떻게 지정하는지 보여주고 있습니다:\n\n<div class=\"content-ad\"></div>\n\n\n{ config(\n    materialized='incremental',\n    incremental_strategy='merge',\n    unique_key=['session_id'],\n    partitioned_by=['day'],\n    table_type='iceberg',\n    format='parquet'\n) }\n\n\n우리는 dbt에 대해 매우 만족하고 있습니다; 이 도구는 우리의 데이터 변환을 어떻게 관리하는지에 혁명을 일으켰습니다. 이 도구는 견고한 버전 관리, 코드의 재사용성 및 데이터 흐름의 명확한 문서화를 제공하여 복잡한 변환 작업을 관리하고 레이크하우스 아키텍처 전반에서 데이터 무결성을 유지하는 것을 크게 간소화합니다.\n\ndbt와의 성공을 토대로, 우리는 현재 데이터 관리 역량을 더욱 강화하기 위해 새로운 도구를 탐색 중입니다. Montara.io가 강력한 기능 세트를 제공하여 워크플로우를 최적화하는 우리의 dbt Git 리포지토리와 직접 통합되었습니다. Montara는 자동 CI/CD, 팀원들이 dbt 전문 지식이 적은 경우에도 모델을 작성하고 테스트할 수 있는 사용자 친화적인 UI를 제공하며 데이터 계보 표시, 데이터 카탈로그 및 관찰 가능성과 같은 가치 있는 도구를 제공합니다.\n\nMontara에 감명을 받았습니다; 이 도구는 우리의 dbt 워크플로우를 크게 간소화시켜 팀 전체에서 데이터 변환을 보다 접근 가능하고 관리하기 쉽게 만듭니다. 이 도구가 비교적 새로운 것이며 여전히 발전 중이므로 가끔씩 일부 문제와 기능의 빈틈을 겪기도 하지만, 우리의 경험은 전반적으로 매우 긍정적입니다. Montara 팀은 우수한 지원을 제공하며 우리와 긴밀히 협력하여 발생하는 어려움을 신속하게 해결하고 계속되는 제품 향상에 우리의 피드백을 통합합니다. 이 협력적인 접근은 문제를 신속하게 해결할 뿐만 아니라 Montara.io가 우리의 데이터 인프라 요구와 완벽하게 일치하도록 발전하도록 보장합니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n분석 및 BI 도구:\n\nApache Superset은 저희가 선택한 분석 및 비즈니스 인텔리전스 도구로, 오픈 소스와 강력한 데이터 시각화 능력으로 유명합니다. 다른 BI 도구와 비교했을 때 유연성과 비용 효율성을 강점으로 삼아 Superset을 선택했습니다. 다양한 사용자 정의 옵션과 사용자 친화적 인터페이스를 통해 우리 팀은 대시보드와 보고서를 자신들의 필요에 맞게 맞춤화할 수 있으며, 특히 Athena를 주 데이터 원본으로 사용하는 저희 독특한 분석 환경에 특히 적합합니다.\n\n데이터 분석가들은 저희 회사의 다양한 부서를 위한 대시보드를 만들기 위해 주로 Superset을 사용합니다. 더불어, Superset의 기능을 활용하여 차트를 애플리케이션에 직접 임베드하여 고객에게 유용한 통찰을 제공합니다.\n\n현재, 일부 차트는 브론즈 계층의 데이터에 직접 접근하여 실시간으로 변환 작업을 수행합니다. 그러나 더 이상 원시 데이터 쿼리의 부하를 줄이기 위해 이 접근 방식을 수정 중이며, 최종적으로 LF-tags를 사용하여 브론즈 계층의 액세스를 제한할 계획입니다.\n\n<div class=\"content-ad\"></div>\n\nSuperset은 대시보딩에 편리하고 효과적인 도구라고 생각하지만, 시간이 지남에 따라 생성된 데이터셋이 증가하면 어느 정도 어수석해질 수 있습니다. Superset의 각 데이터셋은 개별적으로 구성되어 있어 대시보드의 수와 복잡성이 증가함에 따라 중복과 관리 도전이 발생할 수 있습니다. 그럼에도 불구하고, 이러한 어려움에도 불구하고, Superset은 우리의 요구 사항을 잘 충족시켜 주며 조직 전반에서 데이터를 시각화하고 상호 작용하는 다재다능한 플랫폼을 제공합니다.\n\n오케스트레이션과 워크플로우 관리\n\nApache Airflow는 데이터 환경 내에서 워크플로우를 조정하고 관리하는 데 중요한 역할을 합니다. 오픈 소스 도구인 Airflow는 유연성, 확장성, 그리고 강력한 커뮤니티 지원을 제공하여 우리의 운영 요구에 필수적인 요소를 제공합니다. Airflow를 활용하여 모든 데이터 파이프라인이 데이터 레이크로 정확하게 트리거되어 데이터의 신선도와 신뢰성을 유지하도록 합니다.\n\n현재, 저희는 저희 레이크하우스 운영에 필수적인 세 가지 주요 DAGs (방향이 있는 비순환 그래프)를 관리하고 있습니다. 첫 번째 DAG는 AirbyteOperator를 활용하여 동기화를 위해 필요한 모든 업무를 트리거하여 브론즈 레이어에 데이터를 효율적으로 삽입하는 작업을 담당합니다. 두 번째 DAG는 dbt 변환을 실행하여 데이터를 처리하고 실버 및 골드 레이어로 옮기는 업무를 담당합니다. 세 번째 DAG는 전체 워크플로를 감독하며 데이터 처리의 원활한 흐름을 유지하기 위해 순차적으로 삽입 DAG 및 이후에 dbt DAG를 트리거합니다.\n\n<div class=\"content-ad\"></div>\n\n또한, 이러한 워크플로우 내에 Slack 알림을 통합했습니다. 이 설정은 DAG(작업 방향성 비순환 그래프) 실패 시 실시간 알림을 제공하여 지속적인 운영 및 데이터 무결성 유지를 위해 즉각적인 모니터링과 대응이 가능하게 합니다.\n\n## 결론: 전략적 이점을 위한 데이터 활용\n\n마지막으로, 데이터 레이크하우스 아키텍처를 구축하고 정제하는 우리의 여정은 도전적이고 보람찼습니다. 우리는 데이터 관리 역량을 혁신한 여러 도구와 기술을 성공적으로 통합하여, 다양한 데이터를 통합하여 동적 의사 결정을 지원하는 견고한 분석 엔진으로 변화시켰습니다. Apache Superset, AWS S3, AWS Glue Catalog, Apache Airflow, 그리고 dbt를 활용한 우리의 사용은 복잡한 데이터 과제에 대응하기 위해 첨단 기술을 채용하는 데 드러난 우리의 의지를 보여줍니다.\n\n이러한 도구들은 우리의 운영 효율을 향상시키는데 그치지 않고 회사 전반에서 더 통찰력있는 데이터 분석과 보고의 길을 열었습니다. 우리의 데이터 인프라를 계속 발전시키면서, 우리는 데이터 능력을 더욱 향상시킬 수 있는 새로운 기술과 방법을 탐구하는 데 헌신하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n저는 비슷한 여정을 걸어가고 있는 독자들로부터의 피드백과 질문을 환영합니다. LinkedIn에서 저와 연락하셔서 더 자세한 토론을 나누거나 아이디어를 교환해 주세요.\n\n","ogImage":{"url":"/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_0.png"},"coverImage":"/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_0.png","tag":["Tech"],"readingTime":13},{"title":"DBT 증분 전략과 동등성","description":"","date":"2024-05-27 12:53","slug":"2024-05-27-DBTIncrementalStrategyandIdempotency","content":"\n\n![Screenshot](/assets/img/2024-05-27-DBTIncrementalStrategyandIdempotency_0.png)\n\n# 배경\n\n안녕하세요, 저는 데이터 엔지니어인 Todd입니다. 저는 Nowcast에서 데이터 온보딩에 주로 관여하고 있습니다. 이 기술 블로그에서는 Nowcast에서의 ETL 파이프라인 디자인의 간략한 역사를 소개하고, Airflow와 DBT의 \"Incremental Models\" 사이에서 발생한 문제를 설명하고 우리가 개발한 해결책을 소개하겠습니다.\n\n# Python으로 ETL\n\n\n<div class=\"content-ad\"></div>\n\n역사적으로 Nowcast에서는 ETL 파이프라인을 Python을 사용하여 작성했습니다. 이 파이프라인은 AWS S3, Athena, RDBMS 등에 저장된 데이터에 변환을 적용하는 많은 Python 스크립트로 구성되어 있었습니다. 우리는 이러한 스크립트를 포함하는 도커 이미지를 작성하여 ECR에 업로드하고, Airflow에서 ECS 작업을 호출했습니다. 이러한 스크립트는 보통 데이트와 같은 파티션 필드를 매개변수로 사용하여 멱등성이 있도록 설계되었습니다. 즉, 2024-01-01을 전달하면 2024-01-01의 데이터가 처리되었습니다.\n\n이러한 스크립트 중 하나를 호출할 때, 실제로 실행되는 명령은 아래와 같이 보일 것입니다. 이때 데이트 매개변수는 Airflow에서 관리됩니다:\n\n```js\npython transform_data.py 2024-01-01 --some --other --arguments\n```\n\n# Airflow\n\n<div class=\"content-ad\"></div>\n\nAirflow은 Nowcast에서 많은 해동안 사용되어온 스케줄링 및 워크플로우 관리 도구입니다. 기본적으로 두 가지로 사용되고 있어요:\n\n1. 작업 스케줄러\n2. 작업 의존성 관리\n\n역사적으로 Airflow는 매일 실행되며 여러 Python 스크립트에 '실행 날짜' 매개변수를 전달하여 데이터를 처리합니다. 문제가 발생하거나 특정 기간의 작업을 다시 실행해야 할 때는 Airflow DAG에서 해당 작업을 다시 실행할 수 있습니다. 예를 들어, 2024년 01월 01일에 어떤 데이터 변환 스크립트가 실패하면 문제를 식별하고 수정한 후 해당 스크립트를 다시 실행할 수 있어요. 이는 스크립트가 한 번에 하나의 파티션만 처리하고 날짜를 매개변수로 입력받기 때문에 가능한 일입니다.\n\n# DBT에서 ETL\n\n2022년 말쯤 Python ETL 플로우를 Snowflake로 이전하기 시작했습니다. 그 결과 더 빠르고 저렴하며 깨끗한 파이프라인이 만들어졌어요. 우리는 파이프라인 실행 도구로 DBT를 사용하기로 결정했습니다 — DBT는 SQL 위에 위치한 레이어로 DB 모델 정의, 템플릿, 의존성 관리 및 데이터 회귀 테스트와 같은 다양한 기능을 포함하고 있어요. 빠르고 효율적으로 ETL 파이프라인을 구축하는 데 매우 유용한 도구입니다. 파이썬에서는 파이프라인의 각 변환을 스크립트로 작성하지만, DBT에서는 템플릿화된 SQL CTAS 쿼리로 작성됩니다. 이 쿼리들은 복잡한 수천 줄의 코드로 이루어진 스크립트와 비교했을 때 매우 읽기 쉽습니다.\n\n<div class=\"content-ad\"></div>\n\nBest practise in DBT is to use Incremental Models:\n\n# Incremental Models\n\nIncremental models are an efficient way of defining how to (incrementally) add data to our SQL models — consider we have a table that describes credit card transactions — we can make a DBT model (CTAS) that looks something like this:\n\n```js\n{\n materialized=\"table\"\n}\nselect\n transaction_id,\n transaction_date,\n user_id,\n store_name_description,\n transaction_amount\nfrom { ref('external_table_transaction') }\n```\n\n<div class=\"content-ad\"></div>\n\n이 코드는 table external_table_transaction에서 거래 데이터를 불러오는 테이블을 만듭니다. 문제는이 쿼리를 다시 실행할 때마다 전체 테이블을 다시로드한다는 것입니다. 테이블에 데이터가 많아질수록 쿼리가 느려지고 비용이 많이 발생합니다. 이 문제의 해결책은 증분 모델을 사용하는 것입니다:\n\n```js\n{\n config(\n materialized=\"incremental\",\n unique_key=[\"transaction_id\"],\n incremental_strategy=\"delete+insert\",\n )\n}\nselect\n transaction_id,\n transaction_date,\n user_id,\n store_name_description,\n transaction_amount\nfrom { ref('external_table_transaction') }\n {- if is_incremental() }\n where transaction_date = (select max(transaction_date) + 1 as next_date from { this })\n{- endif }\n```\n\n여기에서 우리는 DBT를 강력하게 만드는 일부 매크로/템플릿 기능을 볼 수 있습니다. 이제 기본적으로 하는 것은 테이블의 최신 데이터보다 1일이 더 늦은 거래 데이터만 external_table_transaction에서 로드해야 한다는 것입니다. 이것은 간단하면서도 강력합니다. 업데이트마다 계속 커지는 수십억 개의 데이터 행 처리 대신 이제 이전에 볼 수 없던 행만 처리하면 됩니다. 그리고 필요하다면 전체 갱신으로 테이블을 다시로드할 수 있는 옵션도 있습니다.\n\n# 문제\n\n<div class=\"content-ad\"></div>\n\n점진적 모델은 매우 매력적입니다 — 수학적으로 아름답고 데이터 스트림을 다룰 때 매우 잘 작동합니다. 문제는 처리하려는 데이터를 제어해야 할 때 발생합니다 — 점진적 모델은 특정 파티션만 다시 실행할 수 없으며 대신에 증분 모델의 규칙에 따라 데이터를 로드합니다. 이론적으로는 문제가 되지 않을 수도 있지만, 점진적 모델이 이상적인 환경에서 실행된다면 모든 데이터가 정확히 한 번만 로드될 것입니다 — 하지만 현실은 복잡합니다 — DAG가 깨지고, 데이터가 늦게 전달되거나 아예 제공되지 않는 경우가 발생하며 때로는 역사적 기록을 다시 로드해야 할 때가 있습니다. 게다가 Airflow 파이프라인이 어떤 이유로든 실패할 경우 DBT 작업이 Airflow 실행과 동기화되지 않을 수 있습니다. Nowcast로 마이그레이션한 이후 DBT를 사용하면서 경험한 점진적 모델과 관련된 이슈 목록이 아래에 나와 있습니다:\n\n- 한 파이프라인에서 수리가 진행된 것이 있었는데, 이는 2년 전으로 거슬러 올라가야 했으므로 역사적 데이터를 로드해야 했는데 (증분) 데이터 파이프라인이 역사적 재실행을 처리할 수 없어서 즉시 처리해야 했습니다.\n- 다른 DAG에서 상류 이슈로 3일 동안 깨졌으며, 3일 동안 데이터가 로드되지 않았고, DAG가 4일째 실행될 때 1일부터 데이터를 로드했으므로 동기화가 맞지 않았습니다.\n- 세 번째 파이프라인에서 상류 스킵 날짜(데이터가 빠진 날)가 발생했고, 점진적 모델은 데이터를 로드하기 위해 데이터에서 최대 날짜에 `1`을 추가하는 방식으로 처리했으나 해당 날짜가 나타나지 않아 데이터가 로드되지 않은 채로 수동 처리가 필요해졌습니다.\n\n하지만 우리는 단순히 점진적 모델을 포기할 수 없습니다 — 일부 파이프라인은 수십억 개의 행을 처리해야 하므로, 테이블을 대량으로 처리할 쿼리를 작성하면 느리고 비용이 많이 소요될 것입니다.\n\n# 동형성(idempotency) 및 분할의 중요성.\n\n<div class=\"content-ad\"></div>\n\n점진적 모델의 주요 문제는 이 모델이 멱등성을 갖지 않으며 특정 파티션에 대해 실행 구성이 불가능하다는 것입니다. 우리가 ETL 파이프라인에 대해 예전에 채택한 방식은 멱등 스크립트가 여러 번 다시 실행할 수 있는 횟수에 제한이 없는 것이었습니다. 과거 데이터에 문제가 발생하면 특정 파티션을 다시 생성할 수 있었고, 스크립트가 멱등성을 가졌기 때문에 특정한 날짜를 여러 번 실행해도 문제가 발생하지 않았습니다. 하지만 점진적 모델은 데이터의 특정 파티션을 다시 실행할 수 있는 능력이 없으며, 대신 모든 데이터를 스트림처럼 처리하여 보지 않은 데이터만을 로드합니다. 다시 말해 특정 규칙을 충족하는 데이터를 로드하는 것이죠.\n\n우리가 Airflow라는 스케줄링 도구를 사용하고 있기 때문에 데이터 파이프라인은 어떤 종류의 시간적 분할과 일치해야 합니다. 시간별, 일별, 주별, 월별 등 다양한 분할 방식이 될 수 있지만 중요한 점은 Airflow가 어떤 일정에 따라 실행되고 있다는 것입니다. 만약 과거 Airflow 작업을 다시 실행한다면 해당 작업을 호출할 때 해당하는 시간적 파티션에 맞게 실행되기를 기대하지만, 점진적 모델은 항상 앞으로만 '보기' 때문에 과거의 파티션에 대해 구성되지 않습니다. 이것은 Airflow에서 작업을 실행할 때 예상하는 것과는 다릅니다.\n\n하루마다 실행되는 2개의 Airflow DAG를 고려해보죠. 하나의 DAG는 매개변수로 날짜를 사용하여 해당 파티션만 실행하는 작업을 가지고 있습니다. 다른 DAG는 점진적 모델을 사용하며 실행할 때 보지 않은 데이터를 처리합니다. 둘 다 정상적으로 실행될 때 이전에 보지 못한 일별 데이터를 처리하게 되며 두 DAG는 동일하게 동작합니다. 하지만 문제가 발생하여 특정 날짜인 2024년 1월 1일을 다시 로드해야 할 때는 어떨까요? 파티션화된 DAG는 예상대로동작하여 2024년 1월 1일을 다시 실행할 것이지만, 점진적 모델은 Airflow에 전달되는 날짜와 관계없이 이전에 본 적 없는 데이터만을 로드할 것입니다.\n\n점진적 모델의 한계에 대해 논평한 댓글에서는:\n\n<div class=\"content-ad\"></div>\n\n간단히 말하면 - Airflow와 같은 일정 관리 도구를 사용할 때 시간 분할을 기대하는 경우, 점진적 모델이 잘 작동하지 않습니다.\n\n# 해결책\n\n해결책은 간단합니다 - DBT 변수를 사용할 수 있습니다. 또한 점진적 모델의 기능을 완전히 포기할 필요가 없습니다. 하나 이상의 변수를 추가하여 하나 이상의 분할에 명시적으로 실행할 수 있습니다:\n\n```js\n{- set target_date = var(\"target_date\", \"\") }\n{\n config(\n materialized=\"incremental\",\n unique_key=[\"transaction_id\"],\n incremental_strategy=\"delete+insert\",\n )\n}\nselect\n transaction_id,\n transaction_date,\n user_id,\n store_name_description,\n transaction_amount\nfrom { ref('external_table_transaction') }\n{- if target_date != \"\" }\n where transaction_date = '{ target_date }'\n{- else }\n {- if is_incremental() }\n where transaction_date = (select max(transaction_date) + 1 as next_date from { this })\n {- endif }\n{- endif }\n```\n\n<div class=\"content-ad\"></div>\n\n이것은 DBT 모델에 `target_date`라는 새 매개변수를 추가합니다. `target_date`가 정의되지 않은 경우 모델은 증분 동작으로 실행되지만, 변수가 전달된 경우 지정된 파티션에 대해 실행됩니다. 이 모델 구조화 방식은 Airflow에서 호출될 때 훨씬 더 잘 작동합니다.\n\n게다가, 이 모델은 이제 멱등성이 생겼습니다. 즉, 원본 데이터가 동일한 경우 동일한 쿼리와 매개변수로 실행하고 동일한 결과를 얻을 수 있습니다. 반면 증분 모델의 경우 로드된 데이터는 테이블 내용 및 상위 스트림에서 발생한 변경 내용에 따라 달라집니다.\n\n이 솔루션은 병렬, 증분 및 파티션화의 3가지 모드를 효과적으로 제공합니다. 따라서 Airflow와 DBT의 의도된 증분 전략과 잘 어울리며 이를 사용할 경우 잘 작동합니다. 아래와 같이 인수 없이 DBT를 실행하면 증분 모델을 사용할 것입니다:\n\n```js\ndbt run --select my_model\n```\n\n<div class=\"content-ad\"></div>\n\n명시적으로 새로 고침을 실행하면 대량 적재가 발생합니다:\n\n```js\ndbt run --select my_model --full-refresh\n```\n\n그리고 추가한 target_date 매개변수를 전달하면 특정 파티션에 대해서만 실행되도록 할 수 있습니다:\n\n```js\ndbt run --select my_model  --vars \"{target_date : '2024-01-01'}\"\n```\n\n<div class=\"content-ad\"></div>\n\n이제 Airflow가 전달되는 날짜 매개변수를 제어할 수 있는 명령으로 돌아왔어요. 이렇게 하면 훨씬 더 부드러운 통합이 가능해요!\n\n# 참고 자료\n\n이 문제를 연구하는 데 사용된 다음 문서들입니다:\n\nDBT — 증분성의 한계에 대해\nMedium — DBT와 Airflow를 사용한 멱등데이터 파이프라인\n\n<div class=\"content-ad\"></div>\n\n# Nowcast의 엔지니어링\n\n만약 DBT에서 데이터 파이프라인을 구축하는 방법에 대해 알고 싶으면 아래 링크를 사용하여 친목을 돈 미팅을 예약해보세요. '문의 사항'란에 'Todd와 이야기하고 싶어요'라고 작성해주세요.\n\nNowcast는 현재 데이터 엔지니어를 채용 중입니다! 관심이 있으시면 [여기에서 지원하세요](application_link).\n","ogImage":{"url":"/assets/img/2024-05-27-DBTIncrementalStrategyandIdempotency_0.png"},"coverImage":"/assets/img/2024-05-27-DBTIncrementalStrategyandIdempotency_0.png","tag":["Tech"],"readingTime":7}],"page":"109","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":12,"currentPageGroup":5},"__N_SSG":true}