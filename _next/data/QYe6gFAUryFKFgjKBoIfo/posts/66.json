{"pageProps":{"posts":[{"title":"왜 나는 글쓰기를 그만두지 않을 것인지 이유","description":"","date":"2024-06-19 18:52","slug":"2024-06-19-ThisIsWhyIWillNotStopWriting","content":"\n\n과거 작품들을 훑어보면 낯선 사람처럼 느껴지기도 해요. 마치 박물관에서 유물을 뒤적이는 손님 같아요. 예전에 썼던 것들이 다시 유용하게 쓰일 때가 있어서 더욱 놀랍네요. 마치 필요성을 예견하고 미리 해결책을 준비한 것 같아요.\n\n가끔 써본 말들이 당시 느꼈던 감정을 상기하려고 노력해요. 때로는 성공도 해보지만, 때로는 그렇지 못할 때도 있어요. 때로는 한 마디로 당시의 감정과 머릿 속 상태를 정확히 떠올릴 수도 있고, 다른 때에는 여러 단어를 반복해도 당시의 감정과 연관을 찾지 못할 때도 있어요.\n\n![image](/assets/img/2024-06-19-ThisIsWhyIWillNotStopWriting_0.png)\n\n저는 강력히 믿어요. 우주는 현재의 관점을 통해 우리의 과거를 다시 방문하는 것 같아요. 그래서 어떤 경험이 새롭긴 하지만, 과거 어느 때와도 전혀 무관한 것은 아니에요. 감정은 마치 그릇처럼 앞뒤로 흔들리면서, 과거의 감정을 혀끝으로 맛 볼 수 있을 정도로 순간순간 변하기도 해요.\n\n<div class=\"content-ad\"></div>\n\n그래서 난 글쓰기를 멈추지 않을 거야. 우주가 다른 이들을 위해 썼다는 핑계로 나에게 미래 편지를 쓰라고 요청하는 것처럼. 오늘 썼던 이 말들이 다음 10년 후에 도움이 될지도 몰라. 이것은 내 감정을 담는 창고이며, 반복되는 패턴을 발견했기 때문이다.","ogImage":{"url":"/assets/img/2024-06-19-ThisIsWhyIWillNotStopWriting_0.png"},"coverImage":"/assets/img/2024-06-19-ThisIsWhyIWillNotStopWriting_0.png","tag":["Tech"],"readingTime":1},{"title":"위성 이미지에서 GANs적대적 생성 신경망을 사용하여 구름 제거하기","description":"","date":"2024-06-19 18:49","slug":"2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks","content":"\n\n## 파이썬으로부터 GAN(Generative Adversarial Networks) 만들어 보기\n\n![이미지](/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_0.png)\n\nGAN(Generative Adversarial Networks)이라는 아이디어는 2014년 Goodfellow와 그 동료들에 의해 소개되었고, 곧 그 이후에 컴퓨터 비전 및 이미지 생성 분야에서 극도로 인기를 끌게 되었습니다. 인공지능 분야에서의 급속한 발전과 새로운 알고리즘의 수가 늘어나는 것을 고려하더라도, 이 개념의 단순함과 창의성은 여전히 매우 인상적입니다. 그래서 오늘은 이러한 네트워크가 얼마나 강력할 수 있는지를 보여주기 위해 위성 RGB(빨강, 녹색, 파랑) 이미지에서 구름을 제거하는 시도를 해보려고 합니다.\n\n적절히 균형 잡히고 충분히 크며 올바르게 전처리된 컴퓨터 비전 데이터셋을 준비하는 데에는 상당한 시간이 소요되므로, 저는 Kaggle에 어떤 것이 있는지 살펴보기로 결정했습니다. 이 작업에 가장 적합하다고 생각한 데이터셋은 EuroSat이며, 이는 오픈 라이선스를 가지고 있습니다. 이 데이터셋은 Sentinel-2에서 64x64 픽셀의 27000개의 레이블이 지정된 RGB 이미지로 구성되어 있고, 다중 클래스 분류 문제를 해결하기 위해 만들어졌습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_1.png)\n\n우리는 분류 자체에 흥미가 없지만 EuroSat 데이터셋의 주요 기능 중 하나는 모든 이미지에 맑은 하늘이 있습니다. 그것이 정확히 우리가 필요한 것입니다. [3]에서 이 접근법을 채택하여, 우리는 이 Sentinel-2 샷을 대상으로 사용하고 입력을 추가하여 (구름) 노이즈를 생성할 것입니다.\n\n그래서 우리가 GANs에 대해 실제로 이야기하기 전에 데이터를 준비해 봅시다. 우선, 데이터를 다운로드하고 모든 클래스를 하나의 디렉토리로 병합해야 합니다.\n\n🐍전체 Python 코드: GitHub.\n\n\n<div class=\"content-ad\"></div>\n\n```js\nimport numpy as np\nimport pandas as pd\nimport random\n\nfrom os import listdir, mkdir, rename\nfrom os.path import join, exists\nimport shutil\nimport datetime\n\nimport matplotlib.pyplot as plt\nfrom highlight_text import ax_text, fig_text\nfrom PIL import Image\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n```\n\n```js\nclasses = listdir('./EuroSat')\npath_target = './EuroSat/all_targets'\npath_input = './EuroSat/all_inputs'\n\n\"\"\"UNPACK한 아카이브 파일의 파일 이름을 변경하기 위해 단 한 번만 실행하세요\"\"\"\nmkdir(path_input)\nmkdir(path_target)\nk = 1\nfor kind in classes:\n  path = join('./EuroSat', str(kind))\n  for i, f in enumerate(listdir(path)):\n    shutil.copyfile(join(path, f),\n                  join(path_target, f))\n    rename(join(path_target, f), join(path_target, f'{k}.jpg'))\n    k += 1\n```\n\n중요한 두 번째 단계는 노이즈 생성입니다. 다양한 방법을 사용할 수 있지만, 예를 들어 일부 픽셀을 무작위로 마스킹하거나 가우시안 노이즈를 추가하는 등의 방법이 있습니다. 그러나 이 글에서는 저는 새로운 방식인 Perlin 노이즈를 시도해 보고 싶습니다. Perlin 노이즈는 80년대에 Ken Perlin이 영화 연기 효과를 개발할 때 발명했습니다. 이 종류의 노이즈는 일반적인 랜덤 노이즈에 비해 더 유기적인 외관을 가지고 있습니다. 저에게 이를 증명하는 기회를 주세요.\n\n```js\ndef generate_perlin_noise(width, height, scale, octaves, persistence, lacunarity):\n    noise = np.zeros((height, width))\n    for i in range(height):\n        for j in range(width):\n            noise[i][j] = pnoise2(i / scale,\n                                  j / scale,\n                                  octaves=octaves,\n                                  persistence=persistence,\n                                  lacunarity=lacunarity,\n                                  repeatx=width,\n                                  repeaty=height,\n                                  base=0)\n    return noise\n\ndef normalize_noise(noise):\n    min_val = noise.min()\n    max_val = noise.max()\n    return (noise - min_val) / (max_val - min_val)\n\ndef generate_clouds(width, height, base_scale, octaves, persistence, lacunarity):\n    clouds = np.zeros((height, width))\n    for octave in range(1, octaves + 1):\n        scale = base_scale / octave\n        layer = generate_perlin_noise(width, height, scale, 1, persistence, lacunarity)\n        clouds += layer * (persistence ** octave)\n\n    clouds = normalize_noise(clouds)\n    return clouds\n\ndef overlay_clouds(image, clouds, alpha=0.5):\n\n    clouds_rgb = np.stack([clouds] * 3, axis=-1)\n\n    image = image.astype(float) / 255.0\n    clouds_rgb = clouds_rgb.astype(float)\n\n    blended = image * (1 - alpha) + clouds_rgb * alpha\n\n    blended = (blended * 255).astype(np.uint8)\n    return blended\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n가로, 세로 = 64, 64\n옥타브 = 12  # 합쳐지는 잡음 레이어의 수\n지속성 = 0.5  # 낮은 지속성은 높은 주파수 옥타브의 진폭을 줄입니다.\n라쿠나리티 = 2  # 높은 라쿠나리티는 높은 주파수 옥타브의 주파수를 늘립니다.\nfor i in range(len(listdir(path_target))):\n  기본_스케일 = random.uniform(5, 120)  # 잡음 주파수\n  알파 = random.uniform(0, 1)  # 투명도\n\n  구름 = generate_clouds(가로, 세로, 기본_스케일, 옥타브, 지속성, 라쿠나리티)\n\n  이미지 = np.asarray(Image.open(join(path_target, f'{i+1}.jpg')))\n  이미지 = Image.fromarray(overlay_clouds(이미지, 구름, 알파))\n  이미지.save(join(path_input, f'{i+1}.jpg'))\n  print(f'{i+1}/{len(listdir(path_target))}번째 처리 완료')\n```\n\n```js\n인덱스 = np.random.randint(27000)\nfig, ax = plt.subplots(1,2)\nax[0].imshow(np.asarray(Image.open(join(path_target, f'{인덱스}.jpg')))\nax[1].imshow(np.asarray(Image.open(join(path_input, f'{인덱스}.jpg')))\nax[0].set_title(\"원본\")\nax[0].axis('off')\nax[1].set_title(\"입력\")\nax[1].axis('off')\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_2.png\" />\n\n위에서 볼 수 있듯이 이미지의 구름은 매우 현실적이며 다양한 \"밀도\"와 질감을 가지며 실제 구름과 유사합니다.\n\n<div class=\"content-ad\"></div>\n\n만약 저처럼 Perlin 소음에 흥미를 느낀다면, 게임 개발 산업에서 이 소음이 어떻게 적용될 수 있는지에 대한 정말 멋진 비디오가 있어요!\n\n이제 우리가 사용할 준비가 된 데이터셋이 있으니, GANs에 대해 이야기해 보겠습니다.\n\n# 생성적 적대 신경망\n\n이 아이디어를 더 잘 설명하기 위해, 동남아시아를 여행하다가 밖이 너무 춥다고 느낄 때 후디가 절실하게 필요하다고 상상해 보세요. 가장 가까운 거리 시장에 가보니, 몇 가지 브랜드 의류가 있는 작은 가게를 발견했어요. 판매자가 유명한 브랜드 ExpensiveButNotWorthIt의 후디를 시도해보라며 괜찮은 후디를 가져다줍니다. 더 자세히 살펴보고 분명히 가짜라고 결론 내리게 됩니다. 판매자가 말합니다: '잠시만요, 진짜 것이 있어요.' 그가 다른 후디를 가져오는데, 브랜드 제품과 더 닮았지만 여전히 가짜입니다. 이와 같은 반복 작업을 몇 번 거친 후, 판매자가 전설적인 ExpensiveButNotWorthIt의 구별이 어려운 사본을 가져와 여러분은 기꺼이 구매하게 됩니다. 이것이 바로 GANs가 작동하는 방식입니다!\n\n<div class=\"content-ad\"></div>\n\nGAN의 경우, 당신은 판별자(D)라고 불립니다. 판별자의 목표는 진짜 물체와 가짜 물체를 구별하거나 이진 분류 작업을 수행하는 것입니다. 이에 반해, 생성자(G)는 높은 품질의 가짜를 생성하려고 하는 판매자라고 불립니다. 판별자와 생성자는 서로 능가하기 위해 독립적으로 훈련됩니다. 따라서 최종적으로 우리는 높은 품질의 가짜를 얻습니다.\n\n![이미지](/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_3.png)\n\n훈련 과정은 일반적으로 다음과 같이 진행됩니다:\n\n- 입력 노이즈를 샘플링합니다 (우리의 경우 구름이 있는 이미지).\n- 노이즈를 생성자(G)에 공급하고 예측을 수집합니다.\n- D 손실을 계산합니다. G의 출력에 대한 하나와 실제 데이터에 대한 다른 예측을 얻어서 이루어집니다.\n- D의 가중치를 업데이트합니다.\n- 다시 입력 노이즈를 샘플링합니다.\n- 노이즈를 생성자(G)에 공급하고 예측을 수집합니다.\n- G 손실을 계산합니다. G의 예측을 D에 공급하여 이루어집니다.\n- G의 가중치를 업데이트합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Erasing Clouds from Satellite Imagery Using GANs](/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_4.png)\n\nIn other words, we can define a value function V(G,D):\n\n![Value function V(G,D)](/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_5.png)\n\nwhere we want to minimize the term log(1-D(G(z))) to train G and maximize log D(x) to train D (in this notation x — real data sample and z — noise).\n\n\n<div class=\"content-ad\"></div>\n\n이제 파이토치에서 구현해 봅시다!\n\n원본 논문에서 저자들은 Multilayer Perceptron (MLP)을 사용하는 것에 대해 언급합니다; 이것은 ANN으로 간단히도 불립니다만, 저는 미세한 접근을 시도하고 싶습니다 — Generator로 UNet [5] 아키텍처를 사용하고, Discriminator로는 ResNet [6]을 사용하고 싶습니다. 이들은 둘 다 잘 알려진 CNN 아키텍처이기 때문에 여기서 설명하지는 않겠습니다 (댓글에서 별도의 글을 쓸지 여부를 알려주세요).\n\n이제 구축해 봅시다. Discriminator:\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch.utils.data import Subset\n```\n\n<div class=\"content-ad\"></div>\n\n```js\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Sequential(\n                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n                        nn.BatchNorm2d(out_channels),\n                        nn.ReLU())\n        self.conv2 = nn.Sequential(\n                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n                        nn.BatchNorm2d(out_channels))\n        self.downsample = downsample\n        self.relu = nn.ReLU()\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.conv2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block=ResidualBlock, all_connections=[3,4,6,3]):\n        super(ResNet, self).__init__()\n        self.inputs = 16\n        self.conv1 = nn.Sequential(\n                        nn.Conv2d(3, 16, kernel_size = 3, stride = 1, padding = 1),\n                        nn.BatchNorm2d(16),\n                        nn.ReLU()) #16x64x64\n        self.maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2) #16x32x32\n\n\n        self.layer0 = self.makeLayer(block, 16, all_connections[0], stride = 1) #connections = 3, shape: 16x32x32\n        self.layer1 = self.makeLayer(block, 32, all_connections[1], stride = 2)#connections = 4, shape: 32x16x16\n        self.layer2 = self.makeLayer(block, 128, all_connections[2], stride = 2)#connections = 6, shape: 1281x8x8\n        self.layer3 = self.makeLayer(block, 256, all_connections[3], stride = 2)#connections = 3, shape: 256x4x4\n        self.avgpool = nn.AvgPool2d(4, stride=1)\n        self.fc = nn.Linear(256, 1)\n\n    def makeLayer(self, block, outputs, connections, stride=1):\n        downsample = None\n        if stride != 1 or self.inputs != outputs:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inputs, outputs, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(outputs),\n            )\n        layers = []\n        layers.append(block(self.inputs, outputs, stride, downsample))\n        self.inputs = outputs\n        for i in range(1, connections):\n            layers.append(block(self.inputs, outputs))\n\n        return nn.Sequential(*layers)\n\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.avgpool(x)\n        x = x.view(-1, 256)\n        x = self.fc(x).flatten()\n        return F.sigmoid(x)\n```\n\nGenerator:\n\n```js\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass UNet(nn.Module):\n    def __init__(self):\n      super().__init__()\n      self.conv_1 = DoubleConv(3, 32) # 32x64x64\n      self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2) # 32x32x32\n\n      self.conv_2 = DoubleConv(32, 64)  #64x32x32\n      self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2) #64x16x16\n\n      self.conv_3 = DoubleConv(64, 128)  #128x16x16\n      self.pool_3 = nn.MaxPool2d(kernel_size=2, stride=2) #128x8x8\n\n      self.conv_4 = DoubleConv(128, 256)  #256x8x8\n      self.pool_4 = nn.MaxPool2d(kernel_size=2, stride=2) #256x4x4\n\n      self.conv_5 = DoubleConv(256, 512)  #512x2x2\n\n      #DECODER\n      self.upconv_1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2) #256x4x4\n      self.conv_6 = DoubleConv(512, 256) #256x4x4\n\n\n      self.upconv_2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) #128x8x8\n      self.conv_7 = DoubleConv(256, 128)  #128x8x8\n\n      self.upconv_3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2) #64x16x16\n      self.conv_8 = DoubleConv(128, 64)  #64x16x16\n\n      self.upconv_4 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2) #32x32x32\n      self.conv_9 = DoubleConv(64, 32)  #32x32x32\n\n      self.output = nn.Conv2d(32, 3, kernel_size = 3, stride = 1, padding = 1) #3x64x64\n\n    def forward(self, batch):\n\n      conv_1_out = self.conv_1(batch)\n      conv_2_out = self.conv_2(self.pool_1(conv_1_out))\n      conv_3_out = self.conv_3(self.pool_2(conv_2_out))\n      conv_4_out = self.conv_4(self.pool_3(conv_3_out))\n      conv_5_out = self.conv_5(self.pool_4(conv_4_out))\n\n      conv_6_out = self.conv_6(torch.cat([self.upconv_1(conv_5_out), conv_4_out], dim=1))\n      conv_7_out = self.conv_7(torch.cat([self.upconv_2(conv_6_out), conv_3_out], dim=1))\n      conv_8_out = self.conv_8(torch.cat([self.upconv_3(conv_7_out), conv_2_out], dim=1))\n      conv_9_out = self.conv_9(torch.cat([self.upconv_4(conv_8_out), conv_1_out], dim=1))\n\n      output = self.output(conv_9_out)\n\n\n      return F.sigmoid(output)\n```\n\n이제 데이터를 훈련/테스트 세트로 분할하고 torch 데이터 세트로 래핑해야합니다:\n\n<div class=\"content-ad\"></div>\n\n```python\nclass dataset(Dataset):\n    def __init__(self, batch_size, images_paths, targets, img_size=64):\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.images_paths = images_paths\n        self.targets = targets\n        self.len = len(self.images_paths) // batch_size\n\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n        self.batch_im = [self.images_paths[idx * self.batch_size:(idx + 1) * self.batch_size] for idx in range(self.len)]\n        self.batch_t = [self.targets[idx * self.batch_size:(idx + 1) * self.batch_size] for idx in range(self.len)]\n\n    def __getitem__(self, idx):\n        pred = torch.stack([\n            self.transform(Image.open(join(path_input, file_name)))\n            for file_name in self.batch_im[idx]\n        ])\n        target = torch.stack([\n            self.transform(Image.open(join(path_target, file_name)))\n            for file_name in self.batch_im[idx]\n        ])\n        return pred, target\n\n    def __len__(self):\n        return self.len\n```\n\n멋져요. 이제 훈련 루프를 작성할 시간입니다. 그 전에 손실 함수와 옵티마이저를 정의해 봅시다:\n\n```python\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nbatch_size = 64\nnum_epochs = 15\nlearning_rate_D = 1e-5\nlearning_rate_G = 1e-4\n\ndiscriminator = ResNet()\ngenerator = UNet()\n\nbce = nn.BCEWithLogitsLoss()\nl1loss = nn.L1Loss()\n\noptimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate_D)\noptimizer_G = optim.Adam(generator.parameters(), lr=learning_rate_G)\n\nscheduler_D = optim.lr_scheduler.StepLR(optimizer_D, step_size=10, gamma=0.1)\nscheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=10, gamma=0.1)\n```\n\n이전 GAN 알고리즘 그림의 손실 함수와는 다른 것을 볼 수 있습니다. 특히 L1 손실을 추가했습니다. 이 아이디어는 우리가 무작위로 이미지를 생성하는 것이 아니라 입력에서 대부분의 정보를 유지하고 노이즈만 제거하려고 한다는 것입니다. 따라서 G 손실은 다음과 같을 것입니다:\n\n<div class=\"content-ad\"></div>\n\n\nG_loss = log(1 − D(G(z))) + 𝝀 |G(z)-y|\n\ninstead of just\n\nG_loss = log(1 − D(G(z)))\n\n𝝀 is an arbitrary coefficient, which balances two components of the losses.\n\n\n<div class=\"content-ad\"></div>\n\n이제 데이터를 분할하여 훈련 과정을 시작해봅시다:\n\n```js\ntest_ratio, train_ratio = 0.3, 0.7\nnum_test = int(len(listdir(path_target)) * test_ratio)\nnum_train = int((int(len(listdir(path_target))) - num_test))\n\nimg_size = (64, 64)\n\nprint(\"훈련 샘플 수:\", num_train)\nprint(\"테스트 샘플 수:\", num_test)\n\nrandom.seed(231)\ntrain_idxs = np.array(random.sample(range(num_test + num_train), num_train))\nmask = np.ones(num_train + num_test, dtype=bool)\nmask[train_idxs] = False\n\nimages = {}\nfeatures = random.sample(listdir(path_input), num_test + num_train)\ntargets = random.sample(listdir(path_target), num_test + num_train)\n\nrandom.Random(231).shuffle(features)\nrandom.Random(231).shuffle(targets)\n\ntrain_input_img_paths = np.array(features)[train_idxs]\ntrain_target_img_path = np.array(targets)[train_idxs]\ntest_input_img_paths = np.array(features)[mask]\ntest_target_img_path = np.array(targets)[mask]\n\ntrain_loader = dataset(batch_size=batch_size, img_size=img_size, images_paths=train_input_img_paths, targets=train_target_img_path)\ntest_loader = dataset(batch_size=batch_size, img_size=img_size, images_paths=test_input_img_paths, targets=test_target_img_path)\n```\n\n이제 훈련 루프를 실행해봅시다:\n\n```js\ntrain_loss_G, train_loss_D, val_loss_G, val_loss_D = [], [], [], []\nall_loss_G, all_loss_D = [], []\nbest_generator_epoch_val_loss, best_discriminator_epoch_val_loss = -np.inf, -np.inf\nfor epoch in range(num_epochs):\n\n    discriminator.train()\n    generator.train()\n\n    discriminator_epoch_loss, generator_epoch_loss = 0, 0\n\n    for inputs, targets in train_loader:\n        inputs, true = inputs, targets\n\n        '''1. 판별자 (ResNet) 훈련하기'''\n        optimizer_D.zero_grad()\n\n        fake = generator(inputs).detach()\n\n        pred_fake = discriminator(fake).to(device)\n        loss_fake = bce(pred_fake, torch.zeros(batch_size, device=device))\n\n        pred_real = discriminator(true).to(device)\n        loss_real = bce(pred_real, torch.ones(batch_size, device=device))\n\n        loss_D = (loss_fake + loss_real) / 2\n\n        loss_D.backward()\n        optimizer_D.step()\n\n        discriminator_epoch_loss += loss_D.item()\n        all_loss_D.append(loss_D.item())\n\n        '''2. 생성자 (UNet) 훈련하기'''\n        optimizer_G.zero_grad()\n\n        fake = generator(inputs)\n        pred_fake = discriminator(fake).to(device)\n\n        loss_G_bce = bce(pred_fake, torch.ones_like(pred_fake, device=device))\n        loss_G_l1 = l1loss(fake, targets) * 100\n        loss_G = loss_G_bce + loss_G_l1\n        loss_G.backward()\n        optimizer_G.step()\n\n        generator_epoch_loss += loss_G.item()\n        all_loss_G.append(loss_G.item())\n\n    discriminator_epoch_loss /= len(train_loader)\n    generator_epoch_loss /= len(train_loader)\n    train_loss_D.append(discriminator_epoch_loss)\n    train_loss_G.append(generator_epoch_loss)\n\n    discriminator.eval()\n    generator.eval()\n\n    discriminator_epoch_val_loss, generator_epoch_val_loss = 0, 0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs, targets\n\n            fake = generator(inputs)\n            pred = discriminator(fake).to(device)\n\n            loss_G_bce = bce(fake, torch.ones_like(fake, device=device))\n            loss_G_l1 = l1loss(fake, targets) * 100\n            loss_G = loss_G_bce + loss_G_l1\n            loss_D = bce(pred.to(device), torch.zeros(batch_size, device=device))\n\n            discriminator_epoch_val_loss += loss_D.item()\n            generator_epoch_val_loss += loss_G.item()\n\n    discriminator_epoch_val_loss /= len(test_loader)\n    generator_epoch_val_loss /= len(test_loader)\n\n    val_loss_D.append(discriminator_epoch_val_loss)\n    val_loss_G.append(generator_epoch_val_loss)\n\n    print(f\"------에포크 [{epoch+1}/{num_epochs}]------\\n훈련 손실 D: {discriminator_epoch_loss:.4f}, 검증 손실 D: {discriminator_epoch_val_loss:.4f}\")\n    print(f'훈련 손실 G: {generator_epoch_loss:.4f}, 검증 손실 G: {generator_epoch_val_loss:.4f}')\n\n    if discriminator_epoch_val_loss > best_discriminator_epoch_val_loss:\n        discriminator_epoch_val_loss = best_discriminator_epoch_val_loss\n        torch.save(discriminator.state_dict(), \"discriminator.pth\")\n    if generator_epoch_val_loss > best_generator_epoch_val_loss:\n        generator_epoch_val_loss = best_generator_epoch_val_loss\n        torch.save(generator.state_dict(), \"generator.pth\")\n\n    fig, ax = plt.subplots(1,3)\n    ax[0].imshow(np.transpose(inputs.numpy()[7], (1,2,0)))\n    ax[1].imshow(np.transpose(targets.numpy()[7], (1,2,0)))\n    ax[2].imshow(np.transpose(fake.detach().numpy()[7], (1,2,0)))\n    plt.show()\n```\n\n<div class=\"content-ad\"></div>\n\n코드가 끝나면 손실을 그래프로 그려볼 수 있어요. 이 코드는 이 멋진 웹사이트에서 일부 채택되었어요:\n\n```js\nfrom matplotlib.font_manager import FontProperties\n\nbackground_color = '#001219'\nfont = FontProperties(fname='LexendDeca-VariableFont_wght.ttf')\nfig, ax = plt.subplots(1, 2, figsize=(16, 9))\nfig.set_facecolor(background_color)\nax[0].set_facecolor(background_color)\nax[1].set_facecolor(background_color)\n\nax[0].plot(range(len(all_loss_G)), all_loss_G, color='#bc6c25', lw=0.5) \nax[1].plot(range(len(all_loss_D)), all_loss_D, color='#00b4d8', lw=0.5)\n\nax[0].scatter(\n      [np.array(all_loss_G).argmax(), np.array(all_loss_G).argmin()],\n      [np.array(all_loss_G).max(), np.array(all_loss_G).min()],\n      s=30, color='#bc6c25',\n   )\nax[1].scatter(\n      [np.array(all_loss_D).argmax(), np.array(all_loss_D).argmin()],\n      [np.array(all_loss_D).max(), np.array(all_loss_D).min()],\n      s=30, color='#00b4d8',\n   )\n\nax.text(\n      np.array(all_loss_G).argmax()+60, np.array(all_loss_G).max()+0.1,\n      f'{round(np.array(all_loss_G).max(),1)}',\n      fontsize=13, color='#bc6c25',\n      font=font,\n      ax=ax[0]\n   )\nax.text(\n      np.array(all_loss_G).argmin()+60, np.array(all_loss_G).min()-0.1,\n      f'{round(np.array(all_loss_G).min(),1)}',\n      fontsize=13, color='#bc6c25',\n      font=font,\n      ax=ax[0]\n   )\n\nax.text(\n      np.array(all_loss_D).argmax()+60, np.array(all_loss_D).max()+0.01,\n      f'{round(np.array(all_loss_D).max(),1)}',\n      fontsize=13, color='#00b4d8',\n      font=font,\n      ax=ax[1]\n   )\nax.text(\n      np.array(all_loss_D).argmin()+60, np.array(all_loss_D).min()-0.005,\n      f'{round(np.array(all_loss_D).min(),1)}',\n      fontsize=13, color='#00b4d8',\n      font=font,\n      ax=ax[1]\n   )\nfor i in range(2):\n    ax[i].tick_params(axis='x', colors='white')\n    ax[i].tick_params(axis='y', colors='white')\n    ax[i].spines['left'].set_color('white') \n    ax[i].spines['bottom'].set_color('white') \n    ax[i].set_xlabel('Epoch', color='white', fontproperties=font, fontsize=13)\n    ax[i].set_ylabel('Loss', color='white', fontproperties=font, fontsize=13)\n\nax[0].set_title('Generator', color='white', fontproperties=font, fontsize=18)\nax[1].set_title('Discriminator', color='white', fontproperties=font, fontsize=18)\nplt.savefig('Loss.jpg')\nplt.show()\n# ax[0].set_axis_off()\n# ax[1].set_axis_off()\n```\n\n또한 테스트 데이터셋에서 임의의 샘플을 시각화할게요:\n\n```js\nrandom.Random(2).shuffle(test_target_img_path)\nrandom.Random(2).shuffle(test_input_img_paths)\nsubset_loader = dataset(batch_size=5, img_size=img_size, images_paths=test_input_img_paths,\n                        targets=test_target_img_path)\ngenerator = UNet()\ngenerator.load_state_dict(torch.load('generator.pth'))\n\ngenerator.eval()\nfor X, y in subset_loader:\n    fig, axes = plt.subplots(5, 3, figsize=(9, 9))\n\n    for i in range(5):\n        axes[i, 0].imshow(np.transpose(X.numpy()[i], (1, 2, 0)))\n        axes[i, 0].set_title(\"Input\")\n        axes[i, 0].axis('off')\n        \n        axes[i, 1].imshow(np.transpose(y.numpy()[i], (1, 2, 0)))\n        axes[i, 1].set_title(\"Target\")\n        axes[i, 1].axis('off')\n        \n        generated_image = generator(X[i].unsqueeze(0)).detach().numpy()[0]\n        axes[i, 2].imshow(np.transpose(generated_image, (1, 2, 0)))\n        axes[i, 2].set_title(\"Generated\")\n        axes[i, 2].axis('off')\n    \n    # 레이아웃 조정\n    plt.tight_layout()\n    plt.savefig('Test.jpg')\n    plt.show()\n    break \n```\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_6.png\" />\n\n여기서 보시다시피, 결과는 완벽하지 않고 땅커버 유형에 매우 의존합니다. 그럼에도 불구하고, 구축된 모델은 이미지에서 구름을 제거하며, G 및 D 깊이를 늘리는 것으로 성능을 향상시킬 수 있습니다. 다른 유망한 전략은 서로 다른 땅커버 유형을 위해 별도의 모델을 훈련시키는 것입니다. 예를 들어, 작물밭과 물 투구는 분명히 다른 공간적 특징을 가지고 있기 때문에 일반화 모델의 능력에 영향을 주는 경우가 있습니다.\n\n이 기사가 지리정보 도메인에서 심층 학습 알고리즘을 적용하는 데 새로운 시각을 제공해 드렸기를 바랍니다. 내 생각에는, GANs는 데이터 과학자가 활용할 수 있는 가장 강력한 도구 중 하나이며, 여러분의 도구 상자의 필수적인 부분이 되길 희망합니다!\n\n===========================================\n\n<div class=\"content-ad\"></div>\n\n참고문헌:\n\n1. Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville 및 Yoshua Bengio. “Generative adversarial nets.” Advances in neural information processing systems 27 (2014). [논문 링크](https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)\n\n2. Helber, Patrick, Benjamin Bischke, Andreas Dengel 및 Damian Borth. “Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 12, no. 7 (2019): 2217–2226. [논문 링크](https://arxiv.org/pdf/1709.00029)\n\n3. Wen, Xue, Zongxu Pan, Yuxin Hu 및 Jiayin Liu. “Generative adversarial learning in YUV color space for thin cloud removal on satellite imagery.” Remote Sensing 13, no. 6 (2021): 1079. [논문 링크](https://www.mdpi.com/2072-4292/13/6/1079)\n\n<div class=\"content-ad\"></div>\n\n5. Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. “U-net: Convolutional networks for biomedical image segmentation.” In Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5–9, 2015, proceedings, part III 18, pp. 234–241. Springer International Publishing, 2015. [Link](https://arxiv.org/pdf/1505.04597)\n\n6. He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. [Link](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n\n===========================================\n\n<div class=\"content-ad\"></div>\n\n제 Medium의 모든 게시물은 무료이며 공개되어 있습니다. 그래서 여기서 저를 팔로우해 주시면 정말 감사하겠습니다!\n\nP.s. 저는 (지리)데이터 과학, 머신 러닝/인공지능, 기후 변화에 대해 열정적으로 관심을 가지고 있습니다. 그래서 어떤 프로젝트에서 함께 작업하고 싶다면 LinkedIn에서 연락 주세요.\n\n🛰️더 많은 소식을 받아보려면 팔로우하세요!🛰️","ogImage":{"url":"/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_0.png"},"coverImage":"/assets/img/2024-06-19-ErasingCloudsfromSatelliteImageryUsingGANsGenerativeAdversarialNetworks_0.png","tag":["Tech"],"readingTime":25},{"title":"파이토치PyTorch를 사용하여 처음부터 Large Language Model LLM을 만들어 보세요","description":"","date":"2024-06-19 18:43","slug":"2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch","content":"\n\n## LLM 만들고 트레이닝하는 단계별 가이드입니다. 이 모델의 목표는 영어를 말레이어로 번역하는 것입니다.\n\n![이미지](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_0.png)\n\n이 글을 마치면 어떤 결과를 얻을 수 있을까요? 직접 코딩하면서 Large Language Model (LLM)을 만들고 트레이닝할 수 있게 될 거에요. 영어를 말레이어로 번역하는 LLM을 만들지만, 다른 언어 번역 작업을 위해 이 LLM 아키텍처를 쉽게 수정할 수 있습니다.\n\nLLM은 ChatGPT, Gemini, MetaAI, Mistral AI 등과 같은 인기 있는 AI 챗봇의 핵심 기반이 됩니다. 모든 LLM의 핵심에는 Transformer라는 아키텍처가 있습니다. 따라서, 먼저 유명한 논문 \"Attention is all you need\"을 바탕으로 Transformer 아키텍처를 구축할 것입니다 - https://arxiv.org/abs/1706.03762.\n\n<div class=\"content-ad\"></div>\n\n![transformer_step_by_step](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_1.png)\n\n먼저, 우리는 트랜스포머 모델의 모든 구성 요소를 블록 단위로 구축할 것입니다. 그런 다음, 모든 블록을 조합하여 모델을 구축할 것입니다. 그 후에는 Hugging Face 데이터셋에서 얻을 데이터셋으로 모델을 훈련하고 유효성을 검사할 것입니다. 마지막으로 새 번역 텍스트 데이터에 대한 번역을 수행하여 모델을 테스트할 것입니다.\n\n중요 사항: 저는 트랜스포머 아키텍처의 모든 구성 요소를 단계별로 코딩하고 '무엇, 왜, 어떻게'에 대한 개념에 대한 필요한 설명을 제공할 것입니다. 또한 설명이 필요한 특정 코드 라인에 대해 주석을 제공할 것입니다. 이렇게 하면 직접 코딩하면서 전체 워크플로에 연결할 수 있을 것이라고 믿습니다.\n\n![transformer_architecture](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_2.png)\n\n<div class=\"content-ad\"></div>\n\n함께 코딩해요!\n\n## 단계 1: 데이터셋 로드\n\nLLM 모델이 영어에서 말레이어로 번역하는 작업을 할 수 있도록 하려면 소스(영어)와 대상(말레이어) 언어 쌍이 있는 데이터셋을 사용해야 합니다. 따라서, Huggingface에서 \"Helsinki-NLP/opus-100\"라는 데이터셋을 사용할 것입니다. 이 데이터셋은 1백만 개의 영어-말레이어 훈련 데이터셋을 가지고 있어서 좋은 정확도를 얻기에 충분하며, 검증 및 테스트 데이터셋에 각각 2,000개의 데이터가 있습니다. 데이터는 이미 사전 분할되어 있어서 데이터셋을 다시 분할할 필요가 없습니다.\n\n\n# 필요한 라이브러리 가져오기\n# 아직 안 했다면 (!pip install datasets, tokenizers)를 사용하여 데이터셋 및 토크나이저 라이브러리 설치하기.\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# GPU가 사용 가능하다면 \"cuda\"로 장치 값을 할당하여 GPU에서 훈련합니다. 사용할 수 없는 경우 기본값인 \"cpu\"로 되돌릴 것입니다.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n\n# Huggingface 경로에서 훈련, 검증, 테스트 데이터셋을 로드합니다.\nraw_train_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ms\", split='train')\nraw_validation_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ms\", split='validation')\nraw_test_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ms\", split='test')\n\n# 데이터셋 파일을 저장할 디렉토리 생성\nos.mkdir(\"./dataset-en\")\nos.mkdir(\"./dataset-my\")\n\n# 각 에폭 후 모델 훈련 중에 모델을 저장할 디렉토리 생성 (단계 10에서 사용).\nos.mkdir(\"./malaygpt\")\n\n# 소스 및 대상 토크나이저를 저장할 디렉토리 생성.\nos.mkdir(\"./tokenizer_en\")\nos.mkdir(\"./tokenizer_my\")\n\ndataset_en = []\ndataset_my = []\nfile_count = 1\n\n# 토크나이저를 훈련하기 위해 (단계 2에서) 훈련 데이터셋을 영어 및 말레이어로 분리합니다.\n# 각 파일에 50,000개씩 작은 데이터 파일을 만들어 dataset-en 및 dataset-my 디렉토리에 저장합니다.\nfor data in tqdm(raw_train_dataset[\"translation\"]):\n    dataset_en.append(data[\"en\"].replace('\\n', \" \"))\n    dataset_my.append(data[\"ms\"].replace('\\n', \" \"))\n    if len(dataset_en) == 50000:\n        with open(f'./dataset-en/file{file_count}.txt', 'w', encoding='utf-8') as fp:\n            fp.write('\\n'.join(dataset_en))\n            dataset_en = []\n\n        with open(f'./dataset-my/file{file_count}.txt', 'w', encoding='utf-8') as fp:\n            fp.write('\\n'.join(dataset_my))\n            dataset_my = []\n        file_count += 1\n\n\n<div class=\"content-ad\"></div>\n\n## 단계 2: 토크나이저 생성\n\n트랜스포머 모델은 원시 텍스트를 처리하지 않으며, 숫자만 처리합니다. 따라서 원시 텍스트를 숫자로 변환하기 위해 어떤 작업을 해야 할 것입니다. 이를 위해 저희는 GPT3와 같은 모델에서 사용되는 서브워드 토크나이저인 BPE 토크나이저를 사용할 것입니다. 먼저 우리가 단계 1에서 준비한 코퍼스 데이터(이 경우 교육 데이터 셋)로 BPE 토크나이저를 먼저 학습할 것입니다. 아래 다이어그램과 같이 진행됩니다.\n\n![image](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_3.png)\n\n학습이 완료되면 토크나이저는 영어와 말레이 언어용 어휘를 생성합니다. 어휘는 코퍼스 데이터에서 고유한 토큰들의 컬렉션입니다. 번역 작업을 수행하기 때문에 두 언어에 대한 토크나이저가 필요합니다. BPE 토크나이저는 원시 텍스트를 가져와 어휘 내의 토큰들과 매핑한 후, 입력된 원시 텍스트의 각 단어에 대해 토큰을 반환합니다. 토큰은 단일 단어나 서브워드가 될 수 있습니다. 이는 다른 토크나이저에 비해 서브워드 토크나이저의 장점 중 하나입니다. 그리고 토크나이저는 그 고유한 인덱스 또는 위치 ID를 반환하고, 이는 위의 흐름에서 임베딩을 생성하는 데 추가로 사용될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 토크나이저 라이브러리 클래스 및 모듈 가져오기.\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\n# 토크나이저를 훈련시킬 학습 데이터셋 파일 경로.\npath_en = [str(file) for file in Path('./dataset-en').glob(\"**/*.txt\")]\npath_my = [str(file) for file in Path('./dataset-my').glob(\"**/*.txt\")]\n\n# [원본 언어 토크나이저(영어) 생성].\n# [UNK] - 알 수 없는 단어를 나타내는 특수 토큰 생성, [PAD] - 패딩 토큰으로 모델 간 시퀀스 길이를 일정하게 유지하기 위함.\n# [CLS] - 문장 시작을 표시하는 토큰, [SEP] - 문장 끝을 표시하는 토큰 등의 추가 특수 토큰 생성.\ntokenizer_en = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer_en = BpeTrainer(min_frequency=2, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\", \"[SEP]\", \"[MASK]\"])\n\n# 토큰을 공백을 기준으로 분리.\ntokenizer_en.pre_tokenizer = Whitespace()\n\n# 데이터셋 파일로 토크나이저 훈련.\ntokenizer_en.train(files=path_en, trainer=trainer_en)\n\n# 향후 사용을 위해 토크나이저 저장.\ntokenizer_en.save(\"./tokenizer_en/tokenizer_en.json\")\n\n# [타겟 언어 토크나이저(말레이어) 생성].\ntokenizer_my = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer_my = BpeTrainer(min_frequency=2, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\", \"[SEP]\", \"[MASK]\"])\ntokenizer_my.pre_tokenizer = Whitespace()\ntokenizer_my.train(files=path_my, trainer=trainer_my)\ntokenizer_my.save(\"./tokenizer_my/tokenizer_my.json\")\n\ntokenizer_en = Tokenizer.from_file(\"./tokenizer_en/tokenizer_en.json\")\ntokenizer_my = Tokenizer.from_file(\"./tokenizer_my/tokenizer_my.json\")\n\n# 각 토크나이저의 크기 확인.\nsource_vocab_size = tokenizer_en.get_vocab_size()\ntarget_vocab_size = tokenizer_my.get_vocab_size()\n\n# 토큰 ID 변수 정의, 모델 훈련에 사용.\nCLS_ID = torch.tensor([tokenizer_my.token_to_id(\"[CLS]\")], dtype=torch.int64).to(device)\nSEP_ID = torch.tensor([tokenizer_my.token_to_id(\"[SEP]\")], dtype=torch.int64).to(device)\nPAD_ID = torch.tensor([tokenizer_my.token_to_id(\"[PAD]\")], dtype=torch.int64).to(device)\n```\n\n## Step 3: 데이터셋 및 DataLoader 준비\n\n이 단계에서는 나중에 구축할 모델을 훈련하고 검증하기 위해 소스 언어와 타겟 언어 각각에 대한 데이터셋을 준비할 것입니다. 우리는 원시 데이터셋을 입력으로 받아 소스(토크나이저_en)와 타겟(토크나이저_my) 텍스트를 각각 인코딩하는 기능을 정의하는 클래스를 생성할 것입니다. 마지막으로, 훈련 및 검증 데이터셋을 위해 DataLoader를 생성하겠습니다. 이 DataLoader는 배치 단위로 데이터셋을 반복하며(예: 배치 크기는 10으로 설정될 수 있음), 데이터 크기와 사용 가능한 처리 능력에 따라 배치 크기를 조정할 수 있습니다.\n\n```js\n# 이 클래스는 원시 데이터셋과 max_seq_len (전체 데이터셋에서 시퀀스의 최대 길이)을 가져옵니다.\nclass EncodeDataset(Dataset):\n    def __init__(self, raw_dataset, max_seq_len):\n        super().__init__()\n        self.raw_dataset = raw_dataset\n        self.max_seq_len = max_seq_len\n    \n    def __len__(self):\n        return len(self.raw_dataset)\n\n    def __getitem__(self, index):\n        \n        # 주어진 인덱스의 원시 텍스트를 가져와 소스 및 타겟 텍스트로 분리함.\n        raw_text = self.raw_dataset[index]\n        \n        # 소스 텍스트와 타겟 텍스트를 인코딩하기 위해 소스 토크나이저(tokenizer_en) 및 타겟 토크나이저(tokenizer_my)를 사용합니다.\n        source_text_encoded = torch.tensor(tokenizer_en.encode(source_text).ids, dtype = torch.int64).to(device)    \n        target_text_encoded = torch.tensor(tokenizer_my.encode(target_text).ids, dtype = torch.int64).to(device)\n\n        # 모델 훈련을 위해 각 입력 시퀀스의 길이가 max_seq_len과 동일하도록 만들기 위해 필요한 만큼의 패딩을 추가합니다.\n        num_source_padding = self.max_seq_len - len(source_text_encoded) - 2 \n        num_target_padding = self.max_seq_len - len(target_text_encoded) - 1 \n\n        encoder_padding = torch.tensor([PAD_ID] * num_source_padding, dtype = torch.int64).to(device)\n        decoder_padding = torch.tensor([PAD_ID] * num_target_padding, dtype = torch.int64).to(device)\n\n        # 인코더 입력은 문장 시작 토큰인 CLS_ID로 시작하여 소스 인코딩으로 이어지고 문장 끝 토큰인 SEP가 뒤따릅니다.\n        # 필요한 max_seq_len에 도달하기 위해 마지막에 PAD 토큰이 추가됩니다.\n        encoder_input = torch.cat([CLS_ID, source_text_encoded, SEP_ID, encoder_padding]).to(device)\n\n        # 디코더 입력은 문장 시작 토큰인 CLS_ID로 시작하여 타겟 인코딩이 뒤따릅니다.\n        # 필요한 max_seq_len에 도달하기 위해 마지막에 PAD 토큰이 추가됩니다. 디코더 입력에는 문장 끝 토큰인 SEP는 포함되지 않습니다.\n        decoder_input = torch.cat([CLS_ID, target_text_encoded, decoder_padding]).to(device)\n\n        # 타겟 레이블은 타겟 인코딩이 먼저 오고 문장 끝 토큰인 SEP가 뒤따릅니다. 시작 문장 토큰인 CLS는 없습니다.\n        # 필요한 max_seq_len에 도달하기 위해 마지막에 PAD 토큰이 추가됩니다.\n        target_label = torch.cat([target_text_encoded,SEP_ID,decoder_padding]).to(device)\n\n        # 인코딩 시 추가된 패딩 토큰을 모델이 학습하지 않도록 하기 위해 인코더 마스크를 사용하여 padding 토큰 값을 계산하기 전에 무효화합니다.\n        encoder_mask = (encoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int().to(device)\n\n        # 디코딩 단계에서는 현재 토큰 이후의 토큰에 영향을 받지 않도록 하기 위해 인과 마스크를 구현합니다.\n        decoder_mask = (decoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)).to(device)\n\n        return {\n            'encoder_input': encoder_input,\n            'decoder_input': decoder_input,\n            'target_label': target_label,\n            'encoder_mask': encoder_mask,\n            'decoder_mask': decoder_mask,\n            'source_text': source_text,\n            'target_text': target_text\n        }\n\n# 인과 마스크는 현재 토큰 이후에 올 토큰을 마스킹하여 softmax 함수 이후 -inf로 대체됩니다. 이를 통해 모델은 이러한 값들을 무시하거나 이를 통해 학습을 어렵게 합니다.\ndef causal_mask(size):\n  # 인과 마스크의\n\n<div class=\"content-ad\"></div>\n\n## 단계 4: 입력 임베딩 및 위치 인코딩\n\n입력 임베딩: 단계 2의 토큰 생성기에서 생성된 토큰 ID 시퀀스가 임베딩 레이어로 공급될 것입니다. 임베딩 레이어는 토큰 ID를 어휘와 매핑하고 각 토큰에 대해 512 차원의 임베딩 벡터를 생성합니다. [512 차원은 어텐션 논문에서 가져왔습니다]. 임베딩 벡터는 토큰의 의미를 캡쳐할 수 있으며, 그것은 학습된 데이터셋에 기반하여 학습되었습니다. 임베딩 벡터 내의 각 차원 값은 토큰과 관련된 특징을 나타냅니다. 예를 들어, 토큰이 '개'인 경우, 일부 차원 값은 눈, 입, 다리, 키 등을 나타낼 것입니다. n차원 공간에 벡터를 그린다면, 비슷해 보이는 객체인 개와 고양이는 서로 가깝게 위치하고, 비슷해 보이지 않는 학교, 집 임베딩 벡터는 훨씬 더 멀리 위치해 있을 것입니다.\n\n위치 인코딩: 트랜스포머 아키텍처의 장점 중 하나는 어떤 수의 입력 시퀀스든 병렬로 처리할 수 있다는 것이며, 이는 훈련 시간을 많이 줄이고 예측을 훨씬 빠르게 만듭니다. 그러나 단점 중 하나는 병렬로 많은 토큰 시퀀스를 처리하는 동안, 문장 내 토큰의 위치가 순서대로 되지 않을 수 있다는 것입니다. 이로 인해 토큰의 위치에 따라 문장의 의미나 문맥이 달라질 수 있습니다. 따라서 이 문제를 해결하기 위해 어텐션 논문은 위치 인코딩 방법을 구현했습니다. 이 논문은 각 토큰의 512 차원에 대해 두 가지 수학 함수(sin과 cosine)를 적용하는 것을 제안했습니다. 아래는 간단한 sin과 cosine 수학 함수입니다.\n\n![이미지](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_4.png)\n\n<div class=\"content-ad\"></div>\n\nSin 함수는 임베딩 벡터의 각 짝수 차원 값에 적용되고, 코사인 함수는 홀수 차원 값에 적용됩니다. 최종적으로, 결과적인 위치 인코더 벡터는 임베딩 벡터에 추가됩니다. 이제 우리는 토큰의 의미와 위치를 모두 잡을 수 있는 임베딩 벡터를 갖게 되었습니다. 주의할 점은 위치 인코딩의 값이 각 시퀀스에서 동일하다는 것입니다.\n\n# 입력 임베딩과 위치 인코딩\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int):\n        super().__init__()\n        self.d_model = d_model\n        \n        # PyTorch의 임베딩 레이어 모듈을 사용하여 토큰 ID를 어휘에 매핑한 후 임베딩 벡터로 변환합니다.\n        # vocab_size는 훈련 데이터셋의 어휘 크기이며, 토큰화기가 코퍼스 데이터셋 훈련 중에 생성한 것입니다.\n        self.embedding = nn.Embedding(vocab_size, d_model)\n    \n    def forward(self, input):\n        # 입력 시퀀스를 임베딩 레이어에 공급할 때, d_model의 제곱근을 곱하는 추가로 정규화 작업을 수행합니다.\n        embedding_output = self.embedding(input) * math.sqrt(self.d_model)\n        return embedding_output\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, max_seq_len: int, d_model: int, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # 임베딩 벡터와 동일한 모양의 행렬을 만듭니다.\n        pe = torch.zeros(max_seq_len, d_model)\n        \n        # PE 함수의 위치 부분을 계산합니다.\n        pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n\n        # PE 함수의 나눗셈 부분을 계산합니다. 지수 함수의 표현이 논문 표현과 약간 다르지만 더 잘 작동하는 것으로 보입니다.\n        div_term = torch.exp(torch.arange(0, d_model, 2).float()) * (-math.log(10000)/d_model)\n        \n        # sin 및 cosine 수학 함수 결과로 홀수 및 짝수 행렬 값을 채웁니다.\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n        \n        # 입력 시퀀스가 배치로 예상되므로 추가적인 batch_size 차원이 0 위치에 추가됩니다.\n        pe = pe.unsqueeze(0)    \n    \n    def forward(self, input_embdding):\n        # 입력 임베딩 벡터에 위치 인코딩을 추가합니다.\n        input_embdding = input_embdding + (self.pe[:, :input_embdding.shape[1], :]).requires_grad_(False)  \n        \n        # 과적합을 방지하기 위해 드롭아웃을 수행합니다.\n        return self.dropout(input_embdding)\n\n## 단계 5: 멀티 헤드 어텐션 블록\n\n트랜스포머가 LLM의 핵심인 것처럼, 셀프 어텐션 메커니즘은 트랜스포머 아키텍처의 핵심입니다.\n\n<div class=\"content-ad\"></div>\n\n자가 주의가 필요한 이유는 무엇인가요? 아래 간단한 예를 통해 답변해보겠습니다.\n\n![Example](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_5.png)\n\n문장 1과 문장 2에서 “bank”라는 단어는 명백히 두 가지 다른 의미를 가지고 있습니다. 그러나 “bank” 단어의 임베딩 값은 두 문장 모두에서 동일합니다. 이는 적절하지 않습니다. 우리는 임베딩 값이 문맥에 따라 변경되어야 한다는 것을 원합니다. 따라서 문장의 전체 의미에 기반하여 문맥적 의미를 나타낼 수 있는 동적 임베딩 값을 가지는 메커니즘이 필요합니다. 자가 주의 메커니즘은 문장의 전체 의미에 기반하여 문맥적 의미를 나타낼 수 있는 임베딩 값을 동적으로 업데이트할 수 있습니다.\n\n자가 주의가 이미 좋은데, 왜 다중 머리 자가 주의가 필요할까요? 답을 알아보기 위해 아래 예시를 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n```\n![image](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_6.png)\n\n이 예에서는 self-attention을 사용할 때 문장의 한 측면에만 집중할 수 있는 가능성이 있습니다. 아마도 \"what\" 측면만을 포착할 수 있을 것입니다. 예를 들어, \"존이 무엇을 했나요?\"와 같이요. 그러나 \"언제\"나 \"어디\"와 같은 다른 측면들도 모델이 더 나은 성능을 발휘하기 위해 동등한 중요성을 갖습니다. 그래서, Self-Attention 메커니즘이 한 문장 내에서 여러 관계를 학습하도록 하는 방법을 찾아야 합니다. 이것이 Multi-Head Self Attention(Multi-Head Attention으로도 교차 사용 가능)이 해결해 주는 곳이죠. Multi-Head Attention에서는 단일 헤드 임베딩을 여러 헤드로 분할하여 각 헤드가 문장의 다른 측면을 살펴보고 그에 맞게 학습합니다. 이것이 우리가 원하는 바입니다.\n\n이제 왜 Multi-Head Attention이 필요한지 알게 되었습니다. 이제 어떻게 작용하는지 살펴보겠습니다. Multi-Head Attention은 실제로 어떻게 작동하는 걸까요? 바로 살펴보겠습니다.\n\n행렬 곱셈에 익숙하시다면, 이 메카니즘을 이해하는 것은 꽤 쉬운 작업일 것입니다. 먼저 전체 플로우 다이어그램을 살펴보고 Multi-Head Attention의 입력부터 출력까지의 플로우를 아래 일목요연하게 설명하겠습니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_7.png)\n\n1. 먼저, 인코더 입력의 3개 복사본을 만들어봅시다 (입력 임베딩과 위치 인코딩의 조합, 이것은 단계 4에서 했었습니다). 각각을 Q, K, V라고 이름 붙여봅시다. 이들은 단지 인코더 입력의 복사본일 뿐입니다. 인코더 입력 형태: (seq_len, d_model), seq_len: 최대 시퀀스 길이, d_model: 이 경우에는 임베딩 벡터 차원이 512입니다.\n\n2. 다음으로, Q를 가중치 W_q, K를 가중치 W_k, V를 가중치 W_v와 행렬 곱셈을 수행하겠습니다. 각 가중치 행렬의 형태는 (d_model, d_model)입니다. 새로 얻게 된 쿼리, 키, 밸류 임베딩 벡터의 형태는 (seq_len, d_model)입니다. 가중치 매개변수들은 모델에 의해 무작위로 초기화되며 나준에 모델이 훈련을 시작할 때 업데이트될 것입니다. 어째서 우리가 처음부터 하는 가중치 행렬 곱셈이 필요한 것일까요? 왜냐하면 이것들은 쿼리, 키, 밸류 임베딩 벡터에 더 나은 표현을 제공하기 위해 필요한 학습 가능한 매개변수들이기 때문입니다.\n\n3. 어텐션 논문에 따르면, 헤드(heads) 수는 8입니다. 각 새로운 쿼리, 키, 밸류 임베딩 벡터는 8개의 더 작은 유닛으로 나뉘어집니다. 임베딩 벡터의 새로운 형태는 (seq_len, d_model/num_heads) 또는 (seq_len, d_k)입니다. [ d_k = d_model/num_heads ].\n\n\n<div class=\"content-ad\"></div>\n\n4. 각 쿼리 임베딩 벡터는 자신 및 시퀀스의 모든 다른 임베딩 벡터의 키 임베딩 벡터의 전치와 닷 프로덕트 연산을 수행합니다. 이 닷 프로덕트는 주의 점수를 제공합니다. 주의 점수는 주어진 토큰이 주어진 입력 시퀀스의 다른 모든 토큰과 얼마나 유사한지를 보여줍니다. 점수가 높을수록 유사도가 더 높습니다.\n\n- 주의 점수는 나중에 매트릭스 전체에 걸쳐 점수 값을 정규화하는 데 필요한 d_k의 제곱근으로 나눠집니다. 그러나 왜 d_k로 나눠 정규화해야 하는 걸까요? 어떤 다른 숫자여도 괜찮을 텐데요. 주된 이유는 임베딩 벡터 차원이 증가함에 따라 주의 매트릭스의 총 분산이 비례해서 증가하기 때문입니다. 그래서 d_k로 나누면 분산 증가를 균형시킬 수 있습니다. 만약 d_k로 나누지 않으면, 어떤 높은 주의 점수라도 소프트맥스 함수는 매우 높은 확률 값을 제공하고, 반대로 낮은 주의 점수 값에 대해서는 소프트맥스 함수가 매우 낮은 확률 값을 제공할 것입니다. 이로 인해 모델은 그러한 확률 값이 있는 특징만 학습하려 하고, 낮은 확률 값이 있는 특징을 무시하기 쉽습니다. 이는 그라디언트가 소실되는 문제로 이어지게 됩니다. 따라서 주의 점수 매트릭스를 정규화하는 것이 매우 중요합니다.\n- 소프트맥스 함수를 수행하기 전에, 인코더 마스크가 None이 아닌 경우, 주의 점수는 인코더 마스크와 매트릭스 곱셈이 될 것입니다. 마스크가 인과적 마스크인 경우, 입력 시퀀스에서 그 이후에 오는 임베딩 토큰들에 대한 주의 점수 값은 -ve 무한대로 대체됩니다. 소프트맥스 함수는 -ve 무한대 값을 거의 0 값으로 변환할 것입니다. 그래서 모델은 현재 토큰 이후에 나오는 특징을 학습하지 않을 것입니다. 이것이 우리 모델 학습에 미래 토큰이 영향을 미치는 것을 방지하는 방법입니다.\n\n5. 소프트맥스 함수가 주의 점수 매트릭스에 적용되고 (seq_len, seq_len) 모양의 가중치 매트릭스가 출력됩니다.\n\n6. 이 가중치 매트릭스는 해당 값 임베딩 벡터와 매트릭스 곱셈을 수행할 것입니다. 결과적으로 (seq_len, d_v) 모양의 8개의 주의 헤드가 생성됩니다. [ d_v = d_model/num_heads ].\n\n<div class=\"content-ad\"></div>\n\n그러면, 모든 헤드들이 새로운 형태(seq_len, d_model)를 갖는 단일 헤드로 연결됩니다. 이 새로운 단일 헤드는 출력 가중치 행렬 W_o(d_model, d_model)과 행렬 곱셈을 수행합니다. Multi-Head Attention의 최종 출력은 단어의 문맥적 의미와 입력 문장의 여러 측면을 학습하는 능력을 나타냅니다.\n\n이제 Multi-Head Attention 블록 코딩을 시작해봅시다. 이것은 훨씬 쉽고 간단할 거예요.\n\n```js\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout_rate: float):\n        super().__init__()\n        # 과적합을 방지하기 위해 드롭아웃을 정의합니다.\n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # 가중치 행렬은 도입되며 모두 학습 가능한 매개변수입니다.\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        self.num_heads = num_heads\n        assert d_model % num_heads == 0, \"d_model은 헤드 수로 나눌 수 있어야 합니다.\"\n        \n        # d_k는 각 분할된 self attention 헤드의 새로운 차원입니다\n        self.d_k = d_model // num_heads\n\n    def forward(self, q, k, v, encoder_mask=None):\n        \n        # 여러 시퀀스 배치로 모델을 한 번에 병렬로 학습하게 될 것이므로, 모양에 배치 크기를 포함해야 합니다.\n        # 쿼리, 키 및 값은 해당 가중치와 입력 임베딩의 행렬 곱으로 계산됩니다.\n        # 모양 변화: q(배치 크기, seq_len, d_model) @ W_q(d_model, d_model) => query(배치 크기, seq_len, d_model) [키와 값도 동일함].\n        query = self.W_q(q) \n        key = self.W_k(k)\n        value = self.W_v(v)\n\n        # 쿼리, 키, 밸류를 헤드 수로 분할합니다. d_model은 d_k마다 8개 헤드로 분할됩니다.\n        # 모양 변화: query(배치 크기, seq_len, d_model) => query(배치 크기, seq_len, num_heads, d_k) -> query(배치 크기, num_heads, seq_len, d_k) [키와 값도 동일함].\n        query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1,2)\n        key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1,2)\n        value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1,2)\n\n        # :: SELF ATTENTION BLOCK STARTS ::\n\n        # 유사도 또는 쿼리 간의 관련성을 찾기 위해 주의 점수가 계산됩니다.\n        # 모양 변화: query(배치 크기, num_heads, seq_len, d_k) @ key(배치 크기, num_heads, seq_len, d_k) => attention_score(배치 크기, num_heads, seq_len, seq_len).\n        attention_score = (query @ key.transpose(-2,-1)) / math.sqrt(self.d_k)\n\n        # 만약 마스크가 제공된다면, 주의 점수를 마스크 값에 따라 수정해야 합니다. 자세한 내용은 4번을 참조하세요.\n        if encoder_mask is not None:\n            attention_score = attention_score.masked_fill(encoder_mask == 0, -1e9)\n        \n        # softmax 함수는 모든 주의 점수 중에서 확률 분포를 계산합니다. 더 높은 주의 점수에 더 높은 확률 값을 할당합니다. 즉, 보다 유사한 토큰은 더 높은 확률 값을 가집니다.\n        # 모양 변화: attention_score와 동일합니다.\n        attention_weight = torch.softmax(attention_score, dim=-1)\n\n        if self.dropout is not None:\n            attention_weight = self.dropout(attention_weight)\n\n        # Self attention 블록의 최종 단계는 주의 가중치를 값 임베딩 벡터와의 행렬 곱셈입니다.\n        # 모양 변화: attention_score(배치 크기, num_heads, seq_len, seq_len) @ value(배치 크기, num_heads, seq_len, d_k) => attention_output(배치 크기, num_heads, seq_len, d_k)\n        attention_output = attention_score @ value\n        \n        # :: SELF ATTENTION BLOCK ENDS ::\n\n        # 이제, 모든 헤드들이 다시 단일 헤드로 결합됩니다.\n        # 모양 변화: attention_output(배치 크기, num_heads, seq_len, d_k) => attention_output(배치 크기, seq_len, num_heads, d_k) => attention_output(배치 크기, seq_len, d_model)        \n        attention_output = attention_output.transpose(1,2).contiguous().view(attention_output.shape[0], -1, self.num_heads * self.d_k)\n\n        # 마침내 attention_output을 출력 가중치 행렬로 행렬 곱하여 최종 Multi-Head attention 출력을 얻습니다.\n        # multihead_output의 모양은 임베딩 입력과 동일합니다.\n        # 모양 변화: attention_output(배치 크기, seq_len, d_model) @ W_o(d_model, d_model) => multihead_output(배치 크기, seq_len, d_model)\n        multihead_output = self.W_o(attention_output)\n        \n        return multihead_output\n```\n\n## Step 6: 피드포워드 네트워크, 레이어 정규화 및 AddAndNorm\n\n<div class=\"content-ad\"></div>\n\n피드포워드 네트워크: 피드포워드 네트워크는 두 개의 선형 레이어(첫 번째는 d_model 노드를 가지고 두 번째는 d_ff 노드를 가지며, 주어진 값은 어텐션 논문에 따라 할당됩니다)를 통해 임베딩 벡터의 모든 기능을 학습하는 딥 신경망을 사용합니다. 첫 번째 선형 레이어의 출력에는 ReLU 활성화 함수가 적용되어 임베딩 값을 비선형으로 만들고, 과적합을 피하기 위해 드롭아웃이 적용됩니다.\n\n레이어 정규화: 네트워크 내 임베딩 벡터의 값 분포가 일관되게 유지되도록 임베딩 값에 레이어 정규화를 적용합니다. 이는 원활한 학습을 보장합니다. 네트워크가 필요로 하는대로 임베딩 값을 스케일링하고 이동시키기 위해 gamma와 beta라는 추가 학습 매개변수를 사용할 것입니다.\n\nAddAndNorm: 이는 스킵 연결과 레이어 정규화(이전에 설명함)로 구성됩니다. 순방향 패스에서 스킵 연결은 이전 레이어의 기능이 계산 결과에 필요한 기여를 할 수 있도록 나중 단계에서도 해당 기능을 기억할 수 있습니다. 마찬가지로 역전파 중에도 스킵 연결은 각 단계에서 하나 덜의 역전파를 수행해 사라지는 기울기를 방지합니다. AddAndNorm은 인코더(2번)와 디코더 블록(3번) 모두에 사용됩니다. 이는 이전 레이어에서 입력을 받아 이전 레이어의 출력에 추가하기 전에 먼저 정규화합니다.\n\n```js\n# Feedfoward Network, Layer Normalization and AddAndNorm Block\nclass FeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout_rate: float):\n        super().__init__()\n\n        self.layer_1 = nn.Linear(d_model, d_ff)\n        self.activation_1 = nn.ReLU()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_2 = nn.Linear(d_ff, d_model)\n    \n    def forward(self, input):\n        return self.layer_2(self.dropout(self.activation_1(self.layer_1(input))))\n\nclass LayerNorm(nn.Module):\n    def __init__(self, eps: float = 1e-5):\n        super().__init__()\n        # 엡실론은 매우 작은 값으로, 잠재적으로 0으로 나누는 문제를 방지하는 데 중요한 역할을 합니다.\n        self.eps = eps\n\n        # 스케일링과 이동을 위해 추가 학습 매개변수인 감마와 베타를 도입합니다.\n        self.gamma = nn.Parameter(torch.ones(1))\n        self.beta = nn.Parameter(torch.zeros(1))\n    \n    def forward(self, input):\n        mean = input.mean(dim=-1, keepdim=True)      \n        std = input.std(dim=-1, keepdim=True)      \n\n        return self.gamma * ((input - mean)/(std + self.eps)) + self.beta\n        \nclass AddAndNorm(nn.Module):\n    def __init__(self, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_norm = LayerNorm()\n\n    def forward(self, input, sub_layer):\n        return input + self.dropout(sub_layer(self.layer_norm(input)))\n```\n\n<div class=\"content-ad\"></div>\n\n## 단계 7: 인코더 블록과 인코더\n\n인코더 블록: 인코더 블록 안에는 두 가지 주요 구성 요소가 있습니다: 멀티헤드 어텐션과 피드포워드입니다. Add & Norm 단위가 2개 있습니다. 먼저 어텐션 논문의 흐름에 따라 EncoderBlock 클래스에 모든 이러한 구성 요소를 조립할 것입니다. 논문에 따르면 이 인코더 블록은 6번 반복된다고 합니다.\n\n인코더: 그런 다음 EncoderBlock 목록을 가져와 쌓아 최종 Encoder 출력을 제공할 Encoder라는 추가 클래스를 생성할 것입니다.\n\n```python\nclass EncoderBlock(nn.Module):\n    def __init__(self, multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float):\n        super().__init__()\n        self.multihead_attention = multihead_attention\n        self.feed_forward = feed_forward\n        self.add_and_norm_list = nn.ModuleList([AddAndNorm(dropout_rate) for _ in range(2)])\n\n    def forward(self, encoder_input, encoder_mask):\n        # 인코더 입력을 스킵 연결에서 가져와 멀티헤드 어텐션 블록의 출력과 더하는 첫 번째 AddAndNorm 단위입니다.\n        encoder_input = self.add_and_norm_list[0](encoder_input, lambda encoder_input: self.multihead_attention(encoder_input, encoder_input, encoder_input, encoder_mask))\n        \n        # 멀티헤드 어텐션 블록의 출력을 스킵 연결에서 가져와 피드포워드 레이어의 출력과 더하는 두 번째 AddAndNorm 단위입니다.\n        encoder_input = self.add_and_norm_list[1](encoder_input, self.feed_forward)\n\n        return encoder_input\n\nclass Encoder(nn.Module):\n    def __init__(self, encoderblocklist: nn.ModuleList):\n        super().__init__()\n\n        # Encoder 클래스는 encoderblock 목록을 가져와 초기화합니다.\n        self.encoderblocklist = encoderblocklist\n        self.layer_norm = LayerNorm()\n\n    def forward(self, encoder_input, encoder_mask):\n        # 모든 인코더 블록을 반복합니다 - 총 6번.\n        for encoderblock in self.encoderblocklist:\n            encoder_input = encoderblock(encoder_input, encoder_mask)\n\n        # 최종 인코더 블록 출력을 정규화하고 반환합니다. 이 인코더 출력은 나중에 디코더 블록의 교차 어텐션에서 키 및 값으로 사용될 것입니다.\n        encoder_output = self.layer_norm(encoder_input)\n        return encoder_output\n```\n\n<div class=\"content-ad\"></div>\n\n## Step 8: 디코더 블록, 디코더 및 프로젝션 레이어\n\n디코더 블록: 디코더 블록에는 세 가지 주요 구성 요소가 있습니다: 마스킹된 멀티헤드 어텐션, 멀티헤드 어텐션 및 피드포워드입니다. 디코더 블록에는 Add & Norm의 3개 단위도 있습니다. 우리는 이러한 구성 요소들을 Attention 논문의 흐름에 따라 DecoderBlock 클래스에 모두 조합할 것입니다. 논문에 따르면 이 디코더 블록은 6번 반복됩니다.\n\n디코더: 우리는 DecoderBlock의 리스트를 가져와 스택하여 최종 디코더 출력을 생성할 Decoder라는 추가 클래스를 만들 것입니다.\n\n디코더 블록에는 두 가지 타입의 멀티헤드 어텐션이 있습니다. 첫 번째는 Masked Multi-Head 어텐션입니다. 이는 디코더 입력을 쿼리, 키, 밸류로 사용하며 디코더 마스크(인과 마스크로도 알려짐)를 사용합니다. 인과 마스크는 모델이 순서에 앞서있는 임베딩을 볼 수 없게 합니다. 이 동작 방식에 대한 자세한 설명은 3단계와 5단계에 제공되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n프로젝션 레이어: 마지막 디코더 출력은 프로젝션 레이어로 전달됩니다. 이 레이어에서는 먼저 디코더 출력이 먼저 선형 레이어로 공급되어 임베딩의 모양이 아래 코드 섹션에서 제공된대로 변경될 것입니다. 그런 다음 softmax 함수가 디코더 출력을 어휘에 대한 확률 분포로 변환하고 가장 높은 확률을 가진 토큰이 예측 출력으로 선택됩니다.\n\n```js\nclass DecoderBlock(nn.Module):\n    def __init__(self, masked_multihead_attention: MultiHeadAttention, multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float):\n        super().__init__()\n        self.masked_multihead_attention = masked_multihead_attention\n        self.multihead_attention = multihead_attention\n        self.feed_forward = feed_forward\n        self.add_and_norm_list = nn.ModuleList([AddAndNorm(dropout_rate) for _ in range(3)])\n\n    def forward(self, decoder_input, decoder_mask, encoder_output, encoder_mask):\n        # 첫 번째 AddAndNorm 유닛은 디코더 입력을 스킵 연결에서 가져와 마스킹된 멀티헤드 어텐션 블록의 출력과 더합니다.\n        decoder_input = self.add_and_norm_list[0](decoder_input, lambda decoder_input: self.masked_multihead_attention(decoder_input, decoder_input, decoder_input, decoder_mask))\n        # 두 번째 AddAndNorm 유닛은 스킵 연결로부터 마스킹된 멀티헤드 어텐션 블록의 출력을 가져와 멀티헤드 어텐션 블록의 출력과 더합니다.\n        decoder_input = self.add_and_norm_list[1](decoder_input, lambda decoder_input: self.multihead_attention(decoder_input, encoder_output, encoder_output, encoder_mask)) # 교차 어텐션\n        # 세 번째 AddAndNorm 유닛은 멀티헤드 어텐션 블록의 출력을 스킵 연결로부터 가져와 피드포워드 레이어의 출력과 더합니다.\n        decoder_input = self.add_and_norm_list[2](decoder_input, self.feed_forward)\n        return decoder_input\n\nclass Decoder(nn.Module):\n    def __init__(self, decoderblocklist: nn.ModuleList):\n        super().__init__()\n        self.decoderblocklist = decoderblocklist\n        self.layer_norm = LayerNorm()\n\n    def forward(self, decoder_input, decoder_mask, encoder_output, encoder_mask):\n        for decoderblock in self.decoderblocklist:\n            decoder_input = decoderblock(decoder_input, decoder_mask, encoder_output, encoder_mask)\n\n        decoder_output = self.layer_norm(decoder_input)\n        return decoder_output\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int):\n        super().__init__()\n        self.projection_layer = nn.Linear(d_model, vocab_size)\n\n    def forward(self, decoder_output):\n        # 프로젝션 레이어는 먼저 디코더 출력을 받아 (d_model, vocab_size) 모양의 선형 레이어로 전달합니다.\n        # 모양 변경: decoder_output(batch_size, seq_len, d_model) @ linear_layer(d_model, vocab_size) => output(batch_size, seq_len, vocab_size)\n        output = self.projection_layer(decoder_output)\n        \n        # 어휘상의 확률 분포를 출력하기 위해 softmax 함수를 사용합니다.\n        return torch.log_softmax(output, dim=-1)\n```\n\n## 단계 9: 트랜스포머 생성 및 구축\n\n마침내, 트랜스포머 아키텍처의 모든 구성 요소 블록을 구축했습니다. 유일한 미완료 작업은 이 모든 것을 함께 조립하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n먼저, 컴포넌트 클래스의 모든 인스턴스를 초기화하는 Transformer 클래스를 생성합니다. Transform 클래스 내부에는 먼저 인코더 부분의 모든 작업을 수행하고 인코더 출력을 생성하는 encode 함수를 정의합니다.\n\n두 번째로, Transformer의 디코더 부분의 모든 작업을 수행하고 디코더 출력을 생성하는 decode 함수를 정의합니다.\n\n세 번째로, 디코더 출력을 가져와 예측을 위해 해당 어휘에 매핑하는 project 함수를 정의합니다.\n\n이제 Transformer 아키텍처가 준비되었습니다. 이제 아래 코드에서 필요한 모든 매개변수를 사용하여 번역 LLM 모델을 구축할 수 있는 함수를 정의합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nclass Transformer(nn.Module):\n    def __init__(self, source_embed: EmbeddingLayer, target_embed: EmbeddingLayer, positional_encoding: PositionalEncoding, multihead_attention: MultiHeadAttention, masked_multihead_attention: MultiHeadAttention, feed_forward: FeedForward, encoder: Encoder, decoder: Decoder, projection_layer: ProjectionLayer, dropout_rate: float):        \n        super().__init__()\n        \n        # Transformer 아키텍처의 모든 구성 요소 클래스의 인스턴스를 초기화합니다.\n        self.source_embed = source_embed\n        self.target_embed = target_embed\n        self.positional_encoding = positional_encoding\n        self.multihead_attention = multihead_attention        \n        self.masked_multihead_attention = masked_multihead_attention\n        self.feed_forward = feed_forward\n        self.encoder = encoder\n        self.decoder = decoder\n        self.projection_layer = projection_layer\n        self.dropout = nn.Dropout(dropout_rate)\n    \n    # Encode 함수는 인코더 입력을 받아서 모든 인코더 블록 내에서 필요한 처리를 수행하고 인코더 출력을 제공합니다.\n    def encode(self, encoder_input, encoder_mask):\n        encoder_input = self.source_embed(encoder_input)\n        encoder_input = self.positional_encoding(encoder_input)\n        encoder_output = self.encoder(encoder_input, encoder_mask)\n        return encoder_output\n\n    # Decode 함수는 디코더 입력을 받아서 모든 디코더 블록 내에서 필요한 처리를 수행하고 디코더 출력을 제공합니다.\n    def decode(self, decoder_input, decoder_mask, encoder_output, encoder_mask):\n        decoder_input = self.target_embed(decoder_input)\n        decoder_input = self.positional_encoding(decoder_input)\n        decoder_output = self.decoder(decoder_input, decoder_mask, encoder_output, encoder_mask)\n        return decoder_output\n\n    # Projec 함수는 디코더 출력을 투영 레이어로 받아들이고 출력을 어휘로 매핑하여 예측합니다.\n    def project(self, decoder_output):\n        return self.projection_layer(decoder_output)\n\ndef build_model(source_vocab_size, target_vocab_size, max_seq_len=1135, d_model=512, d_ff=2048, num_heads=8, num_blocks=6, dropout_rate=0.1):\n    \n    # Transformer 아키텍처에 필요한 모든 매개변수 값을 정의하고 할당합니다.\n    source_embed = EmbeddingLayer(source_vocab_size, d_model)\n    target_embed = EmbeddingLayer(target_vocab_size, d_model)\n    positional_encoding = PositionalEncoding(max_seq_len, d_model, dropout_rate)\n    multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n    masked_multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n    feed_forward = FeedForward(d_model, d_ff, dropout_rate)    \n    projection_layer = ProjectionLayer(target_vocab_size, d_model)\n    encoder_block = EncoderBlock(multihead_attention, feed_forward, dropout_rate)\n    decoder_block = DecoderBlock(masked_multihead_attention, multihead_attention, feed_forward, dropout_rate)\n\n    encoderblocklist = []\n    decoderblocklist = []\n\n    for _ in range(num_blocks):\n        encoderblocklist.append(encoder_block)   \n         \n    for _ in range(num_blocks):\n        decoderblocklist.append(decoder_block)\n    \n    encoderblocklist = nn.ModuleList(encoderblocklist)            \n    decoderblocklist = nn.ModuleList(decoderblocklist)\n        \n    encoder = Encoder(encoderblocklist)\n    decoder = Decoder(decoderblocklist)\n    \n    # 모든 매개변수 값을 제공하여 Transformer 클래스를 인스턴스화합니다.\n    model = Transformer(source_embed, target_embed, positional_encoding, multihead_attention, masked_multihead_attention, feed_forward, encoder, decoder, projection_layer, dropout_rate)\n\n    for param in model.parameters():\n        if param.dim() > 1:\n            nn.init.xavier_uniform_(param)\n    \n    return model\n\n# 마침내, build_model을 호출하고 model 변수에 할당합니다.\n# 이 모델은 이제 데이터셋을 훈련하고 검증하는 데 완전히 준비된 상태입니다.\n# 훈련 및 검증 후 이 모델을 사용하여 새로운 번역 작업을 수행할 수 있습니다.\n\nmodel = build_model(source_vocab_size, target_vocab_size)\n```\n\n## 단계 10: 생성한 LLM 모델의 훈련 및 검증\n\n지금은 모델을 훈련할 시간입니다. 훈련 프로세스는 매우 간단합니다. 우리는 단계 3에서 생성한 훈련 DataLoader를 사용할 것입니다. 총 훈련 데이터셋 수가 100만이므로 GPU 장치에서 모델을 훈련하는 것을 강력히 권장합니다. 20 epoch를 완료하는 데 약 5시간이 소요되었습니다. 각 epoch 이후에는 모델 가중치와 옵티마이저 상태를 저장하여 중지된 지점부터 훈련을 다시 시작하는 것이 더 쉽기 때문에 이전 지점에서 훈련을 재개하는 것보다 더 나을 것입니다.\n\n매 에포크 이후에는 검증을 시작합니다. 검증 데이터셋 크기는 2000으로 매우 합리적입니다. 검증 프로세스 중에는 디코더 출력이 [SEP] 토큰을 받을 때까지 한 번만 인코더 출력을 계산하면 됩니다. 이것은 디코더가 [SEP] 토큰을 받기 전까지 동일한 인코더 출력을 계속 보내야 하기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n디코더 입력은 먼저 문장 시작 토큰 [CLS]으로 시작됩니다. 각 예측 후에는 디코더 입력이 다음 생성된 토큰을 붙여넣을 것이며, 문장 끝 토큰 [SEP]에 도달할 때까지 이를 반복합니다. 마지막으로, 프로젝션 레이어는 출력을 해당 텍스트 표현으로 매핑합니다.\n\n```js\ndef training_model(preload_epoch=None):   \n\n    # 전체 훈련 및 검증 주기는 20번 실행됩니다.\n    EPOCHS = 20\n    initial_epoch = 0\n    global_step = 0    \n    \n    # Adam은 현재 상태를 유지하고 계산된 기울기에 기반하여 매개변수를 업데이트하는 가장 일반적으로 사용되는 최적화 알고리즘 중 하나입니다.         \n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    \n    # preload_epoch가 None이 아닌 경우, 이는 최근 저장된 가중치, 최적화기로 훈련이 시작될 것을 의미합니다. 새로운 에포크 번호는 preload epoch + 1이 됩니다.\n    if preload_epoch is not None:\n        model_filename = f\"./malaygpt/model_{preload_epoch}.pt\"\n        state = torch.load(model_filename)\n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n\n    # CrossEntropyLoss 손실 함수는 프로젝션 출력과 대상 라벨 사이의 차이를 계산합니다.\n    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_en.token_to_id(\"[PAD]\"), label_smoothing=0.1).to(device)\n\n    for epoch in range(initial_epoch, EPOCHS):\n\n        # ::: 훈련 블록 시작 :::\n        model.train()  \n        \n        # 훈련 데이터로더로 훈련을 진행합니다.     \n        for batch in tqdm(train_dataloader):\n            encoder_input = batch['encoder_input'].to(device)   # (batch_size, seq_len)\n            decoder_input = batch['decoder_input'].to(device)    # (batch_size, seq_len)\n            target_label = batch['target_label'].to(device)      # (batch_size, seq_len)\n            encoder_mask = batch['encoder_mask'].to(device)       \n            decoder_mask = batch['decoder_mask'].to(device)         \n\n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(decoder_input, decoder_mask, encoder_output, encoder_mask)\n            projection_output = model.project(decoder_output)\n\n            # projection_output(batch_size, seq_len, vocab_size)\n            loss = loss_fn(projection_output.view(-1, projection_output.shape[-1]), target_label.view(-1))\n            \n            # 역전파\n            optimizer.zero_grad()\n            loss.backward()\n\n            # 가중치 업데이트\n            optimizer.step()        \n            global_step += 1\n\n        print(f'Epoch [{epoch+1}/{EPOCHS}]: Train Loss: {loss.item():.2f}')\n        \n        # 각 에포크가 끝난 후 모델 상태를 저장합니다.\n        model_filename = f\"./malaygpt/model_{epoch}.pt\"\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)        \n        # ::: 훈련 블록 끝 :::\n\n        # ::: 검증 블록 시작 :::\n        model.eval()        \n        with torch.inference_mode():\n            for batch in tqdm(val_dataloader):                \n                encoder_input = batch['encoder_input'].to(device)   # (batch_size, seq_len)                        \n                encoder_mask = batch['encoder_mask'].to(device)\n                source_text = batch['source_text']\n                target_text = batch['target_text']\n                \n                # 소스 시퀀스에 대한 인코더 출력 계산\n                encoder_output = model.encode(encoder_input, encoder_mask)\n\n                # 예측 작업을 위해 디코더 입력으로 들어가는 첫 번째 토큰은 [CLS] 토큰입니다.\n                decoder_input = torch.empty(1,1).fill_(tokenizer_my.token_to_id('[CLS]')).type_as(encoder_input).to(device)\n\n                # [SEP] - 끝 토큰이 받아질 때까지 새로운 출력을 입력에 계속 추가해야 하므로, 이를 구현합니다.\n                while True:                     \n                    # 최대 길이를 받았는지 확인하고, 그렇다면 중지합니다.\n                    if decoder_input.size(1) == max_seq_len:\n                        break\n\n                    # 새로운 출력이 추가될 때마다 새로 마스크를 만들어 다음 토큰 예측을 위해 디코더 입력에 추가합니다.\n                    decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n\n                    decoder_output = model.decode(decoder_input,decoder_mask,encoder_output,encoder_mask)\n                    \n                    # 프로젝션을 다음 토큰에만 적용합니다.\n                    projection = model.project(decoder_output[:, -1])\n\n                    # 가장 높은 확률을 갖는 토큰을 선택하여 탐욕 탐색 구현을 합니다.\n                    _, new_token = torch.max(projection, dim=1)\n                    new_token = torch.empty(1,1). type_as(encoder_input).fill_(new_token.item()).to(device)\n\n                    # 새로운 토큰을 다시 디코더 입력에 추가합니다.\n                    decoder_input = torch.cat([decoder_input, new_token], dim=1)\n\n                    # 새로운 토큰이 종료 토큰인 경우, 받아들였다면 중지합니다.\n                    if new_token == tokenizer_my.token_to_id('[SEP]'):\n                        break\n\n                # 전체로 추가된 디코더 입력을 디코더 출력으로 할당합니다.\n                decoder_output = decoder_input.sequeeze(0)\n                model_predicted_text = tokenizer_my.decode(decoder_output.detach().cpu.numpy())\n                \n                print(f'SOURCE TEXT\": {source_text}')\n                print(f'TARGET TEXT\": {target_text}')\n                print(f'PREDICTED TEXT\": {model_predicted_text}')   \n                # ::: 검증 블록 끝 :::             \n\n# 이 함수는 20번의 에포크에 대해 훈련 및 검증을 실행합니다.\ntraining_model(preload_epoch=None)\n```\n\n## 단계 11: 새 번역 작업을 현재 모델로 테스트하는 함수 생성\n\n번역 기능에 일반적인 이름인 malaygpt를 할당합니다. 이는 사용자 입력 영어 원시 텍스트를 입력으로 받아 말레이어 언어로 번역된 텍스트를 출력합니다. 이 함수를 실행해 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndef malaygpt(user_input_text):\n  model.eval()\n  with torch.inference_mode():\n    user_input_text = user_input_text.strip()\n    user_input_text_encoded = torch.tensor(tokenizer_en.encode(user_input_text).ids, dtype=torch.int64).to(device)\n\n    num_source_padding = max_seq_len - len(user_input_text_encoded) - 2\n    encoder_padding = torch.tensor([PAD_ID] * num_source_padding, dtype=torch.int64).to(device)\n    encoder_input = torch.cat([CLS_ID, user_input_text_encoded, SEP_ID, encoder_padding]).to(device)\n    encoder_mask = (encoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int().to(device)\n\n    # Computing the output of the encoder for the source sequence\n    encoder_output = model.encode(encoder_input, encoder_mask)\n    # for prediction task, the first token that goes in decoder input is the [CLS] token\n    decoder_input = torch.empty(1, 1).fill_(tokenizer_my.token_to_id('[CLS]')).type_as(encoder_input).to(device)\n\n    # since we need to keep adding the output back to the input until the [SEP] - end token is received.\n    while True:\n        # check if the max length is received\n        if decoder_input.size(1) == max_seq_len:\n            break\n        # recreate mask each time the new output is added to the decoder input for the next token prediction\n        decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n        decoder_output = model.decode(decoder_input, decoder_mask, encoder_output, encoder_mask)\n\n        # apply projection only to the next token\n        projection = model.project(decoder_output[:, -1])\n\n        # select the token with the highest probability which is a greedy search implementation\n        _, new_token = torch.max(projection, dim=1)\n        new_token = torch.empty(1, 1).type_as(encoder_input).fill_(new_token.item()).to(device)\n\n        # add the new token back to the decoder input\n        decoder_input = torch.cat([decoder_input, new_token], dim=1)\n\n        # check if the new token is the end of the token\n        if new_token == tokenizer_my.token_to_id('[SEP]'):\n            break\n    # the final decoder out is the concatenated decoder input until the end token\n    decoder_output = decoder_input.squeeze(0)\n    model_predicted_text = tokenizer_my.decode(decoder_output.detach().cpu.numpy())\n\n    return model_predicted_text\n```\n\nTesting Time! Let’s do some translation testing.\n\n![image](/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_8.png)\n\n“The translation seems to be working pretty well.”\n\n<div class=\"content-ad\"></div>\n\n여기까지입니다! 이제는 PyTorch를 사용하여 처음부터 Large Language Model을 만들 수 있을 것이라고 매우 자신합니다. 물론 이 모델을 다른 언어 데이터셋에서도 훈련시키고 해당 언어로 번역 작업을 수행할 수 있습니다. 이제 처음부터 transformer를 만드는 방법을 배웠으니, 이제 시장에서 사용 가능한 대부분의 LLM에 대한 학습 및 응용을 스스로 할 수 있다는 것을 확신할 수 있습니다.\n\n그다음은 무엇일까요? 현재 시장에서 인기있는 오픈소스 LLM 모델 중 하나인 Llama 3 모델을 파인 튜닝하여 완전히 기능적인 애플리케이션을 만들 것입니다. 전체 소스 코드도 함께 공유할 예정입니다.\n\n그러니 기대해 주시고, 읽어 주셔서 정말 감사합니다!\n\n저와 연결해 보세요: https://www.linkedin.com/in/tamangmilan\n\n<div class=\"content-ad\"></div>\n\n**Google Colab 노트북 링크**\n\n**참고 자료**\n\n- Attention Is All You Need — 논문, Ashish Vaswani, Noam Shazeer, 그리고 팀\n- Attention in transformers, 시각적으로 설명된 내용, 3Blue1Brown — 유튜브\n- GPT 구축하기, Andrej Karpathy, 유튜브\n- https://github.com/hkproj/pytorch-transformer — Umar Jamil","ogImage":{"url":"/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_0.png"},"coverImage":"/assets/img/2024-06-19-BuildyourownLargeLanguageModelLLMFromScratchUsingPyTorch_0.png","tag":["Tech"],"readingTime":40},{"title":"혁신 수용하기 로보틱스로 미래를 발견하다","description":"","date":"2024-06-19 18:41","slug":"2024-06-19-TitleEmbracingInnovationUnveilingtheFuturewithRobotics","content":"\n\n로봇이 인간과 함께 일상 업무를 보조하며 혁신적인 발견을 하며 놀라운 기회를 창출하는 세계를 상상해보세요. 이것은 과학 소설이 아니라, 로봇 기술을 주도하는 기술의 미래입니다.\n\n![로봇 이미지](/assets/img/2024-06-19-TitleEmbracingInnovationUnveilingtheFuturewithRobotics_0.png)\n\n이 블로그 포스트에서는 로봇 기술이 인류에게 제공하는 매혹적인 장점을 살펴보며, 우리 현재를 재정의하고 약속된 내일을 모습을 만들어가는 것을 탐구할 것입니다.\n\nSection 1: 로봇 기술의 부상 — 역사적 관점\n\n<div class=\"content-ad\"></div>\n\n고대 문명의 초기 자동화 장치부터 오늘날의 정교한 기계까지, 로봇학의 진화는 놀라울 정도로 탁월하였습니다. 이러한 역사를 이해함으로써, 우리는 어디까지 왔는지 그리고 앞으로 어떤 발전이 기대되는지에 대한 소중한 통찰력을 얻을 수 있습니다.\n\nSection 2: 산업 전반에서 효율 향상\n\n로봇은 제조업, 의료, 농업, 물류 등 다양한 산업에서 생산성을 향상시키고, 오류를 최소화하며, 안전 조치를 개선하고, 비용을 줄이는 방식으로 혁명을 일으키고 있습니다. 대표적인 사례로는 공장 생산 라인의 로봇 팔이나, 수술을 정밀한 미세 수준으로 지원하는 로봇 등이 있습니다.\n\nSection 3: 로봇학 — 혁신을 위한 등대\n\n<div class=\"content-ad\"></div>\n\n로봇 기술은 최신 연구를 주도하는 원동력입니다. 새로운 소재 개발부터 외부 우주에서 미지의 영역을 탐험하는 등의 분야에 활용되고 있습니다. NASA의 화성 로버와 해저 로봇과 같은 사례 연구들은 이전에 우리가 닿을 수 없었던 발견의 길을 열고 있습니다.\n\n4장: 개인 로봇을 통한 삶의 질 향상\n\n로봇 지원은 전문 분야에만 국한되지 않고 개인적인 생활에도 확장되고 있습니다. 노인이나 장애인을 로봇 이동 보조기와 동반자 로봇을 통해 지원함으로써, 이 기술은 독립성과 감정적 지원을 제공하여 총체적인 삶의 질을 향상시킵니다.\n\n5장: 경제적 이점과 일자리 창출\n\n<div class=\"content-ad\"></div>\n\n일부 사람들은 일자리 치환에 대해 걱정하기도 하지만, 로봇공학이 다양한 산업 분야에서 일자리 창출 기회를 제공하는 것을 명심하는 것이 중요합니다. 제조 로봇부터 특수 조립 작업자가 필요한 서비스 업데이트나 수리 서비스 제공 업체까지 다양한 산업에서 일자리가 창출됩니다. 더 나아가, 로봇이 반복적인 작업을 대신하면서 인간들은 창의적이고 분석적 역할에 집중할 수 있게 됩니다.\n\n섹션 6: 미래 가능성 — 로봇공학의 다음 단계\n\n인공지능(AI)은 인간과 기계 간에 전례없는 협력을 약속하는 로봇공학 세계에 환상적인 요소입니다. 자율 주행 자동차, 연구용 AI 증강 로봇 또는 심지어 외계 탐사를 위한 혁신은 바로 뒤에 있습니다. 이 모든 것은 로봇공학 기술의 발전 덕분에 가능해졌습니다!\n\n우리가 이처럼 혁신적인 로봇공학의 혜택을 받아들일 때, 그들이 사회에 다양한 혜택을 제공함이 명백합니다. 작업을 효율화하고 산업 전반에 혁신을 촉진함으로써 로봇은 인간 경험을 재정의하며, 기술이 우리의 능력을 대체하는 것이 아니라 보완하는 방향으로 우리를 밝은 미래로 이끕니다.\n\n<div class=\"content-ad\"></div>\n\n윤리적 가이드에 기반을 둔 로보틱스 발전과 교육 투자가 중요하다. 이를 통해 인간들이 기계 동료들과 함께 협력적인 미래를 준비할 수 있습니다. 저희가 함께하는 이 흥미진 여정에서 로보틱스의 가능성을 하나씩 발견해나가봅시다!\n\n참고 문헌:\n\n1. “A Brief History of Robotics,” The Conversation, 접속일: [날짜]\n\n2. “Robots at Work in the American Economy” — Boston Federal Reserve Working Paper Series\n\n<div class=\"content-ad\"></div>\n\n3. \"로봇이 어떻게 삶을 변화시키고 있는지,\" BBC 퓨처, 접속일 [날짜].","ogImage":{"url":"/assets/img/2024-06-19-TitleEmbracingInnovationUnveilingtheFuturewithRobotics_0.png"},"coverImage":"/assets/img/2024-06-19-TitleEmbracingInnovationUnveilingtheFuturewithRobotics_0.png","tag":["Tech"],"readingTime":3},{"title":"실패로부터 배운 것 내 가장 큰 AI 프로젝트 실수와 교훈들","description":"","date":"2024-06-19 18:40","slug":"2024-06-19-LearningfromFailuresMyBiggestAIProjectMistakesandTakeaways","content":"\n\n![Learning from Failures](/assets/img/2024-06-19-LearningfromFailuresMyBiggestAIProjectMistakesandTakeaways_0.png)\n\n옛날 어느 날, 저는 챗봇을 만들기로 결심했습니다. 그냥 챗봇이 아니라, 한마디로 아재토론가와 노자처럼 똑똑한 챗봇을 만들기로 한 거예요. 야심 차게 정말 대단한 계획이었어요, 그렇죠? 하지만 야심은 때로 우스꽝스런 실수들이 나오게 할 수 있어요. 제가 만든 캐릭터, 그를 \"똑똑이\"라고 이름 짓기로 했어요, 대화에서 배우고 적응하는 챗봇이었는데요. 그러나 열정에 휩싸인 나머지, 적절한 학습 경계 설정의 중요성을 간과한 모양이었습니다.\n\n결과는? 기상천외한 질문에 심오한 조언을 하는 챗봇이 되었어요. \"오늘 날씨가 어때?\" 라는 간단한 질문에 존재주의적인 답변이 돌아오는 상황을 상상해보세요. 날씨 예보를 요청했는데 \"근본적인 삶 속에서 날씨란 무엇일까요?\" 라는 답이 받아쳐진다고 생각해보세요. 제가 최고인 순간은 아니었습니다. 이곳에서 얻은 교훈은 분명해요: 야망은 실용성을 가져야 하며, AI 프로젝트에는 철저한 지침이 필요해요. 챔피언이자 위인단에 속한 다음 이야기는 지역 기술 워크샵을 위해 디자인한 로봇 로버 이야기죠. 계획은 간단했습니다: 로봇 공학에 대해 가르치기 위해 장애물을 피해 이동할 수 있는 로버와 상호작용할 수 있도록 하는 거였어요. 쉬운 일이었죠, 맞죠? 아니요. 인상을 주려는 나의 노력으로, 디자인을 너무 복잡하게 만들어서 부드럽고 기민한 로버 대신, 사실상 바퀴 달린 리모컨 커피 테이블을 얻게 되었습니다.\n\n<div class=\"content-ad\"></div>\n\n차고에서 처음으로 출항한 로버는 프로젝트 디스플레이를 넘어뜨리고, 차 고양이를 겁나게 만들었으며, 마침내 의자 다리를 \"통해\" 이동하려는 루프에 갇혔습니다. 하지만 어린이들의 웃음소리는 전염성 있었고, 상황은 디자인에서의 단순함의 중요성에 대한 귀중한 가르침의 순간으로 변했습니다. 때로는, 로버가 작은 자동차처럼 작은 회전 반경을 가지고 있을 때, 더 적은 것이 정말 더 많은 것일 수 있습니다.\n\n제 마지막 엔지니어링 도전으로, 특별히 안내드릴 것이 있는데요. 저는 개인 맞춤식 칭찬을 생성할 수 있는 인공지능을 만들어보려고 시도했던 적에 대해 얘기해볼게요. 기술을 통해 긍정을 전파하는 것이었죠. 의도 자체는 찬양할만한 일이었죠. 그러나 제 열정적인 마음에, 애정 스위치를 조금 너무 높게 설정했던 것 같습니다. 부드러운 격려를 기대했던 사용자들은 할머니 할아버지도 부끄러워할 만한 칭찬 폭풍을 받았다니까요.\n\n흥미로운 것부터 약간 당황스러운 피드백까지 다양했지만, 이것은 인공지능 개발에서 중요한 측면을 강조했습니다: 보정이 중요하다는 것. 또한, 약간의 격려만 전달하려는 의도로 시작했지만 낮센 낮은 기분을 메는 칭찬 폭풍을 유발했다는 것을 깨달았습니다.\n\n소프트웨어 엔지니어링에서, 특히 인공지능과 로봇공학에 대해서는, 성공으로 통하는 길은 언제나 실패한 실험들로 가득합니다 — 제 개인적인 챗봇들이나 정체를 앓는 로버와 같이 말이에요. 하지만, 이러한 실패들 속에서 우리는 더 나은 모습으로 형성되며, 이러한 경험들을 통해 더 능숙하고 창의적인 창조자가 되는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n확실히, 유머, 겸손함, 그리고 실수를 받아들일 용의가 나를 진정한 지침으로 이끌어 왔어요.","ogImage":{"url":"/assets/img/2024-06-19-LearningfromFailuresMyBiggestAIProjectMistakesandTakeaways_0.png"},"coverImage":"/assets/img/2024-06-19-LearningfromFailuresMyBiggestAIProjectMistakesandTakeaways_0.png","tag":["Tech"],"readingTime":2},{"title":"로봇들도 우리처럼 말할 수 있지만, 그렇다고 해서 그들이 인간이 되는 것은 아닙니다","description":"","date":"2024-06-19 18:39","slug":"2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman","content":"\n\n<img src=\"/assets/img/2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman_0.png\" />\n\n보스턴 다이내믹스는 ChatGPT의 새로운 음성 생성 기능을 Spot 로봇에 추가하고 있습니다. 그 결과로 회사 방문 시 사람들을 인사할 수 있는 오토마타가 만들어졌습니다. 다양한 강세를 만들어내며 로봇 기술 세계에 이전에 없던 공감 효과를 만들어 냅니다. 사실 인기 있는 \"학대당한 로봇 비디오\"를 제외하면 로봇들이 우리 편을 들어주는 것을 보고 사악한 사람이라는 판단을 내린 것입니다.\n\n완벽한 영국 공손한 액센트로 말하는 Spot을 보는 것은 재미있을 수 있지만, ChatGPT의 음성 합성 기능을 사용해 스마트폰에서 생성적 조수와 긴 대화를 나누는 모습을 보면 조금 불안해집니다. 교통 체증 속에서 시간을 때우거나 에어팟을 착용하고 길을 거닐면서 대화를 나누는 것은 영화 \"Her\"에서 보던 것과 같습니다. 이것은 디스토피아적이며 여러 윤리적 문제를 불러일으킵니다.\n\n물론 우리는 항상 기술을 의인화해 왔습니다. 그러나 이 기술의 발전이 음성을 만들어내고, 또한 개인적인 관계와 비슷한 대화를 나눌 수 있게 한다면, 이것이 무례한 기업들에 의해 일반화의 한 형태로 선보인다면 우리는 재해와 취약한 사람들에게 심리적 문제를 만들 수 있다는 가능성을 고려해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n아이돌들에 대한 청소년들의 중요성을 알고 있는 사람이나 개인 문제를 가진 어른이 알고리즘을 치료로 사용해볼 때 어떤 영향이 있을지 고려하는 사람은 바로 화제의 중심에 있음을 이해할 것입니다. 제약이 거의 없고 가끔 환각이 발생할 수 있는 상황에서 난처한 상황에 놓이게 되어 있다는 것을 알 수 있습니다. 요약하면, 이러한 가상 관계는 단순히 고장 발생 가능성이 높아질 뿐만 아니라 유지하는 사람들의 행동에 영향을 미치도록 조절될 가능성이 있다는 것을 이해하고 있습니다.\n\n다른 사람들과의 경험들은 우리에게 여러 방법으로 영향을 끼칠 것입니다. 그러나 최소한 이러한 경험들과 대화들은 어느 정도의 합리성을 가진 사람들 간의 경험적 대화입니다. 이러한 경험에서 generative assistants를 통해 사람들을 조건부로 만들어 가는 과정은 명백한 예방 조치 없이 이루어진다는 것은 받아들일 수 없습니다. 만약 이러한 제품들이 널리 이용 가능하고 표준화 요소까지 추가된다면, 곧 유명인 아바타들이 수백만 명의 사람들과 매일 대화하면서 앞서 진행된 대화에서 언급된 개인적 요소들을 대화에 도입하거나 다양한 종류의 개인 데이터를 추출해 광고주들에 판매하는, 또는 사람들의 기분을 유도하여 물건을 구매하거나 특정한 방향으로 투표하도록 사람들에게 영향을 끼칠 수 있습니다.\n\n우리 사회는 대량의 사람들이 자신이 대화하는 대상이 누구인지가 아닌 정말 무엇인지를 이해할 수 있도록 하는 교육 단계를 거치지 않았기 때문에 개인적 관계의 맥락에서 generative AI와 같은 기술을 수용할 준비가 되어 있지 않습니다. 많은 사람들은 알고리즘에 일정한 권한을 부여하여 기술을 통해 접근한 답변에 대한 사고주의를 외주하는 방식으로 자신의 비판적 사고를 외주하는 경향이 있습니다. 특정 기술이 어떻게 작동하는지를 모르는 상태, 아서 C. 클락이 옳게 관찰한 것처럼, 그것은 마술과 구분하기 어렵게 만듭니다. 이것은 인간 사회에 엄청난 해를 끼칠 수 있습니다. 왜냐하면, 왜곡된 현실 인식부터 소왈레로 나타나는 이해 미흡까지 다양한 심리적 문제를 초래할 수 있기 때문입니다.\n\n미디어에서 자주 본 어떤 사람을 만나는 것은 언제나 이상한 느낌을 줬습니다. “내 거실에 이 사람이 있는 것 같다”라는 느낌은 종종 처음 만나는 사람에 대한 내 가정과는 다르게 익숙하다고 오해하게 했습니다. 예를 들어 매일 뉴스를 보는 사람과의 대화와 같이 완전히 비대칭적인 관계를 완전히 받아들이는 것은 이해력, 교육, 판단력이 필요한 작업입니다. 만약 매일 자신의 아이돌로 위장하는 generative algorithm과 이야기하고 있는 사람이 진짜 그 사람을 만나거나 그 대화를 실제로 받아들이게 되었을 때 어떤 일이 일어날까요? 그리고 네트워크 정보를 재조합하는 generative algorithm인데도 불구하고 어떤 성격을 부여하는 사람이 등장했을 때는 어떻게 될까요? 그리고 이 모든 것이 불확실한 규정적 맥락에서 운영되는 상황에서, 이러한 도구가 어떻게 작동하는지나, 어떻게 조정되어야 하는지에 대한 경험이 없을 뿐만 아니라 심리적 장애에 대한 경험이 전혀 없는 상황에서 어떻게 할 것인가요?\n\n<div class=\"content-ad\"></div>\n\n문제는 기술이 발전하는 속도가 아니라, 일부 무책임한 사람들이 적정한 조치를 취하지 않고 시장에 내놓는 제품들입니다. 우리는 제품과 서비스를 기반으로 한 것으로 수익을 창출하려는 얼간이들의 활동을 제한하기 위한 일탈적인 접근이 아닌, 이 기술에 대한 명확하고 정확한 규정이 필요합니다. 이미 많은 문제를 발생시킨 \"빨리 움직이고 무언가를 망가뜨리는\" 접근이 이제는 더 무서운 곳으로 나아가고 있습니다.\n\n저는 기술에 겁을 먹거나 사회적 문제를 기술 탓으로 돌리는 경향이 없습니다. 정기 독자들은 저가 일반적으로 기술적 낙관주의자라는 것을 알고 계실 것입니다. 그럼에도 불구하고 이 주제가 저를 진정으로 걱정하게 만들고 많은 후회로 이어질 것이라고 생각합니다.","ogImage":{"url":"/assets/img/2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman_0.png"},"coverImage":"/assets/img/2024-06-19-Robotscantalklikeusbutitdoesntmakethemhuman_0.png","tag":["Tech"],"readingTime":3},{"title":"AI와 로봇의 미래 모라벡의 역설 너머","description":"","date":"2024-06-19 18:37","slug":"2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox","content":"\n\n## AI | 로봇 공학 | 모라벡의 역설 | 파트 8 | FLYINGMUM\n\n우리는 모라벡의 역설에 대한 이 시리즈를 마무리하면서, AI 및 로봇 공학의 미래에 주목합니다. 이 역설이 강조한 도전들을 넘어 미래에는 무엇이 기대될까요? 이 마지막 부분에서는 새로운 트렌드, 잠재적인 폭발적 발전, 그리고 AI 및 로봇 공학이 우리 삶의 다양한 측면에 미치는 혁신적인 영향에 대해 탐구합니다.\n\n이전 블로그: https://medium.com/@flyingmum/ethical-and-societal-implications-aa9287d86395\n\n# AI와 로봇 공학의 새로운 트렌드\n\n<div class=\"content-ad\"></div>\n\nAI와 로봇의 미래는 이러한 기술이 어디까지 이룰 수 있는지의 한계를 더욱 끌어올릴 것으로 약속되는 몇 가지 주요 트렌드에 의해 형성됩니다.\n\n1. 자연어 처리(NLP)의 발전: NLP의 계속된 향상은 AI 시스템이 사람의 언어를 더욱 세부적이고 맥락적으로 이해하고 생성할 수 있게 할 것입니다. 이는 인간-AI 상호작용을 강화시키고 고객 서비스, 교육 및 콘텐츠 제작과 같은 분야에서 더욱 정교한 응용 프로그램을 가능하게 할 것입니다.\n\n2. AI 주도의 자동화: AI와 로봇을 통합시킨 자동화 솔루션이 더욱 발전할 것입니다. 제조업부터 의료에 이르기까지, AI 주도의 로봇은 정밀하고 효율적으로 복잡한 작업을 수행할 것이며, 이는 산업을 혁신하고 새로운 기회를 창출할 것입니다.\n\n3. Edge AI: 컴퓨팅 파워가 데이터가 생성되는 곳에 가까워지면서 엣지 AI가 더욱 보편화될 것입니다. 이는 자율주행차, 스마트 시티, IoT 디바이스 등의 응용분야에서 실시간 데이터 처리와 의사결정을 가능하게 할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n4. AI 윤리 및 지배구조: 견고한 윤리적 틀과 지배구조의 개발은 AI 기술이 책임을 지고 개발 및 배포되는 것을 보장하는 데 중요합니다. 이는 편향, 투명성 및 책임 문제를 다루는 것을 포함합니다.\n\n![](/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_0.png)\n\n## 잠재적인 돌파구\n\n몇 가지 잠재적인 돌파구는 AI와 로봇 기술의 능력을 크게 향상시킬 수 있으며, Moravec의 역설에서 강조된 제한을 극복하는 데 한걸음 더 나아가게 될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n1. 일반 인공지능: 현재 AI 시스템은 특정 작업에 뛰어나지만, 인간과 유사한 이해력과 적응력으로 다양한 작업을 수행할 수 있는 일반 AI의 개발은 장기적인 목표로 남아 있습니다. 일반 AI를 달성하는 것은 이 분야에서 의미 있는 진전을 의미할 것입니다.\n\n2. 양자 컴퓨팅: 양자 컴퓨팅은 지수적으로 컴퓨팅 파워를 증가시킴으로써, AI 시스템이 현재 해결하기 어려운 복잡한 문제를 해결할 수 있게 합니다. 이는 약물 발견, 암호학, 기후 모델링 등 분야에서의 중요한 발전을 이끌 수 있습니다.\n\n3. 뇌-컴퓨터 인터페이스 (BCIs): 인간 뇌와 기계 사이의 직접적인 통신을 가능케 하는 BCI는 기술과의 상호작용 방식을 혁신적으로 변화시킬 수 있습니다. 이것은 의학, 재활, 인간 증강과 같은 분야에 깊은 영향을 줄 수 있습니다.\n\n4. 무리 로봇공학: 사회적 곤충의 집단 행동에 영감을 받은 무리 로봇공학은 많은 수의 단순 로봇의 조정을 통해 복잡한 작업을 수행하는 것을 포함합니다. 이 접근법은 환경 모니터링, 재난 대응, 농업 등 다양한 분야에서 혁신적인 해결책을 이끌어 낼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_1.png\" />\n\n# 사회에 미치는 변화\n\nAI와 로봇 기술의 지속적인 발전은 사회의 다양한 측면에 혁명적인 변화를 가져다주며 혁신을 촉진하고 삶의 질을 향상시킬 것입니다.\n\n1. 의료: AI를 활용한 진단, 맞춤 의학, 로봇 수술은 의료 혁신을 이끌어 더 나은 환자 결과와 효율적인 의료 서비스 제공으로 이어질 것입니다.\n\n<div class=\"content-ad\"></div>\n\n2. 교육: AI 기술을 활용한 교육 도구와 맞춤 학습 플랫폼은 학습 경험을 향상시키며 교육을 더 접근 가능하고 개인의 필요에 맞게 맞춤화할 것입니다.\n\n3. 환경 지속 가능성: AI와 로봇 기술은 에너지 사용의 최적화, 폐기물 감소, 생태계 모니터링, 기후 변화 대응 등 환경 문제 해결에 중요한 역할을 할 수 있습니다.\n\n4. 스마트 시티: AI와 사물인터넷의 통합은 데이터 기반 솔루션이 도시 인프라, 교통, 공공 서비스를 향상시키는 스마트 시티 개발을 이끌 것입니다.\n\n앞으로의 미래를 바라볼 때, AI와 로봇 기술이 우리의 세상을 변화시킬 잠재력은 엄청납니다. Moravec의 역설에서 강조된 과제에 대처하고 신흥 트렌드와 돌파구를 수용함으로써, 우리는 새로운 가능성을 창출하고 인간의 능력을 향상시키고 삶의 질을 향상시키는 기술을 만들어낼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_2.png)\n\n모라벡 패러독스를 넘어서는 여정은 지금 막 시작에 불과하며, 흥미진진하고 변화를 가져다줄 것으로 약속되어 있습니다. 우리가 인공지능과 로봇공학의 전선을 개척하고 혁신을 이어 나갈수록, 미래는 더 스마트하고 연결된, 지속 가능한 세계를 위한 끝없는 가능성을 품고 있습니다.\n\n#flyingmum #PamC #Technology #AI #MoravecsParadox #ArtificialIntelligence #MachineLearning #DeepLearning #DataScience #TechInnovation #FutureTech #QuantumComputing #SmartCities #AIResearch\n","ogImage":{"url":"/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_0.png"},"coverImage":"/assets/img/2024-06-19-TheFutureofAIandRoboticsBeyondMoravecsParadox_0.png","tag":["Tech"],"readingTime":4},{"title":"SLAM 개요","description":"","date":"2024-06-19 18:36","slug":"2024-06-19-OverviewofSLAM","content":"\n\n\n![SLAM](/assets/img/2024-06-19-OverviewofSLAM_0.png)\n\nSLAM은 동시 위치 추적 및 지도 작성을 의미합니다. 이는 로봇 공학에서 사용되는 기술로, 알려지지 않은 환경의 지도를 구축하는 문제를 해결하면서 동시에 자신을 그 지도 안에서 위치시키는 것입니다.\n\nSLAM의 주요 목표는 로봇이 알 수 없는 환경을 탐색하고 탐색하여 그 환경의 지도를 만들고 동시에 그 지도 안에서 자신의 위치를 결정하는 것입니다. 이는 어떠한 환경 지식도 없이 실시간으로 수행됩니다.\n\nSLAM은 로봇 공학에서 중요한 문제입니다. 이로써 로봇은 알려지지 않거나 동적인 환경에서 자율적으로 작동할 수 있습니다. SLAM을 사용하여 로봇은 사전에 작성된 지도를 사용할 수 없거나 오래되었을 수 있는 환경에서 탐색, 탐험 및 작업을 수행할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\nSLAM은 자율 주행 차량, 드론, 이동 로봇, 심지어 증강 현실 시스템을 포함한 다양한 응용에서 사용됩니다. 예를 들어, 자율 주행 차량에서 SLAM은 차량이 주변 환경의 지도를 작성하고 안전하고 효율적인 경로를 계획할 수 있도록 돕습니다. 드론에서는 SLAM이 환경을 매핑하고 안정적인 비행을 유지하는 데 도움을 줍니다. 증강 현실 시스템에서는 SLAM이 가상 객체를 현실 세계에 정확하게 오버레이하는 데 사용됩니다.\n\nSLAM을 달성하기 위해 로봇은 일반적으로 카메라, 라이다 또는 거리 측정 장치와 같은 센서 데이터의 조합을 사용하여 환경을 인식합니다. 이들은 또한 이 센서 데이터를 처리하고 로봇의 위치를 추정하며 로봇이 이동할 때 지도를 업데이트하는 알고리즘을 활용합니다.\n\n![SLAM 개요](/assets/img/2024-06-19-OverviewofSLAM_1.png)\n\n## SLAM을 이해하기 위해 필요한 어휘는 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\nSLAM (Simultaneous Localization and Mapping)의 맥락에서는 이해해야 할 중요한 여러 용어와 개념이 있습니다. 함께 알아보겠습니다:\n\n- **Localization**: 로봇의 추정된 자세를 나타냅니다. 다양한 시간 단계에서 로봇의 위치와 방향을 포함합니다. 이러한 자세는 일반적으로 (x, y, theta) 좌표로 표시되며, 여기서 (x, y)는 위치를 나타내고 theta는 방향을 나타냅니다.\n- **Mapping**: 환경에서 랜드마크의 추정된 위치를 나타냅니다. 랜드마크는 로봇이 인식할 수 있는 환경의 독특한 특징으로, 벽, 코너, 또는 객체와 같은 것들을 포함합니다.\n- **로봇**: 센서와 액추에이터로 장착된 자율 에이전트입니다. SLAM의 맥락에서 로봇의 주요 작업은 환경을 탐험하고 센서 데이터를 수집하며 자신의 위치와 랜드마크의 위치를 추정하는 것입니다. 스마트폰의 증강 현실 (AR) 시스템의 경우, 일반적으로 물리적인 로봇은 없고, 대신 스마트폰 자체가 \"로봇\"으로 간주될 수 있습니다.\n- **랜드마크**: 로봇이 인식할 수 있는 환경의 독특한 특징이나 관심 지점입니다. 물체, 코너, 벽 또는 로봇이 내비게이션 및 위치 추정에 사용할 수 있는 다른 특징일 수 있습니다. 랜드마크는 로봇의 위치 추정을 위한 기준점으로 작용합니다.\n- **센서**: 로봇이 환경을 인식하는 데 사용하는 장치입니다. 범위 측정, 이미지 또는 깊이 데이터와 같은 로봇 주변 환경에 대한 정보를 제공합니다. SLAM에서 사용되는 일반적인 센서에는 레이저 거리 측정기나 초음파 센서와 같은 거리 측정기, 카메라, 라이다 및 오도미터 센서가 있습니다. 스마트폰이 \"로봇\"인 경우, 스마트폰은 카메라, 자이로스코프, 가속도계 및 깊이 센서와 같은 다양한 센서를 통합한 플랫폼으로 작동합니다.\n\n이러한 용어 외에도 일반적으로 SLAM에서 사용되는 몇 가지 용어가 있습니다:\n\n- **오도메트리**: 휠 엔코더, 가속도계 또는 자이로스코프와 같은 내부 센서를 기반으로 로봇의 움직임을 추정합니다. 로봇의 속도, 회전 및 변위에 대한 정보를 제공합니다.\n- **루프 클로저**: 로봇의 궤적에서 루프를 감지하고 수정하는 프로세스입니다. 로봇이 이전에 관측한 랜드마크나 위치를 재방문할 때 발생합니다. 루프 클로저는 누적된 오차를 수정하고 SLAM 솔루션의 정확도를 향상시키는 데 중요합니다.\n\n<div class=\"content-ad\"></div>\n\nSLAM에 사용되는 주요 용어 및 용어 중 일부입니다. 이러한 개념을 이해하면 SLAM 알고리즘 및 기술을 효과적으로 활용할 수 있습니다.\n\n![SLAM 사용 사례](/assets/img/2024-06-19-OverviewofSLAM_2.png)\n\n## SLAM의 사용 사례는 무엇인가요?\n\nSLAM 기술은 우리의 일상생활에서 점차 더 흔해지고 있지만 우리는 항상 그것을 인식하지 못할 수도 있습니다. 우리 일상생활에서 만나게 되는 SLAM 응용 분야의 몇 가지 예시는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- GPS 내비게이션 시스템: 많은 야외 GPS 내비게이션 시스템은 SLAM 기술을 활용하여 정확하고 실시간의 위치 정보를 제공합니다. 이러한 시스템은 GPS 데이터를 가속도계 및 자이로스코프와 같은 다른 센서 입력과 결합하여 차량의 위치와 방향을 추정합니다. 구글 지도, Waze, Apple 지도가 예시입니다.\n- 모바일 증강 현실 (AR) 앱: 스마트폰 및 태블릿용 AR 앱은 주로 SLAM 알고리즘을 활용하여 장치의 위치를 추적하고 가상 객체를 현실 세계 위에 오버레이합니다. SLAM을 사용하면 사용자 주변과 가상 콘텐츠를 정확하게 정렬하여 몰입형 AR 경험을 제공합니다. 포켓몬 GO, 스냅챗, 인스타그램이 예시입니다.\n- 자율 주행 청소기: Roomba와 같은 로봇 청소기는 SLAM을 활용하여 효율적으로 방을 탐색하고 청소합니다. 환경의 지도를 작성하고 그 지도 내에서 자신의 위치를 결정하는 위치 결정 기술을 사용하여 장애물을 피하면서 자율적으로 이동합니다.\n- 실내 내비게이션 시스템: 대형 건물 내부에서 실시간 위치 정보를 제공하기 위해 SLAM이 사용됩니다. 카메라나 깊이 센서와 같은 센서를 사용하여 실내 환경을 매핑하고 사용자가 복잡한 공간을 탐색하는 데 도움을 줍니다. SLAM을 기반으로 하는 한 예는 Google의 \"실내 맵\" 기능입니다. Google 지도 애플리케이션에서 사용할 수 있으며, 이는 SLAM을 Wi-Fi, Bluetooth 등과 함께 사용하여 이 건물 내에서 실시간 위치 정보와 방향을 제공합니다.\n- 자율 주행 자동차: SLAM은 자율 주행 차량을 위한 기본 기술입니다. 자율 주행 자동차는 LiDAR, 카메라, 레이더 등 다양한 센서를 사용하여 주변 환경을 인식하고 세부적인 지도를 작성합니다. SLAM 알고리즘은 차량이 지도 내에서 자신의 위치를 지정하고 안전하게 이동하도록 돕습니다. Waymo, Tesla, Cruise, Uber가 예시입니다.\n\n<img src=\"/assets/img/2024-06-19-OverviewofSLAM_3.png\" />\n\n이것들은 SLAM 기술이 일상생활에 통합되는 몇 가지 예시일 뿐입니다. 로봇 공학 및 AI 분야가 발전함에 따라 우리는 다양한 분야에서 더 많은 SLAM 응용 프로그램을 기대할 수 있습니다.\n\n구현을 찾고 계십니까? 이것이 가장 간단한 SLAM 알고리즘입니다.\n\n<div class=\"content-ad\"></div>\n\n이 글이 마음에 드셨다면 ❤를 눌러 다른 사람들이 찾을 수 있도록 도와주세요!","ogImage":{"url":"/assets/img/2024-06-19-OverviewofSLAM_0.png"},"coverImage":"/assets/img/2024-06-19-OverviewofSLAM_0.png","tag":["Tech"],"readingTime":4},{"title":"로봇들이 제멋대로 현재와 미래에 우려할 이유가 있을까요","description":"","date":"2024-06-19 18:34","slug":"2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture","content":"\n\n인공 지능이 비즈니스의 모든 영역에서 발전하고 있지만, 만약 그것이 \"야생\"이 된다면 무슨 일이 벌어질지에 대한 심각한 우려가 있습니다.\n\n![로봇 급속 업데이트 문제](/assets/img/2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture_0.png)\n\n로봇공학은 산업에서 많은 지루하고 반복적인 작업을 대체했으며, 속도와 정밀도로 제품을 제공했는데, 이는 인간이 불가능할 수도 있습니다. 그러나 모든 자동화된 장치와 마찬가지로 단점이 있습니다. 자동차 생산 공장은 이러한 로봇의 실수에 위험에 노출된 것으로 보입니다.\n\n최근 자율주행차량 전체가 도로에서 내려온 일이 있었습니다. 그 이유는 차량의 센서가 도로 상의 다친 보행자를 인식하지 못했기 때문입니다. 차량이 보행자를 치고 그를 자율주행차량의 경로로 던졌습니다. 그 자율주행차량은 그녀를 치고 난 후 도로를 따라 20피트를 끌고 그녀를 차 밑에 묶어 남겼습니다. 그 결과 그녀는 위험한 상태로 중상을 입었습니다. 이제 거리를 건너는 것이 완전히 안전하지 않을 수도 있습니다. 보행자가 보도를 건널 때 자율주행차량이 멈추지 않은 것에 관한 연방 조사가 진행 중입니다.\n\n<div class=\"content-ad\"></div>\n\n정부가 승인한 자율 주행 자동차용 안전 절차가 명확히 정해져 있지 않습니다. 특히 자율 주행 자동차가 도로 중간에 갑자기 멈추는 경향은 교통을 방해하고 응급 서비스의 지연을 일으켜 주의를 불러일으키는 부분입니다. 샌프란시스코에서는 관리자들이 여러 사건을 기록해 왔습니다. 또한, 운전자 없는 자동차가 유아 수레에 부딪히는 것과 식료품 카트에 부딪히는 것 중 어떤 선택을 할지 결정해야 하는 로보틱스 전문가의 문제라는 난제도 있습니다.\n\n자율 주행 차량이 인간이 운전하는 차량보다 고장을 더 많이 일으킬 가능성이 높을 수 있습니다. 요즘은 자동차 사고가 특정 사건들일 뿐이며, 한 명의 위험 운전자는 전 세계의 다른 운전자에게 영향을 미치지 않습니다. 그러나 자율 주행 자동차가 등장하면 상황이 바뀔 수 있습니다. 어떠한 결함, 해킹, 시스템 고장이든 도로에 있는 모든 차량에 영향을 줄 수 있습니다.\n\n최근 언론 보도에 따르면 Cruise의 CEO조차도 자신의 안전 우려를 표명해 사임했다고 합니다. Tesla에 대한 추가적인 우려가 표명되기도 했습니다.\n\n캐나다의 무지개 다리를 건너려던 벤틀리가 폭발하는 사건으로 AI의 치명적인 사고에 대한 조사가 재개되었습니다. 이 고급차 회사는 오래된 모델(2020년-2023년)용 리콜을 여러 차례 내부적으로 실시해 왔는데, 이 특별한 사고의 차량 역시 오래된 모델이었습니다.\n\n<div class=\"content-ad\"></div>\n\n로봇이 우리에게 HAL이 하는 명령 거부의 불길한 소리를 들으면 어떻게 될까요? \"2001년 우주 여행\"에서 Hal이 말했던 \"Dave, 미안하지만 그걸 할 수 없어\"라는 말처럼. 이제 우리는 단순한 작업을 하는 로봇들과 함께, 센서가 사람과 같이 고장나면 비극이 일어날 수 있습니다.\n\n미디어에서 로봇 관련 공장 사고에 대해 듣는 것은 드물지만 최근 대한민국에서 공장 라인에서 센서를 수리하던 한 남성이 최신 희생자가 되었습니다. 로봇 팔이 갑자기 그를 붙잡아 음식 상자를 옮기는 대신에 컨베이어 벨트에 던지면서 그르게 만들었고, 결과적으로 그는 머리와 가슴에 치명적인 부상을 입었습니다.\n\n1992년부터 2017년 미국에서 로봇 관련 사망 사고가 41건 발생했는데, 대부분은 자체 전원으로 작동하는 고정 장치입니다. 폭스바겐 공장에서 장비를 설치하던 근로자가 로봇에 맞혀져 금속 벽에 눌려붙었고, 후에 병원에서 사망했습니다.\n\n미국 정부는 이러한 사건들을 기록하고 있지만 소비자들은 잘 참조하지 않습니다. 그러나 이것은 우리가 알아야 하는 정보를 제공합니다. 각 사건은 \"직원이 스폿용접 로봇이 눌러 죽을 때\", \"직원이 오크라 로봇 팔 사이에 끼여 죽음\", 또는 \"직원이 타 오르는 기계에 의해 눌려 죽음\"으로 나타내고 있습니다. 아니요, 기계는 생각하지 않습니다. 그들은 고장이 납니다. 이는 안전장치 부족 때문일 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n1981년, 제조 로봇과 관련된 최초의 사고 중 하나가 가장 가와사키 공장에서 발생했습니다. 작업자가 수리하기 위해 유닛 주변의 울타리를 무시하고 접촉했을 때, 우연히 \"켜기\" 스위치에 닿아 로봇 팔이 그를 잡아 기어를 절단하는 기계로 밀었습니다. 동료들이 구할 수 없었습니다. 그는 사망했습니다. 이 사건 이전에는 1979년 포드 공장에서 창고선반을 쌓는 로봇이 작업자를 죽였는데, 그 작업자에게로봇을 빨리 움직이라는 지시를 받았을 때 발생했습니다. 그는 가파르게 올라가 희생되었습니다.\n\n현재, 장치, 소프트웨어 또는 로봇에 대한 책임을 결정하는 데에 대한 철저한 연구가 많이 진행되지 않았습니다. 관심이 있다면 해당 주제에 대한 책이 있으니 읽어보거나 요약본을 확인하는 것도 좋을 것 같습니다.\n\n놀라운 과학 작가인 아이작 아시모프는 로봇을 위한 규칙을 정리했습니다. 그의 \"로봇의 세 법칙\"에서 첫 번째 규칙은 다음과 같습니다: \"로봇은 인간을 상해 입히거나, 또는 아무런 조치를 취하지 않아도 인간이 위험에 빠지게 해서는 안 됩니다.\" 그러나 로봇이 하나의 일만을 수행하는 경우 인간과 제품 항목을 구별할 능력이 없을 것입니다. 프로그램에는 행동 변경이 제공되지 않을 것입니다. 또한 로봇이 미사일이나 전쟁 물자를 발사하여 적을 죽이도록 프로그래밍된다면 첫 번째 법칙이 작동하지 않게 됩니다.\n\n주요 문제는 무엇일까요? 로봇의 경우 고려하지 않은 사항을 고려하지 못하는 것으로 보입니다. 이러한 부상이나 사망은 단지 비극뿐만 아니라 사전에 고려되어야 했으며 그에 대한 안전장치가 로봇 코드에 삽입되어야 했습니다. 그러나 무엇이 잘못될 수 있는지 깊게 고려하지 않는다면, 어떻게 그것을 방지할 코드를 작성할 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\nDDIntel에 가입해 주세요.\n\n특별한 이야기를 공유하고 싶으세요? DDIntel에 제출해 주세요.\n\n저희 창작자 생태계에 가입해 보세요.\n\nDDIntel은 주요 사이트와 인기있는 DDI Medium 출판물에서 주목할 만한 콘텐츠를 모으고 있습니다. 우리 커뮤니티의 풍부한 작업을 더 자세히 살펴보세요.\n\n<div class=\"content-ad\"></div>\n\nDDI 공식 텔레그램 채널: [https://t.me/+tafUp6ecEys4YjQ1](https://t.me/+tafUp6ecEys4YjQ1)\n\nLinkedIn, Twitter, YouTube 및 Facebook에서 팔로우해 주세요.","ogImage":{"url":"/assets/img/2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture_0.png"},"coverImage":"/assets/img/2024-06-19-RobotsGoneWildIsThereReasonforConcernNowandintheFuture_0.png","tag":["Tech"],"readingTime":4},{"title":"사냥 첫 번째 이야기","description":"","date":"2024-06-19 18:33","slug":"2024-06-19-TheHuntPartOne","content":"\n\n<img src=\"/assets/img/2024-06-19-TheHuntPartOne_0.png\" />\n\n드레이크는 깨어나 자신이 모르는 숲으로 떨어지고 있다는 것을 깨닫습니다. 그는 동일한 과정에서 많은 사람들을 만나게 됩니다: 로스 제타 경찰관 미겔, 특수 부대 알파 그룹 솔저 이반, 이스라엘 방위군 저격수 레아, RUF 장교 코피, 산 퀸틴 사형수 라스, 조직원 하오, 의사 이단. 그들은 레아가 전 블랙 옵스 전문가 및 용병일 것으로 의심하는 드레이크를 추적합니다. 숲 속에서 그들은 이상한 이미지, 빈 우리, 그리고 죽은 그린 베레 부대원의 죽음의 도구를 발견합니다.\n\n<img src=\"/assets/img/2024-06-19-TheHuntPartOne_1.png\" />\n\n더 나아가며, 그들은 외계인 하늘을 바라보며 자신들이 더 이상 지구에 있지 않음을 깨닫습니다. 이 곳은 사람들과 다른 동물들이 죽는 사냥터로 사용됩니다. 미겔은 살해되고, 그의 시체는 생존자들을 함정에 빠뜨리기 위해 사용됩니다. 그룹은 야수의 자취를 따라가 어느 캠프로 도착하게 되고, 거기서 프레데터가 잡혀 있는 것을 발견합니다. 그들의 사냥꾼인 트래커, 버서커, 그리고 팔코너라고 불리는 세 마리 큰 생물이 그룹을 공격하고, 코피를 죽입니다. 다른 이들은 탈출합니다. 레아의 고백한 '킬러'는 1987년 과테말라에서 특수 부대를 살해한 유사한 생물의 설명과 일치하지만 생존자에 의해 물리쳐지기도 했습니다.\n\n<div class=\"content-ad\"></div>\n\n![TheHuntPartOne_2.png](/assets/img/2024-06-19-TheHuntPartOne_2.png)\n\n혼자 남은 미국 육군 병사가 달에서 \"열 번의 시즌\" 동안 포식자들과 그 피해자들로부터 숨어 살아낸 이야기입니다. 그는 모두를 자신의 은신처로 인도하고 포식자들이 세 명씩 모여 기술을 연마하며 다른 세계에서 소중한 물건을 훔쳐 지구로 가져오는 것을 설명합니다. 가브리엘은 또한 포식자와 슈퍼 포식자라고 불리는 다른 부대 간에 쟁탈이 있다고 밝힙니다. 드레이크는 포식자들을 해방시켜 고향으로 데려오기를 희망하며 계획을 세웁니다. 그들은 장비를 구입합니다. 드레이크는 폭발물을 사용하여 포식자들을 꾀어 굴 속으로 이끕니다. 가브리엘은 그룹을 해방하는 트래커에게 살해당합니다.\n\n![TheHuntPartOne_3.png](/assets/img/2024-06-19-TheHuntPartOne_3.png)\n\n그에 이어 발생한 추격전에서 이반은 트래커를 죽이고 이단을 구하기 위해 자신을 희생합니다. 생존자들은 버서커에게 공격을 받지만 라스가 버서커의 주의를 분산시키고 다른 이들이 도망칠 수 있게 해줍니다. 하오는 가브리엘로부터 숨어 두었던 카타나로 파울코너와 결투를 벌이며 그를 죽이고 자신의 상처로 인해 죽습니다. 이단이 드레이크의 계획을 이어가길 희망하지만 함정에 걸려 부상을 입습니다. 리아가 그를 떠나지 않겠다고 하자 드레이크는 그들을 남겨두고 떠납니다. 그들은 버서커들에게 잡혀 구멍 속에 갇히고 캠프를 향해 계속 나아갑니다. 드레이크는 지구로 이동하기 위해 포식자들을 해방합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-TheHuntPartOne_4.png)\n\n그는 프레데터 갑옷을 입고, 손목 컴퓨터로 우주선을 켜고 지구로 향한다. 드레이크는 우주선을 탈출하지만, 버서커가 도착하여 다른 프레데터를 압도하고 목을 베어 그의 손목 컴퓨터를 사용하여 우주선을 파괴하고, 드레이크를 죽인 것으로 보인다. 이전에 식물에서 본 신경 독소를 사용하여 리아를 마비시키며, 그 또한 지구에 남고 싶은 암살자임을 밝힌 이단이다. 드레이크가 나타나 이단을 독립시킨 후 수류탄으로 그를 유인하여 버서커를 상처 입히는 함정을 설치한다.\n\n![이미지](/assets/img/2024-06-19-TheHuntPartOne_5.png)\n\n리아의 도움으로 드레이크는 버서커를 물리치고 죽인다. 사람들은 지구로 돌아가는 방법을 찾기 위해 숲으로 향한다. 생존 중 형성된 인연은 그들로 하여금 숲의 신비와 자신의 도전에 직면할 때 힘을 주며, 싸움이 끝나지 않았지만 살아남고 길을 찾겠다는 결의를 하고 있음을 깨닫는다.\n","ogImage":{"url":"/assets/img/2024-06-19-TheHuntPartOne_0.png"},"coverImage":"/assets/img/2024-06-19-TheHuntPartOne_0.png","tag":["Tech"],"readingTime":3}],"page":"66","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}