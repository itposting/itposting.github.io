{"pageProps":{"posts":[{"title":"RAG 평가 향상하기 FaaF와 ARES의 시너지 효과 및 구조화된 출력 방식 사용","description":"","date":"2024-06-23 18:43","slug":"2024-06-23-ElevatingRAGEvaluationTheSynergyofFaaFandARESthroughStructuredOutput","content":"\n\n인공 지능 소프트웨어가 이 기사의 텍스트 문법, 흐름 및 가독성을 향상시키는 데 사용되었습니다.\n\nRetrieval Augmented Generation (RAG) 시스템은 자연어 처리 (NLP)에서 유망한 접근 방식으로 나타났으며 정보 검색과 언어 생성 기술을 통합하여 생성된 응답의 품질과 관련성을 향상시키는 것을 목표로 합니다.\n\n이러한 시스템은 질문 응답, 대화 시스템 및 콘텐츠 생성을 포함한 다양한 응용 프로그램에서 상당한 잠재력을 보여주었습니다. RAG 시스템은 대규모 지식 베이스에서 관련 정보에 액세스하기 위해 검색의 힘을 활용하고 언어 모델의 생성 능력과 결합하여 더 정확하고 맥락적으로 관련성이 높고 정보를 제공하는 응답을 생성할 수 있습니다.\n\n그러나 RAG 시스템의 성능을 평가하는 것은 특히 원인 재현력을 평가하는 데 독특한 도전을 제기합니다. 왜냐하면 소스 자료에서 관련 정보를 검색하고 정확하게 전달하는 능력을 평가하는 것이 중요하기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n사실적 기억은 RAG 시스템 성능에 있어 중요한 측면이며, 생성된 응답의 신뢰성과 신뢰도에 직접적인 영향을 미칩니다. 관련 사실을 부정확하게 가져오거나 불완전하게 가져오면 잘못된 결과물이나 오도하는 결과물로 이어질 수 있어 시스템의 유용성과 신뢰성이 떨어집니다.\n\nRAG 시스템에 대한 전통적 평가 방법은 종종 인간 주석이나 휴리스틱 프롬프트를 의존하는데, 이는 여러 가지 제약사항이 있습니다. 인간 주석은 유용한 통찰력을 제공하지만, 시간이 많이 소요되며 비용이 많이 들며 개인적 편향과 불일치가 있을 수 있습니다.\n\n반면에 휴리스틱 프롬프트는 입력 공간의 다양한 변화와 미묘한 차이를 충분히 포착하지 못할 수 있어 시스템 능력의 불완전한 평가로 이어질 수 있습니다.\n\n게다가 이러한 방법들은 일반적으로 대규모 및 자동화된 평가가 필요한 현실 세계에서의 실용성과 신뢰성이 부족할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이러한 도전에 대응하고 RAG 시스템을 더 효과적으로 평가하기 위해 연구자들은 이를 위해 특별히 설계된 새로운 평가 프레임워크를 제안해왔습니다. 최근 논문에서 주목을 받고 있는 두 가지 중요한 접근 방식은 팩트를 함수로써 (FaaF)와 ARES (자동 검색 보완 생성 평가 시스템)입니다. 두 프레임워크 모두 RAG 시스템의 보다 효율적이고 신뢰할 수 있으며 통찰력 있는 평가를 제공하려고 하지만 방법론과 초점에서 차이가 있습니다.\n\n카트라니디스 등이 제안한 팩트를 함수로써(FaaF)는 RAG 시스템에 대한 사실 확인 및 사실 회상 평가를 위한 새로운 접근 방식을 제안합니다. FaaF 뒤에 있는 핵심 아이디어는 JSON 객체를 사용하여 사실을 호출 가능한 함수로 나타내어 전통적인 프롬프트 기반 방법에 비해 보다 구조화되고 효율적인 다중 사실 평가를 가능하게 합니다. 구조화된 표현과 프로그래밍 언어와 같은 인터페이스의 힘을 활용함으로써 FaaF는 RAG 평가의 해석 가능성, 모듈성 및 확장성을 향상시키기 위해 노력합니다.\n\n반면에 사드-팔콘 등이 제안한 ARES는 RAG 평가에 보다 포괄적인 접근 방식을 취하며, 문맥 상관성, 답변 충실도 및 답변 관련성과 같은 여러 차원을 고려합니다. ARES는 대규모 언어 모델(Large Language Models, LLMs)을 사용하여 합성 쿼리-패스지-답변 쌍을 생성하고 이를 통해 RAG 구성 요소를 평가하기 위해 LLM 판단자를 세밀하게 조정합니다.\n\n그것은 기계 생성 및 인간 주석 데이터를 활용하여 신뢰 구간 및 순위를 위한 사람 선호도 확증 세트를 활용하는 예측 기반 추론(Prediction-Powered Inference, PPI)를 채택합니다.\n\n<div class=\"content-ad\"></div>\n\nFaaF와 ARES는 접근 방식과 초점에서 차이가 있지만, 둘 다 RAG 평가에서 구조화된 출력의 중요성을 강조합니다. JSON 표현과 프로그래밍 언어와 비슷한 인터페이스를 사용하여 이러한 프레임워크는 일반 텍스트 프롬프트보다 신뢰성이 더 높고 모듈식이며 확장 가능한 평가를 가능하게 합니다.\n\nRAG 평가에서 구조화된 출력의 이점은 Pydantic과 같은 데이터 유효성 검사를 위한 라이브러리 사용과 유사한데요. Python에서 데이터 유효성 검사를 위해 Pydantic과 같은 라이브러리를 사용하는 것과 유사하게, 유형 안전성, 자동 변환, 사용자 정의 유효성 로직, 오류 처리 등의 장점을 제공합니다.\n\n# Facts as a Function (FaaF)이란?\n\n<div class=\"content-ad\"></div>\n\nFaaF는 Katranidis 등이 2024년에 발표한 \"RAG 시스템의 평가를 위한 함수로서의 사실\" 논문에서 소개된, 팩트 검증 및 사실 회상 평가에 대한 혁신적인 접근 방식입니다.\n\nFaaF의 핵심 아이디어는 사실을 JSON 객체를 사용하여 호출 가능한 함수로 표현함으로써, 전통적인 프롬프트 기반 방법과 비교하여 보다 구조화되고 효율적인 다중 사실 평가를 가능케 한다는 것입니다.\n\nJSON 대 프롬프트: FaaF는 언어 모델(LM)에게 사실을 전달할 때 자연어 프롬프트 대신 JSON 표현을 활용합니다. 이 접근 방식을 통해 LM은 함수 변수 간의 독립성을 학습하고, 변수 메타데이터와 타입 힌트를 통해 더 나은 제약을 제공하며, 예상 출력 형식에 대한 준수를 위한 가파른 경사를 가능하게 합니다. JSON을 사용함으로써, FaaF는 LM이 사실과 그 관계들의 구조화된 성격을 더 잘 파악할 수 있도록 하고자 합니다.\n\n텍스트 생성물을 단위로: FaaF는 관련된 모든 사실을 한 개체 함수로 캡슐화하여 생성된 텍스트를 단일 단위로 취급합니다. 이를 통해 각 사실을 개별적으로 판단하는 대신 단일 LM 호출로 다중 사실을 평가함으로써 효율성을 크게 향상시킵니다. 생성된 텍스트를 전체적으로 고려함으로써, FaaF는 RAG 시스템의 출력물의 전반적인 사실 일관성과 일관성을 포착할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n아웃소싱 판단: FaaF는 LM(Language Model)로부터 일부 결정론적 판단을 아웃소싱하는 개념을 도입합니다. 예를 들어, 구문 분석 함수는 LM 응답 범위를 이진 True/False 형식으로 매핑하여 LM이 명시적으로 진실 여부를 판단하는 부담을 줄입니다. 이 관심 분리를 통해 더 유연하고 해석 가능한 평가가 가능해지며, LM은 불확실성을 표현할 수 있으면서 최종 결정은 사전 정의된 함수 논리에 의해 이루어집니다.\n\nFaaF는 이러한 아이디어를 구체화하여 사실 명세서 목록을 JSON 객체로 매핑하는 과정을 정의합니다. LM 평가자(LMeval)는 입력 텍스트를 기반으로 이러한 필드를 채우는 역할을 하며, 구문 분석 함수는 LMeval 출력을 처리하여 사실을 확인합니다. 이 구조화된 방식은 FaaF가 RAG 시스템에서 사실 회상의 보다 명확하고 효율적인 평가를 제공할 수 있도록 합니다.\n\n# FaaF와 ARES 비교:\n\nARES(Automated Retrieval-Augmented Generation Evaluation System)는 RAG 시스템을 평가하기 위한 또 다른 주목할만한 프레임워크로, Saad-Falcon 및 그 동료가 \"ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\" 논문에서 제안했습니다. FaaF와 ARES 모두 RAG 평가를 개선하고자 하지만, 두 접근 방식과 초점에 차이가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 평가 차원:\n\n- FaaF는 사실적 기억에 중점을 둡니다 — 필요한 정보가 RAG 응답에서 정확하게 검색되고 전달되는 정도를 측정합니다. 이는 RAG 시스템이 소스 자료에서 관련된 사실을 통합하고 전달하는 능력을 명확히 평가하기 위한 목적을 갖고 있습니다.\n- 반면에 ARES는 RAG 시스템을 context relevance(맥락 중요성), answer faithfulness(답변 충실도), answer relevance(답변 관련성) 세 가지 차원에서 평가합니다. 이는 단순히 사실적 정확성뿐만 아니라 생성된 응답의 적합성과 일관성을 고려하는 종합적인 접근을 취합니다. 주어진 맥락 및 질문과 관련하여 생성된 응답의 확실성도 고려합니다.\n\n## 접근 방식:\n\n- FaaF는 JSON 객체를 사용하여 사실을 호출 가능한 함수로 나타내고 입력 텍스트를 기반으로 함수 필드를 채우기 위해 LM 평가자를 사용합니다. 구문 분석 함수는 LM 평가 출력을 처리하여 사실을 확인하며, 구조화되고 모듈화된 평가 프로세스를 가능하게 합니다.\n- 반면에 ARES는 LLMs를 사용하여 합성 쿼리-담화-답변 쌍을 생성하고, 이러한 합성 데이터에서 LLM 평가자를 세밀하게 조정하여 RAG 구성 요소를 평가합니다. 이는 기계 생성 및 인간 주석된 데이터를 활용하여 확신 구간과 순위를 위해 인간 선호 검증 세트와 함께 예측을 지원하는 추론(PPI)를 수행합니다.\n\n<div class=\"content-ad\"></div>\n\n## 데이터 효율성:\n\n- FaaF는 초기 데이터셋(예: WikiEval) 이상의 추가 인간 주석이 필요하지 않아 더 많은 데이터 효율성을 가지며 구현하기 쉽습니다.\n- ARES는 선호도 유효성 검사 세트를 위해 작은 인간 주석 집합(150~300 데이터 포인트)이 필요하며, 전문 지식이 필요한 특수 도메인에서 특히 도전적이고 자원이 소모될 수 있습니다.\n\n## 일반화 능력:\n\n- FaaF는 기본적으로 단일 데이터셋(WikiEval)에서 테스트되었으며 사실적 기억 평가에 중점을 두고 있습니다. 유망하지만 다른 데이터셋 및 평가 차원으로의 일반화 가능성은 아직 탐구되지 않았습니다.\n- ARES는 KILT, SuperGLUE 및 AIS 벤치마크의 여덟 데이터셋에서 강력한 성능을 보여주며 RAG 시스템의 여러 차원을 평가합니다. 이는 다양한 도메인 및 평가 기준에 걸쳐 일반화 가능성과 견고성이 높다는 것을 시사합니다.\n\n\n<div class=\"content-ad\"></div>\n\n# FaaF 및 ARES의 상보적 성격:\n\nFaaF와 ARES는 각각의 방법 및 초점에 차이가 있지만, 그들이 상호 배타적이 아니라 오히려 RAG 시스템을 평가하는 보완적인 프레임워크임을 인식하는 것이 중요합니다. 각 프레임워크는 강점과 한계가 있으며, 이들을 결합하여 연구자와 프랙티셔너들은 자신들의 RAG 모델의 성능과 한계에 대한 보다 포괄적인 이해를 얻을 수 있습니다.\n\nFaaF는 사실 기억에 대한 명시적 초점과 사실 표현에 대한 구조화된 JSON 기반 방식으로, RAG의 결과물의 사실적 정확성을 평가하기 위한 효율적이고 해석 가능한 프레임워크로 만들어졌습니다. 단일 LM 호출로 여러 사실을 평가하고, 결정론적 판단을 구문 논리에 위탁함으로써, 보다 간편하고 모듈화된 평가 과정이 가능해졌습니다.\n\n그 반면, ARES는 맥락 관련성, 답변 충실도 및 답변 관련성을 고려한 다차원 평가로, RAG 성능에 대한 더 포괄적인 평가를 제공합니다. 합성 데이터 생성 및 세밀하게 조정된 LLM 판단자들의 사용은 보다 확장 가능하고 자동화된 평가 프로세스를 가능케 하며, 인간의 선호도 확인 세트의 포함은 평가 지표를 인간의 판단과 일치시킴을 보장합니다.\n\n<div class=\"content-ad\"></div>\n\nFaaF와 ARES를 함께 사용하면 두 프레임워크의 강점을 활용하여 RAG 시스템을 보다 깊이 이해할 수 있습니다. 예를 들어, FaaF를 사용하여 RAG 출력물의 사실적 기억을 빠르게 평가하여 검색된 정보의 빈틈이나 모순을 식별할 수 있습니다. 이후에 ARES를 사용하여 보다 포괄적인 평가를 실시할 수 있는데, ARES는 생성된 응답의 문맥적 적합성, 충실성 및 전반적인 품질에 대한 통찰력을 제공할 수 있습니다.\n\n또한, FaaF와 ARES의 보완적 성격은 통합 및 확장 가능성에도 이어집니다. FaaF의 구조화된 JSON 기반 출력물은 ARES 평가 파이프라인에 쉽게 통합되어 RAG 평가의 넓은 맥락 내에서 사실적 기억을 보다 세밀하게 평가할 수 있도록 합니다. 마찬가지로, ARES에서 사용되는 합성 데이터 생성 및 LLM 판단 세밀 조정 방법은 FaaF의 효율성과 일반화 가능성을 향상시키기 위해 적용될 수 있어서, 더 넓은 데이터셋과 도메인에 적용 가능하도록 만듭니다.\n\n# 인공지능 언어 앱에서 구조화된 출력의 중요성 :\n\n구조화된 출력은 Facts as a Function (FaaF) 및 ARES 프레임워크에서 강조하는 바와 같이, 검색 증강 생성 (RAG) 시스템의 평가에서 중요한 역할을 합니다. JSON 객체와 프로그래밍 언어와 유사한 인터페이스와 같은 구조화된 표현을 사용함으로써, 이러한 접근 방법은 전통적인 일반 텍스트 프롬프트와 비교하여 더 신뢰성이 있고 모듈식이며 확장 가능한 평가를 가능케 합니다. RAG 평가에서 구조화된 출력의 장점은 다양하며, 이는 평가 프로세스의 품질과 효율성을 현저히 향상시킬 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n신뢰성과 일관성: 구조화된 출력은 사실, 쿼리 및 생성된 응답을 나타내는 잘 정의된 형식을 강제합니다. 일관된 구조를 준수함으로써 FaaF 및 ARES와 같은 평가 프레임워크는 평가 프로세스가 신뢰성 있고 재현 가능하다고 보장할 수 있습니다. 구조화된 형식은 모호성을 제거하고 자유 형식의 텍스트 표현에서 발생할 수 있는 오해 또는 일관성 부족의 위험을 줄입니다. 이 일관성은 다양한 RAG 시스템 간 보다 정확한 비교를 가능하게 하며 표준화된 평가 기준의 개발을 촉진합니다.\n\n조기 문제 감지: 구조화된 출력은 생성된 응답의 문제와 불일치를 조기에 감지할 수 있습니다. 출력의 구조 및 데이터 유형을 미리 정의된 스키마나 모델에 대해 유효성 검사함으로써 평가 프레임워크는 기대 형식에서의 어떠한 이탈도 빠르게 식별할 수 있습니다. 이 조기 감지 메커니즘은 평가 파이프라인에서 오류(예: 누락된 필드 또는 잘못된 필드)가 더 이상 전파되는 것을 방지합니다. 이러한 문제를 조기에 발견함으로써 연구원과 실무자들은 신속히 처리하여 디버깅 프로세스에서 시간과 노력을 절약할 수 있습니다.\n\n모듈화 및 조립성: 구조화된 출력은 RAG 평가에서 모듈화 및 조립성을 촉진합니다. 평가 프로세스를 사실 기억, 문맥 관련성 또는 답변 성실성과 같은 특정 측면에 집중하는 별도 구성 요소로 분해함으로써 구조화된 출력은 RAG 시스템 성능에 대한 보다 세분화된 목표적인 평가를 가능하게 합니다. 이 모듈화 접근법은 연구자들이 개별 구성 요소를 독립적으로 평가할 수 있도록 하여 시스템의 특정 영역에서의 강점과 약점을 식별하기 쉽게 합니다. 뿐만 아니라 구조화된 출력의 조립성은 여러 평가 지표나 프레임워크를 결합하여 더 포괄적인 RAG 성능 평가를 제공하는 평가 파이프라인을 생성할 수 있습니다.\n\n확장성과 적응성: 구조화된 출력은 RAG 평가 프레임워크의 확장성과 적응성을 촉진합니다. 잘 정의된 인터페이스와 데이터 모델을 활용함으로써 연구자들은 쉽게 새로운 평가 지표, 데이터 집합 또는 도메인별 요구 사항을 통합하여 기존 평가 프레임워크를 확장할 수 있습니다. 출력의 구조화된 성격은 핵심 평가 프레임워크에 중대한 수정을 요구하지 않고 추가 구성 요소나 모듈을 원활하게 통합할 수 있습니다. 이 확장성은 연구자들이 평가 프로세스를 자신의 특정 요구에 맞게 맞추고 변화하는 연구 문제와 응용 분야에 적응할 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 유효성 검증 라이브러리와의 통합: RAG 평가에서 구조화된 출력의 이점은 Python의 Pydantic과 같은 데이터 유효성 검증 라이브러리가 제공하는 것과 유사합니다. Pydantic을 사용하면 Python 유형 어노테이션을 활용하여 데이터 모델 또는 스키마를 정의하여 데이터의 구조와 유형을 유효성 검사할 수 있습니다. 유형 안정성, 자동 유형 변환, 사용자 정의 유효성 검사 논리, 정보를 제공하는 오류 메시지 등의 장점을 제공합니다. 구조화된 출력을 데이터 유효성 검증 라이브러리와 통합함으로써 RAG 평가 프레임워크는 이러한 강력한 기능을 활용하여 평가 데이터의 무결성과 일관성을 보장할 수 있습니다.\n\n예를 들어, 입력 쿼리, 검색된 단락, 생성된 응답에 대한 Pydantic 모델을 정의함으로써, 평가 프레임워크는 지정된 스키마에 대해 데이터를 자동으로 유효성 검사할 수 있습니다. 이 유효성 검사 과정은 유형 불일치, 필드 손실 또는 잘못된 값 등을 잡아내어 데이터 품질 문제에 대한 조기 피드백을 제공합니다. 또한, Pydantic의 사용자 정의 유효성 검사 논리 지원을 통해 연구자들은 도메인별 제한 사항이나 규칙을 정의하여 평가 프로세스를 더 개선할 수 있습니다.\n\n명확한 오류 메시지 및 디버깅: 구조화된 출력은 Pydantic과 같은 데이터 유효성 검증 라이브러리와 함께 사용될 때 평가 과정 중 문제가 감지될 때 명확하고 유익한 오류 메시지를 생성할 수 있습니다. 이러한 오류 메시지는 문제의 특정 위치와 성격을 정확히 지정하여 연구자들이 RAG 시스템이나 평가 파이프라인 자체의 문제를 식별하고 디버깅하기 쉽게 만듭니다. 출력의 구조적 특성은 더 정확한 오류 보고를 가능하게 하여 문제 해결에 필요한 시간과 노력을 줄이고 RAG 시스템 개발 및 정제의 빠른 반복 주기를 가능하게 합니다.\n\n상호 운용성 및 프레임워크 간 비교: 구조화된 출력은 RAG 평가에서 상호 운용성을 증진시키고 프레임워크 간 비교를 용이하게 합니다. 공통 구조화된 형식을 채택함으로써 다른 평가 프레임워크는 데이터와 결과를 더 쉽게 교환할 수 있습니다. 이 상호 운용성은 연구자들이 여러 프레임워크에서 얻은 통찰력을 결합하여 각 접근 방식의 장점을 활용하여 RAG 시스템 성능에 대한 더 포괄적인 이해를 얻을 수 있도록 합니다. 또한, 구조화된 출력은 다른 평가 프레임워크 간 직접적인 비교를 가능하게 하여 연구 커뮤니티 내에서 협력과 지식 공유를 촉진합니다.\n\n<div class=\"content-ad\"></div>\n\n다른 도구 및 워크플로와 통합: 구조화된 출력은 RAG 평가 프레임워크를 NLP 파이프라인의 다른 도구 및 워크플로와 완벽하게 통합할 수 있도록 합니다. JSON과 같은 구조화된 형식으로 평가 데이터를 표현함으로써, 평가 프레임워크는 데이터 처리 라이브러리, 시각화 도구 및 보고 시스템과 쉽게 상호 작용할 수 있습니다. 이 통합 기능은 평가 프로세스를 간소화하고 데이터 준비, 모델 학습, 평가 및 결과 분석을 포함하는 최종 결과 워크플로를 만들 수 있게 합니다. 이러한 출력의 구조화된 성격은 자동화와 재현성을 용이하게 하여 연구자가 견고하고 효율적인 평가 파이프라인을 구축할 수 있도록 합니다.\n\n# 결론:\n\nRAG 평가에서 구조화된 출력의 중요성을 과소 평가할 수 없습니다. JSON 객체 및 프로그래밍 언어와 유사한 인터페이스와 같은 구조화된 표현을 채택함으로써, FaaF 및 ARES와 같은 평가 프레임워크는 일반 텍스트 프롬프트 대비 보다 신뢰할 수 있고 모듈식이며 확장 가능한 평가를 가능하게 합니다. 구조화된 출력의 장점은 다재다능하며 초기 문제 발견 및 모듈화부터 확장성 및 Pydantic와 같은 데이터 유효성 검사 라이브러리와의 통합까지 다양합니다.\n\n구조화된 출력은 일관성을 촉진하고 모호성을 줄이며 표준화된 평가 기준의 작성을 가능하게 합니다. 개별 RAG 구성 요소의 타겟팅된 평가를 허용하며, 새로운 도메인 및 요구 사항으로 평가 프레임워크의 확장과 적응을 용이하게 하며, 효과적인 디버깅을 위한 명확한 오류 메시지를 생성합니다. 더불어, 구조화된 출력은 상호 운용성을 촉진하며, 프레임워크 간 비교를 가능하게 하며, NLP 파이프라인의 다른 도구 및 워크플로와 원활하게 통합할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n`Retrieval Augmented Generation` 분야가 계속 발전함에 따라 평가 프레임워크에서 구조화된 출력물의 채택은 최신 기술의 발전에 중요한 역할을 할 것입니다. 구조화된 표현과 데이터 유효성 검사 라이브러리의 이점을 활용하면 연구원과 실무자는 보다 견고하고 신뢰할 수 있으며 통찰력있는 평가 방법론을 개발할 수 있습니다. 결과적으로, 더 효과적이고 신뢰할 수 있는 RAG 시스템을 만들어 다양한 자연어 처리 응용 프로그램 및 그 이상을 혁신할 수 있습니다.\n\n\n\n# 친근한 언어로 설명 🚀\n\nIn Plain English 커뮤니티의 일원이 되어 주셔서 감사합니다! 마지막으로:\n\n- 작성자를 좋아요하고 팔로우 하세요 👏\n- 팔로우하기: X | LinkedIn | YouTube | Discord | Newsletter\n- 다른 플랫폼 방문하기: Stackademic | CoFeed | Venture | Cubed\n- PlainEnglish.io에서 더 많은 콘텐츠 확인하기","ogImage":{"url":"/assets/img/2024-06-23-ElevatingRAGEvaluationTheSynergyofFaaFandARESthroughStructuredOutput_0.png"},"coverImage":"/assets/img/2024-06-23-ElevatingRAGEvaluationTheSynergyofFaaFandARESthroughStructuredOutput_0.png","tag":["Tech"],"readingTime":11},{"title":"간단한 딥러닝 전략으로 암호화폐 백테스팅 결과 공개 버전 2","description":"","date":"2024-06-23 18:42","slug":"2024-06-23-BacktestingCryptocurrenciesAstonishingResultsfromaSimpleDeepLearningStrategyV2","content":"\n\n과거 글이 많은 댓글을 유발하고 의심스럽다는 점을 감안해 코드를 수정하고 결과를 보여드렸어요. 만약 수정할 부분을 발견하시면 알려주세요. 코드나 ONNX 모델이 필요하면 Github 페이지 링크를 남겨드릴게요.\n\n수정한 코드 결과는 다음과 같아요:\n\n![image](/assets/img/2024-06-23-BacktestingCryptocurrenciesAstonishingResultsfromaSimpleDeepLearningStrategyV2_0.png)\n\n```js\n평균 절대 오차(MAE): 3.343354936528137\n제곱근 평균 제곱 오차(RMSE): 4.441722762531883\nR-제곱(R2): 0.980843427945431\n평균 절대 백분율 오차(MAPE): 0.023306784351538545\n'SOL_USDT_price_prediction.png'로 저장된 그래프\n실제 가격과 예측 가격의 상관 관계: 0.9927086445091788\n'Estrategia Original'의 샤프 비율: -5.9404285882999135\n'New Strategy'의 샤프 비율: 18.33714244833774\n'New Strategy'의 Sortino Ratio: 117.31843386969027\n'New Strategy'의 Beta: 0.09456945402128551\n'New Strategy'의 Alpha: 0.03367816559565025\nCross-Validation 평균 절대 오차: 91.0050376257974 ± 41.98080676345999\nSMA 평균 절대 오차(MAE): 21.129903598484848\nSMA 제곱근 평균 제곱 오차(RMSE): 30.88072668067209\nSMA R-제곱(R2): 0.6086608043283657\n```\n\n<div class=\"content-ad\"></div>\n\n이것은 설명 동영상입니다:\n\n다음은 수정된 코드입니다:\n\n```js\nimport ccxt\nimport pandas as pd\nimport numpy as np\nimport onnx\nimport onnxruntime as ort\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\nfrom sklearn.model_selection import TimeSeriesSplit\n\n# 바이낸스 거래소의 인스턴스 생성\nbinance = ccxt.binance()\n\n# 시장 심볼과 시간 간격 설정\nsymbol = 'SOL/USDT'\ntimeframe = '1d'\nlimit = 1000  # 120일의 데이터 윈도우를 보장하기 위한 충분한 데이터 다운로드\n\n# 과거 데이터 다운로드\nohlcv = binance.fetch_ohlcv(symbol, timeframe, limit=limit)\n\n# 데이터를 판다스 DataFrame으로 변환\ndf = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\ndf['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n\n# 데이터를 CSV 파일로 저장\ndf.to_csv('binance_data.csv', index=False)\nprint(\"'binance_data.csv'에 다운로드 및 저장된 데이터\")\n\n# 다운로드된 데이터 로드\ndata = pd.read_csv('binance_data.csv')\n\n# 'timestamp' 열을 날짜 형식으로 변경\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\n\n# 정규화 값 (사용한 값에 맞게 조정)\nmin_close = data['close'].min()\nmax_close = data['close'].max()\n\n# 종가 데이터를 정규화\ndata['close_normalized'] = (data['close'] - min_close) / (max_close - min_close)\n\n# ONNX 모델 로드\nmodel = onnx.load('model_solusdt.onnx')\nonnx.checker.check_model(model)\n\n# 런타임 세션 생성\nort_session = ort.InferenceSession('model_solusdt.onnx')\n\n# 슬라이딩 윈도우 형태로 모델에 입력할 데이터 준비\ninput_name = ort_session.get_inputs()[0].name\nsequence_length = 120  # 모델에 따라 조정\n\n# 예측값 저장할 리스트 생성\npredictions_list = []\n\n# 예측 시작 날짜 설정\nstart_date = pd.Timestamp('2024-01-01')\nend_date = pd.Timestamp.today()\n\n# 하루씩 추론 실행\ncurrent_date = start_date\nwhile current_date < end_date:\n    # 현재 날짜 이전 120일 데이터 선택\n    end_idx = data[data['timestamp'] < current_date].index[-1]\n    start_idx = end_idx - sequence_length + 1\n    \n    if start_idx < 0:\n        print(f\"{current_date}일에 대한 충분한 데이터가 없습니다.\")\n        break\n    \n    # 정규화된 데이터 윈도우 가져오기\n    window = data['close_normalized'].values[start_idx:end_idx+1]\n    \n    if len(window) < sequence_length:\n        print(f\"{current_date}일에 대한 충분한 데이터가 없습니다.\")\n        break\n    \n    # 모델을 위한 데이터 준비\n    input_window = np.array(window).astype(np.float32)\n    input_window = np.expand_dims(input_window, axis=0)  # 배치 사이즈 차원 추가\n    input_window = np.expand_dims(input_window, axis=2)  # 특성 차원 추가\n    \n    # 추론 실행\n    output = ort_session.run(None, {input_name: input_window})\n    prediction = output[0][0][0]\n    \n    # 예측값 역정규화\n    prediction = prediction * (max_close - min_close) + min_close\n    \n    # 예측값 저장\n    predictions_list.append({'date': current_date, 'prediction': prediction})\n    \n    # 날짜 증가\n    current_date += pd.Timedelta(days=1)\n\n# 예측값 리스트를 DataFrame으로 변환\npredictions_df = pd.DataFrame(predictions_list)\n\n# 예측값을 CSV 파일로 저장\npredictions_df.to_csv('predicted_data.csv', index=False)\nprint(\"'predicted_data.csv'에 저장된 예측값\")\n\n# 예측값과 실제값 비교\ncomparison_df = pd.merge(predictions_df, data[['timestamp', 'close']], left_on='date', right_on='timestamp')\ncomparison_df = comparison_df.drop(columns=['timestamp'])\ncomparison_df = comparison_df.rename(columns={'close': 'actual'})\n\n# 에러 메트릭스 계산\nmae = mean_absolute_error(comparison_df['actual'], comparison_df['prediction'])\nrmse = np.sqrt(mean_squared_error(comparison_df['actual'], comparison_df['prediction'])\nr2 = r2_score(comparison_df['actual'], comparison_df['prediction'])\nmape = mean_absolute_percentage_error(comparison_df['actual'], comparison_df['prediction'])\nprint(f'Mean Absolute Error (MAE): {mae}')\nprint(f'Root Mean Squared Error (RMSE): {rmse}')\nprint(f'R-squared (R2): {r2}')\nprint(f'Mean Absolute Percentage Error (MAPE): {mape}')\n\n# 그래프 그리기\nplt.figure(figsize=(14, 7))\nplt.plot(comparison_df['date'], comparison_df['actual'], label='실제 가격', color='blue')\nplt.plot(comparison_df['date'], comparison_df['prediction'], label='예측된 가격', color='red')\nplt.fill_between(comparison_df['date'], comparison_df['prediction'] - mae, comparison_df['prediction'] + mae, color='gray', alpha=0.2, label='에러 밴드 (MAE)')\nplt.xlabel('날짜')\nplt.ylabel('가격')\nplt.title(f'{symbol} 가격 예측 대 비교')\nplt.legend()\nplt.savefig(f\"{symbol.replace('/', '_')}_price_prediction.png\")\nplt.show()\nprint(f\"'{symbol.replace('/', '_')}_price_prediction.png'로 그래프 저장됨\")\n\n# 잔차 분석\nresiduals = comparison_df['actual'] - comparison_df['prediction']\nplt.figure(figsize=(14, 7))\nplt.plot(comparison_df['date'], residuals, label='잔차', color='purple')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8)\nplt.xlabel('날짜')\nplt.ylabel('잔차')\nplt.title(f'{symbol} 예측 잔차')\nplt.legend()\nplt.savefig(f\"{symbol.replace('/', '_')}_residuals.png\")\nplt.show()\nprint(f\"'{symbol.replace('/', '_')}_residuals.png'로 잔차 그래프 저장됨\")\n\n# 상관 관계 분석\ncorrelation = comparison_df['actual'].corr(comparison_df['prediction'])\nprint(f'실제 가격과 예측 가격 간 상관 관계: {correlation}')\n\n# 투자 전략 시뮬레이션 (기존 전략)\ninvestment_df = comparison_df.copy()\ninvestment_df['strategy_returns'] = (investment_df['prediction'].shift(-1) - investment_df['actual']) / investment_df['actual']\ninvestment_df['buy_and_hold_returns'] = (investment_df['actual'].shift(-1) - investment_df['actual']) / investment_df['actual']\n\nstrategy_cumulative_returns = (investment_df['strategy_returns'] + 1).cumprod() - 1\nbuy_and_hold_cumulative_returns = (investment_df['buy_and_hold_returns'] + 1).cumprod() - 1\n\nplt.figure(figsize=(14, 7))\nplt.plot(investment_df['date'], strategy_cumulative_returns, label='전략 누적 수익', color='green')\nplt.plot(investment_df['date'], buy_and_hold_cumulative_returns, label='보유 및 보유 누적 수익', color='orange')\nplt.xlabel('날짜')\nplt.ylabel('누적 수익')\nplt.title(f'{symbol} 투자 전략 대 보유 및 보유')\nplt.legend()\nplt.savefig(f\"{symbol.replace('/', '_')}_investment_strategy.png\")\nplt.show()\nprint(f\"'{symbol.replace('/', '_')}_investment_strategy.png'로 투자 전략 그래프 저장됨\")\n\n# 손실 표시 계산\ninvestment_df['drawdown'] = strategy_cumulative_returns.cummax() - strategy_cumulative_returns\ninvestment_df['max_drawdown'] = investment_df['drawdown'].max()\n\nplt.figure(figsize=(14, 7))\nplt.plot(investment_df['date'], investment_df['drawdown'], label='손실', color='red')\nplt.xlabel('날짜')\nplt.ylabel('손실')\nplt.title(f'{symbol} 전략 손실')\nplt.legend()\nplt.savefig(f\"{symbol.replace('/', '_')}_drawdown.png\")\nplt.show()\nprint(f\"'{symbol.replace\n\n<div class=\"content-ad\"></div>\n\n```\nimport ccxt\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Dropout, Flatten\nimport tf2onnx\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# Binance 데이터를 다운로드하는 함수\ndef descargar_datos(symbol, timeframe='1d', start_date='2000-01-01T00:00:00Z', end_date='2024-01-01T00:00:00Z'):\n    exchange = ccxt.binance({'enableRateLimit': False})\n    since = exchange.parse8601(start_date)\n    end_date_timestamp = pd.to_datetime(end_date, utc=True)\n    all_data = []\n\n    while since < end_date_timestamp.timestamp() * 1000:\n        ohlc = exchange.fetch_ohlcv(symbol, timeframe=timeframe, since=since)\n        all_data.extend(ohlc)\n        since = ohlc[-1][0] + 1  # `since`를 1밀리초 증가\n\n    df = pd.DataFrame(all_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n    df.set_index('timestamp', inplace=True)\n\n    # 두 시간대가 timezone-aware하지 않으면 변환\n    if df.index.tz is None:\n        df.index = df.index.tz_localize('utc')\n    \n    df = df[df.index <= end_date_timestamp]\n    print(df)\n    return df['close'].values\n\n# 데이터 불러오기\ndata = descargar_datos('SOL/USDT')\n\n# 데이터 정규화\nscaler = MinMaxScaler(feature_range=(0, 1))\ndata = scaler.fit_transform(data.reshape(-1, 1))\n\n# 시퀀스에서 샘플을 생성하는 함수\ndef crear_muestras(dataset, pasos_de_tiempo=120):\n    X, y = [], []\n    for i in range(pasos_de_tiempo, len(dataset)):\n        X.append(dataset[i-pasos_de_tiempo:i, 0])\n        y.append(dataset[i, 0])\n    return np.array(X), np.array(y)\n\n# 훈련 및 테스트 데이터 준비\npasos_de_tiempo = 120\nX, y = crear_muestras(data, pasos_de_tiempo)\nX = X.reshape(X.shape[0], X.shape[1], 1)  # LSTM에 맞게 재구성\n\n# 데이터 분할 (훈련용 80%)\nsplit = int(0.8 * len(X))\nX_train, X_test = X[:split], X[split:]\ny_train, y_test = y[:split], y[split:]\n\n# 모델 훈련\n\nmodel = Sequential()\nmodel.add(Conv1D(filters=256, kernel_size=2, activation='relu',padding = 'same',input_shape=(X_train.shape[1],1)))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(LSTM(100, return_sequences = True))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100, return_sequences = False))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(units=1, activation = 'sigmoid'))\nmodel.compile(optimizer='adam', loss= 'mse' , metrics = [tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n\n# 조기 종료 설정\nearly_stopping = callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    restore_best_weights=True,\n)\n# 가장 좋은 모델 저장\ncheckpoint = ModelCheckpoint(\n    'best_model.h5', \n    monitor='val_loss', \n    save_best_only=True, \n    save_weights_only=False\n)\n\n\n# 300 에포크로 모델 훈련\nhistory = model.fit(X_train, y_train, epochs = 300 , validation_data = (X_test,y_test), batch_size=32, callbacks=[early_stopping, checkpoint], verbose=2)\n\n# 훈련 이력 그래프\nplt.figure(figsize=(10, 5))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.plot(history.history['rmse'], label='Train RMSE')\nplt.plot(history.history['val_rmse'], label='Validation RMSE')\nplt.title('Model Training History')\nplt.xlabel('Epochs')\nplt.ylabel('Loss/RMSE')\nplt.legend()\nplt.savefig('SOLUSDT.png')  # 그래프를 이미지 파일로 저장\n\n# 모델을 ONNX로 변환\nonnx_model, _ = tf2onnx.convert.from_keras(model, opset=13, output_path=\"model_solusdt.onnx\")\nprint(\"ONNX 모델을 'model_solusdt.onnx'로 저장했습니다.\")\n\n# 모델 평가\ntrain_loss, train_rmse = model.evaluate(X_train, y_train, verbose=0)\ntest_loss, test_rmse = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"train_loss={train_loss:.3f}, train_rmse={train_rmse:.3f}\")\nprint(f\"test_loss={test_loss:.3f}, test_rmse={test_rmse:.3f}\")\n\n\ngithub: Back-testing Cryptocurrencies Astonishing Results from a Simple Deep Learning Strategy\n\nyoutube explanation: [여기를 클릭하여 유튜브 설명 보기](https://youtu.be/z_taWHp_HaI)\n","ogImage":{"url":"/assets/img/2024-06-23-BacktestingCryptocurrenciesAstonishingResultsfromaSimpleDeepLearningStrategyV2_0.png"},"coverImage":"/assets/img/2024-06-23-BacktestingCryptocurrenciesAstonishingResultsfromaSimpleDeepLearningStrategyV2_0.png","tag":["Tech"],"readingTime":12},{"title":"아물지 않는 상처를 치료하는 방법","description":"","date":"2024-06-23 18:41","slug":"2024-06-23-AWoundThatWontHeal","content":"\n\n## 시간이 흐르고도 마음의 상처가 멈추지 않는 이유는 무엇일까요?\n\n![image](/assets/img/2024-06-23-AWoundThatWontHeal_0.png)\n\n사람들이 \"시간이 모든 상처를 치유한다\"고 말할 때, 그게 무슨 의미인지 궁금했을 거예요. 어렸을 때부터 저는 항상 그 생각에 고민을 했어요. 깊이 생각해 보려고 노력했지만, 여전히 그 의미를 이해하지 못했어요. 제가 상처받은 사람의 시각에서 보기 때문일까요? 그러나 시간이 흘러가면서 그것의 표면적인 면을 보게 되었어요.\n\n부모님께서 당신 앞에서 말다툼을 하실 때 어떻게 하시나요?\n\n<div class=\"content-ad\"></div>\n\n빠른 초기 반응은 울음일 거라고 생각하실 거에요, 맞죠? 저도 그랬어요. 어린 시절, 부모님들이 부엌에서 다투는 걸 보고 순간적으로 울음이 터져나왔어요. 왜 그런지 모르겠지만 마음이 찢기는 듯한 느낌이었어요. 제가 울면 그들이 싸움을 그만 둘지도 모를 것 같다는 생각이 들었거든요. 그러나 속은 찢어진 채로 지속되던 상황은 11살이 될 때까지 이어졌어요.\n\n내가 마지막으로 목격한 다툼은 아버지가 결혼증명서를 찢어버리는 거였어요. 그 이유를 기억하지 못하지만, 그 밤 본 것은 맑고 맑았어요. 울면서 그들이 그만둬서 화해하리라 생각했지만, 다시 한 번 실망했어요. 그 밤 그들은 영원히 헤어지기로 결정했어요.\n\n어린이로서 여러분은 어떤 느낌일까요?\n\n저는 상처받았어요. 너무 많이 상처 받았어요. 나조차 버려진 느낌이 들었어요. 내 존재를 의심했어요. \"가족의 일원이 아닌 걸까? 왜 결정에 참여하지 못하는 걸까?\" 오랜 기간 동안 상처받았어요. 어워드 시상식에서 외로웠고, 학교 모임에서 두 부모가 함께 참석해야하는 자리에서도 외로웠고, 부모님이 함께 참석하길 바랐던 대회에서도 외로웠어요. 하지만 제가 더 이상 울지 않았어요.\n\n<div class=\"content-ad\"></div>\n\n그것은 여전히 나를 괴롭히고 슬프게 만들어. 특히 양 부모가 이미 새로운 파트너를 만났을 때는 더 그럽니다. 처음에는 충격을 받았고, 화가 나있었고, 상처받았는데 여전히 그들이 넘어가는 것이 내 안 깊이 아프다는 생각에 충격을 받았다. 그러나 시간이 지남에 따라 나는 이에 익숙해졌어. 두 부모가 주위에 없는 것에 익숙해지고, 내가 달성한 모든 이정표에서 한 명만 참석했다. 내가 그것에 대해 통고하지 않은 것은 나에게 이미 받아 들였음을 알았기 때문에 나는 그 위로 잘 받아들였다. 나는 고등학교 시절에 예상보다 덜 신경 쓰게 됐습니다.\n\n그런데 오늘, 우연히 나 자신이 내 이야기를 낯선 사람에게 말하고 있었다. 나는 어떻게 가족이 헤어지게 되었는지에 대해 작은 세부 사항을 공유했어. 나는 심심해서 지나칠 정도로 많이 얘기한 것이 아니라서 그 사람으로부터 한 단락 온다고는 정말 예상치 않았다. 나에게 그것은 큰 문제가 아니었다.\n\n그 사람의 한 마디로 인해 나는 자신이 아직 치유되지 않은 통증들을 회상했다. \"모든 것을 즉시 받아들이거나 시간이 지나게 익숙해지는 것이 필요한 건 아닙니다. 모든 일을 처리하는 데는 많은 연도가 걸릴 수 있습니다.\"\n\n나에게는 좋은 하루였는데, 그 문장을 읽었을 때 그런 기분이었다. 그것이 내게 얼마나 큰 영향을 미쳤는지를 깨달았을 때 나는 우는 소리가 나왔다. 나는 그런 일들로부터 치유받지 못했다는 것을 깨달았고, 얼마나 망가졌는지 알게 됐다. 나는 그것을 혐오한다.\n\n<div class=\"content-ad\"></div>\n\n아포리즘 \"시간이 모든 종류의 상처를 치유한다\"는 미신입니다. 상처는 치유되지 않습니다. 우리 안에서 계속 퍼져나가서 그 상처가 낫기 전에 우리 전체 존재가 그 상처에 의해 소멸될 때까지 기다립니다. 그래서 나에게 있어서 그 고통이 너에게 가해졌을 때가 몇 년이 지났는지 상관없이, 그것은 끝나지 않습니다. 네 고통은 끝나지 않을 것입니다. 다행인 것은, 몇 번이나 그것을 직면하더라도, 아무리 그것이 나았다 생각하고 넘어갔다고 생각해도, 당신은 그저 자신을 속이고 있는 것뿐입니다. \n\n어느 날, 수십 년 전에 일어났던 일의 일부를 기억하게 되는 순간, 그때와 같이 다시 전부 무너지게 될 것입니다. 고통은 끊임없이, 영원히 계속됩니다. 끝이 없습니다.\n\nConan Gray의 \"The Exit\"\n\n[여기를 클릭하여 노래 듣기](https://music.youtube.com/watch?v=E84bB00pie4&si=92YH-WsRGT1wLn4J)","ogImage":{"url":"/assets/img/2024-06-23-AWoundThatWontHeal_0.png"},"coverImage":"/assets/img/2024-06-23-AWoundThatWontHeal_0.png","tag":["Tech"],"readingTime":3},{"title":"파이썬으로 감성 분석 모델 만드는 방법","description":"","date":"2024-06-23 18:40","slug":"2024-06-23-HowtoCreateaSentimentAnalysisModelinPython","content":"\n\n<img src=\"/assets/img/2024-06-23-HowtoCreateaSentimentAnalysisModelinPython_0.png\" />\n\n이 글에서는 예측 분석의 영역에 대해 탐구하며, COT 보고서에서 제공되는 다양한 통찰력을 활용하여 기계 학습 모델의 기능을 향상시킬 수 있는 방법을 살펴봅니다. 트레이더 포지션 내의 패턴을 해석함으로써, 우리는 금융 세계에서 더 정확하고 데이터 기반의 의사 결정을 내릴 수 있는 잠재력을 발견할 수 있습니다.\n\n# KNN 알고리즘과 COT 보고서 소개\n\nK-Nearest Neighbors (KNN)은 분류 및 회귀 작업에 모두 사용되는 간단하고 직관적인 기계 학습 알고리즘입니다. 하지만 먼저, 분류와 회귀가 무엇을 의미하는지 알아야 합니다.\n\n<div class=\"content-ad\"></div>\n\n- 분류(Classification)는 지도 학습의 한 유형으로, 데이터 포인트를 미리 정의된 클래스나 레이블로 분류하는 것을 목표로 합니다. 분류에서 모델은 입력의 특징에 기반하여 클래스나 범주를 지정하는 방법을 학습합니다. 분류의 출력은 이산적이며 범주나 클래스 레이블을 나타냅니다. 예를 들어, 이메일을 스팸 또는 비스팸으로 분류하는 것입니다.\n- 반면에 회귀(Regression)는 지도 학습의 한 유형으로, 연속적인 수치 값을 예측하는 것을 목표로 합니다. 회귀에서 모델은 입력 특징과 출력 간의 관계를 수립하는 것을 학습하며, 출력은 연속적인 범위의 값이며 양이나 수치 값을 나타냅니다. 예를 들어, 집의 가격을 예측하거나 주가를 예측하는 것입니다.\n\nKNN은 게으른 학습(lazy learning)의 한 유형으로, 훈련 중에 모델을 구축하지 않고 대신 전체 훈련 데이터 집합을 기억하고 새 데이터 포인트와 기존 데이터 포인트 간의 유사성을 바탕으로 예측합니다.\n\n그래서 KNN의 핵심 아이디어는 유사한 특징을 가진 객체(데이터 포인트)이 특징 공간에서 서로 가깝다는 것입니다. KNN은 특징 공간에서 특정 테스트 데이터 포인트와 가장 가까운 K개의 훈련 예제를 찾아서 그 주변 이웃들의 레이블이나 값에 따라 테스트 포인트에 레이블이나 값을 할당합니다. 이는 K가 훈련 단계에서 조절할 수 있는 변수라는 것을 의미합니다.\n\n다음 그림을 살펴보세요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-HowtoCreateaSentimentAnalysisModelinPython_1.png\" />\n\n두 가지 클래스가 있습니다. 클래스 A(바나나)와 클래스 B(사과)입니다. 우리는 파란색 물체의 클래스(또는 레이블)를 식별하려고 합니다. 가장 가까운 이웃은 바나나로 보이기 때문에 클래스 A입니다. 이것이 KNN 알고리즘의 작동 방식입니다.\n\n<img src=\"/assets/img/2024-06-23-HowtoCreateaSentimentAnalysisModelinPython_2.png\" />\n\nKNN은 회귀 작업에도 사용될 수 있습니다. KNN 회귀에서 목표는 새로운 데이터 포인트의 연속 값을 예측하는 것입니다. 이 값은 이웃 K개의 값에 기반하여 예측됩니다. KNN 회귀의 단계는 KNN 분류와 유사하지만 클래스 레이블을 세는 대신, K개 이웃의 대상 값들의 평균 또는 가중 평균을 계산하여 테스트 포인트의 값을 예측합니다.\n\n<div class=\"content-ad\"></div>\n\n커밋먼츠 오브 트레이더스(COT) 보고서는 미국 상품 선물 거래위원회(CFTC)가 매주 발간하는 보고서로, 선물 시장의 다양한 참여자들의 포지션에 대한 통찰을 제공합니다. 이러한 참여자들에는 상업 헤지거들, 대형 투기자들, 그리고 소규모 거래자들이 포함됩니다.\n\n보고서는 이러한 그룹들의 순 포지션을 보여주어 그들이 상승을 기대하고 있는지(매수 포지션) 또는 하락을 예상하고 있는지(매도 포지션)를 나타냅니다. 순 COT 값은 각 그룹의 총 매수 및 매도 포지션의 차이를 통해 계산됩니다.\n\n순 COT 값을 분석하면 잠재적인 미래 가격 움직임에 대한 어떤 단서를 제공할 수 있습니다. 예를 들어, 대형 투기자들이 상당히 많은 순 매수 포지션을 보유하고 있다면 시장에서 매수 센티먼트가 있다는 것을 시사할 수 있습니다. 그러나 다른 요소들을 고려하고 COT 보고서를 예측의 여러 도구 중 하나로 활용하는 것이 중요합니다. 트렌드는 변할 수 있고, 시장 역학은 COT 데이터 이상의 다양한 요인에 영향을 받습니다.\n\n간단히 말하면, COT 보고서는 시장에서 어떤 그룹들의 거래자들이 어떻게 베팅을 하고 있는지 요약해줍니다. 그들의 포지션에 주목할 만한 변화가 있다면, 시장 트렌드의 가능성 있는 변화를 나타낼 수 있습니다. 그러나 이것이 마법구슬은 아니며, 더 포괄적인 분석을 위해 다른 요소들을 고려해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Sentiment Analysis Model in Python](/assets/img/2024-06-23-HowtoCreateaSentimentAnalysisModelinPython_3.png)\n\nIf you want to see more of my work, you can visit my website for the books catalogue by simply following the link attached the picture:\n\n![Books Catalogue](/assets/img/2024-06-23-HowtoCreateaSentimentAnalysisModelinPython_4.png)\n\n# Creating the Algorithm\n\n\n<div class=\"content-ad\"></div>\n\nKNN 회귀 예제 코드를 시도해봅시다. 주요 작업은 GBP의 순 COT 값 변화를 예측하는 것입니다. 아래 코드를 사용하여 작업을 수행해보세요:\n\n```js\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsRegressor\n\ndata = pd.read_excel('COT_GBP.xlsx')\ndata = np.reshape(np.array(data), (-1))\ndata = np.diff(data)\n\ndef data_preprocessing(data, num_lags, train_test_split):\n    # 훈련용 데이터 준비\n    x = []\n    y = []\n    for i in range(len(data) - num_lags):\n        x.append(data[i:i + num_lags])\n        y.append(data[i + num_lags])\n    x = np.array(x)\n    y = np.array(y)\n\n    split_index = int(train_test_split * len(x))\n    x_train = x[:split_index]\n    y_train = y[:split_index]\n    x_test = x[split_index:]\n    y_test = y[split_index:]\n\n    return x_train, y_train, x_test, y_test\n\nnum_lags = 20\ntrain_test_split = 0.80\nx_train, y_train, x_test, y_test = data_preprocessing(data, num_lags, 0.85)\n\nmodel = KNeighborsRegressor(n_neighbors=2)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\nplt.plot(y_pred[-100:], label='예측 데이터', linestyle='--', marker='.', color='red')\nplt.plot(y_test[-100:], label='실제 데이터', marker='.', alpha=0.7, color='blue')\nplt.legend()\nplt.grid()\nplt.axhline(y=0, color='black', linestyle='--')\nsame_sign_count = np.sum(np.sign(y_pred) == np.sign(y_test)) / len(y_test) * 100\nprint('일치율 =', same_sign_count, '%')\n```\n\nCFTC 웹사이트에서 COT GBP 데이터를 찾을 수 있습니다. 다음 차트는 실제 데이터와 예측 데이터를 비교한 것입니다:\n\nK 값이 2이고 입력값으로 20개의 래그 값을 사용한 알고리즘의 일치율(정확도)은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n\nHit Ratio =  67.92 %\n\n![Image](/assets/img/2024-06-23-HowtoCreateaSentimentAnalysisModelinPython_5.png)\n","ogImage":{"url":"/assets/img/2024-06-23-HowtoCreateaSentimentAnalysisModelinPython_0.png"},"coverImage":"/assets/img/2024-06-23-HowtoCreateaSentimentAnalysisModelinPython_0.png","tag":["Tech"],"readingTime":5},{"title":"경량 GPU를 위한 실시간 고정확도 객체 탐지  YOLO-ReT 상세 리뷰","description":"","date":"2024-06-23 18:39","slug":"2024-06-23-BriefReviewYOLO-ReTTowardsHighAccuracyReal-timeObjectDetectiononEdgeGPUs","content":"\n\n## YOLO-Ret, Jetson Nano 및 Jetson Xavier NX에서 실시간으로 작동하는 Jetson Xavier NGX\n\n- 최신 기술의 다양한 특징 규모 간의 결합적 연결 부족을 활용하여 새로운 다중 규모 특징 상호 작용을 제안합니다.\n- 또한 새로운 전이 학습 백본 절단도 제안됩니다.\n\n# 개요\n\n- YOLO-Ret\n- 결과\n\n<div class=\"content-ad\"></div>\n\n# 1. YOLO-Ret\n\n![Image](/assets/img/2024-06-23-BriefReviewYOLO-ReTTowardsHighAccuracyReal-timeObjectDetectiononEdgeGPUs_0.png)\n\n## 1.1. Raw Feature Collection and Redistribution (RFCR) Module\n\n- 기존의 다중 스케일 피처 상호 작용 방식은 한 번에 두 개의 인접한 피처 스케일에만 초점을 맞추고 있습니다.\n- 게다가 상하 방향의 반복 사용 시, 모델의 감지 정확도가 포화될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 이러한 레이어에는 무거운 계산이나 매개변수가 포함되지 않지만, 각 특성 스케일의 모든 쌍 사이에 직접적인 링크를 가능하게 합니다.\n- YOLOv3 탐지 헤드에 3개의 출력 스케일이 있지만, RFCR은 네 가지 다른 백본 피처를 사용하여 모델 성능을 향상시킬 수 있는 보다 세분된 저수준 피처를 활용할 수 있습니다.\n\n## 1.2. 백본 Truncation\n\n![image](/assets/img/2024-06-23-BriefReviewYOLO-ReTTowardsHighAccuracyReal-timeObjectDetectiononEdgeGPUs_1.png)\n\n- 실험에는 MobileNetV2 (×0.75 및 ×1.4) 및 EfficientNet-B3와 같이 3가지 일반적으로 사용되는 백본이 사용되며, 이들 백본은 다양한 블록으로 나뉩니다.\n\n\n<div class=\"content-ad\"></div>\n\n- 위의 도표 2에서 결과를 기반으로 하면, MobileNetV2 버전의 마지막 두 블록과 EfficientNet의 마지막 세 블록을 백본으로 채택할 때 잘려진다.\n\n## 2. 결과\n\n### 2.1. 제거 연구\n\n![도표 2](/assets/img/2024-06-23-BriefReviewYOLO-ReTTowardsHighAccuracyReal-timeObjectDetectiononEdgeGPUs_2.png)\n\n<div class=\"content-ad\"></div>\n\n- 제안된 RFCR 모듈에는 추가로 ’shortcut’ 연결이 도입되었습니다. 이 추가된 ‘shortcut’은 백본의 더 얇은 레이어에서 비롯되어 정확성을 더 향상시키며, 정확한 탐지 작업에 대한 낮은 수준의 기능의 중요성을 강조합니다.\n\n## 2.2. SOTA 비교\n\n![이미지](/assets/img/2024-06-23-BriefReviewYOLO-ReTTowardsHighAccuracyReal-timeObjectDetectiononEdgeGPUs_3.png)\n\n- 모델은 Jetson Nano, Jetson Xavier NX, Jetson Xavier NGX에 배포됩니다.","ogImage":{"url":"/assets/img/2024-06-23-BriefReviewYOLO-ReTTowardsHighAccuracyReal-timeObjectDetectiononEdgeGPUs_0.png"},"coverImage":"/assets/img/2024-06-23-BriefReviewYOLO-ReTTowardsHighAccuracyReal-timeObjectDetectiononEdgeGPUs_0.png","tag":["Tech"],"readingTime":2},{"title":"AVP의 GrayMatter Robotics 투자에 대해 알아야 할 5가지 사항","description":"","date":"2024-06-23 18:38","slug":"2024-06-23-AVPsInvestmentinGrayMatterRobotics","content":"\n\n![AVP's Investment in Gray Matter Robotics](/assets/img/2024-06-23-AVPsInvestmentinGrayMatterRobotics_0.png)\n\n저희는 제조업, 농업, 의료 및 호텔업 등 물리적 산업 전반에서 AI 기반 로봇 플랫폼이 대규모 인간 노동을 보강할 기술적 역량에 도달하면서 혁명을 이루는 과정의 첫 이닝에서 있습니다. 로봇 플랫폼은 과거에는 환경의 실시간 변화, 높은 공정 또는 제품 변이성, 복잡한 제조 공정에서 흔한 다른 요인에 대한 반응 불가능으로 인해 간단하고 반복적인 프로세스로 제약되어 왔습니다. 그러나 처리 능력의 발전, 대규모 데이터 집합을 처리, 저장 및 분석할 수 있는 능력, 그리고 생성적 인공지능의 지수 함수적인 발전을 통해 로봇 기술은 인간 노동과 동등하거나 그 이상의 속도, 정확도, 품질로 고복잡도, 고변이성 작업을 완료할 수 있는 수준에 이르렀습니다. 하드웨어의 가격 하락과 기능 향상이 결합되어, 로봇 솔루션이 기업에게 경제적으로 실용적인 수준에서 마침내 솔루션이 되어, 대규모 채택을 위한 길을 열고 있습니다.\n\n이러한 주장을 추구하는 동안, 저희는 차기 투자 사실을 기쁘게 알려드립니다. Advance Venture Partners(AVP)는 GrayMatter Robotics의 시리즈 B 자금 조달에 투자하고 있습니다. 이 투자는 공장 마감 처리 및 처리를 포함한 갠능한 제조 과제를 수행하는 로봇 솔루션을 구축하는 공동 창업자 Ariyan Kabir, Brual Shah 및 SK Gupta를 지원합니다.\n\nAriyan, Brual 및 SK와 처음 만나보았을 때, 그들은 혁신적인 로봇 기업을 설립할 수 있는 고급 기술 지식을 보유할 뿐만 아니라 고객 요구에 부합하는 플랫폼을 설계하고 GrayMatter를 포함한 모든 이해 관계자에게 매력적인 경제를 창출하는 비즈니스 마인드를 지닌 것을 즉시 알아챘습니다. USC 고급 제조업 센터에서 활동했을 때, Ariyan과 Brual은 제조 업계를 기반으로 한 AI 기반 로봇 플랫폼을 구축하여 생산 과정에서 가장 흔한 표면 마감 및 처리 작업을 시작점으로 삼을 수 있는 기회를 발견했습니다. 표면 마무리 및 처리는 기타 것들로 기타 제품까지 $2.5조 규모의 미국 제조 산업에서 중요한 단계로, 이 업무는 유해하고 힘든 성격 및 필요로 하는 방대한 훈련 때문에 심각한 인력 부족을 겪는 여러 작업 중 하나이며, 이것들은 특히 청년 세대들이 매력적인 직업 기회로 보지 않는 무거운 일과 위험한 역할입니다.\n\n<div class=\"content-ad\"></div>\n\n거대한 산업에 중립적인 문제를 염두에 두고 GrayMatter 팀은 샌딩, 버핑, 그라인딩, 폴리싱과 같은 표면 마무리 및 처리를 자동화하는 전체 스택 AI 솔루션을 구축했습니다. 기업 플랫폼은 현재까지의 로봇 플랫폼의 주요 문제를 극복했습니다 - 동적이고 변화가 빈번한 환경에서 사람들과 마찬가지로 복잡한 작업을 동일한 처리량과 품질로 수행하지 못하는 점입니다. GrayMatter의 로봇들은 심지어 시스템이 아직 보지 못한 물체에 대해서도 본 시간에 복잡한 작업을 자율적으로 처리할 수 있습니다. 결과적으로 고객들은 GrayMatter를 활용함으로써 다면적인 투자수익을 누릴 수 있습니다. 로봇들은 수동 작업보다 2~4배 빠르게 작업할 뿐만 아니라 사람이 6개월이 걸릴 작업을 머신러능이 1일 이내에 훈련받을 수 있어서 인건비(임금, 고용, 훈련, 노동자 보상)의 직간접 비용을 줄이며, 고객의 매출을 증가시킴으로써 공정 처리량을 개선하고 백로그를 감소시키며, 재작업과 폐기물이 줄어드는 고품질 결과물을 만듭니다. 로봇들은 또한 소비하는 소모품 비용(샌드페이퍼, 페인트 등)을 수동작업보다 30% 이상 절약할 수 있어서 고객이 지속가능성 목표를 달성하는데 도움이 됩니다. 많은 산업에서 심각한 노동력 부족 문제에 추가되어, 고객이 인식한 구체적인 투자수익은 GrayMatter를 선택해야 하는 단계로 만듭니다.\n\n우수한 기술과 명확한 제품-시장 적합성을 바탕으로 GrayMatter는 중요하게 로봇서비스 비즈니스 모델을 통해 자체적으로 매력적인 경제를 창출할 수 있는 능력을 입증했습니다. 많은 로봇 기업들이 우수한 기술을 개발했지만 확장 가능한 흥미로운 비즈니스 모델을 구축하지 못했습니다. Ariyan과 Brual은 GrayMatter와 고객을 위해 초기에 강력한 경제를 창출할 필요성을 이해했습니다. 회사는 이미 항공우주 및 국방, 조선업, 특수 차량, 레크리에이션과 소비재, 기타 제조업을 포함한 다양한 산업의 인상적인 고객 목록을 구축했으며, 해당 고객 기반은 시간이 지남에 따라 제품 기능을 추가하여 계속 성장할 것입니다.\n\n기술 발전과 광범위한 노동시장 역동성은 산업 전반에 걸쳐 로봇 플랫폼의 대규모 채택 기회를 열어두었다고 믿습니다. AVP 팀은 GrayMatter Robotics에 백업하고 Ariyan, Brual, SK와 함께 제조업체에 장기간 자동화 요구를 해결할 수 있는 AI 기반 로봇 솔루션을 제공합니다.\n\nGrayMatter Robotics 또는 AVP 웹사이트를 방문하여 자세한 정보를 확인해주세요. 로봇 투자에 대해 더 심층적으로 논의하고 싶다면 언제든지 AVP 팀에 연락해주세요!","ogImage":{"url":"/assets/img/2024-06-23-AVPsInvestmentinGrayMatterRobotics_0.png"},"coverImage":"/assets/img/2024-06-23-AVPsInvestmentinGrayMatterRobotics_0.png","tag":["Tech"],"readingTime":3},{"title":"차동 구동 로봇을 위한 Wheel Odometry 모델 분석","description":"","date":"2024-06-23 18:35","slug":"2024-06-23-WheelOdometryModelforDifferentialDriveRobotics","content":"\n\n휠 오도메트리란 바퀴의 움직임과 위치를 추정하는 것을 의미합니다. 이는 회전 엔코더(즉, 바퀴의 모터에 부착되어 회전을 측정하는 센서)를 사용하여 이루어집니다. 바퀴로봇이나 자율 주행 차량의 위치 추정에 유용한 기술입니다.\n\n이 글에서는 바퀴 오도메트리 모델에 대해 깊이 다루어 보겠습니다.\n\n# 시나리오 정의\n\n바퀴로봇 기술의 세계는 복잡하고 광활합니다. 마찬가지로, 회전 엔코더 센서에는 다양한 옵션이 있습니다. 따라서 휠 오도메트리 모델을 만들기 전에 우리가 모델링할 시나리오를 정의해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 두 개의 바퀴로 이루어진 로봇에 초점을 맞출 것입니다. 우리의 모델에서 로봇의 모양과 실루엣은 중요하지 않습니다. 로봇을 공간에서 한 점으로 취급할 것이기 때문에 이렇게 간단히 모델링할 수 있습니다. 다이어그램에서 우리는 로봇을 간단히 사각형으로 표현할 것입니다. 우리의 로봇에 강제할 유일한 물리적 제약은 두 바퀴가 평행하다는 것입니다. 두 바퀴 사이에 동일한 거리에 위치한 기준점을 정의할 것입니다. 이는 모델의 동력을 간단하게 만들어 줄 것입니다. 또한, 시각화를 위해 로봇의 전방 방향은 삼각형 모양으로 나타낼 것입니다. 따라서, 우리 다이어그램에서 로봇은 다음과 같이 보일 것입니다:\n\n<img src=\"/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_0.png\" />\n\n이 기사에서는 차동 구동 로봇에 초점을 맞출 것입니다. 차동 구동이란 각각의 바퀴가 독립된 모터를 가지고 있고 서로 독립적으로 작동할 수 있다는 것을 의미합니다. 차동 구동 로봇에서 각 바퀴를 제어하는 모터는 별도로 있으며 각 모터는 서로 다른 속도와 방향(즉, 전진 또는 후진)으로 바퀴를 회전시킬 수 있습니다. 두 바퀴의 속도에 따라 다른 로봇 동작을 볼 수 있습니다. 두 바퀴 모터가 동일한 속도로 동일한 방향으로 움직이면, 로봇은 해당 방향으로 직진할 것입니다 (예: 두 모터가 동일한 속도로 전진 회전 중이면, 로봇은 직진 경로로 전진할 것입니다). 한쪽 바퀴 모터의 속도가 더 빠르면 로봇은 더 빠른 모터 쪽의 반대 방향으로 회전할 것입니다 (예: 오른쪽 모터가 왼쪽 보다 더 빠르게 후진 회전 중이면, 로봇은 후진하고 왼쪽으로 회전할 것입니다).\n\n<img src=\"/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_1.png\" />\n\n<div class=\"content-ad\"></div>\n\n로봇이 앞으로도 뒤로도 움직일 수 있고, 좌우 방향은 로봇의 방향에 따라 반전될 수 있으므로 로봇의 방향 움직임을 논의하는 것이 혼란스러울 수 있습니다. 이 잠재적인 혼동을 해소하기 위해 우리는 컴퍼스가 참 북쪽을 가지고 있는 것처럼, 방향을 절대적인 용어로 정의할 것입니다. 이 글에서는 로봇과의 방향을 상대로 설명할 때 고정된 기준 프레임으로 설정해, 로봇의 전방이 항상 앞쪽을 가리키도록 합니다. 이렇게 설정하면 로봇의 방향에 상관없이 앞, 뒤, 좌, 우가 항상 로봇의 전면을 기준으로 동일하게 유지됩니다. 위 다이어그램에서 보듯, 로봇의 방향이 어디를 향하고 있건, 방향은 항상 전방이 향하고 있는 곳을 기준으로 조정됩니다.\n\n우리 오도메트리 모델의 데이터/측정은 로터리 엔코더에서 나옵니다. 일반적으로 로터리 엔코더는 모터에 부착되어 회전에 대한 데이터를 수집합니다. 이 상황에서는 왼쪽 바퀴의 모터에 부착된 하나와 오른쪽 바퀴의 모터에 부착된 다른 하나, 즉 두 개의 로터리 엔코더가 있습니다. 그리고 이러한 로터리 엔코더의 속성을 이용해 각 바퀴가 이동한 거리 등의 정보를 결정할 수 있습니다. 로터리 엔코더가 어떻게 작동하는지 보여주기 위해 예제에 중점을 둘 것입니다.\n\n이 글에서 중점을 둘 로터리 엔코더는 증분식 광학 엔코더입니다. 증분식 광학 엔코더는 발광 다이오드(LED), 슬릿이 있는 디스크, 그리고 포토 센서가 있는 회로를 활용하는 엔코더로, 디스크는 LED와 포토 센서가 있는 회로를 슬릿으로 분리합니다. 모터가 회전할 때, 디스크는 LED에서 빛을 슬릿을 통해 포토 센서로 보내는 방식으로 회전하며, 이는 회로의 전압을 변화시킵니다. 전압이 변하는 횟수는 슬릿을 통과한 횟수에 해당하며, 이는 회전 각도에 대한 정보를 제공합니다 (각 슬릿은 일정량의 회전에 대응됩니다). 이는 각 측정마다 이전 시간 단계에서 얼마나 회전했는지를 알게 해주는 증분식 엔코더가 될 것인데, 이는 절대식 엔코더와 대조적입니다. 절대식 엔코더는 각 측정마다 모터의 정확한 방향이 결정됩니다.\n\n![다이어그램](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_2.png)\n\n<div class=\"content-ad\"></div>\n\n회전 데이터와 함께 엔코더의 정보, 예를 들어 반지름이나 둘레와 같은 정보를 함께 사용하여 바퀴가 이동한 거리를 추정할 수 있습니다. 각 홈은 회전 각도를 나타내며, 특정 시간 간격 사이의 회전량을 알 수 있도록 통과한 홈의 수를 알면 됩니다. 광학 엔코더의 경우, 모든 홈이 동일한 간격으로 배치되어 있으므로, 통과한 홈의 수를 단일 홈이 나타내는 회전량으로 곱하면 시간 간격 사이의 총 회전 각도를 얻을 수 있습니다. 회전 각도를 결정한 후에는 엔코더의 둘레와 곱하여 바퀴가 이동한 거리를 구할 수 있습니다.\n\n저희의 측정 모델은 사용하는 엔코더에 의존하지 않습니다. 사실, 거리를 결정할 수 있다면 어떤 유형의 엔코더도 작동할 것입니다. 점진적 광학 엔코더의 경우, 엔코더 (즉, 그 차원)의 특성과 함께 수집된 회전 데이터를 사용하여 엔코더 측정치를 거리로 변환할 수 있습니다. 저희는 이 문서에 직관적인 성질을 갖고 있는 점진적 광학 엔코더를 선택했습니다. 그러나 다른 유형의 엔코더에서 거리를 추출할 수도 있지만, 절차가 다를 수 있고, 이러한 엔코더는 이 문서의 측정 모델에 적합할 것입니다.\n\n요약하면, 이 문서의 바퀴 측정 모델은 두 바퀴가 평행하게 배치되고 로봇이 두 바퀴 중간에 위치한 단일 참조점으로 나타내는 차동 구동 로봇을 위한 것입니다. 로봇을 점으로 취급하고 로봇의 물리적 특성을 무시하며 로봇의 전방 방향과 관련하여 방향을 정의할 것입니다. 엔코더의 경우, 각 시간 간격마다 바퀴가 이동한 거리를 추출할 수 있다면 어떤 것이든 작동할 것입니다. 엔코더의 직관을 구축하기 위해 임의적으로 점진적 광학 엔코더를 살펴봤습니다.\n\n# 바퀴 측정 모델\n\n<div class=\"content-ad\"></div>\n\n저희 오도메트리 모델의 목표는 로봇의 위치와 방향을 추정하는 것입니다. 이를 달성하기 위해 회전 엔코더에서 얻는 데이터, 로봇의 치수, 그리고 기하학을 활용할 것입니다. 이전에 설명한대로, 엔코더는 각 바퀴가 각 시간 단계에서 이동한 거리에 대한 정보를 제공할 것입니다. 로봇의 치수를 사용할 때, 로봇을 한 점으로 표현하기 때문에 많이 필요하지 않습니다. 필요한 유일한 치수는 좌우 바퀴로부터 점까지의 거리입니다. 두 바퀴 사이의 거리를 절반으로 나누어 점이 두 바퀴로부터 동일한 거리에 위치하도록 하였으므로, 하나의 숫자만 추적하면 됩니다.\n\n이제 이러한 아이디어를 추적하기 위해 몇 가지 변수를 정의해 봅시다:\n\n![image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_3.png)\n\n첫 두 변수는 특정 시간 단계에서 각 바퀴가 이동한 거리를 대응합니다. 이 정보는 회전 엔코더에서 얻을 것입니다. 세 번째 변수는 두 바퀴 사이의 거리를 측정하고, 점이 두 바퀴로부터 동일한 거리에 위치하므로 해당 거리를 반으로 나누어 파생될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_4](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_4.png)\n\n로봇의 운동 모델을 정의해 봅시다. 로봇의 운동은 항상 어느 호를 따라 이동한다고 가정합니다. 수학적으로, 이것은 어떤 반지름의 원 위를 이동하는 것을 의미합니다.\n\n![2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_5](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_5.png)\n\n원 위의 곡선으로 운동을 모델링하는 동기는 거리와 방향 각도를 해결하기 위해 다양한 기하학적 속성을 사용할 수 있기 때문입니다.\n\n\n<div class=\"content-ad\"></div>\n\n위 다이어그램에서 모델은 왼쪽으로 전진하는 모습을 보여줍니다. 그럼 직진, 오른쪽, 그리고 후진은 어떨까요? 다행히도, 우리 모델은 여전히 유효합니다 - 그 이유를 살펴봅시다.\n\n우리 모델에서 직진은 매우 작은 각도와/또는 매우 큰 반지름을 가진 곡선에 해당할 것입니다. 각도가 더 작아지거나 반지름이 증가함에 따라 곡선의 곡률이 감소하고 곡선은 더 평평해집니다. 매우 작은 각도와/또는 큰 반지름에서는 그 곡선이 직선처럼 보일 것입니다. 따라서 이 모델로 직진 운동을 포괄할 수 있습니다.\n\n오른쪽 운동의 경우, 위 다이어그램을 수평으로 뒤집어 모델링할 수 있다는 점에 주목해보세요. 이는 대칭성을 보여줍니다 - 실제로, 뒤집힌 버전을 유도한다면, 왼쪽 바퀴에 관련된 변수/값이 오른쪽 바퀴에 관련된 변수/값으로 뒤바뀌지만 동일한 방정식을 얻을 것입니다. 결과적으로, 모델에서의 방향성 추정값은 부호가 뒤바뀔 것이지만 (즉, 양수에서 음수로, 음수에서 양수로), 거리 추정값은 동일하게 유지될 것입니다. 각도는 양 방향으로 정의되며 부호 뒤집파와 유사한 대칭성을 나타내므로 실제로 이 모델로 오른쪽 운동을 포괄할 수 있습니다.\n\n후진 운동을 포함하는 논의는 비슷한 형태입니다. 후진 운동은 단순히 음의 방향으로 이동하는 거리입니다. 따라서 우리는 음의 거리값을 통해 후진 운동을 포괄할 수 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n지금 이렇게 곡선/호를 사용하여 로봇 움직임을 포착하는 것이 합리적임을 확인했으니, 이제 모델 뒤에 있는 기하학을 살펴봅시다. \n\n첫 번째 기하학적 개념은 각도에 대한 단위입니다. 각도는 일반적으로 도(degrees) 또는 래디언(radians)으로 측정됩니다. 도의 경우, 원을 360개의 동일한 부분으로 나누고 각 잘린 부분의 각도가 1도의 크기입니다. 래디언의 경우, 호의 길이로 정의되며, 단위 원(반지름이 1인 원)의 곡선에 대한 호의 길이와 각도를 관계짓는다 - 바로 호의 길이에 해당하는 각도를 나타내는 1 래디안입니다.\n\n래디언과 각도 간 변환하는 공식과 표가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_6.png)\n\n삼각함수 및 기하학 속성 정의에는 각도의 단위로 라디안과 도를 모두 사용할 수 있습니다. 이 글에서는 라디안과 도를 모두 활용할 것입니다. 각도의 기본 단위는 라디안이며, 다르게 명시되지 않는 한 항상 라디안이 사용됩니다.\n\n이제 각도의 단위에 대한 우리의 맥락을 알게 되었으므로, 휠 오도메트리 모델에서 핵심 공식 중 하나는 호의 길이 공식(라디안 사용)일 것입니다:\n\n![image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_7.png)\n\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_8.png)\n\nThe last few geometric ideas we’ll state are:\n\n- The sum of all angles on a straight line is 180°\n\n![Image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_9.png)\n\n\n<div class=\"content-ad\"></div>\n\n- 삼각형의 모든 각의 합은 180°입니다\n\n![Triangle](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_10.png)\n\n- 원에 대한 접선은 원과 접촉점에서 수직이며(즉, 90° 각도), 접선과 현의 각도는 현이 만드는 호의 각도의 절반입니다\n\n![Circle](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_11.png)\n\n<div class=\"content-ad\"></div>\n\n지금은 우리의 오도메트리 모델을 알려진 변수와 관심 변수로 주석 달기를 시작해 봅시다. 혼잡을 피하기 위해 현재 시간 아래 첨자를 삭제하고 오도메트리 모델의 핵심 관계를 찾아가는 동안 작업해 보겠습니다. 나중에는 흐름 분석에 중요해지는 변수들로 알아봅시다.\n\n![이미지1](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_12.png)\n\n![이미지2](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_13.png)\n\n첫 세 가지 변수는 직접 측정할 수 있습니다(첫 두 변수는 엔코더를 사용하고 세 번째 변수는 자를 이용할 수 있습니다). 마지막 세 변수는 직접 측정할 수 없지만, 대신 이러한 변수들을 측정 가능한 양과 관련시키기 위해 기하학을 사용해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n먼저 호 길이 공식을 사용하여 시작해 보겠습니다. 왼쪽 바퀴, 오른쪽 바퀴 및 참조점의 경로는 호입니다. 이들은 모두 같은 각도를 공유하며, 각각의 반지름은 참조점을 포함하는 곡선의 반지름과 참조점과 바퀴 사이의 거리와 관련하여 표현할 수 있습니다.\n\n![image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_14.png)\n\n이제 소거법을 사용하여 방정식 체계를 풀어 회전 각도의 변화를 해결해 봅시다.\n\n- 곱셈 분배\n\n<div class=\"content-ad\"></div>\n\n<table>\n<tr>\n<td><img src=\"/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_15.png\" /></td>\n</tr>\n</table>\n\n- 좌측 바퀴 거리 방정식의 양쪽을 음수로 곱하기\n\n<table>\n<tr>\n<td><img src=\"/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_16.png\" /></td>\n</tr>\n</table>\n\n- 변수 소거와 대수학\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_17.png\" />\n\n그래서, 우리는 측정 가능한 양에 대한 회전 각도의 변화를 해결할 수 있었고 다음의 관계를 얻었습니다:\n\n<img src=\"/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_18.png\" />\n\n이제 방정식을 재배열하고 우리가 알고 있는 것을 대입하여 참조점을 포함하는 곡선의 반지름을 구해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n- 참조점을 포함하는 곡선의 반지름이 한 쪽에 있는 방정식을 재배열하세요\n\n![그림](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_19.png)\n\n- 우리는 방정식을 뒤집어서 원하는 해를 풀고자 하는 관심 대상의 양이 자연스럽게 읽히도록 오른쪽에 오도록 할 것입니다\n\n![그림](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_20.png)\n\n<div class=\"content-ad\"></div>\n\n곡선의 반지름을 측정 가능한 양의 관점에서 구했습니다. 이제 참조점이 이동한 거리로 넘어가봅시다. 우리가 한 결과를 대입하고 간단히 정리하기만 하면 됩니다.\n\n![이미지](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_21.png)\n\n측정 가능한 양의 관점에서 모든 변수를 구했습니다. 로봇의 위치와 방향에 관심을 가지고 있으므로, 주요 변수는 참조점이 이동한 거리와 회전 각도의 변화일 것입니다. 참조점이 이동한 거리는 위치를 알려주고, 회전 각도의 변화는 방향을 알려줍니다. 참조점을 포함하는 곡선의 반지름은 유도에 유용하지만 더 이상 필요하지 않습니다. 그래서 지금까지 우리 모델에서 중요한 결과는 다음과 같습니다:\n\n![이미지](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_22.png)\n\n<div class=\"content-ad\"></div>\n\n지금까지의 결과를 통해 한 시간 단계에서 다음 시간 단계로의 거리 및 방향 변화를 결정할 수 있습니다. 결과는 시간 간의 상대적인 운동을 설명합니다.\n\n그러나 로봇의 방향이나 새로운 방향을 알고 싶다면 해당 정보가 누락되어 있습니다. 우리는 이동한 거리를 알고 있지만 방향은 알지 못 합니다. 우리는 방향 각도가 얼마나 바뀌었는지 알고 있지만 새로운 방향 각도는 알 수 없습니다. 이는 우리 오도메트리 모델의 다음 부분을 동기부여할 것입니다.\n\n이동 거리의 방향을 결정하려고 시작해봅시다. 우리 모델을 단순화하기 위해 참조점이 이동한 거리를 곡선이 아닌 선으로 표현할 것입니다.\n\n<img src=\"/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_23.png\" />\n\n<div class=\"content-ad\"></div>\n\n일반적으로 휠 엔코더를 사용한 휠 오도미터에서 데이터 샘플링이 매우 높기 때문에 이러한 단순화를 할 수 있습니다. 이는 엔코더가 데이터를 매우 자주 수집할 수 있어 측정 간의 시간 창이 매우 작다는 것을 의미합니다. 시간 창이 매우 작기 때문에 각 시간 단계에서 캡처된 운동량도 매우 작을 것입니다. 모델링할 때 이는 호의 곡률이 매우 작아져 직선과 유사하다는 것을 의미합니다. 따라서 거리를 이제 직선으로 나타내는 것은 안전한 가정이며 단순화입니다.\n\n이 거리가 이동하는 각도에 관심이 있습니다.\n\n![image 1](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_24.png)\n\n![image 2](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_25.png)\n\n<div class=\"content-ad\"></div>\n\n이 각도는 삼각형의 속성, 즉 삼각형의 각이 180°가 되고, 원에 대한 접선의 성질, 즉 선분과 원이 직교하기 때문에 접하는 지점에서 각이 90°가 된다는 것을 활용하여 구할 수 있어요. 참고: 로봇 몸체를 제거하여 혼란을 줄였어요.\n\n![image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_26.png)\n\n방정식을 세우고 문제를 해결할 수 있어요. 참고: 이 각도는 도입니다(일반 대중이 각도를 더 익숙하게 다루기 때문에), 하지만 라디안으로 문제를 해결할 수도 있었어요 - 정답이 달라지지는 않았을 거예요.\n\n![image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_27.png)\n\n<div class=\"content-ad\"></div>\n\n좋아요 — 이제 거리의 각도를 이전에 해결한 변수로 풀었어요.\n\n이제 새로운 로봇의 방향을 풀기 위해 주목해봐요.\n\n![image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_28.png)\n\n이전과 마찬가지로 기하학 원리를 사용해서 새로운 방향의 각도를 풀어볼거에요. 이것이 바로 우리 다이어그램이에요 (로봇 본문은 제거해서 혼란을 줄였어요):\n\n<div class=\"content-ad\"></div>\n\nMarkdown 형식으로 테이블 태그를 변경하십시오.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_31.png)\n\n이제 직선 위의 각도들은 180°가 되어야 한다는 사실을 이용하여 문제를 해결할 수 있습니다.\n\n![이미지](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_32.png)\n\n따라서, 우리의 현재 오도메트리 모델은 다음과 같이 보입니다 (덜 중요하거나 중간 변수들을 걸러냄):\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_33.png)\n\n로봇이 얼마나 멀리 이동했는지, 이동한 각도, 회전 각도의 변화 및 서로 다른 시간 단계 간의 방향 각도를 알게 되었습니다.\n\n# 휠 오도메트리 절대 운동\n\n이전 섹션의 결과를 토대로 상대 운동(즉, 한 시간 단계에서 다음으로 이동하는 것)을 추정할 수 있습니다. 그러나 절대 운동을 설명하는 오도메트리 모델을 확장할 수 있습니다. 절대 운동에서는 로봇이 탐색하는 환경을 좌표 평면 시스템(일반적으로 x 및 y 방향으로)으로 정의할 것입니다. 이 좌표 시스템에서 로봇의 운동은 로봇의 절대 위치를 나타내는 좌표에 의해 포착될 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n방위에 대해 정의할 수 있는 것은 x-축에서의 각도입니다. 로봇이 양의 x-축 방향을 향할 때, 방위 각도는 0°입니다. 로봇이 돌아서 제1 사분면 어딘가를 향할 때 방위 각도는 0°에서 90°까지입니다. 로봇이 제2 사분면 어딘가를 향할 때 방위 각도는 90°에서 180°까지입니다. 로봇이 제3 사분면 어딘가를 향할 때 방위 각도는 180°에서 270°까지입니다. 그리고 로봇이 제4 사분면 어딘가를 향할 때 방위 각도는 270°에서 360°까지입니다. 이는 주변에 도(도 및 라디안을 가지고 있고 중앙에 사분면 레이블이 있는)을 가진 그래픽을 통해 시각화할 수 있습니다:\n\n![image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_34.png)\n\n상대적 방향과의 주요 차이점은 절대 방위가 항상 동일한 기준 프레임, 즉 고정된 좌표 평면의 양의 x-축에서의 각도에서 오는 것이라는 것입니다. 반면에 상대적 방향은 관점 / 기준 프레임에 따라 달라질 수 있습니다.\n\n지금까지 저희의 오도메트리 모델은 초기 위치가 왼쪽을 향하도록 그려져 있었습니다. 이는 현재까지 저희의 오도메트리 모델에서 로봇이 항상 제로 라디안의 절대 방위로 시작했다는 의미입니다(기존 방향을 고려할 필요가 없었습니다).\n\n<div class=\"content-ad\"></div>\n\n로봇이 다른 방향에서 시작할 때 어떻게 되는지 궁금하셨군요? 상대 운동에 대한 모든 작업은 그대로 유지되지만, 초기 방향을 고려하여 완전한 위치와 방향을 올바르게 계산할 수 있도록 조정해야 합니다.\n\n![image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_35.png)\n\n왜 초기 방향이 변경되더라도 모든 상대 운동이 여전히 유지되는지 궁금할 수 있습니다. 그 이유는 관점 때문입니다. 위 예시를 들어보겠습니다. 이미 존재하는 좌표계를 회전시켜 초기 방향을 제로 라디안(즉, x축이 로봇의 현재 방향과 평행하도록)으로 만들면 어떻게 될까요? 이전에 논의한 상대 운동에 대한 모든 작업을 적용할 수 있을 것입니다. 좌표계를 회전시키는 것은 기본적으로 아무것도 변경하지 않습니다. 단지 관점을 바꿔서 볼 뿐입니다.\n\n사실, 절대 운동 모델을 얻는 한 가지 전략은 회전하고 다시 원래 좌표계로 변환하는 등 각각의 새로운 좌표계를 계속 생성하는 것입니다. 이러한 좌표 변환 방법은 좌표계를 회전시키는 (회전) 행렬을 사용하여 더 복잡하며 고급 기술입니다.\n\n<div class=\"content-ad\"></div>\n\n그러나 이것은 기하학에 대한 멋진 관점을 제공합니다. 우리의 로봇은 일부 절대 방향으로 시작합니다. 로봇이 방향이 0 라디안이되도록 좌표계를 수정하기로 결정하면, 기존 좌표 평면을 절대 방향 각도만큼 회전해야 합니다. 이는 우리가 절대 방향을 양의 x축에서의 각도로 정의했기 때문입니다. 기본적으로 우리는 절대 방향 각도로 좌표 평면을 조정하고 있습니다. 전체 좌표 평면을 이동하는 대신, 아래에서 설명하는 대로 상대적 방향 계산에 이를 추가할 수 있습니다:\n\n![image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_36.png)\n\n이 다이어그램에서 시간 t에서의 오도메트리 모델은 이전 시간 단계의 절대 방향 각도를 추가할 것입니다. 이전 시간 단계의 방향을 추가해도 기준점의 이동 거리나 회전 각도 변경에는 영향을 미치지 않습니다. 왜냐하면 이전에 유도한 공식이 방향 각도에 의존하지 않고 (이동한 바퀴 거리에만 의존하기 때문에)입니다. 변경되는 것은 로봇의 방향입니다. 시간 단계 간에 상대적인 것에서 좌표 평면 상의 절대 방향으로 변경됩니다. 따라서 어떤 시간 단계에서도 절대 방향 각도는 다음과 같이 정의될 수 있습니다:\n\n![image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_37.png)\n\n<div class=\"content-ad\"></div>\n\n절대 운동을 다룰 때, 로봇이 각 시간 단계마다 좌표점을 가지게 됩니다. 좌표 위치가 업데이트되는 방법은 삼각함수의 특성을 사용하는 것인데, 즉 각의 코싸인이 이웃변을 가각변으로 나눈 값이고 사인이 이웃변을 빗변으로 나눈 값이라는 것입니다. 참조점이 이동한 거리를 빗변으로 하고 이전 시간 단계의 방향 각도 및 이동으로 인한 각도를 더하여 x와 y 방향으로 이동한 거리를 계산할 수 있습니다.\n\n![image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_38.png)\n\n이전 시간 단계의 좌표에 x와 y 거리를 더함으로써 로봇의 새로운 좌표 위치를 결정할 수 있습니다. 이러한 방정식을 사용하여 역학을 설명할 수 있습니다:\n\n![image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_39.png)\n\n<div class=\"content-ad\"></div>\n\n현재 시간의 위치는 이전 시간 단계의 위치와 각도에 따라 결정되며 현재 시간 단계에서 회전 각도의 변화에 기반합니다. 하나의 방정식에서 두 가지 다른 시간 단계의 양을 사용하는 것을 감안하면 차이를 명확히 하기 위해 다시 시간 첨자를 사용하는 것이 좋습니다.\n\n# 결론\n\n우리의 인코더는 각 바퀴가 이동한 거리를 수집하고, 바퀴와 참조점 사이의 거리를 측정할 수 있습니다. 호의 길이 공식을 사용하여, 시스템의 방정식을 얻었고, 우리는 거리 이동 및 시간 단계 사이의 회전 각도 변화(라디안)를 구하기 위해 이를 해결했습니다.\n\n![image](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_40.png)\n\n<div class=\"content-ad\"></div>\n\n우리는 휠 인코더로 데이터를 빈번하게 수집하기 때문에 우리의 이동 거리를 호선 대신 선으로 나타낼 수 있다는 것을 깨달았어요. 높은 데이터 수집 빈도로 인해 곡선이 더 직선처럼 동작하고 보이게 됩니다. 그런 다음, 각도의 기하학을 사용하여 우리는 움직임에 의해 발생하는 방향 각도(라디안 단위)와 최종적인 상대적 방향을 찾았어요.\n\n그 후에, 우리는 절대 위치 및 방향에 대한 모델을 확장했어요. 여기서 우리는 정의된 좌표 평면이 있는 절대 시스템에서 우리의 오도메트리 모델을 조정해야 해요. 절대 시스템에서는 이전 시간 단계의 절대 방향 각도를 고려하여 오도메트리 모델을 조정해야 해요. 그런 다음, 우리는 삼각함수 관계식을 사용하여 새로운 절대 방향 각도(라디안)와 좌표 위치를 결정할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n절대 운동에 대해, x 및 y 구성 요소로 이뤄진 좌표 위치와 절대 방향 각도(라디안 단위)가 있습니다. 절대 운동에 대한 오도메트리 모델을 정의하는 세 가지 방정식은 다음과 같습니다:\n\n![equation 1](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_43.png)\n\n또한 일반적으로 방정식을 벡터 형태로 표현하는 것이 일반적입니다:\n\n![equation 2](/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_44.png)\n\n<div class=\"content-ad\"></div>\n\n이 글에서 우리는 두 바퀴 차동 구동 로봇을 위한 오도메트리 모델을 개발했습니다. 이 모델을 사용하면 로터리 엔코더에서 수집한 데이터를 사용하여 위치와 방향을 추적할 수 있습니다. 로터리 엔코더는 매우 저렴하고 데이터 샘플링이 높기 때문에 휠 로보틱스에서 센서로 자주 사용됩니다. 그러나 엔코더를 사용하는 데 있어서의 한 가지 어려움은 잡음과 측정 오차입니다. 우리 모델을 계속 개발하려면 다음 단계로 우리의 측정 값과 함께 통계적 불확실성과 오차를 고려하는 것이 좋은 방향입니다.","ogImage":{"url":"/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_0.png"},"coverImage":"/assets/img/2024-06-23-WheelOdometryModelforDifferentialDriveRobotics_0.png","tag":["Tech"],"readingTime":16},{"title":"로봇 공학 적용의 시대","description":"","date":"2024-06-23 18:32","slug":"2024-06-23-Theageofappliedrobotics","content":"\n\n![Amazon Sparrow](/assets/img/2024-06-23-Theageofappliedrobotics_0.png)\n\n안녕하세요! 아마존이 새로운 창고 로봇인 Amazon Sparrow를 발표했어요. 이 로봇은 패커들의 역할을 대체하기 위해 디자인된 것인데, 패커들은 주로 피커들이 가져온 아이템을 패킹하는 일을 하는 분들이죠. 피커들 자체가 로봇들에 의해 대체되고 있어요. 로봇들은 창고 외곽의 위치에 해당하는 선반을 가져다놓는 로봇인데요.\n\nSparrow는 창고에서 두 가지 가장 중요한 기능을 자동화하는 것을 가능하게 했어요. 제품을 이동시키는 것과 출고 배포를 위해 제품을 포장하는 것인데요. 제품의 이동은 오랫동안 선반을 들어들고 이동시키는 로봇에 의해 수행되어 왔는데, 이로 인해 인간들이 접근할 수 없는 미래적인 환경들이 만들어졌어요.\n\n그리고 다루고 포장하는 일은 매우 복잡했어요. 왜냐하면 매우 다양한 치수, 재료 등을 다루기 위해서 적응적 능력이 필요했기 때문이죠. 그러므로 그 기술적 요소들을 제어할 수 있는 로봇 팔을 디자인하는 방법은 무엇인가요? 그에 대한 답은 하나뿐이에요: 기계 학습이죠. 창고의 모든 제품에 대한 참조가 있는 거대한 카탈로그에서 물체를 인식할 수 있는 알고리즘을 사용하여, 제품의 특성에 따라 올바른 흡입컵, 부품 및 필요한 힘으로 제품을 조작할 수 있게 되는 거죠.\n\n<div class=\"content-ad\"></div>\n\n기계 학습은 로봇 공학을 새로운 수준으로 끌어올리고 있습니다. 동시에 전 세계적으로 수백만 개의 직업이 위험에 처해 있다는 것도 사실입니다. 아마존은 이에 대해 걱정할 필요가 없다고 말하며, 자사의 역사를 통해 자동화를 지속해왔고 거대한 기계화된 군대를 구축하면서 동시에 사람을 고용해왔다는 주장을 펼치고 있습니다(팬데믹 이후를 제외하고). 동시에, 우리는 인간들이 더 이상 편안하게 수행하기를 원치 않는 작업들의 기계화에 대해 이야기하고 있으며, 이로 인해 불만족한 직원들이 자신의 급여와 근무 조건에 반발하기도 합니다.\n\n최근 연구에 따르면 로봇화가 일자리에 미치는 위협은 우려할 필요가 없을 뿐만 아니라, 사실은 인간에게 더 큰 부가 가치를 제공하는 다른 유형의 직업에 대한 필요로 상쇄될 수도 있다고 합니다. 실제로, 우리는 아마존이 정의한 '응용 로봇공학 시대'에 진입하게 됩니다: 반복적인 작업만 수행하는 것이 아니라 다양한 가능성을 선택할 수 있는 로봇들이 등장함으로써 훨씬 더 발전된 자동화가 이루어지고 있습니다.\n\n앞으로 더 많은 공장들이 로봇을 이용하여 인간이 해서는 안 되는 더러운, 위험한, 단조롭고, 멸시받는 작업에 대해 책임하게 될 것인가요? 고급 자동화는 노동의 존엄성을 높일 것인가요, 아니면 많은 직업의 상실과 더 심화된 불평등을 야기할 것인가요? 아무튼, 로보틱스와 기계 학습을 결합한 고급 자동화는 경쟁력의 핵심 요소이며, 따라서 결과에 관계없이 필요에 의해 채택될 것입니다. 그렇다면 우리가 이에 대해 탐구하고 가능성을 이해할수록 좋을 것입니다.","ogImage":{"url":"/assets/img/2024-06-23-Theageofappliedrobotics_0.png"},"coverImage":"/assets/img/2024-06-23-Theageofappliedrobotics_0.png","tag":["Tech"],"readingTime":2},{"title":"소설에서 영화까지 블레이드 러너의 모든 것","description":"","date":"2024-06-23 18:31","slug":"2024-06-23-BooktoLifeBladeRunner","content":"\n\n<img src=\"/assets/img/2024-06-23-BooktoLifeBladeRunner_0.png\" />\n\n영화 전공 학생으로서, 내가 본 영화의 수가 상대적으로 적다는 비판을 받아왔어. 이번 여름에는 이를 바꾸기 위해 영화/TV 시청과 독서를 일상에 편입할 예정이야. 수업으로 인한 바쁜 시간이 없기 때문에 평론을 쓰면서 더 많이 배우고 발견할 수 있을 거라 믿어.\n\n어릴 적에는 판타지, 신화, 미스터리 책에 몰입했었어. 이런 장르들이 내 창의력과 문제 해결 능력에 크게 기여한 것 같아. 오랜 기간 동안 내 맥락 형성에 이른 나의 메이어스-브릭스 성격 유형은 ENFJ였는데, 이는 이러한 초기 영향들 때문이라고 생각해.\n\n내가 성숙해지면서, 흥미는 디스토피아와 과학 소설로 옮겼어. 이제 “1984”, “Dune”, “Blade Runner”와 같은 책들에서 삶의 교훈을 찾고 있어.\n\n<div class=\"content-ad\"></div>\n\nDenis Villeneuve의 \"블레이드 러너 2049\"는 아직 보지 않았지만, 다가오는 몇 주 안에는 계획 중에 있습니다. Philip K. Dick의 공상 과학 소설 \"블레이드 러너: 안드로이드는 전기양을 꿈꾸는가?\"는 특히 정신적으로 자극적이었습니다. 과학 소설을 읽을 때, 세계를 생생하게 상상하며, 단 3일 안에 완독한 후 완전히 몰입했어요. 이 소설은 250쪽 미만으로 짧지만 의미가 풍부하여 결말은 예측 가능하지만 만족스럽다고 느꼈어요. 약 한 달 전에 완독한 \"1984\" 이후 처음으로 완독한 책이에요.\n\n지난 추수감사절에는 친구와 신앙과 생물학의 공존에 대해 3시간 간의 논쟁 끝에(나는 회의론자였고, 그는 지지자였어요), 인간이 무엇인지에 대해 고찰하기 시작했어요. 인공지능과 로봇이 주도하는 새로운 시대에 접어들면서, 이러한 주제에 더 깊이 고민하고 있어요. 졸업을 앞둔 지금, 현재 일하고 있는 스타트업은 전통적인 소상공인이 현금 흐름 문제를 피할 수 있도록 중점을 두고 있는데, 이는 인공지능과 로봇의 직접적인 영향에서 나를 보호해 줄 거라고 믿어요. 스타트업 여정은 길지만, 회사 내에서 배우게 될 교훈들에 설레고 있어요.\n\n7~8년 후, 회사를 떠날 때 깊은 기술 기반 회사를 창업하고 싶어요. \"블레이드 러너: 안드로이드는 전기양을 꿈꾸는가?\"의 세계에서 영감을 받은 깊은 과학 기술 회사를 시작하고 싶어 해요. Elon Musk와 같은 선구자적인 창업자들을 존경하는데, 그들은 공상 과학에 집착하고 있어요. Musk가 경고하는 AI의 발전은 블레이드 러너 이야기에서 명확하게 표현되고 있어요. 인간들이 화성에 콜로니를 창설하기 전에 넥서스-6와 유사한 안드로이드를 개발할 수도 있다고 믿어요. Musk의 Optimus - Gen 2는 이미 테슬라 공장에서 많은 노동을 대체할 수 있는 흥미로운 작업들을 하고 있어요.\n\nPhilip K. Dick이 창조한 세계는 매혹적입니다. 이 디스토피아에서는 세계 전쟁 종말(WWT)로 인해 대부분의 생명체가 위험에 처하거나 멸종됐습니다. Kipple, 방사능 물질과 먼지로 지구를 덮어 애완동물이 귀한 보물이 되었어요. 많은 사람들은 일정한 사회적 지위를 투영하기 위해 전자 동물을 소유하게 되었어요.\n\n<div class=\"content-ad\"></div>\n\n이 과학 소설 우주에서 Dick이 상상하는 것들:\n\n- 자동차를 없애기 위해 레이저 건을 든 현상금 사냥꾼들\n- 사용자 제어 키보드를 통해 감정 상태를 수정하는 Penfield 감정 장기\n- 먼지로 가득한 이 세계에서 전통적인 도로가 쓸모 없어서 비행 자동차\n- 로켓을 이용한 교통수단으로, 시애틀에서 LA까지 1시간 만에 도착\n- 물론 안드로이드들\n\n저는 실생활에서 경찰관들이 레이저를 더 자주 사용하지 않는 이유를 의문합니다. 일부 파장은 인간 눈에 보이지 않기 때문입니다. 레이저 스타트업에서 일한 경험으로 그런 환경에서 안전 고글의 중요성을 알고 있습니다. 또한 비행 자동차가 과학 소설의 상징이지만 실용적인 구현은 아직 멀어보입니다. 규제와 개발 비용만 놓고 봐도 전통적인 차량에 비해 비행 자동차는 실용적인 투자가 아닙니다.\n\nPenfield 감정 장기는 흥미롭게 느껴지지만, 인간 감정 조작에 대한 제 도덕적 입장과 상충됩니다. 정신 질환을 가진 사람들에게 도움이 될 수 있지만 소설에서는 Rick의 우울증을 앓는 아내에게 효과적이지 않다고 나옵니다. 이것은 정신 질환을 치료할 수 없다는 것을 시사할 수 있으며, 기계는 일시적인 안위만 제공합니다. 경제적인 관점에서는 그런 장치의 퇴보하는 한계 효용에 대해 의문을 품고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n로켓 기반 교통 수단은 현실적이면서도 현실적이지 않습니다. 긴급 여행에 유용할 수 있지만, 필요한 훈련과 비용은 일등급 비행기 표와 비교했을 때 비싼 만큼 실용적이지 않습니다. 파이콘 9 좌석에 드는 5500만 달러의 우주 관광 비용(파이콘 9의 페이로드 당 1,200달러)은 이러한 모험의 금전적으로 방해되는 비용을 강조합니다.\n\n안드로이드는 매혹적이면서 무서운 요소도 있어, 로봇공학이랑 인간의 윤리적 질문을 자극합니다. Rick이 Rachel Rosen과 \"잠을 자는\" 것이 사기인가요? 의식 있는 안드로이드를 죽이는 것은 불법인가요? 소설 제목이 시사하는 것처럼, 안드로이드는 꿈을 꾸는 걸까요? 이런 질문들은 이 새로운 산업시대에 접어들기 전에 답할 필요가 있습니다, 세계전쟁 황폐한 경계선이 먼저 일어나는 경우를 제외하고는요...\n\n그럼에도 불구하고, 저는 미래에 대해 낙관적입니다. 이 기술적 발전과 \"블레이드 러너\" 세계에 관련성 있는 무언가를 30대 초반쯤에 창조할 것입니다. 그러나 가장 짜증나는 것은 내가 이해하려는 개념에 대해 항공우주/기계공학자가 수업에서 배운 것을 이해하기 위해 10개의 질문을 해야 한다는 지식 장벽입니다.\n\n무언가를 만들고 싶다면, 가장 똑똑한 사람들을 주변에 둘러싸고, 위대한 일을 이루기 위해 그들을 하나로 모으는 단일한 비전을 가져야 합니다.","ogImage":{"url":"/assets/img/2024-06-23-BooktoLifeBladeRunner_0.png"},"coverImage":"/assets/img/2024-06-23-BooktoLifeBladeRunner_0.png","tag":["Tech"],"readingTime":3},{"title":"Gradient Descent 알고리즘과 맞춤형 웨이포인트로 AWS DeepRacer 성능 향상시키는 방법","description":"","date":"2024-06-23 18:29","slug":"2024-06-23-EnhancingYourAWSDeepRacerPerformancewithGradientDescentAlgorithmandPersonalizedWaypoints","content":"\n\n# AWS DeepRacer에 대해\n\nAWS DeepRacer는 Amazon Web Services (AWS)의 흥미로운 프로젝트로, 자율 주행 레이싱의 즐거움과 강화 학습 (RL)의 힘을 시원하게 결합한 것이죠. DeepRacer의 핵심은 RL 분야를 분석하고 민주화하는 데 중점을 둔 포괄적이고 인터랙티브한 플랫폼입니다. 이를 통해 숙련된 개발자부터 초보자까지 RL 분야에 대한 접근성을 높일 수 있습니다.\n\n# 커뮤니티 레이스\n\nDeepRacer 커뮤니티 레이스는 AWS DeepRacer 생태계 내에서 참여자들 간의 커뮤니티와 경쟁 의식을 촉진하기 위해 디자인된 매력적이고 협업적인 프로그램입니다. 이 흥미진진한 프로젝트는 모든 수준의 참가자들을 초대하여 그들의 AI 레이싱 스킬을 겨룰 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\n![DeepRacer Performance](/assets/img/2024-06-23-EnhancingYourAWSDeepRacerPerformancewithGradientDescentAlgorithmandPersonalizedWaypoints_0.png)\n\n우리 회사의 DeepRacer 경주에 참여한 기회를 가졌는데, 이 경험은 즐겁고 교육적이었습니다. 이 여정을 통해 강화 학습의 세계에 심취하며 자율 주행 레이싱 카를 훈련하는 데 중추적인 역할을 하는 보상 함수(RF)에 대한 깊은 이해를 얻을 수 있었습니다.\n\n저는 경사하강 알고리즘과 코딩을 통해 3바퀴를 24.58초로 완주한 경험을 공유할 예정입니다.\n\n![DeepRacer GIF](https://miro.medium.com/v2/resize:fit:1200/1*7vcRCVvk4TG3Hy_9AzB9KQ.gif)\n\n<div class=\"content-ad\"></div>\n\n# 최적 경로 계획\n\nAWS Invent:2018 트랙 레이스를 위한 나의 모델 훈련은 핵심 작업인 최적 레이싱 경로 계획으로 시작되었습니다. 이 노력에서는 GitHub Repository에서 제시된 방법론에서 영감을 받았습니다. 이 방법론은 경사 하강 알고리즘의 힘을 이용하고 있습니다. 먼저, 저는 아래와 같이 빨간색으로 표시된 상한선과 하한선을 설정하는 데 집중했습니다:\n\n![그림](/assets/img/2024-06-23-EnhancingYourAWSDeepRacerPerformancewithGradientDescentAlgorithmandPersonalizedWaypoints_1.png)\n\n그 후 알고리즘의 목표는 자동차의 최적 경로를 계산하는 것이었습니다. 이때의 주요 목표는 트랙의 직선 구간을 최대화하는 것이었습니다. 이 전략적인 접근 방식은 급한 회전을 줄여주어 자동차가 장기간 최대 속도를 유지할 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\n다음으로, AWS에서 제공한 미리 정의된 웨이포인트를 계획된 경로 위에 겹쳤습니다.\n\n그로 인해 차량이 가속해야 하는 최적 구간과 회전을 준비하기 위해 감속해야 하는 지역을 명확히 식별할 수 있었습니다. 이 포괄적인 이해는 내 보상 함수 개발의 기초가 되었습니다:\n\n<div class=\"content-ad\"></div>\n\n아래는 경로 계획에 이어 트랙상의 각 인덱스에 적합한 속도를 결정하는 과정이 집중적으로 다루어졌습니다. 최적의 성능을 보장하기 위해 속도 범위는 1.5에서 4.0 사이로 제한했습니다. 특히, U턴 및 중요 지점 근처에서 계산된 속도가 낮아지는 것을 관찰할 수 있는데, 이는 차량이 정밀하게 이 도전적인 섹션을 운행할 수 있도록 도와줍니다.\n\n이 방식을 활용하여 차량의 속도를 다양한 트랙 구간에 맞게 섬세하게 조절할 수 있는 폭넓은 속도 값 범위를 포착했습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-EnhancingYourAWSDeepRacerPerformancewithGradientDescentAlgorithmandPersonalizedWaypoints_6.png\" />\n\n이 속도 범위는 모델 훈련 과정을 가속화하고 향상시키는 데 중요한 역할을 합니다. 당연히 이러한 범위를 조정하여 여러분의 구체적인 트랙과 목표에 맞출 수 있는 유연성이 있습니다. 이렇게 하면 성능을 더욱 향상시킬 수 있습니다.\n\n앞으로 계획된 작업 공간에 대해 자세히 살펴볼 수 있습니다. 각 행은 예상 작업을 나타내며 각 인덱스에 해당합니다 (웨이포인트와 동일함). 이 작업 공간은 [x, y, speed, time]로 표시됩니다. 훈련 중에 \"시간\" 구성 요소는 안정성이 다소 일관되지 않았기 때문에 크게 사용하지 않았습니다.\n\n이 트랙에 118개의 인덱스가 존재하지만 할당할 수 있는 작업은 단 30개뿐이므로, 속도와 작업 가능성을 더 관리할 수 있는 집합으로 압축하기 위해 클러스터링 접근 방법이 필요했습니다. 이를 위해 K-means 클러스터링 방법을 선택하여 작업을 13개의 구분된 클러스터로 효과적으로 줄였습니다. 이 접근 방법을 통해 모델이 학습하고 레이싱 결정 중에 적용할 수 있는 보다 간결하고 실용적인 작업 집합이 보장되었습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-EnhancingYourAWSDeepRacerPerformancewithGradientDescentAlgorithmandPersonalizedWaypoints_7.png)\n\n그리고 이것이 이산적인 액션들입니다:\n\n![이미지](/assets/img/2024-06-23-EnhancingYourAWSDeepRacerPerformancewithGradientDescentAlgorithmandPersonalizedWaypoints_8.png)\n\n# 모델 훈련\n\n\n<div class=\"content-ad\"></div>\n\n강화 학습을 탐험하면서, 상대적으로 새로운 개념인데도 불구하고, 그 근본적인 원리에 대해 파고들어 내 나름대로의 이해로 정리했습니다:\n\n모델 훈련 측면에서, 제 개인적인 경험은 큰 학습 속도 및 작은 배치 크기로 시작하는 것이 효과적이라는 것을 알려 주었습니다. 이 전략은 모델이 일반적인 경로와 속도 경향을 빠르게 파악하는 데 도움이 됩니다. 점진적으로, 모델이 진행됨에 따라 이러한 초매개변수를 점차 감소시키는 것을 권장하며, 더 구체적이고 정교한 훈련 요구에 적응하도록 합니다. 대부분의 훈련에 사용된 하이퍼파라미터는 다음과 같습니다:\n\n[하이퍼파라미터 테이블](/assets/img/2024-06-23-EnhancingYourAWSDeepRacerPerformancewithGradientDescentAlgorithmandPersonalizedWaypoints_9.png)\n\n총 30시간 정도의 시간을 들여, 모델을 복제하고 훈련하여 비교적 안정된 상태에 이르렀습니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-23-EnhancingYourAWSDeepRacerPerformancewithGradientDescentAlgorithmandPersonalizedWaypoints_10.png\" />\n\n열정적이고 철저한 훈련 노력 끝에, 나는 리더보드에서 선두를 차지했습니다.\n\n이 보상 함수가 효과적인 면에서 이점을 제공하지만, 간단한 보상 함수보다 수렴하는 데 더 많은 시간이 걸릴 수 있다는 점을 인지하는 것이 중요합니다. 이 요소로 인해 마지막 제출이 레이스 마감 시간을 넘어섰고 두 번째 위치를 확보한 타이밍이었습니다. 이것은 강화 학습과 레이싱의 동적 세계에서 복잡성과 수렴 속도 사이의 적절한 균형을 찾는 것이 중요한 도전임을 상기시켜줍니다. 그러나 이는 DeepRacer 여정을 풍부하게 하는 귀중한 학습 경험이기도 합니다.\n\n<img src=\"/assets/img/2024-06-23-EnhancingYourAWSDeepRacerPerformancewithGradientDescentAlgorithmandPersonalizedWaypoints_11.png\" />\n\n<div class=\"content-ad\"></div>\n\n# 데이터 분석으로 보상 기능 향상하기\n\n매 훈련 세션 이후, 훈련 로그를 다운로드하고 분석하는 과정은 여정 중 중추적인 단계가 됩니다. 저는 주로 세 가지 필수 플롯을 분석하여 가치 있는 통찰을 얻습니다:\n\n- 에피소드 진행 플롯 (왼쪽): 이 플롯은 각 에피소드에서 이루어진 진행을 시각적으로 나타내며 학습 곡선을 명확하게 보여줍니다.\n- 완주 당 총 단계 플롯 (가운데): 여기서 나는 자동차가 각 랩을 완주하기 위해 필요한 총 단계 수를 평가합니다. 이 통찰은 효율성과 전반적인 성능을 평가하는 데 중요합니다.\n- 보상 플롯 (오른쪽): 보상 그래프는 매우 중요한데, 보상 함수의 효과적인지를 시험하는 역할을 합니다. 이 그래프는 함수가 자동차가 더 짧은 걸음으로 최적의 경로를 따르도록 적절하게 장려하는지를 강조합니다.\n\n이러한 플롯들은 훈련 진행 상황에 대한 종합적인 시각을 제공하며 최적의 결과를 위해 모델 수정을 도와줍니다.\n\n<div class=\"content-ad\"></div>\n\n아래와 같이 Markdown 형식으로 수정하세요.\n\n\n![이미지](/assets/img/2024-06-23-EnhancingYourAWSDeepRacerPerformancewithGradientDescentAlgorithmandPersonalizedWaypoints_12.png)\n\n실제로 리워드 대 웨이포인트 그래프는 특정 웨이포인트를 찾아내는 데 매우 유용한 도구입니다. 이 그래프를 면밀히 살펴보면, 리워드가 낮은 웨이포인트를 식별하여 자동차가 코스를 이탈하게 하는 요인을 찾아낼 수 있습니다. 이러한 특정 통찰력은 리워드 기능을 정밀하게 조정하여 자동차의 전반적인 성능을 향상시키고, 더 일관되게 코스를 유지할 수 있도록 하는 데 도움이 됩니다.\n\n![이미지](/assets/img/2024-06-23-EnhancingYourAWSDeepRacerPerformancewithGradientDescentAlgorithmandPersonalizedWaypoints_13.png)\n\n당연히, 훈련 트랙을 시각화할 수 있는 능력은 최적 경로를 설계하는 데만 있는 것이 아니라, 훈련 과정에 대한 보다 깊은 통찰력을 얻을 수 있는 귀중한 자원입니다. \"Training_analysis.ipynb\"를 탐험하는 것을 강력히 권장합니다. 이 파일은 이러한 유용한 플롯 생성을 자동화하여 분석을 단순화하고, 모델을 더욱 개선하기 위한 데이터 기반 의사결정을 도와줍니다. DeepRacer 여정에서 귀하의 성공에 크게 기여할 수 있는 강력한 도구입니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 코드 세부 정보\n\n보상 함수는 다음과 같습니다:\n\n```js\n# 속도 범위: 1.5 - 4\nimport math\n\n\nclass Reward:\n    def __init__(self):\n        self.first_racingpoint_index = 0\n\n    def reward_function(self, params):\n\n        ################## 도우미 함수 ###################\n\n        def dist_2_points(x1, x2, y1, y2):\n            return abs(abs(x1-x2)**2 + abs(y1-y2)**2)**0.5\n\n        def closest_2_racing_points_index(racing_coords, car_coords):\n\n            # 모든 레이싱 포인트까지의 거리 계산\n            distances = []\n            for i in range(len(racing_coords)):\n                distance = dist_2_points(x1=racing_coords[i][0], x2=car_coords[0],\n                                         y1=racing_coords[i][1], y2=car_coords[1])\n                distances.append(distance)\n\n            # 가장 가까운 레이싱 포인트의 인덱스 가져오기\n            closest_index = distances.index(min(distances))\n\n            # 두 번째로 가까운 레이싱 포인트의 인덱스 가져오기\n            distances_no_closest = distances.copy()\n            distances_no_closest[closest_index] = 999\n            second_closest_index = distances_no_closest.index(\n                min(distances_no_closest))\n\n            return [closest_index, second_closest_index]\n\n        def dist_to_racing_line(closest_coords, second_closest_coords, car_coords):\n\n            # 가장 가까운 2개 레이싱 포인트 사이의 거리 계산\n            a = abs(dist_2_points(x1=closest_coords[0],\n                                  x2=second_closest_coords[0],\n                                  y1=closest_coords[1],\n                                  y2=second_closest_coords[1]))\n\n            # 차와 가장 가까운 레이싱 포인트 사이의 거리와 두 번째로 가까운 레이싱 포인트 사이의 거리\n            b = abs(dist_2_points(x1=car_coords[0],\n                                  x2=closest_coords[0],\n                                  y1=car_coords[1],\n                                  y2=closest_coords[1]))\n            c = abs(dist_2_points(x1=car_coords[0],\n                                  x2=second_closest_coords[0],\n                                  y1=car_coords[1],\n                                  y2=second_closest_coords[1]))\n\n            # 차와 레이싱 라인 사이의 거리 계산 (가장 가까운 2개의 레이싱 포인트를 통과)\n            # DeepRacer에서 드문 버그인 경우를 위해 try-except 사용\n            try:\n                distance = abs(-(a**4) + 2*(a**2)*(b**2) + 2*(a**2)*(c**2) -\n                               (b**4) + 2*(b**2)*(c**2) - (c**4))**0.5 / (2*a)\n            except:\n                distance = b\n\n            return distance\n\n        # 다음인 레이싱 포인트 및 이전 레이싱 포인트 계산\n        def next_prev_racing_point(closest_coords, second_closest_coords, car_coords, heading):\n\n            # 차를 더 많이 향하도록 설정\n            heading_vector = [math.cos(math.radians(\n                heading)), math.sin(math.radians(heading))]\n            new_car_coords = [car_coords[0]+heading_vector[0],\n                              car_coords[1]+heading_vector[1]]\n\n            # 새로운 차 좌표와 두 가장 가까운 레이싱 포인트까지의 거리 계산\n            distance_closest_coords_new = dist_2_points(x1=new_car_coords[0],\n                                                        x2=closest_coords[0],\n                                                        y1=new_car_coords[1],\n                                                        y2=closest_coords[1])\n            distance_second_closest_coords_new = dist_2_points(x1=new_car_coords[0],\n                                                               x2=second_closest_coords[0],\n                                                               y1=new_car_coords[1],\n                                                               y2=second_closest_coords[1])\n\n            if distance_closest_coords_new <= distance_second_closest_coords_new:\n                next_point_coords = closest_coords\n                prev_point_coords = second_closest_coords\n            else:\n                next_point_coords = second_closest_coords\n                prev_point_coords = closest_coords\n\n            return [next_point_coords, prev_point_coords]\n\n        def racing_direction_diff(closest_coords, second_closest_coords, car_coords, heading):\n\n            # 가장 가까운 웨이포인트를 기반으로 센터 라인의 방향 계산\n            next_point, prev_point = next_prev_racing_point(closest_coords,\n                                                            second_closest_coords,\n                                                            car_coords,\n                                                            heading)\n\n            # 방향 계산(radian 값, arctan2(dy, dx), 결과는 라디안 단위로 (-pi, pi))\n            track_direction = math.atan2(\n                next_point[1] - prev_point[1], next_point[0] - prev_point[0])\n\n            # 도수로 변환\n            track_direction = math.degrees(track_direction)\n\n            # 트랙 방향과 차량의 진행 방향 간의 차이 계산\n            direction_diff = abs(track_direction - heading)\n            if direction_diff > 180:\n                direction_diff = 360 - direction_diff\n\n            return direction_diff\n\n        #################### 레이싱 라인 ######################\n\n        # Spain 트랙을 위한 최적의 레이싱 라인\n        # 각 행: [x,y,speed,timeFromPreviousPoint]\n        racing_track = [\n            [3.06664, 0.69989, 4.0, 0.03654],\n            ...\n            [2.77639, 0.72086, 4.0, 0.03581],\n            [2.92074, 0.70874, 4.0, 0.03621]]\n\n        ...\n\nreward_object = Reward()\n\n\ndef reward_function(params):\n    return reward_object.reward_function(params)\n```\n\n<div class=\"content-ad\"></div>\n\n코드 구현의 세부 사항에 관심이 있는 분들을 위해, 제 GitHub 저장소를 살펴보실 것을 초대합니다:\n\n# 감사의 말\n\n이 모델을 교육하는 경험은 절대적으로 즐거웠으며 가치 있는 학습 여정이었습니다. 강화 학습에 대한 실용적인 통찰을 얻을 뿐만 아니라, 즐거운 레이싱 게임 측면 때문에 이를 수행하는 데 즐거움을 느꼈습니다. 이 경험을 가능하게 해 준 주최자들께 감사의 말씀을 전하고 싶습니다 (교육 시간은 싸지 않습니다)!\n\n이 GitHub 저장소를 완성하는 데 중요한 참고 자료로 사용된 두 명의 저자 dgnzlz 및 GitHub 사용자 oscarYCL의 역할에 감사의 의미를 표합니다. Capstone_AWS_DeepRacer 및 deepracer-waypoints-workshop이라는 저장소는 그들의 창의성과 기여가 귀중했습니다.\n\n<div class=\"content-ad\"></div>\n\n마지막으로, 이겪은 것은 팀워크입니다. 제 동료들과 함께 이 모델을 개발하는 과정을 즐겼습니다.\n\n# 참고\n\n- https://github.com/dgnzlz/Capstone_AWS_DeepRacer\n- https://github.com/oscarYCL/deepracer-waypoints-workshop\n- https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-reward-function-input.html","ogImage":{"url":"/assets/img/2024-06-23-EnhancingYourAWSDeepRacerPerformancewithGradientDescentAlgorithmandPersonalizedWaypoints_0.png"},"coverImage":"/assets/img/2024-06-23-EnhancingYourAWSDeepRacerPerformancewithGradientDescentAlgorithmandPersonalizedWaypoints_0.png","tag":["Tech"],"readingTime":12}],"page":"10","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true}