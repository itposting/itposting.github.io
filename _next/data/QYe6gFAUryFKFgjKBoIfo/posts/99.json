{"pageProps":{"posts":[{"title":"SQL과 No-SQL 솔루션 중 어떤 것을 선택해야 할까요","description":"","date":"2024-06-19 01:46","slug":"2024-06-19-HowIchoosebetweenSQLandNo-SQLsolutions","content":"\n\n이 기사에서는 솔루션을 선택하기 위해 SQL 및 No-SQL 데이터베이스 중 어떤 것을 선택할 것인지에 대해 설명하고 있습니다. 이 결정의 일환으로, 구조화된 및 비구조화된 데이터가 결정에 어떤 영향을 미치는지 및 기타 요소를 탐구합니다. 이것은 복잡한 결정일 수 있습니다 [소프트웨어 관리에 관한 기사].\n\n![이미지](/assets/img/2024-06-19-HowIchoosebetweenSQLandNo-SQLsolutions_0.png)\n\n한 대학 강사가 '나는 여러분을 엔지니어가 되도록 가르치러 온 게 아니에요. 엔지니어링은 돈에 관한 것이에요!'라고 말했던 것을 기억합니다.\n\n나는 내 엔지니어링 경력을 돌아보았을 때, 그가 옳았다는 것을 알 수 있었습니다. 기술에 대한 결정은 기술이 작업에 적합한지 여부만큼 비용에 대한 문제도 중요합니다. 둘 다 옳아야 결정을 내릴 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nNo-SQL 대 SQL을 결정할 때도 같은 것이 적용됩니다.\n\n일반적으로 No-SQL 대 SQL에 대한 논의는 주어진 솔루션에 사용할 데이터베이스 기술에 대한 논의입니다.\n\n우리가 해야 할 일을 제대로 처리하기 위해 ‘적합한지 여부’부터 살펴보아야 합니다. 이는 결국 데이터가 구조화되었는지 아니면 비구조화되었는지에 대한 문제로 시작됩니다.\n\n그런 다음에는 결정의 비기술적 측면을 살펴볼 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 구조화된 데이터 vs 구조화되지 않은 데이터\n\n많은 사람들이 자신의 데이터가 구조화되지 않았다는 사실에 대해 이야기한다는 것을 듣습니다. 일부 보고서에는 기업이 약 80%의 구조화되지 않은 데이터로 운영한다고 언급됩니다.\n\n구조화된 데이터에 대해 이야기하는 사람들을 듣는 것은 훨씬 적습니다. 아마도 이는 어느 수준에서든 모든 데이터가 어느 정도로든 구조화되어 있다고 생각되기 때문일 것입니다.\n\n그렇다면 구조화된 데이터 또는 구조화되지 않은 데이터가 무엇을 의미하는 걸까요?\n\n<div class=\"content-ad\"></div>\n\n우리는 구조화된 데이터인지 비구조화된 데이터인지 결정할 수 있는 두 가지 방법이 있어요:\n\n- 데이터에 대한 정보(메타데이터)를 우리가 코드를 작성하기 전에 알 수 있나요?\n- 받는 데이터가 런타임에서 형식을 변경할까요?\n\n이 두 질문에 대한 대답이 '그렇다, 아니요'일 가능성이 높아요.\n\n텍스트 페이지를 예로 들어볼게요. 이는 일반적으로 비구조화물로 분류돼요. 이게 좀 이상하게 들릴 수 있지만, 그것은 매우 잘 구조화돼 있어요. 한정된 기호 집합을 사용하여 언어로 작성되었고(일반적으로) 해당 언어의 문법 규칙을 따라 쓰여졌기 때문이죠. 이 모든 것은 우리의 비구조화된 텍스트 페이지가 매우 구조화된 메타데이터를 갖고 있다는 것을 보여줘요.\n\n<div class=\"content-ad\"></div>\n\n해당 텍스트의 실제 의미는 무엇이든 될 수 있습니다. 따라서 의미론적으로 구조화되지 않았습니다.\n\n따라서, 하나의 수준에서는 구조화된 데이터를 가지고 있지만 다른 수준에서는 구조화되지 않았습니다. 이는 대부분의 데이터에 대한 일반적인 경우입니다.\n\n실제로 구조화되지 않은 데이터는 무작위이며 적용할 수 있는 규칙이 적을 수 있습니다. 존재 및 크기에 제한이 있으며 변환할 수 있는 능력(예: 암호화)과 관련이 있습니다.\n\n다양한 기능을 갖춘 솔루션을 위해 소프트웨어는 데이터나 해당 메타데이터가 구조화되어 있다고 가정합니다. 이를 깨달았을 때, 구조화되거나 구조화되지 않은 데이터가 있는지에 대해 이야기하는 것은 중요하지 않습니다. 아마도 둘 다 가지고 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 데이터 유동성\n\n구조에 대해 이야기하는 것보다는 데이터의 유동성에 대해 이야기하는 것이 더 나을 것 같아요.\n\n당신의 데이터가 엄격한 타입 규칙에 적합하다면, 그것은 집 벽돌처럼 단단한 것으로 간주될 수 있어요. 그 구조는 설정되어 있고 변경할 수 없어요.\n\n만약 데이터가 어떤 타입 규칙에도 적합하지 않는다면, 그것은 물과 같이 유동적인 것으로 간주될 수 있어요. 그 구조는 정의되지 않았으며 사실상 무작위적이에요.\n\n<div class=\"content-ad\"></div>\n\n이제 데이터의 유동성을 살펴볼 차례입니다. 의심의 여지 없이 벽과 물 사이 어딘가에 해당될 것이며, 젤리(또는 젤로)에 더 가까울 것입니다.\n\n해결하려는 문제에 따라 데이터 유동성은 벽에 더 가깝거나 물에 더 가까울 것입니다. 이 유동성을 이해하는 것은 해당 기술을 지원하는 데 필요한 기술을 이해하는 데 도움이 될 것입니다.\n\n# 지속된 데이터\n\n이전에 No-SQL 대 SQL에 관한 결정은 실제로 데이터베이스 기술 또는 지속성 계층에 관한 결정이라고 언급했습니다. 이게 무슨 의미인지 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n만약에 구조화된 vs 비구조화된에 대해 구글링을 하면, 대부분의 설명이 데이터가 데이터베이스에 저장(또는 보존)되는 방식과 관련이 있다는 것을 알 수 있을 거에요. 이 정의를 보면 SQL 기반 데이터베이스가 구조화된 데이터(벽돌과 같은)를 저장하는 데 사용되는 반면, NoSQL 기반 데이터베이스는 비구조화된 데이터(물과 같은)를 저장하는 데 사용된다는 것을 알 수 있을 거에요.\n\n이게 어떻게 작용하는지 살펴봐요.\n\n\n![이미지](/assets/img/2024-06-19-HowIchoosebetweenSQLandNo-SQLsolutions_1.png)\n\n\n## SQL 데이터베이스의 구조화된 데이터\n\n<div class=\"content-ad\"></div>\n\n구조화된 데이터는 고정된 데이터 정의(또는 스키마)를 갖습니다. 데이터베이스는 각각의 새로운 데이터(또는 레코드)를 테이블의 행으로 저장합니다. 레코드 내의 각 필드는 행 안의 열로 저장됩니다. 각 열은 특정 유형의 데이터입니다. 이렇게하여 모든 구조화된 데이터가 테이블, 행 및 열의 고정된 3차원 그리드 내에 저장됩니다.\n\n데이터를 저장하는 데 사용되는 엄격한 구조 때문에 데이터를 생성, 읽기, 업데이트 또는 삭제하는 데는 구조화된 쿼리 언어(SQL)가 사용됩니다. 이제 코드는 데이터 및 그 구조를 처리할 때 데이터를 신뢰할 수 있습니다.\n\n## 비구조화된 데이터와 No-SQL 데이터베이스\n\n비구조화된 데이터는 정의된 스키마가 없습니다. 데이터베이스는 레코드가 무엇을 포함하거나 어떻게 보이는지에 대한 엄격한 정의가 없습니다. 완전히 중립적이며 데이터가 무엇인지는 중요하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 스스로에게 물을 수 있을 것입니다. \"구조가 없다면 데이터에 어떻게 접근할까요?\"\n\n그렇습니다. 구조화되지 않은 데이터도 어떤 방식으로든 참조되어야 합니다. 일반적으로 SQL 데이터베이스의 테이블과 행은 컬렉션과 문서로 대체됩니다.\n\n이것은 이해하기 쉽습니다. 일상 언어로, 문서에는 무엇이든 담길 수 있으며, 구조화되지 않은 데이터도 동일합니다. 컬렉션은 일정한 주제를 가진 문서들의 집합이며, 예를 들어 물고기에 관한 문서들의 컬렉션이 될 수 있습니다. 문서에는 어떤 유형의 데이터든 포함될 수 있으며, 심지어 동일한 컬렉션 내의 문서라 할지라도 다양한 유형의 데이터를 가질 수 있습니다.\n\n이제 매우 유동적인 데이터가 NoSQL 데이터베이스에 훨씬 더 적합하다는 것이 분명해졌을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 관계형 데이터베이스\n\nSQL 데이터베이스에 대해 이야기할 때, 실제로는 관계형 데이터베이스 관리 시스템(RDBMS)을 언급하는 것입니다.\n\n구조화된 데이터에서는 데이터 간에 관계가 있을 수 있습니다. 예를 들어, 어떤 물고기와 어항의 테이블이 있다면, 어떤 물고기가 어떤 어항에 있는지 알고 싶을 수 있습니다. 이는 데이터, 물고기 및 어항 간의 관계를 의미합니다.\n\n관계형 데이터베이스는 이러한 관계를 이해하고 데이터 레코드 간의 참조를 사용하여 해당 관계를 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n관계가 매우 중요하기 때문에 RDBMS는 관계들을 특별히 관리하며, 참조 무결성 규칙을 사용하여 참조가 영원히 깨지지 않도록 합니다.\n\n이러한 관계들 덕분에 SQL이 데이터베이스 전체에서 데이터를 관리할 수 있으며, 서로 다른 테이블, 행 및 열에 있는 데이터도 관리할 수 있습니다.\n\n## 비관계형 데이터베이스\n\n진정한 비구조화 데이터 세트에서는 데이터 간에 정의된 관계가 없습니다. 예를 들어 도서관의 책들을 생각해보십시오. 이들은 어떠한 방식으로도 서로 관련되어 있지 않을 수 있습니다. 그들은 단지 문서(책)의 모음(도서관)일 뿐입니다.\n\n<div class=\"content-ad\"></div>\n\n구조가 없으면 비구조화된 데이터에 액세스하려면 SQL의 대안이 필요합니다. SQL은 상당히 표준화되어 있지만, No-SQL 쿼리 언어는 기본 저장 기술에 더 의존합니다. 그들이 모두 No-SQL이라고 불리는 것은 공통점이 있습니다.\n\nNo-SQL 쿼리를 찾는 것은 도서관에서 금붕어에 대한 설명을 찾는 것과 비슷합니다.\n\nNo-SQL 데이터베이스의 데이터(문서)가 다른 문서와 관련이 없는 것으로 생각하는 것이 유혹적이지만, 그렇지 않습니다. No-SQL 데이터베이스는 관계를 지원하지만 RDBMS와 동일한 무결성으로 강제하지는 않습니다. 이것이 여러분에게 문제가 될 수도, 그렇지 않을 수도 있습니다.\n\n#일하는 데 가장 적합한 기술\n\n<div class=\"content-ad\"></div>\n\n지금까지 데이터가 일반적으로 완전히 구조화되지 않았고 완전히 구조화되지 않았다고 언급했습니다. SQL 데이터베이스는 둘 다 처리할 수 있고 No-SQL 데이터베이스도 둘 다 처리할 수 있다는 것을 설명했습니다.\n\n그렇다면 어떻게 결정을 내리는 걸까요?\n\n데이터의 유동성을 고려해야 합니다. 데이터가 더 벽돌 모양이라면 SQL 데이터베이스의 엄격한 규칙들이 도움이 될 것입니다. 데이터가 더 물 모양이라면 No-SQL 데이터베이스의 유연성에서 이점을 얻을 수 있을 것입니다.\n\n한 가지 더 고려해야 할 시나리오가 있습니다. 때로는 데이터의 구조를 아직 알 수 없지만 저장해야 하는 경우가 있을 수 있습니다. 구조를 식별하는 데 더 나아질 때까지 처리할 것입니다. 이 경우 데이터의 유동성 수준을 아직 모르기 때문에 No-SQL 데이터베이스를 사용하는 것이 더 나을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n흐음, 아마도 이미 알고 계시겠지만, 데이터가 유동적인 경우, 대부분의 데이터는 한쪽 극단에 있지 않고 중간에 위치합니다. 한쪽 끝에 더 가까울 수도 있고 그 반대에 더 가까울 수도 있으므로 적절한 경우 SQL 또는 No-SQL 데이터베이스를 선택하게 될 것입니다.\n\n하지만 다른 유형의 데이터는 어떻게 처리해야 할까요?\n\n## SQL 데이터베이스에서 비구조화된 데이터 다루기\n\n이전에 언급했듯이, SQL 데이터베이스는 일반적으로 비구조화된 데이터를 처리할 수 있습니다. 그들은 데이터의 덩어리로서 처리할 수 있습니다. 파일처럼, 덩어리는 모든 유형의 데이터를 포함할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n대체로, 테이블 형식으로 저장하는 대신 텍스트 필드로도 저장할 수 있습니다. 일반적으로 JSON 형식의 문자열로 텍스트 필드 내에 저장하는 것이 일반적입니다. JSON은 유동적인 데이터를 위한 흔한 형식입니다 (실제로 엄격한 구조를 갖고 있습니다).\n\n최신의 Postgres SQL 데이터베이스에는 실제로 JSON 형식 데이터를 네이티브로 관리하는 JSONB 필드 유형이 있습니다. 그런 다음 SQL을 사용하여 JSON 필드의 데이터에 액세스할 수 있습니다.\n\n그래서 보시다시피, SQL 데이터베이스에 비정형 데이터를 저장하는 여러 옵션이 있습니다.\n\n## No-SQL 데이터베이스의 구조화된 데이터\n\n<div class=\"content-ad\"></div>\n\n그런 반면, No-SQL 데이터베이스를 선택했을 수 있지만 여전히 사용하기 위해 일정한 구조가 필요할 수 있습니다.\n\nSQL 데이터베이스와 마찬가지로 선택 사항이 있습니다.\n\n완전히 정의되지 않은 데이터베이스 구조(컬렉션 및 문서를 넘어서)가 주어지면 구조화된 데이터를 포함하여 필요한 내용을 저장할 수 있습니다. 데이터베이스에 데이터를 저장하기 전에 원하는 구조에 맞도록 유효성 검사 규칙을 적용하는 것만 신경 쓰면 됩니다.\n\n이렇게 하면 코드 내에서 데이터의 구조를 신뢰할 수 있게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n또 다른 고려 사항은 저장 기술 결정 시 일관성이 필요하다는 점입니다.\n\n예를 들어 한 사용자가 데이터를 변경하면 다른 사용자는 그 변경 사항을 어떻게 경험하나요?\n\n<div class=\"content-ad\"></div>\n\n주요한 두 가지 경험이 있습니다:\n\n1. 단일 트랜잭션 중 하나 이상의 변경 사항이 발생할 때 다른 사용자는 트랜잭션이 종료될 때까지 변경 사항을 볼 수 없습니다. 해당 시점에 트랜잭션 중에 변경된 모든 것이 함께 저장되며 다른 사용자는 항상 일관된 데이터 집합을 볼 수 있습니다.\n\n2. 같은 상황에서 각 변경 사항이 별도로 저장되므로 다른 사용자는 해당 변경 사항을 즉시 볼 수 있습니다. 즉, 데이터가 일관성 없을 수 있으며 트랜잭션이 완료될 때까지 데이터는 일관성이 없을 수 있습니다.\n\n두 번째 옵션은 일관성을 보장하기 위해 일시적으로 데이터가 일관성이 없다는 것을 의미하는 '즉시 일관성'이라고 합니다. 문제가 발생할 경우 데이터가 일관성이 없거나 잘못된 상태에 남아 있을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n아마도 이미 알아챘겠지만, SQL 데이터베이스는 옵션 1을 제공하고 No-SQL 데이터베이스는 옵션 2를 제공합니다.\n\n따라서 모든 데이터를 일관된 상태로 표시해야 하는 솔루션이 필요하다면 SQL 데이터베이스를 사용해야 할 수도 있습니다.\n\n그러나 어느 정도의 불일치를 감수할 수 있다면 혜택이 있습니다. 일관성을 보장하지 않아도 No-SQL 데이터베이스는 수평으로 확장할 수 있습니다. 여러 노드 간의 복제가 필요에 따라 수행될 수 있습니다. 이는 No-SQL 데이터베이스가 대규모 데이터 세트에서 더 잘 수행되며 지리적 영역 간에 콘텐츠 복제를 허용하여 응답 성능을 향상시키므로 특히 데이터가 읽는 횟수가 쓰는 횟수보다 많은 경우입니다.\n\n솔루션에서 '최종 일관성'을 왜 허용해야 하는지 궁금해 할 수도 있습니다. 결국 컴퓨터를 사용하는 이유가 그 정확성을 제공하기 때문이 아닌가요?\n\n<div class=\"content-ad\"></div>\n\n실제로, 마이크로서비스 아키텍처를 채택하고 비동기 이벤트 또는 메시징 대기열을 지원하는 경우 이미 결국 일관성을 수용하는 유효한 설계로 인식했을 것입니다. 대부분의 솔루션에서는 수용 가능한 전략으로 보입니다.\n\n이제 일관성과 유동성이 다른 기술을 선택하는 결정의 기술적 요소로 나타납니다.\n\n# 선택을 어떻게 만드는가\n\n지금까지 특정 문제와 해결책에 대해 고려하고 한 기술 또는 다른 기술을 선택했을 것입니다. 그러나 기억하세요, 일을 수행할 수 있는 능력뿐만 아니라 얼마나 비용이 소요될지도 중요하다고 말했었죠.\n\n<div class=\"content-ad\"></div>\n\n해당 비용은 솔루션을 개발하는 비용뿐만 아니라 운영 및 지원 비용까지 포함됩니다. 이 둘이 합쳐져 총 소유 비용(TCO)을 형성합니다. TCO는 시간에 따른 비용을 고려하므로 다음과 같은 비용을 포함할 수 있습니다:\n\n- 놓친 기회 비용 (시장에 나갈 때까지 걸리는 시간)\n- 초기 개발\n- 향상 및 진화\n- 유지보수\n- 품질 보증\n- 제3자 라이선싱 및 지원\n- 서비스 안정성 및 가용성\n- 확장성\n\n실제로, 기술 선택이 기술 능력보다는 TCO와 관련이 더 큰 경우가 많습니다. 일반적으로 솔루션에 할당된 예산이 TCO를 제한하기 때문입니다.\n\n이러한 비용들은 다음과 같이 넓게 분류될 수 있습니다:\n\n\n<div class=\"content-ad\"></div>\n\n- 출시 시기\n- 개발 노력\n- 운영 지원 및 유지보수\n\n이러한 내용들을 좀 더 자세히 살펴보겠습니다.\n\n## 출시 시기\n\n모든 프로젝트에는 마감 기한이 있습니다. 시장 조건을 충족하거나 수익 목표, 비용 절감 또는 다른 종속성을 충족해야 할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n그 마감 기한을 놓치면 회사에는 금전적인 영향이 있을 수 있어요.\n\n따라서, 운송 시간표에 대한 위험을 최소화하는 솔루션이 필요한 상황이에요.\n\n이와 같은 이유로, 시장에 나가는 시간을 기반으로 한 결정은 어떤 기술을 선호하는 것이 아니라 구현 위험이 최소인 것을 선호합니다. 일반적으로 이미 보유하고 있거나 배송 팀이 가장 익숙한 기술입니다.\n\n이러한 결정은 기술적 부채 수준이 높아지도록 할 수 있지만, 모든 부채와 마찬가지로 현재의 비즈니스 목표를 더 빨리 달성할 수 있도록 해 줄 수 있을 거예요.\n\n<div class=\"content-ad\"></div>\n\n## 개발 노력\n\nNo-SQL 데이터베이스를 도입하면 저장 계층을 설계하고 구현할 필요가 없어져서 개발이 빨라진다고 생각되곤 합니다. 현실적으로는 시간을 절약하는 것이 미미할지라도, 비례적으로 전달 속도를 높이고 위험도를 줄일 수 있습니다.\n\n실제로 데이터 계층을 구성하지 않아도, 여전히 설계되고 그 설계를 준수하는 코드를 작성해야 합니다.\n\nNo-SQL 데이터베이스는 알 수 없는 데이터 구조와 비즈니스 요구 사항을 선호하는 경향이 있습니다. 이는 지금 당장 솔루션이 필요하지만 나중에 확인되지 않을 요구 사항이 있는 상황에 이상적입니다. No-SQL 데이터베이스를 사용하면 이러한 결정으로 인해 발생하는 기술 부채의 양을 줄일 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n기술을 결정하기 전에 개발 팀을 고려해야 합니다. 시장 진입 시기, 솔루션의 품질(기능, 비기능 및 보안 요구 사항 측면) 및 기술 부채 감소를 고려할 때, 개발팀이 알고 있는 기술을 사용하는 것이 더 나아요.\n\n만약 개발 팀이 새로운 기술을 배우거나 관련 기술을 갖춘 팀원을 추가할 수 있는 시간과 예산이 있다면, 개발 시간은 결정에 영향을 미치지 않지만, 경험상 이 경우가 드물다고 합니다.\n\n## 운영 지원 및 유지보수\n\n좋아요, 현재 기술과는 다른 기술을 선택하고 그것을 중심으로 솔루션을 개발했습니다. 모든 것이 잘 진행되었고 결정에 만족하고 있습니다. 솔루션을 운영 환경에 적용하고 나면 문제가 시작됩니다.\n\n<div class=\"content-ad\"></div>\n\n유지보수 지원 비용이 증가한다는 것을 알게 됩니다. 새 기술 라이선싱이 기존 계약에 해당하지 않아 추가 비용이 발생합니다.\n\n1, 2, 3 레벨 지원팀을 포함한 운영팀이 새로운 기술을 이해하지 못하고 개발자들과 비슷한 학습 곡선을 겪어야 합니다. 이는 프로젝트 제공 기간 및/또는 서비스 품질에 영향을 미칩니다.\n\n운영팀이 다양한 심각도의 결함을 관리할 수 있도록 새 운영 지원 도구를 구축해야 합니다. 사람들이 학습하는 과정에서 실수가 발생할 수 있으며 이는 사용자에 영향을 줄 수 있습니다. 복구 시간이 늘어날 수 있습니다.\n\n모든 이는 비즈니스에 영향을 줄 수 있으며, 기술 변경을 고려할 때 고려해야 할 사항(및 계획)입니다.\n\n<div class=\"content-ad\"></div>\n\n# 모두 묶어서\n\n지금까지, 이 글에서 데이터베이스 기술을 선택할 때 고려해야 할 사항을 살펴보았습니다. 요약하면, 결정은 'xyz에서 작업하고 싶다'보다 복잡합니다. 고수준에서는 다음을 포함합니다:\n\n- 데이터의 유동성\n- 일관성의 필요성\n- 시장 진입 시간\n- 개발 노력\n- 운영 지원 및 유지 보수\n\n매일 No-SQL과 SQL 데이터베이스 간의 차이와 격차가 좁아지고 있다는 것을 보여 드렸으면 좋겠습니다. 각각이 프로젝트에 도움이 될 수 있는 서로 다른 특성을 갖고 있지만, 전반적으로 이러한 고유한 이점들이 점점 더 작아지고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n기술적으로 모든 것이 동등한 경우 비즈니스 영향으로 돌아가는데, 일반적으로 재무 영향으로 이어집니다. 초록밭 프로젝트의 여유가 있거나 새로운 개발 팀을 모집할 수 있는 상황이 아니라면, 기존 인수기술을 선호하는 요소가 있을 것입니다.\n\n내 경력에서 기술 선택에 참여해 본 적이 있고, 그에 따라 발생하는 도전에 대해 목격했습니다. 초록밭 프로젝트의 혜택을 누린 바 있고, 나만의 개발 팀을 모집할 수도 있었습니다. 내려받은 임베디드 기술과 개발팀을 승계한 경우도 있었으며 다양한 옵션을 고려해야 했습니다.\n\n거의 모든 경우, 내가 답변해야 하는 질문은 예산과 시간표에 미치는 영향에 관한 것입니다. 모든 부채와 마찬가지로 기술적 부채는 나중에 해결될 것이며 아마도 둘 중 어느 쪽으로든 축적될 것입니다.\n\n이야기로 나오는 증거에 따르면 소프트웨어는 대체되기 전에 5-8년 안에 구식이 됩니다. 이 시간 기간은 전체 현대적 아키텍처의 변화, 시장 변동, 공급업체가 기술을 향상시키도록 압박하는 경쟁자의 압력 등을 기반으로 합니다. 5년 후에 사라지는 신용카드를 어떻게 사용하겠습니까?\n\n<div class=\"content-ad\"></div>\n\n위해 No-SQL과 SQL 중에서 선택할 때, 다음 우선순위를 결정하는 데 도움이 될 것 같아요:\n\n- 데이터 유동성 (설계 시 또는 실행 시)\n- 데이터 일관성의 필요성\n- 이용 가능한 예산 / 시간 일정\n- 기술 및 팀 역량\n- 확장성과 성능\n\n이를 몇 가지 시나리오를 통해 살펴볼 수 있어요:\n\n두 가지 다른 프로젝트를 가정해봅시다:\n\n<div class=\"content-ad\"></div>\n\n프로젝트 #1은 유동적인 데이터 세트이며 데이터 일관성이 필요하지 않으며 대규모 확장이 필요합니다.\n\n프로젝트 #2는 비교적 안정적인 데이터 세트이며 일관성이 필요하며 대규모 확장 요구 사항이 없습니다.\n\n- Greenfield 프로젝트가 주어지면, #1은 No-SQL 옵션을 선택할 것입니다.\n- Greenfield 프로젝트가 주어지면, #2는 SQL 옵션을 선택할 것입니다.\n- 기존 기술과 관련 기술을 보유하고 있으며 충분한 예산과 기술 변경 시간이 있다면, 동일한 선택이 적용됩니다 (#1 = No-SQL) 및 (#2 = SQL).\n- 기존 기술과 관련 기술을 보유하고 있지만 기술 변경을 위한 예산이나 시간이 없다면, 어떤 타협을 해결하기 위한 적절한 전략을 사용하여 기존 기술을 사용하여 #1 또는 #2에 대처할 것입니다.\n\n# 요약\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 No-SQL과 SQL 데이터베이스 사이를 결정할 때 고려하는 몇 가지 요소들을 살펴보았습니다.\n\n주로 의사 결정은 '작업을 수행하는 능력'에 기반하고 있지만 기술이 향상되어 격차가 줄어드는 점을 고려하면 이것은 논의해볼 가치가 있는 주제입니다.\n\n다음으로, 의사 결정은 개발 비용, 지원 및 유지 보수 비용, 운영 비용을 모두 고려한 총 소유 비용(TCO)에 기반하고 있습니다.\n\n새로운 기술을 도입하는 비즈니스 케이스를 정당화하는 것은, 기술적인 측면에 '위기요소'가 없다는 가정하에 이미 보유한 것을 활용하는 것보다 훨씬 어려운 일임을 주목해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n모든 프로젝트, 비즈니스 및 의사 결정 프레임워크는 다릅니다. 그러나 제가 고려해야 할 요소에 대해 조명을 켰기를 바랍니다.\n\n이 기사를 즐겁게 읽으셨기를 바라며, 새로운 것을 학습하여 기술을 향상시켰기를 바랍니다. 그것이 작은 일이더라도요.\n\n만약 이 기사가 흥미롭게 느껴지셨다면, 공감 클랩을 주시면 감사하겠습니다. 이를 통해 어떤 정보가 유용하게 여겨지는지와 앞으로 어떤 주제의 기사를 쓸지를 파악할 수 있습니다. 의견이나 제안이 있으시다면 노트나 답글로 추가해 주시기 바랍니다.","ogImage":{"url":"/assets/img/2024-06-19-HowIchoosebetweenSQLandNo-SQLsolutions_0.png"},"coverImage":"/assets/img/2024-06-19-HowIchoosebetweenSQLandNo-SQLsolutions_0.png","tag":["Tech"],"readingTime":12},{"title":" Tiger Analytics 데이터 엔지니어 역할 면접 경험 ","description":"","date":"2024-06-19 01:45","slug":"2024-06-19-TigerAnalyticsDataEngineerRoleInterviewExperience","content":"\n\n<img src=\"/assets/img/2024-06-19-TigerAnalyticsDataEngineerRoleInterviewExperience_0.png\" />\n\nTiger Analytics와의 데이터 엔지니어링 인터뷰 경험 공유: 🐯 ⚡\n\n### 지원 단계:\n\n- Naukri.com을 통해 지원했고, 다음 날에 리크루터로부터 전화를 받았습니다.\n- 대화 중에 리크루터가 총 경험 및 관련 경력, 현재 및 예상 보수, 그리고 통보 기간과 같은 세부 사항을 요청했습니다.\n\n<div class=\"content-ad\"></div>\n\n---\n### 코딩 라운드: (60분 소요)\n\n이 라운드는 중간 수준 이상이었으며 빠른 완료가 필요한 시간이 많이 소요되는 라운드였어요.\n\n**파이썬, SQL 및 스파크 시나리오:**\n\n<div class=\"content-ad\"></div>\n\n**Spark 코딩:**\n\n- Spark RDD를 활용하여 튜플 목록의 요소 발생 횟수를 세는 작업을 `groupByKey`를 피하고 효율적으로 수행했습니다.\n- RDD 내부의 키-값 쌍의 평균을 계산하기 위해 `groupBy`를 사용하지 않고 `combineByKey`와 같은 Spark 변환을 구현했습니다.\n\n**SQL:**\n\n- 복잡한 테이블 조인, CTEs (공통 테이블 표현식) 및 윈도우 함수가 포함된 두 가지 SQL 문제를 성공적으로 해결했습니다. 이러한 도전은 윈도우 함수로 쿼리를 최적화하고 단일 열과 유사한 값이 있는 테이블을 조인하는 데 중점이 있었습니다.\n\n**Python:**\n\n- 중첩된 리스트를 재귀적으로 해체하고 숫자 범위를 사용하여 리스트를 연결하기 위해 Python을 활용했습니다.\n- Hadoop 아키텍처, SQL, Spark 아키텍처, Spark 변환 및 Spark 최적화 기술에 관련된 이론 질문에 답했습니다.\n\n<div class=\"content-ad\"></div>\n\n### 기술 토론 (소요 시간: 60분):\n\n- 이 라운드는 아키텍트 수준의 면접관에의해 수행되었으며 매우 도전적이었습니다.\n- 면접관은 시간을 엄수하며 소개로 시작하여 예의 바르게 행동했습니다.\n- 프로젝트 아키텍처에 대한 깊은 탐구로 토론을 시작했고, 깊이 있는 질문에 대답하였습니다.\n- 복잡한 테이블 조인, CTEs (공통 테이블 표현식), 윈도우 함수를 활용한 2개의 SQL 문제를 성공적으로 해결하였습니다.\n- PySpark로 전환하여 두 가지 PySpark 문제를 해결하는 능력을 증명하였습니다.\n- Apache Spark에 대해 깊이 파헤치며 아키텍처, 재분할 및 병합과 같은 최적화 기술, 응용 프로그램 관리, 단계, 작업, DAG (방향 그래프), 계보 그래프를 다루었습니다.\n- 응답에 대한 소중한 피드백을 받아 기술적 능력 및 팀 기대치 및 문화와의 일치에 대한 통찰력을 얻었습니다.\n\n— -\n\n### 인사담당자 인터뷰:\n\n<div class=\"content-ad\"></div>\n\n- HR 라운드는 친근하게 진행되었습니다. 기본 소개로 시작하여 위치 및 급여에 대한 논의로 이어졌어요.\n- Tiger의 정책을 논의하고 급여 조건을 협상했어요.\n- 한 주 이내에 입사 제안을 받았고, 입사 날짜는 제 퇴직 기간 이후로 예정되었어요.\n- 전체 과정이 한달도 안 되는 시간 내에 완료되었어요.\n\n—-\n\n도움이 되었기를 바랍니다. 😀\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*YPpBH2cBCLbFNc_C2eTlsQ.gif)","ogImage":{"url":"/assets/img/2024-06-19-TigerAnalyticsDataEngineerRoleInterviewExperience_0.png"},"coverImage":"/assets/img/2024-06-19-TigerAnalyticsDataEngineerRoleInterviewExperience_0.png","tag":["Tech"],"readingTime":2},{"title":"SQL 윈도우 함수 데이터 열정가들을 위한 최고의 도구","description":"","date":"2024-06-19 01:43","slug":"2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts","content":"\n\n\n![SQL Window Functions](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_0.png)\n\n요즘에는 엄청난 양의 데이터를 다루고 있습니다. 이 주요한 도전에 따라 다양한 소스의 복잡도도 함께 증가하고 있습니다. 이러한 환경에서 SQL은 여전히 영웅이며, 이 데이터 바다에서 가치 있는 통찰을 추출하고 탐색하는 데 꼭 필요한 도구입니다.\n\nSQL이 제공하는 많은 강력한 기능 중에서도 윈도우 함수는 특히 주목할 만한 요소입니다. 이러한 함수들은 테이블 행 집합을 대상으로 높명한 계산을 가능하게 하며, 고급 데이터 분석에 필수적이며 데이터와 상호작용하는 방법을 변화시키는 데 중요합니다.\n\n이 기사에서는 SQL의 윈도우 함수 개념을 해부하고 이해할 것입니다. 언제 윈도우 함수를 사용해야 하는지, 그리고 SQL 쿼리에서 효과적으로 구현하는 방법에 대해 살펴볼 것입니다. 이 가이드를 마치면 윈도우 함수의 강력함과 유연성에 대한 깊은 이해를 얻게 될 것이며, 데이터 분석 기술을 향상시키기 위한 실제 예제를 활용할 수 있을 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 윈도우 함수가 뭔가요?\n\n경험 수준에 상관 없이 모든 데이터 애호가는 윈도우 함수에 대해 들어봤거나 사용해 본 적이 있을 것입니다. 이 강력한 도구들은 모든 SQL 강좌에서 퍼져 있으며 데이터 작업을 하는 사람들의 일상생활에서 필수불가결합니다.\n\n구글에서 빠르게 검색을 해보죠…몇 분 후에 혹은 TV 광고를 보고 나서, 우리는 윈도우 함수가 다음과 같다는 사실을 알게 됩니다:\n\n<div class=\"content-ad\"></div>\n\n# 문법에 관해서 뭔가 언급했다고 했나요?\n\n그렇습니다. 이 매우 강력한 도구에는 특정 구문과 같은 트릭이 함께 제공됩니다.\n\n![image](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_1.png)\n\n위 이미지에서 볼 수 있듯이, 윈도우 함수의 구문은 네 부분으로 나뉠 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 집계/함수: 여기서 집계 (예: AVG, SUM) 또는 LAG(), LEAD(), ROW_NUMBER(), RANK(), DENSE_RANK()과 같은 특정 창 함수를 배치하여 작업을 시작합니다. 몇 가지 더 있지만, 이 중에서는 가장 일반적으로 사용되는 것들이에요 (적어도 저는 이것들을 가장 많이 사용해요 😁)\n- OVER: 이 키워드는 윈도우 함수를 사용할 것임을 IDE에 \"알리는\" 데에 사용됩니다. 이는 \"여기서 무언가를 할 것이고, 무언가 복잡한 것에 대비해야 한다\"고 말하는 것과 같아요.\n- PARTITION BY: 이 절은 결과를 파티션 또는 창으로 나눕니다. 우리는 이 과정에서 초기에 설정한 집계나 함수를 적용할 것입니다. 이 부분을 작성한 후에는 파티션을 기준으로 필드를 개발해야 합니다. 순위 함수와 함께 사용되지 않아요.\n- ORDER BY: 경우에 따라 선택 사항일 수 있지만, 이것이 하는 일을 알아두는 것이 좋아요. 이는 각 파티션 내의 행을 정렬하는 데 사용되며, RANK(), DENSE_RANK(), ROW_NUMBER()와 같은 순위 함수를 사용할 때 유용합니다.\n\n# 목표를 달성하기 위한 다양한 창 함수\n\n이전 섹션에서 창 함수 구문에 대해 이야기했어요. 창 함수 구문과 독립적으로 작동하지 않는 몇 가지 함수를 언급했죠.\n\n일부는 각 파티션의 각 행에 대한 순위 값을 반환하기 때문에 순위 함수라고 불리며, 다른 것은 시계열 창 함수입니다.\n\n<div class=\"content-ad\"></div>\n\n순위 함수:\n\n- RANK() : 결과 집합의 파티션 내 각 행에 순위를 할당하며, 동일한 값을 가진 행은 동일한 순위를 받습니다.\n- DENSE_RANK() : RANK()와 유사하지만 연속적인 순위 값을 가집니다. 동일한 값은 동일한 순위를 받으며, 다음 순위 값은 다음 연속 정수입니다.\n- NTILE() : 결과 집합을 동일한 그룹으로 분할하고 각 행에 속하는 그룹을 나타내는 숫자를 할당합니다.\n- ROW_NUMBER() : 결과 집합의 파티션 내 각 행에 고유한 연속 정수를 할당하며, 각 파티션의 첫 번째 행부터 1로 시작합니다.\n\n시계열 함수:\n\n- LAG() : 결과 집합 내 이전 행의 값을 가져오는 함수로, 자체 조인이 필요하지 않습니다. 연속된 행 간의 차이를 계산하는 데 도움이 됩니다.\n- LEAD() : 다음 행의 값을 예측하는 데 유용한, 자체 조인 없이 다음 행의 값을 액세스할 수 있습니다. 추세나 값의 변화를 예측하는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 영원한 질문: 왜...\n\n많은 것들에 대해 우리가 하는 일반적인 질문들이 있습니다. SQL의 창 함수도 예왽이 아닙니다. 창 함수가 여러분에게 시간과 노력을 절약해줄 수 있는 상황을 이해하려면 다음을 살펴보겠습니다:\n\n왜 그리고 언제 우리는 창 함수를 사용해야 할까요?\n\n언제부터 시작해볼까요. 언제 우리는 창 함수를 사용할까요? 잘, 우리가 창 함수를 사용해야 하는 시점은 언제든지 우리가 필요로 할 때 입니다:\n\n<div class=\"content-ad\"></div>\n\n- 특정 조건에 따라 데이터 하위 집합에서 누적 합계, 순위, 평균 또는 다른 계산을 계산합니다.\n- 현재 및 이전/다음 행 값 비교\n\n왜 윈도우 함수를 사용해야 하는지 왜도 빼놓지 마세요. 상황에 필요할 때 윈도우 함수를 사용해야 하는 이유는 무엇인가요?\n \n윈도우 함수를 사용해야 하는 이유:\n\n- 행 레벨 세부 정보 유지 — 데이터를 축소하지 않고 계산을 수행할 수 있는데, 이는 원본 데이터를 유지한 채 여러 행을 대상으로 계산할 수 있도록 합니다.\n- 복잡한 쿼리 간소화 — 이 도구를 사용하면 가장 복잡한 쿼리를 간소화하여 읽기 좋고 작성하기 쉽고, 무엇보다도 유지보수하기 쉽게 만들어줍니다.\n- 성능 향상 — SQL 엔진에서 최적화되어 대량 데이터셋의 경우 더 나은 성능을 제공하는 경우가 많습니다.\n- 고급 분석 활성화 — 누적 합계, 이동 평균 및 기타 고급 분석 작업을 실행할 수 있도록 합니다.\n- 자세한 분석을 위한 데이터 파티션 — 특정 기준에 따라 데이터를 분할하여 전체 데이터 집계 없이 그룹 내에서 자세한 분석을 가능하게 합니다.\n- 시계열 및 변경 감지 지원 — 이전 또는 다음 행 값에 액세스하는 내장 지원을 제공하여 시계열 데이터 및 변경 감지에 유용합니다.\n\n<div class=\"content-ad\"></div>\n\n# 실제 사용 사례\n\n은행 분야에서 데이터 엔지니어로 일하고 있는데, 계약의 \"단계\"가 변경된 레코드를 식별하고 이 변경 날짜를 기록해야 하는 요청을 받았어요.\n\n쉽게 말해, 그렇게 하는 게 쉽지 않을 것 같죠? 그렇게는 안 돼요. 윈도우 함수를 사용해서 요청을 완료하고 결과를 빠르게 전달하는 데 도움이 되었어요.\n\n우리가 두 개의 테이블이 있다고 가정해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n\nsource.data_records\n\n![Image 2](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_2.png)\n\nand temp.data_records:\n\n![Image 3](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_3.png)\n\n\n<div class=\"content-ad\"></div>\n\n그리고 다음 안에 테이블을 생성해야 합니다. 그 안에는 다음과 같은 정보가 포함되어 있어야 합니다:\n\n- 식별자\n- 식별자의 현재 레벨\n- 현재 단계의 참조 날짜\n- 식별자의 이전 레벨\n- 이전 참조 날짜\n- 식별자가 레벨을 변경한 날짜\n\n테이블은 아래 코드를 기반으로 생성되었습니다:\n\n```js\ncreate table tmp_change_level_date as\n(\nselect distinct * from ( \n    select \n        fct.identifier, fct.level, fct.date_ref,\n        lag(fct.level) over (partition by fct.identifier order by fct.date_ref) as previous_level,\n        lag(fct.date_ref) over (partition by fct.identifier order by fct.date_ref) as previous_date,\n        case\n            when lag(fct.level) over (partition by fct.identifier order by fct.date_ref) is not null then fct.date_ref\n            else NULL\n        end as change_level_date,\n        dense_rank() over (partition by fct.identifier order by fct.date_ref desc) as ranks\n    from source.data_records fct  join temp.data_records TFCT \n    on fct.identifier = TFCT.identifier\n    where TFCT.amount <> 0 and TFCT.account in (select account_code from accounts_list)\n    ) x\nwhere ranks = 1 \nand level <> previous_level\nand previous_date <> change_level_date\n)\ncommit;\n```\n\n<div class=\"content-ad\"></div>\n\n자, 이제 설명으로 들어가볼게요:\n\n- 우선적으로, loan identifier(대출 식별자), level, date_ref(대출의 실제 단계 및 현재 단계의 기준 날짜)와 같은 정보를 가져오는 주요 SELECT 문을 만들었습니다:\n\n```js\nselect \n        fct.identifier, fct.level, fct.date_ref,\n        lag(fct.level) over (partition by fct.identifier order by fct.date_ref) as previous_level,\n        lag(fct.date_ref) over (partition by fct.identifier order by fct.date_ref) as previous_date,\n        case\n            when lag(fct.level) over (partition by fct.identifier order by fct.date_ref) is not null then fct.date_ref\n            else NULL\n        end as change_level_date,\n        dense_rank() over (partition by fct.identifier order by fct.date_ref desc) as ranks\n    from source.data_records fct  join temp.data_records TFCT \n    on fct.identifier = TFCT.identifier\n    where TFCT.amount <> 0 and TFCT.account in (select account_code from accounts_list)\n    ) x\n```\n\n그 다음으로, 각 대출에 대해 이전 대출 단계와 이전 참조 날짜를 가져오기 위해 LAG() 함수를 사용했습니다. PARTITION BY를 사용하여 식별자에 따라 데이터셋을 작은 파티션으로 나누고, 각 파티션 내에서 레코드를 date_ref에 따라 정렬했습니다.\n\n<div class=\"content-ad\"></div>\n\n\nlag(fct.level) over (partition by fct.identifier order by fct.date_ref) as previous_level,\nlag(fct.date_ref) over (partition by fct.identifier order by fct.date_ref) as previous_date\n\n\nand assign a rank to each record within the partition by using DENSE_RANK() function:\n\n\ndense_rank() over (partition by fct.identifier order by fct.date_ref desc) as ranks\n\n\nThis code will return the following result:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_4.png\" />\n\n더 나아가서, 이전 결과에서 일부 필터를 적용할 수 있도록 다음 SELECT문을 작성합니다 (위의 표에 해당하는):\n\n```js\nselect distinct * from (\n\n---- 이전 select를 하위 쿼리로 사용 ----\n\n) x\nwhere ranks = 1 \nand level <> previous_level\nand previous_date <> change_level_date\n```\n\n그리고 각 식별자에 대해 가장 최근 레코드만 가져와서 (ranks = 1은 설명에서 앞에서 언급한 가장 최근 레코드에 해당함), 현재 레벨이 이전 레벨과 다른 레코드만 가져오도록 필터를 적용하며 (level != previous_level), 변경 날짜가 유효하고 이전 참조 날짜와 다른지 확인합니다. 이러한 필터를 기반으로 결과를 새로운 테이블 tmp_change_level_date에 삽입합니다 (CREATE TABLE table_name AS와 유명한 구문을 사용하여 만든 것):\n\n<div class=\"content-ad\"></div>\n\n\n![SQL Window Functions](/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_5.png)\n\n이 결과를 통해 다음을 알 수 있습니다:\n\n- 식별자 2의 경우: 레벨이 2023년 03월 15일에 A에서 C로 변경되었습니다.\n- 식별자 3의 경우: 레벨이 2023년 02월 20일에 B에서 A로 변경되었습니다.\n\n# 결론\n\n\n<div class=\"content-ad\"></div>\n\nSQL 윈도우 함수는 복잡한 데이터 분석을 간편하게 하고 성능을 향상시킵니다. 이 글에서는 기본 사항, 구문, 랭킹 및 시계열 분석과 같은 일반적인 사용 사례, 실제 예제에 대해 다룹니다. 이러한 함수를 숙달하면 SQL 쿼리를 더 효율적이고 통찰력 있게 만들 수 있습니다.\n\n실습하고 실험하여 그 능력을 최대로 발휘하고 데이터 분석 능력을 향상시키세요.","ogImage":{"url":"/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_0.png"},"coverImage":"/assets/img/2024-06-19-SQLWindowFunctionsTheUltimateToolforDataEnthusiasts_0.png","tag":["Tech"],"readingTime":8},{"title":"최고의 Kafka에서 Delta 적재를 위한 도구들을 벤치마킹합니다","description":"","date":"2024-06-19 01:41","slug":"2024-06-19-NeedforSpeedBenchmarkingtheBestToolsforKafkatoDeltaIngestion","content":"\n\n# 소개\n\n데이터브릭스 플랫폼에서 카프카에서 델타 테이블로 데이터를 수집하는 시리즈의 두 번째 부분에 다시 오신 것을 환영합니다. 이전에 우리가 논의한 것을 더욱 발전시키면서, 카프카 토픽으로 합성 데이터를 생성하고 스트리밍하는 것에 대해 살펴보겠습니다.\n\n본 블로그 포스트에서는 델타 레이크로부터 아파치 카프카에서 스트리밍 데이터를 수집하기 위한 데이터브릭스 플랫폼에서 사용 가능한 세 가지 강력한 옵션을 벤치마킹합니다: 데이터브릭스 잡, 델타 라이브 테이블(DLT) 및 델타 라이브 테이블 서버리스(DLT 서버리스). 주요 목표는 이러한 접근 방식을 통해 카프카에서 델타 테이블로 데이터를 수집할 때의 엔드 투 엔드 지연 시간을 평가하고 비교하는 것입니다.\n\n지연 시간은 중요한 지표이며, 하류 분석 및 의사 결정 프로세스에 사용 가능한 데이터의 신선도와 적시성에 직접적인 영향을 미칩니다. 모든 세 가지 도구가 Apache Spark의 구조적 스트리밍을 내부적으로 활용한다는 점을 강조해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 벤치마크 설정\n\n## 벤치마킹 기준\n\n측정한 주요 메트릭은 대기 시간이었습니다 - 카프카에서 행이 생성된 시점부터 델타 레이크에서 이용 가능해질 때까지 걸리는 시간입니다. 대기 시간은 정밀하게 측정되었으며 정확성을 보장하고 변동성을 고려하기 위해 장기간에 걸쳐 신중하게 측정되었습니다.\n\n## 입력 카프카 피드\n\n<div class=\"content-ad\"></div>\n\n저희가 수행한 벤치마크에서는 매 초 100개의 행을 생성하는 Kafka 피드를 활용했어요. 각 행은 대략 1MB로, 초당 100MB로 이루어져요. 연간으로 계산하면 약 3.15 페타바이트가 되어, 저희가 선택한 도구의 수신 능력을 평가하기 위한 엄격한 테스트 베드가 됐어요.\n\nConfluent Cloud를 사용하여 6개 파티션으로 Kafka 클러스터를 설정했는데, 5분 미만이 걸렸어요. 그리고 실험을 위해 300달러의 크레딧을 제공해 주었어요.\n\n![이미지](/assets/img/2024-06-19-NeedforSpeedBenchmarkingtheBestToolsforKafkatoDeltaIngestion_0.png)\n\n## 비교 도구\n\n<div class=\"content-ad\"></div>\n\n- Databricks 작업: Kafka에서 읽고 Delta Lake 테이블에 쓰기 위해 Apache Spark Structured Streaming을 활용합니다. 작업 구성 및 예약에 유연성을 제공하지만 클러스터 리소스의 수동 관리가 필요합니다.\n- Delta Live Tables (DLT): Kafka에서 Delta Lake로 데이터를 입력하기 위해 선언적 접근 방식을 사용하며, 인프라를 자동으로 관리하고 파이프라인 개발을 간소화합니다.\n- Delta Live Tables Serverless (DLT Serverless): DLT와 동일한 입력 작업을 수행하면서, 인프라 관리를 더 간소화하기 위한 서버리스 모델을 활용합니다. 자동 스케일링과 리소스 최적화를 제공합니다.\n\n## 지연 시간은 어떻게 측정되었나요?\n\n지연 시간은 테이블로의 연속적인 스트리밍 업데이트 타임스탬프 간의 밀리초 단위 시간 차이를 계산하여 측정됩니다. 이는 각 순차적 커밋에 대해 이전 업데이트의 타임스탬프를 현재 업데이트의 타임스탬프에서 뺌으로써 수행되며, 각 업데이트가 이전 업데이트에 비해 처리하는 데 얼마나 걸리는지 분석할 수 있습니다. 분석은 현재 300개의 커밋으로 제한되어 있지만 필요에 따라 조정할 수 있습니다.\n\n```js\nfrom pyspark.sql import DataFrame\n\ndef run_analysis_about_latency(table_name: str) -> DataFrame:\n    # Python 다중 라인 문자열로 형식 지정된 SQL 명령어 텍스트\n    sql_code = f\"\"\"\n        -- 테이블의 업데이트 이력에 대한 가상 뷰 정의\n        WITH VW_TABLE_HISTORY AS (\n          -- 테이블의 역사적 변화 설명\n          DESCRIBE HISTORY {table_name}\n        ),\n        \n        -- 이전 쓰기 작업의 타임스탬프를 계산하는 뷰 정의\n        VW_TABLE_HISTORY_WITH_previous_WRITE_TIMESTAMP AS (\n          SELECT\n            -- 현재 작업 이전의 마지막 쓰기 작업의 타임스탬프를 계산\n            lag(timestamp) OVER (\n              PARTITION BY 1\n              ORDER BY version\n            ) AS previous_write_timestamp,\n            timestamp,\n            version\n          FROM\n            VW_TABLE_HISTORY\n          WHERE\n            operation = 'STREAMING UPDATE'\n        ),\n        \n        -- 연속 커밋 간의 밀착 정도를 분석하는 뷰 정의\n        VW_BOUND_ANALYSIS_TO_N_COMMITS AS (\n          SELECT\n            -- 이전 및 현재 쓰기 타임스탬프 간의 밀리초 단위 시간 차이 계산\n            TIMESTAMPDIFF(\n              MILLISECOND,\n              previous_write_timestamp,\n              timestamp\n            ) AS elapsed_time_ms\n          FROM\n            VW_TABLE_HISTORY_WITH_previous_WRITE_TIMESTAMP\n          ORDER BY\n            version DESC\n          LIMIT\n            300  -- 최근 300개 커밋만 분석\n        )\n        \n        -- 쓰기 지연 시간에 대한 다양한 통계 계산\n        SELECT\n          avg(elapsed_time_ms) AS average_write_latency,\n          percentile_approx(elapsed_time_ms, 0.9) AS p90_write_latency,\n          percentile_approx(elapsed_time_ms, 0.95) AS p95_write_latency,\n          percentile_approx(elapsed_time_ms, 0.99) AS p99_write_latency,\n          max(elapsed_time_ms) AS maximum_write_latency\n        FROM\n          VW_BOUND_ANALYSIS_TO_N_COMMITS\n    \"\"\"\n    # Spark의 SQL 모듈을 사용하여 SQL 쿼리 실행\n    display(spark.sql(sql_code))\n```\n\n<div class=\"content-ad\"></div>\n\n# 데이터 수집\n\n이 코드는 Apache Spark를 사용하여 Kafka topic에서 데이터를 효율적으로 수집하는 스트리밍 데이터 파이프라인을 설정합니다. Kafka 메시지에서 예상되는 데이터 유형 및 열에 맞게 구성된 스키마를 정의하고, 차량 세부 정보, 지리적 좌표 및 텍스트 필드를 포함합니다. read_kafka_stream 함수는 스트리밍 프로세스를 초기화하고 Kafka에 안전하고 신뢰할 수 있는 연결을 구성하며, 지정된 주제를 구독하여 개선된 처리 속도를 위해 여러 파티션을 통해 데이터를 처리합니다. 스트림은 정의된 스키마에 따라 JSON 형식 메시지를 디코딩하고 필수 메타데이터를 추출합니다.\n\n```js\nfrom pyspark.sql.types import StructType, StringType, FloatType\nfrom pyspark.sql.functions import *\n\n# DataFrame 구조에 기반한 스키마 정의\nschema = StructType() \\\n    .add(\"event_id\", StringType()) \\\n    .add(\"vehicle_year_make_model\", StringType()) \\\n    .add(\"vehicle_year_make_model_cat\", StringType()) \\\n    .add(\"vehicle_make_model\", StringType()) \\\n    .add(\"vehicle_make\", StringType()) \\\n    .add(\"vehicle_model\", StringType()) \\\n    .add(\"vehicle_year\", StringType()) \\\n    .add(\"vehicle_category\", StringType()) \\\n    .add(\"vehicle_object\", StringType()) \\\n    .add(\"latitude\", StringType()) \\\n    .add(\"longitude\", StringType()) \\\n    .add(\"location_on_land\", StringType()) \\\n    .add(\"local_latlng\", StringType()) \\\n    .add(\"zipcode\", StringType()) \\\n    .add(\"large_text_col_1\", StringType()) \\\n    .add(\"large_text_col_2\", StringType()) \\\n    .add(\"large_text_col_3\", StringType()) \\\n    .add(\"large_text_col_4\", StringType()) \\\n    .add(\"large_text_col_5\", StringType()) \\\n    .add(\"large_text_col_6\", StringType()) \\\n    .add(\"large_text_col_7\", StringType()) \\\n    .add(\"large_text_col_8\", StringType()) \\\n    .add(\"large_text_col_9\", StringType())\n\ndef read_kafka_stream():\n    kafka_stream = (spark.readStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers_tls ) \n      .option(\"subscribe\", topic )\n      .option(\"failOnDataLoss\",\"false\")\n      .option(\"kafka.security.protocol\", \"SASL_SSL\")\n      .option(\"kafka.sasl.mechanism\", \"PLAIN\") \n      .option(\"kafka.sasl.jaas.config\", f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_api_key}\" password=\"{kafka_api_secret}\";')\n      .option(\"minPartitions\",12)\n      .load()\n      .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"), \"topic\", \"partition\", \"offset\", \"timestamp\", \"timestampType\" )\n      .select(\"topic\", \"partition\", \"offset\", \"timestamp\", \"timestampType\", \"data.*\")\n    )\n    return kafka_stream\n```\n\n## 설명:\n\n<div class=\"content-ad\"></div>\n\n- Connection Setup: 특정 부트스트랩 서버를 사용하여 Kafka에 연결하고 SASL_SSL과 같은 보안 설정을 포함하여 암호화 및 인증된 데이터 전송을 합니다.\n- Topic Subscription: 지정된 Kafka 주제에 가입하여 계속해서 새로운 데이터를 수신합니다.\n- Stream Configuration: 잠재적인 데이터 손실을 처리하고 여러 파티션 간의 데이터를 처리 속도를 높이기 위해 견고하게 구성됩니다.\n- Data Transformation: 수신된 JSON 메시지를 설정된 스키마에 따라 디코딩하기 위해 from_json을 사용하며 Spark 내에서 구조화된 형식으로 변환합니다.\n- Metadata Extraction: Kafka 주제, 파티션 및 메시지 타임 스탬프와 같은 필수 메타데이터를 추출합니다.\n\n이 설정은 Kafka에서 Spark로의 데이터 흡수를 최적화하고 데이터를 추가적으로 처리하거나 Delta Lake와 같은 저장 시스템으로 통합하기 위한 준비를 합니다.\n\n## Databricks Jobs을 위한 추가 코드\n\n구성: 이 방법은 Databricks 작업 및 클러스터 리소스를 설정하는 것을 포함하며 유연한 스케줄링 및 흡수 프로세스 모니터링을 가능하게 합니다만, 올바른 컴퓨팅을 선택하는 것을 이해해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n(  \n  read_kafka_stream()\n  .writeStream\n  .option(\"checkpointLocation\",checkpoint_location_for_delta)\n  .trigger(processingTime='1 second')\n  .toTable(target)\n)\n```\n\n## Delta Live Tables에 대한 추가 코드\n\n구성: Delta Live Tables는 인프라를 자동으로 관리하여 데이터 파이프라인을 구성하는 간단하고 선언적인 방식을 제공합니다.\n\n이 코드 스니펫은 Delta Live Tables (DLT) API를 사용하여 Kafka에서 스트리밍 데이터를 수신하는 데이터 테이블을 정의합니다. @dlt.table 데코레이터를 사용하여 테이블의 이름을 지정하고 (원하는 테이블 이름으로 대체), 파이프라인을 매 초 Kafka를 폴링하도록 설정합니다. 이 신속한 폴링은 거의 실시간 데이터 처리 요구를 지원합니다. dlt_kafka_stream() 함수는 read_kafka_stream()을 호출하여 Kafka 스트리밍을 DLT로 직접 통합하여 Databricks 환경 내에서의 관리 및 운영을 간소화합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n@dlt.table(name=\"여기에 바꿔야 할 DLT 테이블 이름\",\n           spark_conf={\"pipelines.trigger.interval\" : \"1 초\"})\ndef dlt_kafka_stream():\n    read_kafka_stream()\r\n```\n\n# 결론\n\n저희의 벤치마크 결과에 따르면, 델타 라이브 테이블 서버리스는 대기 시간 성능과 운영 간소화 측면에서 우수한 성과를 보여주며, 다양한 데이터 부하가 있는 시나리오에 매우 적합합니다. 한편, 데이타브릭스 잡과 델타 라이브 테이블도 실용적인 솔루션을 제공합니다.\n\n- 대기 시간 비교: 델타 라이브 테이블의 서버리스 버전은 모든 측정 백분위수에서 대기 시간 측면에서 다른 것들을 능가합니다.\n- 운영 복잡성: 델타 라이브 테이블 서버리스는 수동 인프라 관리가 필요하지 않은 가장 단순한 설정을 제공하며, 그 다음으로 델타 라이브 테이블, 그리고 데이타브릭스 잡이 이어집니다. \n\n<div class=\"content-ad\"></div>\n\n## Delta Live Tables Serverless가 표준 Delta Live Tables를 능가하는 이유\n\nDelta Live Tables Serverless가 표준 Delta Live Tables보다 우수한 성능을 발휘하는 핵심 요소는 Stream Pipelining을 활용한 것입니다. 표준 구조화된 스트리밍과 달리 DLT Serverless는 마이크로배치를 동시에 처리함으로써 처리량을 향상시키고 응답 시간을 크게 향상시킵니다. 이 기능을 통해 전체 컴퓨팅 자원 활용도도 크게 향상됩니다. Stream Pipelining은 서버리스 DLT 파이프라인의 기본 기능이며, 데이터 입력과 같은 스트리밍 데이터 워크로드를 위해 성능을 최적화합니다.\n\n또한, 수직 자동 스케일링은 DLT Serverless의 효율성을 향상시키는 중요한 역할을 합니다. 이 기능은 Databricks Enhanced Autoscaling의 수평 자동 스케일링 기능을 보완하여 DLT 파이프라인을 실행하는 데 필요한 가장 비용 효율적인 인스턴스 유형을 자동으로 선택합니다. 더 많은 자원이 필요할 때 적절하게 대규모 인스턴스로 확장하고, 메모리 사용률이 지속적으로 낮을 때 축소합니다. 이러한 다이내믹한 조정은 실시간 요구 사항에 기반하여 드라이버 및 워커 노드를 최적으로 조정하여 중요한 역할을 합니다. 제품 모드에서 파이프라인을 업데이트하거나 개발 중 수동으로 조정하는 경우에도, 수직 자동 스케일링은 메모리 부족 오류가 발생한 후 신속하게 더 큰 인스턴스를 할당하여 서비스 중단 없이 자원 할당을 최적화합니다.\n\nStream Pipelining과 수직 자동 스케일링은 운영 복잡성을 감소시키고 Serverless DLT 파이프라인의 신뢰성 및 비용 효율성을 향상시킵니다. 이러한 기능을 통해 Serverless DLT는 수동 개입을 최소화하면서 변동하는 데이터 입력 부하를 처리하는 데 이상적인 선택지가 되어 더 빠르고 효율적인 데이터 파이프라인 실행을 실현합니다.\n\n<div class=\"content-ad\"></div>\n\n# 각주:\n\n이 글을 읽어 주셔서 감사합니다. 도움이 되었거나 즐거우셨다면 박수를 치는 것을 고려해 주시고, 다른 사람들이 이를 발견할 수 있도록 도와주세요. 더 많은 정보를 찾으려면 제 웹사이트 CanadianDataGuy.com을 방문하시고, 더 많은 통찰력 있는 콘텐츠를 제공하기 위해 제 팔로우도 잊지 마세요. 여러분의 지원과 피드백은 제게 귀중하며, 제 작품에 대한 여러분의 참여를 감사히 여깁니다.","ogImage":{"url":"/assets/img/2024-06-19-NeedforSpeedBenchmarkingtheBestToolsforKafkatoDeltaIngestion_0.png"},"coverImage":"/assets/img/2024-06-19-NeedforSpeedBenchmarkingtheBestToolsforKafkatoDeltaIngestion_0.png","tag":["Tech"],"readingTime":10},{"title":"트위터가 매일 40억 건의 이벤트를 실시간으로 처리하는 방법","description":"","date":"2024-06-19 01:40","slug":"2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily","content":"\n\n## 람다에서 카파로\n\n# 목차\n\n- 문맥과 과제\n- 이전 아키텍처\n- 새로운 아키텍처\n- 평가\n\n# 소개\n\n<div class=\"content-ad\"></div>\n\n몇 주 전에는 우리가 어떻게 Uber가 실시간 인프라를 처리하여 매일 수백만 건의 이벤트를 처리하는지 배웠습니다. 이번 주에는 데이터 실시간 처리 요구 사항에 대한 다른 대형 기술 회사의 처리 방법을 살펴볼 것입니다: 트위터.\n\n# 맥락과 도전 과제\n\n트위터는 매일 400조 건의 이벤트를 실시간으로 처리하고 페타바이트(PB)의 데이터를 생성합니다. 이벤트는 다양한 소스(하둡, 카프카, 구글 빅쿼리, 구글 클라우드 스토리지, 구글 퍼브섭 등)에서 발생합니다. 트위터는 데이터의 대규모 규모에 대응하기 위해 각 요구 사항에 특화된 내부 도구를 구축했습니다: 배치 처리용 Scalding, 스트림 처리용 Heron, 배치 및 실시간 처리용 TimeSeries AggregatoR 프레임워크, 데이터 소비용 데이터 액세스 레이어.\n\n기술의 견고성에도 불구하고 데이터의 성장은 인프라에 압력을 가합니다; 가장 현저한 예는 상호 작용 및 참여 파이프라인인데, 이는 대규모 데이터를 배치 및 실시간으로 처리합니다. 이 파이프라인은 Tweet와 사용자 상호 작용 데이터를 다양한 수준의 집계 및 메트릭스 차원을 사용하여 추출하기 위해 다양한 실시간 스트림 및 서버 및 클라이언트 로그에서 데이터를 수집하고 처리합니다. 이 파이프라인의 집계 데이터는 트위터의 광고 수익과 다양한 데이터 제품 서비스의 진실의 원천 역할을 합니다. 따라서 이 파이프라인은 낮은 지연 시간과 높은 정확성을 보장해야 합니다. 트위터가 이 임무를 처리하는 방법을 살펴봅시다.\n\n<div class=\"content-ad\"></div>\n\n# 오래된 아키텍처\n\n## 개요\n\n![이미지](/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_0.png)\n\n트위터는 처음에 람다 아키텍처를 사용했습니다. 이는 정확한 일괄 데이터 뷰를 제공하는 일괄 처리와 온라인 데이터를 보여주는 실시간 스트림 처리 두 개의 별도 파이프라인이 있습니다. 두 뷰 출력은 하루가 끝날 때 합쳐집니다. 트위터는 다음과 같이 아키텍처를 구축했습니다:\n\n<div class=\"content-ad\"></div>\n\n- Summingbird Platform: 제가 이해한 바로는, 이 플랫폼에는 Scalding과 Heron과 같은 여러 분산 엔진들과 MapReduce 로직을 정의하고 이를 해당 엔진에서 실행할 수 있도록 허용하는 전용 라이브러리가 포함되어 있습니다.\n- TimeSeries AggregatoR: 견고하고 확장 가능한 실시간 이벤트 시계열 집계 프레임워크.\n- Batch: 배치 파이프라인의 소스는 로그, 클라이언트 이벤트 또는 HDFS의 트윗 이벤트에서 나올 수 있습니다. Summingbird 플랫폼으로 데이터를 전처리하고 결과를 맨해튼 분산 저장 시스템에 저장하기 위해 많은 Scalding 파이프라인이 사용됩니다. 비용을 절감하기 위해 Twitter는 배치 파이프라인을 한 데이터 센터에 배치하고 데이터를 다른 2개 데이터 센터에 복제합니다.\n- 실시간: 실시간 파이프라인의 소스는 Kafka 주제에서 나옵니다. 데이터는 Summingbird 플랫폼 내의 Heron으로 \"흘러들어가\", 그런 다음 Heron의 결과가 Twitter Nighthawk 분산 캐시에 저장됩니다. 배치 파이프라인과 달리, 실시간 파이프라인은 3개 서로 다른 데이터 센터에 배포됩니다.\n- 배치 및 실시간 저장소 위에 쿼리 서비스가 있습니다.\n\n## 도전\n\n실시간 데이터의 대규모 및 높은 처리량으로 인해 데이터 손실 및 부정확성의 위험이 있습니다. 이벤트 스트림에 대한 처리 속도가 따라가지 못할 경우, Heron 토폴로지에서 백프레셔가 발생할 수 있습니다 (방향성을 갖는 비순환 그래프는 데이터 처리의 Heron 흐름을 나타냅니다). 시스템이 어느 정도 동압을 겪으면, Heron 볼트(일종의 노동자로 생각할 수 있음)가 지연이 누적되어 전체 시스템 대기 시간이 길어질 수 있습니다.\n\n뿐만 아니라, 백프레셔로 인해 많은 Heron 스트림 매니저 (스트림 매니저는 토폴로지 구성 요소 간의 데이터 라우팅을 관리함)가 실패할 수 있습니다. Twitter의 해결책은 스트림 매니저를 다시 시작하여 스트림 매니저를 복구하는 것입니다. 그러나, 재시작은 행사 손실을 일으킬 가능성이 있어 파이프라인의 전반적 정확성을 줄일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 새 아키텍처\n\n## 개요\n\n![이미지](/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_1.png)\n\n새로운 접근 방식으로 트위터는 Kappa 아키텍처를 사용하여 하나의 실시간 파이프라인으로 솔루션을 간소화했습니다. 이 아키텍처는 트위터 내부 및 Google Cloud Platform 솔루션을 활용할 것입니다:\n\n<div class=\"content-ad\"></div>\n\n- 온프레미스: 그들은 카프카 토픽 이벤트를 구글 파브섭 이벤트 형식으로 변환하는 전처리 서비스를 구축했습니다.\n- 구글 클라우드: 이벤트 들어오기 위해 파브섭을 사용하고, 중복 제거 및 실시간 집계에는 Dataflow 작업을 활용하며, 결과를 저장하기 위해 BigTable을 사용합니다.\n\n새로운 아키텍처의 프로세스 흐름은 다음과 같이 설명할 수 있습니다:\n\n- 단계 1: 소스 카프카 토픽에서 데이터를 소비하고 변환 및 필드 재매핑을 수행한 후 최종 결과를 중간 카프카 토픽으로 보냅니다.\n- 단계 2: 이벤트 프로세서는 중간 카프카 토픽에서 데이터를 파브섭 형식으로 변환하고 이벤트에 데이터 중복 제거를 위해 사용되는 UUID 및 처리 컨텍스트와 관련된 일부 메타정보를 추가합니다.\n- 단계 3: 이벤트 프로세서는 이벤트를 구글 파브섭 토픽으로 보냅니다. 트위터는 메시지가 구글 클라우드로 적어도 한 번 방식으로 전달되도록 PubSub을 게시하는 이 프로세스를 거의 무한정 재시도합니다.\n- 단계 4: 구글 Dataflow 작업은 파브섭에서 데이터를 처리합니다. Dataflow 작업자는 실시간으로 데이터 중복 제거 및 집계를 처리합니다.\n- 단계 5: Dataflow 작업자는 집계 결과를 BigTable에 기록합니다.\n\n# 평가\n\n<div class=\"content-ad\"></div>\n\n## 새로운 접근 방식의 성취\n\n- 오래된 아키텍처의 10초에서 10분 지연과 비교하여 대략 10초의 지연이 안정적으로 유지됩니다.\n- 실시간 파이프라인은 최대 ~100MB/s 인 이전 아키텍처 대비 대략 1GB/s의 처리량을 달성할 수 있습니다.\n- Google Pubsub에 최소 한 번 데이터 게시 및 데이터 흐름으로 부터의 중복 제거 작업으로 거의 정확히 한 번 처리 보장.\n- 일괄 처리 파이프라인 구축 비용 절감.\n- 더 높은 집계 정확도 달성.\n- 늦게 발생하는 이벤트 처리 기능.\n- 다시 시작 시 이벤트 손실 없음\n\n## 중복 비율 모니터링 방법\n\n<img src=\"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_2.png\" />\n\n<div class=\"content-ad\"></div>\n\n트위터는 두 개의 별도 Dataflow 파이프라인을 생성합니다: 하나는 Pubsub에서 원시 데이터를 직접 BigQuery로 전달하고, 다른 하나는 중복 제거된 이벤트 카운트를 BigQuery로 내보냅니다. 이 방식으로 Twitter는 중복 이벤트 백분율 및 중복 제거 후의 백분율 변경을 모니터링할 수 있습니다.\n\n## 이전 배치 파이프라인의 중복 제거된 개수를 새 Dataflow 파이프라인과 비교하는 방법은?\n\n![image](/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_3.png)\n\n- BigTable에 쓰는 것 외에도, 새 워크플로우는 중복 제거된 및 집계된 데이터를 BigQuery로 내보냅니다.\n- 트위터는 또한 이전 배치 데이터 파이프라인 결과를 BigQuery로 로드합니다.\n- 중복 카운트를 비교하기 위해 예약된 쿼리를 실행합니다.\n- 결과는 새로운 파이프라인 결과 중 95% 이상이 이전 배치 파이프라인과 정확히 일치한다는 것입니다. 5%의 차이는 주로 원래 배치 파이프라인이 지연된 이벤트를 버린 반면, 새 파이프라인은 효율적으로 포착할 수 있기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n# 마무리말\n\nTwitter가 새로운 Kappa 아키텍처로 전환함으로써, 예전 아키텍처와 비교하여 지연 시간과 정확성 면에서 크게 개선되었습니다. 더 나은 성능 뿐만 아니라, 새로운 아키텍처는 데이터 파이프라인을 간소화하여 스트림만을 유지했습니다.\n\n다음 블로그에서 뵙겠습니다.\n\n# 참고문헌\n\n<div class=\"content-ad\"></div>\n\n[1] Lu Zhang and Chukwudiuto Malife, 트위터에서 실시간으로 수십억 개의 이벤트 처리하기 (2021)","ogImage":{"url":"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_0.png"},"coverImage":"/assets/img/2024-06-19-HowTwitterprocesses4billioneventsinreal-timedaily_0.png","tag":["Tech"],"readingTime":5},{"title":"보안 데이터 플랫폼 구축 팀 구조 및 접근 전략에 대한 안내","description":"","date":"2024-06-19 01:38","slug":"2024-06-19-BuildingSecureDataPlatformsAGuideforTeamsStructureAccessStrategies","content":"\n\n몇 달 동안 데이터 솔루션을 개발하며, 엔지니어와 데이터 소유자들은 증가된 데이터 액세스 요청으로 인한 병목 현상에 직면할 수 있습니다(전체 테이블, 스키마, 서브셋, 민감한 데이터 등). 응용 프로그램 기술 사용자나 셀프 서비스 이용자와 관련이 있는 이러한 상황을 처리하는 것이 어렵거나 스트레스를 받을 수 있습니다. \n\n데이터 아키텍처, 데이터가 어떻게 구조화되고 모델링되는지, 데이터 액세스 전략과 패턴, 기술 등이 이러한 시나리오를 다루는 데 얼마나 어렵거나 원활한지에 영향을 미칩니다. \n\n이 기사에서는 조직이 데이터 제품을 구조화하고 데이터 플랫폼의 데이터 액세스 전략을 설계할 수 있는 방법에 대해 논의합니다. 유연하고 빠르며 안전한 데이터 소비를 보장합니다. 또한 데이터 아키텍처와 조직 구성이 확장 가능성에 영향을 미치는 다양한 전략과 잠재적인 아키텍처 및 전략을 통해 병목 현상에 대한 대응 전략을 살펴봅니다. 사용자와 팀의 액세스 수명주기 및 일부 기술이 프로세스를 용이하게 할 수 있는 방법에 대해 이야기합니다. 모든 동안 팀과 조직이 그러한 솔루션을 개발하고 적응하는 데 직면할 수 있는 제약 사항을 고려합니다.\n\n# 목차\n\n<div class=\"content-ad\"></div>\n\n1. 소개\n\n2. 장면 설정\n\n3. 간단한 역할 기반 데이터 액세스\n\n4. 팀 내 세밀한 액세스 제어\n\n<div class=\"content-ad\"></div>\n\n5. 다국적 데이터 접근\n\n6. 요약 및 주요 포인트\n\n7. 참고 자료\n\n# 1. 소개\n\n<div class=\"content-ad\"></div>\n\n아키텍처와 솔루션을 상세히 설명하기 전에, 왜 이 주제를 다뤄야 하고 몇 분 동안이라도 이에 집중하는 것이 중요한 지 강조해보겠습니다.\n\n다양한 IT 기관에서 일하고 데이터 솔루션을 구축해온 제 경험과 많은 전문가 및 동료들로부터 받은 피드백들을 종합해보면, 현대 데이터 플랫폼을 구축할 때 이러한 중요한 측면에 대해 자세히 다룬 자료가 부족하다는 결론에 도달하게 되었습니다.\n\n빠른 전달 속도는 종종 초기 개념 설계에 대한 노력이 제한되어 장기적으로 확장성을 방해하는 병목 현상을 일으킵니다. 많은 경우, 이는 재설계 작업 및 추가 개발 노력을 수반하게 됩니다.\n\n이 주제에 대한 중요성은 더 많은 최근 사건들을 살펴볼수록 더욱 빛을 발합니다. 민감한 고객 데이터가 유출된 사건들을 생각해보면요.\n\n<div class=\"content-ad\"></div>\n\n글로벌 기관이 개인 데이터를 처리하는 것에 대한 고객의 신뢰가 상실되었습니다. 몇 가지 이벤트로 인해 액세스 권한으로 인한 800만 명 이상의 사용자 데이터 유출이 발생했습니다. 직원들이 중요한 민감 데이터에 액세스하면서 소셜 엔지니어링에 속아들기도 했고, 때로는 정부 기관까지 영향을 끼쳤습니다. 이 같은 사례가 몇 년 동안 계속되었지만, 이해하셨을 것입니다.\n\n고객의 신뢰는 첫 번째 영향이며, 기관이 사용자 데이터를 보호하는 데 책임이 있다고 판명되면 법적 결과까지 번질 수 있습니다. GDPR, CCPA 등의 법적 틀이 조직이 올바른 방어 조치를 마련하여 이러한 실수를 피하기 위해 설정된 데이터 보호 조치를 의무화합니다.\n\n이제 데이터 액세스 관리를 위한 강력하고 안전한 전략이 중요하다는 사실을 간단히 명확히 한 바 있으니, 이를 실현하기 위한 방법에 대해 이야기해 보겠습니다.\n\n# 2. 상황 설정\n\n<div class=\"content-ad\"></div>\n\n특정 데이터 아키텍처에 대한 해결책을 제시하는 것은 쉽지만, 이는 현실을 반영하지 않습니다. 조직은 다양한 데이터 아키텍처(Data Mesh, 중앙 데이터 웨어하우스, Lakehouse 등)와 구성원 구조를 가지고 있어 구현 가능한 범위에 제한을 둡니다.\n\n우리는 다양한 아키텍처를 고려한 여러 사용 사례에 대해 논의할 것이며, 이는 명확한 해결책 전략으로 나아가는 첫걸음이 될 것입니다.\n\n해결책과 사용 사례를 더 쉽게 이해하기 위해 기본 사례부터 시작하여 추가 요구사항을 시뮬레이션하여 확장해 나갈 것입니다.\n\n기본 사용 사례는 다음과 같이 시작됩니다:\n\n<div class=\"content-ad\"></div>\n\n- 중심이 되는 데이터 팀이 메달리온 기반 데이터 아키텍처를 관리합니다.\n- 데이터 통합 프로세스는 ETL 또는 ELT 방식을 따를 수 있습니다.\n- 여러 소스에서 데이터를 수집하고 셀프 서비스 및 기술 사용자 소비자(응용 프로그램)를 위한 데이터 액세스를 활성화합니다.\n\n데이터 자산에 대한 접근이 주요 관심사가 될 것입니다. 이 아키텍처에는 각 단계별 데이터 자산이 포함될 것입니다. 사용 사례에 따라 소비자들은 소비 사용 사례에 맞는 다양한 단계에 액세스가 필요할 수 있습니다.\n\n우리가 답변하고자 하는 질문은 누가 무엇에 액세스할 것이며 어떻게 하는가인가요?\n\n<div class=\"content-ad\"></div>\n\n이것은 소규모 조직이나 스타트업에서 발견되는 전형적인 설정입니다.\n\n데이터 레이크/하우스 조직은 베이스 메달리온 단계로 나눠집니다. 액세스는 고수준 팀 역할에 따라 분리됩니다.\n\n베이스 케이스 가정\n\n- 사용자들은 최종 품질 테스트된 데이터 제품에만 액세스가 필요합니다.\n- 데이터 제품에는 고객/조직의 민감한 정보가 포함되어 있지 않습니다.\n- 액세스는 조직 내의 셀프 서비스 소비자에 대해 유연합니다.\n\n<div class=\"content-ad\"></div>\n\n최종적으로, 데이터 이용자들은 Gold 단계 내에서 개발된 데이터 제품에 액세스할 수 있어야 합니다. 경우에 따라서는 업무 지속성과 확장을 보장하기 위해 용량이나 시간 제약 때문에 우회되기도 합니다.\n\n각 조직이 데이터 과학자와 분석가 역할에 대해 고유한 정의를 가지고 있다는 점을 인지하는 것이 중요합니다. 여기서 논의하는 맥락에서, 데이터 과학자는 예를 들어 ML 모델을 개선하기 위해 원시 데이터에 접근할 수도 있습니다. 그러나 데이터 과학자들의 구성원은 데이터 분석가 그룹 내에서도 존재할 수 있습니다.\n\n팀을 엔지니어, 분석가 및 과학자로 나누는 것은 보다 세분화된 역할 기반의 액세스 제어 (RBAC)로 나아가는 첫 걸음이 될 것입니다. 이는 조직 전반에 걸쳐 데이터 이용 가능성을 보장하면서도 액세스 범위를 제한하는 것을 의미합니다.\n\n이와 같은 설정에서 준수하는 민감한 데이터 처리를 보장하는 주제는 다른 기사에서 다루고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 시나리오는 단기적으로는 작동할 수 있지만, 조직, 팀 구조 및 액세스 프로세스가 더 복잡해질 것입니다.\n\n## 4. 팀 내의 세분화된 액세스 제어\n\n단순한 액세스 제어는 복잡한 액세스 제어 요구 사항에 대응하기 어려울 수 있습니다. 본 섹션에서는 팀이 액세스 요구 사항에 더 많은 복잡성과 유연성을 도입할 수 있는 잠재적인 아키텍처와 전략을 정의합니다.\n\n아키텍처 설정에 대한 예상된 지형을 구성하는 것부터 시작해보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n- 분석가 분배: 분석가들은 부서/제품팀 사이에 나뉘어 있습니다.\n- 역할 계층구조: 엔지니어 및 분석가들은 조직 구조에 따라 주니어, 시니어 등으로 더 세분화됩니다.\n- 데이터 접근 변동성: 팀 구성원들은 동일한 데이터에 접근 권한을 가지지 않을 수 있습니다.\n- 개인 데이터 기여: 팀 구성원들은 자신의 데이터/자산을 데이터 플랫폼에 제공할 수 있습니다.\n\n이러한 점들을 한 단계씩 처리해 보겠습니다.\n\n- 점 1은 추가적인 팀을 소개합니다.\n- 점 2와 3은 팀 당 추가 역할이 필요합니다.\n- 점 4는 팀 데이터 분석가에게 새로운 요구사항/권한을 제공합니다.\n\n이러한 설정으로 시각적으로 어떻게 보일지 살펴보고, 그 후에 다이어그램 구성 요소를 명확히 설명해보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## 팀 설정\n\n비즈니스/IT 조직을 이해하세요. 올바른 그룹 및 액세스 전략을 설정하려면 사용 및 액세스 패턴을 이해해야 합니다. 이러한 설정은 계속 변경될 수 있습니다. 자동 액세스 및 역할 할당 프로세스를 구축하면 병목 현상을 완화할 수 있습니다.\n\n도표는 팀 수준에서 그룹이 할당되는 방식을 설명합니다. 서브 그룹은 권한을 상속하고 확장하여 세밀한 액세스 제어를 보장하기 위해 추가로 생성됩니다.\n\n서브그룹은 읽기/쓰기 권한 및 민감한 데이터 보호를 위한 추가 보안 계층을 제공하는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n팀의 분리 방법은 확정되어 있지 않습니다. 여러 범위와 수준을 자유롭게 시험해보세요. 그러나 효과적으로 관리하지 않으면 지나치게 복잡한 계층 구조가 될 수 있습니다.\n\n기술 사용자\n\n어플리케이션에 사용되는 기술 사용자들은 팀 그룹에 추가할 수도 있습니다. 이를 통해 데이터 범위를 셀프 서비스 사용자에게도 포함시키는 데 도움이 됩니다. 기술 사용자를 위해 별도의 하위 그룹을 만들면 여러 종류의 사용자를 쉽게 모니터링하고 관리할 수 있습니다.\n\n## 데이터 레이크/하우스 구조화\n\n<div class=\"content-ad\"></div>\n\n데이터가 데이터 레이크나 데이터 웨어하우스 내에서 구조화되고 조직화되는 방식은 액세스 제어 전략의 유연성을 지원하거나 제한할 수 있습니다.\n\n아키텍처에 따라 각 변환 수준에 다른 이름/레이블이 할당됩니다 (다이어그램 참조). 일부는 서로 다른 기능을 수행할 수도 있지만, 액세스 계층은 동일한 패턴을 따릅니다. 다이어그램은 데이터베이스/데이터 레이크, 스키마, 테이블, 행/열 수준의 각 팀별로 액세스 권한이 어떻게 나뉘어지는지 보여줍니다.\n\n보안 요구 사항에 따라 팀들이 데이터베이스 수준에서 더 분리되어야 할 수도 있습니다. 다시 말하지만, 이는 실제로 가능한 일이지만, 확장성과 복잡성 요소에 대해 고려하는 것이 중요합니다.\n\n내가 경험한 또 다른 액세스 전략은 액세스를 제한하기 위해 뷰를 사용하는 것이었습니다. 이는 요구 사항을 충족시키지만, 병목 현상은 뷰를 작성하고 관리하는 데서 나타납니다. 필요한 그룹에 사용자를 간단히 추가하는 속도가 뷰에 비해 일부 시나리오에서 더 나은 결과를 도출할 수 있습니다 (성장 및 확장이 제한된 소규모 팀의 경우).\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-BuildingSecureDataPlatformsAGuideforTeamsStructureAccessStrategies_0.png)\n\n# 5. 다국적 데이터 접근\n\n마지막 섹션에서 논의한 내용을 국제적 수준으로 확장하기 위해 국가 차원을 그룹에 추가합니다.\n\n세계적 규모로 운영하는 기업은 전 섹션에서 논의된 데이터 보안 및 규정 준수 주제에 관해서 다양한 어려움을 겪게 됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n데이터 분리 및 지역성 규칙을 다양한 법적 요구 사항에 맞추는 것은 도전적입니다. 다행히 현재는 액세스 측면을 논의하고 있으며 데이터 준수 법률 컨설팅은 아직 논의하지 않고 있습니다.\n\n그렇다면 이러한 아키텍처는 어떻게 보일까요? 우리가 위에서 스케치한 다이어그램을 다국적 수준에 맞게 조정해 보겠습니다.\n\n각각이 포함하는 구성 요소를 자세히 설명하는 두 가지 아키텍처 레이아웃이 있습니다. 이는 사업 단위 및 부서 수준으로 더 자세히 들어가지만, 그것은 세분화된 액세스 섹션에서 언급한 내용을 확장하는 것에 불과할 것입니다.\n\n일부 팀이 다국적으로 확장될 수 있다는 점도 보여주는 것이 중요합니다. 조직 구성에 따라 그룹은 적절하게 조정되어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n관심이 있다면, 자동 설정 및 구성을 위해 관련 아키텍처를 자세히 살펴보고 싶다면, 주제에 대한 다른 기사를 확인하실 수 있습니다.\n\n# 요약 및 주요 포인트\n\n팀 및 구성원들에 대한 올바른 액세스 전략을 보장하는 것은 완전하고 안전한 데이터 아키텍처를 위한 중요한 요소입니다. 우리는 팀 수준에서 액세스 그룹을 구분하고, 데이터베이스/데이터 레이크, 변환 단계, 테이블, 그리고 열/행 수준별로 더 나누는 중요성에 대해 논의했습니다. 우리가 논의한 내용에서 얻을 수 있는 주요 포인트는 다음과 같습니다:\n\n- 데이터 아키텍처와 비즈니스 조직은 성공적인 액세스 전략을 설정하는 데 중요합니다.\n- 데이터 자산에 액세스하는 구성원들은 항상 자동화된 액세스 그룹의 구성원이어야 하며, 그들의 액세스는 로그 기록 및 모니터링되어야 합니다.\n- 법적 요구 사항은 데이터 아키텍처 및 액세스 그룹을 조정해야 할 수 있습니다.\n- 그룹에 대한 액세스를 쉽게 조절하고 변경할 수 있는 원활한 프로세스를 구축함으로써, 액세스 제어 전략을 관리하는 데 속도, 유연성, 확장성을 제공할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n구독하시면 새 이야기를 게시할 때 알림을 받을 수 있어요.\nLinkedIn에서 언제든지 연락 주시면 됩니다.\n\n더 비슷한 기사를 찾고 계시다면 아래 목록을 확인해보세요:\n\n## 참고 자료","ogImage":{"url":"/assets/img/2024-06-19-BuildingSecureDataPlatformsAGuideforTeamsStructureAccessStrategies_0.png"},"coverImage":"/assets/img/2024-06-19-BuildingSecureDataPlatformsAGuideforTeamsStructureAccessStrategies_0.png","tag":["Tech"],"readingTime":7},{"title":"아이스버그 카탈로그 당신의 레이크하우스의 TIP","description":"","date":"2024-06-19 01:36","slug":"2024-06-19-IcebergCatalogTheTIPofyourLakehouse","content":"\n\n아이스버그 카탈로그 랜드스케이프는 Snowflake와 Databricks로부터 중요한 발표가 속출하며 급속하게 변화하고 있어요. 이 다채로운 생태계에 TIP를 소개합니다. HANSETAG가 선보이는 이 Rust-native Iceberg REST 카탈로그는 데이터 품질, 거버넌스, 그리고 유연성을 우선시합니다. 변경 이벤트, 계약 유효성 검사, 멀티 테넌시와 함께 경량화되고 맞춤형 솔루션으로 혁신을 추구해요.\n\n![이미지](/assets/img/2024-06-19-IcebergCatalogTheTIPofyourLakehouse_0.png)\n\n이제 몇 날 뒤 데이터 랜드스케이프에서는 지표적인 변화가 일어나고 있어요. 특히, Snowflake는 공개 소스 아이스버그 REST 카탈로그인 Polaris를 발표했고, Databricks는 Apache Iceberg를 만든 파이오니어 회사 Tabular.io를 인수했어요. 더불어 Databricks는 아이스버그 REST(읽기) 지원을 하는 Unity Catalog도 오픈소스로 공개했죠. 이제 요즘에는 데이터 회사로 진정으로 인정받으려면 아이스버그 카탈로그의 구현을 오픈소스화해야 하는 것 같아요.\n\n이미 화요일이 다가왔고, 의심스러운 침묵이 느껴집니다. 이번 주에 새로운 카탈로그가 나오지 않았어요. 하지만 걱정하지 마세요! 저희가 TIP를 소개해 드릴게요. 이는 Rust-native, 멀티 테넌트, 싱글 바이너리로 Iceberg REST 카탈로그를 구현한 것이에요. 하지만 주의해 주세요, 우리는 약간 다르게(알려진 대로 😉) 그리고 기존 카탈로그와 다른 방식으로 일을 진행하고 있어요. 이에 대해 더 자세히 설명하겠어요. 함께 시작해 볼까요?\n\n<div class=\"content-ad\"></div>\n\n# 우리는 누구인가\n\nHANSETAG에서는 회사들이 데이터 자산을 구축, 공유 및 활용할 수 있는 첨단 데이터 제품 플랫폼을 구축합니다. 저희 플랫폼은 데이터 계약서에 기록된 강제 SLO를 통해 지속적으로 높은 데이터 품질을 보장합니다. 플랫폼을 구동하기 위해 신뢰할 수 있는 멀티 테넌트 Iceberg 카탈로그가 필요합니다. 기존 구현이 우리의 요구를 충족하지 못했기 때문에 Apache 라이센스 하에 자체 카탈로그를 구축했습니다. 기여해 주시기 바랍니다! 😊\n\n# 데이터 레이크하우스\n\n데이터 레이크하우스는 데이터 및 분석 프로젝트의 기본 아키텍처가 되었으며 데이터 메쉬 구현을 위한 기반을 제공합니다. 데이터 레이크의 유연성과 데이터 웨어하우스의 구조와 편의성을 결합합니다. 모든 레이크하우스가 동일하지 않다는 점을 인식하는 것이 중요합니다. 레이크하우스에는 Apache Iceberg, Delta Lake 및 Apache Hudi와 같은 세 가지 테이블 포맷이 등장했습니다. Iceberg의 신속한 개발과 뛰어난 벤더 독립적인 커뮤니티로 인해 HANSETAG에서는 초기 레이크하우스 형식으로 Iceberg를 선택했습니다.\n\n<div class=\"content-ad\"></div>\n\n# 아이스버그 및 REST 카탈로그\n\n아파치 아이스버그 카탈로그는 데이터 레이크하우스의 핵심으로 대규모 데이터 세트의 효율적인 관리 및 조직을 제공합니다. 이는 네임스페이스, 테이블 및 뷰의 중앙 저장소 역할을 하며, 개별 객체에 대한 세밀한 접근을 관리합니다.\n\n아이스버그는 REST 이외의 다른 카탈로그를 지원합니다. 그러나 최근 시장 활동에 의해 강조된 것처럼, 아이스버그 REST 카탈로그가 가장 밝은 미래를 가지고 있습니다. 또한 현재의 아이스버그 카탈로그 중 유일한 것으로, 클라이언트 측에서 저장 자격 증명을 지정하지 않고 데이터에 대한 클라이언트 접근을 허용합니다. 아이스버그 커뮤니티는 REST 명세의 새 버전의 일부로 모든 다른 카탈로그를 REST 카탈로그를 통해 보완할 것을 고려하고 있습니다.\n\n# TIP가 다른 점\n\n<div class=\"content-ad\"></div>\n\n카탈로그를 구축하는 것은 로켓 과학은 아니지만 경솔하게 결정한 것은 아닙니다. 우리 플랫폼에서 데이터 품질과 데이터 거버넌스를 우선시하며 사용자들이 자체 호스팅이든 클라우드에 있든 데이터에 대해 완전한 통제를 할 수 있기를 바랍니다. 결과적으로 기존 표준을 혁신하는 방법으로 카탈로그를 구축했습니다:\n\n- 멀티 테넌트: 하나의 TIP 배포로 여러 프로젝트를 서비스할 수 있습니다. 각 프로젝트는 여러 창고를 포함할 수 있습니다. 창고와 프로젝트는 실행 중에 REST-API를 통해 동적으로 추가되며 각각 다른 저장 위치에 있습니다.\n- 변경 이벤트: 외부 시스템이 우리 테이블에 어떤 변경이 가해졌는지 알 수 있어야 합니다. 우리는 각 테이블 변경 시 이벤트(Cloudevents)를 발생시킵니다. 예를 들어 테이블 스키마가 변경될 때 이벤트를 발행합니다.\n- 변경 승인 / 계약 검증: 데이터 계약은 HANSETAG에서 하는 주요 작업입니다. 변경 이벤트를 통해 스키마 변경을 알 수 있지만, 데이터 계약에 영향을 미칠 수 있으며 이로 인해 하류 파이프라인이 모두 손상될 수 있습니다. TIP를 사용하면 ContractVerification 트레이트를 사용하여 해당 변경 사항을 금지할 수 있습니다. 데이터 계약의 오픈 소스 구현에 주목하세요!\n- Rust로 작성: 단일 작고 통합된 이진 파일. JVM이나 Python 환경이 필요하지 않습니다.\n- 사용자 정의 가능: TIP는 확장 가능하도록 설계되었지만 필수 구성요소가 함께 제공됩니다. 향후 통합을 추가하고 여러분이 몇 가지 메서드를 구현하여 자체 통합을 작성할 수 있도록 키 인터페이스를 공개합니다. 이러한 인터페이스는 다음과 같습니다:\n   - 백엔드 데이터베이스 (카탈로그): Postgres 외부에 내장\n   - 시크릿 저장소: Postgres 내장, Vault 작업 진행 중\n   - 이벤트 시스템: NATS 내장\n   - 인증: OIDC 내장, Zitadel 작업 진행 중\n   - 권한: 곧 OpenFGA 내장 예정\n   - 계약 검증: 곧 출시될 데이터 계약 라이브러리 지원\n- 저장소 액세스 관리: 클라이언트와 S3 자격 증명을 공유하지 않고 자체 호스팅 및 AWS S3를 지원하는 내장된 S3-Signing을 제공합니다. 또한 판매 자격 정보에 대해 작업 중입니다!\n- 외부 잘 정의된 접근 (FGA): TIP는 권한을 내부적으로 저장하지 않으며 결코 저장하지 않습니다. 인증을 구현하기 위해 일부 메서드를 구현하여 다른 시스템과 통합할 수 있습니다.\n\n# 시작하기\n\n```js\ngit clone https://github.com/hansetag/iceberg-catalog.git\ncd iceberg-catalog/examples\ndocker compose up\n```\n\n<div class=\"content-ad\"></div>\n\nGitHub에서 더 많은 자세한 내용을 확인하러 가보세요! 그리고 별표 한 개 꾸욱 눌러주세요!\n\nGithub: https://github.com/hansetag/iceberg-catalog\nHANSETAG: https://hansetag.com/","ogImage":{"url":"/assets/img/2024-06-19-IcebergCatalogTheTIPofyourLakehouse_0.png"},"coverImage":"/assets/img/2024-06-19-IcebergCatalogTheTIPofyourLakehouse_0.png","tag":["Tech"],"readingTime":4},{"title":"마라톤 결과의 연령 그레이딩을 위한 더 나은 시스템이 있을까요","description":"","date":"2024-06-19 01:33","slug":"2024-06-19-IsThereaBetterSystemforAgeGradingMarathonResults","content":"\n\n\n<img src=\"/assets/img/2024-06-19-IsThereaBetterSystemforAgeGradingMarathonResults_0.png\" />\n\n서로 다른 두 명의 러너 사이에 경주 결과를 공정하게 비교하려면 어떻게 해야 할까요?\n\n예를 들어, 세 명의 러너가 모두 3:00에 마라톤을 완주했다고 가정해 봅시다. 표면적으로 보면 세 명 모두 잘 한 것으로 말할 수 있습니다.\n\n하지만 그 세 명의 러너가 25세 남성, 40세 여성, 60세 남성이라면 어떨까요? 그 시간은 각각의 러너에게 동일한 만큼 힘든 것이 아닙니다. 그렇다면 누가 가장 우수한 성적을 거뒀을까요?\n\n\n<div class=\"content-ad\"></div>\n\n지난 몇 십 년 동안 연령 등급이 시도해 온 질문입니다. 최근 연재한 몇 개의 기사에서 이 문제를 탐구해 왔습니다.\n\n전통적인 연령 등급 체계는 유용하며 비교를 하나의 방법으로 분류하는 데 도움이 됩니다. 그러나 그것은 결함이 없는 것은 아닙니다.\n\n성적을 하나의 숫자로 단순화한다는 점 때문에 항상 정확하고 신뢰할 수 있는 객관적인 측정이라고 생각하는 함정에 빠지기 쉽습니다.\n\n마라톤 성적의 비교적인 강도를 대략적으로 나타내기 위해 몇 가지 대규모 결과 데이터 세트를 수집해 분석한 결과, 성적을 연령별 백분위수로 비교하는 대안을 제시하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 시리즈를 따라오셨다면 다음 두 링크가 가장 흥미로울 것입니다:\n\n- 2023 데이터를 사용하여 업데이트된 (그리고 향상된) 연령 등급 계산기 버전을 확인할 수 있습니다.\n- 퍼센타일을 계산하는 데 사용한 2023 데이터셋의 공개 버전이 여기 있습니다. Kaggle에서 데이터를 사용하거나 CSV 형식으로 전체 데이터셋을 다운로드할 수 있습니다.\n\n# 데이터에 관한 주요 사항\n\n이 문제에 대해 처음 작업을 시작할 때, 2010년부터 2019년까지의 마라톤 샘플로 구성된 데이터셋으로 시작했습니다. 그 순간에는 명확한 결과를 고려하지 않고 데이터를 탐색했습니다.\n\n<div class=\"content-ad\"></div>\n\n퍼센타일이 탐색할 수 있는 좋은 옵션이라고 결정한 후, 2023년 미국 내 모든 마라톤 대회를 포함한 새 데이터셋을 수집했어요.\n\n만약 데이터 분석 도구를 잘 다룬다면, 전체 데이터셋을 Kaggle에 업로드했어요: 2023 마라톤 결과. 또한 데이터를 불러오고 데이터를 간단히 탐색하는 샘플 노트북도 공유했어요.\n\n전체 데이터셋은 641개의 레이스와 42만 9천개가 넘는 개별 완주 시간을 포함하고 있어요.\n\n그러나 퍼센타일 테이블을 계산하는 데 사용한 데이터는 약간 더 작아요. 일부 결과에는 연령 데이터가 없어서 제외되었어요. 또한 LA 마라톤, CIM, 콜로라도와 같은 몇몇 레이스 데이터가 부족했고, 나중에 데이터셋에 추가되었어요.\n\n<div class=\"content-ad\"></div>\n\n아래 공유하는 분석은 대략 40만 건의 경주 결과를 기반으로 합니다. 추가 데이터를 수집한 후 90번째 백분위를 확인하여 유의미한 차이가 있는지 확인했고, 그 차이는 단 10초 정도였습니다.\n\n# 전통적인 연령 평가 방식은 어떻게 작동합니까?\n\n그렇다면 전통적인 연령 평가 방식은 무엇이며, 왜 우리가 가진 방법을 그대로 사용하지 않아야 하는지 알아야 할까요?\n\n전통적인 연령 평가 방법론은 지난 몇 10년 동안 발전해 왔습니다. 하지만 핵심적으로 이것이 어떻게 작동하는지 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n신뢰도 있는 경험적 관찰과 통계 분석을 통해 세계 마스터스 애슬레틱스는 사람이 나이를 먹을수록 얼마나 느려지는지를 추정하는 표를 만들었습니다. 선수의 시간을 나이 계수로 곱하면 나이에 비해 얼마나 빠른지를 알 수 있는 나이 등급 시간이 나옵니다 - 더 어려운 선수일 때의 동등한 시간입니다.\n\n따라서, 60세 남성의 원래 예시에서 나이 계수는 0.8331입니다. 만약 그가 3시간(10,800초) 동안 경주를 한다면, 나이 등급 시간은 2:29:58이 될 것입니다 (0.8331 * 10,800).\n\n25세 남성의 경우, 3시간 결과는 좋지만 놀라운 것은 아닙니다. 그러나 60세 남성의 경우, 이는 25세 남성에게 2:30 마라톤에 해당한다고 여겨집니다 - 정말 놀라운 것입니다.\n\n여기서 멈춰서 시간을 비교할 수도 있습니다. 또는 마지막 단계로 넘어가서 나이 등급 시간을 그 이벤트의 표준 시간으로 나눌 수도 있습니다 - 일반적으로 표가 작성된 당시 오픈 세계 기록입니다.\n\n<div class=\"content-ad\"></div>\n\n남자 마라톤의 이전 세계 신기록 2:01:39를 나누면 결과물인 2:29:58의 점수는 81.12%가 됩니다. 수여를 위해 나이 그레이딩 결과를 제공하는 경우 레이스 결과에서 일반적으로 볼 수 있는 것이죠. 이것은 성적 등급 퍼센트(PPL)라고도 불리워요.\n\n# 그러면, 이 방법에는 무엇이 문제인가요?\n\n저는 전통적인 나이 그레이딩 시스템에 몇 가지 비판과 문제점이 있지만, 가장 중요한 것은 두 가지로 요약할 수 있다고 생각해요: a) 교정과 b) 유용성.\n\n## 교정에 대한 명확한 문제점들이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 두 문제 중에서 더 문제가 되는 것은 때로는 나이 등급 계산이 신뢰할 수 없다는 점이며, 일부 연령군이 다른 연령군보다 좋은 점수를 획들기 쉽다는 것입니다.\n\n이 시리즈의 이전 기사에서는 2023년에 나이 등급별 상위 1,000명을 조사하여 나이와 성별별 분석을 살펴봤습니다. 여기 몇 가지 데이터 점:\n\n- 35세 미만 남성은 전체 샘플에서 1,000명 중 221명을 대표했지만, 나이 등급별 상위 1,000명 중 368명을 획득했습니다.\n- 45~59세 남성은 샘플에서 1,000명 중 65명을 대표했지만, 나이 등급별 상위 1,000명 중 39명만을 획득했습니다.\n- 60~64세 여성은 샘플에서 1,000명 중 14명을 대표했지만, 나이 등급별 상위 1,000명 중 27명을 획득했습니다.\n- 65~69세 여성은 샘플에서 1,000명 중 단 6명이었지만, 나이 등급별 상위 1,000명 중 20명을 획득했습니다.\n\n나이 등급이 실제로 경기장을 평평하게 만들고 다른 연령군과 성별을 비교할 수 있는 신뢰할 수 있는 방법을 제공한다면, 상위 1,000명의 분포는 일반 러너들의 분포와 유사해야 합니다. 그러나 일부 뚜렷한 불일치가 있습니다.\n\n<div class=\"content-ad\"></div>\n\nBoston Marathon 예선 시간 분석에서 나타난 이 문제의 또 다른 예는 남성의 경우입니다.\n\n35세 미만 남성의 예선 시간은 가장 높은 연령 등급 결과 중 하나가 필요하며, 질 충전 비율도 가장 낮다는 것은 예상할 수 있습니다. 이 부분은 이해됩니다.\n\n그러나 여성의 경우에는 명백한 문제가 있습니다. 50대 여성의 예선 시간은 20대, 30대 또는 40대 여성의 시간보다 높은 연령 등급 결과가 필요합니다. 이러한 시간이 동등하게 어려울 경우, 50대 여성 중 예선을 통과하는 비율이 더 낮을 것으로 예상됩니다.\n\n그러나 실제로는, 그들이 더 높은 비율로 통과합니다. 45-49세 여성만 예외입니다. 60대 여성은 연령 등급으로 볼 때 비슷하게 엄격한 기준이 있으며, 그들은 더 높은 합격률을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n만일 연령 등급 시스템이 적절하게 보정되었다면, 자격 요건 시간의 상대적 연령 등급이 서로 다른 연령 그룹의 자격 달성률을 대략적으로 반영할 것으로 예상됩니다. 그러나 이 두 가지 사이에 일치하지 않는 것은 어떤 부분이 문제가 있다는 신호입니다.\n\n이에 대한 가능한 이유들은 여러가지가 있지만, 제 추측으로는 a) 더 어린 연령 그룹의 엘리트 경쟁, b) 일부 더 늙은 연령 그룹의 이상값 결과, 그리고 c) 특히 여성들의 일부 더 늙은 연령 그룹 규모가 작기 때문에 일어난 결과일 것이라고 생각합니다.\n\n## 연령 등급에 의해 제공되는 유용한 정보는 무엇인가요?\n\n두 번째 문제는 — 여러분의 의견은 달라질 수 있습니다 — 연령 등급이 제공하는 정보가 얼마나 유용한지입니다.\n\n<div class=\"content-ad\"></div>\n\n나이 그레이드의 특성은 100%에 가까워질수록 시간의 차이가 적어져도 나이 그레이드에 더 큰 차이가 난다는 것이죠.\n\n예를 들어, 50세 남성이 2시간 35분의 마라톤을 뛴다고 가정해봅시다. 멋진 성과네요. 86.45%의 나이 그레이드를 받습니다. 다섯 분을 줄이면, 2시간 30분의 결과는 89.33%가 됩니다.\n\n그러나, 중간부로 갈수록 시간의 큰 차이가 있어도 나이 그레이드에서는 작은 차이를 보입니다. 만약 50세 남성이 4시간의 레이스를 뛴다면, 평균 이상의 성과입니다. 55.83%의 나이 그레이드를 받습니다.\n\n다섯 분을 줄이면, 3시간 55분은 약간 더 좋아집니다. 57.02%에 해당합니다. 그가 나이 그레이드를 3% 더 올리려면 거의 15분이 더 달려야 할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 경우 테이블 태그를 Markdown 형식으로 변경하십시오.\n\n<div class=\"content-ad\"></div>\n\n그래서 대중들과 비교하는 대신 나 자신을 그들과 비교하게 되는 이유가 뭘까요?\n\n# 대안은 무엇일까요? 백분위수.\n\n제가 제안하는 대안은 특정 연령 및 성별 그룹의 다른 모든 러너와의 비교 결과를 기반으로 하는 것입니다.\n\n러너들의 분포를 가지고 — 예를 들어 50-54세 남성 — 당신은 모든 결과를 정렬하여 90%의 다른 러너보다 빠른 시간이 얼마나 걸릴지, 또는 75%, 또는 50%, 등을 볼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n모든 연령 그룹에서의 시간 분포는 유사합니다. 탁월한 성적을 낸 소수의 러너들, 평균 주변에 처한 러너들이 점차 증가하며, 훨씬 더 느린 시간에 도달하는 소수의 러너들이 있습니다.\n\n![이미지](/assets/img/2024-06-19-IsThereaBetterSystemforAgeGradingMarathonResults_1.png)\n\n러너들이 나이를 먹을수록 분포가 약간 오른쪽으로 이동하지만, 일반적인 모양은 동일합니다. 충분히 많은 수의 러너들을 대상으로 고려하면, 그들이 정말 훌륭한 성적, 좋은 성적, 평균적인 성적, 혹은 평균 이하 성적으로 크게 분할될 것으로 예상됩니다.\n\n따라서 50세의 남성이 50세의 다른 남성 중 90%보다 빠르게(3시간 18분 8초) 달리면, 90%의 동료들을 이기는 40세 여성(3시간 32분 22초)과 거의 동등한 성적을 거둡니다.\n\n<div class=\"content-ad\"></div>\n\n최적 시간에 의존하는 대신 나이 관련 감소가 그 시간의 감소로 나타나는 것을 기다릴 필요가 없습니다. 대신 백분율을 사용하면 전체 시간 분포를 기반으로 하여 모든 참가자와 비교할 수 있습니다.\n\n# 백분율은 나이 측정에 실패한 곳에서 동작합니까?\n\n그렇다면 위에서 언급한 두 가지 불만족에 대해 백분율은 어떻게 대처할까요?\n\n백분율이 전통적인 나이 측정보다 더 정확하게 보정되는가요? 대부분의 러너들에게 보다 유용한 정보를 제공하나요?\n\n<div class=\"content-ad\"></div>\n\n## 백분위수가 더 잘 보정되나요?\n\n한 마디로?\n\n네.\n\n위의 시각화는 세 가지를 보여줍니다:\n\n<div class=\"content-ad\"></div>\n\n- 1,000명당 각 연령/성별 그룹의 완주자 수 (파란색)\n- 나이 등급에 따른 각 연령/성별 그룹의 상위 1,000명 완주자 수 (보라색)\n- 백분위로 나눈 각 연령/성별 그룹의 상위 1,000명 완주자 수 (분홍색)\n\n잘 보정된 측정은 파란색 막대와 유사해야하며, 잘 보정되지 않은 경우에는 종종 차이가 날 것입니다.\n\n파란색 막대와 보라색 막대 사이에 명백한 불일치가 있습니다.\n\n여성들 중에서, 나이가 어린 연령 그룹(35-49세)은 매우 소외되어 있고 노인 연령 그룹(60-79세)은 매우 과대표시되어 있습니다. 남성들 중에서는, 35세 미만 그룹이 지나치게 과대표시되어 있고, 35-49세 그룹은 약간 소외되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n파란 막대와 분홍 막대가 항상 동일하지는 않지만, 그들 사이의 차이는 훨씬 적습니다.\n\n각 측정 항목의 총 분산을 더하면, 백분위수는 117의 총 분산을 가지고 있고, 연령 등급은 그것의 세 배가 넘는 383의 총 분산을 가지고 있습니다.\n\n만약 연령 등급의 목표가 최상위 완주자가 어떤 성별과 연령 그룹에서든 동일하게 나올 가능성이 있는 보다 공정한 경기 환경을 만드는 것이라면, 백분위수를 사용하는 것이 이미 존재하는 연령 등급 시스템을 사용하는 것보다 더 적절하게 보정되어 있습니다.\n\n## 백분위수는 더 유용한 정보를 제공할까요?\n\n<div class=\"content-ad\"></div>\n\n의견은 각양각색일 수 있지만, 저는 이렇게 생각해요.\n\n제 개인적인 진척 상황을 예시로 들어볼게요.\n\n37세 때, 저는 3시간 35분으로 마라톤을 뛰었어요. 38세 때, 3시간 20분으로 뛰었고요. 39세 때, 3시간 10분을 달성했어요. 그리고 40세 때, 3시간 08분을 뛰었죠. 실제로 10월에 시카고에서 3시간을 넘어설 것이라고 가정해봅시다 (제 생각에는 상당히 가능하다고 생각해요).\n\n나이 그레이딩으로 따지면, 제 진척은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- 56.58% - 61.15% - 64.83% - 66.00% - 69.23%\n\n그리고 백분위로 따지면 제 발전은:\n\n- 71.53% - 80.34% - 85.20% - 88.13% - 92.53%\n\n나이 그레이딩을 고려하면 이런 개선은 꽤 중요하지 않아 보일 수도 있어요. 56.58%는 꽤 평균적으로 들릴 수 있어요 — 하지만 37세 남성의 중간 완주시간(4:06)을 월등히 뛰어넘는 3:35에요. 그리고 69.23%는 그다지 더 인상적으로 들리진 않을 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n백분위수는 저와 동료들 간의 성과를 더 잘 이해할 수 있게 해주고, 시간이 지남에 따라 어떻게 변화하는지 보여줍니다. 제 초기 시간(3:35)은 평균보다 우수했지만 놀라울 정도는 아니었습니다. 한편, 10월의 목표 시간은 평균 마라톤러와 비교했을 때 상당히 좋지만(비켈레와의 격차가 많이 남은 것은 사실입니다).\n\n정말 중요한 질문은 이겁니다: 최고를 대비하려는 건가요? 아니면 동료들과 비교하려는 건가요?\n\n제가 최고와 정당하게 경쟁할 가능성이 거의 없다면, 나 자신을 동료들과 비교하는 것이 훨씬 더 합리적으로 보입니다. 그리고 그를 위해 백분위수가 더 유용한 도구인 것 같습니다.\n\n# 백분위수가 완벽한가요?\n\n<div class=\"content-ad\"></div>\n\n안녕하세요!\n\n표를 마크다운 형식으로 변경해주세요.\n\n고맙습니다!\n\n<div class=\"content-ad\"></div>\n\n다른 문제는 연장된 연령 그룹에는 충분히 많은 러너가 없어 신뢰할 수 있는 분포를 만들어내기 어렵다는 것입니다. 70대 여성과 70대 후반 / 80대 남성을 대상으로 하면 문제가 발생하기 시작합니다.\n\n이를 개선하기 위해 몇 가지 수학 모델링을 통해 분포를 완화시킬 수 있습니다. 하지만 이러한 러너들을 위한 더 신뢰할 수 있는 데이터셋을 구축하기 위해 더 많은 데이터가 필요합니다.\n\n아마도 2024년 말까지 또 다른 한 해 동안의 데이터가 추가된다면, 작업을 업데이트하고 일부 개선을 할 수 있을 것입니다.\n\n# 이 도구를 직접 활용하는 방법은 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n이전에 독자들이 마라톤 성적을 백분위로 계산하고 전통적인 연령 점수와 비교할 수 있는 계산기를 만들었습니다.\n\n그 도구를 업데이트하여 새로운 2023 데이터에서 생성된 백분위 표를 반영했습니다.\n\n연령 점수 계산기는 여기에서 이용할 수 있습니다.\n\n또한 계산기를 개선했습니다. 빠르게 작동하고 오류가 발생하면 더 정확하게 실패합니다.\n\n<div class=\"content-ad\"></div>\n\n지금은 이것을 만들어서 내 웹사이트(Running with Rock)에 호스팅했어요. 당신이 러닝 웹사이트를 운영하고 계시고 본인만의 계산기 버전에 관심이 있다면 알려주세요. 실제로 관심이 있는 사람이 있다면 워드프레스 플러그인이나 유사한 것을 만들어볼 수 있을 거에요.\n\n# 다음은 뭐가 있을까요?\n\n지금까지 이 토끼굴을 끝까지 쫓아갔어요. 데이터를 몇 차례 반복하고 유용한 도구를 만들었어요.\n\n나는 이것이 나이 등급과 개인 러닝 퍼포먼스 레벨(PLPs)을 대체할 것이라는 환상은 없어도, 이것이 가치 있는 관점을 제공한다고 생각해요.\n\n<div class=\"content-ad\"></div>\n\n이제부터, 내가 고민 중인 몇 가지 다른 질문들이 있어요. 아마도 다음 몇 주 안에 그에 대해 글을 쓸 것 같아요:\n\n- 이전에 보스턴 마라톤에서 날씨가 미치는 영향에 대한 기사를 썼어요. 이에 대한 후속 기사를 작성 중이고, 타임 예선자와 미예선자 간의 잠재적 차이를 살펴볼 계획이에요.\n- 성별에 따라 러너들의 분포가 시간에 따라 어떻게 변화하는지 살펴보고 있어요. 특히, 연령 그룹 간의 차이와 여성이 데이터에서 역사적으로 배제된 흔적이 여전히 있는지 궁금해요.\n- 나이 등급 및 백분위와 관련된 몇 가지 질문들도 있어요 — 그리고 이 시리즈와 별도로 각 주제를 더 자세히 탐구할 거에요.\n\n만약 이러한 질문들에 관심이 있다면 — 또는 마라톤에 관한 데이터 기반 이야기에 대해 다른 이야기를 원한다면, 이메일 업데이트를 구독해 주세요.\n\n저는 열렬한 러너이자 데이터 열정가에요. 제가 무엇을 하고 있는지 따라가는 방법은 다음과 같아요:\n\n<div class=\"content-ad\"></div>\n\n- 나의 훈련에 대해 알고 싶다면 Running with Rock을 팔로우하세요.\n- 마라톤 훈련 계획 선택에 관한 팁을 읽어보세요.\n- Strava에서 제 프로필을 염탐하세요.","ogImage":{"url":"/assets/img/2024-06-19-IsThereaBetterSystemforAgeGradingMarathonResults_0.png"},"coverImage":"/assets/img/2024-06-19-IsThereaBetterSystemforAgeGradingMarathonResults_0.png","tag":["Tech"],"readingTime":9},{"title":"더 나은 데이터 스토리텔링 탐구적 연구를 통해 시각물을 만드는 방법","description":"","date":"2024-06-19 01:31","slug":"2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch","content":"\n\n\n![Better Data Storytelling - Creating Visuals Through Exploratory Research](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_0.png)\n\n데이터 시각화와 스토리텔링을 통해 명확하고 간결한 이미지를 제시하면 의견을 전달하는 데 큰 가치가 있습니다.\n내닌 Yau가 그의 새 책(Visualize This, 2판)에서 강조한 것처럼, 데이터를 효율적이고 정확하게 제시하기 위해 항상 노력한다면 대부분의 경우 막대 차트를 사용해야 합니다. 대부분의 사람들이 막대 차트를 보고 비교적 쉽게 해석할 수 있습니다. 양적으로 매우 효과적입니다.\n\n하지만 어떻게 하면 당신의 데이터를 돋보이게 만들 수 있을까요? 춤추게 할 수 있을까요? 청중을 행동으로 이끄는 데 무엇을 할 수 있을까요?\n\n\n<div class=\"content-ad\"></div>\n\n어쩌면 좀 더 흥미로운 것을 찾아볼 필요가 있을지도 몰라요. 처음 단계로, 우리는 탐구적 연구를 수행하여 데이터를 다양한 방식으로 시각화하여 우리의 청중을 정말로 사로잡을 시각이나 시각 집합을 찾을 수 있습니다.\n\n탐구적 연구란 무엇일까요? 설명해 드리겠습니다. 또한 데이터 집합을 살아있게 만들기 위해 다양한 시각화를 사용하는 방법에 대한 유용한 예시도 제공해 드리겠습니다.\n\n# 탐구적 연구\n\n데이터 집합으로 이야기를 전할 수 있는 지점에 도달하려면 탐구적 연구 과정을 거쳐야 할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n여기에 자연적인 호기심이 담긴 기회가 있어요. 데이터에 관한 어떤 질문이 있나요? 데이터에 대해 알아보고 싶으신 점이 무엇인가요?\n\nCole Nussbaumer-Knaflic은 2015년에 출간된 대표적인 책인 'Storytelling With Data'에서 탐색 단계를 전복 속에서 진주를 찾는 것과 유사하다고 설명합니다. 100개의 전복 중에서 2개의 진주를 찾을 수 있다고 가정해보세요. 탐색 단계에서 성공하기 위한 핵심은 데이터를 여러 다양한 방법으로 바라보는 것입니다.\n\n그리고 Nathan Yau(2024)가 제시한 유용한 질문 목록이 있습니다. 몇 가지 유용한 질문 중 일부는 다음과 같아요:\n\n- 이 데이터는 무엇에 관한가요?\n- 시간이 지남에 따라 어떻게 변했나요?\n- 어떤 관련성이 있나요?\n- 무엇이 돋보이나요?\n- 이것이 정상인가요?\n\n<div class=\"content-ad\"></div>\n\n큰 데이터 집합인 유엔 난민고등판데사(UNHCR)의 글로벌 망명자 데이터 집합을 사용한 몇 가지 예시를 살펴봅시다.\n\n## 데이터셋\n\n유엔 난민고등판데사(UNHCR)는 전 세계 난민 이동에 대한 통계를 추적합니다.\n\n그 데이터는 [여기](링크)에서 자유롭게 액세스할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n링크를 클릭해서 다운로드 페이지로 이동한 후에, 선택하는 데이터를 자세히 살펴볼 수 있습니다:\n\n![다운로드 이미지](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_1.png)\n\n이 프로젝트에서는 각 난민의 출신 국가와 피난처 국가를 살펴봅시다:\n\n- 출신 국가에서 — 피난민이 이주하는 국가\n- 피난처 국가에서 — 피난민이 출신하는 국가\n\n<div class=\"content-ad\"></div>\n\n데이터셋을 다운로드한 후에는 스프레드시트 형식으로 열어서 어떤 내용을 다루고 있는지 살펴볼 수 있어요:\n\n![Spreadsheet](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_2.png)\n\n이 프로젝트에서 주로 관심 있는 데이터 필드는 다음과 같아요:\n\n- 출신 국가 (3자리 ISO 코드 포함) - 망명을 찾는 사람이 어디서 왔는지\n- 망명 국가 (3자리 ISO 코드 포함) - 실제로 망명을 찾으려는 사람이 있는 곳\n- 인정된 결정 - 망명을 찾는 사람이 수용되었는지 여부 (국가별 숫자 총계)\n\n<div class=\"content-ad\"></div>\n\n원산지와 망명국 두 곳은 신뢰할 수 있는 고유 식별을 위해 사용할 수 있는 3자리 ISO 코드를 갖고 있어요.\n\n# 어떤 이야기들을 할 수 있을까요?\n\n데이터셋이 어떤 내용을 포함하고 있는지 아는 것으로 우리는 다음 단계를 진행할 때 호기심을 안내할 수 있습니다.\n\n저는 캐나다인으로써, 캐나다로 오는 망명 신청자에 관심이 있어요. 반면 캐나다에서 떠나 다른 곳으로 망명을 찾는 사람들보다는 그 수가 많지 않아요.\n\n<div class=\"content-ad\"></div>\n\n마음에 떠오르는 질문들:\n\n- 사람들이 어디서 왔는지?\n- 캐나다가 난민을 받아들이는 면에서 다른 나라들과 어떻게 비교되는가?\n- 그들의 출신지가 시간이 지남에 따라 변했는가?\n- 총 인원이 시간이 지남에 따라 변했는가?\n\n이러한 질문에 대답하기 위해 가장 매력적인 방법으로 가능한 시각화 옵션들을 살펴볼 수 있습니다.\n\n한 번 더 말하지만, 2024년에 출간된 Dr. Yau의 Visualize This에서는 60가지 시각화 옵션이 제공되므로 선택할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_3.png\" />\n\n오늘의 탐색적 연습에서는 주황색으로 강조된 5가지 다양한 탐색 옵션을 살펴보겠습니다:\n\n- 막대 차트\n- 시계열 선 그래프\n- 쌓인 영역 차트\n- 등치지도\n- 선채우기 차트 (원형 차트처럼 비율에 따라 크기가 조절되지만 두 가지 수준이 있음)\n\n본문에 포함된 모든 5가지 예시는 Python Plotly 라이브러리를 사용하여 생성되었습니다(Github에서 모든 파일을 확인할 수 있습니다).\n\n<div class=\"content-ad\"></div>\n\n시작해봅시다!\n\n# 탐사 1. 막대 차트\n\n막대 차트는 오늘날 가장 인기 있는 데이터 시각화 중 하나입니다. 데이터를 처음 탐색할 때 좋은 시작점이 됩니다.\n\n![Bar Chart](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_4.png)\n\n<div class=\"content-ad\"></div>\n\n이 막대 차트를 통해 수용된 망명자 수가 2021년까지 상승 추세였으며 (2020년 코로나19 예외), 그 이후로는 하락 추세임을 알 수 있습니다. 이 차트는 캐나다로의 망명자의 숫자를 매우 명확하게 보여줍니다.\n\n좋아요, 좋아요, 이제 캐나다에서 망명을 찾는 사람들의 구성에 대해 어떻게 생각하세요? 그들은 어디에서 오고 있나요?\n\n우리는 데이터 집합의 범위 (2015–2023)에 대한 숫자를 보여주는 막대 차트로 시작할 수 있습니다:\n\n![차트](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_5.png)\n\n<div class=\"content-ad\"></div>\n\n가로 막대 차트를 사용하면 각 국가를 쉽게 읽고 서로 비교할 수 있어요. 멋지네요!\n\n하지만 만약 이 기간 동안 캐나다를 전 세계와 비교하려면 어떻게 해야 할까요? 막대 차트로는 그것을 보여주기가 어려울 수 있어요. 시계열 차트가 더 나을 수도 있으니 한 번 시도해보죠.\n\n# 2번째 탐색. 시계열 선 그래프\n\n먼저, 이전 예시의 막대 차트를 선 그래프로 변환해봅시다.\n\n<div class=\"content-ad\"></div>\n\n만약 우리가 캐나다만을 대표하는 선 그래프로 연도별 신청자 총 수를 보여준다면:\n\n![line chart](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_6.png)\n\n우리는 코로나 이전에 상승세를 보이다가 코로나 기간 동안 짧은 하락세를 보이는 것을 확인할 수 있습니다 (캐나다는 매우 엄격한 입국 규정을 가지고 있었습니다). 그러나 이 시기 동안 캐나다는 다른 나라들과 어떻게 비교되는 걸까요?\n\n데이터 시각화의 최상의 방법을 활용하여 데이터 세트의 나머지 국가들을 (연한 회색으로) 추가할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![Better Data Storytelling](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_7.png)\n\n이제 캐나다를 주황색으로 표시하고 다른 국가들을 연한 회색으로 표현하면, 캐나다의 다른 나라들에 비한 위치를 알 수 있습니다.\n\n이 차트에서 우리는 많은 다른 국가들에 비해 캐나다가 실제로 많은 난민들을 받아들인다는 것을 볼 수 있습니다. 우리가 2016년에 볼 수 있는 큰 증가는 그때 독일이 많은 수의 시리아 난민을 받아들였던 것입니다.\n\n멋져요!\n\n<div class=\"content-ad\"></div>\n\n지금은 캐나다가 전 세계와 비교되는 방법과 어디서 사람들이 왔는지에 대한 전체 숫자를 알게 되었습니다.\n\n하지만 캐나다의 경우 (출신 국가별로) 연간 변화를 추적하고 싶다면 어떨까요?\n\n여기서 좀 더 세부적으로 파고들어야 합니다. 스택된 면적 차트를 사용하면 시각적으로 이를 파악할 수 있습니다.\n\n# Exploration 3. 스택된 면적 차트\n\n<div class=\"content-ad\"></div>\n\n스택된 영역 차트는 시간이 지남에 따른 체적 변화를 살펴보는 뛰어난 방법입니다. 우리 데이터셋의 경우, 체적 변화는 캐나다로 매년 망명을 찾는 각 출신 국가의 사람 수일 것입니다.\n\n![영역 차트](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_8.png)\n\n이 특정 차트는 캐나다로 이주하는 사람들이 어디에서 망명을 찾고 있는지를 보여줍니다. 사용자가 세부사항에 빠져들지 않도록 상위 10개 국가로 좁혀졌습니다.\n\n자세히 살펴보면, 주황색 영역은 캐나다에서 망명을 찾는 이란 사람들의 많은 수를 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n적절한 데이터로 스택된 영역 차트는 시간에 따른 변화에 대해 확실한 시각적 효과를 제공할 수 있습니다.\n\n이 특정 차트는 Python Plotly express 라이브러리를 사용하여 생성되었습니다. 이 라이브러리에는 이 스타일의 차트를 생성하는 area()라는 내장 함수가 있습니다.\n\n이제 이 3가지 훌륭한 시각화를 통해도, 우리가 아직 부족한 것은 캐나다로의 국가별 양민 신청자의 명확한 전체적 인식을 제공해주는 유용한 시각화입니다.\n\n이를 위해 우리에게 제공하는 유용한 표현은 코로플레스 지도입니다.\n\n<div class=\"content-ad\"></div>\n\n# 4. 코로플레스 맵 고찰\n\n코로플레스 맵은 지리적 영역(예: 국가별) 전체의 데이터 변화를 보여주기 위해 음영 처리된 영역을 제공합니다.\n\n저희 데이터셋과 함께 코로플레스 맵을 사용하면 각 국가별 신청자 수에 대한 전체적인 시각화를 얻을 수 있습니다:\n\n![이미지](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_9.png)\n\n<div class=\"content-ad\"></div>\n\n이 예에서, 더 어두운 색조와 음영은 캐나다로의 많은 수의 난민 신청자를 대표합니다. 이 예에서 대상 국가인 캐나다는 녹색 원(마치 \"녹색 빛\"처럼)으로 강조되어 있습니다.\n\n이전의 시각화(스택된 면적 차트)에서 2023년에 난민 신청을 한 이란 국민들의 수가 많다는 것을 알 수 있었습니다. 우리는 이 지도에서 이란이 가장 어두운 색인 것을 볼 수 있습니다. 이는 이전의 예시와 일치하는 멋진 부분입니다!\n\n추가 기능으로, 코로플레스 맵은 Streamlit(파이썬용)과 같은 현대적인 코딩 라이브러리와 결합하여, 년도를 선택할 수 있게 하는 상호작용성을 제공할 수 있습니다:\n\n![이미지](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_10.png)\n\n<div class=\"content-ad\"></div>\n\n파이썬 Plotly를 사용하여 생성된 코로플레스 맵으로 각 나라 위를 마우스 오버하여 추가 정보(즉, 정확한 숫자)를 확인할 수 있습니다.\n\n# 탐험 5. 썬버스트 차트\n\n썬버스트 차트는 데이터를 표현하는 아름다운 방법입니다. 다양한 형태의 데이터를 시각화하는 가장 효과적인 방법은 아니지만, 썬버스트 차트는 항상 눈길을 끕니다. 나는 그것들을 주시하고 항상 표현하고 있는 데이터 포인트가 무엇인지 궁금해합니다.\n\n우리 데이터셋에서 각 해의 각 나라에 대한 캐나다의 망명 신청 국가 데이터를 나타낼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Better Data Storytelling: Creating Visuals Through Exploratory Research](/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_11.png)\n\n매우 다채롭고 인상적이지만 동시에 매우 복잡합니다. 이런 차트를 소화하는 데는 많은 시간이 걸릴 수 있습니다.\n\n하지만 Python Plotly와 같은 현대 데이터 시각화 도구의 큰 이점 중 하나는 사용자 상호작용을 허용한다는 점입니다. 이 시각화(다시 말해 Python Plotly로 만든 것)에서 각 연도를 클릭하여 데이터를 자세히 살펴볼 수 있습니다.\n\n예를 들어, 2023년을 클릭하면 해당 연도의 데이터만 볼 수 있습니다:\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_12.png\" />\n\n그리고 특정 국가(예: 인도) 위로 마우스를 올리면 해당 국가에 대한 추가 데이터를 볼 수 있습니다. 매우 유용합니다.\n\n# 요약\n\n이 글의 목표는 독자들에게 데이터 집합에서 탐색 분석을 수행하는 다양한 차트 및 매핑 기술을 사용하는 방법에 대한 예시를 제공하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 과정을 통해 데이터에 대해 궁금한 점이 있으면 답변할 수 있습니다. 혹은 아직 질문이 없다면 질문을 세우는 데 도움을 줄 수도 있습니다.\n\n이 연습이 유용하고 즐거우셨기를 바랍니다.\n\n읽어 주셔서 감사합니다!\n\n참고: 이 글의 예시는 모두 Python 코드를 사용하여 (Plotly express 및 Streamlit 라이브러리를 활용하여) 작성되었습니다.\n\n<div class=\"content-ad\"></div>\n\n위 코드 파일과 CSV 파일은 GitHub 저장소에 있습니다. \n\n만약 이 유형의 이야기가 당신의 취향이고 작가로서 저를 지원하고 싶다면, 제 Substack를 구독해주세요.\n\nSubstack에서는 2주에 한 번 뉴스레터와 다른 플랫폼에서 찾을 수 없는 기사들을 게시합니다.","ogImage":{"url":"/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_0.png"},"coverImage":"/assets/img/2024-06-19-BetterDataStorytellingCreatingVisualsThroughExploratoryResearch_0.png","tag":["Tech"],"readingTime":8},{"title":"데이터에서 시각화까지 OpenAI Assistants API 및 GPT-4o와 함께","description":"","date":"2024-06-19 01:28","slug":"2024-06-19-FromDatatoVisualizationwiththeOpenAIAssistantsAPIandGPT-4o","content":"\n\nGPT-4의 능력이 계속해서 확장되면서, OpenAI의 기술을 기반으로 한 도구들은 개발자들에게 점차적으로 강력한 자산으로 발전하고 있습니다.\n\n본 글에서는 최신 버전의 차트 작성 기능을 탐구할 예정입니다. 데이터 파일과 구체적인 지침을 Assistant에 제공하여 우리의 데이터 시각화 아이디어를 구현하는 과정을 살펴볼 것입니다.\n\n이를 위해 Assistant API의 내장 도구들을 활용할 것입니다.\n\n지금 당장 OpenAI Python 패키지(v1.30.0, 작성 시점 기준)에는 Assistants API 안에 File Search, Code Completion 및 Function Calling 도구가 포함되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n기능 호출을 통해 개발자는 작업을 완료하기 위해 AI가 지능적으로 선택할 수 있는 함수를 정의할 수 있으며, 파일 검색을 통해 다양한 파일 유형을 업로드하고 벡터 데이터베이스에 RAG 스타일로 저장할 수 있습니다. 코드 완성은 보조 프로그램이 프로그래밍 및 수학 문제를 해결하기 위해 파이썬 프로그램을 작성하고 실행할 수 있는 격리된 환경에서 작동합니다.\n\n코드 완성은 업로드된 파일과 함께 사용할 수도 있습니다. 이 파일들은 데이터 파일과 차트 이미지를 생성하기 위해 처리될 수 있습니다. 그리고 바로 이 기능을 우리가 여기에서 사용할 것입니다.\n\n아래에서 탐구할 코드는 CSV 형식의 데이터 파일을 로드하고 적절한 프롬프트를 사용하여 데이터에서 그래프를 생성하도록 보조 프로그램에 지시합니다. 그런 다음 그래프를 다운로드하고 표시할 것입니다.\n\n간단한 영어 프롬프트를 사용하여 원시 CSV 데이터에서 아래와 같은 차트를 쉽게 생성할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# OpenAI Assistant\n\n이전에 OpenAI의 Assistant API에 대해 설명했고 이를 시작하는 방법을 설명했습니다(OpenAI의 강력한 새 Assistant API를 사용하여 데이터 분석). API의 새 버전들이 이 기사를 다소 구식으로 만들었지만, Assistant의 설명과 작동 방식은 여전히 대체로 정확하며 OpenAI 계정을 설정하는 방법 또한 그대로 유효합니다.\n\n그래서 더 자세히 살펴보려면 해당 기사를 참고하시고, 여기서는 핵심 사항에 대한 간략한 소개로 한정하겠습니다.\n\n# OpenAI\n\n<div class=\"content-ad\"></div>\n\n먼저, 물론 OpenAI 계정이 필요하며 사용 시 요금이 부과될 것임을 알아야 합니다. 그러나 요금은 높지 않습니다: 여기서 설명할 코드를 실행하는 비용은 몇 센트만 들 것입니다. 파일 저장 등의 기타 요금이 부과될 수 있으나, 이는 해당 맥락에서는 관련성이 없을 수도 있지만 최신 요금을 확인해 보는 것이 좋습니다.\n\n그렇다면, OpenAI 대시보드를 정기적으로 사용하여 사용량을 확인하여 큰 청구서가 발생하지 않도록 해야합니다.\n\n OpenAI 어시스턴트의 모든 출력물과 업로드한 파일이 모두 저장되므로 사용중인 저장소도 확인해야 합니다. 대시보드에서 수동으로 삭제할 수 있으며, 여러분은 이렇게 해야 합니다. 왜냐하면 요금이 부과되지 않을 수 있지만, 시간이 지날수록 작업 공간에 불필요한 파일이 많이 축적될 수 있기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n# 어시스턴트, 스레드, 그리고 런\n\n어시스턴트 API에서 가장 기본적인 세 가지 객체입니다.\n\n- 어시스턴트: 이름에서 알 수 있듯이, 이 설정의 주요 부분입니다. 어시스턴트를 만들 때 우리는 모델 (예: gpt-4o), 모델에게 원하는 행동 유형에 대한 지시사항, 코드 해석기와 파일 검색과 같은 도구, 그리고 모델이 사용해야 하는 파일과 같은 다양한 속성을 지정합니다.\n- 스레드: 이들은 대화의 상태를 저장하고 사용자와 어시스턴트가 생성하는 메시지를 포함합니다. 런(아래 참조)이 시작될 때 스레드는 어시스턴트와 연결됩니다.\n- 런: 런은 스레드의 정보와 어시스턴트를 가져와 LLM(AI 모델)과의 상호 작용을 관리합니다. 완료되기 전에 런은 여러 단계를 거칩니다. 런이 완료되면, 어시스턴트가 만든 응답을 확인하기 위해 스레드를 조사할 수 있습니다.\n\n이 기본 객체들 외에도 스레드에서는 모델에 대한 지시와 해당 응답을 포함하는 메시지가 필요합니다. 또한 어시스턴트가 사용하는 분리된 객체인 파일을 사용하며, 업로드된 파일의 세부 정보를 저장합니다.\n\n<div class=\"content-ad\"></div>\n\n# 비서 코딩하기\n\n우리의 비서를 만들고 실행하기 위해 몇 가지 단계를 거쳐야 합니다. 아래에 나열된 이벤트 순서는 각 구성 요소가 사용되는 방식에 대한 개요를 제공합니다.\n\n다음은 절차입니다:\n\n- API 키로 OpenAI 클라이언트를 생성합니다.\n- 로컬 파일을 업로드하고 나중에 사용할 파일 객체를 검색합니다.\n- 모델에 대한 지침과 업로드된 파일의 ID로 비서를 생성합니다.\n- 파일 ID 및 모델의 지침을 포함하는 스레드를 생성합니다.\n- 비서와 스레드를 실행합니다.\n- 이제 스레드에 있는 사용자 및 AI가 생성한 메시지를 표시합니다(모델이 결과물을 생성하는 과정을 보여주어야 하며, 문제가 발생한 경우 무엇이 문제인지 확인할 수 있습니다).\n- 생성된 이미지를 검색하여 표시합니다.\n\n<div class=\"content-ad\"></div>\n\n위의 각 항목을 Python으로 코드화하고 정확히 무슨 일이 일어나고 있는지 설명하겠습니다. 저는 Jupyter 노트북 형식으로 코드를 작성했으므로, 함께 따라해보고 싶으시다면 각 코드 부분을 새로운 노트북 셀에 복사하여 제 노트북을 복제할 수 있습니다.\n\n첫 번째 단계는 클라이언트를 생성하는 것입니다.\n\n## 클라이언트 생성\n\n클라이언트는 OpenAI API에 접근할 수 있게 해줍니다. API 키를 제공해야 하는데, 아래 코드에서는 사용자가 키를 수동으로 입력해야 하도록 입력문을 포함시켰습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom openai import OpenAI\n\nkey = input(\"API 키\")\nclient = OpenAI(api_key=key)\n```\n\n또는 키를 하드코딩할 수도 있어요 (하지만 코드를 공개하지 않도록 주의하세요).\n\n```js\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"여기에 키를 입력하세요\")\n```\n\n또는 키가 환경 변수로 저장된 경우에는 클라이언트가 자동으로 찾아내기 때문에 코딩할 필요가 없습니다...\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n```\n\n이 중 하나는 여러분의 첫 번째 주피터 노트북 셀입니다.\n\n## 파일 업로드\n\n먼저 파일이 필요합니다! 저는 Our World in Data(OWID) 웹사이트의 데이터를 기반으로 한 CSV 파일을 사용하고 있습니다. OWID는 정보와 데이터의 훌륭한 출처이며, 그들은 친절하게 모든 콘텐츠를 Creative Commons BY 라이센스 하에 자유롭게 사용할 수 있도록 허용하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n파일은 1850년부터 2021년까지 전 세계 CO2 배출량을 기록한 것입니다 (원본 데이터에는 다른 항목의 데이터도 많이 포함되어 있지만, 여기에는 세계 데이터만 포함했습니다). 아래 스크린샷에서 파일이 어떻게 보이는지 확인할 수 있습니다.\n\n파일 이름은 world_df.csv로 지었고, 또한 보조 프로그램에 지정할 이름에 해당하는 변수를 설정하고 싶습니다. 따라서 두 값을 담은 변수를 두 번째 노트북 셀에 넣었습니다.\n\n```js\nfilename = \"world_df.csv\"\nassistant_name = \"data-analyst-v0.1\"\n```\n\n다른 파일을 읽거나 새 보조 프로그램을 만들기 위해 코드를 사용하려면 이 셀에서 값들을 변경할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다음 셀에 파일을 업로드합니다. 파일을 업로드하는 주요 작업은 client.files.create 메서드에 의해 수행됩니다. 아래 코드에서 이 메서드는 파일 자체와 파일의 목적을 나타내는 'assistants' 문자열 두 가지 매개변수를 사용합니다.\n\n이 코드는 파일을 업로드하는 작업 이상을 수행합니다. 코드가 한 번 이상 실행될 것이므로(다른 지시사항과 함께 실행될 수도 있음) 파일을 중복으로 업로드하고 싶지 않습니다. 따라서 파일이 새로운 경우 코드는 파일을 업로드하지만, 이미 업로드된 경우 코드는 해당 기존 파일을 검색합니다.\n\n```js\n# 파일이 이미 업로드되었는지 확인\nfilelist = client.files.list(purpose=\"assistants\")\nfilenames = [x.filename for x in filelist.data]\n\n# \"assistants\" 목적의 파일을 업로드하거나 기존 파일 사용\nif not filename in filenames:\n  file = client.files.create(\n    file=open(filename, \"rb\"),\n    purpose='assistants'\n  )\nelse:\n  for f in filelist:\n    if f.filename == filename:\n      file = client.files.retrieve(f.id)\n      break\n```\n\n이미 업로드된 파일 목록을 다운로드하여 파일이 있는지 확인할 수 있습니다. client.files.list() 메서드는 서버에서 목록을 검색하며, parameterpurpose='assistants'를 전달하여 관심 있는 파일 유형을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n그럼 관심 있는 파일 이름을 찾기 위해 목록을 스캔할 수 있어요. 없다면 업로드하고, 그렇지 않으면 클라이언트에서 파일 객체를 가져와요. 어느 쪽이든, 파일은 파일 객체로 설정됩니다.\n\n앱에서는 이 코드를 파일 객체를 반환하는 함수에 유용하게 배치할 수 있어요.\n\n이제 파일이 업로드되고 파일 객체의 레코드가 생성되었어요. 다음으로, 이 파일을 사용할 보조를 만들어야 해요.\n\n## 보조 만들기\n\n<div class=\"content-ad\"></div>\n\n업로드한 파일과 마찬가지로 이미 존재하는지 확인합니다. 이 코드를 이전에 실행했다면, 도우미가 이미 만들어졌을 것이고, 중복으로 생성하고 싶지 않으므로 기존의 도우미 객체를 가져옵니다. 그렇지 않다면 새로운 객체를 생성합니다.\n\n이 기능에 대한 코드는 파일 업로드에 사용한 것과 거의 동일합니다.\n\n도우미를 생성하는 것은 client.beta.assistants.create()를 호출하여 수행됩니다.\n\n도우미의 이름, 일부 기본 지침(시스템 프롬프트가 될 것입니다), 사용할 모델(이 경우 GPT-4o), 요청하는 도구(코드 해석기) 및 리소스에 대한 매개변수를 설정합니다. 이 마지막 매개변수에서는 업로드한 파일의 파일 객체를 참조하고, 코드 해석기가 파일을 사용할 것임을 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\n# 조수가 이미 존재하는지 확인합니다\nassistant_list = client.beta.assistants.list()\nassistant_names =  [x.name for x in assistant_list.data]\n\nif not assistant_name in assistant_names:\n  # 파일 ID를 사용하여 조수를 생성합니다\n  assistant = client.beta.assistants.create(\n    name = \"data-analyst-v0.1\",\n    instructions=\"You are a data analyst\",\n    model=\"gpt-4o\",\n    tools=[{\"type\": \"code_interpreter\"}],\n    tool_resources={\n      \"code_interpreter\": {\n        \"file_ids\": [file.id]\n      }\n    }\n  )\nelse:\n    for a in assistant_list:\n      if a.name == assistant_name:\n        assistant = client.beta.assistants.retrieve(a.id)\n        break\r\n```\n\n다시 말해서, 앱에서 이것은 조수 객체를 반환하는 함수일 수 있습니다.\n\n## 스레드 생성\n\n스레드를 만들려면 단순히 client.beta.threads.create()를 호출하고, 이 스레드를 사용하여 조수가 실행될 때 조수에 전달되는 첫 번째 메시지를 지정하면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n아래 코드에서 볼 수 있듯이, 메시지에서는 역할을 설정하고 프롬프트를 설정하며 파일 ID를 첨부합니다.\n\n```js\nthread = client.beta.threads.create(\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": \"첨부된 csv 파일을 사용하여 'Year'에 대한 '연간 이산화탄소 배출' 그래프를 표시하십시오\",\n      \"attachments\": [\n        {\n          \"file_id\": file.id,\n          \"tools\": [{\"type\": \"code_interpreter\"}]\n        }\n      ]\n    }\n  ]\n)\n```\n\nLLM으로 전송하는 프롬프트는 다음과 같습니다:\n\n“첨부된 csv 파일을 사용하여 'Year'에 대한 '연간 이산화탄소 배출' 그래프를 표시하십시오”.\n\n<div class=\"content-ad\"></div>\n\n그것은 상당히 간단한 요구 사항이에요. 코드 해석기가 데이터 파일을 분석하고 필요한 코드를 생성해야 해요.\n\n이제 우리는 쓰레드를 사용하여 어시스턴트를 실행할 준비가 모두 끝났어요.\n\n## 실행 만들기\n\n실행은 어시스턴트와 쓰레드를 가져와 LLM에 제출해요. 비동기로 실행되며 완료되기 전에 여러 단계를 거쳐 가요.\n\n<div class=\"content-ad\"></div>\n\n결과를 기다리기 위해서는 두 가지 방법을 사용할 수 있는데요: 폴링 또는 스트리밍 방식이 있습니다. 폴링은 실행 상태가 완료될 때까지 반복적으로 확인하는 방식이에요. 반면에 스트리밍은 다양한 단계가 자동으로 감지되며 함수가 해당 이벤트에 매핑될 수 있는 이벤트 핸들러에 반응하는 방식입니다.\n\n아래는 OpenAI 문서에서 제공하는 스트리밍 코드입니다 (메시지가 변경되었습니다).\n\n```js\nfrom typing_extensions import override\nfrom openai import AssistantEventHandler\n\n# 먼저, 이벤트 핸들러 클래스를 생성하여\n# 응답 스트림에서 이벤트를 처리하는 방법을 정의합니다.\n\nclass EventHandler(AssistantEventHandler):\n  @override\n  def on_text_created(self, text) -> None:\n    print(f\"\\nassistant > {text.value}\", end=\"\", flush=True)\n\n  @override\n  def on_text_delta(self, delta, snapshot):\n    print(delta.value, end=\"\", flush=True)\n\n  def on_tool_call_created(self, tool_call):\n    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n\n  def on_tool_call_delta(self, delta, snapshot):\n    if delta.type == 'code_interpreter':\n      if delta.code_interpreter.input:\n        print(delta.code_interpreter.input, end=\"\", flush=True)\n      if delta.code_interpreter.outputs:\n        print(f\"\\n\\noutput >\", flush=True)\n        for output in delta.code_interpreter.outputs:\n          if output.type == \"logs\":\n            print(f\"\\n{output.logs}\", flush=True)\n\n# 그런 다음, 이벤트 핸들러 클래스를 사용하여\n# `stream` SDK 도우미와 함께 Run을 생성하고\n# 응답을 스트리밍합니다.\n\nwith client.beta.threads.runs.stream(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n  instructions=\"그래프에 대한 다운로드 가능한 파일을 생성하세요\",\n  event_handler=EventHandler(),\n) as stream:\n  stream.until_done()\n```\n\n이번 실행을 시작하는 함수는 client.bet.threads.run.stream()이며, 이 함수에는 이번 실행 및 이벤트 핸들러와 같은 특정 실행을 위한 지침을 포함하여 스레드와 어시스턴트의 ID가 전달됩니다.\n\n<div class=\"content-ad\"></div>\n\n이벤트 핸들러의 작동 방식에 대해서는 자세히 다루지 않겠습니다. 단순히 말하자면 텍스트가 생성되거나 도구가 출력되는 이벤트를 잡아내고 결과를 출력합니다. 이 기능은 실험적인 목적으로 충분하지만 실제 앱을 위해서는 이러한 출력물에 대해 더 정교한 작업을 원할 수도 있습니다.\n\n스레드에서 그래프를 생성하길 원한다고 명시했으며, 여기 실행에서 다운로드 가능한 파일을 생성하도록 요청했습니다.\n\n실행 결과는 아래와 같이 나와 있으며, 주로 어시스턴트에 의해 생성된 Python 코드로 구성되어 있습니다.\n\n```js\nassistant > code_interpreter\n\nimport pandas as pd\n\n# CSV 파일 불러오기\nfile_path = '/mnt/data/file-8XwqMOlaH6hoKEEKOYXPYqTh'\ndata = pd.read_csv(file_path)\n\n# 데이터 프레임의 처음 몇 행을 표시하여 구조를 이해합니다\ndata.head()\n\nimport matplotlib.pyplot as plt\n\n# 'Year' 대 'Annual CO₂ emissions' 그래프 그리기\nplt.figure(figsize=(10, 6))\nplt.plot(data['Year'], data['Annual CO₂ emissions'], marker='o', linestyle='-')\nplt.xlabel('Year')\nplt.ylabel('Annual CO₂ emissions')\nplt.title('Year vs Annual CO₂ emissions')\nplt.grid(True)\nplt.tight_layout()\n\n# 그래프를 파일로 저장\nplot_file_path = '/mnt/data/year_vs_annual_co2_emissions.png'\nplt.savefig(plot_file_path)\nplot_file_path\n\noutput >\n\nassistant > 'Year' 대 'Annual CO₂ emissions'를 나타내는 그래프가 생성되었습니다. 아래 링크를 통해 플롯을 다운로드할 수 있습니다:\n\n[그래프 다운로드](sandbox:/mnt/data/year_vs_annual_co2_emissions.png)\n```\n\n<div class=\"content-ad\"></div>\n\n위의 코드는 주피터 노트북에 포함되어서는 안 됩니다. GPT가 생성하고 실행한 것입니다.\n\n이 출력 결과는 LLM이 우리의 지시를 이해했고 올바른 그래프를 생성하는 코드를 생성하고 실행하여 이미지 파일을 만들었다는 것을 보여줍니다.\n\n## 생성된 파일 다운로드\n\n이제 우리가해야 할 일은 어시스턴트가 생성한 파일을 찾아 내려받는 것뿐입니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 노트북의 마지막 코드 셀이 표시됩니다.\n\n```js\nfilelist = client.files.list(purpose=\"assistants_output\")\n\nimage_list = [x for x in filelist.data if \"png\" in x.filename]\n\nid = image_list[-1].id  # 리스트의 마지막은 최신 파일입니다.\n\nimage_data = client.files.content(id)\nimage_data_bytes = image_data.read()\n\nwith open(\"./my-image.png\", \"wb\") as file:\n    file.write(image_data_bytes)\n```\n\n우리는 이 코드가 이미 실행되었을 수 있다고 가정하므로 이미지 파일이 하나 이상 있을 수 있습니다.\n\n그래서 먼저 \"assistants_output\"로 레이블이 지정된 모든 파일의 목록을 가져온 다음, 이미지 목록(즉, 확장자가 '.png'인 파일)을 만들고 해당 목록에서 마지막 파일을 찾습니다. 그게 바로 생성된 마지막 파일입니다.\n\n<div class=\"content-ad\"></div>\n\n그리고 차트를 표시하기 위해 다음 내용을 포함한 마크다운 셀을 만들 수 있습니다.\n\n```js\n![](my-image.png)\n```\n\n아래 이미지에서 결과물을 확인할 수 있습니다:\n\n![이미지](/assets/img/2024-06-19-FromDatatoVisualizationwiththeOpenAIAssistantsAPIandGPT-4o_1.png)\n\n<div class=\"content-ad\"></div>\n\n# **프롬프트 변경**\n\n다른 차트를 생성하려면 단순히 다른 프롬프트로 코드를 다시 실행하면 됩니다. 예를 들어:\n\n“첨부된 CSV 파일을 사용하여 '연도'에 대한 그래프를 모든 다른 열과 함께 표시하십시오.”\n\n결과는 아래 이미지와 같습니다.\n\n<div class=\"content-ad\"></div>\n\n아래 내용을 읽고 다른 문서에서 이 내용을 인용하실 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nassistant > code_interpreter\n\nimport pandas as pd\n\n# CSV 파일 로드\nfile_path = '/mnt/data/file-8XwqMOlaH6hoKEEKOYXPYqTh'\ndata = pd.read_csv(file_path)\n\n# 데이터프레임의 처음 몇 행을 표시하여 구조를 이해합니다.\ndata.head()\n\n# 데이터를 21세기 이후(2000년 이후)로 필터링합니다.\ndata_21st_century = data[data['Year'] >= 2000]\n\n# 'Year' 대 'Year'를 기준으로 플롯할 열을 정의합니다.\ncolumns_to_plot = data.columns.drop(['Entity', 'Code', 'Year'])\n\n# 데이터 플롯\nplt.figure(figsize=(12, 8))\nfor column in columns_to_plot:\n    plt.plot(data_21st_century['Year'], data_21st_century[column], label=column)\n\nplt.title('21세기 연간 데이터')\nplt.xlabel('년도')\nplt.ylabel('값')\nplt.legend(title='메트릭', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\nplt.tight_layout()\n\n# 플롯을 파일로 저장\nplot_file_path = '/mnt/data/my-image3.png'\nplt.savefig(plot_file_path)\n\nplt.show()\n\nplot_file_path\n\noutput >\n\nassistant > 그래프가 성공적으로 생성되었고 저장되었습니다. 아래 링크를 사용하여 파일을 다운로드할 수 있습니다:\n\n[그래프 다운로드](sandbox:/mnt/data/my-image3.png)None\n```\n\n이를 통해 코드 해석기가 데이터를 필터링하여 원하는 차트를 생성했음을 알 수 있습니다.\n\n아래 차트를 확인할 수 있습니다.\n\n<img src=\"/assets/img/2024-06-19-FromDatatoVisualizationwiththeOpenAIAssistantsAPIandGPT-4o_3.png\" />\n\n# 결론 및 앱 방향으로\n\n<div class=\"content-ad\"></div>\n\n오픈AI의 Assistants API와 코드 인터프리터를 사용하면 일반 영어로 데이터 파일에서 차트를 생성할 수 있는 코드를 생성할 수 있습니다.\n\n이 코드는 특별히 어렵지 않으며 Jupyter Notebook 코드는 단순히 데모용일 뿐입니다. 그러나 이를 쉽게 응용하여 사용자로부터 데이터 파일을 업로드하고 필요한 차트를 설명하는 프롬프트를 입력하도록 요청하고 사용자가 그 차트를 이미지 파일로 다운로드할 수 있는 앱으로 확장할 수 있을 것입니다.\n\n## 업데이트: 프로토타입 앱\n\nGitHub 리포지토리(아래 참조)의 코드를 기반으로 한 Streamlit 앱 프로토타입인 'streamlit' 폴더가 있습니다. 이 앱을 사용하려면 API 키를 제공하고 Streamlit 비밀 파일에 넣어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n앱은 Streamlit 파일 업로드 컨트롤을 사용하여 CSV 파일을 업로드하고 작업하기 위해 프롬프트를 입력할 수 있는 입력 상자가 제공됩니다. 프롬프트가 실행 중일 때 상태 문자열이 표시됩니다. LLM가 프롬프트를 이해하지 못하거나 다른 오류가 발생하는 경우 간단한 오류 메시지가 표시됩니다.\n\n위의 주피터 노트북의 수정된 버전은 로컬 라이브러리 패키지에서 클래스로 코드화되고 Streamlit 앱에서 해당 메서드를 호출합니다. 몇 가지 간단한 데이터 파일도 있습니다. 자유롭게 다운로드하여 수정하고 자신의 목적을 위해 실행할 수 있지만 'streamlit' 폴더의 README.md 파일을 먼저 읽어야 합니다!\n\n읽어 주셔서 감사합니다. GitHub 리포지토리에서 코드와 데이터를 찾을 수 있습니다. 자유롭게 다운로드하거나 복제하거나 포크할 수 있습니다. 더 많은 기사를 보려면 중간에서 제 계정을 팔로우하거나 무료 종간 소식지를 구독해 주세요. 이전 기사는 제 웹페이지에 나열되어 있습니다.\n\n모든 이미지와 스크린샷은 별도로 표시되지 않는 한 제가 작성한 저자에 의해 만들어졌습니다.\n\n<div class=\"content-ad\"></div>\n\n모든 코드는 MIT 라이선스에 의해 보호받습니다 (저장소에서 확인하실 수 있습니다). 언급이 필수는 아니지만 언제나 감사히 받습니다.","ogImage":{"url":"/assets/img/2024-06-19-FromDatatoVisualizationwiththeOpenAIAssistantsAPIandGPT-4o_0.png"},"coverImage":"/assets/img/2024-06-19-FromDatatoVisualizationwiththeOpenAIAssistantsAPIandGPT-4o_0.png","tag":["Tech"],"readingTime":14}],"page":"99","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true}