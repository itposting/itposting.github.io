{"pageProps":{"posts":[{"title":"3분 안에 Airflow와 DAG 이해하기","description":"","date":"2024-06-20 15:27","slug":"2024-06-20-UnderstandingAirflowandDAGsin3minutes","content":"\n\n![Understanding Airflow and DAGs in 3 minutes](/assets/img/2024-06-20-UnderstandingAirflowandDAGsin3minutes_0.png)\n\n안녕하세요! 이 기사에서는 Apache Airflow가 무엇인지 그리고 전통적인 ETL(Extract-Transform-Load) 워크플로우 내에서 어떤 문제를 해결하는지 예를 통해 알아보겠습니다. 또한, Directed Acyclic Graphs (DAGs)가 Airflow 내에서 작업을 구현하는 데 어떻게 활용되는지 살펴볼 것입니다.\n\n예시: 매일 새벽 12시에 벤더 서버에서 매출 데이터 파일을 검색하여 변환한 후 데이터 웨어하우스에 로드하는 ETL 작업이 있다고 상상해보세요. cron 스케줄러를 사용하여 이 작업을 프로덕션 서버에서 성공적으로 트리거합니다.\n\n문제 1: 만약 어느 날 이 작업이 실패한다면 어떻게 될까요? 문제는 추출, 변환 또는 로딩 프로세스에서 발생한 것일까요? 문제 해결을 위해 흩어진 로그를 통해 쥐잡이 게임이 되어버립니다.\n\n<div class=\"content-ad\"></div>\n\n문제 2: 이제 10에서 50개의 ETL 작업을 관리하는 시나리오로 확장해 봅시다. 의존 관계를 추적하고 단일 작업 실패가 혼란을 일으킬 수 있는 것을 방지하는 것은 어려운 과제가 됩니다.\n\n문제 3: 게다가, 한 작업이 다른 작업을 트리거해야 하는 경우가 있고, 그들의 타이밍이 동기화되어 있지 않다면 어떻게 해야 할까요? 생산 서버와 크론 작업만을 이용해서 이러한 복잡성을 관리하는 것은 가리지 않은 눈으로 고양이들을 몰고 다니는 것과 같습니다.\n\nAirflow를 당신의 명령 센터로 생각해보세요. 여기서 ETL 작업을 시각화하고 손쉽게 관리할 수 있습니다. DAG를 사용하여 Airflow는 작업의 순서를 지도화하여 의존 관계가 명확하고 실패가 격리되도록 합니다. DAG가 무엇인지 이해해 봅시다...\n\nDIRECTED ACYCLE GRAPH(DAG):\n\n<div class=\"content-ad\"></div>\n\nApache Airflow의 DAG는 각 작업을 나타내는 작업 단위로 구성되며 작업 간의 관계가 워크플로를 정의합니다.\n\n위의 예에서는 세 가지 작업으로 구성된 \"SALES_ETL_DAG\"를 개념적으로 구성할 수 있습니다:\n\n- 작업 A: 공급 업체 서버에서 Sales 파일에서 데이터 추출\n- 작업 B: 추출한 데이터 변환\n- 작업 C: 변환된 데이터를 데이터 웨어하우스에 로드\n\n(참고: 이러한 모든 작업은 Python으로 작성할 수 있으며 Airflow 자체가 Python으로 개발되었으며 주로 Python과 함께 작동합니다)\n\n<div class=\"content-ad\"></div>\n\n실행 순서는 중요합니다: 작업 A는 성공적으로 완료되어야 작업 B가 시작할 수 있고, 작업 B는 완료되어야 작업 C가 시작할 수 있습니다. 이 순차적인 흐름은 의존성을 존중하고 작업이 역행 없이 단방향으로 진행되도록 보장합니다 — 이것이 DAG의 \"Directed\" 측면입니다.\n\n이 구조 내에는 순환 의존성이나 루프가 없습니다 : 각 작업은 의존성에 의해 정의된 구조적인 순서대로 실행되어 정돈된 작업 진행이 보장됩니다 — 이는 DAG가 \"Acyclic\"임을 보장합니다.\n\nDAG 표현: 작업은 노드로 표시되고 작업 간의 의존성은 이러한 노드 사이의 방향성 있는 에지로 표시됩니다 — 이는 워크플로우의 \"Graph\" 표현을 형성합니다.\n\n![Understanding Airflow and DAGs](/assets/img/2024-06-20-UnderstandingAirflowandDAGsin3minutes_1.png)\n\n<div class=\"content-ad\"></div>\n\nAirflow은 작업의 종속성과 지정된 논리에 따라 작업을 예약하고 모니터링하는 DAGs를 사용하여 복잡한 워크플로우를 쉽게 관리하고 시각화할 수 있습니다.\n\nAirflow은 작업을 예약하고 모니터링하는 것뿐만 아니라 실패한 작업을 다시 시도하고 각 단계를 로깅하며 잠재적인 문제에 대한 경고를 제공합니다. 데이터 작업에 대한 제어 탑을 가지고 있는 것과 같으며 다양한 기술과 서비스와 매끄럽게 통합됩니다.\n\n요약하면, Airflow은 그냥 다른 도구가 아니라 현대 데이터 엔지니어링에 필수적인 자산입니다. ETL 프로세스에 질서, 신뢰성 및 효율성을 제공하여 팀이 혁신에 집중할 수 있도록 도와줍니다. Airflow에 대한 이 개요가 유익하고 통찰력 있었기를 바랍니다.\n\nAirflow 환경을 처음부터 설정하고 기본 ETL 파이프라인을 구현해 보고 싶다면 제 다른 글을 확인해보세요. 여기 링크입니다:\n\n<div class=\"content-ad\"></div>\n\n여기서 중요한 것은 테이블 태그를 마크다운 형식으로 변경하는 것입니다.","ogImage":{"url":"/assets/img/2024-06-20-UnderstandingAirflowandDAGsin3minutes_0.png"},"coverImage":"/assets/img/2024-06-20-UnderstandingAirflowandDAGsin3minutes_0.png","tag":["Tech"],"readingTime":3},{"title":"Snowflake를 위한 CICD 및 DevOps 포괄적인 가이드","description":"","date":"2024-06-20 15:25","slug":"2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide","content":"\n\n데이터 엔지니어링 및 데이터베이스 관리의 세계에서는 지속적 통합/지속적 배포(CI/CD) 실천이 민첩하고 신뢰성 있으며 효율적인 개발 워크플로우에 중요합니다. 성장 중인 클라우드 기반 데이터 웨어하우징 플랫폼인 Snowflake는 확장성, 적응성, 우수한 성능으로 유명합니다. 그러나 Snowflake를 위해 CI/CD를 구현하는 것은 표준화된 실천 방식과 특화된 도구 부재로 인한 독특한 도전에 직면하고 있습니다. 더하여 데이터베이스 프로젝트에 특화된 DevOps 및 CI/CD 워크플로에 대한 모범 사례를 상세히 설명하는 종합적인 문서 부재 문제가 있습니다.\n\n이러한 도전에도 불구하고, 최근 Snowflake 내에서 유망한 발전이 있었으며, 새로운 기능 도입으로 DevOps 및 CI/CD 실천에 대한 명확한 지침과 도구 제공에 대한 가능성을 시사하고 있습니다. 이에 따라 Snowflake 사용자를 위한 DevOps 프로세스 표준화 접근 방식과 향상된 문서 작성을 간소화하기 위한 기대가 커지고 있습니다.\n\n이 기사에서는 최근 기능 및 모범 사례를 활용한 Snowflake용 CI/CD 및 DevOps 설정의 총체적 데모에 대해 심층적으로 다룰 것입니다.\n\n# 소개: SQL Server에서 Snowflake로의 간극을 메꾸는 것\n\n<div class=\"content-ad\"></div>\n\nSQL Server 출신이신 분으로서 Snowflake로 전환하면서, 데이터베이스 객체를 관리하기 위한 표준이 없다는 점이 큰 장벽이었습니다. SQL Server의 SSDT(SQL Server Data Tools) 접근 방식과 DACPAC(Data-tier Application Component Package) 파일은 데이터베이스 변경 관리(DCM)를 선언적으로 처리하는 방법을 제공했지만, Snowflake에는 이와 같은 표준화된 접근 방식이 없었습니다. 초기 접근 방식은 Terraform과 같은 도구나 Schemachange 또는 Flyway와 같은 명령중심의 DCM 솔루션에 의존하는 경향이 있었는데, 이는 모든 사람들의 선호에 부합하지 않을 수 있습니다.\n\n다행히도 Snowflake은 CREATE OR ALTER, EXECUTE IMMEDIATE FROM, Snowflake CLI 및 Git 통합과 같은 선언적 DCM을 위한 기본 블록을 도입하고 있습니다. 이러한 발전을 통해 Snowflake에 대한 CI/CD에 대해 더 구조화되고 효율적인 접근 방식이 가능해졌습니다.\n\n## Snowflake의 CI/CD를 위한 Building Blocks 활용하기\n\nSnowflake의 최근 기능은 CI/CD 및 DevOps 실천을 효과적으로 구현하기 위한 기초를 제공합니다. 이러한 기본 블록은 배포 프로세스를 자동화하고 협업을 강화하며 환경 간 일관성을 보장하는 사용자들을 능력을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_0.png\" />\n\n1. CREATE OR ALTER\n\n스노우플레이크의 CREATE OR ALTER 기능을 사용하면 현재 상태에 대해 걱정할 필요없이 데이터베이스 테이블의 원하는 상태를 정의할 수 있습니다. 이 선언적 접근 방식은 데이터베이스 객체의 관리를 단순화하고 원활한 배포 워크플로우를 용이하게 합니다.\n\n2. EXECUTE IMMEDIATE FROM\n\n<div class=\"content-ad\"></div>\n\n#### 1. EXECUTE IMMEDIATE FROM command\n\nThe EXECUTE IMMEDIATE FROM command allows you to execute SQL statements stored in external files or URLs. This feature is crucial for automating deployment tasks and orchestrating deployment processes within Snowflake.\n\n#### 2. Snowflake CLI\n\nSnowflake CLI allows developers to run SQL queries, ad-hoc queries, or SQL query files effortlessly using the `snow sql` command. This functionality improves development workflows, making it easier to execute and manage queries efficiently within Snowflake environments.\n\n#### 3. Git Integration\n\n<div class=\"content-ad\"></div>\n\nSnowflake와 Git 저장소를 통합하면 코드의 중앙 집중식 데이터 원천이 제공되어 협업 및 버전 제어 기능을 강화할 수 있습니다. 개발자들은 Snowflake 환경 내에서 변경 사항을 추적하고 브랜치를 관리하며 Git 워크플로를 신속하게 활용할 수 있습니다. 이는 팀워크를 촉진하고 효율적인 배포 파이프라인을 용이하게합니다.\n\n# Snowflake를 위한 CI/CD 구현: 단계별 안내\n\n![이미지](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_1.png)\n\n## 사전 요구 사항:\n\n<div class=\"content-ad\"></div>\n\n- Snowflake 계정 (최소 sysadmin 액세스 권한)\n- GitHub 계정\n- Git 저장소 (관리자 액세스 권한)\n\n## 실제 데모 플로우:\n\nSnowflake를 위한 CI/CD를 설정하는 단계를 따라해보세요:\n\n- 데이터베이스 및 스키마 생성\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_2.png\" />\n\n다른 환경에서도 같은 작업을 수행하세요.\n\n참고: 데이터베이스 및 스키마는 기본적으로 변경되지 않는 객체이므로 Terraform과 같은 인프라 코드(IAC) 도구를 통해 이상적으로 관리됩니다. 그러나 이 데모에서는 수동으로 생성합니다.\n\n2. Snowflake를 Git 리포지토리에 연결하기\n\n<div class=\"content-ad\"></div>\n\nStep 2.1: GitHub에서 개인 액세스 토큰(PAT)을 생성합니다.\n\nGitHub 계정의 Settings → Developer Settings → Personal Access Tokens → Tokens (classic)으로 이동하여 PAT을 생성합니다.\n\nStep 2.2: GitHub PAT을 저장할 Secret을 생성합니다.\n\n![GitHub PAT 이미지](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_3.png)\n\n<div class=\"content-ad\"></div>\n\nStep 2.3: Snowflake 내에서 Git API 통합을 생성하세요.\n\n![이미지](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_4.png)\n\nStep 2.4: Snowflake 객체 및 구성을 저장할 Git 저장소를 설정하세요.\n\n![이미지](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_5.png)\n\n<div class=\"content-ad\"></div>\n\n3. 배포를 위한 서비스 계정 및 기타 Snowflake 객체 생성\n\n배포를 위해 서비스 계정 및 필요한 기타 Snowflake 객체를 설정하세요. 적절한 권한 및 접근 제어를 보장해주세요.\n\n![이미지](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_6.png)\n\n4. 로컬 개발\n\n<div class=\"content-ad\"></div>\n\n저희 지역 개발 환경 설정에 대해 Snowflake 프로젝트의 폴더 계층 구조를 아래 이미지처럼 구성하는 것을 권장합니다. 각 스키마마다 별도의 폴더가 할당되며, 각 스키마 폴더 내에서는 객체 유형에 따라 하위 폴더로 객체를 더 구성할 수 있습니다.\n\n뿐만 아니라, 루트 수준에 스크립트 폴더를 포함하는 것을 제안합니다. 이를 통해 사전 배포 및 사후 배포 작업을 수용하여 DACPAC이 데이터베이스 프로젝트를 구성하는 방식과 유사하게 잘 구성되고 쉽게 탐색할 수 있는 구조를 유지할 수 있습니다.\n\n테이블을 생성할 때 Snowflake의 CREATE OR ALTER 문을 사용하는 간편함과 효율성을 보여 드릴 수 있습니다. 이를 통해 새 테이블을 생성하거나 기존 테이블을 수정하는 등 테이블 스키마를 효과적으로 관리할 수 있습니다. CREATE OR REPLACE로 이미 정의된 테이블의 경우, CREATE OR ALTER로의 전환은 빠르게 조정할 수 있어 배포 파이프라인과의 원활한 호환성을 보장할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_8.png\" />\n\n또한 Jinja 템플릿을 통합하여 배포 프로세스를 확장할 수 있습니다. 이를 통해 유연성과 확장성을 제공합니다. 예를 들어 데이터베이스 이름을 매개변수화함으로써 배포 중에 대상 환경을 동적으로 선택할 수 있으며, 이는 다양한 배포 시나리오에서 프로세스를 간소화합니다.\n\n<img src=\"/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_9.png\" />\n\n다시 한번 제시된 폴더 계층 구조를 강조하면, 프로젝트 폴더 안에서 sf_deploy_dev.sql과 sf_deploy_tst.sql 두 가지 필수 스크립트를 찾을 수 있습니다. 이 스크립트는 Snowflake 객체의 배포를 조정하는 배포 진입점으로 작용합니다.\n\n<div class=\"content-ad\"></div>\n\n여기에 그들의 구조를 보여주는 샘플 스니펫이 있어요:\n\n\n![Snowflake CLI](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_10.png)\n\n\n5. 로컬 Snow CLI 사용\n\n내 경험상 Snowflake CLI를 전체 배포 파이프라인에 뛰어들기 전에 로컬에서 미리 테스트하는 것이 유익합니다. 이를 통해 만든 서비스 계정을 사용하여 연결을 확인하고 올바른 액세스 권한을 확인하며 쉬운 디버깅 및 테스트를 용이하게 할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n먼저 Snow CLI를 설치해야 합니다. Snow CLI는 Snowflake 문서에 제공된 설치 지침을 따라 간단히 설치할 수 있어요.\n\n![Snowflake CLI](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_11.png)\n\n설치가 완료되면 CLI 사용 및 명령어에 대한 안내를 위해 문서를 참조할 거에요. 우리의 경우 배포 워크플로에서 두 가지 SQL 명령어를 주로 활용할 거에요.\n\n먼저 ALTER GIT REPOSITORY 명령어를 사용하여 링크된 Git 저장소에서 업데이트를 가져올 거에요. 이를 통해 Snowflake 환경이 코드베이스의 최신 변경 사항과 동기화되도록 할 거에요.\n\n<div class=\"content-ad\"></div>\n\n표시되어 있는 내용을 아래와 같이 번역해 드리겠습니다.\n\n\n![2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_12](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_12.png)\n\n그 후에는 EXECUTE IMMEDIATE FROM 명령어를 사용하여 외부 파일이나 URL에 저장된 SQL 문을 실행할 것입니다. 우리의 경우에는 이 명령어를 사용하여 대상 데이터베이스로 객체를 배포할 것이며, 배포 스크립트 경로를 참조하게 됩니다. 예를 들면 sf_deploy_dev.sql과 같습니다.\n\n![2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_13](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_13.png)\n\nSnowflake CLI 명령어에서는 계정, 사용자, 창고와 같은 입력값을 지정하여 연결을 설정할 것입니다. 이후에는 이러한 입력값들을 GitHub 비밀에 저장하여 보안 및 편의성을 갖출 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n6. GitHub Workflow를 만들기\n\nGitHub Actions를 구성하여 대상 환경으로의 배포를 자동화합니다.\n\n7. GitHub Secrets 채우기\n\nGitHub에 필요한 시크릿과 환경 변수를 채워 넣어, 워크플로우 실행 중에 Snowflake 및 Git 리포지토리에 안전하게 액세스합니다.\n\n<div class=\"content-ad\"></div>\n\n리포지토리에서 Settings → Secrets and Variables → Actions → New repository secret로 이동하세요.\n\n![이미지](/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_14.png)\n\n8. 테스트 워크플로우\n\n수동으로 또는 자동화된 이벤트(예: main으로 푸시/머지)를 트리거하여 구성된 GitHub 워크플로우를 유효성 검사하고, 배포 작업의 성공적인 실행과 올바른 처리를 확인하세요.\n\n<div class=\"content-ad\"></div>\n\n# Snowflake에서 유의할 사항 및 권장 사항\n\n## 유의할 사항:\n\n1. CREATE OR ALTER TABLE 사용 시 제한 사항\n\n- 이 명령의 제한 사항은 특히 특정 위치에 열을 삽입해야 할 때 제한적일 수 있습니다.\n- 테이블에 대한 제약은 이 명령을 사용하여 뷰나 저장 프로시저와 같은 다른 데이터베이스 객체를 변경할 수 없다는 것을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n2. Snowsight에서 CREATE OR ALTER 동작\n\n- 명령이하는 작업과 Snowsight에 표시되는 것 사이의 불일치는 정말 혼란스러울 수 있습니다. CREATE OR ALTER를 통해 테이블이 생성되었더라도, Snowsight에서는 정의상 CREATE OR REPLACE로 표시됩니다. 이것이 Snowflake의 의도인지 또는 버그인지 확실하지 않습니다.\n\n3. YAML 파이프라인에서의 단일 데이터베이스 연결\n\n- 모든 환경에서 동일한 데이터베이스 연결을 사용하여 일관성을 유지하는 것이 실용적으로 보일 수 있지만, Snowflake가 이 접근 방식을 권장하거나 요구하는지를 고려하는 것이 중요합니다. 테스트(TST) 및 프로덕션(PRD)과 같이 더 높은 환경으로 전개하더라도, Git 저장소가 연결된 개발(dev) 환경과 일치하는 단일 연결을 유지합니다. 이 선택은 EXECUTE IMMEDIATE와 같은 SQL 명령의 올바른 실행을 보장합니다. 그러나 Snowflake가 각 환경에 대해 별도의 Git 저장소 연결을 설정하여 다른 데이터베이스 연결 사용을 가능하게 하는 것을 필수로 하는지는 명확하지 않습니다. Snowflake의 권장 사항에 대한 명확한 설명이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n## 권장사항:\n\n1. 배포 프로세스 중 더 나은 가시성을 위한 강화된 로깅 기능.\n\n- 현재 배포 중 로그에서는 마지막 EXECUTE IMMEDIATE 명령의 출력만 캡처되어 전체 배포 프로세스의 가시성이 제한됩니다.\n- Snowflake이 배포 중 수행하는 모든 작업을 포괄적으로 보여주는 강화된 로깅 기능을 구현하는 것이 유익할 것입니다. 이렇게 하면 배포 활동을 더 잘 추적하고 문제 해결 및 감사에 도움이 될 수 있습니다.\n\n2. Snowflake에 의한 배포 artifacts의 개발\n\n<div class=\"content-ad\"></div>\n\n- 높은 환경으로의 배포 프로세스를 간단히 하기 위해서는 Snowflake에 의한 배포 아티팩트 또는 배포 패키지의 개발을 탐색하는 것이 좋습니다.\n- DACPAC(배포용 아티팩트로 빌드되는 SSDT의 접근 방식과 유사하게, Snowflake에서는 배포 아티팩트를 생성하고 관리하기 위한 간소화된 메커니즘을 제공할 수 있습니다.\n- 이 접근 방식은 Snowflake가 다른 환경으로의 배포를 위해 필요한 SQL 스크립트, 구성 및 메타데이터를 포함하는 배포 패키지 또는 아티팩트를 생성하는 것을 포함할 것입니다.\n\nSnowflake에 대한 CI/CD 및 DevOps 실천 방법을 구현하면, 기관은 데이터 엔지니어링 워크플로우에서 민첩성, 협업 및 자동화를 채택할 수 있습니다. 이 가이드에서 소개된 Snowflake의 최신 기능과 모베스트 프랙티스를 활용하여, 팀은 배포 프로세스를 최적화하고 오류를 최소화하며 데이터 중심 이니셔티브의 시장 진입 시간을 가속화할 수 있습니다.\n\nSnowflake가 계속 발전함에 따라, 현대 데이터 엔지니어링의 빠르게 변화하는 지형에서 선도하기 위해 CI/CD를 채택하는 것이 중요해집니다.","ogImage":{"url":"/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_0.png"},"coverImage":"/assets/img/2024-06-20-CICDandDevOpsforSnowflakeAComprehensiveGuide_0.png","tag":["Tech"],"readingTime":9},{"title":"Chronon, 에어비앤비의 기계 학습 피처 플랫폼, 이제 오픈 소스로 공개합니다","description":"","date":"2024-06-20 15:22","slug":"2024-06-20-ChrononAirbnbsMLFeaturePlatformIsNowOpenSource","content":"\n\n테이블 태그를 Markdown 형식으로 변경해주세요.\n\nA feature platform that offers observability and management tools, allows ML practitioners to use a variety of data sources, while handling the complexity of data engineering, and provides low latency streaming.\n\n![ChrononAirbnbsMLFeaturePlatformIsNowOpenSource_0](/assets/img/2024-06-20-ChrononAirbnbsMLFeaturePlatformIsNowOpenSource_0.png)\n\nBy: Varant Zanoyan, Nikhil Simha Raprolu\n\n![ChrononAirbnbsMLFeaturePlatformIsNowOpenSource_1](/assets/img/2024-06-20-ChrononAirbnbsMLFeaturePlatformIsNowOpenSource_1.png)\n\n<div class=\"content-ad\"></div>\n\nAirbnb에서는 우리의 ML Feature Platform 인 Chronon을 오픈 소스로 공개하게 된 것을 기쁘게 생각합니다. 우리 커뮤니티 Discord 채널에 참여하여 함께 대화해보세요.\n\n우리는 Stripe와 함께 이 프로젝트의 초기 채택자 및 공동 유지 보수자로서 이 공지를 함께 전하게 되어 기쁘게 생각합니다.\n\n이 블로그 글은 Chronon의 주요 동기와 기능에 대해 다루고 있습니다. Chronon의 핵심 개념에 대한 개요는 이전 게시물을 참조해주세요.\n\n# 배경\n\n<div class=\"content-ad\"></div>\n\nChronon은 ML 실무자들의 공통된 고민을 해소하고자 만들어졌어요: 모델링에 대부분의 시간을 쏟는 대신 모델을 구축하는 데 필요한 데이터를 관리하는 일에 시간을 많이 써야 했기 때문이죠.\n\nChronon 이전에는, 실무자들은 일반적으로 다음 두 가지 방법 중 하나를 사용했어요:\n\n- 오프라인-온라인 복사: ML 실무자들은 데이터 웨어하우스에서 모델을 학습한 뒤 온라인 환경에서 해당 기능을 복제하는 방법을 찾았죠. 이 방법의 이점은 데이터 웨어하우스 전체, 즉 데이터 소스와 대규모 데이터 변환에 사용되는 강력한 도구를 활용할 수 있다는 것이에요. 그러나 이 방법은 온라인 추론에 모델 기능을 제공하는 명확한 방법을 제공하지 않아 일관성 및 레이블 유출로 모델 성능에 심각한 영향을 미치는 단점이 있어요.\n- 기록하고 대기: ML 실무자들은 모델 추론을 실행할 온라인 서빙 환경에서 사용 가능한 데이터로 시작합니다. 관련 특징을 데이터 웨어하우스에 기록합니다. 충분한 데이터가 축적되면, 해당 로그를 사용하여 모델을 학습하고 같은 데이터로 서빙합니다. 이 방법의 이점은 일관성이 보장되며 유출 가능성이 낮다는 것이에요. 그러나 이 방법의 주요 단점은 대기 시간이 길어질 수 있어서 변화하는 사용자 행동에 빠르게 대응하는 능력을 저해할 수 있다는 것이죠.\n\nChronon 접근 방식은 양쪽을 모두 최대한 활용할 수 있게 해줘요. Chronon을 사용하면 ML 실무자들은 특징을 한 번 정의하면 됩니다. 이를 통해 오프라인 흐름은 모델 학습을 위한 것뿐만 아니라 온라인 흐름에서 모델 추론에도 사용됩니다. 또한, Chronon은 특징 연결, 가시성 및 데이터 품질을 위한 강력한 도구, 특징 공유 및 관리도 제공해요.\n\n<div class=\"content-ad\"></div>\n\n# 작동 방식\n\n아래에서는 Chronon의 대부분의 기능을 구동하는 주요 구성 요소를 살펴보며 빠른 시작 가이드에서 파생된 간단한 예제를 사용합니다. 이 예제를 실행하려면 해당 가이드를 따를 수 있습니다.\n\n온라인 대규모 소매업체라고 가정해 보겠습니다. 사용자들이 구매를 하고 나중에 상품을 반품하는 사기 벡터를 감지했습니다. 주어진 거래가 사기적인 반품으로 이어질 가능성을 예측하는 모델을 훈련하고자 합니다. 사용자가 체크아웃 과정을 시작할 때마다 이 모델을 호출할 것입니다.\n\n## 기능 정의\n\n<div class=\"content-ad\"></div>\n\n구매 데이터: 구매 로그 데이터를 사용자 수준으로 집계하여이 사용자가 플랫폼에서 이전에 수행한 활동을 볼 수 있습니다. 구체적으로 다양한 시간 창을 통해 이전 구매 금액의 SUM, COUNT 및 AVERAGE를 계산할 수 있습니다.\n\n```js\nsource = Source(\n    events=EventSource(\n        table=\"data.purchases\", # 이는 매일 일괄 업데이트되는 데이터 웨어하우스의 로그 테이블을 가리킵니다.\n        topic=\"events/purchases\", # 스트리밍 소스 토픽\n        query=Query(\n            selects=select(\"user_id\",\"purchase_price\"), # 우리가 관심 있는 필드를 선택합니다.\n            time_column=\"ts\") # 이벤트 시간\n    ))\n\nwindow_sizes = [Window(length=day, timeUnit=TimeUnit.DAYS) for day in [3, 14, 30]] # 아래에서 사용할 몇 가지 창 크기를 정의합니다.\n\nv1 = GroupBy(\n    sources=[source],\n    keys=[\"user_id\"], # 사용자별 집계 중\n    online=True,\n    aggregations=[Aggregation(\n            input_column=\"purchase_price\",\n            operation=Operation.SUM,\n            windows=window_sizes\n        ), # 다양한 창에서 구매 가격의 합\n        Aggregation(\n            input_column=\"purchase_price\",\n            operation=Operation.COUNT,\n            windows=window_sizes\n        ), # 다양한 창에서 구매 횟수\n        Aggregation(\n            input_column=\"purchase_price\",\n            operation=Operation.AVERAGE,\n            windows=window_sizes\n        ), # 다양한 창에서 사용자별 평균 구매\n        Aggregation(\n            input_column=\"purchase_price\",\n            operation=Operation.LAST_K(10),\n        ), # 목록으로 집계된 마지막 10개 구매 가격\n    ],\n)\n```\n\n이렇게하면 `GroupBy`가 생성되어 `purchases` 이벤트 데이터를 사용자별 주요 키로 계속하여 시간 범위별로 다양한 필드를 집계하여 유용한 기능으로 변환합니다.\n\n이는 사용자 수준에서 구매 기록 데이터를 유용한 기능으로 변환합니다.\n\n<div class=\"content-ad\"></div>\n\n유저 데이터: 유저 데이터를 피처로 변환하는 것은 조금 더 간단합니다. 주로 집계를 수행할 필요가 없기 때문입니다. 이 경우, 원본 데이터의 주요 키와 피처의 주요 키가 동일하므로 행을 대상으로 집계를 수행하는 대신 단순히 열 값을 추출할 수 있습니다:\n\n```js\nsource = Source(\n    entities=EntitySource(\n        snapshotTable=\"data.users\", # 모든 사용자의 일별 스냅샷을 포함하는 테이블을 가리킴\n        query=Query(\n            selects=select(\"user_id\",\"account_created_ds\",\"email_verified\"), # 관심 있는 필드를 선택\n        )\n    ))\n\nv1 = GroupBy(\n    sources=[source],\n    keys=[\"user_id\"], # 주요 키는 소스 테이블의 주요 키와 동일함\n    aggregations=None, # 이 경우 집계나 윈도우를 정의할 필요가 없음\n    online=True,\n) \n```\n\n이를 통해 `user_id`를 주요 키로 사용하여 `data.users` 테이블로부터 차원을 추출하는 `GroupBy`가 생성됩니다.\n\n피처를 결합하기: 다음으로, 이전에 정의된 피처를 백필링되어 모델 훈련에 사용되고 모델 추론을 위해 온라인으로 제공될 수 있는 단일 뷰로 결합해야 합니다. Join API를 사용하여 이를 달성할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n우리의 사용 사례에서 피처가 올바른 타임스탬프로 계산되는 것이 매우 중요합니다. 모델이 체크아웃 플로우를 시작할 때 실행되기 때문에, 온라인 추론에서 모델이 볼 것과 일치하는 피처 값이 모델 훈련을 위해 논리적으로 일치하도록 백필에 해당 타임스탬프를 사용하고 싶습니다.\n\n아래는 정의가 어떻게 보이는지에 대한 예시입니다. 기존에 정의된 피처와 returns(반품)이라는 다른 피처 세트를 API의 right_parts 부분에 결합하는 것을 확인할 수 있습니다.\n\n```js\nsource = Source(\n    events=EventSource(\n        table=\"data.checkouts\",\n        query=Query(\n            selects=select(\"user_id\"), # 다양한 GroupBy를 함께 결합하기 위해 사용되는 기본 키\n            time_column=\"ts\",\n            ) # 피처 값을 계산하기 위해 사용되는 이벤트 시간\n    ))\n\nv1 = Join(\n    left=source,\n    right_parts=[JoinPart(group_by=group_by) for group_by in [purchases_v1, returns_v1, users]] # 세 가지 GroupBy를 포함\n)\n```\n\n# 백필/오프라인 계산\n\n<div class=\"content-ad\"></div>\n\n위에 나와 있는 Join 정의로 사용자가 할 수 있는 첫 번째 작업은 역사적인 피쳐 값들을 모델 훈련을 위해 생성하기 위해 백필을 실행하는 것입니다. Chronon은 몇 가지 주요 이점을 갖고 이 백필을 수행합니다:\n\n- 시점 정확도: 위의 Join에서 \"left\" 쪽으로 사용된 소스를 주목해보세요. 이 소스는 각 행마다 해당 특정 체크아웃의 논리적 시간에 해당하는 \"ts\" 타임스탬프를 포함하는 \"data.checkouts\" 소스 위에 구축되어 있습니다. 모든 피쳐 계산은 해당 타임스탬프를 기준으로 창 정확성이 보장됩니다. 따라서 이전 사용자 구매의 1개월 합계에 대한 경우, 왼쪽 소스에서 제공된 타임스탬프를 기준으로 각 행이 사용자에 대해 계산될 것입니다.\n- Skew 처리: Chronon의 백필 알고리즘은 매우 편향된 데이터셋을 처리하는 데 최적화되어 있어, 귀찮은 OOM과 작업 멈춤을 피할 수 있습니다.\n- 계산 효율 최적화: Chronon은 백엔드에 직접 여러 최적화를 적용할 수 있어 계산 시간과 비용을 줄일 수 있습니다.\n\n# 온라인 계산\n\nChronon은 온라인 피쳐 계산을 위한 많은 복잡성을 추상화합니다. 위의 예에서 해당 피쳐가 일괄 피쳐인지 또는 스트리밍 피쳐인지에 따라 피쳐를 계산할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n일괄 기능 (예: 위의 사용자 기능)\n\n사용자 기능은 기존의 일괄 테이블 위에 구축되어 있기 때문에, Chronon은 매일 배치 작업을 실행하여 새 데이터가 일괄 데이터 저장소에 도착하면 새 기능 값을 계산하고 이를 온라인 KV 저장소에 업로드하여 제공합니다.\n\n스트리밍 기능 (예: 위의 구매 기능)\n\n구매 기능은 소스에 스트리밍 구성 요소를 포함한 소스에 기초로 구축되어 있습니다. 이는 소스에 \"주제\"가 포함되어 있음으로 나타납니다. 이 경우 Chronon은 실시간 업데이트를 위해 배치 업로드와 함께 스트리밍 작업도 실행합니다. 배치 작업은 다음을 담당합니다:\n\n<div class=\"content-ad\"></div>\n\n- 값 시딩하기: 긴 윈도우의 경우, 스트림을 되감아서 모든 원시 이벤트를 재생하는 것은 실용적이지 않을 수 있습니다.\n- \"윈도우의 중간\"을 압축하고 꼬리 정확도 제공하기: 정확한 윈도우 정확도를 위해서는, 윈도우의 머리와 꼬리에서 원시 이벤트가 필요합니다.\n\n그러면 스트리밍 작업이 업데이트를 KV 스토어에 기록하여, 추출 시 특성 값을 최신 상태로 유지합니다.\n\n# 온라인 서빙 / 추출 API\n\nChronon은 낮은 대기 시간으로 특성을 추출하기 위한 API를 제공합니다. 우리는 개별 GroupBys(즉, 위에서 정의한 사용자 또는 구매 특성)에 대한 값이나 Join에 대한 값 중 하나를 가져올 수 있습니다. 다음은 Join에 대한 하나의 해당 요청과 응답의 예시입니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n// 사용자=123에 대한 모든 기능을 검색 중\nMap<String, String> keyMap = new HashMap<>();\nkeyMap.put(\"user\", \"123\")\nFetcher.fetch_join(new Request(\"quickstart_training_set_v1\", keyMap));\n// 샘플 응답 (기능 이름과 값의 맵)\n'{\"purchase_price_avg_3d\":14.2341, \"purchase_price_avg_14d\":11.89352, ...}'\n```\n\n사용자 123에 대한 모든 기능을 가져오는 Java 코드입니다. 반환 형식은 기능 이름과 기능 값의 맵입니다.\n\n위 예제는 Java 클라이언트를 사용합니다. 쉬운 테스트와 디버깅을 위해 Scala 클라이언트와 Python CLI 도구도 있습니다:\n\n```js\nrun.py --mode=fetch -k '{\"user_id\":123}' -n quickstart/training_set -t join\n\n> {\"purchase_price_avg_3d\":14.2341, \"purchase_price_avg_14d\":11.89352, ...}\n```\n\n<div class=\"content-ad\"></div>\n\nrun.py CLI 도구를 활용하여 상단의 Java 코드와 동일한 가져오기 요청을 수행할 수 있어요. run.py는 Chronon 워크플로우를 빠르게 테스트하는 편리한 방법이에요.\n\n또 다른 옵션은 이러한 API를 서비스로 래핑하여 REST 엔드포인트를 통해 요청하는 것이에요. 이 방법은 Airbnb 내에서 Ruby와 같은 Java 이외의 환경에서 기능을 가져오기 위해 사용돼요.\n\n# 온라인-오프라인 일관성\n\nChronon은 온라인-오프라인 정확도에 도움을 주는 것뿐만 아니라 이를 측정하는 방법도 제공해요. 측정 파이프라인은 온라인 가져오기 요청의 로그에서 시작돼요. 이 로그에는 요청의 기본 키와 타임스탬프가 포함되어 있습니다. 그리고 가져온 기능 값도 포함돼요. 그런 다음, Chronon은 키와 타임스탬프를 왼쪽 측으로 하는 Join backfill로 전달하고, 계산 엔진에게 기능 값을 backfill하도록 요청해요. 그런 다음 backfill된 값을 실제 가져온 값과 비교하여 일관성을 측정해요.\n\n<div class=\"content-ad\"></div>\n\n# 다음은 무엇인가요?\n\n오픈 소스는 Stripe와 넓은 커뮤니티와 함께 나아갈 흥미로운 여정의 첫걸음일 뿐입니다.\n\n저희의 비전은 ML 실무자들이 자신들의 데이터를 활용하는 가장 최선의 결정을 내릴 수 있도록 돕고, 이 결정을 실행하는 과정을 가능한 쉽게 만들어 주는 플랫폼을 만드는 것입니다. 현재 우리의 로드맵을 구성하는 데 사용 중인 몇 가지 질문입니다:\n\n반복과 계산 비용을 얼마나 더 낮출 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\n크로논은 이미 Airbnb 및 Stripe와 같은 대기업이 처리하는 데이터 규모에 적합하게 구축되어 있습니다. 그러나 우리의 컴퓨트 엔진을 최적화하여 컴퓨트 비용을 줄이고 새로운 기능을 만들고 실험하는 \"시간 비용\"을 줄일 수 있는 방법이 항상 있습니다.\n\n새로운 기능을 작성하는 것을 얼마나 더 쉽게 만들 수 있을까요?\n\n기능 엔지니어링은 인간이 도메인 지식을 표현하여 모델이 활용할 수 있는 신호를 생성하는 과정입니다. 크로논은 NLP를 통합하여 ML 전문가들이 이러한 기능 아이디어를 자연어로 표현하고 작동하는 기능 정의 코드를 생성할 수 있도록 하여 이터레이션을 시작할 수 있도록 지원할 수 있습니다.\n\n기술적 장벽을 낮추어 기능 생성을 보다 쉽게 하는 것은 결과적으로 ML 전문가들과 가치 있는 도메인 전문 지식을 보유한 파트너들 간의 새로운 협력을 열 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n모델 유지 관리 방식을 개선할 수 있을까요?\n\n사용자 행동 변경으로 모델 성능이 변화할 수 있습니다. 왜냐하면 모델이 훈련된 데이터가 현재 상황에 적용되지 않을 수 있기 때문입니다. 우리는 이러한 변화를 감지하고 이에 대응하는 전략을 조기에 예방적으로 만들어내는 플랫폼을 상상합니다. 이를 위해 모델을 재학습하거나 새로운 기능을 추가하거나 기존 기능을 수정하거나 그 이상의 조합으로 전략을 개발할 수 있습니다.\n\n플랫폼 자체가 ML 실무자가 최상의 모델을 구축하고 배포하는 데 도움이 되는 지능형 에이전트가 될 수 있을까요?\n\n플랫폼 레이어로 수집하는 메타데이터가 많아지면, 일반적인 ML 어시스턴트로서 플랫폼이 더욱 강력해질 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n새로운 데이터로 실험을 자동으로 실행하여 모델을 개선하는 방법을 식별하는 플랫폼을 만드는 목표를 언급했습니다. 이러한 플랫폼은 ML 전문가들이 \"이 사용 사례를 모델링할 때 가장 유용한 특성은 무엇인가요?\" 또는 \"이 목표에 관한 신호를 캡처하는 특성을 만들 수 있는 어떤 데이터 소스가 있을까요?\"와 같은 질문을 할 수 있도록하여 데이터 관리에도 도움이 될 수 있습니다. 이러한 종류의 질문에 답변할 수 있는 플랫폼은 지능적 자동화의 다음 수준을 나타냅니다.\n\n# 시작하기\n\n다음은 시작하는 데 도움이 되는 리소스 또는 팀에 적합한지 평가하는 데 도움이 되는 리소스입니다.\n\n- Github 프로젝트, Chronon 웹 사이트 및 특히 빠른 시작 가이드를 확인해보세요.\n- 커뮤니티 디스코드 채널에 들러보세요. 에어비앤비 및 Stripe 팀은 Chronon이 여러분의 스택에 어떻게 맞을지에 대해 여러분과 채팅하는 데 열정적입니다.\n\n<div class=\"content-ad\"></div>\n\n이런 종류의 작업에 관심이 있으세요? 여기에서 열려 있는 역할을 확인해보세요 — 채용 중입니다.\n\n# 감사의 말\n\n후원자: Henry Saputra Yi Li Jack Song\n\n기여자: Pengyu Hou Cristian Figueroa Haozhen Ding Sophie Wang Vamsee Yarlagadda Haichun Chen Donghan Zhang Hao Cen Yuli Han Evgenii Shapiro Atul Kale Patrick Yoon","ogImage":{"url":"/assets/img/2024-06-20-ChrononAirbnbsMLFeaturePlatformIsNowOpenSource_0.png"},"coverImage":"/assets/img/2024-06-20-ChrononAirbnbsMLFeaturePlatformIsNowOpenSource_0.png","tag":["Tech"],"readingTime":10},{"title":"초보자를 위한 엔드 투 엔드 Airflow 프로젝트 베를린 날씨 데이터 스크래핑 및 Amazon S3에 업로드하기","description":"","date":"2024-06-20 15:21","slug":"2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3","content":"\n\n파이썬 개발과 Apache Airflow에 열정을 가진 데이터 엔지니어로서, 베를린의 최신 날씨 데이터를 가져 와 CSV 파일로 저장하고 Amazon S3로 업로드하는 프로젝트를 시작했습니다. 이 튜토리얼에서는 Python, 웹 스크래핑을 위한 BeautifulSoup, 데이터 조작을 위한 Pandas, 그리고 오케스트레이션을 위한 Airflow를 사용한 전체 설정 및 구현 방법을 안내해 드릴 겁니다.\n\n# 프로젝트 개요\n\n이 프로젝트에서는 다음을 목표로 합니다:\n\n- 날씨 데이터 스크래핑: 날씨 웹사이트에서 베를린의 실시간 날씨 정보를 가져 오는 웹 스크래핑 기술을 활용합니다.\n- 데이터 로컬 저장: 가져온 데이터를 로컬 파일 시스템의 CSV 파일에 저장합니다.\n- Amazon S3로 업로드: 날씨 데이터가 포함된 CSV 파일을 Amazon S3 버킷에 업로드하는 메커니즘을 구현합니다.\n- Airflow로 자동화: Apache Airflow를 사용하여 매 시간마다 데이터 가져오기와 업로드 프로세스를 자동화하고 예약합니다.\n\n<div class=\"content-ad\"></div>\n\n# 사용된 도구 및 기술\n\n- Python: 스크립팅 및 데이터 조작에 사용됩니다.\n- BeautifulSoup: HTML 및 XML 문서 구문 분석을 위한 Python 라이브러리로, 여기서 웹 스크래핑에 사용됩니다.\n- Pandas: 파이썬에서 데이터를 분석하고 조작하는 강력한 도구로, 표 형식의 데이터를 처리하고 다루는 데 활용됩니다.\n- Apache Airflow: 워크플로우를 프로그래밍적으로 작성, 예약 및 모니터링하는 오픈 소스 도구입니다.\n- Amazon S3: 스케일링 가능한 객체 저장 서비스인 Amazon Simple Storage Service로, 데이터를 저장하고 검색하는 데 사용됩니다.\n\n# 구현 단계별 안내\n\n# 1. 환경 설정하기\n\n<div class=\"content-ad\"></div>\n\nPython이 설치되어 있고 필요한 라이브러리(requests, beautifulsoup4, pandas, AWS SDK의 boto3)가 함께 설치되었는지 확인하기 위해 다음 명령을 실행해주세요:\n\n```js\npip install requests beautifulsoup4 pandas boto3\n```\n\n# 2. 날씨 데이터 수집 및 로컬에 데이터 저장\n\n추출한 날씨 데이터를 Pandas를 사용하여 로컬 CSV 파일에 저장하세요:\n\n<div class=\"content-ad\"></div>\n\n'Alex The Analyst' YouTube 채널에서 BeautifulSoup를 사용하여 스크랩을 배웠어요. 완성된 재생 목록을 확인해보세요.\n\n```js\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nimport pytz\n```\n\n```js\n# 날씨 데이터를 업데이트하고 CSV로 저장하는 함수\ndef update_weather(**kwargs):\n    url = 'https://weather.com/weather/today/l/52.52,13.40'\n    page = requests.get(url)\n    if page.status_code != 200:\n        raise Exception(f\"페이지를 가져오지 못했습니다: {page.status_code}\")\n    soup = BeautifulSoup(page.text, 'html.parser')\n    # 체감 온도에 대한 제목 찾기\n    title_element = soup.find(class_='TodayDetailsCard--feelsLikeTempLabel--1UNV1')\n    if title_element:\n        title = title_element.text.strip()\n    else:\n        title = \"체감 온도\"\n    # DataFrame을 저장할 파일 경로\n    file_path = '/opt/airflow/dags/weather_checkin.csv'\n    # 파일이 있는지 확인\n    if os.path.exists(file_path):\n        logging.info(f\"파일 {file_path}이 존재합니다. 기존 DataFrame을 불러옵니다.\")\n        # 기존 DataFrame 불러오기\n        current_weather_berlin_df = pd.read_csv(file_path)\n    else:\n        logging.info(f\"파일 {file_path}이 존재하지 않습니다. 새 DataFrame을 생성합니다.\")\n        # 제목과 날짜 및 시간 열을 가진 새 DataFrame 초기화\n        current_weather_berlin_df = pd.DataFrame(columns=[title, 'date_time'])\n    # 체감 온도 값 찾기\n    value_element = soup.find('span', class_='TodayDetailsCard--feelsLikeTempValue--2icPt')\n    if value_element:\n        feels_like_temp = value_element.text.strip()\n    else:\n        feels_like_temp = None\n    # 베를린 시간대의 현재 날짜 및 시간 가져오기\n    berlin_tz = pytz.timezone('Europe/Berlin')\n    current_datetime = datetime.now(berlin_tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n    # DataFrame에 데이터 추가\n    if feels_like_temp:\n        logging.info(f\"새 데이터 추가 중: {feels_like_temp}, {current_datetime}\")\n        new_data = {title: [feels_like_temp], 'date_time': [current_datetime]}\n        current_weather_berlin_df = current_weather_berlin_df.append(pd.DataFrame(new_data), ignore_index=True)\n    else:\n        logging.error(\"체감 온도 값 찾기를 실패했습니다\")\n    # DataFrame을 CSV 파일로 저장\n    logging.info(f\"{file_path}에 DataFrame을 저장 중\")\n    current_weather_berlin_df.to_csv(file_path, index=False)\n    logging.info(current_weather_berlin_df)\n    # S3에 CSV 업로드\n    bucket_name = 'myfirstbucketsoumya'  # S3 버킷 이름으로 대체\n    s3_key = 'current_weather_berlin.csv'  # 원하는 S3 키로 대체\n    upload_to_s3(file_path, bucket_name, s3_key)\n```\n\n# 3. Amazon S3로 업로드하기\n\n<div class=\"content-ad\"></div>\n\n아래는 boto3를 사용하여 CSV 파일을 Amazon S3에 업로드하는 기능을 구현한 것입니다:\n\n```python\nimport boto3\n```\n\n```python\n# AWS 자격 증명\nAWS_ACCESS_KEY_ID = 'your-access-key-id' # 자격 증명을 하드코딩합니다.\nAWS_SECRET_ACCESS_KEY = 'your-secret-access-key'\nAWS_REGION = 'eu-central-1'\n# 현재 날씨 CSV를 S3에 업로드하는 함수\ndef upload_to_s3(file_path, bucket_name, s3_key):\n    try:\n        # 자격 증명을 사용하여 Amazon S3와의 세션을 초기화합니다.\n        s3 = boto3.client(\n            's3',\n            aws_access_key_id='your-access-key-id', # 값을 하드코딩합니다.\n            aws_secret_access_key='your-secret-access-key', # 값을 하드코딩합니다.\n            region_name='eu-central-1' # 값을 하드코딩합니다.\n        )\n        bucket_name = 'myfirstbucketsoumya'\n        file_key = 'hourly_berlin_weather.txt'\n        # CSV 파일을 S3에 업로드합니다.\n        s3.upload_file(file_path, bucket_name, s3_key)\n        logging.info(f\"날씨 데이터를 S3에 업로드했습니다: s3://{bucket_name}/{s3_key}\")\n    except Exception as e:\n        logging.error(f\"S3로 업로드 실패: {e}\")\n```\n\ndocker-compose.yaml에 몇 가지 변경 사항이 있습니다. AWS 자격 증명과 requirements.txt 컨테이너를 업데이트하십시오.\n\n<div class=\"content-ad\"></div>\n\n```js\n#변경 1\nx-airflow-common:\n  &airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.9.1}\n  environment:\n    &airflow-common-env\n    PYTHONPATH: /opt/airflow/dags/airflow_env_bs/lib/python3.12/site-packages\n \n    #AWS_ACCESS_KEY_ID: your-access-key-id #값을 하드코딩\n    #AWS_SECRET_ACCESS_KEY: your-secret-access-key #값을 하드코딩\n    #AWS_REGION: eu-central-1 #값을 하드코딩\n\n#변경-2\nairflow-init:\n    <<: *airflow-common\n    entrypoint: /bin/bash\n    command: >\n      -c \"pip install -r /requirements.txt && airflow webserver\" \n```\n\n# 5. Apache Airflow로 자동화하기\n\n마지막으로 Apache Airflow를 사용하여 전체 프로세스를 조율하세요. 다음은 DAG를 정의하는 방법입니다:\n\n```js\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n# DAG 정의\nDAG_NAME = 'berlin-weather'\ndefault_args = \n{'owner': 'airflow',\n'depends_on_past': False,\n'start_date': datetime(2023, 6, 19),\n'retries': 1,\n'retry_delay': timedelta(minutes=5),\n}\ndag = DAG(\ndag_id=DAG_NAME,\ndescription='베를린 날씨 매 시간 갱신',\nschedule_interval='@hourly',\ndefault_args=default_args,\ncatchup=False,\n)\n# 작업 정의\nupdate_weather_task = PythonOperator(\ntask_id='update_weather',\npython_callable=update_weather,\ndag=dag,\n)\n# 작업 의존성\nupdate_weather_task\n```\n\n# 5. CSV 파일을 위한 S3 버킷 확인\n\nAirflow 웹 서버를 열고 DAG를 실행하세요.\n\n<img src=\"/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_0.png\" />\n\n<div class=\"content-ad\"></div>\n\nDAG를 트리거한 후에 \"current_weather_berlin.csv\"라는 S3 Bucket을 확인해보세요. 거기에는 데이터 폴더가 있을 겁니다.\n\n![이미지](/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_1.png)\n\n# 결론\n\n이 프로젝트에서는 Python을 사용하여 베를린 날씨 데이터를 가져오고 로컬에 저장하며 Amazon S3로 업로드하는 프로세스를 자동화하는 방법을 탐색했습니다. 이를 위해 웹 스크래핑용 BeautifulSoup, 데이터 처리용 Pandas, 그리고 워크플로우 자동화용 Apache Airflow를 사용했습니다. 이러한 단계를 따라가면 이 프로젝트를 적응하고 확장하여 보다 복잡한 데이터 파이프라인 및 통합을 처리할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n제 Github 저장소를 확인하러 가보세요: Scraping-Berlin-Weather-Data-and-Uploading-to-Amazon-S3\n\n태그: airflow, 데이터 엔지니어링 프로젝트, AWS S3, 도커, 초보자 프로젝트","ogImage":{"url":"/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_0.png"},"coverImage":"/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_0.png","tag":["Tech"],"readingTime":7},{"title":"리눅스에서 도커를 사용하여 키바나 설정하기 단계별 안내","description":"","date":"2024-06-20 15:20","slug":"2024-06-20-SettingUpKibanaonLinuxUsingDockerAStep-by-StepGuide","content":"\n\n![이미지](/assets/img/2024-06-20-SettingUpKibanaonLinuxUsingDockerAStep-by-StepGuide_0.png)\n\nKibana은 Elasticsearch 데이터를 탐색하고 시각화할 수 있는 강력한 시각화 도구입니다. 이 안내서를 통해 Linux 시스템에서 Docker를 사용하여 Kibana를 설정하고 사용하는 방법을 안내해 드리겠습니다. 튜토리얼을 완료하면 Kibana가 실행되고 Elasticsearch 인스턴스에 연결되어 데이터 탐색 및 시각화에 준비될 것입니다.\n\n## 필수 조건\n\n시작하기 전에 다음 사항을 확인하세요:\n\n<div class=\"content-ad\"></div>\n\n- Linux 시스템 (Ubuntu, CentOS 등)\n- Docker가 설치되어 실행 중인 상태\n- 이미 실행 중인 Elasticsearch 컨테이너\n\n## 단계 1: Elasticsearch 버전 확인\n\n호환성을 보장하기 위해 현재 실행 중인 Elasticsearch 버전을 알아야 합니다. 다음 명령어를 실행하여 확인할 수 있습니다:\n\n```js\ncurl -X GET \"localhost:9200\"\n```\n\n<div class=\"content-ad\"></div>\n\nJSON 응답에서 version 필드를 찾아보세요. 예를 들어:\n\n```js\n{\n  \"name\" : \"elasticsearch-node\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"cluster_uuid\" : \"UUID\",\n  \"version\" : {\n    \"number\" : \"7.10.0\",\n    ...\n  },\n  ...\n}\n```\n\n## 단계 2: Kibana 도커 이미지 다운로드\n\n찾은 Elasticsearch 버전을 기반으로 해당하는 Kibana 도커 이미지를 다운로드하세요. 이 안내서에서는 버전이 7.10.0인 것으로 가정합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nsudo docker pull docker.elastic.co/kibana/kibana:7.10.0\n```\n\n## 단계 3: 키바나 실행하기\n\nElasticsearch가 실행 중이라면 이제 키바나 컨테이너를 실행하고 Elasticsearch 인스턴스에 연결할 수 있습니다. Elasticsearch 컨테이너의 이름이 elasticsearch이고 http://localhost:9200에서 접근 가능한 경우 다음 명령을 실행할 수 있습니다:\n\n```js\nsudo docker run -d --name kibana -p 5601:5601 --link elasticsearch:elasticsearch -e ELASTICSEARCH_HOSTS=http://elasticsearch:9200 docker.elastic.co/kibana/kibana:7.10.0\n```\n\n<div class=\"content-ad\"></div>\n\n## 단계 4: 키바나에 접속하기\n\n지금 키바나가 실행 중이며 웹 브라우저를 통해 접속할 수 있습니다. 브라우저를 열고 다음 주소로 이동하세요:\n\n```js\nhttp://localhost:5601\n```\n\n![이미지](/assets/img/2024-06-20-SettingUpKibanaonLinuxUsingDockerAStep-by-StepGuide_1.png)\n\n<div class=\"content-ad\"></div>\n\nKibana 인터페이스가 준비된 것을 확인할 수 있을 거예요.\n\n이 글을 즐겁게 읽으셨다면, 공유하고 팔로우하며 더 많은 업데이트를 받기 위해 이메일 구독도 잊지 마세요!","ogImage":{"url":"/assets/img/2024-06-20-SettingUpKibanaonLinuxUsingDockerAStep-by-StepGuide_0.png"},"coverImage":"/assets/img/2024-06-20-SettingUpKibanaonLinuxUsingDockerAStep-by-StepGuide_0.png","tag":["Tech"],"readingTime":2},{"title":"데이터 스토리텔링 101 인상적인 스토리텔링을 위해 색상을 전략적으로 활용하기","description":"","date":"2024-06-20 15:19","slug":"2024-06-20-DataStorytelling101UsingColorStrategicallyforImpactfulStorytelling","content":"\n\n## 이 기사는 데이터 시각화의 효과적인 데이터 스토리텔링을 강화하는 데 중요한 역할을 하는 회색 색상에 대해 탐구합니다. 또한 데이터 시각화를 위해 올바른 색상을 선택하는 몇 가지 기본 규칙을 제공합니다. 이 주제에 대한 치트 시트는 기사 끝에 포함될 예정입니다.\n\n![이미지](/assets/img/2024-06-20-DataStorytelling101UsingColorStrategicallyforImpactfulStorytelling_0.png)\n\n## 서문\n\n데이터 시각화와 데이터 스토리텔링은 동전의 양면이다. 효과적인 데이터 스토리텔링은 데이터에서 통찰을 전달하기 위해 시각화를 활용합니다. 반면, 중요한 시각적 효과를 만들려면 전달하고자 하는 이야기를 이해해야 합니다. 두 요소는 이해관계자가 의도한 메시지를 명확하게 이해할 수 있도록 함께 작동합니다.\n\n<div class=\"content-ad\"></div>\n\n차트와 그래프의 기본 기능을 이해하는 것 이상으로, 데이터 시각화의 중요한 측면은 적절한 색상을 선택하는 것입니다. 효과적인 색상을 선택하면 시각화물의 명확성과 효과를 향상시키고, 결과적으로 전체 데이터 이야기를 강화할 수 있습니다.\n\n이 기사는 데이터 시각화에서 회색의 놀라운 힘에 대해 다룹니다. 이는 이 분야에서 가장 중요한 색상으로 자주 간주됩니다. 회색이 시각화물을 어떻게 향상시킬 수 있는지 탐구하고, 시각화물을 더욱 높이기 위한 몇 가지 기본 색상 선택 규칙을 제공할 것입니다.\n\n## 파트 1: 회색이 시각화물을 여러 가지 방식으로 어떻게 향상시키는지\n\n회색은 몇 가지 핵심 이유로 데이터 시각화에서 다재다능하고 중요한 역할을 합니다:\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-20-DataStorytelling101UsingColorStrategicallyforImpactfulStorytelling_1.png)\n\n- 주요 정보 강조하기\n회색을 이용하여 격자선, 배경 또는 중요하지 않은 데이터 지점과 같은 중요하지 않은 요소들에 사용함으로써, 색상화된 데이터가 돋보이고 시각화의 가장 중요한 부분에 뷰어의 시선을 끄는 데 도움이 됩니다.\n\n![이미지](/assets/img/2024-06-20-DataStorytelling101UsingColorStrategicallyforImpactfulStorytelling_2.png)\n\n- 맥락 제공하기\n회색의 연한 색조를 사용하여 데이터 세트의 배경 정보나 일반적인 추세를 표시하여 뷰어가 색상에 압도당하지 않고 상황을 이해할 수 있도록 합니다.\n\n\n<div class=\"content-ad\"></div>\n\n![Image 1](/assets/img/2024-06-20-DataStorytelling101UsingColorStrategicallyforImpactfulStorytelling_3.png)\n\n- 정보 가시성 향상\n많은 데이터나 선명한 색상을 사용한 시각화물에 회색을 결합하면 전반적으로 균형 잡힌 안정적인 느낌을 줄 수 있습니다.\n\n![Image 2](/assets/img/2024-06-20-DataStorytelling101UsingColorStrategicallyforImpactfulStorytelling_4.png)\n\n## 파트 2: 데이터 시각화에서 효과적인 색상 선택을 위한 기본 가이드\n\n<div class=\"content-ad\"></div>\n\na. 직관적인 색상 사용하기\n\n차트의 색상을 선택할 때, 대상 군이 그들의 문화적 배경에 기반하여 어떻게 해석할지를 고려해보세요. 이해를 높이기 위해 확립된 색상 관련성을 활용하세요. 예를 들어, 빨강은 보통 위험이나 일반적으로 정지를 의미하며, 초록은 성장이나 진행을 시사할 수 있습니다.\n\n![이미지](/assets/img/2024-06-20-DataStorytelling101UsingColorStrategicallyforImpactfulStorytelling_5.png)\n\nb. 색상 일관성 유지하기\n\n<div class=\"content-ad\"></div>\n\n동일한 변수에 대해 모든 차트에서 일관된 색상 사용을 유지하는 것이 이상적입니다. 이는 변수가 다른 차트에 나타나더라도 적용됩니다. 예를 들어, 실업률을 나타내는 데 항상 파란색을 사용하십시오. 그러나 첫 번째 차트에서 서로 다른 변수에 대해 여러 색상을 도입하는 경우, 후속 차트에서 관련없는 변수에 해당 색상을 사용하지 말아야 합니다. 이러한 반복은 혼란을 초래하고 데이터 포인트 간 비교를 방해할 수 있습니다.\n\n![다른 색상 사용하기](/assets/img/2024-06-20-DataStorytelling101UsingColorStrategicallyforImpactfulStorytelling_6.png)\n\n### c. 범주에 대한 순차적 색상 피하기\n\n동일한 색상의 음영(예: 파란색 그라데이션)을 사용하는 것이 시각적으로 매력적으로 보일 수 있지만, 이는 잘못된 해석을 초래할 수 있습니다. 독자들은 종종 짙은 색상을 \"더 많음\"이나 \"높은\" 값과 연관시키고, 밝은 색상을 \"덜\"이나 \"낮은\" 값과 연관시킵니다. 이는 범주의 의도하지 않은 순위 매기기를 만들어낼 수 있습니다. 이러한 잘못된 해석을 피하기 위해 범주에 대해 구별되는 색조를 선택하십시오. 이를 통해 각 범주를 명확히 구분할 수 있고, 색상을 통해 텍스트에서 이에 대해 논할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-20-DataStorytelling101UsingColorStrategicallyforImpactfulStorytelling_7.png)\n\nd. 명확함을 위한 색상 제한\n\n차트에서는 최대 일곱 가지의 구분되는 색상을 목표로 설정하세요. 색상은 카테고리를 구분하는 데 도움이 되지만, 과도하면 시청자를 압도할 수 있습니다. 혼잡한 색 팔레트는 데이터를 빠르게 해석하기 어렵게 만들고, 독자가 지속적으로 범례를 참조해야 하도록 합니다.\n\n데이터가 일곱 가지 이상의 카테고리를 필요로 할 때는, 복잡한 정보를 제시하는 데 더 적합한 다른 차트 유형을 사용하는 것을 고려하세요. 관련 있는 카테고리를 더 적은 수의 더 넓은 카테고리로 그룹화하는 것도 옵션으로 고려될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-20-DataStorytelling101UsingColorStrategicallyforImpactfulStorytelling_8.png)\n\ne. 색상 해독: 색상의 의미 해석을 놓치지 마세요\n\n막대의 높이나 지도 상의 원의 크기를 설명하는 것과 마찬가지로, 데이터 시각화에서 색상의 의미를 설명하는 것이 중요합니다. 이를 통해 청중이 메시지를 이해할 수 있습니다. 차트를 위한 명확하고 접근 가능한 색상 범례를 만드는 효과적인 세 가지 방법을 소개합니다.\n\n![이미지](/assets/img/2024-06-20-DataStorytelling101UsingColorStrategicallyforImpactfulStorytelling_9.png)\n\n<div class=\"content-ad\"></div>\n\ne. 그라디언트: 낮은 값에는 밝은 색상을 사용하고 높은 값에는 어두운 색상을 사용하세요.\n\n차트를 이해하기 쉽게 만들기 위해 낮은 값에는 밝은 색상을 사용하고 높은 값에는 어두운 색상을 사용하는 것을 고려해보세요. 이것은 대부분의 시청자들이 기본적으로 기대하는 것과 일치하여, 데이터를 빠르게 파악하고 혼란스럽지 않게 만들어줍니다.\n\n![image](/assets/img/2024-06-20-DataStorytelling101UsingColorStrategicallyforImpactfulStorytelling_10.png)\n\nf. 포괄적인 디자인: 색맹 고려하기\n\n<div class=\"content-ad\"></div>\n\n효과적인 데이터 시각화의 중요한 측면 중 하나는 색 시력 결핍(CVD)을 가진 시청자들에 대한 접근성을 보장하는 것입니다. 좋은 소식은 서로 다른 밝기 수준을 가진 그라데이션과 팔레트를 사용하면 색각 이상이 있는 독자가 데이터 포인트를 구별하는 능력이 크게 향상됩니다. 기억하세요, 다양한 종류의 CVD가 있으므로 온라인 도구를 활용하거나 Datawrapper의 자동 색각 이상 검사를 이용하여 선택한 색상이 보편적으로 구별 가능한지 확인해보세요.\n\n이 참고용으로 치트 시트를 다운로드할 수 있습니다:\n\n감사합니다! 읽어 주셔서 감사합니다! 이 가이드가 유익했기를 바랍니다. 문의 사항이 있거나 문제가 발생하면 언제든지 연락해 주세요.\n\n<div class=\"content-ad\"></div>\n\n참고:","ogImage":{"url":"/assets/img/2024-06-20-DataStorytelling101UsingColorStrategicallyforImpactfulStorytelling_0.png"},"coverImage":"/assets/img/2024-06-20-DataStorytelling101UsingColorStrategicallyforImpactfulStorytelling_0.png","tag":["Tech"],"readingTime":5},{"title":"씨본 객체 소개","description":"","date":"2024-06-20 15:16","slug":"2024-06-20-IntroducingSeabornObjects","content":"\n\n## QUICK SUCCESS DATA SCIENCE\n\n![Seaborn Objects](/assets/img/2024-06-20-IntroducingSeabornObjects_0.png)\n\n2022년 말에 소개된 새로운 Seaborn Objects 시스템을 이미 사용해보셨나요? 꼭 사용해보시길 권합니다; 정말 멋진 것입니다.\n\n이 새로운 시스템은 Tableau와 R의 ggplot2에서 사용되는 그래픽 문법 패러다임을 기반으로 하고 있어 더 유연하고 모듈화되어 있으며 직관적입니다. Python을 사용한 플로팅이 이제 더욱 좋아졌습니다.\n\n<div class=\"content-ad\"></div>\n\n빠른 성공 데이터 과학 프로젝트에서, 새로운 시스템의 기본을 빠르게 익힐 수 있는 튜토리얼을 받을 거에요. Seaborn Objects 공식 문서에서 컴파일된 여러 유용한 치트 시트도 받게 될 거에요.\n\n## 라이브러리 설치\n\n이 프로젝트에서는 다음 오픈 소스 라이브러리를 사용할 거예요: pandas, Matplotlib, seaborn. 각 링크에서 설치 지침을 찾을 수 있어요. 이 프로젝트 전용 가상 환경에 설치하거나, Anaconda 사용자라면 이 프로젝트 전용 conda 환경에 설치하는 것을 추천해요.\n\n## 새로운 Seaborn Objects 인터페이스\n\n<div class=\"content-ad\"></div>\n\nSeaborn의 목표는 항상 파이썬의 주요 플로팅 라이브러리인 Matplotlib을 사용하기 쉽고 보기 좋게 만드는 것이었습니다. 이를 위해 Seaborn은 선언적 플로팅에 의존해 왔습니다. 이 방식은 플로팅 코드의 많은 부분을 추상화하여 감춥니다.\n\n새로운 시스템은 더 직관적이고 어려운 Matplotlib 구문을 덜 의존하도록 설계되었습니다. 플롯은 교환 가능한 마커 유형을 사용하여 점진적으로 작성됩니다. 이는 기억해야 할 항목의 수를 줄이면서 논리적이고 반복 가능한 작업 흐름을 가능하게 합니다.\n\n## 모든 것은 Plot()으로 시작합니다\n\n모듈식 접근 방식을 사용하면 플롯을 작성하기 위해 barplot() 또는 scatterplot()과 같은 열두 가지 이상의 메서드 이름을 기억할 필요가 없습니다. 이제 모든 플롯은 단일 Plot() 클래스로 초기화됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Plot](/assets/img/2024-06-20-IntroducingSeabornObjects_1.png)\n\nPlot() 클래스는 그래픽을 위한 빈 캔버스를 설정합니다. 다음 코드를 입력하여 예제를 확인하세요 (JupyterLab을 사용하여 표시됨):\n\n```js\nimport seaborn.objects as so\nso.Plot()\n```\n\n![Plot](/assets/img/2024-06-20-IntroducingSeabornObjects_2.png)\n\n\n<div class=\"content-ad\"></div>\n\n현재 데이터가 없으므로 Seaborn의 내장 오픈소스 팁 데이터셋을 사용해봅시다. 이 데이터셋은 식당 데이터인 총 청구액, 팁 금액, 요일, 파티 규모 등을 기록합니다. 다음은 이를 판다스 DataFrame으로 로드하는 방법입니다:\n\n```js\nimport pandas as pd\nimport seaborn as sns\n\n# 팁 데이터셋 로드하기:\ntips = sns.load_dataset('tips')\n\ntips.head(3)\n```\n\n<img src=\"/assets/img/2024-06-20-IntroducingSeabornObjects_3.png\" />\n\n이제 Plot()을 데이터에 지정하고 x 및 y 축에 값을 할당할 수 있습니다. Seaborn Objects 인터페이스는 예상대로 판다스 DataFrame과 매우 잘 작동합니다.\n\n<div class=\"content-ad\"></div>\n\n\nso.Plot(data=tips, x=\"total_bill\", y=\"tip\")\n\n\n![Image](/assets/img/2024-06-20-IntroducingSeabornObjects_4.png)\n\n이전 플롯보다 훨씬 나아진 것은 아니지만 x축과 y축을 주목해보세요. 이 플롯은 기본 데이터를 인식하고 있습니다. 이제 우리가 할 일은 Plot()에게 플롯 유형을 명시하는 방법을 알려주기만 하면 됩니다. 이것은 플롯 유형을 위해 전용 메소드를 호출해야 하는 것보다 직관적입니다.\n\n```js\nfig = so.Plot(data=tips, x='total_bill', y='tip').add(so.Dot(), color='sex')\n# fig.show() \n```\n\n<div class=\"content-ad\"></div>\n\n\n![그림](/assets/img/2024-06-20-IntroducingSeabornObjects_5.png)\n\n여기에서는 먼저 Plot()을 호출하여 플롯을 초기화한 다음 Dot()을 호출하여 점 표시기를 추가하여 산점도를 생성했습니다! 직관적으로 산점도에는 점이 사용되기 때문에 이로써 산점도가 생성되었습니다.\n\n이 시점에서 새 시스템으로 플롯을 구축하는 데 필요한 기본 구문을 사용했습니다:\n\n- Plot() 호출,\n- 데이터 인수 할당,\n- 좌표 인수 할당 (예: x 및/또는 y),\n- add() 메서드를 사용하여 플롯에 레이어 추가,\n- add() 내부의 메서드를 호출하여 마커/플롯 유형 지정.\n\n\n<div class=\"content-ad\"></div>\n\n여기 가장 기본적인 형태로 표시된 내용이 있어요:\n\n```js\nso.Plot(data=tips, x='total_bill', y='tip').add(so.Dot())\n```\n\nadd() 메서드에 일부 mark를 전달해야하지만, 반드시 Dot()이어야 하는 것은 아닙니다. 다른 옵션들을 곧 살펴보겠어요.\n\n## Plot() 메서드\n\n<div class=\"content-ad\"></div>\n\nPlot() 클래스에는 표시물을 추가하고 데이터를 조정하며, 서브플롯을 만들고 크기를 조절하며, 서브플롯 간에 라벨을 공유하고 플롯을 저장하는 등을 할 수 있게 해주는 더 더러운 메서드가 12개 이상 포함되어 있습니다. 아래의 표에서 공식 문서에서 요약된 내용을 확인할 수 있어요.\n\n![표](/assets/img/2024-06-20-IntroducingSeabornObjects_6.png) \n\n이러한 메서드는 점 표기법을 사용하여 호출됩니다. 이것이 전체 구문입니다 (가져오기 시 별칭을 사용하지 않았다고 가정):\n\nseaborn.objects.Plot.add()\n\n<div class=\"content-ad\"></div>\n\n## 괄호 사용하여 가독성 향상하기\n\nSeaborn 객체를 사용하여 플롯을 작성할 때는 점 표기법(dot notation)을 사용하여 여러 가지 메소드를 연쇄적으로 호출합니다. 이는 코드를 약간 읽기 어렵게 만들 수 있습니다. 이를 완화하기 위해 플로팅 코드를 괄호로 묶는 것이 좋습니다. 이렇게 하면 새로운 메소드를 각각 다른 줄에 호출할 수 있습니다. 다음은 회귀선과 제목을 추가하는 예시입니다:\n\n```js\nfig = (so.Plot(data=tips, x='total_bill', y='tip')\n       .add(so.Dot(), color='sex')\n       .add(so.Line(color='red'), so.PolyFit())\n       .label(title='팁 vs. 계산서'))\n\nfig.show()    \n```\n\n<img src=\"/assets/img/2024-06-20-IntroducingSeabornObjects_7.png\" />\n\n<div class=\"content-ad\"></div>\n\n회귀 선을 레이어로 추가하려면 Line() 및 PolyFit() 클래스를 다시 add() 메서드로 호출했습니다. 그런 다음 Plot() 클래스의 label() 메서드를 사용하여 제목을 추가했습니다. 전체 플로팅 코드를 괄호로 묶어서 각 클래스 및 메서드 호출을 한 줄에 표시하면 쉽게 찾을 수 있습니다.\n\n## Seaborn 객체 시스템 클래스\n\n새 시스템에는 플롯을 생성하고 마커 유형을 추가하고 오차 막대 및 범위를 그리고 텍스트를 추가하고 값들을 집계하고 포스트된 포인트를 이동시키는 두 다섯 개 이상의 클래스가 포함되어 있습니다. 이 내용은 아래 표에서 공식 문서를 요약한 것입니다.\n\n![IntroducingSeabornObjects_8](/assets/img/2024-06-20-IntroducingSeabornObjects_8.png)\n\n<div class=\"content-ad\"></div>\n\n더 자세한 내용과 예시 플롯을 확인하려면 문서를 참조하세요. 마커 유형 스타일링에 대한 자세한 내용은 \"마크 객체의 속성\"을 참조하세요.\n\n## 패싯 그리드 생성\n\n새 시스템은 패싯 그리드 및 페어 플롯과 같은 다중 패널 플롯을 준비하는 데 탁월합니다. 패싯 그리드는 다중 플롯 그리드입니다. 데이터를 하위 집합으로 만들어 데이터 범위가 비교 가능한 플롯 열을 사용하여 하위 집합을 비교(시각화)할 수 있습니다.\n\nSeaborn 객체를 사용하여 패싯 그리드를 생성하려면 Plot() 클래스의 메서드를 사용하세요.\n\n<div class=\"content-ad\"></div>\n\n```js\nfig = (so.Plot(tips, 'total_bill', 'tip')\n       .add(so.Dot(), color='sex')        \n       .add(so.Line(color='red'), so.PolyFit()))\n\nfig.facet(\"sex\")\n```\n\n<img src=\"/assets/img/2024-06-20-IntroducingSeabornObjects_9.png\" />\n\n우리는 지금 이전 그림을 '남성'과 '여성' 데이터 포인트가 섞이지 않도록 분리했습니다. 각 조건부 데이터 세트에서의 추세와 한계를 더 쉽게 파악할 수 있습니다.\n\nSeaborn Objects는 선언적 그래픽 시스템이며, 일반적인 플롯을 만들기 위해 필요한 많은 오버헤드를 추상화하는 데 설계되었습니다. 그러나 이로 인해 융통성이 약간 부족할 수 있습니다. 또한, 이 시스템은 아직 개발 중이므로 일부 작업은 이전 시스템보다 직관적이지 않을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, facet 그리드 맨 위에 \"super\" 제목을 추가하려면 Matplotlib의 pyplot 모듈을 활용해야 합니다. 먼저 이를 import하고 사용하여 figure (fig)의 인스턴스를 만든 다음, 이전 suptitle() 메서드를 호출하여 제목을 입력합니다.\n\n그 후, 새 Plot() 클래스의 on(fig) 메서드를 사용하여 데이터를 figure에 \"post\"합니다. 아래는 어떻게 보이는지에 대한 예시입니다:\n\n```js\nimport matplotlib.pyplot as plt\n\nfig = plt.Figure(figsize=(5, 6))\nfig.suptitle(\"Tips vs. Total Bill by Gender\")\n\n(\n    so.Plot(tips, 'total_bill', 'tip')\n           .add(so.Dot(), color='sex')  \n           .add(so.Line(color='red'), so.PolyFit())\n           .facet(col='sex')\n           .on(fig)\n)   \n```\n\n<img src=\"/assets/img/2024-06-20-IntroducingSeabornObjects_10.png\" />\n\n<div class=\"content-ad\"></div>\n\n## 쌍 플롯 만들기\n\n쌍 플롯 또는 산점도 매트릭스라고도 불리는 것은 데이터 집합 내에서 여러 변수 간의 쌍별 관계를 비교하는 데이터 시각화 기술입니다.\n\nSeaborn 객체를 사용하여 쌍 플롯을 만들려면 Plot()을 사용하여 데이터 집합과 y축 값을 지정한 다음, pair() 메서드를 사용하여 DataFrame에서 x축 열을 선택한 후, add() 메서드를 사용하여 마크 유형과 마커를 열의 값으로 색칠할지 여부를 지정해야 합니다. 다음은 예시입니다:\n\n```js\n(\n    so.Plot(tips, y='tip')\n    .pair(x=['total_bill', 'size', 'sex'])\n    .add(so.Dots(), color='sex')\n)\n```\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-20-IntroducingSeabornObjects_11.png\" />\n\nDots() 대신 Dot()을 사용하는 것에 유의하십시오. 목표는 마커를 비워 놓은 점으로 자동으로 사용하여 과다 게시된 포인트를 보다 명확히 표시하는 것입니다.\n\n## 막대와 계수\n\ntips 데이터 세트에서 탐색적 분석을 수행 중이라면 남성과 여성 고객의 비율을 알고 싶을 것입니다. 이 작업은 Count() 및 Bar() 클래스를 사용하는 것입니다. 다시 한 번 전체적인 괄호를 사용하면 코드를 최대 가독성을 위해 구조화할 수 있다는 점에 주목하세요:\n\n<div class=\"content-ad\"></div>\n\n```js\n(\nso.Plot(tips, x='sex')\n    .add(so.Bar(), \n         so.Count(), \n         color='sex')\n)\n```\n\n<img src=\"/assets/img/2024-06-20-IntroducingSeabornObjects_12.png\" />\n\n데이터셋에는 여성보다 남성이 거의 2배 많이 있어 보입니다.\n\n# Summary\n\n\n<div class=\"content-ad\"></div>\n\n시본(Seaborn)이 맷플롯립(Matplotlib)을 더 좋게 만드는 것처럼, 시본 객체 시스템은 시본을 향상시킵니다. 가장 큰 변화 중 하나는 이전 플로팅 방법을 Plot() 클래스로 대체하는 것입니다. 이 클래스는 \"모든 일을 다 하는 반지\" 역할을 합니다. 이제 모든 그림은 Plot()을 사용하여 초기화됩니다.\n\n이전에는 각 플롯 유형마다 다른 메서드가 있었으며, 막대 차트의 경우 sns.barplot(), 산점도의 경우 sns.scatterplot()과 같은 메서드가 있었습니다. 더 많은 정보를 더하려면 종종 다른 유사한 메서드가 필요했었는데, 예를 들어 회귀선을 위한 sns.regplot()이 그중 하나입니다. 이러한 방식은 복잡한 플롯을 선언적으로 구축하기 어렵게 만들었습니다.\n\n시본 객체의 모듈식 접근 방식으로, 이제 add()와 같은 직관적인 메서드를 사용하여 점(dot), 선(line) 및 막대(bar)와 같이 직관적으로 명명된 마커를 추가할 수 있습니다. 이 \"그래픽 문법\" 접근 방식을 사용하면 일관적이고 논리적인 방법으로 플롯을 빌드할 수 있습니다.\n\n새 시스템은 선언적 플로팅을 사용하여 빠르고 \"표준화된\" 플롯을 만들도록 설계되었습니다. 더 복잡한 플롯을 위해서는 맷플롯립의 명령형 플로팅에 의존해야 할 것입니다 (다시 한번, 선언적 vs. 명령적 플로팅 참고).\n\n<div class=\"content-ad\"></div>\n\n마지막으로, 새로운 시스템이 완전히 개발되지 않았음을 인지해야 합니다. Seaborn 문서를 인용하면 \"새 인터페이스는 현재 실험적이며 완전하지 않습니다. 심각한 용도로 충분히 안정적이지만, 몇 가지 문제점과 누락된 기능이 있을 수 있습니다.\"\n\n## 추가 자료\n\nSeaborn Objects를 파악하는 데 유용한 몇 가지 추가 참고 자료입니다:\n\nAnaconda: Seaborn Objects 시스템 소개\n\n<div class=\"content-ad\"></div>\n\n시본 버전 0.12.0은 ggplot2와 유사한 인터페이스를 가지고 있어요.\n\n시본 0.12: 객체 인터페이스 및 선언적 그래픽에 대한 통찰력 있는 가이드\n\n시본 객체 시스템에 대한 빠른 소개\n\n시본.objects 인터페이스\n\n<div class=\"content-ad\"></div>\n\n그래픽의 문법\n\n# 감사합니다!\n\n읽어 주셔서 감사합니다. 미래에 더 많은 빠른 성공 데이터 과학 프로젝트를 위해 제 팔로우를 부탁드립니다.","ogImage":{"url":"/assets/img/2024-06-20-IntroducingSeabornObjects_0.png"},"coverImage":"/assets/img/2024-06-20-IntroducingSeabornObjects_0.png","tag":["Tech"],"readingTime":8},{"title":"선형 회귀 이해하기 기초부터 실전 응용까지","description":"","date":"2024-06-20 15:15","slug":"2024-06-20-UnderstandingLinearRegressionFromBasicstoPracticalApplications","content":"\n\n안녕하세요 여러분! 오랜만에 이야기를 나누게 되어 기쁩니다. 머신 러닝의 기초에 대한 여정을 떠나기로 했고, 오늘은 선형 회귀에 대해 자세히 살펴볼 예정입니다. 선형 회귀는 결과를 예측할 뿐만 아니라 변수 간의 관계를 직관적으로 이해하는 강력한 도구입니다.\n\n![이미지](/assets/img/2024-06-20-UnderstandingLinearRegressionFromBasicstoPracticalApplications_0.png)\n\n# 선형 회귀를 배워야 하는 이유\n\n선형 회귀는 예측 모델링의 근간이며, 금융, 마케팅에서부터 의료 및 사회과학까지 여러 분야에서 폭넓게 활용됩니다. 그 이유를 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n## 선형 회귀의 중요성\n\n선형 회귀는 우리에게 다음을 가능하게 합니다:\n- 추세 예측: 데이터 포인트에 직선을 맞춤으로써, 과거 데이터를 기반으로 미래 결과를 예측할 수 있습니다.\n- 관계 이해: 변수 간의 관계를 정량화하는 데 도움이 되며, 한 변수의 변화가 다른 변수에 어떻게 영향을 미치는지 파악할 수 있습니다.\n- 정보에 기반한 결정: 비즈니스는 판매를 예측하기 위해, 경제학자는 추세를 분석하기 위해, 과학자는 실험 데이터를 모델링하기 위해 선형 회귀를 사용합니다.\n\n## 언제 선형 회귀를 사용해야 하는가\n\n선형 회귀를 적용할 수 있는 경우:\n- 입력 (독립) 변수와 출력 (종속) 변수 간에 명확한 관계가 있는 경우.\n- 데이터에 대한 통찰을 제공하는 간단하고 해석 가능한 모델이 필요한 경우.\n\n<div class=\"content-ad\"></div>\n\n## 예시: 주택 가격 예측\n\n집의 평방 피트 및 침실 수와 같은 요인에 기반하여 주택 가격을 예측하고 싶다고 상상해봅시다. 선형 회귀는 이 관계를 다음과 같이 모델링할 수 있습니다:\n\n```js\n\\[ \\text{Price} = b_0 + b_1 \\times \\text{Sqft} + b_2 \\times \\text{Bedrooms} \\]\n```\n\n여기서:\n- \\( b_0 \\)은 절편(기본 가격)입니다.\n- \\( b_1 \\) 및 \\( b_2 \\)는 각 변수가 가격에 어떻게 영향을 미치는지를 정량화하는 계수입니다.\n\n<div class=\"content-ad\"></div>\n\n## 선형 회귀 시각화\n\n산점도와 적합한 선을 함께 사용하여 변수 간의 관계를 시각화하세요. 예시는 다음과 같습니다:\n\n![이미지](/assets/img/2024-06-20-UnderstandingLinearRegressionFromBasicstoPracticalApplications_1.png)\n\n이 예시에서 각 점은 주택의 평방 피트와 침실 수를 나타냅니다. 선은 데이터를 가장 잘 맞추어 주택 가격을 예측하는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n## 선형 회귀의 작동 방식\n\n기본 사항\n\n선형 회귀는 예측된 값과 실제 값 간의 제곱 오차의 합을 최소화하는 선을 맞춥니다. 목표는 데이터에 가장 잘 맞는 계수 \\( b_0, b_1, b_2, \\ldots \\) 를 찾는 것입니다.\n\n## 선형 회귀를 위한 라이브러리 사용\n\n<div class=\"content-ad\"></div>\n\n실제로는 Python의 `scikit-learn`과 같은 라이브러리를 사용하여 선형 회귀를 쉽게 구현할 수 있습니다. 간단한 예제를 살펴보겠습니다:\n\n```js\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n# 데이터 예제\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\nY = np.dot(X, np.array([1, 2])) + 3\n# 선형 회귀 모델 생성\nmodel = LinearRegression()\n# 모델 훈련\nmodel.fit(X, Y)\n# 계수 및 절편 출력\nprint(f\"계수: {model.coef_}\")\nprint(f\"절편: {model.intercept_}\")\n```\n\n## 실용적인 응용\n\n- 비즈니스: 마케팅 비용을 기반으로 한 매출 예측.\n- 의료: 생활 양식이 건강 결과에 미치는 영향 분석.\n- 교육: 학습 시간을 기반으로 한 학생 성적 예측.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n선형 회귀는 데이터 관계를 이해하고 예측을 하는 데 강력하고 다재다능한 도구입니다. 머신 러닝에 입문한 지 얼마 되지 않았다면, 또는 기술을 확장하려는 중이라면, 선형 회귀를 습득하는 것은 더 고급 기술을 위한 튼튼한 기반을 마련하는 데 도움이 될 것입니다.\n\n그럼 이만! 읽어 주셔서 감사합니다. 선형 회귀를 어떻게 구현하는지 궁금하다면, 제가 처음부터 설명하는 Kaggle 노트북을 확인해보세요: 기초부터 이해하는 선형 회귀.\n\n곧 더 많은 기본 머신 러닝 알고리즘에 대해 알아보기로 했으니, 많은 기대 부탁드립니다! 그때까지 수고하세요! 🚀🔍","ogImage":{"url":"/assets/img/2024-06-20-UnderstandingLinearRegressionFromBasicstoPracticalApplications_0.png"},"coverImage":"/assets/img/2024-06-20-UnderstandingLinearRegressionFromBasicstoPracticalApplications_0.png","tag":["Tech"],"readingTime":3},{"title":"파이썬을 사용한 탐색적 데이터 분석EDA 완료하기","description":"","date":"2024-06-20 15:10","slug":"2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython","content":"\n\n## 유방암 생존 예측 데이터 집합의 EDA 단계별 안내서\n\n![이미지](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_0.png)\n\n탐색적 데이터 분석 또는 EDA는 데이터 분석의 가장 중요한 단계 중 하나입니다. 이는 데이터 집합의 체계적 조사로 데이터 내의 패턴을 발견하고 이상값을 식별하며 숨겨진 통찰을 발굴하는 과정입니다.\n\n이 블로그에서는 유방암 생존 예측을 중심으로 한 데이터 집합에 EDA 기술을 적용하는 데 초점을 맞출 것입니다. 유방암 생존 예측 데이터 집합을 탐색함으로써 생존률에 영향을 미치는 요인에 대한 통찰을 얻고자 합니다. 요약 통계 및 시각화와 같은 EDA 기술을 사용하여 유용한 정보를 발굴하여 유방암 진닝 및 치료에 대한 이해에 기여할 것입니다. 데이터 집합에는 수술 시 환자의 나이, 수술 연도, 양성 겨드랑이 노드 수 및 생존 상태와 같은 다양한 정보가 포함되어 있습니다. Python과 pandas 라이브러리를 사용하여 데이터 집합 내의 신비를 해결하고 유방암 진닝 및 치료에 대한 이해에 기여할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 데이터셋에서 얻은 통찰\n\n데이터셋을 가져온 후, 첫 번째 단계는 데이터가 올바르게 가져와졌는지 확인하는 것입니다. 이를 위해 아래 코드를 사용할 수 있습니다.\n\n```python\ndf.shape\n```\n\n<div class=\"content-ad\"></div>\n\n해당하는 텍스트는 다음과 같습니다:\n\n데이터셋에는 2000개의 관측치와 10가지 특징이 있습니다.\n\nhead()를 사용하여 데이터셋의 상위 5개 관측치를 표시하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndf.head()\n```\n\n![Exploratory Data Analysis using Python](https://www.example.com/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_2.png)\n\nhead( )와 비슷하게 tail( )을 사용할 수 있습니다. 이를 통해 마지막 5개의 관측값을 표시합니다. 만약 마지막 10개를 표시하고 싶다면, 괄호 안에 숫자를 지정하면 됩니다.\n\n```js\ndf.tail(10)\n```\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_3.png)\n\n데이터에 대해 더 잘 이해하기 위해 각 열의 비 널 레코드 수, 데이터 유형, 데이터 세트의 메모리 사용량을 확인하기 위해 info()를 사용합니다.\n\n```js\ndf.info()\n```\n\n![image](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_4.png)\n\n\n<div class=\"content-ad\"></div>\n\n데이터셋은 각각 다른 데이터 유형으로 구분되는 다양한 기능을 포함하고 있어요. 'Patient_ID,' 'Marital_Status,' 'Radiation_Therapy,' 'Chemotherapy,' 및 'Hormone_Therapy'와 같은 범주형 기능들은 객체 (문자열)로 표현돼요.\n\n정수 데이터 유형 (int64)은 'Age,' 'Year of Operation,' 'Positive_Axillary_Nodes,' 그리고 'Survival_Status'에 할당되어 있어요. 반면 'Tumor_Size' 기능은 소수점 정밀도 정보를 포함하는 float (float64)로 표현돼요.\n\n누락된 값이 존재하는 경우, 정수로 분류된 'Age' 열은 정보가 불완전한 10개의 인스턴스를 가지고 있어요. 마찬가지로, 객체로 표현된 'Marital_Status' 열에는 6개의 누락된 값이 들어있어요. 누락된 값들을 적절하게 처리하는 것은 후속 데이터 분석이나 모델링 과정에서 데이터셋의 무결성을 유지하는 데 중요해요.\n\nDataFrame에서 각 열에 대응하는 고유 값 수를 찾기 위해 nunique( )를 사용해요:\n\n<div class=\"content-ad\"></div>\n\n```js\nfor column in df.columns:\n    num_unique_values = df[column].nunique()\n    print(f'열 {column}의 고유 값 개수: {num_unique_values}')\n```\n\n![EDA Image](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_5.png)\n\n# 피처 분석\n\n우리 데이터셋에는 언급된대로 10가지의 피처가 포함되어 있습니다. 각각의 피처를 자세히 살펴보고 그들이 얼마나 관련이 있는지 확인해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n# 환자 ID:\n\n- 설명: 각 환자를 식별하는 기밀적이고 고유한 식별자로, 데이터셋 내에서 추적을 용이하게 하면서도 프라이버시를 보장합니다.\n\n# 나이:\n\n- 유형: 숫자 (정수)\n- 범위: 30세에서 84세\n- 설명: 유방암에 영향을 받는 다양한 연령대를 반영합니다. 나이는 예후와 치료 결정에 중요한 요소이며, 생존 패턴을 이해하기 위해 탐색하기에 중요한 요소입니다.\n\n<div class=\"content-ad\"></div>\n\n# 결혼 상태:\n\n- 유형: 범주형 (문자열)\n- 값: 싱글, 기혼\n- 설명: 환자들의 결혼 상태를 나타내며, 생존 결과와의 잠재적 상관 관계를 제공합니다.\n\n# 수술 연도:\n\n- 유형: 수치형 (정수)\n- 범위: 1958년부터 1970년까지\n- 설명: 거의 백 년에 걸쳐 역사적 관점을 캡처합니다. 이 속성을 통해 의학적 실천의 발전이 시간을 통해 생존 결과에 어떤 영향을 미쳤는지 조사할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 양성 겨드랑이 림프절 (림프절):\n\n- 유형: 숫자 (정수)\n- 설명: 양성 겨드랑이 림프절의 수를 나타내며, 암 전이 정도에 대한 통찰을 제공합니다. 이 정보는 분류 및 치료 결정에 영향을 미치며, 예후에 매우 중요합니다.\n\n림프절: 림프절은 림프 유체 채널을 따라 여과기로 작용하는 작고 콩 모양의 장기들입니다. 유방에서 림프 유체가 빠져나가 최종적으로 혈류로 돌아갈 때, 림프절은 다른 신체 부위에 도달하기 전에 암 세포를 잡아 남기려고 노력합니다. 겨드랑이 아래의 림프절에 암 세포가 있다면 암이 퍼질 위험이 높다는 것을 나타냅니다.\n\n# 종양 크기:\n\n<div class=\"content-ad\"></div>\n\n- 유형: 숫자 (부동 소수점)\n- 범위: 0.5에서 5.0\n- 설명: 종양의 신체 치수를 직접 측정합니다. 종양 크기는 암의 침습성 및 치료 옵션과 관련이 있어서, 우리 분석에서 중요한 요소입니다.\n\n## 방사선 요법, 화학 요법, 호르몬 요법:\n\n- 유형: 범주 (문자열)\n- 값: 예, 아니요\n- 설명: 유방암과의 싸움에서 다양한 치료 접근 방식을 나타냅니다. 생존 결과에 미치는 영향을 탐구하여 미래 치료 전략에 도움이 됩니다.\n\n## 생존 상태:\n\n<div class=\"content-ad\"></div>\n\n- 유형: 이진 (정수)\n- 값: 1 (생존), 2 (사망)\n- 설명: 각 환자의 궁극적인 결과를 밝혀내는 분석의 중심, 희망과 상실 사이의 미묘한 균형에 영향을 미치는 요인을 해부하는 것이 우리의 탐험적 여정의 핵심입니다.\n\n# 기술 통계 분석\n\n데이터 세트에는 앞서 설명한 모든 속성의 값들이 포함되어 있습니다. 데이터 세트의 기술적인 통계 분석을 수행하기 위해 describe()를 사용하여 시작해봅시다.\n\n```js\ndf.describe(include=\"all\")\n```\n\n<div class=\"content-ad\"></div>\n\n`include` 속성에 'all' 값을 할당하여 범주형 피처도 결과에 포함되도록합니다.\n\n결과는 다음과 같이 나타납니다:\n\n![Image](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_6.png)\n\n# 데이터 정리\n\n<div class=\"content-ad\"></div>\n\n데이터에는 다양한 품질 문제가 발생할 수 있습니다. 그러므로 데이터를 정리하는 것이 중요합니다. 방이 지저분하면 좋지 않은 것처럼 데이터도 지저분하게 되면 분석 결과에 오류가 생기거나 부적절한 결과가 발생할 수 있습니다. 이러한 문제는 데이터 분석 전에 해결해야 합니다. 다양한 데이터 정리 기술이 있으며, 일부는 아래에서 설명했습니다:\n\n## 1. 누락된 값 처리\n\n다음 단계는 데이터셋에서 누락된 값을 확인하는 것입니다. 데이터셋에 누락된 값이 있는 것은 매우 일반적입니다. 이러한 누락된 값은 None이나 NaN 값으로 표시되며 대부분의 머신 러닝 알고리즘에서 지원하지 않습니다.\n\n누락된 데이터에는 세 가지 주요 유형이 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- Missing completely at random (MCAR)\n- Missing at random (MAR)\n- Not missing at random (NMAR)\n\n데이터셋에서 누락된 값의 양을 파악하기 위해 isnull() 함수를 사용할 것입니다.\n\n```js\ndf.isnull().sum()\n```\n\n<div class=\"content-ad\"></div>\n\n데이터셋에는 ‘Age’에서 10개, ‘Marital_Status’에서 6개의 결측값이 있음을 보여줍니다.\n\n데이터셋의 결측값 백분율을 찾기 위해 다음을 사용합니다:\n\n```js\nmissing_percentage = df.isnull().mean() * 100\nprint(\"각 열의 결측값 백분율:\")\nprint(missing_percentage)\n```\n\n# 결측값 시각화\n\n<div class=\"content-ad\"></div>\n\n의심할 여지 없이 데이터셋에서 NaN 값이 어떻게 분포되어 있는지를 잘 이해해야 합니다. Missingno 라이브러리는 NaN 값의 분포를 시각화하는 데 효율적인 방법을 제공합니다. 이 라이브러리는 파이썬 라이브러리로 Pandas와 호환됩니다.\n\n라이브러리 설치 방법\n\n```js\npip install missingno\n```\n\n데이터셋에서 누락된 값을 시각화하는 프로그램\n\n<div class=\"content-ad\"></div>\n\n```js\nimport pandas as pd\nimport missingno as msno\nmsno.bar(df)\n```\n\n![CompleteExploratoryDataAnalysisEDAusingPython](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_8.png)\n\n이 막대그래프는 데이터셋에서 누락되지 않은 데이터에 비례하는 값을 보여줍니다. 누락된 값의 수도 함께 표시됩니다. 데이터 포인트의 총 수가 2000개이므로, 이보다 적은 수의 데이터를 갖는 열에는 누락된 값이 있습니다. 예를 들어, '나이'에 해당하는 '1990' 값은 '나이' 열에 1990개의 누락되지 않은 값이 있다는 것을 의미합니다.\n\n# 누락된 값 보완하기\n\n\n<div class=\"content-ad\"></div>\n\n한 가지 가능한 해결책은 누락된 데이터가 포함된 관측값을 제거하는 것입니다. 그러나 이렇게 하면 중요한 정보를 잃을 수 있습니다. 더 나은 전략은 누락된 값을 보충하는 것입니다. 다시 말해, 우리는 기존 데이터의 일부에서 누락된 값을 추론해야 합니다.\n\n누락된 값 처리하기\n\n## 누락된 값 보충 방법\n\n암 생존 데이터셋에서 '환자_ID', '결혼 여부', '방사선 치료', '화학 요법' 및 '호르몬 치료' 특성은 객체(문자열)로 표현됩니다. '나이', '수술 연도', '양성 겉 부분 림프절', '생존 상태'에 대해 정수 데이터 유형(int64)이 할당됩니다. '종양 크기'는 부동소수점(float64)으로 표시됩니다.\n\n<div class=\"content-ad\"></div>\n\n이러한 기능 내 누락된 값을 처리하는 여러 가지 방법이 존재합니다.\n\n- 임의 값 대체: 'Patient_ID' 또는 'Marital_Status'에 대한 누락된 값에 대한 특정 가정이 있다면, fillna 메서드를 사용하여 교육된 추측치로 대체할 수 있습니다.\n- 평균/중앙값/최빈값 대체: 'Age' 및 'Year of Operation'과 같은 숫자 기능은 평균, 중앙값 또는 최빈값을 사용하여 fillna 메서드로 보완할 수 있습니다.\n- 가장 빈번한 값 대체: 'Marital_Status', 'Radiation_Therapy', 'Chemotherapy', 'Hormone_Therapy'와 같은 범주형 기능은 가장 빈번한 값으로 fillna 메서드를 사용하여 보완할 수 있습니다.\n- 고급 보충 기술: Forward fill, backward fill, 보간 또는 KNNImputer와 같은 머신러닝 기반 방법과 같이 더 정교한 보충을 위해 사용될 수 있습니다.\n\n그러나 우리의 데이터 세트에서는 'Age' 열과 'Marital_Status' 열만이 누락된 값이 있습니다.\n\n'Age' (숫자형 특성)에 대한 보충:\n\n<div class=\"content-ad\"></div>\n\n```js\n# 평균을 사용하여 'Age' 열의 결측값 보정\ndf['Age'].fillna(df['Age'].mean(), inplace=True)\n```\n\n'Marital_Status'에 대한 대체값 (범주형 특성):\n\n```js\n# 최빈값을 사용하여 'Marital_Status' 열의 결측값 보정\ndf['Marital_Status'].fillna(df['Marital_Status'].mode()[0], inplace=True)\n```\n\n이러한 대체 작업을 완료한 후에는 'Age' 및 'Marital_Status'의 모든 결측값이 처리되었는지 확인하기 위해 데이터셋을 다시 확인할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nmissing_after_imputation = df.isnull().sum()\nprint(\"Imputation 후 누락된 값:\")\nprint(missing_after_imputation)\n```\n\n## 2. 관련 없는 속성 제거\n\n일부 속성은 분석에 어떤 가치도 제공하지 않는 경우 데이터 세트에서 제거할 수 있습니다.\n\n예를 들어, 데이터 세트의 'Patient_ID' 열은 의존 변수를 예측하는 데 예측력이 없다고 가정할 때 제거할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndf = df.drop('Patient_ID', axis=1)\n```\n\n# 3. 중복 행 제거\n\n중복된 데이터를 처리하는 것은 데이터 정리 과정에서 중요한 단계로, 데이터셋에 중복 정보가 없는지 확인하는 것이 중요합니다. 데이터셋에서 중복된 행 또는 관측치를 식별하고 제거하는 방법은 다음과 같습니다:\n\n```js\nduplicates = df[df.duplicated()]\nprint(\"중복된 행:\")\nprint(duplicates)\n```\n\n<div class=\"content-ad\"></div>\n\n아래 사진은:\n\n![image](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_9.png)\n\ndrop_duplicates() 함수는 DataFrame에서 중복된 행을 제거합니다. 중복이 제거된 새로운 DataFrame을 반환하며 기존 DataFrame은 변경되지 않습니다.\n\n해당 행이 중복으로 보입니다. 데이터셋에서 이 행을 제거해 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 중복 행 제거 및 데이터프레임 업데이트\ndf_cleaned = df.drop_duplicates()\n```\n\n# 4. 이상치 탐지\n\n# 이상치란?\n\n이상치는 데이터 포인트 중 전체 데이터셋의 패턴에서 크게 벗어나며 비정상적이거나 드문 경우를 나타낼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n의료 데이터에서 이상값은 일반적인 패턴과 크게 벗어나는 독특한 케이스나 이상 현상을 나타낼 수 있으며, 이들의 존재는 예상할 수 있는 것입니다.\n\n이상값을 식별하고 이해하는 것은 종합적인 분석에 중요합니다. 이들은 통계 측정치를 영향을 미치고 데이터셋 내의 기저 의료 상태의 다양성과 복잡성에 대한 통찰을 제공할 수 있습니다.\n\n데이터셋에서 이상값을 감지하려면 통계적 방법이나 시각화를 사용할 수 있습니다. 다음은 몇 가지 방법입니다:\n\n1. 상자그림: 상자그림을 사용하여 각 수치적 특성의 분포를 시각화합니다. 상자 그림의 \"수염\"을 넘어간 점들은 잠재적인 이상값으로 간주될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6)) \nsns.boxplot(x=df['Positive_Axillary_Nodes'], color='lightgreen')\n```\n\n![Image](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_10.png)\n\n2. Z-Score: Calculate the Z-score for each data point, and points with a Z-score beyond a certain threshold (e.g., 3 or -3) can be considered outliers.\n\n```js\nfrom scipy.stats import zscore\nz_scores = zscore(df)\nabs_z_scores = np.abs(z_scores)\noutliers = (abs_z_scores > 3).all(axis=1)\n```\n\n<div class=\"content-ad\"></div>\n\n\n![Exploratory Data Analysis](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_11.png)\n\n3. IQR (Interquartile Range): 이상치를 IQR을 기반으로 식별합니다. IQR 바깥의 일정 범위를 벗어나는 포인트는 이상치로 간주될 수 있습니다.\n\n```js\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\n\noutliers = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)\n```\n\n![Exploratory Data Analysis](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_12.png)\n\n\n<div class=\"content-ad\"></div>\n\n저희 데이터셋에 이상값이 확인되었습니다. 의료 데이터에서 이상값은 데이터셋 내 전형적인 패턴에서 현저하게 벗어나는 비정상적이거나 극단적인 관측치를 나타낼 수 있습니다. 이러한 관측치는 독특한 사례, 드문 조건 또는 대부분의 데이터와 다른 특성을 나타내는 이상점을 가리킬 수 있습니다. 이상값은 종종 일부 데이터셋에서 노이즈나 오류로 간주되지만, 의료 데이터에서는 중요한 임상적 영향을 미칠 수 있습니다.\n\n의료 데이터에서의 이상값은 예상되는 현상이지만, 이를 처리하는 방법을 결정할 때 신중한 고려가 필요합니다. 데이터의 맥락, 이상값의 성격 및 분석이나 모델링에 미칠 수 있는 잠재적인 영향을 철저히 평가해야 합니다.\n\n# 데이터 시각화의 기술\n\n# 일변량 분석\n\n<div class=\"content-ad\"></div>\n\n암 생존 예측 데이터셋을 탐색하면서 데이터 분석 여정에서 중요한 단계인 일변량 분석으로 시작합니다. 일변량 분석은 단일 변수의 분포와 특성을 이해하는 데 도움이 되며, 패턴 인식, 요약 및 통계적 탐색에 기여합니다. 선택한 시각화 방법은 데이터의 성격에 따라 달라지며, 이산형 데이터에는 막대 차트, 연속형 데이터에는 히스토그램, 범주별 분석에는 파이 차트 등을 사용할 수 있습니다.\n\n데이터 시각화의 힘을 활용하여 데이터셋에 대해 중요한 질문에 답해봅시다:\n\n예를 들어:\n\n- 질문: 환자들의 겸상림부 림프절의 분포는 어떻게 되는가?\n\n<div class=\"content-ad\"></div>\n\n```js\nsns.histplot(df['Positive_Axillary_Nodes'], bins=15, kde=True, color='lightgreen')\n```\n\n![Exploratory Data Analysis in Python](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_13.png)\n\n\"kernel density estimate (kde=True)\"의 존재는 부드러운 확률 밀도 함수를 나타내며, 기저 분포 패턴에 대한 통찰을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n분석: 분포가 왼쪽으로 치우쳐져 있는 것으로 보이며, 양성 겨릭 림프절 수가 약 0에서 가장 빈번하게 나타납니다. 분포의 상단에는 최대 50개의 양성 겨릭 림프절에 대한 데이터 포인트도 있습니다. 분포의 왼쪽 꼬리가 오른쪽 꼬리보다 긴 것으로, 양성 겨릭 림프절 수가 적은 데이터 포인트가 더 많음을 의미합니다.\n\n- 질문: 데이터셋이 다양한 결혼 상태에 분포되어 있고, 각 카테고리에 속한 환자의 백분율은 어떻게 되나요?\n\n\nmarital_counts = df['Marital_Status'].value_counts()\nplt.figure(figsize=(10, 6))\nplt.pie(marital_counts, labels=marital_counts.index, autopct='%1.1f%%', startangle=90, colors=sns.color_palette('Blues'))\n\n\n![Marital Status Distribution](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_14.png)\n\n\n<div class=\"content-ad\"></div>\n\n분석: 데이터셋에서 결혼한 사람은 70.4%이고 싱글인 사람은 29.6%입니다.\n\n- 질문: 암 생존 예측 데이터셋의 연령대 누적 분포는 무엇이며, 이는 다른 연령 그룹 간 환자 전체 분포를 어떻게 보여줄까요?\n\n```js\nplt.figure(figsize=(10, 6))\nsns.ecdfplot(df['Age_Group'], color='purple')\n```\n\n<img src=\"/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_15.png\" />\n\n<div class=\"content-ad\"></div>\n\n# 이변량 및 다변량 분석\n\n이변량 분석은 산점도, 추이를 보기 위한 선 그래프, 분포를 보기 위한 상자그림, 상관 관계를 확인하기 위한 히트맵을 사용하여 변수 쌍 간의 관계를 조사합니다. 이러한 시각화는 데이터 세트에서 연결 및 종속성을 발견하는 데 중요합니다.\n\n다변량 분석은 두 개 이상의 변수 간의 상호 작용을 동시에 탐색함으로써 이를 한 단계 발전시킵니다. 주성분 분석 및 클러스터 분석과 같은 기법은 여러 요소를 동시에 고려함으로써 복잡한 데이터 세트에 대한 깊은 이해를 제공합니다. 요약하면, 이변량 및 다변량 분석은 다양한 데이터 세트에서 통찰을 추출하는 강력한 도구입니다.\n\n# 1. 히트맵\n\n<div class=\"content-ad\"></div>\n\n데이터셋을 더 자세히 살펴보고 데이터셋 내 다른 피처들 간의 관계를 깊게 이해하기 위해 상관 분석을 수행할 수 있습니다. 히트맵은 데이터셋 내 피처 변수들 간의 상관 관계를 시각화하는 강력한 도구입니다. 이들은 회귀 분석이나 기타 통계 모델링 작업의 맥락에서 변수 간의 패턴과 관계를 식별하는 직관적인 방법을 제공합니다.\n\n상관 행렬의 맥락에서 각 셀은 두 변수 간의 상관 계수를 나타냅니다. 값은 -1부터 1까지 범위를 가지며 다음을 의미합니다:\n\n- 1은 완벽한 양의 상관 관계를 나타냅니다(한 변수가 증가하면 다른 변수도 증가합니다),\n- -1은 완벽한 음의 상관 관계를 나타냅니다(한 변수가 증가하면 다른 변수는 감소합니다),\n- 0은 상관 관계가 없음을 나타냅니다.\n\n# 데이터 변환\n\n<div class=\"content-ad\"></div>\n\n문제는 숫자가 아닌(범주형) 값이 포함된 데이터 세트에 대한 히트맵을 작성하려고 할 때 발생합니다. 상관 계수는 숫자 데이터를 기반으로 계산되며, 숫자가 아닌 값이 포함되면 오류가 발생할 수 있습니다.\n\n데이터 변환\n\n이 문제를 해결하기 위해 범주형 변수를 숫자 형식으로 인코딩해야 합니다. 인코딩 과정은 각 카테고리에 고유한 숫자 식별자를 할당하는 것을 포함합니다. 이를 통해 히트맵이 숫자 데이터와 함께 작동하므로 상관 관계를 효과적으로 계산할 수 있습니다.\n\n```js\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n# 객체 유형의 열을 숫자 표현으로 변환합니다\ndf['Radiation_Therapy'] = label_encoder.fit_transform(df['Radiation_Therapy'])\ndf['Chemotherapy'] = label_encoder.fit_transform(df['Chemotherapy'])\ndf['Hormone_Therapy'] = label_encoder.fit_transform(df['Hormone_Therapy'])\ndf['Marital_Status'] = df['Marital_Status'].astype('category').cat.codes\n```\n\n우리의 피처들 사이에 상관관계가 있는지 히트맵으로 확인해보겠습니다.\n\n```js\n# 상관 행렬 계산\ncorr_matrix = df.corr()\n```\n\n```js\n# 히트맵 생성\nplt.figure(figsize=(12, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)\n```\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_16.png\" />\n\n결과적으로 나타나는 상관 행렬 및 히트맵은 데이터셋 내의 수치적 특징 사이의 관계를 시각적으로 제공합니다. 상관 행렬을 분석함으로써 어떤 특징이 유방암 생존 예측에 상당한 영향을 미칠 수 있는지 통찰력을 얻을 수 있습니다. 이 정보는 우리의 추가 분석을 이끄는 데 도움이 되며, 생존률에 영향을 미치는 중요한 요소의 발견으로 이어질 수도 있습니다.\n\n기억하세요, 탐색적 데이터 분석은 반복적인 과정이며, 초기 단계에서 얻은 통찰력을 바탕으로 추가 분석 및 시각화를 수행할 수 있습니다.\n\n다변량 분석\n\n\n<div class=\"content-ad\"></div>\n\n# 열지도를 사용하여 데이터를 분석하는 방법은 무엇인가요?\n\n질문: 각 연도별로 각 연령 그룹에서 몇 명의 환자가 수술을 받았나요?\n\n![이미지](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_17.png)\n\n열지도를 통해 연령 그룹의 동적 및 연도별 분포를 확인할 수 있습니다. 이는 정보를 토대로 한 비즈니스 결정에 중요한 세부사항과 패턴을 알려줍니다.\n\n<div class=\"content-ad\"></div>\n\n특정 연도와 연령 그룹의 조합이 두드러지게 나타납니다. 예를 들어, X-축의 '1964'와 Y-축의 '70-75' 조합이 가장 높은 값을 갖습니다. 이는 1964년에 70-75세 연령 그룹에서 수술을 받은 환자 수가 많았다는 것을 시사합니다. 반면에, '1963'과 '80-85'의 조합은 가장 낮은 값을 갖는데, 해당 연도에 해당 연령 그룹의 환자 수가 적다는 것을 의미합니다.\n\n차트를 더 자세히 분석하려면, 두 축의 주요 카테고리를 살펴볼 수 있습니다. X-축에서는 연도 '1964'가 가장 흔한 수술 연도로 돋보이며, Y-축에서는 연령 그룹 '30-35'가 가장 많습니다. 이 정보는 비즈니스 결정과 전략에 유용할 수 있습니다. 다양한 연도와 연령 그룹에 걸쳐 환자 분포에 대한 통찰을 제공하기 때문입니다.\n\n# 2. 상자 그림\n\n상자 그림은 데이터셋의 분포를 시각적으로 효과적으로 나타내는 간결하고 정보를 제공하는 방법입니다. 최솟값, 최댓값, 중앙값, 그리고 사분위수를 포함한 주요 통계 측정치를 요약합니다.\n\n<div class=\"content-ad\"></div>\n\n일부 상자 플롯을 그려볼까요?\n\n```js\nplt.figure(figsize=(15, 4))\n```\n\n```js\n# 서브플롯 1\nplt.subplot(1, 3, 1)\nsns.boxplot(x='Survival_Status', y='Age', data=df, hue='Survival_Status', palette='Blues', legend=False)\n# 서브플롯 2\nplt.subplot(1, 3, 2)\nsns.boxplot(x='Survival_Status', y='Year of Operation', data=df, hue='Survival_Status', palette='Blues', legend=False)\n# 서브플롯 3\nplt.subplot(1, 3, 3)\nsns.boxplot(x='Survival_Status', y='Positive_Axillary_Nodes', data=df, hue='Survival_Status', palette='Blues', legend=False)\n```\n\n질문:\n\n<div class=\"content-ad\"></div>\n\n- 생존한 환자와 그렇지 않은 환자 사이의 연령 분포가 다른가요? 서로 다른 생존 결과에 대해 중앙 연령이나 연령 분포에 뚜렷한 차이가 있나요?\n- 환자들의 생존 상태에 따라 수술 연도 분포에 어떤 추이나 패턴이 있나요? 상자 그림(boxplot)을 통해 생존한 환자와 그렇지 않은 환자들 간의 수술 연도에 뚜렷한 차이가 있나요?\n- 양성 겨드랑이 림프절의 분포는 생존 결과에 따라 어떻게 다른가요? 생존자와 비 생존자 간의 양성 겨드랑이 림프절 분포에 뚜렷한 차이가 있나요?\n\n![이미지](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_18.png)\n\n인사이트: 환자의 연령 및 수술 연도에 대한 상자그림은 유사한 특성을 공유할 수 있음을 나타내는 비교 가능한 통계를 보여줍니다. 반면, 양성 겨드랑이 림프절에 대한 상자그림은 이상치가 많이 존재함을 보여주는데, 이는 의료 데이터셋에서 흔히 관측되는 현상입니다.\n\n암 생존의 맥락에서 양성 겨드랑이 림프절의 이상치는 환자들이 유난히 높은 수의 양성 겨드랑이 림프절을 가지고 있었던 경우를 시사할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 3. 바이올린 플롯\n\n바이올린 플롯은 상자 그림과 커널 밀도 플롯의 측면을 결합한 인사이트 있는 시각화 기법입니다. 데이터 분포에 대한 더 깊은 이해를 제공하며 전통적인 상자 그림보다 미묘한 표현을 제공합니다. 바이올린 플롯에서 특정 값에서 플롯의 너비는 해당 값의 데이터 포인트 밀도에 해당합니다. 데이터 세트에서 선택한 특징에 대해 바이올린 플롯을 작성하는 방법은 다음과 같습니다:\n\n```js\n# 흥미로운 특징\nselected_features = ['나이', '수술 연도', '양성 겨드랑이 림프 노드']\n```\n\n```js\n# 선택한 특징에 대한 바이올린 플롯 플로팅\nfor feature in selected_features:\n    plt.figure(figsize=(8, 6))\n    sns.violinplot(x='생존 상태', y=feature, data=df, hue='생존 상태', palette='Blues', inner='quartile', legend=False)\n    plt.title(f'{feature}에 대한 바이올린 플롯(생존 상태 별)')\n    plt.xlabel('생존 상태')\n    plt.ylabel(feature)\n    plt.show()\n```\n\n<div class=\"content-ad\"></div>\n\n- 생존한 환자와 그렇지 않은 환자 사이의 연령 분포는 어떻게 다른가요?\n- 서로 다른 생존 결과를 가진 환자들의 수술 연도에 관한 어떤 통찰을 얻을 수 있나요?\n- 서로 다른 생존 결과를 가진 환자들의 양성 겨드랑이 림프 결절 분포가 어떻게 다른가요?\n\n![이미지](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_19.png)\n\n통찰: 바이올린 플롯은 상자 플롯에 비해 정보를 더 많이 제공하기 때문에 데이터의 기본 분포와 통계 요약을 나타내는데 더 유익합니다. 양성 겨드랑이 림프 노드의 바이올린 플롯에서 '예' 및 '아니오' 클래스 레이블에 대해 분포가 매우 치우쳐 있는 것을 확인할 수 있습니다. 이는 -\n\n- 대부분의 환자(두 클래스 모두)의 양성 겨드랑이 림프 노드가 적게 감지된다는 것을 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n이러한 관찰은 이전 섹션에서 우리가 한 관찰과 일관성이 있습니다.\n\n## 4. 쌍 플롯\n\n쌍 플롯은 여러 변수 간의 관계를 동시에 탐색하는 강력한 도구로 작용합니다. 이 시각화 기술은 두 변수 간의 연결을 보여주는 그래프 그리드를 생성합니다. 이러한 쌍별 상호작용을 검토함으로써 우리는 패턴, 상관 관계 및 의존성을 발견할 수 있습니다. 이러한 요인은 변수를 개별적으로 살펴볼 때 뚜렷하지 않을 수 있습니다.\n\n이 분석에서 우리는 변수 간의 관계를 시각화하는 것뿐만 아니라 생존 상태에 따라 패턴을 구별하기 위해 쌍 플롯을 사용했습니다. 환자가 생존했는지 여부에 따라 데이터 포인트를 색상으로 구분함으로써, 특정 변수 조합이 더 나은 결과 또는 더 나쁜 결과와 연관이 있는지 여부를 파악할 수 있습니다. 이 접근 방식을 통해 우리는 유방암 생존에 영향을 미치는 요인들의 복잡한 상호작용을 보다 심층적으로 탐구할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nsns.pairplot(df, hue='Survival_Status')\n```\n\n![Pair Plot](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_20.png)\n\n분석: Pair plot을 통해 대각선의 상단과 하단에 대칭 패턴이 나타납니다. 이는 특징 쌍 간의 관계가 축을 교환해도 일관되게 유지됨을 나타냅니다. 이 대칭성은 상삼각형 또는 하삼각형 중 하나를 분석해도 본질적으로 동일한 정보를 제공한다는 것을 시사합니다.\n\nPair plot의 대각선 플롯은 각 구체적인 특징의 단일 변수 분포를 시각적으로 나타내는 커널 밀도 부드러운 히스토그램을 보여줍니다. 이를 통해 각 변수의 분포를 살펴볼 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n그러나 우리의 쌍 플롯에서 주목할 만한 관측 결과는 두 기능 간의 상당한 중첩이 있다는 것인데, 이는 이러한 기능 쌍을 고려할 때 클래스 레이블 간의 명확한 구분이 부족하다는 것을 나타냅니다.\n\n## 5. 산점도\n\n산점도는 일반적으로 그래프 상에 점으로 나타낸 개별 데이터 포인트들을 포함합니다. 그것은 본래 선이 없지만, 변수 간의 추세나 관계를 시각화하기 위해 최적 적합 선이나 회귀 선을 추가하여 나타낼 수 있습니다.\n\n- 질문: Tumor_Size와 Positive_Axillary_Nodes의 수 사이에 상관 관계가 있습니까?\n\n<div class=\"content-ad\"></div>\n\n```js\nplt.figure(figsize=(7, 6))\nsns.scatterplot(x='종양_크기', y='양성_겨드랑이_림프_절', data=df, hue='생존_상태', palette='pastel')\n```\n\n![이미지](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_21.png)\n\n인사이트: 이 산점도에서 양성 겨드랑이 림프절의 수와 종양 크기 간에 중간 정도의 양의 관계를 관찰합니다. 점의 밀도는 데이터의 분포를 시사하며, 종양 크기가 클수록 양성 겨드랑이 림프절 수가 증가하는 경향을 보입니다. 그러나 많은 이상치 점이 이 경향에서 벗어나며, 잠재적 이상 현상을 조사해야 합니다. 이 그래프는 특정 범위의 양성 겨드랑이 림프절 및 종양 크기를 대표하는 밀도 높은 클러스터를 강조하며, 의료 의사 결정에 유용한 통찰을 제공합니다.\n\n# 6. 조인트 플롯\n\n\n<div class=\"content-ad\"></div>\n\n시본의 Joint Plot은 두 개의 그래프가 한 번에 표시됩니다! 이는 두 개의 숫자 변수에 대한 다각적인 시각을 제공하여 산점도와 유익한 히스토그램이 우아하게 결합됩니다. 산점도는 변수 간의 잠재적 상관 관계와 패턴을 보여주며, 축을 따라 나타나는 히스토그램은 각각의 분포를 보여줌으로써 한 눈에 이해할 수 있는 시각화를 제공합니다. 상관 계수와 함께 제공되는 Joint Plot은 선형 관계의 강도와 방향을 측정하여 초기 데이터 탐색과 가설 생성을 돕습니다.\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n\n```python\nsns.jointplot(x='나이', y='양성 겨드랑이 림프 결절', data=df, color='연한 파랑색')\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_22.png\" />\n\n<div class=\"content-ad\"></div>\n\n통찰력:\n\n- 중앙에 있는 산점도는 환자의 연령과 양성 겨드랑이 림프 노드 수 사이의 관계를 보여줍니다.\n- 축을 따라 있는 히스토그램은 각 변수의 분포를 개별적으로 보여줍니다.\n- 상단 가장자리의 히스토그램은 모든 연령 그룹이 수술을 받을 가능성이 거의 동일하다는 것을 나타냅니다.\n- 오른쪽 가장자리의 히스토그램은 대부분의 환자가 양성 겨드랑이 림프 노드 수가 10개 미만임을 나타냅니다.\n\n# 7. 분포 플롯\n\n분포 플롯은 연속 변수의 분포를 시각적으로 나타내는 확률 밀도 함수(PDF) 플롯으로 종종 언급됩니다. 이는 변수 내에서 다른 값들의 빈도수 또는 확률에 대한 통찰력을 제공합니다. 일반적으로 플롯은 분포의 모양과 특성을 보여줌으로써 분석가가 데이터의 중심 경향, 분산 및 잠재적인 패턴을 이해할 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\n질문: 생존한 환자와 그렇지 않은 환자 간의 연령 그룹 분포가 어떻게 다른가요?\n\n![plot](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_23.png)\n\n이 도표는 흥미로워 보여요!\n\n- 모든 연령 그룹 중 50세 미만의 환자들이 가장 많아요.\n- 클래스 레이블 간에 많은 중첩이 있어요. 이는 수술 후 환자의 생존 상태를 환자의 나이로 확인할 수 없다는 것을 의미해요.\n\n<div class=\"content-ad\"></div>\n\n# 8. Contour plot\n\n등고선 그림은 3차원 표면을 2차원 형식으로 그리는 것으로, 상수 z 단면을 등고선이라는 이름으로 표현하는 그래픽 기술입니다. 등고선 그림을 사용하면 데이터를 2차원 플롯으로 시각화할 수 있습니다. 다음은 3차원에서의 정보가 평평한 2차원 차트로 합쳐지는 방식을 도식적으로 나타낸 것입니다 -\n\n![Diagrammatic Representation](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_24.png)\n\n환자의 나이\n\n<div class=\"content-ad\"></div>\n\n시각화한 결과가 멋지네요! seaborn 라이브러리를 사용하여 환자의 나이를 x축으로, 수술 년도를 y축으로 하는 등고선 그림을 그렸어요 —\n\n```js\nsns.jointplot(x='patient_age', y='operation_year', data=df, kind='kde', fill=True)\nplt.show()\n```\n\n결과:\n![Image](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_25.png)\n\n<div class=\"content-ad\"></div>\n\n인사: 위의 등고선 그래프에서 1961년부터 1963년까지 58세에서 75세 사이의 환자들이 더 많이 관찰되었음을 알 수 있습니다.\n\n# 결론:\n\n![그림](/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_26.png)\n\n유방암 생존 예측 데이터셋의 탐색적 데이터 분석(EDA) 여정을 통해, 우리의 주요 목표는 생존율에 영향을 미치는 요인을 밝혀내고 패턴을 파악하며 의미 있는 통찰을 추출하는 것이었습니다. 다양한 EDA 기술과 Python 및 판다스 라이브러리의 활용은 이러한 목표를 달성하는 데 중요한 역할을 하였습니다. 기술 통계와 시각화는 이상치와 생존 결과에 미치는 잠재적인 영향을 명확히 나타내었습니다. 이 EDA는 데이터셋에 대한 깊은 이해를 제공하며 더 많은 분석과 모델링을 위한 기반 단계로 작용합니다.","ogImage":{"url":"/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_0.png"},"coverImage":"/assets/img/2024-06-20-CompleteExploratoryDataAnalysisEDAusingPython_0.png","tag":["Tech"],"readingTime":21},{"title":"복잡한 아이디어를 전달하는 데 미의 힘","description":"","date":"2024-06-20 15:08","slug":"2024-06-20-Thepowerofbeautyincommunicatingcomplexideas","content":"\n\n## 과학이나 혁신을 전달할 때 미를 추구할 수 있을까요?\n\n복잡한 아이디어, 최신 혁신, 또는 새로운 과학 연구를 설명하기 위해 시각적 자료를 만들 때 우리는 종종 이미지가 정보를 전달하는 능력에 집중합니다. 그러나 데이터 시각화, 인포그래픽 또는 구성도에는 우리가 잘 논의하지 않는 숨겨진 힘이 있습니다: 미.\n\n분명히 모든 이미지가 아름답지는 않습니다. 디자이너 알베르토 카이로는 \"아름다움은 사물의 속성이나 특성이 아니라 그 객체들이 일으킬 수 있는 경이, 경외, 즐거움 또는 단순한 놀람의 감정 경험의 측정입니다.\" 미는 경험이다.\n\n그렇다면 과학이나 혁신을 전달할 때 미를 추구하는 것을 옹호할 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\n# 아름다움은 우리의 주목을 끕니다\n\n호기심은 가장 큰 동기부여의 원천입니다. 일반적으로 정보 부족을 만들거나 관객에게 새로운 것을 가르쳐 주겠다는 약속을 함으로써 일어납니다. 두 경우 모두 관객은 답변을 기대하며, 재미있는 것을 배우기를 원하거나 적어도 호기심을 충족시켜 주기를 기대합니다.\n\n하지만 아름다움은 호기심을 다르게 자극합니다. 지식의 약속이 아닌 시각적 만족의 약속으로 컨텐츠 안으로 우리를 끌어들입니다. 지식에 도달하기 전에 작품에 감동을 받습니다. 아름다움은 우리로 하여금 더 깊이 살펴보게 하며, 콘텐츠 주변을 돌아다니며 이미 만족을 느끼면서 지식을 얻게 합니다. 아름다움은 우리 두뇌에게 학습과정을 즐겁게 만들어 줍니다.\n\n이탈리아 데이터 시각화 전문 디자인 에이전시인 Accurat은 주요 디자인 원칙 중 하나로 아름다움을 추구합니다.\n\n<div class=\"content-ad\"></div>\n\n그들의 시각화 시리즈는 La Lettura 신문을 위해 아주 구체적인 주제에 대해 사람들의 관심을 끌기 위해 아름다움을 활용합니다. 예를 들어 위의 시각화는 역사상 가장 중요한 80가지 수학적 문제를 탐구합니다. 그 시각화를 보면 더 알고 싶어지죠. Accurat의 공동 창업자인 조르지아 루피(Giorgia Lupi)의 말처럼: '사람들이 \"오, 이건 아름답군요! 무엇인지 알고 싶어요!\"라고 말하는 아이디어가 좋아요.'\n\n# 아름다움은 감정을 일으킵니다\n\n아름다움은 근본적으로 감정적인 경험이다. 감정은 청중과 깊은 연결을 만들어내는 힘을 갖고 있습니다. 그것들은 시청자가 주제와 개인적인 관련을 맺도록 도와줍니다. 아이디어의 추상적인 아름다움이 감정적인 연결을 만드는 훌륭한 예시는 수학의 아름다움 비디오에서 찾을 수 있습니다. 이 비디오는 각각 자신만의 시각 언어를 갖춘 3가지 추상화 수준을 보여줍니다. 이 비디오의 아름다움은 3개의 장면 간의 연결에 있습니다. 한 수준에서 다른 수준으로 가는 것은 각각을 이해할 수 있게 해주며, 현실의 우리의 시감을 그 뒤에 숨겨진 수학과 물리학과 연결시킨다. 이것을 관람하면 우리는 우리의 세계에 대해 이해할 수 있는 것에 감동받는다.\n\n감정 경험과 합리적 경험의 융합은 우리 뇌에게 매우 만족스러운 느낌이 있습니다. 감정적 경험과 합리적 경험이 모두 우리의 인식과 의사 결정 과정에서 근본적인 역할을 합니다. 신경과학자 안토니오 다마지오(Antonio Damasio)가 쓴 \"감정은 추론 과정에 불가결하다\"라고 합니다. 다시 말해: 감정을 통해 사실들이 우리 속에서 울립니다.\n\n<div class=\"content-ad\"></div>\n\nFernanda Viégas와 Martin Wattenberg의 Wind Map은 아름다운 데이터 시각화의 훌륭한 예시입니다. 이는 미국 규모의 매우 큰 데이터 세트인 바람 패턴을 직관적으로 나타냅니다. 흑백만 사용하여 시각화는 우리에게 바람을 느끼게 합니다. 감정적으로 우리와 소통하면서 정확하고 복잡한 데이터를 전달합니다. 이 시각화에 관한 Eli Holder의 말이 정말 마음에 듭니다:\n\n\"아름다움은 미세한 흰색 선들의 움직임과 복잡한 시스템을 한눈에 이해하는 느낌에 동시에 만족을 창출합니다.\"\n\n# 아름다움은 문화적인 의미를 창출합니다\n\nNASA의 유명한 지구 떠오르는 사진 뒤에 있는 이야기는 매우 흥미로운 것입니다. 이 사진은 아폴로 8 미션 중에 찍힌 것으로, 이는 달 주변을 최초로 여행한 미션입니다. 선원들은 달 표면을 사진으로 촬영하기 위해 카메라를 갖추고 있었습니다. 약 900장의 사진을 찍었는데, 그 대부분은 달의 회색 분화구를 자세히 보여줍니다. 네 번째 달의 궤도 중에 우주 비행사 빌 안더스는 안개 낀 창문을 통해 달 뒤에 떠오르는 지구를 발견했습니다. 그 풍경의 아름다움에 그는 감탄했습니다. 그는 카메라로 향해가서 컬러 필름을 요청했습니다. 그의 동료인 프랭크 보먼은 \"이거 사진을 찍지 말라고\"라고 대답했습니다. 이 사진은 계획에 없었습니다. 안더스의 직감과 사진술 능력이 없었다면 그 사진은 존재하지 않았을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n지구로 돌아와서, 이 사진은 널리 퍼져 다녔습니다. 달 표면의 900 장의 사진은 과학자들에게 확실히 가치 있었지만, 진정으로 아름다운 한 장의 이미지는 전 세계 대중에게 강한 감동을 주며 환경 보호의 세계적인 움직임을 촉발했습니다. 오늘 이 사진을 보면 우리는 지구, 우리의 고향, 우리 자신에 대해 다시 한 번 생각해보게 됩니다. 과학사 인 로레인 다스턴의 말처럼, \"이는 불편한 감정이죠. 당신이 궁금해지는 것이 아니라, 궁금증이 당신을 잡아먹는 것이죠.\"\n\n이 사진은 우리의 주목을 끌며, 우리에게 우리 행성에 대한 모든 것을 전달해줍니다. 감정을 일으키기도 하지만 아마 더 중요한 것은, 이는 전 세계적 규모의 진지한 반성의 순간을 만들어냅니다. 이것은 복잡한 아이디어를 전달하는 데 아름다움의 세 번째 기능입니다: 아름다움은 문화적으로 중요한 객체를 만들어내는 힘을 갖고 있습니다. 우리를 개인적으로 감동시키고 집단적으로 울려 퍼지는 객체들. 그들의 맥락을 초월하여 깊은 문화적 의미를 창출하는 객체들입니다.\n\n복잡한 아이디어를 전달하는 이미지를 만드는 디자이너로서, 우리는 프로세스를 이성적으로 이해하며, 우리의 기술에 객관성을 가져다 주고, 우리의 의사 결정이 합리적인 근거에 기반하고 있다고 클라이언트에게 생각할 수 있도록 노력합니다. 그러나 우리는 또한 우리의 직관과 주관성을 지키고, 아름다움을 추구하는 것을 방어해야 합니다. 왜냐하면 그것이 우리의 가장 강력한 도구 중 하나이기 때문입니다.\n\n본 텍스트는 첨단 혁신과 과학을 전달하는 연구원들을 돕는 디자이너로서의 나의 실천에 말을 건네는 시도입니다. 더 많은 내 작업을 보실 수 있습니다: [https://www.louischarron.io/](https://www.louischarron.io/)\n\n<div class=\"content-ad\"></div>\n\n## 소스\n\n데이터 시각화와 사회. Martin Engebretsen 및 Helen Kennedy 편, Amsterdam University Press (2020)\nhttps://www.aup.nl/en/book/9789048543137/data-visualization-in-society\n\n데이터 시각화: 데이터 주도 디자인을 위한 핸드북. Kirk, A. (2016)\nhttps://book.visualisingdata.com/\n\n과학 이야기에서 줄거리 찾기: 과학 커뮤니케이션 강화를 희망하며. Susana Martinez-Conde 및 Stephen L. Macknik (2017) https://www.pnas.org/doi/10.1073/pnas.1711790114\n\n<div class=\"content-ad\"></div>\n\n방 분위기를 읽어보세요. Effect & Affect, Eli Holder (2024)  \n[링크](https://www.effaff.com/read-the-room-ensemble-effect/)\n\nSeeing Science, 사진술이 우주를 드러내는 방법. Marvin Heiferman (2019)  \n[링크](https://aperture.org/books/seeing-science-how-photography-reveals-the-universe/)\n\nEarthrise 뒤의 사진술. Phil Edwards (2024)  \n[링크](https://youtu.be/B7KR1nCA4Js?si=0cXA-pMwm5xbBS3J)\n\nMercury, Gemini 및 Apollo 디지털 이미지 아카이브  \n[링크](https://tothemoon.ser.asu.edu/)","ogImage":{"url":"/assets/img/2024-06-20-Thepowerofbeautyincommunicatingcomplexideas_0.png"},"coverImage":"/assets/img/2024-06-20-Thepowerofbeautyincommunicatingcomplexideas_0.png","tag":["Tech"],"readingTime":5}],"page":"54","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true}