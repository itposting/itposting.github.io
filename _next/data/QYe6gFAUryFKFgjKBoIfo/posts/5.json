{"pageProps":{"posts":[{"title":"고전부터 현금까지 Midjourney 또는 Stylar AI로 돈 버는 확실한 방법","description":"","date":"2024-06-23 20:20","slug":"2024-06-23-FromClassicstoCashASureWaytoEarnMoneywithMidjourneyorStylarAI","content":"\n\n사이드 일은 좋은 것이지만 작동하는 것을 찾는 것은 큰 도전입니다.\n\n좋은 제품을 만드는 것은 작업 중 작은 부분에 불과합니다. 마케팅과 판매하는 것은 대부분 실패할 만큼의 도전이며, 사이드 일들은 실패합니다.\n\n그러나 만약 사람들이 항상 찾는 좋은 제품을 만드는 방법이 있다면 어떨까요? 높은 순위를 차지한 인기 있는 제품들. 그리고 당신이 미지의 크리에이터라는 문제로 고민할 필요가 없습니다.\n\n만약 Midjourney, Stylart AI 또는 유사한 고품질 AI 이미지 생성 도구를 숙달했다면, 이 일은 여러분에게 맞을지도 모릅니다.\n\n<div class=\"content-ad\"></div>\n\n조용히 웹 페이지에서 카피하고 연습을 해본다는 게 중요합니다. 인상적인 내용을 찾아내실 때 유용한 웹사이트와 도구를 소개요, 함께 사용해봐요.\n\n<div class=\"content-ad\"></div>\n\n그런 책들을 이제는 거의 찾기 힘듭니다. 확실한 것은 그런 오래된 판본들은 천문학적인 가격에만 구할 수 있고, 일부는 살짝 채색이 바뀐 나중 판본도 반 천문학적인 가격에 구할 수 있습니다.\n\n하지만 이런 책들은 책을 사랑하는 사람들에게는 훌륭한 선물입니다. 아이들에게도 좋은 선물일 거예요. 그리고 팔립니다.\n\n이 책들이 퍼블릭 도메인에 있다는 걸 아셨나요? 즉, 여러분이 한 권 가져다가 마음대로 포맷을 바꾸고 일러스트를 추가해서 자신의 방식대로 꾸며서 아마존에 직접 출판할 수 있는 거죠.\n\n이 손고책들에 가치를 더해 고품질이며 매력적인 일러스트와 표지를 만들어서 멋지게 포맷하여 다시 출판하면, 높은 순위에 올라가 요청이 많은 제품을 만들 수 있을 거예요.\n\n<div class=\"content-ad\"></div>\n\n![Screenshot 1](/assets/img/2024-06-23-FromClassicstoCashASureWaytoEarnMoneywithMidjourneyorStylarAI_1.png)\n\n이미 유명한 이름과 관련이 있습니다. 이미 확립된 검색 용어가 있어서 추측할 필요가 없습니다. 또한, 모든 참된 책장 소유자가 그 중 일부를 원하는 것을 알고 계십니다.\n\nMidjourney 또는 Stylar AI는 훌륭한 일러스트를 만들 수 있습니다. 그들은 원래의 조각 그림 스타일로 그것들을 만들 수 있습니다.\n\n![Screenshot 2](/assets/img/2024-06-23-FromClassicstoCashASureWaytoEarnMoneywithMidjourneyorStylarAI_2.png)\n\n<div class=\"content-ad\"></div>\n\n해당 그림을 함께 수록한 원고를 작성하고, 표지를 디자인하여 KDP에서 하드커버 프린트 온 디맨드로 출판하면 판매할 수 있는 책이 완성됩니다.\n\n혹은 새로운 스타일로 재해석하여 예술적인 감성을 더하여 이 책을 또 다시 빛낼 수도 있습니다. 결국, 그것이 생성적 AI의 힘입니다.\n\n듀마나 쥴 베르넹 각별하기 큰 과업이라면, 다소 작은 규모로 시작하는 것은 어떨까요: 멋진 일러스트를 수록한 개별 그림이나 안데르센 동화책을 출판하는 것? 그것들은 여전히 매우 인기가 있습니다.\n\n<img src=\"/assets/img/2024-06-23-FromClassicstoCashASureWaytoEarnMoneywithMidjourneyorStylarAI_3.png\" />\n\n<div class=\"content-ad\"></div>\n\n고전 작품은 항상 수요가 있답니다. 그리고 공개 도메인에 있는 한, 여러분도 사용할 수 있죠.\n\n그래서 중요한 것은 Midjourney나 Stylar AI를 좋은 목적으로 활용해 부수 수입을 올리며 흥행을 확신할 수 있는 무언가를 만들어보는 것이에요.\n\n만약 이 부업이 여러분에게 맞지 않는다면, 그 이야기들에 대한 일러스트를 만드는 것은 Midjourney나 Stylar AI의 능력을 향상시키는 좋은 방법이에요. 그리고 재미도 있답니다.\n\n![이미지](/assets/img/2024-06-23-FromClassicstoCashASureWaytoEarnMoneywithMidjourneyorStylarAI_4.png)\n\n<div class=\"content-ad\"></div>\n\n이 아이디어 한 번 시도해 보시길 바랍니다. 재미로든 돈을 벌기 위해든 상관없이요. 그리고 시간을 쏟아 부어도 후회하지 않으실 거라고 확신합니다.\n\nAivaras Garauzinis","ogImage":{"url":"/assets/img/2024-06-23-FromClassicstoCashASureWaytoEarnMoneywithMidjourneyorStylarAI_0.png"},"coverImage":"/assets/img/2024-06-23-FromClassicstoCashASureWaytoEarnMoneywithMidjourneyorStylarAI_0.png","tag":["Tech"],"readingTime":3},{"title":"AI 이미지 생성에서 사용하는 스케줄러 5가지","description":"","date":"2024-06-23 20:20","slug":"2024-06-23-SchedulersinAIImageGeneration","content":"\n\n# 스케줄러란 무엇인가요?\n\n스케줄러는 때때로 샘플러로 알려져 있으며, SDXL 및 SD1.5와 같은 Stable Diffusion 모델을 사용하여 이미지를 생성하는 확산 파이프라인의 중요한 부분입니다.\n\n스케줄러는 노이즈를 제거하여 최종 이미지 출력물을 생성하는 과정을 안내합니다. 다음을 결정합니다:\n\n- 노이즈 제거 단계의 수 및 각 단계의 크기(예: 큰 단계로 시작하여 최종 이미지에 가까워질수록 작은 단계를 취함)\n- 단계가 무작위인지(확률적) 또는 예측 가능한지(결정론적) 여부\n- 노이즈 제거에 사용되는 특정 방법(알고리즘)\n\n<div class=\"content-ad\"></div>\n\nAI 이미지 생성에 익숙하지 않다면 HuggingFace의 이 기사를 읽어보거나 Invoke YouTube 채널의 Creating with AI 비디오를 시청해보세요.\n\n![Image](/assets/img/2024-06-23-SchedulersinAIImageGeneration_0.png)\n\n## 스케줄러 유형\n\n다양한 종류의 스케줄러가 있으며, 각각은 사용되는 알고리즘에 따라 특정 범주에 속합니다. 여기에는 가장 흔히 사용되는 몇 가지 유형이 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- Euler 및 Euler 계통: Euler는 Invoke의 기본 스케줄러입니다. 속도가 빠른 것으로 알려져 있으며, 이러한 스케줄러들은 일반적으로 고품질 출력물을 생성하는 데 필요한 단계가 적습니다.\n- Denoising Diffusion Implicit Models (DDIM): Stable Diffusion과 함께 사용되는 최초의 스케줄러 중 하나로, 효율성을 위해 설계된 DDIM은 처리 시간을 상당히 단축시켜줍니다.\n- DPM: 이미지 품질 향상을 위한 미분 방정식의 근사 솔루션을 제공하는 이러한 스케줄러들은 단계 및 다단계 변형을 모두 사용할 수 있습니다.\n- Heun Sampling: 이 스케줄러는 적응형 단계 크기 및 노이즈 종속 업데이트를 사용하여, 계산 효율성과 확산 과정의 정확한 추정에 중점을 둡니다.\n- DPM2 Karras 및 DPM2 계속적인 Karras: DPM2 Karras는 이미지 생성에 대한 세밀한 제어를 허용하며, DPM2 계속적인 Karras는 다양성을 향상시키고 새로운 이미지 공간을 탐색합니다. DPM2 Karras는 잘 제어된 다양한 생성 출력물을 제공하는 데 뛰어납니다.\n- UniPC: 샘플링 품질과 효율성을 향상시키는 통합형 예측-교정기 프레임워크입니다. 이 스케줄러는 속도가 중요한 생성에 적합합니다.\n- Denoising Diffusion Probabilistic Models (DDPM): 이러한 모델은 고품질 샘플을 생성하지만, 일반적으로 최종 샘플을 생성하기 위해 많은 반복이 필요합니다. 고품질 출력물이 생성 속도보다 중요할 때 DDPM을 사용할 수 있습니다.\n- 확산 모델용 의사 수치 방법(PNDM): 생성된 샘플 품질에 영향을 미치지 않고 DDPM을 가속화하기 위해 설계된 기술입니다.\n\n# 멋지군요, 그런데 그게 제게 어떤 의미일까요?\n\n스케줄러의 선택은 생성된 이미지의 결과에 상당한 영향을 미칠 수 있습니다.\n\n이미지 생성을 위한 적절한 스케줄러 선택은 이미지 생성의 목표에 따라 달라집니다. 각 스케줄러는 독특한 강점과 트레이드오프를 가지고 있으므로, 이를 이해하면 여러분의 요구에 가장 적합한 스케줄러를 선택하는 데 도움이 될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n가장 인기있는 스케줄러 중 일부가 이미지 생성에 미치는 영향에 대한 예제를 제공합니다. 프롬프트, 시드 및 기타 설정은 동일합니다:\n\n사용된 프롬프트: 회색 머리를 한 남자의 초상화, 8k, UHD\n\n간략히 말씀드리자면, 이미지에 전반적으로 만족하면 다양한 스케줄러를 실험하여 최종 이미지를 얻어보세요!\n\n# 더 알아보기\n\n<div class=\"content-ad\"></div>\n\n더 깊이 들어가서 이미지 생성 및 Invoke에 대해 알아보고 싶나요? 여기 몇 가지 자료가 있어요:\n\n- 웹 사이트를 살펴보세요: [Invoke 웹사이트](https://www.example.com)\n- GitHub 및 문서를 확인하세요: [Invoke GitHub](https://github.com/invoke) 및 [Invoke 문서](https://docs.invoke.com)\n- 디스코드에 가입하세요: [Invoke 디스코드](https://discord.gg/invoke)","ogImage":{"url":"/assets/img/2024-06-23-SchedulersinAIImageGeneration_0.png"},"coverImage":"/assets/img/2024-06-23-SchedulersinAIImageGeneration_0.png","tag":["Tech"],"readingTime":3},{"title":"디자이너를 위한 Midjourney 쉽고 빠르게 삼면 캐릭터 제작하는 방법","description":"","date":"2024-06-23 20:19","slug":"2024-06-23-Midjourneyfordesignerscraftingthree-viewcharacterswithease","content":"\n\n\"Midjourney에서 캐릭터의 세 개의 뷰를 생성하는 것은 비교적 간단합니다. 먼저, 우리는 프롬프트를 작성하여 기본 캐릭터 이미지를 시작합니다. 프롬프트를 구성하는 방법은 다음과 같습니다:\n\n그 다음으로, 세 개의 뷰에 대한 프롬프트를 추가하세요: 세 개의 뷰, 정면 뷰, 후면 뷰, 측면 뷰를 추가하면 가장 기본적인 세 개의 뷰가 됩니다.\n\n![이미지](/assets/img/2024-06-23-Midjourneyfordesignerscraftingthree-viewcharacterswithease_0.png) \n\n그러나 많은 경우, 기존 캐릭터 이미지를 기반으로 세 개의 뷰 디자인을 생성해야 할 필요가 있습니다.\"\n\n<div class=\"content-ad\"></div>\n\n중간발생은 종종 무작위이면서 통제할 수 없는 결과물을 생성합니다. 이는 창의성과 영감에 큰 도움이 되지만 항상 특정한 요구에는 실용적이지 않을 수도 있죠.\n\n어떤 사람들은 안정적인 확산을 사용하여 회사의 캐릭터를 생성할 수 있지만, 오늘은 기존 캐릭터 이미지로부터 세 개의 뷰 디자인을 생성하는 방법을 알려드리려고 합니다.\n\n아래 예시 두 가지를 살펴봅시다.\n\n# 경우 1\n\n<div class=\"content-ad\"></div>\n\n당신의 회사의 캐릭터가 이 고양이라고 가정했을 때, 지금 이 고양이의 삼면도를 어떻게 만들겠어요? (이 자세와 표정이 정말 좋아요 — 항상 저를 웃게 만들죠! ㅋㅋㅋ!)\n\n첫째, 회사 캐릭터인 이 고양이의 이미지를 업로드하세요. 이미지 링크를 얻은 후, 해당 링크를 복사하여 프롬프트 입력 상자에 붙여넣어주세요:\n\n확대하여, Midjourney가 반환한 회사 캐릭터 이미지를 받아보세요. 키가 크고 통통한 고양이 캐릭터 이미지가 좋으면 그 이미지를 선택해주세요.\n\n<div class=\"content-ad\"></div>\n\n아래의 이미지를 Markdown 형식의 표로 변경해주세요.\n\n| ![image](/assets/img/2024-06-23-Midjourneyfordesignerscraftingthree-viewcharacterswithease_2.png) |\n\n그 다음, 이 이미지를 다시 Midjourney에 드롭하여 이미지 링크를 얻고 링크를 복사합니다. 사용자 정의 확대를 클릭하고 나타나는 입력 상자에 링크를 붙여넣고 세 개의 뷰 프롬프트를 입력한 다음 다음 프롬프트로 이미지의 크기를 변경하세요.\n\n| ![image](/assets/img/2024-06-23-Midjourneyfordesignerscraftingthree-viewcharacterswithease_3.png) |\n\n만약 뚱뚱한 고양이가 꼬리가 없고 후면 뷰가 없으면 생성된 \"전면 뷰\"를 다시 굴릴 수 있어요. 측면 뷰와 후면 뷰를 얻을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n미드저니에서 세부 사항을 미세 조정할 수 없기 때문에 \"다시 굴리기\" 기능을 사용하여 측면 및 후면 보기만 얻을 수 있습니다.\n\n이 과정은 인내심이 요구되며, 더 적합한 효과를 얻을 때까지 여러 결과물을 생성하고 비교하기 위해 계속 시도해야 합니다.\n\n## 케이스 2\n\n혼란스러울 때는 걱정하지 마세요. 이전 단계를 주의 깊게 검토해보세요. 이제 다른 캐릭터를 시도해 봅시다. 인터넷에서 가져온 예시 캐릭터 사진이 여기 있습니다:\n\n<div class=\"content-ad\"></div>\n\nMidjourney에 이 이미지를 업로드하고 새 이미지를 생성하도록 프롬프트를 입력하세요:\n\n![image 4](/assets/img/2024-06-23-Midjourneyfordesignerscraftingthree-viewcharacterswithease_4.png)\n\n다음으로, 이 새 이미지를 다시 Midjourney에 업로드하여 이미지 링크를 받으세요. 링크를 복사한 후, 사용자 지정 줌을 클릭하고 링크를 입력 상자에 붙여넣은 후, three-view 프롬프트를 입력하고 프레임의 스케일을 조정하세요.\n\n<div class=\"content-ad\"></div>\n\n\n![Generated Image 1](/assets/img/2024-06-23-Midjourneyfordesignerscraftingthree-viewcharacterswithease_6.png)\n\nSelect the best-generated image and zoom in to expand the view. Repeat this process two more times to get:\n\n![Generated Image 2](/assets/img/2024-06-23-Midjourneyfordesignerscraftingthree-viewcharacterswithease_7.png)\n\n![Generated Image 3](/assets/img/2024-06-23-Midjourneyfordesignerscraftingthree-viewcharacterswithease_8.png)\n\n\n<div class=\"content-ad\"></div>\n\n캐릭터의 외모가 개선되어 전반적인 효과가 조금 더 좋아질 것입니다. 최상의 결과를 얻기 위해 계속 실험해보세요.\n\n# 결론\n\n요약하자면, Midjourney를 사용하여 삼면 캐릭터 디자인을 만들면 작업 흐름에 혁신을 가져다줍니다. 좀 더 인내심을 가지고 연습하면, 당신의 비전을 현실로 구현하는 자세한 전문 캐릭터를 만들 수 있습니다.\n\n- Two Young Design Notes by 公众号: Two Young디자인노트\n\n<div class=\"content-ad\"></div>\n\n💡 더 깊이 알아보고 싶으신가요? 제 Midjourney 컬렉션을 확인해보세요.\n\n## 기사가 마음에 드셨나요?\n\n그렇다면:\n\n- 댓글 남기기\n- 업데이트 팔로우하기\n- 무료 이메일 알림","ogImage":{"url":"/assets/img/2024-06-23-Midjourneyfordesignerscraftingthree-viewcharacterswithease_0.png"},"coverImage":"/assets/img/2024-06-23-Midjourneyfordesignerscraftingthree-viewcharacterswithease_0.png","tag":["Tech"],"readingTime":3},{"title":"Midjourney 패션 가이드 당신의 의상을 위한 15가지 스타일 아이디어","description":"","date":"2024-06-23 20:17","slug":"2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits","content":"\n\n중간 여정 옷장을 현대적인 패션 스타일로 변신하실 준비 되셨나요?\n\n이 독특한 중간 여정 프롬프트들을 살펴보고 패션 게임을 높은 수준으로 끌어올릴 영감의 세계를 발견해보세요.\n\n록, 레트로, 혹은 오뜨 꾸띄르를 좋아하시든 분, 모두에게 어울리는 스타일이 여기 있습니다!\n\n# 프롬프트 템플릿\n\n<div class=\"content-ad\"></div>\n\n먼저 표를 Markdown 형식으로 변경해보세요:\n\n다음으로, prompt 템플릿을 만들어 봅시다:\n\n그 다음, 특정 [의류 스타일]을 prompt에 추가하여 의류 스타일을 정의하세요. 예시:\n\n## 정의\n\n- 전반적으로: 세부 설정을 제한하여 지정된 의류 스타일에서의 변화의 영향을 관찰합니다.\n- 머리 색깔: 머리 색상을 지정하면, 의류도 일반적으로 맞춰지기 때문에 (전체적인 균일성 유지를 위해) \"화려한 머리 색상\"만을 지정합니다.\n\n<div class=\"content-ad\"></div>\n\n# 1. 록 스타일\n\n![Rock Style](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_0.png)\n\n대체 설명:\n\n- 록 앤 롤 패션: 가장 흔한 용어.\n- 록 패션: 록 앤 롤에 영향을 받은 패션.\n- 펑크 패션: 반항적인 메시지를 담은 패션.\n- 메탈 패션: 무거우면서 강렬한 스타일.\n- 고딕 패션: 어둡고 신비로운 스타일.\n\n<div class=\"content-ad\"></div>\n\n# 2. 레트로 패션\n\n![이미지](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_1.png)\n\n# 3. 비즈니스 캐주얼\n\n![이미지](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_2.png)\n\n<div class=\"content-ad\"></div>\n\n# 4. 파티 의상\n\n![이미지](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_3.png)\n\n# 대체 설명:\n\n- 정장 파티: 저녁 드레스, 가운, 롱 드레스.\n- 준정장 파티: 칵테일 드레스, 준정장 드레스.\n- 캐주얼 파티: 파티 드레스, 리틀 블랙 드레스.\n\n<div class=\"content-ad\"></div>\n\n# 5. 웨딩 드레스\n\n![웨딩 드레스 이미지](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_4.png)\n\n# 대체 설명:\n\n- 정장 웨딩: 웨딩 가운, 신부 가운.\n- 캐주얼 웨딩: 웨딩 드레스, 신부 드레스.\n\n<div class=\"content-ad\"></div>\n\n# 6. 스포츠 스타일\n\n![이미지](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_5.png)\n\n# 대체 설명:\n\n- 스포티: 가장 일반적인 용어입니다.\n- 애슬리저: 최근 인기 있는 운동복과 일상복을 결합한 스타일.\n- 액티브웨어: 운동 중에 입는 의류.\n- 스니커 패션: 스니커를 중심으로 한 스타일.\n- 스트릿웨어: 거리 문화에서 유래된 스타일.\n- 아메리칸 캐주얼 (Ameri-casual): 미국 캐주얼 스타일의 약어.\n- 아웃도어 스타일: 야외 활동용 의류.\n- 요가웨어: 요가 중에 입는 의류.\n\n<div class=\"content-ad\"></div>\n\n# 7. 야외 스타일\n\n![이미지](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_6.png)\n\n# 대체 설명:\n\n- 야외복: 가장 흔한 용어.\n- 야외 의류: 야외복과 동의어.\n- 야외 패션: 세련된 야외 의류.\n- 캠핑 의류: 캠핑용 의류.\n- 하이킹 의류: 하이킹용 의류.\n- 낚시 의류: 낚시용 의류.\n- 등반 의류: 등반용 의류.\n- 스키 의류: 스키용 의류.\n- 스노보드 의류: 스노보드용 의류.\n\n<div class=\"content-ad\"></div>\n\n# 8. 홈웨어\n\n![Homewear](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_7.png)\n\n# 대체 설명:\n\n- 룸웨어: 가장 흔한 용어. 편안한 완주복을 말합니다.\n- 잠옷: 주로 잠을 자는 동안 입는 옷을 가리킵니다.\n- 홈웨어: 집에서 착용하는 여유로운 의상.\n- 수면복: 자면 동안 입는 모든 옷을 가리킵니다.\n- 편안한 의류: 편안한 옷을 모두 가리킵니다.\n- 휴식복: 휴식을 취하기 위해 입는 옷입니다.\n\n<div class=\"content-ad\"></div>\n\n# 9. Street Fashion\n\n![Street Fashion](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_8.png)\n\n# 대체 설명:\n\n- 스트릿 패션: 가장 흔한 용어.\n- 스트릿웨어: 힙합 문화에 영향을 받은 스트릿 패션의 스타일.\n- 하이패션 스트릿웨어: 고급 브랜드와 스트릿 패션 스타일의 융합.\n- 스케이터 패션: 스케이트보드 문화에서 파생된 스타일들.\n\n<div class=\"content-ad\"></div>\n\n# 10. 할로윈 의상\n\n![할로윈 의상](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_9.png)\n\n# 11. 오뜨 꾸뛰르\n\n![오뜨 꾸뛰르](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_10.png)\n\n<div class=\"content-ad\"></div>\n\n# 대체 설명:\n- 하이 패션: 가장 일반적인 용어.\n- 모드: 동의어.\n- 아방가르드 패션: 실험적이고 혁신적인 스타일.\n- 현대적인 패션: 현대적인 스타일.\n- 디자이너 브랜드: 유명 디자이너의 브랜드.\n\n참고: 머리 색상 지정을 제거하면 결과가 기대에 더 가까워집니다.\n\n![이미지](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_11.png)\n\n<div class=\"content-ad\"></div>\n\n# 12. Y2K\n\nY2K는 “Year 2000”의 약어입니다.\n\n![이미지](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_12.png)\n\n# 13. Kawaii\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_13.png)\n\n# 14. 사이버 패션\n\n![Image](/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_14.png)\n\n# 대체 설명:\n\n\n<div class=\"content-ad\"></div>\n\n- 사이버 패션: 가장 흔한 용어입니다.\n- 테크웨어: 기술을 포함한 기능적인 패션.\n- 사이버펑크: 근미래 과학 소설 세계에 영향을 받은 패션.\n- 사이버고스: 고딕과 사이버 요소를 결합한 패션.\n- 네온 패션: 네온 색상을 사용한 패션.\n\n# 결론\n\n여기까지입니다! 끝까지 읽어주셔서 감사합니다! 이것은 기본 모델을 기반으로 한 의상 지정 테스트였습니다. 조명, 헤어 컬러, 표정, 사진 분위기, 렌즈 유형, 광 조절 등이 자세히 명시되면 생성된 이미지는 더욱 바람직할 것입니다. 이 기사가 이미지 생성에 대한 유용한 참고 자료가 되기를 바랍니다.\n\n💡더 깊게 파고들고 싶나요? Midjourney 컬렉션을 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n## 이 기사를 좋아하셨나요?\n\n만약 그렇다면:\n\n- 댓글을 남겨주세요\n- 업데이트를 팔로우해주세요\n- 무료 이메일 알림","ogImage":{"url":"/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_0.png"},"coverImage":"/assets/img/2024-06-23-Midjourneyfashionguide15stylestoinspireyouroutfits_0.png","tag":["Tech"],"readingTime":5},{"title":"스타판 사그마이스터의 대담하고 혁신적인 세계로 Artvyai와 함께 ","description":"","date":"2024-06-23 20:16","slug":"2024-06-23-DiveintotheboldandinnovativeworldofStefanSagmeisterwithArtvyai","content":"\n\n당신의 창의력을 발휘하고 AI 아트 세계를 탐험할 준비가 되셨나요? Artvy.ai가 모든 해결책을 제공합니다! 이 플랫폼을 통해 Stefan Sagmeister의 고민을 불러일으키는 디자인에서 영감받은 자신만의 AI 아트를 만들어보세요. 일반을 도전하고 타이포그래피, 감정, 연결성의 세계로 파고들어보세요.\n\n![Artvy.ai](/assets/img/2024-06-23-DiveintotheboldandinnovativeworldofStefanSagmeisterwithArtvyai_0.png)\n\n#deepart와 함께 상상력을 발휘하고 활기찬 #aiartcommunity에 참여하세요. 무료 AI 아트 생성기를 통해 생성적 예술과 알고리즘 아트의 가능성을 탐험해보세요. AI로 생성된 아트의 마법을 발견하고 독특한 스타일을 반영하는 자신만의 걸작을 만들어보세요.\n\nArtvy.ai에서 시작하는 무료 #aiartwork 여행을 시작하고 Stefan Sagmeister와 같은 실제 아티스트의 작품에서 영감을 얻어보세요. 단순히 복사하지 말고 창조하고 혁신하세요!\n\n<div class=\"content-ad\"></div>\n\n# 아트비(Artvy.ai)와 함께 창의력을 발휘해보세요!\n\nArtvy.ai에서 더 많이 즐겨보세요. Stefan Sagmeister와 같은 실제 예술가로부터 영감을 얻어보세요. 더 많은 AI 아트 마법을 경험하려면 DeepArt를 확인해보세요:\n\n- DeepArt.io\n- Dalle3\n- StableDiffusion\n- MidJourney","ogImage":{"url":"/assets/img/2024-06-23-DiveintotheboldandinnovativeworldofStefanSagmeisterwithArtvyai_0.png"},"coverImage":"/assets/img/2024-06-23-DiveintotheboldandinnovativeworldofStefanSagmeisterwithArtvyai_0.png","tag":["Tech"],"readingTime":1},{"title":"우주 탐험 Midjourney AI로 NASA 급 우주 이미지 생성하는 방법","description":"","date":"2024-06-23 20:15","slug":"2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI","content":"\n\n우주 이미지를 좋아해요. 허블 우주 망원경, 신 개발된 웹 우주 망원경, 그리고 카시니, 준호, 뉴 호라이즌스 임무에서 얻은 사진들이 정말 놀라웠어요.\n\n우주는 정말 멋진 모양과 색으로 가득 차 있어요. 우주는 사진을 찍기에 아주 뛰어난 곳이죠. 이 모든 우주 망원경과 임무들은 우리에게 그 먼 곳들에 대한 지식 뿐만 아니라 새로운 아름다움의 범주도 가져다 주었어요.\n\n여기 저기, 이 거대한 광활함 속에는 매력적인 이름을 가진 물체들이 있어요. 자석별, 중성자 별 같은 멋진 이름들도 있고요. 별의 죽음 이후에 행성 크기의 다이아몬드와 흑홀 같은 것도 있답니다.\n\n![우주 이미지](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_0.png)\n\n<div class=\"content-ad\"></div>\n\n우주 흡수분자운에는 태양계보다 훨씬 큰 크기의 에틸포르메이트로 가득 차 있어요. 이것이 산딸기에 달콤한 향기를 주는 원인입니다. 거기에는 알코올과 설탕으로 이루어진 구름들도 있답니다.\n\n![이미지](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_1.png)\n\n우리가 상상한 것보다 더 이상하고 예쁘답니다. Midjourney로 흥미로운 작품 몇 개 만들어보지 않으면 안 될 것 같아요.\n\nMidjourney는 제 기대를 저버리지 않았어요. 우주와 그 안에 있는 아름다운 현상들의 이미지를 만드는 데 꽤 많은 시간을 보냈죠. 그 이미지들 대부분이 \"와우\" 요소를 담고 있는 거죠.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_2.png)\n\nMidjourney을 사용하여 놀라운 우주 이미지를 만드는 데 사용할 단어를 알면 좋습니다. 만약 천문학 애호가라면 문제없이 진행할 수 있을 것입니다.\n\n천문학에 대해 잘 모르는 경우 다음은 시도해볼만한 멋진 시각 우주 현상 중 일부입니다:\n\n- 나선 은하\n\n\n<div class=\"content-ad\"></div>\n\n수은 궤도\n\n금성 궤도\n\n지구 궤도\n\n화성 궤도\n\n<div class=\"content-ad\"></div>\n\n퀘이사르\n\n별 군집\n\n외계 행성\n\n행성 링\n\n<div class=\"content-ad\"></div>\n\n한편, Midjourney는 매우 복잡한 프롬프트 없이 공간 이미지를 잘 다루고 있다고 생각해요. \"스타일화\" 값이 높은 것도 도움이 된다고 생각해요.\n\n여기 몇 가지 나의 프롬프트 예시가 있어요:\n\n\n![ExploringtheCosmos](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_3.png)\n\n\n나는 우주 이미지를 만들 때 처음으로 새로운 \"개인화된\" 설정을 시도할 기회가 있었어요. Midjourney가 며칠 전에 소개했고, 나의 이미지 순위 설정에 기반을 두었어요.\n\n<div class=\"content-ad\"></div>\n\n이제까지 9000개 이상의 이미지를 평가해 왔기 때문에, 내 개인 설정 효과는 시작해보고 최소 요구되는 200개의 이미지만 평가한 사람들과는 매우 다를 것이라고 생각해요.\n\n이 새로운 설정은 몇몇 이미지를 훨씬 더 나아지게 만든 것 같아요. 하지만 일관된 향상이 아니라, 어떻게 작동하는지에 대한 정보도 매우 적기 때문에 아직은 아무것도 확실하지 않아요.\n\n아래는 Markdown 형식의 이미지 링크입니다:\n\n![Exploring the Cosmos](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_4.png)\n\n좋은 소식은 다른 사람들의 개인화된 코드를 사용할 수 있다는 거에요. 만약 내 설정을 시도하고 싶다면, \"— p 7fp27l4\"를 입력해보세요.\n\n<div class=\"content-ad\"></div>\n\n나는 우주 이미지를 만드는 것을 너무 좋아하는 이유 중 하나는 상상력과 탐험을 위한 많은 공간이 주어진다는 것이에요.\n\n사턴의 달 유럽에 있는 갤저들이 어떻게 보일지, 또는 새로 태어난 별을 보고 싶어요. 분자 구름을 통과하며 새끼 별들을 보고 그 위에 살아가는 생물들이 라즈베리 알코올 구름을 먹고 있는지 확인해보고 싶어요.\n\n![space image](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_5.png)\n\n그리고 Midjourney와 함께 만들어진 멋진 이미지들은 스타일 참조로도 사용될 수 있다는 걸 알았어요.\n\n<div class=\"content-ad\"></div>\n\n일련의 프롬프트 기술을 배우는 것도 좋은 방법이에요. 누구나 호스헤드 성운에 대해 알고 있어요. 우리만의 것을 만들어볼까요? 예를 들어 댄서 성운이나 강아지 머리 성운 같은 것 말이에요?\n\n![](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_6.png)\n\n저는 이런 것들에 빠져들게 되면, 곧 나만의 우주 생명체와 함께 전체 세계를 만들기 시작해요. 이것이 전적으로 좋은 일은 아니지만, 그래도 이 글을 쓸 시간이 더 적어지는 거 같아요.\n\n![](/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_7.png)\n\n<div class=\"content-ad\"></div>\n\n그렇게 짧지만, 흥미로운 기사였길 바랍니다. 여러분의 우주 탐험 여행을 시작하는 데 도움이 되었으면 좋겠습니다.\n\n그리고 여러분이 놀라운 성운과 은하, 달과 행성, 그리고 가장 놀라운 생물들로 가득한 자신만의 우주를 만들기를 바랍니다.\n\nAivaras Grauzinis","ogImage":{"url":"/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_0.png"},"coverImage":"/assets/img/2024-06-23-ExploringtheCosmosCreatingNASA-WorthySpaceImageswithMidjourneyAI_0.png","tag":["Tech"],"readingTime":4},{"title":"구도 규칙을 사용하여 Midjourney 이미지를 개선하는 방법","description":"","date":"2024-06-23 20:13","slug":"2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules","content":"\n\n모든 사진 작가는 가장 아름다운 주제, 완벽한 조명, 그리고 멋진 배경을 가졌다 하더라도, 좋은 구도가 없다면 보기 좋은 이미지를 얻을 수 없을 수도 있다는 것을 알고 있어요.\n\n사진과 예술에서의 이미지 구도 규칙들은 인간 심리와 생리학에 깊게 뿌리를 두고 있습니다. 우리 뇌와 몸이 시각 자극을 해석하는 방식을 반영하고 있죠. 이 중 하나인 삼분의 규칙은 이미지를 두 개의 동일한 간격을 두고 수평 및 수직으로 균일하게 나눠 아홉 부분으로 나누는데, 이 구도 지침은 인간 눈이 이미지 내 특정 지점으로 유인되는 자연적 경향과 일치합니다. 시선 추적 연구에 따르면 시청자들의 눈은 이 교차점으로 자연스럽게 이끌리며, 주요 요소를 이 지점에 배치하는 것이 더 균형을 이루고 매력적인 구도를 만들 수 있다는 것을 시사합니다. 이 현상은 복잡한 정보를 효율적으로 처리하는 우리 뇌의 선호성과 관련이 있다고 생각되어, 전체 장면에 압도당하지지 않고 중요한 요소에 집중할 수 있게 합니다.\n\n생리학적으로, 눈과 뇌가 이미지 처리를 함께 하는 방식은 구도 규칙에 영향을 미칩니다. 예를 들어, 선도라는 개념은 이미지 내 자연스러운 선을 활용해 시청자의 눈길을 특정 초점지점으로 이끕니다. 이 기술은 선과 가장자리가 처리되는 시각 체계가 어떻게 작용하는지를 활용하는데, 이것은 인간 눈의 구조와 시각 정보를 해석하는 신경 경로에 뿌리를 두고 있는 시각 인식의 근본적인 측면입니다. 또한 대칭과 패턴의 사용은 조화와 질서감을 조성해 우리의 균형을 선호함에 부합합니다. 이 선호성은 생존에 중요한 패턴을 찾고 예측하고자 하는 우리 뇌의 욕구와 관련이 있다고 생각되어집니다. 이 심리학적이고 생리학적인 기초를 이해하면 사진 작가나 예술가들은 시청자의 깊게 와닿는 구도를 만들어내며, 감정적 반응을 불러일으키고 시각적 흥미를 유지하게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n하지만 Midjourney의 이미지에 관해서는 AI 자체나 사용자들도 구성 규칙에 대해 크게 신경 쓰지 않는 것 같아요. 그것은 나쁜 일이에요; 그래서 Midjourney로 만들어진 몇몇 그 외 완벽한 이미지들이 심심하고 흥미로워 보이지 않는 이유가 되죠.\n\n![이미지](/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_1.png)\n\nMidjourney는 이미지의 주제를 중앙에 배치하려고 할 거예요. 그리고 그것은 우리 눈이 일반적으로 끌리는 이미지 영역을 벗어나게 돼요. 가끔 운 좋게, Midjourney가 선험적인 라인을 만들어내거나 썩이 구성을 만들어내기도 하는데, 그런 경우들은 즉시 더 나아 보일 거에요.\n\n사진과 예술 구성에는 많은 규칙이 있어요. 유감스럽게도 Midjourney는 그 중 많은 규칙을 따르지 않겠지만, 몇 가지 기본 규칙을 추가하면 이미지가 훨씬 더 나아 보일 거에요.\n\n<div class=\"content-ad\"></div>\n\n여기 구성 규칙의 기본 키워드 목록이 있어요:\n\n- 세분의 법칙: 이미지를 가로 세로로 세분하여 주요 대상을 교차점 중 하나에 배치하는 지침.\n- 리딩 라인: 이미지 내부에 있는 라인으로 시선을 주요 대상으로 이끄는 역할.\n- 대칭: 양쪽이 서로 대칭을 이루는 균형 잡힌 이미지 비율.\n- 비대칭: 이미지 내에서 균형을 깨고 흥미와 긴장을 조성할 수 있는 불균형.\n- 리딩 스페이스: 대상 앞에 있는 공간으로 주로 초상화나 행동 사진에서 사용됨.\n- 대각선: 대각선 라인을 사용하여 이미지에 다이나믹한 움직임을 만드는 방법.\n- 골든 삼각형: 이미지를 대각선으로 나누어 동적인 조합을 만드는 방법.\n- 대조: 두 가지 대조적인 요소를 나란히 배치하여 차이점을 부각시키는 방법.\n- 삼각형 구성: 삼각형 모양으로 요소를 배열하여 안정감을 조성하는 방법.\n- 프레임 내 프레임: 장면 요소를 사용하여 대상 주변에 보조 프레임을 만드는 방법.\n\nMidjourney의 구성 가이드와 함께 결과를 살펴보겠습니다. 이렇게 간단한 프롬프트를 사용해볼게요:\n\n결과를 살펴보세요. 이미지에는 여성, 등지고 있는 거리, 오래된 도시가 모두 포함되어 있어요. 그런데 지루하고 나쁜 이미지에요.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_2.png)\n\n이미지 중앙에 우리 여성분이 있지만, 시선을 그녀 쪽으로 이끄는 것은 없습니다. 그녀는 중앙에 있는 어두운 물체일 뿐입니다.\n하지만, 화면 구성 지침으로 프롬프트를 수정하면 상황이 달라집니다:\n\n![이미지](/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_3.png)\n\n\n<div class=\"content-ad\"></div>\n\n이 이미지는 훨씬 더 나아졌어요. 여자가 이미지 가로의 세분의 하나 지점에 위치하고 있어서 눈이 자연스럽게 그녀를 향해 갑니다. 또한 중앙 공간을 비워두어 거리의 깊이로 이어지는 모습을 만들어 더 흥미로운 이미지로 만들었어요.\n\n우리의 프롬프트에 더 많은 구성 규칙을 추가해 봅시다, 이번엔 선도:\n\n\"세분의 일 구도와 선으로 이뤄진 이미지, 오래된 마을에서 습기 낀 랜턴으로 비춰지는 길을 걷는 외로운 여자.\"\n\n![이미지](/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_4.png)\n\n<div class=\"content-ad\"></div>\n\n어떤 사진이 좋네요. 등잔과 포장된 도로가 시선을 우리 레이디 쪽으로 이끌어줍니다. 이미지 너비의 1/3 지점에 여전히 그녀를 배치하고, 밝은 물체와 넓은 공간이 있더라도 그녀는 분명한 주목의 중심입니다.\n\n동일한 안내문의 변형입니다: 다시 한 번 포장된 도로 라인들이 우리의 대상을 향해 눈길을 이끄는 좋은 조각이며 이미지는 흥미롭고 신비로우며 매력적입니다.\n\n![image](/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_5.png)\n\n가끔은 이미지 중앙에 대상을 두고 싶을 수도 있습니다. 특정 상황에서는 합리적이며 흥미로운 이미지를 만들기도 합니다. 하지만 규칙은 그대로입니다: 눈길을 대상물 쪽으로 이끌 것이 있어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n이 경우에는 대칭 구성과 선도 사용하여 작업을 완료할 수 있습니다.\n\n![image](/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_6.png)\n\n이미지가 수평으로 대칭되어 있음을 주목해 보세요. 포장 위의 반사가 수직 대칭 요소를 더해주며, 라인과 등불이 여성의 주목을 이끕니다.\n\n작업에 대칭 구성 규칙을 의존하는 대신, 이미지에 효과나 요소를 추가하여 시선을 주제로 이끄는 것도 가능합니다. 여기에 빛을 이용한 나선 모양 페인팅을 이미지에 추가한 예시가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n마크다운 포맷으로 테이블 태그를 변경하십시오.\n\n<div class=\"content-ad\"></div>\n\n간단한 구성 규칙과 지침을 활용하여 훨씬 강력하고 흥미로운 Midjourney 이미지를 만들 수 있어요. 그리고 그 규칙을 활용하고 Midjourney를 더 높은 수준으로 이끌어내는 것을 즐기시길 바래요. \n\nAivaras Grauzinis","ogImage":{"url":"/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_0.png"},"coverImage":"/assets/img/2024-06-23-CreatingbetterMidjourneyimageswithcompositionrules_0.png","tag":["Tech"],"readingTime":4},{"title":"웹 스크래핑으로 데이터 파이프라인 구축하기 단계별 가이드","description":"","date":"2024-06-23 20:10","slug":"2024-06-23-BuildingDataPipelineswithWebScrapingAStep-by-StepGuide","content":"\n\n웹 스크레이핑은 웹을 거대한 데이터 광산으로 변신시키는 기술이에요. 여기에는 모든 정보가 발굴되기를 기다리는 잠재적 보석이 숨어 있답니다.\n\n웹 스크레이핑 또는 데이터 스크래핑은 인터넷에서 콘텐츠와 정보를 수집하는 편리한 방법이에요. 기본적으로 소프트웨어나 봇을 사용해 웹사이트를 방문하고 페이지를 가져와 필요한 데이터를 추출하는 과정을 말해요. 이 프로세스를 자동화함으로써 이러한 봇은 아주 빠르게 다량의 데이터를 수집할 수 있어요.\n\n어떤 도구를 사용하든, 웹 스크레이핑 도구의 작동 방식은 아래와 같아요:\n\n단계 1: 서버로 HTTP 요청을 보내는 것부터 시작돼요.\n\n<div class=\"content-ad\"></div>\n\n단계 2: 그런 다음, 웹 사이트의 코드를 추출하고 분해합니다.\n\n단계 3: 마지막으로, 중요한 데이터를 로컬로 저장합니다.\n\n웹 스크레이퍼는 사이트에 액세스할 수 있는 권한이 필요합니다. 따라서 먼저 하는 일은 대상 사이트로 HTTP 요청을 보내는 것입니다. 사이트가 승인하면 스크레이퍼가 해당 사이트의 HTML 또는 XML 코드를 읽고 추출할 수 있습니다. 이 코드는 사이트의 콘텐츠와 구조에 대한 청사진과 같습니다.\n\n스크레이퍼는 그런 다음 코드를 구문 분석합니다. 이는 기본적으로 코드를 부분으로 나누어 특정 요소나 객체를 식별하고 원하는 특정 요소나 정보를 가져올 수 있도록 하는 것을 의미합니다. 이것은 특정 텍스트, 평점, 클래스, 태그, ID 또는 필요한 다른 정보와 같은 것들일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n웹 크롤러가 HTML 또는 XML에 접근하여 데이터를 가져오고 구문 분석한 후, 해당 데이터를 로컬로 저장합니다. 이미 봇에게 수집하길 원하는 내용을 알려줬으니, 무엇을 찾아야 하는지 알고 있어요. 이 데이터는 주로 구조화된 형식으로 저장되며, .csv 또는 .xls과 같은 Excel 파일 형식으로 저장합니다.\n\n## 이것이 웹 스크래핑의 기본 아이디어입니다!\n\n먼저, 어떤 웹사이트(들)를 스크래핑하고 어떤 특정 데이터를 추출하려는지 결정해야 합니다. 그런 다음, 이 데이터가 웹사이트의 백엔드 코드에서 어디에 위치하는지 찾아야 합니다. 여러분의 목표는 관련 콘텐츠를 감싸고 있는 고유한 태그를 찾는 것입니다. 예를 들면 `div` 태그 같은 거죠.\n\n이러한 태그들을 식별했다면, 이를 선호하는 스크래핑 소프트웨어에 입력해야 합니다. 이렇게 하면 봇이 정확히 어디를 봐야 하는지와 무엇을 추출해야 하는지 알게 됩니다. 다음 단계는 스크래핑 프로세스를 실행하는 것이죠. 여기서 스크래퍼는 사이트에 액세스 권한을 요청하고, 데이터를 추출하며, 구문 분석합니다.\n\n<div class=\"content-ad\"></div>\n\n데이터를 추출하고 구문 분석하고 수집한 후에는 저장해야합니다. 이제 필요한 데이터를 가지고 있으므로 자유롭게 다루고 원하는 대로 사용할 수 있습니다.\n\n웹 스크래핑에 많이 사용되는 여러 도구와 라이브러리가 있으며, 각각의 특징과 장점이 있습니다. 인기있는 몇 가지 도구는 다음과 같습니다:\n\n- Beautiful Soup:\n\nHTML 및 XML 문서를 구문 분석하는 파이썬 라이브러리입니다. HTML 페이지를 구문 분석하고 데이터를 추출하기 위한 파싱 트리를 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n사용 사례: 간단한 스크래핑 작업 및 데이터 추출.\n\n- Scrapy:\n\n웹 스크레이퍼를 구축하기 위한 오픈 소스 Python 프레임워크입니다. Beautiful Soup보다 더 강력하고 견고하며 더 복잡한 스크래핑 작업에 좋습니다.\n\n사용 사례: 대규모 스크래핑 프로젝트 및 데이터 추출 워크플로우.\n\n<div class=\"content-ad\"></div>\n\n- Selenium:\n\n웹 브라우저를 자동화하는 도구입니다. 주로 웹 애플리케이션을 테스트하는 데 사용되지만 JavaScript와 상호 작용이 필요한 동적 콘텐츠를 스크래핑하는 데도 사용할 수 있습니다.\n\n활용 사례: 양식 작성 및 버튼 클릭과 같이 상호 작용이 필요한 웹 사이트에서 동적 콘텐츠를 스크래핑하는 것.\n\n- Puppeteer:\n\n<div class=\"content-ad\"></div>\n\nNode.js 라이브러리인 Puppeteer는 Chrome 또는 Chromium을 DevTools 프로토콜을 통해 제어하기 위한 고수준 API를 제공합니다. Selenium과 유사하지만 특히 Chrome을 위한 것입니다.\n\n사용 사례: 동적 웹사이트 스크래핑 및 자동 브라우저 작업 수행.\n\nBeautiful Soup와 Scrapy는 Python 사용자에게 훌륭한 도구입니다.\n\n아래는 웹 스크래핑에 관한 프로젝트이며, 아래 웹사이트는 저에게 기초 학습을 도와주었습니다: Web Scraping & NLP in Python | DataCamp\n\n<div class=\"content-ad\"></div>\n\n여기서 Project Gutenberg 웹사이트에서 책 정보를 가져올 것입니다.\n\nProject Gutenberg는 60,000권 이상의 무료 eBook을 제공하는 자원 봉사자 주도의 디지털 도서관입니다. 1971년 Michael S. Hart에 의해 설립되어 세계에서 가장 오래된 디지털 도서관입니다. 이 컬렉션은 무료로 공개적으로 이용 가능합니다. 여기에는 고전 문학, 참고 자료 및 기타 문화적으로 중요한 텍스트들이 포함되어 있습니다. 책은 일반 텍스트, HTML 및 ePub 형식으로 제공되며, 다양한 기기에서 접근할 수 있습니다. Project Gutenberg의 목표는 eBook의 창작과 배포를 촉진하고 이러한 작품들을 미래 세대를 위해 보존하는 것입니다.\n\n스크레이핑을 위해 Python 패키지 requests를 사용하여 이 웹 데이터에서 소설을 추출할 것입니다. 그런 다음 Natural Language ToolKit (nltk)을 사용하여 소설을 분석해볼 것입니다.\n\n아래는 'Dead Men Tell No Tales'라는 epub을 스크랩하는 예시입니다.\n\n<div class=\"content-ad\"></div>\n\n## 1. 라이브러리 설치\n\n```js\n# !pip install html5lib\n# !pip install contractions\n# !pip install spacy\n# python -m spacy download en_core_web_sm (명령 프롬프트에서 실행)\n```\n\nhtml5lib은 HTML 및 XHTML 문서를 구문 분석하기 위한 순수한 Python 라이브러리입니다.\n\ncontractions은 텍스트에서 축약어를 확장하는 데 사용됩니다 (예: \"don`t\"를 \"do not\"로 바꿉니다).\n\n<div class=\"content-ad\"></div>\n\n스파시는 파이썬에서 고급 자연어 처리(NLP)를 위한 오픈 소스 라이브러리입니다.\n\n## 2. 필요한 모든 파이썬 라이브러리와 모듈을 임포트하세요\n\n```js\nimport re  #정규 표현식을 위해\nimport bs4  #HTML 및 XML 문서를 구문 분석하기 위해\nimport nltk  #자연어 처리 (NLP) 작업을 위해\nimport spacy  #고급 NLP 작업을 위해\nimport string  #일반적인 문자열 작업을 위해\nimport requests  #HTTP 요청을 만들기 위해\nimport contractions  #텍스트 내 축약어 확장을 위해\nimport seaborn as sns  #통계적 데이터 시각화를 위해\nfrom nltk.util import ngrams  #n-그램 생성을 위해\nfrom bs4 import BeautifulSoup  #HTML 및 XML을 구문 분석하기 위해\nfrom collections import Counter  #해시 가능한 객체를 계산하기 위해\nimport matplotlib.pyplot as plt  #데이터 플로팅을 위해\nfrom nltk.corpus import stopwords  #일반적인 불용어에 액세스하기 위해\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize #텍스트 토큰화를 위해\n\nnltk.download('punkt')\nnltk.download('words')\nnltk.download('stopwords')\nnltk.download('maxent_ne_chunker')\nnltk.download('averaged_perceptron_tagger')\n```\n\nNLTK 자원 다운로드하기:\n\n<div class=\"content-ad\"></div>\n\n- 그런 다음 코드는 여러 필수 NLTK 자원을 다운로드합니다:\n    - punkt: 텍스트를 문장이나 단어 목록으로 분할하는 토크나이저입니다.\n    - words: 영어 단어 목록입니다.\n    - stopwords: 영어에서 흔히 사용되는 불용어 목록입니다.\n    - maxent_ne_chunker: 미리 훈련된 개체명 청커입니다.\n    - averaged_perceptron_tagger: 품사 태거입니다.\n\n이 설정을 통해 다양한 NLP 및 텍스트 처리 작업을 수행하기 위해 필요한 모든 라이브러리와 자원이 제공됩니다.\n\n## 3. URL에서 HTML 콘텐츠를 가져와 표시하기\n\n```js\n# URL 저장\nurl = 'https://www.gutenberg.org/cache/epub/1703/pg1703-images.html'\n# 요청 보내고 객체 유형 확인\nr = requests.get(url, verify=False)\n# 응답 객체에서 HTML 추출하고 출력\nhtml = r.text\nprint(html)\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n# 출력 예시:\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\"><style>\n#pg-header div, #pg-footer div {\n    all: initial;\n    display: block;\n    margin-top: 1em;\n    margin-bottom: 1em;\n    margin-left: 2em;\n}\n#pg-footer div.agate {\n    font-size: 90%;\n    margin-top: 0;\n    margin-bottom: 0;\n    text-align: center;\n}\n#pg-footer li {\n    all: initial;\n    display: block;\n    margin-top: 1em;\n    margin-bottom: 1em;\n    text-indent: -0.6em;\n}\n#pg-footer div.secthead {\n    font-size: 110%;\n    font-weight: bold;\r\n```\n\n웹페이지의 URL은 url 변수에 저장되어 있습니다. 이 URL은 'Dead Men Tell No Lies' 책의 온라인 epub 버전을 얻을 수 있는 페이지를 가리킵니다.\nrequests.get 메서드는 지정한 URL로 HTTP GET 요청을 보냅니다.\n\nverify=False는 SSL 인증서 확인을 비활성화하는 데 사용됩니다(운영 환경에서 사용하지 않는 것이 좋음).\n\n<div class=\"content-ad\"></div>\n\n응답의 HTML 콘텐츠는 r.text를 사용하여 추출됩니다.\n\n## 4. BeautifulSoup를 사용하여 HTML 구문 분석하기\n\n```js\n# HTML에서 BeautifulSoup 객체 생성\nsoup = BeautifulSoup(html, \"html.parser\")\n# soup 제목 가져오기\nsoup.title\n\n#출력\n<title>\n      Dead Men Tell No Tales, by E. W. Hornung\n    </title>\n```\n\n이 코드는 \"html.parser\"를 사용하여 HTML 콘텐츠를 구문 분석하는 BeautifulSoup 객체를 초기화합니다. 그런 다음 이 웹 페이지의 제목인 \"Dead Men Tell No Tales, by E. W. Hornung\"을 추출하고 출력합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 문자열로 된 스프 타이틀 가져오기\nsoup.title.string\n\n#결과\n'\\r\\n      Dead Men Tell No Tales, by E. W. Hornung\\r\\n    '\n```\n\n이 코드는 타이틀 태그의 텍스트 내용을 문자열로 추출합니다. 결과에는 웹 페이지의 제목이 포함되며, 앞뒤의 공백 문자도 포함되어 있습니다.\n\n## 5. BeautifulSoup로 하이퍼링크 추출하기\n\n```js\n# 스프에서 하이퍼링크 가져오기 및 처음 몇 개 확인하기\nsoup.findAll('a')[:8]\n\n# 결과\n[<a class=\"reference external\" href=\"https://www.gutenberg.org\">www.gutenberg.org</a>,\n <a class=\"pginternal\" href=\"#link2HCH0001\"> CHAPTER I. </a>,\n <a class=\"pginternal\" href=\"#link2HCH0002\"> CHAPTER II. </a>,\n <a class=\"pginternal\" href=\"#link2HCH0003\"> CHAPTER III. </a>,\n <a class=\"pginternal\" href=\"#link2HCH0004\"> CHAPTER IV. </a>,\n <a class=\"pginternal\" href=\"#link2HCH0005\"> CHAPTER V. </a>,\n <a class=\"pginternal\" href=\"#link2HCH0006\"> CHAPTER VI. </a>,\n <a class=\"pginternal\" href=\"#link2HCH0007\"> CHAPTER VII. </a>]\n```\n\n<div class=\"content-ad\"></div>\n\n이 코드는 HTML에서 모든 앵커(`a`) 태그를 찾아 처음 여덟 개를 출력합니다. 각 앵커 태그에는 하이퍼링크(href 속성)와 관련된 텍스트가 포함되어 있습니다.\n\n## 6. HTML에서 텍스트 내용 추출 및 인쇄\n\n```js\n# 뷰티풀수프에서 텍스트를 추출하고 인쇄합니다\ntext = soup.get_text()\nprint(text)\n```\n\n```js\n#출력: 예시\n   Dead Men Tell No Tales, by E. W. Hornung\n\nThe Project Gutenberg eBook of Dead Men Tell No Tales\nThis ebook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this ebook or online\nat www.gutenberg.org. If you are not located in the United States,\nyou will have to check the laws of the country where you are located\nbefore using this eBook.\nTitle: Dead Men Tell No Tales\n\nAuthor: E. W. Hornung\n\nRelease date: April 1, 1999 [eBook #1703]\n                Most recently updated: June 10, 2022\nLanguage: English\nCredits: Produced by An Anonymous Project Gutenberg Volunteer, and David Widger\n\n*** START OF THE PROJECT GUTENBERG EBOOK DEAD MEN TELL NO TALES ***\n\n      DEAD MEN TELL NO TALES\n\n      By E. W. Hornung\n```\n\n<div class=\"content-ad\"></div>\n\n이 코드는 파싱된 HTML 문서에서 모든 텍스트 콘텐츠를 추출하기 위해 get_text() 메서드를 사용합니다. 그런 다음 print 문은 이 텍스트 콘텐츠를 콘솔에 출력하여 HTML 태그없이 웹페이지의 전체 텍스트 콘텐츠를 표시합니다.\n\n## 7. 추출된 텍스트 정리\n\n```js\ndef clean_text(text):\n    # 비 문자 숫자 문자를 제거합니다 (공백 및 구두점 제외)\n    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,]', '', text)\n    # 여러 공백을 단일 공백으로 대체합니다\n    text = re.sub(r'\\s+', ' ', text)\n    # 모든 숫자를 공백으로 대체합니다\n    text = re.sub(r'\\d+', '', text)\n    text = contractions.fix(text)\n    text = text.lower()\n    return  \" \".join(text.split())\n\n\n# 'text' 열에 정리 함수를 적용합니다\ntext_c = clean_text(text)\ntext_c\n```\n\n이 코드는 다음과 같은 정리 단계를 수행합니다:\n\n<div class=\"content-ad\"></div>\n\n- 알파벳 숫자만 남기기: 문자, 숫자, 공백, 마침표 및 쉼표만 유지합니다.\n- 공백 정규화: 여러 개의 공백을 단일 공백으로 대체합니다.\n- 숫자 제거: 텍스트에서 모든 숫자를 제거합니다.\n- 축약형 풀기: 축약형을 전체 형태로 확장합니다(예: \"don't\"를 \"do not\"로) contractions 라이브러리를 사용하여.\n- 소문자로 변환: 모든 문자를 소문자로 변환합니다.\n- 추가 공백 제거: 단어 사이에 선행, 후행 또는 추가 공백이 없는지 확인합니다.\n\n정리된 텍스트는 text_c 변수에 저장됩니다.\n\n```js\n#출력\n\n'dead men tell no tales, by e. w. hornung the project gutenberg ebook of dead men tell no tales this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever. you may copy it, give it away or reuse it under the terms of the project gutenberg license included with this ebook or online at www.gutenberg.org. if you are not located in the united states, you will have to check the laws of the country where you are located before using this ebook. title dead men tell no tales author e. w. hornung release date april , ebook most recently updated june , language english credits produced by an anonymous project gutenberg volunteer, and david widger start of the project gutenberg ebook dead men tell no tales dead men tell no tales by e. w. hornung contents chapter i. love on the ocean chapter ii. the mysterious cargo chapter iii. to the waters edge chapter iv. the silent sea chapter v. my reward chapter vi. the sole survivor chapter vii. i find a friend chapter viii. a small precaution chapter ix. my convalescent home chapter x. wine and weakness chapter xi. i live again chapter xii. my ladys bidding chapter xiii. the longest day of my life chapter xiv. in the garden chapter xv. first blood chapter xvi. a deadlock chapter xvii. thieves fall out chapter xviii. a man of many murders chapter xix. my great hour chapter xx. the statement of francis rattray chapter i. love on the ocean nothing is so easy as falling in love on a long sea voyage, except falling out of love. especially was this the case in the days when the wooden clippers did finely to land you in sydney or in melbourne under the four full months. we all saw far too much of each other, unless, indeed, we were to see still more. our superficial attractions mutually exhausted, we lost heart and patience in the disappointing strata which lie between the surface and the bedrock of most natures. my own experience was confined to the round voyage of the lady jermyn, in the year . it was no common experience, as was only too well known at the time. and i may add that i for my part had not the faintest intention of falling in love on board nay, after all these years, let me confess that i had good because to hold myself proof against such weakness.\n```\n\n## 8. 정제된 텍스트 토큰화\n\n<div class=\"content-ad\"></div>\n\n```js\n# 토크나이저 생성\ntokenizer = RegexpTokenizer('\\w+')\n\n# 토큰 생성\ntokens = tokenizer.tokenize(text_c)\ntokens[:8]\n```\n\n이 코드는 다음 단계를 수행합니다:\n\n토크나이저 생성:\n\n- 알파벳 및 숫자(문자와 숫자) 시퀀스를 캡처하는 정규 표현식 \\w+를 사용하여 단어와 일치하는 RegexpTokenizer를 초기화합니다.\n\n<div class=\"content-ad\"></div>\n\n토크나이저를 사용하여 정제된 텍스트(text_c)를 토큰(개별 단어)으로 분할합니다.\n\n첫 번째 몇 개의 토큰을 표시합니다:\n\n토큰화된 출력물을 간략히 보기 위해 처음 여덟 개의 토큰을 표시합니다.\n\n<div class=\"content-ad\"></div>\n\n이 과정은 텍스트를 단어 목록으로 변환하여 추가 텍스트 분석 및 처리에 사용할 수 있습니다.\n\n```js\n#출력:\n['dead', 'men', 'tell', 'no', 'tales', 'by', 'e', 'w']\n```\n\n## 9. 토큰에서 불용어 제거\n\n```js\nsw = set(stopwords.words('english'))\n\n# 추가 불용어 추가\nadditional_stopwords = {'could', 'said', 'must', 'would', 'should', 'might', 'gutenberg','project'}\nsw.update(additional_stopwords)\n# 새로운 목록 초기화\nwords_ns = []\n\n# words_ns에 tokens에 있는 단어 중 sw에 없는 모든 단어 추가\nfor word in tokens:\n    if word not in sw:\n        words_ns.append(word)\n\n# 상식적인 확인을 위해 몇 가지 항목 출력\nwords_ns[:5]\n```\n\n<div class=\"content-ad\"></div>\n\nStopwords 불러오기:\n\n- NLTK의 영어 말뭉치에서 stopwords.words('english')를 사용하여 stopwords를 가져옵니다. Stopwords란 \"the\", \"and\", \"is\" 등과 같은 일반적이고 종종 텍스트의 의미에 크게 기여하지 않는 단어를 말합니다.\n\n추가적인 Stopwords 추가하기:\n\n- 컨텍스트에 특정한 추가적인 stopwords를 포함하는 additional_stopwords 집합을 정의합니다. 이러한 단어들은 update()를 사용하여 sw 집합에 추가됩니다.\n\n<div class=\"content-ad\"></div>\n\n비어 있는 리스트 초기화:\n\n- 비어 있는 단어 목록인 words_ns를 초기화합니다.\n\n불용어 제거:\n\n- 토큰 목록에서 각 단어를 반복합니다.\n- 단어가 불용어 세트(sw set)에 없는지 확인합니다(즉, 불용어가 아닌지 확인합니다).\n- 만약 단어가 불용어가 아니라면, words_ns 목록에 추가됩니다.\n\n<div class=\"content-ad\"></div>\n\n정신을 차리세요:\n\n- words_ns 목록에서 처음 다섯 항목을 인쇄하여 불용어가 성공적으로 제거되었는지 확인합니다.\n\n이 과정을 통해 words_ns 목록에 원본 토큰화된 텍스트에서 일반적인 불용어 및 추가 지정된 단어를 제외하고 의미 있는 단어만 포함되도록 확인합니다.\n\n```js\n#출력\n['dead', 'men', 'tell', 'tales', 'e']\n```\n\n<div class=\"content-ad\"></div>\n\n## 10. 단어 빈도 시각화\n\n```js\n# 그림을 인라인으로 설정하고 시각화 스타일을 설정합니다\n%matplotlib inline\nsns.set()\n\n# 빈도 분포를 생성하고 플롯합니다\nfreqdist1 = nltk.FreqDist(words_ns)\nfreqdist1.plot(20)\n```\n\n인라인 플로팅 활성화:\n\n- %matplotlib inline은 주피터 노트북에서 사용되는 매직 커맨드로, 플롯이 노트북 내에서 인라인으로 나타날 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\n표 태그를 마크다운 형식으로 바꿔주세요.\n\n| 설정 시각화 스타일:\n- sns.set()은 plot에 대한 Seaborn의 기본 미학 매개변수를 설정합니다.\n\n| 빈도 분포 생성:\n- nltk.FreqDist(words_ns)는 words_ns 리스트의 단어들을 빈도 분포로 생성합니다. 각 단어의 빈도수가 계산됩니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 Frequency Distribution을 시각화합니다:\n\n- freqdist1.plot(20)은 frequency distribution인 freqdist1에서 가장 빈도가 높은 상위 20개의 단어를 시각화합니다.\n\n이 시각화는 정제 및 처리된 텍스트에서 가장 빈번하게 발생하는 단어를 이해하는 데 도움이 되며, 문서의 내용과 초점에 대한 통찰을 제공합니다.\n\n![Frequency Distribution Plot](/assets/img/2024-06-23-BuildingDataPipelineswithWebScrapingAStep-by-StepGuide_0.png)\n\n<div class=\"content-ad\"></div>\n\n만약 추가적인 불용어를 추가하지 않는다면 어떻게 되는지 예시가 아래에 나와 있어요.\n\n![이미지](/assets/img/2024-06-23-BuildingDataPipelineswithWebScrapingAStep-by-StepGuide_1.png)\n\n여기가 웹사이트가 가르치는 내용을 마치는 곳이에요. 이제 우리는 다른 URL로 시도해보고 다른 책들을 스크래핑하여 또 다른 빈도 분포를 생성해 볼 수 있어요.\n\n## 11. 바이그램(bigrams)과 트라이그램(trigrams) 추출\n\n<div class=\"content-ad\"></div>\n\n빅램과 트리그램은 주어진 텍스트나 음성 샘플에서 n개의 항목으로 이루어진 연속적인 시퀀스인 n-그램의 한 유형입니다. 자연어 처리(NLP)에서 이러한 항목은 일반적으로 단어입니다.\n\n빅램:\n\n- 빅램은 두 연이은 단어의 시퀀스입니다.\n- 예를 들어, “I love programming”이라는 문장에서 빅램은 다음과 같습니다:\n- “I love”\n- “love programming”\n\n트리그램:\n\n<div class=\"content-ad\"></div>\n\n- Trigrams은 연이은 세 개의 단어 시퀀스입니다.\n- 예를 들어, \"I love programming\"이라는 문장에서 trigrams는 다음과 같습니다:\n- \"I love programming\"\n\n## NLP에서의 사용:\n\n- Bigrams와 trigrams은 다음과 같은 다양한 NLP 작업에 사용됩니다:\n- 텍스트 분석: 단어 사이의 문맥과 관계를 이해하는 데 사용됩니다.\n- 언어 모델링: 시퀀스에서 다음 단어를 예측하는 데 사용됩니다.\n- 정보 검색: 단어 쌍이나 세 번씩 고려함으로써 검색 알고리즘을 개선하는 데 사용됩니다.\n- 감성 분석: 문구의 감정을 이해하는 데 도움이 되는 단일 단어보다 문맥을 더 잘 포착합니다.\n\n```js\nbigrams = list(ngrams(words_ns, 2))\ntrigrams = list(ngrams(words_ns, 3))\n\n# 빈도 분석\nbigram_freq = Counter(bigrams)\ntrigram_freq = Counter(trigrams)\n\n# 상위 10개의 bigram과 trigram 표시\nprint(\"상위 10개 Bigrams:\")\nfor bigram, freq in bigram_freq.most_common(10):\n    print(bigram, freq)\n\nprint(\"\\n상위 10개 Trigrams:\")\nfor trigram, freq in trigram_freq.most_common(10):\n    print(trigram, freq)\n```\n\n<div class=\"content-ad\"></div>\n\nBigrams과 Trigrams을 사용하면 개별 단어(일그램)를 사용하는 것보다 텍스트에서 더 많은 맥락과 의미를 포착할 수 있어요.\n\n```js\n#결과\n\n상위 10개의 Bigrams:\n('lady', 'jermyn') 33\n('mr', 'cole') 23\n('eva', 'denison') 19\n('miss', 'denison') 17\n('electronic', 'works') 16\n('united', 'states') 15\n('captain', 'harris') 14\n('literary', 'archive') 13\n('archive', 'foundation') 13\n('electronic', 'work') 11\n\n상위 10개의 Trigrams:\n('literary', 'archive', 'foundation') 13\n('dead', 'men', 'tell') 6\n('men', 'tell', 'tales') 6\n('never', 'shall', 'forget') 5\n('protected', 'copyright', 'law') 4\n('e', 'w', 'hornung') 3\n('ebook', 'dead', 'men') 3\n('located', 'united', 'states') 3\n('united', 'states', 'check') 3\n('states', 'check', 'laws') 3\n```\n\n## 12. Trigrams와 Bigrams 그래프 작성:\n\n```js\n# ngram 빈도수를 그리는 함수\ndef plot_ngrams(ngram_freq, title, xlabel, ylabel):\n    ngrams, freqs = zip(*ngram_freq.most_common(10))\n    ngrams = [' '.join(ngram) for ngram in ngrams]\n    \n    # 리스트로 변환\n    ngrams = list(ngrams)\n    freqs = list(freqs)\n    \n    # 디버깅: 타입 및 내용 출력\n    print(\"freqs의 타입:\", type(freqs))\n    print(\"ngrams의 타입:\", type(ngrams))\n    print(\"freqs의 내용:\", freqs)\n    print(\"ngrams의 내용:\", ngrams)\n    \n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=freqs, y=ngrams)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.show()\n\n\n# 상위 10개의 Bigrams 그래프로 나타내기\nplot_ngrams(bigram_freq, '상위 10개의 Bigrams', '빈도수', 'Bigrams')\n\n# 상위 10개의 Trigrams 그래프로 나타내기\nplot_ngrams(trigram_freq, '상위 10개의 Trigrams', '빈도수', 'Trigrams')\n```\n\n<div class=\"content-ad\"></div>\n\n## 13. POS 태그 생성\n\n```js\ndef preprocess_text(text): \n    sentences = sent_tokenize(text) \n    sentences = [nltk.pos_tag(word_tokenize(sent)) for sent in sentences]\n    \n    return sentences\n\nsent_text = preprocess_text(text)\n\n# 디버깅: 토큰화 및 POS 태그가 지정된 문장 출력\nprint(\"토큰화 및 POS 태그가 지정된 문장:\")\nfor sent in sent_text:\n    print(sent)\n```\n\n- nltk.pos_tag는 문장의 각 단어에 대해 품사를 태깅합니다.\n- 이 함수는 각 문장이 (단어, POS 태그) 튜플을 포함하는 리스트인 문장의 리스트를 반환합니다.\n\n텍스트 입력에 대한 출력은 각 문장을 (단어, POS 태그) 튜플의 리스트로 나타낸 문장의 리스트가 됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n#출력\n\n토큰화 및 품사 태깅된 문장:\n[('Dead', 'JJ'), ('Men', 'NNP'), ('Tell', 'NNP'), ('No', 'NNP'), ('Tales', 'NNP'), (',', ','), ('by', 'IN'), ('E.', 'NNP'), ('W.', 'NNP'), ('Hornung', 'NNP'), ('The', 'DT'), ('Project', 'NNP'), ('Gutenberg', 'NNP'), ('eBook', 'NN'), ('of', 'IN'), ('Dead', 'JJ'), ('Men', 'NNP'), ('Tell', 'NNP'), ('No', 'NNP'), ('Tales', 'NNP'), ('This', 'DT'), ('ebook', 'NN'), ('is', 'VBZ'), ('for', 'IN'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('anyone', 'NN'), ('anywhere', 'RB'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('and', 'CC'), ('most', 'JJS'), ('other', 'JJ'), ('parts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('world', 'NN'), ('at', 'IN'), ('no', 'DT'), ('cost', 'NN'), ('and', 'CC'), ('with', 'IN'), ('almost', 'RB'), ('no', 'DT'), ('restrictions', 'NNS'), ('whatsoever', 'RB'), ('.', '.')]\n[('You', 'PRP'), ('may', 'MD'), ('copy', 'VB'), ('it', 'PRP'), (',', ','), ('give', 'VB'), ('it', 'PRP'), ('away', 'RB'), ('or', 'CC'), ('re-use', 'VB'), ('it', 'PRP'), ('under', 'IN'), ('the', 'DT'), ('terms', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Project', 'NNP'), ('Gutenberg', 'NNP'), ('License', 'NNP'), ('included', 'VBD'), ('with', 'IN'), ('this', 'DT'), ('ebook', 'NN'), ('or', 'CC'), ('online', 'NN'), ('at', 'IN'), ('www.gutenberg.org', 'NN'), ('.', '.')]\n[('If', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('not', 'RB'), ('located', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), (',', ','), ('you', 'PRP'), ('will', 'MD'), ('have', 'VB'), ('to', 'TO'), ('check', 'VB'), ('the', 'DT'), ('laws', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('country', 'NN'), ('where', 'WRB'), ('you', 'PRP'), ('are', 'VBP'), ('located', 'VBN'), ('before', 'IN'), ('using', 'VBG'), ('this', 'DT'), ('eBook', 'NN'), ('.', '.')]\r\n```\n\n이 전처리 단계는 명명된 엔터티 인식, 구문 분석 및 의미 분석과 같은 NLP 작업에 유용합니다.\n\n## 14. 명명된 엔터티 추출\n\n```js\r\n# NER 수행\ndef extract_named_entities(sentences):\n    named_entities = []\n    for sent in sentences:\n        # nltk의 ne_chunk를 사용하여 NER 수행\n        tree = nltk.ne_chunk(sent, binary=False)\n        print(tree)\n        for subtree in tree:\n            if hasattr(subtree, 'label'):\n                entity_name = ' '.join([child[0] for child in subtree.leaves()])\n                entity_type = subtree.label()\n                named_entities.append((entity_name, entity_type))\n    return named_entities\n\nnamed_entities = extract_named_entities(sent_text)\n\n# 추출된 명명된 엔터티 출력\nfor entity in named_entities:\n    print(entity)\r\n```\n\n<div class=\"content-ad\"></div>\n\n이 코드 블록은 NLTK를 사용하여 토큰화 및 POS 태깅된 문장에 대해 Named Entity Recognition(NER)을 수행하고 명명된 엔티티를 추출하여 출력하는 함수를 정의합니다.\n\n여기서는 각 문장에 대해 NER을 수행하기 위해 NLTK의 ne_chunk 함수를 사용합니다. binary=False 매개변수는 엔티티가 특정 유형(예: PERSON, ORGANIZATION 등)으로 분류되어야 함을 지정합니다.\n\n```js\n#OUTPUT\n(S\n  Dead/JJ\n  Men/NNP\n  (ORGANIZATION Tell/NNP No/NNP Tales/NNP)\n  ,/,\n  by/IN\n  E./NNP\n  W./NNP\n  Hornung/NNP\n  The/DT\n  (ORGANIZATION Project/NNP Gutenberg/NNP)\n  eBook/NN\n  of/IN\n  (ORGANIZATION Dead/JJ Men/NNP Tell/NNP No/NNP Tales/NNP)\n  This/DT\n  ebook/NN\n  is/VBZ\n  for/IN\n  the/DT\n  use/NN\n  of/IN\n  anyone/NN\n  anywhere/RB\n  in/IN\n  the/DT\n  (GPE United/NNP States/NNPS)\n  and/CC\n  most/JJS\n  other/JJ\n  parts/NNS\n  of/IN\n  the/DT\n  world/NN\n  at/IN\n  no/DT\n  cost/NN\n  and/CC\n  with/IN\n  almost/RB\n  no/DT\n  restrictions/NNS\n  whatsoever/RB\n  ./.)\n(S\n  You/PRP\n  may/MD\n  copy/VB\n  it/PRP\n  ,/,\n  give/VB\n  it/PRP\n  away/RB\n  or/CC\n  re-use/VB\n  it/PRP\n  under/IN\n  the/DT\n  terms/NNS\n  of/IN\n  the/DT\n  (ORGANIZATION Project/NNP Gutenberg/NNP License/NNP)\n  included/VBD\n  with/IN\n  this/DT\n  ebook/NN\n  or/CC\n  online/NN\n  at/IN\n  www.gutenberg.org/NN\n  ./.)\nand so on...\n```\n\n이 과정은 텍스트 내의 인물, 조직 및 위치와 같은 명명된 엔티티를 식별하고 분류하여 다양한 NLP 애플리케이션에 유용한 정보를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nprint(named_entities)\n```\n\n```js\n#OUTPUT\n[('Tell No Tales', 'ORGANIZATION'),\n ('Project Gutenberg', 'ORGANIZATION'),\n ('Dead Men Tell No Tales', 'ORGANIZATION'),\n ('United States', 'GPE'),\n ('Project Gutenberg License', 'ORGANIZATION'),\n ('United States', 'GPE'),\n ('Title', 'GPE'),\n ('Tell No Tales', 'ORGANIZATION'),\n ('Hornung Release', 'PERSON'),\n ('David Widger', 'PERSON'),\n ('THE', 'ORGANIZATION'),\n ('PROJECT', 'ORGANIZATION'),\n ('TELL', 'ORGANIZATION'),\n ('TELL', 'ORGANIZATION'),\n ('Hornung', 'PERSON'),\n ('CONTENTS', 'ORGANIZATION'),\n ('LOVE', 'ORGANIZATION'),\n ('OCEAN', 'ORGANIZATION'),\n ('MYSTERIOUS', 'ORGANIZATION'),\n ('THE', 'ORGANIZATION'),\n ('WATER', 'ORGANIZATION'),\n ('EDGE', 'ORGANIZATION'),\n ('SILENT', 'ORGANIZATION'),\n ('REWARD', 'ORGANIZATION'),\n ('SOLE', 'ORGANIZATION'),\n ('FRIEND', 'ORGANIZATION'),\n ('SMALL', 'ORGANIZATION'),\n ('CONVALESCENT', 'ORGANIZATION'),\n ('WINE', 'ORGANIZATION'),\n ('AND', 'ORGANIZATION'),\n ('WEAKNESS', 'ORGANIZATION'),\n ('AGAIN', 'ORGANIZATION'),\n ('BIDDING', 'ORGANIZATION'),\n.......\n```\n\n## NER entities plotting:\n\n```js\n# Frequency analysis\nentity_freq = Counter([entity_type for entity_name, entity_type in named_entities])\n\n# Display top 10 named entities\nprint(\"Top 10 Named Entities:\")\nfor entity, freq in entity_freq.most_common(10):\n    print(entity, freq)\n\n#OUTPUT\nTop 10 Named Entities:\nPERSON 505\nORGANIZATION 257\nGPE 254\n```\n\n<div class=\"content-ad\"></div>\n\n```python\n# 엔티티 빈도수를 그리는 함수\ndef plot_entity_frequency(entity_freq, title, xlabel, ylabel):\n    entities, freqs = zip(*entity_freq.most_common(10))\n    # seaborn과 호환성을 보장하기 위해 리스트로 변환\n    entities = list(entities)\n    freqs = list(freqs)\n    \n    # 디버깅: 타입과 내용 출력\n    print(\"freqs의 타입:\", type(freqs))\n    print(\"entities의 타입:\", type(entities))\n    print(\"freqs의 내용:\", freqs)\n    print(\"entities의 내용:\", entities)\n    \n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=freqs, y=entities)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.show()\n\n# 상위 10개의 명명된 엔티티 그래프로 나타내기\nplot_entity_frequency(entity_freq, '상위 10개 명명된 엔티티', '빈도수', '엔티티')\n```\n\n![그림](/assets/img/2024-06-23-BuildingDataPipelineswithWebScrapingAStep-by-StepGuide_2.png)\n\n## 15. spaCy를 이용한 명명된 엔티티 인식 개선하기\n\nNLTK 기반의 명명된 엔티티 인식 (NER) 방법은 일반적으로 잘 작동하지만 가끔 일반 단어를 엔티티로 잘못 분류할 수 있습니다. 예를 들어, \"Contents,\" \"Love,\" \"Ocean,\" 그리고 \"Friend\"와 같은 단어들이 기관으로 잘못 식별되었는데, 이는 분명히 정확하지 않습니다.\n\n\n<div class=\"content-ad\"></div>\n\n더 정확한 결과를 얻기 위해 spaCy 라이브러리를 사용하도록 변경하겠습니다. spaCy는 강력하고 현대적인 NLP 라이브러리로, NER을 더 정확하게 처리하기 위해 설계되었습니다. SpaCy의 미리 훈련된 모델은 텍스트의 명명된 엔티티를 인식하는 데 매우 효과적으로 작용하여 이러한 잘못된 분류를 최소화합니다.\n\n```python\n# spaCy 모델을 불러오기\nnlp = spacy.load(\"en_core_web_sm\")\n\n# spaCy를 사용하여 텍스트 처리\ndoc = nlp(text_c)\n\n# 명명된 엔티티 추출 및 출력\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n```\n\nspaCy의 출력:\n\n```python\ne. w. hornung PERSON\nthe united states GPE\nthe united states GPE\ne. w. hornung PERSON\njune DATE\nenglish NORP\ndavid PERSON\ne. w. hornung PERSON\nchapter i. love ORG\nchapter vii LAW\nchapter x. LAW\nthe longest day DATE\nfirst ORDINAL\nfrancis rattray chapter i. love ORG\nthe days DATE\nthe four full months DATE\nthe year DATE\neva denison PERSON\nmore than nineteen years of age DATE\nfirst ORDINAL\nher years DATE\ndenison PERSON\ntwo CARDINAL\njanuary DATE\nthe beginning of the following july DATE\nthe most odious weeks DATE\nblack hill LOC\nas much as four CARDINAL\nhalf CARDINAL\nlondon GPE\nfirst ORDINAL\nfive CARDINAL\nfive pounds QUANTITY\naustralia GPE\none CARDINAL\nonly five CARDINAL\nany minute of the day TIME\nthe hour TIME\ndenison PERSON\none CARDINAL\nsixty CARDINAL\njoaquin santos PERSON\nfirst ORDINAL\ndenison PERSON\na few months later DATE\ndenison PERSON\nengland GPE\nafrica LOC\nsecond ORDINAL\neva denison PERSON\n```\n\n<div class=\"content-ad\"></div>\n\n## Plotting Spacy 파생 엔티티\n\n```js\n# 엔티티 타입 카운트\nentity_types = [ent.label_ for ent in doc.ents]\nentity_type_freq = Counter(entity_types)\n\n# 시본을 사용한 플로팅\nplt.figure(figsize=(8, 5))\nsns.barplot(x=list(entity_type_freq.keys()), y=list(entity_type_freq.values()))\nplt.title('엔티티 타입 및 빈도')\nplt.xlabel('엔티티 타입')\nplt.ylabel('빈도')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-23-BuildingDataPipelineswithWebScrapingAStep-by-StepGuide_3.png\" />\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n이 연습에서는 온라인 소스에서 텍스트 데이터를 추출하고 분석하는 과정을 탐색했습니다. 우리는 먼저 웹페이지의 HTML 내용을 가져와 BeautifulSoup로 구문 분석하고 추출된 텍스트를 정리하여 원치 않는 문자와 불용어를 제거했습니다. 그런 다음 텍스트를 토큰화하고 NLTK를 사용하여 초기 Named Entity Recognition(NER)을 수행했습니다.\n\nNLTK 기반의 NER은 텍스트에 존재하는 엔티티에 대한 기본적인 이해를 제공했지만 공통 단어를 명명된 엔티티로 오분류하는 등의 한계가 있었습니다. 이러한 부정확성에 대응하기 위해 보다 고급이고 정확한 NER 접근 방식을 제공하는 spaCy 라이브러리를 소개했습니다.\n\nspaCy를 사용하여 우리는 더 나은 결과를 얻을 수 있었고, 자연어 처리 작업에 적합한 올바른 도구를 선택하는 중요성을 입증했습니다. spaCy를 사용하여 명명된 엔티티를 인식하는 향상된 정밀도는 텍스트 데이터로부터 신뢰할 수 있는 통찰을 제공하는 가치를 강조합니다.\n\n이 단계를 따라가면 큰 양의 텍스트를 효율적으로 처리하고 분석하여 의미 있는 정보를 추출하고 내용에 대한 깊은 통찰을 얻을 수 있습니다. 이 접근 방식은 데이터 과학, 연구 및 비즈니스 인텔리전스의 다양한 응용 분야에 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n# 참고 자료\n\n- Web Scraping & NLP in Python | DataCamp\n- What Is Web Scraping? [A Complete Step-by-Step Guide] (careerfoundry.com)\n\n일부 설명은 chatGPT의 도움을 받아 작성되었습니다.","ogImage":{"url":"/assets/img/2024-06-23-BuildingDataPipelineswithWebScrapingAStep-by-StepGuide_0.png"},"coverImage":"/assets/img/2024-06-23-BuildingDataPipelineswithWebScrapingAStep-by-StepGuide_0.png","tag":["Tech"],"readingTime":27},{"title":"OpenAI의 CLIP 모델 이해하기 2024 최신 분석 및 기능 소개","description":"","date":"2024-06-23 20:08","slug":"2024-06-23-UnderstandingOpenAIsCLIPmodel","content":"\n\nCLIP은 2021년 OpenAI에 의해 출시되어 그 이후로 많은 다중 모달 AI 시스템의 핵심 구성 요소 중 하나가 되었습니다. 이 기사는 CLIP에 대한 심층적인 내용을 다룹니다. CLIP이 무엇이며, 어떻게 작동하는지, 어떻게 사용되는지, 그리고 어떻게 구현되는지에 대해 소개합니다.\n\n# 소개\n\nCLIP은 Contrastive Language-Image Pre-training의 약자로, 자연어 감독에서 학습하기 위한 효율적인 방법으로 2021년에 소개되었습니다. 이 방법은 Learning Transferable Visual Models From Natural Language Supervision 논문에서 소개되었습니다.\n\n요약하면, CLIP은 4억 개의 이미지와 텍스트 쌍을 이용하여 자가 감독 방식으로 훈련된 공통 이미지 및 텍스트 임베딩 모델입니다. 이는 텍스트와 이미지를 동일한 임베딩 공간에 매핑한다는 것을 의미합니다. 예를 들어, 개의 이미지와 \"개의 이미지\"라는 문장은 매우 유사한 임베딩을 갖게 되고 벡터 공간에서 서로 가까이 위치하게 됩니다. 이는 이미지 데이터베이스를 설명으로 검색하거나 그 반대의 응용 프로그램을 구축할 수 있다는 점에서 매우 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n저자들은 CLIP이 훈련되지 않은 다양한 작업에 사용될 수 있음을 발견했습니다. 예를 들어, CLIP은 ImageNet과 같은 이미지 분류 데이터셋에서 놀라운 제로샷 성능을 거뒀다. 제로샷 러닝은 모델이 ImageNet 데이터셋의 1.28백만 개의 훈련 예시 중 어느 것도 명시적으로 훈련받지 않았다는 사실을 가리킵니다. 그럼에도 불구하고, CLIP은 이미지와 텍스트가 동일한 임베딩 공간에 있고 도트 곱이 임베딩 간 유사성을 계산하기 때문에 \"강아지의 사진\"과의 도트 곱이 가장 높을 가능성이 큽니다. 따라서 이미지를 강아지로 예측할 수 있습니다. CLIP을 진정한 분류기로 바꾸고 싶다면 도트 곱을 소프트맥스 함수를 통해 각 클래스에 대한 예측 확률을 얻을 수 있습니다.\n\n위 과정은 다음 그림의 2단계와 3단계에서 확인할 수 있습니다.\n\n![CLIP 모델 이해](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_0.png)\n\n<div class=\"content-ad\"></div>\n\n이제 CLIP가 어떻게 작동하는지 자세히 살펴보겠습니다.\n\n# 모델 세부정보\n\n## 아키텍처\n\nCLIP 모델에는 텍스트 인코더(텍스트를 임베드하는 부분)와 이미지 인코더(이미지를 임베드하는 부분) 두 가지 주요 구성 요소가 있습니다. 텍스트 인코더에는 Transformer가 사용되었습니다. 이 아키텍처는 2017년 이후부터 NLP 분야를 혁신시켜 왔으며 당연히 사용되었다고 할 수 있습니다. 시각적인 설명이 필요하시면 아래 블로그를 참조하세요.\n\n<div class=\"content-ad\"></div>\n\n이미지 인코더에 대해 작성자는 두 가지 다른 모델을 시도했습니다. ResNet-50 및 Vision Transformer (ViT)입니다. ResNet-50은 이미지 분류에 사용되는 컨볼루션 신경망 (CNN)을 사용한 원래의 최신 아키텍처이며, ViT는 이미지를 위한 원본 Transformer의 최근적인 적응으로 각 이미지를 패치 시퀀스로 분할하고 토큰 시퀀스로 유사하게 모델에 전달합니다. 작성자는 ViT가 더 빠르게 훈련되었음을 발견했습니다.\n\n텍스트 및 이미지 인코더 모두 처음부터 훈련되었습니다.\n\n모든 아키텍처에 대해 논문에서 설명한 대로 소량의 수정이 가해졌습니다.\n\n## 훈련\n\n<div class=\"content-ad\"></div>\n\n저자들은 초기에 이미지 캡션 모델을 훈련시켜보려고 했는데, 이미지를 주면 정확한 캡션/설명을 예측하는 모델이었습니다.\n\n그러나 4억(이미지, 텍스트) 쌍을 훈련시키기에는 규모가 맞지 않다고 판단하여, 비교 표현 학습 접근 방식으로 바꾸기로 결정했습니다. 비교적 표현 학습의 목표는 비슷한 샘플 쌍이 서로 가깝게 유지되고 다른 샘플 쌍이 멀리 떨어지도록 하는 임베딩 공간을 학습하는 것입니다.\n\n표준 비교 학습 접근 방식에서는 모델에 (앵커, 양성, 음성) 형식의 예제를 제공합니다. 여기서 앵커는 한 클래스(예: 개)의 이미지이고, 양성은 같은 클래스(개)의 다른 이미지, 음성은 다른 클래스(예: 새)의 이미지입니다. 그런 다음, 이미지를 임베딩하고 같은 클래스(개)에 대한 두 임베딩 사이의 거리(distance(anchor, positive))를 최소화하고, 서로 다른 클래스(개와 새)에 대한 두 임베딩 사이의 거리(distance(anchor, negative))를 최대화하도록 모델을 훈련합니다. 이는 모델이 동일한 객체에 대해 매우 유사한 임베딩을 출력하고, 서로 다른 객체에 대해 서로 다른 임베딩을 출력하도록 격려합니다.\n\n<div class=\"content-ad\"></div>\n\n동일한 방식은 텍스트와 텍스트와 이미지의 조합에도 적용할 수 있습니다. 예를 들어, CLIP의 경우 단일 훈련 예제에 대해 앵커는 개의 이미지일 수 있으며, 양성은 \"개의 이미지\"라는 캡션일 수 있으며, 부정은 \"새의 이미지\"라는 캡션일 수 있습니다.\n\nCLIP는 다중 클래스 N-pair loss를 사용하여 이를 더 확장하며, 이는 앵커마다 여러 개의 부정과 양성이 있을 때의 상황입니다. 논문에 설명된 바에 따르면:\n\n논문에서 제공된 아래 의사 코드는 핵심 세부 정보를 잘 캡슐화합니다:\n\n![이미지](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_2.png)\n\n<div class=\"content-ad\"></div>\n\n다음은 순서입니다:\n\n- 이미지 인코더로 이미지를 포함하고, 텍스트 인코더로 텍스트를 포함합니다.\n- 이미지 및 텍스트 임베딩은 서로 다른 모델에서 나오며 차원이 다르기 때문에, 학습된 프로젝션 매트릭스와의 곱셈을 통해 동일한 합성 다중 모달 임베딩 공간으로 변환합니다. 예를 들어, np.dot(I_f, W_i)는 크기가 [n, d_i]인 행렬을 크기가 [d_i, d_e]인 행렬과 곱하여 크기가 [n, d_e]인 프로젝트된 행렬을 생성합니다.\n- 새로운 임베딩 벡터를 정규화합니다. 이렇게 하면 단위 벡터로 변환됩니다.\n- 도트 곱의 행렬을 계산합니다.\n- 각 행 및 열에 대한 교차 엔트로피 손실을 계산하고, 각 쌍이 두 번씩 계산되므로 2로 나눕니다.\n\n## 프롬프트 엔지니어링 및 앙상블링\n\n언어 모델의 부상 이후 프롬프트 엔지니어링은 생성 모델에서 좋은 출력을 얻기 위한 매우 흔한 실천법이 되었습니다. CLIP의 텍스트 인코더가 트랜스포머 모델인 만큼 제로샷 성능을 얻기 위해서 매우 중요하다는 점을 저자들이 발견했습니다. 저자들은 사진과 텍스트가 단어 하나로만 이루어진 경우인 경우가 그리 흔하지 않았다는 것을 발견했습니다. 예를 들어, \"개\"와 같이 클래스 레이블을 나타내는 경우입니다. 대신, 이미지와 함께 매칭되는 텍스트가 이미지의 캡션 또는 설명과 같이 전체 문장인 경우가 더 흔했습니다. 따라서 저자들은 \" 'object'의 사진\"이 좋은 기본 프롬프트이지만 특정 경우에는 더 전문화된 프롬프트가 더 잘 작동한다고 발견했습니다. 예를 들어, 위성 이미지의 경우 \" 'object'의 위성 사진\"이 잘 작동했습니다.\n\n<div class=\"content-ad\"></div>\n\n작가들은 다른 모델들을 앙상블하는 실험도 진행했습니다. 앙상블은 여러 다른 모델들의 예측을 결합하여 최종 출력을 얻는 것으로, 머신러닝에서 고분산 및 저편향(과적합) 모델의 문제를 해결하는 일반적인 기술입니다. CLIP의 경우, 작가들은 여러 다양한 프롬프트를 사용하여 분류기를 구성하여 앙상블을 만들었습니다.\n\n프롬프트 엔지니어링과 앙상블 모두 ImageNet에서 상당한 성능 향상을 보여주었습니다.\n\n## 한계\n\n논문은 더 많은 실험과 결과에 대해 논하고 있지만, CLIP가 완벽하지 않으며 다양한 한계가 있다는 점을 언급하는 것도 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n- 앞서 언급한 설계 결정으로 이 모델은 생성 모델이 아니며 이미지 캡셔닝을 할 수 없습니다.\n- 저자는 CLIP가 여전히 최신 기술 수준에서는 멀리 떨어져 있음을 언급하며 (위에 선형 레이어가 있는 ResNet과만 비교 가능함) 일부 작업에 대해 매우 안좋은 일반화 성능을 보입니다. 예를 들어, 쉬운 MNIST 손글씨 숫자 인식 데이터셋에서는 88% 정도만 달성합니다. Training 데이터에 유사한 이미지가 없기 때문일 가능성이 높지만, CLIP는 그런 측면에 대해 거의 다루지 않습니다.\n- CLIP는 인터넷에 있는 이미지와 텍스트의 쌍으로 훈련되었습니다. 이러한 이미지-텍스트 쌍들은 필터링되지 않고 정리되지 않았으며, CLIP 모델은 여러 사회적 편향을 학습하게 됩니다. (현재 LLM들과 유사한 우려사항이 있으며, RLFHF와 직접 선호도 최적화와 같은 기술이 이를 해결하려고 노력하고 있습니다.)\n- 트랜스포머 텍스트 인코더의 최대 시퀀스 길이(전달할 수 있는 토큰의 최대 수)는 원본 구현에서 76으로 제한되었는데, 이는 대부분 이미지와 일반적으로 짧은 문장인 캡션으로 이루어진 데이터셋 때문입니다. 따라서 오프더셸프 사전 훈련 모델은 긴 텍스트와 잘 작동하지 않을 것이며, 76개의 토큰 이후로 잘릴 것이므로, 짧은 텍스트로 훈련되었습니다. \n\n# 구현 세부 사항\n\n## HuggingFace Transformers를 사용한 추론\n\nHuggingFace Transformers 라이브러리를 사용하여 몇 줄의 코드로 자신의 컴퓨터에서 CLIP를 사용할 수 있습니다! 먼저 라이브러리를 가져와서 사전 훈련된 모델을 로드하세요.\n\n<div class=\"content-ad\"></div>\n\n```js\nimport transformers\n\nmodel = transformers.CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = transformers.CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n```\n\n그런 다음 캡션/설명 목록과 이미지 목록을 만듭니다. 이미지는 url 또는 PIL 이미지일 수 있습니다.\n\n```js\nimport PIL.Image\n\nimages = [PIL.Image(\"for_example_a_dog_image.jpeg\")]\npossible_classes = [\"새 이미지\", \"개 이미지\", \"고양이 이미지\"]\n```\n\n텍스트와 이미지을 토큰화하고 모델로 전달할 준비를 단계를 수행하는 processor를 호출합니다. 이는 일반적인 텍스트만 사용하는 경우에 토큰화 도구를 호출하는 것과 매우 유사합니다. 설명의 배치가 있으므로 모두 동일한 길이로 \"패딩\"하여 텐서로 저장하고 최대 시퀀스 길이(이전에 설명한대로 76)에서 긴 문장을 자르기 위해 자른다. 그런 다음 토큰화된 입력을 모델로 전달하고 텍스트 및 이미지 인코더를 통해 전파시킵니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\r\ntorch.no_grad()을 사용하여:\n\n    inputs = processor(text=descriptions, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n    outputs = model(**inputs)\r\n```\n\n이제 두 가지 다른 함수를 사용하여 내적 결과 행렬을 검색할 수 있습니다. logits_per_image를 사용하면 [이미지 수, 텍스트 수] 형태의 내적 결과 행렬을 얻을 수 있고, logits_per_text를 사용하면 [텍스트 수, 이미지 수] 형태의 행렬을 얻을 수 있습니다.\n\n```js\r\ndot_products_per_image = outputs.logits_per_image\ndot_products_per_text = outputs.logits_per_text\r\n```\n\n마지막으로, 각 이미지에 대한 확률 분포를 얻고 싶다면 softmax 함수를 통과시킬 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\nprobabilities = dot_products_per_image.softmax(dim=1)\r\n```\r\n\r\n## 구현 내부의 깊은 이해\r\n\r\ntransformers CLIP의 소스 코드는 깃허브에서 찾을 수 있습니다. 모듈식으로 잘 구현되어 있어서 좋았어요. 주요 모델은 CLIPModel 클래스에 구현되어 있으며, 아래에서 볼 수 있는 forward 메서드에서 주요 로직을 확인할 수 있어요.\r\n\r\n<img src=\"/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_3.png\" />\r\n\n\n<div class=\"content-ad\"></div>\n\n이미지를 통해 알 수 있듯이 비전 모델과 텍스트 모델은 임베딩과 레이어 노름에 약간의 차이가 있습니다.\n\n![이미지1](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_4.png)\n\n![이미지2](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_5.png)\n  \n그러나 두 모델 모두 동일한 CLIPEncoder를 공유합니다. 이 CLIPEncoder는 주요 트랜스포머 인코더이며 많은 하위 블록으로 구성된 CLIPEncoderLayer라고 부릅니다. 트랜스포머 아키텍처에서는 각 인코더와 디코더가 N번 쌓입니다. CLIP에서는 텍스트를 생성하지 않기 때문에 디코더를 사용하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![UnderstandingOpenAIsCLIPmodel_6](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_6.png)\n\nEach `CLIPEncoderLayer` is then comprised of the attention mechanism, a normalization layer, and a simple feedforward layer/multi-layer perceptron (MLP).\n\n![UnderstandingOpenAIsCLIPmodel_7](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_7.png)\n\nFinally, I went through and annotated the implementation for the multi-head attention mechanism in the following gist - enjoy!\n\n\n<div class=\"content-ad\"></div>\n\n# 추가 작업\n\n시작부터 언급한대로 CLIP는 다양한 방법으로 활용할 수 있으며 특히 의미 검색 유형의 응용 프로그램에서 유용하게 사용할 수 있습니다. 예를 들어, 이미지의 설명으로 검색하여 데이터베이스에서 이미지를 검색하는 데 CLIP를 사용할 수 있습니다.\n\nCLIP 및 대안들은 이후에 등장한 많은 다중 모달 모델들의 구성 요소이기도 합니다. 예를 들어 Flamingo의 Vision Language Model에서는 텍스트 및 이미지 시퀀스를 한꺼번에 사용하여 텍스트를 생성할 수 있습니다. Flamingo는 이미지를 텍스트와 동일한 임베딩 공간으로 변환하기 위해 Vision 인코더를 사용합니다.\n\n![image](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_8.png)\n\n<div class=\"content-ad\"></div>\n\n작가들은 CLIP와 비슷한 방식으로 훈련된 자체 버전을 실험했습니다.\n\n마지막으로 Google의 Gemini와 같은 모델은 우리가 잘 알지 못해도, 오디오와 비디오를 포함한 다양한 모달리티의 입력 데이터를 결합하는 유사한 접근법을 사용하고 있을 가능성이 높습니다!\n\n![이미지](/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_9.png)\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n요약하면, CLIP은 여러 응용 프로그램에 사용할 수 있는 텍스트와 임베딩 모델로, 다중 모달 AI 시스템을 구축하는 데 사용할 수 있습니다. 또한 Python에서 CPU 상에서 몇 줄의 코드로 쉽게 실행할 수도 있습니다.\n\n도움이 되었기를 바랍니다. 읽어 주셔서 감사합니다! 만약 즐겁게 보셨다면, 'Flamingo'에 관한 제 논문도 확인해보시는 것을 권해드립니다!","ogImage":{"url":"/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_0.png"},"coverImage":"/assets/img/2024-06-23-UnderstandingOpenAIsCLIPmodel_0.png","tag":["Tech"],"readingTime":9},{"title":"다양한 용도로 활용 가능한 GenAI 기반 챗봇 만들기 방법","description":"","date":"2024-06-23 20:05","slug":"2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot","content":"\n\n\n![이미지](/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_0.png)\n\n대형 언어 모델(LLM)은 엄청난 파워를 지니고 있으며, 질문 응답, 요약, entity 추출 등 다양한 NLP 작업을 해결하는 데 도움이 될 수 있습니다. 생성형 AI 사용 사례가 계속 확장됨에 따라 종종 현실 세계 응용프로그램에서는 이러한 NLP 작업 중 여러 가지를 해결할 수 있는 능력이 필요합니다. 예를 들어 사용자가 상호 작용할 수 있는 챗봇이 있다면, 대화 내용을 요약하는 것이 일반적인 요청이 될 수 있습니다. 이는 의사-환자 대화 기록, 가상 전화/예약 등 다양한 상황에서 활용될 수 있습니다.\n\n이러한 유형의 문제를 해결하는 방법은 무엇일까요? 질문 응답을 위한 하나, 요약을 위한 다른 하나의 LLM을 사용할 수 있습니다. 또 다른 접근 방식은 동일한 LLM을 다양한 도메인에 대해 파인 튜닝하는 것일 수 있지만, 이 경우엔 전자의 접근 방식에 초점을 맞출 것입니다. 그러나 여러 LLM을 사용하면 해결해야 할 특정한 도전 과제들이 있습니다.\n\n심지어 하나의 모델을 호스팅하는 것은 계산적으로 많은 비용이 소요되며, 대형 GPU 인스턴스가 필요합니다. 여러 LLM을 보유하면 모두에 대한 지속적인 엔드포인트/하드웨어가 필요할 것입니다. 이는 또한 여러 엔드포인트를 관리하고 인프라를 제공하기 위해 추가 비용이 발생한다는 것을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\nSageMaker 추론 컴포넌트를 사용하여이 문제를 해결할 수 있습니다. 추론 컴포넌트를 사용하면 단일 엔드포인트에 여러 가지 모델을 호스팅 할 수 있습니다. 각 모델은 고유 한 컨테이너를 가지고 있으며 모델별로 일정한 하드웨어를 할당하고 확장할 수 있습니다. 이를 통해 단일 엔드포인트 뒤에 두 개의 모델을 가지고 비용과 성능을 최적화 할 수 있습니다.\n\n오늘의 기사에서는 질문 응답과 요약이 가능한 일반적인 AI 기반 챗봇을 만드는 방법을 살펴보겠습니다. 여기서 사용할 도구 몇 가지를 간단히 살펴보겠습니다:\n\n- SageMaker 추론 컴포넌트: 모델을 호스팅하기 위해 SageMaker Real-Time 추론을 사용할 것입니다. Real-Time 추론 내에서 추론 컴포넌트 기능을 사용하여 여러 모델을 호스팅하고 각 모델에 대해 하드웨어를 할당할 것입니다. 추론 컴포넌트에 대해 처음이라면 이곳의 시작 기사를 참조해주세요.\n- Streamlit: Streamlit은 웹 개발을 간단하게하는 오픈 소스 Python 라이브러리입니다. Streamlit을 사용하여 질문 응답 및 요약을 위한 ChatBot UI를 구축할 것입니다. Streamlit을 처음 사용하는 경우 Heiko Hotz의 시작 기사를 참조하여 UI를 구축하는 데 템플릿으로 사용할 수 있습니다.\n- 모델\n- 질문 응답 모델: 챗봇의 질문 응답 부분에는 Llama7B Chat 모델을 사용할 것입니다. Llama7B Chat은 채팅 중심 대화에 최적화되어 있습니다. 사용할 모델 서버/컨테이너는 DJL Serving에서 제공되는 AWS Large Model 추론 (LMI) 컨테이너입니다. LMI 컨테이너를 사용하면 모델 파티션 및 배치 및 양자화와 같은 기타 최적화가 가능합니다. 기존 LMI Llama 7B Chat 배포 예제를 사용하여 모델 아티팩트를 빌드할 것입니다.\n- 요약 모델: 대화의 요약에 대해, Karthick Kaliannan Neelamohan (Apache 2.0 라이센스)에 의해 파인튜닝 된 HuggingFace Hub 모델을 사용할 것입니다. 기본 모델은 BART이며 이미 인기있는 SAMSUM 및 DIALOGSUM 데이터 세트에서 파인튜닝되었습니다. 자체 모델과 데이터가있는 경우 직접 파인튜닝도 가능합니다.\n\n이제 다룰 다양한 구성 요소를 이해했으니 예제로 바로 들어가 봅시다!\n\n<div class=\"content-ad\"></div>\n\n# 주의: 본 기사는 Python, LLMs 및 Amazon SageMaker 인퍼런스에 대한 중급 이해를 전제로 합니다. Amazon SageMaker 인퍼런스를 시작하는 데 도움이 될 것으로 보입니다.\n\n### 면책 조항: 저는 AWS의 머신러닝 아키텍트이며 제 의견은 제 개인적인 것입니다.\n\n## 목차\n\n- 설정 및 엔드포인트 생성\n- 인퍼런스 컴포넌트 배포\n  a. Llama7B 챗 인퍼런스 컴포넌트 생성\n  b. BART 요약 인퍼런스 컴포넌트 생성\n- Streamlit UI 생성 및 데모\n- 추가 자료 및 결론\n\n<div class=\"content-ad\"></div>\n\n# 1. 설정 및 엔드포인트 생성\n\n개발을 위해 SageMaker Studio에서 ml.c5.xlarge 인스턴스와 conda_python3 커널을 사용할 것입니다. SageMaker 엔드포인트 및 추론 구성 요소를 생성하기 위해 AWS Boto3 Python SDK 및 상위 수준 SageMaker Python SDK를 사용할 것입니다.\n\n```python\nimport boto3\nimport sagemaker\nimport time\nfrom time import gmtime, strftime\n\n# 설정\nclient = boto3.client(service_name=\"sagemaker\")\nruntime = boto3.client(service_name=\"sagemaker-runtime\")\nboto_session = boto3.session.Session()\ns3 = boto_session.resource('s3')\nregion = boto_session.region_name\nsagemaker_session = sagemaker.Session()\nbucket = sagemaker_session.default_bucket()\nrole = sagemaker.get_execution_role()\nprint(f\"Role ARN: {role}\")\nprint(f\"Region: {region}\")\n\n# 클라이언트 설정\ns3_client = boto3.client(\"s3\")\nsm_client = boto3.client(\"sagemaker\")\nsmr_client = boto3.client(\"sagemaker-runtime\")\n```\n\n추론 구성 요소를 만들기 전에 먼저 SageMaker 실시간 엔드포인트를 만들어야 합니다. 여기에서 인스턴스 유형, 수량 및 엔드포인트 수준에서 관리되는 AutoScaling을 활성화합니다.\n\n<div class=\"content-ad\"></div>\n\n위의 표를 마크다운 형식으로 변경해 주세요.\n\n우녕하세울나, 이것은 모델/추론 컴포넌트 수준에서 AutoScaling을 활성화하는 방법과는 다릅니다. 거기서는 각 추론 컴포넌트마다 AutoScaling 정책을 적용하여 모델 복사본 수를 조절할 수 있습니다. 각 모델 복사본은 추론 컴포넌트에 할당된 하드웨어 양을 유지합니다. 엔드포인트 수준에서 관리되는 AutoScaling을 설정하면 활성화한 추론 컴포넌트 수준의 스케일링을 처리할 충분한 컴퓨팅 자원이 있는지 확인해야 합니다. 엔드포인트 수준에서 자체 AutoScaling 정책을 정의할 수도 있지만, 여기에서 컴포넌트 및 엔드포인트 수준의 정책 간의 충돌 가능성에 유의해야 합니다.\n\n```js\n# Container Parameters, increase health check for LLMs: \nvariant_name = \"AllTraffic\"\ninstance_type = \"ml.g5.12xlarge\" # 하나의 인스턴스 당 4개의 GPU\nmodel_data_download_timeout_in_seconds = 3600\ncontainer_startup_health_check_timeout_in_seconds = 3600\n\n# 엔드포인트 수준에서 관리되는 AutoScaling 설정\ninitial_instance_count = 1\nmax_instance_count = 2\nprint(f\"초기 인스턴스 수: {initial_instance_count}\")\nprint(f\"최대 인스턴스 수: {max_instance_count}\")\n```\n\n또한 LLMs를 처리하기 위해 컨테이너 레벨 매개변수를 활성화하고 최소 미해결 요청(LOR) 라우팅을 활성화합니다.\n\n```js\n# 엔드포인트 구성 생성\nendpoint_config_response = client.create_endpoint_config(\n    EndpointConfigName=epc_name,\n    ExecutionRoleArn=role,\n    ProductionVariants=[\n        {\n            \"VariantName\": variant_name,\n            \"InstanceType\": instance_type,\n            \"InitialInstanceCount\": 1,\n            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n            \"ManagedInstanceScaling\": {\n                \"Status\": \"ENABLED\",\n                \"MinInstanceCount\": initial_instance_count,\n                \"MaxInstanceCount\": max_instance_count,\n            },\n            # 가장 적은 미해결 또는 임의로 설정할 수 있음: https://aws.amazon.com/blogs/machine-learning/minimize-real-time-inference-latency-by-using-amazon-sagemaker-routing-strategies/\n            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n        }\n    ],\n)\n\n# 엔드포인트 생성\nendpoint_name = \"ic-ep-chatbot\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\ncreate_endpoint_response = client.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=epc_name,\n)\nprint(\"엔드포인트 Arn: \" + create_endpoint_response[\"EndpointArn\"])\n```\n\n<div class=\"content-ad\"></div>\n\n내부에서 우리의 엔드포인트를 만든 후에는 두 가지 다른 모델을 나타내는 두 가지 추론 컴포넌트를 추가할 수 있습니다.\n\n## 2. 추론 컴포넌트 배포\n\n추론 컴포넌트는 단일 모델 컨테이너를 나타냅니다. 일반적으로 추론 컴포넌트를 생성하려면 SageMaker 모델 객체를 참조하고 모델 데이터 및 컨테이너 정보를 상속받을 수 있습니다. 이 정보 외에 할당하는 컴퓨팅 및 모델의 복사본 수를 추가할 수 있습니다. 각 복사본은 초기에 할당한 추론 컴포넌트에 지정된 것과 동일한 컴퓨팅을 가질 것입니다.\n\n### a. Llama7B 채팅 추론 컴포넌트 생성\n\n<div class=\"content-ad\"></div>\n\n\"Llama7B Chat\"에는 DJL Serving에서 제공되는 LMI 컨테이너를 사용할 예정입니다. LMI 컨테이너를 사용하면 serving.properties 파일을 지정하여 작업 중인 모델과 일괄 처리 및 양자화와 같은 기타 최적화를 지정할 수 있습니다.\n\n```js\nengine=MPI\noption.model_id=TheBloke/Llama-2-7B-Chat-fp16\noption.task=text-generation\noption.trust_remote_code=true\noption.tensor_parallel_degree=1\noption.max_rolling_batch_size=32\noption.rolling_batch=lmi-dist\noption.dtype=fp16\n```\n\n\"model_id\" 매개변수는 작업 중인 모델을 지정합니다. 이 경우, 모델 가중치는 HuggingFace Model ID에서 지정된 것을 가져옵니다. 사용자 지정으로 사전 조정된 모델이있는 경우 해당 모델 가중치의 S3 경로를 지정할 수 있습니다. 이 serving 파일과 함께 사용자 지정 사전/후 처리 또는 사용자 고유의 모델 로딩 로직이 있는 경우 추론 스크립트를 지정할 수 있습니다. 이를 구성한 후에는 SageMaker 모델 개체에 대해 예상대로 model.tar.gz를 만들고 사용 중인 컨테이너(관리되는 컨테이너 또는 자체 컨테이너)를 지정합니다.\n\n```js\n%%sh\n# tarball 생성\nmkdir mymodel\nrm mymodel.tar.gz\nmv serving.properties mymodel/\nmv model.py mymodel/\ntar czvf mymodel.tar.gz mymodel/\nrm -rf mymodel\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n# 이미지 가져오기\nimage_uri = sagemaker.image_uris.retrieve(\n        framework=\"djl-deepspeed\",\n        region=sagemaker_session.boto_session.region_name,\n        version=\"0.26.0\"\n    )\nprint(f\"사용 중인 이미지: {image_uri}\")\n\n# SageMaker 모델 객체 생성\nfrom sagemaker.utils import name_from_base\nllama_model_name = name_from_base(f\"Llama-7b-chat\")\nprint(llama_model_name)\n\ncreate_model_response = sm_client.create_model(\n    ModelName=llama_model_name,\n    ExecutionRoleArn=role,\n    PrimaryContainer={\"Image\": image_uri, \"ModelDataUrl\": code_artifact},\n)\nmodel_arn = create_model_response[\"ModelArn\"]\n\nprint(f\"모델 생성됨: {model_arn}\")\n```\n\n추론 구성 요소에서는 SageMaker 모델 객체에서 이 메타데이터를 상속할 수 있습니다. 이 외에도 컴퓨팅 요구 사항과 복사본 수를 지정합니다. Llama7B Chat의 경우 한 복사본 당 하나의 GPU를 지정합니다.\n\n```js\nllama7b_ic_name = \"llama7b-chat-ic\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nvariant_name = \"AllTraffic\"\n\n# llama 추론 컴포넌트 반응\ncreate_llama_ic_response = sm_client.create_inference_component(\n    InferenceComponentName=llama7b_ic_name,\n    EndpointName=endpoint_name,\n    VariantName=variant_name,\n    Specification={\n        \"ModelName\": llama_model_name,\n        \"ComputeResourceRequirements\": {\n            # llama 7b 채팅에는 하나의 GPU가 필요합니다\n            \"NumberOfAcceleratorDevicesRequired\": 1,\n            \"NumberOfCpuCoresRequired\": 1,\n            \"MinMemoryRequiredInMb\": 1024,\n        },\n    },\n    # 복사본에 대한 자동 스케일링 설정 가능, 각 복사본은 할당한 하드웨어를 유지합니다\n    RuntimeConfig={\"CopyCount\": 1},\n)\n\nprint(\"IC Llama Arn: \" + create_llama_ic_response[\"InferenceComponentArn\"])\n```\n\n추론 컴포넌트를 생성한 후 샘플 추론을 실행하여 추론 컴포넌트 이름을 헤더로 지정할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\nimport json\r\ncontent_type = \"application/json\"\r\nchat = [\r\n  {\"role\": \"user\", \"content\": \"안녕하세요, 어떻게 지내세요?\"},\r\n  {\"role\": \"assistant\", \"content\": \"저는 잘 지내고 있어요. 오늘 어떻게 도와드릴까요?\"},\r\n  {\"role\": \"user\", \"content\": \"저는 기계 학습에 대해 더 배우고 싶은 소프트웨어 엔지니어입니다.\"},\r\n]\r\n\r\npayload = {\"chat\": chat, \"parameters\": {\"max_tokens\":256, \"do_sample\": True}\r\nresponse = smr_client.invoke_endpoint(\r\n    EndpointName=endpoint_name,\r\n    InferenceComponentName=llama7b_ic_name, # IC 이름 지정\r\n    ContentType=content_type,\r\n    Body=json.dumps(payload),\r\n)\r\nresult = json.loads(response['Body'].read().decode())\r\nprint(type(result['content']))\r\nprint(type(result))\r\n```\r\n\r\n## b. BART Summarization 추론 컴포넌트 생성\r\n\r\nBART 추론 컴포넌트의 생성은 Llama7B 채팅 컴포넌트와 매우 유사합니다. 주요 차이점은 사용하는 컨테이너가 다르기 때문에 모델 데이터와 이미지 URI의 패키지화 방법이 다를 것입니다. 이 경우에는 HuggingFace PyTorch 이미지를 사용하고 HuggingFace 모델 ID와 해결하려는 NLP 작업을 지정합니다.\r\n\r\n```js\r\nfrom sagemaker.utils import name_from_base\r\n\r\nbart_model_name = name_from_base(f\"bart-summarization\")\r\nprint(bart_model_name)\r\n\r\n# 필요한 경우 귀하의 지역으로 교체\r\nhf_transformers_image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-cpu-py39-ubuntu20.04'\r\n\r\n# 환경 변수\r\nenv = {'HF_MODEL_ID': 'knkarthick/MEETING_SUMMARY',\r\n      'HF_TASK':'summarization',\r\n      'SAGEMAKER_CONTAINER_LOG_LEVEL':'20',\r\n      'SAGEMAKER_REGION':'us-east-1'}\r\n\r\ncreate_model_response = sm_client.create_model(\r\n    ModelName=bart_model_name,\r\n    ExecutionRoleArn=role,\r\n    # 이 경우 HF Hub로 직접 모델 데이터를 가리키는 데이터 포인트 없음\r\n    PrimaryContainer={\"Image\": hf_transformers_image_uri, \r\n                      \"Environment\": env},\r\n)\r\nmodel_arn = create_model_response[\"ModelArn\"]\r\nprint(f\"생성된 모델: {model_arn}\")\r\n```\n\n<div class=\"content-ad\"></div>\n\n한 번 더 SageMaker Model 객체를 Inference 구성 요소에 전달하고 모델에 필요한 하드웨어를 지정합니다.\n\n```js\nbart_ic_name = \"bart-summarization-ic\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nvariant_name = \"AllTraffic\"\n\n# BART inference component reaction\ncreate_bart_ic_response = sm_client.create_inference_component(\n    InferenceComponentName=bart_ic_name,\n    EndpointName=endpoint_name,\n    VariantName=variant_name,\n    Specification={\n        \"ModelName\": bart_model_name,\n        \"ComputeResourceRequirements\": {\n            # will reserve one GPU\n            \"NumberOfAcceleratorDevicesRequired\": 1,\n            \"NumberOfCpuCoresRequired\": 8,\n            \"MinMemoryRequiredInMb\": 1024,\n        },\n    },\n    # can setup autoscaling for copies, each copy will retain the hardware you have allocated\n    RuntimeConfig={\"CopyCount\": 1},\n)\n\nprint(\"IC BART Arn: \" + create_bart_ic_response[\"InferenceComponentArn\"])\r\n```\n\nInference 구성 요소가 모두 생성되면 SageMaker Studio UI에서 시각화할 수 있습니다:\n\n<img src=\"/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_1.png\" />\n\n<div class=\"content-ad\"></div>\n\n# 3. Streamlit UI 생성 및 데모\n\n이제 SageMaker 엔드포인트와 추론 컴포넌트를 생성했으니, 이 모든 것을 Streamlit 애플리케이션에서 함께 사용할 수 있습니다. 나중에 호출할 기준으로 환경 변수를 설정해줍니다.\n\n```js\nimport json\nimport os\nimport streamlit as st\nfrom streamlit_chat import message\nimport boto3\n\nsmr_client = boto3.client(\"sagemaker-runtime\")\nos.environ[\"endpoint_name\"] = \"여기에 엔드포인트 이름 입력\"\nos.environ[\"llama_ic_name\"] = \"여기에 llama IC 이름 입력\"\nos.environ[\"bart_ic_name\"] = \"여기에 bart IC 이름 입력\"\n```\n\n또한 사용자 입력, 모델 출력 및 채팅 대화를 유지하기 위해 Streamlit 세션 상태 변수를 설정합니다. 대화를 지우는 클리어 버튼을 만들어 이 버튼을 클릭하면 우리가 정의한 상태 변수를 재설정합니다.\n\n<div class=\"content-ad\"></div>\n\n```python\n# 세션 상태 변수에 사용자 및 모델 입력을 저장합니다.\nif 'generated' not in st.session_state:\n    st.session_state['generated'] = []\nif 'past' not in st.session_state:\n    st.session_state['past'] = []\nif 'chat_history' not in st.session_state:\n    st.session_state['chat_history'] = []\n\n# 클리어 버튼\nclear_button = st.sidebar.button(\"대화 지우기\", key=\"clear\")\n# 클릭 시 모든 것 초기화\nif clear_button:\n    st.session_state['generated'] = []\n    st.session_state['past'] = []\n    st.session_state['chat_history'] = []\n```\n\n사용자 입력을 받기 위한 제출 버튼을 생성하고, 이 버튼을 클릭하면 Llama7B 채팅 모델이 호출됩니다.\n\n```python\nif submit_button and user_input:\n    st.session_state['past'].append(user_input)\n    model_input = {\"role\": \"user\", \"content\": user_input}\n    st.session_state['chat_history'].append(model_input)\n    payload = {\"chat\": st.session_state['chat_history'], \"parameters\": {\"max_tokens\":400, \"do_sample\": True,\n                                                                        \"maxOutputTokens\": 2000}\n    # Llama 호출\n    response = smr_client.invoke_endpoint(\n        EndpointName=os.environ.get(\"endpoint_name\"),\n        InferenceComponentName=os.environ.get(\"llama_ic_name\"), # IC 이름 지정\n        ContentType=\"application/json\",\n        Body=json.dumps(payload),\n    )\n    full_output = json.loads(response['Body'].read().decode())\n    print(full_output)\n    display_output = full_output['content']\n    print(display_output)\n    st.session_state['chat_history'].append(full_output)\n    st.session_state['generated'].append(display_output)\n```\n\n다음 명령으로 앱을 시작하면 UI를 볼 수 있으며 작동 중인 질문 응답 채팅 모델을 확인할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\r\nstreamlit run app.py\r\n```\n\n<img src=\"/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_2.png\" />\n\n사이드에는 요약 버튼도 만들어 봅시다:\n\n```js\r\nsummarize_button = st.sidebar.button(\"대화 요약\", key=\"summarize\")\r\n```\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_3.png\" />\n\n요약 후에 미세 조정된 BART 모델을 호출합니다. BART 모델의 경우 입력이 모델이 이해할 수 있는 형식으로 구조화되어야 합니다. 다음과 유사한 형식으로 구조화된 입력을 원합니다:\n\n<img src=\"/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_4.png\" />\n\n\"chat_history\" 상태 변수에 입력 및 출력을 모두 캡처하여 모델에 맞게 형식화하고 BART Inference Component를 호출합니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\n# 요약을 위한\nif summarize_button:\n    st.header(\"Summary\")\n    st.write(\"요약 생성 중....\")\n    chat_history = st.session_state['chat_history']\n    text = ''''''\n    for resp in chat_history:\n        if resp['role'] == \"user\":\n            text += f\"Ram: {resp['content]}\\n\"\n        elif resp['role'] == \"assistant\":\n            text += f\"AI: {resp['content']}\\n\"\n    summary_payload = {\"inputs\": text}\n    summary_response = smr_client.invoke_endpoint(\n        EndpointName=os.environ.get(\"endpoint_name\"),\n        InferenceComponentName=os.environ.get(\"bart_ic_name\"),  # IC 이름 지정\n        ContentType=\"application/json\",\n        Body=json.dumps(summary_payload),\n    )\n    summary_result = json.loads(summary_response['Body'].read().decode())\n    summary = summary_result[0]['summary_text']\n    st.write(summary)\n```\n\n![이미지](/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_5.png)\n\n# 4. 추가 자료 및 결론\n\n전체 예제 코드는 위 링크에서 찾을 수 있습니다. 다중 LLM(Large Language Model)을 실제 사례에 대해 비용 효율적이고 성능 효율적으로 활용하는 방법을 보여주는 좋은 예제였기를 희망합니다.\n\n\n<div class=\"content-ad\"></div>\n\n위에서 논의한 주제와 그 외에도 더 다뤄볼 GenAI/AWS 기사와 심층적인 내용을 기대해주세요. 항상 읽어주셔서 감사합니다. 의견이 있으시면 언제든지 남겨주세요. \n\n이 기사가 마음에 드셨다면 LinkedIn에서 저와 연결하고 제 Medium 뉴스레터를 구독해보세요.","ogImage":{"url":"/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_0.png"},"coverImage":"/assets/img/2024-06-23-BuildingaMulti-PurposeGenAIPoweredChatbot_0.png","tag":["Tech"],"readingTime":16}],"page":"5","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true}