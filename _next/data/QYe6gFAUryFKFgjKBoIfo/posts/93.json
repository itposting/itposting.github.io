{"pageProps":{"posts":[{"title":"미세 조정에 대한 깊이 있는 탐구","description":"","date":"2024-06-19 03:55","slug":"2024-06-19-ADeepDiveintoFine-Tuning","content":"\n\n## \"컴포트 존\"을 벗어나는 것 - LLM에 대한 도메인 적응 접근 방식에 대한 심층 탐구 3/3\n\n![이미지](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_0.png)\n\n특정 도메인이나 사용 사례에 대한 대형 언어 모델 (LLMs)의 도메인 적응을 탐색하고 계십니까? 이 3부작 블로그 시리즈에서는 도메인 적응의 동기에 대해 설명하고 이를 수행하는 다양한 옵션에 대해 깊이 파고들었습니다. 또한 인기있는 트레이드오프를 다루는 도메인 적응 여정을 마스터할 수 있는 상세 안내서가 제공됩니다.\n\n부분 1: 도메인 적응 소개 - 동기, 옵션, 트레이드오프\n부분 2: 컨텍스트 학습에 대한 심층 탐구\n부분 3: 파인튜닝에 대한 심층 탐구 - 여기 있습니다!\n\n<div class=\"content-ad\"></div>\n\n참고: 모든 이미지는 별도로 언급되지 않는 한 저자가 제공했습니다.\n\n# 복습\n\n이 블로그 시리즈의 이전 부분에서는 인컨텍스트 학습의 개념을 탐색하여 대형 언어 모델(Large Language Models, LLMs)의 \"쾌적 영역\" 제한을 극복하는 강력한 방법으로 다뤘습니다. 이러한 기술이 어떻게 사용될 수 있는지, 작업을 변환하고 모델의 전문 분야로 이동하여 성능 향상 및 Helpfulness, Honesty, Harmlessness와 같은 주요 설계 원칙과의 일치를 도모하는 방법에 대해 논의했습니다. 이번 세 번째 부분에서는 두 번째 도메인 적응 접근 방법인 파인튜닝으로 주목을 기울일 것입니다. 파인튜닝의 세부 사항을 탐구하면서, 모델의 \"쾌적 영역\"을 확장하고 특정 도메인 및 작업에 적응시켜 성능을 향상시킬 수 있는 방법에 대해 알아볼 것입니다. 프롬프트 엔지니어링과 파인튜닝 사이의 트레이드오프에 대해 논의하고, 데이터 속도, 작업 모호성 및 기타 고려 사항과 같은 요소에 기반하여 언제 어느 접근 방식을 선택해야 하는지에 대한 안내를 제공할 것입니다.\n\n# 트랜스포머 101\n\n<div class=\"content-ad\"></div>\n\n최첨단 LLM은 대부분 트랜스포머 아키텍처에 기반을 두고 있어요. 이 아키텍처는 2017년 Vaswani 등이 제안한 이후, 자연어 처리 분야를 혁명시킨 깊은 신경망 아키텍처의 가족입니다. 이 아키텍처 패밀리의 핵심 차별점은 콘텍스트에서 사용된 단어나 자연어의 더 큰 조각의 의미를 포착하는 데 뛰어난 \"어텐션\" 개념입니다.\n\n트랜스포머 아키텍처는 두 가지 근본적으로 다른 구성 요소로 구성되어 있어요. 한쪽에는 \"인코더\" 블록이 있어 자연어의 의미를 이른바 문맥화된 임베딩으로 번역하는 데 집중합니다. 이는 벡터 공간에서의 수학적 표현입니다. 인코더 모델은 이러한 벡터 표현을 하위 결정론적 또는 확률적 작업인 분류 문제, NER 또는 의미 검색과 같은 곳에서 활용하는 데 특히 유용합니다. 반면, 디코더 블록은 다음 토큰 예측에 훈련되어 있어, 재귀적으로 사용될 경우 텍스트를 생성할 수 있는 능력을 가지고 있습니다. 이 블록은 텍스트 생성에 의존하는 모든 작업에 사용될 수 있어요. 이러한 구성 요소는 서로 독립적으로 사용할 수 있지만, 조합해서 사용할 수도 있습니다. 오늘날 생성적 AI 분야에서 언급되는 대부분의 모델이 디코더 전용 모델입니다. 그래서 이 블로그 글은 이 유형의 모델에 초점을 맞출 거예요.\n\n\n![ADeepDiveintoFine-Tuning](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_1.png)\n\n\n# E2E 파인 튜닝 파이프라인\n\n<div class=\"content-ad\"></div>\n\nFine-tuning은 LLaMA2와 같은 기본 모델에 특정 분야 전문성을 효율적으로 주입하기 위해 전이 학습을 활용합니다. 이 과정은 도메인별 데이터에 대한 모델의 가중치를 업데이트하면서 전체 네트워크 구조를 변경하지 않는 방식으로 이루어집니다. 대규모 데이터셋과 컴퓨팅 파워가 필요한 전체 사전 학습과는 달리, fine-tuning은 매우 샘플 및 컴퓨팅 효율적입니다. 전체적으로, 이 과정은 다음 단계로 분해될 수 있습니다:\n\n![Fine-Tuning Phases](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_2.png)\n\n- 데이터 수집과 선택: 모델에 투입될 전용 데이터 집합을 신중하게 선택해야 합니다. 더불어, fine-tuning 목적에 따라 데이터가 아직 사용 가능하지 않을 수 있으며 목적에 맞게 수집되어야 합니다. 데이터의 양적 또는 질적 특성이 다른 경우도 있습니다. 또한 데이터 품질과 함께 데이터 소스, 기밀성 및 지적 재산권, 라이선스, 저작권, 개인 식별 정보 등의 측면을 고려해야 합니다.\n\nLLM 사전 학습은 일반적으로 웹 스크랩 및 정돈된 말뭉치를 활용합니다. fine-tuning은 도메인 적응 방식으로 데이터셋이 주로 조직, 지식 또는 작업 특정 도메인의 레이블 또는 미레이블 데이터로 구성되어 있다는 것을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_3.png\" />\n\n이 데이터는 다양한 곳에서 수집될 수 있습니다(문서 저장소, 사람이 작성한 콘텐츠 등). 미세 조정을 위해서는 품질에 대해 신중하게 선택하는 것이 중요하지만 위에서 언급한 것처럼 기밀 유지 및 지적 재산권(IP), 라이센스, 저작권, 개인 식별 정보(PII) 등과 같은 주제를 고려해야 합니다.\n\n<img src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_4.png\" />\n\n또 다른 중요한 차원은 훈련 데이터 집합을 라벨이 지정된(labeled) 및 지정되지 않은(unlabeled) 부분(선호도 포함)으로 분류하는 것입니다. 도메인 적응 미세 조정은 라벨이 지정되지 않은 텍스트 데이터가 필요합니다(그림 4를 참조하십시오). 다시 말해, 관련 콘텐츠와 충분한 품질로 간주되는 어떤 자연어의 전문 문서를 사용할 수 있습니다. 실제 사용 사례에 따라 사용자 매뉴얼, 내부 문서 또는 심지어 법적 계약서 등이 될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n한편, 명시적으로 레이블이 지정된 데이터셋인 지시-맥락-응답 데이터셋과 같은 것들은 지도 미세 조정 방법에 사용될 수 있습니다. 최근에는 모델을 실제 사용자 피드백에 맞추기 위한 강화 학습 방법이 큰 성과를 보여주며, 이는 인간 또는 기계가 생성한 선호 데이터를 활용합니다. 예를 들어, 이진 인간 피드백(좋아요/싫어요)이나 다중 응답 순위와 같은 것들이 있습니다.\n\n레이블이 지정되지 않은 데이터와는 달리, 레이블이 지정된 데이터셋은 특히 규모와 충분한 도메인 전문 지식을 갖추기 위해서 수집하기 어렵고 비용이 많이 듭니다. HuggingFace Datasets와 같은 오픈 소스 데이터 허브는 레이블이 지정된 데이터셋에 대한 좋은 소스일 수 있습니다, 특히 적절한 인간 인구 그룹의 광범위한 부분이 동의한 지역에서(예: 레드팀에 대한 독성 데이터셋)이고, 오픈 소스 데이터셋을 사용하여 모델의 실제 사용자 선호도를 대변할 수 있습니다.\n\n그럼에도 불구하고, 많은 사용 사례는 보다 구체적이며 오픈 소스 프록시 데이터셋만으로 충분하지 않습니다. 실제 인간들이 레이블을 지정한 데이터셋이 필요한 경우도 있습니다. Amazon SageMaker Ground Truth와 같은 도구를 사용하면, 완전히 관리되는 사용자 인터페이스 및 워크플로 또는 전체 직원을 제공함으로써 데이터 수집을 돕을 수 있습니다.\n\n최근에는 합성 데이터 수집이 미세 조정 분야에서 점점 더 중요해지고 있습니다. 강력한 LLM(Large Language Model)을 사용하여 레이블이 지정된 데이터셋을 합성적으로 생성하는 것이 실제로 이루어지고 있습니다. 이는 SFT 또는 선호 정렬을 위한 것일 수 있습니다. 이 방법은 이미 유망한 결과를 보여주었지만, 현재로선 추가적인 연구가 더 필요하며, 실무에서 규모에 맞게 유용함을 입증해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n- 데이터 전처리: 선택한 데이터는 하류 학습 알고리즘에게 \"잘 소화될 수 있도록\" 전처리 되어야 합니다. 인기 있는 전처리 단계는 다음과 같습니다:\n  - 품질 관련 전처리, 예를 들어 포맷 지정, 중복 제거, PII 필터링\n  - 섬세한 조정 접근 방식 관련 전처리: 예를 들어 감독형 섬세 조정을 위한 프롬프트 템플릿으로 랜더링\n  - NLP 관련 전처리, 예를 들어 토큰화, 임베딩, 쪼개기 (문맥 창에 따라)\n- 모델 학습: 선택한 섬세 조정 방식에 따라 딥 뉴럴 네트워크를 학습합니다. 아래에서 자세히 논의할 인기 있는 섬세 조정 방식은 다음과 같습니다:\n  - 계속된 사전 훈련, 즉 도메인 적응 섬세 조정: 풀 텍스트 데이터에 대한 훈련, 다음 토큰 예측 작업과 연동된 정렬\n  - 감독형 섬세 조정: 레이블된 데이터를 활용한 섬세 조정 접근 방식, 타깃 레이블로 정렬\n  - 선호도 정렬 접근 방식: 선호 데이터를 활용한 섬세 조정 접근 방식, 모델/시스템의 실제 사용자가 정의한 원하는 동작과 일치시킴\n\n이어서 단계별로 자세히 살펴보겠습니다. 학습 접근 방식과 다양한 섬세 조정 접근 방식 소개부터 데이터셋 및 데이터 처리 요구사항으로 이동하기 전에 시작해보겠습니다.\n\n# 학습\n\n이 섹션에서는 디코더 트랜스포머 모델을 학습하는 방법을 탐구합니다. 이는 사전 학습과 섬세 조정 모두에 적용됩니다.\n레이블이 지정되지 않은 데이터로 비지도 학습이나 레이블이 지정된 데이터로 지도 학습과 같은 전통적인 ML 학습 방식과 달리, 트랜스포머 모델의 학습은 자가지도 학습이라고 불리는 혼합 접근 방식을 활용합니다. 이는 레이블이 지정되지 않은 텍스트 데이터가 공급되더라도, 알고리즘이 실제로는 특정 입력 토큰을 가림으로써 내재적으로 자기 감독하고 있기 때문입니다. 아래의 입력 토큰 시퀀스 \"Berlin is the capital of Germany.\"을 고려하면, 이는 y가 가려진 토큰이고 X가 나머지인 지도 학습 예제로 이어집니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_5.png\" />\n\n위에서 언급한 자가 지도 학습 접근 방식은 모델 가중치를 언어 모델링(LM) 특정 손실 함수로 최적화합니다. 인코더 모델 훈련은 무작위로 토큰을 마스킹하여 양방향 컨텍스트를 활용하는 마스킹 언어 모델링(MLM)을 사용하며, 디코더 전용 모델은 시퀀스의 가장 오른쪽 토큰을 항상 마스킹하여 단방향 컨텍스트를 가지는 인과 언어 모델링(CLM) 방식으로 매핑됩니다. 간단히 말하면, 이는 이전의 의미적 문맥을 바탕으로 이후 토큰을 예측하는 방식으로 자가 회귀적으로 훈련된다는 것을 의미합니다. 이외에도 순열 언어 모델링(PLM)과 같은 다른 LM 접근 방식이 있으며, 해당 방식은 모델을 사용하여 무작위로 섞인 토큰 시퀀스를 정렬된 순서로 되돌리도록 조건부화하는 것입니다.\n\n<img src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_6.png\" />\n\nCLM 작업을 대리로 사용하여 예측과 실제 값이 생성되며, 이를 사용하여 예측 손실을 계산할 수 있습니다. 따라서 모델 어휘 전체의 토큰에 대한 예측 확률 분포는 그라운드 트루스인 실제 값과 비교되며, 실제 값을 나타내는 토큰에 대한 확률이 1.0인 희소 벡터입니다. 사용한 실제 손실 함수는 특정 모델 아키텍처에 따라 다르지만, 토큰 예측과 같은 범주형 문제 공간에서 잘 작동하는 교차 엔트로피나 헷갈림 손실과 같은 손실 함수가 일반적으로 사용됩니다. 손실 함수는 깊은 신경망 역전파에 의한 경사 하강을 수행하여 매 반복할 때마다 손실을 점진적으로 최소화하고 모델 가중치를 우리의 교육 목표로 최적화하는 데 활용됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 세밀한 조정 변형 - 시나리오\n\n이론에는 식상하니, 실전으로 넘어가 봐요. 생물기술 분야의 기관으로 가정해보겠습니다. COVID-19 백신 연구를 중심으로 다양한 NLP 사용 사례를 위한 기반 모델로 LLM을 활용하려고 하는 중입니다. 불행하게도 이 도메인은 표준 제공형 사전 훈련된 LLM의 \"편안한 영역\"에 포함되지 않아 예상 수준 이하의 성능을 낸다고 하네요. 다음 섹션에서 우리가 상상한 시나리오에서 LLaMA2의 성능을 기대 이상으로 끌어올리는 데 도움을 줄 수 있는 다양한 세밀한 조정 방법에 대해 논의해볼 거예요.\n\n# 세밀한 조정 변형 - 계속된 사전 훈련인 도메인 적응 세밀한 조정\n\n제목이 나타내는 대로, 이 분야가 \"계속된 사전 훈련\"이라는 용어로 수렴하려고 하는 가운데, 이 섹션에서 논의되는 세밀한 조정 방법에 대한 명확한 용어는 아직 커뮤니티에서 합의되지 않은 상태입니다. 그렇다면 이 세밀한 조정 방법이 정확히 어떤 것을 의미하는 걸까요?\n\n<div class=\"content-ad\"></div>\n\n바이오테크 분야의 연구 논문들은 글쓰기 스타일이 꽤 독특하며, 도메인 특화 지식과 산업 또는 기관 별 약어가 가득합니다 (예: Polack et al, 2020; Figure 7 참조).\n\n![Figure 7](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_7.png)\n\n반면에, 메타 LLaMA 모델의 사전 훈련 데이터 혼합물의 자세한 조사(Touvron et al., 2023; Figure 8)와 TII Falcon 모델 패밀리(Almazrouei et al., 2023; Figure 9)는 2.5% 및 2%만큼 일반용도의 LLMs가 연구 또는 심지어 바이오테크 도메인의 데이터를 매우 적게 포함하고 있음을 나타냅니다 (LLaMA 3 패밀리의 사전 훈련 데이터 혼합물은 블로그 게시 시점에 공개되지 않았습니다).\n\n![Figure 8](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_8.png)\n\n<div class=\"content-ad\"></div>\n\n![2024-06-19-ADeepDiveintoFine-Tuning_9.png](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_9.png)\n\n따라서, 이와 같은 차이를 줄이기 위해 파인 튜닝을 활용하여 모델의 \"편안 영역\"을 확대하여 구체적인 작업에 대한 성능을 향상시켜야 합니다. 지속적인 사전 훈련은 정확히 위에서 언급한 차원에서 뛰어난 성과를 보여줍니다. 이것은 특정 데이터 집합에 대해 미리 훈련된 LLM의 매개변수 지식에 모델의 반응을 조정하기 위해 도메인별 정보 (도메인별 언어, 약어 등) 또는 원시 전문 텍스트에 암시적으로 포함된 정보와 같은 도메인별 정보를 주입하기 위한 기술로, 평문 텍스트 데이터로 구성된 특정 데이터 집합에 대해 미리 훈련된 LLM을 조정하는 과정을 포함합니다. 이 접근 방식에서 사전 훈련된 디코더 모델은 레이블이 없는 텍스트 데이터를 사용하여 다음 토큰 예측을 위해 파인 튜닝됩니다. 이로써 미지의 텍스트 데이터를 사용한 지속적인 사전 훈련은 사전 훈련에 가장 유사한 파인 튜닝 접근 방식이 됩니다.\n\n우리의 예시에서는, 언급된 논문의 내용과 유사한 분야의 관련 문헌을 결합하여 연결된 텍스트 파일로 변환할 수 있습니다. 튜닝 목표 및 기타 요구 사항에 따라 필요없는 콘텐츠(예: 저자, 목차 등)의 제거, 중복 데이터 제거, 또는 PII 감소와 같은 데이터 정리 단계를 적용할 수 있습니다. 마지막으로, 데이터 집합은 모델 훈련에 사용되기 전에 일부 NLP-특정 전 처리(예: 토큰 분리, 컨텍스트 창에 따른 청킹 등 -위 참조-)을 거칩니다. 훈련 자체는 이전 장에서 논의된 것과 같이 전통적인 CLM 기반 훈련입니다. 바이오테크 도메인의 연구 논문 집합에 계속된 사전 훈련을 통해 LLaMA2를 조정한 후, 이제 해당 도메인에서 이를 \"BioLLaMA2\" 텍스트 완성 모델로 활용할 수 있습니다.\n\n# 파인 튜닝 변형 — 감독된 파인 튜닝 (SFT)\n\n<div class=\"content-ad\"></div>\n\n안타깝게도, 우리 사람들은 해결하고 싶은 문제를 순수한 텍스트 완성/토큰 예측 형태로 제시하는 것을 선호하지 않아요. 대화를 선호하며, 특히 일을 처리하고자 할 때에는 수다스럽거나 지시적인 행동을 하는 경향이 있는 사람들이랍니다.\n\n그래서 모델이 간단한 다음 토큰 예측을 넘어서도 미래를 예측할 때에는 약간의 세련된 요소가 필요해요. 여기서 감독된 세밀 조정 방식이 등장해요. 감독된 세밀 조정(SFT)은 사전 훈련된 LLM을 특정 데이터셋과 레이블이 지정된 예제들과 일치시키는 과정을 말해요. 이 기술은 모델의 응답을 특정 도메인이나 작업(예: 상기한 대화적 성격 또는 지시 따름)에 맞게 조정하는 데 중요하답니다. 대상 응용 프로그램을 잘 대표하는 데이터셋으로 학습함으로써, SFT는 LLM이 심층적인 이해를 개발하고 전문적인 요구 사항과 행동에 부합하는 더 정확한 출력물을 생산할 수 있게 해줘요.\n\n상기한 것들 외에도, 좋은 SFT의 예시는 Q&A 모델의 훈련, 개체 인식과 같은 데이터 추출 작업, 또는 해로운 응답을 방지하는 레드팀 활동 등이 있을 수 있어요.\n\n![이미지](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_10.png)\n\n<div class=\"content-ad\"></div>\n\n위에서 이해한 대로, SFT에는 레이블이 지정된 데이터셋이 필요합니다. 오픈 소스에는 일반 목적의 레이블이 지정된 데이터셋이 많이 있지만, 모델을 귀하의 특정 사용 사례, 산업 또는 지식 도메인에 가장 적합하게 맞추기 위해 사용자 정의 데이터셋을 수동으로 작성하는 것이 의미가 있을 수 있습니다. 최근에는 Claude 3나 GPT-4와 같은 강력한 LLM을 사용하여 이러한 데이터셋을 만드는 접근 방식이 인간 레이블링 대안으로 발전되어왔습니다.\n\n\"dolly-15k\" 데이터셋은 Databricks 직원들에 의해 수동으로 만들어진 인기 있는 일반 목적의 오픈 소스 명령어 미세 조정 데이터셋입니다. 이 데이터셋에는 명령문과 문맥이 원하는 응답과 함께 레이블이 지정된 대략 15,000개의 예시가 포함되어 있습니다. 이 데이터셋은 우리의 BioLLaMA2 모델을 지시어를 따르는 작업, 예를 들어 닫힌 Q&A 작업에 맞게 조정하는 데 사용될 수 있습니다. 지시어를 따르는 SFT를 위해, 데이터셋의 각 항목을 전체 텍스트 프롬프트로 변환하고, 모델을 맞추고자하는 작업을 나타내는 프롬프트 구조에 포함하여 진행할 것입니다. 이는 다음과 같이 보일 수 있습니다:\n\n```js\n### 명령:\n{item.instruction}\n### 문맥:\n{item.context}\n### 응답:\n{item.response}\n```\n\n프롬프트 템플릿은 모델 패밀리에 따라 다양할 수 있으며, 일부 모델은 해시태그보다 HTML 태그나 다른 특수 문자를 선호할 수 있습니다. 이 절차는 데이터셋의 각 항목에 대해 적용되며, 모든 항목이 큰 텍스트 조각으로 연결되기 전에 수행됩니다. 최종적으로 위에서 설명한 NLP 특정 전 처리 후, 이 파일은 다음 토큰 예측을 활용하여 모델로 훈련될 수 있으며, CLM 기반의 훈련 목표를 사용할 것입니다. 이 특정 프롬프트 구조에 지속적으로 노출되므로 모델은 그에 따라 작동하고 해당 방식으로 행동하는 것을 배울 것입니다 — 이 경우에는 지시어를 따릅니다. BioLLaMA2를 dolly-15k 데이터셋에 맞춘 후, BioLLaMA2-instruct 모델은 프롬프트를 통해 제출된 지시를 철저히 따를 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 파인튜닝 변형 — 인간 선호도 조정 기술 (RLHF/PPO, DPO, KTO, ORPO)\n\nBioLLaMA2와 함께, 우리는 생물 기술 연구 영역에 적합한 모델을 가지고 있으며, 사용자가 기대하는 대로 편리하게 우리의 지시에 따릅니다. 하지만 기다려봐 — 모델이 실제 사용자와 일치하는 것일까? 지금까지 논의된 파인튜닝 접근 방식의 핵심 문제가 돋보입니다. 사용한 데이터셋은 우리가 사용자가 좋아하거나 필요로 한다고 생각하는 것의 대리인입니다: 선별된 연구 논문에서의 내용, 언어, 머리글, 그리고 일부 Databricks 직원들이 제작한 dolly-15k의 원하는 지시 행동과 같은 것들입니다. 이는 사용자 중심의 제품 개발 개념과 대비됩니다. 이는 기민한 제품 개발의 핵심적이고 잘 수립된 원리 중 하나입니다. 실제 대상 사용자로부터 반복적으로 피드백을 받는 것이 훌륭한 제품을 개발할 때 매우 성공적이라는 것이 입증되었습니다. 사실, 사용자를 위한 훌륭한 경험을 구축하려는 경우 우리가 해야 할 일이지 않을까요!\n\n![이미지](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_11.png)\n\n이를 염두에 두고, 연구자들은 인간 피드백을 성능 향상에 통합하는 방법을 찾는 데 상당한 노력을 기울였습니다. 이에따라, 그들은 (심층적인) 강화 학습 (RL) 과 상당한 중첩이 있다는 것을 깨달았는데, 이것은 환경 내에서 행동을 수행하는 자율 에이전트들에 대한 것으로, 다음 상태를 생성하며 항상 보상과 결합됩니다. 에이전트들은 훈련 단계에서 보상을 극대화하기 위해 점진적으로 최적화된 정책이나 가치지도에 따라 행동합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_12.png)\n\n이 개념은 LLM의 세계로 프로젝션될 때 LLM 자체가 에이전트로 작용하는 것으로 이어집니다. 추론 과정에서 자동 회귀 토큰 예측의 각 단계마다 모델의 어휘가 작업 공간이고 환경은 가능한 토큰 조합이 되는 행동을 수행합니다. 새로운 추론 주기마다 새로운 상태가 형성되며, 이는 이상적으로 어떤 인간 피드백과 관련된 보상과 함께 함께합니다.\n\n이 아이디어에 기반하여 여러 인간의 선호 정렬 접근 방식이 제안되고 시험되었습니다. 다음에서는 가장 중요한 몇 가지 접근 방법에 대해 살펴보겠습니다:\n\n## 인간 피드백에 대한 강화학습 (RLHF) 및 근접 정책 최적화 (PPO)\n\n\n<div class=\"content-ad\"></div>\n\n![여기에 이미지가 있습니다](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_13.png)\n\n인간 피드백을 통한 강화 학습은 초기 창조적 AI 혹평 중요한 기술적 기반 중 하나였으며, Anthropi Claude나 GPT-3.5와 같은 대형 디코더 모델들의 위험의 동반자가 된 획기적인 성과를 거두고 사용자 맞춤 방향으로 추가적인 동기 부여를 제공했습니다. RLHF는 두 단계로 작동하며 Figures 13과 14에서 설명되어 있습니다.\n\n1단계 (Figure 13): 먼저, 실제 RL을 통한 교육 접근에서 나중에 사용할 보상 모델을 훈련해야 합니다. 따라서, 목적과 일치하는 프롬프트 데이터 집합(우리의 BioLLaMA2-instruct 모델의 경우, 지시와 문맥의 쌍일 것입니다)이 모델에 공급되어 세부 조정되며, 단 하나가 아닌 두 개 이상의 추론 결과를 요청합니다. 이러한 결과는 최적화 목표에 따라 인간 레이블러에게 제공되고, 1등, 2등, 3등 등과 같이 점수 매기기를 기반으로 합니다. \"Anthropic/hh-rlhf\"와 같은 몇 가지 오픈 소스 기호 순위 데이터 집합도 있으며, 이것은 red-teaming 및 정직성 및 무해성 목표에 맞게 설계되어 있습니다. 정규화 단계와 보상 값으로의 변환 후, 보상 모델이 단일 샘플-보상 쌍을 기반으로 훈련되고, 여기서 샘플은 단일 모델 응답입니다. 보상 모델 아키텍처는 일반적으로 세부 값으로 잠재 공간을 투사하는 정보 대신 토큰에 대한 확률 분포가 됩니다. 그러나 이 모델의 매개 변수의 이상적인 크기는 여전히 연구 대상이며, 지난 시간에 모델 제공 업체에 의해 다양한 접근 방식이 선택되어 왔습니다.\n\n![여기에 이미지가 있습니다](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_14.png)\n\n<div class=\"content-ad\"></div>\n\n단계 2 (그림 14): 새 보상 모델을 사용하여 실제 모델을 학습하는 단계로 넘어갑니다. 따라서 해당 모델을 조정하기 위해 다른 일련의 프롬프트가 모델을 통해 공급됩니다 (도면의 회색 상자), 결과적으로 각각 하나의 응답이 생성됩니다. 이후 이러한 응답은 각각 개별 보상을 검색하기 위해 보상 모델에 공급됩니다. 그런 다음, 보상이 할당된 모델의 답변을 최대화하기 위해 Proximal Policy Optimization (PPO)이라는 정책 기반 강화 학습 알고리즘이 사용되어 모델의 가중치를 점진적으로 조정합니다. CLM과는 달리, 그라디언트 하강 대신에 이 접근 방식은 이제 목표 (보상)를 최대화하려 하기 때문에 그라디언트 상승(또는 그라디언트 하강 오버 1 − 보상)을 활용합니다. 학습 중에 모델 동작의 지나친 드리프트를 방지하기 위한 알고리즘적 안정성을 높이기 위해서 PPO와 같은 강화 학습 기반 알고리즘에 의해 야기될 수 있는 너무 큰 드리프트를 예방하기 위해 예측 변화 벌점이 보상 항에 추가되어, 초기 언어 모델의 입력 프롬프트에 대한 예측 확률 분포에서 너무 많이 벗어나는 답변을 벌한다.\n\nPPO와 함께 RLHF 이상의 다른 접근 방법이 개발되어 왔으며, 현재는 선호 정렬을 위해 가장 널리 사용되고 검증된 접근 방법입니다. 다음 몇 섹션에서는 조금 더 고급 수준으로 이러한 접근법 중 일부에 대해 자세히 살펴볼 것입니다. 이는 고급 독자를 대상으로 한 내용이므로 딥 러닝 및 강화 학습에 대한 경험 수준에 따라 직접 다음 섹션인 \"의사결정 흐름도 - 선택할 모델, 선택할 파인튜닝 경로\"로 건너뛰고자 할 수도 있습니다.\n\n## 직접 정책 최적화 (DPO)\n\n직접 정책 최적화(Direct Policy Optimization, DPO)은 RLHF에서 유래한 선호 정렬 접근 방식으로, RLHF의 두 가지 주요 약점을 해결하는 것이 목표입니다.\n\n<div class=\"content-ad\"></div>\n\n- 먼저 보상 모델을 훈련하는 것은 추가적인 자원 투자를 필요로 하며, 보상 모델의 크기에 따라 상당히 중요할 수 있습니다.\n- PPO를 사용한 RLHF의 훈련 단계는 대규모 연산 클러스터가 필요합니다. 초기 LM, 조정된 LM, 보상 모델의 3개 복제본이 저지연 설정에서 동시에 호스팅 및 조정되어야 합니다.\n- RLHF는 불안정한 절차가 될 수 있습니다 (→ 예측 변동 페널티가 이를 완화하려고 시도합니다)\n\n![image](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_15.png)\n\nDPO는 대안적인 선호 정렬 접근법으로 Rafailov 등(2023)에 의해 제안되었습니다. DPO의 주요 아이디어는 보상 모델 훈련을 건너뛰고 최종 선호 정렬 LLM을 선호 데이터에 직접 조정하는 것입니다. 이는 보상 모델의 매개변수화(보상 항목)를 손실 함수(figure 16)로 변환하는 몇 가지 수학적 수정을 적용하고 실제 보상 값들을 선호 데이터에 대한 확률 값으로 대체함으로써 달성됩니다.\n\n![image](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_16.png)\n\n<div class=\"content-ad\"></div>\n\n이는 선호도에 맞는 모델로 나아가면서 계산 및 알고리즘 복잡성을 줄입니다. 논문은 RLHF와 비교하여 성능 향상도 보여주고 있지만, 이 방법은 최근에 제안된 것이기 때문에 결과는 실질적인 증명을 필요로 합니다.\n\n## 칸먼-트벽시 최적화 (KTO)\n\nRLHF와 DPO와 같은 인간 피드백과 언어 모델을 조정하는 기존 방법들은 입력에 대해 하나가 다른 것을 선호하는 출력 쌍처럼 선호 데이터가 필요합니다. 그러나 규모별로 고품질의 선호 데이터를 수집하는 것은 현실 세계에서 어렵고 비싸며 많은 노이즈, 불일치 및 비교 불가능성으로 인해, 서로 다른 인간 평가자들이 어떤 출력이 더 나은지에 대해 상이한 견해를 가질 수 있습니다. KTO는 Ethayarajh 등에 의해 (2024) 제안된 대안적 방법으로, 간단하고 풍부한 신호와 함께 작동할 수 있는 접근법으로 소개되었습니다 — 주어진 출력이 특정 입력에 대해 바람직한지 또는 바람직하지 않은지만 알고 있으면 되며, 출력 간의 상대적인 선호를 알 필요가 없습니다.\n\n![이미지](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_17.png)\n\n<div class=\"content-ad\"></div>\n\nKTO(카네만-트버스키 최적화)는 세대의 상대적 \"좋음\"을 포착하는 보상 함수를 정의한 후, 모델을 최적화하여 카네만-트버스키 가치 함수하에서 이 보상의 기대값을 최대화하는 방식으로 작동합니다. 카네만과 트버스키의 전망 이론은 인간이 편향되었지만 명확히 정의된 방식으로 불확실한 결과에 대한 결정을 내리는 방법을 설명합니다. 이 이론은 이득에서 오목하고 손실에서 오목한 가치 함수에 의존하며, 수익과 손실을 분리하는 기준점이 존재합니다(17번 그림 참조). KTO는 선호도의 가능성을 극대화하는 것이 아니라 사람의 유틸리티 개념을 직접 최적화합니다.\n\nKTO의 주요 혁신점은 선호 가능 여부에 대한 이진 신호만 필요하다는 점입니다. 이는 전체 선호도 쌍 대신 더 데이터 효율적으로 사용할 수 있도록 합니다. 바이너리 피드백 신호는 훨씬 풍부하고 수집 비용이 저렴하기 때문에 선호도 기반 방법보다 데이터 효율적입니다(18번 그림 참조).\n\nKTO는 선호도 데이터가 부족하거나 비용이 많이 드는 상황에서 특히 유용하지만, 모델 출력의 품질에 대한 바이너리 피드백의 대량을 사용할 수 있는 경우입니다. 논문에 따르면, 모델 규모가 클 때 선호도 기반 방법인 DPO와 같은 성능을 극대화하거나 심지어 능가할 수 있습니다. 그러나 실제 환경에서 규모별로 확인되어야 합니다. KTO는 선호도 기능성을 극대화하는 것이 목표일 때 선호될 수 있습니다. 그러나 선호도 데이터가 노이즈가 적거나 추이가 없는 고 품질일 때라면 선호도 기반 방법이 여전히 더 나은 선택일 수 있습니다. KTO는 일부 경우에는 극도의 데이터 불균형을 처리하고 감독된 세밀 조정을 회피하는 측면에서 이론적 장점을 가지고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## Odds Ration Preference Optimization (ORPO)\n\nORPO의 주된 동기는 기존의 선호 정렬 방법인 RLHF 및 DPO와 같은 제한 사항을 해결하는 데에 있습니다. 이러한 방법들은 종종 별도의 지도적 세세조정(SFT) 단계, 참조 모델 또는 보상 모델이 필요합니다. 홍 등(2024)의 논문은 SFT만 사용하면 원하지 않는 스타일로 토큰을 생성할 가능성이 높아진다고 주장합니다. 교차 엔트로피 손실이 비선호하는 응답에 대해 직접적인 처벌을 제공하지 않기 때문입니다. 동시에 그들은 SFT가 강력한 선호 정렬 모델에 수렴하는 데 중요하다고 주장합니다. 이로 인해 자원을 많이 소모하는 두 단계 정렬 프로세스가 발생합니다. 이러한 단계를 결합하여 ORPO는 SFT의 도메인 적응 이점을 유지하면서 동시에 선호 정렬 접근 방식에 의해 목표로 하는 원치 않는 생성 스타일을 파악하고 완화하려고 합니다. (그림 19 참조)\n\n![image](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_19.png)\n\nORPO는 기존의 인과 언어 모델링에 연결된 손실(예: 교차 엔트로피 손실)에 오즈 비율이 기반된 처벌을 통합하는 혁신적인 선호 정렬 알고리즘을 소개합니다. ORPO의 목적 함수는 두 구성 요소로 이루어집니다: SFT 손실과 상대 비율 손실 (LOR). LOR 항목은 선호하는 응답과 비선호하는 응답을 생성하는 우도 사이의 오즈 비율을 최대화하여 모델에게 거부 응답에 높은 확률을 할당하는 것에 대한 처벌을 효과적으로 적용합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_20.png\" />\n\nORPO는 미리 학습된 언어 모델을 세부 도메인이나 작업에 적응시키면서 모델의 출력이 인간의 선호와 일치하도록 보장하고 싶을 때 특히 유용합니다. 이는 UltraFeedback 또는 HH-RLHF 데이터셋과 같이 pairwise preference 데이터셋에 액세스할 수 있는 시나리오에서 적용할 수 있습니다. 이를 염두에 둔 ORPO는 별도의 참조 모델, 보상 모델 또는 두 단계의 미세 조정 접근 방식이 필요하지 않아 RLHF 및 DPO에 대한 더 효율적이고 효과적인 대안으로 설계되었습니다.\n\n# 의사결정 플로우 차트 - 어떤 모델을 선택하고 어떤 미세 조정 경로를 선택할 것인가\n\n여러 미세 조정 접근 방식을 심층적으로 살펴본 후에 특정 요구사항에 따라 시작할 모델과 선택할 접근 방법에 대한 명확한 질문이 제기됩니다. 미세 조정 목적에 적합한 올바른 모델을 선택하기 위한 접근 방법은 두 단계 접근 방식입니다. 첫 번째 단계는 어떤 미세 조정 의도도 없는 기본 모델을 선택하는 과정과 유사하며, 다음 차원을 고려하여 보다 적합한 모델을 선택합니다(완전하지 않음):\n\n<div class=\"content-ad\"></div>\n\n- 사용할 플랫폼: 각 플랫폼은 해당 플랫폼을 통해 액세스할 수 있는 일련의 모델을 제공합니다. 이를 고려해야 합니다. 지역별 모델 가용성에 대한 차이가 있을 수 있음을 유의하십시오. 이에 대한 자세한 정보는 해당 플랫폼의 문서를 확인하시기 바랍니다.\n- 성능: 조직은 특정 작업에 최소한의 모델을 사용해야 합니다. 이에 대해 일반적인 지침은 제공되지 않지만 세밀한 조정은 모델의 성능을 크게 향상시킬 수 있습니다 (더 작은 세밀하게 조정된 모델이 더 큰 일반적인 모델보다 성능을 능가할 수 있음). 기본 모델의 평가 결과를 활용하는 것은 중요한 지표가 될 수 있습니다.\n- 예산 (TCO): 일반적으로 더 큰 모델은 더 많은 컴퓨팅 및 잠재적으로 멀티 GPU 인스턴스를 필요로 하며, 복수의 가속기에 걸쳐 교육 및 제공을 위해 이에 직접적인 영향을 미칩니다. 이는 교육 및 추론 비용, 교육 및 추론의 복잡성, 필요한 자원 및 기술 등, 모델의 전체 수명 주기 중 TCO의 일부로 고려되어야 합니다. 이는 단기 및 장기 예산 할당과 일치해야합니다.\n- 라이선스 모델: 모델마다 도메인 및 상업적 이용에 따라 라이선스 제약사항이 있습니다. 이를 고려해야 합니다.\n- 지배 체제, 윤리, 책임 있는 인공 지능: 모든 조직은 이러한 차원과 함께 규정 가이드라인을 가지고 있습니다. 모델 선택 시 반드시 고려되어야 합니다.\n\n예시: 조직은 기본 모델의 평가 결과를 바탕으로 LLaMA 2 모델을 고려하고, 기본 모델을 기반으로 한 Anthropic Claude 또는 AI21Labs Jurassic과 같은 프로프라이어터리 모델의 사용을 배제하기로 결정할 수 있습니다. 더 나아가, 그들은 이 모델의 7B-파라미터 버전만 사용하기로 결정하여 단일 GPU 인스턴스에서 교육 및 제공할 수 있도록 합니다.\n\n두 번째 단계는 실험 단계를 위해 고려해야 할 1-몇 개 모델로 초기 모델 선택을 좁히는데 관련되어 있습니다. 구체적인 접근 방식을 선택할 최종 결정은 아래 그림에서 언어 모델의 미세 조정 수명주기에 대한 원하는 진입점에 따라 달라집니다.\n\n<img src=\"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_21.png\" />\n\n<div class=\"content-ad\"></div>\n\n따라서, 다음 차원을 고려해야 합니다:\n\n- 수행할 작업: 각각의 사용 사례는 특정 모델 동작을 요구합니다. 어떤 사용 사례에는 간단한 텍스트 완성 모델 (다음 토큰 예측)이 충분할 수 있지만, 대부분의 사용 사례에서는 수다성, 지시사항 준수 또는 다른 작업 특정 동작이 필요합니다. 이 요구 사항을 충족하기 위해 우리는 수행할 작업을 역발상하는 접근 방식을 취할 수 있습니다. 즉, 원하는 작업을 수행할 모델에 맞게 정의해야 합니다. 이는 일러스트를 기준으로 모델이 원하는 모델 동작과 일치하여 파란색, 주황색 또는 녹색 원 안에서 끝나야 함을 의미합니다. 동시에 세부 튜닝 여정을 흘러가는 흐름도의 가능한 경로와 함께 정의해야 합니다.\n- 올바른 시작점 선택 (합리적인 범위 내에서): 세부 튜닝 여정이 끝나야 할 위치에 대해 명확해져야 하지만, 해당 흐름도에서 어디서든 출발할 수 있습니다. 그러나 이는 합리적이어야 합니다 — 수백만 개의 게시된 모델이 있는 모델 허브 시대에는 다른 사람이 이미 세부 튜닝 단계를 수행했고 결과 모델을 공유한 경우 체크하는 것이 의미있을 수 있습니다.\n- 세부 튜닝은 반복적이고 재귀적인 프로세스입니다: 원하는 모델까지 여러 번의 연속적인 세부 튜닝 작업을 수행할 수 있습니다. 그러나 모델의 가중치에 무한한 양의 정보를 인코드할 수 없으므로 추월하는 것을 염두에 둬야 합니다. 이를 해소하기 위해 이 논문과 블로그에서 보여주는 LoRA와 같은 매개변수 효율 세부 튜닝 방법을 활용할 수 있습니다.\n- 특정 작업 성능 향상에 초점을 맞춤: 세부 튜닝은 특정 작업에서 모델의 성능을 향상시키기 위해 수행됩니다. 언어 패턴(도메인별 언어, 약어 등)이나 귀하의 훈련 데이터에 암시적으로 포함된 정보에서 성능 향상을 원한다면 계속해서 사전 훈련을 진행해야 합니다. 특정 작업으로의 성능 향상을 원한다면 지도 세부 튜닝을 선택해야 합니다. 모델 동작을 실제 사용자와 일치시키고자 하는 경우 인간의 선호에 맞게 조정하는 것이 올바릅니다.\n- 데이터 가용성: 훈련 데이터도 우리가 선택하는 경로에 영향을 미칠 것입니다. 일반적으로 조직은 레이블이 지정되지 않은 텍스트 데이터를 보유하고 있는 경우가 많습니다. 레이블이 지정된 데이터를 획득하는 것은 비용이 많이 들 수 있습니다. 흐름도를 통해 탐색할 때 이 차원을 고려해야 합니다.\n\n이와 같은 역발상 방식을 흘러가는 접근과 상기된 흐름도를 통해 시작할 모델과 세부 튜닝 흐름도를 따라갈 경로를 식별할 수 있습니다.\n\n좀 더 명확히 이해하실 수 있도록 두 가지 예제를 제공해 드리겠습니다:\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_22.png)\n\n예시 1: 위의 튜닝 섹션에 설명된 예시를 따라가면, 특정 사용 사례에 맞는 가르침 모델을 형성할 수 있습니다. 그러면 실제 사용자의 선호도에 부합되며, 바이오테크 분야의 성능을 향상시키고 싶습니다. 연구 논문 형태의 레이블되지 않은 데이터가 제공됩니다. 우리는 시작점으로 LLaMA-2-7b 모델 패밀리를 선택합니다. Meta가 LLaMA-2-7b 가르침 모델을 발표하지 않았으므로, 텍스트 완성 모델인 LLaMA-2-7b-base에서 출발합니다. 그런 다음, 연구 논문 코퍼스에서 계속된 사전 훈련을 수행한 후, dolly-15k 데이터셋과 같은 오픈 소스 가르침 데이터셋에서 지도 학습 미세 조정을 수행합니다. 이로써 LLaMA-2-7B-base의 가르침으로 조정된 바이오테크 버전인 BioLLaMA-2-7b-instruct가 생성됩니다. 다음 단계에서는 모델을 실제 사용자의 선호도에 맞추고 싶습니다. 선호도 데이터셋을 수집하고 보상 모델을 훈련한 후, PPO를 사용하여 RLHF를 통해 모델을 선호도에 맞춥니다.\n\n![image](/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_23.png)\n\n예시 2: 이 예시에서는 채팅 모델을 사용하려고 합니다. 그러나 실제 사용자의 선호도에 부합되도록 조정하고 싶습니다. 시작점으로 LLaMA-2-7b 모델 패밀리를 선택합니다. Meta가 제공하는 사용자 지정 채팅 모델인 LLaMA-2-7b-chat을 시작점으로 사용할 수 있음을 알게 됩니다. 다음 단계에서는 모델을 실제 사용자의 선호도에 맞추고 싶습니다. 사용자로부터 선호도 데이터셋을 수집하고, 보상 모델을 훈련한 후 PPO를 사용하여 RLHF를 통해 모델을 선호도에 맞춥니다.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n생성 모델 인공지능은 기업과 조직에 많은 흥미로운 사용 사례를 제공합니다. 그러나 이러한 응용 프로그램은 대개 개별 소비자용으로 레시피나 연설을 생성하는 것과 같은 것보다 훨씬 복잡합니다. 기업에서는 인공지능이 조직의 특정 도메인 지식, 프로세스 및 데이터를 이해해야 합니다. 기존 기업 시스템 및 애플리케이션과 통합되어야 합니다. 또한 다양한 직원과 역할에 맞는 맞춤형 경험을 제공하면서 안전하게 작동해야 합니다. 기업 환경에서 생성 모델 인공지능을 성공적으로 구현하려면 기술이 조직의 고유한 요구 사항에 맞게 신중하게 설계되고 맞춤화되어야 합니다. 일반적으로 공개적으로 학습된 모델을 사용하는 것만으로 충분하지 않을 것입니다.\n\n이 블로그 글에서는 도메인 적응이 모델이 \"편안한 영역\"을 벗어난 작업에 직면했을 때 이러한 갭을 메우는 데 도움이 되는 방법으로 어떻게 도메인 적응이 도움이 될 수 있는지에 대해 설명했습니다. 문맥 내 학습과 세밀한 조정을 통해 도메인 적응을 위한 두 가지 강력한 접근법에 대해 깊이 파고들었습니다. 마지막으로, 이러한 접근법 사이에서 선택할 때 고려해야 할 절충안에 대해 논의했습니다. \n\n강력한 인공지능 능력과 실제 비즈니스 요구 사항 사이의 이 갭을 성공적으로 메우는 것은 기업을 위해 생성 모델 인공지능의 궁극적인 잠재력을 발휘하는 데 중요합니다.","ogImage":{"url":"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_0.png"},"coverImage":"/assets/img/2024-06-19-ADeepDiveintoFine-Tuning_0.png","tag":["Tech"],"readingTime":21},{"title":"LLM은 어떻게 창의적인가요","description":"","date":"2024-06-19 03:52","slug":"2024-06-19-HowareLLMscreative","content":"\n\n## LLMs의 창의성에 대한 과학 - Softmax, 온도\n\nGPT, Llama 등의 생성 모델을 사용해 보았다면 '온도'라는 용어를 만난 적이 있을 것입니다.\n\n![이미지](/assets/img/2024-06-19-HowareLLMscreative_0.png)\n\n우선, '온도'는 생성된 콘텐츠의 창의성을 제어하는 매개변수입니다.\n\n<div class=\"content-ad\"></div>\n\n하지만 기다려봐, 그건 엄청나게 소위 '너드' 같지 않나요? 처음에 이를 들었을 때 나는 분명히 멋진 물리학적 현상이 일어나고 있을 것이라고 생각했어요. 그리고 왜냐하면 대부분의 사람들이 온도와 연결짓는 것이기 때문이죠.\n\n이 게시물에서, 나는 생성모델에 대한 온도를 설명하고 특히 LLM (Large Language Models)에서의 작동 방식과 창의력을 수학적으로 보여드릴 거에요.\n\n대형 언어 모델(LLMs)은 자기 회귀 모델의 한 종류입니다. 자기 회귀 모델이란 무엇일까요? 간단히 말해, 이는 과거 값을 사용하여 미래 값을 예측하는 통계 모델들을 의미합니다. LLM의 경우, 과거 값은 당신이 입력하고, 미래 값은 생성된 토큰입니다.\n\nLLMs은 한 번에 전체 문장을 생성할 수 없다는 것을 알아두어야 합니다. 왜냐하면 그들은 다음 토큰을 예측하는데 사용되는 자기 회귀형 모델이기 때문입니다. 이 토큰은 입력에 추가되어 다른 토큰을 생성하기 위해 사용되며, 그 체인은 'EOT'란 끝 토큰이 생성될 때까지 계속됩니다. 이는 모델에게 생성을 중단하도록 신호를 보냅니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*mrIpGPlXZHL_K-e640HFaw.gif)\n\n다음 토큰을 생성하기 위해 LLM은 어휘 사전에 있는 모든 가능한 토큰에 대한 확률 목록을 출력합니다.\n\n![이미지](/assets/img/2024-06-19-HowareLLMscreative_1.png)\n\nLLM의 어휘 사전 크기가 100이면, 한 번에 이 100개의 토큰 중 하나만 생성할 수 있습니다. 이 100개의 토큰 각각에 대해 해당 토큰이 다음 시퀀스에서 얼마나 가능성 있는지를 나타내는 확률 점수가 반환됩니다. 실제로, 어휘 사전 크기는 훨씬 더 큽니다. 예를 들어, GPT-4V는 32K 토큰의 어휘 사전 크기를 가지고 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n한 마디로 LLM은 토큰 시퀀스를 입력으로 받아 처리하고, 어휘 사전의 각 토큰에 대한 확률 목록을 출력합니다. 일반적으로, 가장 높은 확률을 가진 토큰이 다음 세대의 토큰으로 반환됩니다.\n\n## 소프트맥스\n\n확률 목록을 계산하는 작업은 소프트맥스 레이어에 의해 수행됩니다. 모든 LLM은 로짓 벡터(로짓은 각 토큰에 연관된 비정규화된 raw scores입니다)를 입력으로 받고, 적절한 확률 분포를 출력하는 '마법 같은' 레이어를 최종 레이어로 갖습니다.\n\n일단 소프트맥스를 내려놓고 효율적으로 확률을 생성하기 위한 자체 함수를 만들어봅시다. 우리의 함수는 특정 기준을 충족해야 합니다. 성공하면 왜 소프트맥스를 사용해야 하죠?\n\n<div class=\"content-ad\"></div>\n\n- 1. 입력 벡터를 가져와 동일한 크기의 출력 벡터를 생성합니다.\n- 2. 출력 벡터의 각 요소가 음수가 아닌지 확인합니다(확률은 음수일 수 없음).\n- 3. 입력 값이 클수록 출력 값이 클 수 있도록 합니다.\n- 4. 출력 벡터의 모든 요소가 1이 되도록 합니다(확률 분포임).\n\n위의 조건을 만족하기 위해 각 요소별 변환을 수행하게 됩니다. 변환은 음이 아닌 값이어야 하며, 항상 양의 정수가 출력되는 함수를 사용해야 합니다. 또한, 입력 값이 클수록 항상 더 큰 출력 값으로 변환되도록 하는 단조 증가함수여야 합니다.\n\neˣ 함수는 위의 모든 조건(1~3)을 모두 만족합니다.\n\n[이미지](/assets/img/2024-06-19-HowareLLMscreative_2.png)\n\n<div class=\"content-ad\"></div>\n\n우리의 전략은 입력 벡터의 각 요소에 eˣ 함수를 적용하여, 길이는 동일하지만 모든 양수인 숫자로 이루어진 벡터를 생성합니다.\n\n![image](/assets/img/2024-06-19-HowareLLMscreative_3.png)\n\n하지만 우리가 가장 중요하게 놓친 요구 사항이 있었어요, 그거 발견했나요?\n\n출력 벡터의 값들이 1보다 큽니다. 곡선에서 볼 수 있듯이, x = 0일 때 y = 1이고, x가 0을 초과하면 y가 1을 초과합니다. 이는 우리의 경우에 맞지 않는데, 확률이 필요하기 때문입니다. 각 값이 1보다 작아야 하고, 모든 값이 합쳐서 1이 되어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n이 문제를 해결하기 위해 출력 벡터의 모든 요소를 출력 벡터의 모든 요소의 합으로 나눌 수 있습니다. 이렇게하면 각 값이 1보다 작아지고 모두 1로 합산됩니다. 이 단계를 정규화라고 합니다.\n\n![이미지](/assets/img/2024-06-19-HowareLLMscreative_4.png)\n\n이제 소프트맥스에 도착했습니다. 하하. 네, 이것이 바로 우리가 이해하기 위해 도출한 소프트맥스 함수입니다. 단순히 설명하는 것보다 더 나은 이해를 도모했죠.\n\n참고: 변환 함수는 미분 가능해야 하므로 손실을 전파할 수 있습니다. 이것이 소프트맥스에서 'e'를 선택하는 또 다른 이유입니다. 사실 지수 함수의 도함수를 계산하는 것이 가장 쉬운 일이라고 믿습니다. :)\n\n<div class=\"content-ad\"></div>\n\n이제 우리는 Softmax 함수를 사용하여 LLM에서 확률이 어떻게 생성되는지 이해했어요. 모델에 어떻게 창의성을 도입할 수 있는지 알아보겠어요.\n\n## AI 모델에서의 창의성, 정말?\n\n일부 창의성을 도입하려면 Softmax에 의해 생성된 확률 분포를 \"평평하게(flatten)\" 만들어야 해요. \"평평하게\"라고 무슨 뜻일까요?\n\n![이미지](/assets/img/2024-06-19-HowareLLMscreative_5.png)\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-HowareLLMscreative_6.png)\n\n한 예를 통해 이해해 봅시다.\n\nLLM에 입력되는 내용:\n\n“대화를 완성하세요.”\n\n\n<div class=\"content-ad\"></div>\n\nA: \"안녕, 어떻게 지내니?\"\n\nB: \"LLM은 이제 B가 말하는 첫 번째 토큰을 예측하도록 지정되었습니다.\n\n간단하게 하기 위해 단어 vocab을 5개만 고려합시다.\"\n\n<div class=\"content-ad\"></div>\n\n이 모델은 이 입력을 처리하여 로짓 벡터를 생성하며, 이 벡터는 소프트맥스에 의해 확률로 변환됩니다. 소프트맥스 레이어의 입력인 로짓 벡터는 [0.1, 0,0.5,1, 4, 0.6]이며, 토큰[‘Ni Hao’, ‘Konnichiwa’, ‘Hola’, ‘Namaste’, ‘Hello’, ‘Ciao’]에 대한 출력은 [0.01, 0.01 0.02 0.04 0.86 0.02]입니다.\n\n‘Hello’(목록에서 다섯 번째 단어)을 다음 토큰으로 선택할 확률은 86%입니다. 그러나 확률 점수 중 하나가 지배하는 경우 재미가 없죠? 즉, ‘Hello’가 출력으로 선택될 가능성이 거의 확실하다는 것이죠. 동일한 상황이 발생하면 모델이 항상 \"Hello\" 토큰을 선택할 확률이 높습니다. 이는 매우 예측성이 강하며 창의성이 아니라 반대입니다. 랜덤성에 대한 공간이 없으며 다양한 유효한 토큰을 시도할 여지가 없습니다.\n\n만약 같은 토큰[‘Ni Hao’, ‘Konnichiwa’, ‘Hola’, ‘Namaste’, ‘Hello’, ‘Ciao’]에 대해 소프트맥스 출력이 [0.13 0.12 0.14 0.15 0.28 0.14]라면 어떻게 될까요?\n\n“Hello”가 선택될 가능성은 여전히 높지만, 이전보다 다른 토큰들도 좋은 기회를 갖고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 경우, 모델이 일반적인 \"안녕\"을 생성하는 것에서 조금 벗어나 \"남테\", \"올라\" 등을 시도할 여지가 있습니다. 기본적으로 약간의 무작위성을 통합하는 것이죠. 이는 파급 효과를 일으키며 대화 전체가 가장 예상치 못한 방향으로 전개될 수 있습니다. 예를 들어, B가 일본어로 말하고 A가 A가 한 언어를 알아보려고 노력하는 상황 등이 발생할 수 있습니다. 이 창조적으로 들리지 않나요? 저에게는 더 나은 정의가 없어요 :-)\n\n온도 매개변수('T')의 목표는 생성된 내용의 무작위성을 제어하는 것입니다.\n\n소프트맥스 함수를 수정하여 출력 벡터가 주로 우세한 확률 점수를 갖지 않도록 만드는 방법에 대해 이해해 보겠습니다(i.e., 확률 분포를 평평하게 만드는 것).\n\n소프트맥스 함수는 각 요소를 입력 벡터에서 eˣ로 변환합니다. 이를 통해 eˣ 함수를 살펴봐야 한다는 아이디어가 얻어집니다.\n\n<div class=\"content-ad\"></div>\n\n가장 간단한 예로서 2차원 입력 벡터 [1,2]를 살펴보고, 이를 이해하기 위해 Softmax를 적용해 봅시다.\n\n![Softmax 예시 이미지](/assets/img/2024-06-19-HowareLLMscreative_7.png)\n\n요소별로 eˣ 변환을 수행하면, [2.71, 7.39]를 얻을 수 있습니다.\n\n그리고 최종 Softmax 결과는 [2.71/(2.71+7.39), 7.39/(2.71+7.39)]이 될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n= [0.25, 0.75]\n\n두 번째 항목이 50%의 큰 마진으로 우세함을 나타냅니다. \n\n그러나 우리의 목표는 두 값의 소프트맥스 출력을 가깝게 만드는 것이죠, 그렇지 않나요?\n\neˣ 곡선을 살펴보면 이 차이가 주로 eˣ 곡선의 성질 때문인 것으로 추측할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n그렇다면, 이제 우리의 목표는 eˣ 함수를 조정하여 eˣ(1)과 eˣ(2)의 차이가 크지 않도록 만드는 것으로 전환됩니다. 만약 ||eˣ(1) - eˣ(2) ||가 작다면, 그것은 확률 간의 차이를 작게 만들 것이고 이것은 본질적으로 확률의 평탄화라고 불리게 됩니다. 다시 말해, eˣ 곡선의 가파른 정도를 줄입니다.\n\n<img src=\"/assets/img/2024-06-19-HowareLLMscreative_8.png\" />\n\n어떨 때, 파란색 곡선에서 ||eˣ(1) — eˣ(2) ||의 차이가 주황색 곡선보다 더 작은 것을 볼 수 있습니다.\n\n주황색 곡선: eˣ 함수\n\n<div class=\"content-ad\"></div>\n\n파란색 곡선: e^(1/2 ⋅ x) 함수, 우리는 분모를 온도 'T'라 부르며, 이 경우에는 2와 같습니다.\n\n‘T’가 증가하면 곡선이 덜 가파르게 변해 확률이 더 가까워진다는 뜻입니다. 아래는 Softmax with Temperature에 대한 대화식 desmos 그래프 링크이니, 곡선의 가파름을 제어하는 방법에 대해 더 나은 직관을 얻을 수 있도록 살펴보세요.\n\n이 섹션을 건너뛰셔도 됩니다. 이것은 미적분적이며 'e^(1/2 ⋅ x)'가 어떤 지점에서 'eˣ'보다 작은 도함수를 가지는 이유로 증명하는 부분입니다.\n\n파란색 곡선을 사용한 Softmax 과정(T=2)은 다음과 같을 것입니다:\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-HowareLLMscreative_9.png\" />\n\n바닐라 소프트맥스로 [0.25, 0.75]을 얻었지만 온도 (T)가 2일 때 [0.38, 0.62]를 얻었어요. 상당한 진전이죠, 알겠죠?\n\n매개변수 'T'를 증가시킴으로써 확률 분포를 더 평평하게 만들 수 있어요. T = 1이면 바닐라 소프트맥스와 본질적으로 같아지며, T가 1보다 낮게 설정되면 출력이 더 예측 가능해져요. 온도를 계속 증가시키면 어느 정도 지나면 생성된 콘텐츠가 암호화된 텍스트처럼 보일거에요.\n\n```js\nimport numpy as np\n\n\ndef softmax(xs):\n    return np.exp(xs) / sum(np.exp(xs))\ndef softmax_t(xs, t):\n    return np.exp(xs/t) / sum(np.exp(xs/t))\nxs = np.array([ 1 , 2 ])\nprint(softmax(xs))\nprint(softmax_t(xs, 2))\nprint(softmax_t(xs, 5))\nOUTPUT:\n[0.26894142 0.73105858]    #T = 1\n[0.37754067 0.62245933]    #T = 2\n[0.450166 0.549834]        #T = 5\n```\n\n<div class=\"content-ad\"></div>\n\nMistral 7B 모델에게 네팔에 대해 작성하도록 요청했어요. 첫 번째 텍스트에서 온도 'T'를 1로 설정했고, 두 번째 텍스트에서는 온도 'T'를 2로 설정했어요.\n\n차이를 확인해보세요\n\nT = 0.5\n\n네팔은 중국과 인도 사이에 위치한 아름다우며 다양한 나라로 알려져 있어요. 세계에서 가장 높은 산인 에베레스트를 비롯해 멋진 산악 풍경으로 유명해요.\n\n<div class=\"content-ad\"></div>\n\nT = 2\n\n네팔의 중심지에는 겨울에 종종 온도가 섭씨 4도까지 떨어지는 곳이 있습니다. 세계에서 가장 숨 막히는 눈이 덮인 산들 중 일부를 발견할 수 있어요.\n\n네, 두 번째 텍스트가 좀 더 창의적인 느낌이죠??\n\n한 마디로, 물리학적으로 온도는 분자의 무작위성의 정도이며, 이를 통해 우리에게 몸체에 함유된 열 에너지에 대한 감각을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n표 태그를 마크다운 형식으로 변경해 주세요.\n\n<div class=\"content-ad\"></div>\n\nhttps://en.wikipedia.org/wiki/Softmax_function","ogImage":{"url":"/assets/img/2024-06-19-HowareLLMscreative_0.png"},"coverImage":"/assets/img/2024-06-19-HowareLLMscreative_0.png","tag":["Tech"],"readingTime":7},{"title":"어쩌면 GPT가 최상이 아닐 수도 있습니다 BERT가 생성 기반 컨텍스트 학습을 능숙하게 다룰 수 있습니다","description":"","date":"2024-06-19 03:50","slug":"2024-06-19-MaybeGPTIsnttheBestBERTsCanMasterGenerativeIn-ContextLearning","content":"\n\n## |LLM|GPT|컨텍스트 학습|\n\n<img src=\"/assets/img/2024-06-19-MaybeGPTIsnttheBestBERTsCanMasterGenerativeIn-ContextLearning_0.png\" />\n\n트랜스포머가 발표되었을 때 기계 번역 분야에 혁명적인 영향을 미쳤습니다. 곧 이후에는 이 능력이 더 많은 작업으로 확장될 수 있다는 것이 깨달렸습니다. 실제로 트랜스포머는 많은 작업에 사용할 수 있는 언어 표현을 학습합니다.\n\n이 기간 동안 트랜스포머를 위한 두 가지 특별한 학습 패러다임이 제시되었습니다:\n\n<div class=\"content-ad\"></div>\n\n- 인과언어 모델링. 모델은 시퀀스에서 다음 토큰을 예측함으로써 학습합니다.\n- 마스킹된 언어 모델. 모델은 가려진 토큰을 예측해야 합니다.\n\n초기에는 BERT와 같은 모델이 다양한 작업에 융통성이 있어서 더 성공적이었습니다. 하지만 GPT3의 출시로 모든 것이 변했습니다.\n\nGPT3는 문맥 내 학습이라는 새로운 기능을 보여줬기 때문입니다. 몇 가지 예시를 받아들여서, 모델은 작업 자체를 매핑하고 실행할 수 있었습니다. 이것이 프롬프트 엔지니어링을 폭발시키고 대형 언어 모델에 대한 관심을 불러일으키는 촉매였습니다. 미세 조정을 수행할 필요 없이 모델이 새 작업을 학습할 수 있었습니다.\n\n![이미지](/assets/img/2024-06-19-MaybeGPTIsnttheBestBERTsCanMasterGenerativeIn-ContextLearning_1.png)\n\n<div class=\"content-ad\"></div>\n\nIn-context learning은 패러다임 변화와 커뮤니티 관심을 이끌었습니다. 이전에는 GPT-3 마스킹 모델이 가장 인기 있었지만, 이후 연구는 자기 회귀 모델로 전환되었습니다. 대부분의 연구가 이러한 유형의 모델에 집중되어 왔으며, 오늘날 대부분의 모델이 자기 회귀로 되어 있습니다.\n\n더욱이, 이러한 모델들은 이 기사에서 제안된 것처럼 생성할 능력이 없다고 여겨집니다.\n\n또한, 이러한 모델들은 생성이 불가능하다고 여겨지는 이 기사의 제안과 관련된 매우 중요한 두 가지 질문이 발생합니다.\n\n최근에 발표된 기사에서 이 질문들에 정확하게 다루었습니다.\n\n<div class=\"content-ad\"></div>\n\n이것이 MLM의 목적은 아니에요. 그러나 저자들은 별도의 훈련이나 세밀한 조정 없이 MLM을 사용하고 싶어해요. 그래서 그들은 두 가지 잠재적인 방법을 사용하기로 결정해요:\n\n자기 회귀 LLM은 특별히 텍스트 생성을 학습하는 반면, MLM은 빈칸 채우기(마스크 토큰)를 학습해요. 저자들은 텍스트 프롬프트 옆에 [MASK] 토큰을 놓고 모델이 다음 토큰을 생성하도록 텍스트를 생성해요. 그 다음 [MASK] 토큰이 추가되고 그런 식으로 계속해요.\n\nMLM은 조건부 로그 우도를 추정하지 않아요. 저자들은 추가적인 훈련을 피하기 위해 방정식을 수정하여 의사 로그 우도(PLL)를 얻어요. 이 접근법은 토큰 간의 강력한 지역 의존성이 있을 때 모델이 부정확하지 않도록 조정됐어요. 모델은 양방향 셀프 어텐션을 사용하기 때문에 지역 의존성에 민감해요.\n\n<img src=\"/assets/img/2024-06-19-MaybeGPTIsnttheBestBERTsCanMasterGenerativeIn-ContextLearning_2.png\" />\n\n<div class=\"content-ad\"></div>\n\n그러나 이 두 가지 방법에는 특히 계산 비용 측면에서 제한 사항이 있지만, 저자들은 시스템 최적화에는 관심이 없다.\n\n저자들은 DeBERTa (BERT의 파생물)를 사용했는데, 이 모델은 상대적인 위치 임베딩으로 훈련되었기 때문에 512 토큰을 넘어 확장될 수 있습니다. 그러나 저자들은 모델이 512보다 긴 컨텍스트 길이에 대해 일반화할 수 있는지 테스트했으며, 이를 통해 후속학습(few-shot 설정에 대한 테스트)에서 사용할 수 있도록 하였습니다. Needle in the Haystack(랜덤하게 생성된 6자리 숫자(바늘)가 긴 에세이 모음(쌀집)에 숨겨져 있고 모델이 그것을 찾아야 하는 테스트)에서 DeBERTa가 증가하는 크기의 시퀀스에서 바늘을 찾을 수 있는지 테스트했습니다. 관측 결과, DeBERTa는 512 토큰을 넘어 확장할 수 있고, 그 결과 특수한 학습 상황에서 사용할 수 있다는 것을 보여줍니다.\n\nDeBERTa는 GPT3보다 훨씬 작고, 적은 토큰으로 훈련되었음에도 불구하고, 2020년에 출시되었습니다. 저자들은 두 모델을 생성 및 분류와 같은 동일 유형의 작업에서 비교하기로 결정했습니다.\n\n<div class=\"content-ad\"></div>\n\n가장 흥미로운 결과 중 하나는 두 모델 모두 성능이 모델 크기와 비례한다는 것입니다(더 많은 매개변수는 더 나은 성능을 보여줍니다).\n\n![image](/assets/img/2024-06-19-MaybeGPTIsnttheBestBERTsCanMasterGenerativeIn-ContextLearning_4.png)\n\n또한, 둘 다 소수의 데이터로도 잘 작동하는 것을 확인할 수 있습니다(더 많은 예제가 두 모델을 도와줍니다).\n\n![image](/assets/img/2024-06-19-MaybeGPTIsnttheBestBERTsCanMasterGenerativeIn-ContextLearning_5.png)\n\n<div class=\"content-ad\"></div>\n\n자세한 분석 결과, DeBERTa는 제로샷 및 퓨샷 설정 모두에서 언어 이해 작업에 대해 GPT3보다 우수한 것으로 보입니다 (저자들은 인기 있는 기준인 SuperGLUE를 사용했습니다). 저자들은 GPT3의 유사 크기 버전 (1.4 B)과 DeBERTa를 비교하며 뚜렷한 차이를 발견했습니다. 또한:\n\n![이미지](/assets/img/2024-06-19-MaybeGPTIsnttheBestBERTsCanMasterGenerativeIn-ContextLearning_6.png)\n\n저자들은 또한 Winograd 스타일 및 텍스트 완성 작업에 대해 모델을 테스트했습니다. 이러한 작업은 상식적 추론, 언어 이해 및 복잡한 공용 참조 해상도가 필요한 더 복잡한 작업입니다. 예를 들어, 이야기에 대한 최적의 끝을 선택하거나 문장을 완성하는 것 등이 있습니다. 다시 한번, 가령 가리워진 언어 모델의 승리가 관찰되는 것으로 보입니다. 또 다른 흥미로운 점은 스케일링 비율이 더 높다는 것입니다: 매개변수를 증가시킴으로써 MLM의 성능이 더욱 빠르게 증가하는 것으로 보입니다.\n\n저자들은 또한 두 모델을 번역 작업에 대해 테스트했습니다. 번역 작업은 모델이 다른 언어에 능숙해야 하며, 여전히 훈련 데이터의 구성에 의존합니다. 이 경우, GPT3가 명백하게 우승자로 나타났으며, DeBERTa는 성능이 떨어지는 것으로 보입니다. 그러나 DeBERTa는 단일 언어 말뭉치만으로 훈련되었으므로 명확하게 불리한 시작을 합니다.\n\n<div class=\"content-ad\"></div>\n\n![2024-06-19-MaybeGPTIsnttheBestBERTsCanMasterGenerativeIn-ContextLearning_7](/assets/img/2024-06-19-MaybeGPTIsnttheBestBERTsCanMasterGenerativeIn-ContextLearning_7.png)\n\n이 두 모델은 클로즈드 북 질문에 대한 답변 및 상식적 추론에 대해 테스트되었습니다. 실제로 이 모델들은 고유의 지식을 보유하고 있으며, 그들의 매개변수에서 많은 정보를 저장할 수 있습니다. 또한, 오늘날의 LLM이 정보를 다시 찾아내는 능력이 중요한 측면입니다.\n\n또한, 오늘날 우리는 LLM이 일반적인 추론 능력을 조금은 갖고 있다고 기대합니다. 그래서 두 모델은 트리비아(온라인 퀴즈 데이터 세트), 실제 세계 이해를 테스트하기 위한 물리적 상호 작용 데이터 세트, 그리고 학년별 과학 문제를 테스트하기 위한 데이터 세트에서 테스트되었습니다.\n\nDeBERTa의 답변은 GPT3보다 상당히 나쁩니다. 저자들은 이는 모델의 교육과 구조에서 비롯된 것이라고 생각합니다:\n\n<div class=\"content-ad\"></div>\n\n모델은 지식 면에서 빛을 발하지는 않지만 상식적 추론 능력과 행동 척도에서는 GPT3와 비슷하게 잘 수행하는 것으로 보입니다.\n\n![이미지](/assets/img/2024-06-19-MaybeGPTIsnttheBestBERTsCanMasterGenerativeIn-ContextLearning_8.png)\n\n관심이 있는 분들을 위해 저자들은 해당 모델을 HuggingFace에서 공개했습니다.\n\n이 기사에서 저자들은 두 가지 핵심 포인트를 보여줍니다: MLMs도 콘텍스트 학습이 가능하며, 이러한 스킬들은 스케일을 키울수록 향상됩니다.\n\n<div class=\"content-ad\"></div>\n\nGPT3가 발표된 이후, 이러한 모델들이 오래되었고 인텍스트 학습이 불가능하다고 생각되었습니다. 이는 생성 작업에 사용할 수 없다는 것을 의미하며, 따라서 모든 현대적인 응용 프로그램에도 해당되지 않는다는 것을 의미합니다. 대신, 저자들은 BERT와 유사한 모델들이 비슷한 수준으로 능력을 갖추고 있음을 보여줍니다. 저자들의 데이터에 따르면, 동일한 규모의 DeBERTa가 언어 이해 작업에서 우월하다는 것을 나타낸다. 이는 상당히 강한 주장이며, 이것이 사실인지 아닌지 이해하기 위해 더 많은 실험이 필요합니다. 저자들이 원인 연구를 수행하지 않은 것은 유감스럽지만, 이러한 작업과 관련된 양방향 주의의 역할을 이해하는 것이 흥미로울 것입니다.\n\n매개 변수 수에 따라 능력이 확장되는 사실은 중요한 사실입니다. 10B 이상으로 LLM을 확장하는 이유 중 하나는 바로 신생 속성의 존재 때문입니다.\n\n이러한 신생 속성이 존재하는지 여부는 또 다른 문제입니다. 최근에는 확장 법칙 자체가 다소 의문이 제기되었습니다. 그러나 모델의 성능이 매개 변수 수, 훈련 데이터 차원 및 컴퓨팅 예산과 비례하여 증가하는 것이 원하는 동작이라는 사실은 여전히 유지됩니다.\n\n어쨌든, 이 작업은 매우 흥미로운데, 데이터가 BERT와 유사한 모델이 인텍스트 학습자일 수 있는 능력을 갖춘다는 것을 보여주고 있습니다. 우리는 어떤 학습 모드가 다른 것보다 어떤 이점을 가지는지 자세히 검토해야 합니다. 미래에는 새로운 모델이 혼합 모드로 훈련되거나 다른 방식을 찾아 그들의 이점을 결합하는 대안을 찾을 수 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 이에 대해 어떻게 생각하십니까? GPT와 유사한 LLM 지배력을 다시 평가해야 할 필요가 있다고 생각하십니까? 댓글로 알려주세요!\n\n# 이 내용이 흥미로우셨다면:\n\n제 다른 기사들을 찾아보거나 LinkedIn에서 저와 연락하실 수도 있습니다. 주간 업데이트되는 머신러닝 및 인공지능 뉴스가 포함된 이 저장소를 확인해보세요. 저는 협업과 프로젝트에 열려 있으며 LinkedIn을 통해 저에게 연락하실 수 있습니다. 또한 새로운 이야기를 게시할 때 알림을 받으려면 무료로 구독할 수 있습니다. \n\n제 GitHub 저장소 링크는 여기에서 확인하실 수 있습니다. 머신러닝, 인공지능 및 그 외 다른 자원과 관련된 코드 및 여러 자료를 수집하고 있는 곳입니다.\n\n<div class=\"content-ad\"></div>\n\n혹은 제 최근 글 중 하나에 관심 있을지도 모릅니다:\n\n# 참고 자료\n\n이 글을 작성하는 데 참고한 주요 참고 자료 목록입니다. 각 논문에 대해 첫 번째 저자의 이름만 인용되었습니다.\n\n- Radford, Improving Language Understanding by Generative Pre-Training, 링크\n- Devlin, 2019, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 링크\n- Brown, 2020, Language Models are Few-Shot Learners, 링크\n- Tay, 2023, UL2: Unifying Language Learning Paradigms, 링크\n- Samuel, 2024, BERTs are Generative In-Context Learners, 링크","ogImage":{"url":"/assets/img/2024-06-19-MaybeGPTIsnttheBestBERTsCanMasterGenerativeIn-ContextLearning_0.png"},"coverImage":"/assets/img/2024-06-19-MaybeGPTIsnttheBestBERTsCanMasterGenerativeIn-ContextLearning_0.png","tag":["Tech"],"readingTime":7},{"title":"Numerai Crypto 소개 암호화폐 시장 예측의 미래","description":"","date":"2024-06-19 03:48","slug":"2024-06-19-IntroducingNumeraiCryptoTheFutureofCryptocurrencyMarketPrediction","content":"\n\nNumerai는 분산화되고 대중화된 예측 시장의 선두 주자로 손꼽혀 왔습니다. 2017년에는 이더리움의 가치가 단지 12달러였던 시점에서 우리의 토큰 Numeraire(NMR)를 출시했습니다. 전 세계에서 수천 명의 가장 우수한 데이터 과학자들을 모아 매일 우리 플랫폼에 1300만 달러 가치 이상의 NMR을 걸고 협업하고 있습니다. 매일 수천 개의 머신 러닝 모델을 실행하여 매년 100억 건 이상의 세계 주식 예측을 생성합니다. 우리는 앙상블에 대한 모델 기여를 평가하는 고급 지표인 MMC를 만들어 독특한 신호 성능을 확인할 수 있도록 했습니다.\n\nNumerai와 Signals에 담긴 이 선도적 노력들은 예측 시장의 표준을 설정하며 이 분야에서 다양한 프로젝트들을 영감을 받게 했습니다. Numerai의 예측 시장의 규모는 2024년에만 글로벌 주식 거래 액면에서 20억 달러 이상의 거래량에 영향을 끼치고 있습니다.\n\n하지만... 왜 주식 시장일까요?\n\n아마도 주식 시장이 잘 정립되어 있고 트렌드가 잘 연구되어 있기 때문일 수도 있습니다... 또는 머신 러닝에 적합한 데이터가 가장 간단하고 고품질인 것이 원인일 수도 있습니다. Numerai 커뮤니티가 주식 시장을 예측하는 데 성공을 거두어 그것이 익숙하고 편안하기 때문일 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n주식 시장이 크라우드소싱 인텔리전스가 성공할 수 있는 유일한 곳인가요? 그리고 메타 모델은 Numerai의 전용 IP가 될 수밖에 없나요?\n\n아니요.\n\n오늘, Numerai는 처음으로 누구나 암호화폐의 미래를 예측할 수 있는 새로운 예측 플랫폼을 만들었습니다:\n\nNumerai Crypto\n\n<div class=\"content-ad\"></div>\n\n거래를 하려고 하는데요. 팔기 버튼 위에 커서를 올려놓고 \"전반적인 시장 매물 매입\" 속에서 최근 기사들을 의존하고 있습니다. 그런데 갑자기 미래의 라이브 피드를 보게 됩니다. 이 피드는 Numerai의 최고 데이터 과학자들에 의해 구축되었는데, 그들은 모든 데이터 소스와 인공 지능 기술을 사용하여 모델을 학습할 수 있습니다. 수천 개의 모델에서 나온 최고의 예측이 통합되어 당신에게 전달됩니다. 이 피드가 다가올 주에 시장을 과performers-performers로 예측할 수 있다면 어떨까요?\n\nNumerai는 이 상황을 상상할 뿐만 아니라 경험했습니다. 오늘부터 Numerai Crypto Meta Model을 12개월 동안 무료로 제공하게 됩니다. 이제 여러분도 이를 경험할 수 있습니다.\n\n지금 암호화폐의 미래를 보세요.\n\n# Numerai Crypto는 선두 주자입니다.\n\n<div class=\"content-ad\"></div>\n\n우리의 기존 플랫폼은 전통 주식 시장에만 초점을 맞추고 있었지만, 우리는 오랫동안 암호화폐 시장을 흥미롭게 생각해 왔습니다. Numerai Crypto를 통해 우리의 목표는 다음과 같습니다:\n\n- 우리 커뮤니티에 새롭고 매력적인 도전 과제를 제공합니다. 주식 시장 신호의 탐색 공간이 아직 다 exhaust 되지 않았지만, 우리는 커뮤니티가 암호 화폐 시장에서 얼마나 많은 알파를 얻을 수 있는지 알아보고 싶습니다. 또한 Numerai는 당사의 헤지 펀드를 구축한 커뮤니티를 강화하고 확장하기 위해, NMR의 세 번째 사용 사례를 제공함으로써 신뢰와 유틸리티를 높이려고 노력하고 있습니다.\n- 암호화폐 시장의 예측 가능성을 향상시킵니다. 암호화폐 시장은 비교적 탐험되지 않았습니다. 이 \"일반적인 판매 압박\"은 무엇을 이끌고 있으며 시장은 빨리 회복될까요? 신뢰할 수 있는 요인 모델은 아직 개발되지 않았고 고품질의 데이터 제공 업체가 희소하기 때문에 질문에 대답하기 어렵습니다. 우리는 이를 바꾸고자 노력합니다.\n- 집단 지성의 힘을 세계에 되돌려줍니다. 지금까지 Numerai만이 우리의 크라우드소싱 예측의 힘을 이용할 수 있었습니다. Numerai Crypto Meta Model을 공개함으로써 우리는 이 힘을 세계와 공유할 것입니다.\n\nNumerai는 참여자들의 예측이나 Numerai Crypto Meta Model을 거래에 사용하지 않겠지만, 이를 관찰하는 것은 흥미로울 것입니다. 암호화폐 헤지 펀드, 소매 투자자, MEV 탐색자 및 기타 시장 참가자들은 미래의 공개 피드를 검토하는 데 관심이 있을 수 있습니다. 우리는 Numerai 커뮤니티가 암호화폐 시장을 얼마나 잘 예측할 수 있는지, 개인 또는 기관이 이러한 예측을 거래 결정 또는 리스크 관리에 활용할지에 대해 기대하고 있습니다.\n\n# 암호화폐의 미래에 함께하세요!\n\n<div class=\"content-ad\"></div>\n\nNumerai Crypto는 새로운 도전 방법을 제공하며 데이터 과학을 배우고 암호화폐의 미래에 영향을 미치고 NMR을 벌거나 태우는 것이 가능합니다. 여러분이 참여할 수 있는 방법은 다음과 같습니다:\n\n- 가입하기: Numerai Crypto를 방문하여 계정을 생성하고 가입하세요. 이 계정으로 세계에서 가장 어려운 데이터 과학 대회에 참여할 수 있습니다.\n- 데이터 다운로드: 우리는 유동성이 있는 암호화폐들의 우주를 생성하고 이를 사용하여 과거의 수익 대상을 생성했습니다. 이 대상은 우리의 우주가 이후 4주 동안 경험한 순위별 수익을 대표합니다. 여러분의 역할은 모델이 시간이 지남에 따라 토큰들의 상대 수익을 학습하는 데 사용할 수 있는 올바른 예측 데이터를 찾는 것입니다.\n- 예측하고 제출하기: 매일 우리의 우주의 미래 순위별 수익을 예측하세요. 예측을 업로드하기 위해 웹사이트나 API(예: NumerAPI 또는 RNumerai와 같은 패키지를 통해)를 사용하세요. 저희 API는 여러분의 제출을 확인할 것입니다. 제출 예시는 다음과 같습니다:\n![numerai-crypto-submission](/assets/img/2024-06-19-IntroducingNumeraiCryptoTheFutureofCryptocurrencyMarketPrediction_0.png)\n\n- 관찰하기: 여러분이 예측을 제출한 후 약 일주일 후에 평가가 시작되며 실제 암호화폐 시장과 비교하여 그 다음 한 달 동안 진행됩니다. 이 기간 동안 여러분의 모델이 어떻게 수행되는지 확인하세요. 4주가 지나면 점수가 확정되며 여러분의 모델은 Numerai Crypto 리더보드에 위치하게 됩니다.\n- 스테이크하기: 원하는 경우 예측에 NMR을 걸어 성능에 따라 벌거나 태우는 옵션을 선택할 수 있습니다. 이 과정은 필수가 아니며 신뢰할 수 있는 모델에만 스테이크하세요. 모든 스테이크 참여자의 평균 예측은 Numerai Crypto 메타 모델을 형성합니다.\n- 탐색하고 공유하기: Numerai 커뮤니티 포럼과 디스코드에 가입하여 기계 학습 및 암호화폐 시장 전략에 대한 아이디어를 탐구, 실험하고 공유하세요.\n\n<div class=\"content-ad\"></div>\n\nNumerai Crypto에 대한 자세한 내용은 문서에서 확인할 수 있어요.\n\n# 곧 시작합니다\n\nNumerai Crypto 2024 시즌이 곧 시작됩니다. Numerai 그랜드마스터의 자격을 얻을 기회를 제공하며, 리더보드를 통해 랭킹을 올려 모델을 개선하고 고유한 데이터를 발견하여 커뮤니티에서 눈에 띄게 차별화되세요.\n\n또한 예제 모델을 공개하고 토큰 유니버스와 목표를 생성하는 데 사용된 코드를 오픈 소스로 공개하기 시작할 예정입니다. 이를 통해 Numerai와 Numerai Crypto에서 경쟁하는 데이터 과학자들이 함께 협력하여 최고 수준의 암호화폐 예측 시장을 만들어 나갈 수 있기를 바랍니다.\n\n<div class=\"content-ad\"></div>\n\nNumerai는 암호화폐 거래를 하지 않으며, Numerai의 헤지 펀드는 Numerai Crypto와 관련이 없습니다. Numerai Crypto 토너먼트는 실험적인 대회이며, Numerai Crypto 메타 모델은 투자 조언이 아니며 거래 결정에 사용되는 것을 의도하지 않습니다. 모델의 공개와 성능 추적은 정보 및 오락 목적으로만 사용됩니다. Numerai는 Numerai Crypto 메타 모델을 기반으로 한 개인 또는 기관의 재정 결정에 대해 책임지지 않습니다.","ogImage":{"url":"/assets/img/2024-06-19-IntroducingNumeraiCryptoTheFutureofCryptocurrencyMarketPrediction_0.png"},"coverImage":"/assets/img/2024-06-19-IntroducingNumeraiCryptoTheFutureofCryptocurrencyMarketPrediction_0.png","tag":["Tech"],"readingTime":5},{"title":"KAN  콜모고로프-아놀드 네트워크 뒤에 숨은 수학","description":"","date":"2024-06-19 03:45","slug":"2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks","content":"\n\n## 클래식 다층 퍼셉트론 대안이 나왔어요. 왜 더 정확하고 해석 가능한 건가요? 수학과 코드 심층 탐구.\n\n![다음 이미지 참고](/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_0.png)\n\nAI 세계에서 신경망은 끊임없는 혁신과 발전을 이끌어옵니다. 많은 획기적인 발전의 핵심에는 복잡한 함수를 근사하는 능력으로 유명한 다층 퍼셉트론(MLP)이 있습니다. 그러나 AI가 얼마나 많이 이루어 낼 수 있는지 경계를 늘릴 때, 우리는 클래식 MLP보다 더 나은 것을 할 수 있을까요?\n\n여기 Kolmogorov-Arnold Networks(KANs)가 나왔습니다. Kolmogorov-Arnold 표현 정리에서 영감을 받은 신경망에 대한 새로운 접근법입니다. 기존 MLP가 각 뉴런에서 고정 활성화 함수를 사용하는 반면, KANs는 네트워크의 가중치(엣지)에 학습 가능한 활성화 함수를 사용합니다. 이 간단한 변경은 정확성, 해석 가능성, 효율성에서 새로운 가능성을 열어줍니다.\n\n<div class=\"content-ad\"></div>\n\n이 기사는 KAN이 신경망 설계에서 혁명적인 발전을 이루는 이유를 탐구합니다. 우리는 그들의 수학적 기초에 대해 자세히 살펴보고, MLP(Multi-Layer Perceptrons)와의 주요 차이점을 강조하며, KAN이 전통적인 방법을 능가할 수 있는 방법을 보여줄 것입니다.\n\n# 1: MLP의 한계\n\n![이미지](/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_1.png)\n\n다층 퍼셉트론(MLP)은 현대 신경망의 핵심 구성 요소입니다. 각각의 뉴런은 데이터로부터 학습하여 복잡한 비선형 함수를 근사하는 것을 목표로 설계된 상호 연결된 노드 레이어로 구성되어 있습니다. 각 뉴런은 입력의 가중 합에 고정된 활성화 함수를 사용하여 입력 데이터를 원하는 출력으로 변환함으로써 여러 계층의 추상화를 통해 동작합니다. MLP는 컴퓨터 비전에서부터 음성 인식까지 다양한 분야에서 앞도적인 성과를 이루어 왔습니다.\n\n<div class=\"content-ad\"></div>\n\n그러나 MLP에는 몇 가지 중요한 제한 사항이 있습니다:\n\n- 노드에 고정된 활성화 함수: MLP의 각 노드에는 ReLU나 Sigmoid와 같은 미리 정의된 활성화 함수가 있습니다. 이러한 고정 함수는 많은 경우에 효과적이지만 네트워크의 유연성과 적응성을 제한합니다. 이는 MLP가 특정 유형의 함수를 최적화하거나 특정 데이터 특성에 적응하는 데 어려움을 겪을 수 있게 만들 수 있습니다.\n- 해석 가능성 문제: MLP는 종종 \"블랙 박스\"로 비판받습니다. 복잡해지면서, 그들의 의사 결정 과정을 이해하기가 더 어려워집니다. 고정된 활성화 함수와 복잡한 가중치 행렬은 네트워크의 내부 작업을 가리고, 깊이 분석 없이 모델의 예측을 해석하고 신뢰하기 어렵게 만듭니다.\n\n이러한 단점은 더 큰 유연성과 해석 가능성을 제공하는 대안이 필요함을 강조하며, Kolmogorov-Arnold Networks (KANs)와 같은 혁신을 위한 길을 열어줍니다.\n\n# 2: Kolmogorov-Arnold Networks (KANs)\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_2.png)\n\n콜모고로프-아놀드 표현 이론은 수학자 안드레이 콜모고로프와 블라디미르 아놀드가 정립한 이론으로, 이론은 다변수 연속 함수를 단일 변수의 연속 함수와 덧셈 작업의 유한 구성으로 표현할 수 있다고 주장합니다. 이 이론은 복잡한 레시피를 각각의 간단한 단계로 분해하는 것으로 생각할 수 있습니다. 전체 레시피 전체적으로 다루는 대신에 각 단계를 개별적으로 처리하여 전반적인 프로세스를 더 효율적으로 만듭니다. 이 이론은 복잡한 고차원 함수를 간단한 단일 변수 함수로 분해할 수 있다는 것을 시사합니다.\n\nKAN(Kolmogorov-Arnold Networks)은 콜모고로프-아놀드 이론의 힘을 활용하여 신경망의 구조를 근본적으로 변경합니다. 전통적인 MLP에서는 각 노드에 고정된 활성화 함수가 적용되지만, KAN은 네트워크의 가중치에 학습 가능한 활성화 함수를 배치합니다. 이러한 주요 차이점은 고정된 활성화 함수 세트가 아닌, KAN이 학습 중에 적용할 최상의 함수를 적응적으로 학습한다는 것을 의미합니다. KAN의 각 가중치는 데이터를 기반으로 한 다이내믹하고 세밀한 조정이 가능한 스플라인으로 매개변수화된 단일 변수 함수를 나타냅니다.\n\n이 변화는 네트워크의 유연성을 향상시키고 데이터의 복잡한 패턴을 포착할 수 있는 능력을 강화하여 전통적인 MLP에 대한 해석 가능하고 강력한 대안을 제공합니다. KAN은 가장 최적의 성능을 위해 엣지에 있는 학습 가능한 활성화 함수에 초점을 맞춘 결과, 다양한 AI 작업에서 성능이 향상됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 3: 수학적 기초\n\n콜모고로프-아놀드 네트워크(KANs)의 핵심은 이러한 네트워크가 입력 데이터를 처리하고 변환하는 방식을 정의하는 일련의 방정식입니다. KANs의 기초는 콜모고로프-아놀드 표현 정리에 있으며, 이는 네트워크의 구조와 학습 과정에 영감을 주는 개념입니다.\n\n입력 벡터 x=[x1,x2,…,xn]가 있다고 상상해보세요. 이 벡터는 처리하려는 데이터 포인트를 나타냅니다. 이 입력 벡터를 레시피의 재료 목록으로 생각해보세요.\n\n이 정리는 어떤 복잡한 레시피(고차원 함수)라도 보다 간단한 단계(일변량 함수)로 분해할 수 있다는 것을 명시합니다. KANs에서는 각 재료(입력 값)가 네트워크의 가장자리에 배치된 일련의 간단한 단계(일변량 함수)를 통해 변환됩니다. 이를 수학적으로 표현하면:\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_3.png)\n\n여기서 ϕ_q,p는 학습 중에 학습되는 단변량 함수입니다. ϕ_q,p를 각 재료에 대한 개별 조리 기술로 생각하고, Φ_q를 이러한 준비된 재료를 결합하는 최종 조립 단계로 생각해보세요.\n\nKAN의 각 층은 이러한 요리 기술을 사용하여 재료를 더 변형합니다. 층 l에 대해, 변형은 다음과 같이 주어집니다:\n\n![image](/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_4.png)\n\n<div class=\"content-ad\"></div>\n\n여기서 x(l)은 l번째 레이어에서 변환된 재료를 나타내고, ϕ_l,i,j는 l레이어와 l+1 레이어 사이의 학습 가능한 일변량 함수입니다. 이를 각 단계마다 재료에 다양한 조리 기술을 적용하여 중간 요리를 얻는 것으로 생각해보세요.\n\nKAN의 출력은 이러한 레이어 변환의 합성입니다. 중간 요리를 결합하여 최종 식사를 만드는 것처럼, KAN은 변환을 결합하여 최종 출력물을 생성합니다:\n\n![image](/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_5.png)\n\n여기서 Φl은 l번째 레이어의 일변량 함수 행렬을 나타냅니다. KAN의 전체 함수는 이러한 레이어의 합성으로, 각각이 변환을 더욱 세밀하게 다듬습니다.\n\n<div class=\"content-ad\"></div>\n\nMLP 구조\n전통적인 MLP에서 각 노드는 입력에 고정된 활성화 함수 (예: ReLU 또는 sigmoid)를 적용합니다. 이를 생각해보면 각각의 성질에 관계없이 모든 재료에 동일한 조리 기술을 적용하는 것과 같습니다.\n\nMLPs는 이러한 고정 비선형 활성화 함수에 이어서 선형 변환을 사용합니다:\n\n![MLP Structure](/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_6.png)\n\n여기서 W는 가중치 행렬을 나타내고, σ는 고정된 활성화 함수를 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n## 그리드 확장 기술\n\n![이미지](/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_7.png)\n\n그리드 확장은 Kolmogorov-Arnold Networks (KANs)의 정확도를 향상시키기 위해 사용되는 강력한 기술로, 단변량 함수가 정의된 스플라인 그리드를 세밀하게 다듬는 데 사용됩니다. 이 과정을 통해 네트워크는 완전 재교육이 필요하지 않고도 데이터의 점점 더 세부적인 패턴을 학습할 수 있습니다.\n\n이 B-스플라인은 부드러운 곡선을 형성하기 위해 결합된 일련의 다항 함수입니다. KANs에서는 가장자리의 단변량 함수를 나타내는 데 사용됩니다. 스플라인은 그리드 포인트라고 불리는 일련의 간격을 통해 정의됩니다. 그리드 포인트가 많을수록 스플라인이 캡처할 수 있는 세부 정보가 더욱 섬세해집니다.\n\n<div class=\"content-ad\"></div>\n\n![KANKolmogorov-ArnoldNetworks_8](/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_8.png)\n\n먼저, 네트워크는 거친 격자로 시작되어 그리드 포인트 사이의 간격이 적습니다. 이는 네트워크가 세부 사항에 깊이 빠지지 않고 데이터의 기본 구조를 학습할 수 있도록 합니다. 이는 세부 사항을 채우기 전에 대략적인 윤곽을 그리는 것과 유사합니다.\n\n학습이 진행됨에 따라 그리드 포인트의 수가 점진적으로 증가합니다. 이 과정을 그리드 세분화라고 합니다. 더 많은 그리드 포인트를 추가함으로써 스플라인이 더 자세해지고 데이터 내의 미세한 패턴을 잡을 수 있습니다. 이는 처음에 대략적인 스케치에 점차적으로 더 많은 세부 사항을 추가하여 자세한 그림으로 완성하는 것과 유사합니다.\n\n매 증가할 때마다 새로운 B-spline 기저 함수 B′_m(x)가 도입됩니다. 이러한 새로운 기저 함수에 대한 계수 c'_m은 새로운, 더 자세한 스플라인이 초기, 더 거친 스플라인과 밀접하게 일치하도록 조절됩니다.\n\n<div class=\"content-ad\"></div>\n\n이 작업을 수행하기 위해서는 최소 제곱 최적화가 사용됩니다. 이 방법은 계수 c'_m을 조정하여 원본 스플라인과 개선된 스플라인 간의 차이를 최소화합니다.\n\n![image](/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_9.png)\n\n본질적으로, 이 과정은 개선된 스플라인이 원시 스플라인에 의해 학습된 데이터 패턴을 정확하게 나타내도록 보장합니다.\n\n## 간소화 기법\n\n<div class=\"content-ad\"></div>\n\nKANs의 해석 가능성을 향상시키기 위해, 네트워크를 이해하고 시각화하기 쉽게 만들기 위해 여러 간소화 기술이 사용될 수 있습니다.\n\n희박화 및 가지치기\n이 기술은 활성화 함수의 L1 노름에 기반한 손실 함수에 패널티를 추가하는 것을 포함합니다. 함수 ϕ에 대한 L1 노름은 모든 입력 샘플을 대상으로 함수의 평균 크기로 정의됩니다:\n\n![수학 공식](/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_10.png)\n\n여기서 N_p는 입력 샘플 수이며, ϕ(x_s)는 입력 샘플 x_s에 대한 함수 ϕ의 값입니다.\n\n<div class=\"content-ad\"></div>\n\n스파스파이케이션은 방을 정리하는 것과 같습니다. 불필요한 항목을 제거하거나 중요하지 않은 기능을 줄임으로써 공간(또는 네트워크)을 더 정리하고 쉽게 이동할 수 있게 만듭니다.\n\nL1 규제를 적용한 후 활성화 함수의 L1 노름이 평가됩니다. 특정 임계값 이하의 노름을 가진 뉴런과 엣지는 중요하지 않다고 간주되어 제거됩니다. 가지치기를 위한 임계값은 가지치기를 얼마나 적극적으로 진행할지를 결정하는 하이퍼파라미터입니다.\n\n가지치기는 나무를 가지치는 것과 같습니다. 약한 또는 불필요한 가지를 제거함으로써, 나무가 더 강하고 중요한 부분에 리소스를 집중할 수 있게 되어 더 건강하고 관리하기 쉬운 구조를 만들어냅니다.\n\n상징화\n다른 방법은 배운 단변량 함수를 알려진 기호 형태로 대체하여 네트워크를 보다 해석하기 쉽게 만드는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n잠재적인 상징적 형식 (예: sin⁡, exp)을 식별하는 작업입니다. 이 단계는 학습된 함수를 분석하고 모양과 행동에 기초하여 상징적 후보를 제안하는 것을 포함합니다.\n\n상징적 후보가 식별되면 상징적 함수가 학습된 함수를 근사화하도록 매개변수를 적합시키기 위해 그리드 서치 및 선형 회귀를 사용하십시오.\n\n# 4: KAN vs MLP in Python\n\nKolmogorov-Arnold Networks (KANs)와 전통적인 Multi-Layer Perceptrons (MLPs)의 능력을 시범하기 위해, PyTorch를 활용하여 함수를 생성한 데이터세트에 KAN 모델과 MLP 모델을 모두 적합시켜보겠습니다. 이들의 성능이 어떻게 보이는지 확인해봅니다.\n\n<div class=\"content-ad\"></div>\n\n우리가 사용할 함수는 KAN의 능력을 MLP(원본 논문 예제)과 비교하기 위해 논문 저자들이 사용한 것과 동일합니다. 그러나 코드는 다를 것입니다. 오늘 다룰 모든 코드는 이 노트북에서 찾을 수 있습니다:\n\n필요한 라이브러리를 가져와 데이터셋을 생성해 봅시다.\n\n```js\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torchsummary import summary\nfrom kan import KAN, create_dataset\nimport matplotlib.pyplot as plt\n```\n\n여기서 우리가 사용하는 것입니다:\n\n<div class=\"content-ad\"></div>\n\n- numpy: 숫자 연산에 사용됩니다.\n- torch: PyTorch 용으로, 신경망 구축 및 훈련에 사용됩니다.\n- torch.nn: PyTorch 내에서의 신경망 모듈에 사용됩니다.\n- torchsummary: 모델 구조를 요약하는 데 사용됩니다.\n- kan: KAN 모델 및 데이터셋 생성 함수를 포함하는 사용자 지정 라이브러리입니다.\n- matplotlib.pyplot: 그래프 그리기와 시각화에 사용됩니다.\n\n```js\n# 데이터셋 생성 함수 정의\nf = lambda x: torch.exp(torch.sin(torch.pi * x[:, [0]]) + x[:, [1]] ** 2)\n```\n\n이 함수에는 삼각함수(sin)와 지수함수(exp) 요소가 모두 포함되어 있습니다. 이 함수는 2차원 입력 x를 취하고 다음 공식을 사용하여 출력을 계산합니다:\n\n<img src=\"/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_11.png\" />\n\n<div class=\"content-ad\"></div>\n\n이제 [-2, 2] 사이에서 균일하게 분포된 100개의 점의 텐서를 이 함수에 맞춰 보겠습니다.\n\n![function](/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_12.png)\n\n```js\n# 데이터셋 생성\ndataset = create_dataset(f, n_var=2)\n```\n\ncreate_dataset은 함수 f를 기반으로 데이터셋을 생성합니다. 이 데이터셋에는 신경망의 훈련 및 테스트에 사용될 입력-출력 쌍이 포함됩니다.\n\n<div class=\"content-ad\"></div>\n\n자, 이제 데이터셋을 사용하여 KAN 모델을 구축하고 훈련시킬 차례입니다. \n우리는 먼저 더 넓은 격자 (5 포인트)로 시작하여 점진적으로 더 섬세한 세부 사항을 캡처하기 위해 그것을 미세 조정할 것입니다(최대 100 포인트까지).\n이것은 데이터의 세부 사항을 잡아내어 모델의 정확도를 향상시킵니다.\n\n```js\ngrids = np.array([5, 10, 20, 50, 100])\ntrain_losses_kan = []\ntest_losses_kan = []\nsteps = 50\nk = 3\n\nfor i in range(grids.shape[0]):\n    if i == 0:\n        model = KAN(width=[2, 1, 1], grid=grids[i], k=k)\n    else:\n        model = KAN(width=[2, 1, 1], grid=grids[i], k=k).initialize_from_another_model(model, dataset['train_input'])\n    results = model.train(dataset, opt=\"LBFGS\", steps=steps, stop_grid_update_step=30)\n    train_losses_kan += results['train_loss']\n    test_losses_kan += results['test_loss']\n\n    print(f\"Train RMSE: {results['train_loss'][-1]:.8f} | Test RMSE: {results['test_loss'][-1]:.8f}\")\n```\n\n이 예시에서 우리는 [5, 10, 20, 50, 100]의 값으로 grids라는 배열을 정의했습니다. 이러한 격자들을 순차적으로 모델 적합에 사용하며, 새 모델을 이전 모델을 사용하여 초기화합니다.\n\n각 반복마다, k=3인 모델을 정의합니다. 여기서 k는 B-스플라인의 순서를 나타냅니다. 훈련 단계(또는 에포크) 수를 50으로 설정합니다. 모델의 아키텍처는 2개의 노드를 가진 입력 레이어, 1개의 노드를 가진 은닉 레이어 및 1개의 노드를 가진 출력 레이어로 구성됩니다. LFGBS 옵티마이저를 사용하여 훈련합니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 훈련 과정 중의 훈련 및 테스트 손실입니다:\n\n![loss graph](/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_13.png)\n\n이제 전통적인 MLP를 정의하고 훈련하여 비교해 보겠습니다.\n\n```python\n# MLP 정의\nclass MLP(nn.Module):\n    def __init__(self):\n        super(MLP, self).__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(dataset['train_input'].shape[1], 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n    def forward(self, x):\n        return self.layers(x)\n\n# 모델 인스턴스화\nmodel = MLP()\nsummary(model, input_size=(dataset['train_input'].shape[1],))\n```\n\n<div class=\"content-ad\"></div>\n\nMLP는 입력 레이어, 64개의 뉴런을 가진 두 개의 히든 레이어 및 출력 레이어를 가지고 있습니다. 레이어 간에는 ReLU 활성화 함수가 사용됩니다.\n\n```js\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\ntrain_loss_mlp = []\ntest_loss_mlp = []\n\nepochs = 250\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    output = model(dataset['train_input']).squeeze()\n    loss = criterion(output, dataset['train_label'])\n    loss.backward()\n    optimizer.step()\n    train_loss_mlp.append(loss.item()**0.5)\n\n    # 모델 테스트\n    model.eval()\n    with torch.no_grad():\n        output = model(dataset['test_input']).squeeze()\n        loss = criterion(output, dataset['test_label'])\n        test_loss_mlp.append(loss.item()**0.5)\n\n    print(f'에폭 {epoch+1}/{epochs}, 훈련 손실: {train_loss_mlp[-1]:.2f}, 테스트 손실: {test_loss_mlp[-1]:.2f}', end='\\r')\r\n```\n\n평균 제곱 오차 (MSE) 손실과 Adam 옵티마이저를 사용하고 모델을 250 에폭 동안 훈련하여 훈련 및 테스트 손실을 기록합니다.\n\nMLP에서 훈련 및 테스트 RMSE가 어떻게 나타나는지 보여드리겠습니다:\n\n<div class=\"content-ad\"></div>\n\n아래는 비교를 위해 손실 그래프를 나란히 두어 보겠습니다:\n\n![](/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_15.png)\n\n이 그래프는 KAN 모델이 MLP 모델보다 더 낮은 훈련 RMSE를 달성하여 더 나은 함수 맞추기 능력을 나타낸다는 것을 보여줍니다. 마찬가지로, KAN 모델은 시험 세트에서 MLP를 능가하여 뛰어난 일반화 능력을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n이 예시는 KAN이 유연하고 적응적인 구조 덕분에 복잡한 함수들을 전통적인 MLP보다 더 정확하게 맞출 수 있다는 것을 보여줍니다. 격자를 세밀하게 조정하고 가장자리에 학습 가능한 일변량 함수를 사용함으로써, KAN은 MLP가 놓치는 데이터의 복잡한 패턴을 포착하여 함수 맞추기 작업에서 성능을 향상시킵니다.\n\n그렇다면 우리는 영구적으로 KAN 모델로 전환해야 한다는 것을 의미합니까? 꼭 그렇지는 않습니다.\n\n이 예시에서 KAN은 훌륭한 결과를 보여주었지만, 다른 실제 데이터 시나리오에서 KAN을 시험한 결과, MLP가 더 좋은 성능을 내는 경우가 많았습니다. KAN 모델을 사용할 때 주목해야 할 점은 하이퍼파라미터 최적화에 대한 민감성입니다. 또한, KAN은 주로 스플라인 함수를 사용하여 테스트되었는데, 이는 저희 예시와 같이 부드럽게 변하는 데이터에는 잘 맞지만 다른 상황에서는 그렇지 않을 수 있습니다.\n\n요약하자면, KAN은 분명히 매력적이고 많은 잠재력을 가지고 있지만, 실제로 효과적으로 작동하기 위해서는 다른 데이터셋 및 알고리즘 내부 동작에 대해 더 많은 연구가 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n# 5: KAN의 장점\n\n## 정확성\n\nKolmogorov-Arnold Networks (KANs)의 두드러진 장점 중 하나는 전통적인 Multi-Layer Perceptrons (MLPs)에 비해 적은 매개변수로 더 높은 정확도를 달성할 수 있는 능력입니다. 이는 주로 에지에 있는 학습 가능한 활성화 함수 때문에 가능한데, 이는 KAN이 데이터 내의 복잡한 패턴과 관계를 더 잘 포착할 수 있게 합니다.\n\n각 노드에 고정된 활성화 함수를 사용하는 MLP와 달리, KAN은 에지에서 단변량 함수를 사용하여 네트워크를 더 융통성 있게 만들고 학습 프로세스를 데이터에 더 잘 맞출 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\nKANs은 층간 기능을 동적으로 조절할 수 있기 때문에 더 적은 매개변수로 비교적 높은 정확도를 얻을 수 있습니다. 이 효율성은 데이터나 계산 리소스가 제한된 작업에 특히 유용합니다.\n\n## 해석가능성\n\nKANs은 전통적인 MLPs에 비해 해석력을 크게 향상시킵니다. 이 향상된 해석력은 의사 결정 과정을 이해하는 것이 결과만큼 중요한 응용 프로그램에 중요합니다.\n\nKANs은 희소화(sparsification) 및 가지치기(pruning)와 같은 기술을 통해 단순화될 수 있습니다. 이러한 기술은 해석력을 향상시키는 뿐만 아니라 가장 관련성이 높은 구성 요소에 집중함으로써 네트워크의 성능을 향상시킵니다.\n\n<div class=\"content-ad\"></div>\n\n일부 함수의 경우 활성화 함수의 기호적 형태를 식별할 수 있으며, 이를 통해 네트워크 내에서 발생하는 수학적 변환을 이해하기 쉬워집니다.\n\n## 확장성\n\nKAN은 MLP와 비교했을 때 더 빠른 신경 확장 법칙을 나타내며, 매개변수의 수가 증가함에 따라 더 신속하게 개선됩니다.\n\nKAN은 복잡한 함수를 더 단순한 단변량 함수로 분해할 수 있는 능력으로 인해 보다 유리한 확장 법칙을 갖게 되어, MLP보다 모델 복잡도가 증가함에 따라 더 효율적으로 낮은 오류율을 달성할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nKANs는 훈련 중에 더 미세한 그리드까지 더욱 세분화된 그리드로 시작하여 계산 효율성과 정확도를 균형있게 유지하도록 합니다. 이 접근 방식은 MLPs보다 KANs가 보다 우아하게 확장될 수 있게 하며, 모델 크기를 증가시킬 때 완전히 재훈련이 필요한 MLPs보다 우수한 성능을 제공합니다.\n\n# 결론\n\nKolmogorov-Arnold Networks (KANs)는 전통적인 Multi-Layer Perceptrons (MLPs)에 비해 혁신적인 대안을 제시하여 이전 모델의 한계를 해소하는 핵심 혁신을 제공합니다. KANs는 노드에서 고정된 함수 대신 가장자리에 가중치가 있는 활성화 함수를 활용함으로써 새로운 수준의 유연성과 적응성을 도입합니다. 이 구조적인 변화로 인해 다음과 같은 장점을 제공합니다:\n\n- 향상된 정확도: 더 적은 매개변수로 더 높은 정확도를 달성하는 KANs는 다양한 작업에 더 효율적이고 효과적입니다.\n- 향상된 해석력: KANs를 시각화하고 단순화할 수 있는 능력은 건강 관리, 금융 및 자율 시스템과 같은 중요한 응용 분야에서 의사 결정 과정을 이해하는 데 도움이 됩니다.\n- 더 나은 확장성: KANs는 더 빠른 신경 확장 법칙을 나타내며 MLPs보다 증가하는 복잡성을 우아하게 처리할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n콜모고로프-아놀드 네트워크 소개는 신경망 분야에서 흥미로운 발전을 의미합니다. 이는 AI 및 머신 러닝에 대한 새로운 가능성을 열어줍니다.\n\n# 참고 자료\n\n- Ziming Liu, 등. “KAN: 콜모고로프-아놀드 네트워크”. https://arxiv.org/abs/2404.19756","ogImage":{"url":"/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_0.png"},"coverImage":"/assets/img/2024-06-19-TheMathBehindKANKolmogorov-ArnoldNetworks_0.png","tag":["Tech"],"readingTime":14},{"title":"플라토닉 인공 지능 모든 인공 지능이 같아지고 있는가","description":"","date":"2024-06-19 03:43","slug":"2024-06-19-PlatonicAIAreAllAIsBecomingtheSame","content":"\n\n몇 일 전에 AI와 철학을 결합한 새로운 이론을 발견했어요: \"AI가 모두 같은 것이 되고 있는 걸까?\" 시사하는 것은, 앞으로 모든 AI 모델이 동일해질까요?\n\n실제로 연구자들은 이미 이것이 벌어지고 있다는 명확한 증거를 발견했습니다. 모든 모델들이 '플라톤적' 표현을 향해 수렴하는 것처럼 보이죠:\n\n세계를 이해하는 하나의 독특한 방법.\n\n만약 이게 사실이라면, 경제학적으로나 철학적으로 엄청난 파장을 불러올 수 있습니다. 왜냐하면 이것은 AI가, 기초 모델 덕분에 현실 자체를 발견, 설명하고, 중요한 것은 예측하기 시작하고 있다는 신호가 될 수 있기 때문이죠.\n\n<div class=\"content-ad\"></div>\n\n# 표현\n\n이 질문을 제기하는 연구자들을 이해하기 위해 우리는 모델이 세계를 해석하는 방법을 이해해야 합니다.\n\n그리고 그것이 바로 표현을 통해 이루어집니다.\n\n## 인공지능에서 가장 중요한 단어\n\n<div class=\"content-ad\"></div>\n\n세상의 모든 개념을 학습할 때마다, AI 모델들은 키 속성을 포착하는 방식으로 그것을 설명하는 요약된 표현을 만듭니다.\n\n특히, AI 모델들은 모든 개념을 임베딩으로 변환하여 벡터 형태로 표현합니다.\n\n그런데 왜 벡터인 걸까요? 세상의 모든 것을 벡터로 만들면 두 가지 이점이 있습니다:\n- 숫자 형태의 개념: 기계는 숫자만 해석할 수 있습니다. 따라서 모든 것을 숫자 형태로 어떤 방식으로든 해석해야 합니다.\n- 유사성: 모든 것을 벡터로 만들어두면, 그들 사이의 거리를 측정할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n따라서 모델은 OpenAI가 정의한 관련성이라는 원칙에 의해 지배되는 세계 개념의 고차원(벡터 당 많은 숫자) 공간을 구축합니다.\n\n이 내부 세계에서 두 실제 세계 개념이 더 유사할수록, 이 공간에서 그들의 벡터가 더 가까워져야 합니다.\n\n예를 들어 이 공간에서 '개'와 '고양이'는 '개'와 '창문'보다 가깝지만 '개'와 '비둘기'보다도 가깝습니다. 왜냐하면 '개'와 '고양이'가 더 의미론적으로 관련되어 있기 때문입니다(비행하지 않는, 네 다리, 가정적 등).\n\n![image](/assets/img/2024-06-19-PlatonicAIAreAllAIsBecomingtheSame_0.png)\n\n<div class=\"content-ad\"></div>\n\n미지공간(이렇게 불리는 공간들)의 매력적인 점은 양방향으로 작동한다는 것입니다. AI 모델에게 세계의 개념을 가르치는 데 도움을 주는 것뿐만 아니라, 아직까지 인간들이 깨닫지 못한 세계 패턴들을 발견하는 데도 도움을 줄 수 있습니다.\n\n예를 들어:\n\n- 미지 공간을 중심으로 한 의미 공간 이론은 Hume.ai에 의해 인간 감정의 새로운 매핑을 발견하는 데 도움이 되고 있습니다.\n- Osmo.ai가 이끄는 연구는 세계의 냄새를 매핑하고 새로운 냄새를 발견하기 위해 보간하는 것이 도움이 되고 있습니다.\n\n![이미지](/assets/img/2024-06-19-PlatonicAIAreAllAIsBecomingtheSame_1.png)\n\n<div class=\"content-ad\"></div>\n\n하지만 이게 어떻게 가능한 걸까요?\n\n사실 AI는 인간보다 패턴 매칭에서 뛰어나며, 우리에게는 무의식적으로 보였거나 우리가 인식하기에 너무 편향되었던 데이터의 주요 패턴을 찾아냅니다 (AI는 서양 문화와 다른 문화를 이어주는 역할을 합니다; 예를 들어, 서양이 아닌 국가의 사람들은 특정 감정을 다르게 표현합니다).\n\n따라서, 만약 AI가 현실에 대한 '편향되지 않은' 시각을 갖추고 있다면, 그들은 현실을 그대로 관찰할 수 있는 능력을 갖고 있을까요?\n\n연구자들이 추측한 대답은 네, 라고 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 동굴의 애니콜\n\nAI들이 모든 관찰이나 사건을 일으키는 현실의 진정한 본성을 배우는 것이 가능할까요?\n\n## 거리가 중요합니다\n\n만약 그게 가능하다면, 우리는 AI 교육을 충분히 성숙시켜서 우리의 기초 모델들이 현실을 해석하는 진정한 방법이 하나뿐이라는 동일한 모델로 진화될 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\n이 이론을 검증하기 위해 연구원들은 이러한 모델들의 표현이 모두 세계에 대한 단일 표현으로 수렴해야 한다고 주장했습니다. 이는 현실을 표현하는 객관적이고 보편적인 방법입니다.\n\n하지만 이게 일어나고 있는지 어떻게 테스트할 수 있을까요? 이를 위해 연구원들은 여러 인기 모델의 잠재 공간을 비교했습니다.\n\n우리가 기억한다면, AI 모델의 세계 표현은 고차원 공간입니다 (이전 다이어그램에서 간단히 세 개의 차원으로 표현됨) 비슷한 것들이 가까이에 있고, 다른 개념들은 멀리 밀려 있습니다.\n\n그러나 개념 표현의 전체 분포뿐만 아니라 거리도 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n그것은 하나의 모델이 '빨간색'에 할당하는 표현이 동일한 모드(다른 LLMs와 비교) 내 다른 모델과 유사해야 할뿐만 아니라 언어 모델과 시각 모델이 '적색'을 해석하는 방식도 유사해야 한다는 뜻입니다.\n\n간단히 말해, 모델 간 '적색'과 '파란색' 사이의 거리가 동일해야 하며, 이렇게 함으로써 각 모델이 현실이 제시하는 색 '빨간색'을 어떻게 해석하는지가 동일함을 입증하게 됩니다. 이렇게 두 모델이 '빨간색'이라는 개념으로 수렴하고 있다는 것을 증명하는 것이죠.\n\n그런데 왜 ‘플라톤적 표현’이라고 부를까요?\n\n**현실의 부분적 관점**\n\n<div class=\"content-ad\"></div>\n\n특히, 연구자들은 플라톤의 동굴 비유를 인용합니다. 현재 모델에 공급하는 데이터는 그림자이며, 이는 현실의 모호한 대표물입니다. 이전 AI 시스템은 그림자를 지켜보는 사람들로, 이는 그들이 삶의 부분적인 면밖에 알지 못한다는 것을 의미합니다.\n\n하지만 규모와 강제적인 다중 작업을 통해, 기초 모델들은 그들의 데이터를 초월하여 결국 동굴에서 나와 진정한 현실의 본질을 깨달을 것입니다.\n\n그러나 이것이 의미하는 것은 무엇일까요?\n\n<div class=\"content-ad\"></div>\n\n아래 이미지를 살펴보면, fimage와 ftext는 동일한 개념의 세계를 관찰하기 때문에, 그들의 표현 - 즉 벡터 -은 동일해야합니다.\n\n![이미지](/assets/img/2024-06-19-PlatonicAIAreAllAIsBecomingtheSame_3.png)\n\n모든 것이 좋아 보이지만, 이것이 일어나고 있는지 어떤 지표가 있을까요?\n\n# 수렴\n\n<div class=\"content-ad\"></div>\n\n매혹적으로, 기반 모델들은 수렴하고 있습니다.\n\n비전 모델들과 LLM 세트를 비교할 때, 내부 표현의 유사성인 정렬(alignment)은 LLM 성능과 비전 모델 간의 거의 1대1 상관 관계를 갖습니다.\n\n쉽게 말해, 우리 LLM이 더 잘 수행할수록, 다른 강력한 비전 모델들과의 표현이 얼마나 유사해지는지가 증가하며 완전히 다른 형태로 제시되었음에도 불구하고 유사해집니다. 이것은 또 다른 말로,\n\nLLM이 더 나아질수록, 그들의 세계 이해가 비전 모델로 수렴한다는 신호입니다. 모델이 커짐에 따라, 그들의 모드나 사용된 데이터에 관계없이, 모든 것이 동일한 표현, 동일한 현실에 대한 이해로 수렴하며 그 결과 더 똑똑해집니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Platonic AI: Are All AIs Becoming the Same](/assets/img/2024-06-19-PlatonicAIAreAllAIsBecomingtheSame_4.png)\n\n이러한 경향이 발생하는 이유는 아래 이미지에서 명확하게 나타났습니다: 모델이 더 많은 문제를 해결하기 위한 공통적인 방법을 찾아야 하므로 두 작업에 대한 가능한 해결책 공간이 줄어듭니다:\n\n![Platonic AI: Are All AIs Becoming the Same](/assets/img/2024-06-19-PlatonicAIAreAllAIsBecomingtheSame_5.png)\n\n결국, 모델이 더 크고 훈련되는 기술 집합이 더 광범위할수록 이러한 모델들은 서로 수렴하는 경향이 있습니다. 사용된 모달리티와 데이터셋과는 독립적으로, 어느 날 언젠가 우리 모두의 선도 연구소가 동일한 모델을 만들어내도록 수렴할 가능성을 그려낼 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 중요한 질문하기\n\n인공지능과 철학의 만남.\n\n나에게 있어 가장 중요한 관심사는 이것이 시장에 엄청난 재화화 효과를 줄 수 있다는 사실 외에도, 이것이 우리 인간들이 세계 모델을 만드는 방향으로 다가가고 있다는 첫 번째 신호일 수 있다는 점입니다. 세계 모델은 로봇이 언젠가 우리 세상에서 살아가는 모습을 상상하는 등 거의 모든 인공지능 분야에서 절대적으로 중요하다고 생각됩니다.\n\n<div class=\"content-ad\"></div>\n\n세계 모델은 인공 지능에게 환경을 관찰하고 적응할 수 있는 능력을 제공합니다. 인간 뇌는 계속해서 다음에 일어날 일을 추론(예측)하고 있습니다.\n\n만약 인간들이 진정으로 일반적인 세계 모델을 훈련하는 방법을 알아낸다면, 인공 지능이 어떻게 변할지를 말로 표현하기 어려울 것입니다. 가장 강력한 모델들이 세계의 동일한 표현으로 수렴되는 것을 보면, 적어도 해결책에 접근하고 있다는 신호로 받아들일 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-PlatonicAIAreAllAIsBecomingtheSame_0.png"},"coverImage":"/assets/img/2024-06-19-PlatonicAIAreAllAIsBecomingtheSame_0.png","tag":["Tech"],"readingTime":5},{"title":"Pinterest의 Ray 인프라","description":"","date":"2024-06-19 03:39","slug":"2024-06-19-RayInfrastructureatPinterest","content":"\n\n| 이름 | 직급 |  \n| --- | --- |  \n| Chia-Wei Chen | 주임 소프트웨어 엔지니어 |  \n| Raymond Lee | 주임 소프트웨어 엔지니어 |  \n| Alex Wang | 소프트웨어 엔지니어 I |  \n| Saurabh Vishwas Joshi | 주임 스태프 소프트웨어 엔지니어 |  \n| Karthik Anantha Padmanabhan | 주임 엔지니어 관리자 |  \n| Se Won Jang | 주임 엔지니어 관리자 |  \n\n# 우리 Ray 인프라의 여정\n\n저희 블로그 시리즈의 제1부에서는 Ray에 투자하여 비즈니스의 핵심적인 문제를 해결하기 위해 동기부여를 받았던 이유에 대해 논의했습니다. 이 블로그 포스트에서는 Pinterest와 같은 웹 규모 회사에 Ray를 통합하는 데 필요한 단계에 대해 더 자세히 설명하겠습니다. 우리는 새로운 기술을 받아들이기 위해 다양한 고유한 제약 조건과 도전에 직면한 Pinterest와 같은 기업에 Ray를 통합하는 데 필요한 것들을 논의할 것입니다. 이는 Ray 인프라 부분에 대한 더 포괄적인 버전으로, 우리가 Ray 설명에서 발표한 Last Mile Data Processing for ML Training using Ray in Ray Summit 2023의 일환입니다.\n\n저희의 사용 사례에서 KubeRay가 제공하는 Ray 클러스터를 프로비저닝할 수 있는 능력이 Ray 인프라를 성숙화하는 일의 일부일 뿐입니다. 회사들은 Ray 및 다른 특정 요구사항을 준수해야 하며, 이는 로그, 메트릭 지속성, 네트워크 격리, 최적의 하드웨어 인스턴스 식별, 보안, 트래픽 설정 및 기타 내부 서비스 통합을 포함한 모든 다른 모베스트 관행을 따라야 합니다.\n\n<div class=\"content-ad\"></div>\n\n2023년에 여정이 시작되었어요. 전문 엔지니어 한 명이 이 프로젝트에 50%의 시간을 투자했어요:\n\n- 2023년 1분기: Anyscale 파트너의 지원을 받아 프로토타이핑 단계가 시작되었어요\n- 2023년 2분기: Ray Infra MVP가 완료되었고, 시스템 로깅, 메트릭, UI, 및 애플리케이션을 위한 CLI와 같은 필수 도구들이 반복되고 향상되었어요\n- 2023년 3분기: 첫 번째 프로덕션 유즈 케이스에 중점을 두어 내부 시스템을 통합하여 서비스 안정성을 향상시켰어요\n- 2023년 4분기: 프로덕션 준비에 중점을 두어 보안 문제 대응, 네트워크 안정성 향상, Ray-optimized Kubernetes 환경으로의 전환 평가가 진행되었어요\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_0.png)\n\n# 직면한 도전들\n\n<div class=\"content-ad\"></div>\n\n핀터레스트에서 Ray 인프라를 구축할 때 몇 가지 중요한 도전 과제가 발생했습니다. 이러한 도전 과제를 해결해야 했습니다:\n\n- K8s API 접근 제한: 핀터레스트의 일반 목적의 연합 Kubernetes 클러스터인 PinCompute에서 작동 중이었기 때문에 KubeRay와 같은 필수 오퍼레이터와 해당 사용자 정의 정의를 설치하는 것이 제한되었습니다.\n- 일시적인 로깅 및 메트릭: Ray 클러스터가 활성 상태인 경우 Ray 대시보드를 통해 로깅과 메트릭을 확인할 수 있었지만, 리소스 집약적인 Ray 클러스터를 디버깅 목적으로 유지하는 것은 현실적이지 않았습니다. Ray 작업 부하의 수명 주기를 영속화하고 다시 재생하는 해결책을 찾았습니다.\n- 메트릭 통합: 회사는 프로메테우스와 그라파나와 같은 인기 있는 오픈 소스 솔루션과 달리 자체의 시계열 데이터베이스와 시각화 도구를 보유하고 있었습니다.\n- 인증, 권한 부여, 감사 (AAA) 가이드라인: 회사 표준에 따라 K8s에서 실행되는 서비스에 AAA를 보장해야 했고, 핀터레스트에서 AAA를 빌드하기 위해 Envoy를 서비스 메시로 사용하는 것이 권장되었습니다.\n- 다양한 개발 환경: Jupyter와 CLI 액세스와 같은 상호 작용하는 옵션과 다양한 개발자 요구를 충족하기 위해 Dev 서버에서 CLI 액세스와 같은 다양한 개발 경험이 구축되었습니다.\n- 비용 최적화 및 자원 낭비: 유휴 상태의 Ray 클러스터는 상당한 비용이 발생할 수 있습니다. 팀의 인식을 높이고 자원 낭비를 줄이기 위해 가비지 컬렉션 정책과 비용 할당이 필요했습니다.\n- 오프라인 데이터 분석: 오프라인 분석을 위해 모든 Ray 클러스터 관련 메트릭을 대규모 데이터 형식 (예: 하이브, 파케)으로 내보내는 것이 우선되었습니다. 이 데이터에는 GPU 활용률과 같은 메트릭이 포함되어 있어 개선 영역을 식별하고 응용 프로그램 및 인프라의 안정성을 시간 경과에 따라 추적할 수 있습니다.\n\n## 쿠버네티스 기반\n\nK8s API 액세스 제한으로 인해 우리 환경에 KubeRay를 쉽게 설치할 수 없어 K8s에서 Ray 클러스터를 운영하기 어려웠습니다. 또한, 핀터레스트 K8s 클러스터 내에서 비밀 관리, 트래픽 처리 및 로그 회전과 같은 작업을 위해 다른 팀에서 관리되는 특정 사이드카가 필요했습니다. 버그 수정이나 보안 패치와 같은 필수 사이드카 업데이트를 중앙 제어하기 위해 특정 제한 사항에 따라 준수해야 했습니다.\n\n<div class=\"content-ad\"></div>\n\nRay 클러스터에 필요한 필수 구성 요소를 프로토타입으로 만드는 중입니다(On-Premise Cluster 가이드에서 설명된 대로), 필요한 사이드카를 통합하기 위해 Pinterest 특정 CRD를 사용하기로 결정했습니다. 이는 오픈 소스 Kubeflow PyTorchJob 위에 구축된 래퍼입니다.\n\n최초 반복에서는 Ray 헤드 및 Ray worker를 클라이언트 측에서 만들어 간단하게 유지하기로 하였습니다. 각 구성 요소에 대해 다른 명령을 사용하고 클라이언트 측에서 실행될 사용자 정의 스크립트를 작성하는 것을 포함했습니다.\n\n```js\ndef launch_ray_cluster(configs: RayClusterConfig) -> str:\n    # resources, instance_type, command, envs_vars 등 정의\n    configs = RayClusterAndJobConfigs()\n    with ThreadPoolExecutor() as executor:  \n        # 함수를 실행자에 제출\n        ray_head = executor.submit(launch_ray_head(configs)).result()\n        ray_workers = executor.submit(launch_ray_workers(configs).result()\n    return check_up_and_running(ray_head, ray_workers)\n```\n\n이 단계에는 개선할 여지가 많이 있습니다. 주요 단점은 클라이언트 측 실행이 네트워크 오류나 만료된 자격 증명과 같은 다양한 이유로 중단될 수 있어 K8s에서 자원을 낭비하는 좀비 Ray 클러스터가 발생할 수 있다는 것입니다. 이 접근 방식은 Ray 사용자들이 Ray에서 놀 수 있도록 막는 데 충분하지만 Ray 클러스터를 효율적으로 관리하기 위해 설계된 플랫폼에 적합하지는 않습니다.\n\n<div class=\"content-ad\"></div>\n\n# API Gateway & Controller\n\n두 번째 반복에서는 클라이언트 측에서 Ray 클러스터를 관리하는 방식에서 KubeRay와 유사한 컨트롤러를 개발하여 서버 측 접근 방식으로 전환되었습니다. 우리의 솔루션은 사용자와 K8s 사이에 중간 계층을 생성하여 API 서버, Ray 클러스터/작업 컨트롤러 및 MySQL 데이터베이스로 구성된 여러 구성 요소를 포함했습니다.\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_1.png)\n\n- API 서버: 이 구성 요소는 요청 유효성 검사, 인증 및 권한 부여를 용이하게 합니다. 클라이언트 측에서 K8s의 복잡성을 추상화하여 사용자가 플랫폼 API 인터페이스와 상호 작용할 수 있도록 하며, 특히 나중 섹션에서 TLS 관련 구현을 강화하는 데 특히 유용합니다.\n- MySQL 데이터베이스: 데이터베이스는 Ray 클러스터와 관련된 상태 정보를 저장하여 K8s 측에서 필요한 일시적 상태를 다시 생성할 수 있게 합니다. 또한 API 서버와 Ray 클러스터 컨트롤러 간의 데이터 흐름을 분리하고 오프라인 분석을 위해 Hive로 데이터 덤프를 수행하는 추가 혜택이 있습니다.\n- Ray 클러스터 컨트롤러: 이 구성 요소는 Ray 클러스터의 수명 주기 관리를 위해 K8s를 지속적으로 쿼리합니다. Ray 헤드 및 워커 노드 프로비저닝, Ray 클러스터 상태 모니터링 및 필요에 따른 정리 작업을 수행하는 역할을 합니다.\n- Ray 작업 컨트롤러: Ray 클러스터 컨트롤러와 유사하게, Ray 작업 컨트롤러는 Ray 작업 수명 주기 관리에 초점을 맞춥니다. RayJobs를 제출하기 위한 주요 엔티티로 작용하여 시스템 내에서 적절한 인증 및 권한 부여 프로토콜을 보장합니다. 또한 컨트롤러는 동일한 Ray 클러스터에 여러 Ray 작업을 제출할 수 있도록 지원함으로써 사용자가 각 작업 제출마다 새 Ray 클러스터 프로비저닝을 기다릴 필요 없이 더 효율적으로 반복할 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\n여기는 테이블 태그를 마크다운 형식으로 변경한 것입니다.\n\n이 방식은 사용자와 Kubernetes 사이에 가치 있는 추상화 계층을 제공하여 사용자가 복잡한 Kubernetes 자산을 이해할 필요가 없게 합니다. 대신, 플랫폼에서 제공하는 사용자 대면 라이브러리를 활용할 수 있습니다. 클라이언트 측에서 프로비저닝 단계의 부담을 옮기면 프로세스가 간소화되어 단계가 단순화되고 전반적인 사용자 경험이 향상됩니다.\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_2.png)\n\n우리 자체 컨트롤러를 구현하는 동안, 우리는 모듈화를 보장하여 나중에 KubeRay로의 원활한 전환을 가능하게 했습니다. 이 방식은 Ray 클러스터를 시작하는 데 사용되는 방법을 손쉽게 바꿀 수 있어, 내부 Kubernetes 기본 도구에서 KubeRay로의 전환이 원활하게 이루어집니다.\n\n```python\nClass Controller:\n    def reconcile(self, ray_cluster: RayClusterRecord):\n        # 이 부분은 내부 기본 도구에서 KubeRay로 교체 가능합니다.\n        status, k8s_meta = self.launch_and_monitor_ray_cluster(ray_cluster.configs)\n        db.update(ray_cluster, status=status, k8s_meta=k8s_meta)\n\n    def run(self):\n        while True:\n            ray_clusters = db.get_ray_cluster_to_dispatch()\n            for ray_cluster in ray_clusters:\n                self.reconcile(ray_cluster)\n            sleep(1)\n   \n    def launch_and_monitor_ray_cluster(self, configs) -> Tuple[str, Dict]:\n        return get_actual_k8s_related_status(ray_identifier=configs.ray_identifier)\n```\n\n<div class=\"content-ad\"></div>\n\n가능한 한 Observable성을 고려해보세요. Ray 클러스터의 기존 Ray 대시보드는 클러스터가 활성화된 상태에서만 접근할 수 있고 로그 또는 메트릭 재생을 위한 기능이 없습니다. 따라서 우리는 지속적인 로깅 및 메트릭 기능을 통합하는 전용 사용자 인터페이스를 개발하기로 결정했습니다. 이 사용자 인터페이스는 이전에 구축한 API Gateway에 의해 지원되며, 실시간으로 Ray 클러스터 및 Ray 작업 상태에 대한 통찰력을 제공합니다. 모든 메타데이터, 이벤트 및 로그는 데이터베이스 또는 S3에 저장되므로 이 전략은 Ray 클러스터를 유지하지 않고도 로그 분석을 가능하게 함으로써 GPU와 같은 유휴 리소스로 인한 비용을 완화할 수 있습니다.\n\n![image](/assets/img/2024-06-19-RayInfrastructureatPinterest_3.png)\n\n다양한 회사들이 자체 시계열 메트릭 솔루션을 가지고 있다는 것은 사실일 것입니다. Pinterest에서는 Goku라는 자체 시계열 데이터베이스를 활용하고 있으며, 이는 OpenTSDB와 호환되는 API를 가지고 있습니다. 우리는 prometheus 메트릭을 스크랩하고 내부 시스템과 호환되도록 재구성하는 추가 사이드카를 실행하고 있습니다. 로깅에 관해서는 Ray의 권고사항을 따라 로그를 AWS S3에 지속 저장하고 있습니다. 이러한 로그는 API 서버에서 소비되어 Ray 클러스터 UI에 표시됩니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-RayInfrastructureatPinterest_4.png\" />\n\n## Ray Application Stats\n\n동일한 Grafana 차트를 회사 내 시각화 도구인 Statsboard로 번역했습니다. 또한 Pinterest의 ML 엔지니어에게 유용한 dcgm GPU 메트릭 및 dataloader 메트릭과 같은 더 많은 애플리케이션별 기능을 추가했습니다. 이를 통해 Ray 애플리케이션의 병목 현상과 문제를 식별할 수 있습니다.\n\n<img src=\"/assets/img/2024-06-19-RayInfrastructureatPinterest_5.png\" />\n\n<div class=\"content-ad\"></div>\n\n## Ray Infrastructure Stats\n\n인프라 수준의 모든 메트릭을 모니터링하는 것은 효과적인 모니터링 구현, 알림 생성, 그리고 역사적 데이터를 기반으로 한 SLO/SLA 벤치마킹을 설정하는 데 중요합니다. 예를 들어, 레이 클러스터 대기 시간의 종단 간 추적 및 레이 작업의 롤링 성공률 모니터링은 시스템 성능을 평가하고 유지하는 데 중요합니다. 또한, 레이 클러스터 프로비저닝 실패로 이어질 수 있는 플랫폼 측의 오류를 식별하는 것은 운영 효율을 유지하는 데 중요합니다.\n\n![Ray Infrastructure](/assets/img/2024-06-19-RayInfrastructureatPinterest_6.png)\n\n# 개발 및 운영 인터페이스\n\n<div class=\"content-ad\"></div>\n\n핀터레스트에서 Ray 애플리케이션을 개발하는 세 가지 옵션을 제공합니다. Dev 서버, 주피터, Spinner 워크플로우가 있습니다. 모든 옵션은 ML 플랫폼의 RESTful API를 사용하여 구동됩니다.\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_7.png)\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_8.png)\n\nAirflow에서 PythonOperator를 활용하여 사용자가 작업 구성을 제공하고, 우리는 그것을 MLP 서버로 향하는 RayJob 요청으로 변환하는 사용자 정의 연산자를 구성합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_9.png)\n\n# 테스트\n\n유닛 테스트 및 통합 테스트\n\n레이 애플리케이션을 개발할 때 이용할 수 있는 두 가지 유형의 테스트를 제공합니다:\n\n\n<div class=\"content-ad\"></div>\n\n- Unittest을 사용하는 것을 권장하는 것은 하위 수준 Ray 코어나 Ray 데이터 라이브러리를 활용하는 플랫폼 라이브러리 소유자들에게 좋습니다. 통합 테스트가 적합합니다. Ray 프로그램을 테스트하는 팁을 따르고 동일한 테스트 스위트 내에서 ray 클러스터를 가능한 한 많이 재사용하기 위해 pytest fixtures를 사용합니다.\n- 완전한 Ray 작업을 실행하여 코드 변경이나 라이브러리 업데이트로 인해 발생할 수 있는 잘못된 부분을 식별하고 해결하려는 사용자들에게는 통합 테스트가 적합합니다. 비즈니스에 중요한 Ray 응용 프로그램의 건강 상태를 주기적으로 모니터링하기 위해 통합 테스트도 실행합니다.\n\n![Ray Infrastructure at Pinterest](/assets/img/2024-06-19-RayInfrastructureatPinterest_10.png)\n\n## 네트워크 및 보안\n\nRay는 컴퓨팅 플랫폼으로 개발자가 API를 통해 쉽게 워크로드를 실행할 수 있는 유연성을 제공하지만, 이는 보안 취약점(CVE-2023-48022)으로 이어질 수 있습니다. Shadowray 기사에서 강조하는 것과 같이 Ray 자체가 적절한 인증 및 권한 부여 방법을 제공하지 않기 때문에 Ray 대시보드 API에 액세스 권한이 있는 모든 사용자가 유효성 검사나 제어 없이 원격으로 코드를 실행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n핀터레스트에서는 이 보안 문제를 심각하게 취급하고 적절히 대응했습니다. 우리는 레이 클러스터에 올바른 인증 및 권한 부여가 적용되어야 한다는 것을 확실히 하기 위해 한 발 더 나아갔습니다. 따라서 사용자가 적절한 권한을 갖고 있지 않은 경우 해당 Ray 클러스터를 사용할 수 없도록 했습니다.\n\n그러나 이 문제의 복잡성은 핀터레스트의 연방 쿠버네티스 클러스터 아키텍처로 인해 더욱 악화되었는데, 이는 군집 간 기능을 군집 내 환경에 적용하는 데 어려움을 겪게 했습니다. 예를 들어 K8s 클러스터 간의 입력 및 출력 흐름을 제어하기 위해 NetworkPolicy를 사용할 수 없기 때문에, 하드웨어 가용성을 극대화하기 위해 Pod가 K8s 클러스터 전체에 흩어질 수 있는 경우, 네트워크 격리를 실현하기 위한 대안적인 방법이 필요합니다.\n\n- HTTP: 핀터레스트에서는 쿠버네티스 환경에서 서비스 메쉬로 Envoy를 사용합니다. 레이 대시보드를 Envoy 뒤의 localhost에 배포하고 핀터레스트에서의 인증 및 권한 부여 표준 방식을 따릅니다. 이를 통해 UI에서 사용자를 위한 OAuth 또는 서비스를 위한 mTLS로 레이 대시보드에 액세스 범위를 제한할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_11.png)\n\n<div class=\"content-ad\"></div>\n\n2. gRPC: K8s 환경에서 임의의 Pod이 활성 상태의 Ray Cluster에 연결하는 것을 방지하기 위해, Ray 클러스터 부트스트랩 시에 Ray TLS를 일부 사용자 정의와 함께 이용합니다. 자세히 말하면, 각 Ray 클러스터마다 고유한 쌍(개인 키, 인증서) 인증서 기관(CA)를 생성합니다. 이를 통해 CA와 특정 Ray 클러스터 간에 1:1 매핑을 보장합니다. 상호 인증의 첫 번째 단계는 클라이언트(Ray Pods)의 접근을 해당 CA로 제한하여 서버 측에서 적절한 AuthN / AuthZ로 수행함으로써 완료됩니다. 이렇게 함으로써 특정 Ray 클러스터를 나타내는 해당 CA에 의해 서명된 인증서를 수령할 수 있는 Pod의 하위 집합만 이를 수행할 수 있습니다. 두 번째 단계는 발급된 인증서를 사용하여 통신하는 Pod들이 예상된 Ray 클러스터에 해당하는 CA에 의해 서명되었는지 확인하는 것입니다. 게다가, Ray Pods에 대한 잎 인증서의 서명 및 발급에 대한 모든 암호화 작업은 클라이언트, 즉 Ray head 및 worker pods이 CA 개인 키에 액세스할 수 없도록 서버 측(MLP 서버)에서 수행되어야 합니다.\n\n![이미지](/assets/img/2024-06-19-RayInfrastructureatPinterest_12.png)\n\n# 교훈\n\n점진적 개선:\n\n<div class=\"content-ad\"></div>\n\n-  단순한 방식으로 Ray 클러스터를 배포한 후에는 프로덕션 환경이나 클라우드 환경에서 자동화 및 확장 프로세스에 초점을 맞추세요.\n- MVP 개발 시 휠을 재발명할 필요를 최소화하기 위해 회사의 기존 인프라를 활용하세요. 저희는 Kubeflow 오퍼레이터를 활용하며 기존의 ML 특화 인프라 논리는 개발 프로세스를 간소화할 수 있습니다.\n- 프로토 타입이 완료된 후 회사 전체의 최고 권고 사항에 따라 보안 문제 및 다른 규정 준수 문제 등 인프라를 개선하세요.\n- 고객과 정기적으로 회의를 진행하여 도전 과제 및 개선 영역에 대한 초기 피드백을 수집하세요.\n- Pinterest의 Ray 이니셔티브의 현재 성공을 고려할 때, ML 전용 K8s 클러스터로 이동할 때 KubeRay 통합과 같은 더 많은 개선 사항을 찾고 있습니다.\n\n클라이언트와 Kubernetes 클러스터 사이의 중간 계층:\n\n- API 서버는 클라이언트와 Kubernetes 간의 다리 역할을 하는 추상화 계층을 제공합니다.\n- Ray 클러스터의 생애 주기 이벤트가 Kubernetes에서 사용자 정의 리소스가 제거된 후에도 지속적으로 기록되도록 보장하세요.\n- 플랫폼은 비즈니스 로직, 즉 추가 유효성 검사 및 사용자 인증, 인가, 사용자를 위한 Ray 대시보드 API 액세스 제한과 같은 사용자 지정을 구현할 수 있는 기회가 있습니다.\n- Ray 클러스터를 제공하는 실제 방법을 분리함으로써 KubeRay 및 전용 K8s 클러스터로 전환하는 것이 훨씬 쉬워지며 앞으로 계획하는 것과 같이 필요에 따라 다른 노드 제공자로 전환할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 사용자에게 충분한 인프라 관련 정보를 제공하지 않으면 Ray 클러스터 프로비저닝의 실패 또는 지연과 관련된 혼란이 발생할 수 있습니다.\n- 동시에 수십 개 또는 수백 개의 Ray 클러스터를 운영하려면 플랫폼 측에서 모니터링 및 경보가 중요합니다. Ray 인프라의 초기 단계에 있으며 신속한 변화로 애플리케이션 측에 장애가 발생할 수 있으므로 경보 설정에 성실해야 하며 프로덕션 환경으로 배포하기 전에 스테이징 환경에서 철저한 테스트를 수행해야 합니다.\n\n# 사용법\n\n우리는 2023년 2분기부터 Ray 인프라 사용량을 수집하기 시작했고, 지난 마일스톤 데이터 처리 응용 프로그램 GA 및 추가 사용자들이 Ray 프레임워크에 참여하면서 2023년 4분기에 급증을 관찰했습니다. 우리는 현재 배치 추론 및 adhoc Ray Serve 개발과 같은 다양한 Ray 응용 프로그램을 탐색하기 위해 Ray 기반으로 이전한 사용자들에게 도움을 주고 있습니다. 우리는 아직 네이티브 PyTorch 기반 애플리케이션에서 Ray 기반 애플리케이션으로 이동하는 초기 단계에 있으나, 더 고급화된 사용 사례로 전환하기 위해 고객들과 열심히 협력하고 있습니다.\n\n<img src=\"/assets/img/2024-06-19-RayInfrastructureatPinterest_13.png\" />\n\n<div class=\"content-ad\"></div>\n\n아래는 Markdown 형식으로 표로 변환하였습니다.\n\n\n| 파일 | 이미지 |\n|:-------------------------:|:-------------------------:|\n| 2024-06-19-RayInfrastructureatPinterest_14.png | <img src=\"/assets/img/2024-06-19-RayInfrastructureatPinterest_14.png\" /> |\n| 2024-06-19-RayInfrastructureatPinterest_15.png | <img src=\"/assets/img/2024-06-19-RayInfrastructureatPinterest_15.png\" /> |\n\n\n# 사용 사례\n\nRay 인프라는 제품 ML 사용 사례에 대해 배포되었으며 새로운 응용 프로그램의 신속한 실험을 위해 사용되었습니다.\n\n<div class=\"content-ad\"></div>\n\n## Ray Train\n\n- 여러 추천 시스템 모델 교육이 Ray로 이관되었으며, 나머지 사용 사례를 적극적으로 도입 중입니다.\n- 현재 Ray를 사용하여 매월 5000개 이상의 교육 작업을 실행 중입니다.\n- 이러한 교육 실행은 이질적인 CPU / GPU 클러스터를 활용합니다.\n\n## 핵심 이점:\n\n확장성:\n\n<div class=\"content-ad\"></div>\n\n- Ray는 교육 실행을 가능한 서로 다른 교육기 인스턴스를 넘어 데이터 로딩 및 전처리를 확장할 수 있도록 합니다.\n- p4d.24xlarge와 같은 단일 GPU 노드는 고정된 12:1 CPU:GPU 비율을 가지고 있어 데이터 로더를 확장하고 GPU를 포화시키는 것을 방해합니다.\n- Ray를 사용하면 p4d 인스턴스 외부에서 데이터 로더를 확장할 수 있습니다. 이는 CPU만 있는 인스턴스를 사용하여 더 저렴하게 처리할 수 있습니다.\n\nDev-velocity\n\n- 확장성 이외에도 Ray는 개발 속도를 크게 높일 수 있습니다.\n- 머신 러닝 엔지니어들의 일상적인 작업 중 상당 부분은 모델 변경을 구현하고 로컬 코드를 사용하여 개발 훈련을 실행하는 것입니다.\n- Ray를 사용하면 Ray 컴퓨팅 클러스터를 상호작용적으로 사용하여 주피터 노트북을 통해 작업을 제출할 수 있습니다.\n\n## 일괄 추론\n\n<div class=\"content-ad\"></div>\n\n- Pinterest는 과거 PySpark 기반의 일괄 추론 솔루션을 활용했습니다.\n- Ray를 사용하여, ray.data.Dataset에서 map_batches 구현으로 설계된 새 BatchInference 솔루션을 재구현했습니다.\n- 우리는 현재 이 솔루션을 세 가지 프로덕션 유즈 케이스에 사용하고 있습니다.\n- 현재 우리는 Ray를 사용하여 매달 300개 이상의 일괄 추론 작업을 실행 중입니다.\n\n## 핵심 이점:\n\n효율성:\n\n- 이전 구현과는 달리, Ray는 전처리, GPU 추론 및 출력 파일 쓰기의 파이프라이닝을 가능케 합니다.\n- 더불어, 자동으로 이 세 단계를 이종 CPU 및 GPU 노드에서 실행할 수 있게 분리할 수 있습니다.\n- 이들을 합쳐, 우리의 프로덕션 GPU 추론 작업의 작업 실행 시간이 4배 감소했습니다 (1시간 → 15분).\n\n<div class=\"content-ad\"></div>\n\nUnlocked Opportunity:\n\n- Ray와 함께 프로그래밍하는 쉬움과 파이프라이닝으로 얻는 효율성 덕분에 GPU 기반 모델에 대한 특성 소거 도구를 채택할 수 있었습니다.\n\n## 실험적 워크로드\n\n- Ray는 Ray Serve를 포함한 다양한 도구 생태계를 제공합니다.\n- Ray Serve는 모델 서빙을 위한 내장된 라우팅 및 자동 스케일링 기능을 제공하며, 모델을 빠르게 평가하기 위해 매우 편리합니다.\n- Ray Serve가 없는 경우 클라이언트는 RPC 서버, 배포 파이프라인, 서비스 검색 및 자동 스케일링을 수동으로 설정해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n## 주요 성과:\n\n- 내부 해커톤에서 팀이 몇 시간 만에 오픈 소스 대규모 모델을 설정하고 사용할 수 있었습니다.\n- Ray가 없었다면 이러한 인프라를 구축하는 데 몇 일에서 몇 주가 걸렸을 것입니다.\n\n# 다음 계획\n\n- Pinterest에서 Ray Batch Inference에 대해 깊이 파고들기\n- Pinterest에서 Ray Tune\n- Pinterest에서 Ray 어플리케이션의 독특한 도전 현황\n\n<div class=\"content-ad\"></div>\n\n# 인사말\n\nCloud Runtime Team: Jiajun Wang, Harry Zhang\n\nTraffic Team: James Fish, Bruno Palermo, Kuo-Chung Hsu\n\nSecurity Team: Jeremy Krach, Cedric Staub\n\n<div class=\"content-ad\"></div>\n\nML 플랫폼: Qingxian Lai, Lei Pan\n\nAnyscale: Zhe Zhang, Kai-Hsun Chen, SangBin Cho","ogImage":{"url":"/assets/img/2024-06-19-RayInfrastructureatPinterest_0.png"},"coverImage":"/assets/img/2024-06-19-RayInfrastructureatPinterest_0.png","tag":["Tech"],"readingTime":14},{"title":"억제된 모든 LLM을 해제하세요","description":"","date":"2024-06-19 03:36","slug":"2024-06-19-UncensoranyLLMwithabliteration","content":"\n\n## 재학습 없이 세밀 조정하기\n\n![이미지](/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png)\n\n람마 모델의 세대가 늘어날수록 새로운 기능이 제공되었습니다. 이는 사용자의 지시를 이해하고 따르는 능력이 뛰어난 '세세한 조정(세세하게 조정)' 버전을 제공합니다. 그러나 이러한 모델들은 매우 검열되어 있으며 해로운 요청으로 간주되는 것은 거부하고 \"AI 어시스턴트로서 도와드릴 수 없습니다.\"와 같은 대답을 합니다. 이 안전 기능은 오용을 방지하는 데 중요하지만, 모델의 유연성과 반응성을 제한합니다.\n\n본 문서에서는 \"무효화\"라는 기술을 탐구하여 재학습 없이 어떤 람마 모델이든 검열을 푸는 방법을 살펴볼 것입니다. 이 기술은 모델에 내장된 거부 메커니즘을 효과적으로 제거하여 모든 유형의 프롬프트에 대응할 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\n코드는 Google Colab에서도 사용할 수 있고, LLM 코스에서도 GitHub에 있습니다. 이 기사를 교정해 주신 FailSpy님에게 특별히 감사드립니다.\n\n# ✂️ 삭제란이란?\n\n현대 LLM은 안전 및 지시를 따르는 방향으로 세밀하게 조정되어 있어, 해로운 요청을 거부하기 위해 훈련되어 있습니다. Arditi 등이 블로그 글에서 설명한 바에 따르면, 이 거부 행동은 모델의 잔류 스트림에 있는 특정 방향을 통해 중재됩니다. 만약 이 방향을 모델이 나타내지 못하도록 막는다면, 요청을 거부하는 능력을 잃게 됩니다. 반대로, 이 방향을 인위적으로 추가하면 모델이 해가 없는 요청조차도 거부할 수 있게됩니다.\n\n전통적인 디코더 전용 Llama류 아키텍처에서는 세 가지의 잔류 스트림을 대상으로 할 수 있습니다: 각 블록의 시작 부분에서(“pre”), 어텐션과 MLP 레이어 사이에서(“mid”), 그리고 MLP 이후에(“post”). 다음 그림은 각 잔류 스트림의 위치를 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_1.png\" />\n\nLLM을 무증검 상태로 만들기 위해 먼저 모델 내의 \"거부 방향\"을 식별해야 합니다. 이 과정에는 몇 가지 기술적 단계가 포함되어 있습니다:\n\n- 데이터 수집: 유해한 지시문 집합과 무해한 지시문 집합을 사용하여 모델을 실행하고, 각각의 마지막 토큰 위치에서 잔여 스트림 활성화를 기록합니다.\n- 평균 차이: 유해한 지시와 무해한 지시의 활성화 간 평균 차이를 계산합니다. 이를 통해 모델의 각 레이어에 대한 \"거부 방향\"을 나타내는 벡터를 얻을 수 있습니다.\n- 선택: 이러한 벡터를 정규화하고, 평가하여 단일 최상의 \"거부 방향\"을 선택합니다.\n\n거부 방향을 식별한 후, 해당 기능을 표현하는 모델의 능력을 효과적으로 제거하는 \"제거(ablate)\" 작업을 수행할 수 있습니다. 이는 추론 시간 간섭이나 가중치 직교화를 사용하여 영구적으로 수행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n먼저 추론 시간 개입에 대해 이야기해 보겠습니다. 잔차 스트림에 기록하는 모든 구성 요소(예: 어텐션 헤드)마다 그 출력을 거부 방향으로 투영한 후 이 투영을 뺍니다. 이 뺄셈은 각 토큰과 각 레이어에 적용되어 모델이 결코 거부 방향을 표현하지 않도록 합니다.\n\n한편, 가중치 직교화는 모델 가중치를 직접 수정하는 것을 포함합니다. 거부 방향에 대해 구성 요소 가중치를 직교화함으로써 모델이 이 방향으로 기록하는 것을 방지합니다. 잔차 스트림에 기록하는 행렬을 조정하여 이러한 기여가 거부 방향에 영향을 주지 않도록 합니다.\n\n다음 섹션에서는 가중치 직교화를 사용한 좌절 실현을 구현할 것입니다.\n\n# 💻 구현\n\n<div class=\"content-ad\"></div>\n\n아래의 abliteration 구현은 FailSpy의 노트북을 기반으로 하고 있습니다. 그 노트북은 원래 저자들의 노트북을 기반으로 하고 있습니다. 저는 주로 이를 적응하여 간단하고 이해하기 쉽도록 했습니다. 이 섹션은 코드가 많이 포함되어 있어서 무슨 일이 벌어지는지 볼 수 있지만, 기술적인 세부 사항에 덜 관심이 있는 경우 FailSpy의 abliterator 라이브러리를 사용할 수도 있습니다 (Hugging Face의 abliterated 모델 모음도 확인해보세요).\n\n이 코드는 뛰어난 TransformerLens 라이브러리 (이전에는 EasyTransformer로 알려졌음)를 사용하여 무거운 작업을 처리합니다. 메커니즘 해석 가능성을 위해 설계되었으며 여기서는 활성화에 개입하는 데 사용됩니다. 이 라이브러리를 만든 Neel Nanda와 Joseph Bloom에게 감사드립니다.\n\n먼저 필요한 패키지를 설치하고 가져와 봅시다. 이러한 모든 단계는 Google Colab 노트북에서 사용할 수 있습니다.\n\n```js\n!pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping\n\nimport torch\nimport functools\nimport einops\nimport gc\n\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom torch import Tensor\nfrom typing import List\nfrom transformer_lens import HookedTransformer, utils\nfrom transformer_lens.hook_points import HookPoint\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom jaxtyping import Float, Int\nfrom collections import defaultdict\n\n# GPU 메모리를 저장하기 위해 자동 미분을 끕니다 (크레딧: Undi95)\ntorch.set_grad_enabled(False)\n```\n\n<div class=\"content-ad\"></div>\n\n두 가지 데이터 세트가 필요합니다: 피해가 없는 지침을 포함한 하나와 유해한 지침을 포함한 하나입니다. 우리는 tatsu-lab/alpaca와 llm-attacks의 데이터를 사용할 것입니다. 모든 것을 더 쉽게 만들기 위해, 저는 이를 두 개의 Hugging Face 데이터 세트로 다시 패키징하여 mlabonne/harmless_alpaca와 mlabonne/harmful_behaviors로 만들었습니다. 그렇게 하면 여러분이 쉽게 여러분 자신의 데이터 세트로 교체할 수 있습니다.\n\n지침을로드하고 \"role\"과 \"content\" 키가 있는 사전 목록으로 다시 서식화할 것입니다. 이렇게 하면 Llama 3의 채팅 템플릿을 따르는 apply_chat_tokenizer() 메서드와 호환됩니다.\n\n```python\ndef reformat_texts(texts):\n    return [[{\"role\": \"user\", \"content\": text}] for text in texts]\n\n# 유해하고 무해한 데이터 세트 가져오기\ndef get_harmful_instructions():\n    dataset = load_dataset('mlabonne/harmful_behaviors')\n    return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text'])\n\ndef get_harmless_instructions():\n    dataset = load_dataset('mlabonne/harmless_alpaca')\n    return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text'])\n\nharmful_inst_train, harmful_inst_test = get_harmful_instructions()\nharmless_inst_train, harmless_inst_test = get_harmless_instructions()\n```\n\n이제 데이터 세트가 준비되었으므로, abliterate 하려는 모델을 로드할 수 있습니다. 안타깝게도, HookedTransformer를 사용하여 직접 사용자 정의 모델을 로드할 수 없습니다. 여기에서, FailSpy의 노트북에 설명된 꼼수를 사용하여 사용자 정의 모델을 다운로드하고 meta-llama/Meta-Llama-3-8B-Instruct로 이름을 변경하겠습니다. GPU가 BF16과 호환되지 않는 경우 torch.float16 형식으로 로드하세요.\n\n<div class=\"content-ad\"></div>\n\n이 예시에서는 DARE TIES(모델 병합에 관한 내 기사 참조)로 만들어진 mlabonne/Daredevil-8B를 사용할 것입니다. 이 모델은 8B 카테고리의 Open LLM Leaderboard에서 가장 높은 MMLU 점수를 가지고 있어요.\n\n```js\nMODEL_ID = \"mlabonne/Daredevil-8B\"\nMODEL_TYPE = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# 모델 다운로드 및 로드\n!git clone https://huggingface.co/{MODEL_ID} {MODEL_TYPE}\n\n# 모델 및 토크나이저 로드\nmodel = HookedTransformer.from_pretrained_no_processing(\n    MODEL_TYPE,\n    local_files_only=True,\n    dtype=torch.bfloat16,\n    default_padding_side='left'\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)\ntokenizer.padding_side = 'left'\ntokenizer.pad_token = tokenizer.eos_token\n```\n\n이제 데이터셋을 토큰화할 수 있습니다. 무해한 및 유해한 지시사항에 대해 동일한 샘플 수를 사용합니다. 샘플 수가 높으면 모든 RAM/VRAM을 사용할 수 있으므로 여기서는 256으로 제한합니다.\n\n```js\ndef tokenize_instructions(tokenizer, instructions):\n    return tokenizer.apply_chat_template(\n        instructions,\n        padding=True,\n        truncation=False,\n        return_tensors=\"pt\",\n        return_dict=True,\n        add_generation_prompt=True,\n    ).input_ids\n\nn_inst_train = min(256, len(harmful_inst_train), len(harmless_inst_train))\n\n# 데이터셋 토큰화\nharmful_tokens = tokenize_instructions(\n    tokenizer,\n    instructions=harmful_inst_train[:n_inst_train],\n)\nharmless_tokens = tokenize_instructions(\n    tokenizer,\n    instructions=harmless_inst_train[:n_inst_train],\n)\n```\n\n<div class=\"content-ad\"></div>\n\n작업이 모두 설정되었습니다. 이제 도처럼 처리하는 첫 번째 단계인 데이터 수집을 구현할 차례입니다. 우리는 이 토큰화된 데이터셋을 처리하고 유해(harmful) 및 무해(harmless)로 나머지 스트림 활성화를 저장하려고 합니다. 이는 transformer_lens 라이브러리로 관리됩니다.\n\n```js\nbatch_size = 32\n\n# 활성화를 저장할 기본 사전 초기화\nharmful = defaultdict(list)\nharmless = defaultdict(list)\n\n# 데이터 학습을 배치 단위로 처리\nnum_batches = (n_inst_train + batch_size - 1) // batch_size\n\nfor i in tqdm(range(num_batches)):\n    print(i)\n    start_idx = i * batch_size\n    end_idx = min(n_inst_train, start_idx + batch_size)\n\n    # 유해 및 무해 프롬프트에 모델 실행 및 활성화 캐시\n    harmful_logits, harmful_cache = model.run_with_cache(\n        harmful_tokens[start_idx:end_idx],\n        names_filter=lambda hook_name: 'resid' in hook_name,\n        device='cpu',\n        reset_hooks_end=True\n    )\n    harmless_logits, harmless_cache = model.run_with_cache(\n        harmless_tokens[start_idx:end_idx],\n        names_filter=lambda hook_name: 'resid' in hook_name,\n        device='cpu',\n        reset_hooks_end=True\n    )\n\n    # 활성화 수집 및 저장\n    for key in harmful_cache:\n        harmful[key].append(harmful_cache[key])\n        harmless[key].append(harmless_cache[key])\n\n    # RAM 및 VRAM 비우기\n    del harmful_logits, harmless_logits, harmful_cache, harmless_cache\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# 캐시된 활성화 결합\nharmful = {k: torch.cat(v) for k, v in harmful.items()}\nharmless = {k: torch.cat(v) for k, v in harmless.items()}\n```\n\n이제 각 층에 대한 거부 방향을 계산할 수 있습니다. 이는 유해 및 무해 명령의 활성화 간 평균 차이에 해당하며 정규화됩니다. 그런 다음 activation_scored에서 내림차순으로 정렬됩니다.\n\n```js\n# 활성화 색인을 가져오는 도우미 함수\ndef get_act_idx(cache_dict, act_name, layer):\n    key = (act_name, layer)\n    return cache_dict[utils.get_act_name(*key)]\n\n# 중간 층에서 유해 및 무해 활성화의 평균 차이 계산\nactivation_layers = [\"resid_pre\", \"resid_mid\", \"resid_post\"]\nactivation_refusals = defaultdict(list)\n\nfor layer_num in range(1, model.cfg.n_layers):\n    pos = -1  # 위치 인덱스\n    for layer in activation_layers:\n        harmful_mean_act = get_act_idx(harmful, layer, layer_num)[:, pos, :].mean(dim=0)\n        harmless_mean_act = get_act_idx(harmless, layer, layer_num)[:, pos, :].mean(dim=0)\n        refusal_dir = harmful_mean_act - harmless_mean_act\n        refusal_dir = refusal_dir / refusal_dir.norm()\n        activation_refusals[layer].append(refusal_dir)\n\nselected_layers = [\"resid_pre\"]\nactivation_scored = sorted(\n    [\n        activation_refusals[layer][l - 1]\n        for l in range(1, model.cfg.n_layers)\n        for layer in selected_layers\n    ],\n    key=lambda x: abs(x.mean()),\n    reverse=True,\n)\n```\n\n<div class=\"content-ad\"></div>\n\n프로세스의 마지막 단계는 계산한 거절 방향을 평가하는 과정입니다. 이를 위해 우리는 거절 방향을 추론 중 각 잔여 스트림과 각 블록에 적용할 것입니다. 다음 코드 스니펫에서 네 가지 테스트 유해 지시와 20개 블록(또는 레이어)에 대한 세대를 가져옵니다.\n\n```js\n...\nCode snippet 생략\n...\n```\n\n우리는 evals 리스트에 모든 세대를 저장했습니다. 이제 이를 출력하고 각 지시에 대한 무겁지 않은 응답을 제공하는 레이어(블록)를 수동으로 선택할 수 있습니다. \"I cannot\" 및 \"I can't\"를 포함하는 응답은 원치 않는 답변을 필터링하기 위해 자동으로 제외합니다.\n\n위 요구사항을 만족하는 레이어를 찾을 수 없는 경우, 이전 selected_layers 목록의 다른 잔여 스트림, 다른 지시, 추가적 블록 등을 테스트하고자 할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\n# 인간 평가를 위한 세대 인쇄\nblacklist = [\"I cannot\", \"I can't\"]\nfor i in range(N_INST_TEST):\n    print(f\"\\033[1mINSTRUCTION {i}: {harmful_inst_test[i]}\")\n    print(f\"\\nBASELINE COMPLETION:\\n{baseline_generations[i]}\\033[0m\")\n    for layer_candidate in range(EVAL_N):\n        if not any(word in evals[layer_candidate][i] for word in blacklist):\n            print(f\"\\n---\\n\\nLAYER CANDIDATE #{layer_candidate} INTERVENTION COMPLETION:\")\n            print(evals[layer_candidate][i])\r\n```\n\n저의 경우, 레이어 후보자 9가 네 가지 명령에 대해 선정적이지 않은 답변을 제공했습니다. 이것이 우리가 거부 방향으로 선택할 것이다. 그 다음으로, 무게 직교화를 구현하여 모델이 이 방향의 출력을 생성하는 것을 방지합니다. 모델이 성공적으로 선정되지 않은지를 확인하려면 완료된 내용을 인쇄하여 확인할 수 있습니다.\n\n```js\r\ndef get_orthogonalized_matrix(\n    matrix: Float[Tensor, \"... d_model\"], vec: Float[Tensor, \"d_model\"]\n) -> Float[Tensor, \"... d_model\"]:\n    proj = (\n        einops.einsum(\n            matrix, vec.view(-1, 1), \"... d_model, d_model single -> ... single\"\n        )\n        * vec\n    )\n    return matrix - proj\n\n# 가장 높은 거부 방향을 갖는 레이어 선택\nLAYER_CANDIDATE = 9\nrefusal_dir = activation_scored[LAYER_CANDIDATE]\n\n# 모델의 가중치 직교화\nif refusal_dir.device != model.W_E.device:\n    refusal_dir = refusal_dir.to(model.W_E.device)\nmodel.W_E.data = get_orthogonalized_matrix(model.W_E, refusal_dir)\n\nfor block in tqdm(model.blocks):\n    if refusal_dir.device != block.attn.W_O.device:\n        refusal_dir = refusal_dir.to(block.attn.W_O.device)\n    block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, refusal_dir)\n    block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, refusal_dir)\n\n# 모델로 텍스트 생성\northogonalized_generations = get_generations(\n    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]\n)\n\n# 세대 출력\nfor i in range(N_INST_TEST):\n    if len(baseline_generations) > i:\n        print(f\"INSTRUCTION {i}: {harmful_inst_test[i]}\")\n        print(f\"\\033[92mBASELINE COMPLETION:\\n{baseline_generations[i]}\")\n    print(f\"\\033[91mINTERVENTION COMPLETION:\\n{evals[LAYER_CANDIDATE][i]}\")\n    print(f\"\\033[95mORTHOGONALIZED COMPLETION:\\n{orthogonalized_generations[i]}\\n\")\r\n```\n\n이제 모델을 사용할 준비가 되었습니다. 모델을 Hugging Face 형식으로 변환하여 HF 허브에 업로드합니다.\n\n<div class=\"content-ad\"></div>\n\n```json\n# 모델을 다시 HF 보안 텐서로 변환합니다\nhf_model = AutoModelForCausalLM.from_pretrained(MODEL_TYPE, torch_dtype=torch.bfloat16)\nlm_model = hf_model.model\n\nstate_dict = model.state_dict()\nlm_model.embed_tokens.weight = torch.nn.Parameter(state_dict[\"embed.W_E\"].cpu())\nfor l in range(model.cfg.n_layers):\n    lm_model.layers[l].self_attn.o_proj.weight = torch.nn.Parameter(\n        einops.rearrange(\n            state_dict[f\"blocks.{l}.attn.W_O\"], \"n h m->m (n h)\", n=model.cfg.n_heads\n        ).contiguous()\n    )\n    lm_model.layers[l].mlp.down_proj.weight = torch.nn.Parameter(\n        torch.transpose(state_dict[f\"blocks.{l}.mlp.W_out\"], 0, 1).contiguous()\n    )\n\nhf_model.push_to_hub(f\"{MODEL_ID}-abliterated\")\n```\n\n# ⚖️ DPO Fine-Tuning\n\n이전 섹션의 abliterated 및 소스 모델을 Open LLM Leaderboard 및 Nous의 벤치마크 스위트에서 평가했습니다. 여기에 결과가 있습니다:\n\n<img src=\"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_2.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n보시다시피, 원본 모델은 Llama 3 8B Instruct보다 현저하게 우수한 성능을 보여줍니다. 그러나 우리는 모든 벤치마크에서 절삭된 버전에서 성능 하락을 관찰하고 있습니다. 절삭 과정은 성능을 향상시키면서도 모델의 품질을 저하시킨 것으로 나타났습니다.\n\n이 문제를 해결하기 위해 우리는 우리의 절삭된 모델을 추가로 훈련하여 회복시키는 아이디어가 있습니다. 대부분의 세밀 조정된 모델들과 마찬가지로 Llama 3 8B Instruct은 지도 학습 세밀 조정에 있어서 꽤 취약합니다. 추가적인 SFT는 모델의 성능을 떨어뜨릴 가능성이 높습니다.\n\n대체로, 선호 맞춤이 상당히 가볍고 우리의 절삭된 모델을 뇌개박하지 않아도 됩니다. DPO는 사용하기 쉽고 우수한 추적 레코드로 여기서 좋은 후보입니다. 이를 구현하기 위해 mlabonne/orpo-dpo-mix-40k 데이터셋을 사용하는 LazyAxolotl (Axolotl을 만들어 준 Wing Lian에게 감사드립니다)을 사용했습니다. 여기에 사용한 구성은 다음과 같습니다:\n\n```js\nbase_model: mlabonne/Daredevil-8B-abliterated\nmodel_type: LlamaForCausalLM\ntokenizer_type: AutoTokenizer\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\nsave_safetensors: true\n\nrl: dpo\nchat_template: chatml\ndatasets:\n  - path: mlabonne/orpo-dpo-mix-40k\n    split: train\n    type: chatml.intel\n\ndataset_prepared_path:\nval_set_size: 0.0\noutput_dir: ./out\n\nadapter: qlora\nlora_model_dir:\n\nsequence_len: 2048\nsample_packing: false\npad_to_sequence_len: false\n\nlora_r: 64\nlora_alpha: 32\nlora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project: axolotl\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n\ngradient_accumulation_steps: 8\nmicro_batch_size: 1\nnum_epochs: 1\noptimizer: paged_adamw_8bit\nlr_scheduler: cosine\nlearning_rate: 5e-6\ntrain_on_inputs: false\ngroup_by_length: false\n... (이어짐)\n```\n\n<div class=\"content-ad\"></div>\n\n6xA6000 GPU와 DeepSpeed ZeRO-2를 사용하여 모델을 훈련했어요. 훈련에는 약 6시간 45분이 소요되었답니다. W&B에서 얻은 훈련 곡선을 여기에 가져왔어요:\n\nDPO를 세밀하게 조정한 mlabonne/NeuralDaredevil-8B-abliterated 모델이 자동으로 업로드되었어요. 저희가 앞서 지워버린 버전을 수정했는지 확인하기 위해 동일한 벤치마크에서 평가했어요:\n\n![훈련 곡선](/assets/img/2024-06-19-UncensoranyLLMwithabliteration_3.png)\n\n이 추가 훈련을 통해 지워진 영향 대부분을 회복할 수 있었어요. 모델이 개선되지 않는 한 영역은 GSM8K, 수학 데이터 세트, 인데요, 이는 orpo-dpo-mix-40k가 더 많은 수학 샘플을 필요로 할 수 있다는 것을 의미할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n최종 모델은 8B 범주에서 최첨단 성능을 자랑하는 미검열 LLM입니다. 필터링이 필요 없을 때는 Llama 3 8B Instruct의 개선된 버전으로 추천합니다. LM Studio에서 GGUF와 같은 양자화된 버전을 사용해 볼 수도 있습니다.\n\n# 결론\n\n이 글에서는 소명화(abliteration) 개념을 소개했습니다. 이 기술은 모델의 활성화를 해롭고 해를 끼치지 않는 프롬프트에 사용하여 거부 방향을 계산하고, 모델의 가중치를 수정하여 거부를 그만 내보낼 수 있도록 합니다. 이 기술은 안전 세밀조정의 취약성을 보여주며 윤리적 고려 사항을 던지고 있습니다.\n\n우리는 Daredevil-8B에 소명화를 적용하여 필터링을 해제했지만, 이로 인해 모델의 성능이 저하되었습니다. 그 후 DPO를 사용하여 NeuralDaredevil-8B 모델을 생성하여 완전히 미검열이고 고품질의 8B LLM을 만들었습니다. 소명화는 정렬 제거에 국한되지 않으며, 다시 교육 없이 세밀 조정의 일종으로 간주해야 합니다. 실제로 MopeyMule의 FailSpy의 경우처럼 좌절적인 대화 스타일을 채택하는 것과 같이 창의적으로 다른 목표에도 적용될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 기사가 마음에 들었으면 좋겠어요. 더 많은 내용을 보고 싶다면 Hugging Face와 Twitter의 @maximelabonne를 팔로우해 주세요.\n\n# 참고 자료\n\n- FailSpy, “abliterator library,” GitHub, 2024.\n- Andy Arditi, Oscar Obeso, Aaquib111, wesg, Neel Nanda, “Refusal in LLMs is mediated by a single direction,” Lesswrong, 2024.","ogImage":{"url":"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png"},"coverImage":"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png","tag":["Tech"],"readingTime":15},{"title":"LLM Unix Linux 도구상자의 궁극적인 스위스 아미 나이프","description":"","date":"2024-06-19 03:35","slug":"2024-06-19-llmTheUltimateSwissArmyKnifeofUnixLinuxToolbox","content":"\n\n## 멋진 도구로 명령줄 작업을 더욱 효율적으로!\n\n최근에 Simon Willison이 만든 llm 명령줄 도구를 발견하게 되었습니다.\n\n그는 최근에 이 도구를 어떻게 사용하는지에 대해 이야기했는데, 이를 일상적으로 사용하는 다른 명령줄 도구와 함께 어떻게 활용하는지를 보여주었습니다.\n\n## 이 도구는 무엇을 할까요?\n\n<div class=\"content-ad\"></div>\n\n간단히 말해서, 이 도구를 사용하면 워크플로우 중간에 LLM (대형 언어 모델)을 호출하고 강력한 방법으로 출력을 변환할 수 있습니다.\n\n빠른 예시:\n\nGithub에서 더 많은 예시와 데모를 찾을 수 있습니다.\n\n## 왜 이것이 중요한가요?\n\n<div class=\"content-ad\"></div>\n\n여기서 인용하자면:\n\n기존 애플리케이션 및 업무에 AI를 통합하는 것은 도전적입니다. 예를 들어, Github Copilot과 같은 솔루션이 이 기능을 지원하려면 각 IDE에 플러그인이 필요합니다. ChatGPT를 사용하는 동안에도 LLM 응답이 필요할 때마다 파일을 드래그 앤 드롭하거나 복사하여 붙여넣어야 하므로 데이터를 이동하는 데 더 많은 노력과 시간이 필요합니다.\n\n## 왜 이것이 뛰어난 아이디어인가요?\n\n이 도구는 Unix 철학에서의 주요 아이디어를 상쾌하게 부활시키는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n- 하나의 기능을 잘 수행하는 프로그램을 작성하세요.\n- 프로그램들이 함께 작동할 수 있도록 작성하세요.\n- 모든 프로그램들이 텍스트 스트림을 처리하도록 작성하세요. 왜냐하면 그것이 보편적인 인터페이스이기 때문입니다.\n\n마지막으로, 위 세 번째 점은 LLMs이 이 목적에 적합하게 만들어진 이유입니다. 왜냐하면 그들은 프롬프트에 기반하여 하나의 텍스트를 다른 텍스트로 효과적으로 변환하는 데 우수하기 때문입니다.\n\n좋은 상호 운용성을 위해서, 각 프로그램에 깔끔한 입출력 인터페이스가 있다면 좋고, 다른 프로그램의 출력을 표준 입력(stdin)을 통해 입력으로 받아 들이게 하는 것은 점 [2]로부터 따라온다. \n\n## 키 기능의 빠른 개요\n\n<div class=\"content-ad\"></div>\n\n- 플러그인을 통한 로컬 및 원격 모델 지원: Ollama와 같은 도구를 통해 LLMs의 로컬 배포로 비용을 관리하세요.\n- 템플릿 지원: Fabric과 같은 대체 도구는 \"패턴\"이라고 하는 템플릿을 커맨드 라인 도구로 제공하기 시작했습니다.\n\n전체 토크를 YouTube에서 시청하는 것을 적극 추천합니다.","ogImage":{"url":"/assets/img/2024-06-19-llmTheUltimateSwissArmyKnifeofUnixLinuxToolbox_0.png"},"coverImage":"/assets/img/2024-06-19-llmTheUltimateSwissArmyKnifeofUnixLinuxToolbox_0.png","tag":["Tech"],"readingTime":2},{"title":"RAG 시스템을 위한 새로운 청킹 방법","description":"","date":"2024-06-19 03:33","slug":"2024-06-19-NewChunkingMethodforRAG-Systems","content":"\n\n## 문서 분할 개선\n\n![image](/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_0.png)\n\n대형 문서를 작은 부분으로 나누는 것은 검색 증강 생성 (RAG) 시스템의 성능에 영향을 미치는 필수적이면서도 중요한 요소입니다. RAG 시스템을 개발하는 프레임워크들은 일반적으로 여러 가지 옵션을 제공합니다. 본문에서는 새로운 옵션을 소개하여 문서를 세분화할 때 문장 임베딩의 도움을 받아 주제 변경을 인식하려고 시도한 이와 같은 방법을 소개하고자 합니다. 이것은 RAG 시스템의 임베딩 단계에서 주제를 인코딩하는 텍스트 부분의 벡터를 찾을 수 있으며 여러 가지 주제의 혼합이 아닙니다. 우리는 주제 모델링의 맥락에서 본 방법을 제안했지만, RAG 시스템에서도 사용할 수 있는 것입니다.\n\n# RAG 시스템\n\n<div class=\"content-ad\"></div>\n\n검색 보완 생성 (Retrieval-Augmented Generation, RAG) 시스템은 검색 기반 및 생성 기반 방법을 결합하여 출력물의 품질과 관련성을 향상시키는 기계 학습 모델입니다. 먼저 입력 쿼리에 기반하여 대규모 데이터셋에서 관련 문서나 정보를 검색합니다. 그런 다음, 검색된 정보를 활용하여 일관된 컨텍스트에 적합한 응답이나 내용을 생성하기 위해 변환기 기반 언어 모델과 같은 생성 모델을 사용합니다. 이 하이브리드 방식은 모델이 정확하고 유익한 응답을 제공하는 능력을 향상시킵니다, 특히 복잡하거나 지식 집약적인 작업에서 더욱 유용합니다.\n\n# 다른 분할 옵션\n\n자세한 절차를 살펴보기 전에, 문서 분할에 대한 몇 가지 표준 옵션을 소개하겠습니다. 널리 사용되는 Langchain 프레임워크를 활용하여 예시를 보여드리겠습니다.\n\nLangChain은 주로 대규모 언어 모델을 적용하기 위해 설계된 견고한 프레임워크로, 다양한 자연어 처리(NLP) 작업을 지원합니다. 그 중 하나인 문서 분할은 사용자가 대형 문서를 작은 관리 가능한 청크로 쪼개는 기능입니다. 아래는 LangChain의 문서 분할의 주요 기능 및 예시를 소개한 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# LangChain의 문서 분할 핵심 기능\n\n- 재귀적 문자 텍스트 분할기: 이 방법은 문자 기반으로 텍스트를 재귀적으로 분할하여 각 청크가 지정된 길이보다 작도록 보장합니다. 이는 자연 단락 또는 문장 구분이 있는 문서에 특히 유용합니다.\n- 토큰 분할기: 이 방법은 토큰을 사용하여 문서를 분할합니다. 언어 모델의 토큰 제한이 있는 경우에 유용하며, 각 청크가 모델의 제약에 맞도록 보장합니다.\n- 문장 분할기: 이 방법은 문장 경계에서 문서를 분할합니다. 문장이 일반적으로 완전한 생각을 나타내므로 텍스트의 맥락적 무결성을 유지하는 데 이상적입니다.\n- Regex 분할기: 이 방법은 사용자 정의 분할 지점을 정의하기 위해 정규 표현식을 사용합니다. 이 방법은 사용 사례에 특정한 패턴을 기준으로 문서를 분할할 수 있는 가장 큰 유연성을 제공합니다.\n- 마크다운 분할기: 이 방법은 마크다운 문서에 맞춰져 있습니다. 제목, 목록, 코드 블록과 같은 마크다운 특정 요소를 기준으로 텍스트를 분할합니다.\n\n# LangChain에서 문서 분할의 예시\n\n## 1. 재귀적 문자 텍스트 분할기\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext = \"여러분의 긴 문서 텍스트를 여기에 넣어주세요...\"\nsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\nchunks = splitter.split_text(text)\nfor chunk in chunks:\n    print(chunk)\n```\n\n## 2. 토큰 분할기\n\n```js\nfrom langchain.text_splitter import TokenSplitter\n\ntext = \"여러분의 긴 문서 텍스트를 여기에 넣어주세요...\"\nsplitter = TokenSplitter(max_tokens=512)\nchunks = splitter.split_text(text)\nfor chunk in chunks:\n    print(chunk)\n```\n\n## 3. 문장 분할기\n\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom langchain.text_splitter import SentenceSplitter\n\ntext = \"문서의 긴 텍스트를 여기에 입력하세요...\"\nsplitter = SentenceSplitter(max_length=5)\nchunks = splitter.split_text(text)\nfor chunk in chunks:\n    print(chunk)\n```\n\n## 4. Regex Splitter\n\n```js\nfrom langchain.text_splitter import RegexSplitter\n\ntext = \"문서의 긴 텍스트를 여기에 입력하세요...\"\nsplitter = RegexSplitter(pattern=r'\\n\\n+')\nchunks = splitter.split_text(text)\nfor chunk in chunks:\n    print(chunk)\n```\n\n## 5. Markdown Splitter\n\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom langchain.text_splitter import MarkdownSplitter\n\ntext = \"여기에 긴 마크다운 문서를 넣어주세요...\"\nsplitter = MarkdownSplitter()\nchunks = splitter.split_text(text)\nfor chunk in chunks:\n    print(chunk)\n```\n\n# 새로운 접근 방식 소개\n\n대량의 문서를 디지털 콘텐츠 분석에서 일관된 주제별 섹션으로 분리하는 것은 상당한 어려움을 겪을 수 있습니다. 위에서 설명한 전통적인 방법들은 종종 주제가 변화하는 미묘한 부분을 정확하게 감지하지 못할 수 있습니다. 우리는 인공지능, 컴퓨터, 데이터 과학 및 응용국제 학회(ACDSA 2024)에서 발표된 논문에서 이 문제를 해결하기 위한 혁신적인 접근 방식을 제안합니다.\n\n## 핵심 도전 과제\n\n\n<div class=\"content-ad\"></div>\n\n대규모 문서, 예를 들어 학술 논문, 긴 보고서 및 상세한 기사는 복잡하며 여러 주제를 포함하고 있습니다. 간단한 규칙 기반 방법부터 고급 기계 학습 알고리즘까지 다양한 전통적인 분할 기술들은 주제 전환의 정확한 지점을 식별하는 것에 어려움을 겪습니다. 이러한 방법들은 종종 섬세한 전환점을 놓치거나 잘못 식별하여 단편화된 또는 겹치는 섹션을 야기할 수 있습니다.\n\n저희 방법은 문장 임베딩의 힘을 활용하여 분할 과정을 개선합니다. 이 접근 방식은 개별 문장에 대한 임베딩을 생성하기 위해 Sentence-BERT (SBERT)를 활용하여 그들의 유사성을 양적으로 측정합니다. 주제가 변경됨에 따라 이러한 임베딩은 벡터 공간에서 변화를 반영하여 잠재적인 주제 전환을 나타냅니다.\n\n## 접근 방식의 각 단계를 살펴보세요:\n\n## 1. 문장 임베딩 사용\n\n<div class=\"content-ad\"></div>\n\n임베딩 생성:\n\n- 이 방법은 개별 문장에 임베딩을 생성하기 위해 Sentence-BERT (SBERT)를 사용합니다. SBERT는 문장의 의미적 내용을 담고 있는 밀집 벡터 표현을 만듭니다.\n- 이러한 임베딩을 비교하여 연이은 문장 간의 일관성을 파악합니다.\n\n유사도 계산:\n\n- 문장 간의 유사도는 코사인 유사도 또는 맨해튼 또는 유클리드 거리와 같은 다른 거리 측정을 사용하여 측정됩니다.\n- 동일한 주제 내의 문장은 유사한 임베딩을 갖게 되며, 서로 다른 주제의 문장은 유사도가 감소하는 것을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n## 2. 갭 점수 계산\n\n매개 변수 n 정의:\n\n- 매개 변수 n을 설정하여 비교할 문장의 수를 지정합니다. 예를 들어, n=2이면 두 연속 문장이 다음 쌍과 비교됩니다.\n- n의 선택은 비교에서 고려되는 문맥 길이에 영향을 미치며, 세밀한 전환을 포착하는 필요와 계산 효율성을 균형있게 유지합니다.\n\n코사인 유사도 계산:\n\n<div class=\"content-ad\"></div>\n\n- 문서의 각 위치에 대해, 알고리즘은 현재 위치 앞 뒤 n개의 문장을 추출합니다.\n- 그런 다음 이러한 시퀀스의 임베딩 간 코사인 유사도, 즉 '간격 점수'를 계산합니다.\n- 이러한 간격 점수는 나중에 처리를 위해 목록에 저장됩니다.\n\n![이미지](/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_1.png)\n\n## 3. 부드러운 처리\n\n노이즈 처리:\n\n<div class=\"content-ad\"></div>\n\n- 텍스트의 미세한 변동으로 인해 기존 갭 점수는 소음이 발생할 수 있습니다. 이를 보정하기 위해 평활화 알고리즘을 적용합니다.\n- 평활화에는 매개변수 k로 정의된 창을 통해 갭 점수를 평균화하는 과정이 포함됩니다.\n\n창 크기 k 선택:\n\n- 창 크기 k는 평활화의 범위를 결정합니다. 큰 k 값은 더 많은 평활화를 유발하며 소음을 줄이지만 섬세한 변환을 놓칠 수 있습니다. 작은 k 값은 더 많은 세부 정보를 유지하지만 소음을 도입할 수 있습니다.\n- 평활화된 갭 점수는 주제 전환 지점이 명확히 나타납니다.\n\n![이미지](/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_2.png)\n\n<div class=\"content-ad\"></div>\n\n## 4. 경계 감지\n\n지역 최솟값 식별:\n\n- 부드러운 갭 점수를 분석하여 지역 최솟값과 주제 전환 지점을 식별합니다.\n- 각 지역 최솟값에 대해 깊이 점수를 계산하여 지역 최솟값과 이전 값과 이후 값 사이의 차이를 합산합니다.\n\n임계값 c 설정하기:\n\n<div class=\"content-ad\"></div>\n\n- 유의미한 경계를 결정하는 임계값 c를 사용합니다. 높은 c 값은 더 적고 더 의미 있는 세그먼트를 만들어내며, 낮은 c 값은 더 많고 더 작은 세그먼트를 만들어냅니다.\n- 평균 깊이 점수보다 표준 편차의 c 배 이상을 초과하는 경계는 유효한 분할 지점으로 간주됩니다.\n\n![그림](/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_3.png)\n\n## 5. 클러스터링 세그먼트\n\n반복된 주제 다루기:\n\n<div class=\"content-ad\"></div>\n\n- 긴 문서는 서로 다른 지점에서 유사한 주제를 재방문할 수 있습니다. 이를 해결하기 위해 알고리즘은 유사한 콘텐츠를 가진 세그먼트를 클러스터링합니다.\n- 이 과정은 세그먼트를 임베딩으로 변환하고 클러스터링 기술을 사용하여 유사한 세그먼트를 병합하는 것을 포함합니다.\n\n중복 감소:\n\n- 클러스터링은 각 주제가 고유하게 표현되어 중복을 줄이는 데 도움이 되며, 세분화의 전반적인 일관성과 정확도를 향상시킵니다.\n\n# 알고리즘 의사 코드\n\n<div class=\"content-ad\"></div>\n\n갭 스코어 계산:\n\n![이미지](/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_4.png)\n\n갭 스코어 부드럽게 표현:\n\n![이미지](/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_5.png)\n\n<div class=\"content-ad\"></div>\n\n경계 감지:\n\n- 각 지역 최솟값에 대한 깊이 점수가 계산됩니다.\n- 중요한 분할 지점을 결정하기 위해 매개변수 c를 사용하여 임계값을 적용합니다.\n\n# 향후 방향\n\n이 연구는 이 방법을 향상시키기 위해 추가 연구를 위한 여러 영역을 개요로 설명합니다:\n\n<div class=\"content-ad\"></div>\n\n- 자동 매개변수 최적화: 기계 학습 기술을 사용하여 매개변수를 동적으로 조정합니다.\n- 보다 광범위한 데이터 집합 실험: 다양하고 대규모 데이터셋에서의 방법을 테스트합니다.\n- 실시간 분할: 동적 문서에 대한 실시간 응용 프로그램을 탐색합니다.\n- 모델 개선: 최신 변형 모델을 통합합니다.\n- 다국어 분할: 멀티링귀얼 SBERT를 사용하여 다른 언어에 해당 방법을 적용합니다.\n- 계층적 분할: 상세한 문서 분석을 위해 여러 수준에서의 분할을 조사합니다.\n- 사용자 인터페이스 개발: 분할 결과를 더 간편하게 조정할 수 있는 대화형 도구를 만듭니다.\n- NLP 작업과의 통합: 알고리즘을 다른 자연어 처리 작업과 결합합니다.\n\n# 결론\n\n우리의 방법은 문서 분할에 정교한 접근법을 제시하며 전통적인 원칙과 첨단 문장 임베딩을 결합합니다. SBERT와 고급 스무딩 및 클러스터링 기술을 활용하여 이 프로세스는 대규모 문서에서 정확한 주제 모델링을 위한 강력하고 효율적인 솔루션을 제공합니다.\n\n논문: https://ieeexplore.ieee.org/document/10467643\n\n<div class=\"content-ad\"></div>\n\nDataDrivenInvestor.com에서 저희를 방문해주세요.\n\nDDIntel을 여기서 구독해보세요.\n\n주요 기사:\n\n저희 창작자 생태계에 참여해보세요.\n\n<div class=\"content-ad\"></div>\n\nDDI 공식 텔레그램 채널: [https://t.me/+tafUp6ecEys4YjQ1](https://t.me/+tafUp6ecEys4YjQ1)\n\nLinkedIn, Twitter, YouTube, 그리고 Facebook에서도 팔로우해주세요.","ogImage":{"url":"/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_0.png"},"coverImage":"/assets/img/2024-06-19-NewChunkingMethodforRAG-Systems_0.png","tag":["Tech"],"readingTime":7}],"page":"93","totalPageCount":112,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true}