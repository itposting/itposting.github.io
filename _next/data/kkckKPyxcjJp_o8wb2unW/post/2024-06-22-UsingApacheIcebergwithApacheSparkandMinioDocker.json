{"pageProps":{"post":{"title":" Docker로 Apache Spark와 Minio를 사용하여 Apache Iceberg 활용하는 방법","description":"","date":"2024-06-22 17:09","slug":"2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker","content":"\n\n<img src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png\" />\n\n이 게시물에서는 Apache Iceberg(데이터 테이블 형식), 분산 처리 엔진인 Apache Spark, 고성능 객체 저장 솔루션인 Minio를 결합하는 방법에 대해 탐구합니다. 주요 초점은 이러한 구성 요소를 Docker 컨테이너 내에 설정하여 통제된 환경에서 격리된 환경을 제공하는 데 있습니다. 이러한 기술을 결합하여 ACID 트랜잭션, 스키마 진화 및 Minio 내에서 효율적인 데이터 파티셔닝을 통한 효율적인 데이터 관리 기능을 확보할 수 있습니다.\n\n# 아키텍처 개요\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*gM-qHwR03S6IEh32mgJpjA.gif\" />  \n\n<div class=\"content-ad\"></div>\n\n# 컴포넌트 개요 이론적 개요📖\n\n실제 구현에 들어가기 전에, 사용할 기술들인 Apache Iceberg, Apache Spark, 그리고 Minio에 대한 간단한 이론적 개요를 살펴보겠습니다:\n\n![이미지](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_1.png)\n\n- Apache Iceberg: 차세대 데이터 테이블 형식\n\n<div class=\"content-ad\"></div>\n\n- 데이터 레이크 및 데이터 웨어하우스용으로 설계됨: Iceberg는 대규모 데이터의 복잡성을 처리하도록 특별히 구축되었습니다. 데이터 레이크와 데이터 웨어하우스 내에서 강력한 데이터 관리 기능을 제공하면서 분석 워크로드의 성능을 최적화합니다.\n- ACID 트랜잭션: Iceberg는 ACID(원자성, 일관성, 고립성, 지속성) 트랜잭션을 지원하여 동시 쓰기 시나리오에서도 데이터 무결성과 일관성을 보장합니다. 이는 동일한 데이터에 여러 애플리케이션이나 프로세스가 동시에 쓰기를 하는 경우에 데이터 유효성을 유지하고 데이터 손상을 방지하는 데 중요합니다.\n- 스키마 진화: Iceberg는 기존 데이터 형식과 달리 데이터 테이블의 스키마를 손실 없이 진화시킬 수 있습니다. 이를 통해 분석 요구 사항이나 데이터 소스가 시간이 지남에 따라 변경될 때 데이터 구조를 조정할 수 있습니다. 기존 데이터를 다시 작성하지 않고도 열을 추가, 제거 또는 수정할 수 있습니다.\n- 타임 트래블 쿼리: Iceberg는 타임 트래블 쿼리를 지원하여 언제든지 과거 데이터 스냅샷에 액세스할 수 있습니다. 이는 변경 사항 감사, 분석 파이프라인 디버깅 및 역사적 분석에 매우 유용합니다. Iceberg는 데이터 버전을 추적하여 필요한 경우 특정 버전의 테이블을 검색할 수 있습니다.\n- 효율적인 파티셔닝: Iceberg는 특정 열을 기준으로 데이터 테이블을 파티션하여 읽기 성능과 데이터 관리를 최적화합니다. 해당 파티션 값에 따라 데이터 파일을 지능적으로 저장하므로 Spark가 특정 쿼리에 대한 관련 데이터만 효율적으로 스캔할 수 있습니다. 이는 쿼리 속도를 크게 향상시킵니다.\n- 데이터 조직 및 압축: Iceberg는 자동으로 Parquet 또는 ORC와 같은 효율적인 파일 형식으로 데이터를 구성하여 효율적인 데이터 압축과 열 액세스를 용이하게 합니다. 또한 데이터 압축을 수행하여 저장 공간을 최소화하고 시간이 지남에 따라 읽기 성능을 향상시킵니다.\n\n2. Apache Spark: 통합 분석 엔진\n\n![이미지](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_2.png)\n\n- 대규모 데이터 처리: Apache Spark는 대규모 데이터셋을 클러스터로 효과적으로 처리하는 강력한 오픈 소스 분산 처리 엔진입니다. 관계형 데이터베이스, NoSQL 데이터베이스, CSV 파일 및 Minio와 같은 객체 저장소를 포함한 다양한 데이터 원본을 지원합니다.\n- 인메모리 처리: Spark는 인메모리 계산을 활용하여 성능을 향상시키며, 자주 액세스되는 데이터를 디스크 기반 솔루션에 비해 더 빠른 처리를 위해 메모리에 유지합니다.\n- 구조적, 반구조적 및 비구조적 데이터: Spark는 CSV, JSON과 같은 구조적 데이터, XML과 같은 반구조적 데이터, 텍스트와 같은 비구조적 데이터를 포함한 다양한 데이터 형식을 처리할 수 있습니다. 이러한 유연성으로 인해 현대 데이터 생태계에서 다양한 데이터 유형을 다루는 데 이상적입니다.\n- 머신러닝 및 스트림 처리: 전통적인 분석 이상으로 Spark는 머신러닝 파이프라인 및 실시간 데이터 처리까지 확장됩니다. Spark는 TensorFlow, PyTorch와 같은 인기있는 머신러닝 라이브러리를 효율적인 모델 훈련 및 배포를 위해 통합합니다. 또한 Apache Flink와 같은 스트림 처리 프레임워크를 지원하여 거의 실시간 데이터 분석을 수행할 수 있습니다.\n- Iceberg와의 원활한 통합: Spark는 네이티브 Iceberg 지원을 제공하여 Spark 애플리케이션에서 Iceberg 테이블을 직접 읽고 쓰기 및 쿼리할 수 있습니다. 이를 통해 Spark 워크플로우 내에서 데이터 관리를 간편하게 할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n3. Minio: 고성능 객체 저장 서버\n\n![Minio Image](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_3.png)\n\n- 오픈소스 및 비용 효율적: Minio는 무료 및 오픈소스 객체 저장 솔루션으로, 대형 클라우드 공급업체가 제공하는 프로프리어터리 객체 저장 서비스에 대안으로 경제적입니다. 자체 데이터 저장 인프라를 관리하고 데이터 레이크 또는 데이터 웨어하우스 내에서 해당 기능을 활용할 수 있습니다.\n- 확장성 및 성능: Minio는 확장성을 고려하여 제작되었습니다. 데이터 저장 필요가 증가함에 따라 더 많은 노드를 Minio 클러스터에 쉽게 추가할 수 있어 증가하는 데이터 양을 처리할 수 있습니다. 또한 효율적인 데이터 액세스를 제공하여 Spark 애플리케이션에 대한 효율적인 데이터 액세스를 제공합니다.\n- S3 호환성: Minio는 Amazon S3와 API 호환성이 있어서 S3와 작동하는 기존 도구 및 애플리케이션과의 원활한 통합이 가능합니다. 이는 전통적인 클라우드 객체 저장에서 더 경제적인 온프레미스 솔루션으로의 원활한 전환을 용이하게 합니다.\n- 내구성 및 신뢰성: Minio는 데이터 중복 및 복제 메커니즘을 제공하여 데이터가 장애에 대비하여 보호되도록 합니다. 다중 노드 또는 저장 장치 간의 데이터 복제를 구성하여 하드웨어 문제 발생 시 데이터 손실 위험을 최소화할 수 있습니다.\n\n4. Docker 통합\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_4.png\" />\n\n도커는 구성 요소의 실행을 격리하고 관리하는 데 사용할 수있는 컨테이너화 플랫폼을 제공합니다. 이 아키텍처에서 도커가 어떻게 적합한지 살펴보겠습니다:\n\n- **도커 이미지**: 각 서비스(Spark Master, Spark Worker 및 Minio)를 위한 도커 이미지를 만들 수 있습니다. 필요한 모든 종속성(Spark, Minio 이진 파일, Iceberg 라이브러리 등)을 포함하여 일관된 환경을 제공하고 다양한 기기에 배포를 단순화합니다.\n- **도커 콤포즈**: 도커 콤포즈와 같은 도구를 사용하여 모든 서비스의 구성 및 배포를 함께 관리할 수 있습니다. 이 도구는 서비스, 종속성 및 환경 변수를 단일 YAML 파일에 정의하여 전체 환경 설정 프로세스를 간편화합니다.\n\n도커를 활용하면 이동성이 뛰어나고 격리된 개발 또는 프로덕션 환경을 구축할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 도커 컴포즈 구성\n\n아래는 Minio, Spark 마스터 및 Spark 워커 서비스를 설정하는 Docker Compose 파일입니다.\n\n```js\nversion: '3.9'\nservices:\n  minio:\n    image: minio/minio\n    container_name: minio\n    environment:\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    command: server /data --console-address \":9001\"\n    volumes:\n      - minio_data:/data\n\nspark-master:\n    image: bitnami/spark:latest\n    container_name: spark-master-minio-iceberg\n    environment:\n      - SPARK_MODE=master\n      - SPARK_SUBMIT_ARGS=--packages org.apache.iceberg:iceberg-spark3-runtime:0.12.0\n    ports:\n      - \"7077:7077\"\n      - \"8080:8080\"\n  spark-worker:\n    image: bitnami/spark:latest\n    container_name: spark-worker-minio-iceberg\n    environment:\n      - SPARK_MODE=worker\n      - SPARK_MASTER_URL=spark://spark-master:7077\n      - SPARK_SUBMIT_ARGS=--packages org.apache.iceberg:iceberg-spark3-runtime:0.12.0\n    depends_on:\n      - spark-master\n    ports:\n      - \"8081:8081\"\nvolumes:\n  minio_data:\n```\n\n# Docker Compose 파일 설명\n\n<div class=\"content-ad\"></div>\n\n- Minio 서비스: Minio 서버를 실행하여 포트 9000(API)과 9001(콘솔)을 노출합니다. Minio 데이터는 minio_data라는 이름의 Docker 볼륨에 저장됩니다.\n- Spark Master 서비스: Delta Lake 및 Iceberg를 위한 패키지가 포함된 Spark 마스터 노드를 실행합니다. Spark UI 및 마스터 통신을 위한 포트 7077 및 8080이 노출됩니다.\n- Spark Worker 서비스: Spark 워커 노드를 실행하고 Spark 마스터에 연결됩니다. Delta Lake 및 Iceberg를 위한 패키지가 포함되어 있습니다.\n\n# 이미지 빌드\n\n다음 명령을 실행하여\n\n```js\ndocker-compose up -d\n```\n\n<div class=\"content-ad\"></div>\n\n모든 서비스가 정상적으로 작동 중이거나 문제가 발생한 경우 알림이 표시됩니다 (문제가 발생하지 않기를 희망합니다 😄)\n\n![Image 5](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_5.png)\n\n![Image 6](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_6.png)\n\n먼저 http://localhost:9001/browser를 방문하여 아래 이미지를 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_7.png\" />\n\n모든 서비스가 실행되면 Apache Iceberg를 사용하여 Apache Spark 및 Minio로 읽기와 쓰기를 하는 간단한 데이터 파이프라인을 생성해 봅시다.\n\n# Spark 작업 설정 및 실행하기\n\n다음의 Python 코드는 Minio와 상호 작용하고 Iceberg를 사용하여 Spark를 사용하는 방법을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n# Minio 클라이언트 초기화\n\n제공된 자격 증명으로 Minio 서버에 연결합니다.\n\n```js\nfrom minio import Minio\nclient = Minio(\n    \"127.0.0.1:9000\",\n    access_key=\"your_admin_name_account\",\n    secret_key=\"your_password\",\n    secure=False\n)\n```\n\n## 설명\n\n<div class=\"content-ad\"></div>\n\n- Minio 초기화: 이 코드는 Minio 클라이언트를 초기화하여 127.0.0.1의 9000 포트에서 실행 중인 Minio 서버에 연결합니다. 액세스 키와 시크릿 키로 minioadmin을 사용하고, SSL을 사용하지 않는 연결이므로 secure를 False로 설정합니다.\n\n# 버킷 관리\n\n버킷이 존재하는지 확인하고, 존재하지 않으면 생성합니다.\n\n```js\nminio_bucket = \"my-first-bucket\"\nfound = client.bucket_exists(minio_bucket)\nif not found:\n    client.make_bucket(minio_bucket)\n```\n\n<div class=\"content-ad\"></div>\n\n## 설명\n\n- Bucket 존재 여부 확인: bucket_exists 메서드는 Minio에 my-first-bucket이라는 이름의 버킷이 이미 있는지 확인합니다.\n- Bucket 생성: 버킷이 존재하지 않는 경우 make_bucket 메서드를 사용하여 my-first-bucket이라는 새 버킷을 만듭니다.\n\n# 파일 업로드\n\nMinio에 지정된 버킷에 CSV 파일을 업로드합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n목적지_파일 = 'data.csv'\n원본_파일 = './data/data.csv'  # 프로젝트 폴더에이 파일이 존재하는지 확인하세요\nclient.fput_object(minio_bucket, 목적지_파일, 원본_파일)\r\n```\n\n## 설명\n\n- 파일 업로드: fput_object 메서드는 로컬 파일 ./data/data.csv를 Minio 버킷 my-first-bucket으로 객체 이름이 data.csv인 파일로 업로드합니다.\n\n## 코드 실행 후🎦\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*rZUjqaNCls7_73eojd2RQw.gif)\n\n# SparkSession 구성\n\nSpark를 Iceberg 및 Minio를 스토리지 백엔드로 사용하도록 구성합니다.\n\n```js\nfrom pyspark.sql import SparkSession\niceberg_builder = SparkSession.builder \\\n    .appName(\"iceberg-spark-minio-example\") \\\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.iceberg:iceberg-hive-runtime:1.5.0\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n    .config(\"spark.sql.catalog.spark_catalog.warehouse\", f\"s3a://{minio_bucket}/iceberg_data/\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"your_admin_name_account\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"your_pasword\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .enableHiveSupport()\n```\n\n<div class=\"content-ad\"></div>\n\n## 설명\n\n- SparkSession 빌더: Iceberg 및 Minio를 사용하기 위해 구성된 Spark 세션 빌더를 생성합니다.\n- 애플리케이션 이름: \"iceberg-spark-minio-example\"로 애플리케이션 이름을 설정합니다.\n- JAR 패키지: Hadoop AWS 및 Iceberg에 필요한 JAR를 포함합니다.\n- Spark 확장: Spark SQL을 위해 Iceberg 확장 기능을 활성화합니다.\n- 카탈로그 구성: 카탈로그 유형을 Hadoop으로 구성하고, 웨어하우스 위치를 Minio 버킷을 가리키는 S3 경로로 설정합니다.\n- Minio 자격 증명: Minio의 액세스 및 시크릿 키를 설정합니다.\n- 엔드포인트 구성: S3 엔드포인트를 로컬 Minio 서버를 가리키도록 구성하고, S3 URI에 대한 경로 스타일 액세스를 활성화합니다.\n\n# Iceberg용 SparkSession 빌드\n\nIceberg 테이블과 상호 작용하기 위한 Spark 세션을 빌드합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\niceberg_spark = iceberg_builder.getOrCreate()\n```\n\n## 설명\n\n- SparkSession 생성: getOrCreate 메서드는 지정된 구성으로 Spark 세션을 초기화합니다.\n\n# 데이터 로드\n\n\n<div class=\"content-ad\"></div>\n\n스파크 데이터프레임으로 CSV 파일을 읽습니다.\n\n```js\ndf = iceberg_spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(source_file)\n```\n\n## 설명\n\n- CSV 파일 읽기: ./data/data.csv 파일을 스파크 데이터프레임으로 로드합니다. header 옵션은 첫 번째 행을 열 이름으로 사용하도록 설정되었고, inferSchema 옵션은 데이터 유형을 자동으로 추정하도록 설정되었습니다.\n\n<div class=\"content-ad\"></div>\n\n# 아이스버그 테이블 위치 정의\n\n미니오 내 아이스버그 테이블의 위치를 지정합니다.\n\n```js\niceberg_table_location = f\"s3a://{minio_bucket}/iceberg_data/default\"\n```\n\n## 설명\n\n<div class=\"content-ad\"></div>\n\n- Iceberg Table 위치: Minio에 Iceberg 테이블 데이터를 저장하는 데 사용되는 S3 경로를 정의합니다. 경로는 버킷 이름과 하위 디렉터리 iceberg_data/default을 사용하여 구성됩니다.\n\n# Iceberg 테이블에 쓰기\n\nDataFrame 데이터를 Minio의 Iceberg 테이블에 씁니다.\n\n```js\ndf.write \\\n    .format(\"iceberg\") \\\n    .mode(\"append\") \\\n    .saveAsTable(\"iceberg_table_name\")  # Iceberg 테이블의 이름\n```\n\n<div class=\"content-ad\"></div>\n\n## 설명\n\n- DataFrame 작성: Iceberg 형식을 사용하여 append 모드로 iceberg_table_name이라는 Iceberg 테이블에 DataFrame을 작성합니다.\n\n## 코드 실행 후🎦\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*oUTGnwgU4yk2nG31-kuIWA.gif\" />\n\n<div class=\"content-ad\"></div>\n\n# Iceberg 테이블에서 읽기\n\nIceberg 테이블에서 데이터를 읽어 스키마와 몇 가지 샘플 레코드를 표시합니다.\n\n```js\niceberg_df = iceberg_spark.read.format(\"iceberg\").load(f\"{iceberg_table_location}/iceberg_table_name\")\niceberg_df.printSchema()\niceberg_df.show()\n```\n\n## 설명\n\n<div class=\"content-ad\"></div>\n\n- 아이스버그 테이블 읽기: 아이스버그 테이블에서 데이터를 읽어와 DataFrame에로드합니다.\n- 스키마 출력: DataFrame의 스키마를 표시합니다.\n- 데이터 표시: DataFrame에서 몇 가지 샘플 레코드를 표시합니다.\n\n## 코드 실행 후🎦\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*qnK1mFZ6ecqTXBGCtEWi0w.gif\" />\n\n# 전체 코드🖥️\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom minio import Minio\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\n# Minio 클라이언트 및 버킷 생성\nclient = Minio(\n    \"127.0.0.1:9000\",\n    access_key=\"minioadmin\",\n    secret_key=\"minioadmin\",\n    secure=False\n)\n\nminio_bucket = \"my-first-bucket\"\n\nfound = client.bucket_exists(minio_bucket)\nif not found:\n    client.make_bucket(minio_bucket)\n\ndestination_file = 'data.csv'\nsource_file = './data/data.csv' ## 프로젝트 폴더 내에 파일이 있어야 함\n\n# Minio에 파일 업로드\nclient.fput_object(minio_bucket, destination_file, source_file,)\n\n# Iceberg와 Minio 설정을 사용한 SparkSession 빌더 생성\niceberg_builder = SparkSession.builder \\\n    .appName(\"iceberg-concurrent-write-isolation-test\") \\\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.iceberg:iceberg-hive-runtime:1.5.0\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n    .config(\"spark.sql.catalog.spark_catalog.warehouse\", f\"s3a://{minio_bucket}/iceberg_data/\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .enableHiveSupport()\n\n# Iceberg를 위한 SparkSession 생성\niceberg_spark = iceberg_builder.getOrCreate()\n\n# Iceberg 테이블을 Minio에 쓰기\ndf.write \\\n    .format(\"iceberg\") \\\n    .mode(\"append\") \\\n    .saveAsTable(\"iceberg_table_name\")  # Iceberg 테이블 이름\n\n# Iceberg 테이블에서 데이터 읽기\niceberg_df = iceberg_spark.read.format(\"iceberg\").load(f\"{iceberg_table_location}/iceberg_table_name\")\n\n# 데이터프레임 스키마 및 데이터 출력\nprint(\"**************************\")\nprint(\"This the Dataframe schema \")\nprint(\"**************************\")\niceberg_df.printSchema()\n\nprint(\"**************************\")\nprint(\"******Dataframe Data******\")\nprint(\"**************************\")\niceberg_df.show()\n```\n\n# GitHub\n\n프로젝트 링크\n\n# Summary\n\n\n<div class=\"content-ad\"></div>\n\n이 설정을 사용하면 Spark 및 Iceberg로 대규모 데이터를 효율적으로 관리하고 처리할 수 있고, 확장 가능한 객체 저장소로 Minio를 사용할 수 있습니다. Docker를 활용하면 이러한 구성 요소의 배포와 관리가 간편하고 재현 가능해집니다.\n\n즐거운 학습이 되길 바래요 😉","ogImage":{"url":"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png"},"coverImage":"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png","tag":["Tech"],"readingTime":14},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<img src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png\">\n<p>이 게시물에서는 Apache Iceberg(데이터 테이블 형식), 분산 처리 엔진인 Apache Spark, 고성능 객체 저장 솔루션인 Minio를 결합하는 방법에 대해 탐구합니다. 주요 초점은 이러한 구성 요소를 Docker 컨테이너 내에 설정하여 통제된 환경에서 격리된 환경을 제공하는 데 있습니다. 이러한 기술을 결합하여 ACID 트랜잭션, 스키마 진화 및 Minio 내에서 효율적인 데이터 파티셔닝을 통한 효율적인 데이터 관리 기능을 확보할 수 있습니다.</p>\n<h1>아키텍처 개요</h1>\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*gM-qHwR03S6IEh32mgJpjA.gif\">  \n<div class=\"content-ad\"></div>\n<h1>컴포넌트 개요 이론적 개요📖</h1>\n<p>실제 구현에 들어가기 전에, 사용할 기술들인 Apache Iceberg, Apache Spark, 그리고 Minio에 대한 간단한 이론적 개요를 살펴보겠습니다:</p>\n<p><img src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_1.png\" alt=\"이미지\"></p>\n<ul>\n<li>Apache Iceberg: 차세대 데이터 테이블 형식</li>\n</ul>\n<div class=\"content-ad\"></div>\n<ul>\n<li>데이터 레이크 및 데이터 웨어하우스용으로 설계됨: Iceberg는 대규모 데이터의 복잡성을 처리하도록 특별히 구축되었습니다. 데이터 레이크와 데이터 웨어하우스 내에서 강력한 데이터 관리 기능을 제공하면서 분석 워크로드의 성능을 최적화합니다.</li>\n<li>ACID 트랜잭션: Iceberg는 ACID(원자성, 일관성, 고립성, 지속성) 트랜잭션을 지원하여 동시 쓰기 시나리오에서도 데이터 무결성과 일관성을 보장합니다. 이는 동일한 데이터에 여러 애플리케이션이나 프로세스가 동시에 쓰기를 하는 경우에 데이터 유효성을 유지하고 데이터 손상을 방지하는 데 중요합니다.</li>\n<li>스키마 진화: Iceberg는 기존 데이터 형식과 달리 데이터 테이블의 스키마를 손실 없이 진화시킬 수 있습니다. 이를 통해 분석 요구 사항이나 데이터 소스가 시간이 지남에 따라 변경될 때 데이터 구조를 조정할 수 있습니다. 기존 데이터를 다시 작성하지 않고도 열을 추가, 제거 또는 수정할 수 있습니다.</li>\n<li>타임 트래블 쿼리: Iceberg는 타임 트래블 쿼리를 지원하여 언제든지 과거 데이터 스냅샷에 액세스할 수 있습니다. 이는 변경 사항 감사, 분석 파이프라인 디버깅 및 역사적 분석에 매우 유용합니다. Iceberg는 데이터 버전을 추적하여 필요한 경우 특정 버전의 테이블을 검색할 수 있습니다.</li>\n<li>효율적인 파티셔닝: Iceberg는 특정 열을 기준으로 데이터 테이블을 파티션하여 읽기 성능과 데이터 관리를 최적화합니다. 해당 파티션 값에 따라 데이터 파일을 지능적으로 저장하므로 Spark가 특정 쿼리에 대한 관련 데이터만 효율적으로 스캔할 수 있습니다. 이는 쿼리 속도를 크게 향상시킵니다.</li>\n<li>데이터 조직 및 압축: Iceberg는 자동으로 Parquet 또는 ORC와 같은 효율적인 파일 형식으로 데이터를 구성하여 효율적인 데이터 압축과 열 액세스를 용이하게 합니다. 또한 데이터 압축을 수행하여 저장 공간을 최소화하고 시간이 지남에 따라 읽기 성능을 향상시킵니다.</li>\n</ul>\n<ol start=\"2\">\n<li>Apache Spark: 통합 분석 엔진</li>\n</ol>\n<p><img src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_2.png\" alt=\"이미지\"></p>\n<ul>\n<li>대규모 데이터 처리: Apache Spark는 대규모 데이터셋을 클러스터로 효과적으로 처리하는 강력한 오픈 소스 분산 처리 엔진입니다. 관계형 데이터베이스, NoSQL 데이터베이스, CSV 파일 및 Minio와 같은 객체 저장소를 포함한 다양한 데이터 원본을 지원합니다.</li>\n<li>인메모리 처리: Spark는 인메모리 계산을 활용하여 성능을 향상시키며, 자주 액세스되는 데이터를 디스크 기반 솔루션에 비해 더 빠른 처리를 위해 메모리에 유지합니다.</li>\n<li>구조적, 반구조적 및 비구조적 데이터: Spark는 CSV, JSON과 같은 구조적 데이터, XML과 같은 반구조적 데이터, 텍스트와 같은 비구조적 데이터를 포함한 다양한 데이터 형식을 처리할 수 있습니다. 이러한 유연성으로 인해 현대 데이터 생태계에서 다양한 데이터 유형을 다루는 데 이상적입니다.</li>\n<li>머신러닝 및 스트림 처리: 전통적인 분석 이상으로 Spark는 머신러닝 파이프라인 및 실시간 데이터 처리까지 확장됩니다. Spark는 TensorFlow, PyTorch와 같은 인기있는 머신러닝 라이브러리를 효율적인 모델 훈련 및 배포를 위해 통합합니다. 또한 Apache Flink와 같은 스트림 처리 프레임워크를 지원하여 거의 실시간 데이터 분석을 수행할 수 있습니다.</li>\n<li>Iceberg와의 원활한 통합: Spark는 네이티브 Iceberg 지원을 제공하여 Spark 애플리케이션에서 Iceberg 테이블을 직접 읽고 쓰기 및 쿼리할 수 있습니다. 이를 통해 Spark 워크플로우 내에서 데이터 관리를 간편하게 할 수 있습니다.</li>\n</ul>\n<div class=\"content-ad\"></div>\n<ol start=\"3\">\n<li>Minio: 고성능 객체 저장 서버</li>\n</ol>\n<p><img src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_3.png\" alt=\"Minio Image\"></p>\n<ul>\n<li>오픈소스 및 비용 효율적: Minio는 무료 및 오픈소스 객체 저장 솔루션으로, 대형 클라우드 공급업체가 제공하는 프로프리어터리 객체 저장 서비스에 대안으로 경제적입니다. 자체 데이터 저장 인프라를 관리하고 데이터 레이크 또는 데이터 웨어하우스 내에서 해당 기능을 활용할 수 있습니다.</li>\n<li>확장성 및 성능: Minio는 확장성을 고려하여 제작되었습니다. 데이터 저장 필요가 증가함에 따라 더 많은 노드를 Minio 클러스터에 쉽게 추가할 수 있어 증가하는 데이터 양을 처리할 수 있습니다. 또한 효율적인 데이터 액세스를 제공하여 Spark 애플리케이션에 대한 효율적인 데이터 액세스를 제공합니다.</li>\n<li>S3 호환성: Minio는 Amazon S3와 API 호환성이 있어서 S3와 작동하는 기존 도구 및 애플리케이션과의 원활한 통합이 가능합니다. 이는 전통적인 클라우드 객체 저장에서 더 경제적인 온프레미스 솔루션으로의 원활한 전환을 용이하게 합니다.</li>\n<li>내구성 및 신뢰성: Minio는 데이터 중복 및 복제 메커니즘을 제공하여 데이터가 장애에 대비하여 보호되도록 합니다. 다중 노드 또는 저장 장치 간의 데이터 복제를 구성하여 하드웨어 문제 발생 시 데이터 손실 위험을 최소화할 수 있습니다.</li>\n</ul>\n<ol start=\"4\">\n<li>Docker 통합</li>\n</ol>\n<div class=\"content-ad\"></div>\n<img src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_4.png\">\n<p>도커는 구성 요소의 실행을 격리하고 관리하는 데 사용할 수있는 컨테이너화 플랫폼을 제공합니다. 이 아키텍처에서 도커가 어떻게 적합한지 살펴보겠습니다:</p>\n<ul>\n<li><strong>도커 이미지</strong>: 각 서비스(Spark Master, Spark Worker 및 Minio)를 위한 도커 이미지를 만들 수 있습니다. 필요한 모든 종속성(Spark, Minio 이진 파일, Iceberg 라이브러리 등)을 포함하여 일관된 환경을 제공하고 다양한 기기에 배포를 단순화합니다.</li>\n<li><strong>도커 콤포즈</strong>: 도커 콤포즈와 같은 도구를 사용하여 모든 서비스의 구성 및 배포를 함께 관리할 수 있습니다. 이 도구는 서비스, 종속성 및 환경 변수를 단일 YAML 파일에 정의하여 전체 환경 설정 프로세스를 간편화합니다.</li>\n</ul>\n<p>도커를 활용하면 이동성이 뛰어나고 격리된 개발 또는 프로덕션 환경을 구축할 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<h1>도커 컴포즈 구성</h1>\n<p>아래는 Minio, Spark 마스터 및 Spark 워커 서비스를 설정하는 Docker Compose 파일입니다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-attr\">version</span>: <span class=\"hljs-string\">'3.9'</span>\n<span class=\"hljs-attr\">services</span>:\n  <span class=\"hljs-attr\">minio</span>:\n    <span class=\"hljs-attr\">image</span>: minio/minio\n    <span class=\"hljs-attr\">container_name</span>: minio\n    <span class=\"hljs-attr\">environment</span>:\n      <span class=\"hljs-attr\">MINIO_ACCESS_KEY</span>: minioadmin\n      <span class=\"hljs-attr\">MINIO_SECRET_KEY</span>: minioadmin\n    <span class=\"hljs-attr\">ports</span>:\n      - <span class=\"hljs-string\">\"9000:9000\"</span>\n      - <span class=\"hljs-string\">\"9001:9001\"</span>\n    <span class=\"hljs-attr\">command</span>: server /data --<span class=\"hljs-variable language_\">console</span>-address <span class=\"hljs-string\">\":9001\"</span>\n    <span class=\"hljs-attr\">volumes</span>:\n      - <span class=\"hljs-attr\">minio_data</span>:/data\n\nspark-<span class=\"hljs-attr\">master</span>:\n    <span class=\"hljs-attr\">image</span>: bitnami/<span class=\"hljs-attr\">spark</span>:latest\n    <span class=\"hljs-attr\">container_name</span>: spark-master-minio-iceberg\n    <span class=\"hljs-attr\">environment</span>:\n      - <span class=\"hljs-variable constant_\">SPARK_MODE</span>=master\n      - <span class=\"hljs-variable constant_\">SPARK_SUBMIT_ARGS</span>=--packages org.<span class=\"hljs-property\">apache</span>.<span class=\"hljs-property\">iceberg</span>:iceberg-spark3-<span class=\"hljs-attr\">runtime</span>:<span class=\"hljs-number\">0.12</span><span class=\"hljs-number\">.0</span>\n    <span class=\"hljs-attr\">ports</span>:\n      - <span class=\"hljs-string\">\"7077:7077\"</span>\n      - <span class=\"hljs-string\">\"8080:8080\"</span>\n  spark-<span class=\"hljs-attr\">worker</span>:\n    <span class=\"hljs-attr\">image</span>: bitnami/<span class=\"hljs-attr\">spark</span>:latest\n    <span class=\"hljs-attr\">container_name</span>: spark-worker-minio-iceberg\n    <span class=\"hljs-attr\">environment</span>:\n      - <span class=\"hljs-variable constant_\">SPARK_MODE</span>=worker\n      - <span class=\"hljs-variable constant_\">SPARK_MASTER_URL</span>=<span class=\"hljs-attr\">spark</span>:<span class=\"hljs-comment\">//spark-master:7077</span>\n      - <span class=\"hljs-variable constant_\">SPARK_SUBMIT_ARGS</span>=--packages org.<span class=\"hljs-property\">apache</span>.<span class=\"hljs-property\">iceberg</span>:iceberg-spark3-<span class=\"hljs-attr\">runtime</span>:<span class=\"hljs-number\">0.12</span><span class=\"hljs-number\">.0</span>\n    <span class=\"hljs-attr\">depends_on</span>:\n      - spark-master\n    <span class=\"hljs-attr\">ports</span>:\n      - <span class=\"hljs-string\">\"8081:8081\"</span>\n<span class=\"hljs-attr\">volumes</span>:\n  <span class=\"hljs-attr\">minio_data</span>:\n</code></pre>\n<h1>Docker Compose 파일 설명</h1>\n<div class=\"content-ad\"></div>\n<ul>\n<li>Minio 서비스: Minio 서버를 실행하여 포트 9000(API)과 9001(콘솔)을 노출합니다. Minio 데이터는 minio_data라는 이름의 Docker 볼륨에 저장됩니다.</li>\n<li>Spark Master 서비스: Delta Lake 및 Iceberg를 위한 패키지가 포함된 Spark 마스터 노드를 실행합니다. Spark UI 및 마스터 통신을 위한 포트 7077 및 8080이 노출됩니다.</li>\n<li>Spark Worker 서비스: Spark 워커 노드를 실행하고 Spark 마스터에 연결됩니다. Delta Lake 및 Iceberg를 위한 패키지가 포함되어 있습니다.</li>\n</ul>\n<h1>이미지 빌드</h1>\n<p>다음 명령을 실행하여</p>\n<pre><code class=\"hljs language-js\">docker-compose up -d\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>모든 서비스가 정상적으로 작동 중이거나 문제가 발생한 경우 알림이 표시됩니다 (문제가 발생하지 않기를 희망합니다 😄)</p>\n<p><img src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_5.png\" alt=\"Image 5\"></p>\n<p><img src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_6.png\" alt=\"Image 6\"></p>\n<p>먼저 <a href=\"http://localhost:9001/browser%EB%A5%BC\" rel=\"nofollow\" target=\"_blank\">http://localhost:9001/browser를</a> 방문하여 아래 이미지를 확인해보세요.</p>\n<div class=\"content-ad\"></div>\n<img src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_7.png\">\n<p>모든 서비스가 실행되면 Apache Iceberg를 사용하여 Apache Spark 및 Minio로 읽기와 쓰기를 하는 간단한 데이터 파이프라인을 생성해 봅시다.</p>\n<h1>Spark 작업 설정 및 실행하기</h1>\n<p>다음의 Python 코드는 Minio와 상호 작용하고 Iceberg를 사용하여 Spark를 사용하는 방법을 보여줍니다.</p>\n<div class=\"content-ad\"></div>\n<h1>Minio 클라이언트 초기화</h1>\n<p>제공된 자격 증명으로 Minio 서버에 연결합니다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> minio <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Minio</span>\nclient = <span class=\"hljs-title class_\">Minio</span>(\n    <span class=\"hljs-string\">\"127.0.0.1:9000\"</span>,\n    access_key=<span class=\"hljs-string\">\"your_admin_name_account\"</span>,\n    secret_key=<span class=\"hljs-string\">\"your_password\"</span>,\n    secure=<span class=\"hljs-title class_\">False</span>\n)\n</code></pre>\n<h2>설명</h2>\n<div class=\"content-ad\"></div>\n<ul>\n<li>Minio 초기화: 이 코드는 Minio 클라이언트를 초기화하여 127.0.0.1의 9000 포트에서 실행 중인 Minio 서버에 연결합니다. 액세스 키와 시크릿 키로 minioadmin을 사용하고, SSL을 사용하지 않는 연결이므로 secure를 False로 설정합니다.</li>\n</ul>\n<h1>버킷 관리</h1>\n<p>버킷이 존재하는지 확인하고, 존재하지 않으면 생성합니다.</p>\n<pre><code class=\"hljs language-js\">minio_bucket = <span class=\"hljs-string\">\"my-first-bucket\"</span>\nfound = client.<span class=\"hljs-title function_\">bucket_exists</span>(minio_bucket)\n<span class=\"hljs-keyword\">if</span> not <span class=\"hljs-attr\">found</span>:\n    client.<span class=\"hljs-title function_\">make_bucket</span>(minio_bucket)\n</code></pre>\n<div class=\"content-ad\"></div>\n<h2>설명</h2>\n<ul>\n<li>Bucket 존재 여부 확인: bucket_exists 메서드는 Minio에 my-first-bucket이라는 이름의 버킷이 이미 있는지 확인합니다.</li>\n<li>Bucket 생성: 버킷이 존재하지 않는 경우 make_bucket 메서드를 사용하여 my-first-bucket이라는 새 버킷을 만듭니다.</li>\n</ul>\n<h1>파일 업로드</h1>\n<p>Minio에 지정된 버킷에 CSV 파일을 업로드합니다.</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\">목적지_파일 = <span class=\"hljs-string\">'data.csv'</span>\n원본_파일 = <span class=\"hljs-string\">'./data/data.csv'</span>  # 프로젝트 폴더에이 파일이 존재하는지 확인하세요\nclient.<span class=\"hljs-title function_\">fput_object</span>(minio_bucket, 목적지_파일, 원본_파일)\n</code></pre>\n<h2>설명</h2>\n<ul>\n<li>파일 업로드: fput_object 메서드는 로컬 파일 ./data/data.csv를 Minio 버킷 my-first-bucket으로 객체 이름이 data.csv인 파일로 업로드합니다.</li>\n</ul>\n<h2>코드 실행 후🎦</h2>\n<div class=\"content-ad\"></div>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*rZUjqaNCls7_73eojd2RQw.gif\" alt=\"이미지\"></p>\n<h1>SparkSession 구성</h1>\n<p>Spark를 Iceberg 및 Minio를 스토리지 백엔드로 사용하도록 구성합니다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">SparkSession</span>\niceberg_builder = <span class=\"hljs-title class_\">SparkSession</span>.<span class=\"hljs-property\">builder</span> \\\n    .<span class=\"hljs-title function_\">appName</span>(<span class=\"hljs-string\">\"iceberg-spark-minio-example\"</span>) \\\n    .<span class=\"hljs-title function_\">config</span>(<span class=\"hljs-string\">\"spark.jars.packages\"</span>, <span class=\"hljs-string\">\"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.iceberg:iceberg-hive-runtime:1.5.0\"</span>) \\\n    .<span class=\"hljs-title function_\">config</span>(<span class=\"hljs-string\">\"spark.sql.extensions\"</span>, <span class=\"hljs-string\">\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"</span>) \\\n    .<span class=\"hljs-title function_\">config</span>(<span class=\"hljs-string\">\"spark.sql.catalog.spark_catalog\"</span>, <span class=\"hljs-string\">\"org.apache.iceberg.spark.SparkCatalog\"</span>) \\\n    .<span class=\"hljs-title function_\">config</span>(<span class=\"hljs-string\">\"spark.sql.catalog.spark_catalog.type\"</span>, <span class=\"hljs-string\">\"hadoop\"</span>) \\\n    .<span class=\"hljs-title function_\">config</span>(<span class=\"hljs-string\">\"spark.sql.catalog.spark_catalog.warehouse\"</span>, f<span class=\"hljs-string\">\"s3a://{minio_bucket}/iceberg_data/\"</span>) \\\n    .<span class=\"hljs-title function_\">config</span>(<span class=\"hljs-string\">\"spark.hadoop.fs.s3a.access.key\"</span>, <span class=\"hljs-string\">\"your_admin_name_account\"</span>) \\\n    .<span class=\"hljs-title function_\">config</span>(<span class=\"hljs-string\">\"spark.hadoop.fs.s3a.secret.key\"</span>, <span class=\"hljs-string\">\"your_pasword\"</span>) \\\n    .<span class=\"hljs-title function_\">config</span>(<span class=\"hljs-string\">\"spark.hadoop.fs.s3a.endpoint\"</span>, <span class=\"hljs-string\">\"http://localhost:9000\"</span>) \\\n    .<span class=\"hljs-title function_\">config</span>(<span class=\"hljs-string\">\"spark.hadoop.fs.s3a.path.style.access\"</span>, <span class=\"hljs-string\">\"true\"</span>) \\\n    .<span class=\"hljs-title function_\">enableHiveSupport</span>()\n</code></pre>\n<div class=\"content-ad\"></div>\n<h2>설명</h2>\n<ul>\n<li>SparkSession 빌더: Iceberg 및 Minio를 사용하기 위해 구성된 Spark 세션 빌더를 생성합니다.</li>\n<li>애플리케이션 이름: \"iceberg-spark-minio-example\"로 애플리케이션 이름을 설정합니다.</li>\n<li>JAR 패키지: Hadoop AWS 및 Iceberg에 필요한 JAR를 포함합니다.</li>\n<li>Spark 확장: Spark SQL을 위해 Iceberg 확장 기능을 활성화합니다.</li>\n<li>카탈로그 구성: 카탈로그 유형을 Hadoop으로 구성하고, 웨어하우스 위치를 Minio 버킷을 가리키는 S3 경로로 설정합니다.</li>\n<li>Minio 자격 증명: Minio의 액세스 및 시크릿 키를 설정합니다.</li>\n<li>엔드포인트 구성: S3 엔드포인트를 로컬 Minio 서버를 가리키도록 구성하고, S3 URI에 대한 경로 스타일 액세스를 활성화합니다.</li>\n</ul>\n<h1>Iceberg용 SparkSession 빌드</h1>\n<p>Iceberg 테이블과 상호 작용하기 위한 Spark 세션을 빌드합니다.</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\">iceberg_spark = iceberg_builder.<span class=\"hljs-title function_\">getOrCreate</span>()\n</code></pre>\n<h2>설명</h2>\n<ul>\n<li>SparkSession 생성: getOrCreate 메서드는 지정된 구성으로 Spark 세션을 초기화합니다.</li>\n</ul>\n<h1>데이터 로드</h1>\n<div class=\"content-ad\"></div>\n<p>스파크 데이터프레임으로 CSV 파일을 읽습니다.</p>\n<pre><code class=\"hljs language-js\">df = iceberg_spark.<span class=\"hljs-property\">read</span>.<span class=\"hljs-title function_\">format</span>(<span class=\"hljs-string\">'csv'</span>).<span class=\"hljs-title function_\">option</span>(<span class=\"hljs-string\">'header'</span>, <span class=\"hljs-string\">'true'</span>).<span class=\"hljs-title function_\">option</span>(<span class=\"hljs-string\">'inferSchema'</span>, <span class=\"hljs-string\">'true'</span>).<span class=\"hljs-title function_\">load</span>(source_file)\n</code></pre>\n<h2>설명</h2>\n<ul>\n<li>CSV 파일 읽기: ./data/data.csv 파일을 스파크 데이터프레임으로 로드합니다. header 옵션은 첫 번째 행을 열 이름으로 사용하도록 설정되었고, inferSchema 옵션은 데이터 유형을 자동으로 추정하도록 설정되었습니다.</li>\n</ul>\n<div class=\"content-ad\"></div>\n<h1>아이스버그 테이블 위치 정의</h1>\n<p>미니오 내 아이스버그 테이블의 위치를 지정합니다.</p>\n<pre><code class=\"hljs language-js\">iceberg_table_location = f<span class=\"hljs-string\">\"s3a://{minio_bucket}/iceberg_data/default\"</span>\n</code></pre>\n<h2>설명</h2>\n<div class=\"content-ad\"></div>\n<ul>\n<li>Iceberg Table 위치: Minio에 Iceberg 테이블 데이터를 저장하는 데 사용되는 S3 경로를 정의합니다. 경로는 버킷 이름과 하위 디렉터리 iceberg_data/default을 사용하여 구성됩니다.</li>\n</ul>\n<h1>Iceberg 테이블에 쓰기</h1>\n<p>DataFrame 데이터를 Minio의 Iceberg 테이블에 씁니다.</p>\n<pre><code class=\"hljs language-js\">df.<span class=\"hljs-property\">write</span> \\\n    .<span class=\"hljs-title function_\">format</span>(<span class=\"hljs-string\">\"iceberg\"</span>) \\\n    .<span class=\"hljs-title function_\">mode</span>(<span class=\"hljs-string\">\"append\"</span>) \\\n    .<span class=\"hljs-title function_\">saveAsTable</span>(<span class=\"hljs-string\">\"iceberg_table_name\"</span>)  # <span class=\"hljs-title class_\">Iceberg</span> 테이블의 이름\n</code></pre>\n<div class=\"content-ad\"></div>\n<h2>설명</h2>\n<ul>\n<li>DataFrame 작성: Iceberg 형식을 사용하여 append 모드로 iceberg_table_name이라는 Iceberg 테이블에 DataFrame을 작성합니다.</li>\n</ul>\n<h2>코드 실행 후🎦</h2>\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*oUTGnwgU4yk2nG31-kuIWA.gif\">\n<div class=\"content-ad\"></div>\n<h1>Iceberg 테이블에서 읽기</h1>\n<p>Iceberg 테이블에서 데이터를 읽어 스키마와 몇 가지 샘플 레코드를 표시합니다.</p>\n<pre><code class=\"hljs language-js\">iceberg_df = iceberg_spark.<span class=\"hljs-property\">read</span>.<span class=\"hljs-title function_\">format</span>(<span class=\"hljs-string\">\"iceberg\"</span>).<span class=\"hljs-title function_\">load</span>(f<span class=\"hljs-string\">\"{iceberg_table_location}/iceberg_table_name\"</span>)\niceberg_df.<span class=\"hljs-title function_\">printSchema</span>()\niceberg_df.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<h2>설명</h2>\n<div class=\"content-ad\"></div>\n<ul>\n<li>아이스버그 테이블 읽기: 아이스버그 테이블에서 데이터를 읽어와 DataFrame에로드합니다.</li>\n<li>스키마 출력: DataFrame의 스키마를 표시합니다.</li>\n<li>데이터 표시: DataFrame에서 몇 가지 샘플 레코드를 표시합니다.</li>\n</ul>\n<h2>코드 실행 후🎦</h2>\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*qnK1mFZ6ecqTXBGCtEWi0w.gif\">\n<h1>전체 코드🖥️</h1>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> minio <span class=\"hljs-keyword\">import</span> Minio\n<span class=\"hljs-keyword\">from</span> pyspark.sql <span class=\"hljs-keyword\">import</span> SparkSession\n<span class=\"hljs-keyword\">from</span> pyspark.sql.types <span class=\"hljs-keyword\">import</span> StructType, StructField, StringType, IntegerType\n\n<span class=\"hljs-comment\"># Minio 클라이언트 및 버킷 생성</span>\nclient = Minio(\n    <span class=\"hljs-string\">\"127.0.0.1:9000\"</span>,\n    access_key=<span class=\"hljs-string\">\"minioadmin\"</span>,\n    secret_key=<span class=\"hljs-string\">\"minioadmin\"</span>,\n    secure=<span class=\"hljs-literal\">False</span>\n)\n\nminio_bucket = <span class=\"hljs-string\">\"my-first-bucket\"</span>\n\nfound = client.bucket_exists(minio_bucket)\n<span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> found:\n    client.make_bucket(minio_bucket)\n\ndestination_file = <span class=\"hljs-string\">'data.csv'</span>\nsource_file = <span class=\"hljs-string\">'./data/data.csv'</span> <span class=\"hljs-comment\">## 프로젝트 폴더 내에 파일이 있어야 함</span>\n\n<span class=\"hljs-comment\"># Minio에 파일 업로드</span>\nclient.fput_object(minio_bucket, destination_file, source_file,)\n\n<span class=\"hljs-comment\"># Iceberg와 Minio 설정을 사용한 SparkSession 빌더 생성</span>\niceberg_builder = SparkSession.builder \\\n    .appName(<span class=\"hljs-string\">\"iceberg-concurrent-write-isolation-test\"</span>) \\\n    .config(<span class=\"hljs-string\">\"spark.jars.packages\"</span>, <span class=\"hljs-string\">\"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.iceberg:iceberg-hive-runtime:1.5.0\"</span>) \\\n    .config(<span class=\"hljs-string\">\"spark.sql.extensions\"</span>, <span class=\"hljs-string\">\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"</span>) \\\n    .config(<span class=\"hljs-string\">\"spark.sql.catalog.spark_catalog\"</span>, <span class=\"hljs-string\">\"org.apache.iceberg.spark.SparkCatalog\"</span>) \\\n    .config(<span class=\"hljs-string\">\"spark.sql.catalog.spark_catalog.type\"</span>, <span class=\"hljs-string\">\"hadoop\"</span>) \\\n    .config(<span class=\"hljs-string\">\"spark.sql.catalog.spark_catalog.warehouse\"</span>, <span class=\"hljs-string\">f\"s3a://<span class=\"hljs-subst\">{minio_bucket}</span>/iceberg_data/\"</span>) \\\n    .config(<span class=\"hljs-string\">\"spark.hadoop.fs.s3a.access.key\"</span>, <span class=\"hljs-string\">\"minioadmin\"</span>) \\\n    .config(<span class=\"hljs-string\">\"spark.hadoop.fs.s3a.secret.key\"</span>, <span class=\"hljs-string\">\"minioadmin\"</span>) \\\n    .config(<span class=\"hljs-string\">\"spark.hadoop.fs.s3a.endpoint\"</span>, <span class=\"hljs-string\">\"http://localhost:9000\"</span>) \\\n    .config(<span class=\"hljs-string\">\"spark.hadoop.fs.s3a.path.style.access\"</span>, <span class=\"hljs-string\">\"true\"</span>) \\\n    .enableHiveSupport()\n\n<span class=\"hljs-comment\"># Iceberg를 위한 SparkSession 생성</span>\niceberg_spark = iceberg_builder.getOrCreate()\n\n<span class=\"hljs-comment\"># Iceberg 테이블을 Minio에 쓰기</span>\ndf.write \\\n    .<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-string\">\"iceberg\"</span>) \\\n    .mode(<span class=\"hljs-string\">\"append\"</span>) \\\n    .saveAsTable(<span class=\"hljs-string\">\"iceberg_table_name\"</span>)  <span class=\"hljs-comment\"># Iceberg 테이블 이름</span>\n\n<span class=\"hljs-comment\"># Iceberg 테이블에서 데이터 읽기</span>\niceberg_df = iceberg_spark.read.<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-string\">\"iceberg\"</span>).load(<span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{iceberg_table_location}</span>/iceberg_table_name\"</span>)\n\n<span class=\"hljs-comment\"># 데이터프레임 스키마 및 데이터 출력</span>\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"**************************\"</span>)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"This the Dataframe schema \"</span>)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"**************************\"</span>)\niceberg_df.printSchema()\n\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"**************************\"</span>)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"******Dataframe Data******\"</span>)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"**************************\"</span>)\niceberg_df.show()\n</code></pre>\n<h1>GitHub</h1>\n<p>프로젝트 링크</p>\n<h1>Summary</h1>\n<div class=\"content-ad\"></div>\n<p>이 설정을 사용하면 Spark 및 Iceberg로 대규모 데이터를 효율적으로 관리하고 처리할 수 있고, 확장 가능한 객체 저장소로 Minio를 사용할 수 있습니다. Docker를 활용하면 이러한 구성 요소의 배포와 관리가 간편하고 재현 가능해집니다.</p>\n<p>즐거운 학습이 되길 바래요 😉</p>\n</body>\n</html>\n"},"__N_SSG":true}