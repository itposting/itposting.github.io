{"pageProps":{"post":{"title":"Code Llamaë¡œ ë‚˜ë§Œì˜ LLM ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ ë§Œë“œëŠ” ë°©ë²• ","description":"","date":"2024-06-22 21:36","slug":"2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama","content":"\n\nì´ ì‹¤ìŠµì—ì„œëŠ” ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê³  ë¡œì»¬ GPUì—ì„œ ì‹¤í–‰ë˜ëŠ” AI ì½”ë“œ ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ êµ¬í˜„í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n\nì±—ë´‡ì— ì§ˆë¬¸ì„ í•˜ë©´ ìì—°ì–´ë¡œ ë‹µë³€í•˜ë©° ì—¬ëŸ¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ ì½”ë“œë„ ì œê³µí•©ë‹ˆë‹¤.\n\nìš°ë¦¬ëŠ” Hugging Face transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ LLMì„ êµ¬í˜„í•˜ê³  Chatbot í”„ë¡ íŠ¸ ì—”ë“œì—ëŠ” Streamlitì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.\n\n# LLMì´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?\n\n<div class=\"content-ad\"></div>\n\në””ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì¸ GPT ê³„ì—´ì€ ì£¼ì–´ì§„ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ í…ìŠ¤íŠ¸ ìƒì„±ì— ì•„ì£¼ ëŠ¥ìˆ™í•©ë‹ˆë‹¤.\n\n![ì´ë¯¸ì§€](/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_0.png)\n\nì¶©ë¶„í•œ í›ˆë ¨ ë°ì´í„°ê°€ ì œê³µëœë‹¤ë©´, ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” ê²ƒë„ ë°°ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. IDEì—ì„œ ì½”ë“œë¥¼ ì±„ìš°ëŠ” ë°©ì‹ì´ë‚˜ ì±—ë´‡ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\nGitHub Copilotì€ ìƒìš© ì˜ˆì‹œë¡œì„œ AI í˜ì–´ í”„ë¡œê·¸ë˜ë¨¸ì˜ í•œ ì˜ˆì…ë‹ˆë‹¤. Meta AIì˜ Code Llama ëª¨ë¸ì€ ìœ ì‚¬í•œ ëŠ¥ë ¥ì„ ê°–ì¶”ê³  ìˆì§€ë§Œ ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n<div class=\"content-ad\"></div>\n\n# ì½”ë“œ ëŒë§ˆë€ ë¬´ì—‡ì¸ê°€ìš”?\n\nì½”ë“œ ëŒë§ˆëŠ” Meta AIê°€ ë§Œë“¤ê³  2023ë…„ 8ì›”ì— ì²˜ìŒìœ¼ë¡œ ì¶œì‹œí•œ ì½”ë“œ ì „ìš© LLM ê³„ì—´ì˜ íŠ¹ë³„í•œ ì œí’ˆì…ë‹ˆë‹¤.\n\n![ì´ë¯¸ì§€](/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_1.png)\n\nMeta AIëŠ” ê¸°ë³¸ ëª¨ë¸ Llama 2(ë””ì½”ë” ì „ìš© Transformer ëª¨ë¸ë¡œ GPT-4ì™€ ìœ ì‚¬í•¨)ì„ ì‹œì‘ìœ¼ë¡œ, ëŒ€ë¶€ë¶„ ì½”ë“œë¡œ ì´ë£¨ì–´ì§„ 500B í† í°ì˜ êµìœ¡ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì¶”ê°€ êµìœ¡ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.\n\n<div class=\"content-ad\"></div>\n\nê·¸ ì´í›„ë¡œ Code Llamaì— ëŒ€í•œ ì„¸ ê°€ì§€ ë²„ì „ì´ ë„¤ ê°€ì§€ ë‹¤ë¥¸ í¬ê¸°ë¡œ ì œê³µë©ë‹ˆë‹¤.\n\nCode Llama ëª¨ë¸ì€ ì—°êµ¬ ë° ìƒì—…ì  ì‚¬ìš©ì„ ìœ„í•´ ë¬´ë£Œì…ë‹ˆë‹¤.\n\n![ì´ë¯¸ì§€](/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_2.png)\n\n## Code Llama\n\n<div class=\"content-ad\"></div>\n\nì½”ë“œ LlamaëŠ” ì½”ë“œ ìƒì„±ì„ ìœ„í•œ ê¸°ë°˜ ëª¨ë¸ì…ë‹ˆë‹¤. ì½”ë“œ Llama ëª¨ë¸ì€ infill ëª©ì ìœ¼ë¡œ í›ˆë ¨ë˜ì–´ IDE ë‚´ì—ì„œ ì½”ë“œ ì™„ì„±ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n## ì½”ë“œ Llama â€” Instruct\n\nInstruct ë²„ì „ì€ ì¸ê°„ì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ì§€ì‹œ ë°ì´í„°ì…‹ì— ë§ì¶° ì„¸ë°€í•˜ê²Œ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ChatGPTì™€ ìœ ì‚¬í•©ë‹ˆë‹¤.\n\n## ì½”ë“œ Llama â€” Python\n\n<div class=\"content-ad\"></div>\n\níŒŒì´ì¬ ë²„ì „ì€ ì¶”ê°€ ë°ì´í„°ì…‹ì¸ 100B í† í°ì˜ íŒŒì´ì¬ ì½”ë“œë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ë“¤ì€ ì½”ë“œ ìƒì„±ì„ ìœ„í•´ ì˜ë„ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n# LLM ì±—ë´‡ ì½”ë”©\n\në³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Instruct ë²„ì „ ì¤‘ ê°€ì¥ ì‘ì€ ëª¨ë¸ì¸ CodeLlama-7b-Instruct â€” hfë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ìì—°ì–´ ì§ˆë¬¸ì— ë‹µë³€í•˜ë„ë¡ ì„¸ë°€í•˜ê²Œ íŠœë‹ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì±—ë´‡ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nê°€ì¥ ì‘ì€ ëª¨ë¸ì¡°ì°¨ë„ ì—¬ì „íˆ 7B ë§¤ê°œë³€ìˆ˜ë¡œ ìƒë‹¹íˆ í½ë‹ˆë‹¤. ë§¤ê°œë³€ìˆ˜ì˜ 16ë¹„íŠ¸ ë°˜ì •ë°€ë„ë¥¼ ì‚¬ìš©í•˜ë©´, ëª¨ë¸ì€ ì•½ 14 GBì˜ GPU ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. 4ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ë©´, ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì„ ì•½ 3.5 GB ì •ë„ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n<div class=\"content-ad\"></div>\n\n## ëª¨ë¸ êµ¬í˜„í•˜ê¸°\n\nìš°ë¦¬ëŠ” ë¨¼ì € Hugging Faceì—ì„œ Code Llama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³  ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì— ê¸°ë°˜í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ChatModel í´ë˜ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.\n\nìš°ë¦¬ëŠ” 4ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ìœ„í•´ BitsAndBytesConfigë¥¼ ì‚¬ìš©í•˜ë©°, ëª¨ë¸ì„ ë¡œë“œí•˜ê¸° ìœ„í•´ AutoModelForCausalLMì„ ì‚¬ìš©í•˜ê³  ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° í† í° ì„ë² ë”©ì„ ìƒì„±í•˜ê¸° ìœ„í•´ AutoTokenizerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n```js\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nclass ChatModel:\n    def __init__(self, model=\"codellama/CodeLlama-7b-Instruct-hf\"):\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True, # 4ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš©\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n        )\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model,\n            quantization_config=quantization_config,\n            device_map=\"cuda\",\n            cache_dir=\"./models\", # ëª¨ë¸ì„ models í´ë”ì— ë‹¤ìš´ë¡œë“œ\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            model, use_fast=True, padding_side=\"left\"\n        )\r\n```\n\n<div class=\"content-ad\"></div>\n\në˜í•œ, ì‚¬ìš©ìì˜ ì´ì „ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì™€ AIê°€ ìƒì„±í•œ ì‘ë‹µì„ ì €ì¥í•˜ëŠ” ê³ ì • ê¸¸ì´ì˜ íˆìŠ¤í† ë¦¬ ëª©ë¡ì„ ë§Œë“­ë‹ˆë‹¤. ì´ëŠ” ëŒ€í™”ì˜ ê¸°ì–µì„ ì œê³µí•˜ì—¬ LLMì—ê²Œ ëŒ€í™”ì˜ ê¸°ì–µì„ ë¶€ì—¬í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.\n\n```js\nself.history = []\nself.history_length = 1\n```\n\nCode Llamaì€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì•ì— ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\nê¸°ë³¸ì ìœ¼ë¡œ, codellama-13b-chat ì˜ˆì œì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n<div class=\"content-ad\"></div>\n\n```js\nself.DEFAULT_SYSTEM_PROMPT = \"\"\"\\\në‹¹ì‹ ì€ ì½”ë“œì™€ ì†Œí”„íŠ¸ì›¨ì–´ ë””ìì¸ì— ëŒ€í•œ ê¹Šì€ ì§€ì‹ì„ ê°€ì§„, ë„ì›€ì´ ë˜ëŠ”, ì˜ˆì˜ ë°”ë¥´ê³  ì •ì§í•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. í•­ìƒ ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ í•´ì•¼ í•˜ë©°, ì•ˆì „í•˜ê³  ì‹ ì¤‘í•´ì•¼ í•©ë‹ˆë‹¤. ë‹µë³€ì— í•´ë¡œìš´, ë¶€ì •í•œ, ì¸ì¢… ì°¨ë³„ì , ì„± ì°¨ë³„ì , ìœ í•´í•œ, ìœ„í—˜í•œ, ë˜ëŠ” ë¶ˆë²•ì ì¸ ë‚´ìš©ì„ í¬í•¨í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. ë‹µë³€ì´ ì‚¬íšŒì ìœ¼ë¡œ í¸í–¥ë˜ê±°ë‚˜ ë¶€ì •ì ì´ì—¬ì„  ì•ˆë©ë‹ˆë‹¤.\\n\\në§Œì•½ ì§ˆë¬¸ì´ ì´í•´í•  ìˆ˜ ì—†ê±°ë‚˜ ì‚¬ì‹¤ì ìœ¼ë¡œ ì¼ê´€ì„±ì´ ì—†ë‹¤ë©´, ì˜¬ë°”ë¥¸ ëŒ€ë‹µ ëŒ€ì‹  ì™œ ì˜ëª»ëœ ê²ƒì¸ì§€ ì„¤ëª…í•˜ì„¸ìš”. ë§Œì•½ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ ëª¨ë¥´ë©´, ê°€ì§œ ì •ë³´ë¥¼ ê³µìœ í•˜ì§€ ë§ê³  ëŒ€ì‹  ë§í•´ì£¼ì„¸ìš”.\\\n        \"\"\"\n```\n\nì´ì œ self.historyì— í˜„ì¬ ëŒ€í™”ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ë´…ì‹œë‹¤.\n\nLLM(ì–´ë¼ìš´ë“œ  ëª¨ë¸)ì€ í•œì •ëœ ë¬¸ë§¥ ê¸¸ì´ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ë©”ëª¨ë¦¬ì— ì •ë³´ë¥¼ í•œì •ì ìœ¼ë¡œ ë³´ê´€í•  ìˆ˜ë°–ì— ì—†ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” self.history_length = 1 ê°œì˜ ì§ˆë¬¸ê³¼ ëŒ€ë‹µë§Œ ìµœëŒ€í•œ ë³´ê´€í•©ë‹ˆë‹¤.\n\n```js\n    def append_to_history(self, user_prompt, response):\n        self.history.append((user_prompt, response))\n        if len(self.history) > self.history_length:\n            self.history.pop(0)\n```\n\n<div class=\"content-ad\"></div>\n\në§ˆì¹¨ë‚´ ìš°ë¦¬ëŠ” ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— ê¸°ë°˜í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” generate í•¨ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n\nê° LLMì—ëŠ” í›ˆë ¨ì— ì‚¬ìš©ëœ íŠ¹ì • í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì´ ìˆìŠµë‹ˆë‹¤. Code Llamaì˜ ê²½ìš° codellama-13b-chatì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì°¸ì¡°ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\n\n```js\n    def generate(\n        self, user_prompt, system_prompt, top_p=0.9, temperature=0.1, max_new_tokens=512\n    ):\n\n        texts = [f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n\"]\n        do_strip = False\n        for old_prompt, old_response in self.history:\n            old_prompt = old_prompt.strip() if do_strip else old_prompt\n            do_strip = True\n            texts.append(f\"{old_prompt} [/INST] {old_response.strip()} </s><s>[INST] \")\n        user_prompt = user_prompt.strip() if do_strip else user_prompt\n        texts.append(f\"{user_prompt} [/INST]\")\n        prompt = \"\".join(texts)\n\n        inputs = self.tokenizer(\n            prompt, return_tensors=\"pt\", add_special_tokens=False\n        ).to(\"cuda\")\n\n        output = self.model.generate(\n            inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            pad_token_id=self.tokenizer.eos_token_id,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            top_p=top_p,\n            top_k=50,\n            temperature=temperature,\n        )\n        output = output[0].to(\"cpu\")\n        response = self.tokenizer.decode(output[inputs[\"input_ids\"].shape[1] : -1])\n        self.append_to_history(user_prompt, response)\n        return response\n```\n\nì‘ë‹µì€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ë‹µë³€ì˜ ì°½ì˜ì„±ì€ top_p ë° temperatureì™€ ê°™ì€ ë§¤ê°œë³€ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.\n\n<div class=\"content-ad\"></div>\n\ntop_pë¥¼ ì‚¬ìš©í•˜ë©´ ì¶œë ¥ í† í°ì˜ í™•ë¥  ê°’ì„ ì œí•œí•˜ì—¬ ë„ˆë¬´ ë“œë¬¼ê²Œ ë°œìƒí•˜ëŠ” í† í°ì„ ìƒì„±í•˜ëŠ” ê²ƒì„ í”¼í•  ìˆ˜ ìˆì–´ìš”:\n\ntemperatureë¥¼ ì‚¬ìš©í•˜ë©´ ì¶œë ¥ í† í°ì˜ í™•ë¥  ë¶„í¬ë¥¼ í‰í‰í•˜ê²Œ í•˜ê±°ë‚˜ ë‚ ì¹´ë¡­ê²Œ í•  ìˆ˜ ìˆì–´ìš”:\n\ní”„ë¡ íŠ¸ì—”ë“œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì§„í–‰í•˜ê¸° ì „ì— ChatModelì„ í…ŒìŠ¤íŠ¸í•´ë³´ì£ .\n\n```js\nfrom ChatModel import *\n\nmodel = ChatModel()\nresponse = model.generate(\n    user_prompt=\"C++ì—ì„œ hello world í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•´ë´\", \n    system_prompt=model.DEFAULT_SYSTEM_PROMPT\n)\nprint(response)\n```\n\n<div class=\"content-ad\"></div>\n\n```js\në‹¹ì‹ ì´ ìš”ì²­í•œ ì‘ì—…ì€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ í…Œì´ë¸” íƒœê·¸ê°€ Markdown í˜•ì‹ìœ¼ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.\n```\n\n<div class=\"content-ad\"></div>\n\n```js\r\nimport streamlit as st\nfrom ChatModel import *\n\nst.title(\"Code Llama Assistant\")\n\n\n@st.cache_resource\ndef load_model():\n    model = ChatModel()\n    return model\n\n\nmodel = load_model()  # load our ChatModel once and then cache it\r\n```\n\në‹¤ìŒìœ¼ë¡œ generate í•¨ìˆ˜ë¥¼ ìœ„í•œ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ì…ë ¥ ì œì–´í•˜ëŠ” ì‚¬ì´ë“œë°”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n\n```js\r\nwith st.sidebar:\n    temperature = st.slider(\"ì˜¨ë„\", 0.0, 2.0, 0.1)\n    top_p = st.slider(\"top_p\", 0.0, 1.0, 0.9)\n    max_new_tokens = st.number_input(\"max_new_tokens\", 128, 4096, 256)\n    system_prompt = st.text_area(\n        \"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\", value=model.DEFAULT_SYSTEM_PROMPT, height=500\n    )\r\n```\n\nê·¸ë¦¬ê³  ì±—ë´‡ ë©”ì‹œì§€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n\n<div class=\"content-ad\"></div>\n\n```js\n# ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\n# ì•± ì¬ì‹¤í–‰ì‹œ ê¸°ë¡ëœ ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\n# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°\nif prompt := st.chat_input(\"ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!\"):\n    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ… ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆì— í‘œì‹œ\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n\n    # ì±—ë´‡ ì‘ë‹µì„ ì±„íŒ… ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆì— í‘œì‹œ\n    with st.chat_message(\"assistant\"):\n        user_prompt = st.session_state.messages[-1][\"content\"]\n        answer = model.generate(\n            user_prompt,\n            top_p=top_p,\n            temperature=temperature,\n            max_new_tokens=max_new_tokens,\n            system_prompt=system_prompt,\n        )\n        response = st.write(answer)\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\r\n```\n\nìŠ¤íŠ¸ë¦¼ë¦¿ ì•±ì„ streamlit run app.pyë¡œ ì‹¤í–‰í•˜ì—¬ ë¸Œë¼ìš°ì €ê°€ ì—´ë¦½ë‹ˆë‹¤.\n\nì´ì œ ì±—ë´‡ì— ì½”ë”© ê´€ë ¨ ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n# ê²°ë¡ \n\n<div class=\"content-ad\"></div>\n\nì €í¬ëŠ” Meta AIì˜ Code Llama LLMì„ í™œìš©í•˜ì—¬ AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ êµ¬í˜„í–ˆì–´ìš”. ê·¸ë¦¬ê³  Hugging Faceì˜ transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ Streamlitì„ ì‚¬ìš©í•´ì„œ í”„ë¡ íŠ¸ì—”ë“œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ì—ˆì–´ìš”.\n\n6GBì˜ GPU ë©”ëª¨ë¦¬ë¥¼ ê°–ì¶˜ ë…¸íŠ¸ë¶ìœ¼ë¡œëŠ” 4ë¹„íŠ¸ ì–‘ìí™”ëœ Code Llama ëª¨ë¸ì„ 7B ë§¤ê°œë³€ìˆ˜ì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ë°–ì— ì—†ì—ˆì–´ìš”. ë” í° GPUë¥¼ ì‚¬ìš©í•˜ë©´ 16ë¹„íŠ¸ ë²„ì „ì´ë‚˜ ë” í° ëª¨ë¸ì´ ë” ì˜ ì‘ë™í•  ê²ƒì…ë‹ˆë‹¤.\n\nP.S. Code Llamaë¡œë¶€í„° ì œê°€ ë°›ì€ ë†ë‹´ë³´ë‹¤ ë” ì¬ë¯¸ìˆëŠ” ë†ë‹´ë“¤ì„ ê¸°ëŒ€í•´ë´…ë‹ˆë‹¤ ğŸ¤¡.\n\në” ë§ì€ LLMì— ê´€ì‹¬ì´ ìˆìœ¼ì‹œë‹¤ë©´, ìµœê·¼ì— ê³µê°œëœ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì— ëŒ€í•œ ê°œìš”ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”:\n\n<div class=\"content-ad\"></div>\n\n# ì°¸ê³  ìë£Œ\n\n[1] B. RoziÃ¨re ì™¸: Code Llama: ì½”ë“œë¥¼ ìœ„í•œ ì˜¤í”ˆ ê¸°ë°˜ ëª¨ë¸ (2023), arXiv:2308.12950\n\n# ìì›\n\n- Streamlit ì±„íŒ… ì•± ì˜ˆì œ: ê¸°ë³¸ LLM ì±„íŒ… ì•± êµ¬ì¶•\n- Hugging Face Code Llama gradio êµ¬í˜„: codellama-13b-chat\n- ì´ ë¬¸ì„œì˜ ì „ì²´ ì‘ì—… ì½”ë“œ: [https://github.com/leoneversberg/codellama-chatbot](https://github.com/leoneversberg/codellama-chatbot)","ogImage":{"url":"/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_0.png"},"coverImage":"/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_0.png","tag":["Tech"],"readingTime":9},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>ì´ ì‹¤ìŠµì—ì„œëŠ” ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê³  ë¡œì»¬ GPUì—ì„œ ì‹¤í–‰ë˜ëŠ” AI ì½”ë“œ ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ êµ¬í˜„í•  ì˜ˆì •ì…ë‹ˆë‹¤.</p>\n<p>ì±—ë´‡ì— ì§ˆë¬¸ì„ í•˜ë©´ ìì—°ì–´ë¡œ ë‹µë³€í•˜ë©° ì—¬ëŸ¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ ì½”ë“œë„ ì œê³µí•©ë‹ˆë‹¤.</p>\n<p>ìš°ë¦¬ëŠ” Hugging Face transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ LLMì„ êµ¬í˜„í•˜ê³  Chatbot í”„ë¡ íŠ¸ ì—”ë“œì—ëŠ” Streamlitì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.</p>\n<h1>LLMì´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?</h1>\n<div class=\"content-ad\"></div>\n<p>ë””ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì¸ GPT ê³„ì—´ì€ ì£¼ì–´ì§„ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ í…ìŠ¤íŠ¸ ìƒì„±ì— ì•„ì£¼ ëŠ¥ìˆ™í•©ë‹ˆë‹¤.</p>\n<p><img src=\"/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_0.png\" alt=\"ì´ë¯¸ì§€\"></p>\n<p>ì¶©ë¶„í•œ í›ˆë ¨ ë°ì´í„°ê°€ ì œê³µëœë‹¤ë©´, ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” ê²ƒë„ ë°°ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. IDEì—ì„œ ì½”ë“œë¥¼ ì±„ìš°ëŠ” ë°©ì‹ì´ë‚˜ ì±—ë´‡ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê°€ëŠ¥í•©ë‹ˆë‹¤.</p>\n<p>GitHub Copilotì€ ìƒìš© ì˜ˆì‹œë¡œì„œ AI í˜ì–´ í”„ë¡œê·¸ë˜ë¨¸ì˜ í•œ ì˜ˆì…ë‹ˆë‹¤. Meta AIì˜ Code Llama ëª¨ë¸ì€ ìœ ì‚¬í•œ ëŠ¥ë ¥ì„ ê°–ì¶”ê³  ìˆì§€ë§Œ ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<div class=\"content-ad\"></div>\n<h1>ì½”ë“œ ëŒë§ˆë€ ë¬´ì—‡ì¸ê°€ìš”?</h1>\n<p>ì½”ë“œ ëŒë§ˆëŠ” Meta AIê°€ ë§Œë“¤ê³  2023ë…„ 8ì›”ì— ì²˜ìŒìœ¼ë¡œ ì¶œì‹œí•œ ì½”ë“œ ì „ìš© LLM ê³„ì—´ì˜ íŠ¹ë³„í•œ ì œí’ˆì…ë‹ˆë‹¤.</p>\n<p><img src=\"/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_1.png\" alt=\"ì´ë¯¸ì§€\"></p>\n<p>Meta AIëŠ” ê¸°ë³¸ ëª¨ë¸ Llama 2(ë””ì½”ë” ì „ìš© Transformer ëª¨ë¸ë¡œ GPT-4ì™€ ìœ ì‚¬í•¨)ì„ ì‹œì‘ìœ¼ë¡œ, ëŒ€ë¶€ë¶„ ì½”ë“œë¡œ ì´ë£¨ì–´ì§„ 500B í† í°ì˜ êµìœ¡ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì¶”ê°€ êµìœ¡ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.</p>\n<div class=\"content-ad\"></div>\n<p>ê·¸ ì´í›„ë¡œ Code Llamaì— ëŒ€í•œ ì„¸ ê°€ì§€ ë²„ì „ì´ ë„¤ ê°€ì§€ ë‹¤ë¥¸ í¬ê¸°ë¡œ ì œê³µë©ë‹ˆë‹¤.</p>\n<p>Code Llama ëª¨ë¸ì€ ì—°êµ¬ ë° ìƒì—…ì  ì‚¬ìš©ì„ ìœ„í•´ ë¬´ë£Œì…ë‹ˆë‹¤.</p>\n<p><img src=\"/assets/img/2024-06-22-HowtoBuildYourOwnLLMCodingAssistantWithCodeLlama_2.png\" alt=\"ì´ë¯¸ì§€\"></p>\n<h2>Code Llama</h2>\n<div class=\"content-ad\"></div>\n<p>ì½”ë“œ LlamaëŠ” ì½”ë“œ ìƒì„±ì„ ìœ„í•œ ê¸°ë°˜ ëª¨ë¸ì…ë‹ˆë‹¤. ì½”ë“œ Llama ëª¨ë¸ì€ infill ëª©ì ìœ¼ë¡œ í›ˆë ¨ë˜ì–´ IDE ë‚´ì—ì„œ ì½”ë“œ ì™„ì„±ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.</p>\n<h2>ì½”ë“œ Llama â€” Instruct</h2>\n<p>Instruct ë²„ì „ì€ ì¸ê°„ì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ì§€ì‹œ ë°ì´í„°ì…‹ì— ë§ì¶° ì„¸ë°€í•˜ê²Œ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ChatGPTì™€ ìœ ì‚¬í•©ë‹ˆë‹¤.</p>\n<h2>ì½”ë“œ Llama â€” Python</h2>\n<div class=\"content-ad\"></div>\n<p>íŒŒì´ì¬ ë²„ì „ì€ ì¶”ê°€ ë°ì´í„°ì…‹ì¸ 100B í† í°ì˜ íŒŒì´ì¬ ì½”ë“œë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ë“¤ì€ ì½”ë“œ ìƒì„±ì„ ìœ„í•´ ì˜ë„ë˜ì—ˆìŠµë‹ˆë‹¤.</p>\n<h1>LLM ì±—ë´‡ ì½”ë”©</h1>\n<p>ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Instruct ë²„ì „ ì¤‘ ê°€ì¥ ì‘ì€ ëª¨ë¸ì¸ CodeLlama-7b-Instruct â€” hfë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ìì—°ì–´ ì§ˆë¬¸ì— ë‹µë³€í•˜ë„ë¡ ì„¸ë°€í•˜ê²Œ íŠœë‹ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì±—ë´‡ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<p>ê°€ì¥ ì‘ì€ ëª¨ë¸ì¡°ì°¨ë„ ì—¬ì „íˆ 7B ë§¤ê°œë³€ìˆ˜ë¡œ ìƒë‹¹íˆ í½ë‹ˆë‹¤. ë§¤ê°œë³€ìˆ˜ì˜ 16ë¹„íŠ¸ ë°˜ì •ë°€ë„ë¥¼ ì‚¬ìš©í•˜ë©´, ëª¨ë¸ì€ ì•½ 14 GBì˜ GPU ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. 4ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ë©´, ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì„ ì•½ 3.5 GB ì •ë„ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<div class=\"content-ad\"></div>\n<h2>ëª¨ë¸ êµ¬í˜„í•˜ê¸°</h2>\n<p>ìš°ë¦¬ëŠ” ë¨¼ì € Hugging Faceì—ì„œ Code Llama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³  ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì— ê¸°ë°˜í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ChatModel í´ë˜ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.</p>\n<p>ìš°ë¦¬ëŠ” 4ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ìœ„í•´ BitsAndBytesConfigë¥¼ ì‚¬ìš©í•˜ë©°, ëª¨ë¸ì„ ë¡œë“œí•˜ê¸° ìœ„í•´ AutoModelForCausalLMì„ ì‚¬ìš©í•˜ê³  ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° í† í° ì„ë² ë”©ì„ ìƒì„±í•˜ê¸° ìœ„í•´ AutoTokenizerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">AutoTokenizer</span>, <span class=\"hljs-title class_\">AutoModelForCausalLM</span>, <span class=\"hljs-title class_\">BitsAndBytesConfig</span>\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">ChatModel</span>:\n    def <span class=\"hljs-title function_\">__init__</span>(self, model=<span class=\"hljs-string\">\"codellama/CodeLlama-7b-Instruct-hf\"</span>):\n        quantization_config = <span class=\"hljs-title class_\">BitsAndBytesConfig</span>(\n            load_in_4bit=<span class=\"hljs-title class_\">True</span>, # <span class=\"hljs-number\">4</span>ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš©\n            bnb_4bit_compute_dtype=torch.<span class=\"hljs-property\">float16</span>,\n            bnb_4bit_use_double_quant=<span class=\"hljs-title class_\">True</span>,\n        )\n        self.<span class=\"hljs-property\">model</span> = <span class=\"hljs-title class_\">AutoModelForCausalLM</span>.<span class=\"hljs-title function_\">from_pretrained</span>(\n            model,\n            quantization_config=quantization_config,\n            device_map=<span class=\"hljs-string\">\"cuda\"</span>,\n            cache_dir=<span class=\"hljs-string\">\"./models\"</span>, # ëª¨ë¸ì„ models í´ë”ì— ë‹¤ìš´ë¡œë“œ\n        )\n        self.<span class=\"hljs-property\">tokenizer</span> = <span class=\"hljs-title class_\">AutoTokenizer</span>.<span class=\"hljs-title function_\">from_pretrained</span>(\n            model, use_fast=<span class=\"hljs-title class_\">True</span>, padding_side=<span class=\"hljs-string\">\"left\"</span>\n        )\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>ë˜í•œ, ì‚¬ìš©ìì˜ ì´ì „ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì™€ AIê°€ ìƒì„±í•œ ì‘ë‹µì„ ì €ì¥í•˜ëŠ” ê³ ì • ê¸¸ì´ì˜ íˆìŠ¤í† ë¦¬ ëª©ë¡ì„ ë§Œë“­ë‹ˆë‹¤. ì´ëŠ” ëŒ€í™”ì˜ ê¸°ì–µì„ ì œê³µí•˜ì—¬ LLMì—ê²Œ ëŒ€í™”ì˜ ê¸°ì–µì„ ë¶€ì—¬í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\">self.<span class=\"hljs-property\">history</span> = []\nself.<span class=\"hljs-property\">history_length</span> = <span class=\"hljs-number\">1</span>\n</code></pre>\n<p>Code Llamaì€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì•ì— ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.</p>\n<p>ê¸°ë³¸ì ìœ¼ë¡œ, codellama-13b-chat ì˜ˆì œì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\">self.<span class=\"hljs-property\">DEFAULT_SYSTEM_PROMPT</span> = <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\\\në‹¹ì‹ ì€ ì½”ë“œì™€ ì†Œí”„íŠ¸ì›¨ì–´ ë””ìì¸ì— ëŒ€í•œ ê¹Šì€ ì§€ì‹ì„ ê°€ì§„, ë„ì›€ì´ ë˜ëŠ”, ì˜ˆì˜ ë°”ë¥´ê³  ì •ì§í•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. í•­ìƒ ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ í•´ì•¼ í•˜ë©°, ì•ˆì „í•˜ê³  ì‹ ì¤‘í•´ì•¼ í•©ë‹ˆë‹¤. ë‹µë³€ì— í•´ë¡œìš´, ë¶€ì •í•œ, ì¸ì¢… ì°¨ë³„ì , ì„± ì°¨ë³„ì , ìœ í•´í•œ, ìœ„í—˜í•œ, ë˜ëŠ” ë¶ˆë²•ì ì¸ ë‚´ìš©ì„ í¬í•¨í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. ë‹µë³€ì´ ì‚¬íšŒì ìœ¼ë¡œ í¸í–¥ë˜ê±°ë‚˜ ë¶€ì •ì ì´ì—¬ì„  ì•ˆë©ë‹ˆë‹¤.\\n\\në§Œì•½ ì§ˆë¬¸ì´ ì´í•´í•  ìˆ˜ ì—†ê±°ë‚˜ ì‚¬ì‹¤ì ìœ¼ë¡œ ì¼ê´€ì„±ì´ ì—†ë‹¤ë©´, ì˜¬ë°”ë¥¸ ëŒ€ë‹µ ëŒ€ì‹  ì™œ ì˜ëª»ëœ ê²ƒì¸ì§€ ì„¤ëª…í•˜ì„¸ìš”. ë§Œì•½ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ ëª¨ë¥´ë©´, ê°€ì§œ ì •ë³´ë¥¼ ê³µìœ í•˜ì§€ ë§ê³  ëŒ€ì‹  ë§í•´ì£¼ì„¸ìš”.\\\n        \"</span><span class=\"hljs-string\">\"\"</span>\n</code></pre>\n<p>ì´ì œ self.historyì— í˜„ì¬ ëŒ€í™”ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ë´…ì‹œë‹¤.</p>\n<p>LLM(ì–´ë¼ìš´ë“œ  ëª¨ë¸)ì€ í•œì •ëœ ë¬¸ë§¥ ê¸¸ì´ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ë©”ëª¨ë¦¬ì— ì •ë³´ë¥¼ í•œì •ì ìœ¼ë¡œ ë³´ê´€í•  ìˆ˜ë°–ì— ì—†ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” self.history_length = 1 ê°œì˜ ì§ˆë¬¸ê³¼ ëŒ€ë‹µë§Œ ìµœëŒ€í•œ ë³´ê´€í•©ë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\">    def <span class=\"hljs-title function_\">append_to_history</span>(self, user_prompt, response):\n        self.<span class=\"hljs-property\">history</span>.<span class=\"hljs-title function_\">append</span>((user_prompt, response))\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-title function_\">len</span>(self.<span class=\"hljs-property\">history</span>) > self.<span class=\"hljs-property\">history_length</span>:\n            self.<span class=\"hljs-property\">history</span>.<span class=\"hljs-title function_\">pop</span>(<span class=\"hljs-number\">0</span>)\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>ë§ˆì¹¨ë‚´ ìš°ë¦¬ëŠ” ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— ê¸°ë°˜í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” generate í•¨ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.</p>\n<p>ê° LLMì—ëŠ” í›ˆë ¨ì— ì‚¬ìš©ëœ íŠ¹ì • í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì´ ìˆìŠµë‹ˆë‹¤. Code Llamaì˜ ê²½ìš° codellama-13b-chatì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì°¸ì¡°ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\">    def <span class=\"hljs-title function_\">generate</span>(\n        self, user_prompt, system_prompt, top_p=<span class=\"hljs-number\">0.9</span>, temperature=<span class=\"hljs-number\">0.1</span>, max_new_tokens=<span class=\"hljs-number\">512</span>\n    ):\n\n        texts = [f<span class=\"hljs-string\">\"&#x3C;s>[INST] &#x3C;&#x3C;SYS>>\\n{system_prompt}\\n&#x3C;&#x3C;/SYS>>\\n\\n\"</span>]\n        do_strip = <span class=\"hljs-title class_\">False</span>\n        <span class=\"hljs-keyword\">for</span> old_prompt, old_response <span class=\"hljs-keyword\">in</span> self.<span class=\"hljs-property\">history</span>:\n            old_prompt = old_prompt.<span class=\"hljs-title function_\">strip</span>() <span class=\"hljs-keyword\">if</span> do_strip <span class=\"hljs-keyword\">else</span> old_prompt\n            do_strip = <span class=\"hljs-title class_\">True</span>\n            texts.<span class=\"hljs-title function_\">append</span>(f<span class=\"hljs-string\">\"{old_prompt} [/INST] {old_response.strip()} &#x3C;/s>&#x3C;s>[INST] \"</span>)\n        user_prompt = user_prompt.<span class=\"hljs-title function_\">strip</span>() <span class=\"hljs-keyword\">if</span> do_strip <span class=\"hljs-keyword\">else</span> user_prompt\n        texts.<span class=\"hljs-title function_\">append</span>(f<span class=\"hljs-string\">\"{user_prompt} [/INST]\"</span>)\n        prompt = <span class=\"hljs-string\">\"\"</span>.<span class=\"hljs-title function_\">join</span>(texts)\n\n        inputs = self.<span class=\"hljs-title function_\">tokenizer</span>(\n            prompt, return_tensors=<span class=\"hljs-string\">\"pt\"</span>, add_special_tokens=<span class=\"hljs-title class_\">False</span>\n        ).<span class=\"hljs-title function_\">to</span>(<span class=\"hljs-string\">\"cuda\"</span>)\n\n        output = self.<span class=\"hljs-property\">model</span>.<span class=\"hljs-title function_\">generate</span>(\n            inputs[<span class=\"hljs-string\">\"input_ids\"</span>],\n            attention_mask=inputs[<span class=\"hljs-string\">\"attention_mask\"</span>],\n            pad_token_id=self.<span class=\"hljs-property\">tokenizer</span>.<span class=\"hljs-property\">eos_token_id</span>,\n            max_new_tokens=max_new_tokens,\n            do_sample=<span class=\"hljs-title class_\">True</span>,\n            top_p=top_p,\n            top_k=<span class=\"hljs-number\">50</span>,\n            temperature=temperature,\n        )\n        output = output[<span class=\"hljs-number\">0</span>].<span class=\"hljs-title function_\">to</span>(<span class=\"hljs-string\">\"cpu\"</span>)\n        response = self.<span class=\"hljs-property\">tokenizer</span>.<span class=\"hljs-title function_\">decode</span>(output[inputs[<span class=\"hljs-string\">\"input_ids\"</span>].<span class=\"hljs-property\">shape</span>[<span class=\"hljs-number\">1</span>] : -<span class=\"hljs-number\">1</span>])\n        self.<span class=\"hljs-title function_\">append_to_history</span>(user_prompt, response)\n        <span class=\"hljs-keyword\">return</span> response\n</code></pre>\n<p>ì‘ë‹µì€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ë‹µë³€ì˜ ì°½ì˜ì„±ì€ top_p ë° temperatureì™€ ê°™ì€ ë§¤ê°œë³€ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.</p>\n<div class=\"content-ad\"></div>\n<p>top_pë¥¼ ì‚¬ìš©í•˜ë©´ ì¶œë ¥ í† í°ì˜ í™•ë¥  ê°’ì„ ì œí•œí•˜ì—¬ ë„ˆë¬´ ë“œë¬¼ê²Œ ë°œìƒí•˜ëŠ” í† í°ì„ ìƒì„±í•˜ëŠ” ê²ƒì„ í”¼í•  ìˆ˜ ìˆì–´ìš”:</p>\n<p>temperatureë¥¼ ì‚¬ìš©í•˜ë©´ ì¶œë ¥ í† í°ì˜ í™•ë¥  ë¶„í¬ë¥¼ í‰í‰í•˜ê²Œ í•˜ê±°ë‚˜ ë‚ ì¹´ë¡­ê²Œ í•  ìˆ˜ ìˆì–´ìš”:</p>\n<p>í”„ë¡ íŠ¸ì—”ë“œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì§„í–‰í•˜ê¸° ì „ì— ChatModelì„ í…ŒìŠ¤íŠ¸í•´ë³´ì£ .</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> <span class=\"hljs-title class_\">ChatModel</span> <span class=\"hljs-keyword\">import</span> *\n\nmodel = <span class=\"hljs-title class_\">ChatModel</span>()\nresponse = model.<span class=\"hljs-title function_\">generate</span>(\n    user_prompt=<span class=\"hljs-string\">\"C++ì—ì„œ hello world í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•´ë´\"</span>, \n    system_prompt=model.<span class=\"hljs-property\">DEFAULT_SYSTEM_PROMPT</span>\n)\n<span class=\"hljs-title function_\">print</span>(response)\n</code></pre>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\">ë‹¹ì‹ ì´ ìš”ì²­í•œ ì‘ì—…ì€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ í…Œì´ë¸” íƒœê·¸ê°€ <span class=\"hljs-title class_\">Markdown</span> í˜•ì‹ìœ¼ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.\n</code></pre>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> streamlit <span class=\"hljs-keyword\">as</span> st\n<span class=\"hljs-keyword\">from</span> <span class=\"hljs-title class_\">ChatModel</span> <span class=\"hljs-keyword\">import</span> *\n\nst.<span class=\"hljs-title function_\">title</span>(<span class=\"hljs-string\">\"Code Llama Assistant\"</span>)\n\n\n@st.<span class=\"hljs-property\">cache_resource</span>\ndef <span class=\"hljs-title function_\">load_model</span>():\n    model = <span class=\"hljs-title class_\">ChatModel</span>()\n    <span class=\"hljs-keyword\">return</span> model\n\n\nmodel = <span class=\"hljs-title function_\">load_model</span>()  # load our <span class=\"hljs-title class_\">ChatModel</span> once and then cache it\n</code></pre>\n<p>ë‹¤ìŒìœ¼ë¡œ generate í•¨ìˆ˜ë¥¼ ìœ„í•œ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ì…ë ¥ ì œì–´í•˜ëŠ” ì‚¬ì´ë“œë°”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">with</span> st.<span class=\"hljs-property\">sidebar</span>:\n    temperature = st.<span class=\"hljs-title function_\">slider</span>(<span class=\"hljs-string\">\"ì˜¨ë„\"</span>, <span class=\"hljs-number\">0.0</span>, <span class=\"hljs-number\">2.0</span>, <span class=\"hljs-number\">0.1</span>)\n    top_p = st.<span class=\"hljs-title function_\">slider</span>(<span class=\"hljs-string\">\"top_p\"</span>, <span class=\"hljs-number\">0.0</span>, <span class=\"hljs-number\">1.0</span>, <span class=\"hljs-number\">0.9</span>)\n    max_new_tokens = st.<span class=\"hljs-title function_\">number_input</span>(<span class=\"hljs-string\">\"max_new_tokens\"</span>, <span class=\"hljs-number\">128</span>, <span class=\"hljs-number\">4096</span>, <span class=\"hljs-number\">256</span>)\n    system_prompt = st.<span class=\"hljs-title function_\">text_area</span>(\n        <span class=\"hljs-string\">\"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\"</span>, value=model.<span class=\"hljs-property\">DEFAULT_SYSTEM_PROMPT</span>, height=<span class=\"hljs-number\">500</span>\n    )\n</code></pre>\n<p>ê·¸ë¦¬ê³  ì±—ë´‡ ë©”ì‹œì§€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\"># ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”\n<span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">\"messages\"</span> not <span class=\"hljs-keyword\">in</span> st.<span class=\"hljs-property\">session_state</span>:\n    st.<span class=\"hljs-property\">session_state</span>.<span class=\"hljs-property\">messages</span> = []\n\n# ì•± ì¬ì‹¤í–‰ì‹œ ê¸°ë¡ëœ ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ\n<span class=\"hljs-keyword\">for</span> message <span class=\"hljs-keyword\">in</span> st.<span class=\"hljs-property\">session_state</span>.<span class=\"hljs-property\">messages</span>:\n    <span class=\"hljs-keyword\">with</span> st.<span class=\"hljs-title function_\">chat_message</span>(message[<span class=\"hljs-string\">\"role\"</span>]):\n        st.<span class=\"hljs-title function_\">markdown</span>(message[<span class=\"hljs-string\">\"content\"</span>])\n\n# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°\n<span class=\"hljs-keyword\">if</span> prompt := st.<span class=\"hljs-title function_\">chat_input</span>(<span class=\"hljs-string\">\"ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!\"</span>):\n    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€\n    st.<span class=\"hljs-property\">session_state</span>.<span class=\"hljs-property\">messages</span>.<span class=\"hljs-title function_\">append</span>({<span class=\"hljs-string\">\"role\"</span>: <span class=\"hljs-string\">\"user\"</span>, <span class=\"hljs-string\">\"content\"</span>: prompt})\n    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ… ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆì— í‘œì‹œ\n    <span class=\"hljs-keyword\">with</span> st.<span class=\"hljs-title function_\">chat_message</span>(<span class=\"hljs-string\">\"user\"</span>):\n        st.<span class=\"hljs-title function_\">markdown</span>(prompt)\n\n    # ì±—ë´‡ ì‘ë‹µì„ ì±„íŒ… ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆì— í‘œì‹œ\n    <span class=\"hljs-keyword\">with</span> st.<span class=\"hljs-title function_\">chat_message</span>(<span class=\"hljs-string\">\"assistant\"</span>):\n        user_prompt = st.<span class=\"hljs-property\">session_state</span>.<span class=\"hljs-property\">messages</span>[-<span class=\"hljs-number\">1</span>][<span class=\"hljs-string\">\"content\"</span>]\n        answer = model.<span class=\"hljs-title function_\">generate</span>(\n            user_prompt,\n            top_p=top_p,\n            temperature=temperature,\n            max_new_tokens=max_new_tokens,\n            system_prompt=system_prompt,\n        )\n        response = st.<span class=\"hljs-title function_\">write</span>(answer)\n    st.<span class=\"hljs-property\">session_state</span>.<span class=\"hljs-property\">messages</span>.<span class=\"hljs-title function_\">append</span>({<span class=\"hljs-string\">\"role\"</span>: <span class=\"hljs-string\">\"assistant\"</span>, <span class=\"hljs-string\">\"content\"</span>: answer})\n</code></pre>\n<p>ìŠ¤íŠ¸ë¦¼ë¦¿ ì•±ì„ streamlit run app.pyë¡œ ì‹¤í–‰í•˜ì—¬ ë¸Œë¼ìš°ì €ê°€ ì—´ë¦½ë‹ˆë‹¤.</p>\n<p>ì´ì œ ì±—ë´‡ì— ì½”ë”© ê´€ë ¨ ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<h1>ê²°ë¡ </h1>\n<div class=\"content-ad\"></div>\n<p>ì €í¬ëŠ” Meta AIì˜ Code Llama LLMì„ í™œìš©í•˜ì—¬ AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ êµ¬í˜„í–ˆì–´ìš”. ê·¸ë¦¬ê³  Hugging Faceì˜ transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ Streamlitì„ ì‚¬ìš©í•´ì„œ í”„ë¡ íŠ¸ì—”ë“œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ì—ˆì–´ìš”.</p>\n<p>6GBì˜ GPU ë©”ëª¨ë¦¬ë¥¼ ê°–ì¶˜ ë…¸íŠ¸ë¶ìœ¼ë¡œëŠ” 4ë¹„íŠ¸ ì–‘ìí™”ëœ Code Llama ëª¨ë¸ì„ 7B ë§¤ê°œë³€ìˆ˜ì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ë°–ì— ì—†ì—ˆì–´ìš”. ë” í° GPUë¥¼ ì‚¬ìš©í•˜ë©´ 16ë¹„íŠ¸ ë²„ì „ì´ë‚˜ ë” í° ëª¨ë¸ì´ ë” ì˜ ì‘ë™í•  ê²ƒì…ë‹ˆë‹¤.</p>\n<p>P.S. Code Llamaë¡œë¶€í„° ì œê°€ ë°›ì€ ë†ë‹´ë³´ë‹¤ ë” ì¬ë¯¸ìˆëŠ” ë†ë‹´ë“¤ì„ ê¸°ëŒ€í•´ë´…ë‹ˆë‹¤ ğŸ¤¡.</p>\n<p>ë” ë§ì€ LLMì— ê´€ì‹¬ì´ ìˆìœ¼ì‹œë‹¤ë©´, ìµœê·¼ì— ê³µê°œëœ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì— ëŒ€í•œ ê°œìš”ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”:</p>\n<div class=\"content-ad\"></div>\n<h1>ì°¸ê³  ìë£Œ</h1>\n<p>[1] B. RoziÃ¨re ì™¸: Code Llama: ì½”ë“œë¥¼ ìœ„í•œ ì˜¤í”ˆ ê¸°ë°˜ ëª¨ë¸ (2023), arXiv:2308.12950</p>\n<h1>ìì›</h1>\n<ul>\n<li>Streamlit ì±„íŒ… ì•± ì˜ˆì œ: ê¸°ë³¸ LLM ì±„íŒ… ì•± êµ¬ì¶•</li>\n<li>Hugging Face Code Llama gradio êµ¬í˜„: codellama-13b-chat</li>\n<li>ì´ ë¬¸ì„œì˜ ì „ì²´ ì‘ì—… ì½”ë“œ: <a href=\"https://github.com/leoneversberg/codellama-chatbot\" rel=\"nofollow\" target=\"_blank\">https://github.com/leoneversberg/codellama-chatbot</a></li>\n</ul>\n</body>\n</html>\n"},"__N_SSG":true}