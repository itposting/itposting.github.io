{"pageProps":{"posts":[{"title":"품질 엔지니어를 위한 RAG 사용 가이드","description":"","date":"2024-06-22 17:14","slug":"2024-06-22-RAGforQualityEngineers","content":"\n\n## RAG 만들기는 쉽지만, 품질 있는 RAG 만들기는 어려워요\n\n![image](/assets/img/2024-06-22-RAGforQualityEngineers_0.png)\n\n검색 확장 생성(RAG)은 대규모 언어 모델(LLMs)의 기능을 확장하는 일반적인 패턴이 되었습니다.\n\n이론적으로 RAG는 간단합니다(컨텍스트 창에 데이터를 추가하기만 하면 됩니다!) 하지만 실제로는 복잡합니다. 상자 다이어그램 너머에는 고급 청킹 전략, 재랭킹, 다중 쿼리 리트리버, 작은 데이터부터 큰 데이터 검색, 가상 문서 임베딩, 사전 임베딩 데이터 보강, 동적 라우팅, 맞춤 임베딩 모델... 등이 숨어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n초기 파이프라인을 설정하는 것은 빠르고 쉽지만, 프로덕션 수준의 품질에 도달하는 것은 상당히 복잡합니다. 신중한 고려 없이 RAG 시스템은 부정확하거나 관련성이 없는 정보를 반환할 수 있습니다. 비효율적으로 비싼 리소스를 소비하거나 프로덕션 규모의 소스 데이터로 확장할 때 병목 현상이 발생할 수도 있습니다.\n\nRAG 시스템의 품질을 효과적으로 평가하고 효율적으로 이해하는 것은 각 개별 구성 요소가 전체 RAG 파이프라인을 만드는 과정을 이해하는 데 필요합니다. 이러한 각 부분에 대한 설계 결정은 품질에 영향을 미치며, RAG 애플리케이션을 배포하려는 모든 사람이 알아야 합니다.\n\n이 글에서는 테스트와 품질 관점에서 RAG 개념과 패턴에 대한 소개를 제공합니다. RAG가 왜 가치 있는지에 대한 소개로 시작하여, 프로덕션 품질의 RAG를 구축하는 데 내재된 많은 설계 결정이 어떻게 향상되는지에 대해 설명합니다. 이 소개는 특정 평가 방법과 기법에 대해 논의하기 전에 필요한 기초를 제공할 것입니다.\n\n# LLM의 한계\n\n<div class=\"content-ad\"></div>\n\nRAG 파이프라인을 이해하려면, RAG가 대응하려는 LLM의 한계를 먼저 이해해야 합니다.\n\nLLM의 핵심은 간단합니다: 프롬프트를 보내면 응답을 받습니다.\n\nLLM이 응답을 반환하려면 모델과 추론 계산을 실행해야 합니다. 이 계산은 입력을 수백억 개 또는 수조의 매개변수와 결합하는 것을 포함합니다. 이는 비용이 많이 드는 계산입니다.\n\nLLM을 호출하는 것만큼이나 LLM을 훈련시키는 것은 훨씬 어렵습니다. 훈련은 모델 내 매개변수에 대한 최적값을 결정하는 과정입니다. 최상의 가중치를 계산하는 데 사용되는 다양한 알고리즘이 있지만, 모두 특정 입력에서 모델을 실행하고 오차를 계산한 후에 조정을 하는 반복적 과정을 포함합니다. 이 과정은 많은 횟수로 많은 입력에서 계속되며, 결국 훈련된 모델을 얻게 됩니다. 모델 추론은 몇 초만에 완료될 수 있지만, 모델 훈련은 광범위한 GPU 클러스터에서도 몇 주가 걸릴 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-RAGforQualityEngineers_1.png\" />\n\n엄청난 교육 비용은 새로운 정보를 LLM에 통합하는데 병목 현상을 야기합니다. 대부분의 기업은 모델을 교육할 자원이 없으며, 단순히 사적 데이터로 교육하여 LLM에 \"새로운 정보를 추가\"할 수 없습니다. 대신, 잘 자금을 지원받는 대규모 기술 기업은 대형 공개 데이터 세트에서 일반 목적의 기반이 되는 모델을 교육하고, 이러한 모델은 RAG와 같은 보조 프로세스로 새로운 능력 및 정보가 부가됩니다.\n\n구체적으로 RAG는 새로운 모델을 교육하는 높은 비용을 우회하는 방식으로 LLM이 추가적인 지식에 접근하도록 하는 것을 목표로 합니다.\n\n# RAG 기본 원리\n\n<div class=\"content-ad\"></div>\n\nRAG가 실제로 작동하는 방식에 대해 알아봅시다.\n\nLLM에 전송된 프롬프트는 제한된 길이인 context window을 가지고 있습니다. Context window은 토큰(단어에 대한 대략적인 동등물) 단위로 측정됩니다. Context window의 크기는 보통 1K, 4K 또는 그 이상의 토큰으로 표시되지만 더 큰 context window이 사용 가능해지고 있습니다(예: Gemini 1.5 Pro의 128K).\n\n많은 사람들이 직관적으로 context window은 할 수 있는 가장 긴 질문이라고 생각하지만, 이는 context window에 대한 한정적인 사고 방식입니다. LLM의 작동 방식으로 인해, context window에 제공된 정보는 LLM이 응답을 생성하는 동안 LLM에게 사용 가능합니다. 따라서 추가 정보를 제공하는 데 사용될 수 있습니다. 일반적으로 이를 in-context learning 이라고 합니다.\n\n따라서, 우리는 context window을 사용하여 LLM이 질문에 답변하기 위해 필요한 새로운 지식을 제공할 수 있습니다. 예를 들어, 우리가 상조에 관한 회사 정책에 대해 물어보는 프롬프트를 만들고, context window 내에서 이에 관한 전체 회사 안내서(상조에 관한 섹션 포함)를 넣는다면 새로운 정보를 제공하여 LLM이 응답할 수 있게 할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n해당 솔루션은 이미 관련 정보가 있고 그 정보가 문맥 창 안에 쏙 들어갈 수 있는 경우 간단합니다. 불행히도 항상 그런 것은 아닙니다. 따라서 우리는 우리의 프롬프트에 관련 정보만 검색 및 다운 선택할 수 있는 메커니즘이 필요합니다.\n\n무식한 접근법은 프롬프트에서 용어를 검색하여 관련할 수 있는 데이터 전체에서 주변 텍스트를 복사한 후 이를 프롬프트에 추가하는 것입니다.\n\n이 간단한 키워드 검색 형태의 RAG는 LLM 응답을 향상시킬 수 있으며 어떤 맥락에서는 유용할 수 있지만 가끔식 거짓 양성 (다른 맥락에서 사용된 키워드) 때문에 문제가 될 수도 있습니다. 다행히도 우리는 텍스트의 의미에 맞추어 일치시키는 의미 검색을 활용하여 더 나은 결과를 얻을 수 있습니다.\n\n구체적으로, 우리는 포지블리 관련 데이터 청크에서 임베딩 모델을 활용하여 임베딩을 생성한 다음 이 임베딩을 통해 데이터를 검색하여 우리의 프롬프트에 관련된 데이터를 찾을 수 있습니다. 이 방법은 매우 단순화된 접근법이지만, 진정한 RAG와 같은 결과를 얻기 시작하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# RAG 및 임베딩\n\n간단한 키워드 검색보다 RAG가 제공하는 이점을 이해하려면 임베딩 모델의 목적과 성격을 이해해야 합니다. 이는 그 자체로 심도 있는 주제이지만, RAG를 이해하는 데 중요합니다.\n\n임베딩 모델은 우리의 원래 LLM과 유사하지만, 새로운 콘텐츠를 생성하는 대신 입력을 벡터(숫자 목록)로 축소합니다. 임베딩 모델은 매우 큰 숫자 목록입니다. 임베딩 모델이 생성하는 벡터는 일반적으로 768 또는 1536 숫자(차원)지만, 다른 크기의 벡터도 존재합니다.\n\n임베딩 모델에 의해 생성된 벡터는 단순히 임의의 숫자 세트가 아니라 모델에 따라 입력 데이터의 의미를 요약한 것입니다. 이 벡터는 다른 모델에게는 의미가 없지만 \"유사한\" 텍스트는 같은 모델에서 유사한 벡터를 생성할 것입니다. \"유사하다\"는 단순히 \"동일한 키워드를 가지고 있다\" 이상을 의미합니다. 임베딩 모델은 구조화되지 않은 데이터로부터 보다 심층적인 의미를 추출하기 위해 특별히 훈련되었습니다. 예를 들어 \"남자 말이 날지 않는다\"와 \"날개가 있는 사나이가 말 타고 있다\"는 비슷한 단어를 가지고 있지만 같은 모델에서는 서로 멀리 떨어진 벡터를 생성할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n벡터의 멋진 점은 그들에게 수학 연산을 수행할 수 있다는 것입니다. 빠른 수학이죠. 수백만 개의 벡터를 검색하여 비교적 짧은 시간 안에 유사한 벡터를 찾을 수 있습니다. (여기에 사용된 일부 알고리즘이 있습니다.)\n\n이제 우리의 RAG 파이프라인 조각들이 모두 마련되었으니, 단계별로 진행해봅시다.\n\n첫 네 단계는 한 번에 수행되거나 소스 데이터가 변경될 때 업데이트됩니다. 다섯 번부터 여덟 번까지의 단계는 각 추론 요청마다 수행됩니다:\n\n- 모든 가능성 있는 데이터를 수집합니다. - 이토록 많은 데이터가 있어서 우리의 프롬프트의 컨텍스트 창에 쏙 들어가기 불가능합니다.\n- 이 데이터를 더 작은 조각으로 나눕니다 (나중에 자세히 설명하겠습니다).\n- 그런 다음 각 조각을 임베딩 모델을 통해 실행하여 조각의 의미를 포함하는 벡터를 생성합니다.\n- 해당 벡터를 벡터 데이터베이스에 저장합니다.\n- 각 추론 요청으로: 프롬프트를 받으면, 그 프롬프트를 조각낸 소스 데이터와 동일한 임베딩 모델을 통해 실행하여 다른 벡터 (프롬프트 벡터 또는 쿼리 벡터라고 함)를 생성합니다.\n- 우리의 벡터 데이터베이스에서 우리의 프롬프트 벡터와 유사한 벡터를 검색합니다. 반환된 벡터들은 생 데이터를 키워드로 검색했다면 얻었을 것보다 더 나은 매치일 것입니다.\n- 식별된 관련 벡터를 (선택적으로) 재정렬하고, 그런 다음 상위 벡터 각각의 생 데이터를 반환합니다.\n- 원시 데이터는 초기 프롬프트와 결합되어 LLM으로 보내집니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_2.png)\n\n여기, 우리 LLM은 이제 새로운 독점 데이터를 모두 훈련한 것처럼 작동하며, 기본 모델 훈련을 비용 문제로 수행할 필요가 없습니다.\n\n이론상으로는 이렇게 작동해야 합니다. 그러나 실제로는 이 너무 단순한 파이프라인은 여러분의 제품 요구를 만족시키지 못할 가능성이 높으며, 우수한 품질의 제품에 도달하려면 RAG 파이프라인을 준비하려면 특정 애플리케이션의 요구를 충족시키기 위해 다양한 부분을 적응, 개선, 교체 또는 확장해야 할 것입니다.\n\n# RAG 디자인과 품질\n\n\n<div class=\"content-ad\"></div>\n\n위에서는 RAG를 소개하지만, RAG는 실제로 매우 복잡할 수 있으며, 이러한 실제 세계의 복잡성은 응용 프로그램의 품질에 영향을 미칠 수 있습니다. RAG 파이프라인 내에서 사용 가능한 일부 구현 도전, 품질 위험 및 대안을 이해하기 위해 각 단계를 살펴보겠습니다.\n\n## #1—관련 데이터 수집, 적재 및 풍부화\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_3.png)\n\n시작부터 모든 \"가능성 있는 데이터\"를 찾아야 합니다.\n\n<div class=\"content-ad\"></div>\n\n위의 RAG 다이어그램에서 1번 아이콘은 여러 소스에서 데이터를 수집하고, 정리하고, 변환하며, 익명화하고, 토큰화하는 데이터 파이프라인(또는 파이프라인 세트!)일 가능성이 높습니다.\n\n일부 파이프라인은 특히 텍스트 이외의 형식을 가진 원시 데이터의 경우 매우 복잡해질 수 있습니다. 예를 들어, 일부 파이프라인은 대량의 스캔된 물리 문서를 처리하기 위해 OCR 기술을 널리 활용합니다.\n\n데이터 파이프라인의 복잡성은 데이터 파이프라인 테스트의 모든 과제가 따라옵니다.\n\n가장 잘 구현된 RAG 파이프라인도 소스 데이터가 심지어 벡터 DB로 전달되지 않는다면 완전히 실패할 수 있으며, 이 데이터의 다양성, 속도 및 양에 따라 RAG의 이 단계는 복합적이며 많은 응용프로그램 품질 문제의 원인이 될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n일반적인 데이터 파이프라인 활동에 추가로, RAG는 데이터 풍부화를 통해 혜택을 얻을 수 있습니다. 종종, 다른 시스템(또는 사람들)은 소스 데이터에 대한 맥락을 알고 있어서 그 의미를 평가하는 데 엄청난 도움이 될 수 있습니다. 예를 들어, 고객 데이터베이스에는 다른 시스템에서 제공하는 태그나 주석과 같은 관련 정보를 추가하여 데이터를 풍부화할 수 있습니다. 종종, 다른 생성 모델이나 자연어 처리(NLP)가 더 깨끗하거나 요약된 메타데이터를 생성하는 데 사용됩니다. 모두 \"임베딩 생성\" 전의 \"전처리\"로 생각해보세요. 그리고 제대로 수행된다면, 검색 품질을 크게 향상시킬 수 있습니다.\n\n당신이 RAG 검색 시스템의 품질을 평가하고 있다면, 데이터가 실제로 어떻게 소스되고 흡수되는지 이해하는 데 시간을 투자하는 것이 가치가 있습니다. 훌륭한 AI 부분에 도착하기 전에 RAG 파이프라인에 도달하기 전에 데이터가 어떻게 가져오고 처리되는지 알아두세요.\n\n## #2—Chunking\n\n![2024-06-22-RAGforQualityEngineers_4](/assets/img/2024-06-22-RAGforQualityEngineers_4.png)\n\n<div class=\"content-ad\"></div>\n\n데이터가 수신되고 임베딩 모델을 실행하기 전에는 데이터를 diskrete pieces로 나눠야 합니다. 그렇다면 데이터를 어떻게 분할할지 어떻게 결정하나요? 이를 chunking strategy라고 합니다.\n\n최적의 크기는 얼마나 크거나 작아야 할까요? 청크들은 서로 중첩되어야 할까요? 페이지, 단락 또는 일정한 길이로만 나누는 것 이외에 더 스마트한 분할 방법이 있을까요? 비표준 형식의 데이터(code, JSON 등)는 어떻게 chunk해야 할까요?\n\n이러한 질문들은 chunking strategy가 답하려고 노력하는 것이며 완벽한 해결책은 없습니다. 서로 다른 전략은 서로 다른 타협점을 갖습니다. 일부는 간단하고 빠르게 구현할 수 있으며, 보통 결과를 제공합니다. 다른 전략은 더 복잡하고 관련이 깊습니다. 더 나은 히트율과 LLM 응답 품질을 제공할 수 있습니다. 데이터를 너무 거칠게 나누면 의미 없는 데이터로 context window를 채우거나 다른 관련 청크를 밀어내거나 의미 있는 일치를 얻을 수 없을 정도로 일반적인 임베딩을 생성할 수 있습니다. 너무 세밀하게 나누면 관련 데이터를 잘라낼 수 있습니다.\n\n이 문서에서는 다섯 가지 chunking 범주를 탐구합니다: 고정 크기, 재귀, 문서 기반, 의미 기반, 그리고 AI를 사용한 Agentic(체킹에 인공 지능 사용, 멋지죠!)입니다.\n\n<div class=\"content-ad\"></div>\n\n많은 기타 접근 방식이 있습니다. 예를 들어, 작은 것에서 큰 것으로 검색을 최적화하려면 작은 청크를 사용하지만 각 청크는 큰 부모 청크에 연결되어 있어서 삽입될 컨텍스트 모델에 검색됩니다. 콘텍스트 인식 청킹은 문서의 성격에 대해 기존 지식을 활용하여 문서를 논리적 청크로 적절하게 분할합니다.\n\n위 목록은 아마 완전하지 않을 수 있지만, RAG 구현자가 사용할 수 있는 다양한 옵션과 애플리케이션 전체의 품질에 적합하고 조정된 청킹 전략의 중요성을 보여줍니다. Pinecone 블로그에서 이러한 전략 중 많은 내용을 자세히 다루고 있습니다.\n\n## #3—임베딩 모델 선택과 구성\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_5.png)\n\n<div class=\"content-ad\"></div>\n\n임베딩을 생성하는 데 사용할 수 있는 여러 모델이 있습니다. 서로 다른 모델은 다양한 상황에서 더 나은 또는 나쁘게 수행할 수 있습니다. 일부 모델은 일반 사용을 위해 사전 훈련되어 있고 일부는 특정 도메인(예: 의학 기록)에 대해 세밀하게 조정되어 있습니다. 또한 응용 프로그램에서 처리하는 특정 데이터에 대해 자체 임베딩 모델을 세밀하게 조정할 수도 있습니다.\n\n또한 많은 모델이 다른 크기로 제공되며(임베딩 생성 비용 및 시간에 영향을 미침), 다른 입력 길이(처리 가능한 최대 청크 크기) 및 다른 출력 벡터 차원(높은 차원 = 더 정확하지만 더 많은 공간 요구와 느린 속도)으로 제공됩니다.\n\n일부 임베딩 모델은 API를 통해만 액세스할 수 있습니다(예: OpenAI 임베딩 엔드포인트), 다른 모델은 완전한 오픈 소스로 제공되어 로컬에서 다운로드하고 실행하거나 클라우드 공급업체에 호스팅할 수 있습니다.\n\n응용 프로그램 내에서 다른 데이터 경로에 대해 다른 임베딩 모델을 사용하는 것도 가능합니다.\n\n<div class=\"content-ad\"></div>\n\n일반적으로 좋은 임베딩 모델은 RAG 응용 프로그램에 충분할 수 있지만, 일부는 특정 임베딩 모델이나 사용자 지정된 모델을 사용하여 혜택을 얻을 수 있습니다.\n\n임베딩 전략에 대한 설계 고려 사항과 해당 선택의 품질 특성을 알면 응용 프로그램의 평가 요구 사항과 접근 방식에 대한 통찰력을 제공할 것입니다. 임베딩 모델 선택을 평가하는 더 깊은 논의는 여기에서 확인할 수 있습니다.\n\n## #5—쿼리 처리 및 임베딩\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_6.png)\n\n<div class=\"content-ad\"></div>\n\n수신한 쿼리에 임베딩 모델을 정확히 실행해야 한다는 규칙은 없습니다. 실제로 이 쿼리를 최적화하여 애플리케이션의 전반적인 품질을 향상시킬 수 있는 다양한 방법이 있습니다. 이는 특히 쿼리가 사람 사용자로부터 직접 제출되었으며 모호하고 애매한 쿼리일 경우에 더욱 참된 것입니다.\n\n애플리케이션의 특성 또는 의도에 대한 추가 지식을 통해 LLM 또는 전통적인 논리를 사용하여 쿼리를 축소하거나 다시 작성하는 것이 가능할 수 있습니다. 다시 말해, 의도된 것이 아닌 실제로 묻는 것이었던 쿼리를 재작성하는 방식으로 쿼리를 재구성할 수 있습니다.\n\n쿼리 처리의 고급 형태인 HyDE도 있습니다. 여기서는 가상 문서를 작성하여 유사한 문서(답변에서 답변)를 벡터 검색하고 임베딩 및 쿼리 검색(질문에서 답변)을 하는 것대신 사용할 수 있습니다.\n\n또 다른 옵션은 쿼리를 여러 관련된 쿼리로 분할하고 각각을 병렬로 실행한 다음 결과를 결합하는 것입니다. 이는 처리 비용이 듬성들지만 검색 품질을 향상시킬 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n특정 사용 사례에 따라 사용자 지정 쿼리 처리가 필요할 수 있으며 응용 프로그램의 품질과 동작에 큰 영향을 미칠 수 있습니다.\n\n## #4, #6—Vector DB 및 Vector Search\n\n![image](/assets/img/2024-06-22-RAGforQualityEngineers_7.png)\n\n벡터 검색은 빠르지만, 쿼리와 유사한 임베딩을 찾기 위해 벡터 DB를 검색하는 데에는 시간 (그리고 돈) 비용이 소요될 수 있습니다. 이 비용을 최소화하는 한 가지 방법은 의미 캐싱입니다. 의미 캐싱에서는 임베딩이 처음 검색된 후 응답이 캐시되어, 향후 유사한 검색이 캐시로부터 직접 데이터를 반환하게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n물론, 캐싱은 복잡성을 증가시킵니다 (그리고 컴퓨터 과학에서의 두 번째 어려운 문제 중 하나입니다—다른 하나의 이름을 기억하지 못하겠군요). 캐싱은 성능을 향상시킬 수 있지만, 오래된 캐시는 변동성 있는 소스 데이터 환경에서 특히 응답 품질을 해치는 요인이 될 수 있습니다.\n\n## #7—재랭킹\n\n![이미지](/assets/img/2024-06-22-RAGforQualityEngineers_8.png)\n\n위에서 설명한 내용에서 우리는 단순히 우리의 벡터 검색으로 반환된 모든 관련 데이터를 컨텍스트 창에 채울 수 있다고 가정했습니다. 이것은 명백히 간소화된 내용이며, 반환된 모든 벡터 중 어느 것이 컨텍스트 창에 포함될 것인지를 결정하기 위한 과정이 있어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n검색 결과를 콘텍스트 창 안에 맞출 수 있을 때에도, 많은 연구에서 콘텍스트 Stuffing(콘텍스트 창 채우기)가 LLM 회상을 부정적으로 영향을 줄 수 있다는 것을 지적합니다. 이는 중간에서 사라지는 문제를 도입하여 응답 품질(회상은 LLM이 콘텍스트 창에 있는 정보를 사용하는 능력입니다)에 영향을 줄 수 있습니다.\n\n해결책은 초기 벡터 검색 후에 추가 단계로 재랭킹을 추가하는 것입니다.\n\n재랭킹의 TLDR(요약): 임베딩 모델은 속도에 최적화되어 있으며 많은 문서에 대해 실행되어야 하므로 빠릅니다. 재랭킹 모델(또는 교차 인코더)이라 불리는 다른 모델은 느리지만 정확도에 최적화되어 있습니다. 그래서 빠르고 부정확한 임베딩 모델을 사용하여 임베딩을 생성한 다음, 작은 집합에서 최고 품질의 문서를 찾기 위해 느리고 정확한 모델을 사용합니다. 느린 정확한 검색에서 가장 일치하는 결과는 콘텍스트 창에서 우선순위를 갖습니다.\n\n다시 말하지만, 이보다 더 많은 내용이 있지만, 재랭킹의 본질은 바로 이것입니다. Pinecone 블로그에서 더 자세한 설명을 찾을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다시 순위를 매기면 RAG에서 반환된 데이터의 관련성을 크게 향상시킬 수 있습니다. 컨텍스트 창에서 더 관련성이 높은(또는 무관련성이 적은) 데이터는 응답 품질을 향상시킬 것입니다. 그러나 복잡성과 지연이 증가하지만, 품질의 트레이드오프는 많은 RAG 응용 프로그램에서 가치 있는 요소일 수 있습니다.\n\n## 큰 컨텍스트 창 vs. RAG\n\n우리는 마침내 LLM을 호출하는 지점에 도달했지만, 프롬프트 엔지니어링에 대해 이야기하기 전에 RAG와 큰 컨텍스트 창 간의 관계에 대해 언급할 시간을 가져야 합니다.\n\nLLM 기술은 빠르게 발전하고 있으며 개선의 한 가지 측면은 컨텍스트 창의 크기입니다. 한 가지 대표적인 예는 2024년 2월에 출시된 Gemini 1.5 Pro이며, 128K 컨텍스트 창을 제공하며(공개적으로 출시되지 않음) 최대 백만(!!!) 토큰까지 확장할 수 있는 옵션이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n일부 사람들은 100만 토큰 컨텍스트 창이 RAG 파이프라인을 사용할 때 사용되지 않을 것이라고 예상했습니다. 그러나 실제로는 그렇지 않습니다. 이 블로그에서는 RAG가 왜 유용하며 (심지어 필수적이기도 한) 거대한 컨텍스트 창을 사용할 때도 필요한 이유에 대해 설명합니다. (스포일러: 비용, 지연 시간 및 회수 품질)\n\n대규모 컨텍스트 모델은 유용하며, LLMs가 많은 사실들 간에 종합적인 결론을 요구하는 쿼리에 응답하는 데 도움이 될 수 있습니다 (이러한 사실들이 RAG를 통해 선별되어 있는지 여부는 상관 없음).\n\n큰 컨텍스트 창과 RAG 간의 관계는 계속 발전할 것이며, RAG를 구현하고 테스트하는 사람들은 응용 프로그램 품질에 미치는 이러한 트레이드오프와 그들의 영향을 이해해야 합니다.\n\n## #8—프롬프트 생성\n\n<div class=\"content-ad\"></div>\n\n![2024-06-22-RAGforQualityEngineers_9](/assets/img/2024-06-22-RAGforQualityEngineers_9.png)\n\n벡터DB에서 관련 데이터를 많이 받아 재설정하고, LLM(Large Language Model)의 문맥 창 안에 맞는 적절한 데이터 세트로 마무리했습니다. 그럼 이제 어떻게 해야 할까요? 받은 데이터를 초기 질문 뒤에 밀어 넣고 끝내면 될까요?\n\nLLM을 다뤄본 사람이라면 알 수 있듯이, 그것만큼 간단한 일이 아닙니다. LLM은 강력할 수 있지만, 변덕스럽고 짜증을 유발할 수도 있습니다. 당신의 프롬프트에 작은 세부 사항이 응담 품질에 상당한 영향을 미칠 수 있다는 것이 밝혀졌습니다. 프롬프트의 단어 선택, 데이터 순서, 사용하는 어조, \"시간을 들이다\"와 같은 제안, 심리적 언어 사용까지 모두 LLM 응답 품질에 영향을 미칠 수 있습니다. 프롬프트를 자동으로 생성하는 최적 프롬프트 생성 전략이 있습니다. ...맞아, 프롬프트 생성에 특별히 훈련된 다른 모델을 사용하는 것입니다. 이것은 신속히 진화하는 프롬프트 엔지니어링 분야의 일부입니다.\n\n최상의 품질의 응답을 생성할 정확한 프롬프트 템플릿은 보통 모델 및 응용 프로그램에 따라 다르며 종종 실험과 시행착오가 필요할 수 있습니다. RAG의 이 보이지 않는 작은 세부 사항이 가지는 품질 영향을 감안할 때, 적용된 특정 프롬프트 엔지니어링은 시스템의 다른 부분과 마찬가지로 면밀히 평가되고 심사되어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n# RAG 시스템의 측정 및 평가\n\n우리는 RAG 파이프라인의 주요 구성 요소들을 살펴보고 (간략하게) 이들이 애플리케이션 품질에 미치는 영향에 대해 이야기해 보았습니다. 이것은 소개였지만, 이러한 유형의 애플리케이션의 내부 작업 및 품질 도전에 대한 통찰력을 제공해야 합니다. RAG에 대해 더 깊이 파고드는 많은 훌륭한 기사, 블로그, 논문이 있습니다. 시작할 때 하나만 선택한다면, \"Retrieval-Augmented Generation for Large Language Models—A Survey\"를 읽어보세요.\n\n주요 교훈: RAG를 구현할 때 선택할 수 있는 옵션과 선택지가 많으며, 각각에는 품질에 대한 대가와 영향이 있습니다. 이러한 선택들 중 일부는 직접적으로 평가될 수 있고, 일부는 전반적인 검색이나 응답 품질에 영향을 미칩니다. 이러한 선택 각각과 이들이 당신의 RAG 시스템에 어떤 영향을 미칠 수 있는지 이해하는 것이 전반적인 애플리케이션의 제품 품질을 달성하는 데 중요합니다.\n\n당연한 다음 질문은 다음과 같습니다: 좋아, 그런데 RAG를 어떻게 평가할까요? 개방형 자유형식 응답의 품질을 어떻게 측정할까요? 어떤 지표를 사용하여 실제로 측정할 수 있을까요? 이러한 평가를 자동화할 수 있을까요, 그리고 어느 수준에서 할 수 있을까요? LLM은 본질적으로 비결정론적이고 그들이 소비하는 데이터도 본질적으로 불안정할 때 품질을 어떻게 보장할까요?\n\n<div class=\"content-ad\"></div>\n\n이런 건들로 이루어진 큰 질문들이에요. 우리는 ARC나 HellaSwag 같은 프레임워크를 이용한 모델 평가, LLM-as-a-judge와 같은 접근 방식, 바늘 찾기 테스트와 같은 테스트, 어려움, 신뢰성, 그리고 관련성과 같은 측정 항목, Ragas와 LlamaIndex와 같은 도구 등의 주제를 다룰 거에요.\n\n하지만, 이 모든 재미로움은 다음 블로그를 기다려야 해요.\n\n본 글에 대한 기술적 피드백으로 Etienne Ohl와 Jack Bennetto에게 특별히 감사드려요.","ogImage":{"url":"/assets/img/2024-06-22-RAGforQualityEngineers_0.png"},"coverImage":"/assets/img/2024-06-22-RAGforQualityEngineers_0.png","tag":["Tech"],"readingTime":13},{"title":"데이터 메시에 대한 도전 과제 및 해결책  3부","description":"","date":"2024-06-22 17:11","slug":"2024-06-22-ChallengesandSolutionsinDataMeshPart3","content":"\n\n'연합된 계산 기반 거버넌스'는 안전하고 신뢰할 수 있으며 상호 운용 가능한 데이터 메시를 보장합니다. 상호 운용성으로부터 추가 가치는 종종 \"전체가 부분의 합보다 더 크다\"는 구절로 요약됩니다. HTTP와 같은 표준 프로토콜, 효율적인 데이터 전송 메커니즘, 그리고 구성 요소의 포괄적인 버전 관리와 같은 기술적 측면을 설정하는 것이 상호 운용성을 활성화하는 데 중요하다면, 여기서 우리의 초점은 전체적인 데이터 일관성과 호환성 유지에 있을 것입니다.\n\n상호 운용성을 참으로 지원하려면, 우리는 모든 데이터 제품에서 내용을 포괄하는 일관된 혹은 일관성 있는 모델을 보장해야 합니다. 이 모델은 새로운 데이터 제품이 추가됨에 따라 동적으로 업데이트되어야 합니다. 나의 삼부작 시리즈의 마지막 글에서는 본래 체계적이며 최신으로 유지되는 종합적인 관점을 유지하는 실용적인 방법을 보여드리도록 하겠습니다.\n\n거버넌스는 종종 발전을 억제할 수 있는 엄격한 규칙으로 여겨집니다. 즉시적인 혜택 없이 부가 작업을 추가하는 부담으로 여겨집니다. 거버넌스 프로세스를 자동화함으로써(즉 '계산 기반'으로 만들면) 데이터 메시 내에서 그 적용을 효율적으로 만들 수 있지만, 프로젝트나 제품 비용은 그대로 남아 있을지도 모릅니다.\n\n그렇다면, 우리는 어떻게 거버넌스를 변경하여 분산된 팀에 가치를 제공하고 동시에 참여를 촉진할 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\n우리는 지배 규칙을 유용한 프레임워크로 변환하고 커뮤니티 기반의 아이디어와 시장 기반의 메커니즘을 활용함으로써 이를 달성할 수 있습니다. 이러한 전략은 참여와 가이드라인 준수를 효과적으로 장려할 수 있습니다.\n\n## 가이드로 연방화\n\n세계적인 인터넷과 그 개발에서 얻을 수 있는 교훈을 고려해봅시다. 웹은 모든 정보를 일관되고 구조적으로 조직화하기 위한 중앙 기관이나 카탈로그 없이 운영됩니다.\n\n예전에야후는 웹의 정보를 구조화하는 데 중앙에서 분류 체계를 작성하고 정리하는 것이 옳은 방식이라고 믿었습니다. 그러나 구글은 분산된 자동화 검색 기능으로 월드 와이드 웹의 정보를 보다 획기적으로 처리하여 더 큰 성공을 거두었습니다. 구글의 방식은 작업을 분배하고 자동화를 통해 아래에서 위로 데이터를 합치는 것을 지원했기 때문에 더욱 효과적이었습니다.\n\n<div class=\"content-ad\"></div>\n\n웹의 비구조화된 다양한 정보는 대기업의 명확성 부족을 반영하기도 하지만, 우리는 여전히 조직 내에서 선제적으로 행동할 기회가 있습니다. 그러나 Google 검색 엔진과 같은 하위 검색 기능은 마술처럼 누락된 메타데이터를 생성하거나 일관성 없는 데이터를 일관성 있게 만들어 줄 수 없습니다. 따라서 데이터가 일관되게 정의되고 비즈니스 컨텍스트를 위한 메타데이터가 데이터 제품에 통합되도록 선제적으로 행동하고 보장하는 것이 중요합니다.\n\n비즈니스 도메인에서 정보 다양성을 유도하여 통합된 하나로 집계할 수 있도록 해야 합니다. 데이터 메쉬는 다중 비즈니스 도메인 경계를 가로지르는 폴리세미에 대해 말하지만 중앙에서 모델링해야 하는 점이 모호합니다. 이를 구체화하고 중앙 집중화와 분산화 사이의 적절한 균형을 이룰 수 있는 모델링이 어떻게 구현될 수 있는지 살펴봅시다.\n\n프로세스 월드(운영 시스템)에서는 두 가지 접근 방식이 나왔습니다. 이전에 생성된 정보의 고립된 영역(엔터프라이즈 응용 프로그램 통합 또는 EAI)을 다시 통합하거나 처음부터 이러한 영역이 만들어지는 것을 방지하는 것(도메인 주도 설계 또는 DDD)입니다. DDD는 \"Bounded Contexts\"의 전체 인터페이스를 모델링하기 위해 \"Context Maps\"를 참조합니다. EAI는 응용 프로그램 간 마찰없는 데이터 교환을 가능하게 하는 중심 요소인 \"Canonical Data Model\"을 참조합니다. 분석적인 세계에서 Ralph Kimball은 공유 비즈니스 차원을 가진 데이터 웨어하우스 버스 아키텍처를 소개하고, Dan Linstedt는 비즈니스를 세밀하게 대표하고 비즈니스 키를 사용하는 교차 도메인 허브의 필요성을 강조하는 데이터 보트 모델을 옹호했습니다. 모든 접근 방식은 정보 고립 영역의 급증을 방지하기 위해 비즈니스 개념의 핵심을 포착하는 기업 데이터 모델의 필요성을 강조합니다.\n\n이러한 포괄적 모델 개발의 불가능성에 대한 이야기가 많이 나오지만, 많은 프로젝트에서 실용적인 접근 방식이 성공을 거두었습니다. 중요한 것은 도메인별 데이터 모델을 통합할 온톨로지(프레임워크)를 수립하는 것입니다. DDD 용어로 말하면, \"Context Maps\"를 통해 Bounded Contexts의 명확하게 인식된 중첩 목록이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n다시 말해, 핵심 비즈니스 컨셉에 대한 지표를 제공하고 개요를 제공하는 고수준 비즈니스 모델입니다. 개별 도메인 모델의 상세한 디자인은 온톨로지의 범위를 벗어납니다. 온톨로지는 다수의 도메인 모델을 통합하고 비즈니스 도메인 전반에 걸쳐 유효한 공통점과 추상화를 설명합니다. 다의어를 정의하는 것뿐만 아니라 공통 비즈니스 용어, 객체 및 관계를 식별하는 데 중점을 둡니다.\n\n여러 테스트된 모델 패턴과 산업 모델이 솔리드한 기반이 됩니다. 이를 통해 바퀴를 재창조하는 것을 피할 수 있습니다. 그러나 최종 해결책으로서가 아닌 시작점으로 활용하세요. 공통 업계 모델이 아닌 고유한 비즈니스 요구사항을 대표하는 것이 중요합니다.\n\n때로는 기업 모델이 이미 존재하지만 프로젝트에서 활발하게 활용되지 않을 수 있습니다. 이러한 자료를 활용하여 특정 컨텍스트에 맞추도록 '활성화'를 시도하세요. 반면에, 존 지일즈는 \"냉장고 속 코끼리\"라는 책에서 \"어떻게 '충분한' 기업 데이터 모델을 직접 만드는가\"란 특정한 챕터에서 처음부터 새로운 온톨로지를 만드는 실용적인 접근 방법을 소개합니다.\n\n기존 모델을 활용하고 새로운 모델을 만드는 두 가지 접근법을 균형 있게 조화시키는 것은 많은 프로젝트에서 긍정적인 결과를 이끌어냈습니다.\n\n<div class=\"content-ad\"></div>\n\n이 기사에서 발견된 풍부한 조언들을 완전히 다 다루지는 못하지만, 두 가지 주요 아이디어를 강조하겠습니다:\n\n- 온톨로지에서 비즈니스 참여 활성화\n비즈니스 정보를 성공적으로 통합하기 위해서는, 여러 영역을 중심으로 조정할 수 있다는 오해를 피해야 합니다. 그러나 중앙 감독 없이는 조정될 수 없습니다. 그리고 중앙화된 IT 데이터 모델러 팀은 종종 각각의 비즈니스 도전에 대한 구체적인 지식이 부족할 수 있습니다. 따라서, 비즈니스 전문가들을 참여시키는 것이 중요합니다.\n연합 접근 방식을 유지하기 위해서, 모델링 팀은 IT 데이터 모델링 전문가들이 중재하는 모든 비즈니스 영역의 전문가들을 포함해야 합니다. 그 목표는 비즈니스에 대한 통합적이고 고수준의 이해를 달성하는 것입니다. 기억하세요, 데이터 모델링은 근본적으로 비즈니스 모델링이라는 것을.\n\n- 고수준 유지 및 세부 사항 연방화\nOntology는 고수준의 공통점에 관한 것입니다. 핵심 개체 및 관계를 작업하기 위해 추상 모델링 패턴을 사용하세요, 예를 들어 다음과 같은 것들.\n\n| 주체 & 역할: 비즈니스 역할을 수행하는 개인 또는 조직 (예: 고객, 에이전트, 공급업체).\n| 위치: 지리적 위치, 건물 또는 지역.\n| 이벤트: 사고와 같은 중요한 이벤트 또는 '애플리케이션이 생성됨'과 같은 루틴 이벤트.\n| 문서: 물리적 또는 전자 문서, 계약서 또는 신분증의 스캔 이미지, 또는 모든 유형의 데이터 파일을 포함합니다.\n| 합의: 종종 형식적으로 문서화된 당사자 간의 계약.\n| 계정: 레코드의 일반적인 표현.\n| 작업: 계획된 또는 실제 작업 항목.\n| 자원 / 자산: 회사 건물, 컴퓨터 또는 차량과 같은 자산.\n| 제품: 고객에게 제공되는 상품 및 서비스.\n\n- 이러한 모델링 패턴은 다음과 같은 관계를 가지고 있습니다.\n제품 -` 주체가 활용하는 -` 합의를 체결한 -` 계정을 용이하게 하는 합의 -` 위치에 유지된 -` 자원의 장소이고, 그렇게하여야 된다.\n\n<div class=\"content-ad\"></div>\n\n희망을 통해 일반적인 개념을 제공했기를 바랍니다 — 참조 도서에서 더 자세하고 유용한 조언을 찾을 수 있을 거에요. 일반적인 패턴은 비즈니스 개념을 설명하는 상향식 프레임워크를 제공하여 다양한 영역간의 이해를 통합하는 데 도움이 됩니다. 자세한 비즈니스 도메인 모델은 상향식으로 개발된 상위 온톨로지에 명시적으로 연결되는 분산 도메인 팀에 의해 아래에서 위로 보완되어야 합니다. 이는 다양한 관점을 조화시키기 위한 강력한 토론 없이는 성공할 수 없는 창의적인 과정입니다.\n\n도메인 주도 설계는 Shared Kernel, Customer/Supplier Dev Teams, Conformist, Anticorruption Layer, Separate Ways, Open Host Service, 또는 Published Language과 같은 이 과정을 중재하는 실용적인 패턴을 제공합니다. 많은 용어와 설명이 주로 기능적 통합으로부터 비롯되었다 하더라도, 그 원칙들은 데이터 관리와 모델링에도 적용됩니다.\n\n전반적으로, 모든 데이터 생산자는 상세한 비즈니스 도메인 데이터 모델을 온톨로지에 매핑해야 합니다. 이 정보는 데이터 메시에 발행된 데이터 제품에 캡슐화된 메타데이터로 제공됩니다. 모든 가능한 데이터 제품의 메타데이터에서 파생된 데이터 메시는 언제든지 소비자를 위한 최신의 기업용 데이터 모델을 제공할 수 있습니다.\n\n# 참여를 활발하게 하는 동기부여\n\n<div class=\"content-ad\"></div>\n\n데이터 메시 내에서 효율적인 거버넌스는 참여자들이 데이터 기여를 온톨로지에 맞추도록 장려하며, 기업 전체의 이익과 개별 또는 부서별 목표를 균형 있게 조율합니다.\n\n기업 당사자들이 적극 참여하도록 어떻게 동기부여할 수 있을까요?\n\n## 커뮤니티 및 오픈 소스 원칙\n\n오픈 소스 운동에서 배울 점이 있습니다. 오픈 소스 운동의 철학에 부합하여, 데이터 모델의 특정 부분의 소유권이 아닌 일관된 전체적인 협력에 대한 작업이 중요합니다. 협업은 정보를 공유하는 것이며, 따라서 데이터 모델에 대한 모든 변경 사항은 시간이 지남에 따라 공개되고 문서화되어야 합니다. 거버넌스 프로세스는 이러한 공유를 가장 간단하고 보상적으로 만들어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n오픈 소스 개발에서는 Git과 같은 코드 저장소를 활용하여 배포 및 업데이트 작업을 투명하게 처리하고 코드의 전체 내용을 분산하여 업데이트할 수 있습니다. 개발자들은 제품 상태에 대한 포괄적인 개요를 언제든 확인할 수 있습니다. 풀 리퀘스트를 사용하면 코드 변경에 대한 다양한 관점을 토론하고 중재할 수 있는 가치 있는 플랫폼을 구축할 수 있습니다.\n\n이와 유사하게, 데이터 메시를 활용하여 기업 데이터 모델의 분산 작업에 동일한 기능을 제공할 수 있습니다. 이 시리즈의 제2부에서는 데이터 자체에 메타데이터를 캡슐화하는 방법을 소개하여 전체 온톨로지에 맞게 자립적인 데이터 제품을 만들었습니다. 데이터 모델의 변경 사항은 이후 새로운 데이터 원자로 스트리밍되거나 변환으로 추가될 수 있습니다. 발행된 업데이트는 신속하게 기업 데이터 모델의 최신 상태를 생성하기 위해 소비될 수 있습니다. 데이터 모델에 대한 상충되는 변화나 다른 관점은 조정되고 토론될 수 있습니다. 온톨로지는 전반적 일관성을 보장하며 모든 변경 사항은 원본 데이터 모델로 귀결할 수 있는 투명하고 추적 가능한 형태로 남습니다. 이 접근법은 분산 데이터 모델링 팀 간의 협업을 촉진합니다.\n\n기업 내에서 데이터 커뮤니티를 형성하면 회사의 성공에 대한 공헌도 높일 수 있습니다. 데이터 메시는 가치 있는 데이터를 공유할 수 있도록 모두가 동참할 수 있는 협업 환경으로 작용합니다. 종종, 애플리케이션 소유자들은 생성하는 데이터가 기업의 다른 맥락에서 실질적인 추가 가치를 창출할 수 있다는 사실을 충분히 인식하지 못합니다. 이러한 데이터 제품은 온톨로지와 조화를 이룬 경우, 생산자가 자세한 응용 프로그램을 이해하지 않고도 다른 비즈니스 맥락에서 원활하게 활용할 수 있습니다. 개별 데이터 제품마다 이중 데이터 계약을 작성하는 대신 애플리케이션 소유자는 기업과 단일 가상 계약을 체결합니다. 이를 통해 제품 소유자의 데이터를 발행하고 다양한 비즈니스 영역에서 활용하여 전반적인 가치와 효율성을 향상시킬 수 있습니다.\n\n## 데이터 제품 시장\n\n<div class=\"content-ad\"></div>\n\n알truistic 동기 외에도, 시장 기반 메커니즘은 기업 내 분산 모델링 노력에 참여를 동기부여할 수 있습니다. 이러한 메커니즘은 데이터 제품을 위한 고객을 찾는 데 도움을 주는 것뿐만 아니라 데이터의 명확한 정의와 구조화를 장려합니다.\n\n참여 동기:\n\n- 생산자와 고객에게 부가 가치 제공\n생산자는 기업 내에서 자신의 응용 프로그램의 중요성과 가시성을 높여 혜택을 얻습니다. 반면에 고객은 자신의 데이터 요구를 효율적으로 충족시켜주는 것으로, 생산자와 직접 소통할 수 있는 명확한 개요를 얻을 수 있습니다.\n- 참여를 간소화하고 민주화\n프로세스를 간소화하기 위해 자동화된, 셀프 서비스 데이터 패브릭을 구현합니다. 정확한 정보 매핑에 대한 긴 조정은 데이터 제품을 빠르게 출시하고 싶은 생산자들에게 장벽이 될 수 있습니다. 이 프로세스를 간소화함으로써 참여를 장려하여 지각된 장벽을 줄입니다.\n\n마켓플레이스에 제공되는 데이터 제품의 경우, 시장 메커니즘을 활용하여 데이터 제품의 가치를 결정할 수 있습니다. 본문에서 상세한 가치 평가 방법에 대해 다룰 수 없지만, 사용자 사용량을 추적하고 평가 및 리뷰와 같은 소비자 피드백을 가능하게 함으로써 간단하면서도 효과적인 접근 방법이 있습니다. 이러한 메커니즘들은 참여를 장려할 뿐만 아니라 데이터 제품의 품질을 평가하고 개선하기 위한 가치 있는 통찰을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n기업 데이터 모델을 만드는 것이 어려운 일로 여겨졌지만, 이제는 고품질 데이터 제품을 위한 자체 지속 경쟁으로 변모했습니다.\n\n다음은 데이터 메쉬에서 '연방형 컴퓨터 관리'의 개선 내용을 요약한 것입니다:\n\n- 거버넌스 프로세스를 접근하기 쉽고, 마찰이 적도록 만듭니다.\n- 비즈니스 도메인 팀에 데이터 모델링 활동을 연방화하지만, 프레임워크로서의 안내적 온톨로지를 제공합니다.\n- 데이터 모델에 대한 협업 작업을 허용하고, 변경 사항을 모두 데이터 메쉬를 통해 투명하게 게시합니다.\n- 데이터 메쉬를 통해 데이터 시장을 제품으로서 활용할 수 있게 배치하여 투자 수익률(ROI)을 얻을 수 있도록 합니다.\n\n이로써 데이터 메쉬의 도전과 해결책에 관한 세 번의 시리즈가 마무리됩니다. 종합적으로, Zhamak Dehghani의 정의에 따라 데이터 메쉬를 개선하기 위해 다음과 같은 조언을 제공했습니다:\n\n<div class=\"content-ad\"></div>\n\n부분 1\n\n- 데이터 메시의 운영 및 분석 데이터 평면 사이의 큰 차이를 줄입니다.\n\n부분 2\n\n- '데이터 제품'을 스마트 데이터 구조로 구현하여 전체 데이터 계보와 비즈니스 컨텍스트를 포함하도록 하고, 자체 생성이 가능한 새로운 '슈퍼 객체'로 사용하지 않도록 합니다.\n- 데이터 메시의 하부에 데이터 제품 인프라를 제공하여 느슨하게 결합된 운영 및 분석 시스템 간에 데이터 제품을 원활하게 교환할 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\n파트 3\n\n- '활성화'하면서 연방화된 기업 데이터 모델링을 위한 거버넌스 프로세스를 활성화하고, 쉽게 접근 가능한 서비스를 통해 개방형 협업 활성화.\n- 데이터 메쉬를 시장에서 데이터 제품으로 사용되는 하부 구조로 배치하여 참여자에게 투자 수익을 보장합니다.","ogImage":{"url":"/assets/img/2024-06-22-ChallengesandSolutionsinDataMeshPart3_0.png"},"coverImage":"/assets/img/2024-06-22-ChallengesandSolutionsinDataMeshPart3_0.png","tag":["Tech"],"readingTime":8},{"title":" Docker로 Apache Spark와 Minio를 사용하여 Apache Iceberg 활용하는 방법","description":"","date":"2024-06-22 17:09","slug":"2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker","content":"\n\n<img src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png\" />\n\n이 게시물에서는 Apache Iceberg(데이터 테이블 형식), 분산 처리 엔진인 Apache Spark, 고성능 객체 저장 솔루션인 Minio를 결합하는 방법에 대해 탐구합니다. 주요 초점은 이러한 구성 요소를 Docker 컨테이너 내에 설정하여 통제된 환경에서 격리된 환경을 제공하는 데 있습니다. 이러한 기술을 결합하여 ACID 트랜잭션, 스키마 진화 및 Minio 내에서 효율적인 데이터 파티셔닝을 통한 효율적인 데이터 관리 기능을 확보할 수 있습니다.\n\n# 아키텍처 개요\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*gM-qHwR03S6IEh32mgJpjA.gif\" />  \n\n<div class=\"content-ad\"></div>\n\n# 컴포넌트 개요 이론적 개요📖\n\n실제 구현에 들어가기 전에, 사용할 기술들인 Apache Iceberg, Apache Spark, 그리고 Minio에 대한 간단한 이론적 개요를 살펴보겠습니다:\n\n![이미지](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_1.png)\n\n- Apache Iceberg: 차세대 데이터 테이블 형식\n\n<div class=\"content-ad\"></div>\n\n- 데이터 레이크 및 데이터 웨어하우스용으로 설계됨: Iceberg는 대규모 데이터의 복잡성을 처리하도록 특별히 구축되었습니다. 데이터 레이크와 데이터 웨어하우스 내에서 강력한 데이터 관리 기능을 제공하면서 분석 워크로드의 성능을 최적화합니다.\n- ACID 트랜잭션: Iceberg는 ACID(원자성, 일관성, 고립성, 지속성) 트랜잭션을 지원하여 동시 쓰기 시나리오에서도 데이터 무결성과 일관성을 보장합니다. 이는 동일한 데이터에 여러 애플리케이션이나 프로세스가 동시에 쓰기를 하는 경우에 데이터 유효성을 유지하고 데이터 손상을 방지하는 데 중요합니다.\n- 스키마 진화: Iceberg는 기존 데이터 형식과 달리 데이터 테이블의 스키마를 손실 없이 진화시킬 수 있습니다. 이를 통해 분석 요구 사항이나 데이터 소스가 시간이 지남에 따라 변경될 때 데이터 구조를 조정할 수 있습니다. 기존 데이터를 다시 작성하지 않고도 열을 추가, 제거 또는 수정할 수 있습니다.\n- 타임 트래블 쿼리: Iceberg는 타임 트래블 쿼리를 지원하여 언제든지 과거 데이터 스냅샷에 액세스할 수 있습니다. 이는 변경 사항 감사, 분석 파이프라인 디버깅 및 역사적 분석에 매우 유용합니다. Iceberg는 데이터 버전을 추적하여 필요한 경우 특정 버전의 테이블을 검색할 수 있습니다.\n- 효율적인 파티셔닝: Iceberg는 특정 열을 기준으로 데이터 테이블을 파티션하여 읽기 성능과 데이터 관리를 최적화합니다. 해당 파티션 값에 따라 데이터 파일을 지능적으로 저장하므로 Spark가 특정 쿼리에 대한 관련 데이터만 효율적으로 스캔할 수 있습니다. 이는 쿼리 속도를 크게 향상시킵니다.\n- 데이터 조직 및 압축: Iceberg는 자동으로 Parquet 또는 ORC와 같은 효율적인 파일 형식으로 데이터를 구성하여 효율적인 데이터 압축과 열 액세스를 용이하게 합니다. 또한 데이터 압축을 수행하여 저장 공간을 최소화하고 시간이 지남에 따라 읽기 성능을 향상시킵니다.\n\n2. Apache Spark: 통합 분석 엔진\n\n![이미지](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_2.png)\n\n- 대규모 데이터 처리: Apache Spark는 대규모 데이터셋을 클러스터로 효과적으로 처리하는 강력한 오픈 소스 분산 처리 엔진입니다. 관계형 데이터베이스, NoSQL 데이터베이스, CSV 파일 및 Minio와 같은 객체 저장소를 포함한 다양한 데이터 원본을 지원합니다.\n- 인메모리 처리: Spark는 인메모리 계산을 활용하여 성능을 향상시키며, 자주 액세스되는 데이터를 디스크 기반 솔루션에 비해 더 빠른 처리를 위해 메모리에 유지합니다.\n- 구조적, 반구조적 및 비구조적 데이터: Spark는 CSV, JSON과 같은 구조적 데이터, XML과 같은 반구조적 데이터, 텍스트와 같은 비구조적 데이터를 포함한 다양한 데이터 형식을 처리할 수 있습니다. 이러한 유연성으로 인해 현대 데이터 생태계에서 다양한 데이터 유형을 다루는 데 이상적입니다.\n- 머신러닝 및 스트림 처리: 전통적인 분석 이상으로 Spark는 머신러닝 파이프라인 및 실시간 데이터 처리까지 확장됩니다. Spark는 TensorFlow, PyTorch와 같은 인기있는 머신러닝 라이브러리를 효율적인 모델 훈련 및 배포를 위해 통합합니다. 또한 Apache Flink와 같은 스트림 처리 프레임워크를 지원하여 거의 실시간 데이터 분석을 수행할 수 있습니다.\n- Iceberg와의 원활한 통합: Spark는 네이티브 Iceberg 지원을 제공하여 Spark 애플리케이션에서 Iceberg 테이블을 직접 읽고 쓰기 및 쿼리할 수 있습니다. 이를 통해 Spark 워크플로우 내에서 데이터 관리를 간편하게 할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n3. Minio: 고성능 객체 저장 서버\n\n![Minio Image](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_3.png)\n\n- 오픈소스 및 비용 효율적: Minio는 무료 및 오픈소스 객체 저장 솔루션으로, 대형 클라우드 공급업체가 제공하는 프로프리어터리 객체 저장 서비스에 대안으로 경제적입니다. 자체 데이터 저장 인프라를 관리하고 데이터 레이크 또는 데이터 웨어하우스 내에서 해당 기능을 활용할 수 있습니다.\n- 확장성 및 성능: Minio는 확장성을 고려하여 제작되었습니다. 데이터 저장 필요가 증가함에 따라 더 많은 노드를 Minio 클러스터에 쉽게 추가할 수 있어 증가하는 데이터 양을 처리할 수 있습니다. 또한 효율적인 데이터 액세스를 제공하여 Spark 애플리케이션에 대한 효율적인 데이터 액세스를 제공합니다.\n- S3 호환성: Minio는 Amazon S3와 API 호환성이 있어서 S3와 작동하는 기존 도구 및 애플리케이션과의 원활한 통합이 가능합니다. 이는 전통적인 클라우드 객체 저장에서 더 경제적인 온프레미스 솔루션으로의 원활한 전환을 용이하게 합니다.\n- 내구성 및 신뢰성: Minio는 데이터 중복 및 복제 메커니즘을 제공하여 데이터가 장애에 대비하여 보호되도록 합니다. 다중 노드 또는 저장 장치 간의 데이터 복제를 구성하여 하드웨어 문제 발생 시 데이터 손실 위험을 최소화할 수 있습니다.\n\n4. Docker 통합\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_4.png\" />\n\n도커는 구성 요소의 실행을 격리하고 관리하는 데 사용할 수있는 컨테이너화 플랫폼을 제공합니다. 이 아키텍처에서 도커가 어떻게 적합한지 살펴보겠습니다:\n\n- **도커 이미지**: 각 서비스(Spark Master, Spark Worker 및 Minio)를 위한 도커 이미지를 만들 수 있습니다. 필요한 모든 종속성(Spark, Minio 이진 파일, Iceberg 라이브러리 등)을 포함하여 일관된 환경을 제공하고 다양한 기기에 배포를 단순화합니다.\n- **도커 콤포즈**: 도커 콤포즈와 같은 도구를 사용하여 모든 서비스의 구성 및 배포를 함께 관리할 수 있습니다. 이 도구는 서비스, 종속성 및 환경 변수를 단일 YAML 파일에 정의하여 전체 환경 설정 프로세스를 간편화합니다.\n\n도커를 활용하면 이동성이 뛰어나고 격리된 개발 또는 프로덕션 환경을 구축할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 도커 컴포즈 구성\n\n아래는 Minio, Spark 마스터 및 Spark 워커 서비스를 설정하는 Docker Compose 파일입니다.\n\n```js\nversion: '3.9'\nservices:\n  minio:\n    image: minio/minio\n    container_name: minio\n    environment:\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    command: server /data --console-address \":9001\"\n    volumes:\n      - minio_data:/data\n\nspark-master:\n    image: bitnami/spark:latest\n    container_name: spark-master-minio-iceberg\n    environment:\n      - SPARK_MODE=master\n      - SPARK_SUBMIT_ARGS=--packages org.apache.iceberg:iceberg-spark3-runtime:0.12.0\n    ports:\n      - \"7077:7077\"\n      - \"8080:8080\"\n  spark-worker:\n    image: bitnami/spark:latest\n    container_name: spark-worker-minio-iceberg\n    environment:\n      - SPARK_MODE=worker\n      - SPARK_MASTER_URL=spark://spark-master:7077\n      - SPARK_SUBMIT_ARGS=--packages org.apache.iceberg:iceberg-spark3-runtime:0.12.0\n    depends_on:\n      - spark-master\n    ports:\n      - \"8081:8081\"\nvolumes:\n  minio_data:\n```\n\n# Docker Compose 파일 설명\n\n<div class=\"content-ad\"></div>\n\n- Minio 서비스: Minio 서버를 실행하여 포트 9000(API)과 9001(콘솔)을 노출합니다. Minio 데이터는 minio_data라는 이름의 Docker 볼륨에 저장됩니다.\n- Spark Master 서비스: Delta Lake 및 Iceberg를 위한 패키지가 포함된 Spark 마스터 노드를 실행합니다. Spark UI 및 마스터 통신을 위한 포트 7077 및 8080이 노출됩니다.\n- Spark Worker 서비스: Spark 워커 노드를 실행하고 Spark 마스터에 연결됩니다. Delta Lake 및 Iceberg를 위한 패키지가 포함되어 있습니다.\n\n# 이미지 빌드\n\n다음 명령을 실행하여\n\n```js\ndocker-compose up -d\n```\n\n<div class=\"content-ad\"></div>\n\n모든 서비스가 정상적으로 작동 중이거나 문제가 발생한 경우 알림이 표시됩니다 (문제가 발생하지 않기를 희망합니다 😄)\n\n![Image 5](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_5.png)\n\n![Image 6](/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_6.png)\n\n먼저 http://localhost:9001/browser를 방문하여 아래 이미지를 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_7.png\" />\n\n모든 서비스가 실행되면 Apache Iceberg를 사용하여 Apache Spark 및 Minio로 읽기와 쓰기를 하는 간단한 데이터 파이프라인을 생성해 봅시다.\n\n# Spark 작업 설정 및 실행하기\n\n다음의 Python 코드는 Minio와 상호 작용하고 Iceberg를 사용하여 Spark를 사용하는 방법을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n# Minio 클라이언트 초기화\n\n제공된 자격 증명으로 Minio 서버에 연결합니다.\n\n```js\nfrom minio import Minio\nclient = Minio(\n    \"127.0.0.1:9000\",\n    access_key=\"your_admin_name_account\",\n    secret_key=\"your_password\",\n    secure=False\n)\n```\n\n## 설명\n\n<div class=\"content-ad\"></div>\n\n- Minio 초기화: 이 코드는 Minio 클라이언트를 초기화하여 127.0.0.1의 9000 포트에서 실행 중인 Minio 서버에 연결합니다. 액세스 키와 시크릿 키로 minioadmin을 사용하고, SSL을 사용하지 않는 연결이므로 secure를 False로 설정합니다.\n\n# 버킷 관리\n\n버킷이 존재하는지 확인하고, 존재하지 않으면 생성합니다.\n\n```js\nminio_bucket = \"my-first-bucket\"\nfound = client.bucket_exists(minio_bucket)\nif not found:\n    client.make_bucket(minio_bucket)\n```\n\n<div class=\"content-ad\"></div>\n\n## 설명\n\n- Bucket 존재 여부 확인: bucket_exists 메서드는 Minio에 my-first-bucket이라는 이름의 버킷이 이미 있는지 확인합니다.\n- Bucket 생성: 버킷이 존재하지 않는 경우 make_bucket 메서드를 사용하여 my-first-bucket이라는 새 버킷을 만듭니다.\n\n# 파일 업로드\n\nMinio에 지정된 버킷에 CSV 파일을 업로드합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n목적지_파일 = 'data.csv'\n원본_파일 = './data/data.csv'  # 프로젝트 폴더에이 파일이 존재하는지 확인하세요\nclient.fput_object(minio_bucket, 목적지_파일, 원본_파일)\r\n```\n\n## 설명\n\n- 파일 업로드: fput_object 메서드는 로컬 파일 ./data/data.csv를 Minio 버킷 my-first-bucket으로 객체 이름이 data.csv인 파일로 업로드합니다.\n\n## 코드 실행 후🎦\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*rZUjqaNCls7_73eojd2RQw.gif)\n\n# SparkSession 구성\n\nSpark를 Iceberg 및 Minio를 스토리지 백엔드로 사용하도록 구성합니다.\n\n```js\nfrom pyspark.sql import SparkSession\niceberg_builder = SparkSession.builder \\\n    .appName(\"iceberg-spark-minio-example\") \\\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.iceberg:iceberg-hive-runtime:1.5.0\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n    .config(\"spark.sql.catalog.spark_catalog.warehouse\", f\"s3a://{minio_bucket}/iceberg_data/\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"your_admin_name_account\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"your_pasword\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .enableHiveSupport()\n```\n\n<div class=\"content-ad\"></div>\n\n## 설명\n\n- SparkSession 빌더: Iceberg 및 Minio를 사용하기 위해 구성된 Spark 세션 빌더를 생성합니다.\n- 애플리케이션 이름: \"iceberg-spark-minio-example\"로 애플리케이션 이름을 설정합니다.\n- JAR 패키지: Hadoop AWS 및 Iceberg에 필요한 JAR를 포함합니다.\n- Spark 확장: Spark SQL을 위해 Iceberg 확장 기능을 활성화합니다.\n- 카탈로그 구성: 카탈로그 유형을 Hadoop으로 구성하고, 웨어하우스 위치를 Minio 버킷을 가리키는 S3 경로로 설정합니다.\n- Minio 자격 증명: Minio의 액세스 및 시크릿 키를 설정합니다.\n- 엔드포인트 구성: S3 엔드포인트를 로컬 Minio 서버를 가리키도록 구성하고, S3 URI에 대한 경로 스타일 액세스를 활성화합니다.\n\n# Iceberg용 SparkSession 빌드\n\nIceberg 테이블과 상호 작용하기 위한 Spark 세션을 빌드합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\niceberg_spark = iceberg_builder.getOrCreate()\n```\n\n## 설명\n\n- SparkSession 생성: getOrCreate 메서드는 지정된 구성으로 Spark 세션을 초기화합니다.\n\n# 데이터 로드\n\n\n<div class=\"content-ad\"></div>\n\n스파크 데이터프레임으로 CSV 파일을 읽습니다.\n\n```js\ndf = iceberg_spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(source_file)\n```\n\n## 설명\n\n- CSV 파일 읽기: ./data/data.csv 파일을 스파크 데이터프레임으로 로드합니다. header 옵션은 첫 번째 행을 열 이름으로 사용하도록 설정되었고, inferSchema 옵션은 데이터 유형을 자동으로 추정하도록 설정되었습니다.\n\n<div class=\"content-ad\"></div>\n\n# 아이스버그 테이블 위치 정의\n\n미니오 내 아이스버그 테이블의 위치를 지정합니다.\n\n```js\niceberg_table_location = f\"s3a://{minio_bucket}/iceberg_data/default\"\n```\n\n## 설명\n\n<div class=\"content-ad\"></div>\n\n- Iceberg Table 위치: Minio에 Iceberg 테이블 데이터를 저장하는 데 사용되는 S3 경로를 정의합니다. 경로는 버킷 이름과 하위 디렉터리 iceberg_data/default을 사용하여 구성됩니다.\n\n# Iceberg 테이블에 쓰기\n\nDataFrame 데이터를 Minio의 Iceberg 테이블에 씁니다.\n\n```js\ndf.write \\\n    .format(\"iceberg\") \\\n    .mode(\"append\") \\\n    .saveAsTable(\"iceberg_table_name\")  # Iceberg 테이블의 이름\n```\n\n<div class=\"content-ad\"></div>\n\n## 설명\n\n- DataFrame 작성: Iceberg 형식을 사용하여 append 모드로 iceberg_table_name이라는 Iceberg 테이블에 DataFrame을 작성합니다.\n\n## 코드 실행 후🎦\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*oUTGnwgU4yk2nG31-kuIWA.gif\" />\n\n<div class=\"content-ad\"></div>\n\n# Iceberg 테이블에서 읽기\n\nIceberg 테이블에서 데이터를 읽어 스키마와 몇 가지 샘플 레코드를 표시합니다.\n\n```js\niceberg_df = iceberg_spark.read.format(\"iceberg\").load(f\"{iceberg_table_location}/iceberg_table_name\")\niceberg_df.printSchema()\niceberg_df.show()\n```\n\n## 설명\n\n<div class=\"content-ad\"></div>\n\n- 아이스버그 테이블 읽기: 아이스버그 테이블에서 데이터를 읽어와 DataFrame에로드합니다.\n- 스키마 출력: DataFrame의 스키마를 표시합니다.\n- 데이터 표시: DataFrame에서 몇 가지 샘플 레코드를 표시합니다.\n\n## 코드 실행 후🎦\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*qnK1mFZ6ecqTXBGCtEWi0w.gif\" />\n\n# 전체 코드🖥️\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom minio import Minio\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\n# Minio 클라이언트 및 버킷 생성\nclient = Minio(\n    \"127.0.0.1:9000\",\n    access_key=\"minioadmin\",\n    secret_key=\"minioadmin\",\n    secure=False\n)\n\nminio_bucket = \"my-first-bucket\"\n\nfound = client.bucket_exists(minio_bucket)\nif not found:\n    client.make_bucket(minio_bucket)\n\ndestination_file = 'data.csv'\nsource_file = './data/data.csv' ## 프로젝트 폴더 내에 파일이 있어야 함\n\n# Minio에 파일 업로드\nclient.fput_object(minio_bucket, destination_file, source_file,)\n\n# Iceberg와 Minio 설정을 사용한 SparkSession 빌더 생성\niceberg_builder = SparkSession.builder \\\n    .appName(\"iceberg-concurrent-write-isolation-test\") \\\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.iceberg:iceberg-hive-runtime:1.5.0\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n    .config(\"spark.sql.catalog.spark_catalog.warehouse\", f\"s3a://{minio_bucket}/iceberg_data/\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .enableHiveSupport()\n\n# Iceberg를 위한 SparkSession 생성\niceberg_spark = iceberg_builder.getOrCreate()\n\n# Iceberg 테이블을 Minio에 쓰기\ndf.write \\\n    .format(\"iceberg\") \\\n    .mode(\"append\") \\\n    .saveAsTable(\"iceberg_table_name\")  # Iceberg 테이블 이름\n\n# Iceberg 테이블에서 데이터 읽기\niceberg_df = iceberg_spark.read.format(\"iceberg\").load(f\"{iceberg_table_location}/iceberg_table_name\")\n\n# 데이터프레임 스키마 및 데이터 출력\nprint(\"**************************\")\nprint(\"This the Dataframe schema \")\nprint(\"**************************\")\niceberg_df.printSchema()\n\nprint(\"**************************\")\nprint(\"******Dataframe Data******\")\nprint(\"**************************\")\niceberg_df.show()\n```\n\n# GitHub\n\n프로젝트 링크\n\n# Summary\n\n\n<div class=\"content-ad\"></div>\n\n이 설정을 사용하면 Spark 및 Iceberg로 대규모 데이터를 효율적으로 관리하고 처리할 수 있고, 확장 가능한 객체 저장소로 Minio를 사용할 수 있습니다. Docker를 활용하면 이러한 구성 요소의 배포와 관리가 간편하고 재현 가능해집니다.\n\n즐거운 학습이 되길 바래요 😉","ogImage":{"url":"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png"},"coverImage":"/assets/img/2024-06-22-UsingApacheIcebergwithApacheSparkandMinioDocker_0.png","tag":["Tech"],"readingTime":14},{"title":"모든 문제를 해결하는 하나의 계층  시맨틱 레이어란 무엇인가","description":"","date":"2024-06-22 17:05","slug":"2024-06-22-SemanticLayerOneLayertoServeThemAll","content":"\n\n## 기술과 비즈니스 간의 간극을 메우다\n\n![이미지](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_0.png)\n\n- 시멘틱 레이어는 복잡한 데이터 구조와 비즈니스 용어 사이의 다리 역할을 하며, 데이터의 통합된 보기를 제공하여 접근을 간소화하고 조직적 의사결정의 일관성을 보장합니다.\n- 시멘틱 레이어를 활용하면 데이터 거버넌스를 개선하고 AI 통합을 용이하게 하여 신뢰할 수 있고 투명한 분석 및 정보에 기초한 결정이 가능해지며, 조직 프로세스를 효율성과 적응성을 높일 수 있습니다.\n\n# 목차\n\n<div class=\"content-ad\"></div>\n\n# 소개\n\n시맨틱 레이어는 현대 데이터 관리에서 점점 더 중요한 요소가 되고 있습니다. 이러한 레이어의 부재는 제한된 데이터 가용성, 보고서 상의 불일치 및 잘못된 의사 결정으로 이어지며 IT 자원에 부담을 가중시킵니다. 데이터 민주화에 대한 노력은 다양한 비즈니스 인텔리전스 도구와 데이터 소스의 증가로 어려움이 더해져 종종 일관되지 않은 분석 결과와 거버네스 문제를 야기합니다.\n\n시맨틱 레이어를 도입함으로써 이러한 문제를 해결할 수 있습니다. 비즈니스 데이터의 일관된 표현으로 작용하면서 복잡한 데이터 구조를 익숙한 비즈니스 용어로 변환함으로써 조직 전체에서 데이터의 통합된 관점이 만들어지며, 접근이 간소화되고 일관성이 보장됩니다. 전문가들은 데이터 엔지니어링과 비즈니스 분석 사이의 간극을 좁히는 중요성을 강조하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 기사는 시맨틱 레이어의 정의와 기능을 살펴보고 기업 데이터를 조직화하고 추상화하여 의사 결정 프로세스를 용이하게 하는 중추적 역할을 설명합니다. 시맨틱 레이어를 구현하는 다양한 이점을 탐구하며 데이터 일관성, 거버넌스 및 민첩성이 향상되는 등의 이점을 논의합니다. 현대 기업 데이터와 AI 관리의 복잡성을 다루면서, 시맨틱 레이어는 운영 효율성 향상과 더 많은 정보에 기반한 의사 결정 능력을 약속하는 중심 요소로 등장합니다.\n\n# 시맨틱 레이어가 필요한 이유는?\n\nMicah Horner (TimeXtender)는 간결히 설명합니다. 그는 시맨틱 레이어가 없으면 조직이 데이터를 효과적으로 활용하는 데 제한이 발생한다고 주장합니다:\n\n- 제한된 데이터 접근 및 사용: 데이터 격리와 복잡성은 비기술적 사용자가 데이터에 액세스하기 어렵게 만들며, 시맨틱 레이어 없이는 정보 기반 결정과 데이터 기반 이니셔티브가 저해됩니다.\n- 통일된 데이터 언어 부재: 부서 간 다른 용어들은 혼란과 오해를 야기하며, 시맨틱 레벨이 없으면 비즈니스 목표를 조정하기 어렵게 합니다.\n- 보고 및 분석의 일관성 부재: 일관되지 않은 데이터 정의와 계산으로 인해 모순된 결과와 신뢰성 없는 결정이 발생하며, 비용이 발생합니다.\n- IT 부담 증가: 시맨틱 레이어가 없으면 IT 팀은 데이터 액세스 요청 및 문제에 너무 많은 시간을 할애하며, 전략적 이니셔티브로부터 자원을 분산시킵니다.\n- 제한된 민첩성과 확장성: 수동 데이터 통합 프로세스로 인해 비즈니스 변화에 대응하거나 영업을 확장하는 능력이 제한되어 시장 변화에 대한 대응 능력이 제한됩니다.\n- 데이터 거버넌스 및 규정 준수 위험: 일관되지 않은 데이터 관리는 기업 거버넌스와 규정 준수에 위험을 초래하며, 법적 및 재정적 결과가 발생할 수 있습니다.\n- 소실된 통찰과 경쟁 우위: 접근할 수 없는 데이터와 공유된 이해 부재로 인해 시장에서의 놓친 통찰과 경쟁 우위가 감소합니다.\n\n<div class=\"content-ad\"></div>\n\nKieran O’Driscoll(AtScale), Kyle Hale and Soham Bhatt (Databricks)가 블로그 글에서 대부분의 기업이 여전히 데이터 민주화에 어려움을 겪고 있다고 설명합니다.\n\n의사결정자에게 데이터를 제공하는 것은 특히 대규모 조직에서 어려운 과제입니다. 기업의 절반 이상이 세 개 이상의 비즈니스 인텔리전스 도구를 사용하며, 데이터 과학자와 애플리케이션 개발자는 각자 선호하는 도구를 가지고 있습니다.\n\n다양한 도구와 쿼리 언어는 상반되는 분석 결과를 내놓습니다. 서로 다른 데이터 복사본이나 Tableau Hyper Extracts, Power BI Premium Imports, 또는 SSAS와 같은 OLAP 솔루션을 사용하는 여러 사업 부서들은 이 문제를 더욱 악화시킵니다.\n\n다양한 마트, 데이터 웨어하우스 및 보고 도구에 데이터를 저장하면 단일 진실의 버전을 유지하는 것이 어렵습니다. 이로 인해 데이터 이동, ETL, 보안 및 복잡성이 증가하며 데이터 거버넌스 문제가 발생하고 잠재적으로 오래된 데이터에 의존하게 됩니다.\n\n<div class=\"content-ad\"></div>\n\nAtScale은 기업이 의사소통의 간극을 좁히기 위해 시멘틱 레이어가 필요한 이유를 매우 명확하게 설명하는 예를 사용합니다.\n\nDonald Farmer(AtScale)은 데이터 엔지니어와 비즈니스 분석가 간의 심각한 도전 과제를 강조하고 시멘틱 레이어가 이 간극을 메우는 데 도움이 되는 이유에 대해 설명합니다.\n\n많은 조직은 데이터 엔지니어(코드 기반 환경을 선호하는)와 비즈니스 분석가(비코드 인터페이스를 선호하는) 간의 연결 부재로 데이터 기반 의사결정에 어려움을 겪고 있습니다. 이러한 불일치는 비효율성, 일관되지 않은 데이터 정의 및 부정확한 의사결정을 초래합니다. 견고한 시멘틱 레이어는 엔지니어를 위한 API와 분석가를 위한 직관적 인터페이스를 제공함으로써 이러한 간극을 메울 수 있습니다. 이는 조직의 기술과 기술을 최적화합니다.\n\n시멘틱 레이어는 다양한 작업 스타일을 수용하는 통합 플랫폼을 제공하며 협업, 데이터 거버넌스 및 혁신을 강화합니다. 엔지니어가 데이터에 프로그래밍적으로 액세스하고 조작하는 데 사용할 수 있는 견고한 API를 제공하여 워크플로에 원활하게 통합됩니다. 동시에, 기술 전문지식이 많이 필요하지 않는 분석가를 위한 직관적 인터페이스를 제공합니다. 이 통합된 기반은 일관된 정의와 지표를 보장하며 더 나은 의사소통과 조정을 촉진합니다. 공유 작업 영역, 버전 관리 및 주석 기능과 같은 기능은 협업과 지식 공유를 촉진하며 조직이 효과적으로 데이터를 활용하고 비즈니스 가치를 실현할 수 있도록 돕습니다.\n\n<div class=\"content-ad\"></div>\n\n# 시맨틱 레이어란 무엇인가요\n\n위키피디아에서는 시맨틱 레이어를 다음과 같이 정의합니다:\n\nAtScale에서는 시맨틱 레이어를 다음과 같이 정의합니다:\n\n그리고 추가 설명...\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_1.png)\n\nLulit Tesfaye (Enterprise Knowledge) defines the Semantic Layer as follows:\n\nand further …\n\nSummarizing these definitions, the semantic layer can be defined as follows:\n\n\n<div class=\"content-ad\"></div>\n\n시맨틱 레이어는 복잡한 기업 데이터를 친숙한 비즈니스 용어로 번역하고 서로 다른 데이터 원본을 매핑하고 데이터 관계를 관리하여 통합된 보기를 제공합니다. 사용자를 위해 데이터 모델을 간소화하며 구조화된, 비구조화된 및 반구조화된 데이터를 포함합니다. 메타데이터 레이어 역할을 하여 콘텐츠의 관리와 분석을 향상시킵니다. 데이터베이스와 최종 사용자 사이에 추상화 레이어 역할을 하며 일관된 데이터 뷰를 제공하고 SQL 지식 없이 직관적인 쿼리를 지원합니다. 또한 접근 제어, 데이터 품질 보증 및 정책 강제를 통해 데이터 관리를 지원합니다.\n\n# 시맨틱 레이어가 비즈니스 요구를 어떻게 지원하는가\n\n시맨틱 레이어는 데이터의 통합된 보기를 제공하여 일관된 액세스와 쿼리를 가능하게 합니다. 사용자 경험을 향상시키고 조직의 효율성을 높이며 기업 전반에 걸친 분석에 표준화된 접근 방법을 제공하여 다수의 혜택을 얻을 수 있습니다:\n\n- 진실의 단일 원천: 데이터는 원본에 관계 없이 표준 형식으로 제공되어 사용자가 다양한 도구와 기술로 분석할 수 있습니다. 시맨틱 레이어를 사용하는 기업은 단일 데이터원을 제한되지 않고 부서 간 분석을 수행할 수 있습니다.\n- 간소화된 데이터 액세스: 시맨틱 레이어는 복잡한 데이터 구조의 간소화된 통합 보기를 제공하며 사용자가 심도 있는 기술 지식 없이 데이터에 액세스하고 이해하는 것을 용이하게 합니다. 이로써 기업에서 데이터 액세스를 민주화할 수 있습니다.\n- 분석과 AI의 민주화: 데이터 분석이 확대됨에 따라 모든 요구 사항에 대해 단일 BI 또는 ML 플랫폼에 의존하는 것은 비현실적입니다. 시맨틱 레이어 플랫폼은 다양한 데이터 플랫폼, 프로토콜 및 도구를 연결하여 데이터와 사용을 분리하고 분석 및 ML의 민주화를 가능하게 합니다. [계속]\n\n<div class=\"content-ad\"></div>\n\n# 시맨틱 레이어 구현\n\n기업은 다양한 소스에서 데이터를 추상화하고 그 문맥을 이해하며 실행 가능한 통찰을 추출하는 도구가 필요합니다. 이를 통해 모든 사용자에 대한 데이터 문해력을 제고할 수 있습니다. 현대적인 \"범용\" 시맨틱 레이어 플랫폼은 시맨틱 레이어의 원래 강점을 강화하여 거버넌스 중앙화를 실현하고 비즈니스 지향적 데이터 관점을 촉진합니다.\n\n## 시맨틱 레이어의 구성 요소\n\nMicah Horner (TimeXtender)는 시맨틱 레이어의 구성 요소를 설명하며 복잡한 데이터 구조를 직관적인 사용자 경험으로 손쉽게 전환하는 방법을 소개합니다:\n\n<div class=\"content-ad\"></div>\n\n1. 데이터 수집\n\n- 여러 출처에서 데이터 수집: 데이터베이스, 스프레드시트, API 등에서 데이터를 수집하여 중요한 정보를 중앙 집중화하여 후속 처리 및 언어 계층 생성을 시작합니다.\n\n2. 데이터 준비\n\n- 데이터 변환 및 준비: 수집된 데이터를 정리, 유효성 검사하고 변환하여 정확성과 분석에 사용하기 쉬운 신뢰할 수 있는 데이터 세트를 만듭니다.\n- 데이터의 차원 모델링: 데이터를 차원과 사실로 구조화하여 복잡한 관계를 단순화하며 의미 있는 통찰력을 제공하는 언어 계층 구축에 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n3. 데이터 전달\n\n- 의미론적 레이어: 기술적 데이터를 비즈니스 친화적 용어로 번역하여 데이터를 이해하기 쉽고 모든 사용자에게 관련성 있게 만드는 의미 모델을 생성합니다.\n- 데이터 제품: 부서별 모델(데이터 제품)을 개발하여 맞춤형 데이터 액세스를 제공하여 각 팀이 필요로 하는 데이터를 과다하게 받지 않고 얻을 수 있도록 합니다.\n\nDavid P. Mariani(AtScale)은 A16Z 데이터 스택의 메트릭 레이어에서 변환 서비스를 활용한 범용 의미론적 레이어의 구현을 주장합니다. 데이터 모델링, 워크플로우 관리 및 권한 및 보안 등의 기능을 수행합니다. 이 레이어는 적절하게 조정되었을 때 다음과 같은 중요한 이점을 제공합니다:\n\n- 어떤 분석 도구에서도 접근할 수 있는 기업 메트릭과 계층적 차원을 위한 단일 진실의 소스 생성\n- 수월하게 업데이트하거나 새로운 메트릭을 정의하고 데이터의 도메인별 뷰를 설계하며 새로운 원시 데이터 자산을 통합하는 민첩성 제공\n- 분석 성능을 최적화하고 클라우드 자원 소비를 모니터링 및 최적화\n- 액세스 제어, 정의, 성능 및 자원 소비와 관련된 거버넌스 정책 강화\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_2.png)\n\n성공은 사용자가 의미적 확장 없이 자유롭게 혁신할 수 있는 중앙에서 관리되는 의미론적 레이어를 활용하는 데 달려 있어요.\n\n메트릭 레이어: 메트릭 레이어는 기업 메트릭의 진실된 단일 원천으로서, 다양한 분석 도구에서 접근할 수 있습니다. 이는 BI 도구, 응용 프로그램, 역방향 ETL 및 데이터 과학 도구를 위한 메트릭 저장소를 제공하며, 설계 및 변경 관리가 그 기능의 일부입니다. 효과적인 메트릭 레이어는 일관성, 효율성 및 사용자 경험이 원활하도록 하기 위한 쿠레이션, 변경 관리, 탐색 및 서비스 기능이 필요합니다.\n\n데이터 모델링: 데이터 모델링은 데이터 레이크하우스나 창고의 논리적 데이터 개념을 물리적 구조에 매핑하는 과정을 말합니다. 비주얼 프레임워크나 코드 기반 언어를 사용할 수 있습니다. 중요한 활동에는 데이터를 \"분석용으로 준비함\", 일관된 차원 정의, 메트릭 설계가 포함됩니다. 이는 비즈니스 의미론을 데이터 모델에 포함시키며, 복합적인 분석 접근을 통해 일관성, 거버넌스 및 혁신을 촉진합니다.\n\n<div class=\"content-ad\"></div>\n\n워크플로우 관리: 워크플로우 관리는 의미론적 레이어를 위한 물리적 변환을 조율하여 비용과 성능을 최적화합니다. 사용자는 데이터 집계물 생성이 필요한 최소한의 쿼리 대기 시간을 요구하는데, 이는 클라우드 규모의 데이터로 인한 것입니다. 성능 관리는 자동으로 생성물을 처리하며, 클라우드 및 노동 비용을 고려하면서 동적으로 적응합니다. 의미론적 레이어 데이터를 활용하여 워크플로우 관리는 성능과 비용을 최적화합니다.\n\n자격 및 보안: 의미론적 레이어의 자격 및 보안은 쿼리 시에 데이터 거버넌스 정책을 동적으로 시행하여 사용자가 올바른 데이터에 액세스하는 것을 보장합니다. 여러 자격과 일관된 정의를 관리함으로써 신뢰와 무결성을 유지합니다. 성능 최적화는 사용자의 자격 및 유즈 케이스 우선순위를 고려합니다. 실시간 정책 시행이 중요한데, 의미론적 레이어를 넘어 더 넓은 보안 서비스를 제공합니다.\n\n## 모던 데이터 스택 내 의미론적 레이어 통합\n\nDavid P. Mariani은 또한 모던 데이터 스택의 레이어가 주변 레이어와 원활하게 통합되어야 한다고 언급합니다.\n\n<div class=\"content-ad\"></div>\n\n의미 계층은 데이터 플랫폼, 분석 및 출력 계층, 그리고 메타데이터 및 서비스 계층과 깊게 통합되어야 합니다.\n\n클라우드 데이터 플랫폼에서의 범용 의미 계층은 데이터를 데이터 웨어하우스나 레이크하우스에 중앙 집중화합니다. 하이브리드/멀티 클라우드 설정에서는 플랫폼 간 쿼리를 위해 데이터 가상화가 필요합니다. 효율적인 워크플로 관리는 다양한 데이터 플랫폼 아키텍처와의 밀접한 통합을 필요로 합니다:\n\n쿼리 엔진 오케스트레이션: 고객으로부터 쿼리를 플랫폼별 SQL로 동적으로 변환하며, 플랫폼 고유 기능에 최적화되고 의미 모델에서 논리적-물리적 매핑을 반영합니다.\n\n변환 오케스트레이션: 뷰를 물리적 테이블로 재질화하며, 데이터 플랫폼 내에서 성능 및 비용을 최적화합니다.\n\n<div class=\"content-ad\"></div>\n\n### 워크플로 오케스트레이션: \n데이터 플랫폼 내에서 새로운 데이터 또는 메타데이터를 생성하며, 사용자 또는 AI/ML 상호 작용을 기반으로 합니다.\n\n### 사용자 정의 함수 (UDF): \n클라우드 데이터 플랫폼의 함수 라이브러리를 활용하여 분석 및 출력을 수행하여 시맨틱 레이어 기능을 향상시킵니다.\n\n### 메타데이터 및 지원 서비스: \n시맨틱 레이어는 메타데이터 및 지원 서비스와 통합하여 데이터 패브릭 생태계의 다양한 도구들과 협력합니다.\n\n- 시맨틱 레이어는 기업용 데이터 카탈로그 도구에 메트릭 및 데이터 모델 검색을 위한 메타데이터 및 계보를 공유해야 합니다.\n- 시맨틱 레이어는 다른 도구로부터 메타데이터를 가져와 시맨틱 데이터 모델을 자동화하고 표준화하기 위한 능력이 있어야 합니다.\n- 시맨틱 레이어에는 사용자 액세스, 가동 시간 및 시스템 성능을 관리하기 위한 모니터링 엔드포인트가 있어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n## AtScale과 Databricks를 활용한 의미론적 레이크하우스 구축\n\nAtScale과 Databricks의 협업은 물리적 테이블에 추상화 레이어를 제공하여 의미론적 레이크하우스를 만들고 있습니다. 이를 통해 엔티티, 속성 및 조인을 정의하여 데이터 소비를 단순화하여 분석가와 최종 사용자에게 비즈니스 친화적인 뷰를 제공합니다.\n\nAtScale의 의미론적 레이어는 분석 도구와 Databricks 레이크하우스 사이에 위치하여 데이터를 추상화하여 쉽게 소비할 수 있도록 합니다. Hive SQL, SSAS 큐브 또는 웹 서비스를 통해 연결하여 최적화된 SQL 실행을 위해 쿼리를 Databricks로 전달하여 성능과 확장성을 보장합니다.\n\nAtScale의 Universal Semantic Layer는 쿼리 패턴을 식별하고 자동으로 집계를 관리하는 자율 성능 최적화를 사용합니다. 이로써 수동 노력을 없애고 Delta Lake에서 \"Diamond Layer\" 집계를 생성하여 BI 보고 성능을 향상시키고 분석 데이터 파이프라인과 엔지니어링을 단순화합니다.\n\n<div class=\"content-ad\"></div>\n\n툴에 독립적인 의미론적 레이크하우스를 만들기\n\nDatabricks 레이크하우스 플랫폼은 데이터, 분석 및 AI 워크로드를 통합합니다. AtScale의 의미론적 레이크하우스는 BI 및 AI/ML을 지원하여 Tableau, Power BI, Excel 및 Looker를 통해 일관된 사용을 가능하게 하는 툴에 독립적인 의미론적 레이어를 확장합니다.\n\n![image](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_3.png)\n\nAtScale의 유니버설 의미론적 레이어는 BI 및 AI/ML 팀을 통합하여 기업 데이터에 일관된 액세스를 제공합니다. 엑셀을 사용하는 비즈니스 사용자와 노트북을 사용하는 데이터 과학자들이 Databricks 레이크하우스의 모든 기능을 활용할 수 있도록 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n# 데이터 분석의 의미론적 층\n\n조직이 민첩성을 유지하고 신속하게 정보된 결정을 내리길 원하는 경우, 데이터 분석의 유연성은 매우 중요합니다. 의미론적 층은 데이터 층을 시각화 층에서 추상화함으로써 이러한 유연성을 가능케하며 핵심적인 역할을 합니다. 이 추상화를 통해 분석가는 차트, 표 및 그래프를 포함한 데이터의 다른 관점을 쉽게 만들고 분석 필드 및 측정 항목을 손쉽게 변경할 수 있습니다. 결과적으로, 분석가는 빠르게 새로운 통찰을 얻을 수 있게 되며 기업이 변화하는 시장 조건에 빠르게 대응할 수 있도록 전략을 조정하는 데 도움을 줄 수 있습니다.\n\n리처드 마카라(Reconfigured)는 의미론적 층을 갖는 기업들이 사용 가능한 데이터가 업데이트되는 즉시에 액세스할 수 있기 때문에 의사결정에서 더 유연해지고 있다고 언급합니다. 이 실시간 데이터 액세스는 비즈니스 환경의 변화에 적극적으로 대응하고 새로운 기회와 잠재적 위험을 식별하는 데 도움을 줄 수 있습니다. 이러한 유연성을 활용할 수 있는 능력은 투자하는 회사들이 경쟁 우위를 확보하고 현대의 빠르게 변화하는 비즈니스 환경에서 정보에 기반한 결정을 내리는 데 중요합니다. 이는 사용자들이 다음과 같은 일들을 수행할 수 있게 합니다:\n\n- 데이터 표현 방법을 빠르고 쉽게 변경\n- 비즈니스 요구 사항의 변화에 대응\n- 다양한 소스에서 데이터에 쉽게 액세스하고 분석\n- 기존 보고서에서 쉽게 보이지 않는 추세, 이상점 및 다른 통찰력 식별 \n- 협업 데이터 분석 프로젝트에 더 쉽게 참여\n\n<div class=\"content-ad\"></div>\n\n데이터 분석의 유연성이 기업이 비즈니스 환경 변화에 신속히 대응하고 데이터 분석 및 보고서 작성에서 혁신을 유발하는 데 도움이 됩니다. 다양한 데이터 원본과 분석 요구 사항을 수용하는 다재다능한 도구를 제공하여, 기업은 경쟁력을 향상시키고 데이터 가치를 극대화할 수 있습니다.\n\n의미 계층은 데이터 분석을 강화시켜 동일한 데이터 원본에 관계 없이 통합 데이터 원천을 제공함으로써 다부서 간 다재다능한 분석을 가능하게 합니다. 비즈니스 요구 사항의 변화에 따라 데이터 모델을 쉽게 조정할 수 있어 사용자를 방해하지 않고 유연하게 대응할 수 있습니다. 추상화 능력은 복잡한 보고서와 시각화물을 간단하게 작성할 수 있도록 도와 분석적 창의력을 촉진합니다. 데이터 원본의 기술적 제약이 분석을 제한하지 않아 유연성이 더욱 향상됩니다. 사용자는 데이터에 효율적으로 액세스하고 분석하여 의사 결정을 강화할 수 있습니다. 외부 소스와 맞춤형 데이터 계층의 통합은 다차원 분석을 용이하게 합니다.\n\n요컨대, 의미 계층은 현대 기업이 데이터 분석을 미래 지향적으로 준비하고 효율성을 향상시키며 정보에 기반한 의사 결정을 가능하게 함으로써 기업을 강화합니다.\n\n## 데이터 분석에서 의미 계층의 혜택\n\n<div class=\"content-ad\"></div>\n\nSean Leslie (data.world)은 의미론적 레이어가 데이터 분석에 도움이 되는 여러 이유 중에 다음을 강조합니다:\n\n데이터 통합 및 추상화를 단순화합니다: 의미론적 레이어는 다양한 소스에서 데이터를 통합하여 통일된 관점을 제공하고, 데이터 통합을 간소화하여 데이터 분석가와 비즈니스 사용자가 효율적으로 데이터를 조합하고 접근할 수 있도록 합니다.\n\n데이터 이해도와 접근성을 향상시킵니다: 의미론적 레이어는 공통의 비즈니스 어휘를 사용하여 기술적 데이터와 비즈니스 사용자를 연결하여 셀프 서비스 분석 및 BI 도구를 통해 쉬운, 비즈니스에 맞는 데이터 탐색 및 분석을 가능하게 합니다.\n\n데이터 지배 및 보안을 용이하게 합니다: 의미론적 레이어는 데이터 일관성을 위해 비즈니스 규칙을 적용하고 데이터 무결성을 유지하며 역할에 따라 액세스 컨트롤을 시행하여 안전하고 규정 준수된 데이터 액세스를 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n# 의미 계층이 데이터 거버넌스에 미치는 영향\n\n리처드 마카라 (Reconfigured)는 데이터 거버넌스가 데이터 보안, 가용성, 사용 가능성 및 무결성을 관리하는 과정을 지칭한다고 설명합니다. 이는 데이터 자산을 관리하기 위한 정책, 절차 및 통제 방법을 수립하는 것을 포함합니다. 데이터 거버넌스의 목적은 비즈니스 가치 증대, 리스크 감소 및 규정 준수를 보장하는 것입니다.\n\n데이터 거버넌스는 정책과 통제를 통해 올바른 데이터 관리를 보장합니다. 그 중요성은 데이터 무결성, 규정 준수, 보안 유지 및 효과적인 의사 결정을 원활하게 하는 데 있습니다:\n\n- 규정 준수: 데이터 거버넌스는 GDPR, HIPAA 등 관련 법률 및 규정을 준수하는 것을 보장합니다.\n- 신뢰할 수 있는 데이터: 데이터 거버넌스는 데이터 표준화 및 메타데이터 관리 정책을 수립하는 데 도움을 줄 수 있으며, 이는 더 정확한 데이터와 더 나은 분석으로 이어질 수 있습니다.\n- 더 나은 의사 결정: 데이터 거버넌스는 믿을 만한 데이터를 생성하는 데 도움을 줄 수 있어 더 나은 비즈니스 결정을 내릴 수 있게 합니다.\n- 리스크 완화: 데이터 거버넌스 정책이 있을 때 조직은 데이터 손실, 데이터 유출 및 관련 문제와 관련된 리스크를 완화할 수 있습니다.\n- 데이터 보안: 데이터 거버넌스에는 권한이 없는 또는 악의적인 접근을 방지하는 데이터 보안 정책이 포함될 수 있습니다.\n- 데이터 정책 강화: 강력한 데이터 거버넌스를 통해 조직은 각 기능 및 사업 단위 전반에서 데이터 정책의 준수를 보장할 수 있습니다.\n- 비용 절감: 효과적인 데이터 거버넌스를 통해 부정확한 데이터 의사 결정, 신뢰할 수 없는 분석 및 데이터 중복으로 인한 비용을 줄일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 거버넌스는 조직 내 데이터 품질과 데이터 관리에 상당한 영향을 미칩니다. 빅데이터와 생성적 AI의 급격한 증가로 인해 그 중요성은 높아지고 있으며, 성장하는 정보 양을 효과적으로 처리하기 위한 적응 가능한 데이터 전략이 필요합니다.\n\n## 어떻게 의미론적 레이어가 데이터 거버넌스를 개선하는가\n\n의미론적 레이어는 다양한 소스로부터의 복잡한 데이터를 비즈니스 용어로 단순화하여 조직 전반에 걸쳐 일관된 이해를 보장합니다. 데이터 정의를 중앙 집중화하여 모델링을 단순화하고 변화를 효율적으로 관리할 수 있도록 합니다. 보고 및 분석을 위한 단일 액세스 포인트로 작용하여 데이터 검색을 향상시킵니다. 이는 데이터 거버넌스를 통해 일관성, 표준화, 효율성을 촉진함으로써 데이터 품질, 정확성, 보안 및 규제 준수를 효과적으로 관리합니다:\n\n데이터의 명확한 정의: 의미론적 레이어는 데이터의 일관된 이해를 보장하여 명확한 정의와 문맥을 통해 일관성 부족으로 인한 혼돈과 오류를 제거합니다.\n\n<div class=\"content-ad\"></div>\n\n향상된 데이터 품질: 시맨틱 레이어는 데이터 일관성과 정확성을 보장하여 에러를 최소화하고 데이터 품질을 향상시킵니다. 표준화된 정의는 결정 신뢰성, 효율성, 고객 경험을 향상시키며 에러를 방지하고 정확한 데이터 관리와 검색을 용이하게 합니다.\n\n데이터 모델링의 유연성: 시맨틱 레이어는 데이터 모델링을 간소화하여 여러 소스에 대한 통일된 모델을 제공하며 일관성을 보장하고 진화하는 비즈니스 요구 사항에 적응하며 관리와 유지 보수 프로세스를 간소화하여 전체 데이터 관리 효율성을 향상시킵니다.\n\n제어된 접근: 시맨틱 레이어는 정확한 데이터 접근 제어를 가능케 하여 인가된 사용자가 미리 정의된 규칙에 따라 데이터를 처리할 수 있도록 하여 관리와 데이터 보안을 보장합니다.\n\n변경 관리: 시맨틱 레이어는 데이터 정의를 중앙 집중화하여 변경 관리를 간소화하고 여러 소스 간에 수동 업데이트를 제거하여 효율성을 향상시키고 일관성을 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n감사 기능: 시맨틱 레이어는 데이터 접근을 규제하여 감사 기능을 향상시키며, 사용자 활동 추적을 가능하게 하고 책임 소지 및 데이터 사용 및 관리의 투명성을 촉진합니다.\n\n효율적인 워크플로우: 시맨틱 레이어는 데이터 분석, 공유 및 협업을 위한 통합 환경을 제공하여 작업 흐름을 최적화하고 중복을 최소화하며 전반적인 효율성을 향상시킵니다.\n\n데이터 일관성: 시맨틱 레이어는 정의된 규칙과 표준에 따라 통제되어 일관된 데이터 접근성과 무결성을 보장하므로 조직 전반에 걸쳐 보다 정확하고 신뢰할 수 있는 통찰력을 얻을 수 있습니다.\n\n향상된 데이터 계보: 시맨틱 레이어는 데이터 계보 추적을 가능하게 하여 소스에서 비즈니스 개념 매핑을 단순화하고 변환을 투명하게 문서화합니다. 이로써 조기 오류 탐지를 통해 데이터 품질, 규정 준수 및 개선된 의사 결정을 보장하여 거버넌스를 강화합니다.\n\n<div class=\"content-ad\"></div>\n\n간소화된 데이터 감사: 의미론적 레이어를 통해 더 나은 데이터 거버넌스를 위해 세밀한 수준에서 데이터를 추적하고 감사할 수 있습니다. 이를 통해 데이터의 오류와 불일치를 식별하기가 더 쉬워집니다.\n\n간소화된 데이터 거버넌스: 의미론적 레이어는 메타데이터와 모델을 중앙 집중화하여 데이터 거버넌스를 간소화하며 정책 이행, 자동화된 품질 점검, 일관된 기준을 유지하는 데 도움을 줍니다. 이는 모든 출처에서의 데이터 정확성과 효율성을 향상시키고 리스크를 완화합니다.\n\n개인정보 보호 및 보안 강화: 점점 더 많은 데이터가 처리되고 수집되는 상황에서 데이터 보안과 개인정보 보호의 필요성도 더욱 커졌습니다. 의미론적 레이어는 데이터에 대한 액세스를 승인된 인원만 가능하게 해 개인정보 보호를 강화합니다.\n\n거버넌스 및 감사: 범용적인 의미론적 레이어를 사용하면 변경 이력을 기록하고 명확한 소유권을 파악할 수 있습니다. 또한 새로운 메트릭을 정의할 수 있는 사용자를 명시하거나 제한하는 것이 더 쉬워집니다.\n\n<div class=\"content-ad\"></div>\n\n향상된 규정 준수: 더 나은 데이터 거버넌스를 통해 의미론적 계층은 모든 데이터가 규정 요구 사항에 따라 처리되도록 보장하여 데이터 침해의 위험을 줄입니다.\n\n의미론적 계층을 구현하면 데이터 거버넌스에 상당한 혜택이 있습니다. 데이터의 통합된 표시를 제공함으로써 관리 프로세스를 간소화하고 데이터 품질과 보안을 향상시켜 효율적인 데이터 관리와 상식적인 의사 결정을 위한 필수적인 도구가 됩니다.\n\n## 의미론적 계층으로 AI 강화하기\n\nAtScale의 David P. Mariani는 선도적인 데이터 기관이 진단형, 예측형, 권고형 분석을 통합하는 AI와 함께 증강 분석에 중점을 두고 있다고 지적하며, 데이터 이동을 줄이며 효율성을 향상시키기 위해 데이터 과학/기계 학습 플랫폼을 통합한다고 합니다.\n\n<div class=\"content-ad\"></div>\n\n이문 찌리(Progress)는 기업 데이터와 생성 AI를 통합하는 것이 신뢰성, 투명성, 보안을 향상시키며 데이터 품질과 확장성을 향상시킬 수 있다고 강조했습니다. 의미론적 레이어를 활용하면 기업은 오류를 줄이고 신뢰성을 보장하며 거버넌스 표준을 준수하면서 혁신적인 비용 절감 기회를 창출하고 의사 결정 및 운영 효율성을 향상시킬 수 있습니다. 의미론적 레이어의 두 가지 주요 이점:\n\n\\[ \\begin{aligned} \n1. 생성 AI의 결과물에서 환각 감소: \\end{aligned} \\]\n생성 AI 모델은 종종 인간의 추론과 이해 부족으로 인해 알려진 환각이라고 불리는 잘못된 답변을 생성합니다. 이러한 오류는 15-20%의 확률로 발생하며 의미론적 데이터 플랫폼을 사용하여 데이터를 맥락화하고 조화시킴으로써 감소할 수 있습니다. 적절한 데이터 정리, 정리 및 모델링은 AI의 정확성을 향상시키는 데 필수적입니다.\n\n\\[ \\begin{aligned} \n2. 생성 AI의 결과물의 신뢰성과 신뢰성 향상: \\end{aligned} \\]\n생성 AI 모델은 종종 비관련, 부정확 또는 편향적인 결과물을 생성하는데, 이는 중요한 비즈니스 결정에 문제를 일으킬 수 있습니다. 기업 의미론적 데이터 시스템과 생성 AI를 통합함으로써 기업은 AI 결과물의 정확성과 신뢰성을 향상시킬 수 있습니다. 사설 의미론적으로 태그된 데이터를 사용해 생성 AI는 조직의 고유한 맥락에 대한 깊은 통찰력을 얻게 됩니다. 이 통합으로 인해 AI 시스템은 실시간 업데이트된 데이터에 접근하여 \"훈련 데이터 차단\" 문제를 해결하고 생성된 답변의 전반적인 품질을 향상할 수 있습니다.\n\n큐브는 대규모 언어 모델(LLM)에 대한 핵심적인 맥락을 제공하는 의미론적 레이어가 AI 기반 데이터 경험에서 중요하다고 강조합니다. 데이터를 의미 있는 비즈니스 정의로 조직하고 쿼리 인터페이스를 제공하여 LLM이 데이터의 맥락을 이해하도록 보장함으로써 오류와 환각을 줄이고 혁신적인 AI 응용프로그램을 가능하게 하며 쿼리 프로세스를 간소화합니다.\n\n<div class=\"content-ad\"></div>\n\nLLMs은 혁신적이지만, \"입력이 쓰레기면 출력도 쓰레기\" 문제 때문에 정확한 결과물을 생성하는 데 한계가 있습니다. LLMs는 환각을 일으킬 수 있습니다. 그들에게 단순히 데이터베이스 스키마를 공급하는 것만으로 올바른 SQL을 생성하는 데 충분하지 않습니다. 데이터 컨텍스트를 포함하여 데이터를 개념적으로 이해하기 위해 의미론적 계층이 필요합니다. 이 계층은 쿼리를 위해 데이터를 비즈니스 정의로 구성하고, LLM이 이를 통해 쿼리할 수 있도록하여 정확성을 보장합니다. 따라서 의미론적 계층은 LLM이 환각하는 문제를 해결함으로써 필요한 컨텍스트를 제공하고, 쿼리와 데이터 출력의 정확성을 보장합니다.\n\n![이미지](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_4.png)\n\nLLMs와 의미론적 계층을 결합하면 새로운 AI 주도 데이터 경험을 제공합니다. 이러한 계층은 AI 에이전트에 필수적인 컨텍스트를 제공하여 정확한 데이터 쿼리를 가능하게 하고, 조직이 사용자 정의 LLM 애플리케이션을 구축할 수 있도록 돕습니다. 데이터 웨어하우스 상단에 위치한 의미론적 계층은 AI 기술과의 원활한 통합을 촉진합니다.\n\n![이미지](/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_5.png)\n\n<div class=\"content-ad\"></div>\n\n의미론적 계층 데이터 모델은 LLM이 데이터를 이해하고 올바른 쿼리를 생성하기 위한 맥락으로 사용하는 구조와 정의를 제공합니다. 복잡한 조인 및 계산을 추상화하여, 의미론적 계층은 비즈니스 수준 용어를 기반으로 한 단순화된 인터페이스를 제공하여 오류를 줄이고 환각을 방지합니다.\n\n아르투르 키둔노프(Cube)는 모든 BI 도구 간 데이터 소비를 효율적으로 만들어 실수를 줄이고 일관된 데이터 원천과 신뢰를 유지하는 통합된 의미론적 계층의 중요성을 강조합니다. 모든 데이터 환경에 대한 메트릭 및 메타데이터를 정의하여 BI 소프트웨어부터 AI 도구에 이르기까지 다양한 플랫폼에서 데이터에 접근할 수 있도록 보장합니다. 이 다양성은 데이터 전달의 진화하는 풍경을 수용하고, 차세대 데이터 주도 애플리케이션을 지원합니다.\n\n일관된 데이터 없이는 AI도 불가능합니다: 고품질의 데이터는 AI에게 방대한 데이터셋으로부터 신뢰할 수 있는 통찰력을 제공할 수 있게 해줍니다. 통합된 의미론적 계층은 이 맥락에서 중요한 역할을 하며, 비즈니스 컨텍스트와 정의를 제공하여 AI 도구가 오류를 방지할 수 있습니다. AI는 비즈니스 사용자를 위해 자연어 쿼리를 통해 데이터 큐레이션과 민주화를 용이하게하는 정의 및 코드 개선을 제안하여 의미론적 계층을 향상시킬 수 있습니다.\n\n적용된 AI의 부상: AI의 보급은 대규모 플랫폼뿐만 아니라 도메인별 응용 프로그램으로도 확대되면서, 일관되고 정확한 데이터를 위해 통합된 의미론적 계층이 필요해졌습니다. 의미론적 계층은 입력이 정확하고 관련성이 있으며 일관되도록 보장합니다. 이는 AI 주도 경험에서 정확한 결과와 경쟁 우위를 위해 중요합니다.\n\n<div class=\"content-ad\"></div>\n\nAI에 준비된 범용 의미론적 층의 장점: AI에 준비된 범용 의미론적 층은 다양한 데이터 플랫폼을 연결하고, 데이터 민주화를 촉진하며 고객을 대상으로 하는 애플리케이션을 지원하는 데 필수적입니다.\n\nPatrk Liu Tran(Validio)은 의미론적 층이 AI 및 LLM과 혁신적인 데이터 경험을 창출하려는 기업들로부터 많은 관심을 받고 있다고 강조하고 있습니다. 중요한 목표 중 하나는 자연어 쿼리를 LLM에 가능하게 함으로써 데이터 검색을 간소화하고 분석가들을 하찮은 작업으로부터 해방시키는 것입니다.\n\n의미론적 층과 LLM을 통합하면 정확도가 최대 300%까지 개선됩니다. 메트릭스를 미리 정의하고 잘못된 가정의 위험을 줄임으로써 정확성이 높아집니다. 의미론적 층을 사용하면 LLM이 합의된 비즈니스 메트릭스에 따라 작동하여 정밀성을 향상시킵니다. LLM이 보다 보편화되면 데이터 중심적 기관들에게 의미론적 층의 중요성이 점점 더 분명해지고 있습니다. 이러한 접근법은 더욱 효율적이고 정확한 데이터 처리 및 분석을 약속하며, 결과적으로 더 나은 의사 결정 및 운영 효율성을 가능하게 합니다.\n\n# 미래 지향 및 신흥 트렌드\n\n<div class=\"content-ad\"></div>\n\n\nTomasz Tunguz(Theory Ventures)은 시맨틱 모델이 중요한 추세로 떠오를 것이라며, 조직간의 정의를 통일하여 사람의 이해와 대형 언어 모델의 의미 합성을 위한 단순화된 분석을 개선하고 재사용성과 구성 가능성을 향상시킬 것이라고 언급했습니다.\n\nTimeXTender에 따르면 시맨틱 레이어 기술은 지속적으로 발전하며 데이터 관리 전략에 영향을 미치고 다양한 산업에서 새로운 응용 가능성을 열어줄 것입니다.\n\nAtScale은 시맨틱 레이어가 계속해서 중요성을 갖게 될 것으로 보며, 조직 간의 정의를 통일하여 분석을 단순화하고 재사용성을 촉진하며 인간의 이해와 대규모 언어 모델의 의미 합성을 가능하게 합니다.\n\nGenAI와 같은 책임 있는 AI 도입은 투명성과 편향과 같은 윤리적 측면을 우선시합니다. AI 관리자와 같은 신생 직무들은 윤리적인 실행을 보장합니다. 기업은 점점 데이터를 제품으로 취급하며 맞춤형 비즈니스 결과를 위해 재사용 가능한 데이터 제품을 만들어냅니다. 생성적 AI와 대규모 언어 모델(LLMs)의 통합은 데이터 탐색을 변형시키며, MDX 생성은 쿼리 유연성을 높이고 자연 언어 인터페이스는 데이터 접근을 민주화합니다. 예측과 이상 탐지와 같은 AI 기능들은 의사 결정력을 강화하여 분석 접근성을 넓히고 통찰력을 심화시켜 혁신을 이끌어갈 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n의미적 레이어는 현대 데이터 관리의 다양한 도전에 대한 중심적인 해결책이 됩니다. 데이터의 통합된 관점을 제공하고 액세스를 간소화함으로써, 데이터의 제한된 가용성과 일관성 없는 보고 문제를 해결합니다. 또한 이러한 통합은 개선된 데이터 거버넌스를 촉진하고 AI 통합을 용이하게 하여 조직의 효율성과 유연성을 증가시킵니다. 의미적 레이어의 중요성은 데이터 분석을 혁신하고 AI 애플리케이션을 지원하여 보다 정보화된 의사 결정 과정을 가능케하는 변화력 있는 잠재력에 있습니다. 기업이 점점 데이터 중심적인 세계의 복잡성에 대처함에 따라, 의미적 레이어는 데이터 자원을 관리하고 활용하는 데 더 큰 유연성과 효과성을 제공하는 기반 요소입니다.\n\n# 참고문헌\n\nAtScale 및 Databricks를 활용한 의미적 레이크하우스 구축\n\n<div class=\"content-ad\"></div>\n\n**범용 의미 계층의 힘**\n\n두 세계를 하나로: 데이터 엔지니어와 비즈니스 애널리스트 결합하기\n\n의미 계층이란 무엇인가요?\n\n데이터 전략에서 의미 계층이 차지하는 위치\n\n<div class=\"content-ad\"></div>\n\n시맨틱 레이어가 데이터 거버넌스에 미치는 영향\n\n데이터 시각화에서 시맨틱 레이어 사용의 이점\n\n시맨틱 레이어에 대한 궁극적인 가이드\n\n유니버설 시맨틱 레이어로 현대 비즈니스 인텔리전스를 재구축하기\n\n<div class=\"content-ad\"></div>\n\n현대 데이터 스택의 의미론적 레이어\n\n의미론적 레이어란 무엇이며, 데이터를 지식으로 변환하는 방법은 무엇인가요?\n\n의미론적 레이어를 구현하는 세 가지 방법\n\n의미론적 레이어란 무엇인가요? (구성 요소 및 기업 애플리케이션)\n\n<div class=\"content-ad\"></div>\n\nAtScale은 AI, GenAI 모델을 위한 의미론적 레이어 지원 추가\n\n의미론적 레이어의 의미\n\n의미론적 레이어: AI 기반 데이터 경험의 중추\n\n의미론적 레이어는 AI 기반 분석을 위한 빠져있던 조각입니다\n\n<div class=\"content-ad\"></div>\n\n기업을 위한 생성 AI의 혜택과 의미론적 데이터 통합\n\n만약 범용 의미론적 레이어가 없다면 실시간 AI 경험을 진화시킬 수 없습니다\n\n의미론적 레이어 101: 데이터 팀이 데이터보다는 지표에 집중해야 하는 이유\n\n기업 분석과 생성 AI를 위한 2024 의미론적 레이어 혁신\n\n<div class=\"content-ad\"></div>\n\n현대 데이터 인프라를 위한 신흥 아키텍처","ogImage":{"url":"/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_0.png"},"coverImage":"/assets/img/2024-06-22-SemanticLayerOneLayertoServeThemAll_0.png","tag":["Tech"],"readingTime":20},{"title":"Rockset이 OpenAI에 인수되다 사용자들에게 어떤 의미가 있을까","description":"","date":"2024-06-22 17:03","slug":"2024-06-22-RocksetIsAcquiredbyOpenAIWhatDoesItMeanforItsUsers","content":"\n\n2024년 6월 21일, OpenAI가 데이터 색인 및 쿼리 기능으로 유명한 실시간 분석 데이터베이스인 Rockset을 인수했다고 발표했습니다. 이 인수는 Rockset 사용자들에게 중요한 변화를 알립니다. 사용자들은 이 플랫폼을 이탈해야 하는 제한된 시간을 가지고 있으며 다음 단계에 대해 궁금해하고 있습니다. 본 문서는 Rockset 사용자들이 이 전환을 안내하며, OpenAI가 왜 이러한 결정을 내렸는지, 즉각적으로 필요한 조치는 무엇인지, 그리고 Rockset 사용자들과 실시간 분석 요구 사항을 위한 이상적인 대안으로 어떤 솔루션이 적합한지에 대해 통찰을 제공할 것입니다.\n\n# OpenAI가 Rockset을 인수한 이유는 무엇인가요?\n\nOpenAI는 Rockset의 기술을 통합하여 제품 전반에 걸쳐 검색 인프라를 강화하려고 합니다. 이는 인공지능 업계에서 실시간 데이터 액세스와 처리의 중요성을 명확하게 보여 주는 지표입니다. 게다가 Rockset 인수를 통해 OpenAI는 실시간 분석 전문가 팀을 흡수하여 OpenAI의 능력을 계속 강화할 것입니다.\n\n# Rockset 사용자가 해야 할 첫 번째 일\n\n<div class=\"content-ad\"></div>\n\nRockset 사용자 분들에게 시한부가 닥쳐오고 있습니다. Rockset의 자세한 FAQ에 따르면, 계약 없는 월간 요금제 사용자들은 2024년 9월 30일까지 오프보딩을 진행해야합니다. 계약된 고객들은 Rockset 계정 팀과 협력하여 적절한 오프보딩 계획을 개발할 수 있지만, 모든 고객들은 빠르게 Rockset 대체안을 찾아야 합니다. 인수가 계획된 상황에서, 이제는 Rockset 사용자들이 다음 단계를 취해야 할 때입니다.\n\n![이미지](/assets/img/2024-06-22-RocksetIsAcquiredbyOpenAIWhatDoesItMeanforItsUsers_0.png)\n\nRockset 사용자들은 다음 단계를 시작해야 합니다:\n\n- 현재 사용량과 요구 사항을 평가하십시오: 솔루션을 평가하기 전에 무엇을 찾고 있는지 알아두는 것이 좋습니다. 이는 많은 시간을 절약할 수 있습니다.\n- 비슷하거나 더 나은 기능을 제공하는 대체 플랫폼 목록 작성 시작: 기존에 Rockset을 어떻게 사용했느냐에 따라 비즈니스의 요구 사항이 단순할 수도 복잡할 수도 있습니다. 각 플랫폼은 장단점이 있습니다. 비즈니스에 중요한 성능과 능력을 제공할 수 없는 솔루션을 평가하는데 소중한 시간을 낭비하지 않도록 어떤 플랫폼이 무엇을 할 수 있어야 하는지 알아두는 것이 중요합니다.\n- 작업 중단 없이 마이그레이션 과정을 계획하기 시작: 오픈 소스든 상용 솔루션이든, 솔루션과 함께 제공되는 지원이나 커뮤니티를 평가하는 것이 중요합니다. 성공적인 POC를 진행할 수 있는 옆에서 지원해줄 파트너를 찾거나 24시간 내내 문제 해결을 도와줄 활성 Slack 커뮤니티를 찾는 것은 마이그레이션이 원활히 진행되도록 하는 데 도움이 될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# Rockset 사용자를 위한 대안\n\nRockset 사용자가 다음 단계를 계획할 때, 각 합당한 대안을 탐색하는 것이 중요합니다. 사용 사례와 성능 요구에 따라, 다양한 플랫폼이 원하는 능력을 제공할 수 있습니다. 고려할만한 몇 가지 옵션은 다음과 같습니다:\n\n# 오픈 소스 실시간 분석 SQL 워크로드를 위한:\n\n- Apache Druid: Druid는 대규모 데이터에서 실시간 및 일괄 처리 쿼리를 하위 초 단위로 제공하는 고성능 실시간 분석 데이터베이스입니다.\n- ClickHouse: ClickHouse는 고속 오픈 소스 열 지향 데이터베이스 관리 시스템으로, SQL 쿼리를 사용하여 실시간으로 분석 데이터 보고서를 생성할 수 있습니다.\n- StarRocks: 확장 가능한 JOIN 쿼리를 실행하고 정규화 파이프라인 없이 실시간 분석을 제공하기에 적합합니다. StarRocks는 기본적으로 실시간 데이터 업서트를 지원하며, 열 지향 저장소에서 직접 가변 데이터로 두 번째 수준의 데이터 신선도를 제공할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 프로프리업 관리형 솔루션을 통한 실시간 분석 SQL 워크로드:\n\n- Imply: 엔터프라이즈 지원을 통한 클라우드 상의 관리형 Apache Druid.\n- CelerData: StarRocks 프로젝트의 주도 및 유지보수를 지원하는 클라우드 관리형 StarRocks.\n\n# 오픈 소스 벡터 검색(VectorDB)을 위한:\n\n- Weaviate: Weaviate는 객체 및 벡터를 저장하는 오픈 소스 벡터 데이터베이스로, 클라우드 네이티브 데이터베이스의 내결함성 및 확장성과 함께 벡터 검색을 구조화된 필터링과 결합할 수 있습니다.\n- Milvus: 차세대 AI 애플리케이션을 위한 클라우드 네이티브 벡터 데이터베이스 및 스토리지\n- Qdrant: 다음 세대 AI를 위한 고성능 대규모 벡터 데이터베이스.\n\n<div class=\"content-ad\"></div>\n\n# 관리되는 벡터 검색 (VectorDB):\n\n- SingleStore: SQL 능력 이외에도 SingleStore는 관리되는 벡터 검색 기능을 제공하여 양종의 워크로드에 대한 포괄적인 솔루션을 제공합니다.\n- Zilliz: Milvus 뒤의 회사인 Zilliz는 Milvus의 혜택을 추가 지원 및 유지 보수로 제공하는 관리되는 벡터 검색 서비스를 제공합니다.\n- Pinecone: 배포 및 벡터 검색 애플리케이션의 스케일링을 간소화하고 높은 가용성과 성능을 보장하는 완전히 관리되는 벡터 검색 플랫폼입니다.\n\n전환을 해야 하는 시긴데요. 중요한 인프라가 그대로 유지되고 작동되도록 해야 합니다. 이 플랫폼마다 고유한 장점이 있으며, 당신의 특정 요구 사항을 기반으로 평가하여 성공적인 마이그레이션이 이루어질 것입니다.\n\n# 왜 StarRocks가 Rockset 이주민들에게 가장 좋은 다음 단계인가요?\n\n<div class=\"content-ad\"></div>\n\n많은 Rockset 사용자들이 실시간 분석 요구를 위해 채택했습니다. 따라서 특히 현재 실시간 분야에서 한 명의 선도 업체 중 하나를 언급하는 것이 중요합니다: StarRocks. 실시간 분석을 위한 강력하고 효율적인 대안을 찾는 Rockset 사용자들에게 StarRocks는 매력적인 선택지를 제시합니다. 이유는 다음과 같습니다:\n\n- 확장 가능한 JOIN 쿼리: StarRocks를 사용하면 확장 가능한 JOIN 쿼리를 실행할 수 있으며 데노멀라이제이션 파이프라인이 필요없이 실시간 분석을 제공하여 데이터 처리를 간소화하고 성능을 향상시킵니다.\n- 실시간 데이터 업서트: Rockset에서 StarRocks로 전환할 때 데이터 신선도를 유지할 수 있습니다.\n- 우수한 성능: 컬럼형 스토리지, 벡터화 및 SIMD를 활용하여 StarRocks는 Rockset보다 우수한 성능을 달성하며 저장 공간의 일부만 사용하여 비용 효율적인 솔루션이 됩니다.\n- 오픈 소스 커뮤니티: Apache 라이선스를 받은 리눅스 재단 프로젝트인 StarRocks는 거대하고 성장 중인 글로벌 커뮤니티가 언제든지 도와줄 준비가 되어 있습니다.\n\n# 다음 단계\n\nOpenAI에 의한 Rockset 인수는 사용자들에게 도전과 기회를 제시합니다. 전환은 어렵게 느껴질 수 있지만, 우수한 성능과 확장 가능성을 제공하는 플랫폼으로 업그레이드할 기회이기도 합니다. StarRocks의 실시간 분석 성능에 대해 더 알아보고 이주하는 동안 지원을 받으려면 Slack 커뮤니티에 가입하세요.\n\n<div class=\"content-ad\"></div>\n\n마지막 순간까지 기다리지 말고 오늘 Rockset에서의 원활한 이전을 보장하려면 이전 계획을 시작하세요.","ogImage":{"url":"/assets/img/2024-06-22-RocksetIsAcquiredbyOpenAIWhatDoesItMeanforItsUsers_0.png"},"coverImage":"/assets/img/2024-06-22-RocksetIsAcquiredbyOpenAIWhatDoesItMeanforItsUsers_0.png","tag":["Tech"],"readingTime":4},{"title":"파이썬과 Streamlit으로 멀티페이지 금융 대시보드 만들기 처음부터 끝까지 완성하기","description":"","date":"2024-06-22 17:01","slug":"2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch","content":"\n\n<img src=\"/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_0.png\" />\n\n개인화된 작업 공간을 상상해보세요. 여기서는 상품, ETF, 과소평가된 주식, 그리고 변동성 있는 암호화폐 시장을 각각 별도의 탭에서 동시에 모니터링할 수 있습니다. 이러한 설정은 데이터를 손쉽게 접근할 수 있는 것 이상을 제공합니다. 표준 도구들이 제공하는 유연성 부족으로 자주 허용되지 않는 방식으로 이 데이터를 통합하고 상호작용할 수 있게됩니다. 여러분의 대시보드를 구축함으로써, 관련성 있는 지표를 강조함으로써 유연성을 얻을 수 있고, 사용자 정의 필터를 적용하고, 심지어 시장 변동에 실시간으로 반응하는 고급 분석 도구를 통합할 수 있습니다.\n\n투자 결정에 중요한 고유한 지표들을 간과할 수 있는 범용 인터페이스에 의존할 필요가 없습니다. 여러분의 대시보드를 생성함으로써, 데이터 분석 과정을 통제하고 시장에 대한 더 깊은 이해를 기르는 전략적인 과정이 됩니다. 이제 이 혁신적인 도구를 만드는 것을 시작해봅시다.\n\n데이터 작업에 파이썬보다 더 나은 것이 무엇인가요? 그리고 데이터를 분석하는 데 가장 사용자 친화적인 방법이 무엇인가요? 우리는 파이썬에 대해 더 많은 이해가 있고, Streamlit을 소개할게요. 이 Streamlit은 데이터 분석을 위한 대화형 웹 애플리케이션을 빠르고 쉽게 구축할 수 있는 강력한 오픈 소스 파이썬 라이브러리입니다.\n\n<div class=\"content-ad\"></div>\n\n모든 난이도의 개발자를 대상으로 설계된 Streamlit은 데이터 스크립트를 공유 가능한 웹 앱으로 쉽게 변환할 수 있도록 도와줍니다.\n\n# Streamlit을 특별하게 만드는 요소는 무엇인가요?\n\n1. 사용 용이성: Streamlit의 매력은 그 간단함에 있습니다. 직관적인 Python 코드로 앱을 작성하므로 HTML, CSS 또는 JavaScript와 같은 복잡한 웹 기술을 알 필요가 없습니다. 이 사용자 친화적인 방식은 웹 프로그래밍 경험이 부족한 데이터 과학자와 분석가들에게 앱 개발의 가능성을 열어줍니다.\n\n2. 빠른 프로토타이핑: Streamlit을 사용하면 코드를 수정하면 해당 변경 사항이 앱 인터페이스에 자동으로 업데이트되어 이터레이션 과정이 매우 신속해집니다. 이 기능을 이용하면 앱을 동적으로 조정하고 실시간으로 결과를 확인할 수 있어 빠른 프로토타이핑과 실험에 매우 유용합니다.\n\n<div class=\"content-ad\"></div>\n\n3. 다양한 내장 위젯: Streamlit에는 슬라이더, 체크박스, 드롭다운과 같은 다양한 내장 위젯이 포함되어 있어 데이터와 상호 작용하기가 매우 쉽습니다. 이러한 요소들은 코드를 최소한으로 사용하여 추가할 수 있어 사용자가 표시된 데이터나 수행 중인 계산을 조작할 수 있게 해줍니다.\n\n4. 데이터 시각화 지원: Streamlit은 Matplotlib, Seaborn, Plotly와 같은 주요 Python 데이터 시각화 라이브러리와 완벽하게 통합되어 있습니다. 이 통합을 통해 차트, 지도 및 그래프를 쉽게 앱에 통합하여 데이터를 더욱 매력적이고 정보를 제공하는 방식으로 시각화할 수 있습니다.\n\n자, 그럼 더 이상의 말이 필요 없죠. 이를 만들어 봅시다. 먼저, 필요한 것을 이해해봅시다: 사용자로서, 주요 재정 데이터를 한 곳에 표시하고 4가지 관심 영역(Crypto, ETF, 주식, 상품) 간에 쉽게 탐색할 수 있기를 원합니다. 아래는 이 모든 세그먼트에 대한 요구 사항입니다.\n\n- Cryptos: 시가 총액을 기준으로 상위 500개의 가상 화폐에 대한 실시간 가격 및 이전 종가, 시장 변동 폭, 7일 변동 사항을 표시합니다.\n- ETFs: 펀드의 성과 지표: 현재 가격, 52주 최고 및 최저가, 연 별 ETF 수익률(%), 배당 수익률 및 ETF의 실제 옵션에 대한 정보입니다.\n- Stocks: 주식의 현재 가격을 수집하고 주요 지표인 주가수익(P/E) 비율, 주당순이익(EPS)을 함께 제시합니다. 그것을 기반으로 설정한 임계값과 함께, 주식의 공정 가치를 계산하여 현재 주식 가치와 공정 주식 가치 간의 차이를 보여줍니다(차이가 충분히 크다면, 아마도 가격이 저평가된 주식일 것입니다).\n- Commodities: 현재 시장 가격, 과거 추이 (그래프), 판매 단위를 표시합니다(주식과는 다르게 각 상품에는 다른 측정 단위가 있으며, 이전 종가로부터의 가격 변동).\n\n<div class=\"content-ad\"></div>\n\n\"알았다고 했잖아!\", 버니 맥의 캐릭터가 \"오션스 13”에서 강조했던 것처럼. 코드 작업을 시작해봐요.\n\n새로운 파이썬 프로젝트를 만들고 중요한 몇 가지 파일을 포함해보세요: ETF 목록과 코드로 분석할 주식 목록이 필요할 거에요. 두 파일 모두 준비해 놨어요 (그동안 가지고 있었고, 관심이 있는 분들께 DM을 통해 공유할 수 있어요).\n\n# Streamlit 환경 설정하기\n\nStreamlit과 데이터 처리 및 시각화를 위해 pandas와 matplotlib/plotly와 같은 필요한 라이브러리를 설치해보세요.\n\n<div class=\"content-ad\"></div>\n\n\n```js\npip install streamlit pandas matplotlib plotly\n```\n\n메인 스크립트(streamlit_app.py)를 만들고 각 대시보드 패널을 위한 개별 스크립트를 작성하세요. 앱을 구성하기 위해 Streamlit의 레이아웃 기능을 활용해주세요:\n\n- 주요 네비게이션: st.sidebar.radio 또는 st.sidebar.selectbox를 사용하여 다른 금융 세그먼트 간에 탐색할 수 있게 사이드바를 활용하세요.\n- 대시보드 콘텐츠: 암호화폐, ETF, 주식 및 상품 모듈에 있는 각 페이지 함수(app())에서 데이터를 표시하는 데 필요한 표, 차트 및 상호 작용 위젯을 설정해주세요.\n\n우리의 streamlit_app.py 파일은 프로젝트의 루트 폴더에 있어야 합니다. 또한 대시보드 페이지가 있는 pages 폴더가 필요합니다 (각 대시보드마다 한 페이지씩).\n\n\n<div class=\"content-ad\"></div>\n\n\n## 내 Streamlit 어플리케이션 폴더 구조:\n- **pages/**: 대시보드의 여러 페이지를 위한 디렉토리\n    - **__init__.py**: 'pages'를 파이썬 패키지로 만듦\n    - **commodities.py**: 상품 대시보드 모듈\n    - **cryptos.py**: 암호화폐 대시보드 모듈\n    - **etfs_value.py**: ETF 대시보드 모듈\n    - **underpriced_stocks.py**: 저평가 주식 대시보드 모듈\n- **stramlit_app.py**: 주 Streamlit 어플리케이션 파일\n\n\n주 Streamlit 어플리케이션 코드부터 시작해봅시다. (특정 대시보드 페이지에 대한 중요 부분은 이미 이전 게시물 몇 개에서 살펴봤습니다.)\n\n```python\nimport streamlit as st\nfrom pages import commodities, cryptos, etfs_value, underpriced_stocks\n\n# 페이지 딕셔너리\npages = {\n    \"상품\": commodities,\n    \"암호화폐\": cryptos,\n    \"ETFs 가치\": etfs_value,\n    \"저평가 주식\": underpriced_stocks\n}\n\nst.sidebar.title('Navigation')\nchoice = st.sidebar.radio(\"페이지 선택:\", list(pages.keys()))\n\npage = pages[choice]\npage.app()  # 각 모듈이 페이지를 실행하기 위한 app 함수를 갖고 있다고 가정\n```\n\n앱 구조를 보면 매우 간단합니다. 필요한 대시보드 페이지와 Streamlit 라이브러리를 import하고, 페이지 딕셔너리를 작성하고, 사이드바 제목을 설정하고, 선택 메커니즘을 구현합니다. 이를 통해 사용자가 앱의 다른 섹션으로 이동할 수 있도록 합니다. 마지막으로 page.app() 메서드를 호출하여 사용자의 선택에 따라 적절한 페이지를 동적으로 로드합니다. 이 방법은 각 페이지 모듈 내의 특정 기능과 연결되어 대시보드를 렌더링하는 데 사용됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n작은 이정표를 달성했어요: 더 큰 목표를 향해 나아가요.\n대시보드 페이지 만들기\n\npages/commodities.py\n\n```js\nimport streamlit as st\nimport yfinance as yf\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 상품, 측정 단위 및 이름 정의\ncommodities_info = {\n    \"CL=F\": {\"unit\": \"배럴\", \"name\": \"원유 (WTI)\"},\n    \"BZ=F\": {\"unit\": \"배럴\", \"name\": \"브렌트 원유\"},\n    \"NG=F\": {\"unit\": \"mmBtu\", \"name\": \"천연가스\"},\n    \"HO=F\": {\"unit\": \"갤런\", \"name\": \"난방유\"},\n    // (중략)\n}\n\n@st.cache  # 데이터 캐싱을 통해 과도한 API 호출을 방지합니다\ndef fetch_commodity_data(tickers, period=\"6d\", interval=\"1d\"):\n    try:\n        data = yf.download(tickers, period=period, interval=interval)\n        return data\n    except Exception as e:\n        st.error(f\"상품 데이터 검색에 실패했습니다: {str(e)}\")\n        return pd.DataFrame()  # 실패 시 빈 DataFrame 반환\n\ndef app():\n    st.title(\"상품 대시보드\")\n\n    // (중략)\n\nif __name__ == \"__main__\":\n    app()\r\n```\n\n이 Streamlit 앱은 선택한 기간 동안 상품 가격과 변동을 보여주는 대시보드를 표시하는 데 사용됩니다. 사용자들은 특정 상품, 시간대 및 데이터 세부 사항을 기반으로 사용자 정의로 표시를 조정할 수 있습니다. 특정 라이브러리를 가져온 후, commodities_info라는 딕셔너리를 만들었습니다. 이 딕셔너리는 관심 상품을 정의하며, 해당 시장 티커 심볼, 측정 단위 및 이름을 포함합니다. 이 딕셔너리는 특정 상품을 티커 심볼로 참조하면서 사용자 친화적인 이름과 단위를 표시하는 데 앱 전반에서 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n다음 부분은 매우 중요합니다: @st.cache 데코레이터를 사용하여 함수 호출 결과를 캐싱하는 중요한 임무를 수행하고 있습니다. 이를 통해 입력을 기반으로 함수 호출 결과를 캐싱함으로써 yfinance에 대한 API 호출 수를 줄여 대역폭을 절약할 뿐만 아니라 첫 로드 후 사용자 상호작용 속도도 높일 수 있습니다.\n\n- tickers: 상품 기호 목록.\n- period: 데이터를 가져올 시간 기간을 지정하는 문자열 (기본값은 \"6d\" 또는 6일). 대시보드에서 사용자가 더 많은 내용을 볼 수 있도록 변경할 수 있습니다.\n- interval: 데이터 포인트의 정밀도 (기본값은 \"1d\" 또는 매일). 대시보드에서도 사용자 정의가 가능합니다.\n\n## 앱 기능\n\n이 함수는 주요 응용 프로그램 인터페이스를 정의합니다:\n\n<div class=\"content-ad\"></div>\n\n- st.title(\"상품 대시보드\"): 대시보드의 제목을 설정합니다.\n- 사이드바 입력란을 통해 사용자는 데이터의 기간과 간격을 선택하고 어떤 상품을 표시할지 선택할 수 있습니다.\n\n## 데이터 로드 및 표시\n\n상품이 선택된 경우, 앱은 fetch_commodity_data 함수를 사용하여 데이터를 검색합니다. 성공적인 데이터 검색은 최신 및 이전 종가를 사용하여 변동률을 계산하는 데이터 처리를 트리거합니다. 이 데이터는 그런 다음 데이터 프레임에 표시됩니다. 그 후에는 시각적인 그래픽을 위한 플로팅 함수를 정의하고 있습니다 (이 대시보드에서 그래프와 플롯만 사용 가능합니다. 다른 대시보드의 경우 주식/ETF/암호화폐의 수가 100보다 많아 시스템에 불필요한 부하가 될 수 있습니다).\n\n## 실행 시작점\n\n<div class=\"content-ad\"></div>\n\n```js\nif __name__ == \"__main()\":\n    app()\n```\n\n이 줄은 스크립트가 직접 실행되었는지 확인한 후 streamlit 애플리케이션을 시작하는 app() 함수를 호출합니다.\n\n축하드립니다! 네가 만든 대시보드 페이지 중 첫 번째를 성공적으로 만들었어요!\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_1.png)\n\n<div class=\"content-ad\"></div>\n\n앗, 분할 취소를 요청해주셨군요. 물론이죠, 질문이 있으시면 언제든지 물어보세요! 😉\n\n<div class=\"content-ad\"></div>\n\nMarkdown 형식으로 표를 나타냅니다.\n\n파일 pages/underpriced_stocks.py\n\n```js\nimport streamlit as st\nimport yfinance as yf\nimport pandas as pd\nimport requests\n\n# API 액세스를 위한 상수\nAPI_KEY = 'Your API Key'\nBASE_URL = 'https://financialmodelingprep.com/api/v3'\n\n@st.cache_resource\ndef fetch_sp500_tickers():\n    \"\"\"\n    API를 사용하여 현재 S&P 500 소속 티커를 가져옵니다.\n    \"\"\"\n    url = f\"{BASE_URL}/sp500_constituent?apikey={API_KEY}\"\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            data = response.json()\n            tickers = [item['symbol'] for item in data]\n            return tickers\n        else:\n            st.error(f\"티커를 가져오지 못했습니다: HTTP 상태 코드 {response.status_code}\")\n            return []\n    except Exception as e:\n        st.error(f\"요청 실패: {e}\")\n        return []\n\n@st.cache\ndef fetch_stock_data(tickers):\n    \"\"\"\n    주어진 티커에 대한 주식 데이터를 가져와 주식이 저평가되었는지 계산합니다.\n    \"\"\"\n    data = []\n    for symbol in tickers:\n        stock = yf.Ticker(symbol)\n        try:\n            info = stock.info\n            if 'currentPrice' in info and 'trailingEps' in info:\n                current_price = info['currentPrice']\n                eps = info['trailingEps']\n                pe_ratio = info.get('trailingPE', float('inf'))  # 사용 가능한 경우 trailing P/E 사용\n\n                # 목표 P/E 비율 가정\n                target_pe = 15\n                fair_value = eps * target_pe\n\n                underpriced = current_price < fair_value\n                price_gap = ((fair_value - current_price) / current_price) * 100 if current_price else 0\n\n                data.append({\n                    'Symbol': symbol,\n                    'Current Price': current_price,\n                    'EPS': eps,\n                    'Fair Market Value': fair_value,\n                    'Underpriced': '네' if underpriced else '아니요',\n                    'Price Gap (%)': round(price_gap, 2)\n                })\n        except Exception as e:\n            print(f\"{symbol}에 대한 데이터를 가져오지 못했습니다: {e}\")\n\n    return pd.DataFrame(data)\n\ndef app():\n    \"\"\"\n    S&P 500 주식 및 저평가 상태를 표시하는 Streamlit 앱입니다.\n    \"\"\"\n    st.title(\"S&P 500 주식 분석\")\n\n    tickers = fetch_sp500_tickers()\n\n    if tickers:\n        st.write(\"S&P 500 회사의 티커가 로드되었습니다.\")\n        df = fetch_stock_data(tickers)\n        if not df.empty:\n            st.dataframe(df)\n        else:\n            st.write(\"제공된 티커에 대한 데이터를 찾을 수 없습니다.\")\n    else:\n        st.write(\"주식 티커를 로드할 수 없습니다. API 설정 및 네트워크 연결을 확인해주세요.\")\n\nif __name__ == \"__main__\":\n    app()\n\n```\n\n암호화폐처럼 동일한 방식으로: 필요한 라이브러리 가져오기, financialmodelingprep 라이브러리에서 SP500 티커 가져오기 및 yfinance에서 데이터 가져오기: 각 주식별로 데이터를 가져오는 동안 limitation에 도달하는 것을 피하기 위해 이 두 작업을 서로 다른 소스 사이에 분리했습니다. financialmodelingprep의 최소 결제 요금제(매월 19.99 미국 달러)에는 분당 300회의 호출 제한이 있으므로 우리가 주식을 하나씩 가져올 때 쉽게 이 제한에 도달할 것입니다.\n\n<div class=\"content-ad\"></div>\n\nyfinance에서 무엇을 얻고 있습니까? 적절한 기준을 설정하고 특정 주식이 성장 잠재력이 있는지 고려하는 데 도움이 되는 여러 가지 지표 목록을 얻고 있습니다.\n현재 가격: 주식의 최신 거래 가격입니다.\nEPS (주당 수익): 회사가 주당 주식에 대해 벌어들이는 돈을 나타냅니다.\n목표 P/E 비율: 이는 많은 가치 투자자들을 위한 전형적인 기준인 15로 설정됩니다. 여기서는 해당 주식의 이익에 기초하여 합리적인 가격으로 간주될 수 있는 것을 예상하기 위해 사용됩니다. 목표 P/E 15는 성장과 가치 속성을 균형 있게 고려할 수 있는 중도 기준으로 선택되었습니다. 브로드 산업 범위에 역사적으로 적용되었던 산업에 대해 사용된 보수적인 수치로, 오버밸류된 시장에서 상대적 가치 평가가 낮은 주식을 식별하는 데 도움을 줄 수 있습니다.\n공정시장가치 계산: EPS * 목표 P/E로 계산됩니다. 이는 주식이 목표 P/E 비율인 15로 가치 평가되었다면 해당 주식의 공정 가치를 나타냅니다. 낮은 P/E는 주식이 수익에 비해 저평가되었을 수 있다는 것을 시사할 수 있습니다.\n저평가 여부 확인: 만일 현재 시장 가격이 계산된 공정시장가치보다 낮다면 해당 주식이 저평가되었다고 간주됩니다.\n가격 격차(%): 공정시장가치와 현재 가격 사이의 백분율 차이를 보여주며, 주식 가격이 추정된 공정 가치에 도달하기 위해 얼마나 증가해야 하는지를 나타냅니다.\n\n![Financial Dashboard](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_3.png)\n\n우리는 4개 중 3개를 얻었습니다: 마지막은 ETF 분석 대시보드입니다.\n\n파일: pages/etfs_value.py\n\n<div class=\"content-ad\"></div>\n\n```python\nimport streamlit as st\nimport yfinance as yf\nimport pandas as pd\n\nst.set_page_config(layout=\"wide\")\n\n@st.cache_resource(ttl=300, show_spinner=True)\ndef fetch_options_data(symbol):\n    \"\"\" Yahoo Finance에서 ETF 심볼의 옵션 데이터를 가져옵니다. \"\"\"\n    etf = yf.Ticker(symbol)\n    try:\n        expiration_dates = etf.options\n        options_info = []\n        for expiration_date in expiration_dates:\n            options_chain = etf.option_chain(expiration_date)\n            puts = options_chain.puts\n            calls = options_chain.calls\n            options_info.append({\n                '만기일': expiration_date,\n                '풋 옵션 개수': len(puts),\n                '콜 옵션 개수': len(calls)\n            })\n        return options_info\n    except Exception as e:\n        st.error(f\"{symbol}에 대한 옵션 데이터를 가져올 수 없습니다: {e}\")\n        return []\n\ndef format_assets(assets):\n    \"\"\" 큰 숫자를 읽기 쉬운 형식으로 변환합니다. \"\"\"\n    if assets >= 1e9:\n        return f\"{assets / 1e9:.2f}B\"\n    elif assets >= 1e6:\n        return f\"{assets / 1e6:.2f}M\"\n    return str(assets)\n\n@st.cache_data(show_spinner=True)\ndef fetch_data(symbol):\n    \"\"\" Yahoo Finance에서 ETF에 대한 금융 데이터 및 메트릭을 가져옵니다. \"\"\"\n    etf = yf.Ticker(symbol)\n    info = etf.info\n    options_info = fetch_options_data(symbol)\n    return {\n        '이름': info.get('longName', 'N/A'),\n        '최신 가격': f\"${info.get('previousClose', 'N/A')}\",\n        '52주 최고가': f\"${info.get('fiftyTwoWeekHigh', 'N/A')}\",\n        '52주 최저가': f\"${info.get('fiftyTwoWeekLow', 'N/A')}\",\n        '1년 수익률': f\"{info.get('ytdReturn', 'N/A') * 100:.2f}%\" if info.get('ytdReturn') is not None else \"N/A\",\n        '3년 수익률': f\"{info.get('threeYearAverageReturn', 'N/A') * 100:.2f}%\" if info.get('threeYearAverageReturn') is not None else \"N/A\",\n        '5년 수익률': f\"{info.get('fiveYearAverageReturn', 'N/A') * 100:.2f}%\" if info.get('fiveYearAverageReturn') is not None else \"N/A\",\n        '총 자산': format_assets(info.get('totalAssets', 'N/A')),\n        '배당 수익률': f\"{info.get('yield', 'N/A') * 100:.2f}%\" if info.get('yield') is not None else \"N/A\",\n        '평균 거래량': info.get('averageVolume', 'N/A'),\n        '옵션 상세정보': \"; \".join([f\"만기일: {opt['만기일']}, 풋: {opt['풋 옵션 개수']}, 콜: {opt['콜 옵션 개수']}\" for opt in options_info]),\n    }\n\ndef app():\n    \"\"\" ETF 분석을 표시하는 Streamlit 애플리케이션의 진입점입니다. \"\"\"\n    st.title(\"ETF 분석\")\n    refresh_button = st.button(\"데이터 새로고침\")\n\n    if refresh_button:\n        st.experimental_rerun()\n\n    file_path = \"etfs.txt\"\n    try:\n        with open(file_path, 'r') as file:\n            symbols = [line.strip().upper() for line in file.readlines()]\n            data = [fetch_data(symbol) for symbol in symbols]\n            df = pd.DataFrame(data)\n            st.table(df)\n    except FileNotFoundError:\n        st.error(\"ETF 심볼 파일을 찾을 수 없습니다. 현재 디렉토리에 'etfs.txt' 파일이 있는지 확인해 주세요.\")\n\nif __name__ == \"__main__\":\n    app()\r\n```\n\n<div class=\"content-ad\"></div>\n\n데이터를 가져와서 서식을 지정한 후 Streamlit의 st.table() 함수를 사용하여 각 ETF의 주요 지표를 명확하고 조직적으로 보여줍니다. 이 테이블에는 최신 가격, 올해의 최고가와 최저가, 수익률, 총 자산, 배당 수익률 및 자세한 옵션 데이터와 같은 세부 정보가 포함되어 있습니다.\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_4.png)\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_5.png)\n\nETF의 사용 가능한 옵션 수가 다르기 때문에 테이블의 높이와 가시성에 영향을 줍니다. 그래서 이 대시보드의 스크린샷을 2개 두었습니다.\n\n<div class=\"content-ad\"></div>\n\n그래요, 우리 최종 대시보드가 준비되었어요. 터미널을 열고 마법의 열쇠를 입력해볼까요? \"알라딘의 비밀 금고 여는 방법\"이 아니라 이런 모습을 하겠죠.\n\n```js\nstreamlit run streamlit_app.py\n```\n\n그리고, 와! 대시보드가 실행 중이에요.\n\n당신의 IDE 터미널에서 대시보드에 액세스하는 URL을 확인할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_6.png)\n\n로컬 URL 링크를 클릭하세요. 기본 브라우저에서 페이지를 열고 성취 결과를 확인할 수 있을 겁니다.\n\n![이미지](/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_7.png)\n\n테이블에 대해 \"csv로 다운로드\", 검색 및 전체화면 옵션이 제공되었는지 확인하세요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_8.png\" />\n\n요약하자면 — 우리는 큰 한걸음을 내디디었어요: 어떤 데이터 표시 소스의 UI 한계에 국한되어 사용자일 뿐이었던 것으로부터, 이제 우리는 직접 대시보드를 개발할 수 있게 되었어요: 신뢰할 만한 정보 소스를 찾아내고, 원하는 형태로 정보를 제공받을 수 있도록 결정하고, 그에 맞게 조작할 수 있게 되었죠. 너무 복잡하지 않죠, 아마도 자기 계발과 금융 교육 여정에서 다음 목표에 도달하기 위한 단계에 또 다른 발걸음인 것 같아요. 코딩에 행운을 빕니다!\n\n# 쉽게 말해보면 🚀\n\nIn Plain English 커뮤니티의 일원이 되어 주셔서 감사합니다! 떠나시기 전에:\n\n<div class=\"content-ad\"></div>\n\n- 글쓴이를 클립하고 팔로우 해주세요! 👏️️\n- 팔로우하기: X | LinkedIn | YouTube | Discord | 뉴스레터\n- 다른 플랫폼 방문하기: Stackademic | CoFeed | Venture | Cubed\n- PlainEnglish.io 에서 더 많은 콘텐츠를 확인하세요!","ogImage":{"url":"/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_0.png"},"coverImage":"/assets/img/2024-06-22-Multi-pagefinancialdashboardwithPythonandStreamlitBuilditfromscratch_0.png","tag":["Tech"],"readingTime":15},{"title":"Z-점수를 이용해 연령대별 달리기 성능을 비교할 수 있을까요","description":"","date":"2024-06-22 16:58","slug":"2024-06-22-CanWeUseZ-ScorestoCompareRunningPerformancesBetweenAgeGroups","content":"\n\n![image](/assets/img/2024-06-22-CanWeUseZ-ScorestoCompareRunningPerformancesBetweenAgeGroups_0.png)\n\n다른 연령대 간 경주 결과를 효과적으로 비교하려면 어떻게 해야 할까요?\n\n그것이 제가 계속 탐구하고 있는 질문입니다.\n\n우리는 나이가 들수록 느리게 움직이게 됩니다. 어떤 사람들에게는 더 큰 영향을 미치지만 60세 남성이 25세 남성과 레벨한 경쟁을 할 수 없다는 것이 결론입니다.\n\n<div class=\"content-ad\"></div>\n\n마스터 러너들을 위해 흥미로운 것을 유지하고 포용적인 러닝 커뮤니티를 유지하기 위해 많은 노력이 기울여졌습니다. 그것을 가능하게 하는 시스템인 연령 등급화라는 것을 개발하는 데 많은 노력이 기울여졌습니다. 최근 몇 주 동안, 몇 가지 대안을 탐색하기 위해 데이터를 사용해보았습니다.\n\n이전에 백분위수를 다른 비교 방법으로 제안했습니다. 이 주제에 대한 후속 기사는 여기에서 읽을 수 있습니다.\n\n잠재력이 있는 부분도 있지만 일부 단점도 있습니다. 그 주제로 나중에 돌아가서 그 중 몇 가지를 해결하고 상황을 개선해보겠습니다.\n\n그러나 오늘은 또 다른 비교 가능한 방법, 즉 Z-점수에 집중하고 싶습니다.\n\n<div class=\"content-ad\"></div>\n\n이 통계 도구가 다른 종류의 러너들의 경주 결과를 효과적으로 비교하는 데 도움이 될까요? 한번 살펴보고 알아봅시다.\n\n# Z-스코어란 무엇인가요?\n\n먼저, 기본적인 통계를 살펴봅시다. 개별 경주 결과의 z-스코어를 계산하기 전에 몇 가지 중요한 개념을 알 필요가 있습니다.\n\n샘플 경주 결과가 있다고 가정해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n런너를 '전형적'이거나 '평균적'이라고 표현하는 다양한 방법이 있습니다. 우리는 평균을 사용할 것입니다. 평균을 계산하려면 모든 결과를 더한 다음 결과의 개수로 나누면 됩니다.\n\n우리 경우, 35세 미만의 남성들의 360,075개의 경주 결과를 포함한 샘플이 있습니다. 만약 수학을 해본다면 (저는 Python의 Pandas 패키지를 사용하여 컴퓨터에 일을 시켰습니다),이 그룹의 평균 완주 시간은 4:16:36입니다 (나중에 샘플에 대해 더 언급하겠습니다).\n\n그러나 이것은 이야기의 일부에 불과합니다. 많은 러너가 4:16 정도의 완주 시간을 기록하지만, 이 샘플에서 가장 빠른 러너는 2:03:45로 완주했습니다. 다른 사람들은 6, 7 또는 8시간이 걸렸습니다.\n\n분산과 편차라는 다른 기본 개념이 있습니다. 개별 결과가 평균에서 얼마나 벗어나 있는가? 결과들은 밀접하게 뭉쳐 있거나 퍼져 있나요?\n\n<div class=\"content-ad\"></div>\n\n수학적인 내용에는 들어가지 않고, 우리는 판다스를 사용하여 이 샘플의 표준 편차를 계산할 수 있습니다: 54:07. 이는 주어진 경주 결과와 평균 완주 시간 사이의 평균 거리를 나타냅니다.\n\nZ-점수는 특정 결과가 평균으로부터 얼마나 떨어져 있는지를 이해하는 표준화된 방법입니다 — 해당 샘플의 표준 편차를 기반으로 합니다.\n\n만약 한 러너가 평균(3:22:29)보다 54:07 빨리 완주했다면, 해당 결과의 z-점수는 -1일 것입니다 — 평균보다 한 표준 편차 떨어진 값입니다. 만약 한 러너가 두 배 빠르게(2:28:22) 완주했다면, 그것은 평균보다 두 배의 표준 편차 떨어진 값일 것입니다.\n\n위의 시각화는 이 개념을 설명해 줍니다.\n\n<div class=\"content-ad\"></div>\n\n히스토그램은 360,075개의 개별 레이스 결과를 나타내며 각 5분 간격에 속하는 결과의 백분율을 보여줍니다 (즉, 3:55에서 4:00 사이).\n\n점선은 평균 값이 있는 곳 뿐만 아니라 평균 값의 한 표준 편차 위 및 아래, 그리고 두 표준 편차 위 및 아래를 나타냅니다.\n\n평균 값으로부터 멀어질수록 해당 시간을 뛰어넘은 참가자가 줄어든다는 것을 알 수 있습니다.\n\n이 개념을 다른 샘플, 예를 들어 다른 연령대에 적용해보면 실제 평균 값과 표준 편차가 달라질 수 있지만, 일반적인 원칙은 유지될 것입니다. 평균 값에서 두 표준 편차 떨어진 곳에서 레이스를 마친 선수는 한 표준 편차 떨어진 곳에서 레이스를 마친 선수보다 훨씬 적을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이것은 잠재적으로 개별 결과가 평균 이하 얼마나 떨어져 있는지를 측정하여 \"좋은\" 결과를 비교하는 방법을 제공할 수 있습니다.\n\n# 각 연령 그룹의 평균과 표준 편차 계산\n\n첫 번째 단계는 많은 데이터를 수집하는 것입니다 — 이미 수행했습니다. 여기서 더 많은 내용을 읽을 수 있습니다. 하지만, 여기에 짧게 설명하겠습니다.\n\n이 분석을 용이하게 하기 위해, 샘플로 사용할 일련의 경주를 식별했습니다. 이는 2010년부터 2019년까지 미국에서 9월, 10월 또는 11월에 개최된 500명 이상의 완주자가 있는 모든 마라톤을 포함합니다. 그런 다음, 각 완주자의 성별, 연령 및 완주 시간을 수집했는데 — 총 2,017,493개의 결과가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n몇 가지 정리와 준비 작업을 한 후, 이 결과를 Pandas 데이터프레임에 로드하여 쉬운 분석이 가능하게 했어요. 이 시리즈를 마치면 Kaggle에 완전한 데이터셋을 공유할 예정이에요. 만약 여러분이 자체적으로 분석을 하고 싶다면 이 데이터셋을 사용해보세요.\n\nPandas로 결과를 처리하면 성별 및 연령대로 결과를 그룹화하고 각 그룹의 평균과 표준 편차를 계산하는 것이 상당히 간단해집니다. 참고로, 나는 Boston Marathon 참가 자격을 얻기 위해 BAA가 사용하는 연령 구간을 사용하고 있어요. 그리고 샘플이 너무 작기 때문에 80세 이상의 러너는 포함하지 않았어요.\n\n위의 시각화는 각 연령대별 평균 완주 시간을 나타냅니다. 빨간 점은 남성이고, 초록 점은 여성입니다.\n\n여기서 놀라운 점은 없어요. 오른쪽으로 이동할수록 평균 완주 시간이 더 느려집니다. 어린 러너들 사이에는 큰 차이가 없지만, 50대와 60대로 갈수록 그 차이가 뚜렷해집니다.\n\n<div class=\"content-ad\"></div>\n\n평균적으로, 동일 연령대의 남성보다 여성이 느린 속도로 완주합니다. 그러나 연령에 따른 추세는 여성과 남성 모두 유사합니다.\n\n위의 시각화 자료는 각 그룹의 표준 편차를 보여줍니다.\n\n이건 정확히 내가 예상한 것과는 조금 다르네요 — 그리고 이는 문제가 될 수도 있습니다.\n\n각 그룹 사이의 표준 편차는 거의 비슷합니다. 노인 남성을 제외하고, 모든 그룹의 표준 편차는 50분에서 55분 사이에 있습니다. 세 가지 노인 남성 연령대 중에서는 약간 더 높습니다(55분에서 60분까지).\n\n<div class=\"content-ad\"></div>\n\n저는 정확히 무엇을 기대했는지 확신할 수 없지만, 이것이 어떤 방식으로든 평균과 함께 조정될 것으로 생각했습니다. 아래에서 어떻게 진행되는지 확인해볼게요. 그러나 한 그룹이 다른 그룹보다 더 극단적인 z-점수를 가지거나 다른 그룹과 비교할 때 과대평가될 수도 있다는 느낌이 듭니다.\n\n# 개별 결과에 정규화된 Z-점수 적용하기\n\n각 그룹에 대한 평균과 표준 편차를 계산한 후, 결과의 일부(2019)를 취하여 각 결과에 대한 z-점수를 계산했습니다.\n\n실제 실습 중에 어떻게 작용하는지 몇 가지 레이스를 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 미네아폴리스의 트윈시티 마라톤을 시작으로 하겠습니다. 여기 주요 10명의 완주자들을 z-점수로 정리해 봤어요. 비교를 위해 2020년 연령 등급 표에 따른 연령 등급 점수도 포함했어요.\n\n```js\n| 성별    | 나이  | 완주 시간 | z-점수 | 연령 등급 점수 |\n|---------|------|----------|---------|--------------|\n| F       | 40   | 02:34:07 | -2.55  | 89.12        |\n| F       | 27   | 02:31:29 | -2.54  | 88.5         |\n| F       | 24   | 02:32:49 | -2.51  | 87.73        |\n| F       | 30   | 02:35:50 | -2.45  | 86.03        |\n| F       | 33   | 02:36:34 | -2.44  | 85.69        |\n| F       | 31   | 02:38:46 | -2.4   | 84.44        |\n| F       | 26   | 02:40:08 | -2.37  | 83.72        |\n| F       | 37   | 02:40:24 | -2.37  | 84.42        |\n| F       | 30   | 02:40:13 | -2.37  | 83.68        |\n| F       | 29   | 02:41:13 | -2.35  | 83.16        |\n```\n\n여기서 눈에 띄는 것은 이 10명의 완주자가 모두 여성이라는 점이에요. 그들은 높은 연령 등급을 가지고 있으며 훌륭한 완주 시간을 보여주지만... 비슷한 시간을 보이는 남성이 전혀 없다는 것이 납득하기 어렵네요.\n\n두 번째 예로, 작은 규모의 대회인 뉴저지의 애틀랜틱 시티 마라톤을 살펴보겠습니다 (그리고 이 마라톤이 거친 첫 번째 마라톤입니다).\n\n<div class=\"content-ad\"></div>\n\n\n| Gender | Age | Finish  | zScore | Age Grade |\r\n|--------|-----|---------|--------|-----------|\r\n| F      | 28  | 02:42:48| -2.32  | 82.35     |\r\n| M      | 35  | 02:19:15| -2.27  | 87.84     |\r\n| M      | 32  | 02:21:46| -2.13  | 85.83     |\r\n| M      | 61  | 03:00:04| -2.06  | 83.75     |\r\n| F      | 41  | 03:07:27| -1.92  | 73.72     |\r\n| M      | 34  | 02:33:10| -1.92  | 79.66     |\r\n| M      | 56  | 02:58:22| -1.91  | 80.66     |\r\n| M      | 56  | 03:01:43| -1.84  | 79.17     |\r\n| F      | 41  | 03:12:30| -1.83  | 71.79     |\r\n| F      | 24  | 03:11:47| -1.76  | 69.91     |\r\n\n\r\n여기에는 결과에서 남성과 여성이 섞여 있습니다. 그러나 상위 두 결과를 살펴보세요.\r\n\r\n최고의 결과는 z-점수가 -2.32인 28세 여성으로, 2시간 42분을 달렸습니다. 이는 훌륭한 시간입니다 (그녀는 첫 번째 여성 완주자이자 총 완주자 중 네 번째이지만, 이것이 2:19를 달린 35세 남성보다 더 나은 결과일까요?\r\n\r\n여기 마지막 예시 — 버지니아주 리치먼드 마라톤을 살펴보겠습니다.\r\n\n\n<div class=\"content-ad\"></div>\n\n\n| Gender   |   Age | Finish   |   zScore |   Age Grade |\n|----------|-------|----------|----------|-------------|\n| F        |    23 | 02:36:19 |    -2.44 |       85.77 |\n| F        |    30 | 02:36:30 |    -2.44 |       85.67 |\n| F        |    28 | 02:40:08 |    -2.37 |       83.72 |\n| F        |    29 | 02:43:31 |    -2.3  |       81.99 |\n| F        |    23 | 02:47:03 |    -2.24 |       80.26 |\n| F        |    36 | 02:47:54 |    -2.23 |       80.38 |\n| F        |    35 | 02:48:45 |    -2.21 |       79.76 |\n| F        |    26 | 02:49:08 |    -2.2  |       79.27 |\n| F        |    28 | 02:49:29 |    -2.19 |       79.1  |\n| F        |    28 | 02:50:19 |    -2.17 |       78.72 |\n\n\n여기서 다시 한번 말해요, 상위 10명의 완주자는 모두 여성입니다. 상위 두 명은 (2:36) 꽤 인상적하지만, 여전히 이 목록에 들어갈만한 남성이 없다는 것은 믿기 어렵습니다.\n\n이 여성들은 모두 젊으며, 35~39세 그룹을 넘는 사람은 한 명도 없습니다.\n\n조금 더 자세히 살펴보기 위해, 리치먼드 마라톤에서 z-점수로 상위 열 명의 남성입니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\n| 성별 | 나이 | 완주 시간 | z-점수 | 나이 등급 |\n|------|------|-----------|--------|-----------|\n| M    | 31   | 02:19:43  | -2.17  | 87.07     |\n| M    | 25   | 02:20:54  | -2.15  | 86.34     |\n| M    | 30   | 02:21:34  | -2.14  | 85.93     |\n| M    | 24   | 02:22:00  | -2.13  | 85.67     |\n| M    | 35   | 02:27:14  | -2.11  | 83.08     |\n| M    | 25   | 02:24:14  | -2.09  | 84.34     |\n| M    | 55   | 02:48:50  | -2.08  | 84.45     |\n| M    | 22   | 02:25:32  | -2.06  | 83.59     |\n| M    | 26   | 02:26:26  | -2.05  | 83.08     |\n| M    | 42   | 02:36:17  | -2.01  | 81.55     |\n``` \n\n그래서 몇몇 정말 우수한 남성분들이 있었어요. 그렇지만 31살 남성분이 2시간 19분만에 완주했는데도 z-점수가 -2.17밖에 되지 않았어요. 우연히도, 그게 2시간 50분에 완주한 여성 10위보다 뒤에 오네요.\n\n여기에는 조금 더 다양한 연령대가 있어요(55살과 42살), 하지만 이 완주자들의 대부분은 35세 미만의 남성분들이에요. \n\n# z-점수를 사용하는 데 문제점\n\n\n<div class=\"content-ad\"></div>\n\n이 문제의 가장 분명한 문제는 이 측정 방법이 여성의 결과를 과대평가하기 쉽다는 것입니다 — 특히 젊은 여성의 결과를 말이죠. 큰 규모의 두 마라톤에서 전체 상위 열 명 리스트가 모두 여성으로 차지되는 경우가 있었습니다.\n\n이게 왜 발생하는 걸까요?\n\n각 연령 그룹별 평균과 표준 편차를 생각해 보십시오.\n\n35세 미만 여성의 평균 완주 시간은 4시간 43분 20초입니다. 이는 동일 연령대 남성보다 27분 늦고, 55-59세 이상의 모든 남성 연령 그룹보다도 느립니다.\n\n<div class=\"content-ad\"></div>\n\n동시에, 그들의 표준 편차(51:59)는 가장 낮은 편 중 하나입니다. 동일 연령의 남성들보다 약 두 분 더 낮습니다.\n\n이것들이 결합되면, 최고의 여성들이 평균보다 훨씬 낮은 점수를 얻을 수 있는 여지가 많아지며, 다른 연령 그룹에 비해 낮은 z-점수를 달성할 수 있습니다.\n\n해당 시점(2019년)에서, 마라톤 남성들의 세계 신기록은 2:01:39로, (2018년 베를린에서 일리우드 킵초게가 달성했습니다). 이는 -2.51의 z-점수를 달성할 것입니다.\n\n한편, 그때의 여성 세계 기록은 2:14:04로, (2019년 시카고에서 브리짓 코스게이가 달성했습니다). 이는 -2.86의 z-점수를 달성할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 시스템에서는 35세 미만의 여성에게 큰 내부적 이점이 있습니다. 2:30으로 달리는 여성이라도 Kipchoge의 세계 기록 아래에있는 z-점수(-2.55)를 달성할 수 있습니다.\n\n이 시각화는 각 연령 그룹의 달리기자 중 얼마나 많은 비율이 z-점수가 -2(파란색) 미만 또는 -1과 -2 사이 (보라색)인지 보여줍니다.\n\n그룹이 여전히 작지만, 35세 미만의 여성 중 2(1.27%) 미만의 점수를 받은 여성이 남성(0.79%)보다 더 많습니다.\n\n35~39세에는 -2 이하의 점수를 받은 여성(0.89%)이 남성(0.38%)보다 약 두 배 더 많습니다.\n\n<div class=\"content-ad\"></div>\n\n그런 노릇하며 60대와 70대 러너 사이에서도 재밌는 일들이 벌어지고 있어요. 하지만 이 그룹들이 너무 작아서 개별 레이스 결과에 명확히 나타나지 않을 수도 있어요.\n\n# 그래서 Z-점수는 레이스 결과를 이해하는 데 유용할까요?\n\n만약 연령 등급을 완전한 대체품으로 찾고 있다면, 그렇지 않다고 말할 거예요.\n\n이것은 불균형적이고, 일부 그룹이 다른 그룹보다 우위를 가지고 있다는 것이 분명해요. 젊은 여성들 사이에서 높은 평균 완주 시간은 그들이 그 평균 아래서 끝내고 낮은 Z-점수를 받을 공간이 훨씬 더 많다는 것을 명백히 보여줘요.\n\n<div class=\"content-ad\"></div>\n\n개념적으로, 일반적인 비교와 판단을 하려면 유용한 것 같아요. 어떤 러너가 평균값보다 표준 편차 하나나 둘 미만인지 아는 것은 그들이 얼마나 뛰어난지와 그 결과가 얼마나 어려운지에 대한 일반적인 감각을 제공해줍니다.\n\n하지만 이것을 극단적인 경우에 더 나은 비교를 하기 위해 보정하는 좋은 방법은 없다고 생각해요. \n\n나이 등급은 보정에 문제가 있을 수 있지만, 이 시스템을 개선된 것으로 보지는 않아요.\n\n어쩌면 다른 사람이 이 데이터를 가지고 더 나은 결과를 얻을 수 있을지도 모르겠지만, 저는 일단 지금은 이걸 잠시 내려놓고 리스트에서 지우려고 해요.\n\n<div class=\"content-ad\"></div>\n\n# 그럼 다음은 무엇인가요?\n\n지금까지 우리는 두 가지 대안을 살펴봤어요 — 백분위수와 z-점수.\n\n앞으로 나아가서, 이 두 가지 대안이 나이 등급과 어떻게 비교되는지 다시 한 번 살펴보고 싶어요. 또한 여러분이 직접 결과를 측정하고 비교할 수 있는 계산기를 만들어 공유하고 싶어요.\n\n그 다음으로, 2023년 데이터로 업데이트하고, 백분위수 시스템을 약간 수정하여 2023년 최신 나이 등급 테이블과 비교하고 싶어요.\n\n<div class=\"content-ad\"></div>\n\n이 시리즈를 마무리 짓는 것이 좋을 것 같아요. 만약 여러분이 자신의 분석을 원하신다면, 전체 데이터셋을 Kaggle에 공유할 예정이니까 참고해주세요.\n\n그 중에 하나라도 관심이 있다면, 다음 몇 개의 글을 받아보시려면 이메일 업데이트 구독을 확실하게 해주세요. 저는 다음 2~3주 안에 글을 올릴 예정이에요.\n\n만약 이 분석을 돕는 피드백이나 아이디어가 있다면, 응답을 남겨주세요. 항상 두 번째(또는 세 번째, 네 번째) 의견을 가지고 있으면 도움이 되죠!\n\n저는 열정적인 달리기 사랑자이자 데이터 좋아하는 사람입니다. 이제 40살이 되었는데, 연령대별 결과를 비교하는 것이 특히 저에게 흥미로운 분야에요. 제 활동을 계속 지켜보려면 아래와 같이 따라갈 수 있어요:\n\n<div class=\"content-ad\"></div>\n\n- 제 훈련에 대해 알아보려면 Running with Rock을 팔로우하세요\n- 마라톤 훈련 계획을 선택하는 팁을 읽어보세요\n- Strava에서 제를 스토킹하세요","ogImage":{"url":"/assets/img/2024-06-22-CanWeUseZ-ScorestoCompareRunningPerformancesBetweenAgeGroups_0.png"},"coverImage":"/assets/img/2024-06-22-CanWeUseZ-ScorestoCompareRunningPerformancesBetweenAgeGroups_0.png","tag":["Tech"],"readingTime":10},{"title":"파이썬을 사용한 탐색적 데이터 분석 EDA 방법","description":"","date":"2024-06-22 16:56","slug":"2024-06-22-ExploratoryDataAnalysisEDAUsingPython","content":"\n\n## Python에서 탐색적 데이터 분석 및 데이터 시각화에 대한 기본 예제\n\n![image](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_0.png)\n\n# 소개\n\n탐색적 데이터 분석(EDA)는 데이터 분석 과정에서 중요한 단계로, 데이터셋을 더 잘 이해하기 위한 작업입니다. EDA를 통해 데이터의 주요 특징, 변수 간의 관계, 문제에 관련이 있는 변수들을 이해할 수 있습니다. 또한 EDA를 통해 데이터에서 누락된 값, 중복된 값, 이상값 및 오류를 식별하고 처리할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 게시물에서는 Pandas, Numpy, Matplotlib 및 Seaborn과 같은 다양한 라이브러리를 사용하여 EDA(탐색적 데이터 분석)를 수행할 때 Python을 사용합니다. Pandas는 데이터 조작 및 분석을 위한 라이브러리입니다. Numpy는 숫자 계산을 위한 라이브러리입니다. Matplotlib 및 Seaborn은 데이터 시각화를 위한 라이브러리입니다.\n\n이 프로젝트에서는 2023년 Goodreads Choice Awards 베스트 북을 분석할 것입니다.\n\n![image](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_1.png)\n\n분석을 수행하고 연습을 시작하려면 Kaggle에서 이 데이터셋을 찾을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 데이터 이해하기\n\n## 필요한 라이브러리 가져오기\n\n```js\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\n## 데이터셋을 판다스 데이터프레임에 불러오기\n\n<div class=\"content-ad\"></div>\n\n```python\n# CSV 파일에 데이터가 저장되어 있으므로, Pandas 함수를 사용하여 CSV 파일을 읽을 것입니다.\ndf = pd.read_csv('Good_Reads_Book_Awards_Crawl_2023_12_27_11_14.csv')\n\ndf.sample(5) # 데이터 샘플을 표시합니다\n\n```\n\n![image](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_2.png)\n\n다음으로 데이터셋에서 일부 불필요한 열을 제거할 것이며, 이 단계는 선택 사항입니다. 사용하지 않을 열을 제거하여 DataFrame의 크기를 줄이는 것이 좋습니다.\n\n```python\n# 사용하지 않을 열은 source_URL, Book Description, About the Author입니다\ndf.drop(['source_URL','Book Description','About the Author'], axis=1, inplace=True)\n```\n\n<div class=\"content-ad\"></div>\n\n## 데이터프레임 확인하기\n\n이제 각 열의 데이터 유형을 확인하고 숫자 열의 요약을 확인하여 다음 단계를 결정할 수 있습니다.\n\n```js\ndf.info()\n```\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_3.png)\n\n<div class=\"content-ad\"></div>\n\n.info()를 통해 데이터 세트는 누락된 값이 없는 상태로 괜찮아 보입니다. 또한 데이터 세트의 형태(컬럼 수: 12, 행 수: 299) 및 각 컬럼의 데이터 유형과 같은 정보를 제공해줍니다.\n\n```js\ndf.describe()\n```\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_4.png)\n\n.describe() 메서드는 DataFrame의 숫자형 열에 대한 요약 통계를 제공합니다. 각 숫자형 열의 평균값, 중간값, 표준 편차, 최솟값 및 최댓값을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n## Int/Float 크기를 축소하고 데이터 유형 할당하기\n\n데이터의 외관과 숫자 구성을 식별했다면, 데이터 분석의 후속 단계를 결정할 수 있습니다. .info()로부터 데이터의 크기인 28.2 KB와 각 열의 데이터 유형을 알 수 있습니다. .describe() 메서드는 각 열의 최솟값과 최댓값, 그리고 평균값과 같은 숫자 열의 통계를 보여줍니다.\n\n이 결과를 통해 Number of Ratings와 Number of Reviews와 같이 여전히 숫자 열이어야 하는 열이 누락되어 있는 것을 알 수 있습니다. 이러한 열들은 천 단위 구분자로 쉼표 “,”를 사용합니다. Readers Choice Votes와 같이 일반 서식으로 저장된 열은 천 단위 구분자가 없습니다. 숫자를 쉼표로 구분하지 않고 일반 숫자를 사용하는 이유는 식별자일 수도 있고 여러 자리 숫자일 수도 있어 숫자를 분리하는 것이 적절하지 않을 수 있기 때문입니다. 따라서 쉼표를 제거하고 공백으로 대체해야 합니다.\n\n```js\nnumeric_columns = ['Number of Ratings', 'Number of Reviews']\n\n# 해당 열에서 캐릭터 쉼표를 제거하고 Int32로 변환\nfor column in numeric_columns:\n    df[column] = df[column].replace(',', '', regex=True).astype('int32')\n```\n\n<div class=\"content-ad\"></div>\n\n이 방법은 위의 열에서 모든 쉼표를 제거할 것입니다. .astype(`int32`) 이 부분은 남겨두셔도 좋습니다. 왜냐하면 판다스가 자동으로 데이터 유형을 int64로 할당해줄 것이기 때문입니다. 여기서 64와 32는 각 행당 메모리 양이나 비트 수를 나타냅니다.\n\n그대로 둘 수 있지만, 데이터프레임을 더 효율적으로 만들기 위해 이러한 값을 낮은 값으로 다운캐스팅하고 있습니다. 아래 숫자형 열의 범위를 살펴보면:\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_5.png)\n\n각 숫자형 열의 가장 작은 값과 가장 큰 값을 보여줍니다. 예를 들어 'Readers Choice Votes' 열에서, 가장 작은 값은 935이고 가장 큰 값은 397565입니다.\n\n<div class=\"content-ad\"></div>\n\n값의 범위를 알면 해당 값을 저장하는 데 필요한 비트를 결정할 수 있습니다. 참고로:\n\n- Int8 변수는 -128에서 127까지의 값을 저장할 수 있습니다.\n- Int16 변수는 -32,768에서 32,767까지의 값을 저장할 수 있습니다.\n- Int32 변수는 -2,147,483,648에서 2,147,483,647까지의 값을 저장할 수 있습니다.\n- Int64 변수는 -9,223,372,036,854,775,808에서 9,223,372,036,854,775,807까지의 값을 저장할 수 있습니다.\n\nInt32가 값 범위에 딱 맞는 옵션입니다. Int64를 사용할 수도 있지만 더 많은 메모리를 사용하고 DataFrame을 덜 효율적으로 만든다는 점에서 적절하지 않습니다.\n\n실수의 경우에는 약간 다릅니다. 실수는 데이터가 저장할 수 있는 소수 자릿수에 실제로 영향을 줍니다. Float16는 4자리 소수를 저장하고, Float32는 8자리 소수를 저장하며, Float64는 16자리 소수를 저장할 수 있습니다. DataFrame에서 많은 소수 자릿수를 사용할 필요는 없지만 원래 값과 동일하게 유지하고 싶은 경우 Float16을 사용하는 것이 가장 좋은 선택입니다.\n\n<div class=\"content-ad\"></div>\n\n이제 각 열의 값을 범위로 알고 있으므로 해당 열을 올바른 데이터 유형으로 할당할 것입니다.\n\n또한 일부 열은 텍스트 값을 저장합니다. 해당 열의 데이터 유형을 문자열 또는 범주로 할당할 수 있습니다. 판다스 문서에 따르면 범주형 데이터 유형은 소수의 다른 값으로 구성된 문자열 변수(예: 성별, 사회 계급, 혈액형, 국가 소속 등)에 유용합니다. 이 정의에 따라 'Column Readers Choice Category'가 범주형 데이터 유형을 사용하는 가장 적합한 선택입니다.\n\n```js\n# 나머지 열을 올바른 데이터 유형으로 변환합니다.\nconvert_dict = {'Readers Choice Votes': 'int32',\n                'Readers Choice Category': 'category',\n                'Title': 'string',\n                'Author': 'string',\n                'Total Avg Rating': 'float16',\n                'Number of Pages': 'int16',\n                'Edition': 'category',\n                'First Published date': 'datetime64[ns]',\n                'Kindle Price': 'float16'}\ndf = df.astype(convert_dict)\n```\n\n'Kindle Version and Price' 열의 경우 이미 가격 데이터를 저장하는 'Kindle Price'라는 다른 열이 있으므로 가격을 제거할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```jsx\n#텍스트에서 통화를 분리하여 새 열에 넣습니다\ndf['Kindle Version'] = df['Kindle Version and Price'].str.extract('([a-zA-Z ]+)', expand=False).str.strip()\n\n#열을 올바른 데이터 유형으로 변경합니다\ndf['Kindle Version'] = df['Kindle Version'].astype('category')\n\n#이전 열을 제거합니다\ndf = df.drop('Kindle Version and Price', axis=1)\n```\n\n이제 데이터프레임을 다시 확인해보겠습니다:\n\n```jsx\ndf.info()\n```\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_6.png)\n\n<div class=\"content-ad\"></div>\n\n```js\ndf.describe()\n```\n\n![Exploratory Data Analysis using Python](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_7.png)\n\n```js\ndf.sample(10)\n```\n\n![Exploratory Data Analysis using Python](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_8.png)\n\n\n<div class=\"content-ad\"></div>\n\n최근 결과에서 보듯이 데이터프레임의 메모리 크기를 크게 줄일 수 있습니다. 데이터 유형을 변경하고 정수 및 부동 소수점을 다운캐스팅하여 메모리 크기를 줄이는 방법입니다. 이 단계는 선택 사항이지만 대규모 데이터셋을 다룰 때 매우 유용할 수 있습니다.\n\n# 데이터 분석 및 시각화\n\n## 카테고리 분포\n\n첫 번째 분석은 데이터셋 내의 다양한 카테고리별 책 분포를 찾는 것입니다. 그런 다음 seaborn 모듈을 사용하여 시각화할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ncat_counts = df['독자 선호 카테고리'].value_counts()\nprint(cat_counts)\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=cat_counts.index, y=cat_counts.values, palette='Blues_d')\nplt.title('카테고리별 분포')\nplt.xlabel('카테고리')\nplt.ylabel('책 수')\nplt.xticks(rotation=30, ha='right')\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_9.png\" />\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_10.png\" />\n\n우리 데이터는 모든 카테고리에 고르게 분포되어 있습니다. 다만, 데뷔 소설 카테고리만 19권의 책이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다음은 각 카테고리별 투표, 평점, 리뷰, 페이지 및 가격 분포를 분석할 것입니다. 이 분포를 시각화하기 위해 상자 그림을 사용할 것입니다.\n\n```js\nfig, axes = plt.subplots(3, 2, figsize=(16, 18), sharey=False, sharex=True)\n\n# 첫 번째 플롯: 독자 선호 투표 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Readers Choice Votes', palette='Set3', ax=axes[0, 0])\naxes[0, 0].set_title('각 카테고리별 독자 선호 투표 분포')\naxes[0, 0].set_ylabel('투표')\n\n# 두 번째 플롯: 평균 평점 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Total Avg Rating', palette='Set3', ax=axes[0, 1])\naxes[0, 1].set_title('각 카테고리별 평균 평점 분포')\naxes[0, 1].set_ylabel('평균 평점')\n\n# 세 번째 플롯: 평가 수 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Number of Ratings', palette='Set3', ax=axes[1, 0])\naxes[1, 0].set_title('각 카테고리별 평가 수 분포')\naxes[1, 0].set_ylabel('평가 수')\n\n# 네 번째 플롯: 리뷰 수 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Number of Reviews', palette='Set3', ax=axes[1, 1])\naxes[1, 1].set_title('각 카테고리별 리뷰 수 분포')\naxes[1, 1].set_ylabel('리뷰 수')\n\n# 다섯 번째 플롯: 페이지 수 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Number of Pages', palette='Set3', ax=axes[2, 0])\naxes[2, 0].set_title('각 카테고리별 페이지 수 분포')\naxes[2, 0].set_ylabel('페이지 수')\n\n# 여섯 번째 플롯: 킨들 가격 분포\nsns.boxplot(data=df, x='Readers Choice Category', y='Kindle Price', palette='Set3', ax=axes[2, 1])\naxes[2, 1].set_title('각 카테고리별 킨들 가격 분포')\naxes[2, 1].set_ylabel('킨들 가격')\n\nfor ax in axes[2, :]:\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha='right')\n    \nfig.tight_layout()\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_11.png\" />\n\n대부분의 분포가 편향되어 있으며, 몇몇 카테고리에서 극단값이 있습니다. 평균 평점 분포는 데이터가 정규 분포되어 있습니다. 편향된 데이터의 중심 경향성을 측정하는 가장 좋은 방법은 중앙값을 사용하는 것입니다. 중앙값은 극단값(이상치)에 민감하지 않기 때문에 적합합니다.\n\n<div class=\"content-ad\"></div>\n\n## 카테고리별 분석\n\n이제 투표, 평점, 리뷰, 페이지 및 가격별로 카테고리를 조사하여 2023년 가장 인기 있는 카테고리를 찾아보겠습니다.\n\n```js\n# 집계할 열을 결정합니다.\naggregations = {'Readers Choice Votes': 'sum', \n                'Total Avg Rating': 'mean',\n               'Number of Ratings': 'sum',\n               'Number of Reviews': 'sum',\n                'Number of Pages': 'median',\n                'Kindle Price': 'median',\n               }\n\n# 책 분야별로 그룹화합니다.\ncategory_vote = df.groupby('Readers Choice Category').agg(aggregations).sort_values('Readers Choice Votes', ascending=False)\n\n# 각 카테고리의 총 투표, 총 평점 및 총 리뷰의 백분율을 계산합니다.\ntotal_votes = category_vote['Readers Choice Votes'].sum()\ntotal_ratings = category_vote['Number of Ratings'].sum()\ntotal_reviews = category_vote['Number of Reviews'].sum()\npercent_of_total_votes = (category_vote['Readers Choice Votes'] / total_votes) * 100\npercent_of_total_ratings = (category_vote['Number of Ratings'] / total_ratings) * 100\npercent_of_total_reviews = (category_vote['Number of Reviews'] / total_reviews) * 100\n\n# 새로운 Votes, Ratings 및 Reviews의 데이터프레임을 생성합니다.\nresult_df = pd.DataFrame({\n    'Votes (합산)': category_vote['Readers Choice Votes'], \n    '% 투표': percent_of_total_votes, \n    '평균 평점': category_vote['Total Avg Rating'].round(2),\n    'Number of Ratings': category_vote['Number of Ratings'],\n    '% 총 평점': percent_of_total_ratings.round(2),\n    'Number of Reviews': category_vote['Number of Reviews'],\n    '% 총 리뷰': percent_of_total_reviews.round(2),\n    'Median Pages': category_vote['Number of Pages'],\n    'Median Kindle Price': category_vote['Kindle Price'].round(2)\n    })\n\n# 가장 많이 투표된 카테고리 찾기\nmax_voted_cat = result_df['Votes (합산)'].idxmax()\nmax_votes = result_df['Votes (합산)'].max()\navg_rat = result_df.loc[max_voted_cat, '평균 평점']\n\n# 가장 많이 평가된 카테고리 찾기\nmax_rated_cat = result_df['Number of Ratings'].idxmax()\nmax_rates = result_df['Number of Ratings'].max()\npct_max_rates = result_df['% 총 평점'].max()\n\n# 가장 많이 리뷰된 카테고리 찾기\nmax_reviewed_cat = result_df['Number of Reviews'].idxmax()\nmax_reviews = result_df['Number of Reviews'].max()\npct_max_reviews = result_df['% 총 리뷰'].max()\n\n# 결과 출력\nprint(f\"'{max_voted_cat}' 카테고리가 2023년 가장 많이 투표된 카테고리로 선정되었습니다. {max_votes:,}표를 획득했습니다.\")\nprint(f\"'{max_rated_cat}' 카테고리가 2023년 가장 많이 평가된 카테고리로 선정되었습니다. 평균 평점은 {format(avg_rat, '.2f')}이며, 평가 횟수는 {max_rates:,}회이며, 전체 평점의 {format(pct_max_rates, '.2f')}%를 차지하고 있습니다.\")\nprint(f\"'{max_reviewed_cat}' 카테고리가 2023년 가장 많이 리뷰된 카테고리로 선정되었습니다. 리뷰 수는 {max_reviews:,}개이며, 전체 리뷰의 {format(pct_max_reviews, '.2f')}%를 차지하고 있습니다.\")\n\nresult_df\n```\n\n![이미지](/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_12.png)\n\n\n<div class=\"content-ad\"></div>\n\n다음으로는 데이터를 시각화하여 더 나은 이해와 시각화를 하려고 합니다.\n\n```js\nfig, axes = plt.subplots(3, 2, figsize=(16, 18), sharey=False)\n\n# 첫 번째 그래프\nsns.barplot(x=result_df.index, y=result_df['Votes (sum)'], palette='Blues_d', order=result_df.index, ax=axes[0, 0])\naxes[0, 0].set_title('각 카테고리별 독자 투표수')\naxes[0, 0].set_ylabel('투표수')\naxes[0, 0].set_xticklabels(labels=result_df.index, rotation=30, ha='right')\n\n# 두 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Avg Ratings', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Avg Ratings'], palette='Blues_d', order=result_df_sorted.index, ax=axes[0, 1])\naxes[0, 1].set_title('각 카테고리별 평균 평점')\naxes[0, 1].set_ylabel('평균 평점')\naxes[0, 1].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\n# 세 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Number of Ratings', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Number of Ratings'], palette='Blues_d', order=result_df_sorted.index, ax=axes[1, 0])\naxes[1, 0].set_title('각 카테고리별 평점 수')\naxes[1, 0].set_ylabel('평점 수')\naxes[1, 0].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\n# 네 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Number of Reviews', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Number of Reviews'], palette='Blues_d', order=result_df_sorted.index, ax=axes[1, 1])\naxes[1, 1].set_title('각 카테고리별 리뷰 수')\naxes[1, 1].set_ylabel('리뷰 수')\naxes[1, 1].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\n# 다섯 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Median Pages', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Median Pages'], palette='Blues_d', order=result_df_sorted.index, ax=axes[2, 0])\naxes[2, 0].set_title('각 카테고리별 평균 페이지 수')\naxes[2, 0].set_ylabel('페이지 수')\naxes[2, 0].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\n# 여섯 번째 그래프\nresult_df_sorted = result_df.sort_values(by='Median Kindle Price', ascending=False)\nsns.barplot(x=result_df_sorted.index, y=result_df_sorted['Median Kindle Price'], palette='Blues_d', order=result_df_sorted.index, ax=axes[2, 1])\naxes[2, 1].set_title('각 카테고리별 평균 킨들 가격')\naxes[2, 1].set_ylabel('킨들 가격 ($)')\naxes[2, 1].set_xticklabels(labels=result_df_sorted.index, rotation=30, ha='right')\n\nplt.tight_layout()\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_13.png\" />\n\n그럼 여기까지입니다. 평균 평점이 가장 높지 않은에도 불구하고, Romance이 2023년에 가장 인기 있는 책 카테고리로 선정되었습니다. 다른 카테고리보다 투표, 평점 및 리뷰에서 우위를 차지했습니다. Ratings와 Reviews에서 두 번째로 높은 등수를 차지하며 숫자가 두 배 더 많습니다. 한편, Humor와 History & Biography는 2023년에 가장 인기 없는 두 가지 책 카테고리로 순위했습니다.\n\n<div class=\"content-ad\"></div>\n\n각 카테고리의 가격은 매우 유사한 편이지만, Romance과 Romantasy는 모든 카테고리 중에서 가장 낮은 중간 가격을 가지고 있습니다. 이에도 불구하고 투표 수, 평점 및 리뷰 수가 많습니다.\n\n## 상관 관계 찾기\n\n이제 질문이 등장합니다. 투표 수, 리뷰, 평점 또는 페이지 수와 가격 사이에 어떤 상관 관계가 있는지 알아볼까요? 페이지 수가 많으면 평점이 높을까요? 아니면 낮은 가격 카테고리일수록 더 많은 리뷰와 높은 평점을 받게 될까요? 알아보도록 하겠습니다.\n\n```js\n# 열 할당\ncolumns_of_interest = ['리뷰 수', '평가 수', '페이지 수', '총평균 평점', '최다선정 표', '킨들 가격']\n\n# 상관 행렬 계산\ncorrelation_matrix = df[columns_of_interest].corr()\n\n# 상관 행렬 표시\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\nplt.title('상관 행렬')\nplt.xticks(rotation=30, ha='right')\nplt.show()\n```\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_14.png\" />\n\n이 행렬을 통해 독자 선호투표와 리뷰 수와 평가 수 간에 높은 상관 관계가 있다는 것을 알 수 있습니다. 높은 리뷰와 평점은 높은 투표와 관련이 있습니다. 페이지 수와 가격은 투표, 평점, 리뷰 사이에 강한 연결이 없습니다. 즉, 책의 가격과 두께가 투표, 평점, 리뷰에 실제로 영향을 미치지 않는다는 것을 의미합니다.\n\n## 책으로 분석하기\n\n2023년 가장 투표를 많이 받은 책이 무엇인지 확인해 보는 시간입니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\nmost_voted_books = df[['Title', 'Readers Choice Category', 'Readers Choice Votes', 'Total Avg Rating', 'Number of Ratings', 'Number of Reviews', 'Number of Pages']].sort_values(by=['Readers Choice Votes', 'Number of Ratings', 'Number of Reviews'], ascending=False).head(20)\n\nplt.figure(figsize=(14, 6))\nsns.barplot(x=most_voted_books['Title'], y=most_voted_books['Readers Choice Votes'], data=most_voted_books, palette='Blues_d')\nplt.title('2023년 가장 많이 투표된 책')\nplt.xlabel('책 제목')\nplt.ylabel('투표 수')\nplt.xticks(rotation=30, ha='right')\nplt.show()\n\nmost_voted_books\n```\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_15.png\" />\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_16.png\" />\n\n그래서 이겨낸 책이 결정되었습니다. Fourth Wing은 2023 독자 선호 투표에서 가장 인기 있는 책으로 우승을 차지했습니다. 2위인 Yellowface보다 거의 2배 많은 표를 받아 거의 백만 개의 평가를 받았습니다. 로맨티지 카테고리에 해당하는 투표 중 절반 이상을 차지했어요.\n\n<div class=\"content-ad\"></div>\n\n\n이제 모든 카테고리에서 수상자들을 살펴보겠습니다.\n\n```js\nmax_votes_index = df.groupby('Readers Choice Category')['Readers Choice Votes'].idxmax()\ntitles_with_max_votes = df.loc[max_votes_index, ['Readers Choice Category', 'Title', 'Readers Choice Votes', 'Total Avg Rating', 'Number of Ratings', 'Number of Reviews', 'Number of Pages']].sort_values('Readers Choice Votes', ascending=False)\ntitles_with_max_votes\n```\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_18.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n다음으로, 2023년에 각 월별로 발행된 책의 수를 알아내기 위해 barplot을 사용하여 분석할 예정입니다.\n\n```js\nimport calendar\ndf['First Published date'] = pd.to_datetime(df['First Published date'])\n\n# 2023년에 발행된 책만 추출합니다.\nbooks_2023 = df[df['First Published date'].dt.year == 2023]\n\n# 각 월별 발행된 책의 수를 계산합니다.\nbooks_per_month = books_2023.groupby(books_2023['First Published date'].dt.month)['Title'].count().reset_index()\nbooks_per_month['Month'] = books_per_month['First Published date'].apply(lambda x: calendar.month_abbr[x])\n\nplt.figure(figsize=(14, 8))\nsns.barplot(data=books_per_month, x='Month', y='Title', palette='Blues_d')\nplt.title('2023년에 발행된 책의 분포')\nplt.xlabel('월')\nplt.ylabel('발행된 책의 수')\nplt.show()\n\nbooks_per_month[['Month','Title']]\n```\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_19.png\" />\n\n<img src=\"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_20.png\" />\n\n<div class=\"content-ad\"></div>\n\n이 그래프를 통해 우리는 11월에 가장 적은 책이 출간되었음을 발견했습니다. 반면에 9월과 1월은 가장 많은 책이 출간된 달이었습니다.\n\n# 결론\n\n우리의 분석을 통해 2023년에 어떤 카테고리가 가장 인기가 많고 가장 인기가 적은지를 결정했습니다. 또한 페이지, 투표, 평점 및 리뷰 사이의 연결 여부를 확인하기 위해 분포 분석과 상관 분석을 수행했습니다.\n\n내가 방금 보여준 것은 Python을 데이터 분석과 시각화 도구로 사용하는 놀라운 방법의 일감입니다. 이 기사에서는 데이터 집합을 더 깊게 이해할 수 있도록 중요한 기본 단계를 다루었는데, 이를 위해 분석을 위한 pandas와 시각화를 위한 matplotlib/seaborn과 같은 라이브러리를 사용했습니다.\n\n<div class=\"content-ad\"></div>\n\n제 글을 읽어 주셔서 감사합니다. 읽으시는 데 즐거움을 느끼셨기를 바라고 파이썬에서의 탐색적 데이터 분석을 이해하는 데 도움이 되었기를 바랍니다.\n\n여기서 저의 전체 코드를 제 Github에서 찾아볼 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_0.png"},"coverImage":"/assets/img/2024-06-22-ExploratoryDataAnalysisEDAUsingPython_0.png","tag":["Tech"],"readingTime":18},{"title":"포인트 클라우드 시각화 및 렌더링을 위한 블렌더 핸드북","description":"","date":"2024-06-22 16:51","slug":"2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering","content":"\n\n## BLENDER\n\n이 튜토리얼에서는 대량의 포인트 클라우드 데이터 세트를 조작하고 시각화하는 데 최고의 3D 도구 중 하나를 활용하는 방법에 대해 채우고자 합니다.\n\n![이미지](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_0.png)\n\n이 도구는 Blender라고 합니다. 다양한 데이터 시각화 기술을 실험하여 복잡한 분석 시나리오에 대응할 수 있습니다. 그리고 이것이 바로 저희를 함께 모이게 한 이유입니다.\n\n<div class=\"content-ad\"></div>\n\n블렌더의 확장된 데이터 시각화 능력과 결합하여 리얼리티 캡처 데이터셋(포인트 클라우드 형태)을 처리하는 최상의 기본적인 워크플로우는 무엇인가요?\n\n🦊Florent: 리얼리티 캡처는 상당히 혼란스러울 수 있는 \"새로운\" 용어로, 몇몇 소프트웨어와 회사들이 이 용어에서 이름을 딴 것을 알 수 있습니다. 이 \"전문 분야\"를 \"3D 맵핑\"의 특화된 분야로 볼 수 있으며, 목표는 LiDAR나 패시브 카메라(포토그래미터리와 3D 컴퓨터 비전을 통해)와 같은 다양한 센서를 사용하여 실제 세계의 3D 기하학을 캡처하는 것입니다. 이 과정을 보여드리는 기사는 여기에서 확인하실 수 있습니다: 포토그래메트리를 활용한 3D 재구성 가이드\n\n이 가이드에서는 아래에 설명된 9단계의 프로세스로 나누어 설명하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다음은 데이터 시각화를 위한 분석 내용으로 여러 경로 추출 시갠 제품을 생성하도록 해줍니다. 하지만 바로 시작하기 전에, 데이터 시각화에 대해 어떻게 생각하시나요?\n\n🎵 독자분들께: 이 실전 안내서는 UTWENTE의 F. Poux 및 P. Raposo 공동 저자와 함께 작업한 결과물입니다. 트웬테 대학교 ITC 학부에서 부여된 ITC -프로젝트에서 디지털 트윈스로부터의 재정적 기여를 인정합니다. 모든 이미지는 Florent Poux의 저작권 소유입니다.\n\n# 데이터 시각화를 위한 분석\n\n<div class=\"content-ad\"></div>\n\n무언가를 생각할 때 어떤 시각적 표현을 만들어 놓는다고 느끼시나요? 희귀종인 나르월(Monodon monoceros) 같은 것을 언급한다면 이미 알고 있는 \"데이터/지식 점\"이 있다면, 바로 이 '바다의 유니콘'으로 불리는 이 생물의 긴 나선 모양의 엄니를 떠올리게 될 것입니다.\n\n🦊 Florent: 요즘 나르월에 좀 관심을 갖고 있어요 🦄. 직접 본 적은 없지만, 이제 북극 바다인 캐나다와 그린란드 인근으로 가야 이 아름다운 생물들을 발견할 수 있는 기회가 된다는 걸 알았어요. 그들은 '떼'를 이루는 사교적 동물로 발견되며 굉장한 다이빙 능력을 갖고 있다고도 배워요. 1.5분 정도 밑물에 참는다고 해도 행복한데; 나르월은 1,500m(4,921피트) 깊이까지 다이빙 하며 종종 25분 이상 숨을 참을 수 있다고 해요. 정말 멋진 힘이에요!\n\n그리고 이것은 우리에게 얼마나 본능적으로 \"데이터 시각화\"가 중요한지를 보여줍니다. 우리는 강력한 능력을 갖고 있어서 다양한 주제를 시각적으로 지원하여 종합하거나 표현할 수 있어요. 분석 작업의 경우, 아래에 나와 있는 것처럼 이러한 '지원'과 그 범위를 구체화할 수 있습니다.\n\n하지만 더욱 강력한 건 현대 컴퓨팅 시스템을 통해 실제 물체를 나타내는 3D 지오메트리와 속성을 다루는 능력입니다. 실제로, 이것은 우리가 좀더 관련성을 갖고 설정한 목표에 맞는 더 나은 시각화를 만들 수 있도록 하는 필수적인 도구로 작용합니다.\n\n<div class=\"content-ad\"></div>\n\n그리고 중요성에 대해 이야기할 때, 자크 베르탱(1918~2010)에 대해 들어보셨나요? 벨기에 가수 자크 브렐이나 프랑스 전 대통령 자크 시라크, 해군 장교 자크 쿠스토가 아니라 자크 베르탱에 관해 한 번 이야기해 보렐까요?\n\n## 베르탱의 시각 변수\n\n우리가 지리 공간 맥락에 둔 발코니 데이터 시각화에 대해 이야기하는 것은 어려운 일입니다. 그리고 베르탱의 시각 변수를 논하지 않고 싶진 않습니다. 이들은 프랑스 지도 제작자 자크 베르탱에 의해 소개된 기본적인 속성들로, 데이터를 그래픽적으로 나타내는 데 사용될 수 있습니다. 이들은 정보를 시각적으로 명확하고 효과적으로 인코딩하는 방법을 제공합니다. 아래에 이러한 변수들을 요약한 그림이 있습니다.\n\n이 시각 변수들은 매력적인 시각화를 디자인하기 위한 구조화된 프레임워크를 제공합니다. 이러한 변수들을 전략적으로 선택하고 결합함으로써, 우리는 복잡한 정보를 효과적으로 전달하는 시각적 표현물을 만들어 낼 수 있습니다. 이를 통해 시청자가 데이터를 이해하고 해석하기 쉽게 만들어줍니다.\n\n<div class=\"content-ad\"></div>\n\n🦊 플로랑: 내가 너의 마음에 이 작은 씨앗을 심음으로써, 너가 이 글을 읽으면서 다양한 렌더링 옵션들과 놀면서 변수가 데이터 시각화 제품의 목표에 전달되는 메시지에 어떻게 영향을 미치는지에 대해 어떤 평행을 유발할 수 있기를 바란다!\n\n그리고 이제, 더 이상 말미암은 것 없이, 엄청난 시각화를 만드는 시간이다. 준비되었나요?\n\n# 단계 1. 블렌더 환경 설정하기\n\n우리의 환경을 설정하는 것으로 시작해보겠습니다. 이를 위해 브라우저를 열고 Blender를 검색하세요. Blender 웹사이트에 도착하면 다운로드 섹션으로 이동하여 Blender를 다운로드하세요.\n\n<div class=\"content-ad\"></div>\n\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*Duf49e4mj8YYS2Bj9S2wJg.gif)\n\nBlender의 버전은 적어도 3.6 이상이어야 합니다. 이 버전은 특히 포인트 클라우드에 대한 몇 가지 조정이 있습니다.\n\n🦊 Florent: 낮은 버전을 사용 중이라면 이 튜토리얼에서 사용할 몇 가지 포인트 클라우드 가져오기 함수를 놓칠 수 있습니다. 그러나 낮은 버전을 사용한다면 함수를 직접 작성할 수 있지만, 이로 인해 이 튜토리얼의 범위가 확장되고 다른 세션에서 보여줍니다. 더 높은 버전인 4.0과 같은 경우에는 과정이 더 간단합니다.\n\nOS에 Blender 3.6.4 이상을 성공적으로 다운로드했다면, 설치를 해보세요. 이 튜토리얼에서는 Windows를 사용하겠지만, Linux나 MacOS에서도 동일하게 작동해야 합니다. Blender를 설치했다면, 두 번째 단계로 넘어가서 3D 포인트 클라우드 데이터셋을 수집할 차례입니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 단계 2. 3D 포인트 클라우드 전처리\n\n우리는 중요한 장면의 포인트 클라우드 데이터를 수집해야 합니다. 오늘은 제가 친구 로만 로브룩과 함께 촬영한 오래된 공장을 가져왔어요. 아래의 안내서는 포토그램메트리와 LiDAR 처리를 사용하여 데이터셋을 얻는 방법을 설명합니다.\n\n데이터 전처리 단계를 돕기 위해, 이미 \"PLY\" 파일 형식으로 준비된 포인트 클라우드 데이터를 제공해드렸어요.\n\n🦚 참고: \"PLY\" 파일 형식은 \"다각형 파일 형식\"을 나타내며 3D 기하학 데이터를 저장하는 데 널리 사용됩니다. 이 형식은 Stanford 대학의 Greg Turk와 Marc Levoy에 의해 개발되었으며 3D 모델을 정점(공간상의 점)과 다각형(일반적으로 삼각형 또는 사각형이지만, 다른 다각형 유형도 사용할 수 있음)의 집합으로 표현할 수 있습니다. 이는 다양하며 각 정점 또는 면과 관련된 데이터 속성 범위를 지원하며 색상, 법선 벡터, 텍스처 좌표 등을 포함할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n포인트 클라우드의 경우, 우리는 얼굴이 없고 점만 가지고 있습니다. 이 점들은 공간에서의 x, y, z 좌표로 정의되는 3D 점들입니다. \"PLY\" 파일을 준비할 때, 각 점이 연관된 하나의 추가 속성(색상)을 가지도록 합니다.\n\n🦊Florent: \"PLY\" 파일은 ASCII 및 BINARY 형식으로 제공됩니다. ASCII PLY 형식은 데이터를 평문으로 표현하여 사람이 읽을 수 있지만 파일 크기가 커질 수 있습니다. BINARY PLY는 이진 인코딩을 사용하여 파일 크기를 줄이지만 사람이 읽을 수 없는 내용을 가지고 있습니다. 양식을 사용하면 파일이 어떤 내용을 포함하고 있는지 빠르게 파악할 수 있으며, 이는 각 정점의 수와 관련된 속성의 유형 등을 나타냅니다.\n\n그러나 데이터셋을 캡처한 후, 추가 단계를 거쳤습니다: 시나리오 내의 구성 요소를 구분하기 위해 비지도 분할 체계를 적용했습니다.\n\n![이미지](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_2.png)\n\n<div class=\"content-ad\"></div>\n\n초기 장면을 하위 요소로 효율적으로 분할해서 관심 지점을 담고 있는 \"레이어\"로 데이터를 준비할 수 있었어요.\n\n구분된 포인트 클라우드 결과물은 드라이브 폴더에서 찾을 수 있어요. 각 클라우드는 색상이 있는 PLY 파일이에요: [드라이브 폴더](Access to the Drive Folder)에서 확인할 수 있어요. 이 폴더에는 1에서 9까지 번호가 매겨진 아홉 개의 포인트 클라우드가 들어 있어요. 각각이 장면의 일부를 갖고 있어요.\n\n![이미지](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_3.png)\n\n이 분할된 요소를 사용하기 전에, 전체 포인트 클라우드를 하나의 entity로 내보내기도 했어요. 여기서 다운로드할 수 있어요: [포인트 클라우드 데이터 다운로드](Point Cloud Data Download).\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_4.png)\n\nStep 3 이동 전에 전체 포인트 클라우드 데이터 세트를 다운로드하여 전체 장면을 설정합니다.\n\n# Step 3. Blender에 포인트 클라우드 가져오기\n\n이제 3D 데이터 세트와 Blender가 모두 설치되었으므로 사용할 시간입니다!\n\n\n<div class=\"content-ad\"></div>\n\n🦊Florent: 블렌더 내에서 3D 포인트 클라우드를 가져오는 것은 도전적일 수 있으며, 최상의 및 가장 빠른 방법을 찾는 데 시간이 걸렸어요. 그러나, 적어도 10시간 이상의 복잡한 시행착오를 거칠 필요 없이 작동하는 방법을 발견했답니다. Blender 버전 (3.6.4+)을 사용하면 색상 정보가 포함된 포인트 클라우드를 가져올 수 있게 되었는데, 이전에는 불가능했어요.\n\nBlender 3.6을 실행하려면, 프로그램을 그냥 열어주세요. 안으로 들어가면 작은 팝업 창에 버전 번호가 표시되어 있을 거예요 (제 경우엔 3.6.4로 보였어요). 이 창을 닫으려면 화면 어디든 클릭하세요. Blender에 들어가면 3D 씬 안에 자신을 발견하게 될 겁니다. 이 씬에는 큐브, 카메라, 그리고 빛(보기 어려울 수 있어요)이 포함되어 있어요. 카메라 관점에서 씬을 탐색하려면 휠을 클릭하고 드래그하여 중심 지점 주변을 회전하세요. (카메라를 옆으로 이동시키려면) 휠을 끌면서 Shift 키를 누르세요.\n\n아래와 같이 표시된 바와 같이, 시작하기 전에 큐브를 선택하고, 이 시나리오에 사용되지 않을 것이기 때문에 삭제하세요.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*A8Ih-UvMf1OViKKcwz7k3Q.gif)\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*3Q10Ik1O01ccdsxOVjD0Sg.gif)\n\n그런 다음 `파일`로 가서 “실험적 Stanford .PLY” 옵션을 선택하여 아래와 같이 포인트 클라우드를 가져올 수 있습니다.\n\n🦊Florent [업데이트]: 최신 버전의 Blender 4.0+에서는 비실험적인 Stanford PLY (.ply)를 사용할 수 있습니다. 이제 이 실험적 기능을 통합하고 RGB 정보를 읽을 수 있습니다.\n\n![이미지](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_5.png)\n\n\n<div class=\"content-ad\"></div>\n\n이제 이전에 다운로드한 .PLY 데이터 세트를 가져올 수 있습니다. 파일을 클릭하여 가져오세요. 가져온 후 1500만 점의 포인트 클라우드를 로드하는 데 몇 초만 소요됨을 알 수 있습니다. 세부 사항을 더 잘 확인하려면 휠을 사용하여 회전 및 확대할 수 있습니다.\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*jsj7jy6NgZqnQ9eCFdSlqQ.gif\" />\n\n좋아요, 이 단계에서는 블렌더 내에 포인트 클라우드가 있습니다. 그런데 이제 어떻게 해야 하죠? 블렌더에서 3D 포인트 클라우드를 가장 잘 사용할 수 있는 템플릿 레이아웃을 제공해 드리겠습니다.\n\n# 단계 4. 3D 데이터 시각화를 위한 블렌더 UI 설정\n\n<div class=\"content-ad\"></div>\n\n블렌더 UI를 초기 레이아웃보다 더 잘 정리해 봅시다. 여기서는 조금 기술적이니 충분한 집중력과 에너지가 필요할 거에요!\n\n먼저 Geometry Node Editor를 위로 옮길 거예요. 그런 다음 화면을 두 부분으로 분할할 거에요. 이를 위해 왼쪽 모퉁이에 있는 작은 십자를 클릭하고 드래그할 거에요. 오른쪽에는 우리의 geometries에 재질을 적용하는 Shader Editor를 열거에요. 이미 Properties 메뉴가 열려 있으니 그대로 남겨둘 거에요. 왼쪽에 화면을 늘려 Text Editor와 Python Console을 열 거에요. 이렇게 하면 필요한 모든 것을 손끝에 두게 될 거에요.\n\n🦊 Florent: 그렇습니다! 잘 읽으셨어요! 우리는 Blender 내에서 Python을 사용할 수 있어요! 정말 멋진 기능이죠. 이는 이 튜토리얼의 범위를 넓혀주며 다음 집중 에피소드에서 다룰 예정이에요.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*B2Bde5Z9sSLC4oojCbjkgg.gif)\n\n<div class=\"content-ad\"></div>\n\n그럼요, 이제 시작할 준비가 모두 완료되었습니다. 우리는 포인트 클라우드를 성공적으로 로드했지만 모두 검은 색으로 보입니다. 그러나 걱정하실 필요는 없어요. 곧 이 문제를 해결할 거에요. 중요한 점은 이제 카메라, 메시 및 광원이 포함된 씬 콜렉션을 갖고 있다는 것입니다.\n\n메시를 클릭하면 각 벡터의 위치 및 RGBA 또는 투명도 정보를 가진 16.3백만 개의 벡터를 포함하는 스프레드시트 뷰어에서 확인할 수 있습니다.\n\n현재 모든 포인트는 재질이 없기 때문에 메시가 검은색으로 보입니다(또는 선택한 경우 주황색으로 나타날 수 있습니다).\n\n우리의 목표는 이 메시 객체를 포인트 클라우드 객체로 변환하고 각 포인트의 색상을 읽는 것입니다. 이를 위해 지오메트리 노드를 생성할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n🦊Florent: 간단히 적어둡니다. Control+C 및 Control+V와 같은 키보드 바로 가기는 마우스 위치에만 작동합니다. 특정 영역에서 무언가를 복사하고 붙여넣기하려면 마우스가 해당 영역에 있는지 확인하세요.\n\n# 단계 5. 3D 도형을 위한 Geometry 노드\n\nBlender의 Geometry 노드는 사용자가 3D 지오메트리를 절차적으로 생성하고 조작할 수 있도록 2.93 버전에서 소개된 강력한 기능입니다. 이는 개별 정점, 에지 또는 면을 수동으로 조작하지 않고도 복잡한 3D 지오메트리를 생성, 수정 및 애니메이션화할 수 있다는 것을 의미합니다. 실제로 노드 기반 절차적 워크플로우 덕분에 전통적인 모델링 기술 대신 시각적 인터페이스를 사용하여 지오메트리를 생성하고 편집할 수 있습니다. 이제 이것을 3D 포인트 클라우드로 시험해보겠습니다.\n\n먼저 메시를 선택한 후 Geometry 노드 창으로 이동해야 합니다. 거기서 \"New\"를 클릭하여 지오메트리 노드를 생성합니다. 이를 간단히 설명하겠습니다. 그것은 geometry라는 두 개의 그룹 입력과 출력으로 구성됩니다.\n\n<div class=\"content-ad\"></div>\n\n\"지오메트리를 수정하기 위해 \"메쉬를 포인트로\" 노드를 추가할 거에요. 추가하면 노드가 자동으로 연결되어, 마법같은 일이 일어날 거예요. 모든 포인트에는 공 모양의 지오메트리가 부착되고, 각 공의 반지름을 0.01 (1센티미터)로 설정하여 수정할 수 있어요. 보시다시피, 모든 포인트가 노드에 표시돼 있어요.\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*7Ycq0Rlt2b1TT712hE7xtA.gif\" />\n\n정말 멋지죠, 하지만 한 가지 문제가 있어요 - 이 모드에서는 색상을 볼 수 없어요. 걱정하지 마세요, 이건 정상이에요! 블렌더에는 쉐이딩, 솔리드, 소재 미리보기, 그리고 렌더링 표시 미리보기와 같은 다양한 디스플레이 모드가 있어요. 우리는 와이어프레임이 없기 때문에 (메쉬가 아니기 때문에), 거기를 클릭하면 벡터(포인트)만 표시될 거예요. 우리가 솔리드 렌더링을 보려면 적절한 디스플레이 모드를 선택해야 해요.\n\n<img src=\"/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_6.png\" />\"\n\n<div class=\"content-ad\"></div>\n\n렌더링된 디스플레이 미리보기는 재질을 설정한 후에 작동합니다. 하지만 우리는 아직 재질을 하나도 가지고 있지 않기 때문에 작동하지 않습니다. 따라서 논리적으로 다음 단계는 각 포인트에 재질을 부착하는 것입니다.\n\n# 단계 6. 색상을 위한 쉐이더 노드\n\nGeometry Nodes에 해당하는 것으로 Shader Nodes가 있습니다. 이들은 Blender의 재료 시스템의 기본 구성 요소입니다. 서로 다른 노드를 연결함으로써 복잡한 재료를 만들 수 있습니다. 각 노드는 재료의 외관을 나타내는 특정 측면을 대표합니다.\n\n재질은 물체가 빛과 상호 작용하는 방식을 결정하여 외관을 부여합니다. Shader Nodes는 Shader Editor에서 편집됩니다. 이는 Blender 내에서 전용 워크스페이스로, 노드를 사용하여 재료를 만들고 편집하는 시각적 인터페이스를 제공합니다. 이제 시작해보죠.\n\n<div class=\"content-ad\"></div>\n\nShader Editor에서 \"새로운 재질\"을 만듭니다. \"새로 만들기\" 버튼을 클릭하여 'material.001'이라는 이름의 재질을 만들 수 있습니다. 이 재질에는 주요 BSDF와 재질 출력이 포함되어 있습니다. 그러나 사용하기 위해서는 몇 가지 속성을 추가해야 합니다. 속성을 추가하기 위해 \"추가\" 버튼을 클릭하고 \"속성\" 옵션을 찾습니다. 그런 다음 컬러 매개변수를 선택하고 이를 우리의 주요 BSDF의 베이스 컬러 매개변수에 연결합니다.\n\n🦊 Florent: BSDF는 양방향 산란 분포 함수를 나타냅니다. 결국에는 특정한 광선이 주어진 각도에서 반사(산란)될 확률을 결정하는 수학적인 함수입니다.\n\n다음 단계는 색상에 대한 올바른 속성을 읽고 설정하는 것입니다. 스프레드시트 뷰에서는 더 이상 메쉬가 아닌 \"Col\" 속성이 있는 포인트 클라우드 유형으로 나타납니다.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_7.png\" />\n\n따라서 쉐이더 편집기 필드에서 그 이름(\"Col\")을 사용해야 합니다. 이 단계 이후에는 아무 변화가 없는 것처럼 보일 것입니다. 이는 우리가 추가할 두 가지 요소(포인트 클라우드에 재료 설정 및 렌더링 엔진 설정)가 아직 남아 있기 때문에 예상된 동작입니다. 재료 설정부터 시작해봅시다.\n\n## 포인트 클라우드 지오메트리 노드에 재료 추가하기\n\n우리의 지오메트리에 새 재료를 부착하려면, \"Set material\"이라는 새로운 노드를 지오메트리 노드에서 만들어야 합니다. 이 과정은 간단합니다. 노드를 가운데에 놓으면 자동으로 간격을 맞출 것입니다. 그러고나서 드롭다운에서 재료를 선택하면 끝입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*Ph-9AagMnvT_HvdXcdv0wQ.gif)\n\n이게 전부에요. 이제 두 번째 문제를 해결하는 데로 넘어가 봅시다: 3D 포인트 클라우드를 위한 적절한 렌더링 엔진을 설정하세요.\n\n## 3D 포인트 클라우드를 지원하는 렌더링 엔진 설정\n\n해당 단계를 완료했다면 여전히 화면에 아무것도 보이지 않아서 답답할 수 있습니다. 포인트 클라우드를 다루고 있기 때문에 특정 유형의 후처리 렌더러 인 Cycles가 필요합니다.\n\n\n<div class=\"content-ad\"></div>\n\n🦊 Florent: 싸이클(Cycles)은 Blender에서 사용되는 렌더링 엔진으로 현실적인 이미지를 생성하는 데 사용됩니다. 이는 빛의 행동을 시뮬레이션하는 패스 추적 렌더러로, 반사, 굴절, 그리고 전체 조명 같은 복잡한 효과를 가능하게 합니다. 싸이클은 고품질 출력으로 유명하며 Blender에서 사실적인 장면을 만드는 데 널리 사용됩니다. 저는 정말 좋아해요.\n\n싸이클로 전환하려면 속성 영역으로 이동하고 렌더 탭을 클릭하세요. 기본 렌더 엔진은 Eevee라고 불리지만, 이를 싸이클로 변경해야 합니다. 한 번 변경하면 다른 설정을 수정할 필요는 없지만, 과도한 렌더링 시간을 피하기 위해 30초의 시간 제한 설정을 권장합니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*2YlfnGW6IYTLs2C07j8fGQ.gif)\n\n이제 렌더러를 설정했으므로 최종 결과물을 보기 위해 미리보기 렌더링을 클릭할 수 있습니다. 결과가 마음에 들지 않는다면 재미있는 상황이네요. 하하, 그것은 함정이었어요!\n\n<div class=\"content-ad\"></div>\n\n시각적으로 조명이 필요합니다! 재료가 빛에 반응하는 효과를 얻기 위해 장면에 조명을 추가해야 합니다. 준비되셨나요?\n\n# 단계 7. 3D 장면 조명 및 렌더링 설정\n\nBlender에서 조명은 장면을 현실적으로 조명하기 위해 조명 소스를 배치하고 구성하는 것을 의미합니다. 포인트, 스폿, 선, 영역, 및 방출 소재와 같은 다양한 유형의 조명을 시뮬레이션할 수 있습니다. 뿐만 아니라, 조명의 세기, 색상, 감쇠 및 그림자 속성을 조절할 수도 있습니다. 현실 세계에서 빛이 어떻게 작용하는지 이해하고 이러한 설정을 실험하는 것은 Blender에서 강렬한 시각적 결과를 얻는 데 중요합니다. 그러므로 저는 장면에서 다양한 기하학 및 조명 소스를 다루는 데 도움을 드리겠습니다.\n\n먼저, 조명을 선택하고 움직이기 아이콘을 클릭하여 원하는 위치로 정확히 이동시킵니다. 이렇게 하면 모든 것이 부드럽고 더 현실적으로 보이게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*_CNOkMDpuqilMNLkjkKJcQ.gif)\n\n🦊 Florent: 신이 가려진 장면의 경우에도 빛을 배치하여 더 아름답게 보이게 할 수 있어요. 우리는 조명을 베이크된 색상 위치에 배치함으로써 탁월한 사실적인 효과를 얻을 거에요.\n\n빛을 배치한 후, 나는 특정 각도에서 장면을 보기 위해 뷰포트 렌더링을 클릭해요. 그리고 이제.... 타다! 전체 장면이 렌더링되어 놀라운 것처럼 보여요.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*_RuwKTykijg3v-VhZ4uISQ.gif)\n\n\n<div class=\"content-ad\"></div>\n\n거기서는 이미 하얀색인 점들에 대한 빛 효과를 수정할 수 있어요. 빛을 움직이면 실시간으로 변화를 관찰할 수 있어요.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*V3x1j0z6cu7gequS7LUYpg.gif)\n\n이 흥미진진한 기능은 3D 포인트 클라우드 데이터를 메싱 단계를 거치지 않고 직접 사용할 수 있게 해줘요. 노력을 많이 들이지 않고 일관된 시간 프레임 내에서 기본으로 사용할 수 있어요. 이 마일스톤을 달성하여 축하드려요! 다음 단계는 아직 남아 있어요.\n\n# 단계 8. 스토리보드 정의\n\n<div class=\"content-ad\"></div>\n\n좋아요, 이제 무거운 주제에 들어가 봅시다. 이 새로운 기술 세트를 구체적인 응용 프로그램에 사용해 보겠습니다: 추출 경로 계획.\n\n![이미지](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_8.png)\n\n여기가 간략한 설명입니다. 산업 연구원이 알려준 독특한 유물을 회수하기 위해 추출팀을 이끄는 중입니다. 이 고대 유물은 양 제조 공정에 대한 새로운 비밀을 밝히는 데 중요하며, 전 세계 산업 공정을 뒤집을 수 있는 가능성이 있습니다. 문제는 해당 사이트가 매우 오염되어 있으며 목표물을 회수할 시간이 60초 밖에 없다는 것입니다.\n\n당신의 연락망 덕분에 그곳의 3D 스캔 자료를 손에 넣었고, 이제 추출의 성공을 보장하기 위한 최적의 네비게이션 지도를 작성할 차례입니다. 이를 달성하기 위해 최초로 추출 계획에 포함해야 할 다섯 가지 주요 포인트를 포함한 명세서를 설정했어요:\n\n<div class=\"content-ad\"></div>\n\n- 초기 포인트 클라우드의 다양한 (3-5) 관점 뷰\n- 관심 대상의 다양한 돋보인 객체로 아티팩트의 상대적인 공간 안에 잘 배치\n- 객체를 추출하기 위한 정확한 \"강조\" (원뿔)\n- 추출 경로 정의, 위쪽에서 본 보기\n- 추출 경로의 다양한 시점을 퍼스트-퍼슨으로 본 뷰\n\n데이터 시각화 전문가로서, 자크 베르탱의 작업을 회상하고 시각 변수를 활용하여 명확한 커뮤니케이션 지원을 달성하는 데 최선을 다해보세요.\n\n# 단계 9. 3D 씬 추출 경로 계획\n\n프로세스를 시작하기 전에, 제 Step 2 (포인트 클라우드 사전 처리)에서 분해된 하위 요소들을 다운로드하는 것을 권장합니다. 그런 다음, 모든 포인트 클라우드를 로드하고 위에서 언급한 단계를 따라 각 포인트 클라우드를 다음과 같이 얻을 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_9.png\" />\n\n🦊Florent: 작업 속도를 높이기 위해 몇 가지 권장 사항을 안내해드릴게요. 먼저, 포인트 클라우드를 가져오기 전에 Scene Collection 메뉴에서 새 컬렉션을 만드세요 (우클릭 `새 컬렉션`), 그 안에 모든 포인트 클라우드를 끌어다 놓을 거에요. 가져오고 나면 전체 포인트 클라우드를 선택한 후 Geometry Node Editor에서 Geometry Nodes를 복사하세요. 그런 다음 로드된 모든 포인트 클라우드를 순차적으로 선택하여 새 Geometry Node를 만들고 미리 채워진 것을 지우고 \"템플릿\"을 붙여 넣으세요. 이를 모든 포인트 클라우드에 대해 반복하면 준비 완료입니다.\n\n준비되셨으면 명세서 항목 목록을 진행할 수 있어요\n\n## 1. 포인트 클라우드 렌더링\n\n<div class=\"content-ad\"></div>\n\n다양한 관점을 생성하기 위해서는 장면을 탐색하여 교차점을 명확히 보여줄 수 있는 최적의 장소를 찾아야 합니다. 예를 들어 먼저 아래에 표시된 대로 다양한 장소에 조명을 추가해야 할 수도 있습니다.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*G1CAE69MWwZslrCOVymaUA.gif)\n\n이 작업을 완료하고 만족스러운 시점을 찾았다면 현재 시점을 카메라 위치로 사용하는 방식으로 Ctrl + Shift + 0을 눌러 카메라를 배치해야 합니다.\n\n🧙‍♂️ 전문가: 카메라 설정을 조정하고 싶다면 초점 거리(Focal Length)를 변경하여 할 수 있습니다. 아래 그림에서처럼 저는 25mm 초점 거리를 사용했습니다. 또한 점의 반경도 독립적으로 조절할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n거기서 Render Image 탭 버튼을 누르면 현재 카메라 위치에서 이미지를 내보낼 수 있습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*9afTGK9vyFB7zGJ1lxhU2w.gif)\n\n좋아요, 이제 우리는 주변 상황의 주요 아이디어를 갖고 있어요. 가스로 가득 찼어요! ☣️\n\n## 2. 관심 대상\n\n<div class=\"content-ad\"></div>\n\n목표는 테이블 위의 의자, 가스 탱크, 양털 기계 및 목표물을 강조하여 공간과 목표를 보다 명확하게 전달하는 것입니다. 자크 베르탱의 작업을 따라 가능한 조정할 수 있는 매개변수를 조정할 수 있습니다. 저는 각 개별 객체에 부여한 기본 색상으로 새로운 소재를 만들었습니다:\n\n![각 객체에 대해 이렇게 함으로써 우리는 시선을 더 잘 이끄는 렌더를 생성할 수 있습니다:](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_10.png)\n\n각 개체에 대해 이렇게 하면 눈을 더 정확히 이끌 수 있는 렌더를 생성할 수 있습니다:\n\n![각 객체에 대해 이렇게 함으로써 우리 눈을 더 잘 이끌 수 있는 렌더를 생성할 수 있습니다:](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_11.png)\n\n<div class=\"content-ad\"></div>\n\n🌱 성장 중: 전달된 메시지를 어떻게 개선할 수 있을까요? 변수 조정(색상)의 선택이 타당하다고 생생하십니까?\n\n## 3. 추출 대상 강조\n\n멋지네요! 여기서부터, 관심 대상에 더 많은 강조를 더하고 싶어합니다. 다시 말씀드리지만, 가능성은 많습니다. 저희는 눈에 뷰 가이드 역할을 하는 3D 원뿔 메시를 사용하는 방법을 안내해 드리겠습니다.\n\n우선, 원뿔 메시 오브젝트를 생성해야 합니다. '메쉬' - '원뿔 추가' 옵션으로 이동하여 씬의 중심에 원뿔 메시를 생성할 수 있습니다. 그 다음, 해당 원뿔을 선택한 후, 크기를 조절하고 왼쪽에 있는 버튼을 사용하여 회전시키십시오. 마지막으로, 아래에 표시된 대로 목표물 위에 오브젝트의 위치를 변경하고 재료를 부여하면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](https://miro.medium.com/v2/resize:fit:1400/1*73JNEOXDwlsNfg2M6xqIwg.gif)\n\nFrom these steps, and after creating a render from a camera position, you should get something looking like this:\n\nBeautiful! It's time to get onto the extraction route.\n\n## 4. Top-down Extraction Route\n\n\n<div class=\"content-ad\"></div>\n\nGrease Pencil을 사용하면 마우스로 그림을 그릴 수 있어요. 올바르게 사용하려면 먼저 `Grease Pencil`을 추가해야 해요. 그 다음 \"Top View\"로 이동해서 \"Draw Mode\"를 눌러 경로를 그릴 수 있어요. 경로를 그린 후에는 Object Mode로 돌아가서 그리스 경로를 적절한 위치로 이동시켜야 해요. 아래 이미지를 참고해주세요.\n\n![그림1](https://miro.medium.com/v2/resize:fit:1400/1*HYgQi-DuB38Piisdo1Y9IQ.gif)\n\n마지막 단계는 경로에 색상을 추가하기 위해 그리스에 텍스처를 추가하는 거에요:\n\n![그림2](https://miro.medium.com/v2/resize:fit:1400/1*mE_t1JPB6wJvlHk8ECXSzQ.gif)\n\n<div class=\"content-ad\"></div>\n\n여기 있습니다. 우리는 추출 경로의 매력적인 전경을 얻기 위해 렌더링을 실행합니다!\n\n![image](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_12.png)\n\n## 5. POV 추출 경로\n\n마지막 단계는 경로의 일인칭 시점을 얻는 것입니다. 만약 직접 렌더링을 한다면, 다음과 같은 결과물이 나올 것입니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_13.png\" />\n\n사실 그리스 펜슬에는 깊이 테스트가 없습니다. 여기서 또 다른 꿀팁을 알려드릴게요. Z-깊이 테스트를 활성화하려면 ` 에디터 패널 ` 뷰 레이어 속성 ` 패스 -` 데이터로 이동하여 Z를 활성화하세요.\n\n<img src=\"/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_14.png\" />\n\n이렇게 하면 렌더를 다시 생성할 때 가려지는 부분 테스트를 통과할 수 있어서 포인트 클라우드를 처리하는 데 훌륭합니다!\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n이것은 계속해서 발전하는 여정이었습니다! 우리는 블렌더에서 3D 포인트 클라우드를 통합하고 처리하는 과정을 분석했습니다. 포인트 클라우드 데이터의 가져오기와 내보내기부터 씬 설정과 렌더링의 복잡성까지 세심하게 다루며, 블렌더의 기능의 복잡성을 탐험하고 3D 시각화와 렌더링의 전체 잠재력을 활용할 수 있게 되었습니다.\n\n지금까지 따라오면서, 블렌더에서 포인트 클라우드 데이터를 다루는 프로젝트에 자신감을 갖고 해결할 수 있는 지식과 기술을 얻었습니다. 이를 통해 3D 작업에서 창의성과 정밀도에 대한 새로운 가능성을 열 수 있게 되었습니다.\n\n🦊 플로랑: 블렌더의 강력한 도구와 새로운 전문 지식을 바탕으로, 몰입감 있고 고품질의 실내 시각화를 위한 가능성이 이제 여러분 손안에 있습니다. 그래서 이 안내서를 들고 자신감 있게 여러분의 다음 3D 여정에 돌입해보세요. 원시 포인트 데이터를 멋진 시각적 표현으로 변환할 수 있다는 것을 인지하며, 행복한 블렌딩이 되길 바랍니다!\n\n<div class=\"content-ad\"></div>\n\n# 참고 자료\n\n- Poux, Florent, Valembois, Q., Mattes, C., Kobbelt, L., & Billen, R. (2020). Initial user-centered design of a virtual reality heritage system: Applications for digital tourism. Remote Sensing, 12(16), 2583. [DOI](https://doi.org/10.3390/rs12162583)\n- Poux, Florent, Neuville, R., Van Wersch, L., Nys, G. A., Billen, R., Van Wersch, L., … & Billen, R. (2017). 3D Point Clouds in Archaeology: Advances in Acquisition. Processing and Knowledge Integration Applied to Quasi-Planar Objects, 96. [DOI](https://doi.org/10.3390/geosciences7040096)\n\n![이미지](/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_15.png)\n\n# 🔷기타 자료\n\n<div class=\"content-ad\"></div>\n\n- 🍇 데이터에 액세스하려면 여기를 방문하세요: 3D 데이터셋\n- 👨‍🏫 3D 온라인 데이터 과학 코스: 3D 아카데미\n- 📖 3D 자습서의 초기 액세스를 위해 구독하세요: 3D AI 자동화\n- 🧑‍🎓 석사 학위 취득: ITC Utwente\n\n# 🎓작가의 추천\n\n데이터 획득부터 가상 투어 생성까지 엔드 투 엔드 시스템을 구축하려면, 여기서 제공된 이전에 게시된 기사를 살펴보십시오. 데이터 처리에 사용되는 방법을 보여주는 것도 포함돼 있습니다. 즐거운 기술 습득되길 바랍니다!","ogImage":{"url":"/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_0.png"},"coverImage":"/assets/img/2024-06-22-TheBlenderHandbookfor3DPointCloudVisualizationandRendering_0.png","tag":["Tech"],"readingTime":19},{"title":"위성 열 영상을 1000m에서 10m로 다운스케일링하는 방법 Python","description":"","date":"2024-06-22 16:48","slug":"2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython","content":"\n\n![image](/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_0.png)\n\n# 목차\n\n- 🌅 소개\n- 💾 Sentinel-3 (1000 m) 및 Sentinel-2 이미지 다운로드\n- ⚙️ Sentinel-3 이미지 처리\n- 🌡️ 온도-NDVI 공간\n- 📐 열화상 이미지 선명화 (1000 m에서 10 m)\n- 🗺️ 선명화된 열화상 이미지 시각화\n- 📄 결론\n- 📚 참고 자료\n\n## 🌅 소개\n\n<div class=\"content-ad\"></div>\n\n위성에서 촬영한 열화상 이미지를 축소하는 연구는 열화상 이미지를 제공하는 위성들의 공간 및 시간 해상도 사이의 상충 관계 때문에 광범위하게 연구되었습니다. 예를 들어, Landsat-8의 재방문 주기는 16일이며, 원래 열 해상도는 100미터입니다. 반면에 Sentinel-3은 매일 열화상 이미지를 제공할 수 있지만, 공간 해상도는 1000미터입니다.\n\n![이미지](/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_1.png)\n\n열화상 이미지의 굵은 해상도를 해결하는 한 가지 방법은 NASA의 Landsat-9와 같은 열 센서가 장착된 추가 위성을 발사하는 것일 수 있습니다. Landsat-9의 경우, Landsat-8과 Landsat-9의 임시 해상도는 8일입니다 (하나의 위성보다는 16일), 맑은 하늘을 전제로 할 때.\n\n그러나 이 접근 방식은 수십억 달러의 투자와 몇 년의 노력이 필요합니다. 대신, 연구자들은 통계적 방법에 집중하여, 공간 해상도는 높지만 임시 해상도는 낮은 위성의 시정/근적외선 (VNIR) 밴드를 열화상 이미지와 상관시킴으로써 열화상 이미지의 낮은 공간 해상도 (하지만 높은 임시 해상도)와 연관시키는데 초점을 맞추었습니다. 예를 들어, 연구들은 Sentinel-2의 VNIR 밴드로부터 계산된 정규화 된 차이 채취 지수 (NDVI)가 Sentinel-3의 열화상 이미지와 역 상관 관계가 있음을 보여주었습니다.\n\n<div class=\"content-ad\"></div>\n\n답변을 요약하면 Sentinel-2의 NDVI와 Sentinel-3의 열화상 이미지 간의 상관 관계가 충분히 강하다면, 해당 방정식을 10m 해상도로 조정하여 10m 해상도의 열화상 이미지를 생성할 수 있습니다.\n\n위성 대역과 센서 스펙트럼에 대해 자세히 알고 싶다면 다음을 참조하십시오:\n\n이 게시물에서는 Sentinel-3로부터 낮은 공간 해상도 열화상 이미지와 Sentinel-2로부터 높은 공간 해상도 VNIR 이미지를 다운로드할 것입니다. 이 두 이미지는 각각의 위성에 의해 동시에 촬영되었습니다. 그런 다음, VNIR 대역을 사용하여 NDVI를 계산하고((NIR-Red)/(NIR+Red)), 이를 1000m로 업스케일하고 NDVI와 열대 대역 간의 상관 관계(둘 다 1000m 해상도)를 탐색할 것입니다. 마지막으로, 이 상관 관계를 사용하여 10m 해상도의 온도 지도를 생성할 것입니다.\n\n## 💾 Sentinel-3(1000 m) 및 Sentinel-2 이미지(10 m) 다운로드\n\n<div class=\"content-ad\"></div>\n\n이미 R 및 Python에서 Sentinel-2 이미지를 다운로드하는 방법에 대해 세 번의 게시물을 작성했습니다. 또한 Python에서 Sentinel-3 이미지를 다운로드하는 방법에 대한 게시물도 있습니다. 이곳에서는 해당 단계들을 반복하고 싶지 않아서 이 게시물을 참조하시기 바랍니다:\n\nR에서 Sentinel-2 이미지 다운로드:\n\nPython에서 Sentinel-2 이미지 다운로드:\n\nPython에서 Sentinel-3 이미지 다운로드:\n\n<div class=\"content-ad\"></div>\n\n만약 코드를 작성하지 않고 이미지를 다운로드하고 싶지 않다면, 다음 게시물을 확인해보세요:\n\n열화상 이미지를 축소화하기 위한 통계적 방법을 적용하는 중요한 단계는 위성에서 동시에 촬영된 선명한 이미지를 찾는 것입니다. 이미지를 다운로드하기 전에 날짜, 구름 양, 그리고 귀하의 관심 지역(AOI)에 기반하여 메타데이터를 필터링할 수 있습니다. Sentinel 메타데이터(관심 지역, 구름 양 등)를 필터링하고 처리하는 방법을 더 알고 싶다면 다음을 참조해보세요:\n\n이 게시물에서, 제 관심 지역(AOI)는 캘리포니아에 위치하며, 2023년 6월 19일에 Sentinel-2 및 Sentinel-3로 촬영된 선명한 이미지를 발견했습니다. 다른 위치나 날짜를 검색하고 싶다면 자유롭게 찾아보세요. 그러나 제가 다운로드한 이미지를 사용하길 원한다면, 다음 정보가 있습니다:\n\nSentinel-2: S2B_MSIL2A_20230620T183919_N0509_R070_T10SFG_20230620T224951\n\n<div class=\"content-ad\"></div>\n\n\nsatellite = “SENTINEL-2”\n\nlevel = “S2MSI2A”\n\nAOI = “POLYGON ((-121.0616 37.6391, -120.966 37.6391, -120.966 37.6987, -121.0616 37.6987, -121.0616 37.6391))”\n\nstart_date = “2023–06–19” ; end_date = “2023–06–21”\n\n\n<div class=\"content-ad\"></div>\n\n\nsatellite = “SENTINEL-3”\n\nlevel= “LST”\n\nAOI = “POLYGON ((-121.0616 37.6391, -120.966 37.6391, -120.966 37.6987, -121.0616 37.6987, -121.0616 37.6391))”\n\n\n<div class=\"content-ad\"></div>\n\nstart_date = “2023–06–19” ; end_date = “2023–06–21”\n\nSentinel-2에서 NIR 및 레드 밴드(NDVI를 계산하는 데 필요한 밴드)를 10m로 다운로드한 후 Sentinel-3에서 열화상 이미지를 다운로드하면 디렉토리에 이 세 파일이 있어야 합니다:\n\n![Image](/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_2.png)\n\n## ⚙️ Sentinel-3 이미지 처리\n\n<div class=\"content-ad\"></div>\n\nSentinel-3 이미지는 Sentinel-2보다 훨씬 넓은 장면을 커버합니다. 따라서 Sentinel-3 각 픽셀의 평균 NDVI 값을 필요로 하므로 Sentinel-3 이미지를 Sentinel-2 이미지의 범위에 따라 클리핑해야 합니다. 이를 위해 첫 번째 단계는 Sentinel-3 이미지를 Sentinel-2 이미지와 동일한 투영으로 재매핑하는 것입니다. 이를 수행하기 위해 다음과 같은 코드를 사용할 수 있습니다:\n\n```js\nimport numpy as np\nimport rasterio\nfrom rasterio.warp import calculate_default_transform, reproject, Resampling\nfrom pyproj import Transformer\n\ninput_raster = 'Sentinel-3_L2_LST_reproj.tif'\noutput_raster = 'Sentinel-3_L2_LST_reproj_32610.tif'\ndst_crs = 'EPSG:32610'\n\n# 입력 래스터 파일 읽기\nwith rasterio.open(input_raster) as src:\n    # 목적지 CRS에 대한 변환, 너비 및 높이 가져오기\n    transform, width, height = calculate_default_transform(src.crs, dst_crs, src.width, src.height, *src.bounds)\n\n    # 목적지 설정\n    kwargs = src.meta.copy()\n    kwargs.update({\n        'crs': dst_crs,\n        'transform': transform,\n        'width': width,\n        'height': height,\n        'dtype': np.float32,\n    })\n\n    # 목적지 생성 및 재매핑된 데이터 작성\n    with rasterio.open(output_raster, 'w', **kwargs) as dst:\n        # 재매핑 수행\n        for i in range(1, src.count + 1):\n            reproject(\n                source=src.read(1).astype(np.float32) * src.scales[0] + src.offsets[0],\n                destination=rasterio.band(dst, i),\n                src_transform=src.transform,\n                src_crs=src.crs,\n                dst_transform=transform,\n                dst_crs=dst_crs,\n                resampling=Resampling.bilinear)\n```\n\n이 스크립트에서는 이전 단계에서 다운로드한 Sentinel-3 래스터 이미지를 읽고, 지정된 CRS로 재매핑하기 위한 변환 매개변수를 계산하고, 재매핑된 데이터를 포함한 새로운 래스터 파일을 내보냅니다. 이러한 단계를 거치면 디렉토리에 다음 네 가지 파일이 있어야 합니다:\n\n![image](/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_3.png)\n\n\n<div class=\"content-ad\"></div>\n\nSentinel-3 열화상 이미지가 Sentinel-2 좌표 시스템으로 변경되었으므로 이제 Sentinel-2 데이터 범위를 기반으로 열화상 이미지를 잘라내는 작업을 할 수 있습니다. 아래 코드를 사용하여 작업할 수 있어요:\n\n```js\nimport rasterio\nimport numpy as np\n\n# 두 래스터 파일 열기\nwith rasterio.open('T10SFG_20230620T183919_B08_10m.jp2') as small_raster:\n    with rasterio.open('Sentinel-3_L2_LST_reproj_32610.tif') as big_raster:\n\n        # 더 작은 래스터의 범위 가져오기\n        min_x, min_y, max_x, max_y = small_raster.bounds\n\n        # 더 큰 래스터에서 더 작은 래스터의 범위 내의 데이터 읽기\n        window = rasterio.windows.from_bounds(min_x, min_y, max_x, max_y, big_raster.transform)\n        data = big_raster.read(window=window)\n\n        # 더 큰 래스터의 메타데이터 업데이트하여 더 작은 래스터의 범위와 일치시키기\n        clipped_meta = big_raster.meta.copy()\n        clipped_meta.update({\n            'height': window.height,\n            'width': window.width,\n            'transform': rasterio.windows.transform(window, big_raster.transform),\n            'dtype': data.dtype\n        })\n\n        # 잘라낸 데이터 쓰기\n        with rasterio.open('Sentinel-3_L2_LST_reproj_32610_clipped.tif', 'w', **clipped_meta) as clipped_raster:\n            clipped_raster.write(data)\n```\n\n이 스크립트에서는 두 래스터 파일(Sentinle-3 및 Sentinel-2 이미지)을 읽고, 더 작은 래스터의 범위(Sentinel-2)를 추출하고, 더 큰 래스터(Sentinle-3 열화상 이미지)에서 해당 데이터를 읽어와서 잘라낸 Sentinle-3의 메타데이터를 Sentinel-2 이미지와 일치하도록 업데이트한 후, 잘라낸 열화상 이미지를 새 TIFF 파일에 작성합니다.\n\n<img src=\"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_4.png\" />\n\n<div class=\"content-ad\"></div>\n\nNDVI 및 클리핑된 온도 맵을 옆으로 나란히 플롯해 봅시다:\n\n```js\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport rasterio\nfrom rasterio.plot import show\n\n# 파일 경로\nred_path = '/content/T10SFG_20230620T183919_B04_10m.jp2'\nnir_path = '/content/T10SFG_20230620T183919_B08_10m.jp2'\n\nclipped_temperature_path = '/content/Sentinel-3_L2_LST_reproj_32610_clipped.tif'\n\n# 래스터 데이터 읽기\nwith rasterio.open(red_path) as red_src:\n  red = red_src.read(1)\n\nwith rasterio.open(nir_path) as nir_src:\n  nir = nir_src.read(1)\n\nwith rasterio.open(clipped_temperature_path) as clipped_temp_ds:\n  clipped_temperature = clipped_temp_ds.read(1)\n\n# NDVI 계산\nndvi = (nir - red) / (nir + red)\n\n# NDVI와 온도를 옆으로 플롯\nfig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n\n# NDVI 플롯\nim1 = ax1.imshow(ndvi, cmap=ndvi_cmap, vmin=0, vmax=0.6)\nax1.set_title('NDVI', fontweight='bold', fontsize=14)\nfig.colorbar(im1, ax=ax1, shrink=0.5)\n\n# 클리핑된 온도 플롯\nim2= ax2.imshow(clipped_temperature, cmap=ndvi_cmap.reversed(), vmin=300, vmax=315)\nax2.set_title('Clipped Temperature', fontweight='bold', fontsize=14)\nfig.colorbar(im2, ax=ax2, shrink=0.5)\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_5.png\" />\n\n위와 같이 Sentinel-3 열화상 이미지를 Sentinel-2 맵의 범위에 맞게 성공적으로 클립했습니다. 그러나 Sentinel-2 이미지도 잘린 상태이며, 온도 픽셀의 평균 NDVI 값을 얻기 위해 존속 통계를 실행하면 많은 NaN 값을 얻게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_6.png)\n\n다음 단계에서 데이터프레임에서 NaN 값을 제외하여 수정할 것입니다.\n\n## 🌡️ 온도-NDVI 공간\n\n같은 투영 및 범위를 갖는 두 개의 명확한 이미지, 즉 센티넬-3에서의 열화상 이미지와 센티넬-2에서의 VNIR 이미지가 있을 때, 각 온도 픽셀의 평균 NDVI 값을 얻기 위해 존 채택 통계를 실행할 수 있습니다. 기본적으로 존 채택 통계 방식을 통해 10m에서 1000m로 NDVI 지도를 집계하여 온도-NDVI 공간에서 온도 값에 대한 NDVI 값을 플로팅할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n소개에서 언급했듯이 열값은 NDVI 값과는 역상관 관계에 있어야 합니다. 높은 NDVI는 식물의 비율이 더 높은 것을 나타내며 더 차가운 픽셀에 해당하고, 낮은 NDVI는 적은 식물이나 벌거벗은 토양에 해당하여 더 따뜻한 픽셀에 해당합니다. 우리의 AOI(관심 영역)에서 열과 NDVI 값 사이의 공간을 탐색하기 위해 존솔 통계를 수행해 보겠습니다:\n\n```js\nimport rasterio\nimport rasterio.features\nimport rasterio.mask\nimport pandas as pd\nimport geopandas as gpd\nimport rasterstats\n\nimport rasterio\nfrom rasterio.features import shapes\nmask = None\n\n# 입력 래스터 열기\nwith rasterio.open('Sentinel-3_L2_LST_reproj_32610_clipped.tif') as src:\n    # 래스터 밴드 읽기\n    image = src.read(1).astype(np.float32) * src.scales[0] + src.offsets[0]\n    results = (\n        {'properties': {'Temperature': v}, 'geometry': s}\n        for i, (s, v)\n        in enumerate(\n            shapes(image, mask=mask, transform=src.transform)))\n    geoms = list(results)\n    gpd_polygonized_raster = gpd.GeoDataFrame.from_features(geoms)\n\n# 래스터 열기\nwith rasterio.open('T10SFG_20230620T183919_B08_10m.jp2') as nir_src:\n    with rasterio.open('T10SFG_20230620T183919_B04_10m.jp2') as red_src:\n\n        # 데이터를 float32로 읽기\n        nir = nir_src.read(1).astype(np.float32) * nir_src.scales[0] + nir_src.offsets[0]\n        red = red_src.read(1).astype(np.float32) * red_src.scales[0] + red_src.offsets[0]\n\n        # NDVI 계산\n        ndvi = (nir - red) / (nir + red)\n\n        # 각 다각형에 대한 존솔 통계 계산\n        stats = rasterstats.zonal_stats(gpd_polygonized_raster.geometry, ndvi, affine=nir_src.transform, stats='mean')\n\n        # NDVI의 평균값을 데이터프레임에 추가\n        gpd_polygonized_raster['NDVI'] = [s['mean'] for s in stats]\n\n# 다각형 레이어를 shapefile로 저장\ngpd_polygonized_raster.to_file('output_polygons.shp')\n\n# geodataframe로부터 pandas 데이터프레임 생성\nstats_df = pd.DataFrame(gpd_polygonized_raster.drop(columns='geometry'))\n\n# 데이터프레임 출력\nprint(stats_df)\n```\n\n이 스크립트에서는 열 래스터를 여각화하고, Sentinel-2 이미지에서 NDVI를 계산하고, 각 온도 픽셀에 대한 존솔 통계(평균 NDVI)를 계산합니다.\n\n<div class=\"content-ad\"></div>\n\n위 표와 이전에 논의한 것처럼, Sentinel-3에서 열화 데이터를 가지고 있는 일부 픽셀에는 NaN 값이 있습니다. 이는 Sentinel-2 이미지의 자른 부분 때문입니다. Sentinel-2에서 NaN 값을 가진 행을 제외하겠습니다:\n\n```python\n# NaN 값이 있는 행 삭제\ndf_clean = stats_df.dropna(subset=['NDVI'])\ndf_clean\n```\n\n결과는 다음과 같습니다:\n\n<img src=\"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_8.png\" />\n\n<div class=\"content-ad\"></div>\n\n이 깔끔한 데이터프레임을 사용하여 온도-NDVI 공간을 그래프로 표현할 수 있어요:\n\n```js\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import linregress\n\n# 산점도 생성\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='NDVI', y='Temperature', data=df_clean, palette='coolwarm')\n\n# 그래프 제목 및 축 레이블 설정\nplt.title('NDVI 대 온도 그래프', fontsize=16, fontweight='bold')\nplt.xlabel('NDVI', fontsize=14)\nplt.ylabel('Temperature', fontsize=14)\n\n# 그래프 보여주기\nplt.show()\n```\n\n결과는 다음과 같아요:\n\n<img src=\"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_9.png\" />\n\n<div class=\"content-ad\"></div>\n\n위 그림에서 NDVI와 온도 사이의 역상관 관계를 관찰할 수 있지만 몇 가지 점이 이상치로 나타납니다. 이는 식물 및 맨 소토 외의 다른 특징을 나타내는 이미지의 픽셀 때문일 수 있습니다. 이러한 픽셀을 제거하기 위해 NDVI 값이 0.1에서 0.6 사이이고 온도 값이 300 켈빈에서 330 켈빈 사이인 값만 유지한 다음, 그에 맞게 그림을 업데이트하겠습니다:\n\n```js\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import linregress\n\n# 지정된 조건에 기반하여 데이터프레임 필터링\nfiltered_df = df_clean[(df_clean['NDVI'] >= 0.1) & (df_clean['NDVI'] <= 0.6) &\n                 (df_clean['Temperature'] >= 300) & (df_clean['Temperature'] <= 330)]\n\n# 온도 대 NDVI의 산점도 그리기\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='NDVI', y='Temperature', data=filtered_df, palette='coolwarm')\n\n# 선형 회귀 모델 적합\nslope, intercept = np.polyfit(filtered_df['NDVI'], filtered_df['Temperature'], 1)\n\n# 적합된 선 그리기\nx_line = np.linspace(min(filtered_df['NDVI']), max(filtered_df['NDVI']), 100)\ny_line = slope * x_line + intercept\nplt.plot(x_line, y_line, 'r', label='Fitted line')\n\n# 방정식 문자열 작성\nequation_str = f'y = {slope:.2f}x + {intercept:.2f}'\n\n# 그림에 방정식 표시\nplt.text(min(filtered_df['NDVI']), max(filtered_df['Temperature']), equation_str, fontsize=12, color='red')\n\n# 그림 제목과 축 레이블 설정\nplt.title('1-1. 온도 vs NDVI 그래프', fontsize=16, fontweight='bold')\nplt.xlabel('온도', fontsize=14)\nplt.ylabel('NDVI', fontsize=14)\n\n# 그림 표시\nplt.show()\n```\n\n결과는 다음과 같습니다:\n\n<img src=\"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_10.png\" />\n\n<div class=\"content-ad\"></div>\n\n이제는 역상관 관계가 더 잘 보이고 있으며, NDVI 값과 온도 간 관계를 설명하는 방정식도 나와 있습니다.\n\n## 📐 열화상 이미지 선명하게하기 (1000m에서 10m)\n\n이번 단계에서는 1000m 해상도에서 집계된 NDVI와 온도 간의 관계를 발견한 방정식이 10m 해상도에도 유효할 수 있다고 가정할 것입니다. 이 방정식을 원래의 NDVI 지도에 적용하여 10m 해상도에서 온도를 추정할 수 있습니다.\n\n```js\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# NIR 및 RED 파일 열기\nwith rasterio.open('T10SFG_20230620T183919_B08_10m.jp2') as src:\n    nir = src.read(1)\n    meta = src.meta\n\nwith rasterio.open('T10SFG_20230620T183919_B04_10m.jp2') as src:\n    red = src.read(1)\n\n# NDVI 계산\nndvi = (nir - red) / (nir + red)\n\n# NDVI를 사용하여 온도 추정\ntemp = -21.85 * ndvi + 314.9\n\n# NDVI를위한 컬러 램프 만들기\nndvi_cmap = plt.cm.RdYlGn\n\n# NDVI 및 온도를 나란히 그리기\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n\n# NDVI 그리기\nim1 = ax1.imshow(ndvi, cmap=ndvi_cmap, vmin=0, vmax=0.6)\nax1.set_title('NDVI', fontweight='bold', fontsize=14)\nfig.colorbar(im1, ax=ax1, shrink=0.7)\n\n# 온도 그리기\nim2 = ax2.imshow(temp, cmap=ndvi_cmap.reversed(), vmin=300, vmax=315)\nax2.set_title('Temperature', fontweight='bold', fontsize=14)\nfig.colorbar(im2, ax=ax2, shrink=0.7)\n\nplt.show()\n```\n\n<div class=\"content-ad\"></div>\n\nNIR 및 빨간색 대역을 읽어 NDVI를 계산하고 유도된 방정식에 기반하여 온도 값을 추정합니다. 다음으로, NDVI 및 온도에 대한 결과를 시각화하여 옆에 플롯을 표시합니다. 지도는 다음과 같습니다:\n\n![맵](/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_11.png)\n\n## 🗺️ 날카로워진 열 영상의 시각화\n\n이 섹션에서는 시각화 측면에 더욱 집중할 것입니다. 우리는 이미지의 중앙 영역을 확대하고, 이미지를 중앙을 기준으로 자릅니다. 또한 온도 차트를 제시하기 위해 Sentinel-2의 NDVI 지도, 원본 Sentinel-3 열 영상(1000m 해상도) 및 분석 결과를 기반으로한 날카로운 Sentinel-3 영상(10m 해상도)을 비교 및 자세한 검토를 위해 옆에 함께 제시할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```python\n# NDVI 및 온도를 옆으로 나란히 플로팅합니다\nfig, axs = plt.subplots(ncols=3, figsize=(15, 5))\n\n# NDVI 플로팅\nvmin, vmax = 0, 0.6\nndvi_subset = ndvi[int(0.75 * ndvi.shape[0]):, int(0.75 * ndvi.shape[1]):]\nim1 = axs[0].imshow(ndvi_subset, cmap=ndvi_cmap, vmin=vmin, vmax=vmax)\naxs[0].set_title('NDVI', fontweight='bold', fontsize=14)\naxs[0].set_xticks([])\naxs[0].set_yticks([])\nfig.colorbar(im1, ax=axs[0], shrink=0.7)\n\n# 온도 플로팅\nwith rasterio.open('Sentinel-3_L2_LST_reproj_32610_clipped.tif') as src:\n    original_temp = src.read(1)\n\nvmin, vmax = 300, 315\ntemp_subset = original_temp[int(0.75 * original_temp.shape[0]):, int(0.75 * original_temp.shape[1]):]\nim3 = axs[1].imshow(temp_subset, cmap=ndvi_cmap.reversed(), vmin=vmin, vmax=vmax)\naxs[1].set_title('Temperature', fontweight='bold', fontsize=14)\naxs[1].set_xticks([])\naxs[1].set_yticks([])\nfig.colorbar(im3, ax=axs[1], shrink=0.7)\n\nvmin, vmax = 300, 315\ntemp_subset = temp[int(0.75 * temp.shape[0]):, int(0.75 * temp.shape[1]):]\nim2 = axs[2].imshow(temp_subset, cmap=ndvi_cmap.reversed(), vmin=vmin, vmax=vmax)\naxs[2].set_title('Temperature', fontweight='bold', fontsize=14)\naxs[2].set_xticks([])\naxs[2].set_yticks([])\nfig.colorbar(im2, ax=axs[2], shrink=0.7)\n\n# 서브플롯 간 간격 조정\nfig.subplots_adjust(wspace=0.2)\n\nplt.show()\r\n```\n\n지도는:\n\n<img src=\"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_12.png\" />\n\n## 📄 결론\n\n\n<div class=\"content-ad\"></div>\n\nVisible과 Near-Infrared (VNIR) 대역과 열 이미지 간의 직접적 상관 관계는 열 이미지의 해상도를 향상시키는 유용한 방법으로 입증되었습니다. 이 기술은 위성의 적절한 공간 해상도를 가진 위성이 없을 때 온도를 높은 공간 해상도로 추정하는 데 실용적으로 활용됩니다. 이 다운스케일링 방법은 고해상도 열지도가 필요할 때 유용한 도구로 작용하며 소규모 온도 변화에 대한 세부 정보를 제공합니다. 앞으로 더 많은 고급 열 센서를 갖춘 위성을 발사함에 따라 빈도가 더 높은 고해상도 열 이미지를 얻을 수 있게 될 것입니다. 그 전까지는 이 방법이 더 높은 해상도의 열 이미지를 구현하는 비용 효율적인 선택지로 남아 있습니다.\n\n## 📚 참고 자료\n\nCopernicus 센티넬 데이터 [2024] - 센티넬 데이터에 대한 정보\n\nCopernicus 서비스 정보 [2024] - Copernicus 서비스 정보에 관한 정보\n\n<div class=\"content-ad\"></div>\n\n아감, N., 쿠스타스, W. P., 앤더슨, M. C., 리, F., 닐, C. M. U. (2007). 열화상 이미지 공간 개선을 위한 식물 지수 기반 기술. Remote Sensing of Environment, 107(4), 545–558. ISSN 0034–4257.\n\n가오, F., 쿠스타스, W. P., 앤더슨, M. C. (2012). 육지 위의 열화상 위성 이미지 개선을 위한 데이터 마이닝 접근 방식. Remote Sensing, 4, 3287–3319.\n\n휴리나, H., 코헨, Y., 카르니엘리, A., 파노프, N., 쿠스타스, W. P., 아감, N. (2019). Sentinel-3 위성 이미지의 열화상 개선을 위한 TsHARP 유틸리티 평가. Remote Sensing, 11, 2304.\n\n📱 저와 더 다양한 콘텐츠를 공유하려면 다른 플랫폼에서 연락하세요! LinkedIn, ResearchGate, Github 및 Twitter.\n\n<div class=\"content-ad\"></div>\n\n여기 해당 링크를 통해 제공되는 관련 게시물들이 있습니다:","ogImage":{"url":"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_0.png"},"coverImage":"/assets/img/2024-06-22-DownscalingaSatelliteThermalImagefrom1000mto10mPython_0.png","tag":["Tech"],"readingTime":17}],"page":"39","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true}