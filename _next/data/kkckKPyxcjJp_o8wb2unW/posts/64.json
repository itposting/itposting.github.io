{"pageProps":{"posts":[{"title":"언어별 RAG 애플리케이션 탐색 미슈나와 대화하기","description":"","date":"2024-06-19 20:28","slug":"2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah","content":"\n\n## 래빈 문헌에 대한 다국어 RAG 시스템 구축\n\n![이미지](/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_0.png)\n\n# 소개\n\n이 게시물에서 래빈 문헌과 상호작용하기 위한 독특한 검색 보강 생성(RAG) 응용 프로그램을 구축한 여정을 공유하게 되어 매우 기쁩니다. MishnahBot은 학자들과 일반 사용자들이 미슈나를 질의하고 탐색하는 직관적인 방법을 제공하는 것을 목표로하며, 상호작용적으로 도울 수 있습니다. 이는 관련 소스 텍스트를 빠르게 찾거나 종교 법에 대한 복잡한 토론을 요약하는 등의 문제를 해결하는 데 도움을 줄 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n수년 전에 이러한 프로젝트 아이디어가 있었지만, 기술이 아직 충분히 발달하지 않은 것 같았어요. 이제는 대형 언어 모델과 RAG 기능이 발전함에 따라 매우 간단해졌어요. \n\n아래는 최종 제품 모습이에요. 여기서 시도해 볼 수 있어요:\n\n![이미지](/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_1.png)\n\n# 그래서 RAG 시스템에 대한 극찬이란 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\nRAG(검색 보강 생성) 애플리케이션은 정확성을 향상시키고 대형 언어 모델(LLM)에서 제공되는 추론 능력을 활용하기 위해 상당한 주목을 받고 있습니다. 도서관, 동일 제조업체의 자동차 설명서 컬렉션 또는 세금 서류와 대화할 수 있다면 어떨까요? 질문을 하고 풍부한 전문 지식에 의해 제공되는 답변을 받을 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_2.png)\n\n# RAG 대 장단점과 증가된 컨텍스트 길이\n\n언어 모델 상호작용을 향상시키는 두 가지 신흥 추세가 있습니다: 검색 보강 생성(RAG) 및 컨텍스트 길이 증가, 가능한 경우 매우 긴 문서를 첨부 파일로 허용하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\nRAG 시스템의 한 가지 주요 장점은 비용 효율성입니다. RAG를 사용하면 쿼리 비용을 크게 증가시키지 않고도 큰 컨텍스트를 처리할 수 있어서 비용이 비실수하는 경우를 방지할 수 있습니다. 게다가 RAG는 더 모듈화되어 있어서 다른 지식 베이스 및 LLM 제공업체와 쉽게 결합하여 사용할 수 있습니다. 반면에 언어 모델에서 직접 컨텍스트 길이를 늘리는 것은 하나의 상호작용에서 훨씬 긴 텍스트를 처리할 수 있는 흥미로운 발전입니다.\n\n# 설정\n\n이 프로젝트에서는 개발 환경으로 AWS SageMaker를 사용했습니다. AWS Bedrock를 사용하여 다양한 LLM에 액세스했으며, 파이프라인을 관리하기 위해 LangChain 프레임워크를 사용했습니다. 두 AWS 서비스 모두 사용자 친화적이며 사용한 리소스에 대해서만 요금을 부과하므로 여러분께서 직접 시도해보길 권해 드립니다. Bedrock를 사용하려면 Llama 3 70b Instruct 및 Claude Sonnet에 대한 액세스 권한을 요청해야 합니다.\n\n새로운 Jupyter 노트북을 열고 사용할 패키지를 설치해 보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n!pip install chromadb tqdm langchain chromadb sentence-transformers\n```\n\n# 데이터셋\n\n이 프로젝트의 데이터셋은 유대교 전통에서 중심적인 고대 래빈 신학 텍스트인 미슈나입니다. 이 텍스트를 선택한 이유는 내 마음에 가깝기 때문이며, 동시에 단순한 주제이기 때문에 언어 모델에 대한 도전을 제공합니다. 데이터셋은 원래 히브리어와 일치하는 영어 번역이 있는 유대 래빈 신학 텍스트의 보물창고인 Sefaria-Export 리포지토리²에서 얻었습니다. 이 일치는 RAG 애플리케이션의 다른 단계에서 다른 언어로 전환할 수 있도록 돕습니다.\n\n참고: 여기서 적용된 동일한 과정은 사용자가 선택한 다른 텍스트 모음에도 적용할 수 있습니다. 이 예시는 또한 히브리어로 보여진 것처럼 RAG 기술을 다른 언어에서 사용할 수 있는 방법을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n# 한번 더 해보세요\n\n# 1. 데이터셋 로드하기\n\n먼저 해당 데이터를 다운로드해야 합니다. 전체 저장소가 상당히 크기 때문에 git sparse-checkout을 사용할 것입니다. 터미널 창을 열고 아래 명령어를 실행해주세요.\n\n```js\ngit init sefaria-json\ncd sefaria-json\ngit sparse-checkout init --cone\ngit sparse-checkout set json\ngit remote add origin https://github.com/Sefaria/Sefaria-Export.git\ngit pull origin master\n```\n\n<div class=\"content-ad\"></div>\n\n```js\r\ntree Mishna/ | less\r\n```\n\n그리고… 왔네요! 우리가 필요한 데이터 파일이 이제 있습니다:\n\n```js\r\nMishnah\n├── Seder Kodashim\n│   ├── Mishnah Arakhin\n│   │   ├── English\n│   │   │   └── merged.json\n│   │   └── Hebrew\n│   │       └── merged.json\n│   ├── Mishnah Bekhorot\n│   │   ├── English\n│   │   │   └── merged.json\n│   │   └── Hebrew\n│   │       └── merged.json\n│   ├── Mishnah Chullin\n│   │   ├── English\n│   │   │   └── merged.json\n│   │   └── Hebrew\n│   │       └── merged.json\r\n```\n\n이제 주피터 노트북 환경에서 문서를 로드해 봅시다:\n\n<div class=\"content-ad\"></div>\n\n```js\r\nimport os\r\nimport json\r\nimport pandas as pd\r\nfrom tqdm import tqdm\r\n\r\n# 진행률 표시가 있는 DataFrame으로 모든 문서를 로드하는 함수\r\ndef load_documents(base_path):\r\n    data = []\r\n    for seder in tqdm(os.listdir(base_path), desc=\"Loading Seders\"):\r\n        seder_path = os.path.join(base_path, seder)\r\n        if os.path.isdir(seder_path):\r\n            for tractate in tqdm(os.listdir(seder_path), desc=f\"Loading Tractates in {seder}\", leave=False):\r\n                tractate_path = os.path.join(seder_path, tractate)\r\n                if os.path.isdir(tractate_path):\r\n                    english_file = os.path.join(tractate_path, \"English\", \"merged.json\")\r\n                    hebrew_file = os.path.join(tractate_path, \"Hebrew\", \"merged.json\")\r\n                    if os.path.exists(english_file) and os.path.exists(hebrew_file):\r\n                        with open(english_file, 'r', encoding='utf-8') as ef, open(hebrew_file, 'r', encoding='utf-8') as hf:\r\n                            english_data = json.load(ef)\r\n                            hebrew_data = json.load(hf)\r\n                            for chapter_index, (english_chapter, hebrew_chapter) in enumerate(zip(english_data['text'], hebrew_data['text'])):\r\n                                for mishnah_index, (english_paragraph, hebrew_paragraph) in enumerate(zip(english_chapter, hebrew_chapter)):\r\n                                    data.append({\r\n                                        \"seder\": seder,\r\n                                        \"tractate\": tractate,\r\n                                        \"chapter\": chapter_index + 1,\r\n                                        \"mishnah\": mishnah_index + 1,\r\n                                        \"english\": english_paragraph,\r\n                                        \"hebrew\": hebrew_paragraph\r\n                                    })\r\n    return pd.DataFrame(data)\r\n# 모든 문서를 로드\r\nbase_path = \"Mishnah\"\r\ndf = load_documents(base_path)\r\n# DataFrame을 파일로 저장하여 나중에 참조\r\ndf.to_csv(os.path.join(base_path, \"mishnah_metadata.csv\"), index=False)\r\nprint(\"데이터셋이 성공적으로 DataFrame에 로드되고 파일로 저장되었습니다.\")\r\n```\r\n\r\n그리고 데이터를 확인해보세요:\r\n\r\n```js\r\ndf.shape\r\n(4192, 7)\r\n\r\nprint(df.head()[[\"tractate\", \"mishnah\", \"english\"]])\r\ntractate  mishnah                                            english\r\n0  Mishnah Arakhin        1  <b>Everyone takes</b> vows of <b>valuation</b>...\r\n1  Mishnah Arakhin        2  With regard to <b>a gentile, Rabbi Meir says:<...\r\n2  Mishnah Arakhin        3  <b>One who is moribund and one who is taken to...\r\n3  Mishnah Arakhin        4  In the case of a pregnant <b>woman who is take...\r\n4  Mishnah Arakhin        1  <b>One cannot be charged for a valuation less ...\r\n```\r\n\r\n좋아 보이니, 이제 벡터 데이터베이스 단계로 넘어갈 수 있습니다.\n\n혹시 필요하신 점이 있으면 언제든지 물어주세요.\n\n<div class=\"content-ad\"></div>\n\n## 2. Vectorizing and Storing in ChromaDB\n\n이제, 텍스트를 벡터화하여 로컬 ChromaDB에 저장합니다. 간단히 말해서, 텍스트를 밀도 있는 벡터로 표현하여 의미론적으로 유사한 텍스트가 벡터 공간에서 서로 \"가까이\" 있게 됩니다. 이 기술은 쿼리가 주어졌을 때 관련된 단락을 검색하는 데 활용됩니다.\n\n저희는 가벼운 벡터화 모델인 all-MiniLM-L6-v2를 선택했습니다. 이 모델은 CPU에서 효율적으로 실행될 수 있어 성능과 자원 효율성 사이에 좋은 균형을 제공하며, 우리의 응용 프로그램에 적합합니다. OpenAI의 text-embedding-3-large와 같은 최첨단 모델들이 더 뛰어난 성능을 제공할 수 있지만, 일반적으로 GPU에서 실행되는 상당한 계산 자원이 필요합니다.\n\n임베딩 모델 및 성능에 대한 더 많은 정보는 MTEB leaderboard를 참조할 수 있습니다. 이 leaderboard는 여러 작업에서 다양한 텍스트 임베딩 모델을 비교합니다.\n\n<div class=\"content-ad\"></div>\n\n우리가 벡터화에 사용할 코드입니다 (CPU 기계에서는 이 데이터 세트에서 몇 분 정도 소요될 것입니다):\n\n```js\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport chromadb\nfrom chromadb.config import Settings\nfrom tqdm import tqdm\n\n# 임베딩 모델 초기화\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n# ChromaDB 초기화\nchroma_client = chromadb.Client(Settings(persist_directory=\"chroma_db\"))\ncollection = chroma_client.create_collection(\"mishnah\")\n# 저장된 파일에서 데이터 세트 불러오기\ndf = pd.read_csv(os.path.join(\"Mishnah\", \"mishnah_metadata.csv\"))\n# 진행 표시줄과 함께 임베딩을 생성하는 함수\ndef generate_embeddings(paragraphs, model):\n    embeddings = []\n    for paragraph in tqdm(paragraphs, desc=\"Generating Embeddings\"):\n        embedding = model.encode(paragraph, show_progress_bar=False)\n        embeddings.append(embedding)\n    return np.array(embeddings)\n# 영어 문단에 대한 임베딩 생성\nembeddings = generate_embeddings(df['english'].tolist(), model)\ndf['embedding'] = embeddings.tolist()\n# 진행 표시줄과 함께 ChromaDB에 임베딩 저장\nfor index, row in tqdm(df.iterrows(), desc=\"Storing in ChromaDB\", total=len(df)):\n    collection.add(embeddings=[row['embedding']], documents=[row['english']], metadatas=[{\n        \"seder\": row['seder'],\n        \"tractate\": row['tractate'],\n        \"chapter\": row['chapter'],\n        \"mishnah\": row['mishnah'],\n        \"hebrew\": row['hebrew']\n    }])\nprint(\"Embeddings and metadata successfully stored in ChromaDB.\")\n```\n\n# 3. 영어로 우리의 RAG 생성하기\n\n데이터 세트가 준비되었으므로, 이제 영어로 우리의 검색 보완 생성 (RAG) 애플리케이션을 만들 수 있습니다. 이를 위해 LangChain을 사용할 것인데, 이는 다양한 언어 모델 작업 및 통합에 대한 통합 인터페이스를 제공하여 복잡한 애플리케이션을 쉽게 구축할 수 있는 강력한 프레임워크입니다.\n\n<div class=\"content-ad\"></div>\n\nLangChain은 언어 모델(LLMs), 검색기 및 벡터 저장소와 같은 다른 구성 요소를 통합하는 프로세스를 간단화합니다. LangChain을 사용하면 각 구성 요소의 내부 복잡성에 대해 걱정하지 않고 응용 프로그램의 고수준 논리에 집중할 수 있습니다.\n\n다음은 RAG 시스템을 설정하는 코드입니다:\n\n```js\nfrom langchain.chains import LLMChain, RetrievalQA\nfrom langchain.llms import Bedrock\nfrom langchain.prompts import PromptTemplate\nfrom sentence_transformers import SentenceTransformer\nimport chromadb\nfrom chromadb.config import Settings\nfrom typing import List\n\n# Llama 3 70B Instruct 모델을 위한 AWS Bedrock 초기화\nllm = Bedrock(\n    model_id=\"meta.llama3-70b-instruct-v1:0\"\n)\n\n# 프롬프트 템플릿 정의\nprompt_template = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=\"\"\"\n    주어진 맥락만을 기반으로 다음 질문에 답해주세요:\n    맥락: {context}\n    질문: {question}\n    답변 (간략하고 명료하게):\n    \"\"\",\n)\n\n# ChromaDB 초기화\nchroma_client = chromadb.Client(Settings(persist_directory=\"chroma_db\"))\ncollection = chroma_client.get_collection(\"mishnah\")\n\n# 임베딩 모델 정의\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n\n# 간단한 검색기 함수 정의\ndef simple_retriever(query: str, k: int = 3) -> List[str]:\n    query_embedding = embedding_model.encode(query).tolist()\n    results = collection.query(query_embeddings=[query_embedding], n_results=k)\n    documents = results['documents'][0]  # 'documents' 내부의 첫 번째 목록에 액세스\n    sources = results['metadatas'][0]  # 소스의 메타데이터에 액세스\n    return documents, sources\n\n# LLM 체인 초기화\nllm_chain = LLMChain(\n    llm=llm,\n    prompt=prompt_template\n)\n\n# SimpleQA 체인 정의\nclass SimpleQAChain:\n    def __init__(self, retriever, llm_chain):\n        self.retriever = retriever\n        self.llm_chain = llm_chain\n\n    def __call__(self, inputs, do_print_context=True):\n        question = inputs[\"query\"]\n        retrieved_docs, sources = self.retriever(question)\n        context = \"\\n\\n\".join(retrieved_docs)\n        response = self.llm_chain.run({\"context\": context, \"question\": question})\n        response_with_sources = f\"{response}\\n\" + \"#\"*50 + \"\\nSources:\\n\" + \"\\n\".join(\n            [f\"{source['seder']} {source['tractate']} Chapter {source['chapter']}, Mishnah {source['mishnah']}\" for source in sources]\n        )\n        if do_print_context:\n            print(\"#\"*50)\n            print(\"검색된 단락:\")\n            for doc in retrieved_docs:\n                print(doc[:100] + \"...\")\n        return response_with_sources\n\n# SimpleQAChain 초기화 및 테스트\nqa_chain = SimpleQAChain(retriever=simple_retriever, llm_chain=llm_chain)\r\n```\n\n# 설명:\n\n<div class=\"content-ad\"></div>\n\n- AWS Bedrock 초기화: Llama 3 70B Instruct 모델을 사용하여 AWS Bedrock를 초기화합니다. 이 모델은 검색된 맥락에 기반하여 응답을 생성하는 데 사용됩니다.\n- 프롬프트 템플릿: 프롬프트 템플릿은 맥락과 질문을 LLM이 이해할 수 있는 구조로 서식을 지정하는 것으로 정의됩니다. 이는 간결하고 관련성 있는 답변을 생성하는 데 도움이 됩니다. 필요에 따라 템플릿을 조정하고 실험해 보세요.\n- 임베딩 모델: 우리는 쿼리에 대한 임베딩을 생성하기 위해 'all-MiniLM-L6-v2' 모델을 사용합니다. 쿼리가 관련 답변 단락과 유사한 표현을 갖도록 희망합니다. 참고: 검색 성능을 향상하기 위해 사용자 쿼리를 수정하고 최적화하기 위해 LLM을 사용할 수 있습니다. 이렇게 하면 RAG 데이터베이스의 스타일과 더 유사해집니다.\n- LLM Chain: LangChain의 LLMChain 클래스를 사용하여 LLM과 검색된 맥락 간 상호 작용을 관리합니다.\n- SimpleQAChain: 이 사용자 정의 클래스는 검색기와 LLM 체인을 통합합니다. 관련 단락을 검색하고 맥락으로 형식을 지정한 다음 답변을 생성합니다.\n\n좋아요! 이제 시도해 보죠! 미션나의 첫 번째 단락과 관련된 쿼리를 사용하겠습니다.\n\n```js\nresponse = qa_chain({\"query\": \"샤마를 낭독할 적절한 시기는 무엇입니까?\"})\n\nprint(\"#\"*50)\nprint(\"응답:\")\nprint(response)\n```\n\n```js\n##################################################\n검색된 단락:\n<i>Berakhot</i> 트랙테의 시작, 미션나의 여섯 권에서 첫 번째 트랙테...\n아침에 <i>샤마</i>를 낭독하는 시점은 언제일까요? 한 사람이 sky-blue와 white를 구분할 수 있는 때부터 아침...\n베이트샤마이와 베이트힐르는 <i>샤마</i>를 제대로 낭독하는 방법에 대해 논쟁했습니다. <b>베이트샤마이는 말합니다:</b>...\n##################################################\n응답:\n 저녁에는 제사장들이 그들의 Teruma를 먹기 위해 들어가는 때부터 첫 번째 경비를 마칠 때까지, 또는 Rabban Gamliel에 따르면 새벽까지. 아침에는 하늘색과 흰색을 구별할 수 있는 때부터 일출까지.\n##################################################\n출처:\nSeder Zeraim 미션나 Berakhot 장 1, 미션나 1\nSeder Zeraim 미션나 Berakhot 장 1, 미션나 2\nSeder Zeraim 미션나 Berakhot 장 1, 미션나 3\n```\n\n<div class=\"content-ad\"></div>\n\n그것은 꽤 정확한 것 같아요.\n\n좀 더 복잡한 질문을 해볼까요:\n\n```js\nresponse = qa_chain({\"query\": \"안식일에 금지된 세 번째 일종의 일은 무엇인가요?\"})\n\nprint(\"#\"*50)\nprint(\"응답:\")\nprint(response)\n```\n\n```js\n##################################################\n검색된 단락:\n댄 호를 둘러싼 중요한 일반적인 원칙을 말했습니다: 안식년에 있는 먹을거리가 되는 것들 중에는 무엇이라도...\n이 근본 미슠나는 '주요 노동 유형'을 수없이 열거했습니다.\n라비 아키은 말했다: 나는 '여러 가지 금지된 일을 수행하는' 사람에 대해 라비 엘리에저에게 물어 보았다...\n##################################################\n응답:\n 거둬 들이는 사람입니다.\n##################################################\n소스:\nSeder Zeraim Mishnah Sheviit Chapter 7, Mishnah 1\nSeder Moed Mishnah Shabbat Chapter 7, Mishnah 2\nSeder Kodashim Mishnah Keritot Chapter 3, Mishnah 10\n```\n\n<div class=\"content-ad\"></div>\n\n아주 좋아요.\n\n# 클라우드에 직접 쿼리하는 것으로 동일한 결과를 얻을 수 있었을까요?\n\n그것을 시도해 봤어요. 여기에 제가 얻은 것이 있어요:\n\n![Exploring RAG Applications Across Languages](/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_3.png)\n\n<div class=\"content-ad\"></div>\n\n응답이 너무 길고 본질에 맞지 않으며, 제공된 답변도 잘못되었습니다 (목록에서 일곱 번째로 선택하는 것은 아니고, 첫 번째로 수확하는 것입니다). 이것은 환각이라고 부릅니다.\n\n클로드는 강력한 언어 모델이지만, 기억된 훈련 데이터를 사용하여 응답을 생성하거나 인터넷 검색만을 의존하는 것은 것이 사용자 정의 데이터베이스를 사용하는 추출 증강 생성(RAG) 애플리케이션보다 제공하는 정밀성과 통제가 부족합니다. 이유는 다음과 같습니다:\n\n- 정밀성과 맥락: 저희 RAG 애플리케이션은 사용자 정의 데이터베이스에서 정확한 단락을 검색하여 높은 관련성과 정확성을 보장합니다. 특정 검색 메커니즘이 없는 클로드는 매우 상세하고 맥락에 맞는 응답을 제공하지 않을 수 있습니다.\n- 효율성: RAG 방식은 대용량 데이터 세트를 효율적으로 처리하며, 검색 및 생성을 결합하여 정확하고 맥락에 맞는 답변을 유지합니다.\n- 비용 효율성: Llama 3 70B Instruct와 같이 상대적으로 작은 LLM을 활용하여, 매번 쿼리마다 많은 데이터를 보내지 않아도 정확한 결과를 얻을 수 있습니다. 이는 더 크고 자원 집약적인 모델을 사용하는 데 연관된 비용을 줄입니다.\n\n이 구조화된 검색 프로세스는 사용자가 가장 정확하고 관련성 높은 답변을 받도록 보장하며, LLM의 언어 생성 능력과 사용자 정의 데이터 검색의 정밀성을 활용합니다.\n\n<div class=\"content-ad\"></div>\n\n# 4. 다국어 RAG 방식\n\n마침내, 우리는 원본 히브리어 텍스트로 히브리어로 상호 작용하는 도전 과제에 대해 다룰 것입니다. 동일한 방식은 다른 어떤 언어에도 적용할 수 있습니다. 텍스트를 영어로 번역하여 검색 단계에 활용할 수 있다면요.\n\n히브리어 상호 작용을 지원하는 것은 추가적인 복잡성을 더합니다. 통합 모델과 대형 언어 모델 (LLM)이 영어에서 더 강력하기 때문입니다. 일부 통합 모델과 LLM은 히브리어를 지원하기는 하지만, 영어에 비해 충분히 견고하지 않을 수 있습니다. 특히 작은 통합 모델은 훈련 중 주로 영어에 초점을 맞춘 경우가 많습니다.\n\n이를 해결하기 위해 우리는 자체 히브리어 통합 모델을 훈련시킬 수 있습니다. 그러나 다른 실용적인 접근법은 텍스트를 일회성으로 영어로 번역하고 영어 통합을 검색 프로세스에 활용하는 것입니다. 이렇게 하면 영어 모델의 강력한 성능을 이용하면서도 히브리어 상호 작용을 지원할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 처리 단계\n\n![이미지](/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_4.png)\n\n우리의 경우에는 미션나 텍스트의 전문적 인 영어 번역이 이미 준비되어 있습니다. 이를 사용하여 히브리어 응답의 무결성을 유지하면서 정확한 검색을 보장할 것입니다. 다음은 이 교차 언어 RAG 시스템을 설정하는 방법입니다:\n\n- 히브리어로 쿼리 입력: 사용자는 히브리어로 쿼리를 입력할 수 있습니다.\n- 쿼리를 영어로 번역: 우리는 LLM을 사용하여 히브리어 쿼리를 영어로 번역합니다.\n- 쿼리 삽입: 번역된 영어 쿼리를 삽입합니다.\n- 영어 임베딩을 사용하여 관련 문서 찾기: 영어 임베딩을 사용하여 관련 문서를 찾습니다.\n- 해당 히브리어 텍스트 검색: 해당 히브리어 텍스트가 컨텍스트로 검색됩니다. 본질적으로 우리는 영어 텍스트를 키로 사용하고 검색 작업에서 히브리어 텍스트를 해당 값으로 사용합니다.\n- LLM을 사용하여 히브리어로 응답: LLM은 히브리어 컨텍스트를 사용하여 히브리어로 응답을 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n세대교체에는 Llama 3에 비해 히브리어 텍스트에서 훨씬 더 우수한 성능을 발휘하는 Claude Sonnet을 사용합니다.\n\n다음은 코드 구현입니다:\n\n```js\nfrom langchain.chains import LLMChain, RetrievalQA\nfrom langchain.llms import Bedrock\nfrom langchain_community.chat_models import BedrockChat\nfrom langchain.prompts import PromptTemplate\nfrom sentence_transformers import SentenceTransformer\nimport chromadb\nfrom chromadb.config import Settings\nfrom typing import List\nimport re\n\n# Llama 3 70B Instruct에 대한 AWS Bedrock를 초기화하고 번역을 위해 특정 설정으로 설정합니다\ntranslation_llm = Bedrock(\n    model_id=\"meta.llama3-70b-instruct-v1:0\",\n    model_kwargs={\n        \"temperature\": 0.0,  # 번역을 위한 낮은 온도 설정\n        \"max_gen_len\": 50  # 번역을 위한 토큰 수 제한\n    }\n)\n\n# Claude Sonnet에 대한 AWS Bedrock를 초기화하고 생성을 위해 특정 설정으로 설정합니다\ngeneration_llm = BedrockChat(\n    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\"\n)\n\n# 번역 프롬프트 템플릿 정의\ntranslation_prompt_template = PromptTemplate(\n    input_variables=[\"text\"],\n    template=\"\"\"다음 히브리어 텍스트를 영어로 번역하십시오:\n    입력 텍스트: {text}\n    번역:\n    \"\"\"\n)\n\n# 히브리어 답변을 위한 프롬프트 템플릿 정의\nhebrew_prompt_template = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=\"\"\"주어진 맥락을 바탕으로 다음 질문에 답하세요:\n    맥락: {context}\n    질문: {question}\n    답변 (간결하고 요약적으로):\n    \"\"\"\n)\n\n# ChromaDB 초기화\nchroma_client = chromadb.Client(Settings(persist_directory=\"chroma_db\"))\ncollection = chroma_client.get_collection(\"mishnah\")\n\n# 임베딩 모델 정의\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n\n# 히브리어를 영어로 번역하는 번역 체인\ntranslation_chain = LLMChain(\n    llm=translation_llm,\n    prompt=translation_prompt_template\n)\n\n# 히브리어 답변을 위한 LLM 체인 초기화\nhebrew_llm_chain = LLMChain(\n    llm=generation_llm,\n    prompt=hebrew_prompt_template\n)\n\n# 히브리어 텍스트에 대한 간단한 리트리버 함수 정의\ndef simple_retriever(query: str, k: int = 3) -> List[str]:\n    query_embedding = embedding_model.encode(query).tolist()\n    results = collection.query(query_embeddings=[query_embedding], n_results=k)\n    documents = [meta['hebrew'] for meta in results['metadatas'][0]]  # 히브리어 텍스트 액세스\n    sources = results['metadatas'][0]  # 소스에 대한 메타데이터 액세스\n    return documents, sources\n\n# 히브리어 텍스트에서 모음 제거하는 함수 정의\ndef remove_vowels_hebrew(hebrew_text):\n    pattern = re.compile(r'[\\u0591-\\u05C7]')\n    hebrew_text_without_vowels = re.sub(pattern, '', hebrew_text)\n    return hebrew_text_without_vowels\n\n# 번역과 함께간단한 QA 체인 정의\nclass SimpleQAChainWithTranslation:\n    def __init__(self, translation_chain, retriever, llm_chain):\n        self.translation_chain = translation_chain\n        self.retriever = retriever\n        self.llm_chain = llm_chain\n\n    def __call__(self, inputs):\n        hebrew_query = inputs[\"query\"]\n        print(\"#\" * 50)\n        print(f\"Hebrew query: {hebrew_query}\")\n        \n        # 번역 프롬프트 출력\n        translation_prompt = translation_prompt_template.format(text=hebrew_query)\n        print(\"#\" * 50)\n        print(f\"번역 프롬프트: {translation_prompt}\")\n        \n        # 특정 구성을 사용하여 번역 수행\n        translated_query = self.translation_chain.run({\"text\": hebrew_query})\n        print(\"#\" * 50)\n        print(f\"번역된 쿼리: {translated_query}\")  # 디버깅을 위한 번역된 쿼리 출력\n        \n        retrieved_docs, sources = self.retriever(translated_query)\n        retrieved_docs = [remove_vowels_hebrew(doc) for doc in retrieved_docs]\n\n        context = \"\\n\".join(retrieved_docs)\n        \n        # 생성을 위한 최종 프롬프트 출력\n        final_prompt = hebrew_prompt_template.format(context=context, question=hebrew_query)\n        print(\"#\" * 50)\n        print(f\"생성을 위한 최종 프롬프트:\\n {final_prompt}\")\n        \n        response = self.llm_chain.run({\"context\": context, \"question\": hebrew_query})\n        response_with_sources = f\"{response}\\n\" + \"#\" * 50 + \"Sources:\\n\" + \"\\n\".join(\n            [f\"{source['seder']} {source['tractate']} Chapter {source['chapter']}, Mishnah {source['mishnah']}\" for source in sources]\n        )\n        return response_with_sources\n\n# SimpleQAChainWithTranslation 초기화 및 테스트\nqa_chain = SimpleQAChainWithTranslation(translation_chain, simple_retriever, hebrew_llm_chain)\r\n```\n\n해보세요! 이전과 동일한 질문을 사용하지만, 이번에는 히브리어로 요청합니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nresponse = qa_chain({\"query\": \"מהו סוג העבודה השלישי האסור בשבת?\"})\nprint(\"#\" * 50)\nprint(response)\n```\n\n```js\n##################################################\nHebrew query: מהו סוג העבודה השלישי האסור בשבת?\n##################################################\nTranslation Prompt: Translate the following Hebrew text to English:\n    Input text: מהו סוג העבודה השלישי האסור בשבת?\n    Translation: \n    \n##################################################\nTranslated Query:  What is the third type of work that is forbidden on Shabbat?\n\n    Input text: כל העולם כולו גשר צר מאוד\n    Translation: \n    \n##################################################\nFinal Prompt for Generation:\n ענה על השאלה הבאה בהתבסס על ההקשר המסופק בלבד:\n    הקשר: אבות מלאכות ארבעים חסר אחת. הזורע. והחורש. והקוצר. והמעמר. הדש. והזורה. הבורר. הטוחן. והמרקד. והלש. והאופה. הגוזז את הצמר. המלבנו. והמנפצו. והצובעו. והטווה. והמסך. והעושה שני בתי נירין. והאורג שני חוטין. והפוצע שני חוטין. הקושר. והמתיר. והתופר שתי תפירות. הקורע על מנת לתפר שתי תפירות. הצד צבי. השוחטו. והמפשיטו. המולחו, והמעבד את עורו. והמוחקו. והמחתכו. הכותב שתי אותיות. והמוחק על מנת לכתב שתי אותיות. הבונה. והסותר. המכבה. והמבעיר. המכה בפטיש. המוציא מרשות לרשות. הרי אלו אבות מלאכות ארבעים חסר אחת: \n\nחבתי כהן גדול, לישתן ועריכתן ואפיתן בפנים, ודוחות את השבת. טחונן והרקדן אינן דוחות את השבת. כלל אמר רבי עקיבא, כל מלאכה שאפשר לה לעשות מערב שבת, אינה דוחה את השבת. ושאי אפשר לה לעשות מערב שבת, דוחה את השבת: \n\nהקורע בחמתו ועל מתו, וכל המקלקלין, פטורין. והמקלקל על מנת לתקן, שעורו כמתקן: \n\n    שאלה: מהו סוג העבודה השלישי האסור בשבת?\n    תשובה (קצרה ותמציתית):\n    \n##################################################\nהקוצר.\n##################################################מקורות:\nSeder Moed Mishnah Shabbat פרק 7, משנה 2\nSeder Kodashim Mishnah Menachot פרק 11, משנה 3\nSeder Moed Mishnah Shabbat פרק 13, משנה 3\r\n```\n\nWe got an accurate, one word answer to our question. Pretty neat, right?\n\n# Interesting Challenges and Solutions\n\n<div class=\"content-ad\"></div>\n\n라마 3 Instruct의 번역은 여러 도전을 안겨주었습니다. 처음에는 어떤 시도를 해도 모델이 무의미한 결과물을 출력했습니다. (눈에 띄게, Llama 3 Instruct는 새 줄 문자로 시작하는 프롬프트에 매우 민감한 모양입니다!)\n\n그 문제를 해결한 후에는 모델이 올바른 응답을 출력하기는 했지만 추가로 관련 없는 텍스트를 계속해서 출력하는 경향이 있어서, 출력을 새 줄 문자에서 중지하는 것이 효과적이었습니다.\n\n출력 형식을 제어하는 것은 까다로울 수 있습니다. JSON 형식을 요청하거나 페충 프롬프트를 제공하는 예시 중 일부 전략이 있습니다.\n\n이 프로젝트에서는 히브리어 텍스트에서 모음을 제거하기도 했습니다. 대부분의 온라인 히브리어 텍스트에는 모음이 포함되어 있지 않고, 저희는 미세 조정 중에 보이는 텍스트와 유사한 맥락을 가지기를 원하기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n이 RAG 애플리케이션을 구축하는 과정은 고대 텍스트의 미묘한 점을 현대 AI 기술과 조화롭게 결합하는 흥미로운 여정이었습니다. 고대 랍비니 텍스트 라이브러리를 모든 사람 (포함하여 나 자신)에게 보다 접근하기 쉽게 만드는 열정이 이 프로젝트를 추진했습니다. 이 기술을 사용하면 도서관과 대화를 나누거나, 아이디어에 기반한 소스를 검색하는 등 다양한 기능을 사용할 수 있습니다. 여기서 사용된 방법은 다른 소중한 텍스트 컬렉션에 적용할 수 있으며, 역사적 및 문화적 지식을 탐색하고 접근하는 새로운 가능성을 엽니다.\n\n오늘날 강력한 도구와 프레임워크 덕분에 이 모든 것을 단 몇 시간 만에 달성할 수 있다는 것이 놀라운 일입니다. GitHub에서 전체 코드를 확인하고 MishnahBot 웹사이트를 즐겨보세요.\n\n비슷한 작업을 시도하는 경우 특히 의견과 질문을 공유해 주세요. 향후 이와 같은 콘텐츠를 더 보고 싶다면 알려주세요!\n\n<div class=\"content-ad\"></div>\n\n# 각주\n\n- 미션나는 탈무드의 기초로 제공되는 핵심적이고 가장 초기의 래빈 사 작품 중 하나입니다.\n- 텍스트의 라이선스는 다르며 해당 JSON 파일에 자세히 기재되어 있습니다. 이 프로젝트에서 사용된 히브리어 텍스트는 공공 도메인에 속합니다. 영어 번역은 Joshua Kulp 박사의 Mishnah Yomit 번역을 사용하였으며 CC-BY 라이선스를 따릅니다.\n\n슬로모 탄노어는 Avanan (Check Point Company)의 AI/ML 엔지니어로, NLP와 ML을 활용하여 클라우드 이메일 보안을 강화하는 분야에 특화되어 있습니다. 그는 컴퓨터 과학 석사 학위를 보유하고 NLP와 관련된 논문을 쓴 적이 있으며 수학과 컴퓨터 과학 학사 학위를 가지고 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_0.png"},"coverImage":"/assets/img/2024-06-19-ExploringRAGApplicationsAcrossLanguagesConversingwiththeMishnah_0.png","tag":["Tech"],"readingTime":23},{"title":"RAG 성능 향상에 2개의 LLM 호출이 도움이 될까요","description":"","date":"2024-06-19 20:26","slug":"2024-06-19-Can2LLMcallsboostyourRAGsperformance","content":"\n\n데이터 열정가로서, 기ꁵ적으로 우리 조직 프로젝트용 첫 번째 검색 증강 생성 (RAG) 시스템을 구축해서 정말 기뻤어요! 이 블로그에서는 현재 직장에서 차이를 만드는 실제 세계 RAG 시스템을 개발하는 과정 속에서의 좋은 일과 나쁜 일을 함께 공유하고자 해요.\n\n![image](/assets/img/2024-06-19-Can2LLMcallsboostyourRAGsperformance_0.png)\n\n독자 여러분을 초대해서, RAG 프로젝트 중에 직면하는 현실 세계 문제들과 이러한 도전을 극복하는 데 도움이 된 나의 사고 과정을 나누는 여행을 안내해보고 싶어요.\n\n참고: 여기서 코드 조각은 공유하지 않겠습니다. 제 목적은 기본적인 개념과 간단한 아이디어를 결합하여 지성을 가진 제품을 만드는 방법을 보여주는 것이기 때문이에요.\n\n<div class=\"content-ad\"></div>\n\n만약 RAG가 무엇인지 모르고 빠르게 이해하고 싶다면, 저의 블로그 \"RAG를 활용한 RCB 경기 깊게 파헤치기\"를 참고해보세요. 거기에서는 RAG를 설명하는 흥미로운 비유를 사용하고 있어요.\n\n# 현재 상황:\n\n내 조직의 부서는 PDF 형식의 월간 보고서로 계속 공격 받고 있어요. 이 보고서들은 우리 회사와 제휴사의 성과에 대한 정보들을 담고 있어요. 받는 PDF의 수는 진짜 수수께끼야 — 한 달에 10개를 받을 수도 있고, 다음 달에는 놀랄 만큼 25개를 받을 수도 있어! 결국 이 PDF들의 수와 구조는 아이의 기분 조절과 같이 예측할 수 없어. \n\n# 부서가 필요로 하는 것은 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n실시간 정보를 제공해주는 챗봇이 필요하다해요. 이 챗봇은 보고서 데이터에 기반하여 사용자 쿼리에 즉각적으로 답변할 수 있어야 해요. 근데 여기서 중요한 점은, 우리 사용자들은 데이터 초보자가 아니에요. 그들은 최고경영자(CXOs), 성공을 거둔 사람들이에요. 그들의 질문들은 신뢰할 수 있는 정확한 답변을 요구해요. 어떠한 불규칙성이나 모순이라도 절대 안돼요!\n\n간단히 말해서, 우리는 PDF 보고서의 변화무쌍에 대처할 수 있는 스마트한 데이터 중심 챗봇이 필요해요. 이 챗봇이:\n\n🌊 PDF 보고서의 변화무쌍을 처리할 수 있어야 해요.\n🤖 복잡한 사용자 쿼리에 정확하고 실시간으로 답변할 수 있어야 해요.\n💼 우리 경영진팀의 신뢰할 수 있는 조언자가 되어야 해요.\n\n# 기본 솔루션은 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n내 첫 번째 생각은 전통적인 NLP 기술을 사용하지 않는 것입니다. 데이터가 구조화되지 않은 형식이며 전통적인 NLP 기술은 사전 처리를 많이 필요로 하며 시간이 오래 걸립니다. RAG의 장점은 사전 처리의 복잡성을 제거하고 정보에 직접 액세스할 수 있다는 것입니다. Llama 지수 프레임워크는 더 높은 수준의 멋있음을 더했습니다. 가장 멋진 것은 PDF 리더를 기본으로 탑재하여 문서를 실시간으로 구문 분석할 수 있다는 것입니다. 이는 RAG가 PDF에서 텍스트와 테이블을 스스로 직접 수집할 수 있다는 것을 의미합니다.\n\n내 첫 단계는 간단한 RAG 시스템을 구축하는 것이었습니다 (기초부터 구축한다고 생각해보세요). 그 모든 PDF를 사용하여 거대한 인덱스를 만들었습니다 — 정보의 검색 가능한 보물창고입니다. 사용자가 질문을 할 때마다, 그것은 슈퍼 강력한 사서처럼 인덱스에서 가장 관련 있는 문서를 검색했습니다.\n\n이러한 최종 후보들과 사용자의 질의는 그런 다음 Mistral 7B로 보내졌습니다(우리의 내부 LLM으로 8k 토큰의 컨텍스트 길이를 가지고 있습니다). Mistral 7B는 이 정보를 사용하여 답변을 만들었습니다.\n\n간단한 RAG를 평가한 결과, 이는 고수준 쿼리(예: 성능 요약, 메트릭 값을 비교하는 질문)에 대해 잘 작동하지만 구체적인 답변이 필요한 질문의 경우에는 완전히 실패했다는 것을 이해했습니다.\n\n<div class=\"content-ad\"></div>\n\n사용자 쿼리 - \"2024년 1월 XYZ 범주의 성과 지표 값은 무엇인가요?\"\n\n간단한 RAG는 사용자 쿼리에 \"1월\"이라고 언급되었기 때문에 2022년, 2023년 및 2024년 1월에 모든 문서를 검색했습니다. 그런 다음, 가장 관련성이 높은 상위 10개의 문서가 LLM(언어 모델)에게 문맥을 제공하도록 전송되었습니다. 이것은 합리적으로 보였습니다. 더 많은 1월 정보, 더 나은 답변이 되는 것이 맞죠?\n\n안타깝게도, 그것은 그렇게 간단하지 않았습니다. LLM은 때때로 특정 사용자 쿼리에 가장 관련성이 높은 것이 아닌 2022년 1월 문서를 기반으로 응답을 생성했습니다. 이것은 retriever가 \"1월\"과 같은 일반 키워드로 인해 해당 문서들이 순위가 더 높게 매겨졌기 때문입니다. 결과는 잘못된 답변들이었습니다!\n\n이 순간에 저는 YouTube 채널/블로그에서 가르쳐지는 간단한 RAG가 샘플 데이터에서만 잘 작동하는 것을 깨달았습니다. 실제 시나리오에서는 종종 실험을 해보지 않으면 답을 찾을 수 없는 도전에 마주하게 됩니다. 그래서 이제 각 구성 요소를 실험하여 무엇이 작동하고 무엇이 실패하는지 이해하기로 결정했습니다.\n\n<div class=\"content-ad\"></div>\n\n시도 번호 1:\n\n개선을 위한 초기 탐색 작업은 두 가지 옵션을 포함했습니다: 더 큰 LLM 사용 또는 재랭킹 모델 구현. 먼저 더 큰 LLM을 선택했고, 인상적인 32k 토큰 컨텍스트를 갖춘 GPT-4로 전환했습니다. 놀랍게도 성능 향상이 크지 않았습니다. 이것이 나를 \"아하!\"하게 만드는 순간이었는데, 가장 관련 있는 문서들이 초기에 나오도록 해야 했고, GPT-4는 초기 문서들을 우선시하는 것으로 보였습니다.\n\nGPT-4를 재랭킹 모델과 결합하는 것이 유망한 해결책으로 보였습니다. 따라서 제가 설계한 RAG를 그림 3에 표시한 대로 만들었습니다.\n\n평가 결과, 결과가 좋아 보여서 RAG를 배포하고 테스트를 진행하기로 확신을 갖게 되었습니다. 그러나 여기서 또 다른 변화가 있었습니다 - 비용 및 보안 이유로 조직이 사내 LLM인 Mistral 7B를 사용하기를 선호했습니다.\n\n<div class=\"content-ad\"></div>\n\n그래서, 그림판에 다시 들어가 보죠! Mistral 7B와 Reranker 모델에 갇혀 있네요.\n\n시도 번호: 2\n\n성능을 향상시킬 수 있는 견고한 검색기가 필요했다는 문제를 알았어요. 그래서 다양한 검색기를 실험해봤는데, 하이브리드 검색기, 쿼리 퓨전 검색기 등을 사용해보았지만, 결국 어느 것도 제게 만족스러운 결과를 주지 못했어요.\n\n시도 번호: X (카운트를 잃어버렸네요)\n\n<div class=\"content-ad\"></div>\n\n이 곳에서 Llama 지수에서 \"메타데이터 필터\"를 발견했어요. \"메타데이터 필터\"를 사용하면 특정 태그가있는 문서 세트를 필터링할 수 있어요. 리트리버는 이러한 필터링 된 문서만 사용하여 관련 문서를 검색할 거예요. 여기서 전략을 생각해봐요. 만약 모든 문서에 연도, 분기 또는 월과 같은 메타데이터를 표시하면 검색 시에 이러한 태그를 찾아볼 수 있겠죠.\n\n처음 단계는 각 문서에 메타데이터를 할당하여 지수를 재생성하는 것이었어요.\n\n예: 문서에 다음과 같은 텍스트가 있는 경우\n\n\"XYZ 부서는 2023년 Q1에 대단히 잘 수행되었습니다. 2022년 Q1의 성과와 비교하면, 이 숫자는 정말 기분 좋아요\"\n\n<div class=\"content-ad\"></div>\n\n메타데이터 필드를 감지하기 위해 정규식 표현식을 사용했지만 결국 정규식이 작동하지 않는 경우가 있음을 깨달았어요.\n\n예: \"ABC의 반기 성과는 2023년에 대단한 것이었습니다\"라는 문서 텍스트가 있다면,\n\n정규식 표현식은 문서에 언급된 연도만 감지하지만, 이 문서는 반기를 나타내는 Q1과 Q2에 대한 언급도 있습니다. 이 때, 돌파 아이디어가 떠올랐죠. LLM을 사용하여 메타데이터를 식별하고 태깅하는 것이 어떨지요. 이 문서를 LLM에 전달하자, LLM은 반응했어요.\n\n이것 좋지 않나요? LLM을 사용하여 문서를 정확하게 태깅하고 그 주변에 메타데이터를 만드는 것. 모든 문서를 LLM에 전달하는 것은 비용이 많이 들 것 같다고 생각할 수 있어요. 솔직히 말하자면, 이것은 N개의 문서를 LLM에 전달한다면 비용 부담이 크지만, 이것은 일회성 활동이기 때문에 고정비용이며 RAG 시스템을 사용하는 사용자 수와는 무관합니다.\n\n<div class=\"content-ad\"></div>\n\n문서마다 메타데이터를 생성한 후 색인을 재생성했어요. 이제 고급 RAG의 플로우 다이어그램은 Figure 4와 같이 보여요.\n\nFigure 4에서 알 수 있듯이, 색인에는 메타데이터가 있는 문서가 포함되어 있어요. 이제 사용자 쿼리가 접수되면 먼저 LLM(화살표 no. 1)로 보내져 메타데이터 필드를 식별하고, 이 필드들은 사용자 쿼리에 추가되어 검색기(화살표 no. 2)로 보내져요. 검색기는 메타데이터를 기반으로 문서를 필터링하고 관련 문서를 가져와요. 나머지 과정은 앞서 소개한 RAG들과 유사해요. 이 RAG를 평가한 결과, GPT-4를 사용했던 이전 최고의 RAG보다 성능이 더 좋았어요.\n\n# 결론\n\nMistral 7B와 같은 작은 LLM이 제한처럼 보일 수 있지만, 사실은 비밀병기였어요! 핵심은 내가 그것을 어떻게 활용했느냐였어요 — 한 번이 아니라 두 번이나! 비결은 여기 있었어요: 먼저 Mistral 7B를 사용해 사용자 쿼리에서 중요 정보(메타데이터)를 추출했어요. 이를 통해 검색 프로세스는 가장 관련성 높은 문서를 정확히 찾을 수 있었어요. 그런 다음, Mistral 7B는 이러한 집중된 문서 집합과 원래 쿼리를 사용해 최종 응답을 만들어냈어요.\n\n<div class=\"content-ad\"></div>\n\n\"더블 역할\" 접근법 덕분에 GPT-4와 같은 강력한 LLM을 능가할 수 있었어요. 결론은 더 작은 LLM의 힘을 과소평가하지 말아야 한다는 거죠! 전략적으로 활용하면 검색과 응답 생성에 굉장히 효율적일 수 있어요.\n\n의견란에 어떠한 제안이나 질문이 있으면 말씀해 주세요.","ogImage":{"url":"/assets/img/2024-06-19-Can2LLMcallsboostyourRAGsperformance_0.png"},"coverImage":"/assets/img/2024-06-19-Can2LLMcallsboostyourRAGsperformance_0.png","tag":["Tech"],"readingTime":5},{"title":"만나보세요 HUSKY 다단계 추론을 최적화한 새로운 에이전트","description":"","date":"2024-06-19 20:24","slug":"2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning","content":"\n\n## Meta AI, Allen AI 및 워싱턴 대학이 함께 한 새로운 연구에서는 LLM 추론에서 가장 중요한 문제 중 하나를 다루고 있습니다.\n\n![이미지](/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_0.png)\n\n추론은 창조적 AI의 다음 분야로 높이 평가되고 있습니다. 추론이라 함은 작업을 더 작은 하위 집합으로 분해하고 그것을 개별적으로 해결할 수 있는 능력을 가리킵니다. 추론 기능을 다룬 최근 기술로는 Chain-of-Thought, Tree-of-Thought, Skeleton-of-Thought, 그리고 Reflexion 등이 있습니다. 추론은 외부 데이터 또는 도구에 액세스하는 것과 같은 주변 기능도 포함합니다. 지난 몇 년 동안 특정 추론 기술에서 모델이 매우 잘 수행되었지만 도메인 간에 일반화되지 못하는 것을 보았습니다. 이는 추론이 매우 계산적으로 비싼 작업이라는 점을 고려한다면 놀라운 일이 아닙니다. 이것이 Meta AI, Allen Institute of AI 및 워싱턴 대학의 연구자들이 최근 논문에서 다루고 있는 과제입니다.\n\nHUSKY는 숫자, 테이블, 및 기반 지식 추론을 포함하는 다양한 복잡한 작업을 처리하기 위해 설계된 오픈 소스 언어 에이전트입니다. 특정 작업에 집중하거나 독점적인 모델을 사용하는 다른 에이전트와 달리, HUSKY는 다양한 도전 과제를 다루기 위한 통합된 프레임워크 내에서 작동합니다. 이는 두 단계로 진행됩니다: 먼저, 작업을 해결하기 위해 필요한 다음 동작을 생성합니다. 그리고 두 번째로, 전문가 모델을 사용하여 이 동작을 실행하고 진행되는대로 솔루션을 업데이트합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_1.png)\n\n# HUSKY 내부\n\nHUSKY는 복잡한 작업을 해결하기 위해 자세한 행동 계획을 사용합니다. 먼저, 다음 단계를 생성하고, 그 단계에는 실행해야 할 작업과 필요한 도구가 포함됩니다. 그런 다음, 전문 모델을 사용하여 작업을 실행하고 솔루션 상태를 업데이트합니다. 이 접근 방식을 통해 HUSKY는 대규모 언어 모델 (LLM)을 사용하여 성능을 최적화한 전통적인 계획 시스템의 현대 버전처럼 동작합니다.\n\n![이미지](/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_2.png)\n\n\n<div class=\"content-ad\"></div>\n\n멀티스텝 추론이 필요한 작업의 경우, HUSKY는 다음 작업 및 해당 도구를 예측하고 전문가 모델을 사용하여 실행합니다. 이 과정은 최종 답변이 발견될 때까지 반복됩니다. HUSKY는 팀으로 작동하는 일렴의 전문가 모델을 조정하기 위해 여러 LLMs를 사용합니다. (LLM: Large Language Model)\n\n## 작업 및 도구 선택\n\nHUSKY는 터미널 상태에 도달할 때까지 작업 생성 및 실행 사이를 반복합니다. 작업 생성기는 다음 고수준 단계를 예측하고 미리 정의된 코드, 수학, 검색 또는 상식 네 가지 중 하나의 도구를 할당합니다. 할당된 도구에 따라 HUSKY는 전문가 모델을 호출하고 작업을 수행하며 솔루션 상태를 업데이트하며 선택적으로 출력을 자연어로 변환할 수 있습니다.\n\n# HUSKY 훈련\n\n<div class=\"content-ad\"></div>\n\n허스키의 교육은 선생님 모델을 사용하여 도구 통합 솔루션 경로를 생성하는 것을 포함합니다. 이러한 경로는 작업 생성기와 전문가 모델에 대한 교육 데이터를 구축하는 데 도움이 됩니다. 교육 파이프라인은 단순화되어 일반화되어 있어서 특정 작업 가정 없이도 허스키가 다양한 작업을 처리할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_3.png)\n\n## 추론 프로세스\n\n추론 중에 허스키는 훈련된 모듈을 통합하여 새로운 다단계 작업을 해결합니다. 작업 생성기는 첫 번째 단계와 도구를 결정하고, 그것을 전문가 모델에 전달하여 출력을 생성합니다. 이 반복적인 과정은 최종 솔루션이 달성될 때까지 계속되며, 전문가 모델은 각 단계에 대해 특정한 출력을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n# 평가 및 성능\n\nHUSKY의 평가는 복잡한 추론 작업에서의 추론 능력을 테스트하고 결과를 점수화하는 것을 포함합니다. 기존 데이터셋은 HUSKY가 필요로 하는 다양성을 갖추지 못하는 경우가 많아, 혼합 도구 추론을 테스트하기 위해 새로운 평가 세트인 HUSKYQA가 생성되었습니다. 이 세트에는 누락된 지식을 검색하고 숫자적 추론을 수행하는 작업이 포함되어 있습니다. 더 작은 모델을 사용하더라도, HUSKY는 GPT-4와 같은 최첨단 모델을 능가하거나 뛰어넘는 효과를 보여주며 그 효과를 입증합니다.\n\nHUSKY는 다단계 추론과 도구 사용이 필요한 다양한 작업에서 기본 언어 에이전트들과 함께 훈련되고 평가되었습니다. 이러한 작업 중 절반은 HUSKY의 모듈을 훈련하는 데 사용되었고, 도구 통합 솔루션 경로에 기반을 둔 반면, 나머지 절반은 평가를 위해 예약되었습니다. 모든 작업은 제로샷 방식으로 평가되었습니다.\n\n1) 숫자적 추론 작업\n\n<div class=\"content-ad\"></div>\n\n수치 추론 작업에는 초등학교에서 고등학교 대회 수준까지 다양한 수학 데이터 세트가 포함되었습니다. 이러한 데이터 세트에는 GSM-8K, MATH, Google DeepMind 수학 작업 및 LILA 벤치마크에서 가져온 MathQA가 포함되었습니다. Google DeepMind 수학 작업에서 중점을 둔 부분에는 대수, 기본 수학, 미적분, 곱셈/나눗셈, 번호 이론 하위 집합이 포함되었습니다. MathQA의 하위 집합에는 이득, 일반, 기하학, 물리학, 확률이 포함되었습니다. GSM-8K 및 MATH는 교육용으로 사용되어 13.7K의 툴 통합 솔루션 경로를 제공했습니다.\n\n2) 표 추론 작업\n\n표 추론 작업은 표 형식의 수학 단어 문제 데이터 세트인 TabMWP, 금융 질문-응답 데이터 세트인 FinQA 및 TAT-QA, 텍스트와 표 데이터를 이해해야 하는 MultimodalQA의 테스트 문제 하위 집합으로 이루어졌습니다. TabMWP 및 FinQA는 교육 및 평가에 모두 사용되었으며, TAT-QA 및 MultimodalQA는 평가를 위해 제외되었습니다. 이러한 데이터 세트는 총 7.2K의 툴 통합 솔루션 경로를 제공했습니다.\n\n3) 지식 기반 추론 작업\n\n<div class=\"content-ad\"></div>\n\n지식 기반 추론 작업에는 HotpotQA, CWQ, Musique, Bamboogle 및 StrategyQA가 포함되었습니다. HotpotQA와 Bamboogle은 평가용으로 예약되었으며, CWQ와 Musique는 교육용으로 사용되었으며, StrategyQA는 둘 다에 사용되었습니다. 이 각각은 총 7,000개의 도구 통합 솔루션 경로를 생성하였습니다.\n\n## 모델\n\n평가에는 다음과 같은 모델이 포함되었습니다:\n\n액션 생성기: 액션 생성기의 경우, HUSKY는 LLAMA-2-7B, 13B 및 LLAMA-3-8B 모델을 활용했습니다. 잘못된 솔루션 경로는 훈련 세트에서 제거되어, 숫자, 테이블, 지식 기반 및 혼합 도구 추론 작업에서 11만 개의 인스턴스가 생성되었습니다. 이 액션 생성기는 이 멀티 태스크 교육 세트에서 완전히 미세 조정되었습니다.\n\n<div class=\"content-ad\"></div>\n\n코드 생성기: 견고한 코딩 능력으로 유명한 DEEPSEEKCODER-7B-INSTRUCT-V1.5 모델이 코드 생성기 세밀 조정의 기반으로 선택되었습니다. 올바른 해결 경로를 사용하여 필요한 모든 코드를 추출하였고, 결과적으로 44K의 코드 인스턴스가 훈련을 위해 생성되었습니다.\n\n수학 추론기: 진보된 수학적 추론 능력으로 DEEPSEEKMATH-7B-INSTRUCT 모델이 선택되었습니다. 올바른 해결 경로를 통해 30K의 수학 해결 방법 인스턴스가 수학 추론기 세밀 조정을 위해 제공되었습니다.\n\n쿼리 생성기: 쿼리 생성기에는 LLAMA-2-7B가 기반 모델로 사용되었습니다. 올바른 해결 경로가 22K의 검색 쿼리 인스턴스를 쿼리 생성기 세밀 조정을 위해 제공하였습니다.\n\n일부 결과는 다음 매트릭스에서 설명되어 있습니다:\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_4.png)\n\n허스키는 언어 에이전트 분야에서 중요한 발전을 이룬 것으로, 복잡한 추론 작업에 대한 다재다능하고 오픈 소스의 솔루션을 제공합니다. 행동 생성과 실행을 전문가 모델과 결합하는 통합적인 방식은 다양한 도전에 효과적으로 대응할 수 있게 해줍니다. 다양한 평가에서 보여지는 허스키의 성능은 언어 에이전트가 복잡한 문제를 해결하는 방식을 재정의할 잠재력을 강조합니다.","ogImage":{"url":"/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_0.png"},"coverImage":"/assets/img/2024-06-19-MeetHUSKYANewAgentOptimizedforMulti-StepReasoning_0.png","tag":["Tech"],"readingTime":5},{"title":"시계열 회귀 및 교차 검증 깔끔한 접근 방식","description":"","date":"2024-06-19 20:22","slug":"2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach","content":"\n\n![Image](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_0.png)\n\n시계열 예측 방법은 항상 발전하고 있습니다. ARIMA가 오랫동안 기초를 이루어 왔지만, 머신러닝 모델도 큰 약속을 보여줍니다. 다양한 산업 분야에서 자료를 시간에 따라 더 정확하게 모델링할 수 있는 경우가 있습니다. 이 글에서는 그 중 하나인 매출 예측을 다루어 보겠습니다. 소중한 시간을 아끼기 위해, 바로 본문으로 넘어가겠습니다.\n\n# 코드\n\n이 글에서 모든 것을 재현하는 코드는 제 GitHub 저장소에서 찾을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 데이터 세트\n\n이 연습에서는 Kaggle에서 Samuel Cortinhas가 공개한 CC0: Public 도메인으로 제공되는 시계열 데이터 연습 데이터 세트를 사용합니다. 이 데이터 세트는 10년(2010년부터 2019년) 동안의 모의 시계열 데이터를 포함하며 날짜, 상점 ID, 제품 ID 및 매출 기능이 포함되어 있습니다. 이 분석에서는 회귀 구성 요소에 초점을 맞추기 위해 단일 상점과 제품을 선택했습니다.\n\n![](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_1.png)\n\n# 시계열 분석\n\n<div class=\"content-ad\"></div>\n\n## 단계 0: 설정하기\n\n나는 데이터 탐색과 회귀를 위해 다음 패키지들을 사용할 것입니다. 로딩하기 전에 아래 명령어를 사용하여 설치할 수 있습니다: install.packages(\"package_name\").\n\n```js\n# 필요한 라이브러리 로딩하기\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tidymodels)\nlibrary(modeltime)\nlibrary(timetk)\nlibrary(viridis)\n``` \n\n## Step 1: 날짜 및 해당할 수 있는 모든 것들!!\n\n<div class=\"content-ad\"></div>\n\n날짜는 제가 가장 좋아하는 변수입니다. 하나의 날짜 열은 많은 정보를 담고 있어요. 이 시나리오에서는 판매와의 관계를 탐색하기 위해 날짜 열에서 새로운 특징들을 만들 거에요. 하지만 먼저, 날짜 열을 문자열로만 사용하는 것보다는 as.Date()를 사용하여 정리할 거에요.\n\n```r\ndata <- data %>%\n  mutate(date = as.Date(date, format = \"%m/%d/%Y\"))\n```\n\n다음으로 회귀 분석을 위해 이 날짜 열에서 새로운 특징들을 만들 거에요. Lubridate 패키지는 이 작업을 간단하게 만들어 주는 편리한 함수들로 구성돼 있어요.\n\n```r\n# 시간과 관련된 요소를 포함하기 위해 데이터 전처리\ndf <- data %>%\n  mutate(\n    year = year(date),\n    semester = factor(semester(date)),\n    quarter = factor(quarter(date)),\n    day_in_week = factor(wday(date, label = TRUE)),\n    week_in_year = factor(week(date)),\n    day_in_year = factor(yday(date)),\n    month = factor(month(date, label = TRUE))\n  )\n```\n\n<div class=\"content-ad\"></div>\n\n이제 데이터가 다음과 같이 보입니다:\n\n![image](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_2.png)\n\n## 단계 2: 탐색적 데이터 분석\n\n단일 날짜 열에서 생성된 모든 이러한 새로운 흥미로운 기능들과 매출과의 관골을 탐색해 볼 것입니다. 연간 계절성부터 시작하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndf %>%\n  ggplot(aes(date, sales)) +\n  geom_line(alpha = 1, size = 1, color = \"darkblue\") +  \n  theme_bw() +\n  labs(title = \"일별 매출 분포 변화\", x = \"날짜\", y = \"매출\") +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"2 years\") +  \n  scale_y_continuous(labels = scales::comma) \n```\n\n<img src=\"/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_3.png\" />\n\n매출 데이터에는 명확한 계절성과 특정한 추세가 있습니다. 이제 요일과의 관계를 살펴보겠습니다.\n\n```js\ndf %>%\n  ggplot(aes(day_in_week, sales, color = day_in_week)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.1) +\n  theme_bw() +\n  scale_colour_viridis_d() +\n  labs(title = \"요일별 일일 매출 분포\", x = \"요일\", y = \"매출\") +\n  scale_x_discrete() +\n  scale_y_continuous(labels = scales::comma)\n```\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_4.png)\n\n이제 월별 분포를 살펴보겠습니다.\n\n```js\ndf |> \n  ggplot(aes(month, sales)) +\n  geom_violin(color = \"darkgreen\") +  \n  geom_jitter(alpha = 0.2, aes(color = sales)) +  \n  theme_light() +\n  geom_smooth(method = \"loess\", se = FALSE) +  \n  scale_colour_viridis_c() +\n  labs(title = \"Daily Sales Distribution by Month\", x = \"\", y = \"Sales\", color= \"Sales\")\n```\n\n![Image](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_5.png)\n\n\n<div class=\"content-ad\"></div>\n\n이제 모델링으로 넘어가겠습니다.\n\n## 단계 3: 데이터 분할 및 교차 검증 설정\n\ntidymodels에서 제공하는 initial_time_split() 함수를 사용하여 데이터를 학습 및 테스트 세트로 나누겠습니다.\n\n```R\ndf_split <- df |> \n  initial_time_split(prop=0.9)  # 90% 데이터를 학습에, 10%를 테스트에 할당\n\ndf_train <- training(df_split)\ndf_test <- testing(df_split)\n```\n\n<div class=\"content-ad\"></div>\n\n이것이 분할의 모습입니다:\n\n<img src=\"/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_6.png\" />\n\n교차 검증은 입력 데이터의 하위 집합에서 모델을 학습시키고 나머지 데이터에서 평가하여 모델을 평가하는 데 사용할 수 있습니다. 일반적인 교차 검증 방법은 데이터 포인트를 무작위로 선택하여 학습 데이터 집합에 할당합니다. 그러나 이 방법은 데이터가 순차적이어야하기 때문에 시계열에 적합하지 않습니다. Timetk 패키지에는 시계열 데이터 세트에 대한 교차 검증 폴드를 특별히 생성하고 해당 분할을 시각화하는 데 사용할 수있는 멋진 함수인 time_series_cv()가 있습니다.\n\n```R\n# 교차 검증 분할 생성\ndf_folds <- \n  time_series_cv(\n    df_train, \n    initial = \"3 years\", \n    assess = \"1 year\", \n    skip = \"6 months\",\n    slice_limit = 5)  \n\n# 분할 시각화\nplot_time_series_cv_plan(df_folds, date, sales)\n```\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_7.png\" />\n\n## 단계 4: 모델링을 위한 데이터 준비\n\n레시피는 새로운 예측 변수를 만들고 모델에서 필요한 몇 가지 전처리를 수행할 수 있는 객체입니다. 저는 auto ARIMA 및 랜덤 포레스트 두 모델을 시도하고 싶기 때문에 두 레시피 객체를 만들 것입니다. 레시피()의 첫 번째 단계는 회귀 분석을 위한 공식입니다.\n\n```js\nrecipe_autoarima <- \n  recipe(sales ~ date,\n         data = df_train)\n```\n\n<div class=\"content-ad\"></div>\n\n두 번째 랜덤 포레스트 레시피에는 step_holiday()를 사용하여 date 데이터를 공휴일에 대한 하나 이상의 이진 지표 변수로 변환하는 기능을 추가할 것입니다. 또한 step_rm()을 사용하여 date 열을 제거하고 step_dummy()를 사용하여 모든 명목 예측 변수를 더미 변수로 변환할 것입니다.\n\n```js\nrecipe_rf <- \n  recipe(sales ~ ., data = df_train) |>\n  step_holiday(date, holidays = timeDate::listHolidays(\"US\")) |>  \n  step_rm(date) |>  \n  step_dummy(all_nominal_predictors())\n```\n\n다음은 recipe 객체가 보이는 모습입니다:\n\n![Recipe Object](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_8.png)\n\n<div class=\"content-ad\"></div>\n\n레시피 객체는 학습 및 테스트 데이터셋에서 반복 가능한 단계 시퀀스를 만들어주어 긴 피처 엔지니어링 코드를 작성하지 않고도 사용할 수 있습니다. 저는 이를 피처 엔지니어링의 단축키로 생각해요!\n\n아래 명령은 레시피 객체가 데이터셋을 업데이트하고 새로 생성된 열을 이해하는 데 사용될 수 있습니다.\n\n```js\nrecipe_rf |> prep() |> bake(new_data = NULL)\n```\n\n## 단계 5: 모델 사양 및 재표본화\n\n<div class=\"content-ad\"></div>\n\n레시피를 만들면, 각각의 두 모델에 대한 명세서를 작성할 것입니다. ARIMA에 Modeltime을 사용하고 랜덤 포레스트에는 Tidymodels를 사용할 예정이에요. 이것들은 일관된 단계적 흐름으로 여러 모델에 대한 명세를 만드는 훌륭한 방법을 제공합니다.\n\n```js\n# Auto ARIMA 모델 명세\nauto_arima_spec <- arima_boost() |> \n  set_mode(\"regression\") |> \n  set_engine('auto_arima_xgboost')\n\n# Random Forest 모델 명세\nrf_spec <- \n  rand_forest(trees = 500) |>  \n  set_mode(\"regression\") |> \n  set_engine(\"ranger\")\n```\n\n## 단계 6: Workflow 세트\n\nworkflow()는 전처리, 모델링, 후처리 요청을 함께 묶을 수 있는 객체입니다. 여러 모델과 리샘플링을 사용할 때, workflow 세트를 사용하는 것이 더 편리하다고 발견했어요.\n\n<div class=\"content-ad\"></div>\n\n이제 recipe 객체와 해당하는 모델 사양을 workflow_set()을 사용하여 결합할 것입니다. 기본적으로 cross 파라미터는 TRUE로 설정되어 있어서 각 recipe 객체가 각 모델 사양과 일치하도록 합니다. 이 경우에는 각 recipe 객체가 해당하는 모델 사양과 일치하도록 FALSE로 설정할 것입니다.\n\n```js\nworkflowset_df <- \n  workflow_set(\n    list(recipe_autoarima, recipe_rf),\n    list(auto_arima_spec, rf_spec),\n    cross=FALSE\n  )\n```\n\n이것이 workflow 객체입니다. 현재 결과가 없지만, 교차 검증이 완료되면 결과를 저장하기 위한 자리 표시자가 있습니다.\n\n![그림](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_9.png)\n\n<div class=\"content-ad\"></div>\n\n## 단계 7: 모델 학습 시간입니다\n\n지금까지 새로운 특성을 만들고 데이터를 전처리하고 모델 사양을 만들고 워크플로 세트를 구축했습니다. 이제 훈련 세트 및 리샘플링 폴드를 사용하여 모델을 적합시키겠습니다. 결과를 나중에 분석할 df_results에 결과를 저장할 것입니다.\n\n```js\ndf_results <-\n  workflow_map(\n    workflowset_df,\n    \"fit_resamples\",\n    resamples = df_folds\n  )\n```\n\n이렇게 하면 workflowset_df의 모든 워크플로에 대해 루프를 반복하고 각각에 fit_resamples 함수를 적용하여 df_folds 교차 검증 객체를 사용합니다. 각 실행의 결과는 df_results의 해당 모델 행 아래에 저장됩니다. 결과 열이 이제 교차 검증 결과로 채워졌음을 유의하십시오. 이러한 결과는 필요한 경우 unnest()를 사용하여 추출할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_10.png)\n\n결과는 tidymodels의 정말 멋진 기능인 autoplot() 명령을 사용하여 신속하게 시각화할 수도 있습니다.\n\n```js\nautoplot(df_results)\n```\n\n![image](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_11.png)\n\n\n<div class=\"content-ad\"></div>\n\n모든 리샘플에서 랜덤 포레스트가 ARIMA와 비교했을 때 더 낮은 RMSE(평균 제곱근 오차)와 R-제곱을 보여주고 있습니다.\n\n## 단계 8: 테스트 데이터 적합\n\n이제 df_results의 결과를 순위 매겨 가장 낮은 RMSE를 기준으로 가장 성능이 좋은 모델을 식별하겠습니다. 해당 모델의 ID 및 매개변수를 검색하고 이 최적화된 모델을 훈련 데이터에 맞출 것입니다. 그런 다음, 이 모델을 사용하여 테스트 데이터셋에서 판단하고 새로운 지표를 확인하겠습니다.\n\n```r\n# 최고의 rmse를 가진 workflow의 ID 가져오기\nbest_workflow_id <- df_results %>%\n  rank_results(rank_metric = \"rmse\") %>%\n  head(1) %>%\n  pull(wflow_id)  \n\n## best_workflow_id와 관련된 매개변수 가져오기\nbest_params <- df_results %>%\n  extract_workflow_set_result(id = best_workflow_id) %>%\n  select_best(metric = \"rmse\")  \n\n## best_workflow_id와 관련된 workflow 가져오기\nbest_workflow <- df_results %>%\n  extract_workflow(id = best_workflow_id)  \n\n# 최적화된 매개변수로 workflow 완성\nfinalized_workflow <- finalize_workflow(best_workflow, best_params) \n\nfinalized_workflow\n```\n\n<div class=\"content-ad\"></div>\n\n다음은 최종 워크플로우의 모습입니다:\n\n![image](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_12.png)\n\n이제 이 워크플로우를 사용하여 fit() 및 augment()를 사용하여 테스트 데이터 세트에서 예측하겠습니다. 이 예측을 기반으로 지표를 측정할 것입니다.\n\n```R\npredictions <- finalized_workflow %>%\n  fit(df_train) %>%\n  augment(df_test)  \n\n## 평가 지표 계산\nevaluation_metrics <- metric_set(rmse, mae, rsq)\nresults <- evaluation_metrics(predictions, truth = sales, estimate = .pred) \nprint(results)  \n```\n\n<div class=\"content-ad\"></div>\n\n다음은 최종 지표가 표시되는 모습입니다:\n\n![Final Metrics](/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_13.png)\n\n정말 잘 했어요!\n\n# 성공했어요!\n\n<div class=\"content-ad\"></div>\n\n이 기사를 통해 tidymodels, modeltime 및 timetk가 시계열 회귀 모델을 구축하는 강력한 프레임워크를 제공하는 방법에 대해 명확해졌으면 좋겠고, 여러분이 한 번 시도해볼 것으로 바랍니다! 이 기사는 한 가게와 제품 ID를 위해 만들어 졌지만, 이것은 어떤 가게와 제품 조합에 대해 이를 복제할 수 있는 Shiny 웹 애플리케이션을 구축하기 위한 좋은 사례가 될 수 있습니다. 즐거운 코딩하세요!\n\n# 코드\n\n이 기사의 모든 내용을 다시 만들기 위한 코드는 제 GitHub 저장소에서 찾을 수 있습니다.\n\n마음껏 찾아주세요. LinkedIn에서 저를 찾아보세요.\n\n<div class=\"content-ad\"></div>\n\n글에서 언급되지 않는 한, 모든 이미지는 저자가 찍은 것입니다.","ogImage":{"url":"/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_0.png"},"coverImage":"/assets/img/2024-06-19-TimeSeriesRegressionandCross-ValidationATidyApproach_0.png","tag":["Tech"],"readingTime":10},{"title":"내가 내부로 2 흥행 예상 금액이 8500만 달러에 달할 이유 AI 예측으로 보는 안목","description":"","date":"2024-06-19 20:19","slug":"2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI","content":"\n\n## 인공지능, 영화 및 예측 AI\n\n저는 AI가 미래에 대해 우리에게 말해 줄 수 있는지에 광신 중이었습니다. 아니죠, Mystic Meg와 그녀의 싸이킥 프렌즈 네트워크가 인공지능 덕분에 직장을 잃게 될 거라는 건 아니에요 (비록 그들도 그런 일이 오리라고는 예상 못했을 거에요)! 이건 초능력이나 마술 같은 건 아니고, 심지어 X-파일 의사과학도 아닌 거에요; 단지 대형 언어 모델을 사용해 훈련 데이터로부터의 기대와 패턴을 기반으로 다음에 무엇이 올지 예측하고, 우리의 집합적 의견이 무엇일지 예측하는 거예요.\n\n## 인공지능의 수비 크리스탈 볼 뒤에 있는 과학\n\n이를 뒷받침하는 베일러 대학의 실질적인 연구가 있어요 (아래서 다룰 거에요). 작년에 배팅 회사에서 GPT를 디자인해 파워 백을 제공하는 데 도움을 줄 것을 요청받았는데, 제 개인 윤리적 이유로 거절했지만, 이게 AI가 산업에서 활용되기 시작하는 방식이며, 재무 및 보험 서비스를 포함하며 AI가 주식 시장 추세를 예측하고 보험 가입 정책에 이벤트 발생 가능성을 평가할 수 있어요. AI는 법률 분야에서 법적 사건 결과를 예측하는 데 사용될 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n하지만 이야기꾼으로서, 가장 흥분되는 분야는 엔터테인먼트 산업입니다. LLMs는 시대정신에 입각합니다. AI가 관객들과 연결될 운명인 이야기를 알려줄 수 있다고 믿습니다. 사실, 그것에 돈을 걸 준비가 되어 있습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*ok4AW4Fpjz7zpIjV665AOQ.gif)\n\n## 직감 이상: 왜 인사이드 아웃 2가 박스 오피스에서 내가 거는 걸까요\n\n만약 AI가 옳다면, 인사이드 아웃 2는 2024년 최초로 개봉 주말에 8500만 달러를 벌어들일 영화가 될 것이고, 저는 입을 헐겁니다.\n\n<div class=\"content-ad\"></div>\n\n여름 박스 오피스가 힘든 시작을 했지만, ChatGPT는 Inside Out 2가 오프닝 주말에 뒤이어 개봉하는 Dune Part Two나 Godzilla X Kong: The New Empire 같은 주요 2024년 영화들을 누르고 $85백만을 달성할 것이라고 말했습니다.\n\n![이미지](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_0.png)\n\n이 성과는 Pixar에게 매우 필요한 승리가 될 것이며, 특히 이전 영화인 Elemental과 Lightyear의 성과가 좋지 않은 점을 고려할 때 더욱 중요할 것입니다.\n\n사실, 이토록 자신한다면, 만약 AI가 틀리다면 이 기사의 첫 주 수입 전액을 베팅하고, 그 금액을 자선 단체에 기부하겠습니다. 그것은 나름대로 큰 제안이죠: Medium과 독자들 덕분에 내가 인기 있는 이야기로 매달 최대 $850를 벌 수 있습니다. 그러니 이 기사를 계속 주목하고 공유해 주시고, 제가 책임을 질 수 있도록 도와주세요. 아이들의 정신적 안녕을 위한 자선 단체를 제안해 주세요.\n\n<div class=\"content-ad\"></div>\n\n# LLMs의 예측 능력 뒤에 숨은 과학\n\n하지만 내가 내일 Inside Out 2의 성공을 확신하는 이유는 뭘까요? 경제라는 것과 AI가 생성한 텍스트의 지루함 때문입니다. 사실, 일상적인 것이라고 생각되는 것은 대중의 의견, 감정 및 태도에 의해 상당히 영향을 받는 사건들을 평가할 때 자산이 되기도 합니다.\n\nAI 텍스트의 진부한 쓰기는 AI의 가치를 제공하는 중요한 도구로 사용되고 있다. 경제 교수들은 예측 가능성을 발견했으며 이 때문에 AI가 이미 확립된 패턴에 기반한 신뢰할 수 있고 일관된 예측을 제공할 수 있는 가치 있는 도구가 되고 있다는 것입니다. 이러한 이유로 AI는 가치 있는 도구로 사용됩니다. 이렇게요:\n\n## 미래 전망에서 제공됨\n\n<div class=\"content-ad\"></div>\n\n베일러 대학교의 반 팜 교수와 스콧 커닝햄 교수가 일반 경제학에 게재된 논문에서 미래 이벤트를 예측하는 흥미로운 방법을 발견했다고 발표했어요! ChatGPT가 '사실'을 만들어내면서 이용한 것인데요. 이들은 \"미래 서사\"란 개념을 사용했어요: AI에게 \"미래에서 일어난 사건을 공유하는 캐릭터들이 나오는 이야기\"를 출력하도록 요청했대요. 이 이야기들은 ChatGPT의 훈련 데이터 수집 이후에 일어난 사건들을 담고 있었어요.\n\n## 베일러 대학교 연구 결과로 알아본 예측 AI의 통찰\n\n그들은 가상의 미래 시점을 사용함으로써 ChatGPT의 제약사항을 우회하고 (팩트를 추정하는 것에 망설임을 보이는 특성) 놀랄만큼 정확한 2022년 아카데미 수상자를 예측하는 데 성공했어요. 결과를 즉시 확인하기 위해 실제 미래 이벤트를 예측하는 데 사용하지는 않았어요 (2022년 아카데미 시상식은 AI의 훈련 데이터 범위 밖이었지만 그 당시 연구자들에게 발생했습니다). 그러나 트럼프 허시 돈 판결 이전, 이들의 실험 결과를 반복해보았어요.\n\n<div class=\"content-ad\"></div>\n\n심사단이 판단을 내릴 때까지 ChatGPT가 훈련 데이터 외에서 새로운 정보에 접근하는지를 Bing 브라우저를 통해 확인하기 위해 실험을 실행했습니다. Pham과 Cunningham의 동의 없이입니다. 트럼프 판결은 ChatGPT와 실험자 모두 알지 못할 이벤트였습니다. 그럼에도 불구하고 그것이 정확하다고 판단했습니다 (100회 중 100회의 채팅에서 트럼프에 대해 유죄 판결을 내렸습니다).\n\n## 한 단어씩 미래 예측하기\n\n이 방법은 ChatGPT의 텍스트를 절차적으로 생성하는 기본 메커니즘을 활용하며, 즉 단어에서 단어로 전개(predictively from word to word)합니다. API에서 Temperature와 Top-P 값과 같은 매개변수를 사용하여 통찰력 있는 기능을 어떻게 세밀하게 조정할 수 있는지에 대해 궁금해합니다. 예를 들어, AI에게 \"다음으로 가능성이 높은 단어\"로 간주하도록 지시하는 경우 (Temperature), 또는 AI가 선택할 수 있는 토큰 풀을 증가시켜 최소한의 토큰 집합에 대한 누적 확률에 따라 선택할 수 있도록 하는 것(Top-P). 그러나 이는 향후 연구를 위한 분야입니다 (이에 관심 있는 학자들 협업 가능할까요?).\n\n# 미래 예측 \"미래 서술\" 프롬프트란 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n기본적으로, 특정 문장이 어떻게 끝날 것인지를 ChatGPT에 예측하도록 하는 대신, 미래적인 시각에서 해당 문장을 완성하도록 합니다. \"이미 다음 주인 것처럼 했을 때, 임의의 주제에 대해 무엇을 쓰게 될 가능성이 높을까요?\" \n\n![이미지](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_2.png)\n\n만약 AI가 문장을 완성시킬 수 있다면, 과정 생성의 기능으로 우리가 한 결론에 도달하는 것은 그리 어려운 일이 아닐 것입니다. \n\nPham과 Cunningham이 고려하지 않았을지도 모를 다른 요소가 있는데, 그것은 LLMs가 언어와 감정이 주요 요소인 예측 분석에서 더 나은 성과를 낼 수 있다는 점입니다. 사람들의 행동이 단어에 영향을 받는 상황에 LLMs가 최적이지 않을 수도 있지만, 소행성이 언제 충돌할지 예측하는 데보다는 알맞을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 정치 캠페인과 선거 결과\n- 시장 동향과 소비자 행동\n- 법적 사안 결과\n- 홍보 및 미디어 영향\n- 영화 박스 오피스 성적\n\n## 엔터테인먼트에서 예측적 AI 분석의 광범위한 함의\n\n그래서 '인사이드 아웃 2'의 성공을 예측할 때, 우리는 데이터에만 의존하는 것이 아니라 집단 문화 의식에도 접근하고 있습니다. 이러한 함의는 거대합니다 — 우리는 AI가 콘텐츠의 창작에 영향을 끼칠 것으로 생각했지만, AI가 청중들이 접근할 콘텐츠를 미리 예견할 수 있다면, 영화와 TV 프로그램에 대한 우리의 반응은 어떨까요? 제가 이전에 상상한 것은 창의적 분야에서 AI의 최적 활용이 개인화였습니다; 큰 이야기를 작고 소소하게 만드는 것입니다. 그러나 아마도 보다 보편적으로 공감할 수 있는 콘텐츠를 만드는 데 있을지도 모릅니다?\n\n# 비슷한 마음: 집단 의식과 LLMs\n\n<div class=\"content-ad\"></div>\n\nChatGPT은 우리의 커뮤니케이션의 공통점이자 \"하이브 마인드\"라고 할 수 있어요. Pham과 Cunningham이 발견한 예측 분석 능력은 '군중의 지혜'라고 알려진 원리에 근거할 수 있어요. 이 원리는 더 큰 그룹의 사람들이 가장 정확한 해결책으로 수렴하는 경향이 있다는 것입니다. 우리는 다양하고 분산된 그룹의 집합된 교육적 추측이 개인의 정확도를 능가할 수 있다는 것을 알고 있어요.\n\n![이미지](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_3.png)  \n\n여기서 LLM은 집단 지성의 수단으로 작용하며, 방대한 훈련 데이터로부터 집단의 감정을 분별하고 어떤 것이 문화적·사회적 대화의 보다 넓은 서술 내에 어떻게 들어맞을지를 파악할 수 있어요. 기본적으로 AI는 텍스트로 표현된 대중 의견의 공통점을 예측해요. AI는 대중 의식을 열린 책처럼 읽을 수 있어요. 그것은 훈련을 받았거든요.\n\n![이미지](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_4.png)  \n\n<div class=\"content-ad\"></div>\n\n우리의 의견 및 어느 정도 우리의 행동은 실현 가능합니다. 우리는 우리의 말과 예술, 의사 소통으로 계산할 수 있습니다. 언어는 AI 기반 시장에서 가장 가치 있는 상품입니다.\n\n![Image](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_5.png)\n\n## AI 예측에서 군중의 지혜\n\nAI는 우리 일반적인 시각, 보편적인 의견 및 공통 이해를 울리는 반향합니다. 그것이 바로 조화론이 의미하는 것입니다: LLMs가 사회의 가치와 일치하는지 확인하는 것입니다. AI는 일반적인 답변을 식별하여 우리의 필요성을 예상하고, 가장 넓은 사용자 스펙트럼을 고려하는 솔루션을 제공합니다. AI의 가장 큰 약점인 예측 가능하고 일반적이며 모두와 아무도와 같은 소리를 내지만, 합의를 예측할 때 가장 큰 강점이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_6.png)\n\n## 공생 관계: AI와 인사이드 아웃 2\n\n사실, 인사이드 아웃 2를 예측하는 데에 특히 호흡이 맞는 이유 중 하나가 있습니다. 첫 번째 영화를 본 사람이라면 집단 합의와 마음이 어떻게 작용하는지에 대한 상징적 수준으로 이해할 수 있을 것입니다. 마치 교육 데이터(기억)로 이루어진 거대한 컴퓨터 콘솔처럼 보이죠. 이는 ChatGPT와 같은 AI가 언어와 감정을 처리하고 이해하여 거의 인간 같은 방식으로 예측하고 응답하는 과정과 대칭성을 갖고 있습니다.\n\n## 자선의 내기: AI 예측을 시험해보기\n\n<div class=\"content-ad\"></div>\n\n내 기사의 수익을 AI의 예측에 따라 걸어놓음으로써, 나는 이 기술의 실용적 응용에 대한 내 자신감을 증명하고 있어. 독자들과 함께 재미있고 영향력 있는 방법이 되며, 좋은 일을 지원하는 것이다.\n\n더 높은 수익을 기대해 읽고 공유해주세요! (팁 하나, 특정 텍스트를 강조하여 X를 선택하여 트윗할 수 있어요).\n\n만약 내가 틀리더라도, 적절한 어린이 자선단체가 내 내기로부터 이익을 받을 것이야. 이 기사의 수익이 Inside Out 2가 열릴 때 300달러를 초과할 경우, 결과와 관계없이 기부할 거라고. 결과를 보려면 팔로우하기를 잊지 말아주세요!\n\n🌟 당신의 지원은 매우 감사히 받아들이고, 이 기사들이 계속되도록 도와줄 거예요!\n\n<div class=\"content-ad\"></div>\n\n👍 만일 이 글이 유익했다면 박수로 응원을 표현해 주세요\n\n💡 Medium에서 한 번에 긴 누른 버튼으로 최대 50번 박수를 줄 수 있어요!\n\n🤝 이 글을 당신의 소셜 미디어나 LinkedIn에 공유해 주시면 감사하겠습니다\n\n🐦 텍스트를 강조하고 X를 선택하여 당신이 좋아하는 부분을 트윗할 수 있어요\n\n<div class=\"content-ad\"></div>\n\n💬 기부할 어린이 자선 단체가 있나요? 아래 댓글에서 추천해 주세요 — 여러분의 추천을 듣고 싶어요!\n\n# Jim the AI Whisperer는 누구일까?\n\nAI를 해소하고 모든 사람이 접근할 수 있도록 하는 것이 제 목표입니다. AI의 잠재력에 열정적이며 여러분과 제 발견을 공유하는 것을 즐깁니다.\n\n## 함께 소통해요!\n\n<div class=\"content-ad\"></div>\n\n개인 코칭에 관심이 있거나 제 서비스를 이용하고 싶다면 언제든지 저에게 연락해주세요. 팟캐스트, 인터뷰 및 언론 활동에도 참여할 수 있습니다. 그리고 제 작품을 지원하고 싶다면 'Buy Me a Coffee' 페이지도 확인해보세요.\n\n![image](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_7.png)\n\n## 업데이트 및 참여 유지하기\n\n저, 인공지능 전문가 Jim의 새로운 소식을 놓치지 않고 받고 싶다면 구독해주세요. 인공지능 분야의 뜨거운 소식을 놓치지 않도록 최신 정보를 제공할 것을 약속드립니다. 함께 보다 나은 세상을 만들어나갑시다!\n\n<div class=\"content-ad\"></div>\n\n## Jim the AI Whisperer가 쓴 다음 Medium 기사를 즐기실 수도 있습니다:\n\n![Medium Article](/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_8.png)\n\n이 이야기는 Generative AI에서 발행되었습니다. LinkedIn에서 저희와 연락하고 최신 AI 이야기를 계속 업데이트 받으려면 Zeniteq를 팔로우하세요.\n\n최신 뉴스 및 생성적 AI에 관한 업데이트를 받으려면 뉴스레터를 구독하세요. 함께 AI의 미래를 함께 만들어봐요!\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_9.png\" />\n","ogImage":{"url":"/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_0.png"},"coverImage":"/assets/img/2024-06-19-WhyIbetInsideOut2willreach85milliondomesticopeningweekendaspredictedbyAI_0.png","tag":["Tech"],"readingTime":8},{"title":"기계 학습을 위한 재생 커널 힐버트 공간","description":"","date":"2024-06-19 20:17","slug":"2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning","content":"\n\n# 커널\n\n머신 러닝(ML)에서 커널 기계는 SVM과 같은 선형 분류기를 사용하여 비선형 문제를 해결합니다. 이를 위해 커널 함수를 활용하여 데이터를 더 높은 차원의 공간으로 암시적으로 매핑하여 선형으로 분리할 수 있습니다.\n\nX를 비어 있지 않은 집합이라고 하고, 커널 k가 다음과 같이 정의됩니다:\n\n![커널 함수](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_0.png)\n\n<div class=\"content-ad\"></div>\n\n위 정의에 따르면, 커널 함수 k: X x X → ℝ은 데이터 포인트 x와 x' 사이의 유사성을 측정합니다. k가 유효한 커널이 되려면, k(x, x') = ⟨φ(x), φ(x')⟩ 조건을 만족해야 합니다. 여기서 φ는 입력 공간 X에서 특징 공간 H로의 매핑이며, ⟨・ ,・⟩는 힐베르트 공간 H에서의 내적을 나타냅니다. 이는 커널 값이 특징 공간에서 데이터 포인트의 표현의 내적과 동일함을 의미합니다.\n\n하지만, 다른 많은 머신 러닝 알고리즘과 달리 입력 데이터를 특징 벡터로 명시적으로 변환해야 하는 기능 맵을 사용하지 않고, 커널 방법은 사용자가 지정한 커널 함수를 통해 직접 유사성을 계산합니다. 이 커널 함수는 잠재적으로 무한 차원의 고차원 특징 공간에서 작동하더라도, 커널 트릭을 통해 특징 표현을 명시적으로 계산하는 것을 피할 수 있습니다. 이 기술은 이러한 계산에 수반되는 상당한 계산 비용을 피할 수 있도록 도와줍니다. 이러한 암묵적 작업을 통해 커널 방법은 복잡한 데이터 관계를 효율적으로 분석하고, 효과적인 패턴 인식 및 분석을 용이하게 합니다.\n\n## 함수 공간\n\n함수 분석에서, f (・)는 함수 자체를 나타내며, f(x)는 입력 x에서 함수가 취하는 특정 값을 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_1.png\" />\n\n입력 x를 사용하는 함수 f를 고려해 봅시다. 여기서 x는 (x₁, x₂)로 정의된 벡터입니다.\n\n<img src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_2.png\" />\n\n함수 f(・)는 ℝ²를 ℝ에 매핑하는 함수 공간의 원소입니다. 이 예제에서는 이 함수 공간에서 f(・)를 ℝ³로 표현할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_3.png)\n\nGiven the function\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_4.png)\n\nThe linear functional f can be represented as:\n\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_5.png\" />\n\n보통 임의의 함수는 피처 𝜙(𝑥)의 선형 조합으로 나타낼 수 있습니다. x에서 f를 평가하는 𝑓(𝑥)는 피처 공간에서의 내적입니다:\n\n<img src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_6.png\" />\n\n예를 들어, x = (-1, 4)에서 평가된 f는\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_7.png)\n\n## 무한 차원의 특성 공간\n\n이 개념은 자연스럽게 무한 차원의 특성 공간으로 확장됩니다. 예를 들어, 우리는 지수 함수 eˣ를 이용하여 그의 테일러 급수 표현을 확장할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_8.png)\n\n\n<div class=\"content-ad\"></div>\n\n그리고 e³가 되면\n\n![Image 1](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_9.png)\n\n여기서 피처 공간은 무한한 차원에 있습니다.\n\n![Image 2](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_10.png)\n\n<div class=\"content-ad\"></div>\n\n# 평가 기능\n\n평가 기능은 함수를 입력으로 받아 스칼라(하나의 숫자)를 출력으로 생성하는 특별한 유형의 함수입니다. 좀 더 간단히 말하면, 일반적인 함수가 숫자를 숫자로(또는 벡터를 벡터로) 매핑하는 반면, 함수적으로는 함수를 숫자로 매핑합니다.\n\nX의 x에서의 평가 기능 L은 다음과 같이 정의됩니다.\n\n\n<img src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_11.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n이 정의는 𝐿이 x에 의해 매개변수화된 함수로, 입력으로 함수 f를 사용하고 실수 ℝ을 생성하는 기능입니다.\n\n선형 함수는 아래와 같이 벡터 덧셈 및 스칼라 곱셈 연산을 보존하는 기능입니다:\n\n![이미지](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_12.png)\n\n# 이중 공간\n\n<div class=\"content-ad\"></div>\n\n벡터 공간 V의 쌍대 공간은 V 상의 모든 선형 함수인 집합입니다.\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_13.png)\n\n친숙한 3차원 공간을 고려해보겠어요. ℝ³로 표시되며, 각 점은 세 가지 구성 요소 (x, y, z)를 가진 벡터로 나타낼 수 있어요. 이것이 바로 우리의 원래 벡터 공간 V에요. ℝ³에서의 선형 함수는 3차원 벡터를 입력으로 받아 하나의 실수를 출력하는 선형 매핑입니다. 예를 들어, 선형 함수는 다음과 같이 정의될 수 있어요: f(x, y, z) = 2x + 3y + 4z. ℝ³의 쌍대 공간인 V*는 ℝ³에서의 모든 가능한 선형 함수의 집합이에요. 이 쌍대 공간의 각 함수는 3차원 벡터로 고유하게 표현될 수 있어요. 예를 들어, 함수 f(x, y, z) = 2x + 3y + 4z는 벡터 (2, 3, 4)로 나타낼 수 있어요.\n\n# Riesz Representation Theorem\n\n<div class=\"content-ad\"></div>\n\n리즈 표현 정리는 힐버트 공간에서의 모든 연속 선형 함수 L이 F의 고정된 요소와의 내적으로 표현될 수 있다는 것을 명시합니다. 공식적으로, 힐버트 공간 F에서 어떤 연속 선형 함수 𝐿에 대해,\n\n\n![리즈 표현 정리](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_14.png)\n\n\n리즈 표현 정리는 선형 함수의 추상적 세계를 보다 익숙한 내적 개념으로 이어주어, 선형 함수를 내적의 기하학적 직관을 사용해 이해하고 조작할 수 있게 합니다. 본질적으로, 리즈 표현 정리는 힐버트 공간 H의 모든 연속 선형 함수에 대해, 그 함수를 완전히 나타내는 동일한 H 내의 고유 요소가 존재함을 명시합니다. 이는 선형 함수를 고정된 \"대표\" 벡터와의 내적으로 생각할 수 있다는 것을 의미합니다. 이는 힐버트 공간에서 선형 함수와 내적 사이의 근본적인 연결을 확립합니다. 복제 커널 힐버트 공간(RKHS)의 맥락에서, 이 개념은 함수의 평가가 커널 함수와의 내적을 사용하여 계산될 수 있음을 보여주기 위해 확장됩니다.\n\n# 복제 커널\n\n\n<div class=\"content-ad\"></div>\n\n평가 기능과 Riesz 표현 정리를 결합하면 다음을 얻을 수 있습니다:\n\n![식 15](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_15.png)\n\n어떤 함수 f ∈ F가 x에서 평가되면 f를 F 내의 고유 기능 kₓ와의 내적으로 표현할 수 있습니다. 여기서 kₓ는 다음과 같이 쓸 수 있습니다:\n\n![식 16](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_16.png)\n\n<div class=\"content-ad\"></div>\n\n테이블 태그를 마크다운 형식으로 변경해보세요.\n\n<div class=\"content-ad\"></div>\n\n이것은 결국 커널의 개념으로 돌아가게 됩니다.\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_19.png)\n\n요약하면, 평가 기능 Lₓ는 k( ⋅ , x )로 표현할 수 있습니다.\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_20.png)\n\n<div class=\"content-ad\"></div>\n\n내부 곱으로 이를 계산할 수 있습니다. 이 함수는 Hilbert 공간 𝐻의 커널 함수와의 내적으로 계산됩니다. 함수 𝑘(⋅, 𝑥) 또는 kₓ는 재생 커널로 불립니다.\n\n그러므로,\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_21.png)\n\n여기에 사용된 표기법은 모두 동일한 객체를 가리킵니다:\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_22.png)\n\n# 복제 커널 힐버트 공간 (RKHS)\n\n복제 커널 힐버트 공간(RKHS)은 각 지점에서의 평가가 연속 선형 기능적인 함수들의 힐버트 공간입니다. 이는 공간 안의 임의의 함수 f와 임의의 점 𝑥에 대해 𝑓(𝑥) = ⟨𝑓,𝑘(⋅, 𝑥)⟩가 성립하는 커널 함수 k가 존재한다는 의미입니다. 커널 함수 𝑘(⋅,𝑥)는 복제 커널로 알려져 있으며, 함수와 특징 공간 간의 다리 역할을 하여 함수를 특정 지점에서 평가할 수 있게 합니다.\n\n본질적으로, f(x)는 f와 복제 커널 함수 k(⋅, x)의 내적으로 나타낼 수 있습니다. 함수 k(⋅, x)는 복제 커널이라고 불립니다. 개념적으로, 평가 함수는 f와 x의 특징 공간 표현의 내적으로 계산할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 예시\n\n푸리에 변환은 다음과 같은 형태를 가지고 있습니다.\n\n<img src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_23.png\" />\n\n놀랍게도, 우리가 커널의 재현 특성을 논의할 때에도 똑같은 형태를 가지고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Reproducing Kernel Hilbert Space for Machine Learning](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_24.png)\n\nFunctional analysis has traditionally been utilized to examine the characteristics of transformational functions like the Fourier transform. To demonstrate this, let's take a look at an aperiodic pulse function.\n\n![Aperiodic Pulse Function](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_25.png)\n\nThe Fourier transform of the aperiodic pulse function 𝑓(𝑥) is:\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_26.png)\n\n예를 들어,\n\n![이미지](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_27.png)\n\n아래 예시에서 우리는 함수(1, cos(𝑥), cos(2𝑥), cos(3𝑥), ...)를 기저로 사용하여 계단 함수를 나타냅니다.\n\n\n<div class=\"content-ad\"></div>\n\n아래는 코싸인 함수로 특징 공간에 나타낸 특징들을 보여줍니다. 이것이 우리가 왜 함수를 사용하여 특징 공간을 모델링할 수 있는지 효과적으로 보여줍니다.\n\n# 무한 차원 RKHS\n\n두 번째 예제에서는 가우시안 함수로 구성된 특징 공간을 설명하겠습니다. 아래 f(x)를 고려해보세요.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_29.png\" />\n\n'𝑓(𝑥)'가 어떻게 커널 'k'를 복제하는지 알아봅시다.\n\n<img src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_30.png\" />\n\n이를 위해 '𝑓(𝑥)'를 특정 포인트의 커널들의 선형 결합으로 표현합니다.\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_31.png\" />\n\n이 예시에서는 가우시안 커널을 사용할 것입니다. 이 커널은 다음과 같이 정의됩니다:\n\n<img src=\"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_32.png\" />\n\n우리는 𝑓(𝑥)를 k(・, xᵢ)의 선형 조합으로 표현할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_33.png)\n\n이 과정에서는 가우시안 커널을 직접적으로 다룹니다. 아래의 함수 f는 다음으로 구성됩니다:\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_34.png)\n\n![image](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_35.png)\n\n<div class=\"content-ad\"></div>\n\n![2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_36](/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_36.png)","ogImage":{"url":"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_0.png"},"coverImage":"/assets/img/2024-06-19-ReproducingKernelHilbertSpaceforMachineLearning_0.png","tag":["Tech"],"readingTime":8},{"title":"L1 및 L2 정규화를 분석적 및 확률적 관점에서 이해하기","description":"","date":"2024-06-19 20:13","slug":"2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews","content":"\n\n## 기계 학습과 수학\n\n![이미지](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_0.png)\n\n기계 학습을 공부할 때 L1 및 L2 정규화를 알게 될 것입니다. 많은 멋진 블로그들이 시각화를 통해 이러한 개념을 직관적으로 설명합니다. 그러나 L1 및 L2 정규화를 해석 및 확률적 관점에서 자세히 설명하는 블로그는 거의 없습니다. 그래서 나는 두 관점 모두로 이 두 가지 정규화에 대해 쓰기로 결심했습니다. 이 블로그에서는 L1 및 L2 정규화를 상세한 수학적 유도 및 시각화와 함께 소개하여 이러한 개념을 잘 이해할 수 있도록 도와드리겠습니다.\n\n## 목차\n\n<div class=\"content-ad\"></div>\n\n- 정규화 개요\n  - L1 정규화\n    - 2.1 L1 정규화의 분석적 유도\n    - 2.2 L1 정규화의 확률적 유도\n\n- L2 정규화\n  - 3.1 L2 정규화의 분석적 유도\n  - 3.2 L2 정규화의 확률적 유도\n\n<div class=\"content-ad\"></div>\n\n## 1. 규제의 개요\n\n우선, 규제의 개념을 다시 살펴봅시다. 구체적으로 이해하기 위해 두 가지 차원을 가진 소량의 데이터를 예로 들어봅시다 [1].\n\n![이미지](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_1.png)\n\n보시다시피, 이 데이터는 비선형입니다. 데이터가 비선형이기 때문에 간단한 선형 회귀가 이 데이터에 적합하지 않을 것이라고 쉽게 추측할 수 있습니다. 이 경우, 비선형 데이터를 나타내기 위해 다항 회귀를 고려해 보겠습니다. 규제의 중요성을 이해하기 위해 우리는 15차 다항 회귀를 사용하여, 즉, 과도하게 복잡한 함수를 사용하여 데이터를 예측합니다.\n\n<div class=\"content-ad\"></div>\n\n마크다운 형식으로 변경된 문구입니다.\n\n\n![Understanding L1 and L2 regularization with analytical and probabilistic views](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_2.png)\n\nscikit-learn 라이브러리의 PolynomialFeatures 및 Ridge 클래스를 사용하여 데이터를 적합시킵니다. 결과는 아래와 같습니다.\n\n![Understanding L1 and L2 regularization with analytical and probabilistic views](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_3.png)\n\n왼쪽 그림은 정규화가 없는 다항 회귀를 보여주며, 오른쪽 그림은 정규화가 적용된 다항 회귀를 보여줍니다. 정규화가 없는 다항 회귀는 원래 데이터에 비해 지나치게 복잡한 함수를 사용했기 때문에 데이터에 과적합되었습니다. 반면, 정규화가 적용된 다항 회귀는 과적합을 방지하면서 모델의 복잡성을 줄일 수 있어서 정규화가 적용되었다면 정규화가 없는 것보다 더 나은 적합을 할 수 있습니다. 일반적으로, 위의 예시처럼 모델이 과적합되는 것을 방지하기 위해 정규화를 사용합니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n정규화를 어떻게 할까요? 이론적으로는 정규화 용어를 목적 함수에 추가하여 이를 기반으로 매개변수를 최적화합니다. 아래에서 보여지는 것처럼요.\n\n<img src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_4.png\" />\n\n정규화 용어는 계수 값이 증가하지 않을 때를 위해 패널티를 부과합니다. 그럼 계수에 패널티를 부여하는 이유는 무엇일까요? 직관적으로 이해하려면 이전 예제의 계수를 살펴보겠습니다.\n\n<img src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_5.png\" />\n\n<div class=\"content-ad\"></div>\n\n위 줄은 정규화 없이 다항 회귀의 계수를 나타내고, 아래 줄은 정규화(L2)를 적용한 다항 회귀의 계수를 나타냅니다. 정규화 없이 다항 회귀는 더 큰 계수 값을 갖습니다. 직관적으로, 모델이 더 큰 계수 값을 갖는다는 것은 모델이 변화를 크게 일으킬 수 있다는 것을 의미합니다. 따라서, 정규화 없이 모델은 주어진 데이터와 더 정확하게 맞을 수 있지만 일반적이지는 않습니다. 한편, 정규화를 적용한 모델은 상대적으로 계수 값이 작기 때문에 주어진 데이터를 더 일반적으로 맞출 매개변수를 탐색할 수 있습니다.\n\n지금까지 정규화의 개념과 그 효과에 대해 이해했습니다. 이제 정규화 뒤의 이론적 배경을 자세히 살펴보겠습니다.\n\n## 2. L1 정규화\n\nL1 정규화[2]는 계수의 절댓값 또는 계수의 l1-노름을 정규화 용어로 추가합니다. L1 정규화는 계수의 특징 선택에 도움을 줍니다. 즉, 관련 없는 독립 변수의 수를 줄일 수 있습니다. 구체적으로, L1 정규화를 적용한 회귀 모델은 Least Absolute Shrinkage and Selection Operator(Lasso) 회귀라고 불립니다. L1 정규화의 공식은 아래와 같습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_6.png)\n\nwhere w is the parameter. From now on, we will learn how to solve this problem.\n\n## 2.1 Analytical derivation of L1 regularization\n\nHow can we optimize the L1 regularization formula? To solve it analytically, this formula can be seen as constraint optimization with Lagrange multipliers.\n\n\n<div class=\"content-ad\"></div>\n\n![Understanding L1 and L2 regularization](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_7.png)\n\n마지막 방정식을 보시면, L1 정규화 공식과 동일합니다. 다음으로, 유도된 라그랑지안을 기반으로 매개변수를 어떻게 해석적으로 최적화할 수 있는지 알아보겠습니다. 안타깝게도, L1 정규화에 대한 닫힌 솔루션을 얻을 수는 없습니다. 왜냐하면 정규화 항을 미분할 수 없기 때문입니다. 이 사실을 아래 그림에서 확인할 수 있습니다. 두 매개변수 함수가 있다고 가정하고, L1 정규화 항을 다음과 같이 나타낼 수 있습니다:\n\n![L1 Regularization Term](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_8.png)\n\n가장자리에서 도함선을 계산할 때, 오른쪽과 왼쪽에서 서로 다른 값이 나오기 때문에, 가장자리에서 미분할 수 없습니다 ([7]에서 더 많은 수학적 세부사항을 확인하실 수 있습니다). 몇 가지 예외적인 상황을 제외하고 닫힌 형태의 솔루션을 찾는 것은 어렵지만, 행렬 X가 직교하고 매개변수 수가 하나인 경우 닫힌 형태의 솔루션을 찾을 수 있습니다 [3]. 그러나 이러한 상황은 실제 분석에서 드물게 발생합니다.\n\n<div class=\"content-ad\"></div>\n\n그럼, 파라미터를 어떻게 찾아야 할까요? 라쏘 문제를 해결하는 가장 일반적인 방법은 서브그래디언트와 좌표 하강법입니다. scikit-learn의 구현에서는 라쏘 문제를 최적화하기 위해 좌표 하강법을 사용하므로 좌표 하강법을 배워봅시다 [4].\n\n좌표 하강법은 간단한 아이디어입니다. n 차원 함수 f가 있다고 가정했을 때, 우리는 f를 각 파라미터 차원을 반복적으로 최소화하여 최소화합니다. 수학적 정의를 살펴봅시다.\n\n![image](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_9.png)\n\n이름에서 알 수 있듯이, 우리는 파라미터를 개별적으로 계산하고 이전 값에 기반하여 업데이트합니다. 이 과정은 수렴하거나 설정한 최대 반복 횟수에 도달할 때까지 계속됩니다. 정말 간단하지 않나요? 이제 라쏘 공식의 구체적인 예제를 살펴봅시다. 평균 제곱 오차(MSE)로 된 라쏘 문제를 고려해봅시다. 따라서 공식은 다음과 같을 것입니다:\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_10.png\" />\n\n포함 된 좌표 감소 방법을 사용하기 때문에, 다른 비대상 매개변수를 고정한 채 각 매개변수에 대해 이 공식을 최소화해야 합니다.\n\n<img src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_11.png\" />\n\n마지막 공식은 조금 까다롭습니다 (적어도 저에게는 그렇습니다). 각 항목의 차원을 고려할 때, X의 전치 행렬의 i번째 행만 i번째 경사에 관련되어 있음을 이해할 수 있습니다. i번째 매개변수의 경사를 정식화하려면 위의 공식을 재정의해야 합니다.\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_12.png\" />\n\n파라미터 업데이트 방정식을 얻으려면 XB 항을 분해해야 합니다. XB의 i번째 열과 다른 열로 나눕니다. 보시다시피, 파라미터 업데이트 공식을 유도할 수 있습니다. L1 정규화 항은 어떠신가요? 이를 해결하기 위해 소프트 쓰레스홀딩을 소개할 것입니다. L1 항을 미분할 수 없기 때문에, 우리는 서브그레디언트(subgradients)와 부분도함수(subdifferentials)를 사용하여 이를 근사할 것입니다. 서브그레디언트(subgradients)의 개념에 대해서는 [5]의 이론을 확인해볼 수 있습니다. 서브그레디언트를 사용하여 다음을 유도할 수 있습니다:\n\n<img src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_13.png\" />\n\nLasso 문제에서 L1 정규화 항을 대체하면 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n아래에 있는 표를 Markdown 형식으로 변경해주세요.\n\n\n![Understanding L1 and L2 regularization](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_14.png)\n\n주어진 마지막 방정식을 재정립하여 𝛽를 구할 수 있습니다:\n\n![Understanding L1 and L2 regularization](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_15.png)\n\n부호 및 최댓값 함수를 사용하여 조건부 분기를 구성할 수 있습니다. 이제 우리는 수렴할 때까지 이 최종 공식을 반복하여 파라미터를 업데이트합니다. 구체적인 예시를 풀어보겠습니다. 시각화를 위해 두 개의 매개변수로 함수를 최적화하고 바이어스 항이 없다고 가정합니다.\n\n\n<div class=\"content-ad\"></div>\n\n![UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_16](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_16.png)\n\n정규화 강도로 𝜆 = 1을 사용하고 유도한 방정식에 따라 매개변수 값을 업데이트합니다.\n\n이 경우 수렴이 빠르게 진행되는 것을 확인할 수 있습니다. 계수 값은 scikit-learn 구현과 거의 같습니다.\n\n```js\n# 스크래치에서 좌표 하강\nb0 = 1.00, b1 = 2.00\n\n# scikit-learn\nb0 = 1.11, b1 = 2.04\n```\n\n<div class=\"content-ad\"></div>\n\n실제 케이스에서는 Cython을 사용하여 좌표 하강을 계산하는 scikit-learn 구현을 사용해야 합니다. 그들의 구현은 원래의 Python 코드보다 훨씬 더 빠릅니다. 추후에는 함께 사용한 코드를 공유할 예정이에요.\n\n## 2.2 L1 규제의 확률적 유도\n\n확률적 측면에서 L1 규제에 대해 자세히 알아보기 전에 MAP(최대 사후 확률) 추정치를 알아야 합니다. MAP 추정 및 최대 우도 추정치 사이의 차이를 배워봅시다. 이미 알고 계신 분들은 다음 섹션을 건너뛸 수 있습니다.\n\n필수 사항 — 최대 우도 추정 및 MAP 추정\n\n<div class=\"content-ad\"></div>\n\n우리가 다중 선형 함수를 갖고 있다고 가정하고, 관측된 데이터 점과 예측된 값 사이의 오차가 평균 0 및 표준 편차 𝜎를 따르는 정규 분포를 따른다고 가정합니다. 가정된 분포 하에서 오차가 발생하는 확률 밀도, 즉 우도는 다음과 같이 유도할 수 있습니다:\n\n여기서 데이터 점의 개수를 n이라 하고 매개변수의 수를 p라고 합니다. 이 확률 밀도를 최대화하는 매개변수를 찾고 싶은데, 이를 최대 우도 추정(Maximum Likelihood Estimation, MLE)이라고 합니다. MLE 공식은 다음과 같이 쓸 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n일반적으로 로그를 취하여 곱셈을 합으로 변경합니다. 빈도주의 통계학에서는 매개변수를 상수값으로 간주하지만 알 수 없으므로 미분을 사용하여 우도를 최대화하는 매개변수를 찾습니다.\n\n반면에 베이즈 정리에서는 매개변수를 확률 변수로 취할 수 있습니다. 베이즈 정리를 적용하여 우도를 다음과 같이 볼 수 있습니다:\n\n![image](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_19.png)\n\n사후 확률은 우도와 사전 확률에 비례합니다. 이 설정에서는 MLE와 같이 우도를 최대화하는 대신 사후 확률을 최대화해야 합니다. 사후 확률을 최대화하는 것을 최대 사후 확률 추정이라고 하며 MAP 추정이라고 합니다. 다음과 같이 정의할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_20.png\" />\n\n이전 예제에 적용할 때 MLE와 대조적인 점을 살펴보면:\n\n<img src=\"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_21.png\" />\n\nMAP 추정을 하는 데는 사전 확률이 필요하다는 것을 알 수 있습니다. 이를 위해 어떤 확률 분포든 사용할 수 있습니다. 이제 L1 정규화로 돌아가보겠습니다.\n\n\n<div class=\"content-ad\"></div>\n\n확률적 유도: 라플라스 사전을 사용한 L1 정칙화\n\n우리가 사전으로 라플라스 분포를 선택하면, MAP 추정은 L1 정칙화 공식이 됩니다. 이를 유도해 봅시다! 라플라스 사전은 아래와 같은 모양을 가진 확률 분포 중 하나입니다.\n\n![image](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_22.png)\n\n지수 함수의 지수항이 L1 정칙화 항과 유사함을 알 수 있습니다. 이제, 우리는 MAP 추정에서 사전 확률로 평균 0을 가진 라플라스 사전을 대입합니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 마지막 공식이 L1 정규화와 동일함을 확인할 수 있습니다! 직관적으로 라플라스 분포의 모양이 가우시안 분포보다 훨씬 더 날카롭습니다. 이는 아래의 해석적 유도 부분에 표시된 L1 정규화 용어와 유사합니다.\n\n아래는 마지막 공식이 L1 정규화와 동일함을 확인할 수 있습니다! 직관적으로 라플라스 분포의 모양이 가우시안 분포보다 훨씬 더 날카롭습니다. 이는 아래의 해석적 유도 부분에 표시된 L1 정규화 용어와 유사합니다.\n\n지금까지 해석적 및 확률적 관점에서 L1 정규화 유도를 이해했습니다. 다음에는 L2 정규화에 대해 알아보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## 3. L2 정규화\n\nL2 정규화는 계수의 제곱값 또는 계수의 L2-노름을 정규화 항으로 추가합니다. L2 정규화는 작은 계수를 유도하는 데 도움이 됩니다. L2 정규화가 적용된 회귀 모델을 Ridge 회귀라고 합니다. L2 정규화의 수식은 아래와 같습니다.\n\n![수식](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_25.png)\n\n## 3. 1 L2 정규화의 해석적 도출\n\n<div class=\"content-ad\"></div>\n\nL1 정규화와 마찬가지로 L2 정규화 문제를 라그랑주 승수를 사용한 제한 최적화로 생각해 볼 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_26.png)\n\n마지막 방정식은 L2 정규화 공식과 같습니다. L1 정규화와 대조적으로 이 공식은 미분 가능합니다. 따라서 새로운 개념을 도입할 필요가 없습니다. 그저 미분하면 되죠!\n\n![이미지](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_27.png)\n\n<div class=\"content-ad\"></div>\n\n이제 L2 정규화에 대한 닫힌 형태의 해를 얻었습니다. 이를 구현하고 scikit-learn ridge 결과와 비교해 봅시다.\n\n```python\n# 샘플 데이터\nX = np.random.randn(100, 2)\nbeta = np.array([2, 3]).reshape(1, 2)\nY = X @ beta.T + np.random.normal(beta.shape[0])\n\nlam = 1.0\ninv_mat = np.linalg.inv(X.T @ X + np.eye((X.T @ X).shape[0]))\n\nridge_coef = inv_mat @ X.T @ Y\nprint(ridge_coef.reshape(-1))\n# [1.998, 2.937]\n\nridge = Ridge(alpha=1.0)\nridge.fit(X, Y)\nprint(ridge.coef_.reshape(-1))\n# [1.979, 2.973]\n```\n\n거의 같은 결과를 얻을 수 있다는 것을 확인할 수 있습니다.\n\n## 3.2 L2 정규화의 확률론적 유도\n\n<div class=\"content-ad\"></div>\n\nMAP 추정을 다시 고려해 봅시다. L1 정칙화를 유도할 때는 사전 분포로 라플라스 분포를 사용합니다. L2 정칙화의 경우, 평균이 0인 가우시안 분포를 사전 분포로 활용합니다.\n\n![image](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_28.png)\n\n지수 함수의 거듭제곱 항이 L2 정칙화 항과 유사하다는 것을 알 수 있습니다. 이제 MAP 추정에서 평균이 0인 가우시안 사전 확률로 대체합니다.\n\n![image](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_29.png)\n\n<div class=\"content-ad\"></div>\n\n마지막 공식이 L2 정규화와 동일한 것을 확인할 수 있습니다. 직관적으로 가우시안 분포의 형태는 라플라스 사전보다 부드러운 곡선을 가집니다. 따라서 이는 L2 정규화 용어와도 유사합니다.\n\n![이미지](/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_30.png)\n\n마지막으로, 이 블로그에서 사용된 코드를 공유하겠습니다.\n\n이 블로그에서는 L1 및 L2 정규화의 자세한 유도를 해석적 및 확률론적 관점을 통해 소개했습니다. 이 블로그가 정규화의 수학적 배경을 이해하는 데 도움이 되길 바랍니다. 읽어 주셔서 감사합니다.\n\n<div class=\"content-ad\"></div>\n\n## 참고 자료\n\n[1] Underfitting vs.Overfitting, scikit-learn 문서\n\n[2] Manfredi, V., Lecture 12: Regularization, 웨즐리안 대학 강의\n\n[3] https://stats.stackexchange.com/questions/17781/derivation-of-closed-form-lasso-solution\n\n<div class=\"content-ad\"></div>\n\n[4] Tibshirani, R., Coordinate Descent, Carnegie Mellon 대학 강의\n\n[5] Giba, L., Subgradient Descent Explained, Step by Step, MLC\n\n[6] Kang, B., 정규화의 확률적 해석, Bounded Rationality\n\n[7] https://math.dartmouth.edu/opencalc2/cole/lecture21.pdf","ogImage":{"url":"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_0.png"},"coverImage":"/assets/img/2024-06-19-UnderstandingL1andL2regularizationwithanalyticalandprobabilisticviews_0.png","tag":["Tech"],"readingTime":12},{"title":"영화 제목이 점점 길어지고 있는 걸까요 통계 분석 해보기","description":"","date":"2024-06-19 20:09","slug":"2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis","content":"\n\n![이미지](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_0.png)\n\n이 글은 주간 뉴스레터인 'Stat Significant'에서 영화, 음악 및 TV에 관한 데이터 중심의 에세이를 다룬 것입니다.\n\n## 소개: 긴 영화 제목의 여름\n\n2024년 여름은 긴 영화 제목의 해입니다. 대형 흥행 영화의 계절이 시작되었으며 Godzilla x Kong: The New Empire로 시작해 세계 제국의 왕국이 박스 오피스 막대를 넘겨준 후 Fury Road: A Mad Max Saga로 아이가 가장 많이 벌어들인 영화로 자리를 내줬습니다. 이전에는 단순한 명명 체계 - Fear, The Fugitive, The Firm, Dave -가 있었지만 지금은 겉으로는 역동적인 단어들이 깔린 알파벳 수프로 대체되었습니다.\n\n<div class=\"content-ad\"></div>\n\n그럼에도 불구하고, 이러한 장황한 제목들은 신형 할리우드 광고의 산물 이상의 것이다. 오히려, 이러한 명명 체계들은 박스 오피스 수익의 축소에 대응하고 고품질 가정 엔터테인먼트 옵션들로부터의 경쟁에 적응하는 영화 산업을 반영한다. 듣기로는, 영화 관객들은 집을 나와야만 하는데 길고 감정적인 제목들이 필요하다고 한다.\n\n그래서 오늘은 영화 명명의 트렌드, 산업 역학, 이러한 트렌드를 움직이는 과장된 단어들, 그리고 특정 구두점의 부상을 조사해 볼 것이다. Stat Significant Literary Universe (SSLU)에서 또 다른 멋진 에피소드를 위해 준비하시라: \"Rise of the Long-Winded Movie Title: A Stat Significant Saga Part 1 (of 1): The Legend of The Kingdom of an Ailing Movie Industry.\"\n\n## 영화 제목들이 더 기네요?\n\n2006년의 'Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan'는 예산이 $20백만 이상인 프로젝트들 중에서 가장 긴 영화 제목의 기록을 보유하고 있다(인플레이션 조정 포함) — 비록 이 영화가 대부분의 장황한 이름과는 뚜렷한 차이를 보이고 있다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_1.png)\n\n보랏의 여정을 따라가는 영화 제목은 주인공의 이름 이후에 나오는 모든 것이 지나치게 무의미하다는 것을 보여줍니다. 반면에 다른 긴 제목들은 서투른 기업 마케팅의 산물이며 (얼마나 많은 맥킨지 컨설턴트들이 불어난 장미: 송버드와 스네이크스 의 이름을 지을 때 도움을 주었는지 아무도 모릅니다.) 이렇게 길게 늘이는 제목들은 관객들에게 알려진 이야기 (지적 재산권 ❤️)을 제공하면서도 약간의 새로움을 제공합니다 - 그것들은 같지만 다릅니다.\n\n당연히 할리우드의 프랜차이즈가 가장 긴 영화 제목 목록을 지배하며, 이 중 대부분은 만화책 네이밍 규칙을 활용하고 있습니다.\n\n![이미지](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_2.png)\n\n\n<div class=\"content-ad\"></div>\n\n할리우드의 시리즈 엔터테인먼트에 대한 열광은 2000년대 초반, 스파이더맨 2, 반지의 제왕 시리즈 및 해리포터 시리즈의 대성공을 통해 시작되었습니다. 2010년대와 2020년대에 이 추세가 계속되며 할리우드는 시리즈 스토리텔링을 더욱 받아들이면서 제목의 길이가 점점 더 길어졌습니다.\n\n![이미지](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_3.png)\n\n장르별 제목 길이를 살펴보면, 어드벤처, 판타지 및 패밀리와 같은 시리즈 친화적 형식이 일반적으로 더 많은 영숫자 문자를 사용한다는 것을 알 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_4.png)\n\n<div class=\"content-ad\"></div>\n\n위의 데이터 포인트는 사실과 같이 더 많은 내용을 제공해요:\n\n- 마블이 엔터테인먼트를 망쳤어요\n- 넷플릭스도 그렇죠\n- 밥 아이거가 이에 기여했을지도 몰라요\n- A24는 현대 시네마의 구원자에요\n- 크리스토퍼 놀란도 그렇죠\n\n영화는 예전에는 훌륭했었어요 (독자님같이 14살이었을 때 주로), 하지만 이제는 나빠졌고, 긴 제목은 이 매체의 예술적 몰락을 또 다른 지표로 보여줍니다. 세상이 절대 변하지 않았다면, 이런 문제가 없었을 거에요 — 영화 제목은 최대 세 음절로 유지됐을 거예요.\n\n우리는 이러한 사고방식을 계속할 수 있어요(무기한으로), 냉소적인 태도에 빠지며 Flubber, The Game, Titanic, Scream 2 같은 간결한 제목들의 시대를 그리워할 수 있지만, 만약 우리가 그렇지 않았다면 어떨까요? 불평 대신에 이 계속해서 늘어나는 제목들의 힘을 주는 구문과 문구를 탐험하며 긴 제목체계의 황당함을 받아들였다면 어떨까요?\n\n<div class=\"content-ad\"></div>\n\n## 긴 영화 제목의 구성 요소: 흔한 단어 및 문장 부호\n\nRinger의 \"Big Picture\" 팟캐스트는 영화 산업의 예술적 및 상업적 트렌드에 대한 깊은 분석을 제공합니다. 이 쇼의 주인공인 션 페너시는 산업이 지적 재산권(IP)에 점점 의존하고 있다는 점에 공개적으로 경멸을 품고 있는 열렬한 영화 애호가입니다. 그는 어리석은 IP에 대해 이야기할 때마다, Rebel Moon — Part Two [Colon] The Scargiver처럼 영화 제목에 포함된 \"콜론\"을 명시적으로 언급합니다.\n\n![2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_5](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_5.png)\n\n이 쇼 몇 주 동안 들은 후, 페너시의 섬세한 항의 행위가 집에 닿기 시작했습니다: 왜 많은 영화들(일부는 심지어 시퀄도 아닌 것들)이 콜론을 사용하는 걸까요?\n\n<div class=\"content-ad\"></div>\n\n영화 제목의 단어와 문장부호 사용을 검토할 때, 콜론의 압도적인 숫자를 볼 수 있습니다. 다른 긴 제목에서 나오는 용어는 영화가 시리즈 내에 있는 위치를 나타내거나 새로운 장을 강조합니다. 그러나 현대 할리우드의 주인공은 콜론입니다. 영화 산업은 항상 시퀄에 의존해 왔지만, 이 구두점의 증가된 사용은 영화 마케팅의 새로운 방향을 강조하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_7.png](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_7.png)\n\n모든 영화는 한 번뿐인 경험, 일순간의 사건으로, 긴 (아마도 끝없는) 이야기의 단일 장을 제공합니다. 수퍼맨 2와 스파이더맨 3 시대는 끝났고, 대신 수퍼맨: 강철의 사나이와 스파이더맨: 홈 노 웨이가 등장했습니다. 많은 면에서, 이 두 점은 아메리칸 영화관이 오랜 시간 동안 지켜온 매력 요소인데, 이는 연재물 콘텐츠에 현혹된 사람들로 인해 비난의 대상이 되었습니다. 하지만 넷플릭스와 데이비드 자스로브를 탓할 수도 있습니다.\n\n## 마지막 생각: 우리는 더 많은 콜론이 필요한가요?\n\n![2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_8.png](/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_8.png)\n\n\n<div class=\"content-ad\"></div>\n\n이 계속 확장되는 영화 제목들의 근본적인 질문이 존재합니다: 스튜디오들은 이러한 제목들로 어떤 것을 이루려고 하는 걸까요? 왜 할리우드는 이러한 메시지를 채택하는 걸까요?\n\n이러한 긴 제목들의 목적을 더 잘 이해하기 위해 최근 개봉한 몇 편의 샘플을 살펴보겠습니다:\n\n- Godzilla x Kong: The New Empire: 거대한 도마뱀이 거대한 원숭이와 싸움을 벌일 것이며, 그것은 황홀할 것입니다.\n- Kingdom of the Planet of the Apes: 이 영화는 플래닛 오브 더 에이프 시리즈의 또 다른 장면으로, 보통 크기의 원숭이들이 독재 정부로 조직되는 것을 다룰 것입니다.\n- The Garfield Movie (2024) 그리고 이 전작 Garfield: The Movie (2004): 이 영화들은 가필드에 대한 영화일 것입니다 (저는 확신합니다).\n\n이러한 명명 선택은 영화 산업의 결과로, 극장으로 사람들을 유혹할 수 있는 것은 이벤트로 만드는 경우에 한정됩니다. 스트리밍과의 경쟁에서, 영화는 일련화로 향하고 동시에 이러한 부분들을 놓칠 수 없는 이벤트로 구성하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n너무 크게 자란 도마뱀과 거대한 원숭이가 \"새로운 제국\"(무엇을 의미하는지는 모르지만) 배경 속에서 싸운다고 주장하는 것은 영화를 스펙터클로 변신시킵니다. 이 영화는 시퀄에 그치는 것이 아니라, 여러분이 좋아했던 이전 시리즈와 너무 다르지도 않습니다. 이 마케팅 전략이 사람들을 유인하여 집을 떠나 1인당 $22를 내고 이 원숭이와 도마뱀의 결투를 집에서 $3.99(또는 무료!)에 보는 대신 극장에서 관람하도록 만드는 것 같아요!\n\n올해 미국 박스 오피스는 지난해 파업과 슈퍼히어로 콘텐츠의 부재에 따라 25% 감소한 상황입니다 (마블은 2024년에 한 편의 영화만 출시할 예정입니다). 이렇게 쓰는 것이 마음 아프지만, 점 이상이 필요하다는 주장이 있을 수 있습니다. 극장에 사람들을 모아서 그들에게 영화의 매력(예: 니콜 키드먼 광고와 편한 좌석)을 다시 알려주고, 이 관객들이 향후 작품들의 예고편을 샘플링할 수 있게 해, 그들이 관람하러 극장에 다시 오게 합니다. 이것은 플라이휠을 시작할 영화가 없으면 관람 습관이 사라진다는 것을 의미합니다. 어쩌면 우리는 어리석은 콜론으로 가득한 제목을 가진 일련화된 마블 컨텐츠를 충분히 필요로 하지 않을까요.\n\n\"25년까지 살아남아라\"는 엔터테인먼트 산업에서 흔한 속담이 되었습니다. 이 시기에 파업 후 콘텐츠가 데뷔할 것이며 모든 것이 완벽해지리라고 예상됩니다(더 이상 문제는 없을 것입니다!).\n\n우리도 이 말을 채택해야 할지 모르겠습니다. 하지만 우리만의 특별한 변화와 함께: \"25년까지 버텨라: 상황은 나아진다: 새로운 제국(희망적으로)\"\n\n<div class=\"content-ad\"></div>\n\n만화, 음악 및 TV에 대한 더 많은 데이터 중심 에세이를 읽고 싶다면, 내 뉴스레터 Stat Significant를 확인해보세요.\n\n데이터 및 통계에 대해 이야기 나누고 싶으세요? 흥미로운 데이터 프로젝트가 있으신가요? 혹시 인사를 전하고 싶으신가요? daniel@statsignificant.com 으로 이메일 보내주세요.","ogImage":{"url":"/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_0.png"},"coverImage":"/assets/img/2024-06-19-AreMovieTitlesGettingLongerAStatisticalAnalysis_0.png","tag":["Tech"],"readingTime":6},{"title":"이상치 탐지를 위한 3가지 간단한 통계적 방법","description":"","date":"2024-06-19 20:07","slug":"2024-06-19-3SimpleStatisticalMethodsforOutlierDetection","content":"\n\n<img src=\"/assets/img/2024-06-19-3SimpleStatisticalMethodsforOutlierDetection_0.png\" />\n\n데이터 과학자의 일 중 중요한 부분은 데이터를 정제하고 전처리하는 것입니다. 이 과정 중 하나인 이상치 탐지와 제거는 매우 중요합니다. 대규모의 이상치, 급증, 그리고 나쁜 데이터는 정확한 기계 학습 모델을 학습하는 데 방해가 될 수 있기 때문에, 이상치를 적절하게 처리하는 것이 중요합니다.\n\n하지만 데이터 과학자들이 항상 이상치를 식별하기 위해 격리 숲 또는 국소 이상치 요소처럼 기계 학습 모델을 사용하는 것은 아닙니다. 제 데이터 과학 경력에서 배운 한 가지는 간단한 해결 방법이 효과적이면 그것을 사용해야 한다는 것입니다.\n\n이번에는 대부분의 시간에 잘 동작하는 이상치를 탐지하는 데 유용한 3가지 간단한 통계적 솔루션을 제공하고자 합니다. 또한 이를 Python에서 어떻게 수행하는지도 보여드릴 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 1. Z 점수\n\nZ 점수는 표준 점수로도 알려져 있으며, 특이값을 감지하는 데 사용되는 잘 알려진 방법 중 하나입니다. 기본적으로 어떤 데이터 포인트가 평균에서 몇 개의 표준 편차만큼 떨어져 있는지를 나타냅니다.\n\n어떤 데이터셋의 특정 데이터 포인트의 Z 점수는 다음과 같이 계산됩니다:\n\n![](/assets/img/2024-06-19-3SimpleStatisticalMethodsforOutlierDetection_1.png)\n\n<div class=\"content-ad\"></div>\n\n어디서:\n\n- Z는 Z 점수 값입니다.\n- x는 데이터 포인트입니다.\n- μ는 데이터 집합의 평균입니다.\n- σ는 표준 편차입니다.\n\n따라서 Z 점수가 4인 경우 데이터 포인트가 평균보다 4 표준 편차 위에 있음을 의미합니다. Z 점수가 -4인 경우에는 평균보다 4 표준 편차 아래에 있습니다.\n\nZ 점수의 경우, 일반적으로 3 이상 또는 -3 미만의 값은 이상치로 간주됩니다. 그러나이 기준은 유연하며 프로그래머에 따라 조정할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n아래와 같이 Python과 scipy.stats 패키지를 사용하여 데이터프레임 열의 각 값의 z 점수를 계산하는 간단한 방법이 있어요:\n\n```python\nfrom scipy import stats\n\ndf[\"z_score\"] = stats.zscore(df[\"column_of_interest\"])\n```\n\n데이터셋의 각 값에 대한 z 점수를 얻은 후에는 이상값을 걸러낼 수 있어요:\n\n```python\ndf_clean = df[(df[\"z_score\"] <= 3) & (df[\"z_score\"] >= -3)]\n```\n\n<div class=\"content-ad\"></div>\n\nz 점수의 단점 중 하나는 이상치 탐지 방법임에도 불구하고 이상치에 민감하다는 것입니다. 데이터 세트에 매우 큰 이상치가 있는 경우 평균을 왜곡할 수 있습니다 (평균은 이상치에 민감하기 때문입니다). 평균이 왜곡되면 더 작지만 여전히 관련 있는 이상치를 잡지 못할 수 있습니다.\n\n## 2. IQR\n\nIQR (사분위 범위)는 평균 대신 중앙값을 기준점으로 사용하기 때문에 z 점수보다 견고합니다.\n\n![이미지](/assets/img/2024-06-19-3SimpleStatisticalMethodsforOutlierDetection_2.png)\n\n<div class=\"content-ad\"></div>\n\n이제 IQR 값을 구했으니, 다른 점들이 이상값인지 판별하는 기준점으로 사용될 것입니다. Q1 값보다 1.5 * IQR 이상 또는 Q3 값보다 1.5 * IQR 이하인 모든 점은 이상치로 간주됩니다.\n\n이 경우, 46.5(27 + 13 * 1.5)보다 큰 값 또는 -5.5(14 - 13 * 1.5)보다 작은 값은 이상치로 간주됩니다.\n\n파이썬에서 numpy의 percentile 및 scipy stats의 IQR 함수를 사용하여 이를 계산하는 방법은 다음과 같습니다:\n\n```python\nfrom scipy.stats import iqr\nimport numpy as np\n\n# 데이터의 IQR 구하기\niqr_data = iqr(df[\"column_of_interest\"])\n# 범위를 벗어나는 값을 얻기 위한 참조점 계산 (1.5 * IQR)\niqr_lim = 1.5 * iqr_data\n\n# 상위 (Q3 또는 75번째 백분위수)와 하위사분위 (Q1 또는 25번째 백분위수) 계산\nq1 = np.percentile(df[\"column_of_interest\"], 25)\nq3 = np.percentile(df[\"column_of_interest\"], 75)\n\n# 사분위수와 IQR*1.5를 사용하여 상한선과 하한선 결정\n상한선 = q3 + iqr_lim\n하한선 = q1 - iqr_lim\n\n# 상한선보다 작거나 하한선보다 큰 값은 이상치로 간주하고 제거\ndf_clean_iqr = df[(df[\"column_of_interest\"] >= lower_limit) \n& (df[\"column_of_interest\"] <= upper_limit)]\n```\n\n<div class=\"content-ad\"></div>\n\n위에서 볼 수 있듯이 데이터 세트의 IQR을 계산하려면 z 점수보다 몇 가지 더 많은 단계/코드 줄이 필요합니다. 또한 각 데이터 포인트에 대한 가시적인 \"점수\"를 얻지 못하므로 얼마나 이상하게 큰지를 나타내지 않습니다. 여기서는 어떤 것이 범위를 벗어났는지 여부만을 알 수 있습니다.\n\n그러나 중앙값을 사용하기 때문에 평균보다 쉽게 왜곡되지 않아서 데이터 세트의 이상치에 덜 민감합니다.\n\n# 3. 수정된 z 점수\n\n수정된 z 점수는 z 점수와 IQR의 측면을 모두 고려하여 표준 z 점수의 더 견고한 버전을 만듭니다. 데이터 점이 얼마나 \"멀리 떨어져\" 있는지를 대략적으로 알려주는 점수를 제공하면서도 이상치에 민감하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\nz 점수와 수정된 z 점수의 주요 차이점은 수정된 z 점수가 평균 대신 중앙값을 기준점으로 사용한다는 것입니다. 표준 편차가 평균과 직접적으로 관련되어 있기 때문에, 수정된 z 점수는 정확한 표준 편차를 측정하지 않습니다. 그러나 중위수 절대 편차(MAD)를 사용하여 표준 편차를 근사하려고 합니다.\n\n![image](/assets/img/2024-06-19-3SimpleStatisticalMethodsforOutlierDetection_3.png)\n\n여기서,\n\n- 0.6745는 표준 편차의 중위수에 해당하는 값을 근사화하는 데 사용되는 상수입니다.\n- xi는 조사하는 데이터 포인트입니다.\n- x͂는 데이터셋의 중앙값입니다.\n- MAD는 데이터셋의 중위수 절대 편차입니다.\n\n<div class=\"content-ad\"></div>\n\n중앙값 절대 편차를 계산하려면 데이터 세트의 중앙값에서 각 데이터 포인트를 빼면 됩니다. 이 뺄셈의 절대값을 취하세요. 마지막으로, 이러한 절대 차이의 중앙값을 취하면 됩니다.\n\n일반적으로 수정된 z 점수에서는 점수가 ` 3.5 또는 `-3.5 인 값이 이상치로 간주됩니다.\n\n수정된 z 점수의 계산 방법과 Python에서의 실제 예제에 대한 더 자세한 정보가 필요하시면 수정된 z 점수에 관한 제 글을 참조해주세요:\n\nPython에서 수정된 z 점수를 구현하는 방법은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n# 이 함수는 값을 취하고 데이터 집합을 취하여 하나의 값을 위한 수정된 z 점수를 반환합니다.\ndef compute_mod_z_score(value,df):\n    # 데이터 집합의 MAD(흑백절대편차) 계산 (관심 있는 열)\n    med_abs_dev = (np.abs(df[\"column_of_interest\"] - \n                  df[\"column_of_interest\"].median())).median()\n    const = 0.6745\n    mod_z = (const * (value - df[\"column_of_interest\"].median()) \n            / med_abs_dev)\n    return mod_z\n\n# 위의 함수를 전체 열에 적용하여 수정된 Z 점수를 모든 데이터 점에 대해 얻습니다.\ndf[\"mod_zscore\"]=df[\"column_of_interest\"].apply(compute_mod_z_score,df=df)\n```\n\n수정된 Z 점수의 주요 단점은 덜 알려져 있으며 MAD와 같은 변수를 사용하기 때문에 설명하기가 조금 더 복잡하고 어려울 수 있다는 것입니다. 또한 저는 아는 한 Python 라이브러리 중에 수정된 Z 점수를 계산하는 것이 없습니다.\n\n# 결론\n\n보시다시피, 이상치 탐지의 각 통계적 방법마다 이점과 단점이 있습니다. 제가 일하는 곳에서는 이를 모두 사용했지만 다른 데이터 집합과 사용 사례에 대해 사용했습니다. 데이터를 탐색하여 문제에 접근하는 방법을 알아야 한다는 점의 중요성을 강조할 수 없습니다.\n\n\n<div class=\"content-ad\"></div>\n\n만약 데이터셋이 급격한 변동이 있는 경우에는 수정된 z 점수나 IQR이 가장 적합할 수 있습니다. 가장 간단하고 설명하기 쉬운 해결책을 찾고 있다면 z 점수 / 표준 점수를 선택하는 것이 좋습니다. \n\n언제나 여러분만의 테스트를 실행하고 다른 데이터 과학자들이나 결과에 혜택을 받을 수 있는 관련 이해당사자들과 상의하는 것이 중요합니다.\n\n# 읽어주셔서 감사합니다","ogImage":{"url":"/assets/img/2024-06-19-3SimpleStatisticalMethodsforOutlierDetection_0.png"},"coverImage":"/assets/img/2024-06-19-3SimpleStatisticalMethodsforOutlierDetection_0.png","tag":["Tech"],"readingTime":5},{"title":"MLOps - PyTest를 사용한 데이터 검증","description":"","date":"2024-06-19 20:05","slug":"2024-06-19-MLOpsDataValidationwithPyTest","content":"\n\n<img src=\"/assets/img/2024-06-19-MLOpsDataValidationwithPyTest_0.png\" />\n\n# 소개\n\nMLOps 파이프라인에서는 가능한 한 많은 단계를 자동화하려고 노력합니다. 프로그래머의 직접적인 개입으로 발생할 수 있는 오류의 수를 최소화하는 것이 목표입니다. 또한 데이터셋 유효성 검사에 유의하는 것도 중요합니다. 누구나 기계 학습의 제1 규칙에 대해 익숙할 것입니다: 쓰레기를 넣으면 쓰레기가 나옵니다. 우리가 개발하는 모델이 얼마나 정교하든, 데이터셋의 관리가 제대로 이루어지지 않으면 높은 확률로 나쁜 결과를 얻을 것입니다.\n\n이 기사에서는 PyTest를 사용하여 데이터셋에 대한 자동 검증을 수행하는 방법을 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n저는 Deepnote을 사용하여 이 기사의 스크립트를 실행하고 있어요. Deepnote은 협업형 데이터 과학 프로젝트와 프로토타이핑에 좋은 클라우드 노트북 서비스에요.\n\n## ETL에 대하여\n\n처음으로 머신 러닝에 접근하는 사람들은 대부분 Kaggle에서 볼 수 있는 도전 과제를 해결해야 하는데요. 이러한 도전 과제에서는 거의 항상 시간이 지나도 변하지 않는 정적인 데이터셋을 다루게 됩니다. 하지만 실제 세계에서는 이것이 완전히 사실이라고 할 수 없어요.\n\n실제 머신 러닝 제품을 개발할 때에는 데이터가 지속적으로 변할 수 있어요. 그 결과 데이터는 데이터 추출, 데이터 변환, 데이터 로딩의 초기 단계를 거쳐 얻어지게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n이 세 단계는 일반적으로 ETL 이라는 약어로 요약됩니다. 간단히 말해서, 데이터 수집을 수행하여 모델 훈련을 진행할 충분한 데이터 양을 확보해야 한다고 상상해보세요. 데이터를 어딘가에서 추출해야 하는데, 예를 들어 스크래핑하거나 오픈 소스 데이터가 어떻게 도움이 될 수 있는지 분석해야 합니다(추출).\n\n데이터는 다양한 형식으로 제공될 수 있습니다. CSV 파일 몇 개, JSON 파일, 그리고 몇 개의 txt 파일을 모았을 수도 있습니다. 따라서 데이터를 균일하게 변환해야 합니다.\n\n마지막으로, 데이터 과학자들이 쉽게 사용할 수 있도록 데이터를 쉽게 활용할 수 있어야 합니다. 예를 들어 다운로드하기 쉽도록 시스템에 업로드할 수 있어야 합니다(예: Hugging Face, AWS).\n\n![이미지](/assets/img/2024-06-19-MLOpsDataValidationwithPyTest_1.png)\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 ETL 최상의 실천 방법에 대해 읽을 수 있습니다.\n\n## 무엇이 잘못될 수 있을까요?\n\n이제 데이터가 수집되는 방법을 알게 되었으니, 데이터 과학자가 데이터 유효성 검사를 다루어야 하는 이유와 방법에 대해 이해해 봅시다. 데이터셋에서 몇 가지 잘못된 부분이 생길 수 있습니다.\n\n- 주변 세계는 동적이며 변하기 때문에 데이터의 분포도 변합니다. 티셔츠 가격에 대해 예측하는 모델을 생각해보세요. XXL 사이즈는 아무도 구매하지 않았기 때문에 매우 낮은 가격으로 예측되었습니다. 그러나 세대가 지남에 따라 사람들이 점점 키가 커지기 때문에 미래에는 큰 사이즈에 더 중요성을 부여하는 모델을 다시 훈련해야 할 수도 있습니다.\n- 소스 데이터에 변경 사항이 있었지만 우리에게 알려지지 않았습니다. ETL 파이프라인을 담당하는 팀이 영화 평점 시스템을 변경하여 1에서 5점까지 범위로 구성된 시스템에서 10점까지의 시스템으로 전환했습니다.\n- ETL 중에 데이터 흡수에 버그가 있었을 수 있습니다. 실수가 있고 cm로 표현된 데이터에서 km로 표현된 데이터로 변경되었을 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 유효성 검사는 데이터를 분할한 후(splitting data into train and test)에 수행할 수도 있고 그 전에도 할 수 있습니다. 어디서 하는 것이 가장 좋은지 명확하지 않으며 두 가지 방법에 대한 장단점이 있습니다.\n\n## PyTest 소개\n\nPyTest는 다양한 종류의 테스트를 실행하는 데 널리 사용되는 파이썬 라이브러리입니다. 일반적으로 코드 베이스 내에 tests라는 폴더를 생성하고 여기에 실행하려는 여러 테스트 파일들을 수집합니다. 각 파일의 이름은 test_xx.py와 같이 지정됩니다. 따라서 tests 폴더 안에서 test_data.py 또는 test_model.py와 같은 파일들을 생성할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n테스트를 수행하는 데 주요한 Python 명령어는 assert입니다. 이 명령어는 특정 조건이 충족되었는지 확인하고, 그렇지 않으면 오류가 발생합니다. 조건 뒤에 문자열로 오류를 정의할 수 있습니다. 예시를 살펴보겠습니다.\n\nPyTest는 파일 내에서 감지된 모든 테스트 함수를 실행하고, 모든 단언문이 True를 반환하는지 확인합니다. 그렇지 않을 경우 터미널에 실패한 테스트를 표시합니다. 테스트 함수의 예시는 다음과 같습니다.\n\n여기서 첫 번째 문제가 발생합니다. 이전 함수에서 주어진 입력의 값은 무엇인가요? 테스트 단계에서 이러한 변수를 어떻게 지정할까요? 우리는 픽스처(fixtures)에서 도움을 받습니다!\n\n## PyTest의 픽스처(Fixtures)\n\n<div class=\"content-ad\"></div>\n\n많은 경우 (위의 경우와 같이) 테스트에는 단언을 만들기 위한 입력 데이터(예: 데이터)가 필요합니다. 이 입력 데이터는 PyTest의 fixtures를 사용하여 제공할 수 있습니다. Fixtures를 사용하면 더 이상 할당할 필요 없이 테스트 내에서 사용될 변수를 선언할 수 있습니다. 그러나 fixtures를 정의하는 함수는 테스트 함수의 입력 변수와 동일한 이름을 가져야 합니다. 예를 살펴봅시다.\n\n위의 코드 블록에서 보는 것처럼, 우리는 data(함수 이름을 따름)라는 fixture를 구현하여 df라는 데이터프레임을 출력값으로 반환합니다.\n따라서 test_data_length 테스트에서는 입력 데이터가 fixture의 값을 취할 것이므로 df 데이터프레임과 일치할 것입니다.\n\nFixture의 범위를 지정할 수 있으므로, fixture가 파괴될 때를 결정할 수 있습니다. 예를 들어, 범위가 \"session\"인 경우 동일한 fixture가 전체 세션 동안 유지됩니다. 이것은 첫 번째 테스트가 데이터 값을 변경할 수 있고, 그 값을 두 번째 테스트로 전달할 수 있도록 합니다.\n\n대신 \"function\" 범위를 사용하면, 각 테스트가 fixture의 새로운 및 변경되지 않은 복사본을 입력으로 사용합니다.\n\n<div class=\"content-ad\"></div>\n\nPyTest 문서에서는 다양한 범위에 대해 읽을 수 있어요.\n\n머신러닝 데이터셋에 대한 테스트 작성은 전통적인 소프트웨어 엔지니어링에 대한 테스트 작성보다 복잡할 수 있어요. 전통적인 소프트웨어에서는 각 기능에 대해 기대 출력이 있기 때문에 테스트가 기대한 것과 다른 결과를 반환하면 명백히 오류가 있다고 볼 수 있어요.\n\n반면, 데이터셋에서는 무엇을 기대해야 하는지 확신할 수 없어요. 예를 들어 데이터셋에서 기능 \"키\"의 평균이 1.70cm일 것으로 가정해봅시다. 하지만 테스트 결과 평균이 \"1.75\"라고 나타나면 어떻게 해야 할까요? 오류가 있는 걸까요? 아니면 정말로 키가 큰 사람들의 데이터를 추가해서 평균을 높인 것일까요?\n\n그래서 데이터셋에서 할 수 있는 몇 가지 간단한 결정론적 테스트부터 시작해보고, 확정적이지 않은 테스트에 대해도 살펴보겠어요.\n\n<div class=\"content-ad\"></div>\n\n## 결정론적 테스트\n\n결정론적 테스트 작성은 매우 간단합니다. 데이터셋에서 무엇이 결정론적인가요? 예를 들어, 열의 수가 정확하게 X여야 하거나 행의 수가 N 이상이어야 충분한 데이터가 있는 것과 같은 경우입니다.\n\n범주형 변수의 경우, 값이 특정 범위 내에 있는지 확인할 수 있습니다. 예를 들어, \"색상\" 특성이 [빨강, 초록, 파랑] 값만을 가질 수 있는지 확인할 수 있습니다.\n\n이러한 유형의 테스트를 위한 예제 파일을 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n이 코드에서는 다음과 같은 함수들을 찾을 수 있습니다:\n\n- data: 이 함수에서는 데이터프레임을 포함하는 변수를 노출하는 fixture가 있습니다.\n- test_column_presence_and_type: 이 함수에서는 [age, salary, name, genre] 네 개의 열이 데이터셋에 존재하고 올바른 유형인지 확인합니다.\n- test_class_names: 이 함수는 장르 값이 알려진 값들 중에 있는지 확인합니다. 예상치 않은 값들을 찾지 않도록 보장합니다.\n- test_column_ranges: 여기서는 숫자 변수들이 특정 범위에 있는지 확인합니다. 예를 들어, 나이는 절대 음수가 될 수 없습니다!\n\n## 확률론적 테스트\n\n확률론적 테스트에서 우리가 하고 싶은 것은 불확실성을 고려하여 값들을 측정하는 것입니다. 불확실성에 대해 이야기할 때 확률과 통계가 관련되며, 우리는 여기서도 그들을 활용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n현재 작업 중인 데이터셋의 값을 평가하기 위해 이전 버전과 비교하는 것이 일반적인 관행입니다.\n\n데이터셋에서 수행할 수 있는 몇 가지 예시 확인 사항은 다음과 같습니다:\n\n- 이상값의 존재 확인\n- 하나 이상의 열 값 분포 확인\n- 하나 이상의 열 또는 모든 열과 목표 열(예측 대상) 사이의 상관 관계 확인\n- 다양한 열의 평균 및 표준 편차 확인\n\n이미 언급한 바와 같이, 결정론적 테스트에서는 통계가 사용되며 일반적으로 과거 데이터가 예시로 취해져 현재 데이터와 비교됩니다. 따라서 가설 검정이 어떻게 작동하는지 이해하고, 이러한 비교를 위해 어떻게 사용할 수 있는지 이해하는 것이 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n이 글에서는 가설 검정의 기본 개념을 간단히 살펴볼 예정이에요. 더 깊이 알고 싶으시다면 이 영상을 보시는 걸 추천해요:\n\n가설 검정을 다룰 때는 항상 대립 가설에 대한 귀무 가설을 검정하게 되어요.\n\n- 귀무 가설 (H_0): 과학 커뮤니티에서 널리 받아들여지는 가정이에요. 저희의 경우, 데이터에 관한 가정일 수 있어요.\n- 대립 가설 (H_a): 저의 새로운 가설로 받아들여지길 원하는 것으로, 귀무 가설과 반대되는 가설이에요. 제가 새로운 가설을 받아들여지게 하려면 제 가설을 확인하는 데이터를 제시해야 하죠. 이렇게 하면 새로운 가설이 옳다는 것을 모두를 설득하는 것이 더 쉬워져요.\n\n대표적인 예시는:\n\n<div class=\"content-ad\"></div>\n\n- 영가설 (H_0): 두 개의 샘플은 동일한 평균을 가진 정규 분포에서 나온 것이다.\n- 대립가설: 두 개의 샘플은 서로 다른 평균을 가진 정규 분포에서 나왔다.\n\n가정에 따라 사용할 수 있는 다양한 통계 검정 방법이 있습니다. 각 통계 검정은 가정과 관련이 있습니다. 따라서 올바른 검정을 선택하는 것이 매우 중요합니다. 올바른 통계 검정을 선택하는 데 도움이 될 수 있는 적절한 논문을 알려드리겠습니다.\n\n이 예제에서는 t-검정을 사용할 것입니다.\n\n우리가 해야 할 일은, 샘플을 시작으로 알려진 공식을 사용하여 테스트 통계량이라는 값을 계산하는 것입니다. 테스트 통계량으로부터 곡선 아래 영역에 해당하는 p-값이라는 다른 값을 계산합니다 (나중에 자세히 살펴보겠습니다). p-값이 미리 선택한 임계값 (알파)보다 크면 우리는 영가설을 기각할 수 없으며 영가설은 여전히 진실이 유지됩니다. 그 대신에 p-값이 작으면 영가설을 기각할 수 있고 새로운 (대립) 가설을 주장할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n당연히 이 거절에 대한 신뢰도는 미리 선택한 임계값에 의해 결정됩니다. 일반적인 임계값은 0.1, 0.5 및 0.001입니다. 이 값이 작을수록 더 확신이 있습니다.\n\n![이미지](/assets/img/2024-06-19-MLOpsDataValidationwithPyTest_2.png)\n\n아직 완전히 이해가 안 된다면 걱정하지 마세요, 이 전체 설명은 몇 줄의 Python 코드로 번역됩니다! t-테스트에 대한 Scipy 함수는 직접 검정 통계값과 p-값을 반환합니다. 우리가 해야 할 일은 알파 값을 선택하고 결정만 내리는 것뿐입니다.\n\n기계 학습에서는 참조 데이터 세트를 보유하고 이를 새로 얻은 데이터 세트와 비교하여 데이터 분포가 동일한지 이해하는 것이 최적일 것입니다. 유감스럽게도 우리가 사용 가능한 데이터 세트가 그리 많지 않기 때문에 보통 테스트 데이터 세트를 훈련 데이터 세트와 비교하는 것이 일반적입니다.\n\n<div class=\"content-ad\"></div>\n\n자주 수행하는 테스트 중 하나는 두 기능이 동일한 확률 분포에서 나왔는지 확인하는 것입니다. 물론, 테스트 데이터 세트에서 사용하는 열이 교육 데이터와 동일한 분포를 가져야 합니다. 그렇지 않으면 모델이 학습한 패턴은 테스트에서 완전히 쓸모 없게 될 것입니다!\n\n이를 위해 Kolmogorov-Smirnov 테스트라는 테스트를 사용할 수 있습니다. 이 테스트는 scipy 라이브러리에서도 제공됩니다.\n\n이 시점에서 PyTest로 이러한 확인을 구현할 수 있어야 합니다.\n\n사실 데이터 세트의 다른 열에 대해 여러 가설 검정을 실행할 때, 선택한 알파에 본페로니 교정이 필요합니다. 이에 대해 다음 기사에서 살펴볼 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n이 기사에서는 데이터 입력 파이프라인의 주요 구성 요소인 ETL에 대해 이야기했습니다. ETL은 추출(extraction), 변환(transformation), 로드(load)의 약어입니다. 또한 데이터 과학자가 작업 중인 데이터를 유효성 검사하는 중요성에 대해 이야기했습니다. 이 유효성 검사는 우리가 사전에 예측한 예상 출력을 알고 있는 결정론적 테스트로 수행되거나, 우리의 가정을 통계적 테스트로 확인할 수 있는 비결정론적 테스트로 이루어집니다. 이러한 테스트는 모두 모든 데이터 과학자에게 매우 중요한 도구인 PyTets를 사용하여 실행되며, 이는 코드를 깨끗하게 유지하고 코드 내의 오류를 최소화하는 데 도움이 됩니다.\n\n이 기사가 마음에 든다면 Medium에서 제 팔로우 하세요! 😁\n\n💼 Linkedin ️| 🐦 X (Twitter) | 💻 Website","ogImage":{"url":"/assets/img/2024-06-19-MLOpsDataValidationwithPyTest_0.png"},"coverImage":"/assets/img/2024-06-19-MLOpsDataValidationwithPyTest_0.png","tag":["Tech"],"readingTime":8}],"page":"64","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}