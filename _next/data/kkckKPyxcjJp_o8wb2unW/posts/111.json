{"pageProps":{"posts":[{"title":"블라인드 SQL Injection 관리자 패스워드 한 글자씩 알아내기-Lab9","description":"","date":"2024-05-27 13:06","slug":"2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9","content":"\n\n안녕 친구. 다시 오신 걸 환영합니다. 이번에도 이전 글을 이번 글의 끝에 링크하겠습니다.\n\n# Lab9: 조건부 응답으로 인한 Blind SQL Injection\n\n이 랩에는 Blind SQL Injection 취약점이 포함되어 있습니다. 애플리케이션은 분석을 위해 추적 쿠키를 사용하고, 제출된 쿠키 값이 포함된 SQL 쿼리를 수행합니다.\n\n<div class=\"content-ad\"></div>\n\nSQL 쿼리의 결과가 반환되지 않고 오류 메시지가 표시되지 않습니다. 그러나 쿼리가 어떤 행도 반환할 때 페이지에 \"다시 오신 것을 환영합니다\" 메시지가 표시됩니다.\n\n데이터베이스에는 사용자 이름과 비밀번호라는 열이 있는 다른 테이블인 사용자가 있습니다. 관리자 사용자의 비밀번호를 알아내기 위해 시각 SQL 인젝션 취약점을 악용해야 합니다.\n\n이 랩을 해결하려면 관리자 사용자로 로그인하세요.\n\n해결책\n\n<div class=\"content-ad\"></div>\n\n이 시나리오는 이전에 다룬 문제와 비슷해 보입니다. 이전에 작성한 글에서는 카테고리를 클릭하면 인터페이스의 왼쪽 상단에 표시되는 웰컴 백 메시지가 나타납니다.\n\n![이미지](/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_1.png)\n\n버프 스위트를 열고 몇 가지 요청을 보냈으며, 이 중 하나를 Repeater로 보내어 불리언 페이로드를 사용하여 SQLi 취약점을 가진 쿠키 값을 테스트했습니다.\n\n```js\n' AND 1=1-- # 웰컴 백 메시지를 받는 결과\n\n' AND 1=2-- # 웰컴 백 메시지를 받지 못함.\n```\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_2.png\" />\n\n내가 시도한 것은 다음과 같았어:\n\n```js\n' UNION SELECT username,password FROM users--\n```\n\n하지만 잘 되지 않았어. 심지어 시간 기반 페이로드도 작동하지 않았어.\n\n<div class=\"content-ad\"></div>\n\n수업 실습을 검토하면, 데이터를 추출하기 위해 substring을 사용할 수 있다는 제안이 있었습니다. 따라서, username이 \"administrator\"인 것을 알면, 비밀번호만을 추출할 필요가 있습니다.\n\n<img src=\"/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_3.png\" />\n\n우리의 페이로드는 다음과 같아야 합니다:\n\n```js\n' AND SUBSTRING((SELECT password FROM users WHERE username='administrator'), 1, 1) = 'a\n```\n\n<div class=\"content-ad\"></div>\n\n1 (시작 위치): 이는 부분 문자열 추출이 문자열(비밀번호)의 첫 번째 문자에서 시작해야 함을 지정합니다.\n1 (길이): 이는 하나의 문자만 추출해야 함을 나타냅니다.\n\n이것을 침입자에게 보내서 payload가 작동하는지 확인하기 위해 a=z로 대체하고 0-9로 자동으로 테스트하도록 지시합시다.\n\n![이미지](/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_4.png)\n\n<div class=\"content-ad\"></div>\n\n저희의 페이로드가 작동 중이에요. 아래 스크린샷을 확인해주세요. 컨텐츠 길이의 차이를 주목해 주세요. A에 도달하면 \"welcome back\" 메시지가 반환돼요.\n\n![이미지](/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_5.png)\n\n다음 단계로 넘어가야 해요. 다만, 패스워드의 길이를 모르기 때문에 패스워드를 알아내는 데 얼마나 시간이 걸릴지 알 수가 없어요. 수동으로 해야 할 것 같아요.\n\n다음에 사용할 페이로드는 다음과 같을 거에요:\n\n<div class=\"content-ad\"></div>\n\n```js\n' AND SUBSTRING((SELECT password FROM users WHERE username='administrator'), 2, 1) = 'a\n```\n\n![Blind SQL Injection](/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_6.png)\n\n![Blind SQL Injection](/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_7.png)\n\n제가 'a7eb5rsh00a9n7jffq9v'라는 패스워드를 추출했어요. 하지만, Burp Suite Repeater를 사용하여 확인해보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n페이로드:\n\n```js\n' AND SUBSTRING((SELECT username FROM users WHERE password='a7eb5rsh00a9n7jffq9v'), 1, 1) = 'a\n```\n\n![이미지](/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_8.png)\n\n관리자용으로 올바른 자격 증명이 필요합니다. 도전적이었지만 새로운 것을 배웠어요😊. 끝까지 머물러 주셔서 감사합니다. 재미있고 유익했다면 50번 클릭해주세요😊.","ogImage":{"url":"/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_0.png"},"coverImage":"/assets/img/2024-05-27-BlindSQLInjectionUncoveringAdministratorPasswordOneCharacterataTime-Lab9_0.png","tag":["Tech"],"readingTime":3},{"title":"비기술 직군 팀에 SQL을 가르치면서 얻은 교훈","description":"","date":"2024-05-27 13:06","slug":"2024-05-27-LessonsfromTeachingSQLtoNon-TechnicalTeams","content":"\n\n## 조정된 방식에서 더 맞춤화된 방식으로 — 그리고 먼 거리에서의 코칭이 미래라고 생각하는 이유\n\n나의 경력 동안, 내가 내부 SQL 교육을 진행하는 다양한 상황에 처해왔습니다. 이러한 교육 세션은 항상 제게 최우선 순위는 아니었지만, 가장 만족스러운 프로젝트 중 하나였습니다. 누군가가 자신의 쿼리를 실행하는 데 익숙해지고, 스스로 필요한 정보를 찾아 대시보드를 작성하며, 이 새로 습득한 기술에 흥분하는 것을 보게 되면, 나는 몰라요 — 그냥 기분이 좋습니다.\n\n최근에, 예전에 교육받은 한 명의 \"학생\" 이름이 한 명의 어려운 SQL 질문을 하기 위해 공동 그룹에 등장한 것을 보았는데, 그때 나의 반응은 마치 \"다크 나이트 라이즈\"에서 알프레드가 브루스 웨인에게 고개를 끄덕이는 것과 같았습니다 (만약 이 참조를 모르시면 여기 있습니다).\n\n이 기사의 목표는 내가 내부 SQL 교육을 운영하면서 처한 내 여정과 배운 점을 전달하여 전체적으로 비기술적인 (또는 적어도 SQL을 잘 알지 못하는) 팀에게 가르칠 수 있는 방법을 알려주어, 당신의 조직에서 지식의 선물을 나누고 나처럼 유사한 기쁨을 느낄 수 있기를 희망합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-LessonsfromTeachingSQLtoNon-TechnicalTeams_0.png\" />\n\n# 처음으로 이 교육을 진행한 이유\n\n일반적으로, 나를 교육을 진행하게 한 상황은 크게 두 가지 범주로 나뉩니다:\n\n- 역량 강화 필요: 조직 내에서 SQL 역량 부족으로 인해 한계에 부딪히는 경우가 있습니다. 이는 여러 도구와 스프레드시트를 사용하여 최종 보고서에 도달하기 위한 혼잡한 프로세스의 출현으로 일반적으로 드러납니다. 당연히 해결책이 항상 SQL 쪽에 있는 것은 아니지만, 제 경험상 시간이 많이 소요되는 다중 단계 프로세스 중 하나를 보유하고 있고 내면에 더 나은 방법이 있다고 생각한다면, 아마도 그 방법이 있을 확률이 높습니다.\n- 자원 부족: 분석 관련 자원이 부족한 조직에서는 \"이웃 스킬\"을 갖춘 개인들(즉, 스프레드시트 및 데이터 작업에 익숙한 사람들)을 식별하고 역량 개발을 제안하는 것이 조직과 개인 양쪽에 매우 유익하다고 생각했습니다. 개인의 시야를 확대하면서 사업에 더 많은 가치를 창출할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이러한 교육을 진행하고 싶은 이유는 다양할 수 있습니다 (위 목록이 전부가 아닙니다; 이는 상호배타적인 것도 아니라는 주장이 나올 수 있습니다). 이곳에서 이루고자 하는 목표를 명확히 하는 것이 중요합니다. 목표에 따라 교육을 실행하는 방식이 크게 달라질 수 있습니다.\n\n# 초기 반복 사례 또는 \"만능\" 유형의 교육 한계를 발견한 방법\n\n2015년의 초기 버전에서 저는 점진적인 방식을 시도해 보았습니다. 보통의 교실 형식으로, X주 동안의 프로그램을 제공했습니다. 매주 1시간씩 수업을 진행했으며(항상 같은 요일 같은 시간에), SQL을 배우고자 하는 모든 관심 있는 사람들에게 열려 있었습니다. 주로 SQL에 초점을 맞춰 진행되었습니다.\n\n- 매주 그룹은 무언가 새로운 것을 배웠으며, SQL의 \"Hello World\"부터 시작해 (SELECT * FROM TABLE LIMIT 1) CTE 여러 개로 윈도우 함수를 사용하는 방법, 쿼리 최적화까지 모두 포함했습니다.\n- 각 수업 간에 그룹은 수업에서 배우는 지식을 시험하고 고착화하기 위해 숙제를 수행해야 했습니다.\n\n<div class=\"content-ad\"></div>\n\n일부 사람들은 끝까지 계속했지만 성공률(성공은 누군가가 교육 후에 새롭게 습득한 SQL 기술을 계속 사용하는 경우로 정의됨)은 극히 낮았습니다. 매 세션마다 오는 사람들이 점점 줄어들었습니다. 수업 외에 제안된 연습을 하는 사람은 소수였습니다. 사실적으로 말하자면 성공하지 못했어요.\n\n하지만 이로부터 많은 교훈을 얻었습니다:\n- 멘토링을 즐겼습니다: 다른 사람들에게 새로운 기술을 가르치고 지도하는 즐거움에 대해 배웠고, 결국 이 블로그와 다른 다양한 활동을 통해 보상을 얻었다.\n- SQL이 \"너무 기술적이다\"는 두려움: 많은 사람들이 그 무료 교육에 참여하지 않았거나 매우 첫 번째 장애물에서 포기했던 이유는 SQL을 기술적인 사람들만을 위한 것으로 생각했기 때문이고, 그들은 자신을 기술적인 사람으로 생각하지 않았기 때문입니다.\n- \"유지\" 메커니즘 없이 교육을 실시하는 것은 실패할 운명이다: 사람들이 이 교육을 완수할 수 있을 것이라는 사람들의 자율을 믿는 것은 합리적인 생각이 아님을 이해하게 되었습니다. 어떤 조직에서든 지속적인 교육을 완수하지 않을 수 있는 많은 경쟁 우선 순위와 사유가 있습니다. 따라서 학생들을 찾아내어 교육을 듣기 위해 강한 내재적 동기부여가 있는 사람들(예: SQL을 배우기 위한 명확한 목표가 있는 경우)이나 강력한 외부적 동기부여를 제공해야 합니다(예: 그들의 매니저가 SQL을 배우라고 요구하여 더 기술적인 프로젝트를 맡을 것을 기대하는 경우).\n- SQL을 가르치는 것은 방정식의 한 부분일 뿐입니다: 마지막으로, 더 중요한 것은 SQL을 가르치기 위해 SQL만을 가르치는 것이 중요하지 않다는 것을 깨달았습니다. 누구나 SQL을 고립시키지 않고 사용합니다. SQL의 현실은 다음과 같습니다:\n\n- SQL 코드를 작성하기 전에 조직 내에서 올바른 데이터 세트를 찾아야 합니다(성숙한 조직에서 쉬울 수 있지만, 성장하기 시작한 조직이나 존재하지 않는 조직에서는 복잡할 수 있습니다).\n- 데이터 세트를 찾았다면, 쿼리에 쓸 올바른 필드를 찾아야하고 이 필드가 원하는 정보를 담고 있는지 확인해야 합니다(이것 또한 하나의 기술입니다).\n- 그러고 나서 데이터 세트에 액세스 권한을 요청해야 하며, 액세스 권한이 승인되면 특정 가이드라인과 기능이 있는 특정 도구에 SQL 코드를 작성해야 합니다.\n- 쿼리를 작성하는 동안, 컴퓨팅 비용을 주시하고 쿼리를 실행하기 전에 필요에 따라 다시 구조를 잡아야 합니다.\n- 등등. 만약 이러한 요소들을 가르치지 않는다면, 학생들이 SQL을 사용하기 어려울 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 모든 배움이 나의 프로그램을 개선하는 데 길을 열었다 - 더 맞춤화된 방식으로.\n\n# 더 최근에 - 더 맞춤화된 방향으로의 전환\n\n위와 같이 개선된 몇 차례의 반복 뒤, 길을 따라 얻은 모든 배움을 반성하고 새로운 방식을 시도해 보았습니다: 규모를 잊고 완전히 반대 방향으로 나아갔습니다. 전체 반에서 매주 1시간 수업을 듣는 대신, 몇 명 선택된 개인들과 매주 짧은 1:1 세션을 갖기 시작했습니다.\n\n프로그램은 여전히 누구에게나 열려 있었지만, 이제는 참여할 수 있는 사람을 선정하는 프로세스가 있었습니다. \"들어가고 싶은\" 사람들은 다음을 보여 주어야 했습니다:\n\n<div class=\"content-ad\"></div>\n\n- SQL 학습에 대한 명확한 필요성: 잠재적인 학생들은 SQL을 배우고 싶은 이유와 SQL이 필요한 프로젝트를 설명하는 양식을 작성해야 했습니다(예: \"X 보고서를 자동화하고 싶어요, Y 대시보드를 구축하고 싶어요\"). 선택되면, 이 프로젝트가 프로그램 전체 기간 동안 작업할 프로젝트가 되었을 것입니다.\n- 이미 있는 인접한 기술들: 잠재적인 학생들은 \"인접 기술\"이라고 부르는 것을 보여줘야 했습니다. 즉, SQL이나 데이터 분석을 위한 필요한 기술과 유사한 기술들을 보여주어야 했습니다.\n- 일정에 충분한 시간을 할애할 수 있는 능력(및 의지): 프로그램에 대한 \"신청\"의 일환으로 - 학생들은 자신의 \"학습\" 프로젝트를 자신의 관리자와 검증하고 다음 X 주 동안 적어도 X0%의 시간을 학습에 할애할 의지가 있어야 했습니다. X0%는 많이 보일 수 있지만 - 사실 그것은 X0%에 관한 것이 아닌 메시지를 보내는 것이었습니다. 이 프로그램은 시간이 많이 소요되므로 잠재적인 학생들은 성공하기 위해 필요한 시간을 할애할 수 있는지 확인해야 했습니다.\n\n교육 자체에 대해서 - 초점은 SQL에서 프로젝트로 전환되었습니다. 교육의 첫 세션은 그들의 프로젝트를 마일스톤으로 나누는 데 시간을 보냈습니다. 첫 번째 마일스톤은 모두에게 동일했습니다: SQL 기초를 배울 수 있는 (온라인 또는 오프라인, 무료 또는 유료 - 본인이 선호하는 것으로) 자원을 찾아내고 완료하는 것입니다.\n\n저는 이것이 약간 실망스러울 수 있다는 것을 인식하고 싶습니다 - \"SQL 가르치기\"에 대한 글을 \"SQL 학습\" 부분에서 그렇게 탐내지 않을 것이라고 기대할 수 있습니다. 저의 일반적인 신념은 SQL의 주요 개념을 매우 짧은 시간에 배울 수 있지만, 실제로 여러 달 또는 몇 년이 걸려야 진정으로 뛰어난 수준에 도달할 것이라고 생각하며, 가능한 빨리 실제 문제에 적용하기 시작하면 더 빨리 견고한 수준에 도달할 수 있다고 생각합니다. 따라서 프로그램의 대부분은 실제 문제에 적용하는 데 소요되며, 기본적인 SQL을 이해하는 데 (인터넷의 훌륭한 것을 통해 쉽게 얻을 수 있는 것) 소요되는 시간에 대해 많은 시간이 소요되는 것은 아닙니다.\n\n위의 첫 번째 단계가 완료되면, 우리는 다음 마일스톤을 향해 노력할 것입니다. 예를 들어, 대시보드를 구축하고 싶은 사람을 위해 프로젝트를 분할해보면:\n\n<div class=\"content-ad\"></div>\n\n- SQL 기초 학습\n- 적절한 데이터셋 및 쿼리 로직 찾기 (필요한 정보 획득 방법 학습)\n- 필요한 경우 데이터베이스 구축 (데이터베이스 구축과 관련된 역할 및 책임)\n- 이 데이터베이스를 대시보드 도구에 연결\n- 대시보드 설계\n- 대시보드 작성\n\n그리고 여기서, 매주 다양한 이정표에 도달할 것으로 예상했습니다. 학생들은 주에서 어디서 걸릴 경우 언제든지 제게 피드백을 요청하거나 이메일을 보낼 수 있었지만, 일반적으로 진행 상황에 따라 독립적으로 이정표를 완수해야 했습니다.\n\n이 시스템을 통해 낮은 실패율을 관찰했습니다 (성공은 훈련 후에 새로 습득한 SQL 기술을 계속 사용하는 사람으로 정의됩니다). 이때 뒷받침되는 이유들을 곰곰히 생각해보면 이러한 이유가 있습니다:\n\n- 선발 과정 추가: 더해진 선발 과정은 실제 프로젝트를 가진 가장 동기가 부여된 사람들만 훈련의 일부가 되도록 보장했습니다.\n- 이정표 시스템은 강제 기능이 좋았습니다: 목표를 설정하는 것은 훌륭한 시작입니다, 그러나 목표를 달성하기 위해 필요한 단계나 궁극적으로 목표를 달성할 때 필요한 작업에 대해 고민해보지 않으면 목표를 달성할 가능성이 적습니다. 명확한 마감일 아래에서 명확한 결과물을 제공하는 이정표 시스템은 학생의 성장을 크게 도와주는 책임감과 피드백 루프를 만들어냅니다.\n- 처음부터 올바른 기대 설정이 모든 것을 더욱 단순하게 만들었습니다: 어떤 것을 성공으로 이끌어가는 큰 부분은 마음가짐과 일에 대한 우리의 인식과 연관이 있다고 믿습니다. 이 프로그램을 시작하자마자, 올바른 기대 설정을 하려고 노력했습니다: (1) 시간이 많이 들 것이다 (2) 도전적일 것이다 (3) 오랜 기간이 걸릴 것이다\n- (4) 그러나 시간을 투자하고 도전을 하나씩 극복하겠고, 궁극적으로 승리할 것입니다\n- 사람들에게 SQL 학습 방법을 가르치는 것 대 SQL을 가르치는 것: 마지막으로 — 이 주요 변경 사항이 프로그램에서 큰 차이를 만들었습니다. 이것은 사용자들이 필요한 핵심 정보를 찾아내고 실험하며 배우면서 익숙해지도록 했습니다. 그들이 더 자립적이 되어 계속成长할 수 있도록 했고, 프로그램이 종료된 후에도 지속적으로 발전할 수 있도록 했습니다.\n\n<div class=\"content-ad\"></div>\n\n지금까지 위의 방법은 제가 시도한 가운데 가장 성공적인 하나입니다. 그러나 시간이 많이 소요되고 개선할 여지가 많이 보입니다.\n\n# 보다 하이브리드 방식으로\n\n이 시점에서 가장 중요한 질문은 다음과 같습니다: 위의 프로그램을 어떻게 확장할 수 있을까요? 이 교육에서 제가 한 역할을 반성해보면, 주로 방향을 제시하고 사람들을 정직하게 유지하는 데 중점을 두었습니다:\n\n- 처음에: 학생들이 프로젝트를 구조화하고 단계별로 나누는 데 도움을 주었습니다.\n- 각 단계마다: 각 장애물에 접근하는 가장 좋은 방법에 대한 조언을 했습니다. 만약 막힌다면, 어떻게 해제할지에 대한 지침을 제공했습니다.\n- 프로그램 전반에 걸쳐: 그들의 승리를 축하하고 도전하며, 힘들 때 동기부여를 시도했지만, 동시에 그들이 설정한 일정 내에 무슨 일을 해야 하는지 제시했습니다.\n\n<div class=\"content-ad\"></div>\n\n위의 내용을 자동화하는 것은 어렵지 않을까요? 혹은 혹시 LLMs를 이용해서 가능할지도 모르겠네요. 요즘 세상은 뭐가 되는지 모르겠어요. 그래도 어떻게든 표준화하고 최적화할 수 있고, 비동기적으로 많은 작업을 처리할 수 있으니 매주 회의를 필요로 하지 않는 방식으로 개선할 수 있을 거예요. 다음 반복에서 저는 학생 당 소요 시간을 줄여서 더 많은 학생들을 교육할 수 있는 방법을 시도해보고 싶네요.\n\n저자 주: 요즘 피트니스 인플루언서들이 “거리에서의 코칭”을 제공하고 있는 것을 점점 더 많이 보게 되는데, 여기서 코치와 이메일로 소통하고 훈련 영상을 보내며 맞춤형 프로그램을 받을 수 있어요. 데이터 분석에서도 비슷한 방법이 있을 수 있을까요?\n\n프로그램 자체에 대해, “커뮤니티” 요소를 통합하고 싶어요. 특히, 페이만 기법을 강력하게 신봉하는 편인데요. 페이만 기법은 배운 것을 가르치는 것인데요. 구체적으로 말하자면, 학생들이 배운 내용을 문서화하고 새로운 학생들에게 공유하도록 유도하고 싶어요 (마치 영화 “이웃에게 선물하기”처럼 말이에요). 여기에는 몇 가지 장점이 있을 거 같아요:\n\n- 이를 통해 프로그램의 규모를 확장할 수 있고, 더 많은 사람들이 지식을 활용할 수 있게 될 거예요\n- 이제는 선생님인 학생들이 이해해야 할 핵심 개념을 내재화하고 자신의 이해의 빈틈을 찾아낼 수 있게 도와줄 거에요\n- 거대한 지식 베이스를 시작할 수 있고, 그러면 프로그램에 참여할 수 없는 고도로 동기 부여된 개인들을 위한 자기 서비스 접근 방식을 더 활용할 수 있게 될 거에요.\n\n<div class=\"content-ad\"></div>\n\n항상 그렇지만, 아이디어는 쉽게 얻을 수 있어요. 실행 단계에서 어떤 것이 잘 작동하고 어떤 것이 그렇지 않은지를 이해하게 돼요. 곧 그것을 실험해 보고, 나중에 미래의 글에서 결과를 공유할 예정이에요.\n\n# 결론\n\n지난 8년 동안, 동료와 보고서를 SQL 전문가로 발전시키는 여러 프로그램을 시도해봤어요. 항상 성공한 것은 아니었지만, 몇 년 전에 광범위한 프로그램에서 좀 더 맞춤화된 멘토십으로 전환한 것이 많은 성공과 유익한 교훈을 안겨줬어요.\n\n지금 나의 진정한 도전은 그 방법을 확장하는 것이에요. 어떻게 하면 선택된 개인들을 위해 최대한 가치를 창출하는 데 집중하기 위해 모든 불필요한 것을 단순화하고 제거할 수 있을까요? 그렇게 하면 그들이 자신의 조직에서 일으키는 영향력을 10배로 향상시킬 수 있게 될 거에요. 아마도 피트니스 인플루언서들이 뭔가를 알고 있을지도 몰라요…\n\n<div class=\"content-ad\"></div>\n\n# 이 글을 즐겁게 읽으셨기를 바랍니다! 공유하고 싶은 조언이 있으시면 댓글 섹션에 남겨주세요!\n\n그리고 더 많이 읽고 싶으시다면, 아마도 다음 게시물들도 맘에 드실지도 몰라요:\n\nPS: 본 글은 다양한 분석 업무 경험을 바탕으로 얻은 지식을 담은 뉴스레터인 'Analytics Explained'에 동시 게시되었습니다. 싱가폴 스타트업부터 SF 대형 기업까지에서 배운 내용을 요약하고, 독자들의 분석, 성장, 경력에 관한 질문에 답변하고 있습니다.","ogImage":{"url":"/assets/img/2024-05-27-LessonsfromTeachingSQLtoNon-TechnicalTeams_0.png"},"coverImage":"/assets/img/2024-05-27-LessonsfromTeachingSQLtoNon-TechnicalTeams_0.png","tag":["Tech"],"readingTime":8},{"title":"세계에서 가장 강력한 SQL LLM을 구축하는 Snowflake의 방법","description":"","date":"2024-05-27 13:03","slug":"2024-05-27-HowSnowflakebuildingthemostpowerfulSQLLLMintheworld","content":"\nAI가 데이터 민주화를 위한 새로운 기회를 만들어냈습니다. SQL을 필요로 하지 않고 언어 모델 이해력의 힘을 이용하여 통찰력을 얻는 것으로 많은 사용자가 현재 대량의 데이터에 감춰진 가치 있는 통찰을 발견할 수 있게 되었습니다.\n\n지난 달, Snowflake의 공학 부사장인 Vivek Raghunathan이 Fully Connected 컨퍼런스에서 혁신적인 Snowflake Copilot에 대해 이야기했습니다. 이 비디오는 지난 주 YouTube에서 공개되었습니다. WrenAI 팀은 몇 달 동안 텍스트를 SQL로 변환하는 작업을 진행해왔고, 우리는 Vivek이 최근에 공유한 SQL LLM 주제에 대한 생각과 기술을 많이 배웠습니다. 오늘, 이 비디오에서 얻은 생각과 교훈을 정리하고자 합니다. 이것이 다른 개발자들이 텍스트를 SQL로 더 빨리 혁신할 수 있도록 도움이 되기를 바랍니다!\n\n만약 전체 비디오에 관심이 있다면, 아래 링크를 확인해보세요!\n\n자, 시작해봅시다!\n\n<div class=\"content-ad\"></div>\n\n# 데이터 분석에서 인공지능의 현재와 미래\n\nVivek은 데이터 분석에서 인공지능의 현재와 미래에 대해 이야기했습니다.\n\n## 현재: 당신을 돕는 인공지능\n\n- 분석가를 위한 대화형 도우미\n- 자연어를 SQL로 전환: 분석가가 SQL을 실행\n- 인터페이스를 통해 반복적인 데이터 및 스키마 발견\n\n<div class=\"content-ad\"></div>\n\n하지만 미래에는 비즈니스 사용자들을 위한 완전한 대화형 기사로 발전할 것입니다. SQL을 모르는 사람들도 자연어로 질문하고 답변을 받을 수 있어서 더 많은 개인들이 능력을 얻을 것입니다.\n\n## 미래: 의존할 수 있는 AI\n\n- 비즈니스 사용자들을 위한 대화형 기사\n- 답변을 위한 자연어: SQL 전문 지식 불필요\n- 상호작용 가능한 데이터 및 시각화를 허용하는 인터페이스\n\n## 우리의 생각:\n\n<div class=\"content-ad\"></div>\n\nWrenAI를 개발하면서도 이를 경험했습니다. 현재 기술 혁신의 한계로 모든 비즈니스 사용자를 대상으로 한 완전한 AI 팔로우업 서비스를 제공하는 것은 여전히 제한적인 면이 있습니다. 이는 자율 주행 자동차에서 경험하는 것과 비슷합니다. 수년 동안 자동차 제조사들은 운전자와 동행하는 자율 주행 시스템을 출시해왔습니다. 자율 주행 기능을 통해 고객과 운전자 경험이 전환되는 것을 확인할 때까지는 여전히 데이터 분석가들의 도움이 필요할 것입니다.\n\n# 현실 세계에서 Text-to-SQL은 그리 간단한가요?\n\nVivek는 Text-to-SQL이 마치 빙산 문제와 같다고 언급했습니다. 문제는 표면상 간단해 보이지만 실제로는 매우 복잡하다고합니다.\n\n대화 중 인용문:\n\n<div class=\"content-ad\"></div>\n\n그의 강연에서 언급된 몇 가지 분명한 도전 과제들은 빠르게 직면하게 될 것입니다:\n\n- 현실 세계는 지저분한 스키마와 데이터를 가지고 있으며, 종종 수만 개의 테이블과 수십만 개의 열이 포함된 데이터베이스가 있습니다.\n- 현실 세계의 의미론은 더 복잡합니다: rev1, rev2 및 rev3로 레이블이 지정된 열을 가진 테이블이 있을 수 있지만, 수익 열은 무엇인가요? 미국 달러인가요, 아니면 현지 통화입니까? 몇 주 전에 전송된 이메일에서 폐기되었나요? 이 중 어느 것이 현재 진실의 근원인가요?\n- 테이블 간에는 올바르게 결합할 수 있는 여러 가지 방법이 있어 더욱 복잡해집니다.\n\n## 저희의 생각\n\n정말 간단한 문제가 아니네요!\n\n<div class=\"content-ad\"></div>\n\nWrenAI는 데이터와 의미 사이의 도전을 해결하는 데 주안점을 둡니다; 텍스트에서 SQL로 신뢰할 수 있는 변환을 만들기 위한 핵심은 기존 데이터 구조의 위에 의미 아키텍처에 대응하는 신뢰할 수 있는 의미 엔진을 구축하는 방법입니다. 실제로, 의미 관계, 계산, 집계를 정의하는 것과 같이, LLMs는 서로 다른 시나리오에서 서로 다른 문맥을 다루는 방법을 배워야 합니다. 이는 견고한 의미적 계층에 많이 의존합니다.\n\n# Snowflake 실험 v0부터 v4까지\n\nSnowflake는 v0부터 v4까지 여러 번의 실험을 거쳤습니다. 다행히도 Vivek은 개선된 텍스트에서 SQL로의 혁신을 위해 다음 버전에서 시도하고 배운 내용을 너그럽게 공유했습니다.\n\n자세히 알아봅시다!\n\n<div class=\"content-ad\"></div>\n\n# V0: Optimized for Spider\n\n대화에서 Vivek은 말했습니다:\n\n## 처음으로 직면한 빙산 (도전):\n\nV0 버전에서는, 최상의 모델을 사용하여 스파이더 데이터셋을 사용하여 82%의 결과를 달성하였지만, 최적화되지 않은 Zero-shot GPT-4는 74%의 결과를 가져왔습니다. 그러나 실제 세계 데이터에서는, 최상의 모델을 사용하여 정확도가 9%로 떨어지고, prompt-optimized된 GPT-4를 사용하면 14%의 결과를 얻었습니다.\n\n<div class=\"content-ad\"></div>\n\n그들이 중요성을 깨닫기 시작한 때입니다. \"의미론적 카탈로그\"는 의미론이 데이터 검색에 엄청 중요하다는 것을, 사전 훈련된 LLMs가 귀하의 비즈니스 콘텍스트에 대해 아무것도 모르기 때문에 유일한 방법은 RAG를 통해 의미론을 제공하는 것입니다.\n\n## 우리의 생각:\n\n의미론은 텍스트에서 SQL로의 전환 도전을 해결하는 데 핵심적이며, WrenAI를 구현하기 시작할 때 중심적인 핵심 설계입니다.\n\n# V1: 실세계에서 검색이 중요합니다\n\n<div class=\"content-ad\"></div>\n\nV1 버전에서는 Snowflake 팀이 고민하며 시작했습니다. 웹 품질의 검색을 기업 메타데이터 검색에 적용하고 그 결과를 LLM(언어 모델)에 공급한다면 성능이 혁신적으로 향상될 수 있다는 아이디어가 나왔죠.\n\n다시 말해, 이 LLM에 어떤 내용을 포함해야 할지라는 더 단순한 문제를 해결하기 위해 보다 어려운 문제를 먼저 해결하려고 합니다.\n\n아래는 결과인데, Snowflake의 최고 모델은 9%에서 24%로 개선되었고, GPT-4 버전도 14%에서 28%로 성장했습니다. 따라서 의미론적 카탈로그 검색이 중요하다는 직관이 옳다는 것이 입증되었습니다.\n\n![Snowflake](/assets/img/2024-05-27-HowSnowflakebuildingthemostpowerfulSQLLLMintheworld_0.png)\n\n<div class=\"content-ad\"></div>\n\nVivek 씨가 말했던 것처럼, Snowflake에서 대화 카탈로그 검색 결과를 어떻게 검색하는지 설명했어요.\n\n## 그들이 직면한 두 번째 어려움(도전):\n\nSnowflake 팀이 직면한 다른 어려움은 레이터입니다. 모든 것을 수작업으로 다시 주석을 다는 작업을 했고, 성능을 평가하기 위해 더 복잡한 예제를 추가했으며, 단일 테이블 대 복수 테이블 및 간단한 조인 대 복잡한 조인과 같은 데이터 의미론에 기반하여 슬라이스를 나눴다.\n\n아래에는 Vivek가 그의 발표에서 공유한 내용이 있습니다:\n\n<div class=\"content-ad\"></div>\n\n## 우리의 생각:\n\n이 통찰은 우리에게 흥미로워 보입니다. WrenAI를 구현할 때 검색 최적화를 많이 하지 않았습니다. 현재는 벡터 저장소에서 Top-N 선택만 사용합니다. 검색 기술의 더 자세한 세부 사항을 살펴보고, 이 영역을 개선하기 위한 작업을 진행 중입니다.\n\n우리 팀은 또한 내부 검증 데이터 세트를 구축하고 있습니다. 이 데이터 세트는 단일 및 다중 테이블, 그리고 단순 및 복잡한 조인 데이터 세트를 포함하여 보다 복잡한 시나리오를 갖도록 설계되어 있습니다. 이로써 솔루션이 실제 세계에서 정확하게 유지되도록 합니다.\n\n이 주제에 대한 토론을 위한 새로운 이슈를 오픈해 주시면 환영입니다! 우리 팀은 더 많은 개선사항을 살펴보기를 원합니다!\n\n<div class=\"content-ad\"></div>\n\n# V2: Text2SQL 모델링 통찰\n\n다음으로, Vivek은 이러한 발전에도 불구하고 모델이 대화 능력에서 여전히 어려움을 겪고 있다고 공유했습니다. SQL 작업을 최적화했지만 대화를 처리하고 지시를 따르는 것에 어려움이 있었다고 합니다.\n\n아래는 그들이 공유한 몇 가지 통찰입니다:\n\n- 더 나은 기본 LLMs: 코드 LLMs가 SQL 작업에서 아주 잘 수행된다는 것이 밝혀졌습니다.\n- 더 나은 신호: 몇 가지는 LLM 생성에서 나오며, 예를 들어 주석 등이 있습니다. 일부는 스노우플레이크 문서 등의 고전적인 기술에서 얻어집니다. 그 중 하나는 쿼리 이력인데, 실제 사람들이 실제로 하는 작업에 관한 보물창고입니다.\n- 사고의 연결: 처음에는 테이블을 선택하고, 그 다음에 조인하고, 그 다음에 열을 선택하고, 그리고 마지막으로 디코드 시에 정확성을 확인합니다. LLM이 JSON을 생성할 때는 출력이 스키마와 일치하는지 확인하는 의존성 파서가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 새로운 깔끔한 평가를 통해 상당한 개선을 확인했습니다. 기본 모델 27%가 39%가 되었고, GPT-4를 사용한 경우 40%가 46%로 상승했습니다.\n\n## 그들이 마주한 세 번째 빙하(도전):\n\n다음 도전 과제는 제로 샷 텍스트 - SQL 솔루션 대신 대화형 공동 조종사를 구축하는 것입니다. 대화를 처리하고 분석가들이 쿼리를 점진적으로 개선할 수 있도록 해야 합니다.\n\n시스템의 한 부분을 최적화하면 전체 시스템이 의도치 않게 최적화되지 않을 수 있습니다. 두 가지 큰 문제가 발생했습니다:\n\n<div class=\"content-ad\"></div>\n\n- 모델은 지시에 따라 더 이상 우수하지 않았습니다. 왜냐하면 텍스트-SQL 작업만을 보았기 때문입니다.\n- 이는 대화에서 메모리가 떨어졌습니다. 여러 턴의 경우가 아닌, 제로샷 사례만을 경험했기 때문입니다.\n\n## 저희의 생각\n\n이 강의에서 언급한 텍스트-SQL 도전 과제는 제로샷 방식이 아닌 상호작용적인 접근이 필요합니다. WrenAI에서도 이러한 접근을 추구하고 있으며, 지속적으로 개선하고 있는 몇 가지 경험을 진행 중에 있습니다.\n\n우리의 구현을 통해 \"증강 및 생성\" RAG 파이프라인에서 검증, 수정 및 명확화 다이얼로그와 같은 정교한 프로세스를 구현했습니다. 이를 통해 LLMs가 사용자의 의도를 완전히 이해할 수 있도록 지원하고 있습니다. 물론, 계속해서 개선할 부분이 많이 남아 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Snowflake](/assets/img/2024-05-27-HowSnowflakebuildingthemostpowerfulSQLLLMintheworld_1.png)\n\n## V3: 지시 따르기, 도구 사용\n\nVivek은 텍스트를 SQL로 바꾸는 작업과 더 일반적인 지시 따르기 작업을 섞어 LLM을 지시 따르도록 다시 교육했다. 그리고 이를 Multi-LLM 설정에 오케스트레이터 LLM으로 계층화했다.\n\n오케스트레이터 모델의 책임은 고객과 대화하는 것입니다. 이는 주체적인 방식과 유사합니다. SQL을 작성해야 할 때마다 다른 크기의 텍스트를 SQL 모델을 사용하도록 허용했습니다. 지능적으로 작업을 위임하는 이 방법은 많은 문제를 해결해 주었습니다.\n\n\n<div class=\"content-ad\"></div>\n\n그리고 숫자들은 더욱 향상되었어요. 최고 모델은 이제 추가된 지시 따르기 능력으로 38%에서 41%로 상승했고, GPT-4 플러스 최적화를 통해 46%의 평가도달을 이루었어요.\n\n### 그들이 직면한 네 번째 얼음산 (도전):\n\n46.4%에서 99%로 나아가면, 텍스트-투-SQL의 목표는 SQL을 모르는 비즈니스 사용자를 위한 대화형 조종사를 구축하는 것이에요. 이것이 기회이며, 그들은 99%의 정확도가 필요해요.\n\n### 우리의 생각\n\n<div class=\"content-ad\"></div>\n\nWrenAI에서는 텍스트-SQL의 미래에 대해 낙관적입니다. 우리는 LLM 혁신이 숨막히게 빠르게 진행되고 있어 곧 LLM이 의미적 맥락을 통해 인간 수준의 이해력에 가깝게 발전하여 이상적인 데이터 민주화 세계를 이룰 것이라 믿습니다.\n\n# V4: 정확도 99%로 향상\n\n이제 스노우플레이크에서 진행 중입니다! 고객이 제공한 의미적 맥락을 활용하여 메트릭 및 조인 경로 등을 이해합니다.\n\n## 우리의 생각\n\n<div class=\"content-ad\"></div>\n\n눈송이에서 더 많은 정보 공유를 기대하고 있어요! 정말 흥미로워요!\n\n# 모든 수업의 최종 결론\n\n마지막으로, Vivek이 모든 도전을 경험하고 얻은 주요 교훈과 배운 점을 아래에 정리했어요.\n\n- 👏 제품은 전체 e2e 시스템입니다: LLM 모델링뿐만 아니라요\n- 👏 의미 체계 카탈로그 검색은 중요합니다: 실제 LLM 검색 엔진으로 강화하기\n- 👏 SQL 주석 품질이 중요합니다: 주석 처리 담당자는 전문가여야 해요\n- 👏 대화형 SQL은 LLM의 불사조 문제입니다: 복잡한 명령어 튜닝, 사고 과정 연결, 도구 사용\n- 👏 99%까지 가려면 돌파구가 필요합니다\n\n<div class=\"content-ad\"></div>\n\n그게 다에요! Vivek 씨와 Snowflake 팀이 이번 토롤에서 많은 귀중한 교훈을 나눠 주셨어요. 당신으로부터 많은 것을 배웠습니다!\n\n앞으로도 기대됩니다!\n\nWrenAI를 확인해보지 않았다면, 방문해보세요!\n\n👉 GitHub: https://github.com/Canner/WrenAI\n\n<div class=\"content-ad\"></div>\n\n👉 X: [https://twitter.com/getwrenai](https://twitter.com/getwrenai)\n\n👉 Medium: [https://blog.getwren.ai/](https://blog.getwren.ai/)\n\n이 글을 즐겨 보셨다면 깃헙에서 ⭐ WrenAI에 스타를 주시는 걸 잊지 마세요 ⭐ 언제나 독자 여러분 감사합니다.\n","ogImage":{"url":"/assets/img/2024-05-27-HowSnowflakebuildingthemostpowerfulSQLLLMintheworld_0.png"},"coverImage":"/assets/img/2024-05-27-HowSnowflakebuildingthemostpowerfulSQLLLMintheworld_0.png","tag":["Tech"],"readingTime":7},{"title":"SQL 트랜잭션 및 ACID 속성","description":"","date":"2024-05-27 13:02","slug":"2024-05-27-SQLTransactionsandACIDProperties","content":"\n![image](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_0.png)\n\n# SQL에서 거래 소개\n\nSQL을 데이터베이스로 사용하는 은행 시스템을 상상해봅시다. 사용자 A가 사용자 B의 계좌로 돈을 입금하려고 합니다. 돈을 송금하면 사용자 A의 계좌 잔액에서 해당 금액을 빼내고 이 돈을 사용자 B의 계좌에 입금하려는데, 갑자기 데이터베이스가 크래시될 경우 어떻게 될까요?\n\n![image](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_1.png)\n\n<div class=\"content-ad\"></div>\n\n사용자 A의 잔고에서 인출한 돈이 사라졌다는 것을 의미합니까? SQL 데이터베이스에서는 그렇지 않습니다. 왜냐하면 SQL 트랜잭션을 사용하기 때문입니다.\n\n# 트랜잭션과 ACID 속성\n\n트랜잭션은 단일 원자 단위로 수행되는 하나 이상의 SQL 작업 시퀀스입니다. 이는 데이터베이스에서 데이터 일관성을 보장하기 위한 목적으로 사용됩니다. 트랜잭션은 주로 ACID 약어로 불리는 다음 속성을 갖습니다:\n\n- 원자성: 전체 트랜잭션은 전체가 성공하거나 전체가 실패하는 단위로 처리됩니다.\n- 일관성: 트랜잭션은 데이터베이스를 하나의 유효한 상태에서 다른 유효한 상태로 변환시키며 데이터베이스 불변을 유지합니다.\n- 고립성: 동시에 실행되는 트랜잭션에 의해 수행된 수정 사항은 서로 분리되어 커밋될 때까지 격리됩니다.\n- 지속성: 트랜잭션이 커밋되면 시스템 장애가 발생하더라도 지속되어 유지됩니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-SQLTransactionsandACIDProperties_2.png\" />\n\n# SQL 트랜잭션에서의 중요 명령어\n\nSQL 트랜잭션의 시작을 BEGIN TRANSACTION 키워드로 표시합니다.\n\n<img src=\"/assets/img/2024-05-27-SQLTransactionsandACIDProperties_3.png\" />\n\n<div class=\"content-ad\"></div>\n\n모든 트랜잭션 중에 발생한 변경 사항을 저장하려면 변경 사항을 데이터베이스에 COMMIT합니다.\n\n![이미지](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_4.png)\n\n트랜잭션 중에 문제가 발생하면 ROLLBACK 명령을 사용하여 트랜잭션 중에 수행된 모든 변경 사항을 되돌릴 수 있으며 데이터베이스를 트랜잭션 시작 시점의 상태로 되돌릴 수 있습니다.\n\n![이미지](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_5.png)\n\n<div class=\"content-ad\"></div>\n\n## 예시\n\n우리 간단한 은행 애플리케이션 예제로 돌아가 봅시다. 여기서는 계좌 A에서 계좌 B로 $100을 이체해야 합니다. 이 과정은 두 단계로 이루어집니다:\n\n- 계좌 A에서 금액을 차감하기\n- 그 금액을 계좌 B에 추가하기\n\n이 두 가지 단계는 모두 성공적으로 완료되어야 합니다. SQL 트랜잭션으로 이를 어떻게 작성할 수 있는지 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n\nBEGIN TRANSACTION;\n\n- Account A 잔액에서 100을 차감합니다.\n  UPDATE Accounts\n  SET balance = balance - 100\n  WHERE account_id = 'A';\n  -- Account A에 충분한 잔액이 있는지 확인하고, 부족하다면 롤백합니다.\n  IF @@ROWCOUNT = 0\n  ROLLBACK;\n  -- Account B 잔액에 100을 추가합니다.\n  UPDATE Accounts\n  SET balance = balance + 100\n  WHERE account_id = 'B';\n  -- 모든 것이 잘 되었다면 트랜잭션을 커밋합니다.\n  COMMIT;\n\n\n이 트랜잭션은 다음을 수행합니다:\n\n- 트랜잭션을 시작하여 다음 작업이 단일 원자적 프로세스의 일부임을 보장합니다.\n- Account A에서 $100을 차감합니다: account_id와 balance 열이 있는 'accounts'라는 테이블이 있다고 가정합니다.\n- Account A에 충분한 자금이 있는지 확인합니다: Account A에 충분한 금액이 없으면, ROLLBACK TRANSACTION을 사용하여 모든 변경 사항이 취소되는 롤백이 수행됩니다.\n- Account B에 $100을 추가합니다: Account A에 충분한 금액이 있다면, Account B에 $100이 추가됩니다.\n- 트랜잭션을 커밋합니다: 두 개의 업데이트가 모두 성공적으로 수행되면, COMMIT TRANSACTION 명령이 실행되어이 트랜잭션 중에 수행된 변경 사항을 영구적으로 적용합니다.\n\n이것은 두 계정이 적절히 업데이트되거나 어느 시점에서 문제가 발생할 경우 변경 내용이 적용되지 않으므로 데이터의 무결성이 유지됩니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n# SQL 트랜잭션의 격리 수준\n\n데이터베이스 트랜잭션의 격리 수준은 트랜잭션 무결성이 유지되는 방식 및 각 트랜잭션이 다른 트랜잭션과 얼마나 격리되는지를 결정합니다.\n\nSQL 표준은 일관성과 성능 간의 균형을 맞추기 위해 네 가지 격리 수준을 정의합니다.\n\n## 1. Read Uncommitted\n\n<div class=\"content-ad\"></div>\n\n- 설명: 격리의 가장 낮은 수준입니다. 트랜잭션이 커밋되기 전에 다른 트랜잭션이 작업한 변경 사항을 볼 수 있습니다.\n- 예시에 미치는 영향: 돈을 송금하는 도중에 다른 트랜잭션이 계좌 A 또는 B의 잔액을 업데이트하고 있다면, 이 트랜잭션은 이러한 커밋되지 않은 값을 읽을 수 있습니다. 이는 실제로 존재하지 않는 잔액을 보는 등의 문제를 발생시킬 수 있습니다 (다른 트랜잭션이 실패하고 롤백될 경우).\n\n## 2. Read Committed\n\n- 설명: 트랜잭션이 커밋된 데이터만 읽을 수 있도록 보장합니다.\n- 예시에 미치는 영향: 이 수준은 'Read Uncommitted'의 문제를 피하기 위해 커밋된 계좌 A와 B의 잔액만 읽도록 합니다. 하지만 트랜잭션 내에서 잔액을 여러 번 읽는 경우, 다른 트랜잭션이 데이터를 수정하는 경우 다른 값들을 보게 될 수 있습니다 (반복할 수 없는 읽기).\n\n## 3. Repeatable Read\n\n<div class=\"content-ad\"></div>\n\n- 설명: 거래가 데이터를 두 번째로 읽을 때 동일한 데이터 값을 찾을 수 있도록 보장합니다(반복되지 않는 읽기를 피함).\n- 예시에 미치는 영향: 이 수준은 거래 내에서 동일한 데이터의 여러 번의 읽기 사이에 다른 사람에 의해 생긴 변경 사항을 볼 수 없도록 합니다. 이는 잔액 확인 및 업데이트 작업 중 일관된 읽기 결과를 유지하는 데 도움이 됩니다. 그러나 다른 거래에 의해 추가된 새로운 행이 발생하는 유령 읽기를 막을 수는 없을 수도 있습니다.\n\n## 4. Serializable\n\n- 설명: 최고 수준의 격리. 거래가 직렬로 실행된 것처럼 완전히 격리됩니다.\n- 예시에 미치는 영향: 이는 완전한 격리를 보장합니다. 다른 거래가 이체 프로세스에 간섭할 수 없습니다. 모든 동시성 문제(더티 리드, 반복되지 않는 읽기 및 유령 읽기)를 방지하지만 동시성이 감소하고 잠금으로 인한 잠재적 성능 문제가 발생할 수 있습니다.\n\n다양한 격리 수준에서 여러 현상이 발생할 수 있으며, 더티 리드, 반복되지 않는 읽기 또는 유령 읽기와 같은 현상이 발생할 수 있습니다. 아래에서 이 용어가 의미하는 바를 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n# Dirty Reads\n\n더티 리드는 트랜잭션이 동시에 커밋되지 않은 다른 트랜잭션에 의해 작성된 데이터를 읽을 때 발생합니다. 결과적으로 다른 트랜잭션이 롤백하면 처음 트랜잭션이 데이터를 읽게 됩니다. 하지만 해당 데이터는 데이터베이스에 공식적으로 기록된 적이 없습니다.\n\n예시:\n\n- 트랜잭션 1이 시작되고 계좌 A에서 계좌 B로 $100을 이체합니다.\n- 트랜잭션 1이 커밋되기 전에 트랜잭션 2가 시작되고 계좌 A의 잔액을 읽습니다.\n- 트랜잭션 1이 실패하고 롤백되면, 트랜잭션 2는 공식적으로 커밋되지 않은 잔액을 읽게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n\n![image](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_6.png)\n\n# Non-Repeatable Reads (Read Uncommitted)\n\n이것은 트랜잭션 진행 중 같은 행이 두 번 조회되고, 두 번 조회 사이에 행 내의 값이 다른 경우 발생합니다. 본질적으로 다른 트랜잭션이 두 번의 조회 사이에 행을 수정한 경우입니다.\n\n예시:\n\n\n\n<div class=\"content-ad\"></div>\n\n- 거래 1이 시작되고 계정 A의 잔액을 읽습니다.\n- 거래 2가 계정 A에서 계정 B로 $100을 이체하고 커밋합니다.\n- 거래 1이 다시 계정 A의 잔액을 읽으면 이전과 다른 잔액을 볼 수 있습니다.\n\n![이미지](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_7.png)\n\n# 환상 읽기\n\n환상 읽기란 다른 트랜잭션에서 읽고 있는 레코드에 새로운 행이 추가되거나 (또는 기존 행이 삭제되는 경우) 트랜잭션 중에 발생합니다. 이는 동일한 트랜잭션에서의 후속 읽기가 원래 읽기의 일부가 아니었던 새로 추가된 행을 포함하거나 삭제되지 않은 행을 제외한 행 집합을 반환할 수 있음을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n예시:\n\n- Transaction 1은 계정 A의 거래 수를 세는 쿼리를 시작합니다.\n- Transaction 2는 계정 A에 새로운 거래 기록을 삽입하고 커밋합니다.\n- Transaction 1은 다시 계정 A의 거래 수를 세어보고 이전보다 더 많은 거래를 발견합니다.\n\n![그림](/assets/img/2024-05-27-SQLTransactionsandACIDProperties_8.png)\n\n높은 격리 수준은 발생할 수 있는 현상의 종류를 줄이지만 더 낮은 동시성과 잠재적인 성능 영향을 감수해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n만약 SQL 데이터베이스의 확장 메커니즘에 대해 더 알고 싶다면, 데이터베이스 샤드, 복제 등을 다루는 깊이 있는 Database Essentials 비디오를 확인해보세요:\n\n여기 처음 오신 분들을 위해, 저는 Hayk입니다. 저는 웹 개발자 분들이 첫 번째 기술 직을 확보하거나 웹 개발 마스터리 커뮤니티에서 시니어 역할로 진출하는 데 도와드리고 있어요.\n\n웹 개발에 대한 주간 통찰력을 놓치고 싶지 않다면, 내 뉴스레터를 구독해주세요.\n\n","ogImage":{"url":"/assets/img/2024-05-27-SQLTransactionsandACIDProperties_0.png"},"coverImage":"/assets/img/2024-05-27-SQLTransactionsandACIDProperties_0.png","tag":["Tech"],"readingTime":6},{"title":"90의 작업을 처리할 수 있는 10가지 SQL 문장","description":"","date":"2024-05-27 13:00","slug":"2024-05-27-10SQLStatementsThatCanHandle90ofTasks","content":"\n구조화된 쿼리 언어(SQL)는 관계형 데이터베이스를 관리하고 질의하는 강력한 도구입니다. 초보자든 경험 많은 데이터 전문가든 상관없이, 여러분은 자주 사용하게 될 특정 SQL 문을 발견할 것입니다. 본 문서에서는 데이터베이스 작업의 90%를 처리할 수 있는 10가지 필수 SQL 문을 다룹니다. 코드 예시를 함께 제공할 것입니다.\n\n![이미지](/assets/img/2024-05-27-10SQLStatementsThatCanHandle90ofTasks_0.png)\n\n# 1. 소개\n\n# SQL의 중요성\n\n<div class=\"content-ad\"></div>\n\nSQL은 관계형 데이터베이스와 상호 작용하기 위한 표준 언어입니다. 데이터를 검색하거나 데이터베이스 구조를 수정하는 등 다양한 작업을 수행할 수 있습니다. SQL을 이해하는 것은 데이터 작업을 하는 사람에게 필수적이며, 데이터 분석, 보고 및 애플리케이션 개발을 위한 기초를 제공합니다.\n\n# 2. SELECT 문\n\n# 데이터 검색\n\nSELECT 문은 하나 이상의 테이블에서 데이터를 검색하는 데 사용됩니다. 검색할 열을 지정하고 결과를 필터링할 조건을 추가할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 테이블에서 모든 열을 검색합니다\nSELECT * FROM employees;\n\n-- 특정 열을 검색합니다\nSELECT first_name, last_name FROM employees;\n\n-- 결과를 필터링하기 위해 조건을 추가합니다\nSELECT product_name, price FROM products WHERE price > 50;\n```\n\n# 3. INSERT INTO 문\n\n# 새 데이터 추가\n\nINSERT INTO 문을 사용하면 테이블에 새로운 데이터 행을 추가할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 단일 행 삽입\nINSERT INTO customers (first_name, last_name, email) VALUES ('John', 'Doe', 'john@example.com');\n\n-- 여러 행 삽입\nINSERT INTO orders (order_date, total_amount) VALUES\n    ('2023-01-15', 150.00),\n    ('2023-01-16', 220.50),\n    ('2023-01-17', 75.25);\n```\n\n# 4. UPDATE 문\n\n# 기존 데이터 수정하기\n\nUPDATE 문은 테이블의 기존 데이터를 수정하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 하나의 행 업데이트하기\nUPDATE products SET price = 25.99 WHERE product_id = 101;\n\n-- 여러 행 업데이트하기\nUPDATE employees SET manager_id = 105 WHERE department = 'Sales';\n```\n\n# 5. DELETE 문\n\n# 데이터 삭제\n\nDELETE 문은 테이블에서 행을 제거하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 한 행 삭제\nDELETE FROM customers WHERE customer_id = 201;\n\n-- 조건을 충족하는 모든 행 삭제\nDELETE FROM orders WHERE order_date < '2023-01-15';\n```\n\n## 6. CREATE TABLE 문\n\n### 새 테이블 생성\n\nCREATE TABLE 문은 지정된 열과 데이터 유형을 가진 새 테이블을 생성하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n\nCREATE TABLE products (\n    product_id INT PRIMARY KEY,\n    product_name VARCHAR(255),\n    price DECIMAL(10, 2)\n);\n\n\n## 7. ALTER TABLE 문\n\n## 테이블 수정\n\nALTER TABLE 문을 사용하면 기존 테이블을 추가, 수정 또는 삭제하여 테이블을 수정할 수 있습니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 새 열 추가\nALTER TABLE employees ADD COLUMN hire_date DATE;\n\n-- 열 데이터 유형 수정\nALTER TABLE customers ALTER COLUMN phone_number VARCHAR(15);\n```\n\n# 8. DROP TABLE 문\n\n# 테이블 삭제하기\n\nDROP TABLE 문은 기존 테이블과 해당 데이터를 모두 삭제하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 테이블 삭제\nDROP TABLE products;\n```\n\n## 9. WHERE 절\n\n## 데이터 필터링\n\nWHERE 절은 지정된 조건에 따라 행을 필터링하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n-- 가격이 50보다 큰 제품 조회\nSELECT product_name, price FROM products WHERE price > 50;\n\n-- 영업 부서의 직원 조회\nSELECT first_name, last_name FROM employees WHERE department = 'Sales';\n```\n\n# 10. JOIN 절\n\n# 여러 테이블에서 데이터 결합\n\nJOIN 절을 사용하여 서로 관련된 열을 기반으로 두 개 이상의 테이블에서 행을 결합합니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 고객 이름과 주문 조회하기\nSELECT c.first_name, c.last_name, o.order_date\nFROM customers AS c\nJOIN orders AS o ON c.customer_id = o.customer_id;\n```\n\n# 11. GROUP BY 절\n\n# 데이터 집계\n\nGROUP BY 절은 특정 열의 값을 가진 행을 그룹화하는 데 사용되며, 종종 SUM 및 COUNT와 같은 집계 함수와 함께 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 각 제품별 총 매출 계산\nSELECT product_id, SUM(quantity * price) AS total_sales\nFROM order_details\nGROUP BY product_id;\n```\n\n# 12. 결론\n\n# 기초 마스터\n\n이 10가지 SQL 문은 관계형 데이터베이스 작업 시 대부분의 작업을 다룹니다. 이 문장들을 이해하고 숙달함으로써 데이터베이스 관리 및 데이터 조작에 대한 견고한 기초를 갖게 될 것입니다. SQL은 다양한 기능을 제공하는 언어이며, 데이터 작업에 더욱 강력한 방법을 발견하면서 더욱 쉽게 작업할 수 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# SQL 기초 지식\n\n시간을 내어 주셔서 감사합니다! 🚀\nSQL 기초 지식에서 더 많은 콘텐츠를 찾아보실 수 있어요! 💫\n","ogImage":{"url":"/assets/img/2024-05-27-10SQLStatementsThatCanHandle90ofTasks_0.png"},"coverImage":"/assets/img/2024-05-27-10SQLStatementsThatCanHandle90ofTasks_0.png","tag":["Tech"],"readingTime":4},{"title":"사용 기반 API 요금 청구 및 미터링을 위한 실시간 분석 솔루션","description":"","date":"2024-05-27 12:58","slug":"2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering","content":"\n\n![Real-Time Analytics Solution](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_0.png)\n\nDisclaimer: The author of this article is a Developer Advocate at Redpanda, which is a critical component of the solution discussed. The author also brings prior expertise in API Management and Apache Pinot to the table. Hence, the proposed solution is a combination of these technologies aimed at solving a prevalent problem.\n\nAn API business refers to a company that packages its services or functionalities as a set of API (Application Programming Interface) products. These APIs can be sold to new and existing customers, who can then integrate these functionalities into their own applications. The company can generate revenue by charging these customers based on their usage of the APIs.\n\nA company operating an API business needs a data infrastructure component to track API call volume and bill consumers accordingly.\n\n\n<div class=\"content-ad\"></div>\n\n이 게시물에서는 Apache APISIX, Redpanda 및 Apache Pinot를 사용하여 실시간 API 사용 추적 솔루션을 구축하기 위한 참조 아키텍처를 제시합니다. 이 게시물은 \"어떻게\"보다는 \"왜\"에 중점을 두었습니다. 이를 솔루션 설계 연습으로 간주하고 심층 튜토리얼이 아니라는 것을 고려해 주세요. 저는 솔루션 패턴을 청사진으로 추출하여 향후 프로젝트에서 재사용할 수 있도록 돕고자 합니다.\n\n다른 말 없이 시작해 봅시다.\n\n# APIs 및 API 관리\n\nAPI 및 API 관리 개념에 대해 처음이라면, 먼저 간단히 소개해 드리겠습니다.\n\n<div class=\"content-ad\"></div>\n\n디지털 비즈니스에서 API는 비즈니스 작업에 프로그래밍 방식으로 액세스할 수 있도록 해줘서 인간을 제외할 수 있어요. 이러한 비즈니스 작업에는 주문 생성, 자금 이체, CRM에서 고객 주소 업데이트 등이 포함될 수 있어요.\n\n비즈니스에서 전형적인 API 환경에는 세 가지 당사자가 관련돼요:\n\n- 백엔드 시스템 — 비즈니스 작업을 실행하는 시스템이에요.\n- 소비자 — 비즈니스 작업을 사용하려는 1차 및 3차 애플리케이션이에요.\n- API 프록시 — 프록시로서 작용하며 중간자 역할을 하는 요소에요.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_1.png)\n\n<div class=\"content-ad\"></div>\n\nAPI의 역할은 내부 비즈니스 시스템을 소비자로부터 분리하여, 소비자가 백엔드 시스템의 복잡성을 처리할 필요 없이 이를 제공하는 것입니다. 이러한 방식으로, API는 추상화 계층 역할을 합니다. API는 HTTP를 포함한 다양한 통신 프로토콜을 통해 작동하며, RESTful 및 GraphQL API 스타일을 볼 수 있습니다.\n\n운영 중에는 조직이 API 수명주기의 다양한 단계를 관리하기 위해 전체 수명주기 API 관리 플랫폼을 사용합니다. API 프록시 디자인, 배포, 런타임 정책 참여 및 모니터링과 같은 API 수명주기의 각 단계에 대한 전용 구성 요소를 번들로 제공하는 API 관리 플랫폼이 있습니다. 이 문서의 문맥에서 Apache APISIX를 사용할 것이며, 이는 Apache 라이선스 하에 배포되는 오픈 소스 API 관리 플랫폼입니다.\n\n그렇다고 해서 여기서 구축하는 솔루션이 APISIX와 무조건적으로 결합된 것은 아닙니다. 시장에서 구할 수 있는 대부분의 전체 수명주기 API 관리 제품과 통합할 수 있습니다. 단, 적합한 통합 인터페이스를 제공해야 합니다.\n\n![Real-Time Analytics Solution for Usage-Based API Billing and Metering](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_2.png)\n\n번역 시 일부 용어는 컨텍스트에 맞게 번역되었습니다.\n\n<div class=\"content-ad\"></div>\n\n# API를 활용하여 수익을 창출하는 방법\n\n이제, API를 활용하여 수익을 창출하는 방법에 대해 살펴봅시다. 즉, 사용량에 따라 소비자에게 요금을 부과하는 수익 모델을 찾아보는 것입니다.\n\n더 나은 설명을 위해 현실적인 예시를 들어보겠습니다.\n\n부동산 감정 회사를 고려해보세요. 이 회사는 주택 구매자와 판매자에게 즉각적인 부동산 평가를 제공합니다. 이 평가는 우편번호, 부동산 유형, 지역과 같은 간단한 요소를 기반으로 합니다. 현재, 이 회사는 웹 기반 사용자 인터페이스만 제공하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_3.png)\n\n비지니스 운영을 확장하고 더 많은 고객을 유치하며 새로운 시장 세그먼트에 진입하기 위해 이 회사는 API 비지니스로 진출하기로 결정했습니다. 이 말은 평가 엔진을 API 제품 세트로 패키징하여 새로운 및 기존 소비자에게 판매하고 그들의 API 호출 사용량에 따라 청구할 것을 의미합니다.\n\n이를 위해 먼저 평가 엔진을 분리하고 API 관리 플랫폼 뒤에 배치하여 달성합니다. 이를 통해 소비자들이 일련의 API를 통해 기능에 액세스할 수 있게 됩니다.\n\n![image](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_4.png)\n\n\n<div class=\"content-ad\"></div>\n\n평가 API는 부동산 회사, 은행, 보험사, 정부 등 다양한 분야의 잠재 고객들을 유치할 것입니다:\n\n- 부동산 회사 — 주택 구매자를 위한 정확한 평가값 제공.\n- 은행 — 모기지 승인 전 주택 평가.\n- 보험 제공업자 — 주택 및 내용 보험에 대한 더 정확한 견적 제공.\n- 정부 — 부동산 세금 쉽게 계산.\n\n## API 수익화 모델\n\n이 회사는 어떻게 수익을 창출할까요? 평가 API를 API 제품 세트로 포장하여 구독 계층과 함께 판매하세요!\n\n<div class=\"content-ad\"></div>\n\n가입 등급은 소비자가 매달 고정된 API 호출 횟수를 사용할 수 있는 할당량입니다. 그 할당량을 초과하면 사용자는 제한을 받거나 초과 사용량에 대해 요금을 지불해야 합니다.\n\n예를 들어, 가치평가 API는 다음과 같이 세 가지 가입 등급으로 제공될 수 있습니다.\n\n- 브론즈: 매달 10,000건의 요청\n- 골드: 매달 100,000건의 요청\n- 플래티넘: 매달 무제한의 요청\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_5.png)\n\n<div class=\"content-ad\"></div>\n\n고객은 예상 사용량에 따라 다양한 티어 중에서 선택하여 API에 가입할 수 있습니다. 한 달의 끝에 회사는 실제 사용량을 기반으로 고객에게 청구할 것입니다.\n\n우리의 목표는 각 고객의 API 사용량을 효율적이고 신뢰할 수 있는 방법으로 측정하는 것입니다.\n\n# 솔루션 계획\n\n이제 우리가 해결하려는 문제를 이해했으니, 구현에 들어가기 전에 몇 가지 설계 결정사항을 설명해 드리겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## KPI 지표\n\n첫 번째 단계는 솔루션으로부터 기대하는 KPI 또는 지표를 식별하는 것입니다. 특히 다음 다섯 가지에 관심이 있습니다.\n\n- API 사용량 — 시간당 소비자 당 API 호출 횟수\n- API 지연 — 느린, 느린 API를 식별하기 위한 종단 간 지연 시간\n- 고유 사용자 — API 당 고유 호출 수는?\n- 지리적 사용량 분포 — 주로 API 사용자가 어디에서 왔는가?\n- 오류 수 — 호출 오류가 많으면 백엔드에 문제가 있다는 것을 의미합니다.\n\n이상적으로 이런 것들이 모두 이렇게 시각화된 대시보드에서 보고 싶습니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 마크다운 형식으로 표시 변환한 코드입니다.\n\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_6.png)\n\n## 이해 관계자\n\n두 번째 디자인 결정은 솔루션 이해 관계자 - 이러한 지표를 전달해야 하는 대상. 주로 세 가지 당사자가 있습니다.\n\n고객 및 협력사 - 소비자는 실시간 대시 보드에서 할당량 사용량과 청구 추정을 확인하는 것을 좋아합니다.\n\n\n<div class=\"content-ad\"></div>\n\nAPI 운영 팀 - 이 팀은 API 관리 인프라를 관리합니다. API의 건강 정보에 특히 관심이 있으며, 지연시간, 처리량, 오류 등을 주로 다룹니다.\n\nAPI 제품 팀 - 이 팀은 API 제품을 소유하고 있습니다. 그들은 소비자의 인구 통계, API 중에서 더 인기 있는 것이 무엇인지 등을 확인하기 위해 즉시 실험을 실행하고 싶어 합니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_7.png)\n\n## 일괄 처리 또는 실시간 처리?\n\n<div class=\"content-ad\"></div>\n\n최종 디자인 결정으로, 실시간 및 일괄 메트릭 사이에 80:20의 분할을 설정하겠습니다.\n\n데이터는 시간이 경과함에 따라 가치를 잃습니다. 데이터를 빨리 처리할수록 적절한 조치를 취할 수 있습니다. 그래서 우리는 API 트래픽 및 헬스 메트릭을 실시간으로 처리하겠습니다.\n\n소비자 API 키가 유출된 상황을 생각해보세요. 악의적인 API 클라이언트가 훔친 키나 인증 토큰을 사용하여 소비자를 대신해 API를 호출할 수 있습니다. 시스템은 이 API 키에서의 급격한 트래픽 증가를 감지하여 비정상으로 식별하고 키를 차단하면서 소비자에게 경보를 보낼 수 있습니다. 경보를 받은 소비자는 즉시 API 키를 재발급하여 비용을 최소화할 수 있습니다.\n\n그러나 모든 사용 사례가 실시간 처리를 필요로 하는 것은 아닙니다. 어떤 사용 사례는 자연스럽게 일괄 처리에 적합합니다.\n\n<div class=\"content-ad\"></div>\n\n- 고객을 위한 월별 사용량에 기반한 청구 보고서.\n- 업무팀을 위한 주간 API 건강 보고서.\n- 제품팀을 위한 매일 API 트래픽 보고서.\n\n# 구현\n\n지금은 이 기사의 중간 지점에 도달했으며 지금까지의 토론을 바탕으로 다음 솔루션 아키텍처를 제시합니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_8.png)\n\n<div class=\"content-ad\"></div>\n\n다이어그램이 복잡하고 많은 알 수없는 기술이 있어서 알겠습니다. 그래서, 세 개의 레이어로 나누어서 각각에 대해 자세히 설명하겠습니다.\n\n## 데이터 수집\n\n이전에 언급했듯이, API 관리 시스템은 디자인 및 런타임 측면에서 트래픽 모양 만들기, 인증 및 구독 관리와 같은 API 라이프사이클 관리 작업을 수행하는 여러 이동 부품을 가지고 있습니다. 이에 대한 추가 측면도 있습니다.\n\n![image](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_9.png)\n\n<div class=\"content-ad\"></div>\n\n그러나, 우리가 가장 관심 있는 구성 요소는 API 게이트웨이입니다. 이 곳은 모든 API 트래픽이 백엔드로 흐르는 곳입니다.\n\n우리의 첫 번째 작업은 API 게이트웨이에서 접촉점을 확인하는 것입니다. 이를 통해 왕복하는 API 요청과 응답을 수집할 수 있습니다. 그런 다음 이 정보를 실시간으로 분석용 데이터 저장소로 이동시키는 데이터 파이프라인을 구축할 것입니다. 이를 통해 향후 질의를 용이하게 할 것입니다.\n\n그러나, 이 쓰기 경로를 구현할 때 직접 데이터를 기본 데이터 저장소에 쓰는 것은 여러 문제점을 야기할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nAPIM 시스템과 분석 인프라 사이의 강한 결합 - 나중에 새 APIM 공급업체로 전환할 때 분석 인프라의 상당 부분을 다시 작성해야 할 수도 있습니다.\n\n동기 쓰기 - 운영 중에 두 시스템 모두 사용 가능해야 하므로 분석 시스템을 유지보수 목적으로 중지하기 어려울 수 있습니다.\n\n확장 가능한 데이터 수집 - API 게이트웨이의 돌발적인 트래픽 급증으로 인해 분석 시스템이 과부하되어 두 시스템 모두 협조하여 확장해야 할 수 있습니다.\n\n이로 인해 APIM과 분석 인프라를 분리하는 방법을 모색할 필요가 있습니다. Apache Kafka와 같은 스트리밍 데이터 플랫폼은 API 게이트웨이에서 높은 처리량 데이터 스트림을 낮은 지연 시간으로 수신할 수 있으므로 여기에 적합할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n해당 솔루션에서는 성능과 간편함 측면에서 Kafka보다 우월한 Redpanda, Kafka API 호환 스트리밍 데이터 플랫폼을 사용할 것입니다. 하지만 만약 Kafka만 사용하길 원한다면 괜찮습니다. 해당 솔루션은 두 기술에 모두 매끄럽게 작동합니다.\n\nRedpanda를 중심으로 한 데이터 파이프라인은 다음과 같이 구성됩니다:\n\n![Redpanda Data Pipeline](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_11.png)\n\nRedpanda의 추가로 두 시스템이 분리되었고 쓰기 경로가 비동기로 동작합니다. 이는 분석 시스템이 유지 보수를 위해 오프라인 상태로 들어갈 수 있고, 중단된 지점부터 다시 재개할 수 있도록 합니다. 게다가 Redpanda는 갑작스러운 트래픽 급증을 흡수하여 분석 시스템이 과부하를 받거나 API 게이트웨이에 맞춰 스케일링할 필요가 없도록 해줍니다.\n\n<div class=\"content-ad\"></div>\n\n이제 APISIX와 Redpanda 사이의 연결을 어떻게 만들어야 할지 궁금할 것입니다. 다행히도, APISIX는 Kafka를 위한 내장 데이터 싱크를 제공합니다. 게이트웨이로 API 요청이 발생하고 응답이 반환될 때, 이 싱크는 실시간으로 Kafka 토픽에 레코드를 발행합니다. 우리는 Redpanda와 함께 이 싱크를 사용할 수 있습니다. 왜냐하면 Redpanda가 Kafka API와 호환되기 때문입니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_12.png)\n\nAPISIX는 개별 API 호출을 JSON 이벤트로 형식화하고 지연 시간, HTTP 상태 및 타임스탬프와 같은 중요한 메트릭을 포함시킵니다. 이러한 정보들은 분석 데이터 저장소에서 관련 있는 차원으로 매핑될 것입니다.\n\n![이미지](/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_13.png)\n\n<div class=\"content-ad\"></div>\n\nAPI 관리 플랫폼에 Kafka 싱크가 없는 경우 어떻게 할까요? 그럼 대안으로 API 게이트웨이의 HTTP 액세스 로그를 Kafka로 스트림 처리할 수도 있습니다. 이를 위해 Filebeat나 유사한 도구를 사용할 수 있습니다.\n\n## 분석 데이터베이스\n\n이제 Redpanda에 API 호출 이벤트가 랜딩되고 있으니, 다음 단계는 적합한 분석 데이터베이스 기술을 식별하는 것입니다.\n\nOLTP 데이터베이스, 키-값 저장소 또는 데이터 웨어하우스가 될 수 있을까요? 다음 기대 기준에 따라 각각을 평가해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n- 스트리밍 데이터 수집 - 실시간 데이터 원본인 Kafka에서 가져와야 합니다. 여기서는 배치 데이터 로딩이 없어야 합니다. 스트리밍 수집은 더 높은 데이터 신선도를 보장합니다.\n- 낮은 지연 쿼리 - 쿼리 지연 시간은 하위 초 범위 내여야 하며 사용자를 위한 분석 대시보드를 만족시켜야 합니다.\n- 높은 쿼리 처리량 - 사용자를 대상으로 하는 분석 대시보드에서 동시에 발생하는 쿼리를 처리할 수 있어야 하며 지연 시간을 훼손하지 않아야 합니다.\n\n위의 모든 조건을 충족하는 분석 데이터베이스로 Apache Pinot을 선택했습니다.\n\nApache Pinot은 실시간 분산 OLAP 데이터베이스로, 스트리밍 데이터에서 OLAP 워크로드를 처리하도록 설계되어 극히 낮은 지연 시간과 높은 동시성을 제공합니다. Pinot은 Kafka와 원활하게 통합되어 Kafka 주제에서 데이터가 생성될 때마다 실시간 수집을 허용합니다. 수집된 데이터는 색인이 작성되고 열 형식으로 저장되어 효율적인 쿼리 실행을 가능케 합니다.\n\n아키텍처에서 Pinot을 사용하면 엔드 투 엔드 데이터 파이프라인은 다음과 같이 보입니다. Pinot은 Redpanda와 API 호환성으로 원활하게 통합됩니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_14.png\" />\n\n파이프라인에 스트림 프로세서가 필요한가요? 정말 필요한지는 사용 사례에 따라 다릅니다.\n\n스트림 프로세서 대신 Redpanda의 Wasm 기반 인브로커 데이터 변환을 사용하여 API 이벤트 페이로드에서 민감한 필드를 제거할 수 있습니다. 그러나 아파치 Flink와 같은 상태 보유형 스트림 프로세서는 다음과 같을 때 파이프라인에 더 많은 가치를 더할 수 있습니다:\n\n- 실시간 결합 및 보강이 필요할 때 — 핀오토로 전달할 추가 차원이 필요하며, 이는 여러 스트림을 결합하여 파생할 수 있습니다. 예: IP 지오코딩.\n- 알림 — 사용량의 이상 현상을 기반으로 알림을 트리거하고 하류 이벤트 주도형 워크플로를 실행합니다.\n\n<div class=\"content-ad\"></div>\n\n## Serving layer\n\n우리의 분석 데이터 파이프라인이 이제 완성되었습니다. 모든 파이프라인 구성 요소는 데이터 인프라 구조층에 있습니다. 필요하다면 Pinot 쿼리 콘솔에 액호크 SQL 쿼리를 실행하여 메트릭을 생성할 수 있습니다.\n\n그러나 솔루션의 모든 이해 관계자/사용자가 그렇게 하길 원하는 것은 아닙니다. 우리는 각 사용자 그룹에게 메트릭을 직관적이고 편안하게 찾을 수 있는 방식으로 제시해야 합니다. 이것이 우리가 분석의 마지막 단계인 서빙 레이어를 구현하는 곳입니다.\n\n<img src=\"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_15.png\" />\n\n<div class=\"content-ad\"></div>\n\n우리의 우선순위는 소비자들입니다. 그들은 사용량과 청구 예상을 시각화하는 실시간 대시보드가 필요합니다. 이를 위해 Streamlit과 같은 프레임워크를 활용하여 Python 기반 데이터 어플리케이션을 개발할 수 있습니다. Pinot Python 드라이버 pinotdb를 사용하면 애플리케이션과 Pinot 쿼리 환경을 연결할 수 있습니다.\n\nBI 및 즉석 탐색이 필요한 사용자 그룹, 특히 API 제품 소유자는 Tableau와 Apache Superset과 같은 선호하는 BI 도구를 연동할 수 있는 Pinot의 ODBC 인터페이스를 사용할 수 있습니다.\n\n일괄 작업을 위해 Pinot는 Presto나 Trino와 같은 쿼리 연합 엔진에 Pinot 커넥터를 통해 연동할 수 있습니다.\n\n# 요약\n\n<div class=\"content-ad\"></div>\n\n다음은 파이프라인 구현 단계 순서를 나열하여 글을 마무리해 봅시다.\n\n1. Redpanda 클러스터를 프로비저닝하고 토픽을 생성하고 ACL을 설정합니다.\n2. APISIX에서 Kafka 싱크를 구성합니다.\n3. Pinot 스키마와 테이블을 생성합니다.\n4. 필요에 따라 데이터를 가공합니다.\n5. 대시보드를 생성하거나 연결합니다.\n\n이 솔루션은 비즈니스 자체에서 호스팅하고 관리하는 자체 호스팅 배포 모델을 전제로 합니다. 그러나 동일한 설계 원칙이 이 도구들의 호스팅 버전을 선택해도 적용될 수 있다는 점을 알아두는 것이 중요합니다. 아키텍처의 각 구성 요소는 호스팅 서비스로 대체될 수 있으며, 이를 통해 다양한 배포 전략에 대응할 수 있는 유연한 해결책이 될 수 있습니다.\n\n이전에 언급했듯이, 이 글은 \"어떻게\"보다는 \"왜\"에 대해 주로 다루고 있습니다. 목표는 정확한 실행 방법보다는 근본적인 해결책 패턴을 이해하는 것입니다. 이 글을 다음 실시간 분석 프로젝트의 청사진으로 삼아 보세요. 필요에 따라 다른 기술을 통합하여 조정할 수 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_0.png"},"coverImage":"/assets/img/2024-05-27-Real-TimeAnalyticsSolutionforUsage-BasedAPIBillingandMetering_0.png","tag":["Tech"],"readingTime":11},{"title":"데이터 폭풍 속을 해마하는 여정 정교한 레이크하우스 플랫폼 구축하기","description":"","date":"2024-05-27 12:55","slug":"2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform","content":"\n![이미지](/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_0.png)\n\n목요일 오후 늦은 시간, 우리는 깨달음을 얻었습니다. 어두운 사무실에서 깜박이는 화면들을 둘러싸고, 두 명의 예리한 데이터 분석가 동료와 데이터 과학자로 구성된 헌신적인 팀, 그리고 나, 데이터 엔지니어는 PostgreSQL과 MySQL 데이터베이스에서 데이터를 조인하려고 깊이 파고들고 있었습니다. 이 작업은 간단할텐데 어쩌면 서로 다른 부서에서 오는 불일치된 데이터 구조와 충돌하는 스키마로 애를 쓰고 있었습니다. 이러한 이질적인 데이터 세트를 수동으로 맞추려고 할수록 복잡성이 압도되는 느낌이었습니다. 혼돈스러운 정보 동기화 시도마다 실패할 때마다 공기는 분노로 더 두꺼워졌습니다. \"이렇게 일할 수는 없어,\" 살짝 중얼거렸던 저의 목소리엔 스트레스가 느껴졌습니다. 우리의 현재 시스템이 현실에 부합하지 않다는 것은 명백했습니다—우리는 흩어진 데이터 정복 뿐만 아니라 이 넓은 데이터 정글을 이해할 수 있는 통합 플랫폼이 필요했던 것입니다. 이 순간이 우리에게 중대한 변화가 필수적이라는 것을 알게 된 시점이었고, 큰 변화가 곧 찾아올 것임을 알게 된 시점이었습니다.\n\n이 여정을 시작하면서, 저희는 우리 회사의 의사 결정 프로세스의 기반이 되는 견고한 데이터 플랫폼 아키텍처를 만들었습니다. 이 블로그 글에서는 다양한 데이터를 단일하고 강력한 분석 엔진으로 통합하는 것뿐만 아니라 지속적으로 발전하고 시간이 지남에 따라 더 많은 데이터 소스를 통합하는 유연하고 확장 가능한 데이터 인프라를 구축하는 과정에서 우리가 직면한 인사이트와 도전에 대해 탐구할 것입니다.\n\n대용량의 원시 데이터를 원래 형식 그대로 저장할 수 있는 유연성으로 기업들에게 빠르게 적응하고 효율적으로 확장할 수 있는 도구로써 데이터 레이크가 부각되었습니다. 그러나 계속 진행함에 따라 데이터 관리에 더 구조적인 접근이 필요하다는 것을 깨달았고, 그로 인해 레이크하우스 구조를 채택하게 되었습니다. 이 하이브리드 모델은 데이터 레이크의 확장성과 유연성을 데이터 웨어하우스의 관리 기능과 결합하여 데이터 전략을 향상시킵니다. 이 이야기는 이러한 기술을 활용하기 위해 우리가 취한 실질적인 단계를 살펴보며, 데이터 레이크를 레이크하우스 프레임워크로 통합함으로써 데이터 주도 기업에게 혁신적인 자산이 될 수 있는 방법을 밝힐 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 레이크하우스 이전의 데이터 전략\n\n레이크하우스를 개발하기 전에, 데이터 관리는 간단했지만 우리의 요구사항이 증가함에 따라 비효율적으로 변했습니다. 처음에는 BI용 Apache Superset을 사용하여 3개의 주요 데이터 소스를 관리했는데, 처음에는 최소한의 복잡성으로 우리의 요구사항을 충족시켰습니다.\n\n그러나 우리의 데이터 요구사항이 증가함에 따라 시스템의 한계가 나타나기 시작했습니다. 두 가지 다른 소스에서 데이터를 조인해야 할 필요가 발생했을 때 중대한 도전이 발생했습니다. 당시 우리의 솔루션은 매우 효율적이지 못했습니다: 필요한 데이터를 한 소스에서 다른 소스로 수동으로 복제했습니다. 이 프로세스는 시간이 많이 걸릴 뿐만 아니라 데이터를 동기화하기 위해 빈번한 업데이트가 필요했기 때문에 오류를 발생시킬 가능성도 있었습니다.\n\n또한 다양한 팀과 프로젝트가 발전함에 따라 Superset 내에서 여러 데이터셋이 생성되었는데, 각각이 특정한 분석 요구에 맞게 조정되었습니다. 불행히도, 이로 인해 여러 데이터셋에 중복 변환 요소가 코딩되어 복잡성이 증가했을 뿐만 아니라 이러한 변환을 유지하고 업데이트하는 것이 점점 더 부담스러워졌습니다.\n\n<div class=\"content-ad\"></div>\n\n## 데이터 아키텍처 결정: Lake, Warehouse 또는 Lakehouse?\n\n저희 데이터 인프라에 적합한 아키텍처를 선택하는 것은 중요한 결정이었습니다. 세 가지 주요 옵션 중에서 선택을 고민했습니다: 데이터 레이크, 데이터 웨어하우스 및 레이크하우스. 각각에 대한 간단한 개요를 살펴보겠습니다:\n\n• **데이터 레이크**: 데이터 레이크는 원시 형식으로 방대한 양의 데이터를 저장합니다. 다양한 소스에서 대량의 다양한 데이터를 처리하는 데 이상적이며 높은 유연성과 확장성을 제공합니다. 그러나 구조화된 데이터 환경의 처리 효율성 일부가 부족합니다.\n\n• **데이터 웨어하우스**: 이것은 질의 및 분석에 최적화된 구조화된 형식으로 데이터를 저장하는 시스템입니다. 데이터 웨어하우스는 구조화된 데이터에 대한 빠른 쿼리 성능에 뛰어나지만 변경사항 및 새로운 데이터 유형의 수용에 있어서 덜 유연할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n• Lakehouse: 데이터 레이크와 데이터 웨어하우스의 장점을 결합한 하이브리드 모델입니다. 데이터 레이크의 넓은 저장 공간과 유연성을 제공하면서 데이터 웨어하우스의 효율적인 쿼리 기능을 갖추고 있습니다.\n\n신중한 고려 끝에 저희는 여러 가지 이유로 레이크하우스 아키텍처를 도입하기로 결정했습니다:\n\n1. 유연성: 레이크하우스 아키텍처는 필요한 적응성을 제공했습니다. 기존의 데이터 웨어하우스는 새로운 데이터 소스나 유형을 빠르게 통합하는 것이 어려워 변경에 제한이 있고 느립니다.\n\n2. 단순화된 아키텍처: 처음에는 기존의 ETL 프로세스를 데이터 웨어하우스와 데이터 레이크에 별도로 구축하는 것을 고려했지만, 두 개의 별도 시스템을 유지할 명확한 이유를 찾지 못했습니다. 레이크하우스 모델은 강력한 쿼리 및 저장 기능을 하나의 보다 관리하기 쉬운 시스템으로 통합한 간소화된 접근 방식을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n다음은 주요 구성 요소에 대한 개요입니다:\n\nOur Data Lakehouse stack\n\n우리의 레이크하우스 아키텍처는 AWS 기술과 오픈 소스 솔루션의 최선을 활용하여 데이터를 효율적으로 관리하고 분석하는 것을 목표로 합니다.\n\n이미지를 Markdown 형식으로 변경했습니다.\n\n\n![Lakehouse Overview](/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_1.png)\n\n\n<div class=\"content-ad\"></div>\n\n**데이터 저장:**\n\n우리는 데이터 저장을 위해 AWS S3를 활용하며, 환경을 개발 및 프로덕션용 2개의 전용 버킷으로 구성합니다. 메달리온 아키텍처를 채택하여 각 버킷 내에 bronze, silver 및 gold 세 가지 독립적인 레이어(폴더)를 설정했습니다. 각 레이어는 데이터 관리 수명주기에서 특정 목적을 제공합니다. 메달리온 아키텍처는 데이터를 세 개의 레이어로 분류하는 레이크하우스 시스템에 사용되는 계층화된 데이터 처리 모델입니다:\n\n- Bronze Layer (Raw Layer): 이 기본 레벨에서는 다양한 소스로부터 도착한 대로 데이터를 정확히 저장하여 JSON, CSV 등의 원래 형식으로 보존합니다. 이 레이어는 주로 데이터 엔지니어링 팀이 디버깅 및 데이터 무결성 확인을 위해 액세스하는 데 중요하며 데이터 과학자가 초기 인사이트를 얻고 데이터 품질을 측정하기 위해 탐색 분석을 시작하는 중요한 역할을 합니다. 초기 인사이트는 더 나은 데이터 처리 전략을 안내하는 데 중요합니다.\n- Silver Layer (Cleansed Layer): 데이터가 실버 레이어로 이동하면 필요한 클렌징 및 변환 프로세스를 수행합니다. 여기서 우리는 불일치를 수정하고 데이터를 풍부하게하여 구체적인 비즈니스 규칙을 적용하여 구조화되고 유용하게 만듭니다. 우리의 분석 엔지니어는 이 클렌징 된 데이터와 작업하여 복잡한 변환을 실행하고 내부 분석을 이끄는 자세한 보고서를 생성합니다. 더 나아가 이 레이어는 우리의 데이터 과학자가 정교한 모델을 구축하는 데 의존하는 정돈된 데이터 환경을 제공합니다.\n- Gold Layer (Aggregated Layer): 이 곳에서 데이터는 가장 높은 가치를 얻으며, 비즈니스 수준의 집계 및 핵심 성능 지표로 변환됩니다. 신속한 검색 및 고속 분석을 위해 최적화된 골드 레이어는 주로 의사 결정자를 위해 액세스됩니다. 이들은 회사 전반의 전략 및 운영에 영향을 미치는 실행 가능한 인사이트를 위해 정제된 데이터에 의존합니다. 더불어 이 레이어는 기업 수준의 보고서 및 대시보드의 기반 역할을 합니다.\n\n실버 및 골드 레이어에서는 쿼리 성능을 최적화하기 위해 데이터 파일을 Parquet 파일로 저장합니다. Parquet의 효율적인 열 지향 저장 형식 덕분에 쿼리 성능이 최적화됩니다. 또한, 이러한 Parquet 파일 위에서 Apache Iceberg를 활용하여 레이크하우스 아키텍처에 여러 가지 중요한 기능을 제공합니다. Apache Iceberg를 사용하면 데이터 레이크를 전통적인 데이터베이스처럼 다루되 더 큰 유연성과 확장성을 갖습니다. 스냅샷, 트랜잭션, 업서트 및 삭제와 같은 복잡한 작업을 지원함으로써 데이터 레이크를 더 동적이고 다재다능한 시스템으로 변환할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 카탈로그:\n\n저희의 데이터 카탈로그 관리에는 비용 효율성, 다른 AWS 서비스와의 깊은 통합, 그리고 직관적인 메타데이터 관리 기능으로 인해 AWS Glue Catalog를 선택했습니다. AWS Glue Catalog는 중앙 메타데이터 저장소로 기능하며, 이를 통해 각종 AWS 서비스 간의 데이터 자산을 보다 쉽게 관리하고 접근할 수 있습니다. AWS Glue Crawler를 활용하여 S3에 저장된 데이터를 자동으로 발견하고 분류하여 데이터 카탈로그 테이블을 손쉽게 생성하고 업데이트할 수 있습니다.\n\n하지만 AWS Glue Catalog는 운영 요구에 맞게 데이터 검색을 용이하게 하는 측면에서 제한이 있음을 인지하고 있습니다. 잘 통합되어 비용 효율적이지만 세련된 데이터 카탈로그의 세부 기능 중 일부를 지원하지 않습니다. 특히 대규모 데이터 작업에 필수적인 향상된 검색 및 발견 도구와 같은 기능을 지원하지 않습니다. 이는 모델링을 위해 다양한 데이터세트에 빠르게 액세스해야 하는 데이터 과학자, 비즈니스 결정에 신속한 통찰을 얻어야 하는 데이터 분석가, 그리고 철저한 데이터 탐색에 의존하여 포괄적인 보고서를 작성하는 비즈니스 인텔리전스 전문가를 포함한 여러 팀 멤버에 영향을 줄 수 있습니다. 앞으로는 저희 조직의 중요한 역할들의 요구를 충족시키기 위해 데이터 검색을 지원하는 더 편리하고 포괄적인 데이터 카탈로그 솔루션을 탐구할 계획입니다.\n\n데이터 액세스 및 쿼리 엔진:\n\n<div class=\"content-ad\"></div>\n\nAWS Athena는 주요 쿼리 엔진으로 사용되며 AWS Glue 카탈로그와 원활하게 통합됩니다. 이 간편한 설정을 통해 데이터 레이크를 효과적으로 쿼리할 수 있어 Athena는 우리 데이터 아키텍처의 중요한 구성 요소입니다. Athena를 사용하는 주요 장점 중 하나는 비용 효율성입니다. Athena는 쿼리 중 스캔된 데이터 양에 따라 요금이 부과되기 때문에 현재 우리의 쿼리는 과도한 데이터 양을 스캔하지 않아 비용을 상당히 낮게 유지할 수 있었습니다.\n\n그러나 우리는 데이터 레이크 사용을 확대함에 따라(특히 애플리케이션 내 차트를 통한 직접 데이터 쿼리 통합이 예정된) Athena와 관련된 비용이 증가할 수 있다는 점을 알고 있습니다. 이러한 잠재적 시나리오에 대비하기 위해 Trino로 전환을 고려 중이며, 이는 EKS에서 실행되어 AWS Glue 메타스토어에 연결될 것입니다. Athena와 Trino 사이의 기본적인 유사성으로 인해 이 마이그레이션은 간단할 것으로 예상됩니다.\n\n현재 Athena에서 두 가지 서로 다른 워크그룹을 활용하고 있습니다 - SQL 쿼리를 위한 하나와 Spark(Python) 연산을 위한 다른 하나입니다. 앞으로, 우리는 이러한 설정을 세분화하여 변환, 고객 분석 등과 같은 다양한 비즈니스 요구에 대해 별도의 워크그룹을 생성하여 운영 효율성과 비용 관리를 향상시킬 계획입니다.\n\n데이터 거버넌스:\n\n<div class=\"content-ad\"></div>\n\nAWS Lake Formation은 저희 데이터 거버넌스에 중요한 역할을 합니다. 레이크하우스 아키텍처에서 데이터 보안과 권한 관리를 크게 향상시킵니다. 이는 PHI 및 민감한 데이터를 다루는 데 핵심적인 엄격한 접근 제어를 시행하는 데 도움이 됩니다.\n\n강력한 접근 제어를 위한 LF-Tags 구현: 데이터가 안전하게 액세스되고 엄격한 정책을 준수하는 데 필요한 접근 권한을 정교하게 제어하기 위해, 우리는 데이터베이스 및 테이블 수준에서 권한을 세밀하게 제어하기 위해 LF-Tags를 활용합니다. 우리의 태그 전략은 체계적으로 설계되어 있으며, 데이터베이스는 일반적으로 태그가 지정되며, 더 구체적인 요구 사항에 따라 테이블 수준에서 권한을 관리합니다. 우리가 사용하는 가능한 태그에는 다음과 같은 것들이 있습니다:\n\n- 환경: dev, prod\n- 부서: app, internal, devops, hr, customers, infra, sales, ds\n- PHI: true\n- 데이터 레이크 레이어: gold, silver, bronze\n- 클라이언트 대면: true (데이터를 고객에게 노출할 수 있는지 여부를 나타냄)\n\n각 데이터베이스와 테이블에 여러 태그를 적용하여 세밀한 역할 기반 접근 제어를 가능하게 합니다. 예를 들어, 우리 애플리케이션에서 데이터를 쿼리하는 고객을 위한 역할은 client_facing: true, data_lake_layer: gold, 그리고 environment: prod와 같은 태그 조합을 통해 액세스 권한을 부여받을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n최초에 AWS Lake Formation을 설정하는 것은 간단하지 않았습니다. 이 플랫폼은 강력하지만 직관적이지 않았고, 권한 행동을 우리의 거버넌스 요구에 맞게 조정하는 데 상당한 노력과 시간이 걸렸습니다. 이러한 도전을 극복하기 위해서는 가파른 학습 곡선이 필요했는데, 다양한 구성을 실제로 실험해야 했고, 우리가 필요로 하는 상세한 액세스 제어를 효과적으로 구현하고 관리하는 방법을 이해해야 했습니다.\n\n![이미지](/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_2.png)\n\n데이터 수집:\n\n우리는 단순하게 사용할 수 있고 기관 전체의 데이터를 수집해야 할 필요가 있는 미래의 요구를 예측하여 다양한 커넥터를 지원하는 플랫폼을 찾았습니다. 두 플랫폼인 Airbyte(오픈 소스 솔루션)와 Rivery(SaaS 솔루션)을 비교하는 POC를 진행한 후, 몇 가지 설득력있는 이유로 Airbyte를 선택했습니다.\n\n<div class=\"content-ad\"></div>\n\n먼저, 저희의 결정은 데이터 양에 따라 청구되지 않는 비용 효율적인 솔루션에 더 기울였습니다. 저희는 증가하는 비용을 걱정하지 않고 자유롭게 데이터를 가져오길 선호했습니다. 게다가 Airbyte의 개발 도구는 특히 인상적이었습니다. 플랫폼의 Connector Builder SDK는 Connector Builder UI와 로우코드 커넥터 개발 환경이 모두 포함되어 있어 필요한 간편함과 유연성을 제공했습니다. 이 기능 덕분에 우리는 우리의 특정 요구에 맞게 데이터 커넥터를 쉽게 구축하고 맞춤화할 수 있었습니다.\n\nAirbyte는 대규모이자 활발한 커뮤니티를 자랑하지만 제품에 몇 가지 어려움을 겪었습니다. 처음에는 데이터 수집 속도가 느렸습니다. 특히 PostgreSQL에서 20GB를 2일 이상으로 전송하는 데 시간이 걸렸습니다. 먼저 AWS-data-lake 목적지를 사용해 보았지만 느리고 지속적인 동기화를 지원하지 않았습니다. 이 문제를 해결하기 위해 이 문제를 고치기위한 pull request를 제출했지만 3개월이 걸렸습니다. 더 나은 해결책을 찾기위해 여러 다른 목적지를 실험해 보았습니다. Parquet을 사용할 때 타임스탬프가 struct로 형식화되는 짜증나는 문제가 있는 S3 목적지가 있었습니다. 이 구체적인 문제는 2년째 해결되지 않은 상태로 있는데, 이는 지원 측면에서 중요한 차이점을 보여줍니다. 유망한 Iceberg 목적지는 AWS Glue Catalog를 지원하지 않았습니다. 그래서 AWS-Glue 목적지를 시도했지만 JSON 출력만 지원해서 비효율적이라는 것을 발견했습니다.\n\n최종적으로 이러한 옵션 중 어느 것도 우리의 요구 사항을 완전히 충족시키지 못했기 때문에, 우리는 자체적으로 사용자 정의 AWS-data-lake 목적지를 개발하기로 결정했습니다. 우리는 원래 코드를 복제하고 우리의 요구 사항에 맞게 특별히 맞춤화하여 데이터 수집 프로세스를 크게 향상시킨 맞춤형 솔루션을 만들었습니다.\n\n이러한 어려움에도 불구하고 Airbyte는 효과적으로 우리의 요구 사항을 모두 충족시켰습니다. 오늘날, Airbyte를 사용하여 약 15가지 다른 데이터 소스를 데이터 레이크에 성공적으로 통합했으며, 데이터 수집 능력과 전체 데이터 전략을 크게 향상시켰습니다.\n\n<div class=\"content-ad\"></div>\n\n데이터 처리:\n\n저희의 데이터 처리 워크플로우는 dbt Core에 의해 강력하게 주도되며, ELT (추출, 로드, 변환) 접근 방식을 사용합니다. 모든 데이터가 브론즈층에서 시작되어 점진적으로 실버층과 골드층으로 변환됩니다.\n\n저희는 dbt Athena 어댑터를 사용하고 있으며, SQL 및 Python (PySpark) 모델을 지원합니다. 이 다양성은 더 복잡한 변환을 효과적으로 처리하는 데 중요합니다. dbt-Athena 어댑터는 활기찬 커뮤니티의 혜택을 받고 있으며, 정기적인 업데이트로 계속 발전하고 있습니다. 처음에는 Python Athena 통합을 채택하는 데 약간 주저했었는데, 그 당시의 혁신성과 제한된 추적 레코드 때문이었습니다. 그러나 철저한 테스트와 유효성 검사를 거친 후에는 어떠한 문제도 발생하지 않았고, 안정성과 효율성을 확인하며 우리의 프로덕션 환경에 성공적으로 구현했습니다.\n\ndbt에서 테이블 속성을 구성하는 것은 직관적이고 유연하며, 우리의 데이터 관리 능력을 크게 향상시킵니다. 예를 들어, 증분 테이블을 널리 사용하는데, 이는 새 데이터 또는 변경된 데이터만 효율적으로 가져오는 데 중요합니다. Iceberg 테이블 형식을 활용하여 병합 증분 전략을 채택하면 데이터세트를 중복 처리 없이 원활하게 업데이트할 수 있습니다. 또한, dbt에서 데이터 파티션을 관리하는 것도 간단해졌습니다. 파티션은 테이블 속성 내에서 직접 선언할 수 있습니다. 아래는 우리의 골드층 테이블에 대한 테이블 속성 구성 예시입니다. 우리가 재료화 전략, 파티셔닝 및 데이터 형식을 어떻게 지정하는지 보여주고 있습니다:\n\n<div class=\"content-ad\"></div>\n\n\n{ config(\n    materialized='incremental',\n    incremental_strategy='merge',\n    unique_key=['session_id'],\n    partitioned_by=['day'],\n    table_type='iceberg',\n    format='parquet'\n) }\n\n\n우리는 dbt에 대해 매우 만족하고 있습니다; 이 도구는 우리의 데이터 변환을 어떻게 관리하는지에 혁명을 일으켰습니다. 이 도구는 견고한 버전 관리, 코드의 재사용성 및 데이터 흐름의 명확한 문서화를 제공하여 복잡한 변환 작업을 관리하고 레이크하우스 아키텍처 전반에서 데이터 무결성을 유지하는 것을 크게 간소화합니다.\n\ndbt와의 성공을 토대로, 우리는 현재 데이터 관리 역량을 더욱 강화하기 위해 새로운 도구를 탐색 중입니다. Montara.io가 강력한 기능 세트를 제공하여 워크플로우를 최적화하는 우리의 dbt Git 리포지토리와 직접 통합되었습니다. Montara는 자동 CI/CD, 팀원들이 dbt 전문 지식이 적은 경우에도 모델을 작성하고 테스트할 수 있는 사용자 친화적인 UI를 제공하며 데이터 계보 표시, 데이터 카탈로그 및 관찰 가능성과 같은 가치 있는 도구를 제공합니다.\n\nMontara에 감명을 받았습니다; 이 도구는 우리의 dbt 워크플로우를 크게 간소화시켜 팀 전체에서 데이터 변환을 보다 접근 가능하고 관리하기 쉽게 만듭니다. 이 도구가 비교적 새로운 것이며 여전히 발전 중이므로 가끔씩 일부 문제와 기능의 빈틈을 겪기도 하지만, 우리의 경험은 전반적으로 매우 긍정적입니다. Montara 팀은 우수한 지원을 제공하며 우리와 긴밀히 협력하여 발생하는 어려움을 신속하게 해결하고 계속되는 제품 향상에 우리의 피드백을 통합합니다. 이 협력적인 접근은 문제를 신속하게 해결할 뿐만 아니라 Montara.io가 우리의 데이터 인프라 요구와 완벽하게 일치하도록 발전하도록 보장합니다.\n\n\n\n<div class=\"content-ad\"></div>\n\n분석 및 BI 도구:\n\nApache Superset은 저희가 선택한 분석 및 비즈니스 인텔리전스 도구로, 오픈 소스와 강력한 데이터 시각화 능력으로 유명합니다. 다른 BI 도구와 비교했을 때 유연성과 비용 효율성을 강점으로 삼아 Superset을 선택했습니다. 다양한 사용자 정의 옵션과 사용자 친화적 인터페이스를 통해 우리 팀은 대시보드와 보고서를 자신들의 필요에 맞게 맞춤화할 수 있으며, 특히 Athena를 주 데이터 원본으로 사용하는 저희 독특한 분석 환경에 특히 적합합니다.\n\n데이터 분석가들은 저희 회사의 다양한 부서를 위한 대시보드를 만들기 위해 주로 Superset을 사용합니다. 더불어, Superset의 기능을 활용하여 차트를 애플리케이션에 직접 임베드하여 고객에게 유용한 통찰을 제공합니다.\n\n현재, 일부 차트는 브론즈 계층의 데이터에 직접 접근하여 실시간으로 변환 작업을 수행합니다. 그러나 더 이상 원시 데이터 쿼리의 부하를 줄이기 위해 이 접근 방식을 수정 중이며, 최종적으로 LF-tags를 사용하여 브론즈 계층의 액세스를 제한할 계획입니다.\n\n<div class=\"content-ad\"></div>\n\nSuperset은 대시보딩에 편리하고 효과적인 도구라고 생각하지만, 시간이 지남에 따라 생성된 데이터셋이 증가하면 어느 정도 어수석해질 수 있습니다. Superset의 각 데이터셋은 개별적으로 구성되어 있어 대시보드의 수와 복잡성이 증가함에 따라 중복과 관리 도전이 발생할 수 있습니다. 그럼에도 불구하고, 이러한 어려움에도 불구하고, Superset은 우리의 요구 사항을 잘 충족시켜 주며 조직 전반에서 데이터를 시각화하고 상호 작용하는 다재다능한 플랫폼을 제공합니다.\n\n오케스트레이션과 워크플로우 관리\n\nApache Airflow는 데이터 환경 내에서 워크플로우를 조정하고 관리하는 데 중요한 역할을 합니다. 오픈 소스 도구인 Airflow는 유연성, 확장성, 그리고 강력한 커뮤니티 지원을 제공하여 우리의 운영 요구에 필수적인 요소를 제공합니다. Airflow를 활용하여 모든 데이터 파이프라인이 데이터 레이크로 정확하게 트리거되어 데이터의 신선도와 신뢰성을 유지하도록 합니다.\n\n현재, 저희는 저희 레이크하우스 운영에 필수적인 세 가지 주요 DAGs (방향이 있는 비순환 그래프)를 관리하고 있습니다. 첫 번째 DAG는 AirbyteOperator를 활용하여 동기화를 위해 필요한 모든 업무를 트리거하여 브론즈 레이어에 데이터를 효율적으로 삽입하는 작업을 담당합니다. 두 번째 DAG는 dbt 변환을 실행하여 데이터를 처리하고 실버 및 골드 레이어로 옮기는 업무를 담당합니다. 세 번째 DAG는 전체 워크플로를 감독하며 데이터 처리의 원활한 흐름을 유지하기 위해 순차적으로 삽입 DAG 및 이후에 dbt DAG를 트리거합니다.\n\n<div class=\"content-ad\"></div>\n\n또한, 이러한 워크플로우 내에 Slack 알림을 통합했습니다. 이 설정은 DAG(작업 방향성 비순환 그래프) 실패 시 실시간 알림을 제공하여 지속적인 운영 및 데이터 무결성 유지를 위해 즉각적인 모니터링과 대응이 가능하게 합니다.\n\n## 결론: 전략적 이점을 위한 데이터 활용\n\n마지막으로, 데이터 레이크하우스 아키텍처를 구축하고 정제하는 우리의 여정은 도전적이고 보람찼습니다. 우리는 데이터 관리 역량을 혁신한 여러 도구와 기술을 성공적으로 통합하여, 다양한 데이터를 통합하여 동적 의사 결정을 지원하는 견고한 분석 엔진으로 변화시켰습니다. Apache Superset, AWS S3, AWS Glue Catalog, Apache Airflow, 그리고 dbt를 활용한 우리의 사용은 복잡한 데이터 과제에 대응하기 위해 첨단 기술을 채용하는 데 드러난 우리의 의지를 보여줍니다.\n\n이러한 도구들은 우리의 운영 효율을 향상시키는데 그치지 않고 회사 전반에서 더 통찰력있는 데이터 분석과 보고의 길을 열었습니다. 우리의 데이터 인프라를 계속 발전시키면서, 우리는 데이터 능력을 더욱 향상시킬 수 있는 새로운 기술과 방법을 탐구하는 데 헌신하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n저는 비슷한 여정을 걸어가고 있는 독자들로부터의 피드백과 질문을 환영합니다. LinkedIn에서 저와 연락하셔서 더 자세한 토론을 나누거나 아이디어를 교환해 주세요.\n\n","ogImage":{"url":"/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_0.png"},"coverImage":"/assets/img/2024-05-27-NavigatingtheDataDelugeAJourneytoCraftingaSophisticatedLakehousePlatform_0.png","tag":["Tech"],"readingTime":13},{"title":"DBT 증분 전략과 동등성","description":"","date":"2024-05-27 12:53","slug":"2024-05-27-DBTIncrementalStrategyandIdempotency","content":"\n\n![Screenshot](/assets/img/2024-05-27-DBTIncrementalStrategyandIdempotency_0.png)\n\n# 배경\n\n안녕하세요, 저는 데이터 엔지니어인 Todd입니다. 저는 Nowcast에서 데이터 온보딩에 주로 관여하고 있습니다. 이 기술 블로그에서는 Nowcast에서의 ETL 파이프라인 디자인의 간략한 역사를 소개하고, Airflow와 DBT의 \"Incremental Models\" 사이에서 발생한 문제를 설명하고 우리가 개발한 해결책을 소개하겠습니다.\n\n# Python으로 ETL\n\n\n<div class=\"content-ad\"></div>\n\n역사적으로 Nowcast에서는 ETL 파이프라인을 Python을 사용하여 작성했습니다. 이 파이프라인은 AWS S3, Athena, RDBMS 등에 저장된 데이터에 변환을 적용하는 많은 Python 스크립트로 구성되어 있었습니다. 우리는 이러한 스크립트를 포함하는 도커 이미지를 작성하여 ECR에 업로드하고, Airflow에서 ECS 작업을 호출했습니다. 이러한 스크립트는 보통 데이트와 같은 파티션 필드를 매개변수로 사용하여 멱등성이 있도록 설계되었습니다. 즉, 2024-01-01을 전달하면 2024-01-01의 데이터가 처리되었습니다.\n\n이러한 스크립트 중 하나를 호출할 때, 실제로 실행되는 명령은 아래와 같이 보일 것입니다. 이때 데이트 매개변수는 Airflow에서 관리됩니다:\n\n```js\npython transform_data.py 2024-01-01 --some --other --arguments\n```\n\n# Airflow\n\n<div class=\"content-ad\"></div>\n\nAirflow은 Nowcast에서 많은 해동안 사용되어온 스케줄링 및 워크플로우 관리 도구입니다. 기본적으로 두 가지로 사용되고 있어요:\n\n1. 작업 스케줄러\n2. 작업 의존성 관리\n\n역사적으로 Airflow는 매일 실행되며 여러 Python 스크립트에 '실행 날짜' 매개변수를 전달하여 데이터를 처리합니다. 문제가 발생하거나 특정 기간의 작업을 다시 실행해야 할 때는 Airflow DAG에서 해당 작업을 다시 실행할 수 있습니다. 예를 들어, 2024년 01월 01일에 어떤 데이터 변환 스크립트가 실패하면 문제를 식별하고 수정한 후 해당 스크립트를 다시 실행할 수 있어요. 이는 스크립트가 한 번에 하나의 파티션만 처리하고 날짜를 매개변수로 입력받기 때문에 가능한 일입니다.\n\n# DBT에서 ETL\n\n2022년 말쯤 Python ETL 플로우를 Snowflake로 이전하기 시작했습니다. 그 결과 더 빠르고 저렴하며 깨끗한 파이프라인이 만들어졌어요. 우리는 파이프라인 실행 도구로 DBT를 사용하기로 결정했습니다 — DBT는 SQL 위에 위치한 레이어로 DB 모델 정의, 템플릿, 의존성 관리 및 데이터 회귀 테스트와 같은 다양한 기능을 포함하고 있어요. 빠르고 효율적으로 ETL 파이프라인을 구축하는 데 매우 유용한 도구입니다. 파이썬에서는 파이프라인의 각 변환을 스크립트로 작성하지만, DBT에서는 템플릿화된 SQL CTAS 쿼리로 작성됩니다. 이 쿼리들은 복잡한 수천 줄의 코드로 이루어진 스크립트와 비교했을 때 매우 읽기 쉽습니다.\n\n<div class=\"content-ad\"></div>\n\nBest practise in DBT is to use Incremental Models:\n\n# Incremental Models\n\nIncremental models are an efficient way of defining how to (incrementally) add data to our SQL models — consider we have a table that describes credit card transactions — we can make a DBT model (CTAS) that looks something like this:\n\n```js\n{\n materialized=\"table\"\n}\nselect\n transaction_id,\n transaction_date,\n user_id,\n store_name_description,\n transaction_amount\nfrom { ref('external_table_transaction') }\n```\n\n<div class=\"content-ad\"></div>\n\n이 코드는 table external_table_transaction에서 거래 데이터를 불러오는 테이블을 만듭니다. 문제는이 쿼리를 다시 실행할 때마다 전체 테이블을 다시로드한다는 것입니다. 테이블에 데이터가 많아질수록 쿼리가 느려지고 비용이 많이 발생합니다. 이 문제의 해결책은 증분 모델을 사용하는 것입니다:\n\n```js\n{\n config(\n materialized=\"incremental\",\n unique_key=[\"transaction_id\"],\n incremental_strategy=\"delete+insert\",\n )\n}\nselect\n transaction_id,\n transaction_date,\n user_id,\n store_name_description,\n transaction_amount\nfrom { ref('external_table_transaction') }\n {- if is_incremental() }\n where transaction_date = (select max(transaction_date) + 1 as next_date from { this })\n{- endif }\n```\n\n여기에서 우리는 DBT를 강력하게 만드는 일부 매크로/템플릿 기능을 볼 수 있습니다. 이제 기본적으로 하는 것은 테이블의 최신 데이터보다 1일이 더 늦은 거래 데이터만 external_table_transaction에서 로드해야 한다는 것입니다. 이것은 간단하면서도 강력합니다. 업데이트마다 계속 커지는 수십억 개의 데이터 행 처리 대신 이제 이전에 볼 수 없던 행만 처리하면 됩니다. 그리고 필요하다면 전체 갱신으로 테이블을 다시로드할 수 있는 옵션도 있습니다.\n\n# 문제\n\n<div class=\"content-ad\"></div>\n\n점진적 모델은 매우 매력적입니다 — 수학적으로 아름답고 데이터 스트림을 다룰 때 매우 잘 작동합니다. 문제는 처리하려는 데이터를 제어해야 할 때 발생합니다 — 점진적 모델은 특정 파티션만 다시 실행할 수 없으며 대신에 증분 모델의 규칙에 따라 데이터를 로드합니다. 이론적으로는 문제가 되지 않을 수도 있지만, 점진적 모델이 이상적인 환경에서 실행된다면 모든 데이터가 정확히 한 번만 로드될 것입니다 — 하지만 현실은 복잡합니다 — DAG가 깨지고, 데이터가 늦게 전달되거나 아예 제공되지 않는 경우가 발생하며 때로는 역사적 기록을 다시 로드해야 할 때가 있습니다. 게다가 Airflow 파이프라인이 어떤 이유로든 실패할 경우 DBT 작업이 Airflow 실행과 동기화되지 않을 수 있습니다. Nowcast로 마이그레이션한 이후 DBT를 사용하면서 경험한 점진적 모델과 관련된 이슈 목록이 아래에 나와 있습니다:\n\n- 한 파이프라인에서 수리가 진행된 것이 있었는데, 이는 2년 전으로 거슬러 올라가야 했으므로 역사적 데이터를 로드해야 했는데 (증분) 데이터 파이프라인이 역사적 재실행을 처리할 수 없어서 즉시 처리해야 했습니다.\n- 다른 DAG에서 상류 이슈로 3일 동안 깨졌으며, 3일 동안 데이터가 로드되지 않았고, DAG가 4일째 실행될 때 1일부터 데이터를 로드했으므로 동기화가 맞지 않았습니다.\n- 세 번째 파이프라인에서 상류 스킵 날짜(데이터가 빠진 날)가 발생했고, 점진적 모델은 데이터를 로드하기 위해 데이터에서 최대 날짜에 `1`을 추가하는 방식으로 처리했으나 해당 날짜가 나타나지 않아 데이터가 로드되지 않은 채로 수동 처리가 필요해졌습니다.\n\n하지만 우리는 단순히 점진적 모델을 포기할 수 없습니다 — 일부 파이프라인은 수십억 개의 행을 처리해야 하므로, 테이블을 대량으로 처리할 쿼리를 작성하면 느리고 비용이 많이 소요될 것입니다.\n\n# 동형성(idempotency) 및 분할의 중요성.\n\n<div class=\"content-ad\"></div>\n\n점진적 모델의 주요 문제는 이 모델이 멱등성을 갖지 않으며 특정 파티션에 대해 실행 구성이 불가능하다는 것입니다. 우리가 ETL 파이프라인에 대해 예전에 채택한 방식은 멱등 스크립트가 여러 번 다시 실행할 수 있는 횟수에 제한이 없는 것이었습니다. 과거 데이터에 문제가 발생하면 특정 파티션을 다시 생성할 수 있었고, 스크립트가 멱등성을 가졌기 때문에 특정한 날짜를 여러 번 실행해도 문제가 발생하지 않았습니다. 하지만 점진적 모델은 데이터의 특정 파티션을 다시 실행할 수 있는 능력이 없으며, 대신 모든 데이터를 스트림처럼 처리하여 보지 않은 데이터만을 로드합니다. 다시 말해 특정 규칙을 충족하는 데이터를 로드하는 것이죠.\n\n우리가 Airflow라는 스케줄링 도구를 사용하고 있기 때문에 데이터 파이프라인은 어떤 종류의 시간적 분할과 일치해야 합니다. 시간별, 일별, 주별, 월별 등 다양한 분할 방식이 될 수 있지만 중요한 점은 Airflow가 어떤 일정에 따라 실행되고 있다는 것입니다. 만약 과거 Airflow 작업을 다시 실행한다면 해당 작업을 호출할 때 해당하는 시간적 파티션에 맞게 실행되기를 기대하지만, 점진적 모델은 항상 앞으로만 '보기' 때문에 과거의 파티션에 대해 구성되지 않습니다. 이것은 Airflow에서 작업을 실행할 때 예상하는 것과는 다릅니다.\n\n하루마다 실행되는 2개의 Airflow DAG를 고려해보죠. 하나의 DAG는 매개변수로 날짜를 사용하여 해당 파티션만 실행하는 작업을 가지고 있습니다. 다른 DAG는 점진적 모델을 사용하며 실행할 때 보지 않은 데이터를 처리합니다. 둘 다 정상적으로 실행될 때 이전에 보지 못한 일별 데이터를 처리하게 되며 두 DAG는 동일하게 동작합니다. 하지만 문제가 발생하여 특정 날짜인 2024년 1월 1일을 다시 로드해야 할 때는 어떨까요? 파티션화된 DAG는 예상대로동작하여 2024년 1월 1일을 다시 실행할 것이지만, 점진적 모델은 Airflow에 전달되는 날짜와 관계없이 이전에 본 적 없는 데이터만을 로드할 것입니다.\n\n점진적 모델의 한계에 대해 논평한 댓글에서는:\n\n<div class=\"content-ad\"></div>\n\n간단히 말하면 - Airflow와 같은 일정 관리 도구를 사용할 때 시간 분할을 기대하는 경우, 점진적 모델이 잘 작동하지 않습니다.\n\n# 해결책\n\n해결책은 간단합니다 - DBT 변수를 사용할 수 있습니다. 또한 점진적 모델의 기능을 완전히 포기할 필요가 없습니다. 하나 이상의 변수를 추가하여 하나 이상의 분할에 명시적으로 실행할 수 있습니다:\n\n```js\n{- set target_date = var(\"target_date\", \"\") }\n{\n config(\n materialized=\"incremental\",\n unique_key=[\"transaction_id\"],\n incremental_strategy=\"delete+insert\",\n )\n}\nselect\n transaction_id,\n transaction_date,\n user_id,\n store_name_description,\n transaction_amount\nfrom { ref('external_table_transaction') }\n{- if target_date != \"\" }\n where transaction_date = '{ target_date }'\n{- else }\n {- if is_incremental() }\n where transaction_date = (select max(transaction_date) + 1 as next_date from { this })\n {- endif }\n{- endif }\n```\n\n<div class=\"content-ad\"></div>\n\n이것은 DBT 모델에 `target_date`라는 새 매개변수를 추가합니다. `target_date`가 정의되지 않은 경우 모델은 증분 동작으로 실행되지만, 변수가 전달된 경우 지정된 파티션에 대해 실행됩니다. 이 모델 구조화 방식은 Airflow에서 호출될 때 훨씬 더 잘 작동합니다.\n\n게다가, 이 모델은 이제 멱등성이 생겼습니다. 즉, 원본 데이터가 동일한 경우 동일한 쿼리와 매개변수로 실행하고 동일한 결과를 얻을 수 있습니다. 반면 증분 모델의 경우 로드된 데이터는 테이블 내용 및 상위 스트림에서 발생한 변경 내용에 따라 달라집니다.\n\n이 솔루션은 병렬, 증분 및 파티션화의 3가지 모드를 효과적으로 제공합니다. 따라서 Airflow와 DBT의 의도된 증분 전략과 잘 어울리며 이를 사용할 경우 잘 작동합니다. 아래와 같이 인수 없이 DBT를 실행하면 증분 모델을 사용할 것입니다:\n\n```js\ndbt run --select my_model\n```\n\n<div class=\"content-ad\"></div>\n\n명시적으로 새로 고침을 실행하면 대량 적재가 발생합니다:\n\n```js\ndbt run --select my_model --full-refresh\n```\n\n그리고 추가한 target_date 매개변수를 전달하면 특정 파티션에 대해서만 실행되도록 할 수 있습니다:\n\n```js\ndbt run --select my_model  --vars \"{target_date : '2024-01-01'}\"\n```\n\n<div class=\"content-ad\"></div>\n\n이제 Airflow가 전달되는 날짜 매개변수를 제어할 수 있는 명령으로 돌아왔어요. 이렇게 하면 훨씬 더 부드러운 통합이 가능해요!\n\n# 참고 자료\n\n이 문제를 연구하는 데 사용된 다음 문서들입니다:\n\nDBT — 증분성의 한계에 대해\nMedium — DBT와 Airflow를 사용한 멱등데이터 파이프라인\n\n<div class=\"content-ad\"></div>\n\n# Nowcast의 엔지니어링\n\n만약 DBT에서 데이터 파이프라인을 구축하는 방법에 대해 알고 싶으면 아래 링크를 사용하여 친목을 돈 미팅을 예약해보세요. '문의 사항'란에 'Todd와 이야기하고 싶어요'라고 작성해주세요.\n\nNowcast는 현재 데이터 엔지니어를 채용 중입니다! 관심이 있으시면 [여기에서 지원하세요](application_link).\n","ogImage":{"url":"/assets/img/2024-05-27-DBTIncrementalStrategyandIdempotency_0.png"},"coverImage":"/assets/img/2024-05-27-DBTIncrementalStrategyandIdempotency_0.png","tag":["Tech"],"readingTime":7},{"title":"데이터 품질 관리의 과거, 현재, 그리고 미래 2024년에 알아야 할 테스트, 모니터링, 그리고 데이터 관찰 가능성","description":"","date":"2024-05-27 12:51","slug":"2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024","content":"\n\n## 데이터 환경이 진화하고 있으며, 데이터 품질 관리도 함께 발전해야 합니다. 다음은 AI 시대에 데이터 품질 관리가 향하는 방향과 세 가지 일반적인 접근 방식에 대한 정보입니다.\n\n![이미지](/assets/img/2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024_0.png)\n\n서로 다른 용어일까요? 같은 문제에 대한 독특한 접근 방식일까요? 아니면 다른 것일까요?\n\n그리고 더 중요한 것은 — 모두 세 가지가 정말 필요한가요?\n\n<div class=\"content-ad\"></div>\n\n데이터 엔지니어링에서처럼, 데이터 품질 관리도 초속으로 진화하고 있어요. 기업에서 데이터와 AI의 급부상으로 인해, 현대 비즈니스에 있어 데이터 품질은 제로 데이 위험이 되었고 데이터 팀이 해결해야 할 핵심 문제가 되었어요. 중첩 용어가 많아서 어떻게 모두 맞는지 또는 맞는지 여부가 항상 명확하지 않아요.\n\n그러나 몇몇이 주장하는 것과는 달리, 데이터 품질 모니터링, 데이터 테스트 및 데이터 가시화는 데이터 품질 관리에 대한 대안적인 접근 방식도 아니고, 상충되는 것도 아니에요. 이것들은 하나의 해결책의 보완적 요소들이에요.\n\n이 글에서, 이 세 가지 방법론의 구체적인 내용, 각각이 어디에서 가장 잘 작동하며, 어디서 약점이 있는지, 그리고 2024년에 데이터 신뢰를 증진할 수 있는 데이터 품질 실무를 최적화하는 방법에 대해 살펴볼게요.\n\n# 현대 데이터 품질 문제 이해하기\n\n<div class=\"content-ad\"></div>\n\n현재 솔루션을 이해하기 전에 문제를 이해해야 합니다. 시간이 지남에 따라 어떻게 변화했는지 알아야 합니다. 다음 유사성을 고려해 봅시다.\n\n상상해보세요. 당신이 지역 수도 공급을 책임지는 엔지니어라고 상상해봅시다. 당신이 이 직무를 맡을 때, 그 도시에는 단 1,000 명의 주민이 있었습니다. 그러나 도시 아래에 금이 발견되자, 당신의 1,000 명 주민들의 작은 커뮤니티가 1,000,000 명의 진정한 도시로 변모했습니다.\n\n이것이 당신이 하는 일에 어떻게 영향을 미칠까요?\n\n먼저, 작은 환경에서는 실수 포인트가 상대적으로 적습니다. 파이프가 고장나면, 근본 원인을 냉동 파이프, 누군가가 수도관에 파고들면, 일반적인 몇 가지 원인 중 하나로 좁힐 수 있고, 1~2 명의 직원이 리소스로 문제를 빠르게 해결할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n100만 명의 신규 거주민을 디자인하고 유지하기 위한 뱀과 같은 파이프라인, 수요 충족을 위해 필요한 광란스러운 속도, 그리고 팀의 한계적인 능력(및 가시성) 때문에 예상했던 모든 문제를 찾아 해결하거나 감시해야 할 수 있는 능력이 더 이상 동일하지 않습니다. \n\n현대 데이터 환경도 마찬가지입니다. 데이터 팀은 금광을 발견했고 이해 관계자들은 그 발전 상황에 참여하고 싶어합니다. 데이터 환경이 커질수록 데이터 품질 유지가 더 어려워지며 전통적인 데이터 품질 방법이 덜 효과적일 수 있습니다.\n\n그들의 주장이 완전히 틀렸다고 할 수는 없습니다. 하지만 그것만으로 충분하지는 않습니다.\n\n# 그래서 데이터 모니터링, 테스트 및 관찰의 차이는 무엇일까요?\n\n<div class=\"content-ad\"></div>\n\n매우 명확하게, 이러한 방법 중 각각은 데이터 품질에 대응하려는 시도입니다. 따라서, 당신이 해결해야 할 문제가 그것이라면, 이 중 하나는 원칙적으로 그 문제를 확인할 것입니다. 하지만, 이 모두가 데이터 품질 솔루션이라는 것은 실제로 데이터 품질 문제를 해결해주지 않을 수 있다는 뜻입니다.\n\n이러한 솔루션들이 언제 어떻게 사용되어야 하는지는 그것보다는 조금 더 복잡합니다.\n\n가장 간단하면서, 데이터 품질을 문제로 생각할 수 있고, 테스트 및 모니터링을 품질 문제를 식별하는 방법으로 생각할 수 있으며, 데이터 가시성은 더 품질 문제를 해결할 수 있는 더 심도 있는 가시성과 해결 기능을 결합하고 확장하는 다양하고 포괄적인 접근 방식으로 생각할 수 있습니다.\n\n더 간단히 말하여, 모니터링과 테스팅은 문제를 확인하고, 데이터 가시성은 문제를 확인하고 해결책을 제시함으로써 실질적인 대응이 가능합니다.\n\n<div class=\"content-ad\"></div>\n\n여기 데이터 관찰이 데이터 품질 성숙도 곡선에서 어디에 위치하는지 시각화하는 빠른 그림이 있습니다.\n\n![Data Quality Maturity Curve](/assets/img/2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024_1.png)\n\n이제 각 방법에 대해 조금 더 자세히 알아보겠습니다.\n\n# 데이터 테스트\n\n<div class=\"content-ad\"></div>\n\n데이터 품질에 대한 전통적인 두 가지 방법 중 첫 번째는 데이터 테스트입니다. 데이터 품질 테스트(또는 간단히 데이터 테스트)는 사용자 정의 제약 조건이나 규칙을 사용하여 데이터 집합 내에서 특정 알려진 문제를 식별하는 감지 방법으로, 데이터 무결성을 확인하고 특정 데이터 품질 기준을 보장합니다.\n\n데이터 테스트를 생성하기 위해 데이터 품질 소유자는 SQL이나 dbt와 같은 모듈화된 솔루션을 활용하여 특정 문제(예: 과도한 널 비율 또는 잘못된 문자열 패턴)를 감지하는 일련의 수동 스크립트를 작성할 것입니다.\n\n데이터 요구 사항 — 따라서 데이터 품질 요구 사항도 — 가 매우 작은 경우, 많은 팀이 간단한 데이터 테스트에서 필요한 것을 충분히 얻을 수 있을 것입니다. 그러나 데이터가 커지고 복잡해지면, 새로운 데이터 품질 문제에 직면하게 되고 이를 해결하기 위한 새로운 능력이 필요해질 것입니다. 그 시간은 빨리 다가오게 될 것입니다.\n\n데이터 테스트는 데이터 품질 프레임워크의 필수 구성 요소로 남을 것이지만, 몇 가지 중요한 영역에서 제한이 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 깊은 데이터 지식이 필요합니다 — 데이터 테스트에는 데이터 엔지니어가 1) 품질을 정의하기 위해 충분한 전문 분야 지식이 필요하고, 2) 데이터가 어떻게 실패할 수 있는지에 대한 충분한 지식이 필요합니다.\n- 알 수 없는 문제에 대한 검토가 불가능합니다 — 데이터 테스트는 예상되는 문제에 대해서만 알려줄 뿐, 예상치 못한 사건에 대해서는 알려주지 않습니다. 특정 문제를 커버하기 위해 테스트가 작성되지 않은 경우, 테스트는 해당 문제를 발견할 수 없습니다.\n- 확장성이 없습니다 — 30개 테이블에 대해 10개의 테스트를 작성하는 것은 3,000개 테이블에 대해 100개의 테스트를 작성하는 것과 많은 차이가 있습니다.\n- 제한된 가시성 — 데이터 테스트는 데이터 자체만을 테스트하므로 문제가 데이터, 시스템 또는 해당 시스템을 제공하는 코드와 관련이 있는지 알려줄 수 없습니다.\n- 해결 방법이 없습니다 — 데이터 테스트로 문제를 감지해도, 이를 해결하는 데나 영향을 받는 내용을 이해하는 데는 도움이 되지 않습니다.\n\n어떤 규모에 있어서도, 테스트는 데이터에서 \"불!\"이라고 외치는 것과 같다. 그 후 아무도 어디서 이것을 본 것인지 알려주지 않고 걷어나가는 데이터 버전입니다.\n\n# 데이터 품질 모니터링\n\n데이터 품질 모니터링은 데이터 품질에 대한 또 다른 전통적이면서 다소 세련된 접근 방식으로, 수동 임계값 설정 또는 머신러닝을 통해 계속해서 모니터링하고 데이터에서 숨어있는 알 수 없는 이상 현상을 식별하는 영구적인 솔루션입니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, 데이터가 제때 도착했나요? 예상했던 행 수를 얻었나요?\n\n데이터 품질 모니터링의 주요 이점은 알려지지 않은 알려지지 않은 사항에 대해 보다 넓은 범위의 커버리지를 제공하며, 모든 데이터셋마다 테스트를 작성하거나 복제하여 공통 문제를 수동으로 식별해야 하는 데이터 엔지니어들을 해방시켜줍니다.\n\n어느 면에서는, 데이터 품질 모니터링이 테스트보다 전체적인 측면에서 더 ganzonden입니다. 시간이 흘러도 해당 메트릭을 비교하고 팀이 이미 알려진 문제의 데이터에 대한 단일 단위 테스트에서 보지 못할 패턴을 발견할 수 있도록 해줍니다.\n\n유감스럽게도, 데이터 품질 모니터링은 몇 가지 중요한 측면에서 부족함이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 컴퓨팅 비용 증가 - 데이터 품질 모니터링은 비용이 많이 듭니다. 데이터 테스트와 마찬가지로 데이터 품질 모니터링은 데이터를 직접 쿼리하지만, 알려지지 않은 알려지지 않은 사항을 식별하기 위해 넓게 적용되어야 하므로 큰 컴퓨트 비용이 듭니다.\n- 가치창출 시간이 느림 - 모니터링 임계값은 머신 러닝으로 자동화할 수 있지만, 먼저 각 모니터를 직접 구축해야 합니다. 이는 데이터 환경이 시간이 지남에 따라 확장됨에 따라 각 문제에 대해 많은 양의 코딩을 하고 그 모니터를 수동으로 확장해야 한다는 것을 의미합니다.\n- 제한된 가시성 - 데이터가 다양한 이유로 손상될 수 있습니다. 테스트와 마찬가지로 모니터링은 데이터 자체만을 살펴보기 때문에 이상 사항이 발생했음을 알려줄 뿐, 그 이유를 알려주지는 않습니다.\n- 해결책이 없음 - 모니터링은 테스트보다 더 많은 이상 사항을 감지할 수는 있지만, 여전히 어떤 것이 영향을 받았는지, 누가 그것을 알아야 하는지 또는 그 중 어느 것이 중요한지를 알려줄 수 없습니다.\n\n게다가, 데이터 품질 모니터링이 경고를 전달하는 데에만 더 효과적일 뿐 관리하지는 않는다는 점 때문에 여러분의 데이터 팀은 시간이 지남에 따라 실제로 데이터 신뢰성을 향상시키기보다 경보 피로를 경험할 가능성이 훨씬 더 큽니다.\n\n# 데이터 관측성\n\n이것이 데이터 관측성입니다. 위에서 언급된 방법들과는 달리 데이터 관측성은 종합적인 공급업체 중립적 솔루션을 의미하며, 확장 가능하고 실행 가능한 완전한 데이터 품질 커버리지를 제공하도록 설계되었습니다.\n\n<div class=\"content-ad\"></div>\n\n소프트웨어 엔지니어링의 최고의 실천 방법을 모티브로 한 데이터 관찰은 데이터 품질 관리의 종단간 AI 지원 접근법으로, 데이터 품질 문제에 대한 \"무엇, 누가, 왜, 어떻게\"를 단일 플랫폼 내에서 해결하기 위해 설계되었습니다. 이는 기존 데이터 품질 방법의 한계를 보완하기 위해 테스트와 완전 자동화된 데이터 품질 모니터링을 결합하여 단일 시스템으로 확장한 후, 그것을 데이터, 시스템 및 코드 수준으로 확장하여 데이터 환경을 커버합니다.\n\n중요 사건 관리 및 해결 기능 (자동 열 수준 라인형 및 경보 프로토콜과 같은)과 결합된 데이터 관찰은 데이터 팀이 수집부터 사용까지 데이터 품질 문제를 감지, 분류 및 해결할 수 있도록 돕습니다.\n\n더불어 데이터 관찰은 데이터 엔지니어, 분석가, 데이터 소유자 및 이해 관계자를 포함한 팀 간 협업을 촉진하여 교차 기능적 가치를 제공하도록 설계되었습니다.\n\n데이터 관찰은 전통적인 데이터 품질 실무의 단점을 다음 네 가지 핵심 방식으로 해결합니다:\n\n<div class=\"content-ad\"></div>\n\n- 견고한 사건 분류 및 해결 - 가장 중요한 것은 데이터 관찰성이 사건을 빨리 해결할 수 있는 리소스를 제공합니다. 태깅 및 경보 외에도 데이터 관찰성은 자동 열 수준 계보를 통해 원인 분석 프로세스를 빠르게 처리하여 팀이 영향을 받은 것, 누가 알아야 하는지, 고치러 가야 할 곳을 한 눈에 볼 수 있도록 돕습니다.\n- 완벽한 가시성 - 데이터 관찰성은 데이터 소스를 초월하여 인프라, 파이프라인 및 데이터 이동 및 변환하는 포스트 인게스션 시스템까지 확대하여 회사 전반의 도메인 팀을 위해 데이터 문제를 해결합니다.\n- 가치 실현 속도 향상 - 데이터 관찰성은 ML 기반 모니터를 사용하여 설정 프로세스를 완전히 자동화하고 코딩이나 임계값 설정 없이 즉시 커버리지를 제공하여 환경에 따라 시간이 경과함에 따라 자동으로 확장되는 커버리지를 빠르게 얻을 수 있습니다 (사용자가 정의한 테스트가 쉬워지는 사용자 정의 테스트를 위한 커스텀 인사이트 및 단순화된 코딩 도구도 포함됨).\n- 데이터 제품 건강 추적 - 데이터 관찰성은 전통적인 테이블 형식을 벗어나 모니터링 및 건강 추적을 확장하여 특정 데이터 제품이나 중요 자산의 건강 상태를 모니터링, 측정 및 시각화합니다.\n\n# 데이터 관찰성과 AI\n\n우리는 모두 \"쓰레기를 넣으면 쓰레기가 나온다\"는 말을 들어본 적이 있을 것입니다. 그런데, 그 말은 AI 애플리케이션에 대해 두 배로 참된 것입니다. 그러나 AI는 단순히 출력물을 제공하기 위해 더 나은 데이터 품질 관리가 필요한 것뿐만 아니라 진화하는 데이터 자산의 확장성을 극대화하기 위해 데이터 품질 관리 자체가 AI에 의해 지원되어야 합니다.\n\n데이터 관찰성은 기업 데이터 팀이 AI에 신뢰할 수 있는 데이터를 효과적으로 제공할 수 있도록 해주는 사실상 유일한 데이터 품질 관리 솔루션이자 가능성이라고 볼 수 있습니다. 이를 달성하는 한 가지 방법은 AI로 구동되는 솔루션이기도 하기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\nAI를 활용하여 모니터 생성, 이상 징후 감지, 원인 분석을 통해 데이터 관찰력은 실시간 데이터 스트리밍, RAG 아키텍처 및 기타 AI 사용 사례를 위한 초스케일 데이터 품질 관리를 가능하게 합니다.\n\n# 그렇다면, 2024년에는 데이터 품질이 어떻게 변화할까요?\n\n기업 및 그 이상을 위한 데이터 에스테이트가 계속 발전함에 따라, 전통적인 데이터 품질 방법으로는 데이터 플랫폼이 망가질 수 있는 모든 방법을 감시할 수 없거나 그 문제를 해결하는 데 도움을 줄 수 없습니다.\n\n특히 AI 시대에는 데이터 품질이 비즈니스 리스크뿐만 아니라 존립적인 리스크이기도 합니다. 모델로 공급되는 데이터의 전부를 신뢰할 수 없다면, AI의 출력도 신뢰할 수 없게 됩니다. AI의 빠른 규모에서는 전통적인 데이터 품질 방법으로는 이러한 데이터 자산의 가치나 신뢰성을 보호하는 데 충분하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n효과적인 데이터 품질 관리를 위해서는 테스트와 모니터링이 하나로 통합된 플랫폼에 솔루션이 필요합니다. 이 솔루션은 데이터 환경 전체를 객관적으로 모니터링하고 데이터 팀이 문제를 신속히 해결할 수 있는 자원을 제공해야 합니다.\n\n다시 말해, 최신 데이터 팀이 필요한 것은 데이터 관측성입니다.\n\n첫 번째 단계. 감지하기. 두 번째 단계. 해결하기. 세 번째 단계. 성공하기.","ogImage":{"url":"/assets/img/2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024_0.png"},"coverImage":"/assets/img/2024-05-27-ThePastPresentandFutureofDataQualityManagementUnderstandingTestingMonitoringandDataObservabilityin2024_0.png","tag":["Tech"],"readingTime":8},{"title":"어떻게 Apache Airflow에서 2000개 이상의 DBT 모델을 조율하는지","description":"","date":"2024-05-27 12:49","slug":"2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow","content":"\n\n![How we orchestrate 2000 DBT models in Apache Airflow](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_0.png)\n\n요즘에는 DBT (Data Build Tool)가 매우 표현력 있는 SQL과 Jinja 템플릿을 사용하여 변환을 선언하는 방식을 통해 다양한 처리 엔진에 연결되는 데이터 변환 워크플로우로 자리 잡았습니다. 이에 더불어 DBT는 문서 작성, 테스트, 그리고 기본 기능을 확장하는 커뮤니티 제작 패키지에 대한 좋은 지원을 제공합니다. ELT에서의 T를 훨씬 더 쉽고 즐겁게 만들었습니다.\n\nDBT Core는 모델 간의 계보를 다루지만, 프로덕션 환경에서 실행되어야 하는 위치와 시기에 대한 솔루션을 제공하지 않습니다. 다시 말해, 오케스트레이션은 기본적으로 제공되지 않습니다.\n\n본 글에서는 Airflow를 활용하여 DBT Core 프로젝트를 오케스트레이션하는 방법을 살펴볼 수 있습니다. 이를 통해 데이터 분석가 및 심지어 제품 소유자도 자신만의 데이터 모델을 생성하고 유지할 수 있는 직관적인 파이프라인을 만들었습니다. SQL과 Git의 기본 지식만 있으면, 비즈니스의 다양한 사람들이 몇 분 만에 자신의 모델을 Airflow DAG로 전환하여 분산 및 확장 가능한 환경에서 실행할 수 있습니다. 이는 경보, 데이터 품질 테스트, 그리고 내장된 액세스 제어와 함께, Airflow DAG가 무엇인지 알 필요 없이 UI에서 상호 작용할 수 있습니다 😄\n\n<div class=\"content-ad\"></div>\n\n주요 부분으로 나눠 보겠습니다:\n\n- Mono vs Multi DAG 접근 방식\n- 프로젝트 구조 및 DAG 레이아웃\n- DAG 생성 파이프라인\n- DBTOperator를 생성한 방법과 이유\n- 결론 및 앞으로의 계획\n\n# Mono vs Multi DAG 접근 방식\n\n이 문제에 대한 직관적인 방법은 전체 DBT 프로젝트를 \"하나의 큰 DAG\"로 모델링하는 것입니다. 이는 DBT 계보를 고려하여 작업을 연결하기 쉽게 만들어주며 Airflow에서 전체 DBT 프로젝트의 멋진 계보 뷰를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n하지만 Monon DAG 방식에는 이 프로젝트를 시작할 때 우리에게 중요한 몇 가지 단점이 있습니다:\n\n- DAG 레벨에서 일정이 설정되므로 프로젝트 전체가 동일한 일정으로 실행됩니다. 이는 프로젝트 전체에서 모델에 대해 다른 SLA가 있는 경우 문제가 될 수 있습니다.\n- 큰 DAG는 탐색하기 어려울 수 있습니다. 프로젝트에 2000개 이상의 모델이 있는 경우 이 거대한 DAG를 통해 길을 찾는 것은 분석가나 비즈니스 사용자들에게 특히 Airflow에 익숙하지 않은 사람들에게 도전이 될 수 있습니다.\n- 접근 제어에 효율적이지 않습니다. 서로 다른 팀이 DBT 프로젝트의 다른 부분을 소유하고 있기 때문에 Airflow에서도 이 분리를 활용해야 합니다. 예를 들어, 당신의 팀만 당신의 모델을 수동으로 트리거하거나 완전한 새로 고침을 수행할 수 있어야 합니다. 하나의 큰 DAG만 있는 것은 전체 프로젝트에 대한 하나의 접근 제어 계층을 의미합니다.\n- 모델 실패의 경우 알림을 분할하기가 어려울 수 있습니다. 다시 말하지만, 모델 실패의 경우 관련 팀에만 알림을 보내기를 원했습니다.\n\n매우 중요한 참고: DBT는 여러 프로젝트를 네이티브로 지원하기 전에 프로젝트를 시작했습니다. DBT 코어에서 완전히 지원되지 않았지만, DBT 매쉬는 프로젝트를 분할하고 프로젝트 당 하나의 DAG를 갖는 경험을 보다 수월하게 만들 수 있는 방법일 수 있습니다.\n\n## DBT 프로젝트를 여러 DAG로 분리하기\n\n<div class=\"content-ad\"></div>\n\n위에서 언급한 문제를 해결하기 위해, 우리는 조직에 맞는 그룹화 규칙에 따라 프로젝트를 다른 DAG로 나누기로 결정했습니다. 이를 통해 프로젝트의 다른 부분에 대해 서로 다른 SLA를 가질 수 있고, DAG 수준에서 액세스 제어 및 콜백 함수에서 알림/알림 대상을 다르게 설정할 수 있습니다. 또한, 팀은 자신들의 DAG만 쉽게 필터링할 수 있으며, Airflow에서 모델을 더 잘 탐색할 수 있습니다.\n\n그러나, 어떤 모델을 어떤 DAG에 그룹화할지 결정하는 방법 및 종속 DAG를 어떻게 연결할지에 대한 자연스러운 질문들이 제기됩니다.\n\n이 중요한 질문들은 우리를 오늘날의 솔루션을 개발하는 데 이끌었습니다. 여기에서 언급해야 할 중요한 점은 Airflow에서 DBT 라인어지 전체를 볼 수 있는 것이 우리에게 그다지 중요하지 않다는 점입니다. 우리는 데이터 탐색을 위해 Datahub를 사용하며, 이는 매우 좋은 라인어지보기를 제공합니다. 따라서, 우리는 Airflow를 가능한 가장 효율적인 방법으로 모델 실행을 관리하기 위한 도구로 사용하기로 결정했으며, 데이터 발견 도구로 사용하지는 않기로 했습니다.\n\n# 프로젝트 구조 및 DAG 배치\n\n<div class=\"content-ad\"></div>\n\n이전에 언급된 질문들을 고려할 때, 우리는 모델 그룹 개념을 고안해냈습니다. 모델 그룹은 서로 깊게 관련된 데이터 변환의 집합입니다. 예를 들어 함께 새로 고쳐져야하고 단위로서만 의미가 있는 같은 데이터 마트의 테이블들입니다. 또한, 이러한 테이블들은 단일 팀에 의해 소유되고 유지보수됩니다. 모델 그룹은 비즈니스 목표를 달성하기 위해 고안되었습니다. 중간 변환 수행 및 테이블 그룹 준비, 데이터 마트 생성, KPI 계산 등을 수행합니다.\n\n따라서, 우리는 각 모델 그룹 당 하나의 DAG를 가지기로 결정했습니다. 모델들이 서로 밀접하게 관련되어 있고 함께 스케줄되어야하기 때문입니다.\n\n아래에 제시된 최소주의 프로젝트 구조는 레이아웃을 이해하는 데 도움이 될 수 있습니다.\n\n![이미지](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_1.png)\n\n<div class=\"content-ad\"></div>\n\n다음과 같이 설명해보겠습니다:\n\n- dbt_project.yml: 이것은 프로젝트의 루트에 있는 일반 dbt_project 파일입니다. 여기에는 특별한 내용이 없습니다.\n- deployment.yml: 이 파일에는 배포할 모델 그룹을 등록합니다. 즉, 모델 그룹을 DAG로 변환하기 위한 작업을 수행합니다. 실행 일정, 태그, 소유자 등을 지정합니다. 다음과 같이 보일 것입니다:\n\n```js\n# deployment.yml\n---\nmodel_groups:\n  - name: model_group_a # 폴더의 이름입니다.\n    schedule: 0 0 * * * # DAG 일정입니다.\n    owner: Team_A # Airflow에서 DAG의 소유자 (역할)입니다.\n    tags: [tag1, tag2] # Airflow DAG용 태그입니다.\n    description: 추가 변환을 위해 테이블을 준비합니다. # DAG 설명입니다.\n\n  - name: model_group_b\n    schedule: 0 2 * * *\n    owner: Team_A\n    tags: [tag1, tag2]\n    description: 여러 테이블을 조인하여 데이터 마트를 생성합니다.\n```\n\n- model_group_a 및 model_group_b: SQL 모델(동일한 방식으로 DBT Python 모델도 작동합니다)이 포함된 폴더입니다. 이 예제에서는 model_group_a의 model2.sql이 종속성으로 model_group_a의 model1.sql을 참조한다고 가정합니다. 모델 그룹은 DBT 프로젝트의 폴더이며 모델을 포함합니다. 원하는 만큼 모델을 넣을 수 있으며 하위 폴더에 대한 DAG 생성도 허용합니다.\n\n<div class=\"content-ad\"></div>\n\nAirflow에서는 이 구조가 다음과 같이 보일 것입니다.\n\n이 구조를 통해 몇 가지 중요한 점을 보장할 수 있습니다:\n\n- 의존하는 DAG는 센서로 연결됩니다: 이를 통해 각 모델 그룹이 다른 일정에 따라 실행되도록하고 동시에 실패가 하향으로 전파되는 것을 방지할 수 있습니다. 센서 검사에 실패하면 하향 모델은 건너뛰게 됩니다. 여기 중요한 점은 우리가 기본 Airflow 외부 작업 센서를 분기시켜야 했단 점입니다. 이는 우리가 상류 모델 실행의 최종 상태를 확인하려고 했기 때문입니다. 기본 센서는 특정 실행 날짜만 터치할 수 있기 때문입니다.\n- 동일한 DAG 내에서 실행의 계통은 dbt-test 작업을 기반으로 합니다: 이는 데이터 품질 오류가 하향으로 전파되는 것을 방지하여, 데이터 품질 문제가 계속 악화되는 눈덩이 효과를 피할 수 있습니다.\n- 각 DAG (모델 그룹)에는 소유자가 있습니다: 이는 해당 DAG에서 수동 작업(전체 갱신 실행 트리거, 작업 지우기 등)을 취할 수 있는 사람이 적합한 팀 멤버뿐이라는 것을 의미합니다.\n- DAG의 수와 크기는 유연하며, DBT 프로젝트 레이아웃을 따릅니다: 모든 DAG가 모델 그룹을 기준으로 동적으로 생성되기 때문에, 그 크기나 세분성은 원하는 대로 조절할 수 있습니다. DBT 프로젝트 안의 모델 그룹에 포함된 모델 수가 DAG 레이아웃을 지배합니다.\n\n# DAG 생성 파이프라인\n\n<div class=\"content-ad\"></div>\n\n이제 팀원이 DBT 저장소에서 PR을 생성하는 순간부터 어떤 일이 발생하는지 살펴보겠습니다. 간단히 말해서, DBT 프로젝트의 배포 파이프라인은 다음과 같습니다.\n\n![DBT 프로젝트 배포 파이프라인](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_2.png)\n\n매 PR에 CI 단계로서 아래 두 가지 매우 중요한 요소가 도입되었습니다:\n\n- 조직 거버넌스 요구 사항 확인: 각 모델은 소유자와 적절한 태그, 설명 등을 가져야 합니다. 이는 매우 중요한데, 데이터 카탈로그를 풍부하고 의미 있는 것으로 만들어주기 때문입니다.\n- 스테이징 환경에서 업데이트된 모델 실행: 이를 통해 도입되는 변경 사항이 업데이트된 모델 및 하위 종속성에 대한 성공적인 실행을 보장합니다. DBT CI 실행을 위한 우리의 스테이징 영역에는 생산 모델의 대표적인 샘플이 포함되어 있어 CI 테스트 실행 비용을 최소화합니다. DBT 모델을 CI에서 적절히 테스트하는 것은 별도의 포스트가 필요하며 이에 대해 별도의 글이 필요할 것입니다.\n\n<div class=\"content-ad\"></div>\n\nPR이 병합되면 DAG 생성 프로세스가 시작됩니다. 이 프로세스는 DBT manifest.json 파일을 구문 분석하여 전체 그래프를 가져오는 방식으로 작동합니다. 그런 다음 deployment.yaml에 정의된 모델 그룹 규칙에 따라 다른 DAG가 생성됩니다.\n\n여기서 중요한 개념은 DBT manifest를 구문 분석할 때 \"내부\" 및 \"외부\" 모델을 구분하는 것입니다. 내부 모델은 해당 모델 그룹에 포함된 모델이며, 외부 모델은 주어진 모델 그룹 외부의 종속성입니다. 이 구분을 통해 외부 최신 작업 센서인 ExternalLatestTaskSensor를 사용하여 적절한 센서를 할당할 수 있습니다. 이 센서는 Airflow 외부 작업 센서의 파생 버전입니다. 우리는 메타데이터 데이터베이스 쿼리를 수정하여 상위 작업의 최신 상태를 가져와서 (실행 날짜별로 정렬) 센서가 상위 작업의 최신 dbt-test 결과를 확인할 수 있도록 했습니다.\n\n따라서 각 모델 그룹은 개별 일정에 따라 실행될 수 있도록 센서로 연결됩니다. 우리가 고려한 다른 옵션은 TriggerDagRunOperator를 사용하는 것이었지만, 이는 상위 최상위 모델에서만 일정을 설정할 수 있도록 했습니다.\n\n이미지 소스:\n![How we orchestrate 2000 DBT models in Apache Airflow](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_3.png)\n\n<div class=\"content-ad\"></div>\n\n흐름그림(DAGs)을 생성하는 작업 자체는 Jinja를 사용한 템플릿화를 통해 이루어집니다. 결국 우리는 단지 Python 파일들을 생성하는 것이니까요 😃. 특정 DAG에 포함할 모델들, 그들의 \"내부\" 선조 및 \"외부\" 모델 의존성(센서)을 결정하면 됩니다.\n\n마지막으로 생성 작업이 완료되면, DAG와 DBT 프로젝트 자산은 Airflow의 자산 버킷에 푸시됩니다. 거기서 다른 프로세스(Airflow에서 실행 중)가 이를 가져갈 것입니다. Airflow 측면에서 작동 방식을 알고 싶다면, 저의 Airflow 글을 참조해주세요.\n\n# 우리가 DBTOperator를 만든 방법과 이유\n\nAirflow에서 DBT를 실행 중이라면 BashOperator를 사용하여 dbt 명령을 실행하거나, 그 작업을 처리할 DBTOperator를 생성할 수 있습니다. 후자의 옵션은 전자보다 많은 이점을 가지고 있으며, 왜 여러분이 자체 DBTOperator를 만들어야 하는지 설명하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n저희는 airflow-dbt 프로젝트의 오픈 소스 구현을 사용하여 DBTOperator 여정을 시작했습니다. 몇 달 동안 잘 사용해 왔지만, 우리만의 Operator를 만드는 것이 가장 좋을 것이라는 것을 깨달았습니다.\n\n우리는 서브프로세스 명령이 아닌 DBT 프로그래밍 방식의 호출을 사용하고 싶었습니다. 이는 실행 결과를 더 잘 처리하는 방법을 제공하며 또한 모범 사례를 준수합니다. dbt cli를 위한 Python 진입점을 사용한 후 코드가 더 깔끔하고 가독성이 향상되었습니다.\n\n가장 중요한 것은 DBT Orchestration 솔루션의 명백한 제한 사항을 해결하고자 했습니다, 특히 버그 수정을 위한 수동 개입을 처리할 때입니다. 이러한 제한 사항 중 일부는 아래에 나열되어 있습니다.\n\n## 증분 모델의 스키마 변경\n\n<div class=\"content-ad\"></div>\n\nDBT에서 기본으로 제공하는 on_schema_change 옵션 중 우리 문제를 해결하는 데는 거의 모든 경우에서 부가 정보를 백필할 필요가 있기 때문에 문제 해결이 되지 않았습니다. 예를 들어 열이 추가될 때 정보를 백필해야 하는 경우가 대부분이었습니다. 그래서 스키마 변경 시 유일한 옵션은 전체 새로 고침을 트리거하는 것이었습니다. 우리는 예상된 소스 스키마 변경으로 인해 많은 모델이 실패했고, 그 당시 \"트리거\"를 하려면 Snowflake에서 테이블을 삭제해야 했습니다 😅.\n\n물론, 이것은 이상적이지 않습니다. 그래서 우리가 처음으로 구현한 것 중 하나는 사용자 지정 DBTOperator에서 실행이 실패한 후 dbt-run 실행 로그를 구문 분석하여 실패가 스키마 변경으로 인한 것인지 감지하면 --full-refresh 플래그를 전달하여 해당 모델을 자동으로 다시 트리거하는 기능이었습니다. 이 간단한 기능 덕분에 DBT 모델의 일일 유지 보수 시간이 단축되었습니다.\n\n## 대규모 모델이나 전체 새로 고침의 초기 처리\n\n가끔 아주 큰 모델의 초기 처리를 할 때나 여러 이유로 수동으로 전체 새로 고침을 트리거할 때, Snowflake DBT Warehouse를 과부하시키는 경우가 있습니다. 그를 피하기 위해 DBTOperator에 기능을 만들어서 해당 모델을 실행하는 데 사용하는 웨어하우스를 동적으로 변경하고 크기(소형, 중형, 대형 등)를 설정하는 기능을 만들었습니다.\n\n<div class=\"content-ad\"></div>\n\n이렇게 함으로써, 모든 기본적인 작은 증분 모델을 동시에 동일한 DBT Warehouse에서 실행할 수 있으면서, 전용 리소스가 할당된 격리된 Warehouse에서 대규모 실행을 수행할 수 있습니다. 이는 Snowflake 쿼리 실행 대기열의 증가를 방지합니다.\n\n또한, 데이터 분석가들이 Airflow 인터페이스에서 직접 전체 새로 고침을 트리거할 수 있게하여, 테이블을 삭제할 필요가 없습니다. DBTOperator가 하는 일은 적절할 때 dbt run 명령에 --full-refresh 플래그를 전달하는 것 뿐입니다.\n\n## 모델 그룹에서 개별 모델 수동 트리거\n\n때로는 데이터 분석가들이 DAG의 모델 그룹에서 하나 또는 두 개의 모델만 실행하도록 트리거해야 할 필요가 있습니다. 가끔 이러한 실행은 지정된 모델의 전체 새로 고침이어야 할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n위 문제를 해결하기 위해, Airflow DAG의 매개변수로 사용 가능한 옵션을 만들어 선택한 분석가가 DAG에서 특정 모델만 트리거할 수 있도록 했습니다. 그 특정 DagRun에 선택되지 않은 다른 모든 모델은 건너뛰게 됩니다. 이 접근 방식은 한 두 개의 모델만 실행해야 할 때 모든 모델을 실행하여 리소스를 낭비하는 것을 방지합니다.\n\nAirflow의 clear task 옵션을 사용하는 것도 해결책이지만, 사용자가 전체 리프레시를 실행하거나 그 모델을 실행하는 데이터 웨어하우스를 변경해야 하는 경우에는 제한적입니다. Airflow에서 작업을 지우기만 해서 매개변수로 실행을 사용자 정의할 수는 없습니다. 이 사용자 정의 옵션을 통해 분석가가 더 정확하게 그들의 요구 사항을 지정할 수 있어 모델 실행의 효율성과 유연성을 향상시킬 수 있습니다.\n\n## 모델 수정 후 하류 종속성의 트리거\n\n우리의 Airflow-DBT 구조에서 모델 그룹에 따라 많은 DAG가 있으며, 일부 모델은 4~5개의 DAG로 구성된 긴 종속성 체인을 가지고 있습니다. 그 체인의 첫 번째 DAG에서 모델 실행(실행 또는 테스트)이 실패하면 다른 DAG에서의 모든 하류 모델이 오류 전파를 방지하기 위해 건너뛰게 됩니다. 첫 번째 모델이 수정된 후에도 우리는 모든 하류 DAG를 다시 실행할 수 있도록 어떻게 보장할 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\n이전에는 이 작업을 수동으로 처리했었어요 😞. 데이터 분석가들은 모델 수정이 적용된 후 재시작해야 하는 하향 DAG들을 계속 추적해야 했어요. 이 과정은 시간이 많이 소요되고 오류가 발생하기 쉬웠어요.\n\n이 문제를 해결하기 위해, 우리는 Airflow DAG에서 사용되는 DBTOperator의 사용자 정의 로직에 의해 구동되는 하향 트리거 옵션을 만들었어요. DAG 실행 시 이 값을 설정하면 모든 모델이 성공하면 DAG는 자동으로 모든 하향 종속성을 인지하고 해당 종속성들의 DagRun을 트리거합니다. 이를 통해 버그 수정 후 DAG를 수동으로 트리거하는 프로세스가 불필요해졌어요.\n\n우리는 dbt ls 명령을 사용하여 종속성 그래프에 있는 모델을 나열하는데 구현이 간단했어요. 그런 다음, 해당 모델을 DAG와 매핑하고 Airflow의 trigger_dag() 함수를 사용하여 하향 실행을 자동으로 트리거했어요.\n\n더 중요한 것은, 이 프로세스가 \"체인 반응\"으로 자동으로 계속된다는 것이에요: 트리거된 DAG는 완료되면 하향 종속성도 트리거하도록 인수 플래그를 받아 이 프로세스는 체인에서 마지막 DAG가 완료될 때까지 계속됩니다.\n\n<div class=\"content-ad\"></div>\n\n## DAG 매개변수를 통한 DBT 실행 인터페이스\n\nDBTOperator에 위에서 언급한 솔루션들을 구현한 후, 우리는 또한 DAG 매개변수를 생성하여 일부 구성을 사용자에게 노출시켜 수동 실행을 사용자 정의할 수 있도록 했습니다.\n\n![image](/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_4.png)\n\n이로써 데이터 분석가들과 분석 엔지니어들의 일상에 큰 변화가 생겼습니다. 이제 필요할 때 수동 실행을 완전히 사용자 정의할 수 있게 되었습니다.\n\n<div class=\"content-ad\"></div>\n\n이러한 모든 매개변수는 이전에 설명한 대로 DAG를 자동으로 생성할 때 템플릿에 동적으로 추가됩니다. 따라서 예를 들어, 해당 모델 그룹의 사용 가능한 모델을 사용하여 Models 드롭다운을 채우게 됩니다.\n\n또한 DAG 생성 파이프라인에서 사용되는 \"매개변수 주입\" 방법은 매우 확장 가능하여 미래에 필요에 따라 더 많은 매개변수를 생성할 수 있습니다. \n\n# 결론과 앞으로의 방향\n\n저는 이 게시물이 Airflow에서 DBT 오케스트레이션에 대한 다른 관점을 제공할 수 있기를 바랍니다. 이 구현은 2년이 지난 후에도 여전히 우리의 요구를 충족시키지만, 완벽하거나 이상적이지는 않으며 개선할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n비슷한 오픈 소스 구현으로는 뛰어난 Astronomer Cosmos 프로젝트를 찾을 수 있어요. 여기서 재미있는 기능은 각 모델에 대한 실행 및 테스트를 결합하기 위해 작업 그룹을 사용한다는 점이에요 (우리도 그랬죠 😍) 그리고 프로젝트 구성을 통해 매우 쉽고 깔끔하게 DbtDag를 선언하는 방식이에요.\n\n프로젝트를 둘 이상의 DAG로 분할하는 것도 가능해요, 생성자가 dbt select 인수를 허용하기 때문에요. 따라서 태그를 전달하고 다른 태그에 따라 프로젝트를 분할할 수 있어요. 그러나 저는 DAG 간의 가능한 상호 작용 (모델 참조)을 어떻게 다루는지는 분명하지 않아요. DBT 조정 여정을 시작한다면 매우 깔끔한 추상화를 제공하기 때문에 꼭 확인해보세요.\n\n지금은 우리 앞에 있는 것이 데이터 계약의 구현이며 그것이 우리가 DBT와 상호 작용하는 방식에 큰 영향을 미치고 있어요. 소스 시스템과 데이터 레이크의 테이블 사이를 잇기 위해 계약을 사용함으로써 연결자(데이터 추출기)의 프로비저닝과 업무 지식이 필요하지 않은 초기(기본) 변환(유형 캐스팅, 열 이름 표준화, 복잡한 필드의 언네스팅)을 수행하는 DBT 모델을 자동화할 수 있어요. 결과적으로, 이전에 설명한 일부 모델 그룹은 데이터 계약을 기반으로 완전히 자동화된 방식으로 생성되고 있어요.\n\nDBT-Airflow 구현에 대해 더 논의할 준비가 되어 있고 커뮤니티가 이 문제를 해결하는 방법에 대해 듣는 것에 매우 열려 있어요. 따라서 유사한 구현이 있다면 어떻게 하는지 알려주세요 😆. 진정으로 높은 가치를 제공하는 멋진 솔루션을 만들 수 있는 것은 연결된 커뮤니티 덕분이에요.","ogImage":{"url":"/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_0.png"},"coverImage":"/assets/img/2024-05-27-Howweorchestrate2000DBTmodelsinApacheAirflow_0.png","tag":["Tech"],"readingTime":12}],"page":"111","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":13,"currentPageGroup":5},"__N_SSG":true}