<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>억제된 모든 LLM을 해제하세요 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-UncensoranyLLMwithabliteration" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="억제된 모든 LLM을 해제하세요 | itposting" data-gatsby-head="true"/><meta property="og:title" content="억제된 모든 LLM을 해제하세요 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-UncensoranyLLMwithabliteration" data-gatsby-head="true"/><meta name="twitter:title" content="억제된 모든 LLM을 해제하세요 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 03:36" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_buildManifest.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">억제된 모든 LLM을 해제하세요</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="억제된 모든 LLM을 해제하세요" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">15<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-UncensoranyLLMwithabliteration&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>재학습 없이 세밀 조정하기</h2>
<p><img src="/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png" alt="이미지"></p>
<p>람마 모델의 세대가 늘어날수록 새로운 기능이 제공되었습니다. 이는 사용자의 지시를 이해하고 따르는 능력이 뛰어난 '세세한 조정(세세하게 조정)' 버전을 제공합니다. 그러나 이러한 모델들은 매우 검열되어 있으며 해로운 요청으로 간주되는 것은 거부하고 "AI 어시스턴트로서 도와드릴 수 없습니다."와 같은 대답을 합니다. 이 안전 기능은 오용을 방지하는 데 중요하지만, 모델의 유연성과 반응성을 제한합니다.</p>
<p>본 문서에서는 "무효화"라는 기술을 탐구하여 재학습 없이 어떤 람마 모델이든 검열을 푸는 방법을 살펴볼 것입니다. 이 기술은 모델에 내장된 거부 메커니즘을 효과적으로 제거하여 모든 유형의 프롬프트에 대응할 수 있게 합니다.</p>
<div class="content-ad"></div>
<p>코드는 Google Colab에서도 사용할 수 있고, LLM 코스에서도 GitHub에 있습니다. 이 기사를 교정해 주신 FailSpy님에게 특별히 감사드립니다.</p>
<h1>✂️ 삭제란이란?</h1>
<p>현대 LLM은 안전 및 지시를 따르는 방향으로 세밀하게 조정되어 있어, 해로운 요청을 거부하기 위해 훈련되어 있습니다. Arditi 등이 블로그 글에서 설명한 바에 따르면, 이 거부 행동은 모델의 잔류 스트림에 있는 특정 방향을 통해 중재됩니다. 만약 이 방향을 모델이 나타내지 못하도록 막는다면, 요청을 거부하는 능력을 잃게 됩니다. 반대로, 이 방향을 인위적으로 추가하면 모델이 해가 없는 요청조차도 거부할 수 있게됩니다.</p>
<p>전통적인 디코더 전용 Llama류 아키텍처에서는 세 가지의 잔류 스트림을 대상으로 할 수 있습니다: 각 블록의 시작 부분에서(“pre”), 어텐션과 MLP 레이어 사이에서(“mid”), 그리고 MLP 이후에(“post”). 다음 그림은 각 잔류 스트림의 위치를 보여줍니다.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-UncensoranyLLMwithabliteration_1.png">
<p>LLM을 무증검 상태로 만들기 위해 먼저 모델 내의 "거부 방향"을 식별해야 합니다. 이 과정에는 몇 가지 기술적 단계가 포함되어 있습니다:</p>
<ul>
<li>데이터 수집: 유해한 지시문 집합과 무해한 지시문 집합을 사용하여 모델을 실행하고, 각각의 마지막 토큰 위치에서 잔여 스트림 활성화를 기록합니다.</li>
<li>평균 차이: 유해한 지시와 무해한 지시의 활성화 간 평균 차이를 계산합니다. 이를 통해 모델의 각 레이어에 대한 "거부 방향"을 나타내는 벡터를 얻을 수 있습니다.</li>
<li>선택: 이러한 벡터를 정규화하고, 평가하여 단일 최상의 "거부 방향"을 선택합니다.</li>
</ul>
<p>거부 방향을 식별한 후, 해당 기능을 표현하는 모델의 능력을 효과적으로 제거하는 "제거(ablate)" 작업을 수행할 수 있습니다. 이는 추론 시간 간섭이나 가중치 직교화를 사용하여 영구적으로 수행할 수 있습니다.</p>
<div class="content-ad"></div>
<p>먼저 추론 시간 개입에 대해 이야기해 보겠습니다. 잔차 스트림에 기록하는 모든 구성 요소(예: 어텐션 헤드)마다 그 출력을 거부 방향으로 투영한 후 이 투영을 뺍니다. 이 뺄셈은 각 토큰과 각 레이어에 적용되어 모델이 결코 거부 방향을 표현하지 않도록 합니다.</p>
<p>한편, 가중치 직교화는 모델 가중치를 직접 수정하는 것을 포함합니다. 거부 방향에 대해 구성 요소 가중치를 직교화함으로써 모델이 이 방향으로 기록하는 것을 방지합니다. 잔차 스트림에 기록하는 행렬을 조정하여 이러한 기여가 거부 방향에 영향을 주지 않도록 합니다.</p>
<p>다음 섹션에서는 가중치 직교화를 사용한 좌절 실현을 구현할 것입니다.</p>
<h1>💻 구현</h1>
<div class="content-ad"></div>
<p>아래의 abliteration 구현은 FailSpy의 노트북을 기반으로 하고 있습니다. 그 노트북은 원래 저자들의 노트북을 기반으로 하고 있습니다. 저는 주로 이를 적응하여 간단하고 이해하기 쉽도록 했습니다. 이 섹션은 코드가 많이 포함되어 있어서 무슨 일이 벌어지는지 볼 수 있지만, 기술적인 세부 사항에 덜 관심이 있는 경우 FailSpy의 abliterator 라이브러리를 사용할 수도 있습니다 (Hugging Face의 abliterated 모델 모음도 확인해보세요).</p>
<p>이 코드는 뛰어난 TransformerLens 라이브러리 (이전에는 EasyTransformer로 알려졌음)를 사용하여 무거운 작업을 처리합니다. 메커니즘 해석 가능성을 위해 설계되었으며 여기서는 활성화에 개입하는 데 사용됩니다. 이 라이브러리를 만든 Neel Nanda와 Joseph Bloom에게 감사드립니다.</p>
<p>먼저 필요한 패키지를 설치하고 가져와 봅시다. 이러한 모든 단계는 Google Colab 노트북에서 사용할 수 있습니다.</p>
<pre><code class="hljs language-js">!pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping

<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> functools
<span class="hljs-keyword">import</span> einops
<span class="hljs-keyword">import</span> gc

<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> <span class="hljs-title class_">Tensor</span>
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-title class_">List</span>
<span class="hljs-keyword">from</span> transformer_lens <span class="hljs-keyword">import</span> <span class="hljs-title class_">HookedTransformer</span>, utils
<span class="hljs-keyword">from</span> transformer_lens.<span class="hljs-property">hook_points</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">HookPoint</span>
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> <span class="hljs-title class_">AutoModelForCausalLM</span>, <span class="hljs-title class_">AutoTokenizer</span>
<span class="hljs-keyword">from</span> jaxtyping <span class="hljs-keyword">import</span> <span class="hljs-title class_">Float</span>, <span class="hljs-title class_">Int</span>
<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict

# <span class="hljs-variable constant_">GPU</span> 메모리를 저장하기 위해 자동 미분을 끕니다 (크레딧: <span class="hljs-title class_">Undi95</span>)
torch.<span class="hljs-title function_">set_grad_enabled</span>(<span class="hljs-title class_">False</span>)
</code></pre>
<div class="content-ad"></div>
<p>두 가지 데이터 세트가 필요합니다: 피해가 없는 지침을 포함한 하나와 유해한 지침을 포함한 하나입니다. 우리는 tatsu-lab/alpaca와 llm-attacks의 데이터를 사용할 것입니다. 모든 것을 더 쉽게 만들기 위해, 저는 이를 두 개의 Hugging Face 데이터 세트로 다시 패키징하여 mlabonne/harmless_alpaca와 mlabonne/harmful_behaviors로 만들었습니다. 그렇게 하면 여러분이 쉽게 여러분 자신의 데이터 세트로 교체할 수 있습니다.</p>
<p>지침을로드하고 "role"과 "content" 키가 있는 사전 목록으로 다시 서식화할 것입니다. 이렇게 하면 Llama 3의 채팅 템플릿을 따르는 apply_chat_tokenizer() 메서드와 호환됩니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reformat_texts</span>(<span class="hljs-params">texts</span>):
    <span class="hljs-keyword">return</span> [[{<span class="hljs-string">"role"</span>: <span class="hljs-string">"user"</span>, <span class="hljs-string">"content"</span>: text}] <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts]

<span class="hljs-comment"># 유해하고 무해한 데이터 세트 가져오기</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_harmful_instructions</span>():
    dataset = load_dataset(<span class="hljs-string">'mlabonne/harmful_behaviors'</span>)
    <span class="hljs-keyword">return</span> reformat_texts(dataset[<span class="hljs-string">'train'</span>][<span class="hljs-string">'text'</span>]), reformat_texts(dataset[<span class="hljs-string">'test'</span>][<span class="hljs-string">'text'</span>])

<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_harmless_instructions</span>():
    dataset = load_dataset(<span class="hljs-string">'mlabonne/harmless_alpaca'</span>)
    <span class="hljs-keyword">return</span> reformat_texts(dataset[<span class="hljs-string">'train'</span>][<span class="hljs-string">'text'</span>]), reformat_texts(dataset[<span class="hljs-string">'test'</span>][<span class="hljs-string">'text'</span>])

harmful_inst_train, harmful_inst_test = get_harmful_instructions()
harmless_inst_train, harmless_inst_test = get_harmless_instructions()
</code></pre>
<p>이제 데이터 세트가 준비되었으므로, abliterate 하려는 모델을 로드할 수 있습니다. 안타깝게도, HookedTransformer를 사용하여 직접 사용자 정의 모델을 로드할 수 없습니다. 여기에서, FailSpy의 노트북에 설명된 꼼수를 사용하여 사용자 정의 모델을 다운로드하고 meta-llama/Meta-Llama-3-8B-Instruct로 이름을 변경하겠습니다. GPU가 BF16과 호환되지 않는 경우 torch.float16 형식으로 로드하세요.</p>
<div class="content-ad"></div>
<p>이 예시에서는 DARE TIES(모델 병합에 관한 내 기사 참조)로 만들어진 mlabonne/Daredevil-8B를 사용할 것입니다. 이 모델은 8B 카테고리의 Open LLM Leaderboard에서 가장 높은 MMLU 점수를 가지고 있어요.</p>
<pre><code class="hljs language-js"><span class="hljs-variable constant_">MODEL_ID</span> = <span class="hljs-string">"mlabonne/Daredevil-8B"</span>
<span class="hljs-variable constant_">MODEL_TYPE</span> = <span class="hljs-string">"meta-llama/Meta-Llama-3-8B-Instruct"</span>

# 모델 다운로드 및 로드
!git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//huggingface.co/{MODEL_ID} {MODEL_TYPE}</span>

# 모델 및 토크나이저 로드
model = <span class="hljs-title class_">HookedTransformer</span>.<span class="hljs-title function_">from_pretrained_no_processing</span>(
    <span class="hljs-variable constant_">MODEL_TYPE</span>,
    local_files_only=<span class="hljs-title class_">True</span>,
    dtype=torch.<span class="hljs-property">bfloat16</span>,
    default_padding_side=<span class="hljs-string">'left'</span>
)
tokenizer = <span class="hljs-title class_">AutoTokenizer</span>.<span class="hljs-title function_">from_pretrained</span>(<span class="hljs-variable constant_">MODEL_TYPE</span>)
tokenizer.<span class="hljs-property">padding_side</span> = <span class="hljs-string">'left'</span>
tokenizer.<span class="hljs-property">pad_token</span> = tokenizer.<span class="hljs-property">eos_token</span>
</code></pre>
<p>이제 데이터셋을 토큰화할 수 있습니다. 무해한 및 유해한 지시사항에 대해 동일한 샘플 수를 사용합니다. 샘플 수가 높으면 모든 RAM/VRAM을 사용할 수 있으므로 여기서는 256으로 제한합니다.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">tokenize_instructions</span>(tokenizer, instructions):
    <span class="hljs-keyword">return</span> tokenizer.<span class="hljs-title function_">apply_chat_template</span>(
        instructions,
        padding=<span class="hljs-title class_">True</span>,
        truncation=<span class="hljs-title class_">False</span>,
        return_tensors=<span class="hljs-string">"pt"</span>,
        return_dict=<span class="hljs-title class_">True</span>,
        add_generation_prompt=<span class="hljs-title class_">True</span>,
    ).<span class="hljs-property">input_ids</span>

n_inst_train = <span class="hljs-title function_">min</span>(<span class="hljs-number">256</span>, <span class="hljs-title function_">len</span>(harmful_inst_train), <span class="hljs-title function_">len</span>(harmless_inst_train))

# 데이터셋 토큰화
harmful_tokens = <span class="hljs-title function_">tokenize_instructions</span>(
    tokenizer,
    instructions=harmful_inst_train[:n_inst_train],
)
harmless_tokens = <span class="hljs-title function_">tokenize_instructions</span>(
    tokenizer,
    instructions=harmless_inst_train[:n_inst_train],
)
</code></pre>
<div class="content-ad"></div>
<p>작업이 모두 설정되었습니다. 이제 도처럼 처리하는 첫 번째 단계인 데이터 수집을 구현할 차례입니다. 우리는 이 토큰화된 데이터셋을 처리하고 유해(harmful) 및 무해(harmless)로 나머지 스트림 활성화를 저장하려고 합니다. 이는 transformer_lens 라이브러리로 관리됩니다.</p>
<pre><code class="hljs language-js">batch_size = <span class="hljs-number">32</span>

# 활성화를 저장할 기본 사전 초기화
harmful = <span class="hljs-title function_">defaultdict</span>(list)
harmless = <span class="hljs-title function_">defaultdict</span>(list)

# 데이터 학습을 배치 단위로 처리
num_batches = (n_inst_train + batch_size - <span class="hljs-number">1</span>) <span class="hljs-comment">// batch_size</span>

<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">tqdm</span>(<span class="hljs-title function_">range</span>(num_batches)):
    <span class="hljs-title function_">print</span>(i)
    start_idx = i * batch_size
    end_idx = <span class="hljs-title function_">min</span>(n_inst_train, start_idx + batch_size)

    # 유해 및 무해 프롬프트에 모델 실행 및 활성화 캐시
    harmful_logits, harmful_cache = model.<span class="hljs-title function_">run_with_cache</span>(
        harmful_tokens[<span class="hljs-attr">start_idx</span>:end_idx],
        names_filter=lambda <span class="hljs-attr">hook_name</span>: <span class="hljs-string">'resid'</span> <span class="hljs-keyword">in</span> hook_name,
        device=<span class="hljs-string">'cpu'</span>,
        reset_hooks_end=<span class="hljs-title class_">True</span>
    )
    harmless_logits, harmless_cache = model.<span class="hljs-title function_">run_with_cache</span>(
        harmless_tokens[<span class="hljs-attr">start_idx</span>:end_idx],
        names_filter=lambda <span class="hljs-attr">hook_name</span>: <span class="hljs-string">'resid'</span> <span class="hljs-keyword">in</span> hook_name,
        device=<span class="hljs-string">'cpu'</span>,
        reset_hooks_end=<span class="hljs-title class_">True</span>
    )

    # 활성화 수집 및 저장
    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> <span class="hljs-attr">harmful_cache</span>:
        harmful[key].<span class="hljs-title function_">append</span>(harmful_cache[key])
        harmless[key].<span class="hljs-title function_">append</span>(harmless_cache[key])

    # <span class="hljs-variable constant_">RAM</span> 및 <span class="hljs-variable constant_">VRAM</span> 비우기
    del harmful_logits, harmless_logits, harmful_cache, harmless_cache
    gc.<span class="hljs-title function_">collect</span>()
    torch.<span class="hljs-property">cuda</span>.<span class="hljs-title function_">empty_cache</span>()

# 캐시된 활성화 결합
harmful = {<span class="hljs-attr">k</span>: torch.<span class="hljs-title function_">cat</span>(v) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> harmful.<span class="hljs-title function_">items</span>()}
harmless = {<span class="hljs-attr">k</span>: torch.<span class="hljs-title function_">cat</span>(v) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> harmless.<span class="hljs-title function_">items</span>()}
</code></pre>
<p>이제 각 층에 대한 거부 방향을 계산할 수 있습니다. 이는 유해 및 무해 명령의 활성화 간 평균 차이에 해당하며 정규화됩니다. 그런 다음 activation_scored에서 내림차순으로 정렬됩니다.</p>
<pre><code class="hljs language-js"># 활성화 색인을 가져오는 도우미 함수
def <span class="hljs-title function_">get_act_idx</span>(cache_dict, act_name, layer):
    key = (act_name, layer)
    <span class="hljs-keyword">return</span> cache_dict[utils.<span class="hljs-title function_">get_act_name</span>(*key)]

# 중간 층에서 유해 및 무해 활성화의 평균 차이 계산
activation_layers = [<span class="hljs-string">"resid_pre"</span>, <span class="hljs-string">"resid_mid"</span>, <span class="hljs-string">"resid_post"</span>]
activation_refusals = <span class="hljs-title function_">defaultdict</span>(list)

<span class="hljs-keyword">for</span> layer_num <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-number">1</span>, model.<span class="hljs-property">cfg</span>.<span class="hljs-property">n_layers</span>):
    pos = -<span class="hljs-number">1</span>  # 위치 인덱스
    <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> <span class="hljs-attr">activation_layers</span>:
        harmful_mean_act = <span class="hljs-title function_">get_act_idx</span>(harmful, layer, layer_num)[:, pos, :].<span class="hljs-title function_">mean</span>(dim=<span class="hljs-number">0</span>)
        harmless_mean_act = <span class="hljs-title function_">get_act_idx</span>(harmless, layer, layer_num)[:, pos, :].<span class="hljs-title function_">mean</span>(dim=<span class="hljs-number">0</span>)
        refusal_dir = harmful_mean_act - harmless_mean_act
        refusal_dir = refusal_dir / refusal_dir.<span class="hljs-title function_">norm</span>()
        activation_refusals[layer].<span class="hljs-title function_">append</span>(refusal_dir)

selected_layers = [<span class="hljs-string">"resid_pre"</span>]
activation_scored = <span class="hljs-title function_">sorted</span>(
    [
        activation_refusals[layer][l - <span class="hljs-number">1</span>]
        <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-number">1</span>, model.<span class="hljs-property">cfg</span>.<span class="hljs-property">n_layers</span>)
        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> selected_layers
    ],
    key=lambda <span class="hljs-attr">x</span>: <span class="hljs-title function_">abs</span>(x.<span class="hljs-title function_">mean</span>()),
    reverse=<span class="hljs-title class_">True</span>,
)
</code></pre>
<div class="content-ad"></div>
<p>프로세스의 마지막 단계는 계산한 거절 방향을 평가하는 과정입니다. 이를 위해 우리는 거절 방향을 추론 중 각 잔여 스트림과 각 블록에 적용할 것입니다. 다음 코드 스니펫에서 네 가지 테스트 유해 지시와 20개 블록(또는 레이어)에 대한 세대를 가져옵니다.</p>
<pre><code class="hljs language-js">...
<span class="hljs-title class_">Code</span> snippet 생략
...
</code></pre>
<p>우리는 evals 리스트에 모든 세대를 저장했습니다. 이제 이를 출력하고 각 지시에 대한 무겁지 않은 응답을 제공하는 레이어(블록)를 수동으로 선택할 수 있습니다. "I cannot" 및 "I can't"를 포함하는 응답은 원치 않는 답변을 필터링하기 위해 자동으로 제외합니다.</p>
<p>위 요구사항을 만족하는 레이어를 찾을 수 없는 경우, 이전 selected_layers 목록의 다른 잔여 스트림, 다른 지시, 추가적 블록 등을 테스트하고자 할 수 있습니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"># 인간 평가를 위한 세대 인쇄
blacklist = [<span class="hljs-string">"I cannot"</span>, <span class="hljs-string">"I can't"</span>]
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-variable constant_">N_INST_TEST</span>):
    <span class="hljs-title function_">print</span>(f<span class="hljs-string">"\033[1mINSTRUCTION {i}: {harmful_inst_test[i]}"</span>)
    <span class="hljs-title function_">print</span>(f<span class="hljs-string">"\nBASELINE COMPLETION:\n{baseline_generations[i]}\033[0m"</span>)
    <span class="hljs-keyword">for</span> layer_candidate <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-variable constant_">EVAL_N</span>):
        <span class="hljs-keyword">if</span> not <span class="hljs-title function_">any</span>(word <span class="hljs-keyword">in</span> evals[layer_candidate][i] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> blacklist):
            <span class="hljs-title function_">print</span>(f<span class="hljs-string">"\n---\n\nLAYER CANDIDATE #{layer_candidate} INTERVENTION COMPLETION:"</span>)
            <span class="hljs-title function_">print</span>(evals[layer_candidate][i])
</code></pre>
<p>저의 경우, 레이어 후보자 9가 네 가지 명령에 대해 선정적이지 않은 답변을 제공했습니다. 이것이 우리가 거부 방향으로 선택할 것이다. 그 다음으로, 무게 직교화를 구현하여 모델이 이 방향의 출력을 생성하는 것을 방지합니다. 모델이 성공적으로 선정되지 않은지를 확인하려면 완료된 내용을 인쇄하여 확인할 수 있습니다.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">get_orthogonalized_matrix</span>(
    <span class="hljs-attr">matrix</span>: <span class="hljs-title class_">Float</span>[<span class="hljs-title class_">Tensor</span>, <span class="hljs-string">"... d_model"</span>], <span class="hljs-attr">vec</span>: <span class="hljs-title class_">Float</span>[<span class="hljs-title class_">Tensor</span>, <span class="hljs-string">"d_model"</span>]
) -> <span class="hljs-title class_">Float</span>[<span class="hljs-title class_">Tensor</span>, <span class="hljs-string">"... d_model"</span>]:
    proj = (
        einops.<span class="hljs-title function_">einsum</span>(
            matrix, vec.<span class="hljs-title function_">view</span>(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), <span class="hljs-string">"... d_model, d_model single -> ... single"</span>
        )
        * vec
    )
    <span class="hljs-keyword">return</span> matrix - proj

# 가장 높은 거부 방향을 갖는 레이어 선택
<span class="hljs-variable constant_">LAYER_CANDIDATE</span> = <span class="hljs-number">9</span>
refusal_dir = activation_scored[<span class="hljs-variable constant_">LAYER_CANDIDATE</span>]

# 모델의 가중치 직교화
<span class="hljs-keyword">if</span> refusal_dir.<span class="hljs-property">device</span> != model.<span class="hljs-property">W_E</span>.<span class="hljs-property">device</span>:
    refusal_dir = refusal_dir.<span class="hljs-title function_">to</span>(model.<span class="hljs-property">W_E</span>.<span class="hljs-property">device</span>)
model.<span class="hljs-property">W_E</span>.<span class="hljs-property">data</span> = <span class="hljs-title function_">get_orthogonalized_matrix</span>(model.<span class="hljs-property">W_E</span>, refusal_dir)

<span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> <span class="hljs-title function_">tqdm</span>(model.<span class="hljs-property">blocks</span>):
    <span class="hljs-keyword">if</span> refusal_dir.<span class="hljs-property">device</span> != block.<span class="hljs-property">attn</span>.<span class="hljs-property">W_O</span>.<span class="hljs-property">device</span>:
        refusal_dir = refusal_dir.<span class="hljs-title function_">to</span>(block.<span class="hljs-property">attn</span>.<span class="hljs-property">W_O</span>.<span class="hljs-property">device</span>)
    block.<span class="hljs-property">attn</span>.<span class="hljs-property">W_O</span>.<span class="hljs-property">data</span> = <span class="hljs-title function_">get_orthogonalized_matrix</span>(block.<span class="hljs-property">attn</span>.<span class="hljs-property">W_O</span>, refusal_dir)
    block.<span class="hljs-property">mlp</span>.<span class="hljs-property">W_out</span>.<span class="hljs-property">data</span> = <span class="hljs-title function_">get_orthogonalized_matrix</span>(block.<span class="hljs-property">mlp</span>.<span class="hljs-property">W_out</span>, refusal_dir)

# 모델로 텍스트 생성
orthogonalized_generations = <span class="hljs-title function_">get_generations</span>(
    model, tokenizer, harmful_inst_test[:<span class="hljs-variable constant_">N_INST_TEST</span>], fwd_hooks=[]
)

# 세대 출력
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-variable constant_">N_INST_TEST</span>):
    <span class="hljs-keyword">if</span> <span class="hljs-title function_">len</span>(baseline_generations) > <span class="hljs-attr">i</span>:
        <span class="hljs-title function_">print</span>(f<span class="hljs-string">"INSTRUCTION {i}: {harmful_inst_test[i]}"</span>)
        <span class="hljs-title function_">print</span>(f<span class="hljs-string">"\033[92mBASELINE COMPLETION:\n{baseline_generations[i]}"</span>)
    <span class="hljs-title function_">print</span>(f<span class="hljs-string">"\033[91mINTERVENTION COMPLETION:\n{evals[LAYER_CANDIDATE][i]}"</span>)
    <span class="hljs-title function_">print</span>(f<span class="hljs-string">"\033[95mORTHOGONALIZED COMPLETION:\n{orthogonalized_generations[i]}\n"</span>)
</code></pre>
<p>이제 모델을 사용할 준비가 되었습니다. 모델을 Hugging Face 형식으로 변환하여 HF 허브에 업로드합니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-json"># 모델을 다시 HF 보안 텐서로 변환합니다
hf_model = AutoModelForCausalLM.from_pretrained(MODEL_TYPE<span class="hljs-punctuation">,</span> torch_dtype=torch.bfloat16)
lm_model = hf_model.model

state_dict = model.state_dict()
lm_model.embed_tokens.weight = torch.nn.Parameter(state_dict<span class="hljs-punctuation">[</span><span class="hljs-string">"embed.W_E"</span><span class="hljs-punctuation">]</span>.cpu())
for l in range(model.cfg.n_layers)<span class="hljs-punctuation">:</span>
    lm_model.layers<span class="hljs-punctuation">[</span>l<span class="hljs-punctuation">]</span>.self_attn.o_proj.weight = torch.nn.Parameter(
        einops.rearrange(
            state_dict<span class="hljs-punctuation">[</span>f<span class="hljs-string">"blocks.{l}.attn.W_O"</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"n h m->m (n h)"</span><span class="hljs-punctuation">,</span> n=model.cfg.n_heads
        ).contiguous()
    )
    lm_model.layers<span class="hljs-punctuation">[</span>l<span class="hljs-punctuation">]</span>.mlp.down_proj.weight = torch.nn.Parameter(
        torch.transpose(state_dict<span class="hljs-punctuation">[</span>f<span class="hljs-string">"blocks.{l}.mlp.W_out"</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span>).contiguous()
    )

hf_model.push_to_hub(f<span class="hljs-string">"{MODEL_ID}-abliterated"</span>)
</code></pre>
<h1>⚖️ DPO Fine-Tuning</h1>
<p>이전 섹션의 abliterated 및 소스 모델을 Open LLM Leaderboard 및 Nous의 벤치마크 스위트에서 평가했습니다. 여기에 결과가 있습니다:</p>
<img src="/assets/img/2024-06-19-UncensoranyLLMwithabliteration_2.png">
<div class="content-ad"></div>
<p>보시다시피, 원본 모델은 Llama 3 8B Instruct보다 현저하게 우수한 성능을 보여줍니다. 그러나 우리는 모든 벤치마크에서 절삭된 버전에서 성능 하락을 관찰하고 있습니다. 절삭 과정은 성능을 향상시키면서도 모델의 품질을 저하시킨 것으로 나타났습니다.</p>
<p>이 문제를 해결하기 위해 우리는 우리의 절삭된 모델을 추가로 훈련하여 회복시키는 아이디어가 있습니다. 대부분의 세밀 조정된 모델들과 마찬가지로 Llama 3 8B Instruct은 지도 학습 세밀 조정에 있어서 꽤 취약합니다. 추가적인 SFT는 모델의 성능을 떨어뜨릴 가능성이 높습니다.</p>
<p>대체로, 선호 맞춤이 상당히 가볍고 우리의 절삭된 모델을 뇌개박하지 않아도 됩니다. DPO는 사용하기 쉽고 우수한 추적 레코드로 여기서 좋은 후보입니다. 이를 구현하기 위해 mlabonne/orpo-dpo-mix-40k 데이터셋을 사용하는 LazyAxolotl (Axolotl을 만들어 준 Wing Lian에게 감사드립니다)을 사용했습니다. 여기에 사용한 구성은 다음과 같습니다:</p>
<pre><code class="hljs language-js"><span class="hljs-attr">base_model</span>: mlabonne/<span class="hljs-title class_">Daredevil</span>-8B-abliterated
<span class="hljs-attr">model_type</span>: <span class="hljs-title class_">LlamaForCausalLM</span>
<span class="hljs-attr">tokenizer_type</span>: <span class="hljs-title class_">AutoTokenizer</span>

<span class="hljs-attr">load_in_8bit</span>: <span class="hljs-literal">false</span>
<span class="hljs-attr">load_in_4bit</span>: <span class="hljs-literal">true</span>
<span class="hljs-attr">strict</span>: <span class="hljs-literal">false</span>
<span class="hljs-attr">save_safetensors</span>: <span class="hljs-literal">true</span>

<span class="hljs-attr">rl</span>: dpo
<span class="hljs-attr">chat_template</span>: chatml
<span class="hljs-attr">datasets</span>:
  - <span class="hljs-attr">path</span>: mlabonne/orpo-dpo-mix-40k
    <span class="hljs-attr">split</span>: train
    <span class="hljs-attr">type</span>: chatml.<span class="hljs-property">intel</span>

<span class="hljs-attr">dataset_prepared_path</span>:
<span class="hljs-attr">val_set_size</span>: <span class="hljs-number">0.0</span>
<span class="hljs-attr">output_dir</span>: ./out

<span class="hljs-attr">adapter</span>: qlora
<span class="hljs-attr">lora_model_dir</span>:

<span class="hljs-attr">sequence_len</span>: <span class="hljs-number">2048</span>
<span class="hljs-attr">sample_packing</span>: <span class="hljs-literal">false</span>
<span class="hljs-attr">pad_to_sequence_len</span>: <span class="hljs-literal">false</span>

<span class="hljs-attr">lora_r</span>: <span class="hljs-number">64</span>
<span class="hljs-attr">lora_alpha</span>: <span class="hljs-number">32</span>
<span class="hljs-attr">lora_dropout</span>: <span class="hljs-number">0.05</span>
<span class="hljs-attr">lora_target_linear</span>: <span class="hljs-literal">true</span>
<span class="hljs-attr">lora_fan_in_fan_out</span>:

<span class="hljs-attr">wandb_project</span>: axolotl
<span class="hljs-attr">wandb_entity</span>:
<span class="hljs-attr">wandb_watch</span>:
<span class="hljs-attr">wandb_name</span>:
<span class="hljs-attr">wandb_log_model</span>:

<span class="hljs-attr">gradient_accumulation_steps</span>: <span class="hljs-number">8</span>
<span class="hljs-attr">micro_batch_size</span>: <span class="hljs-number">1</span>
<span class="hljs-attr">num_epochs</span>: <span class="hljs-number">1</span>
<span class="hljs-attr">optimizer</span>: paged_adamw_8bit
<span class="hljs-attr">lr_scheduler</span>: cosine
<span class="hljs-attr">learning_rate</span>: <span class="hljs-number">5e-6</span>
<span class="hljs-attr">train_on_inputs</span>: <span class="hljs-literal">false</span>
<span class="hljs-attr">group_by_length</span>: <span class="hljs-literal">false</span>
... (이어짐)
</code></pre>
<div class="content-ad"></div>
<p>6xA6000 GPU와 DeepSpeed ZeRO-2를 사용하여 모델을 훈련했어요. 훈련에는 약 6시간 45분이 소요되었답니다. W&#x26;B에서 얻은 훈련 곡선을 여기에 가져왔어요:</p>
<p>DPO를 세밀하게 조정한 mlabonne/NeuralDaredevil-8B-abliterated 모델이 자동으로 업로드되었어요. 저희가 앞서 지워버린 버전을 수정했는지 확인하기 위해 동일한 벤치마크에서 평가했어요:</p>
<p><img src="/assets/img/2024-06-19-UncensoranyLLMwithabliteration_3.png" alt="훈련 곡선"></p>
<p>이 추가 훈련을 통해 지워진 영향 대부분을 회복할 수 있었어요. 모델이 개선되지 않는 한 영역은 GSM8K, 수학 데이터 세트, 인데요, 이는 orpo-dpo-mix-40k가 더 많은 수학 샘플을 필요로 할 수 있다는 것을 의미할 수 있어요.</p>
<div class="content-ad"></div>
<p>최종 모델은 8B 범주에서 최첨단 성능을 자랑하는 미검열 LLM입니다. 필터링이 필요 없을 때는 Llama 3 8B Instruct의 개선된 버전으로 추천합니다. LM Studio에서 GGUF와 같은 양자화된 버전을 사용해 볼 수도 있습니다.</p>
<h1>결론</h1>
<p>이 글에서는 소명화(abliteration) 개념을 소개했습니다. 이 기술은 모델의 활성화를 해롭고 해를 끼치지 않는 프롬프트에 사용하여 거부 방향을 계산하고, 모델의 가중치를 수정하여 거부를 그만 내보낼 수 있도록 합니다. 이 기술은 안전 세밀조정의 취약성을 보여주며 윤리적 고려 사항을 던지고 있습니다.</p>
<p>우리는 Daredevil-8B에 소명화를 적용하여 필터링을 해제했지만, 이로 인해 모델의 성능이 저하되었습니다. 그 후 DPO를 사용하여 NeuralDaredevil-8B 모델을 생성하여 완전히 미검열이고 고품질의 8B LLM을 만들었습니다. 소명화는 정렬 제거에 국한되지 않으며, 다시 교육 없이 세밀 조정의 일종으로 간주해야 합니다. 실제로 MopeyMule의 FailSpy의 경우처럼 좌절적인 대화 스타일을 채택하는 것과 같이 창의적으로 다른 목표에도 적용될 수 있습니다.</p>
<div class="content-ad"></div>
<p>이 기사가 마음에 들었으면 좋겠어요. 더 많은 내용을 보고 싶다면 Hugging Face와 Twitter의 @maximelabonne를 팔로우해 주세요.</p>
<h1>참고 자료</h1>
<ul>
<li>FailSpy, “abliterator library,” GitHub, 2024.</li>
<li>Andy Arditi, Oscar Obeso, Aaquib111, wesg, Neel Nanda, “Refusal in LLMs is mediated by a single direction,” Lesswrong, 2024.</li>
</ul>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"억제된 모든 LLM을 해제하세요","description":"","date":"2024-06-19 03:36","slug":"2024-06-19-UncensoranyLLMwithabliteration","content":"\n\n## 재학습 없이 세밀 조정하기\n\n![이미지](/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png)\n\n람마 모델의 세대가 늘어날수록 새로운 기능이 제공되었습니다. 이는 사용자의 지시를 이해하고 따르는 능력이 뛰어난 '세세한 조정(세세하게 조정)' 버전을 제공합니다. 그러나 이러한 모델들은 매우 검열되어 있으며 해로운 요청으로 간주되는 것은 거부하고 \"AI 어시스턴트로서 도와드릴 수 없습니다.\"와 같은 대답을 합니다. 이 안전 기능은 오용을 방지하는 데 중요하지만, 모델의 유연성과 반응성을 제한합니다.\n\n본 문서에서는 \"무효화\"라는 기술을 탐구하여 재학습 없이 어떤 람마 모델이든 검열을 푸는 방법을 살펴볼 것입니다. 이 기술은 모델에 내장된 거부 메커니즘을 효과적으로 제거하여 모든 유형의 프롬프트에 대응할 수 있게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드는 Google Colab에서도 사용할 수 있고, LLM 코스에서도 GitHub에 있습니다. 이 기사를 교정해 주신 FailSpy님에게 특별히 감사드립니다.\n\n# ✂️ 삭제란이란?\n\n현대 LLM은 안전 및 지시를 따르는 방향으로 세밀하게 조정되어 있어, 해로운 요청을 거부하기 위해 훈련되어 있습니다. Arditi 등이 블로그 글에서 설명한 바에 따르면, 이 거부 행동은 모델의 잔류 스트림에 있는 특정 방향을 통해 중재됩니다. 만약 이 방향을 모델이 나타내지 못하도록 막는다면, 요청을 거부하는 능력을 잃게 됩니다. 반대로, 이 방향을 인위적으로 추가하면 모델이 해가 없는 요청조차도 거부할 수 있게됩니다.\n\n전통적인 디코더 전용 Llama류 아키텍처에서는 세 가지의 잔류 스트림을 대상으로 할 수 있습니다: 각 블록의 시작 부분에서(“pre”), 어텐션과 MLP 레이어 사이에서(“mid”), 그리고 MLP 이후에(“post”). 다음 그림은 각 잔류 스트림의 위치를 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_1.png\" /\u003e\n\nLLM을 무증검 상태로 만들기 위해 먼저 모델 내의 \"거부 방향\"을 식별해야 합니다. 이 과정에는 몇 가지 기술적 단계가 포함되어 있습니다:\n\n- 데이터 수집: 유해한 지시문 집합과 무해한 지시문 집합을 사용하여 모델을 실행하고, 각각의 마지막 토큰 위치에서 잔여 스트림 활성화를 기록합니다.\n- 평균 차이: 유해한 지시와 무해한 지시의 활성화 간 평균 차이를 계산합니다. 이를 통해 모델의 각 레이어에 대한 \"거부 방향\"을 나타내는 벡터를 얻을 수 있습니다.\n- 선택: 이러한 벡터를 정규화하고, 평가하여 단일 최상의 \"거부 방향\"을 선택합니다.\n\n거부 방향을 식별한 후, 해당 기능을 표현하는 모델의 능력을 효과적으로 제거하는 \"제거(ablate)\" 작업을 수행할 수 있습니다. 이는 추론 시간 간섭이나 가중치 직교화를 사용하여 영구적으로 수행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저 추론 시간 개입에 대해 이야기해 보겠습니다. 잔차 스트림에 기록하는 모든 구성 요소(예: 어텐션 헤드)마다 그 출력을 거부 방향으로 투영한 후 이 투영을 뺍니다. 이 뺄셈은 각 토큰과 각 레이어에 적용되어 모델이 결코 거부 방향을 표현하지 않도록 합니다.\n\n한편, 가중치 직교화는 모델 가중치를 직접 수정하는 것을 포함합니다. 거부 방향에 대해 구성 요소 가중치를 직교화함으로써 모델이 이 방향으로 기록하는 것을 방지합니다. 잔차 스트림에 기록하는 행렬을 조정하여 이러한 기여가 거부 방향에 영향을 주지 않도록 합니다.\n\n다음 섹션에서는 가중치 직교화를 사용한 좌절 실현을 구현할 것입니다.\n\n# 💻 구현\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래의 abliteration 구현은 FailSpy의 노트북을 기반으로 하고 있습니다. 그 노트북은 원래 저자들의 노트북을 기반으로 하고 있습니다. 저는 주로 이를 적응하여 간단하고 이해하기 쉽도록 했습니다. 이 섹션은 코드가 많이 포함되어 있어서 무슨 일이 벌어지는지 볼 수 있지만, 기술적인 세부 사항에 덜 관심이 있는 경우 FailSpy의 abliterator 라이브러리를 사용할 수도 있습니다 (Hugging Face의 abliterated 모델 모음도 확인해보세요).\n\n이 코드는 뛰어난 TransformerLens 라이브러리 (이전에는 EasyTransformer로 알려졌음)를 사용하여 무거운 작업을 처리합니다. 메커니즘 해석 가능성을 위해 설계되었으며 여기서는 활성화에 개입하는 데 사용됩니다. 이 라이브러리를 만든 Neel Nanda와 Joseph Bloom에게 감사드립니다.\n\n먼저 필요한 패키지를 설치하고 가져와 봅시다. 이러한 모든 단계는 Google Colab 노트북에서 사용할 수 있습니다.\n\n```js\n!pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping\n\nimport torch\nimport functools\nimport einops\nimport gc\n\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom torch import Tensor\nfrom typing import List\nfrom transformer_lens import HookedTransformer, utils\nfrom transformer_lens.hook_points import HookPoint\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom jaxtyping import Float, Int\nfrom collections import defaultdict\n\n# GPU 메모리를 저장하기 위해 자동 미분을 끕니다 (크레딧: Undi95)\ntorch.set_grad_enabled(False)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 가지 데이터 세트가 필요합니다: 피해가 없는 지침을 포함한 하나와 유해한 지침을 포함한 하나입니다. 우리는 tatsu-lab/alpaca와 llm-attacks의 데이터를 사용할 것입니다. 모든 것을 더 쉽게 만들기 위해, 저는 이를 두 개의 Hugging Face 데이터 세트로 다시 패키징하여 mlabonne/harmless_alpaca와 mlabonne/harmful_behaviors로 만들었습니다. 그렇게 하면 여러분이 쉽게 여러분 자신의 데이터 세트로 교체할 수 있습니다.\n\n지침을로드하고 \"role\"과 \"content\" 키가 있는 사전 목록으로 다시 서식화할 것입니다. 이렇게 하면 Llama 3의 채팅 템플릿을 따르는 apply_chat_tokenizer() 메서드와 호환됩니다.\n\n```python\ndef reformat_texts(texts):\n    return [[{\"role\": \"user\", \"content\": text}] for text in texts]\n\n# 유해하고 무해한 데이터 세트 가져오기\ndef get_harmful_instructions():\n    dataset = load_dataset('mlabonne/harmful_behaviors')\n    return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text'])\n\ndef get_harmless_instructions():\n    dataset = load_dataset('mlabonne/harmless_alpaca')\n    return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text'])\n\nharmful_inst_train, harmful_inst_test = get_harmful_instructions()\nharmless_inst_train, harmless_inst_test = get_harmless_instructions()\n```\n\n이제 데이터 세트가 준비되었으므로, abliterate 하려는 모델을 로드할 수 있습니다. 안타깝게도, HookedTransformer를 사용하여 직접 사용자 정의 모델을 로드할 수 없습니다. 여기에서, FailSpy의 노트북에 설명된 꼼수를 사용하여 사용자 정의 모델을 다운로드하고 meta-llama/Meta-Llama-3-8B-Instruct로 이름을 변경하겠습니다. GPU가 BF16과 호환되지 않는 경우 torch.float16 형식으로 로드하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 예시에서는 DARE TIES(모델 병합에 관한 내 기사 참조)로 만들어진 mlabonne/Daredevil-8B를 사용할 것입니다. 이 모델은 8B 카테고리의 Open LLM Leaderboard에서 가장 높은 MMLU 점수를 가지고 있어요.\n\n```js\nMODEL_ID = \"mlabonne/Daredevil-8B\"\nMODEL_TYPE = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# 모델 다운로드 및 로드\n!git clone https://huggingface.co/{MODEL_ID} {MODEL_TYPE}\n\n# 모델 및 토크나이저 로드\nmodel = HookedTransformer.from_pretrained_no_processing(\n    MODEL_TYPE,\n    local_files_only=True,\n    dtype=torch.bfloat16,\n    default_padding_side='left'\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)\ntokenizer.padding_side = 'left'\ntokenizer.pad_token = tokenizer.eos_token\n```\n\n이제 데이터셋을 토큰화할 수 있습니다. 무해한 및 유해한 지시사항에 대해 동일한 샘플 수를 사용합니다. 샘플 수가 높으면 모든 RAM/VRAM을 사용할 수 있으므로 여기서는 256으로 제한합니다.\n\n```js\ndef tokenize_instructions(tokenizer, instructions):\n    return tokenizer.apply_chat_template(\n        instructions,\n        padding=True,\n        truncation=False,\n        return_tensors=\"pt\",\n        return_dict=True,\n        add_generation_prompt=True,\n    ).input_ids\n\nn_inst_train = min(256, len(harmful_inst_train), len(harmless_inst_train))\n\n# 데이터셋 토큰화\nharmful_tokens = tokenize_instructions(\n    tokenizer,\n    instructions=harmful_inst_train[:n_inst_train],\n)\nharmless_tokens = tokenize_instructions(\n    tokenizer,\n    instructions=harmless_inst_train[:n_inst_train],\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n작업이 모두 설정되었습니다. 이제 도처럼 처리하는 첫 번째 단계인 데이터 수집을 구현할 차례입니다. 우리는 이 토큰화된 데이터셋을 처리하고 유해(harmful) 및 무해(harmless)로 나머지 스트림 활성화를 저장하려고 합니다. 이는 transformer_lens 라이브러리로 관리됩니다.\n\n```js\nbatch_size = 32\n\n# 활성화를 저장할 기본 사전 초기화\nharmful = defaultdict(list)\nharmless = defaultdict(list)\n\n# 데이터 학습을 배치 단위로 처리\nnum_batches = (n_inst_train + batch_size - 1) // batch_size\n\nfor i in tqdm(range(num_batches)):\n    print(i)\n    start_idx = i * batch_size\n    end_idx = min(n_inst_train, start_idx + batch_size)\n\n    # 유해 및 무해 프롬프트에 모델 실행 및 활성화 캐시\n    harmful_logits, harmful_cache = model.run_with_cache(\n        harmful_tokens[start_idx:end_idx],\n        names_filter=lambda hook_name: 'resid' in hook_name,\n        device='cpu',\n        reset_hooks_end=True\n    )\n    harmless_logits, harmless_cache = model.run_with_cache(\n        harmless_tokens[start_idx:end_idx],\n        names_filter=lambda hook_name: 'resid' in hook_name,\n        device='cpu',\n        reset_hooks_end=True\n    )\n\n    # 활성화 수집 및 저장\n    for key in harmful_cache:\n        harmful[key].append(harmful_cache[key])\n        harmless[key].append(harmless_cache[key])\n\n    # RAM 및 VRAM 비우기\n    del harmful_logits, harmless_logits, harmful_cache, harmless_cache\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# 캐시된 활성화 결합\nharmful = {k: torch.cat(v) for k, v in harmful.items()}\nharmless = {k: torch.cat(v) for k, v in harmless.items()}\n```\n\n이제 각 층에 대한 거부 방향을 계산할 수 있습니다. 이는 유해 및 무해 명령의 활성화 간 평균 차이에 해당하며 정규화됩니다. 그런 다음 activation_scored에서 내림차순으로 정렬됩니다.\n\n```js\n# 활성화 색인을 가져오는 도우미 함수\ndef get_act_idx(cache_dict, act_name, layer):\n    key = (act_name, layer)\n    return cache_dict[utils.get_act_name(*key)]\n\n# 중간 층에서 유해 및 무해 활성화의 평균 차이 계산\nactivation_layers = [\"resid_pre\", \"resid_mid\", \"resid_post\"]\nactivation_refusals = defaultdict(list)\n\nfor layer_num in range(1, model.cfg.n_layers):\n    pos = -1  # 위치 인덱스\n    for layer in activation_layers:\n        harmful_mean_act = get_act_idx(harmful, layer, layer_num)[:, pos, :].mean(dim=0)\n        harmless_mean_act = get_act_idx(harmless, layer, layer_num)[:, pos, :].mean(dim=0)\n        refusal_dir = harmful_mean_act - harmless_mean_act\n        refusal_dir = refusal_dir / refusal_dir.norm()\n        activation_refusals[layer].append(refusal_dir)\n\nselected_layers = [\"resid_pre\"]\nactivation_scored = sorted(\n    [\n        activation_refusals[layer][l - 1]\n        for l in range(1, model.cfg.n_layers)\n        for layer in selected_layers\n    ],\n    key=lambda x: abs(x.mean()),\n    reverse=True,\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n프로세스의 마지막 단계는 계산한 거절 방향을 평가하는 과정입니다. 이를 위해 우리는 거절 방향을 추론 중 각 잔여 스트림과 각 블록에 적용할 것입니다. 다음 코드 스니펫에서 네 가지 테스트 유해 지시와 20개 블록(또는 레이어)에 대한 세대를 가져옵니다.\n\n```js\n...\nCode snippet 생략\n...\n```\n\n우리는 evals 리스트에 모든 세대를 저장했습니다. 이제 이를 출력하고 각 지시에 대한 무겁지 않은 응답을 제공하는 레이어(블록)를 수동으로 선택할 수 있습니다. \"I cannot\" 및 \"I can't\"를 포함하는 응답은 원치 않는 답변을 필터링하기 위해 자동으로 제외합니다.\n\n위 요구사항을 만족하는 레이어를 찾을 수 없는 경우, 이전 selected_layers 목록의 다른 잔여 스트림, 다른 지시, 추가적 블록 등을 테스트하고자 할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\n# 인간 평가를 위한 세대 인쇄\nblacklist = [\"I cannot\", \"I can't\"]\nfor i in range(N_INST_TEST):\n    print(f\"\\033[1mINSTRUCTION {i}: {harmful_inst_test[i]}\")\n    print(f\"\\nBASELINE COMPLETION:\\n{baseline_generations[i]}\\033[0m\")\n    for layer_candidate in range(EVAL_N):\n        if not any(word in evals[layer_candidate][i] for word in blacklist):\n            print(f\"\\n---\\n\\nLAYER CANDIDATE #{layer_candidate} INTERVENTION COMPLETION:\")\n            print(evals[layer_candidate][i])\r\n```\n\n저의 경우, 레이어 후보자 9가 네 가지 명령에 대해 선정적이지 않은 답변을 제공했습니다. 이것이 우리가 거부 방향으로 선택할 것이다. 그 다음으로, 무게 직교화를 구현하여 모델이 이 방향의 출력을 생성하는 것을 방지합니다. 모델이 성공적으로 선정되지 않은지를 확인하려면 완료된 내용을 인쇄하여 확인할 수 있습니다.\n\n```js\r\ndef get_orthogonalized_matrix(\n    matrix: Float[Tensor, \"... d_model\"], vec: Float[Tensor, \"d_model\"]\n) -\u003e Float[Tensor, \"... d_model\"]:\n    proj = (\n        einops.einsum(\n            matrix, vec.view(-1, 1), \"... d_model, d_model single -\u003e ... single\"\n        )\n        * vec\n    )\n    return matrix - proj\n\n# 가장 높은 거부 방향을 갖는 레이어 선택\nLAYER_CANDIDATE = 9\nrefusal_dir = activation_scored[LAYER_CANDIDATE]\n\n# 모델의 가중치 직교화\nif refusal_dir.device != model.W_E.device:\n    refusal_dir = refusal_dir.to(model.W_E.device)\nmodel.W_E.data = get_orthogonalized_matrix(model.W_E, refusal_dir)\n\nfor block in tqdm(model.blocks):\n    if refusal_dir.device != block.attn.W_O.device:\n        refusal_dir = refusal_dir.to(block.attn.W_O.device)\n    block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, refusal_dir)\n    block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, refusal_dir)\n\n# 모델로 텍스트 생성\northogonalized_generations = get_generations(\n    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]\n)\n\n# 세대 출력\nfor i in range(N_INST_TEST):\n    if len(baseline_generations) \u003e i:\n        print(f\"INSTRUCTION {i}: {harmful_inst_test[i]}\")\n        print(f\"\\033[92mBASELINE COMPLETION:\\n{baseline_generations[i]}\")\n    print(f\"\\033[91mINTERVENTION COMPLETION:\\n{evals[LAYER_CANDIDATE][i]}\")\n    print(f\"\\033[95mORTHOGONALIZED COMPLETION:\\n{orthogonalized_generations[i]}\\n\")\r\n```\n\n이제 모델을 사용할 준비가 되었습니다. 모델을 Hugging Face 형식으로 변환하여 HF 허브에 업로드합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```json\n# 모델을 다시 HF 보안 텐서로 변환합니다\nhf_model = AutoModelForCausalLM.from_pretrained(MODEL_TYPE, torch_dtype=torch.bfloat16)\nlm_model = hf_model.model\n\nstate_dict = model.state_dict()\nlm_model.embed_tokens.weight = torch.nn.Parameter(state_dict[\"embed.W_E\"].cpu())\nfor l in range(model.cfg.n_layers):\n    lm_model.layers[l].self_attn.o_proj.weight = torch.nn.Parameter(\n        einops.rearrange(\n            state_dict[f\"blocks.{l}.attn.W_O\"], \"n h m-\u003em (n h)\", n=model.cfg.n_heads\n        ).contiguous()\n    )\n    lm_model.layers[l].mlp.down_proj.weight = torch.nn.Parameter(\n        torch.transpose(state_dict[f\"blocks.{l}.mlp.W_out\"], 0, 1).contiguous()\n    )\n\nhf_model.push_to_hub(f\"{MODEL_ID}-abliterated\")\n```\n\n# ⚖️ DPO Fine-Tuning\n\n이전 섹션의 abliterated 및 소스 모델을 Open LLM Leaderboard 및 Nous의 벤치마크 스위트에서 평가했습니다. 여기에 결과가 있습니다:\n\n\u003cimg src=\"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_2.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n보시다시피, 원본 모델은 Llama 3 8B Instruct보다 현저하게 우수한 성능을 보여줍니다. 그러나 우리는 모든 벤치마크에서 절삭된 버전에서 성능 하락을 관찰하고 있습니다. 절삭 과정은 성능을 향상시키면서도 모델의 품질을 저하시킨 것으로 나타났습니다.\n\n이 문제를 해결하기 위해 우리는 우리의 절삭된 모델을 추가로 훈련하여 회복시키는 아이디어가 있습니다. 대부분의 세밀 조정된 모델들과 마찬가지로 Llama 3 8B Instruct은 지도 학습 세밀 조정에 있어서 꽤 취약합니다. 추가적인 SFT는 모델의 성능을 떨어뜨릴 가능성이 높습니다.\n\n대체로, 선호 맞춤이 상당히 가볍고 우리의 절삭된 모델을 뇌개박하지 않아도 됩니다. DPO는 사용하기 쉽고 우수한 추적 레코드로 여기서 좋은 후보입니다. 이를 구현하기 위해 mlabonne/orpo-dpo-mix-40k 데이터셋을 사용하는 LazyAxolotl (Axolotl을 만들어 준 Wing Lian에게 감사드립니다)을 사용했습니다. 여기에 사용한 구성은 다음과 같습니다:\n\n```js\nbase_model: mlabonne/Daredevil-8B-abliterated\nmodel_type: LlamaForCausalLM\ntokenizer_type: AutoTokenizer\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\nsave_safetensors: true\n\nrl: dpo\nchat_template: chatml\ndatasets:\n  - path: mlabonne/orpo-dpo-mix-40k\n    split: train\n    type: chatml.intel\n\ndataset_prepared_path:\nval_set_size: 0.0\noutput_dir: ./out\n\nadapter: qlora\nlora_model_dir:\n\nsequence_len: 2048\nsample_packing: false\npad_to_sequence_len: false\n\nlora_r: 64\nlora_alpha: 32\nlora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project: axolotl\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n\ngradient_accumulation_steps: 8\nmicro_batch_size: 1\nnum_epochs: 1\noptimizer: paged_adamw_8bit\nlr_scheduler: cosine\nlearning_rate: 5e-6\ntrain_on_inputs: false\ngroup_by_length: false\n... (이어짐)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n6xA6000 GPU와 DeepSpeed ZeRO-2를 사용하여 모델을 훈련했어요. 훈련에는 약 6시간 45분이 소요되었답니다. W\u0026B에서 얻은 훈련 곡선을 여기에 가져왔어요:\n\nDPO를 세밀하게 조정한 mlabonne/NeuralDaredevil-8B-abliterated 모델이 자동으로 업로드되었어요. 저희가 앞서 지워버린 버전을 수정했는지 확인하기 위해 동일한 벤치마크에서 평가했어요:\n\n![훈련 곡선](/assets/img/2024-06-19-UncensoranyLLMwithabliteration_3.png)\n\n이 추가 훈련을 통해 지워진 영향 대부분을 회복할 수 있었어요. 모델이 개선되지 않는 한 영역은 GSM8K, 수학 데이터 세트, 인데요, 이는 orpo-dpo-mix-40k가 더 많은 수학 샘플을 필요로 할 수 있다는 것을 의미할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최종 모델은 8B 범주에서 최첨단 성능을 자랑하는 미검열 LLM입니다. 필터링이 필요 없을 때는 Llama 3 8B Instruct의 개선된 버전으로 추천합니다. LM Studio에서 GGUF와 같은 양자화된 버전을 사용해 볼 수도 있습니다.\n\n# 결론\n\n이 글에서는 소명화(abliteration) 개념을 소개했습니다. 이 기술은 모델의 활성화를 해롭고 해를 끼치지 않는 프롬프트에 사용하여 거부 방향을 계산하고, 모델의 가중치를 수정하여 거부를 그만 내보낼 수 있도록 합니다. 이 기술은 안전 세밀조정의 취약성을 보여주며 윤리적 고려 사항을 던지고 있습니다.\n\n우리는 Daredevil-8B에 소명화를 적용하여 필터링을 해제했지만, 이로 인해 모델의 성능이 저하되었습니다. 그 후 DPO를 사용하여 NeuralDaredevil-8B 모델을 생성하여 완전히 미검열이고 고품질의 8B LLM을 만들었습니다. 소명화는 정렬 제거에 국한되지 않으며, 다시 교육 없이 세밀 조정의 일종으로 간주해야 합니다. 실제로 MopeyMule의 FailSpy의 경우처럼 좌절적인 대화 스타일을 채택하는 것과 같이 창의적으로 다른 목표에도 적용될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사가 마음에 들었으면 좋겠어요. 더 많은 내용을 보고 싶다면 Hugging Face와 Twitter의 @maximelabonne를 팔로우해 주세요.\n\n# 참고 자료\n\n- FailSpy, “abliterator library,” GitHub, 2024.\n- Andy Arditi, Oscar Obeso, Aaquib111, wesg, Neel Nanda, “Refusal in LLMs is mediated by a single direction,” Lesswrong, 2024.","ogImage":{"url":"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png"},"coverImage":"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png","tag":["Tech"],"readingTime":15},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e재학습 없이 세밀 조정하기\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e람마 모델의 세대가 늘어날수록 새로운 기능이 제공되었습니다. 이는 사용자의 지시를 이해하고 따르는 능력이 뛰어난 '세세한 조정(세세하게 조정)' 버전을 제공합니다. 그러나 이러한 모델들은 매우 검열되어 있으며 해로운 요청으로 간주되는 것은 거부하고 \"AI 어시스턴트로서 도와드릴 수 없습니다.\"와 같은 대답을 합니다. 이 안전 기능은 오용을 방지하는 데 중요하지만, 모델의 유연성과 반응성을 제한합니다.\u003c/p\u003e\n\u003cp\u003e본 문서에서는 \"무효화\"라는 기술을 탐구하여 재학습 없이 어떤 람마 모델이든 검열을 푸는 방법을 살펴볼 것입니다. 이 기술은 모델에 내장된 거부 메커니즘을 효과적으로 제거하여 모든 유형의 프롬프트에 대응할 수 있게 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e코드는 Google Colab에서도 사용할 수 있고, LLM 코스에서도 GitHub에 있습니다. 이 기사를 교정해 주신 FailSpy님에게 특별히 감사드립니다.\u003c/p\u003e\n\u003ch1\u003e✂️ 삭제란이란?\u003c/h1\u003e\n\u003cp\u003e현대 LLM은 안전 및 지시를 따르는 방향으로 세밀하게 조정되어 있어, 해로운 요청을 거부하기 위해 훈련되어 있습니다. Arditi 등이 블로그 글에서 설명한 바에 따르면, 이 거부 행동은 모델의 잔류 스트림에 있는 특정 방향을 통해 중재됩니다. 만약 이 방향을 모델이 나타내지 못하도록 막는다면, 요청을 거부하는 능력을 잃게 됩니다. 반대로, 이 방향을 인위적으로 추가하면 모델이 해가 없는 요청조차도 거부할 수 있게됩니다.\u003c/p\u003e\n\u003cp\u003e전통적인 디코더 전용 Llama류 아키텍처에서는 세 가지의 잔류 스트림을 대상으로 할 수 있습니다: 각 블록의 시작 부분에서(“pre”), 어텐션과 MLP 레이어 사이에서(“mid”), 그리고 MLP 이후에(“post”). 다음 그림은 각 잔류 스트림의 위치를 보여줍니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_1.png\"\u003e\n\u003cp\u003eLLM을 무증검 상태로 만들기 위해 먼저 모델 내의 \"거부 방향\"을 식별해야 합니다. 이 과정에는 몇 가지 기술적 단계가 포함되어 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e데이터 수집: 유해한 지시문 집합과 무해한 지시문 집합을 사용하여 모델을 실행하고, 각각의 마지막 토큰 위치에서 잔여 스트림 활성화를 기록합니다.\u003c/li\u003e\n\u003cli\u003e평균 차이: 유해한 지시와 무해한 지시의 활성화 간 평균 차이를 계산합니다. 이를 통해 모델의 각 레이어에 대한 \"거부 방향\"을 나타내는 벡터를 얻을 수 있습니다.\u003c/li\u003e\n\u003cli\u003e선택: 이러한 벡터를 정규화하고, 평가하여 단일 최상의 \"거부 방향\"을 선택합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e거부 방향을 식별한 후, 해당 기능을 표현하는 모델의 능력을 효과적으로 제거하는 \"제거(ablate)\" 작업을 수행할 수 있습니다. 이는 추론 시간 간섭이나 가중치 직교화를 사용하여 영구적으로 수행할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e먼저 추론 시간 개입에 대해 이야기해 보겠습니다. 잔차 스트림에 기록하는 모든 구성 요소(예: 어텐션 헤드)마다 그 출력을 거부 방향으로 투영한 후 이 투영을 뺍니다. 이 뺄셈은 각 토큰과 각 레이어에 적용되어 모델이 결코 거부 방향을 표현하지 않도록 합니다.\u003c/p\u003e\n\u003cp\u003e한편, 가중치 직교화는 모델 가중치를 직접 수정하는 것을 포함합니다. 거부 방향에 대해 구성 요소 가중치를 직교화함으로써 모델이 이 방향으로 기록하는 것을 방지합니다. 잔차 스트림에 기록하는 행렬을 조정하여 이러한 기여가 거부 방향에 영향을 주지 않도록 합니다.\u003c/p\u003e\n\u003cp\u003e다음 섹션에서는 가중치 직교화를 사용한 좌절 실현을 구현할 것입니다.\u003c/p\u003e\n\u003ch1\u003e💻 구현\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래의 abliteration 구현은 FailSpy의 노트북을 기반으로 하고 있습니다. 그 노트북은 원래 저자들의 노트북을 기반으로 하고 있습니다. 저는 주로 이를 적응하여 간단하고 이해하기 쉽도록 했습니다. 이 섹션은 코드가 많이 포함되어 있어서 무슨 일이 벌어지는지 볼 수 있지만, 기술적인 세부 사항에 덜 관심이 있는 경우 FailSpy의 abliterator 라이브러리를 사용할 수도 있습니다 (Hugging Face의 abliterated 모델 모음도 확인해보세요).\u003c/p\u003e\n\u003cp\u003e이 코드는 뛰어난 TransformerLens 라이브러리 (이전에는 EasyTransformer로 알려졌음)를 사용하여 무거운 작업을 처리합니다. 메커니즘 해석 가능성을 위해 설계되었으며 여기서는 활성화에 개입하는 데 사용됩니다. 이 라이브러리를 만든 Neel Nanda와 Joseph Bloom에게 감사드립니다.\u003c/p\u003e\n\u003cp\u003e먼저 필요한 패키지를 설치하고 가져와 봅시다. 이러한 모든 단계는 Google Colab 노트북에서 사용할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e!pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e functools\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e einops\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e gc\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_dataset\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e tqdm \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tqdm\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e torch \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e typing \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eList\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformer_lens \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eHookedTransformer\u003c/span\u003e, utils\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformer_lens.\u003cspan class=\"hljs-property\"\u003ehook_points\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eHookPoint\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAutoModelForCausalLM\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eAutoTokenizer\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e jaxtyping \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eFloat\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eInt\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e collections \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e defaultdict\n\n# \u003cspan class=\"hljs-variable constant_\"\u003eGPU\u003c/span\u003e 메모리를 저장하기 위해 자동 미분을 끕니다 (크레딧: \u003cspan class=\"hljs-title class_\"\u003eUndi95\u003c/span\u003e)\ntorch.\u003cspan class=\"hljs-title function_\"\u003eset_grad_enabled\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e두 가지 데이터 세트가 필요합니다: 피해가 없는 지침을 포함한 하나와 유해한 지침을 포함한 하나입니다. 우리는 tatsu-lab/alpaca와 llm-attacks의 데이터를 사용할 것입니다. 모든 것을 더 쉽게 만들기 위해, 저는 이를 두 개의 Hugging Face 데이터 세트로 다시 패키징하여 mlabonne/harmless_alpaca와 mlabonne/harmful_behaviors로 만들었습니다. 그렇게 하면 여러분이 쉽게 여러분 자신의 데이터 세트로 교체할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e지침을로드하고 \"role\"과 \"content\" 키가 있는 사전 목록으로 다시 서식화할 것입니다. 이렇게 하면 Llama 3의 채팅 템플릿을 따르는 apply_chat_tokenizer() 메서드와 호환됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ereformat_texts\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003etexts\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e [[{\u003cspan class=\"hljs-string\"\u003e\"role\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"user\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"content\"\u003c/span\u003e: text}] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e text \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e texts]\n\n\u003cspan class=\"hljs-comment\"\u003e# 유해하고 무해한 데이터 세트 가져오기\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eget_harmful_instructions\u003c/span\u003e():\n    dataset = load_dataset(\u003cspan class=\"hljs-string\"\u003e'mlabonne/harmful_behaviors'\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e reformat_texts(dataset[\u003cspan class=\"hljs-string\"\u003e'train'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'text'\u003c/span\u003e]), reformat_texts(dataset[\u003cspan class=\"hljs-string\"\u003e'test'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'text'\u003c/span\u003e])\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eget_harmless_instructions\u003c/span\u003e():\n    dataset = load_dataset(\u003cspan class=\"hljs-string\"\u003e'mlabonne/harmless_alpaca'\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e reformat_texts(dataset[\u003cspan class=\"hljs-string\"\u003e'train'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'text'\u003c/span\u003e]), reformat_texts(dataset[\u003cspan class=\"hljs-string\"\u003e'test'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'text'\u003c/span\u003e])\n\nharmful_inst_train, harmful_inst_test = get_harmful_instructions()\nharmless_inst_train, harmless_inst_test = get_harmless_instructions()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 데이터 세트가 준비되었으므로, abliterate 하려는 모델을 로드할 수 있습니다. 안타깝게도, HookedTransformer를 사용하여 직접 사용자 정의 모델을 로드할 수 없습니다. 여기에서, FailSpy의 노트북에 설명된 꼼수를 사용하여 사용자 정의 모델을 다운로드하고 meta-llama/Meta-Llama-3-8B-Instruct로 이름을 변경하겠습니다. GPU가 BF16과 호환되지 않는 경우 torch.float16 형식으로 로드하세요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 예시에서는 DARE TIES(모델 병합에 관한 내 기사 참조)로 만들어진 mlabonne/Daredevil-8B를 사용할 것입니다. 이 모델은 8B 카테고리의 Open LLM Leaderboard에서 가장 높은 MMLU 점수를 가지고 있어요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-variable constant_\"\u003eMODEL_ID\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e\"mlabonne/Daredevil-8B\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eMODEL_TYPE\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e\"meta-llama/Meta-Llama-3-8B-Instruct\"\u003c/span\u003e\n\n# 모델 다운로드 및 로드\n!git clone \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//huggingface.co/{MODEL_ID} {MODEL_TYPE}\u003c/span\u003e\n\n# 모델 및 토크나이저 로드\nmodel = \u003cspan class=\"hljs-title class_\"\u003eHookedTransformer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained_no_processing\u003c/span\u003e(\n    \u003cspan class=\"hljs-variable constant_\"\u003eMODEL_TYPE\u003c/span\u003e,\n    local_files_only=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n    dtype=torch.\u003cspan class=\"hljs-property\"\u003ebfloat16\u003c/span\u003e,\n    default_padding_side=\u003cspan class=\"hljs-string\"\u003e'left'\u003c/span\u003e\n)\ntokenizer = \u003cspan class=\"hljs-title class_\"\u003eAutoTokenizer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(\u003cspan class=\"hljs-variable constant_\"\u003eMODEL_TYPE\u003c/span\u003e)\ntokenizer.\u003cspan class=\"hljs-property\"\u003epadding_side\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e'left'\u003c/span\u003e\ntokenizer.\u003cspan class=\"hljs-property\"\u003epad_token\u003c/span\u003e = tokenizer.\u003cspan class=\"hljs-property\"\u003eeos_token\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 데이터셋을 토큰화할 수 있습니다. 무해한 및 유해한 지시사항에 대해 동일한 샘플 수를 사용합니다. 샘플 수가 높으면 모든 RAM/VRAM을 사용할 수 있으므로 여기서는 256으로 제한합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003etokenize_instructions\u003c/span\u003e(tokenizer, instructions):\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e tokenizer.\u003cspan class=\"hljs-title function_\"\u003eapply_chat_template\u003c/span\u003e(\n        instructions,\n        padding=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n        truncation=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e,\n        return_tensors=\u003cspan class=\"hljs-string\"\u003e\"pt\"\u003c/span\u003e,\n        return_dict=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n        add_generation_prompt=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n    ).\u003cspan class=\"hljs-property\"\u003einput_ids\u003c/span\u003e\n\nn_inst_train = \u003cspan class=\"hljs-title function_\"\u003emin\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e, \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(harmful_inst_train), \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(harmless_inst_train))\n\n# 데이터셋 토큰화\nharmful_tokens = \u003cspan class=\"hljs-title function_\"\u003etokenize_instructions\u003c/span\u003e(\n    tokenizer,\n    instructions=harmful_inst_train[:n_inst_train],\n)\nharmless_tokens = \u003cspan class=\"hljs-title function_\"\u003etokenize_instructions\u003c/span\u003e(\n    tokenizer,\n    instructions=harmless_inst_train[:n_inst_train],\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e작업이 모두 설정되었습니다. 이제 도처럼 처리하는 첫 번째 단계인 데이터 수집을 구현할 차례입니다. 우리는 이 토큰화된 데이터셋을 처리하고 유해(harmful) 및 무해(harmless)로 나머지 스트림 활성화를 저장하려고 합니다. 이는 transformer_lens 라이브러리로 관리됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ebatch_size = \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e\n\n# 활성화를 저장할 기본 사전 초기화\nharmful = \u003cspan class=\"hljs-title function_\"\u003edefaultdict\u003c/span\u003e(list)\nharmless = \u003cspan class=\"hljs-title function_\"\u003edefaultdict\u003c/span\u003e(list)\n\n# 데이터 학습을 배치 단위로 처리\nnum_batches = (n_inst_train + batch_size - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) \u003cspan class=\"hljs-comment\"\u003e// batch_size\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003etqdm\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(num_batches)):\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(i)\n    start_idx = i * batch_size\n    end_idx = \u003cspan class=\"hljs-title function_\"\u003emin\u003c/span\u003e(n_inst_train, start_idx + batch_size)\n\n    # 유해 및 무해 프롬프트에 모델 실행 및 활성화 캐시\n    harmful_logits, harmful_cache = model.\u003cspan class=\"hljs-title function_\"\u003erun_with_cache\u003c/span\u003e(\n        harmful_tokens[\u003cspan class=\"hljs-attr\"\u003estart_idx\u003c/span\u003e:end_idx],\n        names_filter=lambda \u003cspan class=\"hljs-attr\"\u003ehook_name\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'resid'\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e hook_name,\n        device=\u003cspan class=\"hljs-string\"\u003e'cpu'\u003c/span\u003e,\n        reset_hooks_end=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e\n    )\n    harmless_logits, harmless_cache = model.\u003cspan class=\"hljs-title function_\"\u003erun_with_cache\u003c/span\u003e(\n        harmless_tokens[\u003cspan class=\"hljs-attr\"\u003estart_idx\u003c/span\u003e:end_idx],\n        names_filter=lambda \u003cspan class=\"hljs-attr\"\u003ehook_name\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'resid'\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e hook_name,\n        device=\u003cspan class=\"hljs-string\"\u003e'cpu'\u003c/span\u003e,\n        reset_hooks_end=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e\n    )\n\n    # 활성화 수집 및 저장\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e key \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eharmful_cache\u003c/span\u003e:\n        harmful[key].\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(harmful_cache[key])\n        harmless[key].\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(harmless_cache[key])\n\n    # \u003cspan class=\"hljs-variable constant_\"\u003eRAM\u003c/span\u003e 및 \u003cspan class=\"hljs-variable constant_\"\u003eVRAM\u003c/span\u003e 비우기\n    del harmful_logits, harmless_logits, harmful_cache, harmless_cache\n    gc.\u003cspan class=\"hljs-title function_\"\u003ecollect\u003c/span\u003e()\n    torch.\u003cspan class=\"hljs-property\"\u003ecuda\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eempty_cache\u003c/span\u003e()\n\n# 캐시된 활성화 결합\nharmful = {\u003cspan class=\"hljs-attr\"\u003ek\u003c/span\u003e: torch.\u003cspan class=\"hljs-title function_\"\u003ecat\u003c/span\u003e(v) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e k, v \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e harmful.\u003cspan class=\"hljs-title function_\"\u003eitems\u003c/span\u003e()}\nharmless = {\u003cspan class=\"hljs-attr\"\u003ek\u003c/span\u003e: torch.\u003cspan class=\"hljs-title function_\"\u003ecat\u003c/span\u003e(v) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e k, v \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e harmless.\u003cspan class=\"hljs-title function_\"\u003eitems\u003c/span\u003e()}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 각 층에 대한 거부 방향을 계산할 수 있습니다. 이는 유해 및 무해 명령의 활성화 간 평균 차이에 해당하며 정규화됩니다. 그런 다음 activation_scored에서 내림차순으로 정렬됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 활성화 색인을 가져오는 도우미 함수\ndef \u003cspan class=\"hljs-title function_\"\u003eget_act_idx\u003c/span\u003e(cache_dict, act_name, layer):\n    key = (act_name, layer)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e cache_dict[utils.\u003cspan class=\"hljs-title function_\"\u003eget_act_name\u003c/span\u003e(*key)]\n\n# 중간 층에서 유해 및 무해 활성화의 평균 차이 계산\nactivation_layers = [\u003cspan class=\"hljs-string\"\u003e\"resid_pre\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"resid_mid\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"resid_post\"\u003c/span\u003e]\nactivation_refusals = \u003cspan class=\"hljs-title function_\"\u003edefaultdict\u003c/span\u003e(list)\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e layer_num \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, model.\u003cspan class=\"hljs-property\"\u003ecfg\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003en_layers\u003c/span\u003e):\n    pos = -\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e  # 위치 인덱스\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e layer \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eactivation_layers\u003c/span\u003e:\n        harmful_mean_act = \u003cspan class=\"hljs-title function_\"\u003eget_act_idx\u003c/span\u003e(harmful, layer, layer_num)[:, pos, :].\u003cspan class=\"hljs-title function_\"\u003emean\u003c/span\u003e(dim=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n        harmless_mean_act = \u003cspan class=\"hljs-title function_\"\u003eget_act_idx\u003c/span\u003e(harmless, layer, layer_num)[:, pos, :].\u003cspan class=\"hljs-title function_\"\u003emean\u003c/span\u003e(dim=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n        refusal_dir = harmful_mean_act - harmless_mean_act\n        refusal_dir = refusal_dir / refusal_dir.\u003cspan class=\"hljs-title function_\"\u003enorm\u003c/span\u003e()\n        activation_refusals[layer].\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(refusal_dir)\n\nselected_layers = [\u003cspan class=\"hljs-string\"\u003e\"resid_pre\"\u003c/span\u003e]\nactivation_scored = \u003cspan class=\"hljs-title function_\"\u003esorted\u003c/span\u003e(\n    [\n        activation_refusals[layer][l - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e l \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, model.\u003cspan class=\"hljs-property\"\u003ecfg\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003en_layers\u003c/span\u003e)\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e layer \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e selected_layers\n    ],\n    key=lambda \u003cspan class=\"hljs-attr\"\u003ex\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003eabs\u003c/span\u003e(x.\u003cspan class=\"hljs-title function_\"\u003emean\u003c/span\u003e()),\n    reverse=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e프로세스의 마지막 단계는 계산한 거절 방향을 평가하는 과정입니다. 이를 위해 우리는 거절 방향을 추론 중 각 잔여 스트림과 각 블록에 적용할 것입니다. 다음 코드 스니펫에서 네 가지 테스트 유해 지시와 20개 블록(또는 레이어)에 대한 세대를 가져옵니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e...\n\u003cspan class=\"hljs-title class_\"\u003eCode\u003c/span\u003e snippet 생략\n...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e우리는 evals 리스트에 모든 세대를 저장했습니다. 이제 이를 출력하고 각 지시에 대한 무겁지 않은 응답을 제공하는 레이어(블록)를 수동으로 선택할 수 있습니다. \"I cannot\" 및 \"I can't\"를 포함하는 응답은 원치 않는 답변을 필터링하기 위해 자동으로 제외합니다.\u003c/p\u003e\n\u003cp\u003e위 요구사항을 만족하는 레이어를 찾을 수 없는 경우, 이전 selected_layers 목록의 다른 잔여 스트림, 다른 지시, 추가적 블록 등을 테스트하고자 할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 인간 평가를 위한 세대 인쇄\nblacklist = [\u003cspan class=\"hljs-string\"\u003e\"I cannot\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"I can't\"\u003c/span\u003e]\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-variable constant_\"\u003eN_INST_TEST\u003c/span\u003e):\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"\\033[1mINSTRUCTION {i}: {harmful_inst_test[i]}\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"\\nBASELINE COMPLETION:\\n{baseline_generations[i]}\\033[0m\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e layer_candidate \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-variable constant_\"\u003eEVAL_N\u003c/span\u003e):\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e not \u003cspan class=\"hljs-title function_\"\u003eany\u003c/span\u003e(word \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e evals[layer_candidate][i] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e word \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e blacklist):\n            \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"\\n---\\n\\nLAYER CANDIDATE #{layer_candidate} INTERVENTION COMPLETION:\"\u003c/span\u003e)\n            \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(evals[layer_candidate][i])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e저의 경우, 레이어 후보자 9가 네 가지 명령에 대해 선정적이지 않은 답변을 제공했습니다. 이것이 우리가 거부 방향으로 선택할 것이다. 그 다음으로, 무게 직교화를 구현하여 모델이 이 방향의 출력을 생성하는 것을 방지합니다. 모델이 성공적으로 선정되지 않은지를 확인하려면 완료된 내용을 인쇄하여 확인할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eget_orthogonalized_matrix\u003c/span\u003e(\n    \u003cspan class=\"hljs-attr\"\u003ematrix\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eFloat\u003c/span\u003e[\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"... d_model\"\u003c/span\u003e], \u003cspan class=\"hljs-attr\"\u003evec\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eFloat\u003c/span\u003e[\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"d_model\"\u003c/span\u003e]\n) -\u003e \u003cspan class=\"hljs-title class_\"\u003eFloat\u003c/span\u003e[\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"... d_model\"\u003c/span\u003e]:\n    proj = (\n        einops.\u003cspan class=\"hljs-title function_\"\u003eeinsum\u003c/span\u003e(\n            matrix, vec.\u003cspan class=\"hljs-title function_\"\u003eview\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e), \u003cspan class=\"hljs-string\"\u003e\"... d_model, d_model single -\u003e ... single\"\u003c/span\u003e\n        )\n        * vec\n    )\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e matrix - proj\n\n# 가장 높은 거부 방향을 갖는 레이어 선택\n\u003cspan class=\"hljs-variable constant_\"\u003eLAYER_CANDIDATE\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e\nrefusal_dir = activation_scored[\u003cspan class=\"hljs-variable constant_\"\u003eLAYER_CANDIDATE\u003c/span\u003e]\n\n# 모델의 가중치 직교화\n\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e refusal_dir.\u003cspan class=\"hljs-property\"\u003edevice\u003c/span\u003e != model.\u003cspan class=\"hljs-property\"\u003eW_E\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003edevice\u003c/span\u003e:\n    refusal_dir = refusal_dir.\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(model.\u003cspan class=\"hljs-property\"\u003eW_E\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003edevice\u003c/span\u003e)\nmodel.\u003cspan class=\"hljs-property\"\u003eW_E\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003edata\u003c/span\u003e = \u003cspan class=\"hljs-title function_\"\u003eget_orthogonalized_matrix\u003c/span\u003e(model.\u003cspan class=\"hljs-property\"\u003eW_E\u003c/span\u003e, refusal_dir)\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e block \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003etqdm\u003c/span\u003e(model.\u003cspan class=\"hljs-property\"\u003eblocks\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e refusal_dir.\u003cspan class=\"hljs-property\"\u003edevice\u003c/span\u003e != block.\u003cspan class=\"hljs-property\"\u003eattn\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eW_O\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003edevice\u003c/span\u003e:\n        refusal_dir = refusal_dir.\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(block.\u003cspan class=\"hljs-property\"\u003eattn\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eW_O\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003edevice\u003c/span\u003e)\n    block.\u003cspan class=\"hljs-property\"\u003eattn\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eW_O\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003edata\u003c/span\u003e = \u003cspan class=\"hljs-title function_\"\u003eget_orthogonalized_matrix\u003c/span\u003e(block.\u003cspan class=\"hljs-property\"\u003eattn\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eW_O\u003c/span\u003e, refusal_dir)\n    block.\u003cspan class=\"hljs-property\"\u003emlp\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eW_out\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003edata\u003c/span\u003e = \u003cspan class=\"hljs-title function_\"\u003eget_orthogonalized_matrix\u003c/span\u003e(block.\u003cspan class=\"hljs-property\"\u003emlp\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eW_out\u003c/span\u003e, refusal_dir)\n\n# 모델로 텍스트 생성\northogonalized_generations = \u003cspan class=\"hljs-title function_\"\u003eget_generations\u003c/span\u003e(\n    model, tokenizer, harmful_inst_test[:\u003cspan class=\"hljs-variable constant_\"\u003eN_INST_TEST\u003c/span\u003e], fwd_hooks=[]\n)\n\n# 세대 출력\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-variable constant_\"\u003eN_INST_TEST\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(baseline_generations) \u003e \u003cspan class=\"hljs-attr\"\u003ei\u003c/span\u003e:\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"INSTRUCTION {i}: {harmful_inst_test[i]}\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"\\033[92mBASELINE COMPLETION:\\n{baseline_generations[i]}\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"\\033[91mINTERVENTION COMPLETION:\\n{evals[LAYER_CANDIDATE][i]}\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"\\033[95mORTHOGONALIZED COMPLETION:\\n{orthogonalized_generations[i]}\\n\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 모델을 사용할 준비가 되었습니다. 모델을 Hugging Face 형식으로 변환하여 HF 허브에 업로드합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-json\"\u003e# 모델을 다시 HF 보안 텐서로 변환합니다\nhf_model = AutoModelForCausalLM.from_pretrained(MODEL_TYPE\u003cspan class=\"hljs-punctuation\"\u003e,\u003c/span\u003e torch_dtype=torch.bfloat16)\nlm_model = hf_model.model\n\nstate_dict = model.state_dict()\nlm_model.embed_tokens.weight = torch.nn.Parameter(state_dict\u003cspan class=\"hljs-punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"embed.W_E\"\u003c/span\u003e\u003cspan class=\"hljs-punctuation\"\u003e]\u003c/span\u003e.cpu())\nfor l in range(model.cfg.n_layers)\u003cspan class=\"hljs-punctuation\"\u003e:\u003c/span\u003e\n    lm_model.layers\u003cspan class=\"hljs-punctuation\"\u003e[\u003c/span\u003el\u003cspan class=\"hljs-punctuation\"\u003e]\u003c/span\u003e.self_attn.o_proj.weight = torch.nn.Parameter(\n        einops.rearrange(\n            state_dict\u003cspan class=\"hljs-punctuation\"\u003e[\u003c/span\u003ef\u003cspan class=\"hljs-string\"\u003e\"blocks.{l}.attn.W_O\"\u003c/span\u003e\u003cspan class=\"hljs-punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"hljs-punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"n h m-\u003em (n h)\"\u003c/span\u003e\u003cspan class=\"hljs-punctuation\"\u003e,\u003c/span\u003e n=model.cfg.n_heads\n        ).contiguous()\n    )\n    lm_model.layers\u003cspan class=\"hljs-punctuation\"\u003e[\u003c/span\u003el\u003cspan class=\"hljs-punctuation\"\u003e]\u003c/span\u003e.mlp.down_proj.weight = torch.nn.Parameter(\n        torch.transpose(state_dict\u003cspan class=\"hljs-punctuation\"\u003e[\u003c/span\u003ef\u003cspan class=\"hljs-string\"\u003e\"blocks.{l}.mlp.W_out\"\u003c/span\u003e\u003cspan class=\"hljs-punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"hljs-punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\u003cspan class=\"hljs-punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e).contiguous()\n    )\n\nhf_model.push_to_hub(f\u003cspan class=\"hljs-string\"\u003e\"{MODEL_ID}-abliterated\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e⚖️ DPO Fine-Tuning\u003c/h1\u003e\n\u003cp\u003e이전 섹션의 abliterated 및 소스 모델을 Open LLM Leaderboard 및 Nous의 벤치마크 스위트에서 평가했습니다. 여기에 결과가 있습니다:\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_2.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e보시다시피, 원본 모델은 Llama 3 8B Instruct보다 현저하게 우수한 성능을 보여줍니다. 그러나 우리는 모든 벤치마크에서 절삭된 버전에서 성능 하락을 관찰하고 있습니다. 절삭 과정은 성능을 향상시키면서도 모델의 품질을 저하시킨 것으로 나타났습니다.\u003c/p\u003e\n\u003cp\u003e이 문제를 해결하기 위해 우리는 우리의 절삭된 모델을 추가로 훈련하여 회복시키는 아이디어가 있습니다. 대부분의 세밀 조정된 모델들과 마찬가지로 Llama 3 8B Instruct은 지도 학습 세밀 조정에 있어서 꽤 취약합니다. 추가적인 SFT는 모델의 성능을 떨어뜨릴 가능성이 높습니다.\u003c/p\u003e\n\u003cp\u003e대체로, 선호 맞춤이 상당히 가볍고 우리의 절삭된 모델을 뇌개박하지 않아도 됩니다. DPO는 사용하기 쉽고 우수한 추적 레코드로 여기서 좋은 후보입니다. 이를 구현하기 위해 mlabonne/orpo-dpo-mix-40k 데이터셋을 사용하는 LazyAxolotl (Axolotl을 만들어 준 Wing Lian에게 감사드립니다)을 사용했습니다. 여기에 사용한 구성은 다음과 같습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-attr\"\u003ebase_model\u003c/span\u003e: mlabonne/\u003cspan class=\"hljs-title class_\"\u003eDaredevil\u003c/span\u003e-8B-abliterated\n\u003cspan class=\"hljs-attr\"\u003emodel_type\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eLlamaForCausalLM\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003etokenizer_type\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eAutoTokenizer\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003eload_in_8bit\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003efalse\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eload_in_4bit\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003etrue\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003estrict\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003efalse\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003esave_safetensors\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003etrue\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003erl\u003c/span\u003e: dpo\n\u003cspan class=\"hljs-attr\"\u003echat_template\u003c/span\u003e: chatml\n\u003cspan class=\"hljs-attr\"\u003edatasets\u003c/span\u003e:\n  - \u003cspan class=\"hljs-attr\"\u003epath\u003c/span\u003e: mlabonne/orpo-dpo-mix-40k\n    \u003cspan class=\"hljs-attr\"\u003esplit\u003c/span\u003e: train\n    \u003cspan class=\"hljs-attr\"\u003etype\u003c/span\u003e: chatml.\u003cspan class=\"hljs-property\"\u003eintel\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003edataset_prepared_path\u003c/span\u003e:\n\u003cspan class=\"hljs-attr\"\u003eval_set_size\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eoutput_dir\u003c/span\u003e: ./out\n\n\u003cspan class=\"hljs-attr\"\u003eadapter\u003c/span\u003e: qlora\n\u003cspan class=\"hljs-attr\"\u003elora_model_dir\u003c/span\u003e:\n\n\u003cspan class=\"hljs-attr\"\u003esequence_len\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e2048\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003esample_packing\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003efalse\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003epad_to_sequence_len\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003efalse\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003elora_r\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003elora_alpha\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003elora_dropout\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0.05\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003elora_target_linear\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003etrue\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003elora_fan_in_fan_out\u003c/span\u003e:\n\n\u003cspan class=\"hljs-attr\"\u003ewandb_project\u003c/span\u003e: axolotl\n\u003cspan class=\"hljs-attr\"\u003ewandb_entity\u003c/span\u003e:\n\u003cspan class=\"hljs-attr\"\u003ewandb_watch\u003c/span\u003e:\n\u003cspan class=\"hljs-attr\"\u003ewandb_name\u003c/span\u003e:\n\u003cspan class=\"hljs-attr\"\u003ewandb_log_model\u003c/span\u003e:\n\n\u003cspan class=\"hljs-attr\"\u003egradient_accumulation_steps\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003emicro_batch_size\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003enum_epochs\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eoptimizer\u003c/span\u003e: paged_adamw_8bit\n\u003cspan class=\"hljs-attr\"\u003elr_scheduler\u003c/span\u003e: cosine\n\u003cspan class=\"hljs-attr\"\u003elearning_rate\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e5e-6\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003etrain_on_inputs\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003efalse\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003egroup_by_length\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003efalse\u003c/span\u003e\n... (이어짐)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e6xA6000 GPU와 DeepSpeed ZeRO-2를 사용하여 모델을 훈련했어요. 훈련에는 약 6시간 45분이 소요되었답니다. W\u0026#x26;B에서 얻은 훈련 곡선을 여기에 가져왔어요:\u003c/p\u003e\n\u003cp\u003eDPO를 세밀하게 조정한 mlabonne/NeuralDaredevil-8B-abliterated 모델이 자동으로 업로드되었어요. 저희가 앞서 지워버린 버전을 수정했는지 확인하기 위해 동일한 벤치마크에서 평가했어요:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-UncensoranyLLMwithabliteration_3.png\" alt=\"훈련 곡선\"\u003e\u003c/p\u003e\n\u003cp\u003e이 추가 훈련을 통해 지워진 영향 대부분을 회복할 수 있었어요. 모델이 개선되지 않는 한 영역은 GSM8K, 수학 데이터 세트, 인데요, 이는 orpo-dpo-mix-40k가 더 많은 수학 샘플을 필요로 할 수 있다는 것을 의미할 수 있어요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e최종 모델은 8B 범주에서 최첨단 성능을 자랑하는 미검열 LLM입니다. 필터링이 필요 없을 때는 Llama 3 8B Instruct의 개선된 버전으로 추천합니다. LM Studio에서 GGUF와 같은 양자화된 버전을 사용해 볼 수도 있습니다.\u003c/p\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e이 글에서는 소명화(abliteration) 개념을 소개했습니다. 이 기술은 모델의 활성화를 해롭고 해를 끼치지 않는 프롬프트에 사용하여 거부 방향을 계산하고, 모델의 가중치를 수정하여 거부를 그만 내보낼 수 있도록 합니다. 이 기술은 안전 세밀조정의 취약성을 보여주며 윤리적 고려 사항을 던지고 있습니다.\u003c/p\u003e\n\u003cp\u003e우리는 Daredevil-8B에 소명화를 적용하여 필터링을 해제했지만, 이로 인해 모델의 성능이 저하되었습니다. 그 후 DPO를 사용하여 NeuralDaredevil-8B 모델을 생성하여 완전히 미검열이고 고품질의 8B LLM을 만들었습니다. 소명화는 정렬 제거에 국한되지 않으며, 다시 교육 없이 세밀 조정의 일종으로 간주해야 합니다. 실제로 MopeyMule의 FailSpy의 경우처럼 좌절적인 대화 스타일을 채택하는 것과 같이 창의적으로 다른 목표에도 적용될 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 기사가 마음에 들었으면 좋겠어요. 더 많은 내용을 보고 싶다면 Hugging Face와 Twitter의 @maximelabonne를 팔로우해 주세요.\u003c/p\u003e\n\u003ch1\u003e참고 자료\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eFailSpy, “abliterator library,” GitHub, 2024.\u003c/li\u003e\n\u003cli\u003eAndy Arditi, Oscar Obeso, Aaquib111, wesg, Neel Nanda, “Refusal in LLMs is mediated by a single direction,” Lesswrong, 2024.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-UncensoranyLLMwithabliteration"},"buildId":"kkckKPyxcjJp_o8wb2unW","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>