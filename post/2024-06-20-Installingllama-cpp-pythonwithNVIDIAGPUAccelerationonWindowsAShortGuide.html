<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>윈도우에서 NVIDIA GPU 가속을 활용해 llama-cpp-python 설치하기 간단 안내 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="윈도우에서 NVIDIA GPU 가속을 활용해 llama-cpp-python 설치하기 간단 안내 | itposting" data-gatsby-head="true"/><meta property="og:title" content="윈도우에서 NVIDIA GPU 가속을 활용해 llama-cpp-python 설치하기 간단 안내 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide" data-gatsby-head="true"/><meta name="twitter:title" content="윈도우에서 NVIDIA GPU 가속을 활용해 llama-cpp-python 설치하기 간단 안내 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-20 14:56" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">윈도우에서 NVIDIA GPU 가속을 활용해 llama-cpp-python 설치하기 간단 안내</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="윈도우에서 NVIDIA GPU 가속을 활용해 llama-cpp-python 설치하기 간단 안내" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 20, 2024</span><span class="posts_reading_time__f7YPP">3<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<img src="/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_0.png">
<p>개발자이신가요? 로컬 LLM 개발을 위해 Windows에서 하드웨어 가속된 llama-cpp-python의 성능을 끌어올리고 싶으신가요? 더 이상 찾지 마세요! 이 안내서에서는 스텝별로 안내하여 본인이 설치하는 동안 겪은 문제점을 피할 수 있도록 도와드리겠습니다.</p>
<h1>Prerequisites:</h1>
<ul>
<li>Visual Studio 설치:</li>
</ul>
<div class="content-ad"></div>
<ul>
<li>Windows용 C++ CMake 도구.</li>
<li>C++ 핵심 기능</li>
<li>Windows 10/11 SDK.</li>
</ul>
<p><img src="/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_1.png" alt="이미지"></p>
<ol start="2">
<li>CUDA Toolkit:</li>
</ol>
<ul>
<li>NVIDIA 공식 웹사이트에서 CUDA Toolkit 12.2를 다운로드하고 설치합니다.</li>
<li>nvcc --version 및 nvidia-smi로 설치 여부 확인합니다.</li>
</ul>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_2.png">
<ul>
<li>환경 변수에 CUDA_PATH (C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.2)를 추가하세요.</li>
</ul>
<h2>설치 단계:</h2>
<p>새 명령 프롬프트를 열고 Python 환경을 활성화하세요 (예: conda 사용). 다음 명령을 실행하세요:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-shell">CMAKE_ARGS=-DLLAMA_CUBLAS=on을 설정합니다.
FORCE_CMAKE=1로 설정합니다.
pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir
<span class="hljs-meta prompt_">
# </span><span class="bash">컴파일 시 cuBLAS가 사용되고 있는지 확인하려면 --verbose를 사용하세요.</span>
</code></pre>
<p>설치 중 --verbose 옵션을 추가하면 CUDA가 컴파일에 사용되는지 확인할 수 있습니다.</p>
<p><img src="/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_3.png" alt="이미지"></p>
<p>CUDA가 올바르게 구성되지 않았다면, llama-cpp-python은 하드웨어 가속을 사용하지 않고 설치됩니다.</p>
<div class="content-ad"></div>
<p>만약 Cuda가 감지되지만 No CUDA toolset founderror가 발생한다면 다음을 수행하세요:</p>
<ul>
<li>파일을 복사합니다: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.2\extras\visual_studio_integration\MSBuildExtensions 에서 아래 경로로:
(Enterprise 버전 인 경우) C:\Program Files\Microsoft Visual Studio\2022\Enterprise\MSBuild\Microsoft\VC\v170\BuildCustomizations
또는
(Community 버전인 경우) C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Microsoft\VC\v170\BuildCustomizations</li>
</ul>
<pre><code class="hljs language-js">copy <span class="hljs-string">"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.2\extras\visual_studio_integration\MSBuildExtensions"</span> <span class="hljs-string">"C:\Program Files\Microsoft Visual Studio\2022\Enterprise\MSBuild\Microsoft\VC\v170\BuildCustomizations"</span>
</code></pre>
<p>(설치에 기반하여 경로를 조정하세요)</p>
<div class="content-ad"></div>
<h1>테스트</h1>
<ul>
<li>다음의 Python 코드를 실행하여 설치를 확인하세요:</li>
</ul>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> llama_cpp <span class="hljs-keyword">import</span> <span class="hljs-title class_">Llama</span>
llm = <span class="hljs-title class_">Llama</span>(model_path=<span class="hljs-string">"model.gguf"</span>, n_gpu_layers=<span class="hljs-number">30</span>, n_ctx=<span class="hljs-number">3584</span>, n_batch=<span class="hljs-number">521</span>, verbose=<span class="hljs-title class_">True</span>)
# <span class="hljs-variable constant_">GPU</span> 및 모델에 맞게 n_gpu_layers를 조정하세요
output = <span class="hljs-title function_">llm</span>(<span class="hljs-string">"Q: 태양계의 행성을 말해주세요? A: "</span>, max_tokens=<span class="hljs-number">32</span>, stop=[<span class="hljs-string">"Q:"</span>, <span class="hljs-string">"\n"</span>], echo=<span class="hljs-title class_">True</span>)
<span class="hljs-title function_">print</span>(output)
</code></pre>
<p><img src="/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_4.png" alt="링크 텍스트"></p>
<div class="content-ad"></div>
<p>만약 설치가 올바르게 되었다면, 모델 속성에서 BLAS = 1 지표가 표시될 것입니다.</p>
<h1>결론:</h1>
<p>이 단계를 따르면, Windows 기기에 cuBLAS 가속을 사용하여 llama-cpp-python을 성공적으로 설치했을 것입니다. 이 안내서는 과정을 간소화하고 흔한 문제를 피할 수 있도록 돕는 것을 목표로 합니다.</p>
<p>이제 향상된 성능을 갖는 로컬 llama 개발에 뛰어들 준비가 되었습니다. GPU 오프로딩에 행운을 빕니다!</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"윈도우에서 NVIDIA GPU 가속을 활용해 llama-cpp-python 설치하기 간단 안내","description":"","date":"2024-06-20 14:56","slug":"2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide","content":"\n\n\u003cimg src=\"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_0.png\" /\u003e\n\n개발자이신가요? 로컬 LLM 개발을 위해 Windows에서 하드웨어 가속된 llama-cpp-python의 성능을 끌어올리고 싶으신가요? 더 이상 찾지 마세요! 이 안내서에서는 스텝별로 안내하여 본인이 설치하는 동안 겪은 문제점을 피할 수 있도록 도와드리겠습니다.\n\n# Prerequisites:\n\n- Visual Studio 설치:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Windows용 C++ CMake 도구.\n- C++ 핵심 기능\n- Windows 10/11 SDK.\n\n![이미지](/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_1.png)\n\n2. CUDA Toolkit:\n\n- NVIDIA 공식 웹사이트에서 CUDA Toolkit 12.2를 다운로드하고 설치합니다.\n- nvcc --version 및 nvidia-smi로 설치 여부 확인합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_2.png\" /\u003e\n\n- 환경 변수에 CUDA_PATH (C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.2)를 추가하세요.\n\n## 설치 단계:\n\n새 명령 프롬프트를 열고 Python 환경을 활성화하세요 (예: conda 사용). 다음 명령을 실행하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```shell\nCMAKE_ARGS=-DLLAMA_CUBLAS=on을 설정합니다.\nFORCE_CMAKE=1로 설정합니다.\npip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir\n\n# 컴파일 시 cuBLAS가 사용되고 있는지 확인하려면 --verbose를 사용하세요.\n```\n\n설치 중 --verbose 옵션을 추가하면 CUDA가 컴파일에 사용되는지 확인할 수 있습니다.\n\n![이미지](/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_3.png)\n\nCUDA가 올바르게 구성되지 않았다면, llama-cpp-python은 하드웨어 가속을 사용하지 않고 설치됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 Cuda가 감지되지만 No CUDA toolset founderror가 발생한다면 다음을 수행하세요:\n\n- 파일을 복사합니다: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.2\\extras\\visual_studio_integration\\MSBuildExtensions 에서 아래 경로로:\n(Enterprise 버전 인 경우) C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\n또는\n(Community 버전인 경우) C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\n\n```js\ncopy \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.2\\extras\\visual_studio_integration\\MSBuildExtensions\" \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\"\n```\n\n(설치에 기반하여 경로를 조정하세요)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 테스트\n\n- 다음의 Python 코드를 실행하여 설치를 확인하세요:\n\n```js\nfrom llama_cpp import Llama\nllm = Llama(model_path=\"model.gguf\", n_gpu_layers=30, n_ctx=3584, n_batch=521, verbose=True)\n# GPU 및 모델에 맞게 n_gpu_layers를 조정하세요\noutput = llm(\"Q: 태양계의 행성을 말해주세요? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)\nprint(output)\n```\n\n![링크 텍스트](/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 설치가 올바르게 되었다면, 모델 속성에서 BLAS = 1 지표가 표시될 것입니다.\n\n# 결론:\n\n이 단계를 따르면, Windows 기기에 cuBLAS 가속을 사용하여 llama-cpp-python을 성공적으로 설치했을 것입니다. 이 안내서는 과정을 간소화하고 흔한 문제를 피할 수 있도록 돕는 것을 목표로 합니다.\n\n이제 향상된 성능을 갖는 로컬 llama 개발에 뛰어들 준비가 되었습니다. GPU 오프로딩에 행운을 빕니다!","ogImage":{"url":"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_0.png"},"coverImage":"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_0.png","tag":["Tech"],"readingTime":3},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cimg src=\"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_0.png\"\u003e\n\u003cp\u003e개발자이신가요? 로컬 LLM 개발을 위해 Windows에서 하드웨어 가속된 llama-cpp-python의 성능을 끌어올리고 싶으신가요? 더 이상 찾지 마세요! 이 안내서에서는 스텝별로 안내하여 본인이 설치하는 동안 겪은 문제점을 피할 수 있도록 도와드리겠습니다.\u003c/p\u003e\n\u003ch1\u003ePrerequisites:\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eVisual Studio 설치:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eWindows용 C++ CMake 도구.\u003c/li\u003e\n\u003cli\u003eC++ 핵심 기능\u003c/li\u003e\n\u003cli\u003eWindows 10/11 SDK.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eCUDA Toolkit:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eNVIDIA 공식 웹사이트에서 CUDA Toolkit 12.2를 다운로드하고 설치합니다.\u003c/li\u003e\n\u003cli\u003envcc --version 및 nvidia-smi로 설치 여부 확인합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_2.png\"\u003e\n\u003cul\u003e\n\u003cli\u003e환경 변수에 CUDA_PATH (C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.2)를 추가하세요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e설치 단계:\u003c/h2\u003e\n\u003cp\u003e새 명령 프롬프트를 열고 Python 환경을 활성화하세요 (예: conda 사용). 다음 명령을 실행하세요:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-shell\"\u003eCMAKE_ARGS=-DLLAMA_CUBLAS=on을 설정합니다.\nFORCE_CMAKE=1로 설정합니다.\npip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir\n\u003cspan class=\"hljs-meta prompt_\"\u003e\n# \u003c/span\u003e\u003cspan class=\"bash\"\u003e컴파일 시 cuBLAS가 사용되고 있는지 확인하려면 --verbose를 사용하세요.\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e설치 중 --verbose 옵션을 추가하면 CUDA가 컴파일에 사용되는지 확인할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_3.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eCUDA가 올바르게 구성되지 않았다면, llama-cpp-python은 하드웨어 가속을 사용하지 않고 설치됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e만약 Cuda가 감지되지만 No CUDA toolset founderror가 발생한다면 다음을 수행하세요:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e파일을 복사합니다: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.2\\extras\\visual_studio_integration\\MSBuildExtensions 에서 아래 경로로:\n(Enterprise 버전 인 경우) C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\n또는\n(Community 버전인 경우) C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ecopy \u003cspan class=\"hljs-string\"\u003e\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.2\\extras\\visual_studio_integration\\MSBuildExtensions\"\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(설치에 기반하여 경로를 조정하세요)\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e테스트\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e다음의 Python 코드를 실행하여 설치를 확인하세요:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_cpp \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eLlama\u003c/span\u003e\nllm = \u003cspan class=\"hljs-title class_\"\u003eLlama\u003c/span\u003e(model_path=\u003cspan class=\"hljs-string\"\u003e\"model.gguf\"\u003c/span\u003e, n_gpu_layers=\u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e, n_ctx=\u003cspan class=\"hljs-number\"\u003e3584\u003c/span\u003e, n_batch=\u003cspan class=\"hljs-number\"\u003e521\u003c/span\u003e, verbose=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n# \u003cspan class=\"hljs-variable constant_\"\u003eGPU\u003c/span\u003e 및 모델에 맞게 n_gpu_layers를 조정하세요\noutput = \u003cspan class=\"hljs-title function_\"\u003ellm\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Q: 태양계의 행성을 말해주세요? A: \"\u003c/span\u003e, max_tokens=\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, stop=[\u003cspan class=\"hljs-string\"\u003e\"Q:\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"\\n\"\u003c/span\u003e], echo=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(output)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide_4.png\" alt=\"링크 텍스트\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e만약 설치가 올바르게 되었다면, 모델 속성에서 BLAS = 1 지표가 표시될 것입니다.\u003c/p\u003e\n\u003ch1\u003e결론:\u003c/h1\u003e\n\u003cp\u003e이 단계를 따르면, Windows 기기에 cuBLAS 가속을 사용하여 llama-cpp-python을 성공적으로 설치했을 것입니다. 이 안내서는 과정을 간소화하고 흔한 문제를 피할 수 있도록 돕는 것을 목표로 합니다.\u003c/p\u003e\n\u003cp\u003e이제 향상된 성능을 갖는 로컬 llama 개발에 뛰어들 준비가 되었습니다. GPU 오프로딩에 행운을 빕니다!\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-20-Installingllama-cpp-pythonwithNVIDIAGPUAccelerationonWindowsAShortGuide"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>