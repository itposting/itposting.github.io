<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>RT-2, 구글의 새 혁신 실제 월-E를 만드는 비결 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="RT-2, 구글의 새 혁신 실제 월-E를 만드는 비결 | itposting" data-gatsby-head="true"/><meta property="og:title" content="RT-2, 구글의 새 혁신 실제 월-E를 만드는 비결 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E" data-gatsby-head="true"/><meta name="twitter:title" content="RT-2, 구글의 새 혁신 실제 월-E를 만드는 비결 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-22 19:34" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_buildManifest.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">RT-2, 구글의 새 혁신 실제 월-E를 만드는 비결</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="RT-2, 구글의 새 혁신 실제 월-E를 만드는 비결" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 22, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>안녕하세요! 아래는 Markdown 형식으로 변경된 표입니다.</p>
<p><img src="/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_0.png" alt="이미지"></p>
<p>ChatGPT가 2022년 11월에 출시된 이후로, 전 세계가 AI를 중심으로 돌아가는 것 같죠.</p>
<p>하지만 걱정하지 마세요. 이 글은 'ChatGPT는 놀라운 기술이다'를 다시 말하는 글이 아니에요. 오늘은 더 혁명적인 주제에 대해 이야기할 거예요.</p>
<p>바로 Google Deepmind의 새로운 로봇, RT-2에 대해요.</p>
<div class="content-ad"></div>
<p>로봇 팔이라 해도 RT-2는 그 중에서도 뛰어난 능력을 자랑해요. 실제로 RT-2를 만드는 데 구글은 이제까지 본 적 없는 새로운 AI 모델을 만들어야 했죠.</p>
<p>융합된 지능이 이곳에 있습니다.</p>
<p>그리고 아마도 월-이도요.</p>
<h2>여기에서 함께 나와 AI에 대해 쉽게 배워보세요!</h2>
<div class="content-ad"></div>
<h1>새로운 모델 클래스</h1>
<p>지난 6개월 동안 인류가 AI로 이룬 성과는 정말 놀라운 것입니다.</p>
<p>간단히 말해, 우리는 인간과 자연어를 통해 소통하는 기계를 누구나 이용할 수 있게 만들었습니다.</p>
<p>하지만 AI의 잠재력은 단순한 텍스트를 뛰어 넘습니다. 많은 연구자들이 더 큰 목표를 향해 눈치를 돌리고 있습니다.</p>
<div class="content-ad"></div>
<h2>다중 모드 구축을 향한 길</h2>
<p>최근 팟캐스트에서 AI 슈퍼히어로 앤드류 엔지가 컴퓨터 비전이라는 이미지를 처리하고 이해하기 위해 모델을 훈련시키는 AI 분야가 "텍스트 프롬프팅"보다 약 "두 년 뒤"라고 언급했습니다. 그러나 그는 그것이 해당 모델들과 동등한 혁명이 될 것으로 예상했습니다.</p>
<p>그가 "텍스트 프롬프팅"로 언급한 것은 다름 아닌 대형 언어 모델 또는 LLM(Large Language Models)로, 그 중 가장 대표적인 예가 ChatGPT입니다.</p>
<p>그러나 우리 모두가 알다시피, 우리가 능력 면에서 인간 수준의 기계를 구축하려면 우리인간이 다중 모드인것보다 훨씬 더 많은 것이 필요합니다.</p>
<div class="content-ad"></div>
<p>일반적인 용어로 말하면, 우리는 세상에 대한 이해를 텍스트만으로 구축하지 않습니다. 우리에겐 눈이 있고, 귀가 있고, 촉감도 있죠... 우리의 모든 감각이 세상이 무엇인지를 파악하는 데 도움을 줍니다.</p>
<p>실제로 이러한 감각들은 우리가 어릴 때 세상에 대해 배우는 데 도움을 줍니다. 우리가 맨 처음 문장을 읽기 보다 훨씬 이전부터 말이죠.</p>
<p>그러므로 AI 연구자들이 텍스트뿐만 아니라 다른 것들도 처리하는 모델, 혹은 명확히 말하자면 멀티모달 모델을 만들고 싶어하는 것은 자연스러운 일입니다.</p>
<p>그리고 이에 대한 AI 공간에서 가장 흥미로운 혁신 중 하나는 시각-언어 모델 중 하나입니다.</p>
<div class="content-ad"></div>
<h2>VLMs, 더 ‘인간적인’ 기계들</h2>
<p>비전-언어 모델(VLMs)은 OpenAI의 CLIP이나 Microsoft의 Kosmos와 같은 모델로, 이름에서 알 수 있듯이 텍스트뿐만 아니라 이미지도 처리하는 모델입니다.</p>
<p>예를 들어, OpenAI의 CLIP은 대조 손실 절차를 따라 이미지와 텍스트 사이에 얽혀 있는 임베딩 공간을 생성함으로써 작동합니다(간단히 말하면 동일한 것을 설명하는 이미지와 텍스트를 모아주는 것이 중요하며 Dall-E와 같은 확산 모델을 구축하는 데 필수적입니다). 그러나 Kosmos와 같은 모델은 아직 탐험하지 않은 잠재력을 보여주는 모델입니다.</p>
<p>Kosmos의 작동 방식은 매우 간단합니다. 이미지나 텍스트(또는 둘 다)를 보내면, 이에 기반하여 텍스트를 제공합니다.</p>
<div class="content-ad"></div>
<p>참고로, Carnegie Mellon University에서 개발한 이름이 GILL인 또 다른 VLM을 아래에서 확인할 수 있습니다. 시각적 프롬프팅이 정확히 무엇인지 명확하게 이해할 수 있습니다:</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1056/1*IXxrTUmN37X9vs8dqK85-w.gif" alt="GILL"></p>
<p>하지만 이제 Google은 한 단계 더 나아가고 있습니다.</p>
<p>그들은 이러한 모델을 현실 세계로 가져오고 있습니다.</p>
<div class="content-ad"></div>
<p>로봇공학을 더 나은 수준으로 발전시키려는 시도 중에, Google Deepmind는 깨달음을 얻었습니다:</p>
<p>그리고 그것으로, Google은 인공지능 로봇학의 새로운 핵심 요소인 Vision-language-action 모델(VLAs)을 만들었습니다.</p>
<p>간단히 말해, VLAs는 로봇이 이후 이동을 수행하는 데 사용할 수 있는 동작을 출력할 수 있는 모델입니다.</p>
<p>하지만 그것들이 정말 무엇이며, 왜 그렇게 혁신적인 것일까요?</p>
<div class="content-ad"></div>
<h2>Wall-e, 그게 진짜야?</h2>
<p>로봇 공학 분야에서 연구자들을 미치게 만드는 것이 있다면, 그것은 분명히 데이터일 겁니다.</p>
<p>사실 데이터의 부재일 때든요.</p>
<p>AI 모델이 데이터의 양과 질에 완전히 의존한다는 점을 고려할 때, 후자는 가능했지만 전자는 정말 악몽이었어요.</p>
<div class="content-ad"></div>
<p>따라서, 로봇공학을 더 나은 수준으로 발전시키기 위해 구글이 베팅을 했습니다:</p>
<p>그들은 웹 규모의 텍스트와 이미지 데이터에 접근할 수 있는 VLM(범용 언어 모델)을 사용해 이러한 모델이 학습한 표현을 로봇으로 전이할 수 있다고 가설을 세웠습니다.</p>
<p>간단히 말해서, 로봇 데이터를 기반으로 세계에 대한 고수준 의미 지식을 갖춘 로봇을 만드는 것이 이루기 어려운 일이라면, 기존의 VLM 모델을 사용하고 고품질의 로봇 데이터로 적합화시키는 것은 어떨까요?</p>
<p>그리고 이제 이를 VLAs(로봇 언어 모델)라고 부를 수 있게 된 것입니다.</p>
<div class="content-ad"></div>
<h2>행동 예측기</h2>
<p>LLMs나 VLMs와 마찬가지로 VLAs도 동일한 일을 수행합니다. 토큰을 예측합니다.</p>
<p>그러나 ChatGPT와 같이 텍스트 토큰을 예측하는 대신, RT-2와 같은 VLAs는 로봇이 수행해야 할 작업을 카메라 관측에 기반하여 예측할 수도 있습니다. 아래 이미지에서 확인할 수 있습니다:</p>
<p><img src="/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_1.png" alt="이미지"></p>
<div class="content-ad"></div>
<p>얻은 결과는 우리에게 매우 중요한 두 가지 교훈을 알려줍니다.</p>
<h2>일반화와 발생</h2>
<p>AI로봇 기술의 최첨단인 RT-2를 평가할 때 결과는 매우 명확합니다(RT-2 모델은 녹색과 파란색으로 표시됨):</p>
<p><img src="/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_2.png" alt="RT-2 모델"></p>
<div class="content-ad"></div>
<p>안녕하세요! 항상 그렇듯이, 요점은 세부사항에 있습니다. RT-2를 주의 깊게 관찰하면, 이는 로봇공학의 가장 어려운 두 가지 임무, 즉 일반화와 신흥성에 뛰어난 성과를 보여줍니다.</p>
<p>간단히 말하자면, RT-2는 교육 중 본 적이 없는 상황, 예를 들어 보이지 않는 물체나 환경과 같은 상황에서도 잘 수행하는 능력(일반화)과 VLM 덕분에 얻은 언어 지식의 규모로 학습한 새로운 예상치 못한 능력(신흥성)을 펼쳐냈습니다.</p>
<p>가장 놀라운 점은: 이것이 첫 번째 추론이 가능한 로봇이라는 것입니다.</p>
<p>이 점을 증명하기 위해, 구글이 진행한 6,000건 이상의 시험 중에서 몇 가지 인상적인 행동이 포함되어 있습니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_3.png" alt="이미지"></p>
<p>한 예를 들어 ‘추론’으로 볼 수 있는 것을 분석해 봅시다. 맨 위 두 번째 왼쪽을 보면, 로봇은 두 번째 과자봉지의 나쁜 위치를 이해하여 요청에 완벽히 대답할 수 있습니다.</p>
<p>이 작업을 수행하려면, 모델은 카메라로 촬영된 이미지의 밀도가 높은 의미 지식이 필요했습니다. 즉, 탁자 끝에 있는 봉지가 떨어질 수 있다는 것을 이해하면서, 다른 봉지는 그렇지 않다는 것을 알아냈습니다.</p>
<p>비슷하게, 아래 오른쪽 예에서 모델은 당나귀와 문어의 차이를 이해할 뿐만 아니라, “육지 동물”이 당나귀를 의미한다는 것을 생각할 수도 있습니다.</p>
<div class="content-ad"></div>
<p>더 정량적인 관점에서 구글은 모델을 세 가지 기준으로 평가했습니다:</p>
<ul>
<li>심볼 이해</li>
<li>추론</li>
<li>인간 인식</li>
</ul>
<p>놀랍게도, RT-2는 이 모든 면에서 우수하게 성과를 거두었으며, 다음에 나온 예시와 같은 동작을 수행할 수 있는 능력을 입증했습니다:</p>
<p><img src="/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_4.png" alt="이미지"></p>
<div class="content-ad"></div>
<p>인상적인 점은 RT-2가 알 수 없는 상황에서 잘 작동할 수 있었던 것처럼, 위의 이미지들도 새로운 업무를 수행할 수 있는 능력을 보여줍니다. 다시 말해, 시각 언어 모델을 추가하더라도 새로운 로봇 동작을 생성하는 데는 한계가 있었지만 (논문에서 인정함), 로봇에 풍부한 의미 지식을 전달하여 배치, 물체 인식, 논리 추론과 같은 복잡하고 신흥 개념에 대해 훨씬 더 인식력을 갖도록 만들었습니다.</p>
<h1>혁신의 바퀴는 계속 회전합니다</h1>
<p>RT-2를 본 이후에는 내년 말까지 세계 각국의 제조업체가 이러한 로봇을 사용하여 프로세스를 개선하는 것이 놀라운 일이 아닐 것입니다.</p>
<div class="content-ad"></div>
<p>인공지능 로봇들은 주변 환경에 대해 훨씬 더 인식력을 가지고 있습니다. 그들의 신경망 속에 점점 더 복잡한 세계 모델을 구축하여 하루가 다르게 진화하고 있는데, 이는 수천 년이 걸린 인간들의 진화에 의해 이루어졌던 것과 매우 가까워지고 있습니다.</p>
<p>그러나 많은 사람들은 이러한 모델들이 단순한 확률론적 앵무새에 불과하다고 주장할 것입니다. 이들은 그저 "지능적인 활동"을 단순히 암기하는 것으로 여기는 것이죠.</p>
<p>그러나 가장 이상하고 예상치 못한 경우에도 "지능적으로" 행동하는 기계들을 보면, 이 모델들이 자신이 하는 일을 실제로 이해한다고 주장하지 않을 수 있는 것이 정말로 도전이 됩니다. 이제 이러한 모델들은 "체화된 지능"이라고 묘사하는 것과 같이 지능적인 물리적 행동을 수행할 수 있는 조건에 이르렀습니다.</p>
<p>RT-2에 관한 것을 생각해보면, 이는 바로 진행 중인 로봇 기술이 미래의 몇 달 동안 어떻게 진화할지에 대한 단순한 꼭대기에 지나지 않을 것입니다. 그래서 아마도 영화 '월-E'가 우리 삶에서 생각했던 것보다 그렇게 멀리 떨어져 있지 않다는 생각이 들 수도 있겠네요.</p>
<div class="content-ad"></div>
<p>프레스 릴리스와 연구 논문 링크입니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"RT-2, 구글의 새 혁신 실제 월-E를 만드는 비결","description":"","date":"2024-06-22 19:34","slug":"2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E","content":"\n\n안녕하세요! 아래는 Markdown 형식으로 변경된 표입니다.\n\n\n![이미지](/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_0.png)\n\n\nChatGPT가 2022년 11월에 출시된 이후로, 전 세계가 AI를 중심으로 돌아가는 것 같죠.\n\n하지만 걱정하지 마세요. 이 글은 'ChatGPT는 놀라운 기술이다'를 다시 말하는 글이 아니에요. 오늘은 더 혁명적인 주제에 대해 이야기할 거예요.\n\n바로 Google Deepmind의 새로운 로봇, RT-2에 대해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로봇 팔이라 해도 RT-2는 그 중에서도 뛰어난 능력을 자랑해요. 실제로 RT-2를 만드는 데 구글은 이제까지 본 적 없는 새로운 AI 모델을 만들어야 했죠.\n\n융합된 지능이 이곳에 있습니다.\n\n그리고 아마도 월-이도요.\n\n## 여기에서 함께 나와 AI에 대해 쉽게 배워보세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 새로운 모델 클래스\n\n지난 6개월 동안 인류가 AI로 이룬 성과는 정말 놀라운 것입니다.\n\n간단히 말해, 우리는 인간과 자연어를 통해 소통하는 기계를 누구나 이용할 수 있게 만들었습니다.\n\n하지만 AI의 잠재력은 단순한 텍스트를 뛰어 넘습니다. 많은 연구자들이 더 큰 목표를 향해 눈치를 돌리고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 다중 모드 구축을 향한 길\n\n최근 팟캐스트에서 AI 슈퍼히어로 앤드류 엔지가 컴퓨터 비전이라는 이미지를 처리하고 이해하기 위해 모델을 훈련시키는 AI 분야가 \"텍스트 프롬프팅\"보다 약 \"두 년 뒤\"라고 언급했습니다. 그러나 그는 그것이 해당 모델들과 동등한 혁명이 될 것으로 예상했습니다.\n\n그가 \"텍스트 프롬프팅\"로 언급한 것은 다름 아닌 대형 언어 모델 또는 LLM(Large Language Models)로, 그 중 가장 대표적인 예가 ChatGPT입니다.\n\n그러나 우리 모두가 알다시피, 우리가 능력 면에서 인간 수준의 기계를 구축하려면 우리인간이 다중 모드인것보다 훨씬 더 많은 것이 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적인 용어로 말하면, 우리는 세상에 대한 이해를 텍스트만으로 구축하지 않습니다. 우리에겐 눈이 있고, 귀가 있고, 촉감도 있죠... 우리의 모든 감각이 세상이 무엇인지를 파악하는 데 도움을 줍니다.\n\n실제로 이러한 감각들은 우리가 어릴 때 세상에 대해 배우는 데 도움을 줍니다. 우리가 맨 처음 문장을 읽기 보다 훨씬 이전부터 말이죠.\n\n그러므로 AI 연구자들이 텍스트뿐만 아니라 다른 것들도 처리하는 모델, 혹은 명확히 말하자면 멀티모달 모델을 만들고 싶어하는 것은 자연스러운 일입니다.\n\n그리고 이에 대한 AI 공간에서 가장 흥미로운 혁신 중 하나는 시각-언어 모델 중 하나입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## VLMs, 더 ‘인간적인’ 기계들\n\n비전-언어 모델(VLMs)은 OpenAI의 CLIP이나 Microsoft의 Kosmos와 같은 모델로, 이름에서 알 수 있듯이 텍스트뿐만 아니라 이미지도 처리하는 모델입니다.\n\n예를 들어, OpenAI의 CLIP은 대조 손실 절차를 따라 이미지와 텍스트 사이에 얽혀 있는 임베딩 공간을 생성함으로써 작동합니다(간단히 말하면 동일한 것을 설명하는 이미지와 텍스트를 모아주는 것이 중요하며 Dall-E와 같은 확산 모델을 구축하는 데 필수적입니다). 그러나 Kosmos와 같은 모델은 아직 탐험하지 않은 잠재력을 보여주는 모델입니다.\n\nKosmos의 작동 방식은 매우 간단합니다. 이미지나 텍스트(또는 둘 다)를 보내면, 이에 기반하여 텍스트를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n참고로, Carnegie Mellon University에서 개발한 이름이 GILL인 또 다른 VLM을 아래에서 확인할 수 있습니다. 시각적 프롬프팅이 정확히 무엇인지 명확하게 이해할 수 있습니다:\n\n![GILL](https://miro.medium.com/v2/resize:fit:1056/1*IXxrTUmN37X9vs8dqK85-w.gif)\n\n하지만 이제 Google은 한 단계 더 나아가고 있습니다.\n\n그들은 이러한 모델을 현실 세계로 가져오고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로봇공학을 더 나은 수준으로 발전시키려는 시도 중에, Google Deepmind는 깨달음을 얻었습니다:\n\n그리고 그것으로, Google은 인공지능 로봇학의 새로운 핵심 요소인 Vision-language-action 모델(VLAs)을 만들었습니다.\n\n간단히 말해, VLAs는 로봇이 이후 이동을 수행하는 데 사용할 수 있는 동작을 출력할 수 있는 모델입니다.\n\n하지만 그것들이 정말 무엇이며, 왜 그렇게 혁신적인 것일까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Wall-e, 그게 진짜야?\n\n로봇 공학 분야에서 연구자들을 미치게 만드는 것이 있다면, 그것은 분명히 데이터일 겁니다.\n\n사실 데이터의 부재일 때든요.\n\nAI 모델이 데이터의 양과 질에 완전히 의존한다는 점을 고려할 때, 후자는 가능했지만 전자는 정말 악몽이었어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서, 로봇공학을 더 나은 수준으로 발전시키기 위해 구글이 베팅을 했습니다:\n\n그들은 웹 규모의 텍스트와 이미지 데이터에 접근할 수 있는 VLM(범용 언어 모델)을 사용해 이러한 모델이 학습한 표현을 로봇으로 전이할 수 있다고 가설을 세웠습니다.\n\n간단히 말해서, 로봇 데이터를 기반으로 세계에 대한 고수준 의미 지식을 갖춘 로봇을 만드는 것이 이루기 어려운 일이라면, 기존의 VLM 모델을 사용하고 고품질의 로봇 데이터로 적합화시키는 것은 어떨까요?\n\n그리고 이제 이를 VLAs(로봇 언어 모델)라고 부를 수 있게 된 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 행동 예측기\n\nLLMs나 VLMs와 마찬가지로 VLAs도 동일한 일을 수행합니다. 토큰을 예측합니다.\n\n그러나 ChatGPT와 같이 텍스트 토큰을 예측하는 대신, RT-2와 같은 VLAs는 로봇이 수행해야 할 작업을 카메라 관측에 기반하여 예측할 수도 있습니다. 아래 이미지에서 확인할 수 있습니다:\n\n![이미지](/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n얻은 결과는 우리에게 매우 중요한 두 가지 교훈을 알려줍니다.\n\n## 일반화와 발생\n\nAI로봇 기술의 최첨단인 RT-2를 평가할 때 결과는 매우 명확합니다(RT-2 모델은 녹색과 파란색으로 표시됨):\n\n![RT-2 모델](/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n안녕하세요! 항상 그렇듯이, 요점은 세부사항에 있습니다. RT-2를 주의 깊게 관찰하면, 이는 로봇공학의 가장 어려운 두 가지 임무, 즉 일반화와 신흥성에 뛰어난 성과를 보여줍니다.\n\n간단히 말하자면, RT-2는 교육 중 본 적이 없는 상황, 예를 들어 보이지 않는 물체나 환경과 같은 상황에서도 잘 수행하는 능력(일반화)과 VLM 덕분에 얻은 언어 지식의 규모로 학습한 새로운 예상치 못한 능력(신흥성)을 펼쳐냈습니다.\n\n가장 놀라운 점은: 이것이 첫 번째 추론이 가능한 로봇이라는 것입니다.\n\n이 점을 증명하기 위해, 구글이 진행한 6,000건 이상의 시험 중에서 몇 가지 인상적인 행동이 포함되어 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_3.png)\n\n한 예를 들어 ‘추론’으로 볼 수 있는 것을 분석해 봅시다. 맨 위 두 번째 왼쪽을 보면, 로봇은 두 번째 과자봉지의 나쁜 위치를 이해하여 요청에 완벽히 대답할 수 있습니다.\n\n이 작업을 수행하려면, 모델은 카메라로 촬영된 이미지의 밀도가 높은 의미 지식이 필요했습니다. 즉, 탁자 끝에 있는 봉지가 떨어질 수 있다는 것을 이해하면서, 다른 봉지는 그렇지 않다는 것을 알아냈습니다.\n\n비슷하게, 아래 오른쪽 예에서 모델은 당나귀와 문어의 차이를 이해할 뿐만 아니라, “육지 동물”이 당나귀를 의미한다는 것을 생각할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 정량적인 관점에서 구글은 모델을 세 가지 기준으로 평가했습니다:\n\n- 심볼 이해\n- 추론\n- 인간 인식\n\n놀랍게도, RT-2는 이 모든 면에서 우수하게 성과를 거두었으며, 다음에 나온 예시와 같은 동작을 수행할 수 있는 능력을 입증했습니다:\n\n![이미지](/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인상적인 점은 RT-2가 알 수 없는 상황에서 잘 작동할 수 있었던 것처럼, 위의 이미지들도 새로운 업무를 수행할 수 있는 능력을 보여줍니다. 다시 말해, 시각 언어 모델을 추가하더라도 새로운 로봇 동작을 생성하는 데는 한계가 있었지만 (논문에서 인정함), 로봇에 풍부한 의미 지식을 전달하여 배치, 물체 인식, 논리 추론과 같은 복잡하고 신흥 개념에 대해 훨씬 더 인식력을 갖도록 만들었습니다.\n\n# 혁신의 바퀴는 계속 회전합니다\n\nRT-2를 본 이후에는 내년 말까지 세계 각국의 제조업체가 이러한 로봇을 사용하여 프로세스를 개선하는 것이 놀라운 일이 아닐 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인공지능 로봇들은 주변 환경에 대해 훨씬 더 인식력을 가지고 있습니다. 그들의 신경망 속에 점점 더 복잡한 세계 모델을 구축하여 하루가 다르게 진화하고 있는데, 이는 수천 년이 걸린 인간들의 진화에 의해 이루어졌던 것과 매우 가까워지고 있습니다.\n\n그러나 많은 사람들은 이러한 모델들이 단순한 확률론적 앵무새에 불과하다고 주장할 것입니다. 이들은 그저 \"지능적인 활동\"을 단순히 암기하는 것으로 여기는 것이죠.\n\n그러나 가장 이상하고 예상치 못한 경우에도 \"지능적으로\" 행동하는 기계들을 보면, 이 모델들이 자신이 하는 일을 실제로 이해한다고 주장하지 않을 수 있는 것이 정말로 도전이 됩니다. 이제 이러한 모델들은 \"체화된 지능\"이라고 묘사하는 것과 같이 지능적인 물리적 행동을 수행할 수 있는 조건에 이르렀습니다.\n\nRT-2에 관한 것을 생각해보면, 이는 바로 진행 중인 로봇 기술이 미래의 몇 달 동안 어떻게 진화할지에 대한 단순한 꼭대기에 지나지 않을 것입니다. 그래서 아마도 영화 '월-E'가 우리 삶에서 생각했던 것보다 그렇게 멀리 떨어져 있지 않다는 생각이 들 수도 있겠네요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n프레스 릴리스와 연구 논문 링크입니다.","ogImage":{"url":"/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_0.png"},"coverImage":"/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_0.png","tag":["Tech"],"readingTime":6},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e안녕하세요! 아래는 Markdown 형식으로 변경된 표입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eChatGPT가 2022년 11월에 출시된 이후로, 전 세계가 AI를 중심으로 돌아가는 것 같죠.\u003c/p\u003e\n\u003cp\u003e하지만 걱정하지 마세요. 이 글은 'ChatGPT는 놀라운 기술이다'를 다시 말하는 글이 아니에요. 오늘은 더 혁명적인 주제에 대해 이야기할 거예요.\u003c/p\u003e\n\u003cp\u003e바로 Google Deepmind의 새로운 로봇, RT-2에 대해요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e로봇 팔이라 해도 RT-2는 그 중에서도 뛰어난 능력을 자랑해요. 실제로 RT-2를 만드는 데 구글은 이제까지 본 적 없는 새로운 AI 모델을 만들어야 했죠.\u003c/p\u003e\n\u003cp\u003e융합된 지능이 이곳에 있습니다.\u003c/p\u003e\n\u003cp\u003e그리고 아마도 월-이도요.\u003c/p\u003e\n\u003ch2\u003e여기에서 함께 나와 AI에 대해 쉽게 배워보세요!\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e새로운 모델 클래스\u003c/h1\u003e\n\u003cp\u003e지난 6개월 동안 인류가 AI로 이룬 성과는 정말 놀라운 것입니다.\u003c/p\u003e\n\u003cp\u003e간단히 말해, 우리는 인간과 자연어를 통해 소통하는 기계를 누구나 이용할 수 있게 만들었습니다.\u003c/p\u003e\n\u003cp\u003e하지만 AI의 잠재력은 단순한 텍스트를 뛰어 넘습니다. 많은 연구자들이 더 큰 목표를 향해 눈치를 돌리고 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e다중 모드 구축을 향한 길\u003c/h2\u003e\n\u003cp\u003e최근 팟캐스트에서 AI 슈퍼히어로 앤드류 엔지가 컴퓨터 비전이라는 이미지를 처리하고 이해하기 위해 모델을 훈련시키는 AI 분야가 \"텍스트 프롬프팅\"보다 약 \"두 년 뒤\"라고 언급했습니다. 그러나 그는 그것이 해당 모델들과 동등한 혁명이 될 것으로 예상했습니다.\u003c/p\u003e\n\u003cp\u003e그가 \"텍스트 프롬프팅\"로 언급한 것은 다름 아닌 대형 언어 모델 또는 LLM(Large Language Models)로, 그 중 가장 대표적인 예가 ChatGPT입니다.\u003c/p\u003e\n\u003cp\u003e그러나 우리 모두가 알다시피, 우리가 능력 면에서 인간 수준의 기계를 구축하려면 우리인간이 다중 모드인것보다 훨씬 더 많은 것이 필요합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e일반적인 용어로 말하면, 우리는 세상에 대한 이해를 텍스트만으로 구축하지 않습니다. 우리에겐 눈이 있고, 귀가 있고, 촉감도 있죠... 우리의 모든 감각이 세상이 무엇인지를 파악하는 데 도움을 줍니다.\u003c/p\u003e\n\u003cp\u003e실제로 이러한 감각들은 우리가 어릴 때 세상에 대해 배우는 데 도움을 줍니다. 우리가 맨 처음 문장을 읽기 보다 훨씬 이전부터 말이죠.\u003c/p\u003e\n\u003cp\u003e그러므로 AI 연구자들이 텍스트뿐만 아니라 다른 것들도 처리하는 모델, 혹은 명확히 말하자면 멀티모달 모델을 만들고 싶어하는 것은 자연스러운 일입니다.\u003c/p\u003e\n\u003cp\u003e그리고 이에 대한 AI 공간에서 가장 흥미로운 혁신 중 하나는 시각-언어 모델 중 하나입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003eVLMs, 더 ‘인간적인’ 기계들\u003c/h2\u003e\n\u003cp\u003e비전-언어 모델(VLMs)은 OpenAI의 CLIP이나 Microsoft의 Kosmos와 같은 모델로, 이름에서 알 수 있듯이 텍스트뿐만 아니라 이미지도 처리하는 모델입니다.\u003c/p\u003e\n\u003cp\u003e예를 들어, OpenAI의 CLIP은 대조 손실 절차를 따라 이미지와 텍스트 사이에 얽혀 있는 임베딩 공간을 생성함으로써 작동합니다(간단히 말하면 동일한 것을 설명하는 이미지와 텍스트를 모아주는 것이 중요하며 Dall-E와 같은 확산 모델을 구축하는 데 필수적입니다). 그러나 Kosmos와 같은 모델은 아직 탐험하지 않은 잠재력을 보여주는 모델입니다.\u003c/p\u003e\n\u003cp\u003eKosmos의 작동 방식은 매우 간단합니다. 이미지나 텍스트(또는 둘 다)를 보내면, 이에 기반하여 텍스트를 제공합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e참고로, Carnegie Mellon University에서 개발한 이름이 GILL인 또 다른 VLM을 아래에서 확인할 수 있습니다. 시각적 프롬프팅이 정확히 무엇인지 명확하게 이해할 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1056/1*IXxrTUmN37X9vs8dqK85-w.gif\" alt=\"GILL\"\u003e\u003c/p\u003e\n\u003cp\u003e하지만 이제 Google은 한 단계 더 나아가고 있습니다.\u003c/p\u003e\n\u003cp\u003e그들은 이러한 모델을 현실 세계로 가져오고 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e로봇공학을 더 나은 수준으로 발전시키려는 시도 중에, Google Deepmind는 깨달음을 얻었습니다:\u003c/p\u003e\n\u003cp\u003e그리고 그것으로, Google은 인공지능 로봇학의 새로운 핵심 요소인 Vision-language-action 모델(VLAs)을 만들었습니다.\u003c/p\u003e\n\u003cp\u003e간단히 말해, VLAs는 로봇이 이후 이동을 수행하는 데 사용할 수 있는 동작을 출력할 수 있는 모델입니다.\u003c/p\u003e\n\u003cp\u003e하지만 그것들이 정말 무엇이며, 왜 그렇게 혁신적인 것일까요?\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003eWall-e, 그게 진짜야?\u003c/h2\u003e\n\u003cp\u003e로봇 공학 분야에서 연구자들을 미치게 만드는 것이 있다면, 그것은 분명히 데이터일 겁니다.\u003c/p\u003e\n\u003cp\u003e사실 데이터의 부재일 때든요.\u003c/p\u003e\n\u003cp\u003eAI 모델이 데이터의 양과 질에 완전히 의존한다는 점을 고려할 때, 후자는 가능했지만 전자는 정말 악몽이었어요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e따라서, 로봇공학을 더 나은 수준으로 발전시키기 위해 구글이 베팅을 했습니다:\u003c/p\u003e\n\u003cp\u003e그들은 웹 규모의 텍스트와 이미지 데이터에 접근할 수 있는 VLM(범용 언어 모델)을 사용해 이러한 모델이 학습한 표현을 로봇으로 전이할 수 있다고 가설을 세웠습니다.\u003c/p\u003e\n\u003cp\u003e간단히 말해서, 로봇 데이터를 기반으로 세계에 대한 고수준 의미 지식을 갖춘 로봇을 만드는 것이 이루기 어려운 일이라면, 기존의 VLM 모델을 사용하고 고품질의 로봇 데이터로 적합화시키는 것은 어떨까요?\u003c/p\u003e\n\u003cp\u003e그리고 이제 이를 VLAs(로봇 언어 모델)라고 부를 수 있게 된 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e행동 예측기\u003c/h2\u003e\n\u003cp\u003eLLMs나 VLMs와 마찬가지로 VLAs도 동일한 일을 수행합니다. 토큰을 예측합니다.\u003c/p\u003e\n\u003cp\u003e그러나 ChatGPT와 같이 텍스트 토큰을 예측하는 대신, RT-2와 같은 VLAs는 로봇이 수행해야 할 작업을 카메라 관측에 기반하여 예측할 수도 있습니다. 아래 이미지에서 확인할 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e얻은 결과는 우리에게 매우 중요한 두 가지 교훈을 알려줍니다.\u003c/p\u003e\n\u003ch2\u003e일반화와 발생\u003c/h2\u003e\n\u003cp\u003eAI로봇 기술의 최첨단인 RT-2를 평가할 때 결과는 매우 명확합니다(RT-2 모델은 녹색과 파란색으로 표시됨):\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_2.png\" alt=\"RT-2 모델\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e안녕하세요! 항상 그렇듯이, 요점은 세부사항에 있습니다. RT-2를 주의 깊게 관찰하면, 이는 로봇공학의 가장 어려운 두 가지 임무, 즉 일반화와 신흥성에 뛰어난 성과를 보여줍니다.\u003c/p\u003e\n\u003cp\u003e간단히 말하자면, RT-2는 교육 중 본 적이 없는 상황, 예를 들어 보이지 않는 물체나 환경과 같은 상황에서도 잘 수행하는 능력(일반화)과 VLM 덕분에 얻은 언어 지식의 규모로 학습한 새로운 예상치 못한 능력(신흥성)을 펼쳐냈습니다.\u003c/p\u003e\n\u003cp\u003e가장 놀라운 점은: 이것이 첫 번째 추론이 가능한 로봇이라는 것입니다.\u003c/p\u003e\n\u003cp\u003e이 점을 증명하기 위해, 구글이 진행한 6,000건 이상의 시험 중에서 몇 가지 인상적인 행동이 포함되어 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_3.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e한 예를 들어 ‘추론’으로 볼 수 있는 것을 분석해 봅시다. 맨 위 두 번째 왼쪽을 보면, 로봇은 두 번째 과자봉지의 나쁜 위치를 이해하여 요청에 완벽히 대답할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이 작업을 수행하려면, 모델은 카메라로 촬영된 이미지의 밀도가 높은 의미 지식이 필요했습니다. 즉, 탁자 끝에 있는 봉지가 떨어질 수 있다는 것을 이해하면서, 다른 봉지는 그렇지 않다는 것을 알아냈습니다.\u003c/p\u003e\n\u003cp\u003e비슷하게, 아래 오른쪽 예에서 모델은 당나귀와 문어의 차이를 이해할 뿐만 아니라, “육지 동물”이 당나귀를 의미한다는 것을 생각할 수도 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e더 정량적인 관점에서 구글은 모델을 세 가지 기준으로 평가했습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e심볼 이해\u003c/li\u003e\n\u003cli\u003e추론\u003c/li\u003e\n\u003cli\u003e인간 인식\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e놀랍게도, RT-2는 이 모든 면에서 우수하게 성과를 거두었으며, 다음에 나온 예시와 같은 동작을 수행할 수 있는 능력을 입증했습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E_4.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e인상적인 점은 RT-2가 알 수 없는 상황에서 잘 작동할 수 있었던 것처럼, 위의 이미지들도 새로운 업무를 수행할 수 있는 능력을 보여줍니다. 다시 말해, 시각 언어 모델을 추가하더라도 새로운 로봇 동작을 생성하는 데는 한계가 있었지만 (논문에서 인정함), 로봇에 풍부한 의미 지식을 전달하여 배치, 물체 인식, 논리 추론과 같은 복잡하고 신흥 개념에 대해 훨씬 더 인식력을 갖도록 만들었습니다.\u003c/p\u003e\n\u003ch1\u003e혁신의 바퀴는 계속 회전합니다\u003c/h1\u003e\n\u003cp\u003eRT-2를 본 이후에는 내년 말까지 세계 각국의 제조업체가 이러한 로봇을 사용하여 프로세스를 개선하는 것이 놀라운 일이 아닐 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e인공지능 로봇들은 주변 환경에 대해 훨씬 더 인식력을 가지고 있습니다. 그들의 신경망 속에 점점 더 복잡한 세계 모델을 구축하여 하루가 다르게 진화하고 있는데, 이는 수천 년이 걸린 인간들의 진화에 의해 이루어졌던 것과 매우 가까워지고 있습니다.\u003c/p\u003e\n\u003cp\u003e그러나 많은 사람들은 이러한 모델들이 단순한 확률론적 앵무새에 불과하다고 주장할 것입니다. 이들은 그저 \"지능적인 활동\"을 단순히 암기하는 것으로 여기는 것이죠.\u003c/p\u003e\n\u003cp\u003e그러나 가장 이상하고 예상치 못한 경우에도 \"지능적으로\" 행동하는 기계들을 보면, 이 모델들이 자신이 하는 일을 실제로 이해한다고 주장하지 않을 수 있는 것이 정말로 도전이 됩니다. 이제 이러한 모델들은 \"체화된 지능\"이라고 묘사하는 것과 같이 지능적인 물리적 행동을 수행할 수 있는 조건에 이르렀습니다.\u003c/p\u003e\n\u003cp\u003eRT-2에 관한 것을 생각해보면, 이는 바로 진행 중인 로봇 기술이 미래의 몇 달 동안 어떻게 진화할지에 대한 단순한 꼭대기에 지나지 않을 것입니다. 그래서 아마도 영화 '월-E'가 우리 삶에서 생각했던 것보다 그렇게 멀리 떨어져 있지 않다는 생각이 들 수도 있겠네요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e프레스 릴리스와 연구 논문 링크입니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-22-RT-2GooglesNewBreakthroughToBuildWall-E"},"buildId":"kkckKPyxcjJp_o8wb2unW","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>