<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>에이전트 AI의 핵심 과제인 계획 문제 강화학습으로 해결하는 방법 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="에이전트 AI의 핵심 과제인 계획 문제 강화학습으로 해결하는 방법 | itposting" data-gatsby-head="true"/><meta property="og:title" content="에이전트 AI의 핵심 과제인 계획 문제 강화학습으로 해결하는 방법 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning" data-gatsby-head="true"/><meta name="twitter:title" content="에이전트 AI의 핵심 과제인 계획 문제 강화학습으로 해결하는 방법 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-23 19:42" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">에이전트 AI의 핵심 과제인 계획 문제 강화학습으로 해결하는 방법</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="에이전트 AI의 핵심 과제인 계획 문제 강화학습으로 해결하는 방법" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 23, 2024</span><span class="posts_reading_time__f7YPP">12<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>창의력을 발휘하여 복잡한 비즈니스 전략 문제를 해결하기 위해 원활하게 협업하는 AI 에이전트 팀을 상상해보세요. 시장 동향을 조사하는 한 명의 에이전트, 재무 데이터를 분석하는 다른 한 명, 그리고 권고 사항을 준비하는 세 번째 에이전트가 모두 공통 목표를 향해 노력하고 있습니다.</p>
<p>이 협력적 인 인공 지능의 논리, 즉 앤젠틱 AI를 알아보면 자동화와 문제 해결의 다음 단계를 나타냅니다. AI 시스템이 더욱 정교해지면서 미리 정의된 고정적인 프로세스를 벗어나 유연성, 적응력 및 AI 에이전트 간의 팀워크를 받아들이는 데 관심이 증가하고 있습니다.</p>
<p>앤젠틱 AI는 기존의 전통적인 자동화 기술로 해결하기 어려웠던 복잡한, 개방형 작업을 자동화하는데 큰 약속을 합니다. 복잡한 문제를 전문화된 역할로 분해하고 개별 AI 에이전트의 고유한 능력을 활용함으로써, 다양한 에이전트 시스템은 이전에 상상도 못 했던 방식으로 지능적인 자동화를 조율할 수 있습니다. CrewAI, Langraph, Autogen과 같은 개척적인 프레임워크는 이 새로운 패러다임을 위한 길을 열며, 개발자가 복잡한 워크플로를 자율적으로 탐색하고 실행할 수 있는 AI 에이전트 팀을 디자인하고 배포할 수 있도록 도와주고 있습니다.</p>
<p>그러나 이 새로운 협업 AI 영역으로 나아가면 앤젠틱 시스템의 핵심에 있는 근본적인 도전 과제를 마주하게 됩니다: 계획.</p>
<div class="content-ad"></div>
<p>AI 에이전트들이 효과적으로 행동을 계획하고 서로 협력하며 동적이고 개방적인 환경에서 자신들의 전략을 적응시킬 수 있게 하는 방법은 무엇일까요?</p>
<p>이 문서는 계획이 에이전트 AI의 핵심 과제이며 강화 학습(RL)이 이 중요한 문제에 대한 유망한 해결책을 제시한다고 주장합니다.</p>
<p>다음 섹션에서는 에이전트 AI의 부상과 주요 원칙에 대해 탐구하고, 이러한 시스템에서 계획이 이러한 의미 있는 과제로 작용하는 이유를 설명하며, 강화 학습 기법이 이러한 어려움을 해결할 수 있는 방법을 살펴볼 것입니다.</p>
<p>에이전트 AI에서 계획과 강화 학습의 상호작용을 이해함으로써, 지능적 자동화와 협력적인 인공지능의 미래에 대한 중요한 통찰을 얻을 수 있습니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning_0.png" alt="Planning as the Core Challenge in Agentic AI: Solving it with Reinforcement Learning"></p>
<h1>Agentic AI의 부상</h1>
<p>Agentic AI는 인공지능 시스템을 개념화하고 구현하는 방식에서 패러다임이 바뀌었다. 핵심적으로, Agentic AI는 자율적인 AI 에이전트들이 복잡하고 개방적인 과제에 대처하기 위해 팀 또는 "크루"로 함께 일하는 모습을 상상한다. 이 접근 방식은 단일 모델 AI 시스템의 제약을 넘어서 전문화와 협력의 힘을 활용하여 더 정교하고 유연한 문제 해결 능력을 실현한다.</p>
<p>이 Agentic AI 혁명의 전선에는 다수의 에이전트들 사이의 협력에 대한 독특한 접근 방식을 제공하는 여러 중요한 프레임워크들이 등장했다:</p>
<div class="content-ad"></div>
<ul>
<li>CrewAI: 이 프레임워크는 특정 역할을 갖는 AI 팀을 설계할 수 있게 해 주어, 그들이 특정 작업에 따라 선별된 연구 및 분석 도구 세트를 갖추도록 돕습니다.</li>
<li>Langraph: Langraph는 더 구조화된 방식을 채택하여, 명시적인 방향 그래프를 사용하여 에이전트 간의 작업 흐름을 정의합니다. 이를 통해 개발자들은 에이전트 조정과 작업 할당에 대해 세밀한 제어를 할 수 있습니다.</li>
<li>Autogen: Autogen은 에이전트 간의 다중 대화로부터 발생하는 신생 작업 흐름에 의존하여, 보다 동적이고 적응적인 협업 패턴을 가능하게 합니다.</li>
</ul>
<p>이 프레임워크들은 구체적인 구현에서 차이가 있지만, 모두 에이전틱 AI 접근 방식을 정의하는 중심 원칙을 공유합니다:</p>
<p>전문화와 협업: 이러한 시스템 전반에 걸쳐 두드러지는 공통점 중 하나는 다수의 특화된 에이전트를 활용하여 스스로 작업하는 방식입니다. 단일 대형 모델에 의존하는 대신, 에이전틱 AI는 작업을 하위 작업으로 분해하여 각각 다른 역할과 기술을 갖춘 에이전트에 위임합니다. 이러한 전문화는 각 에이전트가 자신의 전문 분야에 집중할 수 있도록 하고, 협업은 팀이 어떤 개별 에이전트에겐 도전적일 수 있는 문제들을 해결할 수 있도록 돕습니다.</p>
<p>예를 들어 채용 상황에서, 크루는 기술 직군 연구, 인적 프로필 엔지니어링, 이력서 전략 및 면접 준비에 특화된 에이전트로 구성될 수 있습니다. 이러한 특화된 에이전트들이 함께 작업하여 단일 일반적 AI보다 개인을 고용 전 과정에서 효과적으로 안내할 수 있습니다.</p>
<div class="content-ad"></div>
<p>언어 모델과 외부 도구의 활용: 에이전트 AI 시스템에서 또 다른 중요한 패턴은 각 에이전트를 뒷받침하는 "두뇌"로서 대형 언어 모델(LLMs)을 사용하는 것입니다. 이러한 미리 학습된 모델은 에이전트가 열린 대화를 할 수 있게 하며 자연어 질의를 해석하고 유창한 응답을 생성하며 판단을 내릴 수 있도록 합니다.</p>
<p>그러나 에이전트 AI는 언어 모델만을 의존하지는 않습니다. 에이전트의 지식을 기반을 다지고 그들의 능력을 확장하기 위해, 이러한 시스템은 또한 외부 도구와 데이터 소스에 연결합니다. 웹에서 단락을 검색하거나 구조화된 데이터베이스를 질의하거나 타사 API를 호출하는 등의 방식으로, 에이전트들은 실제 세계 정보를 활용하여 자신들의 결정과 행동에 영감을 얻습니다.</p>
<p>이러한 언어적 유연성과 외부 기반의 결합으로 인해 에이전트 AI 시스템은 넓은 세계로부터 통찰을 얻으면서 일관된 대화를 유지할 수 있습니다. 이는 인간이 언어를 지식과 행동의 관문으로 활용하는 방식을 재현하는 데 필수적인 한 걸음입니다.</p>
<p>에이전트 상태 및 워크플로우 관리: 에이전트 AI 설계의 가장 다양한 측면은 플랫폼이 에이전트 팀의 상태와 워크플로우 오케스트레이션을 어떻게 다루는지입니다. 에이전트 작업은 종종 다수의 단계와 에이전트 출력 간의 의존성을 포함하므로 일관된 전역 상태와 제어 흐름을 유지하는 것이 중요합니다.</p>
<div class="content-ad"></div>
<p>이 도전 과제에 대한 접근 방식은 플랫폼마다 다양합니다. Langraph는 워크플로우를 정의하기 위해 명시적인 방향 그래프를 사용하여 개발자에게 세밀한 제어를 제공합니다. Autogen은 에이전트 간의 멀티턴 대화에서 발생하는 신흥 워크플로에 더 의존합니다. CrewAI는 상호 작용을 안내하는 고수준 태스크 플로우를 갖추고 있지만 에이전트들이 서브태스크를 자율적으로 위임하고 응답할 수 있는 유연성을 가지고 있습니다.</p>
<p>이러한 차이점에도 불구하고, 에이전트 상태 및 워크플로우 관리를 위한 일관된 우선순위 목록이 도출됩니다:</p>
<ul>
<li>에이전트가 시간이 지남에 따라 다른 에이전트들의 작업 및 결정을 발전시킬 수 있는 메커니즘 제공</li>
<li>태스크 분할 및 에이전트 조정 패턴의 유연한 정의 가능</li>
<li>에이전트 역할, 도구 및 위임 권한의 태스크별 맞춤화 허용</li>
<li>예외 처리 및 에이전트 출력 간 비선형 종속성 그래프 우아하게 처리</li>
</ul>
<p>보다시피, 에이전트 AI의 부상은 유연하고 지능적인 자동화의 엄청난 잠재력을 가져옵니다. 특화, 협업, 외부 데이터를 기반으로 한 언어 모델의 강점을 활용하여 이러한 시스템은 기존의 전통적인 AI 접근 방식으로는 이루기 힘든 복잡하고 개방적인 작업에 대처할 수 있습니다.</p>
<div class="content-ad"></div>
<p>그러나 이러한 잠재력은 상당한 도전과 함께 옵니다. 그 중에서도 가장 중요한 것은 계획 문제입니다. 어떻게 하면 다양한 AI 에이전트 팀이 효과적으로 행동을 조정하고 불확실성 하에서 결정을 내리며, 동적 환경에서 전략을 적응할 수 있도록 할 수 있을까요? 이것이 에이전트 AI 시스템의 핵심 도전에 대한 핵심을 담고 있습니다.</p>
<h1>핵심 도전으로서의 계획</h1>
<p>에이전트 AI 시스템이 복잡성과 능력을 키우면 효과적인 계획의 필요성이 점점 더 중요해집니다. 이 문맥에서의 계획은 AI 에이전트들이 목표를 달성하기 위해 행동 순서를 결정하고, 다른 에이전트들과 협력하며, 변화하는 상황에 적응하는 과정을 말합니다. 계획은 지적 행동의 기본적인 측면이지만, 에이전트 AI의 영역에서 특히 어려운 도전을 제기합니다.</p>
<p>왜 계획이 특히 복잡한가요, 특히 다중 에이전트 시나리오에서? 이러한 어려움에 기여하는 몇 가지 주요 요소가 있습니다:</p>
<div class="content-ad"></div>
<ul>
<li>고차원 상태 및 행동 공간: 에이전틱 인공지능에서 상태 공간(환경 및 에이전트의 모든 가능한 구성)과 행동 공간(에이전트가 취할 수 있는 모든 가능한 행동)은 매우 크고 복잡합니다. 이는 각각의 능력과 잠재적 행동을 갖는 여러 에이전트가 상호작용하는 경우 조합 폭발로 인한 것입니다.</li>
<li>부분 관측성: 에이전트들은 종종 환경의 상태와 다른 에이전트의 행동에 대해 불완전한 정보를 갖습니다. 이러한 불확실성으로 인해 행동의 결과를 예측하고 효과적으로 계획하기 어려워집니다.</li>
<li>비정상적인 환경: 다중 에이전트 시스템에서는 환경이 에이전트가 행동을 취하고 서로 상호작용함에 따라 지속적으로 변화합니다. 이러한 비정상성은 시간이 지남에 따라 작용의 효과가 일관되지 않아 계획 과정을 복잡하게 만듭니다.</li>
<li>장기 의존성: 에이전틱 AI의 많은 작업은 단계 간에 의존성을 가진 장기적인 행동 시퀀스를 필요로 합니다. 이러한 확장된 시간 경계를 통해 계획을 수행하는 것은 계산적으로 어려우며 즉시적 보상과 장기적 목표를 균형있게 유지해야 합니다.</li>
<li>조정 및 통신 부담: 다중 에이전트 시스템에서 효과적인 계획은 에이전트 간의 조정이 필요하며 이는 의사 결정 과정에서 추가 복잡성과 병목현상을 초래할 수 있습니다.</li>
</ul>
<p>이러한 도전에 대처하기 위해 연구자들은 에이전틱 AI의 계획 문제를 마르코프 결정 과정(MDP)으로 정의하고 있습니다. MDP는 상황에 따라 결과가 일부적으로 무작위이고 일부적으로 의사 결정자의 통제 아래 있는 상황에서 의사 결정을 모델링하기 위한 수학적인 프레임워크를 제공합니다.</p>
<p>에이전틱 AI의 맥락에서 MDP의 구성 요소를 다음과 같이 정의할 수 있습니다:</p>
<ul>
<li>상태 공간 (S): 모든 가능한 사고 과정 및 환경 구성의 공간</li>
<li>동작 공간 (A): 사고나 문서 검색의 모든 가능한 조합</li>
<li>전이 역학 (P): 이전 사고와 행동을 기반으로 새로운 사고가 생성되는 방법</li>
<li>보상 함수 (R): 답변의 품질이나 목표에 대한 진전을 평가하는 것</li>
<li>할인 계수 (γ): 단기 vs. 장기적 보상의 우선순위</li>
<li>문제 기간 (T): 허용되는 추론 단계의 최대 수</li>
</ul>
<div class="content-ad"></div>
<p>계획 문제를 MDP로 프레임화함으로써, 강화 학습 분야의 다양한 기술을 활용하여 AI의 계획 과제를 해결할 수 있습니다. 그러나 이 정식화는 계획 과정에서 근본적인 긴장을 강조하기도 합니다: 탐험-활용 딜레마.</p>
<p>탐험-활용 딜레마는 새로운 것을 탐색하거나 잘 알려진 좋은 솔루션을 활용하는 사이의 교환 비용을 가리킵니다. 에이전트 AI 계획의 맥락에서 이것은 다음과 같은 균형으로 나타납니다:</p>
<ul>
<li>탐험: 새로운 사고 조합을 시도하거나 다양한 문서를 검색하거나 혁신적인 추론 방향을 추구하여 중요한 솔루션에 이르는 일들에 대한 베스트.</li>
<li>활용: 이미 알려진 효과적인 전략에 초점을 맞추거나 성공적인 사고 과정을 발전시키거나 최대의 즉각적 보상을 위해 기존 솔루션을 정제하는 일에 베스트.</li>
</ul>
<p>탐험과 활용 사이의 적절한 균형을 찾는 것은 에이전트 AI 시스템에서의 효과적인 계획에 중요합니다. 탐험이 과도하면 낭비되는 컴퓨팅 자원과 일관성 없는 성능을 야기할 수 있으며, 활용이 지나치다면 최적의 솔루션을 허술하게 만들거나 새로운 상황에 적응하지 못하는 문제를 야기할 수 있습니다.</p>
<div class="content-ad"></div>
<p>전통적인 계획 접근 방식인 상징적 AI나 철저한 검색을 기반으로 한 방법은 종종 에이전틱 AI의 맥락에서 이러한 도전 과제를 해결하는 데 어려움을 겪는다. 이러한 방법들은 일반적으로 환경의 완전한 지식, 결정론적 행동 결과, 명확히 정의된 목표 상태에 의존하는데, 이는 에이전틱 AI가 활동하는 복잡하고 불확실하며 개방적인 도메인에서 거의 적용되지 않는 가정들이다.</p>
<p>대신 필요한 건 유연하고 적응적인 계획 접근 방식으로, 다음과 같은 기능을 갖추어야 한다:</p>
<ul>
<li>고차원 상태 및 행동 공간을 효율적으로 처리</li>
<li>부분 관찰 가능성과 불확실성 다루기</li>
<li>비정상적인 환경에 적응하기</li>
<li>복잡한 종속성이 있는 긴 시간 범위에 계획 수립</li>
<li>탐색과 활용을 동적으로 균형있게 유지</li>
<li>여러 전문화된 에이전트 간의 행동 조정</li>
</ul>
<p>여기서 강화 학습이 등장하여 에이전틱 AI 시스템이 제기하는 독특한 계획 도전 과제를 해결하기에 적합한 강력한 기법 세트를 제공한다.</p>
<div class="content-ad"></div>
<h1>강화 학습 및 고급 기술로서의 솔루션</h1>
<p>강화 학습(RL)은 에이전트형 AI에서 복잡한 계획 도전에 대처하는 유망한 접근법으로 부각되었습니다. RL은 에이전트가 환경과 상호 작용하면서 보상이나 처벌의 형태로 피드백을 받아 결정을 내리는 방식의 머신러닝 유형입니다.</p>
<p>이러한 학습 패러다임은 에이전트형 AI에서 계획 문제에 특히 적합한 이유가 여럿 있습니다:</p>
<ul>
<li>경험으로부터 학습: RL 에이전트들은 환경의 완전한 모델을 요구하지 않고 시행착오를 통해 최적의 전략을 학습할 수 있습니다. 이는 에이전트형 AI가 작동하는 복잡한, 부분 관측 가능한 도메인에서 중요합니다.</li>
<li>탐험과 이용 사이의 균형 유지: RL 알고리즘에는 탐사-이용 교환을 관리하는 내장 기구가 있어, 에이전트들이 새로운 전략을 발견하면서도 알려진 좋은 해결책을 활용할 수 있게 합니다.</li>
<li>불확실성 다루기: RL 방법은 확률적 환경에서 작동하도록 설계되어, 다중 에이전트 시스템에 내재된 불확실성에 탄력적으로 대처할 수 있습니다.</li>
<li>장기 계획: 많은 RL 알고리즘은 명시적으로 장기 보상을 최적화하기 위해 설계되어 있어, 긴 시간 대역으로 계획을 수립하고 행위 간의 복잡한 종속성을 포착할 수 있습니다.</li>
<li>적응성: RL 에이전트들은 새로운 경험을 기반으로 전략을 지속적으로 업데이트할 수 있어, 변동성 있는 환경에 적합합니다.</li>
</ul>
<div class="content-ad"></div>
<p>특히 계획 도전 과제를 해결하는 데 유망한 강화 학습 기법 중 하나는 몬테 카를로 트리 탐색(Monte Carlo Tree Search, MCTS)입니다. MCTS는 휴리스틱 탐색 알고리즘이며, 랜덤 샘플링과 트리 탐색을 결합하여 복잡한 공간에서 결정을 내립니다. 이 기법은 다양한 분야에 성공적으로 적용되었으며, 알파고(AlphaGo)와 같은 게임 플레이 인공지능에서 사용되었습니다.</p>
<p>에이전틱 인공지능 계획의 맥락에서, MCTS는 가능한 사고 과정과 행동 시퀀스의 광범위한 공간을 효율적으로 탐색하는 데 사용될 수 있습니다. MCTS의 주요 단계는 다음과 같습니다:</p>
<ul>
<li>선택(Selection): 루트에서 시작하여 탐색하는 동안 탐색과 활용을 균형있게 고려하는 트리 정책(예: 상한 신뢰 경계)을 사용합니다.</li>
<li>확장(Expansion): 새로운 자식 노드를 추가하여 트리를 확장합니다.</li>
<li>시뮬레이션(Simulation): 새 노드에서 랜덤 시뮬레이션을 실행하여 값을 추정합니다.</li>
<li>역전파(Backpropagation): 루트에 되돌아가는 경로를 따라 노드 통계를 업데이트합니다.</li>
</ul>
<p>이러한 단계를 반복적으로 적용함으로써, MCTS는 검색 공간의 가장 유망한 지역에 계산 리소스를 집중할 수 있어 에이전틱 인공지능 계획에서 마주치는 고차원 상태 및 행동 공간에 적합합니다.</p>
<div class="content-ad"></div>
<p>다른 중요한 강화 학습 개념 중 하나는 에이전틱 AI 계획에 적용할 수 있는 Q-러닝입니다. Q-러닝은 모델이 없는 강화 학습 알고리즘으로, 주어진 상태에서 특정 행동을 취했을 때 기대되는 누적 보상(Q-값)을 추정하는 방법을 학습합니다. 에이전틱 AI 환경에서는 Q-러닝을 사용하여 다양한 사고 과정이나 문서 검색의 가치를 추정할 수 있습니다.</p>
<p>이 분야의 최근 발전은 이러한 기본적인 강화 학습 개념을 바탕으로 한 몇 가지 혁신적인 접근 방식의 발전을 이끌어내어, 에이전틱 AI 시스템에서 계획 및 추론의 특정 도전 과제를 해결하기 위한 기술적 발전을 이끌어내고 있습니다.</p>
<p>또한 특히 유망한 세 가지 혁신적 기술을 살펴보겠습니다:</p>
<h2>Q*: 딥러닝 계획을 통한 다단계 추론 개선</h2>
<div class="content-ad"></div>
<p>Wang 및 다른 사람(2024)이 소개한 Q* 프레임워크는 대형 언어 모델(Large Language Models, LLMs)의 다단계 추론 능력을 향상시키는 데 중요한 발전을 이끌어냅니다. Q<em>는 A</em> 검색의 능력을 결합하여 학습된 Q-value 모델로 LLMs을 복잡한 추론 작업 중에서 가장 유망한 다음 단계를 선택하도록 안내합니다.</p>
<p>Q*의 주요 특징은 다음과 같습니다:</p>
<ul>
<li>각 노드가 주어진 문제에 대한 부분 솔루션을 나타내는 그래프로 추론 프로세스를 모델링합니다.</li>
<li>A* 검색을 위한 학습된 Q-value 모델을 휴리스틱 함수로 사용하여, 전체 문제를 해결하는 데 각 잠재적인 다음 단계가 얼마나 유망한지를 추정합니다.</li>
<li>가능한 추론 경로의 방대한 공간을 효율적으로 탐색하기 위해 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)을 사용합니다.</li>
<li>LLM이 자체 정제된 답변에 대한 점수를 매기는 자가 평가 메커니즘을 통합하여 추론 프로세스를 지속적으로 향상시킬 수 있습니다.</li>
</ul>
<p>Q* 프레임워크는 에이전트 AI 계획에서 몇 가지 중요한 도전에 대처합니다:</p>
<div class="content-ad"></div>
<ul>
<li>긴 콘텍스트 처리: Q*는 전통적인 LLM의 고정된 콘텍스트 창의 제약을 극복하기 위해 지식 원본에서 대규모 문서 배치를 처리할 수 있습니다.</li>
<li>무관한 정보에 대한 견고성: 다양한 추론 분기를 탐색함으로써 Q*는 실패한 정보 검색 및 오도된 문서에 대해 저항력을 갖습니다.</li>
<li>적응성: 이 프레임워크는 기본 LLM의 작업별 특정 조정 없이 다양한 추론 작업에 적용할 수 있습니다.</li>
</ul>
<p>실험 결과에 따르면 Q*는 다양한 수학적 추론 및 코드 생성 작업에서 기준선 방법을 크게 앞섰으며, 지식 기반 AI 시스템의 계획 및 추론 능력을 향상시킬 잠재력을 입증했습니다.</p>
<h2>병렬 함수 호출을 위한 LLM 컴파일러</h2>
<p>Q*가 추론 프로세스 자체를 개선하는 데 초점을 맞추고 있을 때, LLM 컴파일러 접근 방식은 에이전틱 AI 계획의 또 다른 중요한 측면을 다룹니다: 병렬 함수 호출의 효율적인 조율. 이 기법은 고전 컴파일러 설계에서 영감을 받아 대규모 언어 모델에서 여러 함수 호출을 실행을 최적화하는 것을 목표로 합니다.</p>
<div class="content-ad"></div>
<p>LLM 컴파일러 방식의 주요 측면은 다음과 같습니다:</p>
<ul>
<li>사용자 입력을 상호 의존성을 가진 일련의 작업으로 자동 분해합니다.</li>
<li>독립적인 작업을 병렬로 실행하여 복잡한 워크플로에서 발생하는 지연 시간을 크게 감소시킵니다.</li>
<li>작업의 방향성 비순환 그래프(DAG)를 생성하는 계획 단계를 통해 효율적인 일정 계획 및 실행이 가능합니다.</li>
<li>외부 도구 및 API와 통합하여 LLM의 능력을 언어 처리 이상으로 확장합니다.</li>
</ul>
<p>LLM 컴파일러는 공별한 AI 계획에서 여러 중요한 도전 과제를 다룹니다:</p>
<ul>
<li>효율성: 병렬화 가능한 패턴을 식별하고 함수 호출 의존성을 관리함으로써, 컴파일러는 복잡한 작업의 지연 시간을 크게 줄일 수 있습니다.</li>
<li>확장성: 이 방식은 다수의 함수 호출과 데이터 의존성이 포함된 대규모 및 복잡한 작업을 다루도록 설계되었습니다.</li>
<li>유연성: 컴파일러는 다양한 종류의 LLM 및 작업 부하에 적응할 수 있어 다양한 AI 응용 프로그램에 대한 다재다능한 도구가 됩니다.</li>
</ul>
<div class="content-ad"></div>
<p>초기 결과에 따르면, LLM 컴파일러는 순차 실행 방법과 비교하여 상당한 속도 향상을 달성할 수 있다는 것이 밝혀졌습니다. 최대 3.7배의 대기 시간 개선과 일부 작업에서 최대 6.7배까지의 비용 절감이 가능합니다.</p>
<h2>수학 올림피아드 솔루션을 위한 몬테카를로 트리 자기 수정</h2>
<p>타 분야에서 MCTS의 성공을 바탕으로, 연구자들은 복잡한 수리 추론 작업, 특히 수학 올림피아드에서 마주하는 작업들을 처리하기 위해 특별히 개발된 몬테카를로 트리 자기 수정(MCTSr) 알고리즘을 개발했습니다.</p>
<p>MCTSr의 주요 기능은 다음과 같습니다:</p>
<div class="content-ad"></div>
<ul>
<li>대규모 언어 모델과 몬테카를로 트리 탐색을 통합하여 문제 해결 능력을 향상시킵니다.</li>
<li>선택, 자가 세부화, 자가 평가 및 역전파 단계를 포함하는 반복적인 과정입니다.</li>
<li>모델이 솔루션을 반복적으로 향상시킬 수 있는 피드백 안내형 세분화 과정입니다.</li>
<li>진정으로 개선된 솔루션이 높은 점수를 받도록 하는 엄격하고 비판적인 점수 매커니즘입니다.</li>
</ul>
<p>MCTSr은 수학적 추론과 계획에서 여러 가지 도전에 대응합니다:</p>
<ul>
<li>복잡한 다단계 문제 다루기: 이 알고리즘은 다단계 추론 단계와 전략적 사고가 필요한 복잡한 수학적 작업을 다루도록 설계되었습니다.</li>
<li>지속적인 개선: 자가 세부화 및 자가 평가 메커니즘을 통해 MCTSr은 솔루션 품질을 점진적으로 향상시킬 수 있습니다.</li>
<li>다양한 문제 유형에 대한 적응성: 이 프레임워크는 초등학교 산술부터 올림피아드 수준의 도전 과제까지 다양한 수학 영역에서 성공을 거두었습니다.</li>
</ul>
<p>실험 결과에서는 MCTSr이 LLaMA-3 8B와 같은 훨씬 작은 모델을 사용하여 수학 올림피아드 문제에서 GPT-4 수준의 성능을 달성할 수 있다는 것을 입증하였으며, 이는 인공지능 시스템의 추론 능력을 혁신적으로 향상시킬 잠재력을 보여줍니다.</p>
<div class="content-ad"></div>
<p>이 세 가지 접근법인 Q*, LLM Compiler 및 MCTSr은 에이전틱 AI의 계획 및 추론 기술의 최신 동향을 대표합니다. 이러한 방법들은 강화 학습 원칙과 혁신적인 탐색 및 최적화 전략을 결합하여 AI 주도 문제 해결에서 가능한 범위를 넓히고 있습니다.</p>
<p>그러나 이러한 고급 기술을 에이전틱 AI 계획에 적용하는 데는 다음과 같은 도전 과제가 있습니다:</p>
<ul>
<li>계산 복잡성: 이러한 방법들은 대부분 고도의 계산 과정을 통합하며, 대규모 응용 프로그램에는 리소스가 많이 필요할 수 있습니다.</li>
<li>탐험과 활용의 균형: 새로운 솔루션을 발견하고 기존의 좋은 전략을 활용하는 적절한 균형을 찾는 것은 여전히 까다로운 작업입니다.</li>
<li>해석 가능성: 이러한 시스템이 더 복잡해지면서 의사 결정 과정에서의 투명성과 해석 가능성을 보장하는 것이 점점 어려워지고 있습니다.</li>
<li>일반화: 이러한 방법은 특정 도메인에서 인상적인 결과를 보여주었지만, 다양한 작업 유형 간의 일반화 능력을 평가하기 위해 추가 연구가 필요합니다.</li>
</ul>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"에이전트 AI의 핵심 과제인 계획 문제 강화학습으로 해결하는 방법","description":"","date":"2024-06-23 19:42","slug":"2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning","content":"\n\n창의력을 발휘하여 복잡한 비즈니스 전략 문제를 해결하기 위해 원활하게 협업하는 AI 에이전트 팀을 상상해보세요. 시장 동향을 조사하는 한 명의 에이전트, 재무 데이터를 분석하는 다른 한 명, 그리고 권고 사항을 준비하는 세 번째 에이전트가 모두 공통 목표를 향해 노력하고 있습니다.\n\n이 협력적 인 인공 지능의 논리, 즉 앤젠틱 AI를 알아보면 자동화와 문제 해결의 다음 단계를 나타냅니다. AI 시스템이 더욱 정교해지면서 미리 정의된 고정적인 프로세스를 벗어나 유연성, 적응력 및 AI 에이전트 간의 팀워크를 받아들이는 데 관심이 증가하고 있습니다.\n\n앤젠틱 AI는 기존의 전통적인 자동화 기술로 해결하기 어려웠던 복잡한, 개방형 작업을 자동화하는데 큰 약속을 합니다. 복잡한 문제를 전문화된 역할로 분해하고 개별 AI 에이전트의 고유한 능력을 활용함으로써, 다양한 에이전트 시스템은 이전에 상상도 못 했던 방식으로 지능적인 자동화를 조율할 수 있습니다. CrewAI, Langraph, Autogen과 같은 개척적인 프레임워크는 이 새로운 패러다임을 위한 길을 열며, 개발자가 복잡한 워크플로를 자율적으로 탐색하고 실행할 수 있는 AI 에이전트 팀을 디자인하고 배포할 수 있도록 도와주고 있습니다.\n\n그러나 이 새로운 협업 AI 영역으로 나아가면 앤젠틱 시스템의 핵심에 있는 근본적인 도전 과제를 마주하게 됩니다: 계획.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI 에이전트들이 효과적으로 행동을 계획하고 서로 협력하며 동적이고 개방적인 환경에서 자신들의 전략을 적응시킬 수 있게 하는 방법은 무엇일까요?\n\n이 문서는 계획이 에이전트 AI의 핵심 과제이며 강화 학습(RL)이 이 중요한 문제에 대한 유망한 해결책을 제시한다고 주장합니다.\n\n다음 섹션에서는 에이전트 AI의 부상과 주요 원칙에 대해 탐구하고, 이러한 시스템에서 계획이 이러한 의미 있는 과제로 작용하는 이유를 설명하며, 강화 학습 기법이 이러한 어려움을 해결할 수 있는 방법을 살펴볼 것입니다.\n\n에이전트 AI에서 계획과 강화 학습의 상호작용을 이해함으로써, 지능적 자동화와 협력적인 인공지능의 미래에 대한 중요한 통찰을 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Planning as the Core Challenge in Agentic AI: Solving it with Reinforcement Learning](/assets/img/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning_0.png)\n\n# Agentic AI의 부상\n\nAgentic AI는 인공지능 시스템을 개념화하고 구현하는 방식에서 패러다임이 바뀌었다. 핵심적으로, Agentic AI는 자율적인 AI 에이전트들이 복잡하고 개방적인 과제에 대처하기 위해 팀 또는 \"크루\"로 함께 일하는 모습을 상상한다. 이 접근 방식은 단일 모델 AI 시스템의 제약을 넘어서 전문화와 협력의 힘을 활용하여 더 정교하고 유연한 문제 해결 능력을 실현한다.\n\n이 Agentic AI 혁명의 전선에는 다수의 에이전트들 사이의 협력에 대한 독특한 접근 방식을 제공하는 여러 중요한 프레임워크들이 등장했다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- CrewAI: 이 프레임워크는 특정 역할을 갖는 AI 팀을 설계할 수 있게 해 주어, 그들이 특정 작업에 따라 선별된 연구 및 분석 도구 세트를 갖추도록 돕습니다.\n- Langraph: Langraph는 더 구조화된 방식을 채택하여, 명시적인 방향 그래프를 사용하여 에이전트 간의 작업 흐름을 정의합니다. 이를 통해 개발자들은 에이전트 조정과 작업 할당에 대해 세밀한 제어를 할 수 있습니다.\n- Autogen: Autogen은 에이전트 간의 다중 대화로부터 발생하는 신생 작업 흐름에 의존하여, 보다 동적이고 적응적인 협업 패턴을 가능하게 합니다.\n\n이 프레임워크들은 구체적인 구현에서 차이가 있지만, 모두 에이전틱 AI 접근 방식을 정의하는 중심 원칙을 공유합니다:\n\n전문화와 협업: 이러한 시스템 전반에 걸쳐 두드러지는 공통점 중 하나는 다수의 특화된 에이전트를 활용하여 스스로 작업하는 방식입니다. 단일 대형 모델에 의존하는 대신, 에이전틱 AI는 작업을 하위 작업으로 분해하여 각각 다른 역할과 기술을 갖춘 에이전트에 위임합니다. 이러한 전문화는 각 에이전트가 자신의 전문 분야에 집중할 수 있도록 하고, 협업은 팀이 어떤 개별 에이전트에겐 도전적일 수 있는 문제들을 해결할 수 있도록 돕습니다.\n\n예를 들어 채용 상황에서, 크루는 기술 직군 연구, 인적 프로필 엔지니어링, 이력서 전략 및 면접 준비에 특화된 에이전트로 구성될 수 있습니다. 이러한 특화된 에이전트들이 함께 작업하여 단일 일반적 AI보다 개인을 고용 전 과정에서 효과적으로 안내할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n언어 모델과 외부 도구의 활용: 에이전트 AI 시스템에서 또 다른 중요한 패턴은 각 에이전트를 뒷받침하는 \"두뇌\"로서 대형 언어 모델(LLMs)을 사용하는 것입니다. 이러한 미리 학습된 모델은 에이전트가 열린 대화를 할 수 있게 하며 자연어 질의를 해석하고 유창한 응답을 생성하며 판단을 내릴 수 있도록 합니다.\n\n그러나 에이전트 AI는 언어 모델만을 의존하지는 않습니다. 에이전트의 지식을 기반을 다지고 그들의 능력을 확장하기 위해, 이러한 시스템은 또한 외부 도구와 데이터 소스에 연결합니다. 웹에서 단락을 검색하거나 구조화된 데이터베이스를 질의하거나 타사 API를 호출하는 등의 방식으로, 에이전트들은 실제 세계 정보를 활용하여 자신들의 결정과 행동에 영감을 얻습니다.\n\n이러한 언어적 유연성과 외부 기반의 결합으로 인해 에이전트 AI 시스템은 넓은 세계로부터 통찰을 얻으면서 일관된 대화를 유지할 수 있습니다. 이는 인간이 언어를 지식과 행동의 관문으로 활용하는 방식을 재현하는 데 필수적인 한 걸음입니다.\n\n에이전트 상태 및 워크플로우 관리: 에이전트 AI 설계의 가장 다양한 측면은 플랫폼이 에이전트 팀의 상태와 워크플로우 오케스트레이션을 어떻게 다루는지입니다. 에이전트 작업은 종종 다수의 단계와 에이전트 출력 간의 의존성을 포함하므로 일관된 전역 상태와 제어 흐름을 유지하는 것이 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 도전 과제에 대한 접근 방식은 플랫폼마다 다양합니다. Langraph는 워크플로우를 정의하기 위해 명시적인 방향 그래프를 사용하여 개발자에게 세밀한 제어를 제공합니다. Autogen은 에이전트 간의 멀티턴 대화에서 발생하는 신흥 워크플로에 더 의존합니다. CrewAI는 상호 작용을 안내하는 고수준 태스크 플로우를 갖추고 있지만 에이전트들이 서브태스크를 자율적으로 위임하고 응답할 수 있는 유연성을 가지고 있습니다.\n\n이러한 차이점에도 불구하고, 에이전트 상태 및 워크플로우 관리를 위한 일관된 우선순위 목록이 도출됩니다:\n\n- 에이전트가 시간이 지남에 따라 다른 에이전트들의 작업 및 결정을 발전시킬 수 있는 메커니즘 제공\n- 태스크 분할 및 에이전트 조정 패턴의 유연한 정의 가능\n- 에이전트 역할, 도구 및 위임 권한의 태스크별 맞춤화 허용\n- 예외 처리 및 에이전트 출력 간 비선형 종속성 그래프 우아하게 처리\n\n보다시피, 에이전트 AI의 부상은 유연하고 지능적인 자동화의 엄청난 잠재력을 가져옵니다. 특화, 협업, 외부 데이터를 기반으로 한 언어 모델의 강점을 활용하여 이러한 시스템은 기존의 전통적인 AI 접근 방식으로는 이루기 힘든 복잡하고 개방적인 작업에 대처할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 이러한 잠재력은 상당한 도전과 함께 옵니다. 그 중에서도 가장 중요한 것은 계획 문제입니다. 어떻게 하면 다양한 AI 에이전트 팀이 효과적으로 행동을 조정하고 불확실성 하에서 결정을 내리며, 동적 환경에서 전략을 적응할 수 있도록 할 수 있을까요? 이것이 에이전트 AI 시스템의 핵심 도전에 대한 핵심을 담고 있습니다.\n\n# 핵심 도전으로서의 계획\n\n에이전트 AI 시스템이 복잡성과 능력을 키우면 효과적인 계획의 필요성이 점점 더 중요해집니다. 이 문맥에서의 계획은 AI 에이전트들이 목표를 달성하기 위해 행동 순서를 결정하고, 다른 에이전트들과 협력하며, 변화하는 상황에 적응하는 과정을 말합니다. 계획은 지적 행동의 기본적인 측면이지만, 에이전트 AI의 영역에서 특히 어려운 도전을 제기합니다.\n\n왜 계획이 특히 복잡한가요, 특히 다중 에이전트 시나리오에서? 이러한 어려움에 기여하는 몇 가지 주요 요소가 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 고차원 상태 및 행동 공간: 에이전틱 인공지능에서 상태 공간(환경 및 에이전트의 모든 가능한 구성)과 행동 공간(에이전트가 취할 수 있는 모든 가능한 행동)은 매우 크고 복잡합니다. 이는 각각의 능력과 잠재적 행동을 갖는 여러 에이전트가 상호작용하는 경우 조합 폭발로 인한 것입니다.\n- 부분 관측성: 에이전트들은 종종 환경의 상태와 다른 에이전트의 행동에 대해 불완전한 정보를 갖습니다. 이러한 불확실성으로 인해 행동의 결과를 예측하고 효과적으로 계획하기 어려워집니다.\n- 비정상적인 환경: 다중 에이전트 시스템에서는 환경이 에이전트가 행동을 취하고 서로 상호작용함에 따라 지속적으로 변화합니다. 이러한 비정상성은 시간이 지남에 따라 작용의 효과가 일관되지 않아 계획 과정을 복잡하게 만듭니다.\n- 장기 의존성: 에이전틱 AI의 많은 작업은 단계 간에 의존성을 가진 장기적인 행동 시퀀스를 필요로 합니다. 이러한 확장된 시간 경계를 통해 계획을 수행하는 것은 계산적으로 어려우며 즉시적 보상과 장기적 목표를 균형있게 유지해야 합니다.\n- 조정 및 통신 부담: 다중 에이전트 시스템에서 효과적인 계획은 에이전트 간의 조정이 필요하며 이는 의사 결정 과정에서 추가 복잡성과 병목현상을 초래할 수 있습니다.\n\n이러한 도전에 대처하기 위해 연구자들은 에이전틱 AI의 계획 문제를 마르코프 결정 과정(MDP)으로 정의하고 있습니다. MDP는 상황에 따라 결과가 일부적으로 무작위이고 일부적으로 의사 결정자의 통제 아래 있는 상황에서 의사 결정을 모델링하기 위한 수학적인 프레임워크를 제공합니다.\n\n에이전틱 AI의 맥락에서 MDP의 구성 요소를 다음과 같이 정의할 수 있습니다:\n\n- 상태 공간 (S): 모든 가능한 사고 과정 및 환경 구성의 공간\n- 동작 공간 (A): 사고나 문서 검색의 모든 가능한 조합\n- 전이 역학 (P): 이전 사고와 행동을 기반으로 새로운 사고가 생성되는 방법\n- 보상 함수 (R): 답변의 품질이나 목표에 대한 진전을 평가하는 것\n- 할인 계수 (γ): 단기 vs. 장기적 보상의 우선순위\n- 문제 기간 (T): 허용되는 추론 단계의 최대 수\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n계획 문제를 MDP로 프레임화함으로써, 강화 학습 분야의 다양한 기술을 활용하여 AI의 계획 과제를 해결할 수 있습니다. 그러나 이 정식화는 계획 과정에서 근본적인 긴장을 강조하기도 합니다: 탐험-활용 딜레마.\n\n탐험-활용 딜레마는 새로운 것을 탐색하거나 잘 알려진 좋은 솔루션을 활용하는 사이의 교환 비용을 가리킵니다. 에이전트 AI 계획의 맥락에서 이것은 다음과 같은 균형으로 나타납니다:\n\n- 탐험: 새로운 사고 조합을 시도하거나 다양한 문서를 검색하거나 혁신적인 추론 방향을 추구하여 중요한 솔루션에 이르는 일들에 대한 베스트.\n- 활용: 이미 알려진 효과적인 전략에 초점을 맞추거나 성공적인 사고 과정을 발전시키거나 최대의 즉각적 보상을 위해 기존 솔루션을 정제하는 일에 베스트.\n\n탐험과 활용 사이의 적절한 균형을 찾는 것은 에이전트 AI 시스템에서의 효과적인 계획에 중요합니다. 탐험이 과도하면 낭비되는 컴퓨팅 자원과 일관성 없는 성능을 야기할 수 있으며, 활용이 지나치다면 최적의 솔루션을 허술하게 만들거나 새로운 상황에 적응하지 못하는 문제를 야기할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n전통적인 계획 접근 방식인 상징적 AI나 철저한 검색을 기반으로 한 방법은 종종 에이전틱 AI의 맥락에서 이러한 도전 과제를 해결하는 데 어려움을 겪는다. 이러한 방법들은 일반적으로 환경의 완전한 지식, 결정론적 행동 결과, 명확히 정의된 목표 상태에 의존하는데, 이는 에이전틱 AI가 활동하는 복잡하고 불확실하며 개방적인 도메인에서 거의 적용되지 않는 가정들이다.\n\n대신 필요한 건 유연하고 적응적인 계획 접근 방식으로, 다음과 같은 기능을 갖추어야 한다:\n\n- 고차원 상태 및 행동 공간을 효율적으로 처리\n- 부분 관찰 가능성과 불확실성 다루기\n- 비정상적인 환경에 적응하기\n- 복잡한 종속성이 있는 긴 시간 범위에 계획 수립\n- 탐색과 활용을 동적으로 균형있게 유지\n- 여러 전문화된 에이전트 간의 행동 조정\n\n여기서 강화 학습이 등장하여 에이전틱 AI 시스템이 제기하는 독특한 계획 도전 과제를 해결하기에 적합한 강력한 기법 세트를 제공한다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 강화 학습 및 고급 기술로서의 솔루션\n\n강화 학습(RL)은 에이전트형 AI에서 복잡한 계획 도전에 대처하는 유망한 접근법으로 부각되었습니다. RL은 에이전트가 환경과 상호 작용하면서 보상이나 처벌의 형태로 피드백을 받아 결정을 내리는 방식의 머신러닝 유형입니다.\n\n이러한 학습 패러다임은 에이전트형 AI에서 계획 문제에 특히 적합한 이유가 여럿 있습니다:\n\n- 경험으로부터 학습: RL 에이전트들은 환경의 완전한 모델을 요구하지 않고 시행착오를 통해 최적의 전략을 학습할 수 있습니다. 이는 에이전트형 AI가 작동하는 복잡한, 부분 관측 가능한 도메인에서 중요합니다.\n- 탐험과 이용 사이의 균형 유지: RL 알고리즘에는 탐사-이용 교환을 관리하는 내장 기구가 있어, 에이전트들이 새로운 전략을 발견하면서도 알려진 좋은 해결책을 활용할 수 있게 합니다.\n- 불확실성 다루기: RL 방법은 확률적 환경에서 작동하도록 설계되어, 다중 에이전트 시스템에 내재된 불확실성에 탄력적으로 대처할 수 있습니다.\n- 장기 계획: 많은 RL 알고리즘은 명시적으로 장기 보상을 최적화하기 위해 설계되어 있어, 긴 시간 대역으로 계획을 수립하고 행위 간의 복잡한 종속성을 포착할 수 있습니다.\n- 적응성: RL 에이전트들은 새로운 경험을 기반으로 전략을 지속적으로 업데이트할 수 있어, 변동성 있는 환경에 적합합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특히 계획 도전 과제를 해결하는 데 유망한 강화 학습 기법 중 하나는 몬테 카를로 트리 탐색(Monte Carlo Tree Search, MCTS)입니다. MCTS는 휴리스틱 탐색 알고리즘이며, 랜덤 샘플링과 트리 탐색을 결합하여 복잡한 공간에서 결정을 내립니다. 이 기법은 다양한 분야에 성공적으로 적용되었으며, 알파고(AlphaGo)와 같은 게임 플레이 인공지능에서 사용되었습니다.\n\n에이전틱 인공지능 계획의 맥락에서, MCTS는 가능한 사고 과정과 행동 시퀀스의 광범위한 공간을 효율적으로 탐색하는 데 사용될 수 있습니다. MCTS의 주요 단계는 다음과 같습니다:\n\n- 선택(Selection): 루트에서 시작하여 탐색하는 동안 탐색과 활용을 균형있게 고려하는 트리 정책(예: 상한 신뢰 경계)을 사용합니다.\n- 확장(Expansion): 새로운 자식 노드를 추가하여 트리를 확장합니다.\n- 시뮬레이션(Simulation): 새 노드에서 랜덤 시뮬레이션을 실행하여 값을 추정합니다.\n- 역전파(Backpropagation): 루트에 되돌아가는 경로를 따라 노드 통계를 업데이트합니다.\n\n이러한 단계를 반복적으로 적용함으로써, MCTS는 검색 공간의 가장 유망한 지역에 계산 리소스를 집중할 수 있어 에이전틱 인공지능 계획에서 마주치는 고차원 상태 및 행동 공간에 적합합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다른 중요한 강화 학습 개념 중 하나는 에이전틱 AI 계획에 적용할 수 있는 Q-러닝입니다. Q-러닝은 모델이 없는 강화 학습 알고리즘으로, 주어진 상태에서 특정 행동을 취했을 때 기대되는 누적 보상(Q-값)을 추정하는 방법을 학습합니다. 에이전틱 AI 환경에서는 Q-러닝을 사용하여 다양한 사고 과정이나 문서 검색의 가치를 추정할 수 있습니다.\n\n이 분야의 최근 발전은 이러한 기본적인 강화 학습 개념을 바탕으로 한 몇 가지 혁신적인 접근 방식의 발전을 이끌어내어, 에이전틱 AI 시스템에서 계획 및 추론의 특정 도전 과제를 해결하기 위한 기술적 발전을 이끌어내고 있습니다.\n\n또한 특히 유망한 세 가지 혁신적 기술을 살펴보겠습니다:\n\n## Q*: 딥러닝 계획을 통한 다단계 추론 개선\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nWang 및 다른 사람(2024)이 소개한 Q* 프레임워크는 대형 언어 모델(Large Language Models, LLMs)의 다단계 추론 능력을 향상시키는 데 중요한 발전을 이끌어냅니다. Q*는 A* 검색의 능력을 결합하여 학습된 Q-value 모델로 LLMs을 복잡한 추론 작업 중에서 가장 유망한 다음 단계를 선택하도록 안내합니다.\n\nQ*의 주요 특징은 다음과 같습니다:\n\n- 각 노드가 주어진 문제에 대한 부분 솔루션을 나타내는 그래프로 추론 프로세스를 모델링합니다.\n- A* 검색을 위한 학습된 Q-value 모델을 휴리스틱 함수로 사용하여, 전체 문제를 해결하는 데 각 잠재적인 다음 단계가 얼마나 유망한지를 추정합니다.\n- 가능한 추론 경로의 방대한 공간을 효율적으로 탐색하기 위해 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)을 사용합니다.\n- LLM이 자체 정제된 답변에 대한 점수를 매기는 자가 평가 메커니즘을 통합하여 추론 프로세스를 지속적으로 향상시킬 수 있습니다.\n\nQ* 프레임워크는 에이전트 AI 계획에서 몇 가지 중요한 도전에 대처합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 긴 콘텍스트 처리: Q*는 전통적인 LLM의 고정된 콘텍스트 창의 제약을 극복하기 위해 지식 원본에서 대규모 문서 배치를 처리할 수 있습니다.\n- 무관한 정보에 대한 견고성: 다양한 추론 분기를 탐색함으로써 Q*는 실패한 정보 검색 및 오도된 문서에 대해 저항력을 갖습니다.\n- 적응성: 이 프레임워크는 기본 LLM의 작업별 특정 조정 없이 다양한 추론 작업에 적용할 수 있습니다.\n\n실험 결과에 따르면 Q*는 다양한 수학적 추론 및 코드 생성 작업에서 기준선 방법을 크게 앞섰으며, 지식 기반 AI 시스템의 계획 및 추론 능력을 향상시킬 잠재력을 입증했습니다.\n\n## 병렬 함수 호출을 위한 LLM 컴파일러\n\nQ*가 추론 프로세스 자체를 개선하는 데 초점을 맞추고 있을 때, LLM 컴파일러 접근 방식은 에이전틱 AI 계획의 또 다른 중요한 측면을 다룹니다: 병렬 함수 호출의 효율적인 조율. 이 기법은 고전 컴파일러 설계에서 영감을 받아 대규모 언어 모델에서 여러 함수 호출을 실행을 최적화하는 것을 목표로 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM 컴파일러 방식의 주요 측면은 다음과 같습니다:\n\n- 사용자 입력을 상호 의존성을 가진 일련의 작업으로 자동 분해합니다.\n- 독립적인 작업을 병렬로 실행하여 복잡한 워크플로에서 발생하는 지연 시간을 크게 감소시킵니다.\n- 작업의 방향성 비순환 그래프(DAG)를 생성하는 계획 단계를 통해 효율적인 일정 계획 및 실행이 가능합니다.\n- 외부 도구 및 API와 통합하여 LLM의 능력을 언어 처리 이상으로 확장합니다.\n\nLLM 컴파일러는 공별한 AI 계획에서 여러 중요한 도전 과제를 다룹니다:\n\n- 효율성: 병렬화 가능한 패턴을 식별하고 함수 호출 의존성을 관리함으로써, 컴파일러는 복잡한 작업의 지연 시간을 크게 줄일 수 있습니다.\n- 확장성: 이 방식은 다수의 함수 호출과 데이터 의존성이 포함된 대규모 및 복잡한 작업을 다루도록 설계되었습니다.\n- 유연성: 컴파일러는 다양한 종류의 LLM 및 작업 부하에 적응할 수 있어 다양한 AI 응용 프로그램에 대한 다재다능한 도구가 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n초기 결과에 따르면, LLM 컴파일러는 순차 실행 방법과 비교하여 상당한 속도 향상을 달성할 수 있다는 것이 밝혀졌습니다. 최대 3.7배의 대기 시간 개선과 일부 작업에서 최대 6.7배까지의 비용 절감이 가능합니다.\n\n## 수학 올림피아드 솔루션을 위한 몬테카를로 트리 자기 수정\n\n타 분야에서 MCTS의 성공을 바탕으로, 연구자들은 복잡한 수리 추론 작업, 특히 수학 올림피아드에서 마주하는 작업들을 처리하기 위해 특별히 개발된 몬테카를로 트리 자기 수정(MCTSr) 알고리즘을 개발했습니다.\n\nMCTSr의 주요 기능은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 대규모 언어 모델과 몬테카를로 트리 탐색을 통합하여 문제 해결 능력을 향상시킵니다.\n- 선택, 자가 세부화, 자가 평가 및 역전파 단계를 포함하는 반복적인 과정입니다.\n- 모델이 솔루션을 반복적으로 향상시킬 수 있는 피드백 안내형 세분화 과정입니다.\n- 진정으로 개선된 솔루션이 높은 점수를 받도록 하는 엄격하고 비판적인 점수 매커니즘입니다.\n\nMCTSr은 수학적 추론과 계획에서 여러 가지 도전에 대응합니다:\n\n- 복잡한 다단계 문제 다루기: 이 알고리즘은 다단계 추론 단계와 전략적 사고가 필요한 복잡한 수학적 작업을 다루도록 설계되었습니다.\n- 지속적인 개선: 자가 세부화 및 자가 평가 메커니즘을 통해 MCTSr은 솔루션 품질을 점진적으로 향상시킬 수 있습니다.\n- 다양한 문제 유형에 대한 적응성: 이 프레임워크는 초등학교 산술부터 올림피아드 수준의 도전 과제까지 다양한 수학 영역에서 성공을 거두었습니다.\n\n실험 결과에서는 MCTSr이 LLaMA-3 8B와 같은 훨씬 작은 모델을 사용하여 수학 올림피아드 문제에서 GPT-4 수준의 성능을 달성할 수 있다는 것을 입증하였으며, 이는 인공지능 시스템의 추론 능력을 혁신적으로 향상시킬 잠재력을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 세 가지 접근법인 Q*, LLM Compiler 및 MCTSr은 에이전틱 AI의 계획 및 추론 기술의 최신 동향을 대표합니다. 이러한 방법들은 강화 학습 원칙과 혁신적인 탐색 및 최적화 전략을 결합하여 AI 주도 문제 해결에서 가능한 범위를 넓히고 있습니다.\n\n그러나 이러한 고급 기술을 에이전틱 AI 계획에 적용하는 데는 다음과 같은 도전 과제가 있습니다:\n\n- 계산 복잡성: 이러한 방법들은 대부분 고도의 계산 과정을 통합하며, 대규모 응용 프로그램에는 리소스가 많이 필요할 수 있습니다.\n- 탐험과 활용의 균형: 새로운 솔루션을 발견하고 기존의 좋은 전략을 활용하는 적절한 균형을 찾는 것은 여전히 까다로운 작업입니다.\n- 해석 가능성: 이러한 시스템이 더 복잡해지면서 의사 결정 과정에서의 투명성과 해석 가능성을 보장하는 것이 점점 어려워지고 있습니다.\n- 일반화: 이러한 방법은 특정 도메인에서 인상적인 결과를 보여주었지만, 다양한 작업 유형 간의 일반화 능력을 평가하기 위해 추가 연구가 필요합니다.","ogImage":{"url":"/assets/img/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning_0.png"},"coverImage":"/assets/img/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning_0.png","tag":["Tech"],"readingTime":12},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e창의력을 발휘하여 복잡한 비즈니스 전략 문제를 해결하기 위해 원활하게 협업하는 AI 에이전트 팀을 상상해보세요. 시장 동향을 조사하는 한 명의 에이전트, 재무 데이터를 분석하는 다른 한 명, 그리고 권고 사항을 준비하는 세 번째 에이전트가 모두 공통 목표를 향해 노력하고 있습니다.\u003c/p\u003e\n\u003cp\u003e이 협력적 인 인공 지능의 논리, 즉 앤젠틱 AI를 알아보면 자동화와 문제 해결의 다음 단계를 나타냅니다. AI 시스템이 더욱 정교해지면서 미리 정의된 고정적인 프로세스를 벗어나 유연성, 적응력 및 AI 에이전트 간의 팀워크를 받아들이는 데 관심이 증가하고 있습니다.\u003c/p\u003e\n\u003cp\u003e앤젠틱 AI는 기존의 전통적인 자동화 기술로 해결하기 어려웠던 복잡한, 개방형 작업을 자동화하는데 큰 약속을 합니다. 복잡한 문제를 전문화된 역할로 분해하고 개별 AI 에이전트의 고유한 능력을 활용함으로써, 다양한 에이전트 시스템은 이전에 상상도 못 했던 방식으로 지능적인 자동화를 조율할 수 있습니다. CrewAI, Langraph, Autogen과 같은 개척적인 프레임워크는 이 새로운 패러다임을 위한 길을 열며, 개발자가 복잡한 워크플로를 자율적으로 탐색하고 실행할 수 있는 AI 에이전트 팀을 디자인하고 배포할 수 있도록 도와주고 있습니다.\u003c/p\u003e\n\u003cp\u003e그러나 이 새로운 협업 AI 영역으로 나아가면 앤젠틱 시스템의 핵심에 있는 근본적인 도전 과제를 마주하게 됩니다: 계획.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eAI 에이전트들이 효과적으로 행동을 계획하고 서로 협력하며 동적이고 개방적인 환경에서 자신들의 전략을 적응시킬 수 있게 하는 방법은 무엇일까요?\u003c/p\u003e\n\u003cp\u003e이 문서는 계획이 에이전트 AI의 핵심 과제이며 강화 학습(RL)이 이 중요한 문제에 대한 유망한 해결책을 제시한다고 주장합니다.\u003c/p\u003e\n\u003cp\u003e다음 섹션에서는 에이전트 AI의 부상과 주요 원칙에 대해 탐구하고, 이러한 시스템에서 계획이 이러한 의미 있는 과제로 작용하는 이유를 설명하며, 강화 학습 기법이 이러한 어려움을 해결할 수 있는 방법을 살펴볼 것입니다.\u003c/p\u003e\n\u003cp\u003e에이전트 AI에서 계획과 강화 학습의 상호작용을 이해함으로써, 지능적 자동화와 협력적인 인공지능의 미래에 대한 중요한 통찰을 얻을 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning_0.png\" alt=\"Planning as the Core Challenge in Agentic AI: Solving it with Reinforcement Learning\"\u003e\u003c/p\u003e\n\u003ch1\u003eAgentic AI의 부상\u003c/h1\u003e\n\u003cp\u003eAgentic AI는 인공지능 시스템을 개념화하고 구현하는 방식에서 패러다임이 바뀌었다. 핵심적으로, Agentic AI는 자율적인 AI 에이전트들이 복잡하고 개방적인 과제에 대처하기 위해 팀 또는 \"크루\"로 함께 일하는 모습을 상상한다. 이 접근 방식은 단일 모델 AI 시스템의 제약을 넘어서 전문화와 협력의 힘을 활용하여 더 정교하고 유연한 문제 해결 능력을 실현한다.\u003c/p\u003e\n\u003cp\u003e이 Agentic AI 혁명의 전선에는 다수의 에이전트들 사이의 협력에 대한 독특한 접근 방식을 제공하는 여러 중요한 프레임워크들이 등장했다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eCrewAI: 이 프레임워크는 특정 역할을 갖는 AI 팀을 설계할 수 있게 해 주어, 그들이 특정 작업에 따라 선별된 연구 및 분석 도구 세트를 갖추도록 돕습니다.\u003c/li\u003e\n\u003cli\u003eLangraph: Langraph는 더 구조화된 방식을 채택하여, 명시적인 방향 그래프를 사용하여 에이전트 간의 작업 흐름을 정의합니다. 이를 통해 개발자들은 에이전트 조정과 작업 할당에 대해 세밀한 제어를 할 수 있습니다.\u003c/li\u003e\n\u003cli\u003eAutogen: Autogen은 에이전트 간의 다중 대화로부터 발생하는 신생 작업 흐름에 의존하여, 보다 동적이고 적응적인 협업 패턴을 가능하게 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이 프레임워크들은 구체적인 구현에서 차이가 있지만, 모두 에이전틱 AI 접근 방식을 정의하는 중심 원칙을 공유합니다:\u003c/p\u003e\n\u003cp\u003e전문화와 협업: 이러한 시스템 전반에 걸쳐 두드러지는 공통점 중 하나는 다수의 특화된 에이전트를 활용하여 스스로 작업하는 방식입니다. 단일 대형 모델에 의존하는 대신, 에이전틱 AI는 작업을 하위 작업으로 분해하여 각각 다른 역할과 기술을 갖춘 에이전트에 위임합니다. 이러한 전문화는 각 에이전트가 자신의 전문 분야에 집중할 수 있도록 하고, 협업은 팀이 어떤 개별 에이전트에겐 도전적일 수 있는 문제들을 해결할 수 있도록 돕습니다.\u003c/p\u003e\n\u003cp\u003e예를 들어 채용 상황에서, 크루는 기술 직군 연구, 인적 프로필 엔지니어링, 이력서 전략 및 면접 준비에 특화된 에이전트로 구성될 수 있습니다. 이러한 특화된 에이전트들이 함께 작업하여 단일 일반적 AI보다 개인을 고용 전 과정에서 효과적으로 안내할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e언어 모델과 외부 도구의 활용: 에이전트 AI 시스템에서 또 다른 중요한 패턴은 각 에이전트를 뒷받침하는 \"두뇌\"로서 대형 언어 모델(LLMs)을 사용하는 것입니다. 이러한 미리 학습된 모델은 에이전트가 열린 대화를 할 수 있게 하며 자연어 질의를 해석하고 유창한 응답을 생성하며 판단을 내릴 수 있도록 합니다.\u003c/p\u003e\n\u003cp\u003e그러나 에이전트 AI는 언어 모델만을 의존하지는 않습니다. 에이전트의 지식을 기반을 다지고 그들의 능력을 확장하기 위해, 이러한 시스템은 또한 외부 도구와 데이터 소스에 연결합니다. 웹에서 단락을 검색하거나 구조화된 데이터베이스를 질의하거나 타사 API를 호출하는 등의 방식으로, 에이전트들은 실제 세계 정보를 활용하여 자신들의 결정과 행동에 영감을 얻습니다.\u003c/p\u003e\n\u003cp\u003e이러한 언어적 유연성과 외부 기반의 결합으로 인해 에이전트 AI 시스템은 넓은 세계로부터 통찰을 얻으면서 일관된 대화를 유지할 수 있습니다. 이는 인간이 언어를 지식과 행동의 관문으로 활용하는 방식을 재현하는 데 필수적인 한 걸음입니다.\u003c/p\u003e\n\u003cp\u003e에이전트 상태 및 워크플로우 관리: 에이전트 AI 설계의 가장 다양한 측면은 플랫폼이 에이전트 팀의 상태와 워크플로우 오케스트레이션을 어떻게 다루는지입니다. 에이전트 작업은 종종 다수의 단계와 에이전트 출력 간의 의존성을 포함하므로 일관된 전역 상태와 제어 흐름을 유지하는 것이 중요합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 도전 과제에 대한 접근 방식은 플랫폼마다 다양합니다. Langraph는 워크플로우를 정의하기 위해 명시적인 방향 그래프를 사용하여 개발자에게 세밀한 제어를 제공합니다. Autogen은 에이전트 간의 멀티턴 대화에서 발생하는 신흥 워크플로에 더 의존합니다. CrewAI는 상호 작용을 안내하는 고수준 태스크 플로우를 갖추고 있지만 에이전트들이 서브태스크를 자율적으로 위임하고 응답할 수 있는 유연성을 가지고 있습니다.\u003c/p\u003e\n\u003cp\u003e이러한 차이점에도 불구하고, 에이전트 상태 및 워크플로우 관리를 위한 일관된 우선순위 목록이 도출됩니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e에이전트가 시간이 지남에 따라 다른 에이전트들의 작업 및 결정을 발전시킬 수 있는 메커니즘 제공\u003c/li\u003e\n\u003cli\u003e태스크 분할 및 에이전트 조정 패턴의 유연한 정의 가능\u003c/li\u003e\n\u003cli\u003e에이전트 역할, 도구 및 위임 권한의 태스크별 맞춤화 허용\u003c/li\u003e\n\u003cli\u003e예외 처리 및 에이전트 출력 간 비선형 종속성 그래프 우아하게 처리\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e보다시피, 에이전트 AI의 부상은 유연하고 지능적인 자동화의 엄청난 잠재력을 가져옵니다. 특화, 협업, 외부 데이터를 기반으로 한 언어 모델의 강점을 활용하여 이러한 시스템은 기존의 전통적인 AI 접근 방식으로는 이루기 힘든 복잡하고 개방적인 작업에 대처할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그러나 이러한 잠재력은 상당한 도전과 함께 옵니다. 그 중에서도 가장 중요한 것은 계획 문제입니다. 어떻게 하면 다양한 AI 에이전트 팀이 효과적으로 행동을 조정하고 불확실성 하에서 결정을 내리며, 동적 환경에서 전략을 적응할 수 있도록 할 수 있을까요? 이것이 에이전트 AI 시스템의 핵심 도전에 대한 핵심을 담고 있습니다.\u003c/p\u003e\n\u003ch1\u003e핵심 도전으로서의 계획\u003c/h1\u003e\n\u003cp\u003e에이전트 AI 시스템이 복잡성과 능력을 키우면 효과적인 계획의 필요성이 점점 더 중요해집니다. 이 문맥에서의 계획은 AI 에이전트들이 목표를 달성하기 위해 행동 순서를 결정하고, 다른 에이전트들과 협력하며, 변화하는 상황에 적응하는 과정을 말합니다. 계획은 지적 행동의 기본적인 측면이지만, 에이전트 AI의 영역에서 특히 어려운 도전을 제기합니다.\u003c/p\u003e\n\u003cp\u003e왜 계획이 특히 복잡한가요, 특히 다중 에이전트 시나리오에서? 이러한 어려움에 기여하는 몇 가지 주요 요소가 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e고차원 상태 및 행동 공간: 에이전틱 인공지능에서 상태 공간(환경 및 에이전트의 모든 가능한 구성)과 행동 공간(에이전트가 취할 수 있는 모든 가능한 행동)은 매우 크고 복잡합니다. 이는 각각의 능력과 잠재적 행동을 갖는 여러 에이전트가 상호작용하는 경우 조합 폭발로 인한 것입니다.\u003c/li\u003e\n\u003cli\u003e부분 관측성: 에이전트들은 종종 환경의 상태와 다른 에이전트의 행동에 대해 불완전한 정보를 갖습니다. 이러한 불확실성으로 인해 행동의 결과를 예측하고 효과적으로 계획하기 어려워집니다.\u003c/li\u003e\n\u003cli\u003e비정상적인 환경: 다중 에이전트 시스템에서는 환경이 에이전트가 행동을 취하고 서로 상호작용함에 따라 지속적으로 변화합니다. 이러한 비정상성은 시간이 지남에 따라 작용의 효과가 일관되지 않아 계획 과정을 복잡하게 만듭니다.\u003c/li\u003e\n\u003cli\u003e장기 의존성: 에이전틱 AI의 많은 작업은 단계 간에 의존성을 가진 장기적인 행동 시퀀스를 필요로 합니다. 이러한 확장된 시간 경계를 통해 계획을 수행하는 것은 계산적으로 어려우며 즉시적 보상과 장기적 목표를 균형있게 유지해야 합니다.\u003c/li\u003e\n\u003cli\u003e조정 및 통신 부담: 다중 에이전트 시스템에서 효과적인 계획은 에이전트 간의 조정이 필요하며 이는 의사 결정 과정에서 추가 복잡성과 병목현상을 초래할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이러한 도전에 대처하기 위해 연구자들은 에이전틱 AI의 계획 문제를 마르코프 결정 과정(MDP)으로 정의하고 있습니다. MDP는 상황에 따라 결과가 일부적으로 무작위이고 일부적으로 의사 결정자의 통제 아래 있는 상황에서 의사 결정을 모델링하기 위한 수학적인 프레임워크를 제공합니다.\u003c/p\u003e\n\u003cp\u003e에이전틱 AI의 맥락에서 MDP의 구성 요소를 다음과 같이 정의할 수 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e상태 공간 (S): 모든 가능한 사고 과정 및 환경 구성의 공간\u003c/li\u003e\n\u003cli\u003e동작 공간 (A): 사고나 문서 검색의 모든 가능한 조합\u003c/li\u003e\n\u003cli\u003e전이 역학 (P): 이전 사고와 행동을 기반으로 새로운 사고가 생성되는 방법\u003c/li\u003e\n\u003cli\u003e보상 함수 (R): 답변의 품질이나 목표에 대한 진전을 평가하는 것\u003c/li\u003e\n\u003cli\u003e할인 계수 (γ): 단기 vs. 장기적 보상의 우선순위\u003c/li\u003e\n\u003cli\u003e문제 기간 (T): 허용되는 추론 단계의 최대 수\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e계획 문제를 MDP로 프레임화함으로써, 강화 학습 분야의 다양한 기술을 활용하여 AI의 계획 과제를 해결할 수 있습니다. 그러나 이 정식화는 계획 과정에서 근본적인 긴장을 강조하기도 합니다: 탐험-활용 딜레마.\u003c/p\u003e\n\u003cp\u003e탐험-활용 딜레마는 새로운 것을 탐색하거나 잘 알려진 좋은 솔루션을 활용하는 사이의 교환 비용을 가리킵니다. 에이전트 AI 계획의 맥락에서 이것은 다음과 같은 균형으로 나타납니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e탐험: 새로운 사고 조합을 시도하거나 다양한 문서를 검색하거나 혁신적인 추론 방향을 추구하여 중요한 솔루션에 이르는 일들에 대한 베스트.\u003c/li\u003e\n\u003cli\u003e활용: 이미 알려진 효과적인 전략에 초점을 맞추거나 성공적인 사고 과정을 발전시키거나 최대의 즉각적 보상을 위해 기존 솔루션을 정제하는 일에 베스트.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e탐험과 활용 사이의 적절한 균형을 찾는 것은 에이전트 AI 시스템에서의 효과적인 계획에 중요합니다. 탐험이 과도하면 낭비되는 컴퓨팅 자원과 일관성 없는 성능을 야기할 수 있으며, 활용이 지나치다면 최적의 솔루션을 허술하게 만들거나 새로운 상황에 적응하지 못하는 문제를 야기할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e전통적인 계획 접근 방식인 상징적 AI나 철저한 검색을 기반으로 한 방법은 종종 에이전틱 AI의 맥락에서 이러한 도전 과제를 해결하는 데 어려움을 겪는다. 이러한 방법들은 일반적으로 환경의 완전한 지식, 결정론적 행동 결과, 명확히 정의된 목표 상태에 의존하는데, 이는 에이전틱 AI가 활동하는 복잡하고 불확실하며 개방적인 도메인에서 거의 적용되지 않는 가정들이다.\u003c/p\u003e\n\u003cp\u003e대신 필요한 건 유연하고 적응적인 계획 접근 방식으로, 다음과 같은 기능을 갖추어야 한다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e고차원 상태 및 행동 공간을 효율적으로 처리\u003c/li\u003e\n\u003cli\u003e부분 관찰 가능성과 불확실성 다루기\u003c/li\u003e\n\u003cli\u003e비정상적인 환경에 적응하기\u003c/li\u003e\n\u003cli\u003e복잡한 종속성이 있는 긴 시간 범위에 계획 수립\u003c/li\u003e\n\u003cli\u003e탐색과 활용을 동적으로 균형있게 유지\u003c/li\u003e\n\u003cli\u003e여러 전문화된 에이전트 간의 행동 조정\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e여기서 강화 학습이 등장하여 에이전틱 AI 시스템이 제기하는 독특한 계획 도전 과제를 해결하기에 적합한 강력한 기법 세트를 제공한다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e강화 학습 및 고급 기술로서의 솔루션\u003c/h1\u003e\n\u003cp\u003e강화 학습(RL)은 에이전트형 AI에서 복잡한 계획 도전에 대처하는 유망한 접근법으로 부각되었습니다. RL은 에이전트가 환경과 상호 작용하면서 보상이나 처벌의 형태로 피드백을 받아 결정을 내리는 방식의 머신러닝 유형입니다.\u003c/p\u003e\n\u003cp\u003e이러한 학습 패러다임은 에이전트형 AI에서 계획 문제에 특히 적합한 이유가 여럿 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e경험으로부터 학습: RL 에이전트들은 환경의 완전한 모델을 요구하지 않고 시행착오를 통해 최적의 전략을 학습할 수 있습니다. 이는 에이전트형 AI가 작동하는 복잡한, 부분 관측 가능한 도메인에서 중요합니다.\u003c/li\u003e\n\u003cli\u003e탐험과 이용 사이의 균형 유지: RL 알고리즘에는 탐사-이용 교환을 관리하는 내장 기구가 있어, 에이전트들이 새로운 전략을 발견하면서도 알려진 좋은 해결책을 활용할 수 있게 합니다.\u003c/li\u003e\n\u003cli\u003e불확실성 다루기: RL 방법은 확률적 환경에서 작동하도록 설계되어, 다중 에이전트 시스템에 내재된 불확실성에 탄력적으로 대처할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e장기 계획: 많은 RL 알고리즘은 명시적으로 장기 보상을 최적화하기 위해 설계되어 있어, 긴 시간 대역으로 계획을 수립하고 행위 간의 복잡한 종속성을 포착할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e적응성: RL 에이전트들은 새로운 경험을 기반으로 전략을 지속적으로 업데이트할 수 있어, 변동성 있는 환경에 적합합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e특히 계획 도전 과제를 해결하는 데 유망한 강화 학습 기법 중 하나는 몬테 카를로 트리 탐색(Monte Carlo Tree Search, MCTS)입니다. MCTS는 휴리스틱 탐색 알고리즘이며, 랜덤 샘플링과 트리 탐색을 결합하여 복잡한 공간에서 결정을 내립니다. 이 기법은 다양한 분야에 성공적으로 적용되었으며, 알파고(AlphaGo)와 같은 게임 플레이 인공지능에서 사용되었습니다.\u003c/p\u003e\n\u003cp\u003e에이전틱 인공지능 계획의 맥락에서, MCTS는 가능한 사고 과정과 행동 시퀀스의 광범위한 공간을 효율적으로 탐색하는 데 사용될 수 있습니다. MCTS의 주요 단계는 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e선택(Selection): 루트에서 시작하여 탐색하는 동안 탐색과 활용을 균형있게 고려하는 트리 정책(예: 상한 신뢰 경계)을 사용합니다.\u003c/li\u003e\n\u003cli\u003e확장(Expansion): 새로운 자식 노드를 추가하여 트리를 확장합니다.\u003c/li\u003e\n\u003cli\u003e시뮬레이션(Simulation): 새 노드에서 랜덤 시뮬레이션을 실행하여 값을 추정합니다.\u003c/li\u003e\n\u003cli\u003e역전파(Backpropagation): 루트에 되돌아가는 경로를 따라 노드 통계를 업데이트합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이러한 단계를 반복적으로 적용함으로써, MCTS는 검색 공간의 가장 유망한 지역에 계산 리소스를 집중할 수 있어 에이전틱 인공지능 계획에서 마주치는 고차원 상태 및 행동 공간에 적합합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다른 중요한 강화 학습 개념 중 하나는 에이전틱 AI 계획에 적용할 수 있는 Q-러닝입니다. Q-러닝은 모델이 없는 강화 학습 알고리즘으로, 주어진 상태에서 특정 행동을 취했을 때 기대되는 누적 보상(Q-값)을 추정하는 방법을 학습합니다. 에이전틱 AI 환경에서는 Q-러닝을 사용하여 다양한 사고 과정이나 문서 검색의 가치를 추정할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이 분야의 최근 발전은 이러한 기본적인 강화 학습 개념을 바탕으로 한 몇 가지 혁신적인 접근 방식의 발전을 이끌어내어, 에이전틱 AI 시스템에서 계획 및 추론의 특정 도전 과제를 해결하기 위한 기술적 발전을 이끌어내고 있습니다.\u003c/p\u003e\n\u003cp\u003e또한 특히 유망한 세 가지 혁신적 기술을 살펴보겠습니다:\u003c/p\u003e\n\u003ch2\u003eQ*: 딥러닝 계획을 통한 다단계 추론 개선\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eWang 및 다른 사람(2024)이 소개한 Q* 프레임워크는 대형 언어 모델(Large Language Models, LLMs)의 다단계 추론 능력을 향상시키는 데 중요한 발전을 이끌어냅니다. Q\u003cem\u003e는 A\u003c/em\u003e 검색의 능력을 결합하여 학습된 Q-value 모델로 LLMs을 복잡한 추론 작업 중에서 가장 유망한 다음 단계를 선택하도록 안내합니다.\u003c/p\u003e\n\u003cp\u003eQ*의 주요 특징은 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e각 노드가 주어진 문제에 대한 부분 솔루션을 나타내는 그래프로 추론 프로세스를 모델링합니다.\u003c/li\u003e\n\u003cli\u003eA* 검색을 위한 학습된 Q-value 모델을 휴리스틱 함수로 사용하여, 전체 문제를 해결하는 데 각 잠재적인 다음 단계가 얼마나 유망한지를 추정합니다.\u003c/li\u003e\n\u003cli\u003e가능한 추론 경로의 방대한 공간을 효율적으로 탐색하기 위해 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)을 사용합니다.\u003c/li\u003e\n\u003cli\u003eLLM이 자체 정제된 답변에 대한 점수를 매기는 자가 평가 메커니즘을 통합하여 추론 프로세스를 지속적으로 향상시킬 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eQ* 프레임워크는 에이전트 AI 계획에서 몇 가지 중요한 도전에 대처합니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e긴 콘텍스트 처리: Q*는 전통적인 LLM의 고정된 콘텍스트 창의 제약을 극복하기 위해 지식 원본에서 대규모 문서 배치를 처리할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e무관한 정보에 대한 견고성: 다양한 추론 분기를 탐색함으로써 Q*는 실패한 정보 검색 및 오도된 문서에 대해 저항력을 갖습니다.\u003c/li\u003e\n\u003cli\u003e적응성: 이 프레임워크는 기본 LLM의 작업별 특정 조정 없이 다양한 추론 작업에 적용할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e실험 결과에 따르면 Q*는 다양한 수학적 추론 및 코드 생성 작업에서 기준선 방법을 크게 앞섰으며, 지식 기반 AI 시스템의 계획 및 추론 능력을 향상시킬 잠재력을 입증했습니다.\u003c/p\u003e\n\u003ch2\u003e병렬 함수 호출을 위한 LLM 컴파일러\u003c/h2\u003e\n\u003cp\u003eQ*가 추론 프로세스 자체를 개선하는 데 초점을 맞추고 있을 때, LLM 컴파일러 접근 방식은 에이전틱 AI 계획의 또 다른 중요한 측면을 다룹니다: 병렬 함수 호출의 효율적인 조율. 이 기법은 고전 컴파일러 설계에서 영감을 받아 대규모 언어 모델에서 여러 함수 호출을 실행을 최적화하는 것을 목표로 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eLLM 컴파일러 방식의 주요 측면은 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e사용자 입력을 상호 의존성을 가진 일련의 작업으로 자동 분해합니다.\u003c/li\u003e\n\u003cli\u003e독립적인 작업을 병렬로 실행하여 복잡한 워크플로에서 발생하는 지연 시간을 크게 감소시킵니다.\u003c/li\u003e\n\u003cli\u003e작업의 방향성 비순환 그래프(DAG)를 생성하는 계획 단계를 통해 효율적인 일정 계획 및 실행이 가능합니다.\u003c/li\u003e\n\u003cli\u003e외부 도구 및 API와 통합하여 LLM의 능력을 언어 처리 이상으로 확장합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLLM 컴파일러는 공별한 AI 계획에서 여러 중요한 도전 과제를 다룹니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e효율성: 병렬화 가능한 패턴을 식별하고 함수 호출 의존성을 관리함으로써, 컴파일러는 복잡한 작업의 지연 시간을 크게 줄일 수 있습니다.\u003c/li\u003e\n\u003cli\u003e확장성: 이 방식은 다수의 함수 호출과 데이터 의존성이 포함된 대규모 및 복잡한 작업을 다루도록 설계되었습니다.\u003c/li\u003e\n\u003cli\u003e유연성: 컴파일러는 다양한 종류의 LLM 및 작업 부하에 적응할 수 있어 다양한 AI 응용 프로그램에 대한 다재다능한 도구가 됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e초기 결과에 따르면, LLM 컴파일러는 순차 실행 방법과 비교하여 상당한 속도 향상을 달성할 수 있다는 것이 밝혀졌습니다. 최대 3.7배의 대기 시간 개선과 일부 작업에서 최대 6.7배까지의 비용 절감이 가능합니다.\u003c/p\u003e\n\u003ch2\u003e수학 올림피아드 솔루션을 위한 몬테카를로 트리 자기 수정\u003c/h2\u003e\n\u003cp\u003e타 분야에서 MCTS의 성공을 바탕으로, 연구자들은 복잡한 수리 추론 작업, 특히 수학 올림피아드에서 마주하는 작업들을 처리하기 위해 특별히 개발된 몬테카를로 트리 자기 수정(MCTSr) 알고리즘을 개발했습니다.\u003c/p\u003e\n\u003cp\u003eMCTSr의 주요 기능은 다음과 같습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e대규모 언어 모델과 몬테카를로 트리 탐색을 통합하여 문제 해결 능력을 향상시킵니다.\u003c/li\u003e\n\u003cli\u003e선택, 자가 세부화, 자가 평가 및 역전파 단계를 포함하는 반복적인 과정입니다.\u003c/li\u003e\n\u003cli\u003e모델이 솔루션을 반복적으로 향상시킬 수 있는 피드백 안내형 세분화 과정입니다.\u003c/li\u003e\n\u003cli\u003e진정으로 개선된 솔루션이 높은 점수를 받도록 하는 엄격하고 비판적인 점수 매커니즘입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMCTSr은 수학적 추론과 계획에서 여러 가지 도전에 대응합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e복잡한 다단계 문제 다루기: 이 알고리즘은 다단계 추론 단계와 전략적 사고가 필요한 복잡한 수학적 작업을 다루도록 설계되었습니다.\u003c/li\u003e\n\u003cli\u003e지속적인 개선: 자가 세부화 및 자가 평가 메커니즘을 통해 MCTSr은 솔루션 품질을 점진적으로 향상시킬 수 있습니다.\u003c/li\u003e\n\u003cli\u003e다양한 문제 유형에 대한 적응성: 이 프레임워크는 초등학교 산술부터 올림피아드 수준의 도전 과제까지 다양한 수학 영역에서 성공을 거두었습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e실험 결과에서는 MCTSr이 LLaMA-3 8B와 같은 훨씬 작은 모델을 사용하여 수학 올림피아드 문제에서 GPT-4 수준의 성능을 달성할 수 있다는 것을 입증하였으며, 이는 인공지능 시스템의 추론 능력을 혁신적으로 향상시킬 잠재력을 보여줍니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 세 가지 접근법인 Q*, LLM Compiler 및 MCTSr은 에이전틱 AI의 계획 및 추론 기술의 최신 동향을 대표합니다. 이러한 방법들은 강화 학습 원칙과 혁신적인 탐색 및 최적화 전략을 결합하여 AI 주도 문제 해결에서 가능한 범위를 넓히고 있습니다.\u003c/p\u003e\n\u003cp\u003e그러나 이러한 고급 기술을 에이전틱 AI 계획에 적용하는 데는 다음과 같은 도전 과제가 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e계산 복잡성: 이러한 방법들은 대부분 고도의 계산 과정을 통합하며, 대규모 응용 프로그램에는 리소스가 많이 필요할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e탐험과 활용의 균형: 새로운 솔루션을 발견하고 기존의 좋은 전략을 활용하는 적절한 균형을 찾는 것은 여전히 까다로운 작업입니다.\u003c/li\u003e\n\u003cli\u003e해석 가능성: 이러한 시스템이 더 복잡해지면서 의사 결정 과정에서의 투명성과 해석 가능성을 보장하는 것이 점점 어려워지고 있습니다.\u003c/li\u003e\n\u003cli\u003e일반화: 이러한 방법은 특정 도메인에서 인상적인 결과를 보여주었지만, 다양한 작업 유형 간의 일반화 능력을 평가하기 위해 추가 연구가 필요합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-23-PlanningastheCoreChallengeinAgenticAISolvingitwithReinforcementLearning"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>