<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지 | itposting" data-gatsby-head="true"/><meta property="og:title" content="이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV" data-gatsby-head="true"/><meta name="twitter:title" content="이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-20 18:20" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 20, 2024</span><span class="posts_reading_time__f7YPP">18<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png" alt="Image"></p>
<p>객체 감지는 딥러닝의 한 분야 중 하나로, 많은 발전이 이루어졌습니다. 다양한 모델을 사용하여 우리는 사진에서 물체를 감지할 수 있고, 그 결과로 비디오에서도 물체를 감지할 수 있습니다. 요즘에는 웹캠 이미지를 사용한 실시간 객체 감지가 흔한 일입니다!</p>
<p>이 튜토리얼에서는 TensorFlow를 사용하여 객체 감지 시스템을 구축할 것입니다. 구체적으로 TensorFlow Object Detection API를 사용할 것입니다. 단계별로 모든 필요한 종속성을 설치하고, TensorFlow Model Zoo의 사전 훈련된 모델을 살펴보고, 객체 감지기를 구축할 것입니다.</p>
<p>다시 말해, 이 튜토리얼을 읽은 후에는...</p>
<div class="content-ad"></div>
<ul>
<li>TensorFlow 기반 객체 검출기를 구축하기 위해 설치해야 하는 것을 알게 되었습니다.</li>
<li>사전 훈련된 모델을 찾고 시스템에 다운로드하는 위치를 알고 있습니다.</li>
<li>사진 및 비디오와 함께 사용할 수 있는 실제 객체 검출 시스템을 구축했습니다.</li>
</ul>
<p>그리고 이미지는 항상 수많은 말보다 많은 것을 전달합니다. 아래 시스템을 만들어 보세요!</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1200/1*qa_qXkly0MvO82-7uV7LpA.gif" alt="이미지"></p>
<p>한번 살펴보시죠!</p>
<div class="content-ad"></div>
<h2>객체 탐지기 구축: 필수 조건</h2>
<p>텐서플로 Object Detection API를 사용하여 객체 탐지 시스템을 구축하려면 다음 세 가지 단계를 완료해야 합니다:</p>
<ul>
<li>TensorFlow 및 OpenCV 설치하기. 우리는 TF 기능을 위해 TensorFlow가 필요하며, 이미지 I/O를 위해 OpenCV가 필요합니다. 보통 시스템에 이미 설치되어 있지만, 완전성을 위해 여기에 포함시켰습니다.</li>
<li>TensorFlow Object Detection API 설치하기. 이 추가 기능 세트는 별도로 설치해야 합니다. 어떻게 설치할 수 있는지 살펴보겠습니다.</li>
<li>TensorFlow Model Zoo에서 적절한 사전 학습된 모델 찾기. TensorFlow의 제작자들이 다양한 모델 아키텍처를 사용하여 사전 학습된 여러 모델을 TensorFlow Model Zoo에 넣었습니다. 간단히 살펴보고 모델을 선택할 것입니다.</li>
</ul>
<p><img src="/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_1.png" alt="이미지"></p>
<div class="content-ad"></div>
<h2>TensorFlow와 OpenCV 설치하기</h2>
<p>실제 객체 탐지기를 구축하기 전에 TensorFlow와 OpenCV를 설치해야 합니다.</p>
<p>여기서는 이미 시스템에 Python이 설치되어 있다고 가정합니다. 그렇지 않은 경우 먼저 Python을 설치해 주세요.</p>
<p>요즘은 TensorFlow를 설치하는 것이 정말 쉽습니다. Python에 액세스할 수 있는 터미널에서 다음을 실행하면 됩니다:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"># 최신 버전의 pip가 필요합니다
pip install --upgrade pip

# <span class="hljs-variable constant_">CPU</span> 및 <span class="hljs-variable constant_">GPU</span>용 현재 안정적인 릴리스
pip install tensorflow
</code></pre>
<p>먼저 pip를 최신 버전으로 업그레이드하고 TensorFlow를 설치합니다. 이제 CPU 또는 GPU 버전을 수동으로 지정해야 했던 것이 오늘에는 그렇지 않습니다. 그냥 tensorflow를 설치하면 GPU 버전이 정확하게 설정된 경우 GPU 버전이 자동으로 설치됩니다. 실제로 GPU와 CPU 사이를 자유롭게 전환할 수 있지만, 이에 대해서는 나중에 다시 이야기하겠습니다.</p>
<p>OpenCV를 설치하는 것도 어렵지 않습니다: pip install opencv-python으로 해결할 수 있습니다.</p>
<p>이제 기본 패키지가 설치되었으므로 TensorFlow Object Detection API를 살펴볼 수 있습니다.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_2.png">
<p>강아지... 와우!</p>
<h2>TensorFlow Object Detection API 설치</h2>
<p>GitHub에서 tensorflow/models에서 Object Detection API를 찾을 수 있습니다:</p>
<div class="content-ad"></div>
<p>이름에서 알 수 있듯이, 이것은 객체 감지 목적으로 사용할 수 있습니다. 특히, 사전 훈련된 모델을 로드하고 이미지 및 비디오에 경계 상자를 추가하는 기능을 제공합니다. 우리의 객체 감지 시스템이 이러한 API를 활용할 수 있다는 것은 우리가 모든 것을 직접 개발할 필요가 없다는 멋진 점입니다.</p>
<p>나중에 사전 훈련된 모델을 살펴볼 것입니다. 먼저 Object Detection API를 설치해 봅시다. 이것은 시스템에 Git이 설치되어 있는 것을 가정합니다. 또한 protoc 명령을 실행할 수 있는지 확인해 주세요. 여기서 확인하는 방법을 찾아보세요.</p>
<ul>
<li>먼저 tensorflow/models 저장소 전체를 복제합니다. 한 단계 깊이만 복제하도록 주의하세요. 다음 명령을 실행하여 저장소를 복제하세요: git clone --depth 1 <a href="https://github.com/tensorflow/models" rel="nofollow" target="_blank">https://github.com/tensorflow/models</a></li>
<li>이제 models/research/ 디렉토리로 이동한 다음 protoc object_detection/protos/*.proto --python_out=. 명령을 실행하세요.</li>
<li>그런 다음 cp object_detection/packages/tf2/setup.py 명령을 사용하여 설정 파일을 현재 디렉토리로 복사합니다.</li>
<li>마지막으로 python -m pip install 명령을 통해 Object Detection API를 pip를 통해 설치하세요.</li>
</ul>
<h2>TensorFlow 모델 동물원: 객체 감지용 사전 훈련된 모델</h2>
<div class="content-ad"></div>
<p>저희 물체 감지 시스템은 TensorFlow 모델 위에 구축될 예정이에요. 이 모델은 다양한 종류의 물체를 감지할 수 있어요. 이 모델을 훈련하는 과정은 다음과 같아요:</p>
<ul>
<li>다양한 물체가 포함된 많은 이미지를 수집하는 것</li>
<li>이러한 이미지들에 레이블을 달아 모든 클래스가 균형을 이루도록 하는 것</li>
<li>모델을 훈련하는 것</li>
</ul>
<p>이 과정은 많은 노력이 필요할 거예요. 다행히 TensorFlow 팀은 TensorFlow Detection Model Zoo에서 다양한 사전 훈련된 물체 감지 모델을 제공하고 있어요.</p>
<p>이러한 물체 감지기는 이미 훈련이 완료되어 있으며 TensorFlow Object Detection API에서 이용할 수 있어요 (괄호 안에는 내부 모델 구조가 나와 있어요):</p>
<div class="content-ad"></div>
<ul>
<li>CenterNet (HourGlass104, Resnet50 V1, Resnet101 V1, Resnet50 V2).</li>
<li>EfficientDet (D0, D1, D2, D3, D4, D5, D6, D7).</li>
<li>SSD (MobileNet V1 FPN, V2, V2 FPNLite; ResNet50 V1; Resnet101 V1).</li>
<li>Faster R-CNN (ResNet50; ResNet101; ResNet152; Inception ResNet V2).</li>
<li>Mask R-CNN (Inception ResNet V2).</li>
<li>ExtremeNet.</li>
</ul>
<p>당연히 직접 모델을 만드실 수도 있습니다. 하지만 이 강좌에서 다루지는 않습니다.</p>
<p>오늘은 SSD MobileNet V2 FPNLite 640x640 모델을 사용할 것입니다. Zoo에서 원하는 모델을 선택하실 수 있지만, 이 사전 훈련된 모델은 용량이 20MB밖에 되지 않아서 빠른 인터넷 속도를 가진 많은 사람들이 다운로드할 수 있습니다.</p>
<p>이제 우리의 디텍터를 만들어봅시다!</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_3.png">
<h2>객체 탐지기 생성</h2>
<p>여기서는 객체 탐지 시스템을 구축하는 방법을 살펴보겠습니다. 이 과정은 세 가지 별개이지만 순차적인 단계로 나눌 수 있습니다:</p>
<ul>
<li>기반을 놓기. 이곳에서는 중요한 imports를 지정하고, 클래스를 정의하고, 초기화 작업을 설명하고, 준비 작업 정의를 작성할 것입니다.</li>
<li>탐지 함수 작성. 이것이 탐지기의 핵심입니다. 이것은 일반적으로 탐지를 수행하고, 특히 이미지와 비디오에 대한 예측을 생성할 수 있게 합니다.</li>
<li>탐지 호출 생성. 마지막으로, 우리의 탐지기가 준비되면 사용할 수 있도록 다음 추가 코드를 추가할 것입니다.</li>
</ul>
<div class="content-ad"></div>
<p>코드 에디터를 열고 objectdetector.py와 같은 Python 파일을 생성해 주세요. 코드 작성 시작할까요?</p>
<h2>파트 1: 기반 구축하기</h2>
<p>TensorFlow Object Detection API를 기억하시나요? 이것은 물체 감지기를 구축하기 위한 TensorFlow 위의 프레임워크입니다. 다시 말해, 머신 러닝 모델을 만들기 위한 잘 알려진 라이브러리 위에 또 다른 층이란 의미죠. 이 API 위에 Object Detection API를 사용하는 물체 감지기 층을 추가할 계획입니다.</p>
<p>이 TFObjectDetector의 기반을 구축하기 위해서는 Python 임포트 추가, 필요한 경우 GPU 비활성화, TFObjectDetector 클래스 작성 및 초기화, 물체 감지기를 위한 설정 메커니즘 작성, 마지막으로 몇 가지 도우미 함수를 작성해야 합니다.</p>
<div class="content-ad"></div>
<h2>파이썬 라이브러리 가져오기</h2>
<p>첫 번째 코드는 항상 파이썬 라이브러리를 가져와야 합니다. 오늘도 마찬가지에요:</p>
<pre><code class="hljs language-js"># 모델 라이브러리 지정
<span class="hljs-keyword">from</span> object_detection.<span class="hljs-property">builders</span> <span class="hljs-keyword">import</span> model_builder
<span class="hljs-keyword">from</span> object_detection.<span class="hljs-property">utils</span> <span class="hljs-keyword">import</span> config_util
<span class="hljs-keyword">from</span> object_detection.<span class="hljs-property">utils</span> <span class="hljs-keyword">import</span> label_map_util
<span class="hljs-keyword">from</span> object_detection.<span class="hljs-property">utils</span> <span class="hljs-keyword">import</span> visualization_utils <span class="hljs-keyword">as</span> viz_utils
<span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
</code></pre>
<p>object_detection 패키지에서 많은 함수를 가져왔네요 - 이는 TensorFlow Object Detection API를 나타냅니다. 모델 빌더를 사용하여 감지 모델(예: SSD MobileNet 모델)을 구축할 거에요. config_util을 사용하면 TensorFlow에 올바른 모델을 로드하도록 알려주는 구성을 로드할 수 있습니다. 클래스 이름을 나타내는 레이블은 label_map_util을 사용하여 로드할 수 있고, viz_utils는 이미지나 비디오에 경계 상자를 추가하는 데 유용하게 사용될 거에요.</p>
<div class="content-ad"></div>
<p>OpenCV (cv2)는 이미지의 입력 및 출력에 사용되며, NumPy (np)는 숫자 처리에 사용되고, os는 운영 체제 기능에 사용되며, 마지막으로 TensorFlow를 import합니다.</p>
<h2>필요한 경우 GPU 비활성화</h2>
<p>두 번째 단계는 GPU를 비활성화하는 것인데, 이것은 선택 사항입니다 — 다시 말해, 원할 경우에만 수행하십시오. 특히 GPU를 보유하고 있지만 구성이 잘못된 경우에 유용할 수 있습니다. 그때 CUDA 가시 장치를 환경에서 모두 지워야 합니다. TensorFlow의 GPU 버전을 사용하지 않는 경우에는 이 코드를 생략할 수 있습니다.</p>
<pre><code class="hljs language-js"># 필요한 경우 <span class="hljs-variable constant_">GPU</span> 비활성화
os.<span class="hljs-property">environ</span>[<span class="hljs-string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="hljs-string">'-1'</span>
</code></pre>
<div class="content-ad"></div>
<h2>클래스와 초기화자 만들기</h2>
<p>이제 진짜 작업을 시작할 시간입니다. 우리의 객체 탐지기의 모든 기능을 다루는 TFObjectDetector 클래스를 만들어봅시다.</p>
<pre><code class="hljs language-js"># 객체 탐지기 생성
<span class="hljs-keyword">class</span> <span class="hljs-title class_">TFObjectDetector</span>():
</code></pre>
<p>우리는 즉시 <strong>init</strong> 정의를 추가합니다. 이는 클래스의 생성자를 나타내며 다시 말해, TFObjectDetector를 로딩하는 즉시 실행됩니다. 입력 값으로 다음을 받는다는 것을 주목하세요:</p>
<div class="content-ad"></div>
<ul>
<li>객체 검출에 대한 경로는 시스템에 설치된 Object Detection API의 TensorFlow 2.x 구성 파일 경로를 나타냅니다.</li>
<li>실행 중인 모델의 모델 체크포인트 경로 (우리의 경우 SSD MobileNet 모델).</li>
<li>텍스트 레이블에 클래스 ID를 매핑하는 사전을 구성할 수 있도록 하는 레이블 파일의 경로.</li>
<li>모델 이름.</li>
</ul>
<p>⚠ 나중에 실제로 검출기를 사용할 때 상황에 맞게 입력값을 설정하는 방법을 설명할 것입니다.</p>
<p>생성자에서는 여러 작업을 수행합니다. 우선, 입력값을 검출기 전체에 재사용할 수 있도록 많은 인스턴스 변수를 채웁니다. 또한 Object Detection API 폴더에 있는 파이프라인 구성을 로드하고, 우리 모델에 해당하는 구성 파일을 로드하며, 마지막으로 self.setup_model()을 호출합니다.</p>
<p>이로써 모델의 설정 메커니즘을 시작하며, 지금 바로 살펴보겠습니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"># 객체 검출기 생성
<span class="hljs-keyword">class</span> <span class="hljs-title class_">TFObjectDetector</span>():

  # 생성자
  def <span class="hljs-title function_">__init__</span>(self, path_to_object_detection=<span class="hljs-string">'./models/research/object_detection/configs/tf2'</span>,\
    path_to_model_checkpoint=<span class="hljs-string">'./checkpoint'</span>, path_to_labels=<span class="hljs-string">'./labels.pbtxt'</span>,\
      model_name=<span class="hljs-string">'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'</span>):
    self.<span class="hljs-property">model_name</span> = model_name
    self.<span class="hljs-property">pipeline_config_path</span> = path_to_object_detection
    self.<span class="hljs-property">pipeline_config</span> = os.<span class="hljs-property">path</span>.<span class="hljs-title function_">join</span>(f<span class="hljs-string">'{self.pipeline_config_path}/{self.model_name}.config'</span>)
    self.<span class="hljs-property">full_config</span> = config_util.<span class="hljs-title function_">get_configs_from_pipeline_file</span>(self.<span class="hljs-property">pipeline_config</span>)
    self.<span class="hljs-property">path_to_model_checkpoint</span> = path_to_model_checkpoint
    self.<span class="hljs-property">path_to_labels</span> = path_to_labels
    self.<span class="hljs-title function_">setup_model</span>()
</code></pre>
<h2>설정 매커니즘</h2>
<p>설정 매커니즘은 모델을 백그라운드에서 설정하고 객체 검출기를 사용할 수 있게 만드는 역할을 담당합니다. 다음 단계로 구성됩니다:</p>
<ul>
<li><strong>init</strong> 함수에서 로드된 모델 구성을 사용하여 모델을 빌드하는 과정.</li>
<li>특정 상태로 모델을 복원하는 단계, 즉 훈련된 특정 상태로 모델을 복원합니다.</li>
<li>예측을 생성하는 데 사용할 수있는 tf.function인 모델 검출 함수를 검색하는 단계.</li>
<li>클래스 ID 및 텍스트 라벨 간의 매핑을 생성하는 단계으로, 라벨을 준비하는 과정입니다.</li>
</ul>
<div class="content-ad"></div>
<p>위의 단계들의 실행을 setup_model() 정의로 그룹화해 봅시다. 이 정의는 위에서 지정된 <strong>init</strong> 정의에서 호출되며, 따라서 우리의 객체 탐지기를 생성할 때 호출됩니다.</p>
<pre><code class="hljs language-js">  # 모델 설정
  def <span class="hljs-title function_">setup_model</span>(self):
    self.<span class="hljs-title function_">build_model</span>()
    self.<span class="hljs-title function_">restore_checkpoint</span>()
    self.<span class="hljs-property">detection_function</span> = self.<span class="hljs-title function_">get_model_detection_function</span>()
    self.<span class="hljs-title function_">prepare_labels</span>()
</code></pre>
<p>그 다음으로 build_model()를 만들어 봅시다:</p>
<pre><code class="hljs language-js">  # 탐지 모델 빌드
  def <span class="hljs-title function_">build_model</span>(self):
    model_config = self.<span class="hljs-property">full_config</span>[<span class="hljs-string">'model'</span>]
    assert model_config is not <span class="hljs-title class_">None</span>
    self.<span class="hljs-property">model</span> = model_builder.<span class="hljs-title function_">build</span>(model_config=model_config, is_training=<span class="hljs-title class_">False</span>)
    <span class="hljs-keyword">return</span> self.<span class="hljs-property">model</span>
</code></pre>
<div class="content-ad"></div>
<p>이 정의는 구성을 검색하고 존재하는지 확인한 뒤 모델을 빌드합니다. 이 모델은 인스턴스 변수에 할당되어 객체 탐지기 전반에 걸쳐 재사용될 수 있도록 합니다.</p>
<p>restore_checkpoint() 함수를 사용하면 TensorFlow Detection Model Zoo에서 제공하는 체크포인트 위치/상태로 모델을 되돌릴 수 있습니다.</p>
<pre><code class="hljs language-js">  # 모델로 체크포인트 복원
  def <span class="hljs-title function_">restore_checkpoint</span>(self):
    assert self.<span class="hljs-property">model</span> is not <span class="hljs-title class_">None</span>
    self.<span class="hljs-property">checkpoint</span> = tf.<span class="hljs-property">train</span>.<span class="hljs-title class_">Checkpoint</span>(model=self.<span class="hljs-property">model</span>)
    self.<span class="hljs-property">checkpoint</span>.<span class="hljs-title function_">restore</span>(os.<span class="hljs-property">path</span>.<span class="hljs-title function_">join</span>(self.<span class="hljs-property">path_to_model_checkpoint</span>, <span class="hljs-string">'ckpt-0'</span>)).<span class="hljs-title function_">expect_partial</span>()
</code></pre>
<p>그런 다음 탐지를 위한 tf.function을 생성할 수 있습니다. 이 함수는 모델을 활용하여 이미지를 전처리하고 예측을 생성한 후 감지를 처리하고 모든 것을 반환합니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">  # 탐지를 위한 tf.<span class="hljs-property">function</span> 가져오기
  def <span class="hljs-title function_">get_model_detection_function</span>(self):
    assert self.<span class="hljs-property">model</span> is not <span class="hljs-title class_">None</span>
    
    @tf.<span class="hljs-property">function</span>
    def <span class="hljs-title function_">detection_function</span>(image):
      image, shapes = self.<span class="hljs-property">model</span>.<span class="hljs-title function_">preprocess</span>(image)
      prediction_dict = self.<span class="hljs-property">model</span>.<span class="hljs-title function_">predict</span>(image, shapes)
      detections = self.<span class="hljs-property">model</span>.<span class="hljs-title function_">postprocess</span>(prediction_dict, shapes)
      <span class="hljs-keyword">return</span> detections, prediction_dict, tf.<span class="hljs-title function_">reshape</span>(shapes, [-<span class="hljs-number">1</span>])
    
    <span class="hljs-keyword">return</span> detection_function
</code></pre>
<p>마지막으로, prepare_labels()라는 정의를 생성합니다. TensorFlow의 사람들에 의해 만들어졌으며 클래스 식별자를 텍스트 레이블로 매핑하는 책임이 있습니다. 이것은 이 인스턴스 변수로 설정됩니다.</p>
<pre><code class="hljs language-js">  # 레이블 준비
  # 출처: <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb</span>
  def <span class="hljs-title function_">prepare_labels</span>(self):
    label_map = label_map_util.<span class="hljs-title function_">load_labelmap</span>(self.<span class="hljs-property">path_to_labels</span>)
    categories = label_map_util.<span class="hljs-title function_">convert_label_map_to_categories</span>(
        label_map,
        max_num_classes=label_map_util.<span class="hljs-title function_">get_max_label_map_index</span>(label_map),
        use_display_name=<span class="hljs-title class_">True</span>)
    self.<span class="hljs-property">category_index</span> = label_map_util.<span class="hljs-title function_">create_category_index</span>(categories)
    self.<span class="hljs-property">label_map_dict</span> = label_map_util.<span class="hljs-title function_">get_label_map_dict</span>(label_map, use_display_name=<span class="hljs-title class_">True</span>)
</code></pre>
<h2>도우미 함수들</h2>
<div class="content-ad"></div>
<p>지금까지 우리는 객체 탐지기를 준비할 수 있는 기반을 만들었습니다. 이 부분을 완료하려면 두 가지 더 도와주는 함수를 만들기만 하면 됩니다. 첫 번째 함수는 키포인트 튜플을 재구성하고, 두 번째 함수는 이미지를 준비합니다. 즉, 이미지를 텐서로 변환해줍니다.</p>
<pre><code class="hljs language-js">  # <span class="hljs-title class_">Get</span> keypoint tuples
  # 출처: <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb</span>
  def <span class="hljs-title function_">get_keypoint_tuples</span>(self, eval_config):
    tuple_list = []
    kp_list = eval_config.<span class="hljs-property">keypoint_edge</span>
    <span class="hljs-keyword">for</span> edge <span class="hljs-keyword">in</span> <span class="hljs-attr">kp_list</span>:
      tuple_list.<span class="hljs-title function_">append</span>((edge.<span class="hljs-property">start</span>, edge.<span class="hljs-property">end</span>))
    <span class="hljs-keyword">return</span> tuple_list

  
  # <span class="hljs-title class_">Prepare</span> image
  def <span class="hljs-title function_">prepare_image</span>(self, image):
    <span class="hljs-keyword">return</span> tf.<span class="hljs-title function_">convert_to_tensor</span>(
      np.<span class="hljs-title function_">expand_dims</span>(image, <span class="hljs-number">0</span>), dtype=tf.<span class="hljs-property">float32</span>
    )
</code></pre>
<h2>파트 2: 탐지 함수 작성</h2>
<p>와우, 이미 2부에 도착했네요! 이번에는 탐지 함수를 작성할 거예요. 더 자세히 말하면, 세 가지 정의를 만들 것입니다:</p>
<div class="content-ad"></div>
<ul>
<li>일반 탐지 기능입니다. 이 기능은 이미지 및 비디오 감지에 재사용할 수 있는 일반 탐지 코드를 포함하고 있습니다.</li>
<li>이미지 감지입니다. 이 코드는 이미지 내 객체 감지를 위해 특히 사용됩니다.</li>
<li>비디오 감지입니다. 이 코드는 비디오 내 객체 감지를 위해 사용됩니다.</li>
</ul>
<h2>일반 탐지 기능</h2>
<p>첫 번째 정의는 일반 탐지 기능입니다. 이곳에서 일반은 이미지와 비디오에서 감지하는 기능을 공유한다는 뜻입니다. 다시 말해, 무의미하게 두 번 추가할 필요가 없는 것들을 포함합니다! 다음 세그먼트가 포함되어 있습니다:</p>
<ul>
<li>우선, 감지 함수가 None이 아닌지 확인합니다 (위의 Part 1에서). 이는 설정되어 있지 않으면 감지를 수행할 수 없음을 의미합니다.</li>
<li>이미지를 복사하고 텐서로 변환하여 준비합니다. 그런 다음 예측이 포함된 사전 및 모양 정보를 가진 객체를 생성합니다.</li>
<li>키포인트가 있는 경우 사용합니다.</li>
<li>Object Detection API에서 제공하는 viz_utils API를 사용하여 예측과 함께 바운딩 박스를 이미지에 추가합니다.</li>
<li>마지막으로 바운딩 박스가 있는 이미지를 반환합니다.</li>
</ul>
<div class="content-ad"></div>
<pre><code class="hljs language-python"><span class="hljs-comment"># 객체 감지 실행</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">detect</span>(<span class="hljs-params">self, image, label_offset = <span class="hljs-number">1</span></span>):
    <span class="hljs-comment"># 감지 함수가 있는지 확인</span>
    <span class="hljs-keyword">assert</span> self.detection_function <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
    
    <span class="hljs-comment"># 이미지 준비 및 예측 수행</span>
    image = image.copy()
    image_tensor = self.prepare_image(image)
    detections, predictions_dict, shapes = self.detection_function(image_tensor)

    <span class="hljs-comment"># 제공된 키포인트 사용</span>
    keypoints, keypoint_scores = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>
    <span class="hljs-keyword">if</span> <span class="hljs-string">'detection_keypoints'</span> <span class="hljs-keyword">in</span> detections:
        keypoints = detections[<span class="hljs-string">'detection_keypoints'</span>][<span class="hljs-number">0</span>].numpy()
        keypoint_scores = detections[<span class="hljs-string">'detection_keypoint_scores'</span>][<span class="hljs-number">0</span>].numpy()
    
    <span class="hljs-comment"># 출력 이미지/프레임에 시각화 수행</span>
    viz_utils.visualize_boxes_and_labels_on_image_array(
        image,
        detections[<span class="hljs-string">'detection_boxes'</span>][<span class="hljs-number">0</span>].numpy(),
        (detections[<span class="hljs-string">'detection_classes'</span>][<span class="hljs-number">0</span>].numpy() + label_offset).astype(<span class="hljs-built_in">int</span>),
        detections[<span class="hljs-string">'detection_scores'</span>][<span class="hljs-number">0</span>].numpy(),
        self.category_index,
        use_normalized_coordinates=<span class="hljs-literal">True</span>,
        max_boxes_to_draw=<span class="hljs-number">25</span>,
        min_score_thresh=<span class="hljs-number">.40</span>,
        agnostic_mode=<span class="hljs-literal">False</span>,
        keypoints=keypoints,
        keypoint_scores=keypoint_scores,
        keypoint_edges=self.get_keypoint_tuples(self.full_config[<span class="hljs-string">'eval_config'</span>]))
    
    <span class="hljs-comment"># 이미지 반환</span>
    <span class="hljs-keyword">return</span> image
</code></pre>
<h2>이미지용 감지 함수</h2>
<p>이제 어떤 이미지 위의 객체를 감지하는 것이 쉽습니다. OpenCV를 사용하여 경로에서 이미지를 읽고, 일반적인 감지 정의를 호출한 후 결과를 출력 경로에 작성하는 것으로 간단하게 수행할 수 있습니다.</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># 폴더에서 이미지 예측</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">detect_image</span>(<span class="hljs-params">self, path, output_path</span>):

    <span class="hljs-comment"># 이미지 로드</span>
    image = cv2.imread(path)

    <span class="hljs-comment"># 객체 감지 수행 및 출력 파일에 추가</span>
    output_file = self.detect(image)
    
    <span class="hljs-comment"># 출력 파일을 시스템에 작성</span>
    cv2.imwrite(output_path, output_file)
</code></pre>
<div class="content-ad"></div>
<h2>비디오용 Detect 함수</h2>
<p>비디오에서 객체를 감지하는 것은 조금 더 어렵지만 여전히 매우 쉽습니다. 비디오는 종종 초당 25프레임의 이미지로 이루어진 이미지 집합에 불과하다는 것을 상기해주세요. 이 특성을 활용하여 비디오에서 객체 감지를 수행할 것입니다!</p>
<p>이 세그먼트는 다음 단계로 구성되어 있습니다:</p>
<ul>
<li>먼저 출력 비디오 라이터와 코덱을 설정합니다. 이를 통해 바운딩 박스가 그려진 각 프레임을 출력 비디오에 작성할 수 있습니다. 이것은 사실상 비디오 프레임을 바운딩 박스와 함께 한 프레임씩 재구성하는 것을 의미합니다.</li>
<li>그런 다음 OpenCV의 VideoCapture 기능을 사용하여 경로에서 비디오를 읽습니다.</li>
<li>vidcap.read()를 사용하여 첫 번째 프레임(이미지)을 읽고 성공적으로 읽었는지 표시합니다. 프레임 수를 0으로 설정합니다.</li>
<li>이제 프레임을 순환하며 감지를 수행하고(이것이 사실상 이미지에서의 감지임을 알아두세요!) 프레임을 출력 비디오에 작성합니다. 다음 프레임을 읽어 나가며, 더 이상 프레임을 읽을 수 없을 때(즉, frame_read != True가 될 때까지) 계속합니다.</li>
<li>모든 프레임을 처리한 후 출력 비디오를 out.release()를 사용하여 해제합니다.</li>
</ul>
<div class="content-ad"></div>
<pre><code class="hljs language-js">  # 폴더로부터 비디오를 예측합니다
  def <span class="hljs-title function_">detect_video</span>(self, path, output_path):
    
    # 코덱을 사용하여 출력 비디오 작성기를 설정합니다
    fourcc = cv2.<span class="hljs-title class_">VideoWriter</span>_fourcc(*<span class="hljs-string">'mp4v'</span>)
    out = cv2.<span class="hljs-title class_">VideoWriter</span>(output_path, fourcc, <span class="hljs-number">25.0</span>, (<span class="hljs-number">1920</span>, <span class="hljs-number">1080</span>))
    
    # 비디오를 읽어옵니다
    vidcap = cv2.<span class="hljs-title class_">VideoCapture</span>(path)
    frame_read, image = vidcap.<span class="hljs-title function_">read</span>()
    count = <span class="hljs-number">0</span>
    
    # 각 프레임을 반복하면서 예측을 수행합니다
    <span class="hljs-keyword">while</span> <span class="hljs-attr">frame_read</span>:
        
      # 물체 감지를 수행하고 출력 파일에 추가합니다
      output_file = self.<span class="hljs-title function_">detect</span>(image)
      
      # 예측과 함께 프레임을 비디오에 작성합니다
      out.<span class="hljs-title function_">write</span>(output_file)
      
      # 다음 프레임 읽기
      frame_read, image = vidcap.<span class="hljs-title function_">read</span>()
      count += <span class="hljs-number">1</span>
        
    # 비디오 파일을 릴리스합니다
    out.<span class="hljs-title function_">release</span>()
</code></pre>
<h2>섹션 3: ​​감지 호출 생성</h2>
<p>1부 및 2부에서 TFObjectDetector 클래스를 작성하며 감지기를 완성했습니다. 이제 완료 되었으므로 호출해 보는 시간이 됐어요. 다음 코드로 호출할 수 있습니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
  detector = <span class="hljs-title class_">TFObjectDetector</span>(<span class="hljs-string">'../../tf-models/research/object_detection/configs/tf2'</span>, <span class="hljs-string">'./checkpoint'</span>, <span class="hljs-string">'./labels.pbtxt'</span>, <span class="hljs-string">'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'</span>)
  detector.<span class="hljs-title function_">detect_image</span>(<span class="hljs-string">'./shop.jpg'</span>, <span class="hljs-string">'./shopout.jpg'</span>)
  detector.<span class="hljs-title function_">detect_video</span>(<span class="hljs-string">'./video.mp4'</span>, <span class="hljs-string">'./videooutput.mp4'</span>)
</code></pre>
<div class="content-ad"></div>
<p>이 코드는 다음을 수행합니다:</p>
<ul>
<li>직접적으로 실행될 때, 즉 다른 클래스의 컨텍스트 내에서 실행되지 않을 때, 먼저 TFObjectDetector의 새 인스턴스를 생성합니다. 여기에서는 다음 정보를 전달합니다:</li>
<li>클론된 TensorFlow 모델의 tf2 구성 폴더로의 절대 또는 상대 경로입니다.</li>
<li>다운로드한 모델의 모델 체크포인트 폴더로의 절대 또는 상대 경로입니다. 사용하는 SSD MobileNet의 경우, 폴더를 해제하고 열면 ./checkpoint 폴더가 나타납니다. 거기를 참조하세요.</li>
<li>클래스 인덱스와 레이블 이름 간의 매핑에 사용되는 레이블 파일의 절대 또는 상대 경로입니다. 없는 경우 TensorFlow Detection Model Zoo 모델 중 하나에 대해 여기에서 다운로드할 수 있습니다.</li>
<li>모델의 이름입니다. 우리 경우에는 지정한 어려운 이름입니다. Model Zoo에서 다른 이름들 중 하나를 사용할 수도 있지만 그에 맞는 체크포인트를 사용해야 합니다.</li>
<li>./shop.jpg라는 이미지에서 이미지 검출을 수행하고 결과물(즉, 바운딩 상자가 오버레이된 이미지)을 ./shopout.jpg에 저장합니다.</li>
<li>./video.mp4라는 비디오에서 비디오 검출을 수행하고 출력물을 ./videooutput.mp4로 저장합니다.</li>
</ul>
<h2>전체 모델 코드</h2>
<p>바로 코드로 이동하려는 사용자를 위해 전체 모델 코드는 내 Github 저장소에서 찾을 수 있습니다.</p>
<div class="content-ad"></div>
<h2>객체 탐지기 실행하기</h2>
<p>이제 객체 탐지기를 실행한 결과를 살펴보겠습니다.</p>
<p>이 사진들과 동영상은 Pexels 라이선스 하에 다운로드되어 사용되었습니다.</p>
<h2>사진에서</h2>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_4.png" alt="Image 1"></p>
<p><img src="/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_5.png" alt="Image 2"></p>
<p><img src="/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_6.png" alt="Image 3"></p>
<h2>On videos</h2>
<div class="content-ad"></div>
<p>아래는 Markdown 형식으로 표시한 이미지 링크입니다.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1200/1*alxO3bGUDN3dzJnQ6wIoZQ.gif" alt="이미지1"></p>
<p><img src="https://miro.medium.com/v2/resize:fit:1200/1*hTy8jS8LYMGNPgJ6Q_jj2w.gif" alt="이미지2"></p>
<h2>요약</h2>
<p>머신 러닝에서 객체 감지에는 많은 유용한 사례가 있습니다. 이 튜토리얼을 통해 TensorFlow 객체 감지 API와 사전 훈련된 모델을 사용하여 이미지와 비디오에서 객체 감지를 수행하는 방법을 배웠습니다.</p>
<div class="content-ad"></div>
<p>오늘의 글에서 무언가를 배워가셨나요? 궁금한 점, 의견 또는 제안이 있으면 언제든지 환영합니다. 읽어 주셔서 감사합니다!</p>
<h2>참고 자료</h2>
<p>TensorFlow, TensorFlow 로고 및 관련 상표는 Google Inc.의 상표입니다.</p>
<p>TensorFlow. (2020, 9월 9일). TensorFlow/models. GitHub. <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md" rel="nofollow" target="_blank">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md</a></p>
<div class="content-ad"></div>
<p>TensorFlow. (2020, 11). TensorFlow/models. GitHub. <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb" rel="nofollow" target="_blank">https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb</a></p>
<p>TensorFlow. (n.d.). TensorFlow/models. GitHub. <a href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="nofollow" target="_blank">https://github.com/tensorflow/models/tree/master/research/object_detection</a></p>
<p>"현대 합성곱 객체 탐지기의 속도/정확도 균형." 황재식, 라토드 비니트, 썬 첸, 주 만멍, 코라티카라 아니쉬, 파티 아드리아노, 피셔 이안, 우예노비치 세르게이, 송 양초, 과다라마 세르게이, 머피 케빈, CVPR 2017</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"이미지와 비디오를 위한 TensorFlow와 OpenCV를 사용한 객체 감지","description":"","date":"2024-06-20 18:20","slug":"2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV","content":"\n\n![Image](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png)\n\n객체 감지는 딥러닝의 한 분야 중 하나로, 많은 발전이 이루어졌습니다. 다양한 모델을 사용하여 우리는 사진에서 물체를 감지할 수 있고, 그 결과로 비디오에서도 물체를 감지할 수 있습니다. 요즘에는 웹캠 이미지를 사용한 실시간 객체 감지가 흔한 일입니다!\n\n이 튜토리얼에서는 TensorFlow를 사용하여 객체 감지 시스템을 구축할 것입니다. 구체적으로 TensorFlow Object Detection API를 사용할 것입니다. 단계별로 모든 필요한 종속성을 설치하고, TensorFlow Model Zoo의 사전 훈련된 모델을 살펴보고, 객체 감지기를 구축할 것입니다.\n\n다시 말해, 이 튜토리얼을 읽은 후에는...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- TensorFlow 기반 객체 검출기를 구축하기 위해 설치해야 하는 것을 알게 되었습니다.\n- 사전 훈련된 모델을 찾고 시스템에 다운로드하는 위치를 알고 있습니다.\n- 사진 및 비디오와 함께 사용할 수 있는 실제 객체 검출 시스템을 구축했습니다.\n\n그리고 이미지는 항상 수많은 말보다 많은 것을 전달합니다. 아래 시스템을 만들어 보세요!\n\n![이미지](https://miro.medium.com/v2/resize:fit:1200/1*qa_qXkly0MvO82-7uV7LpA.gif)\n\n한번 살펴보시죠!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 객체 탐지기 구축: 필수 조건\n\n텐서플로 Object Detection API를 사용하여 객체 탐지 시스템을 구축하려면 다음 세 가지 단계를 완료해야 합니다:\n\n- TensorFlow 및 OpenCV 설치하기. 우리는 TF 기능을 위해 TensorFlow가 필요하며, 이미지 I/O를 위해 OpenCV가 필요합니다. 보통 시스템에 이미 설치되어 있지만, 완전성을 위해 여기에 포함시켰습니다.\n- TensorFlow Object Detection API 설치하기. 이 추가 기능 세트는 별도로 설치해야 합니다. 어떻게 설치할 수 있는지 살펴보겠습니다.\n- TensorFlow Model Zoo에서 적절한 사전 학습된 모델 찾기. TensorFlow의 제작자들이 다양한 모델 아키텍처를 사용하여 사전 학습된 여러 모델을 TensorFlow Model Zoo에 넣었습니다. 간단히 살펴보고 모델을 선택할 것입니다.\n\n![이미지](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## TensorFlow와 OpenCV 설치하기\n\n실제 객체 탐지기를 구축하기 전에 TensorFlow와 OpenCV를 설치해야 합니다.\n\n여기서는 이미 시스템에 Python이 설치되어 있다고 가정합니다. 그렇지 않은 경우 먼저 Python을 설치해 주세요.\n\n요즘은 TensorFlow를 설치하는 것이 정말 쉽습니다. Python에 액세스할 수 있는 터미널에서 다음을 실행하면 됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 최신 버전의 pip가 필요합니다\npip install --upgrade pip\n\n# CPU 및 GPU용 현재 안정적인 릴리스\npip install tensorflow\n```\n\n먼저 pip를 최신 버전으로 업그레이드하고 TensorFlow를 설치합니다. 이제 CPU 또는 GPU 버전을 수동으로 지정해야 했던 것이 오늘에는 그렇지 않습니다. 그냥 tensorflow를 설치하면 GPU 버전이 정확하게 설정된 경우 GPU 버전이 자동으로 설치됩니다. 실제로 GPU와 CPU 사이를 자유롭게 전환할 수 있지만, 이에 대해서는 나중에 다시 이야기하겠습니다.\n\nOpenCV를 설치하는 것도 어렵지 않습니다: pip install opencv-python으로 해결할 수 있습니다.\n\n이제 기본 패키지가 설치되었으므로 TensorFlow Object Detection API를 살펴볼 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_2.png\" /\u003e\n\n강아지... 와우!\n\n## TensorFlow Object Detection API 설치\n\nGitHub에서 tensorflow/models에서 Object Detection API를 찾을 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이름에서 알 수 있듯이, 이것은 객체 감지 목적으로 사용할 수 있습니다. 특히, 사전 훈련된 모델을 로드하고 이미지 및 비디오에 경계 상자를 추가하는 기능을 제공합니다. 우리의 객체 감지 시스템이 이러한 API를 활용할 수 있다는 것은 우리가 모든 것을 직접 개발할 필요가 없다는 멋진 점입니다.\n\n나중에 사전 훈련된 모델을 살펴볼 것입니다. 먼저 Object Detection API를 설치해 봅시다. 이것은 시스템에 Git이 설치되어 있는 것을 가정합니다. 또한 protoc 명령을 실행할 수 있는지 확인해 주세요. 여기서 확인하는 방법을 찾아보세요.\n\n- 먼저 tensorflow/models 저장소 전체를 복제합니다. 한 단계 깊이만 복제하도록 주의하세요. 다음 명령을 실행하여 저장소를 복제하세요: git clone --depth 1 https://github.com/tensorflow/models\n- 이제 models/research/ 디렉토리로 이동한 다음 protoc object_detection/protos/*.proto --python_out=. 명령을 실행하세요.\n- 그런 다음 cp object_detection/packages/tf2/setup.py 명령을 사용하여 설정 파일을 현재 디렉토리로 복사합니다.\n- 마지막으로 python -m pip install 명령을 통해 Object Detection API를 pip를 통해 설치하세요.\n\n## TensorFlow 모델 동물원: 객체 감지용 사전 훈련된 모델\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희 물체 감지 시스템은 TensorFlow 모델 위에 구축될 예정이에요. 이 모델은 다양한 종류의 물체를 감지할 수 있어요. 이 모델을 훈련하는 과정은 다음과 같아요:\n\n- 다양한 물체가 포함된 많은 이미지를 수집하는 것\n- 이러한 이미지들에 레이블을 달아 모든 클래스가 균형을 이루도록 하는 것\n- 모델을 훈련하는 것\n\n이 과정은 많은 노력이 필요할 거예요. 다행히 TensorFlow 팀은 TensorFlow Detection Model Zoo에서 다양한 사전 훈련된 물체 감지 모델을 제공하고 있어요.\n\n이러한 물체 감지기는 이미 훈련이 완료되어 있으며 TensorFlow Object Detection API에서 이용할 수 있어요 (괄호 안에는 내부 모델 구조가 나와 있어요):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- CenterNet (HourGlass104, Resnet50 V1, Resnet101 V1, Resnet50 V2).\n- EfficientDet (D0, D1, D2, D3, D4, D5, D6, D7).\n- SSD (MobileNet V1 FPN, V2, V2 FPNLite; ResNet50 V1; Resnet101 V1).\n- Faster R-CNN (ResNet50; ResNet101; ResNet152; Inception ResNet V2).\n- Mask R-CNN (Inception ResNet V2).\n- ExtremeNet.\n\n당연히 직접 모델을 만드실 수도 있습니다. 하지만 이 강좌에서 다루지는 않습니다.\n\n오늘은 SSD MobileNet V2 FPNLite 640x640 모델을 사용할 것입니다. Zoo에서 원하는 모델을 선택하실 수 있지만, 이 사전 훈련된 모델은 용량이 20MB밖에 되지 않아서 빠른 인터넷 속도를 가진 많은 사람들이 다운로드할 수 있습니다.\n\n이제 우리의 디텍터를 만들어봅시다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_3.png\" /\u003e\n\n## 객체 탐지기 생성\n\n여기서는 객체 탐지 시스템을 구축하는 방법을 살펴보겠습니다. 이 과정은 세 가지 별개이지만 순차적인 단계로 나눌 수 있습니다:\n\n- 기반을 놓기. 이곳에서는 중요한 imports를 지정하고, 클래스를 정의하고, 초기화 작업을 설명하고, 준비 작업 정의를 작성할 것입니다.\n- 탐지 함수 작성. 이것이 탐지기의 핵심입니다. 이것은 일반적으로 탐지를 수행하고, 특히 이미지와 비디오에 대한 예측을 생성할 수 있게 합니다.\n- 탐지 호출 생성. 마지막으로, 우리의 탐지기가 준비되면 사용할 수 있도록 다음 추가 코드를 추가할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n코드 에디터를 열고 objectdetector.py와 같은 Python 파일을 생성해 주세요. 코드 작성 시작할까요?\n\n## 파트 1: 기반 구축하기\n\nTensorFlow Object Detection API를 기억하시나요? 이것은 물체 감지기를 구축하기 위한 TensorFlow 위의 프레임워크입니다. 다시 말해, 머신 러닝 모델을 만들기 위한 잘 알려진 라이브러리 위에 또 다른 층이란 의미죠. 이 API 위에 Object Detection API를 사용하는 물체 감지기 층을 추가할 계획입니다.\n\n이 TFObjectDetector의 기반을 구축하기 위해서는 Python 임포트 추가, 필요한 경우 GPU 비활성화, TFObjectDetector 클래스 작성 및 초기화, 물체 감지기를 위한 설정 메커니즘 작성, 마지막으로 몇 가지 도우미 함수를 작성해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 파이썬 라이브러리 가져오기\n\n첫 번째 코드는 항상 파이썬 라이브러리를 가져와야 합니다. 오늘도 마찬가지에요:\n\n```js\n# 모델 라이브러리 지정\nfrom object_detection.builders import model_builder\nfrom object_detection.utils import config_util\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import visualization_utils as viz_utils\nimport cv2\nimport numpy as np\nimport os\nimport tensorflow as tf\n```\n\nobject_detection 패키지에서 많은 함수를 가져왔네요 - 이는 TensorFlow Object Detection API를 나타냅니다. 모델 빌더를 사용하여 감지 모델(예: SSD MobileNet 모델)을 구축할 거에요. config_util을 사용하면 TensorFlow에 올바른 모델을 로드하도록 알려주는 구성을 로드할 수 있습니다. 클래스 이름을 나타내는 레이블은 label_map_util을 사용하여 로드할 수 있고, viz_utils는 이미지나 비디오에 경계 상자를 추가하는 데 유용하게 사용될 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nOpenCV (cv2)는 이미지의 입력 및 출력에 사용되며, NumPy (np)는 숫자 처리에 사용되고, os는 운영 체제 기능에 사용되며, 마지막으로 TensorFlow를 import합니다.\n\n## 필요한 경우 GPU 비활성화\n\n두 번째 단계는 GPU를 비활성화하는 것인데, 이것은 선택 사항입니다 — 다시 말해, 원할 경우에만 수행하십시오. 특히 GPU를 보유하고 있지만 구성이 잘못된 경우에 유용할 수 있습니다. 그때 CUDA 가시 장치를 환경에서 모두 지워야 합니다. TensorFlow의 GPU 버전을 사용하지 않는 경우에는 이 코드를 생략할 수 있습니다.\n\n```js\n# 필요한 경우 GPU 비활성화\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 클래스와 초기화자 만들기\n\n이제 진짜 작업을 시작할 시간입니다. 우리의 객체 탐지기의 모든 기능을 다루는 TFObjectDetector 클래스를 만들어봅시다.\n\n```js\n# 객체 탐지기 생성\nclass TFObjectDetector():\n```\n\n우리는 즉시 __init__ 정의를 추가합니다. 이는 클래스의 생성자를 나타내며 다시 말해, TFObjectDetector를 로딩하는 즉시 실행됩니다. 입력 값으로 다음을 받는다는 것을 주목하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 객체 검출에 대한 경로는 시스템에 설치된 Object Detection API의 TensorFlow 2.x 구성 파일 경로를 나타냅니다.\n- 실행 중인 모델의 모델 체크포인트 경로 (우리의 경우 SSD MobileNet 모델).\n- 텍스트 레이블에 클래스 ID를 매핑하는 사전을 구성할 수 있도록 하는 레이블 파일의 경로.\n- 모델 이름.\n\n⚠ 나중에 실제로 검출기를 사용할 때 상황에 맞게 입력값을 설정하는 방법을 설명할 것입니다.\n\n생성자에서는 여러 작업을 수행합니다. 우선, 입력값을 검출기 전체에 재사용할 수 있도록 많은 인스턴스 변수를 채웁니다. 또한 Object Detection API 폴더에 있는 파이프라인 구성을 로드하고, 우리 모델에 해당하는 구성 파일을 로드하며, 마지막으로 self.setup_model()을 호출합니다.\n\n이로써 모델의 설정 메커니즘을 시작하며, 지금 바로 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 객체 검출기 생성\nclass TFObjectDetector():\n\n  # 생성자\n  def __init__(self, path_to_object_detection='./models/research/object_detection/configs/tf2',\\\n    path_to_model_checkpoint='./checkpoint', path_to_labels='./labels.pbtxt',\\\n      model_name='ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'):\n    self.model_name = model_name\n    self.pipeline_config_path = path_to_object_detection\n    self.pipeline_config = os.path.join(f'{self.pipeline_config_path}/{self.model_name}.config')\n    self.full_config = config_util.get_configs_from_pipeline_file(self.pipeline_config)\n    self.path_to_model_checkpoint = path_to_model_checkpoint\n    self.path_to_labels = path_to_labels\n    self.setup_model()\n```\n\n## 설정 매커니즘\n\n설정 매커니즘은 모델을 백그라운드에서 설정하고 객체 검출기를 사용할 수 있게 만드는 역할을 담당합니다. 다음 단계로 구성됩니다:\n\n- __init__ 함수에서 로드된 모델 구성을 사용하여 모델을 빌드하는 과정.\n- 특정 상태로 모델을 복원하는 단계, 즉 훈련된 특정 상태로 모델을 복원합니다.\n- 예측을 생성하는 데 사용할 수있는 tf.function인 모델 검출 함수를 검색하는 단계.\n- 클래스 ID 및 텍스트 라벨 간의 매핑을 생성하는 단계으로, 라벨을 준비하는 과정입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 단계들의 실행을 setup_model() 정의로 그룹화해 봅시다. 이 정의는 위에서 지정된 __init__ 정의에서 호출되며, 따라서 우리의 객체 탐지기를 생성할 때 호출됩니다.\n\n```js\n  # 모델 설정\n  def setup_model(self):\n    self.build_model()\n    self.restore_checkpoint()\n    self.detection_function = self.get_model_detection_function()\n    self.prepare_labels()\n```\n\n그 다음으로 build_model()를 만들어 봅시다:\n\n```js\n  # 탐지 모델 빌드\n  def build_model(self):\n    model_config = self.full_config['model']\n    assert model_config is not None\n    self.model = model_builder.build(model_config=model_config, is_training=False)\n    return self.model\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 정의는 구성을 검색하고 존재하는지 확인한 뒤 모델을 빌드합니다. 이 모델은 인스턴스 변수에 할당되어 객체 탐지기 전반에 걸쳐 재사용될 수 있도록 합니다.\n\nrestore_checkpoint() 함수를 사용하면 TensorFlow Detection Model Zoo에서 제공하는 체크포인트 위치/상태로 모델을 되돌릴 수 있습니다.\n\n```js\n  # 모델로 체크포인트 복원\n  def restore_checkpoint(self):\n    assert self.model is not None\n    self.checkpoint = tf.train.Checkpoint(model=self.model)\n    self.checkpoint.restore(os.path.join(self.path_to_model_checkpoint, 'ckpt-0')).expect_partial()\n```\n\n그런 다음 탐지를 위한 tf.function을 생성할 수 있습니다. 이 함수는 모델을 활용하여 이미지를 전처리하고 예측을 생성한 후 감지를 처리하고 모든 것을 반환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n  # 탐지를 위한 tf.function 가져오기\n  def get_model_detection_function(self):\n    assert self.model is not None\n    \n    @tf.function\n    def detection_function(image):\n      image, shapes = self.model.preprocess(image)\n      prediction_dict = self.model.predict(image, shapes)\n      detections = self.model.postprocess(prediction_dict, shapes)\n      return detections, prediction_dict, tf.reshape(shapes, [-1])\n    \n    return detection_function\n```\n\n마지막으로, prepare_labels()라는 정의를 생성합니다. TensorFlow의 사람들에 의해 만들어졌으며 클래스 식별자를 텍스트 레이블로 매핑하는 책임이 있습니다. 이것은 이 인스턴스 변수로 설정됩니다.\n\n```js\n  # 레이블 준비\n  # 출처: https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\n  def prepare_labels(self):\n    label_map = label_map_util.load_labelmap(self.path_to_labels)\n    categories = label_map_util.convert_label_map_to_categories(\n        label_map,\n        max_num_classes=label_map_util.get_max_label_map_index(label_map),\n        use_display_name=True)\n    self.category_index = label_map_util.create_category_index(categories)\n    self.label_map_dict = label_map_util.get_label_map_dict(label_map, use_display_name=True)\n```\n\n## 도우미 함수들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지 우리는 객체 탐지기를 준비할 수 있는 기반을 만들었습니다. 이 부분을 완료하려면 두 가지 더 도와주는 함수를 만들기만 하면 됩니다. 첫 번째 함수는 키포인트 튜플을 재구성하고, 두 번째 함수는 이미지를 준비합니다. 즉, 이미지를 텐서로 변환해줍니다.\n\n```js\n  # Get keypoint tuples\n  # 출처: https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\n  def get_keypoint_tuples(self, eval_config):\n    tuple_list = []\n    kp_list = eval_config.keypoint_edge\n    for edge in kp_list:\n      tuple_list.append((edge.start, edge.end))\n    return tuple_list\n\n  \n  # Prepare image\n  def prepare_image(self, image):\n    return tf.convert_to_tensor(\n      np.expand_dims(image, 0), dtype=tf.float32\n    )\n```\n\n## 파트 2: 탐지 함수 작성\n\n와우, 이미 2부에 도착했네요! 이번에는 탐지 함수를 작성할 거예요. 더 자세히 말하면, 세 가지 정의를 만들 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 일반 탐지 기능입니다. 이 기능은 이미지 및 비디오 감지에 재사용할 수 있는 일반 탐지 코드를 포함하고 있습니다.\n- 이미지 감지입니다. 이 코드는 이미지 내 객체 감지를 위해 특히 사용됩니다.\n- 비디오 감지입니다. 이 코드는 비디오 내 객체 감지를 위해 사용됩니다.\n\n## 일반 탐지 기능\n\n첫 번째 정의는 일반 탐지 기능입니다. 이곳에서 일반은 이미지와 비디오에서 감지하는 기능을 공유한다는 뜻입니다. 다시 말해, 무의미하게 두 번 추가할 필요가 없는 것들을 포함합니다! 다음 세그먼트가 포함되어 있습니다:\n\n- 우선, 감지 함수가 None이 아닌지 확인합니다 (위의 Part 1에서). 이는 설정되어 있지 않으면 감지를 수행할 수 없음을 의미합니다.\n- 이미지를 복사하고 텐서로 변환하여 준비합니다. 그런 다음 예측이 포함된 사전 및 모양 정보를 가진 객체를 생성합니다.\n- 키포인트가 있는 경우 사용합니다.\n- Object Detection API에서 제공하는 viz_utils API를 사용하여 예측과 함께 바운딩 박스를 이미지에 추가합니다.\n- 마지막으로 바운딩 박스가 있는 이미지를 반환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\n# 객체 감지 실행\ndef detect(self, image, label_offset = 1):\n    # 감지 함수가 있는지 확인\n    assert self.detection_function is not None\n    \n    # 이미지 준비 및 예측 수행\n    image = image.copy()\n    image_tensor = self.prepare_image(image)\n    detections, predictions_dict, shapes = self.detection_function(image_tensor)\n\n    # 제공된 키포인트 사용\n    keypoints, keypoint_scores = None, None\n    if 'detection_keypoints' in detections:\n        keypoints = detections['detection_keypoints'][0].numpy()\n        keypoint_scores = detections['detection_keypoint_scores'][0].numpy()\n    \n    # 출력 이미지/프레임에 시각화 수행\n    viz_utils.visualize_boxes_and_labels_on_image_array(\n        image,\n        detections['detection_boxes'][0].numpy(),\n        (detections['detection_classes'][0].numpy() + label_offset).astype(int),\n        detections['detection_scores'][0].numpy(),\n        self.category_index,\n        use_normalized_coordinates=True,\n        max_boxes_to_draw=25,\n        min_score_thresh=.40,\n        agnostic_mode=False,\n        keypoints=keypoints,\n        keypoint_scores=keypoint_scores,\n        keypoint_edges=self.get_keypoint_tuples(self.full_config['eval_config']))\n    \n    # 이미지 반환\n    return image\n```\n\n## 이미지용 감지 함수\n\n이제 어떤 이미지 위의 객체를 감지하는 것이 쉽습니다. OpenCV를 사용하여 경로에서 이미지를 읽고, 일반적인 감지 정의를 호출한 후 결과를 출력 경로에 작성하는 것으로 간단하게 수행할 수 있습니다.\n\n```python\n# 폴더에서 이미지 예측\ndef detect_image(self, path, output_path):\n\n    # 이미지 로드\n    image = cv2.imread(path)\n\n    # 객체 감지 수행 및 출력 파일에 추가\n    output_file = self.detect(image)\n    \n    # 출력 파일을 시스템에 작성\n    cv2.imwrite(output_path, output_file)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 비디오용 Detect 함수\n\n비디오에서 객체를 감지하는 것은 조금 더 어렵지만 여전히 매우 쉽습니다. 비디오는 종종 초당 25프레임의 이미지로 이루어진 이미지 집합에 불과하다는 것을 상기해주세요. 이 특성을 활용하여 비디오에서 객체 감지를 수행할 것입니다!\n\n이 세그먼트는 다음 단계로 구성되어 있습니다:\n\n- 먼저 출력 비디오 라이터와 코덱을 설정합니다. 이를 통해 바운딩 박스가 그려진 각 프레임을 출력 비디오에 작성할 수 있습니다. 이것은 사실상 비디오 프레임을 바운딩 박스와 함께 한 프레임씩 재구성하는 것을 의미합니다.\n- 그런 다음 OpenCV의 VideoCapture 기능을 사용하여 경로에서 비디오를 읽습니다.\n- vidcap.read()를 사용하여 첫 번째 프레임(이미지)을 읽고 성공적으로 읽었는지 표시합니다. 프레임 수를 0으로 설정합니다.\n- 이제 프레임을 순환하며 감지를 수행하고(이것이 사실상 이미지에서의 감지임을 알아두세요!) 프레임을 출력 비디오에 작성합니다. 다음 프레임을 읽어 나가며, 더 이상 프레임을 읽을 수 없을 때(즉, frame_read != True가 될 때까지) 계속합니다.\n- 모든 프레임을 처리한 후 출력 비디오를 out.release()를 사용하여 해제합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n  # 폴더로부터 비디오를 예측합니다\n  def detect_video(self, path, output_path):\n    \n    # 코덱을 사용하여 출력 비디오 작성기를 설정합니다\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, 25.0, (1920, 1080))\n    \n    # 비디오를 읽어옵니다\n    vidcap = cv2.VideoCapture(path)\n    frame_read, image = vidcap.read()\n    count = 0\n    \n    # 각 프레임을 반복하면서 예측을 수행합니다\n    while frame_read:\n        \n      # 물체 감지를 수행하고 출력 파일에 추가합니다\n      output_file = self.detect(image)\n      \n      # 예측과 함께 프레임을 비디오에 작성합니다\n      out.write(output_file)\n      \n      # 다음 프레임 읽기\n      frame_read, image = vidcap.read()\n      count += 1\n        \n    # 비디오 파일을 릴리스합니다\n    out.release()\n```\n\n## 섹션 3: ​​감지 호출 생성\n\n1부 및 2부에서 TFObjectDetector 클래스를 작성하며 감지기를 완성했습니다. 이제 완료 되었으므로 호출해 보는 시간이 됐어요. 다음 코드로 호출할 수 있습니다.\n\n```js\nif __name__ == '__main__':\n  detector = TFObjectDetector('../../tf-models/research/object_detection/configs/tf2', './checkpoint', './labels.pbtxt', 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8')\n  detector.detect_image('./shop.jpg', './shopout.jpg')\n  detector.detect_video('./video.mp4', './videooutput.mp4')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 코드는 다음을 수행합니다:\n\n- 직접적으로 실행될 때, 즉 다른 클래스의 컨텍스트 내에서 실행되지 않을 때, 먼저 TFObjectDetector의 새 인스턴스를 생성합니다. 여기에서는 다음 정보를 전달합니다:\n- 클론된 TensorFlow 모델의 tf2 구성 폴더로의 절대 또는 상대 경로입니다.\n- 다운로드한 모델의 모델 체크포인트 폴더로의 절대 또는 상대 경로입니다. 사용하는 SSD MobileNet의 경우, 폴더를 해제하고 열면 ./checkpoint 폴더가 나타납니다. 거기를 참조하세요.\n- 클래스 인덱스와 레이블 이름 간의 매핑에 사용되는 레이블 파일의 절대 또는 상대 경로입니다. 없는 경우 TensorFlow Detection Model Zoo 모델 중 하나에 대해 여기에서 다운로드할 수 있습니다.\n- 모델의 이름입니다. 우리 경우에는 지정한 어려운 이름입니다. Model Zoo에서 다른 이름들 중 하나를 사용할 수도 있지만 그에 맞는 체크포인트를 사용해야 합니다.\n- ./shop.jpg라는 이미지에서 이미지 검출을 수행하고 결과물(즉, 바운딩 상자가 오버레이된 이미지)을 ./shopout.jpg에 저장합니다.\n- ./video.mp4라는 비디오에서 비디오 검출을 수행하고 출력물을 ./videooutput.mp4로 저장합니다.\n\n## 전체 모델 코드\n\n바로 코드로 이동하려는 사용자를 위해 전체 모델 코드는 내 Github 저장소에서 찾을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 객체 탐지기 실행하기\n\n이제 객체 탐지기를 실행한 결과를 살펴보겠습니다.\n\n이 사진들과 동영상은 Pexels 라이선스 하에 다운로드되어 사용되었습니다.\n\n## 사진에서\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_4.png)\n\n![Image 2](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_5.png)\n\n![Image 3](/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_6.png)\n\n## On videos\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식으로 표시한 이미지 링크입니다.\n\n![이미지1](https://miro.medium.com/v2/resize:fit:1200/1*alxO3bGUDN3dzJnQ6wIoZQ.gif)\n\n![이미지2](https://miro.medium.com/v2/resize:fit:1200/1*hTy8jS8LYMGNPgJ6Q_jj2w.gif)\n\n## 요약\n\n머신 러닝에서 객체 감지에는 많은 유용한 사례가 있습니다. 이 튜토리얼을 통해 TensorFlow 객체 감지 API와 사전 훈련된 모델을 사용하여 이미지와 비디오에서 객체 감지를 수행하는 방법을 배웠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오늘의 글에서 무언가를 배워가셨나요? 궁금한 점, 의견 또는 제안이 있으면 언제든지 환영합니다. 읽어 주셔서 감사합니다!\n\n## 참고 자료\n\nTensorFlow, TensorFlow 로고 및 관련 상표는 Google Inc.의 상표입니다.\n\nTensorFlow. (2020, 9월 9일). TensorFlow/models. GitHub. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTensorFlow. (2020, 11). TensorFlow/models. GitHub. https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\n\nTensorFlow. (n.d.). TensorFlow/models. GitHub. https://github.com/tensorflow/models/tree/master/research/object_detection\n\n\"현대 합성곱 객체 탐지기의 속도/정확도 균형.\" 황재식, 라토드 비니트, 썬 첸, 주 만멍, 코라티카라 아니쉬, 파티 아드리아노, 피셔 이안, 우예노비치 세르게이, 송 양초, 과다라마 세르게이, 머피 케빈, CVPR 2017","ogImage":{"url":"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png"},"coverImage":"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png","tag":["Tech"],"readingTime":18},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_0.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003e객체 감지는 딥러닝의 한 분야 중 하나로, 많은 발전이 이루어졌습니다. 다양한 모델을 사용하여 우리는 사진에서 물체를 감지할 수 있고, 그 결과로 비디오에서도 물체를 감지할 수 있습니다. 요즘에는 웹캠 이미지를 사용한 실시간 객체 감지가 흔한 일입니다!\u003c/p\u003e\n\u003cp\u003e이 튜토리얼에서는 TensorFlow를 사용하여 객체 감지 시스템을 구축할 것입니다. 구체적으로 TensorFlow Object Detection API를 사용할 것입니다. 단계별로 모든 필요한 종속성을 설치하고, TensorFlow Model Zoo의 사전 훈련된 모델을 살펴보고, 객체 감지기를 구축할 것입니다.\u003c/p\u003e\n\u003cp\u003e다시 말해, 이 튜토리얼을 읽은 후에는...\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eTensorFlow 기반 객체 검출기를 구축하기 위해 설치해야 하는 것을 알게 되었습니다.\u003c/li\u003e\n\u003cli\u003e사전 훈련된 모델을 찾고 시스템에 다운로드하는 위치를 알고 있습니다.\u003c/li\u003e\n\u003cli\u003e사진 및 비디오와 함께 사용할 수 있는 실제 객체 검출 시스템을 구축했습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e그리고 이미지는 항상 수많은 말보다 많은 것을 전달합니다. 아래 시스템을 만들어 보세요!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1200/1*qa_qXkly0MvO82-7uV7LpA.gif\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e한번 살펴보시죠!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e객체 탐지기 구축: 필수 조건\u003c/h2\u003e\n\u003cp\u003e텐서플로 Object Detection API를 사용하여 객체 탐지 시스템을 구축하려면 다음 세 가지 단계를 완료해야 합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTensorFlow 및 OpenCV 설치하기. 우리는 TF 기능을 위해 TensorFlow가 필요하며, 이미지 I/O를 위해 OpenCV가 필요합니다. 보통 시스템에 이미 설치되어 있지만, 완전성을 위해 여기에 포함시켰습니다.\u003c/li\u003e\n\u003cli\u003eTensorFlow Object Detection API 설치하기. 이 추가 기능 세트는 별도로 설치해야 합니다. 어떻게 설치할 수 있는지 살펴보겠습니다.\u003c/li\u003e\n\u003cli\u003eTensorFlow Model Zoo에서 적절한 사전 학습된 모델 찾기. TensorFlow의 제작자들이 다양한 모델 아키텍처를 사용하여 사전 학습된 여러 모델을 TensorFlow Model Zoo에 넣었습니다. 간단히 살펴보고 모델을 선택할 것입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003eTensorFlow와 OpenCV 설치하기\u003c/h2\u003e\n\u003cp\u003e실제 객체 탐지기를 구축하기 전에 TensorFlow와 OpenCV를 설치해야 합니다.\u003c/p\u003e\n\u003cp\u003e여기서는 이미 시스템에 Python이 설치되어 있다고 가정합니다. 그렇지 않은 경우 먼저 Python을 설치해 주세요.\u003c/p\u003e\n\u003cp\u003e요즘은 TensorFlow를 설치하는 것이 정말 쉽습니다. Python에 액세스할 수 있는 터미널에서 다음을 실행하면 됩니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 최신 버전의 pip가 필요합니다\npip install --upgrade pip\n\n# \u003cspan class=\"hljs-variable constant_\"\u003eCPU\u003c/span\u003e 및 \u003cspan class=\"hljs-variable constant_\"\u003eGPU\u003c/span\u003e용 현재 안정적인 릴리스\npip install tensorflow\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e먼저 pip를 최신 버전으로 업그레이드하고 TensorFlow를 설치합니다. 이제 CPU 또는 GPU 버전을 수동으로 지정해야 했던 것이 오늘에는 그렇지 않습니다. 그냥 tensorflow를 설치하면 GPU 버전이 정확하게 설정된 경우 GPU 버전이 자동으로 설치됩니다. 실제로 GPU와 CPU 사이를 자유롭게 전환할 수 있지만, 이에 대해서는 나중에 다시 이야기하겠습니다.\u003c/p\u003e\n\u003cp\u003eOpenCV를 설치하는 것도 어렵지 않습니다: pip install opencv-python으로 해결할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이제 기본 패키지가 설치되었으므로 TensorFlow Object Detection API를 살펴볼 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_2.png\"\u003e\n\u003cp\u003e강아지... 와우!\u003c/p\u003e\n\u003ch2\u003eTensorFlow Object Detection API 설치\u003c/h2\u003e\n\u003cp\u003eGitHub에서 tensorflow/models에서 Object Detection API를 찾을 수 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이름에서 알 수 있듯이, 이것은 객체 감지 목적으로 사용할 수 있습니다. 특히, 사전 훈련된 모델을 로드하고 이미지 및 비디오에 경계 상자를 추가하는 기능을 제공합니다. 우리의 객체 감지 시스템이 이러한 API를 활용할 수 있다는 것은 우리가 모든 것을 직접 개발할 필요가 없다는 멋진 점입니다.\u003c/p\u003e\n\u003cp\u003e나중에 사전 훈련된 모델을 살펴볼 것입니다. 먼저 Object Detection API를 설치해 봅시다. 이것은 시스템에 Git이 설치되어 있는 것을 가정합니다. 또한 protoc 명령을 실행할 수 있는지 확인해 주세요. 여기서 확인하는 방법을 찾아보세요.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e먼저 tensorflow/models 저장소 전체를 복제합니다. 한 단계 깊이만 복제하도록 주의하세요. 다음 명령을 실행하여 저장소를 복제하세요: git clone --depth 1 \u003ca href=\"https://github.com/tensorflow/models\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/tensorflow/models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e이제 models/research/ 디렉토리로 이동한 다음 protoc object_detection/protos/*.proto --python_out=. 명령을 실행하세요.\u003c/li\u003e\n\u003cli\u003e그런 다음 cp object_detection/packages/tf2/setup.py 명령을 사용하여 설정 파일을 현재 디렉토리로 복사합니다.\u003c/li\u003e\n\u003cli\u003e마지막으로 python -m pip install 명령을 통해 Object Detection API를 pip를 통해 설치하세요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eTensorFlow 모델 동물원: 객체 감지용 사전 훈련된 모델\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e저희 물체 감지 시스템은 TensorFlow 모델 위에 구축될 예정이에요. 이 모델은 다양한 종류의 물체를 감지할 수 있어요. 이 모델을 훈련하는 과정은 다음과 같아요:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e다양한 물체가 포함된 많은 이미지를 수집하는 것\u003c/li\u003e\n\u003cli\u003e이러한 이미지들에 레이블을 달아 모든 클래스가 균형을 이루도록 하는 것\u003c/li\u003e\n\u003cli\u003e모델을 훈련하는 것\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이 과정은 많은 노력이 필요할 거예요. 다행히 TensorFlow 팀은 TensorFlow Detection Model Zoo에서 다양한 사전 훈련된 물체 감지 모델을 제공하고 있어요.\u003c/p\u003e\n\u003cp\u003e이러한 물체 감지기는 이미 훈련이 완료되어 있으며 TensorFlow Object Detection API에서 이용할 수 있어요 (괄호 안에는 내부 모델 구조가 나와 있어요):\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eCenterNet (HourGlass104, Resnet50 V1, Resnet101 V1, Resnet50 V2).\u003c/li\u003e\n\u003cli\u003eEfficientDet (D0, D1, D2, D3, D4, D5, D6, D7).\u003c/li\u003e\n\u003cli\u003eSSD (MobileNet V1 FPN, V2, V2 FPNLite; ResNet50 V1; Resnet101 V1).\u003c/li\u003e\n\u003cli\u003eFaster R-CNN (ResNet50; ResNet101; ResNet152; Inception ResNet V2).\u003c/li\u003e\n\u003cli\u003eMask R-CNN (Inception ResNet V2).\u003c/li\u003e\n\u003cli\u003eExtremeNet.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e당연히 직접 모델을 만드실 수도 있습니다. 하지만 이 강좌에서 다루지는 않습니다.\u003c/p\u003e\n\u003cp\u003e오늘은 SSD MobileNet V2 FPNLite 640x640 모델을 사용할 것입니다. Zoo에서 원하는 모델을 선택하실 수 있지만, 이 사전 훈련된 모델은 용량이 20MB밖에 되지 않아서 빠른 인터넷 속도를 가진 많은 사람들이 다운로드할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이제 우리의 디텍터를 만들어봅시다!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_3.png\"\u003e\n\u003ch2\u003e객체 탐지기 생성\u003c/h2\u003e\n\u003cp\u003e여기서는 객체 탐지 시스템을 구축하는 방법을 살펴보겠습니다. 이 과정은 세 가지 별개이지만 순차적인 단계로 나눌 수 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e기반을 놓기. 이곳에서는 중요한 imports를 지정하고, 클래스를 정의하고, 초기화 작업을 설명하고, 준비 작업 정의를 작성할 것입니다.\u003c/li\u003e\n\u003cli\u003e탐지 함수 작성. 이것이 탐지기의 핵심입니다. 이것은 일반적으로 탐지를 수행하고, 특히 이미지와 비디오에 대한 예측을 생성할 수 있게 합니다.\u003c/li\u003e\n\u003cli\u003e탐지 호출 생성. 마지막으로, 우리의 탐지기가 준비되면 사용할 수 있도록 다음 추가 코드를 추가할 것입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e코드 에디터를 열고 objectdetector.py와 같은 Python 파일을 생성해 주세요. 코드 작성 시작할까요?\u003c/p\u003e\n\u003ch2\u003e파트 1: 기반 구축하기\u003c/h2\u003e\n\u003cp\u003eTensorFlow Object Detection API를 기억하시나요? 이것은 물체 감지기를 구축하기 위한 TensorFlow 위의 프레임워크입니다. 다시 말해, 머신 러닝 모델을 만들기 위한 잘 알려진 라이브러리 위에 또 다른 층이란 의미죠. 이 API 위에 Object Detection API를 사용하는 물체 감지기 층을 추가할 계획입니다.\u003c/p\u003e\n\u003cp\u003e이 TFObjectDetector의 기반을 구축하기 위해서는 Python 임포트 추가, 필요한 경우 GPU 비활성화, TFObjectDetector 클래스 작성 및 초기화, 물체 감지기를 위한 설정 메커니즘 작성, 마지막으로 몇 가지 도우미 함수를 작성해야 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e파이썬 라이브러리 가져오기\u003c/h2\u003e\n\u003cp\u003e첫 번째 코드는 항상 파이썬 라이브러리를 가져와야 합니다. 오늘도 마찬가지에요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 모델 라이브러리 지정\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e object_detection.\u003cspan class=\"hljs-property\"\u003ebuilders\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e model_builder\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e object_detection.\u003cspan class=\"hljs-property\"\u003eutils\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e config_util\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e object_detection.\u003cspan class=\"hljs-property\"\u003eutils\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e label_map_util\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e object_detection.\u003cspan class=\"hljs-property\"\u003eutils\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e visualization_utils \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e viz_utils\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e cv2\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tensorflow \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e tf\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eobject_detection 패키지에서 많은 함수를 가져왔네요 - 이는 TensorFlow Object Detection API를 나타냅니다. 모델 빌더를 사용하여 감지 모델(예: SSD MobileNet 모델)을 구축할 거에요. config_util을 사용하면 TensorFlow에 올바른 모델을 로드하도록 알려주는 구성을 로드할 수 있습니다. 클래스 이름을 나타내는 레이블은 label_map_util을 사용하여 로드할 수 있고, viz_utils는 이미지나 비디오에 경계 상자를 추가하는 데 유용하게 사용될 거에요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eOpenCV (cv2)는 이미지의 입력 및 출력에 사용되며, NumPy (np)는 숫자 처리에 사용되고, os는 운영 체제 기능에 사용되며, 마지막으로 TensorFlow를 import합니다.\u003c/p\u003e\n\u003ch2\u003e필요한 경우 GPU 비활성화\u003c/h2\u003e\n\u003cp\u003e두 번째 단계는 GPU를 비활성화하는 것인데, 이것은 선택 사항입니다 — 다시 말해, 원할 경우에만 수행하십시오. 특히 GPU를 보유하고 있지만 구성이 잘못된 경우에 유용할 수 있습니다. 그때 CUDA 가시 장치를 환경에서 모두 지워야 합니다. TensorFlow의 GPU 버전을 사용하지 않는 경우에는 이 코드를 생략할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 필요한 경우 \u003cspan class=\"hljs-variable constant_\"\u003eGPU\u003c/span\u003e 비활성화\nos.\u003cspan class=\"hljs-property\"\u003eenviron\u003c/span\u003e[\u003cspan class=\"hljs-string\"\u003e'CUDA_VISIBLE_DEVICES'\u003c/span\u003e] = \u003cspan class=\"hljs-string\"\u003e'-1'\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e클래스와 초기화자 만들기\u003c/h2\u003e\n\u003cp\u003e이제 진짜 작업을 시작할 시간입니다. 우리의 객체 탐지기의 모든 기능을 다루는 TFObjectDetector 클래스를 만들어봅시다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 객체 탐지기 생성\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTFObjectDetector\u003c/span\u003e():\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e우리는 즉시 \u003cstrong\u003einit\u003c/strong\u003e 정의를 추가합니다. 이는 클래스의 생성자를 나타내며 다시 말해, TFObjectDetector를 로딩하는 즉시 실행됩니다. 입력 값으로 다음을 받는다는 것을 주목하세요:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e객체 검출에 대한 경로는 시스템에 설치된 Object Detection API의 TensorFlow 2.x 구성 파일 경로를 나타냅니다.\u003c/li\u003e\n\u003cli\u003e실행 중인 모델의 모델 체크포인트 경로 (우리의 경우 SSD MobileNet 모델).\u003c/li\u003e\n\u003cli\u003e텍스트 레이블에 클래스 ID를 매핑하는 사전을 구성할 수 있도록 하는 레이블 파일의 경로.\u003c/li\u003e\n\u003cli\u003e모델 이름.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e⚠ 나중에 실제로 검출기를 사용할 때 상황에 맞게 입력값을 설정하는 방법을 설명할 것입니다.\u003c/p\u003e\n\u003cp\u003e생성자에서는 여러 작업을 수행합니다. 우선, 입력값을 검출기 전체에 재사용할 수 있도록 많은 인스턴스 변수를 채웁니다. 또한 Object Detection API 폴더에 있는 파이프라인 구성을 로드하고, 우리 모델에 해당하는 구성 파일을 로드하며, 마지막으로 self.setup_model()을 호출합니다.\u003c/p\u003e\n\u003cp\u003e이로써 모델의 설정 메커니즘을 시작하며, 지금 바로 살펴보겠습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 객체 검출기 생성\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTFObjectDetector\u003c/span\u003e():\n\n  # 생성자\n  def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, path_to_object_detection=\u003cspan class=\"hljs-string\"\u003e'./models/research/object_detection/configs/tf2'\u003c/span\u003e,\\\n    path_to_model_checkpoint=\u003cspan class=\"hljs-string\"\u003e'./checkpoint'\u003c/span\u003e, path_to_labels=\u003cspan class=\"hljs-string\"\u003e'./labels.pbtxt'\u003c/span\u003e,\\\n      model_name=\u003cspan class=\"hljs-string\"\u003e'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'\u003c/span\u003e):\n    self.\u003cspan class=\"hljs-property\"\u003emodel_name\u003c/span\u003e = model_name\n    self.\u003cspan class=\"hljs-property\"\u003epipeline_config_path\u003c/span\u003e = path_to_object_detection\n    self.\u003cspan class=\"hljs-property\"\u003epipeline_config\u003c/span\u003e = os.\u003cspan class=\"hljs-property\"\u003epath\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ejoin\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'{self.pipeline_config_path}/{self.model_name}.config'\u003c/span\u003e)\n    self.\u003cspan class=\"hljs-property\"\u003efull_config\u003c/span\u003e = config_util.\u003cspan class=\"hljs-title function_\"\u003eget_configs_from_pipeline_file\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003epipeline_config\u003c/span\u003e)\n    self.\u003cspan class=\"hljs-property\"\u003epath_to_model_checkpoint\u003c/span\u003e = path_to_model_checkpoint\n    self.\u003cspan class=\"hljs-property\"\u003epath_to_labels\u003c/span\u003e = path_to_labels\n    self.\u003cspan class=\"hljs-title function_\"\u003esetup_model\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e설정 매커니즘\u003c/h2\u003e\n\u003cp\u003e설정 매커니즘은 모델을 백그라운드에서 설정하고 객체 검출기를 사용할 수 있게 만드는 역할을 담당합니다. 다음 단계로 구성됩니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003einit\u003c/strong\u003e 함수에서 로드된 모델 구성을 사용하여 모델을 빌드하는 과정.\u003c/li\u003e\n\u003cli\u003e특정 상태로 모델을 복원하는 단계, 즉 훈련된 특정 상태로 모델을 복원합니다.\u003c/li\u003e\n\u003cli\u003e예측을 생성하는 데 사용할 수있는 tf.function인 모델 검출 함수를 검색하는 단계.\u003c/li\u003e\n\u003cli\u003e클래스 ID 및 텍스트 라벨 간의 매핑을 생성하는 단계으로, 라벨을 준비하는 과정입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e위의 단계들의 실행을 setup_model() 정의로 그룹화해 봅시다. 이 정의는 위에서 지정된 \u003cstrong\u003einit\u003c/strong\u003e 정의에서 호출되며, 따라서 우리의 객체 탐지기를 생성할 때 호출됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e  # 모델 설정\n  def \u003cspan class=\"hljs-title function_\"\u003esetup_model\u003c/span\u003e(self):\n    self.\u003cspan class=\"hljs-title function_\"\u003ebuild_model\u003c/span\u003e()\n    self.\u003cspan class=\"hljs-title function_\"\u003erestore_checkpoint\u003c/span\u003e()\n    self.\u003cspan class=\"hljs-property\"\u003edetection_function\u003c/span\u003e = self.\u003cspan class=\"hljs-title function_\"\u003eget_model_detection_function\u003c/span\u003e()\n    self.\u003cspan class=\"hljs-title function_\"\u003eprepare_labels\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그 다음으로 build_model()를 만들어 봅시다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e  # 탐지 모델 빌드\n  def \u003cspan class=\"hljs-title function_\"\u003ebuild_model\u003c/span\u003e(self):\n    model_config = self.\u003cspan class=\"hljs-property\"\u003efull_config\u003c/span\u003e[\u003cspan class=\"hljs-string\"\u003e'model'\u003c/span\u003e]\n    assert model_config is not \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e\n    self.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e = model_builder.\u003cspan class=\"hljs-title function_\"\u003ebuild\u003c/span\u003e(model_config=model_config, is_training=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 정의는 구성을 검색하고 존재하는지 확인한 뒤 모델을 빌드합니다. 이 모델은 인스턴스 변수에 할당되어 객체 탐지기 전반에 걸쳐 재사용될 수 있도록 합니다.\u003c/p\u003e\n\u003cp\u003erestore_checkpoint() 함수를 사용하면 TensorFlow Detection Model Zoo에서 제공하는 체크포인트 위치/상태로 모델을 되돌릴 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e  # 모델로 체크포인트 복원\n  def \u003cspan class=\"hljs-title function_\"\u003erestore_checkpoint\u003c/span\u003e(self):\n    assert self.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e is not \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e\n    self.\u003cspan class=\"hljs-property\"\u003echeckpoint\u003c/span\u003e = tf.\u003cspan class=\"hljs-property\"\u003etrain\u003c/span\u003e.\u003cspan class=\"hljs-title class_\"\u003eCheckpoint\u003c/span\u003e(model=self.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e)\n    self.\u003cspan class=\"hljs-property\"\u003echeckpoint\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erestore\u003c/span\u003e(os.\u003cspan class=\"hljs-property\"\u003epath\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ejoin\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003epath_to_model_checkpoint\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'ckpt-0'\u003c/span\u003e)).\u003cspan class=\"hljs-title function_\"\u003eexpect_partial\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그런 다음 탐지를 위한 tf.function을 생성할 수 있습니다. 이 함수는 모델을 활용하여 이미지를 전처리하고 예측을 생성한 후 감지를 처리하고 모든 것을 반환합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e  # 탐지를 위한 tf.\u003cspan class=\"hljs-property\"\u003efunction\u003c/span\u003e 가져오기\n  def \u003cspan class=\"hljs-title function_\"\u003eget_model_detection_function\u003c/span\u003e(self):\n    assert self.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e is not \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e\n    \n    @tf.\u003cspan class=\"hljs-property\"\u003efunction\u003c/span\u003e\n    def \u003cspan class=\"hljs-title function_\"\u003edetection_function\u003c/span\u003e(image):\n      image, shapes = self.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003epreprocess\u003c/span\u003e(image)\n      prediction_dict = self.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(image, shapes)\n      detections = self.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003epostprocess\u003c/span\u003e(prediction_dict, shapes)\n      \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e detections, prediction_dict, tf.\u003cspan class=\"hljs-title function_\"\u003ereshape\u003c/span\u003e(shapes, [-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n    \n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e detection_function\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e마지막으로, prepare_labels()라는 정의를 생성합니다. TensorFlow의 사람들에 의해 만들어졌으며 클래스 식별자를 텍스트 레이블로 매핑하는 책임이 있습니다. 이것은 이 인스턴스 변수로 설정됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e  # 레이블 준비\n  # 출처: \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\u003c/span\u003e\n  def \u003cspan class=\"hljs-title function_\"\u003eprepare_labels\u003c/span\u003e(self):\n    label_map = label_map_util.\u003cspan class=\"hljs-title function_\"\u003eload_labelmap\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003epath_to_labels\u003c/span\u003e)\n    categories = label_map_util.\u003cspan class=\"hljs-title function_\"\u003econvert_label_map_to_categories\u003c/span\u003e(\n        label_map,\n        max_num_classes=label_map_util.\u003cspan class=\"hljs-title function_\"\u003eget_max_label_map_index\u003c/span\u003e(label_map),\n        use_display_name=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n    self.\u003cspan class=\"hljs-property\"\u003ecategory_index\u003c/span\u003e = label_map_util.\u003cspan class=\"hljs-title function_\"\u003ecreate_category_index\u003c/span\u003e(categories)\n    self.\u003cspan class=\"hljs-property\"\u003elabel_map_dict\u003c/span\u003e = label_map_util.\u003cspan class=\"hljs-title function_\"\u003eget_label_map_dict\u003c/span\u003e(label_map, use_display_name=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e도우미 함수들\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e지금까지 우리는 객체 탐지기를 준비할 수 있는 기반을 만들었습니다. 이 부분을 완료하려면 두 가지 더 도와주는 함수를 만들기만 하면 됩니다. 첫 번째 함수는 키포인트 튜플을 재구성하고, 두 번째 함수는 이미지를 준비합니다. 즉, 이미지를 텐서로 변환해줍니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e  # \u003cspan class=\"hljs-title class_\"\u003eGet\u003c/span\u003e keypoint tuples\n  # 출처: \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\u003c/span\u003e\n  def \u003cspan class=\"hljs-title function_\"\u003eget_keypoint_tuples\u003c/span\u003e(self, eval_config):\n    tuple_list = []\n    kp_list = eval_config.\u003cspan class=\"hljs-property\"\u003ekeypoint_edge\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e edge \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ekp_list\u003c/span\u003e:\n      tuple_list.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e((edge.\u003cspan class=\"hljs-property\"\u003estart\u003c/span\u003e, edge.\u003cspan class=\"hljs-property\"\u003eend\u003c/span\u003e))\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e tuple_list\n\n  \n  # \u003cspan class=\"hljs-title class_\"\u003ePrepare\u003c/span\u003e image\n  def \u003cspan class=\"hljs-title function_\"\u003eprepare_image\u003c/span\u003e(self, image):\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e tf.\u003cspan class=\"hljs-title function_\"\u003econvert_to_tensor\u003c/span\u003e(\n      np.\u003cspan class=\"hljs-title function_\"\u003eexpand_dims\u003c/span\u003e(image, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e), dtype=tf.\u003cspan class=\"hljs-property\"\u003efloat32\u003c/span\u003e\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e파트 2: 탐지 함수 작성\u003c/h2\u003e\n\u003cp\u003e와우, 이미 2부에 도착했네요! 이번에는 탐지 함수를 작성할 거예요. 더 자세히 말하면, 세 가지 정의를 만들 것입니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e일반 탐지 기능입니다. 이 기능은 이미지 및 비디오 감지에 재사용할 수 있는 일반 탐지 코드를 포함하고 있습니다.\u003c/li\u003e\n\u003cli\u003e이미지 감지입니다. 이 코드는 이미지 내 객체 감지를 위해 특히 사용됩니다.\u003c/li\u003e\n\u003cli\u003e비디오 감지입니다. 이 코드는 비디오 내 객체 감지를 위해 사용됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e일반 탐지 기능\u003c/h2\u003e\n\u003cp\u003e첫 번째 정의는 일반 탐지 기능입니다. 이곳에서 일반은 이미지와 비디오에서 감지하는 기능을 공유한다는 뜻입니다. 다시 말해, 무의미하게 두 번 추가할 필요가 없는 것들을 포함합니다! 다음 세그먼트가 포함되어 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e우선, 감지 함수가 None이 아닌지 확인합니다 (위의 Part 1에서). 이는 설정되어 있지 않으면 감지를 수행할 수 없음을 의미합니다.\u003c/li\u003e\n\u003cli\u003e이미지를 복사하고 텐서로 변환하여 준비합니다. 그런 다음 예측이 포함된 사전 및 모양 정보를 가진 객체를 생성합니다.\u003c/li\u003e\n\u003cli\u003e키포인트가 있는 경우 사용합니다.\u003c/li\u003e\n\u003cli\u003eObject Detection API에서 제공하는 viz_utils API를 사용하여 예측과 함께 바운딩 박스를 이미지에 추가합니다.\u003c/li\u003e\n\u003cli\u003e마지막으로 바운딩 박스가 있는 이미지를 반환합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# 객체 감지 실행\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003edetect\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, image, label_offset = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\u003c/span\u003e):\n    \u003cspan class=\"hljs-comment\"\u003e# 감지 함수가 있는지 확인\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eassert\u003c/span\u003e self.detection_function \u003cspan class=\"hljs-keyword\"\u003eis\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\n    \n    \u003cspan class=\"hljs-comment\"\u003e# 이미지 준비 및 예측 수행\u003c/span\u003e\n    image = image.copy()\n    image_tensor = self.prepare_image(image)\n    detections, predictions_dict, shapes = self.detection_function(image_tensor)\n\n    \u003cspan class=\"hljs-comment\"\u003e# 제공된 키포인트 사용\u003c/span\u003e\n    keypoints, keypoint_scores = \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'detection_keypoints'\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e detections:\n        keypoints = detections[\u003cspan class=\"hljs-string\"\u003e'detection_keypoints'\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].numpy()\n        keypoint_scores = detections[\u003cspan class=\"hljs-string\"\u003e'detection_keypoint_scores'\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].numpy()\n    \n    \u003cspan class=\"hljs-comment\"\u003e# 출력 이미지/프레임에 시각화 수행\u003c/span\u003e\n    viz_utils.visualize_boxes_and_labels_on_image_array(\n        image,\n        detections[\u003cspan class=\"hljs-string\"\u003e'detection_boxes'\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].numpy(),\n        (detections[\u003cspan class=\"hljs-string\"\u003e'detection_classes'\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].numpy() + label_offset).astype(\u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e),\n        detections[\u003cspan class=\"hljs-string\"\u003e'detection_scores'\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].numpy(),\n        self.category_index,\n        use_normalized_coordinates=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n        max_boxes_to_draw=\u003cspan class=\"hljs-number\"\u003e25\u003c/span\u003e,\n        min_score_thresh=\u003cspan class=\"hljs-number\"\u003e.40\u003c/span\u003e,\n        agnostic_mode=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e,\n        keypoints=keypoints,\n        keypoint_scores=keypoint_scores,\n        keypoint_edges=self.get_keypoint_tuples(self.full_config[\u003cspan class=\"hljs-string\"\u003e'eval_config'\u003c/span\u003e]))\n    \n    \u003cspan class=\"hljs-comment\"\u003e# 이미지 반환\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e image\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e이미지용 감지 함수\u003c/h2\u003e\n\u003cp\u003e이제 어떤 이미지 위의 객체를 감지하는 것이 쉽습니다. OpenCV를 사용하여 경로에서 이미지를 읽고, 일반적인 감지 정의를 호출한 후 결과를 출력 경로에 작성하는 것으로 간단하게 수행할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# 폴더에서 이미지 예측\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003edetect_image\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, path, output_path\u003c/span\u003e):\n\n    \u003cspan class=\"hljs-comment\"\u003e# 이미지 로드\u003c/span\u003e\n    image = cv2.imread(path)\n\n    \u003cspan class=\"hljs-comment\"\u003e# 객체 감지 수행 및 출력 파일에 추가\u003c/span\u003e\n    output_file = self.detect(image)\n    \n    \u003cspan class=\"hljs-comment\"\u003e# 출력 파일을 시스템에 작성\u003c/span\u003e\n    cv2.imwrite(output_path, output_file)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e비디오용 Detect 함수\u003c/h2\u003e\n\u003cp\u003e비디오에서 객체를 감지하는 것은 조금 더 어렵지만 여전히 매우 쉽습니다. 비디오는 종종 초당 25프레임의 이미지로 이루어진 이미지 집합에 불과하다는 것을 상기해주세요. 이 특성을 활용하여 비디오에서 객체 감지를 수행할 것입니다!\u003c/p\u003e\n\u003cp\u003e이 세그먼트는 다음 단계로 구성되어 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e먼저 출력 비디오 라이터와 코덱을 설정합니다. 이를 통해 바운딩 박스가 그려진 각 프레임을 출력 비디오에 작성할 수 있습니다. 이것은 사실상 비디오 프레임을 바운딩 박스와 함께 한 프레임씩 재구성하는 것을 의미합니다.\u003c/li\u003e\n\u003cli\u003e그런 다음 OpenCV의 VideoCapture 기능을 사용하여 경로에서 비디오를 읽습니다.\u003c/li\u003e\n\u003cli\u003evidcap.read()를 사용하여 첫 번째 프레임(이미지)을 읽고 성공적으로 읽었는지 표시합니다. 프레임 수를 0으로 설정합니다.\u003c/li\u003e\n\u003cli\u003e이제 프레임을 순환하며 감지를 수행하고(이것이 사실상 이미지에서의 감지임을 알아두세요!) 프레임을 출력 비디오에 작성합니다. 다음 프레임을 읽어 나가며, 더 이상 프레임을 읽을 수 없을 때(즉, frame_read != True가 될 때까지) 계속합니다.\u003c/li\u003e\n\u003cli\u003e모든 프레임을 처리한 후 출력 비디오를 out.release()를 사용하여 해제합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e  # 폴더로부터 비디오를 예측합니다\n  def \u003cspan class=\"hljs-title function_\"\u003edetect_video\u003c/span\u003e(self, path, output_path):\n    \n    # 코덱을 사용하여 출력 비디오 작성기를 설정합니다\n    fourcc = cv2.\u003cspan class=\"hljs-title class_\"\u003eVideoWriter\u003c/span\u003e_fourcc(*\u003cspan class=\"hljs-string\"\u003e'mp4v'\u003c/span\u003e)\n    out = cv2.\u003cspan class=\"hljs-title class_\"\u003eVideoWriter\u003c/span\u003e(output_path, fourcc, \u003cspan class=\"hljs-number\"\u003e25.0\u003c/span\u003e, (\u003cspan class=\"hljs-number\"\u003e1920\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1080\u003c/span\u003e))\n    \n    # 비디오를 읽어옵니다\n    vidcap = cv2.\u003cspan class=\"hljs-title class_\"\u003eVideoCapture\u003c/span\u003e(path)\n    frame_read, image = vidcap.\u003cspan class=\"hljs-title function_\"\u003eread\u003c/span\u003e()\n    count = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n    \n    # 각 프레임을 반복하면서 예측을 수행합니다\n    \u003cspan class=\"hljs-keyword\"\u003ewhile\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eframe_read\u003c/span\u003e:\n        \n      # 물체 감지를 수행하고 출력 파일에 추가합니다\n      output_file = self.\u003cspan class=\"hljs-title function_\"\u003edetect\u003c/span\u003e(image)\n      \n      # 예측과 함께 프레임을 비디오에 작성합니다\n      out.\u003cspan class=\"hljs-title function_\"\u003ewrite\u003c/span\u003e(output_file)\n      \n      # 다음 프레임 읽기\n      frame_read, image = vidcap.\u003cspan class=\"hljs-title function_\"\u003eread\u003c/span\u003e()\n      count += \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n        \n    # 비디오 파일을 릴리스합니다\n    out.\u003cspan class=\"hljs-title function_\"\u003erelease\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e섹션 3: ​​감지 호출 생성\u003c/h2\u003e\n\u003cp\u003e1부 및 2부에서 TFObjectDetector 클래스를 작성하며 감지기를 완성했습니다. 이제 완료 되었으므로 호출해 보는 시간이 됐어요. 다음 코드로 호출할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e __name__ == \u003cspan class=\"hljs-string\"\u003e'__main__'\u003c/span\u003e:\n  detector = \u003cspan class=\"hljs-title class_\"\u003eTFObjectDetector\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'../../tf-models/research/object_detection/configs/tf2'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'./checkpoint'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'./labels.pbtxt'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'\u003c/span\u003e)\n  detector.\u003cspan class=\"hljs-title function_\"\u003edetect_image\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'./shop.jpg'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'./shopout.jpg'\u003c/span\u003e)\n  detector.\u003cspan class=\"hljs-title function_\"\u003edetect_video\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'./video.mp4'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'./videooutput.mp4'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 코드는 다음을 수행합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e직접적으로 실행될 때, 즉 다른 클래스의 컨텍스트 내에서 실행되지 않을 때, 먼저 TFObjectDetector의 새 인스턴스를 생성합니다. 여기에서는 다음 정보를 전달합니다:\u003c/li\u003e\n\u003cli\u003e클론된 TensorFlow 모델의 tf2 구성 폴더로의 절대 또는 상대 경로입니다.\u003c/li\u003e\n\u003cli\u003e다운로드한 모델의 모델 체크포인트 폴더로의 절대 또는 상대 경로입니다. 사용하는 SSD MobileNet의 경우, 폴더를 해제하고 열면 ./checkpoint 폴더가 나타납니다. 거기를 참조하세요.\u003c/li\u003e\n\u003cli\u003e클래스 인덱스와 레이블 이름 간의 매핑에 사용되는 레이블 파일의 절대 또는 상대 경로입니다. 없는 경우 TensorFlow Detection Model Zoo 모델 중 하나에 대해 여기에서 다운로드할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e모델의 이름입니다. 우리 경우에는 지정한 어려운 이름입니다. Model Zoo에서 다른 이름들 중 하나를 사용할 수도 있지만 그에 맞는 체크포인트를 사용해야 합니다.\u003c/li\u003e\n\u003cli\u003e./shop.jpg라는 이미지에서 이미지 검출을 수행하고 결과물(즉, 바운딩 상자가 오버레이된 이미지)을 ./shopout.jpg에 저장합니다.\u003c/li\u003e\n\u003cli\u003e./video.mp4라는 비디오에서 비디오 검출을 수행하고 출력물을 ./videooutput.mp4로 저장합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e전체 모델 코드\u003c/h2\u003e\n\u003cp\u003e바로 코드로 이동하려는 사용자를 위해 전체 모델 코드는 내 Github 저장소에서 찾을 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e객체 탐지기 실행하기\u003c/h2\u003e\n\u003cp\u003e이제 객체 탐지기를 실행한 결과를 살펴보겠습니다.\u003c/p\u003e\n\u003cp\u003e이 사진들과 동영상은 Pexels 라이선스 하에 다운로드되어 사용되었습니다.\u003c/p\u003e\n\u003ch2\u003e사진에서\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_4.png\" alt=\"Image 1\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_5.png\" alt=\"Image 2\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV_6.png\" alt=\"Image 3\"\u003e\u003c/p\u003e\n\u003ch2\u003eOn videos\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래는 Markdown 형식으로 표시한 이미지 링크입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1200/1*alxO3bGUDN3dzJnQ6wIoZQ.gif\" alt=\"이미지1\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1200/1*hTy8jS8LYMGNPgJ6Q_jj2w.gif\" alt=\"이미지2\"\u003e\u003c/p\u003e\n\u003ch2\u003e요약\u003c/h2\u003e\n\u003cp\u003e머신 러닝에서 객체 감지에는 많은 유용한 사례가 있습니다. 이 튜토리얼을 통해 TensorFlow 객체 감지 API와 사전 훈련된 모델을 사용하여 이미지와 비디오에서 객체 감지를 수행하는 방법을 배웠습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e오늘의 글에서 무언가를 배워가셨나요? 궁금한 점, 의견 또는 제안이 있으면 언제든지 환영합니다. 읽어 주셔서 감사합니다!\u003c/p\u003e\n\u003ch2\u003e참고 자료\u003c/h2\u003e\n\u003cp\u003eTensorFlow, TensorFlow 로고 및 관련 상표는 Google Inc.의 상표입니다.\u003c/p\u003e\n\u003cp\u003eTensorFlow. (2020, 9월 9일). TensorFlow/models. GitHub. \u003ca href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eTensorFlow. (2020, 11). TensorFlow/models. GitHub. \u003ca href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTensorFlow. (n.d.). TensorFlow/models. GitHub. \u003ca href=\"https://github.com/tensorflow/models/tree/master/research/object_detection\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/tensorflow/models/tree/master/research/object_detection\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\"현대 합성곱 객체 탐지기의 속도/정확도 균형.\" 황재식, 라토드 비니트, 썬 첸, 주 만멍, 코라티카라 아니쉬, 파티 아드리아노, 피셔 이안, 우예노비치 세르게이, 송 양초, 과다라마 세르게이, 머피 케빈, CVPR 2017\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-20-ObjectDetectionforImagesandVideoswithTensorFlowandOpenCV"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>