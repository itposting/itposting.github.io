<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>Llama-3 추론을 Intel Core Ultra 5에서 실행하기 DirectML 및 ONNX 대 IPEX-LLM | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="Llama-3 추론을 Intel Core Ultra 5에서 실행하기 DirectML 및 ONNX 대 IPEX-LLM | itposting" data-gatsby-head="true"/><meta property="og:title" content="Llama-3 추론을 Intel Core Ultra 5에서 실행하기 DirectML 및 ONNX 대 IPEX-LLM | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM" data-gatsby-head="true"/><meta name="twitter:title" content="Llama-3 추론을 Intel Core Ultra 5에서 실행하기 DirectML 및 ONNX 대 IPEX-LLM | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 01:21" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">Llama-3 추론을 Intel Core Ultra 5에서 실행하기 DirectML 및 ONNX 대 IPEX-LLM</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="Llama-3 추론을 Intel Core Ultra 5에서 실행하기 DirectML 및 ONNX 대 IPEX-LLM" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">10<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>이전 글에서 언급했듯이 Intel은 ONNX + DirectML을 위한 하드웨어 가속화를 제공합니다. 이에 대해 몇 가지 실험을 진행했습니다.</p>
<p>Microsoft은 PyTorch를 위한 DirectML 인터페이스를 제공합니다. 현재 16비트와 32비트 부동 소수점에서만 추론을 지원합니다. 예제에서 초당 토큰 수를 측정하기 위해 포크를 생성했습니다.</p>
<pre><code class="hljs language-js">conda create --name pytdml python=<span class="hljs-number">3.10</span> -y
conda activate pytdml
pip install torch-directml
git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/luweigen/DirectML</span>
cd <span class="hljs-title class_">DirectML</span>/<span class="hljs-title class_">PyTorch</span>/llm
pip install -r requirements.<span class="hljs-property">txt</span>
python app.<span class="hljs-property">py</span> --precision float16 --model_repo <span class="hljs-string">"meta-llama/Meta-Llama-3-8B-Instruct"</span> --stream_every_n=<span class="hljs-number">143</span>
</code></pre>
<img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png">
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_1.png">
<p>이전 실험에서 🤗Transformers + IPEX-LLM이 최상의 성능을 보여줬기 때문에 이 설정에서는 16비트 부동 소수점 추론만 비교할 것입니다.</p>
<p>테스트 프로그램 ipex-llm-llama3.py는 다음과 같이 수정되었습니다.</p>
<pre><code class="hljs language-js"># <span class="hljs-title class_">Wei</span> <span class="hljs-title class_">Lu</span>(mailwlu@gmail.<span class="hljs-property">com</span>)에 의해 수정됨
# <span class="hljs-number">2016</span>년 <span class="hljs-title class_">The</span> <span class="hljs-title class_">BigDL</span> <span class="hljs-title class_">Authors</span>에 저작권 속함
#
# <span class="hljs-title class_">Apache</span> 라이선스, 버전 <span class="hljs-number">2.0</span>에 따라 라이선스 부여
# 이 파일을 라이선스와 준수하면서 사용해야 합니다.
# 라이선스 사본은 다음에서 얻을 수 있습니다.
#
#     <span class="hljs-attr">http</span>:<span class="hljs-comment">//www.apache.org/licenses/LICENSE-2.0</span>
#
# 적용되는 법률에 따라 필요하거나 서면으로 합의되거나, 소프트웨어가
# <span class="hljs-string">"있는 그대로"</span> 배포됩니다. 조건이나 보증 없이
# 명시 또는 묵시적으로, 까지도 어떤 종류의 조건도 보증 없이,
# 명시적 또는 묵시적으로. 언어 특정 권한과 관련해야 합니다.
# 권한 및 제한 사항
#

<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> argparse

<span class="hljs-keyword">from</span> ipex_llm.<span class="hljs-property">transformers</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">AutoModelForCausalLM</span>
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> <span class="hljs-title class_">AutoTokenizer</span>

# 모델을 기반으로 프롬프트를 조정할 수 있습니다.
# 여기서 프롬프트 조정은 다음을 참조합니다. <span class="hljs-attr">https</span>:<span class="hljs-comment">//llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3</span>
기본_시스템_프롬프트 = <span class="hljs-string">""</span><span class="hljs-string">"\
"</span><span class="hljs-string">""</span>

def <span class="hljs-title function_">get_prompt</span>(<span class="hljs-attr">user_input</span>: str, <span class="hljs-attr">chat_history</span>: list[tuple[str, str]],
               <span class="hljs-attr">system_prompt</span>: str) -> <span class="hljs-attr">str</span>:
    prompt_texts = [f<span class="hljs-string">'&#x3C;|begin_of_text|>'</span>]

    <span class="hljs-keyword">if</span> system_prompt != <span class="hljs-string">''</span>:
        prompt_texts.<span class="hljs-title function_">append</span>(f<span class="hljs-string">'&#x3C;|start_header_id|>system&#x3C;|end_header_id|>\n\n{system_prompt}&#x3C;|eot_id|>'</span>)

    <span class="hljs-keyword">for</span> history_input, history_response <span class="hljs-keyword">in</span> <span class="hljs-attr">chat_history</span>:
        prompt_texts.<span class="hljs-title function_">append</span>(f<span class="hljs-string">'&#x3C;|start_header_id|>user&#x3C;|end_header_id|>\n\n{history_input.strip()}&#x3C;|eot_id|>'</span>)
        prompt_texts.<span class="hljs-title function_">append</span>(f<span class="hljs-string">'&#x3C;|start_header_id|>assistant&#x3C;|end_header_id|>\n\n{history_response.strip()}&#x3C;|eot_id|>'</span>)

    prompt_texts.<span class="hljs-title function_">append</span>(f<span class="hljs-string">'&#x3C;|start_header_id|>user&#x3C;|end_header_id|>\n\n{user_input.strip()}&#x3C;|eot_id|>&#x3C;|start_header_id|>assistant&#x3C;|end_header_id|>\n\n'</span>)
    <span class="hljs-keyword">return</span> <span class="hljs-string">''</span>.<span class="hljs-title function_">join</span>(prompt_texts)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    parser = argparse.<span class="hljs-title class_">ArgumentParser</span>(description=<span class="hljs-string">'Llama3 모델을 사용하여 `generate()` API를 사용하여 토큰 예측'</span>)
    parser.<span class="hljs-title function_">add_argument</span>(<span class="hljs-string">'--repo-id-or-model-path'</span>, type=str, <span class="hljs-keyword">default</span>=<span class="hljs-string">"meta-llama/Meta-Llama-3-70B-Instruct"</span>,
                        help=<span class="hljs-string">'Meta-Llama-3 (예: `meta-llama/Meta-Llama-3-70B-Instruct`)를 다운로드할 Huggingface 저장소 ID'</span>
                             <span class="hljs-string">'또는 Huggingface 체크포인트 폴더에 대한 경로'</span>)
    parser.<span class="hljs-title function_">add_argument</span>(<span class="hljs-string">'--prompt'</span>, type=str, <span class="hljs-keyword">default</span>=<span class="hljs-string">"OpenVINO is"</span>,
                        help=<span class="hljs-string">'추론할 프롬프트'</span>)
    parser.<span class="hljs-title function_">add_argument</span>(<span class="hljs-string">'--n-predict'</span>, type=int, <span class="hljs-keyword">default</span>=<span class="hljs-number">128</span>,
                        help=<span class="hljs-string">'예측할 최대 토큰 수'</span>)
    parser.<span class="hljs-title function_">add_argument</span>(<span class="hljs-string">'--bit'</span>, type=str, <span class="hljs-keyword">default</span>=<span class="hljs-string">"4"</span>,
                        help=<span class="hljs-string">'4로 설정하면 4비트로 로드하거나 off 또는 load_in_low_bit 옵션은 sym_int4, asym_int4, sym_int5, asym_int5, sym_int8,nf3,nf4, fp4, fp8, fp8_e4m3, fp8_e5m2, fp6, gguf_iq2_xxs, gguf_iq2_xs, gguf_iq1_s, gguf_q4k_m, gguf_q4k_s, fp16, bf16, fp6_k'</span>)

    args = parser.<span class="hljs-title function_">parse_args</span>()
    model_path = args.<span class="hljs-property">repo_id_or_model_path</span>

    <span class="hljs-keyword">if</span> args.<span class="hljs-property">bit</span> == <span class="hljs-string">"4"</span>:
        # <span class="hljs-number">4</span>비트로 모델 로드,
        # 모델의 관련 레이어를 <span class="hljs-title class_">INT4</span> 형식으로 변환합니다.
        # iGPU를 사용하는 <span class="hljs-title class_">Windows</span> 사용자의 경우, <span class="hljs-variable constant_">LLM</span>을 실행할 때 <span class="hljs-string">`cpu_embedding=True`</span>를 from_pretrained 함수에서 설정하는 것을 권장합니다.
        # 이렇게 하면 메모리 집약적인 임베딩 레이어가 iGPU 대신 <span class="hljs-variable constant_">CPU</span>를 활용할 수 있습니다. 이 경우에는 도움이 되지 않습니다.
        model = <span class="hljs-title class_">AutoModelForCausalLM</span>.<span class="hljs-title function_">from_pretrained</span>(model_path,
                                                    load_in_4bit=<span class="hljs-title class_">True</span>,
                                                    optimize_model=<span class="hljs-title class_">True</span>,
                                                    trust_remote_code=<span class="hljs-title class_">True</span>,
                                                    use_cache=<span class="hljs-title class_">True</span>)
    elif args.<span class="hljs-property">bit</span> == <span class="hljs-string">"off"</span> or args.<span class="hljs-property">bit</span> == <span class="hljs-string">"32"</span>:
        model = <span class="hljs-title class_">AutoModelForCausalLM</span>.<span class="hljs-title function_">from_pretrained</span>(model_path,
                                                    optimize_model=<span class="hljs-title class_">True</span>,
                                                    trust_remote_code=<span class="hljs-title class_">True</span>,
                                                    use_cache=<span class="hljs-title class_">True</span>)
    <span class="hljs-attr">else</span>:
        model = <span class="hljs-title class_">AutoModelForCausalLM</span>.<span class="hljs-title function_">from_pretrained</span>(model_path,
                                                    load_in_low_bit=args.<span class="hljs-property">bit</span>,
                                                    optimize_model=<span class="hljs-title class_">True</span>,
                                                    trust_remote_code=<span class="hljs-title class_">True</span>,
                                                    use_cache=<span class="hljs-title class_">True</span>)

    <span class="hljs-keyword">if</span> args.<span class="hljs-property">bit</span> == <span class="hljs-string">"32"</span>:
        model = model.<span class="hljs-title function_">to</span>(<span class="hljs-string">'xpu'</span>)
    <span class="hljs-attr">else</span>:
        model = model.<span class="hljs-title function_">half</span>().<span class="hljs-title function_">to</span>(<span class="hljs-string">'xpu'</span>)

    # 토크나이저 로드
    tokenizer = <span class="hljs-title class_">AutoTokenizer</span>.<span class="hljs-title function_">from_pretrained</span>(model_path, trust_remote_code=<span class="hljs-title class_">True</span>)

    # 여기서 종결자는 다음을 참조합니다. <span class="hljs-attr">https</span>:<span class="hljs-comment">//huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct#transformers-automodelforcausallm</span>
    종결자 = [
        tokenizer.<span class="hljs-property">eos_token_id</span>,
        tokenizer.<span class="hljs-title function_">convert_tokens_to_ids</span>(<span class="hljs-string">"&#x3C;|eot_id|>"</span>),
    ]

    # 예측된 토큰 생성
    <span class="hljs-keyword">with</span> torch.<span class="hljs-title function_">inference_mode</span>():
        prompt = <span class="hljs-title function_">get_prompt</span>(args.<span class="hljs-property">prompt</span>, [], system_prompt=<span class="hljs-variable constant_">DEFAULT_SYSTEM_PROMPT</span>)
        input_ids = tokenizer.<span class="hljs-title function_">encode</span>(prompt, return_tensors=<span class="hljs-string">"pt"</span>).<span class="hljs-title function_">to</span>(<span class="hljs-string">'xpu'</span>)
        # ipex_llm 모델은 워밍업이 필요하므로 추론 시간을 정확하게 할 수 있습니다.
        output = model.<span class="hljs-title function_">generate</span>(input_ids,
                                eos_token_id=terminators,
                                max_new_tokens=<span class="hljs-number">20</span>)

        # 추론 시작
        st = time.<span class="hljs-title function_">time</span>()
        output = model.<span class="hljs-title function_">generate</span>(input_ids,
                                eos_token_id=terminators,
                                max_new_tokens=args.<span class="hljs-property">n_predict</span>)
        torch.<span class="hljs-property">xpu</span>.<span class="hljs-title function_">synchronize</span>()
        end = time.<span class="hljs-title function_">time</span>()
        output = output.<span class="hljs-title function_">cpu</span>()
        output_str = tokenizer.<span class="hljs-title function_">decode</span>(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-title class_">False</span>)
        <span class="hljs-title function_">print</span>(f<span class="hljs-string">'추론 시간: {end-st} s, 토큰: {len(output[0])}, t/s:{len(output[0])/(end-st)}'</span>)
        <span class="hljs-title function_">print</span>(<span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>, <span class="hljs-string">'프롬프트'</span>, <span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>)
        <span class="hljs-title function_">print</span>(prompt)
        <span class="hljs-title function_">print</span>(<span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>, <span class="hljs-string">'출력 (skip_special_tokens=False)'</span>, <span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>)
        <span class="hljs-title function_">print</span>(output_str)
</code></pre>
<div class="content-ad"></div>
<pre><code class="hljs language-js">python ipex-llm-llama3.<span class="hljs-property">py</span> --repo-id-or-model-path=meta-llama/<span class="hljs-title class_">Meta</span>-<span class="hljs-title class_">Llama</span>-<span class="hljs-number">3</span>-8B-<span class="hljs-title class_">Instruct</span> --bit=fp16
</code></pre>
<p><img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_2.png" alt="image"></p>
<p><img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_3.png" alt="image"></p>
<p>DirectML은 낮은 비트 양자화를 지원하는 ONNX Runtime의 Execution Provider가 될 수도 있습니다. 우리의 테스트는 이 python API를 기반으로 합니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">pip install onnxruntime-genai --pre
pip install onnxruntime-genai-directml --pre
pip install torch transformers onnx onnxruntime
conda install conda-<span class="hljs-attr">forge</span>::vs2015_runtime
</code></pre>
<p>마지막 줄은 이 문제를 해결하기 위한 것입니다.</p>
<p>라마 모델은 다음과 같이 변환되었습니다.</p>
<pre><code class="hljs language-js">python -m onnxruntime_genai.<span class="hljs-property">models</span>.<span class="hljs-property">builder</span> -m meta-llama/<span class="hljs-title class_">Meta</span>-<span class="hljs-title class_">Llama</span>-<span class="hljs-number">3</span>-8B-<span class="hljs-title class_">Instruct</span> -o llama-<span class="hljs-number">3</span>-8B-<span class="hljs-title class_">Instruct</span>-int4-onnx-directml -p int4 -e dml -c ..\.<span class="hljs-property">cache</span>\huggingface\hub\
</code></pre>
<div class="content-ad"></div>
<p>변환된 모델이 🤗 허브에 업로드되었습니다.</p>
<p>다음은 테스트 프로그램 genai-llama3.py입니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> argparse
<span class="hljs-keyword">import</span> onnxruntime_genai <span class="hljs-keyword">as</span> og

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    parser = argparse.ArgumentParser(description=<span class="hljs-string">'Predict Tokens using onnxruntime_genai'</span>)
    parser.add_argument(<span class="hljs-string">'--model-path'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">"llama-3-8B-Instruct-int4-onnx-directml"</span>,
                        <span class="hljs-built_in">help</span>=<span class="hljs-string">'model path'</span>)
    parser.add_argument(<span class="hljs-string">'--prompt'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">"OpenVINO is"</span>,
                        <span class="hljs-built_in">help</span>=<span class="hljs-string">'Prompt to infer'</span>)
    parser.add_argument(
        <span class="hljs-string">'--max-length'</span>,
        <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>,
        default=<span class="hljs-number">143</span>,
        <span class="hljs-built_in">help</span>=<span class="hljs-string">'max lengths'</span>
    )
    args = parser.parse_args()

    model = og.Model(args.model_path)
    tokenizer = og.Tokenizer(model)
    
    <span class="hljs-comment"># Set the max length to something sensible by default,</span>
    <span class="hljs-comment"># since otherwise it will be set to the entire context length</span>
    search_options = {}
    search_options[<span class="hljs-string">'max_length'</span>] = args.max_length

    chat_template = <span class="hljs-string">'&#x3C;|user|>\n{input} &#x3C;|end|>\n&#x3C;|assistant|>'</span>

    text = args.prompt
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> text:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"오류, 입력이 비어 있을 수 없습니다"</span>)
        exit

    prompt = <span class="hljs-string">f'<span class="hljs-subst">{chat_template.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">input</span>=text)}</span>'</span>

    input_tokens = tokenizer.encode(prompt)

    params = og.GeneratorParams(model)
    params.set_search_options(**search_options)
    params.input_ids = input_tokens

    st =  time.time()
    output = model.generate(params)
    out_txt = tokenizer.decode(output[<span class="hljs-number">0</span>])

    sec = time.time() - st
    cnt = <span class="hljs-built_in">len</span>(output[<span class="hljs-number">0</span>])

    <span class="hljs-built_in">print</span>(<span class="hljs-string">"생성 결과:"</span>, out_txt)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'추론 시간: <span class="hljs-subst">{sec}</span> 초, 토큰 수: <span class="hljs-subst">{cnt}</span>, 초당 토큰 수:<span class="hljs-subst">{cnt/sec}</span>'</span>)
</code></pre>
<img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_4.png">
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_5.png" alt="이미지"></p>
<p>🤗Transformers + IPEX-LLM과 비교해 볼 수 있어요.</p>
<pre><code class="hljs language-js">python ipex-llm-llama3.<span class="hljs-property">py</span> --repo-id-or-model-path=meta-llama/<span class="hljs-title class_">Meta</span>-<span class="hljs-title class_">Llama</span>-<span class="hljs-number">3</span>-8B-<span class="hljs-title class_">Instruct</span> --bit=<span class="hljs-number">4</span>
</code></pre>
<p><img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_6.png" alt="이미지"></p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_7.png" alt="이미지1"></p>
<p>결과를 함께 살펴봅시다.</p>
<p><img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_8.png" alt="이미지2"></p>
<p>앞으로 어떻게 이러한 흥미로운 차이가 생겼는지에 대한 구현 세부사항을 탐구할 것입니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Llama-3 추론을 Intel Core Ultra 5에서 실행하기 DirectML 및 ONNX 대 IPEX-LLM","description":"","date":"2024-06-19 01:21","slug":"2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM","content":"\n\n이전 글에서 언급했듯이 Intel은 ONNX + DirectML을 위한 하드웨어 가속화를 제공합니다. 이에 대해 몇 가지 실험을 진행했습니다.\n\nMicrosoft은 PyTorch를 위한 DirectML 인터페이스를 제공합니다. 현재 16비트와 32비트 부동 소수점에서만 추론을 지원합니다. 예제에서 초당 토큰 수를 측정하기 위해 포크를 생성했습니다.\n\n```js\nconda create --name pytdml python=3.10 -y\nconda activate pytdml\npip install torch-directml\ngit clone https://github.com/luweigen/DirectML\ncd DirectML/PyTorch/llm\npip install -r requirements.txt\npython app.py --precision float16 --model_repo \"meta-llama/Meta-Llama-3-8B-Instruct\" --stream_every_n=143\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_1.png\" /\u003e\n\n이전 실험에서 🤗Transformers + IPEX-LLM이 최상의 성능을 보여줬기 때문에 이 설정에서는 16비트 부동 소수점 추론만 비교할 것입니다.\n\n테스트 프로그램 ipex-llm-llama3.py는 다음과 같이 수정되었습니다.\n\n```js\n# Wei Lu(mailwlu@gmail.com)에 의해 수정됨\n# 2016년 The BigDL Authors에 저작권 속함\n#\n# Apache 라이선스, 버전 2.0에 따라 라이선스 부여\n# 이 파일을 라이선스와 준수하면서 사용해야 합니다.\n# 라이선스 사본은 다음에서 얻을 수 있습니다.\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# 적용되는 법률에 따라 필요하거나 서면으로 합의되거나, 소프트웨어가\n# \"있는 그대로\" 배포됩니다. 조건이나 보증 없이\n# 명시 또는 묵시적으로, 까지도 어떤 종류의 조건도 보증 없이,\n# 명시적 또는 묵시적으로. 언어 특정 권한과 관련해야 합니다.\n# 권한 및 제한 사항\n#\n\nimport torch\nimport time\nimport argparse\n\nfrom ipex_llm.transformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer\n\n# 모델을 기반으로 프롬프트를 조정할 수 있습니다.\n# 여기서 프롬프트 조정은 다음을 참조합니다. https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3\n기본_시스템_프롬프트 = \"\"\"\\\n\"\"\"\n\ndef get_prompt(user_input: str, chat_history: list[tuple[str, str]],\n               system_prompt: str) -\u003e str:\n    prompt_texts = [f'\u003c|begin_of_text|\u003e']\n\n    if system_prompt != '':\n        prompt_texts.append(f'\u003c|start_header_id|\u003esystem\u003c|end_header_id|\u003e\\n\\n{system_prompt}\u003c|eot_id|\u003e')\n\n    for history_input, history_response in chat_history:\n        prompt_texts.append(f'\u003c|start_header_id|\u003euser\u003c|end_header_id|\u003e\\n\\n{history_input.strip()}\u003c|eot_id|\u003e')\n        prompt_texts.append(f'\u003c|start_header_id|\u003eassistant\u003c|end_header_id|\u003e\\n\\n{history_response.strip()}\u003c|eot_id|\u003e')\n\n    prompt_texts.append(f'\u003c|start_header_id|\u003euser\u003c|end_header_id|\u003e\\n\\n{user_input.strip()}\u003c|eot_id|\u003e\u003c|start_header_id|\u003eassistant\u003c|end_header_id|\u003e\\n\\n')\n    return ''.join(prompt_texts)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Llama3 모델을 사용하여 `generate()` API를 사용하여 토큰 예측')\n    parser.add_argument('--repo-id-or-model-path', type=str, default=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                        help='Meta-Llama-3 (예: `meta-llama/Meta-Llama-3-70B-Instruct`)를 다운로드할 Huggingface 저장소 ID'\n                             '또는 Huggingface 체크포인트 폴더에 대한 경로')\n    parser.add_argument('--prompt', type=str, default=\"OpenVINO is\",\n                        help='추론할 프롬프트')\n    parser.add_argument('--n-predict', type=int, default=128,\n                        help='예측할 최대 토큰 수')\n    parser.add_argument('--bit', type=str, default=\"4\",\n                        help='4로 설정하면 4비트로 로드하거나 off 또는 load_in_low_bit 옵션은 sym_int4, asym_int4, sym_int5, asym_int5, sym_int8,nf3,nf4, fp4, fp8, fp8_e4m3, fp8_e5m2, fp6, gguf_iq2_xxs, gguf_iq2_xs, gguf_iq1_s, gguf_q4k_m, gguf_q4k_s, fp16, bf16, fp6_k')\n\n    args = parser.parse_args()\n    model_path = args.repo_id_or_model_path\n\n    if args.bit == \"4\":\n        # 4비트로 모델 로드,\n        # 모델의 관련 레이어를 INT4 형식으로 변환합니다.\n        # iGPU를 사용하는 Windows 사용자의 경우, LLM을 실행할 때 `cpu_embedding=True`를 from_pretrained 함수에서 설정하는 것을 권장합니다.\n        # 이렇게 하면 메모리 집약적인 임베딩 레이어가 iGPU 대신 CPU를 활용할 수 있습니다. 이 경우에는 도움이 되지 않습니다.\n        model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                    load_in_4bit=True,\n                                                    optimize_model=True,\n                                                    trust_remote_code=True,\n                                                    use_cache=True)\n    elif args.bit == \"off\" or args.bit == \"32\":\n        model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                    optimize_model=True,\n                                                    trust_remote_code=True,\n                                                    use_cache=True)\n    else:\n        model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                    load_in_low_bit=args.bit,\n                                                    optimize_model=True,\n                                                    trust_remote_code=True,\n                                                    use_cache=True)\n\n    if args.bit == \"32\":\n        model = model.to('xpu')\n    else:\n        model = model.half().to('xpu')\n\n    # 토크나이저 로드\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n    # 여기서 종결자는 다음을 참조합니다. https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct#transformers-automodelforcausallm\n    종결자 = [\n        tokenizer.eos_token_id,\n        tokenizer.convert_tokens_to_ids(\"\u003c|eot_id|\u003e\"),\n    ]\n\n    # 예측된 토큰 생성\n    with torch.inference_mode():\n        prompt = get_prompt(args.prompt, [], system_prompt=DEFAULT_SYSTEM_PROMPT)\n        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('xpu')\n        # ipex_llm 모델은 워밍업이 필요하므로 추론 시간을 정확하게 할 수 있습니다.\n        output = model.generate(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=20)\n\n        # 추론 시작\n        st = time.time()\n        output = model.generate(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=args.n_predict)\n        torch.xpu.synchronize()\n        end = time.time()\n        output = output.cpu()\n        output_str = tokenizer.decode(output[0], skip_special_tokens=False)\n        print(f'추론 시간: {end-st} s, 토큰: {len(output[0])}, t/s:{len(output[0])/(end-st)}')\n        print('-'*20, '프롬프트', '-'*20)\n        print(prompt)\n        print('-'*20, '출력 (skip_special_tokens=False)', '-'*20)\n        print(output_str)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\npython ipex-llm-llama3.py --repo-id-or-model-path=meta-llama/Meta-Llama-3-8B-Instruct --bit=fp16\n```\n\n![image](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_2.png)\n\n![image](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_3.png)\n\nDirectML은 낮은 비트 양자화를 지원하는 ONNX Runtime의 Execution Provider가 될 수도 있습니다. 우리의 테스트는 이 python API를 기반으로 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\npip install onnxruntime-genai --pre\npip install onnxruntime-genai-directml --pre\npip install torch transformers onnx onnxruntime\nconda install conda-forge::vs2015_runtime\n```\n\n마지막 줄은 이 문제를 해결하기 위한 것입니다.\n\n라마 모델은 다음과 같이 변환되었습니다.\n\n```js\npython -m onnxruntime_genai.models.builder -m meta-llama/Meta-Llama-3-8B-Instruct -o llama-3-8B-Instruct-int4-onnx-directml -p int4 -e dml -c ..\\.cache\\huggingface\\hub\\\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n변환된 모델이 🤗 허브에 업로드되었습니다.\n\n다음은 테스트 프로그램 genai-llama3.py입니다.\n\n```python\nimport time\nimport argparse\nimport onnxruntime_genai as og\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Predict Tokens using onnxruntime_genai')\n    parser.add_argument('--model-path', type=str, default=\"llama-3-8B-Instruct-int4-onnx-directml\",\n                        help='model path')\n    parser.add_argument('--prompt', type=str, default=\"OpenVINO is\",\n                        help='Prompt to infer')\n    parser.add_argument(\n        '--max-length',\n        type=int,\n        default=143,\n        help='max lengths'\n    )\n    args = parser.parse_args()\n\n    model = og.Model(args.model_path)\n    tokenizer = og.Tokenizer(model)\n    \n    # Set the max length to something sensible by default,\n    # since otherwise it will be set to the entire context length\n    search_options = {}\n    search_options['max_length'] = args.max_length\n\n    chat_template = '\u003c|user|\u003e\\n{input} \u003c|end|\u003e\\n\u003c|assistant|\u003e'\n\n    text = args.prompt\n    if not text:\n        print(\"오류, 입력이 비어 있을 수 없습니다\")\n        exit\n\n    prompt = f'{chat_template.format(input=text)}'\n\n    input_tokens = tokenizer.encode(prompt)\n\n    params = og.GeneratorParams(model)\n    params.set_search_options(**search_options)\n    params.input_ids = input_tokens\n\n    st =  time.time()\n    output = model.generate(params)\n    out_txt = tokenizer.decode(output[0])\n\n    sec = time.time() - st\n    cnt = len(output[0])\n\n    print(\"생성 결과:\", out_txt)\n    print(f'추론 시간: {sec} 초, 토큰 수: {cnt}, 초당 토큰 수:{cnt/sec}')\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_4.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_5.png)\n\n🤗Transformers + IPEX-LLM과 비교해 볼 수 있어요.\n\n```js\npython ipex-llm-llama3.py --repo-id-or-model-path=meta-llama/Meta-Llama-3-8B-Instruct --bit=4\n```\n\n![이미지](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_6.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지1](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_7.png)\n\n결과를 함께 살펴봅시다.\n\n![이미지2](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_8.png)\n\n앞으로 어떻게 이러한 흥미로운 차이가 생겼는지에 대한 구현 세부사항을 탐구할 것입니다.\n","ogImage":{"url":"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png"},"coverImage":"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png","tag":["Tech"],"readingTime":10},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e이전 글에서 언급했듯이 Intel은 ONNX + DirectML을 위한 하드웨어 가속화를 제공합니다. 이에 대해 몇 가지 실험을 진행했습니다.\u003c/p\u003e\n\u003cp\u003eMicrosoft은 PyTorch를 위한 DirectML 인터페이스를 제공합니다. 현재 16비트와 32비트 부동 소수점에서만 추론을 지원합니다. 예제에서 초당 토큰 수를 측정하기 위해 포크를 생성했습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003econda create --name pytdml python=\u003cspan class=\"hljs-number\"\u003e3.10\u003c/span\u003e -y\nconda activate pytdml\npip install torch-directml\ngit clone \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/luweigen/DirectML\u003c/span\u003e\ncd \u003cspan class=\"hljs-title class_\"\u003eDirectML\u003c/span\u003e/\u003cspan class=\"hljs-title class_\"\u003ePyTorch\u003c/span\u003e/llm\npip install -r requirements.\u003cspan class=\"hljs-property\"\u003etxt\u003c/span\u003e\npython app.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e --precision float16 --model_repo \u003cspan class=\"hljs-string\"\u003e\"meta-llama/Meta-Llama-3-8B-Instruct\"\u003c/span\u003e --stream_every_n=\u003cspan class=\"hljs-number\"\u003e143\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_1.png\"\u003e\n\u003cp\u003e이전 실험에서 🤗Transformers + IPEX-LLM이 최상의 성능을 보여줬기 때문에 이 설정에서는 16비트 부동 소수점 추론만 비교할 것입니다.\u003c/p\u003e\n\u003cp\u003e테스트 프로그램 ipex-llm-llama3.py는 다음과 같이 수정되었습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# \u003cspan class=\"hljs-title class_\"\u003eWei\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eLu\u003c/span\u003e(mailwlu@gmail.\u003cspan class=\"hljs-property\"\u003ecom\u003c/span\u003e)에 의해 수정됨\n# \u003cspan class=\"hljs-number\"\u003e2016\u003c/span\u003e년 \u003cspan class=\"hljs-title class_\"\u003eThe\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eBigDL\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAuthors\u003c/span\u003e에 저작권 속함\n#\n# \u003cspan class=\"hljs-title class_\"\u003eApache\u003c/span\u003e 라이선스, 버전 \u003cspan class=\"hljs-number\"\u003e2.0\u003c/span\u003e에 따라 라이선스 부여\n# 이 파일을 라이선스와 준수하면서 사용해야 합니다.\n# 라이선스 사본은 다음에서 얻을 수 있습니다.\n#\n#     \u003cspan class=\"hljs-attr\"\u003ehttp\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//www.apache.org/licenses/LICENSE-2.0\u003c/span\u003e\n#\n# 적용되는 법률에 따라 필요하거나 서면으로 합의되거나, 소프트웨어가\n# \u003cspan class=\"hljs-string\"\u003e\"있는 그대로\"\u003c/span\u003e 배포됩니다. 조건이나 보증 없이\n# 명시 또는 묵시적으로, 까지도 어떤 종류의 조건도 보증 없이,\n# 명시적 또는 묵시적으로. 언어 특정 권한과 관련해야 합니다.\n# 권한 및 제한 사항\n#\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e time\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e argparse\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e ipex_llm.\u003cspan class=\"hljs-property\"\u003etransformers\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAutoModelForCausalLM\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAutoTokenizer\u003c/span\u003e\n\n# 모델을 기반으로 프롬프트를 조정할 수 있습니다.\n# 여기서 프롬프트 조정은 다음을 참조합니다. \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3\u003c/span\u003e\n기본_시스템_프롬프트 = \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\\\n\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\ndef \u003cspan class=\"hljs-title function_\"\u003eget_prompt\u003c/span\u003e(\u003cspan class=\"hljs-attr\"\u003euser_input\u003c/span\u003e: str, \u003cspan class=\"hljs-attr\"\u003echat_history\u003c/span\u003e: list[tuple[str, str]],\n               \u003cspan class=\"hljs-attr\"\u003esystem_prompt\u003c/span\u003e: str) -\u003e \u003cspan class=\"hljs-attr\"\u003estr\u003c/span\u003e:\n    prompt_texts = [f\u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;|begin_of_text|\u003e'\u003c/span\u003e]\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e system_prompt != \u003cspan class=\"hljs-string\"\u003e''\u003c/span\u003e:\n        prompt_texts.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;|start_header_id|\u003esystem\u0026#x3C;|end_header_id|\u003e\\n\\n{system_prompt}\u0026#x3C;|eot_id|\u003e'\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e history_input, history_response \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003echat_history\u003c/span\u003e:\n        prompt_texts.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;|start_header_id|\u003euser\u0026#x3C;|end_header_id|\u003e\\n\\n{history_input.strip()}\u0026#x3C;|eot_id|\u003e'\u003c/span\u003e)\n        prompt_texts.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;|start_header_id|\u003eassistant\u0026#x3C;|end_header_id|\u003e\\n\\n{history_response.strip()}\u0026#x3C;|eot_id|\u003e'\u003c/span\u003e)\n\n    prompt_texts.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;|start_header_id|\u003euser\u0026#x3C;|end_header_id|\u003e\\n\\n{user_input.strip()}\u0026#x3C;|eot_id|\u003e\u0026#x3C;|start_header_id|\u003eassistant\u0026#x3C;|end_header_id|\u003e\\n\\n'\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e''\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ejoin\u003c/span\u003e(prompt_texts)\n\n\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e __name__ == \u003cspan class=\"hljs-string\"\u003e'__main__'\u003c/span\u003e:\n    parser = argparse.\u003cspan class=\"hljs-title class_\"\u003eArgumentParser\u003c/span\u003e(description=\u003cspan class=\"hljs-string\"\u003e'Llama3 모델을 사용하여 `generate()` API를 사용하여 토큰 예측'\u003c/span\u003e)\n    parser.\u003cspan class=\"hljs-title function_\"\u003eadd_argument\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'--repo-id-or-model-path'\u003c/span\u003e, type=str, \u003cspan class=\"hljs-keyword\"\u003edefault\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"meta-llama/Meta-Llama-3-70B-Instruct\"\u003c/span\u003e,\n                        help=\u003cspan class=\"hljs-string\"\u003e'Meta-Llama-3 (예: `meta-llama/Meta-Llama-3-70B-Instruct`)를 다운로드할 Huggingface 저장소 ID'\u003c/span\u003e\n                             \u003cspan class=\"hljs-string\"\u003e'또는 Huggingface 체크포인트 폴더에 대한 경로'\u003c/span\u003e)\n    parser.\u003cspan class=\"hljs-title function_\"\u003eadd_argument\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'--prompt'\u003c/span\u003e, type=str, \u003cspan class=\"hljs-keyword\"\u003edefault\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"OpenVINO is\"\u003c/span\u003e,\n                        help=\u003cspan class=\"hljs-string\"\u003e'추론할 프롬프트'\u003c/span\u003e)\n    parser.\u003cspan class=\"hljs-title function_\"\u003eadd_argument\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'--n-predict'\u003c/span\u003e, type=int, \u003cspan class=\"hljs-keyword\"\u003edefault\u003c/span\u003e=\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e,\n                        help=\u003cspan class=\"hljs-string\"\u003e'예측할 최대 토큰 수'\u003c/span\u003e)\n    parser.\u003cspan class=\"hljs-title function_\"\u003eadd_argument\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'--bit'\u003c/span\u003e, type=str, \u003cspan class=\"hljs-keyword\"\u003edefault\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"4\"\u003c/span\u003e,\n                        help=\u003cspan class=\"hljs-string\"\u003e'4로 설정하면 4비트로 로드하거나 off 또는 load_in_low_bit 옵션은 sym_int4, asym_int4, sym_int5, asym_int5, sym_int8,nf3,nf4, fp4, fp8, fp8_e4m3, fp8_e5m2, fp6, gguf_iq2_xxs, gguf_iq2_xs, gguf_iq1_s, gguf_q4k_m, gguf_q4k_s, fp16, bf16, fp6_k'\u003c/span\u003e)\n\n    args = parser.\u003cspan class=\"hljs-title function_\"\u003eparse_args\u003c/span\u003e()\n    model_path = args.\u003cspan class=\"hljs-property\"\u003erepo_id_or_model_path\u003c/span\u003e\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e args.\u003cspan class=\"hljs-property\"\u003ebit\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e\"4\"\u003c/span\u003e:\n        # \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e비트로 모델 로드,\n        # 모델의 관련 레이어를 \u003cspan class=\"hljs-title class_\"\u003eINT4\u003c/span\u003e 형식으로 변환합니다.\n        # iGPU를 사용하는 \u003cspan class=\"hljs-title class_\"\u003eWindows\u003c/span\u003e 사용자의 경우, \u003cspan class=\"hljs-variable constant_\"\u003eLLM\u003c/span\u003e을 실행할 때 \u003cspan class=\"hljs-string\"\u003e`cpu_embedding=True`\u003c/span\u003e를 from_pretrained 함수에서 설정하는 것을 권장합니다.\n        # 이렇게 하면 메모리 집약적인 임베딩 레이어가 iGPU 대신 \u003cspan class=\"hljs-variable constant_\"\u003eCPU\u003c/span\u003e를 활용할 수 있습니다. 이 경우에는 도움이 되지 않습니다.\n        model = \u003cspan class=\"hljs-title class_\"\u003eAutoModelForCausalLM\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(model_path,\n                                                    load_in_4bit=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n                                                    optimize_model=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n                                                    trust_remote_code=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n                                                    use_cache=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n    elif args.\u003cspan class=\"hljs-property\"\u003ebit\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e\"off\"\u003c/span\u003e or args.\u003cspan class=\"hljs-property\"\u003ebit\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e\"32\"\u003c/span\u003e:\n        model = \u003cspan class=\"hljs-title class_\"\u003eAutoModelForCausalLM\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(model_path,\n                                                    optimize_model=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n                                                    trust_remote_code=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n                                                    use_cache=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n    \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n        model = \u003cspan class=\"hljs-title class_\"\u003eAutoModelForCausalLM\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(model_path,\n                                                    load_in_low_bit=args.\u003cspan class=\"hljs-property\"\u003ebit\u003c/span\u003e,\n                                                    optimize_model=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n                                                    trust_remote_code=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n                                                    use_cache=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e args.\u003cspan class=\"hljs-property\"\u003ebit\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e\"32\"\u003c/span\u003e:\n        model = model.\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'xpu'\u003c/span\u003e)\n    \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n        model = model.\u003cspan class=\"hljs-title function_\"\u003ehalf\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'xpu'\u003c/span\u003e)\n\n    # 토크나이저 로드\n    tokenizer = \u003cspan class=\"hljs-title class_\"\u003eAutoTokenizer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(model_path, trust_remote_code=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\n    # 여기서 종결자는 다음을 참조합니다. \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct#transformers-automodelforcausallm\u003c/span\u003e\n    종결자 = [\n        tokenizer.\u003cspan class=\"hljs-property\"\u003eeos_token_id\u003c/span\u003e,\n        tokenizer.\u003cspan class=\"hljs-title function_\"\u003econvert_tokens_to_ids\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"\u0026#x3C;|eot_id|\u003e\"\u003c/span\u003e),\n    ]\n\n    # 예측된 토큰 생성\n    \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e torch.\u003cspan class=\"hljs-title function_\"\u003einference_mode\u003c/span\u003e():\n        prompt = \u003cspan class=\"hljs-title function_\"\u003eget_prompt\u003c/span\u003e(args.\u003cspan class=\"hljs-property\"\u003eprompt\u003c/span\u003e, [], system_prompt=\u003cspan class=\"hljs-variable constant_\"\u003eDEFAULT_SYSTEM_PROMPT\u003c/span\u003e)\n        input_ids = tokenizer.\u003cspan class=\"hljs-title function_\"\u003eencode\u003c/span\u003e(prompt, return_tensors=\u003cspan class=\"hljs-string\"\u003e\"pt\"\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'xpu'\u003c/span\u003e)\n        # ipex_llm 모델은 워밍업이 필요하므로 추론 시간을 정확하게 할 수 있습니다.\n        output = model.\u003cspan class=\"hljs-title function_\"\u003egenerate\u003c/span\u003e(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)\n\n        # 추론 시작\n        st = time.\u003cspan class=\"hljs-title function_\"\u003etime\u003c/span\u003e()\n        output = model.\u003cspan class=\"hljs-title function_\"\u003egenerate\u003c/span\u003e(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=args.\u003cspan class=\"hljs-property\"\u003en_predict\u003c/span\u003e)\n        torch.\u003cspan class=\"hljs-property\"\u003expu\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003esynchronize\u003c/span\u003e()\n        end = time.\u003cspan class=\"hljs-title function_\"\u003etime\u003c/span\u003e()\n        output = output.\u003cspan class=\"hljs-title function_\"\u003ecpu\u003c/span\u003e()\n        output_str = tokenizer.\u003cspan class=\"hljs-title function_\"\u003edecode\u003c/span\u003e(output[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], skip_special_tokens=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'추론 시간: {end-st} s, 토큰: {len(output[0])}, t/s:{len(output[0])/(end-st)}'\u003c/span\u003e)\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'-'\u003c/span\u003e*\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'프롬프트'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'-'\u003c/span\u003e*\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(prompt)\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'-'\u003c/span\u003e*\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'출력 (skip_special_tokens=False)'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'-'\u003c/span\u003e*\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(output_str)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epython ipex-llm-llama3.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e --repo-id-or-model-path=meta-llama/\u003cspan class=\"hljs-title class_\"\u003eMeta\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eLlama\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e-8B-\u003cspan class=\"hljs-title class_\"\u003eInstruct\u003c/span\u003e --bit=fp16\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_2.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_3.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eDirectML은 낮은 비트 양자화를 지원하는 ONNX Runtime의 Execution Provider가 될 수도 있습니다. 우리의 테스트는 이 python API를 기반으로 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epip install onnxruntime-genai --pre\npip install onnxruntime-genai-directml --pre\npip install torch transformers onnx onnxruntime\nconda install conda-\u003cspan class=\"hljs-attr\"\u003eforge\u003c/span\u003e::vs2015_runtime\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e마지막 줄은 이 문제를 해결하기 위한 것입니다.\u003c/p\u003e\n\u003cp\u003e라마 모델은 다음과 같이 변환되었습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epython -m onnxruntime_genai.\u003cspan class=\"hljs-property\"\u003emodels\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ebuilder\u003c/span\u003e -m meta-llama/\u003cspan class=\"hljs-title class_\"\u003eMeta\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eLlama\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e-8B-\u003cspan class=\"hljs-title class_\"\u003eInstruct\u003c/span\u003e -o llama-\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e-8B-\u003cspan class=\"hljs-title class_\"\u003eInstruct\u003c/span\u003e-int4-onnx-directml -p int4 -e dml -c ..\\.\u003cspan class=\"hljs-property\"\u003ecache\u003c/span\u003e\\huggingface\\hub\\\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e변환된 모델이 🤗 허브에 업로드되었습니다.\u003c/p\u003e\n\u003cp\u003e다음은 테스트 프로그램 genai-llama3.py입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e time\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e argparse\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e onnxruntime_genai \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e og\n\n\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e __name__ == \u003cspan class=\"hljs-string\"\u003e'__main__'\u003c/span\u003e:\n    parser = argparse.ArgumentParser(description=\u003cspan class=\"hljs-string\"\u003e'Predict Tokens using onnxruntime_genai'\u003c/span\u003e)\n    parser.add_argument(\u003cspan class=\"hljs-string\"\u003e'--model-path'\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003etype\u003c/span\u003e=\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e, default=\u003cspan class=\"hljs-string\"\u003e\"llama-3-8B-Instruct-int4-onnx-directml\"\u003c/span\u003e,\n                        \u003cspan class=\"hljs-built_in\"\u003ehelp\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e'model path'\u003c/span\u003e)\n    parser.add_argument(\u003cspan class=\"hljs-string\"\u003e'--prompt'\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003etype\u003c/span\u003e=\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e, default=\u003cspan class=\"hljs-string\"\u003e\"OpenVINO is\"\u003c/span\u003e,\n                        \u003cspan class=\"hljs-built_in\"\u003ehelp\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e'Prompt to infer'\u003c/span\u003e)\n    parser.add_argument(\n        \u003cspan class=\"hljs-string\"\u003e'--max-length'\u003c/span\u003e,\n        \u003cspan class=\"hljs-built_in\"\u003etype\u003c/span\u003e=\u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e,\n        default=\u003cspan class=\"hljs-number\"\u003e143\u003c/span\u003e,\n        \u003cspan class=\"hljs-built_in\"\u003ehelp\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e'max lengths'\u003c/span\u003e\n    )\n    args = parser.parse_args()\n\n    model = og.Model(args.model_path)\n    tokenizer = og.Tokenizer(model)\n    \n    \u003cspan class=\"hljs-comment\"\u003e# Set the max length to something sensible by default,\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e# since otherwise it will be set to the entire context length\u003c/span\u003e\n    search_options = {}\n    search_options[\u003cspan class=\"hljs-string\"\u003e'max_length'\u003c/span\u003e] = args.max_length\n\n    chat_template = \u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;|user|\u003e\\n{input} \u0026#x3C;|end|\u003e\\n\u0026#x3C;|assistant|\u003e'\u003c/span\u003e\n\n    text = args.prompt\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e text:\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"오류, 입력이 비어 있을 수 없습니다\"\u003c/span\u003e)\n        exit\n\n    prompt = \u003cspan class=\"hljs-string\"\u003ef'\u003cspan class=\"hljs-subst\"\u003e{chat_template.\u003cspan class=\"hljs-built_in\"\u003eformat\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003einput\u003c/span\u003e=text)}\u003c/span\u003e'\u003c/span\u003e\n\n    input_tokens = tokenizer.encode(prompt)\n\n    params = og.GeneratorParams(model)\n    params.set_search_options(**search_options)\n    params.input_ids = input_tokens\n\n    st =  time.time()\n    output = model.generate(params)\n    out_txt = tokenizer.decode(output[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\n\n    sec = time.time() - st\n    cnt = \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(output[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\n\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"생성 결과:\"\u003c/span\u003e, out_txt)\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef'추론 시간: \u003cspan class=\"hljs-subst\"\u003e{sec}\u003c/span\u003e 초, 토큰 수: \u003cspan class=\"hljs-subst\"\u003e{cnt}\u003c/span\u003e, 초당 토큰 수:\u003cspan class=\"hljs-subst\"\u003e{cnt/sec}\u003c/span\u003e'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_4.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_5.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e🤗Transformers + IPEX-LLM과 비교해 볼 수 있어요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epython ipex-llm-llama3.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e --repo-id-or-model-path=meta-llama/\u003cspan class=\"hljs-title class_\"\u003eMeta\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eLlama\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e-8B-\u003cspan class=\"hljs-title class_\"\u003eInstruct\u003c/span\u003e --bit=\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_6.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_7.png\" alt=\"이미지1\"\u003e\u003c/p\u003e\n\u003cp\u003e결과를 함께 살펴봅시다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_8.png\" alt=\"이미지2\"\u003e\u003c/p\u003e\n\u003cp\u003e앞으로 어떻게 이러한 흥미로운 차이가 생겼는지에 대한 구현 세부사항을 탐구할 것입니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>