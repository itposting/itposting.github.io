<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>Llama-3 ì¶”ë¡ ì„ Intel Core Ultra 5ì—ì„œ ì‹¤í–‰í•˜ê¸° DirectML ë° ONNX ëŒ€ IPEX-LLM | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="Llama-3 ì¶”ë¡ ì„ Intel Core Ultra 5ì—ì„œ ì‹¤í–‰í•˜ê¸° DirectML ë° ONNX ëŒ€ IPEX-LLM | itposting" data-gatsby-head="true"/><meta property="og:title" content="Llama-3 ì¶”ë¡ ì„ Intel Core Ultra 5ì—ì„œ ì‹¤í–‰í•˜ê¸° DirectML ë° ONNX ëŒ€ IPEX-LLM | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM" data-gatsby-head="true"/><meta name="twitter:title" content="Llama-3 ì¶”ë¡ ì„ Intel Core Ultra 5ì—ì„œ ì‹¤í–‰í•˜ê¸° DirectML ë° ONNX ëŒ€ IPEX-LLM | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 01:21" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">Llama-3 ì¶”ë¡ ì„ Intel Core Ultra 5ì—ì„œ ì‹¤í–‰í•˜ê¸° DirectML ë° ONNX ëŒ€ IPEX-LLM</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="Llama-3 ì¶”ë¡ ì„ Intel Core Ultra 5ì—ì„œ ì‹¤í–‰í•˜ê¸° DirectML ë° ONNX ëŒ€ IPEX-LLM" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">10<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>ì´ì „ ê¸€ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ Intelì€ ONNX + DirectMLì„ ìœ„í•œ í•˜ë“œì›¨ì–´ ê°€ì†í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ì— ëŒ€í•´ ëª‡ ê°€ì§€ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.</p>
<p>Microsoftì€ PyTorchë¥¼ ìœ„í•œ DirectML ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í˜„ì¬ 16ë¹„íŠ¸ì™€ 32ë¹„íŠ¸ ë¶€ë™ ì†Œìˆ˜ì ì—ì„œë§Œ ì¶”ë¡ ì„ ì§€ì›í•©ë‹ˆë‹¤. ì˜ˆì œì—ì„œ ì´ˆë‹¹ í† í° ìˆ˜ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ í¬í¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.</p>
<pre><code class="hljs language-js">conda create --name pytdml python=<span class="hljs-number">3.10</span> -y
conda activate pytdml
pip install torch-directml
git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/luweigen/DirectML</span>
cd <span class="hljs-title class_">DirectML</span>/<span class="hljs-title class_">PyTorch</span>/llm
pip install -r requirements.<span class="hljs-property">txt</span>
python app.<span class="hljs-property">py</span> --precision float16 --model_repo <span class="hljs-string">"meta-llama/Meta-Llama-3-8B-Instruct"</span> --stream_every_n=<span class="hljs-number">143</span>
</code></pre>
<img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png">
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_1.png">
<p>ì´ì „ ì‹¤í—˜ì—ì„œ ğŸ¤—Transformers + IPEX-LLMì´ ìµœìƒì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ê¸° ë•Œë¬¸ì— ì´ ì„¤ì •ì—ì„œëŠ” 16ë¹„íŠ¸ ë¶€ë™ ì†Œìˆ˜ì  ì¶”ë¡ ë§Œ ë¹„êµí•  ê²ƒì…ë‹ˆë‹¤.</p>
<p>í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ ipex-llm-llama3.pyëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
<pre><code class="hljs language-js"># <span class="hljs-title class_">Wei</span> <span class="hljs-title class_">Lu</span>(mailwlu@gmail.<span class="hljs-property">com</span>)ì— ì˜í•´ ìˆ˜ì •ë¨
# <span class="hljs-number">2016</span>ë…„ <span class="hljs-title class_">The</span> <span class="hljs-title class_">BigDL</span> <span class="hljs-title class_">Authors</span>ì— ì €ì‘ê¶Œ ì†í•¨
#
# <span class="hljs-title class_">Apache</span> ë¼ì´ì„ ìŠ¤, ë²„ì „ <span class="hljs-number">2.0</span>ì— ë”°ë¼ ë¼ì´ì„ ìŠ¤ ë¶€ì—¬
# ì´ íŒŒì¼ì„ ë¼ì´ì„ ìŠ¤ì™€ ì¤€ìˆ˜í•˜ë©´ì„œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
# ë¼ì´ì„ ìŠ¤ ì‚¬ë³¸ì€ ë‹¤ìŒì—ì„œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
#     <span class="hljs-attr">http</span>:<span class="hljs-comment">//www.apache.org/licenses/LICENSE-2.0</span>
#
# ì ìš©ë˜ëŠ” ë²•ë¥ ì— ë”°ë¼ í•„ìš”í•˜ê±°ë‚˜ ì„œë©´ìœ¼ë¡œ í•©ì˜ë˜ê±°ë‚˜, ì†Œí”„íŠ¸ì›¨ì–´ê°€
# <span class="hljs-string">"ìˆëŠ” ê·¸ëŒ€ë¡œ"</span> ë°°í¬ë©ë‹ˆë‹¤. ì¡°ê±´ì´ë‚˜ ë³´ì¦ ì—†ì´
# ëª…ì‹œ ë˜ëŠ” ë¬µì‹œì ìœ¼ë¡œ, ê¹Œì§€ë„ ì–´ë–¤ ì¢…ë¥˜ì˜ ì¡°ê±´ë„ ë³´ì¦ ì—†ì´,
# ëª…ì‹œì  ë˜ëŠ” ë¬µì‹œì ìœ¼ë¡œ. ì–¸ì–´ íŠ¹ì • ê¶Œí•œê³¼ ê´€ë ¨í•´ì•¼ í•©ë‹ˆë‹¤.
# ê¶Œí•œ ë° ì œí•œ ì‚¬í•­
#

<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> argparse

<span class="hljs-keyword">from</span> ipex_llm.<span class="hljs-property">transformers</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">AutoModelForCausalLM</span>
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> <span class="hljs-title class_">AutoTokenizer</span>

# ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ì—¬ê¸°ì„œ í”„ë¡¬í”„íŠ¸ ì¡°ì •ì€ ë‹¤ìŒì„ ì°¸ì¡°í•©ë‹ˆë‹¤. <span class="hljs-attr">https</span>:<span class="hljs-comment">//llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3</span>
ê¸°ë³¸_ì‹œìŠ¤í…œ_í”„ë¡¬í”„íŠ¸ = <span class="hljs-string">""</span><span class="hljs-string">"\
"</span><span class="hljs-string">""</span>

def <span class="hljs-title function_">get_prompt</span>(<span class="hljs-attr">user_input</span>: str, <span class="hljs-attr">chat_history</span>: list[tuple[str, str]],
               <span class="hljs-attr">system_prompt</span>: str) -> <span class="hljs-attr">str</span>:
    prompt_texts = [f<span class="hljs-string">'&#x3C;|begin_of_text|>'</span>]

    <span class="hljs-keyword">if</span> system_prompt != <span class="hljs-string">''</span>:
        prompt_texts.<span class="hljs-title function_">append</span>(f<span class="hljs-string">'&#x3C;|start_header_id|>system&#x3C;|end_header_id|>\n\n{system_prompt}&#x3C;|eot_id|>'</span>)

    <span class="hljs-keyword">for</span> history_input, history_response <span class="hljs-keyword">in</span> <span class="hljs-attr">chat_history</span>:
        prompt_texts.<span class="hljs-title function_">append</span>(f<span class="hljs-string">'&#x3C;|start_header_id|>user&#x3C;|end_header_id|>\n\n{history_input.strip()}&#x3C;|eot_id|>'</span>)
        prompt_texts.<span class="hljs-title function_">append</span>(f<span class="hljs-string">'&#x3C;|start_header_id|>assistant&#x3C;|end_header_id|>\n\n{history_response.strip()}&#x3C;|eot_id|>'</span>)

    prompt_texts.<span class="hljs-title function_">append</span>(f<span class="hljs-string">'&#x3C;|start_header_id|>user&#x3C;|end_header_id|>\n\n{user_input.strip()}&#x3C;|eot_id|>&#x3C;|start_header_id|>assistant&#x3C;|end_header_id|>\n\n'</span>)
    <span class="hljs-keyword">return</span> <span class="hljs-string">''</span>.<span class="hljs-title function_">join</span>(prompt_texts)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    parser = argparse.<span class="hljs-title class_">ArgumentParser</span>(description=<span class="hljs-string">'Llama3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ `generate()` APIë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ì˜ˆì¸¡'</span>)
    parser.<span class="hljs-title function_">add_argument</span>(<span class="hljs-string">'--repo-id-or-model-path'</span>, type=str, <span class="hljs-keyword">default</span>=<span class="hljs-string">"meta-llama/Meta-Llama-3-70B-Instruct"</span>,
                        help=<span class="hljs-string">'Meta-Llama-3 (ì˜ˆ: `meta-llama/Meta-Llama-3-70B-Instruct`)ë¥¼ ë‹¤ìš´ë¡œë“œí•  Huggingface ì €ì¥ì†Œ ID'</span>
                             <span class="hljs-string">'ë˜ëŠ” Huggingface ì²´í¬í¬ì¸íŠ¸ í´ë”ì— ëŒ€í•œ ê²½ë¡œ'</span>)
    parser.<span class="hljs-title function_">add_argument</span>(<span class="hljs-string">'--prompt'</span>, type=str, <span class="hljs-keyword">default</span>=<span class="hljs-string">"OpenVINO is"</span>,
                        help=<span class="hljs-string">'ì¶”ë¡ í•  í”„ë¡¬í”„íŠ¸'</span>)
    parser.<span class="hljs-title function_">add_argument</span>(<span class="hljs-string">'--n-predict'</span>, type=int, <span class="hljs-keyword">default</span>=<span class="hljs-number">128</span>,
                        help=<span class="hljs-string">'ì˜ˆì¸¡í•  ìµœëŒ€ í† í° ìˆ˜'</span>)
    parser.<span class="hljs-title function_">add_argument</span>(<span class="hljs-string">'--bit'</span>, type=str, <span class="hljs-keyword">default</span>=<span class="hljs-string">"4"</span>,
                        help=<span class="hljs-string">'4ë¡œ ì„¤ì •í•˜ë©´ 4ë¹„íŠ¸ë¡œ ë¡œë“œí•˜ê±°ë‚˜ off ë˜ëŠ” load_in_low_bit ì˜µì…˜ì€ sym_int4, asym_int4, sym_int5, asym_int5, sym_int8,nf3,nf4, fp4, fp8, fp8_e4m3, fp8_e5m2, fp6, gguf_iq2_xxs, gguf_iq2_xs, gguf_iq1_s, gguf_q4k_m, gguf_q4k_s, fp16, bf16, fp6_k'</span>)

    args = parser.<span class="hljs-title function_">parse_args</span>()
    model_path = args.<span class="hljs-property">repo_id_or_model_path</span>

    <span class="hljs-keyword">if</span> args.<span class="hljs-property">bit</span> == <span class="hljs-string">"4"</span>:
        # <span class="hljs-number">4</span>ë¹„íŠ¸ë¡œ ëª¨ë¸ ë¡œë“œ,
        # ëª¨ë¸ì˜ ê´€ë ¨ ë ˆì´ì–´ë¥¼ <span class="hljs-title class_">INT4</span> í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        # iGPUë¥¼ ì‚¬ìš©í•˜ëŠ” <span class="hljs-title class_">Windows</span> ì‚¬ìš©ìì˜ ê²½ìš°, <span class="hljs-variable constant_">LLM</span>ì„ ì‹¤í–‰í•  ë•Œ <span class="hljs-string">`cpu_embedding=True`</span>ë¥¼ from_pretrained í•¨ìˆ˜ì—ì„œ ì„¤ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
        # ì´ë ‡ê²Œ í•˜ë©´ ë©”ëª¨ë¦¬ ì§‘ì•½ì ì¸ ì„ë² ë”© ë ˆì´ì–´ê°€ iGPU ëŒ€ì‹  <span class="hljs-variable constant_">CPU</span>ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°ì—ëŠ” ë„ì›€ì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        model = <span class="hljs-title class_">AutoModelForCausalLM</span>.<span class="hljs-title function_">from_pretrained</span>(model_path,
                                                    load_in_4bit=<span class="hljs-title class_">True</span>,
                                                    optimize_model=<span class="hljs-title class_">True</span>,
                                                    trust_remote_code=<span class="hljs-title class_">True</span>,
                                                    use_cache=<span class="hljs-title class_">True</span>)
    elif args.<span class="hljs-property">bit</span> == <span class="hljs-string">"off"</span> or args.<span class="hljs-property">bit</span> == <span class="hljs-string">"32"</span>:
        model = <span class="hljs-title class_">AutoModelForCausalLM</span>.<span class="hljs-title function_">from_pretrained</span>(model_path,
                                                    optimize_model=<span class="hljs-title class_">True</span>,
                                                    trust_remote_code=<span class="hljs-title class_">True</span>,
                                                    use_cache=<span class="hljs-title class_">True</span>)
    <span class="hljs-attr">else</span>:
        model = <span class="hljs-title class_">AutoModelForCausalLM</span>.<span class="hljs-title function_">from_pretrained</span>(model_path,
                                                    load_in_low_bit=args.<span class="hljs-property">bit</span>,
                                                    optimize_model=<span class="hljs-title class_">True</span>,
                                                    trust_remote_code=<span class="hljs-title class_">True</span>,
                                                    use_cache=<span class="hljs-title class_">True</span>)

    <span class="hljs-keyword">if</span> args.<span class="hljs-property">bit</span> == <span class="hljs-string">"32"</span>:
        model = model.<span class="hljs-title function_">to</span>(<span class="hljs-string">'xpu'</span>)
    <span class="hljs-attr">else</span>:
        model = model.<span class="hljs-title function_">half</span>().<span class="hljs-title function_">to</span>(<span class="hljs-string">'xpu'</span>)

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = <span class="hljs-title class_">AutoTokenizer</span>.<span class="hljs-title function_">from_pretrained</span>(model_path, trust_remote_code=<span class="hljs-title class_">True</span>)

    # ì—¬ê¸°ì„œ ì¢…ê²°ìëŠ” ë‹¤ìŒì„ ì°¸ì¡°í•©ë‹ˆë‹¤. <span class="hljs-attr">https</span>:<span class="hljs-comment">//huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct#transformers-automodelforcausallm</span>
    ì¢…ê²°ì = [
        tokenizer.<span class="hljs-property">eos_token_id</span>,
        tokenizer.<span class="hljs-title function_">convert_tokens_to_ids</span>(<span class="hljs-string">"&#x3C;|eot_id|>"</span>),
    ]

    # ì˜ˆì¸¡ëœ í† í° ìƒì„±
    <span class="hljs-keyword">with</span> torch.<span class="hljs-title function_">inference_mode</span>():
        prompt = <span class="hljs-title function_">get_prompt</span>(args.<span class="hljs-property">prompt</span>, [], system_prompt=<span class="hljs-variable constant_">DEFAULT_SYSTEM_PROMPT</span>)
        input_ids = tokenizer.<span class="hljs-title function_">encode</span>(prompt, return_tensors=<span class="hljs-string">"pt"</span>).<span class="hljs-title function_">to</span>(<span class="hljs-string">'xpu'</span>)
        # ipex_llm ëª¨ë¸ì€ ì›Œë°ì—…ì´ í•„ìš”í•˜ë¯€ë¡œ ì¶”ë¡  ì‹œê°„ì„ ì •í™•í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        output = model.<span class="hljs-title function_">generate</span>(input_ids,
                                eos_token_id=terminators,
                                max_new_tokens=<span class="hljs-number">20</span>)

        # ì¶”ë¡  ì‹œì‘
        st = time.<span class="hljs-title function_">time</span>()
        output = model.<span class="hljs-title function_">generate</span>(input_ids,
                                eos_token_id=terminators,
                                max_new_tokens=args.<span class="hljs-property">n_predict</span>)
        torch.<span class="hljs-property">xpu</span>.<span class="hljs-title function_">synchronize</span>()
        end = time.<span class="hljs-title function_">time</span>()
        output = output.<span class="hljs-title function_">cpu</span>()
        output_str = tokenizer.<span class="hljs-title function_">decode</span>(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-title class_">False</span>)
        <span class="hljs-title function_">print</span>(f<span class="hljs-string">'ì¶”ë¡  ì‹œê°„: {end-st} s, í† í°: {len(output[0])}, t/s:{len(output[0])/(end-st)}'</span>)
        <span class="hljs-title function_">print</span>(<span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>, <span class="hljs-string">'í”„ë¡¬í”„íŠ¸'</span>, <span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>)
        <span class="hljs-title function_">print</span>(prompt)
        <span class="hljs-title function_">print</span>(<span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>, <span class="hljs-string">'ì¶œë ¥ (skip_special_tokens=False)'</span>, <span class="hljs-string">'-'</span>*<span class="hljs-number">20</span>)
        <span class="hljs-title function_">print</span>(output_str)
</code></pre>
<div class="content-ad"></div>
<pre><code class="hljs language-js">python ipex-llm-llama3.<span class="hljs-property">py</span> --repo-id-or-model-path=meta-llama/<span class="hljs-title class_">Meta</span>-<span class="hljs-title class_">Llama</span>-<span class="hljs-number">3</span>-8B-<span class="hljs-title class_">Instruct</span> --bit=fp16
</code></pre>
<p><img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_2.png" alt="image"></p>
<p><img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_3.png" alt="image"></p>
<p>DirectMLì€ ë‚®ì€ ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ì§€ì›í•˜ëŠ” ONNX Runtimeì˜ Execution Providerê°€ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ í…ŒìŠ¤íŠ¸ëŠ” ì´ python APIë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">pip install onnxruntime-genai --pre
pip install onnxruntime-genai-directml --pre
pip install torch transformers onnx onnxruntime
conda install conda-<span class="hljs-attr">forge</span>::vs2015_runtime
</code></pre>
<p>ë§ˆì§€ë§‰ ì¤„ì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.</p>
<p>ë¼ë§ˆ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
<pre><code class="hljs language-js">python -m onnxruntime_genai.<span class="hljs-property">models</span>.<span class="hljs-property">builder</span> -m meta-llama/<span class="hljs-title class_">Meta</span>-<span class="hljs-title class_">Llama</span>-<span class="hljs-number">3</span>-8B-<span class="hljs-title class_">Instruct</span> -o llama-<span class="hljs-number">3</span>-8B-<span class="hljs-title class_">Instruct</span>-int4-onnx-directml -p int4 -e dml -c ..\.<span class="hljs-property">cache</span>\huggingface\hub\
</code></pre>
<div class="content-ad"></div>
<p>ë³€í™˜ëœ ëª¨ë¸ì´ ğŸ¤— í—ˆë¸Œì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.</p>
<p>ë‹¤ìŒì€ í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ genai-llama3.pyì…ë‹ˆë‹¤.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> argparse
<span class="hljs-keyword">import</span> onnxruntime_genai <span class="hljs-keyword">as</span> og

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    parser = argparse.ArgumentParser(description=<span class="hljs-string">'Predict Tokens using onnxruntime_genai'</span>)
    parser.add_argument(<span class="hljs-string">'--model-path'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">"llama-3-8B-Instruct-int4-onnx-directml"</span>,
                        <span class="hljs-built_in">help</span>=<span class="hljs-string">'model path'</span>)
    parser.add_argument(<span class="hljs-string">'--prompt'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">"OpenVINO is"</span>,
                        <span class="hljs-built_in">help</span>=<span class="hljs-string">'Prompt to infer'</span>)
    parser.add_argument(
        <span class="hljs-string">'--max-length'</span>,
        <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>,
        default=<span class="hljs-number">143</span>,
        <span class="hljs-built_in">help</span>=<span class="hljs-string">'max lengths'</span>
    )
    args = parser.parse_args()

    model = og.Model(args.model_path)
    tokenizer = og.Tokenizer(model)
    
    <span class="hljs-comment"># Set the max length to something sensible by default,</span>
    <span class="hljs-comment"># since otherwise it will be set to the entire context length</span>
    search_options = {}
    search_options[<span class="hljs-string">'max_length'</span>] = args.max_length

    chat_template = <span class="hljs-string">'&#x3C;|user|>\n{input} &#x3C;|end|>\n&#x3C;|assistant|>'</span>

    text = args.prompt
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> text:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"ì˜¤ë¥˜, ì…ë ¥ì´ ë¹„ì–´ ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"</span>)
        exit

    prompt = <span class="hljs-string">f'<span class="hljs-subst">{chat_template.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">input</span>=text)}</span>'</span>

    input_tokens = tokenizer.encode(prompt)

    params = og.GeneratorParams(model)
    params.set_search_options(**search_options)
    params.input_ids = input_tokens

    st =  time.time()
    output = model.generate(params)
    out_txt = tokenizer.decode(output[<span class="hljs-number">0</span>])

    sec = time.time() - st
    cnt = <span class="hljs-built_in">len</span>(output[<span class="hljs-number">0</span>])

    <span class="hljs-built_in">print</span>(<span class="hljs-string">"ìƒì„± ê²°ê³¼:"</span>, out_txt)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'ì¶”ë¡  ì‹œê°„: <span class="hljs-subst">{sec}</span> ì´ˆ, í† í° ìˆ˜: <span class="hljs-subst">{cnt}</span>, ì´ˆë‹¹ í† í° ìˆ˜:<span class="hljs-subst">{cnt/sec}</span>'</span>)
</code></pre>
<img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_4.png">
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_5.png" alt="ì´ë¯¸ì§€"></p>
<p>ğŸ¤—Transformers + IPEX-LLMê³¼ ë¹„êµí•´ ë³¼ ìˆ˜ ìˆì–´ìš”.</p>
<pre><code class="hljs language-js">python ipex-llm-llama3.<span class="hljs-property">py</span> --repo-id-or-model-path=meta-llama/<span class="hljs-title class_">Meta</span>-<span class="hljs-title class_">Llama</span>-<span class="hljs-number">3</span>-8B-<span class="hljs-title class_">Instruct</span> --bit=<span class="hljs-number">4</span>
</code></pre>
<p><img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_6.png" alt="ì´ë¯¸ì§€"></p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_7.png" alt="ì´ë¯¸ì§€1"></p>
<p>ê²°ê³¼ë¥¼ í•¨ê»˜ ì‚´í´ë´…ì‹œë‹¤.</p>
<p><img src="/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_8.png" alt="ì´ë¯¸ì§€2"></p>
<p>ì•ìœ¼ë¡œ ì–´ë–»ê²Œ ì´ëŸ¬í•œ í¥ë¯¸ë¡œìš´ ì°¨ì´ê°€ ìƒê²¼ëŠ”ì§€ì— ëŒ€í•œ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì„ íƒêµ¬í•  ê²ƒì…ë‹ˆë‹¤.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Llama-3 ì¶”ë¡ ì„ Intel Core Ultra 5ì—ì„œ ì‹¤í–‰í•˜ê¸° DirectML ë° ONNX ëŒ€ IPEX-LLM","description":"","date":"2024-06-19 01:21","slug":"2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM","content":"\n\nì´ì „ ê¸€ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ Intelì€ ONNX + DirectMLì„ ìœ„í•œ í•˜ë“œì›¨ì–´ ê°€ì†í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ì— ëŒ€í•´ ëª‡ ê°€ì§€ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.\n\nMicrosoftì€ PyTorchë¥¼ ìœ„í•œ DirectML ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í˜„ì¬ 16ë¹„íŠ¸ì™€ 32ë¹„íŠ¸ ë¶€ë™ ì†Œìˆ˜ì ì—ì„œë§Œ ì¶”ë¡ ì„ ì§€ì›í•©ë‹ˆë‹¤. ì˜ˆì œì—ì„œ ì´ˆë‹¹ í† í° ìˆ˜ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ í¬í¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\n\n```js\nconda create --name pytdml python=3.10 -y\nconda activate pytdml\npip install torch-directml\ngit clone https://github.com/luweigen/DirectML\ncd DirectML/PyTorch/llm\npip install -r requirements.txt\npython app.py --precision float16 --model_repo \"meta-llama/Meta-Llama-3-8B-Instruct\" --stream_every_n=143\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_1.png\" /\u003e\n\nì´ì „ ì‹¤í—˜ì—ì„œ ğŸ¤—Transformers + IPEX-LLMì´ ìµœìƒì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ê¸° ë•Œë¬¸ì— ì´ ì„¤ì •ì—ì„œëŠ” 16ë¹„íŠ¸ ë¶€ë™ ì†Œìˆ˜ì  ì¶”ë¡ ë§Œ ë¹„êµí•  ê²ƒì…ë‹ˆë‹¤.\n\ní…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ ipex-llm-llama3.pyëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n```js\n# Wei Lu(mailwlu@gmail.com)ì— ì˜í•´ ìˆ˜ì •ë¨\n# 2016ë…„ The BigDL Authorsì— ì €ì‘ê¶Œ ì†í•¨\n#\n# Apache ë¼ì´ì„ ìŠ¤, ë²„ì „ 2.0ì— ë”°ë¼ ë¼ì´ì„ ìŠ¤ ë¶€ì—¬\n# ì´ íŒŒì¼ì„ ë¼ì´ì„ ìŠ¤ì™€ ì¤€ìˆ˜í•˜ë©´ì„œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n# ë¼ì´ì„ ìŠ¤ ì‚¬ë³¸ì€ ë‹¤ìŒì—ì„œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# ì ìš©ë˜ëŠ” ë²•ë¥ ì— ë”°ë¼ í•„ìš”í•˜ê±°ë‚˜ ì„œë©´ìœ¼ë¡œ í•©ì˜ë˜ê±°ë‚˜, ì†Œí”„íŠ¸ì›¨ì–´ê°€\n# \"ìˆëŠ” ê·¸ëŒ€ë¡œ\" ë°°í¬ë©ë‹ˆë‹¤. ì¡°ê±´ì´ë‚˜ ë³´ì¦ ì—†ì´\n# ëª…ì‹œ ë˜ëŠ” ë¬µì‹œì ìœ¼ë¡œ, ê¹Œì§€ë„ ì–´ë–¤ ì¢…ë¥˜ì˜ ì¡°ê±´ë„ ë³´ì¦ ì—†ì´,\n# ëª…ì‹œì  ë˜ëŠ” ë¬µì‹œì ìœ¼ë¡œ. ì–¸ì–´ íŠ¹ì • ê¶Œí•œê³¼ ê´€ë ¨í•´ì•¼ í•©ë‹ˆë‹¤.\n# ê¶Œí•œ ë° ì œí•œ ì‚¬í•­\n#\n\nimport torch\nimport time\nimport argparse\n\nfrom ipex_llm.transformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer\n\n# ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n# ì—¬ê¸°ì„œ í”„ë¡¬í”„íŠ¸ ì¡°ì •ì€ ë‹¤ìŒì„ ì°¸ì¡°í•©ë‹ˆë‹¤. https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3\nê¸°ë³¸_ì‹œìŠ¤í…œ_í”„ë¡¬í”„íŠ¸ = \"\"\"\\\n\"\"\"\n\ndef get_prompt(user_input: str, chat_history: list[tuple[str, str]],\n               system_prompt: str) -\u003e str:\n    prompt_texts = [f'\u003c|begin_of_text|\u003e']\n\n    if system_prompt != '':\n        prompt_texts.append(f'\u003c|start_header_id|\u003esystem\u003c|end_header_id|\u003e\\n\\n{system_prompt}\u003c|eot_id|\u003e')\n\n    for history_input, history_response in chat_history:\n        prompt_texts.append(f'\u003c|start_header_id|\u003euser\u003c|end_header_id|\u003e\\n\\n{history_input.strip()}\u003c|eot_id|\u003e')\n        prompt_texts.append(f'\u003c|start_header_id|\u003eassistant\u003c|end_header_id|\u003e\\n\\n{history_response.strip()}\u003c|eot_id|\u003e')\n\n    prompt_texts.append(f'\u003c|start_header_id|\u003euser\u003c|end_header_id|\u003e\\n\\n{user_input.strip()}\u003c|eot_id|\u003e\u003c|start_header_id|\u003eassistant\u003c|end_header_id|\u003e\\n\\n')\n    return ''.join(prompt_texts)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Llama3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ `generate()` APIë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ì˜ˆì¸¡')\n    parser.add_argument('--repo-id-or-model-path', type=str, default=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n                        help='Meta-Llama-3 (ì˜ˆ: `meta-llama/Meta-Llama-3-70B-Instruct`)ë¥¼ ë‹¤ìš´ë¡œë“œí•  Huggingface ì €ì¥ì†Œ ID'\n                             'ë˜ëŠ” Huggingface ì²´í¬í¬ì¸íŠ¸ í´ë”ì— ëŒ€í•œ ê²½ë¡œ')\n    parser.add_argument('--prompt', type=str, default=\"OpenVINO is\",\n                        help='ì¶”ë¡ í•  í”„ë¡¬í”„íŠ¸')\n    parser.add_argument('--n-predict', type=int, default=128,\n                        help='ì˜ˆì¸¡í•  ìµœëŒ€ í† í° ìˆ˜')\n    parser.add_argument('--bit', type=str, default=\"4\",\n                        help='4ë¡œ ì„¤ì •í•˜ë©´ 4ë¹„íŠ¸ë¡œ ë¡œë“œí•˜ê±°ë‚˜ off ë˜ëŠ” load_in_low_bit ì˜µì…˜ì€ sym_int4, asym_int4, sym_int5, asym_int5, sym_int8,nf3,nf4, fp4, fp8, fp8_e4m3, fp8_e5m2, fp6, gguf_iq2_xxs, gguf_iq2_xs, gguf_iq1_s, gguf_q4k_m, gguf_q4k_s, fp16, bf16, fp6_k')\n\n    args = parser.parse_args()\n    model_path = args.repo_id_or_model_path\n\n    if args.bit == \"4\":\n        # 4ë¹„íŠ¸ë¡œ ëª¨ë¸ ë¡œë“œ,\n        # ëª¨ë¸ì˜ ê´€ë ¨ ë ˆì´ì–´ë¥¼ INT4 í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n        # iGPUë¥¼ ì‚¬ìš©í•˜ëŠ” Windows ì‚¬ìš©ìì˜ ê²½ìš°, LLMì„ ì‹¤í–‰í•  ë•Œ `cpu_embedding=True`ë¥¼ from_pretrained í•¨ìˆ˜ì—ì„œ ì„¤ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n        # ì´ë ‡ê²Œ í•˜ë©´ ë©”ëª¨ë¦¬ ì§‘ì•½ì ì¸ ì„ë² ë”© ë ˆì´ì–´ê°€ iGPU ëŒ€ì‹  CPUë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°ì—ëŠ” ë„ì›€ì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n        model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                    load_in_4bit=True,\n                                                    optimize_model=True,\n                                                    trust_remote_code=True,\n                                                    use_cache=True)\n    elif args.bit == \"off\" or args.bit == \"32\":\n        model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                    optimize_model=True,\n                                                    trust_remote_code=True,\n                                                    use_cache=True)\n    else:\n        model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                    load_in_low_bit=args.bit,\n                                                    optimize_model=True,\n                                                    trust_remote_code=True,\n                                                    use_cache=True)\n\n    if args.bit == \"32\":\n        model = model.to('xpu')\n    else:\n        model = model.half().to('xpu')\n\n    # í† í¬ë‚˜ì´ì € ë¡œë“œ\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n    # ì—¬ê¸°ì„œ ì¢…ê²°ìëŠ” ë‹¤ìŒì„ ì°¸ì¡°í•©ë‹ˆë‹¤. https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct#transformers-automodelforcausallm\n    ì¢…ê²°ì = [\n        tokenizer.eos_token_id,\n        tokenizer.convert_tokens_to_ids(\"\u003c|eot_id|\u003e\"),\n    ]\n\n    # ì˜ˆì¸¡ëœ í† í° ìƒì„±\n    with torch.inference_mode():\n        prompt = get_prompt(args.prompt, [], system_prompt=DEFAULT_SYSTEM_PROMPT)\n        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('xpu')\n        # ipex_llm ëª¨ë¸ì€ ì›Œë°ì—…ì´ í•„ìš”í•˜ë¯€ë¡œ ì¶”ë¡  ì‹œê°„ì„ ì •í™•í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n        output = model.generate(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=20)\n\n        # ì¶”ë¡  ì‹œì‘\n        st = time.time()\n        output = model.generate(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=args.n_predict)\n        torch.xpu.synchronize()\n        end = time.time()\n        output = output.cpu()\n        output_str = tokenizer.decode(output[0], skip_special_tokens=False)\n        print(f'ì¶”ë¡  ì‹œê°„: {end-st} s, í† í°: {len(output[0])}, t/s:{len(output[0])/(end-st)}')\n        print('-'*20, 'í”„ë¡¬í”„íŠ¸', '-'*20)\n        print(prompt)\n        print('-'*20, 'ì¶œë ¥ (skip_special_tokens=False)', '-'*20)\n        print(output_str)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\npython ipex-llm-llama3.py --repo-id-or-model-path=meta-llama/Meta-Llama-3-8B-Instruct --bit=fp16\n```\n\n![image](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_2.png)\n\n![image](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_3.png)\n\nDirectMLì€ ë‚®ì€ ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ì§€ì›í•˜ëŠ” ONNX Runtimeì˜ Execution Providerê°€ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ í…ŒìŠ¤íŠ¸ëŠ” ì´ python APIë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\npip install onnxruntime-genai --pre\npip install onnxruntime-genai-directml --pre\npip install torch transformers onnx onnxruntime\nconda install conda-forge::vs2015_runtime\n```\n\në§ˆì§€ë§‰ ì¤„ì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.\n\në¼ë§ˆ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n```js\npython -m onnxruntime_genai.models.builder -m meta-llama/Meta-Llama-3-8B-Instruct -o llama-3-8B-Instruct-int4-onnx-directml -p int4 -e dml -c ..\\.cache\\huggingface\\hub\\\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\në³€í™˜ëœ ëª¨ë¸ì´ ğŸ¤— í—ˆë¸Œì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n\në‹¤ìŒì€ í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ genai-llama3.pyì…ë‹ˆë‹¤.\n\n```python\nimport time\nimport argparse\nimport onnxruntime_genai as og\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Predict Tokens using onnxruntime_genai')\n    parser.add_argument('--model-path', type=str, default=\"llama-3-8B-Instruct-int4-onnx-directml\",\n                        help='model path')\n    parser.add_argument('--prompt', type=str, default=\"OpenVINO is\",\n                        help='Prompt to infer')\n    parser.add_argument(\n        '--max-length',\n        type=int,\n        default=143,\n        help='max lengths'\n    )\n    args = parser.parse_args()\n\n    model = og.Model(args.model_path)\n    tokenizer = og.Tokenizer(model)\n    \n    # Set the max length to something sensible by default,\n    # since otherwise it will be set to the entire context length\n    search_options = {}\n    search_options['max_length'] = args.max_length\n\n    chat_template = '\u003c|user|\u003e\\n{input} \u003c|end|\u003e\\n\u003c|assistant|\u003e'\n\n    text = args.prompt\n    if not text:\n        print(\"ì˜¤ë¥˜, ì…ë ¥ì´ ë¹„ì–´ ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        exit\n\n    prompt = f'{chat_template.format(input=text)}'\n\n    input_tokens = tokenizer.encode(prompt)\n\n    params = og.GeneratorParams(model)\n    params.set_search_options(**search_options)\n    params.input_ids = input_tokens\n\n    st =  time.time()\n    output = model.generate(params)\n    out_txt = tokenizer.decode(output[0])\n\n    sec = time.time() - st\n    cnt = len(output[0])\n\n    print(\"ìƒì„± ê²°ê³¼:\", out_txt)\n    print(f'ì¶”ë¡  ì‹œê°„: {sec} ì´ˆ, í† í° ìˆ˜: {cnt}, ì´ˆë‹¹ í† í° ìˆ˜:{cnt/sec}')\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_4.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![ì´ë¯¸ì§€](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_5.png)\n\nğŸ¤—Transformers + IPEX-LLMê³¼ ë¹„êµí•´ ë³¼ ìˆ˜ ìˆì–´ìš”.\n\n```js\npython ipex-llm-llama3.py --repo-id-or-model-path=meta-llama/Meta-Llama-3-8B-Instruct --bit=4\n```\n\n![ì´ë¯¸ì§€](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_6.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![ì´ë¯¸ì§€1](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_7.png)\n\nê²°ê³¼ë¥¼ í•¨ê»˜ ì‚´í´ë´…ì‹œë‹¤.\n\n![ì´ë¯¸ì§€2](/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_8.png)\n\nì•ìœ¼ë¡œ ì–´ë–»ê²Œ ì´ëŸ¬í•œ í¥ë¯¸ë¡œìš´ ì°¨ì´ê°€ ìƒê²¼ëŠ”ì§€ì— ëŒ€í•œ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì„ íƒêµ¬í•  ê²ƒì…ë‹ˆë‹¤.\n","ogImage":{"url":"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png"},"coverImage":"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png","tag":["Tech"],"readingTime":10},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003eì´ì „ ê¸€ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ Intelì€ ONNX + DirectMLì„ ìœ„í•œ í•˜ë“œì›¨ì–´ ê°€ì†í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ì— ëŒ€í•´ ëª‡ ê°€ì§€ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.\u003c/p\u003e\n\u003cp\u003eMicrosoftì€ PyTorchë¥¼ ìœ„í•œ DirectML ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í˜„ì¬ 16ë¹„íŠ¸ì™€ 32ë¹„íŠ¸ ë¶€ë™ ì†Œìˆ˜ì ì—ì„œë§Œ ì¶”ë¡ ì„ ì§€ì›í•©ë‹ˆë‹¤. ì˜ˆì œì—ì„œ ì´ˆë‹¹ í† í° ìˆ˜ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ í¬í¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003econda create --name pytdml python=\u003cspan class=\"hljs-number\"\u003e3.10\u003c/span\u003e -y\nconda activate pytdml\npip install torch-directml\ngit clone \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/luweigen/DirectML\u003c/span\u003e\ncd \u003cspan class=\"hljs-title class_\"\u003eDirectML\u003c/span\u003e/\u003cspan class=\"hljs-title class_\"\u003ePyTorch\u003c/span\u003e/llm\npip install -r requirements.\u003cspan class=\"hljs-property\"\u003etxt\u003c/span\u003e\npython app.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e --precision float16 --model_repo \u003cspan class=\"hljs-string\"\u003e\"meta-llama/Meta-Llama-3-8B-Instruct\"\u003c/span\u003e --stream_every_n=\u003cspan class=\"hljs-number\"\u003e143\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_0.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_1.png\"\u003e\n\u003cp\u003eì´ì „ ì‹¤í—˜ì—ì„œ ğŸ¤—Transformers + IPEX-LLMì´ ìµœìƒì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ê¸° ë•Œë¬¸ì— ì´ ì„¤ì •ì—ì„œëŠ” 16ë¹„íŠ¸ ë¶€ë™ ì†Œìˆ˜ì  ì¶”ë¡ ë§Œ ë¹„êµí•  ê²ƒì…ë‹ˆë‹¤.\u003c/p\u003e\n\u003cp\u003eí…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ ipex-llm-llama3.pyëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# \u003cspan class=\"hljs-title class_\"\u003eWei\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eLu\u003c/span\u003e(mailwlu@gmail.\u003cspan class=\"hljs-property\"\u003ecom\u003c/span\u003e)ì— ì˜í•´ ìˆ˜ì •ë¨\n# \u003cspan class=\"hljs-number\"\u003e2016\u003c/span\u003eë…„ \u003cspan class=\"hljs-title class_\"\u003eThe\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eBigDL\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAuthors\u003c/span\u003eì— ì €ì‘ê¶Œ ì†í•¨\n#\n# \u003cspan class=\"hljs-title class_\"\u003eApache\u003c/span\u003e ë¼ì´ì„ ìŠ¤, ë²„ì „ \u003cspan class=\"hljs-number\"\u003e2.0\u003c/span\u003eì— ë”°ë¼ ë¼ì´ì„ ìŠ¤ ë¶€ì—¬\n# ì´ íŒŒì¼ì„ ë¼ì´ì„ ìŠ¤ì™€ ì¤€ìˆ˜í•˜ë©´ì„œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n# ë¼ì´ì„ ìŠ¤ ì‚¬ë³¸ì€ ë‹¤ìŒì—ì„œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n#\n#     \u003cspan class=\"hljs-attr\"\u003ehttp\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//www.apache.org/licenses/LICENSE-2.0\u003c/span\u003e\n#\n# ì ìš©ë˜ëŠ” ë²•ë¥ ì— ë”°ë¼ í•„ìš”í•˜ê±°ë‚˜ ì„œë©´ìœ¼ë¡œ í•©ì˜ë˜ê±°ë‚˜, ì†Œí”„íŠ¸ì›¨ì–´ê°€\n# \u003cspan class=\"hljs-string\"\u003e\"ìˆëŠ” ê·¸ëŒ€ë¡œ\"\u003c/span\u003e ë°°í¬ë©ë‹ˆë‹¤. ì¡°ê±´ì´ë‚˜ ë³´ì¦ ì—†ì´\n# ëª…ì‹œ ë˜ëŠ” ë¬µì‹œì ìœ¼ë¡œ, ê¹Œì§€ë„ ì–´ë–¤ ì¢…ë¥˜ì˜ ì¡°ê±´ë„ ë³´ì¦ ì—†ì´,\n# ëª…ì‹œì  ë˜ëŠ” ë¬µì‹œì ìœ¼ë¡œ. ì–¸ì–´ íŠ¹ì • ê¶Œí•œê³¼ ê´€ë ¨í•´ì•¼ í•©ë‹ˆë‹¤.\n# ê¶Œí•œ ë° ì œí•œ ì‚¬í•­\n#\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e time\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e argparse\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e ipex_llm.\u003cspan class=\"hljs-property\"\u003etransformers\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAutoModelForCausalLM\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAutoTokenizer\u003c/span\u003e\n\n# ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n# ì—¬ê¸°ì„œ í”„ë¡¬í”„íŠ¸ ì¡°ì •ì€ ë‹¤ìŒì„ ì°¸ì¡°í•©ë‹ˆë‹¤. \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3\u003c/span\u003e\nê¸°ë³¸_ì‹œìŠ¤í…œ_í”„ë¡¬í”„íŠ¸ = \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\\\n\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\ndef \u003cspan class=\"hljs-title function_\"\u003eget_prompt\u003c/span\u003e(\u003cspan class=\"hljs-attr\"\u003euser_input\u003c/span\u003e: str, \u003cspan class=\"hljs-attr\"\u003echat_history\u003c/span\u003e: list[tuple[str, str]],\n               \u003cspan class=\"hljs-attr\"\u003esystem_prompt\u003c/span\u003e: str) -\u003e \u003cspan class=\"hljs-attr\"\u003estr\u003c/span\u003e:\n    prompt_texts = [f\u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;|begin_of_text|\u003e'\u003c/span\u003e]\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e system_prompt != \u003cspan class=\"hljs-string\"\u003e''\u003c/span\u003e:\n        prompt_texts.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;|start_header_id|\u003esystem\u0026#x3C;|end_header_id|\u003e\\n\\n{system_prompt}\u0026#x3C;|eot_id|\u003e'\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e history_input, history_response \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003echat_history\u003c/span\u003e:\n        prompt_texts.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;|start_header_id|\u003euser\u0026#x3C;|end_header_id|\u003e\\n\\n{history_input.strip()}\u0026#x3C;|eot_id|\u003e'\u003c/span\u003e)\n        prompt_texts.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;|start_header_id|\u003eassistant\u0026#x3C;|end_header_id|\u003e\\n\\n{history_response.strip()}\u0026#x3C;|eot_id|\u003e'\u003c/span\u003e)\n\n    prompt_texts.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;|start_header_id|\u003euser\u0026#x3C;|end_header_id|\u003e\\n\\n{user_input.strip()}\u0026#x3C;|eot_id|\u003e\u0026#x3C;|start_header_id|\u003eassistant\u0026#x3C;|end_header_id|\u003e\\n\\n'\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e''\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ejoin\u003c/span\u003e(prompt_texts)\n\n\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e __name__ == \u003cspan class=\"hljs-string\"\u003e'__main__'\u003c/span\u003e:\n    parser = argparse.\u003cspan class=\"hljs-title class_\"\u003eArgumentParser\u003c/span\u003e(description=\u003cspan class=\"hljs-string\"\u003e'Llama3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ `generate()` APIë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ì˜ˆì¸¡'\u003c/span\u003e)\n    parser.\u003cspan class=\"hljs-title function_\"\u003eadd_argument\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'--repo-id-or-model-path'\u003c/span\u003e, type=str, \u003cspan class=\"hljs-keyword\"\u003edefault\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"meta-llama/Meta-Llama-3-70B-Instruct\"\u003c/span\u003e,\n                        help=\u003cspan class=\"hljs-string\"\u003e'Meta-Llama-3 (ì˜ˆ: `meta-llama/Meta-Llama-3-70B-Instruct`)ë¥¼ ë‹¤ìš´ë¡œë“œí•  Huggingface ì €ì¥ì†Œ ID'\u003c/span\u003e\n                             \u003cspan class=\"hljs-string\"\u003e'ë˜ëŠ” Huggingface ì²´í¬í¬ì¸íŠ¸ í´ë”ì— ëŒ€í•œ ê²½ë¡œ'\u003c/span\u003e)\n    parser.\u003cspan class=\"hljs-title function_\"\u003eadd_argument\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'--prompt'\u003c/span\u003e, type=str, \u003cspan class=\"hljs-keyword\"\u003edefault\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"OpenVINO is\"\u003c/span\u003e,\n                        help=\u003cspan class=\"hljs-string\"\u003e'ì¶”ë¡ í•  í”„ë¡¬í”„íŠ¸'\u003c/span\u003e)\n    parser.\u003cspan class=\"hljs-title function_\"\u003eadd_argument\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'--n-predict'\u003c/span\u003e, type=int, \u003cspan class=\"hljs-keyword\"\u003edefault\u003c/span\u003e=\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e,\n                        help=\u003cspan class=\"hljs-string\"\u003e'ì˜ˆì¸¡í•  ìµœëŒ€ í† í° ìˆ˜'\u003c/span\u003e)\n    parser.\u003cspan class=\"hljs-title function_\"\u003eadd_argument\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'--bit'\u003c/span\u003e, type=str, \u003cspan class=\"hljs-keyword\"\u003edefault\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"4\"\u003c/span\u003e,\n                        help=\u003cspan class=\"hljs-string\"\u003e'4ë¡œ ì„¤ì •í•˜ë©´ 4ë¹„íŠ¸ë¡œ ë¡œë“œí•˜ê±°ë‚˜ off ë˜ëŠ” load_in_low_bit ì˜µì…˜ì€ sym_int4, asym_int4, sym_int5, asym_int5, sym_int8,nf3,nf4, fp4, fp8, fp8_e4m3, fp8_e5m2, fp6, gguf_iq2_xxs, gguf_iq2_xs, gguf_iq1_s, gguf_q4k_m, gguf_q4k_s, fp16, bf16, fp6_k'\u003c/span\u003e)\n\n    args = parser.\u003cspan class=\"hljs-title function_\"\u003eparse_args\u003c/span\u003e()\n    model_path = args.\u003cspan class=\"hljs-property\"\u003erepo_id_or_model_path\u003c/span\u003e\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e args.\u003cspan class=\"hljs-property\"\u003ebit\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e\"4\"\u003c/span\u003e:\n        # \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003eë¹„íŠ¸ë¡œ ëª¨ë¸ ë¡œë“œ,\n        # ëª¨ë¸ì˜ ê´€ë ¨ ë ˆì´ì–´ë¥¼ \u003cspan class=\"hljs-title class_\"\u003eINT4\u003c/span\u003e í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n        # iGPUë¥¼ ì‚¬ìš©í•˜ëŠ” \u003cspan class=\"hljs-title class_\"\u003eWindows\u003c/span\u003e ì‚¬ìš©ìì˜ ê²½ìš°, \u003cspan class=\"hljs-variable constant_\"\u003eLLM\u003c/span\u003eì„ ì‹¤í–‰í•  ë•Œ \u003cspan class=\"hljs-string\"\u003e`cpu_embedding=True`\u003c/span\u003eë¥¼ from_pretrained í•¨ìˆ˜ì—ì„œ ì„¤ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n        # ì´ë ‡ê²Œ í•˜ë©´ ë©”ëª¨ë¦¬ ì§‘ì•½ì ì¸ ì„ë² ë”© ë ˆì´ì–´ê°€ iGPU ëŒ€ì‹  \u003cspan class=\"hljs-variable constant_\"\u003eCPU\u003c/span\u003eë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°ì—ëŠ” ë„ì›€ì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n        model = \u003cspan class=\"hljs-title class_\"\u003eAutoModelForCausalLM\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(model_path,\n                                                    load_in_4bit=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n                                                    optimize_model=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n                                                    trust_remote_code=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n                                                    use_cache=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n    elif args.\u003cspan class=\"hljs-property\"\u003ebit\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e\"off\"\u003c/span\u003e or args.\u003cspan class=\"hljs-property\"\u003ebit\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e\"32\"\u003c/span\u003e:\n        model = \u003cspan class=\"hljs-title class_\"\u003eAutoModelForCausalLM\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(model_path,\n                                                    optimize_model=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n                                                    trust_remote_code=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n                                                    use_cache=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n    \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n        model = \u003cspan class=\"hljs-title class_\"\u003eAutoModelForCausalLM\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(model_path,\n                                                    load_in_low_bit=args.\u003cspan class=\"hljs-property\"\u003ebit\u003c/span\u003e,\n                                                    optimize_model=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n                                                    trust_remote_code=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n                                                    use_cache=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e args.\u003cspan class=\"hljs-property\"\u003ebit\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e\"32\"\u003c/span\u003e:\n        model = model.\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'xpu'\u003c/span\u003e)\n    \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n        model = model.\u003cspan class=\"hljs-title function_\"\u003ehalf\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'xpu'\u003c/span\u003e)\n\n    # í† í¬ë‚˜ì´ì € ë¡œë“œ\n    tokenizer = \u003cspan class=\"hljs-title class_\"\u003eAutoTokenizer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(model_path, trust_remote_code=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\n    # ì—¬ê¸°ì„œ ì¢…ê²°ìëŠ” ë‹¤ìŒì„ ì°¸ì¡°í•©ë‹ˆë‹¤. \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct#transformers-automodelforcausallm\u003c/span\u003e\n    ì¢…ê²°ì = [\n        tokenizer.\u003cspan class=\"hljs-property\"\u003eeos_token_id\u003c/span\u003e,\n        tokenizer.\u003cspan class=\"hljs-title function_\"\u003econvert_tokens_to_ids\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"\u0026#x3C;|eot_id|\u003e\"\u003c/span\u003e),\n    ]\n\n    # ì˜ˆì¸¡ëœ í† í° ìƒì„±\n    \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e torch.\u003cspan class=\"hljs-title function_\"\u003einference_mode\u003c/span\u003e():\n        prompt = \u003cspan class=\"hljs-title function_\"\u003eget_prompt\u003c/span\u003e(args.\u003cspan class=\"hljs-property\"\u003eprompt\u003c/span\u003e, [], system_prompt=\u003cspan class=\"hljs-variable constant_\"\u003eDEFAULT_SYSTEM_PROMPT\u003c/span\u003e)\n        input_ids = tokenizer.\u003cspan class=\"hljs-title function_\"\u003eencode\u003c/span\u003e(prompt, return_tensors=\u003cspan class=\"hljs-string\"\u003e\"pt\"\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'xpu'\u003c/span\u003e)\n        # ipex_llm ëª¨ë¸ì€ ì›Œë°ì—…ì´ í•„ìš”í•˜ë¯€ë¡œ ì¶”ë¡  ì‹œê°„ì„ ì •í™•í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n        output = model.\u003cspan class=\"hljs-title function_\"\u003egenerate\u003c/span\u003e(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)\n\n        # ì¶”ë¡  ì‹œì‘\n        st = time.\u003cspan class=\"hljs-title function_\"\u003etime\u003c/span\u003e()\n        output = model.\u003cspan class=\"hljs-title function_\"\u003egenerate\u003c/span\u003e(input_ids,\n                                eos_token_id=terminators,\n                                max_new_tokens=args.\u003cspan class=\"hljs-property\"\u003en_predict\u003c/span\u003e)\n        torch.\u003cspan class=\"hljs-property\"\u003expu\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003esynchronize\u003c/span\u003e()\n        end = time.\u003cspan class=\"hljs-title function_\"\u003etime\u003c/span\u003e()\n        output = output.\u003cspan class=\"hljs-title function_\"\u003ecpu\u003c/span\u003e()\n        output_str = tokenizer.\u003cspan class=\"hljs-title function_\"\u003edecode\u003c/span\u003e(output[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], skip_special_tokens=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'ì¶”ë¡  ì‹œê°„: {end-st} s, í† í°: {len(output[0])}, t/s:{len(output[0])/(end-st)}'\u003c/span\u003e)\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'-'\u003c/span\u003e*\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'í”„ë¡¬í”„íŠ¸'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'-'\u003c/span\u003e*\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(prompt)\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'-'\u003c/span\u003e*\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'ì¶œë ¥ (skip_special_tokens=False)'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'-'\u003c/span\u003e*\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(output_str)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epython ipex-llm-llama3.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e --repo-id-or-model-path=meta-llama/\u003cspan class=\"hljs-title class_\"\u003eMeta\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eLlama\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e-8B-\u003cspan class=\"hljs-title class_\"\u003eInstruct\u003c/span\u003e --bit=fp16\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_2.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_3.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eDirectMLì€ ë‚®ì€ ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ì§€ì›í•˜ëŠ” ONNX Runtimeì˜ Execution Providerê°€ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ í…ŒìŠ¤íŠ¸ëŠ” ì´ python APIë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epip install onnxruntime-genai --pre\npip install onnxruntime-genai-directml --pre\npip install torch transformers onnx onnxruntime\nconda install conda-\u003cspan class=\"hljs-attr\"\u003eforge\u003c/span\u003e::vs2015_runtime\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eë§ˆì§€ë§‰ ì¤„ì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.\u003c/p\u003e\n\u003cp\u003eë¼ë§ˆ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epython -m onnxruntime_genai.\u003cspan class=\"hljs-property\"\u003emodels\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ebuilder\u003c/span\u003e -m meta-llama/\u003cspan class=\"hljs-title class_\"\u003eMeta\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eLlama\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e-8B-\u003cspan class=\"hljs-title class_\"\u003eInstruct\u003c/span\u003e -o llama-\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e-8B-\u003cspan class=\"hljs-title class_\"\u003eInstruct\u003c/span\u003e-int4-onnx-directml -p int4 -e dml -c ..\\.\u003cspan class=\"hljs-property\"\u003ecache\u003c/span\u003e\\huggingface\\hub\\\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eë³€í™˜ëœ ëª¨ë¸ì´ ğŸ¤— í—ˆë¸Œì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\u003c/p\u003e\n\u003cp\u003eë‹¤ìŒì€ í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ genai-llama3.pyì…ë‹ˆë‹¤.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e time\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e argparse\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e onnxruntime_genai \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e og\n\n\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e __name__ == \u003cspan class=\"hljs-string\"\u003e'__main__'\u003c/span\u003e:\n    parser = argparse.ArgumentParser(description=\u003cspan class=\"hljs-string\"\u003e'Predict Tokens using onnxruntime_genai'\u003c/span\u003e)\n    parser.add_argument(\u003cspan class=\"hljs-string\"\u003e'--model-path'\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003etype\u003c/span\u003e=\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e, default=\u003cspan class=\"hljs-string\"\u003e\"llama-3-8B-Instruct-int4-onnx-directml\"\u003c/span\u003e,\n                        \u003cspan class=\"hljs-built_in\"\u003ehelp\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e'model path'\u003c/span\u003e)\n    parser.add_argument(\u003cspan class=\"hljs-string\"\u003e'--prompt'\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003etype\u003c/span\u003e=\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e, default=\u003cspan class=\"hljs-string\"\u003e\"OpenVINO is\"\u003c/span\u003e,\n                        \u003cspan class=\"hljs-built_in\"\u003ehelp\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e'Prompt to infer'\u003c/span\u003e)\n    parser.add_argument(\n        \u003cspan class=\"hljs-string\"\u003e'--max-length'\u003c/span\u003e,\n        \u003cspan class=\"hljs-built_in\"\u003etype\u003c/span\u003e=\u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e,\n        default=\u003cspan class=\"hljs-number\"\u003e143\u003c/span\u003e,\n        \u003cspan class=\"hljs-built_in\"\u003ehelp\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e'max lengths'\u003c/span\u003e\n    )\n    args = parser.parse_args()\n\n    model = og.Model(args.model_path)\n    tokenizer = og.Tokenizer(model)\n    \n    \u003cspan class=\"hljs-comment\"\u003e# Set the max length to something sensible by default,\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e# since otherwise it will be set to the entire context length\u003c/span\u003e\n    search_options = {}\n    search_options[\u003cspan class=\"hljs-string\"\u003e'max_length'\u003c/span\u003e] = args.max_length\n\n    chat_template = \u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;|user|\u003e\\n{input} \u0026#x3C;|end|\u003e\\n\u0026#x3C;|assistant|\u003e'\u003c/span\u003e\n\n    text = args.prompt\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e text:\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"ì˜¤ë¥˜, ì…ë ¥ì´ ë¹„ì–´ ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"\u003c/span\u003e)\n        exit\n\n    prompt = \u003cspan class=\"hljs-string\"\u003ef'\u003cspan class=\"hljs-subst\"\u003e{chat_template.\u003cspan class=\"hljs-built_in\"\u003eformat\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003einput\u003c/span\u003e=text)}\u003c/span\u003e'\u003c/span\u003e\n\n    input_tokens = tokenizer.encode(prompt)\n\n    params = og.GeneratorParams(model)\n    params.set_search_options(**search_options)\n    params.input_ids = input_tokens\n\n    st =  time.time()\n    output = model.generate(params)\n    out_txt = tokenizer.decode(output[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\n\n    sec = time.time() - st\n    cnt = \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(output[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\n\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"ìƒì„± ê²°ê³¼:\"\u003c/span\u003e, out_txt)\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef'ì¶”ë¡  ì‹œê°„: \u003cspan class=\"hljs-subst\"\u003e{sec}\u003c/span\u003e ì´ˆ, í† í° ìˆ˜: \u003cspan class=\"hljs-subst\"\u003e{cnt}\u003c/span\u003e, ì´ˆë‹¹ í† í° ìˆ˜:\u003cspan class=\"hljs-subst\"\u003e{cnt/sec}\u003c/span\u003e'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_4.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_5.png\" alt=\"ì´ë¯¸ì§€\"\u003e\u003c/p\u003e\n\u003cp\u003eğŸ¤—Transformers + IPEX-LLMê³¼ ë¹„êµí•´ ë³¼ ìˆ˜ ìˆì–´ìš”.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epython ipex-llm-llama3.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e --repo-id-or-model-path=meta-llama/\u003cspan class=\"hljs-title class_\"\u003eMeta\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eLlama\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e-8B-\u003cspan class=\"hljs-title class_\"\u003eInstruct\u003c/span\u003e --bit=\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_6.png\" alt=\"ì´ë¯¸ì§€\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_7.png\" alt=\"ì´ë¯¸ì§€1\"\u003e\u003c/p\u003e\n\u003cp\u003eê²°ê³¼ë¥¼ í•¨ê»˜ ì‚´í´ë´…ì‹œë‹¤.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM_8.png\" alt=\"ì´ë¯¸ì§€2\"\u003e\u003c/p\u003e\n\u003cp\u003eì•ìœ¼ë¡œ ì–´ë–»ê²Œ ì´ëŸ¬í•œ í¥ë¯¸ë¡œìš´ ì°¨ì´ê°€ ìƒê²¼ëŠ”ì§€ì— ëŒ€í•œ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì„ íƒêµ¬í•  ê²ƒì…ë‹ˆë‹¤.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-Llama-3inferencesonIntelCoreUltra5DirectMLandONNXvsIPEX-LLM"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>