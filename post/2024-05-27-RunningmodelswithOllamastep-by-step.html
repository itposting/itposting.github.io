<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>Ollama를 사용하여 모델 실행하기 단계별 안내 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-05-27-RunningmodelswithOllamastep-by-step" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="Ollama를 사용하여 모델 실행하기 단계별 안내 | itposting" data-gatsby-head="true"/><meta property="og:title" content="Ollama를 사용하여 모델 실행하기 단계별 안내 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-05-27-RunningmodelswithOllamastep-by-step" data-gatsby-head="true"/><meta name="twitter:title" content="Ollama를 사용하여 모델 실행하기 단계별 안내 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-27 14:51" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">Ollama를 사용하여 모델 실행하기 단계별 안내</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="Ollama를 사용하여 모델 실행하기 단계별 안내" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 27, 2024</span><span class="posts_reading_time__f7YPP">10<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-27-RunningmodelswithOllamastep-by-step&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>LLM을 빠르게 테스트할 수 있는 방법을 찾고 계신가요? 전체 인프라를 설정할 필요 없이 테스트할 수 있는 방법이 있다면 정말 훌륭하죠. 이 짧은 기사에서 우리가 할 일이 바로 그거에요.</p>
<p><img src="/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png" alt="이미지"></p>
<p>Ollama에 관해 경험이 있는 경우에는 특정 단락으로 이동해도 됩니다. 이 기사에서 찾을 수 있는 내용은 다음과 같아요:</p>
<ul>
<li>Ollama가 무엇인가요?</li>
<li>Windows에 Ollama 설치하기.</li>
<li>Ollama [cmd] 실행하기.</li>
<li>로컬로 모델 다운로드하기.</li>
<li>다양한 용도에 맞는 다른 모델.</li>
<li>모델 실행하기 [cmd].</li>
<li>CPU에 친화적인 양자화된 모델.</li>
<li>다른 소스에서 모델 통합하기.</li>
<li>Ollama-파워드 (Python) 앱으로 개발자들의 삶을 더 쉽게 만들기.</li>
<li>요약.</li>
</ul>
<div class="content-ad"></div>
<h1>1. Ollama이란?</h1>
<p>Ollama는 오픈 소스 코드로, 로컬에서 또는 본인의 서버에서 언어 모델과의 원활한 통합을 가능하게 하는 사용 준비 도구입니다. 이를 통해 상업용 API의 유료 버전을 사용하지 않아도 되므로, 특히 이제 Meta가 Llama2 모델을 상용으로 사용 가능하게 한 것을 고려하면, 자신의 데이터셋에서 추가 학습에 적합합니다.</p>
<p>➡️ GitHub 저장소: <a href="https://github.com/ollama/ollama" rel="nofollow" target="_blank">https://github.com/ollama/ollama</a></p>
<p>➡️ Ollama 공식 웹페이지: <a href="https://ollama.com" rel="nofollow" target="_blank">https://ollama.com</a></p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_1.png" alt="이미지"></p>
<h1>2. Windows에서 Ollama 설치하기</h1>
<p>Ollama는 Windows, Mac 및 Linux에서도 원활하게 작동합니다. 이 간단한 자습서는 특히 Windows 10용 설치 단계를 안내합니다. 설치 후 프로그램은 약 384MB를 차지합니다. 그러나 다운로드한 모델이 가벼운 것은 아닐 수 있습니다.</p>
<p>만약 도커 컨테이너에서 Ollama를 실행하길 원한다면, 아래 설명을 건너뛰고</p>
<p>감십시오.</p>
<div class="content-ad"></div>
<p>➡️ <a href="https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image" rel="nofollow" target="_blank">https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image</a></p>
<p><img src="/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_2.png" alt="Running Models with Ollama"></p>
<p>➡️ Ollama 홈페이지로 이동하여 .exe 파일을 다운로드하세요: <a href="https://ollama.com" rel="nofollow" target="_blank">https://ollama.com</a></p>
<p><img src="/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_3.png" alt="Running Models with Ollama"></p>
<div class="content-ad"></div>
<p>Ollama를 다운로드하고 Windows에 설치하세요. 보통 다음 경로에 위치한 기본 모델 저장 경로를 사용할 수 있습니다:</p>
<pre><code class="hljs language-js"><span class="hljs-attr">C</span>:\<span class="hljs-title class_">Users</span>\your_user\.<span class="hljs-property">ollama</span>
</code></pre>
<p>그러나 C: 파티션에 공간이 제한적이라면, 대안 디렉토리로 전환하는 것이 권장됩니다. D:\와 같은 다른 파티션이 있는 경우, 간단하게:</p>
<ul>
<li>데스크탑의 컴퓨터 아이콘을 마우스 오른쪽 클릭합니다.</li>
<li>속성을 선택한 후 "고급 시스템 설정"으로 이동합니다.</li>
<li>환경 변수를 클릭합니다.</li>
<li>...을 위한 사용자 변수에서 모델을 저장할 디렉토리의 절대 경로를 삽입하십시오. 예를 들면:</li>
</ul>
<div class="content-ad"></div>
<pre><code class="hljs language-js">변수: <span class="hljs-variable constant_">OLLAMA_MODELS</span>
값: <span class="hljs-attr">D</span>:\your_directory\models
</code></pre>
<p>OLLAMA_MODELS 변수의 이름을 변경하지 마십시오. 이 변수는 Ollama가 정확히 아래와 같이 검색할 것입니다.</p>
<p>Windows의 하단 표시줄에 Ollama 아이콘이 나타납니다. 프로그램이 시작되지 않으면 Windows 프로그램에서 찾아서 거기서 시작하십시오.</p>
<img src="/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_4.png">
<div class="content-ad"></div>
<p>이제 Ollama를 실행하고 모델을 다운로드할 준비가 되었어요 :)</p>
<h1>3. Ollama 실행하기 [cmd]</h1>
<p><img src="/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_5.png" alt="image"></p>
<p>Ollama를 설정하고 나면 윈도우에서 cmd(명령줄)를 열고 로컬로 일부 모델을 다운로드할 수 있어요.</p>
<div class="content-ad"></div>
<p>Ollama 로컬 대시보드를 사용하려면 웹 브라우저에서 다음 URL을 입력하세요:</p>
<pre><code class="hljs language-js"><span class="hljs-attr">http</span>:<span class="hljs-comment">//localhost:11434/api/</span>
</code></pre>
<p>Ollama를 실행하는 것은 그렇게 어렵지 않습니다. 나중에 CMD 및 Python 코드를 통해 어떻게 활용하는지 알아보겠습니다.</p>
<p>중요한 몇 가지 명령어:</p>
<div class="content-ad"></div>
<p>로컬로 사용 가능한 모델을 확인하려면 다음을 cmd에 입력하세요:</p>
<pre><code class="hljs language-js">ollama list
</code></pre>
<p>특정 모델에 해당하는 SHA 파일을 확인하려면 cmd에 입력하세요 (예: llama2:7b 모델 확인을 위한 예시):</p>
<pre><code class="hljs language-js">ollama show --modelfile <span class="hljs-attr">llama2</span>:7b
</code></pre>
<div class="content-ad"></div>
<p>모델을 제거하려면:</p>
<pre><code class="hljs language-js">ollama rm <span class="hljs-attr">llama2</span>:7b
</code></pre>
<p>모델을 서버에 올리려면:</p>
<pre><code class="hljs language-js">ollama serve
</code></pre>
<div class="content-ad"></div>
<h1>4. 모델을 로컬로 다운로드하기</h1>
<p>웹사이트 ➡️ <a href="https://ollama.com/library" rel="nofollow" target="_blank">https://ollama.com/library</a> 에서는 여러 다양한 파라미터 크기로 제공되는 다수의 모델을 다운로드할 수 있습니다.</p>
<p>로컬로 모델을 다운로드하기 전에, 해당 모델을 로딩할 충분한 메모리를 가지고 있는지 확인해주세요. 테스트할 때는 애플리케이션에 통합하기에 적합한 작은 모델인 '7B'로 레이블이 지정된 모델을 사용하는 것이 좋습니다.</p>
<p>⚠️ 부드러운 모델 작동을 위해 적어도 하나의 GPU를 보유하는 것이 강력하게 권장됩니다.</p>
<div class="content-ad"></div>
<p>아래에는 내가 테스트하고 추천하는 여러 모델이 있습니다. 명령을 복사하여 명령 프롬프트에 붙여넣어 지정된 모델을 로컬로 가져올 수 있습니다.</p>
<p>👉Meta에서의 Llama2 모델</p>
<p>대화 시나리오를 위해 최적화된 생성 텍스트 모델 세트입니다. Ollama의 많은 모델과 마찬가지로 Llama2는 다양한 구성으로 제공됩니다:</p>
<p><img src="/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_6.png" alt="image"></p>
<div class="content-ad"></div>
<p>아래는 해당 모델을 가져오는 몇 가지 예시입니다:</p>
<p>표준 모델:</p>
<pre><code class="hljs language-js">ollama pull llama2
</code></pre>
<p>검열되지 않은 버전:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">ollama pull llama2-비겁하지 않은:7b
</code></pre>
<p>채팅 7B 모델:</p>
<pre><code class="hljs language-js">ollama pull <span class="hljs-attr">llama2</span>:7b-채팅
</code></pre>
<p>➡️ 더 읽기: <a href="https://llama.meta.com/llama2" rel="nofollow" target="_blank">https://llama.meta.com/llama2</a></p>
<div class="content-ad"></div>
<p>👉 구글의 젬마</p>
<p>주요 7B 크기 모델과 유사한 견고한 성능을 제공하는 오픈 소스 모델입니다.</p>
<pre><code class="hljs language-js">ollama pull <span class="hljs-attr">gemma</span>:7b
</code></pre>
<p>➡️ 자세히 보기: <a href="https://blog.google/technology/developers/gemma-open-models/" rel="nofollow" target="_blank">https://blog.google/technology/developers/gemma-open-models/</a></p>
<div class="content-ad"></div>
<p>👉 Haotian Liu 등의 LLava.</p>
<p>이미지에서 텍스트 설명을 다루는 데 뛰어나며 시각 및 언어 모델 모두에 대한 견고한 지원을 제공하는 멀티모달 모델입니다.</p>
<pre><code class="hljs language-js">ollama pull llava
</code></pre>
<p>➡️ 자세히 알아보기: <a href="https://llava-vl.github.io/" rel="nofollow" target="_blank">https://llava-vl.github.io/</a></p>
<div class="content-ad"></div>
<ol start="5">
<li>서로 다른 목적을 위한 다양한 모델</li>
</ol>
<p><img src="/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_7.png" alt="이미지"></p>
<p>일부 모델은 특정 데이터셋에서 훈련되어 코드 완성, 대화 또는 이미지에서 텍스트로 변환과 같은 특정 작업에 더 적합합니다. Ollama에서는 다양한 목적을 위해 설계된 모델을 찾을 수 있습니다.</p>
<p>첫 번째 그룹은 대화, 텍스트 완성, 요약 등을 용이하게 하는 데 초점을 맞춘 모델을 포함하고 있습니다. Gemma, Llama2, Falcon 또는 OpenChat과 같은 모델이 포함됩니다.</p>
<div class="content-ad"></div>
<p>일부 예시:</p>
<ul>
<li>
<p><a href="https://ollama.com/library/falcon" rel="nofollow" target="_blank">Falcon</a></p>
</li>
<li>
<p><a href="https://ollama.com/library/gemma" rel="nofollow" target="_blank">Gemma</a></p>
</li>
<li>
<p><a href="https://ollama.com/library/openchat" rel="nofollow" target="_blank">Openchat</a></p>
</li>
</ul>
<div class="content-ad"></div>
<p>다음 그룹은 대화를 나누거나 챗봇 역할을 하는 다중 모달 모델과 이미지 설명(시각 모델), 텍스트 요약, 질문-답변(Q/A) 애플리케이션을 구동할 수 있는 모델들로 구성됩니다.</p>
<p>일부 예시:</p>
<p>➡️ <a href="https://ollama.com/library/llava" rel="nofollow" target="_blank">https://ollama.com/library/llava</a></p>
<p>➡️ <a href="https://ollama.com/library/bakllava" rel="nofollow" target="_blank">https://ollama.com/library/bakllava</a></p>
<div class="content-ad"></div>
<p>마지막으로, 매우 전문화된 그룹은 Ollama에서 이용 가능한 모델을 활용하여 개발자의 작업을 지원합니다. 코델라마, 돌핀-미스트랄, 돌핀-믹스트랄(코딩 작업에 능숙한 Mixtral 전문가 모델을 기반으로 세밀하게 조정된 모델)과 같은 모델들이 있으며, 계속해서 크리에이터들이 추가하고 있습니다.</p>
<p>몇 가지 예시:</p>
<p>➡️ <a href="https://ollama.com/library/codellama" rel="nofollow" target="_blank">https://ollama.com/library/codellama</a></p>
<p>➡️ <a href="https://ollama.com/library/dolphin-mistral" rel="nofollow" target="_blank">https://ollama.com/library/dolphin-mistral</a></p>
<div class="content-ad"></div>
<p>➡️ <a href="https://ollama.com/library/dolphin-mixtral" rel="nofollow" target="_blank">https://ollama.com/library/dolphin-mixtral</a></p>
<h1>6. 모델 실행하기 [cmd]</h1>
<p>다운로드한 모델을 실행하려면, ollama run 모델이름:파라미터 "당신의 프롬프트"를 입력하세요. 예를 들어:</p>
<pre><code class="hljs language-js">ollama run <span class="hljs-attr">llama2</span>:7b <span class="hljs-string">"당신의 프롬프트"</span>
</code></pre>
<div class="content-ad"></div>
<p>다중 모달 모델을 사용하면 기본 프롬프트를 벗어난 파일, 로컬 이미지 경로 등을 포함할 수 있어 더 많은 기능을 확장할 수 있습니다.</p>
<h1>6. CPU 친화적 양자화 모델</h1>
<p>양자화는 모델의 정밀도를 유지하는 비용을 줄이는 것으로 관련 비용을 줄이는 것입니다. 이 과정 뒤에 숨은 직관력을 구축하는 데 도움이 되는 이 크고 훌륭한 기사에서 자세한 설명을 찾아볼 수 있습니다:</p>
<p>📃 양자화 LLMs란 무엇인가? (Miguel Carreira Neves의 글):</p>
<div class="content-ad"></div>
<p>➡️ <a href="https://www.tensorops.ai/post/what-are-quantized-llms" rel="nofollow" target="_blank">https://www.tensorops.ai/post/what-are-quantized-llms</a></p>
<p>추가 자료:</p>
<p>📃 Extreme Compression of Large Language Models via Additive Quantization:</p>
<p>➡️ <a href="https://arxiv.org/html/2401.06118v2" rel="nofollow" target="_blank">https://arxiv.org/html/2401.06118v2</a></p>
<div class="content-ad"></div>
<p>📃 SmoothQuant: 대형 언어 모델을 위한 정확하고 효율적인 사후 훈련 양자화:</p>
<p>➡️ <a href="https://arxiv.org/pdf/2211.10438.pdf" rel="nofollow" target="_blank">논문 링크</a></p>
<p>📃 BiLLM: LLMs를 위한 사후 훈련 양자화 한계 돌파:</p>
<p>➡️ <a href="https://arxiv.org/pdf/2402.04291.pdf" rel="nofollow" target="_blank">논문 링크</a></p>
<div class="content-ad"></div>
<p>간단하게 말하면, 양자화는 가중치 정밀도를 조정하여 모델 크기를 줄이고 중요한 정확도 하락 없이 성능을 쉽게 감소시킬 수 있는 하드웨어에서 실행할 수 있게 해줍니다.</p>
<p>이 글과 함께 제공된 이미지를 통해 양자화 후에 모델이 원래 버전보다 상당히 적은 공간을 차지하는 것을 확인할 수 있습니다:</p>
<p><img src="/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_8.png" alt="Quantized Models"></p>
<p>Ollama는 양자화된 모델을 지원하여 별도로 처리하는 부담을 덜어줍니다.</p>
<div class="content-ad"></div>
<h1>7. 다른 소스에서 모델 통합하기</h1>
<p><img src="/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_9.png" alt="Running Models With Ollama Step-by-Step"></p>
<p>Ollama의 모델은 다양성을 제공하지만 현재 모든 모델에 액세스할 수 있는 것은 아닙니다. 그러나 로컬에 직접 모델을 통합하는 것은 간단한 프로세스입니다. 새로운 모델을 지역 Ollama에 통합하는 방법을 알아봅시다.</p>
<p>The Bloke의 HuggingFace 계정에서 많은 양자화된 모델을 사용할 수 있습니다. 의학 논문을 위해서 우리는 편리하게 medicine-chat-GGUF 모델을 선택할 수 있습니다:</p>
<div class="content-ad"></div>
<p>➡️ <a href="https://huggingface.co/TheBloke/medicine-chat-GGUF" rel="nofollow" target="_blank">https://huggingface.co/TheBloke/medicine-chat-GGUF</a></p>
<p>해당 링크를 열고 파일 및 버전을 클릭하세요.</p>
<p><img src="/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_10.png" alt="Files and versions"></p>
<p>Ollama 모델에 포함하고 싶은 모델을 다운로드하세요:</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_11.png">
<p>Modelfile이라는 빈 파일을 생성하고 아래 지정된 데이터를 삽입하세요 (저장된 모델의 절대 경로로 경로를 대체하십시오). 이 예제는 기본적이며, 모델의 온도, 시스템 메시지 등과 같은 여러 옵션을 포함하여 확장될 수 있습니다. 필요한 경우 파일에서 '#'를 제거하여 해당 옵션을 활성화하세요.</p>
<pre><code class="hljs language-js"><span class="hljs-variable constant_">FROM</span> <span class="hljs-attr">D</span>:\...\medicine-chat.<span class="hljs-property">Q4_0</span>.<span class="hljs-property">gguf</span>
# <span class="hljs-variable constant_">PARAMETER</span> 온도 <span class="hljs-number">0.6</span>
# <span class="hljs-variable constant_">SYSTEM</span> <span class="hljs-string">""</span><span class="hljs-string">"도움이 되는 의학 조수입니다."</span><span class="hljs-string">""</span>
</code></pre>
<p>Modelfile을 저장한 후, cmd에 다음을 입력하세요:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-bash">ollama create 모델_이름 -f 모델_파일
</code></pre>
<h1>9. Ollama를 활용한 (Python) 앱으로 개발자의 삶을 더 쉽게 만들기</h1>
<p>백그라운드에서 실행되는 Ollama는 일반적인 REST API와 같이 접근할 수 있습니다. 따라서 requests와 같은 라이브러리 또는 조금 더 발전된 FastAPI, Flask 또는 Django와 같은 프레임워크를 사용하여 응용 프로그램에 쉽게 통합할 수 있습니다.</p>
<p>Ollama python 패키지를 쉽게 pip를 통해 설치하세요.</p>
<div class="content-ad"></div>
<p>⬆️ <a href="https://pypi.org/project/ollama/0.1.3" rel="nofollow" target="_blank">https://pypi.org/project/ollama/0.1.3</a>:</p>
<pre><code class="hljs language-js">pip install ollama
</code></pre>
<p>Python 코드를 통해 임베딩을 생성하는 방법:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> ollama

embedding = ollama.<span class="hljs-title function_">embeddings</span>(model=<span class="hljs-string">"llama2:7b"</span>, prompt=<span class="hljs-string">"Hello Ollama!"</span>)
</code></pre>
<div class="content-ad"></div>
<p>간단히 CURL을 사용하여:</p>
<pre><code class="hljs language-js">curl <span class="hljs-attr">http</span>:<span class="hljs-comment">//localhost:11434/api/embeddings -d '{</span>
  <span class="hljs-string">"model"</span>: <span class="hljs-string">"llama2:7b"</span>,
  <span class="hljs-string">"prompt"</span>: <span class="hljs-string">"Here is an article about llamas..."</span>
}<span class="hljs-string">'
</span></code></pre>
<p>Ollama 엔드포인트에 대해 더 알아보려면 다음 링크를 방문해주세요:</p>
<p>➡️ <a href="https://github.com/ollama/ollama/blob/main/docs/api.md" rel="nofollow" target="_blank">https://github.com/ollama/ollama/blob/main/docs/api.md</a></p>
<div class="content-ad"></div>
<p>Ollama가 Langchain 프레임워크에 원활하게 통합되어 개발 노력을 최적화하고 기술 측면의 작업을 더욱 간편하게 만들었습니다:</p>
<p>➡️ <a href="https://python.langchain.com/docs/integrations/llms/ollama" rel="nofollow" target="_blank">https://python.langchain.com/docs/integrations/llms/ollama</a></p>
<p>임베딩을 만드는 간단함을 감상해보세요:</p>
<pre><code class="hljs language-js"># pip install langchain_community
<span class="hljs-keyword">from</span> langchain_community.<span class="hljs-property">embeddings</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">OllamaEmbeddings</span>


embed = <span class="hljs-title class_">OllamaEmbeddings</span>(model=<span class="hljs-string">"llama2:7b"</span>)
embedding = embed.<span class="hljs-title function_">embed_query</span>(<span class="hljs-string">"Hello Ollama!"</span>)
</code></pre>
<div class="content-ad"></div>
<h1>10. 요약</h1>
<p>본 기사는 당신을 Ollama를 사용하여 모델을 실행하는 과정을 단계별로 안내하여, 전체 인프라 구성 없이 LLM을 테스트할 수 있는 원활한 방법을 제공합니다.</p>
<p>올라마는 오픈 소스 도구로, Meta의 Llama2 모델을 무료로 사용할 수 있게 해주는 로컬 또는 서버 기반의 언어 모델 통합을 용이하게 합니다. 윈도우에서의 설치 과정과 명령줄을 통해 Ollama를 실행하는 방법에 대해 설명되어 있습니다.</p>
<p>이 기사에서는 모델 다운로드, 특정 작업을 위한 다양한 모델 옵션, 다양한 명령어를 사용하여 모델 실행, CPU 친화적인 양자화된 모델, 그리고 외부 모델 통합에 대해 탐구합니다. 또한, 개발자들을 위해 Ollama를 활용한 파이썬 애플리케이션을 강조하고 있습니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Ollama를 사용하여 모델 실행하기 단계별 안내","description":"","date":"2024-05-27 14:51","slug":"2024-05-27-RunningmodelswithOllamastep-by-step","content":"\n\nLLM을 빠르게 테스트할 수 있는 방법을 찾고 계신가요? 전체 인프라를 설정할 필요 없이 테스트할 수 있는 방법이 있다면 정말 훌륭하죠. 이 짧은 기사에서 우리가 할 일이 바로 그거에요.\n\n![이미지](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png)\n\nOllama에 관해 경험이 있는 경우에는 특정 단락으로 이동해도 됩니다. 이 기사에서 찾을 수 있는 내용은 다음과 같아요:\n\n- Ollama가 무엇인가요?\n- Windows에 Ollama 설치하기.\n- Ollama [cmd] 실행하기.\n- 로컬로 모델 다운로드하기.\n- 다양한 용도에 맞는 다른 모델.\n- 모델 실행하기 [cmd].\n- CPU에 친화적인 양자화된 모델.\n- 다른 소스에서 모델 통합하기.\n- Ollama-파워드 (Python) 앱으로 개발자들의 삶을 더 쉽게 만들기.\n- 요약.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 1. Ollama이란?\n\nOllama는 오픈 소스 코드로, 로컬에서 또는 본인의 서버에서 언어 모델과의 원활한 통합을 가능하게 하는 사용 준비 도구입니다. 이를 통해 상업용 API의 유료 버전을 사용하지 않아도 되므로, 특히 이제 Meta가 Llama2 모델을 상용으로 사용 가능하게 한 것을 고려하면, 자신의 데이터셋에서 추가 학습에 적합합니다.\n\n➡️ GitHub 저장소: https://github.com/ollama/ollama\n\n➡️ Ollama 공식 웹페이지: https://ollama.com\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_1.png)\n\n# 2. Windows에서 Ollama 설치하기\n\nOllama는 Windows, Mac 및 Linux에서도 원활하게 작동합니다. 이 간단한 자습서는 특히 Windows 10용 설치 단계를 안내합니다. 설치 후 프로그램은 약 384MB를 차지합니다. 그러나 다운로드한 모델이 가벼운 것은 아닐 수 있습니다.\n\n만약 도커 컨테이너에서 Ollama를 실행하길 원한다면, 아래 설명을 건너뛰고 \n\n감십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n➡️ https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image\n\n![Running Models with Ollama](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_2.png)\n\n➡️ Ollama 홈페이지로 이동하여 .exe 파일을 다운로드하세요: https://ollama.com\n\n![Running Models with Ollama](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nOllama를 다운로드하고 Windows에 설치하세요. 보통 다음 경로에 위치한 기본 모델 저장 경로를 사용할 수 있습니다:\n\n```js\nC:\\Users\\your_user\\.ollama\n```\n\n그러나 C: 파티션에 공간이 제한적이라면, 대안 디렉토리로 전환하는 것이 권장됩니다. D:\\와 같은 다른 파티션이 있는 경우, 간단하게:\n\n- 데스크탑의 컴퓨터 아이콘을 마우스 오른쪽 클릭합니다.\n- 속성을 선택한 후 \"고급 시스템 설정\"으로 이동합니다.\n- 환경 변수를 클릭합니다.\n- ...을 위한 사용자 변수에서 모델을 저장할 디렉토리의 절대 경로를 삽입하십시오. 예를 들면:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n변수: OLLAMA_MODELS\n값: D:\\your_directory\\models\n```\n\nOLLAMA_MODELS 변수의 이름을 변경하지 마십시오. 이 변수는 Ollama가 정확히 아래와 같이 검색할 것입니다.\n\nWindows의 하단 표시줄에 Ollama 아이콘이 나타납니다. 프로그램이 시작되지 않으면 Windows 프로그램에서 찾아서 거기서 시작하십시오.\n\n\u003cimg src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_4.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 Ollama를 실행하고 모델을 다운로드할 준비가 되었어요 :)\n\n# 3. Ollama 실행하기 [cmd]\n\n![image](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_5.png)\n\nOllama를 설정하고 나면 윈도우에서 cmd(명령줄)를 열고 로컬로 일부 모델을 다운로드할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nOllama 로컬 대시보드를 사용하려면 웹 브라우저에서 다음 URL을 입력하세요:\n\n```js\nhttp://localhost:11434/api/\n```\n\nOllama를 실행하는 것은 그렇게 어렵지 않습니다. 나중에 CMD 및 Python 코드를 통해 어떻게 활용하는지 알아보겠습니다.\n\n중요한 몇 가지 명령어:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로컬로 사용 가능한 모델을 확인하려면 다음을 cmd에 입력하세요:\n\n```js\nollama list\n```\n\n특정 모델에 해당하는 SHA 파일을 확인하려면 cmd에 입력하세요 (예: llama2:7b 모델 확인을 위한 예시):\n\n```js\nollama show --modelfile llama2:7b\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델을 제거하려면:\n\n```js\nollama rm llama2:7b\n```\n\n모델을 서버에 올리려면:\n\n```js\nollama serve\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 4. 모델을 로컬로 다운로드하기\n\n웹사이트 ➡️ https://ollama.com/library 에서는 여러 다양한 파라미터 크기로 제공되는 다수의 모델을 다운로드할 수 있습니다.\n\n로컬로 모델을 다운로드하기 전에, 해당 모델을 로딩할 충분한 메모리를 가지고 있는지 확인해주세요. 테스트할 때는 애플리케이션에 통합하기에 적합한 작은 모델인 '7B'로 레이블이 지정된 모델을 사용하는 것이 좋습니다.\n\n⚠️ 부드러운 모델 작동을 위해 적어도 하나의 GPU를 보유하는 것이 강력하게 권장됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래에는 내가 테스트하고 추천하는 여러 모델이 있습니다. 명령을 복사하여 명령 프롬프트에 붙여넣어 지정된 모델을 로컬로 가져올 수 있습니다.\n\n👉Meta에서의 Llama2 모델\n\n대화 시나리오를 위해 최적화된 생성 텍스트 모델 세트입니다. Ollama의 많은 모델과 마찬가지로 Llama2는 다양한 구성으로 제공됩니다:\n\n![image](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 해당 모델을 가져오는 몇 가지 예시입니다:\n\n표준 모델:\n\n```js\nollama pull llama2\n```\n\n검열되지 않은 버전:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nollama pull llama2-비겁하지 않은:7b\n```\n\n채팅 7B 모델:\n\n```js\nollama pull llama2:7b-채팅\n```\n\n➡️ 더 읽기: https://llama.meta.com/llama2\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n👉 구글의 젬마\n\n주요 7B 크기 모델과 유사한 견고한 성능을 제공하는 오픈 소스 모델입니다.\n\n```js\nollama pull gemma:7b\n```\n\n➡️ 자세히 보기: https://blog.google/technology/developers/gemma-open-models/\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n👉 Haotian Liu 등의 LLava.\n\n이미지에서 텍스트 설명을 다루는 데 뛰어나며 시각 및 언어 모델 모두에 대한 견고한 지원을 제공하는 멀티모달 모델입니다.\n\n```js\nollama pull llava\n```\n\n➡️ 자세히 알아보기: https://llava-vl.github.io/\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5. 서로 다른 목적을 위한 다양한 모델\n\n![이미지](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_7.png)\n\n일부 모델은 특정 데이터셋에서 훈련되어 코드 완성, 대화 또는 이미지에서 텍스트로 변환과 같은 특정 작업에 더 적합합니다. Ollama에서는 다양한 목적을 위해 설계된 모델을 찾을 수 있습니다.\n\n첫 번째 그룹은 대화, 텍스트 완성, 요약 등을 용이하게 하는 데 초점을 맞춘 모델을 포함하고 있습니다. Gemma, Llama2, Falcon 또는 OpenChat과 같은 모델이 포함됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일부 예시:\n\n- [Falcon](https://ollama.com/library/falcon)\n\n- [Gemma](https://ollama.com/library/gemma)\n\n- [Openchat](https://ollama.com/library/openchat)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 그룹은 대화를 나누거나 챗봇 역할을 하는 다중 모달 모델과 이미지 설명(시각 모델), 텍스트 요약, 질문-답변(Q/A) 애플리케이션을 구동할 수 있는 모델들로 구성됩니다.\n\n일부 예시:\n\n➡️ https://ollama.com/library/llava\n\n➡️ https://ollama.com/library/bakllava\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, 매우 전문화된 그룹은 Ollama에서 이용 가능한 모델을 활용하여 개발자의 작업을 지원합니다. 코델라마, 돌핀-미스트랄, 돌핀-믹스트랄(코딩 작업에 능숙한 Mixtral 전문가 모델을 기반으로 세밀하게 조정된 모델)과 같은 모델들이 있으며, 계속해서 크리에이터들이 추가하고 있습니다.\n\n몇 가지 예시:\n\n➡️ https://ollama.com/library/codellama\n\n➡️ https://ollama.com/library/dolphin-mistral\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n➡️ https://ollama.com/library/dolphin-mixtral\n\n# 6. 모델 실행하기 [cmd]\n\n다운로드한 모델을 실행하려면, ollama run 모델이름:파라미터 \"당신의 프롬프트\"를 입력하세요. 예를 들어:\n\n```js\nollama run llama2:7b \"당신의 프롬프트\"\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다중 모달 모델을 사용하면 기본 프롬프트를 벗어난 파일, 로컬 이미지 경로 등을 포함할 수 있어 더 많은 기능을 확장할 수 있습니다.\n\n# 6. CPU 친화적 양자화 모델\n\n양자화는 모델의 정밀도를 유지하는 비용을 줄이는 것으로 관련 비용을 줄이는 것입니다. 이 과정 뒤에 숨은 직관력을 구축하는 데 도움이 되는 이 크고 훌륭한 기사에서 자세한 설명을 찾아볼 수 있습니다:\n\n📃 양자화 LLMs란 무엇인가? (Miguel Carreira Neves의 글):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n➡️ https://www.tensorops.ai/post/what-are-quantized-llms\n\n추가 자료:\n\n📃 Extreme Compression of Large Language Models via Additive Quantization:\n\n➡️ https://arxiv.org/html/2401.06118v2\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n📃 SmoothQuant: 대형 언어 모델을 위한 정확하고 효율적인 사후 훈련 양자화:\n\n➡️ [논문 링크](https://arxiv.org/pdf/2211.10438.pdf)\n\n📃 BiLLM: LLMs를 위한 사후 훈련 양자화 한계 돌파:\n\n➡️ [논문 링크](https://arxiv.org/pdf/2402.04291.pdf)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단하게 말하면, 양자화는 가중치 정밀도를 조정하여 모델 크기를 줄이고 중요한 정확도 하락 없이 성능을 쉽게 감소시킬 수 있는 하드웨어에서 실행할 수 있게 해줍니다.\n\n이 글과 함께 제공된 이미지를 통해 양자화 후에 모델이 원래 버전보다 상당히 적은 공간을 차지하는 것을 확인할 수 있습니다:\n\n![Quantized Models](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_8.png)\n\nOllama는 양자화된 모델을 지원하여 별도로 처리하는 부담을 덜어줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 7. 다른 소스에서 모델 통합하기\n\n![Running Models With Ollama Step-by-Step](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_9.png)\n\nOllama의 모델은 다양성을 제공하지만 현재 모든 모델에 액세스할 수 있는 것은 아닙니다. 그러나 로컬에 직접 모델을 통합하는 것은 간단한 프로세스입니다. 새로운 모델을 지역 Ollama에 통합하는 방법을 알아봅시다.\n\nThe Bloke의 HuggingFace 계정에서 많은 양자화된 모델을 사용할 수 있습니다. 의학 논문을 위해서 우리는 편리하게 medicine-chat-GGUF 모델을 선택할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n➡️ https://huggingface.co/TheBloke/medicine-chat-GGUF\n\n해당 링크를 열고 파일 및 버전을 클릭하세요.\n\n![Files and versions](/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_10.png)\n\nOllama 모델에 포함하고 싶은 모델을 다운로드하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_11.png\" /\u003e\n\nModelfile이라는 빈 파일을 생성하고 아래 지정된 데이터를 삽입하세요 (저장된 모델의 절대 경로로 경로를 대체하십시오). 이 예제는 기본적이며, 모델의 온도, 시스템 메시지 등과 같은 여러 옵션을 포함하여 확장될 수 있습니다. 필요한 경우 파일에서 '#'를 제거하여 해당 옵션을 활성화하세요.\n\n```js\nFROM D:\\...\\medicine-chat.Q4_0.gguf\n# PARAMETER 온도 0.6\n# SYSTEM \"\"\"도움이 되는 의학 조수입니다.\"\"\"\n```\n\nModelfile을 저장한 후, cmd에 다음을 입력하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```bash\nollama create 모델_이름 -f 모델_파일\n```\n\n# 9. Ollama를 활용한 (Python) 앱으로 개발자의 삶을 더 쉽게 만들기\n\n백그라운드에서 실행되는 Ollama는 일반적인 REST API와 같이 접근할 수 있습니다. 따라서 requests와 같은 라이브러리 또는 조금 더 발전된 FastAPI, Flask 또는 Django와 같은 프레임워크를 사용하여 응용 프로그램에 쉽게 통합할 수 있습니다.\n\nOllama python 패키지를 쉽게 pip를 통해 설치하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n⬆️ https://pypi.org/project/ollama/0.1.3:\n\n```js\npip install ollama\n```\n\nPython 코드를 통해 임베딩을 생성하는 방법:\n\n```js\nimport ollama\n\nembedding = ollama.embeddings(model=\"llama2:7b\", prompt=\"Hello Ollama!\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단히 CURL을 사용하여:\n\n```js\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"llama2:7b\",\n  \"prompt\": \"Here is an article about llamas...\"\n}'\n```\n\nOllama 엔드포인트에 대해 더 알아보려면 다음 링크를 방문해주세요:\n\n➡️ https://github.com/ollama/ollama/blob/main/docs/api.md\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nOllama가 Langchain 프레임워크에 원활하게 통합되어 개발 노력을 최적화하고 기술 측면의 작업을 더욱 간편하게 만들었습니다:\n\n➡️ https://python.langchain.com/docs/integrations/llms/ollama\n\n임베딩을 만드는 간단함을 감상해보세요:\n\n```js\n# pip install langchain_community\nfrom langchain_community.embeddings import OllamaEmbeddings\n\n\nembed = OllamaEmbeddings(model=\"llama2:7b\")\nembedding = embed.embed_query(\"Hello Ollama!\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 10. 요약\n\n본 기사는 당신을 Ollama를 사용하여 모델을 실행하는 과정을 단계별로 안내하여, 전체 인프라 구성 없이 LLM을 테스트할 수 있는 원활한 방법을 제공합니다.\n\n올라마는 오픈 소스 도구로, Meta의 Llama2 모델을 무료로 사용할 수 있게 해주는 로컬 또는 서버 기반의 언어 모델 통합을 용이하게 합니다. 윈도우에서의 설치 과정과 명령줄을 통해 Ollama를 실행하는 방법에 대해 설명되어 있습니다.\n\n이 기사에서는 모델 다운로드, 특정 작업을 위한 다양한 모델 옵션, 다양한 명령어를 사용하여 모델 실행, CPU 친화적인 양자화된 모델, 그리고 외부 모델 통합에 대해 탐구합니다. 또한, 개발자들을 위해 Ollama를 활용한 파이썬 애플리케이션을 강조하고 있습니다.","ogImage":{"url":"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png"},"coverImage":"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png","tag":["Tech"],"readingTime":10},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003eLLM을 빠르게 테스트할 수 있는 방법을 찾고 계신가요? 전체 인프라를 설정할 필요 없이 테스트할 수 있는 방법이 있다면 정말 훌륭하죠. 이 짧은 기사에서 우리가 할 일이 바로 그거에요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eOllama에 관해 경험이 있는 경우에는 특정 단락으로 이동해도 됩니다. 이 기사에서 찾을 수 있는 내용은 다음과 같아요:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOllama가 무엇인가요?\u003c/li\u003e\n\u003cli\u003eWindows에 Ollama 설치하기.\u003c/li\u003e\n\u003cli\u003eOllama [cmd] 실행하기.\u003c/li\u003e\n\u003cli\u003e로컬로 모델 다운로드하기.\u003c/li\u003e\n\u003cli\u003e다양한 용도에 맞는 다른 모델.\u003c/li\u003e\n\u003cli\u003e모델 실행하기 [cmd].\u003c/li\u003e\n\u003cli\u003eCPU에 친화적인 양자화된 모델.\u003c/li\u003e\n\u003cli\u003e다른 소스에서 모델 통합하기.\u003c/li\u003e\n\u003cli\u003eOllama-파워드 (Python) 앱으로 개발자들의 삶을 더 쉽게 만들기.\u003c/li\u003e\n\u003cli\u003e요약.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e1. Ollama이란?\u003c/h1\u003e\n\u003cp\u003eOllama는 오픈 소스 코드로, 로컬에서 또는 본인의 서버에서 언어 모델과의 원활한 통합을 가능하게 하는 사용 준비 도구입니다. 이를 통해 상업용 API의 유료 버전을 사용하지 않아도 되므로, 특히 이제 Meta가 Llama2 모델을 상용으로 사용 가능하게 한 것을 고려하면, 자신의 데이터셋에서 추가 학습에 적합합니다.\u003c/p\u003e\n\u003cp\u003e➡️ GitHub 저장소: \u003ca href=\"https://github.com/ollama/ollama\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/ollama/ollama\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e➡️ Ollama 공식 웹페이지: \u003ca href=\"https://ollama.com\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch1\u003e2. Windows에서 Ollama 설치하기\u003c/h1\u003e\n\u003cp\u003eOllama는 Windows, Mac 및 Linux에서도 원활하게 작동합니다. 이 간단한 자습서는 특히 Windows 10용 설치 단계를 안내합니다. 설치 후 프로그램은 약 384MB를 차지합니다. 그러나 다운로드한 모델이 가벼운 것은 아닐 수 있습니다.\u003c/p\u003e\n\u003cp\u003e만약 도커 컨테이너에서 Ollama를 실행하길 원한다면, 아래 설명을 건너뛰고\u003c/p\u003e\n\u003cp\u003e감십시오.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e➡️ \u003ca href=\"https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_2.png\" alt=\"Running Models with Ollama\"\u003e\u003c/p\u003e\n\u003cp\u003e➡️ Ollama 홈페이지로 이동하여 .exe 파일을 다운로드하세요: \u003ca href=\"https://ollama.com\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_3.png\" alt=\"Running Models with Ollama\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eOllama를 다운로드하고 Windows에 설치하세요. 보통 다음 경로에 위치한 기본 모델 저장 경로를 사용할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-attr\"\u003eC\u003c/span\u003e:\\\u003cspan class=\"hljs-title class_\"\u003eUsers\u003c/span\u003e\\your_user\\.\u003cspan class=\"hljs-property\"\u003eollama\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그러나 C: 파티션에 공간이 제한적이라면, 대안 디렉토리로 전환하는 것이 권장됩니다. D:\\와 같은 다른 파티션이 있는 경우, 간단하게:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e데스크탑의 컴퓨터 아이콘을 마우스 오른쪽 클릭합니다.\u003c/li\u003e\n\u003cli\u003e속성을 선택한 후 \"고급 시스템 설정\"으로 이동합니다.\u003c/li\u003e\n\u003cli\u003e환경 변수를 클릭합니다.\u003c/li\u003e\n\u003cli\u003e...을 위한 사용자 변수에서 모델을 저장할 디렉토리의 절대 경로를 삽입하십시오. 예를 들면:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e변수: \u003cspan class=\"hljs-variable constant_\"\u003eOLLAMA_MODELS\u003c/span\u003e\n값: \u003cspan class=\"hljs-attr\"\u003eD\u003c/span\u003e:\\your_directory\\models\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOLLAMA_MODELS 변수의 이름을 변경하지 마십시오. 이 변수는 Ollama가 정확히 아래와 같이 검색할 것입니다.\u003c/p\u003e\n\u003cp\u003eWindows의 하단 표시줄에 Ollama 아이콘이 나타납니다. 프로그램이 시작되지 않으면 Windows 프로그램에서 찾아서 거기서 시작하십시오.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_4.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제 Ollama를 실행하고 모델을 다운로드할 준비가 되었어요 :)\u003c/p\u003e\n\u003ch1\u003e3. Ollama 실행하기 [cmd]\u003c/h1\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_5.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eOllama를 설정하고 나면 윈도우에서 cmd(명령줄)를 열고 로컬로 일부 모델을 다운로드할 수 있어요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eOllama 로컬 대시보드를 사용하려면 웹 브라우저에서 다음 URL을 입력하세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-attr\"\u003ehttp\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//localhost:11434/api/\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOllama를 실행하는 것은 그렇게 어렵지 않습니다. 나중에 CMD 및 Python 코드를 통해 어떻게 활용하는지 알아보겠습니다.\u003c/p\u003e\n\u003cp\u003e중요한 몇 가지 명령어:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e로컬로 사용 가능한 모델을 확인하려면 다음을 cmd에 입력하세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eollama list\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e특정 모델에 해당하는 SHA 파일을 확인하려면 cmd에 입력하세요 (예: llama2:7b 모델 확인을 위한 예시):\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eollama show --modelfile \u003cspan class=\"hljs-attr\"\u003ellama2\u003c/span\u003e:7b\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e모델을 제거하려면:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eollama rm \u003cspan class=\"hljs-attr\"\u003ellama2\u003c/span\u003e:7b\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e모델을 서버에 올리려면:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eollama serve\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e4. 모델을 로컬로 다운로드하기\u003c/h1\u003e\n\u003cp\u003e웹사이트 ➡️ \u003ca href=\"https://ollama.com/library\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/library\u003c/a\u003e 에서는 여러 다양한 파라미터 크기로 제공되는 다수의 모델을 다운로드할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e로컬로 모델을 다운로드하기 전에, 해당 모델을 로딩할 충분한 메모리를 가지고 있는지 확인해주세요. 테스트할 때는 애플리케이션에 통합하기에 적합한 작은 모델인 '7B'로 레이블이 지정된 모델을 사용하는 것이 좋습니다.\u003c/p\u003e\n\u003cp\u003e⚠️ 부드러운 모델 작동을 위해 적어도 하나의 GPU를 보유하는 것이 강력하게 권장됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래에는 내가 테스트하고 추천하는 여러 모델이 있습니다. 명령을 복사하여 명령 프롬프트에 붙여넣어 지정된 모델을 로컬로 가져올 수 있습니다.\u003c/p\u003e\n\u003cp\u003e👉Meta에서의 Llama2 모델\u003c/p\u003e\n\u003cp\u003e대화 시나리오를 위해 최적화된 생성 텍스트 모델 세트입니다. Ollama의 많은 모델과 마찬가지로 Llama2는 다양한 구성으로 제공됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_6.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래는 해당 모델을 가져오는 몇 가지 예시입니다:\u003c/p\u003e\n\u003cp\u003e표준 모델:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eollama pull llama2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e검열되지 않은 버전:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eollama pull llama2-비겁하지 않은:7b\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e채팅 7B 모델:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eollama pull \u003cspan class=\"hljs-attr\"\u003ellama2\u003c/span\u003e:7b-채팅\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e➡️ 더 읽기: \u003ca href=\"https://llama.meta.com/llama2\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://llama.meta.com/llama2\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e👉 구글의 젬마\u003c/p\u003e\n\u003cp\u003e주요 7B 크기 모델과 유사한 견고한 성능을 제공하는 오픈 소스 모델입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eollama pull \u003cspan class=\"hljs-attr\"\u003egemma\u003c/span\u003e:7b\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e➡️ 자세히 보기: \u003ca href=\"https://blog.google/technology/developers/gemma-open-models/\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://blog.google/technology/developers/gemma-open-models/\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e👉 Haotian Liu 등의 LLava.\u003c/p\u003e\n\u003cp\u003e이미지에서 텍스트 설명을 다루는 데 뛰어나며 시각 및 언어 모델 모두에 대한 견고한 지원을 제공하는 멀티모달 모델입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eollama pull llava\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e➡️ 자세히 알아보기: \u003ca href=\"https://llava-vl.github.io/\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://llava-vl.github.io/\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003col start=\"5\"\u003e\n\u003cli\u003e서로 다른 목적을 위한 다양한 모델\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_7.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e일부 모델은 특정 데이터셋에서 훈련되어 코드 완성, 대화 또는 이미지에서 텍스트로 변환과 같은 특정 작업에 더 적합합니다. Ollama에서는 다양한 목적을 위해 설계된 모델을 찾을 수 있습니다.\u003c/p\u003e\n\u003cp\u003e첫 번째 그룹은 대화, 텍스트 완성, 요약 등을 용이하게 하는 데 초점을 맞춘 모델을 포함하고 있습니다. Gemma, Llama2, Falcon 또는 OpenChat과 같은 모델이 포함됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e일부 예시:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://ollama.com/library/falcon\" rel=\"nofollow\" target=\"_blank\"\u003eFalcon\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://ollama.com/library/gemma\" rel=\"nofollow\" target=\"_blank\"\u003eGemma\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://ollama.com/library/openchat\" rel=\"nofollow\" target=\"_blank\"\u003eOpenchat\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다음 그룹은 대화를 나누거나 챗봇 역할을 하는 다중 모달 모델과 이미지 설명(시각 모델), 텍스트 요약, 질문-답변(Q/A) 애플리케이션을 구동할 수 있는 모델들로 구성됩니다.\u003c/p\u003e\n\u003cp\u003e일부 예시:\u003c/p\u003e\n\u003cp\u003e➡️ \u003ca href=\"https://ollama.com/library/llava\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/library/llava\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e➡️ \u003ca href=\"https://ollama.com/library/bakllava\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/library/bakllava\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e마지막으로, 매우 전문화된 그룹은 Ollama에서 이용 가능한 모델을 활용하여 개발자의 작업을 지원합니다. 코델라마, 돌핀-미스트랄, 돌핀-믹스트랄(코딩 작업에 능숙한 Mixtral 전문가 모델을 기반으로 세밀하게 조정된 모델)과 같은 모델들이 있으며, 계속해서 크리에이터들이 추가하고 있습니다.\u003c/p\u003e\n\u003cp\u003e몇 가지 예시:\u003c/p\u003e\n\u003cp\u003e➡️ \u003ca href=\"https://ollama.com/library/codellama\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/library/codellama\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e➡️ \u003ca href=\"https://ollama.com/library/dolphin-mistral\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/library/dolphin-mistral\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e➡️ \u003ca href=\"https://ollama.com/library/dolphin-mixtral\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/library/dolphin-mixtral\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e6. 모델 실행하기 [cmd]\u003c/h1\u003e\n\u003cp\u003e다운로드한 모델을 실행하려면, ollama run 모델이름:파라미터 \"당신의 프롬프트\"를 입력하세요. 예를 들어:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eollama run \u003cspan class=\"hljs-attr\"\u003ellama2\u003c/span\u003e:7b \u003cspan class=\"hljs-string\"\u003e\"당신의 프롬프트\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다중 모달 모델을 사용하면 기본 프롬프트를 벗어난 파일, 로컬 이미지 경로 등을 포함할 수 있어 더 많은 기능을 확장할 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e6. CPU 친화적 양자화 모델\u003c/h1\u003e\n\u003cp\u003e양자화는 모델의 정밀도를 유지하는 비용을 줄이는 것으로 관련 비용을 줄이는 것입니다. 이 과정 뒤에 숨은 직관력을 구축하는 데 도움이 되는 이 크고 훌륭한 기사에서 자세한 설명을 찾아볼 수 있습니다:\u003c/p\u003e\n\u003cp\u003e📃 양자화 LLMs란 무엇인가? (Miguel Carreira Neves의 글):\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e➡️ \u003ca href=\"https://www.tensorops.ai/post/what-are-quantized-llms\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://www.tensorops.ai/post/what-are-quantized-llms\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e추가 자료:\u003c/p\u003e\n\u003cp\u003e📃 Extreme Compression of Large Language Models via Additive Quantization:\u003c/p\u003e\n\u003cp\u003e➡️ \u003ca href=\"https://arxiv.org/html/2401.06118v2\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://arxiv.org/html/2401.06118v2\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e📃 SmoothQuant: 대형 언어 모델을 위한 정확하고 효율적인 사후 훈련 양자화:\u003c/p\u003e\n\u003cp\u003e➡️ \u003ca href=\"https://arxiv.org/pdf/2211.10438.pdf\" rel=\"nofollow\" target=\"_blank\"\u003e논문 링크\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e📃 BiLLM: LLMs를 위한 사후 훈련 양자화 한계 돌파:\u003c/p\u003e\n\u003cp\u003e➡️ \u003ca href=\"https://arxiv.org/pdf/2402.04291.pdf\" rel=\"nofollow\" target=\"_blank\"\u003e논문 링크\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e간단하게 말하면, 양자화는 가중치 정밀도를 조정하여 모델 크기를 줄이고 중요한 정확도 하락 없이 성능을 쉽게 감소시킬 수 있는 하드웨어에서 실행할 수 있게 해줍니다.\u003c/p\u003e\n\u003cp\u003e이 글과 함께 제공된 이미지를 통해 양자화 후에 모델이 원래 버전보다 상당히 적은 공간을 차지하는 것을 확인할 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_8.png\" alt=\"Quantized Models\"\u003e\u003c/p\u003e\n\u003cp\u003eOllama는 양자화된 모델을 지원하여 별도로 처리하는 부담을 덜어줍니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e7. 다른 소스에서 모델 통합하기\u003c/h1\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_9.png\" alt=\"Running Models With Ollama Step-by-Step\"\u003e\u003c/p\u003e\n\u003cp\u003eOllama의 모델은 다양성을 제공하지만 현재 모든 모델에 액세스할 수 있는 것은 아닙니다. 그러나 로컬에 직접 모델을 통합하는 것은 간단한 프로세스입니다. 새로운 모델을 지역 Ollama에 통합하는 방법을 알아봅시다.\u003c/p\u003e\n\u003cp\u003eThe Bloke의 HuggingFace 계정에서 많은 양자화된 모델을 사용할 수 있습니다. 의학 논문을 위해서 우리는 편리하게 medicine-chat-GGUF 모델을 선택할 수 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e➡️ \u003ca href=\"https://huggingface.co/TheBloke/medicine-chat-GGUF\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://huggingface.co/TheBloke/medicine-chat-GGUF\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e해당 링크를 열고 파일 및 버전을 클릭하세요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_10.png\" alt=\"Files and versions\"\u003e\u003c/p\u003e\n\u003cp\u003eOllama 모델에 포함하고 싶은 모델을 다운로드하세요:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-05-27-RunningmodelswithOllamastep-by-step_11.png\"\u003e\n\u003cp\u003eModelfile이라는 빈 파일을 생성하고 아래 지정된 데이터를 삽입하세요 (저장된 모델의 절대 경로로 경로를 대체하십시오). 이 예제는 기본적이며, 모델의 온도, 시스템 메시지 등과 같은 여러 옵션을 포함하여 확장될 수 있습니다. 필요한 경우 파일에서 '#'를 제거하여 해당 옵션을 활성화하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-variable constant_\"\u003eFROM\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eD\u003c/span\u003e:\\...\\medicine-chat.\u003cspan class=\"hljs-property\"\u003eQ4_0\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003egguf\u003c/span\u003e\n# \u003cspan class=\"hljs-variable constant_\"\u003ePARAMETER\u003c/span\u003e 온도 \u003cspan class=\"hljs-number\"\u003e0.6\u003c/span\u003e\n# \u003cspan class=\"hljs-variable constant_\"\u003eSYSTEM\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"도움이 되는 의학 조수입니다.\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eModelfile을 저장한 후, cmd에 다음을 입력하세요:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003eollama create 모델_이름 -f 모델_파일\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e9. Ollama를 활용한 (Python) 앱으로 개발자의 삶을 더 쉽게 만들기\u003c/h1\u003e\n\u003cp\u003e백그라운드에서 실행되는 Ollama는 일반적인 REST API와 같이 접근할 수 있습니다. 따라서 requests와 같은 라이브러리 또는 조금 더 발전된 FastAPI, Flask 또는 Django와 같은 프레임워크를 사용하여 응용 프로그램에 쉽게 통합할 수 있습니다.\u003c/p\u003e\n\u003cp\u003eOllama python 패키지를 쉽게 pip를 통해 설치하세요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e⬆️ \u003ca href=\"https://pypi.org/project/ollama/0.1.3\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://pypi.org/project/ollama/0.1.3\u003c/a\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epip install ollama\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePython 코드를 통해 임베딩을 생성하는 방법:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e ollama\n\nembedding = ollama.\u003cspan class=\"hljs-title function_\"\u003eembeddings\u003c/span\u003e(model=\u003cspan class=\"hljs-string\"\u003e\"llama2:7b\"\u003c/span\u003e, prompt=\u003cspan class=\"hljs-string\"\u003e\"Hello Ollama!\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e간단히 CURL을 사용하여:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ecurl \u003cspan class=\"hljs-attr\"\u003ehttp\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//localhost:11434/api/embeddings -d '{\u003c/span\u003e\n  \u003cspan class=\"hljs-string\"\u003e\"model\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"llama2:7b\"\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"prompt\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"Here is an article about llamas...\"\u003c/span\u003e\n}\u003cspan class=\"hljs-string\"\u003e'\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOllama 엔드포인트에 대해 더 알아보려면 다음 링크를 방문해주세요:\u003c/p\u003e\n\u003cp\u003e➡️ \u003ca href=\"https://github.com/ollama/ollama/blob/main/docs/api.md\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/ollama/ollama/blob/main/docs/api.md\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eOllama가 Langchain 프레임워크에 원활하게 통합되어 개발 노력을 최적화하고 기술 측면의 작업을 더욱 간편하게 만들었습니다:\u003c/p\u003e\n\u003cp\u003e➡️ \u003ca href=\"https://python.langchain.com/docs/integrations/llms/ollama\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://python.langchain.com/docs/integrations/llms/ollama\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e임베딩을 만드는 간단함을 감상해보세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# pip install langchain_community\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain_community.\u003cspan class=\"hljs-property\"\u003eembeddings\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOllamaEmbeddings\u003c/span\u003e\n\n\nembed = \u003cspan class=\"hljs-title class_\"\u003eOllamaEmbeddings\u003c/span\u003e(model=\u003cspan class=\"hljs-string\"\u003e\"llama2:7b\"\u003c/span\u003e)\nembedding = embed.\u003cspan class=\"hljs-title function_\"\u003eembed_query\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Hello Ollama!\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e10. 요약\u003c/h1\u003e\n\u003cp\u003e본 기사는 당신을 Ollama를 사용하여 모델을 실행하는 과정을 단계별로 안내하여, 전체 인프라 구성 없이 LLM을 테스트할 수 있는 원활한 방법을 제공합니다.\u003c/p\u003e\n\u003cp\u003e올라마는 오픈 소스 도구로, Meta의 Llama2 모델을 무료로 사용할 수 있게 해주는 로컬 또는 서버 기반의 언어 모델 통합을 용이하게 합니다. 윈도우에서의 설치 과정과 명령줄을 통해 Ollama를 실행하는 방법에 대해 설명되어 있습니다.\u003c/p\u003e\n\u003cp\u003e이 기사에서는 모델 다운로드, 특정 작업을 위한 다양한 모델 옵션, 다양한 명령어를 사용하여 모델 실행, CPU 친화적인 양자화된 모델, 그리고 외부 모델 통합에 대해 탐구합니다. 또한, 개발자들을 위해 Ollama를 활용한 파이썬 애플리케이션을 강조하고 있습니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-27-RunningmodelswithOllamastep-by-step"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>