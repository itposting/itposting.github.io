<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>실시간 얼굴 인식 끝에서 끝까지의 프로젝트 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="실시간 얼굴 인식 끝에서 끝까지의 프로젝트 | itposting" data-gatsby-head="true"/><meta property="og:title" content="실시간 얼굴 인식 끝에서 끝까지의 프로젝트 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject" data-gatsby-head="true"/><meta name="twitter:title" content="실시간 얼굴 인식 끝에서 끝까지의 프로젝트 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 06:12" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">실시간 얼굴 인식 끝에서 끝까지의 프로젝트</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="실시간 얼굴 인식 끝에서 끝까지의 프로젝트" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">16<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>단계별로 배우세요! PiCam을 사용하여 실시간으로 얼굴을 인식하는 방법을 배워보세요.</p>
<p><img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_0.png" alt="PiCam Image"></p>
<h1>1. 소개</h1>
<p>OpenCV를 탐구하는 내 교재에서는 자동 비전 객체 추적을 배웠습니다. 이제 PiCam을 사용하여 실시간으로 얼굴을 인식해보겠습니다. 위에서 볼 수 있듯이, 함께해요!</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_1.png" alt="image"></p>
<p>이 프로젝트는 이 훌륭한 "Open Source Computer Vision Library" 인 OpenCV를 사용하여 수행되었습니다. 이 튜토리얼에서는 Raspberry Pi (즉, Raspbian을 사용한 OS)와 Python에 중점을 두지만 Mac에서 코드를 테스트하고 또한 잘 작동하는 것을 확인했습니다.</p>
<p>OpenCV는 계산 효율성을 위해 설계되었으며 실시간 애플리케이션에 중점을 두고 있습니다. 따라서 카메라를 사용한 실시간 얼굴 인식에 적합합니다.</p>
<h2>3 단계</h2>
<div class="content-ad"></div>
<p>얼굴인식에 대한 완전한 프로젝트를 생성하려면 3가지 매우 다른 단계에서 작업해야 합니다:</p>
<ul>
<li>얼굴 감지 및 데이터 수집</li>
<li>인식기 훈련</li>
<li>얼굴 인식</li>
</ul>
<p>다음 블록 다이어그램은 이러한 단계들을 요약합니다:</p>
<p><img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_2.png" alt="Face Recognition Project Phases"></p>
<div class="content-ad"></div>
<h1>2. OpenCV 3 패키지 설치</h1>
<p>저는 최신 버전의 라스비안(Stretch)이 설치된 라즈베리 파이 V3를 사용하고 있습니다. 따라서 OpenCV를 설치하는 가장 좋은 방법은 Adrian Rosebrock이 개발한 훌륭한 튜토리얼을 따라하는 것입니다: "Raspbian Stretch: 라즈베리 파이에 OpenCV 3 + Python 설치".</p>
<p>Adrian의 튜토리얼을 완료하면 라즈베리 파이에서 실험을 실행할 준비가 된 OpenCV 가상 환경이 준비됩니다.</p>
<p>이제 가상 환경으로 이동하여 OpenCV 3이 올바르게 설치되었는지 확인해 보겠습니다.</p>
<div class="content-ad"></div>
<p>에드리안이 새 터미널을 열 때마다 "source" 명령을 실행하여 시스템 변수가 올바르게 설정되었는지 확인하는 것을 권장합니다.</p>
<pre><code class="hljs language-js">source ~/.<span class="hljs-property">profile</span>
</code></pre>
<p>다음으로, 가상 환경에 들어가 봅시다:</p>
<pre><code class="hljs language-js">workon cv
</code></pre>
<div class="content-ad"></div>
<p>만약 당신이 cv 가상환경 앞에 있는 텍스트를 보신다면, cv 가상환경 안에 있습니다:</p>
<pre><code class="hljs language-js">(cv) pi@<span class="hljs-attr">raspberry</span>:~$
</code></pre>
<p>이제 파이썬 인터프리터로 들어가보세요:</p>
<pre><code class="hljs language-js">python
</code></pre>
<div class="content-ad"></div>
<p>저희가 현재 3.5 버전 (또는 그 이상)을 실행 중이라고 확인해주세요.</p>
<p>인터프리터 안에 ( 가 표시될 것입니다), OpenCV 라이브러리를 import 해주세요:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> cv2
</code></pre>
<p>만약 에러 메시지가 나타나지 않는다면, OpenCV가 정확하게 파이썬 가상 환경에 설치된 것입니다.</p>
<div class="content-ad"></div>
<p>설치된 OpenCV 버전을 확인할 수도 있어요:</p>
<pre><code class="hljs language-js">cv2.<span class="hljs-property">__version__</span>
</code></pre>
<p>3.3.0이 표시되어야 해요 (또는 미래에 출시될 우수한 버전).</p>
<img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_3.png">
<div class="content-ad"></div>
<p>위의 터미널 스크린샷은 이전 단계를 보여줍니다.</p>
<h1>3. 카메라 테스트</h1>
<p>RPi에 OpenCV를 설치했다면 카메라가 제대로 작동하는지 확인하기 위해 테스트를 해봅시다.</p>
<p>이미 PiCam을 설치하고 활성화했다는 것을 전제로 합니다.</p>
<div class="content-ad"></div>
<p>아래 Python 코드를 IDE에 입력해보세요:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> cv2
cap = cv2.<span class="hljs-title class_">VideoCapture</span>(<span class="hljs-number">0</span>)
cap.<span class="hljs-title function_">set</span>(<span class="hljs-number">3</span>,<span class="hljs-number">640</span>) # 너비 설정
cap.<span class="hljs-title function_">set</span>(<span class="hljs-number">4</span>,<span class="hljs-number">480</span>) # 높이 설정
<span class="hljs-keyword">while</span>(<span class="hljs-title class_">True</span>):
    ret, frame = cap.<span class="hljs-title function_">read</span>()
    frame = cv2.<span class="hljs-title function_">flip</span>(frame, -<span class="hljs-number">1</span>) # 카메라 세로로 뒤집기
    gray = cv2.<span class="hljs-title function_">cvtColor</span>(frame, cv2.<span class="hljs-property">COLOR_BGR2GRAY</span>)
    
    cv2.<span class="hljs-title function_">imshow</span>(<span class="hljs-string">'frame'</span>, frame)
    cv2.<span class="hljs-title function_">imshow</span>(<span class="hljs-string">'gray'</span>, gray)
    
    k = cv2.<span class="hljs-title function_">waitKey</span>(<span class="hljs-number">30</span>) &#x26; <span class="hljs-number">0xff</span>
    <span class="hljs-keyword">if</span> k == <span class="hljs-number">27</span>: # <span class="hljs-string">'ESC'</span> 키를 눌러 종료
        <span class="hljs-keyword">break</span>
cap.<span class="hljs-title function_">release</span>()
cv2.<span class="hljs-title function_">destroyAllWindows</span>()
</code></pre>
<p>위 코드는 PiCam에서 생성된 비디오 스트림을 캡처하여 BGR 색상과 회색 모드로 모두 표시합니다.</p>
<p>또는 GitHub에서 코드를 다운로드할 수도 있습니다: simpleCamTest.py</p>
<div class="content-ad"></div>
<p>해당 스크립트를 실행하려면 다음 명령어를 입력해주세요:</p>
<pre><code class="hljs language-js">python simpleCamTest.<span class="hljs-property">py</span>
</code></pre>
<img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_4.png">
<p>위의 그림은 결과를 보여줍니다.</p>
<div class="content-ad"></div>
<p>일부 사용자가 카메라를 열려고 시도할 때 문제가 발생하고 "Assertion failed" 오류 메시지가 표시된다는 것을 발겠어요. 이것은 카메라가 OpenCV 설치 중에 활성화되지 않았거나 카메라 드라이버가 올바르게 설치되지 않아 발생할 수 있습니다. 수정하려면 다음 명령을 사용하세요:</p>
<pre><code class="hljs language-js">sudo modprobe bcm2835-v4l2
</code></pre>
<p>OpenCV에 대해 더 알고 싶다면 다음 튜토리얼을 참고해보세요: loading-video-python-opencv-tutorial</p>
<h1>4. 얼굴 인식</h1>
<div class="content-ad"></div>
<p>Face Recognition에서 가장 기본적인 작업은 물론 "얼굴 감지"입니다. 무엇보다 먼저, 미래에 캡처된 새 얼굴과 비교할 수 있도록 얼굴을 캡처해야 합니다(Phase 1).</p>
<p>(혹시 이전에 말한 'Capture'부분 해석이 잘못된 것 같아 원문에서 대체했습니다.)</p>
<p>한 객체를 감지하는 가장 일반적인 방법은 "Haar Cascade classifier"를 사용하는 것입니다.</p>
<p>"Haar 특징 기반 캐스케이드 분류기"를 사용한 객체 감지는 Paul Viola와 Michael Jones가 2001년에 발표한 논문 "Rapid Object Detection using a Boosted Cascade of Simple Features"에서 제안된 효과적인 객체 감지 방법입니다. 이는 많은 양의 긍정 이미지와 부정 이미지로부터 케스케이드 함수가 훈련된 기계 학습 기반 방법으로, 다른 이미지에서 객체를 감지하는 데 사용됩니다.</p>
<p>여기서는 얼굴 감지 작업을 수행할 것입니다. 먼저, 알고리즘에는 분류기를 훈련시키기 위해 많은 양의 양성 이미지(얼굴 이미지)와 음성 이미지(얼굴이 없는 이미지)가 필요합니다. 그런 다음 그로부터 특징을 추출해야 합니다. 좋은 소식은 OpenCV에 트레이너와 탐지기가 함께 제공된다는 것입니다. 자동차, 비행기 등의 모든 객체에 대해 자체 분류기를 훈련시키고 싶다면 OpenCV를 사용해 생성할 수 있습니다. 자세한 내용은 여기를 참고하십시오: 캐스케이드 분류기 훈련.</p>
<div class="content-ad"></div>
<p>만약 여러분이 자체 분류기를 만들고 싶지 않다면, OpenCV에는 이미 얼굴, 눈, 웃음 등을 위한 사전 훈련된 분류기가 많이 포함되어 있습니다. 이러한 XML 파일은 haarcascades 디렉토리에서 다운로드할 수 있습니다.</p>
<p>이론은 이만하고, 이제 OpenCV를 사용하여 얼굴 탐지기를 만들어 봅시다!</p>
<p>제 GitHub에서 파일 faceDetection.py를 다운로드하세요.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> cv2
faceCascade = cv2.CascadeClassifier(<span class="hljs-string">'Cascades/haarcascade_frontalface_default.xml'</span>)
cap = cv2.VideoCapture(<span class="hljs-number">0</span>)
cap.<span class="hljs-built_in">set</span>(<span class="hljs-number">3</span>,<span class="hljs-number">640</span>) <span class="hljs-comment"># 너비 설정</span>
cap.<span class="hljs-built_in">set</span>(<span class="hljs-number">4</span>,<span class="hljs-number">480</span>) <span class="hljs-comment"># 높이 설정</span>
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
    ret, img = cap.read()
    img = cv2.flip(img, -<span class="hljs-number">1</span>)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = faceCascade.detectMultiScale(
        gray,
        scaleFactor=<span class="hljs-number">1.2</span>,
        minNeighbors=<span class="hljs-number">5</span>,
        minSize=(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>)
    )
    <span class="hljs-keyword">for</span> (x,y,w,h) <span class="hljs-keyword">in</span> faces:
        cv2.rectangle(img,(x,y),(x+w,y+h),(<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">2</span>)
        roi_gray = gray[y:y+h, x:x+w]
        roi_color = img[y:y+h, x:x+w]
    cv2.imshow(<span class="hljs-string">'video'</span>,img)
    k = cv2.waitKey(<span class="hljs-number">30</span>) &#x26; <span class="hljs-number">0xff</span>
    <span class="hljs-keyword">if</span> k == <span class="hljs-number">27</span>: <span class="hljs-comment"># 'ESC'를 눌러 종료</span>
        <span class="hljs-keyword">break</span>
cap.release()
cv2.destroyAllWindows()
</code></pre>
<div class="content-ad"></div>
<p>믿을 수 있건대, Python과 OpenCV를 사용하여 얼굴을 감지하는 데 필요한 코드는 바로 위의 몇 줄 뿐입니다.</p>
<p>카메라를 테스트하는 데 사용된 마지막 코드와 비교해보면, 약간의 부분이 추가되었음을 깨달을 것입니다. 아래의 줄을 주목해주세요:</p>
<pre><code class="hljs language-js">faceCascade = cv2.<span class="hljs-title class_">CascadeClassifier</span>(<span class="hljs-string">'Cascades/haarcascade_frontalface_default.xml'</span>)
</code></pre>
<p>이 줄은 "classifier"를 로드하는 줄입니다 (프로젝트 디렉토리 하위에 "Cascades/"라는 디렉토리에 있어야 합니다).</p>
<div class="content-ad"></div>
<p>그럼, 우리는 카메라를 설정하고 루프 내에서 입력 비디오를 회색조 모드로 불러올 것입니다 (이전에 보았던 것과 동일합니다).</p>
<p>이제 분류기 함수를 호출해야 하는데, 이때 매우 중요한 매개변수들을 전달해주어야 합니다. 이들은 scale factor, 이웃의 수, 그리고 감지된 얼굴의 최소 크기입니다.</p>
<pre><code class="hljs language-js">faces = faceCascade.<span class="hljs-title function_">detectMultiScale</span>(
        gray,     
        scaleFactor=<span class="hljs-number">1.2</span>,
        minNeighbors=<span class="hljs-number">5</span>,     
        minSize=(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>)
        )
</code></pre>
<p>여기서,</p>
<div class="content-ad"></div>
<ul>
<li>gray는 입력 그레이스케일 이미지입니다.</li>
<li>scaleFactor는 각 이미지 스케일에서 이미지 크기가 감소하는 정도를 지정하는 매개 변수입니다. 스케일 피라미드를 만드는 데 사용됩니다.</li>
<li>minNeighbors는 각 후보 사각형이 유지해야 하는 이웃 수를 지정하는 매개 변수입니다. 숫자가 높을수록 낮은 거짓 양성이 발생합니다.</li>
<li>minSize는 고려해야 할 최소 사각형 크기입니다.</li>
</ul>
<p>함수는 이미지에서 얼굴을 감지합니다. 다음으로 이미지에서 얼굴을 "표시"해야 합니다. 예를 들어, 파란색 사각형 등을 사용합니다. 이 작업은 다음 코드 부분으로 수행됩니다:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">for</span> (x,y,w,h) <span class="hljs-keyword">in</span> <span class="hljs-attr">faces</span>:
    cv2.<span class="hljs-title function_">rectangle</span>(img,(x,y),(x+w,y+h),(<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">2</span>)
    roi_gray = gray[<span class="hljs-attr">y</span>:y+h, <span class="hljs-attr">x</span>:x+w]
    roi_color = img[<span class="hljs-attr">y</span>:y+h, <span class="hljs-attr">x</span>:x+w]
</code></pre>
<p>얼굴이 발견되면, 감지된 얼굴의 위치를 왼쪽 위 모서리인 (x,y)로 하는 네모 상자로 반환하며, "w"를 너비, "h"를 높이로 가집니다 == (x, y, w, h). 사진을 참조하세요.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_5.png">
<p>이 위치들을 얻으면, 얼굴을 위한 "ROI" (사각형으로 그린)를 만들고 imshow() 함수를 사용하여 결과를 표시할 수 있습니다.</p>
<p>Rpi 터미널을 사용하여 위의 Python 스크립트를 Python 환경에서 실행해보세요:</p>
<pre><code class="hljs language-js">python faceDetection.<span class="hljs-property">py</span>
</code></pre>
<div class="content-ad"></div>
<p>결과:</p>
<img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_6.png">
<p>당신은 "눈 탐지" 또는 "미소 탐지"를 위한 분류기도 포함할 수 있습니다. 해당 경우에는 얼굴 루프 내에 분류기 함수와 사각형 그리기를 포함해야 합니다. 왜냐하면 얼굴 외부에서 눈이나 미소를 탐지하는 것은 의미가 없기 때문입니다.</p>
<h2>예시</h2>
<div class="content-ad"></div>
<p>내 GitHub에서 다른 예제들을 찾을 수 있어요:</p>
<ul>
<li>faceEyeDetection.py</li>
<li>faceSmileDetection.py</li>
<li>faceSmileEyeDetection.py</li>
</ul>
<p>그리고 사진에서 결과를 확인할 수 있어요.</p>
<p><img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_7.png" alt="image"></p>
<div class="content-ad"></div>
<p>아래 튜토리얼을 따라해서 얼굴 감지에 대해 더 잘 이해할 수 있어요:</p>
<p>Haar Cascade Object Detection Face &#x26; Eye OpenCV Python Tutorial</p>
<h1>5. 데이터 수집</h1>
<p>우선, 사진을 통한 얼굴 인식에 대한 람리즈 라자의 훌륭한 작업에 감사드려야 해요!</p>
<div class="content-ad"></div>
<p>OPENCV와 Python을 사용한 얼굴 인식: 초보자를 위한 안내서</p>
<p>그리고 비디오를 사용해 매우 포괄적인 튜토리얼을 개발한 Anirban Kar:</p>
<p>얼굴 인식 - 3 부분</p>
<p>두 튜토리얼을 꼭 확인해보시기를 적극 추천합니다.</p>
<div class="content-ad"></div>
<p>그렇게 말씀하시면 프로젝트의 첫 단계를 시작하겠습니다. 여기서 우리가 할 일은 마지막 단계(얼굴 감지)부터 시작하여 간단히 데이터 세트를 만드는 것입니다. 각 ID에 대해, 얼굴 감지에 사용된 부분이 회색으로 표시된 사진 그룹을 저장할 것입니다.</p>
<p><img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_8.png" alt="이미지"></p>
<p>먼저, 프로젝트를 개발할 디렉터리를 만드세요. 예를 들어, FacialRecognitionProject:</p>
<pre><code class="hljs language-js">mkdir <span class="hljs-title class_">FacialRecognitionProject</span>
</code></pre>
<div class="content-ad"></div>
<p>이 디렉토리에는 프로젝트용으로 만들 3개의 Python 스크립트뿐만 아니라 Facial Classifier도 저장해 두어야 해요. 이를 다운로드할 수 있는 GitHub 링크는 haarcascade_frontalface_default.xml입니다.</p>
<p>그리고 우리의 얼굴 샘플을 저장할 하위 디렉토리를 만들어 이름을 "dataset"으로 지어줘요:</p>
<pre><code class="hljs language-bash"><span class="hljs-built_in">mkdir</span> dataset
</code></pre>
<p>그리고 저의 GitHub에서 코드를 다운로드하세요: 01_face_dataset.py</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">import</span> os
cam = cv2.<span class="hljs-title class_">VideoCapture</span>(<span class="hljs-number">0</span>)
cam.<span class="hljs-title function_">set</span>(<span class="hljs-number">3</span>, <span class="hljs-number">640</span>) # 비디오 너비 설정
cam.<span class="hljs-title function_">set</span>(<span class="hljs-number">4</span>, <span class="hljs-number">480</span>) # 비디오 높이 설정
face_detector = cv2.<span class="hljs-title class_">CascadeClassifier</span>(<span class="hljs-string">'haarcascade_frontalface_default.xml'</span>)
# 각 사람에 대해 하나의 숫자 얼굴 <span class="hljs-variable constant_">ID</span> 입력
face_id = <span class="hljs-title function_">input</span>(<span class="hljs-string">'\n 사용자 ID를 입력하고 &#x3C;return>을 누르세요 ==> '</span>)
<span class="hljs-title function_">print</span>(<span class="hljs-string">"\n [INFO] 얼굴 캡처 초기화. 카메라를 응시하고 기다리세요 ..."</span>)
# 개별 샘플링 얼굴 카운트 초기화
count = <span class="hljs-number">0</span>
<span class="hljs-keyword">while</span>(<span class="hljs-title class_">True</span>):
    ret, img = cam.<span class="hljs-title function_">read</span>()
    img = cv2.<span class="hljs-title function_">flip</span>(img, -<span class="hljs-number">1</span>) # 비디오 이미지 수직으로 뒤집기
    gray = cv2.<span class="hljs-title function_">cvtColor</span>(img, cv2.<span class="hljs-property">COLOR_BGR2GRAY</span>)
    faces = face_detector.<span class="hljs-title function_">detectMultiScale</span>(gray, <span class="hljs-number">1.3</span>, <span class="hljs-number">5</span>)
    <span class="hljs-keyword">for</span> (x,y,w,h) <span class="hljs-keyword">in</span> <span class="hljs-attr">faces</span>:
        cv2.<span class="hljs-title function_">rectangle</span>(img, (x,y), (x+w,y+h), (<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>), <span class="hljs-number">2</span>)     
        count += <span class="hljs-number">1</span>
        # 캡처된 이미지를 데이터셋 폴더에 저장
        cv2.<span class="hljs-title function_">imwrite</span>(<span class="hljs-string">"dataset/User."</span> + <span class="hljs-title function_">str</span>(face_id) + <span class="hljs-string">'.'</span> +  
                    <span class="hljs-title function_">str</span>(count) + <span class="hljs-string">".jpg"</span>, gray[<span class="hljs-attr">y</span>:y+h,<span class="hljs-attr">x</span>:x+w])
        cv2.<span class="hljs-title function_">imshow</span>(<span class="hljs-string">'image'</span>, img)
    k = cv2.<span class="hljs-title function_">waitKey</span>(<span class="hljs-number">100</span>) &#x26; <span class="hljs-number">0xff</span> # <span class="hljs-string">'ESC'</span> 키를 눌러 비디오 종료
    <span class="hljs-keyword">if</span> k == <span class="hljs-number">27</span>:
        <span class="hljs-keyword">break</span>
    elif count >= <span class="hljs-number">30</span>: # <span class="hljs-number">30</span>개의 얼굴 샘플 촬영 후 비디오 중지
         <span class="hljs-keyword">break</span>
# 청소 조금
<span class="hljs-title function_">print</span>(<span class="hljs-string">"\n [INFO] 프로그램 종료 및 정리 진행"</span>)
cam.<span class="hljs-title function_">release</span>()
cv2.<span class="hljs-title function_">destroyAllWindows</span>()
</code></pre>
<p>코드는 얼굴 감지를 위한 코드와 매우 유사합니다. 추가한 부분은 "사용자 ID를 캡처하기 위한 입력 명령"이며, 이는 정수 번호(1, 2, 3 등)여야 합니다.</p>
<pre><code class="hljs language-js">face_id = <span class="hljs-title function_">input</span>(<span class="hljs-string">'\n 사용자 ID를 입력하고 &#x3C;return>을 누르세요 ==> '</span>)
</code></pre>
<p>그리고 각 캡처된 프레임마다 "데이터셋" 디렉토리에 파일로 저장해야 합니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">cv2.<span class="hljs-title function_">imwrite</span>(<span class="hljs-string">"dataset/User."</span> + <span class="hljs-title function_">str</span>(face_id) + <span class="hljs-string">'.'</span> + <span class="hljs-title function_">str</span>(count) + <span class="hljs-string">".jpg"</span>, gray[<span class="hljs-attr">y</span>:y+h,<span class="hljs-attr">x</span>:x+w])
</code></pre>
<p>위의 파일을 저장하기 위해서는 라이브러리 "os"를 import해야 합니다. 각 파일의 이름은 다음과 같은 구조를 따릅니다:</p>
<pre><code class="hljs language-js"><span class="hljs-title class_">User</span>.<span class="hljs-property">face_id</span>.<span class="hljs-property">count</span>.<span class="hljs-property">jpg</span>
</code></pre>
<p>예를 들어, face_id가 1인 사용자의 경우, dataset/ 디렉토리에 있는 네 번째 샘플 파일은 아래와 같이 될 것입니다:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-title class_">User</span><span class="hljs-number">.1</span><span class="hljs-number">.4</span>.<span class="hljs-property">jpg</span>
</code></pre>
<p>PI 사진에서 보여지는 것과 같이:</p>
<p><img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_9.png" alt="Image"></p>
<p>내 코드에서는 각 ID로부터 30개의 샘플을 캡처하고 있습니다. 마지막 "elif"에서 이를 변경할 수 있습니다. 샘플의 수는 얼굴 샘플을 캡처하는 루프를 종료하는 데 사용됩니다.</p>
<div class="content-ad"></div>
<p>파이썬 스크립트를 실행하고 몇 개의 ID를 캡처하세요. 새 사용자를 집계하거나 이미 존재하는 사용자의 사진을 변경하려면 매번 스크립트를 실행해야 합니다.</p>
<h1>6. 트레이너</h1>
<p>이 두 번째 단계에서는 데이터셋에서 모든 사용자 데이터를 가져와 OpenCV Recognizer를 "트레이닝"해야 합니다. 이 작업은 특정한 OpenCV 함수를 사용하여 직접 수행됩니다. 결과는 "trainer/" 디렉토리에 저장된 .yml 파일이 될 것입니다.</p>
<p><img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_10.png" alt="이미지"></p>
<div class="content-ad"></div>
<p>그러면, 훈련 데이터를 저장할 하위 디렉토리를 만드는 것으로 시작해봐요:</p>
<pre><code class="hljs language-js">mkdir trainer
</code></pre>
<p>제 GitHub에서 두 번째 파이썬 스크립트를 다운로드하실 거에요: 02_face_training.py</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> <span class="hljs-variable constant_">PIL</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Image</span>
<span class="hljs-keyword">import</span> os
# 얼굴 이미지 데이터베이스 경로
path = <span class="hljs-string">'dataset'</span>
recognizer = cv2.<span class="hljs-property">face</span>.<span class="hljs-title class_">LBPHFaceRecognizer</span>_create()
detector = cv2.<span class="hljs-title class_">CascadeClassifier</span>(<span class="hljs-string">"haarcascade_frontalface_default.xml"</span>);
# 이미지 및 레이블 데이터 가져오는 함수
def <span class="hljs-title function_">getImagesAndLabels</span>(path):
    imagePaths = [os.<span class="hljs-property">path</span>.<span class="hljs-title function_">join</span>(path,f) <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> os.<span class="hljs-title function_">listdir</span>(path)]     
    faceSamples=[]
    ids = []
    <span class="hljs-keyword">for</span> imagePath <span class="hljs-keyword">in</span> <span class="hljs-attr">imagePaths</span>:
        PIL_img = <span class="hljs-title class_">Image</span>.<span class="hljs-title function_">open</span>(imagePath).<span class="hljs-title function_">convert</span>(<span class="hljs-string">'L'</span>) # 흑백
        img_numpy = np.<span class="hljs-title function_">array</span>(PIL_img,<span class="hljs-string">'uint8'</span>)
        id = <span class="hljs-title function_">int</span>(os.<span class="hljs-property">path</span>.<span class="hljs-title function_">split</span>(imagePath)[-<span class="hljs-number">1</span>].<span class="hljs-title function_">split</span>(<span class="hljs-string">"."</span>)[<span class="hljs-number">1</span>])
        faces = detector.<span class="hljs-title function_">detectMultiScale</span>(img_numpy)
        <span class="hljs-keyword">for</span> (x,y,w,h) <span class="hljs-keyword">in</span> <span class="hljs-attr">faces</span>:
            faceSamples.<span class="hljs-title function_">append</span>(img_numpy[<span class="hljs-attr">y</span>:y+h,<span class="hljs-attr">x</span>:x+w])
            ids.<span class="hljs-title function_">append</span>(id)
    <span class="hljs-keyword">return</span> faceSamples,ids
print (<span class="hljs-string">"\n [INFO] 얼굴을 학습 중입니다. 몇 초가 걸릴 것입니다. 기다려 주세요..."</span>)
faces,ids = <span class="hljs-title function_">getImagesAndLabels</span>(path)
recognizer.<span class="hljs-title function_">train</span>(faces, np.<span class="hljs-title function_">array</span>(ids))
# 모델을 trainer/trainer.<span class="hljs-property">yml</span>에 저장하기
recognizer.<span class="hljs-title function_">write</span>(<span class="hljs-string">'trainer/trainer.yml'</span>) 
# 학습된 얼굴 수 및 프로그램 종료 출력
<span class="hljs-title function_">print</span>(<span class="hljs-string">"\n [INFO] {0} 개의 얼굴을 학습했습니다. 프로그램을 종료합니다."</span>.<span class="hljs-title function_">format</span>(<span class="hljs-title function_">len</span>(np.<span class="hljs-title function_">unique</span>(ids))))
</code></pre>
<div class="content-ad"></div>
<p>라즈베리파이에 PIL 라이브러리가 설치되어 있는지 확인해주세요. 만약 설치되어 있지 않다면, 아래의 명령을 터미널에서 실행해주세요:</p>
<pre><code class="hljs language-js">pip install pillow
</code></pre>
<p>우리는 OpenCV 패키지에 포함된 LBPH (LOCAL BINARY PATTERNS HISTOGRAMS) 얼굴 인식기(recognizer)를 사용할 것입니다. 아래와 같이 코드를 작성해주세요:</p>
<pre><code class="hljs language-js">recognizer = cv2.<span class="hljs-property">face</span>.<span class="hljs-title class_">LBPHFaceRecognizer</span>_create()
</code></pre>
<div class="content-ad"></div>
<p>"getImagesAndLabels (path)" 함수는 "dataset/" 디렉토리의 모든 사진을 가져와서 "Ids"와 "faces" 두 가지 배열을 반환합니다. 이 배열을 입력으로 사용하여 "인식기를 학습"할 것입니다:</p>
<pre><code class="hljs language-js">recognizer.<span class="hljs-title function_">train</span>(faces, ids)
</code></pre>
<p>이 결과로 "trainer.yml"이라는 파일이 우리가 이전에 생성한 트레이너 디렉토리에 저장됩니다.</p>
<p>여기까지입니다! 훈련시킨 사용자의 얼굴 수를 확인하기 위해 마지막으로 출력 문을 포함했습니다.</p>
<div class="content-ad"></div>
<p>매번 Phase 1을 실행할 때마다 Phase 2도 실행해야 합니다.</p>
<h1>7. 인식기</h1>
<p>지금까지 우리 프로젝트의 마지막 단계에 도달했습니다. 여기서 우리는 카메라로 새로운 얼굴을 촬영하고, 이 사람이 이전에 그의 얼굴을 촬영하고 훈련했다면, 우리의 인식기는 "예측"을 수행하여 해당 ID와 일치 여부를 나타내는 지수를 반환합니다. 이로써 해당 일치에 대해 인식기가 얼마나 확신하는지 보여줍니다.</p>
<p><img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_11.png" alt="이미지"></p>
<div class="content-ad"></div>
<p>GitHub에서 3단계 Python 스크립트를 내려받아봅시다: 03_face_recognition.py.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> os 
recognizer = cv2.face.LBPHFaceRecognizer_create()
recognizer.read(<span class="hljs-string">'trainer/trainer.yml'</span>)
cascadePath = <span class="hljs-string">"haarcascade_frontalface_default.xml"</span>
faceCascade = cv2.CascadeClassifier(cascadePath)
font = cv2.FONT_HERSHEY_SIMPLEX
<span class="hljs-comment"># 아이디 카운터 초기화</span>
<span class="hljs-built_in">id</span> = <span class="hljs-number">0</span>
<span class="hljs-comment"># id에 대응하는 이름: 예) Marcelo: id=1,  등</span>
names = [<span class="hljs-string">'없음'</span>, <span class="hljs-string">'마르셀로'</span>, <span class="hljs-string">'파울라'</span>, <span class="hljs-string">'일자'</span>, <span class="hljs-string">'Z'</span>, <span class="hljs-string">'W'</span>] 
<span class="hljs-comment"># 실시간 비디오 캡처 시작</span>
cam = cv2.VideoCapture(<span class="hljs-number">0</span>)
cam.<span class="hljs-built_in">set</span>(<span class="hljs-number">3</span>, <span class="hljs-number">640</span>) <span class="hljs-comment"># 비디오 너비 설정</span>
cam.<span class="hljs-built_in">set</span>(<span class="hljs-number">4</span>, <span class="hljs-number">480</span>) <span class="hljs-comment"># 비디오 높이 설정</span>
<span class="hljs-comment"># 얼굴 인식으로 인정할 최소 윈도우 크기 정의</span>
minW = <span class="hljs-number">0.1</span>*cam.get(<span class="hljs-number">3</span>)
minH = <span class="hljs-number">0.1</span>*cam.get(<span class="hljs-number">4</span>)
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
    ret, img =cam.read()
    img = cv2.flip(img, -<span class="hljs-number">1</span>) <span class="hljs-comment"># 수직으로 뒤집기</span>
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    faces = faceCascade.detectMultiScale( 
        gray,
        scaleFactor = <span class="hljs-number">1.2</span>,
        minNeighbors = <span class="hljs-number">5</span>,
        minSize = (<span class="hljs-built_in">int</span>(minW), <span class="hljs-built_in">int</span>(minH)),
       )
    <span class="hljs-keyword">for</span>(x,y,w,h) <span class="hljs-keyword">in</span> faces:
        cv2.rectangle(img, (x,y), (x+w,y+h), (<span class="hljs-number">0</span>,<span class="hljs-number">255</span>,<span class="hljs-number">0</span>), <span class="hljs-number">2</span>)
        <span class="hljs-built_in">id</span>, confidence = recognizer.predict(gray[y:y+h,x:x+w])
        <span class="hljs-comment"># 신뢰도가 100 미만이면 "0" : 완벽 일치</span>
        <span class="hljs-keyword">if</span> (confidence &#x3C; <span class="hljs-number">100</span>):
            <span class="hljs-built_in">id</span> = names[<span class="hljs-built_in">id</span>]
            confidence = <span class="hljs-string">"  {0}%"</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">round</span>(<span class="hljs-number">100</span> - confidence))
        <span class="hljs-keyword">else</span>:
            <span class="hljs-built_in">id</span> = <span class="hljs-string">"알수없음"</span>
            confidence = <span class="hljs-string">"  {0}%"</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">round</span>(<span class="hljs-number">100</span> - confidence))
        
        cv2.putText(
                    img, 
                    <span class="hljs-built_in">str</span>(<span class="hljs-built_in">id</span>), 
                    (x+<span class="hljs-number">5</span>,y-<span class="hljs-number">5</span>), 
                    font, 
                    <span class="hljs-number">1</span>, 
                    (<span class="hljs-number">255</span>,<span class="hljs-number">255</span>,<span class="hljs-number">255</span>), 
                    <span class="hljs-number">2</span>
                   )
        cv2.putText(
                    img, 
                    <span class="hljs-built_in">str</span>(confidence), 
                    (x+<span class="hljs-number">5</span>,y+h-<span class="hljs-number">5</span>), 
                    font, 
                    <span class="hljs-number">1</span>, 
                    (<span class="hljs-number">255</span>,<span class="hljs-number">255</span>,<span class="hljs-number">0</span>), 
                    <span class="hljs-number">1</span>
                   )  
    
    cv2.imshow(<span class="hljs-string">'camera'</span>,img) 
    k = cv2.waitKey(<span class="hljs-number">10</span>) &#x26; <span class="hljs-number">0xff</span> <span class="hljs-comment"># 'ESC'를 눌러 비디오 종료</span>
    <span class="hljs-keyword">if</span> k == <span class="hljs-number">27</span>:
        <span class="hljs-keyword">break</span>
<span class="hljs-comment"># 정리 작업</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"\n [INFO] 프로그램 종료 및 정리 작업"</span>)
cam.release()
cv2.destroyAllWindows()
</code></pre>
<p>여기에 새 배열을 추가했으므로, 숫자로 된 ID 대신 "names"를 표시할 것입니다:</p>
<pre><code class="hljs language-python">names = [<span class="hljs-string">'없음'</span>, <span class="hljs-string">'마르셀로'</span>, <span class="hljs-string">'파울라'</span>, <span class="hljs-string">'일자'</span>, <span class="hljs-string">'Z'</span>, <span class="hljs-string">'W'</span>]
</code></pre>
<div class="content-ad"></div>
<p>예를 들어, Marcelo는 id=1인 사용자이고, Paula는 id=2인 등입니다.</p>
<p>다음으로, 우리는 얼굴을 감지할 것이며, 이전과 같이 haasCascade 분류기를 사용할 것입니다. 감지된 얼굴을 가지고 우리는 위의 코드에서 가장 중요한 함수를 호출할 수 있습니다:</p>
<pre><code class="hljs language-js">id, confidence = recognizer.<span class="hljs-title function_">predict</span>(얼굴의 회색 부분)
</code></pre>
<p>recognizer.predict()는 분석할 얼굴의 캡처된 부분을 매개변수로 사용하고, 해당 소유자와 일치에 대한 신뢰도를 나타내는 id 및 인식기의 신뢰도 값을 반환할 것입니다.</p>
<div class="content-ad"></div>
<p>그리고 마지막으로, 인식기가 얼굴을 예측할 수 있다면, 이미지 위에 “확률적 ID”와 일치가 올바른지에 대한 “확률”이 표시됩니다 (“확률” = 100 - 신뢰도 지수). 그렇지 않으면, “알 수 없음” 레이블이 얼굴 위에 표시됩니다.</p>
<p>아래는 결과를 보여주는 gif입니다:</p>
<p><img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_12.png" alt="결과"></p>
<p>이 사진에서는 이 프로젝트로 수행한 일부 테스트를 보여드립니다. 여기서 인식기가 작동하는지 확인하기 위해 사진을 사용했습니다.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_13.png">
<h1>8. 결론</h1>
<img src="/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_14.png">
<p>언제나처럼, 이 프로젝트가 다른 사람들이 전자기술의 흥미로운 세계로 진입하는 데 도움이 되기를 바랍니다!</p>
<div class="content-ad"></div>
<p>세부 정보 및 최종 코드는 제 GitHub 저장소를 방문해주세요:</p>
<p><a href="https://github.com/Mjrovai/OpenCV-Face-Recognition" rel="nofollow" target="_blank">https://github.com/Mjrovai/OpenCV-Face-Recognition</a></p>
<p>더 많은 프로젝트를 보시려면 제 블로그를 방문해주세요: MJRoBot.org</p>
<p>이른 아침 인사드립니다!</p>
<div class="content-ad"></div>
<p>다음 글에서 봐요!</p>
<p>감사합니다,</p>
<p>마르셀로</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"실시간 얼굴 인식 끝에서 끝까지의 프로젝트","description":"","date":"2024-06-19 06:12","slug":"2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject","content":"\n\n단계별로 배우세요! PiCam을 사용하여 실시간으로 얼굴을 인식하는 방법을 배워보세요.\n\n![PiCam Image](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_0.png)\n\n# 1. 소개\n\nOpenCV를 탐구하는 내 교재에서는 자동 비전 객체 추적을 배웠습니다. 이제 PiCam을 사용하여 실시간으로 얼굴을 인식해보겠습니다. 위에서 볼 수 있듯이, 함께해요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_1.png)\n\n이 프로젝트는 이 훌륭한 \"Open Source Computer Vision Library\" 인 OpenCV를 사용하여 수행되었습니다. 이 튜토리얼에서는 Raspberry Pi (즉, Raspbian을 사용한 OS)와 Python에 중점을 두지만 Mac에서 코드를 테스트하고 또한 잘 작동하는 것을 확인했습니다.\n\nOpenCV는 계산 효율성을 위해 설계되었으며 실시간 애플리케이션에 중점을 두고 있습니다. 따라서 카메라를 사용한 실시간 얼굴 인식에 적합합니다.\n\n## 3 단계\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n얼굴인식에 대한 완전한 프로젝트를 생성하려면 3가지 매우 다른 단계에서 작업해야 합니다:\n\n- 얼굴 감지 및 데이터 수집\n- 인식기 훈련\n- 얼굴 인식\n\n다음 블록 다이어그램은 이러한 단계들을 요약합니다:\n\n![Face Recognition Project Phases](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2. OpenCV 3 패키지 설치\n\n저는 최신 버전의 라스비안(Stretch)이 설치된 라즈베리 파이 V3를 사용하고 있습니다. 따라서 OpenCV를 설치하는 가장 좋은 방법은 Adrian Rosebrock이 개발한 훌륭한 튜토리얼을 따라하는 것입니다: \"Raspbian Stretch: 라즈베리 파이에 OpenCV 3 + Python 설치\".\n\nAdrian의 튜토리얼을 완료하면 라즈베리 파이에서 실험을 실행할 준비가 된 OpenCV 가상 환경이 준비됩니다.\n\n이제 가상 환경으로 이동하여 OpenCV 3이 올바르게 설치되었는지 확인해 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n에드리안이 새 터미널을 열 때마다 \"source\" 명령을 실행하여 시스템 변수가 올바르게 설정되었는지 확인하는 것을 권장합니다.\n\n```js\nsource ~/.profile\n```\n\n다음으로, 가상 환경에 들어가 봅시다:\n\n```js\nworkon cv\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 당신이 cv 가상환경 앞에 있는 텍스트를 보신다면, cv 가상환경 안에 있습니다:\n\n```js\n(cv) pi@raspberry:~$\n```\n\n이제 파이썬 인터프리터로 들어가보세요:\n\n```js\npython\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희가 현재 3.5 버전 (또는 그 이상)을 실행 중이라고 확인해주세요.\n\n인터프리터 안에 ( 가 표시될 것입니다), OpenCV 라이브러리를 import 해주세요:\n\n```js\nimport cv2\n```\n\n만약 에러 메시지가 나타나지 않는다면, OpenCV가 정확하게 파이썬 가상 환경에 설치된 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n설치된 OpenCV 버전을 확인할 수도 있어요:\n\n```js\ncv2.__version__\n```\n\n3.3.0이 표시되어야 해요 (또는 미래에 출시될 우수한 버전). \n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_3.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 터미널 스크린샷은 이전 단계를 보여줍니다.\n\n# 3. 카메라 테스트\n\nRPi에 OpenCV를 설치했다면 카메라가 제대로 작동하는지 확인하기 위해 테스트를 해봅시다.\n\n이미 PiCam을 설치하고 활성화했다는 것을 전제로 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 Python 코드를 IDE에 입력해보세요:\n\n```js\nimport numpy as np\nimport cv2\ncap = cv2.VideoCapture(0)\ncap.set(3,640) # 너비 설정\ncap.set(4,480) # 높이 설정\nwhile(True):\n    ret, frame = cap.read()\n    frame = cv2.flip(frame, -1) # 카메라 세로로 뒤집기\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    \n    cv2.imshow('frame', frame)\n    cv2.imshow('gray', gray)\n    \n    k = cv2.waitKey(30) \u0026 0xff\n    if k == 27: # 'ESC' 키를 눌러 종료\n        break\ncap.release()\ncv2.destroyAllWindows()\n```\n\n위 코드는 PiCam에서 생성된 비디오 스트림을 캡처하여 BGR 색상과 회색 모드로 모두 표시합니다.\n\n또는 GitHub에서 코드를 다운로드할 수도 있습니다: simpleCamTest.py\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해당 스크립트를 실행하려면 다음 명령어를 입력해주세요:\n\n```js\npython simpleCamTest.py\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_4.png\" /\u003e\n\n위의 그림은 결과를 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일부 사용자가 카메라를 열려고 시도할 때 문제가 발생하고 \"Assertion failed\" 오류 메시지가 표시된다는 것을 발겠어요. 이것은 카메라가 OpenCV 설치 중에 활성화되지 않았거나 카메라 드라이버가 올바르게 설치되지 않아 발생할 수 있습니다. 수정하려면 다음 명령을 사용하세요:\n\n```js\nsudo modprobe bcm2835-v4l2\n```\n\nOpenCV에 대해 더 알고 싶다면 다음 튜토리얼을 참고해보세요: loading-video-python-opencv-tutorial\n\n# 4. 얼굴 인식\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nFace Recognition에서 가장 기본적인 작업은 물론 \"얼굴 감지\"입니다. 무엇보다 먼저, 미래에 캡처된 새 얼굴과 비교할 수 있도록 얼굴을 캡처해야 합니다(Phase 1).\n\n(혹시 이전에 말한 'Capture'부분 해석이 잘못된 것 같아 원문에서 대체했습니다.)\n\n한 객체를 감지하는 가장 일반적인 방법은 \"Haar Cascade classifier\"를 사용하는 것입니다.\n\n\"Haar 특징 기반 캐스케이드 분류기\"를 사용한 객체 감지는 Paul Viola와 Michael Jones가 2001년에 발표한 논문 \"Rapid Object Detection using a Boosted Cascade of Simple Features\"에서 제안된 효과적인 객체 감지 방법입니다. 이는 많은 양의 긍정 이미지와 부정 이미지로부터 케스케이드 함수가 훈련된 기계 학습 기반 방법으로, 다른 이미지에서 객체를 감지하는 데 사용됩니다.\n\n여기서는 얼굴 감지 작업을 수행할 것입니다. 먼저, 알고리즘에는 분류기를 훈련시키기 위해 많은 양의 양성 이미지(얼굴 이미지)와 음성 이미지(얼굴이 없는 이미지)가 필요합니다. 그런 다음 그로부터 특징을 추출해야 합니다. 좋은 소식은 OpenCV에 트레이너와 탐지기가 함께 제공된다는 것입니다. 자동차, 비행기 등의 모든 객체에 대해 자체 분류기를 훈련시키고 싶다면 OpenCV를 사용해 생성할 수 있습니다. 자세한 내용은 여기를 참고하십시오: 캐스케이드 분류기 훈련.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 여러분이 자체 분류기를 만들고 싶지 않다면, OpenCV에는 이미 얼굴, 눈, 웃음 등을 위한 사전 훈련된 분류기가 많이 포함되어 있습니다. 이러한 XML 파일은 haarcascades 디렉토리에서 다운로드할 수 있습니다.\n\n이론은 이만하고, 이제 OpenCV를 사용하여 얼굴 탐지기를 만들어 봅시다!\n\n제 GitHub에서 파일 faceDetection.py를 다운로드하세요.\n\n```python\nimport numpy as np\nimport cv2\nfaceCascade = cv2.CascadeClassifier('Cascades/haarcascade_frontalface_default.xml')\ncap = cv2.VideoCapture(0)\ncap.set(3,640) # 너비 설정\ncap.set(4,480) # 높이 설정\nwhile True:\n    ret, img = cap.read()\n    img = cv2.flip(img, -1)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = faceCascade.detectMultiScale(\n        gray,\n        scaleFactor=1.2,\n        minNeighbors=5,\n        minSize=(20, 20)\n    )\n    for (x,y,w,h) in faces:\n        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n        roi_gray = gray[y:y+h, x:x+w]\n        roi_color = img[y:y+h, x:x+w]\n    cv2.imshow('video',img)\n    k = cv2.waitKey(30) \u0026 0xff\n    if k == 27: # 'ESC'를 눌러 종료\n        break\ncap.release()\ncv2.destroyAllWindows()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n믿을 수 있건대, Python과 OpenCV를 사용하여 얼굴을 감지하는 데 필요한 코드는 바로 위의 몇 줄 뿐입니다.\n\n카메라를 테스트하는 데 사용된 마지막 코드와 비교해보면, 약간의 부분이 추가되었음을 깨달을 것입니다. 아래의 줄을 주목해주세요:\n\n```js\nfaceCascade = cv2.CascadeClassifier('Cascades/haarcascade_frontalface_default.xml')\n```\n\n이 줄은 \"classifier\"를 로드하는 줄입니다 (프로젝트 디렉토리 하위에 \"Cascades/\"라는 디렉토리에 있어야 합니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼, 우리는 카메라를 설정하고 루프 내에서 입력 비디오를 회색조 모드로 불러올 것입니다 (이전에 보았던 것과 동일합니다).\n\n이제 분류기 함수를 호출해야 하는데, 이때 매우 중요한 매개변수들을 전달해주어야 합니다. 이들은 scale factor, 이웃의 수, 그리고 감지된 얼굴의 최소 크기입니다.\n\n```js\nfaces = faceCascade.detectMultiScale(\n        gray,     \n        scaleFactor=1.2,\n        minNeighbors=5,     \n        minSize=(20, 20)\n        )\n```\n\n여기서,\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- gray는 입력 그레이스케일 이미지입니다.\n- scaleFactor는 각 이미지 스케일에서 이미지 크기가 감소하는 정도를 지정하는 매개 변수입니다. 스케일 피라미드를 만드는 데 사용됩니다.\n- minNeighbors는 각 후보 사각형이 유지해야 하는 이웃 수를 지정하는 매개 변수입니다. 숫자가 높을수록 낮은 거짓 양성이 발생합니다.\n- minSize는 고려해야 할 최소 사각형 크기입니다.\n\n함수는 이미지에서 얼굴을 감지합니다. 다음으로 이미지에서 얼굴을 \"표시\"해야 합니다. 예를 들어, 파란색 사각형 등을 사용합니다. 이 작업은 다음 코드 부분으로 수행됩니다:\n\n```js\nfor (x,y,w,h) in faces:\n    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n    roi_gray = gray[y:y+h, x:x+w]\n    roi_color = img[y:y+h, x:x+w]\n```\n\n얼굴이 발견되면, 감지된 얼굴의 위치를 왼쪽 위 모서리인 (x,y)로 하는 네모 상자로 반환하며, \"w\"를 너비, \"h\"를 높이로 가집니다 == (x, y, w, h). 사진을 참조하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_5.png\" /\u003e\n\n이 위치들을 얻으면, 얼굴을 위한 \"ROI\" (사각형으로 그린)를 만들고 imshow() 함수를 사용하여 결과를 표시할 수 있습니다.\n\nRpi 터미널을 사용하여 위의 Python 스크립트를 Python 환경에서 실행해보세요:\n\n```js\npython faceDetection.py\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과:\n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_6.png\" /\u003e\n\n당신은 \"눈 탐지\" 또는 \"미소 탐지\"를 위한 분류기도 포함할 수 있습니다. 해당 경우에는 얼굴 루프 내에 분류기 함수와 사각형 그리기를 포함해야 합니다. 왜냐하면 얼굴 외부에서 눈이나 미소를 탐지하는 것은 의미가 없기 때문입니다.\n\n## 예시\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내 GitHub에서 다른 예제들을 찾을 수 있어요:\n\n- faceEyeDetection.py\n- faceSmileDetection.py\n- faceSmileEyeDetection.py\n\n그리고 사진에서 결과를 확인할 수 있어요.\n\n![image](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 튜토리얼을 따라해서 얼굴 감지에 대해 더 잘 이해할 수 있어요:\n\nHaar Cascade Object Detection Face \u0026 Eye OpenCV Python Tutorial\n\n# 5. 데이터 수집\n\n우선, 사진을 통한 얼굴 인식에 대한 람리즈 라자의 훌륭한 작업에 감사드려야 해요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nOPENCV와 Python을 사용한 얼굴 인식: 초보자를 위한 안내서\n\n그리고 비디오를 사용해 매우 포괄적인 튜토리얼을 개발한 Anirban Kar:\n\n얼굴 인식 - 3 부분\n\n두 튜토리얼을 꼭 확인해보시기를 적극 추천합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그렇게 말씀하시면 프로젝트의 첫 단계를 시작하겠습니다. 여기서 우리가 할 일은 마지막 단계(얼굴 감지)부터 시작하여 간단히 데이터 세트를 만드는 것입니다. 각 ID에 대해, 얼굴 감지에 사용된 부분이 회색으로 표시된 사진 그룹을 저장할 것입니다.\n\n![이미지](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_8.png)\n\n먼저, 프로젝트를 개발할 디렉터리를 만드세요. 예를 들어, FacialRecognitionProject:\n\n```js\nmkdir FacialRecognitionProject\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 디렉토리에는 프로젝트용으로 만들 3개의 Python 스크립트뿐만 아니라 Facial Classifier도 저장해 두어야 해요. 이를 다운로드할 수 있는 GitHub 링크는 haarcascade_frontalface_default.xml입니다.\n\n그리고 우리의 얼굴 샘플을 저장할 하위 디렉토리를 만들어 이름을 \"dataset\"으로 지어줘요:\n\n```bash\nmkdir dataset\n```\n\n그리고 저의 GitHub에서 코드를 다운로드하세요: 01_face_dataset.py\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport cv2\nimport os\ncam = cv2.VideoCapture(0)\ncam.set(3, 640) # 비디오 너비 설정\ncam.set(4, 480) # 비디오 높이 설정\nface_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n# 각 사람에 대해 하나의 숫자 얼굴 ID 입력\nface_id = input('\\n 사용자 ID를 입력하고 \u003creturn\u003e을 누르세요 ==\u003e ')\nprint(\"\\n [INFO] 얼굴 캡처 초기화. 카메라를 응시하고 기다리세요 ...\")\n# 개별 샘플링 얼굴 카운트 초기화\ncount = 0\nwhile(True):\n    ret, img = cam.read()\n    img = cv2.flip(img, -1) # 비디오 이미지 수직으로 뒤집기\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = face_detector.detectMultiScale(gray, 1.3, 5)\n    for (x,y,w,h) in faces:\n        cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)     \n        count += 1\n        # 캡처된 이미지를 데이터셋 폴더에 저장\n        cv2.imwrite(\"dataset/User.\" + str(face_id) + '.' +  \n                    str(count) + \".jpg\", gray[y:y+h,x:x+w])\n        cv2.imshow('image', img)\n    k = cv2.waitKey(100) \u0026 0xff # 'ESC' 키를 눌러 비디오 종료\n    if k == 27:\n        break\n    elif count \u003e= 30: # 30개의 얼굴 샘플 촬영 후 비디오 중지\n         break\n# 청소 조금\nprint(\"\\n [INFO] 프로그램 종료 및 정리 진행\")\ncam.release()\ncv2.destroyAllWindows()\n``` \n\n코드는 얼굴 감지를 위한 코드와 매우 유사합니다. 추가한 부분은 \"사용자 ID를 캡처하기 위한 입력 명령\"이며, 이는 정수 번호(1, 2, 3 등)여야 합니다.\n\n```js\nface_id = input('\\n 사용자 ID를 입력하고 \u003creturn\u003e을 누르세요 ==\u003e ')\n```\n\n그리고 각 캡처된 프레임마다 \"데이터셋\" 디렉토리에 파일로 저장해야 합니다. \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ncv2.imwrite(\"dataset/User.\" + str(face_id) + '.' + str(count) + \".jpg\", gray[y:y+h,x:x+w])\n```\n\n위의 파일을 저장하기 위해서는 라이브러리 \"os\"를 import해야 합니다. 각 파일의 이름은 다음과 같은 구조를 따릅니다:\n\n```js\nUser.face_id.count.jpg\n```\n\n예를 들어, face_id가 1인 사용자의 경우, dataset/ 디렉토리에 있는 네 번째 샘플 파일은 아래와 같이 될 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nUser.1.4.jpg\n```\n\nPI 사진에서 보여지는 것과 같이:\n\n\n![Image](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_9.png)\n\n내 코드에서는 각 ID로부터 30개의 샘플을 캡처하고 있습니다. 마지막 \"elif\"에서 이를 변경할 수 있습니다. 샘플의 수는 얼굴 샘플을 캡처하는 루프를 종료하는 데 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파이썬 스크립트를 실행하고 몇 개의 ID를 캡처하세요. 새 사용자를 집계하거나 이미 존재하는 사용자의 사진을 변경하려면 매번 스크립트를 실행해야 합니다.\n\n# 6. 트레이너\n\n이 두 번째 단계에서는 데이터셋에서 모든 사용자 데이터를 가져와 OpenCV Recognizer를 \"트레이닝\"해야 합니다. 이 작업은 특정한 OpenCV 함수를 사용하여 직접 수행됩니다. 결과는 \"trainer/\" 디렉토리에 저장된 .yml 파일이 될 것입니다.\n\n![이미지](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러면, 훈련 데이터를 저장할 하위 디렉토리를 만드는 것으로 시작해봐요:\n\n```js\nmkdir trainer\n```\n\n제 GitHub에서 두 번째 파이썬 스크립트를 다운로드하실 거에요: 02_face_training.py\n\n```js\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport os\n# 얼굴 이미지 데이터베이스 경로\npath = 'dataset'\nrecognizer = cv2.face.LBPHFaceRecognizer_create()\ndetector = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\");\n# 이미지 및 레이블 데이터 가져오는 함수\ndef getImagesAndLabels(path):\n    imagePaths = [os.path.join(path,f) for f in os.listdir(path)]     \n    faceSamples=[]\n    ids = []\n    for imagePath in imagePaths:\n        PIL_img = Image.open(imagePath).convert('L') # 흑백\n        img_numpy = np.array(PIL_img,'uint8')\n        id = int(os.path.split(imagePath)[-1].split(\".\")[1])\n        faces = detector.detectMultiScale(img_numpy)\n        for (x,y,w,h) in faces:\n            faceSamples.append(img_numpy[y:y+h,x:x+w])\n            ids.append(id)\n    return faceSamples,ids\nprint (\"\\n [INFO] 얼굴을 학습 중입니다. 몇 초가 걸릴 것입니다. 기다려 주세요...\")\nfaces,ids = getImagesAndLabels(path)\nrecognizer.train(faces, np.array(ids))\n# 모델을 trainer/trainer.yml에 저장하기\nrecognizer.write('trainer/trainer.yml') \n# 학습된 얼굴 수 및 프로그램 종료 출력\nprint(\"\\n [INFO] {0} 개의 얼굴을 학습했습니다. 프로그램을 종료합니다.\".format(len(np.unique(ids))))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n라즈베리파이에 PIL 라이브러리가 설치되어 있는지 확인해주세요. 만약 설치되어 있지 않다면, 아래의 명령을 터미널에서 실행해주세요:\n\n```js\npip install pillow\n```\n\n우리는 OpenCV 패키지에 포함된 LBPH (LOCAL BINARY PATTERNS HISTOGRAMS) 얼굴 인식기(recognizer)를 사용할 것입니다. 아래와 같이 코드를 작성해주세요:\n\n```js\nrecognizer = cv2.face.LBPHFaceRecognizer_create()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"getImagesAndLabels (path)\" 함수는 \"dataset/\" 디렉토리의 모든 사진을 가져와서 \"Ids\"와 \"faces\" 두 가지 배열을 반환합니다. 이 배열을 입력으로 사용하여 \"인식기를 학습\"할 것입니다:\n\n```js\nrecognizer.train(faces, ids)\n```\n\n이 결과로 \"trainer.yml\"이라는 파일이 우리가 이전에 생성한 트레이너 디렉토리에 저장됩니다.\n\n여기까지입니다! 훈련시킨 사용자의 얼굴 수를 확인하기 위해 마지막으로 출력 문을 포함했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n매번 Phase 1을 실행할 때마다 Phase 2도 실행해야 합니다.\n\n# 7. 인식기\n\n지금까지 우리 프로젝트의 마지막 단계에 도달했습니다. 여기서 우리는 카메라로 새로운 얼굴을 촬영하고, 이 사람이 이전에 그의 얼굴을 촬영하고 훈련했다면, 우리의 인식기는 \"예측\"을 수행하여 해당 ID와 일치 여부를 나타내는 지수를 반환합니다. 이로써 해당 일치에 대해 인식기가 얼마나 확신하는지 보여줍니다.\n\n![이미지](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_11.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGitHub에서 3단계 Python 스크립트를 내려받아봅시다: 03_face_recognition.py.\n\n```python\nimport cv2\nimport numpy as np\nimport os \nrecognizer = cv2.face.LBPHFaceRecognizer_create()\nrecognizer.read('trainer/trainer.yml')\ncascadePath = \"haarcascade_frontalface_default.xml\"\nfaceCascade = cv2.CascadeClassifier(cascadePath)\nfont = cv2.FONT_HERSHEY_SIMPLEX\n# 아이디 카운터 초기화\nid = 0\n# id에 대응하는 이름: 예) Marcelo: id=1,  등\nnames = ['없음', '마르셀로', '파울라', '일자', 'Z', 'W'] \n# 실시간 비디오 캡처 시작\ncam = cv2.VideoCapture(0)\ncam.set(3, 640) # 비디오 너비 설정\ncam.set(4, 480) # 비디오 높이 설정\n# 얼굴 인식으로 인정할 최소 윈도우 크기 정의\nminW = 0.1*cam.get(3)\nminH = 0.1*cam.get(4)\nwhile True:\n    ret, img =cam.read()\n    img = cv2.flip(img, -1) # 수직으로 뒤집기\n    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    \n    faces = faceCascade.detectMultiScale( \n        gray,\n        scaleFactor = 1.2,\n        minNeighbors = 5,\n        minSize = (int(minW), int(minH)),\n       )\n    for(x,y,w,h) in faces:\n        cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n        id, confidence = recognizer.predict(gray[y:y+h,x:x+w])\n        # 신뢰도가 100 미만이면 \"0\" : 완벽 일치\n        if (confidence \u003c 100):\n            id = names[id]\n            confidence = \"  {0}%\".format(round(100 - confidence))\n        else:\n            id = \"알수없음\"\n            confidence = \"  {0}%\".format(round(100 - confidence))\n        \n        cv2.putText(\n                    img, \n                    str(id), \n                    (x+5,y-5), \n                    font, \n                    1, \n                    (255,255,255), \n                    2\n                   )\n        cv2.putText(\n                    img, \n                    str(confidence), \n                    (x+5,y+h-5), \n                    font, \n                    1, \n                    (255,255,0), \n                    1\n                   )  \n    \n    cv2.imshow('camera',img) \n    k = cv2.waitKey(10) \u0026 0xff # 'ESC'를 눌러 비디오 종료\n    if k == 27:\n        break\n# 정리 작업\nprint(\"\\n [INFO] 프로그램 종료 및 정리 작업\")\ncam.release()\ncv2.destroyAllWindows()\n```\n\n여기에 새 배열을 추가했으므로, 숫자로 된 ID 대신 \"names\"를 표시할 것입니다:\n\n```python\nnames = ['없음', '마르셀로', '파울라', '일자', 'Z', 'W']\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, Marcelo는 id=1인 사용자이고, Paula는 id=2인 등입니다.\n\n다음으로, 우리는 얼굴을 감지할 것이며, 이전과 같이 haasCascade 분류기를 사용할 것입니다. 감지된 얼굴을 가지고 우리는 위의 코드에서 가장 중요한 함수를 호출할 수 있습니다:\n\n```js\nid, confidence = recognizer.predict(얼굴의 회색 부분)\n```\n\nrecognizer.predict()는 분석할 얼굴의 캡처된 부분을 매개변수로 사용하고, 해당 소유자와 일치에 대한 신뢰도를 나타내는 id 및 인식기의 신뢰도 값을 반환할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 마지막으로, 인식기가 얼굴을 예측할 수 있다면, 이미지 위에 “확률적 ID”와 일치가 올바른지에 대한 “확률”이 표시됩니다 (“확률” = 100 - 신뢰도 지수). 그렇지 않으면, “알 수 없음” 레이블이 얼굴 위에 표시됩니다.\n\n아래는 결과를 보여주는 gif입니다:\n\n![결과](/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_12.png)\n\n이 사진에서는 이 프로젝트로 수행한 일부 테스트를 보여드립니다. 여기서 인식기가 작동하는지 확인하기 위해 사진을 사용했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_13.png\" /\u003e\n\n# 8. 결론\n\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_14.png\" /\u003e\n\n언제나처럼, 이 프로젝트가 다른 사람들이 전자기술의 흥미로운 세계로 진입하는 데 도움이 되기를 바랍니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n세부 정보 및 최종 코드는 제 GitHub 저장소를 방문해주세요:\n\nhttps://github.com/Mjrovai/OpenCV-Face-Recognition\n\n더 많은 프로젝트를 보시려면 제 블로그를 방문해주세요: MJRoBot.org\n\n이른 아침 인사드립니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 글에서 봐요!\n\n감사합니다,\n\n마르셀로","ogImage":{"url":"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_0.png"},"coverImage":"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_0.png","tag":["Tech"],"readingTime":16},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e단계별로 배우세요! PiCam을 사용하여 실시간으로 얼굴을 인식하는 방법을 배워보세요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_0.png\" alt=\"PiCam Image\"\u003e\u003c/p\u003e\n\u003ch1\u003e1. 소개\u003c/h1\u003e\n\u003cp\u003eOpenCV를 탐구하는 내 교재에서는 자동 비전 객체 추적을 배웠습니다. 이제 PiCam을 사용하여 실시간으로 얼굴을 인식해보겠습니다. 위에서 볼 수 있듯이, 함께해요!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_1.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e이 프로젝트는 이 훌륭한 \"Open Source Computer Vision Library\" 인 OpenCV를 사용하여 수행되었습니다. 이 튜토리얼에서는 Raspberry Pi (즉, Raspbian을 사용한 OS)와 Python에 중점을 두지만 Mac에서 코드를 테스트하고 또한 잘 작동하는 것을 확인했습니다.\u003c/p\u003e\n\u003cp\u003eOpenCV는 계산 효율성을 위해 설계되었으며 실시간 애플리케이션에 중점을 두고 있습니다. 따라서 카메라를 사용한 실시간 얼굴 인식에 적합합니다.\u003c/p\u003e\n\u003ch2\u003e3 단계\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e얼굴인식에 대한 완전한 프로젝트를 생성하려면 3가지 매우 다른 단계에서 작업해야 합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e얼굴 감지 및 데이터 수집\u003c/li\u003e\n\u003cli\u003e인식기 훈련\u003c/li\u003e\n\u003cli\u003e얼굴 인식\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e다음 블록 다이어그램은 이러한 단계들을 요약합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_2.png\" alt=\"Face Recognition Project Phases\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e2. OpenCV 3 패키지 설치\u003c/h1\u003e\n\u003cp\u003e저는 최신 버전의 라스비안(Stretch)이 설치된 라즈베리 파이 V3를 사용하고 있습니다. 따라서 OpenCV를 설치하는 가장 좋은 방법은 Adrian Rosebrock이 개발한 훌륭한 튜토리얼을 따라하는 것입니다: \"Raspbian Stretch: 라즈베리 파이에 OpenCV 3 + Python 설치\".\u003c/p\u003e\n\u003cp\u003eAdrian의 튜토리얼을 완료하면 라즈베리 파이에서 실험을 실행할 준비가 된 OpenCV 가상 환경이 준비됩니다.\u003c/p\u003e\n\u003cp\u003e이제 가상 환경으로 이동하여 OpenCV 3이 올바르게 설치되었는지 확인해 보겠습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e에드리안이 새 터미널을 열 때마다 \"source\" 명령을 실행하여 시스템 변수가 올바르게 설정되었는지 확인하는 것을 권장합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003esource ~/.\u003cspan class=\"hljs-property\"\u003eprofile\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e다음으로, 가상 환경에 들어가 봅시다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eworkon cv\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e만약 당신이 cv 가상환경 앞에 있는 텍스트를 보신다면, cv 가상환경 안에 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e(cv) pi@\u003cspan class=\"hljs-attr\"\u003eraspberry\u003c/span\u003e:~$\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 파이썬 인터프리터로 들어가보세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epython\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e저희가 현재 3.5 버전 (또는 그 이상)을 실행 중이라고 확인해주세요.\u003c/p\u003e\n\u003cp\u003e인터프리터 안에 ( 가 표시될 것입니다), OpenCV 라이브러리를 import 해주세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e cv2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e만약 에러 메시지가 나타나지 않는다면, OpenCV가 정확하게 파이썬 가상 환경에 설치된 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e설치된 OpenCV 버전을 확인할 수도 있어요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ecv2.\u003cspan class=\"hljs-property\"\u003e__version__\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e3.3.0이 표시되어야 해요 (또는 미래에 출시될 우수한 버전).\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_3.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e위의 터미널 스크린샷은 이전 단계를 보여줍니다.\u003c/p\u003e\n\u003ch1\u003e3. 카메라 테스트\u003c/h1\u003e\n\u003cp\u003eRPi에 OpenCV를 설치했다면 카메라가 제대로 작동하는지 확인하기 위해 테스트를 해봅시다.\u003c/p\u003e\n\u003cp\u003e이미 PiCam을 설치하고 활성화했다는 것을 전제로 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래 Python 코드를 IDE에 입력해보세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e cv2\ncap = cv2.\u003cspan class=\"hljs-title class_\"\u003eVideoCapture\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\ncap.\u003cspan class=\"hljs-title function_\"\u003eset\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e640\u003c/span\u003e) # 너비 설정\ncap.\u003cspan class=\"hljs-title function_\"\u003eset\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e480\u003c/span\u003e) # 높이 설정\n\u003cspan class=\"hljs-keyword\"\u003ewhile\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e):\n    ret, frame = cap.\u003cspan class=\"hljs-title function_\"\u003eread\u003c/span\u003e()\n    frame = cv2.\u003cspan class=\"hljs-title function_\"\u003eflip\u003c/span\u003e(frame, -\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) # 카메라 세로로 뒤집기\n    gray = cv2.\u003cspan class=\"hljs-title function_\"\u003ecvtColor\u003c/span\u003e(frame, cv2.\u003cspan class=\"hljs-property\"\u003eCOLOR_BGR2GRAY\u003c/span\u003e)\n    \n    cv2.\u003cspan class=\"hljs-title function_\"\u003eimshow\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'frame'\u003c/span\u003e, frame)\n    cv2.\u003cspan class=\"hljs-title function_\"\u003eimshow\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'gray'\u003c/span\u003e, gray)\n    \n    k = cv2.\u003cspan class=\"hljs-title function_\"\u003ewaitKey\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e) \u0026#x26; \u003cspan class=\"hljs-number\"\u003e0xff\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e k == \u003cspan class=\"hljs-number\"\u003e27\u003c/span\u003e: # \u003cspan class=\"hljs-string\"\u003e'ESC'\u003c/span\u003e 키를 눌러 종료\n        \u003cspan class=\"hljs-keyword\"\u003ebreak\u003c/span\u003e\ncap.\u003cspan class=\"hljs-title function_\"\u003erelease\u003c/span\u003e()\ncv2.\u003cspan class=\"hljs-title function_\"\u003edestroyAllWindows\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e위 코드는 PiCam에서 생성된 비디오 스트림을 캡처하여 BGR 색상과 회색 모드로 모두 표시합니다.\u003c/p\u003e\n\u003cp\u003e또는 GitHub에서 코드를 다운로드할 수도 있습니다: simpleCamTest.py\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e해당 스크립트를 실행하려면 다음 명령어를 입력해주세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epython simpleCamTest.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_4.png\"\u003e\n\u003cp\u003e위의 그림은 결과를 보여줍니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e일부 사용자가 카메라를 열려고 시도할 때 문제가 발생하고 \"Assertion failed\" 오류 메시지가 표시된다는 것을 발겠어요. 이것은 카메라가 OpenCV 설치 중에 활성화되지 않았거나 카메라 드라이버가 올바르게 설치되지 않아 발생할 수 있습니다. 수정하려면 다음 명령을 사용하세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003esudo modprobe bcm2835-v4l2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOpenCV에 대해 더 알고 싶다면 다음 튜토리얼을 참고해보세요: loading-video-python-opencv-tutorial\u003c/p\u003e\n\u003ch1\u003e4. 얼굴 인식\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eFace Recognition에서 가장 기본적인 작업은 물론 \"얼굴 감지\"입니다. 무엇보다 먼저, 미래에 캡처된 새 얼굴과 비교할 수 있도록 얼굴을 캡처해야 합니다(Phase 1).\u003c/p\u003e\n\u003cp\u003e(혹시 이전에 말한 'Capture'부분 해석이 잘못된 것 같아 원문에서 대체했습니다.)\u003c/p\u003e\n\u003cp\u003e한 객체를 감지하는 가장 일반적인 방법은 \"Haar Cascade classifier\"를 사용하는 것입니다.\u003c/p\u003e\n\u003cp\u003e\"Haar 특징 기반 캐스케이드 분류기\"를 사용한 객체 감지는 Paul Viola와 Michael Jones가 2001년에 발표한 논문 \"Rapid Object Detection using a Boosted Cascade of Simple Features\"에서 제안된 효과적인 객체 감지 방법입니다. 이는 많은 양의 긍정 이미지와 부정 이미지로부터 케스케이드 함수가 훈련된 기계 학습 기반 방법으로, 다른 이미지에서 객체를 감지하는 데 사용됩니다.\u003c/p\u003e\n\u003cp\u003e여기서는 얼굴 감지 작업을 수행할 것입니다. 먼저, 알고리즘에는 분류기를 훈련시키기 위해 많은 양의 양성 이미지(얼굴 이미지)와 음성 이미지(얼굴이 없는 이미지)가 필요합니다. 그런 다음 그로부터 특징을 추출해야 합니다. 좋은 소식은 OpenCV에 트레이너와 탐지기가 함께 제공된다는 것입니다. 자동차, 비행기 등의 모든 객체에 대해 자체 분류기를 훈련시키고 싶다면 OpenCV를 사용해 생성할 수 있습니다. 자세한 내용은 여기를 참고하십시오: 캐스케이드 분류기 훈련.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e만약 여러분이 자체 분류기를 만들고 싶지 않다면, OpenCV에는 이미 얼굴, 눈, 웃음 등을 위한 사전 훈련된 분류기가 많이 포함되어 있습니다. 이러한 XML 파일은 haarcascades 디렉토리에서 다운로드할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이론은 이만하고, 이제 OpenCV를 사용하여 얼굴 탐지기를 만들어 봅시다!\u003c/p\u003e\n\u003cp\u003e제 GitHub에서 파일 faceDetection.py를 다운로드하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e cv2\nfaceCascade = cv2.CascadeClassifier(\u003cspan class=\"hljs-string\"\u003e'Cascades/haarcascade_frontalface_default.xml'\u003c/span\u003e)\ncap = cv2.VideoCapture(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\ncap.\u003cspan class=\"hljs-built_in\"\u003eset\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e640\u003c/span\u003e) \u003cspan class=\"hljs-comment\"\u003e# 너비 설정\u003c/span\u003e\ncap.\u003cspan class=\"hljs-built_in\"\u003eset\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e480\u003c/span\u003e) \u003cspan class=\"hljs-comment\"\u003e# 높이 설정\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003ewhile\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e:\n    ret, img = cap.read()\n    img = cv2.flip(img, -\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = faceCascade.detectMultiScale(\n        gray,\n        scaleFactor=\u003cspan class=\"hljs-number\"\u003e1.2\u003c/span\u003e,\n        minNeighbors=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e,\n        minSize=(\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)\n    )\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (x,y,w,h) \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e faces:\n        cv2.rectangle(img,(x,y),(x+w,y+h),(\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e),\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n        roi_gray = gray[y:y+h, x:x+w]\n        roi_color = img[y:y+h, x:x+w]\n    cv2.imshow(\u003cspan class=\"hljs-string\"\u003e'video'\u003c/span\u003e,img)\n    k = cv2.waitKey(\u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e) \u0026#x26; \u003cspan class=\"hljs-number\"\u003e0xff\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e k == \u003cspan class=\"hljs-number\"\u003e27\u003c/span\u003e: \u003cspan class=\"hljs-comment\"\u003e# 'ESC'를 눌러 종료\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003ebreak\u003c/span\u003e\ncap.release()\ncv2.destroyAllWindows()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e믿을 수 있건대, Python과 OpenCV를 사용하여 얼굴을 감지하는 데 필요한 코드는 바로 위의 몇 줄 뿐입니다.\u003c/p\u003e\n\u003cp\u003e카메라를 테스트하는 데 사용된 마지막 코드와 비교해보면, 약간의 부분이 추가되었음을 깨달을 것입니다. 아래의 줄을 주목해주세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003efaceCascade = cv2.\u003cspan class=\"hljs-title class_\"\u003eCascadeClassifier\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'Cascades/haarcascade_frontalface_default.xml'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 줄은 \"classifier\"를 로드하는 줄입니다 (프로젝트 디렉토리 하위에 \"Cascades/\"라는 디렉토리에 있어야 합니다).\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그럼, 우리는 카메라를 설정하고 루프 내에서 입력 비디오를 회색조 모드로 불러올 것입니다 (이전에 보았던 것과 동일합니다).\u003c/p\u003e\n\u003cp\u003e이제 분류기 함수를 호출해야 하는데, 이때 매우 중요한 매개변수들을 전달해주어야 합니다. 이들은 scale factor, 이웃의 수, 그리고 감지된 얼굴의 최소 크기입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003efaces = faceCascade.\u003cspan class=\"hljs-title function_\"\u003edetectMultiScale\u003c/span\u003e(\n        gray,     \n        scaleFactor=\u003cspan class=\"hljs-number\"\u003e1.2\u003c/span\u003e,\n        minNeighbors=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e,     \n        minSize=(\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)\n        )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e여기서,\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003egray는 입력 그레이스케일 이미지입니다.\u003c/li\u003e\n\u003cli\u003escaleFactor는 각 이미지 스케일에서 이미지 크기가 감소하는 정도를 지정하는 매개 변수입니다. 스케일 피라미드를 만드는 데 사용됩니다.\u003c/li\u003e\n\u003cli\u003eminNeighbors는 각 후보 사각형이 유지해야 하는 이웃 수를 지정하는 매개 변수입니다. 숫자가 높을수록 낮은 거짓 양성이 발생합니다.\u003c/li\u003e\n\u003cli\u003eminSize는 고려해야 할 최소 사각형 크기입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e함수는 이미지에서 얼굴을 감지합니다. 다음으로 이미지에서 얼굴을 \"표시\"해야 합니다. 예를 들어, 파란색 사각형 등을 사용합니다. 이 작업은 다음 코드 부분으로 수행됩니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (x,y,w,h) \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003efaces\u003c/span\u003e:\n    cv2.\u003cspan class=\"hljs-title function_\"\u003erectangle\u003c/span\u003e(img,(x,y),(x+w,y+h),(\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e),\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n    roi_gray = gray[\u003cspan class=\"hljs-attr\"\u003ey\u003c/span\u003e:y+h, \u003cspan class=\"hljs-attr\"\u003ex\u003c/span\u003e:x+w]\n    roi_color = img[\u003cspan class=\"hljs-attr\"\u003ey\u003c/span\u003e:y+h, \u003cspan class=\"hljs-attr\"\u003ex\u003c/span\u003e:x+w]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e얼굴이 발견되면, 감지된 얼굴의 위치를 왼쪽 위 모서리인 (x,y)로 하는 네모 상자로 반환하며, \"w\"를 너비, \"h\"를 높이로 가집니다 == (x, y, w, h). 사진을 참조하세요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_5.png\"\u003e\n\u003cp\u003e이 위치들을 얻으면, 얼굴을 위한 \"ROI\" (사각형으로 그린)를 만들고 imshow() 함수를 사용하여 결과를 표시할 수 있습니다.\u003c/p\u003e\n\u003cp\u003eRpi 터미널을 사용하여 위의 Python 스크립트를 Python 환경에서 실행해보세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epython faceDetection.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e결과:\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_6.png\"\u003e\n\u003cp\u003e당신은 \"눈 탐지\" 또는 \"미소 탐지\"를 위한 분류기도 포함할 수 있습니다. 해당 경우에는 얼굴 루프 내에 분류기 함수와 사각형 그리기를 포함해야 합니다. 왜냐하면 얼굴 외부에서 눈이나 미소를 탐지하는 것은 의미가 없기 때문입니다.\u003c/p\u003e\n\u003ch2\u003e예시\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e내 GitHub에서 다른 예제들을 찾을 수 있어요:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003efaceEyeDetection.py\u003c/li\u003e\n\u003cli\u003efaceSmileDetection.py\u003c/li\u003e\n\u003cli\u003efaceSmileEyeDetection.py\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e그리고 사진에서 결과를 확인할 수 있어요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_7.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래 튜토리얼을 따라해서 얼굴 감지에 대해 더 잘 이해할 수 있어요:\u003c/p\u003e\n\u003cp\u003eHaar Cascade Object Detection Face \u0026#x26; Eye OpenCV Python Tutorial\u003c/p\u003e\n\u003ch1\u003e5. 데이터 수집\u003c/h1\u003e\n\u003cp\u003e우선, 사진을 통한 얼굴 인식에 대한 람리즈 라자의 훌륭한 작업에 감사드려야 해요!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eOPENCV와 Python을 사용한 얼굴 인식: 초보자를 위한 안내서\u003c/p\u003e\n\u003cp\u003e그리고 비디오를 사용해 매우 포괄적인 튜토리얼을 개발한 Anirban Kar:\u003c/p\u003e\n\u003cp\u003e얼굴 인식 - 3 부분\u003c/p\u003e\n\u003cp\u003e두 튜토리얼을 꼭 확인해보시기를 적극 추천합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그렇게 말씀하시면 프로젝트의 첫 단계를 시작하겠습니다. 여기서 우리가 할 일은 마지막 단계(얼굴 감지)부터 시작하여 간단히 데이터 세트를 만드는 것입니다. 각 ID에 대해, 얼굴 감지에 사용된 부분이 회색으로 표시된 사진 그룹을 저장할 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_8.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e먼저, 프로젝트를 개발할 디렉터리를 만드세요. 예를 들어, FacialRecognitionProject:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003emkdir \u003cspan class=\"hljs-title class_\"\u003eFacialRecognitionProject\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 디렉토리에는 프로젝트용으로 만들 3개의 Python 스크립트뿐만 아니라 Facial Classifier도 저장해 두어야 해요. 이를 다운로드할 수 있는 GitHub 링크는 haarcascade_frontalface_default.xml입니다.\u003c/p\u003e\n\u003cp\u003e그리고 우리의 얼굴 샘플을 저장할 하위 디렉토리를 만들어 이름을 \"dataset\"으로 지어줘요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-built_in\"\u003emkdir\u003c/span\u003e dataset\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그리고 저의 GitHub에서 코드를 다운로드하세요: 01_face_dataset.py\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e cv2\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\ncam = cv2.\u003cspan class=\"hljs-title class_\"\u003eVideoCapture\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\ncam.\u003cspan class=\"hljs-title function_\"\u003eset\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e640\u003c/span\u003e) # 비디오 너비 설정\ncam.\u003cspan class=\"hljs-title function_\"\u003eset\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e480\u003c/span\u003e) # 비디오 높이 설정\nface_detector = cv2.\u003cspan class=\"hljs-title class_\"\u003eCascadeClassifier\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'haarcascade_frontalface_default.xml'\u003c/span\u003e)\n# 각 사람에 대해 하나의 숫자 얼굴 \u003cspan class=\"hljs-variable constant_\"\u003eID\u003c/span\u003e 입력\nface_id = \u003cspan class=\"hljs-title function_\"\u003einput\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'\\n 사용자 ID를 입력하고 \u0026#x3C;return\u003e을 누르세요 ==\u003e '\u003c/span\u003e)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"\\n [INFO] 얼굴 캡처 초기화. 카메라를 응시하고 기다리세요 ...\"\u003c/span\u003e)\n# 개별 샘플링 얼굴 카운트 초기화\ncount = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003ewhile\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e):\n    ret, img = cam.\u003cspan class=\"hljs-title function_\"\u003eread\u003c/span\u003e()\n    img = cv2.\u003cspan class=\"hljs-title function_\"\u003eflip\u003c/span\u003e(img, -\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) # 비디오 이미지 수직으로 뒤집기\n    gray = cv2.\u003cspan class=\"hljs-title function_\"\u003ecvtColor\u003c/span\u003e(img, cv2.\u003cspan class=\"hljs-property\"\u003eCOLOR_BGR2GRAY\u003c/span\u003e)\n    faces = face_detector.\u003cspan class=\"hljs-title function_\"\u003edetectMultiScale\u003c/span\u003e(gray, \u003cspan class=\"hljs-number\"\u003e1.3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (x,y,w,h) \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003efaces\u003c/span\u003e:\n        cv2.\u003cspan class=\"hljs-title function_\"\u003erectangle\u003c/span\u003e(img, (x,y), (x+w,y+h), (\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e), \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)     \n        count += \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n        # 캡처된 이미지를 데이터셋 폴더에 저장\n        cv2.\u003cspan class=\"hljs-title function_\"\u003eimwrite\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"dataset/User.\"\u003c/span\u003e + \u003cspan class=\"hljs-title function_\"\u003estr\u003c/span\u003e(face_id) + \u003cspan class=\"hljs-string\"\u003e'.'\u003c/span\u003e +  \n                    \u003cspan class=\"hljs-title function_\"\u003estr\u003c/span\u003e(count) + \u003cspan class=\"hljs-string\"\u003e\".jpg\"\u003c/span\u003e, gray[\u003cspan class=\"hljs-attr\"\u003ey\u003c/span\u003e:y+h,\u003cspan class=\"hljs-attr\"\u003ex\u003c/span\u003e:x+w])\n        cv2.\u003cspan class=\"hljs-title function_\"\u003eimshow\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'image'\u003c/span\u003e, img)\n    k = cv2.\u003cspan class=\"hljs-title function_\"\u003ewaitKey\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e) \u0026#x26; \u003cspan class=\"hljs-number\"\u003e0xff\u003c/span\u003e # \u003cspan class=\"hljs-string\"\u003e'ESC'\u003c/span\u003e 키를 눌러 비디오 종료\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e k == \u003cspan class=\"hljs-number\"\u003e27\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003ebreak\u003c/span\u003e\n    elif count \u003e= \u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e: # \u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e개의 얼굴 샘플 촬영 후 비디오 중지\n         \u003cspan class=\"hljs-keyword\"\u003ebreak\u003c/span\u003e\n# 청소 조금\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"\\n [INFO] 프로그램 종료 및 정리 진행\"\u003c/span\u003e)\ncam.\u003cspan class=\"hljs-title function_\"\u003erelease\u003c/span\u003e()\ncv2.\u003cspan class=\"hljs-title function_\"\u003edestroyAllWindows\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e코드는 얼굴 감지를 위한 코드와 매우 유사합니다. 추가한 부분은 \"사용자 ID를 캡처하기 위한 입력 명령\"이며, 이는 정수 번호(1, 2, 3 등)여야 합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eface_id = \u003cspan class=\"hljs-title function_\"\u003einput\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'\\n 사용자 ID를 입력하고 \u0026#x3C;return\u003e을 누르세요 ==\u003e '\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그리고 각 캡처된 프레임마다 \"데이터셋\" 디렉토리에 파일로 저장해야 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ecv2.\u003cspan class=\"hljs-title function_\"\u003eimwrite\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"dataset/User.\"\u003c/span\u003e + \u003cspan class=\"hljs-title function_\"\u003estr\u003c/span\u003e(face_id) + \u003cspan class=\"hljs-string\"\u003e'.'\u003c/span\u003e + \u003cspan class=\"hljs-title function_\"\u003estr\u003c/span\u003e(count) + \u003cspan class=\"hljs-string\"\u003e\".jpg\"\u003c/span\u003e, gray[\u003cspan class=\"hljs-attr\"\u003ey\u003c/span\u003e:y+h,\u003cspan class=\"hljs-attr\"\u003ex\u003c/span\u003e:x+w])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e위의 파일을 저장하기 위해서는 라이브러리 \"os\"를 import해야 합니다. 각 파일의 이름은 다음과 같은 구조를 따릅니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title class_\"\u003eUser\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eface_id\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ecount\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ejpg\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e예를 들어, face_id가 1인 사용자의 경우, dataset/ 디렉토리에 있는 네 번째 샘플 파일은 아래와 같이 될 것입니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title class_\"\u003eUser\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.1\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.4\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ejpg\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePI 사진에서 보여지는 것과 같이:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_9.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003e내 코드에서는 각 ID로부터 30개의 샘플을 캡처하고 있습니다. 마지막 \"elif\"에서 이를 변경할 수 있습니다. 샘플의 수는 얼굴 샘플을 캡처하는 루프를 종료하는 데 사용됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e파이썬 스크립트를 실행하고 몇 개의 ID를 캡처하세요. 새 사용자를 집계하거나 이미 존재하는 사용자의 사진을 변경하려면 매번 스크립트를 실행해야 합니다.\u003c/p\u003e\n\u003ch1\u003e6. 트레이너\u003c/h1\u003e\n\u003cp\u003e이 두 번째 단계에서는 데이터셋에서 모든 사용자 데이터를 가져와 OpenCV Recognizer를 \"트레이닝\"해야 합니다. 이 작업은 특정한 OpenCV 함수를 사용하여 직접 수행됩니다. 결과는 \"trainer/\" 디렉토리에 저장된 .yml 파일이 될 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_10.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그러면, 훈련 데이터를 저장할 하위 디렉토리를 만드는 것으로 시작해봐요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003emkdir trainer\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e제 GitHub에서 두 번째 파이썬 스크립트를 다운로드하실 거에요: 02_face_training.py\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e cv2\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003ePIL\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eImage\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n# 얼굴 이미지 데이터베이스 경로\npath = \u003cspan class=\"hljs-string\"\u003e'dataset'\u003c/span\u003e\nrecognizer = cv2.\u003cspan class=\"hljs-property\"\u003eface\u003c/span\u003e.\u003cspan class=\"hljs-title class_\"\u003eLBPHFaceRecognizer\u003c/span\u003e_create()\ndetector = cv2.\u003cspan class=\"hljs-title class_\"\u003eCascadeClassifier\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"haarcascade_frontalface_default.xml\"\u003c/span\u003e);\n# 이미지 및 레이블 데이터 가져오는 함수\ndef \u003cspan class=\"hljs-title function_\"\u003egetImagesAndLabels\u003c/span\u003e(path):\n    imagePaths = [os.\u003cspan class=\"hljs-property\"\u003epath\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ejoin\u003c/span\u003e(path,f) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e f \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e os.\u003cspan class=\"hljs-title function_\"\u003elistdir\u003c/span\u003e(path)]     \n    faceSamples=[]\n    ids = []\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e imagePath \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eimagePaths\u003c/span\u003e:\n        PIL_img = \u003cspan class=\"hljs-title class_\"\u003eImage\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eopen\u003c/span\u003e(imagePath).\u003cspan class=\"hljs-title function_\"\u003econvert\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'L'\u003c/span\u003e) # 흑백\n        img_numpy = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(PIL_img,\u003cspan class=\"hljs-string\"\u003e'uint8'\u003c/span\u003e)\n        id = \u003cspan class=\"hljs-title function_\"\u003eint\u003c/span\u003e(os.\u003cspan class=\"hljs-property\"\u003epath\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003esplit\u003c/span\u003e(imagePath)[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e].\u003cspan class=\"hljs-title function_\"\u003esplit\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\".\"\u003c/span\u003e)[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n        faces = detector.\u003cspan class=\"hljs-title function_\"\u003edetectMultiScale\u003c/span\u003e(img_numpy)\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (x,y,w,h) \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003efaces\u003c/span\u003e:\n            faceSamples.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(img_numpy[\u003cspan class=\"hljs-attr\"\u003ey\u003c/span\u003e:y+h,\u003cspan class=\"hljs-attr\"\u003ex\u003c/span\u003e:x+w])\n            ids.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(id)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e faceSamples,ids\nprint (\u003cspan class=\"hljs-string\"\u003e\"\\n [INFO] 얼굴을 학습 중입니다. 몇 초가 걸릴 것입니다. 기다려 주세요...\"\u003c/span\u003e)\nfaces,ids = \u003cspan class=\"hljs-title function_\"\u003egetImagesAndLabels\u003c/span\u003e(path)\nrecognizer.\u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e(faces, np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(ids))\n# 모델을 trainer/trainer.\u003cspan class=\"hljs-property\"\u003eyml\u003c/span\u003e에 저장하기\nrecognizer.\u003cspan class=\"hljs-title function_\"\u003ewrite\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'trainer/trainer.yml'\u003c/span\u003e) \n# 학습된 얼굴 수 및 프로그램 종료 출력\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"\\n [INFO] {0} 개의 얼굴을 학습했습니다. 프로그램을 종료합니다.\"\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eformat\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(np.\u003cspan class=\"hljs-title function_\"\u003eunique\u003c/span\u003e(ids))))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e라즈베리파이에 PIL 라이브러리가 설치되어 있는지 확인해주세요. 만약 설치되어 있지 않다면, 아래의 명령을 터미널에서 실행해주세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epip install pillow\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e우리는 OpenCV 패키지에 포함된 LBPH (LOCAL BINARY PATTERNS HISTOGRAMS) 얼굴 인식기(recognizer)를 사용할 것입니다. 아래와 같이 코드를 작성해주세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003erecognizer = cv2.\u003cspan class=\"hljs-property\"\u003eface\u003c/span\u003e.\u003cspan class=\"hljs-title class_\"\u003eLBPHFaceRecognizer\u003c/span\u003e_create()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\"getImagesAndLabels (path)\" 함수는 \"dataset/\" 디렉토리의 모든 사진을 가져와서 \"Ids\"와 \"faces\" 두 가지 배열을 반환합니다. 이 배열을 입력으로 사용하여 \"인식기를 학습\"할 것입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003erecognizer.\u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e(faces, ids)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 결과로 \"trainer.yml\"이라는 파일이 우리가 이전에 생성한 트레이너 디렉토리에 저장됩니다.\u003c/p\u003e\n\u003cp\u003e여기까지입니다! 훈련시킨 사용자의 얼굴 수를 확인하기 위해 마지막으로 출력 문을 포함했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e매번 Phase 1을 실행할 때마다 Phase 2도 실행해야 합니다.\u003c/p\u003e\n\u003ch1\u003e7. 인식기\u003c/h1\u003e\n\u003cp\u003e지금까지 우리 프로젝트의 마지막 단계에 도달했습니다. 여기서 우리는 카메라로 새로운 얼굴을 촬영하고, 이 사람이 이전에 그의 얼굴을 촬영하고 훈련했다면, 우리의 인식기는 \"예측\"을 수행하여 해당 ID와 일치 여부를 나타내는 지수를 반환합니다. 이로써 해당 일치에 대해 인식기가 얼마나 확신하는지 보여줍니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_11.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eGitHub에서 3단계 Python 스크립트를 내려받아봅시다: 03_face_recognition.py.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e cv2\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os \nrecognizer = cv2.face.LBPHFaceRecognizer_create()\nrecognizer.read(\u003cspan class=\"hljs-string\"\u003e'trainer/trainer.yml'\u003c/span\u003e)\ncascadePath = \u003cspan class=\"hljs-string\"\u003e\"haarcascade_frontalface_default.xml\"\u003c/span\u003e\nfaceCascade = cv2.CascadeClassifier(cascadePath)\nfont = cv2.FONT_HERSHEY_SIMPLEX\n\u003cspan class=\"hljs-comment\"\u003e# 아이디 카운터 초기화\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eid\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# id에 대응하는 이름: 예) Marcelo: id=1,  등\u003c/span\u003e\nnames = [\u003cspan class=\"hljs-string\"\u003e'없음'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'마르셀로'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'파울라'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'일자'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Z'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'W'\u003c/span\u003e] \n\u003cspan class=\"hljs-comment\"\u003e# 실시간 비디오 캡처 시작\u003c/span\u003e\ncam = cv2.VideoCapture(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\ncam.\u003cspan class=\"hljs-built_in\"\u003eset\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e640\u003c/span\u003e) \u003cspan class=\"hljs-comment\"\u003e# 비디오 너비 설정\u003c/span\u003e\ncam.\u003cspan class=\"hljs-built_in\"\u003eset\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e480\u003c/span\u003e) \u003cspan class=\"hljs-comment\"\u003e# 비디오 높이 설정\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# 얼굴 인식으로 인정할 최소 윈도우 크기 정의\u003c/span\u003e\nminW = \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e*cam.get(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)\nminH = \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e*cam.get(\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e)\n\u003cspan class=\"hljs-keyword\"\u003ewhile\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e:\n    ret, img =cam.read()\n    img = cv2.flip(img, -\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) \u003cspan class=\"hljs-comment\"\u003e# 수직으로 뒤집기\u003c/span\u003e\n    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    \n    faces = faceCascade.detectMultiScale( \n        gray,\n        scaleFactor = \u003cspan class=\"hljs-number\"\u003e1.2\u003c/span\u003e,\n        minNeighbors = \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e,\n        minSize = (\u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e(minW), \u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e(minH)),\n       )\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e(x,y,w,h) \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e faces:\n        cv2.rectangle(img, (x,y), (x+w,y+h), (\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e), \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n        \u003cspan class=\"hljs-built_in\"\u003eid\u003c/span\u003e, confidence = recognizer.predict(gray[y:y+h,x:x+w])\n        \u003cspan class=\"hljs-comment\"\u003e# 신뢰도가 100 미만이면 \"0\" : 완벽 일치\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (confidence \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e):\n            \u003cspan class=\"hljs-built_in\"\u003eid\u003c/span\u003e = names[\u003cspan class=\"hljs-built_in\"\u003eid\u003c/span\u003e]\n            confidence = \u003cspan class=\"hljs-string\"\u003e\"  {0}%\"\u003c/span\u003e.\u003cspan class=\"hljs-built_in\"\u003eformat\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003eround\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e - confidence))\n        \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n            \u003cspan class=\"hljs-built_in\"\u003eid\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e\"알수없음\"\u003c/span\u003e\n            confidence = \u003cspan class=\"hljs-string\"\u003e\"  {0}%\"\u003c/span\u003e.\u003cspan class=\"hljs-built_in\"\u003eformat\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003eround\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e - confidence))\n        \n        cv2.putText(\n                    img, \n                    \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003eid\u003c/span\u003e), \n                    (x+\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e,y-\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e), \n                    font, \n                    \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \n                    (\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e), \n                    \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e\n                   )\n        cv2.putText(\n                    img, \n                    \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(confidence), \n                    (x+\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e,y+h-\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e), \n                    font, \n                    \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \n                    (\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e), \n                    \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n                   )  \n    \n    cv2.imshow(\u003cspan class=\"hljs-string\"\u003e'camera'\u003c/span\u003e,img) \n    k = cv2.waitKey(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e) \u0026#x26; \u003cspan class=\"hljs-number\"\u003e0xff\u003c/span\u003e \u003cspan class=\"hljs-comment\"\u003e# 'ESC'를 눌러 비디오 종료\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e k == \u003cspan class=\"hljs-number\"\u003e27\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003ebreak\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# 정리 작업\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"\\n [INFO] 프로그램 종료 및 정리 작업\"\u003c/span\u003e)\ncam.release()\ncv2.destroyAllWindows()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e여기에 새 배열을 추가했으므로, 숫자로 된 ID 대신 \"names\"를 표시할 것입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003enames = [\u003cspan class=\"hljs-string\"\u003e'없음'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'마르셀로'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'파울라'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'일자'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Z'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'W'\u003c/span\u003e]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e예를 들어, Marcelo는 id=1인 사용자이고, Paula는 id=2인 등입니다.\u003c/p\u003e\n\u003cp\u003e다음으로, 우리는 얼굴을 감지할 것이며, 이전과 같이 haasCascade 분류기를 사용할 것입니다. 감지된 얼굴을 가지고 우리는 위의 코드에서 가장 중요한 함수를 호출할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eid, confidence = recognizer.\u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(얼굴의 회색 부분)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003erecognizer.predict()는 분석할 얼굴의 캡처된 부분을 매개변수로 사용하고, 해당 소유자와 일치에 대한 신뢰도를 나타내는 id 및 인식기의 신뢰도 값을 반환할 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그리고 마지막으로, 인식기가 얼굴을 예측할 수 있다면, 이미지 위에 “확률적 ID”와 일치가 올바른지에 대한 “확률”이 표시됩니다 (“확률” = 100 - 신뢰도 지수). 그렇지 않으면, “알 수 없음” 레이블이 얼굴 위에 표시됩니다.\u003c/p\u003e\n\u003cp\u003e아래는 결과를 보여주는 gif입니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_12.png\" alt=\"결과\"\u003e\u003c/p\u003e\n\u003cp\u003e이 사진에서는 이 프로젝트로 수행한 일부 테스트를 보여드립니다. 여기서 인식기가 작동하는지 확인하기 위해 사진을 사용했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_13.png\"\u003e\n\u003ch1\u003e8. 결론\u003c/h1\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject_14.png\"\u003e\n\u003cp\u003e언제나처럼, 이 프로젝트가 다른 사람들이 전자기술의 흥미로운 세계로 진입하는 데 도움이 되기를 바랍니다!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e세부 정보 및 최종 코드는 제 GitHub 저장소를 방문해주세요:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Mjrovai/OpenCV-Face-Recognition\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/Mjrovai/OpenCV-Face-Recognition\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e더 많은 프로젝트를 보시려면 제 블로그를 방문해주세요: MJRoBot.org\u003c/p\u003e\n\u003cp\u003e이른 아침 인사드립니다!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다음 글에서 봐요!\u003c/p\u003e\n\u003cp\u003e감사합니다,\u003c/p\u003e\n\u003cp\u003e마르셀로\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-Real-TimeFaceRecognitionAnEnd-To-EndProject"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>