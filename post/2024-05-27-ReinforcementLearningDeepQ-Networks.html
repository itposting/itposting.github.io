<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>강화 학습 딥 Q-네트워크 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-05-27-ReinforcementLearningDeepQ-Networks" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="강화 학습 딥 Q-네트워크 | itposting" data-gatsby-head="true"/><meta property="og:title" content="강화 학습 딥 Q-네트워크 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-05-27-ReinforcementLearningDeepQ-Networks" data-gatsby-head="true"/><meta name="twitter:title" content="강화 학습 딥 Q-네트워크 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-27 14:10" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">강화 학습 딥 Q-네트워크</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="강화 학습 딥 Q-네트워크" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 27, 2024</span><span class="posts_reading_time__f7YPP">35<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-27-ReinforcementLearningDeepQ-Networks&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>Python을 사용하여 달에 착륙하는 셔틀 가르치기: Deep Q-Networks를 활용한 강화 학습의 수학적 탐구</h2>
<p><img src="/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_0.png" alt="Reinforcement Learning"></p>
<p>강화 학습(RL)에서 Q-학습은 에이전트가 환경을 탐색하면서 누적 보상을 극대화하기 위한 정책을 학습하는 데 도움이 되는 기본 알고리즘입니다. 이를 통해 특정 상태에서 특정 작업을 수행했을 때 기대되는 유틸리티를 추정하는 작업-값 함수를 업데이트 함으로써 보상을 받고 미래 추정에 기반합니다 (이게 익숙하지 않으신가요? 걱정 마세요. 나중에 함께 자세히 살펴볼 겁니다).</p>
<p>그러나 전통적인 Q-학습에는 도전 과제가 있습니다. 상태 공간이 확장됨에 따라 확장 가능성에 어려움을 겪으며 연속적인 상태 및 작업 공간을 갖는 환경에서 효과적이지 않습니다. 이때 Deep Q Networks (DQNs)가 나타납니다. DQNs는 Q-값을 근사하기 위해 신경망을 사용하여 에이전트가 보다 크고 복잡한 환경을 효과적으로 처리할 수 있도록 합니다.</p>
<div class="content-ad"></div>
<p>본 기사에서는 Deep Q Networks에 대해 자세히 살펴보겠습니다. DQNs가 기존의 Q-learning의 한계를 극복하는 방법과 DQN을 구성하는 주요 구성 요소에 대해 탐구할 것입니다. 또한 처음부터 DQN을 구현하고 더 복잡한 환경에 적용하는 과정을 살펴볼 것입니다. 이 기사를 마치면 DQN이 어떻게 작동하는지 이해하고 도전적인 강화 학습 문제를 해결하는 데 사용하는 방법을 알게 될 것입니다.</p>
<h2>목차</h2>
<p>1: 전통적인 Q-러닝
∘ 1.1: 상태와 행동
∘ 1.2: Q-값
∘ 1.3: Q-테이블
∘ 1.4: 학습 과정</p>
<p>2: Q-러닝에서 Deep Q-네트워크로
∘ 2.1: 전통적인 Q-러닝의 한계
∘ 2.2: 신경망</p>
<div class="content-ad"></div>
<p>3: Deep Q-Network의 해부학</p>
<ul>
<li>3.1: DQN의 구성요소</li>
<li>3.2: DQN 알고리즘</li>
</ul>
<p>4: 처음부터 Deep Q-Network 구현하기</p>
<ul>
<li>4.1: 환경 설정</li>
<li>4.2: 딥 신경망 구축</li>
<li>4.3: 경험 재생 구현</li>
<li>4.4: 타깃 네트워크 구현</li>
<li>4.5: Deep Q-Network 훈련</li>
<li>4.6: 모델 튜닝</li>
<li>4.7: 모델 실행</li>
</ul>
<p>5: 결론
참고 문헌</p>
<h1>1: 전통적인 Q-Learning</h1>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_1.png" alt="Reinforcement Learning Deep Q Networks"></p>
<p>Q-러닝은 환경에서 누적 보상을 극대화하기 위한 최적 조치를 학습하는 에이전트를 안내합니다. 딥 Q-네트워크에 집중하기 전에, 그 선구자인 Q-러닝 뒤에 있는 메커니즘을 간단히 검토하는 것이 좋습니다.</p>
<h2>1.1: 상태 및 조치</h2>
<p>미로를 탐색하는 로봇이라고 상상해보세요. 미로에서 차지하는 각 위치를 "상태"라고 합니다. 왼쪽, 오른쪽, 위 또는 아래로 이동하는 것과 같은 각각의 움직임을 "조치"라고 합니다. 목표는 결국 미로를 통해 최적 경로를 찾으려면 각 상태에서 어떤 조치를 취할지 결정하는 것입니다.</p>
<div class="content-ad"></div>
<h2>1.2: Q-Values</h2>
<p>Q-Learning의 핵심은 Q-값으로, 𝑄(𝑠, 𝑎)로 표시됩니다. 이 값은 특정 상태 s에서 특정 행동 a를 취한 후 더 나은 경로(정책)를 따를 때 기대되는 미래 보상을 나타냅니다.</p>
<p>Q-값을 가이드북의 항목으로 생각해보세요. 각 가능한 이동의 장기적 이점을 평가하는 것입니다. 예를 들어 미로의 특정 위치에 있다고 가정했을 때 왼쪽으로 이동하는 경우, Q-값은 미래 보상 측면에서 그 이동이 얼마나 유익할지 알려줍니다. 더 높은 Q-값은 더 나은 이동을 나타냅니다.</p>
<h2>1.3: The Q-Table</h2>
<div class="content-ad"></div>
<p>Q-Learning은 Q-값을 추적하는 데 Q-테이블을 사용합니다. Q-테이블은 기본적으로 각 행이 상태에 해당하고 각 열이 행동에 해당하며 각 셀이 해당 상태-행동 쌍의 Q-값을 포함하는 대형 스프레드시트입니다.</p>
<p>Q-테이블을 거대한 스프레드시트로 상상해보세요. 각 셀은 미로의 특정 위치에서 특정 이동을 하였을 때 잠재적 미래 보상을 나타냅니다. 환경에 대해 더 많이 배우면이 보상의 더 나은 추정치로이 스프레드시트를 업데이트합니다.</p>
<h2>1.4: 학습 과정</h2>
<p>Q-러닝의 학습 과정은 반복적입니다. 초기 상태 s에서 시작합니다. 그런 다음 작업 a를 결정합니다. 이 선택은 다음을 기반으로 할 수 있습니다:</p>
<div class="content-ad"></div>
<ul>
<li>탐험: 효과를 발견하기 위해 새로운 조치를 시도합니다.</li>
<li>개척: 가장 높은 알려진 Q-값을 갖는 조치를 선택하기 위해 기존 지식을 사용합니다.</li>
</ul>
<p>선택한 조치를 수행하고 보상 r을 관찰하며 다음 상태 s'로 이동합니다. Q-러닝 공식을 사용하여 상태-조치 쌍 (s, a)의 Q-값을 업데이트합니다:</p>
<p><a href="/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_2.png">이미지</a></p>
<p>여기에:</p>
<div class="content-ad"></div>
<ul>
<li>α는 학습 속도로, 새로운 정보가 이전 정보를 얼마나 덮어쓸지를 결정합니다.</li>
<li>γ는 할인 요소로, 즉각적 보상을 먼 미래의 보상보다 더 가치 있게 여깁니다.</li>
<li>maxa′Q(s′,a′)는 다음 상태 s′에서 가능한 모든 행동 a′에 대해 최고의 Q값을 나타냅니다.</li>
</ul>
<p>매번 안내서를 업데이트하고 있다고 상상해 보세요. 각 이동 후에 성공적인지 실패인지에 대한 피드백(보상)을 받습니다. 그런 다음 새로운 정보를 반영하도록 가이드북의 등급(Q값)을 조정하여 미래의 결정을 더 잘 하게 됩니다.</p>
<p>Q값이 수렴할 때까지 이 과정을 반복하면, 에이전트는 미로를 탐색하는 최적 정책을 학습한 것입니다. 시간이 흘러, 미로를 반복적으로 탐험하고 경험에 기반하여 가이드북을 업데이트함으로써 최상의 보상을 얻기 위한 최적의 움직임을 알려주는 포괄적인 전략을 개발하게 됩니다.</p>
<p>Q-러닝에 대해 자세히 알아보려면 이 기사를 확인해 보세요: <a href="%EB%A7%81%ED%81%AC">링크</a></p>
<div class="content-ad"></div>
<h1>2: Q-Learning에서 Deep Q-Network로</h1>
<h2>2.1: 전통적인 Q-Learning의 한계</h2>
<p>Q-Learning은 강화 학습에 대한 강력한 알고리즘이지만, 더 복잡한 환경에서 효과적으로 동작하는 데 제약 사항이 몇 가지 있습니다:</p>
<p>확장성 문제: 전통적인 Q-Learning은 각 상태-행동 쌍이 Q-값에 매핑된 Q-테이블을 유지합니다. 상태 공간이 성장함에 따라, 특히 고차원 또는 연속적인 환경에서는 Q-테이블이 불필요하게 커져 메모리 비효율성과 학습 속도 저하를 초래합니다.</p>
<div class="content-ad"></div>
<p>이산 상태 및 행동 공간: Q-Learning은 상태와 행동이 이산적이고 유한한 환경에서 잘 동작합니다. 하지만 현실 세계의 많은 문제는 연속적인 상태와 행동 공간을 포함하고 있습니다. 이러한 전통적인 Q-Learning은 이러한 공간을 이산화하지 않고는 효율적으로 처리할 수 없으며, 이로 인해 정보 손실과 최적 정책의 하락을 초래할 수 있습니다.</p>
<h2>2.2: 신경망</h2>
<p>이제 신경망을 소개해 보겠습니다. 신경망은 딥 네트워크에서 중요한 역할을 하는데, 인간 두뇌의 구조와 기능을 모방하여 데이터로부터 복잡한 패턴을 학습할 수 있는 강력한 함수 근사기입니다. 신경망은 입력 데이터를 처리하고 가중치와 편향을 통해 변환하여 출력을 생성하는 연결된 노드(뉴런)의 계층으로 이루어져 있습니다.</p>
<p>강화 학습의 맥락에서 신경망은 Q-함수를 근사화하는 데 사용될 수 있습니다. 이는 상태-행동 쌍을 Q-값에 매핑하는 데 도움이 되며, 특히 Q-테이블을 유지하는 것이 적절하지 않은 대규모나 연속적인 공간에서 상태와 행동 간에 일반화를 더 잘할 수 있도록 합니다.</p>
<div class="content-ad"></div>
<p>따라서, Deep Q-networks(DQNs)은 Q-Learning의 원리를 신경망의 함수 근사 능력과 결합시켜요. 그렇게 하면 전통적인 Q-learning의 주요 제약 사항을 다룰 수 있어요.</p>
<p>DQNs은 Q-값을 테이블에 저장하는 대신 신경망을 사용하여 Q-함수를 근사해요. 이 네트워크는 상태를 입력으로 받아 가능한 모든 행동에 대한 Q-값을 출력해요. 환경에서의 경험으로 네트워크를 학습시켜 에이전트는 각 행동에 대한 예상 보상을 예측하도록합니다. 이를 통해 다양한 상태와 행동에 걸쳐 일반화할 수 있어요.</p>
<p>체스를 배우는 것을 상상해보세요. 가능한 모든 체스판 구성과 각 동작에 대한 최상의 수를 외우는 대신(불가능한 일이죠), 전략과 원칙(예를 들어 보드 중앙을 제어하고 왕을 보호하는 것과 같은 것)을 배우게 됩니다. 비슷하게, DQN는 신경망을 통해 일반적인 패턴과 전략을 배우고 모든 가능한 상태를 외우지 않고도 정보를 바탕으로 결정할 수 있어요.</p>
<p>신경망 사용은 DQN이 크거나 연속된 상태 공간을 다룰 수 있게 해요. 네트워크는 주요 특징을 잡아내는 상태 공간의 표현을 학습해 중요한 결정을 취할 수 있도록 해줍니다.</p>
<p>큰 도시를 이동하려면 고려합시다. 모든 거리와 건물의 배치를 외우는 대신 표지판과 중요 도로를 인식해 길을 찾게 됩니다. DQN의 신경망도 비슷하게 작용하며, 에이전트가 복잡한 환경에서 이동하는 것을 돕는 상태 공간의 중요한 특징을 인식하도록 학습합니다.</p>
<div class="content-ad"></div>
<p>다양한 경험을 훈련함으로써 모델은 과거 경험에서 일반화하는 법을 배우게 됩니다. 즉, 에이전트는 배운 것을 새로운, 보지 못한 상태와 행동에 적용할 수 있어서 다양한 상황에서 더 적응력이 있고 효율적일 수 있습니다.</p>
<h1>3: 딥 Q-네트워크의 구성 요소</h1>
<h2>3.1: DQN의 구성 요소</h2>
<p>딥 Q-네트워크 (DQN)가 어떻게 작동하는지 이해하려면 그 주요 구성 요소를 자세히 살펴보는 것이 중요합니다:</p>
<div class="content-ad"></div>
<p>3.1.1: 신경망</p>
<p><img src="/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_3.png" alt="신경망"></p>
<p>DQN의 핵심은 Q값을 위한 함수 근사기 역할을 하는 신경망입니다. 아키텍처는 일반적으로 다음과 같이 구성됩니다:</p>
<p>입력 레이어: 에이전트의 "눈"으로 상상해보세요. 이 레이어는 환경으로부터 상태 표현을 받아들이는데, 마치 당신의 눈이 주변의 시각적 정보를 받아들이는 것과 유사합니다. 위의 이미지에서 왼쪽에 두 개의 노드가 있는 첫 번째 레이어입니다.</p>
<div class="content-ad"></div>
<p>Hidden Layers: 이러한 레이어들은 에이전트의 "뇌"로 생각할 수 있습니다. 눈을 통해 받은 정보를 다수의 사고 단계를 거쳐 처리하여 복잡한 특징과 패턴을 식별합니다. 마치 당신의 뇌가 세계를 처리하고 이해하는 방식과 비슷합니다. 위 이미지에서는 세 개의 노드가 있는 중간 레이어입니다.</p>
<p>Output Layer: 이는 에이전트의 "의사 결정" 부분과 같습니다. 입력 상태에 따라 모든 가능한 행동에 대한 Q 값(값함수)을 생성합니다. 당신이 보고 생각한 것에 기반하여 최선의 행동을 결정하는 방식과 유사합니다. 각 출력은 특정 행동을 취했을 때 기대되는 보상에 해당합니다. 위 이미지에서는 한 개의 노드를 가진 오른쪽의 마지막 레이어입니다.</p>
<p>위 이미지는 간단한 피드포워드 신경망을 나타냅니다. 이는 신경망의 가장 기본적인 형태입니다. 이 구조는 기본적이지만 "깊은" 신경망은 아닙니다. 깊은 신경망으로 변환하기 위해서는 더 많은 은닉 레이어를 추가하여 신경망의 깊이를 증가시켜야 합니다. 또한, 다양한 아키텍처와 구성을 실험하여 더 발전된 모델을 개발할 수 있습니다. 각 레이어의 노드 수는 고정되지 않으며, 특정 훈련 데이터셋과 작업에 따라 다양합니다. 이러한 유연성을 통해 네트워크를 특정 목적에 더 잘 맞게 조정할 수 있습니다.</p>
<p>신경망에 대해 더 알고 싶다면, 나는 아래의 글을 강력히 추천합니다:</p>
<div class="content-ad"></div>
<p>3.1.2: 경험 재생
이제 목록의 다음 항목인 경험 재생으로 넘어가 봅시다. 이것은 DQNs에서 학습 과정을 안정화하고 향상시키는 기술입니다. 다음을 포함합니다:</p>
<p>메모리 버퍼: 에이전트의 "일기"로 생각해보세요. 이것은 에이전트의 경험을 시간이 지남에 따라 저장합니다 (상태, 행동, 보상, 다음 상태, 완료), 마치 매일 당신이 무슨 일이 일어났는지 기록하는 것처럼.</p>
<p>랜덤 샘플링: 훈련 중에 에이전트는 지난 경험을 배우기 위해 일기의 랜덤한 페이지를 넘깁니다. 이는 사건의 순서를 깨어주어 에이전트가 경험의 순서에 과적합되는 것을 방지하여 보다 견고하게 학습하도록 돕습니다.</p>
<p>3.1.3: 타겟 네트워크
마지막으로, 타겟 네트워크는 훈련을 위해 타겟 Q-값을 계산하는 데 사용되는 별도의 신경망입니다. 주 신경망과 동일한 구조를 가지고 있지만 주 신경망의 가중치가 정기적으로 업데이트되어 일치하도록 고정되어 있습니다. 에이전트를 위한 "안정된 안내서"로 생각해보세요. 주 신경망이 지속적으로 학습하고 업데이트되는 반면, 타겟 네트워크는 안정된 Q-값을 제공하여 훈련에 도움을 줍니다. 학습을 안정적이고 일관되게 유지하는 데 도움이 되는 신뢰할 수 있는, 주기적으로 업데이트되는 매뉴얼이 있는 것과 같습니다.</p>
<div class="content-ad"></div>
<h2>3.2: DQN 알고리즘</h2>
<p>이러한 구성 요소가 준비되면 DQN 알고리즘은 다음과 같은 몇 가지 중요한 단계로 개요를 제시할 수 있습니다:</p>
<h3>3.2.1: 순방향 전파</h3>
<p>먼저, 우리는 Q-values를 예측하는 데 중요한 순방향 전파로 시작합니다. 이러한 Q-values는 특정 상태에서 특정 행동을 취했을 때 기대되는 미래 보상을 저장합니다. 이 프로세스는 상태 입력부터 시작됩니다.</p>
<h4>상태 입력</h4>
<p>에이전트는 환경에서 현재 상태 s를 관찰합니다. 이 상태는 에이전트의 현재 상황을 설명하는 특징 벡터로 표현됩니다. 상태를 에이전트 주변 세계의 스냅숏으로 생각해보세요. 눈이 주변을 둘러보는 것처럼 시각 장면을 촬영할 때와 유사합니다. 이 스냅숏에는 에이전트가 결정을 내리기 위해 필요한 모든 세부 정보가 포함되어 있습니다.</p>
<div class="content-ad"></div>
<p>Q-Value Prediction
이제 이 관측된 상태 s가 신경망으로 전달됩니다. 이 신경망은 여러 층을 통해 이 입력을 처리하고 Q-값 세트를 출력합니다. 각 Q-값은 가능한 작업 a에 해당하며, 매개 변수 θ는 네트워크의 가중치와 편향을 나타냅니다.</p>
<p><img src="/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_4.png" alt="image"></p>
<p>신경망을 에이전트 뇌의 복잡한 의사 결정 기계로 상상해보세요. 캡처(상태)를 받으면 이 정보를 여러 단계(층)를 통해 처리하여 다양한 작업에 대한 잠재적 결과(Q-값)를 찾습니다. 보이는 것을 바탕으로 취할 수 있는 다양한 작업을 고려해보는 것과 유사합니다.</p>
<p>작업 선택
그런 다음 에이전트는 가장 높은 Q-값을 가진 작업 a∗를 다음 움직임으로 선택하며, 이에 따라 탐욕적 작업 선택 정책을 따릅니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_5.png" alt="image"></p>
<p>이것은 모든 옵션을 심사한 후에 최선의 움직임을 결정하는 것과 유사합니다. 에이전트는 가장 높은 보상을 가져다줄 것으로 믿는 행동을 선택하며, 마치 당신이 보고 이해한 것을 기반으로 가장 유망한 길을 선택하는 것과 같습니다.</p>
<p>3.2.2: 경험 재생
다음으로, 우리는 학습 과정을 안정화하고 향상시키는 데 도움이 되는 경험 재생에 대해 이야기하겠습니다.</p>
<p>경험 저장
에이전트가 행동 a를 취하고 보상 r을 받은 후 새로운 상태 s′를 받으면, 이 경험을 (s, a, r, s′, done) 튜플로 저장하여 플레이백 버퍼에 저장합니다. 변수 done은 에피소드가 종료되었는지를 나타냅니다. 플레이백 버퍼를 에이전트가 경험을 기록하는 다이어리로 생각해보세요. 이는 당신이 하루 중 주목할 만한 사건을 메모하는 것과 유사합니다.</p>
<div class="content-ad"></div>
<p>샘플 미니배치
훈련 중에는 경험의 미니배치가 임의로 선택되어 재생 버퍼에서 샘플링됩니다. 이 배치는 타겟 Q-값을 계산하고 손실을 최소화하여 네트워크를 업데이트하는 데 사용됩니다. 에이전트가 훈련할 때, 과거 경험을 학습하기 위해 일기장의 임의의 페이지를 넘겨보게 됩니다. 이 임의 샘플링은 사건의 순서를 깨고 다양한 학습 예제를 제공하며, 일기의 서로 다른 날짜를 검토하여 보다 넓은 시야를 얻는 것과 유사한 역할을 합니다.</p>
<p>3.2.3: 역전파
최종 단계는 역전파로, 이는 네트워크를 업데이트하여 예측을 개선합니다.</p>
<div class="content-ad"></div>
<p>타겟 Q-값 계산
미니 배치의 각 경험에 대해, 에이전트는 타겟 Q-값 y_를 계산합니다. 만약 다음 상태 s′가 종료 상태(즉, done이 true인 경우)라면, 타겟 Q-값은 간단히 보상 r입니다. 그렇지 않으면, 타겟 Q-값은 보상에 다음 상태 s′에서 타겟 네트워크 Qtarget에 의해 예측된 할인된 최대 Q-값을 더한 값입니다:</p>
<p><img src="/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_7.png" alt="image"></p>
<p>여기서 γ는 할인 계수(0 ≤ γ ≤ 1)입니다. 이 단계는 과거 경험에 기반해 미래를 계획하는 것과 같습니다. 경험이 여행(에피소드)을 끝낼 경우, 타겟은 받은 보상입니다. 계속된다면, 타겟에는 즉시와 미래 혜택을 모두 고려하여 행동을 계획하는 방식과 유사한 예상 미래 보상이 포함됩니다.</p>
<p>손실 계산
다음으로, 손실은 메인 네트워크에서 예측된 Q-값 Q(s_i, a_i; θ)과 타겟 Q-값 yi 사이의 평균 제곱 오차로 계산됩니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_8.png" alt="image"></p>
<p>손실을 계산하는 것은 예측이 실제 결과와 얼마나 차이가 나는지를 평가하는 것과 같습니다. 실제 결과와 비교하여 추측의 정확성을 확인하고 차이점을 주목하는 것과 같습니다.</p>
<p>역전파 및 최적화
마지막으로, 이 손실을 최소화하기 위해 역전파를 수행합니다. 계산된 손실은 네트워크를 통해 역전파되어 SGD(Stochastic Gradient Descent) 또는 Adam과 같은 최적화 알고리즘을 사용하여 가중치를 업데이트합니다. 이 프로세스는 손실을 최소화하기 위해 네트워크 매개변수 θ를 조정합니다:</p>
<p><img src="/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_9.png" alt="image"></p>
<div class="content-ad"></div>
<p>여기서 α는 학습률을 나타내고, ∇θLoss는 네트워크 매개변수에 대한 손실의 그래디언트를 나타냅니다. 역전파는 실수로부터 배우는 것과 같습니다. 예측이 얼마나 잘못되었는지를 깨달았을 때 (손실), 전략(네트워크 가중치)을 조정하여 미래의 결정을 개선합니다. 피드백을 바탕으로 자신의 접근 방식을 미세 조정하여 다음에 더 나은 결과를 얻는 것과 비슷합니다.</p>
<p>이 아키텍처를 사용하여 에이전트는 정책을 반복적으로 개선합니다. 시간이 지남에 따라 누적 보상을 극대화하는 조치를 취하는 것을 배웁니다. 신경망, 경험 재생 및 타겟 네트워크의 결합으로 DQN은 복잡한 고차원 환경에서 효과적으로 학습할 수 있습니다. 이 과정은 에이전트가 환경을 탐색하는 데 능숙해질 때까지 계속됩니다.</p>
<h1>4: 처음부터 Deep Q-Network 구현</h1>
<p>이 섹션에서는 처음부터 Deep Q-Network (DQN)의 구현을 안내합니다. 이 섹션의 끝에는 Python에서 DQN을 구축하고 훈련하는 방법을 명확히 이해하게 될 것입니다.</p>
<div class="content-ad"></div>
<p>우리는 OpenAI Gym의 Lunar Lander 환경을 사용할 것입니다. 이 환경에서의 목표는 달 착륙선을 조종하여 지정된 착륙 패드에 성공적으로 착륙하는 것입니다. 착륙선은 환경을 통해 비행할 때 추진기를 사용하여 움직임과 방향을 조절해야 합니다. 이 환경은 상업적으로 사용할 수 있습니다. 라이센스 및 사용 권한에 대한 자세한 내용은 OpenAI Gym GitHub 페이지에서 확인할 수 있습니다.</p>
<p>오늘 다룰 모든 코드는 여기에서 찾을 수 있습니다:</p>
<h2>4.1: 환경 설정</h2>
<p>우리는 OpenAI Gym의 LunarLander 환경을 사용할 것이며, 이는 우리의 에이전트가 해결해야 할 어려운 문제를 제공합니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> pickle
<span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.<span class="hljs-property">nn</span> <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.<span class="hljs-property">optim</span> <span class="hljs-keyword">as</span> optim
<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> deque
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> optuna
</code></pre>
<p>여기서 필요한 라이브러리들을 import 합니다. gym은 환경을 위해 사용되며, torch는 우리의 신경망을 구축하고 훈련하는 데 사용되며, collections, random, 및 optuna는 경험 재생과 하이퍼파라미터 최적화에 도움이 됩니다.</p>
<pre><code class="hljs language-js">env = gym.<span class="hljs-title function_">make</span>(<span class="hljs-string">"LunarLander-v2"</span>, (render_mode = <span class="hljs-string">"rgb_array"</span>));
state_dim = env.<span class="hljs-property">observation_space</span>.<span class="hljs-property">shape</span>[<span class="hljs-number">0</span>];
action_dim = env.<span class="hljs-property">action_space</span>.<span class="hljs-property">n</span>;
</code></pre>
<p>우리는 LunarLander 환경을 초기화하고 상태 및 액션 공간의 차원을 가져옵니다. state_dim은 상태의 특징 수를 나타내고, action_dim은 가능한 액션 수를 나타냅니다.</p>
<div class="content-ad"></div>
<h2>4.2: 딥 신경망 구축</h2>
<p>우리의 딥-NN에서는 DQN이라는 클래스를 생성할 것입니다. 이 클래스는 세 개의 완전 연결 계층을 가진 신경망을 정의합니다. 입력 계층은 상태 표현을 수신하며, 은닉 계층은 이 정보를 선형 변환과 ReLU 활성화 함수를 통해 처리하고, 출력 계층은 각 가능한 동작에 대한 Q-값을 생성합니다.</p>
<p>먼저 코드를 확인한 다음 분석해 봅시다:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DQN</span>(nn.<span class="hljs-property">Module</span>):
    def <span class="hljs-title function_">__init__</span>(self, state_dim, action_dim):
        <span class="hljs-variable language_">super</span>(<span class="hljs-variable constant_">DQN</span>, self).<span class="hljs-title function_">__init__</span>()
        self.<span class="hljs-property">fc1</span> = nn.<span class="hljs-title class_">Linear</span>(state_dim, <span class="hljs-number">128</span>)
        self.<span class="hljs-property">fc2</span> = nn.<span class="hljs-title class_">Linear</span>(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>)
        self.<span class="hljs-property">fc3</span> = nn.<span class="hljs-title class_">Linear</span>(<span class="hljs-number">128</span>, action_dim)

    def <span class="hljs-title function_">forward</span>(self, x):
        x = torch.<span class="hljs-title function_">relu</span>(self.<span class="hljs-title function_">fc1</span>(x))
        x = torch.<span class="hljs-title function_">relu</span>(self.<span class="hljs-title function_">fc2</span>(x))
        <span class="hljs-keyword">return</span> self.<span class="hljs-title function_">fc3</span>(x)
</code></pre>
<div class="content-ad"></div>
<p>4.2.1: 클래스 초기화</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DQN</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, state_dim, action_dim</span>):
        <span class="hljs-built_in">super</span>(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, <span class="hljs-number">128</span>)
        self.fc2 = nn.Linear(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>)
        self.fc3 = nn.Linear(<span class="hljs-number">128</span>, action_dim)
</code></pre>
<p>우리는 DQN이라는 클래스를 정의했습니다. 이 클래스는 PyTorch의 모든 신경망 모듈에 사용되는 기본 클래스인 nn.Module을 상속받습니다. 이를 통해 우리는 PyTorch의 내장 함수와 기능을 활용할 수 있습니다.</p>
<p><strong>init</strong> 메서드는 객체의 속성을 초기화하는 특별한 메서드입니다. 우리의 경우에는 신경망의 레이어를 설정하게 됩니다. 완전 연결층 (Fully Connected Layers):</p>
<div class="content-ad"></div>
<p>우리는 세 개의 완전 연결 (선형) 레이어를 정의합니다:</p>
<ul>
<li>self.fc1 = nn.Linear(state_dim, 128): 첫 번째 레이어는 입력 상태 차원 (상태의 피쳐 수)을 받아서 128개의 뉴런으로 매핑합니다.</li>
<li>self.fc2 = nn.Linear(128, 128): 두 번째 레이어는 첫 번째 레이어에서 나온 128개의 뉴런을 또 다른 128개의 뉴런으로 매핑합니다.</li>
<li>self.fc3 = nn.Linear(128, action_dim): 세 번째 레이어는 두 번째 레이어에서 나온 128개의 뉴런을 액션 차원 (가능한 액션 수)으로 매핑합니다.</li>
</ul>
<p>각 nn.Linear 레이어는 입력 데이터에 대해 선형 변환을 수행합니다:</p>
<p><img src="/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_10.png" alt="이미지"></p>
<div class="content-ad"></div>
<p>4.2.2: Forward Method
앞서 명시된 forward 메소드는 데이터가 네트워크를 통해 흐르는 방법을 정의합니다. 이 방법은 네트워크를 통해 데이터를 전달할 때 자동으로 호출됩니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
    x = torch.relu(self.fc1(x))
    x = torch.relu(self.fc2(x))
    <span class="hljs-keyword">return</span> self.fc3(x)
</code></pre>
<p>첫 번째 layer에서 입력 데이터 x는 첫 번째 fully connected layer (self.fc1)를 통해 전달됩니다. 그런 다음 출력은 ReLU (Rectified Linear Unit) 활성화 함수를 사용하여 변환됩니다:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">x = torch.<span class="hljs-title function_">relu</span>(self.<span class="hljs-title function_">fc1</span>(x));
</code></pre>
<p>ReLU(Recified Linear Unit) 활성화 함수는 다음과 같이 정의됩니다:</p>
<p><img src="/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_11.png" alt="ReLU activation function"></p>
<p>모델에 비선형성을 도입하여 네트워크가 더 복잡한 기능을 학습할 수 있게합니다.</p>
<div class="content-ad"></div>
<p>두 번째 레이어에서는 첫 번째 레이어의 출력이 두 번째 완전 연결 레이어 (self.fc2)를 통과하고 다시 ReLU 활성화 함수를 사용하여 변환됩니다:</p>
<pre><code class="hljs language-js">x = torch.<span class="hljs-title function_">relu</span>(self.<span class="hljs-title function_">fc2</span>(x));
</code></pre>
<p>마지막으로 출력 레이어에서는 두 번째 레이어의 출력이 활성화 함수 없이 세 번째 완전 연결 레이어 (self.fc3)를 통해 전달됩니다:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">return</span> self.<span class="hljs-title function_">fc3</span>(x);
</code></pre>
<div class="content-ad"></div>
<p>이 레이어는 각 액션에 대한 최종 Q-값을 생성합니다. 각 값은 해당 상태에서 그 액션을 취했을 때의 예상 미래 보상을 나타냅니다.</p>
<h2>4.3: 경험 재생 구현</h2>
<p>ReplayBuffer 클래스는 경험을 저장하고 샘플링하는 메커니즘을 제공하여 DQN에서 학습 과정을 안정화하고 개선하는 데 필수적입니다. 따라서 에이전트가 다양한 과거 경험 세트로부터 학습할 수 있도록 해주어 일반화하고 복잡한 환경에서 잘 수행할 수 있는 능력을 향상시킵니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ReplayBuffer</span>:
    def <span class="hljs-title function_">__init__</span>(self, capacity):
        self.<span class="hljs-property">buffer</span> = <span class="hljs-title function_">deque</span>(maxlen=capacity)

    def <span class="hljs-title function_">push</span>(self, state, action, reward, next_state, done):
        self.<span class="hljs-property">buffer</span>.<span class="hljs-title function_">append</span>((state, action, reward, next_state, done))

    def <span class="hljs-title function_">sample</span>(self, batch_size):
        state, action, reward, next_state, done = <span class="hljs-title function_">zip</span>(*random.<span class="hljs-title function_">sample</span>(self.<span class="hljs-property">buffer</span>, batch_size))
        <span class="hljs-keyword">return</span> state, action, reward, next_state, done

    def <span class="hljs-title function_">__len__</span>(self):
        <span class="hljs-keyword">return</span> <span class="hljs-title function_">len</span>(self.<span class="hljs-property">buffer</span>)
</code></pre>
<div class="content-ad"></div>
<p>4.3.1: 클래스 초기화</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ReplayBuffer</span>:
    def <span class="hljs-title function_">__init__</span>(self, capacity):
        self.<span class="hljs-property">buffer</span> = <span class="hljs-title function_">deque</span>(maxlen=capacity)
</code></pre>
<p><strong>init</strong> 메서드는 고정된 용량을 갖는 deque(덱, 이중 연결 리스트)를 초기화합니다. 덱은 양쪽 끝에서 효율적으로 항목을 추가하고 제거할 수 있는 자료 구조입니다. 빠른 양쪽 끝에서의 추가와 제거가 필요한 큐(queue)나 스택(stack)을 구현할 때 유용합니다.</p>
<p>self.buffer = deque(maxlen=capacity)는 capacity만큼의 경험을 저장할 수 있는 deque를 생성합니다. 버퍼가 가득 차면 새로운 경험을 추가하면 가장 오래된 경험이 자동으로 제거됩니다.</p>
<div class="content-ad"></div>
<p>4.3.2: Push Method</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">push</span>(<span class="hljs-params">self, state, action, reward, next_state, done</span>):
    self.buffer.append((state, action, reward, next_state, done))
</code></pre>
<p>푸시 메서드는 버퍼에 새로운 경험을 추가합니다. 각 경험은 상태(state), 액션(action), 보상(reward), 다음 상태(next_state), 완료 여부(done)로 구성된 튜플입니다:</p>
<ul>
<li>state: 현재 상태.</li>
<li>action: 에이전트가 취한 행동.</li>
<li>reward: 행동을 취한 후 받은 보상.</li>
<li>next_state: 행동을 취한 후 에이전트가 이동한 상태.</li>
<li>done: 에피소드가 종료되었는지를 나타내는 부울 값.</li>
</ul>
<div class="content-ad"></div>
<p>4.3.3: 샘플 메서드</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sample</span>(<span class="hljs-params">self, batch_size</span>):
    state, action, reward, next_state, done = <span class="hljs-built_in">zip</span>(*random.sample(self.buffer, batch_size))
    <span class="hljs-keyword">return</span> state, action, reward, next_state, done
</code></pre>
<p>샘플 메서드는 버퍼에서 무작위로 일괄 경험을 검색합니다.</p>
<p>random.sample(self.buffer, batch_size)는 버퍼에서 batch_size개의 경험을 무작위로 선택합니다.</p>
<div class="content-ad"></div>
<p>"zip(*random.sample(self.buffer, batch_size))"은 경험 목록을 상태, 행동, 보상, 다음 상태 및 완료에 대한 별도의 튜플로 풀어낼 수 있습니다.</p>
<p>이 메서드는 샘플된 경험들로 이러한 튜플을 반환합니다.</p>
<p>4.3.4: Length Method</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):
    <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.buffer)
</code></pre>
<div class="content-ad"></div>
<p><strong>len</strong> 메서드는 버퍼에 저장된 현재 경험 수를 반환합니다.</p>
<h2>4.4: 타겟 네트워크 구현</h2>
<p>타겟 네트워크를 통해 안정적인 Q 값 세트를 제공하여 훈련을 위해 학습 프로세스를 안정화하고 복잡한 환경에서 에이전트의 성능을 향상시킵니다. 타겟 네트워크는 주 네트워크보다 덜 자주 업데이트되어 메인 네트워크의 가중치를 업데이트하는 데 사용되는 Q 값 추정치가 더 안정적임을 보장합니다.</p>
<p>DQNTrainer라는 클래스 내에 타겟 네트워크를 구현할 것이며, 이 클래스는 DQN의 훈련 프로세스를 관리하고 주 및 타겟 네트워크, 옵티마이저 및 재생 버퍼를 포함합니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DQNTrainer</span>:
    def <span class="hljs-title function_">__init__</span>(self, env, main_network, target_network, optimizer, replay_buffer, model_path=<span class="hljs-string">'model/model.pth'</span>, gamma=<span class="hljs-number">0.99</span>, batch_size=<span class="hljs-number">64</span>, target_update_frequency=<span class="hljs-number">1000</span>):
        self.<span class="hljs-property">env</span> = env
        self.<span class="hljs-property">main_network</span> = main_network
        self.<span class="hljs-property">target_network</span> = target_network
        self.<span class="hljs-property">optimizer</span> = optimizer
        self.<span class="hljs-property">replay_buffer</span> = replay_buffer
        self.<span class="hljs-property">model_path</span> = model_path
        self.<span class="hljs-property">gamma</span> = gamma
        self.<span class="hljs-property">batch_size</span> = batch_size
        self.<span class="hljs-property">target_update_frequency</span> = target_update_frequency
        self.<span class="hljs-property">step_count</span> = <span class="hljs-number">0</span>
</code></pre>
<ul>
<li>DQNTrainer 클래스 정의:</li>
</ul>
<div class="content-ad"></div>
<p><strong>init</strong> 메서드는 학습에 필요한 다양한 구성 요소를 초기화합니다:</p>
<ul>
<li>
<p>env: 에이전트가 작동하는 환경입니다.</p>
</li>
<li>
<p>main_network: 훈련 중인 주요 신경망입니다.</p>
</li>
<li>
<p>target_network: Q-값 추정을 안정화하는 데 사용되는 대상 신경망입니다.</p>
</li>
<li>
<p>optimizer: 주요 신경망의 가중치를 업데이트하는 데 사용되는 옵티마이저입니다.</p>
</li>
<li>
<p>replay_buffer: 경험을 저장하고 샘플링하는 버퍼입니다.</p>
</li>
<li>
<p>model_path: 훈련된 모델을 저장하고 로드하기 위한 경로입니다.</p>
</li>
<li>
<p>gamma: 미래 보상에 대한 할인 계수입니다.</p>
</li>
<li>
<p>batch_size: 각 훈련 단계에서 재생 버퍼에서 샘플링된 경험의 수입니다.</p>
</li>
<li>
<p>target_update_frequency: 대상 네트워크의 가중치를 주요 네트워크의 가중치에 맞게 업데이트하는 빈도입니다.</p>
</li>
<li>
<p>step_count: 훈련 중에 취한 단계 수를 추적하는 카운터입니다.</p>
<p>4.4.2: 모델 로딩</p>
</li>
</ul>
<pre><code class="hljs language-js"># 모델이 있으면 로드
        <span class="hljs-keyword">if</span> os.<span class="hljs-property">path</span>.<span class="hljs-title function_">exists</span>(os.<span class="hljs-property">path</span>.<span class="hljs-title function_">dirname</span>(self.<span class="hljs-property">model_path</span>)):
            <span class="hljs-keyword">if</span> os.<span class="hljs-property">path</span>.<span class="hljs-title function_">isfile</span>(self.<span class="hljs-property">model_path</span>):
                self.<span class="hljs-property">main_network</span>.<span class="hljs-title function_">load_state_dict</span>(torch.<span class="hljs-title function_">load</span>(self.<span class="hljs-property">model_path</span>))
                self.<span class="hljs-property">target_network</span>.<span class="hljs-title function_">load_state_dict</span>(torch.<span class="hljs-title function_">load</span>(self.<span class="hljs-property">model_path</span>))
                <span class="hljs-title function_">print</span>(<span class="hljs-string">"디스크에서 모델 로드됨"</span>)
        <span class="hljs-attr">else</span>:
            os.<span class="hljs-title function_">makedirs</span>(os.<span class="hljs-property">path</span>.<span class="hljs-title function_">dirname</span>(self.<span class="hljs-property">model_path</span>))
</code></pre>
<div class="content-ad"></div>
<p>우리는 모델 경로의 디렉토리가 존재하는지 os.path.exists(os.path.dirname(self.model_path))를 사용하여 확인합니다. 저장된 모델이 있으면, 훈련을 멈춘 지점부터 계속하기 위해 불러옵니다:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">if</span> os.<span class="hljs-property">path</span>.<span class="hljs-title function_">isfile</span>(self.<span class="hljs-property">model_path</span>):
    self.<span class="hljs-property">main_network</span>.<span class="hljs-title function_">load_state_dict</span>(torch.<span class="hljs-title function_">load</span>(self.<span class="hljs-property">model_path</span>))
    self.<span class="hljs-property">target_network</span>.<span class="hljs-title function_">load_state_dict</span>(torch.<span class="hljs-title function_">load</span>(self.<span class="hljs-property">model_path</span>))
    <span class="hljs-title function_">print</span>(<span class="hljs-string">"디스크에서 모델을 불러왔습니다"</span>)
</code></pre>
<p>torch.load는 load_state_dict를 사용하여 저장된 모델 가중치를 메인 및 타겟 네트워크에 불러옵니다. 모델 디렉토리가 존재하지 않는 경우, os.makedirs를 사용하여 만듭니다.</p>
<h2>4.5: 딥 Q-네트워크 훈련</h2>
<div class="content-ad"></div>
<p>다음으로, 우리는 DQN을 훈련하기 위한 학습 루프를 구현할 것입니다. 이 방법은 DQNTrainer 내에 이루어집니다. DQN을 위한 훈련 루프를 실행하며, 에이전트가 환경과 상호 작용하고 경험을 수집하며 네트워크를 업데이트하고 성능을 추적합니다.</p>
<p>다음은 학습 루프의 코드입니다:</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">train</span>(self, num_episodes, save=<span class="hljs-title class_">True</span>):
    total_rewards = []
    <span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(num_episodes):
        state, _ = self.<span class="hljs-property">env</span>.<span class="hljs-title function_">reset</span>()
        done = <span class="hljs-title class_">False</span>
        total_reward = <span class="hljs-number">0</span>

        <span class="hljs-keyword">while</span> not <span class="hljs-attr">done</span>:
            self.<span class="hljs-property">env</span>.<span class="hljs-title function_">render</span>()  # 환경을 렌더링하기 위해 이 줄을 추가합니다
            action = self.<span class="hljs-title function_">main_network</span>(torch.<span class="hljs-title class_">FloatTensor</span>(state).<span class="hljs-title function_">unsqueeze</span>(<span class="hljs-number">0</span>)).<span class="hljs-title function_">argmax</span>(dim=<span class="hljs-number">1</span>).<span class="hljs-title function_">item</span>()
            next_state, reward, done, _, _ = self.<span class="hljs-property">env</span>.<span class="hljs-title function_">step</span>(action)
            self.<span class="hljs-property">replay_buffer</span>.<span class="hljs-title function_">push</span>(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward

            <span class="hljs-keyword">if</span> <span class="hljs-title function_">len</span>(self.<span class="hljs-property">replay_buffer</span>) >= self.<span class="hljs-property">batch_size</span>:
                self.<span class="hljs-title function_">update_network</span>()

        total_rewards.<span class="hljs-title function_">append</span>(total_reward)
        <span class="hljs-title function_">print</span>(f<span class="hljs-string">"Episode {episode}, Total Reward: {total_reward}"</span>)

    <span class="hljs-keyword">if</span> <span class="hljs-attr">save</span>:
        torch.<span class="hljs-title function_">save</span>(self.<span class="hljs-property">main_network</span>.<span class="hljs-title function_">state_dict</span>(), self.<span class="hljs-property">model_path</span>)
        <span class="hljs-title function_">print</span>(<span class="hljs-string">"모델을 디스크에 저장했습니다"</span>)

    self.<span class="hljs-property">env</span>.<span class="hljs-title function_">close</span>()
    <span class="hljs-keyword">return</span> <span class="hljs-title function_">sum</span>(total_rewards) / <span class="hljs-title function_">len</span>(total_rewards)
</code></pre>
<p>train 메서드는 지정된 에피소드 수에 대해 훈련 루프를 실행합니다. 이 루프는 에이전트가 경험을 쌓고 의사 결정 능력을 향상시키는 데 중요합니다.</p>
<div class="content-ad"></div>
<p>4.5.1: 훈련 루프
우선 total_rewards를 빈 리스트로 초기화해 봅시다:</p>
<pre><code class="hljs language-js">total_rewards = [];
</code></pre>
<p>이제 훈련 루프를 만들어 봅시다:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(num_episodes):
</code></pre>
<div class="content-ad"></div>
<p>이 루프는 지정된 에피소드 수만큼 실행됩니다. 각 에피소드는 환경과의 완전한 상호작용 순서를 나타냅니다.</p>
<p>4.5.2: 환경 재설정
각 에피소드의 시작 시점에는 환경이 초기 상태로 재설정됩니다.</p>
<pre><code class="hljs language-js">state, (_ = self.<span class="hljs-property">env</span>.<span class="hljs-title function_">reset</span>());
done = <span class="hljs-title class_">False</span>;
total_reward = <span class="hljs-number">0</span>;
</code></pre>
<ul>
<li>self.env.reset()은 환경을 초기화하고 초기 상태를 반환합니다.</li>
<li>done = False는 에피소드가 완료되지 않았음을 나타냅니다.</li>
<li>total_reward = 0은 현재 에피소드의 총 보상을 초기화합니다.</li>
</ul>
<div class="content-ad"></div>
<h3>4.4.3: Action Selection</h3>
<p>에이전트는 현재 상태를 기반으로 메인 네트워크를 사용하여 작업을 선택합니다.</p>
<pre><code class="hljs language-python">action = self.main_network(torch.FloatTensor(state).unsqueeze(<span class="hljs-number">0</span>)).argmax(dim=<span class="hljs-number">1</span>).item()
</code></pre>
<p>torch.FloatTensor(state).unsqueeze(0)은 상태를 PyTorch 텐서로 변환하고 네트워크가 예상하는 입력 형태와 일치하도록 추가 차원을 추가합니다.</p>
<p>self.main_network(...).argmax(dim=1).item()는 메인 네트워크가 예측한 가장 높은 Q 값으로 작업을 선택합니다.</p>
<div class="content-ad"></div>
<p>4.5.4: 단계 및 저장 환경
에이전트가 선택한 동작을 수행하고 보상 및 다음 상태를 관찰한 후, 해당 경험을 재생 버퍼에 저장합니다.</p>
<pre><code class="hljs language-js">next_state, reward, done, _, (_ = self.<span class="hljs-property">env</span>.<span class="hljs-title function_">step</span>(action));
self.<span class="hljs-property">replay_buffer</span>.<span class="hljs-title function_">push</span>(state, action, reward, next_state, done);
state = next_state;
total_reward += reward;
</code></pre>
<ul>
<li>
<p>self.env.step(action)은 동작을 실행하고 다음 상태, 보상 및 에피소드 완료 여부를 반환합니다.</p>
</li>
<li>
<p>self.replay_buffer.push(...)는 재생 버퍼에 경험을 저장합니다.</p>
</li>
<li>
<p>state = next_state는 현재 상태를 다음 상태로 업데이트합니다.</p>
</li>
<li>
<p>total_reward += reward은 현재 에피소드의 보상을 누적합니다.</p>
<p>4.5.5: 네트워크 업데이트
재생 버퍼에 충분한 경험이 있을 경우, 네트워크가 업데이트됩니다.</p>
</li>
</ul>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">if</span> <span class="hljs-title function_">len</span>(self.<span class="hljs-property">replay_buffer</span>) >= self.<span class="hljs-property">batch_size</span>:
    self.<span class="hljs-title function_">update_network</span>()
</code></pre>
<p><code>if len(self.replay_buffer) >= self.batch_size</code>은 replay buffer가 적어도 batch_size의 경험을 가지고 있는지 확인합니다.</p>
<p>self.update_network()은 replay buffer에서 일괄적인 경험을 사용하여 네트워크를 업데이트합니다.</p>
<p>4.5.6: 에피소드 종료
총 보상은 각 에피소드의 끝에서 기록되고 출력됩니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">total_rewards.<span class="hljs-title function_">append</span>(total_reward)
<span class="hljs-title function_">print</span>(f<span class="hljs-string">"에피소드 {episode}, 총 보상: {total_reward}"</span>)
</code></pre>
<p>total_rewards.append(total_reward)는 현재 에피소드의 총 보상을 총 보상 목록에 추가합니다.</p>
<p>print(f"에피소드 {episode}, 총 보상: {total_reward}")은 에피소드 번호와 총 보상을 출력합니다.</p>
<p>4.5.7: 모델 저장
훈련 후, 모델은 디스크에 저장됩니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">save가 <span class="hljs-title class_">True</span>이면:
   torch.<span class="hljs-title function_">save</span>(self.<span class="hljs-property">main_network</span>.<span class="hljs-title function_">state_dict</span>(), self.<span class="hljs-property">model_path</span>)
   <span class="hljs-title function_">print</span>(<span class="hljs-string">"모델이 디스크에 저장되었습니다."</span>)
</code></pre>
<p>if save:는 save 플래그가 True인지 확인합니다.</p>
<p>torch.save(self.main_network.state_dict(), self.model_path)는 메인 네트워크의 상태 딕셔너리를 지정된 파일 경로에 저장합니다.</p>
<p>4.5.8: 평균 보상 반환
마지막으로, 이 메서드는 환경을 닫고 모든 에피소드에 대한 평균 보상을 반환합니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">self.<span class="hljs-property">env</span>.<span class="hljs-title function_">close</span>();
<span class="hljs-keyword">return</span> <span class="hljs-title function_">sum</span>(total_rewards) / <span class="hljs-title function_">len</span>(total_rewards);
</code></pre>
<p>self.env.close()는 환경을 닫습니다.</p>
<p>return sum(total_rewards) / len(total_rewards)는 평균 보상을 계산하고 반환합니다.</p>
<h2>4.6: 모델 튜닝</h2>
<div class="content-ad"></div>
<p>마침내 훈련된 모델을 평가하고 튜닝하는 방법을 살펴보겠습니다. DQN의 성능을 향상시키기 위해 하이퍼파라미터를 최적화할 책임을 가질 Optimizer 클래스를 만들어보겠습니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Optimizer</span>:
    def <span class="hljs-title function_">__init__</span>(self, env, main_network, target_network, replay_buffer, model_path, params_path=<span class="hljs-string">'params.pkl'</span>):
        self.<span class="hljs-property">env</span> = env
        self.<span class="hljs-property">main_network</span> = main_network
        self.<span class="hljs-property">target_network</span> = target_network
        self.<span class="hljs-property">replay_buffer</span> = replay_buffer
        self.<span class="hljs-property">model_path</span> = model_path
        self.<span class="hljs-property">params_path</span> = params_path

    def <span class="hljs-title function_">objective</span>(self, trial, n_episodes=<span class="hljs-number">10</span>):
        lr = trial.<span class="hljs-title function_">suggest_loguniform</span>(<span class="hljs-string">'lr'</span>, <span class="hljs-number">1e-5</span>, <span class="hljs-number">1e-1</span>)
        gamma = trial.<span class="hljs-title function_">suggest_uniform</span>(<span class="hljs-string">'gamma'</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">0.999</span>)
        batch_size = trial.<span class="hljs-title function_">suggest_categorical</span>(<span class="hljs-string">'batch_size'</span>, [<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">128</span>])
        target_update_frequency = trial.<span class="hljs-title function_">suggest_categorical</span>(<span class="hljs-string">'target_update_frequency'</span>, [<span class="hljs-number">500</span>, <span class="hljs-number">1000</span>, <span class="hljs-number">2000</span>])

        optimizer = optim.<span class="hljs-title class_">Adam</span>(self.<span class="hljs-property">main_network</span>.<span class="hljs-title function_">parameters</span>(), lr=lr)
        trainer = <span class="hljs-title class_">DQNTrainer</span>(self.<span class="hljs-property">env</span>, self.<span class="hljs-property">main_network</span>, self.<span class="hljs-property">target_network</span>, optimizer, self.<span class="hljs-property">replay_buffer</span>, self.<span class="hljs-property">model_path</span>, gamma=gamma, batch_size=batch_size, target_update_frequency=target_update_frequency)
        reward = trainer.<span class="hljs-title function_">train</span>(n_episodes, save=<span class="hljs-title class_">False</span>)
        <span class="hljs-keyword">return</span> reward

    def <span class="hljs-title function_">optimize</span>(self, n_trials=<span class="hljs-number">100</span>, save_params=<span class="hljs-title class_">True</span>):
        <span class="hljs-keyword">if</span> not <span class="hljs-variable constant_">TRAIN</span> and os.<span class="hljs-property">path</span>.<span class="hljs-title function_">isfile</span>(self.<span class="hljs-property">params_path</span>):
            <span class="hljs-keyword">with</span> <span class="hljs-title function_">open</span>(self.<span class="hljs-property">params_path</span>, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> <span class="hljs-attr">f</span>:
                best_params = pickle.<span class="hljs-title function_">load</span>(f)
            <span class="hljs-title function_">print</span>(<span class="hljs-string">"디스크에서 매개변수를 불러왔습니다"</span>)
        elif not <span class="hljs-attr">FINETUNE</span>:
            best_params = {
                <span class="hljs-string">'lr'</span>: <span class="hljs-variable constant_">LEARNING_RATE</span>,
                <span class="hljs-string">'gamma'</span>: <span class="hljs-variable constant_">GAMMA</span>,
                <span class="hljs-string">'batch_size'</span>: <span class="hljs-variable constant_">BATCH_SIZE</span>,
                <span class="hljs-string">'target_update_frequency'</span>: <span class="hljs-variable constant_">TARGET_UPDATE_FREQUENCY</span>
                }
            <span class="hljs-title function_">print</span>(f<span class="hljs-string">"기본 매개변수 사용 중: {best_params}"</span>)
        <span class="hljs-attr">else</span>:
            <span class="hljs-title function_">print</span>(<span class="hljs-string">"하이퍼파라미터 최적화 중"</span>)
            study = optuna.<span class="hljs-title function_">create_study</span>(direction=<span class="hljs-string">'maximize'</span>)
            study.<span class="hljs-title function_">optimize</span>(self.<span class="hljs-property">objective</span>, n_trials=n_trials)
            best_params = study.<span class="hljs-property">best_params</span>

            <span class="hljs-keyword">if</span> <span class="hljs-attr">save_params</span>:
                <span class="hljs-keyword">with</span> <span class="hljs-title function_">open</span>(self.<span class="hljs-property">params_path</span>, <span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> <span class="hljs-attr">f</span>:
                    pickle.<span class="hljs-title function_">dump</span>(best_params, f)
                <span class="hljs-title function_">print</span>(<span class="hljs-string">"매개변수를 디스크에 저장했습니다"</span>)

        <span class="hljs-keyword">return</span> best_params
</code></pre>
<p>4.6.1: 클래스 정의</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Optimizer</span>:
    def <span class="hljs-title function_">__init__</span>(self, env, main_network, target_network, replay_buffer, model_path, params_path=<span class="hljs-string">'params.pkl'</span>):
        self.<span class="hljs-property">env</span> = env
        self.<span class="hljs-property">main_network</span> = main_network
        self.<span class="hljs-property">target_network</span> = target_network
        self.<span class="hljs-property">replay_buffer</span> = replay_buffer
        self.<span class="hljs-property">model_path</span> = model_path
        self.<span class="hljs-property">params_path</span> = params_path
</code></pre>
<div class="content-ad"></div>
<p><strong>init</strong> 메서드는 최적화에 필요한 다양한 구성 요소를 초기화합니다:</p>
<ul>
<li>
<p>env: 에이전트가 작동하는 환경.</p>
</li>
<li>
<p>main_network: 주요 신경망.</p>
</li>
<li>
<p>target_network: 타겟 신경망.</p>
</li>
<li>
<p>replay_buffer: 경험을 저장하고 샘플링하는 버퍼.</p>
</li>
<li>
<p>model_path: 훈련된 모델을 저장하거나 불러오는 경로.</p>
</li>
<li>
<p>params_path: 최적의 하이퍼파라미터를 저장하거나 불러오는 경로.</p>
<p>4.6.2: 목적 메서드</p>
</li>
</ul>
<pre><code class="hljs language-js">def <span class="hljs-title function_">objective</span>(self, trial, n_episodes=<span class="hljs-number">10</span>):
        lr = trial.<span class="hljs-title function_">suggest_loguniform</span>(<span class="hljs-string">'lr'</span>, <span class="hljs-number">1e-5</span>, <span class="hljs-number">1e-1</span>)
        gamma = trial.<span class="hljs-title function_">suggest_uniform</span>(<span class="hljs-string">'gamma'</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">0.999</span>)
        batch_size = trial.<span class="hljs-title function_">suggest_categorical</span>(<span class="hljs-string">'batch_size'</span>, [<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">128</span>])
        target_update_frequency = trial.<span class="hljs-title function_">suggest_categorical</span>(<span class="hljs-string">'target_update_frequency'</span>, [<span class="hljs-number">500</span>, <span class="hljs-number">1000</span>, <span class="hljs-number">2000</span>])

        optimizer = optim.<span class="hljs-title class_">Adam</span>(self.<span class="hljs-property">main_network</span>.<span class="hljs-title function_">parameters</span>(), lr=lr)
        trainer = <span class="hljs-title class_">DQNTrainer</span>(self.<span class="hljs-property">env</span>, self.<span class="hljs-property">main_network</span>, self.<span class="hljs-property">target_network</span>, optimizer, self.<span class="hljs-property">replay_buffer</span>, self.<span class="hljs-property">model_path</span>, gamma=gamma, batch_size=batch_size, target_update_frequency=target_update_frequency)
        reward = trainer.<span class="hljs-title function_">train</span>(n_episodes, save=<span class="hljs-title class_">False</span>)
        <span class="hljs-keyword">return</span> reward
</code></pre>
<div class="content-ad"></div>
<p>목표 함수는 하이퍼파라미터에 대한 값을 제안하고 이러한 값을 사용하여 모델을 훈련합니다.</p>
<ul>
<li>lr = trial.suggest_loguniform(<code>lr</code>, 1e-5, 1e-1): 범위 [1e-5, 1e-1] 내의 학습률을 제안합니다.</li>
<li>gamma = trial.suggest_uniform(<code>gamma</code>, 0.9, 0.999): 범위 [0.9, 0.999] 내의 할인 요인을 제안합니다.</li>
<li>batch_size = trial.suggest_categorical(<code>batch_size</code>, [32, 64, 128]): 지정된 목록에서 배치 크기를 제안합니다.</li>
<li>target_update_frequency = trial.suggest_categorical(<code>target_update_frequency</code>, [500, 1000, 2000]): 지정된 목록에서 대상 업데이트 빈도를 제안합니다.</li>
</ul>
<pre><code class="hljs language-js">optimizer = optim.<span class="hljs-title class_">Adam</span>(self.<span class="hljs-property">main_network</span>.<span class="hljs-title function_">parameters</span>(), (lr = lr));
</code></pre>
<p>여기서는 주어진 학습률로 Adam 옵티마이저를 설정합니다. Adam은 주로 신경망 훈련에 사용되는 최적화 알고리즘인 Adaptive Moment Estimation의 약자입니다.</p>
<div class="content-ad"></div>
<p>신경망에서 각 매개변수에 대해 Adam은 손실 함수의 기울기를 계산합니다. 그것은 기울기의 지수 이동 평균 (첫 번째 모멘트로 표시되는 m)과 제곱 기울기 (두 번째 모멘트로 표시되는 v)를 추적합니다.</p>
<p>이동 평균의 초기화 편향을 고려하기 위해 Adam은 첫 번째 및 두 번째 모멘트 추정치에 바이어스 보정을 적용합니다. 그런 다음 매개변수는 수정된 첫 번째 및 두 번째 모멘트를 사용하여 업데이트됩니다. 업데이트 규칙은 학습률과 모멘트를 통합하여 기울기의 크기와 방향을 모두 고려하는 방식으로 매개변수를 조정합니다.</p>
<p>다음은 Adam에 대한 보다 포괄적인 기사입니다:</p>
<pre><code class="hljs language-js">trainer = <span class="hljs-title class_">DQNTrainer</span>(
  self.<span class="hljs-property">env</span>,
  self.<span class="hljs-property">main_network</span>,
  self.<span class="hljs-property">target_network</span>,
  optimizer,
  self.<span class="hljs-property">replay_buffer</span>,
  self.<span class="hljs-property">model_path</span>,
  (gamma = gamma),
  (batch_size = batch_size),
  (target_update_frequency = target_update_frequency)
);
</code></pre>
<div class="content-ad"></div>
<p>이 코드는 제안된 하이퍼파라미터로 theDQNTrainer 인스턴스를 초기화합니다.</p>
<pre><code class="hljs language-js">reward = trainer.<span class="hljs-title function_">train</span>(n_episodes, (save = <span class="hljs-title class_">False</span>));
</code></pre>
<p>마지막으로, 이 코드는 지정된 에피소드 수로 모델을 학습하고 평균 보상을 반환합니다.</p>
<p>4.6.3: 최적화 메소드
이 섹션에서는 모델의 성능을 극대화하는 조합을 효율적으로 찾을 수 있도록 하이퍼파라미터 공간을 체계적으로 탐색하는 파이썬 라이브러리인 Optuna를 사용하겠습니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">optimize</span>(<span class="hljs-params">self, n_trials=<span class="hljs-number">100</span>, save_params=<span class="hljs-literal">True</span></span>):
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> TRAIN <span class="hljs-keyword">and</span> os.path.isfile(self.params_path):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(self.params_path, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> f:
            best_params = pickle.load(f)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"디스크에서 매개변수를 불러왔습니다."</span>)
    <span class="hljs-keyword">elif</span> <span class="hljs-keyword">not</span> FINETUNE:
        best_params = {
            <span class="hljs-string">'lr'</span>: LEARNING_RATE,
            <span class="hljs-string">'gamma'</span>: GAMMA,
            <span class="hljs-string">'batch_size'</span>: BATCH_SIZE,
            <span class="hljs-string">'target_update_frequency'</span>: TARGET_UPDATE_FREQUENCY
        }
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"기본 매개변수를 사용합니다: <span class="hljs-subst">{best_params}</span>"</span>)
    <span class="hljs-keyword">else</span>:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"하이퍼파라미터를 최적화 중입니다."</span>)
        study = optuna.create_study(direction=<span class="hljs-string">'maximize'</span>)
        study.optimize(self.objective, n_trials=n_trials)
        best_params = study.best_params

        <span class="hljs-keyword">if</span> save_params:
            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(self.params_path, <span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> f:
                pickle.dump(best_params, f)
            <span class="hljs-built_in">print</span>(<span class="hljs-string">"매개변수를 디스크에 저장했습니다."</span>)

    <span class="hljs-keyword">return</span> best_params
</code></pre>
<p><code>optimize</code> 메소드는 지정된 횟수의 시도에 대해 최적화 프로세스를 실행합니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> TRAIN <span class="hljs-keyword">and</span> os.path.isfile(self.params_path):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(self.params_path, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> f:
            best_params = pickle.load(f)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"디스크에서 매개변수를 불러왔습니다."</span>)
</code></pre>
<p>학습이 필요하지 않은 경우 (TRAIN이 아닌 경우) 및 매개변수 파일이 존재하는 경우, 매개변수가 디스크에서 로드됩니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-python"><span class="hljs-keyword">elif</span> <span class="hljs-keyword">not</span> FINETUNE:
    best_params = {
        <span class="hljs-string">'lr'</span>: LEARNING_RATE,
        <span class="hljs-string">'gamma'</span>: GAMMA,
        <span class="hljs-string">'batch_size'</span>: BATCH_SIZE,
        <span class="hljs-string">'target_update_frequency'</span>: TARGET_UPDATE_FREQUENCY
    }
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"기본 매개변수 사용 중: <span class="hljs-subst">{best_params}</span>"</span>)
</code></pre>
<p>만약 파라미터 튜닝이 필요하지 않다면 (not FINETUNE), 기본 매개변수가 사용됩니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">else</span>:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"하이퍼파라미터 최적화 중"</span>)
    study = optuna.create_study(direction=<span class="hljs-string">'maximize'</span>)
    study.optimize(self.objective, n_trials=n_trials)
    best_params = study.best_params

    <span class="hljs-keyword">if</span> save_params:
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(self.params_path, <span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> f:
            pickle.dump(best_params, f)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"매개변수를 디스크에 저장했습니다"</span>)
</code></pre>
<p>하이퍼파라미터 최적화가 필요한 경우, Optuna를 사용하여 최적의 매개변수를 찾습니다.</p>
<div class="content-ad"></div>
<p>study = optuna.create_study(direction='maximize')을 사용하면 목적 함수를 최대화하는 Optuna 스터디를 생성할 수 있어요.</p>
<p>study.optimize(self.objective, n_trials=n_trials)은 지정된 횟수의 시행을 위한 최적화를 실행해요.</p>
<p>save_params를 True로 설정하면, 최적의 매개변수가 디스크에 저장돼요.</p>
<p>다음은 Optuna를 깊이 들여다보는 포함한 다양한 세밀 조정 기법을 탐구한 멋진 기사에요:</p>
<div class="content-ad"></div>
<h2>4.7: 모델 실행하기</h2>
<p>마지막으로, 모든 과정을 다시 한번 확인하고 코드를 실행해 봅시다!</p>
<p>4.7.1: 훈련 및 파인튜닝 설정</p>
<pre><code class="hljs language-js"><span class="hljs-variable constant_">TRAIN</span> = <span class="hljs-title class_">True</span>
<span class="hljs-variable constant_">FINETUNE</span> = <span class="hljs-title class_">False</span>

# 다음 하이퍼파라미터를 설정하세요 (<span class="hljs-variable constant_">FINETUNE</span>이 <span class="hljs-title class_">False</span>인 경우)
<span class="hljs-variable constant_">GAMMA</span> = <span class="hljs-number">0.99</span>
<span class="hljs-variable constant_">BATCH_SIZE</span> = <span class="hljs-number">64</span>
<span class="hljs-variable constant_">TARGET_UPDATE_FREQUENCY</span> = <span class="hljs-number">1000</span>
<span class="hljs-variable constant_">LEARNING_RATE</span> = <span class="hljs-number">1e-3</span>
</code></pre>
<div class="content-ad"></div>
<p>TRAIN = True은 모델을 학습할지 여부를 나타냅니다. False로 설정하면 학습이 건너뛰어집니다.</p>
<p>FINETUNE = False는 모델을 fine-tune할지 여부를 나타냅니다. True로 설정하면 기존 매개변수가 사용되고 fine-tune됩니다.</p>
<p>만약 FINETUNE이 False인 경우, 다음 하이퍼파라미터를 설정합니다:</p>
<ul>
<li>GAMMA = 0.99: 미래 보상에 대한 할인 계수입니다. 이는 즉시 보상에 비해 미래 보상이 얼마나 중요한지를 결정합니다.</li>
<li>BATCH_SIZE = 64: 각 학습 단계마다 재생 버퍼에서 샘플링된 경험의 수입니다.</li>
<li>TARGET_UPDATE_FREQUENCY = 1000: 타겟 네트워크의 가중치가 주요 네트워크의 가중치와 일치하도록 업데이트되는 빈도(스텝 단위).</li>
<li>LEARNING_RATE = 1e-3: 최적화기(optimizer)의 학습률로, 모델 가중치가 업데이트될 때 추정 오차에 따라 모델을 얼마나 변경할지를 제어합니다.</li>
</ul>
<div class="content-ad"></div>
<p>4.7.2: 네트워크 및 재생 버퍼 초기화</p>
<pre><code class="hljs language-js">main_network = <span class="hljs-title function_">DQN</span>(state_dim, action_dim);
target_network = <span class="hljs-title function_">DQN</span>(state_dim, action_dim);
target_network.<span class="hljs-title function_">load_state_dict</span>(main_network.<span class="hljs-title function_">state_dict</span>());
target_network.<span class="hljs-built_in">eval</span>();

replay_buffer = <span class="hljs-title class_">ReplayBuffer</span>(<span class="hljs-number">10000</span>);
</code></pre>
<p>main_network = DQN(state_dim, action_dim)은 지정된 상태 및 액션 차원으로 메인 네트워크를 초기화합니다.</p>
<p>target_network = DQN(state_dim, action_dim)은 메인 네트워크와 동일한 구조로 대상 네트워크를 초기화합니다.</p>
<div class="content-ad"></div>
<p>target_network.load_state_dict(main_network.state_dict()) 함수는 메인 네트워크의 가중치를 타겼 네트워크로 복사합니다.</p>
<p>target_network.eval() 함수는 타겟 네트워크를 평가 모드로 설정합니다. 이는 추론 중에 드롭아웃과 배치 정규화와 같은 특정 레이어가 적절하게 동작하도록 합니다.</p>
<p>replay_buffer = ReplayBuffer(10000)은 10,000개의 경험을 저장할 수 있는 용량을 가진 리플레이 버퍼를 초기화합니다.</p>
<p>4.7.3: 단계 카운트 설정</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-variable constant_">STEP_COUNT</span> = <span class="hljs-number">0</span>;
</code></pre>
<p>STEP_COUNT = 0은 훈련 중 취한 단계 수를 추적하는 카운터를 초기화합니다.</p>
<p>4.7.4: 옵티마이저 초기화 및 하이퍼파라미터 최적화</p>
<pre><code class="hljs language-js">optimizer = <span class="hljs-title class_">Optimizer</span>(env, main_network, target_network, replay_buffer, f<span class="hljs-string">'{os.path.dirname(__file__)}/model/model.pth'</span>, f<span class="hljs-string">'{os.path.dirname(__file__)}/model/params.pkl'</span>)
best_params = optimizer.<span class="hljs-title function_">optimize</span>(n_trials=<span class="hljs-number">2</span>, save_params=<span class="hljs-title class_">True</span>)
</code></pre>
<div class="content-ad"></div>
<p><code>optimizer = Optimizer(...)</code>은 환경, 네트워크, 리플레이 버퍼, 모델 경로 및 매개변수 경로로 Optimizer 클래스를 초기화합니다.</p>
<p><code>best_params = optimizer.optimize(n_trials=2, save_params=True)</code>는 최적의 하이퍼파라미터를 찾기 위해 최적화 프로세스를 실행합니다. 이 함수는 다음과 같은 기능을 수행합니다:</p>
<ul>
<li>
<p>지정된 횟수(n_trials=2)만큼 최적화를 실행합니다.</p>
</li>
<li>
<p><code>save_params</code>가 True인 경우 최적의 하이퍼파라미터를 디스크에 저장합니다.</p>
<p>4.7.5: PyTorch Optimizer 및 DQN Trainer 생성</p>
</li>
</ul>
<div class="content-ad"></div>
<pre><code class="hljs language-js">optimizer = optim.<span class="hljs-title class_">Adam</span>(main_network.<span class="hljs-title function_">parameters</span>(), lr=best_params[<span class="hljs-string">'lr'</span>])
trainer = <span class="hljs-title class_">DQNTrainer</span>(env, main_network, target_network, optimizer, replay_buffer, f<span class="hljs-string">'{os.path.dirname(__file__)}/model/model.pth'</span>, gamma=best_params[<span class="hljs-string">'gamma'</span>], batch_size=best_params[<span class="hljs-string">'batch_size'</span>], target_update_frequency=best_params[<span class="hljs-string">'target_update_frequency'</span>])
trainer.<span class="hljs-title function_">train</span>(<span class="hljs-number">1000</span>)
</code></pre>
<p><code>optimizer = optim.Adam(main_network.parameters(), lr=best_params['lr'])</code>은 최적의 하이퍼파라미터에서 학습률을 사용하여 Adam 옵티마이저를 생성합니다.</p>
<p><code>trainer = DQNTrainer(...)</code>는 환경, 네트워크, 옵티마이저, 리플레이 버퍼, 모델 경로 및 최적의 하이퍼파라미터로 DQNTrainer 클래스를 초기화합니다.</p>
<p><code>trainer.train(1000)</code>은 모델을 1000번의 에피소드 동안 훈련합니다.</p>
<div class="content-ad"></div>
<p>이제 요정의 훈련 초기 10 에피소드를 살펴보겠습니다:</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1200/1*ncnLXIRABedg4uKwVL0L5w.gif" alt="agent training"></p>
<p>여기서 모델은 서툴러서 무작위로 종종 비최적적인 결정을 내립니다. 요정이 환경을 탐험하고 기초를 배우기 때문에 이는 예상되는 현상입니다. 아직 보상을 극대화하기 위한 견고한 전략을 개발하지 못했습니다. 추가적인 훈련 에피소드를 거치면서 시간이 지남에 따라, 요정의 성능은 정책을 미세 조정하고 경험을 통해 배우면서 크게 향상되어야 합니다.</p>
<p>이제 모델이 1000번 훈련된 후의 10개의 훈련 에피소드를 살펴봅시다:</p>
<div class="content-ad"></div>
<p><img src="https://miro.medium.com/v2/resize:fit:1200/1*s4j6V4V-nLfkc18C2Z2-zA.gif" alt="image"></p>
<p>이것은 주목할 만한 개선입니다. 모델이 아직 NASA에 완성되지는 않았지만, 몇 가지 주요 향상 사항을 관찰할 수 있습니다:</p>
<ul>
<li>에이전트가 더 신중하고 전략적인 결정을 내립니다.</li>
<li>환경을 더 효율적으로 탐색합니다.</li>
<li>부적절한 조치의 빈도가 크게 감소했습니다.</li>
</ul>
<p>지속적인 훈련과 세밀한 조정을 통해, 에이전트의 성능은 더 개선될 것으로 예상되며, 최적의 행동에 더 가까워질 것입니다.</p>
<div class="content-ad"></div>
<p>이제 여러분의 차례입니다. 모델을 더 발전시켜 보세요. 하이퍼파라미터를 조정하거나 다른 모델 구조를 실험해 보세요. 창의성과 인내심으로 최선을 다하면 얼마든지 성과를 낼 수 있을 거에요. 곧 완벽하게 패치된 셔틀은 원활하게 착륙할 거예요!</p>
<h1>5: 결론</h1>
<p>딥 Q-네트워크를 구축, 훈련 및 평가하는 방법을 잘 이해하셨으니, 이제 다양한 환경에서 이 DQN을 테스트하고 다양한 도전에 적응하는 모습을 관찰해 보세요.</p>
<p>에이전트의 성능을 향상시키기 위해 고급 기술을 구현하고 새로운 아키텍처를 탐험해 보세요. 예를 들어 다양한 하이퍼파라미터를 설정해보거나 다른 최적화 알고리즘(예: SGD 또는 Nadam)을 사용하거나 다른 미세조정 알고리즘을 사용해 볼 수 있어요!</p>
<div class="content-ad"></div>
<h1>참고 자료</h1>
<ul>
<li>Sutton, R. S., &#x26; Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT Press.</li>
<li>Lin, L. J. (1992). “Self-improving reactive agents based on reinforcement learning, planning and teaching.” Machine Learning, 8(3–4), 293–321.</li>
<li>OpenAI. “LunarLander-v2.” OpenAI Gym. <a href="https://gym.openai.com/envs/LunarLander-v2/" rel="nofollow" target="_blank">링크</a></li>
<li>버클리 AI 연구소 (BAIR). “Experience Replay.” <a href="https://bair.berkeley.edu/blog/2020/03/20/experiencereplay/" rel="nofollow" target="_blank">링크</a></li>
<li>Towards Data Science. “Reinforcement Learning 101: Q-Learning.” <a href="https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292" rel="nofollow" target="_blank">링크</a></li>
<li>Towards Data Science. “신경망 뒤의 수학.” <a href="https://towardsdatascience.com/the-math-behind-neural-networks-3a18b7f8d8dc" rel="nofollow" target="_blank">링크</a></li>
<li>Towards Data Science. “Adam Optimizer 뒤의 수학.” <a href="https://towardsdatascience.com/the-math-behind-adam-optimizer-3a18b7f8d8dc" rel="nofollow" target="_blank">링크</a></li>
<li>Towards Data Science. “Deep Neural Networks 맞춤화 뒤의 수학.” <a href="https://towardsdatascience.com/the-math-behind-fine-tuning-deep-neural-networks-3a18b7f8d8dc" rel="nofollow" target="_blank">링크</a></li>
</ul>
<p>마지막까지 읽어 주셔서 축하드립니다! 이 기사가 유익하고 즐거우셨기를 바랍니다. 만약 그렇다면, 박수를 남기고 더 많은 이런 기사를 보고 싶다면 저를 팔로우해 주세요. 앞으로 다루었으면 하는 주제나 기사에 대한 의견을 들을 수 있습니다. 피드백과 지원에 감사드립니다. 읽어 주셔서 감사합니다!</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"강화 학습 딥 Q-네트워크","description":"","date":"2024-05-27 14:10","slug":"2024-05-27-ReinforcementLearningDeepQ-Networks","content":"\n## Python을 사용하여 달에 착륙하는 셔틀 가르치기: Deep Q-Networks를 활용한 강화 학습의 수학적 탐구\n\n![Reinforcement Learning](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_0.png)\n\n강화 학습(RL)에서 Q-학습은 에이전트가 환경을 탐색하면서 누적 보상을 극대화하기 위한 정책을 학습하는 데 도움이 되는 기본 알고리즘입니다. 이를 통해 특정 상태에서 특정 작업을 수행했을 때 기대되는 유틸리티를 추정하는 작업-값 함수를 업데이트 함으로써 보상을 받고 미래 추정에 기반합니다 (이게 익숙하지 않으신가요? 걱정 마세요. 나중에 함께 자세히 살펴볼 겁니다).\n\n그러나 전통적인 Q-학습에는 도전 과제가 있습니다. 상태 공간이 확장됨에 따라 확장 가능성에 어려움을 겪으며 연속적인 상태 및 작업 공간을 갖는 환경에서 효과적이지 않습니다. 이때 Deep Q Networks (DQNs)가 나타납니다. DQNs는 Q-값을 근사하기 위해 신경망을 사용하여 에이전트가 보다 크고 복잡한 환경을 효과적으로 처리할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n본 기사에서는 Deep Q Networks에 대해 자세히 살펴보겠습니다. DQNs가 기존의 Q-learning의 한계를 극복하는 방법과 DQN을 구성하는 주요 구성 요소에 대해 탐구할 것입니다. 또한 처음부터 DQN을 구현하고 더 복잡한 환경에 적용하는 과정을 살펴볼 것입니다. 이 기사를 마치면 DQN이 어떻게 작동하는지 이해하고 도전적인 강화 학습 문제를 해결하는 데 사용하는 방법을 알게 될 것입니다.\n\n## 목차\n\n1: 전통적인 Q-러닝\n∘ 1.1: 상태와 행동\n∘ 1.2: Q-값\n∘ 1.3: Q-테이블\n∘ 1.4: 학습 과정\n\n2: Q-러닝에서 Deep Q-네트워크로\n∘ 2.1: 전통적인 Q-러닝의 한계\n∘ 2.2: 신경망\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3: Deep Q-Network의 해부학\n\n- 3.1: DQN의 구성요소\n- 3.2: DQN 알고리즘\n\n4: 처음부터 Deep Q-Network 구현하기\n\n- 4.1: 환경 설정\n- 4.2: 딥 신경망 구축\n- 4.3: 경험 재생 구현\n- 4.4: 타깃 네트워크 구현\n- 4.5: Deep Q-Network 훈련\n- 4.6: 모델 튜닝\n- 4.7: 모델 실행\n\n5: 결론\n참고 문헌\n\n# 1: 전통적인 Q-Learning\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Reinforcement Learning Deep Q Networks](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_1.png)\n\nQ-러닝은 환경에서 누적 보상을 극대화하기 위한 최적 조치를 학습하는 에이전트를 안내합니다. 딥 Q-네트워크에 집중하기 전에, 그 선구자인 Q-러닝 뒤에 있는 메커니즘을 간단히 검토하는 것이 좋습니다.\n\n## 1.1: 상태 및 조치\n\n미로를 탐색하는 로봇이라고 상상해보세요. 미로에서 차지하는 각 위치를 \"상태\"라고 합니다. 왼쪽, 오른쪽, 위 또는 아래로 이동하는 것과 같은 각각의 움직임을 \"조치\"라고 합니다. 목표는 결국 미로를 통해 최적 경로를 찾으려면 각 상태에서 어떤 조치를 취할지 결정하는 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 1.2: Q-Values\n\nQ-Learning의 핵심은 Q-값으로, 𝑄(𝑠, 𝑎)로 표시됩니다. 이 값은 특정 상태 s에서 특정 행동 a를 취한 후 더 나은 경로(정책)를 따를 때 기대되는 미래 보상을 나타냅니다.\n\nQ-값을 가이드북의 항목으로 생각해보세요. 각 가능한 이동의 장기적 이점을 평가하는 것입니다. 예를 들어 미로의 특정 위치에 있다고 가정했을 때 왼쪽으로 이동하는 경우, Q-값은 미래 보상 측면에서 그 이동이 얼마나 유익할지 알려줍니다. 더 높은 Q-값은 더 나은 이동을 나타냅니다.\n\n## 1.3: The Q-Table\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nQ-Learning은 Q-값을 추적하는 데 Q-테이블을 사용합니다. Q-테이블은 기본적으로 각 행이 상태에 해당하고 각 열이 행동에 해당하며 각 셀이 해당 상태-행동 쌍의 Q-값을 포함하는 대형 스프레드시트입니다.\n\nQ-테이블을 거대한 스프레드시트로 상상해보세요. 각 셀은 미로의 특정 위치에서 특정 이동을 하였을 때 잠재적 미래 보상을 나타냅니다. 환경에 대해 더 많이 배우면이 보상의 더 나은 추정치로이 스프레드시트를 업데이트합니다.\n\n## 1.4: 학습 과정\n\nQ-러닝의 학습 과정은 반복적입니다. 초기 상태 s에서 시작합니다. 그런 다음 작업 a를 결정합니다. 이 선택은 다음을 기반으로 할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 탐험: 효과를 발견하기 위해 새로운 조치를 시도합니다.\n- 개척: 가장 높은 알려진 Q-값을 갖는 조치를 선택하기 위해 기존 지식을 사용합니다.\n\n선택한 조치를 수행하고 보상 r을 관찰하며 다음 상태 s'로 이동합니다. Q-러닝 공식을 사용하여 상태-조치 쌍 (s, a)의 Q-값을 업데이트합니다:\n\n[이미지](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_2.png)\n\n여기에:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- α는 학습 속도로, 새로운 정보가 이전 정보를 얼마나 덮어쓸지를 결정합니다.\n- γ는 할인 요소로, 즉각적 보상을 먼 미래의 보상보다 더 가치 있게 여깁니다.\n- maxa′Q(s′,a′)는 다음 상태 s′에서 가능한 모든 행동 a′에 대해 최고의 Q값을 나타냅니다.\n\n매번 안내서를 업데이트하고 있다고 상상해 보세요. 각 이동 후에 성공적인지 실패인지에 대한 피드백(보상)을 받습니다. 그런 다음 새로운 정보를 반영하도록 가이드북의 등급(Q값)을 조정하여 미래의 결정을 더 잘 하게 됩니다.\n\nQ값이 수렴할 때까지 이 과정을 반복하면, 에이전트는 미로를 탐색하는 최적 정책을 학습한 것입니다. 시간이 흘러, 미로를 반복적으로 탐험하고 경험에 기반하여 가이드북을 업데이트함으로써 최상의 보상을 얻기 위한 최적의 움직임을 알려주는 포괄적인 전략을 개발하게 됩니다.\n\nQ-러닝에 대해 자세히 알아보려면 이 기사를 확인해 보세요: [링크](링크)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2: Q-Learning에서 Deep Q-Network로\n\n## 2.1: 전통적인 Q-Learning의 한계\n\nQ-Learning은 강화 학습에 대한 강력한 알고리즘이지만, 더 복잡한 환경에서 효과적으로 동작하는 데 제약 사항이 몇 가지 있습니다:\n\n확장성 문제: 전통적인 Q-Learning은 각 상태-행동 쌍이 Q-값에 매핑된 Q-테이블을 유지합니다. 상태 공간이 성장함에 따라, 특히 고차원 또는 연속적인 환경에서는 Q-테이블이 불필요하게 커져 메모리 비효율성과 학습 속도 저하를 초래합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이산 상태 및 행동 공간: Q-Learning은 상태와 행동이 이산적이고 유한한 환경에서 잘 동작합니다. 하지만 현실 세계의 많은 문제는 연속적인 상태와 행동 공간을 포함하고 있습니다. 이러한 전통적인 Q-Learning은 이러한 공간을 이산화하지 않고는 효율적으로 처리할 수 없으며, 이로 인해 정보 손실과 최적 정책의 하락을 초래할 수 있습니다.\n\n## 2.2: 신경망\n\n이제 신경망을 소개해 보겠습니다. 신경망은 딥 네트워크에서 중요한 역할을 하는데, 인간 두뇌의 구조와 기능을 모방하여 데이터로부터 복잡한 패턴을 학습할 수 있는 강력한 함수 근사기입니다. 신경망은 입력 데이터를 처리하고 가중치와 편향을 통해 변환하여 출력을 생성하는 연결된 노드(뉴런)의 계층으로 이루어져 있습니다.\n\n강화 학습의 맥락에서 신경망은 Q-함수를 근사화하는 데 사용될 수 있습니다. 이는 상태-행동 쌍을 Q-값에 매핑하는 데 도움이 되며, 특히 Q-테이블을 유지하는 것이 적절하지 않은 대규모나 연속적인 공간에서 상태와 행동 간에 일반화를 더 잘할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서, Deep Q-networks(DQNs)은 Q-Learning의 원리를 신경망의 함수 근사 능력과 결합시켜요. 그렇게 하면 전통적인 Q-learning의 주요 제약 사항을 다룰 수 있어요.\n\nDQNs은 Q-값을 테이블에 저장하는 대신 신경망을 사용하여 Q-함수를 근사해요. 이 네트워크는 상태를 입력으로 받아 가능한 모든 행동에 대한 Q-값을 출력해요. 환경에서의 경험으로 네트워크를 학습시켜 에이전트는 각 행동에 대한 예상 보상을 예측하도록합니다. 이를 통해 다양한 상태와 행동에 걸쳐 일반화할 수 있어요.\n\n체스를 배우는 것을 상상해보세요. 가능한 모든 체스판 구성과 각 동작에 대한 최상의 수를 외우는 대신(불가능한 일이죠), 전략과 원칙(예를 들어 보드 중앙을 제어하고 왕을 보호하는 것과 같은 것)을 배우게 됩니다. 비슷하게, DQN는 신경망을 통해 일반적인 패턴과 전략을 배우고 모든 가능한 상태를 외우지 않고도 정보를 바탕으로 결정할 수 있어요.\n\n신경망 사용은 DQN이 크거나 연속된 상태 공간을 다룰 수 있게 해요. 네트워크는 주요 특징을 잡아내는 상태 공간의 표현을 학습해 중요한 결정을 취할 수 있도록 해줍니다.\n\n큰 도시를 이동하려면 고려합시다. 모든 거리와 건물의 배치를 외우는 대신 표지판과 중요 도로를 인식해 길을 찾게 됩니다. DQN의 신경망도 비슷하게 작용하며, 에이전트가 복잡한 환경에서 이동하는 것을 돕는 상태 공간의 중요한 특징을 인식하도록 학습합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다양한 경험을 훈련함으로써 모델은 과거 경험에서 일반화하는 법을 배우게 됩니다. 즉, 에이전트는 배운 것을 새로운, 보지 못한 상태와 행동에 적용할 수 있어서 다양한 상황에서 더 적응력이 있고 효율적일 수 있습니다.\n\n# 3: 딥 Q-네트워크의 구성 요소\n\n## 3.1: DQN의 구성 요소\n\n딥 Q-네트워크 (DQN)가 어떻게 작동하는지 이해하려면 그 주요 구성 요소를 자세히 살펴보는 것이 중요합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3.1.1: 신경망\n\n![신경망](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_3.png)\n\nDQN의 핵심은 Q값을 위한 함수 근사기 역할을 하는 신경망입니다. 아키텍처는 일반적으로 다음과 같이 구성됩니다:\n\n입력 레이어: 에이전트의 \"눈\"으로 상상해보세요. 이 레이어는 환경으로부터 상태 표현을 받아들이는데, 마치 당신의 눈이 주변의 시각적 정보를 받아들이는 것과 유사합니다. 위의 이미지에서 왼쪽에 두 개의 노드가 있는 첫 번째 레이어입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nHidden Layers: 이러한 레이어들은 에이전트의 \"뇌\"로 생각할 수 있습니다. 눈을 통해 받은 정보를 다수의 사고 단계를 거쳐 처리하여 복잡한 특징과 패턴을 식별합니다. 마치 당신의 뇌가 세계를 처리하고 이해하는 방식과 비슷합니다. 위 이미지에서는 세 개의 노드가 있는 중간 레이어입니다.\n\nOutput Layer: 이는 에이전트의 \"의사 결정\" 부분과 같습니다. 입력 상태에 따라 모든 가능한 행동에 대한 Q 값(값함수)을 생성합니다. 당신이 보고 생각한 것에 기반하여 최선의 행동을 결정하는 방식과 유사합니다. 각 출력은 특정 행동을 취했을 때 기대되는 보상에 해당합니다. 위 이미지에서는 한 개의 노드를 가진 오른쪽의 마지막 레이어입니다.\n\n위 이미지는 간단한 피드포워드 신경망을 나타냅니다. 이는 신경망의 가장 기본적인 형태입니다. 이 구조는 기본적이지만 \"깊은\" 신경망은 아닙니다. 깊은 신경망으로 변환하기 위해서는 더 많은 은닉 레이어를 추가하여 신경망의 깊이를 증가시켜야 합니다. 또한, 다양한 아키텍처와 구성을 실험하여 더 발전된 모델을 개발할 수 있습니다. 각 레이어의 노드 수는 고정되지 않으며, 특정 훈련 데이터셋과 작업에 따라 다양합니다. 이러한 유연성을 통해 네트워크를 특정 목적에 더 잘 맞게 조정할 수 있습니다.\n\n신경망에 대해 더 알고 싶다면, 나는 아래의 글을 강력히 추천합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3.1.2: 경험 재생\n이제 목록의 다음 항목인 경험 재생으로 넘어가 봅시다. 이것은 DQNs에서 학습 과정을 안정화하고 향상시키는 기술입니다. 다음을 포함합니다:\n\n메모리 버퍼: 에이전트의 \"일기\"로 생각해보세요. 이것은 에이전트의 경험을 시간이 지남에 따라 저장합니다 (상태, 행동, 보상, 다음 상태, 완료), 마치 매일 당신이 무슨 일이 일어났는지 기록하는 것처럼.\n\n랜덤 샘플링: 훈련 중에 에이전트는 지난 경험을 배우기 위해 일기의 랜덤한 페이지를 넘깁니다. 이는 사건의 순서를 깨어주어 에이전트가 경험의 순서에 과적합되는 것을 방지하여 보다 견고하게 학습하도록 돕습니다.\n\n3.1.3: 타겟 네트워크\n마지막으로, 타겟 네트워크는 훈련을 위해 타겟 Q-값을 계산하는 데 사용되는 별도의 신경망입니다. 주 신경망과 동일한 구조를 가지고 있지만 주 신경망의 가중치가 정기적으로 업데이트되어 일치하도록 고정되어 있습니다. 에이전트를 위한 \"안정된 안내서\"로 생각해보세요. 주 신경망이 지속적으로 학습하고 업데이트되는 반면, 타겟 네트워크는 안정된 Q-값을 제공하여 훈련에 도움을 줍니다. 학습을 안정적이고 일관되게 유지하는 데 도움이 되는 신뢰할 수 있는, 주기적으로 업데이트되는 매뉴얼이 있는 것과 같습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 3.2: DQN 알고리즘\n\n이러한 구성 요소가 준비되면 DQN 알고리즘은 다음과 같은 몇 가지 중요한 단계로 개요를 제시할 수 있습니다:\n\n### 3.2.1: 순방향 전파\n\n먼저, 우리는 Q-values를 예측하는 데 중요한 순방향 전파로 시작합니다. 이러한 Q-values는 특정 상태에서 특정 행동을 취했을 때 기대되는 미래 보상을 저장합니다. 이 프로세스는 상태 입력부터 시작됩니다.\n\n#### 상태 입력\n\n에이전트는 환경에서 현재 상태 s를 관찰합니다. 이 상태는 에이전트의 현재 상황을 설명하는 특징 벡터로 표현됩니다. 상태를 에이전트 주변 세계의 스냅숏으로 생각해보세요. 눈이 주변을 둘러보는 것처럼 시각 장면을 촬영할 때와 유사합니다. 이 스냅숏에는 에이전트가 결정을 내리기 위해 필요한 모든 세부 정보가 포함되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nQ-Value Prediction\n이제 이 관측된 상태 s가 신경망으로 전달됩니다. 이 신경망은 여러 층을 통해 이 입력을 처리하고 Q-값 세트를 출력합니다. 각 Q-값은 가능한 작업 a에 해당하며, 매개 변수 θ는 네트워크의 가중치와 편향을 나타냅니다.\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_4.png)\n\n신경망을 에이전트 뇌의 복잡한 의사 결정 기계로 상상해보세요. 캡처(상태)를 받으면 이 정보를 여러 단계(층)를 통해 처리하여 다양한 작업에 대한 잠재적 결과(Q-값)를 찾습니다. 보이는 것을 바탕으로 취할 수 있는 다양한 작업을 고려해보는 것과 유사합니다.\n\n작업 선택\n그런 다음 에이전트는 가장 높은 Q-값을 가진 작업 a∗를 다음 움직임으로 선택하며, 이에 따라 탐욕적 작업 선택 정책을 따릅니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_5.png)\n\n이것은 모든 옵션을 심사한 후에 최선의 움직임을 결정하는 것과 유사합니다. 에이전트는 가장 높은 보상을 가져다줄 것으로 믿는 행동을 선택하며, 마치 당신이 보고 이해한 것을 기반으로 가장 유망한 길을 선택하는 것과 같습니다.\n\n3.2.2: 경험 재생\n다음으로, 우리는 학습 과정을 안정화하고 향상시키는 데 도움이 되는 경험 재생에 대해 이야기하겠습니다.\n\n경험 저장\n에이전트가 행동 a를 취하고 보상 r을 받은 후 새로운 상태 s′를 받으면, 이 경험을 (s, a, r, s′, done) 튜플로 저장하여 플레이백 버퍼에 저장합니다. 변수 done은 에피소드가 종료되었는지를 나타냅니다. 플레이백 버퍼를 에이전트가 경험을 기록하는 다이어리로 생각해보세요. 이는 당신이 하루 중 주목할 만한 사건을 메모하는 것과 유사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n샘플 미니배치\n훈련 중에는 경험의 미니배치가 임의로 선택되어 재생 버퍼에서 샘플링됩니다. 이 배치는 타겟 Q-값을 계산하고 손실을 최소화하여 네트워크를 업데이트하는 데 사용됩니다. 에이전트가 훈련할 때, 과거 경험을 학습하기 위해 일기장의 임의의 페이지를 넘겨보게 됩니다. 이 임의 샘플링은 사건의 순서를 깨고 다양한 학습 예제를 제공하며, 일기의 서로 다른 날짜를 검토하여 보다 넓은 시야를 얻는 것과 유사한 역할을 합니다.\n\n3.2.3: 역전파\n최종 단계는 역전파로, 이는 네트워크를 업데이트하여 예측을 개선합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n타겟 Q-값 계산\n미니 배치의 각 경험에 대해, 에이전트는 타겟 Q-값 y\\_를 계산합니다. 만약 다음 상태 s′가 종료 상태(즉, done이 true인 경우)라면, 타겟 Q-값은 간단히 보상 r입니다. 그렇지 않으면, 타겟 Q-값은 보상에 다음 상태 s′에서 타겟 네트워크 Qtarget에 의해 예측된 할인된 최대 Q-값을 더한 값입니다:\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_7.png)\n\n여기서 γ는 할인 계수(0 ≤ γ ≤ 1)입니다. 이 단계는 과거 경험에 기반해 미래를 계획하는 것과 같습니다. 경험이 여행(에피소드)을 끝낼 경우, 타겟은 받은 보상입니다. 계속된다면, 타겟에는 즉시와 미래 혜택을 모두 고려하여 행동을 계획하는 방식과 유사한 예상 미래 보상이 포함됩니다.\n\n손실 계산\n다음으로, 손실은 메인 네트워크에서 예측된 Q-값 Q(s_i, a_i; θ)과 타겟 Q-값 yi 사이의 평균 제곱 오차로 계산됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_8.png)\n\n손실을 계산하는 것은 예측이 실제 결과와 얼마나 차이가 나는지를 평가하는 것과 같습니다. 실제 결과와 비교하여 추측의 정확성을 확인하고 차이점을 주목하는 것과 같습니다.\n\n역전파 및 최적화\n마지막으로, 이 손실을 최소화하기 위해 역전파를 수행합니다. 계산된 손실은 네트워크를 통해 역전파되어 SGD(Stochastic Gradient Descent) 또는 Adam과 같은 최적화 알고리즘을 사용하여 가중치를 업데이트합니다. 이 프로세스는 손실을 최소화하기 위해 네트워크 매개변수 θ를 조정합니다:\n\n![image](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_9.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 α는 학습률을 나타내고, ∇θLoss는 네트워크 매개변수에 대한 손실의 그래디언트를 나타냅니다. 역전파는 실수로부터 배우는 것과 같습니다. 예측이 얼마나 잘못되었는지를 깨달았을 때 (손실), 전략(네트워크 가중치)을 조정하여 미래의 결정을 개선합니다. 피드백을 바탕으로 자신의 접근 방식을 미세 조정하여 다음에 더 나은 결과를 얻는 것과 비슷합니다.\n\n이 아키텍처를 사용하여 에이전트는 정책을 반복적으로 개선합니다. 시간이 지남에 따라 누적 보상을 극대화하는 조치를 취하는 것을 배웁니다. 신경망, 경험 재생 및 타겟 네트워크의 결합으로 DQN은 복잡한 고차원 환경에서 효과적으로 학습할 수 있습니다. 이 과정은 에이전트가 환경을 탐색하는 데 능숙해질 때까지 계속됩니다.\n\n# 4: 처음부터 Deep Q-Network 구현\n\n이 섹션에서는 처음부터 Deep Q-Network (DQN)의 구현을 안내합니다. 이 섹션의 끝에는 Python에서 DQN을 구축하고 훈련하는 방법을 명확히 이해하게 될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 OpenAI Gym의 Lunar Lander 환경을 사용할 것입니다. 이 환경에서의 목표는 달 착륙선을 조종하여 지정된 착륙 패드에 성공적으로 착륙하는 것입니다. 착륙선은 환경을 통해 비행할 때 추진기를 사용하여 움직임과 방향을 조절해야 합니다. 이 환경은 상업적으로 사용할 수 있습니다. 라이센스 및 사용 권한에 대한 자세한 내용은 OpenAI Gym GitHub 페이지에서 확인할 수 있습니다.\n\n오늘 다룰 모든 코드는 여기에서 찾을 수 있습니다:\n\n## 4.1: 환경 설정\n\n우리는 OpenAI Gym의 LunarLander 환경을 사용할 것이며, 이는 우리의 에이전트가 해결해야 할 어려운 문제를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport os\nimport pickle\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport random\nimport optuna\n```\n\n여기서 필요한 라이브러리들을 import 합니다. gym은 환경을 위해 사용되며, torch는 우리의 신경망을 구축하고 훈련하는 데 사용되며, collections, random, 및 optuna는 경험 재생과 하이퍼파라미터 최적화에 도움이 됩니다.\n\n```js\nenv = gym.make(\"LunarLander-v2\", (render_mode = \"rgb_array\"));\nstate_dim = env.observation_space.shape[0];\naction_dim = env.action_space.n;\n```\n\n우리는 LunarLander 환경을 초기화하고 상태 및 액션 공간의 차원을 가져옵니다. state_dim은 상태의 특징 수를 나타내고, action_dim은 가능한 액션 수를 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4.2: 딥 신경망 구축\n\n우리의 딥-NN에서는 DQN이라는 클래스를 생성할 것입니다. 이 클래스는 세 개의 완전 연결 계층을 가진 신경망을 정의합니다. 입력 계층은 상태 표현을 수신하며, 은닉 계층은 이 정보를 선형 변환과 ReLU 활성화 함수를 통해 처리하고, 출력 계층은 각 가능한 동작에 대한 Q-값을 생성합니다.\n\n먼저 코드를 확인한 다음 분석해 봅시다:\n\n```js\nclass DQN(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, action_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.2.1: 클래스 초기화\n\n```python\nclass DQN(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, action_dim)\n```\n\n우리는 DQN이라는 클래스를 정의했습니다. 이 클래스는 PyTorch의 모든 신경망 모듈에 사용되는 기본 클래스인 nn.Module을 상속받습니다. 이를 통해 우리는 PyTorch의 내장 함수와 기능을 활용할 수 있습니다.\n\n**init** 메서드는 객체의 속성을 초기화하는 특별한 메서드입니다. 우리의 경우에는 신경망의 레이어를 설정하게 됩니다. 완전 연결층 (Fully Connected Layers):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 세 개의 완전 연결 (선형) 레이어를 정의합니다:\n\n- self.fc1 = nn.Linear(state_dim, 128): 첫 번째 레이어는 입력 상태 차원 (상태의 피쳐 수)을 받아서 128개의 뉴런으로 매핑합니다.\n- self.fc2 = nn.Linear(128, 128): 두 번째 레이어는 첫 번째 레이어에서 나온 128개의 뉴런을 또 다른 128개의 뉴런으로 매핑합니다.\n- self.fc3 = nn.Linear(128, action_dim): 세 번째 레이어는 두 번째 레이어에서 나온 128개의 뉴런을 액션 차원 (가능한 액션 수)으로 매핑합니다.\n\n각 nn.Linear 레이어는 입력 데이터에 대해 선형 변환을 수행합니다:\n\n![이미지](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.2.2: Forward Method\n앞서 명시된 forward 메소드는 데이터가 네트워크를 통해 흐르는 방법을 정의합니다. 이 방법은 네트워크를 통해 데이터를 전달할 때 자동으로 호출됩니다.\n\n```python\ndef forward(self, x):\n    x = torch.relu(self.fc1(x))\n    x = torch.relu(self.fc2(x))\n    return self.fc3(x)\n```\n\n첫 번째 layer에서 입력 데이터 x는 첫 번째 fully connected layer (self.fc1)를 통해 전달됩니다. 그런 다음 출력은 ReLU (Rectified Linear Unit) 활성화 함수를 사용하여 변환됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nx = torch.relu(self.fc1(x));\n```\n\nReLU(Recified Linear Unit) 활성화 함수는 다음과 같이 정의됩니다:\n\n![ReLU activation function](/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_11.png)\n\n모델에 비선형성을 도입하여 네트워크가 더 복잡한 기능을 학습할 수 있게합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 번째 레이어에서는 첫 번째 레이어의 출력이 두 번째 완전 연결 레이어 (self.fc2)를 통과하고 다시 ReLU 활성화 함수를 사용하여 변환됩니다:\n\n```js\nx = torch.relu(self.fc2(x));\n```\n\n마지막으로 출력 레이어에서는 두 번째 레이어의 출력이 활성화 함수 없이 세 번째 완전 연결 레이어 (self.fc3)를 통해 전달됩니다:\n\n```js\nreturn self.fc3(x);\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 레이어는 각 액션에 대한 최종 Q-값을 생성합니다. 각 값은 해당 상태에서 그 액션을 취했을 때의 예상 미래 보상을 나타냅니다.\n\n## 4.3: 경험 재생 구현\n\nReplayBuffer 클래스는 경험을 저장하고 샘플링하는 메커니즘을 제공하여 DQN에서 학습 과정을 안정화하고 개선하는 데 필수적입니다. 따라서 에이전트가 다양한 과거 경험 세트로부터 학습할 수 있도록 해주어 일반화하고 복잡한 환경에서 잘 수행할 수 있는 능력을 향상시킵니다.\n\n```js\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n\n    def push(self, state, action, reward, next_state, done):\n        self.buffer.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n        return state, action, reward, next_state, done\n\n    def __len__(self):\n        return len(self.buffer)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.3.1: 클래스 초기화\n\n```js\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n```\n\n**init** 메서드는 고정된 용량을 갖는 deque(덱, 이중 연결 리스트)를 초기화합니다. 덱은 양쪽 끝에서 효율적으로 항목을 추가하고 제거할 수 있는 자료 구조입니다. 빠른 양쪽 끝에서의 추가와 제거가 필요한 큐(queue)나 스택(stack)을 구현할 때 유용합니다.\n\nself.buffer = deque(maxlen=capacity)는 capacity만큼의 경험을 저장할 수 있는 deque를 생성합니다. 버퍼가 가득 차면 새로운 경험을 추가하면 가장 오래된 경험이 자동으로 제거됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.3.2: Push Method\n\n```python\ndef push(self, state, action, reward, next_state, done):\n    self.buffer.append((state, action, reward, next_state, done))\n```\n\n푸시 메서드는 버퍼에 새로운 경험을 추가합니다. 각 경험은 상태(state), 액션(action), 보상(reward), 다음 상태(next_state), 완료 여부(done)로 구성된 튜플입니다:\n\n- state: 현재 상태.\n- action: 에이전트가 취한 행동.\n- reward: 행동을 취한 후 받은 보상.\n- next_state: 행동을 취한 후 에이전트가 이동한 상태.\n- done: 에피소드가 종료되었는지를 나타내는 부울 값.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.3.3: 샘플 메서드\n\n```python\ndef sample(self, batch_size):\n    state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n    return state, action, reward, next_state, done\n```\n\n샘플 메서드는 버퍼에서 무작위로 일괄 경험을 검색합니다.\n\nrandom.sample(self.buffer, batch_size)는 버퍼에서 batch_size개의 경험을 무작위로 선택합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"zip(\\*random.sample(self.buffer, batch_size))\"은 경험 목록을 상태, 행동, 보상, 다음 상태 및 완료에 대한 별도의 튜플로 풀어낼 수 있습니다.\n\n이 메서드는 샘플된 경험들로 이러한 튜플을 반환합니다.\n\n4.3.4: Length Method\n\n```python\ndef __len__(self):\n    return len(self.buffer)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**len** 메서드는 버퍼에 저장된 현재 경험 수를 반환합니다.\n\n## 4.4: 타겟 네트워크 구현\n\n타겟 네트워크를 통해 안정적인 Q 값 세트를 제공하여 훈련을 위해 학습 프로세스를 안정화하고 복잡한 환경에서 에이전트의 성능을 향상시킵니다. 타겟 네트워크는 주 네트워크보다 덜 자주 업데이트되어 메인 네트워크의 가중치를 업데이트하는 데 사용되는 Q 값 추정치가 더 안정적임을 보장합니다.\n\nDQNTrainer라는 클래스 내에 타겟 네트워크를 구현할 것이며, 이 클래스는 DQN의 훈련 프로세스를 관리하고 주 및 타겟 네트워크, 옵티마이저 및 재생 버퍼를 포함합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nclass DQNTrainer:\n    def __init__(self, env, main_network, target_network, optimizer, replay_buffer, model_path='model/model.pth', gamma=0.99, batch_size=64, target_update_frequency=1000):\n        self.env = env\n        self.main_network = main_network\n        self.target_network = target_network\n        self.optimizer = optimizer\n        self.replay_buffer = replay_buffer\n        self.model_path = model_path\n        self.gamma = gamma\n        self.batch_size = batch_size\n        self.target_update_frequency = target_update_frequency\n        self.step_count = 0\n```\n\n- DQNTrainer 클래스 정의:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**init** 메서드는 학습에 필요한 다양한 구성 요소를 초기화합니다:\n\n- env: 에이전트가 작동하는 환경입니다.\n- main_network: 훈련 중인 주요 신경망입니다.\n- target_network: Q-값 추정을 안정화하는 데 사용되는 대상 신경망입니다.\n- optimizer: 주요 신경망의 가중치를 업데이트하는 데 사용되는 옵티마이저입니다.\n- replay_buffer: 경험을 저장하고 샘플링하는 버퍼입니다.\n- model_path: 훈련된 모델을 저장하고 로드하기 위한 경로입니다.\n- gamma: 미래 보상에 대한 할인 계수입니다.\n- batch_size: 각 훈련 단계에서 재생 버퍼에서 샘플링된 경험의 수입니다.\n- target_update_frequency: 대상 네트워크의 가중치를 주요 네트워크의 가중치에 맞게 업데이트하는 빈도입니다.\n- step_count: 훈련 중에 취한 단계 수를 추적하는 카운터입니다.\n\n  4.4.2: 모델 로딩\n\n```js\n# 모델이 있으면 로드\n        if os.path.exists(os.path.dirname(self.model_path)):\n            if os.path.isfile(self.model_path):\n                self.main_network.load_state_dict(torch.load(self.model_path))\n                self.target_network.load_state_dict(torch.load(self.model_path))\n                print(\"디스크에서 모델 로드됨\")\n        else:\n            os.makedirs(os.path.dirname(self.model_path))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 모델 경로의 디렉토리가 존재하는지 os.path.exists(os.path.dirname(self.model_path))를 사용하여 확인합니다. 저장된 모델이 있으면, 훈련을 멈춘 지점부터 계속하기 위해 불러옵니다:\n\n```js\nif os.path.isfile(self.model_path):\n    self.main_network.load_state_dict(torch.load(self.model_path))\n    self.target_network.load_state_dict(torch.load(self.model_path))\n    print(\"디스크에서 모델을 불러왔습니다\")\n```\n\ntorch.load는 load_state_dict를 사용하여 저장된 모델 가중치를 메인 및 타겟 네트워크에 불러옵니다. 모델 디렉토리가 존재하지 않는 경우, os.makedirs를 사용하여 만듭니다.\n\n## 4.5: 딥 Q-네트워크 훈련\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로, 우리는 DQN을 훈련하기 위한 학습 루프를 구현할 것입니다. 이 방법은 DQNTrainer 내에 이루어집니다. DQN을 위한 훈련 루프를 실행하며, 에이전트가 환경과 상호 작용하고 경험을 수집하며 네트워크를 업데이트하고 성능을 추적합니다.\n\n다음은 학습 루프의 코드입니다:\n\n```js\ndef train(self, num_episodes, save=True):\n    total_rewards = []\n    for episode in range(num_episodes):\n        state, _ = self.env.reset()\n        done = False\n        total_reward = 0\n\n        while not done:\n            self.env.render()  # 환경을 렌더링하기 위해 이 줄을 추가합니다\n            action = self.main_network(torch.FloatTensor(state).unsqueeze(0)).argmax(dim=1).item()\n            next_state, reward, done, _, _ = self.env.step(action)\n            self.replay_buffer.push(state, action, reward, next_state, done)\n            state = next_state\n            total_reward += reward\n\n            if len(self.replay_buffer) \u003e= self.batch_size:\n                self.update_network()\n\n        total_rewards.append(total_reward)\n        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n\n    if save:\n        torch.save(self.main_network.state_dict(), self.model_path)\n        print(\"모델을 디스크에 저장했습니다\")\n\n    self.env.close()\n    return sum(total_rewards) / len(total_rewards)\n```\n\ntrain 메서드는 지정된 에피소드 수에 대해 훈련 루프를 실행합니다. 이 루프는 에이전트가 경험을 쌓고 의사 결정 능력을 향상시키는 데 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.5.1: 훈련 루프\n우선 total_rewards를 빈 리스트로 초기화해 봅시다:\n\n```js\ntotal_rewards = [];\n```\n\n이제 훈련 루프를 만들어 봅시다:\n\n```js\nfor episode in range(num_episodes):\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 루프는 지정된 에피소드 수만큼 실행됩니다. 각 에피소드는 환경과의 완전한 상호작용 순서를 나타냅니다.\n\n4.5.2: 환경 재설정\n각 에피소드의 시작 시점에는 환경이 초기 상태로 재설정됩니다.\n\n```js\nstate, (_ = self.env.reset());\ndone = False;\ntotal_reward = 0;\n```\n\n- self.env.reset()은 환경을 초기화하고 초기 상태를 반환합니다.\n- done = False는 에피소드가 완료되지 않았음을 나타냅니다.\n- total_reward = 0은 현재 에피소드의 총 보상을 초기화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n### 4.4.3: Action Selection\n\n에이전트는 현재 상태를 기반으로 메인 네트워크를 사용하여 작업을 선택합니다.\n\n```python\naction = self.main_network(torch.FloatTensor(state).unsqueeze(0)).argmax(dim=1).item()\n```\n\ntorch.FloatTensor(state).unsqueeze(0)은 상태를 PyTorch 텐서로 변환하고 네트워크가 예상하는 입력 형태와 일치하도록 추가 차원을 추가합니다.\n\nself.main_network(...).argmax(dim=1).item()는 메인 네트워크가 예측한 가장 높은 Q 값으로 작업을 선택합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.5.4: 단계 및 저장 환경\n에이전트가 선택한 동작을 수행하고 보상 및 다음 상태를 관찰한 후, 해당 경험을 재생 버퍼에 저장합니다.\n\n```js\nnext_state, reward, done, _, (_ = self.env.step(action));\nself.replay_buffer.push(state, action, reward, next_state, done);\nstate = next_state;\ntotal_reward += reward;\n```\n\n- self.env.step(action)은 동작을 실행하고 다음 상태, 보상 및 에피소드 완료 여부를 반환합니다.\n- self.replay_buffer.push(...)는 재생 버퍼에 경험을 저장합니다.\n- state = next_state는 현재 상태를 다음 상태로 업데이트합니다.\n- total_reward += reward은 현재 에피소드의 보상을 누적합니다.\n\n  4.5.5: 네트워크 업데이트\n  재생 버퍼에 충분한 경험이 있을 경우, 네트워크가 업데이트됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nif len(self.replay_buffer) \u003e= self.batch_size:\n    self.update_network()\n```\n\n`if len(self.replay_buffer) \u003e= self.batch_size`은 replay buffer가 적어도 batch_size의 경험을 가지고 있는지 확인합니다.\n\nself.update_network()은 replay buffer에서 일괄적인 경험을 사용하여 네트워크를 업데이트합니다.\n\n4.5.6: 에피소드 종료\n총 보상은 각 에피소드의 끝에서 기록되고 출력됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ntotal_rewards.append(total_reward)\nprint(f\"에피소드 {episode}, 총 보상: {total_reward}\")\n```\n\ntotal_rewards.append(total_reward)는 현재 에피소드의 총 보상을 총 보상 목록에 추가합니다.\n\nprint(f\"에피소드 {episode}, 총 보상: {total_reward}\")은 에피소드 번호와 총 보상을 출력합니다.\n\n4.5.7: 모델 저장\n훈련 후, 모델은 디스크에 저장됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsave가 True이면:\n   torch.save(self.main_network.state_dict(), self.model_path)\n   print(\"모델이 디스크에 저장되었습니다.\")\n```\n\nif save:는 save 플래그가 True인지 확인합니다.\n\ntorch.save(self.main_network.state_dict(), self.model_path)는 메인 네트워크의 상태 딕셔너리를 지정된 파일 경로에 저장합니다.\n\n4.5.8: 평균 보상 반환\n마지막으로, 이 메서드는 환경을 닫고 모든 에피소드에 대한 평균 보상을 반환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nself.env.close();\nreturn sum(total_rewards) / len(total_rewards);\n```\n\nself.env.close()는 환경을 닫습니다.\n\nreturn sum(total_rewards) / len(total_rewards)는 평균 보상을 계산하고 반환합니다.\n\n## 4.6: 모델 튜닝\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내 훈련된 모델을 평가하고 튜닝하는 방법을 살펴보겠습니다. DQN의 성능을 향상시키기 위해 하이퍼파라미터를 최적화할 책임을 가질 Optimizer 클래스를 만들어보겠습니다.\n\n```js\nclass Optimizer:\n    def __init__(self, env, main_network, target_network, replay_buffer, model_path, params_path='params.pkl'):\n        self.env = env\n        self.main_network = main_network\n        self.target_network = target_network\n        self.replay_buffer = replay_buffer\n        self.model_path = model_path\n        self.params_path = params_path\n\n    def objective(self, trial, n_episodes=10):\n        lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n        gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n        target_update_frequency = trial.suggest_categorical('target_update_frequency', [500, 1000, 2000])\n\n        optimizer = optim.Adam(self.main_network.parameters(), lr=lr)\n        trainer = DQNTrainer(self.env, self.main_network, self.target_network, optimizer, self.replay_buffer, self.model_path, gamma=gamma, batch_size=batch_size, target_update_frequency=target_update_frequency)\n        reward = trainer.train(n_episodes, save=False)\n        return reward\n\n    def optimize(self, n_trials=100, save_params=True):\n        if not TRAIN and os.path.isfile(self.params_path):\n            with open(self.params_path, 'rb') as f:\n                best_params = pickle.load(f)\n            print(\"디스크에서 매개변수를 불러왔습니다\")\n        elif not FINETUNE:\n            best_params = {\n                'lr': LEARNING_RATE,\n                'gamma': GAMMA,\n                'batch_size': BATCH_SIZE,\n                'target_update_frequency': TARGET_UPDATE_FREQUENCY\n                }\n            print(f\"기본 매개변수 사용 중: {best_params}\")\n        else:\n            print(\"하이퍼파라미터 최적화 중\")\n            study = optuna.create_study(direction='maximize')\n            study.optimize(self.objective, n_trials=n_trials)\n            best_params = study.best_params\n\n            if save_params:\n                with open(self.params_path, 'wb') as f:\n                    pickle.dump(best_params, f)\n                print(\"매개변수를 디스크에 저장했습니다\")\n\n        return best_params\n```\n\n4.6.1: 클래스 정의\n\n```js\nclass Optimizer:\n    def __init__(self, env, main_network, target_network, replay_buffer, model_path, params_path='params.pkl'):\n        self.env = env\n        self.main_network = main_network\n        self.target_network = target_network\n        self.replay_buffer = replay_buffer\n        self.model_path = model_path\n        self.params_path = params_path\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**init** 메서드는 최적화에 필요한 다양한 구성 요소를 초기화합니다:\n\n- env: 에이전트가 작동하는 환경.\n- main_network: 주요 신경망.\n- target_network: 타겟 신경망.\n- replay_buffer: 경험을 저장하고 샘플링하는 버퍼.\n- model_path: 훈련된 모델을 저장하거나 불러오는 경로.\n- params_path: 최적의 하이퍼파라미터를 저장하거나 불러오는 경로.\n\n  4.6.2: 목적 메서드\n\n```js\ndef objective(self, trial, n_episodes=10):\n        lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n        gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n        target_update_frequency = trial.suggest_categorical('target_update_frequency', [500, 1000, 2000])\n\n        optimizer = optim.Adam(self.main_network.parameters(), lr=lr)\n        trainer = DQNTrainer(self.env, self.main_network, self.target_network, optimizer, self.replay_buffer, self.model_path, gamma=gamma, batch_size=batch_size, target_update_frequency=target_update_frequency)\n        reward = trainer.train(n_episodes, save=False)\n        return reward\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n목표 함수는 하이퍼파라미터에 대한 값을 제안하고 이러한 값을 사용하여 모델을 훈련합니다.\n\n- lr = trial.suggest_loguniform(`lr`, 1e-5, 1e-1): 범위 [1e-5, 1e-1] 내의 학습률을 제안합니다.\n- gamma = trial.suggest_uniform(`gamma`, 0.9, 0.999): 범위 [0.9, 0.999] 내의 할인 요인을 제안합니다.\n- batch_size = trial.suggest_categorical(`batch_size`, [32, 64, 128]): 지정된 목록에서 배치 크기를 제안합니다.\n- target_update_frequency = trial.suggest_categorical(`target_update_frequency`, [500, 1000, 2000]): 지정된 목록에서 대상 업데이트 빈도를 제안합니다.\n\n```js\noptimizer = optim.Adam(self.main_network.parameters(), (lr = lr));\n```\n\n여기서는 주어진 학습률로 Adam 옵티마이저를 설정합니다. Adam은 주로 신경망 훈련에 사용되는 최적화 알고리즘인 Adaptive Moment Estimation의 약자입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n신경망에서 각 매개변수에 대해 Adam은 손실 함수의 기울기를 계산합니다. 그것은 기울기의 지수 이동 평균 (첫 번째 모멘트로 표시되는 m)과 제곱 기울기 (두 번째 모멘트로 표시되는 v)를 추적합니다.\n\n이동 평균의 초기화 편향을 고려하기 위해 Adam은 첫 번째 및 두 번째 모멘트 추정치에 바이어스 보정을 적용합니다. 그런 다음 매개변수는 수정된 첫 번째 및 두 번째 모멘트를 사용하여 업데이트됩니다. 업데이트 규칙은 학습률과 모멘트를 통합하여 기울기의 크기와 방향을 모두 고려하는 방식으로 매개변수를 조정합니다.\n\n다음은 Adam에 대한 보다 포괄적인 기사입니다:\n\n```js\ntrainer = DQNTrainer(\n  self.env,\n  self.main_network,\n  self.target_network,\n  optimizer,\n  self.replay_buffer,\n  self.model_path,\n  (gamma = gamma),\n  (batch_size = batch_size),\n  (target_update_frequency = target_update_frequency)\n);\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 코드는 제안된 하이퍼파라미터로 theDQNTrainer 인스턴스를 초기화합니다.\n\n```js\nreward = trainer.train(n_episodes, (save = False));\n```\n\n마지막으로, 이 코드는 지정된 에피소드 수로 모델을 학습하고 평균 보상을 반환합니다.\n\n4.6.3: 최적화 메소드\n이 섹션에서는 모델의 성능을 극대화하는 조합을 효율적으로 찾을 수 있도록 하이퍼파라미터 공간을 체계적으로 탐색하는 파이썬 라이브러리인 Optuna를 사용하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\ndef optimize(self, n_trials=100, save_params=True):\n    if not TRAIN and os.path.isfile(self.params_path):\n        with open(self.params_path, 'rb') as f:\n            best_params = pickle.load(f)\n        print(\"디스크에서 매개변수를 불러왔습니다.\")\n    elif not FINETUNE:\n        best_params = {\n            'lr': LEARNING_RATE,\n            'gamma': GAMMA,\n            'batch_size': BATCH_SIZE,\n            'target_update_frequency': TARGET_UPDATE_FREQUENCY\n        }\n        print(f\"기본 매개변수를 사용합니다: {best_params}\")\n    else:\n        print(\"하이퍼파라미터를 최적화 중입니다.\")\n        study = optuna.create_study(direction='maximize')\n        study.optimize(self.objective, n_trials=n_trials)\n        best_params = study.best_params\n\n        if save_params:\n            with open(self.params_path, 'wb') as f:\n                pickle.dump(best_params, f)\n            print(\"매개변수를 디스크에 저장했습니다.\")\n\n    return best_params\n```\n\n`optimize` 메소드는 지정된 횟수의 시도에 대해 최적화 프로세스를 실행합니다.\n\n```python\nif not TRAIN and os.path.isfile(self.params_path):\n        with open(self.params_path, 'rb') as f:\n            best_params = pickle.load(f)\n        print(\"디스크에서 매개변수를 불러왔습니다.\")\n```\n\n학습이 필요하지 않은 경우 (TRAIN이 아닌 경우) 및 매개변수 파일이 존재하는 경우, 매개변수가 디스크에서 로드됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nelif not FINETUNE:\n    best_params = {\n        'lr': LEARNING_RATE,\n        'gamma': GAMMA,\n        'batch_size': BATCH_SIZE,\n        'target_update_frequency': TARGET_UPDATE_FREQUENCY\n    }\n    print(f\"기본 매개변수 사용 중: {best_params}\")\n```\n\n만약 파라미터 튜닝이 필요하지 않다면 (not FINETUNE), 기본 매개변수가 사용됩니다.\n\n```python\nelse:\n    print(\"하이퍼파라미터 최적화 중\")\n    study = optuna.create_study(direction='maximize')\n    study.optimize(self.objective, n_trials=n_trials)\n    best_params = study.best_params\n\n    if save_params:\n        with open(self.params_path, 'wb') as f:\n            pickle.dump(best_params, f)\n        print(\"매개변수를 디스크에 저장했습니다\")\n```\n\n하이퍼파라미터 최적화가 필요한 경우, Optuna를 사용하여 최적의 매개변수를 찾습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nstudy = optuna.create_study(direction='maximize')을 사용하면 목적 함수를 최대화하는 Optuna 스터디를 생성할 수 있어요.\n\nstudy.optimize(self.objective, n_trials=n_trials)은 지정된 횟수의 시행을 위한 최적화를 실행해요.\n\nsave_params를 True로 설정하면, 최적의 매개변수가 디스크에 저장돼요.\n\n다음은 Optuna를 깊이 들여다보는 포함한 다양한 세밀 조정 기법을 탐구한 멋진 기사에요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4.7: 모델 실행하기\n\n마지막으로, 모든 과정을 다시 한번 확인하고 코드를 실행해 봅시다!\n\n4.7.1: 훈련 및 파인튜닝 설정\n\n```js\nTRAIN = True\nFINETUNE = False\n\n# 다음 하이퍼파라미터를 설정하세요 (FINETUNE이 False인 경우)\nGAMMA = 0.99\nBATCH_SIZE = 64\nTARGET_UPDATE_FREQUENCY = 1000\nLEARNING_RATE = 1e-3\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTRAIN = True은 모델을 학습할지 여부를 나타냅니다. False로 설정하면 학습이 건너뛰어집니다.\n\nFINETUNE = False는 모델을 fine-tune할지 여부를 나타냅니다. True로 설정하면 기존 매개변수가 사용되고 fine-tune됩니다.\n\n만약 FINETUNE이 False인 경우, 다음 하이퍼파라미터를 설정합니다:\n\n- GAMMA = 0.99: 미래 보상에 대한 할인 계수입니다. 이는 즉시 보상에 비해 미래 보상이 얼마나 중요한지를 결정합니다.\n- BATCH_SIZE = 64: 각 학습 단계마다 재생 버퍼에서 샘플링된 경험의 수입니다.\n- TARGET_UPDATE_FREQUENCY = 1000: 타겟 네트워크의 가중치가 주요 네트워크의 가중치와 일치하도록 업데이트되는 빈도(스텝 단위).\n- LEARNING_RATE = 1e-3: 최적화기(optimizer)의 학습률로, 모델 가중치가 업데이트될 때 추정 오차에 따라 모델을 얼마나 변경할지를 제어합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4.7.2: 네트워크 및 재생 버퍼 초기화\n\n```js\nmain_network = DQN(state_dim, action_dim);\ntarget_network = DQN(state_dim, action_dim);\ntarget_network.load_state_dict(main_network.state_dict());\ntarget_network.eval();\n\nreplay_buffer = ReplayBuffer(10000);\n```\n\nmain_network = DQN(state_dim, action_dim)은 지정된 상태 및 액션 차원으로 메인 네트워크를 초기화합니다.\n\ntarget_network = DQN(state_dim, action_dim)은 메인 네트워크와 동일한 구조로 대상 네트워크를 초기화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\ntarget_network.load_state_dict(main_network.state_dict()) 함수는 메인 네트워크의 가중치를 타겼 네트워크로 복사합니다.\n\ntarget_network.eval() 함수는 타겟 네트워크를 평가 모드로 설정합니다. 이는 추론 중에 드롭아웃과 배치 정규화와 같은 특정 레이어가 적절하게 동작하도록 합니다.\n\nreplay_buffer = ReplayBuffer(10000)은 10,000개의 경험을 저장할 수 있는 용량을 가진 리플레이 버퍼를 초기화합니다.\n\n4.7.3: 단계 카운트 설정\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nSTEP_COUNT = 0;\n```\n\nSTEP_COUNT = 0은 훈련 중 취한 단계 수를 추적하는 카운터를 초기화합니다.\n\n4.7.4: 옵티마이저 초기화 및 하이퍼파라미터 최적화\n\n```js\noptimizer = Optimizer(env, main_network, target_network, replay_buffer, f'{os.path.dirname(__file__)}/model/model.pth', f'{os.path.dirname(__file__)}/model/params.pkl')\nbest_params = optimizer.optimize(n_trials=2, save_params=True)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`optimizer = Optimizer(...)`은 환경, 네트워크, 리플레이 버퍼, 모델 경로 및 매개변수 경로로 Optimizer 클래스를 초기화합니다.\n\n`best_params = optimizer.optimize(n_trials=2, save_params=True)`는 최적의 하이퍼파라미터를 찾기 위해 최적화 프로세스를 실행합니다. 이 함수는 다음과 같은 기능을 수행합니다:\n\n- 지정된 횟수(n_trials=2)만큼 최적화를 실행합니다.\n- `save_params`가 True인 경우 최적의 하이퍼파라미터를 디스크에 저장합니다.\n\n  4.7.5: PyTorch Optimizer 및 DQN Trainer 생성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\noptimizer = optim.Adam(main_network.parameters(), lr=best_params['lr'])\ntrainer = DQNTrainer(env, main_network, target_network, optimizer, replay_buffer, f'{os.path.dirname(__file__)}/model/model.pth', gamma=best_params['gamma'], batch_size=best_params['batch_size'], target_update_frequency=best_params['target_update_frequency'])\ntrainer.train(1000)\n```\n\n`optimizer = optim.Adam(main_network.parameters(), lr=best_params['lr'])`은 최적의 하이퍼파라미터에서 학습률을 사용하여 Adam 옵티마이저를 생성합니다.\n\n`trainer = DQNTrainer(...)`는 환경, 네트워크, 옵티마이저, 리플레이 버퍼, 모델 경로 및 최적의 하이퍼파라미터로 DQNTrainer 클래스를 초기화합니다.\n\n`trainer.train(1000)`은 모델을 1000번의 에피소드 동안 훈련합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 요정의 훈련 초기 10 에피소드를 살펴보겠습니다:\n\n![agent training](https://miro.medium.com/v2/resize:fit:1200/1*ncnLXIRABedg4uKwVL0L5w.gif)\n\n여기서 모델은 서툴러서 무작위로 종종 비최적적인 결정을 내립니다. 요정이 환경을 탐험하고 기초를 배우기 때문에 이는 예상되는 현상입니다. 아직 보상을 극대화하기 위한 견고한 전략을 개발하지 못했습니다. 추가적인 훈련 에피소드를 거치면서 시간이 지남에 따라, 요정의 성능은 정책을 미세 조정하고 경험을 통해 배우면서 크게 향상되어야 합니다.\n\n이제 모델이 1000번 훈련된 후의 10개의 훈련 에피소드를 살펴봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](https://miro.medium.com/v2/resize:fit:1200/1*s4j6V4V-nLfkc18C2Z2-zA.gif)\n\n이것은 주목할 만한 개선입니다. 모델이 아직 NASA에 완성되지는 않았지만, 몇 가지 주요 향상 사항을 관찰할 수 있습니다:\n\n- 에이전트가 더 신중하고 전략적인 결정을 내립니다.\n- 환경을 더 효율적으로 탐색합니다.\n- 부적절한 조치의 빈도가 크게 감소했습니다.\n\n지속적인 훈련과 세밀한 조정을 통해, 에이전트의 성능은 더 개선될 것으로 예상되며, 최적의 행동에 더 가까워질 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 여러분의 차례입니다. 모델을 더 발전시켜 보세요. 하이퍼파라미터를 조정하거나 다른 모델 구조를 실험해 보세요. 창의성과 인내심으로 최선을 다하면 얼마든지 성과를 낼 수 있을 거에요. 곧 완벽하게 패치된 셔틀은 원활하게 착륙할 거예요!\n\n# 5: 결론\n\n딥 Q-네트워크를 구축, 훈련 및 평가하는 방법을 잘 이해하셨으니, 이제 다양한 환경에서 이 DQN을 테스트하고 다양한 도전에 적응하는 모습을 관찰해 보세요.\n\n에이전트의 성능을 향상시키기 위해 고급 기술을 구현하고 새로운 아키텍처를 탐험해 보세요. 예를 들어 다양한 하이퍼파라미터를 설정해보거나 다른 최적화 알고리즘(예: SGD 또는 Nadam)을 사용하거나 다른 미세조정 알고리즘을 사용해 볼 수 있어요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n- Sutton, R. S., \u0026 Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT Press.\n- Lin, L. J. (1992). “Self-improving reactive agents based on reinforcement learning, planning and teaching.” Machine Learning, 8(3–4), 293–321.\n- OpenAI. “LunarLander-v2.” OpenAI Gym. [링크](https://gym.openai.com/envs/LunarLander-v2/)\n- 버클리 AI 연구소 (BAIR). “Experience Replay.” [링크](https://bair.berkeley.edu/blog/2020/03/20/experiencereplay/)\n- Towards Data Science. “Reinforcement Learning 101: Q-Learning.” [링크](https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292)\n- Towards Data Science. “신경망 뒤의 수학.” [링크](https://towardsdatascience.com/the-math-behind-neural-networks-3a18b7f8d8dc)\n- Towards Data Science. “Adam Optimizer 뒤의 수학.” [링크](https://towardsdatascience.com/the-math-behind-adam-optimizer-3a18b7f8d8dc)\n- Towards Data Science. “Deep Neural Networks 맞춤화 뒤의 수학.” [링크](https://towardsdatascience.com/the-math-behind-fine-tuning-deep-neural-networks-3a18b7f8d8dc)\n\n마지막까지 읽어 주셔서 축하드립니다! 이 기사가 유익하고 즐거우셨기를 바랍니다. 만약 그렇다면, 박수를 남기고 더 많은 이런 기사를 보고 싶다면 저를 팔로우해 주세요. 앞으로 다루었으면 하는 주제나 기사에 대한 의견을 들을 수 있습니다. 피드백과 지원에 감사드립니다. 읽어 주셔서 감사합니다!\n","ogImage":{"url":"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_0.png"},"coverImage":"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_0.png","tag":["Tech"],"readingTime":35},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003ePython을 사용하여 달에 착륙하는 셔틀 가르치기: Deep Q-Networks를 활용한 강화 학습의 수학적 탐구\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_0.png\" alt=\"Reinforcement Learning\"\u003e\u003c/p\u003e\n\u003cp\u003e강화 학습(RL)에서 Q-학습은 에이전트가 환경을 탐색하면서 누적 보상을 극대화하기 위한 정책을 학습하는 데 도움이 되는 기본 알고리즘입니다. 이를 통해 특정 상태에서 특정 작업을 수행했을 때 기대되는 유틸리티를 추정하는 작업-값 함수를 업데이트 함으로써 보상을 받고 미래 추정에 기반합니다 (이게 익숙하지 않으신가요? 걱정 마세요. 나중에 함께 자세히 살펴볼 겁니다).\u003c/p\u003e\n\u003cp\u003e그러나 전통적인 Q-학습에는 도전 과제가 있습니다. 상태 공간이 확장됨에 따라 확장 가능성에 어려움을 겪으며 연속적인 상태 및 작업 공간을 갖는 환경에서 효과적이지 않습니다. 이때 Deep Q Networks (DQNs)가 나타납니다. DQNs는 Q-값을 근사하기 위해 신경망을 사용하여 에이전트가 보다 크고 복잡한 환경을 효과적으로 처리할 수 있도록 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e본 기사에서는 Deep Q Networks에 대해 자세히 살펴보겠습니다. DQNs가 기존의 Q-learning의 한계를 극복하는 방법과 DQN을 구성하는 주요 구성 요소에 대해 탐구할 것입니다. 또한 처음부터 DQN을 구현하고 더 복잡한 환경에 적용하는 과정을 살펴볼 것입니다. 이 기사를 마치면 DQN이 어떻게 작동하는지 이해하고 도전적인 강화 학습 문제를 해결하는 데 사용하는 방법을 알게 될 것입니다.\u003c/p\u003e\n\u003ch2\u003e목차\u003c/h2\u003e\n\u003cp\u003e1: 전통적인 Q-러닝\n∘ 1.1: 상태와 행동\n∘ 1.2: Q-값\n∘ 1.3: Q-테이블\n∘ 1.4: 학습 과정\u003c/p\u003e\n\u003cp\u003e2: Q-러닝에서 Deep Q-네트워크로\n∘ 2.1: 전통적인 Q-러닝의 한계\n∘ 2.2: 신경망\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e3: Deep Q-Network의 해부학\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e3.1: DQN의 구성요소\u003c/li\u003e\n\u003cli\u003e3.2: DQN 알고리즘\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e4: 처음부터 Deep Q-Network 구현하기\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e4.1: 환경 설정\u003c/li\u003e\n\u003cli\u003e4.2: 딥 신경망 구축\u003c/li\u003e\n\u003cli\u003e4.3: 경험 재생 구현\u003c/li\u003e\n\u003cli\u003e4.4: 타깃 네트워크 구현\u003c/li\u003e\n\u003cli\u003e4.5: Deep Q-Network 훈련\u003c/li\u003e\n\u003cli\u003e4.6: 모델 튜닝\u003c/li\u003e\n\u003cli\u003e4.7: 모델 실행\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e5: 결론\n참고 문헌\u003c/p\u003e\n\u003ch1\u003e1: 전통적인 Q-Learning\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_1.png\" alt=\"Reinforcement Learning Deep Q Networks\"\u003e\u003c/p\u003e\n\u003cp\u003eQ-러닝은 환경에서 누적 보상을 극대화하기 위한 최적 조치를 학습하는 에이전트를 안내합니다. 딥 Q-네트워크에 집중하기 전에, 그 선구자인 Q-러닝 뒤에 있는 메커니즘을 간단히 검토하는 것이 좋습니다.\u003c/p\u003e\n\u003ch2\u003e1.1: 상태 및 조치\u003c/h2\u003e\n\u003cp\u003e미로를 탐색하는 로봇이라고 상상해보세요. 미로에서 차지하는 각 위치를 \"상태\"라고 합니다. 왼쪽, 오른쪽, 위 또는 아래로 이동하는 것과 같은 각각의 움직임을 \"조치\"라고 합니다. 목표는 결국 미로를 통해 최적 경로를 찾으려면 각 상태에서 어떤 조치를 취할지 결정하는 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e1.2: Q-Values\u003c/h2\u003e\n\u003cp\u003eQ-Learning의 핵심은 Q-값으로, 𝑄(𝑠, 𝑎)로 표시됩니다. 이 값은 특정 상태 s에서 특정 행동 a를 취한 후 더 나은 경로(정책)를 따를 때 기대되는 미래 보상을 나타냅니다.\u003c/p\u003e\n\u003cp\u003eQ-값을 가이드북의 항목으로 생각해보세요. 각 가능한 이동의 장기적 이점을 평가하는 것입니다. 예를 들어 미로의 특정 위치에 있다고 가정했을 때 왼쪽으로 이동하는 경우, Q-값은 미래 보상 측면에서 그 이동이 얼마나 유익할지 알려줍니다. 더 높은 Q-값은 더 나은 이동을 나타냅니다.\u003c/p\u003e\n\u003ch2\u003e1.3: The Q-Table\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eQ-Learning은 Q-값을 추적하는 데 Q-테이블을 사용합니다. Q-테이블은 기본적으로 각 행이 상태에 해당하고 각 열이 행동에 해당하며 각 셀이 해당 상태-행동 쌍의 Q-값을 포함하는 대형 스프레드시트입니다.\u003c/p\u003e\n\u003cp\u003eQ-테이블을 거대한 스프레드시트로 상상해보세요. 각 셀은 미로의 특정 위치에서 특정 이동을 하였을 때 잠재적 미래 보상을 나타냅니다. 환경에 대해 더 많이 배우면이 보상의 더 나은 추정치로이 스프레드시트를 업데이트합니다.\u003c/p\u003e\n\u003ch2\u003e1.4: 학습 과정\u003c/h2\u003e\n\u003cp\u003eQ-러닝의 학습 과정은 반복적입니다. 초기 상태 s에서 시작합니다. 그런 다음 작업 a를 결정합니다. 이 선택은 다음을 기반으로 할 수 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e탐험: 효과를 발견하기 위해 새로운 조치를 시도합니다.\u003c/li\u003e\n\u003cli\u003e개척: 가장 높은 알려진 Q-값을 갖는 조치를 선택하기 위해 기존 지식을 사용합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e선택한 조치를 수행하고 보상 r을 관찰하며 다음 상태 s'로 이동합니다. Q-러닝 공식을 사용하여 상태-조치 쌍 (s, a)의 Q-값을 업데이트합니다:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_2.png\"\u003e이미지\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e여기에:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eα는 학습 속도로, 새로운 정보가 이전 정보를 얼마나 덮어쓸지를 결정합니다.\u003c/li\u003e\n\u003cli\u003eγ는 할인 요소로, 즉각적 보상을 먼 미래의 보상보다 더 가치 있게 여깁니다.\u003c/li\u003e\n\u003cli\u003emaxa′Q(s′,a′)는 다음 상태 s′에서 가능한 모든 행동 a′에 대해 최고의 Q값을 나타냅니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e매번 안내서를 업데이트하고 있다고 상상해 보세요. 각 이동 후에 성공적인지 실패인지에 대한 피드백(보상)을 받습니다. 그런 다음 새로운 정보를 반영하도록 가이드북의 등급(Q값)을 조정하여 미래의 결정을 더 잘 하게 됩니다.\u003c/p\u003e\n\u003cp\u003eQ값이 수렴할 때까지 이 과정을 반복하면, 에이전트는 미로를 탐색하는 최적 정책을 학습한 것입니다. 시간이 흘러, 미로를 반복적으로 탐험하고 경험에 기반하여 가이드북을 업데이트함으로써 최상의 보상을 얻기 위한 최적의 움직임을 알려주는 포괄적인 전략을 개발하게 됩니다.\u003c/p\u003e\n\u003cp\u003eQ-러닝에 대해 자세히 알아보려면 이 기사를 확인해 보세요: \u003ca href=\"%EB%A7%81%ED%81%AC\"\u003e링크\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e2: Q-Learning에서 Deep Q-Network로\u003c/h1\u003e\n\u003ch2\u003e2.1: 전통적인 Q-Learning의 한계\u003c/h2\u003e\n\u003cp\u003eQ-Learning은 강화 학습에 대한 강력한 알고리즘이지만, 더 복잡한 환경에서 효과적으로 동작하는 데 제약 사항이 몇 가지 있습니다:\u003c/p\u003e\n\u003cp\u003e확장성 문제: 전통적인 Q-Learning은 각 상태-행동 쌍이 Q-값에 매핑된 Q-테이블을 유지합니다. 상태 공간이 성장함에 따라, 특히 고차원 또는 연속적인 환경에서는 Q-테이블이 불필요하게 커져 메모리 비효율성과 학습 속도 저하를 초래합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이산 상태 및 행동 공간: Q-Learning은 상태와 행동이 이산적이고 유한한 환경에서 잘 동작합니다. 하지만 현실 세계의 많은 문제는 연속적인 상태와 행동 공간을 포함하고 있습니다. 이러한 전통적인 Q-Learning은 이러한 공간을 이산화하지 않고는 효율적으로 처리할 수 없으며, 이로 인해 정보 손실과 최적 정책의 하락을 초래할 수 있습니다.\u003c/p\u003e\n\u003ch2\u003e2.2: 신경망\u003c/h2\u003e\n\u003cp\u003e이제 신경망을 소개해 보겠습니다. 신경망은 딥 네트워크에서 중요한 역할을 하는데, 인간 두뇌의 구조와 기능을 모방하여 데이터로부터 복잡한 패턴을 학습할 수 있는 강력한 함수 근사기입니다. 신경망은 입력 데이터를 처리하고 가중치와 편향을 통해 변환하여 출력을 생성하는 연결된 노드(뉴런)의 계층으로 이루어져 있습니다.\u003c/p\u003e\n\u003cp\u003e강화 학습의 맥락에서 신경망은 Q-함수를 근사화하는 데 사용될 수 있습니다. 이는 상태-행동 쌍을 Q-값에 매핑하는 데 도움이 되며, 특히 Q-테이블을 유지하는 것이 적절하지 않은 대규모나 연속적인 공간에서 상태와 행동 간에 일반화를 더 잘할 수 있도록 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e따라서, Deep Q-networks(DQNs)은 Q-Learning의 원리를 신경망의 함수 근사 능력과 결합시켜요. 그렇게 하면 전통적인 Q-learning의 주요 제약 사항을 다룰 수 있어요.\u003c/p\u003e\n\u003cp\u003eDQNs은 Q-값을 테이블에 저장하는 대신 신경망을 사용하여 Q-함수를 근사해요. 이 네트워크는 상태를 입력으로 받아 가능한 모든 행동에 대한 Q-값을 출력해요. 환경에서의 경험으로 네트워크를 학습시켜 에이전트는 각 행동에 대한 예상 보상을 예측하도록합니다. 이를 통해 다양한 상태와 행동에 걸쳐 일반화할 수 있어요.\u003c/p\u003e\n\u003cp\u003e체스를 배우는 것을 상상해보세요. 가능한 모든 체스판 구성과 각 동작에 대한 최상의 수를 외우는 대신(불가능한 일이죠), 전략과 원칙(예를 들어 보드 중앙을 제어하고 왕을 보호하는 것과 같은 것)을 배우게 됩니다. 비슷하게, DQN는 신경망을 통해 일반적인 패턴과 전략을 배우고 모든 가능한 상태를 외우지 않고도 정보를 바탕으로 결정할 수 있어요.\u003c/p\u003e\n\u003cp\u003e신경망 사용은 DQN이 크거나 연속된 상태 공간을 다룰 수 있게 해요. 네트워크는 주요 특징을 잡아내는 상태 공간의 표현을 학습해 중요한 결정을 취할 수 있도록 해줍니다.\u003c/p\u003e\n\u003cp\u003e큰 도시를 이동하려면 고려합시다. 모든 거리와 건물의 배치를 외우는 대신 표지판과 중요 도로를 인식해 길을 찾게 됩니다. DQN의 신경망도 비슷하게 작용하며, 에이전트가 복잡한 환경에서 이동하는 것을 돕는 상태 공간의 중요한 특징을 인식하도록 학습합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다양한 경험을 훈련함으로써 모델은 과거 경험에서 일반화하는 법을 배우게 됩니다. 즉, 에이전트는 배운 것을 새로운, 보지 못한 상태와 행동에 적용할 수 있어서 다양한 상황에서 더 적응력이 있고 효율적일 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e3: 딥 Q-네트워크의 구성 요소\u003c/h1\u003e\n\u003ch2\u003e3.1: DQN의 구성 요소\u003c/h2\u003e\n\u003cp\u003e딥 Q-네트워크 (DQN)가 어떻게 작동하는지 이해하려면 그 주요 구성 요소를 자세히 살펴보는 것이 중요합니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e3.1.1: 신경망\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_3.png\" alt=\"신경망\"\u003e\u003c/p\u003e\n\u003cp\u003eDQN의 핵심은 Q값을 위한 함수 근사기 역할을 하는 신경망입니다. 아키텍처는 일반적으로 다음과 같이 구성됩니다:\u003c/p\u003e\n\u003cp\u003e입력 레이어: 에이전트의 \"눈\"으로 상상해보세요. 이 레이어는 환경으로부터 상태 표현을 받아들이는데, 마치 당신의 눈이 주변의 시각적 정보를 받아들이는 것과 유사합니다. 위의 이미지에서 왼쪽에 두 개의 노드가 있는 첫 번째 레이어입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eHidden Layers: 이러한 레이어들은 에이전트의 \"뇌\"로 생각할 수 있습니다. 눈을 통해 받은 정보를 다수의 사고 단계를 거쳐 처리하여 복잡한 특징과 패턴을 식별합니다. 마치 당신의 뇌가 세계를 처리하고 이해하는 방식과 비슷합니다. 위 이미지에서는 세 개의 노드가 있는 중간 레이어입니다.\u003c/p\u003e\n\u003cp\u003eOutput Layer: 이는 에이전트의 \"의사 결정\" 부분과 같습니다. 입력 상태에 따라 모든 가능한 행동에 대한 Q 값(값함수)을 생성합니다. 당신이 보고 생각한 것에 기반하여 최선의 행동을 결정하는 방식과 유사합니다. 각 출력은 특정 행동을 취했을 때 기대되는 보상에 해당합니다. 위 이미지에서는 한 개의 노드를 가진 오른쪽의 마지막 레이어입니다.\u003c/p\u003e\n\u003cp\u003e위 이미지는 간단한 피드포워드 신경망을 나타냅니다. 이는 신경망의 가장 기본적인 형태입니다. 이 구조는 기본적이지만 \"깊은\" 신경망은 아닙니다. 깊은 신경망으로 변환하기 위해서는 더 많은 은닉 레이어를 추가하여 신경망의 깊이를 증가시켜야 합니다. 또한, 다양한 아키텍처와 구성을 실험하여 더 발전된 모델을 개발할 수 있습니다. 각 레이어의 노드 수는 고정되지 않으며, 특정 훈련 데이터셋과 작업에 따라 다양합니다. 이러한 유연성을 통해 네트워크를 특정 목적에 더 잘 맞게 조정할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e신경망에 대해 더 알고 싶다면, 나는 아래의 글을 강력히 추천합니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e3.1.2: 경험 재생\n이제 목록의 다음 항목인 경험 재생으로 넘어가 봅시다. 이것은 DQNs에서 학습 과정을 안정화하고 향상시키는 기술입니다. 다음을 포함합니다:\u003c/p\u003e\n\u003cp\u003e메모리 버퍼: 에이전트의 \"일기\"로 생각해보세요. 이것은 에이전트의 경험을 시간이 지남에 따라 저장합니다 (상태, 행동, 보상, 다음 상태, 완료), 마치 매일 당신이 무슨 일이 일어났는지 기록하는 것처럼.\u003c/p\u003e\n\u003cp\u003e랜덤 샘플링: 훈련 중에 에이전트는 지난 경험을 배우기 위해 일기의 랜덤한 페이지를 넘깁니다. 이는 사건의 순서를 깨어주어 에이전트가 경험의 순서에 과적합되는 것을 방지하여 보다 견고하게 학습하도록 돕습니다.\u003c/p\u003e\n\u003cp\u003e3.1.3: 타겟 네트워크\n마지막으로, 타겟 네트워크는 훈련을 위해 타겟 Q-값을 계산하는 데 사용되는 별도의 신경망입니다. 주 신경망과 동일한 구조를 가지고 있지만 주 신경망의 가중치가 정기적으로 업데이트되어 일치하도록 고정되어 있습니다. 에이전트를 위한 \"안정된 안내서\"로 생각해보세요. 주 신경망이 지속적으로 학습하고 업데이트되는 반면, 타겟 네트워크는 안정된 Q-값을 제공하여 훈련에 도움을 줍니다. 학습을 안정적이고 일관되게 유지하는 데 도움이 되는 신뢰할 수 있는, 주기적으로 업데이트되는 매뉴얼이 있는 것과 같습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e3.2: DQN 알고리즘\u003c/h2\u003e\n\u003cp\u003e이러한 구성 요소가 준비되면 DQN 알고리즘은 다음과 같은 몇 가지 중요한 단계로 개요를 제시할 수 있습니다:\u003c/p\u003e\n\u003ch3\u003e3.2.1: 순방향 전파\u003c/h3\u003e\n\u003cp\u003e먼저, 우리는 Q-values를 예측하는 데 중요한 순방향 전파로 시작합니다. 이러한 Q-values는 특정 상태에서 특정 행동을 취했을 때 기대되는 미래 보상을 저장합니다. 이 프로세스는 상태 입력부터 시작됩니다.\u003c/p\u003e\n\u003ch4\u003e상태 입력\u003c/h4\u003e\n\u003cp\u003e에이전트는 환경에서 현재 상태 s를 관찰합니다. 이 상태는 에이전트의 현재 상황을 설명하는 특징 벡터로 표현됩니다. 상태를 에이전트 주변 세계의 스냅숏으로 생각해보세요. 눈이 주변을 둘러보는 것처럼 시각 장면을 촬영할 때와 유사합니다. 이 스냅숏에는 에이전트가 결정을 내리기 위해 필요한 모든 세부 정보가 포함되어 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eQ-Value Prediction\n이제 이 관측된 상태 s가 신경망으로 전달됩니다. 이 신경망은 여러 층을 통해 이 입력을 처리하고 Q-값 세트를 출력합니다. 각 Q-값은 가능한 작업 a에 해당하며, 매개 변수 θ는 네트워크의 가중치와 편향을 나타냅니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_4.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e신경망을 에이전트 뇌의 복잡한 의사 결정 기계로 상상해보세요. 캡처(상태)를 받으면 이 정보를 여러 단계(층)를 통해 처리하여 다양한 작업에 대한 잠재적 결과(Q-값)를 찾습니다. 보이는 것을 바탕으로 취할 수 있는 다양한 작업을 고려해보는 것과 유사합니다.\u003c/p\u003e\n\u003cp\u003e작업 선택\n그런 다음 에이전트는 가장 높은 Q-값을 가진 작업 a∗를 다음 움직임으로 선택하며, 이에 따라 탐욕적 작업 선택 정책을 따릅니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_5.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e이것은 모든 옵션을 심사한 후에 최선의 움직임을 결정하는 것과 유사합니다. 에이전트는 가장 높은 보상을 가져다줄 것으로 믿는 행동을 선택하며, 마치 당신이 보고 이해한 것을 기반으로 가장 유망한 길을 선택하는 것과 같습니다.\u003c/p\u003e\n\u003cp\u003e3.2.2: 경험 재생\n다음으로, 우리는 학습 과정을 안정화하고 향상시키는 데 도움이 되는 경험 재생에 대해 이야기하겠습니다.\u003c/p\u003e\n\u003cp\u003e경험 저장\n에이전트가 행동 a를 취하고 보상 r을 받은 후 새로운 상태 s′를 받으면, 이 경험을 (s, a, r, s′, done) 튜플로 저장하여 플레이백 버퍼에 저장합니다. 변수 done은 에피소드가 종료되었는지를 나타냅니다. 플레이백 버퍼를 에이전트가 경험을 기록하는 다이어리로 생각해보세요. 이는 당신이 하루 중 주목할 만한 사건을 메모하는 것과 유사합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e샘플 미니배치\n훈련 중에는 경험의 미니배치가 임의로 선택되어 재생 버퍼에서 샘플링됩니다. 이 배치는 타겟 Q-값을 계산하고 손실을 최소화하여 네트워크를 업데이트하는 데 사용됩니다. 에이전트가 훈련할 때, 과거 경험을 학습하기 위해 일기장의 임의의 페이지를 넘겨보게 됩니다. 이 임의 샘플링은 사건의 순서를 깨고 다양한 학습 예제를 제공하며, 일기의 서로 다른 날짜를 검토하여 보다 넓은 시야를 얻는 것과 유사한 역할을 합니다.\u003c/p\u003e\n\u003cp\u003e3.2.3: 역전파\n최종 단계는 역전파로, 이는 네트워크를 업데이트하여 예측을 개선합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e타겟 Q-값 계산\n미니 배치의 각 경험에 대해, 에이전트는 타겟 Q-값 y_를 계산합니다. 만약 다음 상태 s′가 종료 상태(즉, done이 true인 경우)라면, 타겟 Q-값은 간단히 보상 r입니다. 그렇지 않으면, 타겟 Q-값은 보상에 다음 상태 s′에서 타겟 네트워크 Qtarget에 의해 예측된 할인된 최대 Q-값을 더한 값입니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_7.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서 γ는 할인 계수(0 ≤ γ ≤ 1)입니다. 이 단계는 과거 경험에 기반해 미래를 계획하는 것과 같습니다. 경험이 여행(에피소드)을 끝낼 경우, 타겟은 받은 보상입니다. 계속된다면, 타겟에는 즉시와 미래 혜택을 모두 고려하여 행동을 계획하는 방식과 유사한 예상 미래 보상이 포함됩니다.\u003c/p\u003e\n\u003cp\u003e손실 계산\n다음으로, 손실은 메인 네트워크에서 예측된 Q-값 Q(s_i, a_i; θ)과 타겟 Q-값 yi 사이의 평균 제곱 오차로 계산됩니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_8.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e손실을 계산하는 것은 예측이 실제 결과와 얼마나 차이가 나는지를 평가하는 것과 같습니다. 실제 결과와 비교하여 추측의 정확성을 확인하고 차이점을 주목하는 것과 같습니다.\u003c/p\u003e\n\u003cp\u003e역전파 및 최적화\n마지막으로, 이 손실을 최소화하기 위해 역전파를 수행합니다. 계산된 손실은 네트워크를 통해 역전파되어 SGD(Stochastic Gradient Descent) 또는 Adam과 같은 최적화 알고리즘을 사용하여 가중치를 업데이트합니다. 이 프로세스는 손실을 최소화하기 위해 네트워크 매개변수 θ를 조정합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_9.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e여기서 α는 학습률을 나타내고, ∇θLoss는 네트워크 매개변수에 대한 손실의 그래디언트를 나타냅니다. 역전파는 실수로부터 배우는 것과 같습니다. 예측이 얼마나 잘못되었는지를 깨달았을 때 (손실), 전략(네트워크 가중치)을 조정하여 미래의 결정을 개선합니다. 피드백을 바탕으로 자신의 접근 방식을 미세 조정하여 다음에 더 나은 결과를 얻는 것과 비슷합니다.\u003c/p\u003e\n\u003cp\u003e이 아키텍처를 사용하여 에이전트는 정책을 반복적으로 개선합니다. 시간이 지남에 따라 누적 보상을 극대화하는 조치를 취하는 것을 배웁니다. 신경망, 경험 재생 및 타겟 네트워크의 결합으로 DQN은 복잡한 고차원 환경에서 효과적으로 학습할 수 있습니다. 이 과정은 에이전트가 환경을 탐색하는 데 능숙해질 때까지 계속됩니다.\u003c/p\u003e\n\u003ch1\u003e4: 처음부터 Deep Q-Network 구현\u003c/h1\u003e\n\u003cp\u003e이 섹션에서는 처음부터 Deep Q-Network (DQN)의 구현을 안내합니다. 이 섹션의 끝에는 Python에서 DQN을 구축하고 훈련하는 방법을 명확히 이해하게 될 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e우리는 OpenAI Gym의 Lunar Lander 환경을 사용할 것입니다. 이 환경에서의 목표는 달 착륙선을 조종하여 지정된 착륙 패드에 성공적으로 착륙하는 것입니다. 착륙선은 환경을 통해 비행할 때 추진기를 사용하여 움직임과 방향을 조절해야 합니다. 이 환경은 상업적으로 사용할 수 있습니다. 라이센스 및 사용 권한에 대한 자세한 내용은 OpenAI Gym GitHub 페이지에서 확인할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e오늘 다룰 모든 코드는 여기에서 찾을 수 있습니다:\u003c/p\u003e\n\u003ch2\u003e4.1: 환경 설정\u003c/h2\u003e\n\u003cp\u003e우리는 OpenAI Gym의 LunarLander 환경을 사용할 것이며, 이는 우리의 에이전트가 해결해야 할 어려운 문제를 제공합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pickle\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e gym\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch.\u003cspan class=\"hljs-property\"\u003enn\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e nn\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch.\u003cspan class=\"hljs-property\"\u003eoptim\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e optim\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e collections \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e deque\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e random\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e optuna\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e여기서 필요한 라이브러리들을 import 합니다. gym은 환경을 위해 사용되며, torch는 우리의 신경망을 구축하고 훈련하는 데 사용되며, collections, random, 및 optuna는 경험 재생과 하이퍼파라미터 최적화에 도움이 됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eenv = gym.\u003cspan class=\"hljs-title function_\"\u003emake\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"LunarLander-v2\"\u003c/span\u003e, (render_mode = \u003cspan class=\"hljs-string\"\u003e\"rgb_array\"\u003c/span\u003e));\nstate_dim = env.\u003cspan class=\"hljs-property\"\u003eobservation_space\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e];\naction_dim = env.\u003cspan class=\"hljs-property\"\u003eaction_space\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003en\u003c/span\u003e;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e우리는 LunarLander 환경을 초기화하고 상태 및 액션 공간의 차원을 가져옵니다. state_dim은 상태의 특징 수를 나타내고, action_dim은 가능한 액션 수를 나타냅니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e4.2: 딥 신경망 구축\u003c/h2\u003e\n\u003cp\u003e우리의 딥-NN에서는 DQN이라는 클래스를 생성할 것입니다. 이 클래스는 세 개의 완전 연결 계층을 가진 신경망을 정의합니다. 입력 계층은 상태 표현을 수신하며, 은닉 계층은 이 정보를 선형 변환과 ReLU 활성화 함수를 통해 처리하고, 출력 계층은 각 가능한 동작에 대한 Q-값을 생성합니다.\u003c/p\u003e\n\u003cp\u003e먼저 코드를 확인한 다음 분석해 봅시다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eDQN\u003c/span\u003e(nn.\u003cspan class=\"hljs-property\"\u003eModule\u003c/span\u003e):\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, state_dim, action_dim):\n        \u003cspan class=\"hljs-variable language_\"\u003esuper\u003c/span\u003e(\u003cspan class=\"hljs-variable constant_\"\u003eDQN\u003c/span\u003e, self).\u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e()\n        self.\u003cspan class=\"hljs-property\"\u003efc1\u003c/span\u003e = nn.\u003cspan class=\"hljs-title class_\"\u003eLinear\u003c/span\u003e(state_dim, \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e)\n        self.\u003cspan class=\"hljs-property\"\u003efc2\u003c/span\u003e = nn.\u003cspan class=\"hljs-title class_\"\u003eLinear\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e)\n        self.\u003cspan class=\"hljs-property\"\u003efc3\u003c/span\u003e = nn.\u003cspan class=\"hljs-title class_\"\u003eLinear\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, action_dim)\n\n    def \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(self, x):\n        x = torch.\u003cspan class=\"hljs-title function_\"\u003erelu\u003c/span\u003e(self.\u003cspan class=\"hljs-title function_\"\u003efc1\u003c/span\u003e(x))\n        x = torch.\u003cspan class=\"hljs-title function_\"\u003erelu\u003c/span\u003e(self.\u003cspan class=\"hljs-title function_\"\u003efc2\u003c/span\u003e(x))\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.\u003cspan class=\"hljs-title function_\"\u003efc3\u003c/span\u003e(x)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e4.2.1: 클래스 초기화\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eDQN\u003c/span\u003e(nn.Module):\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, state_dim, action_dim\u003c/span\u003e):\n        \u003cspan class=\"hljs-built_in\"\u003esuper\u003c/span\u003e(DQN, self).__init__()\n        self.fc1 = nn.Linear(state_dim, \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e)\n        self.fc2 = nn.Linear(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e)\n        self.fc3 = nn.Linear(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, action_dim)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e우리는 DQN이라는 클래스를 정의했습니다. 이 클래스는 PyTorch의 모든 신경망 모듈에 사용되는 기본 클래스인 nn.Module을 상속받습니다. 이를 통해 우리는 PyTorch의 내장 함수와 기능을 활용할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003einit\u003c/strong\u003e 메서드는 객체의 속성을 초기화하는 특별한 메서드입니다. 우리의 경우에는 신경망의 레이어를 설정하게 됩니다. 완전 연결층 (Fully Connected Layers):\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e우리는 세 개의 완전 연결 (선형) 레이어를 정의합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eself.fc1 = nn.Linear(state_dim, 128): 첫 번째 레이어는 입력 상태 차원 (상태의 피쳐 수)을 받아서 128개의 뉴런으로 매핑합니다.\u003c/li\u003e\n\u003cli\u003eself.fc2 = nn.Linear(128, 128): 두 번째 레이어는 첫 번째 레이어에서 나온 128개의 뉴런을 또 다른 128개의 뉴런으로 매핑합니다.\u003c/li\u003e\n\u003cli\u003eself.fc3 = nn.Linear(128, action_dim): 세 번째 레이어는 두 번째 레이어에서 나온 128개의 뉴런을 액션 차원 (가능한 액션 수)으로 매핑합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e각 nn.Linear 레이어는 입력 데이터에 대해 선형 변환을 수행합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_10.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e4.2.2: Forward Method\n앞서 명시된 forward 메소드는 데이터가 네트워크를 통해 흐르는 방법을 정의합니다. 이 방법은 네트워크를 통해 데이터를 전달할 때 자동으로 호출됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, x\u003c/span\u003e):\n    x = torch.relu(self.fc1(x))\n    x = torch.relu(self.fc2(x))\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.fc3(x)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e첫 번째 layer에서 입력 데이터 x는 첫 번째 fully connected layer (self.fc1)를 통해 전달됩니다. 그런 다음 출력은 ReLU (Rectified Linear Unit) 활성화 함수를 사용하여 변환됩니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ex = torch.\u003cspan class=\"hljs-title function_\"\u003erelu\u003c/span\u003e(self.\u003cspan class=\"hljs-title function_\"\u003efc1\u003c/span\u003e(x));\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReLU(Recified Linear Unit) 활성화 함수는 다음과 같이 정의됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-ReinforcementLearningDeepQ-Networks_11.png\" alt=\"ReLU activation function\"\u003e\u003c/p\u003e\n\u003cp\u003e모델에 비선형성을 도입하여 네트워크가 더 복잡한 기능을 학습할 수 있게합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e두 번째 레이어에서는 첫 번째 레이어의 출력이 두 번째 완전 연결 레이어 (self.fc2)를 통과하고 다시 ReLU 활성화 함수를 사용하여 변환됩니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ex = torch.\u003cspan class=\"hljs-title function_\"\u003erelu\u003c/span\u003e(self.\u003cspan class=\"hljs-title function_\"\u003efc2\u003c/span\u003e(x));\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e마지막으로 출력 레이어에서는 두 번째 레이어의 출력이 활성화 함수 없이 세 번째 완전 연결 레이어 (self.fc3)를 통해 전달됩니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.\u003cspan class=\"hljs-title function_\"\u003efc3\u003c/span\u003e(x);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 레이어는 각 액션에 대한 최종 Q-값을 생성합니다. 각 값은 해당 상태에서 그 액션을 취했을 때의 예상 미래 보상을 나타냅니다.\u003c/p\u003e\n\u003ch2\u003e4.3: 경험 재생 구현\u003c/h2\u003e\n\u003cp\u003eReplayBuffer 클래스는 경험을 저장하고 샘플링하는 메커니즘을 제공하여 DQN에서 학습 과정을 안정화하고 개선하는 데 필수적입니다. 따라서 에이전트가 다양한 과거 경험 세트로부터 학습할 수 있도록 해주어 일반화하고 복잡한 환경에서 잘 수행할 수 있는 능력을 향상시킵니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eReplayBuffer\u003c/span\u003e:\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, capacity):\n        self.\u003cspan class=\"hljs-property\"\u003ebuffer\u003c/span\u003e = \u003cspan class=\"hljs-title function_\"\u003edeque\u003c/span\u003e(maxlen=capacity)\n\n    def \u003cspan class=\"hljs-title function_\"\u003epush\u003c/span\u003e(self, state, action, reward, next_state, done):\n        self.\u003cspan class=\"hljs-property\"\u003ebuffer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e((state, action, reward, next_state, done))\n\n    def \u003cspan class=\"hljs-title function_\"\u003esample\u003c/span\u003e(self, batch_size):\n        state, action, reward, next_state, done = \u003cspan class=\"hljs-title function_\"\u003ezip\u003c/span\u003e(*random.\u003cspan class=\"hljs-title function_\"\u003esample\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ebuffer\u003c/span\u003e, batch_size))\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e state, action, reward, next_state, done\n\n    def \u003cspan class=\"hljs-title function_\"\u003e__len__\u003c/span\u003e(self):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ebuffer\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e4.3.1: 클래스 초기화\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eReplayBuffer\u003c/span\u003e:\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, capacity):\n        self.\u003cspan class=\"hljs-property\"\u003ebuffer\u003c/span\u003e = \u003cspan class=\"hljs-title function_\"\u003edeque\u003c/span\u003e(maxlen=capacity)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003einit\u003c/strong\u003e 메서드는 고정된 용량을 갖는 deque(덱, 이중 연결 리스트)를 초기화합니다. 덱은 양쪽 끝에서 효율적으로 항목을 추가하고 제거할 수 있는 자료 구조입니다. 빠른 양쪽 끝에서의 추가와 제거가 필요한 큐(queue)나 스택(stack)을 구현할 때 유용합니다.\u003c/p\u003e\n\u003cp\u003eself.buffer = deque(maxlen=capacity)는 capacity만큼의 경험을 저장할 수 있는 deque를 생성합니다. 버퍼가 가득 차면 새로운 경험을 추가하면 가장 오래된 경험이 자동으로 제거됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e4.3.2: Push Method\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003epush\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, state, action, reward, next_state, done\u003c/span\u003e):\n    self.buffer.append((state, action, reward, next_state, done))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e푸시 메서드는 버퍼에 새로운 경험을 추가합니다. 각 경험은 상태(state), 액션(action), 보상(reward), 다음 상태(next_state), 완료 여부(done)로 구성된 튜플입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003estate: 현재 상태.\u003c/li\u003e\n\u003cli\u003eaction: 에이전트가 취한 행동.\u003c/li\u003e\n\u003cli\u003ereward: 행동을 취한 후 받은 보상.\u003c/li\u003e\n\u003cli\u003enext_state: 행동을 취한 후 에이전트가 이동한 상태.\u003c/li\u003e\n\u003cli\u003edone: 에피소드가 종료되었는지를 나타내는 부울 값.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e4.3.3: 샘플 메서드\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esample\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, batch_size\u003c/span\u003e):\n    state, action, reward, next_state, done = \u003cspan class=\"hljs-built_in\"\u003ezip\u003c/span\u003e(*random.sample(self.buffer, batch_size))\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e state, action, reward, next_state, done\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e샘플 메서드는 버퍼에서 무작위로 일괄 경험을 검색합니다.\u003c/p\u003e\n\u003cp\u003erandom.sample(self.buffer, batch_size)는 버퍼에서 batch_size개의 경험을 무작위로 선택합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\"zip(*random.sample(self.buffer, batch_size))\"은 경험 목록을 상태, 행동, 보상, 다음 상태 및 완료에 대한 별도의 튜플로 풀어낼 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이 메서드는 샘플된 경험들로 이러한 튜플을 반환합니다.\u003c/p\u003e\n\u003cp\u003e4.3.4: Length Method\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__len__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(self.buffer)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003elen\u003c/strong\u003e 메서드는 버퍼에 저장된 현재 경험 수를 반환합니다.\u003c/p\u003e\n\u003ch2\u003e4.4: 타겟 네트워크 구현\u003c/h2\u003e\n\u003cp\u003e타겟 네트워크를 통해 안정적인 Q 값 세트를 제공하여 훈련을 위해 학습 프로세스를 안정화하고 복잡한 환경에서 에이전트의 성능을 향상시킵니다. 타겟 네트워크는 주 네트워크보다 덜 자주 업데이트되어 메인 네트워크의 가중치를 업데이트하는 데 사용되는 Q 값 추정치가 더 안정적임을 보장합니다.\u003c/p\u003e\n\u003cp\u003eDQNTrainer라는 클래스 내에 타겟 네트워크를 구현할 것이며, 이 클래스는 DQN의 훈련 프로세스를 관리하고 주 및 타겟 네트워크, 옵티마이저 및 재생 버퍼를 포함합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eDQNTrainer\u003c/span\u003e:\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, env, main_network, target_network, optimizer, replay_buffer, model_path=\u003cspan class=\"hljs-string\"\u003e'model/model.pth'\u003c/span\u003e, gamma=\u003cspan class=\"hljs-number\"\u003e0.99\u003c/span\u003e, batch_size=\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, target_update_frequency=\u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e):\n        self.\u003cspan class=\"hljs-property\"\u003eenv\u003c/span\u003e = env\n        self.\u003cspan class=\"hljs-property\"\u003emain_network\u003c/span\u003e = main_network\n        self.\u003cspan class=\"hljs-property\"\u003etarget_network\u003c/span\u003e = target_network\n        self.\u003cspan class=\"hljs-property\"\u003eoptimizer\u003c/span\u003e = optimizer\n        self.\u003cspan class=\"hljs-property\"\u003ereplay_buffer\u003c/span\u003e = replay_buffer\n        self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e = model_path\n        self.\u003cspan class=\"hljs-property\"\u003egamma\u003c/span\u003e = gamma\n        self.\u003cspan class=\"hljs-property\"\u003ebatch_size\u003c/span\u003e = batch_size\n        self.\u003cspan class=\"hljs-property\"\u003etarget_update_frequency\u003c/span\u003e = target_update_frequency\n        self.\u003cspan class=\"hljs-property\"\u003estep_count\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eDQNTrainer 클래스 정의:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003einit\u003c/strong\u003e 메서드는 학습에 필요한 다양한 구성 요소를 초기화합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eenv: 에이전트가 작동하는 환경입니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003emain_network: 훈련 중인 주요 신경망입니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etarget_network: Q-값 추정을 안정화하는 데 사용되는 대상 신경망입니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eoptimizer: 주요 신경망의 가중치를 업데이트하는 데 사용되는 옵티마이저입니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ereplay_buffer: 경험을 저장하고 샘플링하는 버퍼입니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003emodel_path: 훈련된 모델을 저장하고 로드하기 위한 경로입니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003egamma: 미래 보상에 대한 할인 계수입니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ebatch_size: 각 훈련 단계에서 재생 버퍼에서 샘플링된 경험의 수입니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etarget_update_frequency: 대상 네트워크의 가중치를 주요 네트워크의 가중치에 맞게 업데이트하는 빈도입니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003estep_count: 훈련 중에 취한 단계 수를 추적하는 카운터입니다.\u003c/p\u003e\n\u003cp\u003e4.4.2: 모델 로딩\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 모델이 있으면 로드\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e os.\u003cspan class=\"hljs-property\"\u003epath\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eexists\u003c/span\u003e(os.\u003cspan class=\"hljs-property\"\u003epath\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003edirname\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e)):\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e os.\u003cspan class=\"hljs-property\"\u003epath\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eisfile\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e):\n                self.\u003cspan class=\"hljs-property\"\u003emain_network\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eload_state_dict\u003c/span\u003e(torch.\u003cspan class=\"hljs-title function_\"\u003eload\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e))\n                self.\u003cspan class=\"hljs-property\"\u003etarget_network\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eload_state_dict\u003c/span\u003e(torch.\u003cspan class=\"hljs-title function_\"\u003eload\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e))\n                \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"디스크에서 모델 로드됨\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n            os.\u003cspan class=\"hljs-title function_\"\u003emakedirs\u003c/span\u003e(os.\u003cspan class=\"hljs-property\"\u003epath\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003edirname\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e우리는 모델 경로의 디렉토리가 존재하는지 os.path.exists(os.path.dirname(self.model_path))를 사용하여 확인합니다. 저장된 모델이 있으면, 훈련을 멈춘 지점부터 계속하기 위해 불러옵니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e os.\u003cspan class=\"hljs-property\"\u003epath\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eisfile\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e):\n    self.\u003cspan class=\"hljs-property\"\u003emain_network\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eload_state_dict\u003c/span\u003e(torch.\u003cspan class=\"hljs-title function_\"\u003eload\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e))\n    self.\u003cspan class=\"hljs-property\"\u003etarget_network\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eload_state_dict\u003c/span\u003e(torch.\u003cspan class=\"hljs-title function_\"\u003eload\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e))\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"디스크에서 모델을 불러왔습니다\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003etorch.load는 load_state_dict를 사용하여 저장된 모델 가중치를 메인 및 타겟 네트워크에 불러옵니다. 모델 디렉토리가 존재하지 않는 경우, os.makedirs를 사용하여 만듭니다.\u003c/p\u003e\n\u003ch2\u003e4.5: 딥 Q-네트워크 훈련\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다음으로, 우리는 DQN을 훈련하기 위한 학습 루프를 구현할 것입니다. 이 방법은 DQNTrainer 내에 이루어집니다. DQN을 위한 훈련 루프를 실행하며, 에이전트가 환경과 상호 작용하고 경험을 수집하며 네트워크를 업데이트하고 성능을 추적합니다.\u003c/p\u003e\n\u003cp\u003e다음은 학습 루프의 코드입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e(self, num_episodes, save=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e):\n    total_rewards = []\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e episode \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(num_episodes):\n        state, _ = self.\u003cspan class=\"hljs-property\"\u003eenv\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ereset\u003c/span\u003e()\n        done = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e\n        total_reward = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\n        \u003cspan class=\"hljs-keyword\"\u003ewhile\u003c/span\u003e not \u003cspan class=\"hljs-attr\"\u003edone\u003c/span\u003e:\n            self.\u003cspan class=\"hljs-property\"\u003eenv\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erender\u003c/span\u003e()  # 환경을 렌더링하기 위해 이 줄을 추가합니다\n            action = self.\u003cspan class=\"hljs-title function_\"\u003emain_network\u003c/span\u003e(torch.\u003cspan class=\"hljs-title class_\"\u003eFloatTensor\u003c/span\u003e(state).\u003cspan class=\"hljs-title function_\"\u003eunsqueeze\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)).\u003cspan class=\"hljs-title function_\"\u003eargmax\u003c/span\u003e(dim=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003eitem\u003c/span\u003e()\n            next_state, reward, done, _, _ = self.\u003cspan class=\"hljs-property\"\u003eenv\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003estep\u003c/span\u003e(action)\n            self.\u003cspan class=\"hljs-property\"\u003ereplay_buffer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003epush\u003c/span\u003e(state, action, reward, next_state, done)\n            state = next_state\n            total_reward += reward\n\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ereplay_buffer\u003c/span\u003e) \u003e= self.\u003cspan class=\"hljs-property\"\u003ebatch_size\u003c/span\u003e:\n                self.\u003cspan class=\"hljs-title function_\"\u003eupdate_network\u003c/span\u003e()\n\n        total_rewards.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(total_reward)\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"Episode {episode}, Total Reward: {total_reward}\"\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003esave\u003c/span\u003e:\n        torch.\u003cspan class=\"hljs-title function_\"\u003esave\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emain_network\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003estate_dict\u003c/span\u003e(), self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e)\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"모델을 디스크에 저장했습니다\"\u003c/span\u003e)\n\n    self.\u003cspan class=\"hljs-property\"\u003eenv\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eclose\u003c/span\u003e()\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esum\u003c/span\u003e(total_rewards) / \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(total_rewards)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003etrain 메서드는 지정된 에피소드 수에 대해 훈련 루프를 실행합니다. 이 루프는 에이전트가 경험을 쌓고 의사 결정 능력을 향상시키는 데 중요합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e4.5.1: 훈련 루프\n우선 total_rewards를 빈 리스트로 초기화해 봅시다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003etotal_rewards = [];\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 훈련 루프를 만들어 봅시다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e episode \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(num_episodes):\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 루프는 지정된 에피소드 수만큼 실행됩니다. 각 에피소드는 환경과의 완전한 상호작용 순서를 나타냅니다.\u003c/p\u003e\n\u003cp\u003e4.5.2: 환경 재설정\n각 에피소드의 시작 시점에는 환경이 초기 상태로 재설정됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003estate, (_ = self.\u003cspan class=\"hljs-property\"\u003eenv\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ereset\u003c/span\u003e());\ndone = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e;\ntotal_reward = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eself.env.reset()은 환경을 초기화하고 초기 상태를 반환합니다.\u003c/li\u003e\n\u003cli\u003edone = False는 에피소드가 완료되지 않았음을 나타냅니다.\u003c/li\u003e\n\u003cli\u003etotal_reward = 0은 현재 에피소드의 총 보상을 초기화합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch3\u003e4.4.3: Action Selection\u003c/h3\u003e\n\u003cp\u003e에이전트는 현재 상태를 기반으로 메인 네트워크를 사용하여 작업을 선택합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003eaction = self.main_network(torch.FloatTensor(state).unsqueeze(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)).argmax(dim=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e).item()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003etorch.FloatTensor(state).unsqueeze(0)은 상태를 PyTorch 텐서로 변환하고 네트워크가 예상하는 입력 형태와 일치하도록 추가 차원을 추가합니다.\u003c/p\u003e\n\u003cp\u003eself.main_network(...).argmax(dim=1).item()는 메인 네트워크가 예측한 가장 높은 Q 값으로 작업을 선택합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e4.5.4: 단계 및 저장 환경\n에이전트가 선택한 동작을 수행하고 보상 및 다음 상태를 관찰한 후, 해당 경험을 재생 버퍼에 저장합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003enext_state, reward, done, _, (_ = self.\u003cspan class=\"hljs-property\"\u003eenv\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003estep\u003c/span\u003e(action));\nself.\u003cspan class=\"hljs-property\"\u003ereplay_buffer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003epush\u003c/span\u003e(state, action, reward, next_state, done);\nstate = next_state;\ntotal_reward += reward;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eself.env.step(action)은 동작을 실행하고 다음 상태, 보상 및 에피소드 완료 여부를 반환합니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eself.replay_buffer.push(...)는 재생 버퍼에 경험을 저장합니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003estate = next_state는 현재 상태를 다음 상태로 업데이트합니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etotal_reward += reward은 현재 에피소드의 보상을 누적합니다.\u003c/p\u003e\n\u003cp\u003e4.5.5: 네트워크 업데이트\n재생 버퍼에 충분한 경험이 있을 경우, 네트워크가 업데이트됩니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ereplay_buffer\u003c/span\u003e) \u003e= self.\u003cspan class=\"hljs-property\"\u003ebatch_size\u003c/span\u003e:\n    self.\u003cspan class=\"hljs-title function_\"\u003eupdate_network\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003eif len(self.replay_buffer) \u003e= self.batch_size\u003c/code\u003e은 replay buffer가 적어도 batch_size의 경험을 가지고 있는지 확인합니다.\u003c/p\u003e\n\u003cp\u003eself.update_network()은 replay buffer에서 일괄적인 경험을 사용하여 네트워크를 업데이트합니다.\u003c/p\u003e\n\u003cp\u003e4.5.6: 에피소드 종료\n총 보상은 각 에피소드의 끝에서 기록되고 출력됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003etotal_rewards.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(total_reward)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"에피소드 {episode}, 총 보상: {total_reward}\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003etotal_rewards.append(total_reward)는 현재 에피소드의 총 보상을 총 보상 목록에 추가합니다.\u003c/p\u003e\n\u003cp\u003eprint(f\"에피소드 {episode}, 총 보상: {total_reward}\")은 에피소드 번호와 총 보상을 출력합니다.\u003c/p\u003e\n\u003cp\u003e4.5.7: 모델 저장\n훈련 후, 모델은 디스크에 저장됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003esave가 \u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e이면:\n   torch.\u003cspan class=\"hljs-title function_\"\u003esave\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emain_network\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003estate_dict\u003c/span\u003e(), self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e)\n   \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"모델이 디스크에 저장되었습니다.\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eif save:는 save 플래그가 True인지 확인합니다.\u003c/p\u003e\n\u003cp\u003etorch.save(self.main_network.state_dict(), self.model_path)는 메인 네트워크의 상태 딕셔너리를 지정된 파일 경로에 저장합니다.\u003c/p\u003e\n\u003cp\u003e4.5.8: 평균 보상 반환\n마지막으로, 이 메서드는 환경을 닫고 모든 에피소드에 대한 평균 보상을 반환합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eself.\u003cspan class=\"hljs-property\"\u003eenv\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eclose\u003c/span\u003e();\n\u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esum\u003c/span\u003e(total_rewards) / \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(total_rewards);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eself.env.close()는 환경을 닫습니다.\u003c/p\u003e\n\u003cp\u003ereturn sum(total_rewards) / len(total_rewards)는 평균 보상을 계산하고 반환합니다.\u003c/p\u003e\n\u003ch2\u003e4.6: 모델 튜닝\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e마침내 훈련된 모델을 평가하고 튜닝하는 방법을 살펴보겠습니다. DQN의 성능을 향상시키기 위해 하이퍼파라미터를 최적화할 책임을 가질 Optimizer 클래스를 만들어보겠습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOptimizer\u003c/span\u003e:\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, env, main_network, target_network, replay_buffer, model_path, params_path=\u003cspan class=\"hljs-string\"\u003e'params.pkl'\u003c/span\u003e):\n        self.\u003cspan class=\"hljs-property\"\u003eenv\u003c/span\u003e = env\n        self.\u003cspan class=\"hljs-property\"\u003emain_network\u003c/span\u003e = main_network\n        self.\u003cspan class=\"hljs-property\"\u003etarget_network\u003c/span\u003e = target_network\n        self.\u003cspan class=\"hljs-property\"\u003ereplay_buffer\u003c/span\u003e = replay_buffer\n        self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e = model_path\n        self.\u003cspan class=\"hljs-property\"\u003eparams_path\u003c/span\u003e = params_path\n\n    def \u003cspan class=\"hljs-title function_\"\u003eobjective\u003c/span\u003e(self, trial, n_episodes=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e):\n        lr = trial.\u003cspan class=\"hljs-title function_\"\u003esuggest_loguniform\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'lr'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1e-5\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1e-1\u003c/span\u003e)\n        gamma = trial.\u003cspan class=\"hljs-title function_\"\u003esuggest_uniform\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'gamma'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.9\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.999\u003c/span\u003e)\n        batch_size = trial.\u003cspan class=\"hljs-title function_\"\u003esuggest_categorical\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'batch_size'\u003c/span\u003e, [\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e])\n        target_update_frequency = trial.\u003cspan class=\"hljs-title function_\"\u003esuggest_categorical\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'target_update_frequency'\u003c/span\u003e, [\u003cspan class=\"hljs-number\"\u003e500\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2000\u003c/span\u003e])\n\n        optimizer = optim.\u003cspan class=\"hljs-title class_\"\u003eAdam\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emain_network\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eparameters\u003c/span\u003e(), lr=lr)\n        trainer = \u003cspan class=\"hljs-title class_\"\u003eDQNTrainer\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003eenv\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003emain_network\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003etarget_network\u003c/span\u003e, optimizer, self.\u003cspan class=\"hljs-property\"\u003ereplay_buffer\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e, gamma=gamma, batch_size=batch_size, target_update_frequency=target_update_frequency)\n        reward = trainer.\u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e(n_episodes, save=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e reward\n\n    def \u003cspan class=\"hljs-title function_\"\u003eoptimize\u003c/span\u003e(self, n_trials=\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, save_params=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e):\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e not \u003cspan class=\"hljs-variable constant_\"\u003eTRAIN\u003c/span\u003e and os.\u003cspan class=\"hljs-property\"\u003epath\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eisfile\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003eparams_path\u003c/span\u003e):\n            \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eopen\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003eparams_path\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'rb'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ef\u003c/span\u003e:\n                best_params = pickle.\u003cspan class=\"hljs-title function_\"\u003eload\u003c/span\u003e(f)\n            \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"디스크에서 매개변수를 불러왔습니다\"\u003c/span\u003e)\n        elif not \u003cspan class=\"hljs-attr\"\u003eFINETUNE\u003c/span\u003e:\n            best_params = {\n                \u003cspan class=\"hljs-string\"\u003e'lr'\u003c/span\u003e: \u003cspan class=\"hljs-variable constant_\"\u003eLEARNING_RATE\u003c/span\u003e,\n                \u003cspan class=\"hljs-string\"\u003e'gamma'\u003c/span\u003e: \u003cspan class=\"hljs-variable constant_\"\u003eGAMMA\u003c/span\u003e,\n                \u003cspan class=\"hljs-string\"\u003e'batch_size'\u003c/span\u003e: \u003cspan class=\"hljs-variable constant_\"\u003eBATCH_SIZE\u003c/span\u003e,\n                \u003cspan class=\"hljs-string\"\u003e'target_update_frequency'\u003c/span\u003e: \u003cspan class=\"hljs-variable constant_\"\u003eTARGET_UPDATE_FREQUENCY\u003c/span\u003e\n                }\n            \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"기본 매개변수 사용 중: {best_params}\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n            \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"하이퍼파라미터 최적화 중\"\u003c/span\u003e)\n            study = optuna.\u003cspan class=\"hljs-title function_\"\u003ecreate_study\u003c/span\u003e(direction=\u003cspan class=\"hljs-string\"\u003e'maximize'\u003c/span\u003e)\n            study.\u003cspan class=\"hljs-title function_\"\u003eoptimize\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003eobjective\u003c/span\u003e, n_trials=n_trials)\n            best_params = study.\u003cspan class=\"hljs-property\"\u003ebest_params\u003c/span\u003e\n\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003esave_params\u003c/span\u003e:\n                \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eopen\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003eparams_path\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'wb'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ef\u003c/span\u003e:\n                    pickle.\u003cspan class=\"hljs-title function_\"\u003edump\u003c/span\u003e(best_params, f)\n                \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"매개변수를 디스크에 저장했습니다\"\u003c/span\u003e)\n\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e best_params\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e4.6.1: 클래스 정의\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOptimizer\u003c/span\u003e:\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, env, main_network, target_network, replay_buffer, model_path, params_path=\u003cspan class=\"hljs-string\"\u003e'params.pkl'\u003c/span\u003e):\n        self.\u003cspan class=\"hljs-property\"\u003eenv\u003c/span\u003e = env\n        self.\u003cspan class=\"hljs-property\"\u003emain_network\u003c/span\u003e = main_network\n        self.\u003cspan class=\"hljs-property\"\u003etarget_network\u003c/span\u003e = target_network\n        self.\u003cspan class=\"hljs-property\"\u003ereplay_buffer\u003c/span\u003e = replay_buffer\n        self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e = model_path\n        self.\u003cspan class=\"hljs-property\"\u003eparams_path\u003c/span\u003e = params_path\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003einit\u003c/strong\u003e 메서드는 최적화에 필요한 다양한 구성 요소를 초기화합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eenv: 에이전트가 작동하는 환경.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003emain_network: 주요 신경망.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etarget_network: 타겟 신경망.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ereplay_buffer: 경험을 저장하고 샘플링하는 버퍼.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003emodel_path: 훈련된 모델을 저장하거나 불러오는 경로.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eparams_path: 최적의 하이퍼파라미터를 저장하거나 불러오는 경로.\u003c/p\u003e\n\u003cp\u003e4.6.2: 목적 메서드\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eobjective\u003c/span\u003e(self, trial, n_episodes=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e):\n        lr = trial.\u003cspan class=\"hljs-title function_\"\u003esuggest_loguniform\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'lr'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1e-5\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1e-1\u003c/span\u003e)\n        gamma = trial.\u003cspan class=\"hljs-title function_\"\u003esuggest_uniform\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'gamma'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.9\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.999\u003c/span\u003e)\n        batch_size = trial.\u003cspan class=\"hljs-title function_\"\u003esuggest_categorical\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'batch_size'\u003c/span\u003e, [\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e])\n        target_update_frequency = trial.\u003cspan class=\"hljs-title function_\"\u003esuggest_categorical\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'target_update_frequency'\u003c/span\u003e, [\u003cspan class=\"hljs-number\"\u003e500\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2000\u003c/span\u003e])\n\n        optimizer = optim.\u003cspan class=\"hljs-title class_\"\u003eAdam\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emain_network\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eparameters\u003c/span\u003e(), lr=lr)\n        trainer = \u003cspan class=\"hljs-title class_\"\u003eDQNTrainer\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003eenv\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003emain_network\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003etarget_network\u003c/span\u003e, optimizer, self.\u003cspan class=\"hljs-property\"\u003ereplay_buffer\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e, gamma=gamma, batch_size=batch_size, target_update_frequency=target_update_frequency)\n        reward = trainer.\u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e(n_episodes, save=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e reward\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e목표 함수는 하이퍼파라미터에 대한 값을 제안하고 이러한 값을 사용하여 모델을 훈련합니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elr = trial.suggest_loguniform(\u003ccode\u003elr\u003c/code\u003e, 1e-5, 1e-1): 범위 [1e-5, 1e-1] 내의 학습률을 제안합니다.\u003c/li\u003e\n\u003cli\u003egamma = trial.suggest_uniform(\u003ccode\u003egamma\u003c/code\u003e, 0.9, 0.999): 범위 [0.9, 0.999] 내의 할인 요인을 제안합니다.\u003c/li\u003e\n\u003cli\u003ebatch_size = trial.suggest_categorical(\u003ccode\u003ebatch_size\u003c/code\u003e, [32, 64, 128]): 지정된 목록에서 배치 크기를 제안합니다.\u003c/li\u003e\n\u003cli\u003etarget_update_frequency = trial.suggest_categorical(\u003ccode\u003etarget_update_frequency\u003c/code\u003e, [500, 1000, 2000]): 지정된 목록에서 대상 업데이트 빈도를 제안합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eoptimizer = optim.\u003cspan class=\"hljs-title class_\"\u003eAdam\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003emain_network\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eparameters\u003c/span\u003e(), (lr = lr));\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e여기서는 주어진 학습률로 Adam 옵티마이저를 설정합니다. Adam은 주로 신경망 훈련에 사용되는 최적화 알고리즘인 Adaptive Moment Estimation의 약자입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e신경망에서 각 매개변수에 대해 Adam은 손실 함수의 기울기를 계산합니다. 그것은 기울기의 지수 이동 평균 (첫 번째 모멘트로 표시되는 m)과 제곱 기울기 (두 번째 모멘트로 표시되는 v)를 추적합니다.\u003c/p\u003e\n\u003cp\u003e이동 평균의 초기화 편향을 고려하기 위해 Adam은 첫 번째 및 두 번째 모멘트 추정치에 바이어스 보정을 적용합니다. 그런 다음 매개변수는 수정된 첫 번째 및 두 번째 모멘트를 사용하여 업데이트됩니다. 업데이트 규칙은 학습률과 모멘트를 통합하여 기울기의 크기와 방향을 모두 고려하는 방식으로 매개변수를 조정합니다.\u003c/p\u003e\n\u003cp\u003e다음은 Adam에 대한 보다 포괄적인 기사입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003etrainer = \u003cspan class=\"hljs-title class_\"\u003eDQNTrainer\u003c/span\u003e(\n  self.\u003cspan class=\"hljs-property\"\u003eenv\u003c/span\u003e,\n  self.\u003cspan class=\"hljs-property\"\u003emain_network\u003c/span\u003e,\n  self.\u003cspan class=\"hljs-property\"\u003etarget_network\u003c/span\u003e,\n  optimizer,\n  self.\u003cspan class=\"hljs-property\"\u003ereplay_buffer\u003c/span\u003e,\n  self.\u003cspan class=\"hljs-property\"\u003emodel_path\u003c/span\u003e,\n  (gamma = gamma),\n  (batch_size = batch_size),\n  (target_update_frequency = target_update_frequency)\n);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 코드는 제안된 하이퍼파라미터로 theDQNTrainer 인스턴스를 초기화합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ereward = trainer.\u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e(n_episodes, (save = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e));\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e마지막으로, 이 코드는 지정된 에피소드 수로 모델을 학습하고 평균 보상을 반환합니다.\u003c/p\u003e\n\u003cp\u003e4.6.3: 최적화 메소드\n이 섹션에서는 모델의 성능을 극대화하는 조합을 효율적으로 찾을 수 있도록 하이퍼파라미터 공간을 체계적으로 탐색하는 파이썬 라이브러리인 Optuna를 사용하겠습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eoptimize\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, n_trials=\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, save_params=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e TRAIN \u003cspan class=\"hljs-keyword\"\u003eand\u003c/span\u003e os.path.isfile(self.params_path):\n        \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eopen\u003c/span\u003e(self.params_path, \u003cspan class=\"hljs-string\"\u003e'rb'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e f:\n            best_params = pickle.load(f)\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"디스크에서 매개변수를 불러왔습니다.\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e FINETUNE:\n        best_params = {\n            \u003cspan class=\"hljs-string\"\u003e'lr'\u003c/span\u003e: LEARNING_RATE,\n            \u003cspan class=\"hljs-string\"\u003e'gamma'\u003c/span\u003e: GAMMA,\n            \u003cspan class=\"hljs-string\"\u003e'batch_size'\u003c/span\u003e: BATCH_SIZE,\n            \u003cspan class=\"hljs-string\"\u003e'target_update_frequency'\u003c/span\u003e: TARGET_UPDATE_FREQUENCY\n        }\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"기본 매개변수를 사용합니다: \u003cspan class=\"hljs-subst\"\u003e{best_params}\u003c/span\u003e\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"하이퍼파라미터를 최적화 중입니다.\"\u003c/span\u003e)\n        study = optuna.create_study(direction=\u003cspan class=\"hljs-string\"\u003e'maximize'\u003c/span\u003e)\n        study.optimize(self.objective, n_trials=n_trials)\n        best_params = study.best_params\n\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e save_params:\n            \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eopen\u003c/span\u003e(self.params_path, \u003cspan class=\"hljs-string\"\u003e'wb'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e f:\n                pickle.dump(best_params, f)\n            \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"매개변수를 디스크에 저장했습니다.\"\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e best_params\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003eoptimize\u003c/code\u003e 메소드는 지정된 횟수의 시도에 대해 최적화 프로세스를 실행합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e TRAIN \u003cspan class=\"hljs-keyword\"\u003eand\u003c/span\u003e os.path.isfile(self.params_path):\n        \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eopen\u003c/span\u003e(self.params_path, \u003cspan class=\"hljs-string\"\u003e'rb'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e f:\n            best_params = pickle.load(f)\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"디스크에서 매개변수를 불러왔습니다.\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e학습이 필요하지 않은 경우 (TRAIN이 아닌 경우) 및 매개변수 파일이 존재하는 경우, 매개변수가 디스크에서 로드됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e FINETUNE:\n    best_params = {\n        \u003cspan class=\"hljs-string\"\u003e'lr'\u003c/span\u003e: LEARNING_RATE,\n        \u003cspan class=\"hljs-string\"\u003e'gamma'\u003c/span\u003e: GAMMA,\n        \u003cspan class=\"hljs-string\"\u003e'batch_size'\u003c/span\u003e: BATCH_SIZE,\n        \u003cspan class=\"hljs-string\"\u003e'target_update_frequency'\u003c/span\u003e: TARGET_UPDATE_FREQUENCY\n    }\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"기본 매개변수 사용 중: \u003cspan class=\"hljs-subst\"\u003e{best_params}\u003c/span\u003e\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e만약 파라미터 튜닝이 필요하지 않다면 (not FINETUNE), 기본 매개변수가 사용됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"하이퍼파라미터 최적화 중\"\u003c/span\u003e)\n    study = optuna.create_study(direction=\u003cspan class=\"hljs-string\"\u003e'maximize'\u003c/span\u003e)\n    study.optimize(self.objective, n_trials=n_trials)\n    best_params = study.best_params\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e save_params:\n        \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eopen\u003c/span\u003e(self.params_path, \u003cspan class=\"hljs-string\"\u003e'wb'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e f:\n            pickle.dump(best_params, f)\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"매개변수를 디스크에 저장했습니다\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e하이퍼파라미터 최적화가 필요한 경우, Optuna를 사용하여 최적의 매개변수를 찾습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003estudy = optuna.create_study(direction='maximize')을 사용하면 목적 함수를 최대화하는 Optuna 스터디를 생성할 수 있어요.\u003c/p\u003e\n\u003cp\u003estudy.optimize(self.objective, n_trials=n_trials)은 지정된 횟수의 시행을 위한 최적화를 실행해요.\u003c/p\u003e\n\u003cp\u003esave_params를 True로 설정하면, 최적의 매개변수가 디스크에 저장돼요.\u003c/p\u003e\n\u003cp\u003e다음은 Optuna를 깊이 들여다보는 포함한 다양한 세밀 조정 기법을 탐구한 멋진 기사에요:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e4.7: 모델 실행하기\u003c/h2\u003e\n\u003cp\u003e마지막으로, 모든 과정을 다시 한번 확인하고 코드를 실행해 봅시다!\u003c/p\u003e\n\u003cp\u003e4.7.1: 훈련 및 파인튜닝 설정\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-variable constant_\"\u003eTRAIN\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eFINETUNE\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e\n\n# 다음 하이퍼파라미터를 설정하세요 (\u003cspan class=\"hljs-variable constant_\"\u003eFINETUNE\u003c/span\u003e이 \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e인 경우)\n\u003cspan class=\"hljs-variable constant_\"\u003eGAMMA\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0.99\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eBATCH_SIZE\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eTARGET_UPDATE_FREQUENCY\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eLEARNING_RATE\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e1e-3\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eTRAIN = True은 모델을 학습할지 여부를 나타냅니다. False로 설정하면 학습이 건너뛰어집니다.\u003c/p\u003e\n\u003cp\u003eFINETUNE = False는 모델을 fine-tune할지 여부를 나타냅니다. True로 설정하면 기존 매개변수가 사용되고 fine-tune됩니다.\u003c/p\u003e\n\u003cp\u003e만약 FINETUNE이 False인 경우, 다음 하이퍼파라미터를 설정합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGAMMA = 0.99: 미래 보상에 대한 할인 계수입니다. 이는 즉시 보상에 비해 미래 보상이 얼마나 중요한지를 결정합니다.\u003c/li\u003e\n\u003cli\u003eBATCH_SIZE = 64: 각 학습 단계마다 재생 버퍼에서 샘플링된 경험의 수입니다.\u003c/li\u003e\n\u003cli\u003eTARGET_UPDATE_FREQUENCY = 1000: 타겟 네트워크의 가중치가 주요 네트워크의 가중치와 일치하도록 업데이트되는 빈도(스텝 단위).\u003c/li\u003e\n\u003cli\u003eLEARNING_RATE = 1e-3: 최적화기(optimizer)의 학습률로, 모델 가중치가 업데이트될 때 추정 오차에 따라 모델을 얼마나 변경할지를 제어합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e4.7.2: 네트워크 및 재생 버퍼 초기화\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003emain_network = \u003cspan class=\"hljs-title function_\"\u003eDQN\u003c/span\u003e(state_dim, action_dim);\ntarget_network = \u003cspan class=\"hljs-title function_\"\u003eDQN\u003c/span\u003e(state_dim, action_dim);\ntarget_network.\u003cspan class=\"hljs-title function_\"\u003eload_state_dict\u003c/span\u003e(main_network.\u003cspan class=\"hljs-title function_\"\u003estate_dict\u003c/span\u003e());\ntarget_network.\u003cspan class=\"hljs-built_in\"\u003eeval\u003c/span\u003e();\n\nreplay_buffer = \u003cspan class=\"hljs-title class_\"\u003eReplayBuffer\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e10000\u003c/span\u003e);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003emain_network = DQN(state_dim, action_dim)은 지정된 상태 및 액션 차원으로 메인 네트워크를 초기화합니다.\u003c/p\u003e\n\u003cp\u003etarget_network = DQN(state_dim, action_dim)은 메인 네트워크와 동일한 구조로 대상 네트워크를 초기화합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003etarget_network.load_state_dict(main_network.state_dict()) 함수는 메인 네트워크의 가중치를 타겼 네트워크로 복사합니다.\u003c/p\u003e\n\u003cp\u003etarget_network.eval() 함수는 타겟 네트워크를 평가 모드로 설정합니다. 이는 추론 중에 드롭아웃과 배치 정규화와 같은 특정 레이어가 적절하게 동작하도록 합니다.\u003c/p\u003e\n\u003cp\u003ereplay_buffer = ReplayBuffer(10000)은 10,000개의 경험을 저장할 수 있는 용량을 가진 리플레이 버퍼를 초기화합니다.\u003c/p\u003e\n\u003cp\u003e4.7.3: 단계 카운트 설정\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-variable constant_\"\u003eSTEP_COUNT\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSTEP_COUNT = 0은 훈련 중 취한 단계 수를 추적하는 카운터를 초기화합니다.\u003c/p\u003e\n\u003cp\u003e4.7.4: 옵티마이저 초기화 및 하이퍼파라미터 최적화\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eoptimizer = \u003cspan class=\"hljs-title class_\"\u003eOptimizer\u003c/span\u003e(env, main_network, target_network, replay_buffer, f\u003cspan class=\"hljs-string\"\u003e'{os.path.dirname(__file__)}/model/model.pth'\u003c/span\u003e, f\u003cspan class=\"hljs-string\"\u003e'{os.path.dirname(__file__)}/model/params.pkl'\u003c/span\u003e)\nbest_params = optimizer.\u003cspan class=\"hljs-title function_\"\u003eoptimize\u003c/span\u003e(n_trials=\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, save_params=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003ccode\u003eoptimizer = Optimizer(...)\u003c/code\u003e은 환경, 네트워크, 리플레이 버퍼, 모델 경로 및 매개변수 경로로 Optimizer 클래스를 초기화합니다.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ebest_params = optimizer.optimize(n_trials=2, save_params=True)\u003c/code\u003e는 최적의 하이퍼파라미터를 찾기 위해 최적화 프로세스를 실행합니다. 이 함수는 다음과 같은 기능을 수행합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e지정된 횟수(n_trials=2)만큼 최적화를 실행합니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003esave_params\u003c/code\u003e가 True인 경우 최적의 하이퍼파라미터를 디스크에 저장합니다.\u003c/p\u003e\n\u003cp\u003e4.7.5: PyTorch Optimizer 및 DQN Trainer 생성\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eoptimizer = optim.\u003cspan class=\"hljs-title class_\"\u003eAdam\u003c/span\u003e(main_network.\u003cspan class=\"hljs-title function_\"\u003eparameters\u003c/span\u003e(), lr=best_params[\u003cspan class=\"hljs-string\"\u003e'lr'\u003c/span\u003e])\ntrainer = \u003cspan class=\"hljs-title class_\"\u003eDQNTrainer\u003c/span\u003e(env, main_network, target_network, optimizer, replay_buffer, f\u003cspan class=\"hljs-string\"\u003e'{os.path.dirname(__file__)}/model/model.pth'\u003c/span\u003e, gamma=best_params[\u003cspan class=\"hljs-string\"\u003e'gamma'\u003c/span\u003e], batch_size=best_params[\u003cspan class=\"hljs-string\"\u003e'batch_size'\u003c/span\u003e], target_update_frequency=best_params[\u003cspan class=\"hljs-string\"\u003e'target_update_frequency'\u003c/span\u003e])\ntrainer.\u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003eoptimizer = optim.Adam(main_network.parameters(), lr=best_params['lr'])\u003c/code\u003e은 최적의 하이퍼파라미터에서 학습률을 사용하여 Adam 옵티마이저를 생성합니다.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003etrainer = DQNTrainer(...)\u003c/code\u003e는 환경, 네트워크, 옵티마이저, 리플레이 버퍼, 모델 경로 및 최적의 하이퍼파라미터로 DQNTrainer 클래스를 초기화합니다.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003etrainer.train(1000)\u003c/code\u003e은 모델을 1000번의 에피소드 동안 훈련합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제 요정의 훈련 초기 10 에피소드를 살펴보겠습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1200/1*ncnLXIRABedg4uKwVL0L5w.gif\" alt=\"agent training\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서 모델은 서툴러서 무작위로 종종 비최적적인 결정을 내립니다. 요정이 환경을 탐험하고 기초를 배우기 때문에 이는 예상되는 현상입니다. 아직 보상을 극대화하기 위한 견고한 전략을 개발하지 못했습니다. 추가적인 훈련 에피소드를 거치면서 시간이 지남에 따라, 요정의 성능은 정책을 미세 조정하고 경험을 통해 배우면서 크게 향상되어야 합니다.\u003c/p\u003e\n\u003cp\u003e이제 모델이 1000번 훈련된 후의 10개의 훈련 에피소드를 살펴봅시다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1200/1*s4j6V4V-nLfkc18C2Z2-zA.gif\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e이것은 주목할 만한 개선입니다. 모델이 아직 NASA에 완성되지는 않았지만, 몇 가지 주요 향상 사항을 관찰할 수 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e에이전트가 더 신중하고 전략적인 결정을 내립니다.\u003c/li\u003e\n\u003cli\u003e환경을 더 효율적으로 탐색합니다.\u003c/li\u003e\n\u003cli\u003e부적절한 조치의 빈도가 크게 감소했습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e지속적인 훈련과 세밀한 조정을 통해, 에이전트의 성능은 더 개선될 것으로 예상되며, 최적의 행동에 더 가까워질 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제 여러분의 차례입니다. 모델을 더 발전시켜 보세요. 하이퍼파라미터를 조정하거나 다른 모델 구조를 실험해 보세요. 창의성과 인내심으로 최선을 다하면 얼마든지 성과를 낼 수 있을 거에요. 곧 완벽하게 패치된 셔틀은 원활하게 착륙할 거예요!\u003c/p\u003e\n\u003ch1\u003e5: 결론\u003c/h1\u003e\n\u003cp\u003e딥 Q-네트워크를 구축, 훈련 및 평가하는 방법을 잘 이해하셨으니, 이제 다양한 환경에서 이 DQN을 테스트하고 다양한 도전에 적응하는 모습을 관찰해 보세요.\u003c/p\u003e\n\u003cp\u003e에이전트의 성능을 향상시키기 위해 고급 기술을 구현하고 새로운 아키텍처를 탐험해 보세요. 예를 들어 다양한 하이퍼파라미터를 설정해보거나 다른 최적화 알고리즘(예: SGD 또는 Nadam)을 사용하거나 다른 미세조정 알고리즘을 사용해 볼 수 있어요!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e참고 자료\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eSutton, R. S., \u0026#x26; Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT Press.\u003c/li\u003e\n\u003cli\u003eLin, L. J. (1992). “Self-improving reactive agents based on reinforcement learning, planning and teaching.” Machine Learning, 8(3–4), 293–321.\u003c/li\u003e\n\u003cli\u003eOpenAI. “LunarLander-v2.” OpenAI Gym. \u003ca href=\"https://gym.openai.com/envs/LunarLander-v2/\" rel=\"nofollow\" target=\"_blank\"\u003e링크\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e버클리 AI 연구소 (BAIR). “Experience Replay.” \u003ca href=\"https://bair.berkeley.edu/blog/2020/03/20/experiencereplay/\" rel=\"nofollow\" target=\"_blank\"\u003e링크\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eTowards Data Science. “Reinforcement Learning 101: Q-Learning.” \u003ca href=\"https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292\" rel=\"nofollow\" target=\"_blank\"\u003e링크\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eTowards Data Science. “신경망 뒤의 수학.” \u003ca href=\"https://towardsdatascience.com/the-math-behind-neural-networks-3a18b7f8d8dc\" rel=\"nofollow\" target=\"_blank\"\u003e링크\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eTowards Data Science. “Adam Optimizer 뒤의 수학.” \u003ca href=\"https://towardsdatascience.com/the-math-behind-adam-optimizer-3a18b7f8d8dc\" rel=\"nofollow\" target=\"_blank\"\u003e링크\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eTowards Data Science. “Deep Neural Networks 맞춤화 뒤의 수학.” \u003ca href=\"https://towardsdatascience.com/the-math-behind-fine-tuning-deep-neural-networks-3a18b7f8d8dc\" rel=\"nofollow\" target=\"_blank\"\u003e링크\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e마지막까지 읽어 주셔서 축하드립니다! 이 기사가 유익하고 즐거우셨기를 바랍니다. 만약 그렇다면, 박수를 남기고 더 많은 이런 기사를 보고 싶다면 저를 팔로우해 주세요. 앞으로 다루었으면 하는 주제나 기사에 대한 의견을 들을 수 있습니다. 피드백과 지원에 감사드립니다. 읽어 주셔서 감사합니다!\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-27-ReinforcementLearningDeepQ-Networks"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>