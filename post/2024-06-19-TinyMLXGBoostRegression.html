<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>작은 머신러닝  XGBoost 회귀 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-TinyMLXGBoostRegression" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="작은 머신러닝  XGBoost 회귀 | itposting" data-gatsby-head="true"/><meta property="og:title" content="작은 머신러닝  XGBoost 회귀 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-TinyMLXGBoostRegression_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-TinyMLXGBoostRegression" data-gatsby-head="true"/><meta name="twitter:title" content="작은 머신러닝  XGBoost 회귀 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-TinyMLXGBoostRegression_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 05:58" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">작은 머신러닝  XGBoost 회귀</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="작은 머신러닝  XGBoost 회귀" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">14<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-TinyMLXGBoostRegression&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>수학적 기초부터 엣지 구현까지</p>
<h1>소셜 미디어:</h1>
<p>👨🏽‍💻 Github: thommaskevin/TinyML (github.com)
👷🏾 Linkedin: Thommas Kevin | LinkedIn
📽 Youtube: Thommas Kevin — YouTube
👨🏻‍🏫 연구 그룹: Conecta.ai (ufrn.br)</p>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_0.png" alt="이미지"></p>
<div class="content-ad"></div>
<h2>요약</h2>
<h3>1 - XGBoost 회귀 이론</h3>
<p>보완적으로, 부스팅은 일련의 모델 집합 𝑡가 순차적으로 훈련되는 앙상블 접근 방식을 나타냅니다. 각 모델 𝑡는 이전 모델, 𝑡−1에서 발견된 결함을 보정하는 목적으로 설계되었습니다.</p>
<p>타겟 값 yᵢ와 샘플 xᵢ에 대한 모델 𝑡의 예측 ŷᵢᵗ을 고려하고, 평균 제곱 오차 (MSE) 등의 일반적인 오류 함수 l로 나타내고, 총 샘플 수를 n으로 표시할 때, 반복 t에서의 모델의 오류(또는 손실)는 다음과 같이 정의됩니다:</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_1.png">
<p>모델이 단계적으로 구축되었다는 것을 관찰할 수 있습니다. t 단계에서의 예측은 t-1 단계에서의 예측에 새 모델 fₜ의 예측을 더한 결과입니다:</p>
<img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_2.png">
<p>우리는 모델의 복잡성을 조절하는 데 기여하는 정규화항을 도입할 것이며(나중에 이 항의 구체적인 기능이 명확해질 것입니다).</p>
<div class="content-ad"></div>
<p>XGBoost의 기본 개념은 각 트리의 포함이 전략적이라는 것을 전제로 합니다: 목표는 항상 오차를 최소화하는 최적의 트리를 구축하는 것입니다. 이를 위해, 우리는 함수 L을 최적화 문제로 다룰 것이며, 결국 L을 최소화하는 fₜ를 결정하려고 합니다. 그러나 이 작업의 복잡성은 오차 함수 l을 선택하는 데 따라 다를 수 있습니다.</p>
<p>따라서 우리는 이 함수를 Taylor 전개를 통해 간소화하기로 결정했습니다. 어떤 무한 차별화 가능한 함수도 다음 형식으로 표현할 수 있다는 것이 널리 인정받았습니다:</p>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_3.png" alt="수식"></p>
<p>중간 단계에서 시리즈를 자르면 함수의 근사치를 얻을 수 있습니다. 현재 상황에서는 확장을 둘째 차수에서 중지하기로 선택했습니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_4.png" alt="image1"></p>
<p>gᵢ (gradient)와 hᵢ (Hessian)로 도함수를 대체할 것입니다:</p>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_5.png" alt="image2"></p>
<p>만약 이 방정식을 최소화하는 fₜ를 찾는 것이 목적이라면, 상수항인 l은 필요하지 않습니다. 따라서 l을 버리면 다음과 같이 됩니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_6.png" alt="링크 텍스트"></p>
<p>XGBoost의 주목할만한 특성 중 하나는 손실 함수가 두 번 미분 가능해야 한다는 요구사항입니다. 특정 문제에 대해 사용자 정의 오류 함수를 이용하여 XGBoost를 적용하려는 경우, 오류 계산 뿐만 아니라 그레이디언트(일차 도함수) 및 헤시안(이차 도함수)에 대한 정보도 필요하다는 점을 명심하는 것이 중요합니다.</p>
<h2>1.1 — 의사 결정 트리</h2>
<p>의사 결정 트리의 작동을 고려할 때, 방정식 L을 다시 쓸 필요가 있습니다. 각 샘플 xᵢ가 leaf j와 연관되어 있음을 알 수 있습니다. 따라서 각 leaf에 대해 샘플이 포함된 집합 인덱스 Iⱼ를 만들 수 있습니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_7.png" alt="image"></p>
<p>Iⱼ가 정의되어 있으며, Iⱼ에 속하는 각 인덱스 i에 대해 샘플 xᵢ가 통과한 결정 경로 q는 잎 j로 이어진다.</p>
<p>또한, 모델이 샘플 xᵢ에 대해 응답하는 것이 xᵢ가 속한 잎에서 관련된 가중치임을 알 수 있습니다:</p>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_8.png" alt="image"></p>
<div class="content-ad"></div>
<p>그 결과, 방정식의 일부 용어를 다시 정의할 수 있습니다:</p>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_9.png" alt="image"></p>
<p>대체를 수행하면 다음과 같이 얻을 수 있습니다:</p>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_10.png" alt="image"></p>
<div class="content-ad"></div>
<p>정규화 항도 확장할 거에요:</p>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_11.png" alt=""></p>
<h2>1.2 — 예측 오류 최적화</h2>
<p>나무의 모든 리프를 고려하는 대신에, 특정 리프에 초점을 맞출 거에요. 이 리프는 j로 표시돼요.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_12.png" alt="Image 1"></p>
<p>The objective is to find the set of weights w that minimizes L. This may seem challenging at first glance, but let’s analyze it more closely.</p>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_13.png" alt="Image 2"></p>
<p>As previously noted, our error function for a leaf is quadratic, implying that the minimum is determined by the inflection point of the curve, where the first derivative is equal to zero.</p>
<div class="content-ad"></div>
<table>
  <tbody><tr>
    <td><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_14.png"></td>
  </tr>
</tbody></table>
<p>wᵈ를 고립시키면 다음과 같이 됩니다:</p>
<table>
  <tbody><tr>
    <td><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_15.png"></td>
  </tr>
</tbody></table>
<p>이제 임의의 리프에 대해 최적 가중치를 제공하는 식을 확인했습니다. 따라서 L에 대한 우리의 식에 이 식을 대입함으로써 우리는 다음을 얻습니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_16.png" alt="image"></p>
<p>이전 방정식은 각 새 트리의 각 분리를 평가하는 데 사용됩니다. 엔트로피나 지니 계수가 전통적으로 의사결정 트리 구성에 사용되는 방법과 마찬가지로 분리에서 양쪽 노드인 왼쪽 노드와 오른쪽 노드가 생성됩니다. 분할별 이득은 새로운 리프인 Lₗ(왼쪽)과 Lᵣ(오른쪽)의 합을 이전 오차인 Lₜ에서 뺀 것으로 정의됩니다. (우리가 Leaf가 하나만 분석하므로 T=1이라고 가정합니다.)</p>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_17.png" alt="image"></p>
<div class="content-ad"></div>
<h2>1.3 — 하이퍼파라미터 튜닝</h2>
<p>이러한 방정식을 이해하면 XGBoost의 하이퍼파라미터와 기능을 더 잘 이해할 수 있습니다.</p>
<p>reg_lambda: 이 파라미터는 잎의 가중치에 영향을 미치며, 값이 클수록 가중치의 절대값이 작아집니다. 이러한 이유로 𝜆은 모델의 복잡성을 제어하는 매개변수로, 가중치가 너무 커지는 것을 방지합니다. 보다 정확히는 L2 정규화입니다.</p>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_18.png" alt="이미지"></p>
<div class="content-ad"></div>
<ul>
<li>reg_alpha: 분모를 제로에 가깝게 만들어서 중요성이 적은 트리 또는 분할을 제외하는 효과가 있습니다. 유도된 값의 모듈리(0보다 작을 때 -1, 0보다 클 때 1)의 행동으로 인해 가중 함수가 두 가지 경우로 나누어짐을 언급해야 합니다.</li>
</ul>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_19.png" alt="image"></p>
<ul>
<li>gamma: 𝛾는 분할이 발생하는 최솟값으로, 𝛾보다 낮은 값은 결과적으로 부정적인 이득이 발생하여 실제 결과를 악화시킬 수 있으므로 고려되지 않습니다.</li>
</ul>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_20.png" alt="image"></p>
<div class="content-ad"></div>
<ul>
<li>learning_rate: 문 개선을 위해 각 가중치에 0에서 1 사이의 값을 곱하여 나무의 개별적인 중요성을 감소시키고 학습 과정을 늦춰 미래 나무의 포함 여지를 늘립니다.</li>
</ul>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_21.png" alt="image"></p>
<p>여기서 η는 트리 ft의 전체 예측에 미치는 영향을 직접 조절하며 가중치 계산 방식을 수정하지 않습니다.</p>
<ul>
<li>max_delta_step: 각 반복의 최대 절대 가중치를 상수 𝛿로 제한하여 가중치의 부호를 항상 보존합니다.</li>
</ul>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_22.png" alt="Image 1"></p>
<ul>
<li>max_child_weight: 자식 노드마다 ℎ의 합이 이 매개변수로 설정된 값보다 크기 때문에 분할이 수행됩니다. ℎ는 오차 함수(𝑙)의 도함수에 의해 결정됩니다. 따라서 ℎ의 값이 낮을 때는 해당 리프가 이미 충분히 "순수"하며 더 이상 분할할 필요가 없다는 것을 나타냅니다.</li>
</ul>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_23.png" alt="Image 2"></p>
<p>여기서 Python 구현에 사용 가능한 매개변수 전체 목록을 찾을 수 있습니다.</p>
<div class="content-ad"></div>
<h1>2— TinyML 구현</h1>
<p>위 예제를 통해 ESP32, Arduino, Raspberry 및 기타 다양한 마이크로컨트롤러 또는 IoT 장치에서 머신러닝 알고리즘을 구현할 수 있습니다.</p>
<p>2.0 — requirements.txt 파일에 나열된 라이브러리 설치</p>
<pre><code class="hljs language-js">!pip install -r requirements.<span class="hljs-property">txt</span>
</code></pre>
<div class="content-ad"></div>
<p>2.1 — 라이브러리 가져오기</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> sklearn.<span class="hljs-property">datasets</span> <span class="hljs-keyword">import</span> load_diabetes
<span class="hljs-keyword">from</span> sklearn.<span class="hljs-property">model_selection</span> <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn.<span class="hljs-property">metrics</span> <span class="hljs-keyword">import</span> mean_squared_error
<span class="hljs-keyword">from</span> sklearn.<span class="hljs-property">model_selection</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">RandomizedSearchCV</span>
<span class="hljs-keyword">import</span> m2cgen <span class="hljs-keyword">as</span> m2c
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> scipy.<span class="hljs-property">stats</span> <span class="hljs-keyword">import</span> uniform, randint
<span class="hljs-keyword">import</span> matplotlib.<span class="hljs-property">pyplot</span> <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> xgboost <span class="hljs-keyword">as</span> xgb
<span class="hljs-keyword">from</span> xgboost <span class="hljs-keyword">import</span> plot_tree
</code></pre>
<p>2.2— 데이터셋 로드</p>
<p>당뇨병 데이터셋은 Bradley Efron, Trevor Hastie, Iain Johnstone 및 Robert Tibshirani가 스탠포드 대학에서 만들었습니다. 그들의 당뇨병 진행 예측 연구에 사용되었습니다.</p>
<div class="content-ad"></div>
<ul>
<li>데이터셋은 임상 및 인구 통계 변수인 열 개의 기준 변수로 구성되어 있습니다:</li>
</ul>
<ol>
<li>
<p>나이</p>
</li>
<li>
<p>성별</p>
</li>
<li>
<p>체질량 지수 (BMI)</p>
</li>
</ol>
<div class="content-ad"></div>
<ol start="4">
<li>
<p>평균 혈압</p>
</li>
<li>
<p>S1 — TC, T-세포 (백혈구의 일종)</p>
</li>
<li>
<p>S2 — LDL, 저밀도 리포닛</p>
</li>
<li>
<p>S3 — HDL, 고밀도 리포닛</p>
</li>
</ol>
<div class="content-ad"></div>
<ol start="8">
<li>
<p>S4 - TCH, 총 콜레스테롤</p>
</li>
<li>
<p>S5 - LTG, 혈청 트리글리세리드 수준의 로그 가능성</p>
</li>
<li>
<p>S6 - 포도당, 혈당 수준</p>
</li>
</ol>
<ul>
<li>데이터셋에는 442개의 인스턴스(환자)가 있습니다.</li>
</ul>
<div class="content-ad"></div>
<ul>
<li>대상 변수는 기준선 이후 1년 후의 질병 진행의 양을 양적으로 측정한 것입니다. 데이터 집합에 명시적으로 언급되지 않은 요소를 기반으로 질병 진행을 표현합니다. 이는 연속 변수입니다.</li>
</ul>
<pre><code class="hljs language-python"><span class="hljs-comment"># 데이터셋 불러오기</span>
data = load_diabetes() <span class="hljs-comment"># 데이터 불러오기</span>

<span class="hljs-comment"># DataFrame 생성</span>
df_diabetes = pd.DataFrame(data.data, columns=data.feature_names)

<span class="hljs-comment"># 대상 변수를 DataFrame에 추가</span>
df_diabetes[<span class="hljs-string">'target'</span>] = data.target

<span class="hljs-comment"># NaN 값 제거</span>
df = df_diabetes.dropna(axis=<span class="hljs-string">'rows'</span>) <span class="hljs-comment"># NaN 값 제거</span>

<span class="hljs-comment"># DataFrame 표시</span>
df_diabetes.head()
</code></pre>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_24.png" alt="이미지"></p>
<pre><code class="hljs language-python">df_diabetes.info()
</code></pre>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_25.png" alt="image"></p>
<pre><code class="hljs language-js">df_diabetes.<span class="hljs-title function_">describe</span>()
</code></pre>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_26.png" alt="image"></p>
<p>2.3— Exploratory Data Analysis</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">sns.<span class="hljs-title function_">pairplot</span>(df_diabetes)
</code></pre>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_27.png" alt="image"></p>
<p>2.4— 데이터를 학습 및 테스트 세트로 분할</p>
<pre><code class="hljs language-js">df = df_diabetes.<span class="hljs-property">iloc</span>[:<span class="hljs-number">100</span>,<span class="hljs-number">0</span>:<span class="hljs-number">10</span>]
</code></pre>
<div class="content-ad"></div>
<pre><code class="hljs language-js">X=df.<span class="hljs-title function_">to_numpy</span>()

y=df_diabetes.<span class="hljs-property">iloc</span>[:<span class="hljs-number">100</span>,-<span class="hljs-number">1</span>}
</code></pre>
<pre><code class="hljs language-js">X_train, X_test, y_train, y_test = <span class="hljs-title function_">train_test_split</span>(X, y, test_size = <span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">42</span>)
</code></pre>
<p>2.5 — Create the regressor model</p>
<pre><code class="hljs language-js">model = xgb.<span class="hljs-title class_">XGBRegressor</span>(objective=<span class="hljs-string">"reg:linear"</span>, random_state=<span class="hljs-number">42</span>)
</code></pre>
<div class="content-ad"></div>
<p>2.6 — 모델 훈련</p>
<pre><code class="hljs language-js">model.<span class="hljs-title function_">fit</span>(X_train, y_train)
</code></pre>
<p>2.7 — 모델 최적화</p>
<p>RandomizedSearchCV는 scikit-learn 라이브러리에서 제공하는 함수로, 머신 러닝 모델의 하이퍼파라미터 튜닝을 위해 교차 검증을 통해 주로 사용됩니다. 이 기술은 하이퍼파라미터의 폭넓은 탐색 영역을 다룰 때 유용하며, 가장 효과적인 값을 결정하는 데 도움이 됩니다.</p>
<div class="content-ad"></div>
<p>단계별 설명</p>
<ol>
<li>매개변수 공간 정의:</li>
</ol>
<p>RandomizedSearchCV를 활용하기 전에, 모델의 하이퍼파라미터를 위한 탐색 공간을 지정해야 합니다. 특정 값의 그리드를 제공하는 대신, 각 하이퍼파라미터에 대해 분포를 정의합니다.</p>
<ol start="2">
<li>무작위 샘플링:</li>
</ol>
<div class="content-ad"></div>
<p>GridSearchCV와 같이 모든 가맹 별로 동시에 평가하는 것이 아니라, RandomizedSearchCV는 평가를 위해 일정한 조합을 무작위로 선택합니다. 이는 큰 탐색 공간을 다룰 때 유리합니다.</p>
<ol start="3">
<li>모델 훈련:</li>
</ol>
<p>랜덤으로 선택된 각 하이퍼파라미터 집합에 대해 RandomizedSearchCV는 교차 검증을 사용하여 모델을 훈련합니다. 데이터는 폴드로 나누어지며, 모델은 일부 폴드에서 훈련되고 나머지 폴드에서 평가됩니다.</p>
<ol start="4">
<li>성능 평가:</li>
</ol>
<div class="content-ad"></div>
<p>성능은 특정 메트릭(예: 정확도, F1 점수)을 사용하여 측정됩니다. 목표는 주어진 문제에 따라 이 메트릭을 최대화하거나 최소화하는 하이퍼파라미터를 찾는 것입니다(예: 분류 문제에서 정확도를 최대화).</p>
<ol start="5">
<li>최적 모델 선택:</li>
</ol>
<p>랜덤 서치를 완료하면 RandomizedSearchCV가 교차 검증 중 가장 우수한 평균 성능을 보인 하이퍼파라미터 세트를 반환합니다.</p>
<p>RandomizedSearchCV를 사용하면 대규모 탐색 공간을 다룰 때 특히 모든 가능한 조합을 평가하는 그리드 서치(GridSearchCV)와 비교하여 계산 시간을 단축할 수 있습니다. 이 효율성은 모든 가능한 조합을 평가하는 대신 하이퍼파라미터 공간의 무작위 샘플을 탐색하는 데서 비롯됩니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">params = {
    <span class="hljs-string">"colsample_bytree"</span>: <span class="hljs-title function_">uniform</span>(<span class="hljs-number">0.7</span>, <span class="hljs-number">0.3</span>),
    <span class="hljs-string">"gamma"</span>: <span class="hljs-title function_">uniform</span>(<span class="hljs-number">0</span>, <span class="hljs-number">0.5</span>),
    <span class="hljs-string">"learning_rate"</span>: <span class="hljs-title function_">uniform</span>(<span class="hljs-number">0.03</span>, <span class="hljs-number">0.3</span>), # 기본값 <span class="hljs-number">0.1</span> 
    <span class="hljs-string">"max_depth"</span>: <span class="hljs-title function_">randint</span>(<span class="hljs-number">2</span>, <span class="hljs-number">6</span>), # 기본값 <span class="hljs-number">3</span>
    <span class="hljs-string">"n_estimators"</span>: <span class="hljs-title function_">randint</span>(<span class="hljs-number">100</span>, <span class="hljs-number">150</span>), # 기본값 <span class="hljs-number">100</span>
    <span class="hljs-string">"subsample"</span>: <span class="hljs-title function_">uniform</span>(<span class="hljs-number">0.6</span>, <span class="hljs-number">0.4</span>)
}

best_model = <span class="hljs-title class_">RandomizedSearchCV</span>(model, param_distributions=params, random_state=<span class="hljs-number">42</span>, n_iter=<span class="hljs-number">200</span>, cv=<span class="hljs-number">3</span>, verbose=<span class="hljs-number">1</span>, n_jobs=<span class="hljs-number">1</span>, return_train_score=<span class="hljs-title class_">True</span>)

best_model.<span class="hljs-title function_">fit</span>(X_train, y_train, early_stopping_rounds=<span class="hljs-number">5</span>, eval_set=[(X_test, y_test)]
</code></pre>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_28.png" alt="이미지"></p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">report_best_scores</span>(results, n_top=<span class="hljs-number">3</span>):
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-number">1</span>, n_top + <span class="hljs-number">1</span>):
        candidates = np.<span class="hljs-title function_">flatnonzero</span>(results[<span class="hljs-string">'rank_test_score'</span>] == i)
        <span class="hljs-keyword">for</span> candidate <span class="hljs-keyword">in</span> <span class="hljs-attr">candidates</span>:
            <span class="hljs-title function_">print</span>(<span class="hljs-string">"순위 {0}인 모델"</span>.<span class="hljs-title function_">format</span>(i))
            <span class="hljs-title function_">print</span>(<span class="hljs-string">"평균 검증 점수: {0:.3f} (표준편차: {1:.3f})"</span>.<span class="hljs-title function_">format</span>(
                results[<span class="hljs-string">'mean_test_score'</span>][candidate],
                results[<span class="hljs-string">'std_test_score'</span>][candidate]))
            best_params = results[<span class="hljs-string">'params'</span>][candidate]
            <span class="hljs-title function_">print</span>(<span class="hljs-string">"찾은 최적의 매개변수:"</span>)
            <span class="hljs-keyword">for</span> param, value <span class="hljs-keyword">in</span> best_params.<span class="hljs-title function_">items</span>():
                <span class="hljs-title function_">print</span>(<span class="hljs-string">"  {0}: {1}"</span>.<span class="hljs-title function_">format</span>(param, value))
            <span class="hljs-title function_">print</span>(<span class="hljs-string">""</span>)
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-title function_">report_best_scores</span>(best_model.<span class="hljs-property">cv_results_</span>, <span class="hljs-number">1</span>)
</code></pre>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_29.png" alt="Image"></p>
<pre><code class="hljs language-js">model =  xgb.<span class="hljs-title class_">XGBRegressor</span>(objective=<span class="hljs-string">"reg:linear"</span>, max_depth= <span class="hljs-number">5</span>, learning_rate= <span class="hljs-number">0.29302969102852483</span>, gamma = <span class="hljs-number">0.38122934287034527</span>)
model.<span class="hljs-title function_">fit</span>(X_train, y_train)
</code></pre>
<p>2.8 — Visualization</p>
<pre><code class="hljs language-js">fig, ax = plt.<span class="hljs-title function_">subplots</span>(figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">10</span>))
<span class="hljs-title function_">plot_tree</span>(model, num_trees=<span class="hljs-number">0</span>, ax=ax)
plt.<span class="hljs-title function_">show</span>()
</code></pre>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_30.png" alt="image"></p>
<p>2.9— 훈련 데이터로 모델 평가</p>
<pre><code class="hljs language-js">score = model.<span class="hljs-title function_">score</span>(X_train, y_train)
training_predict = model.<span class="hljs-title function_">predict</span>(X_train)
mse = <span class="hljs-title function_">mean_squared_error</span>(y_train, training_predict)

<span class="hljs-title function_">print</span>(<span class="hljs-string">"R-squared:"</span>, score)
<span class="hljs-title function_">print</span>(<span class="hljs-string">"MSE: "</span>, mse)
<span class="hljs-title function_">print</span>(<span class="hljs-string">"RMSE: "</span>, mse**(<span class="hljs-number">1</span>/<span class="hljs-number">2.0</span>))
</code></pre>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_31.png" alt="image"></p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">x_ax = <span class="hljs-title function_">range</span>(<span class="hljs-title function_">len</span>(y_train))
plt.<span class="hljs-title function_">plot</span>(x_ax, y_train, label=<span class="hljs-string">"원본"</span>)
plt.<span class="hljs-title function_">plot</span>(x_ax, training_predict, label=<span class="hljs-string">"예측된 값"</span>)
plt.<span class="hljs-title function_">title</span>(<span class="hljs-string">"훈련 및 예측된 데이터"</span>)
plt.<span class="hljs-title function_">xlabel</span>(<span class="hljs-string">'X축'</span>)
plt.<span class="hljs-title function_">ylabel</span>(<span class="hljs-string">'Y축'</span>)
plt.<span class="hljs-title function_">legend</span>(loc=<span class="hljs-string">'best'</span>, fancybox=<span class="hljs-title class_">True</span>, shadow=<span class="hljs-title class_">True</span>)
plt.<span class="hljs-title function_">grid</span>(<span class="hljs-title class_">True</span>)
plt.<span class="hljs-title function_">show</span>()
</code></pre>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_32.png" alt="image"></p>
<p>2.10— 테스트 데이터로 모델 평가</p>
<pre><code class="hljs language-js">score = model.<span class="hljs-title function_">score</span>(X_test, y_test)
test_predict = model.<span class="hljs-title function_">predict</span>(X_test)
mse = <span class="hljs-title function_">mean_squared_error</span>(y_test, test_predict)

<span class="hljs-title function_">print</span>(<span class="hljs-string">"R-squared:"</span>, score)
<span class="hljs-title function_">print</span>(<span class="hljs-string">"MSE: "</span>, mse)
<span class="hljs-title function_">print</span>(<span class="hljs-string">"RMSE: "</span>, mse**(<span class="hljs-number">1</span>/<span class="hljs-number">2.0</span>))
</code></pre>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_33.png">
<pre><code class="hljs language-js">x_ax = <span class="hljs-title function_">range</span>(<span class="hljs-title function_">len</span>(y_test))
plt.<span class="hljs-title function_">plot</span>(x_ax, y_test, label=<span class="hljs-string">"original"</span>)
plt.<span class="hljs-title function_">plot</span>(x_ax, test_predict, label=<span class="hljs-string">"predicted"</span>)
plt.<span class="hljs-title function_">title</span>(<span class="hljs-string">"Testing and predicted data"</span>)
plt.<span class="hljs-title function_">xlabel</span>(<span class="hljs-string">'X-axis'</span>)
plt.<span class="hljs-title function_">ylabel</span>(<span class="hljs-string">'Y-axis'</span>)
plt.<span class="hljs-title function_">legend</span>(loc=<span class="hljs-string">'best'</span>,fancybox=<span class="hljs-title class_">True</span>, shadow=<span class="hljs-title class_">True</span>)
plt.<span class="hljs-title function_">grid</span>(<span class="hljs-title class_">True</span>)
plt.<span class="hljs-title function_">show</span>()
</code></pre>
<img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_34.png">
<p>2.11 — 테스트 데이터를 사용하여 모델 평가하기</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">code = m2c.<span class="hljs-title function_">export_to_c</span>(model)
<span class="hljs-title function_">print</span>(code)
</code></pre>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_35.png" alt="Image"></p>
<p>2.12 — 템플릿을 .h 파일에 저장합니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">with</span> <span class="hljs-title function_">open</span>(<span class="hljs-string">'./XGBRegressor.h'</span>, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> <span class="hljs-attr">file</span>:
    file.<span class="hljs-title function_">write</span>(code)
</code></pre>
<div class="content-ad"></div>
<p>2.13 — 모델 배포</p>
<p>이 예제를 통해 ESP32, 아두이노, 아두이노 Portenta H7 with Vision Shield, 라즈베리 파이 및 기타 다양한 마이크로컨트롤러 또는 IoT 장치에서 머신 러닝 알고리즘을 구현할 수 있습니다.</p>
<p>2.13.1 — 완성된 아두이노 스케치</p>
<pre><code class="hljs language-js">#include <span class="hljs-string">"XGBRegressor.h"</span>


<span class="hljs-keyword">void</span> <span class="hljs-title function_">setup</span>(<span class="hljs-params"></span>) {
  <span class="hljs-title class_">Serial</span>.<span class="hljs-title function_">begin</span>(<span class="hljs-number">115200</span>);
}

<span class="hljs-keyword">void</span> <span class="hljs-title function_">loop</span>(<span class="hljs-params"></span>) {
  double <span class="hljs-variable constant_">X_1</span>[] = { <span class="hljs-number">2.71782911e-02</span>,  <span class="hljs-number">5.06801187e-02</span>,  <span class="hljs-number">1.75059115e-02</span>,
                  -<span class="hljs-number">3.32135761e-02</span>, -<span class="hljs-number">7.07277125e-03</span>,  <span class="hljs-number">4.59715403e-02</span>,
                  -<span class="hljs-number">6.54906725e-02</span>,  <span class="hljs-number">7.12099798e-02</span>, -<span class="hljs-number">9.64332229e-02</span>,
                  -<span class="hljs-number">5.90671943e-02</span>};
  double result_1 = <span class="hljs-title function_">score</span>(<span class="hljs-variable constant_">X_1</span>);
  <span class="hljs-title class_">Serial</span>.<span class="hljs-title function_">print</span>(<span class="hljs-string">"입력 X1로 예측 결과 (실제 값 = 69):"</span>);
  <span class="hljs-title class_">Serial</span>.<span class="hljs-title function_">println</span>(<span class="hljs-title class_">String</span>(result_1, <span class="hljs-number">7</span>));
  <span class="hljs-title function_">delay</span>(<span class="hljs-number">2000</span>);
}
</code></pre>
<div class="content-ad"></div>
<p>3.12 — 결과</p>
<p><img src="/assets/img/2024-06-19-TinyMLXGBoostRegression_36.png" alt="image"></p>
<p>전체 프로젝트: <a href="github.com">TinyML/14_XGBRegression at main · thommaskevin/TinyML</a></p>
<h2>만약 마음에 드셨다면, 제 커피 한 잔 사주세요 ☕️💰 (Bitcoin)</h2>
<div class="content-ad"></div>
<pre><code class="hljs language-plaintext">코드: bc1qzydjy4m9yhmjjrkgtrzhsgmkq79qenvcvc7qzn

![Image](/assets/img/2024-06-19-TinyMLXGBoostRegression_37.png)
</code></pre>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"작은 머신러닝  XGBoost 회귀","description":"","date":"2024-06-19 05:58","slug":"2024-06-19-TinyMLXGBoostRegression","content":"\n\n수학적 기초부터 엣지 구현까지\n\n# 소셜 미디어:\n\n👨🏽‍💻 Github: thommaskevin/TinyML (github.com)\n👷🏾 Linkedin: Thommas Kevin | LinkedIn\n📽 Youtube: Thommas Kevin — YouTube\n👨🏻‍🏫 연구 그룹: Conecta.ai (ufrn.br)\n\n![이미지](/assets/img/2024-06-19-TinyMLXGBoostRegression_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 요약\n\n### 1 - XGBoost 회귀 이론\n\n보완적으로, 부스팅은 일련의 모델 집합 𝑡가 순차적으로 훈련되는 앙상블 접근 방식을 나타냅니다. 각 모델 𝑡는 이전 모델, 𝑡−1에서 발견된 결함을 보정하는 목적으로 설계되었습니다.\n\n타겟 값 yᵢ와 샘플 xᵢ에 대한 모델 𝑡의 예측 ŷᵢᵗ을 고려하고, 평균 제곱 오차 (MSE) 등의 일반적인 오류 함수 l로 나타내고, 총 샘플 수를 n으로 표시할 때, 반복 t에서의 모델의 오류(또는 손실)는 다음과 같이 정의됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_1.png\" /\u003e\n\n모델이 단계적으로 구축되었다는 것을 관찰할 수 있습니다. t 단계에서의 예측은 t-1 단계에서의 예측에 새 모델 fₜ의 예측을 더한 결과입니다:\n\n\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_2.png\" /\u003e\n\n우리는 모델의 복잡성을 조절하는 데 기여하는 정규화항을 도입할 것이며(나중에 이 항의 구체적인 기능이 명확해질 것입니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nXGBoost의 기본 개념은 각 트리의 포함이 전략적이라는 것을 전제로 합니다: 목표는 항상 오차를 최소화하는 최적의 트리를 구축하는 것입니다. 이를 위해, 우리는 함수 L을 최적화 문제로 다룰 것이며, 결국 L을 최소화하는 fₜ를 결정하려고 합니다. 그러나 이 작업의 복잡성은 오차 함수 l을 선택하는 데 따라 다를 수 있습니다.\n\n따라서 우리는 이 함수를 Taylor 전개를 통해 간소화하기로 결정했습니다. 어떤 무한 차별화 가능한 함수도 다음 형식으로 표현할 수 있다는 것이 널리 인정받았습니다:\n\n![수식](/assets/img/2024-06-19-TinyMLXGBoostRegression_3.png)\n\n중간 단계에서 시리즈를 자르면 함수의 근사치를 얻을 수 있습니다. 현재 상황에서는 확장을 둘째 차수에서 중지하기로 선택했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image1](/assets/img/2024-06-19-TinyMLXGBoostRegression_4.png)\n\ngᵢ (gradient)와 hᵢ (Hessian)로 도함수를 대체할 것입니다:\n\n![image2](/assets/img/2024-06-19-TinyMLXGBoostRegression_5.png)\n\n만약 이 방정식을 최소화하는 fₜ를 찾는 것이 목적이라면, 상수항인 l은 필요하지 않습니다. 따라서 l을 버리면 다음과 같이 됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![링크 텍스트](/assets/img/2024-06-19-TinyMLXGBoostRegression_6.png)\n\nXGBoost의 주목할만한 특성 중 하나는 손실 함수가 두 번 미분 가능해야 한다는 요구사항입니다. 특정 문제에 대해 사용자 정의 오류 함수를 이용하여 XGBoost를 적용하려는 경우, 오류 계산 뿐만 아니라 그레이디언트(일차 도함수) 및 헤시안(이차 도함수)에 대한 정보도 필요하다는 점을 명심하는 것이 중요합니다.\n\n## 1.1 — 의사 결정 트리\n\n의사 결정 트리의 작동을 고려할 때, 방정식 L을 다시 쓸 필요가 있습니다. 각 샘플 xᵢ가 leaf j와 연관되어 있음을 알 수 있습니다. 따라서 각 leaf에 대해 샘플이 포함된 집합 인덱스 Iⱼ를 만들 수 있습니다. \n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_7.png)\n\nIⱼ가 정의되어 있으며, Iⱼ에 속하는 각 인덱스 i에 대해 샘플 xᵢ가 통과한 결정 경로 q는 잎 j로 이어진다.\n\n또한, 모델이 샘플 xᵢ에 대해 응답하는 것이 xᵢ가 속한 잎에서 관련된 가중치임을 알 수 있습니다:\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_8.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그 결과, 방정식의 일부 용어를 다시 정의할 수 있습니다:\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_9.png)\n\n대체를 수행하면 다음과 같이 얻을 수 있습니다:\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정규화 항도 확장할 거에요:\n\n![](/assets/img/2024-06-19-TinyMLXGBoostRegression_11.png)\n\n## 1.2 — 예측 오류 최적화\n\n나무의 모든 리프를 고려하는 대신에, 특정 리프에 초점을 맞출 거에요. 이 리프는 j로 표시돼요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-06-19-TinyMLXGBoostRegression_12.png)\n\nThe objective is to find the set of weights w that minimizes L. This may seem challenging at first glance, but let’s analyze it more closely.\n\n![Image 2](/assets/img/2024-06-19-TinyMLXGBoostRegression_13.png)\n\nAs previously noted, our error function for a leaf is quadratic, implying that the minimum is determined by the inflection point of the curve, where the first derivative is equal to zero.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_14.png\" /\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\nwᵈ를 고립시키면 다음과 같이 됩니다:\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_15.png\" /\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n이제 임의의 리프에 대해 최적 가중치를 제공하는 식을 확인했습니다. 따라서 L에 대한 우리의 식에 이 식을 대입함으로써 우리는 다음을 얻습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_16.png)\n\n이전 방정식은 각 새 트리의 각 분리를 평가하는 데 사용됩니다. 엔트로피나 지니 계수가 전통적으로 의사결정 트리 구성에 사용되는 방법과 마찬가지로 분리에서 양쪽 노드인 왼쪽 노드와 오른쪽 노드가 생성됩니다. 분할별 이득은 새로운 리프인 Lₗ(왼쪽)과 Lᵣ(오른쪽)의 합을 이전 오차인 Lₜ에서 뺀 것으로 정의됩니다. (우리가 Leaf가 하나만 분석하므로 T=1이라고 가정합니다.)\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_17.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 1.3 — 하이퍼파라미터 튜닝\n\n이러한 방정식을 이해하면 XGBoost의 하이퍼파라미터와 기능을 더 잘 이해할 수 있습니다.\n\nreg_lambda: 이 파라미터는 잎의 가중치에 영향을 미치며, 값이 클수록 가중치의 절대값이 작아집니다. 이러한 이유로 𝜆은 모델의 복잡성을 제어하는 매개변수로, 가중치가 너무 커지는 것을 방지합니다. 보다 정확히는 L2 정규화입니다.\n\n![이미지](/assets/img/2024-06-19-TinyMLXGBoostRegression_18.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- reg_alpha: 분모를 제로에 가깝게 만들어서 중요성이 적은 트리 또는 분할을 제외하는 효과가 있습니다. 유도된 값의 모듈리(0보다 작을 때 -1, 0보다 클 때 1)의 행동으로 인해 가중 함수가 두 가지 경우로 나누어짐을 언급해야 합니다.\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_19.png)\n\n- gamma: 𝛾는 분할이 발생하는 최솟값으로, 𝛾보다 낮은 값은 결과적으로 부정적인 이득이 발생하여 실제 결과를 악화시킬 수 있으므로 고려되지 않습니다.\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_20.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- learning_rate: 문 개선을 위해 각 가중치에 0에서 1 사이의 값을 곱하여 나무의 개별적인 중요성을 감소시키고 학습 과정을 늦춰 미래 나무의 포함 여지를 늘립니다.\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_21.png)\n\n여기서 η는 트리 ft의 전체 예측에 미치는 영향을 직접 조절하며 가중치 계산 방식을 수정하지 않습니다.\n\n- max_delta_step: 각 반복의 최대 절대 가중치를 상수 𝛿로 제한하여 가중치의 부호를 항상 보존합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-06-19-TinyMLXGBoostRegression_22.png)\n\n- max_child_weight: 자식 노드마다 ℎ의 합이 이 매개변수로 설정된 값보다 크기 때문에 분할이 수행됩니다. ℎ는 오차 함수(𝑙)의 도함수에 의해 결정됩니다. 따라서 ℎ의 값이 낮을 때는 해당 리프가 이미 충분히 \"순수\"하며 더 이상 분할할 필요가 없다는 것을 나타냅니다.\n\n![Image 2](/assets/img/2024-06-19-TinyMLXGBoostRegression_23.png)\n\n여기서 Python 구현에 사용 가능한 매개변수 전체 목록을 찾을 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2— TinyML 구현\n\n위 예제를 통해 ESP32, Arduino, Raspberry 및 기타 다양한 마이크로컨트롤러 또는 IoT 장치에서 머신러닝 알고리즘을 구현할 수 있습니다.\n\n2.0 — requirements.txt 파일에 나열된 라이브러리 설치\n\n```js\n!pip install -r requirements.txt\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2.1 — 라이브러리 가져오기\n\n```js\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV\nimport m2cgen as m2c\nimport numpy as np\nfrom scipy.stats import uniform, randint\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom xgboost import plot_tree\n```\n\n2.2— 데이터셋 로드\n\n당뇨병 데이터셋은 Bradley Efron, Trevor Hastie, Iain Johnstone 및 Robert Tibshirani가 스탠포드 대학에서 만들었습니다. 그들의 당뇨병 진행 예측 연구에 사용되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터셋은 임상 및 인구 통계 변수인 열 개의 기준 변수로 구성되어 있습니다:\n\n1. 나이\n\n2. 성별\n\n3. 체질량 지수 (BMI)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4. 평균 혈압\n\n5. S1 — TC, T-세포 (백혈구의 일종)\n\n6. S2 — LDL, 저밀도 리포닛\n\n7. S3 — HDL, 고밀도 리포닛\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n8. S4 - TCH, 총 콜레스테롤\n\n9. S5 - LTG, 혈청 트리글리세리드 수준의 로그 가능성\n\n10. S6 - 포도당, 혈당 수준\n\n- 데이터셋에는 442개의 인스턴스(환자)가 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 대상 변수는 기준선 이후 1년 후의 질병 진행의 양을 양적으로 측정한 것입니다. 데이터 집합에 명시적으로 언급되지 않은 요소를 기반으로 질병 진행을 표현합니다. 이는 연속 변수입니다.\n\n```python\n# 데이터셋 불러오기\ndata = load_diabetes() # 데이터 불러오기\n\n# DataFrame 생성\ndf_diabetes = pd.DataFrame(data.data, columns=data.feature_names)\n\n# 대상 변수를 DataFrame에 추가\ndf_diabetes['target'] = data.target\n\n# NaN 값 제거\ndf = df_diabetes.dropna(axis='rows') # NaN 값 제거\n\n# DataFrame 표시\ndf_diabetes.head()\n```\n\n![이미지](/assets/img/2024-06-19-TinyMLXGBoostRegression_24.png)\n\n```python\ndf_diabetes.info()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_25.png)\n\n```js\ndf_diabetes.describe()\n```\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_26.png)\n\n2.3— Exploratory Data Analysis\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsns.pairplot(df_diabetes)\n```\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_27.png)\n\n2.4— 데이터를 학습 및 테스트 세트로 분할\n\n```js\ndf = df_diabetes.iloc[:100,0:10]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nX=df.to_numpy()\n\ny=df_diabetes.iloc[:100,-1}\n```\n\n```js\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n```\n\n2.5 — Create the regressor model\n\n```js\nmodel = xgb.XGBRegressor(objective=\"reg:linear\", random_state=42)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2.6 — 모델 훈련\n\n```js\nmodel.fit(X_train, y_train)\n```\n\n2.7 — 모델 최적화\n\nRandomizedSearchCV는 scikit-learn 라이브러리에서 제공하는 함수로, 머신 러닝 모델의 하이퍼파라미터 튜닝을 위해 교차 검증을 통해 주로 사용됩니다. 이 기술은 하이퍼파라미터의 폭넓은 탐색 영역을 다룰 때 유용하며, 가장 효과적인 값을 결정하는 데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계별 설명\n\n1. 매개변수 공간 정의:\n\nRandomizedSearchCV를 활용하기 전에, 모델의 하이퍼파라미터를 위한 탐색 공간을 지정해야 합니다. 특정 값의 그리드를 제공하는 대신, 각 하이퍼파라미터에 대해 분포를 정의합니다.\n\n2. 무작위 샘플링:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGridSearchCV와 같이 모든 가맹 별로 동시에 평가하는 것이 아니라, RandomizedSearchCV는 평가를 위해 일정한 조합을 무작위로 선택합니다. 이는 큰 탐색 공간을 다룰 때 유리합니다.\n\n3. 모델 훈련:\n\n랜덤으로 선택된 각 하이퍼파라미터 집합에 대해 RandomizedSearchCV는 교차 검증을 사용하여 모델을 훈련합니다. 데이터는 폴드로 나누어지며, 모델은 일부 폴드에서 훈련되고 나머지 폴드에서 평가됩니다.\n\n4. 성능 평가:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n성능은 특정 메트릭(예: 정확도, F1 점수)을 사용하여 측정됩니다. 목표는 주어진 문제에 따라 이 메트릭을 최대화하거나 최소화하는 하이퍼파라미터를 찾는 것입니다(예: 분류 문제에서 정확도를 최대화).\n\n5. 최적 모델 선택:\n\n랜덤 서치를 완료하면 RandomizedSearchCV가 교차 검증 중 가장 우수한 평균 성능을 보인 하이퍼파라미터 세트를 반환합니다.\n\nRandomizedSearchCV를 사용하면 대규모 탐색 공간을 다룰 때 특히 모든 가능한 조합을 평가하는 그리드 서치(GridSearchCV)와 비교하여 계산 시간을 단축할 수 있습니다. 이 효율성은 모든 가능한 조합을 평가하는 대신 하이퍼파라미터 공간의 무작위 샘플을 탐색하는 데서 비롯됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nparams = {\n    \"colsample_bytree\": uniform(0.7, 0.3),\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), # 기본값 0.1 \n    \"max_depth\": randint(2, 6), # 기본값 3\n    \"n_estimators\": randint(100, 150), # 기본값 100\n    \"subsample\": uniform(0.6, 0.4)\n}\n\nbest_model = RandomizedSearchCV(model, param_distributions=params, random_state=42, n_iter=200, cv=3, verbose=1, n_jobs=1, return_train_score=True)\n\nbest_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_test, y_test)]\n```\n\n![이미지](/assets/img/2024-06-19-TinyMLXGBoostRegression_28.png)\n\n```js\ndef report_best_scores(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"순위 {0}인 모델\".format(i))\n            print(\"평균 검증 점수: {0:.3f} (표준편차: {1:.3f})\".format(\n                results['mean_test_score'][candidate],\n                results['std_test_score'][candidate]))\n            best_params = results['params'][candidate]\n            print(\"찾은 최적의 매개변수:\")\n            for param, value in best_params.items():\n                print(\"  {0}: {1}\".format(param, value))\n            print(\"\")\n```\n\n```js\nreport_best_scores(best_model.cv_results_, 1)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-19-TinyMLXGBoostRegression_29.png)\n\n```js\nmodel =  xgb.XGBRegressor(objective=\"reg:linear\", max_depth= 5, learning_rate= 0.29302969102852483, gamma = 0.38122934287034527)\nmodel.fit(X_train, y_train)\n```\n\n2.8 — Visualization\n\n```js\nfig, ax = plt.subplots(figsize=(20, 10))\nplot_tree(model, num_trees=0, ax=ax)\nplt.show()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_30.png)\n\n2.9— 훈련 데이터로 모델 평가\n\n```js\nscore = model.score(X_train, y_train)\ntraining_predict = model.predict(X_train)\nmse = mean_squared_error(y_train, training_predict)\n\nprint(\"R-squared:\", score)\nprint(\"MSE: \", mse)\nprint(\"RMSE: \", mse**(1/2.0))\n```\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_31.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nx_ax = range(len(y_train))\nplt.plot(x_ax, y_train, label=\"원본\")\nplt.plot(x_ax, training_predict, label=\"예측된 값\")\nplt.title(\"훈련 및 예측된 데이터\")\nplt.xlabel('X축')\nplt.ylabel('Y축')\nplt.legend(loc='best', fancybox=True, shadow=True)\nplt.grid(True)\nplt.show()\n```\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_32.png)\n\n2.10— 테스트 데이터로 모델 평가\n\n```js\nscore = model.score(X_test, y_test)\ntest_predict = model.predict(X_test)\nmse = mean_squared_error(y_test, test_predict)\n\nprint(\"R-squared:\", score)\nprint(\"MSE: \", mse)\nprint(\"RMSE: \", mse**(1/2.0))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_33.png\" /\u003e\n\n```js\nx_ax = range(len(y_test))\nplt.plot(x_ax, y_test, label=\"original\")\nplt.plot(x_ax, test_predict, label=\"predicted\")\nplt.title(\"Testing and predicted data\")\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend(loc='best',fancybox=True, shadow=True)\nplt.grid(True)\nplt.show()\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_34.png\" /\u003e\n\n2.11 — 테스트 데이터를 사용하여 모델 평가하기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ncode = m2c.export_to_c(model)\nprint(code)\n```\n\n![Image](/assets/img/2024-06-19-TinyMLXGBoostRegression_35.png)\n\n2.12 — 템플릿을 .h 파일에 저장합니다.\n\n```js\nwith open('./XGBRegressor.h', 'w') as file:\n    file.write(code)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2.13 — 모델 배포\n\n이 예제를 통해 ESP32, 아두이노, 아두이노 Portenta H7 with Vision Shield, 라즈베리 파이 및 기타 다양한 마이크로컨트롤러 또는 IoT 장치에서 머신 러닝 알고리즘을 구현할 수 있습니다.\n\n2.13.1 — 완성된 아두이노 스케치\n\n```js\n#include \"XGBRegressor.h\"\n\n\nvoid setup() {\n  Serial.begin(115200);\n}\n\nvoid loop() {\n  double X_1[] = { 2.71782911e-02,  5.06801187e-02,  1.75059115e-02,\n                  -3.32135761e-02, -7.07277125e-03,  4.59715403e-02,\n                  -6.54906725e-02,  7.12099798e-02, -9.64332229e-02,\n                  -5.90671943e-02};\n  double result_1 = score(X_1);\n  Serial.print(\"입력 X1로 예측 결과 (실제 값 = 69):\");\n  Serial.println(String(result_1, 7));\n  delay(2000);\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3.12 — 결과\n\n![image](/assets/img/2024-06-19-TinyMLXGBoostRegression_36.png)\n\n전체 프로젝트: [TinyML/14_XGBRegression at main · thommaskevin/TinyML](github.com)\n\n## 만약 마음에 드셨다면, 제 커피 한 잔 사주세요 ☕️💰 (Bitcoin)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```plaintext\n코드: bc1qzydjy4m9yhmjjrkgtrzhsgmkq79qenvcvc7qzn\n\n![Image](/assets/img/2024-06-19-TinyMLXGBoostRegression_37.png)\n```","ogImage":{"url":"/assets/img/2024-06-19-TinyMLXGBoostRegression_0.png"},"coverImage":"/assets/img/2024-06-19-TinyMLXGBoostRegression_0.png","tag":["Tech"],"readingTime":14},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e수학적 기초부터 엣지 구현까지\u003c/p\u003e\n\u003ch1\u003e소셜 미디어:\u003c/h1\u003e\n\u003cp\u003e👨🏽‍💻 Github: thommaskevin/TinyML (github.com)\n👷🏾 Linkedin: Thommas Kevin | LinkedIn\n📽 Youtube: Thommas Kevin — YouTube\n👨🏻‍🏫 연구 그룹: Conecta.ai (ufrn.br)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e요약\u003c/h2\u003e\n\u003ch3\u003e1 - XGBoost 회귀 이론\u003c/h3\u003e\n\u003cp\u003e보완적으로, 부스팅은 일련의 모델 집합 𝑡가 순차적으로 훈련되는 앙상블 접근 방식을 나타냅니다. 각 모델 𝑡는 이전 모델, 𝑡−1에서 발견된 결함을 보정하는 목적으로 설계되었습니다.\u003c/p\u003e\n\u003cp\u003e타겟 값 yᵢ와 샘플 xᵢ에 대한 모델 𝑡의 예측 ŷᵢᵗ을 고려하고, 평균 제곱 오차 (MSE) 등의 일반적인 오류 함수 l로 나타내고, 총 샘플 수를 n으로 표시할 때, 반복 t에서의 모델의 오류(또는 손실)는 다음과 같이 정의됩니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_1.png\"\u003e\n\u003cp\u003e모델이 단계적으로 구축되었다는 것을 관찰할 수 있습니다. t 단계에서의 예측은 t-1 단계에서의 예측에 새 모델 fₜ의 예측을 더한 결과입니다:\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_2.png\"\u003e\n\u003cp\u003e우리는 모델의 복잡성을 조절하는 데 기여하는 정규화항을 도입할 것이며(나중에 이 항의 구체적인 기능이 명확해질 것입니다).\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eXGBoost의 기본 개념은 각 트리의 포함이 전략적이라는 것을 전제로 합니다: 목표는 항상 오차를 최소화하는 최적의 트리를 구축하는 것입니다. 이를 위해, 우리는 함수 L을 최적화 문제로 다룰 것이며, 결국 L을 최소화하는 fₜ를 결정하려고 합니다. 그러나 이 작업의 복잡성은 오차 함수 l을 선택하는 데 따라 다를 수 있습니다.\u003c/p\u003e\n\u003cp\u003e따라서 우리는 이 함수를 Taylor 전개를 통해 간소화하기로 결정했습니다. 어떤 무한 차별화 가능한 함수도 다음 형식으로 표현할 수 있다는 것이 널리 인정받았습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_3.png\" alt=\"수식\"\u003e\u003c/p\u003e\n\u003cp\u003e중간 단계에서 시리즈를 자르면 함수의 근사치를 얻을 수 있습니다. 현재 상황에서는 확장을 둘째 차수에서 중지하기로 선택했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_4.png\" alt=\"image1\"\u003e\u003c/p\u003e\n\u003cp\u003egᵢ (gradient)와 hᵢ (Hessian)로 도함수를 대체할 것입니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_5.png\" alt=\"image2\"\u003e\u003c/p\u003e\n\u003cp\u003e만약 이 방정식을 최소화하는 fₜ를 찾는 것이 목적이라면, 상수항인 l은 필요하지 않습니다. 따라서 l을 버리면 다음과 같이 됩니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_6.png\" alt=\"링크 텍스트\"\u003e\u003c/p\u003e\n\u003cp\u003eXGBoost의 주목할만한 특성 중 하나는 손실 함수가 두 번 미분 가능해야 한다는 요구사항입니다. 특정 문제에 대해 사용자 정의 오류 함수를 이용하여 XGBoost를 적용하려는 경우, 오류 계산 뿐만 아니라 그레이디언트(일차 도함수) 및 헤시안(이차 도함수)에 대한 정보도 필요하다는 점을 명심하는 것이 중요합니다.\u003c/p\u003e\n\u003ch2\u003e1.1 — 의사 결정 트리\u003c/h2\u003e\n\u003cp\u003e의사 결정 트리의 작동을 고려할 때, 방정식 L을 다시 쓸 필요가 있습니다. 각 샘플 xᵢ가 leaf j와 연관되어 있음을 알 수 있습니다. 따라서 각 leaf에 대해 샘플이 포함된 집합 인덱스 Iⱼ를 만들 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_7.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eIⱼ가 정의되어 있으며, Iⱼ에 속하는 각 인덱스 i에 대해 샘플 xᵢ가 통과한 결정 경로 q는 잎 j로 이어진다.\u003c/p\u003e\n\u003cp\u003e또한, 모델이 샘플 xᵢ에 대해 응답하는 것이 xᵢ가 속한 잎에서 관련된 가중치임을 알 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_8.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그 결과, 방정식의 일부 용어를 다시 정의할 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_9.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e대체를 수행하면 다음과 같이 얻을 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_10.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e정규화 항도 확장할 거에요:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_11.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e1.2 — 예측 오류 최적화\u003c/h2\u003e\n\u003cp\u003e나무의 모든 리프를 고려하는 대신에, 특정 리프에 초점을 맞출 거에요. 이 리프는 j로 표시돼요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_12.png\" alt=\"Image 1\"\u003e\u003c/p\u003e\n\u003cp\u003eThe objective is to find the set of weights w that minimizes L. This may seem challenging at first glance, but let’s analyze it more closely.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_13.png\" alt=\"Image 2\"\u003e\u003c/p\u003e\n\u003cp\u003eAs previously noted, our error function for a leaf is quadratic, implying that the minimum is determined by the inflection point of the curve, where the first derivative is equal to zero.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n    \u003ctd\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_14.png\"\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003ewᵈ를 고립시키면 다음과 같이 됩니다:\u003c/p\u003e\n\u003ctable\u003e\n  \u003ctbody\u003e\u003ctr\u003e\n    \u003ctd\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_15.png\"\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e이제 임의의 리프에 대해 최적 가중치를 제공하는 식을 확인했습니다. 따라서 L에 대한 우리의 식에 이 식을 대입함으로써 우리는 다음을 얻습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_16.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e이전 방정식은 각 새 트리의 각 분리를 평가하는 데 사용됩니다. 엔트로피나 지니 계수가 전통적으로 의사결정 트리 구성에 사용되는 방법과 마찬가지로 분리에서 양쪽 노드인 왼쪽 노드와 오른쪽 노드가 생성됩니다. 분할별 이득은 새로운 리프인 Lₗ(왼쪽)과 Lᵣ(오른쪽)의 합을 이전 오차인 Lₜ에서 뺀 것으로 정의됩니다. (우리가 Leaf가 하나만 분석하므로 T=1이라고 가정합니다.)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_17.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e1.3 — 하이퍼파라미터 튜닝\u003c/h2\u003e\n\u003cp\u003e이러한 방정식을 이해하면 XGBoost의 하이퍼파라미터와 기능을 더 잘 이해할 수 있습니다.\u003c/p\u003e\n\u003cp\u003ereg_lambda: 이 파라미터는 잎의 가중치에 영향을 미치며, 값이 클수록 가중치의 절대값이 작아집니다. 이러한 이유로 𝜆은 모델의 복잡성을 제어하는 매개변수로, 가중치가 너무 커지는 것을 방지합니다. 보다 정확히는 L2 정규화입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_18.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003ereg_alpha: 분모를 제로에 가깝게 만들어서 중요성이 적은 트리 또는 분할을 제외하는 효과가 있습니다. 유도된 값의 모듈리(0보다 작을 때 -1, 0보다 클 때 1)의 행동으로 인해 가중 함수가 두 가지 경우로 나누어짐을 언급해야 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_19.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003egamma: 𝛾는 분할이 발생하는 최솟값으로, 𝛾보다 낮은 값은 결과적으로 부정적인 이득이 발생하여 실제 결과를 악화시킬 수 있으므로 고려되지 않습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_20.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003elearning_rate: 문 개선을 위해 각 가중치에 0에서 1 사이의 값을 곱하여 나무의 개별적인 중요성을 감소시키고 학습 과정을 늦춰 미래 나무의 포함 여지를 늘립니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_21.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서 η는 트리 ft의 전체 예측에 미치는 영향을 직접 조절하며 가중치 계산 방식을 수정하지 않습니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emax_delta_step: 각 반복의 최대 절대 가중치를 상수 𝛿로 제한하여 가중치의 부호를 항상 보존합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_22.png\" alt=\"Image 1\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emax_child_weight: 자식 노드마다 ℎ의 합이 이 매개변수로 설정된 값보다 크기 때문에 분할이 수행됩니다. ℎ는 오차 함수(𝑙)의 도함수에 의해 결정됩니다. 따라서 ℎ의 값이 낮을 때는 해당 리프가 이미 충분히 \"순수\"하며 더 이상 분할할 필요가 없다는 것을 나타냅니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_23.png\" alt=\"Image 2\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서 Python 구현에 사용 가능한 매개변수 전체 목록을 찾을 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e2— TinyML 구현\u003c/h1\u003e\n\u003cp\u003e위 예제를 통해 ESP32, Arduino, Raspberry 및 기타 다양한 마이크로컨트롤러 또는 IoT 장치에서 머신러닝 알고리즘을 구현할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e2.0 — requirements.txt 파일에 나열된 라이브러리 설치\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e!pip install -r requirements.\u003cspan class=\"hljs-property\"\u003etxt\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e2.1 — 라이브러리 가져오기\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.\u003cspan class=\"hljs-property\"\u003edatasets\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_diabetes\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.\u003cspan class=\"hljs-property\"\u003emodel_selection\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e train_test_split\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.\u003cspan class=\"hljs-property\"\u003emetrics\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e mean_squared_error\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.\u003cspan class=\"hljs-property\"\u003emodel_selection\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eRandomizedSearchCV\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e m2cgen \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e m2c\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e scipy.\u003cspan class=\"hljs-property\"\u003estats\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e uniform, randint\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.\u003cspan class=\"hljs-property\"\u003epyplot\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e xgboost \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e xgb\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e xgboost \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e plot_tree\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e2.2— 데이터셋 로드\u003c/p\u003e\n\u003cp\u003e당뇨병 데이터셋은 Bradley Efron, Trevor Hastie, Iain Johnstone 및 Robert Tibshirani가 스탠포드 대학에서 만들었습니다. 그들의 당뇨병 진행 예측 연구에 사용되었습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e데이터셋은 임상 및 인구 통계 변수인 열 개의 기준 변수로 구성되어 있습니다:\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e나이\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e성별\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e체질량 지수 (BMI)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003e\n\u003cp\u003e평균 혈압\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eS1 — TC, T-세포 (백혈구의 일종)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eS2 — LDL, 저밀도 리포닛\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eS3 — HDL, 고밀도 리포닛\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003col start=\"8\"\u003e\n\u003cli\u003e\n\u003cp\u003eS4 - TCH, 총 콜레스테롤\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eS5 - LTG, 혈청 트리글리세리드 수준의 로그 가능성\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eS6 - 포도당, 혈당 수준\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e데이터셋에는 442개의 인스턴스(환자)가 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e대상 변수는 기준선 이후 1년 후의 질병 진행의 양을 양적으로 측정한 것입니다. 데이터 집합에 명시적으로 언급되지 않은 요소를 기반으로 질병 진행을 표현합니다. 이는 연속 변수입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# 데이터셋 불러오기\u003c/span\u003e\ndata = load_diabetes() \u003cspan class=\"hljs-comment\"\u003e# 데이터 불러오기\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# DataFrame 생성\u003c/span\u003e\ndf_diabetes = pd.DataFrame(data.data, columns=data.feature_names)\n\n\u003cspan class=\"hljs-comment\"\u003e# 대상 변수를 DataFrame에 추가\u003c/span\u003e\ndf_diabetes[\u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e] = data.target\n\n\u003cspan class=\"hljs-comment\"\u003e# NaN 값 제거\u003c/span\u003e\ndf = df_diabetes.dropna(axis=\u003cspan class=\"hljs-string\"\u003e'rows'\u003c/span\u003e) \u003cspan class=\"hljs-comment\"\u003e# NaN 값 제거\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# DataFrame 표시\u003c/span\u003e\ndf_diabetes.head()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_24.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003edf_diabetes.info()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_25.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edf_diabetes.\u003cspan class=\"hljs-title function_\"\u003edescribe\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_26.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e2.3— Exploratory Data Analysis\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003esns.\u003cspan class=\"hljs-title function_\"\u003epairplot\u003c/span\u003e(df_diabetes)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_27.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e2.4— 데이터를 학습 및 테스트 세트로 분할\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edf = df_diabetes.\u003cspan class=\"hljs-property\"\u003eiloc\u003c/span\u003e[:\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eX=df.\u003cspan class=\"hljs-title function_\"\u003eto_numpy\u003c/span\u003e()\n\ny=df_diabetes.\u003cspan class=\"hljs-property\"\u003eiloc\u003c/span\u003e[:\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e,-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eX_train, X_test, y_train, y_test = \u003cspan class=\"hljs-title function_\"\u003etrain_test_split\u003c/span\u003e(X, y, test_size = \u003cspan class=\"hljs-number\"\u003e0.3\u003c/span\u003e, random_state=\u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e2.5 — Create the regressor model\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003emodel = xgb.\u003cspan class=\"hljs-title class_\"\u003eXGBRegressor\u003c/span\u003e(objective=\u003cspan class=\"hljs-string\"\u003e\"reg:linear\"\u003c/span\u003e, random_state=\u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e2.6 — 모델 훈련\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003emodel.\u003cspan class=\"hljs-title function_\"\u003efit\u003c/span\u003e(X_train, y_train)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e2.7 — 모델 최적화\u003c/p\u003e\n\u003cp\u003eRandomizedSearchCV는 scikit-learn 라이브러리에서 제공하는 함수로, 머신 러닝 모델의 하이퍼파라미터 튜닝을 위해 교차 검증을 통해 주로 사용됩니다. 이 기술은 하이퍼파라미터의 폭넓은 탐색 영역을 다룰 때 유용하며, 가장 효과적인 값을 결정하는 데 도움이 됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e단계별 설명\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e매개변수 공간 정의:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eRandomizedSearchCV를 활용하기 전에, 모델의 하이퍼파라미터를 위한 탐색 공간을 지정해야 합니다. 특정 값의 그리드를 제공하는 대신, 각 하이퍼파라미터에 대해 분포를 정의합니다.\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e무작위 샘플링:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eGridSearchCV와 같이 모든 가맹 별로 동시에 평가하는 것이 아니라, RandomizedSearchCV는 평가를 위해 일정한 조합을 무작위로 선택합니다. 이는 큰 탐색 공간을 다룰 때 유리합니다.\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e모델 훈련:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e랜덤으로 선택된 각 하이퍼파라미터 집합에 대해 RandomizedSearchCV는 교차 검증을 사용하여 모델을 훈련합니다. 데이터는 폴드로 나누어지며, 모델은 일부 폴드에서 훈련되고 나머지 폴드에서 평가됩니다.\u003c/p\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003e성능 평가:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e성능은 특정 메트릭(예: 정확도, F1 점수)을 사용하여 측정됩니다. 목표는 주어진 문제에 따라 이 메트릭을 최대화하거나 최소화하는 하이퍼파라미터를 찾는 것입니다(예: 분류 문제에서 정확도를 최대화).\u003c/p\u003e\n\u003col start=\"5\"\u003e\n\u003cli\u003e최적 모델 선택:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e랜덤 서치를 완료하면 RandomizedSearchCV가 교차 검증 중 가장 우수한 평균 성능을 보인 하이퍼파라미터 세트를 반환합니다.\u003c/p\u003e\n\u003cp\u003eRandomizedSearchCV를 사용하면 대규모 탐색 공간을 다룰 때 특히 모든 가능한 조합을 평가하는 그리드 서치(GridSearchCV)와 비교하여 계산 시간을 단축할 수 있습니다. 이 효율성은 모든 가능한 조합을 평가하는 대신 하이퍼파라미터 공간의 무작위 샘플을 탐색하는 데서 비롯됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eparams = {\n    \u003cspan class=\"hljs-string\"\u003e\"colsample_bytree\"\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003euniform\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0.7\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.3\u003c/span\u003e),\n    \u003cspan class=\"hljs-string\"\u003e\"gamma\"\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003euniform\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e),\n    \u003cspan class=\"hljs-string\"\u003e\"learning_rate\"\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003euniform\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0.03\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.3\u003c/span\u003e), # 기본값 \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e \n    \u003cspan class=\"hljs-string\"\u003e\"max_depth\"\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003erandint\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e), # 기본값 \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"n_estimators\"\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003erandint\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e), # 기본값 \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"subsample\"\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003euniform\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0.6\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.4\u003c/span\u003e)\n}\n\nbest_model = \u003cspan class=\"hljs-title class_\"\u003eRandomizedSearchCV\u003c/span\u003e(model, param_distributions=params, random_state=\u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e, n_iter=\u003cspan class=\"hljs-number\"\u003e200\u003c/span\u003e, cv=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, verbose=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, n_jobs=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, return_train_score=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\nbest_model.\u003cspan class=\"hljs-title function_\"\u003efit\u003c/span\u003e(X_train, y_train, early_stopping_rounds=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, eval_set=[(X_test, y_test)]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_28.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003ereport_best_scores\u003c/span\u003e(results, n_top=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, n_top + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e):\n        candidates = np.\u003cspan class=\"hljs-title function_\"\u003eflatnonzero\u003c/span\u003e(results[\u003cspan class=\"hljs-string\"\u003e'rank_test_score'\u003c/span\u003e] == i)\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e candidate \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ecandidates\u003c/span\u003e:\n            \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"순위 {0}인 모델\"\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eformat\u003c/span\u003e(i))\n            \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"평균 검증 점수: {0:.3f} (표준편차: {1:.3f})\"\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eformat\u003c/span\u003e(\n                results[\u003cspan class=\"hljs-string\"\u003e'mean_test_score'\u003c/span\u003e][candidate],\n                results[\u003cspan class=\"hljs-string\"\u003e'std_test_score'\u003c/span\u003e][candidate]))\n            best_params = results[\u003cspan class=\"hljs-string\"\u003e'params'\u003c/span\u003e][candidate]\n            \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"찾은 최적의 매개변수:\"\u003c/span\u003e)\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e param, value \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e best_params.\u003cspan class=\"hljs-title function_\"\u003eitems\u003c/span\u003e():\n                \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"  {0}: {1}\"\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eformat\u003c/span\u003e(param, value))\n            \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title function_\"\u003ereport_best_scores\u003c/span\u003e(best_model.\u003cspan class=\"hljs-property\"\u003ecv_results_\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_29.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003emodel =  xgb.\u003cspan class=\"hljs-title class_\"\u003eXGBRegressor\u003c/span\u003e(objective=\u003cspan class=\"hljs-string\"\u003e\"reg:linear\"\u003c/span\u003e, max_depth= \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, learning_rate= \u003cspan class=\"hljs-number\"\u003e0.29302969102852483\u003c/span\u003e, gamma = \u003cspan class=\"hljs-number\"\u003e0.38122934287034527\u003c/span\u003e)\nmodel.\u003cspan class=\"hljs-title function_\"\u003efit\u003c/span\u003e(X_train, y_train)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e2.8 — Visualization\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003efig, ax = plt.\u003cspan class=\"hljs-title function_\"\u003esubplots\u003c/span\u003e(figsize=(\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e))\n\u003cspan class=\"hljs-title function_\"\u003eplot_tree\u003c/span\u003e(model, num_trees=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, ax=ax)\nplt.\u003cspan class=\"hljs-title function_\"\u003eshow\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_30.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e2.9— 훈련 데이터로 모델 평가\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003escore = model.\u003cspan class=\"hljs-title function_\"\u003escore\u003c/span\u003e(X_train, y_train)\ntraining_predict = model.\u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(X_train)\nmse = \u003cspan class=\"hljs-title function_\"\u003emean_squared_error\u003c/span\u003e(y_train, training_predict)\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"R-squared:\"\u003c/span\u003e, score)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"MSE: \"\u003c/span\u003e, mse)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"RMSE: \"\u003c/span\u003e, mse**(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e2.0\u003c/span\u003e))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_31.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ex_ax = \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(y_train))\nplt.\u003cspan class=\"hljs-title function_\"\u003eplot\u003c/span\u003e(x_ax, y_train, label=\u003cspan class=\"hljs-string\"\u003e\"원본\"\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003eplot\u003c/span\u003e(x_ax, training_predict, label=\u003cspan class=\"hljs-string\"\u003e\"예측된 값\"\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003etitle\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"훈련 및 예측된 데이터\"\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003exlabel\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'X축'\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003eylabel\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'Y축'\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003elegend\u003c/span\u003e(loc=\u003cspan class=\"hljs-string\"\u003e'best'\u003c/span\u003e, fancybox=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e, shadow=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003egrid\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003eshow\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_32.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e2.10— 테스트 데이터로 모델 평가\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003escore = model.\u003cspan class=\"hljs-title function_\"\u003escore\u003c/span\u003e(X_test, y_test)\ntest_predict = model.\u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(X_test)\nmse = \u003cspan class=\"hljs-title function_\"\u003emean_squared_error\u003c/span\u003e(y_test, test_predict)\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"R-squared:\"\u003c/span\u003e, score)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"MSE: \"\u003c/span\u003e, mse)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"RMSE: \"\u003c/span\u003e, mse**(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e2.0\u003c/span\u003e))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_33.png\"\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ex_ax = \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(y_test))\nplt.\u003cspan class=\"hljs-title function_\"\u003eplot\u003c/span\u003e(x_ax, y_test, label=\u003cspan class=\"hljs-string\"\u003e\"original\"\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003eplot\u003c/span\u003e(x_ax, test_predict, label=\u003cspan class=\"hljs-string\"\u003e\"predicted\"\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003etitle\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Testing and predicted data\"\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003exlabel\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'X-axis'\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003eylabel\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'Y-axis'\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003elegend\u003c/span\u003e(loc=\u003cspan class=\"hljs-string\"\u003e'best'\u003c/span\u003e,fancybox=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e, shadow=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003egrid\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003eshow\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_34.png\"\u003e\n\u003cp\u003e2.11 — 테스트 데이터를 사용하여 모델 평가하기\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ecode = m2c.\u003cspan class=\"hljs-title function_\"\u003eexport_to_c\u003c/span\u003e(model)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(code)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_35.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003e2.12 — 템플릿을 .h 파일에 저장합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eopen\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'./XGBRegressor.h'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'w'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003efile\u003c/span\u003e:\n    file.\u003cspan class=\"hljs-title function_\"\u003ewrite\u003c/span\u003e(code)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e2.13 — 모델 배포\u003c/p\u003e\n\u003cp\u003e이 예제를 통해 ESP32, 아두이노, 아두이노 Portenta H7 with Vision Shield, 라즈베리 파이 및 기타 다양한 마이크로컨트롤러 또는 IoT 장치에서 머신 러닝 알고리즘을 구현할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e2.13.1 — 완성된 아두이노 스케치\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e#include \u003cspan class=\"hljs-string\"\u003e\"XGBRegressor.h\"\u003c/span\u003e\n\n\n\u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esetup\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\u003c/span\u003e) {\n  \u003cspan class=\"hljs-title class_\"\u003eSerial\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ebegin\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e115200\u003c/span\u003e);\n}\n\n\u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eloop\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\u003c/span\u003e) {\n  double \u003cspan class=\"hljs-variable constant_\"\u003eX_1\u003c/span\u003e[] = { \u003cspan class=\"hljs-number\"\u003e2.71782911e-02\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e5.06801187e-02\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e1.75059115e-02\u003c/span\u003e,\n                  -\u003cspan class=\"hljs-number\"\u003e3.32135761e-02\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e7.07277125e-03\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e4.59715403e-02\u003c/span\u003e,\n                  -\u003cspan class=\"hljs-number\"\u003e6.54906725e-02\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e7.12099798e-02\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e9.64332229e-02\u003c/span\u003e,\n                  -\u003cspan class=\"hljs-number\"\u003e5.90671943e-02\u003c/span\u003e};\n  double result_1 = \u003cspan class=\"hljs-title function_\"\u003escore\u003c/span\u003e(\u003cspan class=\"hljs-variable constant_\"\u003eX_1\u003c/span\u003e);\n  \u003cspan class=\"hljs-title class_\"\u003eSerial\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"입력 X1로 예측 결과 (실제 값 = 69):\"\u003c/span\u003e);\n  \u003cspan class=\"hljs-title class_\"\u003eSerial\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eprintln\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eString\u003c/span\u003e(result_1, \u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e));\n  \u003cspan class=\"hljs-title function_\"\u003edelay\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e2000\u003c/span\u003e);\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e3.12 — 결과\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyMLXGBoostRegression_36.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e전체 프로젝트: \u003ca href=\"github.com\"\u003eTinyML/14_XGBRegression at main · thommaskevin/TinyML\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e만약 마음에 드셨다면, 제 커피 한 잔 사주세요 ☕️💰 (Bitcoin)\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-plaintext\"\u003e코드: bc1qzydjy4m9yhmjjrkgtrzhsgmkq79qenvcvc7qzn\n\n![Image](/assets/img/2024-06-19-TinyMLXGBoostRegression_37.png)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-TinyMLXGBoostRegression"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>