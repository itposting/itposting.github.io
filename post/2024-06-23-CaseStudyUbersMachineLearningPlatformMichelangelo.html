<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석 | itposting" data-gatsby-head="true"/><meta property="og:title" content="사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo" data-gatsby-head="true"/><meta name="twitter:title" content="사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-23 16:14" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_buildManifest.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 23, 2024</span><span class="posts_reading_time__f7YPP">5<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>Michelangelo은 Uber 내부 ML-as-a-service 플랫폼으로, 기업의 요구를 충족하기 위해 AI를 확장하고 머신 러닝을 민주화하는 서비스입니다. 데이터 관리, 모델 학습, 평가, 배포, 예측 및 모니터링 기능을 하며 전통적인 머신 러닝 모델부터 딥 러닝까지 다양한 모델을 다룰 수 있습니다. Michelangelo은 오랫동안 Uber에서 사용 중이며 여러 Uber 데이터 센터에 배포되어 있습니다. 이 기사에서는 Michelangelo의 개발 동기와 아키텍처에 대해 자세히 살펴보겠습니다.</p>
<p>Michelangelo 이전에 Uber는 scikit-learn, R 등과 같은 온라인에서 사용 가능한 오픈 소스 도구를 사용했으나 ML의 영향력은 소수의 데이터 과학자와 엔지니어가 주로 오픈 소스 도구를 사용하여 구축할 수 있는 한도를 벗어날 수 없었습니다. 따라서 Uber는 규모화된 훈련 및 예측 데이터를 관리하기 위한 신뢰성 있고 일관된, 재현 가능한 파이프라인을 구축하고자 했습니다. 가장 중요한 것은 실험 비교 및 모델을 프로덕션 환경에 배포하는 과정이 명확히 정립되지 않았습니다. Uber의 엔지니어링 팀은 해당 프로젝트에 특정한 사용자 정의 서빙 컨테이너를 만들어야 했습니다. Michelangelo는 이러한 문제를 해결하기 위해 팀 간 워크플로우를 표준화하고 엔지니어들이 쉽게 규모에 맞게 머신 러닝 시스템을 구축하고 운영할 수 있도록 하는 엔드 투 엔드 시스템을 통해 설계되었습니다. Michelangelo은 주로 아이디어에서 첫 프로덕션 모델로의 경로 축소와 그 이후 빠른 반복에 초점을 맞추었습니다. UberEATS의 사용 사례를 통해 전체 프로세스를 더 자세히 알아보겠습니다.</p>
<p>UberEATS에는 Michelangelo에서 실행되는 여러 머신 러닝 모델이 있습니다. 배달 시간 예측, 검색 순위, 음식점 순위 등을 다루는 모델들이 포함되어 있습니다. 배송 시간 모델은 주문이 발생하기 전에 음식이 준비되고 배달되는 데 얼마나 걸릴지 예측합니다. 그렇다면 UberEATS는 어떻게 작동할까요? 고객이 주문하면 레스토랑으로 전송되어 처리됩니다. 레스토랑은 주문을 확인하고 식사를 준비해야 하며, 주문의 복잡성과 레스토랑의 혼잡 정도에 따라 필요한 시간이 달라집니다. 식사가 준비되기 직전에 Uber 딜리버리 파트너가 음식을 가져가도록 지시됩니다. 그런 다음, 딜리버리 파트너는 레스토랑에 도착하고 음식을 가져와 고객의 위치로 운전하고(경로, 교통 등에 따라 달라집니다) 고객의 집으로 걸어가 배달을 완료해야 합니다. 이 복잡한 프로세스의 총 소요 시간을 예측하고 프로세스의 각 단계에서 이러한 시간까지 재계산하는 것이 목표입니다.</p>
<p>UberEATS의 데이터 과학자들은 end-to-end delivery 시간을 예측하기 위해 그라디언트 부스트된 의사 결정 트리 회귀 모델을 사용합니다. 모델의 features로는 하루 중 시간, 배달 위치, 평균 식사 준비 시간 등이 포함됩니다. 이러한 모델들은 Michelangelo 모델 서빙 컨테이너에 배포되어 Uber의 데이터 센터 전체에 통합되며, UberEATS 마이크로서비스에 의해 네트워크 요청을 통해 호출됩니다. 이러한 예측은 고객이 식사가 준비되고 배달되는 동안에 표시됩니다. Michelangelo은 Uber 내부에서 개발된 구성 요소와 오픈 소스 시스템의 혼합을 사용하여 구축되었습니다. XGBoost, Tensorflow, Spark 등의 오픈 소스 구성 요소가 사용되었습니다.</p>
<div class="content-ad"></div>
<p>Michelangelo의 데이터 관리 구성 요소는 온라인과 오프라인 파이프라인으로 나뉩니다. 오프라인 파이프라인은 일괄 모델 훈련 및 일괄 예측 작업에 사용되며, 온라인 파이프라인은 온라인, 낮은 지연 시간 예측을 수행합니다. 또한 Michelangelo에는 기능 스토어가 있어 팀이 기계 학습 문제용 다른 기능 세트를 공유하고 발견할 수 있습니다. Uber의 모든 데이터는 먼저 HDFS 데이터 레이크에 저장되며, 오프라인 파이프라인에서 기능을 계산하기 위해 액세스할 수 있으며 온라인에 배포된 모델은 HDFS에 저장된 데이터에 액세스할 수 없습니다. 따라서 온라인 모델용 기능은 미리 계산되어 Cassandra 기능 스토어에 저장되어서 예측 시 낮은 대기 시간으로 읽을 수 있습니다.</p>
<p>앞에서 말했듯이 Uber는 중앙 집중식 기능 스토어를 만드는 데 높은 우선순위를 둡니다. Uber의 팀은 기능을 생성하고 관리하고 다른 사람과 공유할 수 있기 때문에 새로운 기능을 추가하는 것이 쉬워지며 기능 스토어에 기능이 있으면 사용하기 매우 쉬워집니다. Michelangelo에는 모델러가 훈련 및 예측 시 모델로 보내는 기능을 선택, 변환 및 결합하는 데 사용하는 DSL(도메인 특화 언어)이라는 것이 있습니다. DSL은 모델 구성 자체의 일부이며, 훈련 시점과 예측 시점에 적용되어 동일한 최종 기능 집합이 두 경우에 모델로 생성되어 보내지도록 보장합니다.</p>
<p>Uber는 수억 개의 샘플을 처리할 수 있는 분산 모델 훈련 시스템을 사용하며 의사결정 트리, 신경망 및 선형 모델과 같은 알고리즘을 위한 빠른 반복을 위해 작은 데이터 세트로 축소할 수도 있습니다. 모델 구성은 모델 유형, 초매개변수, 데이터 소스 참조 및 계산 리소스 요구 사항을 지정하며, 이것은 훈련 작업을 구성하는 데 사용됩니다. 모델 훈련 후 성능 메트릭이 계산되고 모델 평가 보고서에 통합됩니다. 훈련이 완료된 후, 원래 구성, 학습된 매개변수 및 평가 보고서가 분석 및 배포를 위해 모델 저장소에 저장됩니다. Michelangelo는 모든 모델 유형에 대한 초매개변수 검색을 지원하며 모든 훈련 작업은 API 및 워크플로우 도구를 통해 관리됩니다.</p>
<div class="content-ad"></div>
<p>아래는 마이셀란젤로가 어떻게 모델을 학습하고 평가하며 배포하는지에 대해 설명한 내용입니다. 마이셀란젤로는 특정 사용 사례에 이상적인 모델에 이르기까지 수백 개의 모델을 학습합니다. 이러한 수백 개의 모델은 엔지니어들을 최적의 성능을 보이는 모델 구성으로 안내하며, 학습된 모델을 추적하고 평가하고 비교하는 것이 마이셀란젤로에서 주요 관심사입니다. 모델 훈련 시, 마이셀란젤로에 저장된 각 모델은 학습자, 모델 구성, 정확도 측정, 학습된 매개변수 등의 정보가 포함된 버전화된 객체로 카산드라 모델 저장소에 저장됩니다. 이러한 정보는 웹 UI 또는 API를 통해 쉽게 확인하고 비교할 수 있습니다. 마이셀란젤로는 또한 모델이 동작하는 이유를 이해하고 필요한 경우 디버깅하는 데 도움이 되는 시각화 도구를 제공합니다. 특성 보고서에는 각 특성이 모델의 중요도에 따라 부분 의존 플롯 및 분포 히스토그램이 표시됩니다.</p>
<p>배포를 위해 마이셀란젤로는 UI 또는 API를 통해 모델 배포를 관리하는 엔드 투 엔드 지원을 제공합니다. 오프라인 모델은 요청에 따라 또는 정기적인 일정에 따라 실행되는 스파크 작업에서 실행되는 컨테이너에 배포됩니다. 온라인 모델은 수신 요청을 기반으로 예측하는 서비스 클러스터에 배포됩니다. 두 경우 모두, 모델 아티팩트는 ZIP 아카이브로 패키징되어 우버의 데이터 센터 각 위치에 복사됩니다. 예측 컨테이너는 자동으로 디스크에서 새 모델을 로드하고 예측 요청을 처리하기 시작합니다. 우버의 여러 팀은 마이셀란젤로의 API를 통해 정기적인 모델 재훈련과 배포를 예약하기 위한 자동화 스크립트를 보유하고 있습니다. 모델이 배포된 후, 데이터 파이프라인 또는 클라이언트 서비스에서 로드된 특성을 기반으로 예측을 수행합니다. 온라인 모델의 경우 예측은 네트워크를 통해 클라이언트 서비스로 반환되며, 오프라인 모델의 경우 예측 결과는 하이브(데이터 웨어하우스)에 다시 기록되어 SQL 기반 쿼리 도구를 통해 직접 액세스할 수 있습니다.</p>
<div class="content-ad"></div>
<p>한 번에 여러 모델을 동일한 서빙 컨테이너에 배포할 수 있습니다. 이를 통해 이전 모델에서 새로운 모델로의 쉬운 전환과 모델의 A/B 테스트를 병행할 수 있습니다. 서빙 시간에는 모델이 해당 태그로 최근에 배포된 모델을 사용하여 식별됩니다. 온라인 모델의 경우 확장을 위해 예측 서비스 클러스터에 더 많은 호스트가 추가되고 로드 밸런서가 부하를 분산시킵니다. 오프라인 예측의 경우 더 많은 Spark 실행자가 추가되며 Spark가 병렬성을 관리합니다. 예측을 모니터링하기 위해 Michelangelo는 자동으로 로그를 기록하고 선택적으로 생성된 예측의 일부를 보류하고 나중에 해당 예측을 데이터 파이프라인이 생성한 레이블과 비교합니다. 앞으로 Uber는 AutoML, 분산 딥 러닝 및 기타 도구 및 서비스를 추가하여 기존 시스템을 강화할 계획입니다.</p>
<pre><code class="hljs language-js">참고 자료:
<span class="hljs-number">1.</span> <span class="hljs-title class_">Uber</span> 엔지니어링 블로그
</code></pre>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"사례 연구 Uber의 머신러닝 플랫폼 Michelangelo 심층 분석","description":"","date":"2024-06-23 16:14","slug":"2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo","content":"\n\nMichelangelo은 Uber 내부 ML-as-a-service 플랫폼으로, 기업의 요구를 충족하기 위해 AI를 확장하고 머신 러닝을 민주화하는 서비스입니다. 데이터 관리, 모델 학습, 평가, 배포, 예측 및 모니터링 기능을 하며 전통적인 머신 러닝 모델부터 딥 러닝까지 다양한 모델을 다룰 수 있습니다. Michelangelo은 오랫동안 Uber에서 사용 중이며 여러 Uber 데이터 센터에 배포되어 있습니다. 이 기사에서는 Michelangelo의 개발 동기와 아키텍처에 대해 자세히 살펴보겠습니다.\n\nMichelangelo 이전에 Uber는 scikit-learn, R 등과 같은 온라인에서 사용 가능한 오픈 소스 도구를 사용했으나 ML의 영향력은 소수의 데이터 과학자와 엔지니어가 주로 오픈 소스 도구를 사용하여 구축할 수 있는 한도를 벗어날 수 없었습니다. 따라서 Uber는 규모화된 훈련 및 예측 데이터를 관리하기 위한 신뢰성 있고 일관된, 재현 가능한 파이프라인을 구축하고자 했습니다. 가장 중요한 것은 실험 비교 및 모델을 프로덕션 환경에 배포하는 과정이 명확히 정립되지 않았습니다. Uber의 엔지니어링 팀은 해당 프로젝트에 특정한 사용자 정의 서빙 컨테이너를 만들어야 했습니다. Michelangelo는 이러한 문제를 해결하기 위해 팀 간 워크플로우를 표준화하고 엔지니어들이 쉽게 규모에 맞게 머신 러닝 시스템을 구축하고 운영할 수 있도록 하는 엔드 투 엔드 시스템을 통해 설계되었습니다. Michelangelo은 주로 아이디어에서 첫 프로덕션 모델로의 경로 축소와 그 이후 빠른 반복에 초점을 맞추었습니다. UberEATS의 사용 사례를 통해 전체 프로세스를 더 자세히 알아보겠습니다.\n\nUberEATS에는 Michelangelo에서 실행되는 여러 머신 러닝 모델이 있습니다. 배달 시간 예측, 검색 순위, 음식점 순위 등을 다루는 모델들이 포함되어 있습니다. 배송 시간 모델은 주문이 발생하기 전에 음식이 준비되고 배달되는 데 얼마나 걸릴지 예측합니다. 그렇다면 UberEATS는 어떻게 작동할까요? 고객이 주문하면 레스토랑으로 전송되어 처리됩니다. 레스토랑은 주문을 확인하고 식사를 준비해야 하며, 주문의 복잡성과 레스토랑의 혼잡 정도에 따라 필요한 시간이 달라집니다. 식사가 준비되기 직전에 Uber 딜리버리 파트너가 음식을 가져가도록 지시됩니다. 그런 다음, 딜리버리 파트너는 레스토랑에 도착하고 음식을 가져와 고객의 위치로 운전하고(경로, 교통 등에 따라 달라집니다) 고객의 집으로 걸어가 배달을 완료해야 합니다. 이 복잡한 프로세스의 총 소요 시간을 예측하고 프로세스의 각 단계에서 이러한 시간까지 재계산하는 것이 목표입니다.\n\nUberEATS의 데이터 과학자들은 end-to-end delivery 시간을 예측하기 위해 그라디언트 부스트된 의사 결정 트리 회귀 모델을 사용합니다. 모델의 features로는 하루 중 시간, 배달 위치, 평균 식사 준비 시간 등이 포함됩니다. 이러한 모델들은 Michelangelo 모델 서빙 컨테이너에 배포되어 Uber의 데이터 센터 전체에 통합되며, UberEATS 마이크로서비스에 의해 네트워크 요청을 통해 호출됩니다. 이러한 예측은 고객이 식사가 준비되고 배달되는 동안에 표시됩니다. Michelangelo은 Uber 내부에서 개발된 구성 요소와 오픈 소스 시스템의 혼합을 사용하여 구축되었습니다. XGBoost, Tensorflow, Spark 등의 오픈 소스 구성 요소가 사용되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMichelangelo의 데이터 관리 구성 요소는 온라인과 오프라인 파이프라인으로 나뉩니다. 오프라인 파이프라인은 일괄 모델 훈련 및 일괄 예측 작업에 사용되며, 온라인 파이프라인은 온라인, 낮은 지연 시간 예측을 수행합니다. 또한 Michelangelo에는 기능 스토어가 있어 팀이 기계 학습 문제용 다른 기능 세트를 공유하고 발견할 수 있습니다. Uber의 모든 데이터는 먼저 HDFS 데이터 레이크에 저장되며, 오프라인 파이프라인에서 기능을 계산하기 위해 액세스할 수 있으며 온라인에 배포된 모델은 HDFS에 저장된 데이터에 액세스할 수 없습니다. 따라서 온라인 모델용 기능은 미리 계산되어 Cassandra 기능 스토어에 저장되어서 예측 시 낮은 대기 시간으로 읽을 수 있습니다.\n\n앞에서 말했듯이 Uber는 중앙 집중식 기능 스토어를 만드는 데 높은 우선순위를 둡니다. Uber의 팀은 기능을 생성하고 관리하고 다른 사람과 공유할 수 있기 때문에 새로운 기능을 추가하는 것이 쉬워지며 기능 스토어에 기능이 있으면 사용하기 매우 쉬워집니다. Michelangelo에는 모델러가 훈련 및 예측 시 모델로 보내는 기능을 선택, 변환 및 결합하는 데 사용하는 DSL(도메인 특화 언어)이라는 것이 있습니다. DSL은 모델 구성 자체의 일부이며, 훈련 시점과 예측 시점에 적용되어 동일한 최종 기능 집합이 두 경우에 모델로 생성되어 보내지도록 보장합니다.\n\nUber는 수억 개의 샘플을 처리할 수 있는 분산 모델 훈련 시스템을 사용하며 의사결정 트리, 신경망 및 선형 모델과 같은 알고리즘을 위한 빠른 반복을 위해 작은 데이터 세트로 축소할 수도 있습니다. 모델 구성은 모델 유형, 초매개변수, 데이터 소스 참조 및 계산 리소스 요구 사항을 지정하며, 이것은 훈련 작업을 구성하는 데 사용됩니다. 모델 훈련 후 성능 메트릭이 계산되고 모델 평가 보고서에 통합됩니다. 훈련이 완료된 후, 원래 구성, 학습된 매개변수 및 평가 보고서가 분석 및 배포를 위해 모델 저장소에 저장됩니다. Michelangelo는 모든 모델 유형에 대한 초매개변수 검색을 지원하며 모든 훈련 작업은 API 및 워크플로우 도구를 통해 관리됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 마이셀란젤로가 어떻게 모델을 학습하고 평가하며 배포하는지에 대해 설명한 내용입니다. 마이셀란젤로는 특정 사용 사례에 이상적인 모델에 이르기까지 수백 개의 모델을 학습합니다. 이러한 수백 개의 모델은 엔지니어들을 최적의 성능을 보이는 모델 구성으로 안내하며, 학습된 모델을 추적하고 평가하고 비교하는 것이 마이셀란젤로에서 주요 관심사입니다. 모델 훈련 시, 마이셀란젤로에 저장된 각 모델은 학습자, 모델 구성, 정확도 측정, 학습된 매개변수 등의 정보가 포함된 버전화된 객체로 카산드라 모델 저장소에 저장됩니다. 이러한 정보는 웹 UI 또는 API를 통해 쉽게 확인하고 비교할 수 있습니다. 마이셀란젤로는 또한 모델이 동작하는 이유를 이해하고 필요한 경우 디버깅하는 데 도움이 되는 시각화 도구를 제공합니다. 특성 보고서에는 각 특성이 모델의 중요도에 따라 부분 의존 플롯 및 분포 히스토그램이 표시됩니다.\n\n배포를 위해 마이셀란젤로는 UI 또는 API를 통해 모델 배포를 관리하는 엔드 투 엔드 지원을 제공합니다. 오프라인 모델은 요청에 따라 또는 정기적인 일정에 따라 실행되는 스파크 작업에서 실행되는 컨테이너에 배포됩니다. 온라인 모델은 수신 요청을 기반으로 예측하는 서비스 클러스터에 배포됩니다. 두 경우 모두, 모델 아티팩트는 ZIP 아카이브로 패키징되어 우버의 데이터 센터 각 위치에 복사됩니다. 예측 컨테이너는 자동으로 디스크에서 새 모델을 로드하고 예측 요청을 처리하기 시작합니다. 우버의 여러 팀은 마이셀란젤로의 API를 통해 정기적인 모델 재훈련과 배포를 예약하기 위한 자동화 스크립트를 보유하고 있습니다. 모델이 배포된 후, 데이터 파이프라인 또는 클라이언트 서비스에서 로드된 특성을 기반으로 예측을 수행합니다. 온라인 모델의 경우 예측은 네트워크를 통해 클라이언트 서비스로 반환되며, 오프라인 모델의 경우 예측 결과는 하이브(데이터 웨어하우스)에 다시 기록되어 SQL 기반 쿼리 도구를 통해 직접 액세스할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 번에 여러 모델을 동일한 서빙 컨테이너에 배포할 수 있습니다. 이를 통해 이전 모델에서 새로운 모델로의 쉬운 전환과 모델의 A/B 테스트를 병행할 수 있습니다. 서빙 시간에는 모델이 해당 태그로 최근에 배포된 모델을 사용하여 식별됩니다. 온라인 모델의 경우 확장을 위해 예측 서비스 클러스터에 더 많은 호스트가 추가되고 로드 밸런서가 부하를 분산시킵니다. 오프라인 예측의 경우 더 많은 Spark 실행자가 추가되며 Spark가 병렬성을 관리합니다. 예측을 모니터링하기 위해 Michelangelo는 자동으로 로그를 기록하고 선택적으로 생성된 예측의 일부를 보류하고 나중에 해당 예측을 데이터 파이프라인이 생성한 레이블과 비교합니다. 앞으로 Uber는 AutoML, 분산 딥 러닝 및 기타 도구 및 서비스를 추가하여 기존 시스템을 강화할 계획입니다.\n\n```js\n참고 자료:\n1. Uber 엔지니어링 블로그\n```","ogImage":{"url":"/assets/img/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo_0.png"},"coverImage":"/assets/img/2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo_0.png","tag":["Tech"],"readingTime":5},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003eMichelangelo은 Uber 내부 ML-as-a-service 플랫폼으로, 기업의 요구를 충족하기 위해 AI를 확장하고 머신 러닝을 민주화하는 서비스입니다. 데이터 관리, 모델 학습, 평가, 배포, 예측 및 모니터링 기능을 하며 전통적인 머신 러닝 모델부터 딥 러닝까지 다양한 모델을 다룰 수 있습니다. Michelangelo은 오랫동안 Uber에서 사용 중이며 여러 Uber 데이터 센터에 배포되어 있습니다. 이 기사에서는 Michelangelo의 개발 동기와 아키텍처에 대해 자세히 살펴보겠습니다.\u003c/p\u003e\n\u003cp\u003eMichelangelo 이전에 Uber는 scikit-learn, R 등과 같은 온라인에서 사용 가능한 오픈 소스 도구를 사용했으나 ML의 영향력은 소수의 데이터 과학자와 엔지니어가 주로 오픈 소스 도구를 사용하여 구축할 수 있는 한도를 벗어날 수 없었습니다. 따라서 Uber는 규모화된 훈련 및 예측 데이터를 관리하기 위한 신뢰성 있고 일관된, 재현 가능한 파이프라인을 구축하고자 했습니다. 가장 중요한 것은 실험 비교 및 모델을 프로덕션 환경에 배포하는 과정이 명확히 정립되지 않았습니다. Uber의 엔지니어링 팀은 해당 프로젝트에 특정한 사용자 정의 서빙 컨테이너를 만들어야 했습니다. Michelangelo는 이러한 문제를 해결하기 위해 팀 간 워크플로우를 표준화하고 엔지니어들이 쉽게 규모에 맞게 머신 러닝 시스템을 구축하고 운영할 수 있도록 하는 엔드 투 엔드 시스템을 통해 설계되었습니다. Michelangelo은 주로 아이디어에서 첫 프로덕션 모델로의 경로 축소와 그 이후 빠른 반복에 초점을 맞추었습니다. UberEATS의 사용 사례를 통해 전체 프로세스를 더 자세히 알아보겠습니다.\u003c/p\u003e\n\u003cp\u003eUberEATS에는 Michelangelo에서 실행되는 여러 머신 러닝 모델이 있습니다. 배달 시간 예측, 검색 순위, 음식점 순위 등을 다루는 모델들이 포함되어 있습니다. 배송 시간 모델은 주문이 발생하기 전에 음식이 준비되고 배달되는 데 얼마나 걸릴지 예측합니다. 그렇다면 UberEATS는 어떻게 작동할까요? 고객이 주문하면 레스토랑으로 전송되어 처리됩니다. 레스토랑은 주문을 확인하고 식사를 준비해야 하며, 주문의 복잡성과 레스토랑의 혼잡 정도에 따라 필요한 시간이 달라집니다. 식사가 준비되기 직전에 Uber 딜리버리 파트너가 음식을 가져가도록 지시됩니다. 그런 다음, 딜리버리 파트너는 레스토랑에 도착하고 음식을 가져와 고객의 위치로 운전하고(경로, 교통 등에 따라 달라집니다) 고객의 집으로 걸어가 배달을 완료해야 합니다. 이 복잡한 프로세스의 총 소요 시간을 예측하고 프로세스의 각 단계에서 이러한 시간까지 재계산하는 것이 목표입니다.\u003c/p\u003e\n\u003cp\u003eUberEATS의 데이터 과학자들은 end-to-end delivery 시간을 예측하기 위해 그라디언트 부스트된 의사 결정 트리 회귀 모델을 사용합니다. 모델의 features로는 하루 중 시간, 배달 위치, 평균 식사 준비 시간 등이 포함됩니다. 이러한 모델들은 Michelangelo 모델 서빙 컨테이너에 배포되어 Uber의 데이터 센터 전체에 통합되며, UberEATS 마이크로서비스에 의해 네트워크 요청을 통해 호출됩니다. 이러한 예측은 고객이 식사가 준비되고 배달되는 동안에 표시됩니다. Michelangelo은 Uber 내부에서 개발된 구성 요소와 오픈 소스 시스템의 혼합을 사용하여 구축되었습니다. XGBoost, Tensorflow, Spark 등의 오픈 소스 구성 요소가 사용되었습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eMichelangelo의 데이터 관리 구성 요소는 온라인과 오프라인 파이프라인으로 나뉩니다. 오프라인 파이프라인은 일괄 모델 훈련 및 일괄 예측 작업에 사용되며, 온라인 파이프라인은 온라인, 낮은 지연 시간 예측을 수행합니다. 또한 Michelangelo에는 기능 스토어가 있어 팀이 기계 학습 문제용 다른 기능 세트를 공유하고 발견할 수 있습니다. Uber의 모든 데이터는 먼저 HDFS 데이터 레이크에 저장되며, 오프라인 파이프라인에서 기능을 계산하기 위해 액세스할 수 있으며 온라인에 배포된 모델은 HDFS에 저장된 데이터에 액세스할 수 없습니다. 따라서 온라인 모델용 기능은 미리 계산되어 Cassandra 기능 스토어에 저장되어서 예측 시 낮은 대기 시간으로 읽을 수 있습니다.\u003c/p\u003e\n\u003cp\u003e앞에서 말했듯이 Uber는 중앙 집중식 기능 스토어를 만드는 데 높은 우선순위를 둡니다. Uber의 팀은 기능을 생성하고 관리하고 다른 사람과 공유할 수 있기 때문에 새로운 기능을 추가하는 것이 쉬워지며 기능 스토어에 기능이 있으면 사용하기 매우 쉬워집니다. Michelangelo에는 모델러가 훈련 및 예측 시 모델로 보내는 기능을 선택, 변환 및 결합하는 데 사용하는 DSL(도메인 특화 언어)이라는 것이 있습니다. DSL은 모델 구성 자체의 일부이며, 훈련 시점과 예측 시점에 적용되어 동일한 최종 기능 집합이 두 경우에 모델로 생성되어 보내지도록 보장합니다.\u003c/p\u003e\n\u003cp\u003eUber는 수억 개의 샘플을 처리할 수 있는 분산 모델 훈련 시스템을 사용하며 의사결정 트리, 신경망 및 선형 모델과 같은 알고리즘을 위한 빠른 반복을 위해 작은 데이터 세트로 축소할 수도 있습니다. 모델 구성은 모델 유형, 초매개변수, 데이터 소스 참조 및 계산 리소스 요구 사항을 지정하며, 이것은 훈련 작업을 구성하는 데 사용됩니다. 모델 훈련 후 성능 메트릭이 계산되고 모델 평가 보고서에 통합됩니다. 훈련이 완료된 후, 원래 구성, 학습된 매개변수 및 평가 보고서가 분석 및 배포를 위해 모델 저장소에 저장됩니다. Michelangelo는 모든 모델 유형에 대한 초매개변수 검색을 지원하며 모든 훈련 작업은 API 및 워크플로우 도구를 통해 관리됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래는 마이셀란젤로가 어떻게 모델을 학습하고 평가하며 배포하는지에 대해 설명한 내용입니다. 마이셀란젤로는 특정 사용 사례에 이상적인 모델에 이르기까지 수백 개의 모델을 학습합니다. 이러한 수백 개의 모델은 엔지니어들을 최적의 성능을 보이는 모델 구성으로 안내하며, 학습된 모델을 추적하고 평가하고 비교하는 것이 마이셀란젤로에서 주요 관심사입니다. 모델 훈련 시, 마이셀란젤로에 저장된 각 모델은 학습자, 모델 구성, 정확도 측정, 학습된 매개변수 등의 정보가 포함된 버전화된 객체로 카산드라 모델 저장소에 저장됩니다. 이러한 정보는 웹 UI 또는 API를 통해 쉽게 확인하고 비교할 수 있습니다. 마이셀란젤로는 또한 모델이 동작하는 이유를 이해하고 필요한 경우 디버깅하는 데 도움이 되는 시각화 도구를 제공합니다. 특성 보고서에는 각 특성이 모델의 중요도에 따라 부분 의존 플롯 및 분포 히스토그램이 표시됩니다.\u003c/p\u003e\n\u003cp\u003e배포를 위해 마이셀란젤로는 UI 또는 API를 통해 모델 배포를 관리하는 엔드 투 엔드 지원을 제공합니다. 오프라인 모델은 요청에 따라 또는 정기적인 일정에 따라 실행되는 스파크 작업에서 실행되는 컨테이너에 배포됩니다. 온라인 모델은 수신 요청을 기반으로 예측하는 서비스 클러스터에 배포됩니다. 두 경우 모두, 모델 아티팩트는 ZIP 아카이브로 패키징되어 우버의 데이터 센터 각 위치에 복사됩니다. 예측 컨테이너는 자동으로 디스크에서 새 모델을 로드하고 예측 요청을 처리하기 시작합니다. 우버의 여러 팀은 마이셀란젤로의 API를 통해 정기적인 모델 재훈련과 배포를 예약하기 위한 자동화 스크립트를 보유하고 있습니다. 모델이 배포된 후, 데이터 파이프라인 또는 클라이언트 서비스에서 로드된 특성을 기반으로 예측을 수행합니다. 온라인 모델의 경우 예측은 네트워크를 통해 클라이언트 서비스로 반환되며, 오프라인 모델의 경우 예측 결과는 하이브(데이터 웨어하우스)에 다시 기록되어 SQL 기반 쿼리 도구를 통해 직접 액세스할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e한 번에 여러 모델을 동일한 서빙 컨테이너에 배포할 수 있습니다. 이를 통해 이전 모델에서 새로운 모델로의 쉬운 전환과 모델의 A/B 테스트를 병행할 수 있습니다. 서빙 시간에는 모델이 해당 태그로 최근에 배포된 모델을 사용하여 식별됩니다. 온라인 모델의 경우 확장을 위해 예측 서비스 클러스터에 더 많은 호스트가 추가되고 로드 밸런서가 부하를 분산시킵니다. 오프라인 예측의 경우 더 많은 Spark 실행자가 추가되며 Spark가 병렬성을 관리합니다. 예측을 모니터링하기 위해 Michelangelo는 자동으로 로그를 기록하고 선택적으로 생성된 예측의 일부를 보류하고 나중에 해당 예측을 데이터 파이프라인이 생성한 레이블과 비교합니다. 앞으로 Uber는 AutoML, 분산 딥 러닝 및 기타 도구 및 서비스를 추가하여 기존 시스템을 강화할 계획입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e참고 자료:\n\u003cspan class=\"hljs-number\"\u003e1.\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eUber\u003c/span\u003e 엔지니어링 블로그\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-23-CaseStudyUbersMachineLearningPlatformMichelangelo"},"buildId":"kkckKPyxcjJp_o8wb2unW","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>