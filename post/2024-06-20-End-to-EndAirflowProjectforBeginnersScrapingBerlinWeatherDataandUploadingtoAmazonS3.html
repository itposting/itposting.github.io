<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>초보자를 위한 엔드 투 엔드 Airflow 프로젝트 베를린 날씨 데이터 스크래핑 및 Amazon S3에 업로드하기 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="초보자를 위한 엔드 투 엔드 Airflow 프로젝트 베를린 날씨 데이터 스크래핑 및 Amazon S3에 업로드하기 | itposting" data-gatsby-head="true"/><meta property="og:title" content="초보자를 위한 엔드 투 엔드 Airflow 프로젝트 베를린 날씨 데이터 스크래핑 및 Amazon S3에 업로드하기 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3" data-gatsby-head="true"/><meta name="twitter:title" content="초보자를 위한 엔드 투 엔드 Airflow 프로젝트 베를린 날씨 데이터 스크래핑 및 Amazon S3에 업로드하기 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-20 15:21" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_buildManifest.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">초보자를 위한 엔드 투 엔드 Airflow 프로젝트 베를린 날씨 데이터 스크래핑 및 Amazon S3에 업로드하기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="초보자를 위한 엔드 투 엔드 Airflow 프로젝트 베를린 날씨 데이터 스크래핑 및 Amazon S3에 업로드하기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 20, 2024</span><span class="posts_reading_time__f7YPP">7<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>파이썬 개발과 Apache Airflow에 열정을 가진 데이터 엔지니어로서, 베를린의 최신 날씨 데이터를 가져 와 CSV 파일로 저장하고 Amazon S3로 업로드하는 프로젝트를 시작했습니다. 이 튜토리얼에서는 Python, 웹 스크래핑을 위한 BeautifulSoup, 데이터 조작을 위한 Pandas, 그리고 오케스트레이션을 위한 Airflow를 사용한 전체 설정 및 구현 방법을 안내해 드릴 겁니다.</p>
<h1>프로젝트 개요</h1>
<p>이 프로젝트에서는 다음을 목표로 합니다:</p>
<ul>
<li>날씨 데이터 스크래핑: 날씨 웹사이트에서 베를린의 실시간 날씨 정보를 가져 오는 웹 스크래핑 기술을 활용합니다.</li>
<li>데이터 로컬 저장: 가져온 데이터를 로컬 파일 시스템의 CSV 파일에 저장합니다.</li>
<li>Amazon S3로 업로드: 날씨 데이터가 포함된 CSV 파일을 Amazon S3 버킷에 업로드하는 메커니즘을 구현합니다.</li>
<li>Airflow로 자동화: Apache Airflow를 사용하여 매 시간마다 데이터 가져오기와 업로드 프로세스를 자동화하고 예약합니다.</li>
</ul>
<div class="content-ad"></div>
<h1>사용된 도구 및 기술</h1>
<ul>
<li>Python: 스크립팅 및 데이터 조작에 사용됩니다.</li>
<li>BeautifulSoup: HTML 및 XML 문서 구문 분석을 위한 Python 라이브러리로, 여기서 웹 스크래핑에 사용됩니다.</li>
<li>Pandas: 파이썬에서 데이터를 분석하고 조작하는 강력한 도구로, 표 형식의 데이터를 처리하고 다루는 데 활용됩니다.</li>
<li>Apache Airflow: 워크플로우를 프로그래밍적으로 작성, 예약 및 모니터링하는 오픈 소스 도구입니다.</li>
<li>Amazon S3: 스케일링 가능한 객체 저장 서비스인 Amazon Simple Storage Service로, 데이터를 저장하고 검색하는 데 사용됩니다.</li>
</ul>
<h1>구현 단계별 안내</h1>
<h1>1. 환경 설정하기</h1>
<div class="content-ad"></div>
<p>Python이 설치되어 있고 필요한 라이브러리(requests, beautifulsoup4, pandas, AWS SDK의 boto3)가 함께 설치되었는지 확인하기 위해 다음 명령을 실행해주세요:</p>
<pre><code class="hljs language-js">pip install requests beautifulsoup4 pandas boto3
</code></pre>
<h1>2. 날씨 데이터 수집 및 로컬에 데이터 저장</h1>
<p>추출한 날씨 데이터를 Pandas를 사용하여 로컬 CSV 파일에 저장하세요:</p>
<div class="content-ad"></div>
<p>'Alex The Analyst' YouTube 채널에서 BeautifulSoup를 사용하여 스크랩을 배웠어요. 완성된 재생 목록을 확인해보세요.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> <span class="hljs-title class_">BeautifulSoup</span>
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> pytz
</code></pre>
<pre><code class="hljs language-js"># 날씨 데이터를 업데이트하고 <span class="hljs-variable constant_">CSV</span>로 저장하는 함수
def <span class="hljs-title function_">update_weather</span>(**kwargs):
    url = <span class="hljs-string">'https://weather.com/weather/today/l/52.52,13.40'</span>
    page = requests.<span class="hljs-title function_">get</span>(url)
    <span class="hljs-keyword">if</span> page.<span class="hljs-property">status_code</span> != <span class="hljs-number">200</span>:
        raise <span class="hljs-title class_">Exception</span>(f<span class="hljs-string">"페이지를 가져오지 못했습니다: {page.status_code}"</span>)
    soup = <span class="hljs-title class_">BeautifulSoup</span>(page.<span class="hljs-property">text</span>, <span class="hljs-string">'html.parser'</span>)
    # 체감 온도에 대한 제목 찾기
    title_element = soup.<span class="hljs-title function_">find</span>(class_=<span class="hljs-string">'TodayDetailsCard--feelsLikeTempLabel--1UNV1'</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-attr">title_element</span>:
        title = title_element.<span class="hljs-property">text</span>.<span class="hljs-title function_">strip</span>()
    <span class="hljs-attr">else</span>:
        title = <span class="hljs-string">"체감 온도"</span>
    # <span class="hljs-title class_">DataFrame</span>을 저장할 파일 경로
    file_path = <span class="hljs-string">'/opt/airflow/dags/weather_checkin.csv'</span>
    # 파일이 있는지 확인
    <span class="hljs-keyword">if</span> os.<span class="hljs-property">path</span>.<span class="hljs-title function_">exists</span>(file_path):
        logging.<span class="hljs-title function_">info</span>(f<span class="hljs-string">"파일 {file_path}이 존재합니다. 기존 DataFrame을 불러옵니다."</span>)
        # 기존 <span class="hljs-title class_">DataFrame</span> 불러오기
        current_weather_berlin_df = pd.<span class="hljs-title function_">read_csv</span>(file_path)
    <span class="hljs-attr">else</span>:
        logging.<span class="hljs-title function_">info</span>(f<span class="hljs-string">"파일 {file_path}이 존재하지 않습니다. 새 DataFrame을 생성합니다."</span>)
        # 제목과 날짜 및 시간 열을 가진 새 <span class="hljs-title class_">DataFrame</span> 초기화
        current_weather_berlin_df = pd.<span class="hljs-title class_">DataFrame</span>(columns=[title, <span class="hljs-string">'date_time'</span>])
    # 체감 온도 값 찾기
    value_element = soup.<span class="hljs-title function_">find</span>(<span class="hljs-string">'span'</span>, class_=<span class="hljs-string">'TodayDetailsCard--feelsLikeTempValue--2icPt'</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-attr">value_element</span>:
        feels_like_temp = value_element.<span class="hljs-property">text</span>.<span class="hljs-title function_">strip</span>()
    <span class="hljs-attr">else</span>:
        feels_like_temp = <span class="hljs-title class_">None</span>
    # 베를린 시간대의 현재 날짜 및 시간 가져오기
    berlin_tz = pytz.<span class="hljs-title function_">timezone</span>(<span class="hljs-string">'Europe/Berlin'</span>)
    current_datetime = datetime.<span class="hljs-title function_">now</span>(berlin_tz).<span class="hljs-title function_">strftime</span>(<span class="hljs-string">"%Y-%m-%d %H:%M:%S"</span>)
    # <span class="hljs-title class_">DataFrame</span>에 데이터 추가
    <span class="hljs-keyword">if</span> <span class="hljs-attr">feels_like_temp</span>:
        logging.<span class="hljs-title function_">info</span>(f<span class="hljs-string">"새 데이터 추가 중: {feels_like_temp}, {current_datetime}"</span>)
        new_data = {<span class="hljs-attr">title</span>: [feels_like_temp], <span class="hljs-string">'date_time'</span>: [current_datetime]}
        current_weather_berlin_df = current_weather_berlin_df.<span class="hljs-title function_">append</span>(pd.<span class="hljs-title class_">DataFrame</span>(new_data), ignore_index=<span class="hljs-title class_">True</span>)
    <span class="hljs-attr">else</span>:
        logging.<span class="hljs-title function_">error</span>(<span class="hljs-string">"체감 온도 값 찾기를 실패했습니다"</span>)
    # <span class="hljs-title class_">DataFrame</span>을 <span class="hljs-variable constant_">CSV</span> 파일로 저장
    logging.<span class="hljs-title function_">info</span>(f<span class="hljs-string">"{file_path}에 DataFrame을 저장 중"</span>)
    current_weather_berlin_df.<span class="hljs-title function_">to_csv</span>(file_path, index=<span class="hljs-title class_">False</span>)
    logging.<span class="hljs-title function_">info</span>(current_weather_berlin_df)
    # <span class="hljs-variable constant_">S3</span>에 <span class="hljs-variable constant_">CSV</span> 업로드
    bucket_name = <span class="hljs-string">'myfirstbucketsoumya'</span>  # <span class="hljs-variable constant_">S3</span> 버킷 이름으로 대체
    s3_key = <span class="hljs-string">'current_weather_berlin.csv'</span>  # 원하는 <span class="hljs-variable constant_">S3</span> 키로 대체
    <span class="hljs-title function_">upload_to_s3</span>(file_path, bucket_name, s3_key)
</code></pre>
<h1>3. Amazon S3로 업로드하기</h1>
<div class="content-ad"></div>
<p>아래는 boto3를 사용하여 CSV 파일을 Amazon S3에 업로드하는 기능을 구현한 것입니다:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> boto3
</code></pre>
<pre><code class="hljs language-python"><span class="hljs-comment"># AWS 자격 증명</span>
AWS_ACCESS_KEY_ID = <span class="hljs-string">'your-access-key-id'</span> <span class="hljs-comment"># 자격 증명을 하드코딩합니다.</span>
AWS_SECRET_ACCESS_KEY = <span class="hljs-string">'your-secret-access-key'</span>
AWS_REGION = <span class="hljs-string">'eu-central-1'</span>
<span class="hljs-comment"># 현재 날씨 CSV를 S3에 업로드하는 함수</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">upload_to_s3</span>(<span class="hljs-params">file_path, bucket_name, s3_key</span>):
    <span class="hljs-keyword">try</span>:
        <span class="hljs-comment"># 자격 증명을 사용하여 Amazon S3와의 세션을 초기화합니다.</span>
        s3 = boto3.client(
            <span class="hljs-string">'s3'</span>,
            aws_access_key_id=<span class="hljs-string">'your-access-key-id'</span>, <span class="hljs-comment"># 값을 하드코딩합니다.</span>
            aws_secret_access_key=<span class="hljs-string">'your-secret-access-key'</span>, <span class="hljs-comment"># 값을 하드코딩합니다.</span>
            region_name=<span class="hljs-string">'eu-central-1'</span> <span class="hljs-comment"># 값을 하드코딩합니다.</span>
        )
        bucket_name = <span class="hljs-string">'myfirstbucketsoumya'</span>
        file_key = <span class="hljs-string">'hourly_berlin_weather.txt'</span>
        <span class="hljs-comment"># CSV 파일을 S3에 업로드합니다.</span>
        s3.upload_file(file_path, bucket_name, s3_key)
        logging.info(<span class="hljs-string">f"날씨 데이터를 S3에 업로드했습니다: s3://<span class="hljs-subst">{bucket_name}</span>/<span class="hljs-subst">{s3_key}</span>"</span>)
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        logging.error(<span class="hljs-string">f"S3로 업로드 실패: <span class="hljs-subst">{e}</span>"</span>)
</code></pre>
<p>docker-compose.yaml에 몇 가지 변경 사항이 있습니다. AWS 자격 증명과 requirements.txt 컨테이너를 업데이트하십시오.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">#변경 <span class="hljs-number">1</span>
x-airflow-<span class="hljs-attr">common</span>:
  &#x26;airflow-common
  <span class="hljs-attr">image</span>: ${<span class="hljs-attr">AIRFLOW_IMAGE_NAME</span>:-apache/<span class="hljs-attr">airflow</span>:<span class="hljs-number">2.9</span><span class="hljs-number">.1</span>}
  <span class="hljs-attr">environment</span>:
    &#x26;airflow-common-env
    <span class="hljs-attr">PYTHONPATH</span>: <span class="hljs-regexp">/opt/</span>airflow/dags/airflow_env_bs/lib/python3<span class="hljs-number">.12</span>/site-packages
 
    #<span class="hljs-attr">AWS_ACCESS_KEY_ID</span>: your-access-key-id #값을 하드코딩
    #<span class="hljs-attr">AWS_SECRET_ACCESS_KEY</span>: your-secret-access-key #값을 하드코딩
    #<span class="hljs-attr">AWS_REGION</span>: eu-central-<span class="hljs-number">1</span> #값을 하드코딩

#변경-<span class="hljs-number">2</span>
airflow-<span class="hljs-attr">init</span>:
    &#x3C;&#x3C;: *airflow-common
    <span class="hljs-attr">entrypoint</span>: <span class="hljs-regexp">/bin/</span>bash
    <span class="hljs-attr">command</span>: >
      -c <span class="hljs-string">"pip install -r /requirements.txt &#x26;&#x26; airflow webserver"</span> 
</code></pre>
<h1>5. Apache Airflow로 자동화하기</h1>
<p>마지막으로 Apache Airflow를 사용하여 전체 프로세스를 조율하세요. 다음은 DAG를 정의하는 방법입니다:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">from</span> airflow <span class="hljs-keyword">import</span> <span class="hljs-variable constant_">DAG</span>
<span class="hljs-keyword">from</span> airflow.<span class="hljs-property">operators</span>.<span class="hljs-property">python_operator</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">PythonOperator</span>
</code></pre>
<div class="content-ad"></div>
<pre><code class="hljs language-js"># <span class="hljs-variable constant_">DAG</span> 정의
<span class="hljs-variable constant_">DAG_NAME</span> = <span class="hljs-string">'berlin-weather'</span>
default_args = 
{<span class="hljs-string">'owner'</span>: <span class="hljs-string">'airflow'</span>,
<span class="hljs-string">'depends_on_past'</span>: <span class="hljs-title class_">False</span>,
<span class="hljs-string">'start_date'</span>: <span class="hljs-title function_">datetime</span>(<span class="hljs-number">2023</span>, <span class="hljs-number">6</span>, <span class="hljs-number">19</span>),
<span class="hljs-string">'retries'</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">'retry_delay'</span>: <span class="hljs-title function_">timedelta</span>(minutes=<span class="hljs-number">5</span>),
}
dag = <span class="hljs-title function_">DAG</span>(
dag_id=<span class="hljs-variable constant_">DAG_NAME</span>,
description=<span class="hljs-string">'베를린 날씨 매 시간 갱신'</span>,
schedule_interval=<span class="hljs-string">'@hourly'</span>,
default_args=default_args,
catchup=<span class="hljs-title class_">False</span>,
)
# 작업 정의
update_weather_task = <span class="hljs-title class_">PythonOperator</span>(
task_id=<span class="hljs-string">'update_weather'</span>,
python_callable=update_weather,
dag=dag,
)
# 작업 의존성
update_weather_task
</code></pre>
<h1>5. CSV 파일을 위한 S3 버킷 확인</h1>
<p>Airflow 웹 서버를 열고 DAG를 실행하세요.</p>
<img src="/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_0.png">
<div class="content-ad"></div>
<p>DAG를 트리거한 후에 "current_weather_berlin.csv"라는 S3 Bucket을 확인해보세요. 거기에는 데이터 폴더가 있을 겁니다.</p>
<p><img src="/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_1.png" alt="이미지"></p>
<h1>결론</h1>
<p>이 프로젝트에서는 Python을 사용하여 베를린 날씨 데이터를 가져오고 로컬에 저장하며 Amazon S3로 업로드하는 프로세스를 자동화하는 방법을 탐색했습니다. 이를 위해 웹 스크래핑용 BeautifulSoup, 데이터 처리용 Pandas, 그리고 워크플로우 자동화용 Apache Airflow를 사용했습니다. 이러한 단계를 따라가면 이 프로젝트를 적응하고 확장하여 보다 복잡한 데이터 파이프라인 및 통합을 처리할 수 있습니다.</p>
<div class="content-ad"></div>
<p>제 Github 저장소를 확인하러 가보세요: Scraping-Berlin-Weather-Data-and-Uploading-to-Amazon-S3</p>
<p>태그: airflow, 데이터 엔지니어링 프로젝트, AWS S3, 도커, 초보자 프로젝트</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"초보자를 위한 엔드 투 엔드 Airflow 프로젝트 베를린 날씨 데이터 스크래핑 및 Amazon S3에 업로드하기","description":"","date":"2024-06-20 15:21","slug":"2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3","content":"\n\n파이썬 개발과 Apache Airflow에 열정을 가진 데이터 엔지니어로서, 베를린의 최신 날씨 데이터를 가져 와 CSV 파일로 저장하고 Amazon S3로 업로드하는 프로젝트를 시작했습니다. 이 튜토리얼에서는 Python, 웹 스크래핑을 위한 BeautifulSoup, 데이터 조작을 위한 Pandas, 그리고 오케스트레이션을 위한 Airflow를 사용한 전체 설정 및 구현 방법을 안내해 드릴 겁니다.\n\n# 프로젝트 개요\n\n이 프로젝트에서는 다음을 목표로 합니다:\n\n- 날씨 데이터 스크래핑: 날씨 웹사이트에서 베를린의 실시간 날씨 정보를 가져 오는 웹 스크래핑 기술을 활용합니다.\n- 데이터 로컬 저장: 가져온 데이터를 로컬 파일 시스템의 CSV 파일에 저장합니다.\n- Amazon S3로 업로드: 날씨 데이터가 포함된 CSV 파일을 Amazon S3 버킷에 업로드하는 메커니즘을 구현합니다.\n- Airflow로 자동화: Apache Airflow를 사용하여 매 시간마다 데이터 가져오기와 업로드 프로세스를 자동화하고 예약합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 사용된 도구 및 기술\n\n- Python: 스크립팅 및 데이터 조작에 사용됩니다.\n- BeautifulSoup: HTML 및 XML 문서 구문 분석을 위한 Python 라이브러리로, 여기서 웹 스크래핑에 사용됩니다.\n- Pandas: 파이썬에서 데이터를 분석하고 조작하는 강력한 도구로, 표 형식의 데이터를 처리하고 다루는 데 활용됩니다.\n- Apache Airflow: 워크플로우를 프로그래밍적으로 작성, 예약 및 모니터링하는 오픈 소스 도구입니다.\n- Amazon S3: 스케일링 가능한 객체 저장 서비스인 Amazon Simple Storage Service로, 데이터를 저장하고 검색하는 데 사용됩니다.\n\n# 구현 단계별 안내\n\n# 1. 환경 설정하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPython이 설치되어 있고 필요한 라이브러리(requests, beautifulsoup4, pandas, AWS SDK의 boto3)가 함께 설치되었는지 확인하기 위해 다음 명령을 실행해주세요:\n\n```js\npip install requests beautifulsoup4 pandas boto3\n```\n\n# 2. 날씨 데이터 수집 및 로컬에 데이터 저장\n\n추출한 날씨 데이터를 Pandas를 사용하여 로컬 CSV 파일에 저장하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n'Alex The Analyst' YouTube 채널에서 BeautifulSoup를 사용하여 스크랩을 배웠어요. 완성된 재생 목록을 확인해보세요.\n\n```js\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nimport pytz\n```\n\n```js\n# 날씨 데이터를 업데이트하고 CSV로 저장하는 함수\ndef update_weather(**kwargs):\n    url = 'https://weather.com/weather/today/l/52.52,13.40'\n    page = requests.get(url)\n    if page.status_code != 200:\n        raise Exception(f\"페이지를 가져오지 못했습니다: {page.status_code}\")\n    soup = BeautifulSoup(page.text, 'html.parser')\n    # 체감 온도에 대한 제목 찾기\n    title_element = soup.find(class_='TodayDetailsCard--feelsLikeTempLabel--1UNV1')\n    if title_element:\n        title = title_element.text.strip()\n    else:\n        title = \"체감 온도\"\n    # DataFrame을 저장할 파일 경로\n    file_path = '/opt/airflow/dags/weather_checkin.csv'\n    # 파일이 있는지 확인\n    if os.path.exists(file_path):\n        logging.info(f\"파일 {file_path}이 존재합니다. 기존 DataFrame을 불러옵니다.\")\n        # 기존 DataFrame 불러오기\n        current_weather_berlin_df = pd.read_csv(file_path)\n    else:\n        logging.info(f\"파일 {file_path}이 존재하지 않습니다. 새 DataFrame을 생성합니다.\")\n        # 제목과 날짜 및 시간 열을 가진 새 DataFrame 초기화\n        current_weather_berlin_df = pd.DataFrame(columns=[title, 'date_time'])\n    # 체감 온도 값 찾기\n    value_element = soup.find('span', class_='TodayDetailsCard--feelsLikeTempValue--2icPt')\n    if value_element:\n        feels_like_temp = value_element.text.strip()\n    else:\n        feels_like_temp = None\n    # 베를린 시간대의 현재 날짜 및 시간 가져오기\n    berlin_tz = pytz.timezone('Europe/Berlin')\n    current_datetime = datetime.now(berlin_tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n    # DataFrame에 데이터 추가\n    if feels_like_temp:\n        logging.info(f\"새 데이터 추가 중: {feels_like_temp}, {current_datetime}\")\n        new_data = {title: [feels_like_temp], 'date_time': [current_datetime]}\n        current_weather_berlin_df = current_weather_berlin_df.append(pd.DataFrame(new_data), ignore_index=True)\n    else:\n        logging.error(\"체감 온도 값 찾기를 실패했습니다\")\n    # DataFrame을 CSV 파일로 저장\n    logging.info(f\"{file_path}에 DataFrame을 저장 중\")\n    current_weather_berlin_df.to_csv(file_path, index=False)\n    logging.info(current_weather_berlin_df)\n    # S3에 CSV 업로드\n    bucket_name = 'myfirstbucketsoumya'  # S3 버킷 이름으로 대체\n    s3_key = 'current_weather_berlin.csv'  # 원하는 S3 키로 대체\n    upload_to_s3(file_path, bucket_name, s3_key)\n```\n\n# 3. Amazon S3로 업로드하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 boto3를 사용하여 CSV 파일을 Amazon S3에 업로드하는 기능을 구현한 것입니다:\n\n```python\nimport boto3\n```\n\n```python\n# AWS 자격 증명\nAWS_ACCESS_KEY_ID = 'your-access-key-id' # 자격 증명을 하드코딩합니다.\nAWS_SECRET_ACCESS_KEY = 'your-secret-access-key'\nAWS_REGION = 'eu-central-1'\n# 현재 날씨 CSV를 S3에 업로드하는 함수\ndef upload_to_s3(file_path, bucket_name, s3_key):\n    try:\n        # 자격 증명을 사용하여 Amazon S3와의 세션을 초기화합니다.\n        s3 = boto3.client(\n            's3',\n            aws_access_key_id='your-access-key-id', # 값을 하드코딩합니다.\n            aws_secret_access_key='your-secret-access-key', # 값을 하드코딩합니다.\n            region_name='eu-central-1' # 값을 하드코딩합니다.\n        )\n        bucket_name = 'myfirstbucketsoumya'\n        file_key = 'hourly_berlin_weather.txt'\n        # CSV 파일을 S3에 업로드합니다.\n        s3.upload_file(file_path, bucket_name, s3_key)\n        logging.info(f\"날씨 데이터를 S3에 업로드했습니다: s3://{bucket_name}/{s3_key}\")\n    except Exception as e:\n        logging.error(f\"S3로 업로드 실패: {e}\")\n```\n\ndocker-compose.yaml에 몇 가지 변경 사항이 있습니다. AWS 자격 증명과 requirements.txt 컨테이너를 업데이트하십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n#변경 1\nx-airflow-common:\n  \u0026airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.9.1}\n  environment:\n    \u0026airflow-common-env\n    PYTHONPATH: /opt/airflow/dags/airflow_env_bs/lib/python3.12/site-packages\n \n    #AWS_ACCESS_KEY_ID: your-access-key-id #값을 하드코딩\n    #AWS_SECRET_ACCESS_KEY: your-secret-access-key #값을 하드코딩\n    #AWS_REGION: eu-central-1 #값을 하드코딩\n\n#변경-2\nairflow-init:\n    \u003c\u003c: *airflow-common\n    entrypoint: /bin/bash\n    command: \u003e\n      -c \"pip install -r /requirements.txt \u0026\u0026 airflow webserver\" \n```\n\n# 5. Apache Airflow로 자동화하기\n\n마지막으로 Apache Airflow를 사용하여 전체 프로세스를 조율하세요. 다음은 DAG를 정의하는 방법입니다:\n\n```js\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# DAG 정의\nDAG_NAME = 'berlin-weather'\ndefault_args = \n{'owner': 'airflow',\n'depends_on_past': False,\n'start_date': datetime(2023, 6, 19),\n'retries': 1,\n'retry_delay': timedelta(minutes=5),\n}\ndag = DAG(\ndag_id=DAG_NAME,\ndescription='베를린 날씨 매 시간 갱신',\nschedule_interval='@hourly',\ndefault_args=default_args,\ncatchup=False,\n)\n# 작업 정의\nupdate_weather_task = PythonOperator(\ntask_id='update_weather',\npython_callable=update_weather,\ndag=dag,\n)\n# 작업 의존성\nupdate_weather_task\n```\n\n# 5. CSV 파일을 위한 S3 버킷 확인\n\nAirflow 웹 서버를 열고 DAG를 실행하세요.\n\n\u003cimg src=\"/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_0.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDAG를 트리거한 후에 \"current_weather_berlin.csv\"라는 S3 Bucket을 확인해보세요. 거기에는 데이터 폴더가 있을 겁니다.\n\n![이미지](/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_1.png)\n\n# 결론\n\n이 프로젝트에서는 Python을 사용하여 베를린 날씨 데이터를 가져오고 로컬에 저장하며 Amazon S3로 업로드하는 프로세스를 자동화하는 방법을 탐색했습니다. 이를 위해 웹 스크래핑용 BeautifulSoup, 데이터 처리용 Pandas, 그리고 워크플로우 자동화용 Apache Airflow를 사용했습니다. 이러한 단계를 따라가면 이 프로젝트를 적응하고 확장하여 보다 복잡한 데이터 파이프라인 및 통합을 처리할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 Github 저장소를 확인하러 가보세요: Scraping-Berlin-Weather-Data-and-Uploading-to-Amazon-S3\n\n태그: airflow, 데이터 엔지니어링 프로젝트, AWS S3, 도커, 초보자 프로젝트","ogImage":{"url":"/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_0.png"},"coverImage":"/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_0.png","tag":["Tech"],"readingTime":7},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e파이썬 개발과 Apache Airflow에 열정을 가진 데이터 엔지니어로서, 베를린의 최신 날씨 데이터를 가져 와 CSV 파일로 저장하고 Amazon S3로 업로드하는 프로젝트를 시작했습니다. 이 튜토리얼에서는 Python, 웹 스크래핑을 위한 BeautifulSoup, 데이터 조작을 위한 Pandas, 그리고 오케스트레이션을 위한 Airflow를 사용한 전체 설정 및 구현 방법을 안내해 드릴 겁니다.\u003c/p\u003e\n\u003ch1\u003e프로젝트 개요\u003c/h1\u003e\n\u003cp\u003e이 프로젝트에서는 다음을 목표로 합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e날씨 데이터 스크래핑: 날씨 웹사이트에서 베를린의 실시간 날씨 정보를 가져 오는 웹 스크래핑 기술을 활용합니다.\u003c/li\u003e\n\u003cli\u003e데이터 로컬 저장: 가져온 데이터를 로컬 파일 시스템의 CSV 파일에 저장합니다.\u003c/li\u003e\n\u003cli\u003eAmazon S3로 업로드: 날씨 데이터가 포함된 CSV 파일을 Amazon S3 버킷에 업로드하는 메커니즘을 구현합니다.\u003c/li\u003e\n\u003cli\u003eAirflow로 자동화: Apache Airflow를 사용하여 매 시간마다 데이터 가져오기와 업로드 프로세스를 자동화하고 예약합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e사용된 도구 및 기술\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003ePython: 스크립팅 및 데이터 조작에 사용됩니다.\u003c/li\u003e\n\u003cli\u003eBeautifulSoup: HTML 및 XML 문서 구문 분석을 위한 Python 라이브러리로, 여기서 웹 스크래핑에 사용됩니다.\u003c/li\u003e\n\u003cli\u003ePandas: 파이썬에서 데이터를 분석하고 조작하는 강력한 도구로, 표 형식의 데이터를 처리하고 다루는 데 활용됩니다.\u003c/li\u003e\n\u003cli\u003eApache Airflow: 워크플로우를 프로그래밍적으로 작성, 예약 및 모니터링하는 오픈 소스 도구입니다.\u003c/li\u003e\n\u003cli\u003eAmazon S3: 스케일링 가능한 객체 저장 서비스인 Amazon Simple Storage Service로, 데이터를 저장하고 검색하는 데 사용됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e구현 단계별 안내\u003c/h1\u003e\n\u003ch1\u003e1. 환경 설정하기\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003ePython이 설치되어 있고 필요한 라이브러리(requests, beautifulsoup4, pandas, AWS SDK의 boto3)가 함께 설치되었는지 확인하기 위해 다음 명령을 실행해주세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epip install requests beautifulsoup4 pandas boto3\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e2. 날씨 데이터 수집 및 로컬에 데이터 저장\u003c/h1\u003e\n\u003cp\u003e추출한 날씨 데이터를 Pandas를 사용하여 로컬 CSV 파일에 저장하세요:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e'Alex The Analyst' YouTube 채널에서 BeautifulSoup를 사용하여 스크랩을 배웠어요. 완성된 재생 목록을 확인해보세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e bs4 \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eBeautifulSoup\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e requests\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e pd\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pytz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 날씨 데이터를 업데이트하고 \u003cspan class=\"hljs-variable constant_\"\u003eCSV\u003c/span\u003e로 저장하는 함수\ndef \u003cspan class=\"hljs-title function_\"\u003eupdate_weather\u003c/span\u003e(**kwargs):\n    url = \u003cspan class=\"hljs-string\"\u003e'https://weather.com/weather/today/l/52.52,13.40'\u003c/span\u003e\n    page = requests.\u003cspan class=\"hljs-title function_\"\u003eget\u003c/span\u003e(url)\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e page.\u003cspan class=\"hljs-property\"\u003estatus_code\u003c/span\u003e != \u003cspan class=\"hljs-number\"\u003e200\u003c/span\u003e:\n        raise \u003cspan class=\"hljs-title class_\"\u003eException\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"페이지를 가져오지 못했습니다: {page.status_code}\"\u003c/span\u003e)\n    soup = \u003cspan class=\"hljs-title class_\"\u003eBeautifulSoup\u003c/span\u003e(page.\u003cspan class=\"hljs-property\"\u003etext\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'html.parser'\u003c/span\u003e)\n    # 체감 온도에 대한 제목 찾기\n    title_element = soup.\u003cspan class=\"hljs-title function_\"\u003efind\u003c/span\u003e(class_=\u003cspan class=\"hljs-string\"\u003e'TodayDetailsCard--feelsLikeTempLabel--1UNV1'\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003etitle_element\u003c/span\u003e:\n        title = title_element.\u003cspan class=\"hljs-property\"\u003etext\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003estrip\u003c/span\u003e()\n    \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n        title = \u003cspan class=\"hljs-string\"\u003e\"체감 온도\"\u003c/span\u003e\n    # \u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e을 저장할 파일 경로\n    file_path = \u003cspan class=\"hljs-string\"\u003e'/opt/airflow/dags/weather_checkin.csv'\u003c/span\u003e\n    # 파일이 있는지 확인\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e os.\u003cspan class=\"hljs-property\"\u003epath\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eexists\u003c/span\u003e(file_path):\n        logging.\u003cspan class=\"hljs-title function_\"\u003einfo\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"파일 {file_path}이 존재합니다. 기존 DataFrame을 불러옵니다.\"\u003c/span\u003e)\n        # 기존 \u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e 불러오기\n        current_weather_berlin_df = pd.\u003cspan class=\"hljs-title function_\"\u003eread_csv\u003c/span\u003e(file_path)\n    \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n        logging.\u003cspan class=\"hljs-title function_\"\u003einfo\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"파일 {file_path}이 존재하지 않습니다. 새 DataFrame을 생성합니다.\"\u003c/span\u003e)\n        # 제목과 날짜 및 시간 열을 가진 새 \u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e 초기화\n        current_weather_berlin_df = pd.\u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e(columns=[title, \u003cspan class=\"hljs-string\"\u003e'date_time'\u003c/span\u003e])\n    # 체감 온도 값 찾기\n    value_element = soup.\u003cspan class=\"hljs-title function_\"\u003efind\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'span'\u003c/span\u003e, class_=\u003cspan class=\"hljs-string\"\u003e'TodayDetailsCard--feelsLikeTempValue--2icPt'\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003evalue_element\u003c/span\u003e:\n        feels_like_temp = value_element.\u003cspan class=\"hljs-property\"\u003etext\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003estrip\u003c/span\u003e()\n    \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n        feels_like_temp = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e\n    # 베를린 시간대의 현재 날짜 및 시간 가져오기\n    berlin_tz = pytz.\u003cspan class=\"hljs-title function_\"\u003etimezone\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'Europe/Berlin'\u003c/span\u003e)\n    current_datetime = datetime.\u003cspan class=\"hljs-title function_\"\u003enow\u003c/span\u003e(berlin_tz).\u003cspan class=\"hljs-title function_\"\u003estrftime\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"%Y-%m-%d %H:%M:%S\"\u003c/span\u003e)\n    # \u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e에 데이터 추가\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003efeels_like_temp\u003c/span\u003e:\n        logging.\u003cspan class=\"hljs-title function_\"\u003einfo\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"새 데이터 추가 중: {feels_like_temp}, {current_datetime}\"\u003c/span\u003e)\n        new_data = {\u003cspan class=\"hljs-attr\"\u003etitle\u003c/span\u003e: [feels_like_temp], \u003cspan class=\"hljs-string\"\u003e'date_time'\u003c/span\u003e: [current_datetime]}\n        current_weather_berlin_df = current_weather_berlin_df.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(pd.\u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e(new_data), ignore_index=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n    \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n        logging.\u003cspan class=\"hljs-title function_\"\u003eerror\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"체감 온도 값 찾기를 실패했습니다\"\u003c/span\u003e)\n    # \u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e을 \u003cspan class=\"hljs-variable constant_\"\u003eCSV\u003c/span\u003e 파일로 저장\n    logging.\u003cspan class=\"hljs-title function_\"\u003einfo\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"{file_path}에 DataFrame을 저장 중\"\u003c/span\u003e)\n    current_weather_berlin_df.\u003cspan class=\"hljs-title function_\"\u003eto_csv\u003c/span\u003e(file_path, index=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\n    logging.\u003cspan class=\"hljs-title function_\"\u003einfo\u003c/span\u003e(current_weather_berlin_df)\n    # \u003cspan class=\"hljs-variable constant_\"\u003eS3\u003c/span\u003e에 \u003cspan class=\"hljs-variable constant_\"\u003eCSV\u003c/span\u003e 업로드\n    bucket_name = \u003cspan class=\"hljs-string\"\u003e'myfirstbucketsoumya'\u003c/span\u003e  # \u003cspan class=\"hljs-variable constant_\"\u003eS3\u003c/span\u003e 버킷 이름으로 대체\n    s3_key = \u003cspan class=\"hljs-string\"\u003e'current_weather_berlin.csv'\u003c/span\u003e  # 원하는 \u003cspan class=\"hljs-variable constant_\"\u003eS3\u003c/span\u003e 키로 대체\n    \u003cspan class=\"hljs-title function_\"\u003eupload_to_s3\u003c/span\u003e(file_path, bucket_name, s3_key)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e3. Amazon S3로 업로드하기\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래는 boto3를 사용하여 CSV 파일을 Amazon S3에 업로드하는 기능을 구현한 것입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e boto3\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# AWS 자격 증명\u003c/span\u003e\nAWS_ACCESS_KEY_ID = \u003cspan class=\"hljs-string\"\u003e'your-access-key-id'\u003c/span\u003e \u003cspan class=\"hljs-comment\"\u003e# 자격 증명을 하드코딩합니다.\u003c/span\u003e\nAWS_SECRET_ACCESS_KEY = \u003cspan class=\"hljs-string\"\u003e'your-secret-access-key'\u003c/span\u003e\nAWS_REGION = \u003cspan class=\"hljs-string\"\u003e'eu-central-1'\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# 현재 날씨 CSV를 S3에 업로드하는 함수\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eupload_to_s3\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003efile_path, bucket_name, s3_key\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003etry\u003c/span\u003e:\n        \u003cspan class=\"hljs-comment\"\u003e# 자격 증명을 사용하여 Amazon S3와의 세션을 초기화합니다.\u003c/span\u003e\n        s3 = boto3.client(\n            \u003cspan class=\"hljs-string\"\u003e's3'\u003c/span\u003e,\n            aws_access_key_id=\u003cspan class=\"hljs-string\"\u003e'your-access-key-id'\u003c/span\u003e, \u003cspan class=\"hljs-comment\"\u003e# 값을 하드코딩합니다.\u003c/span\u003e\n            aws_secret_access_key=\u003cspan class=\"hljs-string\"\u003e'your-secret-access-key'\u003c/span\u003e, \u003cspan class=\"hljs-comment\"\u003e# 값을 하드코딩합니다.\u003c/span\u003e\n            region_name=\u003cspan class=\"hljs-string\"\u003e'eu-central-1'\u003c/span\u003e \u003cspan class=\"hljs-comment\"\u003e# 값을 하드코딩합니다.\u003c/span\u003e\n        )\n        bucket_name = \u003cspan class=\"hljs-string\"\u003e'myfirstbucketsoumya'\u003c/span\u003e\n        file_key = \u003cspan class=\"hljs-string\"\u003e'hourly_berlin_weather.txt'\u003c/span\u003e\n        \u003cspan class=\"hljs-comment\"\u003e# CSV 파일을 S3에 업로드합니다.\u003c/span\u003e\n        s3.upload_file(file_path, bucket_name, s3_key)\n        logging.info(\u003cspan class=\"hljs-string\"\u003ef\"날씨 데이터를 S3에 업로드했습니다: s3://\u003cspan class=\"hljs-subst\"\u003e{bucket_name}\u003c/span\u003e/\u003cspan class=\"hljs-subst\"\u003e{s3_key}\u003c/span\u003e\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003eexcept\u003c/span\u003e Exception \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e e:\n        logging.error(\u003cspan class=\"hljs-string\"\u003ef\"S3로 업로드 실패: \u003cspan class=\"hljs-subst\"\u003e{e}\u003c/span\u003e\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003edocker-compose.yaml에 몇 가지 변경 사항이 있습니다. AWS 자격 증명과 requirements.txt 컨테이너를 업데이트하십시오.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e#변경 \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\nx-airflow-\u003cspan class=\"hljs-attr\"\u003ecommon\u003c/span\u003e:\n  \u0026#x26;airflow-common\n  \u003cspan class=\"hljs-attr\"\u003eimage\u003c/span\u003e: ${\u003cspan class=\"hljs-attr\"\u003eAIRFLOW_IMAGE_NAME\u003c/span\u003e:-apache/\u003cspan class=\"hljs-attr\"\u003eairflow\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e2.9\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.1\u003c/span\u003e}\n  \u003cspan class=\"hljs-attr\"\u003eenvironment\u003c/span\u003e:\n    \u0026#x26;airflow-common-env\n    \u003cspan class=\"hljs-attr\"\u003ePYTHONPATH\u003c/span\u003e: \u003cspan class=\"hljs-regexp\"\u003e/opt/\u003c/span\u003eairflow/dags/airflow_env_bs/lib/python3\u003cspan class=\"hljs-number\"\u003e.12\u003c/span\u003e/site-packages\n \n    #\u003cspan class=\"hljs-attr\"\u003eAWS_ACCESS_KEY_ID\u003c/span\u003e: your-access-key-id #값을 하드코딩\n    #\u003cspan class=\"hljs-attr\"\u003eAWS_SECRET_ACCESS_KEY\u003c/span\u003e: your-secret-access-key #값을 하드코딩\n    #\u003cspan class=\"hljs-attr\"\u003eAWS_REGION\u003c/span\u003e: eu-central-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e #값을 하드코딩\n\n#변경-\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e\nairflow-\u003cspan class=\"hljs-attr\"\u003einit\u003c/span\u003e:\n    \u0026#x3C;\u0026#x3C;: *airflow-common\n    \u003cspan class=\"hljs-attr\"\u003eentrypoint\u003c/span\u003e: \u003cspan class=\"hljs-regexp\"\u003e/bin/\u003c/span\u003ebash\n    \u003cspan class=\"hljs-attr\"\u003ecommand\u003c/span\u003e: \u003e\n      -c \u003cspan class=\"hljs-string\"\u003e\"pip install -r /requirements.txt \u0026#x26;\u0026#x26; airflow webserver\"\u003c/span\u003e \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e5. Apache Airflow로 자동화하기\u003c/h1\u003e\n\u003cp\u003e마지막으로 Apache Airflow를 사용하여 전체 프로세스를 조율하세요. 다음은 DAG를 정의하는 방법입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e datetime \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e datetime\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e airflow \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eDAG\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e airflow.\u003cspan class=\"hljs-property\"\u003eoperators\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003epython_operator\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003ePythonOperator\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# \u003cspan class=\"hljs-variable constant_\"\u003eDAG\u003c/span\u003e 정의\n\u003cspan class=\"hljs-variable constant_\"\u003eDAG_NAME\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e'berlin-weather'\u003c/span\u003e\ndefault_args = \n{\u003cspan class=\"hljs-string\"\u003e'owner'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'airflow'\u003c/span\u003e,\n\u003cspan class=\"hljs-string\"\u003e'depends_on_past'\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e,\n\u003cspan class=\"hljs-string\"\u003e'start_date'\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003edatetime\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e2023\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e19\u003c/span\u003e),\n\u003cspan class=\"hljs-string\"\u003e'retries'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\n\u003cspan class=\"hljs-string\"\u003e'retry_delay'\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003etimedelta\u003c/span\u003e(minutes=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e),\n}\ndag = \u003cspan class=\"hljs-title function_\"\u003eDAG\u003c/span\u003e(\ndag_id=\u003cspan class=\"hljs-variable constant_\"\u003eDAG_NAME\u003c/span\u003e,\ndescription=\u003cspan class=\"hljs-string\"\u003e'베를린 날씨 매 시간 갱신'\u003c/span\u003e,\nschedule_interval=\u003cspan class=\"hljs-string\"\u003e'@hourly'\u003c/span\u003e,\ndefault_args=default_args,\ncatchup=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e,\n)\n# 작업 정의\nupdate_weather_task = \u003cspan class=\"hljs-title class_\"\u003ePythonOperator\u003c/span\u003e(\ntask_id=\u003cspan class=\"hljs-string\"\u003e'update_weather'\u003c/span\u003e,\npython_callable=update_weather,\ndag=dag,\n)\n# 작업 의존성\nupdate_weather_task\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e5. CSV 파일을 위한 S3 버킷 확인\u003c/h1\u003e\n\u003cp\u003eAirflow 웹 서버를 열고 DAG를 실행하세요.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_0.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eDAG를 트리거한 후에 \"current_weather_berlin.csv\"라는 S3 Bucket을 확인해보세요. 거기에는 데이터 폴더가 있을 겁니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e이 프로젝트에서는 Python을 사용하여 베를린 날씨 데이터를 가져오고 로컬에 저장하며 Amazon S3로 업로드하는 프로세스를 자동화하는 방법을 탐색했습니다. 이를 위해 웹 스크래핑용 BeautifulSoup, 데이터 처리용 Pandas, 그리고 워크플로우 자동화용 Apache Airflow를 사용했습니다. 이러한 단계를 따라가면 이 프로젝트를 적응하고 확장하여 보다 복잡한 데이터 파이프라인 및 통합을 처리할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e제 Github 저장소를 확인하러 가보세요: Scraping-Berlin-Weather-Data-and-Uploading-to-Amazon-S3\u003c/p\u003e\n\u003cp\u003e태그: airflow, 데이터 엔지니어링 프로젝트, AWS S3, 도커, 초보자 프로젝트\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-20-End-to-EndAirflowProjectforBeginnersScrapingBerlinWeatherDataandUploadingtoAmazonS3"},"buildId":"kkckKPyxcjJp_o8wb2unW","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>