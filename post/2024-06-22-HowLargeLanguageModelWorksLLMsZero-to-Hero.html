<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>대형 언어 모델LLM 작동 방식 완벽 가이드 기초부터 고급까지 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="대형 언어 모델LLM 작동 방식 완벽 가이드 기초부터 고급까지 | itposting" data-gatsby-head="true"/><meta property="og:title" content="대형 언어 모델LLM 작동 방식 완벽 가이드 기초부터 고급까지 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero" data-gatsby-head="true"/><meta name="twitter:title" content="대형 언어 모델LLM 작동 방식 완벽 가이드 기초부터 고급까지 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-22 20:56" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">대형 언어 모델LLM 작동 방식 완벽 가이드 기초부터 고급까지</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="대형 언어 모델LLM 작동 방식 완벽 가이드 기초부터 고급까지" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 22, 2024</span><span class="posts_reading_time__f7YPP">8<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>제 ‘제로 투 히어로’ 시리즈의 두 번째 기사입니다. 이 기사에서는 대형 언어 모델 (LLM)이 작동하는 방식을 쉽게 설명해 드릴 예정입니다.</p>
<h1>LLM 작동 방식</h1>
<p>먼저 문서 완성 모델이 어떻게 작동하는지 살펴봅시다:</p>
<p><img src="/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_0.png" alt="이미지"></p>
<div class="content-ad"></div>
<p>사용자 프롬프트:</p>
<pre><code class="hljs language-js">바나나는
</code></pre>
<p>모델 응답:</p>
<pre><code class="hljs language-js">납작하고 먹을 수 있는 과일
</code></pre>
<div class="content-ad"></div>
<p>그럼, 문서 생성기 모델은 이렇게 작동합니다:</p>
<p><img src="/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_1.png" alt="image"></p>
<p>사용자 프롬프트:</p>
<pre><code class="hljs language-js">나는 새 차를 사고 싶어요
</code></pre>
<div class="content-ad"></div>
<p>"테이블 태그를 Markdown 형식으로 바꿔주세요."</p>
<p>위의 두 가지 사항 사이의 차이점을 주목해주세요.</p>
<p>첫 번째 모델은 문서 완성기로, 다음 문자가 될 가능성이 가장 높은 것을 찾아서 프롬프트만 완성합니다. 이 모델은 인터넷 데이터의 일부분을 학습한 것으로, 기본 모델이라고 불립니다.</p>
<div class="content-ad"></div>
<p>두 번째 모델은 문서 생성기입니다. 이 모델은 프롬프트 질문을 기반으로 한 인간과 유사한 응답을 생성합니다. 이것은 ChatGPT 모델입니다.</p>
<p>ChatGPT 모델은 프롬프트 질문을 기반으로 한 응답을 생성할 수 있는 추론 모델입니다. 저는 이 모델이 99% 베이스 모델이지만 두 가지 추가 단계인 파인튜닝 단계와 인간 피드백에서의 강화 학습 단계가 있습니다.</p>
<h1>사전 훈련: 베이스 모델</h1>
<p>이것은 인공 지능 혁명의 핵심이자 실제 마법이 일어나는 곳입니다.</p>
<div class="content-ad"></div>
<p>모델을 훈련하는 것은 많은 데이터를 제공하고 그것으로부터 배우는 과정입니다.</p>
<p>GPT-3 논문에 설명된 대로, 기본 모델은 인터넷 데이터의 대량을 바탕으로 훈련됩니다. 그것은 여러분과 같은 일반인에게는 쉬운 일이 아닙니다. 데이터를 획득하는 것 뿐만 아니라 GPU 및 TPU와 같은 많은 컴퓨팅 파워도 필요합니다.</p>
<p>하지만 걱정 마세요. 우리 자신의 컴퓨터에서 작은 GPT 모델을 훈련하는 방법을 여전히 배울 수 있습니다. 다음 주제에서 어떻게 하느지 보여 드리겠습니다.</p>
<p>LLM 훈련 뒤에 있는 혁신은 Transformer 아키텍처의 도입에 있습니다. 이를 통해 모델은 광범위한 데이터에서 배우면서 입력의 서로 다른 부분 간의 중요한 맥락적 관골을 유지할 수 있습니다.</p>
<div class="content-ad"></div>
<p>이러한 연결을 유지함으로써 모델은 제공된 문맥을 기반으로 새로운 통찰력을 효과적으로 추론할 수 있습니다. 이 문맥은 개별 단어, 문장, 문단 또는 그 이상일 수 있습니다. 이 능력을 통해 LLM 훈련은 자연어 처리 및 생성 작업에 대한 새로운 기회를 열어주어 기계가 인간의 의사 소통에 더 잘 이해하고 응답할 수 있도록 합니다.</p>
<p>기본 모델을 훈련하는 데 사용된 트랜스포머 아키텍처는 아래에 표시됩니다:</p>
<p><img src="/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_2.png" alt="LLM"></p>
<p>이는 일부 이전 및 새로운 기법을 사용하여 훈련된 신경망 모델입니다: 토큰화, 임베딩, 위치 인코딩, 피드포워드, 정규화, 소프트맥스, 선형 변환 및 가장 중요한 것은 멀티헤드 어텐션입니다.</p>
<div class="content-ad"></div>
<p>이 부분은 당신과 저 모두가 대부분 관심을 가지고 있는 부분이에요. 우리는 아키텍처 뒤의 아이디어와 교육 과정이 정확히 어떻게 이루어졌는지 명확하게 이해하고 싶어해요. 그래서 다음 글부터는 논문, 코드, 그리고 기본 모델을 훈련하는 데 사용된 수학적인 부분을 자세히 살펴볼 거에요.</p>
<h1>Fine-tuning: 어시스턴트 훈련하기</h1>
<p>Fine-tuning은 아주 똑똑한 구현이에요. 아마 OpenAI에 의해 처음 수행된 것 같아요. 아이디어는 아주 간단하지만 지능적으로 작동해요: 인간 라벨러를 고용하여 수많은 Q&#x26;A 대화쌍을 생성하게 해요(예: 10만 대화). 그런 다음 모델에 대화쌍을 주입시켜 이를 통해 학습시키는 거죠.</p>
<p>이 과정을 Fine-tuning이라고 해요. 그 10만 개의 대화 샘플 데이터가 모델에 훈련되면 뭔 일이 벌어질까요? 모델이 인간처럼 응답하기 시작할 거에요!</p>
<div class="content-ad"></div>
<p>위의 샘플 레이블 대화를 살펴보겠습니다:</p>
<pre><code class="hljs language-js">인간 레이블된 Q&#x26;A

<span class="hljs-attr">Q</span>: 이름이 뭐에요?
<span class="hljs-attr">A</span>: 제 이름은 존입니다.
</code></pre>
<pre><code class="hljs language-js">인간 레이블된 Q&#x26;A

<span class="hljs-attr">Q</span>: 중국의 수도는 무엇인가요?
<span class="hljs-attr">A</span>: 중국의 수도는 북경입니다.
</code></pre>
<pre><code class="hljs language-js">인간 레이블된 Q&#x26;A

<span class="hljs-attr">Q</span>: 영화 타이타닉의 줄거리를 요약해주세요.
<span class="hljs-attr">A</span>: 영화 타이타닉은 바다에서 침몰하는 배에 관한 이야기입니다.
</code></pre>
<div class="content-ad"></div>
<p>와, 이 샘플 Q&#x26;A들은 우리가 서로 대화하는 방식을 조롱하고 있는 것 같아요.</p>
<p>모델에게 이러한 응답 스타일을 가르쳐주면, 관련된 맥락에 대한 응답 확률이 매우 높아지며 사용자의 질문에 응답할 수 있게 될 거예요. 모델을 여러 대화 스타일로 훈련시킴으로써, 관련성이 높고 맥락에 맞는 응답을 제공할 가능성이 높아집니다.</p>
<p>이것이 언어 모델이 얼마나 지적이고 인간 같아 보일 수 있는지; 실제 대화의 리듬과 패턴을 모방하면, 사용자와의 대화를 흐름 있게 시뮬레이션할 수 있어요.</p>
<p>우리는 여기서 어시스턴트 모델을 얻었다고 말할 수 있어요.</p>
<div class="content-ad"></div>
<p>아래는 기본 모델을 사전 훈련에서 보조 모델을 미세 조정하는 과정 중 일부 강조 사항을 보여주는 다이어그램입니다:</p>
<p><img src="/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_3.png" alt="다이어그램"></p>
<h1>RLHF: 인간 피드백으로부터 강화 학습</h1>
<p>2022년 1월, OpenAI는 언어 모델을 지시 사항을 따르도록 조정하는 연구를 발표했습니다. 블로그 게시물에서 그들은 모델이 인간 피드백으로 더욱 미세하게 조정된 방법을 설명했습니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_4.png" alt="이미지"></p>
<p>이 부분은 조금 까다로울 수 있어요. 아이디어는 모델이 인간 피드백에서 배울 수 있게 하는 것입니다. 약 10만 개의 레이블이 지정된 Q&#x26;A 쌍을 제공하는 대신, 사용자의 프롬프트와 모델 응답을 수집한 다음 사람들이 순위 매기도록 합니다. 이 순위 정리된 대화를 가장 원하는 Q&#x26;A 샘플로 삼아 다시 모델에 피드하고 전반적인 성능을 향상시키도록 합니다.</p>
<p>이 프로세스는 OpenAI의 블로그에서 소개되었습니다.</p>
<p>우리 모델을 보다 안전하고 유용하며 일치하게 만들기 위해 기존 기술인 인간 피드백 강화 학습 (RLHF)을 사용합니다. API에 고객이 제출한 프롬프트에서, 우리의 레이블러는 원하는 모델 행동을 시연하고 모델에서 여러 출력을 순위 매깁니다. 그런 다음 우리는 이 데이터를 사용하여 GPT-3를 세밀하게 조정합니다.</p>
<div class="content-ad"></div>
<p>여기에 베이스 모델 대 미세 조정/RLHF 응답 비교가 있어요:</p>
<p><img src="/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_5.png" alt="Comparison"></p>
<p>미세 조정 및 RLHF 없이 모델은 단순히 문서 완성기일 뿐임을 확인할 수 있어요.</p>
<h1>프롬프트 엔지니어링</h1>
<div class="content-ad"></div>
<p>더 세밀한 조정과 RLHF를 사용하더라도 모델은 여전히 원하는 응답을 얻기 위해 도움이 필요합니다. 그리고 이것이 바로 프롬프트 엔지니어링이 필요한 이유입니다.</p>
<p>간단히 말해, 우리는 모델로부터 원하는 응답을 얻기 위해 프롬프트를 신중하게 디자인할 수 있습니다 (때로는 세밀한 조정 없이도).</p>
<p>수학과 코드에 너무 깊이 들어가지 않으려는 경우, 프롬프트 엔지니어링이 주목할 가치가 있는 방법입니다. 왜냐하면 더 나은 프롬프트를 입력함으로써 LLM 모델에서 최상의 결과를 얻을 수 있기 때문입니다.</p>
<p>이제 예시를 살펴보겠습니다:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-title class_">The</span> sky is
</code></pre>
<p>blue.</p>
<div class="content-ad"></div>
<p>아래 테이블을 Markdown 형식으로 변경해 주세요.</p>




















<table><thead><tr><th>이름</th><th>나이</th><th>성별</th></tr></thead><tbody><tr><td>민수</td><td>25</td><td>남성</td></tr><tr><td>지영</td><td>22</td><td>여성</td></tr></tbody></table>
<div class="content-ad"></div>
<pre><code class="hljs language-js">낮에는 파란색이고 밤에는 어두운 색입니다.
</code></pre>
<p>프롬프트에 일부 지침을 포함하면 모델이 무엇을 해야 하는지와 어떻게 응답해야 하는지 알 수 있습니다.</p>
<p>또 다른 흥미로운 예제를 살펴보겠습니다:</p>
<p>프롬프트:</p>
<div class="content-ad"></div>
<p>6살 때 내 언니 나이의 절반이었어. 지금 나는 70살, 내 언니 몇 살일까?</p>
<p>결과:</p>
<pre><code class="hljs language-js"><span class="hljs-number">35</span>
</code></pre>
<p>답은 틀렸어요. 정답은 67살이에요. 모델은 질문을 이해하는 것 같지만 논리적 추론 대신 수학 계산을 참고한 것 같네요.</p>
<div class="content-ad"></div>
<p>미세 조정 및 RLHF 없이도 프롬프트에 더 많은 예제 지침을 추가하는 것만으로도 올바른 답을 얻을 수 있습니다:</p>
<p>프롬프트:</p>
<pre><code class="hljs language-js"><span class="hljs-attr">Q</span>: 수목원에는 <span class="hljs-number">15</span> 그루의 나무가 있습니다. 오늘 나무원 작업자들이 나무를 심을 것입니다. 작업을 마치고 나면, <span class="hljs-number">21</span> 그루의 나무가 될 것입니다. 나무원 작업자들은 오늘 몇 그루의 나무를 심었습니까?
<span class="hljs-attr">A</span>: 우리는 <span class="hljs-number">15</span> 그루의 나무로 시작합니다. 나중에는 <span class="hljs-number">21</span> 그루의 나무가 있습니다. 차이는 그들이 심은 나무의 수여야 합니다. 따라서, 그들은 <span class="hljs-number">21</span> - <span class="hljs-number">15</span> = <span class="hljs-number">6</span> 그루의 나무를 심었을 것입니다. 정답은 <span class="hljs-number">6</span>입니다.
<span class="hljs-attr">Q</span>: 주차장에 차가 <span class="hljs-number">3</span>대 있고 더 <span class="hljs-number">2</span>대의 차가 도착한다면, 주차장에는 몇 대의 차가 있습니까?
<span class="hljs-attr">A</span>: 주차장에는 이미 <span class="hljs-number">3</span>대의 차가 있습니다. <span class="hljs-number">2</span>대가 추가로 도착합니다. 이제 차가 <span class="hljs-number">3</span> + <span class="hljs-number">2</span> = <span class="hljs-number">5</span>대 있습니다. 정답은 <span class="hljs-number">5</span>입니다.
<span class="hljs-attr">Q</span>: 리아는 초콜릿 <span class="hljs-number">32</span>개를 가지고 있었고, 그녀의 여동생은 <span class="hljs-number">42</span>개를 가지고 있었습니다. 그들이 <span class="hljs-number">35</span>개를 먹었다면, 두 사람이 남은 총 조각 수는 얼마입니까?
<span class="hljs-attr">A</span>: 리아는 초콜릿 <span class="hljs-number">32</span>개를 가지고 있었고, 리아의 여동생은 <span class="hljs-number">42</span>개를 가지고 있었습니다. 이는 원래 <span class="hljs-number">32</span> + <span class="hljs-number">42</span> = <span class="hljs-number">74</span>개의 초콜릿이 있었음을 의미합니다. <span class="hljs-number">35</span>개가 먹혔습니다. 그래서 총으로 계산하면 <span class="hljs-number">74</span> - <span class="hljs-number">35</span> = <span class="hljs-number">39</span>개의 초콜릿이 남게 됩니다. 정답은 <span class="hljs-number">39</span>입니다.
<span class="hljs-attr">Q</span>: 제이슨은 막대사탕 <span class="hljs-number">20</span>개를 가지고 있었습니다. 그는 덴니에게 일부 막대사탕을 주었습니다. 지금은 제이슨이 막대사탕 <span class="hljs-number">12</span>개를 가지고 있습니다. 제이슨이 덴니에게 몇 개의 막대사탕을 주었습니까?
<span class="hljs-attr">A</span>: 제이슨은 막대사탕 <span class="hljs-number">20</span>개를 가지고 있었습니다. 지금은 <span class="hljs-number">12</span>개밖에 없으므로, 나머지를 덴니에게 줬을 것입니다. 따라서, 덴니에게 준 막대사탕 수는 <span class="hljs-number">20</span> - <span class="hljs-number">12</span> = <span class="hljs-number">8</span>개입니다. 정답은 <span class="hljs-number">8</span>입니다.
<span class="hljs-attr">Q</span>: 숀은 <span class="hljs-number">5</span>개의 장난감을 가지고 있습니다. 크리스마스 때, 엄마와 아빠로부터 각각 <span class="hljs-number">2</span>개의 장난감을 받았습니다. 지금은 몇 개의 장난감이 있습니까?
<span class="hljs-attr">A</span>: 그는 <span class="hljs-number">5</span>개의 장난감을 가지고 있습니다. 엄마로부터 <span class="hljs-number">2</span>개를 받았으므로, 이후 그는 <span class="hljs-number">5</span> + <span class="hljs-number">2</span> = <span class="hljs-number">7</span>개의 장난감을 가지고 있습니다. 그리고 아빠로부터 또 <span class="hljs-number">2</span>개를 받았으므로 총으로 계산하면 <span class="hljs-number">7</span> + <span class="hljs-number">2</span> = <span class="hljs-number">9</span>개의 장난감이 있습니다. 정답은 <span class="hljs-number">9</span>입니다.
<span class="hljs-attr">Q</span>: 서버실에는 컴퓨터가 <span class="hljs-number">9</span>대 있었습니다. 월요일부터 목요일까지 매일 추가로 <span class="hljs-number">5</span>대의 컴퓨터가 설치되었습니다. 지금 서버실에는 몇 대의 컴퓨터가 있습니까?
<span class="hljs-attr">A</span>: 월요일부터 목요일까지는 <span class="hljs-number">4</span>일이 있습니다. 매일 <span class="hljs-number">5</span>대의 컴퓨터가 추가되었습니다. 이는 총 <span class="hljs-number">4</span> * <span class="hljs-number">5</span> = <span class="hljs-number">20</span>대의 컴퓨터가 추가되었음을 의미합니다. 처음에는 <span class="hljs-number">9</span>대의 컴퓨터가 있었으므로, 현재 서버실에는 <span class="hljs-number">9</span> + <span class="hljs-number">20</span> = <span class="hljs-number">29</span>대의 컴퓨터가 있습니다. 정답은 <span class="hljs-number">29</span>입니다.
<span class="hljs-attr">Q</span>: 마이클은 골프공 <span class="hljs-number">58</span>개를 가지고 있습니다. 화요일에 <span class="hljs-number">23</span>개의 골프공을 잃었습니다. 수요일에 또 <span class="hljs-number">2</span>개를 잃었습니다. 수요일 종료 시에는 몇 개의 골프공이 있습니까?
<span class="hljs-attr">A</span>: 마이클은 처음에 <span class="hljs-number">58</span>개의 골프공을 가지고 있었습니다. 화요일에 <span class="hljs-number">23</span>개를 잃었으므로, 그 후에는 <span class="hljs-number">58</span> - <span class="hljs-number">23</span> = <span class="hljs-number">35</span>개의 골프공을 가지고 있습니다. 수요일에 <span class="hljs-number">2</span>개를 더 잃었으므로, 이제 <span class="hljs-number">35</span> - <span class="hljs-number">2</span> = <span class="hljs-number">33</span>개의 골프공이 있습니다. 정답은 <span class="hljs-number">33</span>입니다.
<span class="hljs-attr">Q</span>: 올리비아는 $23을 가지고 있습니다. 그녀는 각각 $3에 <span class="hljs-number">5</span>개의 베이글을 샀습니다. 그녀가 남은 돈은 얼마입니까?
<span class="hljs-attr">A</span>: 그녀는 각각 $3에 <span class="hljs-number">5</span>개의 베이글을 샀습니다. 이는 총 $15를 소비한 것을 의미합니다. 그녀는 $8을 남겼습니다.
<span class="hljs-attr">Q</span>: 제가 <span class="hljs-number">6</span>살이었을 때, 내 여동생은 내 나이의 절반이었습니다. 지금 저는 <span class="hljs-number">70</span>살입니다. 내 여동생은 몇 살입니까?
<span class="hljs-attr">A</span>:
</code></pre>
<p>결과:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-number">6</span>살 때 내 언니는 저의 나이의 절반, 즉 <span class="hljs-number">3</span>살이어서 <span class="hljs-number">3</span>살이었습니다. 이제 저는 <span class="hljs-number">70</span>살이므로, 언니는 <span class="hljs-number">70</span> - <span class="hljs-number">3</span> = <span class="hljs-number">67</span>살입니다. 정답은 <span class="hljs-number">67</span>입니다.
</code></pre>
<p>output 2:</p>
<pre><code class="hljs language-js">이야기의 주인공이 <span class="hljs-number">6</span>살일 때, 그의 언니는 그의 나이의 절반인 <span class="hljs-number">3</span>살이었습니다. 이제 주인공이 <span class="hljs-number">70</span>살이 되었으므로, 언니는 <span class="hljs-number">70</span> - <span class="hljs-number">3</span> = <span class="hljs-number">67</span>세가 될 것입니다. 정답은 <span class="hljs-number">67</span>입니다.
</code></pre>
<p>두 번째 답변도 정확합니다! 우리는 문제에 대한 논리적 설명 예시를 추가하여 모델이 질문을 이해하고 올바르게 답변할 수 있도록 했습니다.</p>
<div class="content-ad"></div>
<p>위의 예시는 Wang 등(2022)에 의해 소개된 것으로, 최종 답안을 계산하는 과정은 몇 단계로 이루어져 있습니다.</p>
<p>강력한 프롬프트는 모델이 수학 문제 해결이나 텍스트 요약과 같은 복잡한 작업을 수행하는 데 도움이 될 수 있습니다. 따라서 프롬프트 엔지니어링은 LLM 생태계의 매우 중요한 역할을 합니다.</p>
<p>프롬프트 엔지니어링에 대해 더 알고 싶다면, 여기 좋은 프롬프트 가이드 튜토리얼이 있습니다.</p>
<h1>요약</h1>
<div class="content-ad"></div>
<p>여기까지 읽어주셔서 정말 감사합니다. 특히 LLM 세계에 처음 접하는 분들에게는 모든 정보를 소화하는 데 시간이 걸릴 것이라고 확신합니다.</p>
<p>이제 기본 개념과 배경 정보에 대해 충분한 내용을 다루었다고 믿습니다. 이제 우리 자신의 대형 언어 모델을 구축하기 위한 준비를 시작할 때입니다. 이론에는 지겨워했으니, 다음 기사에서는 Transformers 아키텍처의 중요한 구성 요소로 나아갈 것입니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"대형 언어 모델LLM 작동 방식 완벽 가이드 기초부터 고급까지","description":"","date":"2024-06-22 20:56","slug":"2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero","content":"\n\n제 ‘제로 투 히어로’ 시리즈의 두 번째 기사입니다. 이 기사에서는 대형 언어 모델 (LLM)이 작동하는 방식을 쉽게 설명해 드릴 예정입니다.\n\n# LLM 작동 방식\n\n먼저 문서 완성 모델이 어떻게 작동하는지 살펴봅시다:\n\n![이미지](/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사용자 프롬프트:\n\n```js\n바나나는\n```\n\n모델 응답:\n\n```js\n납작하고 먹을 수 있는 과일\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼, 문서 생성기 모델은 이렇게 작동합니다:\n\n![image](/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_1.png)\n\n사용자 프롬프트:\n\n```js\n나는 새 차를 사고 싶어요\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"테이블 태그를 Markdown 형식으로 바꿔주세요.\"\n\n위의 두 가지 사항 사이의 차이점을 주목해주세요.\n\n첫 번째 모델은 문서 완성기로, 다음 문자가 될 가능성이 가장 높은 것을 찾아서 프롬프트만 완성합니다. 이 모델은 인터넷 데이터의 일부분을 학습한 것으로, 기본 모델이라고 불립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 번째 모델은 문서 생성기입니다. 이 모델은 프롬프트 질문을 기반으로 한 인간과 유사한 응답을 생성합니다. 이것은 ChatGPT 모델입니다.\n\nChatGPT 모델은 프롬프트 질문을 기반으로 한 응답을 생성할 수 있는 추론 모델입니다. 저는 이 모델이 99% 베이스 모델이지만 두 가지 추가 단계인 파인튜닝 단계와 인간 피드백에서의 강화 학습 단계가 있습니다.\n\n# 사전 훈련: 베이스 모델\n\n이것은 인공 지능 혁명의 핵심이자 실제 마법이 일어나는 곳입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델을 훈련하는 것은 많은 데이터를 제공하고 그것으로부터 배우는 과정입니다.\n\nGPT-3 논문에 설명된 대로, 기본 모델은 인터넷 데이터의 대량을 바탕으로 훈련됩니다. 그것은 여러분과 같은 일반인에게는 쉬운 일이 아닙니다. 데이터를 획득하는 것 뿐만 아니라 GPU 및 TPU와 같은 많은 컴퓨팅 파워도 필요합니다.\n\n하지만 걱정 마세요. 우리 자신의 컴퓨터에서 작은 GPT 모델을 훈련하는 방법을 여전히 배울 수 있습니다. 다음 주제에서 어떻게 하느지 보여 드리겠습니다.\n\nLLM 훈련 뒤에 있는 혁신은 Transformer 아키텍처의 도입에 있습니다. 이를 통해 모델은 광범위한 데이터에서 배우면서 입력의 서로 다른 부분 간의 중요한 맥락적 관골을 유지할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 연결을 유지함으로써 모델은 제공된 문맥을 기반으로 새로운 통찰력을 효과적으로 추론할 수 있습니다. 이 문맥은 개별 단어, 문장, 문단 또는 그 이상일 수 있습니다. 이 능력을 통해 LLM 훈련은 자연어 처리 및 생성 작업에 대한 새로운 기회를 열어주어 기계가 인간의 의사 소통에 더 잘 이해하고 응답할 수 있도록 합니다.\n\n기본 모델을 훈련하는 데 사용된 트랜스포머 아키텍처는 아래에 표시됩니다:\n\n\n![LLM](/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_2.png)\n\n\n이는 일부 이전 및 새로운 기법을 사용하여 훈련된 신경망 모델입니다: 토큰화, 임베딩, 위치 인코딩, 피드포워드, 정규화, 소프트맥스, 선형 변환 및 가장 중요한 것은 멀티헤드 어텐션입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 부분은 당신과 저 모두가 대부분 관심을 가지고 있는 부분이에요. 우리는 아키텍처 뒤의 아이디어와 교육 과정이 정확히 어떻게 이루어졌는지 명확하게 이해하고 싶어해요. 그래서 다음 글부터는 논문, 코드, 그리고 기본 모델을 훈련하는 데 사용된 수학적인 부분을 자세히 살펴볼 거에요.\n\n# Fine-tuning: 어시스턴트 훈련하기\n\nFine-tuning은 아주 똑똑한 구현이에요. 아마 OpenAI에 의해 처음 수행된 것 같아요. 아이디어는 아주 간단하지만 지능적으로 작동해요: 인간 라벨러를 고용하여 수많은 Q\u0026A 대화쌍을 생성하게 해요(예: 10만 대화). 그런 다음 모델에 대화쌍을 주입시켜 이를 통해 학습시키는 거죠.\n\n이 과정을 Fine-tuning이라고 해요. 그 10만 개의 대화 샘플 데이터가 모델에 훈련되면 뭔 일이 벌어질까요? 모델이 인간처럼 응답하기 시작할 거에요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 샘플 레이블 대화를 살펴보겠습니다:\n\n```js\n인간 레이블된 Q\u0026A\n\nQ: 이름이 뭐에요?\nA: 제 이름은 존입니다.\n```\n\n```js\n인간 레이블된 Q\u0026A\n\nQ: 중국의 수도는 무엇인가요?\nA: 중국의 수도는 북경입니다.\n```\n\n```js\n인간 레이블된 Q\u0026A\n\nQ: 영화 타이타닉의 줄거리를 요약해주세요.\nA: 영화 타이타닉은 바다에서 침몰하는 배에 관한 이야기입니다.\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n와, 이 샘플 Q\u0026A들은 우리가 서로 대화하는 방식을 조롱하고 있는 것 같아요.\n\n모델에게 이러한 응답 스타일을 가르쳐주면, 관련된 맥락에 대한 응답 확률이 매우 높아지며 사용자의 질문에 응답할 수 있게 될 거예요. 모델을 여러 대화 스타일로 훈련시킴으로써, 관련성이 높고 맥락에 맞는 응답을 제공할 가능성이 높아집니다.\n\n이것이 언어 모델이 얼마나 지적이고 인간 같아 보일 수 있는지; 실제 대화의 리듬과 패턴을 모방하면, 사용자와의 대화를 흐름 있게 시뮬레이션할 수 있어요.\n\n우리는 여기서 어시스턴트 모델을 얻었다고 말할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 기본 모델을 사전 훈련에서 보조 모델을 미세 조정하는 과정 중 일부 강조 사항을 보여주는 다이어그램입니다:\n\n![다이어그램](/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_3.png)\n\n# RLHF: 인간 피드백으로부터 강화 학습\n\n2022년 1월, OpenAI는 언어 모델을 지시 사항을 따르도록 조정하는 연구를 발표했습니다. 블로그 게시물에서 그들은 모델이 인간 피드백으로 더욱 미세하게 조정된 방법을 설명했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_4.png)\n\n이 부분은 조금 까다로울 수 있어요. 아이디어는 모델이 인간 피드백에서 배울 수 있게 하는 것입니다. 약 10만 개의 레이블이 지정된 Q\u0026A 쌍을 제공하는 대신, 사용자의 프롬프트와 모델 응답을 수집한 다음 사람들이 순위 매기도록 합니다. 이 순위 정리된 대화를 가장 원하는 Q\u0026A 샘플로 삼아 다시 모델에 피드하고 전반적인 성능을 향상시키도록 합니다.\n\n이 프로세스는 OpenAI의 블로그에서 소개되었습니다.\n\n우리 모델을 보다 안전하고 유용하며 일치하게 만들기 위해 기존 기술인 인간 피드백 강화 학습 (RLHF)을 사용합니다. API에 고객이 제출한 프롬프트에서, 우리의 레이블러는 원하는 모델 행동을 시연하고 모델에서 여러 출력을 순위 매깁니다. 그런 다음 우리는 이 데이터를 사용하여 GPT-3를 세밀하게 조정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기에 베이스 모델 대 미세 조정/RLHF 응답 비교가 있어요:\n\n![Comparison](/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_5.png)\n\n미세 조정 및 RLHF 없이 모델은 단순히 문서 완성기일 뿐임을 확인할 수 있어요.\n\n# 프롬프트 엔지니어링\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 세밀한 조정과 RLHF를 사용하더라도 모델은 여전히 원하는 응답을 얻기 위해 도움이 필요합니다. 그리고 이것이 바로 프롬프트 엔지니어링이 필요한 이유입니다.\n\n간단히 말해, 우리는 모델로부터 원하는 응답을 얻기 위해 프롬프트를 신중하게 디자인할 수 있습니다 (때로는 세밀한 조정 없이도).\n\n수학과 코드에 너무 깊이 들어가지 않으려는 경우, 프롬프트 엔지니어링이 주목할 가치가 있는 방법입니다. 왜냐하면 더 나은 프롬프트를 입력함으로써 LLM 모델에서 최상의 결과를 얻을 수 있기 때문입니다.\n\n이제 예시를 살펴보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nThe sky is\n```\n\nblue.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 테이블을 Markdown 형식으로 변경해 주세요.\n\n| 이름 | 나이 | 성별 |\n|------|-----|-----|\n| 민수 | 25 | 남성 |\n| 지영 | 22 | 여성 |\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n낮에는 파란색이고 밤에는 어두운 색입니다.\n```\n\n프롬프트에 일부 지침을 포함하면 모델이 무엇을 해야 하는지와 어떻게 응답해야 하는지 알 수 있습니다.\n\n또 다른 흥미로운 예제를 살펴보겠습니다:\n\n프롬프트:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n6살 때 내 언니 나이의 절반이었어. 지금 나는 70살, 내 언니 몇 살일까? \n\n\n결과:\n\n```js\n35\n```\n\n답은 틀렸어요. 정답은 67살이에요. 모델은 질문을 이해하는 것 같지만 논리적 추론 대신 수학 계산을 참고한 것 같네요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n미세 조정 및 RLHF 없이도 프롬프트에 더 많은 예제 지침을 추가하는 것만으로도 올바른 답을 얻을 수 있습니다:\n\n프롬프트:\n\n```js\nQ: 수목원에는 15 그루의 나무가 있습니다. 오늘 나무원 작업자들이 나무를 심을 것입니다. 작업을 마치고 나면, 21 그루의 나무가 될 것입니다. 나무원 작업자들은 오늘 몇 그루의 나무를 심었습니까?\nA: 우리는 15 그루의 나무로 시작합니다. 나중에는 21 그루의 나무가 있습니다. 차이는 그들이 심은 나무의 수여야 합니다. 따라서, 그들은 21 - 15 = 6 그루의 나무를 심었을 것입니다. 정답은 6입니다.\nQ: 주차장에 차가 3대 있고 더 2대의 차가 도착한다면, 주차장에는 몇 대의 차가 있습니까?\nA: 주차장에는 이미 3대의 차가 있습니다. 2대가 추가로 도착합니다. 이제 차가 3 + 2 = 5대 있습니다. 정답은 5입니다.\nQ: 리아는 초콜릿 32개를 가지고 있었고, 그녀의 여동생은 42개를 가지고 있었습니다. 그들이 35개를 먹었다면, 두 사람이 남은 총 조각 수는 얼마입니까?\nA: 리아는 초콜릿 32개를 가지고 있었고, 리아의 여동생은 42개를 가지고 있었습니다. 이는 원래 32 + 42 = 74개의 초콜릿이 있었음을 의미합니다. 35개가 먹혔습니다. 그래서 총으로 계산하면 74 - 35 = 39개의 초콜릿이 남게 됩니다. 정답은 39입니다.\nQ: 제이슨은 막대사탕 20개를 가지고 있었습니다. 그는 덴니에게 일부 막대사탕을 주었습니다. 지금은 제이슨이 막대사탕 12개를 가지고 있습니다. 제이슨이 덴니에게 몇 개의 막대사탕을 주었습니까?\nA: 제이슨은 막대사탕 20개를 가지고 있었습니다. 지금은 12개밖에 없으므로, 나머지를 덴니에게 줬을 것입니다. 따라서, 덴니에게 준 막대사탕 수는 20 - 12 = 8개입니다. 정답은 8입니다.\nQ: 숀은 5개의 장난감을 가지고 있습니다. 크리스마스 때, 엄마와 아빠로부터 각각 2개의 장난감을 받았습니다. 지금은 몇 개의 장난감이 있습니까?\nA: 그는 5개의 장난감을 가지고 있습니다. 엄마로부터 2개를 받았으므로, 이후 그는 5 + 2 = 7개의 장난감을 가지고 있습니다. 그리고 아빠로부터 또 2개를 받았으므로 총으로 계산하면 7 + 2 = 9개의 장난감이 있습니다. 정답은 9입니다.\nQ: 서버실에는 컴퓨터가 9대 있었습니다. 월요일부터 목요일까지 매일 추가로 5대의 컴퓨터가 설치되었습니다. 지금 서버실에는 몇 대의 컴퓨터가 있습니까?\nA: 월요일부터 목요일까지는 4일이 있습니다. 매일 5대의 컴퓨터가 추가되었습니다. 이는 총 4 * 5 = 20대의 컴퓨터가 추가되었음을 의미합니다. 처음에는 9대의 컴퓨터가 있었으므로, 현재 서버실에는 9 + 20 = 29대의 컴퓨터가 있습니다. 정답은 29입니다.\nQ: 마이클은 골프공 58개를 가지고 있습니다. 화요일에 23개의 골프공을 잃었습니다. 수요일에 또 2개를 잃었습니다. 수요일 종료 시에는 몇 개의 골프공이 있습니까?\nA: 마이클은 처음에 58개의 골프공을 가지고 있었습니다. 화요일에 23개를 잃었으므로, 그 후에는 58 - 23 = 35개의 골프공을 가지고 있습니다. 수요일에 2개를 더 잃었으므로, 이제 35 - 2 = 33개의 골프공이 있습니다. 정답은 33입니다.\nQ: 올리비아는 $23을 가지고 있습니다. 그녀는 각각 $3에 5개의 베이글을 샀습니다. 그녀가 남은 돈은 얼마입니까?\nA: 그녀는 각각 $3에 5개의 베이글을 샀습니다. 이는 총 $15를 소비한 것을 의미합니다. 그녀는 $8을 남겼습니다.\nQ: 제가 6살이었을 때, 내 여동생은 내 나이의 절반이었습니다. 지금 저는 70살입니다. 내 여동생은 몇 살입니까?\nA:\n```\n\n결과:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n6살 때 내 언니는 저의 나이의 절반, 즉 3살이어서 3살이었습니다. 이제 저는 70살이므로, 언니는 70 - 3 = 67살입니다. 정답은 67입니다.\n```\n\noutput 2:\n\n```js\n이야기의 주인공이 6살일 때, 그의 언니는 그의 나이의 절반인 3살이었습니다. 이제 주인공이 70살이 되었으므로, 언니는 70 - 3 = 67세가 될 것입니다. 정답은 67입니다.\n```\n\n두 번째 답변도 정확합니다! 우리는 문제에 대한 논리적 설명 예시를 추가하여 모델이 질문을 이해하고 올바르게 답변할 수 있도록 했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 예시는 Wang 등(2022)에 의해 소개된 것으로, 최종 답안을 계산하는 과정은 몇 단계로 이루어져 있습니다.\n\n강력한 프롬프트는 모델이 수학 문제 해결이나 텍스트 요약과 같은 복잡한 작업을 수행하는 데 도움이 될 수 있습니다. 따라서 프롬프트 엔지니어링은 LLM 생태계의 매우 중요한 역할을 합니다.\n\n프롬프트 엔지니어링에 대해 더 알고 싶다면, 여기 좋은 프롬프트 가이드 튜토리얼이 있습니다.\n\n# 요약\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기까지 읽어주셔서 정말 감사합니다. 특히 LLM 세계에 처음 접하는 분들에게는 모든 정보를 소화하는 데 시간이 걸릴 것이라고 확신합니다.\n\n이제 기본 개념과 배경 정보에 대해 충분한 내용을 다루었다고 믿습니다. 이제 우리 자신의 대형 언어 모델을 구축하기 위한 준비를 시작할 때입니다. 이론에는 지겨워했으니, 다음 기사에서는 Transformers 아키텍처의 중요한 구성 요소로 나아갈 것입니다.","ogImage":{"url":"/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_0.png"},"coverImage":"/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_0.png","tag":["Tech"],"readingTime":8},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e제 ‘제로 투 히어로’ 시리즈의 두 번째 기사입니다. 이 기사에서는 대형 언어 모델 (LLM)이 작동하는 방식을 쉽게 설명해 드릴 예정입니다.\u003c/p\u003e\n\u003ch1\u003eLLM 작동 방식\u003c/h1\u003e\n\u003cp\u003e먼저 문서 완성 모델이 어떻게 작동하는지 살펴봅시다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e사용자 프롬프트:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e바나나는\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e모델 응답:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e납작하고 먹을 수 있는 과일\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그럼, 문서 생성기 모델은 이렇게 작동합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_1.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e사용자 프롬프트:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e나는 새 차를 사고 싶어요\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\"테이블 태그를 Markdown 형식으로 바꿔주세요.\"\u003c/p\u003e\n\u003cp\u003e위의 두 가지 사항 사이의 차이점을 주목해주세요.\u003c/p\u003e\n\u003cp\u003e첫 번째 모델은 문서 완성기로, 다음 문자가 될 가능성이 가장 높은 것을 찾아서 프롬프트만 완성합니다. 이 모델은 인터넷 데이터의 일부분을 학습한 것으로, 기본 모델이라고 불립니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e두 번째 모델은 문서 생성기입니다. 이 모델은 프롬프트 질문을 기반으로 한 인간과 유사한 응답을 생성합니다. 이것은 ChatGPT 모델입니다.\u003c/p\u003e\n\u003cp\u003eChatGPT 모델은 프롬프트 질문을 기반으로 한 응답을 생성할 수 있는 추론 모델입니다. 저는 이 모델이 99% 베이스 모델이지만 두 가지 추가 단계인 파인튜닝 단계와 인간 피드백에서의 강화 학습 단계가 있습니다.\u003c/p\u003e\n\u003ch1\u003e사전 훈련: 베이스 모델\u003c/h1\u003e\n\u003cp\u003e이것은 인공 지능 혁명의 핵심이자 실제 마법이 일어나는 곳입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e모델을 훈련하는 것은 많은 데이터를 제공하고 그것으로부터 배우는 과정입니다.\u003c/p\u003e\n\u003cp\u003eGPT-3 논문에 설명된 대로, 기본 모델은 인터넷 데이터의 대량을 바탕으로 훈련됩니다. 그것은 여러분과 같은 일반인에게는 쉬운 일이 아닙니다. 데이터를 획득하는 것 뿐만 아니라 GPU 및 TPU와 같은 많은 컴퓨팅 파워도 필요합니다.\u003c/p\u003e\n\u003cp\u003e하지만 걱정 마세요. 우리 자신의 컴퓨터에서 작은 GPT 모델을 훈련하는 방법을 여전히 배울 수 있습니다. 다음 주제에서 어떻게 하느지 보여 드리겠습니다.\u003c/p\u003e\n\u003cp\u003eLLM 훈련 뒤에 있는 혁신은 Transformer 아키텍처의 도입에 있습니다. 이를 통해 모델은 광범위한 데이터에서 배우면서 입력의 서로 다른 부분 간의 중요한 맥락적 관골을 유지할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이러한 연결을 유지함으로써 모델은 제공된 문맥을 기반으로 새로운 통찰력을 효과적으로 추론할 수 있습니다. 이 문맥은 개별 단어, 문장, 문단 또는 그 이상일 수 있습니다. 이 능력을 통해 LLM 훈련은 자연어 처리 및 생성 작업에 대한 새로운 기회를 열어주어 기계가 인간의 의사 소통에 더 잘 이해하고 응답할 수 있도록 합니다.\u003c/p\u003e\n\u003cp\u003e기본 모델을 훈련하는 데 사용된 트랜스포머 아키텍처는 아래에 표시됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_2.png\" alt=\"LLM\"\u003e\u003c/p\u003e\n\u003cp\u003e이는 일부 이전 및 새로운 기법을 사용하여 훈련된 신경망 모델입니다: 토큰화, 임베딩, 위치 인코딩, 피드포워드, 정규화, 소프트맥스, 선형 변환 및 가장 중요한 것은 멀티헤드 어텐션입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 부분은 당신과 저 모두가 대부분 관심을 가지고 있는 부분이에요. 우리는 아키텍처 뒤의 아이디어와 교육 과정이 정확히 어떻게 이루어졌는지 명확하게 이해하고 싶어해요. 그래서 다음 글부터는 논문, 코드, 그리고 기본 모델을 훈련하는 데 사용된 수학적인 부분을 자세히 살펴볼 거에요.\u003c/p\u003e\n\u003ch1\u003eFine-tuning: 어시스턴트 훈련하기\u003c/h1\u003e\n\u003cp\u003eFine-tuning은 아주 똑똑한 구현이에요. 아마 OpenAI에 의해 처음 수행된 것 같아요. 아이디어는 아주 간단하지만 지능적으로 작동해요: 인간 라벨러를 고용하여 수많은 Q\u0026#x26;A 대화쌍을 생성하게 해요(예: 10만 대화). 그런 다음 모델에 대화쌍을 주입시켜 이를 통해 학습시키는 거죠.\u003c/p\u003e\n\u003cp\u003e이 과정을 Fine-tuning이라고 해요. 그 10만 개의 대화 샘플 데이터가 모델에 훈련되면 뭔 일이 벌어질까요? 모델이 인간처럼 응답하기 시작할 거에요!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e위의 샘플 레이블 대화를 살펴보겠습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e인간 레이블된 Q\u0026#x26;A\n\n\u003cspan class=\"hljs-attr\"\u003eQ\u003c/span\u003e: 이름이 뭐에요?\n\u003cspan class=\"hljs-attr\"\u003eA\u003c/span\u003e: 제 이름은 존입니다.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e인간 레이블된 Q\u0026#x26;A\n\n\u003cspan class=\"hljs-attr\"\u003eQ\u003c/span\u003e: 중국의 수도는 무엇인가요?\n\u003cspan class=\"hljs-attr\"\u003eA\u003c/span\u003e: 중국의 수도는 북경입니다.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e인간 레이블된 Q\u0026#x26;A\n\n\u003cspan class=\"hljs-attr\"\u003eQ\u003c/span\u003e: 영화 타이타닉의 줄거리를 요약해주세요.\n\u003cspan class=\"hljs-attr\"\u003eA\u003c/span\u003e: 영화 타이타닉은 바다에서 침몰하는 배에 관한 이야기입니다.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e와, 이 샘플 Q\u0026#x26;A들은 우리가 서로 대화하는 방식을 조롱하고 있는 것 같아요.\u003c/p\u003e\n\u003cp\u003e모델에게 이러한 응답 스타일을 가르쳐주면, 관련된 맥락에 대한 응답 확률이 매우 높아지며 사용자의 질문에 응답할 수 있게 될 거예요. 모델을 여러 대화 스타일로 훈련시킴으로써, 관련성이 높고 맥락에 맞는 응답을 제공할 가능성이 높아집니다.\u003c/p\u003e\n\u003cp\u003e이것이 언어 모델이 얼마나 지적이고 인간 같아 보일 수 있는지; 실제 대화의 리듬과 패턴을 모방하면, 사용자와의 대화를 흐름 있게 시뮬레이션할 수 있어요.\u003c/p\u003e\n\u003cp\u003e우리는 여기서 어시스턴트 모델을 얻었다고 말할 수 있어요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래는 기본 모델을 사전 훈련에서 보조 모델을 미세 조정하는 과정 중 일부 강조 사항을 보여주는 다이어그램입니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_3.png\" alt=\"다이어그램\"\u003e\u003c/p\u003e\n\u003ch1\u003eRLHF: 인간 피드백으로부터 강화 학습\u003c/h1\u003e\n\u003cp\u003e2022년 1월, OpenAI는 언어 모델을 지시 사항을 따르도록 조정하는 연구를 발표했습니다. 블로그 게시물에서 그들은 모델이 인간 피드백으로 더욱 미세하게 조정된 방법을 설명했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_4.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e이 부분은 조금 까다로울 수 있어요. 아이디어는 모델이 인간 피드백에서 배울 수 있게 하는 것입니다. 약 10만 개의 레이블이 지정된 Q\u0026#x26;A 쌍을 제공하는 대신, 사용자의 프롬프트와 모델 응답을 수집한 다음 사람들이 순위 매기도록 합니다. 이 순위 정리된 대화를 가장 원하는 Q\u0026#x26;A 샘플로 삼아 다시 모델에 피드하고 전반적인 성능을 향상시키도록 합니다.\u003c/p\u003e\n\u003cp\u003e이 프로세스는 OpenAI의 블로그에서 소개되었습니다.\u003c/p\u003e\n\u003cp\u003e우리 모델을 보다 안전하고 유용하며 일치하게 만들기 위해 기존 기술인 인간 피드백 강화 학습 (RLHF)을 사용합니다. API에 고객이 제출한 프롬프트에서, 우리의 레이블러는 원하는 모델 행동을 시연하고 모델에서 여러 출력을 순위 매깁니다. 그런 다음 우리는 이 데이터를 사용하여 GPT-3를 세밀하게 조정합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e여기에 베이스 모델 대 미세 조정/RLHF 응답 비교가 있어요:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero_5.png\" alt=\"Comparison\"\u003e\u003c/p\u003e\n\u003cp\u003e미세 조정 및 RLHF 없이 모델은 단순히 문서 완성기일 뿐임을 확인할 수 있어요.\u003c/p\u003e\n\u003ch1\u003e프롬프트 엔지니어링\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e더 세밀한 조정과 RLHF를 사용하더라도 모델은 여전히 원하는 응답을 얻기 위해 도움이 필요합니다. 그리고 이것이 바로 프롬프트 엔지니어링이 필요한 이유입니다.\u003c/p\u003e\n\u003cp\u003e간단히 말해, 우리는 모델로부터 원하는 응답을 얻기 위해 프롬프트를 신중하게 디자인할 수 있습니다 (때로는 세밀한 조정 없이도).\u003c/p\u003e\n\u003cp\u003e수학과 코드에 너무 깊이 들어가지 않으려는 경우, 프롬프트 엔지니어링이 주목할 가치가 있는 방법입니다. 왜냐하면 더 나은 프롬프트를 입력함으로써 LLM 모델에서 최상의 결과를 얻을 수 있기 때문입니다.\u003c/p\u003e\n\u003cp\u003e이제 예시를 살펴보겠습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title class_\"\u003eThe\u003c/span\u003e sky is\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eblue.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래 테이블을 Markdown 형식으로 변경해 주세요.\u003c/p\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003e이름\u003c/th\u003e\u003cth\u003e나이\u003c/th\u003e\u003cth\u003e성별\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e민수\u003c/td\u003e\u003ctd\u003e25\u003c/td\u003e\u003ctd\u003e남성\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e지영\u003c/td\u003e\u003ctd\u003e22\u003c/td\u003e\u003ctd\u003e여성\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e낮에는 파란색이고 밤에는 어두운 색입니다.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e프롬프트에 일부 지침을 포함하면 모델이 무엇을 해야 하는지와 어떻게 응답해야 하는지 알 수 있습니다.\u003c/p\u003e\n\u003cp\u003e또 다른 흥미로운 예제를 살펴보겠습니다:\u003c/p\u003e\n\u003cp\u003e프롬프트:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e6살 때 내 언니 나이의 절반이었어. 지금 나는 70살, 내 언니 몇 살일까?\u003c/p\u003e\n\u003cp\u003e결과:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-number\"\u003e35\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e답은 틀렸어요. 정답은 67살이에요. 모델은 질문을 이해하는 것 같지만 논리적 추론 대신 수학 계산을 참고한 것 같네요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e미세 조정 및 RLHF 없이도 프롬프트에 더 많은 예제 지침을 추가하는 것만으로도 올바른 답을 얻을 수 있습니다:\u003c/p\u003e\n\u003cp\u003e프롬프트:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-attr\"\u003eQ\u003c/span\u003e: 수목원에는 \u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e 그루의 나무가 있습니다. 오늘 나무원 작업자들이 나무를 심을 것입니다. 작업을 마치고 나면, \u003cspan class=\"hljs-number\"\u003e21\u003c/span\u003e 그루의 나무가 될 것입니다. 나무원 작업자들은 오늘 몇 그루의 나무를 심었습니까?\n\u003cspan class=\"hljs-attr\"\u003eA\u003c/span\u003e: 우리는 \u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e 그루의 나무로 시작합니다. 나중에는 \u003cspan class=\"hljs-number\"\u003e21\u003c/span\u003e 그루의 나무가 있습니다. 차이는 그들이 심은 나무의 수여야 합니다. 따라서, 그들은 \u003cspan class=\"hljs-number\"\u003e21\u003c/span\u003e - \u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e 그루의 나무를 심었을 것입니다. 정답은 \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e입니다.\n\u003cspan class=\"hljs-attr\"\u003eQ\u003c/span\u003e: 주차장에 차가 \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e대 있고 더 \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e대의 차가 도착한다면, 주차장에는 몇 대의 차가 있습니까?\n\u003cspan class=\"hljs-attr\"\u003eA\u003c/span\u003e: 주차장에는 이미 \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e대의 차가 있습니다. \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e대가 추가로 도착합니다. 이제 차가 \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e + \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e대 있습니다. 정답은 \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e입니다.\n\u003cspan class=\"hljs-attr\"\u003eQ\u003c/span\u003e: 리아는 초콜릿 \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e개를 가지고 있었고, 그녀의 여동생은 \u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e개를 가지고 있었습니다. 그들이 \u003cspan class=\"hljs-number\"\u003e35\u003c/span\u003e개를 먹었다면, 두 사람이 남은 총 조각 수는 얼마입니까?\n\u003cspan class=\"hljs-attr\"\u003eA\u003c/span\u003e: 리아는 초콜릿 \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e개를 가지고 있었고, 리아의 여동생은 \u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e개를 가지고 있었습니다. 이는 원래 \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e + \u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e74\u003c/span\u003e개의 초콜릿이 있었음을 의미합니다. \u003cspan class=\"hljs-number\"\u003e35\u003c/span\u003e개가 먹혔습니다. 그래서 총으로 계산하면 \u003cspan class=\"hljs-number\"\u003e74\u003c/span\u003e - \u003cspan class=\"hljs-number\"\u003e35\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e39\u003c/span\u003e개의 초콜릿이 남게 됩니다. 정답은 \u003cspan class=\"hljs-number\"\u003e39\u003c/span\u003e입니다.\n\u003cspan class=\"hljs-attr\"\u003eQ\u003c/span\u003e: 제이슨은 막대사탕 \u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e개를 가지고 있었습니다. 그는 덴니에게 일부 막대사탕을 주었습니다. 지금은 제이슨이 막대사탕 \u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e개를 가지고 있습니다. 제이슨이 덴니에게 몇 개의 막대사탕을 주었습니까?\n\u003cspan class=\"hljs-attr\"\u003eA\u003c/span\u003e: 제이슨은 막대사탕 \u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e개를 가지고 있었습니다. 지금은 \u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e개밖에 없으므로, 나머지를 덴니에게 줬을 것입니다. 따라서, 덴니에게 준 막대사탕 수는 \u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e - \u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e개입니다. 정답은 \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e입니다.\n\u003cspan class=\"hljs-attr\"\u003eQ\u003c/span\u003e: 숀은 \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e개의 장난감을 가지고 있습니다. 크리스마스 때, 엄마와 아빠로부터 각각 \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e개의 장난감을 받았습니다. 지금은 몇 개의 장난감이 있습니까?\n\u003cspan class=\"hljs-attr\"\u003eA\u003c/span\u003e: 그는 \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e개의 장난감을 가지고 있습니다. 엄마로부터 \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e개를 받았으므로, 이후 그는 \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e + \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e개의 장난감을 가지고 있습니다. 그리고 아빠로부터 또 \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e개를 받았으므로 총으로 계산하면 \u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e + \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e개의 장난감이 있습니다. 정답은 \u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e입니다.\n\u003cspan class=\"hljs-attr\"\u003eQ\u003c/span\u003e: 서버실에는 컴퓨터가 \u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e대 있었습니다. 월요일부터 목요일까지 매일 추가로 \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e대의 컴퓨터가 설치되었습니다. 지금 서버실에는 몇 대의 컴퓨터가 있습니까?\n\u003cspan class=\"hljs-attr\"\u003eA\u003c/span\u003e: 월요일부터 목요일까지는 \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e일이 있습니다. 매일 \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e대의 컴퓨터가 추가되었습니다. 이는 총 \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e * \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e대의 컴퓨터가 추가되었음을 의미합니다. 처음에는 \u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e대의 컴퓨터가 있었으므로, 현재 서버실에는 \u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e + \u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e29\u003c/span\u003e대의 컴퓨터가 있습니다. 정답은 \u003cspan class=\"hljs-number\"\u003e29\u003c/span\u003e입니다.\n\u003cspan class=\"hljs-attr\"\u003eQ\u003c/span\u003e: 마이클은 골프공 \u003cspan class=\"hljs-number\"\u003e58\u003c/span\u003e개를 가지고 있습니다. 화요일에 \u003cspan class=\"hljs-number\"\u003e23\u003c/span\u003e개의 골프공을 잃었습니다. 수요일에 또 \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e개를 잃었습니다. 수요일 종료 시에는 몇 개의 골프공이 있습니까?\n\u003cspan class=\"hljs-attr\"\u003eA\u003c/span\u003e: 마이클은 처음에 \u003cspan class=\"hljs-number\"\u003e58\u003c/span\u003e개의 골프공을 가지고 있었습니다. 화요일에 \u003cspan class=\"hljs-number\"\u003e23\u003c/span\u003e개를 잃었으므로, 그 후에는 \u003cspan class=\"hljs-number\"\u003e58\u003c/span\u003e - \u003cspan class=\"hljs-number\"\u003e23\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e35\u003c/span\u003e개의 골프공을 가지고 있습니다. 수요일에 \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e개를 더 잃었으므로, 이제 \u003cspan class=\"hljs-number\"\u003e35\u003c/span\u003e - \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e33\u003c/span\u003e개의 골프공이 있습니다. 정답은 \u003cspan class=\"hljs-number\"\u003e33\u003c/span\u003e입니다.\n\u003cspan class=\"hljs-attr\"\u003eQ\u003c/span\u003e: 올리비아는 $23을 가지고 있습니다. 그녀는 각각 $3에 \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e개의 베이글을 샀습니다. 그녀가 남은 돈은 얼마입니까?\n\u003cspan class=\"hljs-attr\"\u003eA\u003c/span\u003e: 그녀는 각각 $3에 \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e개의 베이글을 샀습니다. 이는 총 $15를 소비한 것을 의미합니다. 그녀는 $8을 남겼습니다.\n\u003cspan class=\"hljs-attr\"\u003eQ\u003c/span\u003e: 제가 \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e살이었을 때, 내 여동생은 내 나이의 절반이었습니다. 지금 저는 \u003cspan class=\"hljs-number\"\u003e70\u003c/span\u003e살입니다. 내 여동생은 몇 살입니까?\n\u003cspan class=\"hljs-attr\"\u003eA\u003c/span\u003e:\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e결과:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e살 때 내 언니는 저의 나이의 절반, 즉 \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e살이어서 \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e살이었습니다. 이제 저는 \u003cspan class=\"hljs-number\"\u003e70\u003c/span\u003e살이므로, 언니는 \u003cspan class=\"hljs-number\"\u003e70\u003c/span\u003e - \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e67\u003c/span\u003e살입니다. 정답은 \u003cspan class=\"hljs-number\"\u003e67\u003c/span\u003e입니다.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eoutput 2:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e이야기의 주인공이 \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e살일 때, 그의 언니는 그의 나이의 절반인 \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e살이었습니다. 이제 주인공이 \u003cspan class=\"hljs-number\"\u003e70\u003c/span\u003e살이 되었으므로, 언니는 \u003cspan class=\"hljs-number\"\u003e70\u003c/span\u003e - \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e67\u003c/span\u003e세가 될 것입니다. 정답은 \u003cspan class=\"hljs-number\"\u003e67\u003c/span\u003e입니다.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e두 번째 답변도 정확합니다! 우리는 문제에 대한 논리적 설명 예시를 추가하여 모델이 질문을 이해하고 올바르게 답변할 수 있도록 했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e위의 예시는 Wang 등(2022)에 의해 소개된 것으로, 최종 답안을 계산하는 과정은 몇 단계로 이루어져 있습니다.\u003c/p\u003e\n\u003cp\u003e강력한 프롬프트는 모델이 수학 문제 해결이나 텍스트 요약과 같은 복잡한 작업을 수행하는 데 도움이 될 수 있습니다. 따라서 프롬프트 엔지니어링은 LLM 생태계의 매우 중요한 역할을 합니다.\u003c/p\u003e\n\u003cp\u003e프롬프트 엔지니어링에 대해 더 알고 싶다면, 여기 좋은 프롬프트 가이드 튜토리얼이 있습니다.\u003c/p\u003e\n\u003ch1\u003e요약\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e여기까지 읽어주셔서 정말 감사합니다. 특히 LLM 세계에 처음 접하는 분들에게는 모든 정보를 소화하는 데 시간이 걸릴 것이라고 확신합니다.\u003c/p\u003e\n\u003cp\u003e이제 기본 개념과 배경 정보에 대해 충분한 내용을 다루었다고 믿습니다. 이제 우리 자신의 대형 언어 모델을 구축하기 위한 준비를 시작할 때입니다. 이론에는 지겨워했으니, 다음 기사에서는 Transformers 아키텍처의 중요한 구성 요소로 나아갈 것입니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-22-HowLargeLanguageModelWorksLLMsZero-to-Hero"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>