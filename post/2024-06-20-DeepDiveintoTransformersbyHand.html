<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>Transformers에 대한 심층적인 탐구  | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-20-DeepDiveintoTransformersbyHand" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="Transformers에 대한 심층적인 탐구  | itposting" data-gatsby-head="true"/><meta property="og:title" content="Transformers에 대한 심층적인 탐구  | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-20-DeepDiveintoTransformersbyHand" data-gatsby-head="true"/><meta name="twitter:title" content="Transformers에 대한 심층적인 탐구  | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-20 19:00" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">Transformers에 대한 심층적인 탐구 </h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="Transformers에 대한 심층적인 탐구 " loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 20, 2024</span><span class="posts_reading_time__f7YPP">5<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-20-DeepDiveintoTransformersbyHand&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>트랜스포머의 힘에 대한 세부 정보 살펴보기</h2>
<p>우리 동네에 새로운 변화가 일어났어요.</p>
<p>아들이 "로보 트럭"이라고 부르는 한 대의 '로보-트럭'이 우리 거리에 새 집을 얻었답니다.</p>
<p>이 트럭은 테슬라 사이버 트럭이고, 저는 아들에게 그 이름의 의미를 여러 번 설명해 주었지만 그는 여전히 로보 트럭이라고 부릅니다. 그래서 이제 로보 트럭을 보면 그 이름을 들으면 항상 로보트로 컨버트할 수 있는 로봇들이 나오는 영화 '트랜스포머'를 떠올립니다.</p>
<div class="content-ad"></div>
<p>오늘 우리가 아는 대로, 트랜스포머가 이 로보트럭을 구동하는 데 사용될 수 있다니 이상하지 않나요? 이것은 거의 한 바퀴 도는 순간입니다. 그렇다면 이 모든 얘기를 하고 있는 나는 어디로 향하고 있을까요?</p>
<p>그래요, 나는 목적지로 가고 있어요 — 트랜스포머입니다. 로봇 자동차 트랜스포머가 아니라 신경망 트랜스포머죠. 여러분도 초대됐어요!</p>
<p><img src="/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_0.png" alt="이미지"></p>
<h2>트랜스포머란 무엇인가요?</h2>
<div class="content-ad"></div>
<p>Transformer는 본질적으로 신경망입니다. 데이터에서 맥락을 학습하는 데 특화된 신경망입니다.</p>
<p>하지만 그들을 특별하게 만드는 것은 레이블이 달린 데이터셋과 신경망 내의 컨볼루션 또는 순환을 필요로 하지 않는 메커니즘이 존재한다는 것입니다.</p>
<h2>이 특별한 메커니즘들은 무엇인가요?</h2>
<p>많은 메커니즘이 있지만, Transformer의 핵심인 어텐션 가중치와 피드포워드 네트워크(FFN)라는 두 가지 메커니즘이 진정으로 그들을 특별하게 만드는 힘입니다.</p>
<div class="content-ad"></div>
<h2>어텐션 가중치란 무엇인가요?</h2>
<p>어텐션 가중치는 모델이 들어오는 시퀀스의 어떤 부분에 집중해야 하는지 학습하는 기술입니다. 모든 시간에 모두 주시하는 '사우론의 눈'이 모든 것을 스캔하고 관련 있는 부분에 빛을 비추는 것으로 생각해보세요.</p>
<h2>FFN은 무엇을 의미하나요?</h2>
<p>트랜스포머의 맥락에서, FFN은 주로 일관된 데이터 벡터 집단에 작용하는 일반적인 다층 퍼셉트론입니다. 어텐션과 결합되어 올바른 '위치-차원' 조합을 생성합니다.</p>
<div class="content-ad"></div>
<h1>어텐션과 FFN은 어떻게 작동할까요?</h1>
<p>그러니 더 이상 말더듬거리지 말고, 어텐션 가중치와 FFN이 트랜스포머를 이렇게 강력하게 만드는 방법에 대해 알아봅시다.</p>
<p>이 토론은 톰 예 교수님의 멋진 '손으로 만드는 인공지능' 시리즈에 기반을 두고 있습니다. (아래 이미지는, 별도 언급이 없는 한, 상기 LinkedIn 게시물 중 톰 예 교수님의 것으로, 교수님의 허락을 받아 편집한 것입니다.)</p>
<p>그럼 시작해봅시다!</p>
<div class="content-ad"></div>
<p>여기 중요한 아이디어는 주의 가중치와 피드포워드 네트워크(FFN)입니다.</p>
<p>이것들을 염두에 두고, 우리에게 다음을 제공받는 경우를 가정해 봅시다:</p>
<ul>
<li>이전 블록으로부터 5개의 입력 특성 (여기서 3x5 행렬인 A로, X1, X2, X3, X4 및 X5가 특성이며 각 행은 각각의 특성을 나타냅니다.)</li>
</ul>
<p><img src="/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_1.png" alt="이미지"></p>
<div class="content-ad"></div>
<p>[1] 주의 집중 가중치 행렬 A 획득</p>
<p>과정에서 첫 번째 단계는 주의 집중 가중치 행렬 A를 획득하는 것입니다. 이 부분은 자기 주의 메커니즘이 작용하는 곳입니다. 이 단계는 입력 시퀀스 중에서 가장 관련성 높은 부분을 찾는 데 사용됩니다.</p>
<p>입력 특성을 쿼리-키(QK) 모듈에 공급하여 수행합니다. 간편하게 말해, QK 모듈의 세부 사항은 여기에 포함되어 있지 않습니다.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/1*DYNNNiaZac_ZNGFVUn4aag.gif" alt="이미지"></p>
<div class="content-ad"></div>
<p>[2] 주의 집중 가중치</p>
<p>주의 집중 가중치 행렬 A (5x5)을 얻으면, 입력 기능 (3x5)을 곱하여 주의 집중된 기능 Z를 얻습니다.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/1*1_VmXxp6iPkwVEdhFwExkg.gif" alt="image"></p>
<p>여기서 중요한 점은 기능이 위치 P1, P2 및 P3에 따라 가로로 결합된다는 것입니다.</p>
<div class="content-ad"></div>
<p>다음과 같이 세분화된 계산을 행별로 수행해보세요:</p>
<p>P1 X A1 = Z1 → 위치 [1,1] = 11</p>
<p>P1 X A2 = Z2 → 위치 [1,2] = 6</p>
<p>P1 X A3 = Z3 → 위치 [1,3] = 7</p>
<div class="content-ad"></div>
<p>P1 X A4 = Z4 → Position [1,4] = 7</p>
<p>P1 X A5 = Z5 → Position [1,5] = 5</p>
<p>.</p>
<p>.</p>
<div class="content-ad"></div>
<p>변경된 내용은 다음과 같습니다:</p>
<p>P2 X A4 = Z4 → Position [2,4] = 3</p>
<p>P3 X A5 = Z5 → Position [3,5] = 1</p>
<p>이것이 예시입니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_2.png" alt="image"></p>
<p>처음에는 조금 귀찮아 보일 수 있지만 행별 곱셈을 따르면 결과는 매우 직관적일 것입니다.</p>
<p>멋진 점은 우리의 주의 가중치 행렬 A가 배열된 방식 때문에 새로운 특징 Z가 X의 조합으로 나타난다는 것이다:</p>
<p>Z1 = X1 + X2</p>
<div class="content-ad"></div>
<p>Z2 = X2 + X3</p>
<p>Z3 = X3 + X4</p>
<p>Z4 = X4 + X5</p>
<p>Z5 = X5 + X1</p>
<div class="content-ad"></div>
<p>(힌트: 행렬 A에서 0과 1의 위치를 살펴보세요).</p>
<p>[3] FFN: 첫 번째 레이어</p>
<p>다음 단계는 어텐션 가중치가 적용된 피쳐를 피드포워드 신경망에 전달하는 것입니다.</p>
<p>그러나 이번에는 이전 단계에서의 위치가 아닌 차원을 가로지르는 값들을 결합하는 것이 차이점입니다. 아래처럼 수행됩니다:</p>
<div class="content-ad"></div>
<p>아래에 있는 링크를 사진으로 보여줄게요.</p>
<ul>
<li>
<p>관심 단계에서는 원래 특징을 기반으로 입력을 결합하여 새로운 특징을 얻었어요.</p>
</li>
<li>
<p>FFN 단계에서는 그들의 특성을 고려하여 새로운 행렬을 얻기 위해 특징을 세로로 결합해요.</p>
</li>
</ul>
<div class="content-ad"></div>
<p>한 번 더 element-wise 행 연산이 도움이 됩니다. 여기서 새 행렬의 차원 수가 4로 증가했다는 점에 주목하세요.</p>
<p>[4] ReLU</p>
<p>저희가 가장 좋아하는 단계 : ReLU는 이전 행렬에서 얻은 음의 값이 0으로 반환되고 양의 값은 변경되지 않습니다.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/1*FmroND2LsW91TrYXNh2UGQ.gif" alt="이미지"></p>
<div class="content-ad"></div>
<p>[5] FFN : 두 번째 레이어</p>
<p>결과 매트릭스의 차원을 4에서 3으로 줄이는 두 번째 레이어를 통과합니다.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/1*z0CE0MMXVIuuu0qPYybrjA.gif" alt="Image"></p>
<p>여기서의 출력은 다음 블록으로 공급할 준비가 되어 있습니다 (원본 매트릭스와 유사성을 확인하십시오) 및 전체 프로세스가 처음부터 반복됩니다.</p>
<div class="content-ad"></div>
<p>여기서 기억해야 할 두 가지 주요 사항은 다음과 같습니다:</p>
<ul>
<li>어텐션 레이어는 위치를 가로 방향으로 결합합니다.</li>
<li>피드포워드 레이어는 차원을 세로 방향으로 결합합니다.</li>
</ul>
<p>이것이 트랜스포머의 강력함에 대한 비밀 소스입니다. 데이터를 여러 방향에서 분석하는 능력입니다.</p>
<p>위의 아이디어를 요약하면 다음과 같은 주요 포인트가 있습니다:</p>
<div class="content-ad"></div>
<ul>
<li>트랜스포머 아키텍처는 어텐션 레이어와 피드-포워드 레이어의 조합으로 이해될 수 있습니다.</li>
<li>어텐션 레이어는 특성을 결합하여 새로운 특성을 생성합니다. 예를 들어 두 로봇 Robo-Truck과 Optimus Prime을 결합하여 새로운 로봇인 Robtimus Prime을 얻는 것을 생각해보세요.</li>
<li>피드-포워드(FFN) 레이어는 특성의 부분이나 특성을 결합하여 새로운 부분/특성을 생성합니다. 예를 들어 Robo-Truck의 바퀴와 Optimus Prime의 이온 레이저가 합쳐져 바퀴 레이저가 될 수 있습니다.</li>
</ul>
<h1>늘같이 강력한 트랜스포머</h1>
<p>신경망은 상당히 오랫동안 존재해왔습니다. 합성곱 신경망(CNN)과 순환 신경망(RNN)이 주류인 동안 2017년에 트랜스포머가 소개되면서 상황이 크게 바뀌었습니다. 그 이후로 인공지능 분야는 기하급수적으로 성장했고 매일 새로운 모델, 새로운 기준, 새로운 배움이 이어졌습니다. 그리고 언제가 미래에 더 큰 변화를 이끌어낼 수 있는 현상적인 아이디어로 발전할지에 대해 시간만이 알게 해줄 것입니다. 그러나 현재는 아이디어가 우리 삶을 어떻게 변화시킬 수 있는지를 생각해보는 것이 잘못된 말이 아닐 것입니다!</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_3.png">
<p>P.S. 이 연습을 혼자 진행하고 싶다면, 여기에 사용할 빈 템플릿이 있어요.</p>
<p>Robtimus Prime를 만들어서 즐거운 시간 보내세요!</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Transformers에 대한 심층적인 탐구 ","description":"","date":"2024-06-20 19:00","slug":"2024-06-20-DeepDiveintoTransformersbyHand","content":"\n\n## 트랜스포머의 힘에 대한 세부 정보 살펴보기\n\n우리 동네에 새로운 변화가 일어났어요.\n\n아들이 \"로보 트럭\"이라고 부르는 한 대의 '로보-트럭'이 우리 거리에 새 집을 얻었답니다.\n\n이 트럭은 테슬라 사이버 트럭이고, 저는 아들에게 그 이름의 의미를 여러 번 설명해 주었지만 그는 여전히 로보 트럭이라고 부릅니다. 그래서 이제 로보 트럭을 보면 그 이름을 들으면 항상 로보트로 컨버트할 수 있는 로봇들이 나오는 영화 '트랜스포머'를 떠올립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오늘 우리가 아는 대로, 트랜스포머가 이 로보트럭을 구동하는 데 사용될 수 있다니 이상하지 않나요? 이것은 거의 한 바퀴 도는 순간입니다. 그렇다면 이 모든 얘기를 하고 있는 나는 어디로 향하고 있을까요?\n\n그래요, 나는 목적지로 가고 있어요 — 트랜스포머입니다. 로봇 자동차 트랜스포머가 아니라 신경망 트랜스포머죠. 여러분도 초대됐어요!\n\n![이미지](/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_0.png)\n\n## 트랜스포머란 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTransformer는 본질적으로 신경망입니다. 데이터에서 맥락을 학습하는 데 특화된 신경망입니다.\n\n하지만 그들을 특별하게 만드는 것은 레이블이 달린 데이터셋과 신경망 내의 컨볼루션 또는 순환을 필요로 하지 않는 메커니즘이 존재한다는 것입니다.\n\n## 이 특별한 메커니즘들은 무엇인가요?\n\n많은 메커니즘이 있지만, Transformer의 핵심인 어텐션 가중치와 피드포워드 네트워크(FFN)라는 두 가지 메커니즘이 진정으로 그들을 특별하게 만드는 힘입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 어텐션 가중치란 무엇인가요?\n\n어텐션 가중치는 모델이 들어오는 시퀀스의 어떤 부분에 집중해야 하는지 학습하는 기술입니다. 모든 시간에 모두 주시하는 '사우론의 눈'이 모든 것을 스캔하고 관련 있는 부분에 빛을 비추는 것으로 생각해보세요.\n\n## FFN은 무엇을 의미하나요?\n\n트랜스포머의 맥락에서, FFN은 주로 일관된 데이터 벡터 집단에 작용하는 일반적인 다층 퍼셉트론입니다. 어텐션과 결합되어 올바른 '위치-차원' 조합을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 어텐션과 FFN은 어떻게 작동할까요?\n\n그러니 더 이상 말더듬거리지 말고, 어텐션 가중치와 FFN이 트랜스포머를 이렇게 강력하게 만드는 방법에 대해 알아봅시다.\n\n이 토론은 톰 예 교수님의 멋진 '손으로 만드는 인공지능' 시리즈에 기반을 두고 있습니다. (아래 이미지는, 별도 언급이 없는 한, 상기 LinkedIn 게시물 중 톰 예 교수님의 것으로, 교수님의 허락을 받아 편집한 것입니다.)\n\n그럼 시작해봅시다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 중요한 아이디어는 주의 가중치와 피드포워드 네트워크(FFN)입니다.\n\n이것들을 염두에 두고, 우리에게 다음을 제공받는 경우를 가정해 봅시다:\n\n- 이전 블록으로부터 5개의 입력 특성 (여기서 3x5 행렬인 A로, X1, X2, X3, X4 및 X5가 특성이며 각 행은 각각의 특성을 나타냅니다.)\n\n![이미지](/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[1] 주의 집중 가중치 행렬 A 획득\n\n과정에서 첫 번째 단계는 주의 집중 가중치 행렬 A를 획득하는 것입니다. 이 부분은 자기 주의 메커니즘이 작용하는 곳입니다. 이 단계는 입력 시퀀스 중에서 가장 관련성 높은 부분을 찾는 데 사용됩니다. \n\n입력 특성을 쿼리-키(QK) 모듈에 공급하여 수행합니다. 간편하게 말해, QK 모듈의 세부 사항은 여기에 포함되어 있지 않습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*DYNNNiaZac_ZNGFVUn4aag.gif)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[2] 주의 집중 가중치\n\n주의 집중 가중치 행렬 A (5x5)을 얻으면, 입력 기능 (3x5)을 곱하여 주의 집중된 기능 Z를 얻습니다.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*1_VmXxp6iPkwVEdhFwExkg.gif)\n\n여기서 중요한 점은 기능이 위치 P1, P2 및 P3에 따라 가로로 결합된다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음과 같이 세분화된 계산을 행별로 수행해보세요:\n\nP1 X A1 = Z1 → 위치 [1,1] = 11\n\nP1 X A2 = Z2 → 위치 [1,2] = 6\n\nP1 X A3 = Z3 → 위치 [1,3] = 7\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nP1 X A4 = Z4 → Position [1,4] = 7\n\nP1 X A5 = Z5 → Position [1,5] = 5\n\n.\n\n.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n변경된 내용은 다음과 같습니다:\n\nP2 X A4 = Z4 → Position [2,4] = 3\n\nP3 X A5 = Z5 → Position [3,5] = 1\n\n이것이 예시입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_2.png)\n\n처음에는 조금 귀찮아 보일 수 있지만 행별 곱셈을 따르면 결과는 매우 직관적일 것입니다.\n\n멋진 점은 우리의 주의 가중치 행렬 A가 배열된 방식 때문에 새로운 특징 Z가 X의 조합으로 나타난다는 것이다:\n\nZ1 = X1 + X2\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nZ2 = X2 + X3\n\nZ3 = X3 + X4\n\nZ4 = X4 + X5\n\nZ5 = X5 + X1\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n(힌트: 행렬 A에서 0과 1의 위치를 살펴보세요).\n\n[3] FFN: 첫 번째 레이어\n\n다음 단계는 어텐션 가중치가 적용된 피쳐를 피드포워드 신경망에 전달하는 것입니다.\n\n그러나 이번에는 이전 단계에서의 위치가 아닌 차원을 가로지르는 값들을 결합하는 것이 차이점입니다. 아래처럼 수행됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래에 있는 링크를 사진으로 보여줄게요.\n\n- 관심 단계에서는 원래 특징을 기반으로 입력을 결합하여 새로운 특징을 얻었어요.\n\n- FFN 단계에서는 그들의 특성을 고려하여 새로운 행렬을 얻기 위해 특징을 세로로 결합해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 번 더 element-wise 행 연산이 도움이 됩니다. 여기서 새 행렬의 차원 수가 4로 증가했다는 점에 주목하세요.\n\n[4] ReLU\n\n저희가 가장 좋아하는 단계 : ReLU는 이전 행렬에서 얻은 음의 값이 0으로 반환되고 양의 값은 변경되지 않습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*FmroND2LsW91TrYXNh2UGQ.gif)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[5] FFN : 두 번째 레이어\n\n결과 매트릭스의 차원을 4에서 3으로 줄이는 두 번째 레이어를 통과합니다.\n\n![Image](https://miro.medium.com/v2/resize:fit:1400/1*z0CE0MMXVIuuu0qPYybrjA.gif)\n\n여기서의 출력은 다음 블록으로 공급할 준비가 되어 있습니다 (원본 매트릭스와 유사성을 확인하십시오) 및 전체 프로세스가 처음부터 반복됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 기억해야 할 두 가지 주요 사항은 다음과 같습니다:\n\n- 어텐션 레이어는 위치를 가로 방향으로 결합합니다.\n- 피드포워드 레이어는 차원을 세로 방향으로 결합합니다.\n\n이것이 트랜스포머의 강력함에 대한 비밀 소스입니다. 데이터를 여러 방향에서 분석하는 능력입니다.\n\n위의 아이디어를 요약하면 다음과 같은 주요 포인트가 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 트랜스포머 아키텍처는 어텐션 레이어와 피드-포워드 레이어의 조합으로 이해될 수 있습니다.\n- 어텐션 레이어는 특성을 결합하여 새로운 특성을 생성합니다. 예를 들어 두 로봇 Robo-Truck과 Optimus Prime을 결합하여 새로운 로봇인 Robtimus Prime을 얻는 것을 생각해보세요.\n- 피드-포워드(FFN) 레이어는 특성의 부분이나 특성을 결합하여 새로운 부분/특성을 생성합니다. 예를 들어 Robo-Truck의 바퀴와 Optimus Prime의 이온 레이저가 합쳐져 바퀴 레이저가 될 수 있습니다.\n\n# 늘같이 강력한 트랜스포머\n\n신경망은 상당히 오랫동안 존재해왔습니다. 합성곱 신경망(CNN)과 순환 신경망(RNN)이 주류인 동안 2017년에 트랜스포머가 소개되면서 상황이 크게 바뀌었습니다. 그 이후로 인공지능 분야는 기하급수적으로 성장했고 매일 새로운 모델, 새로운 기준, 새로운 배움이 이어졌습니다. 그리고 언제가 미래에 더 큰 변화를 이끌어낼 수 있는 현상적인 아이디어로 발전할지에 대해 시간만이 알게 해줄 것입니다. 그러나 현재는 아이디어가 우리 삶을 어떻게 변화시킬 수 있는지를 생각해보는 것이 잘못된 말이 아닐 것입니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_3.png\" /\u003e\n\nP.S. 이 연습을 혼자 진행하고 싶다면, 여기에 사용할 빈 템플릿이 있어요.\n\nRobtimus Prime를 만들어서 즐거운 시간 보내세요!","ogImage":{"url":"/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_0.png"},"coverImage":"/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_0.png","tag":["Tech"],"readingTime":5},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e트랜스포머의 힘에 대한 세부 정보 살펴보기\u003c/h2\u003e\n\u003cp\u003e우리 동네에 새로운 변화가 일어났어요.\u003c/p\u003e\n\u003cp\u003e아들이 \"로보 트럭\"이라고 부르는 한 대의 '로보-트럭'이 우리 거리에 새 집을 얻었답니다.\u003c/p\u003e\n\u003cp\u003e이 트럭은 테슬라 사이버 트럭이고, 저는 아들에게 그 이름의 의미를 여러 번 설명해 주었지만 그는 여전히 로보 트럭이라고 부릅니다. 그래서 이제 로보 트럭을 보면 그 이름을 들으면 항상 로보트로 컨버트할 수 있는 로봇들이 나오는 영화 '트랜스포머'를 떠올립니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e오늘 우리가 아는 대로, 트랜스포머가 이 로보트럭을 구동하는 데 사용될 수 있다니 이상하지 않나요? 이것은 거의 한 바퀴 도는 순간입니다. 그렇다면 이 모든 얘기를 하고 있는 나는 어디로 향하고 있을까요?\u003c/p\u003e\n\u003cp\u003e그래요, 나는 목적지로 가고 있어요 — 트랜스포머입니다. 로봇 자동차 트랜스포머가 아니라 신경망 트랜스포머죠. 여러분도 초대됐어요!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch2\u003e트랜스포머란 무엇인가요?\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eTransformer는 본질적으로 신경망입니다. 데이터에서 맥락을 학습하는 데 특화된 신경망입니다.\u003c/p\u003e\n\u003cp\u003e하지만 그들을 특별하게 만드는 것은 레이블이 달린 데이터셋과 신경망 내의 컨볼루션 또는 순환을 필요로 하지 않는 메커니즘이 존재한다는 것입니다.\u003c/p\u003e\n\u003ch2\u003e이 특별한 메커니즘들은 무엇인가요?\u003c/h2\u003e\n\u003cp\u003e많은 메커니즘이 있지만, Transformer의 핵심인 어텐션 가중치와 피드포워드 네트워크(FFN)라는 두 가지 메커니즘이 진정으로 그들을 특별하게 만드는 힘입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e어텐션 가중치란 무엇인가요?\u003c/h2\u003e\n\u003cp\u003e어텐션 가중치는 모델이 들어오는 시퀀스의 어떤 부분에 집중해야 하는지 학습하는 기술입니다. 모든 시간에 모두 주시하는 '사우론의 눈'이 모든 것을 스캔하고 관련 있는 부분에 빛을 비추는 것으로 생각해보세요.\u003c/p\u003e\n\u003ch2\u003eFFN은 무엇을 의미하나요?\u003c/h2\u003e\n\u003cp\u003e트랜스포머의 맥락에서, FFN은 주로 일관된 데이터 벡터 집단에 작용하는 일반적인 다층 퍼셉트론입니다. 어텐션과 결합되어 올바른 '위치-차원' 조합을 생성합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e어텐션과 FFN은 어떻게 작동할까요?\u003c/h1\u003e\n\u003cp\u003e그러니 더 이상 말더듬거리지 말고, 어텐션 가중치와 FFN이 트랜스포머를 이렇게 강력하게 만드는 방법에 대해 알아봅시다.\u003c/p\u003e\n\u003cp\u003e이 토론은 톰 예 교수님의 멋진 '손으로 만드는 인공지능' 시리즈에 기반을 두고 있습니다. (아래 이미지는, 별도 언급이 없는 한, 상기 LinkedIn 게시물 중 톰 예 교수님의 것으로, 교수님의 허락을 받아 편집한 것입니다.)\u003c/p\u003e\n\u003cp\u003e그럼 시작해봅시다!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e여기 중요한 아이디어는 주의 가중치와 피드포워드 네트워크(FFN)입니다.\u003c/p\u003e\n\u003cp\u003e이것들을 염두에 두고, 우리에게 다음을 제공받는 경우를 가정해 봅시다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e이전 블록으로부터 5개의 입력 특성 (여기서 3x5 행렬인 A로, X1, X2, X3, X4 및 X5가 특성이며 각 행은 각각의 특성을 나타냅니다.)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e[1] 주의 집중 가중치 행렬 A 획득\u003c/p\u003e\n\u003cp\u003e과정에서 첫 번째 단계는 주의 집중 가중치 행렬 A를 획득하는 것입니다. 이 부분은 자기 주의 메커니즘이 작용하는 곳입니다. 이 단계는 입력 시퀀스 중에서 가장 관련성 높은 부분을 찾는 데 사용됩니다.\u003c/p\u003e\n\u003cp\u003e입력 특성을 쿼리-키(QK) 모듈에 공급하여 수행합니다. 간편하게 말해, QK 모듈의 세부 사항은 여기에 포함되어 있지 않습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*DYNNNiaZac_ZNGFVUn4aag.gif\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e[2] 주의 집중 가중치\u003c/p\u003e\n\u003cp\u003e주의 집중 가중치 행렬 A (5x5)을 얻으면, 입력 기능 (3x5)을 곱하여 주의 집중된 기능 Z를 얻습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*1_VmXxp6iPkwVEdhFwExkg.gif\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서 중요한 점은 기능이 위치 P1, P2 및 P3에 따라 가로로 결합된다는 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다음과 같이 세분화된 계산을 행별로 수행해보세요:\u003c/p\u003e\n\u003cp\u003eP1 X A1 = Z1 → 위치 [1,1] = 11\u003c/p\u003e\n\u003cp\u003eP1 X A2 = Z2 → 위치 [1,2] = 6\u003c/p\u003e\n\u003cp\u003eP1 X A3 = Z3 → 위치 [1,3] = 7\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eP1 X A4 = Z4 → Position [1,4] = 7\u003c/p\u003e\n\u003cp\u003eP1 X A5 = Z5 → Position [1,5] = 5\u003c/p\u003e\n\u003cp\u003e.\u003c/p\u003e\n\u003cp\u003e.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e변경된 내용은 다음과 같습니다:\u003c/p\u003e\n\u003cp\u003eP2 X A4 = Z4 → Position [2,4] = 3\u003c/p\u003e\n\u003cp\u003eP3 X A5 = Z5 → Position [3,5] = 1\u003c/p\u003e\n\u003cp\u003e이것이 예시입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_2.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e처음에는 조금 귀찮아 보일 수 있지만 행별 곱셈을 따르면 결과는 매우 직관적일 것입니다.\u003c/p\u003e\n\u003cp\u003e멋진 점은 우리의 주의 가중치 행렬 A가 배열된 방식 때문에 새로운 특징 Z가 X의 조합으로 나타난다는 것이다:\u003c/p\u003e\n\u003cp\u003eZ1 = X1 + X2\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eZ2 = X2 + X3\u003c/p\u003e\n\u003cp\u003eZ3 = X3 + X4\u003c/p\u003e\n\u003cp\u003eZ4 = X4 + X5\u003c/p\u003e\n\u003cp\u003eZ5 = X5 + X1\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e(힌트: 행렬 A에서 0과 1의 위치를 살펴보세요).\u003c/p\u003e\n\u003cp\u003e[3] FFN: 첫 번째 레이어\u003c/p\u003e\n\u003cp\u003e다음 단계는 어텐션 가중치가 적용된 피쳐를 피드포워드 신경망에 전달하는 것입니다.\u003c/p\u003e\n\u003cp\u003e그러나 이번에는 이전 단계에서의 위치가 아닌 차원을 가로지르는 값들을 결합하는 것이 차이점입니다. 아래처럼 수행됩니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래에 있는 링크를 사진으로 보여줄게요.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e관심 단계에서는 원래 특징을 기반으로 입력을 결합하여 새로운 특징을 얻었어요.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFFN 단계에서는 그들의 특성을 고려하여 새로운 행렬을 얻기 위해 특징을 세로로 결합해요.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e한 번 더 element-wise 행 연산이 도움이 됩니다. 여기서 새 행렬의 차원 수가 4로 증가했다는 점에 주목하세요.\u003c/p\u003e\n\u003cp\u003e[4] ReLU\u003c/p\u003e\n\u003cp\u003e저희가 가장 좋아하는 단계 : ReLU는 이전 행렬에서 얻은 음의 값이 0으로 반환되고 양의 값은 변경되지 않습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*FmroND2LsW91TrYXNh2UGQ.gif\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e[5] FFN : 두 번째 레이어\u003c/p\u003e\n\u003cp\u003e결과 매트릭스의 차원을 4에서 3으로 줄이는 두 번째 레이어를 통과합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*z0CE0MMXVIuuu0qPYybrjA.gif\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서의 출력은 다음 블록으로 공급할 준비가 되어 있습니다 (원본 매트릭스와 유사성을 확인하십시오) 및 전체 프로세스가 처음부터 반복됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e여기서 기억해야 할 두 가지 주요 사항은 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e어텐션 레이어는 위치를 가로 방향으로 결합합니다.\u003c/li\u003e\n\u003cli\u003e피드포워드 레이어는 차원을 세로 방향으로 결합합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이것이 트랜스포머의 강력함에 대한 비밀 소스입니다. 데이터를 여러 방향에서 분석하는 능력입니다.\u003c/p\u003e\n\u003cp\u003e위의 아이디어를 요약하면 다음과 같은 주요 포인트가 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e트랜스포머 아키텍처는 어텐션 레이어와 피드-포워드 레이어의 조합으로 이해될 수 있습니다.\u003c/li\u003e\n\u003cli\u003e어텐션 레이어는 특성을 결합하여 새로운 특성을 생성합니다. 예를 들어 두 로봇 Robo-Truck과 Optimus Prime을 결합하여 새로운 로봇인 Robtimus Prime을 얻는 것을 생각해보세요.\u003c/li\u003e\n\u003cli\u003e피드-포워드(FFN) 레이어는 특성의 부분이나 특성을 결합하여 새로운 부분/특성을 생성합니다. 예를 들어 Robo-Truck의 바퀴와 Optimus Prime의 이온 레이저가 합쳐져 바퀴 레이저가 될 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e늘같이 강력한 트랜스포머\u003c/h1\u003e\n\u003cp\u003e신경망은 상당히 오랫동안 존재해왔습니다. 합성곱 신경망(CNN)과 순환 신경망(RNN)이 주류인 동안 2017년에 트랜스포머가 소개되면서 상황이 크게 바뀌었습니다. 그 이후로 인공지능 분야는 기하급수적으로 성장했고 매일 새로운 모델, 새로운 기준, 새로운 배움이 이어졌습니다. 그리고 언제가 미래에 더 큰 변화를 이끌어낼 수 있는 현상적인 아이디어로 발전할지에 대해 시간만이 알게 해줄 것입니다. 그러나 현재는 아이디어가 우리 삶을 어떻게 변화시킬 수 있는지를 생각해보는 것이 잘못된 말이 아닐 것입니다!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-20-DeepDiveintoTransformersbyHand_3.png\"\u003e\n\u003cp\u003eP.S. 이 연습을 혼자 진행하고 싶다면, 여기에 사용할 빈 템플릿이 있어요.\u003c/p\u003e\n\u003cp\u003eRobtimus Prime를 만들어서 즐거운 시간 보내세요!\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-20-DeepDiveintoTransformersbyHand"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>