<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>기존의 GPT-4보다 뛰어난 성능을 자랑하는 스탠포드 대학팀의 대형 모델이 모바일폰에서도 구동될 수 있다는 점이 인기를 끌며 하룻밤 사이에 2천 회 이상 다운로드되었습니다 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="기존의 GPT-4보다 뛰어난 성능을 자랑하는 스탠포드 대학팀의 대형 모델이 모바일폰에서도 구동될 수 있다는 점이 인기를 끌며 하룻밤 사이에 2천 회 이상 다운로드되었습니다 | itposting" data-gatsby-head="true"/><meta property="og:title" content="기존의 GPT-4보다 뛰어난 성능을 자랑하는 스탠포드 대학팀의 대형 모델이 모바일폰에서도 구동될 수 있다는 점이 인기를 끌며 하룻밤 사이에 2천 회 이상 다운로드되었습니다 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight" data-gatsby-head="true"/><meta name="twitter:title" content="기존의 GPT-4보다 뛰어난 성능을 자랑하는 스탠포드 대학팀의 대형 모델이 모바일폰에서도 구동될 수 있다는 점이 인기를 끌며 하룻밤 사이에 2천 회 이상 다운로드되었습니다 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-27 14:30" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-0fd008072af5a644.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_buildManifest.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">기존의 GPT-4보다 뛰어난 성능을 자랑하는 스탠포드 대학팀의 대형 모델이 모바일폰에서도 구동될 수 있다는 점이 인기를 끌며 하룻밤 사이에 2천 회 이상 다운로드되었습니다</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="기존의 GPT-4보다 뛰어난 성능을 자랑하는 스탠포드 대학팀의 대형 모델이 모바일폰에서도 구동될 수 있다는 점이 인기를 끌며 하룻밤 사이에 2천 회 이상 다운로드되었습니다" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 27, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_0.png" alt="Octopus v2"></p>
<p>대규모 모델 구현의 과정에서, 엔드사이드 AI는 매우 중요한 방향입니다.</p>
<p>최근 스탠퍼드 대학의 연구자들이 출시한 Octopus v2는 개발자 커뮤니티로부터 큰 관심을 받으며 인기를 끌고 있습니다. 모델의 다운로드 횟수가 하룻밤 사이에 2천 건을 넘었습니다.</p>
<p>200억 개의 파라미터를 갖는 Octopus v2는 정확도와 대기 시간 측면에서 GPT-4를 능가하며, 콘텍스트 길이를 95% 줄였습니다. 또한, Octopus v2는 Llama7B + RAG 구성보다 36배 빠릅니다.</p>
<p>많은 네티즌들이 한탄했습니다: 디바이스 측 인공지능 에이전트 시대가 도래했습니다!</p>
<p><img src="/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_1.png" alt="image"></p>
<ul>
<li>논문: 옥토퍼스 v2: 슈퍼 에이전트용 기기 내 언어 모델</li>
<li>논문 주소: <a href="https://arxiv.org/abs/2404.01744" rel="nofollow" target="_blank">https://arxiv.org/abs/2404.01744</a></li>
<li>모델 홈페이지: <a href="https://huggingface.co/NexaAIDev/Octopus-v2" rel="nofollow" target="_blank">https://huggingface.co/NexaAIDev/Octopus-v2</a></li>
</ul>
<p>모델 개요</p>
<p>옥토퍼스-V2-2B는 안드로이드 API에 맞게 설계된 20 억 개의 매개변수를 가진 오픈 소스 언어 모델로, 안드로이드 기기에서 원활하게 실행되며 안드로이드 시스템 관리에서 여러 기기 및 다양한 응용 프로그램의 조작까지 확장하는 데 사용됩니다.</p>
<p><img src="/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_2.png" alt="이미지"></p>
<p>일반적으로 검색 증강 생성 (RAG) 방법은 잠재적인 기능 매개변수에 대한 상세한 설명을 필요로 하며 (때로는 수만 개의 입력 토큰이 필요할 수도 있음), 이에 기반하여 옥토퍼스-V2-2B는 훈련 및 추론 단계에서 고유한 기능 토큰 전략을 도입하여 GPT-4와 유사한 성능 수준을 달성할 뿐만 아니라 추론 속도를 크게 향상시켜 RAG 기반 방법을 능가합니다. 이로 인해 엣지 컴퓨팅 기기에 특히 유용합니다.</p>
<p><img src="/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_3.png" alt="이미지"></p>
<p>문어-V2-2B는 다양한 복잡한 시나리오에서 개별, 중첩 및 병렬 함수 호출을 생성할 수 있습니다.</p>
<p>데이터 세트</p>
<p>훈련, 검증 및 테스트 단계에서 고품질 데이터 세트를 사용하고 특히 효율적인 훈련을 달성하기 위해, 연구팀은 데이터 세트를 세 가지 주요 단계로 생성했습니다:</p>
<ul>
<li>관련 쿼리 및 관련 함수 호출 매개변수 생성;</li>
<li>적절한 기능 구성요소에서 관련이 없는 쿼리 생성;</li>
<li>Google Gemini을 통한 이진 검증 지원.</li>
</ul>
<p><img src="/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_4.png" alt="image"></p>
<p>Research team은 이 모델을 훈련하기 위해 20가지의 안드로이드 API 설명을 작성했습니다. 다음은 예시 안드로이드 API 설명입니다:</p>
<pre><code class="hljs language-js">def get_trending_news (category=<span class="hljs-title class_">None</span>, region=<span class="hljs-string">'US'</span>, language=<span class="hljs-string">'en'</span>, max_results=<span class="hljs-number">5</span>):
    <span class="hljs-string">""</span><span class="hljs-string">"
    카테고리, 지역 및 언어에 기반한 트렌드 뉴스 기사를 가져옵니다.
    Parameters:
    - category (str, optional): 필터링할 뉴스 카테고리입니다. 모든 카테고리에 대해 기본값으로 None을 사용합니다. 선택적으로 제공할 수 있습니다.
    - region (str, optional): 지역별 뉴스를 위한 ISO 3166-1 알파-2 국가 코드입니다. 기본값으로 'US'를 사용합니다. 선택적으로 제공할 수 있습니다.
    - language (str, optional): 기사 언어를 위한 ISO 639-1 언어 코드입니다. 기본값으로 'en'을 사용합니다. 선택적으로 제공할 수 있습니다.
    - max_results (int, optional): 반환할 기사의 최대 수입니다. 기본값으로 5를 사용합니다. 선택적으로 제공할 수 있습니다.
    Returns:
    - list [str]: 각각 기사를 나타내는 문자열의 목록입니다. 각 문자열은 기사 제목과 URL을 포함합니다.
    "</span><span class="hljs-string">""</span>
</code></pre>
<p>모델 개발 및 훈련</p>
<p>이 연구는 프레임워크에서 Google Gemma-2B 모델을 사전 학습 모델로 사용하며 두 가지 다른 훈련 방법을 채택합니다: 전체 모델 훈련과 LoRA 모델 훈련.</p>
<p>전체 모델 훈련에서는 AdamW 옵티마이저를 사용하며 학습률은 5e-5로 설정되고 웜업 단계 수는 10으로, 선형 학습률 스케줄러가 사용됩니다.</p>
<p>LoRA 모델 훈련은 전체 모델 훈련과 동일한 옵티마이저와 학습률 구성을 사용하며 LoRA 랭크는 16으로 설정되며, LoRA는 다음 모듈에 적용됩니다: q_proj, k_proj, v_proj, o_proj, up_proj, down_proj. 그 중 LoRA 알파 매개변수는 32로 설정됩니다.</p>
<p>두 훈련 방법 모두 에포크 수는 3으로 설정됩니다.</p>
<p>다음 코드를 사용하면 단일 GPU에서 Octopus-V2-2B 모델을 실행할 수 있습니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> <span class="hljs-title class_">AutoTokenizer</span>, <span class="hljs-title class_">GemmaForCausalLM</span>
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> time

def <span class="hljs-title function_">inference</span>(input_text):
    start_time = time.<span class="hljs-title function_">time</span>()
    input_ids = <span class="hljs-title function_">tokenizer</span>(input_text, return_tensors=<span class="hljs-string">"pt"</span>).<span class="hljs-title function_">to</span>(model.<span class="hljs-property">device</span>)
    input_length = input_ids[<span class="hljs-string">"input_ids"</span>].<span class="hljs-property">shape</span>[<span class="hljs-number">1</span>]
    outputs = model.<span class="hljs-title function_">generate</span>(
        input_ids=input_ids[<span class="hljs-string">"input_ids"</span>],
        max_length=<span class="hljs-number">1024</span>,
        do_sample=<span class="hljs-title class_">False</span>
    )
    generated_sequence = outputs[:, <span class="hljs-attr">input_length</span>:].<span class="hljs-title function_">tolist</span>()
    res = tokenizer.<span class="hljs-title function_">decode</span>(generated_sequence[<span class="hljs-number">0</span>])
    end_time = time.<span class="hljs-title function_">time</span>()
    <span class="hljs-keyword">return</span> {<span class="hljs-string">"output"</span>: res, <span class="hljs-string">"latency"</span>: end_time - start_time}

model_id = <span class="hljs-string">"NexaAIDev/Octopus-v2"</span>
tokenizer = <span class="hljs-title class_">AutoTokenizer</span>.<span class="hljs-title function_">from_pretrained</span>(model_id)
model = <span class="hljs-title class_">GemmaForCausalLM</span>.<span class="hljs-title function_">from_pretrained</span>(
    model_id, torch_dtype=torch.<span class="hljs-property">bfloat16</span>, device_map=<span class="hljs-string">"auto"</span>
)

input_text = <span class="hljs-string">"Take a selfie for me with front camera"</span>
nexa_query = f<span class="hljs-string">"아래는 사용자쿼리입니다. 올바른 함수를 호출하고 함수를 호출하는 매개변수를 생성하십시오.\n\n쿼리: {input_text}\n\n응답:"</span>
start_time = time.<span class="hljs-title function_">time</span>()
<span class="hljs-title function_">print</span>(<span class="hljs-string">"넥사 모델 결과:\n"</span>, <span class="hljs-title function_">inference</span>(nexa_query))
<span class="hljs-title function_">print</span>(<span class="hljs-string">"latency:"</span>, time.<span class="hljs-title function_">time</span>() - start_time, <span class="hljs-string">"초"</span>)
</code></pre>
<p>평가</p>
<p>벤치마크 테스트에서 Octopus-V2-2B는 단일 A100 GPU에서 "Llama7B + RAG 솔루션"보다 36배 빠른 탁월한 추론 속도를 보여주었습니다. 게다가, Octopus-V2-2B는 클러스터화된 A100/H100 GPU에 의존하는 GPT-4-turbo보다 168% 빠릅니다. 이 효율적인 개선은 Octopus-V2-2B의 기능 토큰 디자인에 기인합니다.</p>
<p><img src="/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_5.png" alt="Octopus-V2-2B"></p>
<p>옥토퍼스-V2-2B는 속도뿐만 아니라 정확도 면에서도 우수하며, "라마7B + RAG 솔루션"을 31% 초과하는 함수 호출 정확도로 능가합니다. 옥토퍼스-V2-2B는 GPT-4 및 RAG + GPT-3.5와 비교 가능한 함수 호출 정확도를 달성합니다.</p>
<p><img src="/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_6.png" alt="Learn More"></p>
<p>관심 있는 독자들은 연구 내용에 대한 원본 논문을 읽어서 더 많이 알아볼 수 있습니다.</p>
<p>참고: <a href="https://mp.weixin.qq.com/s/qnFZOPLpdRxW42_cLUcImA" rel="nofollow" target="_blank">https://mp.weixin.qq.com/s/qnFZOPLpdRxW42_cLUcImA</a></p>
<p>기사가 마음에 드셨나요? 더 많은 학습을 원하신다면 제한 없이 읽을 수 있는 Medium 회원이 되어보세요. 이 링크를 통해 회원이 되면 추가 비용 없이 저를 지원해 주시게 됩니다. 미리 감사드리고 앞으로 또 만나요!</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"기존의 GPT-4보다 뛰어난 성능을 자랑하는 스탠포드 대학팀의 대형 모델이 모바일폰에서도 구동될 수 있다는 점이 인기를 끌며 하룻밤 사이에 2천 회 이상 다운로드되었습니다","description":"","date":"2024-05-27 14:30","slug":"2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight","content":"\n\n![Octopus v2](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_0.png)\n\n대규모 모델 구현의 과정에서, 엔드사이드 AI는 매우 중요한 방향입니다.\n\n최근 스탠퍼드 대학의 연구자들이 출시한 Octopus v2는 개발자 커뮤니티로부터 큰 관심을 받으며 인기를 끌고 있습니다. 모델의 다운로드 횟수가 하룻밤 사이에 2천 건을 넘었습니다.\n\n200억 개의 파라미터를 갖는 Octopus v2는 정확도와 대기 시간 측면에서 GPT-4를 능가하며, 콘텍스트 길이를 95% 줄였습니다. 또한, Octopus v2는 Llama7B + RAG 구성보다 36배 빠릅니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은 네티즌들이 한탄했습니다: 디바이스 측 인공지능 에이전트 시대가 도래했습니다!\n\n![image](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_1.png)\n\n- 논문: 옥토퍼스 v2: 슈퍼 에이전트용 기기 내 언어 모델\n- 논문 주소: https://arxiv.org/abs/2404.01744\n- 모델 홈페이지: https://huggingface.co/NexaAIDev/Octopus-v2\n\n모델 개요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n옥토퍼스-V2-2B는 안드로이드 API에 맞게 설계된 20 억 개의 매개변수를 가진 오픈 소스 언어 모델로, 안드로이드 기기에서 원활하게 실행되며 안드로이드 시스템 관리에서 여러 기기 및 다양한 응용 프로그램의 조작까지 확장하는 데 사용됩니다.\n\n![이미지](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_2.png)\n\n일반적으로 검색 증강 생성 (RAG) 방법은 잠재적인 기능 매개변수에 대한 상세한 설명을 필요로 하며 (때로는 수만 개의 입력 토큰이 필요할 수도 있음), 이에 기반하여 옥토퍼스-V2-2B는 훈련 및 추론 단계에서 고유한 기능 토큰 전략을 도입하여 GPT-4와 유사한 성능 수준을 달성할 뿐만 아니라 추론 속도를 크게 향상시켜 RAG 기반 방법을 능가합니다. 이로 인해 엣지 컴퓨팅 기기에 특히 유용합니다.\n\n![이미지](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문어-V2-2B는 다양한 복잡한 시나리오에서 개별, 중첩 및 병렬 함수 호출을 생성할 수 있습니다.\n\n데이터 세트\n\n훈련, 검증 및 테스트 단계에서 고품질 데이터 세트를 사용하고 특히 효율적인 훈련을 달성하기 위해, 연구팀은 데이터 세트를 세 가지 주요 단계로 생성했습니다:\n\n- 관련 쿼리 및 관련 함수 호출 매개변수 생성;\n- 적절한 기능 구성요소에서 관련이 없는 쿼리 생성;\n- Google Gemini을 통한 이진 검증 지원.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_4.png)\n\nResearch team은 이 모델을 훈련하기 위해 20가지의 안드로이드 API 설명을 작성했습니다. 다음은 예시 안드로이드 API 설명입니다:\n\n```js\ndef get_trending_news (category=None, region='US', language='en', max_results=5):\n    \"\"\"\n    카테고리, 지역 및 언어에 기반한 트렌드 뉴스 기사를 가져옵니다.\n    Parameters:\n    - category (str, optional): 필터링할 뉴스 카테고리입니다. 모든 카테고리에 대해 기본값으로 None을 사용합니다. 선택적으로 제공할 수 있습니다.\n    - region (str, optional): 지역별 뉴스를 위한 ISO 3166-1 알파-2 국가 코드입니다. 기본값으로 'US'를 사용합니다. 선택적으로 제공할 수 있습니다.\n    - language (str, optional): 기사 언어를 위한 ISO 639-1 언어 코드입니다. 기본값으로 'en'을 사용합니다. 선택적으로 제공할 수 있습니다.\n    - max_results (int, optional): 반환할 기사의 최대 수입니다. 기본값으로 5를 사용합니다. 선택적으로 제공할 수 있습니다.\n    Returns:\n    - list [str]: 각각 기사를 나타내는 문자열의 목록입니다. 각 문자열은 기사 제목과 URL을 포함합니다.\n    \"\"\"\n```\n\n모델 개발 및 훈련\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 연구는 프레임워크에서 Google Gemma-2B 모델을 사전 학습 모델로 사용하며 두 가지 다른 훈련 방법을 채택합니다: 전체 모델 훈련과 LoRA 모델 훈련.\n\n전체 모델 훈련에서는 AdamW 옵티마이저를 사용하며 학습률은 5e-5로 설정되고 웜업 단계 수는 10으로, 선형 학습률 스케줄러가 사용됩니다.\n\nLoRA 모델 훈련은 전체 모델 훈련과 동일한 옵티마이저와 학습률 구성을 사용하며 LoRA 랭크는 16으로 설정되며, LoRA는 다음 모듈에 적용됩니다: q_proj, k_proj, v_proj, o_proj, up_proj, down_proj. 그 중 LoRA 알파 매개변수는 32로 설정됩니다.\n\n두 훈련 방법 모두 에포크 수는 3으로 설정됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 코드를 사용하면 단일 GPU에서 Octopus-V2-2B 모델을 실행할 수 있습니다.\n\n```js\nfrom transformers import AutoTokenizer, GemmaForCausalLM\nimport torch\nimport time\n\ndef inference(input_text):\n    start_time = time.time()\n    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    input_length = input_ids[\"input_ids\"].shape[1]\n    outputs = model.generate(\n        input_ids=input_ids[\"input_ids\"],\n        max_length=1024,\n        do_sample=False\n    )\n    generated_sequence = outputs[:, input_length:].tolist()\n    res = tokenizer.decode(generated_sequence[0])\n    end_time = time.time()\n    return {\"output\": res, \"latency\": end_time - start_time}\n\nmodel_id = \"NexaAIDev/Octopus-v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = GemmaForCausalLM.from_pretrained(\n    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n\ninput_text = \"Take a selfie for me with front camera\"\nnexa_query = f\"아래는 사용자쿼리입니다. 올바른 함수를 호출하고 함수를 호출하는 매개변수를 생성하십시오.\\n\\n쿼리: {input_text}\\n\\n응답:\"\nstart_time = time.time()\nprint(\"넥사 모델 결과:\\n\", inference(nexa_query))\nprint(\"latency:\", time.time() - start_time, \"초\")\n```\n\n평가\n\n벤치마크 테스트에서 Octopus-V2-2B는 단일 A100 GPU에서 \"Llama7B + RAG 솔루션\"보다 36배 빠른 탁월한 추론 속도를 보여주었습니다. 게다가, Octopus-V2-2B는 클러스터화된 A100/H100 GPU에 의존하는 GPT-4-turbo보다 168% 빠릅니다. 이 효율적인 개선은 Octopus-V2-2B의 기능 토큰 디자인에 기인합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Octopus-V2-2B](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_5.png)\n\n옥토퍼스-V2-2B는 속도뿐만 아니라 정확도 면에서도 우수하며, \"라마7B + RAG 솔루션\"을 31% 초과하는 함수 호출 정확도로 능가합니다. 옥토퍼스-V2-2B는 GPT-4 및 RAG + GPT-3.5와 비교 가능한 함수 호출 정확도를 달성합니다.\n\n![Learn More](/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_6.png)\n\n관심 있는 독자들은 연구 내용에 대한 원본 논문을 읽어서 더 많이 알아볼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n참고: [https://mp.weixin.qq.com/s/qnFZOPLpdRxW42_cLUcImA](https://mp.weixin.qq.com/s/qnFZOPLpdRxW42_cLUcImA)\n\n기사가 마음에 드셨나요? 더 많은 학습을 원하신다면 제한 없이 읽을 수 있는 Medium 회원이 되어보세요. 이 링크를 통해 회원이 되면 추가 비용 없이 저를 지원해 주시게 됩니다. 미리 감사드리고 앞으로 또 만나요!\n","ogImage":{"url":"/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_0.png"},"coverImage":"/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_0.png","tag":["Tech"],"readingTime":6},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_0.png\" alt=\"Octopus v2\"\u003e\u003c/p\u003e\n\u003cp\u003e대규모 모델 구현의 과정에서, 엔드사이드 AI는 매우 중요한 방향입니다.\u003c/p\u003e\n\u003cp\u003e최근 스탠퍼드 대학의 연구자들이 출시한 Octopus v2는 개발자 커뮤니티로부터 큰 관심을 받으며 인기를 끌고 있습니다. 모델의 다운로드 횟수가 하룻밤 사이에 2천 건을 넘었습니다.\u003c/p\u003e\n\u003cp\u003e200억 개의 파라미터를 갖는 Octopus v2는 정확도와 대기 시간 측면에서 GPT-4를 능가하며, 콘텍스트 길이를 95% 줄였습니다. 또한, Octopus v2는 Llama7B + RAG 구성보다 36배 빠릅니다.\u003c/p\u003e\n\u003cp\u003e많은 네티즌들이 한탄했습니다: 디바이스 측 인공지능 에이전트 시대가 도래했습니다!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_1.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e논문: 옥토퍼스 v2: 슈퍼 에이전트용 기기 내 언어 모델\u003c/li\u003e\n\u003cli\u003e논문 주소: \u003ca href=\"https://arxiv.org/abs/2404.01744\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://arxiv.org/abs/2404.01744\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e모델 홈페이지: \u003ca href=\"https://huggingface.co/NexaAIDev/Octopus-v2\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://huggingface.co/NexaAIDev/Octopus-v2\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e모델 개요\u003c/p\u003e\n\u003cp\u003e옥토퍼스-V2-2B는 안드로이드 API에 맞게 설계된 20 억 개의 매개변수를 가진 오픈 소스 언어 모델로, 안드로이드 기기에서 원활하게 실행되며 안드로이드 시스템 관리에서 여러 기기 및 다양한 응용 프로그램의 조작까지 확장하는 데 사용됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e일반적으로 검색 증강 생성 (RAG) 방법은 잠재적인 기능 매개변수에 대한 상세한 설명을 필요로 하며 (때로는 수만 개의 입력 토큰이 필요할 수도 있음), 이에 기반하여 옥토퍼스-V2-2B는 훈련 및 추론 단계에서 고유한 기능 토큰 전략을 도입하여 GPT-4와 유사한 성능 수준을 달성할 뿐만 아니라 추론 속도를 크게 향상시켜 RAG 기반 방법을 능가합니다. 이로 인해 엣지 컴퓨팅 기기에 특히 유용합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_3.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e문어-V2-2B는 다양한 복잡한 시나리오에서 개별, 중첩 및 병렬 함수 호출을 생성할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e데이터 세트\u003c/p\u003e\n\u003cp\u003e훈련, 검증 및 테스트 단계에서 고품질 데이터 세트를 사용하고 특히 효율적인 훈련을 달성하기 위해, 연구팀은 데이터 세트를 세 가지 주요 단계로 생성했습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e관련 쿼리 및 관련 함수 호출 매개변수 생성;\u003c/li\u003e\n\u003cli\u003e적절한 기능 구성요소에서 관련이 없는 쿼리 생성;\u003c/li\u003e\n\u003cli\u003eGoogle Gemini을 통한 이진 검증 지원.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_4.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eResearch team은 이 모델을 훈련하기 위해 20가지의 안드로이드 API 설명을 작성했습니다. 다음은 예시 안드로이드 API 설명입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef get_trending_news (category=\u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, region=\u003cspan class=\"hljs-string\"\u003e'US'\u003c/span\u003e, language=\u003cspan class=\"hljs-string\"\u003e'en'\u003c/span\u003e, max_results=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    카테고리, 지역 및 언어에 기반한 트렌드 뉴스 기사를 가져옵니다.\n    Parameters:\n    - category (str, optional): 필터링할 뉴스 카테고리입니다. 모든 카테고리에 대해 기본값으로 None을 사용합니다. 선택적으로 제공할 수 있습니다.\n    - region (str, optional): 지역별 뉴스를 위한 ISO 3166-1 알파-2 국가 코드입니다. 기본값으로 'US'를 사용합니다. 선택적으로 제공할 수 있습니다.\n    - language (str, optional): 기사 언어를 위한 ISO 639-1 언어 코드입니다. 기본값으로 'en'을 사용합니다. 선택적으로 제공할 수 있습니다.\n    - max_results (int, optional): 반환할 기사의 최대 수입니다. 기본값으로 5를 사용합니다. 선택적으로 제공할 수 있습니다.\n    Returns:\n    - list [str]: 각각 기사를 나타내는 문자열의 목록입니다. 각 문자열은 기사 제목과 URL을 포함합니다.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e모델 개발 및 훈련\u003c/p\u003e\n\u003cp\u003e이 연구는 프레임워크에서 Google Gemma-2B 모델을 사전 학습 모델로 사용하며 두 가지 다른 훈련 방법을 채택합니다: 전체 모델 훈련과 LoRA 모델 훈련.\u003c/p\u003e\n\u003cp\u003e전체 모델 훈련에서는 AdamW 옵티마이저를 사용하며 학습률은 5e-5로 설정되고 웜업 단계 수는 10으로, 선형 학습률 스케줄러가 사용됩니다.\u003c/p\u003e\n\u003cp\u003eLoRA 모델 훈련은 전체 모델 훈련과 동일한 옵티마이저와 학습률 구성을 사용하며 LoRA 랭크는 16으로 설정되며, LoRA는 다음 모듈에 적용됩니다: q_proj, k_proj, v_proj, o_proj, up_proj, down_proj. 그 중 LoRA 알파 매개변수는 32로 설정됩니다.\u003c/p\u003e\n\u003cp\u003e두 훈련 방법 모두 에포크 수는 3으로 설정됩니다.\u003c/p\u003e\n\u003cp\u003e다음 코드를 사용하면 단일 GPU에서 Octopus-V2-2B 모델을 실행할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAutoTokenizer\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eGemmaForCausalLM\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e time\n\ndef \u003cspan class=\"hljs-title function_\"\u003einference\u003c/span\u003e(input_text):\n    start_time = time.\u003cspan class=\"hljs-title function_\"\u003etime\u003c/span\u003e()\n    input_ids = \u003cspan class=\"hljs-title function_\"\u003etokenizer\u003c/span\u003e(input_text, return_tensors=\u003cspan class=\"hljs-string\"\u003e\"pt\"\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(model.\u003cspan class=\"hljs-property\"\u003edevice\u003c/span\u003e)\n    input_length = input_ids[\u003cspan class=\"hljs-string\"\u003e\"input_ids\"\u003c/span\u003e].\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n    outputs = model.\u003cspan class=\"hljs-title function_\"\u003egenerate\u003c/span\u003e(\n        input_ids=input_ids[\u003cspan class=\"hljs-string\"\u003e\"input_ids\"\u003c/span\u003e],\n        max_length=\u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e,\n        do_sample=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e\n    )\n    generated_sequence = outputs[:, \u003cspan class=\"hljs-attr\"\u003einput_length\u003c/span\u003e:].\u003cspan class=\"hljs-title function_\"\u003etolist\u003c/span\u003e()\n    res = tokenizer.\u003cspan class=\"hljs-title function_\"\u003edecode\u003c/span\u003e(generated_sequence[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\n    end_time = time.\u003cspan class=\"hljs-title function_\"\u003etime\u003c/span\u003e()\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e {\u003cspan class=\"hljs-string\"\u003e\"output\"\u003c/span\u003e: res, \u003cspan class=\"hljs-string\"\u003e\"latency\"\u003c/span\u003e: end_time - start_time}\n\nmodel_id = \u003cspan class=\"hljs-string\"\u003e\"NexaAIDev/Octopus-v2\"\u003c/span\u003e\ntokenizer = \u003cspan class=\"hljs-title class_\"\u003eAutoTokenizer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(model_id)\nmodel = \u003cspan class=\"hljs-title class_\"\u003eGemmaForCausalLM\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(\n    model_id, torch_dtype=torch.\u003cspan class=\"hljs-property\"\u003ebfloat16\u003c/span\u003e, device_map=\u003cspan class=\"hljs-string\"\u003e\"auto\"\u003c/span\u003e\n)\n\ninput_text = \u003cspan class=\"hljs-string\"\u003e\"Take a selfie for me with front camera\"\u003c/span\u003e\nnexa_query = f\u003cspan class=\"hljs-string\"\u003e\"아래는 사용자쿼리입니다. 올바른 함수를 호출하고 함수를 호출하는 매개변수를 생성하십시오.\\n\\n쿼리: {input_text}\\n\\n응답:\"\u003c/span\u003e\nstart_time = time.\u003cspan class=\"hljs-title function_\"\u003etime\u003c/span\u003e()\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"넥사 모델 결과:\\n\"\u003c/span\u003e, \u003cspan class=\"hljs-title function_\"\u003einference\u003c/span\u003e(nexa_query))\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"latency:\"\u003c/span\u003e, time.\u003cspan class=\"hljs-title function_\"\u003etime\u003c/span\u003e() - start_time, \u003cspan class=\"hljs-string\"\u003e\"초\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e평가\u003c/p\u003e\n\u003cp\u003e벤치마크 테스트에서 Octopus-V2-2B는 단일 A100 GPU에서 \"Llama7B + RAG 솔루션\"보다 36배 빠른 탁월한 추론 속도를 보여주었습니다. 게다가, Octopus-V2-2B는 클러스터화된 A100/H100 GPU에 의존하는 GPT-4-turbo보다 168% 빠릅니다. 이 효율적인 개선은 Octopus-V2-2B의 기능 토큰 디자인에 기인합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_5.png\" alt=\"Octopus-V2-2B\"\u003e\u003c/p\u003e\n\u003cp\u003e옥토퍼스-V2-2B는 속도뿐만 아니라 정확도 면에서도 우수하며, \"라마7B + RAG 솔루션\"을 31% 초과하는 함수 호출 정확도로 능가합니다. 옥토퍼스-V2-2B는 GPT-4 및 RAG + GPT-3.5와 비교 가능한 함수 호출 정확도를 달성합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight_6.png\" alt=\"Learn More\"\u003e\u003c/p\u003e\n\u003cp\u003e관심 있는 독자들은 연구 내용에 대한 원본 논문을 읽어서 더 많이 알아볼 수 있습니다.\u003c/p\u003e\n\u003cp\u003e참고: \u003ca href=\"https://mp.weixin.qq.com/s/qnFZOPLpdRxW42_cLUcImA\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://mp.weixin.qq.com/s/qnFZOPLpdRxW42_cLUcImA\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e기사가 마음에 드셨나요? 더 많은 학습을 원하신다면 제한 없이 읽을 수 있는 Medium 회원이 되어보세요. 이 링크를 통해 회원이 되면 추가 비용 없이 저를 지원해 주시게 됩니다. 미리 감사드리고 앞으로 또 만나요!\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-27-BetterThanGPT-4theStanfordteamslargemodelthatcanberunonmobilephonesbecamepopularwithover2kdownloadsovernight"},"buildId":"FH3Qr-mLAesqA0X5IFRQr","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>