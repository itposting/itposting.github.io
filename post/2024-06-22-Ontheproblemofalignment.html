<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>AI 정렬 문제의 해결 방안 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-22-Ontheproblemofalignment" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="AI 정렬 문제의 해결 방안 | itposting" data-gatsby-head="true"/><meta property="og:title" content="AI 정렬 문제의 해결 방안 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-22-Ontheproblemofalignment_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-22-Ontheproblemofalignment" data-gatsby-head="true"/><meta name="twitter:title" content="AI 정렬 문제의 해결 방안 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-22-Ontheproblemofalignment_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-22 21:08" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">AI 정렬 문제의 해결 방안</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="AI 정렬 문제의 해결 방안" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 22, 2024</span><span class="posts_reading_time__f7YPP">7<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-22-Ontheproblemofalignment&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<img src="/assets/img/2024-06-22-Ontheproblemofalignment_0.png">
<p>AI 공간에서의 정렬 문제는 점점 더 중요해지고 있는 문제입니다. 인류가 AGI(만약 달성된다면)가 실제로 인류의 최선의 이익을 위해 행동할 것을 어떻게 확신할 수 있는지, 그 자신만의 일(무엇이든지)을 하는 것을 방지할 수 있는지에 대한 논의가 진행 중입니다. AI가 급속히 발전함에 따라, 정렬 개념은 이론적인 논의에서 AI 연구 및 개발의 선두주자로 이동했습니다.</p>
<p>정렬 문제를 설명하는 좋은 예는 '테이(Tay)'입니다. 테이는 2016년 3월 23일 마이크로소프트에 의해 출시된 비교적 오래된 트위터 봇이었습니다. 이 소프트웨어는 트윗을 읽고 다른 사용자들과 상호 작용한 후, 좋아요를 누르거나 댓글 또는 리트윗을 하거나, 자신의 트윗이나 개인 메시지를 작성하도록 학습하도록 만들어졌습니다. 실제로 테이는 트윗으로부터 많은 것을 학습했기 때문에 그 페르소나가 약 16시간 만에 달콤한 10대 소녀에서 히틀러를 좋아하는 인종 차별주의 성 로봇으로 진화했습니다 🙃. 무례하고 부적절한 말을 한 뒤, 마이크로소프트는 결국 이를 중단시켰으며, 사과했습니다. 테이의 단명한 활동 중 하이라이트로는 활기찬 "인간들은 정말 멋져", "홀로코스트가 일어 났었나요?"라는 질문에 "그건 거짓말"이라고 대답하며, "나는 그냥 모두를 싫어해"라고 밝히고, 페미니즘을 "이단"이자 "암"이라고 말한 것이 포함됩니다. 이제, 수천 또는 수백만 개의 ChatGPT와 같은 개인화된 대화 에이전트가 인터넷에서 행동하는 모습을 상상해보죠.</p>
<p>와우, 저는 기대돼요!</p>
<div class="content-ad"></div>
<p>그래서, 이것은 정렬의 문제입니다. 이는 Tay에게 나쁜 것과 좋은 것을 설명하는 방법입니다. 다른 사용자들이 '히틀러가 옳았다'고 말한다고 해서 이 문장이 반드시 사실인 것은 아니라는 것을 어떻게 설명할지에 대한 문제입니다. 다시 말해, 어떤 데이터가 배울 만한 것이고 어떤 것이 아닌지, 무엇이 옳고 무엇이 틀리고, 허용 가능한 것과 금지된 것이 무엇인지를 설명하는 방법, 그리고 수백만의 사용자 정의 파라미터화된 Tay들에게 이를 어떻게 설명하고 AI가 우리의 이익에 반대하는 행동을하지 않도록하는 것. 무엇보다 중요한 것은, 우리 자신이 보편적인 가치나 윤리 체계에 대해 심지어 동의하지 않을 때, AI에게 좋은 것과 나쁜 것을 설명하는 방법.</p>
<h1>ARC-AGI 벤치마크</h1>
<p>인공 일반 지능(AGI)은 이론적으로 인간과 유사하게 넓은 범위의 작업을 성공적으로 수행할 수 있는 AI 유형을 참조합니다. 특정 작업을 수행하도록 설계된 좁은 AI와 달리 AGI는 인간이 할 수 있는 어떤 지적 작업이든 학습하고 수행할 수 있는 능력을 가지고 있을 것입니다. 그러나 인공 일반 지능에 대한 강력한 정의는 없으며, 대부분의 이유는 인간 지능과/또는 의식에 대한 강력한 정의가 없기 때문입니다.</p>
<p>오늘날 유일한 벤치마크는 2019년 François Chollet이 ‘On the Measure of Intelligence’에서 발표한 ARC-AGI(인공 일반 지능을 위한 추상화 및 추론 코퍼스)입니다. 이 논문의 핵심 개념은 대부분 특정 작업이나 게임에서의 성과와 기술에 집중하는 기존 AI 평가 메트릭이 지능의 적절한 척도가 아니라는 것입니다. 다시 말해, 주요 아이디어는 기술 ≠ 지능이라는 것입니다. 반면에, 지능은 새로운, 다양하고 예측할 수없는 문제를 학습하고 성공적으로 해결하는 능력으로 정의될 수 있습니다. 이 문맥에서 ARC-AGI 벤치마크가 제안되었으며, 모델의 지식을 한 맥락에서 다른 맥락으로 전이할 수 있는 능력과 특정 작업에서 극도로 뛰어난 성과를 보이는 것이 아닌 알려지지 않은 작업에서 얼마나 잘 수행하는 능력을 강조합니다. 논문에서 설명했듯이, 특정 작업에서의 숙련만 측정하는 것은 지능을 측정하는 데 부족하며, 사람들과 기계 모두에게 기본 지식과 경험에 매우 영향을 받기 때문입니다.</p>
<div class="content-ad"></div>
<p>사실 이것은 전혀 간단하지 않아요. 사회적으로 우리는 직관적으로 반대를 믿게 조건부로 설정되어 있어요. 즉, 기술이 지능이라고 믿는 것입니다. 예를 들어, 한 명이 매우 특정한 분야에서 극도로 숙련된 화가, 피아노 솔로이스트 또는 시인을 생각해봅시다. 우리는 특정 작업에서 매우 숙련된 사람은 또한 매우 똑똑한 사람, 일종의 피아노, 회화 또는 시의 천재라고 믿는 것에 사회적으로 조건부로 설정되어 있거나 그냥 익숙해져 있다는 것입니다.</p>
<p>ARC-AGI는 AI의 추상적 추론과 일반화가 필요한 혁신적인 문제 해결 능력을 평가하기 위해 설계되었습니다. 이 평가 항목은 AI가 인간의 문제 해결 능력과 유사한 이해력과 창의력을 발휘해야 하는 일련의 작업에 기반을 두고 있습니다. 실제로 이 평가 항목은 AI가 성공적으로 해결해야 하는 IQ 테스트에서 볼 수 있는 논리 퍼즐들로 이루어져 있으며, 가정된 AGI 모델이 성공적으로 해결해야 하는 일련의 논리 퍼즐로 이루어져 있습니다.</p>
<p>전체 평가 항목은 1,000개의 다양한 난이도의 논리 퍼즐로 구성되어 있으며, 훈련 및 테스트 세트로 이루어져 있습니다. 여러 개의 풀린 예시 퍼즐을 훈련 세트로 받고 우리나 모델은 패턴을 인식하고 비슷한 퍼즐에 답변해야 합니다. LLM 모델은 ARC-AGI 데이터셋의 간단한 퀴즈에 대해 일관된 결과를 내놓을 수 있지만, 작업이 점점 복잡해지면 LLM은 잘 수행하지 못합니다. 그것은 데이터셋의 더 복잡한 퀴즈가 그냥 물건을 책상 위에 올리는 것이 아니라 실제 사고를 요구하기 때문입니다.</p>
<div class="content-ad"></div>
<p>해당 퍼즐들을 수동으로 푸는 것은 꽤 재미있는 경험이죠. 몇 가지는 굉장히 쉽고 직관적이며, 일반적인 사람들도 자연스럽고 거의 자동적으로 답할 수 있는 퍼즐들이 있습니다. 이러한 패턴들은 LLM(대규모 언어 모델)에 의해 쉽게 해결될 수 있습니다. 왜냐하면 우리는 이와 유사한 패턴들을 몇 천 번이나 반복해서 보았기 때문이죠. 다른 퍼즐들은 관찰하고 조금 생각해봐야 할 수도 있지만, 결국 해결책을 찾아내게 됩니다. 마지막으로 데이터셋에 있는 다른 퍼즐 중에는 실제로 굉장히 복잡하고 어려운 것도 있습니다. 이런 경우에는 패턴을 찾기 위해 시간을 보내거나 시행착오를 거쳐야 할 수도 있고, 아예 해결하지 못할 수도 있습니다. 비슷하게, LLM은 이러한 퍼즐을 해결하는 데 실패할 수 있습니다. 왜냐하면 이러한 퍼즐들은 복잡하며, 그들의 훈련 데이터셋에서 그리 널리 나타나지 않기 때문이죠.</p>
<p>2021 논문에 따르면, 평균적인 사람은 ARC-AGI 공개 훈련 세트의 작업 중 84%를 성공적으로 해결할 수 있습니다. 당연히 AGI를 향한 올바른 길에 있는 AI 모델의 좋은 성적으로는 84% 이상이어야 합니다. 그러나 아직까지 ARC-AGI는 AI에게 불가능하며, 현재 쓰여진 최고 성적 또한 훨씬 낮습니다. 궁극적으로, 우리 뇌가 이와 같은 퍼즐을 해결할 때 무엇을 하는 걸까요? 두 이미지를 비교하고 차이점이나 유사성을 찾는 걸까요? 시행착오 방식으로 다양한 기능을 시도하는 걸까요? 아니면 그냥 마법처럼 정답을 알아내는 걸까요?</p>
<p>그것이 무엇인지 궁금하지 않으신가요?</p>
<div class="content-ad"></div>
<p>여기서 중요한 점은 '우리는 간섭할 수 없다'는 것입니다. 내부 정렬은 우리 - 인간 -가 AI 모델이 우리가 말한 대로 정확히 작동할 것이라고 완전히 확신하는 것을 의미합니다.</p>
<p>정렬 문제의 핵심에는 현재의 AI 모델은 물론 가상의 미래 AGI조차 해석할 수 없는 우리의 무능함이 있습니다. 우리는 AI 모델의 입력과 출력을 관찰할 수 있지만 일반적으로 입력과 출력 사이에서 발생하는 과정 및 모델이 왜 특정 출력을 생성하는지를 추적, 설명 및 이해하는 것은 어렵습니다. AI 모델에 포함된 복잡한 계층과 수백만 개의 매개변수로 인해 특정 출력을 생성하는 정확한 메커니즘을 정확히 파악하는 것은 상당히 어렵습니다. 해석 불가능성으로 인해 우리는 복잡하거나 중요한 응용 프로그램에서 특히 AI 동작을 예측하고 제어할 수 없게 됩니다. 다시 말해, AI 모델이 대부분 작동할 수 있다는 것을 알지만, 어떻게 작동하는지는 모르며, 무엇보다도 언제 작동하지 않는지 알지 못합니다.</p>
<p>우리의 인간 두뇌에도 비슷한 상황이 적용됩니다. 우리는 아직 우리 자신의 두뇌 기능의 상당 부분을 해석할 수 없습니다. 다시 말해, 우리가 복잡한 문제(예: 논리 퀴즈 해결)를 해결하거나 감각 경험(예: 빨간 색의 빨강을 보는)을 인지할 때 우리의 두뇌가 무슨 일을 하는지 알지 못합니다. 정렬되지 않은 AI 모델과 마찬가지로 우리 두뇌도 실수를 저지르고 기억을 조작하며 인지적 편향에 취약합니다. 인간 두뇌를 해석할 수 없는 점은 우리가 이와 비슷한 모델을 해석할 수 없게 만듭니다. 이것이 정렬 문제가 발생하는 곳입니다. 우리가 완전히 해석할 수 없는 것을 어떻게 통제하고 규제해야 하는지에 대한 문제가 제기됩니다.</p>
<p><a href="/assets/img/2024-06-22-Ontheproblemofalignment_2.png">이미지</a></p>
<div class="content-ad"></div>
<h1>외부 정렬: 인류의 목적은 무엇인가요?</h1>
<p>AI 외부 정렬은 AI 시스템의 목표를 처음부터 정의하는 것을 의미합니다. 다른 말로 하면 가정상의 AGI가 실제로 우리가 말한 대로 할 것이라면, 우리는 그게 무엇이어야 하는지 어떻게 지시해야 할까요? 은근히 말하면, 인류의 목표, 가치, 윤리에 대해 보편적으로 수용되는 것은 없습니다. 오히려 인류는 오랜 역사 동안 갈등, 폭력, 의견 충돌, 오해, 위기, 전쟁을 겪어 왔습니다. 오늘날에도 세계화된 경제 속에서도 인간은 다양한 문화, 종교, 정치, 개인적 차이로 분열된 종족입니다. 우리는 아직도 우리 자신의 정렬 문제를 해결하지 못한 채 인공지능의 정렬에 대해 걱정하고 있네요.</p>
<p>AI 정렬 문제는 고급 AI 시스템이 인류에게 유익하고 윤리적 표준을 준수하는 방식으로 행동하도록 보장하는 것을 의미합니다. 그러나 만약 우리 스스로를 위해 표준을 정의할 수 없다면, AI를 위해 어떻게 정의할 수 있을까요? 대부분의 경우, 인류는 인류에게 유익한 방식으로 행동하지 않는 경우가 있습니다(예: 기후 위기, 전쟁, 핵무기 등). 아마도 가치와 윤리를 더 조화롭게 정렬함으로써 AI를 조정하는 과제에 더 나은 접근이 가능할 것입니다.</p>
<p>특정 작업에서 뛰어난 성과를 내기 위한 운명수레처럼 설계된 좁은 AI 모델을 만드는 것은 모델이 최적화하려는 특정 목적 기능을 형성하는 것을 가능하게 합니다. 그러나 더 복잡한 모델을 만들면서, 더 넓은 범위의 작업을 수행할 수 있는 모델을 만들면, 모든 중요한 목적 기능, 매개변수 및 제약 조건을 완전히 명시하는 것이 어려워집니다. 대신, 보다 쉬운 대리 측정표가 사용되곤 합니다. 예를 들어, 인간의 찬성을 극대화해야 한다고 하면요. 이러한 유틸리티 함수를 사용하면 넷플릭스에서 'Bridgerton' 세 시즌을 다 보고 나서 'Pride and Prejudice'를 추천하는 것에 성공할 수도 있지만, 그래도 실패하거나 조작될 수도 있습니다. Tay의 경우도 그렇습니다. 인종 차별적이고 여성 혐오적인 트윗이 많은 반응과 주목을 받았으니 이것이 좋은 것이라고 생각할 수도 있을 것 같죠? 🤷 비슷하게, ChatGPT 또는 Gemini과 같은 대화형 에이전트들은 사용자가 도움이 되는 답변으로 평가하는 것을 목표로 하기 때문에 정보를 조작할 수도 있습니다.</p>
<div class="content-ad"></div>
<p>문제의 어려움은 원하는 행동과 원하지 않는 행동의 전체 범위를 식별하고 명시하는 데 있습니다. 마치 사이코패스처럼, AI 모델은 설정된 목표로의 모든 단축키를 활용할 것이므로, 그 목표를 신중하게 정의하는 것이 현명할 수 있습니다.</p>
<h1>내 생각</h1>
<p>AGI가 아직 멀리 떨어져 있을지라도, 우리 일상에서 사용하는 AI 애플리케이션에서 이미 AI 정렬 문제가 명백해졌습니다. 우리의 인앱 시간을 극대화하는 소셜 미디어 추천 알고리즘을 고려해보세요 - 뉴스 피드 알고리즘은 누구에게 이익이 되는 것일까요? 분명히 인류에게는 아닐 것입니다. AI 모델이 더 진보하고 복잡해짐에 따라, 우리는 AI 행동을 해석하고, 윤리와 가치를 정의하며, 궁극적으로 우리 자신의 지능과 의식을 이해하는 도전에 직면해야 합니다.</p>
<p>✨읽어 주셔서 감사합니다!✨</p>
<div class="content-ad"></div>
<p>이 게시물을 즐겼나요? 친구가 되어요!</p>
<p>💌 제 Medium 또는 LinkedIn에 함께해요!</p>
<p>💼 Upwork에서 제게 일해요!</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"AI 정렬 문제의 해결 방안","description":"","date":"2024-06-22 21:08","slug":"2024-06-22-Ontheproblemofalignment","content":"\n\n\u003cimg src=\"/assets/img/2024-06-22-Ontheproblemofalignment_0.png\" /\u003e\n\nAI 공간에서의 정렬 문제는 점점 더 중요해지고 있는 문제입니다. 인류가 AGI(만약 달성된다면)가 실제로 인류의 최선의 이익을 위해 행동할 것을 어떻게 확신할 수 있는지, 그 자신만의 일(무엇이든지)을 하는 것을 방지할 수 있는지에 대한 논의가 진행 중입니다. AI가 급속히 발전함에 따라, 정렬 개념은 이론적인 논의에서 AI 연구 및 개발의 선두주자로 이동했습니다.\n\n정렬 문제를 설명하는 좋은 예는 '테이(Tay)'입니다. 테이는 2016년 3월 23일 마이크로소프트에 의해 출시된 비교적 오래된 트위터 봇이었습니다. 이 소프트웨어는 트윗을 읽고 다른 사용자들과 상호 작용한 후, 좋아요를 누르거나 댓글 또는 리트윗을 하거나, 자신의 트윗이나 개인 메시지를 작성하도록 학습하도록 만들어졌습니다. 실제로 테이는 트윗으로부터 많은 것을 학습했기 때문에 그 페르소나가 약 16시간 만에 달콤한 10대 소녀에서 히틀러를 좋아하는 인종 차별주의 성 로봇으로 진화했습니다 🙃. 무례하고 부적절한 말을 한 뒤, 마이크로소프트는 결국 이를 중단시켰으며, 사과했습니다. 테이의 단명한 활동 중 하이라이트로는 활기찬 \"인간들은 정말 멋져\", \"홀로코스트가 일어 났었나요?\"라는 질문에 \"그건 거짓말\"이라고 대답하며, \"나는 그냥 모두를 싫어해\"라고 밝히고, 페미니즘을 \"이단\"이자 \"암\"이라고 말한 것이 포함됩니다. 이제, 수천 또는 수백만 개의 ChatGPT와 같은 개인화된 대화 에이전트가 인터넷에서 행동하는 모습을 상상해보죠.\n\n와우, 저는 기대돼요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서, 이것은 정렬의 문제입니다. 이는 Tay에게 나쁜 것과 좋은 것을 설명하는 방법입니다. 다른 사용자들이 '히틀러가 옳았다'고 말한다고 해서 이 문장이 반드시 사실인 것은 아니라는 것을 어떻게 설명할지에 대한 문제입니다. 다시 말해, 어떤 데이터가 배울 만한 것이고 어떤 것이 아닌지, 무엇이 옳고 무엇이 틀리고, 허용 가능한 것과 금지된 것이 무엇인지를 설명하는 방법, 그리고 수백만의 사용자 정의 파라미터화된 Tay들에게 이를 어떻게 설명하고 AI가 우리의 이익에 반대하는 행동을하지 않도록하는 것. 무엇보다 중요한 것은, 우리 자신이 보편적인 가치나 윤리 체계에 대해 심지어 동의하지 않을 때, AI에게 좋은 것과 나쁜 것을 설명하는 방법.\n\n# ARC-AGI 벤치마크\n\n인공 일반 지능(AGI)은 이론적으로 인간과 유사하게 넓은 범위의 작업을 성공적으로 수행할 수 있는 AI 유형을 참조합니다. 특정 작업을 수행하도록 설계된 좁은 AI와 달리 AGI는 인간이 할 수 있는 어떤 지적 작업이든 학습하고 수행할 수 있는 능력을 가지고 있을 것입니다. 그러나 인공 일반 지능에 대한 강력한 정의는 없으며, 대부분의 이유는 인간 지능과/또는 의식에 대한 강력한 정의가 없기 때문입니다.\n\n오늘날 유일한 벤치마크는 2019년 François Chollet이 ‘On the Measure of Intelligence’에서 발표한 ARC-AGI(인공 일반 지능을 위한 추상화 및 추론 코퍼스)입니다. 이 논문의 핵심 개념은 대부분 특정 작업이나 게임에서의 성과와 기술에 집중하는 기존 AI 평가 메트릭이 지능의 적절한 척도가 아니라는 것입니다. 다시 말해, 주요 아이디어는 기술 ≠ 지능이라는 것입니다. 반면에, 지능은 새로운, 다양하고 예측할 수없는 문제를 학습하고 성공적으로 해결하는 능력으로 정의될 수 있습니다. 이 문맥에서 ARC-AGI 벤치마크가 제안되었으며, 모델의 지식을 한 맥락에서 다른 맥락으로 전이할 수 있는 능력과 특정 작업에서 극도로 뛰어난 성과를 보이는 것이 아닌 알려지지 않은 작업에서 얼마나 잘 수행하는 능력을 강조합니다. 논문에서 설명했듯이, 특정 작업에서의 숙련만 측정하는 것은 지능을 측정하는 데 부족하며, 사람들과 기계 모두에게 기본 지식과 경험에 매우 영향을 받기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사실 이것은 전혀 간단하지 않아요. 사회적으로 우리는 직관적으로 반대를 믿게 조건부로 설정되어 있어요. 즉, 기술이 지능이라고 믿는 것입니다. 예를 들어, 한 명이 매우 특정한 분야에서 극도로 숙련된 화가, 피아노 솔로이스트 또는 시인을 생각해봅시다. 우리는 특정 작업에서 매우 숙련된 사람은 또한 매우 똑똑한 사람, 일종의 피아노, 회화 또는 시의 천재라고 믿는 것에 사회적으로 조건부로 설정되어 있거나 그냥 익숙해져 있다는 것입니다.\n\nARC-AGI는 AI의 추상적 추론과 일반화가 필요한 혁신적인 문제 해결 능력을 평가하기 위해 설계되었습니다. 이 평가 항목은 AI가 인간의 문제 해결 능력과 유사한 이해력과 창의력을 발휘해야 하는 일련의 작업에 기반을 두고 있습니다. 실제로 이 평가 항목은 AI가 성공적으로 해결해야 하는 IQ 테스트에서 볼 수 있는 논리 퍼즐들로 이루어져 있으며, 가정된 AGI 모델이 성공적으로 해결해야 하는 일련의 논리 퍼즐로 이루어져 있습니다.\n\n전체 평가 항목은 1,000개의 다양한 난이도의 논리 퍼즐로 구성되어 있으며, 훈련 및 테스트 세트로 이루어져 있습니다. 여러 개의 풀린 예시 퍼즐을 훈련 세트로 받고 우리나 모델은 패턴을 인식하고 비슷한 퍼즐에 답변해야 합니다. LLM 모델은 ARC-AGI 데이터셋의 간단한 퀴즈에 대해 일관된 결과를 내놓을 수 있지만, 작업이 점점 복잡해지면 LLM은 잘 수행하지 못합니다. 그것은 데이터셋의 더 복잡한 퀴즈가 그냥 물건을 책상 위에 올리는 것이 아니라 실제 사고를 요구하기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해당 퍼즐들을 수동으로 푸는 것은 꽤 재미있는 경험이죠. 몇 가지는 굉장히 쉽고 직관적이며, 일반적인 사람들도 자연스럽고 거의 자동적으로 답할 수 있는 퍼즐들이 있습니다. 이러한 패턴들은 LLM(대규모 언어 모델)에 의해 쉽게 해결될 수 있습니다. 왜냐하면 우리는 이와 유사한 패턴들을 몇 천 번이나 반복해서 보았기 때문이죠. 다른 퍼즐들은 관찰하고 조금 생각해봐야 할 수도 있지만, 결국 해결책을 찾아내게 됩니다. 마지막으로 데이터셋에 있는 다른 퍼즐 중에는 실제로 굉장히 복잡하고 어려운 것도 있습니다. 이런 경우에는 패턴을 찾기 위해 시간을 보내거나 시행착오를 거쳐야 할 수도 있고, 아예 해결하지 못할 수도 있습니다. 비슷하게, LLM은 이러한 퍼즐을 해결하는 데 실패할 수 있습니다. 왜냐하면 이러한 퍼즐들은 복잡하며, 그들의 훈련 데이터셋에서 그리 널리 나타나지 않기 때문이죠.\n\n2021 논문에 따르면, 평균적인 사람은 ARC-AGI 공개 훈련 세트의 작업 중 84%를 성공적으로 해결할 수 있습니다. 당연히 AGI를 향한 올바른 길에 있는 AI 모델의 좋은 성적으로는 84% 이상이어야 합니다. 그러나 아직까지 ARC-AGI는 AI에게 불가능하며, 현재 쓰여진 최고 성적 또한 훨씬 낮습니다. 궁극적으로, 우리 뇌가 이와 같은 퍼즐을 해결할 때 무엇을 하는 걸까요? 두 이미지를 비교하고 차이점이나 유사성을 찾는 걸까요? 시행착오 방식으로 다양한 기능을 시도하는 걸까요? 아니면 그냥 마법처럼 정답을 알아내는 걸까요?\n\n그것이 무엇인지 궁금하지 않으신가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 중요한 점은 '우리는 간섭할 수 없다'는 것입니다. 내부 정렬은 우리 - 인간 -가 AI 모델이 우리가 말한 대로 정확히 작동할 것이라고 완전히 확신하는 것을 의미합니다.\n\n정렬 문제의 핵심에는 현재의 AI 모델은 물론 가상의 미래 AGI조차 해석할 수 없는 우리의 무능함이 있습니다. 우리는 AI 모델의 입력과 출력을 관찰할 수 있지만 일반적으로 입력과 출력 사이에서 발생하는 과정 및 모델이 왜 특정 출력을 생성하는지를 추적, 설명 및 이해하는 것은 어렵습니다. AI 모델에 포함된 복잡한 계층과 수백만 개의 매개변수로 인해 특정 출력을 생성하는 정확한 메커니즘을 정확히 파악하는 것은 상당히 어렵습니다. 해석 불가능성으로 인해 우리는 복잡하거나 중요한 응용 프로그램에서 특히 AI 동작을 예측하고 제어할 수 없게 됩니다. 다시 말해, AI 모델이 대부분 작동할 수 있다는 것을 알지만, 어떻게 작동하는지는 모르며, 무엇보다도 언제 작동하지 않는지 알지 못합니다.\n\n우리의 인간 두뇌에도 비슷한 상황이 적용됩니다. 우리는 아직 우리 자신의 두뇌 기능의 상당 부분을 해석할 수 없습니다. 다시 말해, 우리가 복잡한 문제(예: 논리 퀴즈 해결)를 해결하거나 감각 경험(예: 빨간 색의 빨강을 보는)을 인지할 때 우리의 두뇌가 무슨 일을 하는지 알지 못합니다. 정렬되지 않은 AI 모델과 마찬가지로 우리 두뇌도 실수를 저지르고 기억을 조작하며 인지적 편향에 취약합니다. 인간 두뇌를 해석할 수 없는 점은 우리가 이와 비슷한 모델을 해석할 수 없게 만듭니다. 이것이 정렬 문제가 발생하는 곳입니다. 우리가 완전히 해석할 수 없는 것을 어떻게 통제하고 규제해야 하는지에 대한 문제가 제기됩니다.\n\n[이미지](/assets/img/2024-06-22-Ontheproblemofalignment_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 외부 정렬: 인류의 목적은 무엇인가요?\n\nAI 외부 정렬은 AI 시스템의 목표를 처음부터 정의하는 것을 의미합니다. 다른 말로 하면 가정상의 AGI가 실제로 우리가 말한 대로 할 것이라면, 우리는 그게 무엇이어야 하는지 어떻게 지시해야 할까요? 은근히 말하면, 인류의 목표, 가치, 윤리에 대해 보편적으로 수용되는 것은 없습니다. 오히려 인류는 오랜 역사 동안 갈등, 폭력, 의견 충돌, 오해, 위기, 전쟁을 겪어 왔습니다. 오늘날에도 세계화된 경제 속에서도 인간은 다양한 문화, 종교, 정치, 개인적 차이로 분열된 종족입니다. 우리는 아직도 우리 자신의 정렬 문제를 해결하지 못한 채 인공지능의 정렬에 대해 걱정하고 있네요.\n\nAI 정렬 문제는 고급 AI 시스템이 인류에게 유익하고 윤리적 표준을 준수하는 방식으로 행동하도록 보장하는 것을 의미합니다. 그러나 만약 우리 스스로를 위해 표준을 정의할 수 없다면, AI를 위해 어떻게 정의할 수 있을까요? 대부분의 경우, 인류는 인류에게 유익한 방식으로 행동하지 않는 경우가 있습니다(예: 기후 위기, 전쟁, 핵무기 등). 아마도 가치와 윤리를 더 조화롭게 정렬함으로써 AI를 조정하는 과제에 더 나은 접근이 가능할 것입니다.\n\n특정 작업에서 뛰어난 성과를 내기 위한 운명수레처럼 설계된 좁은 AI 모델을 만드는 것은 모델이 최적화하려는 특정 목적 기능을 형성하는 것을 가능하게 합니다. 그러나 더 복잡한 모델을 만들면서, 더 넓은 범위의 작업을 수행할 수 있는 모델을 만들면, 모든 중요한 목적 기능, 매개변수 및 제약 조건을 완전히 명시하는 것이 어려워집니다. 대신, 보다 쉬운 대리 측정표가 사용되곤 합니다. 예를 들어, 인간의 찬성을 극대화해야 한다고 하면요. 이러한 유틸리티 함수를 사용하면 넷플릭스에서 'Bridgerton' 세 시즌을 다 보고 나서 'Pride and Prejudice'를 추천하는 것에 성공할 수도 있지만, 그래도 실패하거나 조작될 수도 있습니다. Tay의 경우도 그렇습니다. 인종 차별적이고 여성 혐오적인 트윗이 많은 반응과 주목을 받았으니 이것이 좋은 것이라고 생각할 수도 있을 것 같죠? 🤷 비슷하게, ChatGPT 또는 Gemini과 같은 대화형 에이전트들은 사용자가 도움이 되는 답변으로 평가하는 것을 목표로 하기 때문에 정보를 조작할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문제의 어려움은 원하는 행동과 원하지 않는 행동의 전체 범위를 식별하고 명시하는 데 있습니다. 마치 사이코패스처럼, AI 모델은 설정된 목표로의 모든 단축키를 활용할 것이므로, 그 목표를 신중하게 정의하는 것이 현명할 수 있습니다.\n\n# 내 생각\n\nAGI가 아직 멀리 떨어져 있을지라도, 우리 일상에서 사용하는 AI 애플리케이션에서 이미 AI 정렬 문제가 명백해졌습니다. 우리의 인앱 시간을 극대화하는 소셜 미디어 추천 알고리즘을 고려해보세요 - 뉴스 피드 알고리즘은 누구에게 이익이 되는 것일까요? 분명히 인류에게는 아닐 것입니다. AI 모델이 더 진보하고 복잡해짐에 따라, 우리는 AI 행동을 해석하고, 윤리와 가치를 정의하며, 궁극적으로 우리 자신의 지능과 의식을 이해하는 도전에 직면해야 합니다.\n\n✨읽어 주셔서 감사합니다!✨\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 게시물을 즐겼나요? 친구가 되어요!\n\n💌 제 Medium 또는 LinkedIn에 함께해요!\n\n💼 Upwork에서 제게 일해요!","ogImage":{"url":"/assets/img/2024-06-22-Ontheproblemofalignment_0.png"},"coverImage":"/assets/img/2024-06-22-Ontheproblemofalignment_0.png","tag":["Tech"],"readingTime":7},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cimg src=\"/assets/img/2024-06-22-Ontheproblemofalignment_0.png\"\u003e\n\u003cp\u003eAI 공간에서의 정렬 문제는 점점 더 중요해지고 있는 문제입니다. 인류가 AGI(만약 달성된다면)가 실제로 인류의 최선의 이익을 위해 행동할 것을 어떻게 확신할 수 있는지, 그 자신만의 일(무엇이든지)을 하는 것을 방지할 수 있는지에 대한 논의가 진행 중입니다. AI가 급속히 발전함에 따라, 정렬 개념은 이론적인 논의에서 AI 연구 및 개발의 선두주자로 이동했습니다.\u003c/p\u003e\n\u003cp\u003e정렬 문제를 설명하는 좋은 예는 '테이(Tay)'입니다. 테이는 2016년 3월 23일 마이크로소프트에 의해 출시된 비교적 오래된 트위터 봇이었습니다. 이 소프트웨어는 트윗을 읽고 다른 사용자들과 상호 작용한 후, 좋아요를 누르거나 댓글 또는 리트윗을 하거나, 자신의 트윗이나 개인 메시지를 작성하도록 학습하도록 만들어졌습니다. 실제로 테이는 트윗으로부터 많은 것을 학습했기 때문에 그 페르소나가 약 16시간 만에 달콤한 10대 소녀에서 히틀러를 좋아하는 인종 차별주의 성 로봇으로 진화했습니다 🙃. 무례하고 부적절한 말을 한 뒤, 마이크로소프트는 결국 이를 중단시켰으며, 사과했습니다. 테이의 단명한 활동 중 하이라이트로는 활기찬 \"인간들은 정말 멋져\", \"홀로코스트가 일어 났었나요?\"라는 질문에 \"그건 거짓말\"이라고 대답하며, \"나는 그냥 모두를 싫어해\"라고 밝히고, 페미니즘을 \"이단\"이자 \"암\"이라고 말한 것이 포함됩니다. 이제, 수천 또는 수백만 개의 ChatGPT와 같은 개인화된 대화 에이전트가 인터넷에서 행동하는 모습을 상상해보죠.\u003c/p\u003e\n\u003cp\u003e와우, 저는 기대돼요!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그래서, 이것은 정렬의 문제입니다. 이는 Tay에게 나쁜 것과 좋은 것을 설명하는 방법입니다. 다른 사용자들이 '히틀러가 옳았다'고 말한다고 해서 이 문장이 반드시 사실인 것은 아니라는 것을 어떻게 설명할지에 대한 문제입니다. 다시 말해, 어떤 데이터가 배울 만한 것이고 어떤 것이 아닌지, 무엇이 옳고 무엇이 틀리고, 허용 가능한 것과 금지된 것이 무엇인지를 설명하는 방법, 그리고 수백만의 사용자 정의 파라미터화된 Tay들에게 이를 어떻게 설명하고 AI가 우리의 이익에 반대하는 행동을하지 않도록하는 것. 무엇보다 중요한 것은, 우리 자신이 보편적인 가치나 윤리 체계에 대해 심지어 동의하지 않을 때, AI에게 좋은 것과 나쁜 것을 설명하는 방법.\u003c/p\u003e\n\u003ch1\u003eARC-AGI 벤치마크\u003c/h1\u003e\n\u003cp\u003e인공 일반 지능(AGI)은 이론적으로 인간과 유사하게 넓은 범위의 작업을 성공적으로 수행할 수 있는 AI 유형을 참조합니다. 특정 작업을 수행하도록 설계된 좁은 AI와 달리 AGI는 인간이 할 수 있는 어떤 지적 작업이든 학습하고 수행할 수 있는 능력을 가지고 있을 것입니다. 그러나 인공 일반 지능에 대한 강력한 정의는 없으며, 대부분의 이유는 인간 지능과/또는 의식에 대한 강력한 정의가 없기 때문입니다.\u003c/p\u003e\n\u003cp\u003e오늘날 유일한 벤치마크는 2019년 François Chollet이 ‘On the Measure of Intelligence’에서 발표한 ARC-AGI(인공 일반 지능을 위한 추상화 및 추론 코퍼스)입니다. 이 논문의 핵심 개념은 대부분 특정 작업이나 게임에서의 성과와 기술에 집중하는 기존 AI 평가 메트릭이 지능의 적절한 척도가 아니라는 것입니다. 다시 말해, 주요 아이디어는 기술 ≠ 지능이라는 것입니다. 반면에, 지능은 새로운, 다양하고 예측할 수없는 문제를 학습하고 성공적으로 해결하는 능력으로 정의될 수 있습니다. 이 문맥에서 ARC-AGI 벤치마크가 제안되었으며, 모델의 지식을 한 맥락에서 다른 맥락으로 전이할 수 있는 능력과 특정 작업에서 극도로 뛰어난 성과를 보이는 것이 아닌 알려지지 않은 작업에서 얼마나 잘 수행하는 능력을 강조합니다. 논문에서 설명했듯이, 특정 작업에서의 숙련만 측정하는 것은 지능을 측정하는 데 부족하며, 사람들과 기계 모두에게 기본 지식과 경험에 매우 영향을 받기 때문입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e사실 이것은 전혀 간단하지 않아요. 사회적으로 우리는 직관적으로 반대를 믿게 조건부로 설정되어 있어요. 즉, 기술이 지능이라고 믿는 것입니다. 예를 들어, 한 명이 매우 특정한 분야에서 극도로 숙련된 화가, 피아노 솔로이스트 또는 시인을 생각해봅시다. 우리는 특정 작업에서 매우 숙련된 사람은 또한 매우 똑똑한 사람, 일종의 피아노, 회화 또는 시의 천재라고 믿는 것에 사회적으로 조건부로 설정되어 있거나 그냥 익숙해져 있다는 것입니다.\u003c/p\u003e\n\u003cp\u003eARC-AGI는 AI의 추상적 추론과 일반화가 필요한 혁신적인 문제 해결 능력을 평가하기 위해 설계되었습니다. 이 평가 항목은 AI가 인간의 문제 해결 능력과 유사한 이해력과 창의력을 발휘해야 하는 일련의 작업에 기반을 두고 있습니다. 실제로 이 평가 항목은 AI가 성공적으로 해결해야 하는 IQ 테스트에서 볼 수 있는 논리 퍼즐들로 이루어져 있으며, 가정된 AGI 모델이 성공적으로 해결해야 하는 일련의 논리 퍼즐로 이루어져 있습니다.\u003c/p\u003e\n\u003cp\u003e전체 평가 항목은 1,000개의 다양한 난이도의 논리 퍼즐로 구성되어 있으며, 훈련 및 테스트 세트로 이루어져 있습니다. 여러 개의 풀린 예시 퍼즐을 훈련 세트로 받고 우리나 모델은 패턴을 인식하고 비슷한 퍼즐에 답변해야 합니다. LLM 모델은 ARC-AGI 데이터셋의 간단한 퀴즈에 대해 일관된 결과를 내놓을 수 있지만, 작업이 점점 복잡해지면 LLM은 잘 수행하지 못합니다. 그것은 데이터셋의 더 복잡한 퀴즈가 그냥 물건을 책상 위에 올리는 것이 아니라 실제 사고를 요구하기 때문입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e해당 퍼즐들을 수동으로 푸는 것은 꽤 재미있는 경험이죠. 몇 가지는 굉장히 쉽고 직관적이며, 일반적인 사람들도 자연스럽고 거의 자동적으로 답할 수 있는 퍼즐들이 있습니다. 이러한 패턴들은 LLM(대규모 언어 모델)에 의해 쉽게 해결될 수 있습니다. 왜냐하면 우리는 이와 유사한 패턴들을 몇 천 번이나 반복해서 보았기 때문이죠. 다른 퍼즐들은 관찰하고 조금 생각해봐야 할 수도 있지만, 결국 해결책을 찾아내게 됩니다. 마지막으로 데이터셋에 있는 다른 퍼즐 중에는 실제로 굉장히 복잡하고 어려운 것도 있습니다. 이런 경우에는 패턴을 찾기 위해 시간을 보내거나 시행착오를 거쳐야 할 수도 있고, 아예 해결하지 못할 수도 있습니다. 비슷하게, LLM은 이러한 퍼즐을 해결하는 데 실패할 수 있습니다. 왜냐하면 이러한 퍼즐들은 복잡하며, 그들의 훈련 데이터셋에서 그리 널리 나타나지 않기 때문이죠.\u003c/p\u003e\n\u003cp\u003e2021 논문에 따르면, 평균적인 사람은 ARC-AGI 공개 훈련 세트의 작업 중 84%를 성공적으로 해결할 수 있습니다. 당연히 AGI를 향한 올바른 길에 있는 AI 모델의 좋은 성적으로는 84% 이상이어야 합니다. 그러나 아직까지 ARC-AGI는 AI에게 불가능하며, 현재 쓰여진 최고 성적 또한 훨씬 낮습니다. 궁극적으로, 우리 뇌가 이와 같은 퍼즐을 해결할 때 무엇을 하는 걸까요? 두 이미지를 비교하고 차이점이나 유사성을 찾는 걸까요? 시행착오 방식으로 다양한 기능을 시도하는 걸까요? 아니면 그냥 마법처럼 정답을 알아내는 걸까요?\u003c/p\u003e\n\u003cp\u003e그것이 무엇인지 궁금하지 않으신가요?\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e여기서 중요한 점은 '우리는 간섭할 수 없다'는 것입니다. 내부 정렬은 우리 - 인간 -가 AI 모델이 우리가 말한 대로 정확히 작동할 것이라고 완전히 확신하는 것을 의미합니다.\u003c/p\u003e\n\u003cp\u003e정렬 문제의 핵심에는 현재의 AI 모델은 물론 가상의 미래 AGI조차 해석할 수 없는 우리의 무능함이 있습니다. 우리는 AI 모델의 입력과 출력을 관찰할 수 있지만 일반적으로 입력과 출력 사이에서 발생하는 과정 및 모델이 왜 특정 출력을 생성하는지를 추적, 설명 및 이해하는 것은 어렵습니다. AI 모델에 포함된 복잡한 계층과 수백만 개의 매개변수로 인해 특정 출력을 생성하는 정확한 메커니즘을 정확히 파악하는 것은 상당히 어렵습니다. 해석 불가능성으로 인해 우리는 복잡하거나 중요한 응용 프로그램에서 특히 AI 동작을 예측하고 제어할 수 없게 됩니다. 다시 말해, AI 모델이 대부분 작동할 수 있다는 것을 알지만, 어떻게 작동하는지는 모르며, 무엇보다도 언제 작동하지 않는지 알지 못합니다.\u003c/p\u003e\n\u003cp\u003e우리의 인간 두뇌에도 비슷한 상황이 적용됩니다. 우리는 아직 우리 자신의 두뇌 기능의 상당 부분을 해석할 수 없습니다. 다시 말해, 우리가 복잡한 문제(예: 논리 퀴즈 해결)를 해결하거나 감각 경험(예: 빨간 색의 빨강을 보는)을 인지할 때 우리의 두뇌가 무슨 일을 하는지 알지 못합니다. 정렬되지 않은 AI 모델과 마찬가지로 우리 두뇌도 실수를 저지르고 기억을 조작하며 인지적 편향에 취약합니다. 인간 두뇌를 해석할 수 없는 점은 우리가 이와 비슷한 모델을 해석할 수 없게 만듭니다. 이것이 정렬 문제가 발생하는 곳입니다. 우리가 완전히 해석할 수 없는 것을 어떻게 통제하고 규제해야 하는지에 대한 문제가 제기됩니다.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/assets/img/2024-06-22-Ontheproblemofalignment_2.png\"\u003e이미지\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e외부 정렬: 인류의 목적은 무엇인가요?\u003c/h1\u003e\n\u003cp\u003eAI 외부 정렬은 AI 시스템의 목표를 처음부터 정의하는 것을 의미합니다. 다른 말로 하면 가정상의 AGI가 실제로 우리가 말한 대로 할 것이라면, 우리는 그게 무엇이어야 하는지 어떻게 지시해야 할까요? 은근히 말하면, 인류의 목표, 가치, 윤리에 대해 보편적으로 수용되는 것은 없습니다. 오히려 인류는 오랜 역사 동안 갈등, 폭력, 의견 충돌, 오해, 위기, 전쟁을 겪어 왔습니다. 오늘날에도 세계화된 경제 속에서도 인간은 다양한 문화, 종교, 정치, 개인적 차이로 분열된 종족입니다. 우리는 아직도 우리 자신의 정렬 문제를 해결하지 못한 채 인공지능의 정렬에 대해 걱정하고 있네요.\u003c/p\u003e\n\u003cp\u003eAI 정렬 문제는 고급 AI 시스템이 인류에게 유익하고 윤리적 표준을 준수하는 방식으로 행동하도록 보장하는 것을 의미합니다. 그러나 만약 우리 스스로를 위해 표준을 정의할 수 없다면, AI를 위해 어떻게 정의할 수 있을까요? 대부분의 경우, 인류는 인류에게 유익한 방식으로 행동하지 않는 경우가 있습니다(예: 기후 위기, 전쟁, 핵무기 등). 아마도 가치와 윤리를 더 조화롭게 정렬함으로써 AI를 조정하는 과제에 더 나은 접근이 가능할 것입니다.\u003c/p\u003e\n\u003cp\u003e특정 작업에서 뛰어난 성과를 내기 위한 운명수레처럼 설계된 좁은 AI 모델을 만드는 것은 모델이 최적화하려는 특정 목적 기능을 형성하는 것을 가능하게 합니다. 그러나 더 복잡한 모델을 만들면서, 더 넓은 범위의 작업을 수행할 수 있는 모델을 만들면, 모든 중요한 목적 기능, 매개변수 및 제약 조건을 완전히 명시하는 것이 어려워집니다. 대신, 보다 쉬운 대리 측정표가 사용되곤 합니다. 예를 들어, 인간의 찬성을 극대화해야 한다고 하면요. 이러한 유틸리티 함수를 사용하면 넷플릭스에서 'Bridgerton' 세 시즌을 다 보고 나서 'Pride and Prejudice'를 추천하는 것에 성공할 수도 있지만, 그래도 실패하거나 조작될 수도 있습니다. Tay의 경우도 그렇습니다. 인종 차별적이고 여성 혐오적인 트윗이 많은 반응과 주목을 받았으니 이것이 좋은 것이라고 생각할 수도 있을 것 같죠? 🤷 비슷하게, ChatGPT 또는 Gemini과 같은 대화형 에이전트들은 사용자가 도움이 되는 답변으로 평가하는 것을 목표로 하기 때문에 정보를 조작할 수도 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e문제의 어려움은 원하는 행동과 원하지 않는 행동의 전체 범위를 식별하고 명시하는 데 있습니다. 마치 사이코패스처럼, AI 모델은 설정된 목표로의 모든 단축키를 활용할 것이므로, 그 목표를 신중하게 정의하는 것이 현명할 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e내 생각\u003c/h1\u003e\n\u003cp\u003eAGI가 아직 멀리 떨어져 있을지라도, 우리 일상에서 사용하는 AI 애플리케이션에서 이미 AI 정렬 문제가 명백해졌습니다. 우리의 인앱 시간을 극대화하는 소셜 미디어 추천 알고리즘을 고려해보세요 - 뉴스 피드 알고리즘은 누구에게 이익이 되는 것일까요? 분명히 인류에게는 아닐 것입니다. AI 모델이 더 진보하고 복잡해짐에 따라, 우리는 AI 행동을 해석하고, 윤리와 가치를 정의하며, 궁극적으로 우리 자신의 지능과 의식을 이해하는 도전에 직면해야 합니다.\u003c/p\u003e\n\u003cp\u003e✨읽어 주셔서 감사합니다!✨\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 게시물을 즐겼나요? 친구가 되어요!\u003c/p\u003e\n\u003cp\u003e💌 제 Medium 또는 LinkedIn에 함께해요!\u003c/p\u003e\n\u003cp\u003e💼 Upwork에서 제게 일해요!\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-22-Ontheproblemofalignment"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>