<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>Raspberry Pi 5에서 Ollama를 사용하지 않고 Llama를 실행하기 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="Raspberry Pi 5에서 Ollama를 사용하지 않고 Llama를 실행하기 | itposting" data-gatsby-head="true"/><meta property="og:title" content="Raspberry Pi 5에서 Ollama를 사용하지 않고 Llama를 실행하기 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama" data-gatsby-head="true"/><meta name="twitter:title" content="Raspberry Pi 5에서 Ollama를 사용하지 않고 Llama를 실행하기 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 18:11" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">Raspberry Pi 5에서 Ollama를 사용하지 않고 Llama를 실행하기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="Raspberry Pi 5에서 Ollama를 사용하지 않고 Llama를 실행하기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">8<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>그래서 저는 작년 12월에 Raspberry Pi 5 8GB를 구입한 이후로 계속해서 시도해보고 있어요. 그전에 LLM을 설치하는 많은 안내서를 찾았지만 문제에 부딪히며 쉽게 해결할 수 없었어요. 이는 LLM을 회수/구축/양자화해야 하는 원본 컴퓨터와 제 RPi5에 필요한 모든 것을 설치하지 못했기 때문에 발생한 문제의 일부입니다.</p>
<p>그래서 저는 여기에 내가 막혔던 부분을 정확히 지적하고 이를 어떻게 해결했는지를 써 놓은 가이드를 작성하게 되었어요.</p>
<p>그러므로 이것은 전적으로 제가 혼자서 완전히 해결한 가이드는 아니에요 (저는 이런 주제에 대해 전문가가 아닌 걸로 아요), 다만 누군가가 곤란해할 때 도움이 될 수 있는 가이드에요.</p>
<p>저는 LinkedIn에서 발견한 Marek Żelichowski의 이 가이드를 주로 사용했고, 제 기기에서 작동하는 데 필요한 단계를 추가했어요. 그는 자신의 블로그 맨 아래에 여러 사람/소스의 입력을 토대로 한 결과라고 명시하긴 했지만, 공정한 평가를 해야하며, Ollama를 요구하지 않는 저에게는 제게 도움이 된 소수의 가이드 중 하나였어요.</p>
<div class="content-ad"></div>
<p>인용 위치(외부 게시물 및 사이트 링크)를 제공하려고 노력했지만, 빠진 인용이 있으면 알려주시면 기쁘게 추가하겠습니다. 그럼 시작해볼까요!</p>
<h1>필요한 것</h1>
<ul>
<li>LLM('들')을 검색하고 양자화하기 위한 Windows 또는 Linux 배포로 충분한 소스 PC</li>
<li>LLM을 실행할 8GB 라즈베리 파이 5</li>
<li>Raspbian과 같은 미리 설치된 OS가 포함된 적어도 32GB의 메모리 카드 (제가 개인적으로 RPi에는 Ubuntu 23.04를 사용합니다)</li>
<li>소스 PC에서 RPi로 LLM을 전송하기 위해 적어도 22GB의 공간이 있는 USB 스틱</li>
</ul>
<h1>소스 PC 준비하기</h1>
<div class="content-ad"></div>
<p>우리는 (Linux 기반) 소스 PC에서 시작합니다. 저는 Linux 디스트로로 PC를 재설치하거나 듀얼 부팅하는 일을 하기 귀찮아해 소유하지 않기 때문에 "WSL"을 사용하기로 결정했습니다. 이는 Windows 환경에서 Linux 디스트로를 직접 실행할 수 있는 Microsoft의 내장 기능입니다. 이미 Linux PC에서 작업하고 있는 분들은 아래 단계가 필요하지 않을 수 있습니다.</p>
<h2>WSL 설치 방법:</h2>
<p>Windows PC에서 WSL을 설정하는 데 이 안내서를 사용했다는 점을 유의하세요.</p>
<p>먼저 PowerShell을 관리자 권한으로 실행합니다. 이를 위해 PowerShell에 마우스 오른쪽 버튼을 클릭하고 "관리자 권한으로 실행" 옵션을 선택합니다. 여러분의 기기의 보안 구성 방식에 따라 실행에 관리자 암호를 입력해야 할 수도 있습니다.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_0.png">
<p>열리는 PowerShell 창에서 다음 명령을 실행하세요:</p>
<p>이 명령을 실행하면 작업에 사용할 OS로 Ubuntu가 설치됩니다. 또한 다음 명령을 사용하여 wsl을 통해 직접 설치할 수 있는 특정 Linux 배포판을 선택할 수도 있습니다.</p>
<img src="/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_1.png">
<div class="content-ad"></div>
<p>다음 설치 명령에 설치하고 싶은 배포판을 선택하세요. 기본값을 사용했지만 Ubuntu 22.04를 실행하려면 다음 명령을 사용하면 됩니다.</p>
<p>설치가 완료되면 다음 명령을 사용하여 설치되었는지 확인할 수 있습니다.</p>
<p>After the installation is done, you can verify that it is installed with this command</p>
<p>WSL 설치 후 컴퓨터를 다시 시작하는 것이 강력히 권장됩니다. 재부팅 후 시작 메뉴에서 Linux 배포판을 찾을 수 있습니다.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_3.png">
<p>이 프로그램 중 하나를 열면 Ubuntu 설치에 대한 명령 프롬프트 (CMD)가 열립니다. 이 명령을 사용하여 Llama 프로젝트를 다운로드하고 빌드하고, Raspberry Pi에서 실행할 수 있는 모델을 다운로드하고 양자화할 것입니다.</p>
<h2>Linux 소스 PC 설정하기</h2>
<p>이제 Ubuntu 설치가 준비된 상태이거나 위 단계를 건너뛴 경우는 리눅스 배포판을 이미 실행 중이라면, 아래 단계를 실행하기 위해 필요한 종속성을 설치하는 시간입니다.</p>
<div class="content-ad"></div>
<p>먼저, 시스템이 최신 상태인지 확인하고 싶어요.</p>
<p>그 다음으로, 우리는 Llama.cpp 프로젝트를 환경에 클론/다운로드할 때 사용할 Git을 설치하고 싶어요.</p>
<p>마지막으로, 프로젝트를 만들고 LLM을 양자화하는 데 필요한 일부 도구를 설치해야 해요.</p>
<p>이 단계에서 많은 어려움을 겪었어요. 시스템이 2가지에 대해 불평했는데, 지금 설명할게요.</p>
<div class="content-ad"></div>
<p>파이프가 발견되지 않았어요. 그래서 Pip을 설치하기 위해 Python 패키지를 사용하여 apt install의 일부로 Pip을 설치해야 했어요.</p>
<p>이 문제로 x509 오류가 계속해서 발생했는데, "self-signed certificate in certificate chain"라는 오류가 떴어요. 자세한 내용에 대해 들어가지 않으면서 이 오류는 네트워크 구성에 문제가 있어서 해결할 수 없다는 것을 의미해요. 이 문제를 해결하기 위해 여러 가지 방법을 시도했지만 결국 해결책을 찾지 못했어요. 문제는 프록시나 방화벽 뒤에서 시도했기 때문에 구성을 올바르게 설정하는 것이 거의 불가능했을 수도 있어요. 그래서 이런 제한 사항이 없는 다른 시스템으로 전환하면서 모든 것이 완벽하게 작동했어요.</p>
<p>가장 중요한 것은 "python3 -m pip install" 명령을 실행할 때 "externally managed environment"에 있다는 오류가 나타났고, 작동을 위해 특정 가상 환경(venv)을 사용해야 한다는 것이었어요. 그래서 제가 그것을 사용하려고 했지만 작동하지 않았어요. 그래서 제가 한 해결책은 "외부 환경"이라고 불리는 심볼릭 링크를 제거하는 것이었어요. stackoverflow의 이 게시물에서 답변 중 한 댓글을 인용해 보면;</p>
<p>이 모든 문제가 해결됐으니 이제는 Pip을 통해 종속성을 설치하는 데 더 이상 문제가 없을 거에요.</p>
<div class="content-ad"></div>
<p>마지막으로, G++와 Build Essential을 설치해야 합니다.</p>
<h2>Llama 프로젝트 다운로드 및 빌드</h2>
<p>Llama 프로젝트를 워크스페이스로 다운로드하기 위해 "git clone" 명령어를 사용할 것입니다.</p>
<p>다운로드가 완료되면 방금 다운로드한 폴더로 이동합니다.</p>
<div class="content-ad"></div>
<p>이제 우리는 모델을 실행하는 데 필요한 파일을 생성하는 프로젝트를 "만들"할 것입니다.</p>
<p><img src="/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_4.png" alt="이미지"></p>
<p>이 작업이 진행 중이거나 완료된 후에, RPi5에서 실행할 모델 중 하나를 다운로드할 수 있습니다. 모델은 huggingface와 같은 원본에서 다운로드할 수 있지만, 제가 따라온 튜토리얼은 토렌트 클라이언트와 함께 사용할 수 있는 자석 링크를 사용했습니다. 저는 QBitTorrent라는 클라이언트를 사용했는데, 이는 다음과 같이 설치할 수 있습니다.</p>
<p>설치가 완료되면 다음 명령을 사용하여 애플리케이션을 열 수 있습니다.</p>
<div class="content-ad"></div>
<p>토렌트 클라이언트의 GUI가 열립니다. "링크" 아이콘을 클릭하여 자석 링크를 추가하고 다음 자석 링크를 붙여넣을 수 있습니다.</p>
<p>RPi5를 7B 모델로 실행해보려고 합니다 (다른 모델들은 많이 크고 많은 용량이 필요합니다). 그래서 7B 폴더와 그 하위 파일(tokenizer.checklist.chk 및 tokenizer.model)만 선택하려고 합니다.</p>
<img src="/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_5.png">
<p>파일 다운로드가 완료되면 이를 "llama.cpp/models" 폴더로 복사합니다. 이를 터미널의 명령줄을 통해 하거나 파일 탐색기 GUI를 열어서 수행할 수 있습니다. 다음 명령어는 현재 폴더에서 파일 탐색기를 엽니다. 그러면 7B 폴더와 tokenizer 파일을 찾아서 llama.cpp/models 폴더에 복사할 수 있습니다.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_6.png">
<p>이제 RPi5에서 실행할 수 있게 모델을 양자화하기 전에 JSON 파일을 편집해야 합니다. 이 파일에는 모델 빌드를 중단시킬 값이 포함되어 있습니다. 이 값을 파일에 포함된 이유에 대한 불확실성이 있습니다. 해당 값을 조정하면 모든 것이 원활하게 작동합니다.</p>
<p>llama.cpp/models/7B 폴더에서 "params.json" 파일을 찾아 엽니다. 이 파일을 편집할 때 VIM을 사용했습니다.</p>
<p>VIM으로 파일을 열었으면 "i"를 눌러 삽입(편집) 모드로 진입하세요. "vocab_size" 값을 -1에서 32000으로 수정합니다. 편집을 마쳤으면 편집 모드를 종료하려면 "ESC" 키를 누릅니다. 변경 내용을 저장하려면 콜론 ":"를 입력한 후 "wq"를 입력합니다. "Enter" 키를 눌러 진행하세요. 그러면 파일이 업데이트됩니다!</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_7.png" alt="이미지"></p>
<p>이제 모델을 GGML FP16 형식으로 변환해야 합니다. 이 작업은 PC 성능에 따라 다소 시간이 걸릴 수 있습니다. 여기서 주목해야 할 점은 라즈베리 파이에서 작동하지 않는 부분이라는데요. 모델을 변환하기 위해 내장된 기능을 사용합니다. llama.cpp 폴더에서 이를 실행합니다(우리의 명령이 "models/"로 시작하는 이유입니다).</p>
<p>변환 작업이 완료되면 모델을 양자화해야 합니다. 이 큰 모델을 보다 효율적으로 작동하도록 하는 것입니다. 사용한 튜토리얼에서 인용하면:
"이는 모델에서 사용되는 모든 신경망 가중치를 float16에서 init8로 변경하여 성능이 좋지 않은 기기에서 더 간편하게 처리할 수 있게 만드는 것을 의미합니다. 더 자세히 알아보는 것을 강력히 권장합니다(필수는 아님)."</p>
<p>양자화를 수행하기 위해 기존 기능을 다시 사용합니다.</p>
<div class="content-ad"></div>
<p>양자화 후, llama.cpp 폴더에서 다음 명령을 실행하여 모델이 작동하는지 확인할 수 있습니다.</p>
<p>모델을 찾을 수 없다는 오류가 발생했어요.</p>
<p><img src="/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_8.png" alt="이미지"></p>
<p>다행히도 명확한 원인을 찾았어요. chat.sh 스크립트가 "llama-7b" 폴더를 찾기를 기대했지만, "7B" 폴더를 다운로드했었어요. 폴더의 이름을 바꾸면 이 문제가 해결됩니다.</p>
<div class="content-ad"></div>
<p>이제 LLM이 작동되고, Bob이라는 챗봇도 있어서 질문을 할 수 있겠어요!</p>
<p><img src="/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_9.png" alt="Bob Chatbot"></p>
<p>이제 모든 것이 작동된다는 것을 알았으니, 다음 단계는 "모델" 폴더에서 "llama-7b" 폴더를 USB 스틱에 복사하는 것입니다.</p>
<h1>라즈베리 파이에 LLM 설치하기</h1>
<div class="content-ad"></div>
<p>이제 RPi5를 부팅하고 아직 열지 않았다면 터미널을 열어주세요 (CTRL+ALT+T). 우리는 llama 프레임워크를 Pi에도 필요하기 때문에 원본 PC에서 한 것과 같은 몇 가지 명령을 실행해야 합니다. 먼저 시스템이 최신 상태인지 확인하고 프로젝트를 복제하기 위해 Git이 설치되어 있는지 확인해주세요.</p>
<p>다음으로, 우리는 원본 PC에서 했던 것과 같이 llama.cpp 프로젝트를 복제합니다.</p>
<p>라이센스에 설치한 것과 같은 모듈을 설치합니다. 노트북에서 필요했던 해결책을 기억해 주세요. RPi5에서도 필요할 수 있기 때문입니다.</p>
<p>이제 G++와 Build Essential이 설치되어 있는지 확인해주세요.</p>
<div class="content-ad"></div>
<p>llama.cpp 폴더로 이동해서 llama 프로젝트를 만들어보세요</p>
<p>다음으로 외부 드라이브에서 내용을 llama.cpp 프로젝트의 /models/ 폴더로 이동해보세요. 외부 하드 드라이브가 제대로 감지되거나 마운트되지 않아 문제가 생겼는데요. 다행히도 인터넷의 도움을 받아 문제를 해결했어요.</p>
<p>Stackoverflow의 이 게시물을 참고하여 다음 명령어를 얻었어요.</p>
<p>이 명령어는 사용 가능한 디스크를 나열합니다. 여기서 외부 하드 드라이브나 USB 스틱을 찾을 수 있어요</p>
<div class="content-ad"></div>
<p>예를 들어, 외장 드라이브가 /dev/sdxn에 발견되었다면, 명령은 다음과 같을 것입니다.</p>
<p>마운트가 성공적으로 완료된 후, 외장 드라이브의 내용은 /mnt 폴더에서 찾을 수 있습니다. 명령줄 대신 파일 탐색기를 사용하여 파일을 시각적으로 복사하려면, /mnt 폴더로 이동하여 다음을 실행하세요.</p>
<p>이제 해야 할 일은 원본 PC에서 한 것과 정확히 같은 방식으로 모델을 실행하는 것뿐입니다. llama.cpp 폴더 안에서 다음 명령을 실행하세요</p>
<p><img src="/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_10.png" alt="image"></p>
<div class="content-ad"></div>
<p>위에서 말씀드린 대로 Raspberry Pi 5에서 자체 AI 챗봇을 작동시키고 있습니다! 라마는 RPi5에서 매끄럽게 작동하지는 않지만, 이렇게 작은 장치에서 이렇게 큰 모델이 효율적으로 작동할 수 있다는 점이 매우 멋지다는 것을 알려드리고 싶습니다. 제가 본 스크린샷을 보면이 모델이 가지고 있는 "지식"에 대해 완전히 확신을 갖지 못하겠지만요. 그럼에도 불구하고, 우리가 질문을 하면 응답을 생성해주는 점은 아주 멋집니다. 이것이 바로 생성적 AI가 하는 일이라고 볼 수 있습니다.</p>
<p>다시 말씀드리지만, 이 블로그/튜토리얼에서 다루는 주제에 대해 전문가는 아닙니다. 초기 크레딧은 Marek Żelichowski에게 드리며 그의 블로그/튜토리얼에서 한 작업에 있습니다. 저의 목표는 온라인에서 찾을 수 있는 일부 튜토리얼을 따를 때 어려움을 겪었던 일부 단계를 명확하게 설명하는 것이었습니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Raspberry Pi 5에서 Ollama를 사용하지 않고 Llama를 실행하기","description":"","date":"2024-06-19 18:11","slug":"2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama","content":"\n\n그래서 저는 작년 12월에 Raspberry Pi 5 8GB를 구입한 이후로 계속해서 시도해보고 있어요. 그전에 LLM을 설치하는 많은 안내서를 찾았지만 문제에 부딪히며 쉽게 해결할 수 없었어요. 이는 LLM을 회수/구축/양자화해야 하는 원본 컴퓨터와 제 RPi5에 필요한 모든 것을 설치하지 못했기 때문에 발생한 문제의 일부입니다.\n\n그래서 저는 여기에 내가 막혔던 부분을 정확히 지적하고 이를 어떻게 해결했는지를 써 놓은 가이드를 작성하게 되었어요.\n\n그러므로 이것은 전적으로 제가 혼자서 완전히 해결한 가이드는 아니에요 (저는 이런 주제에 대해 전문가가 아닌 걸로 아요), 다만 누군가가 곤란해할 때 도움이 될 수 있는 가이드에요.\n\n저는 LinkedIn에서 발견한 Marek Żelichowski의 이 가이드를 주로 사용했고, 제 기기에서 작동하는 데 필요한 단계를 추가했어요. 그는 자신의 블로그 맨 아래에 여러 사람/소스의 입력을 토대로 한 결과라고 명시하긴 했지만, 공정한 평가를 해야하며, Ollama를 요구하지 않는 저에게는 제게 도움이 된 소수의 가이드 중 하나였어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인용 위치(외부 게시물 및 사이트 링크)를 제공하려고 노력했지만, 빠진 인용이 있으면 알려주시면 기쁘게 추가하겠습니다. 그럼 시작해볼까요!\n\n# 필요한 것\n\n- LLM('들')을 검색하고 양자화하기 위한 Windows 또는 Linux 배포로 충분한 소스 PC\n- LLM을 실행할 8GB 라즈베리 파이 5\n- Raspbian과 같은 미리 설치된 OS가 포함된 적어도 32GB의 메모리 카드 (제가 개인적으로 RPi에는 Ubuntu 23.04를 사용합니다)\n- 소스 PC에서 RPi로 LLM을 전송하기 위해 적어도 22GB의 공간이 있는 USB 스틱\n\n# 소스 PC 준비하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 (Linux 기반) 소스 PC에서 시작합니다. 저는 Linux 디스트로로 PC를 재설치하거나 듀얼 부팅하는 일을 하기 귀찮아해 소유하지 않기 때문에 \"WSL\"을 사용하기로 결정했습니다. 이는 Windows 환경에서 Linux 디스트로를 직접 실행할 수 있는 Microsoft의 내장 기능입니다. 이미 Linux PC에서 작업하고 있는 분들은 아래 단계가 필요하지 않을 수 있습니다.\n\n## WSL 설치 방법:\n\nWindows PC에서 WSL을 설정하는 데 이 안내서를 사용했다는 점을 유의하세요.\n\n먼저 PowerShell을 관리자 권한으로 실행합니다. 이를 위해 PowerShell에 마우스 오른쪽 버튼을 클릭하고 \"관리자 권한으로 실행\" 옵션을 선택합니다. 여러분의 기기의 보안 구성 방식에 따라 실행에 관리자 암호를 입력해야 할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_0.png\" /\u003e\n\n열리는 PowerShell 창에서 다음 명령을 실행하세요:\n\n이 명령을 실행하면 작업에 사용할 OS로 Ubuntu가 설치됩니다. 또한 다음 명령을 사용하여 wsl을 통해 직접 설치할 수 있는 특정 Linux 배포판을 선택할 수도 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_1.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 설치 명령에 설치하고 싶은 배포판을 선택하세요. 기본값을 사용했지만 Ubuntu 22.04를 실행하려면 다음 명령을 사용하면 됩니다.\n\n설치가 완료되면 다음 명령을 사용하여 설치되었는지 확인할 수 있습니다.\n\n\nAfter the installation is done, you can verify that it is installed with this command\n\n\nWSL 설치 후 컴퓨터를 다시 시작하는 것이 강력히 권장됩니다. 재부팅 후 시작 메뉴에서 Linux 배포판을 찾을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_3.png\" /\u003e\n\n이 프로그램 중 하나를 열면 Ubuntu 설치에 대한 명령 프롬프트 (CMD)가 열립니다. 이 명령을 사용하여 Llama 프로젝트를 다운로드하고 빌드하고, Raspberry Pi에서 실행할 수 있는 모델을 다운로드하고 양자화할 것입니다.\n\n## Linux 소스 PC 설정하기\n\n이제 Ubuntu 설치가 준비된 상태이거나 위 단계를 건너뛴 경우는 리눅스 배포판을 이미 실행 중이라면, 아래 단계를 실행하기 위해 필요한 종속성을 설치하는 시간입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저, 시스템이 최신 상태인지 확인하고 싶어요.\n\n그 다음으로, 우리는 Llama.cpp 프로젝트를 환경에 클론/다운로드할 때 사용할 Git을 설치하고 싶어요.\n\n마지막으로, 프로젝트를 만들고 LLM을 양자화하는 데 필요한 일부 도구를 설치해야 해요.\n\n이 단계에서 많은 어려움을 겪었어요. 시스템이 2가지에 대해 불평했는데, 지금 설명할게요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파이프가 발견되지 않았어요. 그래서 Pip을 설치하기 위해 Python 패키지를 사용하여 apt install의 일부로 Pip을 설치해야 했어요.\n\n이 문제로 x509 오류가 계속해서 발생했는데, \"self-signed certificate in certificate chain\"라는 오류가 떴어요. 자세한 내용에 대해 들어가지 않으면서 이 오류는 네트워크 구성에 문제가 있어서 해결할 수 없다는 것을 의미해요. 이 문제를 해결하기 위해 여러 가지 방법을 시도했지만 결국 해결책을 찾지 못했어요. 문제는 프록시나 방화벽 뒤에서 시도했기 때문에 구성을 올바르게 설정하는 것이 거의 불가능했을 수도 있어요. 그래서 이런 제한 사항이 없는 다른 시스템으로 전환하면서 모든 것이 완벽하게 작동했어요.\n\n가장 중요한 것은 \"python3 -m pip install\" 명령을 실행할 때 \"externally managed environment\"에 있다는 오류가 나타났고, 작동을 위해 특정 가상 환경(venv)을 사용해야 한다는 것이었어요. 그래서 제가 그것을 사용하려고 했지만 작동하지 않았어요. 그래서 제가 한 해결책은 \"외부 환경\"이라고 불리는 심볼릭 링크를 제거하는 것이었어요. stackoverflow의 이 게시물에서 답변 중 한 댓글을 인용해 보면;\n\n이 모든 문제가 해결됐으니 이제는 Pip을 통해 종속성을 설치하는 데 더 이상 문제가 없을 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, G++와 Build Essential을 설치해야 합니다.\n\n## Llama 프로젝트 다운로드 및 빌드\n\nLlama 프로젝트를 워크스페이스로 다운로드하기 위해 \"git clone\" 명령어를 사용할 것입니다.\n\n다운로드가 완료되면 방금 다운로드한 폴더로 이동합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 우리는 모델을 실행하는 데 필요한 파일을 생성하는 프로젝트를 \"만들\"할 것입니다.\n\n![이미지](/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_4.png)\n\n이 작업이 진행 중이거나 완료된 후에, RPi5에서 실행할 모델 중 하나를 다운로드할 수 있습니다. 모델은 huggingface와 같은 원본에서 다운로드할 수 있지만, 제가 따라온 튜토리얼은 토렌트 클라이언트와 함께 사용할 수 있는 자석 링크를 사용했습니다. 저는 QBitTorrent라는 클라이언트를 사용했는데, 이는 다음과 같이 설치할 수 있습니다.\n\n설치가 완료되면 다음 명령을 사용하여 애플리케이션을 열 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n토렌트 클라이언트의 GUI가 열립니다. \"링크\" 아이콘을 클릭하여 자석 링크를 추가하고 다음 자석 링크를 붙여넣을 수 있습니다.\n\nRPi5를 7B 모델로 실행해보려고 합니다 (다른 모델들은 많이 크고 많은 용량이 필요합니다). 그래서 7B 폴더와 그 하위 파일(tokenizer.checklist.chk 및 tokenizer.model)만 선택하려고 합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_5.png\" /\u003e\n\n파일 다운로드가 완료되면 이를 \"llama.cpp/models\" 폴더로 복사합니다. 이를 터미널의 명령줄을 통해 하거나 파일 탐색기 GUI를 열어서 수행할 수 있습니다. 다음 명령어는 현재 폴더에서 파일 탐색기를 엽니다. 그러면 7B 폴더와 tokenizer 파일을 찾아서 llama.cpp/models 폴더에 복사할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_6.png\" /\u003e\n\n이제 RPi5에서 실행할 수 있게 모델을 양자화하기 전에 JSON 파일을 편집해야 합니다. 이 파일에는 모델 빌드를 중단시킬 값이 포함되어 있습니다. 이 값을 파일에 포함된 이유에 대한 불확실성이 있습니다. 해당 값을 조정하면 모든 것이 원활하게 작동합니다.\n\nllama.cpp/models/7B 폴더에서 \"params.json\" 파일을 찾아 엽니다. 이 파일을 편집할 때 VIM을 사용했습니다.\n\nVIM으로 파일을 열었으면 \"i\"를 눌러 삽입(편집) 모드로 진입하세요. \"vocab_size\" 값을 -1에서 32000으로 수정합니다. 편집을 마쳤으면 편집 모드를 종료하려면 \"ESC\" 키를 누릅니다. 변경 내용을 저장하려면 콜론 \":\"를 입력한 후 \"wq\"를 입력합니다. \"Enter\" 키를 눌러 진행하세요. 그러면 파일이 업데이트됩니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_7.png)\n\n이제 모델을 GGML FP16 형식으로 변환해야 합니다. 이 작업은 PC 성능에 따라 다소 시간이 걸릴 수 있습니다. 여기서 주목해야 할 점은 라즈베리 파이에서 작동하지 않는 부분이라는데요. 모델을 변환하기 위해 내장된 기능을 사용합니다. llama.cpp 폴더에서 이를 실행합니다(우리의 명령이 \"models/\"로 시작하는 이유입니다).\n\n변환 작업이 완료되면 모델을 양자화해야 합니다. 이 큰 모델을 보다 효율적으로 작동하도록 하는 것입니다. 사용한 튜토리얼에서 인용하면:\n\"이는 모델에서 사용되는 모든 신경망 가중치를 float16에서 init8로 변경하여 성능이 좋지 않은 기기에서 더 간편하게 처리할 수 있게 만드는 것을 의미합니다. 더 자세히 알아보는 것을 강력히 권장합니다(필수는 아님).\"\n\n양자화를 수행하기 위해 기존 기능을 다시 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n양자화 후, llama.cpp 폴더에서 다음 명령을 실행하여 모델이 작동하는지 확인할 수 있습니다.\n\n모델을 찾을 수 없다는 오류가 발생했어요.\n\n![이미지](/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_8.png)\n\n다행히도 명확한 원인을 찾았어요. chat.sh 스크립트가 \"llama-7b\" 폴더를 찾기를 기대했지만, \"7B\" 폴더를 다운로드했었어요. 폴더의 이름을 바꾸면 이 문제가 해결됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 LLM이 작동되고, Bob이라는 챗봇도 있어서 질문을 할 수 있겠어요!\n\n![Bob Chatbot](/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_9.png)\n\n이제 모든 것이 작동된다는 것을 알았으니, 다음 단계는 \"모델\" 폴더에서 \"llama-7b\" 폴더를 USB 스틱에 복사하는 것입니다.\n\n# 라즈베리 파이에 LLM 설치하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 RPi5를 부팅하고 아직 열지 않았다면 터미널을 열어주세요 (CTRL+ALT+T). 우리는 llama 프레임워크를 Pi에도 필요하기 때문에 원본 PC에서 한 것과 같은 몇 가지 명령을 실행해야 합니다. 먼저 시스템이 최신 상태인지 확인하고 프로젝트를 복제하기 위해 Git이 설치되어 있는지 확인해주세요.\n\n다음으로, 우리는 원본 PC에서 했던 것과 같이 llama.cpp 프로젝트를 복제합니다.\n\n라이센스에 설치한 것과 같은 모듈을 설치합니다. 노트북에서 필요했던 해결책을 기억해 주세요. RPi5에서도 필요할 수 있기 때문입니다.\n\n이제 G++와 Build Essential이 설치되어 있는지 확인해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nllama.cpp 폴더로 이동해서 llama 프로젝트를 만들어보세요\n\n다음으로 외부 드라이브에서 내용을 llama.cpp 프로젝트의 /models/ 폴더로 이동해보세요. 외부 하드 드라이브가 제대로 감지되거나 마운트되지 않아 문제가 생겼는데요. 다행히도 인터넷의 도움을 받아 문제를 해결했어요.\n\nStackoverflow의 이 게시물을 참고하여 다음 명령어를 얻었어요.\n\n이 명령어는 사용 가능한 디스크를 나열합니다. 여기서 외부 하드 드라이브나 USB 스틱을 찾을 수 있어요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, 외장 드라이브가 /dev/sdxn에 발견되었다면, 명령은 다음과 같을 것입니다.\n\n마운트가 성공적으로 완료된 후, 외장 드라이브의 내용은 /mnt 폴더에서 찾을 수 있습니다. 명령줄 대신 파일 탐색기를 사용하여 파일을 시각적으로 복사하려면, /mnt 폴더로 이동하여 다음을 실행하세요.\n\n이제 해야 할 일은 원본 PC에서 한 것과 정확히 같은 방식으로 모델을 실행하는 것뿐입니다. llama.cpp 폴더 안에서 다음 명령을 실행하세요\n\n![image](/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서 말씀드린 대로 Raspberry Pi 5에서 자체 AI 챗봇을 작동시키고 있습니다! 라마는 RPi5에서 매끄럽게 작동하지는 않지만, 이렇게 작은 장치에서 이렇게 큰 모델이 효율적으로 작동할 수 있다는 점이 매우 멋지다는 것을 알려드리고 싶습니다. 제가 본 스크린샷을 보면이 모델이 가지고 있는 \"지식\"에 대해 완전히 확신을 갖지 못하겠지만요. 그럼에도 불구하고, 우리가 질문을 하면 응답을 생성해주는 점은 아주 멋집니다. 이것이 바로 생성적 AI가 하는 일이라고 볼 수 있습니다.\n\n다시 말씀드리지만, 이 블로그/튜토리얼에서 다루는 주제에 대해 전문가는 아닙니다. 초기 크레딧은 Marek Żelichowski에게 드리며 그의 블로그/튜토리얼에서 한 작업에 있습니다. 저의 목표는 온라인에서 찾을 수 있는 일부 튜토리얼을 따를 때 어려움을 겪었던 일부 단계를 명확하게 설명하는 것이었습니다.","ogImage":{"url":"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_0.png"},"coverImage":"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_0.png","tag":["Tech"],"readingTime":8},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e그래서 저는 작년 12월에 Raspberry Pi 5 8GB를 구입한 이후로 계속해서 시도해보고 있어요. 그전에 LLM을 설치하는 많은 안내서를 찾았지만 문제에 부딪히며 쉽게 해결할 수 없었어요. 이는 LLM을 회수/구축/양자화해야 하는 원본 컴퓨터와 제 RPi5에 필요한 모든 것을 설치하지 못했기 때문에 발생한 문제의 일부입니다.\u003c/p\u003e\n\u003cp\u003e그래서 저는 여기에 내가 막혔던 부분을 정확히 지적하고 이를 어떻게 해결했는지를 써 놓은 가이드를 작성하게 되었어요.\u003c/p\u003e\n\u003cp\u003e그러므로 이것은 전적으로 제가 혼자서 완전히 해결한 가이드는 아니에요 (저는 이런 주제에 대해 전문가가 아닌 걸로 아요), 다만 누군가가 곤란해할 때 도움이 될 수 있는 가이드에요.\u003c/p\u003e\n\u003cp\u003e저는 LinkedIn에서 발견한 Marek Żelichowski의 이 가이드를 주로 사용했고, 제 기기에서 작동하는 데 필요한 단계를 추가했어요. 그는 자신의 블로그 맨 아래에 여러 사람/소스의 입력을 토대로 한 결과라고 명시하긴 했지만, 공정한 평가를 해야하며, Ollama를 요구하지 않는 저에게는 제게 도움이 된 소수의 가이드 중 하나였어요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e인용 위치(외부 게시물 및 사이트 링크)를 제공하려고 노력했지만, 빠진 인용이 있으면 알려주시면 기쁘게 추가하겠습니다. 그럼 시작해볼까요!\u003c/p\u003e\n\u003ch1\u003e필요한 것\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eLLM('들')을 검색하고 양자화하기 위한 Windows 또는 Linux 배포로 충분한 소스 PC\u003c/li\u003e\n\u003cli\u003eLLM을 실행할 8GB 라즈베리 파이 5\u003c/li\u003e\n\u003cli\u003eRaspbian과 같은 미리 설치된 OS가 포함된 적어도 32GB의 메모리 카드 (제가 개인적으로 RPi에는 Ubuntu 23.04를 사용합니다)\u003c/li\u003e\n\u003cli\u003e소스 PC에서 RPi로 LLM을 전송하기 위해 적어도 22GB의 공간이 있는 USB 스틱\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e소스 PC 준비하기\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e우리는 (Linux 기반) 소스 PC에서 시작합니다. 저는 Linux 디스트로로 PC를 재설치하거나 듀얼 부팅하는 일을 하기 귀찮아해 소유하지 않기 때문에 \"WSL\"을 사용하기로 결정했습니다. 이는 Windows 환경에서 Linux 디스트로를 직접 실행할 수 있는 Microsoft의 내장 기능입니다. 이미 Linux PC에서 작업하고 있는 분들은 아래 단계가 필요하지 않을 수 있습니다.\u003c/p\u003e\n\u003ch2\u003eWSL 설치 방법:\u003c/h2\u003e\n\u003cp\u003eWindows PC에서 WSL을 설정하는 데 이 안내서를 사용했다는 점을 유의하세요.\u003c/p\u003e\n\u003cp\u003e먼저 PowerShell을 관리자 권한으로 실행합니다. 이를 위해 PowerShell에 마우스 오른쪽 버튼을 클릭하고 \"관리자 권한으로 실행\" 옵션을 선택합니다. 여러분의 기기의 보안 구성 방식에 따라 실행에 관리자 암호를 입력해야 할 수도 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_0.png\"\u003e\n\u003cp\u003e열리는 PowerShell 창에서 다음 명령을 실행하세요:\u003c/p\u003e\n\u003cp\u003e이 명령을 실행하면 작업에 사용할 OS로 Ubuntu가 설치됩니다. 또한 다음 명령을 사용하여 wsl을 통해 직접 설치할 수 있는 특정 Linux 배포판을 선택할 수도 있습니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_1.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다음 설치 명령에 설치하고 싶은 배포판을 선택하세요. 기본값을 사용했지만 Ubuntu 22.04를 실행하려면 다음 명령을 사용하면 됩니다.\u003c/p\u003e\n\u003cp\u003e설치가 완료되면 다음 명령을 사용하여 설치되었는지 확인할 수 있습니다.\u003c/p\u003e\n\u003cp\u003eAfter the installation is done, you can verify that it is installed with this command\u003c/p\u003e\n\u003cp\u003eWSL 설치 후 컴퓨터를 다시 시작하는 것이 강력히 권장됩니다. 재부팅 후 시작 메뉴에서 Linux 배포판을 찾을 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_3.png\"\u003e\n\u003cp\u003e이 프로그램 중 하나를 열면 Ubuntu 설치에 대한 명령 프롬프트 (CMD)가 열립니다. 이 명령을 사용하여 Llama 프로젝트를 다운로드하고 빌드하고, Raspberry Pi에서 실행할 수 있는 모델을 다운로드하고 양자화할 것입니다.\u003c/p\u003e\n\u003ch2\u003eLinux 소스 PC 설정하기\u003c/h2\u003e\n\u003cp\u003e이제 Ubuntu 설치가 준비된 상태이거나 위 단계를 건너뛴 경우는 리눅스 배포판을 이미 실행 중이라면, 아래 단계를 실행하기 위해 필요한 종속성을 설치하는 시간입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e먼저, 시스템이 최신 상태인지 확인하고 싶어요.\u003c/p\u003e\n\u003cp\u003e그 다음으로, 우리는 Llama.cpp 프로젝트를 환경에 클론/다운로드할 때 사용할 Git을 설치하고 싶어요.\u003c/p\u003e\n\u003cp\u003e마지막으로, 프로젝트를 만들고 LLM을 양자화하는 데 필요한 일부 도구를 설치해야 해요.\u003c/p\u003e\n\u003cp\u003e이 단계에서 많은 어려움을 겪었어요. 시스템이 2가지에 대해 불평했는데, 지금 설명할게요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e파이프가 발견되지 않았어요. 그래서 Pip을 설치하기 위해 Python 패키지를 사용하여 apt install의 일부로 Pip을 설치해야 했어요.\u003c/p\u003e\n\u003cp\u003e이 문제로 x509 오류가 계속해서 발생했는데, \"self-signed certificate in certificate chain\"라는 오류가 떴어요. 자세한 내용에 대해 들어가지 않으면서 이 오류는 네트워크 구성에 문제가 있어서 해결할 수 없다는 것을 의미해요. 이 문제를 해결하기 위해 여러 가지 방법을 시도했지만 결국 해결책을 찾지 못했어요. 문제는 프록시나 방화벽 뒤에서 시도했기 때문에 구성을 올바르게 설정하는 것이 거의 불가능했을 수도 있어요. 그래서 이런 제한 사항이 없는 다른 시스템으로 전환하면서 모든 것이 완벽하게 작동했어요.\u003c/p\u003e\n\u003cp\u003e가장 중요한 것은 \"python3 -m pip install\" 명령을 실행할 때 \"externally managed environment\"에 있다는 오류가 나타났고, 작동을 위해 특정 가상 환경(venv)을 사용해야 한다는 것이었어요. 그래서 제가 그것을 사용하려고 했지만 작동하지 않았어요. 그래서 제가 한 해결책은 \"외부 환경\"이라고 불리는 심볼릭 링크를 제거하는 것이었어요. stackoverflow의 이 게시물에서 답변 중 한 댓글을 인용해 보면;\u003c/p\u003e\n\u003cp\u003e이 모든 문제가 해결됐으니 이제는 Pip을 통해 종속성을 설치하는 데 더 이상 문제가 없을 거에요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e마지막으로, G++와 Build Essential을 설치해야 합니다.\u003c/p\u003e\n\u003ch2\u003eLlama 프로젝트 다운로드 및 빌드\u003c/h2\u003e\n\u003cp\u003eLlama 프로젝트를 워크스페이스로 다운로드하기 위해 \"git clone\" 명령어를 사용할 것입니다.\u003c/p\u003e\n\u003cp\u003e다운로드가 완료되면 방금 다운로드한 폴더로 이동합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제 우리는 모델을 실행하는 데 필요한 파일을 생성하는 프로젝트를 \"만들\"할 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_4.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e이 작업이 진행 중이거나 완료된 후에, RPi5에서 실행할 모델 중 하나를 다운로드할 수 있습니다. 모델은 huggingface와 같은 원본에서 다운로드할 수 있지만, 제가 따라온 튜토리얼은 토렌트 클라이언트와 함께 사용할 수 있는 자석 링크를 사용했습니다. 저는 QBitTorrent라는 클라이언트를 사용했는데, 이는 다음과 같이 설치할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e설치가 완료되면 다음 명령을 사용하여 애플리케이션을 열 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e토렌트 클라이언트의 GUI가 열립니다. \"링크\" 아이콘을 클릭하여 자석 링크를 추가하고 다음 자석 링크를 붙여넣을 수 있습니다.\u003c/p\u003e\n\u003cp\u003eRPi5를 7B 모델로 실행해보려고 합니다 (다른 모델들은 많이 크고 많은 용량이 필요합니다). 그래서 7B 폴더와 그 하위 파일(tokenizer.checklist.chk 및 tokenizer.model)만 선택하려고 합니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_5.png\"\u003e\n\u003cp\u003e파일 다운로드가 완료되면 이를 \"llama.cpp/models\" 폴더로 복사합니다. 이를 터미널의 명령줄을 통해 하거나 파일 탐색기 GUI를 열어서 수행할 수 있습니다. 다음 명령어는 현재 폴더에서 파일 탐색기를 엽니다. 그러면 7B 폴더와 tokenizer 파일을 찾아서 llama.cpp/models 폴더에 복사할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_6.png\"\u003e\n\u003cp\u003e이제 RPi5에서 실행할 수 있게 모델을 양자화하기 전에 JSON 파일을 편집해야 합니다. 이 파일에는 모델 빌드를 중단시킬 값이 포함되어 있습니다. 이 값을 파일에 포함된 이유에 대한 불확실성이 있습니다. 해당 값을 조정하면 모든 것이 원활하게 작동합니다.\u003c/p\u003e\n\u003cp\u003ellama.cpp/models/7B 폴더에서 \"params.json\" 파일을 찾아 엽니다. 이 파일을 편집할 때 VIM을 사용했습니다.\u003c/p\u003e\n\u003cp\u003eVIM으로 파일을 열었으면 \"i\"를 눌러 삽입(편집) 모드로 진입하세요. \"vocab_size\" 값을 -1에서 32000으로 수정합니다. 편집을 마쳤으면 편집 모드를 종료하려면 \"ESC\" 키를 누릅니다. 변경 내용을 저장하려면 콜론 \":\"를 입력한 후 \"wq\"를 입력합니다. \"Enter\" 키를 눌러 진행하세요. 그러면 파일이 업데이트됩니다!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_7.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e이제 모델을 GGML FP16 형식으로 변환해야 합니다. 이 작업은 PC 성능에 따라 다소 시간이 걸릴 수 있습니다. 여기서 주목해야 할 점은 라즈베리 파이에서 작동하지 않는 부분이라는데요. 모델을 변환하기 위해 내장된 기능을 사용합니다. llama.cpp 폴더에서 이를 실행합니다(우리의 명령이 \"models/\"로 시작하는 이유입니다).\u003c/p\u003e\n\u003cp\u003e변환 작업이 완료되면 모델을 양자화해야 합니다. 이 큰 모델을 보다 효율적으로 작동하도록 하는 것입니다. 사용한 튜토리얼에서 인용하면:\n\"이는 모델에서 사용되는 모든 신경망 가중치를 float16에서 init8로 변경하여 성능이 좋지 않은 기기에서 더 간편하게 처리할 수 있게 만드는 것을 의미합니다. 더 자세히 알아보는 것을 강력히 권장합니다(필수는 아님).\"\u003c/p\u003e\n\u003cp\u003e양자화를 수행하기 위해 기존 기능을 다시 사용합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e양자화 후, llama.cpp 폴더에서 다음 명령을 실행하여 모델이 작동하는지 확인할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e모델을 찾을 수 없다는 오류가 발생했어요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_8.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e다행히도 명확한 원인을 찾았어요. chat.sh 스크립트가 \"llama-7b\" 폴더를 찾기를 기대했지만, \"7B\" 폴더를 다운로드했었어요. 폴더의 이름을 바꾸면 이 문제가 해결됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제 LLM이 작동되고, Bob이라는 챗봇도 있어서 질문을 할 수 있겠어요!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_9.png\" alt=\"Bob Chatbot\"\u003e\u003c/p\u003e\n\u003cp\u003e이제 모든 것이 작동된다는 것을 알았으니, 다음 단계는 \"모델\" 폴더에서 \"llama-7b\" 폴더를 USB 스틱에 복사하는 것입니다.\u003c/p\u003e\n\u003ch1\u003e라즈베리 파이에 LLM 설치하기\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제 RPi5를 부팅하고 아직 열지 않았다면 터미널을 열어주세요 (CTRL+ALT+T). 우리는 llama 프레임워크를 Pi에도 필요하기 때문에 원본 PC에서 한 것과 같은 몇 가지 명령을 실행해야 합니다. 먼저 시스템이 최신 상태인지 확인하고 프로젝트를 복제하기 위해 Git이 설치되어 있는지 확인해주세요.\u003c/p\u003e\n\u003cp\u003e다음으로, 우리는 원본 PC에서 했던 것과 같이 llama.cpp 프로젝트를 복제합니다.\u003c/p\u003e\n\u003cp\u003e라이센스에 설치한 것과 같은 모듈을 설치합니다. 노트북에서 필요했던 해결책을 기억해 주세요. RPi5에서도 필요할 수 있기 때문입니다.\u003c/p\u003e\n\u003cp\u003e이제 G++와 Build Essential이 설치되어 있는지 확인해주세요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003ellama.cpp 폴더로 이동해서 llama 프로젝트를 만들어보세요\u003c/p\u003e\n\u003cp\u003e다음으로 외부 드라이브에서 내용을 llama.cpp 프로젝트의 /models/ 폴더로 이동해보세요. 외부 하드 드라이브가 제대로 감지되거나 마운트되지 않아 문제가 생겼는데요. 다행히도 인터넷의 도움을 받아 문제를 해결했어요.\u003c/p\u003e\n\u003cp\u003eStackoverflow의 이 게시물을 참고하여 다음 명령어를 얻었어요.\u003c/p\u003e\n\u003cp\u003e이 명령어는 사용 가능한 디스크를 나열합니다. 여기서 외부 하드 드라이브나 USB 스틱을 찾을 수 있어요\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e예를 들어, 외장 드라이브가 /dev/sdxn에 발견되었다면, 명령은 다음과 같을 것입니다.\u003c/p\u003e\n\u003cp\u003e마운트가 성공적으로 완료된 후, 외장 드라이브의 내용은 /mnt 폴더에서 찾을 수 있습니다. 명령줄 대신 파일 탐색기를 사용하여 파일을 시각적으로 복사하려면, /mnt 폴더로 이동하여 다음을 실행하세요.\u003c/p\u003e\n\u003cp\u003e이제 해야 할 일은 원본 PC에서 한 것과 정확히 같은 방식으로 모델을 실행하는 것뿐입니다. llama.cpp 폴더 안에서 다음 명령을 실행하세요\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama_10.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e위에서 말씀드린 대로 Raspberry Pi 5에서 자체 AI 챗봇을 작동시키고 있습니다! 라마는 RPi5에서 매끄럽게 작동하지는 않지만, 이렇게 작은 장치에서 이렇게 큰 모델이 효율적으로 작동할 수 있다는 점이 매우 멋지다는 것을 알려드리고 싶습니다. 제가 본 스크린샷을 보면이 모델이 가지고 있는 \"지식\"에 대해 완전히 확신을 갖지 못하겠지만요. 그럼에도 불구하고, 우리가 질문을 하면 응답을 생성해주는 점은 아주 멋집니다. 이것이 바로 생성적 AI가 하는 일이라고 볼 수 있습니다.\u003c/p\u003e\n\u003cp\u003e다시 말씀드리지만, 이 블로그/튜토리얼에서 다루는 주제에 대해 전문가는 아닙니다. 초기 크레딧은 Marek Żelichowski에게 드리며 그의 블로그/튜토리얼에서 한 작업에 있습니다. 저의 목표는 온라인에서 찾을 수 있는 일부 튜토리얼을 따를 때 어려움을 겪었던 일부 단계를 명확하게 설명하는 것이었습니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-RunLlamaonyourRaspberryPi5withoutusingOllama"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>