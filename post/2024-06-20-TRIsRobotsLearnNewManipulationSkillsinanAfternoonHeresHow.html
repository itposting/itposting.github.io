<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>TRI의 로봇들이 오후에 새로운 조작 기술을 익힌 방법을 소개합니다 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="TRI의 로봇들이 오후에 새로운 조작 기술을 익힌 방법을 소개합니다 | itposting" data-gatsby-head="true"/><meta property="og:title" content="TRI의 로봇들이 오후에 새로운 조작 기술을 익힌 방법을 소개합니다 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow" data-gatsby-head="true"/><meta name="twitter:title" content="TRI의 로봇들이 오후에 새로운 조작 기술을 익힌 방법을 소개합니다 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-20 17:53" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">TRI의 로봇들이 오후에 새로운 조작 기술을 익힌 방법을 소개합니다</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="TRI의 로봇들이 오후에 새로운 조작 기술을 익힌 방법을 소개합니다" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 20, 2024</span><span class="posts_reading_time__f7YPP">9<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>작성자: Siyuan Feng, Ben Burchfiel, Toffee Albina, and Russ Tedrake</h2>
<p><img src="/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_0.png" alt="이미지"></p>
<p>TRI는 로봇이 시연을 통해 새로운 미술적 행동을 습득할 수 있는 새로운 접근 방식을 공개했습니다. 이것이 왜 중요한 새로운 기능인지, 이것이 가능하게 된 진전은 무엇이며, 다음으로 어디를 향해 나아가는지 살펴보겠습니다.</p>
<h1>소개</h1>
<div class="content-ad"></div>
<p>TRI의 로봇공학 연구 부문의 미션은 신기술을 개발하여 다음 세대의 민첩한 로봇이 사람들을 지원하고 삶의 질을 향상시키는 데 기여하는 것입니다. 이를 위해서는 로봇이 구조화되지 않은 자연 환경에서 사람들과 함께 작동해야 합니다. 환장한 분들을 도와 요리 준비를 하거나 다친 사람을 의자에서 일어나게 하거나 작업 현장에서 일하는 사람들을 지원하는 것이 있을 수 있습니다. 미래의 로봇들은 세계 어디에서나 사람들의 삶에 엄청난 긍정적인 영향을 미칠 것입니다.</p>
<p>다음 세대의 로봇은 유연하고 적응력이 있어야 합니다. 두 집, 두 작업 현장, 두 사람이 서로 같지 않기 때문에, 우리의 로봇은 오늘날보다 훨씬 더 다재다능해져야 합니다. 현재 로봇들은 임무를 수행할 수 있도록 면밀히 프로그래밍되어 있으며, 사람들은 명백한 모서리 사례를 예상하고 로봇에게 실수 대처 방법을 가르쳐야 합니다. 이 접근 방식은 통제된 환경에서 막대한 성공을 거두었으며, 현대의 공장 및 창고 자동화의 기초가 됩니다. 그러나 이 방식으로 로봇을 만드는 것은 로봇의 환경을 주의 깊게 모델링하고, 로봇의 행동 범위를 주의 깊게 설정하고, 모든 상황을 미리 예견해야 합니다. 이것은 앞으로 야생에서 작동하는 더 능숙한 미래 로봇을 위해 필요한 복잡성으로 확장할 수 없습니다.</p>
<p>그 결과로 로보틱스 분야에서 인공지능과 기계 학습을 사용하는 로봇에 대한 관심이 급증했습니다. 이러한 접근 방식 중 많은 것이 전통적인 로봇공학의 측면을 AI와 융합하여 효과적으로 사용하지만, 중요한 것이 빠졌습니다: 로봇은 아직 주변의 물리적 세계를 제어하는 데 재능이 없습니다. 이제 우리는 사람들과 대화할 수 있는 로봇을 가졌지만 과자 봉지를 열 수 없는 것이든 신발끈을 매 줄 수 없는 로봇이 있습니다. 로봇은 똑똑해지고 있지만 아직 간단한 방식으로 세상과 상호 작용할 뿐입니다.</p>
<p>이를 해결하기 위해 TRI는 최근 최첨단 생성 AI의 진보를 바탕으로 로봇에게 단 하루 만에 새로운 조작 능력을 가르칠 수 있는 능력을 개발했습니다. 동일한 로봇, 동일한 코드 및 동일한 설정을 사용하여 우리는 야채 껍질 벗기기, 핸드 믹서 사용, 스낵 준비 및 팬케이크 뒤집기와 같은 60가지 다른 민첩한 행동을 가르쳤습니다.</p>
<div class="content-ad"></div>
<p>지금은 오늘의 가능성을 끌어올리기만 하는 것이 아니라 유연하고 적응 가능한 손재주를 가진 일반 목적의 로봇의 기반을 마련하기 위해 다양한 행동들의 광범위한 커리큘럼을 개발 중입니다.</p>
<p>이를 가능하게 하는 비하인드 스토리를 자세히 알려드리는 것에 기쁨을 느낍니다.</p>
<h1>가르치는 방법</h1>
<p>새로운 행동을 가르치기 위해 인간 작업자가 로봇을 원하는 작업을 시연하는 방식으로 원격으로 작동시킵니다. 보통 몇 시간의 가르침이 필요하며, 이로 인해 수십 개에서 수백 개의 시연이 이루어집니다.</p>
<div class="content-ad"></div>
<h1>학습 방식</h1>
<p>어떤 특정 행동에 대한 집합의 시연이 수집되면, 로봇은 해당 행동을 자율적으로 수행하도록 학습합니다. 우리 프로세스의 핵심은 최근 이미지 생성 분야를 뒤흔든 생성적 AI 기술인 디퓨전(Diffusion)입니다. 최근 TRI와 우리 대학 파트너들인 송 교수 연구실에서 이 기술을 채택하여 직접적으로 로봇 행동을 생성하는 디퓨전 정책(Diffusion Policy)이라는 방법을 개발했습니다. 자연어에 의존한 이미지 생성이 아닌 센서 관측 및 선택적으로 자연어에 의존한 상황에 따른 로봇 행동을 생성하기 위해 디퓨전을 사용합니다.</p>
<p><img src="/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_1.png" alt="이미지"></p>
<p>로봇 행동을 생성하는 데 디퓨전을 사용하는 것은 이전 방법들보다 세 가지 주요 이점을 제공합니다:</p>
<div class="content-ad"></div>
<ul>
<li>멀티 모달 시연에 대한 적합성. 이것은 인간 시연자들이 행동을 자연스럽게 가르치고 로봇이 혼란스러워하지 않아도 된다는 것을 의미합니다.</li>
<li>고차원 행동 공간에 적합성. 이것은 로봇이 미래를 예측하고 단시간 행동을 피함에 도움이 되는 것을 가능하게 합니다.</li>
<li>안정적이고 신뢰할 수 있는 훈련. 이것은 귀찮은 손조절이나 황금자료점을 찾아다니지 않고도 규모에 맞게 로봇을 훈련하고 신뢰할 수 있다는 것을 의미합니다.</li>
</ul>
<h2>멀티 모달 행동</h2>
<p>대부분의 현실 세계 작업은 여러 가지 다른 방법으로 해결할 수 있습니다. 예를 들어 컵을 들어올릴 때 사람은 위쪽, 옆면 또는 심지어 아래에서 잡을 수 있습니다. 이 같은 현상, 행동의 멀티모달성은 이전까지 행동 학습 방법이 적응하기 어려웠던 것으로, 정상적인 인간 행동에서 널리 퍼져 있음에도 불구하고 어려움을 겪었습니다.</p>
<p>로봇이 테이블 위에 앉아 있는 T자 모양 블록을 목표 위치로 밀어야 하는 간단한 예시를 고려해 보겠습니다. 로봇은 블록을 미끄러뜨려 이동해야 하며 다양한 면에 도달하기 위해 블록 주위로 이동해야 합니다. 이 작업은 본질적으로 멀티모달성을 포함하고 있으며 블록을 좌측이나 우측으로 이동하는 것이 대개 합리적입니다. 두 가지 동등한 올바른 행동 모드가 있습니다. 이 해결책은 단일 행동을 예측하는 대신 행동 분포를 학습하는 것입니다. 확산 정책은 이러한 분포를 더 안정적이고 견고하게 배우며, 이전 방법보다 멀티모달성을 훨씬 잘 포착합니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_2.png" alt="image"></p>
<p>복합 모달 데모를 다룰 수 있는 능력은 복잡한 재치 있는 행동을 가르치는 데 중요한 역할을 한 것으로 입증되었으며, 이러한 유형의 복합성은 보편적입니다. 또한 우리의 로봇이 데이터 수집을 확장하는 동안 여러 선생님으로부터 쉽게 배울 수 있도록 합니다.</p>
<h2>고차원 행동</h2>
<p>확산은 고차원 출력 공간에 맞게 자연스럽게 적합합니다. 예를 들어 이미지를 생성하는 데는 수십만 개의 개별 픽셀을 예측해야 합니다. 로봇 공학에서 이것은 주요한 장점이며, 복잡한 다리를 가진 로봇에 우아하게 확산 기반 행동 모델을 확장할 수 있게 합니다. 또한 한 번의 단계가 아닌 의도된 행동 경로를 예측하는 능력을 제공합니다. 최근 작업 'DP, ACT'는 경로 예측이 종종 성능이 우수한 견고한 정책을 학습하기 위한 중요한 설계 기능임을 보여 주었습니다.</p>
<div class="content-ad"></div>
<h2>안정적인 교육</h2>
<p>확산 정책은 교육을 진행하는 데 괴로워하지 않을 만큼 굉장히 간단합니다. 새로운 행동을 가르치는 데에 많은 비용이 들고 번거로운 실제 세계 평가를 할 필요 없이 최상의 실행 체크포인트와 하이퍼파라미터를 찾는 과정을 요구하지 않습니다. 컴퓨터 비전이나 자연 언어 적용과 달리, AI 기반의 닫힌 루프 시스템은 오프라인 지표로 정확하게 평가되지 않을 수 없습니다. 이 시스템들은 일반적으로 로봇 공학에서 실제 하드웨어에서 평가를 필요로 합니다. 이는 현실 세계 평가 병목 현상으로 인해 광범위한 조정이나 하이퍼파라미터 최적화를 필요로 하는 모든 학습 파이프라인이 실용성을 잃게 만듭니다. 이러한 이유로 확산 정책은 뛰어난 안정성을 바탕으로 작동하여 이 어려움을 우회할 수 있게 해줍니다. 이것이 바로 우리에게 규모의 핵심을 제공해준 핵심적인 요소입니다.</p>
<h1>우리의 플랫폼</h1>
<h2>원격 작동</h2>
<div class="content-ad"></div>
<p>로봇을 인간의 데모를 통해 가르치기 때문에, 어려운 미묘한 동작을 가르치는 데 중요한 것은 좋은 원격 조작(teleoperation) 인터페이스입니다. 저희의 로봇 학습 방법은 텔레오퍼레이션 장치의 선택에 중립적이며, 조이스틱과 같은 다양한 저가의 인터페이스를 사용해왔습니다. 보다 미묘한 동작을 위해 바이매뉼 햅틱(haptic) 장치를 사용하여 가르칩니다. 이때 텔레오퍼레이션 장치와 로봇 사이에 위치-위치 결합이 있습니다. 위치-위치 결합은 입력 장치가 측정된 자세를 명령으로 보내고 로봇이 이러한 자세 명령을 토크를 사용한 운동 공간 제어로 추적한다는 것을 의미합니다. 로봇의 자세 추적 오류를 힘이 변환한 후 이 힘을 선생님이 느낄 수 있도록 다시 입력 장치로 보냅니다. 이를 통해 선생님은 힘을 통해 로봇과의 피드백 루프를 닫을 수 있으며, 우리가 가르치는 가장 어려운 기술 중 많은 부분에 필수적입니다.</p>
<p>양 팔을 사용해 물체를 조작할 때 힘 피드백은 특히 중요합니다. 이를 실험적으로 확인할 수 있는 사례는 수동 핸드믹서와 같은 작동을 필요로 하는 장치를 조작하는 것입니다. 이는 이 피드백 없이 신뢰할 수 없는 방식으로 데모할 수 없었습니다.</p>
<p>로봇이 양 손으로 도구를 잡을 때, 닫힌 기구 체인이 형성됩니다. 로봇과 도구의 특정 설정에 대해 시각적으로 관측할 수 없는 다양한 내부 힘 범위가 있습니다. 그리퍼를 떼어내는 것과 같은 특정 힘 구성은 불안정하며, 로봇의 움직임이 미끄러질 가능성이 높아집니다. 휴먼 데모 사람들이 햅틱 피드백에 접근할 수 없는 경우, 적절한 힘 제어를 느끼거나 가르칠 수 없습니다. 우리는 힙틱 피드백이 양 팔의 결합이 필요한 굵직한 미적 행동을 가르칠 때 데모 성공률을 개선하는 데 중요하다는 것을 발견했습니다.</p>
<h2>촉각 감지</h2>
<div class="content-ad"></div>
<p>장갑을 끼고 신발을 묶으려고 한 적이 있는 사람은 손의 감각이 얼마나 중요한지를 경험했을 것입니다. 꼼꼼한 작업을 할 때, 무슨 일이 일어나고 있는지 느낄 수 있다는 것은 성공에 중요한 추가 정보를 제공합니다. 우리는 로봇도 이와 마찬가지로 촉감을 갖는 것이 도움이 될 것이라고 믿습니다. 그래서 우리는 많은 플랫폼에서 TRI Soft-Bubble 센서를 사용하고 있습니다. 이러한 센서는 내부 카메라가 부풀어 오른 변형 가능한 외부 막을 관찰하는 것으로 구성되어 있습니다. 이러한 센서들은 희박한 힘 신호를 측정하는 것을 넘어 로봇이 접촉 패턴, 기하학, 미끄러짐 및 힘에 관한 공간적으로 밀집된 정보를 인식할 수 있도록 합니다.</p>
<p>이 유형의 센서는 최근 몇 년간 보다 인기가 많아졌지만, 제공하는 정보를 잘 활용하는 것은 어려운 과제였습니다. 확산은 로봇이 이 시각-촉감 센서가 제공하는 풍부한 정보를 사용하는 자연스러운 방법을 제공합니다. 이러한 신호에 대해 조건부로 - 추가 입력으로 사용하는 것으로 - 우리는 이를 임의의 꼼꼼한 작업에 적용할 수 있도록 합니다.</p>
<p>이 방향으로 진행한 초기 실험은 매우 유망합니다. 많은 경우에 접촉 감지를 추가하면 로봇의 재미있는 접촉 단계를 포함한 작업 수행 능력이 크게 향상됨을 발견하고 있습니다 😊</p>
<img src="/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_3.png">
<div class="content-ad"></div>
<h2>안전하고 성능이 우수한 제어</h2>
<p>고성능 로봇의 중간 제어 계층은 중대한 역할을 하며 종종 무시되는 부분입니다. 우리의 경우, 학습된 정책과 인간 데모 사용자는 10Hz로 로봇의 그리퍼에 대한 위치 및 방향 명령을 내립니다. 이러한 명령들은 중간 제어자에 의해 1kHz의 관절 토크 명령으로 업샘플링되고 변환됩니다. 중요한 점은 이 중간 제어자에는 로봇을 안전하게 보호하고 학습된 정책이나 인간 교사로부터 잠재적으로 위험한 상위 명령을 실행하지 못하도록 하는 안전 기능이 내장되어 있다는 것입니다.</p>
<p>저희 방식은 작업 공간 제어에 근거하며 관절 레벨 명령에 대한 제약 조건이 있는 최적화 문제로 정의됩니다. 목표는 데모 사용자나 학습된 정책이 제공하는 고수준 명령을 따르면서 물리 및 충돌 회피와 같은 기타 안전 제약 조건을 준수하는 것입니다. 이 구현은 Drake Systems Framework를 활용하며 엄격한 분석과 테스트를 가능케 하여 시스템의 이 중요한 부분에 대해 확신할 수 있게 해줍니다. 우리는 이 구현을 향후 오픈 소스로 공개할 계획이며 이는 우리 시스템의 주요 성공 요소 중 하나로 여기고 있습니다.</p>
<p>튼튼한 중간 제어자는 고품질의 행동 학습 파이프라인의 기초입니다. 이는 임피던스 제어 및 햅틱 피드백과 같은 임무 중요한 기능을 가능하게 하며 전반적인 시스템에 보다 훌륭한 안전 보호 기능을 제공하여 교사가 로봇을 물리적 한계까지 밀어낼 수 있도록 도와줍니다.</p>
<div class="content-ad"></div>
<h1>현재 어디에 있나요</h1>
<p>로봇공학의 놀라운 새로운 시대에 들어서고 있습니다. 전문 로봇 공학자들이 몇 주의 시간을 투자해야 했던 일들을 누구나 이제 하루 만에 가르치고 성공을 기대할 수 있습니다. 이제는 조정 없이 작동하는 단일 파이프라인을 통해 복잡한 상호 작용을 가진 민첩한 행동들을 가르치는 것이 가능해졌습니다.</p>
<p>더 많이 해야 할 일이 남아 있습니다. 현재 우리가 로봇에 새로운 기술을 가르칠 때, 그것은 부서지기 쉽습니다. 기술은 가르치는 데 사용된 상황과 유사한 상황에서는 잘 작동할 것이지만, 차이가 있는 상황에서 로봇은 고전할 것입니다. 실전에서 우리가 관찰하는 실패 케이스의 가장 흔한 원인은 다음과 같습니다:</p>
<ul>
<li>회복이 증명되지 않은 상태. 이는 너무 깨끗한 데모데이터로 인한 결과일 수 있습니다.</li>
<li>카메라 시점 또는 배경의 중요한 변화.</li>
<li>테스트 시간에 훈련 중 만나지 못한 피조작물.</li>
<li>방해물, 예를 들어 훈련 중에 없던 중요한 혼잡.</li>
</ul>
<div class="content-ad"></div>
<p>그럼 해결책은 무엇일까요? 사람이 평생 경험을 토대로 배우듯이 점차 능력과 적응력이 향상되는 것처럼, 우리는 다양한 행동 교육과정이 유연하고 일반적인 로봇을 만드는 열쇠라고 믿습니다. 이를 위해 TRI는 신체 데이터에 기초한 강력한 행동 교육과정을 실제 로봇 그룹과 강력한 드레이크 시뮬레이션 스위트 모두에 투자하고 있습니다.</p>
<p>지금까지 재치있는 조작 로봇 그룹에 60가지 이상의 행동을 가르쳤으며 올해 말까지 200가지 이상의 행동을 목표로 한 노력의 속도를 증가하고 있습니다. 이러한 행동은 도구 사용부터 가소성 물체 조작, 조심스러운 양손 조율까지 다양한 조작 시나리오에 걸쳐 있습니다.</p>
<h1>다음은</h1>
<p>다양한 기술 데이터셋에서 정책을 학습할 때 초기 성공의 조짐을 보았습니다. 한 예로, 상당히 정리된 장면에서 로봇에게 얼음이 담긴 머그잔을 싱크대로 비우도록 가르쳤습니다. 매우 혼잡한 상황에서 평가할 때, 이 데이터만 가지고 훈련된 정책은 거의 즉시 실패했습니다. 우리는 이 기술의 두 번째 버전을 얼음을 따르는 다른 15가지 작업과 함께 공동으로 가르쳤고, 로봇에게 원하는 행동에 대한 언어 설명을 조건으로 두었습니다. 어지럽힘에서 얼음을 붓는 동일한 시연자에게 접근할 수 있음에도 불구하고, 다중 작업 기술은 단일 작업 버전이 재앙적으로 실패한 매우 혼잡한 상황에서 성공했습니다.</p>
<div class="content-ad"></div>
<p>그러나 우리는 실제 로봇 데이터만으로는 진정으로 일반적인 손재주 로봇을 만들기에 충분하지 않을 것으로 예상합니다. 이 빈 공간을 채우기 위해, 우리는 Drake를 사용하여 강력한 시뮬레이션 전문 기술에 크게 의존하고 있습니다. Drake는 단단한 물체와 부드러운 물체 간 상세한 물리 상호작용을 정확하게 모델링할 수 있는 능력이 독특하며, 정교하고 복잡한 행동에 중요합니다.</p>
<p>이 노력이 진행됨에 따라 성공의 확실한 징후 중 하나는 영사적으로 능숙한 행동 생성과 맥락 속 학습일 것입니다. 기존의 대형 언어 모델은 새로운 방식으로 개념을 구성하고 단일 예제에서 배울 수 있는 강력한 능력을 갖고 있습니다. 지난 한 해 동안 우리는 로봇이 의미론적으로 일반화하도록 함으로써 이를 가능케 하여 신규 물체로 채집 및 배치하는 것을 보았습니다. 다음 큰 이정표는 이 의미론적 능력을 높은 수준의 물리 지능과 창의성과 융합하는 동등한 강력한 대규모 행동 모델을 만드는 것입니다. 이러한 모델은 주변 세계와 풍부하게 상호작용하고 필요할 때 즉흥적으로 새로운 손재주 행동을 만들 수 있는 일반적인 목적의 로봇을 위해 중요할 것입니다.</p>
<p>이 프로젝트에 관해 질문이 있거나 우리로부터 직접 듣고 싶으시다면, TRI LinkedIn 페이지에서 10월 4일 미동부로 1시부터 1시30분까지/태평양부로는 오전 10시부터 10시30분까지 LinkedIn Live Q&#x26;A 세션에 참가해 주세요. TRI의 LinkedIn 페이지에서 이벤트에 등록하세요.</p>
<h1>감사의 글</h1>
<div class="content-ad"></div>
<p>에릭 쿠지노, 나빈 쿠퍼스와미, 파루트 샤와 같은 우리의 다른 동료인 알렉스 알스팩, 라레스 암브루스, 맥스 바지라챠리야, 앤드류 보울리우, 아디티아 바트, 이샨 찬드라트레야, 청 치, 릭 코리, 샘 크리시, 홍카이 다이, 리처드 데니토, 잭 팡, 아드리앙 가이돈, 그랜트 굴드, 쿠니마쓰 하시모토, 브랜든 해서웨이, 앨리슨 헨리, 피비 호건, 제나 홀만, 유타로 이시다, 토마스 콜라, 데일 맥코낸키, 이안 맥마혼, 캘더 필립스-그래프린, 고든 리처드슨, 찰리 리히터, 타로 타카하시, 파벨 톡마코프, 제러드 윌슨, 트리스탄 휘팅, 블레이크 울프와 같은 컨트리뷰터들의 힘든 노력 없이는 이 작업이 가능하지 않았을 것입니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"TRI의 로봇들이 오후에 새로운 조작 기술을 익힌 방법을 소개합니다","description":"","date":"2024-06-20 17:53","slug":"2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow","content":"\n\n## 작성자: Siyuan Feng, Ben Burchfiel, Toffee Albina, and Russ Tedrake\n\n![이미지](/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_0.png)\n\nTRI는 로봇이 시연을 통해 새로운 미술적 행동을 습득할 수 있는 새로운 접근 방식을 공개했습니다. 이것이 왜 중요한 새로운 기능인지, 이것이 가능하게 된 진전은 무엇이며, 다음으로 어디를 향해 나아가는지 살펴보겠습니다.\n\n# 소개\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTRI의 로봇공학 연구 부문의 미션은 신기술을 개발하여 다음 세대의 민첩한 로봇이 사람들을 지원하고 삶의 질을 향상시키는 데 기여하는 것입니다. 이를 위해서는 로봇이 구조화되지 않은 자연 환경에서 사람들과 함께 작동해야 합니다. 환장한 분들을 도와 요리 준비를 하거나 다친 사람을 의자에서 일어나게 하거나 작업 현장에서 일하는 사람들을 지원하는 것이 있을 수 있습니다. 미래의 로봇들은 세계 어디에서나 사람들의 삶에 엄청난 긍정적인 영향을 미칠 것입니다.\n\n다음 세대의 로봇은 유연하고 적응력이 있어야 합니다. 두 집, 두 작업 현장, 두 사람이 서로 같지 않기 때문에, 우리의 로봇은 오늘날보다 훨씬 더 다재다능해져야 합니다. 현재 로봇들은 임무를 수행할 수 있도록 면밀히 프로그래밍되어 있으며, 사람들은 명백한 모서리 사례를 예상하고 로봇에게 실수 대처 방법을 가르쳐야 합니다. 이 접근 방식은 통제된 환경에서 막대한 성공을 거두었으며, 현대의 공장 및 창고 자동화의 기초가 됩니다. 그러나 이 방식으로 로봇을 만드는 것은 로봇의 환경을 주의 깊게 모델링하고, 로봇의 행동 범위를 주의 깊게 설정하고, 모든 상황을 미리 예견해야 합니다. 이것은 앞으로 야생에서 작동하는 더 능숙한 미래 로봇을 위해 필요한 복잡성으로 확장할 수 없습니다.\n\n그 결과로 로보틱스 분야에서 인공지능과 기계 학습을 사용하는 로봇에 대한 관심이 급증했습니다. 이러한 접근 방식 중 많은 것이 전통적인 로봇공학의 측면을 AI와 융합하여 효과적으로 사용하지만, 중요한 것이 빠졌습니다: 로봇은 아직 주변의 물리적 세계를 제어하는 데 재능이 없습니다. 이제 우리는 사람들과 대화할 수 있는 로봇을 가졌지만 과자 봉지를 열 수 없는 것이든 신발끈을 매 줄 수 없는 로봇이 있습니다. 로봇은 똑똑해지고 있지만 아직 간단한 방식으로 세상과 상호 작용할 뿐입니다.\n\n이를 해결하기 위해 TRI는 최근 최첨단 생성 AI의 진보를 바탕으로 로봇에게 단 하루 만에 새로운 조작 능력을 가르칠 수 있는 능력을 개발했습니다. 동일한 로봇, 동일한 코드 및 동일한 설정을 사용하여 우리는 야채 껍질 벗기기, 핸드 믹서 사용, 스낵 준비 및 팬케이크 뒤집기와 같은 60가지 다른 민첩한 행동을 가르쳤습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금은 오늘의 가능성을 끌어올리기만 하는 것이 아니라 유연하고 적응 가능한 손재주를 가진 일반 목적의 로봇의 기반을 마련하기 위해 다양한 행동들의 광범위한 커리큘럼을 개발 중입니다.\n\n이를 가능하게 하는 비하인드 스토리를 자세히 알려드리는 것에 기쁨을 느낍니다.\n\n# 가르치는 방법\n\n새로운 행동을 가르치기 위해 인간 작업자가 로봇을 원하는 작업을 시연하는 방식으로 원격으로 작동시킵니다. 보통 몇 시간의 가르침이 필요하며, 이로 인해 수십 개에서 수백 개의 시연이 이루어집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 학습 방식\n\n어떤 특정 행동에 대한 집합의 시연이 수집되면, 로봇은 해당 행동을 자율적으로 수행하도록 학습합니다. 우리 프로세스의 핵심은 최근 이미지 생성 분야를 뒤흔든 생성적 AI 기술인 디퓨전(Diffusion)입니다. 최근 TRI와 우리 대학 파트너들인 송 교수 연구실에서 이 기술을 채택하여 직접적으로 로봇 행동을 생성하는 디퓨전 정책(Diffusion Policy)이라는 방법을 개발했습니다. 자연어에 의존한 이미지 생성이 아닌 센서 관측 및 선택적으로 자연어에 의존한 상황에 따른 로봇 행동을 생성하기 위해 디퓨전을 사용합니다.\n\n![이미지](/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_1.png)\n\n로봇 행동을 생성하는 데 디퓨전을 사용하는 것은 이전 방법들보다 세 가지 주요 이점을 제공합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 멀티 모달 시연에 대한 적합성. 이것은 인간 시연자들이 행동을 자연스럽게 가르치고 로봇이 혼란스러워하지 않아도 된다는 것을 의미합니다.\n- 고차원 행동 공간에 적합성. 이것은 로봇이 미래를 예측하고 단시간 행동을 피함에 도움이 되는 것을 가능하게 합니다.\n- 안정적이고 신뢰할 수 있는 훈련. 이것은 귀찮은 손조절이나 황금자료점을 찾아다니지 않고도 규모에 맞게 로봇을 훈련하고 신뢰할 수 있다는 것을 의미합니다.\n\n## 멀티 모달 행동\n\n대부분의 현실 세계 작업은 여러 가지 다른 방법으로 해결할 수 있습니다. 예를 들어 컵을 들어올릴 때 사람은 위쪽, 옆면 또는 심지어 아래에서 잡을 수 있습니다. 이 같은 현상, 행동의 멀티모달성은 이전까지 행동 학습 방법이 적응하기 어려웠던 것으로, 정상적인 인간 행동에서 널리 퍼져 있음에도 불구하고 어려움을 겪었습니다.\n\n로봇이 테이블 위에 앉아 있는 T자 모양 블록을 목표 위치로 밀어야 하는 간단한 예시를 고려해 보겠습니다. 로봇은 블록을 미끄러뜨려 이동해야 하며 다양한 면에 도달하기 위해 블록 주위로 이동해야 합니다. 이 작업은 본질적으로 멀티모달성을 포함하고 있으며 블록을 좌측이나 우측으로 이동하는 것이 대개 합리적입니다. 두 가지 동등한 올바른 행동 모드가 있습니다. 이 해결책은 단일 행동을 예측하는 대신 행동 분포를 학습하는 것입니다. 확산 정책은 이러한 분포를 더 안정적이고 견고하게 배우며, 이전 방법보다 멀티모달성을 훨씬 잘 포착합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_2.png)\n\n복합 모달 데모를 다룰 수 있는 능력은 복잡한 재치 있는 행동을 가르치는 데 중요한 역할을 한 것으로 입증되었으며, 이러한 유형의 복합성은 보편적입니다. 또한 우리의 로봇이 데이터 수집을 확장하는 동안 여러 선생님으로부터 쉽게 배울 수 있도록 합니다.\n\n## 고차원 행동\n\n확산은 고차원 출력 공간에 맞게 자연스럽게 적합합니다. 예를 들어 이미지를 생성하는 데는 수십만 개의 개별 픽셀을 예측해야 합니다. 로봇 공학에서 이것은 주요한 장점이며, 복잡한 다리를 가진 로봇에 우아하게 확산 기반 행동 모델을 확장할 수 있게 합니다. 또한 한 번의 단계가 아닌 의도된 행동 경로를 예측하는 능력을 제공합니다. 최근 작업 'DP, ACT'는 경로 예측이 종종 성능이 우수한 견고한 정책을 학습하기 위한 중요한 설계 기능임을 보여 주었습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 안정적인 교육\n\n확산 정책은 교육을 진행하는 데 괴로워하지 않을 만큼 굉장히 간단합니다. 새로운 행동을 가르치는 데에 많은 비용이 들고 번거로운 실제 세계 평가를 할 필요 없이 최상의 실행 체크포인트와 하이퍼파라미터를 찾는 과정을 요구하지 않습니다. 컴퓨터 비전이나 자연 언어 적용과 달리, AI 기반의 닫힌 루프 시스템은 오프라인 지표로 정확하게 평가되지 않을 수 없습니다. 이 시스템들은 일반적으로 로봇 공학에서 실제 하드웨어에서 평가를 필요로 합니다. 이는 현실 세계 평가 병목 현상으로 인해 광범위한 조정이나 하이퍼파라미터 최적화를 필요로 하는 모든 학습 파이프라인이 실용성을 잃게 만듭니다. 이러한 이유로 확산 정책은 뛰어난 안정성을 바탕으로 작동하여 이 어려움을 우회할 수 있게 해줍니다. 이것이 바로 우리에게 규모의 핵심을 제공해준 핵심적인 요소입니다.\n\n# 우리의 플랫폼\n\n## 원격 작동\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로봇을 인간의 데모를 통해 가르치기 때문에, 어려운 미묘한 동작을 가르치는 데 중요한 것은 좋은 원격 조작(teleoperation) 인터페이스입니다. 저희의 로봇 학습 방법은 텔레오퍼레이션 장치의 선택에 중립적이며, 조이스틱과 같은 다양한 저가의 인터페이스를 사용해왔습니다. 보다 미묘한 동작을 위해 바이매뉼 햅틱(haptic) 장치를 사용하여 가르칩니다. 이때 텔레오퍼레이션 장치와 로봇 사이에 위치-위치 결합이 있습니다. 위치-위치 결합은 입력 장치가 측정된 자세를 명령으로 보내고 로봇이 이러한 자세 명령을 토크를 사용한 운동 공간 제어로 추적한다는 것을 의미합니다. 로봇의 자세 추적 오류를 힘이 변환한 후 이 힘을 선생님이 느낄 수 있도록 다시 입력 장치로 보냅니다. 이를 통해 선생님은 힘을 통해 로봇과의 피드백 루프를 닫을 수 있으며, 우리가 가르치는 가장 어려운 기술 중 많은 부분에 필수적입니다.\n\n양 팔을 사용해 물체를 조작할 때 힘 피드백은 특히 중요합니다. 이를 실험적으로 확인할 수 있는 사례는 수동 핸드믹서와 같은 작동을 필요로 하는 장치를 조작하는 것입니다. 이는 이 피드백 없이 신뢰할 수 없는 방식으로 데모할 수 없었습니다.\n\n로봇이 양 손으로 도구를 잡을 때, 닫힌 기구 체인이 형성됩니다. 로봇과 도구의 특정 설정에 대해 시각적으로 관측할 수 없는 다양한 내부 힘 범위가 있습니다. 그리퍼를 떼어내는 것과 같은 특정 힘 구성은 불안정하며, 로봇의 움직임이 미끄러질 가능성이 높아집니다. 휴먼 데모 사람들이 햅틱 피드백에 접근할 수 없는 경우, 적절한 힘 제어를 느끼거나 가르칠 수 없습니다. 우리는 힙틱 피드백이 양 팔의 결합이 필요한 굵직한 미적 행동을 가르칠 때 데모 성공률을 개선하는 데 중요하다는 것을 발견했습니다.\n\n## 촉각 감지\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n장갑을 끼고 신발을 묶으려고 한 적이 있는 사람은 손의 감각이 얼마나 중요한지를 경험했을 것입니다. 꼼꼼한 작업을 할 때, 무슨 일이 일어나고 있는지 느낄 수 있다는 것은 성공에 중요한 추가 정보를 제공합니다. 우리는 로봇도 이와 마찬가지로 촉감을 갖는 것이 도움이 될 것이라고 믿습니다. 그래서 우리는 많은 플랫폼에서 TRI Soft-Bubble 센서를 사용하고 있습니다. 이러한 센서는 내부 카메라가 부풀어 오른 변형 가능한 외부 막을 관찰하는 것으로 구성되어 있습니다. 이러한 센서들은 희박한 힘 신호를 측정하는 것을 넘어 로봇이 접촉 패턴, 기하학, 미끄러짐 및 힘에 관한 공간적으로 밀집된 정보를 인식할 수 있도록 합니다.\n\n이 유형의 센서는 최근 몇 년간 보다 인기가 많아졌지만, 제공하는 정보를 잘 활용하는 것은 어려운 과제였습니다. 확산은 로봇이 이 시각-촉감 센서가 제공하는 풍부한 정보를 사용하는 자연스러운 방법을 제공합니다. 이러한 신호에 대해 조건부로 - 추가 입력으로 사용하는 것으로 - 우리는 이를 임의의 꼼꼼한 작업에 적용할 수 있도록 합니다.\n\n이 방향으로 진행한 초기 실험은 매우 유망합니다. 많은 경우에 접촉 감지를 추가하면 로봇의 재미있는 접촉 단계를 포함한 작업 수행 능력이 크게 향상됨을 발견하고 있습니다 😊\n\n\u003cimg src=\"/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_3.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 안전하고 성능이 우수한 제어\n\n고성능 로봇의 중간 제어 계층은 중대한 역할을 하며 종종 무시되는 부분입니다. 우리의 경우, 학습된 정책과 인간 데모 사용자는 10Hz로 로봇의 그리퍼에 대한 위치 및 방향 명령을 내립니다. 이러한 명령들은 중간 제어자에 의해 1kHz의 관절 토크 명령으로 업샘플링되고 변환됩니다. 중요한 점은 이 중간 제어자에는 로봇을 안전하게 보호하고 학습된 정책이나 인간 교사로부터 잠재적으로 위험한 상위 명령을 실행하지 못하도록 하는 안전 기능이 내장되어 있다는 것입니다.\n\n저희 방식은 작업 공간 제어에 근거하며 관절 레벨 명령에 대한 제약 조건이 있는 최적화 문제로 정의됩니다. 목표는 데모 사용자나 학습된 정책이 제공하는 고수준 명령을 따르면서 물리 및 충돌 회피와 같은 기타 안전 제약 조건을 준수하는 것입니다. 이 구현은 Drake Systems Framework를 활용하며 엄격한 분석과 테스트를 가능케 하여 시스템의 이 중요한 부분에 대해 확신할 수 있게 해줍니다. 우리는 이 구현을 향후 오픈 소스로 공개할 계획이며 이는 우리 시스템의 주요 성공 요소 중 하나로 여기고 있습니다.\n\n튼튼한 중간 제어자는 고품질의 행동 학습 파이프라인의 기초입니다. 이는 임피던스 제어 및 햅틱 피드백과 같은 임무 중요한 기능을 가능하게 하며 전반적인 시스템에 보다 훌륭한 안전 보호 기능을 제공하여 교사가 로봇을 물리적 한계까지 밀어낼 수 있도록 도와줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 현재 어디에 있나요\n\n로봇공학의 놀라운 새로운 시대에 들어서고 있습니다. 전문 로봇 공학자들이 몇 주의 시간을 투자해야 했던 일들을 누구나 이제 하루 만에 가르치고 성공을 기대할 수 있습니다. 이제는 조정 없이 작동하는 단일 파이프라인을 통해 복잡한 상호 작용을 가진 민첩한 행동들을 가르치는 것이 가능해졌습니다.\n\n더 많이 해야 할 일이 남아 있습니다. 현재 우리가 로봇에 새로운 기술을 가르칠 때, 그것은 부서지기 쉽습니다. 기술은 가르치는 데 사용된 상황과 유사한 상황에서는 잘 작동할 것이지만, 차이가 있는 상황에서 로봇은 고전할 것입니다. 실전에서 우리가 관찰하는 실패 케이스의 가장 흔한 원인은 다음과 같습니다:\n\n- 회복이 증명되지 않은 상태. 이는 너무 깨끗한 데모데이터로 인한 결과일 수 있습니다.\n- 카메라 시점 또는 배경의 중요한 변화.\n- 테스트 시간에 훈련 중 만나지 못한 피조작물.\n- 방해물, 예를 들어 훈련 중에 없던 중요한 혼잡.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼 해결책은 무엇일까요? 사람이 평생 경험을 토대로 배우듯이 점차 능력과 적응력이 향상되는 것처럼, 우리는 다양한 행동 교육과정이 유연하고 일반적인 로봇을 만드는 열쇠라고 믿습니다. 이를 위해 TRI는 신체 데이터에 기초한 강력한 행동 교육과정을 실제 로봇 그룹과 강력한 드레이크 시뮬레이션 스위트 모두에 투자하고 있습니다.\n\n지금까지 재치있는 조작 로봇 그룹에 60가지 이상의 행동을 가르쳤으며 올해 말까지 200가지 이상의 행동을 목표로 한 노력의 속도를 증가하고 있습니다. 이러한 행동은 도구 사용부터 가소성 물체 조작, 조심스러운 양손 조율까지 다양한 조작 시나리오에 걸쳐 있습니다.\n\n# 다음은\n\n다양한 기술 데이터셋에서 정책을 학습할 때 초기 성공의 조짐을 보았습니다. 한 예로, 상당히 정리된 장면에서 로봇에게 얼음이 담긴 머그잔을 싱크대로 비우도록 가르쳤습니다. 매우 혼잡한 상황에서 평가할 때, 이 데이터만 가지고 훈련된 정책은 거의 즉시 실패했습니다. 우리는 이 기술의 두 번째 버전을 얼음을 따르는 다른 15가지 작업과 함께 공동으로 가르쳤고, 로봇에게 원하는 행동에 대한 언어 설명을 조건으로 두었습니다. 어지럽힘에서 얼음을 붓는 동일한 시연자에게 접근할 수 있음에도 불구하고, 다중 작업 기술은 단일 작업 버전이 재앙적으로 실패한 매우 혼잡한 상황에서 성공했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 우리는 실제 로봇 데이터만으로는 진정으로 일반적인 손재주 로봇을 만들기에 충분하지 않을 것으로 예상합니다. 이 빈 공간을 채우기 위해, 우리는 Drake를 사용하여 강력한 시뮬레이션 전문 기술에 크게 의존하고 있습니다. Drake는 단단한 물체와 부드러운 물체 간 상세한 물리 상호작용을 정확하게 모델링할 수 있는 능력이 독특하며, 정교하고 복잡한 행동에 중요합니다.\n\n이 노력이 진행됨에 따라 성공의 확실한 징후 중 하나는 영사적으로 능숙한 행동 생성과 맥락 속 학습일 것입니다. 기존의 대형 언어 모델은 새로운 방식으로 개념을 구성하고 단일 예제에서 배울 수 있는 강력한 능력을 갖고 있습니다. 지난 한 해 동안 우리는 로봇이 의미론적으로 일반화하도록 함으로써 이를 가능케 하여 신규 물체로 채집 및 배치하는 것을 보았습니다. 다음 큰 이정표는 이 의미론적 능력을 높은 수준의 물리 지능과 창의성과 융합하는 동등한 강력한 대규모 행동 모델을 만드는 것입니다. 이러한 모델은 주변 세계와 풍부하게 상호작용하고 필요할 때 즉흥적으로 새로운 손재주 행동을 만들 수 있는 일반적인 목적의 로봇을 위해 중요할 것입니다.\n\n이 프로젝트에 관해 질문이 있거나 우리로부터 직접 듣고 싶으시다면, TRI LinkedIn 페이지에서 10월 4일 미동부로 1시부터 1시30분까지/태평양부로는 오전 10시부터 10시30분까지 LinkedIn Live Q\u0026A 세션에 참가해 주세요. TRI의 LinkedIn 페이지에서 이벤트에 등록하세요.\n\n# 감사의 글\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n에릭 쿠지노, 나빈 쿠퍼스와미, 파루트 샤와 같은 우리의 다른 동료인 알렉스 알스팩, 라레스 암브루스, 맥스 바지라챠리야, 앤드류 보울리우, 아디티아 바트, 이샨 찬드라트레야, 청 치, 릭 코리, 샘 크리시, 홍카이 다이, 리처드 데니토, 잭 팡, 아드리앙 가이돈, 그랜트 굴드, 쿠니마쓰 하시모토, 브랜든 해서웨이, 앨리슨 헨리, 피비 호건, 제나 홀만, 유타로 이시다, 토마스 콜라, 데일 맥코낸키, 이안 맥마혼, 캘더 필립스-그래프린, 고든 리처드슨, 찰리 리히터, 타로 타카하시, 파벨 톡마코프, 제러드 윌슨, 트리스탄 휘팅, 블레이크 울프와 같은 컨트리뷰터들의 힘든 노력 없이는 이 작업이 가능하지 않았을 것입니다.","ogImage":{"url":"/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_0.png"},"coverImage":"/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_0.png","tag":["Tech"],"readingTime":9},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e작성자: Siyuan Feng, Ben Burchfiel, Toffee Albina, and Russ Tedrake\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eTRI는 로봇이 시연을 통해 새로운 미술적 행동을 습득할 수 있는 새로운 접근 방식을 공개했습니다. 이것이 왜 중요한 새로운 기능인지, 이것이 가능하게 된 진전은 무엇이며, 다음으로 어디를 향해 나아가는지 살펴보겠습니다.\u003c/p\u003e\n\u003ch1\u003e소개\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eTRI의 로봇공학 연구 부문의 미션은 신기술을 개발하여 다음 세대의 민첩한 로봇이 사람들을 지원하고 삶의 질을 향상시키는 데 기여하는 것입니다. 이를 위해서는 로봇이 구조화되지 않은 자연 환경에서 사람들과 함께 작동해야 합니다. 환장한 분들을 도와 요리 준비를 하거나 다친 사람을 의자에서 일어나게 하거나 작업 현장에서 일하는 사람들을 지원하는 것이 있을 수 있습니다. 미래의 로봇들은 세계 어디에서나 사람들의 삶에 엄청난 긍정적인 영향을 미칠 것입니다.\u003c/p\u003e\n\u003cp\u003e다음 세대의 로봇은 유연하고 적응력이 있어야 합니다. 두 집, 두 작업 현장, 두 사람이 서로 같지 않기 때문에, 우리의 로봇은 오늘날보다 훨씬 더 다재다능해져야 합니다. 현재 로봇들은 임무를 수행할 수 있도록 면밀히 프로그래밍되어 있으며, 사람들은 명백한 모서리 사례를 예상하고 로봇에게 실수 대처 방법을 가르쳐야 합니다. 이 접근 방식은 통제된 환경에서 막대한 성공을 거두었으며, 현대의 공장 및 창고 자동화의 기초가 됩니다. 그러나 이 방식으로 로봇을 만드는 것은 로봇의 환경을 주의 깊게 모델링하고, 로봇의 행동 범위를 주의 깊게 설정하고, 모든 상황을 미리 예견해야 합니다. 이것은 앞으로 야생에서 작동하는 더 능숙한 미래 로봇을 위해 필요한 복잡성으로 확장할 수 없습니다.\u003c/p\u003e\n\u003cp\u003e그 결과로 로보틱스 분야에서 인공지능과 기계 학습을 사용하는 로봇에 대한 관심이 급증했습니다. 이러한 접근 방식 중 많은 것이 전통적인 로봇공학의 측면을 AI와 융합하여 효과적으로 사용하지만, 중요한 것이 빠졌습니다: 로봇은 아직 주변의 물리적 세계를 제어하는 데 재능이 없습니다. 이제 우리는 사람들과 대화할 수 있는 로봇을 가졌지만 과자 봉지를 열 수 없는 것이든 신발끈을 매 줄 수 없는 로봇이 있습니다. 로봇은 똑똑해지고 있지만 아직 간단한 방식으로 세상과 상호 작용할 뿐입니다.\u003c/p\u003e\n\u003cp\u003e이를 해결하기 위해 TRI는 최근 최첨단 생성 AI의 진보를 바탕으로 로봇에게 단 하루 만에 새로운 조작 능력을 가르칠 수 있는 능력을 개발했습니다. 동일한 로봇, 동일한 코드 및 동일한 설정을 사용하여 우리는 야채 껍질 벗기기, 핸드 믹서 사용, 스낵 준비 및 팬케이크 뒤집기와 같은 60가지 다른 민첩한 행동을 가르쳤습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e지금은 오늘의 가능성을 끌어올리기만 하는 것이 아니라 유연하고 적응 가능한 손재주를 가진 일반 목적의 로봇의 기반을 마련하기 위해 다양한 행동들의 광범위한 커리큘럼을 개발 중입니다.\u003c/p\u003e\n\u003cp\u003e이를 가능하게 하는 비하인드 스토리를 자세히 알려드리는 것에 기쁨을 느낍니다.\u003c/p\u003e\n\u003ch1\u003e가르치는 방법\u003c/h1\u003e\n\u003cp\u003e새로운 행동을 가르치기 위해 인간 작업자가 로봇을 원하는 작업을 시연하는 방식으로 원격으로 작동시킵니다. 보통 몇 시간의 가르침이 필요하며, 이로 인해 수십 개에서 수백 개의 시연이 이루어집니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e학습 방식\u003c/h1\u003e\n\u003cp\u003e어떤 특정 행동에 대한 집합의 시연이 수집되면, 로봇은 해당 행동을 자율적으로 수행하도록 학습합니다. 우리 프로세스의 핵심은 최근 이미지 생성 분야를 뒤흔든 생성적 AI 기술인 디퓨전(Diffusion)입니다. 최근 TRI와 우리 대학 파트너들인 송 교수 연구실에서 이 기술을 채택하여 직접적으로 로봇 행동을 생성하는 디퓨전 정책(Diffusion Policy)이라는 방법을 개발했습니다. 자연어에 의존한 이미지 생성이 아닌 센서 관측 및 선택적으로 자연어에 의존한 상황에 따른 로봇 행동을 생성하기 위해 디퓨전을 사용합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e로봇 행동을 생성하는 데 디퓨전을 사용하는 것은 이전 방법들보다 세 가지 주요 이점을 제공합니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e멀티 모달 시연에 대한 적합성. 이것은 인간 시연자들이 행동을 자연스럽게 가르치고 로봇이 혼란스러워하지 않아도 된다는 것을 의미합니다.\u003c/li\u003e\n\u003cli\u003e고차원 행동 공간에 적합성. 이것은 로봇이 미래를 예측하고 단시간 행동을 피함에 도움이 되는 것을 가능하게 합니다.\u003c/li\u003e\n\u003cli\u003e안정적이고 신뢰할 수 있는 훈련. 이것은 귀찮은 손조절이나 황금자료점을 찾아다니지 않고도 규모에 맞게 로봇을 훈련하고 신뢰할 수 있다는 것을 의미합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e멀티 모달 행동\u003c/h2\u003e\n\u003cp\u003e대부분의 현실 세계 작업은 여러 가지 다른 방법으로 해결할 수 있습니다. 예를 들어 컵을 들어올릴 때 사람은 위쪽, 옆면 또는 심지어 아래에서 잡을 수 있습니다. 이 같은 현상, 행동의 멀티모달성은 이전까지 행동 학습 방법이 적응하기 어려웠던 것으로, 정상적인 인간 행동에서 널리 퍼져 있음에도 불구하고 어려움을 겪었습니다.\u003c/p\u003e\n\u003cp\u003e로봇이 테이블 위에 앉아 있는 T자 모양 블록을 목표 위치로 밀어야 하는 간단한 예시를 고려해 보겠습니다. 로봇은 블록을 미끄러뜨려 이동해야 하며 다양한 면에 도달하기 위해 블록 주위로 이동해야 합니다. 이 작업은 본질적으로 멀티모달성을 포함하고 있으며 블록을 좌측이나 우측으로 이동하는 것이 대개 합리적입니다. 두 가지 동등한 올바른 행동 모드가 있습니다. 이 해결책은 단일 행동을 예측하는 대신 행동 분포를 학습하는 것입니다. 확산 정책은 이러한 분포를 더 안정적이고 견고하게 배우며, 이전 방법보다 멀티모달성을 훨씬 잘 포착합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_2.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e복합 모달 데모를 다룰 수 있는 능력은 복잡한 재치 있는 행동을 가르치는 데 중요한 역할을 한 것으로 입증되었으며, 이러한 유형의 복합성은 보편적입니다. 또한 우리의 로봇이 데이터 수집을 확장하는 동안 여러 선생님으로부터 쉽게 배울 수 있도록 합니다.\u003c/p\u003e\n\u003ch2\u003e고차원 행동\u003c/h2\u003e\n\u003cp\u003e확산은 고차원 출력 공간에 맞게 자연스럽게 적합합니다. 예를 들어 이미지를 생성하는 데는 수십만 개의 개별 픽셀을 예측해야 합니다. 로봇 공학에서 이것은 주요한 장점이며, 복잡한 다리를 가진 로봇에 우아하게 확산 기반 행동 모델을 확장할 수 있게 합니다. 또한 한 번의 단계가 아닌 의도된 행동 경로를 예측하는 능력을 제공합니다. 최근 작업 'DP, ACT'는 경로 예측이 종종 성능이 우수한 견고한 정책을 학습하기 위한 중요한 설계 기능임을 보여 주었습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e안정적인 교육\u003c/h2\u003e\n\u003cp\u003e확산 정책은 교육을 진행하는 데 괴로워하지 않을 만큼 굉장히 간단합니다. 새로운 행동을 가르치는 데에 많은 비용이 들고 번거로운 실제 세계 평가를 할 필요 없이 최상의 실행 체크포인트와 하이퍼파라미터를 찾는 과정을 요구하지 않습니다. 컴퓨터 비전이나 자연 언어 적용과 달리, AI 기반의 닫힌 루프 시스템은 오프라인 지표로 정확하게 평가되지 않을 수 없습니다. 이 시스템들은 일반적으로 로봇 공학에서 실제 하드웨어에서 평가를 필요로 합니다. 이는 현실 세계 평가 병목 현상으로 인해 광범위한 조정이나 하이퍼파라미터 최적화를 필요로 하는 모든 학습 파이프라인이 실용성을 잃게 만듭니다. 이러한 이유로 확산 정책은 뛰어난 안정성을 바탕으로 작동하여 이 어려움을 우회할 수 있게 해줍니다. 이것이 바로 우리에게 규모의 핵심을 제공해준 핵심적인 요소입니다.\u003c/p\u003e\n\u003ch1\u003e우리의 플랫폼\u003c/h1\u003e\n\u003ch2\u003e원격 작동\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e로봇을 인간의 데모를 통해 가르치기 때문에, 어려운 미묘한 동작을 가르치는 데 중요한 것은 좋은 원격 조작(teleoperation) 인터페이스입니다. 저희의 로봇 학습 방법은 텔레오퍼레이션 장치의 선택에 중립적이며, 조이스틱과 같은 다양한 저가의 인터페이스를 사용해왔습니다. 보다 미묘한 동작을 위해 바이매뉼 햅틱(haptic) 장치를 사용하여 가르칩니다. 이때 텔레오퍼레이션 장치와 로봇 사이에 위치-위치 결합이 있습니다. 위치-위치 결합은 입력 장치가 측정된 자세를 명령으로 보내고 로봇이 이러한 자세 명령을 토크를 사용한 운동 공간 제어로 추적한다는 것을 의미합니다. 로봇의 자세 추적 오류를 힘이 변환한 후 이 힘을 선생님이 느낄 수 있도록 다시 입력 장치로 보냅니다. 이를 통해 선생님은 힘을 통해 로봇과의 피드백 루프를 닫을 수 있으며, 우리가 가르치는 가장 어려운 기술 중 많은 부분에 필수적입니다.\u003c/p\u003e\n\u003cp\u003e양 팔을 사용해 물체를 조작할 때 힘 피드백은 특히 중요합니다. 이를 실험적으로 확인할 수 있는 사례는 수동 핸드믹서와 같은 작동을 필요로 하는 장치를 조작하는 것입니다. 이는 이 피드백 없이 신뢰할 수 없는 방식으로 데모할 수 없었습니다.\u003c/p\u003e\n\u003cp\u003e로봇이 양 손으로 도구를 잡을 때, 닫힌 기구 체인이 형성됩니다. 로봇과 도구의 특정 설정에 대해 시각적으로 관측할 수 없는 다양한 내부 힘 범위가 있습니다. 그리퍼를 떼어내는 것과 같은 특정 힘 구성은 불안정하며, 로봇의 움직임이 미끄러질 가능성이 높아집니다. 휴먼 데모 사람들이 햅틱 피드백에 접근할 수 없는 경우, 적절한 힘 제어를 느끼거나 가르칠 수 없습니다. 우리는 힙틱 피드백이 양 팔의 결합이 필요한 굵직한 미적 행동을 가르칠 때 데모 성공률을 개선하는 데 중요하다는 것을 발견했습니다.\u003c/p\u003e\n\u003ch2\u003e촉각 감지\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e장갑을 끼고 신발을 묶으려고 한 적이 있는 사람은 손의 감각이 얼마나 중요한지를 경험했을 것입니다. 꼼꼼한 작업을 할 때, 무슨 일이 일어나고 있는지 느낄 수 있다는 것은 성공에 중요한 추가 정보를 제공합니다. 우리는 로봇도 이와 마찬가지로 촉감을 갖는 것이 도움이 될 것이라고 믿습니다. 그래서 우리는 많은 플랫폼에서 TRI Soft-Bubble 센서를 사용하고 있습니다. 이러한 센서는 내부 카메라가 부풀어 오른 변형 가능한 외부 막을 관찰하는 것으로 구성되어 있습니다. 이러한 센서들은 희박한 힘 신호를 측정하는 것을 넘어 로봇이 접촉 패턴, 기하학, 미끄러짐 및 힘에 관한 공간적으로 밀집된 정보를 인식할 수 있도록 합니다.\u003c/p\u003e\n\u003cp\u003e이 유형의 센서는 최근 몇 년간 보다 인기가 많아졌지만, 제공하는 정보를 잘 활용하는 것은 어려운 과제였습니다. 확산은 로봇이 이 시각-촉감 센서가 제공하는 풍부한 정보를 사용하는 자연스러운 방법을 제공합니다. 이러한 신호에 대해 조건부로 - 추가 입력으로 사용하는 것으로 - 우리는 이를 임의의 꼼꼼한 작업에 적용할 수 있도록 합니다.\u003c/p\u003e\n\u003cp\u003e이 방향으로 진행한 초기 실험은 매우 유망합니다. 많은 경우에 접촉 감지를 추가하면 로봇의 재미있는 접촉 단계를 포함한 작업 수행 능력이 크게 향상됨을 발견하고 있습니다 😊\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow_3.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e안전하고 성능이 우수한 제어\u003c/h2\u003e\n\u003cp\u003e고성능 로봇의 중간 제어 계층은 중대한 역할을 하며 종종 무시되는 부분입니다. 우리의 경우, 학습된 정책과 인간 데모 사용자는 10Hz로 로봇의 그리퍼에 대한 위치 및 방향 명령을 내립니다. 이러한 명령들은 중간 제어자에 의해 1kHz의 관절 토크 명령으로 업샘플링되고 변환됩니다. 중요한 점은 이 중간 제어자에는 로봇을 안전하게 보호하고 학습된 정책이나 인간 교사로부터 잠재적으로 위험한 상위 명령을 실행하지 못하도록 하는 안전 기능이 내장되어 있다는 것입니다.\u003c/p\u003e\n\u003cp\u003e저희 방식은 작업 공간 제어에 근거하며 관절 레벨 명령에 대한 제약 조건이 있는 최적화 문제로 정의됩니다. 목표는 데모 사용자나 학습된 정책이 제공하는 고수준 명령을 따르면서 물리 및 충돌 회피와 같은 기타 안전 제약 조건을 준수하는 것입니다. 이 구현은 Drake Systems Framework를 활용하며 엄격한 분석과 테스트를 가능케 하여 시스템의 이 중요한 부분에 대해 확신할 수 있게 해줍니다. 우리는 이 구현을 향후 오픈 소스로 공개할 계획이며 이는 우리 시스템의 주요 성공 요소 중 하나로 여기고 있습니다.\u003c/p\u003e\n\u003cp\u003e튼튼한 중간 제어자는 고품질의 행동 학습 파이프라인의 기초입니다. 이는 임피던스 제어 및 햅틱 피드백과 같은 임무 중요한 기능을 가능하게 하며 전반적인 시스템에 보다 훌륭한 안전 보호 기능을 제공하여 교사가 로봇을 물리적 한계까지 밀어낼 수 있도록 도와줍니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e현재 어디에 있나요\u003c/h1\u003e\n\u003cp\u003e로봇공학의 놀라운 새로운 시대에 들어서고 있습니다. 전문 로봇 공학자들이 몇 주의 시간을 투자해야 했던 일들을 누구나 이제 하루 만에 가르치고 성공을 기대할 수 있습니다. 이제는 조정 없이 작동하는 단일 파이프라인을 통해 복잡한 상호 작용을 가진 민첩한 행동들을 가르치는 것이 가능해졌습니다.\u003c/p\u003e\n\u003cp\u003e더 많이 해야 할 일이 남아 있습니다. 현재 우리가 로봇에 새로운 기술을 가르칠 때, 그것은 부서지기 쉽습니다. 기술은 가르치는 데 사용된 상황과 유사한 상황에서는 잘 작동할 것이지만, 차이가 있는 상황에서 로봇은 고전할 것입니다. 실전에서 우리가 관찰하는 실패 케이스의 가장 흔한 원인은 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e회복이 증명되지 않은 상태. 이는 너무 깨끗한 데모데이터로 인한 결과일 수 있습니다.\u003c/li\u003e\n\u003cli\u003e카메라 시점 또는 배경의 중요한 변화.\u003c/li\u003e\n\u003cli\u003e테스트 시간에 훈련 중 만나지 못한 피조작물.\u003c/li\u003e\n\u003cli\u003e방해물, 예를 들어 훈련 중에 없던 중요한 혼잡.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그럼 해결책은 무엇일까요? 사람이 평생 경험을 토대로 배우듯이 점차 능력과 적응력이 향상되는 것처럼, 우리는 다양한 행동 교육과정이 유연하고 일반적인 로봇을 만드는 열쇠라고 믿습니다. 이를 위해 TRI는 신체 데이터에 기초한 강력한 행동 교육과정을 실제 로봇 그룹과 강력한 드레이크 시뮬레이션 스위트 모두에 투자하고 있습니다.\u003c/p\u003e\n\u003cp\u003e지금까지 재치있는 조작 로봇 그룹에 60가지 이상의 행동을 가르쳤으며 올해 말까지 200가지 이상의 행동을 목표로 한 노력의 속도를 증가하고 있습니다. 이러한 행동은 도구 사용부터 가소성 물체 조작, 조심스러운 양손 조율까지 다양한 조작 시나리오에 걸쳐 있습니다.\u003c/p\u003e\n\u003ch1\u003e다음은\u003c/h1\u003e\n\u003cp\u003e다양한 기술 데이터셋에서 정책을 학습할 때 초기 성공의 조짐을 보았습니다. 한 예로, 상당히 정리된 장면에서 로봇에게 얼음이 담긴 머그잔을 싱크대로 비우도록 가르쳤습니다. 매우 혼잡한 상황에서 평가할 때, 이 데이터만 가지고 훈련된 정책은 거의 즉시 실패했습니다. 우리는 이 기술의 두 번째 버전을 얼음을 따르는 다른 15가지 작업과 함께 공동으로 가르쳤고, 로봇에게 원하는 행동에 대한 언어 설명을 조건으로 두었습니다. 어지럽힘에서 얼음을 붓는 동일한 시연자에게 접근할 수 있음에도 불구하고, 다중 작업 기술은 단일 작업 버전이 재앙적으로 실패한 매우 혼잡한 상황에서 성공했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그러나 우리는 실제 로봇 데이터만으로는 진정으로 일반적인 손재주 로봇을 만들기에 충분하지 않을 것으로 예상합니다. 이 빈 공간을 채우기 위해, 우리는 Drake를 사용하여 강력한 시뮬레이션 전문 기술에 크게 의존하고 있습니다. Drake는 단단한 물체와 부드러운 물체 간 상세한 물리 상호작용을 정확하게 모델링할 수 있는 능력이 독특하며, 정교하고 복잡한 행동에 중요합니다.\u003c/p\u003e\n\u003cp\u003e이 노력이 진행됨에 따라 성공의 확실한 징후 중 하나는 영사적으로 능숙한 행동 생성과 맥락 속 학습일 것입니다. 기존의 대형 언어 모델은 새로운 방식으로 개념을 구성하고 단일 예제에서 배울 수 있는 강력한 능력을 갖고 있습니다. 지난 한 해 동안 우리는 로봇이 의미론적으로 일반화하도록 함으로써 이를 가능케 하여 신규 물체로 채집 및 배치하는 것을 보았습니다. 다음 큰 이정표는 이 의미론적 능력을 높은 수준의 물리 지능과 창의성과 융합하는 동등한 강력한 대규모 행동 모델을 만드는 것입니다. 이러한 모델은 주변 세계와 풍부하게 상호작용하고 필요할 때 즉흥적으로 새로운 손재주 행동을 만들 수 있는 일반적인 목적의 로봇을 위해 중요할 것입니다.\u003c/p\u003e\n\u003cp\u003e이 프로젝트에 관해 질문이 있거나 우리로부터 직접 듣고 싶으시다면, TRI LinkedIn 페이지에서 10월 4일 미동부로 1시부터 1시30분까지/태평양부로는 오전 10시부터 10시30분까지 LinkedIn Live Q\u0026#x26;A 세션에 참가해 주세요. TRI의 LinkedIn 페이지에서 이벤트에 등록하세요.\u003c/p\u003e\n\u003ch1\u003e감사의 글\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e에릭 쿠지노, 나빈 쿠퍼스와미, 파루트 샤와 같은 우리의 다른 동료인 알렉스 알스팩, 라레스 암브루스, 맥스 바지라챠리야, 앤드류 보울리우, 아디티아 바트, 이샨 찬드라트레야, 청 치, 릭 코리, 샘 크리시, 홍카이 다이, 리처드 데니토, 잭 팡, 아드리앙 가이돈, 그랜트 굴드, 쿠니마쓰 하시모토, 브랜든 해서웨이, 앨리슨 헨리, 피비 호건, 제나 홀만, 유타로 이시다, 토마스 콜라, 데일 맥코낸키, 이안 맥마혼, 캘더 필립스-그래프린, 고든 리처드슨, 찰리 리히터, 타로 타카하시, 파벨 톡마코프, 제러드 윌슨, 트리스탄 휘팅, 블레이크 울프와 같은 컨트리뷰터들의 힘든 노력 없이는 이 작업이 가능하지 않았을 것입니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-20-TRIsRobotsLearnNewManipulationSkillsinanAfternoonHeresHow"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>