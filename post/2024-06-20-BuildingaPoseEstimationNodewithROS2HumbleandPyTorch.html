<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기 | itposting" data-gatsby-head="true"/><meta property="og:title" content="ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch" data-gatsby-head="true"/><meta name="twitter:title" content="ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-20 17:48" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 20, 2024</span><span class="posts_reading_time__f7YPP">4<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<img src="/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png">
<p>이 튜토리얼에서는 PyTorch에서 미리 학습된 딥 러닝 모델을 사용하여 실시간 인간 포즈 추정을 위한 ROS 2 노드를 만들겠습니다. 이 노드는 웹캠 이미지를 구독하고 포즈 추정을 수행한 뒤 주석이 달린 이미지를 발행할 것입니다. 구현 세부 내용으로 들어가 봅시다.</p>
<h2>요구 사항</h2>
<p>시작하기 전에 다음이 설치되어 있는지 확인하세요:</p>
<div class="content-ad"></div>
<ul>
<li>ROS 2</li>
<li>Python 3</li>
<li>PyTorch</li>
<li>torchvision</li>
<li>OpenCV</li>
<li>cv_bridge (ROS 패키지로 ROS와 OpenCV 이미지 간 변환을 위한 것)</li>
</ul>
<p>모델: keypointrcnn_resnet50_fpn</p>
<p>우리는 torchvision의 keypointrcnn_resnet50_fpn 모델을 사용합니다. 이 모델은 사람 자세 추정을 위해 설계되어 여러 신체 부위의 키포인트를 예측합니다. 여기에 이 모델의 구성 요소가 있습니다:</p>
<ul>
<li>ResNet-50 백본: ResNet-50는 특징 추출기로 작용하는 합성곱 신경망입니다. 공간적 계층을 효과적으로 캡처하는 데 알려져 있습니다.</li>
<li>FPN (Feature Pyramid Network): FPN은 다중 스케일에서 특성 맵을 구축하여 감지 능력을 향상시킵니다.</li>
<li>Keypoint R-CNN: 이 Faster R-CNN의 변형은 바운딩 박스 외에도 키포인트를 감지하는 데 특화되어 있습니다.</li>
</ul>
<div class="content-ad"></div>
<img src="https://miro.medium.com/v2/resize:fit:1400/1*LbBdaJJRpnNGRLdKExdzdw.gif">
<p>자율 이동 로봇(AMR) 및 로보틱스에서 실시간 포즈 추정의 응용</p>
<p>포즈 추정은 로보틱스 분야에서 강력한 도구이며 자율 이동 로봇(AMR)의 성능을 크게 향상시킬 수 있습니다. 실시간 포즈 추정을 통합함으로써 로봇은 상황 인식, 인간과의 상호 작용, 다양한 작업에서의 성능을 향상시킬 수 있습니다. 다음은 주요 응용 분야 몇 가지입니다:</p>
<ul>
<li>협업 로봇 (Cobots)</li>
<li>감시 및 보안</li>
<li>제조 및 조립 라인</li>
<li>내비게이션 및 장애물 회피</li>
</ul>
<div class="content-ad"></div>
<p>단계별 실행</p>
<ul>
<li>노드 초기화 및 구독</li>
</ul>
<p>우리는 필요한 라이브러리를 가져오는 것으로 시작합니다. 이에는 ROS 2 Python 클라이언트 라이브러리 (rclpy), ROS 메시지 종류 (Image), 이미지 변환을 위한 CvBridge, 그리고 딥러닝을 위한 PyTorch 및 torchvision이 포함됩니다.</p>
<ol start="2">
<li>노드 클래스 생성</li>
</ol>
<div class="content-ad"></div>
<p>PoseEstimationNode 클래스를 정의하고 Node를 상속합니다. 생성자에서는:</p>
<ul>
<li>노드를 pose_estimation_node으로 이름을 지정하여 초기화합니다.</li>
<li>이미지를 수신하기 위해 /jetson_webcam 토픽에 구독합니다.</li>
<li>이미지_pose 토픽에 주석 처리된 이미지를 게시할 발행자를 만듭니다.</li>
<li>ROS 및 OpenCV 이미지 간 변환을 위해 CvBridge를 초기화합니다.</li>
<li>torchvision에서 사전 학습된 자세 추정 모델 keypointrcnn_resnet50_fpn을 평가 모드로 설정하여 로드합니다.</li>
<li>이미지를 텐서로 변환하기 위한 변환을 정의합니다.</li>
</ul>
<ol start="3">
<li>수신된 이미지 처리</li>
</ol>
<p>listener_callback 메서드에서 수신된 이미지를 처리합니다.</p>
<div class="content-ad"></div>
<ul>
<li>받은 이미지의 인코딩을 기록합니다.</li>
<li>CvBridge를 사용하여 이미지를 ROS 형식에서 OpenCV 형식으로 변환하고 다양한 이미지 인코딩을 처리합니다.</li>
</ul>
<ol start="4">
<li>포즈 추정</li>
</ol>
<p>다음으로, OpenCV 이미지를 PIL 이미지로 변환하고 텐서로 변환하기 위한 변환이 적용됩니다. 이 텐서를 모델에 전달하여 예측을 얻고, torch.no_grad()를 사용하여 기울기 계산이 이루어지지 않도록 합니다.</p>
<ol start="5">
<li>이미지 주석 및 게시</li>
</ol>
<div class="content-ad"></div>
<p>그런 다음 이미지에서 주요 지점 위치에 원을 그립니다. 이러한 주요 지점은 모델의 예측에서 추출되어 OpenCV로 그리기 위해 numpy 배열로 변환됩니다. 마지막으로 주석이 달린 이미지를 ROS 메시지로 변환하여 발행합니다.</p>
<p><img src="/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_1.png" alt="이미지"></p>
<ol start="6">
<li>노드 실행하기</li>
</ol>
<p>main 함수는 ROS 2 Python 클라이언트 라이브러리를 초기화하고 노드의 인스턴스를 생성한 다음 종료될 때까지 작동하도록 유지하도록 되어 있습니다. 그 후에는 노드가 제거되고 ROS 2 컨텍스트가 종료됩니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_2.png" alt="Building a Pose Estimation Node with ROS 2"></p>
<h1>결론</h1>
<p>본 튜토리얼을 따라하면 최신 딥러닝 기법을 사용하여 실시간 포즈 추정이 가능한 견고한 ROS 2 노드를 만들 수 있습니다. 이 설정은 인간-로봇 상호작용 및 감시를 포함한 다양한 로봇 응용 프로그램으로 확장할 수 있습니다.</p>
<p>실시간 포즈 추정은 AMR 및 로봇의 능력에 새로운 차원을 추가하여 인간의 동작 및 자세를 이해하고 반응할 수 있습니다. 이 기능은 인간-로봇 상호작용을 향상시키며 안전성을 향상시키고 로봇이 자율적으로 또는 협업적으로 수행할 수 있는 작업 범위를 확장시킵니다. 기술이 계속 발전함에 따라 로봇학의 다양한 분야에서 더 많은 혁신적인 응용 프로그램이 예상됩니다.</p>
<div class="content-ad"></div>
<h1>NVIDIA Jetson 플랫폼에서 ROS 2 및 인공지능을 이용한 로봇 응용 프로그램 구현</h1>
<p><a href="https://developer.nvidia.com/blog/implementing-robotics-applications-with-ros-2-and-ai-on-jetson-platform-2/#ros_2_nodes_for_human_pose_estimation" rel="nofollow" target="_blank">https://developer.nvidia.com/blog/implementing-robotics-applications-with-ros-2-and-ai-on-jetson-platform-2/#ros_2_nodes_for_human_pose_estimation</a></p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"ROS 2 Humble과 PyTorch로 자세 추정 노드 만들기","description":"","date":"2024-06-20 17:48","slug":"2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch","content":"\n\n\u003cimg src=\"/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png\" /\u003e\n\n이 튜토리얼에서는 PyTorch에서 미리 학습된 딥 러닝 모델을 사용하여 실시간 인간 포즈 추정을 위한 ROS 2 노드를 만들겠습니다. 이 노드는 웹캠 이미지를 구독하고 포즈 추정을 수행한 뒤 주석이 달린 이미지를 발행할 것입니다. 구현 세부 내용으로 들어가 봅시다.\n\n## 요구 사항\n\n시작하기 전에 다음이 설치되어 있는지 확인하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- ROS 2\n- Python 3\n- PyTorch\n- torchvision\n- OpenCV\n- cv_bridge (ROS 패키지로 ROS와 OpenCV 이미지 간 변환을 위한 것)\n\n모델: keypointrcnn_resnet50_fpn\n\n우리는 torchvision의 keypointrcnn_resnet50_fpn 모델을 사용합니다. 이 모델은 사람 자세 추정을 위해 설계되어 여러 신체 부위의 키포인트를 예측합니다. 여기에 이 모델의 구성 요소가 있습니다:\n\n- ResNet-50 백본: ResNet-50는 특징 추출기로 작용하는 합성곱 신경망입니다. 공간적 계층을 효과적으로 캡처하는 데 알려져 있습니다.\n- FPN (Feature Pyramid Network): FPN은 다중 스케일에서 특성 맵을 구축하여 감지 능력을 향상시킵니다.\n- Keypoint R-CNN: 이 Faster R-CNN의 변형은 바운딩 박스 외에도 키포인트를 감지하는 데 특화되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*LbBdaJJRpnNGRLdKExdzdw.gif\" /\u003e\n\n자율 이동 로봇(AMR) 및 로보틱스에서 실시간 포즈 추정의 응용\n\n포즈 추정은 로보틱스 분야에서 강력한 도구이며 자율 이동 로봇(AMR)의 성능을 크게 향상시킬 수 있습니다. 실시간 포즈 추정을 통합함으로써 로봇은 상황 인식, 인간과의 상호 작용, 다양한 작업에서의 성능을 향상시킬 수 있습니다. 다음은 주요 응용 분야 몇 가지입니다:\n\n- 협업 로봇 (Cobots)\n- 감시 및 보안\n- 제조 및 조립 라인\n- 내비게이션 및 장애물 회피\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계별 실행\n\n- 노드 초기화 및 구독\n\n우리는 필요한 라이브러리를 가져오는 것으로 시작합니다. 이에는 ROS 2 Python 클라이언트 라이브러리 (rclpy), ROS 메시지 종류 (Image), 이미지 변환을 위한 CvBridge, 그리고 딥러닝을 위한 PyTorch 및 torchvision이 포함됩니다.\n\n2. 노드 클래스 생성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPoseEstimationNode 클래스를 정의하고 Node를 상속합니다. 생성자에서는:\n\n- 노드를 pose_estimation_node으로 이름을 지정하여 초기화합니다.\n- 이미지를 수신하기 위해 /jetson_webcam 토픽에 구독합니다.\n- 이미지_pose 토픽에 주석 처리된 이미지를 게시할 발행자를 만듭니다.\n- ROS 및 OpenCV 이미지 간 변환을 위해 CvBridge를 초기화합니다.\n- torchvision에서 사전 학습된 자세 추정 모델 keypointrcnn_resnet50_fpn을 평가 모드로 설정하여 로드합니다.\n- 이미지를 텐서로 변환하기 위한 변환을 정의합니다.\n\n3. 수신된 이미지 처리\n\nlistener_callback 메서드에서 수신된 이미지를 처리합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 받은 이미지의 인코딩을 기록합니다.\n- CvBridge를 사용하여 이미지를 ROS 형식에서 OpenCV 형식으로 변환하고 다양한 이미지 인코딩을 처리합니다.\n\n4. 포즈 추정\n\n다음으로, OpenCV 이미지를 PIL 이미지로 변환하고 텐서로 변환하기 위한 변환이 적용됩니다. 이 텐서를 모델에 전달하여 예측을 얻고, torch.no_grad()를 사용하여 기울기 계산이 이루어지지 않도록 합니다.\n\n5. 이미지 주석 및 게시\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그런 다음 이미지에서 주요 지점 위치에 원을 그립니다. 이러한 주요 지점은 모델의 예측에서 추출되어 OpenCV로 그리기 위해 numpy 배열로 변환됩니다. 마지막으로 주석이 달린 이미지를 ROS 메시지로 변환하여 발행합니다.\n\n![이미지](/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_1.png)\n\n6. 노드 실행하기\n\nmain 함수는 ROS 2 Python 클라이언트 라이브러리를 초기화하고 노드의 인스턴스를 생성한 다음 종료될 때까지 작동하도록 유지하도록 되어 있습니다. 그 후에는 노드가 제거되고 ROS 2 컨텍스트가 종료됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Building a Pose Estimation Node with ROS 2](/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_2.png)\n\n# 결론\n\n본 튜토리얼을 따라하면 최신 딥러닝 기법을 사용하여 실시간 포즈 추정이 가능한 견고한 ROS 2 노드를 만들 수 있습니다. 이 설정은 인간-로봇 상호작용 및 감시를 포함한 다양한 로봇 응용 프로그램으로 확장할 수 있습니다.\n\n실시간 포즈 추정은 AMR 및 로봇의 능력에 새로운 차원을 추가하여 인간의 동작 및 자세를 이해하고 반응할 수 있습니다. 이 기능은 인간-로봇 상호작용을 향상시키며 안전성을 향상시키고 로봇이 자율적으로 또는 협업적으로 수행할 수 있는 작업 범위를 확장시킵니다. 기술이 계속 발전함에 따라 로봇학의 다양한 분야에서 더 많은 혁신적인 응용 프로그램이 예상됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# NVIDIA Jetson 플랫폼에서 ROS 2 및 인공지능을 이용한 로봇 응용 프로그램 구현\n\nhttps://developer.nvidia.com/blog/implementing-robotics-applications-with-ros-2-and-ai-on-jetson-platform-2/#ros_2_nodes_for_human_pose_estimation","ogImage":{"url":"/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png"},"coverImage":"/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png","tag":["Tech"],"readingTime":4},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cimg src=\"/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_0.png\"\u003e\n\u003cp\u003e이 튜토리얼에서는 PyTorch에서 미리 학습된 딥 러닝 모델을 사용하여 실시간 인간 포즈 추정을 위한 ROS 2 노드를 만들겠습니다. 이 노드는 웹캠 이미지를 구독하고 포즈 추정을 수행한 뒤 주석이 달린 이미지를 발행할 것입니다. 구현 세부 내용으로 들어가 봅시다.\u003c/p\u003e\n\u003ch2\u003e요구 사항\u003c/h2\u003e\n\u003cp\u003e시작하기 전에 다음이 설치되어 있는지 확인하세요:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eROS 2\u003c/li\u003e\n\u003cli\u003ePython 3\u003c/li\u003e\n\u003cli\u003ePyTorch\u003c/li\u003e\n\u003cli\u003etorchvision\u003c/li\u003e\n\u003cli\u003eOpenCV\u003c/li\u003e\n\u003cli\u003ecv_bridge (ROS 패키지로 ROS와 OpenCV 이미지 간 변환을 위한 것)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e모델: keypointrcnn_resnet50_fpn\u003c/p\u003e\n\u003cp\u003e우리는 torchvision의 keypointrcnn_resnet50_fpn 모델을 사용합니다. 이 모델은 사람 자세 추정을 위해 설계되어 여러 신체 부위의 키포인트를 예측합니다. 여기에 이 모델의 구성 요소가 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eResNet-50 백본: ResNet-50는 특징 추출기로 작용하는 합성곱 신경망입니다. 공간적 계층을 효과적으로 캡처하는 데 알려져 있습니다.\u003c/li\u003e\n\u003cli\u003eFPN (Feature Pyramid Network): FPN은 다중 스케일에서 특성 맵을 구축하여 감지 능력을 향상시킵니다.\u003c/li\u003e\n\u003cli\u003eKeypoint R-CNN: 이 Faster R-CNN의 변형은 바운딩 박스 외에도 키포인트를 감지하는 데 특화되어 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*LbBdaJJRpnNGRLdKExdzdw.gif\"\u003e\n\u003cp\u003e자율 이동 로봇(AMR) 및 로보틱스에서 실시간 포즈 추정의 응용\u003c/p\u003e\n\u003cp\u003e포즈 추정은 로보틱스 분야에서 강력한 도구이며 자율 이동 로봇(AMR)의 성능을 크게 향상시킬 수 있습니다. 실시간 포즈 추정을 통합함으로써 로봇은 상황 인식, 인간과의 상호 작용, 다양한 작업에서의 성능을 향상시킬 수 있습니다. 다음은 주요 응용 분야 몇 가지입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e협업 로봇 (Cobots)\u003c/li\u003e\n\u003cli\u003e감시 및 보안\u003c/li\u003e\n\u003cli\u003e제조 및 조립 라인\u003c/li\u003e\n\u003cli\u003e내비게이션 및 장애물 회피\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e단계별 실행\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e노드 초기화 및 구독\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e우리는 필요한 라이브러리를 가져오는 것으로 시작합니다. 이에는 ROS 2 Python 클라이언트 라이브러리 (rclpy), ROS 메시지 종류 (Image), 이미지 변환을 위한 CvBridge, 그리고 딥러닝을 위한 PyTorch 및 torchvision이 포함됩니다.\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e노드 클래스 생성\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003ePoseEstimationNode 클래스를 정의하고 Node를 상속합니다. 생성자에서는:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e노드를 pose_estimation_node으로 이름을 지정하여 초기화합니다.\u003c/li\u003e\n\u003cli\u003e이미지를 수신하기 위해 /jetson_webcam 토픽에 구독합니다.\u003c/li\u003e\n\u003cli\u003e이미지_pose 토픽에 주석 처리된 이미지를 게시할 발행자를 만듭니다.\u003c/li\u003e\n\u003cli\u003eROS 및 OpenCV 이미지 간 변환을 위해 CvBridge를 초기화합니다.\u003c/li\u003e\n\u003cli\u003etorchvision에서 사전 학습된 자세 추정 모델 keypointrcnn_resnet50_fpn을 평가 모드로 설정하여 로드합니다.\u003c/li\u003e\n\u003cli\u003e이미지를 텐서로 변환하기 위한 변환을 정의합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e수신된 이미지 처리\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003elistener_callback 메서드에서 수신된 이미지를 처리합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e받은 이미지의 인코딩을 기록합니다.\u003c/li\u003e\n\u003cli\u003eCvBridge를 사용하여 이미지를 ROS 형식에서 OpenCV 형식으로 변환하고 다양한 이미지 인코딩을 처리합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003e포즈 추정\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e다음으로, OpenCV 이미지를 PIL 이미지로 변환하고 텐서로 변환하기 위한 변환이 적용됩니다. 이 텐서를 모델에 전달하여 예측을 얻고, torch.no_grad()를 사용하여 기울기 계산이 이루어지지 않도록 합니다.\u003c/p\u003e\n\u003col start=\"5\"\u003e\n\u003cli\u003e이미지 주석 및 게시\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그런 다음 이미지에서 주요 지점 위치에 원을 그립니다. 이러한 주요 지점은 모델의 예측에서 추출되어 OpenCV로 그리기 위해 numpy 배열로 변환됩니다. 마지막으로 주석이 달린 이미지를 ROS 메시지로 변환하여 발행합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003e노드 실행하기\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003emain 함수는 ROS 2 Python 클라이언트 라이브러리를 초기화하고 노드의 인스턴스를 생성한 다음 종료될 때까지 작동하도록 유지하도록 되어 있습니다. 그 후에는 노드가 제거되고 ROS 2 컨텍스트가 종료됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch_2.png\" alt=\"Building a Pose Estimation Node with ROS 2\"\u003e\u003c/p\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e본 튜토리얼을 따라하면 최신 딥러닝 기법을 사용하여 실시간 포즈 추정이 가능한 견고한 ROS 2 노드를 만들 수 있습니다. 이 설정은 인간-로봇 상호작용 및 감시를 포함한 다양한 로봇 응용 프로그램으로 확장할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e실시간 포즈 추정은 AMR 및 로봇의 능력에 새로운 차원을 추가하여 인간의 동작 및 자세를 이해하고 반응할 수 있습니다. 이 기능은 인간-로봇 상호작용을 향상시키며 안전성을 향상시키고 로봇이 자율적으로 또는 협업적으로 수행할 수 있는 작업 범위를 확장시킵니다. 기술이 계속 발전함에 따라 로봇학의 다양한 분야에서 더 많은 혁신적인 응용 프로그램이 예상됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003eNVIDIA Jetson 플랫폼에서 ROS 2 및 인공지능을 이용한 로봇 응용 프로그램 구현\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://developer.nvidia.com/blog/implementing-robotics-applications-with-ros-2-and-ai-on-jetson-platform-2/#ros_2_nodes_for_human_pose_estimation\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://developer.nvidia.com/blog/implementing-robotics-applications-with-ros-2-and-ai-on-jetson-platform-2/#ros_2_nodes_for_human_pose_estimation\u003c/a\u003e\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-20-BuildingaPoseEstimationNodewithROS2HumbleandPyTorch"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>