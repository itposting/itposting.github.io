<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>LaMa 탐색하기 푸리에 합성을 활용한 해상도 견고한 대규모 마스크 보정 간략한 개요 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="LaMa 탐색하기 푸리에 합성을 활용한 해상도 견고한 대규모 마스크 보정 간략한 개요 | itposting" data-gatsby-head="true"/><meta property="og:title" content="LaMa 탐색하기 푸리에 합성을 활용한 해상도 견고한 대규모 마스크 보정 간략한 개요 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview" data-gatsby-head="true"/><meta name="twitter:title" content="LaMa 탐색하기 푸리에 합성을 활용한 해상도 견고한 대규모 마스크 보정 간략한 개요 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-20 18:09" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-0fd008072af5a644.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_buildManifest.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">LaMa 탐색하기 푸리에 합성을 활용한 해상도 견고한 대규모 마스크 보정 간략한 개요</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="LaMa 탐색하기 푸리에 합성을 활용한 해상도 견고한 대규모 마스크 보정 간략한 개요" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 20, 2024</span><span class="posts_reading_time__f7YPP">9<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h1>소개</h1>
<p><img src="/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_0.png" alt="이미지"></p>
<p>이미지 인페인팅은 이미지 내 손상된 부분이나 가리워진 영역을 주변 맥락을 기반으로 재구성하는 컴퓨터 비전 기술입니다. 2022년에 발표된 LaMa라는 GAN 기반 네트워크를 만날 수 있습니다. 이 네트워크는 가벼운 아키텍처로 알려져 있으며 대형 마스크 사례를 개선하기 위해 특별히 설계되었습니다.</p>
<p>이미지 인페인팅에서 큰 마스크의 문제는 무엇일까요?</p>
<p><img src="/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_1.png" alt="Exploring LaMa Resolution"></p>
<p>이미지 인페인팅 모델은 주변 패치를 기반으로 빠진 부분을 다시 그리는 작업을 합니다. 더 큰 마스크는 복원 작업을 더 어렵게 만들며, 복구해야 할 정보가 많아지고 또한 의존할 수 있는 컨텍스트가 줄어드는 문제가 있습니다(큰 마스크 - 작은 배경). 그림 2에서는 다양한 크기의 마스크 영역이 있는 4개의 이미지가 있습니다. 배경의 복잡성을 고려하지 않고, 직사각형 마스크의 크기와 도전과제가 함께 증가하는 것을 관찰할 수 있습니다.</p>
<p>LaMa는 혁신적인 구조와 손실 함수로 큰 마스크 영역을 복원하는 데 특화되어 있습니다. 이 아이디어에 대해 궁금하다면, 다음 섹션으로 계속 진행해 보세요.</p>
<h1>방법</h1>
<h2>네트워크 아키텍처</h2>
<p><img src="/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_2.png" alt="아키텍처"></p>
<p>네트워크 아키텍처부터 시작해 봅시다. LaMa는 생성자와 판별자로 구성된 GAN 기반의 작업입니다. 판별자와 손실 함수에 대해는 나중에 이야기할 것입니다. 생성자 네트워크 f의 구조는 위의 그림 3에 설명되어 있습니다.</p>
<ul>
<li>네트워크 입력: 손상된 이미지 x와 이진 마스크 m이 제공되면, 네트워크 입력은 우리가 예측하려는 마스킹된 이미지와 마스크 m의 연결입니다.</li>
<li>네트워크: 네트워크는 시작 부분에서 다운스케일 단계, 중간에 일련의 잔여 블록, 그리고 끝에 역 스케일 업 단계로 구성됩니다. 잔여 블록은 다음 섹션에서 다룰 Fast Fourier Convolution으로 이루어져 있습니다.</li>
<li>네트워크 출력: 네트워크는 회복된 이미지 x̂를 출력합니다. 훈련 단계에서의 손실은 입력 x와 출력 x̂의 불일치에 기반합니다.</li>
</ul>
<p>LaMa의 혁신은 푸리에 변환 컨볼루션의 통합에 있습니다. 다음 섹션에서 세부 사항부터 시작하여 점차적으로 더 큰 맥락을 포함하도록 확장해 봅시다.</p>
<h2>푸리에 변환</h2>
<p><img src="/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_3.png" alt="이미지"></p>
<p>푸리에 변환은 이미지 및 신호 처리에서 공간/시간 영역의 입력을 주파수 영역으로 변환하는 고전적인 방법입니다. 이미지 처리에서 변환된 이미지는 입력 이미지와 동일한 크기를 공유하며 다음과 같은 특성을 갖습니다:</p>
<ul>
<li>변환된 이미지의 각 픽셀은 입력 이미지의 특정 주파수를 나타냅니다. 예를 들어, 그림 4 (b)의 중앙 영역은 낮은 주파수를 나타내며 이미지 테두리 쪽으로 이동할수록 주파수가 높아집니다.</li>
<li>이것은 자기 역변환 가능합니다. 변환된 이미지에 역변환을 적용하여 원본 이미지를 복원할 수 있습니다.</li>
</ul>
<p>Fourier 변환의 중요한 속성 중 하나는 변환된 이미지의 각 픽셀이 공간 영역의 모든 픽셀에서 유래한다는 것입니다. 다시 말하면, 공간 영역의 이미지가 주파수 영역의 각 픽셀에 이미지로 인코딩되어 있습니다.</p>
<h2>Spectrum Transform</h2>
<p><img src="/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_4.png" alt="그림"></p>
<p>Fourier Transform 아이디어를 따라가면서 한 단계 더 들어가 봅시다.</p>
<p>푸리에 변환은 스펙트럴 변환이라는 블록에서 사용됩니다.</p>
<ul>
<li>스펙트럴 변환은 표준 합성곱 블록 (Convolution-BatchNorm-ReLU)으로 시작합니다.</li>
<li>이어서 실수값 고속 푸리에 변환(Real FFT, 푸리에 변환의 한 변형)이 적용되어 특징 맵을 주파수로 변환합니다. 실수값 고속 푸리에 변환에서 사용되는 주파수는 절반뿐입니다.</li>
<li>주파수 영역의 특징 맵에 두 번째 표준 합성곱 블록을 적용합니다.</li>
<li>마지막으로 역 고속 푸리에 변환을 적용하여 특징 맵을 다시 공간 영역으로 변환합니다. 스펙트럼 변환은 채널 수를 두 배로 늘리기 위해 1x1 합성곱으로 끝납니다.</li>
</ul>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SpectralTransform</span>(nn.<span class="hljs-property">Module</span>):

    def <span class="hljs-title function_">__init__</span>(self, in_channels, out_channels, stride=<span class="hljs-number">1</span>, groups=<span class="hljs-number">1</span>, enable_lfu=<span class="hljs-title class_">True</span>, **fu_kwargs):
        # bn_layer 사용하지 않음
        <span class="hljs-variable language_">super</span>(<span class="hljs-title class_">SpectralTransform</span>, self).<span class="hljs-title function_">__init__</span>()
        self.<span class="hljs-property">enable_lfu</span> = enable_lfu
        
        self.<span class="hljs-property">downsample</span> = nn.<span class="hljs-title class_">Identity</span>()

        self.<span class="hljs-property">conv1</span> = nn.<span class="hljs-title class_">Sequential</span>(
            nn.<span class="hljs-title class_">Conv2</span>d(in_channels, out_channels <span class="hljs-comment">// 2, kernel_size=1, groups=groups, bias=False),</span>
            nn.<span class="hljs-title class_">BatchNorm2</span>d(out_channels <span class="hljs-comment">// 2),</span>
            nn.<span class="hljs-title class_">ReLU</span>(inplace=<span class="hljs-title class_">True</span>)
        )

        self.<span class="hljs-property">fu</span> = <span class="hljs-title class_">FourierUnit</span>(out_channels <span class="hljs-comment">// 2, out_channels // 2, groups, **fu_kwargs)</span>
        
        <span class="hljs-keyword">if</span> self.<span class="hljs-property">enable_lfu</span>:
            self.<span class="hljs-property">lfu</span> = <span class="hljs-title class_">FourierUnit</span>(out_channels <span class="hljs-comment">// 2, out_channels // 2, groups)</span>
        
        self.<span class="hljs-property">conv2</span> = torch.<span class="hljs-property">nn</span>.<span class="hljs-title class_">Conv2</span>d(out_channels <span class="hljs-comment">// 2, out_channels, kernel_size=1, groups=groups, bias=False)</span>

    def <span class="hljs-title function_">forward</span>(self, x):

        x = self.<span class="hljs-title function_">downsample</span>(x)
        x = self.<span class="hljs-title function_">conv1</span>(x).  # 논문: [<span class="hljs-title class_">Conv</span>-<span class="hljs-variable constant_">BN</span>-<span class="hljs-title class_">ReLU</span>]
        output = self.<span class="hljs-title function_">fu</span>(x) # 논문: [<span class="hljs-title class_">Real</span> <span class="hljs-title class_">FFT2</span>d - <span class="hljs-title class_">Conv</span>-<span class="hljs-variable constant_">BN</span>-<span class="hljs-title class_">ReLU</span> - <span class="hljs-title class_">Inv</span> <span class="hljs-title class_">Real</span> <span class="hljs-title class_">FFT2</span>d]

        <span class="hljs-keyword">if</span> self.<span class="hljs-property">enable_lfu</span>:
            n, c, h, w = x.<span class="hljs-property">shape</span>
            split_no = <span class="hljs-number">2</span>
            split_s = h <span class="hljs-comment">// split_no</span>
            xs = torch.<span class="hljs-title function_">cat</span>(torch.<span class="hljs-title function_">split</span>(x[:, :c <span class="hljs-comment">// 4], split_s, dim=-2), dim=1).contiguous()</span>
            xs = torch.<span class="hljs-title function_">cat</span>(torch.<span class="hljs-title function_">split</span>(xs, split_s, dim=-<span class="hljs-number">1</span>),dim=<span class="hljs-number">1</span>).<span class="hljs-title function_">contiguous</span>()
            xs = self.<span class="hljs-title function_">lfu</span>(xs)
            xs = xs.<span class="hljs-title function_">repeat</span>(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, split_no, split_no).<span class="hljs-title function_">contiguous</span>()
        <span class="hljs-attr">else</span>:
            xs = <span class="hljs-number">0</span>

        output = self.<span class="hljs-title function_">conv2</span>(x + output + xs). # 논문: [<span class="hljs-title class_">Conv</span> 1x1]

        <span class="hljs-keyword">return</span> output
</code></pre>
<p>위 코드 블록에 구현 내용이 있습니다. 함수 forward() 내 인라인 코멘트는 Figure 5의 직사각형 블록에 대한 매핑을 설명합니다. 이 코드는 이해를 돕기 위해 기본 구성을 사용하여 공식 저장소²에서 간소화되었음을 참고해주세요.</p>
<h2>Fast Fourier Convolution (FFC)</h2>
<p>이제 Figure 5의 왼쪽으로 이동하여 FFC의 개념이 그려진 곳으로 이동합시다. FFC는 여러 네트워크 연산자로 구성된 블록 모듈입니다. 입력 피처 맵은 글로벌 및 로컬 브랜치를 통해 분할되어 전달됩니다.</p>
<ul>
<li>로컬 브랜치: 로컬 브랜치는 일반 컨볼루션 프로세스를 따릅니다: 컨볼루션 - 배치 정규화 - 활성화 함수.</li>
<li>글로벌 브랜치: 방금 살펴본 제안된 스펙트럼 변환은 글로벌 브랜치에서 적용되며 로컬 브랜치의 동일한 컨볼루션 프로세스와 함께 사용됩니다. 표준 컨볼루션 블록과 제안된 FFT 기반 블록을 결합하여 글로벌 브랜치의 출력은 지역적 피처와 전체 이미지 구조를 모두 담고 있습니다.</li>
</ul>
<p>로컬 및 글로벌 브랜치의 출력은 FFC의 끝에서 연결됩니다. 라마의 기본 아키텍처에서는 9개의 FFC가 구성되어 있습니다. FFC 초기화 스크립트는 아래와 같습니다.</p>
<pre><code class="hljs language-js">### <span class="hljs-title class_">Resnet</span> 블록
# n_blocks = <span class="hljs-number">9</span>

<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(n_blocks):
    cur_resblock = <span class="hljs-title class_">FFCResnetBlock</span>(feats_num_bottleneck, padding_type=padding_type, activation_layer=activation_layer,
                                          norm_layer=norm_layer, **resnet_conv_kwargs)
    <span class="hljs-keyword">if</span> spatial_transform_layers is not <span class="hljs-title class_">None</span> and i <span class="hljs-keyword">in</span> <span class="hljs-attr">spatial_transform_layers</span>:
       cur_resblock = <span class="hljs-title class_">LearnableSpatialTransformWrapper</span>(cur_resblock, **spatial_transform_kwargs)
       model += [cur_resblock]
</code></pre>
<h2>손실 함수</h2>
<p>높은 수용 영역 지각 손실 (HRFPL)
네트워크의 초기 레이어에서 FFC에 참여하는 것 외에도, LaMa의 또 다른 혁신은 새로운 손실 함수에 있습니다: 높은 수용 영역 지각 손실 (HRFPL).</p>
<p>HRFPL은 큰 마스크를 가진 샘플이 가시 영역의 컨텍스트가 충분하지 않아 세부 정보를 사용하여 누락된 부분을 복원할 수 없다는 가정에서 나옵니다. 입력 및 출력 이미지 사이의 철저한 픽셀별 비교는 불필요합니다. 대신, 네트워크는 효율적인 네트워크 ϕ에 의해 추출된 상위 수준의 컨텍스트를 살펴볼 수 있습니다. 더 간결한 아키텍처로, 수용 영역은 순전파하는 동안 더 빨리 성장합니다 (레이어가 적을수록 수용 영역이 더 빨리 성장합니다). 공식 구현에서는 Vgg19를 사용하여 이미지 특징을 추출합니다. HRFPL의 공식은 그림 6에서 찾을 수 있습니다.</p>
<pre><code class="hljs language-js"># 참조: <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/advimman/lama/blob/3197c5e5e42503a66868e636ed48a7cefc5e8c28/saicinpainting/training/losses/perceptual.py#L67</span>

손실 = F.<span class="hljs-title function_">mse_loss</span>(features_input, features_target, reduction=<span class="hljs-string">'none'</span>)
손실 = loss.<span class="hljs-title function_">mean</span>(dim=<span class="hljs-title function_">tuple</span>(<span class="hljs-title function_">range</span>(<span class="hljs-number">1</span>, <span class="hljs-title function_">len</span>(loss.<span class="hljs-property">shape</span>)))
</code></pre>
<p>차이를 계산하고 결과를 평균화하는 해당 Pytorch 스크립트를 위에서 찾을 수 있습니다.</p>
<p>적대적 손실</p>
<p><img src="/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_6.png" alt="image"></p>
<p>다른 GAN과 유사하게, LaMa에서 생성자와 구분자 네트워크 간의 관계를 구축하기 위해 적대적 손실이 적용됩니다. LaMa의 적대적 손실은 구분자가 실제 영역에서 생성된 콘텐츠를 더 잘 식별하도록 장려하고, 생성자에게 마스크된 영역 내에서 더 현실적인 콘텐츠를 생성하도록 피드백하는 것을 목표로 합니다.</p>
<pre><code class="hljs language-js"># 참조: <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/advimman/lama/blob/3197c5e5e42503a66868e636ed48a7cefc5e8c28/saicinpainting/training/trainers/default.py#L115</span>
discr_real_pred, discr_real_features = self.<span class="hljs-title function_">discriminator</span>(img)
discr_fake_pred, discr_fake_features = self.<span class="hljs-title function_">discriminator</span>(predicted_img)
adv_gen_loss, adv_metrics = self.<span class="hljs-property">adversarial_loss</span>.<span class="hljs-title function_">generator_loss</span>(real_batch=img, fake_batch=predicted_img, discr_real_pred=discr_real_pred, discr_fake_pred=discr_fake_pred, mask=mask_for_discr)
</code></pre>
<p>discriminator의 입력은 원본 이미지 x와 inpainted 이미지 x̂입니다. discriminator는 각 픽셀의 클래스를 real 또는 fake로 예측합니다. Adversarial Loss는 생성기 손실과 discriminator 손실을 결합하며 다른 기존 GAN과 동일한 개념을 공유합니다. 자세한 내용은 아래의 방정식에서 확인할 수 있습니다.</p>
<p><img src="/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_7.png" alt="equation_7"></p>
<p>총 손실</p>
<p><img src="/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_8.png" alt="equation_8"></p>
<p>손실 함수를 마무리하는 시간이에요. 제안된 HRFPL 및 적대적 손실은 대형 마스크 케이스에서 정확도 측면에서 LaMa를 다른 인풋 네트워크들과 구분짓습니다. 두 가지의 아이디어가 8번 그림에 나와 있어요. HRFPL 및 적대적 손실 이외에도 두 가지 일반적으로 사용되는 손실 함수가 포함돼 있어요.</p>
<ul>
<li>그래디언트 페널티</li>
<li>피처 매칭 손실: 판별자 네트워크의 피처에 대한 인식 손실</li>
</ul>
<p>최종 손실은 네 가지 손실 함수의 가중 합으로 이루어져요.</p>
<p>이 모든 것으로 LaMa가 두드러지는 점에 관한 이야기는 여기까지입니다.</p>
<p>간략히 말하면, LaMa는 Fast Fourier Convolution (FFC)을 통합하고 고수용 필드 지각 손실 (HRFPL)로 안내되는, 가벼운 Resnet과 유사한 네트워크입니다. 이는 채워져야 하는 대상 영역이 더 큰 경우에 특히 강력합니다. LaMa는 이미지 인페인팅 분야에서의 진전을 나타내며, 다양한 해상도에서 견고한 솔루션을 제공하고 어려운 인페인팅 시나리오를 처리할 수 있습니다.</p>
<p>다른 주제들도 원본 논문에서 논의되었습니다. 예를 들어, LaMa는 작은 256x256 이미지로 모델을 훈련하더라도 높은 해상도의 영역을 복원할 수 있다는 내용이 포함되어 있습니다. LaMa에 대해 더 알아보려면 원본 논문을 읽어보세요.</p>
<h1>요약</h1>
<p>이 기사에서 LaMa의 아이디어를 다뤘어요. 이미지 인페인팅에서 인기 있는 네트워크 중 하나로, 방문할 가치가 있는 많은 커뮤니티 자원이 있어요. 일부 온라인 서비스는 람마의 기능을 사용할 수 있는 훌륭한 인터페이스를 제공해요.</p>
<p>늘 읽어 주셔서 감사합니다. 피드백과 의견은 언제나 환영해요. 모두에게 멋진 하루가 되길 바라요!</p>
<h1>참고 자료</h1>
<p>[1] Suvorov, R., Logacheva, E., Mashikhin, A., Remizova, A., Ashukha, A., Silvestrov, A., Kong, N., Goka, H., Park, K., &#x26; Lempitsky, V. (2021). Resolution-robust Large Mask Inpainting with Fourier Convolutions. ArXiv. /abs/2109.07161
[2] 공식 구현: <a href="https://github.com/advimman/lama" rel="nofollow" target="_blank">https://github.com/advimman/lama</a>
[3] <a href="https://developers.google.com/machine-learning/gan/loss" rel="nofollow" target="_blank">https://developers.google.com/machine-learning/gan/loss</a></p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"LaMa 탐색하기 푸리에 합성을 활용한 해상도 견고한 대규모 마스크 보정 간략한 개요","description":"","date":"2024-06-20 18:09","slug":"2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview","content":"\n\n# 소개\n\n![이미지](/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_0.png)\n\n이미지 인페인팅은 이미지 내 손상된 부분이나 가리워진 영역을 주변 맥락을 기반으로 재구성하는 컴퓨터 비전 기술입니다. 2022년에 발표된 LaMa라는 GAN 기반 네트워크를 만날 수 있습니다. 이 네트워크는 가벼운 아키텍처로 알려져 있으며 대형 마스크 사례를 개선하기 위해 특별히 설계되었습니다.\n\n이미지 인페인팅에서 큰 마스크의 문제는 무엇일까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Exploring LaMa Resolution](/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_1.png)\n\n이미지 인페인팅 모델은 주변 패치를 기반으로 빠진 부분을 다시 그리는 작업을 합니다. 더 큰 마스크는 복원 작업을 더 어렵게 만들며, 복구해야 할 정보가 많아지고 또한 의존할 수 있는 컨텍스트가 줄어드는 문제가 있습니다(큰 마스크 - 작은 배경). 그림 2에서는 다양한 크기의 마스크 영역이 있는 4개의 이미지가 있습니다. 배경의 복잡성을 고려하지 않고, 직사각형 마스크의 크기와 도전과제가 함께 증가하는 것을 관찰할 수 있습니다.\n\nLaMa는 혁신적인 구조와 손실 함수로 큰 마스크 영역을 복원하는 데 특화되어 있습니다. 이 아이디어에 대해 궁금하다면, 다음 섹션으로 계속 진행해 보세요.\n\n# 방법\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 네트워크 아키텍처\n\n![아키텍처](/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_2.png)\n\n네트워크 아키텍처부터 시작해 봅시다. LaMa는 생성자와 판별자로 구성된 GAN 기반의 작업입니다. 판별자와 손실 함수에 대해는 나중에 이야기할 것입니다. 생성자 네트워크 f의 구조는 위의 그림 3에 설명되어 있습니다.\n\n- 네트워크 입력: 손상된 이미지 x와 이진 마스크 m이 제공되면, 네트워크 입력은 우리가 예측하려는 마스킹된 이미지와 마스크 m의 연결입니다.\n- 네트워크: 네트워크는 시작 부분에서 다운스케일 단계, 중간에 일련의 잔여 블록, 그리고 끝에 역 스케일 업 단계로 구성됩니다. 잔여 블록은 다음 섹션에서 다룰 Fast Fourier Convolution으로 이루어져 있습니다.\n- 네트워크 출력: 네트워크는 회복된 이미지 x̂를 출력합니다. 훈련 단계에서의 손실은 입력 x와 출력 x̂의 불일치에 기반합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLaMa의 혁신은 푸리에 변환 컨볼루션의 통합에 있습니다. 다음 섹션에서 세부 사항부터 시작하여 점차적으로 더 큰 맥락을 포함하도록 확장해 봅시다.\n\n## 푸리에 변환\n\n![이미지](/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_3.png)\n\n푸리에 변환은 이미지 및 신호 처리에서 공간/시간 영역의 입력을 주파수 영역으로 변환하는 고전적인 방법입니다. 이미지 처리에서 변환된 이미지는 입력 이미지와 동일한 크기를 공유하며 다음과 같은 특성을 갖습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 변환된 이미지의 각 픽셀은 입력 이미지의 특정 주파수를 나타냅니다. 예를 들어, 그림 4 (b)의 중앙 영역은 낮은 주파수를 나타내며 이미지 테두리 쪽으로 이동할수록 주파수가 높아집니다.\n- 이것은 자기 역변환 가능합니다. 변환된 이미지에 역변환을 적용하여 원본 이미지를 복원할 수 있습니다.\n\nFourier 변환의 중요한 속성 중 하나는 변환된 이미지의 각 픽셀이 공간 영역의 모든 픽셀에서 유래한다는 것입니다. 다시 말하면, 공간 영역의 이미지가 주파수 영역의 각 픽셀에 이미지로 인코딩되어 있습니다.\n\n## Spectrum Transform\n\n![그림](/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nFourier Transform 아이디어를 따라가면서 한 단계 더 들어가 봅시다.\n\n푸리에 변환은 스펙트럴 변환이라는 블록에서 사용됩니다.\n\n- 스펙트럴 변환은 표준 합성곱 블록 (Convolution-BatchNorm-ReLU)으로 시작합니다.\n- 이어서 실수값 고속 푸리에 변환(Real FFT, 푸리에 변환의 한 변형)이 적용되어 특징 맵을 주파수로 변환합니다. 실수값 고속 푸리에 변환에서 사용되는 주파수는 절반뿐입니다.\n- 주파수 영역의 특징 맵에 두 번째 표준 합성곱 블록을 적용합니다.\n- 마지막으로 역 고속 푸리에 변환을 적용하여 특징 맵을 다시 공간 영역으로 변환합니다. 스펙트럼 변환은 채널 수를 두 배로 늘리기 위해 1x1 합성곱으로 끝납니다.\n\n```js\nclass SpectralTransform(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride=1, groups=1, enable_lfu=True, **fu_kwargs):\n        # bn_layer 사용하지 않음\n        super(SpectralTransform, self).__init__()\n        self.enable_lfu = enable_lfu\n        \n        self.downsample = nn.Identity()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels // 2, kernel_size=1, groups=groups, bias=False),\n            nn.BatchNorm2d(out_channels // 2),\n            nn.ReLU(inplace=True)\n        )\n\n        self.fu = FourierUnit(out_channels // 2, out_channels // 2, groups, **fu_kwargs)\n        \n        if self.enable_lfu:\n            self.lfu = FourierUnit(out_channels // 2, out_channels // 2, groups)\n        \n        self.conv2 = torch.nn.Conv2d(out_channels // 2, out_channels, kernel_size=1, groups=groups, bias=False)\n\n    def forward(self, x):\n\n        x = self.downsample(x)\n        x = self.conv1(x).  # 논문: [Conv-BN-ReLU]\n        output = self.fu(x) # 논문: [Real FFT2d - Conv-BN-ReLU - Inv Real FFT2d]\n\n        if self.enable_lfu:\n            n, c, h, w = x.shape\n            split_no = 2\n            split_s = h // split_no\n            xs = torch.cat(torch.split(x[:, :c // 4], split_s, dim=-2), dim=1).contiguous()\n            xs = torch.cat(torch.split(xs, split_s, dim=-1),dim=1).contiguous()\n            xs = self.lfu(xs)\n            xs = xs.repeat(1, 1, split_no, split_no).contiguous()\n        else:\n            xs = 0\n\n        output = self.conv2(x + output + xs). # 논문: [Conv 1x1]\n\n        return output\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 코드 블록에 구현 내용이 있습니다. 함수 forward() 내 인라인 코멘트는 Figure 5의 직사각형 블록에 대한 매핑을 설명합니다. 이 코드는 이해를 돕기 위해 기본 구성을 사용하여 공식 저장소²에서 간소화되었음을 참고해주세요.\n\n## Fast Fourier Convolution (FFC)\n\n이제 Figure 5의 왼쪽으로 이동하여 FFC의 개념이 그려진 곳으로 이동합시다. FFC는 여러 네트워크 연산자로 구성된 블록 모듈입니다. 입력 피처 맵은 글로벌 및 로컬 브랜치를 통해 분할되어 전달됩니다.\n\n- 로컬 브랜치: 로컬 브랜치는 일반 컨볼루션 프로세스를 따릅니다: 컨볼루션 - 배치 정규화 - 활성화 함수.\n- 글로벌 브랜치: 방금 살펴본 제안된 스펙트럼 변환은 글로벌 브랜치에서 적용되며 로컬 브랜치의 동일한 컨볼루션 프로세스와 함께 사용됩니다. 표준 컨볼루션 블록과 제안된 FFT 기반 블록을 결합하여 글로벌 브랜치의 출력은 지역적 피처와 전체 이미지 구조를 모두 담고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로컬 및 글로벌 브랜치의 출력은 FFC의 끝에서 연결됩니다. 라마의 기본 아키텍처에서는 9개의 FFC가 구성되어 있습니다. FFC 초기화 스크립트는 아래와 같습니다.\n\n```js\n### Resnet 블록\n# n_blocks = 9\n\nfor i in range(n_blocks):\n    cur_resblock = FFCResnetBlock(feats_num_bottleneck, padding_type=padding_type, activation_layer=activation_layer,\n                                          norm_layer=norm_layer, **resnet_conv_kwargs)\n    if spatial_transform_layers is not None and i in spatial_transform_layers:\n       cur_resblock = LearnableSpatialTransformWrapper(cur_resblock, **spatial_transform_kwargs)\n       model += [cur_resblock]\r\n```\n\n## 손실 함수\n\n\u003cimg src=\"/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_5.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n높은 수용 영역 지각 손실 (HRFPL)\n네트워크의 초기 레이어에서 FFC에 참여하는 것 외에도, LaMa의 또 다른 혁신은 새로운 손실 함수에 있습니다: 높은 수용 영역 지각 손실 (HRFPL).\n\nHRFPL은 큰 마스크를 가진 샘플이 가시 영역의 컨텍스트가 충분하지 않아 세부 정보를 사용하여 누락된 부분을 복원할 수 없다는 가정에서 나옵니다. 입력 및 출력 이미지 사이의 철저한 픽셀별 비교는 불필요합니다. 대신, 네트워크는 효율적인 네트워크 ϕ에 의해 추출된 상위 수준의 컨텍스트를 살펴볼 수 있습니다. 더 간결한 아키텍처로, 수용 영역은 순전파하는 동안 더 빨리 성장합니다 (레이어가 적을수록 수용 영역이 더 빨리 성장합니다). 공식 구현에서는 Vgg19를 사용하여 이미지 특징을 추출합니다. HRFPL의 공식은 그림 6에서 찾을 수 있습니다.\n\n```js\n# 참조: https://github.com/advimman/lama/blob/3197c5e5e42503a66868e636ed48a7cefc5e8c28/saicinpainting/training/losses/perceptual.py#L67\n\n손실 = F.mse_loss(features_input, features_target, reduction='none')\n손실 = loss.mean(dim=tuple(range(1, len(loss.shape)))\n```\n\n차이를 계산하고 결과를 평균화하는 해당 Pytorch 스크립트를 위에서 찾을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n적대적 손실\n\n![image](/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_6.png)\n\n다른 GAN과 유사하게, LaMa에서 생성자와 구분자 네트워크 간의 관계를 구축하기 위해 적대적 손실이 적용됩니다. LaMa의 적대적 손실은 구분자가 실제 영역에서 생성된 콘텐츠를 더 잘 식별하도록 장려하고, 생성자에게 마스크된 영역 내에서 더 현실적인 콘텐츠를 생성하도록 피드백하는 것을 목표로 합니다.\n\n```js\n# 참조: https://github.com/advimman/lama/blob/3197c5e5e42503a66868e636ed48a7cefc5e8c28/saicinpainting/training/trainers/default.py#L115\ndiscr_real_pred, discr_real_features = self.discriminator(img)\ndiscr_fake_pred, discr_fake_features = self.discriminator(predicted_img)\nadv_gen_loss, adv_metrics = self.adversarial_loss.generator_loss(real_batch=img, fake_batch=predicted_img, discr_real_pred=discr_real_pred, discr_fake_pred=discr_fake_pred, mask=mask_for_discr)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ndiscriminator의 입력은 원본 이미지 x와 inpainted 이미지 x̂입니다. discriminator는 각 픽셀의 클래스를 real 또는 fake로 예측합니다. Adversarial Loss는 생성기 손실과 discriminator 손실을 결합하며 다른 기존 GAN과 동일한 개념을 공유합니다. 자세한 내용은 아래의 방정식에서 확인할 수 있습니다.\n\n![equation_7](/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_7.png)\n\n총 손실\n\n![equation_8](/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_8.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n손실 함수를 마무리하는 시간이에요. 제안된 HRFPL 및 적대적 손실은 대형 마스크 케이스에서 정확도 측면에서 LaMa를 다른 인풋 네트워크들과 구분짓습니다. 두 가지의 아이디어가 8번 그림에 나와 있어요. HRFPL 및 적대적 손실 이외에도 두 가지 일반적으로 사용되는 손실 함수가 포함돼 있어요.\n\n- 그래디언트 페널티\n- 피처 매칭 손실: 판별자 네트워크의 피처에 대한 인식 손실\n\n최종 손실은 네 가지 손실 함수의 가중 합으로 이루어져요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 모든 것으로 LaMa가 두드러지는 점에 관한 이야기는 여기까지입니다.\n\n간략히 말하면, LaMa는 Fast Fourier Convolution (FFC)을 통합하고 고수용 필드 지각 손실 (HRFPL)로 안내되는, 가벼운 Resnet과 유사한 네트워크입니다. 이는 채워져야 하는 대상 영역이 더 큰 경우에 특히 강력합니다. LaMa는 이미지 인페인팅 분야에서의 진전을 나타내며, 다양한 해상도에서 견고한 솔루션을 제공하고 어려운 인페인팅 시나리오를 처리할 수 있습니다.\n\n다른 주제들도 원본 논문에서 논의되었습니다. 예를 들어, LaMa는 작은 256x256 이미지로 모델을 훈련하더라도 높은 해상도의 영역을 복원할 수 있다는 내용이 포함되어 있습니다. LaMa에 대해 더 알아보려면 원본 논문을 읽어보세요.\n\n# 요약\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서 LaMa의 아이디어를 다뤘어요. 이미지 인페인팅에서 인기 있는 네트워크 중 하나로, 방문할 가치가 있는 많은 커뮤니티 자원이 있어요. 일부 온라인 서비스는 람마의 기능을 사용할 수 있는 훌륭한 인터페이스를 제공해요.\n\n늘 읽어 주셔서 감사합니다. 피드백과 의견은 언제나 환영해요. 모두에게 멋진 하루가 되길 바라요!\n\n# 참고 자료\n\n[1] Suvorov, R., Logacheva, E., Mashikhin, A., Remizova, A., Ashukha, A., Silvestrov, A., Kong, N., Goka, H., Park, K., \u0026 Lempitsky, V. (2021). Resolution-robust Large Mask Inpainting with Fourier Convolutions. ArXiv. /abs/2109.07161\n[2] 공식 구현: https://github.com/advimman/lama\n[3] https://developers.google.com/machine-learning/gan/loss","ogImage":{"url":"/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_0.png"},"coverImage":"/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_0.png","tag":["Tech"],"readingTime":9},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch1\u003e소개\u003c/h1\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e이미지 인페인팅은 이미지 내 손상된 부분이나 가리워진 영역을 주변 맥락을 기반으로 재구성하는 컴퓨터 비전 기술입니다. 2022년에 발표된 LaMa라는 GAN 기반 네트워크를 만날 수 있습니다. 이 네트워크는 가벼운 아키텍처로 알려져 있으며 대형 마스크 사례를 개선하기 위해 특별히 설계되었습니다.\u003c/p\u003e\n\u003cp\u003e이미지 인페인팅에서 큰 마스크의 문제는 무엇일까요?\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_1.png\" alt=\"Exploring LaMa Resolution\"\u003e\u003c/p\u003e\n\u003cp\u003e이미지 인페인팅 모델은 주변 패치를 기반으로 빠진 부분을 다시 그리는 작업을 합니다. 더 큰 마스크는 복원 작업을 더 어렵게 만들며, 복구해야 할 정보가 많아지고 또한 의존할 수 있는 컨텍스트가 줄어드는 문제가 있습니다(큰 마스크 - 작은 배경). 그림 2에서는 다양한 크기의 마스크 영역이 있는 4개의 이미지가 있습니다. 배경의 복잡성을 고려하지 않고, 직사각형 마스크의 크기와 도전과제가 함께 증가하는 것을 관찰할 수 있습니다.\u003c/p\u003e\n\u003cp\u003eLaMa는 혁신적인 구조와 손실 함수로 큰 마스크 영역을 복원하는 데 특화되어 있습니다. 이 아이디어에 대해 궁금하다면, 다음 섹션으로 계속 진행해 보세요.\u003c/p\u003e\n\u003ch1\u003e방법\u003c/h1\u003e\n\u003ch2\u003e네트워크 아키텍처\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_2.png\" alt=\"아키텍처\"\u003e\u003c/p\u003e\n\u003cp\u003e네트워크 아키텍처부터 시작해 봅시다. LaMa는 생성자와 판별자로 구성된 GAN 기반의 작업입니다. 판별자와 손실 함수에 대해는 나중에 이야기할 것입니다. 생성자 네트워크 f의 구조는 위의 그림 3에 설명되어 있습니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e네트워크 입력: 손상된 이미지 x와 이진 마스크 m이 제공되면, 네트워크 입력은 우리가 예측하려는 마스킹된 이미지와 마스크 m의 연결입니다.\u003c/li\u003e\n\u003cli\u003e네트워크: 네트워크는 시작 부분에서 다운스케일 단계, 중간에 일련의 잔여 블록, 그리고 끝에 역 스케일 업 단계로 구성됩니다. 잔여 블록은 다음 섹션에서 다룰 Fast Fourier Convolution으로 이루어져 있습니다.\u003c/li\u003e\n\u003cli\u003e네트워크 출력: 네트워크는 회복된 이미지 x̂를 출력합니다. 훈련 단계에서의 손실은 입력 x와 출력 x̂의 불일치에 기반합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLaMa의 혁신은 푸리에 변환 컨볼루션의 통합에 있습니다. 다음 섹션에서 세부 사항부터 시작하여 점차적으로 더 큰 맥락을 포함하도록 확장해 봅시다.\u003c/p\u003e\n\u003ch2\u003e푸리에 변환\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_3.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e푸리에 변환은 이미지 및 신호 처리에서 공간/시간 영역의 입력을 주파수 영역으로 변환하는 고전적인 방법입니다. 이미지 처리에서 변환된 이미지는 입력 이미지와 동일한 크기를 공유하며 다음과 같은 특성을 갖습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e변환된 이미지의 각 픽셀은 입력 이미지의 특정 주파수를 나타냅니다. 예를 들어, 그림 4 (b)의 중앙 영역은 낮은 주파수를 나타내며 이미지 테두리 쪽으로 이동할수록 주파수가 높아집니다.\u003c/li\u003e\n\u003cli\u003e이것은 자기 역변환 가능합니다. 변환된 이미지에 역변환을 적용하여 원본 이미지를 복원할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFourier 변환의 중요한 속성 중 하나는 변환된 이미지의 각 픽셀이 공간 영역의 모든 픽셀에서 유래한다는 것입니다. 다시 말하면, 공간 영역의 이미지가 주파수 영역의 각 픽셀에 이미지로 인코딩되어 있습니다.\u003c/p\u003e\n\u003ch2\u003eSpectrum Transform\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_4.png\" alt=\"그림\"\u003e\u003c/p\u003e\n\u003cp\u003eFourier Transform 아이디어를 따라가면서 한 단계 더 들어가 봅시다.\u003c/p\u003e\n\u003cp\u003e푸리에 변환은 스펙트럴 변환이라는 블록에서 사용됩니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e스펙트럴 변환은 표준 합성곱 블록 (Convolution-BatchNorm-ReLU)으로 시작합니다.\u003c/li\u003e\n\u003cli\u003e이어서 실수값 고속 푸리에 변환(Real FFT, 푸리에 변환의 한 변형)이 적용되어 특징 맵을 주파수로 변환합니다. 실수값 고속 푸리에 변환에서 사용되는 주파수는 절반뿐입니다.\u003c/li\u003e\n\u003cli\u003e주파수 영역의 특징 맵에 두 번째 표준 합성곱 블록을 적용합니다.\u003c/li\u003e\n\u003cli\u003e마지막으로 역 고속 푸리에 변환을 적용하여 특징 맵을 다시 공간 영역으로 변환합니다. 스펙트럼 변환은 채널 수를 두 배로 늘리기 위해 1x1 합성곱으로 끝납니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSpectralTransform\u003c/span\u003e(nn.\u003cspan class=\"hljs-property\"\u003eModule\u003c/span\u003e):\n\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, in_channels, out_channels, stride=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, groups=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, enable_lfu=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e, **fu_kwargs):\n        # bn_layer 사용하지 않음\n        \u003cspan class=\"hljs-variable language_\"\u003esuper\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eSpectralTransform\u003c/span\u003e, self).\u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e()\n        self.\u003cspan class=\"hljs-property\"\u003eenable_lfu\u003c/span\u003e = enable_lfu\n        \n        self.\u003cspan class=\"hljs-property\"\u003edownsample\u003c/span\u003e = nn.\u003cspan class=\"hljs-title class_\"\u003eIdentity\u003c/span\u003e()\n\n        self.\u003cspan class=\"hljs-property\"\u003econv1\u003c/span\u003e = nn.\u003cspan class=\"hljs-title class_\"\u003eSequential\u003c/span\u003e(\n            nn.\u003cspan class=\"hljs-title class_\"\u003eConv2\u003c/span\u003ed(in_channels, out_channels \u003cspan class=\"hljs-comment\"\u003e// 2, kernel_size=1, groups=groups, bias=False),\u003c/span\u003e\n            nn.\u003cspan class=\"hljs-title class_\"\u003eBatchNorm2\u003c/span\u003ed(out_channels \u003cspan class=\"hljs-comment\"\u003e// 2),\u003c/span\u003e\n            nn.\u003cspan class=\"hljs-title class_\"\u003eReLU\u003c/span\u003e(inplace=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n        )\n\n        self.\u003cspan class=\"hljs-property\"\u003efu\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eFourierUnit\u003c/span\u003e(out_channels \u003cspan class=\"hljs-comment\"\u003e// 2, out_channels // 2, groups, **fu_kwargs)\u003c/span\u003e\n        \n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003eenable_lfu\u003c/span\u003e:\n            self.\u003cspan class=\"hljs-property\"\u003elfu\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eFourierUnit\u003c/span\u003e(out_channels \u003cspan class=\"hljs-comment\"\u003e// 2, out_channels // 2, groups)\u003c/span\u003e\n        \n        self.\u003cspan class=\"hljs-property\"\u003econv2\u003c/span\u003e = torch.\u003cspan class=\"hljs-property\"\u003enn\u003c/span\u003e.\u003cspan class=\"hljs-title class_\"\u003eConv2\u003c/span\u003ed(out_channels \u003cspan class=\"hljs-comment\"\u003e// 2, out_channels, kernel_size=1, groups=groups, bias=False)\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(self, x):\n\n        x = self.\u003cspan class=\"hljs-title function_\"\u003edownsample\u003c/span\u003e(x)\n        x = self.\u003cspan class=\"hljs-title function_\"\u003econv1\u003c/span\u003e(x).  # 논문: [\u003cspan class=\"hljs-title class_\"\u003eConv\u003c/span\u003e-\u003cspan class=\"hljs-variable constant_\"\u003eBN\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eReLU\u003c/span\u003e]\n        output = self.\u003cspan class=\"hljs-title function_\"\u003efu\u003c/span\u003e(x) # 논문: [\u003cspan class=\"hljs-title class_\"\u003eReal\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eFFT2\u003c/span\u003ed - \u003cspan class=\"hljs-title class_\"\u003eConv\u003c/span\u003e-\u003cspan class=\"hljs-variable constant_\"\u003eBN\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eReLU\u003c/span\u003e - \u003cspan class=\"hljs-title class_\"\u003eInv\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eReal\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eFFT2\u003c/span\u003ed]\n\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003eenable_lfu\u003c/span\u003e:\n            n, c, h, w = x.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e\n            split_no = \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e\n            split_s = h \u003cspan class=\"hljs-comment\"\u003e// split_no\u003c/span\u003e\n            xs = torch.\u003cspan class=\"hljs-title function_\"\u003ecat\u003c/span\u003e(torch.\u003cspan class=\"hljs-title function_\"\u003esplit\u003c/span\u003e(x[:, :c \u003cspan class=\"hljs-comment\"\u003e// 4], split_s, dim=-2), dim=1).contiguous()\u003c/span\u003e\n            xs = torch.\u003cspan class=\"hljs-title function_\"\u003ecat\u003c/span\u003e(torch.\u003cspan class=\"hljs-title function_\"\u003esplit\u003c/span\u003e(xs, split_s, dim=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e),dim=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003econtiguous\u003c/span\u003e()\n            xs = self.\u003cspan class=\"hljs-title function_\"\u003elfu\u003c/span\u003e(xs)\n            xs = xs.\u003cspan class=\"hljs-title function_\"\u003erepeat\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, split_no, split_no).\u003cspan class=\"hljs-title function_\"\u003econtiguous\u003c/span\u003e()\n        \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n            xs = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\n        output = self.\u003cspan class=\"hljs-title function_\"\u003econv2\u003c/span\u003e(x + output + xs). # 논문: [\u003cspan class=\"hljs-title class_\"\u003eConv\u003c/span\u003e 1x1]\n\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e output\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e위 코드 블록에 구현 내용이 있습니다. 함수 forward() 내 인라인 코멘트는 Figure 5의 직사각형 블록에 대한 매핑을 설명합니다. 이 코드는 이해를 돕기 위해 기본 구성을 사용하여 공식 저장소²에서 간소화되었음을 참고해주세요.\u003c/p\u003e\n\u003ch2\u003eFast Fourier Convolution (FFC)\u003c/h2\u003e\n\u003cp\u003e이제 Figure 5의 왼쪽으로 이동하여 FFC의 개념이 그려진 곳으로 이동합시다. FFC는 여러 네트워크 연산자로 구성된 블록 모듈입니다. 입력 피처 맵은 글로벌 및 로컬 브랜치를 통해 분할되어 전달됩니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e로컬 브랜치: 로컬 브랜치는 일반 컨볼루션 프로세스를 따릅니다: 컨볼루션 - 배치 정규화 - 활성화 함수.\u003c/li\u003e\n\u003cli\u003e글로벌 브랜치: 방금 살펴본 제안된 스펙트럼 변환은 글로벌 브랜치에서 적용되며 로컬 브랜치의 동일한 컨볼루션 프로세스와 함께 사용됩니다. 표준 컨볼루션 블록과 제안된 FFT 기반 블록을 결합하여 글로벌 브랜치의 출력은 지역적 피처와 전체 이미지 구조를 모두 담고 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e로컬 및 글로벌 브랜치의 출력은 FFC의 끝에서 연결됩니다. 라마의 기본 아키텍처에서는 9개의 FFC가 구성되어 있습니다. FFC 초기화 스크립트는 아래와 같습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e### \u003cspan class=\"hljs-title class_\"\u003eResnet\u003c/span\u003e 블록\n# n_blocks = \u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(n_blocks):\n    cur_resblock = \u003cspan class=\"hljs-title class_\"\u003eFFCResnetBlock\u003c/span\u003e(feats_num_bottleneck, padding_type=padding_type, activation_layer=activation_layer,\n                                          norm_layer=norm_layer, **resnet_conv_kwargs)\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e spatial_transform_layers is not \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e and i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003espatial_transform_layers\u003c/span\u003e:\n       cur_resblock = \u003cspan class=\"hljs-title class_\"\u003eLearnableSpatialTransformWrapper\u003c/span\u003e(cur_resblock, **spatial_transform_kwargs)\n       model += [cur_resblock]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e손실 함수\u003c/h2\u003e\n\u003cp\u003e높은 수용 영역 지각 손실 (HRFPL)\n네트워크의 초기 레이어에서 FFC에 참여하는 것 외에도, LaMa의 또 다른 혁신은 새로운 손실 함수에 있습니다: 높은 수용 영역 지각 손실 (HRFPL).\u003c/p\u003e\n\u003cp\u003eHRFPL은 큰 마스크를 가진 샘플이 가시 영역의 컨텍스트가 충분하지 않아 세부 정보를 사용하여 누락된 부분을 복원할 수 없다는 가정에서 나옵니다. 입력 및 출력 이미지 사이의 철저한 픽셀별 비교는 불필요합니다. 대신, 네트워크는 효율적인 네트워크 ϕ에 의해 추출된 상위 수준의 컨텍스트를 살펴볼 수 있습니다. 더 간결한 아키텍처로, 수용 영역은 순전파하는 동안 더 빨리 성장합니다 (레이어가 적을수록 수용 영역이 더 빨리 성장합니다). 공식 구현에서는 Vgg19를 사용하여 이미지 특징을 추출합니다. HRFPL의 공식은 그림 6에서 찾을 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 참조: \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/advimman/lama/blob/3197c5e5e42503a66868e636ed48a7cefc5e8c28/saicinpainting/training/losses/perceptual.py#L67\u003c/span\u003e\n\n손실 = F.\u003cspan class=\"hljs-title function_\"\u003emse_loss\u003c/span\u003e(features_input, features_target, reduction=\u003cspan class=\"hljs-string\"\u003e'none'\u003c/span\u003e)\n손실 = loss.\u003cspan class=\"hljs-title function_\"\u003emean\u003c/span\u003e(dim=\u003cspan class=\"hljs-title function_\"\u003etuple\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(loss.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e)))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e차이를 계산하고 결과를 평균화하는 해당 Pytorch 스크립트를 위에서 찾을 수 있습니다.\u003c/p\u003e\n\u003cp\u003e적대적 손실\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_6.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e다른 GAN과 유사하게, LaMa에서 생성자와 구분자 네트워크 간의 관계를 구축하기 위해 적대적 손실이 적용됩니다. LaMa의 적대적 손실은 구분자가 실제 영역에서 생성된 콘텐츠를 더 잘 식별하도록 장려하고, 생성자에게 마스크된 영역 내에서 더 현실적인 콘텐츠를 생성하도록 피드백하는 것을 목표로 합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 참조: \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/advimman/lama/blob/3197c5e5e42503a66868e636ed48a7cefc5e8c28/saicinpainting/training/trainers/default.py#L115\u003c/span\u003e\ndiscr_real_pred, discr_real_features = self.\u003cspan class=\"hljs-title function_\"\u003ediscriminator\u003c/span\u003e(img)\ndiscr_fake_pred, discr_fake_features = self.\u003cspan class=\"hljs-title function_\"\u003ediscriminator\u003c/span\u003e(predicted_img)\nadv_gen_loss, adv_metrics = self.\u003cspan class=\"hljs-property\"\u003eadversarial_loss\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003egenerator_loss\u003c/span\u003e(real_batch=img, fake_batch=predicted_img, discr_real_pred=discr_real_pred, discr_fake_pred=discr_fake_pred, mask=mask_for_discr)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ediscriminator의 입력은 원본 이미지 x와 inpainted 이미지 x̂입니다. discriminator는 각 픽셀의 클래스를 real 또는 fake로 예측합니다. Adversarial Loss는 생성기 손실과 discriminator 손실을 결합하며 다른 기존 GAN과 동일한 개념을 공유합니다. 자세한 내용은 아래의 방정식에서 확인할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_7.png\" alt=\"equation_7\"\u003e\u003c/p\u003e\n\u003cp\u003e총 손실\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview_8.png\" alt=\"equation_8\"\u003e\u003c/p\u003e\n\u003cp\u003e손실 함수를 마무리하는 시간이에요. 제안된 HRFPL 및 적대적 손실은 대형 마스크 케이스에서 정확도 측면에서 LaMa를 다른 인풋 네트워크들과 구분짓습니다. 두 가지의 아이디어가 8번 그림에 나와 있어요. HRFPL 및 적대적 손실 이외에도 두 가지 일반적으로 사용되는 손실 함수가 포함돼 있어요.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e그래디언트 페널티\u003c/li\u003e\n\u003cli\u003e피처 매칭 손실: 판별자 네트워크의 피처에 대한 인식 손실\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e최종 손실은 네 가지 손실 함수의 가중 합으로 이루어져요.\u003c/p\u003e\n\u003cp\u003e이 모든 것으로 LaMa가 두드러지는 점에 관한 이야기는 여기까지입니다.\u003c/p\u003e\n\u003cp\u003e간략히 말하면, LaMa는 Fast Fourier Convolution (FFC)을 통합하고 고수용 필드 지각 손실 (HRFPL)로 안내되는, 가벼운 Resnet과 유사한 네트워크입니다. 이는 채워져야 하는 대상 영역이 더 큰 경우에 특히 강력합니다. LaMa는 이미지 인페인팅 분야에서의 진전을 나타내며, 다양한 해상도에서 견고한 솔루션을 제공하고 어려운 인페인팅 시나리오를 처리할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e다른 주제들도 원본 논문에서 논의되었습니다. 예를 들어, LaMa는 작은 256x256 이미지로 모델을 훈련하더라도 높은 해상도의 영역을 복원할 수 있다는 내용이 포함되어 있습니다. LaMa에 대해 더 알아보려면 원본 논문을 읽어보세요.\u003c/p\u003e\n\u003ch1\u003e요약\u003c/h1\u003e\n\u003cp\u003e이 기사에서 LaMa의 아이디어를 다뤘어요. 이미지 인페인팅에서 인기 있는 네트워크 중 하나로, 방문할 가치가 있는 많은 커뮤니티 자원이 있어요. 일부 온라인 서비스는 람마의 기능을 사용할 수 있는 훌륭한 인터페이스를 제공해요.\u003c/p\u003e\n\u003cp\u003e늘 읽어 주셔서 감사합니다. 피드백과 의견은 언제나 환영해요. 모두에게 멋진 하루가 되길 바라요!\u003c/p\u003e\n\u003ch1\u003e참고 자료\u003c/h1\u003e\n\u003cp\u003e[1] Suvorov, R., Logacheva, E., Mashikhin, A., Remizova, A., Ashukha, A., Silvestrov, A., Kong, N., Goka, H., Park, K., \u0026#x26; Lempitsky, V. (2021). Resolution-robust Large Mask Inpainting with Fourier Convolutions. ArXiv. /abs/2109.07161\n[2] 공식 구현: \u003ca href=\"https://github.com/advimman/lama\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/advimman/lama\u003c/a\u003e\n[3] \u003ca href=\"https://developers.google.com/machine-learning/gan/loss\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://developers.google.com/machine-learning/gan/loss\u003c/a\u003e\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-20-ExploringLaMaResolution-robustLargeMaskInpaintingwithFourierConvolutionsABriefOverview"},"buildId":"FH3Qr-mLAesqA0X5IFRQr","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>