<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>AI의 역사 파트 1 인공지능의 기원과 발전 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-27-AHistoryofAIPart1" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="AI의 역사 파트 1 인공지능의 기원과 발전 | itposting" data-gatsby-head="true"/><meta property="og:title" content="AI의 역사 파트 1 인공지능의 기원과 발전 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-27-AHistoryofAIPart1_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-27-AHistoryofAIPart1" data-gatsby-head="true"/><meta name="twitter:title" content="AI의 역사 파트 1 인공지능의 기원과 발전 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-27-AHistoryofAIPart1_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-27 19:02" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">AI의 역사 파트 1 인공지능의 기원과 발전</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="AI의 역사 파트 1 인공지능의 기원과 발전" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 27, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-27-AHistoryofAIPart1&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>인공지능 연구 논문</h2>
<p>이 기사는 제가 인공지능의 역사를 소개하는 연속 기사 시리즈 중 첫 번째로, 이 분야에서 가장 중요한 연구 논문들을 검토해 소개합니다.</p>
<ul>
<li>퍼셉트론</li>
</ul>
<p>퍼셉트론: 뇌 내 정보 저장 및 조직화를 위한 확률 모델, Frank Rosenblatt (1958)</p>
<div class="content-ad"></div>
<p>이 논문은 더 복잡한 신경망(예: 심층 신경망) 및 기계 학습 알고리즘의 개발을 위한 기초를 마련했습니다. 기계가 데이터로부터 학습할 수 있는 방법을 보여주었습니다.</p>
<p>퍼셉트론은 인공 신경망의 기본 단위입니다. 입력 데이터를 기반으로 가중치를 조정하여 학습하고 결정을 내릴 수 있습니다.</p>
<h1>역전파</h1>
<p>오류를 역전파함으로써 표현을 학습하는 것, David E. Rumelhart, Geoffrey E. Hinton &#x26; Ronald J. Williams (1986)</p>
<div class="content-ad"></div>
<p>이 논문은 역전파 알고리즘을 소개함으로써 인공 신경망 분야를 혁신하였습니다. 이 알고리즘은 다층 신경망(다층 뉴런으로 이루어진 신경 네트워크)을 학습할 수 있도록 하여 딥 러닝의 발전을 이끌었습니다.</p>
<p>역전파는 신경망을 학습시키는 학습 절차로, 실제 출력 벡터와 목표 출력 벡터 사이의 차이를 최소화하도록 연결 가중치를 조정합니다.</p>
<p>그 중요성은 내부 은닉 유닛이 작업 도메인의 중요한 측면을 나타내도록 하는 유용한 새로운 기능을 생성할 수 있는 능력에 있으며, 퍼셉트론 수렴 절차와 같은 이전 방법과 구별됩니다.</p>
<p>효율적인 역전파 구현은 대게 미분법의 연쇄 법칙을 통해 가능하며, 네트워크의 가중치를 조정하는 데 필요한 그래디언트를 체계적으로 계산할 수 있게 합니다.</p>
<div class="content-ad"></div>
<h1>의사 결정 트리</h1>
<p>의사 결정 트리의 유도, J. R. 퀸란 (1986)</p>
<p>이 논문은 ID3 알고리즘을 소개하여 결정 트리를 구축하는 데 중요한 방법으로 사용되었고, 결정 트리 알고리즘을 개선하여 소음이나 불완전한 데이터를 처리하는 연구를 촉진하는 데 영향을 미쳤습니다.</p>
<p>의사 결정 트리는 입력 데이터의 특징에서 유도된 규칙의 계층 구조에 기반하여 결정을 내리는 분류 및 회귀 작업에 사용되는 모델입니다.</p>
<div class="content-ad"></div>
<p>결정 트리를 효율적으로 합성하는 것은 정확도와 계산 자원을 최적화하는 방식으로 트리를 구성하는 것을 의미하며, 소음 및 불완전한 정보를 효과적으로 처리합니다.</p>
<p>ID3(반복 이분화기 3)는 정보 이득을 최대화하는 속성을 기반으로 데이터를 재귀적으로 분할하는 탑다운, 탐욕적 방법을 사용하여 결정 트리를 생성하는 알고리즘입니다.</p>
<p>정보 이득은 데이터 집합을 분류하는 데 속성의 효과를 측정하는 데 사용되며, 데이터 집합이 해당 속성을 기반으로 분할될 때 엔트로피(불확실성)의 감소를 양적으로 측정합니다. 정보 이득을 최대화하는 것은 엔트로피의 최대 감소를 제공하는 속성을 선택함으로써 결정 트리의 정확도를 향상시키는 것을 의미합니다.</p>
<h1>Hidden Markov Models</h1>
<div class="content-ad"></div>
<p>음성인식의 숨겨진 마르코프 모델 및 선택된 응용 프로그램에 대한 교재, L.R. 라비너 (1989)</p>
<p>이 논문은 숨겨진 마르코프 모델(HMMs)에 대한 포괄적이고 실용적인 가이드를 제공하여 연구자와 실무자가 음성인식 및 기타 분야에서 이를 더 쉽게 접근하고 이해할 수 있도록했습니다.</p>
<p>이산 마르코프 체인은 현재 상태에만 의존하며 이전 상태에 의존하지 않는 상태 시퀀스로 구성된 확률 과정인 확률적 과정입니다.</p>
<p>숨겨진 마르코프 모델(HMM)은 모델링되는 시스템이 감지되지 않은(숨겨진) 상태로 마르코프 프로세스를 따른다고 가정하는 통계 모델로, 관찰된 데이터가 이러한 숨겨진 상태와 확률적으로 연결되어 있다는 직관을 가지고 있습니다.</p>
<div class="content-ad"></div>
<p>HMM의 세 가지 기본 문제는 다음과 같습니다: 평가 문제(관측된 시퀀스의 확률 계산), 디코딩 문제(숨겨진 상태의 가장 가능성 있는 시퀀스 결정), 학습 문제(데이터로부터 모델 매개변수 추정).</p>
<p>Rabiner은 음성 신호를 숨겨진 상태의 시퀀스로 모델링하여 음성 인식에 HMM을 적용했습니다. 이를 통해 언어의 최소 음성 단위인 음소를 나타내는 숨겨진 상태의 시퀀스로 연속된 말의 확률적 디코딩 및 인식이 가능해졌습니다.</p>
<h1>다층 피드포워드 네트워크</h1>
<p>다층 피드포워드 네트워크는 1989년 Kurt Hornik, Maxwell Stinchcombe, Halbert White에 의해 제안된 보편적 근사자입니다.</p>
<div class="content-ad"></div>
<p>이 논문은 신경망의 범용 근사 능력에 대한 이론적 기반을 수립하여, 심지어 단순한 신경망 구조조차 복잡한 함수를 원하는 정확도로 근사할 수 있다는 것을 입증했습니다. 이로 인해 다양한 응용 프로그램에서 신경망의 광범위한 사용과 개발이 가능해졌습니다.</p>
<p>표준 다층 피드포워드 네트워크는 노드 사이의 연결이 순환을 형성하지 않는 인공 신경망의 한 종류로, 입력 레이어, 하나 이상의 은닉 레이어 및 출력 레이어로 구성됩니다.</p>
<p>보렬 측정 가능 함수는 한 위상 공간의 요소를 다른 위상 공간으로 매핑하는 함수로, 어떤 보렬 집합의 역사 이미지도 보렬 집합이 되도록 하는 함수입니다. 직관적으로 보렬 측정 가능성은 함수가 예측 가능하고 측정 가능한 방식으로 작동함을 보장합니다.</p>
<p>임의의 압축 함수는 신경망에서 사용되는 활성화 함수의 한 유형으로, 실수 값 입력을 유계 출력에 매핑하여 일반적으로 입력을 유한 범위로 압축합니다. 이 함수에는 시그모이드 또는 tanh 함수가 포함됩니다.</p>
<div class="content-ad"></div>
<p>유니버설 근사기는 충분한 자원(예: 신경망의 충분한 숨겨진 유닛과 같은)가 주어진 경우, 어떤 함수든 원하는 수준의 정확도로 근사할 수 있는 모델 또는 수학적인 함수입니다.</p>
<h1>서포트 벡터 머신</h1>
<p>Bernhard E. Boser, Isabelle M. Guyon, 그리고 Vladimir N. Vapnik (1992)의 최적의 마진 분류기를 위한 훈련 알고리즘입니다.</p>
<p>본 논문은 서포트 벡터 머신(Support Vector Machines, SVMs)을 소개했는데, 이는 클래스 간의 여백을 최대화하고 모델 복잡성을 자동으로 조절하여 광범위한 응용 프로그램(예: 광학 문자 인식)에서 일반화 및 견고성을 향상시킬 수 있는 분류 작업의 기본 도구로 자리 잡았습니다.</p>
<div class="content-ad"></div>
<p>서포트 벡터 머신(Support Vector Machine, SVM)은 분류 및 회귀에 사용되는 지도 학습 알고리즘으로, 서로 다른 클래스 간의 간격을 최대화하는 최적의 초평면을 찾습니다.</p>
<p>SVM 알고리즘은 지원 패턴을 이용해 클래스 간 간격을 최대화하는 초평면을 식별하고, 커널 함수를 사용하여 비선형 분류 문제를 다룰 수 있습니다.</p>
<p>간격은 의사결정 경계(서로 다른 클래스를 구분하는 선 또는 표면)와 가장 가까운 훈련 패턴 간의 거리를 나타내며, 일반적으로 더 큰 간격은 더 나은 일반화를 이끌어냅니다. 지원 패턴은 의사결정 경계에 가장 가까이 위치한 훈련 패턴으로, 경계의 위치와 방향에 직접적인 영향을 미칩니다.</p>
<p>Leave-one-out 방법은 교차 검증 기술 중 하나로, 모델은 특정 훈련 패턴을 제외하고 학습되며, 이 프로세스는 각 패턴에 대해 반복되어 모델의 일반화 성능을 추정합니다.</p>
<div class="content-ad"></div>
<p>VC-차원(밥닉-체르보넨키스 차원)은 일련의 함수 집합의 용량 또는 복잡성을 측정하는 것으로, 모델에 의해 완벽하게 분류된 포인트의 최대 개수를 나타냅니다.</p>
<h2>Bagging</h2>
<p>백팩 예측자, 레오 브레이먼(1996)</p>
<p>이 논문은 앙상블 방법론의 개념을 도입하여 예측 정확도를 향상시키는 여러 모델을 결합하는 방법에 영향을 주었으며, 부트스트랩 반복 및 예측자 불안정성의 중요성을 강조함으로써 기계 학습 분야에 상당한 영향을 미쳤습니다.</p>
<div class="content-ad"></div>
<p>기계 학습에서의 앙상블 방법은 단일 모델 사용보다 전체 예측 정확도와 견고성을 향상시키기 위해 여러 모델을 결합하는 기술입니다.</p>
<p>배깅 (부트스트랩 집계)은 예측기의 여러 버전을 생성하고 이를 사용하여 정확도를 향상시킨 집계 예측기를 만드는 방법입니다.</p>
<p>배깅은 데이터의 서로 다른 하위 집합에서 훈련된 각 모델의 여러 버전을 평균화하여 예측의 분산을 줄이고 노이즈를 제거하며 과적합을 완화하기 때문에 작동합니다.</p>
<p>부트스트래핑은 데이터 집합에서 대체 샘플링을 통해 여러 새로운 훈련 세트를 생성하는 통계적 방법이며, 부트스트랩 복제본은 이러한 새로운 훈련 세트 중 하나를 의미합니다.</p>
<div class="content-ad"></div>
<h1>합성곱 신경망</h1>
<p>Yann LeCun, Léon Bottou, Yoshua Bengio, and P. Haffner이(가) 쓴 article인 "Gradient-based learning applied to document recognition" (1998)</p>
<p>이 논문은 합성곱 신경망(CNNs)의 우수한 성능을 증명하여 2차원 모양을 인식하는 데 특히 필기 문자 인식에 영향을 미쳤고, 복잡하고 다중 모듈 시스템을 교육하기 위한 그래프 변환 신경망(GTNs) 개념을 소개하여 문서 인식 및 다른 실용적인 응용 분야의 발전을 이끌었습니다.</p>
<p>합성곱 연산은 입력(예: 이미지)에 필터를 적용하여 입력의 특정 측면(예: 이미지의 가장자리)을 강조하는 출력 feature map을 생성하는 수학적 과정입니다.</p>
<div class="content-ad"></div>
<p>합성곱 신경망(CNNs)은 그리드 형태의 위상을 가진 데이터, 예를 들어 이미지와 같은 데이터를 처리하고 분석하기 위해 특별히 설계된 심층 신경망의 한 종류입니다. 합성 계층을 사용하여 공간적으로 계층화된 특징들을 자동적으로 적응적으로 학습합니다.</p>
<p>고차원 패턴이라는 용어는 많은 속성 또는 특징을 가진 데이터로, 이로 인해 분류나 분석이 복잡하고 어렵습니다. 예를 들어 손글씨 문자와 같은 것이 있습니다.</p>
<p>그래프 변환 네트워크(GTNs)는 경사 기반 방법을 사용하여 전체적인 성능 척도를 최적화하기 위해 여러 모듈 시스템을 전역적으로 훈련시키는 학습 패러다임의 한 종류입니다.</p>
<p>전역 훈련은 시스템의 여러 구성 요소 또는 모듈을 통합된 목표를 사용하여 동시에 훈련시키는 프로세스로, 시스템 전체를 단독 부분이 아닌 통합적으로 최적화할 수 있도록 합니다.</p>
<div class="content-ad"></div>
<p>읽어 주셔서 감사합니다! 피드백을 환영합니다! 특히 중요한 연구를 놓치지 않았는지 생각하신다면 더욱 감사하겠습니다.</p>
<p><img src="/assets/img/2024-06-27-AHistoryofAIPart1_0.png" alt="A History of AI Part 1"></p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"AI의 역사 파트 1 인공지능의 기원과 발전","description":"","date":"2024-06-27 19:02","slug":"2024-06-27-AHistoryofAIPart1","content":"\n\n## 인공지능 연구 논문\n\n이 기사는 제가 인공지능의 역사를 소개하는 연속 기사 시리즈 중 첫 번째로, 이 분야에서 가장 중요한 연구 논문들을 검토해 소개합니다.\n\n- 퍼셉트론\n\n퍼셉트론: 뇌 내 정보 저장 및 조직화를 위한 확률 모델, Frank Rosenblatt (1958)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 논문은 더 복잡한 신경망(예: 심층 신경망) 및 기계 학습 알고리즘의 개발을 위한 기초를 마련했습니다. 기계가 데이터로부터 학습할 수 있는 방법을 보여주었습니다.\n\n퍼셉트론은 인공 신경망의 기본 단위입니다. 입력 데이터를 기반으로 가중치를 조정하여 학습하고 결정을 내릴 수 있습니다.\n\n# 역전파\n\n오류를 역전파함으로써 표현을 학습하는 것, David E. Rumelhart, Geoffrey E. Hinton \u0026 Ronald J. Williams (1986)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 논문은 역전파 알고리즘을 소개함으로써 인공 신경망 분야를 혁신하였습니다. 이 알고리즘은 다층 신경망(다층 뉴런으로 이루어진 신경 네트워크)을 학습할 수 있도록 하여 딥 러닝의 발전을 이끌었습니다.\n\n역전파는 신경망을 학습시키는 학습 절차로, 실제 출력 벡터와 목표 출력 벡터 사이의 차이를 최소화하도록 연결 가중치를 조정합니다.\n\n그 중요성은 내부 은닉 유닛이 작업 도메인의 중요한 측면을 나타내도록 하는 유용한 새로운 기능을 생성할 수 있는 능력에 있으며, 퍼셉트론 수렴 절차와 같은 이전 방법과 구별됩니다.\n\n효율적인 역전파 구현은 대게 미분법의 연쇄 법칙을 통해 가능하며, 네트워크의 가중치를 조정하는 데 필요한 그래디언트를 체계적으로 계산할 수 있게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 의사 결정 트리\n\n의사 결정 트리의 유도, J. R. 퀸란 (1986)\n\n이 논문은 ID3 알고리즘을 소개하여 결정 트리를 구축하는 데 중요한 방법으로 사용되었고, 결정 트리 알고리즘을 개선하여 소음이나 불완전한 데이터를 처리하는 연구를 촉진하는 데 영향을 미쳤습니다.\n\n의사 결정 트리는 입력 데이터의 특징에서 유도된 규칙의 계층 구조에 기반하여 결정을 내리는 분류 및 회귀 작업에 사용되는 모델입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결정 트리를 효율적으로 합성하는 것은 정확도와 계산 자원을 최적화하는 방식으로 트리를 구성하는 것을 의미하며, 소음 및 불완전한 정보를 효과적으로 처리합니다.\n\nID3(반복 이분화기 3)는 정보 이득을 최대화하는 속성을 기반으로 데이터를 재귀적으로 분할하는 탑다운, 탐욕적 방법을 사용하여 결정 트리를 생성하는 알고리즘입니다.\n\n정보 이득은 데이터 집합을 분류하는 데 속성의 효과를 측정하는 데 사용되며, 데이터 집합이 해당 속성을 기반으로 분할될 때 엔트로피(불확실성)의 감소를 양적으로 측정합니다. 정보 이득을 최대화하는 것은 엔트로피의 최대 감소를 제공하는 속성을 선택함으로써 결정 트리의 정확도를 향상시키는 것을 의미합니다.\n\n# Hidden Markov Models\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n음성인식의 숨겨진 마르코프 모델 및 선택된 응용 프로그램에 대한 교재, L.R. 라비너 (1989)\n\n이 논문은 숨겨진 마르코프 모델(HMMs)에 대한 포괄적이고 실용적인 가이드를 제공하여 연구자와 실무자가 음성인식 및 기타 분야에서 이를 더 쉽게 접근하고 이해할 수 있도록했습니다.\n\n이산 마르코프 체인은 현재 상태에만 의존하며 이전 상태에 의존하지 않는 상태 시퀀스로 구성된 확률 과정인 확률적 과정입니다.\n\n숨겨진 마르코프 모델(HMM)은 모델링되는 시스템이 감지되지 않은(숨겨진) 상태로 마르코프 프로세스를 따른다고 가정하는 통계 모델로, 관찰된 데이터가 이러한 숨겨진 상태와 확률적으로 연결되어 있다는 직관을 가지고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nHMM의 세 가지 기본 문제는 다음과 같습니다: 평가 문제(관측된 시퀀스의 확률 계산), 디코딩 문제(숨겨진 상태의 가장 가능성 있는 시퀀스 결정), 학습 문제(데이터로부터 모델 매개변수 추정).\n\nRabiner은 음성 신호를 숨겨진 상태의 시퀀스로 모델링하여 음성 인식에 HMM을 적용했습니다. 이를 통해 언어의 최소 음성 단위인 음소를 나타내는 숨겨진 상태의 시퀀스로 연속된 말의 확률적 디코딩 및 인식이 가능해졌습니다.\n\n# 다층 피드포워드 네트워크\n\n다층 피드포워드 네트워크는 1989년 Kurt Hornik, Maxwell Stinchcombe, Halbert White에 의해 제안된 보편적 근사자입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 논문은 신경망의 범용 근사 능력에 대한 이론적 기반을 수립하여, 심지어 단순한 신경망 구조조차 복잡한 함수를 원하는 정확도로 근사할 수 있다는 것을 입증했습니다. 이로 인해 다양한 응용 프로그램에서 신경망의 광범위한 사용과 개발이 가능해졌습니다.\n\n표준 다층 피드포워드 네트워크는 노드 사이의 연결이 순환을 형성하지 않는 인공 신경망의 한 종류로, 입력 레이어, 하나 이상의 은닉 레이어 및 출력 레이어로 구성됩니다.\n\n보렬 측정 가능 함수는 한 위상 공간의 요소를 다른 위상 공간으로 매핑하는 함수로, 어떤 보렬 집합의 역사 이미지도 보렬 집합이 되도록 하는 함수입니다. 직관적으로 보렬 측정 가능성은 함수가 예측 가능하고 측정 가능한 방식으로 작동함을 보장합니다.\n\n임의의 압축 함수는 신경망에서 사용되는 활성화 함수의 한 유형으로, 실수 값 입력을 유계 출력에 매핑하여 일반적으로 입력을 유한 범위로 압축합니다. 이 함수에는 시그모이드 또는 tanh 함수가 포함됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n유니버설 근사기는 충분한 자원(예: 신경망의 충분한 숨겨진 유닛과 같은)가 주어진 경우, 어떤 함수든 원하는 수준의 정확도로 근사할 수 있는 모델 또는 수학적인 함수입니다.\n\n# 서포트 벡터 머신\n\nBernhard E. Boser, Isabelle M. Guyon, 그리고 Vladimir N. Vapnik (1992)의 최적의 마진 분류기를 위한 훈련 알고리즘입니다.\n\n본 논문은 서포트 벡터 머신(Support Vector Machines, SVMs)을 소개했는데, 이는 클래스 간의 여백을 최대화하고 모델 복잡성을 자동으로 조절하여 광범위한 응용 프로그램(예: 광학 문자 인식)에서 일반화 및 견고성을 향상시킬 수 있는 분류 작업의 기본 도구로 자리 잡았습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n서포트 벡터 머신(Support Vector Machine, SVM)은 분류 및 회귀에 사용되는 지도 학습 알고리즘으로, 서로 다른 클래스 간의 간격을 최대화하는 최적의 초평면을 찾습니다.\n\nSVM 알고리즘은 지원 패턴을 이용해 클래스 간 간격을 최대화하는 초평면을 식별하고, 커널 함수를 사용하여 비선형 분류 문제를 다룰 수 있습니다.\n\n간격은 의사결정 경계(서로 다른 클래스를 구분하는 선 또는 표면)와 가장 가까운 훈련 패턴 간의 거리를 나타내며, 일반적으로 더 큰 간격은 더 나은 일반화를 이끌어냅니다. 지원 패턴은 의사결정 경계에 가장 가까이 위치한 훈련 패턴으로, 경계의 위치와 방향에 직접적인 영향을 미칩니다.\n\nLeave-one-out 방법은 교차 검증 기술 중 하나로, 모델은 특정 훈련 패턴을 제외하고 학습되며, 이 프로세스는 각 패턴에 대해 반복되어 모델의 일반화 성능을 추정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nVC-차원(밥닉-체르보넨키스 차원)은 일련의 함수 집합의 용량 또는 복잡성을 측정하는 것으로, 모델에 의해 완벽하게 분류된 포인트의 최대 개수를 나타냅니다.\n\n## Bagging \n\n백팩 예측자, 레오 브레이먼(1996)\n\n이 논문은 앙상블 방법론의 개념을 도입하여 예측 정확도를 향상시키는 여러 모델을 결합하는 방법에 영향을 주었으며, 부트스트랩 반복 및 예측자 불안정성의 중요성을 강조함으로써 기계 학습 분야에 상당한 영향을 미쳤습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기계 학습에서의 앙상블 방법은 단일 모델 사용보다 전체 예측 정확도와 견고성을 향상시키기 위해 여러 모델을 결합하는 기술입니다.\n\n배깅 (부트스트랩 집계)은 예측기의 여러 버전을 생성하고 이를 사용하여 정확도를 향상시킨 집계 예측기를 만드는 방법입니다.\n\n배깅은 데이터의 서로 다른 하위 집합에서 훈련된 각 모델의 여러 버전을 평균화하여 예측의 분산을 줄이고 노이즈를 제거하며 과적합을 완화하기 때문에 작동합니다.\n\n부트스트래핑은 데이터 집합에서 대체 샘플링을 통해 여러 새로운 훈련 세트를 생성하는 통계적 방법이며, 부트스트랩 복제본은 이러한 새로운 훈련 세트 중 하나를 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 합성곱 신경망\n\nYann LeCun, Léon Bottou, Yoshua Bengio, and P. Haffner이(가) 쓴 article인 \"Gradient-based learning applied to document recognition\" (1998)\n\n이 논문은 합성곱 신경망(CNNs)의 우수한 성능을 증명하여 2차원 모양을 인식하는 데 특히 필기 문자 인식에 영향을 미쳤고, 복잡하고 다중 모듈 시스템을 교육하기 위한 그래프 변환 신경망(GTNs) 개념을 소개하여 문서 인식 및 다른 실용적인 응용 분야의 발전을 이끌었습니다.\n\n합성곱 연산은 입력(예: 이미지)에 필터를 적용하여 입력의 특정 측면(예: 이미지의 가장자리)을 강조하는 출력 feature map을 생성하는 수학적 과정입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n합성곱 신경망(CNNs)은 그리드 형태의 위상을 가진 데이터, 예를 들어 이미지와 같은 데이터를 처리하고 분석하기 위해 특별히 설계된 심층 신경망의 한 종류입니다. 합성 계층을 사용하여 공간적으로 계층화된 특징들을 자동적으로 적응적으로 학습합니다. \n\n고차원 패턴이라는 용어는 많은 속성 또는 특징을 가진 데이터로, 이로 인해 분류나 분석이 복잡하고 어렵습니다. 예를 들어 손글씨 문자와 같은 것이 있습니다.\n\n그래프 변환 네트워크(GTNs)는 경사 기반 방법을 사용하여 전체적인 성능 척도를 최적화하기 위해 여러 모듈 시스템을 전역적으로 훈련시키는 학습 패러다임의 한 종류입니다.\n\n전역 훈련은 시스템의 여러 구성 요소 또는 모듈을 통합된 목표를 사용하여 동시에 훈련시키는 프로세스로, 시스템 전체를 단독 부분이 아닌 통합적으로 최적화할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n읽어 주셔서 감사합니다! 피드백을 환영합니다! 특히 중요한 연구를 놓치지 않았는지 생각하신다면 더욱 감사하겠습니다.\n\n![A History of AI Part 1](/assets/img/2024-06-27-AHistoryofAIPart1_0.png)","ogImage":{"url":"/assets/img/2024-06-27-AHistoryofAIPart1_0.png"},"coverImage":"/assets/img/2024-06-27-AHistoryofAIPart1_0.png","tag":["Tech"],"readingTime":6},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e인공지능 연구 논문\u003c/h2\u003e\n\u003cp\u003e이 기사는 제가 인공지능의 역사를 소개하는 연속 기사 시리즈 중 첫 번째로, 이 분야에서 가장 중요한 연구 논문들을 검토해 소개합니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e퍼셉트론\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e퍼셉트론: 뇌 내 정보 저장 및 조직화를 위한 확률 모델, Frank Rosenblatt (1958)\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 논문은 더 복잡한 신경망(예: 심층 신경망) 및 기계 학습 알고리즘의 개발을 위한 기초를 마련했습니다. 기계가 데이터로부터 학습할 수 있는 방법을 보여주었습니다.\u003c/p\u003e\n\u003cp\u003e퍼셉트론은 인공 신경망의 기본 단위입니다. 입력 데이터를 기반으로 가중치를 조정하여 학습하고 결정을 내릴 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e역전파\u003c/h1\u003e\n\u003cp\u003e오류를 역전파함으로써 표현을 학습하는 것, David E. Rumelhart, Geoffrey E. Hinton \u0026#x26; Ronald J. Williams (1986)\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 논문은 역전파 알고리즘을 소개함으로써 인공 신경망 분야를 혁신하였습니다. 이 알고리즘은 다층 신경망(다층 뉴런으로 이루어진 신경 네트워크)을 학습할 수 있도록 하여 딥 러닝의 발전을 이끌었습니다.\u003c/p\u003e\n\u003cp\u003e역전파는 신경망을 학습시키는 학습 절차로, 실제 출력 벡터와 목표 출력 벡터 사이의 차이를 최소화하도록 연결 가중치를 조정합니다.\u003c/p\u003e\n\u003cp\u003e그 중요성은 내부 은닉 유닛이 작업 도메인의 중요한 측면을 나타내도록 하는 유용한 새로운 기능을 생성할 수 있는 능력에 있으며, 퍼셉트론 수렴 절차와 같은 이전 방법과 구별됩니다.\u003c/p\u003e\n\u003cp\u003e효율적인 역전파 구현은 대게 미분법의 연쇄 법칙을 통해 가능하며, 네트워크의 가중치를 조정하는 데 필요한 그래디언트를 체계적으로 계산할 수 있게 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e의사 결정 트리\u003c/h1\u003e\n\u003cp\u003e의사 결정 트리의 유도, J. R. 퀸란 (1986)\u003c/p\u003e\n\u003cp\u003e이 논문은 ID3 알고리즘을 소개하여 결정 트리를 구축하는 데 중요한 방법으로 사용되었고, 결정 트리 알고리즘을 개선하여 소음이나 불완전한 데이터를 처리하는 연구를 촉진하는 데 영향을 미쳤습니다.\u003c/p\u003e\n\u003cp\u003e의사 결정 트리는 입력 데이터의 특징에서 유도된 규칙의 계층 구조에 기반하여 결정을 내리는 분류 및 회귀 작업에 사용되는 모델입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e결정 트리를 효율적으로 합성하는 것은 정확도와 계산 자원을 최적화하는 방식으로 트리를 구성하는 것을 의미하며, 소음 및 불완전한 정보를 효과적으로 처리합니다.\u003c/p\u003e\n\u003cp\u003eID3(반복 이분화기 3)는 정보 이득을 최대화하는 속성을 기반으로 데이터를 재귀적으로 분할하는 탑다운, 탐욕적 방법을 사용하여 결정 트리를 생성하는 알고리즘입니다.\u003c/p\u003e\n\u003cp\u003e정보 이득은 데이터 집합을 분류하는 데 속성의 효과를 측정하는 데 사용되며, 데이터 집합이 해당 속성을 기반으로 분할될 때 엔트로피(불확실성)의 감소를 양적으로 측정합니다. 정보 이득을 최대화하는 것은 엔트로피의 최대 감소를 제공하는 속성을 선택함으로써 결정 트리의 정확도를 향상시키는 것을 의미합니다.\u003c/p\u003e\n\u003ch1\u003eHidden Markov Models\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e음성인식의 숨겨진 마르코프 모델 및 선택된 응용 프로그램에 대한 교재, L.R. 라비너 (1989)\u003c/p\u003e\n\u003cp\u003e이 논문은 숨겨진 마르코프 모델(HMMs)에 대한 포괄적이고 실용적인 가이드를 제공하여 연구자와 실무자가 음성인식 및 기타 분야에서 이를 더 쉽게 접근하고 이해할 수 있도록했습니다.\u003c/p\u003e\n\u003cp\u003e이산 마르코프 체인은 현재 상태에만 의존하며 이전 상태에 의존하지 않는 상태 시퀀스로 구성된 확률 과정인 확률적 과정입니다.\u003c/p\u003e\n\u003cp\u003e숨겨진 마르코프 모델(HMM)은 모델링되는 시스템이 감지되지 않은(숨겨진) 상태로 마르코프 프로세스를 따른다고 가정하는 통계 모델로, 관찰된 데이터가 이러한 숨겨진 상태와 확률적으로 연결되어 있다는 직관을 가지고 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eHMM의 세 가지 기본 문제는 다음과 같습니다: 평가 문제(관측된 시퀀스의 확률 계산), 디코딩 문제(숨겨진 상태의 가장 가능성 있는 시퀀스 결정), 학습 문제(데이터로부터 모델 매개변수 추정).\u003c/p\u003e\n\u003cp\u003eRabiner은 음성 신호를 숨겨진 상태의 시퀀스로 모델링하여 음성 인식에 HMM을 적용했습니다. 이를 통해 언어의 최소 음성 단위인 음소를 나타내는 숨겨진 상태의 시퀀스로 연속된 말의 확률적 디코딩 및 인식이 가능해졌습니다.\u003c/p\u003e\n\u003ch1\u003e다층 피드포워드 네트워크\u003c/h1\u003e\n\u003cp\u003e다층 피드포워드 네트워크는 1989년 Kurt Hornik, Maxwell Stinchcombe, Halbert White에 의해 제안된 보편적 근사자입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 논문은 신경망의 범용 근사 능력에 대한 이론적 기반을 수립하여, 심지어 단순한 신경망 구조조차 복잡한 함수를 원하는 정확도로 근사할 수 있다는 것을 입증했습니다. 이로 인해 다양한 응용 프로그램에서 신경망의 광범위한 사용과 개발이 가능해졌습니다.\u003c/p\u003e\n\u003cp\u003e표준 다층 피드포워드 네트워크는 노드 사이의 연결이 순환을 형성하지 않는 인공 신경망의 한 종류로, 입력 레이어, 하나 이상의 은닉 레이어 및 출력 레이어로 구성됩니다.\u003c/p\u003e\n\u003cp\u003e보렬 측정 가능 함수는 한 위상 공간의 요소를 다른 위상 공간으로 매핑하는 함수로, 어떤 보렬 집합의 역사 이미지도 보렬 집합이 되도록 하는 함수입니다. 직관적으로 보렬 측정 가능성은 함수가 예측 가능하고 측정 가능한 방식으로 작동함을 보장합니다.\u003c/p\u003e\n\u003cp\u003e임의의 압축 함수는 신경망에서 사용되는 활성화 함수의 한 유형으로, 실수 값 입력을 유계 출력에 매핑하여 일반적으로 입력을 유한 범위로 압축합니다. 이 함수에는 시그모이드 또는 tanh 함수가 포함됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e유니버설 근사기는 충분한 자원(예: 신경망의 충분한 숨겨진 유닛과 같은)가 주어진 경우, 어떤 함수든 원하는 수준의 정확도로 근사할 수 있는 모델 또는 수학적인 함수입니다.\u003c/p\u003e\n\u003ch1\u003e서포트 벡터 머신\u003c/h1\u003e\n\u003cp\u003eBernhard E. Boser, Isabelle M. Guyon, 그리고 Vladimir N. Vapnik (1992)의 최적의 마진 분류기를 위한 훈련 알고리즘입니다.\u003c/p\u003e\n\u003cp\u003e본 논문은 서포트 벡터 머신(Support Vector Machines, SVMs)을 소개했는데, 이는 클래스 간의 여백을 최대화하고 모델 복잡성을 자동으로 조절하여 광범위한 응용 프로그램(예: 광학 문자 인식)에서 일반화 및 견고성을 향상시킬 수 있는 분류 작업의 기본 도구로 자리 잡았습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e서포트 벡터 머신(Support Vector Machine, SVM)은 분류 및 회귀에 사용되는 지도 학습 알고리즘으로, 서로 다른 클래스 간의 간격을 최대화하는 최적의 초평면을 찾습니다.\u003c/p\u003e\n\u003cp\u003eSVM 알고리즘은 지원 패턴을 이용해 클래스 간 간격을 최대화하는 초평면을 식별하고, 커널 함수를 사용하여 비선형 분류 문제를 다룰 수 있습니다.\u003c/p\u003e\n\u003cp\u003e간격은 의사결정 경계(서로 다른 클래스를 구분하는 선 또는 표면)와 가장 가까운 훈련 패턴 간의 거리를 나타내며, 일반적으로 더 큰 간격은 더 나은 일반화를 이끌어냅니다. 지원 패턴은 의사결정 경계에 가장 가까이 위치한 훈련 패턴으로, 경계의 위치와 방향에 직접적인 영향을 미칩니다.\u003c/p\u003e\n\u003cp\u003eLeave-one-out 방법은 교차 검증 기술 중 하나로, 모델은 특정 훈련 패턴을 제외하고 학습되며, 이 프로세스는 각 패턴에 대해 반복되어 모델의 일반화 성능을 추정합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eVC-차원(밥닉-체르보넨키스 차원)은 일련의 함수 집합의 용량 또는 복잡성을 측정하는 것으로, 모델에 의해 완벽하게 분류된 포인트의 최대 개수를 나타냅니다.\u003c/p\u003e\n\u003ch2\u003eBagging\u003c/h2\u003e\n\u003cp\u003e백팩 예측자, 레오 브레이먼(1996)\u003c/p\u003e\n\u003cp\u003e이 논문은 앙상블 방법론의 개념을 도입하여 예측 정확도를 향상시키는 여러 모델을 결합하는 방법에 영향을 주었으며, 부트스트랩 반복 및 예측자 불안정성의 중요성을 강조함으로써 기계 학습 분야에 상당한 영향을 미쳤습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e기계 학습에서의 앙상블 방법은 단일 모델 사용보다 전체 예측 정확도와 견고성을 향상시키기 위해 여러 모델을 결합하는 기술입니다.\u003c/p\u003e\n\u003cp\u003e배깅 (부트스트랩 집계)은 예측기의 여러 버전을 생성하고 이를 사용하여 정확도를 향상시킨 집계 예측기를 만드는 방법입니다.\u003c/p\u003e\n\u003cp\u003e배깅은 데이터의 서로 다른 하위 집합에서 훈련된 각 모델의 여러 버전을 평균화하여 예측의 분산을 줄이고 노이즈를 제거하며 과적합을 완화하기 때문에 작동합니다.\u003c/p\u003e\n\u003cp\u003e부트스트래핑은 데이터 집합에서 대체 샘플링을 통해 여러 새로운 훈련 세트를 생성하는 통계적 방법이며, 부트스트랩 복제본은 이러한 새로운 훈련 세트 중 하나를 의미합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e합성곱 신경망\u003c/h1\u003e\n\u003cp\u003eYann LeCun, Léon Bottou, Yoshua Bengio, and P. Haffner이(가) 쓴 article인 \"Gradient-based learning applied to document recognition\" (1998)\u003c/p\u003e\n\u003cp\u003e이 논문은 합성곱 신경망(CNNs)의 우수한 성능을 증명하여 2차원 모양을 인식하는 데 특히 필기 문자 인식에 영향을 미쳤고, 복잡하고 다중 모듈 시스템을 교육하기 위한 그래프 변환 신경망(GTNs) 개념을 소개하여 문서 인식 및 다른 실용적인 응용 분야의 발전을 이끌었습니다.\u003c/p\u003e\n\u003cp\u003e합성곱 연산은 입력(예: 이미지)에 필터를 적용하여 입력의 특정 측면(예: 이미지의 가장자리)을 강조하는 출력 feature map을 생성하는 수학적 과정입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e합성곱 신경망(CNNs)은 그리드 형태의 위상을 가진 데이터, 예를 들어 이미지와 같은 데이터를 처리하고 분석하기 위해 특별히 설계된 심층 신경망의 한 종류입니다. 합성 계층을 사용하여 공간적으로 계층화된 특징들을 자동적으로 적응적으로 학습합니다.\u003c/p\u003e\n\u003cp\u003e고차원 패턴이라는 용어는 많은 속성 또는 특징을 가진 데이터로, 이로 인해 분류나 분석이 복잡하고 어렵습니다. 예를 들어 손글씨 문자와 같은 것이 있습니다.\u003c/p\u003e\n\u003cp\u003e그래프 변환 네트워크(GTNs)는 경사 기반 방법을 사용하여 전체적인 성능 척도를 최적화하기 위해 여러 모듈 시스템을 전역적으로 훈련시키는 학습 패러다임의 한 종류입니다.\u003c/p\u003e\n\u003cp\u003e전역 훈련은 시스템의 여러 구성 요소 또는 모듈을 통합된 목표를 사용하여 동시에 훈련시키는 프로세스로, 시스템 전체를 단독 부분이 아닌 통합적으로 최적화할 수 있도록 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e읽어 주셔서 감사합니다! 피드백을 환영합니다! 특히 중요한 연구를 놓치지 않았는지 생각하신다면 더욱 감사하겠습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-27-AHistoryofAIPart1_0.png\" alt=\"A History of AI Part 1\"\u003e\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-27-AHistoryofAIPart1"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>