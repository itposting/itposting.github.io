<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>GPT-3 사용법 소수 샷 학습자를 위한 포괄적 가이드 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="GPT-3 사용법 소수 샷 학습자를 위한 포괄적 가이드 | itposting" data-gatsby-head="true"/><meta property="og:title" content="GPT-3 사용법 소수 샷 학습자를 위한 포괄적 가이드 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners" data-gatsby-head="true"/><meta name="twitter:title" content="GPT-3 사용법 소수 샷 학습자를 위한 포괄적 가이드 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-22 21:29" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">GPT-3 사용법 소수 샷 학습자를 위한 포괄적 가이드</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="GPT-3 사용법 소수 샷 학습자를 위한 포괄적 가이드" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 22, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>메타러닌 학습 프레임워크 내에서 대규모에서 타이타닉 규모로 GPT를 효율적으로 확장하는 방법</h2>
<p><img src="/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_0.png" alt="그림"></p>
<h1>소개</h1>
<p>GPT는 최근 많은 인기를 얻고 있는 언어 모델 가족입니다. 데이터 과학 커뮤니티의 관심은 GPT-3가 2020년에 출시되면서 급속하게 집중되었습니다. GPT-2가 등장한 후, 거의 누구도 1년 안에 175B의 매개변수를 포함한 GPT의 타이타닉 버전이 출현할 것으로 상상하지 못했습니다! 이는 이전 버전과 비교했을 때 2차적으로 두 배 많은 양입니다.</p>
<div class="content-ad"></div>
<p>GPT-3의 엄청난 용량 덕분에 코드 완성, 글 작성, 콘텐츠 생성, 가상 어시스턴트 등 다양한 일상 시나리오에서 사용할 수 있었습니다. 이러한 작업의 품질이 항상 완벽하지는 않지만, GPT-3이 달성한 전반적인 진전은 정말 놀랍습니다!</p>
<p>이 기사에서는 GPT-3의 주요 세부 사항과 GPT-2 창조자들로부터 영감을 받은 유용한 아이디어를 자세히 살펴볼 것입니다. 탐구하는 동안, 공식 GPT-3 논문을 참조하겠습니다. GPT-3의 대부분의 설정은 GPT-2에서 직접 파생된 데이터 수집, 구조 선택 및 사전 훈련 과정을 포함합니다. 그래서 대부분의 시간을 GPT-3의 새로운 측면에 집중할 것입니다.</p>
<h1>메타-러닝 프레임워크</h1>
<p>GPT-3 창조자들은 GPT-2에서 사용된 학습 방법에 대해 매우 관심을 가졌습니다: 일반적인 사전 훈련 + 미세 조정 프레임워크 대신, 저자들은 크고 다양한 데이터 세트를 수집하고 텍스트 입력에 작업 목표를 통합했습니다. 이 방법론은 여러 가지 이유로 편리했습니다:</p>
<div class="content-ad"></div>
<ul>
<li>미세 조정 단계를 제거함으로써 이제는 개별 하위 작업을 위해 여러 대규모 레이블된 데이터 세트가 더 이상 필요하지 않습니다.</li>
<li>다른 작업에 대해 하나의 모델 버전을 여러 개 사용하는 대신 하나만 사용할 수 있습니다.</li>
<li>모델은 사람이 하는 것과 더 유사한 방식으로 작동합니다. 대부분의 경우 사람들은 주어진 작업을 완전히 이해하려면 언어 예제가 전혀 필요하지 않거나 몇 개만 필요합니다. 추론 중에 모델은 해당 예제를 텍스트 형식으로 받을 수 있습니다. 그 결과로 이 측면은 인간과 상호 작용하는 AI 애플리케이션을 개발하는 데 더 나은 전망을 제공합니다.</li>
<li>모델은 한 번만 단일 데이터 세트에서 훈련됩니다. 미세 조정 + 미세 조정 패러다임과 달리, 모델은 완전히 다른 데이터 분포를 가질 수 있던 두 가지 다른 데이터 세트에서 훈련되어야 했다는 점으로 잠재적인 일반화 문제를 야기할 수 있었습니다.</li>
</ul>
<p>공식적으로, 설명된 프레임워크를 메타 학습이라고 합니다. 논문은 공식적인 정의를 제공합니다:</p>
<p>학습 패러다임을 더 자세히 설명하기 위해 내부 및 외부 루프 용어가 소개됩니다. 기본적으로 내부 루프는 훈련 중에 단일 순방향 패스에 해당하며 외부 루프는 모든 내부 루프를 나타냅니다.</p>
<p>훈련 과정 동안 모델은 다른 텍스트 예제에서 유사한 작업을 받을 수 있습니다. 예를 들어, 모델은 서로 다른 배치에서 다음과 같은 예제를 볼 수 있습니다:</p>
<div class="content-ad"></div>
<ul>
<li>"Good"은 "excellent"의 동의어입니다.</li>
<li>"Computer"은 "laptop"의 동의어입니다.</li>
<li>"House"는 "building"의 동의어입니다.</li>
</ul>
<p>이 예시들은 모델이 어떻게 동의어를 이해하는지를 돕는데 도움이 되며, 특정 단어의 동의어를 찾을 때 유용할 수 있습니다. 특정 작업 내에서 비슷한 언어 지식을 습득하도록 도와주는 예시의 조합을 "컨텍스트 학습"이라고 합니다.</p>
<img src="/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_1.png">
<h2>n-shot 학습</h2>
<div class="content-ad"></div>
<p>추론 중에 수행되는 모델의 쿼리는 작업 예제를 추가로 포함할 수 있습니다. 작업 데모가 쿼리의 목적을 더 잘 이해하는 데 중요한 역할을 한다는 것을 알 수 있었습니다. 제공된 작업 예제의 수(샷)에 따라 아래 표에 요약된 세 가지 유형의 학습이 존재합니다:</p>





















<table><thead><tr><th>샷 수</th><th>학습 유형</th></tr></thead><tbody><tr><td>0</td><td>Zero-shot</td></tr><tr><td>1</td><td>One-shot</td></tr><tr><td>2+</td><td>Few-shot</td></tr></tbody></table>
<p>대부분의 경우(항상은 아니지만) 제공된 예제의 수가 모델이 올바른 답변을 제공하는 능력과 긍정적으로 상관 관계가 있음을 알 수 있습니다. 저자들은 다른 크기의 모델을 세 가지 n-shot 설정 중 하나에 사용하여 연구를 완료했습니다. 결과에서 용량이 증가함에 따라 모델이 문맥 학습에 더 능숙해진다는 것을 보여줍니다. 성능 차이가 증가하는 선 그래프로 이를 시연합니다. 적은 수, 한 개, 영 샷 설정 간의 성능 차이가 모델 크기와 함께 더 커짐을 보여줍니다.</p>
<div class="content-ad"></div>
<h1>아키텍처</h1>
<p>이 논문은 GPT-3의 아키텍처 설정을 정확히 설명하고 있습니다.</p>
<h2>데이터셋</h2>
<p>초기에 저자들은 GPT-3를 훈련하기 위해 Common Crawl 데이터셋을 사용하고자 했습니다. 이 굉장히 큰 데이터셋은 다양한 주제의 데이터를 담고 있습니다. 그러나 초기의 원본 데이터셋은 데이터 품질에 문제가 있어 처음에는 필터링되고 중복이 제거되었습니다. 최종 데이터셋을 더욱 다양하게 만들기 위해 아래 다이어그램에 표시된 네 가지 다른 작은 데이터셋과 연결되었습니다:</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_4.png">
<h1>훈련 세부 정보</h1>
<ul>
<li>옵티마이저: Adam (β₁ = 0.9, β₂ = 0.999, ε = 1e-6).</li>
<li>폭주 그래디언트 문제를 방지하기 위해 1.0에서 그래디언트 클리핑이 사용됩니다.</li>
<li>학습 속도 조정을 위해 코사인 감쇠와 선형 웜업의 조합이 사용됩니다.</li>
<li>배치 크기는 학습 중 32K에서 3.2M 토큰으로 점진적으로 증가합니다.</li>
<li>0.1의 가중치 감쇠가 정규화자로 사용됩니다.</li>
<li>더 나은 계산 효율성을 위해 모든 시퀀스의 길이가 2048로 설정됩니다. 단일 시퀀스 내의 다른 문서는 구분자 토큰으로 분리됩니다.</li>
</ul>
<h2>빔 탐색</h2>
<div class="content-ad"></div>
<p>GPT-3는 자기회귀 모델입니다. 이것은 과거의 예측된 단어에 대한 정보를 사용하여 미래의 다음 단어를 예측하는 데 사용합니다.</p>
<p>탐욕 알고리즘은 자기회귀 모델에서 텍스트 시퀀스를 구성하기 위한 가장 단순한 방법 중 하나입니다. 각 반복에서 모델이 가장 가능성이 높은 단어를 선택하도록 강제하고, 이를 다음 단어의 입력으로 사용합니다. 그러나 현재 반복에서 가장 가능성이 높은 단어를 선택하는 것이 로그 우도 최적화에 대한 최선의 방법은 아닙니다!</p>
<p>이미지 링크: <img src="/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_5.png" alt="이미지"></p>
<p>낮은 확률로 현재 단어를 선택하면 나머지 예측된 단어들의 확률이 높아질 수 있는 상황이 발생할 수 있습니다. 한편, 지역 단어를 가장 높은 확률로 선택하는 것은 그 다음 단어들도 높은 확률에 해당한다는 것을 보장하지는 않습니다. 탐욕 전략이 최적으로 작동하지 않는 경우를 보여주는 예시는 아래 다이어그램에 나와 있습니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_6.png" alt="Image"></p>
<p>가능한 해결책은 모든 옵션 중에서 가장 가능성이 높은 시퀀스를 찾는 것입니다. 그러나 이 접근 방식은 가능한 시퀀스의 조합이 무수히 많기 때문에 매우 효율적이지 않습니다.</p>
<p>빔 서치(Beam Search)는 탐욕 알고리즘과 모든 가능한 조합을 탐색하는 것 사이의 좋은 절충안입니다. 각 반복에서 빔 서치는 가장 가능성이 높은 토큰을 여러 개 선택하고 현재 가장 가능성이 높은 시퀀스 집합을 유지합니다. 새로운 더 가능성 있는 시퀀스가 형성될 때마다, 해당 시퀀스 중 가장 가능성이 낮은 것을 대체합니다. 알고리즘의 끝에는 집합에서 가장 가능성이 높은 시퀀스가 반환됩니다.</p>
<p><img src="/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_7.png" alt="Image"></p>
<div class="content-ad"></div>
<p>빔 서치는 최상의 검색 전략을 보장하지는 않지만 실제로는 근사치가 매우 잘 동작합니다. 그 이유로 GPT-3에서 사용됩니다.</p>
<h1>단점</h1>
<p>GPT-3는 인간과 유사한 긴 텍스트 조각을 생성하는 놀라운 능력을 가졌지만 몇 가지 단점이 있습니다:</p>
<ul>
<li>텍스트 생성 중 GPT-3이 내린 결정은 일반적으로 해석하기 어려워 분석하기 어렵습니다.</li>
<li>GPT-3은 모델로 항상 방지할 수 없는 해로운 방식으로 사용될 수 있습니다.</li>
<li>GPT-3는 학습 데이터 세트에 편향이 있어 때때로 공정성 측면에서 취약할 수 있습니다. 특히, 성별 평등, 종교 또는 인종과 같은 민감한 도메인에 관련된 경우입니다.</li>
<li>이전의 GPT-2 보다 GPT-3는 훈련에 수십 배나 더 많은 에너지(수천 페타플랍 / 일)가 필요한데, 이는 친환경적이지 않습니다. 동시에, GPT-3 개발자들은 이 모델이 추론 중에 매우 효율적이기 때문에 평균 소비량이 여전히 낮다는 점으로 이 측면을 정당화합니다.</li>
</ul>
<div class="content-ad"></div>
<h1>결론</h1>
<p>GPT-3는 상상할 수 없는 175B개의 훈련 가능한 매개변수를 보유하여 이전 모델들을 강력히 이기는 몇 가지 최고의 기준들에서 엄청난 인기를 얻었습니다! 그 당시에 GPT-3 결과는 때때로 사람이 생성한 텍스트인지 GPT-3가 생성한 것인지 구별하기가 어려울 정도로 좋았습니다.</p>
<p>GPT-3의 몇 가지 단점과 제한사항에도 불구하고, 이는 연구자들에게 미래에 대한 새로운 탐구와 잠재적 개선의 가능성을 여는 문이 되었습니다.</p>
<h1>자료들</h1>
<div class="content-ad"></div>
<ul>
<li>언어 모델은 소수 샷 학습자들입니다.</li>
</ul>
<p>모든 이미지는 특별히 언급되지 않는 한 작성자가 찍은 것입니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"GPT-3 사용법 소수 샷 학습자를 위한 포괄적 가이드","description":"","date":"2024-06-22 21:29","slug":"2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners","content":"\n\n## 메타러닌 학습 프레임워크 내에서 대규모에서 타이타닉 규모로 GPT를 효율적으로 확장하는 방법\n\n![그림](/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_0.png)\n\n# 소개\n\nGPT는 최근 많은 인기를 얻고 있는 언어 모델 가족입니다. 데이터 과학 커뮤니티의 관심은 GPT-3가 2020년에 출시되면서 급속하게 집중되었습니다. GPT-2가 등장한 후, 거의 누구도 1년 안에 175B의 매개변수를 포함한 GPT의 타이타닉 버전이 출현할 것으로 상상하지 못했습니다! 이는 이전 버전과 비교했을 때 2차적으로 두 배 많은 양입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGPT-3의 엄청난 용량 덕분에 코드 완성, 글 작성, 콘텐츠 생성, 가상 어시스턴트 등 다양한 일상 시나리오에서 사용할 수 있었습니다. 이러한 작업의 품질이 항상 완벽하지는 않지만, GPT-3이 달성한 전반적인 진전은 정말 놀랍습니다!\n\n이 기사에서는 GPT-3의 주요 세부 사항과 GPT-2 창조자들로부터 영감을 받은 유용한 아이디어를 자세히 살펴볼 것입니다. 탐구하는 동안, 공식 GPT-3 논문을 참조하겠습니다. GPT-3의 대부분의 설정은 GPT-2에서 직접 파생된 데이터 수집, 구조 선택 및 사전 훈련 과정을 포함합니다. 그래서 대부분의 시간을 GPT-3의 새로운 측면에 집중할 것입니다.\n\n# 메타-러닝 프레임워크\n\nGPT-3 창조자들은 GPT-2에서 사용된 학습 방법에 대해 매우 관심을 가졌습니다: 일반적인 사전 훈련 + 미세 조정 프레임워크 대신, 저자들은 크고 다양한 데이터 세트를 수집하고 텍스트 입력에 작업 목표를 통합했습니다. 이 방법론은 여러 가지 이유로 편리했습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 미세 조정 단계를 제거함으로써 이제는 개별 하위 작업을 위해 여러 대규모 레이블된 데이터 세트가 더 이상 필요하지 않습니다.\n- 다른 작업에 대해 하나의 모델 버전을 여러 개 사용하는 대신 하나만 사용할 수 있습니다.\n- 모델은 사람이 하는 것과 더 유사한 방식으로 작동합니다. 대부분의 경우 사람들은 주어진 작업을 완전히 이해하려면 언어 예제가 전혀 필요하지 않거나 몇 개만 필요합니다. 추론 중에 모델은 해당 예제를 텍스트 형식으로 받을 수 있습니다. 그 결과로 이 측면은 인간과 상호 작용하는 AI 애플리케이션을 개발하는 데 더 나은 전망을 제공합니다.\n- 모델은 한 번만 단일 데이터 세트에서 훈련됩니다. 미세 조정 + 미세 조정 패러다임과 달리, 모델은 완전히 다른 데이터 분포를 가질 수 있던 두 가지 다른 데이터 세트에서 훈련되어야 했다는 점으로 잠재적인 일반화 문제를 야기할 수 있었습니다.\n\n공식적으로, 설명된 프레임워크를 메타 학습이라고 합니다. 논문은 공식적인 정의를 제공합니다:\n\n학습 패러다임을 더 자세히 설명하기 위해 내부 및 외부 루프 용어가 소개됩니다. 기본적으로 내부 루프는 훈련 중에 단일 순방향 패스에 해당하며 외부 루프는 모든 내부 루프를 나타냅니다.\n\n훈련 과정 동안 모델은 다른 텍스트 예제에서 유사한 작업을 받을 수 있습니다. 예를 들어, 모델은 서로 다른 배치에서 다음과 같은 예제를 볼 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- \"Good\"은 \"excellent\"의 동의어입니다.\n- \"Computer\"은 \"laptop\"의 동의어입니다.\n- \"House\"는 \"building\"의 동의어입니다.\n\n이 예시들은 모델이 어떻게 동의어를 이해하는지를 돕는데 도움이 되며, 특정 단어의 동의어를 찾을 때 유용할 수 있습니다. 특정 작업 내에서 비슷한 언어 지식을 습득하도록 도와주는 예시의 조합을 \"컨텍스트 학습\"이라고 합니다.\n\n\u003cimg src=\"/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_1.png\" /\u003e\n\n## n-shot 학습\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n추론 중에 수행되는 모델의 쿼리는 작업 예제를 추가로 포함할 수 있습니다. 작업 데모가 쿼리의 목적을 더 잘 이해하는 데 중요한 역할을 한다는 것을 알 수 있었습니다. 제공된 작업 예제의 수(샷)에 따라 아래 표에 요약된 세 가지 유형의 학습이 존재합니다:\n\n| 샷 수 | 학습 유형 |\n|-------|------------|\n| 0     | Zero-shot  |\n| 1     | One-shot   |\n| 2+    | Few-shot   |\n\n대부분의 경우(항상은 아니지만) 제공된 예제의 수가 모델이 올바른 답변을 제공하는 능력과 긍정적으로 상관 관계가 있음을 알 수 있습니다. 저자들은 다른 크기의 모델을 세 가지 n-shot 설정 중 하나에 사용하여 연구를 완료했습니다. 결과에서 용량이 증가함에 따라 모델이 문맥 학습에 더 능숙해진다는 것을 보여줍니다. 성능 차이가 증가하는 선 그래프로 이를 시연합니다. 적은 수, 한 개, 영 샷 설정 간의 성능 차이가 모델 크기와 함께 더 커짐을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 아키텍처\n\n이 논문은 GPT-3의 아키텍처 설정을 정확히 설명하고 있습니다.\n\n## 데이터셋\n\n초기에 저자들은 GPT-3를 훈련하기 위해 Common Crawl 데이터셋을 사용하고자 했습니다. 이 굉장히 큰 데이터셋은 다양한 주제의 데이터를 담고 있습니다. 그러나 초기의 원본 데이터셋은 데이터 품질에 문제가 있어 처음에는 필터링되고 중복이 제거되었습니다. 최종 데이터셋을 더욱 다양하게 만들기 위해 아래 다이어그램에 표시된 네 가지 다른 작은 데이터셋과 연결되었습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_4.png\" /\u003e\n\n# 훈련 세부 정보\n\n- 옵티마이저: Adam (β₁ = 0.9, β₂ = 0.999, ε = 1e-6).\n- 폭주 그래디언트 문제를 방지하기 위해 1.0에서 그래디언트 클리핑이 사용됩니다.\n- 학습 속도 조정을 위해 코사인 감쇠와 선형 웜업의 조합이 사용됩니다.\n- 배치 크기는 학습 중 32K에서 3.2M 토큰으로 점진적으로 증가합니다.\n- 0.1의 가중치 감쇠가 정규화자로 사용됩니다.\n- 더 나은 계산 효율성을 위해 모든 시퀀스의 길이가 2048로 설정됩니다. 단일 시퀀스 내의 다른 문서는 구분자 토큰으로 분리됩니다.\n\n## 빔 탐색\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGPT-3는 자기회귀 모델입니다. 이것은 과거의 예측된 단어에 대한 정보를 사용하여 미래의 다음 단어를 예측하는 데 사용합니다.\n\n탐욕 알고리즘은 자기회귀 모델에서 텍스트 시퀀스를 구성하기 위한 가장 단순한 방법 중 하나입니다. 각 반복에서 모델이 가장 가능성이 높은 단어를 선택하도록 강제하고, 이를 다음 단어의 입력으로 사용합니다. 그러나 현재 반복에서 가장 가능성이 높은 단어를 선택하는 것이 로그 우도 최적화에 대한 최선의 방법은 아닙니다!\n\n이미지 링크: ![이미지](/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_5.png)\n\n낮은 확률로 현재 단어를 선택하면 나머지 예측된 단어들의 확률이 높아질 수 있는 상황이 발생할 수 있습니다. 한편, 지역 단어를 가장 높은 확률로 선택하는 것은 그 다음 단어들도 높은 확률에 해당한다는 것을 보장하지는 않습니다. 탐욕 전략이 최적으로 작동하지 않는 경우를 보여주는 예시는 아래 다이어그램에 나와 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_6.png)\n\n가능한 해결책은 모든 옵션 중에서 가장 가능성이 높은 시퀀스를 찾는 것입니다. 그러나 이 접근 방식은 가능한 시퀀스의 조합이 무수히 많기 때문에 매우 효율적이지 않습니다.\n\n빔 서치(Beam Search)는 탐욕 알고리즘과 모든 가능한 조합을 탐색하는 것 사이의 좋은 절충안입니다. 각 반복에서 빔 서치는 가장 가능성이 높은 토큰을 여러 개 선택하고 현재 가장 가능성이 높은 시퀀스 집합을 유지합니다. 새로운 더 가능성 있는 시퀀스가 형성될 때마다, 해당 시퀀스 중 가장 가능성이 낮은 것을 대체합니다. 알고리즘의 끝에는 집합에서 가장 가능성이 높은 시퀀스가 반환됩니다.\n\n![Image](/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_7.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n빔 서치는 최상의 검색 전략을 보장하지는 않지만 실제로는 근사치가 매우 잘 동작합니다. 그 이유로 GPT-3에서 사용됩니다.\n\n# 단점\n\nGPT-3는 인간과 유사한 긴 텍스트 조각을 생성하는 놀라운 능력을 가졌지만 몇 가지 단점이 있습니다:\n\n- 텍스트 생성 중 GPT-3이 내린 결정은 일반적으로 해석하기 어려워 분석하기 어렵습니다.\n- GPT-3은 모델로 항상 방지할 수 없는 해로운 방식으로 사용될 수 있습니다.\n- GPT-3는 학습 데이터 세트에 편향이 있어 때때로 공정성 측면에서 취약할 수 있습니다. 특히, 성별 평등, 종교 또는 인종과 같은 민감한 도메인에 관련된 경우입니다.\n- 이전의 GPT-2 보다 GPT-3는 훈련에 수십 배나 더 많은 에너지(수천 페타플랍 / 일)가 필요한데, 이는 친환경적이지 않습니다. 동시에, GPT-3 개발자들은 이 모델이 추론 중에 매우 효율적이기 때문에 평균 소비량이 여전히 낮다는 점으로 이 측면을 정당화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\nGPT-3는 상상할 수 없는 175B개의 훈련 가능한 매개변수를 보유하여 이전 모델들을 강력히 이기는 몇 가지 최고의 기준들에서 엄청난 인기를 얻었습니다! 그 당시에 GPT-3 결과는 때때로 사람이 생성한 텍스트인지 GPT-3가 생성한 것인지 구별하기가 어려울 정도로 좋았습니다.\n\nGPT-3의 몇 가지 단점과 제한사항에도 불구하고, 이는 연구자들에게 미래에 대한 새로운 탐구와 잠재적 개선의 가능성을 여는 문이 되었습니다.\n\n# 자료들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 언어 모델은 소수 샷 학습자들입니다.\n\n모든 이미지는 특별히 언급되지 않는 한 작성자가 찍은 것입니다.","ogImage":{"url":"/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_0.png"},"coverImage":"/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_0.png","tag":["Tech"],"readingTime":6},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e메타러닌 학습 프레임워크 내에서 대규모에서 타이타닉 규모로 GPT를 효율적으로 확장하는 방법\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_0.png\" alt=\"그림\"\u003e\u003c/p\u003e\n\u003ch1\u003e소개\u003c/h1\u003e\n\u003cp\u003eGPT는 최근 많은 인기를 얻고 있는 언어 모델 가족입니다. 데이터 과학 커뮤니티의 관심은 GPT-3가 2020년에 출시되면서 급속하게 집중되었습니다. GPT-2가 등장한 후, 거의 누구도 1년 안에 175B의 매개변수를 포함한 GPT의 타이타닉 버전이 출현할 것으로 상상하지 못했습니다! 이는 이전 버전과 비교했을 때 2차적으로 두 배 많은 양입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eGPT-3의 엄청난 용량 덕분에 코드 완성, 글 작성, 콘텐츠 생성, 가상 어시스턴트 등 다양한 일상 시나리오에서 사용할 수 있었습니다. 이러한 작업의 품질이 항상 완벽하지는 않지만, GPT-3이 달성한 전반적인 진전은 정말 놀랍습니다!\u003c/p\u003e\n\u003cp\u003e이 기사에서는 GPT-3의 주요 세부 사항과 GPT-2 창조자들로부터 영감을 받은 유용한 아이디어를 자세히 살펴볼 것입니다. 탐구하는 동안, 공식 GPT-3 논문을 참조하겠습니다. GPT-3의 대부분의 설정은 GPT-2에서 직접 파생된 데이터 수집, 구조 선택 및 사전 훈련 과정을 포함합니다. 그래서 대부분의 시간을 GPT-3의 새로운 측면에 집중할 것입니다.\u003c/p\u003e\n\u003ch1\u003e메타-러닝 프레임워크\u003c/h1\u003e\n\u003cp\u003eGPT-3 창조자들은 GPT-2에서 사용된 학습 방법에 대해 매우 관심을 가졌습니다: 일반적인 사전 훈련 + 미세 조정 프레임워크 대신, 저자들은 크고 다양한 데이터 세트를 수집하고 텍스트 입력에 작업 목표를 통합했습니다. 이 방법론은 여러 가지 이유로 편리했습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e미세 조정 단계를 제거함으로써 이제는 개별 하위 작업을 위해 여러 대규모 레이블된 데이터 세트가 더 이상 필요하지 않습니다.\u003c/li\u003e\n\u003cli\u003e다른 작업에 대해 하나의 모델 버전을 여러 개 사용하는 대신 하나만 사용할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e모델은 사람이 하는 것과 더 유사한 방식으로 작동합니다. 대부분의 경우 사람들은 주어진 작업을 완전히 이해하려면 언어 예제가 전혀 필요하지 않거나 몇 개만 필요합니다. 추론 중에 모델은 해당 예제를 텍스트 형식으로 받을 수 있습니다. 그 결과로 이 측면은 인간과 상호 작용하는 AI 애플리케이션을 개발하는 데 더 나은 전망을 제공합니다.\u003c/li\u003e\n\u003cli\u003e모델은 한 번만 단일 데이터 세트에서 훈련됩니다. 미세 조정 + 미세 조정 패러다임과 달리, 모델은 완전히 다른 데이터 분포를 가질 수 있던 두 가지 다른 데이터 세트에서 훈련되어야 했다는 점으로 잠재적인 일반화 문제를 야기할 수 있었습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e공식적으로, 설명된 프레임워크를 메타 학습이라고 합니다. 논문은 공식적인 정의를 제공합니다:\u003c/p\u003e\n\u003cp\u003e학습 패러다임을 더 자세히 설명하기 위해 내부 및 외부 루프 용어가 소개됩니다. 기본적으로 내부 루프는 훈련 중에 단일 순방향 패스에 해당하며 외부 루프는 모든 내부 루프를 나타냅니다.\u003c/p\u003e\n\u003cp\u003e훈련 과정 동안 모델은 다른 텍스트 예제에서 유사한 작업을 받을 수 있습니다. 예를 들어, 모델은 서로 다른 배치에서 다음과 같은 예제를 볼 수 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e\"Good\"은 \"excellent\"의 동의어입니다.\u003c/li\u003e\n\u003cli\u003e\"Computer\"은 \"laptop\"의 동의어입니다.\u003c/li\u003e\n\u003cli\u003e\"House\"는 \"building\"의 동의어입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이 예시들은 모델이 어떻게 동의어를 이해하는지를 돕는데 도움이 되며, 특정 단어의 동의어를 찾을 때 유용할 수 있습니다. 특정 작업 내에서 비슷한 언어 지식을 습득하도록 도와주는 예시의 조합을 \"컨텍스트 학습\"이라고 합니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_1.png\"\u003e\n\u003ch2\u003en-shot 학습\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e추론 중에 수행되는 모델의 쿼리는 작업 예제를 추가로 포함할 수 있습니다. 작업 데모가 쿼리의 목적을 더 잘 이해하는 데 중요한 역할을 한다는 것을 알 수 있었습니다. 제공된 작업 예제의 수(샷)에 따라 아래 표에 요약된 세 가지 유형의 학습이 존재합니다:\u003c/p\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003e샷 수\u003c/th\u003e\u003cth\u003e학습 유형\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e0\u003c/td\u003e\u003ctd\u003eZero-shot\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e1\u003c/td\u003e\u003ctd\u003eOne-shot\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e2+\u003c/td\u003e\u003ctd\u003eFew-shot\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003e대부분의 경우(항상은 아니지만) 제공된 예제의 수가 모델이 올바른 답변을 제공하는 능력과 긍정적으로 상관 관계가 있음을 알 수 있습니다. 저자들은 다른 크기의 모델을 세 가지 n-shot 설정 중 하나에 사용하여 연구를 완료했습니다. 결과에서 용량이 증가함에 따라 모델이 문맥 학습에 더 능숙해진다는 것을 보여줍니다. 성능 차이가 증가하는 선 그래프로 이를 시연합니다. 적은 수, 한 개, 영 샷 설정 간의 성능 차이가 모델 크기와 함께 더 커짐을 보여줍니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e아키텍처\u003c/h1\u003e\n\u003cp\u003e이 논문은 GPT-3의 아키텍처 설정을 정확히 설명하고 있습니다.\u003c/p\u003e\n\u003ch2\u003e데이터셋\u003c/h2\u003e\n\u003cp\u003e초기에 저자들은 GPT-3를 훈련하기 위해 Common Crawl 데이터셋을 사용하고자 했습니다. 이 굉장히 큰 데이터셋은 다양한 주제의 데이터를 담고 있습니다. 그러나 초기의 원본 데이터셋은 데이터 품질에 문제가 있어 처음에는 필터링되고 중복이 제거되었습니다. 최종 데이터셋을 더욱 다양하게 만들기 위해 아래 다이어그램에 표시된 네 가지 다른 작은 데이터셋과 연결되었습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_4.png\"\u003e\n\u003ch1\u003e훈련 세부 정보\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e옵티마이저: Adam (β₁ = 0.9, β₂ = 0.999, ε = 1e-6).\u003c/li\u003e\n\u003cli\u003e폭주 그래디언트 문제를 방지하기 위해 1.0에서 그래디언트 클리핑이 사용됩니다.\u003c/li\u003e\n\u003cli\u003e학습 속도 조정을 위해 코사인 감쇠와 선형 웜업의 조합이 사용됩니다.\u003c/li\u003e\n\u003cli\u003e배치 크기는 학습 중 32K에서 3.2M 토큰으로 점진적으로 증가합니다.\u003c/li\u003e\n\u003cli\u003e0.1의 가중치 감쇠가 정규화자로 사용됩니다.\u003c/li\u003e\n\u003cli\u003e더 나은 계산 효율성을 위해 모든 시퀀스의 길이가 2048로 설정됩니다. 단일 시퀀스 내의 다른 문서는 구분자 토큰으로 분리됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e빔 탐색\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eGPT-3는 자기회귀 모델입니다. 이것은 과거의 예측된 단어에 대한 정보를 사용하여 미래의 다음 단어를 예측하는 데 사용합니다.\u003c/p\u003e\n\u003cp\u003e탐욕 알고리즘은 자기회귀 모델에서 텍스트 시퀀스를 구성하기 위한 가장 단순한 방법 중 하나입니다. 각 반복에서 모델이 가장 가능성이 높은 단어를 선택하도록 강제하고, 이를 다음 단어의 입력으로 사용합니다. 그러나 현재 반복에서 가장 가능성이 높은 단어를 선택하는 것이 로그 우도 최적화에 대한 최선의 방법은 아닙니다!\u003c/p\u003e\n\u003cp\u003e이미지 링크: \u003cimg src=\"/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_5.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e낮은 확률로 현재 단어를 선택하면 나머지 예측된 단어들의 확률이 높아질 수 있는 상황이 발생할 수 있습니다. 한편, 지역 단어를 가장 높은 확률로 선택하는 것은 그 다음 단어들도 높은 확률에 해당한다는 것을 보장하지는 않습니다. 탐욕 전략이 최적으로 작동하지 않는 경우를 보여주는 예시는 아래 다이어그램에 나와 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_6.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003e가능한 해결책은 모든 옵션 중에서 가장 가능성이 높은 시퀀스를 찾는 것입니다. 그러나 이 접근 방식은 가능한 시퀀스의 조합이 무수히 많기 때문에 매우 효율적이지 않습니다.\u003c/p\u003e\n\u003cp\u003e빔 서치(Beam Search)는 탐욕 알고리즘과 모든 가능한 조합을 탐색하는 것 사이의 좋은 절충안입니다. 각 반복에서 빔 서치는 가장 가능성이 높은 토큰을 여러 개 선택하고 현재 가장 가능성이 높은 시퀀스 집합을 유지합니다. 새로운 더 가능성 있는 시퀀스가 형성될 때마다, 해당 시퀀스 중 가장 가능성이 낮은 것을 대체합니다. 알고리즘의 끝에는 집합에서 가장 가능성이 높은 시퀀스가 반환됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners_7.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e빔 서치는 최상의 검색 전략을 보장하지는 않지만 실제로는 근사치가 매우 잘 동작합니다. 그 이유로 GPT-3에서 사용됩니다.\u003c/p\u003e\n\u003ch1\u003e단점\u003c/h1\u003e\n\u003cp\u003eGPT-3는 인간과 유사한 긴 텍스트 조각을 생성하는 놀라운 능력을 가졌지만 몇 가지 단점이 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e텍스트 생성 중 GPT-3이 내린 결정은 일반적으로 해석하기 어려워 분석하기 어렵습니다.\u003c/li\u003e\n\u003cli\u003eGPT-3은 모델로 항상 방지할 수 없는 해로운 방식으로 사용될 수 있습니다.\u003c/li\u003e\n\u003cli\u003eGPT-3는 학습 데이터 세트에 편향이 있어 때때로 공정성 측면에서 취약할 수 있습니다. 특히, 성별 평등, 종교 또는 인종과 같은 민감한 도메인에 관련된 경우입니다.\u003c/li\u003e\n\u003cli\u003e이전의 GPT-2 보다 GPT-3는 훈련에 수십 배나 더 많은 에너지(수천 페타플랍 / 일)가 필요한데, 이는 친환경적이지 않습니다. 동시에, GPT-3 개발자들은 이 모델이 추론 중에 매우 효율적이기 때문에 평균 소비량이 여전히 낮다는 점으로 이 측면을 정당화합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003eGPT-3는 상상할 수 없는 175B개의 훈련 가능한 매개변수를 보유하여 이전 모델들을 강력히 이기는 몇 가지 최고의 기준들에서 엄청난 인기를 얻었습니다! 그 당시에 GPT-3 결과는 때때로 사람이 생성한 텍스트인지 GPT-3가 생성한 것인지 구별하기가 어려울 정도로 좋았습니다.\u003c/p\u003e\n\u003cp\u003eGPT-3의 몇 가지 단점과 제한사항에도 불구하고, 이는 연구자들에게 미래에 대한 새로운 탐구와 잠재적 개선의 가능성을 여는 문이 되었습니다.\u003c/p\u003e\n\u003ch1\u003e자료들\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e언어 모델은 소수 샷 학습자들입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e모든 이미지는 특별히 언급되지 않는 한 작성자가 찍은 것입니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-22-LargeLanguageModelsGPT-3LanguageModelsareFew-ShotLearners"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>