<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>LLM 여행 개념 증명부터 제품화까지 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-AnLLMJourneyFromPOCtoProduction" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="LLM 여행 개념 증명부터 제품화까지 | itposting" data-gatsby-head="true"/><meta property="og:title" content="LLM 여행 개념 증명부터 제품화까지 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-AnLLMJourneyFromPOCtoProduction_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-AnLLMJourneyFromPOCtoProduction" data-gatsby-head="true"/><meta name="twitter:title" content="LLM 여행 개념 증명부터 제품화까지 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-AnLLMJourneyFromPOCtoProduction_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 19:40" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_buildManifest.js" defer=""></script><script src="/_next/static/QYe6gFAUryFKFgjKBoIfo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">LLM 여행 개념 증명부터 제품화까지</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="LLM 여행 개념 증명부터 제품화까지" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">11<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-AnLLMJourneyFromPOCtoProduction&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-06-19-AnLLMJourneyFromPOCtoProduction_0.png" alt="LLM Journey"></p>
<p>이렇게 상상해 봐: LLM(대형 언어 모델)을 사용하여 실행할 수 있는 멋진 프로젝트 아이디어가 있다고 상상해 봐. 빠르게 작업 가능한 개념 증명(PoC)에 도달했다. 네 자신에게 자랑스러워하며 실제로 동작하는 데 필요한 작업이 얼마나 적은지에 놀라게 될 것이다. (5줄의 프롬프트의 마법 ☺)</p>
<p>하지만 이제 어떻게 해야 할까?</p>
<p>LLM을 사용할 때 POC를 작성하는 것은 쉽지만, 실제로 탄생 가능한 제품을 만드는 것은 어려운 작업임을 빨리 깨닫게 될 것이다.</p>
<div class="content-ad"></div>
<p>만약 이 상황에 공감한다면, 이 게시물이 흥미로울 수 있습니다.</p>
<h1>LLM 여정의 시작</h1>
<p>이 여정을 이해하는 가장 좋은 방법은 현재 진행 중인 LLM 프로젝트 중 하나를 살펴보는 것입니다. 여정의 첫 부분은 세 단계로 나눌 수 있습니다.</p>
<h2>동기 부여 찾기</h2>
<div class="content-ad"></div>
<p>복잡한 제품들과 방대한 양의 정보들이 있는 세상에서, 우리 고객들은 종종 길을 잃기도 합니다. 때로 제품에서 기본 작업을 수행하려면 많은 문서 페이지를 읽고, 제품의 UI 페이지를 탐색하거나 로그, 보고서 및 기타 원시 자료를 분석해야 하는 경우도 있습니다.</p>
<p>고객이 자주 묻는 질문 중 하나는 "내가 원하는 대로 내 말로 말하기만 하면 시스템이 해결해주지 않을까?" 입니다.</p>
<p>답은 LLM을 사용하면 가능합니다!</p>
<h2>목표 선언</h2>
<div class="content-ad"></div>
<p>이 경우 프로젝트 목표는 고객이 제공한 자연어 (NL)를 사용하여 제품에 대한 작업을 수행하는 것이었습니다.</p>
<p>먼저 우리 프로젝트는 제품의 API 명세 파일을 학습합니다 (보통 OpenAPI와 같은 표준 형식으로 API를 선언하는 파일) 그런 다음 LLM을 사용하여 NL 요청을 올바른 API로 변환합니다.</p>
<h2>작동하는 개념 증명 (POC)에 도달</h2>
<p>프로젝트 목표를 선언한 후 POC 단계에 도달합니다.</p>
<div class="content-ad"></div>
<p>POC(Port of Call)의 목적은 우리가 가진 아이디어를 실제로 실행할 수 있는지 확인하는 것입니다.</p>
<p>우리는 API 명세 파일과 자연어로 된 사용자 요청을 함께 입력으로 받아 사용자의 요청을 실행하는 기본 시스템을 구축해야 합니다.</p>
<p>이를 위해, 우리는 우리의 LL(Large Language)를 OpenAI GPT로 선택하고, LLM 사용을 래핑하는 라이브러리로서 LangChain을 선택했습니다.</p>
<p>우리는 입력 데이터를 받아 논리적 그룹(서비스)으로 처리하고, 사용자의 요청과 수행 방법에 대한 몇 가지 지침이 포함된 프롬프트와 함께 LangChain 라이브러리로 로드하는 엔진을 작성했습니다. 우리는 LangChain 체인, 도구 및 에이전트를 사용하여 이 작업을 수행했으며, OpenAI의 기능 호출 기능도 사용했습니다.</p>
<div class="content-ad"></div>
<p>LangChain 도구 및 에이전트에 대한 자세한 내용은 여기에서 확인할 수 있어요.</p>
<p>아래 다이어그램은 저희 POC의 주요 참가자들을 설명하고 있어요.</p>
<p>축하해요, 작동하는 POC가 있어요! 이제 실제 여정이 시작돼요...</p>
<h1>먼저 해야 할 일: 정확성</h1>
<div class="content-ad"></div>
<p>최초의 POC 작업에 대한 설령 기대가 식어가기 시작했다 해도, LLM 결정 및 응답에서 일부 결함을 발견하기 시작했습니다.</p>
<p>소프트웨어 산업에서는 종종 결정론적 알고리즘을 사용합니다 (즉, 동일한 입력에 대해 알고리즘은 항상 동일한 출력을 생성합니다).</p>
<p>내 첫 번째 조언은 익숙한 결정론적 예상을 버리는 것입니다. 네, 심지어 OpenAI 온도가 0이더라도요.</p>
<p>목표는 이 새로운 비결정론적 세계에서 어떻게 탐색할지를 배우는 것입니다. 다시 말해, 어떻게 더 예측 가능하게 만들고, 어떻게 다양한 응답을 처리할 수 있는지에 대해 알아보는 것입니다.</p>
<div class="content-ad"></div>
<p>여기 팁 몇 가지 있어요:</p>
<h2>팁 #1: 올바른 프롬프트를 사용하고 있는지 확인하세요</h2>
<p>LLM에게 해야 할 일을 단계별로 설명해야 해요. 입력 내용, 형식, 의미, 예상 출력물, 형식, 의미를 설명해야 해요. 때로는 당신이 원했던 것보다 많은 프롬프트 라인을 사용해야 할 때도 있고, 예시를 포함해야 할 때도 있어요.</p>
<p>아래 Few-Shots Learning 예제에서 프롬프트에 사용 사례를 추가하는 장점을 볼 수 있어요.</p>
<div class="content-ad"></div>
<p>다음 요청을 받으셨군요: "사용자 'user1'의 사용자 세부 정보를 가져와 해당 세부 정보로 새 사용자를 생성하세요".</p>
<p>상당히 기본적이고 명확한 요청이죠? 그렇다고 생각했다면, 틀렸어요! LLM이 한 순환 그 구체적인 단어 "those"를 새 사용자 세부 정보에 넣으려는 기이한 행동을 믿을 수 없을 겁니다. 대신 위 요청의 예시를 팁으로 제시했더니 정상적으로 작동했어요:</p>
<h2>팁 #2: 당신의 LLM에게 도구를 제공하세요</h2>
<p>가끔은 LLM이 스스로 올바른 조치에 대한 도달 방법을 모를 때가 있습니다. 이런 경우 LLM에게 하나의 툴 세트를 제공하여 LLM이 무엇을 해야 하는지 확신이 없을 때 사용할 수 있습니다. 이런 것은 LangChain 도구나 OpenAI 함수 호출로 쉽게 처리할 수 있어요.</p>
<div class="content-ad"></div>
<p>여러분이 사용할 수 있는 다양한 도구 예시를 드리겠습니다:</p>
<ul>
<li>날짜 도구는 날짜를 계산하는 데 도움을 줍니다. 예를 들어 "어제의 모든 로그를 나에게 보여줘" 라고 물어볼 수 있습니다. '어제'가 무엇인지 이해하는 데 어려움이 있을 수 있으므로, 날짜 도구를 사용하여 '어제'를 사용할 수 있는 타임스탬프로 변환할 수 있습니다.</li>
<li>사용자/인간 도구는 사용자로부터 명확한 설명을 얻기 위해 사용됩니다. 예를 들어 "새로운 사용자를 생성해줘" 라고 요청할 수 있습니다. LLM이 추가 정보(예를 들어 사용자 이름)가 필요할 때 사용자 도구를 사용하여 사용자에게 원하는 사용자 이름이 무엇인지 물어볼 수 있습니다.</li>
</ul>
<h2>팁 #3: LLM의 창의성 제한하기</h2>
<p>다른 제안으로, LLM의 창의성을 제한하여 LLM이 모르는 것들을 추측하는 대신에 명확한 설명을 요청하도록 하는 것이 좋습니다.</p>
<div class="content-ad"></div>
<p>예를 들어, "사용자 이름이 'user1'인 새 사용자를 생성하라"고 요청할 때, API의 매개변수 중 하나는 암호입니다.</p>
<p>LLM은 Password123이라는 생성된 비밀번호로 사용자를 만들려고 할 것입니다. 아마 원하는 바가 아닐 겁니다. 당신은 LLM에게 해당 사용 사례에서 추측하는 대신 명확히 설명하도록 요청할 수 있습니다.</p>
<h1>언제나 예기치 못한 것을 기대하세요</h1>
<p>이제 우리의 모든 사용 사례를 지원하고 "happy path"에서 매우 잘 수행하는 엔진을 만들었습니다. 그런데 오류 흐름은 어떠한가요? 예외적인 경우는 어떠한가요?</p>
<div class="content-ad"></div>
<p>여기 처리 방법에 대한 몇 가지 아이디어가 있습니다:</p>
<ul>
<li>예상치 못한 응답 처리 방법을 LLM에게 알려주세요: LLM에게 오류가 발생할 수 있다고 설명하고, 오류를 식별하는 방법과 오류 발생 시 어떻게 대처해야 하는지 알려주세요.</li>
<li>오류 발생 시 사용자에게 명확한 메시지를 제공하여 사용자가 무슨 일이 일어났는지 이해하고 어떻게 해결해야 하는지 알 수 있도록 해주세요.</li>
<li>자동 복구 동작 정의: 일부 경우, LLM은 장애로부터 자동으로 회복할 수 있습니다.</li>
</ul>
<h2>예시 #1: 오류 자동 수정</h2>
<p>“'user1'이라는 사용자를 생성하세요.”</p>
<div class="content-ad"></div>
<p>LLM(링크드리스트 매니저)는 이 사용자 생성을 요청할 수 있습니다. 사용자에 대한 자동 생성 설명은 "이는 관리자 사용자입니다!"와 같이 생성됩니다. 이 경우, 제품이 '!'의 사용이 불법적이라는 오류를 반환할 수 있습니다. 오직 알파벳과 숫자 값만 허용된다는 에러 메시지를 읽고, 유효하지 않은 문자를 포함하지 않는 설명을 사용하여 자동으로 수정하도록 LLM에 지시할 수 있습니다.</p>
<h2>예시 #2: 인증 문제 우회</h2>
<p>"'user1'이라는 사용자를 가져오세요."</p>
<p>제품 시스템에 인증하려는 우리가 사용하는 토큰이 만료되었을 수 있습니다. 이 경우, 제품이 "만료된 토큰"이라는 오류를 반환합니다.</p>
<div class="content-ad"></div>
<p>우리는 LLM에게 인증 문제를 처리하고 특별한 메시지를 반환하도록 지시할 수 있어서 토큰을 자동으로 새로 고치고 요청을 다시 시도할 수 있습니다.</p>
<h1>배포, 호스팅 및 주요 결정</h1>
<p>엔진을 구축하고 모델을 세밀하게 조정한 후 배포에 대한 결정을 내야 합니다. 다양한 장단점이 있는 많은 선택 사항 중 몇 가지 중요한 결정 사항에는 다음이 포함됩니다:</p>
<h2>엔진을 호스팅하는 위치</h2>
<div class="content-ad"></div>
<p>이것은 우리에게 아주 쉬운 결정이었습니다. SaaS를 지원하고 있기 때문에 이것이 우리의 최상의 선택입니다.</p>
<p>우리의 주요 클라우드 제공업체가 AWS이기 때문에 이것 또한 우리의 최상의 선택이었습니다.</p>
<h2>LLM 모델을 호스팅할 위치</h2>
<p>우리는 기능 호출 기능을 적극 활용하고 있는 OpenAI GPT 모델을 사용했는데, 이 모델이 최상의 결과를 제공했습니다. 일부 비교를 거친 후, 우리는 이 모델을 호스팅하는 가장 좋은 방법은 Azure OpenAI를 사용하는 것이라는 결론에 도달했습니다. 따라서 이 프로젝트는 두 개의 클라우드 제공업체를 기반으로 구축되었습니다 — 하지만 이는 전혀 문제가 되지 않습니다.</p>
<div class="content-ad"></div>
<h2>배포 전략 선택</h2>
<p>저는 상태를 가지지 않고 서버리스 솔루션을 선호하는 입장입니다.</p>
<p>이겢이 정적인, 상태를 가지는 기계에 호스팅되지 않는다는 뜻은 아닙니다. 하지만 저는 탄력성, 확장성, 배포 및 관리 측면에서 서버리스와 상태를 가지지 않는 것의 장점이 경쟁하기 너무 어렵다고 믿습니다.</p>
<p>이것이 우리가 다음 도전에 직면하게 되는 이유입니다...</p>
<div class="content-ad"></div>
<h2>LLM 엔진을 무상태(Stateless) 및 서버리스(Serverless)로 만들기</h2>
<p>LLM 엔진을 작성할 때, "세션" 또는 "대화"라는 개념에 마주치게 될 것입니다. 이 경우, 이 세션/대화의 상태를 필요할 때 로드될 수 있는 외부 위치로 추출해야 합니다. 외부 위치는 분산 캐시 또는 데이터베이스일 수 있으며, 여러 엔진 워커에서 액세스할 수 있습니다.</p>
<p>세션 또는 대화의 상태를 추출하는 단계는 다음과 같습니다.</p>
<p>Step 1) 세션 또는 대화 기록을 추출합니다. 과거 메시지를 고려하도록 LLM에게 전달하려면 대화 기록을 전달해야 합니다. 대화 기록은 별도의 위치에 유지되어 있어야 하며, 필요할 때 로드되어 프롬프트 내에 포함되어야 합니다.</p>
<div class="content-ad"></div>
<p>Step 2) LangChain 상태를 추출하세요. LangChain 라이브러리는 에이전트와 도구를 함께 사용하여 설계상 상태를 유지합니다. 메모리에 여러 가지를 보관하여 계속 진행하는 데 도움을 주거나, 현재까지 무슨 일이 있었는지 등의 힌트를 제공합니다. 우리가 마주한 실제 예시 중 하나는 "사용자 명확화 도구"를 사용하는 것입니다: LangChain이 이 도구를 사용하기로 결정하면 명확화를 위해 나가고, 그 후에 정확히 같은 위치에서 정확히 같은 상태로 계속해야 합니다.</p>
<p>이 문제를 해결하기 위해 LangChain 코드를 심층적으로 분석하고, 일부 부분을 다시 작성하여 상태를 외부 위치로 직렬화하고 역직렬화해야 했습니다.</p>
<p>이것은 가장 쉬운 일은 아니었지만, 좋은 소식은 가능하다는 것입니다!</p>
<p>Step 3) 프로젝트/어플리케이션의 비-LLM 상태를 추출하세요. LLM과 관련이 없는 상태가 있다면, 필요할 때 추출하고 로드해야 합니다.</p>
<div class="content-ad"></div>
<p>예를 들어, 엔진 워커 플로우는 다음과 같습니다:</p>
<p>사용자로부터 요청 받기 -> 외부 캐시 리소스에서 상태 로드 -> 요청 처리 -> LLM -> 상태를 외부 캐시 리소스에 저장 -> 사용자에게 응답 반환</p>
<p>아래 다이어그램에서도 확인할 수 있습니다:</p>
<h1>보안, 보안, 보안</h1>
<div class="content-ad"></div>
<p>알겠어요, 이제 무서운 부분에 도착했네요.</p>
<p>당신이 제어할 수 없는 것을 어떻게 안전하게 보호할 수 있을까요?</p>
<p>답은 있습니다: 가능한 한 많은 제어권을 얻어 내는 것으로!</p>
<p>먼저, LLMs를 위한 OWASP Top 10에 익숙해지는 것이 항상 좋은 생각입니다.</p>
<div class="content-ad"></div>
<p>제 경험을 토대로 몇 가지 추가적인 팁을 전해드릴게요:</p>
<h2>팁 #1: 탈옥(Jailbreak) 피하기</h2>
<p>LLMs에서 탈옥 개념은 LLM 프로젝트의 기본 기능 및 안전장치를 우회하고 사용자의 이익을 위해 사용하는 것을 의미합니다. 실제로 이것은 보호하기가 매우 어려운 부분 중 하나예요. 사용자의 요청을 충족시키기 위해 LLM이 규칙을 무시하는 것이 얼마나 쉽게 일어나는지 놀랄 것입니다. 프로젝트에 따라 탈옥으로부터 보호하기 위해 취해야 할 조치를 창의적으로 고민해봐야 할 수도 있어요.</p>
<p>아래에 몇 가지 아이디어가 있습니다:</p>
<div class="content-ad"></div>
<p>시스템 프롬프트를 사용하여 감옥 탈출을 피하기 위한 규칙을 LLM에 제공해보세요. 시스템 프롬프트 규칙은 사용자 프롬프트 규칙보다 더 심각하게 취급되며, 그것들은 무시되는 경우가 적습니다. 이 방법으로도 100%의 보호는 제공되지는 않습니다.</p>
<p>LLM의 기능을 제한하기 위해 화이트리스트 테크닉을 사용해보세요.</p>
<p>LLM 기능이 LangChain 도구를 사용한다면, 각 요청에 대해 사용하는 도구 중 하나가 확실히 사용되도록 할 수 있습니다. 이렇게 하면 요청이 내장 기능과 관련이 있는지 확인할 수 있습니다.</p>
<div class="content-ad"></div>
<p>여기 두 가지 예시가 있어요:</p>
<p>유효한 프롬프트</p>
<p>“’user1’이라는 사용자를 만들어주세요”:</p>
<p>이것은 사용자 기능 API를 사용하는 도구를 이용합니다.</p>
<div class="content-ad"></div>
<p>이 요청을 허용할 수 있어요.</p>
<p>유효하지 않은 프롬프트</p>
<p>“미국의 첫 번째 대통령은 누구였나요?”:</p>
<p>이 질문에 대한 답변을 위해 프로젝트에서 어떤 도구도 사용할 수 없지만, LLM은 자체 지식으로 답변하는 방법을 알고 있어요. 이 요청은 차단해야 해요.</p>
<div class="content-ad"></div>
<h2>팁 #2: 모델 접근을 보호하세요</h2>
<p>LLM 서비스를 호스팅하고 제공하는 것은 비용이 많이 들 수 있으며 할당량 제한이 있을 수 있습니다. 가능한 한 LLM에 대한 접근을 강화하는 것이 좋습니다. OpenAI 키를 사용하는 것 뿐만 아니라 네트워크를 보호하고 접근을 제한하는 것도 중요합니다.</p>
<p>저희 프로젝트에서 Azure OpenAI를 사용할 때는 LLM을 격리된 네트워크( Azure Virtual Network)에 배포할 수 있고, 엔진 사용만을 허용하도록 엔진과 연결된 AWS 엔진 람다를 전용 VPC(Amazon Virtual Private Cloud)에 설치할 수 있습니다. 또한 LLM과 엔진 간의 연결은 VPN을 통해 보호됩니다.</p>
<h2>팁 #3: 모욕적인 응답 차단하기</h2>
<div class="content-ad"></div>
<p>당신의 LLM 프로젝트가 고객을 욕하고 모욕하는 것을 상상해 보세요. 악몽 같죠? 이러한 시나리오를 피하기 위해 우리는 최선을 다해야 합니다.</p>
<p>사용할 수 있는 두 가지 방법은 다음과 같습니다:</p>
<ul>
<li>OpenAI moderation</li>
<li>Azure OpenAI content filtering</li>
</ul>
<h2>팁 #4: LLM 엔진에 제어점/훅을 추가하세요</h2>
<div class="content-ad"></div>
<p>LLM이 여러 작업을 차례대로 실행해야 하는 상황이 있다고 가정해 봅시다. 작업이 의미 있는지 확인하기 위해 "점검 포인트"를 추가하면 좋을 것입니다.</p>
<p>다음은 구체적인 예시입니다:</p>
<p>저희 프로젝트에서 LLM은 NL 입력을 사용하여 API와 매개변수를 생성하고, 제품의 API를 실행하고, 응답을 분석하고 필요할 때 재시도한 다음, 서식이 지정된 결과를 반환해야 합니다.</p>
<p>이 경우, API를 생성한 후 실행하기 전에 주요 제어 포인트가 발생합니다. API, 매개변수를 검증하고 입력을 사용하여 즉시 주입을 방지하기 위해 내용을 정리할 수 있습니다. 다른 제어 포인트는 결과를 고객에게 반환하기 전에 발생할 수 있습니다. 응답의 내용과 형식을 검증할 수 있습니다.</p>
<div class="content-ad"></div>
<h2>팁 #5: 평소 보안 가이드라인을 소홀히 하지 마세요</h2>
<p>이 프로젝트는 다른 프로젝트와 똑같은 위험을 안고 있습니다. 그 이상의 위험도 있습니다. 많은 사람들이 자신의 용도로 무료 LLM 액세스를 얻고자 하며, 특히 자신에게 금지된 자산에 액세스하려는 공격자도 있습니다.</p>
<p>다음에 집중하세요:</p>
<ul>
<li>인증 및 권한 부여. 새 제품에 액세스하는 사람이 그렇게 할 수 있는 권한이 있는지 확인하세요.</li>
<li>테넌트 격리. 항상 사용하던 것과 같은 테넌트 격리 보호를 유지하세요. 새로운 LLM 프로젝트가 중요 정보에 액세스할 수 있는 벽구멍이 되지 않도록 주의하세요.</li>
<li>방화벽 및 쓰로틀링. 받는 요청의 수를 제어하세요. 프로젝트가 남용되지 않도록 주의하세요. LLM이 부담을 견딜 수 있는지 확인하세요.</li>
</ul>
<div class="content-ad"></div>
<h1>고려해야 할 다른 사항</h1>
<p>제작 준비가 거의 끝났어요!</p>
<p>다음은 고려해야 할 몇 가지 사항입니다:</p>
<h2>피드백</h2>
<div class="content-ad"></div>
<p>당신이 LLM 프로젝트가 프로덕션 환경에서 잘 작동하는지 어떻게 알 수 있을까요? 고객이 결과에 만족했는지 어떻게 알 수 있을까요? 프로젝트를 계속 향상시키기 위해 결과를 따르고 평가할 수 있는 피드백 메커니즘을 고려하는 것이 현명할 수도 있겠죠.</p>
<h2>모델 평가</h2>
<p>출시하기 전에 모델이 원하는 대로 작동하는지 어떻게 알 수 있을까요? 모델에 적용하는 변경 사항에 대해 확신을 얻는 방법은 무엇인가요? 모델 평가는 매우 복잡한 주제입니다.</p>
<p>다음 사항을 고려해 보시기 바랍니다:</p>
<div class="content-ad"></div>
<ul>
<li>LLM 프로젝트의 기본 기능을 커버하는 테스트를 추가해보세요. 이번 테스트는 여태 익숙했던 것과는 다를 수 있어요 (새롭고 확정되지 않은 세상을 탐험한다고 언급한 적이 있죠?). 예를 들어, 정확한 단어 일치를 사용할 수 없을 수도 있어요. 대신에 중요한 기능을 테스트해보세요. 우리 프로젝트에서는 사용 사례를 제공한 다음 올바른 API가 선택되고 주요 매개변수가 올바르게 설정되었는지 확인해야 해요.</li>
<li>자체 LLM 엔진을 평가하기 위해 LLM을 활용해보세요. LLM 평가자는 사용 사례를 생성하고 예측을 제공한 다음 실제 결과와 비교할 수 있어요. 참 멋지죠?</li>
</ul>
<h2>법률</h2>
<p>새로운 LLM 프로젝트를 시작할 때는 무엇을 하고 있는지가... 법적으로... 합법적인지 확인하기 위해 법률 팀에 문의해보는 것이 좋아요.</p>
<p>이에는, AI/Gen-AI를 사용하기 위해 고객으로부터 동의를 얻는 것, LLM을 훈련시키기 위해 고객 데이터를 사용하지 않는 것, 대화에 관한 정보를 유지하지 않도록 LLM 호스팅 플랫폼을 어떻게 시행할 것인지, 규정 준수 등이 포함돼요 (예: GDPR 등).</p>
<div class="content-ad"></div>
<h1>여정의 끝</h1>
<p>저희 LLM 제품의 아키텍처를 보여주는 다이어그램이 여기 있어요. 이 블로그에서 설명한 모든 것을 포함하고 있습니다:</p>
<p>주요 참가자들은 다음과 같아요:</p>
<ul>
<li>GPT 모델. Azure OpenAI에 호스팅되며, 우리의 AWS VPC를 통해서만 안전하게 액세스됩니다.</li>
<li>전처리 워커. AWS Lambda에 호스팅되어 있습니다. 세션 시작 시 데이터(API 명세 파일)를 준비하고 외부 저장소에 처리된 데이터를 저장합니다.</li>
<li>엔진 워커. AWS Lambda에 호스팅되어 있습니다. 처리된 데이터와 상태를 로드하고, 우리의 LLM을 사용하여 관련 API 요청을 생성하고 제품에서 작업을 수행합니다.</li>
<li>피드백 워커. AWS Lambda에 호스팅되어 있습니다. 세션 피드백을 수집하고 저장하며, 세션 상태를 초기화합니다.</li>
</ul>
<div class="content-ad"></div>
<p>그리고 이것은 우리 여행의 끝입니다. 아니면 이제 막 시작인 걸까요? ☺</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"LLM 여행 개념 증명부터 제품화까지","description":"","date":"2024-06-19 19:40","slug":"2024-06-19-AnLLMJourneyFromPOCtoProduction","content":"\n\n\n![LLM Journey](/assets/img/2024-06-19-AnLLMJourneyFromPOCtoProduction_0.png)\n\n이렇게 상상해 봐: LLM(대형 언어 모델)을 사용하여 실행할 수 있는 멋진 프로젝트 아이디어가 있다고 상상해 봐. 빠르게 작업 가능한 개념 증명(PoC)에 도달했다. 네 자신에게 자랑스러워하며 실제로 동작하는 데 필요한 작업이 얼마나 적은지에 놀라게 될 것이다. (5줄의 프롬프트의 마법 ☺)\n\n하지만 이제 어떻게 해야 할까?\n\nLLM을 사용할 때 POC를 작성하는 것은 쉽지만, 실제로 탄생 가능한 제품을 만드는 것은 어려운 작업임을 빨리 깨닫게 될 것이다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 이 상황에 공감한다면, 이 게시물이 흥미로울 수 있습니다.\n\n# LLM 여정의 시작\n\n이 여정을 이해하는 가장 좋은 방법은 현재 진행 중인 LLM 프로젝트 중 하나를 살펴보는 것입니다. 여정의 첫 부분은 세 단계로 나눌 수 있습니다.\n\n## 동기 부여 찾기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n복잡한 제품들과 방대한 양의 정보들이 있는 세상에서, 우리 고객들은 종종 길을 잃기도 합니다. 때로 제품에서 기본 작업을 수행하려면 많은 문서 페이지를 읽고, 제품의 UI 페이지를 탐색하거나 로그, 보고서 및 기타 원시 자료를 분석해야 하는 경우도 있습니다.\n\n고객이 자주 묻는 질문 중 하나는 \"내가 원하는 대로 내 말로 말하기만 하면 시스템이 해결해주지 않을까?\" 입니다.\n\n답은 LLM을 사용하면 가능합니다!\n\n## 목표 선언\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 경우 프로젝트 목표는 고객이 제공한 자연어 (NL)를 사용하여 제품에 대한 작업을 수행하는 것이었습니다.\n\n먼저 우리 프로젝트는 제품의 API 명세 파일을 학습합니다 (보통 OpenAPI와 같은 표준 형식으로 API를 선언하는 파일) 그런 다음 LLM을 사용하여 NL 요청을 올바른 API로 변환합니다.\n\n## 작동하는 개념 증명 (POC)에 도달\n\n프로젝트 목표를 선언한 후 POC 단계에 도달합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPOC(Port of Call)의 목적은 우리가 가진 아이디어를 실제로 실행할 수 있는지 확인하는 것입니다.\n\n우리는 API 명세 파일과 자연어로 된 사용자 요청을 함께 입력으로 받아 사용자의 요청을 실행하는 기본 시스템을 구축해야 합니다.\n\n이를 위해, 우리는 우리의 LL(Large Language)를 OpenAI GPT로 선택하고, LLM 사용을 래핑하는 라이브러리로서 LangChain을 선택했습니다.\n\n우리는 입력 데이터를 받아 논리적 그룹(서비스)으로 처리하고, 사용자의 요청과 수행 방법에 대한 몇 가지 지침이 포함된 프롬프트와 함께 LangChain 라이브러리로 로드하는 엔진을 작성했습니다. 우리는 LangChain 체인, 도구 및 에이전트를 사용하여 이 작업을 수행했으며, OpenAI의 기능 호출 기능도 사용했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLangChain 도구 및 에이전트에 대한 자세한 내용은 여기에서 확인할 수 있어요.\n\n아래 다이어그램은 저희 POC의 주요 참가자들을 설명하고 있어요.\n\n축하해요, 작동하는 POC가 있어요! 이제 실제 여정이 시작돼요...\n\n# 먼저 해야 할 일: 정확성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최초의 POC 작업에 대한 설령 기대가 식어가기 시작했다 해도, LLM 결정 및 응답에서 일부 결함을 발견하기 시작했습니다.\n\n소프트웨어 산업에서는 종종 결정론적 알고리즘을 사용합니다 (즉, 동일한 입력에 대해 알고리즘은 항상 동일한 출력을 생성합니다).\n\n내 첫 번째 조언은 익숙한 결정론적 예상을 버리는 것입니다. 네, 심지어 OpenAI 온도가 0이더라도요.\n\n목표는 이 새로운 비결정론적 세계에서 어떻게 탐색할지를 배우는 것입니다. 다시 말해, 어떻게 더 예측 가능하게 만들고, 어떻게 다양한 응답을 처리할 수 있는지에 대해 알아보는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 팁 몇 가지 있어요:\n\n## 팁 #1: 올바른 프롬프트를 사용하고 있는지 확인하세요\n\nLLM에게 해야 할 일을 단계별로 설명해야 해요. 입력 내용, 형식, 의미, 예상 출력물, 형식, 의미를 설명해야 해요. 때로는 당신이 원했던 것보다 많은 프롬프트 라인을 사용해야 할 때도 있고, 예시를 포함해야 할 때도 있어요.\n\n아래 Few-Shots Learning 예제에서 프롬프트에 사용 사례를 추가하는 장점을 볼 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 요청을 받으셨군요: \"사용자 'user1'의 사용자 세부 정보를 가져와 해당 세부 정보로 새 사용자를 생성하세요\".\n\n상당히 기본적이고 명확한 요청이죠? 그렇다고 생각했다면, 틀렸어요! LLM이 한 순환 그 구체적인 단어 \"those\"를 새 사용자 세부 정보에 넣으려는 기이한 행동을 믿을 수 없을 겁니다. 대신 위 요청의 예시를 팁으로 제시했더니 정상적으로 작동했어요:\n\n## 팁 #2: 당신의 LLM에게 도구를 제공하세요\n\n가끔은 LLM이 스스로 올바른 조치에 대한 도달 방법을 모를 때가 있습니다. 이런 경우 LLM에게 하나의 툴 세트를 제공하여 LLM이 무엇을 해야 하는지 확신이 없을 때 사용할 수 있습니다. 이런 것은 LangChain 도구나 OpenAI 함수 호출로 쉽게 처리할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여러분이 사용할 수 있는 다양한 도구 예시를 드리겠습니다:\n\n- 날짜 도구는 날짜를 계산하는 데 도움을 줍니다. 예를 들어 \"어제의 모든 로그를 나에게 보여줘\" 라고 물어볼 수 있습니다. '어제'가 무엇인지 이해하는 데 어려움이 있을 수 있으므로, 날짜 도구를 사용하여 '어제'를 사용할 수 있는 타임스탬프로 변환할 수 있습니다.\n- 사용자/인간 도구는 사용자로부터 명확한 설명을 얻기 위해 사용됩니다. 예를 들어 \"새로운 사용자를 생성해줘\" 라고 요청할 수 있습니다. LLM이 추가 정보(예를 들어 사용자 이름)가 필요할 때 사용자 도구를 사용하여 사용자에게 원하는 사용자 이름이 무엇인지 물어볼 수 있습니다.\n\n## 팁 #3: LLM의 창의성 제한하기\n\n다른 제안으로, LLM의 창의성을 제한하여 LLM이 모르는 것들을 추측하는 대신에 명확한 설명을 요청하도록 하는 것이 좋습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, \"사용자 이름이 'user1'인 새 사용자를 생성하라\"고 요청할 때, API의 매개변수 중 하나는 암호입니다.\n\nLLM은 Password123이라는 생성된 비밀번호로 사용자를 만들려고 할 것입니다. 아마 원하는 바가 아닐 겁니다. 당신은 LLM에게 해당 사용 사례에서 추측하는 대신 명확히 설명하도록 요청할 수 있습니다.\n\n# 언제나 예기치 못한 것을 기대하세요\n\n이제 우리의 모든 사용 사례를 지원하고 \"happy path\"에서 매우 잘 수행하는 엔진을 만들었습니다. 그런데 오류 흐름은 어떠한가요? 예외적인 경우는 어떠한가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 처리 방법에 대한 몇 가지 아이디어가 있습니다:\n\n- 예상치 못한 응답 처리 방법을 LLM에게 알려주세요: LLM에게 오류가 발생할 수 있다고 설명하고, 오류를 식별하는 방법과 오류 발생 시 어떻게 대처해야 하는지 알려주세요.\n- 오류 발생 시 사용자에게 명확한 메시지를 제공하여 사용자가 무슨 일이 일어났는지 이해하고 어떻게 해결해야 하는지 알 수 있도록 해주세요.\n- 자동 복구 동작 정의: 일부 경우, LLM은 장애로부터 자동으로 회복할 수 있습니다.\n\n## 예시 #1: 오류 자동 수정\n\n“'user1'이라는 사용자를 생성하세요.”\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM(링크드리스트 매니저)는 이 사용자 생성을 요청할 수 있습니다. 사용자에 대한 자동 생성 설명은 \"이는 관리자 사용자입니다!\"와 같이 생성됩니다. 이 경우, 제품이 '!'의 사용이 불법적이라는 오류를 반환할 수 있습니다. 오직 알파벳과 숫자 값만 허용된다는 에러 메시지를 읽고, 유효하지 않은 문자를 포함하지 않는 설명을 사용하여 자동으로 수정하도록 LLM에 지시할 수 있습니다.\n\n## 예시 #2: 인증 문제 우회\n\n\"'user1'이라는 사용자를 가져오세요.\"\n\n제품 시스템에 인증하려는 우리가 사용하는 토큰이 만료되었을 수 있습니다. 이 경우, 제품이 \"만료된 토큰\"이라는 오류를 반환합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 LLM에게 인증 문제를 처리하고 특별한 메시지를 반환하도록 지시할 수 있어서 토큰을 자동으로 새로 고치고 요청을 다시 시도할 수 있습니다.\n\n# 배포, 호스팅 및 주요 결정\n\n엔진을 구축하고 모델을 세밀하게 조정한 후 배포에 대한 결정을 내야 합니다. 다양한 장단점이 있는 많은 선택 사항 중 몇 가지 중요한 결정 사항에는 다음이 포함됩니다:\n\n## 엔진을 호스팅하는 위치\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 우리에게 아주 쉬운 결정이었습니다. SaaS를 지원하고 있기 때문에 이것이 우리의 최상의 선택입니다.\n\n우리의 주요 클라우드 제공업체가 AWS이기 때문에 이것 또한 우리의 최상의 선택이었습니다.\n\n## LLM 모델을 호스팅할 위치\n\n우리는 기능 호출 기능을 적극 활용하고 있는 OpenAI GPT 모델을 사용했는데, 이 모델이 최상의 결과를 제공했습니다. 일부 비교를 거친 후, 우리는 이 모델을 호스팅하는 가장 좋은 방법은 Azure OpenAI를 사용하는 것이라는 결론에 도달했습니다. 따라서 이 프로젝트는 두 개의 클라우드 제공업체를 기반으로 구축되었습니다 — 하지만 이는 전혀 문제가 되지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 배포 전략 선택\n\n저는 상태를 가지지 않고 서버리스 솔루션을 선호하는 입장입니다.\n\n이겢이 정적인, 상태를 가지는 기계에 호스팅되지 않는다는 뜻은 아닙니다. 하지만 저는 탄력성, 확장성, 배포 및 관리 측면에서 서버리스와 상태를 가지지 않는 것의 장점이 경쟁하기 너무 어렵다고 믿습니다.\n\n이것이 우리가 다음 도전에 직면하게 되는 이유입니다...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## LLM 엔진을 무상태(Stateless) 및 서버리스(Serverless)로 만들기\n\nLLM 엔진을 작성할 때, \"세션\" 또는 \"대화\"라는 개념에 마주치게 될 것입니다. 이 경우, 이 세션/대화의 상태를 필요할 때 로드될 수 있는 외부 위치로 추출해야 합니다. 외부 위치는 분산 캐시 또는 데이터베이스일 수 있으며, 여러 엔진 워커에서 액세스할 수 있습니다.\n\n세션 또는 대화의 상태를 추출하는 단계는 다음과 같습니다.\n\nStep 1) 세션 또는 대화 기록을 추출합니다. 과거 메시지를 고려하도록 LLM에게 전달하려면 대화 기록을 전달해야 합니다. 대화 기록은 별도의 위치에 유지되어 있어야 하며, 필요할 때 로드되어 프롬프트 내에 포함되어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nStep 2) LangChain 상태를 추출하세요. LangChain 라이브러리는 에이전트와 도구를 함께 사용하여 설계상 상태를 유지합니다. 메모리에 여러 가지를 보관하여 계속 진행하는 데 도움을 주거나, 현재까지 무슨 일이 있었는지 등의 힌트를 제공합니다. 우리가 마주한 실제 예시 중 하나는 \"사용자 명확화 도구\"를 사용하는 것입니다: LangChain이 이 도구를 사용하기로 결정하면 명확화를 위해 나가고, 그 후에 정확히 같은 위치에서 정확히 같은 상태로 계속해야 합니다.\n\n이 문제를 해결하기 위해 LangChain 코드를 심층적으로 분석하고, 일부 부분을 다시 작성하여 상태를 외부 위치로 직렬화하고 역직렬화해야 했습니다.\n\n이것은 가장 쉬운 일은 아니었지만, 좋은 소식은 가능하다는 것입니다!\n\nStep 3) 프로젝트/어플리케이션의 비-LLM 상태를 추출하세요. LLM과 관련이 없는 상태가 있다면, 필요할 때 추출하고 로드해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, 엔진 워커 플로우는 다음과 같습니다:\n\n사용자로부터 요청 받기 -\u003e 외부 캐시 리소스에서 상태 로드 -\u003e 요청 처리 -\u003e LLM -\u003e 상태를 외부 캐시 리소스에 저장 -\u003e 사용자에게 응답 반환\n\n아래 다이어그램에서도 확인할 수 있습니다:\n\n# 보안, 보안, 보안\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알겠어요, 이제 무서운 부분에 도착했네요.\n\n당신이 제어할 수 없는 것을 어떻게 안전하게 보호할 수 있을까요?\n\n답은 있습니다: 가능한 한 많은 제어권을 얻어 내는 것으로!\n\n먼저, LLMs를 위한 OWASP Top 10에 익숙해지는 것이 항상 좋은 생각입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 경험을 토대로 몇 가지 추가적인 팁을 전해드릴게요:\n\n## 팁 #1: 탈옥(Jailbreak) 피하기\n\nLLMs에서 탈옥 개념은 LLM 프로젝트의 기본 기능 및 안전장치를 우회하고 사용자의 이익을 위해 사용하는 것을 의미합니다. 실제로 이것은 보호하기가 매우 어려운 부분 중 하나예요. 사용자의 요청을 충족시키기 위해 LLM이 규칙을 무시하는 것이 얼마나 쉽게 일어나는지 놀랄 것입니다. 프로젝트에 따라 탈옥으로부터 보호하기 위해 취해야 할 조치를 창의적으로 고민해봐야 할 수도 있어요.\n\n아래에 몇 가지 아이디어가 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시스템 프롬프트를 사용하여 감옥 탈출을 피하기 위한 규칙을 LLM에 제공해보세요. 시스템 프롬프트 규칙은 사용자 프롬프트 규칙보다 더 심각하게 취급되며, 그것들은 무시되는 경우가 적습니다. 이 방법으로도 100%의 보호는 제공되지는 않습니다.\n\nLLM의 기능을 제한하기 위해 화이트리스트 테크닉을 사용해보세요.\n\nLLM 기능이 LangChain 도구를 사용한다면, 각 요청에 대해 사용하는 도구 중 하나가 확실히 사용되도록 할 수 있습니다. 이렇게 하면 요청이 내장 기능과 관련이 있는지 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 두 가지 예시가 있어요:\n\n유효한 프롬프트\n\n“’user1’이라는 사용자를 만들어주세요”:\n\n이것은 사용자 기능 API를 사용하는 도구를 이용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 요청을 허용할 수 있어요.\n\n유효하지 않은 프롬프트\n\n“미국의 첫 번째 대통령은 누구였나요?”:\n\n이 질문에 대한 답변을 위해 프로젝트에서 어떤 도구도 사용할 수 없지만, LLM은 자체 지식으로 답변하는 방법을 알고 있어요. 이 요청은 차단해야 해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 팁 #2: 모델 접근을 보호하세요\n\nLLM 서비스를 호스팅하고 제공하는 것은 비용이 많이 들 수 있으며 할당량 제한이 있을 수 있습니다. 가능한 한 LLM에 대한 접근을 강화하는 것이 좋습니다. OpenAI 키를 사용하는 것 뿐만 아니라 네트워크를 보호하고 접근을 제한하는 것도 중요합니다.\n\n저희 프로젝트에서 Azure OpenAI를 사용할 때는 LLM을 격리된 네트워크( Azure Virtual Network)에 배포할 수 있고, 엔진 사용만을 허용하도록 엔진과 연결된 AWS 엔진 람다를 전용 VPC(Amazon Virtual Private Cloud)에 설치할 수 있습니다. 또한 LLM과 엔진 간의 연결은 VPN을 통해 보호됩니다.\n\n## 팁 #3: 모욕적인 응답 차단하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당신의 LLM 프로젝트가 고객을 욕하고 모욕하는 것을 상상해 보세요. 악몽 같죠? 이러한 시나리오를 피하기 위해 우리는 최선을 다해야 합니다.\n\n사용할 수 있는 두 가지 방법은 다음과 같습니다:\n\n- OpenAI moderation\n- Azure OpenAI content filtering\n\n## 팁 #4: LLM 엔진에 제어점/훅을 추가하세요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM이 여러 작업을 차례대로 실행해야 하는 상황이 있다고 가정해 봅시다. 작업이 의미 있는지 확인하기 위해 \"점검 포인트\"를 추가하면 좋을 것입니다.\n\n다음은 구체적인 예시입니다:\n\n저희 프로젝트에서 LLM은 NL 입력을 사용하여 API와 매개변수를 생성하고, 제품의 API를 실행하고, 응답을 분석하고 필요할 때 재시도한 다음, 서식이 지정된 결과를 반환해야 합니다.\n\n이 경우, API를 생성한 후 실행하기 전에 주요 제어 포인트가 발생합니다. API, 매개변수를 검증하고 입력을 사용하여 즉시 주입을 방지하기 위해 내용을 정리할 수 있습니다. 다른 제어 포인트는 결과를 고객에게 반환하기 전에 발생할 수 있습니다. 응답의 내용과 형식을 검증할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 팁 #5: 평소 보안 가이드라인을 소홀히 하지 마세요\n\n이 프로젝트는 다른 프로젝트와 똑같은 위험을 안고 있습니다. 그 이상의 위험도 있습니다. 많은 사람들이 자신의 용도로 무료 LLM 액세스를 얻고자 하며, 특히 자신에게 금지된 자산에 액세스하려는 공격자도 있습니다.\n\n다음에 집중하세요:\n\n- 인증 및 권한 부여. 새 제품에 액세스하는 사람이 그렇게 할 수 있는 권한이 있는지 확인하세요.\n- 테넌트 격리. 항상 사용하던 것과 같은 테넌트 격리 보호를 유지하세요. 새로운 LLM 프로젝트가 중요 정보에 액세스할 수 있는 벽구멍이 되지 않도록 주의하세요.\n- 방화벽 및 쓰로틀링. 받는 요청의 수를 제어하세요. 프로젝트가 남용되지 않도록 주의하세요. LLM이 부담을 견딜 수 있는지 확인하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 고려해야 할 다른 사항\n\n제작 준비가 거의 끝났어요!\n\n다음은 고려해야 할 몇 가지 사항입니다:\n\n## 피드백\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당신이 LLM 프로젝트가 프로덕션 환경에서 잘 작동하는지 어떻게 알 수 있을까요? 고객이 결과에 만족했는지 어떻게 알 수 있을까요? 프로젝트를 계속 향상시키기 위해 결과를 따르고 평가할 수 있는 피드백 메커니즘을 고려하는 것이 현명할 수도 있겠죠.\n\n## 모델 평가\n\n출시하기 전에 모델이 원하는 대로 작동하는지 어떻게 알 수 있을까요? 모델에 적용하는 변경 사항에 대해 확신을 얻는 방법은 무엇인가요? 모델 평가는 매우 복잡한 주제입니다.\n\n다음 사항을 고려해 보시기 바랍니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- LLM 프로젝트의 기본 기능을 커버하는 테스트를 추가해보세요. 이번 테스트는 여태 익숙했던 것과는 다를 수 있어요 (새롭고 확정되지 않은 세상을 탐험한다고 언급한 적이 있죠?). 예를 들어, 정확한 단어 일치를 사용할 수 없을 수도 있어요. 대신에 중요한 기능을 테스트해보세요. 우리 프로젝트에서는 사용 사례를 제공한 다음 올바른 API가 선택되고 주요 매개변수가 올바르게 설정되었는지 확인해야 해요.\n- 자체 LLM 엔진을 평가하기 위해 LLM을 활용해보세요. LLM 평가자는 사용 사례를 생성하고 예측을 제공한 다음 실제 결과와 비교할 수 있어요. 참 멋지죠?\n\n## 법률\n\n새로운 LLM 프로젝트를 시작할 때는 무엇을 하고 있는지가... 법적으로... 합법적인지 확인하기 위해 법률 팀에 문의해보는 것이 좋아요.\n\n이에는, AI/Gen-AI를 사용하기 위해 고객으로부터 동의를 얻는 것, LLM을 훈련시키기 위해 고객 데이터를 사용하지 않는 것, 대화에 관한 정보를 유지하지 않도록 LLM 호스팅 플랫폼을 어떻게 시행할 것인지, 규정 준수 등이 포함돼요 (예: GDPR 등).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 여정의 끝\n\n저희 LLM 제품의 아키텍처를 보여주는 다이어그램이 여기 있어요. 이 블로그에서 설명한 모든 것을 포함하고 있습니다:\n\n주요 참가자들은 다음과 같아요:\n\n- GPT 모델. Azure OpenAI에 호스팅되며, 우리의 AWS VPC를 통해서만 안전하게 액세스됩니다.\n- 전처리 워커. AWS Lambda에 호스팅되어 있습니다. 세션 시작 시 데이터(API 명세 파일)를 준비하고 외부 저장소에 처리된 데이터를 저장합니다.\n- 엔진 워커. AWS Lambda에 호스팅되어 있습니다. 처리된 데이터와 상태를 로드하고, 우리의 LLM을 사용하여 관련 API 요청을 생성하고 제품에서 작업을 수행합니다.\n- 피드백 워커. AWS Lambda에 호스팅되어 있습니다. 세션 피드백을 수집하고 저장하며, 세션 상태를 초기화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 이것은 우리 여행의 끝입니다. 아니면 이제 막 시작인 걸까요? ☺","ogImage":{"url":"/assets/img/2024-06-19-AnLLMJourneyFromPOCtoProduction_0.png"},"coverImage":"/assets/img/2024-06-19-AnLLMJourneyFromPOCtoProduction_0.png","tag":["Tech"],"readingTime":11},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnLLMJourneyFromPOCtoProduction_0.png\" alt=\"LLM Journey\"\u003e\u003c/p\u003e\n\u003cp\u003e이렇게 상상해 봐: LLM(대형 언어 모델)을 사용하여 실행할 수 있는 멋진 프로젝트 아이디어가 있다고 상상해 봐. 빠르게 작업 가능한 개념 증명(PoC)에 도달했다. 네 자신에게 자랑스러워하며 실제로 동작하는 데 필요한 작업이 얼마나 적은지에 놀라게 될 것이다. (5줄의 프롬프트의 마법 ☺)\u003c/p\u003e\n\u003cp\u003e하지만 이제 어떻게 해야 할까?\u003c/p\u003e\n\u003cp\u003eLLM을 사용할 때 POC를 작성하는 것은 쉽지만, 실제로 탄생 가능한 제품을 만드는 것은 어려운 작업임을 빨리 깨닫게 될 것이다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e만약 이 상황에 공감한다면, 이 게시물이 흥미로울 수 있습니다.\u003c/p\u003e\n\u003ch1\u003eLLM 여정의 시작\u003c/h1\u003e\n\u003cp\u003e이 여정을 이해하는 가장 좋은 방법은 현재 진행 중인 LLM 프로젝트 중 하나를 살펴보는 것입니다. 여정의 첫 부분은 세 단계로 나눌 수 있습니다.\u003c/p\u003e\n\u003ch2\u003e동기 부여 찾기\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e복잡한 제품들과 방대한 양의 정보들이 있는 세상에서, 우리 고객들은 종종 길을 잃기도 합니다. 때로 제품에서 기본 작업을 수행하려면 많은 문서 페이지를 읽고, 제품의 UI 페이지를 탐색하거나 로그, 보고서 및 기타 원시 자료를 분석해야 하는 경우도 있습니다.\u003c/p\u003e\n\u003cp\u003e고객이 자주 묻는 질문 중 하나는 \"내가 원하는 대로 내 말로 말하기만 하면 시스템이 해결해주지 않을까?\" 입니다.\u003c/p\u003e\n\u003cp\u003e답은 LLM을 사용하면 가능합니다!\u003c/p\u003e\n\u003ch2\u003e목표 선언\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 경우 프로젝트 목표는 고객이 제공한 자연어 (NL)를 사용하여 제품에 대한 작업을 수행하는 것이었습니다.\u003c/p\u003e\n\u003cp\u003e먼저 우리 프로젝트는 제품의 API 명세 파일을 학습합니다 (보통 OpenAPI와 같은 표준 형식으로 API를 선언하는 파일) 그런 다음 LLM을 사용하여 NL 요청을 올바른 API로 변환합니다.\u003c/p\u003e\n\u003ch2\u003e작동하는 개념 증명 (POC)에 도달\u003c/h2\u003e\n\u003cp\u003e프로젝트 목표를 선언한 후 POC 단계에 도달합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003ePOC(Port of Call)의 목적은 우리가 가진 아이디어를 실제로 실행할 수 있는지 확인하는 것입니다.\u003c/p\u003e\n\u003cp\u003e우리는 API 명세 파일과 자연어로 된 사용자 요청을 함께 입력으로 받아 사용자의 요청을 실행하는 기본 시스템을 구축해야 합니다.\u003c/p\u003e\n\u003cp\u003e이를 위해, 우리는 우리의 LL(Large Language)를 OpenAI GPT로 선택하고, LLM 사용을 래핑하는 라이브러리로서 LangChain을 선택했습니다.\u003c/p\u003e\n\u003cp\u003e우리는 입력 데이터를 받아 논리적 그룹(서비스)으로 처리하고, 사용자의 요청과 수행 방법에 대한 몇 가지 지침이 포함된 프롬프트와 함께 LangChain 라이브러리로 로드하는 엔진을 작성했습니다. 우리는 LangChain 체인, 도구 및 에이전트를 사용하여 이 작업을 수행했으며, OpenAI의 기능 호출 기능도 사용했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eLangChain 도구 및 에이전트에 대한 자세한 내용은 여기에서 확인할 수 있어요.\u003c/p\u003e\n\u003cp\u003e아래 다이어그램은 저희 POC의 주요 참가자들을 설명하고 있어요.\u003c/p\u003e\n\u003cp\u003e축하해요, 작동하는 POC가 있어요! 이제 실제 여정이 시작돼요...\u003c/p\u003e\n\u003ch1\u003e먼저 해야 할 일: 정확성\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e최초의 POC 작업에 대한 설령 기대가 식어가기 시작했다 해도, LLM 결정 및 응답에서 일부 결함을 발견하기 시작했습니다.\u003c/p\u003e\n\u003cp\u003e소프트웨어 산업에서는 종종 결정론적 알고리즘을 사용합니다 (즉, 동일한 입력에 대해 알고리즘은 항상 동일한 출력을 생성합니다).\u003c/p\u003e\n\u003cp\u003e내 첫 번째 조언은 익숙한 결정론적 예상을 버리는 것입니다. 네, 심지어 OpenAI 온도가 0이더라도요.\u003c/p\u003e\n\u003cp\u003e목표는 이 새로운 비결정론적 세계에서 어떻게 탐색할지를 배우는 것입니다. 다시 말해, 어떻게 더 예측 가능하게 만들고, 어떻게 다양한 응답을 처리할 수 있는지에 대해 알아보는 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e여기 팁 몇 가지 있어요:\u003c/p\u003e\n\u003ch2\u003e팁 #1: 올바른 프롬프트를 사용하고 있는지 확인하세요\u003c/h2\u003e\n\u003cp\u003eLLM에게 해야 할 일을 단계별로 설명해야 해요. 입력 내용, 형식, 의미, 예상 출력물, 형식, 의미를 설명해야 해요. 때로는 당신이 원했던 것보다 많은 프롬프트 라인을 사용해야 할 때도 있고, 예시를 포함해야 할 때도 있어요.\u003c/p\u003e\n\u003cp\u003e아래 Few-Shots Learning 예제에서 프롬프트에 사용 사례를 추가하는 장점을 볼 수 있어요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다음 요청을 받으셨군요: \"사용자 'user1'의 사용자 세부 정보를 가져와 해당 세부 정보로 새 사용자를 생성하세요\".\u003c/p\u003e\n\u003cp\u003e상당히 기본적이고 명확한 요청이죠? 그렇다고 생각했다면, 틀렸어요! LLM이 한 순환 그 구체적인 단어 \"those\"를 새 사용자 세부 정보에 넣으려는 기이한 행동을 믿을 수 없을 겁니다. 대신 위 요청의 예시를 팁으로 제시했더니 정상적으로 작동했어요:\u003c/p\u003e\n\u003ch2\u003e팁 #2: 당신의 LLM에게 도구를 제공하세요\u003c/h2\u003e\n\u003cp\u003e가끔은 LLM이 스스로 올바른 조치에 대한 도달 방법을 모를 때가 있습니다. 이런 경우 LLM에게 하나의 툴 세트를 제공하여 LLM이 무엇을 해야 하는지 확신이 없을 때 사용할 수 있습니다. 이런 것은 LangChain 도구나 OpenAI 함수 호출로 쉽게 처리할 수 있어요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e여러분이 사용할 수 있는 다양한 도구 예시를 드리겠습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e날짜 도구는 날짜를 계산하는 데 도움을 줍니다. 예를 들어 \"어제의 모든 로그를 나에게 보여줘\" 라고 물어볼 수 있습니다. '어제'가 무엇인지 이해하는 데 어려움이 있을 수 있으므로, 날짜 도구를 사용하여 '어제'를 사용할 수 있는 타임스탬프로 변환할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e사용자/인간 도구는 사용자로부터 명확한 설명을 얻기 위해 사용됩니다. 예를 들어 \"새로운 사용자를 생성해줘\" 라고 요청할 수 있습니다. LLM이 추가 정보(예를 들어 사용자 이름)가 필요할 때 사용자 도구를 사용하여 사용자에게 원하는 사용자 이름이 무엇인지 물어볼 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e팁 #3: LLM의 창의성 제한하기\u003c/h2\u003e\n\u003cp\u003e다른 제안으로, LLM의 창의성을 제한하여 LLM이 모르는 것들을 추측하는 대신에 명확한 설명을 요청하도록 하는 것이 좋습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e예를 들어, \"사용자 이름이 'user1'인 새 사용자를 생성하라\"고 요청할 때, API의 매개변수 중 하나는 암호입니다.\u003c/p\u003e\n\u003cp\u003eLLM은 Password123이라는 생성된 비밀번호로 사용자를 만들려고 할 것입니다. 아마 원하는 바가 아닐 겁니다. 당신은 LLM에게 해당 사용 사례에서 추측하는 대신 명확히 설명하도록 요청할 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e언제나 예기치 못한 것을 기대하세요\u003c/h1\u003e\n\u003cp\u003e이제 우리의 모든 사용 사례를 지원하고 \"happy path\"에서 매우 잘 수행하는 엔진을 만들었습니다. 그런데 오류 흐름은 어떠한가요? 예외적인 경우는 어떠한가요?\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e여기 처리 방법에 대한 몇 가지 아이디어가 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e예상치 못한 응답 처리 방법을 LLM에게 알려주세요: LLM에게 오류가 발생할 수 있다고 설명하고, 오류를 식별하는 방법과 오류 발생 시 어떻게 대처해야 하는지 알려주세요.\u003c/li\u003e\n\u003cli\u003e오류 발생 시 사용자에게 명확한 메시지를 제공하여 사용자가 무슨 일이 일어났는지 이해하고 어떻게 해결해야 하는지 알 수 있도록 해주세요.\u003c/li\u003e\n\u003cli\u003e자동 복구 동작 정의: 일부 경우, LLM은 장애로부터 자동으로 회복할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e예시 #1: 오류 자동 수정\u003c/h2\u003e\n\u003cp\u003e“'user1'이라는 사용자를 생성하세요.”\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eLLM(링크드리스트 매니저)는 이 사용자 생성을 요청할 수 있습니다. 사용자에 대한 자동 생성 설명은 \"이는 관리자 사용자입니다!\"와 같이 생성됩니다. 이 경우, 제품이 '!'의 사용이 불법적이라는 오류를 반환할 수 있습니다. 오직 알파벳과 숫자 값만 허용된다는 에러 메시지를 읽고, 유효하지 않은 문자를 포함하지 않는 설명을 사용하여 자동으로 수정하도록 LLM에 지시할 수 있습니다.\u003c/p\u003e\n\u003ch2\u003e예시 #2: 인증 문제 우회\u003c/h2\u003e\n\u003cp\u003e\"'user1'이라는 사용자를 가져오세요.\"\u003c/p\u003e\n\u003cp\u003e제품 시스템에 인증하려는 우리가 사용하는 토큰이 만료되었을 수 있습니다. 이 경우, 제품이 \"만료된 토큰\"이라는 오류를 반환합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e우리는 LLM에게 인증 문제를 처리하고 특별한 메시지를 반환하도록 지시할 수 있어서 토큰을 자동으로 새로 고치고 요청을 다시 시도할 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e배포, 호스팅 및 주요 결정\u003c/h1\u003e\n\u003cp\u003e엔진을 구축하고 모델을 세밀하게 조정한 후 배포에 대한 결정을 내야 합니다. 다양한 장단점이 있는 많은 선택 사항 중 몇 가지 중요한 결정 사항에는 다음이 포함됩니다:\u003c/p\u003e\n\u003ch2\u003e엔진을 호스팅하는 위치\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이것은 우리에게 아주 쉬운 결정이었습니다. SaaS를 지원하고 있기 때문에 이것이 우리의 최상의 선택입니다.\u003c/p\u003e\n\u003cp\u003e우리의 주요 클라우드 제공업체가 AWS이기 때문에 이것 또한 우리의 최상의 선택이었습니다.\u003c/p\u003e\n\u003ch2\u003eLLM 모델을 호스팅할 위치\u003c/h2\u003e\n\u003cp\u003e우리는 기능 호출 기능을 적극 활용하고 있는 OpenAI GPT 모델을 사용했는데, 이 모델이 최상의 결과를 제공했습니다. 일부 비교를 거친 후, 우리는 이 모델을 호스팅하는 가장 좋은 방법은 Azure OpenAI를 사용하는 것이라는 결론에 도달했습니다. 따라서 이 프로젝트는 두 개의 클라우드 제공업체를 기반으로 구축되었습니다 — 하지만 이는 전혀 문제가 되지 않습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e배포 전략 선택\u003c/h2\u003e\n\u003cp\u003e저는 상태를 가지지 않고 서버리스 솔루션을 선호하는 입장입니다.\u003c/p\u003e\n\u003cp\u003e이겢이 정적인, 상태를 가지는 기계에 호스팅되지 않는다는 뜻은 아닙니다. 하지만 저는 탄력성, 확장성, 배포 및 관리 측면에서 서버리스와 상태를 가지지 않는 것의 장점이 경쟁하기 너무 어렵다고 믿습니다.\u003c/p\u003e\n\u003cp\u003e이것이 우리가 다음 도전에 직면하게 되는 이유입니다...\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003eLLM 엔진을 무상태(Stateless) 및 서버리스(Serverless)로 만들기\u003c/h2\u003e\n\u003cp\u003eLLM 엔진을 작성할 때, \"세션\" 또는 \"대화\"라는 개념에 마주치게 될 것입니다. 이 경우, 이 세션/대화의 상태를 필요할 때 로드될 수 있는 외부 위치로 추출해야 합니다. 외부 위치는 분산 캐시 또는 데이터베이스일 수 있으며, 여러 엔진 워커에서 액세스할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e세션 또는 대화의 상태를 추출하는 단계는 다음과 같습니다.\u003c/p\u003e\n\u003cp\u003eStep 1) 세션 또는 대화 기록을 추출합니다. 과거 메시지를 고려하도록 LLM에게 전달하려면 대화 기록을 전달해야 합니다. 대화 기록은 별도의 위치에 유지되어 있어야 하며, 필요할 때 로드되어 프롬프트 내에 포함되어야 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eStep 2) LangChain 상태를 추출하세요. LangChain 라이브러리는 에이전트와 도구를 함께 사용하여 설계상 상태를 유지합니다. 메모리에 여러 가지를 보관하여 계속 진행하는 데 도움을 주거나, 현재까지 무슨 일이 있었는지 등의 힌트를 제공합니다. 우리가 마주한 실제 예시 중 하나는 \"사용자 명확화 도구\"를 사용하는 것입니다: LangChain이 이 도구를 사용하기로 결정하면 명확화를 위해 나가고, 그 후에 정확히 같은 위치에서 정확히 같은 상태로 계속해야 합니다.\u003c/p\u003e\n\u003cp\u003e이 문제를 해결하기 위해 LangChain 코드를 심층적으로 분석하고, 일부 부분을 다시 작성하여 상태를 외부 위치로 직렬화하고 역직렬화해야 했습니다.\u003c/p\u003e\n\u003cp\u003e이것은 가장 쉬운 일은 아니었지만, 좋은 소식은 가능하다는 것입니다!\u003c/p\u003e\n\u003cp\u003eStep 3) 프로젝트/어플리케이션의 비-LLM 상태를 추출하세요. LLM과 관련이 없는 상태가 있다면, 필요할 때 추출하고 로드해야 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e예를 들어, 엔진 워커 플로우는 다음과 같습니다:\u003c/p\u003e\n\u003cp\u003e사용자로부터 요청 받기 -\u003e 외부 캐시 리소스에서 상태 로드 -\u003e 요청 처리 -\u003e LLM -\u003e 상태를 외부 캐시 리소스에 저장 -\u003e 사용자에게 응답 반환\u003c/p\u003e\n\u003cp\u003e아래 다이어그램에서도 확인할 수 있습니다:\u003c/p\u003e\n\u003ch1\u003e보안, 보안, 보안\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e알겠어요, 이제 무서운 부분에 도착했네요.\u003c/p\u003e\n\u003cp\u003e당신이 제어할 수 없는 것을 어떻게 안전하게 보호할 수 있을까요?\u003c/p\u003e\n\u003cp\u003e답은 있습니다: 가능한 한 많은 제어권을 얻어 내는 것으로!\u003c/p\u003e\n\u003cp\u003e먼저, LLMs를 위한 OWASP Top 10에 익숙해지는 것이 항상 좋은 생각입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e제 경험을 토대로 몇 가지 추가적인 팁을 전해드릴게요:\u003c/p\u003e\n\u003ch2\u003e팁 #1: 탈옥(Jailbreak) 피하기\u003c/h2\u003e\n\u003cp\u003eLLMs에서 탈옥 개념은 LLM 프로젝트의 기본 기능 및 안전장치를 우회하고 사용자의 이익을 위해 사용하는 것을 의미합니다. 실제로 이것은 보호하기가 매우 어려운 부분 중 하나예요. 사용자의 요청을 충족시키기 위해 LLM이 규칙을 무시하는 것이 얼마나 쉽게 일어나는지 놀랄 것입니다. 프로젝트에 따라 탈옥으로부터 보호하기 위해 취해야 할 조치를 창의적으로 고민해봐야 할 수도 있어요.\u003c/p\u003e\n\u003cp\u003e아래에 몇 가지 아이디어가 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e시스템 프롬프트를 사용하여 감옥 탈출을 피하기 위한 규칙을 LLM에 제공해보세요. 시스템 프롬프트 규칙은 사용자 프롬프트 규칙보다 더 심각하게 취급되며, 그것들은 무시되는 경우가 적습니다. 이 방법으로도 100%의 보호는 제공되지는 않습니다.\u003c/p\u003e\n\u003cp\u003eLLM의 기능을 제한하기 위해 화이트리스트 테크닉을 사용해보세요.\u003c/p\u003e\n\u003cp\u003eLLM 기능이 LangChain 도구를 사용한다면, 각 요청에 대해 사용하는 도구 중 하나가 확실히 사용되도록 할 수 있습니다. 이렇게 하면 요청이 내장 기능과 관련이 있는지 확인할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e여기 두 가지 예시가 있어요:\u003c/p\u003e\n\u003cp\u003e유효한 프롬프트\u003c/p\u003e\n\u003cp\u003e“’user1’이라는 사용자를 만들어주세요”:\u003c/p\u003e\n\u003cp\u003e이것은 사용자 기능 API를 사용하는 도구를 이용합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 요청을 허용할 수 있어요.\u003c/p\u003e\n\u003cp\u003e유효하지 않은 프롬프트\u003c/p\u003e\n\u003cp\u003e“미국의 첫 번째 대통령은 누구였나요?”:\u003c/p\u003e\n\u003cp\u003e이 질문에 대한 답변을 위해 프로젝트에서 어떤 도구도 사용할 수 없지만, LLM은 자체 지식으로 답변하는 방법을 알고 있어요. 이 요청은 차단해야 해요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e팁 #2: 모델 접근을 보호하세요\u003c/h2\u003e\n\u003cp\u003eLLM 서비스를 호스팅하고 제공하는 것은 비용이 많이 들 수 있으며 할당량 제한이 있을 수 있습니다. 가능한 한 LLM에 대한 접근을 강화하는 것이 좋습니다. OpenAI 키를 사용하는 것 뿐만 아니라 네트워크를 보호하고 접근을 제한하는 것도 중요합니다.\u003c/p\u003e\n\u003cp\u003e저희 프로젝트에서 Azure OpenAI를 사용할 때는 LLM을 격리된 네트워크( Azure Virtual Network)에 배포할 수 있고, 엔진 사용만을 허용하도록 엔진과 연결된 AWS 엔진 람다를 전용 VPC(Amazon Virtual Private Cloud)에 설치할 수 있습니다. 또한 LLM과 엔진 간의 연결은 VPN을 통해 보호됩니다.\u003c/p\u003e\n\u003ch2\u003e팁 #3: 모욕적인 응답 차단하기\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e당신의 LLM 프로젝트가 고객을 욕하고 모욕하는 것을 상상해 보세요. 악몽 같죠? 이러한 시나리오를 피하기 위해 우리는 최선을 다해야 합니다.\u003c/p\u003e\n\u003cp\u003e사용할 수 있는 두 가지 방법은 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpenAI moderation\u003c/li\u003e\n\u003cli\u003eAzure OpenAI content filtering\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e팁 #4: LLM 엔진에 제어점/훅을 추가하세요\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eLLM이 여러 작업을 차례대로 실행해야 하는 상황이 있다고 가정해 봅시다. 작업이 의미 있는지 확인하기 위해 \"점검 포인트\"를 추가하면 좋을 것입니다.\u003c/p\u003e\n\u003cp\u003e다음은 구체적인 예시입니다:\u003c/p\u003e\n\u003cp\u003e저희 프로젝트에서 LLM은 NL 입력을 사용하여 API와 매개변수를 생성하고, 제품의 API를 실행하고, 응답을 분석하고 필요할 때 재시도한 다음, 서식이 지정된 결과를 반환해야 합니다.\u003c/p\u003e\n\u003cp\u003e이 경우, API를 생성한 후 실행하기 전에 주요 제어 포인트가 발생합니다. API, 매개변수를 검증하고 입력을 사용하여 즉시 주입을 방지하기 위해 내용을 정리할 수 있습니다. 다른 제어 포인트는 결과를 고객에게 반환하기 전에 발생할 수 있습니다. 응답의 내용과 형식을 검증할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e팁 #5: 평소 보안 가이드라인을 소홀히 하지 마세요\u003c/h2\u003e\n\u003cp\u003e이 프로젝트는 다른 프로젝트와 똑같은 위험을 안고 있습니다. 그 이상의 위험도 있습니다. 많은 사람들이 자신의 용도로 무료 LLM 액세스를 얻고자 하며, 특히 자신에게 금지된 자산에 액세스하려는 공격자도 있습니다.\u003c/p\u003e\n\u003cp\u003e다음에 집중하세요:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e인증 및 권한 부여. 새 제품에 액세스하는 사람이 그렇게 할 수 있는 권한이 있는지 확인하세요.\u003c/li\u003e\n\u003cli\u003e테넌트 격리. 항상 사용하던 것과 같은 테넌트 격리 보호를 유지하세요. 새로운 LLM 프로젝트가 중요 정보에 액세스할 수 있는 벽구멍이 되지 않도록 주의하세요.\u003c/li\u003e\n\u003cli\u003e방화벽 및 쓰로틀링. 받는 요청의 수를 제어하세요. 프로젝트가 남용되지 않도록 주의하세요. LLM이 부담을 견딜 수 있는지 확인하세요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e고려해야 할 다른 사항\u003c/h1\u003e\n\u003cp\u003e제작 준비가 거의 끝났어요!\u003c/p\u003e\n\u003cp\u003e다음은 고려해야 할 몇 가지 사항입니다:\u003c/p\u003e\n\u003ch2\u003e피드백\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e당신이 LLM 프로젝트가 프로덕션 환경에서 잘 작동하는지 어떻게 알 수 있을까요? 고객이 결과에 만족했는지 어떻게 알 수 있을까요? 프로젝트를 계속 향상시키기 위해 결과를 따르고 평가할 수 있는 피드백 메커니즘을 고려하는 것이 현명할 수도 있겠죠.\u003c/p\u003e\n\u003ch2\u003e모델 평가\u003c/h2\u003e\n\u003cp\u003e출시하기 전에 모델이 원하는 대로 작동하는지 어떻게 알 수 있을까요? 모델에 적용하는 변경 사항에 대해 확신을 얻는 방법은 무엇인가요? 모델 평가는 매우 복잡한 주제입니다.\u003c/p\u003e\n\u003cp\u003e다음 사항을 고려해 보시기 바랍니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eLLM 프로젝트의 기본 기능을 커버하는 테스트를 추가해보세요. 이번 테스트는 여태 익숙했던 것과는 다를 수 있어요 (새롭고 확정되지 않은 세상을 탐험한다고 언급한 적이 있죠?). 예를 들어, 정확한 단어 일치를 사용할 수 없을 수도 있어요. 대신에 중요한 기능을 테스트해보세요. 우리 프로젝트에서는 사용 사례를 제공한 다음 올바른 API가 선택되고 주요 매개변수가 올바르게 설정되었는지 확인해야 해요.\u003c/li\u003e\n\u003cli\u003e자체 LLM 엔진을 평가하기 위해 LLM을 활용해보세요. LLM 평가자는 사용 사례를 생성하고 예측을 제공한 다음 실제 결과와 비교할 수 있어요. 참 멋지죠?\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e법률\u003c/h2\u003e\n\u003cp\u003e새로운 LLM 프로젝트를 시작할 때는 무엇을 하고 있는지가... 법적으로... 합법적인지 확인하기 위해 법률 팀에 문의해보는 것이 좋아요.\u003c/p\u003e\n\u003cp\u003e이에는, AI/Gen-AI를 사용하기 위해 고객으로부터 동의를 얻는 것, LLM을 훈련시키기 위해 고객 데이터를 사용하지 않는 것, 대화에 관한 정보를 유지하지 않도록 LLM 호스팅 플랫폼을 어떻게 시행할 것인지, 규정 준수 등이 포함돼요 (예: GDPR 등).\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e여정의 끝\u003c/h1\u003e\n\u003cp\u003e저희 LLM 제품의 아키텍처를 보여주는 다이어그램이 여기 있어요. 이 블로그에서 설명한 모든 것을 포함하고 있습니다:\u003c/p\u003e\n\u003cp\u003e주요 참가자들은 다음과 같아요:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGPT 모델. Azure OpenAI에 호스팅되며, 우리의 AWS VPC를 통해서만 안전하게 액세스됩니다.\u003c/li\u003e\n\u003cli\u003e전처리 워커. AWS Lambda에 호스팅되어 있습니다. 세션 시작 시 데이터(API 명세 파일)를 준비하고 외부 저장소에 처리된 데이터를 저장합니다.\u003c/li\u003e\n\u003cli\u003e엔진 워커. AWS Lambda에 호스팅되어 있습니다. 처리된 데이터와 상태를 로드하고, 우리의 LLM을 사용하여 관련 API 요청을 생성하고 제품에서 작업을 수행합니다.\u003c/li\u003e\n\u003cli\u003e피드백 워커. AWS Lambda에 호스팅되어 있습니다. 세션 피드백을 수집하고 저장하며, 세션 상태를 초기화합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그리고 이것은 우리 여행의 끝입니다. 아니면 이제 막 시작인 걸까요? ☺\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-AnLLMJourneyFromPOCtoProduction"},"buildId":"QYe6gFAUryFKFgjKBoIfo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>