<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유 | itposting" data-gatsby-head="true"/><meta property="og:title" content="윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy" data-gatsby-head="true"/><meta name="twitter:title" content="윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-23 16:15" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 23, 2024</span><span class="posts_reading_time__f7YPP">8<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>GenAI은 윤리적인 어려움이 있습니다. 데이터 리더들은 이를 어떻게 해결해야 할까요? 이 기사에서는 윤리적 AI의 필요성과 데이터 윤리가 AI 윤리임을 고려합니다.</h2>
<p><img src="/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png" alt="이미지"></p>
<p>기술 경쟁에서 미래 성공의 중요한 요소로 빠르게 이동하는 것은 항상 그렇습니다.</p>
<p>안타깝게도, 너무 빨리 움직이는 것은 무시해선 안 될 위험이 기다리고 있다는 것을 의미하기도 합니다.</p>
<div class="content-ad"></div>
<p>옛날부터 전해져 내려오는 이야기입니다. 한 순간에는 선사 시대 모기 유전자를 서열 진행하고, 다음 순간에는 공룡 테마 파크를 개장하고 세계 최초의 실패한 하이퍼루프를 설계하고 있죠 (하지만 분명히 마지막은 아닙니다).</p>
<p>GenAI에 관해서는 인생이 예술을 모방한다고 할 수 있어요.</p>
<p>우리가 AI를 알려진 양으로 생각하고 싶어도, 이 기술의 창조자조차도 그 작동 방식에 대해 완전히 확신하지 못하는 것이 현실입니다.</p>
<p>United Healthcare, 구글, 심지어 캐나다 법원 같은 곳에서 발생한 여러 고프로 AI 사태를 고려할 때, 우리가 어디서 잘못되었는지 다시 생각해 봐야 할 때입니다.</p>
<div class="content-ad"></div>
<p>이해를 돕기 위해 명확하게 말씀드리겠습니다. GenAI(및 AI 전반)는 결국 각 산업에서 중요한 역할을 하게 될 것으로 믿습니다. 공학 업무를 가속화하거나 일반적인 질문에 답변하는 데 이르기까지 모든 산업에서 중요한 역할을 하게 될 것입니다. 그러나 AI의 잠재적 가치를 실현하기 위해서는 먼저 AI 애플리케이션을 개발하는 방식과 데이터 팀이 그 역할에서 어떤 영향을 끼치는지에 대해 비판적으로 사고해야 합니다.</p>
<p>본 글에서는 AI의 세 가지 윤리적 고민, 데이터 팀의 참여 방식, 그리고 데이터 리더로서 당신이 오늘 할 수 있는 일로 내일을 위한 보다 윤리적이고 신뢰할 수 있는 AI를 전달하는 방법에 대해 알아볼 것입니다.</p>
<h1>AI 윤리의 세 가지 층위</h1>
<p>전 뉴욕타임스 데이터 및 인사이트 부문의 전 SVP인 동료인 Shane Murray와 대화를 나누던 중, 그는 처음으로 실제 윤리적 진퇴양난에 직면한 상황 중 하나를 공유해 주었습니다. 뉴욕타임스에서 금융 인센티브를 위한 기계 학습 모델을 개발하던 중, 할인을 결정할 수 있는 기계 학습 모델의 윤리적 영향에 대한 토론이 제기되었습니다.</p>
<div class="content-ad"></div>
<p>일단 할인 코드를 위한 머신 러닝 모델은 모든 것을 고려할 때 꽤 무해한 요청처럼 보였습니다. 그러나 몇 개의 할인 코드를 자동화하는 것이 얼마나 순박한 것인지라고 생각할 수 있는지에 상관없이, 그 비즈니스 문제에서 인간적 공감을 없애는 행위는 팀에게 여러 윤리적 고려사항을 만들었습니다.</p>
<p>단순하지만 기존에는 인간적인 활동이라고 여겨진 것들을 자동화하려는 경쟁은 순전히 실용적인 결정인 것처럼 보입니다 — 효율성을 향상시키는지 아닌지의 단순한 이분법일 뿐입니다. 그러나 인간의 판단을 어떤 부분에서든 배제하면, AI가 관련되었는지 여부와 상관없이, 그 과정의 인간적 영향을 직접적으로 관리하려는 능력도 함께 상실하게 됩니다.</p>
<p>그것은 실제 문제입니다.</p>
<p>인공지능 개발에서 주요한 윤리적 고려사항은 세 가지가 있습니다:</p>
<div class="content-ad"></div>
<ol>
<li>모델 편향</li>
</ol>
<p>뉴욕 타임즈에서의 토론 핵심에 다가가는 항목입니다. 모델 자체가 어떤 의도치 않은 결과를 가져다 줄 수 있어서 한 사람을 다른 사람에 비해 유리하게 하거나 불리하게 할 수 있습니까?</p>
<p>이곳에서의 도전 과제는 모든 다른 사항이 동일하다면, 모든 상호작용에 대해 공정하고 중립적인 결과를 일관되게 제공할 수 있도록 GenAI를 설계하는 것입니다.</p>
<ol start="2">
<li>AI 사용</li>
</ol>
<div class="content-ad"></div>
<p>AI의 윤리적 고려 중 가장 존중받는 부분은 해당 기술이 어떻게 활용될지, 그 사용 사례가 회사나 사회 전반에 어떤 영향을 미칠지를 이해하는 것입니다.</p>
<p>이 AI는 윤리적인 목적을 위해 설계되었습니까? 그 사용이 누군가에게 직접적이거나간접적으로 피해를 주지는 않는지? 그리고 궁극적으로 이 모델이 장기적으로 순수한 선을 제공할 것인가?</p>
<p>쥬라기 공원의 첫 번째 장면에서 이안 말콤 박사가 말한 것처럼, 단지 무언가를 만들 수 있다고 해서 반드시 만들어야 한다는 뜻은 아닙니다.</p>
<ol start="3">
<li>데이터 책임성</li>
</ol>
<div class="content-ad"></div>
<p>마지막으로, 데이터 팀들에게 가장 중요한 고려 사항(또한 이 글에서 제가 대부분의 시간을 할애할 부분): 데이터 자체가 AI가 책임 있게 구축되고 활용되는 데 어떻게 영향을 미치는지에 대한 문제입니다.</p>
<p>이 고려 사항은 우리가 사용하는 데이터를 이해하고, 어떤 상황에서 안전하게 사용할 수 있는지, 그에 따른 위험 요소가 무엇인지를 다룹니다.</p>
<p>예를 들어, 우리는 데이터가 어디에서 왔는지, 어떻게 획득되었는지를 알고 있나요? 특정 모델에 공급되는 데이터에 개인정보 문제가 있는가요? 개인들을 피해로 치는 데 위험에 노출시키는 개인 데이터를 활용하고 있나요?</p>
<p>알 수 없는 데이터 위에 훈련된 LLM에서 안전하게 진행해도 괜찮을까요?</p>
<div class="content-ad"></div>
<p>New York Times가 OpenAI에 대한 소송에서 강조한 대로, 우리는 첫째로 이 데이터를 사용할 권리가 있는 걸까요?</p>
<p>데이터의 품질이 중요한 역할을 하는 곳이기도 합니다. 주어진 모델을 피드하는 데이터의 신뢰성을 신뢰할 수 있을까요? 퀄리티 문제가 AI 제품에 도달할 경우 잠재적인 결과는 무엇일까요?</p>
<p>따라서, 이러한 윤리적인 문제들을 전체적으로 쳐다보았으니, 이제 데이터 팀이 이에 대해 책임을 져야 하는 이유를 살펴봅시다.</p>
<h1>데이터 팀이 AI 윤리에 대해 책임져야 하는 이유</h1>
<div class="content-ad"></div>
<p>데이터 팀과 관련된 모든 윤리적 AI 고려사항 중에서 가장 중요한 것은 데이터 책임 문제입니다.</p>
<p>마찬가지로 GDPR이 비즈니스와 데이터 팀이 함께 데이터 수집 및 활용 방식을 재고하도록 강제했던 것처럼, GenAI는 기업이 어떤 워크플로우를 자동화할 수 있는지를 재고하도록 강제할 것입니다.</p>
<p>우리 데이터 팀으로서 어떤 AI 모델의 구축에 영향을 미치려는 책임이 절대적이지만, 그 설계 결과에 직접적으로 영향을 끼칠 수는 없습니다. 그러나 그 모델에서 잘못된 데이터를 거를 수 있다면, 그 설계 결함에 따른 위험을 완화하는 데 많은 도움이 될 수 있습니다.</p>
<p>모델 자체가 우리의 통제 영역을 벗어나 있다면, 할지 말지에 대한 본질적인 질문은 전혀 다른 달에 있는 것입니다. 우리는 그 문제점을 발견하면 지적할 책임이 있지만, 마무리로 말하자면, 우리가 탑승하든 말든, 로켓은 발사될 것입니다.
가장 중요한 것은 로켓이 안전하게 발사되도록 하는 것입니다. (아니면 비행기 몸통을 훔치는 것도...)</p>
<div class="content-ad"></div>
<p>그래서 데이터 엔지니어들의 삶의 모든 영역과 마찬가지로, 우리가 시간과 노력을 투자하고 싶은 곳은 최대한 많은 사람들에게 가장 직접적인 영향을 미칠 수 있는 곳입니다. 그 기회는 데이터 자체에 있다는 것이 사실입니다.</p>
<h1>데이터 팀이 데이터 책임성에 신경 써야 하는 이유</h1>
<p>말해봐야할 것 같지만 너무 당연한 것 같지만, 그래도 말하겠습니다:</p>
<p>데이터 팀은 데이터가 AI 모델로 어떻게 활용되는지에 대한 책임을 져야 합니다. 왜냐하면 단순히 말하자면 그들만이 그것을 할 수 있는 유일한 팀이기 때문입니다. 물론, 준수 팀, 보안 팀, 심지어 법률 팀들이 윤리가 무시될 때 책임을 져야 할 수 있습니다. 그러나 얼마나 많은 책임을 공유할 수 있더라도, 그 팀들이 결국은 데이터 팀만큼 데이터를 동일한 수준에서 이해하지는 못할 것입니다.</p>
<div class="content-ad"></div>
<p>소프트웨어 엔지니어링 팀이 OpenAI나 Anthropic과 같은 써드파티 LLM을 사용하여 앱을 만든다고 상상해보세요. 그러나 그들은 그들의 애플리케이션에 실제로 필요한 데이터 외에도 위치 데이터를 추적하고 저장하고 있다는 것을 깨닫지 못했습니다. 그들은 모델을 구동하기 위해 전체 데이터베이스를 활용합니다. 올바른 논리 결함이 있으면 나쁜 행위자가 그 데이터세트에 저장된 데이터를 사용하여 임의의 개인을 추적하는 프롬프트를 손쉽게 공학적으로 만들 수 있습니다. (이것은 오픈 소스 LLM과 폐쇄 소스 LLM 사이의 긴장 관계입니다.)</p>
<p>또는 소프트웨어 팀이 위치 데이터에 대해 알고 있지만 위치 데이터가 실제로 근삿값일 수 있다는 것을 깨닫지 못했습니다. 그들은 그 위치 데이터를 사용하여 16세 소년을 단순히 거리에 있는 피자 헛(Pizza Hut)이 아닌 어둡고 좁은 골목으로 안내하는 AI 매핑 기술을 만들 수 있습니다. 물론 이러한 유형의 오류는 의지적이 아니지만 데이터가 활용되는 방식에 내재된 의도하지 않은 위험을 강조합니다.</p>
<p>이러한 예시와 다른 사례들은 윤리적 AI에 관한 문제에서 데이터 팀의 역할을 강조합니다.</p>
<h1>그렇다면, 데이터 팀이 윤리적으로 유지하는 방법은 무엇일까요?</h1>
<div class="content-ad"></div>
<p>대부분의 경우, 데이터 팀은 모델이 작동하도록 대략적이고 대리 데이터를 다루는 데 익숙합니다. 그러나 AI 모델을 제공하는 데이터의 경우, 실제로 훨씬 더 높은 수준의 검증이 필요합니다.</p>
<p>소비자를 위해 틈을 메우기 위해 데이터 팀은 데이터 관행과 그 관행이 조직 전반과 어떻게 관련되는지 신중히 살펴봐야 합니다.</p>
<p>AI의 위험을 완화하는 방법을 고려할 때, 아래는 데이터 팀이 미래에 더 윤리적인 방향으로 AI를 움직이기 위해 취해야 할 3단계입니다.</p>
<h1>1. 회의 참석 자리를 얻으세요</h1>
<div class="content-ad"></div>
<p>데이터 팀은 타조가 아닙니다 - 머리를 모래속에 파묻고 문제가 사라지기를 기대할 수 없습니다. 데이터 팀이 리더십 테이블에 자리를 놓기 위해 싸워온 것처럼, 데이터 팀은 AI 테이블에 자리를 얻을 수 있도록 옹호해야 합니다.</p>
<p>어떤 데이터 품질 위기든, 지구가 이미 타버린 후에 뛰어든다면 충분하지 않습니다. GenAI에 고유하게 내재된 종말적 위험을 다룰 때, 우리 자신의 개인적 책임에 대해 선제적으로 대처하는 것이 이전보다 중요합니다.</p>
<p>그들이 당신이 테이블에 앉을 수 있도록 허락하지 않는다면, 바깥에서 교육하는 책임이 있습니다. 당신의 힘을 다하여 훌륭한 발견, 거버넌스 및 데이터 품질 솔루션을 제공하여 그 팀들에게 정보를 제공하면서 책임 있는 결정을 내릴 수 있도록 해야 합니다. 그들에게 사용할 것, 언제 사용할 것, 그리고 당신 팀의 내부 프로토콜로 유효성을 검증할 수 없는 써드파티 데이터 사용의 위험성을 가르쳐 주세요.</p>
<p>이것은 단지 비즈니스 문제가 아닙니다. 유나이티드 헬스케어와 브리티시 컬럼비아 주가 말할 것처럼, 많은 경우, 이들은 실제 사람들의 삶과 생계에 관한 것입니다. 그러니, 우리가 그 관점으로 운영하고 있는지 확인합시다.</p>
<div class="content-ad"></div>
<h1>2. RAG과 같은 방법론을 활용하여 더 책임감 있는 - 그리고 신뢰할 수 있는 - 데이터 조직화하기</h1>
<p>우리는 종종 RAG(검색 증강 생성) 같은 방법론을 언급하며 인공 지능으로부터 가치를 창출하는 자원으로 생각합니다. 그러나 그것은 그 AI를 구축하고 사용하는 방식을 보호하는 자원이기도 합니다.</p>
<p>예를 들어, 모델이 소비자를 대상으로 하는 채팅 앱에 공급하기 위해 개인 고객 데이터에 액세스하는 경우를 상상해보십시오. 올바른 사용자 프롬프트가 모델이 동작하여 심각한 PII가 폭로되어 나쁜 행위자가 그것을 취할 수도 있습니다. 그래서 데이터가 어디에서 왔는지 확인하고 제어하는 능력은 그 AI 제품의 무결성을 보호하는 데 중요합니다.</p>
<p>책임있는 데이터 팀은 RAG와 같은 방법론을 활용하여 준수되고 더 안전하며 모델에 적합한 데이터를 신중하게 조직화하여 많은 위험을 완화합니다.</p>
<div class="content-ad"></div>
<p>AI 개발에 RAG(빨강, 주황, 초록) 접근 방식을 취하면 너무 많은 데이터를 처리할 때 발생할 수 있는 위험을 최소화하는 데 도움이 됩니다 — 우리의 위치 데이터 예시에서 언급된 대로.</p>
<p>그것이 실제로 어떻게 보이는지 궁금하시죠? 예를 들어 Netflix와 같은 미디어 회사라고 가정해봅시다. 이 회사는 고객 데이터의 일부를 활용하여 맞춤형 추천 모델을 만들어야 합니다. 그런 다음 해당 사용 사례에 대한 구체적이고 제한된 데이터 포인트를 정의하고, 그 데이터를 유지하고 유효성을 검사할 책임자가 누구인지, 어떤 상황에서 데이터를 안전하게 사용할 수 있는지, 그리고 시간이 지남에 따라 그 AI 제품을 개발하고 유지할 가장 적합한 사람이 누구인지를 보다 효과적으로 정의할 수 있게 됩니다.</p>
<p>Data lineage과 같은 도구는 데이터의 출처를 신속하게 확인하여 팀이 어떤 시점에서 데이터가 어디에서 유래되고 사용되는지 또는 잘못 사용되는지를 확인할 수 있도록 도와줄 수도 있습니다.</p>
<div class="content-ad"></div>
<h1>3. 데이터 신뢰도를 우선시하세요</h1>
<p>데이터 제품에 대해 이야기할 때 자주 하는 말이 있습니다. "쓰레기 데이터를 넣으면 쓰레기 데이터가 나온다," 하지만 GenAI의 경우에는 그 말이 조금 모자라다고 볼 수 있습니다. 실제로 쓰레기 데이터가 AI 모델로 들어가면 쓰레기만 나오는 것이 아니라 실제로 사람에게도 영향을 미치는 결과가 나올 수 있습니다.</p>
<p>그래서 모델로 공급되는 데이터를 제어하기 위해 RAG 아키텍처가 필요한만큼, Pinecone과 같은 벡터 데이터베이스에 연결되는 강력한 데이터 관찰 기능이 필요합니다. 데이터가 실제로 깨끗하고 안전하며 신뢰할 수 있는지 확인하기 위해 이 연결이 중요합니다.</p>
<p>AI를 시작하는 고객들로부터 가장 많이 들은 불만 중 하나는, 제품용 AI를 준비하는 것은, 벡터 데이터 파이프라인으로 인덱스를 적극적으로 모니터링하지 않는다면 데이터의 신뢰성을 검증하는 것이 거의 불가능하다는 것입니다.</p>
<div class="content-ad"></div>
<p>대부분의 경우, 데이터 및 AI 엔지니어들이 데이터에 문제가 발생했음을 알게 되는 유일한 방법은 모델이 나쁜 프롬프트 응답을 내뱉을 때입니다. 그때에는 이미 너무 늦은 시점이죠.</p>
<h1>현재가 가장 좋은 때입니다</h1>
<p>2019년에 우리 팀이 데이터 관찰 카테고리를 만들었던 것은 데이터의 신뢰성과 신뢰의 필요성을 높인 동일한 과제입니다.</p>
<p>오늘날 AI가 일상적으로 의지하는 다양한 프로세스 및 시스템을 바꿔놓을 것이라고 약속하는 가운데, 데이터 품질의 도전과 더 중요한 것으로서의 윤리적 영향이 더욱 심각해지고 있습니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"윤리적 AI 개발, 데이터 팀에서 시작해야 하는 이유","description":"","date":"2024-06-23 16:15","slug":"2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy","content":"\n\n## GenAI은 윤리적인 어려움이 있습니다. 데이터 리더들은 이를 어떻게 해결해야 할까요? 이 기사에서는 윤리적 AI의 필요성과 데이터 윤리가 AI 윤리임을 고려합니다.\n\n![이미지](/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png)\n\n기술 경쟁에서 미래 성공의 중요한 요소로 빠르게 이동하는 것은 항상 그렇습니다.\n\n안타깝게도, 너무 빨리 움직이는 것은 무시해선 안 될 위험이 기다리고 있다는 것을 의미하기도 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n옛날부터 전해져 내려오는 이야기입니다. 한 순간에는 선사 시대 모기 유전자를 서열 진행하고, 다음 순간에는 공룡 테마 파크를 개장하고 세계 최초의 실패한 하이퍼루프를 설계하고 있죠 (하지만 분명히 마지막은 아닙니다).\n\nGenAI에 관해서는 인생이 예술을 모방한다고 할 수 있어요.\n\n우리가 AI를 알려진 양으로 생각하고 싶어도, 이 기술의 창조자조차도 그 작동 방식에 대해 완전히 확신하지 못하는 것이 현실입니다.\n\nUnited Healthcare, 구글, 심지어 캐나다 법원 같은 곳에서 발생한 여러 고프로 AI 사태를 고려할 때, 우리가 어디서 잘못되었는지 다시 생각해 봐야 할 때입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이해를 돕기 위해 명확하게 말씀드리겠습니다. GenAI(및 AI 전반)는 결국 각 산업에서 중요한 역할을 하게 될 것으로 믿습니다. 공학 업무를 가속화하거나 일반적인 질문에 답변하는 데 이르기까지 모든 산업에서 중요한 역할을 하게 될 것입니다. 그러나 AI의 잠재적 가치를 실현하기 위해서는 먼저 AI 애플리케이션을 개발하는 방식과 데이터 팀이 그 역할에서 어떤 영향을 끼치는지에 대해 비판적으로 사고해야 합니다.\n\n본 글에서는 AI의 세 가지 윤리적 고민, 데이터 팀의 참여 방식, 그리고 데이터 리더로서 당신이 오늘 할 수 있는 일로 내일을 위한 보다 윤리적이고 신뢰할 수 있는 AI를 전달하는 방법에 대해 알아볼 것입니다.\n\n# AI 윤리의 세 가지 층위\n\n전 뉴욕타임스 데이터 및 인사이트 부문의 전 SVP인 동료인 Shane Murray와 대화를 나누던 중, 그는 처음으로 실제 윤리적 진퇴양난에 직면한 상황 중 하나를 공유해 주었습니다. 뉴욕타임스에서 금융 인센티브를 위한 기계 학습 모델을 개발하던 중, 할인을 결정할 수 있는 기계 학습 모델의 윤리적 영향에 대한 토론이 제기되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일단 할인 코드를 위한 머신 러닝 모델은 모든 것을 고려할 때 꽤 무해한 요청처럼 보였습니다. 그러나 몇 개의 할인 코드를 자동화하는 것이 얼마나 순박한 것인지라고 생각할 수 있는지에 상관없이, 그 비즈니스 문제에서 인간적 공감을 없애는 행위는 팀에게 여러 윤리적 고려사항을 만들었습니다.\n\n단순하지만 기존에는 인간적인 활동이라고 여겨진 것들을 자동화하려는 경쟁은 순전히 실용적인 결정인 것처럼 보입니다 — 효율성을 향상시키는지 아닌지의 단순한 이분법일 뿐입니다. 그러나 인간의 판단을 어떤 부분에서든 배제하면, AI가 관련되었는지 여부와 상관없이, 그 과정의 인간적 영향을 직접적으로 관리하려는 능력도 함께 상실하게 됩니다.\n\n그것은 실제 문제입니다.\n\n인공지능 개발에서 주요한 윤리적 고려사항은 세 가지가 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1. 모델 편향\n\n뉴욕 타임즈에서의 토론 핵심에 다가가는 항목입니다. 모델 자체가 어떤 의도치 않은 결과를 가져다 줄 수 있어서 한 사람을 다른 사람에 비해 유리하게 하거나 불리하게 할 수 있습니까?\n\n이곳에서의 도전 과제는 모든 다른 사항이 동일하다면, 모든 상호작용에 대해 공정하고 중립적인 결과를 일관되게 제공할 수 있도록 GenAI를 설계하는 것입니다.\n\n2. AI 사용\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI의 윤리적 고려 중 가장 존중받는 부분은 해당 기술이 어떻게 활용될지, 그 사용 사례가 회사나 사회 전반에 어떤 영향을 미칠지를 이해하는 것입니다.\n\n이 AI는 윤리적인 목적을 위해 설계되었습니까? 그 사용이 누군가에게 직접적이거나간접적으로 피해를 주지는 않는지? 그리고 궁극적으로 이 모델이 장기적으로 순수한 선을 제공할 것인가?\n\n쥬라기 공원의 첫 번째 장면에서 이안 말콤 박사가 말한 것처럼, 단지 무언가를 만들 수 있다고 해서 반드시 만들어야 한다는 뜻은 아닙니다.\n\n3. 데이터 책임성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, 데이터 팀들에게 가장 중요한 고려 사항(또한 이 글에서 제가 대부분의 시간을 할애할 부분): 데이터 자체가 AI가 책임 있게 구축되고 활용되는 데 어떻게 영향을 미치는지에 대한 문제입니다.\n\n이 고려 사항은 우리가 사용하는 데이터를 이해하고, 어떤 상황에서 안전하게 사용할 수 있는지, 그에 따른 위험 요소가 무엇인지를 다룹니다.\n\n예를 들어, 우리는 데이터가 어디에서 왔는지, 어떻게 획득되었는지를 알고 있나요? 특정 모델에 공급되는 데이터에 개인정보 문제가 있는가요? 개인들을 피해로 치는 데 위험에 노출시키는 개인 데이터를 활용하고 있나요?\n\n알 수 없는 데이터 위에 훈련된 LLM에서 안전하게 진행해도 괜찮을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNew York Times가 OpenAI에 대한 소송에서 강조한 대로, 우리는 첫째로 이 데이터를 사용할 권리가 있는 걸까요?\n\n데이터의 품질이 중요한 역할을 하는 곳이기도 합니다. 주어진 모델을 피드하는 데이터의 신뢰성을 신뢰할 수 있을까요? 퀄리티 문제가 AI 제품에 도달할 경우 잠재적인 결과는 무엇일까요?\n\n따라서, 이러한 윤리적인 문제들을 전체적으로 쳐다보았으니, 이제 데이터 팀이 이에 대해 책임을 져야 하는 이유를 살펴봅시다.\n\n# 데이터 팀이 AI 윤리에 대해 책임져야 하는 이유\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 팀과 관련된 모든 윤리적 AI 고려사항 중에서 가장 중요한 것은 데이터 책임 문제입니다.\n\n마찬가지로 GDPR이 비즈니스와 데이터 팀이 함께 데이터 수집 및 활용 방식을 재고하도록 강제했던 것처럼, GenAI는 기업이 어떤 워크플로우를 자동화할 수 있는지를 재고하도록 강제할 것입니다.\n\n우리 데이터 팀으로서 어떤 AI 모델의 구축에 영향을 미치려는 책임이 절대적이지만, 그 설계 결과에 직접적으로 영향을 끼칠 수는 없습니다. 그러나 그 모델에서 잘못된 데이터를 거를 수 있다면, 그 설계 결함에 따른 위험을 완화하는 데 많은 도움이 될 수 있습니다.\n\n모델 자체가 우리의 통제 영역을 벗어나 있다면, 할지 말지에 대한 본질적인 질문은 전혀 다른 달에 있는 것입니다. 우리는 그 문제점을 발견하면 지적할 책임이 있지만, 마무리로 말하자면, 우리가 탑승하든 말든, 로켓은 발사될 것입니다.\n가장 중요한 것은 로켓이 안전하게 발사되도록 하는 것입니다. (아니면 비행기 몸통을 훔치는 것도...)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서 데이터 엔지니어들의 삶의 모든 영역과 마찬가지로, 우리가 시간과 노력을 투자하고 싶은 곳은 최대한 많은 사람들에게 가장 직접적인 영향을 미칠 수 있는 곳입니다. 그 기회는 데이터 자체에 있다는 것이 사실입니다.\n\n# 데이터 팀이 데이터 책임성에 신경 써야 하는 이유\n\n말해봐야할 것 같지만 너무 당연한 것 같지만, 그래도 말하겠습니다:\n\n데이터 팀은 데이터가 AI 모델로 어떻게 활용되는지에 대한 책임을 져야 합니다. 왜냐하면 단순히 말하자면 그들만이 그것을 할 수 있는 유일한 팀이기 때문입니다. 물론, 준수 팀, 보안 팀, 심지어 법률 팀들이 윤리가 무시될 때 책임을 져야 할 수 있습니다. 그러나 얼마나 많은 책임을 공유할 수 있더라도, 그 팀들이 결국은 데이터 팀만큼 데이터를 동일한 수준에서 이해하지는 못할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n소프트웨어 엔지니어링 팀이 OpenAI나 Anthropic과 같은 써드파티 LLM을 사용하여 앱을 만든다고 상상해보세요. 그러나 그들은 그들의 애플리케이션에 실제로 필요한 데이터 외에도 위치 데이터를 추적하고 저장하고 있다는 것을 깨닫지 못했습니다. 그들은 모델을 구동하기 위해 전체 데이터베이스를 활용합니다. 올바른 논리 결함이 있으면 나쁜 행위자가 그 데이터세트에 저장된 데이터를 사용하여 임의의 개인을 추적하는 프롬프트를 손쉽게 공학적으로 만들 수 있습니다. (이것은 오픈 소스 LLM과 폐쇄 소스 LLM 사이의 긴장 관계입니다.)\n\n또는 소프트웨어 팀이 위치 데이터에 대해 알고 있지만 위치 데이터가 실제로 근삿값일 수 있다는 것을 깨닫지 못했습니다. 그들은 그 위치 데이터를 사용하여 16세 소년을 단순히 거리에 있는 피자 헛(Pizza Hut)이 아닌 어둡고 좁은 골목으로 안내하는 AI 매핑 기술을 만들 수 있습니다. 물론 이러한 유형의 오류는 의지적이 아니지만 데이터가 활용되는 방식에 내재된 의도하지 않은 위험을 강조합니다.\n\n이러한 예시와 다른 사례들은 윤리적 AI에 관한 문제에서 데이터 팀의 역할을 강조합니다.\n\n# 그렇다면, 데이터 팀이 윤리적으로 유지하는 방법은 무엇일까요?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대부분의 경우, 데이터 팀은 모델이 작동하도록 대략적이고 대리 데이터를 다루는 데 익숙합니다. 그러나 AI 모델을 제공하는 데이터의 경우, 실제로 훨씬 더 높은 수준의 검증이 필요합니다.\n\n소비자를 위해 틈을 메우기 위해 데이터 팀은 데이터 관행과 그 관행이 조직 전반과 어떻게 관련되는지 신중히 살펴봐야 합니다.\n\nAI의 위험을 완화하는 방법을 고려할 때, 아래는 데이터 팀이 미래에 더 윤리적인 방향으로 AI를 움직이기 위해 취해야 할 3단계입니다.\n\n# 1. 회의 참석 자리를 얻으세요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 팀은 타조가 아닙니다 - 머리를 모래속에 파묻고 문제가 사라지기를 기대할 수 없습니다. 데이터 팀이 리더십 테이블에 자리를 놓기 위해 싸워온 것처럼, 데이터 팀은 AI 테이블에 자리를 얻을 수 있도록 옹호해야 합니다.\n\n어떤 데이터 품질 위기든, 지구가 이미 타버린 후에 뛰어든다면 충분하지 않습니다. GenAI에 고유하게 내재된 종말적 위험을 다룰 때, 우리 자신의 개인적 책임에 대해 선제적으로 대처하는 것이 이전보다 중요합니다.\n\n그들이 당신이 테이블에 앉을 수 있도록 허락하지 않는다면, 바깥에서 교육하는 책임이 있습니다. 당신의 힘을 다하여 훌륭한 발견, 거버넌스 및 데이터 품질 솔루션을 제공하여 그 팀들에게 정보를 제공하면서 책임 있는 결정을 내릴 수 있도록 해야 합니다. 그들에게 사용할 것, 언제 사용할 것, 그리고 당신 팀의 내부 프로토콜로 유효성을 검증할 수 없는 써드파티 데이터 사용의 위험성을 가르쳐 주세요.\n\n이것은 단지 비즈니스 문제가 아닙니다. 유나이티드 헬스케어와 브리티시 컬럼비아 주가 말할 것처럼, 많은 경우, 이들은 실제 사람들의 삶과 생계에 관한 것입니다. 그러니, 우리가 그 관점으로 운영하고 있는지 확인합시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2. RAG과 같은 방법론을 활용하여 더 책임감 있는 - 그리고 신뢰할 수 있는 - 데이터 조직화하기\n\n우리는 종종 RAG(검색 증강 생성) 같은 방법론을 언급하며 인공 지능으로부터 가치를 창출하는 자원으로 생각합니다. 그러나 그것은 그 AI를 구축하고 사용하는 방식을 보호하는 자원이기도 합니다.\n\n예를 들어, 모델이 소비자를 대상으로 하는 채팅 앱에 공급하기 위해 개인 고객 데이터에 액세스하는 경우를 상상해보십시오. 올바른 사용자 프롬프트가 모델이 동작하여 심각한 PII가 폭로되어 나쁜 행위자가 그것을 취할 수도 있습니다. 그래서 데이터가 어디에서 왔는지 확인하고 제어하는 능력은 그 AI 제품의 무결성을 보호하는 데 중요합니다.\n\n책임있는 데이터 팀은 RAG와 같은 방법론을 활용하여 준수되고 더 안전하며 모델에 적합한 데이터를 신중하게 조직화하여 많은 위험을 완화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI 개발에 RAG(빨강, 주황, 초록) 접근 방식을 취하면 너무 많은 데이터를 처리할 때 발생할 수 있는 위험을 최소화하는 데 도움이 됩니다 — 우리의 위치 데이터 예시에서 언급된 대로.\n\n그것이 실제로 어떻게 보이는지 궁금하시죠? 예를 들어 Netflix와 같은 미디어 회사라고 가정해봅시다. 이 회사는 고객 데이터의 일부를 활용하여 맞춤형 추천 모델을 만들어야 합니다. 그런 다음 해당 사용 사례에 대한 구체적이고 제한된 데이터 포인트를 정의하고, 그 데이터를 유지하고 유효성을 검사할 책임자가 누구인지, 어떤 상황에서 데이터를 안전하게 사용할 수 있는지, 그리고 시간이 지남에 따라 그 AI 제품을 개발하고 유지할 가장 적합한 사람이 누구인지를 보다 효과적으로 정의할 수 있게 됩니다.\n\nData lineage과 같은 도구는 데이터의 출처를 신속하게 확인하여 팀이 어떤 시점에서 데이터가 어디에서 유래되고 사용되는지 또는 잘못 사용되는지를 확인할 수 있도록 도와줄 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 3. 데이터 신뢰도를 우선시하세요\n\n데이터 제품에 대해 이야기할 때 자주 하는 말이 있습니다. \"쓰레기 데이터를 넣으면 쓰레기 데이터가 나온다,\" 하지만 GenAI의 경우에는 그 말이 조금 모자라다고 볼 수 있습니다. 실제로 쓰레기 데이터가 AI 모델로 들어가면 쓰레기만 나오는 것이 아니라 실제로 사람에게도 영향을 미치는 결과가 나올 수 있습니다.\n\n그래서 모델로 공급되는 데이터를 제어하기 위해 RAG 아키텍처가 필요한만큼, Pinecone과 같은 벡터 데이터베이스에 연결되는 강력한 데이터 관찰 기능이 필요합니다. 데이터가 실제로 깨끗하고 안전하며 신뢰할 수 있는지 확인하기 위해 이 연결이 중요합니다.\n\nAI를 시작하는 고객들로부터 가장 많이 들은 불만 중 하나는, 제품용 AI를 준비하는 것은, 벡터 데이터 파이프라인으로 인덱스를 적극적으로 모니터링하지 않는다면 데이터의 신뢰성을 검증하는 것이 거의 불가능하다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대부분의 경우, 데이터 및 AI 엔지니어들이 데이터에 문제가 발생했음을 알게 되는 유일한 방법은 모델이 나쁜 프롬프트 응답을 내뱉을 때입니다. 그때에는 이미 너무 늦은 시점이죠.\n\n# 현재가 가장 좋은 때입니다\n\n2019년에 우리 팀이 데이터 관찰 카테고리를 만들었던 것은 데이터의 신뢰성과 신뢰의 필요성을 높인 동일한 과제입니다.\n\n오늘날 AI가 일상적으로 의지하는 다양한 프로세스 및 시스템을 바꿔놓을 것이라고 약속하는 가운데, 데이터 품질의 도전과 더 중요한 것으로서의 윤리적 영향이 더욱 심각해지고 있습니다.","ogImage":{"url":"/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png"},"coverImage":"/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png","tag":["Tech"],"readingTime":8},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003eGenAI은 윤리적인 어려움이 있습니다. 데이터 리더들은 이를 어떻게 해결해야 할까요? 이 기사에서는 윤리적 AI의 필요성과 데이터 윤리가 AI 윤리임을 고려합니다.\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e기술 경쟁에서 미래 성공의 중요한 요소로 빠르게 이동하는 것은 항상 그렇습니다.\u003c/p\u003e\n\u003cp\u003e안타깝게도, 너무 빨리 움직이는 것은 무시해선 안 될 위험이 기다리고 있다는 것을 의미하기도 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e옛날부터 전해져 내려오는 이야기입니다. 한 순간에는 선사 시대 모기 유전자를 서열 진행하고, 다음 순간에는 공룡 테마 파크를 개장하고 세계 최초의 실패한 하이퍼루프를 설계하고 있죠 (하지만 분명히 마지막은 아닙니다).\u003c/p\u003e\n\u003cp\u003eGenAI에 관해서는 인생이 예술을 모방한다고 할 수 있어요.\u003c/p\u003e\n\u003cp\u003e우리가 AI를 알려진 양으로 생각하고 싶어도, 이 기술의 창조자조차도 그 작동 방식에 대해 완전히 확신하지 못하는 것이 현실입니다.\u003c/p\u003e\n\u003cp\u003eUnited Healthcare, 구글, 심지어 캐나다 법원 같은 곳에서 발생한 여러 고프로 AI 사태를 고려할 때, 우리가 어디서 잘못되었는지 다시 생각해 봐야 할 때입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이해를 돕기 위해 명확하게 말씀드리겠습니다. GenAI(및 AI 전반)는 결국 각 산업에서 중요한 역할을 하게 될 것으로 믿습니다. 공학 업무를 가속화하거나 일반적인 질문에 답변하는 데 이르기까지 모든 산업에서 중요한 역할을 하게 될 것입니다. 그러나 AI의 잠재적 가치를 실현하기 위해서는 먼저 AI 애플리케이션을 개발하는 방식과 데이터 팀이 그 역할에서 어떤 영향을 끼치는지에 대해 비판적으로 사고해야 합니다.\u003c/p\u003e\n\u003cp\u003e본 글에서는 AI의 세 가지 윤리적 고민, 데이터 팀의 참여 방식, 그리고 데이터 리더로서 당신이 오늘 할 수 있는 일로 내일을 위한 보다 윤리적이고 신뢰할 수 있는 AI를 전달하는 방법에 대해 알아볼 것입니다.\u003c/p\u003e\n\u003ch1\u003eAI 윤리의 세 가지 층위\u003c/h1\u003e\n\u003cp\u003e전 뉴욕타임스 데이터 및 인사이트 부문의 전 SVP인 동료인 Shane Murray와 대화를 나누던 중, 그는 처음으로 실제 윤리적 진퇴양난에 직면한 상황 중 하나를 공유해 주었습니다. 뉴욕타임스에서 금융 인센티브를 위한 기계 학습 모델을 개발하던 중, 할인을 결정할 수 있는 기계 학습 모델의 윤리적 영향에 대한 토론이 제기되었습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e일단 할인 코드를 위한 머신 러닝 모델은 모든 것을 고려할 때 꽤 무해한 요청처럼 보였습니다. 그러나 몇 개의 할인 코드를 자동화하는 것이 얼마나 순박한 것인지라고 생각할 수 있는지에 상관없이, 그 비즈니스 문제에서 인간적 공감을 없애는 행위는 팀에게 여러 윤리적 고려사항을 만들었습니다.\u003c/p\u003e\n\u003cp\u003e단순하지만 기존에는 인간적인 활동이라고 여겨진 것들을 자동화하려는 경쟁은 순전히 실용적인 결정인 것처럼 보입니다 — 효율성을 향상시키는지 아닌지의 단순한 이분법일 뿐입니다. 그러나 인간의 판단을 어떤 부분에서든 배제하면, AI가 관련되었는지 여부와 상관없이, 그 과정의 인간적 영향을 직접적으로 관리하려는 능력도 함께 상실하게 됩니다.\u003c/p\u003e\n\u003cp\u003e그것은 실제 문제입니다.\u003c/p\u003e\n\u003cp\u003e인공지능 개발에서 주요한 윤리적 고려사항은 세 가지가 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003col\u003e\n\u003cli\u003e모델 편향\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e뉴욕 타임즈에서의 토론 핵심에 다가가는 항목입니다. 모델 자체가 어떤 의도치 않은 결과를 가져다 줄 수 있어서 한 사람을 다른 사람에 비해 유리하게 하거나 불리하게 할 수 있습니까?\u003c/p\u003e\n\u003cp\u003e이곳에서의 도전 과제는 모든 다른 사항이 동일하다면, 모든 상호작용에 대해 공정하고 중립적인 결과를 일관되게 제공할 수 있도록 GenAI를 설계하는 것입니다.\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eAI 사용\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eAI의 윤리적 고려 중 가장 존중받는 부분은 해당 기술이 어떻게 활용될지, 그 사용 사례가 회사나 사회 전반에 어떤 영향을 미칠지를 이해하는 것입니다.\u003c/p\u003e\n\u003cp\u003e이 AI는 윤리적인 목적을 위해 설계되었습니까? 그 사용이 누군가에게 직접적이거나간접적으로 피해를 주지는 않는지? 그리고 궁극적으로 이 모델이 장기적으로 순수한 선을 제공할 것인가?\u003c/p\u003e\n\u003cp\u003e쥬라기 공원의 첫 번째 장면에서 이안 말콤 박사가 말한 것처럼, 단지 무언가를 만들 수 있다고 해서 반드시 만들어야 한다는 뜻은 아닙니다.\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e데이터 책임성\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e마지막으로, 데이터 팀들에게 가장 중요한 고려 사항(또한 이 글에서 제가 대부분의 시간을 할애할 부분): 데이터 자체가 AI가 책임 있게 구축되고 활용되는 데 어떻게 영향을 미치는지에 대한 문제입니다.\u003c/p\u003e\n\u003cp\u003e이 고려 사항은 우리가 사용하는 데이터를 이해하고, 어떤 상황에서 안전하게 사용할 수 있는지, 그에 따른 위험 요소가 무엇인지를 다룹니다.\u003c/p\u003e\n\u003cp\u003e예를 들어, 우리는 데이터가 어디에서 왔는지, 어떻게 획득되었는지를 알고 있나요? 특정 모델에 공급되는 데이터에 개인정보 문제가 있는가요? 개인들을 피해로 치는 데 위험에 노출시키는 개인 데이터를 활용하고 있나요?\u003c/p\u003e\n\u003cp\u003e알 수 없는 데이터 위에 훈련된 LLM에서 안전하게 진행해도 괜찮을까요?\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eNew York Times가 OpenAI에 대한 소송에서 강조한 대로, 우리는 첫째로 이 데이터를 사용할 권리가 있는 걸까요?\u003c/p\u003e\n\u003cp\u003e데이터의 품질이 중요한 역할을 하는 곳이기도 합니다. 주어진 모델을 피드하는 데이터의 신뢰성을 신뢰할 수 있을까요? 퀄리티 문제가 AI 제품에 도달할 경우 잠재적인 결과는 무엇일까요?\u003c/p\u003e\n\u003cp\u003e따라서, 이러한 윤리적인 문제들을 전체적으로 쳐다보았으니, 이제 데이터 팀이 이에 대해 책임을 져야 하는 이유를 살펴봅시다.\u003c/p\u003e\n\u003ch1\u003e데이터 팀이 AI 윤리에 대해 책임져야 하는 이유\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e데이터 팀과 관련된 모든 윤리적 AI 고려사항 중에서 가장 중요한 것은 데이터 책임 문제입니다.\u003c/p\u003e\n\u003cp\u003e마찬가지로 GDPR이 비즈니스와 데이터 팀이 함께 데이터 수집 및 활용 방식을 재고하도록 강제했던 것처럼, GenAI는 기업이 어떤 워크플로우를 자동화할 수 있는지를 재고하도록 강제할 것입니다.\u003c/p\u003e\n\u003cp\u003e우리 데이터 팀으로서 어떤 AI 모델의 구축에 영향을 미치려는 책임이 절대적이지만, 그 설계 결과에 직접적으로 영향을 끼칠 수는 없습니다. 그러나 그 모델에서 잘못된 데이터를 거를 수 있다면, 그 설계 결함에 따른 위험을 완화하는 데 많은 도움이 될 수 있습니다.\u003c/p\u003e\n\u003cp\u003e모델 자체가 우리의 통제 영역을 벗어나 있다면, 할지 말지에 대한 본질적인 질문은 전혀 다른 달에 있는 것입니다. 우리는 그 문제점을 발견하면 지적할 책임이 있지만, 마무리로 말하자면, 우리가 탑승하든 말든, 로켓은 발사될 것입니다.\n가장 중요한 것은 로켓이 안전하게 발사되도록 하는 것입니다. (아니면 비행기 몸통을 훔치는 것도...)\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그래서 데이터 엔지니어들의 삶의 모든 영역과 마찬가지로, 우리가 시간과 노력을 투자하고 싶은 곳은 최대한 많은 사람들에게 가장 직접적인 영향을 미칠 수 있는 곳입니다. 그 기회는 데이터 자체에 있다는 것이 사실입니다.\u003c/p\u003e\n\u003ch1\u003e데이터 팀이 데이터 책임성에 신경 써야 하는 이유\u003c/h1\u003e\n\u003cp\u003e말해봐야할 것 같지만 너무 당연한 것 같지만, 그래도 말하겠습니다:\u003c/p\u003e\n\u003cp\u003e데이터 팀은 데이터가 AI 모델로 어떻게 활용되는지에 대한 책임을 져야 합니다. 왜냐하면 단순히 말하자면 그들만이 그것을 할 수 있는 유일한 팀이기 때문입니다. 물론, 준수 팀, 보안 팀, 심지어 법률 팀들이 윤리가 무시될 때 책임을 져야 할 수 있습니다. 그러나 얼마나 많은 책임을 공유할 수 있더라도, 그 팀들이 결국은 데이터 팀만큼 데이터를 동일한 수준에서 이해하지는 못할 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e소프트웨어 엔지니어링 팀이 OpenAI나 Anthropic과 같은 써드파티 LLM을 사용하여 앱을 만든다고 상상해보세요. 그러나 그들은 그들의 애플리케이션에 실제로 필요한 데이터 외에도 위치 데이터를 추적하고 저장하고 있다는 것을 깨닫지 못했습니다. 그들은 모델을 구동하기 위해 전체 데이터베이스를 활용합니다. 올바른 논리 결함이 있으면 나쁜 행위자가 그 데이터세트에 저장된 데이터를 사용하여 임의의 개인을 추적하는 프롬프트를 손쉽게 공학적으로 만들 수 있습니다. (이것은 오픈 소스 LLM과 폐쇄 소스 LLM 사이의 긴장 관계입니다.)\u003c/p\u003e\n\u003cp\u003e또는 소프트웨어 팀이 위치 데이터에 대해 알고 있지만 위치 데이터가 실제로 근삿값일 수 있다는 것을 깨닫지 못했습니다. 그들은 그 위치 데이터를 사용하여 16세 소년을 단순히 거리에 있는 피자 헛(Pizza Hut)이 아닌 어둡고 좁은 골목으로 안내하는 AI 매핑 기술을 만들 수 있습니다. 물론 이러한 유형의 오류는 의지적이 아니지만 데이터가 활용되는 방식에 내재된 의도하지 않은 위험을 강조합니다.\u003c/p\u003e\n\u003cp\u003e이러한 예시와 다른 사례들은 윤리적 AI에 관한 문제에서 데이터 팀의 역할을 강조합니다.\u003c/p\u003e\n\u003ch1\u003e그렇다면, 데이터 팀이 윤리적으로 유지하는 방법은 무엇일까요?\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e대부분의 경우, 데이터 팀은 모델이 작동하도록 대략적이고 대리 데이터를 다루는 데 익숙합니다. 그러나 AI 모델을 제공하는 데이터의 경우, 실제로 훨씬 더 높은 수준의 검증이 필요합니다.\u003c/p\u003e\n\u003cp\u003e소비자를 위해 틈을 메우기 위해 데이터 팀은 데이터 관행과 그 관행이 조직 전반과 어떻게 관련되는지 신중히 살펴봐야 합니다.\u003c/p\u003e\n\u003cp\u003eAI의 위험을 완화하는 방법을 고려할 때, 아래는 데이터 팀이 미래에 더 윤리적인 방향으로 AI를 움직이기 위해 취해야 할 3단계입니다.\u003c/p\u003e\n\u003ch1\u003e1. 회의 참석 자리를 얻으세요\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e데이터 팀은 타조가 아닙니다 - 머리를 모래속에 파묻고 문제가 사라지기를 기대할 수 없습니다. 데이터 팀이 리더십 테이블에 자리를 놓기 위해 싸워온 것처럼, 데이터 팀은 AI 테이블에 자리를 얻을 수 있도록 옹호해야 합니다.\u003c/p\u003e\n\u003cp\u003e어떤 데이터 품질 위기든, 지구가 이미 타버린 후에 뛰어든다면 충분하지 않습니다. GenAI에 고유하게 내재된 종말적 위험을 다룰 때, 우리 자신의 개인적 책임에 대해 선제적으로 대처하는 것이 이전보다 중요합니다.\u003c/p\u003e\n\u003cp\u003e그들이 당신이 테이블에 앉을 수 있도록 허락하지 않는다면, 바깥에서 교육하는 책임이 있습니다. 당신의 힘을 다하여 훌륭한 발견, 거버넌스 및 데이터 품질 솔루션을 제공하여 그 팀들에게 정보를 제공하면서 책임 있는 결정을 내릴 수 있도록 해야 합니다. 그들에게 사용할 것, 언제 사용할 것, 그리고 당신 팀의 내부 프로토콜로 유효성을 검증할 수 없는 써드파티 데이터 사용의 위험성을 가르쳐 주세요.\u003c/p\u003e\n\u003cp\u003e이것은 단지 비즈니스 문제가 아닙니다. 유나이티드 헬스케어와 브리티시 컬럼비아 주가 말할 것처럼, 많은 경우, 이들은 실제 사람들의 삶과 생계에 관한 것입니다. 그러니, 우리가 그 관점으로 운영하고 있는지 확인합시다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e2. RAG과 같은 방법론을 활용하여 더 책임감 있는 - 그리고 신뢰할 수 있는 - 데이터 조직화하기\u003c/h1\u003e\n\u003cp\u003e우리는 종종 RAG(검색 증강 생성) 같은 방법론을 언급하며 인공 지능으로부터 가치를 창출하는 자원으로 생각합니다. 그러나 그것은 그 AI를 구축하고 사용하는 방식을 보호하는 자원이기도 합니다.\u003c/p\u003e\n\u003cp\u003e예를 들어, 모델이 소비자를 대상으로 하는 채팅 앱에 공급하기 위해 개인 고객 데이터에 액세스하는 경우를 상상해보십시오. 올바른 사용자 프롬프트가 모델이 동작하여 심각한 PII가 폭로되어 나쁜 행위자가 그것을 취할 수도 있습니다. 그래서 데이터가 어디에서 왔는지 확인하고 제어하는 능력은 그 AI 제품의 무결성을 보호하는 데 중요합니다.\u003c/p\u003e\n\u003cp\u003e책임있는 데이터 팀은 RAG와 같은 방법론을 활용하여 준수되고 더 안전하며 모델에 적합한 데이터를 신중하게 조직화하여 많은 위험을 완화합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eAI 개발에 RAG(빨강, 주황, 초록) 접근 방식을 취하면 너무 많은 데이터를 처리할 때 발생할 수 있는 위험을 최소화하는 데 도움이 됩니다 — 우리의 위치 데이터 예시에서 언급된 대로.\u003c/p\u003e\n\u003cp\u003e그것이 실제로 어떻게 보이는지 궁금하시죠? 예를 들어 Netflix와 같은 미디어 회사라고 가정해봅시다. 이 회사는 고객 데이터의 일부를 활용하여 맞춤형 추천 모델을 만들어야 합니다. 그런 다음 해당 사용 사례에 대한 구체적이고 제한된 데이터 포인트를 정의하고, 그 데이터를 유지하고 유효성을 검사할 책임자가 누구인지, 어떤 상황에서 데이터를 안전하게 사용할 수 있는지, 그리고 시간이 지남에 따라 그 AI 제품을 개발하고 유지할 가장 적합한 사람이 누구인지를 보다 효과적으로 정의할 수 있게 됩니다.\u003c/p\u003e\n\u003cp\u003eData lineage과 같은 도구는 데이터의 출처를 신속하게 확인하여 팀이 어떤 시점에서 데이터가 어디에서 유래되고 사용되는지 또는 잘못 사용되는지를 확인할 수 있도록 도와줄 수도 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e3. 데이터 신뢰도를 우선시하세요\u003c/h1\u003e\n\u003cp\u003e데이터 제품에 대해 이야기할 때 자주 하는 말이 있습니다. \"쓰레기 데이터를 넣으면 쓰레기 데이터가 나온다,\" 하지만 GenAI의 경우에는 그 말이 조금 모자라다고 볼 수 있습니다. 실제로 쓰레기 데이터가 AI 모델로 들어가면 쓰레기만 나오는 것이 아니라 실제로 사람에게도 영향을 미치는 결과가 나올 수 있습니다.\u003c/p\u003e\n\u003cp\u003e그래서 모델로 공급되는 데이터를 제어하기 위해 RAG 아키텍처가 필요한만큼, Pinecone과 같은 벡터 데이터베이스에 연결되는 강력한 데이터 관찰 기능이 필요합니다. 데이터가 실제로 깨끗하고 안전하며 신뢰할 수 있는지 확인하기 위해 이 연결이 중요합니다.\u003c/p\u003e\n\u003cp\u003eAI를 시작하는 고객들로부터 가장 많이 들은 불만 중 하나는, 제품용 AI를 준비하는 것은, 벡터 데이터 파이프라인으로 인덱스를 적극적으로 모니터링하지 않는다면 데이터의 신뢰성을 검증하는 것이 거의 불가능하다는 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e대부분의 경우, 데이터 및 AI 엔지니어들이 데이터에 문제가 발생했음을 알게 되는 유일한 방법은 모델이 나쁜 프롬프트 응답을 내뱉을 때입니다. 그때에는 이미 너무 늦은 시점이죠.\u003c/p\u003e\n\u003ch1\u003e현재가 가장 좋은 때입니다\u003c/h1\u003e\n\u003cp\u003e2019년에 우리 팀이 데이터 관찰 카테고리를 만들었던 것은 데이터의 신뢰성과 신뢰의 필요성을 높인 동일한 과제입니다.\u003c/p\u003e\n\u003cp\u003e오늘날 AI가 일상적으로 의지하는 다양한 프로세스 및 시스템을 바꿔놓을 것이라고 약속하는 가운데, 데이터 품질의 도전과 더 중요한 것으로서의 윤리적 영향이 더욱 심각해지고 있습니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-23-BuildingEthicalAIStartswiththeDataTeamHeresWhy"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>