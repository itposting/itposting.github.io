<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>음성 쓰레기 분류 라즈베리 파이와 티처블 머신 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="음성 쓰레기 분류 라즈베리 파이와 티처블 머신 | itposting" data-gatsby-head="true"/><meta property="og:title" content="음성 쓰레기 분류 라즈베리 파이와 티처블 머신 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine" data-gatsby-head="true"/><meta name="twitter:title" content="음성 쓰레기 분류 라즈베리 파이와 티처블 머신 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 02:46" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-0fd008072af5a644.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_buildManifest.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">음성 쓰레기 분류 라즈베리 파이와 티처블 머신</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="음성 쓰레기 분류 라즈베리 파이와 티처블 머신" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">9<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_0.png" alt="이미지"></p>
<p>전 세계적으로 매년 20억 톤의 가정 폐기물이 생산됩니다. 이것은 우리 환경에 무거운 부담을 줍니다. 매립지와 환경에 끝내 찌르는 폐기물을 줄이기 위해 전 세계 주민들은 가정 폐기물을 분리 수거해야 합니다. 그리고 쓰레기 분리는 큰 비즈니스입니다. Fortunebusinessinsights.com에 따르면, 2019년 글로벌 폐기물 분류 장비 시장은 약 7억 달러였습니다. 그리고 2027년에는 18억 달러에 이를 것으로 예상됩니다.</p>
<p><img src="/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_1.png" alt="이미지"></p>
<p>하지만 유감스럽게도 중국에서는 쓰레기 분리 시스템이 효과적이지 않습니다. 정책이 약하게 시행되며, 위반에 대한 처벌이 거의 없습니다. 더 나쁜 것은 쓰레기 분류가 혼란스럽습니다. 제 건물에는 섬유, 재활용, 음식물, 잔여 폐기물을 각각 수집하는 다섯 개의 용기가 있습니다. 각 라벨 아래에는 예시의 짧은 목록이 있습니다. 예를 들어, 와인과 플라스틱 병은 "재활용" 용기로 가야 합니다. 그리고 옷과 가방은 "섬유"에 속합니다.</p>
<p>하지만 목록은 모든 종류의 쓰레기를 다 다루기에는 너무 짧습니다. 나는 종종 우드 또는 금속판이 어느 컨테이너에 들어가야 하는지 모르기 때문에 컨테이너 앞에서 어리둥절해졌어요. 대부분의 경우, 결국 쓰레기는 "잔여 폐기물" 농푸에 들어갑니다. 이런 경우에는 누군가가 올바른 컨테이너를 알려주면 좋겠죠? 그리고 나는 Freethink의 이 YouTube 비디오를 보았습니다.</p>
<p>이 비디오는 인공지능이 도와주는 로봇 쓰레기 분류 시스템을 보여줍니다. 이 시스템은 컴퓨터 비전을 사용하여 다양한 종류의 쓰레기를 인식한 다음 로봇 팔을 사용하여 분리합니다. 요즘에는 인공지능 컴퓨터 비전이 매우 정확합니다. 우리는 쓰레기 분류 문제에 대처하기 위해 완전히 활용해야 합니다.</p>
<p>이 비디오에 영감을 받아 나는 집에 내 쓰레기 분류 라즈베리 파이를 만들었어요 (그림 1 및 비디오 2).</p>
<p>시스템은 라즈베리 파이, 비디오 카메라 및 스피커로 구성되어 있습니다. 카메라가 쓰레기 물체를 "보게 되면", 라즈베리 파이는 스피커를 통해 쓰레기 카테고리를 말합니다. 컴퓨터 비전 모델은 Google의 Teachable Machine에서 훈련되었습니다. 이 기사에서 시스템을 설명합니다. 이 프로젝트의 코드는 여기 내 GitHub 저장소에 호스팅되어 있습니다.</p>
<p>그리고 Teachable Machine 모델 파일은 제 Google 드라이브에 호스팅되어 있습니다.</p>
<h1>1. 구성 요소</h1>
<p>이 프로젝트에서는 Raspberry Pi 4를 사용했습니다. RAM이 4GB이고 저장 용량이 64GB입니다. 또한 Pi에 USB 비디오 카메라와 USB 스피커를 연결했습니다 (그림 1 및 3).</p>
<p><img src="/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_2.png" alt="이미지"></p>
<p>나는 Google의 Teachable Machine 프로젝트를 데스크톱 컴퓨터에서 작업했어. 그리고 나는 모델과 다른 파일을 내 집 네트워크를 통해 Raspberry Pi로 전송했어.</p>
<h1>2. Teachable Machine에서 쓰레기 분류 모델 훈련하기</h1>
<p>이 프로젝트에서, 나는 재활용 가능한(recyclable), 직물(textiles), 그리고 비어 있는(empty) 세 가지 클래스로 모델을 훈련했어. 나는 kaggle.com에서 보틀과 컵 데이터세트 그리고 의류 데이터세트(이 문서에서 설명함)를 발견했어. 나는 또한 처음 두 카테고리에 내 사진 몇 장을 추가했어. 비어 있는 카테고리에는 다양한 종류의 벽 사진을 포함했어. 이 마지막 클래스는 물체가 없을 때 시스템을 대기 상태로 만들고 싶기 때문에 필수적이야.</p>
<p>Teachable Machine 웹사이트로 이동해서 이미지 프로젝트를 시작해봐. 내가 언급한 세 가지 클래스를 생성해. 각 클래스에 사진을 업로드해. 또는 위에서 제공한 내 공유 모델 파일을 열어볼 수도 있어.</p>
<p><img src="/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_3.png" alt="이미지"></p>
<p>이제 Train Model 버튼을 클릭하여 훈련 프로세스를 시작할 수 있습니다. 훈련 이후 모델을 몇 장의 사진 또는 카메라를 사용하여 테스트할 수 있습니다.</p>
<p><img src="/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_4.png" alt="이미지"></p>
<p>결과에 만족하셨다면 Export Model 버튼을 클릭하세요. Tensorflow 탭을 선택하고 Savedmodel 옵션을 선택한 후 Download my model 버튼을 클릭하세요.</p>
<p><img src="/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_5.png" alt="Image"></p>
<p>당신의 브라우저가 모델이 준비되면 다운로드가 시작됩니다.</p>
<h1>3. 앱 작성</h1>
<p>호스트 컴퓨터에서 pi_garbage_classifier라는 폴더를 만듭니다. 다운로드한 파일을 프로젝트 폴더에 압축해제합니다. 이제 app.py라는 파일을 만들어 각 부분을 조합할 것입니다. 이것은 Teachable Machine의 코드 스니펫에 기반합니다 (Figure 6). 그러나 놀랍게도 샘플 코드는 잘못되었고 오도독도 못하다. keras_model.h5를 열려고 시도했지만 다운로드한 모델은 model.savedmodel이라는 이름입니다.</p>
<p>여기는 app.py의 코드입니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> re
<span class="hljs-keyword">import</span> sys

pathname = os.path.dirname(sys.argv[<span class="hljs-number">0</span>])

<span class="hljs-built_in">print</span>(<span class="hljs-string">'sys.argv[0] ='</span>, sys.argv[<span class="hljs-number">0</span>], <span class="hljs-string">"pathname"</span>, pathname)    

<span class="hljs-comment"># 모델 불러오기</span>
model = load_model(<span class="hljs-string">f'<span class="hljs-subst">{pathname}</span>/model.savedmodel'</span>)

<span class="hljs-comment"># 레이블을 labels.txt 파일에서 가져옵니다. 이것은 나중에 사용됩니다.</span>
labels = <span class="hljs-built_in">open</span>(<span class="hljs-string">f'<span class="hljs-subst">{pathname}</span>/labels.txt'</span>, <span class="hljs-string">'r'</span>).readlines()

<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
    time.sleep(<span class="hljs-number">5</span>)

    <span class="hljs-comment"># CAMERA는 컴퓨터의 기본 카메라에 따라 0 또는 1이 될 수 있습니다.</span>
    camera = cv2.VideoCapture(<span class="hljs-number">0</span>)
    <span class="hljs-comment"># 웹캠 이미지 불러오기</span>
    ret, image = camera.read()
    camera.release()
    
    <span class="hljs-comment"># 이미지를 (224-높이,224-너비) 픽셀로 크기 조정합니다.</span>
    image = cv2.resize(image, (<span class="hljs-number">224</span>, <span class="hljs-number">224</span>), interpolation=cv2.INTER_AREA)
    
    <span class="hljs-comment"># 이미지를 윈도우에 표시합니다.</span>
    
    image = np.asarray(image, dtype=np.float32).reshape(<span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>, <span class="hljs-number">3</span>)
    
    image = (image / <span class="hljs-number">127.5</span>) - <span class="hljs-number">1</span>
    
    probabilities = model.predict(image)
    
    max_prob = np.<span class="hljs-built_in">max</span>(probabilities)
    label = re.sub(<span class="hljs-string">r'\d+\s+'</span>, <span class="hljs-string">''</span>, labels[np.argmax(probabilities)]).strip()

    <span class="hljs-built_in">print</span> (label, <span class="hljs-string">f"proba: <span class="hljs-subst">{max_prob}</span>"</span>)
    
    <span class="hljs-keyword">if</span> label != <span class="hljs-string">"empty"</span> <span class="hljs-keyword">and</span> max_prob > <span class="hljs-number">0.9</span>:
        os.system(<span class="hljs-string">f"festival --tts <span class="hljs-subst">{pathname}</span>/voice/<span class="hljs-subst">{label}</span>.txt"</span>)

    keyboard_input = cv2.waitKey(<span class="hljs-number">1</span>)
    
    <span class="hljs-keyword">if</span> keyboard_input == <span class="hljs-number">27</span>:
        <span class="hljs-keyword">break</span>
</code></pre>
<p>내 코드에 메인 루프에 sleep 함수를 추가했습니다. 이를 통해 시스템은 5초마다 이미지를 캡처합니다. OpenCV는 캡처된 프레임을 버퍼링하기 때문에 각 반복에서 카메라를 초기화해야 합니다. 코드는 또한 예측 확률이 0.9보다 높으면 쓰레기 카테고리를 발표하기 위해 페스티벌 유틸리티를 사용합니다. 페스티벌은 텍스트 음성 변환 유틸리티입니다. 텍스트 파일을 읽어 내용을 읽어 줄 수 있습니다. 이 프로젝트에서는 voice라는 폴더를 만들고 발표 파일을 넣었습니다. 각 발표 파일에는 카테고리 이름만 작성했습니다. 읽기 편의를 위해 파일 내용을 수정하여 발표를 사용자 정의할 수 있습니다.</p>
<p>그래서 최종적으로 폴더 구조는 다음과 같아야 합니다.</p>
<h1>4. Raspberry Pi 설정하기</h1>
<p>저는 learn.adafruit.com에 있는 훌륭한 설정 안내서를 따랐어요. 먼저, Raspberry Pi Imager를 사용하여 마이크로 SD 카드에 Raspberry Pi OS (64비트)를 기록하세요. 고급 옵션에서 SSH를 활성화하고 사용자 이름과 암호를 설정하세요.</p>
<p>카드를 Pi에 넣고 전원을 켜세요. 네트워크에 연결하고 다음 SSH 명령어로 로그인하세요.</p>
<pre><code class="hljs language-js"># 만약 콘솔에서 <span class="hljs-string">"REMOTE HOST IDENTIFICATION HAS CHANGED!"</span> 라고 나오면
# 다음 명령어를 실행하여 리셋하세요
# ssh-keygen -R raspberrypi.<span class="hljs-property">local</span>

ssh pi@raspberrypi.<span class="hljs-property">local</span>
</code></pre>
<p>라즈베리 파이 쉘이 준비되면, 아래 명령어를 하나씩 사용하여 필요한 소프트웨어를 설치하세요.</p>
<pre><code class="hljs language-js"># 라즈베리 파이에서 실행하세요

sudo apt update
sudo apt upgrade -y

# 선택 사항
# sudo apt install -y python3-pip

pip3 install --upgrade setuptools

pip3 install <span class="hljs-title class_">Pillow</span>==<span class="hljs-number">9.2</span><span class="hljs-number">.0</span>

pip install opencv-python

<span class="hljs-variable constant_">RELEASE</span>=<span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/PINTO0309/Tensorflow-bin/releases/download/v2.10.0/tensorflow-2.10.0-cp310-none-linux_aarch64.whl</span>

<span class="hljs-variable constant_">CPVER</span>=$(python --version | grep -<span class="hljs-title class_">Eo</span> <span class="hljs-string">'3\.[0-9]{1,2}'</span> | tr -d <span class="hljs-string">'.'</span>)

pip install $(echo <span class="hljs-string">"$RELEASE"</span> | sed -e <span class="hljs-string">"s/cp[0-9]\{3\}/CP$CPVER/g"</span>)

sudo apt install -y festival

# 선택 사항
# sudo reboot

mkdir ~/pi_garbage_classifier
</code></pre>
<p>위의 마지막 명령어는 라즈베리 파이의 홈 디렉토리에 pi_garbage_classifier 라는 폴더를 만듭니다. 프로젝트 파일을 데스크톱 컴퓨터에서 이 폴더로 전송하려면 다음 명령어를 사용하세요.</p>
<h1>호스트 데스크톱에서</h1>
<p>cd [your_desktop_pi_garbage_classifier_path]
scp -r ./* <a href="mailto:pi@raspberrypi.local">pi@raspberrypi.local</a>:~/pi_garbage_classifier/</p>
<p>파일을 복사한 후에는 다음 명령어로 앱을 테스트하세요:</p>
<h1>라즈베리 파이에서</h1>
<p>python /home/pi/pi_garbage_classifier/app.py</p>
<p>앱이 시작하는 데 몇 초 정도 걸립니다. 카메라 램프가 깜박일 때, 카메라 앞에 다양한 물체를 두어 시스템을 테스트할 수 있습니다. 물체의 클래스를 알리고 콘솔에 다음 출력이 나타날 것입니다 (그림 8).</p>
<p><img src="/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_7.png" alt="이미지"></p>
<p>앱은 종료할 때까지 계속 실행될 거에요.</p>
<h1>5. 라즈베리 파이(Pi) 배포</h1>
<p>이제 시스템을 독립적으로 만들어 배포할 시간입니다. 앱을 Pi의 자동 시작 파일에 추가해야 합니다. rc.local과 cron 두 가지 방법을 시도해봤는데, 이 방법들은 데스크톱을 시작하기 전에 앱을 시작하기 때문에 USB 스피커를 활용할 수 없었어요. 그래서 이 포스트에서 해결책을 찾았어요. 시스템 자동 시작 파일에 관련된 것이죠. 따라서 이 명령어를 실행하여 파일을 편집해보세요.</p>
<pre><code class="hljs language-js">sudo nano /etc/xdg/lxsession/<span class="hljs-variable constant_">LXDE</span>-pi/autostart
</code></pre>
<p>다음 줄을 파일에 추가하세요.</p>
<pre><code class="hljs language-js">@python /home/pi/pi_garbage_classifier/app.<span class="hljs-property">py</span>
</code></pre>
<p>이렇게 autostart 파일이 보이게 됩니다.</p>
<p>프로그램을 종료하려면 Ctrl+X를 누르고 동일한 파일에 변경 사항을 저장하려면 Y를 누르세요. 다음 명령어를 사용하여 장치를 다시 부팅하세요.</p>
<pre><code class="hljs language-js">sudo reboot
</code></pre>
<p>부팅 후 시스템은 자동으로 작업을 수행해야 합니다. 이제 네트워크 케이블을 제거하고 Pi를 원하는 곳에 배치할 수 있습니다 (비디오 2). 이제 쓰레기의 목적지에 대해 확실하지 않을 때에는 항상 상담할 수 있습니다.</p>
<h1>결론</h1>
<p>이 기사에서는 시범용 쓰레기 분류 시스템을 구축하는 방법을 보여드렸어요. 설정하는 데 약 30분 가량 밖에 걸리지 않았죠. 아직까지는 두 가지 종류의 쓰레기와 전체 가정 쓰레기의 일부만 인식할 수 있지만 필수 구성 요소들은 모두 갖추고 있어요. 모델에 더 많은 객체와 클래스를 추가할 수 있습니다. 또한 이 프로젝트는 일종의 프레임워크입니다. 다시 말해, 새로운 모델을 훈련하고 Pi에 모델 파일을 교체하여 식물 분류 시스템이나 물체 인식기와 같이 완전히 다른 것으로 변환할 수 있어요.</p>
<p>Google의 티처블 머신은 정말 우리에게 빛이 되어줬어요. 이전에는 서로 다른 화가들의 그림을 분석하는 데 사용했었죠. 물론 PyTorch나 Keras로 직접 컴퓨터 비전 모델을 사용자화할 수도 있어요. 하지만 그런 작업은 상당한 시간과 노하우를 요구할 수 있어요. 반면, 티처블 머신은 이 모든 것을 아주 간단하게 만들어줘요. 몇 번의 클릭만으로 과연성 있는 모델을 쉽게 얻을 수 있어요. 더불어 모델을 확장하고 수정하는 것도 쉬워요.</p>
<p>하지만 생각해 볼 수 있는 작은 단점도 있어요. 티처블 머신은 인터페이스에서 모델 성능을 보여주지 않아요. 따라서 우리는 모델이 얼마나 잘 작동할지를 알 방법이 없어요. 더 많은 클래스를 추가할수록 모델 성능이 떨어지기 때문에 우리는 모델을 충분히 신뢰하고 배포하기 전에 모델의 정확도를 측정해야 해요.</p>
<p>이 프로젝트를 시도해 보라고 권장합니다. 그리고 피드백을 주시면 좋을 것 같아요!</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"음성 쓰레기 분류 라즈베리 파이와 티처블 머신","description":"","date":"2024-06-19 02:46","slug":"2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine","content":"\n\n\n![이미지](/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_0.png)\n\n전 세계적으로 매년 20억 톤의 가정 폐기물이 생산됩니다. 이것은 우리 환경에 무거운 부담을 줍니다. 매립지와 환경에 끝내 찌르는 폐기물을 줄이기 위해 전 세계 주민들은 가정 폐기물을 분리 수거해야 합니다. 그리고 쓰레기 분리는 큰 비즈니스입니다. Fortunebusinessinsights.com에 따르면, 2019년 글로벌 폐기물 분류 장비 시장은 약 7억 달러였습니다. 그리고 2027년에는 18억 달러에 이를 것으로 예상됩니다.\n\n![이미지](/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_1.png)\n\n하지만 유감스럽게도 중국에서는 쓰레기 분리 시스템이 효과적이지 않습니다. 정책이 약하게 시행되며, 위반에 대한 처벌이 거의 없습니다. 더 나쁜 것은 쓰레기 분류가 혼란스럽습니다. 제 건물에는 섬유, 재활용, 음식물, 잔여 폐기물을 각각 수집하는 다섯 개의 용기가 있습니다. 각 라벨 아래에는 예시의 짧은 목록이 있습니다. 예를 들어, 와인과 플라스틱 병은 \"재활용\" 용기로 가야 합니다. 그리고 옷과 가방은 \"섬유\"에 속합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 목록은 모든 종류의 쓰레기를 다 다루기에는 너무 짧습니다. 나는 종종 우드 또는 금속판이 어느 컨테이너에 들어가야 하는지 모르기 때문에 컨테이너 앞에서 어리둥절해졌어요. 대부분의 경우, 결국 쓰레기는 \"잔여 폐기물\" 농푸에 들어갑니다. 이런 경우에는 누군가가 올바른 컨테이너를 알려주면 좋겠죠? 그리고 나는 Freethink의 이 YouTube 비디오를 보았습니다.\n\n이 비디오는 인공지능이 도와주는 로봇 쓰레기 분류 시스템을 보여줍니다. 이 시스템은 컴퓨터 비전을 사용하여 다양한 종류의 쓰레기를 인식한 다음 로봇 팔을 사용하여 분리합니다. 요즘에는 인공지능 컴퓨터 비전이 매우 정확합니다. 우리는 쓰레기 분류 문제에 대처하기 위해 완전히 활용해야 합니다.\n\n이 비디오에 영감을 받아 나는 집에 내 쓰레기 분류 라즈베리 파이를 만들었어요 (그림 1 및 비디오 2).\n\n시스템은 라즈베리 파이, 비디오 카메라 및 스피커로 구성되어 있습니다. 카메라가 쓰레기 물체를 \"보게 되면\", 라즈베리 파이는 스피커를 통해 쓰레기 카테고리를 말합니다. 컴퓨터 비전 모델은 Google의 Teachable Machine에서 훈련되었습니다. 이 기사에서 시스템을 설명합니다. 이 프로젝트의 코드는 여기 내 GitHub 저장소에 호스팅되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 Teachable Machine 모델 파일은 제 Google 드라이브에 호스팅되어 있습니다.\n\n# 1. 구성 요소\n\n이 프로젝트에서는 Raspberry Pi 4를 사용했습니다. RAM이 4GB이고 저장 용량이 64GB입니다. 또한 Pi에 USB 비디오 카메라와 USB 스피커를 연결했습니다 (그림 1 및 3). \n\n![이미지](/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n나는 Google의 Teachable Machine 프로젝트를 데스크톱 컴퓨터에서 작업했어. 그리고 나는 모델과 다른 파일을 내 집 네트워크를 통해 Raspberry Pi로 전송했어.\n\n# 2. Teachable Machine에서 쓰레기 분류 모델 훈련하기\n\n이 프로젝트에서, 나는 재활용 가능한(recyclable), 직물(textiles), 그리고 비어 있는(empty) 세 가지 클래스로 모델을 훈련했어. 나는 kaggle.com에서 보틀과 컵 데이터세트 그리고 의류 데이터세트(이 문서에서 설명함)를 발견했어. 나는 또한 처음 두 카테고리에 내 사진 몇 장을 추가했어. 비어 있는 카테고리에는 다양한 종류의 벽 사진을 포함했어. 이 마지막 클래스는 물체가 없을 때 시스템을 대기 상태로 만들고 싶기 때문에 필수적이야.\n\nTeachable Machine 웹사이트로 이동해서 이미지 프로젝트를 시작해봐. 내가 언급한 세 가지 클래스를 생성해. 각 클래스에 사진을 업로드해. 또는 위에서 제공한 내 공유 모델 파일을 열어볼 수도 있어.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_3.png)\n\n이제 Train Model 버튼을 클릭하여 훈련 프로세스를 시작할 수 있습니다. 훈련 이후 모델을 몇 장의 사진 또는 카메라를 사용하여 테스트할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_4.png)\n\n결과에 만족하셨다면 Export Model 버튼을 클릭하세요. Tensorflow 탭을 선택하고 Savedmodel 옵션을 선택한 후 Download my model 버튼을 클릭하세요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_5.png)\n\n당신의 브라우저가 모델이 준비되면 다운로드가 시작됩니다.\n\n# 3. 앱 작성\n\n호스트 컴퓨터에서 pi_garbage_classifier라는 폴더를 만듭니다. 다운로드한 파일을 프로젝트 폴더에 압축해제합니다. 이제 app.py라는 파일을 만들어 각 부분을 조합할 것입니다. 이것은 Teachable Machine의 코드 스니펫에 기반합니다 (Figure 6). 그러나 놀랍게도 샘플 코드는 잘못되었고 오도독도 못하다. keras_model.h5를 열려고 시도했지만 다운로드한 모델은 model.savedmodel이라는 이름입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기는 app.py의 코드입니다.\n\n```python\nimport cv2\nimport numpy as np\nfrom keras.models import load_model\nimport os\nimport time\nimport re\nimport sys\n\npathname = os.path.dirname(sys.argv[0])\n\nprint('sys.argv[0] =', sys.argv[0], \"pathname\", pathname)    \n\n# 모델 불러오기\nmodel = load_model(f'{pathname}/model.savedmodel')\n\n# 레이블을 labels.txt 파일에서 가져옵니다. 이것은 나중에 사용됩니다.\nlabels = open(f'{pathname}/labels.txt', 'r').readlines()\n\nwhile True:\n    time.sleep(5)\n\n    # CAMERA는 컴퓨터의 기본 카메라에 따라 0 또는 1이 될 수 있습니다.\n    camera = cv2.VideoCapture(0)\n    # 웹캠 이미지 불러오기\n    ret, image = camera.read()\n    camera.release()\n    \n    # 이미지를 (224-높이,224-너비) 픽셀로 크기 조정합니다.\n    image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_AREA)\n    \n    # 이미지를 윈도우에 표시합니다.\n    \n    image = np.asarray(image, dtype=np.float32).reshape(1, 224, 224, 3)\n    \n    image = (image / 127.5) - 1\n    \n    probabilities = model.predict(image)\n    \n    max_prob = np.max(probabilities)\n    label = re.sub(r'\\d+\\s+', '', labels[np.argmax(probabilities)]).strip()\n\n    print (label, f\"proba: {max_prob}\")\n    \n    if label != \"empty\" and max_prob \u003e 0.9:\n        os.system(f\"festival --tts {pathname}/voice/{label}.txt\")\n\n    keyboard_input = cv2.waitKey(1)\n    \n    if keyboard_input == 27:\n        break\n```\n\n내 코드에 메인 루프에 sleep 함수를 추가했습니다. 이를 통해 시스템은 5초마다 이미지를 캡처합니다. OpenCV는 캡처된 프레임을 버퍼링하기 때문에 각 반복에서 카메라를 초기화해야 합니다. 코드는 또한 예측 확률이 0.9보다 높으면 쓰레기 카테고리를 발표하기 위해 페스티벌 유틸리티를 사용합니다. 페스티벌은 텍스트 음성 변환 유틸리티입니다. 텍스트 파일을 읽어 내용을 읽어 줄 수 있습니다. 이 프로젝트에서는 voice라는 폴더를 만들고 발표 파일을 넣었습니다. 각 발표 파일에는 카테고리 이름만 작성했습니다. 읽기 편의를 위해 파일 내용을 수정하여 발표를 사용자 정의할 수 있습니다. \n\n그래서 최종적으로 폴더 구조는 다음과 같아야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_6.png\" /\u003e\n\n# 4. Raspberry Pi 설정하기\n\n저는 learn.adafruit.com에 있는 훌륭한 설정 안내서를 따랐어요. 먼저, Raspberry Pi Imager를 사용하여 마이크로 SD 카드에 Raspberry Pi OS (64비트)를 기록하세요. 고급 옵션에서 SSH를 활성화하고 사용자 이름과 암호를 설정하세요.\n\n카드를 Pi에 넣고 전원을 켜세요. 네트워크에 연결하고 다음 SSH 명령어로 로그인하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 만약 콘솔에서 \"REMOTE HOST IDENTIFICATION HAS CHANGED!\" 라고 나오면\n# 다음 명령어를 실행하여 리셋하세요\n# ssh-keygen -R raspberrypi.local\n\nssh pi@raspberrypi.local\n```\n\n라즈베리 파이 쉘이 준비되면, 아래 명령어를 하나씩 사용하여 필요한 소프트웨어를 설치하세요.\n\n```js\n# 라즈베리 파이에서 실행하세요\n\nsudo apt update\nsudo apt upgrade -y\n\n# 선택 사항\n# sudo apt install -y python3-pip\n\npip3 install --upgrade setuptools\n\npip3 install Pillow==9.2.0\n\npip install opencv-python\n\nRELEASE=https://github.com/PINTO0309/Tensorflow-bin/releases/download/v2.10.0/tensorflow-2.10.0-cp310-none-linux_aarch64.whl\n\nCPVER=$(python --version | grep -Eo '3\\.[0-9]{1,2}' | tr -d '.')\n\npip install $(echo \"$RELEASE\" | sed -e \"s/cp[0-9]\\{3\\}/CP$CPVER/g\")\n\nsudo apt install -y festival\n\n# 선택 사항\n# sudo reboot\n\nmkdir ~/pi_garbage_classifier\n```\n\n위의 마지막 명령어는 라즈베리 파이의 홈 디렉토리에 pi_garbage_classifier 라는 폴더를 만듭니다. 프로젝트 파일을 데스크톱 컴퓨터에서 이 폴더로 전송하려면 다음 명령어를 사용하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 호스트 데스크톱에서\n\ncd [your_desktop_pi_garbage_classifier_path]\nscp -r ./* pi@raspberrypi.local:~/pi_garbage_classifier/\n\n\n파일을 복사한 후에는 다음 명령어로 앱을 테스트하세요:\n\n\n# 라즈베리 파이에서\n\npython /home/pi/pi_garbage_classifier/app.py\n\n\n앱이 시작하는 데 몇 초 정도 걸립니다. 카메라 램프가 깜박일 때, 카메라 앞에 다양한 물체를 두어 시스템을 테스트할 수 있습니다. 물체의 클래스를 알리고 콘솔에 다음 출력이 나타날 것입니다 (그림 8).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_7.png)\n\n앱은 종료할 때까지 계속 실행될 거에요.\n\n# 5. 라즈베리 파이(Pi) 배포\n\n이제 시스템을 독립적으로 만들어 배포할 시간입니다. 앱을 Pi의 자동 시작 파일에 추가해야 합니다. rc.local과 cron 두 가지 방법을 시도해봤는데, 이 방법들은 데스크톱을 시작하기 전에 앱을 시작하기 때문에 USB 스피커를 활용할 수 없었어요. 그래서 이 포스트에서 해결책을 찾았어요. 시스템 자동 시작 파일에 관련된 것이죠. 따라서 이 명령어를 실행하여 파일을 편집해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nsudo nano /etc/xdg/lxsession/LXDE-pi/autostart\n```\n\n다음 줄을 파일에 추가하세요.\n\n```js\n@python /home/pi/pi_garbage_classifier/app.py\n```\n\n이렇게 autostart 파일이 보이게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_8.png\" /\u003e\n\n프로그램을 종료하려면 Ctrl+X를 누르고 동일한 파일에 변경 사항을 저장하려면 Y를 누르세요. 다음 명령어를 사용하여 장치를 다시 부팅하세요.\n\n```js\nsudo reboot\n```\n\n부팅 후 시스템은 자동으로 작업을 수행해야 합니다. 이제 네트워크 케이블을 제거하고 Pi를 원하는 곳에 배치할 수 있습니다 (비디오 2). 이제 쓰레기의 목적지에 대해 확실하지 않을 때에는 항상 상담할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n이 기사에서는 시범용 쓰레기 분류 시스템을 구축하는 방법을 보여드렸어요. 설정하는 데 약 30분 가량 밖에 걸리지 않았죠. 아직까지는 두 가지 종류의 쓰레기와 전체 가정 쓰레기의 일부만 인식할 수 있지만 필수 구성 요소들은 모두 갖추고 있어요. 모델에 더 많은 객체와 클래스를 추가할 수 있습니다. 또한 이 프로젝트는 일종의 프레임워크입니다. 다시 말해, 새로운 모델을 훈련하고 Pi에 모델 파일을 교체하여 식물 분류 시스템이나 물체 인식기와 같이 완전히 다른 것으로 변환할 수 있어요.\n\nGoogle의 티처블 머신은 정말 우리에게 빛이 되어줬어요. 이전에는 서로 다른 화가들의 그림을 분석하는 데 사용했었죠. 물론 PyTorch나 Keras로 직접 컴퓨터 비전 모델을 사용자화할 수도 있어요. 하지만 그런 작업은 상당한 시간과 노하우를 요구할 수 있어요. 반면, 티처블 머신은 이 모든 것을 아주 간단하게 만들어줘요. 몇 번의 클릭만으로 과연성 있는 모델을 쉽게 얻을 수 있어요. 더불어 모델을 확장하고 수정하는 것도 쉬워요.\n\n하지만 생각해 볼 수 있는 작은 단점도 있어요. 티처블 머신은 인터페이스에서 모델 성능을 보여주지 않아요. 따라서 우리는 모델이 얼마나 잘 작동할지를 알 방법이 없어요. 더 많은 클래스를 추가할수록 모델 성능이 떨어지기 때문에 우리는 모델을 충분히 신뢰하고 배포하기 전에 모델의 정확도를 측정해야 해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 프로젝트를 시도해 보라고 권장합니다. 그리고 피드백을 주시면 좋을 것 같아요!","ogImage":{"url":"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_0.png"},"coverImage":"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_0.png","tag":["Tech"],"readingTime":9},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e전 세계적으로 매년 20억 톤의 가정 폐기물이 생산됩니다. 이것은 우리 환경에 무거운 부담을 줍니다. 매립지와 환경에 끝내 찌르는 폐기물을 줄이기 위해 전 세계 주민들은 가정 폐기물을 분리 수거해야 합니다. 그리고 쓰레기 분리는 큰 비즈니스입니다. Fortunebusinessinsights.com에 따르면, 2019년 글로벌 폐기물 분류 장비 시장은 약 7억 달러였습니다. 그리고 2027년에는 18억 달러에 이를 것으로 예상됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e하지만 유감스럽게도 중국에서는 쓰레기 분리 시스템이 효과적이지 않습니다. 정책이 약하게 시행되며, 위반에 대한 처벌이 거의 없습니다. 더 나쁜 것은 쓰레기 분류가 혼란스럽습니다. 제 건물에는 섬유, 재활용, 음식물, 잔여 폐기물을 각각 수집하는 다섯 개의 용기가 있습니다. 각 라벨 아래에는 예시의 짧은 목록이 있습니다. 예를 들어, 와인과 플라스틱 병은 \"재활용\" 용기로 가야 합니다. 그리고 옷과 가방은 \"섬유\"에 속합니다.\u003c/p\u003e\n\u003cp\u003e하지만 목록은 모든 종류의 쓰레기를 다 다루기에는 너무 짧습니다. 나는 종종 우드 또는 금속판이 어느 컨테이너에 들어가야 하는지 모르기 때문에 컨테이너 앞에서 어리둥절해졌어요. 대부분의 경우, 결국 쓰레기는 \"잔여 폐기물\" 농푸에 들어갑니다. 이런 경우에는 누군가가 올바른 컨테이너를 알려주면 좋겠죠? 그리고 나는 Freethink의 이 YouTube 비디오를 보았습니다.\u003c/p\u003e\n\u003cp\u003e이 비디오는 인공지능이 도와주는 로봇 쓰레기 분류 시스템을 보여줍니다. 이 시스템은 컴퓨터 비전을 사용하여 다양한 종류의 쓰레기를 인식한 다음 로봇 팔을 사용하여 분리합니다. 요즘에는 인공지능 컴퓨터 비전이 매우 정확합니다. 우리는 쓰레기 분류 문제에 대처하기 위해 완전히 활용해야 합니다.\u003c/p\u003e\n\u003cp\u003e이 비디오에 영감을 받아 나는 집에 내 쓰레기 분류 라즈베리 파이를 만들었어요 (그림 1 및 비디오 2).\u003c/p\u003e\n\u003cp\u003e시스템은 라즈베리 파이, 비디오 카메라 및 스피커로 구성되어 있습니다. 카메라가 쓰레기 물체를 \"보게 되면\", 라즈베리 파이는 스피커를 통해 쓰레기 카테고리를 말합니다. 컴퓨터 비전 모델은 Google의 Teachable Machine에서 훈련되었습니다. 이 기사에서 시스템을 설명합니다. 이 프로젝트의 코드는 여기 내 GitHub 저장소에 호스팅되어 있습니다.\u003c/p\u003e\n\u003cp\u003e그리고 Teachable Machine 모델 파일은 제 Google 드라이브에 호스팅되어 있습니다.\u003c/p\u003e\n\u003ch1\u003e1. 구성 요소\u003c/h1\u003e\n\u003cp\u003e이 프로젝트에서는 Raspberry Pi 4를 사용했습니다. RAM이 4GB이고 저장 용량이 64GB입니다. 또한 Pi에 USB 비디오 카메라와 USB 스피커를 연결했습니다 (그림 1 및 3).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e나는 Google의 Teachable Machine 프로젝트를 데스크톱 컴퓨터에서 작업했어. 그리고 나는 모델과 다른 파일을 내 집 네트워크를 통해 Raspberry Pi로 전송했어.\u003c/p\u003e\n\u003ch1\u003e2. Teachable Machine에서 쓰레기 분류 모델 훈련하기\u003c/h1\u003e\n\u003cp\u003e이 프로젝트에서, 나는 재활용 가능한(recyclable), 직물(textiles), 그리고 비어 있는(empty) 세 가지 클래스로 모델을 훈련했어. 나는 kaggle.com에서 보틀과 컵 데이터세트 그리고 의류 데이터세트(이 문서에서 설명함)를 발견했어. 나는 또한 처음 두 카테고리에 내 사진 몇 장을 추가했어. 비어 있는 카테고리에는 다양한 종류의 벽 사진을 포함했어. 이 마지막 클래스는 물체가 없을 때 시스템을 대기 상태로 만들고 싶기 때문에 필수적이야.\u003c/p\u003e\n\u003cp\u003eTeachable Machine 웹사이트로 이동해서 이미지 프로젝트를 시작해봐. 내가 언급한 세 가지 클래스를 생성해. 각 클래스에 사진을 업로드해. 또는 위에서 제공한 내 공유 모델 파일을 열어볼 수도 있어.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_3.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e이제 Train Model 버튼을 클릭하여 훈련 프로세스를 시작할 수 있습니다. 훈련 이후 모델을 몇 장의 사진 또는 카메라를 사용하여 테스트할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_4.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e결과에 만족하셨다면 Export Model 버튼을 클릭하세요. Tensorflow 탭을 선택하고 Savedmodel 옵션을 선택한 후 Download my model 버튼을 클릭하세요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_5.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003e당신의 브라우저가 모델이 준비되면 다운로드가 시작됩니다.\u003c/p\u003e\n\u003ch1\u003e3. 앱 작성\u003c/h1\u003e\n\u003cp\u003e호스트 컴퓨터에서 pi_garbage_classifier라는 폴더를 만듭니다. 다운로드한 파일을 프로젝트 폴더에 압축해제합니다. 이제 app.py라는 파일을 만들어 각 부분을 조합할 것입니다. 이것은 Teachable Machine의 코드 스니펫에 기반합니다 (Figure 6). 그러나 놀랍게도 샘플 코드는 잘못되었고 오도독도 못하다. keras_model.h5를 열려고 시도했지만 다운로드한 모델은 model.savedmodel이라는 이름입니다.\u003c/p\u003e\n\u003cp\u003e여기는 app.py의 코드입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e cv2\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e keras.models \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_model\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e time\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e re\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e sys\n\npathname = os.path.dirname(sys.argv[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\n\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'sys.argv[0] ='\u003c/span\u003e, sys.argv[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], \u003cspan class=\"hljs-string\"\u003e\"pathname\"\u003c/span\u003e, pathname)    \n\n\u003cspan class=\"hljs-comment\"\u003e# 모델 불러오기\u003c/span\u003e\nmodel = load_model(\u003cspan class=\"hljs-string\"\u003ef'\u003cspan class=\"hljs-subst\"\u003e{pathname}\u003c/span\u003e/model.savedmodel'\u003c/span\u003e)\n\n\u003cspan class=\"hljs-comment\"\u003e# 레이블을 labels.txt 파일에서 가져옵니다. 이것은 나중에 사용됩니다.\u003c/span\u003e\nlabels = \u003cspan class=\"hljs-built_in\"\u003eopen\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef'\u003cspan class=\"hljs-subst\"\u003e{pathname}\u003c/span\u003e/labels.txt'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'r'\u003c/span\u003e).readlines()\n\n\u003cspan class=\"hljs-keyword\"\u003ewhile\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e:\n    time.sleep(\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-comment\"\u003e# CAMERA는 컴퓨터의 기본 카메라에 따라 0 또는 1이 될 수 있습니다.\u003c/span\u003e\n    camera = cv2.VideoCapture(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n    \u003cspan class=\"hljs-comment\"\u003e# 웹캠 이미지 불러오기\u003c/span\u003e\n    ret, image = camera.read()\n    camera.release()\n    \n    \u003cspan class=\"hljs-comment\"\u003e# 이미지를 (224-높이,224-너비) 픽셀로 크기 조정합니다.\u003c/span\u003e\n    image = cv2.resize(image, (\u003cspan class=\"hljs-number\"\u003e224\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e224\u003c/span\u003e), interpolation=cv2.INTER_AREA)\n    \n    \u003cspan class=\"hljs-comment\"\u003e# 이미지를 윈도우에 표시합니다.\u003c/span\u003e\n    \n    image = np.asarray(image, dtype=np.float32).reshape(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e224\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e224\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)\n    \n    image = (image / \u003cspan class=\"hljs-number\"\u003e127.5\u003c/span\u003e) - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n    \n    probabilities = model.predict(image)\n    \n    max_prob = np.\u003cspan class=\"hljs-built_in\"\u003emax\u003c/span\u003e(probabilities)\n    label = re.sub(\u003cspan class=\"hljs-string\"\u003er'\\d+\\s+'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e''\u003c/span\u003e, labels[np.argmax(probabilities)]).strip()\n\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e (label, \u003cspan class=\"hljs-string\"\u003ef\"proba: \u003cspan class=\"hljs-subst\"\u003e{max_prob}\u003c/span\u003e\"\u003c/span\u003e)\n    \n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e label != \u003cspan class=\"hljs-string\"\u003e\"empty\"\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eand\u003c/span\u003e max_prob \u003e \u003cspan class=\"hljs-number\"\u003e0.9\u003c/span\u003e:\n        os.system(\u003cspan class=\"hljs-string\"\u003ef\"festival --tts \u003cspan class=\"hljs-subst\"\u003e{pathname}\u003c/span\u003e/voice/\u003cspan class=\"hljs-subst\"\u003e{label}\u003c/span\u003e.txt\"\u003c/span\u003e)\n\n    keyboard_input = cv2.waitKey(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n    \n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e keyboard_input == \u003cspan class=\"hljs-number\"\u003e27\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003ebreak\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e내 코드에 메인 루프에 sleep 함수를 추가했습니다. 이를 통해 시스템은 5초마다 이미지를 캡처합니다. OpenCV는 캡처된 프레임을 버퍼링하기 때문에 각 반복에서 카메라를 초기화해야 합니다. 코드는 또한 예측 확률이 0.9보다 높으면 쓰레기 카테고리를 발표하기 위해 페스티벌 유틸리티를 사용합니다. 페스티벌은 텍스트 음성 변환 유틸리티입니다. 텍스트 파일을 읽어 내용을 읽어 줄 수 있습니다. 이 프로젝트에서는 voice라는 폴더를 만들고 발표 파일을 넣었습니다. 각 발표 파일에는 카테고리 이름만 작성했습니다. 읽기 편의를 위해 파일 내용을 수정하여 발표를 사용자 정의할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e그래서 최종적으로 폴더 구조는 다음과 같아야 합니다.\u003c/p\u003e\n\u003ch1\u003e4. Raspberry Pi 설정하기\u003c/h1\u003e\n\u003cp\u003e저는 learn.adafruit.com에 있는 훌륭한 설정 안내서를 따랐어요. 먼저, Raspberry Pi Imager를 사용하여 마이크로 SD 카드에 Raspberry Pi OS (64비트)를 기록하세요. 고급 옵션에서 SSH를 활성화하고 사용자 이름과 암호를 설정하세요.\u003c/p\u003e\n\u003cp\u003e카드를 Pi에 넣고 전원을 켜세요. 네트워크에 연결하고 다음 SSH 명령어로 로그인하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 만약 콘솔에서 \u003cspan class=\"hljs-string\"\u003e\"REMOTE HOST IDENTIFICATION HAS CHANGED!\"\u003c/span\u003e 라고 나오면\n# 다음 명령어를 실행하여 리셋하세요\n# ssh-keygen -R raspberrypi.\u003cspan class=\"hljs-property\"\u003elocal\u003c/span\u003e\n\nssh pi@raspberrypi.\u003cspan class=\"hljs-property\"\u003elocal\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e라즈베리 파이 쉘이 준비되면, 아래 명령어를 하나씩 사용하여 필요한 소프트웨어를 설치하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 라즈베리 파이에서 실행하세요\n\nsudo apt update\nsudo apt upgrade -y\n\n# 선택 사항\n# sudo apt install -y python3-pip\n\npip3 install --upgrade setuptools\n\npip3 install \u003cspan class=\"hljs-title class_\"\u003ePillow\u003c/span\u003e==\u003cspan class=\"hljs-number\"\u003e9.2\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.0\u003c/span\u003e\n\npip install opencv-python\n\n\u003cspan class=\"hljs-variable constant_\"\u003eRELEASE\u003c/span\u003e=\u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/PINTO0309/Tensorflow-bin/releases/download/v2.10.0/tensorflow-2.10.0-cp310-none-linux_aarch64.whl\u003c/span\u003e\n\n\u003cspan class=\"hljs-variable constant_\"\u003eCPVER\u003c/span\u003e=$(python --version | grep -\u003cspan class=\"hljs-title class_\"\u003eEo\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'3\\.[0-9]{1,2}'\u003c/span\u003e | tr -d \u003cspan class=\"hljs-string\"\u003e'.'\u003c/span\u003e)\n\npip install $(echo \u003cspan class=\"hljs-string\"\u003e\"$RELEASE\"\u003c/span\u003e | sed -e \u003cspan class=\"hljs-string\"\u003e\"s/cp[0-9]\\{3\\}/CP$CPVER/g\"\u003c/span\u003e)\n\nsudo apt install -y festival\n\n# 선택 사항\n# sudo reboot\n\nmkdir ~/pi_garbage_classifier\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e위의 마지막 명령어는 라즈베리 파이의 홈 디렉토리에 pi_garbage_classifier 라는 폴더를 만듭니다. 프로젝트 파일을 데스크톱 컴퓨터에서 이 폴더로 전송하려면 다음 명령어를 사용하세요.\u003c/p\u003e\n\u003ch1\u003e호스트 데스크톱에서\u003c/h1\u003e\n\u003cp\u003ecd [your_desktop_pi_garbage_classifier_path]\nscp -r ./* \u003ca href=\"mailto:pi@raspberrypi.local\"\u003epi@raspberrypi.local\u003c/a\u003e:~/pi_garbage_classifier/\u003c/p\u003e\n\u003cp\u003e파일을 복사한 후에는 다음 명령어로 앱을 테스트하세요:\u003c/p\u003e\n\u003ch1\u003e라즈베리 파이에서\u003c/h1\u003e\n\u003cp\u003epython /home/pi/pi_garbage_classifier/app.py\u003c/p\u003e\n\u003cp\u003e앱이 시작하는 데 몇 초 정도 걸립니다. 카메라 램프가 깜박일 때, 카메라 앞에 다양한 물체를 두어 시스템을 테스트할 수 있습니다. 물체의 클래스를 알리고 콘솔에 다음 출력이 나타날 것입니다 (그림 8).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine_7.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e앱은 종료할 때까지 계속 실행될 거에요.\u003c/p\u003e\n\u003ch1\u003e5. 라즈베리 파이(Pi) 배포\u003c/h1\u003e\n\u003cp\u003e이제 시스템을 독립적으로 만들어 배포할 시간입니다. 앱을 Pi의 자동 시작 파일에 추가해야 합니다. rc.local과 cron 두 가지 방법을 시도해봤는데, 이 방법들은 데스크톱을 시작하기 전에 앱을 시작하기 때문에 USB 스피커를 활용할 수 없었어요. 그래서 이 포스트에서 해결책을 찾았어요. 시스템 자동 시작 파일에 관련된 것이죠. 따라서 이 명령어를 실행하여 파일을 편집해보세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003esudo nano /etc/xdg/lxsession/\u003cspan class=\"hljs-variable constant_\"\u003eLXDE\u003c/span\u003e-pi/autostart\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e다음 줄을 파일에 추가하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e@python /home/pi/pi_garbage_classifier/app.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이렇게 autostart 파일이 보이게 됩니다.\u003c/p\u003e\n\u003cp\u003e프로그램을 종료하려면 Ctrl+X를 누르고 동일한 파일에 변경 사항을 저장하려면 Y를 누르세요. 다음 명령어를 사용하여 장치를 다시 부팅하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003esudo reboot\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e부팅 후 시스템은 자동으로 작업을 수행해야 합니다. 이제 네트워크 케이블을 제거하고 Pi를 원하는 곳에 배치할 수 있습니다 (비디오 2). 이제 쓰레기의 목적지에 대해 확실하지 않을 때에는 항상 상담할 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e이 기사에서는 시범용 쓰레기 분류 시스템을 구축하는 방법을 보여드렸어요. 설정하는 데 약 30분 가량 밖에 걸리지 않았죠. 아직까지는 두 가지 종류의 쓰레기와 전체 가정 쓰레기의 일부만 인식할 수 있지만 필수 구성 요소들은 모두 갖추고 있어요. 모델에 더 많은 객체와 클래스를 추가할 수 있습니다. 또한 이 프로젝트는 일종의 프레임워크입니다. 다시 말해, 새로운 모델을 훈련하고 Pi에 모델 파일을 교체하여 식물 분류 시스템이나 물체 인식기와 같이 완전히 다른 것으로 변환할 수 있어요.\u003c/p\u003e\n\u003cp\u003eGoogle의 티처블 머신은 정말 우리에게 빛이 되어줬어요. 이전에는 서로 다른 화가들의 그림을 분석하는 데 사용했었죠. 물론 PyTorch나 Keras로 직접 컴퓨터 비전 모델을 사용자화할 수도 있어요. 하지만 그런 작업은 상당한 시간과 노하우를 요구할 수 있어요. 반면, 티처블 머신은 이 모든 것을 아주 간단하게 만들어줘요. 몇 번의 클릭만으로 과연성 있는 모델을 쉽게 얻을 수 있어요. 더불어 모델을 확장하고 수정하는 것도 쉬워요.\u003c/p\u003e\n\u003cp\u003e하지만 생각해 볼 수 있는 작은 단점도 있어요. 티처블 머신은 인터페이스에서 모델 성능을 보여주지 않아요. 따라서 우리는 모델이 얼마나 잘 작동할지를 알 방법이 없어요. 더 많은 클래스를 추가할수록 모델 성능이 떨어지기 때문에 우리는 모델을 충분히 신뢰하고 배포하기 전에 모델의 정확도를 측정해야 해요.\u003c/p\u003e\n\u003cp\u003e이 프로젝트를 시도해 보라고 권장합니다. 그리고 피드백을 주시면 좋을 것 같아요!\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-AnAudioTrash-SortingRaspberryPiwithTeachableMachine"},"buildId":"FH3Qr-mLAesqA0X5IFRQr","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>