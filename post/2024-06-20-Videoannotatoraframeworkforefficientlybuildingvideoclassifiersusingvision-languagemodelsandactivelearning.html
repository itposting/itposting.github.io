<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>비디오 주석 도구 비전-언어 모델과 액티브 러닝을 활용하여 효율적으로 비디오 분류기를 만들 수 있는 프레임워크 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="비디오 주석 도구 비전-언어 모델과 액티브 러닝을 활용하여 효율적으로 비디오 분류기를 만들 수 있는 프레임워크 | itposting" data-gatsby-head="true"/><meta property="og:title" content="비디오 주석 도구 비전-언어 모델과 액티브 러닝을 활용하여 효율적으로 비디오 분류기를 만들 수 있는 프레임워크 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning" data-gatsby-head="true"/><meta name="twitter:title" content="비디오 주석 도구 비전-언어 모델과 액티브 러닝을 활용하여 효율적으로 비디오 분류기를 만들 수 있는 프레임워크 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-20 19:05" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-0fd008072af5a644.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_buildManifest.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">비디오 주석 도구 비전-언어 모델과 액티브 러닝을 활용하여 효율적으로 비디오 분류기를 만들 수 있는 프레임워크</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="비디오 주석 도구 비전-언어 모델과 액티브 러닝을 활용하여 효율적으로 비디오 분류기를 만들 수 있는 프레임워크" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 20, 2024</span><span class="posts_reading_time__f7YPP">5<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>아미르 지아이, 아니시 바르타카비, 켈리 그리그스, 유진 로크, 이본 주크스, 알렉스 알론소, 비 이엔가르, 안나 풀리도</p>
<h1>소개</h1>
<h2>문제</h2>
<p>고품질 및 일관된 주석은 견고한 머신 러닝 모델의 성공적인 개발에 필수적입니다. 머신 러닝 분류기를 훈련시키기 위한 전통적인 기술은 자원이 많이 필요합니다. 이들은 도메인 전문가가 데이터 세트에 주석을 달고, 그 후 데이터 과학자에게 모델을 훈련시키고 결과를 검토하고 변경하는 과정을 포함합니다. 이 라벨링 프로세스는 종종 시간이 많이 소요되고 비효율적이며, 때로는 몇 차례의 주석 주기 후 중단됩니다.</p>
<h2>영향</h2>
<p>결과적으로 고품질 데이터셋에 주석을 다는 것에 투자하는 노력이 복잡한 모델 및 알고리즘 방법을 반복하고 성능을 개선하고 엣지 케이스를 수정하는 것보다 적습니다. 이로 인해 머신러닝 시스템은 빠르게 복잡성을 증가시킵니다.</p>
<p>또한, 시간과 자원에 대한 제한으로 인해 도메인 전문가 대신 제3자 주석 달기를 활용하는 경우가 많습니다. 이러한 주석 달기 작업자는 모델의 의도된 배포나 사용 방법에 대한 깊은 이해 없이 레이블 작업을 수행하며, 특히 주관적인 작업에서 과제인 또는 어려운 예제를 일관되게 주석 달기가 어려울 수 있습니다.</p>
<p>이로 인해 도메인 전문가와의 여러 번의 검토 회의가 필요해 예상치 못한 비용과 지연이 발생합니다. 이러한 긴 주기는 모델 이탈로 이어질 수 있으며, 엣지 케이스를 해결하고 새 모델을 배포하는 데 더 오랜 시간이 걸리기 때문에 유용성과 이해 관계자 신뢰를 해치는 결과를 초래할 수 있습니다.</p>
<h2>해결 방법</h2>
<p>우리는 도메인 전문가들의 직접적인 참여을 통한 인간 중심 시스템 사용으로 이러한 실용적인 도전 과제를 해결할 수 있다고 제안합니다. 우리는 활발한 학습 기법과 대규모 비전-언어 모델의 제로샷 기능을 활용한 새로운 Video Annotator (VA) 프레임워크를 소개합니다. 이를 통해 사용자들이 더 어려운 예제에 집중하도록 안내하고 모델의 샘플 효율성을 향상시키며 비용을 낮출 수 있습니다.</p>
<p>VA는 모델 구축을 데이터 주석 프로세스에 매끄럽게 통합하여 배포 전 모델의 사용자 검증을 용이하게 하여 신뢰 구축을 돕고 소유감을 유발합니다. 또한 VA는 연속 주석 프로세스를 지원하여 사용자가 신속하게 모델을 배포하고 제작 과정에서 모델의 품질을 모니터링하고 몇 가지 추가 예제를 주석 처리하고 새로운 모델 버전을 배포하여 엣지 케이스를 신속히 수정할 수 있습니다.</p>
<p>이 self-service 아키텍처는 데이터 과학자나 제3자 주석자의 활발한 참여 없이 사용자들이 빠른 반복을 통해 개선할 수 있도록 돕습니다.</p>
<h1>비디오 이해</h1>
<p>저희는 VA를 디자인하여 비디오 세그먼트 내에서 시각적인 이미지, 개념 및 이벤트를 식별해야 하는 세밀한 비디오 이해를 지원합니다. 비디오 이해는 검색 및 발견, 맞춤화 및 홍보 자산의 생성과 같은 다양한 응용 프로그램에 근본적으로 중요합니다. 저희의 프레임워크는 이진 비디오 분류기의 확장 가능한 세트를 개발하여 비디오 이해를 위해 기계 학습 모델을 효율적으로 훈련할 수 있도록 합니다. 이는 거대한 콘텐츠 카탈로그의 확장 가능한 점수화 및 검색을 가능하게 합니다.</p>
<h2>비디오 분류</h2>
<p>비디오 분류는 임의 길이의 비디오 클립에 레이블을 할당하는 작업으로, 일반적으로 확률 또는 예측 점수와 함께 나타납니다. Fig 1에서 설명되듯이, 고립된 분류 점수와 검색이 가능한 포괄적인 콘텐츠 카탈로그가 함께 동작합니다.</p>
<p><img src="/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_0.png" alt="이미지"></p>
<h2>확장 가능한 비디오 분류기를 통한 비디오 이해</h2>
<p>이진 분류는 독립성과 유연성을 제공하여 다른 모델과 독립적으로 하나의 모델을 추가하거나 개선할 수 있도록 합니다. 또한 사용자에게 이해하기 쉽고 구축하기 쉬운 추가적인 혜택이 있습니다. 여러 모델의 예측을 결합하면 동영상 콘텐츠의 다양한 수준에서 더 깊은 이해를 얻을 수 있습니다. (그림 2 참조)</p>
<p><img src="/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_1.png" alt="이미지"></p>
<h1>비디오 주석 도구 (VA)</h1>
<p>이 섹션에서는 비디오 분류기를 구축하는 VA의 세 단계 프로세스에 대해 설명합니다.</p>
<h2>단계 1 - 검색</h2>
<p>사용자들은 주석 프로세스를 부트스트랩하기 위해 대형이고 다양한 말뭉치 내에서 초기 예제 세트를 찾아 시작합니다. 우리는 비전-언어 모델의 비디오 및 텍스트 인코더에서 임베딩을 추출하는 데 도움을 주는 텍스트-비디오 검색을 활성화합니다. 예를 들어 "건물의 넓은 샷"을 검색하여 ("건물의 넓은 샷"라고 작성된 그림 3에 설명된) 설립 샷 모델에서 작업하는 주석 작업자가 프로세스를 시작할 수 있습니다.</p>
<p><img src="/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_2.png" alt="이미지"></p>
<h2>단계 2 — 액티브 러닝</h2>
<p>다음 단계는 전통적인 액티브 러닝 루프를 포함합니다. VA는 그런 다음 비디오 임베딩 위에 가벼운 이진 분류기를 구축하고, 이 분류기는 대상 코퍼스 내의 모든 클립에 점수를 매기도록 하며, 피드 내의 일부 예시를 주어 더 많은 주석 및 개선을 위해 제시합니다. 이는 그림 4에서 설명된 내용입니다.</p>
<p><img src="/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_3.png" alt="이미지"></p>
<p>최고 점수의 긍정적 및 부정적 피드는 각각 가장 높은 점수와 가장 낮은 점수를 가진 예제를 보여줍니다. 저희 사용자들은 이를 통해 분류기가 훈련 초기에 올바른 개념을 잘 파악하고, 훈련 데이터의 편향 사례를 식별하여 이후 수정할 수 있었다고 보고했습니다. 또한, 모델이 확신을 가지지 못하는 "중간" 예제를 표시합니다. 이러한 피드는 흥미로운 극단적 사례를 발견하는 데 도움이 되며, 추가적인 개념을 레이블 지정할 필요성을 영감을 줍니다. 마지막으로, 무작위 피드에는 임의로 선택된 클립이 포함되어 다양한 예제를 주석 처리하는 데 도움이 되어 일반화에 중요합니다.</p>
<p>주석 작업자는 언제든지 피드 중 하나에 추가적인 클립에 레이블을 지정하고 새로운 분류기를 구축하고 원하는만큼 반복할 수 있습니다.</p>
<h2>단계 3 — 리뷰</h2>
<p>마지막 단계에서는 사용자에게 모든 주석이 달린 클립이 제시됩니다. 주석 실수를 발견하고 추가 주석을 위한 아이디어와 개념을 확인할 수 있는 좋은 기회입니다. 이 단계에서 사용자들은 종종 단계 1로 돌아가거나 주석을 보다 정제하기 위해 단계 2로 돌아갑니다.</p>
<h1>실험</h1>
<p>VA를 평가하기 위해 3명의 비디오 전문가에게 500k 샷의 비디오 코퍼스에서 다양한 56가지 레이블을 주석 달아달라고 요청했습니다. VA를 몇 가지 기준선 방법의 성능과 비교한 결과, VA가 높은 품질의 비디오 분류기를 생성하는 데 효과적임을 관찰했습니다. 그림 5에서 VA의 성능을 주석 달린 클립 수의 함수로 기준선과 비교했습니다.</p>
<p><img src="/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_4.png" alt="그림"></p>
<p>더 많은 정보 및 VA 및 우리의 실험에 대한 자세한 내용은 이 논문에서 확인하실 수 있습니다.</p>
<h1>결론</h1>
<p>저희는 비디오 어노테이터(VA)를 소개했습니다. VA는 기계 학습 분류기 교육에 관련된 다양한 과제를 해결하는 대화형 프레임워크입니다. VA는 대형 비전-언어 모델의 영점(Zero-shot) 능력과 능동 학습 기술을 활용하여 샘플 효율성을 향상시키고 비용을 줄입니다. VA는 비디오 분류 데이터 집합에 어노테이팅, 관리 및 반복 작업에 독특한 접근 방식을 제공하며, 도메인 전문가들이 인간 중심 시스템에 직접 참여하도록 강조합니다. 어노테이션 프로세스 중에 이러한 사용자들이 어려운 샘플에 대해 신속히 정보 기반의 결정을 내릴 수 있도록 함으로써, VA는 시스템의 전체 효율성을 높입니다. 더불어, 이는 지속적인 어노테이션 프로세스를 가능하게 하며, 사용자가 신속하게 모델을 배포하고, 제작에서 그 품질을 모니터하고, 어떤 예외 사례라도 신속히 수정할 수 있도록 합니다.</p>
<p>이 SELF-SERVICE 구조는 도메인 전문가가 데이터 과학자나 제3자 어노테이터의 적극적인 참여 없이 자체 개선을 할 수 있도록 하며, 시스템에 대한 신뢰를 쌓음과 동시에 소유감을 유지하게 합니다.</p>
<p>우리는 VA의 성능을 연구하기 위한 실험을 실시했으며, 여러 비디오 이해 작업에 걸쳐 가장 경쟁력 있는 기준에 비해 평균 정밀도(average precision)에서 중위 8.3 포인트 개선을 나타내었습니다. VA를 사용하여 3명의 전문 비디오 편집자가 어노테이팅한 56가지 비디오 이해 작업에 걸친 153,000개의 레이블이 포함된 데이터셋을 공개하며, 실험을 복제하기 위한 코드도 공개합니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"비디오 주석 도구 비전-언어 모델과 액티브 러닝을 활용하여 효율적으로 비디오 분류기를 만들 수 있는 프레임워크","description":"","date":"2024-06-20 19:05","slug":"2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning","content":"\n\n아미르 지아이, 아니시 바르타카비, 켈리 그리그스, 유진 로크, 이본 주크스, 알렉스 알론소, 비 이엔가르, 안나 풀리도\n\n# 소개\n\n## 문제\n\n고품질 및 일관된 주석은 견고한 머신 러닝 모델의 성공적인 개발에 필수적입니다. 머신 러닝 분류기를 훈련시키기 위한 전통적인 기술은 자원이 많이 필요합니다. 이들은 도메인 전문가가 데이터 세트에 주석을 달고, 그 후 데이터 과학자에게 모델을 훈련시키고 결과를 검토하고 변경하는 과정을 포함합니다. 이 라벨링 프로세스는 종종 시간이 많이 소요되고 비효율적이며, 때로는 몇 차례의 주석 주기 후 중단됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 영향\n\n결과적으로 고품질 데이터셋에 주석을 다는 것에 투자하는 노력이 복잡한 모델 및 알고리즘 방법을 반복하고 성능을 개선하고 엣지 케이스를 수정하는 것보다 적습니다. 이로 인해 머신러닝 시스템은 빠르게 복잡성을 증가시킵니다.\n\n또한, 시간과 자원에 대한 제한으로 인해 도메인 전문가 대신 제3자 주석 달기를 활용하는 경우가 많습니다. 이러한 주석 달기 작업자는 모델의 의도된 배포나 사용 방법에 대한 깊은 이해 없이 레이블 작업을 수행하며, 특히 주관적인 작업에서 과제인 또는 어려운 예제를 일관되게 주석 달기가 어려울 수 있습니다.\n\n이로 인해 도메인 전문가와의 여러 번의 검토 회의가 필요해 예상치 못한 비용과 지연이 발생합니다. 이러한 긴 주기는 모델 이탈로 이어질 수 있으며, 엣지 케이스를 해결하고 새 모델을 배포하는 데 더 오랜 시간이 걸리기 때문에 유용성과 이해 관계자 신뢰를 해치는 결과를 초래할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 해결 방법\n\n우리는 도메인 전문가들의 직접적인 참여을 통한 인간 중심 시스템 사용으로 이러한 실용적인 도전 과제를 해결할 수 있다고 제안합니다. 우리는 활발한 학습 기법과 대규모 비전-언어 모델의 제로샷 기능을 활용한 새로운 Video Annotator (VA) 프레임워크를 소개합니다. 이를 통해 사용자들이 더 어려운 예제에 집중하도록 안내하고 모델의 샘플 효율성을 향상시키며 비용을 낮출 수 있습니다.\n\nVA는 모델 구축을 데이터 주석 프로세스에 매끄럽게 통합하여 배포 전 모델의 사용자 검증을 용이하게 하여 신뢰 구축을 돕고 소유감을 유발합니다. 또한 VA는 연속 주석 프로세스를 지원하여 사용자가 신속하게 모델을 배포하고 제작 과정에서 모델의 품질을 모니터링하고 몇 가지 추가 예제를 주석 처리하고 새로운 모델 버전을 배포하여 엣지 케이스를 신속히 수정할 수 있습니다.\n\n이 self-service 아키텍처는 데이터 과학자나 제3자 주석자의 활발한 참여 없이 사용자들이 빠른 반복을 통해 개선할 수 있도록 돕습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 비디오 이해\n\n저희는 VA를 디자인하여 비디오 세그먼트 내에서 시각적인 이미지, 개념 및 이벤트를 식별해야 하는 세밀한 비디오 이해를 지원합니다. 비디오 이해는 검색 및 발견, 맞춤화 및 홍보 자산의 생성과 같은 다양한 응용 프로그램에 근본적으로 중요합니다. 저희의 프레임워크는 이진 비디오 분류기의 확장 가능한 세트를 개발하여 비디오 이해를 위해 기계 학습 모델을 효율적으로 훈련할 수 있도록 합니다. 이는 거대한 콘텐츠 카탈로그의 확장 가능한 점수화 및 검색을 가능하게 합니다.\n\n## 비디오 분류\n\n비디오 분류는 임의 길이의 비디오 클립에 레이블을 할당하는 작업으로, 일반적으로 확률 또는 예측 점수와 함께 나타납니다. Fig 1에서 설명되듯이, 고립된 분류 점수와 검색이 가능한 포괄적인 콘텐츠 카탈로그가 함께 동작합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_0.png)\n\n## 확장 가능한 비디오 분류기를 통한 비디오 이해\n\n이진 분류는 독립성과 유연성을 제공하여 다른 모델과 독립적으로 하나의 모델을 추가하거나 개선할 수 있도록 합니다. 또한 사용자에게 이해하기 쉽고 구축하기 쉬운 추가적인 혜택이 있습니다. 여러 모델의 예측을 결합하면 동영상 콘텐츠의 다양한 수준에서 더 깊은 이해를 얻을 수 있습니다. (그림 2 참조)\n\n![이미지](/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_1.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 비디오 주석 도구 (VA)\n\n이 섹션에서는 비디오 분류기를 구축하는 VA의 세 단계 프로세스에 대해 설명합니다.\n\n## 단계 1 - 검색\n\n사용자들은 주석 프로세스를 부트스트랩하기 위해 대형이고 다양한 말뭉치 내에서 초기 예제 세트를 찾아 시작합니다. 우리는 비전-언어 모델의 비디오 및 텍스트 인코더에서 임베딩을 추출하는 데 도움을 주는 텍스트-비디오 검색을 활성화합니다. 예를 들어 \"건물의 넓은 샷\"을 검색하여 (\"건물의 넓은 샷\"라고 작성된 그림 3에 설명된) 설립 샷 모델에서 작업하는 주석 작업자가 프로세스를 시작할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_2.png)\n\n## 단계 2 — 액티브 러닝\n\n다음 단계는 전통적인 액티브 러닝 루프를 포함합니다. VA는 그런 다음 비디오 임베딩 위에 가벼운 이진 분류기를 구축하고, 이 분류기는 대상 코퍼스 내의 모든 클립에 점수를 매기도록 하며, 피드 내의 일부 예시를 주어 더 많은 주석 및 개선을 위해 제시합니다. 이는 그림 4에서 설명된 내용입니다.\n\n![이미지](/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_3.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최고 점수의 긍정적 및 부정적 피드는 각각 가장 높은 점수와 가장 낮은 점수를 가진 예제를 보여줍니다. 저희 사용자들은 이를 통해 분류기가 훈련 초기에 올바른 개념을 잘 파악하고, 훈련 데이터의 편향 사례를 식별하여 이후 수정할 수 있었다고 보고했습니다. 또한, 모델이 확신을 가지지 못하는 \"중간\" 예제를 표시합니다. 이러한 피드는 흥미로운 극단적 사례를 발견하는 데 도움이 되며, 추가적인 개념을 레이블 지정할 필요성을 영감을 줍니다. 마지막으로, 무작위 피드에는 임의로 선택된 클립이 포함되어 다양한 예제를 주석 처리하는 데 도움이 되어 일반화에 중요합니다.\n\n주석 작업자는 언제든지 피드 중 하나에 추가적인 클립에 레이블을 지정하고 새로운 분류기를 구축하고 원하는만큼 반복할 수 있습니다.\n\n## 단계 3 — 리뷰\n\n마지막 단계에서는 사용자에게 모든 주석이 달린 클립이 제시됩니다. 주석 실수를 발견하고 추가 주석을 위한 아이디어와 개념을 확인할 수 있는 좋은 기회입니다. 이 단계에서 사용자들은 종종 단계 1로 돌아가거나 주석을 보다 정제하기 위해 단계 2로 돌아갑니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 실험\n\nVA를 평가하기 위해 3명의 비디오 전문가에게 500k 샷의 비디오 코퍼스에서 다양한 56가지 레이블을 주석 달아달라고 요청했습니다. VA를 몇 가지 기준선 방법의 성능과 비교한 결과, VA가 높은 품질의 비디오 분류기를 생성하는 데 효과적임을 관찰했습니다. 그림 5에서 VA의 성능을 주석 달린 클립 수의 함수로 기준선과 비교했습니다.\n\n![그림](/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_4.png)\n\n더 많은 정보 및 VA 및 우리의 실험에 대한 자세한 내용은 이 논문에서 확인하실 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n저희는 비디오 어노테이터(VA)를 소개했습니다. VA는 기계 학습 분류기 교육에 관련된 다양한 과제를 해결하는 대화형 프레임워크입니다. VA는 대형 비전-언어 모델의 영점(Zero-shot) 능력과 능동 학습 기술을 활용하여 샘플 효율성을 향상시키고 비용을 줄입니다. VA는 비디오 분류 데이터 집합에 어노테이팅, 관리 및 반복 작업에 독특한 접근 방식을 제공하며, 도메인 전문가들이 인간 중심 시스템에 직접 참여하도록 강조합니다. 어노테이션 프로세스 중에 이러한 사용자들이 어려운 샘플에 대해 신속히 정보 기반의 결정을 내릴 수 있도록 함으로써, VA는 시스템의 전체 효율성을 높입니다. 더불어, 이는 지속적인 어노테이션 프로세스를 가능하게 하며, 사용자가 신속하게 모델을 배포하고, 제작에서 그 품질을 모니터하고, 어떤 예외 사례라도 신속히 수정할 수 있도록 합니다.\n\n이 SELF-SERVICE 구조는 도메인 전문가가 데이터 과학자나 제3자 어노테이터의 적극적인 참여 없이 자체 개선을 할 수 있도록 하며, 시스템에 대한 신뢰를 쌓음과 동시에 소유감을 유지하게 합니다.\n\n우리는 VA의 성능을 연구하기 위한 실험을 실시했으며, 여러 비디오 이해 작업에 걸쳐 가장 경쟁력 있는 기준에 비해 평균 정밀도(average precision)에서 중위 8.3 포인트 개선을 나타내었습니다. VA를 사용하여 3명의 전문 비디오 편집자가 어노테이팅한 56가지 비디오 이해 작업에 걸친 153,000개의 레이블이 포함된 데이터셋을 공개하며, 실험을 복제하기 위한 코드도 공개합니다.","ogImage":{"url":"/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_0.png"},"coverImage":"/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_0.png","tag":["Tech"],"readingTime":5},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e아미르 지아이, 아니시 바르타카비, 켈리 그리그스, 유진 로크, 이본 주크스, 알렉스 알론소, 비 이엔가르, 안나 풀리도\u003c/p\u003e\n\u003ch1\u003e소개\u003c/h1\u003e\n\u003ch2\u003e문제\u003c/h2\u003e\n\u003cp\u003e고품질 및 일관된 주석은 견고한 머신 러닝 모델의 성공적인 개발에 필수적입니다. 머신 러닝 분류기를 훈련시키기 위한 전통적인 기술은 자원이 많이 필요합니다. 이들은 도메인 전문가가 데이터 세트에 주석을 달고, 그 후 데이터 과학자에게 모델을 훈련시키고 결과를 검토하고 변경하는 과정을 포함합니다. 이 라벨링 프로세스는 종종 시간이 많이 소요되고 비효율적이며, 때로는 몇 차례의 주석 주기 후 중단됩니다.\u003c/p\u003e\n\u003ch2\u003e영향\u003c/h2\u003e\n\u003cp\u003e결과적으로 고품질 데이터셋에 주석을 다는 것에 투자하는 노력이 복잡한 모델 및 알고리즘 방법을 반복하고 성능을 개선하고 엣지 케이스를 수정하는 것보다 적습니다. 이로 인해 머신러닝 시스템은 빠르게 복잡성을 증가시킵니다.\u003c/p\u003e\n\u003cp\u003e또한, 시간과 자원에 대한 제한으로 인해 도메인 전문가 대신 제3자 주석 달기를 활용하는 경우가 많습니다. 이러한 주석 달기 작업자는 모델의 의도된 배포나 사용 방법에 대한 깊은 이해 없이 레이블 작업을 수행하며, 특히 주관적인 작업에서 과제인 또는 어려운 예제를 일관되게 주석 달기가 어려울 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이로 인해 도메인 전문가와의 여러 번의 검토 회의가 필요해 예상치 못한 비용과 지연이 발생합니다. 이러한 긴 주기는 모델 이탈로 이어질 수 있으며, 엣지 케이스를 해결하고 새 모델을 배포하는 데 더 오랜 시간이 걸리기 때문에 유용성과 이해 관계자 신뢰를 해치는 결과를 초래할 수 있습니다.\u003c/p\u003e\n\u003ch2\u003e해결 방법\u003c/h2\u003e\n\u003cp\u003e우리는 도메인 전문가들의 직접적인 참여을 통한 인간 중심 시스템 사용으로 이러한 실용적인 도전 과제를 해결할 수 있다고 제안합니다. 우리는 활발한 학습 기법과 대규모 비전-언어 모델의 제로샷 기능을 활용한 새로운 Video Annotator (VA) 프레임워크를 소개합니다. 이를 통해 사용자들이 더 어려운 예제에 집중하도록 안내하고 모델의 샘플 효율성을 향상시키며 비용을 낮출 수 있습니다.\u003c/p\u003e\n\u003cp\u003eVA는 모델 구축을 데이터 주석 프로세스에 매끄럽게 통합하여 배포 전 모델의 사용자 검증을 용이하게 하여 신뢰 구축을 돕고 소유감을 유발합니다. 또한 VA는 연속 주석 프로세스를 지원하여 사용자가 신속하게 모델을 배포하고 제작 과정에서 모델의 품질을 모니터링하고 몇 가지 추가 예제를 주석 처리하고 새로운 모델 버전을 배포하여 엣지 케이스를 신속히 수정할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이 self-service 아키텍처는 데이터 과학자나 제3자 주석자의 활발한 참여 없이 사용자들이 빠른 반복을 통해 개선할 수 있도록 돕습니다.\u003c/p\u003e\n\u003ch1\u003e비디오 이해\u003c/h1\u003e\n\u003cp\u003e저희는 VA를 디자인하여 비디오 세그먼트 내에서 시각적인 이미지, 개념 및 이벤트를 식별해야 하는 세밀한 비디오 이해를 지원합니다. 비디오 이해는 검색 및 발견, 맞춤화 및 홍보 자산의 생성과 같은 다양한 응용 프로그램에 근본적으로 중요합니다. 저희의 프레임워크는 이진 비디오 분류기의 확장 가능한 세트를 개발하여 비디오 이해를 위해 기계 학습 모델을 효율적으로 훈련할 수 있도록 합니다. 이는 거대한 콘텐츠 카탈로그의 확장 가능한 점수화 및 검색을 가능하게 합니다.\u003c/p\u003e\n\u003ch2\u003e비디오 분류\u003c/h2\u003e\n\u003cp\u003e비디오 분류는 임의 길이의 비디오 클립에 레이블을 할당하는 작업으로, 일반적으로 확률 또는 예측 점수와 함께 나타납니다. Fig 1에서 설명되듯이, 고립된 분류 점수와 검색이 가능한 포괄적인 콘텐츠 카탈로그가 함께 동작합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch2\u003e확장 가능한 비디오 분류기를 통한 비디오 이해\u003c/h2\u003e\n\u003cp\u003e이진 분류는 독립성과 유연성을 제공하여 다른 모델과 독립적으로 하나의 모델을 추가하거나 개선할 수 있도록 합니다. 또한 사용자에게 이해하기 쉽고 구축하기 쉬운 추가적인 혜택이 있습니다. 여러 모델의 예측을 결합하면 동영상 콘텐츠의 다양한 수준에서 더 깊은 이해를 얻을 수 있습니다. (그림 2 참조)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch1\u003e비디오 주석 도구 (VA)\u003c/h1\u003e\n\u003cp\u003e이 섹션에서는 비디오 분류기를 구축하는 VA의 세 단계 프로세스에 대해 설명합니다.\u003c/p\u003e\n\u003ch2\u003e단계 1 - 검색\u003c/h2\u003e\n\u003cp\u003e사용자들은 주석 프로세스를 부트스트랩하기 위해 대형이고 다양한 말뭉치 내에서 초기 예제 세트를 찾아 시작합니다. 우리는 비전-언어 모델의 비디오 및 텍스트 인코더에서 임베딩을 추출하는 데 도움을 주는 텍스트-비디오 검색을 활성화합니다. 예를 들어 \"건물의 넓은 샷\"을 검색하여 (\"건물의 넓은 샷\"라고 작성된 그림 3에 설명된) 설립 샷 모델에서 작업하는 주석 작업자가 프로세스를 시작할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch2\u003e단계 2 — 액티브 러닝\u003c/h2\u003e\n\u003cp\u003e다음 단계는 전통적인 액티브 러닝 루프를 포함합니다. VA는 그런 다음 비디오 임베딩 위에 가벼운 이진 분류기를 구축하고, 이 분류기는 대상 코퍼스 내의 모든 클립에 점수를 매기도록 하며, 피드 내의 일부 예시를 주어 더 많은 주석 및 개선을 위해 제시합니다. 이는 그림 4에서 설명된 내용입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_3.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e최고 점수의 긍정적 및 부정적 피드는 각각 가장 높은 점수와 가장 낮은 점수를 가진 예제를 보여줍니다. 저희 사용자들은 이를 통해 분류기가 훈련 초기에 올바른 개념을 잘 파악하고, 훈련 데이터의 편향 사례를 식별하여 이후 수정할 수 있었다고 보고했습니다. 또한, 모델이 확신을 가지지 못하는 \"중간\" 예제를 표시합니다. 이러한 피드는 흥미로운 극단적 사례를 발견하는 데 도움이 되며, 추가적인 개념을 레이블 지정할 필요성을 영감을 줍니다. 마지막으로, 무작위 피드에는 임의로 선택된 클립이 포함되어 다양한 예제를 주석 처리하는 데 도움이 되어 일반화에 중요합니다.\u003c/p\u003e\n\u003cp\u003e주석 작업자는 언제든지 피드 중 하나에 추가적인 클립에 레이블을 지정하고 새로운 분류기를 구축하고 원하는만큼 반복할 수 있습니다.\u003c/p\u003e\n\u003ch2\u003e단계 3 — 리뷰\u003c/h2\u003e\n\u003cp\u003e마지막 단계에서는 사용자에게 모든 주석이 달린 클립이 제시됩니다. 주석 실수를 발견하고 추가 주석을 위한 아이디어와 개념을 확인할 수 있는 좋은 기회입니다. 이 단계에서 사용자들은 종종 단계 1로 돌아가거나 주석을 보다 정제하기 위해 단계 2로 돌아갑니다.\u003c/p\u003e\n\u003ch1\u003e실험\u003c/h1\u003e\n\u003cp\u003eVA를 평가하기 위해 3명의 비디오 전문가에게 500k 샷의 비디오 코퍼스에서 다양한 56가지 레이블을 주석 달아달라고 요청했습니다. VA를 몇 가지 기준선 방법의 성능과 비교한 결과, VA가 높은 품질의 비디오 분류기를 생성하는 데 효과적임을 관찰했습니다. 그림 5에서 VA의 성능을 주석 달린 클립 수의 함수로 기준선과 비교했습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning_4.png\" alt=\"그림\"\u003e\u003c/p\u003e\n\u003cp\u003e더 많은 정보 및 VA 및 우리의 실험에 대한 자세한 내용은 이 논문에서 확인하실 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e저희는 비디오 어노테이터(VA)를 소개했습니다. VA는 기계 학습 분류기 교육에 관련된 다양한 과제를 해결하는 대화형 프레임워크입니다. VA는 대형 비전-언어 모델의 영점(Zero-shot) 능력과 능동 학습 기술을 활용하여 샘플 효율성을 향상시키고 비용을 줄입니다. VA는 비디오 분류 데이터 집합에 어노테이팅, 관리 및 반복 작업에 독특한 접근 방식을 제공하며, 도메인 전문가들이 인간 중심 시스템에 직접 참여하도록 강조합니다. 어노테이션 프로세스 중에 이러한 사용자들이 어려운 샘플에 대해 신속히 정보 기반의 결정을 내릴 수 있도록 함으로써, VA는 시스템의 전체 효율성을 높입니다. 더불어, 이는 지속적인 어노테이션 프로세스를 가능하게 하며, 사용자가 신속하게 모델을 배포하고, 제작에서 그 품질을 모니터하고, 어떤 예외 사례라도 신속히 수정할 수 있도록 합니다.\u003c/p\u003e\n\u003cp\u003e이 SELF-SERVICE 구조는 도메인 전문가가 데이터 과학자나 제3자 어노테이터의 적극적인 참여 없이 자체 개선을 할 수 있도록 하며, 시스템에 대한 신뢰를 쌓음과 동시에 소유감을 유지하게 합니다.\u003c/p\u003e\n\u003cp\u003e우리는 VA의 성능을 연구하기 위한 실험을 실시했으며, 여러 비디오 이해 작업에 걸쳐 가장 경쟁력 있는 기준에 비해 평균 정밀도(average precision)에서 중위 8.3 포인트 개선을 나타내었습니다. VA를 사용하여 3명의 전문 비디오 편집자가 어노테이팅한 56가지 비디오 이해 작업에 걸친 153,000개의 레이블이 포함된 데이터셋을 공개하며, 실험을 복제하기 위한 코드도 공개합니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-20-Videoannotatoraframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning"},"buildId":"FH3Qr-mLAesqA0X5IFRQr","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>