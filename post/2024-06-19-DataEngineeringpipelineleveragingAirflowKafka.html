<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요  | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요  | itposting" data-gatsby-head="true"/><meta property="og:title" content="특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요  | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka" data-gatsby-head="true"/><meta name="twitter:title" content="특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요  | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 09:48" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 </h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 " loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">10<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h1>소개</h1>
<p>이 문서에서는 Apache Airflow 및 Kafka(오픈 소스)를 활용하여 실시간 날씨 업데이트를 읽고 Twitter의 이벤트 스트림을 분석하여 대중의 반응을 이해하는 데이터 엔지니어링 파이프라인 아키텍처를 솔루션 디자인하는 방법에 대한 기본적인 개요를 제공합니다.</p>
<p><img src="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png" alt="이미지"></p>
<h1>ETL 기본 원칙</h1>
<div class="content-ad"></div>
<p>ETL은 데이터 웨어하우스 또는 데이터 마트와 같은 분석 환경을 위해 데이터 획득 및 준비를 자동화하는 데이터 파이프라인 엔지니어링 방법론인 추출, 변환, 로드를 의미합니다. 이는 다양한 소스에서 데이터를 수집하고 표준 형식으로 편집한 다음 시각화, 탐색 및 모델링을 위한 새로운 환경으로 로드하는 것을 포함하며, 자동화 및 의사 결정을 지원합니다.</p>
<h1>ELT 기본</h1>
<p>ELT 프로세스는 추출, 로드 및 변환을 나타내며, 단곅하계의 단계 순서로 인한 독특한 차이로 인해 ETL과 다릅니다. ELT에서는 데이터가 데이터 레이크와 같은 목적지 환경으로 원본 형식 그대로 직접 로드됩니다. 이를 통해 목적지 플랫폼 내에서 필요에 따라 변환을 수행하고 동적으로 사용자 주도적 변경을 가능하게 합니다.</p>
<h1>ETL과 ELT 비교</h1>
<div class="content-ad"></div>
<p>ETL과 ELT의 차이점: ETL 파이프라인에서는 변환 작업이 목적지에 도달하기 전에 데이터 파이프라인 내에서 발생하는 반면, ELT는 변환 작업을 분리하여 목적지 환경에서 필요한 대로 수행할 수 있습니다. ETL은 엄격하고 목적이 명확하지만, ELT는 유연하며 빅 데이터 처리에 대한 셀프 서비스 분석을 제공합니다.</p>
<h1>데이터 수집 기술</h1>
<p>다양한 데이터 수집 기술에는 다음이 포함됩니다:</p>
<ul>
<li>완전 수집 대 부분 수집</li>
</ul>
<div class="content-ad"></div>
<ul>
<li>전체 로딩: 데이터베이스에 초기 히스토리를 로드합니다. 추적 데이터는 새 창고에서 시작됩니다.</li>
<li>점진적 로딩: 새 데이터를 삽입하거나 이미 로드된 데이터를 업데이트합니다. 거래 히스토리를 누적하는 데 사용됩니다. 데이터 양과 속도에 따라 일괄 또는 스트림 로드될 수 있습니다.</li>
</ul>
<p>정기 로딩 vs. 요청 로딩:</p>
<ul>
<li>정기 로딩: 매일 거래를 데이터베이스에 주기적으로 로드하며, 스크립트 작업에 의해 자동화됩니다.</li>
<li>요청 로딩: 소스 데이터가 지정된 크기에 도달하거나 움직임, 소리 또는 임의의 변경 이벤트와 같은 다양한 이벤트에 의해 트리거됩니다.</li>
</ul>
<p>일괄 처리 vs. 스트림 로딩:</p>
<div class="content-ad"></div>
<ul>
<li>배치 로딩: 시간별로 정의된 단위로 데이터를 로드하며 일반적으로 몇 시간에서 며칠 동안 누적됩니다.</li>
<li>스트림 로딩: 데이터가 제공되는 즉시 실시간으로 로드됩니다.</li>
<li>마이크로 배치 로딩: 즉시 처리를 위한 최근 데이터에 액세스합니다.</li>
</ul>
<p>푸시 대 수신 데이터 로딩:</p>
<ul>
<li>수신 방법: 클라이언트가 서버로부터 데이터를 요청합니다(예: RSS 피드, 이메일).</li>
<li>푸시 방법: 클라이언트가 서버 서비스에 구독하여 데이터를 실시간으로 전달 받습니다(예: 푸시 알림, 즉각 메시징 서비스).</li>
</ul>
<h1>데이터 파이프라인이란 무엇인가요?</h1>
<div class="content-ad"></div>
<p>데이터 파이프라인은 데이터의 이동 또는 수정에 특히 관련이 있어요. 이러한 파이프라인은 데이터를 한 곳이나 형식에서 다른 곳이나 형식으로 운송하는 것을 목표로 하며, 데이터를 추출하고 최종적으로 적재하기 위해 선택적 변환 단계를 통해 안내하는 시스템을 구성합니다.</p>
<p>파이프라인을 통해 흐르는 데이터를 시각화하는 것은 데이터 패킷으로 표현할 수 있으며, 이는 데이터의 단위를 넓게 이야기합니다. 이러한 패킷은 단일 레코드나 이벤트에서 대량 데이터 수집물까지 다양할 수 있어요. 이 맥락에서 데이터 패킷은 파이프라인으로 흡수되기 위해 대기열에 정리되며, 데이터 파이프라인의 길이는 단일 패킷이 횡단하는 데 걸리는 시간을 의미합니다. 패킷 간의 화살표는 처리량 지연이나 연속 패킷 도착 사이의 시간을 나타냅니다.</p>
<p>데이터 파이프라인 주요 성능 지표</p>
<ul>
<li>지연 시간: 데이터 패킷이 파이프라인을 통과하는 총 시간을 의미합니다. 지연 시간은 파이프라인 내 각 처리 단계에서 소요된 개별 시간의 합으로, 파이프라인 내 가장 느린 프로세스에 의해 제한됩니다. 예를 들어, 웹 페이지의 로딩 시간은 서버 속도에 따라 결정되며 인터넷 서비스 속도와 관계없이 서버 속도에 의해 제어됩니다.</li>
<li>처리량: 이는 시간 단위당 파이프라인을 통해 처리될 수 있는 데이터 양을 의미합니다. 처리량을 증가시키는 것은 시간 단위당 더 많은 패킷을 처리하고 큰 상자를 차례차례 통과시키는 우리 친구 사슬 예시와 유사합니다.</li>
</ul>
<div class="content-ad"></div>
<p>데이터 파이프라인 응용:</p>
<ul>
<li>간단한 복사 파이프라인: 파일 백업과 같이 데이터를 한 위치에서 다른 위치로 복사하는 작업을 포함합니다.</li>
<li>데이터 레이크 통합: 분산된 래 데이터 소스를 데이터 레이크에 통합하는 작업입니다.</li>
<li>거래 기록 이동: 거래 기록을 데이터 웨어하우스로 전송하는 작업입니다.</li>
<li>IoT 데이터 스트리밍: IoT 장치에서 데이터를 스트리밍하여 대시보드나 경보 시스템에서 정보를 제공하는 것을 말합니다.</li>
<li>기계 학습용 데이터 준비: 기계 학습의 개발이나 제품화를 위해 래 데이터를 준비하는 작업입니다.</li>
<li>메시지 보내기 및 받기: 이메일, SMS 또는 온라인 비디오 회의와 같은 애플리케이션을 포함합니다.</li>
</ul>
<h1>주요 데이터 파이프라인 프로세스</h1>
<p>데이터 파이프라인 프로세스는 일반적으로 구조화된 일련의 단계를 따릅니다:</p>
<div class="content-ad"></div>
<ul>
<li>추출: 하나 이상의 소스에서 데이터를 검색하는 과정입니다.</li>
<li>투입: 추출된 데이터는 후속 처리를 위해 파이프라인에 투입됩니다.</li>
<li>변환: 파이프라인 내의 선택적인 단계에서 데이터를 변환할 수 있습니다.</li>
<li>로딩: 최종 단계는 변환된 데이터를 대상 시설로 로드합니다.</li>
<li>스케줄링/트리거링: 작업을 필요에 따라 예약하거나 트리거하는 메커니즘입니다.</li>
<li>모니터링: 전체 워크플로우가 효율적으로 작동하도록 모니터링됩니다.</li>
<li>유지보수 및 최적화: 원활한 파이프라인 운영을 보장하기 위해 정기적인 유지보수 및 최적화 작업이 수행됩니다.</li>
</ul>
<h1>Apache Airflow</h1>
<ul>
<li>Python 기반의 오픈 소스 "구성과 코드" 플랫폼입니다. AirBNB에서 오픈 소스로 공개되었습니다.</li>
<li>데이터 파이프라인 워크플로우를 작성, 예약 및 모니터링할 수 있습니다.</li>
<li>확장 가능하며 병렬 컴퓨팅 노드를 지원하며 주요 클라우드 플랫폼과 통합됩니다.</li>
</ul>
<p><img src="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_1.png" alt="이미지"></p>
<div class="content-ad"></div>
<ul>
<li>스케줄러: 예약된 워크플로우를 트리거합니다.</li>
<li>실행기: 작업을 Worker에 할당하여 실행합니다.</li>
<li>웹 서버: DAG 검사, 트리거 및 디버깅을 위한 대화형 UI를 호스팅합니다.</li>
<li>DAG 디렉토리: 스케줄러, 실행기 및 Worker가 액세스할 수 있는 DAG 파일을 저장합니다.</li>
<li>메타데이터 데이터베이스: 각 DAG 및 해당 작업의 상태를 유지합니다.</li>
</ul>
<p>DAG 및 작업 라이프사이클</p>
<p>DAG(유향 비순환 그래프)는 작업 간의 종속성과 실행 순서를 지정합니다. 'DAG'는 순환이나 사이클이 없는 관계를 나타내는 특정 유형의 그래프로, 노드와 간선으로 구성되며 방향성을 가진 간선이 노드 간의 흐름을 보여줍니다.</p>
<p><img src="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_2.png" alt="이미지"></p>
<div class="content-ad"></div>
<p>작업 상태:</p>
<ul>
<li>상태 없음: 실행을 위해 대기 중인 작업.</li>
<li>스케줄됨: 의존성에 따라 실행이 예약된 작업.</li>
<li>제거됨: 실행이 시작된 이후에 사라진 작업.</li>
<li>상류 작업 실패: 상류 작업에서 실패 발생.</li>
<li>대기 중: 워커 가용성을 기다리는 작업.</li>
<li>실행 중: 워커에 의해 실행 중인 작업.</li>
<li>성공: 오류 없이 작업이 완료된 상태.</li>
<li>실패: 실행 중에 오류가 발생한 작업.</li>
<li>재시도 예정: 남은 재시도 횟수가 남아 있는 실패한 작업으로, 다시 예약됨.</li>
<li>이상적인 작업 흐름: '상태 없음'에서 '스케줄됨'으로, '대기 중'으로, '실행 중'으로 이어져 '성공'으로 마무리됨.</li>
</ul>
<p><img src="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_3.png" alt="이미지"></p>
<p>Airflow DAG 스크립트의 논리 블록</p>
<div class="content-ad"></div>
<ul>
<li>라이브러리 가져오기: 필요한 Python 라이브러리 가져오기.</li>
<li>DAG 인수: DAG에 대한 기본 인수(시작 날짜와 같은 것) 정의.</li>
<li>DAG 정의: 특정 속성을 사용하여 DAG 인스턴스화.</li>
<li>작업 정의: DAG 내부의 개별 작업(노드) 정의.</li>
<li>작업 파이프라인: 작업 간의 의존성을 지정하여 작업 간의 흐름을 구축.</li>
</ul>
<p>이러한 논리적 블록이 포함된 Python 스크립트 예제를 살펴보세요.</p>
<p><img src="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_4.png" alt="이미지"></p>
<h1>Kafka를 활용한 스트리밍 파이프라인 구축</h1>
<div class="content-ad"></div>
<p>이벤트는 이벤트 스트리밍의 맥락에서 엔티티의 관찰 가능한 상태 업데이트를 설명하는 데이터를 의미합니다. 예시로는 자동차의 GPS 좌표, 방 온도, 또는 응용 프로그램의 RAM 사용량 등이 있습니다.</p>
<p>이벤트는 다양한 형식으로 제공됩니다:</p>
<ul>
<li>텍스트, 숫자 또는 날짜와 같은 원시 유형</li>
<li>값이 원시 또는 복합 유형인 키-값 쌍 형식(e.g., JSON, XML)</li>
<li>타임스탬프가 포함된 시간 감도를 위한 키-값 형식</li>
</ul>
<p>한 소스에서 한 대상으로: 이벤트 스트리밍은 소스(센서, 데이터베이스, 응용 프로그램)가 실시간 이벤트를 지속적으로 생성하고 이를 대상지(파일 시스템, 데이터베이스, 응용 프로그램)로 전달하는 것을 의미합니다. 이 과정은 이벤트 스트리밍이라고 불립니다.</p>
<div class="content-ad"></div>
<p>많은 출처에서 많은 대상으로: 다양한 통신 프로토콜(FTP, HTTP, JDBC, SCP)을 사용하는 여러 분산 이벤트 소스 및 대상을 관리하는 것은 도전이 될 수 있습니다. 이벤트 스트림 플랫폼(ESP)은 미들웨어로 작용하여 다양한 이벤트 기반 ETL의 처리를 간단하게 만듭니다.</p>
<p><img src="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_5.png" alt="이미지"></p>
<p>ESP 구성 요소</p>
<ul>
<li>이벤트 브로커: 이벤트를 수신하고 소비하는 핵심 구성 요소입니다.</li>
<li>이벤트 저장소: 받은 이벤트를 저장하여 대상이 비동기적으로 검색할 수 있도록 합니다.</li>
<li>분석 및 쿼리 엔진: 저장된 이벤트를 쿼리하고 분석합니다.</li>
</ul>
<div class="content-ad"></div>
<p>이벤트 브로커는 중요한 컴포넌트입니다. 이는 다음과 같은 구성 요소들을 포함합니다:</p>
<ul>
<li>Ingester: 다양한 소스에서 이벤트를 효율적으로 수신합니다.</li>
<li>Processor: 직렬화, 역직렬화, 압축, 압축 해제, 암호화 및 복호화와 같은 작업을 수행합니다.</li>
<li>Consumption: 저장소에서 이벤트를 검색하고 구독된 대상에게 분배합니다.</li>
</ul>
<p>인기 있는 이벤트 처리 시스템 솔루션:</p>
<ul>
<li>Apache Kafka</li>
<li>Amazon Kinesis</li>
<li>Apache Flink</li>
<li>Apache Spark</li>
<li>Apache Storm</li>
</ul>
<div class="content-ad"></div>
<p>아파치 카프카: 독특한 기능과 광범위한 응용 시나리오를 갖춘 가장 인기 있는 ESP 중 하나입니다. 카프카는 분산 클라이언트-서버 모델을 따릅니다.</p>
<ul>
<li>서버 측: 효율적인 협업을 위해 ZooKeeper가 관리하는 여러 브로커로 구성됩니다.</li>
<li>네트워크 통신: 클라이언트와 서버 간의 데이터 교환에 TCP를 활용합니다.</li>
<li>클라이언트 측: CLI, 자바, 스칼라, REST API 및 타사 옵션을 포함한 다양한 클라이언트를 제공합니다.</li>
</ul>
<p>카프카의 인기 이유는?</p>
<ul>
<li>확장성: 데이터를 여러 브로커에 분산하여 확장성과 고 처리량을 보장합니다.</li>
<li>높은 신뢰성: 안정성을 위해 여러 파티션과 복제를 사용합니다.</li>
<li>영구적인 지속성: 이벤트를 영구적으로 저장하여 소비자의 편의에 맞게 사용할 수 있습니다.</li>
<li>오픈 소스: 특정 요구 사항에 맞춰 사용자 정의가 가능하여 무료로 제공됩니다.</li>
</ul>
<div class="content-ad"></div>
<h1>카프카 아키텍처</h1>
<p>카프카 클러스터는 여러 브로커로 구성되어 있으며, 각 브로커는 이벤트를 수신, 저장, 처리 및 배포하는 역할을 합니다. ZooKeeper에 의해 조율되는 이러한 브로커들은 로그 또는 트랜잭션과 같은 특정 이벤트 유형을 저장하는 데이터베이스와 유사한 주제를 관리합니다.</p>
<p>파티셔닝과 복제: 카프카는 장애 허용성과 병렬 이벤트 처리를 위해 파티셔닝과 복제를 사용합니다. 일부 브로커가 실패하더라도, 카프카는 주제 파티션을 운영 중인 브로커에 분산시킴으로써 지속성을 보장합니다.</p>
<p>Kafka CLI를 사용한 주제 관리: 카프카 명령줄 인터페이스는 카프카 클러스터 내에서 주제를 생성, 나열, 설명 및 삭제하는 기능을 제공합니다. 명령에는 정의된 파티션 및 복제로 주제를 생성하고 주제 및 구성에 대한 자세한 정보를 얻는 등의 작업이 포함됩니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_6.png" alt="이미지"></p>
<p>카프카 프로듀서</p>
<p>카프카 프로듀서는 이벤트를 토픽 파티션에 발행하는 클라이언트 앱입니다. 이벤트는 선택적 파티셔닝을 위해 키와 연결될 수 있습니다. 프로듀서 CLI를 사용하면 프로듀서를 관리하고 지정된 토픽에 이벤트를 키와 함께 발행할 수 있습니다.</p>
<p>컨슈머로 이벤트 읽기: 컨슈머는 토픽에 가입하고 저장된 이벤트를 읽어 순차적으로 오프셋을 유지합니다. 오프셋을 재설정함으로써 컨슈머는 처음부터 이벤트를 다시 재생할 수 있습니다. 카프카 컨슈머와 프로듀서는 독립적으로 작동하여 동기화 없이 이벤트를 저장하고 소비할 수 있습니다.</p>
<div class="content-ad"></div>
<p>끝까지 이어지는 이벤트: 날씨 파이프라인</p>
<p>단계 1: 이벤트 소스 정의</p>
<p>극단적인 날씨에 대한 대중의 반응을 이해하기 위해 날씨와 트위터 이벤트 스트림을 분석하고 싶다고 상상해보세요. 두 가지 주요 이벤트 소스를 활용할 것입니다:</p>
<ul>
<li>IBM Weather API: JSON 형식의 실시간 및 예보 날씨 데이터를 제공합니다.</li>
<li>Twitter API: JSON 형식의 실시간 트윗 및 언급을 제공합니다.</li>
</ul>
<div class="content-ad"></div>
<p>단계 2: Kafka 토픽 구성</p>
<p>Kafka 클러스터에서 날씨 및 트위터 이벤트용 전용 토픽을 생성하여 데이터 흐름을 효율적으로 처리하기 위해 적절한 파티션 및 복제를 보장하세요.</p>
<p><img src="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_7.png" alt="이미지"></p>
<p>단계 3: 프로듀서 개발</p>
<div class="content-ad"></div>
<p>각 이벤트 소스에 대해 특정한 프로듀서를 개발하세요. 이들은 JSON 데이터를 바이트로 직렬화하고 해당 Kafka 토픽으로 게시할 것입니다.</p>
<p><img src="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_8.png" alt="이미지"></p>
<p>단계 4: 컨슈머 구현</p>
<p>날씨 및 Twitter 이벤트용 전용 컨슈머를 생성하세요. 이 컨슈머들은 Kafka 토픽에서 바이트를 역직렬화하여 JSON 데이터로 변환한 후 처리할 것입니다.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_9.png">
<p>단계 5: Persistence를 위한 DB Writer 통합</p>
<p>이벤트 데이터를 관계형 데이터베이스에 쓰고 싶다면 DB writer를 사용하십시오. 이 구성 요소는 컨슈머에서 JSON 파일을 구문 분석하고 해당 데이터베이스 레코드를 생성합니다.</p>
<pre><code class="hljs language-js">#db writer <span class="hljs-variable constant_">EXAMPLE</span>
<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> sqlite3

def <span class="hljs-title function_">write_to_database</span>(record):
    connection = sqlite3.<span class="hljs-title function_">connect</span>(<span class="hljs-string">"event_database.db"</span>)
    cursor = connection.<span class="hljs-title function_">cursor</span>()
    cursor.<span class="hljs-title function_">execute</span>(<span class="hljs-string">"INSERT INTO events VALUES (?)"</span>, (json.<span class="hljs-title function_">dumps</span>(record),))
    connection.<span class="hljs-title function_">commit</span>()
    connection.<span class="hljs-title function_">close</span>()

record = {<span class="hljs-string">"event_type"</span>: <span class="hljs-string">"weather"</span>, <span class="hljs-string">"data"</span>: {<span class="hljs-string">"temperature"</span>: <span class="hljs-number">25</span>, <span class="hljs-string">"location"</span>: <span class="hljs-string">"NYC"</span>}}
<span class="hljs-title function_">write_to_database</span>(record)
</code></pre>
<div class="content-ad"></div>
<p>6단계: SQL을 사용한 데이터베이스 상호작용</p>
<p>데이터베이스에 레코드를 쓰기 위해 SQL 삽입문을 사용하세요. 이 단계는 카프카 토픽에서 데이터를 영구 저장소 솔루션으로 전환하는 과정을 완료합니다.</p>
<pre><code class="hljs language-js">-- <span class="hljs-variable constant_">SQL</span> 삽입 예시
<span class="hljs-variable constant_">INSERT</span> <span class="hljs-variable constant_">INTO</span> events <span class="hljs-variable constant_">VALUES</span> (<span class="hljs-string">'{"event_type": "weather", "data": {"temperature": 25, "location": "NYC"}'</span>);
</code></pre>
<p>7단계: 시각화 및 분석</p>
<div class="content-ad"></div>
<p>마침내, 수집하고 저장된 이벤트 데이터로부터 통찰력 있는 시각화와 분석을 위해 데이터베이스 레코드를 쿼리하세요. 수집된 이벤트 데이터로부터 가치 있는 통찰력을 얻기 위해 대시보드를 사용하는 것이 가장 좋습니다.</p>
<p>이 end-to-end 파이프라인은 다양한 구성 요소의 원활한 통합을 보여주며, 이벤트 스트림을 관리하는 Kafka의 유연성과 강력함을 강조합니다.</p>
<p>참고: 본 블로그 게시물에서 제공된 메모 및 정보는 "ETL 및 쉘, Airflow 및 Kafka를 사용한 데이터 파이프라인" 과정 중에 편집되었으며 개인적인 용도를 위한 기본 개요를 제공하기 위한 것입니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"특별한 알고리즘없이도 Python의 기초를 확인할 수 있는 코딩 문제들을 모았습니다 어려울 수 있는 개념을 단계적으로 익힐 수 있도록 구성되어 있어요 ","description":"","date":"2024-06-19 09:48","slug":"2024-06-19-DataEngineeringpipelineleveragingAirflowKafka","content":"\n\n# 소개\n\n이 문서에서는 Apache Airflow 및 Kafka(오픈 소스)를 활용하여 실시간 날씨 업데이트를 읽고 Twitter의 이벤트 스트림을 분석하여 대중의 반응을 이해하는 데이터 엔지니어링 파이프라인 아키텍처를 솔루션 디자인하는 방법에 대한 기본적인 개요를 제공합니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png)\n\n# ETL 기본 원칙\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nETL은 데이터 웨어하우스 또는 데이터 마트와 같은 분석 환경을 위해 데이터 획득 및 준비를 자동화하는 데이터 파이프라인 엔지니어링 방법론인 추출, 변환, 로드를 의미합니다. 이는 다양한 소스에서 데이터를 수집하고 표준 형식으로 편집한 다음 시각화, 탐색 및 모델링을 위한 새로운 환경으로 로드하는 것을 포함하며, 자동화 및 의사 결정을 지원합니다.\n\n# ELT 기본\n\nELT 프로세스는 추출, 로드 및 변환을 나타내며, 단곅하계의 단계 순서로 인한 독특한 차이로 인해 ETL과 다릅니다. ELT에서는 데이터가 데이터 레이크와 같은 목적지 환경으로 원본 형식 그대로 직접 로드됩니다. 이를 통해 목적지 플랫폼 내에서 필요에 따라 변환을 수행하고 동적으로 사용자 주도적 변경을 가능하게 합니다.\n\n# ETL과 ELT 비교\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nETL과 ELT의 차이점: ETL 파이프라인에서는 변환 작업이 목적지에 도달하기 전에 데이터 파이프라인 내에서 발생하는 반면, ELT는 변환 작업을 분리하여 목적지 환경에서 필요한 대로 수행할 수 있습니다. ETL은 엄격하고 목적이 명확하지만, ELT는 유연하며 빅 데이터 처리에 대한 셀프 서비스 분석을 제공합니다.\n\n# 데이터 수집 기술\n\n다양한 데이터 수집 기술에는 다음이 포함됩니다:\n\n- 완전 수집 대 부분 수집\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 전체 로딩: 데이터베이스에 초기 히스토리를 로드합니다. 추적 데이터는 새 창고에서 시작됩니다.\n- 점진적 로딩: 새 데이터를 삽입하거나 이미 로드된 데이터를 업데이트합니다. 거래 히스토리를 누적하는 데 사용됩니다. 데이터 양과 속도에 따라 일괄 또는 스트림 로드될 수 있습니다.\n\n정기 로딩 vs. 요청 로딩:\n\n- 정기 로딩: 매일 거래를 데이터베이스에 주기적으로 로드하며, 스크립트 작업에 의해 자동화됩니다.\n- 요청 로딩: 소스 데이터가 지정된 크기에 도달하거나 움직임, 소리 또는 임의의 변경 이벤트와 같은 다양한 이벤트에 의해 트리거됩니다.\n\n일괄 처리 vs. 스트림 로딩:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 배치 로딩: 시간별로 정의된 단위로 데이터를 로드하며 일반적으로 몇 시간에서 며칠 동안 누적됩니다.\n- 스트림 로딩: 데이터가 제공되는 즉시 실시간으로 로드됩니다.\n- 마이크로 배치 로딩: 즉시 처리를 위한 최근 데이터에 액세스합니다.\n\n푸시 대 수신 데이터 로딩:\n\n- 수신 방법: 클라이언트가 서버로부터 데이터를 요청합니다(예: RSS 피드, 이메일).\n- 푸시 방법: 클라이언트가 서버 서비스에 구독하여 데이터를 실시간으로 전달 받습니다(예: 푸시 알림, 즉각 메시징 서비스).\n\n# 데이터 파이프라인이란 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 파이프라인은 데이터의 이동 또는 수정에 특히 관련이 있어요. 이러한 파이프라인은 데이터를 한 곳이나 형식에서 다른 곳이나 형식으로 운송하는 것을 목표로 하며, 데이터를 추출하고 최종적으로 적재하기 위해 선택적 변환 단계를 통해 안내하는 시스템을 구성합니다.\n\n파이프라인을 통해 흐르는 데이터를 시각화하는 것은 데이터 패킷으로 표현할 수 있으며, 이는 데이터의 단위를 넓게 이야기합니다. 이러한 패킷은 단일 레코드나 이벤트에서 대량 데이터 수집물까지 다양할 수 있어요. 이 맥락에서 데이터 패킷은 파이프라인으로 흡수되기 위해 대기열에 정리되며, 데이터 파이프라인의 길이는 단일 패킷이 횡단하는 데 걸리는 시간을 의미합니다. 패킷 간의 화살표는 처리량 지연이나 연속 패킷 도착 사이의 시간을 나타냅니다.\n\n데이터 파이프라인 주요 성능 지표\n\n- 지연 시간: 데이터 패킷이 파이프라인을 통과하는 총 시간을 의미합니다. 지연 시간은 파이프라인 내 각 처리 단계에서 소요된 개별 시간의 합으로, 파이프라인 내 가장 느린 프로세스에 의해 제한됩니다. 예를 들어, 웹 페이지의 로딩 시간은 서버 속도에 따라 결정되며 인터넷 서비스 속도와 관계없이 서버 속도에 의해 제어됩니다.\n- 처리량: 이는 시간 단위당 파이프라인을 통해 처리될 수 있는 데이터 양을 의미합니다. 처리량을 증가시키는 것은 시간 단위당 더 많은 패킷을 처리하고 큰 상자를 차례차례 통과시키는 우리 친구 사슬 예시와 유사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 파이프라인 응용:\n\n- 간단한 복사 파이프라인: 파일 백업과 같이 데이터를 한 위치에서 다른 위치로 복사하는 작업을 포함합니다.\n- 데이터 레이크 통합: 분산된 래 데이터 소스를 데이터 레이크에 통합하는 작업입니다.\n- 거래 기록 이동: 거래 기록을 데이터 웨어하우스로 전송하는 작업입니다.\n- IoT 데이터 스트리밍: IoT 장치에서 데이터를 스트리밍하여 대시보드나 경보 시스템에서 정보를 제공하는 것을 말합니다.\n- 기계 학습용 데이터 준비: 기계 학습의 개발이나 제품화를 위해 래 데이터를 준비하는 작업입니다.\n- 메시지 보내기 및 받기: 이메일, SMS 또는 온라인 비디오 회의와 같은 애플리케이션을 포함합니다.\n\n# 주요 데이터 파이프라인 프로세스\n\n데이터 파이프라인 프로세스는 일반적으로 구조화된 일련의 단계를 따릅니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 추출: 하나 이상의 소스에서 데이터를 검색하는 과정입니다.\n- 투입: 추출된 데이터는 후속 처리를 위해 파이프라인에 투입됩니다.\n- 변환: 파이프라인 내의 선택적인 단계에서 데이터를 변환할 수 있습니다.\n- 로딩: 최종 단계는 변환된 데이터를 대상 시설로 로드합니다.\n- 스케줄링/트리거링: 작업을 필요에 따라 예약하거나 트리거하는 메커니즘입니다.\n- 모니터링: 전체 워크플로우가 효율적으로 작동하도록 모니터링됩니다.\n- 유지보수 및 최적화: 원활한 파이프라인 운영을 보장하기 위해 정기적인 유지보수 및 최적화 작업이 수행됩니다.\n\n# Apache Airflow\n\n- Python 기반의 오픈 소스 \"구성과 코드\" 플랫폼입니다. AirBNB에서 오픈 소스로 공개되었습니다.\n- 데이터 파이프라인 워크플로우를 작성, 예약 및 모니터링할 수 있습니다.\n- 확장 가능하며 병렬 컴퓨팅 노드를 지원하며 주요 클라우드 플랫폼과 통합됩니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 스케줄러: 예약된 워크플로우를 트리거합니다.\n- 실행기: 작업을 Worker에 할당하여 실행합니다.\n- 웹 서버: DAG 검사, 트리거 및 디버깅을 위한 대화형 UI를 호스팅합니다.\n- DAG 디렉토리: 스케줄러, 실행기 및 Worker가 액세스할 수 있는 DAG 파일을 저장합니다.\n- 메타데이터 데이터베이스: 각 DAG 및 해당 작업의 상태를 유지합니다.\n\nDAG 및 작업 라이프사이클\n\nDAG(유향 비순환 그래프)는 작업 간의 종속성과 실행 순서를 지정합니다. 'DAG'는 순환이나 사이클이 없는 관계를 나타내는 특정 유형의 그래프로, 노드와 간선으로 구성되며 방향성을 가진 간선이 노드 간의 흐름을 보여줍니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n작업 상태:\n\n- 상태 없음: 실행을 위해 대기 중인 작업.\n- 스케줄됨: 의존성에 따라 실행이 예약된 작업.\n- 제거됨: 실행이 시작된 이후에 사라진 작업.\n- 상류 작업 실패: 상류 작업에서 실패 발생.\n- 대기 중: 워커 가용성을 기다리는 작업.\n- 실행 중: 워커에 의해 실행 중인 작업.\n- 성공: 오류 없이 작업이 완료된 상태.\n- 실패: 실행 중에 오류가 발생한 작업.\n- 재시도 예정: 남은 재시도 횟수가 남아 있는 실패한 작업으로, 다시 예약됨.\n- 이상적인 작업 흐름: '상태 없음'에서 '스케줄됨'으로, '대기 중'으로, '실행 중'으로 이어져 '성공'으로 마무리됨.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_3.png)\n\nAirflow DAG 스크립트의 논리 블록\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 라이브러리 가져오기: 필요한 Python 라이브러리 가져오기.\n- DAG 인수: DAG에 대한 기본 인수(시작 날짜와 같은 것) 정의.\n- DAG 정의: 특정 속성을 사용하여 DAG 인스턴스화.\n- 작업 정의: DAG 내부의 개별 작업(노드) 정의.\n- 작업 파이프라인: 작업 간의 의존성을 지정하여 작업 간의 흐름을 구축.\n\n이러한 논리적 블록이 포함된 Python 스크립트 예제를 살펴보세요.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_4.png)\n\n# Kafka를 활용한 스트리밍 파이프라인 구축\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이벤트는 이벤트 스트리밍의 맥락에서 엔티티의 관찰 가능한 상태 업데이트를 설명하는 데이터를 의미합니다. 예시로는 자동차의 GPS 좌표, 방 온도, 또는 응용 프로그램의 RAM 사용량 등이 있습니다.\n\n이벤트는 다양한 형식으로 제공됩니다:\n\n- 텍스트, 숫자 또는 날짜와 같은 원시 유형\n- 값이 원시 또는 복합 유형인 키-값 쌍 형식(e.g., JSON, XML)\n- 타임스탬프가 포함된 시간 감도를 위한 키-값 형식\n\n한 소스에서 한 대상으로: 이벤트 스트리밍은 소스(센서, 데이터베이스, 응용 프로그램)가 실시간 이벤트를 지속적으로 생성하고 이를 대상지(파일 시스템, 데이터베이스, 응용 프로그램)로 전달하는 것을 의미합니다. 이 과정은 이벤트 스트리밍이라고 불립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n많은 출처에서 많은 대상으로: 다양한 통신 프로토콜(FTP, HTTP, JDBC, SCP)을 사용하는 여러 분산 이벤트 소스 및 대상을 관리하는 것은 도전이 될 수 있습니다. 이벤트 스트림 플랫폼(ESP)은 미들웨어로 작용하여 다양한 이벤트 기반 ETL의 처리를 간단하게 만듭니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_5.png)\n\nESP 구성 요소\n\n- 이벤트 브로커: 이벤트를 수신하고 소비하는 핵심 구성 요소입니다.\n- 이벤트 저장소: 받은 이벤트를 저장하여 대상이 비동기적으로 검색할 수 있도록 합니다.\n- 분석 및 쿼리 엔진: 저장된 이벤트를 쿼리하고 분석합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이벤트 브로커는 중요한 컴포넌트입니다. 이는 다음과 같은 구성 요소들을 포함합니다:\n\n- Ingester: 다양한 소스에서 이벤트를 효율적으로 수신합니다.\n- Processor: 직렬화, 역직렬화, 압축, 압축 해제, 암호화 및 복호화와 같은 작업을 수행합니다.\n- Consumption: 저장소에서 이벤트를 검색하고 구독된 대상에게 분배합니다.\n\n인기 있는 이벤트 처리 시스템 솔루션:\n\n- Apache Kafka\n- Amazon Kinesis\n- Apache Flink\n- Apache Spark\n- Apache Storm\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아파치 카프카: 독특한 기능과 광범위한 응용 시나리오를 갖춘 가장 인기 있는 ESP 중 하나입니다. 카프카는 분산 클라이언트-서버 모델을 따릅니다.\n\n- 서버 측: 효율적인 협업을 위해 ZooKeeper가 관리하는 여러 브로커로 구성됩니다.\n- 네트워크 통신: 클라이언트와 서버 간의 데이터 교환에 TCP를 활용합니다.\n- 클라이언트 측: CLI, 자바, 스칼라, REST API 및 타사 옵션을 포함한 다양한 클라이언트를 제공합니다.\n\n카프카의 인기 이유는?\n\n- 확장성: 데이터를 여러 브로커에 분산하여 확장성과 고 처리량을 보장합니다.\n- 높은 신뢰성: 안정성을 위해 여러 파티션과 복제를 사용합니다.\n- 영구적인 지속성: 이벤트를 영구적으로 저장하여 소비자의 편의에 맞게 사용할 수 있습니다.\n- 오픈 소스: 특정 요구 사항에 맞춰 사용자 정의가 가능하여 무료로 제공됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 카프카 아키텍처\n\n카프카 클러스터는 여러 브로커로 구성되어 있으며, 각 브로커는 이벤트를 수신, 저장, 처리 및 배포하는 역할을 합니다. ZooKeeper에 의해 조율되는 이러한 브로커들은 로그 또는 트랜잭션과 같은 특정 이벤트 유형을 저장하는 데이터베이스와 유사한 주제를 관리합니다.\n\n파티셔닝과 복제: 카프카는 장애 허용성과 병렬 이벤트 처리를 위해 파티셔닝과 복제를 사용합니다. 일부 브로커가 실패하더라도, 카프카는 주제 파티션을 운영 중인 브로커에 분산시킴으로써 지속성을 보장합니다.\n\nKafka CLI를 사용한 주제 관리: 카프카 명령줄 인터페이스는 카프카 클러스터 내에서 주제를 생성, 나열, 설명 및 삭제하는 기능을 제공합니다. 명령에는 정의된 파티션 및 복제로 주제를 생성하고 주제 및 구성에 대한 자세한 정보를 얻는 등의 작업이 포함됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_6.png)\n\n카프카 프로듀서\n\n카프카 프로듀서는 이벤트를 토픽 파티션에 발행하는 클라이언트 앱입니다. 이벤트는 선택적 파티셔닝을 위해 키와 연결될 수 있습니다. 프로듀서 CLI를 사용하면 프로듀서를 관리하고 지정된 토픽에 이벤트를 키와 함께 발행할 수 있습니다.\n\n컨슈머로 이벤트 읽기: 컨슈머는 토픽에 가입하고 저장된 이벤트를 읽어 순차적으로 오프셋을 유지합니다. 오프셋을 재설정함으로써 컨슈머는 처음부터 이벤트를 다시 재생할 수 있습니다. 카프카 컨슈머와 프로듀서는 독립적으로 작동하여 동기화 없이 이벤트를 저장하고 소비할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n끝까지 이어지는 이벤트: 날씨 파이프라인\n\n단계 1: 이벤트 소스 정의\n\n극단적인 날씨에 대한 대중의 반응을 이해하기 위해 날씨와 트위터 이벤트 스트림을 분석하고 싶다고 상상해보세요. 두 가지 주요 이벤트 소스를 활용할 것입니다:\n\n- IBM Weather API: JSON 형식의 실시간 및 예보 날씨 데이터를 제공합니다.\n- Twitter API: JSON 형식의 실시간 트윗 및 언급을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계 2: Kafka 토픽 구성\n\nKafka 클러스터에서 날씨 및 트위터 이벤트용 전용 토픽을 생성하여 데이터 흐름을 효율적으로 처리하기 위해 적절한 파티션 및 복제를 보장하세요.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_7.png)\n\n단계 3: 프로듀서 개발\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 이벤트 소스에 대해 특정한 프로듀서를 개발하세요. 이들은 JSON 데이터를 바이트로 직렬화하고 해당 Kafka 토픽으로 게시할 것입니다.\n\n![이미지](/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_8.png)\n\n단계 4: 컨슈머 구현\n\n날씨 및 Twitter 이벤트용 전용 컨슈머를 생성하세요. 이 컨슈머들은 Kafka 토픽에서 바이트를 역직렬화하여 JSON 데이터로 변환한 후 처리할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_9.png\" /\u003e\n\n단계 5: Persistence를 위한 DB Writer 통합\n\n이벤트 데이터를 관계형 데이터베이스에 쓰고 싶다면 DB writer를 사용하십시오. 이 구성 요소는 컨슈머에서 JSON 파일을 구문 분석하고 해당 데이터베이스 레코드를 생성합니다.\n\n```js\n#db writer EXAMPLE\nimport json\nimport sqlite3\n\ndef write_to_database(record):\n    connection = sqlite3.connect(\"event_database.db\")\n    cursor = connection.cursor()\n    cursor.execute(\"INSERT INTO events VALUES (?)\", (json.dumps(record),))\n    connection.commit()\n    connection.close()\n\nrecord = {\"event_type\": \"weather\", \"data\": {\"temperature\": 25, \"location\": \"NYC\"}}\nwrite_to_database(record)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n6단계: SQL을 사용한 데이터베이스 상호작용\n\n데이터베이스에 레코드를 쓰기 위해 SQL 삽입문을 사용하세요. 이 단계는 카프카 토픽에서 데이터를 영구 저장소 솔루션으로 전환하는 과정을 완료합니다.\n\n```js\n-- SQL 삽입 예시\nINSERT INTO events VALUES ('{\"event_type\": \"weather\", \"data\": {\"temperature\": 25, \"location\": \"NYC\"}');\n```\n\n7단계: 시각화 및 분석\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마침내, 수집하고 저장된 이벤트 데이터로부터 통찰력 있는 시각화와 분석을 위해 데이터베이스 레코드를 쿼리하세요. 수집된 이벤트 데이터로부터 가치 있는 통찰력을 얻기 위해 대시보드를 사용하는 것이 가장 좋습니다.\n\n이 end-to-end 파이프라인은 다양한 구성 요소의 원활한 통합을 보여주며, 이벤트 스트림을 관리하는 Kafka의 유연성과 강력함을 강조합니다.\n\n참고: 본 블로그 게시물에서 제공된 메모 및 정보는 \"ETL 및 쉘, Airflow 및 Kafka를 사용한 데이터 파이프라인\" 과정 중에 편집되었으며 개인적인 용도를 위한 기본 개요를 제공하기 위한 것입니다.","ogImage":{"url":"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png"},"coverImage":"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png","tag":["Tech"],"readingTime":10},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch1\u003e소개\u003c/h1\u003e\n\u003cp\u003e이 문서에서는 Apache Airflow 및 Kafka(오픈 소스)를 활용하여 실시간 날씨 업데이트를 읽고 Twitter의 이벤트 스트림을 분석하여 대중의 반응을 이해하는 데이터 엔지니어링 파이프라인 아키텍처를 솔루션 디자인하는 방법에 대한 기본적인 개요를 제공합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch1\u003eETL 기본 원칙\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eETL은 데이터 웨어하우스 또는 데이터 마트와 같은 분석 환경을 위해 데이터 획득 및 준비를 자동화하는 데이터 파이프라인 엔지니어링 방법론인 추출, 변환, 로드를 의미합니다. 이는 다양한 소스에서 데이터를 수집하고 표준 형식으로 편집한 다음 시각화, 탐색 및 모델링을 위한 새로운 환경으로 로드하는 것을 포함하며, 자동화 및 의사 결정을 지원합니다.\u003c/p\u003e\n\u003ch1\u003eELT 기본\u003c/h1\u003e\n\u003cp\u003eELT 프로세스는 추출, 로드 및 변환을 나타내며, 단곅하계의 단계 순서로 인한 독특한 차이로 인해 ETL과 다릅니다. ELT에서는 데이터가 데이터 레이크와 같은 목적지 환경으로 원본 형식 그대로 직접 로드됩니다. 이를 통해 목적지 플랫폼 내에서 필요에 따라 변환을 수행하고 동적으로 사용자 주도적 변경을 가능하게 합니다.\u003c/p\u003e\n\u003ch1\u003eETL과 ELT 비교\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eETL과 ELT의 차이점: ETL 파이프라인에서는 변환 작업이 목적지에 도달하기 전에 데이터 파이프라인 내에서 발생하는 반면, ELT는 변환 작업을 분리하여 목적지 환경에서 필요한 대로 수행할 수 있습니다. ETL은 엄격하고 목적이 명확하지만, ELT는 유연하며 빅 데이터 처리에 대한 셀프 서비스 분석을 제공합니다.\u003c/p\u003e\n\u003ch1\u003e데이터 수집 기술\u003c/h1\u003e\n\u003cp\u003e다양한 데이터 수집 기술에는 다음이 포함됩니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e완전 수집 대 부분 수집\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e전체 로딩: 데이터베이스에 초기 히스토리를 로드합니다. 추적 데이터는 새 창고에서 시작됩니다.\u003c/li\u003e\n\u003cli\u003e점진적 로딩: 새 데이터를 삽입하거나 이미 로드된 데이터를 업데이트합니다. 거래 히스토리를 누적하는 데 사용됩니다. 데이터 양과 속도에 따라 일괄 또는 스트림 로드될 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e정기 로딩 vs. 요청 로딩:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e정기 로딩: 매일 거래를 데이터베이스에 주기적으로 로드하며, 스크립트 작업에 의해 자동화됩니다.\u003c/li\u003e\n\u003cli\u003e요청 로딩: 소스 데이터가 지정된 크기에 도달하거나 움직임, 소리 또는 임의의 변경 이벤트와 같은 다양한 이벤트에 의해 트리거됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e일괄 처리 vs. 스트림 로딩:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e배치 로딩: 시간별로 정의된 단위로 데이터를 로드하며 일반적으로 몇 시간에서 며칠 동안 누적됩니다.\u003c/li\u003e\n\u003cli\u003e스트림 로딩: 데이터가 제공되는 즉시 실시간으로 로드됩니다.\u003c/li\u003e\n\u003cli\u003e마이크로 배치 로딩: 즉시 처리를 위한 최근 데이터에 액세스합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e푸시 대 수신 데이터 로딩:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e수신 방법: 클라이언트가 서버로부터 데이터를 요청합니다(예: RSS 피드, 이메일).\u003c/li\u003e\n\u003cli\u003e푸시 방법: 클라이언트가 서버 서비스에 구독하여 데이터를 실시간으로 전달 받습니다(예: 푸시 알림, 즉각 메시징 서비스).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e데이터 파이프라인이란 무엇인가요?\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e데이터 파이프라인은 데이터의 이동 또는 수정에 특히 관련이 있어요. 이러한 파이프라인은 데이터를 한 곳이나 형식에서 다른 곳이나 형식으로 운송하는 것을 목표로 하며, 데이터를 추출하고 최종적으로 적재하기 위해 선택적 변환 단계를 통해 안내하는 시스템을 구성합니다.\u003c/p\u003e\n\u003cp\u003e파이프라인을 통해 흐르는 데이터를 시각화하는 것은 데이터 패킷으로 표현할 수 있으며, 이는 데이터의 단위를 넓게 이야기합니다. 이러한 패킷은 단일 레코드나 이벤트에서 대량 데이터 수집물까지 다양할 수 있어요. 이 맥락에서 데이터 패킷은 파이프라인으로 흡수되기 위해 대기열에 정리되며, 데이터 파이프라인의 길이는 단일 패킷이 횡단하는 데 걸리는 시간을 의미합니다. 패킷 간의 화살표는 처리량 지연이나 연속 패킷 도착 사이의 시간을 나타냅니다.\u003c/p\u003e\n\u003cp\u003e데이터 파이프라인 주요 성능 지표\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e지연 시간: 데이터 패킷이 파이프라인을 통과하는 총 시간을 의미합니다. 지연 시간은 파이프라인 내 각 처리 단계에서 소요된 개별 시간의 합으로, 파이프라인 내 가장 느린 프로세스에 의해 제한됩니다. 예를 들어, 웹 페이지의 로딩 시간은 서버 속도에 따라 결정되며 인터넷 서비스 속도와 관계없이 서버 속도에 의해 제어됩니다.\u003c/li\u003e\n\u003cli\u003e처리량: 이는 시간 단위당 파이프라인을 통해 처리될 수 있는 데이터 양을 의미합니다. 처리량을 증가시키는 것은 시간 단위당 더 많은 패킷을 처리하고 큰 상자를 차례차례 통과시키는 우리 친구 사슬 예시와 유사합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e데이터 파이프라인 응용:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e간단한 복사 파이프라인: 파일 백업과 같이 데이터를 한 위치에서 다른 위치로 복사하는 작업을 포함합니다.\u003c/li\u003e\n\u003cli\u003e데이터 레이크 통합: 분산된 래 데이터 소스를 데이터 레이크에 통합하는 작업입니다.\u003c/li\u003e\n\u003cli\u003e거래 기록 이동: 거래 기록을 데이터 웨어하우스로 전송하는 작업입니다.\u003c/li\u003e\n\u003cli\u003eIoT 데이터 스트리밍: IoT 장치에서 데이터를 스트리밍하여 대시보드나 경보 시스템에서 정보를 제공하는 것을 말합니다.\u003c/li\u003e\n\u003cli\u003e기계 학습용 데이터 준비: 기계 학습의 개발이나 제품화를 위해 래 데이터를 준비하는 작업입니다.\u003c/li\u003e\n\u003cli\u003e메시지 보내기 및 받기: 이메일, SMS 또는 온라인 비디오 회의와 같은 애플리케이션을 포함합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e주요 데이터 파이프라인 프로세스\u003c/h1\u003e\n\u003cp\u003e데이터 파이프라인 프로세스는 일반적으로 구조화된 일련의 단계를 따릅니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e추출: 하나 이상의 소스에서 데이터를 검색하는 과정입니다.\u003c/li\u003e\n\u003cli\u003e투입: 추출된 데이터는 후속 처리를 위해 파이프라인에 투입됩니다.\u003c/li\u003e\n\u003cli\u003e변환: 파이프라인 내의 선택적인 단계에서 데이터를 변환할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e로딩: 최종 단계는 변환된 데이터를 대상 시설로 로드합니다.\u003c/li\u003e\n\u003cli\u003e스케줄링/트리거링: 작업을 필요에 따라 예약하거나 트리거하는 메커니즘입니다.\u003c/li\u003e\n\u003cli\u003e모니터링: 전체 워크플로우가 효율적으로 작동하도록 모니터링됩니다.\u003c/li\u003e\n\u003cli\u003e유지보수 및 최적화: 원활한 파이프라인 운영을 보장하기 위해 정기적인 유지보수 및 최적화 작업이 수행됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eApache Airflow\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003ePython 기반의 오픈 소스 \"구성과 코드\" 플랫폼입니다. AirBNB에서 오픈 소스로 공개되었습니다.\u003c/li\u003e\n\u003cli\u003e데이터 파이프라인 워크플로우를 작성, 예약 및 모니터링할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e확장 가능하며 병렬 컴퓨팅 노드를 지원하며 주요 클라우드 플랫폼과 통합됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e스케줄러: 예약된 워크플로우를 트리거합니다.\u003c/li\u003e\n\u003cli\u003e실행기: 작업을 Worker에 할당하여 실행합니다.\u003c/li\u003e\n\u003cli\u003e웹 서버: DAG 검사, 트리거 및 디버깅을 위한 대화형 UI를 호스팅합니다.\u003c/li\u003e\n\u003cli\u003eDAG 디렉토리: 스케줄러, 실행기 및 Worker가 액세스할 수 있는 DAG 파일을 저장합니다.\u003c/li\u003e\n\u003cli\u003e메타데이터 데이터베이스: 각 DAG 및 해당 작업의 상태를 유지합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDAG 및 작업 라이프사이클\u003c/p\u003e\n\u003cp\u003eDAG(유향 비순환 그래프)는 작업 간의 종속성과 실행 순서를 지정합니다. 'DAG'는 순환이나 사이클이 없는 관계를 나타내는 특정 유형의 그래프로, 노드와 간선으로 구성되며 방향성을 가진 간선이 노드 간의 흐름을 보여줍니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e작업 상태:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e상태 없음: 실행을 위해 대기 중인 작업.\u003c/li\u003e\n\u003cli\u003e스케줄됨: 의존성에 따라 실행이 예약된 작업.\u003c/li\u003e\n\u003cli\u003e제거됨: 실행이 시작된 이후에 사라진 작업.\u003c/li\u003e\n\u003cli\u003e상류 작업 실패: 상류 작업에서 실패 발생.\u003c/li\u003e\n\u003cli\u003e대기 중: 워커 가용성을 기다리는 작업.\u003c/li\u003e\n\u003cli\u003e실행 중: 워커에 의해 실행 중인 작업.\u003c/li\u003e\n\u003cli\u003e성공: 오류 없이 작업이 완료된 상태.\u003c/li\u003e\n\u003cli\u003e실패: 실행 중에 오류가 발생한 작업.\u003c/li\u003e\n\u003cli\u003e재시도 예정: 남은 재시도 횟수가 남아 있는 실패한 작업으로, 다시 예약됨.\u003c/li\u003e\n\u003cli\u003e이상적인 작업 흐름: '상태 없음'에서 '스케줄됨'으로, '대기 중'으로, '실행 중'으로 이어져 '성공'으로 마무리됨.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_3.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eAirflow DAG 스크립트의 논리 블록\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e라이브러리 가져오기: 필요한 Python 라이브러리 가져오기.\u003c/li\u003e\n\u003cli\u003eDAG 인수: DAG에 대한 기본 인수(시작 날짜와 같은 것) 정의.\u003c/li\u003e\n\u003cli\u003eDAG 정의: 특정 속성을 사용하여 DAG 인스턴스화.\u003c/li\u003e\n\u003cli\u003e작업 정의: DAG 내부의 개별 작업(노드) 정의.\u003c/li\u003e\n\u003cli\u003e작업 파이프라인: 작업 간의 의존성을 지정하여 작업 간의 흐름을 구축.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이러한 논리적 블록이 포함된 Python 스크립트 예제를 살펴보세요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_4.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch1\u003eKafka를 활용한 스트리밍 파이프라인 구축\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이벤트는 이벤트 스트리밍의 맥락에서 엔티티의 관찰 가능한 상태 업데이트를 설명하는 데이터를 의미합니다. 예시로는 자동차의 GPS 좌표, 방 온도, 또는 응용 프로그램의 RAM 사용량 등이 있습니다.\u003c/p\u003e\n\u003cp\u003e이벤트는 다양한 형식으로 제공됩니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e텍스트, 숫자 또는 날짜와 같은 원시 유형\u003c/li\u003e\n\u003cli\u003e값이 원시 또는 복합 유형인 키-값 쌍 형식(e.g., JSON, XML)\u003c/li\u003e\n\u003cli\u003e타임스탬프가 포함된 시간 감도를 위한 키-값 형식\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e한 소스에서 한 대상으로: 이벤트 스트리밍은 소스(센서, 데이터베이스, 응용 프로그램)가 실시간 이벤트를 지속적으로 생성하고 이를 대상지(파일 시스템, 데이터베이스, 응용 프로그램)로 전달하는 것을 의미합니다. 이 과정은 이벤트 스트리밍이라고 불립니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e많은 출처에서 많은 대상으로: 다양한 통신 프로토콜(FTP, HTTP, JDBC, SCP)을 사용하는 여러 분산 이벤트 소스 및 대상을 관리하는 것은 도전이 될 수 있습니다. 이벤트 스트림 플랫폼(ESP)은 미들웨어로 작용하여 다양한 이벤트 기반 ETL의 처리를 간단하게 만듭니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_5.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eESP 구성 요소\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e이벤트 브로커: 이벤트를 수신하고 소비하는 핵심 구성 요소입니다.\u003c/li\u003e\n\u003cli\u003e이벤트 저장소: 받은 이벤트를 저장하여 대상이 비동기적으로 검색할 수 있도록 합니다.\u003c/li\u003e\n\u003cli\u003e분석 및 쿼리 엔진: 저장된 이벤트를 쿼리하고 분석합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이벤트 브로커는 중요한 컴포넌트입니다. 이는 다음과 같은 구성 요소들을 포함합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIngester: 다양한 소스에서 이벤트를 효율적으로 수신합니다.\u003c/li\u003e\n\u003cli\u003eProcessor: 직렬화, 역직렬화, 압축, 압축 해제, 암호화 및 복호화와 같은 작업을 수행합니다.\u003c/li\u003e\n\u003cli\u003eConsumption: 저장소에서 이벤트를 검색하고 구독된 대상에게 분배합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e인기 있는 이벤트 처리 시스템 솔루션:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eApache Kafka\u003c/li\u003e\n\u003cli\u003eAmazon Kinesis\u003c/li\u003e\n\u003cli\u003eApache Flink\u003c/li\u003e\n\u003cli\u003eApache Spark\u003c/li\u003e\n\u003cli\u003eApache Storm\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아파치 카프카: 독특한 기능과 광범위한 응용 시나리오를 갖춘 가장 인기 있는 ESP 중 하나입니다. 카프카는 분산 클라이언트-서버 모델을 따릅니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e서버 측: 효율적인 협업을 위해 ZooKeeper가 관리하는 여러 브로커로 구성됩니다.\u003c/li\u003e\n\u003cli\u003e네트워크 통신: 클라이언트와 서버 간의 데이터 교환에 TCP를 활용합니다.\u003c/li\u003e\n\u003cli\u003e클라이언트 측: CLI, 자바, 스칼라, REST API 및 타사 옵션을 포함한 다양한 클라이언트를 제공합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e카프카의 인기 이유는?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e확장성: 데이터를 여러 브로커에 분산하여 확장성과 고 처리량을 보장합니다.\u003c/li\u003e\n\u003cli\u003e높은 신뢰성: 안정성을 위해 여러 파티션과 복제를 사용합니다.\u003c/li\u003e\n\u003cli\u003e영구적인 지속성: 이벤트를 영구적으로 저장하여 소비자의 편의에 맞게 사용할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e오픈 소스: 특정 요구 사항에 맞춰 사용자 정의가 가능하여 무료로 제공됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e카프카 아키텍처\u003c/h1\u003e\n\u003cp\u003e카프카 클러스터는 여러 브로커로 구성되어 있으며, 각 브로커는 이벤트를 수신, 저장, 처리 및 배포하는 역할을 합니다. ZooKeeper에 의해 조율되는 이러한 브로커들은 로그 또는 트랜잭션과 같은 특정 이벤트 유형을 저장하는 데이터베이스와 유사한 주제를 관리합니다.\u003c/p\u003e\n\u003cp\u003e파티셔닝과 복제: 카프카는 장애 허용성과 병렬 이벤트 처리를 위해 파티셔닝과 복제를 사용합니다. 일부 브로커가 실패하더라도, 카프카는 주제 파티션을 운영 중인 브로커에 분산시킴으로써 지속성을 보장합니다.\u003c/p\u003e\n\u003cp\u003eKafka CLI를 사용한 주제 관리: 카프카 명령줄 인터페이스는 카프카 클러스터 내에서 주제를 생성, 나열, 설명 및 삭제하는 기능을 제공합니다. 명령에는 정의된 파티션 및 복제로 주제를 생성하고 주제 및 구성에 대한 자세한 정보를 얻는 등의 작업이 포함됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_6.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e카프카 프로듀서\u003c/p\u003e\n\u003cp\u003e카프카 프로듀서는 이벤트를 토픽 파티션에 발행하는 클라이언트 앱입니다. 이벤트는 선택적 파티셔닝을 위해 키와 연결될 수 있습니다. 프로듀서 CLI를 사용하면 프로듀서를 관리하고 지정된 토픽에 이벤트를 키와 함께 발행할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e컨슈머로 이벤트 읽기: 컨슈머는 토픽에 가입하고 저장된 이벤트를 읽어 순차적으로 오프셋을 유지합니다. 오프셋을 재설정함으로써 컨슈머는 처음부터 이벤트를 다시 재생할 수 있습니다. 카프카 컨슈머와 프로듀서는 독립적으로 작동하여 동기화 없이 이벤트를 저장하고 소비할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e끝까지 이어지는 이벤트: 날씨 파이프라인\u003c/p\u003e\n\u003cp\u003e단계 1: 이벤트 소스 정의\u003c/p\u003e\n\u003cp\u003e극단적인 날씨에 대한 대중의 반응을 이해하기 위해 날씨와 트위터 이벤트 스트림을 분석하고 싶다고 상상해보세요. 두 가지 주요 이벤트 소스를 활용할 것입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIBM Weather API: JSON 형식의 실시간 및 예보 날씨 데이터를 제공합니다.\u003c/li\u003e\n\u003cli\u003eTwitter API: JSON 형식의 실시간 트윗 및 언급을 제공합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e단계 2: Kafka 토픽 구성\u003c/p\u003e\n\u003cp\u003eKafka 클러스터에서 날씨 및 트위터 이벤트용 전용 토픽을 생성하여 데이터 흐름을 효율적으로 처리하기 위해 적절한 파티션 및 복제를 보장하세요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_7.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e단계 3: 프로듀서 개발\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e각 이벤트 소스에 대해 특정한 프로듀서를 개발하세요. 이들은 JSON 데이터를 바이트로 직렬화하고 해당 Kafka 토픽으로 게시할 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_8.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e단계 4: 컨슈머 구현\u003c/p\u003e\n\u003cp\u003e날씨 및 Twitter 이벤트용 전용 컨슈머를 생성하세요. 이 컨슈머들은 Kafka 토픽에서 바이트를 역직렬화하여 JSON 데이터로 변환한 후 처리할 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-DataEngineeringpipelineleveragingAirflowKafka_9.png\"\u003e\n\u003cp\u003e단계 5: Persistence를 위한 DB Writer 통합\u003c/p\u003e\n\u003cp\u003e이벤트 데이터를 관계형 데이터베이스에 쓰고 싶다면 DB writer를 사용하십시오. 이 구성 요소는 컨슈머에서 JSON 파일을 구문 분석하고 해당 데이터베이스 레코드를 생성합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e#db writer \u003cspan class=\"hljs-variable constant_\"\u003eEXAMPLE\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e json\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e sqlite3\n\ndef \u003cspan class=\"hljs-title function_\"\u003ewrite_to_database\u003c/span\u003e(record):\n    connection = sqlite3.\u003cspan class=\"hljs-title function_\"\u003econnect\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"event_database.db\"\u003c/span\u003e)\n    cursor = connection.\u003cspan class=\"hljs-title function_\"\u003ecursor\u003c/span\u003e()\n    cursor.\u003cspan class=\"hljs-title function_\"\u003eexecute\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"INSERT INTO events VALUES (?)\"\u003c/span\u003e, (json.\u003cspan class=\"hljs-title function_\"\u003edumps\u003c/span\u003e(record),))\n    connection.\u003cspan class=\"hljs-title function_\"\u003ecommit\u003c/span\u003e()\n    connection.\u003cspan class=\"hljs-title function_\"\u003eclose\u003c/span\u003e()\n\nrecord = {\u003cspan class=\"hljs-string\"\u003e\"event_type\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"weather\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"data\"\u003c/span\u003e: {\u003cspan class=\"hljs-string\"\u003e\"temperature\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e25\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"location\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"NYC\"\u003c/span\u003e}}\n\u003cspan class=\"hljs-title function_\"\u003ewrite_to_database\u003c/span\u003e(record)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e6단계: SQL을 사용한 데이터베이스 상호작용\u003c/p\u003e\n\u003cp\u003e데이터베이스에 레코드를 쓰기 위해 SQL 삽입문을 사용하세요. 이 단계는 카프카 토픽에서 데이터를 영구 저장소 솔루션으로 전환하는 과정을 완료합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e-- \u003cspan class=\"hljs-variable constant_\"\u003eSQL\u003c/span\u003e 삽입 예시\n\u003cspan class=\"hljs-variable constant_\"\u003eINSERT\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eINTO\u003c/span\u003e events \u003cspan class=\"hljs-variable constant_\"\u003eVALUES\u003c/span\u003e (\u003cspan class=\"hljs-string\"\u003e'{\"event_type\": \"weather\", \"data\": {\"temperature\": 25, \"location\": \"NYC\"}'\u003c/span\u003e);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e7단계: 시각화 및 분석\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e마침내, 수집하고 저장된 이벤트 데이터로부터 통찰력 있는 시각화와 분석을 위해 데이터베이스 레코드를 쿼리하세요. 수집된 이벤트 데이터로부터 가치 있는 통찰력을 얻기 위해 대시보드를 사용하는 것이 가장 좋습니다.\u003c/p\u003e\n\u003cp\u003e이 end-to-end 파이프라인은 다양한 구성 요소의 원활한 통합을 보여주며, 이벤트 스트림을 관리하는 Kafka의 유연성과 강력함을 강조합니다.\u003c/p\u003e\n\u003cp\u003e참고: 본 블로그 게시물에서 제공된 메모 및 정보는 \"ETL 및 쉘, Airflow 및 Kafka를 사용한 데이터 파이프라인\" 과정 중에 편집되었으며 개인적인 용도를 위한 기본 개요를 제공하기 위한 것입니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-DataEngineeringpipelineleveragingAirflowKafka"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>