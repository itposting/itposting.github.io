<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>엣지 TPU용 LLM 컴파일링 시도기 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="엣지 TPU용 LLM 컴파일링 시도기 | itposting" data-gatsby-head="true"/><meta property="og:title" content="엣지 TPU용 LLM 컴파일링 시도기 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs" data-gatsby-head="true"/><meta name="twitter:title" content="엣지 TPU용 LLM 컴파일링 시도기 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 18:09" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-0fd008072af5a644.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_buildManifest.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">엣지 TPU용 LLM 컴파일링 시도기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="엣지 TPU용 LLM 컴파일링 시도기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">5<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs_0.png" alt="2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs_0"></p>
<p>AI의 세계가 계속 발전함에 따라, Raspberry Pi 5 및 Coral AI Edge TPU와 같은 엣지 장치로 강력한 머신러닝 모델을 가져오는 약속은 매우 매력적입니다. 네트워크 가장자리에 정교한 AI의 강력함을 가지고 있는 것을 상상해보세요. 지속적인 클라우드 연결이 필요하지 않고 로컬에서 실행됩니다. 그러나 이러한 제약된 환경에서 대형 언어 모델(LLMs)을 배치하는 경우에는 상당한 어려움이 있습니다.</p>
<h1>라즈베리 파이 5와 코랄 AI Edge TPU와의 나의 여정</h1>
<p>최근, 나는 라즈베리 파이 5와 Coral AI Edge TPU를 이용하여 엣지에서 LLMs의 기능을 가져오는 임무를 시작했습니다. 익숙하지 않은 사람들을 위해, Edge TPU는 엣지에서 머신러닝 추론을 가속화하기 위해 설계되었으며, 탁월한 성능을 제공하면서 최소한의 전력을 소비합니다. 그러나 내 여정은 빠르게 번거로운 고난으로 변했습니다.</p>
<h1>내가 직면한 어려움들</h1>
<h2>MatMul 작업 제약 사항</h2>
<p>잘못 처리하는 일반적인 이유 중 하나는 LLMs가 행렬 곱셈 (MatMul) 작업에 심각하게 의존하고 있기 때문입니다. 이러한 작업은 신경망의 작동에 있어 기본적이며, 특히 텍스트를 처리하고 생성하기 위해 광범위한 MatMul 계산이 필요한 LLMs에게 특히 중요합니다. Edge TPU는 특정 유형의 모델에 대해 강력하지만, LLMs가 요구하는 복잡하고 대규모의 MatMul 작업에 최적화되어 있지 않습니다. 이는 엣지 디바이스에서 이러한 모델을 실행하려고 할 때 성능 병목 현상이 발생합니다.</p>
<h2>메모리 제한</h2>
<p>모델을 TensorFlow Lite (TFLite)로 변환해도, 라즈베리 파이 5와 같은 RAM 용량이 제한된 장치에서 실행하는 것은 또 다른 중요한 장벽으로 다가옵니다. 이러한 장치는 단순히 큰 모델을 효과적으로 로드하고 실행할만한 메모리 용량이 없습니다. 새로운 방법으로 이를 가능하게 할 수는 있지만, 그에 따르는 속도 저하는 모두가 감당할 의향이 있는 비용이 아닙니다. 엣지 TPU는 m.2. PCIe Gen 2 버스에서 동작하기 때문에, 약 500MB/s 정도의 이론적 제한에 묶이게 되는데, 이는 계산 시간이 아니라 파이와 엣지 TPU 간 데이터 이동에만 해당합니다.</p>
<h2>고급 모델에 대한 지원 부족</h2>
<p>Coral.ai 엣지 TPU는 특정 유형의 신경망 작업에 최적화되어 있습니다. 이미지 인식과 같은 작업에서 뛰어나지만, 언어 모델과 같은 고급 처리 능력이 필요한 작업에는 어려움을 겪을 수 있습니다. 또한 엣지 TPU는 적절한 인터페이싱을 위해 HAT+ M 어댑터가 필요하며, 이는 설정 과정에 더 많은 복잡성을 추가합니다.</p>
<h1>희망이 있는 새로운 프로젝트</h1>
<p>이러한 도전에도 불구하고, 에지 장치에서 LLM(Large Language Model)을 실행하는 것을 더 쉽게 만들기 위해 노력하는 유망한 프로젝트들이 미래에 있습니다:</p>
<h2>Large World Model (LWM)</h2>
<p>Large World Model (LWM) 프로젝트는 에지 배포를 위해 대형 모델을 최적화하는 데 중점을 둡니다. 이 프로젝트는 LLM의 계산 요구 사항을 줄이는 데 중점을 두면서 성능을 유지하려고 노력하고 있어, 에지 AI에 있어서 잠재적인 게임 체인저가 될 수 있습니다.</p>
<h2>Jetstream for JAX Models</h2>
<p>Google Cloud의 Jetstream은 TPUs와 GPUs를 사용하여 AI 추론을 가속화하는 플랫폼을 제공합니다. Jetstream은 LLMs의 계산 요구를 처리하기 위한 필요한 인프라를 제공할 수 있어, 클라우드 및 엣지 배포 사이의 격차를 줄일 수 있을 것입니다.</p>
<h2>Ollama와 AirLLM</h2>
<p>Ollama와 AirLLM과 같은 새로운 이니셔티브들은 소비자 하드웨어에서 LLM을 실행하기 쉽고 친근하게 만들었습니다. Ollama는 대규모 모델을 효율적으로 배포하고 관리하는 데 도움이 되는 도구와 리소스를 제공하며, AirLLM은 성능 상의 중요한 손실 없이 저전력 장치에서 모델을 실행할 수 있도록 최적화함으로써 AI를 보다 접근하기 쉽게 만들고 있습니다. 이러한 프로젝트들은 엣지 AI의 보다 넓은 채택을 위한 길을 열어주고 있습니다.</p>
<h1>TensorFlow Lite로 모델 변환 및 TPU용 컴파일하기</h1>
<p>모델을 가장자리(Edge)로 배포하는 실험에 관심이 있다면, TensorFlow Lite로 모델을 변환하고 Edge TPU에 컴파일하는 기본 예제가 있습니다.</p>
<h2>단계 1: 모델을 TensorFlow Lite로 변환하기</h2>
<p>먼저, 훈련된 모델을 TensorFlow Lite 형식으로 변환해야 합니다. 아래는 간단한 모델을 이와 같이 변환하는 예제입니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment"># 훈련된 모델 로드하기</span>
model = tf.keras.models.load_model(<span class="hljs-string">'your_model.h5'</span>)
<span class="hljs-comment"># 모델을 TensorFlow Lite 형식으로 변환하기</span>
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
<span class="hljs-comment"># 변환된 모델을 파일로 저장하기</span>
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'model.tflite'</span>, <span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> f:
    f.write(tflite_model)
</code></pre>
<h2>단계 2: 엣지 TPU용 TFLite 모델 컴파일하기</h2>
<p>이제 엣지 TPU 컴파일러를 사용하여 엣지 TPU용 TFLite 모델을 컴파일해야 합니다.</p>
<pre><code class="hljs language-js">edgetpu_compiler model.<span class="hljs-property">tflite</span>
</code></pre>
<p>이 명령은 엣지 TPU에 최적화된 model_edgetpu.tflite 파일을 생성합니다.</p>
<h2>단계 3: 엣지 TPU에서 모델 실행</h2>
<p>이제 컴파일된 모델을 엣지 TPU에서 실행할 수 있습니다. Python API를 사용하여 이 작업을 수행하는 예제를 제공합니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> tflite_runtime.<span class="hljs-property">interpreter</span> <span class="hljs-keyword">as</span> tflite
<span class="hljs-keyword">import</span> platform

<span class="hljs-variable constant_">EDGETPU_SHARED_LIB</span> = <span class="hljs-string">'libedgetpu.so.1'</span>
# 컴파일된 <span class="hljs-title class_">TFLite</span> 모델 로드
interpreter = tflite.<span class="hljs-title class_">Interpreter</span>(model_path=<span class="hljs-string">'model_edgetpu.tflite'</span>, 
                                 experimental_delegates=[tflite.<span class="hljs-title function_">load_delegate</span>(<span class="hljs-variable constant_">EDGETPU_SHARED_LIB</span>)])
interpreter.<span class="hljs-title function_">allocate_tensors</span>()
# 입력 및 출력 텐서 가져오기
input_details = interpreter.<span class="hljs-title function_">get_input_details</span>()
output_details = interpreter.<span class="hljs-title function_">get_output_details</span>()
# 입력 데이터 준비
input_data = ...
# 텐서를 입력 데이터를 가리키도록 설정하여 추론 수행
interpreter.<span class="hljs-title function_">set_tensor</span>(input_details[<span class="hljs-number">0</span>][<span class="hljs-string">'index'</span>], input_data)
# 추론 실행
interpreter.<span class="hljs-title function_">invoke</span>()
# 출력 추출
output_data = interpreter.<span class="hljs-title function_">get_tensor</span>(output_details[<span class="hljs-number">0</span>][<span class="hljs-string">'index'</span>])
<span class="hljs-title function_">print</span>(output_data)
</code></pre>
<h1>실행 요청</h1>
<p>LLM(주변 머신)을 엣지에서 실행하는 잠재력은 엄청납니다. 똑똑한 IoT 기기 및 반응성 있는 AI 응용 프로그램을 통한 기회, 그리고 데이터를 로컬에 유지함으로써 개인 정보 보호도 향상됩니다. 그러나 현재 기술 상태는 아직 완전하지는 않습니다. 이를 실현하기 위해 더 많은 커뮤니티 주도 노력과 혁신이 필요합니다.</p>
<h1>도와주세요</h1>
<ul>
<li>경험 공유: LLM(주변 머신 학습)을 엣지 장치에서 실행한 적이 있나요? 아래 댓글에서 성공과 실패를 공유해보세요.</li>
<li>오픈소스 프로젝트 기여: 많은 프로젝트가 엣지용 AI 최적화를 위해 노력하고 있습니다. 여러분의 기여가 큰 차이를 만들 수 있습니다.</li>
<li>기술적 통찰 제공: 모델 최적화나 엣지 컴퓨팅 분야 전문 지식이 있다면, 여러분의 조언과 통찰은 이러한 과제에 어려움을 겪는 우리에게 매우 귀중할 것입니다.</li>
<li>혁신: 엔지니어 또는 개발자인 경우, 이러한 과제 중 일부를 직접 해결해보세요. 혁신적인 솔루션이 절실히 필요합니다.</li>
</ul>
<p>함께하면 AI 엣지 기술의 가능성을 넓힐 수 있습니다. 함께 협력하여 엣지 컴퓨팅의 미래를 이끌어가요.</p>
<p>에지 디바이스에 AI를 배포하는 과정에서 어떤 어려움을 겪으셨나요? 아래 댓글로 귀하의 생각과 경험을 공유해주세요!</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"엣지 TPU용 LLM 컴파일링 시도기","description":"","date":"2024-06-19 18:09","slug":"2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs","content":"\n\n\n![2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs_0](/assets/img/2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs_0.png)\n\nAI의 세계가 계속 발전함에 따라, Raspberry Pi 5 및 Coral AI Edge TPU와 같은 엣지 장치로 강력한 머신러닝 모델을 가져오는 약속은 매우 매력적입니다. 네트워크 가장자리에 정교한 AI의 강력함을 가지고 있는 것을 상상해보세요. 지속적인 클라우드 연결이 필요하지 않고 로컬에서 실행됩니다. 그러나 이러한 제약된 환경에서 대형 언어 모델(LLMs)을 배치하는 경우에는 상당한 어려움이 있습니다.\n\n# 라즈베리 파이 5와 코랄 AI Edge TPU와의 나의 여정\n\n최근, 나는 라즈베리 파이 5와 Coral AI Edge TPU를 이용하여 엣지에서 LLMs의 기능을 가져오는 임무를 시작했습니다. 익숙하지 않은 사람들을 위해, Edge TPU는 엣지에서 머신러닝 추론을 가속화하기 위해 설계되었으며, 탁월한 성능을 제공하면서 최소한의 전력을 소비합니다. 그러나 내 여정은 빠르게 번거로운 고난으로 변했습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 내가 직면한 어려움들\n\n## MatMul 작업 제약 사항\n\n잘못 처리하는 일반적인 이유 중 하나는 LLMs가 행렬 곱셈 (MatMul) 작업에 심각하게 의존하고 있기 때문입니다. 이러한 작업은 신경망의 작동에 있어 기본적이며, 특히 텍스트를 처리하고 생성하기 위해 광범위한 MatMul 계산이 필요한 LLMs에게 특히 중요합니다. Edge TPU는 특정 유형의 모델에 대해 강력하지만, LLMs가 요구하는 복잡하고 대규모의 MatMul 작업에 최적화되어 있지 않습니다. 이는 엣지 디바이스에서 이러한 모델을 실행하려고 할 때 성능 병목 현상이 발생합니다.\n\n## 메모리 제한\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델을 TensorFlow Lite (TFLite)로 변환해도, 라즈베리 파이 5와 같은 RAM 용량이 제한된 장치에서 실행하는 것은 또 다른 중요한 장벽으로 다가옵니다. 이러한 장치는 단순히 큰 모델을 효과적으로 로드하고 실행할만한 메모리 용량이 없습니다. 새로운 방법으로 이를 가능하게 할 수는 있지만, 그에 따르는 속도 저하는 모두가 감당할 의향이 있는 비용이 아닙니다. 엣지 TPU는 m.2. PCIe Gen 2 버스에서 동작하기 때문에, 약 500MB/s 정도의 이론적 제한에 묶이게 되는데, 이는 계산 시간이 아니라 파이와 엣지 TPU 간 데이터 이동에만 해당합니다.\n\n## 고급 모델에 대한 지원 부족\n\nCoral.ai 엣지 TPU는 특정 유형의 신경망 작업에 최적화되어 있습니다. 이미지 인식과 같은 작업에서 뛰어나지만, 언어 모델과 같은 고급 처리 능력이 필요한 작업에는 어려움을 겪을 수 있습니다. 또한 엣지 TPU는 적절한 인터페이싱을 위해 HAT+ M 어댑터가 필요하며, 이는 설정 과정에 더 많은 복잡성을 추가합니다.\n\n# 희망이 있는 새로운 프로젝트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 도전에도 불구하고, 에지 장치에서 LLM(Large Language Model)을 실행하는 것을 더 쉽게 만들기 위해 노력하는 유망한 프로젝트들이 미래에 있습니다:\n\n## Large World Model (LWM)\n\nLarge World Model (LWM) 프로젝트는 에지 배포를 위해 대형 모델을 최적화하는 데 중점을 둡니다. 이 프로젝트는 LLM의 계산 요구 사항을 줄이는 데 중점을 두면서 성능을 유지하려고 노력하고 있어, 에지 AI에 있어서 잠재적인 게임 체인저가 될 수 있습니다.\n\n## Jetstream for JAX Models\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGoogle Cloud의 Jetstream은 TPUs와 GPUs를 사용하여 AI 추론을 가속화하는 플랫폼을 제공합니다. Jetstream은 LLMs의 계산 요구를 처리하기 위한 필요한 인프라를 제공할 수 있어, 클라우드 및 엣지 배포 사이의 격차를 줄일 수 있을 것입니다.\n\n## Ollama와 AirLLM\n\nOllama와 AirLLM과 같은 새로운 이니셔티브들은 소비자 하드웨어에서 LLM을 실행하기 쉽고 친근하게 만들었습니다. Ollama는 대규모 모델을 효율적으로 배포하고 관리하는 데 도움이 되는 도구와 리소스를 제공하며, AirLLM은 성능 상의 중요한 손실 없이 저전력 장치에서 모델을 실행할 수 있도록 최적화함으로써 AI를 보다 접근하기 쉽게 만들고 있습니다. 이러한 프로젝트들은 엣지 AI의 보다 넓은 채택을 위한 길을 열어주고 있습니다.\n\n# TensorFlow Lite로 모델 변환 및 TPU용 컴파일하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델을 가장자리(Edge)로 배포하는 실험에 관심이 있다면, TensorFlow Lite로 모델을 변환하고 Edge TPU에 컴파일하는 기본 예제가 있습니다.\n\n## 단계 1: 모델을 TensorFlow Lite로 변환하기\n\n먼저, 훈련된 모델을 TensorFlow Lite 형식으로 변환해야 합니다. 아래는 간단한 모델을 이와 같이 변환하는 예제입니다.\n\n```python\nimport tensorflow as tf\n\n# 훈련된 모델 로드하기\nmodel = tf.keras.models.load_model('your_model.h5')\n# 모델을 TensorFlow Lite 형식으로 변환하기\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n# 변환된 모델을 파일로 저장하기\nwith open('model.tflite', 'wb') as f:\n    f.write(tflite_model)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 2: 엣지 TPU용 TFLite 모델 컴파일하기\n\n이제 엣지 TPU 컴파일러를 사용하여 엣지 TPU용 TFLite 모델을 컴파일해야 합니다.\n\n```js\nedgetpu_compiler model.tflite\n```\n\n이 명령은 엣지 TPU에 최적화된 model_edgetpu.tflite 파일을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 3: 엣지 TPU에서 모델 실행\n\n이제 컴파일된 모델을 엣지 TPU에서 실행할 수 있습니다. Python API를 사용하여 이 작업을 수행하는 예제를 제공합니다.\n\n```js\nimport tflite_runtime.interpreter as tflite\nimport platform\n\nEDGETPU_SHARED_LIB = 'libedgetpu.so.1'\n# 컴파일된 TFLite 모델 로드\ninterpreter = tflite.Interpreter(model_path='model_edgetpu.tflite', \n                                 experimental_delegates=[tflite.load_delegate(EDGETPU_SHARED_LIB)])\ninterpreter.allocate_tensors()\n# 입력 및 출력 텐서 가져오기\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n# 입력 데이터 준비\ninput_data = ...\n# 텐서를 입력 데이터를 가리키도록 설정하여 추론 수행\ninterpreter.set_tensor(input_details[0]['index'], input_data)\n# 추론 실행\ninterpreter.invoke()\n# 출력 추출\noutput_data = interpreter.get_tensor(output_details[0]['index'])\nprint(output_data)\n```\n\n# 실행 요청\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM(주변 머신)을 엣지에서 실행하는 잠재력은 엄청납니다. 똑똑한 IoT 기기 및 반응성 있는 AI 응용 프로그램을 통한 기회, 그리고 데이터를 로컬에 유지함으로써 개인 정보 보호도 향상됩니다. 그러나 현재 기술 상태는 아직 완전하지는 않습니다. 이를 실현하기 위해 더 많은 커뮤니티 주도 노력과 혁신이 필요합니다.\n\n# 도와주세요\n\n- 경험 공유: LLM(주변 머신 학습)을 엣지 장치에서 실행한 적이 있나요? 아래 댓글에서 성공과 실패를 공유해보세요.\n- 오픈소스 프로젝트 기여: 많은 프로젝트가 엣지용 AI 최적화를 위해 노력하고 있습니다. 여러분의 기여가 큰 차이를 만들 수 있습니다.\n- 기술적 통찰 제공: 모델 최적화나 엣지 컴퓨팅 분야 전문 지식이 있다면, 여러분의 조언과 통찰은 이러한 과제에 어려움을 겪는 우리에게 매우 귀중할 것입니다.\n- 혁신: 엔지니어 또는 개발자인 경우, 이러한 과제 중 일부를 직접 해결해보세요. 혁신적인 솔루션이 절실히 필요합니다.\n\n함께하면 AI 엣지 기술의 가능성을 넓힐 수 있습니다. 함께 협력하여 엣지 컴퓨팅의 미래를 이끌어가요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n에지 디바이스에 AI를 배포하는 과정에서 어떤 어려움을 겪으셨나요? 아래 댓글로 귀하의 생각과 경험을 공유해주세요!","ogImage":{"url":"/assets/img/2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs_0.png"},"coverImage":"/assets/img/2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs_0.png","tag":["Tech"],"readingTime":5},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs_0.png\" alt=\"2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs_0\"\u003e\u003c/p\u003e\n\u003cp\u003eAI의 세계가 계속 발전함에 따라, Raspberry Pi 5 및 Coral AI Edge TPU와 같은 엣지 장치로 강력한 머신러닝 모델을 가져오는 약속은 매우 매력적입니다. 네트워크 가장자리에 정교한 AI의 강력함을 가지고 있는 것을 상상해보세요. 지속적인 클라우드 연결이 필요하지 않고 로컬에서 실행됩니다. 그러나 이러한 제약된 환경에서 대형 언어 모델(LLMs)을 배치하는 경우에는 상당한 어려움이 있습니다.\u003c/p\u003e\n\u003ch1\u003e라즈베리 파이 5와 코랄 AI Edge TPU와의 나의 여정\u003c/h1\u003e\n\u003cp\u003e최근, 나는 라즈베리 파이 5와 Coral AI Edge TPU를 이용하여 엣지에서 LLMs의 기능을 가져오는 임무를 시작했습니다. 익숙하지 않은 사람들을 위해, Edge TPU는 엣지에서 머신러닝 추론을 가속화하기 위해 설계되었으며, 탁월한 성능을 제공하면서 최소한의 전력을 소비합니다. 그러나 내 여정은 빠르게 번거로운 고난으로 변했습니다.\u003c/p\u003e\n\u003ch1\u003e내가 직면한 어려움들\u003c/h1\u003e\n\u003ch2\u003eMatMul 작업 제약 사항\u003c/h2\u003e\n\u003cp\u003e잘못 처리하는 일반적인 이유 중 하나는 LLMs가 행렬 곱셈 (MatMul) 작업에 심각하게 의존하고 있기 때문입니다. 이러한 작업은 신경망의 작동에 있어 기본적이며, 특히 텍스트를 처리하고 생성하기 위해 광범위한 MatMul 계산이 필요한 LLMs에게 특히 중요합니다. Edge TPU는 특정 유형의 모델에 대해 강력하지만, LLMs가 요구하는 복잡하고 대규모의 MatMul 작업에 최적화되어 있지 않습니다. 이는 엣지 디바이스에서 이러한 모델을 실행하려고 할 때 성능 병목 현상이 발생합니다.\u003c/p\u003e\n\u003ch2\u003e메모리 제한\u003c/h2\u003e\n\u003cp\u003e모델을 TensorFlow Lite (TFLite)로 변환해도, 라즈베리 파이 5와 같은 RAM 용량이 제한된 장치에서 실행하는 것은 또 다른 중요한 장벽으로 다가옵니다. 이러한 장치는 단순히 큰 모델을 효과적으로 로드하고 실행할만한 메모리 용량이 없습니다. 새로운 방법으로 이를 가능하게 할 수는 있지만, 그에 따르는 속도 저하는 모두가 감당할 의향이 있는 비용이 아닙니다. 엣지 TPU는 m.2. PCIe Gen 2 버스에서 동작하기 때문에, 약 500MB/s 정도의 이론적 제한에 묶이게 되는데, 이는 계산 시간이 아니라 파이와 엣지 TPU 간 데이터 이동에만 해당합니다.\u003c/p\u003e\n\u003ch2\u003e고급 모델에 대한 지원 부족\u003c/h2\u003e\n\u003cp\u003eCoral.ai 엣지 TPU는 특정 유형의 신경망 작업에 최적화되어 있습니다. 이미지 인식과 같은 작업에서 뛰어나지만, 언어 모델과 같은 고급 처리 능력이 필요한 작업에는 어려움을 겪을 수 있습니다. 또한 엣지 TPU는 적절한 인터페이싱을 위해 HAT+ M 어댑터가 필요하며, 이는 설정 과정에 더 많은 복잡성을 추가합니다.\u003c/p\u003e\n\u003ch1\u003e희망이 있는 새로운 프로젝트\u003c/h1\u003e\n\u003cp\u003e이러한 도전에도 불구하고, 에지 장치에서 LLM(Large Language Model)을 실행하는 것을 더 쉽게 만들기 위해 노력하는 유망한 프로젝트들이 미래에 있습니다:\u003c/p\u003e\n\u003ch2\u003eLarge World Model (LWM)\u003c/h2\u003e\n\u003cp\u003eLarge World Model (LWM) 프로젝트는 에지 배포를 위해 대형 모델을 최적화하는 데 중점을 둡니다. 이 프로젝트는 LLM의 계산 요구 사항을 줄이는 데 중점을 두면서 성능을 유지하려고 노력하고 있어, 에지 AI에 있어서 잠재적인 게임 체인저가 될 수 있습니다.\u003c/p\u003e\n\u003ch2\u003eJetstream for JAX Models\u003c/h2\u003e\n\u003cp\u003eGoogle Cloud의 Jetstream은 TPUs와 GPUs를 사용하여 AI 추론을 가속화하는 플랫폼을 제공합니다. Jetstream은 LLMs의 계산 요구를 처리하기 위한 필요한 인프라를 제공할 수 있어, 클라우드 및 엣지 배포 사이의 격차를 줄일 수 있을 것입니다.\u003c/p\u003e\n\u003ch2\u003eOllama와 AirLLM\u003c/h2\u003e\n\u003cp\u003eOllama와 AirLLM과 같은 새로운 이니셔티브들은 소비자 하드웨어에서 LLM을 실행하기 쉽고 친근하게 만들었습니다. Ollama는 대규모 모델을 효율적으로 배포하고 관리하는 데 도움이 되는 도구와 리소스를 제공하며, AirLLM은 성능 상의 중요한 손실 없이 저전력 장치에서 모델을 실행할 수 있도록 최적화함으로써 AI를 보다 접근하기 쉽게 만들고 있습니다. 이러한 프로젝트들은 엣지 AI의 보다 넓은 채택을 위한 길을 열어주고 있습니다.\u003c/p\u003e\n\u003ch1\u003eTensorFlow Lite로 모델 변환 및 TPU용 컴파일하기\u003c/h1\u003e\n\u003cp\u003e모델을 가장자리(Edge)로 배포하는 실험에 관심이 있다면, TensorFlow Lite로 모델을 변환하고 Edge TPU에 컴파일하는 기본 예제가 있습니다.\u003c/p\u003e\n\u003ch2\u003e단계 1: 모델을 TensorFlow Lite로 변환하기\u003c/h2\u003e\n\u003cp\u003e먼저, 훈련된 모델을 TensorFlow Lite 형식으로 변환해야 합니다. 아래는 간단한 모델을 이와 같이 변환하는 예제입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tensorflow \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e tf\n\n\u003cspan class=\"hljs-comment\"\u003e# 훈련된 모델 로드하기\u003c/span\u003e\nmodel = tf.keras.models.load_model(\u003cspan class=\"hljs-string\"\u003e'your_model.h5'\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# 모델을 TensorFlow Lite 형식으로 변환하기\u003c/span\u003e\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\u003cspan class=\"hljs-comment\"\u003e# 변환된 모델을 파일로 저장하기\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eopen\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'model.tflite'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'wb'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e f:\n    f.write(tflite_model)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e단계 2: 엣지 TPU용 TFLite 모델 컴파일하기\u003c/h2\u003e\n\u003cp\u003e이제 엣지 TPU 컴파일러를 사용하여 엣지 TPU용 TFLite 모델을 컴파일해야 합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eedgetpu_compiler model.\u003cspan class=\"hljs-property\"\u003etflite\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 명령은 엣지 TPU에 최적화된 model_edgetpu.tflite 파일을 생성합니다.\u003c/p\u003e\n\u003ch2\u003e단계 3: 엣지 TPU에서 모델 실행\u003c/h2\u003e\n\u003cp\u003e이제 컴파일된 모델을 엣지 TPU에서 실행할 수 있습니다. Python API를 사용하여 이 작업을 수행하는 예제를 제공합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tflite_runtime.\u003cspan class=\"hljs-property\"\u003einterpreter\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e tflite\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e platform\n\n\u003cspan class=\"hljs-variable constant_\"\u003eEDGETPU_SHARED_LIB\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e'libedgetpu.so.1'\u003c/span\u003e\n# 컴파일된 \u003cspan class=\"hljs-title class_\"\u003eTFLite\u003c/span\u003e 모델 로드\ninterpreter = tflite.\u003cspan class=\"hljs-title class_\"\u003eInterpreter\u003c/span\u003e(model_path=\u003cspan class=\"hljs-string\"\u003e'model_edgetpu.tflite'\u003c/span\u003e, \n                                 experimental_delegates=[tflite.\u003cspan class=\"hljs-title function_\"\u003eload_delegate\u003c/span\u003e(\u003cspan class=\"hljs-variable constant_\"\u003eEDGETPU_SHARED_LIB\u003c/span\u003e)])\ninterpreter.\u003cspan class=\"hljs-title function_\"\u003eallocate_tensors\u003c/span\u003e()\n# 입력 및 출력 텐서 가져오기\ninput_details = interpreter.\u003cspan class=\"hljs-title function_\"\u003eget_input_details\u003c/span\u003e()\noutput_details = interpreter.\u003cspan class=\"hljs-title function_\"\u003eget_output_details\u003c/span\u003e()\n# 입력 데이터 준비\ninput_data = ...\n# 텐서를 입력 데이터를 가리키도록 설정하여 추론 수행\ninterpreter.\u003cspan class=\"hljs-title function_\"\u003eset_tensor\u003c/span\u003e(input_details[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'index'\u003c/span\u003e], input_data)\n# 추론 실행\ninterpreter.\u003cspan class=\"hljs-title function_\"\u003einvoke\u003c/span\u003e()\n# 출력 추출\noutput_data = interpreter.\u003cspan class=\"hljs-title function_\"\u003eget_tensor\u003c/span\u003e(output_details[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'index'\u003c/span\u003e])\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(output_data)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e실행 요청\u003c/h1\u003e\n\u003cp\u003eLLM(주변 머신)을 엣지에서 실행하는 잠재력은 엄청납니다. 똑똑한 IoT 기기 및 반응성 있는 AI 응용 프로그램을 통한 기회, 그리고 데이터를 로컬에 유지함으로써 개인 정보 보호도 향상됩니다. 그러나 현재 기술 상태는 아직 완전하지는 않습니다. 이를 실현하기 위해 더 많은 커뮤니티 주도 노력과 혁신이 필요합니다.\u003c/p\u003e\n\u003ch1\u003e도와주세요\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e경험 공유: LLM(주변 머신 학습)을 엣지 장치에서 실행한 적이 있나요? 아래 댓글에서 성공과 실패를 공유해보세요.\u003c/li\u003e\n\u003cli\u003e오픈소스 프로젝트 기여: 많은 프로젝트가 엣지용 AI 최적화를 위해 노력하고 있습니다. 여러분의 기여가 큰 차이를 만들 수 있습니다.\u003c/li\u003e\n\u003cli\u003e기술적 통찰 제공: 모델 최적화나 엣지 컴퓨팅 분야 전문 지식이 있다면, 여러분의 조언과 통찰은 이러한 과제에 어려움을 겪는 우리에게 매우 귀중할 것입니다.\u003c/li\u003e\n\u003cli\u003e혁신: 엔지니어 또는 개발자인 경우, 이러한 과제 중 일부를 직접 해결해보세요. 혁신적인 솔루션이 절실히 필요합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e함께하면 AI 엣지 기술의 가능성을 넓힐 수 있습니다. 함께 협력하여 엣지 컴퓨팅의 미래를 이끌어가요.\u003c/p\u003e\n\u003cp\u003e에지 디바이스에 AI를 배포하는 과정에서 어떤 어려움을 겪으셨나요? 아래 댓글로 귀하의 생각과 경험을 공유해주세요!\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-MyAttemptatCompilingLLMsforEdgeTPUs"},"buildId":"FH3Qr-mLAesqA0X5IFRQr","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>