<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>LLM Alignments Part 21 RLAIF | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-LLMAlignmentsPart21RLAIF" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="LLM Alignments Part 21 RLAIF | itposting" data-gatsby-head="true"/><meta property="og:title" content="LLM Alignments Part 21 RLAIF | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-LLMAlignmentsPart21RLAIF" data-gatsby-head="true"/><meta name="twitter:title" content="LLM Alignments Part 21 RLAIF | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 20:39" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">LLM Alignments Part 21 RLAIF</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="LLM Alignments Part 21 RLAIF" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">2<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-LLMAlignmentsPart21RLAIF&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>안녕하세요!</p>
<p>오늘의 주제는 조금 가벼울 수 있지만, 이미 RLHF에 대해 다뤘으니 이제 RLAIF에 대해 이야기해야 합니다. RLAIF이 점점 더 보편화되는 것이 중요하기 때문이죠.</p>
<p><img src="/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_0.png" alt="이미지"></p>
<p>RLAIF의 개념은 간단합니다: RLHF의 "H" (Human)를 AI로 교체하는 것만을 의미합니다. 이 전환이 필요한 이유는 인간으로부터 데이터를 수집하는 것이 시간이 많이 소요되고 비용이 많이 들며 확장하기 어려울 수 있기 때문입니다.</p>
<div class="content-ad"></div>
<p>LLM 기반 에이전트의 효과가 입증되었으며, 사고 체인(Chain of thought, CoT)에서 사고 트리(Tree of thought, ToT), ReAct, Reflexion 및 기타 다양한 요소들까지, LLM을 강화 학습(Reinforcement Learning, RL), 검색 보강 생성(Retrieval-augmented generation, RAG) 또는 유사한 프레임워크에 통합함으로써 추론 성능을 크게 향상시킬 수 있다는 것이 명백해졌습니다.</p>
<p>이 논문에서 강조된 바에 따르면, 데이터 생성을 위해 AI를 사용하는 것은 회귀로 이어지지 않으며, 실제로 특정 작업에서 RLHF를 능가할 수도 있습니다.</p>
<p>다음은 위에서 설명한 RLHF와 RLAIF의 차이를 더 자세히 보여주는 다이어그램입니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_2.png" alt="이미지"></p>
<p>LLM(라지앤 러닝 모델)을 사용한 에이전트의 관점에서 대안적인 접근 방식은 학습된 보상 모델(RM)을 입력으로 사용자의 선호도를 받아들이고 엔지니어링된 프롬프트와 에이전트 아키텍처를 통해 점수를 출력하는 에이전트로 대체하는 것입니다.</p>
<p>참고:</p>
<ul>
<li>RLHF로부터 학습된 RM은 종종 SFT 모델에서 증류된 학습을 통해 훈련되기 때문에 증류된 RM으로 언급됩니다.</li>
<li>에이전트 스타일의 RM은 훈련을 필요로 하지 않기 때문에 직접 RM으로 언급됩니다.</li>
</ul>
<div class="content-ad"></div>
<p>오른쪽 표를 보면 직접 RM의 성능이 간접 RM의 성능과 일치한다는 것을 보여줍니다. 여기서 '동일 크기의 RLAIF'는 인공지능에 의해 생성된 교육 데이터가 RLHF에서 사용된 것과 동일한 크기인 간접 RM을 나타냅니다.</p>
<img src="/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_3.png">
<p>마지막으로 RLHF와 RLAIF를 비교합니다. 요약 및 유용성 작업에서 성능이 일치하는 방법과 RLAIF가 무해성 측면에서 RLHF를 능가하는 것을 주목해주세요.</p>
<p>오늘은 여기까지입니다! 다음에는 DPO에 대해 이야기해볼 수 있겠네요~</p>
<div class="content-ad"></div>
<p>참고:</p>
<p>RLAIF: 인공지능 피드백을 활용한 인간 피드백으로 강화 학습 확장</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"LLM Alignments Part 21 RLAIF","description":"","date":"2024-06-19 20:39","slug":"2024-06-19-LLMAlignmentsPart21RLAIF","content":"\n\n안녕하세요!\n\n오늘의 주제는 조금 가벼울 수 있지만, 이미 RLHF에 대해 다뤘으니 이제 RLAIF에 대해 이야기해야 합니다. RLAIF이 점점 더 보편화되는 것이 중요하기 때문이죠.\n\n![이미지](/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_0.png)\n\nRLAIF의 개념은 간단합니다: RLHF의 \"H\" (Human)를 AI로 교체하는 것만을 의미합니다. 이 전환이 필요한 이유는 인간으로부터 데이터를 수집하는 것이 시간이 많이 소요되고 비용이 많이 들며 확장하기 어려울 수 있기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM 기반 에이전트의 효과가 입증되었으며, 사고 체인(Chain of thought, CoT)에서 사고 트리(Tree of thought, ToT), ReAct, Reflexion 및 기타 다양한 요소들까지, LLM을 강화 학습(Reinforcement Learning, RL), 검색 보강 생성(Retrieval-augmented generation, RAG) 또는 유사한 프레임워크에 통합함으로써 추론 성능을 크게 향상시킬 수 있다는 것이 명백해졌습니다.\n\n이 논문에서 강조된 바에 따르면, 데이터 생성을 위해 AI를 사용하는 것은 회귀로 이어지지 않으며, 실제로 특정 작업에서 RLHF를 능가할 수도 있습니다.\n\n다음은 위에서 설명한 RLHF와 RLAIF의 차이를 더 자세히 보여주는 다이어그램입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_2.png)\n\nLLM(라지앤 러닝 모델)을 사용한 에이전트의 관점에서 대안적인 접근 방식은 학습된 보상 모델(RM)을 입력으로 사용자의 선호도를 받아들이고 엔지니어링된 프롬프트와 에이전트 아키텍처를 통해 점수를 출력하는 에이전트로 대체하는 것입니다.\n\n참고:\n- RLHF로부터 학습된 RM은 종종 SFT 모델에서 증류된 학습을 통해 훈련되기 때문에 증류된 RM으로 언급됩니다.\n- 에이전트 스타일의 RM은 훈련을 필요로 하지 않기 때문에 직접 RM으로 언급됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오른쪽 표를 보면 직접 RM의 성능이 간접 RM의 성능과 일치한다는 것을 보여줍니다. 여기서 '동일 크기의 RLAIF'는 인공지능에 의해 생성된 교육 데이터가 RLHF에서 사용된 것과 동일한 크기인 간접 RM을 나타냅니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_3.png\" /\u003e\n\n마지막으로 RLHF와 RLAIF를 비교합니다. 요약 및 유용성 작업에서 성능이 일치하는 방법과 RLAIF가 무해성 측면에서 RLHF를 능가하는 것을 주목해주세요.\n\n오늘은 여기까지입니다! 다음에는 DPO에 대해 이야기해볼 수 있겠네요~\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n참고:\n\nRLAIF: 인공지능 피드백을 활용한 인간 피드백으로 강화 학습 확장","ogImage":{"url":"/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_0.png"},"coverImage":"/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_0.png","tag":["Tech"],"readingTime":2},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e안녕하세요!\u003c/p\u003e\n\u003cp\u003e오늘의 주제는 조금 가벼울 수 있지만, 이미 RLHF에 대해 다뤘으니 이제 RLAIF에 대해 이야기해야 합니다. RLAIF이 점점 더 보편화되는 것이 중요하기 때문이죠.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eRLAIF의 개념은 간단합니다: RLHF의 \"H\" (Human)를 AI로 교체하는 것만을 의미합니다. 이 전환이 필요한 이유는 인간으로부터 데이터를 수집하는 것이 시간이 많이 소요되고 비용이 많이 들며 확장하기 어려울 수 있기 때문입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eLLM 기반 에이전트의 효과가 입증되었으며, 사고 체인(Chain of thought, CoT)에서 사고 트리(Tree of thought, ToT), ReAct, Reflexion 및 기타 다양한 요소들까지, LLM을 강화 학습(Reinforcement Learning, RL), 검색 보강 생성(Retrieval-augmented generation, RAG) 또는 유사한 프레임워크에 통합함으로써 추론 성능을 크게 향상시킬 수 있다는 것이 명백해졌습니다.\u003c/p\u003e\n\u003cp\u003e이 논문에서 강조된 바에 따르면, 데이터 생성을 위해 AI를 사용하는 것은 회귀로 이어지지 않으며, 실제로 특정 작업에서 RLHF를 능가할 수도 있습니다.\u003c/p\u003e\n\u003cp\u003e다음은 위에서 설명한 RLHF와 RLAIF의 차이를 더 자세히 보여주는 다이어그램입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eLLM(라지앤 러닝 모델)을 사용한 에이전트의 관점에서 대안적인 접근 방식은 학습된 보상 모델(RM)을 입력으로 사용자의 선호도를 받아들이고 엔지니어링된 프롬프트와 에이전트 아키텍처를 통해 점수를 출력하는 에이전트로 대체하는 것입니다.\u003c/p\u003e\n\u003cp\u003e참고:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRLHF로부터 학습된 RM은 종종 SFT 모델에서 증류된 학습을 통해 훈련되기 때문에 증류된 RM으로 언급됩니다.\u003c/li\u003e\n\u003cli\u003e에이전트 스타일의 RM은 훈련을 필요로 하지 않기 때문에 직접 RM으로 언급됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e오른쪽 표를 보면 직접 RM의 성능이 간접 RM의 성능과 일치한다는 것을 보여줍니다. 여기서 '동일 크기의 RLAIF'는 인공지능에 의해 생성된 교육 데이터가 RLHF에서 사용된 것과 동일한 크기인 간접 RM을 나타냅니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-LLMAlignmentsPart21RLAIF_3.png\"\u003e\n\u003cp\u003e마지막으로 RLHF와 RLAIF를 비교합니다. 요약 및 유용성 작업에서 성능이 일치하는 방법과 RLAIF가 무해성 측면에서 RLHF를 능가하는 것을 주목해주세요.\u003c/p\u003e\n\u003cp\u003e오늘은 여기까지입니다! 다음에는 DPO에 대해 이야기해볼 수 있겠네요~\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e참고:\u003c/p\u003e\n\u003cp\u003eRLAIF: 인공지능 피드백을 활용한 인간 피드백으로 강화 학습 확장\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-LLMAlignmentsPart21RLAIF"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>