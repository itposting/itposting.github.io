<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>대형 언어 모델LLMs 쉽게 배우기 기초부터 시작하는 가이드 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="대형 언어 모델LLMs 쉽게 배우기 기초부터 시작하는 가이드 | itposting" data-gatsby-head="true"/><meta property="og:title" content="대형 언어 모델LLMs 쉽게 배우기 기초부터 시작하는 가이드 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs" data-gatsby-head="true"/><meta name="twitter:title" content="대형 언어 모델LLMs 쉽게 배우기 기초부터 시작하는 가이드 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-22 21:18" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">대형 언어 모델LLMs 쉽게 배우기 기초부터 시작하는 가이드</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="대형 언어 모델LLMs 쉽게 배우기 기초부터 시작하는 가이드" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 22, 2024</span><span class="posts_reading_time__f7YPP">11<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_0.png" alt="이미지"></p>
<p>여러분 중 많은 분들이 ChatGPT와 같은 도구를 재미있게 활용해 보신 적이 있을 것입니다. 지난 몇 년 동안 이 기술은 비즈니스 프로세스부터 일상적인 업무까지 우리 삶의 모든 측면으로 침투해 왔습니다. 그리고 알고 계세요? 이제 막 시작이라는데요. 많은 사람들이 아직 기계 학습, 신경망 또는 인공지능을 완전히 이해하지 못하고 있지만, 이제는 그것이 변화하려고 합니다. 이것은 시작점입니다. 지금 기초를 잡지 않으면 나중에 따라잡기가 더 어려울 수도 있습니다. 그러니 새로운 지식을 습득하는 것이 높은 시간입니다.</p>
<p>OpenAI가 ChatGPT를 선보인 시점은 게임 체인저였습니다. 이를 통해 Large Language Models(LLMs)의 놀라운 파워가 드러났습니다. 이러한 모델들은 여행 계획부터 요리 레시피까지 모든 것을 변화시켰습니다. 하지만 이제 우리는 그것들을 해체하려고 합니다: 본질적으로, LLMs는 언어 처리에 뛰어난 수학적 모델들입니다.</p>
<p>자연어 처리(Natural Language Processing, NLP)의 하위 집합인 이 기술은 기계 학습에서 주도적인 역할을 하게 되었습니다. 이제 모든 사람들이 LLMs와 NLP에 대해 배우고, 그들의 잠재력을 어떻게 활용할지에 대해 열망하고 있습니다.</p>
<div class="content-ad"></div>
<p>그러니까, 이것을 자세히 살펴보겠습니다 — LLM이란 무엇이며 어떻게 작동하는지, 그리고 학습, 삶, 그 이상에 활용하는 방법에 대해 알아보겠습니다. 이 시리즈를 따라가기 위해 필요한 것은 기본적인 학교 수학 지식, 약간의 Python 프로그래밍 스킬, 그리고 건강한 호기심입니다. 머신러닝에 대한 기본적인 이해는 플러스이지만 필수는 아닙니다. 더 깊이 파고 싶다면 해당 정보를 모두 확인할 수 있는 추가 소스를 제공할 테니 걱정하지 마세요. 이 시리즈는 코더와 연구자뿐만 아니라 소프트웨어 엔지니어들을 위한 것입니다. 이해가 안 된다면 직접 관여하고 LLM을 실험해보세요.</p>
<h1>대형 언어 모델 해석</h1>
<p>LLM의 등장은 OpenAI의 ChatGPT가 화제가 되기 전에 시작되었습니다. 2017년 초에 구글 연구팀이 Transformer라는 혁신적인 딥러닝 모델 구조를 소개했습니다 (연구 논문: "Attention is all you need").</p>
<p>이 구조는 빠르게 다양한 자연어 처리 (NLP) 작업의 표준을 제시했습니다. 아마도 구글 번역, 구글 검색 엔진 또는 자동 완성 등에서 Transformer 모델과 상호작용해본 적이 있을 것입니다. Transformer 이전에는 재귀 신경망(RNNs)이 이러한 작업에 주로 사용되었습니다. 이제 LLM이 어떻게 작동하는지 탐구하고 NLP의 기초를 살펴보겠습니다.</p>
<div class="content-ad"></div>
<p>NLP 또는 자연어 처리는 인간 언어를 이해하고 해석하는 데 초점을 맞춘 기계 학습 분야입니다. 수학적 및 통계적 방법을 사용하여 주로 다루는 두 가지 유형의 작업이 있습니다: 분류 (예: 이메일 유형 식별) 및 회귀 (예: 인공 지능에 관한 시를 생성). 신속한 발전 덕분에 오픈 소스 LLM을 실험할 수 있는 다양한 프레임워크와 패키지가 있습니다. 인기 있는 패키지 중 하나는 Hugging Face의 Transformers입니다.</p>
<p>대형 언어 모델(LLMs)은 일반적으로 Transformer 아키텍처를 기반으로 하며 인간 언어, 코드 및 기타 데이터를 이해하고 생성하도록 설계되었습니다. 이러한 심층 학습 모델은 거대한 데이터셋(페타바이트 규모의 텍스트)에서 훈련되어 인간 언어의 뉘앙스를 포착합니다.</p>
<p>Transformer 신경망 아키텍처는 인상적이면서도 간단합니다. 다른 아키텍처에서는 어렵게 확장할 수 있는 방식으로 고도로 병렬화되고 확장할 수 있습니다. Transformer 아키텍처의 중요한 구성 요소 중 하나는 self-attention으로, 시퀀스의 각 단어가 시퀀스 내의 모든 다른 단어를 고려할 수 있게 해줍니다. 이는 장거리 종속성 및 맥락적 관계를 포착합니다. 그러나 Transformer에도 한계가 있습니다. 입력 맥락 창이 몇 단어까지 처리할 수 있는지가 한 가지 도전입니다. 다른 도전도 있지만 나중에 다룰 것입니다.</p>
<p>이 게시물은 Transformer 아키텍처의 심화 내용을 다루는 것은 아니지만 기본적인 이해가 도움이 됩니다. Transformer의 작동 방식에 대해 궁금하다면, 3Blue1Brown의 Visual Intro To Transformers라는 유튜브 비디오를 강력히 추천합니다.</p>
<div class="content-ad"></div>
<p>NLP 분야에서 가장 일반적인 하위 작업은 다음과 같습니다:</p>
<ul>
<li>분류: 텍스트, 문장 또는 단어를 분류하는 것입니다. 스팸 vs. 스팸이 아닌 이메일 식별, 문법 교정, 책의 감정 파악 또는 장르 정의 등이 예시로挙げられます. 개별 단어와 같은 더 세분화된 요소도 분류할 수 있으며, 이는 문법 태깅이나 명명된 엔티티 인식과 같은 작업으로, 각 단어에 "사람", "장소" 또는 "객체"와 같은 레이블이 지정됩니다.</li>
<li>텍스트 생성: 새로운 텍스트 콘텐츠를 생성하는 것으로, 질문에 대한 답변 생성, 언어 번역을 위한 새로운 문장 작성 또는 완전히 새로운 텍스트 작성 등을 포함합니다.</li>
</ul>
<p>Python 환경에서 트랜스포머를 사용하려면 PiP를 통해 설치하세요:</p>
<pre><code class="hljs language-js">$ pip install transformers
</code></pre>
<div class="content-ad"></div>
<p>**Transformers를 사용하는 것은 가능한 쉽습니다. 예를 들어, 분류 작업:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

classifier = pipeline(<span class="hljs-string">"text-classification"</span>)
text = <span class="hljs-string">"나는 자연어 처리를 위해 트랜스포머를 사용하는 것을 좋아합니다!"</span>
result = classifier(text)

<span class="hljs-built_in">print</span>(result)
</code></pre>
<p>다양한 파이프라인과 모델이 있습니다. 이 파이프라인 내에서 사용 가능한 일반적인 작업 중 일부는 다음과 같습니다:</p>
<ul>
<li>fill-mask: 문장에서 가리기 처리된 토큰을 예측합니다.</li>
<li>feature-extraction: 텍스트의 벡터/임베딩 표현을 제공합니다.</li>
<li>ner: 개체명 인식(Named Entity Recognition).</li>
<li>question-answering: 맥락에 기반하여 질문에 대한 답변을 제공합니다.</li>
<li>sentiment-analysis: 텍스트의 감정을 결정합니다.</li>
<li>summarization: 텍스트를 핵심 요점으로 압축합니다.</li>
<li>text-generation: 프롬프트에 기반하여 새로운 텍스트를 생성합니다.</li>
<li>translation: 한 언어에서 다른 언어로 텍스트를 번역합니다.</li>
<li>zero-shot-classification: 레이블이 지정되지 않은 텍스트를 분류합니다.</li>
</ul>
<div class="content-ad"></div>
<p>예를 들어 Named Entity Recognition을 사용하는 방법:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
ner = pipeline(<span class="hljs-string">"ner"</span>, grouped_entities=<span class="hljs-literal">True</span>)
text = <span class="hljs-string">"The Transformers library is a powerful tool for natural language processing that are used by developers."</span>
entities = ner(text)
<span class="hljs-built_in">print</span>(entities)
</code></pre>
<p>텍스트 생성: 이는 새로운 텍스트 콘텐츠를 생성하는 것을 의미하며, 예를 들어 질문에 대한 답변 생성, 언어 번역을 위한 새로운 문장 작성 또는 새로운 텍스트 작성을 포함합니다.</p>
<p>텍스트 생성 작업의 예시:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
generator = pipeline(<span class="hljs-string">"text-generation"</span>, model=<span class="hljs-string">"gpt2"</span>)

prompt = <span class="hljs-string">"한때 멀고 먼 곳에서,"</span>
generated_text = generator(prompt, max_length=<span class="hljs-number">50</span>, num_return_sequences=<span class="hljs-number">1</span>)

<span class="hljs-built_in">print</span>(generated_text)
</code></pre>
<h1>LLMs가 고수준에서 작동하는 방식:</h1>
<p>오늘날 다양한 LLMs가 제공되고 있으며, 이들의 차이를 이해하지 못하면 혼란스러울 수 있습니다. Transformers는 여러 범주로 나뉘어 특정 작업을 해결하도록 설계되었습니다.</p>
<ol>
<li>GPT와 유사한 모델들 (자기 회귀형 트랜스포머): 이러한 모델들은 한 번에 한 단어씩 텍스트를 생성하며, 각 단어는 이전에 생성된 단어에 의존합니다. 이러한 모델들은 이야기 완성 및 회화 에이전트와 같은 텍스트 생성 작업에서 뛰어납니다. 예시로 GPT-2, GPT-3, 그리고 GPT-4 등이 있습니다.</li>
</ol>
<div class="content-ad"></div>
<ol start="2">
<li>
<p>BERT와 RoBERTa와 같은 BERT와 유사한 모델들: 이러한 모델들은 문장 내 단어들의 맥락을 분석하여 텍스트를 이해하고 해석하기 위해 설계되었습니다. 이러한 모델들은 텍스트 분류, 개체명 인식, 그리고 질문에 대한 답변과 같은 작업에 탁월합니다.</p>
</li>
<li>
<p>BART와 T5와 같은 BART/T5와 유사한 모델들: 이러한 다목적 모델들은 입력 시퀀스를 출력 시퀀스로 변환하여 다양한 작업을 처리할 수 있습니다. 이러한 모델들은 텍스트 요약, 번역, 그리고 기타 변환 작업과 같은 작업에 특히 효과적입니다.</p>
</li>
</ol>
<p>이전에 언급된 Transformer 모델들(GPT, BERT, BART, 그리고 T5와 같은)은 근본적으로 언어 모델로 훈련됩니다. 이러한 모델들은 자가 지도 학습(self-supervised learning)이라는 방법을 통해 대량의 원시 텍스트 데이터에서 학습합니다. 자가 지도 학습에서는 모델이 입력 데이터로부터 자체 학습 신호를 생성하여 인간이 작성한 레이블 데이터가 필요하지 않다는 것을 의미합니다.</p>
<div class="content-ad"></div>
<p>이 교육과정에서 모델들은 노출된 언어에 대한 통계적 이해를 발전시킵니다. 그러나 이 넓은 언어 이해는 특정 실용적인 작업에 직접적으로 적용되지는 않습니다. 이러한 모델을 특정 응용 프로그램에 유용하게 만들기 위해 전이 학습이라는 프로세스를 사용합니다. 전이 학습에서는, 미리 학습된 모델을 특정 작업에 맞게 세부 조정하는데 지도 학습을 사용하며, 이는 특정 작업에 맞춘 인간이 라벨이 지정된 데이터를 포함합니다.</p>
<p>이 문맥에서의 일반적인 작업 중 하나는 인과 언어 모델링인데, 이 때 모델은 문장의 이전 단어를 기반으로 다음 단어를 예측합니다. 이 방법은 미래 입력을 고려하지 않고 과거와 현재 입력을 사용하여 예측을 생성하기 때문에 텍스트 생성 및 자동 완성과 같은 작업에 적합합니다.</p>
<p>반면에 BERT와 같은 모델에서 사용되는 마스크 언어 모델링은 문장에서 특정 단어를 가리고 모델을 훈련시켜 주변 단어에 의해 제공된 문맥에 기반해 이러한 가리기된 단어를 예측하도록 합니다. 이 방법을 통해 모델은 양방향 문맥을 이해할 수 있어서 질문 응답 및 텍스트 분류와 같은 이해 작업에 매우 효과적입니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_3.png" alt="image"></p>
<p>현대의 Transformer 모델은 매우 많은 매개변수와 저장 공간을 가지고 있습니다. 예를 들어, GPT-4 모델에는 1.76조 개의 매개변수가 있습니다! 이러한 모델을 훈련하는 데는 많은 양의 고품질 데이터와 상당한 컴퓨팅 자원이 필요하기 때문에 개별 개발자가 처음부터 이러한 모델을 훈련하는 것은 현실적이지 않습니다.</p>
<p>이러한 도전에도 불구하고 위에서 언급한 기술들, 특히 전이 학습은 특정 사용 사례에 대해 이러한 사전 훈련된 모델을 효과적으로 세밀하게 조정할 수 있도록 합니다. 세밀하게 조정은 사전 훈련된 모델의 가중치를 더 작고 작업별 데이터셋에 맞게 조정하는 것을 의미합니다. 이 과정은 처음부터 모델을 훈련하는 것에 비해 필요한 컴퓨팅 자원과 시간을 상당히 줄여줍니다.</p>
<p>전이 학습 워크플로우 예시:</p>
<div class="content-ad"></div>
<ol>
<li>
<p>Pretraining: 모델은 먼저 대용량의 텍스트 데이터 코퍼스를 사용하여 자기지도 학습으로 초기 학습됩니다. 이 단계는 모델이 문법, 구문 및 의미와 같은 일반적인 언어 기능을 학습하는 데 도움을 줍니다.</p>
</li>
<li>
<p>Fine-Tuning: 사전 학습된 모델은 그 후 특정 응용 프로그램에 특화된 작은 레이블이 지정된 데이터셋에서 세밀하게 조정됩니다. 예를 들어, 고객 리뷰 데이터셋에서 BERT를 세밀하게 조정하면 감성 분석 작업에서 성능을 향상시킬 수 있습니다.</p>
</li>
<li>
<p>평가 및 배포: 세밀하게 조정한 후 모델은 특정 작업에 대한 성능 기준을 충족하는지 확인하기 위해 평가됩니다. 확인된 후 모델은 실제 응용 프로그램에 배포될 수 있습니다.</p>
</li>
</ol>
<p>트랜스포머는 자연어 처리를 혁신했지만, 계산 비용, 에너지 소비, 방대한 데이터셋의 필요성과 같은 도전 과제가 있습니다. 연구자들은 이러한 모델을 더 효율적이고 접근성이 높게 만들기 위해 지속적으로 새로운 아키텍처 및 기술을 탐구하고 있습니다. 예를 들어, 대규모 모델의 크기를 줄이면서 성능을 유지하는 모델 증류와 계산 부담을 줄이려는 희소한 주의 메커니즘과 같은 기술은 희망적인 연구 분야입니다.</p>
<div class="content-ad"></div>
<p>요약하자면, GPT, BERT, BART, T5와 같은 Transformer 모델의 능력을 이해하고 활용하는 것은 자기 지도 학습과 전이 학습을 통해 자연어 처리에서 다양한 실용적인 응용 프로그램을 발전시킬 수 있습니다. 이러한 모델을 특정 작업에 맞게 세밀하게 조정함으로써, 계산 리소스를 많이 필요로하지 않고도 그들의 능력을 활용할 수 있습니다.</p>
<h1>고수준 Transformer 아키텍처:</h1>
<p>오늘날의 LLMs 대부분은 대부분 Transformers입니다. Transformer 모델 아키텍처는 아래에서 보여지며 (처음에는 복잡해 보여도 걱정하지 마세요):</p>
<p><img src="/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_4.png" alt="Transformer Model"></p>
<div class="content-ad"></div>
<p>그러나, 한 걸음씩 천천히 짚어 보겠습니다. 트랜스포머 아키텍처는 주로 두 가지 중요한 구성 요소로 이루어져 있습니다: 인코더(Encoder)와 디코더(Decoder):</p>
<p><img src="/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_5.png" alt="이미지"></p>
<ul>
<li>인코더(Encoder): 인코더는 정교한 독해자처럼 생각할 수 있습니다. 입력 텍스트를 가져와 해당 특징의 상세한 표현을 만들어내어 모델이 입력을 깊이 이해할 수 있도록 돕습니다. 이는 입력을 해당 특징을 캡처하는 연속적인 표현으로 인코딩하는 과정을 포함합니다.</li>
<li>디코더(Decoder): 디코더는 창의적인 작가와 같습니다. 인코더의 표현을 활용하여 다른 입력과 함께 대상 시퀀스를 생성하며, 모델이 출력 생성에 최적화되도록 합니다. 이는 텍스트 번역, 새로운 문장 생성 또는 정보 요약 등을 포함할 수 있습니다.</li>
</ul>
<p>작업에 따라 이러한 구성 요소는 독립적으로 또는 함께 사용될 수 있습니다:</p>
<div class="content-ad"></div>
<ol>
<li>
<p>인코더 전용 모델: 문장 분류 및 개체명 인식과 같이 입력의 심도 있는 이해가 필요한 작업에 이상적입니다. BERT와 RoBERTa가 대표적인 예시입니다.</p>
</li>
<li>
<p>디코더 전용 모델: 텍스트 생성과 같이 텍스트를 생성하는 것이 목표인 생성 작업에 적합합니다. GPT-2, GPT-3 및 GPT-4가 대표적인 예시입니다.</p>
</li>
<li>
<p>인코더-디코더 모델(시퀀스-투-시퀀스 모델이라고도 함): 번역 및 요약과 같이 이해와 생성이 모두 필요한 작업에 완벽합니다. BART와 T5가 대표적인 예시입니다.</p>
</li>
</ol>
<p>Transformer 아키텍처의 중요한 특징 중 하나는 그 고급 어텐션 메커니즘입니다. 이러한 메커니즘은 모델이 입력 시퀀스의 특정 부분에 집중하도록 안내하여 더 정확하고 문맥을 고려한 결과를 얻게 합니다.</p>
<div class="content-ad"></div>
<p>주의는 딥 러닝 모델(RNN, Transformer)에서 사용되는 메커니즘으로, 입력의 다른 부분에 서로 다른 가중치를 할당하여 모델이 생성, 번역 또는 감정 분석과 같은 작업을 수행하는 동안 가장 중요한 정보를 우선적으로 강조할 수 있도록 합니다. 앞서 말한대로, 주의는 입력의 다른 부분에 동적으로 초점을 맞추게 하여 성능을 개선하고 더 높은 정확도를 제공합니다.</p>
<p>주의 메커니즘에는 다음이 포함됩니다:</p>
<ul>
<li>Self-Attention(자가 주의): 문장에서 각 단어의 중요성을 다른 모든 단어에 대비하여 가중치를 부여할 수 있도록 하는 것으로, 모델이 멀리 떨어진 의존성과 관계를 포착할 수 있게 합니다. 자세한 설명은 The Illustrated Transformer를 참조하세요.</li>
<li>Multi-Head Attention(다중 헤드 주의): 여러 주의 헤드를 사용하여 모델이 동시에 입력의 다른 부분에 집중할 수 있는 능력을 향상시킵니다. 이 병렬 처리는 모델의 성능을 크게 향상시킵니다.</li>
</ul>
<p>주의에 대한 비유: 복잡한 책을 읽는 상황을 상상해 보세요. 자가 주의는 메모를 작성하고 서로 교차 참조하여 이야기를 더 잘 이해하는 것과 유사하며, 다중 헤드 주의는 여러 사람이 책을 읽고 각각 중요한 섹션을 강조하는 것처럼, 결합하면 더 풍부한 이해를 제공합니다.</p>
<div class="content-ad"></div>
<p>어떤 LLM이든 어떤 종류의 규칙을 학습하려면, 우리가 텍스트로 인식하는 것을 기계가 해석할 수 있는 형태로 변환해야 합니다. 이 과정은 임베딩/벡터화를 통해 이루어집니다. 이러한 과정의 출력물은 단어, 문장 또는 토큰들의 수학적(벡터) 표현인 임베딩입니다. 임베딩을 통해 우리는 단어, 문장을 표현하고, 벡터 거리 계산을 통해 다른 단어와의 의미론적 의미와 관계를 파악할 수 있습니다. 임베딩에는 몇 가지 유형이 있습니다: 위치적(토큰의 위치 인코딩), 토큰 임베딩(의미론), 혼합형 임베딩:</p>
<ul>
<li>위치 임베딩: 문장 내 토큰의 위치를 인코딩하여 모델이 단어의 순서를 이해하는 데 도움을 줍니다.</li>
<li>토큰 임베딩: 개별 단어나 토큰의 의미를 표현합니다.</li>
<li>혼합형 임베딩: 위치적과 토큰 임베딩을 결합하여 입력의 포괄적인 표현을 제공합니다.</li>
</ul>
<p><img src="/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_6.png" alt="이미지"></p>
<p>지금까지 우리는 Transformers, 인코더, 디코더, 인코더-디코더, 어텐션, 그리고 임베딩에 대한 기본적이고 고수준의 내용을 다뤘습니다. 그러나 LLM 사용의 가장 중요한 부분은 RLHF(인간 피드백으로부터 강화 학습)이라는 기술입니다.</p>
<div class="content-ad"></div>
<p>이 기술은 언어 모델을 사용자의 의도에 맞추는 데 사용됩니다. 이것은 강화 학습(Reinforcement Learning, RL)을 통해 이루어집니다. RLHF는 사전 학습된 LLM을 정렬하는 매커니즘으로, 인간 피드백을 활용하여 성능을 향상시키는 것으로 매우 인기가 있습니다. 이것은 LLM이 상대적으로 적은 양의 인간 피드백에서 배워 행동을 수정할 수 있도록 합니다. RLHF는 GPT-3.5, GPT-4와 같은 현대적인 LLM에서 상당한 개선을 보여주었으며, 심지어 ChatGPT 제품에서도 사용됩니다.</p>
<p>이것은 LLM과 트랜스포머에 대한 고수준 정보였습니다. 저는 트랜스포머 뒤의 세부사항과 수학을 설명하는 별도의 게시물을 가지고 있습니다. 그러나 일단은 LLM이 무엇이며 어떻게 작동하는지 명확하게 알아두는 것으로 충분합니다.</p>
<p>LLM을 실제로 활용할 수 있는 분야는 다음과 같습니다:</p>
<ul>
<li>고전적인 NLP(텍스트/단어/문장 분류)</li>
<li>한 언어에서 다른 언어로 번역</li>
<li>코드/SQL/리뷰/간단한 텍스트 생성</li>
<li>정보 검색</li>
<li>의미론적 검색</li>
<li>챗봇</li>
</ul>
<div class="content-ad"></div>
<p>요약하자면, Transformer 아키텍처는 인코더, 디코더, 고급 어텐션 메커니즘 및 임베딩을 통해 LLMs에 강력한 기반을 제공합니다. RLHF와 같은 기술들은 이러한 모델을 더욱 강화시켜 다양한 NLP 작업에 효과적으로 활용할 수 있게 만들어 줍니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"대형 언어 모델LLMs 쉽게 배우기 기초부터 시작하는 가이드","description":"","date":"2024-06-22 21:18","slug":"2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs","content":"\n\n![이미지](/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_0.png)\n\n여러분 중 많은 분들이 ChatGPT와 같은 도구를 재미있게 활용해 보신 적이 있을 것입니다. 지난 몇 년 동안 이 기술은 비즈니스 프로세스부터 일상적인 업무까지 우리 삶의 모든 측면으로 침투해 왔습니다. 그리고 알고 계세요? 이제 막 시작이라는데요. 많은 사람들이 아직 기계 학습, 신경망 또는 인공지능을 완전히 이해하지 못하고 있지만, 이제는 그것이 변화하려고 합니다. 이것은 시작점입니다. 지금 기초를 잡지 않으면 나중에 따라잡기가 더 어려울 수도 있습니다. 그러니 새로운 지식을 습득하는 것이 높은 시간입니다.\n\nOpenAI가 ChatGPT를 선보인 시점은 게임 체인저였습니다. 이를 통해 Large Language Models(LLMs)의 놀라운 파워가 드러났습니다. 이러한 모델들은 여행 계획부터 요리 레시피까지 모든 것을 변화시켰습니다. 하지만 이제 우리는 그것들을 해체하려고 합니다: 본질적으로, LLMs는 언어 처리에 뛰어난 수학적 모델들입니다.\n\n자연어 처리(Natural Language Processing, NLP)의 하위 집합인 이 기술은 기계 학습에서 주도적인 역할을 하게 되었습니다. 이제 모든 사람들이 LLMs와 NLP에 대해 배우고, 그들의 잠재력을 어떻게 활용할지에 대해 열망하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러니까, 이것을 자세히 살펴보겠습니다 — LLM이란 무엇이며 어떻게 작동하는지, 그리고 학습, 삶, 그 이상에 활용하는 방법에 대해 알아보겠습니다. 이 시리즈를 따라가기 위해 필요한 것은 기본적인 학교 수학 지식, 약간의 Python 프로그래밍 스킬, 그리고 건강한 호기심입니다. 머신러닝에 대한 기본적인 이해는 플러스이지만 필수는 아닙니다. 더 깊이 파고 싶다면 해당 정보를 모두 확인할 수 있는 추가 소스를 제공할 테니 걱정하지 마세요. 이 시리즈는 코더와 연구자뿐만 아니라 소프트웨어 엔지니어들을 위한 것입니다. 이해가 안 된다면 직접 관여하고 LLM을 실험해보세요.\n\n# 대형 언어 모델 해석\n\nLLM의 등장은 OpenAI의 ChatGPT가 화제가 되기 전에 시작되었습니다. 2017년 초에 구글 연구팀이 Transformer라는 혁신적인 딥러닝 모델 구조를 소개했습니다 (연구 논문: \"Attention is all you need\").\n\n이 구조는 빠르게 다양한 자연어 처리 (NLP) 작업의 표준을 제시했습니다. 아마도 구글 번역, 구글 검색 엔진 또는 자동 완성 등에서 Transformer 모델과 상호작용해본 적이 있을 것입니다. Transformer 이전에는 재귀 신경망(RNNs)이 이러한 작업에 주로 사용되었습니다. 이제 LLM이 어떻게 작동하는지 탐구하고 NLP의 기초를 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNLP 또는 자연어 처리는 인간 언어를 이해하고 해석하는 데 초점을 맞춘 기계 학습 분야입니다. 수학적 및 통계적 방법을 사용하여 주로 다루는 두 가지 유형의 작업이 있습니다: 분류 (예: 이메일 유형 식별) 및 회귀 (예: 인공 지능에 관한 시를 생성). 신속한 발전 덕분에 오픈 소스 LLM을 실험할 수 있는 다양한 프레임워크와 패키지가 있습니다. 인기 있는 패키지 중 하나는 Hugging Face의 Transformers입니다.\n\n대형 언어 모델(LLMs)은 일반적으로 Transformer 아키텍처를 기반으로 하며 인간 언어, 코드 및 기타 데이터를 이해하고 생성하도록 설계되었습니다. 이러한 심층 학습 모델은 거대한 데이터셋(페타바이트 규모의 텍스트)에서 훈련되어 인간 언어의 뉘앙스를 포착합니다.\n\nTransformer 신경망 아키텍처는 인상적이면서도 간단합니다. 다른 아키텍처에서는 어렵게 확장할 수 있는 방식으로 고도로 병렬화되고 확장할 수 있습니다. Transformer 아키텍처의 중요한 구성 요소 중 하나는 self-attention으로, 시퀀스의 각 단어가 시퀀스 내의 모든 다른 단어를 고려할 수 있게 해줍니다. 이는 장거리 종속성 및 맥락적 관계를 포착합니다. 그러나 Transformer에도 한계가 있습니다. 입력 맥락 창이 몇 단어까지 처리할 수 있는지가 한 가지 도전입니다. 다른 도전도 있지만 나중에 다룰 것입니다.\n\n이 게시물은 Transformer 아키텍처의 심화 내용을 다루는 것은 아니지만 기본적인 이해가 도움이 됩니다. Transformer의 작동 방식에 대해 궁금하다면, 3Blue1Brown의 Visual Intro To Transformers라는 유튜브 비디오를 강력히 추천합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNLP 분야에서 가장 일반적인 하위 작업은 다음과 같습니다:\n\n- 분류: 텍스트, 문장 또는 단어를 분류하는 것입니다. 스팸 vs. 스팸이 아닌 이메일 식별, 문법 교정, 책의 감정 파악 또는 장르 정의 등이 예시로挙げられます. 개별 단어와 같은 더 세분화된 요소도 분류할 수 있으며, 이는 문법 태깅이나 명명된 엔티티 인식과 같은 작업으로, 각 단어에 \"사람\", \"장소\" 또는 \"객체\"와 같은 레이블이 지정됩니다.\n- 텍스트 생성: 새로운 텍스트 콘텐츠를 생성하는 것으로, 질문에 대한 답변 생성, 언어 번역을 위한 새로운 문장 작성 또는 완전히 새로운 텍스트 작성 등을 포함합니다.\n\nPython 환경에서 트랜스포머를 사용하려면 PiP를 통해 설치하세요:\n\n```js\n$ pip install transformers\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**Transformers를 사용하는 것은 가능한 쉽습니다. 예를 들어, 분류 작업:\n\n```python\nfrom transformers import pipeline\n\nclassifier = pipeline(\"text-classification\")\ntext = \"나는 자연어 처리를 위해 트랜스포머를 사용하는 것을 좋아합니다!\"\nresult = classifier(text)\n\nprint(result)\n```\n\n다양한 파이프라인과 모델이 있습니다. 이 파이프라인 내에서 사용 가능한 일반적인 작업 중 일부는 다음과 같습니다:\n\n- fill-mask: 문장에서 가리기 처리된 토큰을 예측합니다.\n- feature-extraction: 텍스트의 벡터/임베딩 표현을 제공합니다.\n- ner: 개체명 인식(Named Entity Recognition).\n- question-answering: 맥락에 기반하여 질문에 대한 답변을 제공합니다.\n- sentiment-analysis: 텍스트의 감정을 결정합니다.\n- summarization: 텍스트를 핵심 요점으로 압축합니다.\n- text-generation: 프롬프트에 기반하여 새로운 텍스트를 생성합니다.\n- translation: 한 언어에서 다른 언어로 텍스트를 번역합니다.\n- zero-shot-classification: 레이블이 지정되지 않은 텍스트를 분류합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어 Named Entity Recognition을 사용하는 방법:\n\n```python\nfrom transformers import pipeline\nner = pipeline(\"ner\", grouped_entities=True)\ntext = \"The Transformers library is a powerful tool for natural language processing that are used by developers.\"\nentities = ner(text)\nprint(entities)\n```\n\n텍스트 생성: 이는 새로운 텍스트 콘텐츠를 생성하는 것을 의미하며, 예를 들어 질문에 대한 답변 생성, 언어 번역을 위한 새로운 문장 작성 또는 새로운 텍스트 작성을 포함합니다.\n\n텍스트 생성 작업의 예시:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline(\"text-generation\", model=\"gpt2\")\n\nprompt = \"한때 멀고 먼 곳에서,\"\ngenerated_text = generator(prompt, max_length=50, num_return_sequences=1)\n\nprint(generated_text)\n```\n\n# LLMs가 고수준에서 작동하는 방식:\n\n오늘날 다양한 LLMs가 제공되고 있으며, 이들의 차이를 이해하지 못하면 혼란스러울 수 있습니다. Transformers는 여러 범주로 나뉘어 특정 작업을 해결하도록 설계되었습니다.\n\n1. GPT와 유사한 모델들 (자기 회귀형 트랜스포머): 이러한 모델들은 한 번에 한 단어씩 텍스트를 생성하며, 각 단어는 이전에 생성된 단어에 의존합니다. 이러한 모델들은 이야기 완성 및 회화 에이전트와 같은 텍스트 생성 작업에서 뛰어납니다. 예시로 GPT-2, GPT-3, 그리고 GPT-4 등이 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. BERT와 RoBERTa와 같은 BERT와 유사한 모델들: 이러한 모델들은 문장 내 단어들의 맥락을 분석하여 텍스트를 이해하고 해석하기 위해 설계되었습니다. 이러한 모델들은 텍스트 분류, 개체명 인식, 그리고 질문에 대한 답변과 같은 작업에 탁월합니다.\n\n3. BART와 T5와 같은 BART/T5와 유사한 모델들: 이러한 다목적 모델들은 입력 시퀀스를 출력 시퀀스로 변환하여 다양한 작업을 처리할 수 있습니다. 이러한 모델들은 텍스트 요약, 번역, 그리고 기타 변환 작업과 같은 작업에 특히 효과적입니다.\n\n이전에 언급된 Transformer 모델들(GPT, BERT, BART, 그리고 T5와 같은)은 근본적으로 언어 모델로 훈련됩니다. 이러한 모델들은 자가 지도 학습(self-supervised learning)이라는 방법을 통해 대량의 원시 텍스트 데이터에서 학습합니다. 자가 지도 학습에서는 모델이 입력 데이터로부터 자체 학습 신호를 생성하여 인간이 작성한 레이블 데이터가 필요하지 않다는 것을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 교육과정에서 모델들은 노출된 언어에 대한 통계적 이해를 발전시킵니다. 그러나 이 넓은 언어 이해는 특정 실용적인 작업에 직접적으로 적용되지는 않습니다. 이러한 모델을 특정 응용 프로그램에 유용하게 만들기 위해 전이 학습이라는 프로세스를 사용합니다. 전이 학습에서는, 미리 학습된 모델을 특정 작업에 맞게 세부 조정하는데 지도 학습을 사용하며, 이는 특정 작업에 맞춘 인간이 라벨이 지정된 데이터를 포함합니다.\n\n이 문맥에서의 일반적인 작업 중 하나는 인과 언어 모델링인데, 이 때 모델은 문장의 이전 단어를 기반으로 다음 단어를 예측합니다. 이 방법은 미래 입력을 고려하지 않고 과거와 현재 입력을 사용하여 예측을 생성하기 때문에 텍스트 생성 및 자동 완성과 같은 작업에 적합합니다.\n\n반면에 BERT와 같은 모델에서 사용되는 마스크 언어 모델링은 문장에서 특정 단어를 가리고 모델을 훈련시켜 주변 단어에 의해 제공된 문맥에 기반해 이러한 가리기된 단어를 예측하도록 합니다. 이 방법을 통해 모델은 양방향 문맥을 이해할 수 있어서 질문 응답 및 텍스트 분류와 같은 이해 작업에 매우 효과적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_3.png)\n\n현대의 Transformer 모델은 매우 많은 매개변수와 저장 공간을 가지고 있습니다. 예를 들어, GPT-4 모델에는 1.76조 개의 매개변수가 있습니다! 이러한 모델을 훈련하는 데는 많은 양의 고품질 데이터와 상당한 컴퓨팅 자원이 필요하기 때문에 개별 개발자가 처음부터 이러한 모델을 훈련하는 것은 현실적이지 않습니다.\n\n이러한 도전에도 불구하고 위에서 언급한 기술들, 특히 전이 학습은 특정 사용 사례에 대해 이러한 사전 훈련된 모델을 효과적으로 세밀하게 조정할 수 있도록 합니다. 세밀하게 조정은 사전 훈련된 모델의 가중치를 더 작고 작업별 데이터셋에 맞게 조정하는 것을 의미합니다. 이 과정은 처음부터 모델을 훈련하는 것에 비해 필요한 컴퓨팅 자원과 시간을 상당히 줄여줍니다.\n\n전이 학습 워크플로우 예시:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1. Pretraining: 모델은 먼저 대용량의 텍스트 데이터 코퍼스를 사용하여 자기지도 학습으로 초기 학습됩니다. 이 단계는 모델이 문법, 구문 및 의미와 같은 일반적인 언어 기능을 학습하는 데 도움을 줍니다.\n\n2. Fine-Tuning: 사전 학습된 모델은 그 후 특정 응용 프로그램에 특화된 작은 레이블이 지정된 데이터셋에서 세밀하게 조정됩니다. 예를 들어, 고객 리뷰 데이터셋에서 BERT를 세밀하게 조정하면 감성 분석 작업에서 성능을 향상시킬 수 있습니다.\n\n3. 평가 및 배포: 세밀하게 조정한 후 모델은 특정 작업에 대한 성능 기준을 충족하는지 확인하기 위해 평가됩니다. 확인된 후 모델은 실제 응용 프로그램에 배포될 수 있습니다.\n\n트랜스포머는 자연어 처리를 혁신했지만, 계산 비용, 에너지 소비, 방대한 데이터셋의 필요성과 같은 도전 과제가 있습니다. 연구자들은 이러한 모델을 더 효율적이고 접근성이 높게 만들기 위해 지속적으로 새로운 아키텍처 및 기술을 탐구하고 있습니다. 예를 들어, 대규모 모델의 크기를 줄이면서 성능을 유지하는 모델 증류와 계산 부담을 줄이려는 희소한 주의 메커니즘과 같은 기술은 희망적인 연구 분야입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요약하자면, GPT, BERT, BART, T5와 같은 Transformer 모델의 능력을 이해하고 활용하는 것은 자기 지도 학습과 전이 학습을 통해 자연어 처리에서 다양한 실용적인 응용 프로그램을 발전시킬 수 있습니다. 이러한 모델을 특정 작업에 맞게 세밀하게 조정함으로써, 계산 리소스를 많이 필요로하지 않고도 그들의 능력을 활용할 수 있습니다.\n\n# 고수준 Transformer 아키텍처:\n\n오늘날의 LLMs 대부분은 대부분 Transformers입니다. Transformer 모델 아키텍처는 아래에서 보여지며 (처음에는 복잡해 보여도 걱정하지 마세요):\n\n![Transformer Model](/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나, 한 걸음씩 천천히 짚어 보겠습니다. 트랜스포머 아키텍처는 주로 두 가지 중요한 구성 요소로 이루어져 있습니다: 인코더(Encoder)와 디코더(Decoder):\n\n![이미지](/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_5.png)\n\n- 인코더(Encoder): 인코더는 정교한 독해자처럼 생각할 수 있습니다. 입력 텍스트를 가져와 해당 특징의 상세한 표현을 만들어내어 모델이 입력을 깊이 이해할 수 있도록 돕습니다. 이는 입력을 해당 특징을 캡처하는 연속적인 표현으로 인코딩하는 과정을 포함합니다.\n- 디코더(Decoder): 디코더는 창의적인 작가와 같습니다. 인코더의 표현을 활용하여 다른 입력과 함께 대상 시퀀스를 생성하며, 모델이 출력 생성에 최적화되도록 합니다. 이는 텍스트 번역, 새로운 문장 생성 또는 정보 요약 등을 포함할 수 있습니다.\n\n작업에 따라 이러한 구성 요소는 독립적으로 또는 함께 사용될 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1. 인코더 전용 모델: 문장 분류 및 개체명 인식과 같이 입력의 심도 있는 이해가 필요한 작업에 이상적입니다. BERT와 RoBERTa가 대표적인 예시입니다.\n\n2. 디코더 전용 모델: 텍스트 생성과 같이 텍스트를 생성하는 것이 목표인 생성 작업에 적합합니다. GPT-2, GPT-3 및 GPT-4가 대표적인 예시입니다.\n\n3. 인코더-디코더 모델(시퀀스-투-시퀀스 모델이라고도 함): 번역 및 요약과 같이 이해와 생성이 모두 필요한 작업에 완벽합니다. BART와 T5가 대표적인 예시입니다.\n\nTransformer 아키텍처의 중요한 특징 중 하나는 그 고급 어텐션 메커니즘입니다. 이러한 메커니즘은 모델이 입력 시퀀스의 특정 부분에 집중하도록 안내하여 더 정확하고 문맥을 고려한 결과를 얻게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주의는 딥 러닝 모델(RNN, Transformer)에서 사용되는 메커니즘으로, 입력의 다른 부분에 서로 다른 가중치를 할당하여 모델이 생성, 번역 또는 감정 분석과 같은 작업을 수행하는 동안 가장 중요한 정보를 우선적으로 강조할 수 있도록 합니다. 앞서 말한대로, 주의는 입력의 다른 부분에 동적으로 초점을 맞추게 하여 성능을 개선하고 더 높은 정확도를 제공합니다.\n\n주의 메커니즘에는 다음이 포함됩니다:\n\n- Self-Attention(자가 주의): 문장에서 각 단어의 중요성을 다른 모든 단어에 대비하여 가중치를 부여할 수 있도록 하는 것으로, 모델이 멀리 떨어진 의존성과 관계를 포착할 수 있게 합니다. 자세한 설명은 The Illustrated Transformer를 참조하세요.\n- Multi-Head Attention(다중 헤드 주의): 여러 주의 헤드를 사용하여 모델이 동시에 입력의 다른 부분에 집중할 수 있는 능력을 향상시킵니다. 이 병렬 처리는 모델의 성능을 크게 향상시킵니다.\n\n주의에 대한 비유: 복잡한 책을 읽는 상황을 상상해 보세요. 자가 주의는 메모를 작성하고 서로 교차 참조하여 이야기를 더 잘 이해하는 것과 유사하며, 다중 헤드 주의는 여러 사람이 책을 읽고 각각 중요한 섹션을 강조하는 것처럼, 결합하면 더 풍부한 이해를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떤 LLM이든 어떤 종류의 규칙을 학습하려면, 우리가 텍스트로 인식하는 것을 기계가 해석할 수 있는 형태로 변환해야 합니다. 이 과정은 임베딩/벡터화를 통해 이루어집니다. 이러한 과정의 출력물은 단어, 문장 또는 토큰들의 수학적(벡터) 표현인 임베딩입니다. 임베딩을 통해 우리는 단어, 문장을 표현하고, 벡터 거리 계산을 통해 다른 단어와의 의미론적 의미와 관계를 파악할 수 있습니다. 임베딩에는 몇 가지 유형이 있습니다: 위치적(토큰의 위치 인코딩), 토큰 임베딩(의미론), 혼합형 임베딩:\n\n- 위치 임베딩: 문장 내 토큰의 위치를 인코딩하여 모델이 단어의 순서를 이해하는 데 도움을 줍니다.\n- 토큰 임베딩: 개별 단어나 토큰의 의미를 표현합니다.\n- 혼합형 임베딩: 위치적과 토큰 임베딩을 결합하여 입력의 포괄적인 표현을 제공합니다.\n\n![이미지](/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_6.png)\n\n지금까지 우리는 Transformers, 인코더, 디코더, 인코더-디코더, 어텐션, 그리고 임베딩에 대한 기본적이고 고수준의 내용을 다뤘습니다. 그러나 LLM 사용의 가장 중요한 부분은 RLHF(인간 피드백으로부터 강화 학습)이라는 기술입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기술은 언어 모델을 사용자의 의도에 맞추는 데 사용됩니다. 이것은 강화 학습(Reinforcement Learning, RL)을 통해 이루어집니다. RLHF는 사전 학습된 LLM을 정렬하는 매커니즘으로, 인간 피드백을 활용하여 성능을 향상시키는 것으로 매우 인기가 있습니다. 이것은 LLM이 상대적으로 적은 양의 인간 피드백에서 배워 행동을 수정할 수 있도록 합니다. RLHF는 GPT-3.5, GPT-4와 같은 현대적인 LLM에서 상당한 개선을 보여주었으며, 심지어 ChatGPT 제품에서도 사용됩니다.\n\n이것은 LLM과 트랜스포머에 대한 고수준 정보였습니다. 저는 트랜스포머 뒤의 세부사항과 수학을 설명하는 별도의 게시물을 가지고 있습니다. 그러나 일단은 LLM이 무엇이며 어떻게 작동하는지 명확하게 알아두는 것으로 충분합니다.\n\nLLM을 실제로 활용할 수 있는 분야는 다음과 같습니다:\n\n- 고전적인 NLP(텍스트/단어/문장 분류)\n- 한 언어에서 다른 언어로 번역\n- 코드/SQL/리뷰/간단한 텍스트 생성\n- 정보 검색\n- 의미론적 검색\n- 챗봇\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요약하자면, Transformer 아키텍처는 인코더, 디코더, 고급 어텐션 메커니즘 및 임베딩을 통해 LLMs에 강력한 기반을 제공합니다. RLHF와 같은 기술들은 이러한 모델을 더욱 강화시켜 다양한 NLP 작업에 효과적으로 활용할 수 있게 만들어 줍니다.","ogImage":{"url":"/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_0.png"},"coverImage":"/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_0.png","tag":["Tech"],"readingTime":11},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e여러분 중 많은 분들이 ChatGPT와 같은 도구를 재미있게 활용해 보신 적이 있을 것입니다. 지난 몇 년 동안 이 기술은 비즈니스 프로세스부터 일상적인 업무까지 우리 삶의 모든 측면으로 침투해 왔습니다. 그리고 알고 계세요? 이제 막 시작이라는데요. 많은 사람들이 아직 기계 학습, 신경망 또는 인공지능을 완전히 이해하지 못하고 있지만, 이제는 그것이 변화하려고 합니다. 이것은 시작점입니다. 지금 기초를 잡지 않으면 나중에 따라잡기가 더 어려울 수도 있습니다. 그러니 새로운 지식을 습득하는 것이 높은 시간입니다.\u003c/p\u003e\n\u003cp\u003eOpenAI가 ChatGPT를 선보인 시점은 게임 체인저였습니다. 이를 통해 Large Language Models(LLMs)의 놀라운 파워가 드러났습니다. 이러한 모델들은 여행 계획부터 요리 레시피까지 모든 것을 변화시켰습니다. 하지만 이제 우리는 그것들을 해체하려고 합니다: 본질적으로, LLMs는 언어 처리에 뛰어난 수학적 모델들입니다.\u003c/p\u003e\n\u003cp\u003e자연어 처리(Natural Language Processing, NLP)의 하위 집합인 이 기술은 기계 학습에서 주도적인 역할을 하게 되었습니다. 이제 모든 사람들이 LLMs와 NLP에 대해 배우고, 그들의 잠재력을 어떻게 활용할지에 대해 열망하고 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그러니까, 이것을 자세히 살펴보겠습니다 — LLM이란 무엇이며 어떻게 작동하는지, 그리고 학습, 삶, 그 이상에 활용하는 방법에 대해 알아보겠습니다. 이 시리즈를 따라가기 위해 필요한 것은 기본적인 학교 수학 지식, 약간의 Python 프로그래밍 스킬, 그리고 건강한 호기심입니다. 머신러닝에 대한 기본적인 이해는 플러스이지만 필수는 아닙니다. 더 깊이 파고 싶다면 해당 정보를 모두 확인할 수 있는 추가 소스를 제공할 테니 걱정하지 마세요. 이 시리즈는 코더와 연구자뿐만 아니라 소프트웨어 엔지니어들을 위한 것입니다. 이해가 안 된다면 직접 관여하고 LLM을 실험해보세요.\u003c/p\u003e\n\u003ch1\u003e대형 언어 모델 해석\u003c/h1\u003e\n\u003cp\u003eLLM의 등장은 OpenAI의 ChatGPT가 화제가 되기 전에 시작되었습니다. 2017년 초에 구글 연구팀이 Transformer라는 혁신적인 딥러닝 모델 구조를 소개했습니다 (연구 논문: \"Attention is all you need\").\u003c/p\u003e\n\u003cp\u003e이 구조는 빠르게 다양한 자연어 처리 (NLP) 작업의 표준을 제시했습니다. 아마도 구글 번역, 구글 검색 엔진 또는 자동 완성 등에서 Transformer 모델과 상호작용해본 적이 있을 것입니다. Transformer 이전에는 재귀 신경망(RNNs)이 이러한 작업에 주로 사용되었습니다. 이제 LLM이 어떻게 작동하는지 탐구하고 NLP의 기초를 살펴보겠습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eNLP 또는 자연어 처리는 인간 언어를 이해하고 해석하는 데 초점을 맞춘 기계 학습 분야입니다. 수학적 및 통계적 방법을 사용하여 주로 다루는 두 가지 유형의 작업이 있습니다: 분류 (예: 이메일 유형 식별) 및 회귀 (예: 인공 지능에 관한 시를 생성). 신속한 발전 덕분에 오픈 소스 LLM을 실험할 수 있는 다양한 프레임워크와 패키지가 있습니다. 인기 있는 패키지 중 하나는 Hugging Face의 Transformers입니다.\u003c/p\u003e\n\u003cp\u003e대형 언어 모델(LLMs)은 일반적으로 Transformer 아키텍처를 기반으로 하며 인간 언어, 코드 및 기타 데이터를 이해하고 생성하도록 설계되었습니다. 이러한 심층 학습 모델은 거대한 데이터셋(페타바이트 규모의 텍스트)에서 훈련되어 인간 언어의 뉘앙스를 포착합니다.\u003c/p\u003e\n\u003cp\u003eTransformer 신경망 아키텍처는 인상적이면서도 간단합니다. 다른 아키텍처에서는 어렵게 확장할 수 있는 방식으로 고도로 병렬화되고 확장할 수 있습니다. Transformer 아키텍처의 중요한 구성 요소 중 하나는 self-attention으로, 시퀀스의 각 단어가 시퀀스 내의 모든 다른 단어를 고려할 수 있게 해줍니다. 이는 장거리 종속성 및 맥락적 관계를 포착합니다. 그러나 Transformer에도 한계가 있습니다. 입력 맥락 창이 몇 단어까지 처리할 수 있는지가 한 가지 도전입니다. 다른 도전도 있지만 나중에 다룰 것입니다.\u003c/p\u003e\n\u003cp\u003e이 게시물은 Transformer 아키텍처의 심화 내용을 다루는 것은 아니지만 기본적인 이해가 도움이 됩니다. Transformer의 작동 방식에 대해 궁금하다면, 3Blue1Brown의 Visual Intro To Transformers라는 유튜브 비디오를 강력히 추천합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eNLP 분야에서 가장 일반적인 하위 작업은 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e분류: 텍스트, 문장 또는 단어를 분류하는 것입니다. 스팸 vs. 스팸이 아닌 이메일 식별, 문법 교정, 책의 감정 파악 또는 장르 정의 등이 예시로挙げられます. 개별 단어와 같은 더 세분화된 요소도 분류할 수 있으며, 이는 문법 태깅이나 명명된 엔티티 인식과 같은 작업으로, 각 단어에 \"사람\", \"장소\" 또는 \"객체\"와 같은 레이블이 지정됩니다.\u003c/li\u003e\n\u003cli\u003e텍스트 생성: 새로운 텍스트 콘텐츠를 생성하는 것으로, 질문에 대한 답변 생성, 언어 번역을 위한 새로운 문장 작성 또는 완전히 새로운 텍스트 작성 등을 포함합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePython 환경에서 트랜스포머를 사용하려면 PiP를 통해 설치하세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e$ pip install transformers\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e**Transformers를 사용하는 것은 가능한 쉽습니다. 예를 들어, 분류 작업:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pipeline\n\nclassifier = pipeline(\u003cspan class=\"hljs-string\"\u003e\"text-classification\"\u003c/span\u003e)\ntext = \u003cspan class=\"hljs-string\"\u003e\"나는 자연어 처리를 위해 트랜스포머를 사용하는 것을 좋아합니다!\"\u003c/span\u003e\nresult = classifier(text)\n\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(result)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e다양한 파이프라인과 모델이 있습니다. 이 파이프라인 내에서 사용 가능한 일반적인 작업 중 일부는 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003efill-mask: 문장에서 가리기 처리된 토큰을 예측합니다.\u003c/li\u003e\n\u003cli\u003efeature-extraction: 텍스트의 벡터/임베딩 표현을 제공합니다.\u003c/li\u003e\n\u003cli\u003ener: 개체명 인식(Named Entity Recognition).\u003c/li\u003e\n\u003cli\u003equestion-answering: 맥락에 기반하여 질문에 대한 답변을 제공합니다.\u003c/li\u003e\n\u003cli\u003esentiment-analysis: 텍스트의 감정을 결정합니다.\u003c/li\u003e\n\u003cli\u003esummarization: 텍스트를 핵심 요점으로 압축합니다.\u003c/li\u003e\n\u003cli\u003etext-generation: 프롬프트에 기반하여 새로운 텍스트를 생성합니다.\u003c/li\u003e\n\u003cli\u003etranslation: 한 언어에서 다른 언어로 텍스트를 번역합니다.\u003c/li\u003e\n\u003cli\u003ezero-shot-classification: 레이블이 지정되지 않은 텍스트를 분류합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e예를 들어 Named Entity Recognition을 사용하는 방법:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pipeline\nner = pipeline(\u003cspan class=\"hljs-string\"\u003e\"ner\"\u003c/span\u003e, grouped_entities=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\ntext = \u003cspan class=\"hljs-string\"\u003e\"The Transformers library is a powerful tool for natural language processing that are used by developers.\"\u003c/span\u003e\nentities = ner(text)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(entities)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e텍스트 생성: 이는 새로운 텍스트 콘텐츠를 생성하는 것을 의미하며, 예를 들어 질문에 대한 답변 생성, 언어 번역을 위한 새로운 문장 작성 또는 새로운 텍스트 작성을 포함합니다.\u003c/p\u003e\n\u003cp\u003e텍스트 생성 작업의 예시:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pipeline\ngenerator = pipeline(\u003cspan class=\"hljs-string\"\u003e\"text-generation\"\u003c/span\u003e, model=\u003cspan class=\"hljs-string\"\u003e\"gpt2\"\u003c/span\u003e)\n\nprompt = \u003cspan class=\"hljs-string\"\u003e\"한때 멀고 먼 곳에서,\"\u003c/span\u003e\ngenerated_text = generator(prompt, max_length=\u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e, num_return_sequences=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(generated_text)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003eLLMs가 고수준에서 작동하는 방식:\u003c/h1\u003e\n\u003cp\u003e오늘날 다양한 LLMs가 제공되고 있으며, 이들의 차이를 이해하지 못하면 혼란스러울 수 있습니다. Transformers는 여러 범주로 나뉘어 특정 작업을 해결하도록 설계되었습니다.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGPT와 유사한 모델들 (자기 회귀형 트랜스포머): 이러한 모델들은 한 번에 한 단어씩 텍스트를 생성하며, 각 단어는 이전에 생성된 단어에 의존합니다. 이러한 모델들은 이야기 완성 및 회화 에이전트와 같은 텍스트 생성 작업에서 뛰어납니다. 예시로 GPT-2, GPT-3, 그리고 GPT-4 등이 있습니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\n\u003cp\u003eBERT와 RoBERTa와 같은 BERT와 유사한 모델들: 이러한 모델들은 문장 내 단어들의 맥락을 분석하여 텍스트를 이해하고 해석하기 위해 설계되었습니다. 이러한 모델들은 텍스트 분류, 개체명 인식, 그리고 질문에 대한 답변과 같은 작업에 탁월합니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBART와 T5와 같은 BART/T5와 유사한 모델들: 이러한 다목적 모델들은 입력 시퀀스를 출력 시퀀스로 변환하여 다양한 작업을 처리할 수 있습니다. 이러한 모델들은 텍스트 요약, 번역, 그리고 기타 변환 작업과 같은 작업에 특히 효과적입니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e이전에 언급된 Transformer 모델들(GPT, BERT, BART, 그리고 T5와 같은)은 근본적으로 언어 모델로 훈련됩니다. 이러한 모델들은 자가 지도 학습(self-supervised learning)이라는 방법을 통해 대량의 원시 텍스트 데이터에서 학습합니다. 자가 지도 학습에서는 모델이 입력 데이터로부터 자체 학습 신호를 생성하여 인간이 작성한 레이블 데이터가 필요하지 않다는 것을 의미합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 교육과정에서 모델들은 노출된 언어에 대한 통계적 이해를 발전시킵니다. 그러나 이 넓은 언어 이해는 특정 실용적인 작업에 직접적으로 적용되지는 않습니다. 이러한 모델을 특정 응용 프로그램에 유용하게 만들기 위해 전이 학습이라는 프로세스를 사용합니다. 전이 학습에서는, 미리 학습된 모델을 특정 작업에 맞게 세부 조정하는데 지도 학습을 사용하며, 이는 특정 작업에 맞춘 인간이 라벨이 지정된 데이터를 포함합니다.\u003c/p\u003e\n\u003cp\u003e이 문맥에서의 일반적인 작업 중 하나는 인과 언어 모델링인데, 이 때 모델은 문장의 이전 단어를 기반으로 다음 단어를 예측합니다. 이 방법은 미래 입력을 고려하지 않고 과거와 현재 입력을 사용하여 예측을 생성하기 때문에 텍스트 생성 및 자동 완성과 같은 작업에 적합합니다.\u003c/p\u003e\n\u003cp\u003e반면에 BERT와 같은 모델에서 사용되는 마스크 언어 모델링은 문장에서 특정 단어를 가리고 모델을 훈련시켜 주변 단어에 의해 제공된 문맥에 기반해 이러한 가리기된 단어를 예측하도록 합니다. 이 방법을 통해 모델은 양방향 문맥을 이해할 수 있어서 질문 응답 및 텍스트 분류와 같은 이해 작업에 매우 효과적입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_3.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e현대의 Transformer 모델은 매우 많은 매개변수와 저장 공간을 가지고 있습니다. 예를 들어, GPT-4 모델에는 1.76조 개의 매개변수가 있습니다! 이러한 모델을 훈련하는 데는 많은 양의 고품질 데이터와 상당한 컴퓨팅 자원이 필요하기 때문에 개별 개발자가 처음부터 이러한 모델을 훈련하는 것은 현실적이지 않습니다.\u003c/p\u003e\n\u003cp\u003e이러한 도전에도 불구하고 위에서 언급한 기술들, 특히 전이 학습은 특정 사용 사례에 대해 이러한 사전 훈련된 모델을 효과적으로 세밀하게 조정할 수 있도록 합니다. 세밀하게 조정은 사전 훈련된 모델의 가중치를 더 작고 작업별 데이터셋에 맞게 조정하는 것을 의미합니다. 이 과정은 처음부터 모델을 훈련하는 것에 비해 필요한 컴퓨팅 자원과 시간을 상당히 줄여줍니다.\u003c/p\u003e\n\u003cp\u003e전이 학습 워크플로우 예시:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003ePretraining: 모델은 먼저 대용량의 텍스트 데이터 코퍼스를 사용하여 자기지도 학습으로 초기 학습됩니다. 이 단계는 모델이 문법, 구문 및 의미와 같은 일반적인 언어 기능을 학습하는 데 도움을 줍니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFine-Tuning: 사전 학습된 모델은 그 후 특정 응용 프로그램에 특화된 작은 레이블이 지정된 데이터셋에서 세밀하게 조정됩니다. 예를 들어, 고객 리뷰 데이터셋에서 BERT를 세밀하게 조정하면 감성 분석 작업에서 성능을 향상시킬 수 있습니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e평가 및 배포: 세밀하게 조정한 후 모델은 특정 작업에 대한 성능 기준을 충족하는지 확인하기 위해 평가됩니다. 확인된 후 모델은 실제 응용 프로그램에 배포될 수 있습니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e트랜스포머는 자연어 처리를 혁신했지만, 계산 비용, 에너지 소비, 방대한 데이터셋의 필요성과 같은 도전 과제가 있습니다. 연구자들은 이러한 모델을 더 효율적이고 접근성이 높게 만들기 위해 지속적으로 새로운 아키텍처 및 기술을 탐구하고 있습니다. 예를 들어, 대규모 모델의 크기를 줄이면서 성능을 유지하는 모델 증류와 계산 부담을 줄이려는 희소한 주의 메커니즘과 같은 기술은 희망적인 연구 분야입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e요약하자면, GPT, BERT, BART, T5와 같은 Transformer 모델의 능력을 이해하고 활용하는 것은 자기 지도 학습과 전이 학습을 통해 자연어 처리에서 다양한 실용적인 응용 프로그램을 발전시킬 수 있습니다. 이러한 모델을 특정 작업에 맞게 세밀하게 조정함으로써, 계산 리소스를 많이 필요로하지 않고도 그들의 능력을 활용할 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e고수준 Transformer 아키텍처:\u003c/h1\u003e\n\u003cp\u003e오늘날의 LLMs 대부분은 대부분 Transformers입니다. Transformer 모델 아키텍처는 아래에서 보여지며 (처음에는 복잡해 보여도 걱정하지 마세요):\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_4.png\" alt=\"Transformer Model\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그러나, 한 걸음씩 천천히 짚어 보겠습니다. 트랜스포머 아키텍처는 주로 두 가지 중요한 구성 요소로 이루어져 있습니다: 인코더(Encoder)와 디코더(Decoder):\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_5.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e인코더(Encoder): 인코더는 정교한 독해자처럼 생각할 수 있습니다. 입력 텍스트를 가져와 해당 특징의 상세한 표현을 만들어내어 모델이 입력을 깊이 이해할 수 있도록 돕습니다. 이는 입력을 해당 특징을 캡처하는 연속적인 표현으로 인코딩하는 과정을 포함합니다.\u003c/li\u003e\n\u003cli\u003e디코더(Decoder): 디코더는 창의적인 작가와 같습니다. 인코더의 표현을 활용하여 다른 입력과 함께 대상 시퀀스를 생성하며, 모델이 출력 생성에 최적화되도록 합니다. 이는 텍스트 번역, 새로운 문장 생성 또는 정보 요약 등을 포함할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e작업에 따라 이러한 구성 요소는 독립적으로 또는 함께 사용될 수 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e인코더 전용 모델: 문장 분류 및 개체명 인식과 같이 입력의 심도 있는 이해가 필요한 작업에 이상적입니다. BERT와 RoBERTa가 대표적인 예시입니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e디코더 전용 모델: 텍스트 생성과 같이 텍스트를 생성하는 것이 목표인 생성 작업에 적합합니다. GPT-2, GPT-3 및 GPT-4가 대표적인 예시입니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e인코더-디코더 모델(시퀀스-투-시퀀스 모델이라고도 함): 번역 및 요약과 같이 이해와 생성이 모두 필요한 작업에 완벽합니다. BART와 T5가 대표적인 예시입니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTransformer 아키텍처의 중요한 특징 중 하나는 그 고급 어텐션 메커니즘입니다. 이러한 메커니즘은 모델이 입력 시퀀스의 특정 부분에 집중하도록 안내하여 더 정확하고 문맥을 고려한 결과를 얻게 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e주의는 딥 러닝 모델(RNN, Transformer)에서 사용되는 메커니즘으로, 입력의 다른 부분에 서로 다른 가중치를 할당하여 모델이 생성, 번역 또는 감정 분석과 같은 작업을 수행하는 동안 가장 중요한 정보를 우선적으로 강조할 수 있도록 합니다. 앞서 말한대로, 주의는 입력의 다른 부분에 동적으로 초점을 맞추게 하여 성능을 개선하고 더 높은 정확도를 제공합니다.\u003c/p\u003e\n\u003cp\u003e주의 메커니즘에는 다음이 포함됩니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSelf-Attention(자가 주의): 문장에서 각 단어의 중요성을 다른 모든 단어에 대비하여 가중치를 부여할 수 있도록 하는 것으로, 모델이 멀리 떨어진 의존성과 관계를 포착할 수 있게 합니다. 자세한 설명은 The Illustrated Transformer를 참조하세요.\u003c/li\u003e\n\u003cli\u003eMulti-Head Attention(다중 헤드 주의): 여러 주의 헤드를 사용하여 모델이 동시에 입력의 다른 부분에 집중할 수 있는 능력을 향상시킵니다. 이 병렬 처리는 모델의 성능을 크게 향상시킵니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e주의에 대한 비유: 복잡한 책을 읽는 상황을 상상해 보세요. 자가 주의는 메모를 작성하고 서로 교차 참조하여 이야기를 더 잘 이해하는 것과 유사하며, 다중 헤드 주의는 여러 사람이 책을 읽고 각각 중요한 섹션을 강조하는 것처럼, 결합하면 더 풍부한 이해를 제공합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e어떤 LLM이든 어떤 종류의 규칙을 학습하려면, 우리가 텍스트로 인식하는 것을 기계가 해석할 수 있는 형태로 변환해야 합니다. 이 과정은 임베딩/벡터화를 통해 이루어집니다. 이러한 과정의 출력물은 단어, 문장 또는 토큰들의 수학적(벡터) 표현인 임베딩입니다. 임베딩을 통해 우리는 단어, 문장을 표현하고, 벡터 거리 계산을 통해 다른 단어와의 의미론적 의미와 관계를 파악할 수 있습니다. 임베딩에는 몇 가지 유형이 있습니다: 위치적(토큰의 위치 인코딩), 토큰 임베딩(의미론), 혼합형 임베딩:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e위치 임베딩: 문장 내 토큰의 위치를 인코딩하여 모델이 단어의 순서를 이해하는 데 도움을 줍니다.\u003c/li\u003e\n\u003cli\u003e토큰 임베딩: 개별 단어나 토큰의 의미를 표현합니다.\u003c/li\u003e\n\u003cli\u003e혼합형 임베딩: 위치적과 토큰 임베딩을 결합하여 입력의 포괄적인 표현을 제공합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs_6.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e지금까지 우리는 Transformers, 인코더, 디코더, 인코더-디코더, 어텐션, 그리고 임베딩에 대한 기본적이고 고수준의 내용을 다뤘습니다. 그러나 LLM 사용의 가장 중요한 부분은 RLHF(인간 피드백으로부터 강화 학습)이라는 기술입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 기술은 언어 모델을 사용자의 의도에 맞추는 데 사용됩니다. 이것은 강화 학습(Reinforcement Learning, RL)을 통해 이루어집니다. RLHF는 사전 학습된 LLM을 정렬하는 매커니즘으로, 인간 피드백을 활용하여 성능을 향상시키는 것으로 매우 인기가 있습니다. 이것은 LLM이 상대적으로 적은 양의 인간 피드백에서 배워 행동을 수정할 수 있도록 합니다. RLHF는 GPT-3.5, GPT-4와 같은 현대적인 LLM에서 상당한 개선을 보여주었으며, 심지어 ChatGPT 제품에서도 사용됩니다.\u003c/p\u003e\n\u003cp\u003e이것은 LLM과 트랜스포머에 대한 고수준 정보였습니다. 저는 트랜스포머 뒤의 세부사항과 수학을 설명하는 별도의 게시물을 가지고 있습니다. 그러나 일단은 LLM이 무엇이며 어떻게 작동하는지 명확하게 알아두는 것으로 충분합니다.\u003c/p\u003e\n\u003cp\u003eLLM을 실제로 활용할 수 있는 분야는 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e고전적인 NLP(텍스트/단어/문장 분류)\u003c/li\u003e\n\u003cli\u003e한 언어에서 다른 언어로 번역\u003c/li\u003e\n\u003cli\u003e코드/SQL/리뷰/간단한 텍스트 생성\u003c/li\u003e\n\u003cli\u003e정보 검색\u003c/li\u003e\n\u003cli\u003e의미론적 검색\u003c/li\u003e\n\u003cli\u003e챗봇\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e요약하자면, Transformer 아키텍처는 인코더, 디코더, 고급 어텐션 메커니즘 및 임베딩을 통해 LLMs에 강력한 기반을 제공합니다. RLHF와 같은 기술들은 이러한 모델을 더욱 강화시켜 다양한 NLP 작업에 효과적으로 활용할 수 있게 만들어 줍니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-22-AGentleIntroductiontoLargeLanguageModelsLLMs"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>