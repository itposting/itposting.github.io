<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ROS2 겸손한 이미지 세분화 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-20-ROS2HumbleImageSegmentation" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ROS2 겸손한 이미지 세분화 | itposting" data-gatsby-head="true"/><meta property="og:title" content="ROS2 겸손한 이미지 세분화 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-20-ROS2HumbleImageSegmentation_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-20-ROS2HumbleImageSegmentation" data-gatsby-head="true"/><meta name="twitter:title" content="ROS2 겸손한 이미지 세분화 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-20-ROS2HumbleImageSegmentation_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-20 17:50" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-0fd008072af5a644.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_buildManifest.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">ROS2 겸손한 이미지 세분화</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="ROS2 겸손한 이미지 세분화" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 20, 2024</span><span class="posts_reading_time__f7YPP">4<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-20-ROS2HumbleImageSegmentation&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h1>소개</h1>
<p>이 튜토리얼에서는 DeepLabV3 모델을 사용하여 이미지의 의미론적 세분화를 수행하는 ROS2 노드를 만들 것입니다. ResNet-101 백본을 사용한 이 모델은 의미론적 세분할 작업에 대한 최신 아키텍처입니다. 노드는 웹캠 피드를 구독하고 이미지를 처리한 후 세그멘테이션을 수행하고 세분화된 이미지를 게시할 것입니다.</p>
<h2>준비물</h2>
<p>아래 설치되어 있는지 확인하세요:</p>
<ul>
<li>ROS2 (Humble)</li>
<li>PyTorch</li>
<li>OpenCV</li>
<li>cv_bridge (ROS 이미지와 OpenCV 이미지 간 변환을 위한)</li>
<li>torchvision (사전 학습 모델과 이미지 변환을 위한)</li>
</ul>
<h2>DeepLabV3</h2>
<p>DeepLabV3은 시맨틱 세그멘테이션 작업을 위해 설계된 최신 딥러닝 모델입니다. 시맨틱 세그멘테이션은 이미지의 각 픽셀을 미리 정의된 범주로 분류하는 작업을 포함합니다. 물체 감지와 달리 물체를 식별하고 주위에 바운딩 상자를 넣는 대신, 시맨틱 세그멘테이션은 장면에 대한 상세하고 픽셀 수준의 이해를 제공합니다.</p>
<h2>DeepLabV3의 주요 기능</h2>
<p>ResNet 백본: DeepLabV3은 특성 추출을 위해 ResNet-101을 사용합니다. ResNet-101은 잔차 학습 방식으로 유명한 강력하고 깊은 신경망으로, 사라져가는 그래디언트 문제를 해결하여 매우 깊은 네트워크를 효율적으로 훈련할 수 있으며 견고한 특성 추출을 보장합니다.</p>
<h2>DeepLabV3 작동 방식</h2>
<ul>
<li>입력 이미지: 고정된 크기(예: 512x512 픽셀)의 입력 이미지로 프로세스가 시작됩니다.</li>
<li>특성 추출: 입력 이미지는 ResNet-101 백본을 통해 전달됩니다. 이 네트워크는 ImageNet과 같은 대규모 데이터셋에서 사전 훈련되어 다양한 시각적 특성에 대한 견고한 이해력을 갖추고 있습니다.</li>
<li>어트러스 합성곱 레이어: 초기 특성 추출 후, 모델은 다양한 확장률을 가진 일련의 어트러스 합성곱을 적용합니다. 이 단계를 통해 모델은 다양한 크기의 객체를 분할하는 데 중요한 다중 스케일의 특성을 캡처할 수 있습니다.</li>
<li>공간 피라미드 풀링: 어트러스 합성곱의 출력은 공간 피라미드 풀링 모듈로 전달됩니다. 이 모듈은 다양한 스케일에서 특성을 풀링하여 이미지의 풍부한 다중 문맥적 표현을 제공합니다.</li>
<li>분할 맵: 마지막으로, 풀링된 특성은 원본 이미지 해상도로 업샘플링되고 최종 합성곱 레이어가 분할 맵을 생성합니다. 이 맵의 각 픽셀에는 클래스 레이블이 지정되어 이미지의 상세한 세그멘테이션이 이루어집니다.</li>
</ul>
<h2>DeepLabV3</h2>
<ul>
<li>Architecture: DeepLabV3는 시멘틱 이미지 세그멘테이션을 위해 설계된 딥러닝 모델입니다. Dilated (확장된) 합성곱을 활용하여 수용 영역을 확대시키면서도 공간 해상도를 유지하는 방식으로 다중 스케일 문맥을 캡처합니다.</li>
<li>Backbone: 여기서 사용된 백본은 ResNet-101이며, 보다 복잡한 표현을 학습하는 데 도움이 되는 깊은 잔여 네트워크입니다.</li>
<li>데이터 세트: 모델은 COCO 데이터 세트에서 사전 훈련을 받고 VOC 라벨로 세부 조정되어 이러한 데이터 세트에서 발견되는 공통 객체를 세분화할 수 있습니다.</li>
</ul>
<h2>모델 로딩 코드</h2>
<pre><code class="hljs language-js">self.<span class="hljs-property">model</span> = torch.<span class="hljs-property">hub</span>.<span class="hljs-title function_">load</span>(<span class="hljs-string">'pytorch/vision:v0.10.0'</span>, <span class="hljs-string">'deeplabv3_resnet101'</span>, weights=<span class="hljs-string">'DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1'</span>)
</code></pre>
<h2>코드 단계</h2>
<ul>
<li>초기화: 모델은 SegmentationNode 클래스의 <strong>init</strong> 메서드에서 초기화됩니다. torch.hub.load 메서드는 미리 훈련된 deeplabv3_resnet101 모델을 불러옵니다.</li>
<li>전처리: 입력 이미지는 모델의 입력 요구 사항과 일치하도록 크기 조정, 중앙 자르기 및 정규화됩니다.</li>
<li>추론: 이미지가 수신될 때 콜백에서 모델은 전처리된 이미지 텐서에 대해 추론을 수행합니다.</li>
<li>후처리: 모델의 출력은 각 픽셀의 클래스 점수를 포함하는 텐서입니다. 각 픽셀에서 가장 높은 점수가 클래스를 결정합니다. 그런 다음 결과는 PASCAL VOC 컬러 맵을 사용하여 컬러맵으로 변환된 세그멘테이션 이미지로 변환됩니다.</li>
</ul>
<p><img src="/assets/img/2024-06-20-ROS2HumbleImageSegmentation_2.png" alt="image"></p>
<h2>어플리케이션: 시맨틱 세그멘테이션을 위한 ROS2 노드</h2>
<p>실용적인 어플리케이션에서 DeepLabV3를 활용하기 위해 웹캠 이미지로부터 시맨틱 세그멘테이션을 수행하는 ROS2 (로봇 운영 시스템 2) 노드를 만들 수 있습니다. 다음은 단계별 개요입니다:</p>
<ul>
<li>노드 초기화: 웹캠 이미지 토픽을 구독하는 ROS2 노드를 초기화합니다.</li>
<li>이미지 전처리: 들어오는 이미지를 DeepLabV3 모델이 필요로 하는 형식으로 변환합니다. 일반적으로 이미지 크기 조정 및 정규화를 수행합니다.</li>
<li>모델 추론: 전처리된 이미지를 DeepLabV3 모델을 통해 전파하여 세그멘테이션 맵을 얻습니다.</li>
<li>후처리: 세그멘테이션 맵을 원본 이미지 해상도에 맞게 업샘플링하고 클래스 레이블을 시각적으로 식별 가능한 색상으로 변환합니다.</li>
<li>결과 게시: 세그멘트된 이미지를 시각화나 추가 처리를 위해 ROS2 토픽에 게시합니다.</li>
</ul>
<h2>사용법</h2>
<p>이 모델은 웹캠에서 수신한 이미지를 세분화하는 데 사용되며, COCO 및 VOC 데이터 세트에서 학습한 클래스에 따라 장면에서 다양한 개체를 식별하고 색칠합니다. 이 세분화된 이미지는 그런 다음 ROS 주제에 발행됩니다.</p>
<p>이 설정을 통해 물체 인식, 자율 탐사, 그리고 장면 이해를 포함한 여러 로보틱 응용 프로그램에 적합한 실시간 이미지 세분화가 가능합니다.</p>
<h2>자율 탐사</h2>
<p>장애물 감지 및 회피:</p>
<ul>
<li>도시 환경: 자율 주행 자동차에서 시멘틱 세분화는 차로, 인도, 보행자, 차량 및 도시 풍경의 다른 중요 요소를 식별하는 데 도움이 됩니다. 이러한 요소를 세분화함으로써, 자율 주행 차량은 복잡한 환경에서 안전하게 탐색할 수 있습니다.</li>
<li>실내 내비게이션: 실내에서 작동하는 로봇은 벽, 가구, 문 및 기타 장애물을 감지하기 위해 시멘틱 세분화를 사용할 수 있어 효과적으로 탐색과 경로를 계획할 수 있습니다.</li>
</ul>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/1*YcvLdT_wB5wd6jfa6f3xWA.gif" alt="이미지"></p>
<h2>결론</h2>
<p>DeepLabV3는 새로운 atrous convolution과 공간 피라미드 풀링 기술을 통해 높은 정확도와 효율성을 제공하는 시멘틱 세그멘테이션에 강력한 도구입니다. DeepLabV3를 ROS2와 통합함으로써, 개발자들은 환경을 픽셀 수준에서 이해하고 상호 작용하는 지능적인 로봇 응용 프로그램을 만들 수 있습니다. DeepLabV3를 사용하면 자율 주행, 로봇 조작 또는 장면 이해와 같은 영역에서 고급 인식을 위한 새로운 가능성이 열립니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"ROS2 겸손한 이미지 세분화","description":"","date":"2024-06-20 17:50","slug":"2024-06-20-ROS2HumbleImageSegmentation","content":"\n\n\n\u003cimg src=\"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_0.png\" /\u003e\n\n# 소개\n\n이 튜토리얼에서는 DeepLabV3 모델을 사용하여 이미지의 의미론적 세분화를 수행하는 ROS2 노드를 만들 것입니다. ResNet-101 백본을 사용한 이 모델은 의미론적 세분할 작업에 대한 최신 아키텍처입니다. 노드는 웹캠 피드를 구독하고 이미지를 처리한 후 세그멘테이션을 수행하고 세분화된 이미지를 게시할 것입니다.\n\n## 준비물\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 설치되어 있는지 확인하세요:\n\n- ROS2 (Humble)\n- PyTorch\n- OpenCV\n- cv_bridge (ROS 이미지와 OpenCV 이미지 간 변환을 위한)\n- torchvision (사전 학습 모델과 이미지 변환을 위한)\n\n## DeepLabV3\n\nDeepLabV3은 시맨틱 세그멘테이션 작업을 위해 설계된 최신 딥러닝 모델입니다. 시맨틱 세그멘테이션은 이미지의 각 픽셀을 미리 정의된 범주로 분류하는 작업을 포함합니다. 물체 감지와 달리 물체를 식별하고 주위에 바운딩 상자를 넣는 대신, 시맨틱 세그멘테이션은 장면에 대한 상세하고 픽셀 수준의 이해를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## DeepLabV3의 주요 기능\n\nResNet 백본: DeepLabV3은 특성 추출을 위해 ResNet-101을 사용합니다. ResNet-101은 잔차 학습 방식으로 유명한 강력하고 깊은 신경망으로, 사라져가는 그래디언트 문제를 해결하여 매우 깊은 네트워크를 효율적으로 훈련할 수 있으며 견고한 특성 추출을 보장합니다.\n\n## DeepLabV3 작동 방식\n\n- 입력 이미지: 고정된 크기(예: 512x512 픽셀)의 입력 이미지로 프로세스가 시작됩니다.\n- 특성 추출: 입력 이미지는 ResNet-101 백본을 통해 전달됩니다. 이 네트워크는 ImageNet과 같은 대규모 데이터셋에서 사전 훈련되어 다양한 시각적 특성에 대한 견고한 이해력을 갖추고 있습니다.\n- 어트러스 합성곱 레이어: 초기 특성 추출 후, 모델은 다양한 확장률을 가진 일련의 어트러스 합성곱을 적용합니다. 이 단계를 통해 모델은 다양한 크기의 객체를 분할하는 데 중요한 다중 스케일의 특성을 캡처할 수 있습니다.\n- 공간 피라미드 풀링: 어트러스 합성곱의 출력은 공간 피라미드 풀링 모듈로 전달됩니다. 이 모듈은 다양한 스케일에서 특성을 풀링하여 이미지의 풍부한 다중 문맥적 표현을 제공합니다.\n- 분할 맵: 마지막으로, 풀링된 특성은 원본 이미지 해상도로 업샘플링되고 최종 합성곱 레이어가 분할 맵을 생성합니다. 이 맵의 각 픽셀에는 클래스 레이블이 지정되어 이미지의 상세한 세그멘테이션이 이루어집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_1.png\" /\u003e\n\n## DeepLabV3\n\n- Architecture: DeepLabV3는 시멘틱 이미지 세그멘테이션을 위해 설계된 딥러닝 모델입니다. Dilated (확장된) 합성곱을 활용하여 수용 영역을 확대시키면서도 공간 해상도를 유지하는 방식으로 다중 스케일 문맥을 캡처합니다.\n- Backbone: 여기서 사용된 백본은 ResNet-101이며, 보다 복잡한 표현을 학습하는 데 도움이 되는 깊은 잔여 네트워크입니다.\n- 데이터 세트: 모델은 COCO 데이터 세트에서 사전 훈련을 받고 VOC 라벨로 세부 조정되어 이러한 데이터 세트에서 발견되는 공통 객체를 세분화할 수 있습니다.\n\n## 모델 로딩 코드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nself.model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', weights='DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1')\n```\n\n## 코드 단계\n\n- 초기화: 모델은 SegmentationNode 클래스의 __init__ 메서드에서 초기화됩니다. torch.hub.load 메서드는 미리 훈련된 deeplabv3_resnet101 모델을 불러옵니다.\n- 전처리: 입력 이미지는 모델의 입력 요구 사항과 일치하도록 크기 조정, 중앙 자르기 및 정규화됩니다.\n- 추론: 이미지가 수신될 때 콜백에서 모델은 전처리된 이미지 텐서에 대해 추론을 수행합니다.\n- 후처리: 모델의 출력은 각 픽셀의 클래스 점수를 포함하는 텐서입니다. 각 픽셀에서 가장 높은 점수가 클래스를 결정합니다. 그런 다음 결과는 PASCAL VOC 컬러 맵을 사용하여 컬러맵으로 변환된 세그멘테이션 이미지로 변환됩니다.\n\n![image](/assets/img/2024-06-20-ROS2HumbleImageSegmentation_2.png)  \n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 어플리케이션: 시맨틱 세그멘테이션을 위한 ROS2 노드\n\n실용적인 어플리케이션에서 DeepLabV3를 활용하기 위해 웹캠 이미지로부터 시맨틱 세그멘테이션을 수행하는 ROS2 (로봇 운영 시스템 2) 노드를 만들 수 있습니다. 다음은 단계별 개요입니다:\n\n- 노드 초기화: 웹캠 이미지 토픽을 구독하는 ROS2 노드를 초기화합니다.\n- 이미지 전처리: 들어오는 이미지를 DeepLabV3 모델이 필요로 하는 형식으로 변환합니다. 일반적으로 이미지 크기 조정 및 정규화를 수행합니다.\n- 모델 추론: 전처리된 이미지를 DeepLabV3 모델을 통해 전파하여 세그멘테이션 맵을 얻습니다.\n- 후처리: 세그멘테이션 맵을 원본 이미지 해상도에 맞게 업샘플링하고 클래스 레이블을 시각적으로 식별 가능한 색상으로 변환합니다.\n- 결과 게시: 세그멘트된 이미지를 시각화나 추가 처리를 위해 ROS2 토픽에 게시합니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_3.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 사용법\n\n이 모델은 웹캠에서 수신한 이미지를 세분화하는 데 사용되며, COCO 및 VOC 데이터 세트에서 학습한 클래스에 따라 장면에서 다양한 개체를 식별하고 색칠합니다. 이 세분화된 이미지는 그런 다음 ROS 주제에 발행됩니다.\n\n이 설정을 통해 물체 인식, 자율 탐사, 그리고 장면 이해를 포함한 여러 로보틱 응용 프로그램에 적합한 실시간 이미지 세분화가 가능합니다.\n\n## 자율 탐사\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n장애물 감지 및 회피:\n\n- 도시 환경: 자율 주행 자동차에서 시멘틱 세분화는 차로, 인도, 보행자, 차량 및 도시 풍경의 다른 중요 요소를 식별하는 데 도움이 됩니다. 이러한 요소를 세분화함으로써, 자율 주행 차량은 복잡한 환경에서 안전하게 탐색할 수 있습니다.\n- 실내 내비게이션: 실내에서 작동하는 로봇은 벽, 가구, 문 및 기타 장애물을 감지하기 위해 시멘틱 세분화를 사용할 수 있어 효과적으로 탐색과 경로를 계획할 수 있습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*YcvLdT_wB5wd6jfa6f3xWA.gif)\n\n## 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDeepLabV3는 새로운 atrous convolution과 공간 피라미드 풀링 기술을 통해 높은 정확도와 효율성을 제공하는 시멘틱 세그멘테이션에 강력한 도구입니다. DeepLabV3를 ROS2와 통합함으로써, 개발자들은 환경을 픽셀 수준에서 이해하고 상호 작용하는 지능적인 로봇 응용 프로그램을 만들 수 있습니다. DeepLabV3를 사용하면 자율 주행, 로봇 조작 또는 장면 이해와 같은 영역에서 고급 인식을 위한 새로운 가능성이 열립니다.","ogImage":{"url":"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_0.png"},"coverImage":"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_0.png","tag":["Tech"],"readingTime":4},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch1\u003e소개\u003c/h1\u003e\n\u003cp\u003e이 튜토리얼에서는 DeepLabV3 모델을 사용하여 이미지의 의미론적 세분화를 수행하는 ROS2 노드를 만들 것입니다. ResNet-101 백본을 사용한 이 모델은 의미론적 세분할 작업에 대한 최신 아키텍처입니다. 노드는 웹캠 피드를 구독하고 이미지를 처리한 후 세그멘테이션을 수행하고 세분화된 이미지를 게시할 것입니다.\u003c/p\u003e\n\u003ch2\u003e준비물\u003c/h2\u003e\n\u003cp\u003e아래 설치되어 있는지 확인하세요:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eROS2 (Humble)\u003c/li\u003e\n\u003cli\u003ePyTorch\u003c/li\u003e\n\u003cli\u003eOpenCV\u003c/li\u003e\n\u003cli\u003ecv_bridge (ROS 이미지와 OpenCV 이미지 간 변환을 위한)\u003c/li\u003e\n\u003cli\u003etorchvision (사전 학습 모델과 이미지 변환을 위한)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eDeepLabV3\u003c/h2\u003e\n\u003cp\u003eDeepLabV3은 시맨틱 세그멘테이션 작업을 위해 설계된 최신 딥러닝 모델입니다. 시맨틱 세그멘테이션은 이미지의 각 픽셀을 미리 정의된 범주로 분류하는 작업을 포함합니다. 물체 감지와 달리 물체를 식별하고 주위에 바운딩 상자를 넣는 대신, 시맨틱 세그멘테이션은 장면에 대한 상세하고 픽셀 수준의 이해를 제공합니다.\u003c/p\u003e\n\u003ch2\u003eDeepLabV3의 주요 기능\u003c/h2\u003e\n\u003cp\u003eResNet 백본: DeepLabV3은 특성 추출을 위해 ResNet-101을 사용합니다. ResNet-101은 잔차 학습 방식으로 유명한 강력하고 깊은 신경망으로, 사라져가는 그래디언트 문제를 해결하여 매우 깊은 네트워크를 효율적으로 훈련할 수 있으며 견고한 특성 추출을 보장합니다.\u003c/p\u003e\n\u003ch2\u003eDeepLabV3 작동 방식\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e입력 이미지: 고정된 크기(예: 512x512 픽셀)의 입력 이미지로 프로세스가 시작됩니다.\u003c/li\u003e\n\u003cli\u003e특성 추출: 입력 이미지는 ResNet-101 백본을 통해 전달됩니다. 이 네트워크는 ImageNet과 같은 대규모 데이터셋에서 사전 훈련되어 다양한 시각적 특성에 대한 견고한 이해력을 갖추고 있습니다.\u003c/li\u003e\n\u003cli\u003e어트러스 합성곱 레이어: 초기 특성 추출 후, 모델은 다양한 확장률을 가진 일련의 어트러스 합성곱을 적용합니다. 이 단계를 통해 모델은 다양한 크기의 객체를 분할하는 데 중요한 다중 스케일의 특성을 캡처할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e공간 피라미드 풀링: 어트러스 합성곱의 출력은 공간 피라미드 풀링 모듈로 전달됩니다. 이 모듈은 다양한 스케일에서 특성을 풀링하여 이미지의 풍부한 다중 문맥적 표현을 제공합니다.\u003c/li\u003e\n\u003cli\u003e분할 맵: 마지막으로, 풀링된 특성은 원본 이미지 해상도로 업샘플링되고 최종 합성곱 레이어가 분할 맵을 생성합니다. 이 맵의 각 픽셀에는 클래스 레이블이 지정되어 이미지의 상세한 세그멘테이션이 이루어집니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eDeepLabV3\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitecture: DeepLabV3는 시멘틱 이미지 세그멘테이션을 위해 설계된 딥러닝 모델입니다. Dilated (확장된) 합성곱을 활용하여 수용 영역을 확대시키면서도 공간 해상도를 유지하는 방식으로 다중 스케일 문맥을 캡처합니다.\u003c/li\u003e\n\u003cli\u003eBackbone: 여기서 사용된 백본은 ResNet-101이며, 보다 복잡한 표현을 학습하는 데 도움이 되는 깊은 잔여 네트워크입니다.\u003c/li\u003e\n\u003cli\u003e데이터 세트: 모델은 COCO 데이터 세트에서 사전 훈련을 받고 VOC 라벨로 세부 조정되어 이러한 데이터 세트에서 발견되는 공통 객체를 세분화할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e모델 로딩 코드\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eself.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e = torch.\u003cspan class=\"hljs-property\"\u003ehub\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eload\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'pytorch/vision:v0.10.0'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'deeplabv3_resnet101'\u003c/span\u003e, weights=\u003cspan class=\"hljs-string\"\u003e'DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e코드 단계\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e초기화: 모델은 SegmentationNode 클래스의 \u003cstrong\u003einit\u003c/strong\u003e 메서드에서 초기화됩니다. torch.hub.load 메서드는 미리 훈련된 deeplabv3_resnet101 모델을 불러옵니다.\u003c/li\u003e\n\u003cli\u003e전처리: 입력 이미지는 모델의 입력 요구 사항과 일치하도록 크기 조정, 중앙 자르기 및 정규화됩니다.\u003c/li\u003e\n\u003cli\u003e추론: 이미지가 수신될 때 콜백에서 모델은 전처리된 이미지 텐서에 대해 추론을 수행합니다.\u003c/li\u003e\n\u003cli\u003e후처리: 모델의 출력은 각 픽셀의 클래스 점수를 포함하는 텐서입니다. 각 픽셀에서 가장 높은 점수가 클래스를 결정합니다. 그런 다음 결과는 PASCAL VOC 컬러 맵을 사용하여 컬러맵으로 변환된 세그멘테이션 이미지로 변환됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ROS2HumbleImageSegmentation_2.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch2\u003e어플리케이션: 시맨틱 세그멘테이션을 위한 ROS2 노드\u003c/h2\u003e\n\u003cp\u003e실용적인 어플리케이션에서 DeepLabV3를 활용하기 위해 웹캠 이미지로부터 시맨틱 세그멘테이션을 수행하는 ROS2 (로봇 운영 시스템 2) 노드를 만들 수 있습니다. 다음은 단계별 개요입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e노드 초기화: 웹캠 이미지 토픽을 구독하는 ROS2 노드를 초기화합니다.\u003c/li\u003e\n\u003cli\u003e이미지 전처리: 들어오는 이미지를 DeepLabV3 모델이 필요로 하는 형식으로 변환합니다. 일반적으로 이미지 크기 조정 및 정규화를 수행합니다.\u003c/li\u003e\n\u003cli\u003e모델 추론: 전처리된 이미지를 DeepLabV3 모델을 통해 전파하여 세그멘테이션 맵을 얻습니다.\u003c/li\u003e\n\u003cli\u003e후처리: 세그멘테이션 맵을 원본 이미지 해상도에 맞게 업샘플링하고 클래스 레이블을 시각적으로 식별 가능한 색상으로 변환합니다.\u003c/li\u003e\n\u003cli\u003e결과 게시: 세그멘트된 이미지를 시각화나 추가 처리를 위해 ROS2 토픽에 게시합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e사용법\u003c/h2\u003e\n\u003cp\u003e이 모델은 웹캠에서 수신한 이미지를 세분화하는 데 사용되며, COCO 및 VOC 데이터 세트에서 학습한 클래스에 따라 장면에서 다양한 개체를 식별하고 색칠합니다. 이 세분화된 이미지는 그런 다음 ROS 주제에 발행됩니다.\u003c/p\u003e\n\u003cp\u003e이 설정을 통해 물체 인식, 자율 탐사, 그리고 장면 이해를 포함한 여러 로보틱 응용 프로그램에 적합한 실시간 이미지 세분화가 가능합니다.\u003c/p\u003e\n\u003ch2\u003e자율 탐사\u003c/h2\u003e\n\u003cp\u003e장애물 감지 및 회피:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e도시 환경: 자율 주행 자동차에서 시멘틱 세분화는 차로, 인도, 보행자, 차량 및 도시 풍경의 다른 중요 요소를 식별하는 데 도움이 됩니다. 이러한 요소를 세분화함으로써, 자율 주행 차량은 복잡한 환경에서 안전하게 탐색할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e실내 내비게이션: 실내에서 작동하는 로봇은 벽, 가구, 문 및 기타 장애물을 감지하기 위해 시멘틱 세분화를 사용할 수 있어 효과적으로 탐색과 경로를 계획할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*YcvLdT_wB5wd6jfa6f3xWA.gif\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch2\u003e결론\u003c/h2\u003e\n\u003cp\u003eDeepLabV3는 새로운 atrous convolution과 공간 피라미드 풀링 기술을 통해 높은 정확도와 효율성을 제공하는 시멘틱 세그멘테이션에 강력한 도구입니다. DeepLabV3를 ROS2와 통합함으로써, 개발자들은 환경을 픽셀 수준에서 이해하고 상호 작용하는 지능적인 로봇 응용 프로그램을 만들 수 있습니다. DeepLabV3를 사용하면 자율 주행, 로봇 조작 또는 장면 이해와 같은 영역에서 고급 인식을 위한 새로운 가능성이 열립니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-20-ROS2HumbleImageSegmentation"},"buildId":"FH3Qr-mLAesqA0X5IFRQr","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>