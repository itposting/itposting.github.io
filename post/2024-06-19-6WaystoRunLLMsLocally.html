<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>6가지 방법으로 LLMs를 로컬에서 실행하는 방법 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-6WaystoRunLLMsLocally" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="6가지 방법으로 LLMs를 로컬에서 실행하는 방법 | itposting" data-gatsby-head="true"/><meta property="og:title" content="6가지 방법으로 LLMs를 로컬에서 실행하는 방법 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-6WaystoRunLLMsLocally_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-6WaystoRunLLMsLocally" data-gatsby-head="true"/><meta name="twitter:title" content="6가지 방법으로 LLMs를 로컬에서 실행하는 방법 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-6WaystoRunLLMsLocally_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 21:41" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-0fd008072af5a644.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_buildManifest.js" defer=""></script><script src="/_next/static/FH3Qr-mLAesqA0X5IFRQr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">6가지 방법으로 LLMs를 로컬에서 실행하는 방법</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="6가지 방법으로 LLMs를 로컬에서 실행하는 방법" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-6WaystoRunLLMsLocally&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>상용 AI 및 대규모 언어 모델(LLM)은 한 가지 큰 단점이 있습니다: 개인 정보 보호 문제! 민감하거나 자사 데이터를 다룰 때 이러한 도구를 활용할 수 없습니다.</p>
<p>이로 인해 로컬에서 개인 LLM을 운영하는 방법에 대해 알아야 합니다. 오픈 소스 모델은 해결책을 제시하지만 그들만의 일련의 도전과 혜택이 따릅니다.</p>
<p>당신의 컴퓨터에서 실행할 수 있는 ChatGPT의 로컬 대안을 발견하기 위한 여정에 함께해주세요.</p>
<h1>기대치 설정</h1>
<p>오픈 소스는 다양한 모델이 제공되므로 Meta와 같은 대규모 조직에서 제공하는 모델부터 개별 열정가들이 개발한 모델까지 수천 가지가 있습니다. 그러나 이러한 모델을 실행하는 것은 고유의 일련의 도전 과제를 제시할 수 있습니다:</p>
<ul>
<li>강력한 하드웨어가 필요할 수 있습니다: 많은 메모리와 가능한 경우 GPU</li>
<li>오픈 소스 모델은 개선되고 있지만, 대개 ChatGPT와 같은 더 정교한 제품의 능력을 따라잡지 못할 수 있습니다. 이러한 제품들은 대규모 엔지니어 팀의 지원을 받고 있습니다.</li>
<li>모든 모델을 상업적으로 사용할 수 있는 것은 아닙니다.</li>
</ul>
<p>Google의 유출 문서에 따르면 오픈 소스와 폐쇄 소스 모델 간의 간격이 좁혀지고 있다고 합니다.</p>
<p><img src="/assets/img/2024-06-19-6WaystoRunLLMsLocally_0.png" alt="이미지"></p>
<h1>1. Hugging Face와 Transformers</h1>
<p>Hugging Face은 머신러닝과 인공지능을 위한 도커 허브와 같은 서비스로, 다양한 오픈소스 모델을 제공합니다. 다행히도, Hugging Face는 주기적으로 모델을 평가하고 가장 좋은 모델을 선택할 수 있도록 리더보드를 제공합니다.</p>
<p>또한, Hugging Face는 transformers라는 파이썬 라이브러리도 제공하는데, 이 라이브러리는 로컬에서 LLM을 간편하게 실행할 수 있게 해줍니다. 아래 예제는 라이브러리를 사용하여 이전 버전의 GPT-2 microsoft/DialoGPT-medium 모델을 실행하는 방법을 보여줍니다. 첫 번째 실행 시 Transformers는 모델을 다운로드하고, 이 모델을 사용하여 다섯 번의 대화를 할 수 있습니다. 이 스크립트를 실행하기 위해서는 PyTorch도 설치되어 있어야 합니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer
<span class="hljs-keyword">import</span> torch
</code></pre>
<pre><code class="hljs language-js">tokenizer = <span class="hljs-title class_">AutoTokenizer</span>.<span class="hljs-title function_">from_pretrained</span>(<span class="hljs-string">"microsoft/DialoGPT-medium"</span>, padding_side=<span class="hljs-string">'left'</span>)
model = <span class="hljs-title class_">AutoModelForCausalLM</span>.<span class="hljs-title function_">from_pretrained</span>(<span class="hljs-string">"microsoft/DialoGPT-medium"</span>)
# 출처: <span class="hljs-attr">https</span>:<span class="hljs-comment">//huggingface.co/microsoft/DialoGPT-medium</span>
# <span class="hljs-number">5</span>줄 동안 채팅해 봅시다
<span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-number">5</span>):
    # 새로운 사용자 입력을 인코딩하고 eos_token을 추가하여 <span class="hljs-title class_">Pytorch</span>의 텐서를 반환합니다
    new_user_input_ids = tokenizer.<span class="hljs-title function_">encode</span>(<span class="hljs-title function_">input</span>(<span class="hljs-string">">> 사용자:"</span>) + tokenizer.<span class="hljs-property">eos_token</span>, return_tensors=<span class="hljs-string">'pt'</span>)
    # 채팅 기록 텐서에 새로운 사용자 입력 토큰을 추가합니다
    bot_input_ids = torch.<span class="hljs-title function_">cat</span>([chat_history_ids, new_user_input_ids], dim=-<span class="hljs-number">1</span>) <span class="hljs-keyword">if</span> step > <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> new_user_input_ids
    # 총 채팅 기록을 <span class="hljs-number">1000</span> 토큰으로 제한하며 응답을 생성합니다
    chat_history_ids = model.<span class="hljs-title function_">generate</span>(bot_input_ids, max_length=<span class="hljs-number">1000</span>, pad_token_id=tokenizer.<span class="hljs-property">eos_token_id</span>)
    # 최신 출력 토큰을 이쁘게 출력합니다
    <span class="hljs-title function_">print</span>(<span class="hljs-string">"DialoGPT: {}"</span>.<span class="hljs-title function_">format</span>(tokenizer.<span class="hljs-title function_">decode</span>(chat_history_ids[:, bot_input_ids.<span class="hljs-property">shape</span>[-<span class="hljs-number">1</span>]:][<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-title class_">True</span>)))
</code></pre>
<p>Transformers 장점:</p>
<ul>
<li>모델 자동 다운로드</li>
<li>코드 조각 사용 가능</li>
<li>실험 및 학습에 이상적</li>
</ul>
<p>Transformers 단점:</p>
<ul>
<li>ML 및 NLP에 대한 좋은 이해가 필요합니다.</li>
<li>코딩 및 구성 기술이 필요합니다.</li>
</ul>
<h2>2. LangChain</h2>
<p>로컬에서 LLM을 실행하는 또 다른 방법은 LangChain을 사용하는 것입니다. LangChain은 AI 애플리케이션을 구축하기 위한 Python 프레임워크입니다. 지원하는 모델 중 하나 위에 AI 애플리케이션을 개발하기 위한 추상화 및 미들웨어를 제공합니다. 예를 들어 아래 코드는 microsoft/DialoGPT-medium 모델에 한 가지 질문을 하는 코드입니다:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> langchain.<span class="hljs-property">llms</span>.<span class="hljs-property">huggingface_pipeline</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">HuggingFacePipeline</span>
</code></pre>
<pre><code class="hljs language-js">hf = <span class="hljs-title class_">HuggingFacePipeline</span>.<span class="hljs-title function_">from_model_id</span>(
    model_id=<span class="hljs-string">"microsoft/DialoGPT-medium"</span>, task=<span class="hljs-string">"text-generation"</span>, pipeline_kwargs={<span class="hljs-string">"max_new_tokens"</span>: <span class="hljs-number">200</span>, <span class="hljs-string">"pad_token_id"</span>: <span class="hljs-number">50256</span>},
)
<span class="hljs-keyword">from</span> langchain.<span class="hljs-property">prompts</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">PromptTemplate</span>
template = <span class="hljs-string">""</span><span class="hljs-string">"Question: {question}
Answer: Let's think step by step."</span><span class="hljs-string">""</span>
prompt = <span class="hljs-title class_">PromptTemplate</span>.<span class="hljs-title function_">from_template</span>(template)
chain = prompt | hf
question = <span class="hljs-string">"What is electroencephalography?"</span>
<span class="hljs-title function_">print</span>(chain.<span class="hljs-title function_">invoke</span>({<span class="hljs-string">"question"</span>: question}))
</code></pre>
<p>LangChain Pros:</p>
<ul>
<li>모델 관리가 쉽습니다</li>
<li>AI 응용 프로그램 개발을 위한 유용한 유틸리티들</li>
</ul>
<p>LangChain Cons:</p>
<ul>
<li>속도가 제한되어 있어 Transformers와 동일합니다</li>
<li>여전히 애플리케이션 로직을 코딩하거나 적합한 UI를 만들어야 합니다.</li>
</ul>
<h1>3. Llama.cpp</h1>
<p>Llama.cpp은 LLM을 위한 C 및 C++ 기반 추론 엔진으로, Apple 실리콘에 최적화되어 있으며 Meta의 Llama2 모델을 실행할 수 있습니다.</p>
<p>저장소를 복제하고 프로젝트를 빌드한 후에는 다음을 사용하여 모델을 실행할 수 있습니다:</p>
<pre><code class="hljs language-js">$ ./main -m /path/to/model-file.<span class="hljs-property">gguf</span> -p <span class="hljs-string">"안녕하세요!"</span>
</code></pre>
<p>Llama.cpp의 장점:</p>
<ul>
<li>Python 기반의 솔루션보다 높은 성능</li>
<li>Llama 7B와 같은 대형 모델을 저사양 하드웨어에서 지원</li>
<li>다른 언어로 AI 애플리케이션을 빌드하기 위한 바인딩을 제공하며 추론은 Llama.cpp를 통해 실행됨.</li>
</ul>
<p>Llama.cpp의 단점:</p>
<ul>
<li>제한된 모델 지원</li>
<li>도구 빌딩이 필요합니다</li>
</ul>
<h2>4. 람파파일</h2>
<p>Mozilla에서 개발한 람파파일은 LLM(?)을 실행하는 사용자 친화적인 대안을 제공합니다. 람파파일은 휴대성과 하나의 파일로 실행 가능한 능력으로 유명합니다.</p>
<p>람파파일을 다운로드한 후 GGUF 형식의 모델과 함께 사용하면 로컬 브라우저 세션을 시작할 수 있습니다:</p>
<pre><code class="hljs language-js">$ ./llamafile -m /path/to/model.<span class="hljs-property">gguf</span>
</code></pre>
<p>Llamafile 장점:</p>
<ul>
<li>Llama.cpp와 동일한 속도 장점을 누릴 수 있습니다.</li>
<li>모델이 포함된 단일 실행 파일을 빌드할 수 있습니다.</li>
</ul>
<p>Llamafile 단점:</p>
<ul>
<li>프로젝트는 아직 초기 단계에 있어요.</li>
<li>모든 모델이 지원되는 것은 아니에요. Llama.cpp가 지원하는 모델만 지원돼요.</li>
</ul>
<h1>5. Ollama</h1>
<p>Ollama는 Llama.cpp와 Llamafile에 대한 더 사용하기 쉬운 대안이에요. 실행 파일을 다운로드하여 설치하는 서비스를 머신에 설치해요. 설치가 완료되면 터미널을 열고 아래와 같이 실행해주세요.</p>
<pre><code class="hljs language-js">$ ollama run llama2
</code></pre>
<p>Ollama는 모델을 다운로드하고 대화형 세션을 시작할 것입니다.</p>
<p>Ollama 장점:</p>
<ul>
<li>설치 및 사용이 쉽습니다.</li>
<li>람마와 비쿠냐 모델을 실행할 수 있습니다.</li>
<li>속도가 정말 빠릅니다.</li>
</ul>
<p>Ollama 단점:</p>
<ul>
<li>모델 라이브러리 제공이 제한적입니다.</li>
<li>모델을 스스로 관리하며, 사용자 지정 모델을 재사용할 수 없습니다.</li>
<li>LLM을 실행할 때 조정 가능한 옵션이 없습니다.</li>
<li>아직 Windows 버전은 없습니다.</li>
</ul>
<h1>6. GPT4ALL</h1>
<p>GPT4ALL은 직관적인 GUI를 갖춘 사용하기 쉬운 데스크톱 응용 프로그램입니다. 로컬 모델 실행을 지원하며, OpenAI에 API 키를 사용하여 연결할 수 있습니다. GPT4ALL은 문맥을 위해 로컬 문서를 처리하는 능력으로 두드러집니다. 개인 정보 보호를 보장합니다.</p>
<p><img src="/assets/img/2024-06-19-6WaystoRunLLMsLocally_1.png" alt="이미지"></p>
<p>장점:</p>
<ul>
<li>친근한 UI를 가진 정교한 대안</li>
<li>다양한 선별된 모델을 지원</li>
</ul>
<p>단점:</p>
<ul>
<li>모델 선택이 제한적</li>
<li>일부 모델은 상업적 이용 제약이 있음</li>
</ul>
<h1>결론</h1>
<p>LLM을 로컬에서 실행할 수 있는 적합한 도구를 선택하는 것은 여러분의 요구사항과 전문 지식에 달려 있어요. GPT4ALL과 같은 사용자 친화적인 응용 프로그램부터 Llama.cpp 및 Python 기반의 더 기술적인 옵션까지 다양한 선택지가 있어요. 오픈 소스 모델들이 새로운 기능들을 제공하며, 데이터와 개인 정보 보호에 대한 더 많은 통제 기회를 제공하고 있어요.</p>
<p>본 안내서는 로컬 LLM 세계를 탐색하는 데 도움이 되는 직관성을 제공하고 있어요. 이러한 모델들이 발전함에 따라, ChatGPT와 같은 제품들과 경쟁력을 갖출 것으로 약속되고 있어요.</p>
<p>2023년 12월 14일에 <a href="https://semaphoreci.com%EC%97%90" rel="nofollow" target="_blank">https://semaphoreci.com에</a> 게재된 내용입니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"6가지 방법으로 LLMs를 로컬에서 실행하는 방법","description":"","date":"2024-06-19 21:41","slug":"2024-06-19-6WaystoRunLLMsLocally","content":"\n\n상용 AI 및 대규모 언어 모델(LLM)은 한 가지 큰 단점이 있습니다: 개인 정보 보호 문제! 민감하거나 자사 데이터를 다룰 때 이러한 도구를 활용할 수 없습니다.\n\n이로 인해 로컬에서 개인 LLM을 운영하는 방법에 대해 알아야 합니다. 오픈 소스 모델은 해결책을 제시하지만 그들만의 일련의 도전과 혜택이 따릅니다.\n\n당신의 컴퓨터에서 실행할 수 있는 ChatGPT의 로컬 대안을 발견하기 위한 여정에 함께해주세요.\n\n# 기대치 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오픈 소스는 다양한 모델이 제공되므로 Meta와 같은 대규모 조직에서 제공하는 모델부터 개별 열정가들이 개발한 모델까지 수천 가지가 있습니다. 그러나 이러한 모델을 실행하는 것은 고유의 일련의 도전 과제를 제시할 수 있습니다:\n\n- 강력한 하드웨어가 필요할 수 있습니다: 많은 메모리와 가능한 경우 GPU\n- 오픈 소스 모델은 개선되고 있지만, 대개 ChatGPT와 같은 더 정교한 제품의 능력을 따라잡지 못할 수 있습니다. 이러한 제품들은 대규모 엔지니어 팀의 지원을 받고 있습니다.\n- 모든 모델을 상업적으로 사용할 수 있는 것은 아닙니다.\n\nGoogle의 유출 문서에 따르면 오픈 소스와 폐쇄 소스 모델 간의 간격이 좁혀지고 있다고 합니다.\n\n![이미지](/assets/img/2024-06-19-6WaystoRunLLMsLocally_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 1. Hugging Face와 Transformers\n\nHugging Face은 머신러닝과 인공지능을 위한 도커 허브와 같은 서비스로, 다양한 오픈소스 모델을 제공합니다. 다행히도, Hugging Face는 주기적으로 모델을 평가하고 가장 좋은 모델을 선택할 수 있도록 리더보드를 제공합니다.\n\n또한, Hugging Face는 transformers라는 파이썬 라이브러리도 제공하는데, 이 라이브러리는 로컬에서 LLM을 간편하게 실행할 수 있게 해줍니다. 아래 예제는 라이브러리를 사용하여 이전 버전의 GPT-2 microsoft/DialoGPT-medium 모델을 실행하는 방법을 보여줍니다. 첫 번째 실행 시 Transformers는 모델을 다운로드하고, 이 모델을 사용하여 다섯 번의 대화를 할 수 있습니다. 이 스크립트를 실행하기 위해서는 PyTorch도 설치되어 있어야 합니다.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side='left')\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n# 출처: https://huggingface.co/microsoft/DialoGPT-medium\n# 5줄 동안 채팅해 봅시다\nfor step in range(5):\n    # 새로운 사용자 입력을 인코딩하고 eos_token을 추가하여 Pytorch의 텐서를 반환합니다\n    new_user_input_ids = tokenizer.encode(input(\"\u003e\u003e 사용자:\") + tokenizer.eos_token, return_tensors='pt')\n    # 채팅 기록 텐서에 새로운 사용자 입력 토큰을 추가합니다\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step \u003e 0 else new_user_input_ids\n    # 총 채팅 기록을 1000 토큰으로 제한하며 응답을 생성합니다\n    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n    # 최신 출력 토큰을 이쁘게 출력합니다\n    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\r\n```\n\nTransformers 장점:\n\n- 모델 자동 다운로드\n- 코드 조각 사용 가능\n- 실험 및 학습에 이상적\n\nTransformers 단점:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- ML 및 NLP에 대한 좋은 이해가 필요합니다.\n- 코딩 및 구성 기술이 필요합니다.\n\n## 2. LangChain\n\n로컬에서 LLM을 실행하는 또 다른 방법은 LangChain을 사용하는 것입니다. LangChain은 AI 애플리케이션을 구축하기 위한 Python 프레임워크입니다. 지원하는 모델 중 하나 위에 AI 애플리케이션을 개발하기 위한 추상화 및 미들웨어를 제공합니다. 예를 들어 아래 코드는 microsoft/DialoGPT-medium 모델에 한 가지 질문을 하는 코드입니다:\n\n```js\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\n``` \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nhf = HuggingFacePipeline.from_model_id(\n    model_id=\"microsoft/DialoGPT-medium\", task=\"text-generation\", pipeline_kwargs={\"max_new_tokens\": 200, \"pad_token_id\": 50256},\n)\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"Question: {question}\nAnswer: Let's think step by step.\"\"\"\nprompt = PromptTemplate.from_template(template)\nchain = prompt | hf\nquestion = \"What is electroencephalography?\"\nprint(chain.invoke({\"question\": question}))\n```\n\nLangChain Pros:\n\n- 모델 관리가 쉽습니다\n- AI 응용 프로그램 개발을 위한 유용한 유틸리티들\n\nLangChain Cons:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 속도가 제한되어 있어 Transformers와 동일합니다\n- 여전히 애플리케이션 로직을 코딩하거나 적합한 UI를 만들어야 합니다.\n\n# 3. Llama.cpp\n\nLlama.cpp은 LLM을 위한 C 및 C++ 기반 추론 엔진으로, Apple 실리콘에 최적화되어 있으며 Meta의 Llama2 모델을 실행할 수 있습니다.\n\n저장소를 복제하고 프로젝트를 빌드한 후에는 다음을 사용하여 모델을 실행할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n$ ./main -m /path/to/model-file.gguf -p \"안녕하세요!\"\n```\n\nLlama.cpp의 장점:\n\n- Python 기반의 솔루션보다 높은 성능\n- Llama 7B와 같은 대형 모델을 저사양 하드웨어에서 지원\n- 다른 언어로 AI 애플리케이션을 빌드하기 위한 바인딩을 제공하며 추론은 Llama.cpp를 통해 실행됨.\n\nLlama.cpp의 단점:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 제한된 모델 지원\n- 도구 빌딩이 필요합니다\n\n## 4. 람파파일\n\nMozilla에서 개발한 람파파일은 LLM(?)을 실행하는 사용자 친화적인 대안을 제공합니다. 람파파일은 휴대성과 하나의 파일로 실행 가능한 능력으로 유명합니다.\n\n람파파일을 다운로드한 후 GGUF 형식의 모델과 함께 사용하면 로컬 브라우저 세션을 시작할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n$ ./llamafile -m /path/to/model.gguf\n```\n\nLlamafile 장점:\n\n- Llama.cpp와 동일한 속도 장점을 누릴 수 있습니다.\n- 모델이 포함된 단일 실행 파일을 빌드할 수 있습니다.\n\nLlamafile 단점:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 프로젝트는 아직 초기 단계에 있어요.\n- 모든 모델이 지원되는 것은 아니에요. Llama.cpp가 지원하는 모델만 지원돼요.\n\n# 5. Ollama\n\nOllama는 Llama.cpp와 Llamafile에 대한 더 사용하기 쉬운 대안이에요. 실행 파일을 다운로드하여 설치하는 서비스를 머신에 설치해요. 설치가 완료되면 터미널을 열고 아래와 같이 실행해주세요.\n\n```js\n$ ollama run llama2\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nOllama는 모델을 다운로드하고 대화형 세션을 시작할 것입니다.\n\nOllama 장점:\n\n- 설치 및 사용이 쉽습니다.\n- 람마와 비쿠냐 모델을 실행할 수 있습니다.\n- 속도가 정말 빠릅니다.\n\nOllama 단점:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 모델 라이브러리 제공이 제한적입니다.\n- 모델을 스스로 관리하며, 사용자 지정 모델을 재사용할 수 없습니다.\n- LLM을 실행할 때 조정 가능한 옵션이 없습니다.\n- 아직 Windows 버전은 없습니다.\n\n# 6. GPT4ALL\n\nGPT4ALL은 직관적인 GUI를 갖춘 사용하기 쉬운 데스크톱 응용 프로그램입니다. 로컬 모델 실행을 지원하며, OpenAI에 API 키를 사용하여 연결할 수 있습니다. GPT4ALL은 문맥을 위해 로컬 문서를 처리하는 능력으로 두드러집니다. 개인 정보 보호를 보장합니다.\n\n![이미지](/assets/img/2024-06-19-6WaystoRunLLMsLocally_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n장점:\n\n- 친근한 UI를 가진 정교한 대안\n- 다양한 선별된 모델을 지원\n\n단점:\n\n- 모델 선택이 제한적\n- 일부 모델은 상업적 이용 제약이 있음\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\nLLM을 로컬에서 실행할 수 있는 적합한 도구를 선택하는 것은 여러분의 요구사항과 전문 지식에 달려 있어요. GPT4ALL과 같은 사용자 친화적인 응용 프로그램부터 Llama.cpp 및 Python 기반의 더 기술적인 옵션까지 다양한 선택지가 있어요. 오픈 소스 모델들이 새로운 기능들을 제공하며, 데이터와 개인 정보 보호에 대한 더 많은 통제 기회를 제공하고 있어요.\n\n본 안내서는 로컬 LLM 세계를 탐색하는 데 도움이 되는 직관성을 제공하고 있어요. 이러한 모델들이 발전함에 따라, ChatGPT와 같은 제품들과 경쟁력을 갖출 것으로 약속되고 있어요.\n\n2023년 12월 14일에 https://semaphoreci.com에 게재된 내용입니다.","ogImage":{"url":"/assets/img/2024-06-19-6WaystoRunLLMsLocally_0.png"},"coverImage":"/assets/img/2024-06-19-6WaystoRunLLMsLocally_0.png","tag":["Tech"],"readingTime":6},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e상용 AI 및 대규모 언어 모델(LLM)은 한 가지 큰 단점이 있습니다: 개인 정보 보호 문제! 민감하거나 자사 데이터를 다룰 때 이러한 도구를 활용할 수 없습니다.\u003c/p\u003e\n\u003cp\u003e이로 인해 로컬에서 개인 LLM을 운영하는 방법에 대해 알아야 합니다. 오픈 소스 모델은 해결책을 제시하지만 그들만의 일련의 도전과 혜택이 따릅니다.\u003c/p\u003e\n\u003cp\u003e당신의 컴퓨터에서 실행할 수 있는 ChatGPT의 로컬 대안을 발견하기 위한 여정에 함께해주세요.\u003c/p\u003e\n\u003ch1\u003e기대치 설정\u003c/h1\u003e\n\u003cp\u003e오픈 소스는 다양한 모델이 제공되므로 Meta와 같은 대규모 조직에서 제공하는 모델부터 개별 열정가들이 개발한 모델까지 수천 가지가 있습니다. 그러나 이러한 모델을 실행하는 것은 고유의 일련의 도전 과제를 제시할 수 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e강력한 하드웨어가 필요할 수 있습니다: 많은 메모리와 가능한 경우 GPU\u003c/li\u003e\n\u003cli\u003e오픈 소스 모델은 개선되고 있지만, 대개 ChatGPT와 같은 더 정교한 제품의 능력을 따라잡지 못할 수 있습니다. 이러한 제품들은 대규모 엔지니어 팀의 지원을 받고 있습니다.\u003c/li\u003e\n\u003cli\u003e모든 모델을 상업적으로 사용할 수 있는 것은 아닙니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eGoogle의 유출 문서에 따르면 오픈 소스와 폐쇄 소스 모델 간의 간격이 좁혀지고 있다고 합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-6WaystoRunLLMsLocally_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch1\u003e1. Hugging Face와 Transformers\u003c/h1\u003e\n\u003cp\u003eHugging Face은 머신러닝과 인공지능을 위한 도커 허브와 같은 서비스로, 다양한 오픈소스 모델을 제공합니다. 다행히도, Hugging Face는 주기적으로 모델을 평가하고 가장 좋은 모델을 선택할 수 있도록 리더보드를 제공합니다.\u003c/p\u003e\n\u003cp\u003e또한, Hugging Face는 transformers라는 파이썬 라이브러리도 제공하는데, 이 라이브러리는 로컬에서 LLM을 간편하게 실행할 수 있게 해줍니다. 아래 예제는 라이브러리를 사용하여 이전 버전의 GPT-2 microsoft/DialoGPT-medium 모델을 실행하는 방법을 보여줍니다. 첫 번째 실행 시 Transformers는 모델을 다운로드하고, 이 모델을 사용하여 다섯 번의 대화를 할 수 있습니다. 이 스크립트를 실행하기 위해서는 PyTorch도 설치되어 있어야 합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e AutoModelForCausalLM, AutoTokenizer\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003etokenizer = \u003cspan class=\"hljs-title class_\"\u003eAutoTokenizer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"microsoft/DialoGPT-medium\"\u003c/span\u003e, padding_side=\u003cspan class=\"hljs-string\"\u003e'left'\u003c/span\u003e)\nmodel = \u003cspan class=\"hljs-title class_\"\u003eAutoModelForCausalLM\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"microsoft/DialoGPT-medium\"\u003c/span\u003e)\n# 출처: \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//huggingface.co/microsoft/DialoGPT-medium\u003c/span\u003e\n# \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e줄 동안 채팅해 봅시다\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e step \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e):\n    # 새로운 사용자 입력을 인코딩하고 eos_token을 추가하여 \u003cspan class=\"hljs-title class_\"\u003ePytorch\u003c/span\u003e의 텐서를 반환합니다\n    new_user_input_ids = tokenizer.\u003cspan class=\"hljs-title function_\"\u003eencode\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003einput\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"\u003e\u003e 사용자:\"\u003c/span\u003e) + tokenizer.\u003cspan class=\"hljs-property\"\u003eeos_token\u003c/span\u003e, return_tensors=\u003cspan class=\"hljs-string\"\u003e'pt'\u003c/span\u003e)\n    # 채팅 기록 텐서에 새로운 사용자 입력 토큰을 추가합니다\n    bot_input_ids = torch.\u003cspan class=\"hljs-title function_\"\u003ecat\u003c/span\u003e([chat_history_ids, new_user_input_ids], dim=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e step \u003e \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e new_user_input_ids\n    # 총 채팅 기록을 \u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e 토큰으로 제한하며 응답을 생성합니다\n    chat_history_ids = model.\u003cspan class=\"hljs-title function_\"\u003egenerate\u003c/span\u003e(bot_input_ids, max_length=\u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e, pad_token_id=tokenizer.\u003cspan class=\"hljs-property\"\u003eeos_token_id\u003c/span\u003e)\n    # 최신 출력 토큰을 이쁘게 출력합니다\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"DialoGPT: {}\"\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eformat\u003c/span\u003e(tokenizer.\u003cspan class=\"hljs-title function_\"\u003edecode\u003c/span\u003e(chat_history_ids[:, bot_input_ids.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]:][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], skip_special_tokens=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTransformers 장점:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e모델 자동 다운로드\u003c/li\u003e\n\u003cli\u003e코드 조각 사용 가능\u003c/li\u003e\n\u003cli\u003e실험 및 학습에 이상적\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTransformers 단점:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eML 및 NLP에 대한 좋은 이해가 필요합니다.\u003c/li\u003e\n\u003cli\u003e코딩 및 구성 기술이 필요합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e2. LangChain\u003c/h2\u003e\n\u003cp\u003e로컬에서 LLM을 실행하는 또 다른 방법은 LangChain을 사용하는 것입니다. LangChain은 AI 애플리케이션을 구축하기 위한 Python 프레임워크입니다. 지원하는 모델 중 하나 위에 AI 애플리케이션을 개발하기 위한 추상화 및 미들웨어를 제공합니다. 예를 들어 아래 코드는 microsoft/DialoGPT-medium 모델에 한 가지 질문을 하는 코드입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003ellms\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ehuggingface_pipeline\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eHuggingFacePipeline\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ehf = \u003cspan class=\"hljs-title class_\"\u003eHuggingFacePipeline\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_model_id\u003c/span\u003e(\n    model_id=\u003cspan class=\"hljs-string\"\u003e\"microsoft/DialoGPT-medium\"\u003c/span\u003e, task=\u003cspan class=\"hljs-string\"\u003e\"text-generation\"\u003c/span\u003e, pipeline_kwargs={\u003cspan class=\"hljs-string\"\u003e\"max_new_tokens\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e200\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"pad_token_id\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e50256\u003c/span\u003e},\n)\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003eprompts\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003ePromptTemplate\u003c/span\u003e\ntemplate = \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"Question: {question}\nAnswer: Let's think step by step.\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\nprompt = \u003cspan class=\"hljs-title class_\"\u003ePromptTemplate\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_template\u003c/span\u003e(template)\nchain = prompt | hf\nquestion = \u003cspan class=\"hljs-string\"\u003e\"What is electroencephalography?\"\u003c/span\u003e\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(chain.\u003cspan class=\"hljs-title function_\"\u003einvoke\u003c/span\u003e({\u003cspan class=\"hljs-string\"\u003e\"question\"\u003c/span\u003e: question}))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLangChain Pros:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e모델 관리가 쉽습니다\u003c/li\u003e\n\u003cli\u003eAI 응용 프로그램 개발을 위한 유용한 유틸리티들\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLangChain Cons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e속도가 제한되어 있어 Transformers와 동일합니다\u003c/li\u003e\n\u003cli\u003e여전히 애플리케이션 로직을 코딩하거나 적합한 UI를 만들어야 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e3. Llama.cpp\u003c/h1\u003e\n\u003cp\u003eLlama.cpp은 LLM을 위한 C 및 C++ 기반 추론 엔진으로, Apple 실리콘에 최적화되어 있으며 Meta의 Llama2 모델을 실행할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e저장소를 복제하고 프로젝트를 빌드한 후에는 다음을 사용하여 모델을 실행할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e$ ./main -m /path/to/model-file.\u003cspan class=\"hljs-property\"\u003egguf\u003c/span\u003e -p \u003cspan class=\"hljs-string\"\u003e\"안녕하세요!\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLlama.cpp의 장점:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePython 기반의 솔루션보다 높은 성능\u003c/li\u003e\n\u003cli\u003eLlama 7B와 같은 대형 모델을 저사양 하드웨어에서 지원\u003c/li\u003e\n\u003cli\u003e다른 언어로 AI 애플리케이션을 빌드하기 위한 바인딩을 제공하며 추론은 Llama.cpp를 통해 실행됨.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLlama.cpp의 단점:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e제한된 모델 지원\u003c/li\u003e\n\u003cli\u003e도구 빌딩이 필요합니다\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e4. 람파파일\u003c/h2\u003e\n\u003cp\u003eMozilla에서 개발한 람파파일은 LLM(?)을 실행하는 사용자 친화적인 대안을 제공합니다. 람파파일은 휴대성과 하나의 파일로 실행 가능한 능력으로 유명합니다.\u003c/p\u003e\n\u003cp\u003e람파파일을 다운로드한 후 GGUF 형식의 모델과 함께 사용하면 로컬 브라우저 세션을 시작할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e$ ./llamafile -m /path/to/model.\u003cspan class=\"hljs-property\"\u003egguf\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLlamafile 장점:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLlama.cpp와 동일한 속도 장점을 누릴 수 있습니다.\u003c/li\u003e\n\u003cli\u003e모델이 포함된 단일 실행 파일을 빌드할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLlamafile 단점:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e프로젝트는 아직 초기 단계에 있어요.\u003c/li\u003e\n\u003cli\u003e모든 모델이 지원되는 것은 아니에요. Llama.cpp가 지원하는 모델만 지원돼요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e5. Ollama\u003c/h1\u003e\n\u003cp\u003eOllama는 Llama.cpp와 Llamafile에 대한 더 사용하기 쉬운 대안이에요. 실행 파일을 다운로드하여 설치하는 서비스를 머신에 설치해요. 설치가 완료되면 터미널을 열고 아래와 같이 실행해주세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e$ ollama run llama2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOllama는 모델을 다운로드하고 대화형 세션을 시작할 것입니다.\u003c/p\u003e\n\u003cp\u003eOllama 장점:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e설치 및 사용이 쉽습니다.\u003c/li\u003e\n\u003cli\u003e람마와 비쿠냐 모델을 실행할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e속도가 정말 빠릅니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOllama 단점:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e모델 라이브러리 제공이 제한적입니다.\u003c/li\u003e\n\u003cli\u003e모델을 스스로 관리하며, 사용자 지정 모델을 재사용할 수 없습니다.\u003c/li\u003e\n\u003cli\u003eLLM을 실행할 때 조정 가능한 옵션이 없습니다.\u003c/li\u003e\n\u003cli\u003e아직 Windows 버전은 없습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e6. GPT4ALL\u003c/h1\u003e\n\u003cp\u003eGPT4ALL은 직관적인 GUI를 갖춘 사용하기 쉬운 데스크톱 응용 프로그램입니다. 로컬 모델 실행을 지원하며, OpenAI에 API 키를 사용하여 연결할 수 있습니다. GPT4ALL은 문맥을 위해 로컬 문서를 처리하는 능력으로 두드러집니다. 개인 정보 보호를 보장합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-6WaystoRunLLMsLocally_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e장점:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e친근한 UI를 가진 정교한 대안\u003c/li\u003e\n\u003cli\u003e다양한 선별된 모델을 지원\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e단점:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e모델 선택이 제한적\u003c/li\u003e\n\u003cli\u003e일부 모델은 상업적 이용 제약이 있음\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003eLLM을 로컬에서 실행할 수 있는 적합한 도구를 선택하는 것은 여러분의 요구사항과 전문 지식에 달려 있어요. GPT4ALL과 같은 사용자 친화적인 응용 프로그램부터 Llama.cpp 및 Python 기반의 더 기술적인 옵션까지 다양한 선택지가 있어요. 오픈 소스 모델들이 새로운 기능들을 제공하며, 데이터와 개인 정보 보호에 대한 더 많은 통제 기회를 제공하고 있어요.\u003c/p\u003e\n\u003cp\u003e본 안내서는 로컬 LLM 세계를 탐색하는 데 도움이 되는 직관성을 제공하고 있어요. 이러한 모델들이 발전함에 따라, ChatGPT와 같은 제품들과 경쟁력을 갖출 것으로 약속되고 있어요.\u003c/p\u003e\n\u003cp\u003e2023년 12월 14일에 \u003ca href=\"https://semaphoreci.com%EC%97%90\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://semaphoreci.com에\u003c/a\u003e 게재된 내용입니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-6WaystoRunLLMsLocally"},"buildId":"FH3Qr-mLAesqA0X5IFRQr","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>