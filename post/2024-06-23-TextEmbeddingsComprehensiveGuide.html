<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>텍스트 임베딩 종합 가이드 2024 최신 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-23-TextEmbeddingsComprehensiveGuide" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="텍스트 임베딩 종합 가이드 2024 최신 | itposting" data-gatsby-head="true"/><meta property="og:title" content="텍스트 임베딩 종합 가이드 2024 최신 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-23-TextEmbeddingsComprehensiveGuide" data-gatsby-head="true"/><meta name="twitter:title" content="텍스트 임베딩 종합 가이드 2024 최신 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-23 19:53" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">텍스트 임베딩 종합 가이드 2024 최신</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="텍스트 임베딩 종합 가이드 2024 최신" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 23, 2024</span><span class="posts_reading_time__f7YPP">24<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-23-TextEmbeddingsComprehensiveGuide&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>텍스트 임베딩의 진화, 시각화, 그리고 응용</h2>
<p>우리 인간은 텍스트를 읽고 이해할 수 있습니다 (적어도 일부분은요). 컴퓨터는 반대로 "숫자로 생각"하기 때문에 단어와 문장의 의미를 자동으로 파악할 수 없습니다. 만약 우리가 컴퓨터가 자연어를 이해하도록 하려면, 이 정보를 컴퓨터가 작업할 수 있는 형식인 숫자 벡터로 변환해야 합니다.</p>
<p>수십 년 전에 사람들은 텍스트를 기계가 이해할 수 있는 형식으로 변환하는 방법을 배웠습니다 (그 중 하나는 ASCII였습니다). 이러한 방식은 텍스트를 렌더링하고 전송하는 데 도움이 되지만 단어의 의미를 부호화하지는 않습니다. 당시에는 키워드 검색 기술이 표준 검색 기술이었으며 특정 단어나 N-gram을 포함하는 모든 문서를 찾는 방식이었습니다.</p>
<p>그 후 몇 10년이 지난 후, 임베딩이 등장했습니다. 우리는 단어, 문장, 심지어 이미지에 대한 임베딩을 계산할 수 있습니다. 임베딩도 숫자의 벡터입니다만, 의미를 포착할 수 있습니다. 그래서 의미 검색을 수행하거나 다양한 언어로 된 문서를 다루는 데 사용할 수 있습니다.</p>
<div class="content-ad"></div>
<p>이 글에서는 임베딩 주제를 깊이 있게 다루어보고자 합니다:</p>
<ul>
<li>임베딩이 만들어지기 전의 역사와 진화에 대해,</li>
<li>OpenAI 도구를 사용하여 임베딩을 계산하는 방법,</li>
<li>문장이 서로 가까운지 판단하는 방법,</li>
<li>임베딩을 시각화하는 방법,</li>
<li>가장 흥미로운 부분은 임베딩을 실제로 활용하는 방법입니다.</li>
</ul>
<p>이어서 나아가서 임베딩의 진화에 대해 배워보겠습니다.</p>
<h1>임베딩의 진화</h1>
<div class="content-ad"></div>
<p>우리는 텍스트 표현의 역사로 간단한 여행을 시작할 것입니다.</p>
<h2>단어 가방</h2>
<p>텍스트를 벡터로 변환하는 가장 기본적인 방법은 단어 가방입니다. 리처드 P. 페이만의 유명한 명언 중 하나를 살펴보겠습니다. "우리는 아직 발견들을 만들어내는 시대에 살고 있다". 이를 통해 단어 가방 접근법을 설명해보겠습니다.</p>
<p>단어 가방 벡터를 얻는 첫 번째 단계는 텍스트를 단어(토큰)로 나눈 다음, 단어를 기본 형태로 줄이는 것입니다. 예를 들어, "running"은 "run"으로 변환됩니다. 이 과정을 어간 추출(stemming)이라고 합니다. NLTK Python 패키지를 사용할 수 있습니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> nltk.<span class="hljs-property">stem</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">SnowballStemmer</span>
<span class="hljs-keyword">from</span> nltk.<span class="hljs-property">tokenize</span> <span class="hljs-keyword">import</span> word_tokenize

text = <span class="hljs-string">'We are lucky to live in an age in which we are still making discoveries'</span>

# 토큰화 - 텍스트를 단어로 나누기
words = <span class="hljs-title function_">word_tokenize</span>(text)
<span class="hljs-title function_">print</span>(words)
# [<span class="hljs-string">'We'</span>, <span class="hljs-string">'are'</span>, <span class="hljs-string">'lucky'</span>, <span class="hljs-string">'to'</span>, <span class="hljs-string">'live'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'an'</span>, <span class="hljs-string">'age'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'which'</span>,
#  <span class="hljs-string">'we'</span>, <span class="hljs-string">'are'</span>, <span class="hljs-string">'still'</span>, <span class="hljs-string">'making'</span>, <span class="hljs-string">'discoveries'</span>]

stemmer = <span class="hljs-title class_">SnowballStemmer</span>(language=<span class="hljs-string">"english"</span>)
stemmed_words = <span class="hljs-title function_">list</span>(<span class="hljs-title function_">map</span>(lambda <span class="hljs-attr">x</span>: stemmer.<span class="hljs-title function_">stem</span>(x), words))
<span class="hljs-title function_">print</span>(stemmed_words)
# [<span class="hljs-string">'we'</span>, <span class="hljs-string">'are'</span>, <span class="hljs-string">'lucki'</span>, <span class="hljs-string">'to'</span>, <span class="hljs-string">'live'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'an'</span>, <span class="hljs-string">'age'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'which'</span>,
#  <span class="hljs-string">'we'</span>, <span class="hljs-string">'are'</span>, <span class="hljs-string">'still'</span>, <span class="hljs-string">'make'</span>, <span class="hljs-string">'discoveri'</span>]
</code></pre>
<p>자, 이제 우리 단어들의 기본 형태 리스트가 있습니다. 다음 단계는 이들 빈도를 계산하여 벡터를 만드는 것입니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> collections
bag_of_words = collections.<span class="hljs-title class_">Counter</span>(stemmed_words)
<span class="hljs-title function_">print</span>(bag_of_words)
# {<span class="hljs-string">'we'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'are'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'in'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'lucki'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'to'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'live'</span>: <span class="hljs-number">1</span>, 
# <span class="hljs-string">'an'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'age'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'which'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'still'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'make'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'discoveri'</span>: <span class="hljs-number">1</span>}
</code></pre>
<p>사실, 만약 텍스트를 벡터로 변환하고 싶다면, 텍스트에 있는 단어뿐만 아니라 전체 어휘를 고려해야 합니다. "i", "you", "study"도 어휘에 있다고 가정하고, 파인만의 명언에서 벡터를 만들어 봅시다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_0.png" alt="image"></p>
<p>이 방법은 꽤 기본적이며 단어의 의미를 고려하지 않기 때문에 "그 여자는 데이터 과학을 공부하고 있다"와 "젊은 여성이 AI와 ML을 배우고 있다."라는 문장이 서로 가까운 위치에 있지 않을 수 있습니다.</p>
<h2>TF-IDF</h2>
<p>단어 가방 접근법의 약간 개선된 버전인 TF-IDF(Term Frequency — Inverse Document Frequency)입니다. 이것은 두 가지 지표의 곱셈입니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_1.png" alt="Markdown Table"></p>
<ul>
<li>용어 빈도는 문서에서 단어의 빈도를 보여줍니다. 이를 계산하는 가장 흔한 방법은 이 문서에서 용어의 로우 카운트(단어 가방에 있는 것처럼)을 전체 용어(단어) 수로 나누는 것입니다. 그러나 로우 카운트, 부욜리언 "빈도", 정규화에 대한 다양한 접근 방법이 많이 있습니다. 위키피디아에서 다양한 접근 방법에 대해 더 배울 수 있습니다.</li>
</ul>
<p><img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_2.png" alt="Markdown Table"></p>
<ul>
<li>역문서 주파수는 단어가 얼마나 많은 정보를 제공하는지를 나타냅니다. 예를 들어, "a"나 "that" 같은 단어는 문서 주제에 대해 추가 정보를 제공하지 않습니다. 대조적으로, "ChatGPT"나 "생물정보학" 같은 단어는 도메인을 정의하는 데 도움이 될 수 있습니다 (하지만 이 문장에는 해당하지 않음). 이는 전체 문서 수와 해당 단어를 포함하는 문서 수의 비율의 로그함수로 계산됩니다. IDF가 0에 가까울수록 단어가 흔하고 제공하는 정보가 더 적습니다.</li>
</ul>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_3.png">
<p>그래서 결과적으로 우리는 일반적인 단어 ("I"나 "you"와 같은)이 낮은 가중치를 갖는 벡터를 얻게됩니다. 한편, 문서에서 여러 번 발생하는 드문 단어들은 더 높은 가중치를 갖게 됩니다. 이 전략은 약간 더 나은 결과를 제공하지만 여전히 의미적 의미를 잡아내기는 어렵습니다.</p>
<p>이 방법론의 다른 어려움은 상당히 희소한 벡터를 생성한다는 점입니다. 벡터의 길이는 말뭉치 크기와 동일합니다. 영어에는 약 470,000개의 고유 단어가 있습니다(출처). 그러므로 우리는 거대한 벡터를 갖게 될 것입니다. 하지만 문장에는 50개 이상의 고유 단어가 나타나지 않을 것이므로 벡터의 값 중 99.99%는 0일 것입니다. 이는 어떤 정보도 인코딩하지 않습니다. 이에 대해 과학자들은 밀도 있는 벡터 표현에 대해 고민하기 시작했습니다.</p>
<h2>Word2Vec</h2>
<div class="content-ad"></div>
<p>가장 유명한 밀집 표현 방법 중 하나는 구글이 2013년에 Mikolov 등이 제안한 "효율적인 단어 표현 추정을 위한 Word2Vec" 논문에서 소개한 word2vec입니다.</p>
<p>논문에서 언급된 두 가지 word2vec 접근 방식은 Continuous Bag of Words(주변 단어를 기반으로 단어를 예측하는 방법)와 Skip-gram(반대 작업인 단어를 기반으로 문맥을 예측하는 방법)입니다.</p>
<p>밀집 벡터 표현의 핵심 아이디어는 두 모델을 훈련하는 것입니다: 인코더와 디코더. 예를 들어, Skip-gram의 경우 "christmas"라는 단어를 인코더에 전달할 수 있습니다. 그런 다음, 인코더가 "merry", "to", "you"와 같은 단어를 얻을 것으로 예상하여 디코더에 전달할 수 있는 벡터를 생성할 것입니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_5.png" alt="Image"></p>
<p>이 모델은 이제 단어의 의미를 고려하기 시작했습니다. 단어의 맥락에서 훈련되었기 때문입니다. 그러나 형태학(예: "-less"는 무언가의 부족을 의미함)을 무시합니다. 나중에는 GloVe에서 서브워드 스킵-그램을 살펴봄으로써 이 단점을 개선했습니다.</p>
<p>또한, word2vec은 단어와만 작동할 수 있었지만, 우리는 전체 문장을 인코딩하고 싶습니다. 그러니, 트랜스포머로 다음 진화 단계로 넘어가 봅시다.</p>
<h2>트랜스포머와 문장 임베딩</h2>
<div class="content-ad"></div>
<p>다음 진화는 Vaswani 등이 발표한 "Attention Is All You Need" 논문에서 소개된 트랜스포머 접근 방식과 관련이 있었습니다. 트랜스포머는 정보가 풍부한 밀집 벡터를 생성할 수 있었고 현대 언어 모델의 주요 기술로 자리 잡게 되었습니다.</p>
<p>저는 트랜스포머의 구조 세부 사항에 대해 다루지 않겠습니다. 왜냐하면 이것은 우리 주제와 관련이 그리 크지 않고 많은 시간이 소요되기 때문입니다. 더 배우고 싶다면 "Transformers, Explained" 또는 "The Illustrated Transformer"와 같은 다양한 자료가 많이 있습니다.</p>
<p>트랜스포머를 사용하면 동일한 "핵심" 모델을 사용하여 다른 사용 사례에 대해 미세 조정할 수 있으며, 핵심 모델을 다시 학습시킬 필요가 없습니다(시간이 많이 소요되고 상당한 비용이 듭니다). 이것은 사전 훈련된 모델의 등장으로 이어졌습니다. 가장 인기 있는 최초의 모델 중 하나는 Google AI가 개발한 BERT(Bidirectional Encoder Representations from Transformers)였습니다.</p>
<p>내부적으로 BERT는 여전히 word2vec과 유사한 토큰 수준에서 작동하지만, 우리는 여전히 문장 임베딩을 얻고 싶습니다. 따라서, 모든 토큰 벡터의 평균을 취하는 단순한 방법을 적용할 수 있습니다. 유감스럽게도, 이 방법은 좋은 성능을 보여주지 않습니다.</p>
<div class="content-ad"></div>
<p>2019년에 이 문제는 Sentence-BERT가 출시되면서 해결되었습니다. 이는 의미론적 텍스트 유사성 작업에서 이전 방법들을 모두 능가하며 문장 포함 벡터의 계산을 가능하게 했습니다.</p>
<p>이 주제는 매우 방대하기 때문에 이 기사에서 모두 다 다룰 수는 없을 거예요. 그러니 진지하게 관심이 있다면 이 기사에서 문장 포함 벡터에 대해 더 배울 수 있습니다.</p>
<p>우리는 임베딩의 발전을 간략히 다뤘고 이론에 대한 고수준 이해를 얻었습니다. 이제 실습으로 넘어가서 OpenAI 도구를 사용하여 어떻게 임베딩을 계산하는지 배워보겠습니다.</p>
<h1>임베딩 계산</h1>
<div class="content-ad"></div>
<p>이 기사에서는 OpenAI 임베딩을 사용할 것입니다. 최근에 출시된 새로운 모델인 text-embedding-3-small을 시도해볼 것입니다. 이 새로운 모델은 text-embedding-ada-002보다 성능이 더 좋게 나타났습니다:</p>
<ul>
<li>널리 사용되는 다국어 검색 (MIRACL) 벤치마크의 평균 점수가 31.4%에서 44.0%로 상승했습니다.</li>
<li>영어 작업에 대한 자주 사용되는 벤치마크인 MTEB의 평균 성능도 향상되어 61.0%에서 62.3%로 상승했습니다.</li>
</ul>
<p>OpenAI는 또한 새로운 큰 모델인 text-embedding-3-large를 출시했습니다. 이제 이것이 가장 우수한 임베딩 모델입니다.</p>
<p>데이터 소스로는 Stack Exchange Data Dump의 작은 샘플을 사용할 것입니다. 이는 Stack Exchange 네트워크에서 모든 사용자 기여 콘텐츠의 익명화된 덤프입니다. 저는 흥미로운 주제를 선택하고 각각에서 100개의 질문을 샘플링했습니다. 주제는 생성적 AI부터 커피 또는 자전거까지 다양합니다. 그래서 다양한 주제를 볼 수 있을 겁니다.</p>
<div class="content-ad"></div>
<p>먼저, 모든 스택 오버플로우 질문에 대한 임베딩을 계산해야 합니다. 한 번 실행하고 결과를 로컬로 저장하는 것이 좋습니다(파일이나 벡터 저장소에). OpenAI Python 패키지를 사용하여 임베딩을 생성할 수 있습니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> openai <span class="hljs-keyword">import</span> OpenAI
client = OpenAI()

<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embedding</span>(<span class="hljs-params">text, model=<span class="hljs-string">"text-embedding-3-small"</span></span>):
   text = text.replace(<span class="hljs-string">"\n"</span>, <span class="hljs-string">" "</span>)
   <span class="hljs-keyword">return</span> client.embeddings.create(<span class="hljs-built_in">input</span> = [text], model=model)\
       .data[<span class="hljs-number">0</span>].embedding

get_embedding(<span class="hljs-string">"We are lucky to live in an age in which we are still making discoveries."</span>)
</code></pre>
<p>결과적으로, 우리는 부동 소수점 숫자로 이루어진 1536차원 벡터를 얻습니다. 이제 이를 모든 데이터에 대해 반복하고 값을 분석할 수 있습니다.</p>
<p>가장 궁금할 수 있는 주요 질문은 의미적으로 문장들이 얼마나 가까운지입니다. 답을 발견하기 위해 벡터 간의 거리 개념을 논의해 보겠습니다.</p>
<div class="content-ad"></div>
<h1>벡터 간 거리</h1>
<p>임베딩은 사실 벡터입니다. 따라서 두 문장이 얼마나 가까운지 이해하려면 벡터 간 거리를 계산할 수 있습니다. 더 작은 거리는 더 가까운 의미를 나타낼 것입니다.</p>
<p>두 벡터 간의 거리를 측정하는 데 사용할 수 있는 다양한 메트릭이 있습니다:</p>
<ul>
<li>유클리디안 거리 (L2),</li>
<li>맨하탄 거리 (L1),</li>
<li>내적 (Dot product),</li>
<li>코사인 거리.</li>
</ul>
<div class="content-ad"></div>
<p>그들에 대해 이야기해 봅시다. 간단한 예로, 우리는 두 개의 2D 벡터를 사용할 것입니다.</p>
<pre><code class="hljs language-js">vector1 = [<span class="hljs-number">1</span>, <span class="hljs-number">4</span>]
vector2 = [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>]
</code></pre>
<h2>유클리디안 거리 (L2)</h2>
<p>두 지점(또는 벡터) 사이의 거리를 정의하는 가장 표준적인 방법은 유클리디안 거리 또는 L2 norm입니다. 이 측정 기준은 일상생활에서 가장 많이 사용되며, 예를 들어 2개의 도시 사이의 거리를 언급할 때 사용됩니다.</p>
<div class="content-ad"></div>
<p>L2 거리에 대한 시각적 표현과 공식이 있습니다.</p>
<p><img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_6.png" alt="Image"></p>
<p>파이썬 또는 numpy 함수를 사용하여 이 메트릭을 계산할 수 있습니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-built_in">sum</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x, y: (x - y) ** <span class="hljs-number">2</span>, vector1, vector2))) ** <span class="hljs-number">0.5</span>
<span class="hljs-comment"># 2.2361</span>

np.linalg.norm((np.array(vector1) - np.array(vector2)), <span class="hljs-built_in">ord</span> = <span class="hljs-number">2</span>)
<span class="hljs-comment"># 2.2361</span>
</code></pre>
<div class="content-ad"></div>
<h1>맨해튼 거리 (L1)</h1>
<p>다른 일반적으로 사용되는 거리 측정 방법은 L1 노름 또는 맨해튼 거리입니다. 이 거리는 뉴욕의 맨해튼 섬에서 명명되었습니다. 이 섬은 거리가 격자 레이아웃으로 되어 있고, 맨해튼에서 두 지점 사이의 가장 짧은 경로는 격자 모양을 따라야 하므로 L1 거리가 됩니다.</p>
<p><img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_7.png" alt="image"></p>
<p>우리는 이를 처음부터 구현하거나 numpy 함수를 사용하여 구현할 수도 있습니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-title function_">sum</span>(<span class="hljs-title function_">list</span>(<span class="hljs-title function_">map</span>(lambda x, <span class="hljs-attr">y</span>: <span class="hljs-title function_">abs</span>(x - y), vector1, vector2)))
# <span class="hljs-number">3</span>

np.<span class="hljs-property">linalg</span>.<span class="hljs-title function_">norm</span>((np.<span class="hljs-title function_">array</span>(vector1) - np.<span class="hljs-title function_">array</span>(vector2)), ord = <span class="hljs-number">1</span>)
# <span class="hljs-number">3.0</span>
</code></pre>
<h2>내적(Dot product)</h2>
<p>벡터 간 거리를 계산하는 다른 방법은 내적 또는 스칼라 곱을 계산하는 것입니다. 다음은 해당 공식이며 쉽게 구현할 수 있습니다.</p>
<img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_8.png">
<div class="content-ad"></div>
<pre><code class="hljs language-python"><span class="hljs-built_in">sum</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x, y: x*y, vector1, vector2)))
<span class="hljs-comment"># 11</span>

np.dot(vector1, vector2)
<span class="hljs-comment"># 11</span>
</code></pre>
<p>이 메트릭은 해석하기가 조금 까다로운 편이에요. 한편으로는 벡터가 한 방향을 향하고 있는지를 보여줍니다. 다른 한편으로는 결과는 벡터들의 크기에 크게 의존합니다. 예를 들어 두 쌍의 벡터 간의 내적을 계산해볼게요:</p>
<ul>
<li>(1, 1) vs (1, 1)</li>
<li>(1, 1) vs (10, 10).</li>
</ul>
<p>두 경우 모두 벡터가 일직선상에 있지만, 두 번째 경우에 내적은 10배 크게 나와요: 2 대 20.</p>
<div class="content-ad"></div>
<h2>코사인 유사도</h2>
<p>많은 경우, 코사인 유사도가 사용됩니다. 코사인 유사도는 벡터의 크기(또는 노름)에 의해 정규화된 내적입니다.</p>
<p><img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_9.png" alt="Image"></p>
<p>이전처럼 직접 모든 것을 계산하거나 sklearn의 함수를 사용할 수 있습니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">dot_product = <span class="hljs-title function_">sum</span>(<span class="hljs-title function_">list</span>(<span class="hljs-title function_">map</span>(lambda x, <span class="hljs-attr">y</span>: x*y, vector1, vector2)))
norm_vector1 = <span class="hljs-title function_">sum</span>(<span class="hljs-title function_">list</span>(<span class="hljs-title function_">map</span>(lambda <span class="hljs-attr">x</span>: x ** <span class="hljs-number">2</span>, vector1))) ** <span class="hljs-number">0.5</span>
norm_vector2 = <span class="hljs-title function_">sum</span>(<span class="hljs-title function_">list</span>(<span class="hljs-title function_">map</span>(lambda <span class="hljs-attr">x</span>: x ** <span class="hljs-number">2</span>, vector2))) ** <span class="hljs-number">0.5</span>

dot_product/norm_vector1/norm_vector2

# <span class="hljs-number">0.8575</span>

<span class="hljs-keyword">from</span> sklearn.<span class="hljs-property">metrics</span>.<span class="hljs-property">pairwise</span> <span class="hljs-keyword">import</span> cosine_similarity

<span class="hljs-title function_">cosine_similarity</span>(
  np.<span class="hljs-title function_">array</span>(vector1).<span class="hljs-title function_">reshape</span>(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>), 
  np.<span class="hljs-title function_">array</span>(vector2).<span class="hljs-title function_">reshape</span>(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>))[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]

# <span class="hljs-number">0.8575</span>
</code></pre>
<p>cosine_similarity 함수는 2차원 배열을 기대합니다. 그래서 numpy 배열을 reshape 해주어야 합니다.</p>
<p>이 메트릭의 물리적 의미에 대해 조금 이야기해 봅시다. Cosine similarity는 두 벡터 사이의 코사인 값과 같습니다. 벡터가 서로 가까울수록 메트릭 값이 높아집니다.</p>
<img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_10.png">
<div class="content-ad"></div>
<p>우리는 심지어 벡터 사이의 정확한 각도를 도 단위로 계산할 수도 있어요. 약 30도 정도의 결과를 얻었고, 꽤 합리적으로 보이네요.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> math
math.<span class="hljs-title function_">degrees</span>(math.<span class="hljs-title function_">acos</span>(<span class="hljs-number">0.8575</span>))

# <span class="hljs-number">30.96</span>
</code></pre>
<h2>어떤 측정 지표를 사용할까요?</h2>
<p>우리는 두 벡터 사이의 거리를 계산하는 다양한 방법에 대해 토론해 왔고, 여러분은 어떤 방법을 사용할지 고려하기 시작할 수 있을 거예요.</p>
<div class="content-ad"></div>
<p>내가 가진 임베딩을 비교하기 위해 어떤 거리든 사용할 수 있어요. 예를 들어, 다른 클러스터 사이의 평균 거리를 계산했어요. L2 거리와 코사인 유사도 모두 비슷한 결과를 보여줘요:</p>
<ul>
<li>클러스터 내의 객체들은 다른 클러스터보다 서로 더 가까워요. L2 거리에 대해 가까울수록 낮은 거리를 의미하지만 코사인 유사도에서는 가까운 객체일수록 값이 높아져요. 헷갈리지 마세요.</li>
<li>"정치"와 "경제" 또는 "ai"와 "데이터과학"과 같이 일부 주제들이 서로 아주 가까운 것을 알 수 있어요.</li>
</ul>
<img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_11.png">
<img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_12.png">
<div class="content-ad"></div>
<p>그러나 NLP 작업에 대해서는 일반적으로 코사인 유사도를 사용하는 것이 최선의 방법입니다. 몇 가지 그 이유는:</p>
<ul>
<li>코사인 유사도는 -1과 1 사이에 있으며, L1과 L2는 무제한이기 때문에 해석하기 쉽습니다.</li>
<li>실용적인 측면에서 유클리드 거리의 제곱근보다 내적을 계산하는 것이 더 효과적입니다.</li>
<li>코사인 유사도는 차원의 저주에 영향을 덜 받습니다 (이에 대해 뒤에서 더 얘기할 것입니다).</li>
</ul>
<p>위의 결과에서 인트라 및 인터 클러스터 거리 간의 차이가 크지 않다는 점을 알 수 있을 것입니다. 이 현상의 원인은 벡터의 고차원성 때문입니다. 이 효과는 "차원의 저주"라고 불리며, 차원이 높을수록 벡터 간 거리 분포가 좁아진다는 것을 알 수 있습니다. 이에 대해 더 자세히 알아보려면 이 글을 참조해보세요.</p>
<p>간단히 설명드리겠습니다. OpenAI 임베딩 값의 분포를 계산하고 차원이 다른 300개의 벡터 집합을 생성했습니다. 그런 다음, 모든 벡터 사이의 거리를 계산하고 히스토그램을 그렸습니다. 차원이 증가함에 따라 벡터의 거리 분포가 좁아진다는 것을 쉽게 확인할 수 있습니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_13.png" alt="image"></p>
<p>임베딩 사이의 유사성을 측정하는 방법을 배웠어요. 여기서 이론적인 부분은 마쳤고, 더 실용적인 부분(시각화 및 실용적인 응용)으로 넘어가겠습니다. 데이터를 보는 것이 가장 중요하니, 시각화부터 시작해봐요.</p>
<h1>임베딩 시각화</h1>
<p>데이터를 이해하는 가장 좋은 방법은 시각적으로 나타내는 것이에요. 아쉽지만, 임베딩은 1536차원이 있어서 데이터를 살펴보기가 꽤 어려워요. 그러나, 한 가지 방법이 있어요: 차원 축소 기술을 사용하여 벡터를 이차원 공간에 투영하는 것이에요.</p>
<div class="content-ad"></div>
<h2>PCA</h2>
<p>가장 기본적인 차원 축소 기술은 PCA(주성분 분석)입니다. 이를 사용해 봅시다.</p>
<p>먼저, sklearn에 전달하기 위해 임베딩을 2D numpy 배열로 변환해야 합니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
embeddings_array = np.array(df.embedding.values.tolist())
<span class="hljs-built_in">print</span>(embeddings_array.shape)
<span class="hljs-comment"># (1400, 1536)</span>
</code></pre>
<div class="content-ad"></div>
<p>그럼, 우리는 PCA 모델을 n_components = 2로 초기화해야 해요 (2D 시각화를 생성하고 싶기 때문에), 전체 데이터에서 모델을 학습하고 새로운 값을 예측해야 해요.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> sklearn.<span class="hljs-property">decomposition</span> <span class="hljs-keyword">import</span> <span class="hljs-variable constant_">PCA</span>

pca_model = <span class="hljs-title function_">PCA</span>(n_components = <span class="hljs-number">2</span>)
pca_model.<span class="hljs-title function_">fit</span>(embeddings_array)

pca_embeddings_values = pca_model.<span class="hljs-title function_">transform</span>(embeddings_array)
<span class="hljs-title function_">print</span>(pca_embeddings_values.<span class="hljs-property">shape</span>)
# (<span class="hljs-number">1400</span>, <span class="hljs-number">2</span>)
</code></pre>
<p>결과적으로, 우리는 각 질문에 대해 두 개의 특성을 가진 행렬을 얻었으므로, scatter plot에서 쉽게 시각화할 수 있어요.</p>
<pre><code class="hljs language-js">fig = px.<span class="hljs-title function_">scatter</span>(
    x = pca_embeddings_values[:,<span class="hljs-number">0</span>], 
    y = pca_embeddings_values[:,<span class="hljs-number">1</span>],
    color = df.<span class="hljs-property">topic</span>.<span class="hljs-property">values</span>,
    hover_name = df.<span class="hljs-property">full_text</span>.<span class="hljs-property">values</span>,
    title = <span class="hljs-string">'PCA embeddings'</span>, width = <span class="hljs-number">800</span>, height = <span class="hljs-number">600</span>,
    color_discrete_sequence = plotly.<span class="hljs-property">colors</span>.<span class="hljs-property">qualitative</span>.<span class="hljs-property">Alphabet_r</span>
)

fig.<span class="hljs-title function_">update_layout</span>(
    xaxis_title = <span class="hljs-string">'first component'</span>, 
    yaxis_title = <span class="hljs-string">'second component'</span>)
fig.<span class="hljs-title function_">show</span>()
</code></pre>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_14.png" alt="img"></p>
<p>각 주제의 질문들이 서로 꽤 가까이 위치해 있는 것을 볼 수 있어 좋습니다. 그러나 모든 클러스터가 혼재되어 있어서 개선할 부분이 있습니다.</p>
<h2>t-SNE</h2>
<p>PCA는 선형 알고리즘이지만, 대부분의 관계는 실제로는 비선형입니다. 그래서 비선형성 때문에 클러스터를 분리할 수 없을 수도 있습니다. 비선형 알고리즘인 t-SNE을 사용해보고 더 나은 결과를 보여줄 수 있는지 확인해봅시다.</p>
<div class="content-ad"></div>
<p>거의 동일한 코드를 사용했습니다. PCA 대신 t-SNE 모델을 사용했어요.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> sklearn.<span class="hljs-property">manifold</span> <span class="hljs-keyword">import</span> <span class="hljs-variable constant_">TSNE</span>
tsne_model = <span class="hljs-title function_">TSNE</span>(n_components=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">42</span>)
tsne_embeddings_values = tsne_model.<span class="hljs-title function_">fit_transform</span>(embeddings_array)

fig = px.<span class="hljs-title function_">scatter</span>(
    x = tsne_embeddings_values[:,<span class="hljs-number">0</span>], 
    y = tsne_embeddings_values[:,<span class="hljs-number">1</span>],
    color = df.<span class="hljs-property">topic</span>.<span class="hljs-property">values</span>,
    hover_name = df.<span class="hljs-property">full_text</span>.<span class="hljs-property">values</span>,
    title = <span class="hljs-string">'t-SNE embeddings'</span>, width = <span class="hljs-number">800</span>, height = <span class="hljs-number">600</span>,
    color_discrete_sequence = plotly.<span class="hljs-property">colors</span>.<span class="hljs-property">qualitative</span>.<span class="hljs-property">Alphabet_r</span>
)

fig.<span class="hljs-title function_">update_layout</span>(
    xaxis_title = <span class="hljs-string">'first component'</span>, 
    yaxis_title = <span class="hljs-string">'second component'</span>)
fig.<span class="hljs-title function_">show</span>()
</code></pre>
<p>t-SNE 결과가 훨씬 좋아 보여요. 대부분의 클러스터가 분리되어 있지만 "genai", "datascience", "ai" 는 분리되지 않았어요. 그러나 이건 예상한대로에요 - 이러한 주제를 내가 분리할 수 있을지 의심스러워요.</p>
<img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_15.png">
<div class="content-ad"></div>
<p>이 시각화를 보면 임베딩이 의미적 의미를 인코딩하는 데 상당히 효과적임을 확인할 수 있어요.</p>
<p>또한, 데이터를 3D로 시각화할 수 있는 사영(projection)을 만들어볼 수 있어요. 실용적일지는 확실하지 않지만, 데이터를 3D로 살펴보는 것은 흥미롭고 관심을 끌 수 있어요.</p>
<pre><code class="hljs language-js">tsne_model_3d = <span class="hljs-title function_">TSNE</span>(n_components=<span class="hljs-number">3</span>, random_state=<span class="hljs-number">42</span>)
tsne_3d_embeddings_values = tsne_model_3d.<span class="hljs-title function_">fit_transform</span>(embeddings_array)

fig = px.<span class="hljs-title function_">scatter_3d</span>(
    x = tsne_3d_embeddings_values[:,<span class="hljs-number">0</span>], 
    y = tsne_3d_embeddings_values[:,<span class="hljs-number">1</span>],
    z = tsne_3d_embeddings_values[:,<span class="hljs-number">2</span>],
    color = df.<span class="hljs-property">topic</span>.<span class="hljs-property">values</span>,
    hover_name = df.<span class="hljs-property">full_text</span>.<span class="hljs-property">values</span>,
    title = <span class="hljs-string">'t-SNE embeddings'</span>, width = <span class="hljs-number">800</span>, height = <span class="hljs-number">600</span>,
    color_discrete_sequence = plotly.<span class="hljs-property">colors</span>.<span class="hljs-property">qualitative</span>.<span class="hljs-property">Alphabet_r</span>,
    opacity = <span class="hljs-number">0.7</span>
)
fig.<span class="hljs-title function_">update_layout</span>(xaxis_title = <span class="hljs-string">'first component'</span>, yaxis_title = <span class="hljs-string">'second component'</span>)
fig.<span class="hljs-title function_">show</span>()
</code></pre>
<p><img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_16.png" alt="3D 시각화"></p>
<div class="content-ad"></div>
<h2>바코드</h2>
<p>임베딩을 이해하는 방법은 몇 개를 바코드처럼 시각화하여 상관 관계를 확인하는 것입니다. 나는 세 가지 임베딩 예시를 선택했습니다: 두 개는 서로에게 가장 가깝고, 나머지 하나는 데이터 세트에서 가장 멀리 떨어져 있는 예시입니다.</p>
<pre><code class="hljs language-js">embedding1 = df.<span class="hljs-property">loc</span>[<span class="hljs-number">1</span>].<span class="hljs-property">embedding</span>
embedding2 = df.<span class="hljs-property">loc</span>[<span class="hljs-number">616</span>].<span class="hljs-property">embedding</span>
embedding3 = df.<span class="hljs-property">loc</span>[<span class="hljs-number">749</span>].<span class="hljs-property">embedding</span>
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">import</span> matplotlib.<span class="hljs-property">pyplot</span> <span class="hljs-keyword">as</span> plt
embed_len_thr = <span class="hljs-number">1536</span>

sns.<span class="hljs-title function_">heatmap</span>(np.<span class="hljs-title function_">array</span>(embedding1[:embed_len_thr]).<span class="hljs-title function_">reshape</span>(-<span class="hljs-number">1</span>, embed_len_thr),
    cmap = <span class="hljs-string">"Greys"</span>, center = <span class="hljs-number">0</span>, square = <span class="hljs-title class_">False</span>, 
    xticklabels = <span class="hljs-title class_">False</span>, cbar = <span class="hljs-title class_">False</span>)
plt.<span class="hljs-title function_">gcf</span>().<span class="hljs-title function_">set_size_inches</span>(<span class="hljs-number">15</span>,<span class="hljs-number">1</span>)
plt.<span class="hljs-title function_">yticks</span>([<span class="hljs-number">0.5</span>], labels = [<span class="hljs-string">'AI'</span>])
plt.<span class="hljs-title function_">show</span>()

sns.<span class="hljs-title function_">heatmap</span>(np.<span class="hljs-title function_">array</span>(embedding3[:embed_len_thr]).<span class="hljs-title function_">reshape</span>(-<span class="hljs-number">1</span>, embed_len_thr),
    cmap = <span class="hljs-string">"Greys"</span>, center = <span class="hljs-number">0</span>, square = <span class="hljs-title class_">False</span>, 
    xticklabels = <span class="hljs-title class_">False</span>, cbar = <span class="hljs-title class_">False</span>)
plt.<span class="hljs-title function_">gcf</span>().<span class="hljs-title function_">set_size_inches</span>(<span class="hljs-number">15</span>,<span class="hljs-number">1</span>)
plt.<span class="hljs-title function_">yticks</span>([<span class="hljs-number">0.5</span>], labels = [<span class="hljs-string">'AI'</span>])
plt.<span class="hljs-title function_">show</span>()

sns.<span class="hljs-title function_">heatmap</span>(np.<span class="hljs-title function_">array</span>(embedding2[:embed_len_thr]).<span class="hljs-title function_">reshape</span>(-<span class="hljs-number">1</span>, embed_len_thr),
    cmap = <span class="hljs-string">"Greys"</span>, center = <span class="hljs-number">0</span>, square = <span class="hljs-title class_">False</span>, 
    xticklabels = <span class="hljs-title class_">False</span>, cbar = <span class="hljs-title class_">False</span>)
plt.<span class="hljs-title function_">gcf</span>().<span class="hljs-title function_">set_size_inches</span>(<span class="hljs-number">15</span>,<span class="hljs-number">1</span>)
plt.<span class="hljs-title function_">yticks</span>([<span class="hljs-number">0.5</span>], labels = [<span class="hljs-string">'바이오인포매틱스'</span>])
plt.<span class="hljs-title function_">show</span>()
</code></pre>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_17.png" alt="이미지"></p>
<p>우리 경우에는 고차원 때문에 벡터가 서로 가까운지 쉽게 보기 어려울 수 있습니다. 그래도 나는 이 시각화를 좋아합니다. 몇 가지 경우에 도움이 될 수도 있으니, 나는 이 아이디어를 당신과 공유하고자 합니다.</p>
<p>우리는 임베딩을 시각화하는 방법을 배웠고, 텍스트의 의미를 파악하는 능력에 대한 의문은 남지 않았습니다. 이제 실제로 임베딩을 어떻게 활용할 수 있는지에 대해 논의하는 가장 흥미로운 부분으로 넘어가 보겠습니다.</p>
<h1>실용적인 응용</h1>
<div class="content-ad"></div>
<p>물론, 임베딩의 주요 목표는 텍스트를 숫자의 벡터로 인코딩하거나 시각화하기 위해서만 하는 것이 아닙니다. 우리는 텍스트의 의미를 포착하는 능력에서 많은 이점을 얻을 수 있습니다. 실용적인 예제들을 함께 살펴보겠습니다.</p>
<h2>클러스터링</h2>
<p>먼저 클러스터링부터 시작해보죠. 클러스터링은 초기 레이블 없이 데이터를 그룹으로 분할할 수 있는 비지도학습 기술입니다. 클러스터링을 통해 데이터의 내부 구조적 패턴을 이해하는 데 도움을 받을 수 있습니다.</p>
<p>가장 기본적인 클러스터링 알고리즘 중 하나인 K-평균을 사용할 것입니다. K-평균 알고리즘을 위해서는 클러스터의 개수를 지정해야 합니다. 실루엣 스코어를 사용하여 최적의 클러스터 수를 정의할 수 있습니다.</p>
<div class="content-ad"></div>
<p>2부터 50까지의 k (클러스터 수)를 시도해 보겠습니다. 각 k에 대해 모델을 훈련하고 실루엣 점수를 계산할 것입니다. 실루엣 점수가 높을수록, 더 좋은 클러스터링 결과를 얻을 수 있습니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> silhouette_score
<span class="hljs-keyword">import</span> tqdm

silhouette_scores = []
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> tqdm.tqdm(<span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, <span class="hljs-number">51</span>)):
    kmeans = KMeans(n_clusters=k, 
                    random_state=<span class="hljs-number">42</span>, 
                    n_init=<span class="hljs-string">'auto'</span>).fit(embeddings_array)
    kmeans_labels = kmeans.labels_
    silhouette_scores.append(
        {
            <span class="hljs-string">'k'</span>: k,
            <span class="hljs-string">'silhouette_score'</span>: silhouette_score(embeddings_array, 
                                                 kmeans_labels, metric=<span class="hljs-string">'cosine'</span>)
        }
    )

fig = px.line(pd.DataFrame(silhouette_scores).set_index(<span class="hljs-string">'k'</span>),
              title=<span class="hljs-string">'&#x3C;b>K-means 클러스터링을 위한 실루엣 점수&#x3C;/b>'</span>,
              labels={<span class="hljs-string">'value'</span>: <span class="hljs-string">'실루엣 점수'</span>}, 
              color_discrete_sequence=plotly.colors.qualitative.Alphabet)
fig.update_layout(showlegend=<span class="hljs-literal">False</span>)
</code></pre>
<p>우리의 경우, k가 11일 때 실루엣 점수가 최대치에 도달합니다. 따라서 최종 모델에는 이 클러스터 수를 사용합시다.</p>
<img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_18.png">
<div class="content-ad"></div>
<p>클러스터를 시각화해 보는 t-SNE를 이용한 차원 축소를 이미 이전에 수행한 것처럼 해보겠습니다.</p>
<pre><code class="hljs language-js">tsne_model = <span class="hljs-title function_">TSNE</span>(n_components=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">42</span>)
tsne_embeddings_values = tsne_model.<span class="hljs-title function_">fit_transform</span>(embeddings_array)

fig = px.<span class="hljs-title function_">scatter</span>(
    x = tsne_embeddings_values[:,<span class="hljs-number">0</span>], 
    y = tsne_embeddings_values[:,<span class="hljs-number">1</span>],
    color = <span class="hljs-title function_">list</span>(<span class="hljs-title function_">map</span>(lambda <span class="hljs-attr">x</span>: <span class="hljs-string">'클러스터 %s'</span> % x, kmeans_labels)),
    hover_name = df.<span class="hljs-property">full_text</span>.<span class="hljs-property">values</span>,
    title = <span class="hljs-string">'클러스터링을 위한 t-SNE 임베딩'</span>, width = <span class="hljs-number">800</span>, height = <span class="hljs-number">600</span>,
    color_discrete_sequence = plotly.<span class="hljs-property">colors</span>.<span class="hljs-property">qualitative</span>.<span class="hljs-property">Alphabet_r</span>
)
fig.<span class="hljs-title function_">update_layout</span>(
    xaxis_title = <span class="hljs-string">'첫 번째 성분'</span>, 
    yaxis_title = <span class="hljs-string">'두 번째 성분'</span>)
fig.<span class="hljs-title function_">show</span>()
</code></pre>
<p>시각적으로 알고리즘이 클러스터를 상당히 잘 정의했음을 확인할 수 있습니다 — 그들은 꽤 잘 분리되어 있습니다.</p>
<img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_19.png">
<div class="content-ad"></div>
<p>우리는 사실적인 주제 라벨을 가지고 있으므로, 클러스터링이 얼마나 좋은지를 심층적으로 판단할 수도 있어요. 각 클러스터에 대한 주제 혼합을 살펴봅시다.</p>
<pre><code class="hljs language-js">df[<span class="hljs-string">'cluster'</span>] = <span class="hljs-title function_">list</span>(<span class="hljs-title function_">map</span>(lambda <span class="hljs-attr">x</span>: <span class="hljs-string">'클러스터 %s'</span> % x, kmeans_labels))
cluster_stats_df = df.<span class="hljs-title function_">reset_index</span>().<span class="hljs-title function_">pivot_table</span>(
    index=<span class="hljs-string">'cluster'</span>, values=<span class="hljs-string">'id'</span>,
    aggfunc=<span class="hljs-string">'count'</span>, columns=<span class="hljs-string">'topic'</span>).<span class="hljs-title function_">fillna</span>(<span class="hljs-number">0</span>).<span class="hljs-title function_">applymap</span>(int)

cluster_stats_df = cluster_stats_df.<span class="hljs-title function_">apply</span>(
  lambda <span class="hljs-attr">x</span>: <span class="hljs-number">100</span>*x/cluster_stats_df.<span class="hljs-title function_">sum</span>(axis=<span class="hljs-number">1</span>))

fig = px.<span class="hljs-title function_">imshow</span>(
    cluster_stats_df.<span class="hljs-property">values</span>, 
    x=cluster_stats_df.<span class="hljs-property">columns</span>,
    y=cluster_stats_df.<span class="hljs-property">index</span>,
    text_auto=<span class="hljs-string">'.2f'</span>, aspect=<span class="hljs-string">"auto"</span>,
    labels=<span class="hljs-title function_">dict</span>(x=<span class="hljs-string">"클러스터"</span>, y=<span class="hljs-string">"팩트 주제"</span>, color=<span class="hljs-string">"비율, %"</span>),
    color_continuous_scale=<span class="hljs-string">'pubugn'</span>,
    title=<span class="hljs-string">'&#x3C;b>각 클러스터의 주제 비율&#x3C;/b>'</span>, height=<span class="hljs-number">550</span>)

fig.<span class="hljs-title function_">show</span>()
</code></pre>
<img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_20.png">
<p>대부분의 경우, 클러스터링은 완벽하게 작동했어요. 예를 들어, 클러스터 5에는 거의 자전거에 관한 질문만 있고, 클러스터 6은 커피에 관한 것이에요. 그러나 유사한 주제를 구별하지 못했어요:</p>
<div class="content-ad"></div>
<ul>
<li>"ai," "genai," and "datascience"은 동일한 클러스터에 있습니다.</li>
<li>"economics"와 "politics"은 같은 그룹에 속합니다.</li>
</ul>
<p>이 예제에서는 피처로써 임베딩만 사용했지만, 질문을 한 사용자의 나이, 성별 또는 국가와 같은 추가 정보가 있다면 모델에 포함시킬 수도 있습니다.</p>
<h2>분류</h2>
<p>임베딩을 분류 또는 회귀 작업에 사용할 수 있습니다. 예를 들어 고객 리뷰 감정을 예측하는 (분류)이나 NPS 점수를 예측하는 (회귀) 등 다양한 작업에 활용할 수 있습니다.</p>
<div class="content-ad"></div>
<p>분류 및 회귀는 지도 학습이므로 레이블이 필요합니다. 다행히도, 우리는 질문의 주제를 알고 있으므로 모델을 적합시켜 예측할 수 있습니다.</p>
<p>저는 랜덤 포레스트 분류기를 사용할 것입니다. 랜덤 포레스트에 대해 간단히 상기하고 싶다면 여기에서 확인할 수 있어요. 분류 모델의 성능을 올바르게 평가하려면 데이터 세트를 학습 및 테스트 세트(80% 대 20%)로 분할할 것입니다. 그런 다음 학습 데이터 세트에서 모델을 훈련하고 테스트 데이터 세트에서 품질을 측정할 수 있습니다(모델이 이전에 보지 못한 질문).</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> sklearn.<span class="hljs-property">ensemble</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">RandomForestClassifier</span>
<span class="hljs-keyword">from</span> sklearn.<span class="hljs-property">model_selection</span> <span class="hljs-keyword">import</span> train_test_split
class_model = <span class="hljs-title class_">RandomForestClassifier</span>(max_depth = <span class="hljs-number">10</span>)

# 특징 및 대상 정의
X = embeddings_array
y = df.<span class="hljs-property">topic</span>

# 데이터를 학습 및 테스트 세트로 분할
X_train, X_test, y_train, y_test = <span class="hljs-title function_">train_test_split</span>(
    X, y, random_state = <span class="hljs-number">42</span>, test_size=<span class="hljs-number">0.2</span>, stratify=y
)

# 적합 및 예측
class_model.<span class="hljs-title function_">fit</span>(X_train, y_train)
y_pred = class_model.<span class="hljs-title function_">predict</span>(X_test)
</code></pre>
<p>모델의 성능을 추정하기 위해 혼동 행렬을 계산해 보겠습니다. 이상적인 상황에서는 비대각 요소가 모두 0이어야 합니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> sklearn.<span class="hljs-property">metrics</span> <span class="hljs-keyword">import</span> confusion_matrix
cm = <span class="hljs-title function_">confusion_matrix</span>(y_test, y_pred)

fig = px.<span class="hljs-title function_">imshow</span>(
  cm, x = class_model.<span class="hljs-property">classes_</span>,
  y = class_model.<span class="hljs-property">classes_</span>, text_auto=<span class="hljs-string">'d'</span>, 
  aspect=<span class="hljs-string">"auto"</span>, 
  labels=<span class="hljs-title function_">dict</span>(
      x=<span class="hljs-string">"predicted label"</span>, y=<span class="hljs-string">"true label"</span>, 
      color=<span class="hljs-string">"cases"</span>), 
  color_continuous_scale=<span class="hljs-string">'pubugn'</span>,
  title = <span class="hljs-string">'&#x3C;b>혼동 행렬&#x3C;/b>'</span>, height = <span class="hljs-number">550</span>)

fig.<span class="hljs-title function_">show</span>()
</code></pre>
<p><img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_21.png" alt="이미지"></p>
<p>군집화와 유사한 결과를 확인할 수 있습니다. 일부 주제는 쉽게 분류되고 정확도가 100%인 반면, 다른 주제들은 구별하기 어려운 경우도 있습니다(특히 "ai" 주제).</p>
<p>하지만 전체적으로 91.8%의 정확도를 달성했으며, 이는 꽤 좋은 성과입니다.</p>
<div class="content-ad"></div>
<h2>이상 징후 찾기</h2>
<p>데이터에서 이상 징후를 찾기 위해 임베딩을 사용할 수도 있습니다. 예를 들어, t-SNE 그래프에서 "여행" 주제에 대한 몇 가지 질문이 군집에서 꽤 멀리 떨어져 있는 것을 볼 수 있었습니다. 이 테마를 살펴보고 이상 징후를 찾아보겠습니다. 이를 위해 이상 탐지 알고리즘인 Isolation Forest를 사용할 것입니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> sklearn.<span class="hljs-property">ensemble</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">IsolationForest</span>

topic_df = df[df.<span class="hljs-property">topic</span> == <span class="hljs-string">'travel'</span>]
topic_embeddings_array = np.<span class="hljs-title function_">array</span>(topic_df.<span class="hljs-property">embedding</span>.<span class="hljs-property">values</span>.<span class="hljs-title function_">tolist</span>())

clf = <span class="hljs-title class_">IsolationForest</span>(contamination=<span class="hljs-number">0.03</span>, random_state=<span class="hljs-number">42</span>)
topic_df[<span class="hljs-string">'is_anomaly'</span>] = clf.<span class="hljs-title function_">fit_predict</span>(topic_embeddings_array)

topic_df[topic_df.<span class="hljs-property">is_anomaly</span> == -<span class="hljs-number">1</span>][[<span class="hljs-string">'full_text'</span>]]
</code></pre>
<p>여기에서, 여행 주제에 대한 가장 흔하지 않은 댓글을 찾았습니다 (원본).</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">로마 구역의 곳곳에 있는 분수에서 물을 마셔도 안전한가요?

로마를 방문했을 때 오래된 지역을 거닐며 다양한 종류의 분수를 보았습니다. 물이 끊임없이 흘러나오는 분수들이 많았는데, 땅으로 흘러가는 분수도 있고, 대야에 모이는 분수도 있었습니다.

이런 분수에서 나오는 물을 마셔도 괜찮을까요? 방문객이 마실 수 있는 안전한 물일까요? 분수 사용에 대한 방문자들이 알아야 할 예절이 있을까요?
</code></pre>
<p>물에 관한 이야기이기 때문에 이 주석의 기능은 사람들이 물을 따르는 커피 주제와 밀접하게 관련되어 있습니다. 그래서 이 주석의 삽입 표현은 커피 클러스터와 꽤 가까운 것으로 보입니다.</p>
<p>t-SNE 시각화에서 찾아보면 실제로 커피 클러스터에 가까운 것을 확인할 수 있습니다.</p>
<p><img src="/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_22.png" alt="이미지"></p>
<div class="content-ad"></div>
<h2>RAG — 검색 증가 생성</h2>
<p>최근 LLM의 인기가 높아지면서, 임베딩이 RAG 사용 사례에서 널리 사용되고 있습니다.</p>
<p>우리는 많은 문서가 있는 경우(예: 스택 오버플로우의 모든 질문)에 검색 증가 생성이 필요합니다. 그리고 모든 정보를 항상 LLM에 전달할 수 없기 때문에</p>
<ul>
<li>LLM은 컨텍스트 크기에 제한이 있습니다(현재 GPT-4 Turbo의 경우 128K입니다).</li>
<li>우리는 토큰을 구매해야 하므로 모든 정보를 항상 전달하는 것이 더 비십니다.</li>
<li>LLM은 더 큰 컨텍스트에서 성능이 떨어집니다. 자세한 내용은 "바늘 찾기" - LLM의 압력 테스트를 확인할 수 있습니다.</li>
</ul>
<div class="content-ad"></div>
<p>대규모 지식 베이스와 함께 작업할 수 있도록 RAG 방법론을 활용할 수 있어요:</p>
<ul>
<li>모든 문서에 대한 임베딩을 계산하고 벡터 저장소에 저장합니다.</li>
<li>사용자 요청을 받으면 해당 요청의 임베딩을 계산하여 저장소에서 관련 문서를 검색할 수 있어요.</li>
<li>최종 답변을 얻기 위해 LLM에게 관련 문서만 전달하면 돼요.</li>
</ul>
<p>RAG에 대해 더 자세히 알고 싶다면 여기에 더 많은 내용을 담은 제 논문을 읽어보세요.</p>
<h1>요약</h1>
<div class="content-ad"></div>
<p>이 기사에서는 텍스트 임베딩에 대해 많은 세부 내용을 논의했습니다. 이제 여러분은 이 주제에 대해 완전하고 심도 있는 이해를 가졌을 것입니다. 저희 여정을 간단히 요약하면 다음과 같습니다:</p>
<ul>
<li>먼저, 텍스트 작업 방법의 진화를 살펴보았습니다.</li>
<li>그 다음으로, 텍스트 간에 유사한 의미를 가지고 있는지를 이해하는 방법에 대해 논의했습니다.</li>
<li>그 후에는 텍스트 임베딩 시각화의 다양한 접근 방법을 살펴보았습니다.</li>
<li>마지막으로, 임베딩을 클러스터링, 분류, 이상 탐지 및 RAG와 같은 다양한 실용적인 작업에서 특징으로 사용해 보았습니다.</li>
</ul>
<h1>참고</h1>
<p>이 기사에서는 크리에이티브 커먼즈 라이센스 하에 공개된 스택 엑스체인지 데이터 덤프에서 데이터 세트를 사용했습니다.</p>
<div class="content-ad"></div>
<p>이 글은 다음 강좌에서 영감을 받았습니다:</p>
<ul>
<li>DeepLearning.AI와 Google Cloud의 협력으로 진행되는 "Understanding and Applying Text Embeddings",</li>
<li>DeepLearning.AI와 Weaviate의 협력으로 진행되는 "Vector Databases: From Embeddings to Applications".</li>
</ul>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"텍스트 임베딩 종합 가이드 2024 최신","description":"","date":"2024-06-23 19:53","slug":"2024-06-23-TextEmbeddingsComprehensiveGuide","content":"\n\n## 텍스트 임베딩의 진화, 시각화, 그리고 응용\n\n우리 인간은 텍스트를 읽고 이해할 수 있습니다 (적어도 일부분은요). 컴퓨터는 반대로 \"숫자로 생각\"하기 때문에 단어와 문장의 의미를 자동으로 파악할 수 없습니다. 만약 우리가 컴퓨터가 자연어를 이해하도록 하려면, 이 정보를 컴퓨터가 작업할 수 있는 형식인 숫자 벡터로 변환해야 합니다.\n\n수십 년 전에 사람들은 텍스트를 기계가 이해할 수 있는 형식으로 변환하는 방법을 배웠습니다 (그 중 하나는 ASCII였습니다). 이러한 방식은 텍스트를 렌더링하고 전송하는 데 도움이 되지만 단어의 의미를 부호화하지는 않습니다. 당시에는 키워드 검색 기술이 표준 검색 기술이었으며 특정 단어나 N-gram을 포함하는 모든 문서를 찾는 방식이었습니다.\n\n그 후 몇 10년이 지난 후, 임베딩이 등장했습니다. 우리는 단어, 문장, 심지어 이미지에 대한 임베딩을 계산할 수 있습니다. 임베딩도 숫자의 벡터입니다만, 의미를 포착할 수 있습니다. 그래서 의미 검색을 수행하거나 다양한 언어로 된 문서를 다루는 데 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글에서는 임베딩 주제를 깊이 있게 다루어보고자 합니다:\n\n- 임베딩이 만들어지기 전의 역사와 진화에 대해,\n- OpenAI 도구를 사용하여 임베딩을 계산하는 방법,\n- 문장이 서로 가까운지 판단하는 방법,\n- 임베딩을 시각화하는 방법,\n- 가장 흥미로운 부분은 임베딩을 실제로 활용하는 방법입니다.\n\n이어서 나아가서 임베딩의 진화에 대해 배워보겠습니다.\n\n# 임베딩의 진화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 텍스트 표현의 역사로 간단한 여행을 시작할 것입니다.\n\n## 단어 가방\n\n텍스트를 벡터로 변환하는 가장 기본적인 방법은 단어 가방입니다. 리처드 P. 페이만의 유명한 명언 중 하나를 살펴보겠습니다. \"우리는 아직 발견들을 만들어내는 시대에 살고 있다\". 이를 통해 단어 가방 접근법을 설명해보겠습니다.\n\n단어 가방 벡터를 얻는 첫 번째 단계는 텍스트를 단어(토큰)로 나눈 다음, 단어를 기본 형태로 줄이는 것입니다. 예를 들어, \"running\"은 \"run\"으로 변환됩니다. 이 과정을 어간 추출(stemming)이라고 합니다. NLTK Python 패키지를 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\n\ntext = 'We are lucky to live in an age in which we are still making discoveries'\n\n# 토큰화 - 텍스트를 단어로 나누기\nwords = word_tokenize(text)\nprint(words)\n# ['We', 'are', 'lucky', 'to', 'live', 'in', 'an', 'age', 'in', 'which',\n#  'we', 'are', 'still', 'making', 'discoveries']\n\nstemmer = SnowballStemmer(language=\"english\")\nstemmed_words = list(map(lambda x: stemmer.stem(x), words))\nprint(stemmed_words)\n# ['we', 'are', 'lucki', 'to', 'live', 'in', 'an', 'age', 'in', 'which',\n#  'we', 'are', 'still', 'make', 'discoveri']\r\n```\n\n자, 이제 우리 단어들의 기본 형태 리스트가 있습니다. 다음 단계는 이들 빈도를 계산하여 벡터를 만드는 것입니다.\n\n```js\r\nimport collections\nbag_of_words = collections.Counter(stemmed_words)\nprint(bag_of_words)\n# {'we': 2, 'are': 2, 'in': 2, 'lucki': 1, 'to': 1, 'live': 1, \n# 'an': 1, 'age': 1, 'which': 1, 'still': 1, 'make': 1, 'discoveri': 1}\r\n```\n\n사실, 만약 텍스트를 벡터로 변환하고 싶다면, 텍스트에 있는 단어뿐만 아니라 전체 어휘를 고려해야 합니다. \"i\", \"you\", \"study\"도 어휘에 있다고 가정하고, 파인만의 명언에서 벡터를 만들어 봅시다.\r\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_0.png)\n\n이 방법은 꽤 기본적이며 단어의 의미를 고려하지 않기 때문에 \"그 여자는 데이터 과학을 공부하고 있다\"와 \"젊은 여성이 AI와 ML을 배우고 있다.\"라는 문장이 서로 가까운 위치에 있지 않을 수 있습니다.\n\n## TF-IDF\n\n단어 가방 접근법의 약간 개선된 버전인 TF-IDF(Term Frequency — Inverse Document Frequency)입니다. 이것은 두 가지 지표의 곱셈입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Markdown Table](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_1.png)\n\n- 용어 빈도는 문서에서 단어의 빈도를 보여줍니다. 이를 계산하는 가장 흔한 방법은 이 문서에서 용어의 로우 카운트(단어 가방에 있는 것처럼)을 전체 용어(단어) 수로 나누는 것입니다. 그러나 로우 카운트, 부욜리언 \"빈도\", 정규화에 대한 다양한 접근 방법이 많이 있습니다. 위키피디아에서 다양한 접근 방법에 대해 더 배울 수 있습니다.\n\n![Markdown Table](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_2.png)\n\n- 역문서 주파수는 단어가 얼마나 많은 정보를 제공하는지를 나타냅니다. 예를 들어, \"a\"나 \"that\" 같은 단어는 문서 주제에 대해 추가 정보를 제공하지 않습니다. 대조적으로, \"ChatGPT\"나 \"생물정보학\" 같은 단어는 도메인을 정의하는 데 도움이 될 수 있습니다 (하지만 이 문장에는 해당하지 않음). 이는 전체 문서 수와 해당 단어를 포함하는 문서 수의 비율의 로그함수로 계산됩니다. IDF가 0에 가까울수록 단어가 흔하고 제공하는 정보가 더 적습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_3.png\" /\u003e\n\n그래서 결과적으로 우리는 일반적인 단어 (\"I\"나 \"you\"와 같은)이 낮은 가중치를 갖는 벡터를 얻게됩니다. 한편, 문서에서 여러 번 발생하는 드문 단어들은 더 높은 가중치를 갖게 됩니다. 이 전략은 약간 더 나은 결과를 제공하지만 여전히 의미적 의미를 잡아내기는 어렵습니다.\n\n이 방법론의 다른 어려움은 상당히 희소한 벡터를 생성한다는 점입니다. 벡터의 길이는 말뭉치 크기와 동일합니다. 영어에는 약 470,000개의 고유 단어가 있습니다(출처). 그러므로 우리는 거대한 벡터를 갖게 될 것입니다. 하지만 문장에는 50개 이상의 고유 단어가 나타나지 않을 것이므로 벡터의 값 중 99.99%는 0일 것입니다. 이는 어떤 정보도 인코딩하지 않습니다. 이에 대해 과학자들은 밀도 있는 벡터 표현에 대해 고민하기 시작했습니다.\n\n## Word2Vec\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 유명한 밀집 표현 방법 중 하나는 구글이 2013년에 Mikolov 등이 제안한 \"효율적인 단어 표현 추정을 위한 Word2Vec\" 논문에서 소개한 word2vec입니다.\n\n논문에서 언급된 두 가지 word2vec 접근 방식은 Continuous Bag of Words(주변 단어를 기반으로 단어를 예측하는 방법)와 Skip-gram(반대 작업인 단어를 기반으로 문맥을 예측하는 방법)입니다.\n\n밀집 벡터 표현의 핵심 아이디어는 두 모델을 훈련하는 것입니다: 인코더와 디코더. 예를 들어, Skip-gram의 경우 \"christmas\"라는 단어를 인코더에 전달할 수 있습니다. 그런 다음, 인코더가 \"merry\", \"to\", \"you\"와 같은 단어를 얻을 것으로 예상하여 디코더에 전달할 수 있는 벡터를 생성할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_5.png)\n\n이 모델은 이제 단어의 의미를 고려하기 시작했습니다. 단어의 맥락에서 훈련되었기 때문입니다. 그러나 형태학(예: \"-less\"는 무언가의 부족을 의미함)을 무시합니다. 나중에는 GloVe에서 서브워드 스킵-그램을 살펴봄으로써 이 단점을 개선했습니다.\n\n또한, word2vec은 단어와만 작동할 수 있었지만, 우리는 전체 문장을 인코딩하고 싶습니다. 그러니, 트랜스포머로 다음 진화 단계로 넘어가 봅시다.\n\n## 트랜스포머와 문장 임베딩\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 진화는 Vaswani 등이 발표한 \"Attention Is All You Need\" 논문에서 소개된 트랜스포머 접근 방식과 관련이 있었습니다. 트랜스포머는 정보가 풍부한 밀집 벡터를 생성할 수 있었고 현대 언어 모델의 주요 기술로 자리 잡게 되었습니다.\n\n저는 트랜스포머의 구조 세부 사항에 대해 다루지 않겠습니다. 왜냐하면 이것은 우리 주제와 관련이 그리 크지 않고 많은 시간이 소요되기 때문입니다. 더 배우고 싶다면 \"Transformers, Explained\" 또는 \"The Illustrated Transformer\"와 같은 다양한 자료가 많이 있습니다.\n\n트랜스포머를 사용하면 동일한 \"핵심\" 모델을 사용하여 다른 사용 사례에 대해 미세 조정할 수 있으며, 핵심 모델을 다시 학습시킬 필요가 없습니다(시간이 많이 소요되고 상당한 비용이 듭니다). 이것은 사전 훈련된 모델의 등장으로 이어졌습니다. 가장 인기 있는 최초의 모델 중 하나는 Google AI가 개발한 BERT(Bidirectional Encoder Representations from Transformers)였습니다.\n\n내부적으로 BERT는 여전히 word2vec과 유사한 토큰 수준에서 작동하지만, 우리는 여전히 문장 임베딩을 얻고 싶습니다. 따라서, 모든 토큰 벡터의 평균을 취하는 단순한 방법을 적용할 수 있습니다. 유감스럽게도, 이 방법은 좋은 성능을 보여주지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2019년에 이 문제는 Sentence-BERT가 출시되면서 해결되었습니다. 이는 의미론적 텍스트 유사성 작업에서 이전 방법들을 모두 능가하며 문장 포함 벡터의 계산을 가능하게 했습니다.\n\n이 주제는 매우 방대하기 때문에 이 기사에서 모두 다 다룰 수는 없을 거예요. 그러니 진지하게 관심이 있다면 이 기사에서 문장 포함 벡터에 대해 더 배울 수 있습니다.\n\n우리는 임베딩의 발전을 간략히 다뤘고 이론에 대한 고수준 이해를 얻었습니다. 이제 실습으로 넘어가서 OpenAI 도구를 사용하여 어떻게 임베딩을 계산하는지 배워보겠습니다.\n\n# 임베딩 계산\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 OpenAI 임베딩을 사용할 것입니다. 최근에 출시된 새로운 모델인 text-embedding-3-small을 시도해볼 것입니다. 이 새로운 모델은 text-embedding-ada-002보다 성능이 더 좋게 나타났습니다:\n\n- 널리 사용되는 다국어 검색 (MIRACL) 벤치마크의 평균 점수가 31.4%에서 44.0%로 상승했습니다.\n- 영어 작업에 대한 자주 사용되는 벤치마크인 MTEB의 평균 성능도 향상되어 61.0%에서 62.3%로 상승했습니다.\n\nOpenAI는 또한 새로운 큰 모델인 text-embedding-3-large를 출시했습니다. 이제 이것이 가장 우수한 임베딩 모델입니다.\n\n데이터 소스로는 Stack Exchange Data Dump의 작은 샘플을 사용할 것입니다. 이는 Stack Exchange 네트워크에서 모든 사용자 기여 콘텐츠의 익명화된 덤프입니다. 저는 흥미로운 주제를 선택하고 각각에서 100개의 질문을 샘플링했습니다. 주제는 생성적 AI부터 커피 또는 자전거까지 다양합니다. 그래서 다양한 주제를 볼 수 있을 겁니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저, 모든 스택 오버플로우 질문에 대한 임베딩을 계산해야 합니다. 한 번 실행하고 결과를 로컬로 저장하는 것이 좋습니다(파일이나 벡터 저장소에). OpenAI Python 패키지를 사용하여 임베딩을 생성할 수 있습니다.\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\ndef get_embedding(text, model=\"text-embedding-3-small\"):\n   text = text.replace(\"\\n\", \" \")\n   return client.embeddings.create(input = [text], model=model)\\\n       .data[0].embedding\n\nget_embedding(\"We are lucky to live in an age in which we are still making discoveries.\")\n```\n\n결과적으로, 우리는 부동 소수점 숫자로 이루어진 1536차원 벡터를 얻습니다. 이제 이를 모든 데이터에 대해 반복하고 값을 분석할 수 있습니다.\n\n가장 궁금할 수 있는 주요 질문은 의미적으로 문장들이 얼마나 가까운지입니다. 답을 발견하기 위해 벡터 간의 거리 개념을 논의해 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 벡터 간 거리\n\n임베딩은 사실 벡터입니다. 따라서 두 문장이 얼마나 가까운지 이해하려면 벡터 간 거리를 계산할 수 있습니다. 더 작은 거리는 더 가까운 의미를 나타낼 것입니다.\n\n두 벡터 간의 거리를 측정하는 데 사용할 수 있는 다양한 메트릭이 있습니다:\n\n- 유클리디안 거리 (L2),\n- 맨하탄 거리 (L1),\n- 내적 (Dot product),\n- 코사인 거리.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그들에 대해 이야기해 봅시다. 간단한 예로, 우리는 두 개의 2D 벡터를 사용할 것입니다.\n\n```js\nvector1 = [1, 4]\nvector2 = [2, 2]\n```\n\n## 유클리디안 거리 (L2)\n\n두 지점(또는 벡터) 사이의 거리를 정의하는 가장 표준적인 방법은 유클리디안 거리 또는 L2 norm입니다. 이 측정 기준은 일상생활에서 가장 많이 사용되며, 예를 들어 2개의 도시 사이의 거리를 언급할 때 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nL2 거리에 대한 시각적 표현과 공식이 있습니다.\n\n![Image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_6.png)\n\n파이썬 또는 numpy 함수를 사용하여 이 메트릭을 계산할 수 있습니다.\n\n```python\nimport numpy as np\n\nsum(list(map(lambda x, y: (x - y) ** 2, vector1, vector2))) ** 0.5\n# 2.2361\n\nnp.linalg.norm((np.array(vector1) - np.array(vector2)), ord = 2)\n# 2.2361\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 맨해튼 거리 (L1)\n\n다른 일반적으로 사용되는 거리 측정 방법은 L1 노름 또는 맨해튼 거리입니다. 이 거리는 뉴욕의 맨해튼 섬에서 명명되었습니다. 이 섬은 거리가 격자 레이아웃으로 되어 있고, 맨해튼에서 두 지점 사이의 가장 짧은 경로는 격자 모양을 따라야 하므로 L1 거리가 됩니다.\n\n![image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_7.png)\n\n우리는 이를 처음부터 구현하거나 numpy 함수를 사용하여 구현할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\nsum(list(map(lambda x, y: abs(x - y), vector1, vector2)))\r\n# 3\r\n\r\nnp.linalg.norm((np.array(vector1) - np.array(vector2)), ord = 1)\r\n# 3.0\r\n```\r\n\r\n## 내적(Dot product)\r\n\r\n벡터 간 거리를 계산하는 다른 방법은 내적 또는 스칼라 곱을 계산하는 것입니다. 다음은 해당 공식이며 쉽게 구현할 수 있습니다.\r\n\r\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_8.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nsum(list(map(lambda x, y: x*y, vector1, vector2)))\n# 11\n\nnp.dot(vector1, vector2)\n# 11\n```\n\n이 메트릭은 해석하기가 조금 까다로운 편이에요. 한편으로는 벡터가 한 방향을 향하고 있는지를 보여줍니다. 다른 한편으로는 결과는 벡터들의 크기에 크게 의존합니다. 예를 들어 두 쌍의 벡터 간의 내적을 계산해볼게요:\n\n- (1, 1) vs (1, 1)\n- (1, 1) vs (10, 10).\n\n두 경우 모두 벡터가 일직선상에 있지만, 두 번째 경우에 내적은 10배 크게 나와요: 2 대 20.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 코사인 유사도\n\n많은 경우, 코사인 유사도가 사용됩니다. 코사인 유사도는 벡터의 크기(또는 노름)에 의해 정규화된 내적입니다.\n\n![Image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_9.png)\n\n이전처럼 직접 모든 것을 계산하거나 sklearn의 함수를 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndot_product = sum(list(map(lambda x, y: x*y, vector1, vector2)))\nnorm_vector1 = sum(list(map(lambda x: x ** 2, vector1))) ** 0.5\nnorm_vector2 = sum(list(map(lambda x: x ** 2, vector2))) ** 0.5\n\ndot_product/norm_vector1/norm_vector2\n\n# 0.8575\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncosine_similarity(\n  np.array(vector1).reshape(1, -1), \n  np.array(vector2).reshape(1, -1))[0][0]\n\n# 0.8575\n```\n\ncosine_similarity 함수는 2차원 배열을 기대합니다. 그래서 numpy 배열을 reshape 해주어야 합니다.\n\n이 메트릭의 물리적 의미에 대해 조금 이야기해 봅시다. Cosine similarity는 두 벡터 사이의 코사인 값과 같습니다. 벡터가 서로 가까울수록 메트릭 값이 높아집니다.\n\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_10.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 심지어 벡터 사이의 정확한 각도를 도 단위로 계산할 수도 있어요. 약 30도 정도의 결과를 얻었고, 꽤 합리적으로 보이네요.\n\n```js\nimport math\nmath.degrees(math.acos(0.8575))\n\n# 30.96\n```\n\n## 어떤 측정 지표를 사용할까요?\n\n우리는 두 벡터 사이의 거리를 계산하는 다양한 방법에 대해 토론해 왔고, 여러분은 어떤 방법을 사용할지 고려하기 시작할 수 있을 거예요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내가 가진 임베딩을 비교하기 위해 어떤 거리든 사용할 수 있어요. 예를 들어, 다른 클러스터 사이의 평균 거리를 계산했어요. L2 거리와 코사인 유사도 모두 비슷한 결과를 보여줘요:\n\n- 클러스터 내의 객체들은 다른 클러스터보다 서로 더 가까워요. L2 거리에 대해 가까울수록 낮은 거리를 의미하지만 코사인 유사도에서는 가까운 객체일수록 값이 높아져요. 헷갈리지 마세요.\n- \"정치\"와 \"경제\" 또는 \"ai\"와 \"데이터과학\"과 같이 일부 주제들이 서로 아주 가까운 것을 알 수 있어요.\n\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_11.png\" /\u003e\n\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_12.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 NLP 작업에 대해서는 일반적으로 코사인 유사도를 사용하는 것이 최선의 방법입니다. 몇 가지 그 이유는:\n\n- 코사인 유사도는 -1과 1 사이에 있으며, L1과 L2는 무제한이기 때문에 해석하기 쉽습니다.\n- 실용적인 측면에서 유클리드 거리의 제곱근보다 내적을 계산하는 것이 더 효과적입니다.\n- 코사인 유사도는 차원의 저주에 영향을 덜 받습니다 (이에 대해 뒤에서 더 얘기할 것입니다).\n\n위의 결과에서 인트라 및 인터 클러스터 거리 간의 차이가 크지 않다는 점을 알 수 있을 것입니다. 이 현상의 원인은 벡터의 고차원성 때문입니다. 이 효과는 \"차원의 저주\"라고 불리며, 차원이 높을수록 벡터 간 거리 분포가 좁아진다는 것을 알 수 있습니다. 이에 대해 더 자세히 알아보려면 이 글을 참조해보세요.\n\n간단히 설명드리겠습니다. OpenAI 임베딩 값의 분포를 계산하고 차원이 다른 300개의 벡터 집합을 생성했습니다. 그런 다음, 모든 벡터 사이의 거리를 계산하고 히스토그램을 그렸습니다. 차원이 증가함에 따라 벡터의 거리 분포가 좁아진다는 것을 쉽게 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_13.png)\n\n임베딩 사이의 유사성을 측정하는 방법을 배웠어요. 여기서 이론적인 부분은 마쳤고, 더 실용적인 부분(시각화 및 실용적인 응용)으로 넘어가겠습니다. 데이터를 보는 것이 가장 중요하니, 시각화부터 시작해봐요.\n\n# 임베딩 시각화\n\n데이터를 이해하는 가장 좋은 방법은 시각적으로 나타내는 것이에요. 아쉽지만, 임베딩은 1536차원이 있어서 데이터를 살펴보기가 꽤 어려워요. 그러나, 한 가지 방법이 있어요: 차원 축소 기술을 사용하여 벡터를 이차원 공간에 투영하는 것이에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## PCA\n\n가장 기본적인 차원 축소 기술은 PCA(주성분 분석)입니다. 이를 사용해 봅시다.\n\n먼저, sklearn에 전달하기 위해 임베딩을 2D numpy 배열로 변환해야 합니다.\n\n```python\nimport numpy as np\nembeddings_array = np.array(df.embedding.values.tolist())\nprint(embeddings_array.shape)\n# (1400, 1536)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼, 우리는 PCA 모델을 n_components = 2로 초기화해야 해요 (2D 시각화를 생성하고 싶기 때문에), 전체 데이터에서 모델을 학습하고 새로운 값을 예측해야 해요.\n\n```js\nfrom sklearn.decomposition import PCA\n\npca_model = PCA(n_components = 2)\npca_model.fit(embeddings_array)\n\npca_embeddings_values = pca_model.transform(embeddings_array)\nprint(pca_embeddings_values.shape)\n# (1400, 2)\n```\n\n결과적으로, 우리는 각 질문에 대해 두 개의 특성을 가진 행렬을 얻었으므로, scatter plot에서 쉽게 시각화할 수 있어요.\n\n```js\nfig = px.scatter(\n    x = pca_embeddings_values[:,0], \n    y = pca_embeddings_values[:,1],\n    color = df.topic.values,\n    hover_name = df.full_text.values,\n    title = 'PCA embeddings', width = 800, height = 600,\n    color_discrete_sequence = plotly.colors.qualitative.Alphabet_r\n)\n\nfig.update_layout(\n    xaxis_title = 'first component', \n    yaxis_title = 'second component')\nfig.show()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![img](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_14.png)\n\n각 주제의 질문들이 서로 꽤 가까이 위치해 있는 것을 볼 수 있어 좋습니다. 그러나 모든 클러스터가 혼재되어 있어서 개선할 부분이 있습니다.\n\n## t-SNE\n\nPCA는 선형 알고리즘이지만, 대부분의 관계는 실제로는 비선형입니다. 그래서 비선형성 때문에 클러스터를 분리할 수 없을 수도 있습니다. 비선형 알고리즘인 t-SNE을 사용해보고 더 나은 결과를 보여줄 수 있는지 확인해봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n거의 동일한 코드를 사용했습니다. PCA 대신 t-SNE 모델을 사용했어요.\n\n```js\nfrom sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, random_state=42)\ntsne_embeddings_values = tsne_model.fit_transform(embeddings_array)\n\nfig = px.scatter(\n    x = tsne_embeddings_values[:,0], \n    y = tsne_embeddings_values[:,1],\n    color = df.topic.values,\n    hover_name = df.full_text.values,\n    title = 't-SNE embeddings', width = 800, height = 600,\n    color_discrete_sequence = plotly.colors.qualitative.Alphabet_r\n)\n\nfig.update_layout(\n    xaxis_title = 'first component', \n    yaxis_title = 'second component')\nfig.show()\n```\n\nt-SNE 결과가 훨씬 좋아 보여요. 대부분의 클러스터가 분리되어 있지만 \"genai\", \"datascience\", \"ai\" 는 분리되지 않았어요. 그러나 이건 예상한대로에요 - 이러한 주제를 내가 분리할 수 있을지 의심스러워요.\n\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_15.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 시각화를 보면 임베딩이 의미적 의미를 인코딩하는 데 상당히 효과적임을 확인할 수 있어요.\n\n또한, 데이터를 3D로 시각화할 수 있는 사영(projection)을 만들어볼 수 있어요. 실용적일지는 확실하지 않지만, 데이터를 3D로 살펴보는 것은 흥미롭고 관심을 끌 수 있어요.\n\n```js\ntsne_model_3d = TSNE(n_components=3, random_state=42)\ntsne_3d_embeddings_values = tsne_model_3d.fit_transform(embeddings_array)\n\nfig = px.scatter_3d(\n    x = tsne_3d_embeddings_values[:,0], \n    y = tsne_3d_embeddings_values[:,1],\n    z = tsne_3d_embeddings_values[:,2],\n    color = df.topic.values,\n    hover_name = df.full_text.values,\n    title = 't-SNE embeddings', width = 800, height = 600,\n    color_discrete_sequence = plotly.colors.qualitative.Alphabet_r,\n    opacity = 0.7\n)\nfig.update_layout(xaxis_title = 'first component', yaxis_title = 'second component')\nfig.show()\n```\n\n![3D 시각화](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_16.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 바코드\n\n임베딩을 이해하는 방법은 몇 개를 바코드처럼 시각화하여 상관 관계를 확인하는 것입니다. 나는 세 가지 임베딩 예시를 선택했습니다: 두 개는 서로에게 가장 가깝고, 나머지 하나는 데이터 세트에서 가장 멀리 떨어져 있는 예시입니다.\n\n```js\nembedding1 = df.loc[1].embedding\nembedding2 = df.loc[616].embedding\nembedding3 = df.loc[749].embedding\n```\n\n```js\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nembed_len_thr = 1536\n\nsns.heatmap(np.array(embedding1[:embed_len_thr]).reshape(-1, embed_len_thr),\n    cmap = \"Greys\", center = 0, square = False, \n    xticklabels = False, cbar = False)\nplt.gcf().set_size_inches(15,1)\nplt.yticks([0.5], labels = ['AI'])\nplt.show()\n\nsns.heatmap(np.array(embedding3[:embed_len_thr]).reshape(-1, embed_len_thr),\n    cmap = \"Greys\", center = 0, square = False, \n    xticklabels = False, cbar = False)\nplt.gcf().set_size_inches(15,1)\nplt.yticks([0.5], labels = ['AI'])\nplt.show()\n\nsns.heatmap(np.array(embedding2[:embed_len_thr]).reshape(-1, embed_len_thr),\n    cmap = \"Greys\", center = 0, square = False, \n    xticklabels = False, cbar = False)\nplt.gcf().set_size_inches(15,1)\nplt.yticks([0.5], labels = ['바이오인포매틱스'])\nplt.show()\n```  \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_17.png)\n\n우리 경우에는 고차원 때문에 벡터가 서로 가까운지 쉽게 보기 어려울 수 있습니다. 그래도 나는 이 시각화를 좋아합니다. 몇 가지 경우에 도움이 될 수도 있으니, 나는 이 아이디어를 당신과 공유하고자 합니다.\n\n우리는 임베딩을 시각화하는 방법을 배웠고, 텍스트의 의미를 파악하는 능력에 대한 의문은 남지 않았습니다. 이제 실제로 임베딩을 어떻게 활용할 수 있는지에 대해 논의하는 가장 흥미로운 부분으로 넘어가 보겠습니다.\n\n# 실용적인 응용\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n물론, 임베딩의 주요 목표는 텍스트를 숫자의 벡터로 인코딩하거나 시각화하기 위해서만 하는 것이 아닙니다. 우리는 텍스트의 의미를 포착하는 능력에서 많은 이점을 얻을 수 있습니다. 실용적인 예제들을 함께 살펴보겠습니다.\n\n## 클러스터링\n\n먼저 클러스터링부터 시작해보죠. 클러스터링은 초기 레이블 없이 데이터를 그룹으로 분할할 수 있는 비지도학습 기술입니다. 클러스터링을 통해 데이터의 내부 구조적 패턴을 이해하는 데 도움을 받을 수 있습니다.\n\n가장 기본적인 클러스터링 알고리즘 중 하나인 K-평균을 사용할 것입니다. K-평균 알고리즘을 위해서는 클러스터의 개수를 지정해야 합니다. 실루엣 스코어를 사용하여 최적의 클러스터 수를 정의할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2부터 50까지의 k (클러스터 수)를 시도해 보겠습니다. 각 k에 대해 모델을 훈련하고 실루엣 점수를 계산할 것입니다. 실루엣 점수가 높을수록, 더 좋은 클러스터링 결과를 얻을 수 있습니다.\n\n```python\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport tqdm\n\nsilhouette_scores = []\nfor k in tqdm.tqdm(range(2, 51)):\n    kmeans = KMeans(n_clusters=k, \n                    random_state=42, \n                    n_init='auto').fit(embeddings_array)\n    kmeans_labels = kmeans.labels_\n    silhouette_scores.append(\n        {\n            'k': k,\n            'silhouette_score': silhouette_score(embeddings_array, \n                                                 kmeans_labels, metric='cosine')\n        }\n    )\n\nfig = px.line(pd.DataFrame(silhouette_scores).set_index('k'),\n              title='\u003cb\u003eK-means 클러스터링을 위한 실루엣 점수\u003c/b\u003e',\n              labels={'value': '실루엣 점수'}, \n              color_discrete_sequence=plotly.colors.qualitative.Alphabet)\nfig.update_layout(showlegend=False)\n```\n\n우리의 경우, k가 11일 때 실루엣 점수가 최대치에 도달합니다. 따라서 최종 모델에는 이 클러스터 수를 사용합시다.\n\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_18.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n클러스터를 시각화해 보는 t-SNE를 이용한 차원 축소를 이미 이전에 수행한 것처럼 해보겠습니다.\n\n```js\ntsne_model = TSNE(n_components=2, random_state=42)\ntsne_embeddings_values = tsne_model.fit_transform(embeddings_array)\n\nfig = px.scatter(\n    x = tsne_embeddings_values[:,0], \n    y = tsne_embeddings_values[:,1],\n    color = list(map(lambda x: '클러스터 %s' % x, kmeans_labels)),\n    hover_name = df.full_text.values,\n    title = '클러스터링을 위한 t-SNE 임베딩', width = 800, height = 600,\n    color_discrete_sequence = plotly.colors.qualitative.Alphabet_r\n)\nfig.update_layout(\n    xaxis_title = '첫 번째 성분', \n    yaxis_title = '두 번째 성분')\nfig.show()\n```\n\n시각적으로 알고리즘이 클러스터를 상당히 잘 정의했음을 확인할 수 있습니다 — 그들은 꽤 잘 분리되어 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_19.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 사실적인 주제 라벨을 가지고 있으므로, 클러스터링이 얼마나 좋은지를 심층적으로 판단할 수도 있어요. 각 클러스터에 대한 주제 혼합을 살펴봅시다.\n\n```js\ndf['cluster'] = list(map(lambda x: '클러스터 %s' % x, kmeans_labels))\ncluster_stats_df = df.reset_index().pivot_table(\n    index='cluster', values='id',\n    aggfunc='count', columns='topic').fillna(0).applymap(int)\n\ncluster_stats_df = cluster_stats_df.apply(\n  lambda x: 100*x/cluster_stats_df.sum(axis=1))\n\nfig = px.imshow(\n    cluster_stats_df.values, \n    x=cluster_stats_df.columns,\n    y=cluster_stats_df.index,\n    text_auto='.2f', aspect=\"auto\",\n    labels=dict(x=\"클러스터\", y=\"팩트 주제\", color=\"비율, %\"),\n    color_continuous_scale='pubugn',\n    title='\u003cb\u003e각 클러스터의 주제 비율\u003c/b\u003e', height=550)\n\nfig.show()\n```\n\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_20.png\" /\u003e\n\n대부분의 경우, 클러스터링은 완벽하게 작동했어요. 예를 들어, 클러스터 5에는 거의 자전거에 관한 질문만 있고, 클러스터 6은 커피에 관한 것이에요. 그러나 유사한 주제를 구별하지 못했어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- \"ai,\" \"genai,\" and \"datascience\"은 동일한 클러스터에 있습니다.\n- \"economics\"와 \"politics\"은 같은 그룹에 속합니다.\n\n이 예제에서는 피처로써 임베딩만 사용했지만, 질문을 한 사용자의 나이, 성별 또는 국가와 같은 추가 정보가 있다면 모델에 포함시킬 수도 있습니다.\n\n## 분류\n\n임베딩을 분류 또는 회귀 작업에 사용할 수 있습니다. 예를 들어 고객 리뷰 감정을 예측하는 (분류)이나 NPS 점수를 예측하는 (회귀) 등 다양한 작업에 활용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n분류 및 회귀는 지도 학습이므로 레이블이 필요합니다. 다행히도, 우리는 질문의 주제를 알고 있으므로 모델을 적합시켜 예측할 수 있습니다.\n\n저는 랜덤 포레스트 분류기를 사용할 것입니다. 랜덤 포레스트에 대해 간단히 상기하고 싶다면 여기에서 확인할 수 있어요. 분류 모델의 성능을 올바르게 평가하려면 데이터 세트를 학습 및 테스트 세트(80% 대 20%)로 분할할 것입니다. 그런 다음 학습 데이터 세트에서 모델을 훈련하고 테스트 데이터 세트에서 품질을 측정할 수 있습니다(모델이 이전에 보지 못한 질문).\n\n```js\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nclass_model = RandomForestClassifier(max_depth = 10)\n\n# 특징 및 대상 정의\nX = embeddings_array\ny = df.topic\n\n# 데이터를 학습 및 테스트 세트로 분할\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state = 42, test_size=0.2, stratify=y\n)\n\n# 적합 및 예측\nclass_model.fit(X_train, y_train)\ny_pred = class_model.predict(X_test)\n```\n\n모델의 성능을 추정하기 위해 혼동 행렬을 계산해 보겠습니다. 이상적인 상황에서는 비대각 요소가 모두 0이어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nfig = px.imshow(\n  cm, x = class_model.classes_,\n  y = class_model.classes_, text_auto='d', \n  aspect=\"auto\", \n  labels=dict(\n      x=\"predicted label\", y=\"true label\", \n      color=\"cases\"), \n  color_continuous_scale='pubugn',\n  title = '\u003cb\u003e혼동 행렬\u003c/b\u003e', height = 550)\n\nfig.show()\n```\n\n![이미지](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_21.png)\n\n군집화와 유사한 결과를 확인할 수 있습니다. 일부 주제는 쉽게 분류되고 정확도가 100%인 반면, 다른 주제들은 구별하기 어려운 경우도 있습니다(특히 \"ai\" 주제).\n\n하지만 전체적으로 91.8%의 정확도를 달성했으며, 이는 꽤 좋은 성과입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 이상 징후 찾기\n\n데이터에서 이상 징후를 찾기 위해 임베딩을 사용할 수도 있습니다. 예를 들어, t-SNE 그래프에서 \"여행\" 주제에 대한 몇 가지 질문이 군집에서 꽤 멀리 떨어져 있는 것을 볼 수 있었습니다. 이 테마를 살펴보고 이상 징후를 찾아보겠습니다. 이를 위해 이상 탐지 알고리즘인 Isolation Forest를 사용할 것입니다.\n\n```js\nfrom sklearn.ensemble import IsolationForest\n\ntopic_df = df[df.topic == 'travel']\ntopic_embeddings_array = np.array(topic_df.embedding.values.tolist())\n\nclf = IsolationForest(contamination=0.03, random_state=42)\ntopic_df['is_anomaly'] = clf.fit_predict(topic_embeddings_array)\n\ntopic_df[topic_df.is_anomaly == -1][['full_text']]\n```\n\n여기에서, 여행 주제에 대한 가장 흔하지 않은 댓글을 찾았습니다 (원본).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n로마 구역의 곳곳에 있는 분수에서 물을 마셔도 안전한가요?\n\n로마를 방문했을 때 오래된 지역을 거닐며 다양한 종류의 분수를 보았습니다. 물이 끊임없이 흘러나오는 분수들이 많았는데, 땅으로 흘러가는 분수도 있고, 대야에 모이는 분수도 있었습니다.\n\n이런 분수에서 나오는 물을 마셔도 괜찮을까요? 방문객이 마실 수 있는 안전한 물일까요? 분수 사용에 대한 방문자들이 알아야 할 예절이 있을까요?\n```\n\n물에 관한 이야기이기 때문에 이 주석의 기능은 사람들이 물을 따르는 커피 주제와 밀접하게 관련되어 있습니다. 그래서 이 주석의 삽입 표현은 커피 클러스터와 꽤 가까운 것으로 보입니다.\n\nt-SNE 시각화에서 찾아보면 실제로 커피 클러스터에 가까운 것을 확인할 수 있습니다.\n\n![이미지](/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_22.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## RAG — 검색 증가 생성\n\n최근 LLM의 인기가 높아지면서, 임베딩이 RAG 사용 사례에서 널리 사용되고 있습니다.\n\n우리는 많은 문서가 있는 경우(예: 스택 오버플로우의 모든 질문)에 검색 증가 생성이 필요합니다. 그리고 모든 정보를 항상 LLM에 전달할 수 없기 때문에\n\n- LLM은 컨텍스트 크기에 제한이 있습니다(현재 GPT-4 Turbo의 경우 128K입니다).\n- 우리는 토큰을 구매해야 하므로 모든 정보를 항상 전달하는 것이 더 비십니다.\n- LLM은 더 큰 컨텍스트에서 성능이 떨어집니다. 자세한 내용은 \"바늘 찾기\" - LLM의 압력 테스트를 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대규모 지식 베이스와 함께 작업할 수 있도록 RAG 방법론을 활용할 수 있어요:\n\n- 모든 문서에 대한 임베딩을 계산하고 벡터 저장소에 저장합니다.\n- 사용자 요청을 받으면 해당 요청의 임베딩을 계산하여 저장소에서 관련 문서를 검색할 수 있어요.\n- 최종 답변을 얻기 위해 LLM에게 관련 문서만 전달하면 돼요.\n\nRAG에 대해 더 자세히 알고 싶다면 여기에 더 많은 내용을 담은 제 논문을 읽어보세요.\n\n# 요약\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 텍스트 임베딩에 대해 많은 세부 내용을 논의했습니다. 이제 여러분은 이 주제에 대해 완전하고 심도 있는 이해를 가졌을 것입니다. 저희 여정을 간단히 요약하면 다음과 같습니다:\n\n- 먼저, 텍스트 작업 방법의 진화를 살펴보았습니다.\n- 그 다음으로, 텍스트 간에 유사한 의미를 가지고 있는지를 이해하는 방법에 대해 논의했습니다.\n- 그 후에는 텍스트 임베딩 시각화의 다양한 접근 방법을 살펴보았습니다.\n- 마지막으로, 임베딩을 클러스터링, 분류, 이상 탐지 및 RAG와 같은 다양한 실용적인 작업에서 특징으로 사용해 보았습니다.\n\n# 참고\n\n이 기사에서는 크리에이티브 커먼즈 라이센스 하에 공개된 스택 엑스체인지 데이터 덤프에서 데이터 세트를 사용했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글은 다음 강좌에서 영감을 받았습니다:\n\n- DeepLearning.AI와 Google Cloud의 협력으로 진행되는 \"Understanding and Applying Text Embeddings\",\n- DeepLearning.AI와 Weaviate의 협력으로 진행되는 \"Vector Databases: From Embeddings to Applications\".","ogImage":{"url":"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_0.png"},"coverImage":"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_0.png","tag":["Tech"],"readingTime":24},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e텍스트 임베딩의 진화, 시각화, 그리고 응용\u003c/h2\u003e\n\u003cp\u003e우리 인간은 텍스트를 읽고 이해할 수 있습니다 (적어도 일부분은요). 컴퓨터는 반대로 \"숫자로 생각\"하기 때문에 단어와 문장의 의미를 자동으로 파악할 수 없습니다. 만약 우리가 컴퓨터가 자연어를 이해하도록 하려면, 이 정보를 컴퓨터가 작업할 수 있는 형식인 숫자 벡터로 변환해야 합니다.\u003c/p\u003e\n\u003cp\u003e수십 년 전에 사람들은 텍스트를 기계가 이해할 수 있는 형식으로 변환하는 방법을 배웠습니다 (그 중 하나는 ASCII였습니다). 이러한 방식은 텍스트를 렌더링하고 전송하는 데 도움이 되지만 단어의 의미를 부호화하지는 않습니다. 당시에는 키워드 검색 기술이 표준 검색 기술이었으며 특정 단어나 N-gram을 포함하는 모든 문서를 찾는 방식이었습니다.\u003c/p\u003e\n\u003cp\u003e그 후 몇 10년이 지난 후, 임베딩이 등장했습니다. 우리는 단어, 문장, 심지어 이미지에 대한 임베딩을 계산할 수 있습니다. 임베딩도 숫자의 벡터입니다만, 의미를 포착할 수 있습니다. 그래서 의미 검색을 수행하거나 다양한 언어로 된 문서를 다루는 데 사용할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 글에서는 임베딩 주제를 깊이 있게 다루어보고자 합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e임베딩이 만들어지기 전의 역사와 진화에 대해,\u003c/li\u003e\n\u003cli\u003eOpenAI 도구를 사용하여 임베딩을 계산하는 방법,\u003c/li\u003e\n\u003cli\u003e문장이 서로 가까운지 판단하는 방법,\u003c/li\u003e\n\u003cli\u003e임베딩을 시각화하는 방법,\u003c/li\u003e\n\u003cli\u003e가장 흥미로운 부분은 임베딩을 실제로 활용하는 방법입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이어서 나아가서 임베딩의 진화에 대해 배워보겠습니다.\u003c/p\u003e\n\u003ch1\u003e임베딩의 진화\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e우리는 텍스트 표현의 역사로 간단한 여행을 시작할 것입니다.\u003c/p\u003e\n\u003ch2\u003e단어 가방\u003c/h2\u003e\n\u003cp\u003e텍스트를 벡터로 변환하는 가장 기본적인 방법은 단어 가방입니다. 리처드 P. 페이만의 유명한 명언 중 하나를 살펴보겠습니다. \"우리는 아직 발견들을 만들어내는 시대에 살고 있다\". 이를 통해 단어 가방 접근법을 설명해보겠습니다.\u003c/p\u003e\n\u003cp\u003e단어 가방 벡터를 얻는 첫 번째 단계는 텍스트를 단어(토큰)로 나눈 다음, 단어를 기본 형태로 줄이는 것입니다. 예를 들어, \"running\"은 \"run\"으로 변환됩니다. 이 과정을 어간 추출(stemming)이라고 합니다. NLTK Python 패키지를 사용할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e nltk.\u003cspan class=\"hljs-property\"\u003estem\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSnowballStemmer\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e nltk.\u003cspan class=\"hljs-property\"\u003etokenize\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e word_tokenize\n\ntext = \u003cspan class=\"hljs-string\"\u003e'We are lucky to live in an age in which we are still making discoveries'\u003c/span\u003e\n\n# 토큰화 - 텍스트를 단어로 나누기\nwords = \u003cspan class=\"hljs-title function_\"\u003eword_tokenize\u003c/span\u003e(text)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(words)\n# [\u003cspan class=\"hljs-string\"\u003e'We'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'are'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'lucky'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'to'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'live'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'in'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'an'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'age'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'in'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'which'\u003c/span\u003e,\n#  \u003cspan class=\"hljs-string\"\u003e'we'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'are'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'still'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'making'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'discoveries'\u003c/span\u003e]\n\nstemmer = \u003cspan class=\"hljs-title class_\"\u003eSnowballStemmer\u003c/span\u003e(language=\u003cspan class=\"hljs-string\"\u003e\"english\"\u003c/span\u003e)\nstemmed_words = \u003cspan class=\"hljs-title function_\"\u003elist\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003emap\u003c/span\u003e(lambda \u003cspan class=\"hljs-attr\"\u003ex\u003c/span\u003e: stemmer.\u003cspan class=\"hljs-title function_\"\u003estem\u003c/span\u003e(x), words))\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(stemmed_words)\n# [\u003cspan class=\"hljs-string\"\u003e'we'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'are'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'lucki'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'to'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'live'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'in'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'an'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'age'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'in'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'which'\u003c/span\u003e,\n#  \u003cspan class=\"hljs-string\"\u003e'we'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'are'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'still'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'make'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'discoveri'\u003c/span\u003e]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e자, 이제 우리 단어들의 기본 형태 리스트가 있습니다. 다음 단계는 이들 빈도를 계산하여 벡터를 만드는 것입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e collections\nbag_of_words = collections.\u003cspan class=\"hljs-title class_\"\u003eCounter\u003c/span\u003e(stemmed_words)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(bag_of_words)\n# {\u003cspan class=\"hljs-string\"\u003e'we'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'are'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'in'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'lucki'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'to'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'live'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \n# \u003cspan class=\"hljs-string\"\u003e'an'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'age'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'which'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'still'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'make'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'discoveri'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e사실, 만약 텍스트를 벡터로 변환하고 싶다면, 텍스트에 있는 단어뿐만 아니라 전체 어휘를 고려해야 합니다. \"i\", \"you\", \"study\"도 어휘에 있다고 가정하고, 파인만의 명언에서 벡터를 만들어 봅시다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_0.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e이 방법은 꽤 기본적이며 단어의 의미를 고려하지 않기 때문에 \"그 여자는 데이터 과학을 공부하고 있다\"와 \"젊은 여성이 AI와 ML을 배우고 있다.\"라는 문장이 서로 가까운 위치에 있지 않을 수 있습니다.\u003c/p\u003e\n\u003ch2\u003eTF-IDF\u003c/h2\u003e\n\u003cp\u003e단어 가방 접근법의 약간 개선된 버전인 TF-IDF(Term Frequency — Inverse Document Frequency)입니다. 이것은 두 가지 지표의 곱셈입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_1.png\" alt=\"Markdown Table\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e용어 빈도는 문서에서 단어의 빈도를 보여줍니다. 이를 계산하는 가장 흔한 방법은 이 문서에서 용어의 로우 카운트(단어 가방에 있는 것처럼)을 전체 용어(단어) 수로 나누는 것입니다. 그러나 로우 카운트, 부욜리언 \"빈도\", 정규화에 대한 다양한 접근 방법이 많이 있습니다. 위키피디아에서 다양한 접근 방법에 대해 더 배울 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_2.png\" alt=\"Markdown Table\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e역문서 주파수는 단어가 얼마나 많은 정보를 제공하는지를 나타냅니다. 예를 들어, \"a\"나 \"that\" 같은 단어는 문서 주제에 대해 추가 정보를 제공하지 않습니다. 대조적으로, \"ChatGPT\"나 \"생물정보학\" 같은 단어는 도메인을 정의하는 데 도움이 될 수 있습니다 (하지만 이 문장에는 해당하지 않음). 이는 전체 문서 수와 해당 단어를 포함하는 문서 수의 비율의 로그함수로 계산됩니다. IDF가 0에 가까울수록 단어가 흔하고 제공하는 정보가 더 적습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_3.png\"\u003e\n\u003cp\u003e그래서 결과적으로 우리는 일반적인 단어 (\"I\"나 \"you\"와 같은)이 낮은 가중치를 갖는 벡터를 얻게됩니다. 한편, 문서에서 여러 번 발생하는 드문 단어들은 더 높은 가중치를 갖게 됩니다. 이 전략은 약간 더 나은 결과를 제공하지만 여전히 의미적 의미를 잡아내기는 어렵습니다.\u003c/p\u003e\n\u003cp\u003e이 방법론의 다른 어려움은 상당히 희소한 벡터를 생성한다는 점입니다. 벡터의 길이는 말뭉치 크기와 동일합니다. 영어에는 약 470,000개의 고유 단어가 있습니다(출처). 그러므로 우리는 거대한 벡터를 갖게 될 것입니다. 하지만 문장에는 50개 이상의 고유 단어가 나타나지 않을 것이므로 벡터의 값 중 99.99%는 0일 것입니다. 이는 어떤 정보도 인코딩하지 않습니다. 이에 대해 과학자들은 밀도 있는 벡터 표현에 대해 고민하기 시작했습니다.\u003c/p\u003e\n\u003ch2\u003eWord2Vec\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e가장 유명한 밀집 표현 방법 중 하나는 구글이 2013년에 Mikolov 등이 제안한 \"효율적인 단어 표현 추정을 위한 Word2Vec\" 논문에서 소개한 word2vec입니다.\u003c/p\u003e\n\u003cp\u003e논문에서 언급된 두 가지 word2vec 접근 방식은 Continuous Bag of Words(주변 단어를 기반으로 단어를 예측하는 방법)와 Skip-gram(반대 작업인 단어를 기반으로 문맥을 예측하는 방법)입니다.\u003c/p\u003e\n\u003cp\u003e밀집 벡터 표현의 핵심 아이디어는 두 모델을 훈련하는 것입니다: 인코더와 디코더. 예를 들어, Skip-gram의 경우 \"christmas\"라는 단어를 인코더에 전달할 수 있습니다. 그런 다음, 인코더가 \"merry\", \"to\", \"you\"와 같은 단어를 얻을 것으로 예상하여 디코더에 전달할 수 있는 벡터를 생성할 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_5.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003e이 모델은 이제 단어의 의미를 고려하기 시작했습니다. 단어의 맥락에서 훈련되었기 때문입니다. 그러나 형태학(예: \"-less\"는 무언가의 부족을 의미함)을 무시합니다. 나중에는 GloVe에서 서브워드 스킵-그램을 살펴봄으로써 이 단점을 개선했습니다.\u003c/p\u003e\n\u003cp\u003e또한, word2vec은 단어와만 작동할 수 있었지만, 우리는 전체 문장을 인코딩하고 싶습니다. 그러니, 트랜스포머로 다음 진화 단계로 넘어가 봅시다.\u003c/p\u003e\n\u003ch2\u003e트랜스포머와 문장 임베딩\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다음 진화는 Vaswani 등이 발표한 \"Attention Is All You Need\" 논문에서 소개된 트랜스포머 접근 방식과 관련이 있었습니다. 트랜스포머는 정보가 풍부한 밀집 벡터를 생성할 수 있었고 현대 언어 모델의 주요 기술로 자리 잡게 되었습니다.\u003c/p\u003e\n\u003cp\u003e저는 트랜스포머의 구조 세부 사항에 대해 다루지 않겠습니다. 왜냐하면 이것은 우리 주제와 관련이 그리 크지 않고 많은 시간이 소요되기 때문입니다. 더 배우고 싶다면 \"Transformers, Explained\" 또는 \"The Illustrated Transformer\"와 같은 다양한 자료가 많이 있습니다.\u003c/p\u003e\n\u003cp\u003e트랜스포머를 사용하면 동일한 \"핵심\" 모델을 사용하여 다른 사용 사례에 대해 미세 조정할 수 있으며, 핵심 모델을 다시 학습시킬 필요가 없습니다(시간이 많이 소요되고 상당한 비용이 듭니다). 이것은 사전 훈련된 모델의 등장으로 이어졌습니다. 가장 인기 있는 최초의 모델 중 하나는 Google AI가 개발한 BERT(Bidirectional Encoder Representations from Transformers)였습니다.\u003c/p\u003e\n\u003cp\u003e내부적으로 BERT는 여전히 word2vec과 유사한 토큰 수준에서 작동하지만, 우리는 여전히 문장 임베딩을 얻고 싶습니다. 따라서, 모든 토큰 벡터의 평균을 취하는 단순한 방법을 적용할 수 있습니다. 유감스럽게도, 이 방법은 좋은 성능을 보여주지 않습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e2019년에 이 문제는 Sentence-BERT가 출시되면서 해결되었습니다. 이는 의미론적 텍스트 유사성 작업에서 이전 방법들을 모두 능가하며 문장 포함 벡터의 계산을 가능하게 했습니다.\u003c/p\u003e\n\u003cp\u003e이 주제는 매우 방대하기 때문에 이 기사에서 모두 다 다룰 수는 없을 거예요. 그러니 진지하게 관심이 있다면 이 기사에서 문장 포함 벡터에 대해 더 배울 수 있습니다.\u003c/p\u003e\n\u003cp\u003e우리는 임베딩의 발전을 간략히 다뤘고 이론에 대한 고수준 이해를 얻었습니다. 이제 실습으로 넘어가서 OpenAI 도구를 사용하여 어떻게 임베딩을 계산하는지 배워보겠습니다.\u003c/p\u003e\n\u003ch1\u003e임베딩 계산\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 기사에서는 OpenAI 임베딩을 사용할 것입니다. 최근에 출시된 새로운 모델인 text-embedding-3-small을 시도해볼 것입니다. 이 새로운 모델은 text-embedding-ada-002보다 성능이 더 좋게 나타났습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e널리 사용되는 다국어 검색 (MIRACL) 벤치마크의 평균 점수가 31.4%에서 44.0%로 상승했습니다.\u003c/li\u003e\n\u003cli\u003e영어 작업에 대한 자주 사용되는 벤치마크인 MTEB의 평균 성능도 향상되어 61.0%에서 62.3%로 상승했습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOpenAI는 또한 새로운 큰 모델인 text-embedding-3-large를 출시했습니다. 이제 이것이 가장 우수한 임베딩 모델입니다.\u003c/p\u003e\n\u003cp\u003e데이터 소스로는 Stack Exchange Data Dump의 작은 샘플을 사용할 것입니다. 이는 Stack Exchange 네트워크에서 모든 사용자 기여 콘텐츠의 익명화된 덤프입니다. 저는 흥미로운 주제를 선택하고 각각에서 100개의 질문을 샘플링했습니다. 주제는 생성적 AI부터 커피 또는 자전거까지 다양합니다. 그래서 다양한 주제를 볼 수 있을 겁니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e먼저, 모든 스택 오버플로우 질문에 대한 임베딩을 계산해야 합니다. 한 번 실행하고 결과를 로컬로 저장하는 것이 좋습니다(파일이나 벡터 저장소에). OpenAI Python 패키지를 사용하여 임베딩을 생성할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e openai \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OpenAI\nclient = OpenAI()\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eget_embedding\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003etext, model=\u003cspan class=\"hljs-string\"\u003e\"text-embedding-3-small\"\u003c/span\u003e\u003c/span\u003e):\n   text = text.replace(\u003cspan class=\"hljs-string\"\u003e\"\\n\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\" \"\u003c/span\u003e)\n   \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e client.embeddings.create(\u003cspan class=\"hljs-built_in\"\u003einput\u003c/span\u003e = [text], model=model)\\\n       .data[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].embedding\n\nget_embedding(\u003cspan class=\"hljs-string\"\u003e\"We are lucky to live in an age in which we are still making discoveries.\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e결과적으로, 우리는 부동 소수점 숫자로 이루어진 1536차원 벡터를 얻습니다. 이제 이를 모든 데이터에 대해 반복하고 값을 분석할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e가장 궁금할 수 있는 주요 질문은 의미적으로 문장들이 얼마나 가까운지입니다. 답을 발견하기 위해 벡터 간의 거리 개념을 논의해 보겠습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e벡터 간 거리\u003c/h1\u003e\n\u003cp\u003e임베딩은 사실 벡터입니다. 따라서 두 문장이 얼마나 가까운지 이해하려면 벡터 간 거리를 계산할 수 있습니다. 더 작은 거리는 더 가까운 의미를 나타낼 것입니다.\u003c/p\u003e\n\u003cp\u003e두 벡터 간의 거리를 측정하는 데 사용할 수 있는 다양한 메트릭이 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e유클리디안 거리 (L2),\u003c/li\u003e\n\u003cli\u003e맨하탄 거리 (L1),\u003c/li\u003e\n\u003cli\u003e내적 (Dot product),\u003c/li\u003e\n\u003cli\u003e코사인 거리.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그들에 대해 이야기해 봅시다. 간단한 예로, 우리는 두 개의 2D 벡터를 사용할 것입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003evector1 = [\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e]\nvector2 = [\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e유클리디안 거리 (L2)\u003c/h2\u003e\n\u003cp\u003e두 지점(또는 벡터) 사이의 거리를 정의하는 가장 표준적인 방법은 유클리디안 거리 또는 L2 norm입니다. 이 측정 기준은 일상생활에서 가장 많이 사용되며, 예를 들어 2개의 도시 사이의 거리를 언급할 때 사용됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eL2 거리에 대한 시각적 표현과 공식이 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_6.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003e파이썬 또는 numpy 함수를 사용하여 이 메트릭을 계산할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\n\u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003elist\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003emap\u003c/span\u003e(\u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e x, y: (x - y) ** \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, vector1, vector2))) ** \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# 2.2361\u003c/span\u003e\n\nnp.linalg.norm((np.array(vector1) - np.array(vector2)), \u003cspan class=\"hljs-built_in\"\u003eord\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# 2.2361\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e맨해튼 거리 (L1)\u003c/h1\u003e\n\u003cp\u003e다른 일반적으로 사용되는 거리 측정 방법은 L1 노름 또는 맨해튼 거리입니다. 이 거리는 뉴욕의 맨해튼 섬에서 명명되었습니다. 이 섬은 거리가 격자 레이아웃으로 되어 있고, 맨해튼에서 두 지점 사이의 가장 짧은 경로는 격자 모양을 따라야 하므로 L1 거리가 됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_7.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e우리는 이를 처음부터 구현하거나 numpy 함수를 사용하여 구현할 수도 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title function_\"\u003esum\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elist\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003emap\u003c/span\u003e(lambda x, \u003cspan class=\"hljs-attr\"\u003ey\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003eabs\u003c/span\u003e(x - y), vector1, vector2)))\r\n# \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e\r\n\r\nnp.\u003cspan class=\"hljs-property\"\u003elinalg\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003enorm\u003c/span\u003e((np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(vector1) - np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(vector2)), ord = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\r\n# \u003cspan class=\"hljs-number\"\u003e3.0\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e내적(Dot product)\u003c/h2\u003e\n\u003cp\u003e벡터 간 거리를 계산하는 다른 방법은 내적 또는 스칼라 곱을 계산하는 것입니다. 다음은 해당 공식이며 쉽게 구현할 수 있습니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_8.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003elist\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003emap\u003c/span\u003e(\u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e x, y: x*y, vector1, vector2)))\n\u003cspan class=\"hljs-comment\"\u003e# 11\u003c/span\u003e\n\nnp.dot(vector1, vector2)\n\u003cspan class=\"hljs-comment\"\u003e# 11\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 메트릭은 해석하기가 조금 까다로운 편이에요. 한편으로는 벡터가 한 방향을 향하고 있는지를 보여줍니다. 다른 한편으로는 결과는 벡터들의 크기에 크게 의존합니다. 예를 들어 두 쌍의 벡터 간의 내적을 계산해볼게요:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(1, 1) vs (1, 1)\u003c/li\u003e\n\u003cli\u003e(1, 1) vs (10, 10).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e두 경우 모두 벡터가 일직선상에 있지만, 두 번째 경우에 내적은 10배 크게 나와요: 2 대 20.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e코사인 유사도\u003c/h2\u003e\n\u003cp\u003e많은 경우, 코사인 유사도가 사용됩니다. 코사인 유사도는 벡터의 크기(또는 노름)에 의해 정규화된 내적입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_9.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003e이전처럼 직접 모든 것을 계산하거나 sklearn의 함수를 사용할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edot_product = \u003cspan class=\"hljs-title function_\"\u003esum\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elist\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003emap\u003c/span\u003e(lambda x, \u003cspan class=\"hljs-attr\"\u003ey\u003c/span\u003e: x*y, vector1, vector2)))\nnorm_vector1 = \u003cspan class=\"hljs-title function_\"\u003esum\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elist\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003emap\u003c/span\u003e(lambda \u003cspan class=\"hljs-attr\"\u003ex\u003c/span\u003e: x ** \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, vector1))) ** \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e\nnorm_vector2 = \u003cspan class=\"hljs-title function_\"\u003esum\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elist\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003emap\u003c/span\u003e(lambda \u003cspan class=\"hljs-attr\"\u003ex\u003c/span\u003e: x ** \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, vector2))) ** \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e\n\ndot_product/norm_vector1/norm_vector2\n\n# \u003cspan class=\"hljs-number\"\u003e0.8575\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.\u003cspan class=\"hljs-property\"\u003emetrics\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003epairwise\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e cosine_similarity\n\n\u003cspan class=\"hljs-title function_\"\u003ecosine_similarity\u003c/span\u003e(\n  np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(vector1).\u003cspan class=\"hljs-title function_\"\u003ereshape\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e), \n  np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(vector2).\u003cspan class=\"hljs-title function_\"\u003ereshape\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e))[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n\n# \u003cspan class=\"hljs-number\"\u003e0.8575\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ecosine_similarity 함수는 2차원 배열을 기대합니다. 그래서 numpy 배열을 reshape 해주어야 합니다.\u003c/p\u003e\n\u003cp\u003e이 메트릭의 물리적 의미에 대해 조금 이야기해 봅시다. Cosine similarity는 두 벡터 사이의 코사인 값과 같습니다. 벡터가 서로 가까울수록 메트릭 값이 높아집니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_10.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e우리는 심지어 벡터 사이의 정확한 각도를 도 단위로 계산할 수도 있어요. 약 30도 정도의 결과를 얻었고, 꽤 합리적으로 보이네요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e math\nmath.\u003cspan class=\"hljs-title function_\"\u003edegrees\u003c/span\u003e(math.\u003cspan class=\"hljs-title function_\"\u003eacos\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0.8575\u003c/span\u003e))\n\n# \u003cspan class=\"hljs-number\"\u003e30.96\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e어떤 측정 지표를 사용할까요?\u003c/h2\u003e\n\u003cp\u003e우리는 두 벡터 사이의 거리를 계산하는 다양한 방법에 대해 토론해 왔고, 여러분은 어떤 방법을 사용할지 고려하기 시작할 수 있을 거예요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e내가 가진 임베딩을 비교하기 위해 어떤 거리든 사용할 수 있어요. 예를 들어, 다른 클러스터 사이의 평균 거리를 계산했어요. L2 거리와 코사인 유사도 모두 비슷한 결과를 보여줘요:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e클러스터 내의 객체들은 다른 클러스터보다 서로 더 가까워요. L2 거리에 대해 가까울수록 낮은 거리를 의미하지만 코사인 유사도에서는 가까운 객체일수록 값이 높아져요. 헷갈리지 마세요.\u003c/li\u003e\n\u003cli\u003e\"정치\"와 \"경제\" 또는 \"ai\"와 \"데이터과학\"과 같이 일부 주제들이 서로 아주 가까운 것을 알 수 있어요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_11.png\"\u003e\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_12.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그러나 NLP 작업에 대해서는 일반적으로 코사인 유사도를 사용하는 것이 최선의 방법입니다. 몇 가지 그 이유는:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e코사인 유사도는 -1과 1 사이에 있으며, L1과 L2는 무제한이기 때문에 해석하기 쉽습니다.\u003c/li\u003e\n\u003cli\u003e실용적인 측면에서 유클리드 거리의 제곱근보다 내적을 계산하는 것이 더 효과적입니다.\u003c/li\u003e\n\u003cli\u003e코사인 유사도는 차원의 저주에 영향을 덜 받습니다 (이에 대해 뒤에서 더 얘기할 것입니다).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e위의 결과에서 인트라 및 인터 클러스터 거리 간의 차이가 크지 않다는 점을 알 수 있을 것입니다. 이 현상의 원인은 벡터의 고차원성 때문입니다. 이 효과는 \"차원의 저주\"라고 불리며, 차원이 높을수록 벡터 간 거리 분포가 좁아진다는 것을 알 수 있습니다. 이에 대해 더 자세히 알아보려면 이 글을 참조해보세요.\u003c/p\u003e\n\u003cp\u003e간단히 설명드리겠습니다. OpenAI 임베딩 값의 분포를 계산하고 차원이 다른 300개의 벡터 집합을 생성했습니다. 그런 다음, 모든 벡터 사이의 거리를 계산하고 히스토그램을 그렸습니다. 차원이 증가함에 따라 벡터의 거리 분포가 좁아진다는 것을 쉽게 확인할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_13.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e임베딩 사이의 유사성을 측정하는 방법을 배웠어요. 여기서 이론적인 부분은 마쳤고, 더 실용적인 부분(시각화 및 실용적인 응용)으로 넘어가겠습니다. 데이터를 보는 것이 가장 중요하니, 시각화부터 시작해봐요.\u003c/p\u003e\n\u003ch1\u003e임베딩 시각화\u003c/h1\u003e\n\u003cp\u003e데이터를 이해하는 가장 좋은 방법은 시각적으로 나타내는 것이에요. 아쉽지만, 임베딩은 1536차원이 있어서 데이터를 살펴보기가 꽤 어려워요. 그러나, 한 가지 방법이 있어요: 차원 축소 기술을 사용하여 벡터를 이차원 공간에 투영하는 것이에요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003ePCA\u003c/h2\u003e\n\u003cp\u003e가장 기본적인 차원 축소 기술은 PCA(주성분 분석)입니다. 이를 사용해 봅시다.\u003c/p\u003e\n\u003cp\u003e먼저, sklearn에 전달하기 위해 임베딩을 2D numpy 배열로 변환해야 합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\nembeddings_array = np.array(df.embedding.values.tolist())\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(embeddings_array.shape)\n\u003cspan class=\"hljs-comment\"\u003e# (1400, 1536)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그럼, 우리는 PCA 모델을 n_components = 2로 초기화해야 해요 (2D 시각화를 생성하고 싶기 때문에), 전체 데이터에서 모델을 학습하고 새로운 값을 예측해야 해요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.\u003cspan class=\"hljs-property\"\u003edecomposition\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003ePCA\u003c/span\u003e\n\npca_model = \u003cspan class=\"hljs-title function_\"\u003ePCA\u003c/span\u003e(n_components = \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\npca_model.\u003cspan class=\"hljs-title function_\"\u003efit\u003c/span\u003e(embeddings_array)\n\npca_embeddings_values = pca_model.\u003cspan class=\"hljs-title function_\"\u003etransform\u003c/span\u003e(embeddings_array)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(pca_embeddings_values.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e)\n# (\u003cspan class=\"hljs-number\"\u003e1400\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e결과적으로, 우리는 각 질문에 대해 두 개의 특성을 가진 행렬을 얻었으므로, scatter plot에서 쉽게 시각화할 수 있어요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003efig = px.\u003cspan class=\"hljs-title function_\"\u003escatter\u003c/span\u003e(\n    x = pca_embeddings_values[:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], \n    y = pca_embeddings_values[:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e],\n    color = df.\u003cspan class=\"hljs-property\"\u003etopic\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003evalues\u003c/span\u003e,\n    hover_name = df.\u003cspan class=\"hljs-property\"\u003efull_text\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003evalues\u003c/span\u003e,\n    title = \u003cspan class=\"hljs-string\"\u003e'PCA embeddings'\u003c/span\u003e, width = \u003cspan class=\"hljs-number\"\u003e800\u003c/span\u003e, height = \u003cspan class=\"hljs-number\"\u003e600\u003c/span\u003e,\n    color_discrete_sequence = plotly.\u003cspan class=\"hljs-property\"\u003ecolors\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003equalitative\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eAlphabet_r\u003c/span\u003e\n)\n\nfig.\u003cspan class=\"hljs-title function_\"\u003eupdate_layout\u003c/span\u003e(\n    xaxis_title = \u003cspan class=\"hljs-string\"\u003e'first component'\u003c/span\u003e, \n    yaxis_title = \u003cspan class=\"hljs-string\"\u003e'second component'\u003c/span\u003e)\nfig.\u003cspan class=\"hljs-title function_\"\u003eshow\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_14.png\" alt=\"img\"\u003e\u003c/p\u003e\n\u003cp\u003e각 주제의 질문들이 서로 꽤 가까이 위치해 있는 것을 볼 수 있어 좋습니다. 그러나 모든 클러스터가 혼재되어 있어서 개선할 부분이 있습니다.\u003c/p\u003e\n\u003ch2\u003et-SNE\u003c/h2\u003e\n\u003cp\u003ePCA는 선형 알고리즘이지만, 대부분의 관계는 실제로는 비선형입니다. 그래서 비선형성 때문에 클러스터를 분리할 수 없을 수도 있습니다. 비선형 알고리즘인 t-SNE을 사용해보고 더 나은 결과를 보여줄 수 있는지 확인해봅시다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e거의 동일한 코드를 사용했습니다. PCA 대신 t-SNE 모델을 사용했어요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.\u003cspan class=\"hljs-property\"\u003emanifold\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eTSNE\u003c/span\u003e\ntsne_model = \u003cspan class=\"hljs-title function_\"\u003eTSNE\u003c/span\u003e(n_components=\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, random_state=\u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e)\ntsne_embeddings_values = tsne_model.\u003cspan class=\"hljs-title function_\"\u003efit_transform\u003c/span\u003e(embeddings_array)\n\nfig = px.\u003cspan class=\"hljs-title function_\"\u003escatter\u003c/span\u003e(\n    x = tsne_embeddings_values[:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], \n    y = tsne_embeddings_values[:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e],\n    color = df.\u003cspan class=\"hljs-property\"\u003etopic\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003evalues\u003c/span\u003e,\n    hover_name = df.\u003cspan class=\"hljs-property\"\u003efull_text\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003evalues\u003c/span\u003e,\n    title = \u003cspan class=\"hljs-string\"\u003e't-SNE embeddings'\u003c/span\u003e, width = \u003cspan class=\"hljs-number\"\u003e800\u003c/span\u003e, height = \u003cspan class=\"hljs-number\"\u003e600\u003c/span\u003e,\n    color_discrete_sequence = plotly.\u003cspan class=\"hljs-property\"\u003ecolors\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003equalitative\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eAlphabet_r\u003c/span\u003e\n)\n\nfig.\u003cspan class=\"hljs-title function_\"\u003eupdate_layout\u003c/span\u003e(\n    xaxis_title = \u003cspan class=\"hljs-string\"\u003e'first component'\u003c/span\u003e, \n    yaxis_title = \u003cspan class=\"hljs-string\"\u003e'second component'\u003c/span\u003e)\nfig.\u003cspan class=\"hljs-title function_\"\u003eshow\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003et-SNE 결과가 훨씬 좋아 보여요. 대부분의 클러스터가 분리되어 있지만 \"genai\", \"datascience\", \"ai\" 는 분리되지 않았어요. 그러나 이건 예상한대로에요 - 이러한 주제를 내가 분리할 수 있을지 의심스러워요.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_15.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 시각화를 보면 임베딩이 의미적 의미를 인코딩하는 데 상당히 효과적임을 확인할 수 있어요.\u003c/p\u003e\n\u003cp\u003e또한, 데이터를 3D로 시각화할 수 있는 사영(projection)을 만들어볼 수 있어요. 실용적일지는 확실하지 않지만, 데이터를 3D로 살펴보는 것은 흥미롭고 관심을 끌 수 있어요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003etsne_model_3d = \u003cspan class=\"hljs-title function_\"\u003eTSNE\u003c/span\u003e(n_components=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, random_state=\u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e)\ntsne_3d_embeddings_values = tsne_model_3d.\u003cspan class=\"hljs-title function_\"\u003efit_transform\u003c/span\u003e(embeddings_array)\n\nfig = px.\u003cspan class=\"hljs-title function_\"\u003escatter_3d\u003c/span\u003e(\n    x = tsne_3d_embeddings_values[:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], \n    y = tsne_3d_embeddings_values[:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e],\n    z = tsne_3d_embeddings_values[:,\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e],\n    color = df.\u003cspan class=\"hljs-property\"\u003etopic\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003evalues\u003c/span\u003e,\n    hover_name = df.\u003cspan class=\"hljs-property\"\u003efull_text\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003evalues\u003c/span\u003e,\n    title = \u003cspan class=\"hljs-string\"\u003e't-SNE embeddings'\u003c/span\u003e, width = \u003cspan class=\"hljs-number\"\u003e800\u003c/span\u003e, height = \u003cspan class=\"hljs-number\"\u003e600\u003c/span\u003e,\n    color_discrete_sequence = plotly.\u003cspan class=\"hljs-property\"\u003ecolors\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003equalitative\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eAlphabet_r\u003c/span\u003e,\n    opacity = \u003cspan class=\"hljs-number\"\u003e0.7\u003c/span\u003e\n)\nfig.\u003cspan class=\"hljs-title function_\"\u003eupdate_layout\u003c/span\u003e(xaxis_title = \u003cspan class=\"hljs-string\"\u003e'first component'\u003c/span\u003e, yaxis_title = \u003cspan class=\"hljs-string\"\u003e'second component'\u003c/span\u003e)\nfig.\u003cspan class=\"hljs-title function_\"\u003eshow\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_16.png\" alt=\"3D 시각화\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e바코드\u003c/h2\u003e\n\u003cp\u003e임베딩을 이해하는 방법은 몇 개를 바코드처럼 시각화하여 상관 관계를 확인하는 것입니다. 나는 세 가지 임베딩 예시를 선택했습니다: 두 개는 서로에게 가장 가깝고, 나머지 하나는 데이터 세트에서 가장 멀리 떨어져 있는 예시입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eembedding1 = df.\u003cspan class=\"hljs-property\"\u003eloc\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e].\u003cspan class=\"hljs-property\"\u003eembedding\u003c/span\u003e\nembedding2 = df.\u003cspan class=\"hljs-property\"\u003eloc\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e616\u003c/span\u003e].\u003cspan class=\"hljs-property\"\u003eembedding\u003c/span\u003e\nembedding3 = df.\u003cspan class=\"hljs-property\"\u003eloc\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e749\u003c/span\u003e].\u003cspan class=\"hljs-property\"\u003eembedding\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e seaborn \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e sns\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.\u003cspan class=\"hljs-property\"\u003epyplot\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\nembed_len_thr = \u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e\n\nsns.\u003cspan class=\"hljs-title function_\"\u003eheatmap\u003c/span\u003e(np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(embedding1[:embed_len_thr]).\u003cspan class=\"hljs-title function_\"\u003ereshape\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, embed_len_thr),\n    cmap = \u003cspan class=\"hljs-string\"\u003e\"Greys\"\u003c/span\u003e, center = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, square = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e, \n    xticklabels = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e, cbar = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003egcf\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003eset_size_inches\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003eyticks\u003c/span\u003e([\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e], labels = [\u003cspan class=\"hljs-string\"\u003e'AI'\u003c/span\u003e])\nplt.\u003cspan class=\"hljs-title function_\"\u003eshow\u003c/span\u003e()\n\nsns.\u003cspan class=\"hljs-title function_\"\u003eheatmap\u003c/span\u003e(np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(embedding3[:embed_len_thr]).\u003cspan class=\"hljs-title function_\"\u003ereshape\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, embed_len_thr),\n    cmap = \u003cspan class=\"hljs-string\"\u003e\"Greys\"\u003c/span\u003e, center = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, square = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e, \n    xticklabels = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e, cbar = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003egcf\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003eset_size_inches\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003eyticks\u003c/span\u003e([\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e], labels = [\u003cspan class=\"hljs-string\"\u003e'AI'\u003c/span\u003e])\nplt.\u003cspan class=\"hljs-title function_\"\u003eshow\u003c/span\u003e()\n\nsns.\u003cspan class=\"hljs-title function_\"\u003eheatmap\u003c/span\u003e(np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(embedding2[:embed_len_thr]).\u003cspan class=\"hljs-title function_\"\u003ereshape\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, embed_len_thr),\n    cmap = \u003cspan class=\"hljs-string\"\u003e\"Greys\"\u003c/span\u003e, center = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, square = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e, \n    xticklabels = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e, cbar = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003egcf\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003eset_size_inches\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003eyticks\u003c/span\u003e([\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e], labels = [\u003cspan class=\"hljs-string\"\u003e'바이오인포매틱스'\u003c/span\u003e])\nplt.\u003cspan class=\"hljs-title function_\"\u003eshow\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_17.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e우리 경우에는 고차원 때문에 벡터가 서로 가까운지 쉽게 보기 어려울 수 있습니다. 그래도 나는 이 시각화를 좋아합니다. 몇 가지 경우에 도움이 될 수도 있으니, 나는 이 아이디어를 당신과 공유하고자 합니다.\u003c/p\u003e\n\u003cp\u003e우리는 임베딩을 시각화하는 방법을 배웠고, 텍스트의 의미를 파악하는 능력에 대한 의문은 남지 않았습니다. 이제 실제로 임베딩을 어떻게 활용할 수 있는지에 대해 논의하는 가장 흥미로운 부분으로 넘어가 보겠습니다.\u003c/p\u003e\n\u003ch1\u003e실용적인 응용\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e물론, 임베딩의 주요 목표는 텍스트를 숫자의 벡터로 인코딩하거나 시각화하기 위해서만 하는 것이 아닙니다. 우리는 텍스트의 의미를 포착하는 능력에서 많은 이점을 얻을 수 있습니다. 실용적인 예제들을 함께 살펴보겠습니다.\u003c/p\u003e\n\u003ch2\u003e클러스터링\u003c/h2\u003e\n\u003cp\u003e먼저 클러스터링부터 시작해보죠. 클러스터링은 초기 레이블 없이 데이터를 그룹으로 분할할 수 있는 비지도학습 기술입니다. 클러스터링을 통해 데이터의 내부 구조적 패턴을 이해하는 데 도움을 받을 수 있습니다.\u003c/p\u003e\n\u003cp\u003e가장 기본적인 클러스터링 알고리즘 중 하나인 K-평균을 사용할 것입니다. K-평균 알고리즘을 위해서는 클러스터의 개수를 지정해야 합니다. 실루엣 스코어를 사용하여 최적의 클러스터 수를 정의할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e2부터 50까지의 k (클러스터 수)를 시도해 보겠습니다. 각 k에 대해 모델을 훈련하고 실루엣 점수를 계산할 것입니다. 실루엣 점수가 높을수록, 더 좋은 클러스터링 결과를 얻을 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.cluster \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e KMeans\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.metrics \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e silhouette_score\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tqdm\n\nsilhouette_scores = []\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e k \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e tqdm.tqdm(\u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e51\u003c/span\u003e)):\n    kmeans = KMeans(n_clusters=k, \n                    random_state=\u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e, \n                    n_init=\u003cspan class=\"hljs-string\"\u003e'auto'\u003c/span\u003e).fit(embeddings_array)\n    kmeans_labels = kmeans.labels_\n    silhouette_scores.append(\n        {\n            \u003cspan class=\"hljs-string\"\u003e'k'\u003c/span\u003e: k,\n            \u003cspan class=\"hljs-string\"\u003e'silhouette_score'\u003c/span\u003e: silhouette_score(embeddings_array, \n                                                 kmeans_labels, metric=\u003cspan class=\"hljs-string\"\u003e'cosine'\u003c/span\u003e)\n        }\n    )\n\nfig = px.line(pd.DataFrame(silhouette_scores).set_index(\u003cspan class=\"hljs-string\"\u003e'k'\u003c/span\u003e),\n              title=\u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;b\u003eK-means 클러스터링을 위한 실루엣 점수\u0026#x3C;/b\u003e'\u003c/span\u003e,\n              labels={\u003cspan class=\"hljs-string\"\u003e'value'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'실루엣 점수'\u003c/span\u003e}, \n              color_discrete_sequence=plotly.colors.qualitative.Alphabet)\nfig.update_layout(showlegend=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e우리의 경우, k가 11일 때 실루엣 점수가 최대치에 도달합니다. 따라서 최종 모델에는 이 클러스터 수를 사용합시다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_18.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e클러스터를 시각화해 보는 t-SNE를 이용한 차원 축소를 이미 이전에 수행한 것처럼 해보겠습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003etsne_model = \u003cspan class=\"hljs-title function_\"\u003eTSNE\u003c/span\u003e(n_components=\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, random_state=\u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e)\ntsne_embeddings_values = tsne_model.\u003cspan class=\"hljs-title function_\"\u003efit_transform\u003c/span\u003e(embeddings_array)\n\nfig = px.\u003cspan class=\"hljs-title function_\"\u003escatter\u003c/span\u003e(\n    x = tsne_embeddings_values[:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], \n    y = tsne_embeddings_values[:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e],\n    color = \u003cspan class=\"hljs-title function_\"\u003elist\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003emap\u003c/span\u003e(lambda \u003cspan class=\"hljs-attr\"\u003ex\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'클러스터 %s'\u003c/span\u003e % x, kmeans_labels)),\n    hover_name = df.\u003cspan class=\"hljs-property\"\u003efull_text\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003evalues\u003c/span\u003e,\n    title = \u003cspan class=\"hljs-string\"\u003e'클러스터링을 위한 t-SNE 임베딩'\u003c/span\u003e, width = \u003cspan class=\"hljs-number\"\u003e800\u003c/span\u003e, height = \u003cspan class=\"hljs-number\"\u003e600\u003c/span\u003e,\n    color_discrete_sequence = plotly.\u003cspan class=\"hljs-property\"\u003ecolors\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003equalitative\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eAlphabet_r\u003c/span\u003e\n)\nfig.\u003cspan class=\"hljs-title function_\"\u003eupdate_layout\u003c/span\u003e(\n    xaxis_title = \u003cspan class=\"hljs-string\"\u003e'첫 번째 성분'\u003c/span\u003e, \n    yaxis_title = \u003cspan class=\"hljs-string\"\u003e'두 번째 성분'\u003c/span\u003e)\nfig.\u003cspan class=\"hljs-title function_\"\u003eshow\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e시각적으로 알고리즘이 클러스터를 상당히 잘 정의했음을 확인할 수 있습니다 — 그들은 꽤 잘 분리되어 있습니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_19.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e우리는 사실적인 주제 라벨을 가지고 있으므로, 클러스터링이 얼마나 좋은지를 심층적으로 판단할 수도 있어요. 각 클러스터에 대한 주제 혼합을 살펴봅시다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edf[\u003cspan class=\"hljs-string\"\u003e'cluster'\u003c/span\u003e] = \u003cspan class=\"hljs-title function_\"\u003elist\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003emap\u003c/span\u003e(lambda \u003cspan class=\"hljs-attr\"\u003ex\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'클러스터 %s'\u003c/span\u003e % x, kmeans_labels))\ncluster_stats_df = df.\u003cspan class=\"hljs-title function_\"\u003ereset_index\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003epivot_table\u003c/span\u003e(\n    index=\u003cspan class=\"hljs-string\"\u003e'cluster'\u003c/span\u003e, values=\u003cspan class=\"hljs-string\"\u003e'id'\u003c/span\u003e,\n    aggfunc=\u003cspan class=\"hljs-string\"\u003e'count'\u003c/span\u003e, columns=\u003cspan class=\"hljs-string\"\u003e'topic'\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003efillna\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003eapplymap\u003c/span\u003e(int)\n\ncluster_stats_df = cluster_stats_df.\u003cspan class=\"hljs-title function_\"\u003eapply\u003c/span\u003e(\n  lambda \u003cspan class=\"hljs-attr\"\u003ex\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e*x/cluster_stats_df.\u003cspan class=\"hljs-title function_\"\u003esum\u003c/span\u003e(axis=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e))\n\nfig = px.\u003cspan class=\"hljs-title function_\"\u003eimshow\u003c/span\u003e(\n    cluster_stats_df.\u003cspan class=\"hljs-property\"\u003evalues\u003c/span\u003e, \n    x=cluster_stats_df.\u003cspan class=\"hljs-property\"\u003ecolumns\u003c/span\u003e,\n    y=cluster_stats_df.\u003cspan class=\"hljs-property\"\u003eindex\u003c/span\u003e,\n    text_auto=\u003cspan class=\"hljs-string\"\u003e'.2f'\u003c/span\u003e, aspect=\u003cspan class=\"hljs-string\"\u003e\"auto\"\u003c/span\u003e,\n    labels=\u003cspan class=\"hljs-title function_\"\u003edict\u003c/span\u003e(x=\u003cspan class=\"hljs-string\"\u003e\"클러스터\"\u003c/span\u003e, y=\u003cspan class=\"hljs-string\"\u003e\"팩트 주제\"\u003c/span\u003e, color=\u003cspan class=\"hljs-string\"\u003e\"비율, %\"\u003c/span\u003e),\n    color_continuous_scale=\u003cspan class=\"hljs-string\"\u003e'pubugn'\u003c/span\u003e,\n    title=\u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;b\u003e각 클러스터의 주제 비율\u0026#x3C;/b\u003e'\u003c/span\u003e, height=\u003cspan class=\"hljs-number\"\u003e550\u003c/span\u003e)\n\nfig.\u003cspan class=\"hljs-title function_\"\u003eshow\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_20.png\"\u003e\n\u003cp\u003e대부분의 경우, 클러스터링은 완벽하게 작동했어요. 예를 들어, 클러스터 5에는 거의 자전거에 관한 질문만 있고, 클러스터 6은 커피에 관한 것이에요. 그러나 유사한 주제를 구별하지 못했어요:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e\"ai,\" \"genai,\" and \"datascience\"은 동일한 클러스터에 있습니다.\u003c/li\u003e\n\u003cli\u003e\"economics\"와 \"politics\"은 같은 그룹에 속합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이 예제에서는 피처로써 임베딩만 사용했지만, 질문을 한 사용자의 나이, 성별 또는 국가와 같은 추가 정보가 있다면 모델에 포함시킬 수도 있습니다.\u003c/p\u003e\n\u003ch2\u003e분류\u003c/h2\u003e\n\u003cp\u003e임베딩을 분류 또는 회귀 작업에 사용할 수 있습니다. 예를 들어 고객 리뷰 감정을 예측하는 (분류)이나 NPS 점수를 예측하는 (회귀) 등 다양한 작업에 활용할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e분류 및 회귀는 지도 학습이므로 레이블이 필요합니다. 다행히도, 우리는 질문의 주제를 알고 있으므로 모델을 적합시켜 예측할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e저는 랜덤 포레스트 분류기를 사용할 것입니다. 랜덤 포레스트에 대해 간단히 상기하고 싶다면 여기에서 확인할 수 있어요. 분류 모델의 성능을 올바르게 평가하려면 데이터 세트를 학습 및 테스트 세트(80% 대 20%)로 분할할 것입니다. 그런 다음 학습 데이터 세트에서 모델을 훈련하고 테스트 데이터 세트에서 품질을 측정할 수 있습니다(모델이 이전에 보지 못한 질문).\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.\u003cspan class=\"hljs-property\"\u003eensemble\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eRandomForestClassifier\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.\u003cspan class=\"hljs-property\"\u003emodel_selection\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e train_test_split\nclass_model = \u003cspan class=\"hljs-title class_\"\u003eRandomForestClassifier\u003c/span\u003e(max_depth = \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e)\n\n# 특징 및 대상 정의\nX = embeddings_array\ny = df.\u003cspan class=\"hljs-property\"\u003etopic\u003c/span\u003e\n\n# 데이터를 학습 및 테스트 세트로 분할\nX_train, X_test, y_train, y_test = \u003cspan class=\"hljs-title function_\"\u003etrain_test_split\u003c/span\u003e(\n    X, y, random_state = \u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e, test_size=\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e, stratify=y\n)\n\n# 적합 및 예측\nclass_model.\u003cspan class=\"hljs-title function_\"\u003efit\u003c/span\u003e(X_train, y_train)\ny_pred = class_model.\u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(X_test)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e모델의 성능을 추정하기 위해 혼동 행렬을 계산해 보겠습니다. 이상적인 상황에서는 비대각 요소가 모두 0이어야 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.\u003cspan class=\"hljs-property\"\u003emetrics\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e confusion_matrix\ncm = \u003cspan class=\"hljs-title function_\"\u003econfusion_matrix\u003c/span\u003e(y_test, y_pred)\n\nfig = px.\u003cspan class=\"hljs-title function_\"\u003eimshow\u003c/span\u003e(\n  cm, x = class_model.\u003cspan class=\"hljs-property\"\u003eclasses_\u003c/span\u003e,\n  y = class_model.\u003cspan class=\"hljs-property\"\u003eclasses_\u003c/span\u003e, text_auto=\u003cspan class=\"hljs-string\"\u003e'd'\u003c/span\u003e, \n  aspect=\u003cspan class=\"hljs-string\"\u003e\"auto\"\u003c/span\u003e, \n  labels=\u003cspan class=\"hljs-title function_\"\u003edict\u003c/span\u003e(\n      x=\u003cspan class=\"hljs-string\"\u003e\"predicted label\"\u003c/span\u003e, y=\u003cspan class=\"hljs-string\"\u003e\"true label\"\u003c/span\u003e, \n      color=\u003cspan class=\"hljs-string\"\u003e\"cases\"\u003c/span\u003e), \n  color_continuous_scale=\u003cspan class=\"hljs-string\"\u003e'pubugn'\u003c/span\u003e,\n  title = \u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;b\u003e혼동 행렬\u0026#x3C;/b\u003e'\u003c/span\u003e, height = \u003cspan class=\"hljs-number\"\u003e550\u003c/span\u003e)\n\nfig.\u003cspan class=\"hljs-title function_\"\u003eshow\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_21.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e군집화와 유사한 결과를 확인할 수 있습니다. 일부 주제는 쉽게 분류되고 정확도가 100%인 반면, 다른 주제들은 구별하기 어려운 경우도 있습니다(특히 \"ai\" 주제).\u003c/p\u003e\n\u003cp\u003e하지만 전체적으로 91.8%의 정확도를 달성했으며, 이는 꽤 좋은 성과입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e이상 징후 찾기\u003c/h2\u003e\n\u003cp\u003e데이터에서 이상 징후를 찾기 위해 임베딩을 사용할 수도 있습니다. 예를 들어, t-SNE 그래프에서 \"여행\" 주제에 대한 몇 가지 질문이 군집에서 꽤 멀리 떨어져 있는 것을 볼 수 있었습니다. 이 테마를 살펴보고 이상 징후를 찾아보겠습니다. 이를 위해 이상 탐지 알고리즘인 Isolation Forest를 사용할 것입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.\u003cspan class=\"hljs-property\"\u003eensemble\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eIsolationForest\u003c/span\u003e\n\ntopic_df = df[df.\u003cspan class=\"hljs-property\"\u003etopic\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'travel'\u003c/span\u003e]\ntopic_embeddings_array = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(topic_df.\u003cspan class=\"hljs-property\"\u003eembedding\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003evalues\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003etolist\u003c/span\u003e())\n\nclf = \u003cspan class=\"hljs-title class_\"\u003eIsolationForest\u003c/span\u003e(contamination=\u003cspan class=\"hljs-number\"\u003e0.03\u003c/span\u003e, random_state=\u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e)\ntopic_df[\u003cspan class=\"hljs-string\"\u003e'is_anomaly'\u003c/span\u003e] = clf.\u003cspan class=\"hljs-title function_\"\u003efit_predict\u003c/span\u003e(topic_embeddings_array)\n\ntopic_df[topic_df.\u003cspan class=\"hljs-property\"\u003eis_anomaly\u003c/span\u003e == -\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e][[\u003cspan class=\"hljs-string\"\u003e'full_text'\u003c/span\u003e]]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e여기에서, 여행 주제에 대한 가장 흔하지 않은 댓글을 찾았습니다 (원본).\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e로마 구역의 곳곳에 있는 분수에서 물을 마셔도 안전한가요?\n\n로마를 방문했을 때 오래된 지역을 거닐며 다양한 종류의 분수를 보았습니다. 물이 끊임없이 흘러나오는 분수들이 많았는데, 땅으로 흘러가는 분수도 있고, 대야에 모이는 분수도 있었습니다.\n\n이런 분수에서 나오는 물을 마셔도 괜찮을까요? 방문객이 마실 수 있는 안전한 물일까요? 분수 사용에 대한 방문자들이 알아야 할 예절이 있을까요?\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e물에 관한 이야기이기 때문에 이 주석의 기능은 사람들이 물을 따르는 커피 주제와 밀접하게 관련되어 있습니다. 그래서 이 주석의 삽입 표현은 커피 클러스터와 꽤 가까운 것으로 보입니다.\u003c/p\u003e\n\u003cp\u003et-SNE 시각화에서 찾아보면 실제로 커피 클러스터에 가까운 것을 확인할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-TextEmbeddingsComprehensiveGuide_22.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003eRAG — 검색 증가 생성\u003c/h2\u003e\n\u003cp\u003e최근 LLM의 인기가 높아지면서, 임베딩이 RAG 사용 사례에서 널리 사용되고 있습니다.\u003c/p\u003e\n\u003cp\u003e우리는 많은 문서가 있는 경우(예: 스택 오버플로우의 모든 질문)에 검색 증가 생성이 필요합니다. 그리고 모든 정보를 항상 LLM에 전달할 수 없기 때문에\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLLM은 컨텍스트 크기에 제한이 있습니다(현재 GPT-4 Turbo의 경우 128K입니다).\u003c/li\u003e\n\u003cli\u003e우리는 토큰을 구매해야 하므로 모든 정보를 항상 전달하는 것이 더 비십니다.\u003c/li\u003e\n\u003cli\u003eLLM은 더 큰 컨텍스트에서 성능이 떨어집니다. 자세한 내용은 \"바늘 찾기\" - LLM의 압력 테스트를 확인할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e대규모 지식 베이스와 함께 작업할 수 있도록 RAG 방법론을 활용할 수 있어요:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e모든 문서에 대한 임베딩을 계산하고 벡터 저장소에 저장합니다.\u003c/li\u003e\n\u003cli\u003e사용자 요청을 받으면 해당 요청의 임베딩을 계산하여 저장소에서 관련 문서를 검색할 수 있어요.\u003c/li\u003e\n\u003cli\u003e최종 답변을 얻기 위해 LLM에게 관련 문서만 전달하면 돼요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eRAG에 대해 더 자세히 알고 싶다면 여기에 더 많은 내용을 담은 제 논문을 읽어보세요.\u003c/p\u003e\n\u003ch1\u003e요약\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 기사에서는 텍스트 임베딩에 대해 많은 세부 내용을 논의했습니다. 이제 여러분은 이 주제에 대해 완전하고 심도 있는 이해를 가졌을 것입니다. 저희 여정을 간단히 요약하면 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e먼저, 텍스트 작업 방법의 진화를 살펴보았습니다.\u003c/li\u003e\n\u003cli\u003e그 다음으로, 텍스트 간에 유사한 의미를 가지고 있는지를 이해하는 방법에 대해 논의했습니다.\u003c/li\u003e\n\u003cli\u003e그 후에는 텍스트 임베딩 시각화의 다양한 접근 방법을 살펴보았습니다.\u003c/li\u003e\n\u003cli\u003e마지막으로, 임베딩을 클러스터링, 분류, 이상 탐지 및 RAG와 같은 다양한 실용적인 작업에서 특징으로 사용해 보았습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e참고\u003c/h1\u003e\n\u003cp\u003e이 기사에서는 크리에이티브 커먼즈 라이센스 하에 공개된 스택 엑스체인지 데이터 덤프에서 데이터 세트를 사용했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 글은 다음 강좌에서 영감을 받았습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeepLearning.AI와 Google Cloud의 협력으로 진행되는 \"Understanding and Applying Text Embeddings\",\u003c/li\u003e\n\u003cli\u003eDeepLearning.AI와 Weaviate의 협력으로 진행되는 \"Vector Databases: From Embeddings to Applications\".\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-23-TextEmbeddingsComprehensiveGuide"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>