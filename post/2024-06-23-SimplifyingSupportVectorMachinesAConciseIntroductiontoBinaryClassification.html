<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기 | itposting" data-gatsby-head="true"/><meta property="og:title" content="서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification" data-gatsby-head="true"/><meta name="twitter:title" content="서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-23 19:35" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 23, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>데이터와 컴퓨터 프로그램의 세계에서 머신 러닝이라는 개념은 어려운 문제 같을 수도 있어요. 복잡한 수학과 이해하기 어려운 개념이 가득한 것 같죠.</p>
<p>그래서 오늘은 여기서 멈추어서, 제 MLBasics 시리즈의 새로운 이슈를 통해 모든 것이 어떻게 작동하는지 기본적인 사항을 살펴보고 싶어요.</p>
<p>오늘의 안건은 서포트 벡터 머신을 이해하는 것이에요.</p>
<p>이 강력한 도구는 데이터를 명확한 범주로 분류하는 데 도움이 되지만...</p>
<div class="content-ad"></div>
<p>어떻게 동작하는 건가요?</p>
<p>Support Vector Machines 모델을 간단히 설명해 보겠습니다👇🏻</p>
<h1>Support Vector Machine이란?</h1>
<p>Support Vector Machine (SVM)은 두 가지 다른 클래스로 데이터 포인트를 가장 잘 분리하는 초평면을 찾으려는 지도 학습 알고리즘입니다.</p>
<div class="content-ad"></div>
<p>이 문제는 이를 수행할 수 있는 무한한 수의 초평면이 존재한다는 점이 어렵습니다. 그래서 SVM의 목표는 클래스를 최대 여백으로 가장 잘 분리하는 초평면을 식별하는 것입니다.</p>
<p><img src="/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png" alt="image"></p>
<h1>SVM의 주요 개념</h1>
<p>더 깊이 파고들기 전에, 몇 가지 핵심 용어를 이해해 보겠습니다:</p>
<div class="content-ad"></div>
<ul>
<li>Support Vectors(서포트 벡터): 이들은 초평면에 가장 가까운 데이터 포인트로, 초평면의 위치와 방향에 큰 영향을 미칩니다.</li>
<li>여백(Margin): 여백은 초평면과 각 클래스에서 가장 가까운 데이터 포인트 사이의 거리입니다. 더 큰 여백은 분류기의 일반화를 더 잘 시킬 것입니다.</li>
<li>초평면(Hyperplane): 2차원 공간에서 데이터를 두 부분으로 나누는 선입니다. 고차원에서는 평면이나 고차원의 유사 구조체입니다.</li>
</ul>
<p><img src="/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_1.png" alt="이미지"></p>
<h1>SVM이 작동하는 방식</h1>
<p>두 종류의 데이터 포인트가 있는 데이터셋을 상상해보세요.</p>
<div class="content-ad"></div>
<ul>
<li>파란색 🔵</li>
<li>노란색 🟨</li>
</ul>
<p>새 데이터 포인트를 파란색 또는 노란색 중 하나로 분류하고 싶습니다. 주요 과제는 두 클래스를 분리할 수 있는 다양한 하이퍼플레인이 존재한다는 것인데, 그런 다음 큰 질문이 있습니다:</p>
<p>어떻게 최적의 하이퍼플레인을 찾을까요?</p>
<p><img src="/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_2.png" alt="이미지"></p>
<div class="content-ad"></div>
<p>가장 좋은 초평면은 두 클래스로부터 최대 거리를 가지는 것입니다. 이는 가능한 다양한 초평면을 찾고 두 클래스로부터 최대 거리를 가지는 것을 선택함으로써 수행됩니다.</p>
<h1>SVM 뒤에 숨겨진 수학적 직관</h1>
<p>데이터를 분류하는 방법을 이해하기 위해 수학적 측면을 살펴보겠습니다.</p>
<p>점곱은 하나의 벡터를 다른 벡터에 따라 투영하는 것을 말합니다. 그래서 우리는 한 쪽의 점과 다른 쪽의 초평면이 어디에 있는지 결정하는 데 활용할 수 있습니다.</p>
<div class="content-ad"></div>
<p>임의의 점 X를 고려해 보면:</p>
<ul>
<li>만약 X⋅W ` c 이면 — 이것은 양성 샘플입니다.</li>
<li>만약 X⋅W ` c 이면 — 이것은 음성 샘플입니다.</li>
<li>만약 X⋅W = c 이면 — 이것은 결정 경계 상에 있습니다.</li>
</ul>
<p>쉽죠?</p>
<p>그러니까 조금 되감아보고 이 방정식들이 어디에서 왔는지 이해해 봅시다:</p>
<div class="content-ad"></div>
<h2>#1. 하이퍼플레인을 찾는 방법 결정</h2>
<p>우리가 “분리선”을 얻기 위해, 서포트 벡터와 하이퍼플레인 사이의 거리 d를 먼저 계산할 수 있습니다. 여유 공간은 하이퍼플레인으로부터 가장 가까운 서포트 벡터까지의 거리의 두 배이며, 이 여유 공간 내에는 어떤 점도 있어서는 안 됩니다.</p>
<h2>#2. 거리 “d” 투영</h2>
<p>거리 d는 두 서포트 벡터 사이의 차이를 하이퍼플레인의 법선 벡터 w의 방향으로 투영하면 얻을 수 있습니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_3.png" alt="image"></p>
<p>여러분 중 많은 분들이 여기에 도착한 방법을 모르실 것 같아요. 그래서 한 발 물러나서 이 함수가 처음부터 무엇을 의미하는지 더 잘 이해해보도록 해요.</p>
<p>A와 B라는 두 벡터가 있다고 상상해봅시다. 그들 사이에 θ도를 생성해요. 이 스칼라 곱을 사용하여 A가 B 위에 떨어지는 투영을 쉽게 찾을 수 있어요.</p>
<p>즉, A의 B에 대한 투영을 찾을 수 있어요. A와 B 벡터를 알면 다음 수식에서도 확인할 수 있듯이요.</p>
<div class="content-ad"></div>
<p>그래서 이제 우리가 이 기본 원리를 이해했으니, SVM 모델로 돌아가 봅시다. SVM에 동일한 수학적 개념을 적용할 수 있습니다. 여기서 A는 지원 벡터 머신으로 정의된 벡터이고 B는 우리가 분할 초평면의 법선 벡터입니다.</p>
<h2>#3. 제약 조건 정의하기</h2>
<p>이제 여백을 활용하여 제약 조건을 정의할 수 있습니다. 최대 여백 초평면이 (2D 예제에서) 선 방정식을 따라야 한다는 것을 알고 있습니다.</p>
<p>이것은 초평면에 놓인 것은 양수 값을 가질 것이며(양쪽 초평면에 해당), 그 아래에 있는 것은 음수 값을 가질 것입니다(음쪽 초평면에 해당).</p>
<div class="content-ad"></div>
<p>위 두 초평면 사이의 간격을 "마진"이라고 합니다.</p>
<h2>SVM의 마진</h2>
<p>마진은 SVM에서 중요한 개념으로, 초평면 주변에 데이터 포인트가 없는 버퍼 영역으로 작용합니다. 이 마진이 넓을수록 모델이 보이지 않는 데이터에 대해 일반화할 수 있으며, 과적합 가능성을 줄입니다.</p>
<p>양수 또는 음수로 점을 분류하기 위해 초평면과의 상대적 위치를 기반으로 결정 규칙을 설정합니다.</p>
<div class="content-ad"></div>
<ul>
<li>한 쪽에 있는 점들은 한 범주로 분류됩니다 (파란색 🔵)</li>
<li>다른 한 쪽에 있는 점들은 반대 범주에 속합니다 (노란색 🟨).</li>
</ul>
<p>마진을 최대화함으로써 SVM은 의사결정 경계를 최적으로 배치하여 가능한 높은 신뢰도로 클래스를 분리합니다.</p>
<p>그러면 어떻게 최대화할까요?</p>
<h1>최적화와 제약 사항</h1>
<div class="content-ad"></div>
<p>SVM은 여백을 최대화하기 위한 최적화 문제를 해결하는 것을 포함합니다. 이는 선택한 초평면이 각 클래스의 가장 가까운 데이터 포인트에서 충분한 거리를 유지하도록 하는 것을 의미합니다. 이를 서포트 벡터라고 합니다.</p>
<p>이미 이전에 발견한 선 방정식을 기반으로 한 분류 알고리즘이 있습니다. 그래서 출력을 다음과 같이 정의할 수 있습니다:</p>
<ul>
<li>+1 또는 🔵는 양쪽의 데이터를 나타냅니다.</li>
<li>-1 또는 🟨는 음쪽의 데이터를 나타냅니다.</li>
</ul>
<p><img src="/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_4.png" alt="이미지"></p>
<div class="content-ad"></div>
<p>하지만 여전히 w 벡터와 b 매개변수를 찾아야 합니다.</p>
<p>그래서... 어떻게 할까요?</p>
<p>마진 경계에 위치하는 서포트 벡터는 우리의 양의 및 음의 초평면 내에 포함되어 있기 때문에 다음 제약 조건을 만족합니다.</p>
<p>그래서 이를 쉽게 일반화할 수 있어요...</p>
<div class="content-ad"></div>
<h2>일반 제약 조건 방정식</h2>
<p>모든 데이터 포인트 (x, y)가 마진을 넘어가지 않도록 하기 위해, 모든 데이터 포인트에 대한 제약 조건은 다음과 같이 요약될 수 있습니다:</p>
<p><img src="/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_5.png" alt="equation"></p>
<p>그리고 수행할 단계가 하나 더 남았습니다...</p>
<div class="content-ad"></div>
<h2>최적화 목표</h2>
<p>이제 일반적인 제약 방정식을 가지고 있으므로, 벡터 w의 절대값을 최소화하면서 제약 조건을 충족시킬 수 있습니다.</p>
<p>이는 다음과 같이 수학적으로 정의될 수 있습니다:</p>
<p><img src="/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_6.png" alt="equation"></p>
<div class="content-ad"></div>
<p>이 최적화 문제를 해결함으로써, 클래스 간의 최상의 분리를 보장하는 최대 마진을 가지는 초평면을 정의하는 벡터 w와 b의 최적값을 찾을 수 있습니다.</p>
<h1>결론</h1>
<p>서포트 벡터 머신은 데이터 과학자의 무기 중 강력한 도구로, 이진 분류에 효과적인 방법을 제공합니다.</p>
<p>클래스 간의 간격을 최대화하는 데 초점을 맞추면, SVM은 새로운 데이터에 대해 잘 일반화되는 견고한 분류기를 생성하여, 오버피팅의 위험을 줄입니다.</p>
<div class="content-ad"></div>
<p>SVM의 수학적 기반은 최적 초평면의 식별을 보장하여 다양한 분류 작업에 신뢰할 수 있는 선택지로 만듭니다.</p>
<p>복잡한 데이터셋을 다루거나 모델 성능을 향상시키려는 경우, SVM에 대한 이해와 구현은 머신러닝 도구상자를 크게 향상시킬 수 있습니다.</p>
<p>MLBasics 이슈를 좋아하셨나요? 그렇다면 DataBites 뉴스레터를 구독하여 최신 소식을 받아보세요!</p>
<p>내용을 메일로 받아보실 수 있습니다!</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_7.png" alt="2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_7.png"></p>
<p>X, Threads, LinkedIn에서도 만나볼 수 있어요! 거기서는 머신러닝, SQL, Python, 데이터 시각화에 관한 일일 치트시트를 올려요.</p>
<p>다른 멋진 글도 여기 한번 확인해보세요! 😄</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"서포트 벡터 머신 간단 정리  이진 분류 쉽게 이해하기","description":"","date":"2024-06-23 19:35","slug":"2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification","content":"\n\n데이터와 컴퓨터 프로그램의 세계에서 머신 러닝이라는 개념은 어려운 문제 같을 수도 있어요. 복잡한 수학과 이해하기 어려운 개념이 가득한 것 같죠.\n\n그래서 오늘은 여기서 멈추어서, 제 MLBasics 시리즈의 새로운 이슈를 통해 모든 것이 어떻게 작동하는지 기본적인 사항을 살펴보고 싶어요.\n\n오늘의 안건은 서포트 벡터 머신을 이해하는 것이에요.\n\n이 강력한 도구는 데이터를 명확한 범주로 분류하는 데 도움이 되지만...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떻게 동작하는 건가요?\n\nSupport Vector Machines 모델을 간단히 설명해 보겠습니다👇🏻\n\n# Support Vector Machine이란?\n\nSupport Vector Machine (SVM)은 두 가지 다른 클래스로 데이터 포인트를 가장 잘 분리하는 초평면을 찾으려는 지도 학습 알고리즘입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 문제는 이를 수행할 수 있는 무한한 수의 초평면이 존재한다는 점이 어렵습니다. 그래서 SVM의 목표는 클래스를 최대 여백으로 가장 잘 분리하는 초평면을 식별하는 것입니다.\n\n![image](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png)\n\n# SVM의 주요 개념\n\n더 깊이 파고들기 전에, 몇 가지 핵심 용어를 이해해 보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Support Vectors(서포트 벡터): 이들은 초평면에 가장 가까운 데이터 포인트로, 초평면의 위치와 방향에 큰 영향을 미칩니다.\n- 여백(Margin): 여백은 초평면과 각 클래스에서 가장 가까운 데이터 포인트 사이의 거리입니다. 더 큰 여백은 분류기의 일반화를 더 잘 시킬 것입니다.\n- 초평면(Hyperplane): 2차원 공간에서 데이터를 두 부분으로 나누는 선입니다. 고차원에서는 평면이나 고차원의 유사 구조체입니다.\n\n![이미지](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_1.png)\n\n# SVM이 작동하는 방식\n\n두 종류의 데이터 포인트가 있는 데이터셋을 상상해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 파란색 🔵\n- 노란색 🟨\n\n새 데이터 포인트를 파란색 또는 노란색 중 하나로 분류하고 싶습니다. 주요 과제는 두 클래스를 분리할 수 있는 다양한 하이퍼플레인이 존재한다는 것인데, 그런 다음 큰 질문이 있습니다:\n\n어떻게 최적의 하이퍼플레인을 찾을까요?\n\n![이미지](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 좋은 초평면은 두 클래스로부터 최대 거리를 가지는 것입니다. 이는 가능한 다양한 초평면을 찾고 두 클래스로부터 최대 거리를 가지는 것을 선택함으로써 수행됩니다.\n\n# SVM 뒤에 숨겨진 수학적 직관\n\n데이터를 분류하는 방법을 이해하기 위해 수학적 측면을 살펴보겠습니다.\n\n점곱은 하나의 벡터를 다른 벡터에 따라 투영하는 것을 말합니다. 그래서 우리는 한 쪽의 점과 다른 쪽의 초평면이 어디에 있는지 결정하는 데 활용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n임의의 점 X를 고려해 보면:\n\n- 만약 X⋅W ` c 이면 — 이것은 양성 샘플입니다.\n- 만약 X⋅W ` c 이면 — 이것은 음성 샘플입니다.\n- 만약 X⋅W = c 이면 — 이것은 결정 경계 상에 있습니다.\n\n쉽죠?\n\n그러니까 조금 되감아보고 이 방정식들이 어디에서 왔는지 이해해 봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## #1. 하이퍼플레인을 찾는 방법 결정\n\n우리가 “분리선”을 얻기 위해, 서포트 벡터와 하이퍼플레인 사이의 거리 d를 먼저 계산할 수 있습니다. 여유 공간은 하이퍼플레인으로부터 가장 가까운 서포트 벡터까지의 거리의 두 배이며, 이 여유 공간 내에는 어떤 점도 있어서는 안 됩니다.\n\n## #2. 거리 “d” 투영\n\n거리 d는 두 서포트 벡터 사이의 차이를 하이퍼플레인의 법선 벡터 w의 방향으로 투영하면 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_3.png)\n\n여러분 중 많은 분들이 여기에 도착한 방법을 모르실 것 같아요. 그래서 한 발 물러나서 이 함수가 처음부터 무엇을 의미하는지 더 잘 이해해보도록 해요.\n\nA와 B라는 두 벡터가 있다고 상상해봅시다. 그들 사이에 θ도를 생성해요. 이 스칼라 곱을 사용하여 A가 B 위에 떨어지는 투영을 쉽게 찾을 수 있어요.\n\n즉, A의 B에 대한 투영을 찾을 수 있어요. A와 B 벡터를 알면 다음 수식에서도 확인할 수 있듯이요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서 이제 우리가 이 기본 원리를 이해했으니, SVM 모델로 돌아가 봅시다. SVM에 동일한 수학적 개념을 적용할 수 있습니다. 여기서 A는 지원 벡터 머신으로 정의된 벡터이고 B는 우리가 분할 초평면의 법선 벡터입니다.\n\n## #3. 제약 조건 정의하기\n\n이제 여백을 활용하여 제약 조건을 정의할 수 있습니다. 최대 여백 초평면이 (2D 예제에서) 선 방정식을 따라야 한다는 것을 알고 있습니다.\n\n이것은 초평면에 놓인 것은 양수 값을 가질 것이며(양쪽 초평면에 해당), 그 아래에 있는 것은 음수 값을 가질 것입니다(음쪽 초평면에 해당).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 두 초평면 사이의 간격을 \"마진\"이라고 합니다.\n\n## SVM의 마진\n\n마진은 SVM에서 중요한 개념으로, 초평면 주변에 데이터 포인트가 없는 버퍼 영역으로 작용합니다. 이 마진이 넓을수록 모델이 보이지 않는 데이터에 대해 일반화할 수 있으며, 과적합 가능성을 줄입니다.\n\n양수 또는 음수로 점을 분류하기 위해 초평면과의 상대적 위치를 기반으로 결정 규칙을 설정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 한 쪽에 있는 점들은 한 범주로 분류됩니다 (파란색 🔵)\n- 다른 한 쪽에 있는 점들은 반대 범주에 속합니다 (노란색 🟨).\n\n마진을 최대화함으로써 SVM은 의사결정 경계를 최적으로 배치하여 가능한 높은 신뢰도로 클래스를 분리합니다.\n\n그러면 어떻게 최대화할까요?\n\n# 최적화와 제약 사항\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSVM은 여백을 최대화하기 위한 최적화 문제를 해결하는 것을 포함합니다. 이는 선택한 초평면이 각 클래스의 가장 가까운 데이터 포인트에서 충분한 거리를 유지하도록 하는 것을 의미합니다. 이를 서포트 벡터라고 합니다.\n\n이미 이전에 발견한 선 방정식을 기반으로 한 분류 알고리즘이 있습니다. 그래서 출력을 다음과 같이 정의할 수 있습니다:\n\n- +1 또는 🔵는 양쪽의 데이터를 나타냅니다.\n- -1 또는 🟨는 음쪽의 데이터를 나타냅니다.\n\n![이미지](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 여전히 w 벡터와 b 매개변수를 찾아야 합니다.\n\n그래서... 어떻게 할까요?\n\n마진 경계에 위치하는 서포트 벡터는 우리의 양의 및 음의 초평면 내에 포함되어 있기 때문에 다음 제약 조건을 만족합니다.\n\n그래서 이를 쉽게 일반화할 수 있어요...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 일반 제약 조건 방정식\n\n모든 데이터 포인트 (x, y)가 마진을 넘어가지 않도록 하기 위해, 모든 데이터 포인트에 대한 제약 조건은 다음과 같이 요약될 수 있습니다:\n\n![equation](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_5.png)\n\n그리고 수행할 단계가 하나 더 남았습니다...\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 최적화 목표\n\n이제 일반적인 제약 방정식을 가지고 있으므로, 벡터 w의 절대값을 최소화하면서 제약 조건을 충족시킬 수 있습니다.\n\n이는 다음과 같이 수학적으로 정의될 수 있습니다:\n\n![equation](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 최적화 문제를 해결함으로써, 클래스 간의 최상의 분리를 보장하는 최대 마진을 가지는 초평면을 정의하는 벡터 w와 b의 최적값을 찾을 수 있습니다.\n\n# 결론\n\n서포트 벡터 머신은 데이터 과학자의 무기 중 강력한 도구로, 이진 분류에 효과적인 방법을 제공합니다.\n\n클래스 간의 간격을 최대화하는 데 초점을 맞추면, SVM은 새로운 데이터에 대해 잘 일반화되는 견고한 분류기를 생성하여, 오버피팅의 위험을 줄입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSVM의 수학적 기반은 최적 초평면의 식별을 보장하여 다양한 분류 작업에 신뢰할 수 있는 선택지로 만듭니다.\n\n복잡한 데이터셋을 다루거나 모델 성능을 향상시키려는 경우, SVM에 대한 이해와 구현은 머신러닝 도구상자를 크게 향상시킬 수 있습니다.\n\nMLBasics 이슈를 좋아하셨나요? 그렇다면 DataBites 뉴스레터를 구독하여 최신 소식을 받아보세요!\n\n내용을 메일로 받아보실 수 있습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_7.png](/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_7.png)\n\nX, Threads, LinkedIn에서도 만나볼 수 있어요! 거기서는 머신러닝, SQL, Python, 데이터 시각화에 관한 일일 치트시트를 올려요.\n\n다른 멋진 글도 여기 한번 확인해보세요! 😄","ogImage":{"url":"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png"},"coverImage":"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png","tag":["Tech"],"readingTime":6},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e데이터와 컴퓨터 프로그램의 세계에서 머신 러닝이라는 개념은 어려운 문제 같을 수도 있어요. 복잡한 수학과 이해하기 어려운 개념이 가득한 것 같죠.\u003c/p\u003e\n\u003cp\u003e그래서 오늘은 여기서 멈추어서, 제 MLBasics 시리즈의 새로운 이슈를 통해 모든 것이 어떻게 작동하는지 기본적인 사항을 살펴보고 싶어요.\u003c/p\u003e\n\u003cp\u003e오늘의 안건은 서포트 벡터 머신을 이해하는 것이에요.\u003c/p\u003e\n\u003cp\u003e이 강력한 도구는 데이터를 명확한 범주로 분류하는 데 도움이 되지만...\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e어떻게 동작하는 건가요?\u003c/p\u003e\n\u003cp\u003eSupport Vector Machines 모델을 간단히 설명해 보겠습니다👇🏻\u003c/p\u003e\n\u003ch1\u003eSupport Vector Machine이란?\u003c/h1\u003e\n\u003cp\u003eSupport Vector Machine (SVM)은 두 가지 다른 클래스로 데이터 포인트를 가장 잘 분리하는 초평면을 찾으려는 지도 학습 알고리즘입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 문제는 이를 수행할 수 있는 무한한 수의 초평면이 존재한다는 점이 어렵습니다. 그래서 SVM의 목표는 클래스를 최대 여백으로 가장 잘 분리하는 초평면을 식별하는 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_0.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch1\u003eSVM의 주요 개념\u003c/h1\u003e\n\u003cp\u003e더 깊이 파고들기 전에, 몇 가지 핵심 용어를 이해해 보겠습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eSupport Vectors(서포트 벡터): 이들은 초평면에 가장 가까운 데이터 포인트로, 초평면의 위치와 방향에 큰 영향을 미칩니다.\u003c/li\u003e\n\u003cli\u003e여백(Margin): 여백은 초평면과 각 클래스에서 가장 가까운 데이터 포인트 사이의 거리입니다. 더 큰 여백은 분류기의 일반화를 더 잘 시킬 것입니다.\u003c/li\u003e\n\u003cli\u003e초평면(Hyperplane): 2차원 공간에서 데이터를 두 부분으로 나누는 선입니다. 고차원에서는 평면이나 고차원의 유사 구조체입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch1\u003eSVM이 작동하는 방식\u003c/h1\u003e\n\u003cp\u003e두 종류의 데이터 포인트가 있는 데이터셋을 상상해보세요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e파란색 🔵\u003c/li\u003e\n\u003cli\u003e노란색 🟨\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e새 데이터 포인트를 파란색 또는 노란색 중 하나로 분류하고 싶습니다. 주요 과제는 두 클래스를 분리할 수 있는 다양한 하이퍼플레인이 존재한다는 것인데, 그런 다음 큰 질문이 있습니다:\u003c/p\u003e\n\u003cp\u003e어떻게 최적의 하이퍼플레인을 찾을까요?\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e가장 좋은 초평면은 두 클래스로부터 최대 거리를 가지는 것입니다. 이는 가능한 다양한 초평면을 찾고 두 클래스로부터 최대 거리를 가지는 것을 선택함으로써 수행됩니다.\u003c/p\u003e\n\u003ch1\u003eSVM 뒤에 숨겨진 수학적 직관\u003c/h1\u003e\n\u003cp\u003e데이터를 분류하는 방법을 이해하기 위해 수학적 측면을 살펴보겠습니다.\u003c/p\u003e\n\u003cp\u003e점곱은 하나의 벡터를 다른 벡터에 따라 투영하는 것을 말합니다. 그래서 우리는 한 쪽의 점과 다른 쪽의 초평면이 어디에 있는지 결정하는 데 활용할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e임의의 점 X를 고려해 보면:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e만약 X⋅W ` c 이면 — 이것은 양성 샘플입니다.\u003c/li\u003e\n\u003cli\u003e만약 X⋅W ` c 이면 — 이것은 음성 샘플입니다.\u003c/li\u003e\n\u003cli\u003e만약 X⋅W = c 이면 — 이것은 결정 경계 상에 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e쉽죠?\u003c/p\u003e\n\u003cp\u003e그러니까 조금 되감아보고 이 방정식들이 어디에서 왔는지 이해해 봅시다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e#1. 하이퍼플레인을 찾는 방법 결정\u003c/h2\u003e\n\u003cp\u003e우리가 “분리선”을 얻기 위해, 서포트 벡터와 하이퍼플레인 사이의 거리 d를 먼저 계산할 수 있습니다. 여유 공간은 하이퍼플레인으로부터 가장 가까운 서포트 벡터까지의 거리의 두 배이며, 이 여유 공간 내에는 어떤 점도 있어서는 안 됩니다.\u003c/p\u003e\n\u003ch2\u003e#2. 거리 “d” 투영\u003c/h2\u003e\n\u003cp\u003e거리 d는 두 서포트 벡터 사이의 차이를 하이퍼플레인의 법선 벡터 w의 방향으로 투영하면 얻을 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_3.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e여러분 중 많은 분들이 여기에 도착한 방법을 모르실 것 같아요. 그래서 한 발 물러나서 이 함수가 처음부터 무엇을 의미하는지 더 잘 이해해보도록 해요.\u003c/p\u003e\n\u003cp\u003eA와 B라는 두 벡터가 있다고 상상해봅시다. 그들 사이에 θ도를 생성해요. 이 스칼라 곱을 사용하여 A가 B 위에 떨어지는 투영을 쉽게 찾을 수 있어요.\u003c/p\u003e\n\u003cp\u003e즉, A의 B에 대한 투영을 찾을 수 있어요. A와 B 벡터를 알면 다음 수식에서도 확인할 수 있듯이요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그래서 이제 우리가 이 기본 원리를 이해했으니, SVM 모델로 돌아가 봅시다. SVM에 동일한 수학적 개념을 적용할 수 있습니다. 여기서 A는 지원 벡터 머신으로 정의된 벡터이고 B는 우리가 분할 초평면의 법선 벡터입니다.\u003c/p\u003e\n\u003ch2\u003e#3. 제약 조건 정의하기\u003c/h2\u003e\n\u003cp\u003e이제 여백을 활용하여 제약 조건을 정의할 수 있습니다. 최대 여백 초평면이 (2D 예제에서) 선 방정식을 따라야 한다는 것을 알고 있습니다.\u003c/p\u003e\n\u003cp\u003e이것은 초평면에 놓인 것은 양수 값을 가질 것이며(양쪽 초평면에 해당), 그 아래에 있는 것은 음수 값을 가질 것입니다(음쪽 초평면에 해당).\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e위 두 초평면 사이의 간격을 \"마진\"이라고 합니다.\u003c/p\u003e\n\u003ch2\u003eSVM의 마진\u003c/h2\u003e\n\u003cp\u003e마진은 SVM에서 중요한 개념으로, 초평면 주변에 데이터 포인트가 없는 버퍼 영역으로 작용합니다. 이 마진이 넓을수록 모델이 보이지 않는 데이터에 대해 일반화할 수 있으며, 과적합 가능성을 줄입니다.\u003c/p\u003e\n\u003cp\u003e양수 또는 음수로 점을 분류하기 위해 초평면과의 상대적 위치를 기반으로 결정 규칙을 설정합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e한 쪽에 있는 점들은 한 범주로 분류됩니다 (파란색 🔵)\u003c/li\u003e\n\u003cli\u003e다른 한 쪽에 있는 점들은 반대 범주에 속합니다 (노란색 🟨).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e마진을 최대화함으로써 SVM은 의사결정 경계를 최적으로 배치하여 가능한 높은 신뢰도로 클래스를 분리합니다.\u003c/p\u003e\n\u003cp\u003e그러면 어떻게 최대화할까요?\u003c/p\u003e\n\u003ch1\u003e최적화와 제약 사항\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eSVM은 여백을 최대화하기 위한 최적화 문제를 해결하는 것을 포함합니다. 이는 선택한 초평면이 각 클래스의 가장 가까운 데이터 포인트에서 충분한 거리를 유지하도록 하는 것을 의미합니다. 이를 서포트 벡터라고 합니다.\u003c/p\u003e\n\u003cp\u003e이미 이전에 발견한 선 방정식을 기반으로 한 분류 알고리즘이 있습니다. 그래서 출력을 다음과 같이 정의할 수 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e+1 또는 🔵는 양쪽의 데이터를 나타냅니다.\u003c/li\u003e\n\u003cli\u003e-1 또는 🟨는 음쪽의 데이터를 나타냅니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_4.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e하지만 여전히 w 벡터와 b 매개변수를 찾아야 합니다.\u003c/p\u003e\n\u003cp\u003e그래서... 어떻게 할까요?\u003c/p\u003e\n\u003cp\u003e마진 경계에 위치하는 서포트 벡터는 우리의 양의 및 음의 초평면 내에 포함되어 있기 때문에 다음 제약 조건을 만족합니다.\u003c/p\u003e\n\u003cp\u003e그래서 이를 쉽게 일반화할 수 있어요...\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e일반 제약 조건 방정식\u003c/h2\u003e\n\u003cp\u003e모든 데이터 포인트 (x, y)가 마진을 넘어가지 않도록 하기 위해, 모든 데이터 포인트에 대한 제약 조건은 다음과 같이 요약될 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_5.png\" alt=\"equation\"\u003e\u003c/p\u003e\n\u003cp\u003e그리고 수행할 단계가 하나 더 남았습니다...\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e최적화 목표\u003c/h2\u003e\n\u003cp\u003e이제 일반적인 제약 방정식을 가지고 있으므로, 벡터 w의 절대값을 최소화하면서 제약 조건을 충족시킬 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이는 다음과 같이 수학적으로 정의될 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_6.png\" alt=\"equation\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 최적화 문제를 해결함으로써, 클래스 간의 최상의 분리를 보장하는 최대 마진을 가지는 초평면을 정의하는 벡터 w와 b의 최적값을 찾을 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e서포트 벡터 머신은 데이터 과학자의 무기 중 강력한 도구로, 이진 분류에 효과적인 방법을 제공합니다.\u003c/p\u003e\n\u003cp\u003e클래스 간의 간격을 최대화하는 데 초점을 맞추면, SVM은 새로운 데이터에 대해 잘 일반화되는 견고한 분류기를 생성하여, 오버피팅의 위험을 줄입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eSVM의 수학적 기반은 최적 초평면의 식별을 보장하여 다양한 분류 작업에 신뢰할 수 있는 선택지로 만듭니다.\u003c/p\u003e\n\u003cp\u003e복잡한 데이터셋을 다루거나 모델 성능을 향상시키려는 경우, SVM에 대한 이해와 구현은 머신러닝 도구상자를 크게 향상시킬 수 있습니다.\u003c/p\u003e\n\u003cp\u003eMLBasics 이슈를 좋아하셨나요? 그렇다면 DataBites 뉴스레터를 구독하여 최신 소식을 받아보세요!\u003c/p\u003e\n\u003cp\u003e내용을 메일로 받아보실 수 있습니다!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_7.png\" alt=\"2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification_7.png\"\u003e\u003c/p\u003e\n\u003cp\u003eX, Threads, LinkedIn에서도 만나볼 수 있어요! 거기서는 머신러닝, SQL, Python, 데이터 시각화에 관한 일일 치트시트를 올려요.\u003c/p\u003e\n\u003cp\u003e다른 멋진 글도 여기 한번 확인해보세요! 😄\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-23-SimplifyingSupportVectorMachinesAConciseIntroductiontoBinaryClassification"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>