<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>벡터 DB를 사용하여 실시간 뉴스 검색 엔진을 구축하는 방법 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="벡터 DB를 사용하여 실시간 뉴스 검색 엔진을 구축하는 방법 | itposting" data-gatsby-head="true"/><meta property="og:title" content="벡터 DB를 사용하여 실시간 뉴스 검색 엔진을 구축하는 방법 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs" data-gatsby-head="true"/><meta name="twitter:title" content="벡터 DB를 사용하여 실시간 뉴스 검색 엔진을 구축하는 방법 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 16:12" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_buildManifest.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">벡터 DB를 사용하여 실시간 뉴스 검색 엔진을 구축하는 방법</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="벡터 DB를 사용하여 실시간 뉴스 검색 엔진을 구축하는 방법" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">28<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>아파치 카프카, 바이트왁스, 그리고 업스태시 벡터 데이터베이스를 활용한 라이브 뉴스 집계 스트리밍 파이프라인 구현을 위한 실용적인 안내서입니다.</p>
<p>earthweb.com에서 실시한 연구에 따르면, 매일 온라인 및 오프라인에서 유입되는 뉴스 기사는 200-300만 건 사이에 있다고 합니다!</p>
<p>모든 방향에서 우리에게 쏟아지는 뉴스로 인해 정신없을 때가 많습니다. 그래서 우리는 실제로 관심 있는 뉴스를 빠르게 받아볼 수 있는 짧고 빠른 방법을 찾고 있습니다.</p>
<p>본 문서에서는 이러한 문제를 해결하고자 합니다! 좀 더 구체적으로, 여러 출처로부터 데이터를 수집하고 해당 정보 채널을 당신의 관심사에 맞는 종단점으로 좁힐 수 있는 시스템을 구축할 것입니다 — 뉴스 검색 엔진입니다!</p>
<div class="content-ad"></div>
<p>이론에 대해서만 이야기하거나 이러한 시스템을 구현하는 방법을 알려주는 것이 아니라, 우리는 단계별로 설명하고 보여줄 거예요!</p>
<p>시작하기 전에, 이 기사에서 제공하는 모든 것은 Decoding ML Articles GitHub 저장소에서 완전한 코드 지원을 받습니다.</p>
<h1>목차</h1>
<ul>
<li>아키텍처 개요</li>
<li>도구 고려 사항</li>
<li>전제 조건
3.1 새로운 Upstash Kafka 클러스터 생성
3.2 새로운 Upstash Vector 인덱스 생성
3.3 2개의 라이브 뉴스 API에 등록
3.4 설치</li>
<li>데이터 수집
4.1 기사 가져오기 관리자
4.2 Kafka 메시지 제작
4.3 Pydantic을 사용한 데이터 교환 모델
4.4 KafkaProducers 실행</li>
<li>수집 파이프라인
5.1 Kafka에서 메시지 수신
5.2 Bytewax 데이터플로 구현
5.3 기사 정제, 형식 지정, 청크화, 삽입
5.4 벡터 작성 및 VectorDB에 업서트</li>
<li>파이프라인 시작</li>
<li>사용자 인터페이스</li>
<li>결론</li>
<li>참고문헌</li>
</ul>
<div class="content-ad"></div>
<h1>아키텍처 개요</h1>
<p><img src="/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_0.png" alt="이미지"></p>
<p>요약하자면, 우리는 뉴스 API에서 뉴스 기사를 가져와서 생생한 시스템을 구축할 것입니다. 가져온 데이터를 정의된 형식으로 파싱하고 포맷팅한 다음 첫 번째 단계로 Kafka 토픽에 메시지를 직렬화하고 스트리밍할 것입니다.
두 번째 단계에서는 Bytewax를 사용하여 Kafka 토픽에서 메시지를 더 청소, 파싱, 청크, 임베드, 벡터를 업서팅하여 벡터 데이터베이스로 보내고, 데이터베이스와 상호 작용할 수 있는 UI로 마무리됩니다.</p>
<h1>툴 고려 사항</h1>
<div class="content-ad"></div>
<p>올바른 도구를 선택하는 것이 고성능, 확장성, 및 구현 용이성을 달성하는 핵심이었습니다. 프로젝트 전체에서 사용된 도구를 살펴보겠습니다:</p>
<ul>
<li>Upstash Serverless Kafka: 인프라 관리에 대해 걱정할 필요 없이 강력하고 확장 가능한 이벤트 스트리밍을 위해 사용됩니다.</li>
<li>Python 스레딩: 여러 뉴스 API에서 동시에 데이터를 가져오면서 스레드 안전한 Kafka Producer 인스턴스를 공유하여 메모리 풋프린트와 성능을 최적화합니다.</li>
<li>Pydantic 모델: 일관된 및 유효한 데이터 교환 구조를 보장하기 위해 사용됩니다.</li>
<li>Bytewax: 스트리밍 데이터를 처리하고 변환하는 데 간편하고 빠른 속도를 제공하기 때문에 사용됩니다.</li>
<li>Upstash Vector Database: Serverless로 구성이 쉽고 Python, JS, 및 GO 내에서 쉽게 통합됩니다. UI 콘솔 대시보드에서 풍부한 탐색 옵션과 실시간 상태 메트릭을 제공하는 것이 큰 장점입니다.</li>
</ul>
<p>하드웨어 요구 사항에 따르면 GPU는 필요하지 않습니다.</p>
<p>비용은 얼마입니까? — 무료입니다.
이 안내서는 무료 티어 플랜만 사용하도록 설정했으므로 사용한 플랫폼에 대해 지불할 필요가 없습니다!</p>
<div class="content-ad"></div>
<h1>준비 사항</h1>
<p>어떠한 구현을 하기 전에, 각 서비스에 접근할 수 있는지 확인해야 합니다. 따라서 다음을 설정해야 합니다:</p>
<ul>
<li>새로운 Upstash Kafka 클러스터</li>
<li>새로운 Upstash Vector Index</li>
<li>뉴스 API 등록</li>
<li>환경 설치</li>
</ul>
<p>처음 시작할 때는 약 5분 정도 걸립니다. 함께 해보세요!</p>
<div class="content-ad"></div>
<h2>새로운 Upstash Kafka 클러스터 생성</h2>
<p>먼저 Upstash에 등록해야 합니다. 로그인 후에 다음과 같은 대시보드가 나타납니다:</p>
<p><img src="/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_1.png" alt="대시보드 이미지"></p>
<p>다음으로 상단 바에서 Kafka를 선택하고 + 클러스터 생성 버튼을 클릭하여 새 클러스터를 만들어야 합니다. 클릭하면 다음 모달이 나타납니다:</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_2.png">
<p>클러스터에 이름을 지정하고 본인의 위치와 가장 가까운 지역을 선택한 후, 클러스터 생성을 클릭하여 완료하세요. 완료되면 새로운 Kafka 클러스터가 아래에 표시됩니다. 새로운 Kafka 클러스터를 선택하고 아래 화면으로 이동하게 됩니다:</p>
<img src="/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_3.png">
<p>이 뷰에서 주요 구성 요소를 살펴보겠습니다:</p>
<div class="content-ad"></div>
<ul>
<li>상세 정보: 클러스터 개요 및 Upstash가 제공하는 기능을 보여줍니다.</li>
<li>사용량: 생성/소비된 메시지 수, 비용 영향 등을 보여주는 차트입니다.</li>
<li>주제: 이 탭에서는 Kafka 주제를 만들고 세부 정보를 모니터링할 수 있습니다.</li>
</ul>
<p>클러스터를 생성한 다음 해야 할 다음 단계는 메시지를 생성(보내기)하고 소비(받기)할 수 있는 주제를 정의하는 것입니다.</p>
<p>주제 탭 아래에서 "주제 생성"을 선택하면 다음과 같은 화면이 나타납니다:</p>
<img src="/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_4.png">
<div class="content-ad"></div>
<p>주제 이름을 지정하고, 나머지는 기본 상태로 두어서 Create를 클릭하세요.</p>
<p>카프카 클러스터를 성공적으로 생성했습니다. 이제 클러스터에 연결하는 데 도움이 되는 환경 변수를 복사하고 설정해야 합니다. 이를 위해 클러스터 대시보드로 이동하여 세부 정보 탭에서 엔드포인트, 사용자 이름 및 비밀번호를 복사하여 .env 파일에 붙여넣으세요.
그 후, Topics로 이동하여 카프카 토픽 이름을 복사하세요.</p>
<p>지금까지 .env 파일이어야 하는 모습은 다음과 같습니다:</p>
<pre><code class="hljs language-js"><span class="hljs-variable constant_">UPSTASH_KAFKA_UNAME</span>=<span class="hljs-string">"[사용자 이름]"</span>
<span class="hljs-variable constant_">UPSTASH_KAFKA_PASS</span>=<span class="hljs-string">"[비밀번호]"</span>
<span class="hljs-variable constant_">UPSTASH_KAFKA_ENDPOINT</span>=<span class="hljs-string">"[엔드포인트]"</span>
<span class="hljs-variable constant_">UPSTASH_KAFKA_TOPIC</span>=<span class="hljs-string">"[토픽 이름]"</span>
</code></pre>
<div class="content-ad"></div>
<h2>새 Upstash Vector 색인 만들기</h2>
<p>이제 새로운 Vector 데이터베이스를 만들어 보겠습니다. 이를 위해 대시보드에서 Vector를 선택하고 + Index 작성을 클릭하세요. 그러면 다음 뷰로 이동됩니다:</p>
<p><img src="/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_5.png" alt="이미지"></p>
<p>벡터 데이터베이스에 이름을 할당하고, 위치에 가장 가까운 지역을 선택한 다음 Embedding을 생성할 때 사용할 모델로 sentence-transformers/all-MiniLM-L6-v2을 선택하세요. 뉴스 기사의 임베딩을 생성할 때 사용할 모델과 벡터 간 거리 비교에 코사인 유사도 메트릭을 사용할 것입니다.</p>
<div class="content-ad"></div>
<p>새로운 Vector Index를 만든 후에는 Kafka Cluster와 동일한 작업 흐름을 따를 수 있습니다. Index Name, Endpoint, Token을 복사하고 .env 파일에 붙여넣기하세요.</p>
<p>현재 .env 파일은 다음과 같이 보여야 합니다:</p>
<pre><code class="hljs language-js"><span class="hljs-variable constant_">UPSTASH_KAFKA_UNAME</span>=<span class="hljs-string">"[여기에 사용자명 입력]"</span>
<span class="hljs-variable constant_">UPSTASH_KAFKA_PASS</span>=<span class="hljs-string">"[여기에 비밀번호 입력]"</span>
<span class="hljs-variable constant_">UPSTASH_KAFKA_ENDPOINT</span>=<span class="hljs-string">"[여기에 엔드포인트 입력]"</span>
<span class="hljs-variable constant_">UPSTASH_KAFKA_TOPIC</span>=<span class="hljs-string">"[여기에 토픽 이름 입력]"</span>

<span class="hljs-variable constant_">UPSTASH_VECTOR_ENDPOINT</span>=<span class="hljs-string">"[Vector 엔드포인트 입력]"</span>
<span class="hljs-variable constant_">UPSTASH_VECTOR_TOPIC</span>=<span class="hljs-string">"[Vector 이름 입력]"</span>
<span class="hljs-variable constant_">UPSTASH_VECTOR_KEY</span>=<span class="hljs-string">"[Vector 토큰 입력]"</span>
</code></pre>
<h2>뉴스 API에 등록하기</h2>
<div class="content-ad"></div>
<p>다음 APIs를 사용하여 기사를 가져올 예정입니다:</p>
<ul>
<li>🔗 NewsAPI</li>
</ul>
<p>하루에 100번의 API 호출을 할 수 있는 무료 개발자 플랜을 제공합니다.</p>
<ol start="2">
<li>🔗 NewsData</li>
</ol>
<div class="content-ad"></div>
<p>무료 요금제가 제공되며 하루에 200개의 크레딧을 받습니다. 각 크레딧은 10개의 기사와 동일하며, 이는 하루에 총 2000개의 기사를 가져올 수 있다는 것을 의미합니다.</p>
<p>저희 현재의 사용 사례에는 이 API들이 충분한 기능을 제공하여 구축 중인 뉴스 검색 엔진을 구현하고 유효성을 검사할 수 있습니다. 동시에 기존 워크플로우가 동일하게 유지되므로 개선 및 확장할 여지도 남겨두고 있습니다.
무료 요금제에 따른 유일한 제약은 타임드-배치 페치를 수행할 수 없다는 것입니다. 즉, 이 API들을 쿼리할 때 from_date, to_date를 사용할 수 없습니다. 하지만 이는 문제가 되지 않습니다.
대신 페치 호출 간의 대기 시간을 이용하여 이 동작을 모방할 예정입니다.</p>
<p>다음 단계는 두 플랫폼에 등록하는 것입니다 — 걱정 마세요, 가능한 간단합니다.</p>
<ul>
<li>NewsAPI에 등록한 후, /account로 이동하여 API_KEY 필드를 확인한 후 이를 .env 파일의 NEWSAPI_KEY에 복사하여 붙여넣으십시오.</li>
<li>NewsData에 등록한 후, /api-key로 이동하여 API KEY를 확인한 후 이를 .env 파일의 NEWSDATAIO_KEY에 복사하여 붙여넣으십시오.</li>
</ul>
<div class="content-ad"></div>
<p>지루한 부분은 끝났습니다. 이제 이러한 API에 액세스할 수 있고, 기사를 가져올 수 있습니다. 각 API에서 페이로드가 어떻게 보이는지 살펴봅시다:</p>
<p><img src="/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_6.png" alt="image"></p>
<h2>사전 준비 조치 요약</h2>
<p>Kafka 클러스터를 생성하고, 벡터 인덱스를 생성하고, 뉴스 API에 등록하는 이 3단계를 모두 마친 후에 .env 파일은 다음과 같은 모습이어야 합니다:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-variable constant_">UPSTASH_KAFKA_UNAME</span>=<span class="hljs-string">"[여기에 사용자 이름 입력]"</span>
<span class="hljs-variable constant_">UPSTASH_KAFKA_PASS</span>=<span class="hljs-string">"[여기에 암호 입력]"</span>
<span class="hljs-variable constant_">UPSTASH_KAFKA_ENDPOINT</span>=<span class="hljs-string">"[여기에 엔드포인트 입력]"</span>
<span class="hljs-variable constant_">UPSTASH_KAFKA_TOPIC</span>=<span class="hljs-string">"[토픽 이름 입력]"</span>

<span class="hljs-variable constant_">UPSTASH_VECTOR_ENDPOINT</span>=<span class="hljs-string">"[벡터 엔드포인트 입력]"</span>
<span class="hljs-variable constant_">UPSTASH_VECTOR_TOPIC</span>=<span class="hljs-string">"[벡터 이름 입력]"</span>
<span class="hljs-variable constant_">UPSTASH_VECTOR_KEY</span>=<span class="hljs-string">"[벡터 토큰 입력]"</span>

<span class="hljs-variable constant_">NEWSAPI_KEY</span>=<span class="hljs-string">"[NEWSAPI 키 입력]"</span>
<span class="hljs-variable constant_">NEWSDATAIO_KEY</span>=<span class="hljs-string">"[NEWSDATA 키 입력]"</span>
<span class="hljs-variable constant_">NEWS_TOPIC</span> = <span class="hljs-string">"news"</span> # 이것은 가져올 기사의 카테고리입니다

다음 단계는 구현 세부 정보에 들어가기 전에 환경과 필수 패키지를 설치하는 것입니다.
다음은 <span class="hljs-title class_">Makefile</span> 설치 단계의 모습입니다:

# <span class="hljs-title class_">Makefile</span>
...
<span class="hljs-attr">install</span>:
 @echo <span class="hljs-string">"$(GREEN) [CONDA] [$(ENV_NAME)] 파이썬 환경 생성 $(RESET)"</span>
 conda create --name $(<span class="hljs-variable constant_">ENV_NAME</span>) python=<span class="hljs-number">3.9</span> -y
 @echo <span class="hljs-string">"환경 활성화 중..."</span>
 @bash -c <span class="hljs-string">"source $$(conda info --base)/etc/profile.d/conda.sh &#x26;&#x26; conda activate $(ENV_NAME) \
   &#x26;&#x26; pip install poetry \
   poetry env use $(which python)"</span>
 @echo <span class="hljs-string">"패키지 설치 중"</span>
 @echo <span class="hljs-string">"pyproject.toml 위치로 변경 중..."</span>
 @bash -c <span class="hljs-string">" PYTHON_KEYRING_BACKEND=keyring.backends.fail.Keyring poetry install"</span>
...

환경을 준비하려면 make install을 실행하세요.

&#x3C;div <span class="hljs-keyword">class</span>=<span class="hljs-string">"content-ad"</span>>&#x3C;/div>

이제 이 소스로부터 기사를 가져오는 핸들러 구현을 조사해 봅시다.

# 데이터 수집

이 모듈의 목적은 두 <span class="hljs-variable constant_">API</span>를 쿼리하는 기능을 캡슐화하고, 페이로드를 구문 분석하여 두 페이로드에 모두 존재하는 속성을 사용하여 공통 문서 형식으로 포매팅하고, 클러스터로 메시지를 보내기 위해 공유 <span class="hljs-title class_">KafkaProducer</span> 인스턴스를 사용하는 것입니다.

자세히 살펴볼 내용은 다음 하위 모듈들입니다:

<span class="xml"><span class="hljs-tag">&#x3C;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"content-ad"</span>></span><span class="hljs-tag">&#x3C;/<span class="hljs-name">div</span>></span></span>

- <span class="hljs-title class_">Articles</span> <span class="hljs-title class_">Fetching</span> <span class="hljs-title class_">Manager</span> <span class="hljs-title class_">Class</span>
- 카프카 클러스터로 메시지를 보내는 방법
- <span class="hljs-title class_">Pydantic</span> 데이터 모델
- 파이프라인 실행

## <span class="hljs-title class_">Articles</span> <span class="hljs-title class_">Fetching</span> <span class="hljs-title class_">Manager</span> <span class="hljs-title class_">Class</span>

구현 내용에 대해 알아보겠습니다:

<span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">import</span> functools
<span class="hljs-keyword">import</span> logging
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-title class_">Callable</span>, <span class="hljs-title class_">Dict</span>, <span class="hljs-title class_">List</span>

<span class="hljs-keyword">from</span> newsapi <span class="hljs-keyword">import</span> <span class="hljs-title class_">NewsApiClient</span>
<span class="hljs-keyword">from</span> newsdataapi <span class="hljs-keyword">import</span> <span class="hljs-title class_">NewsDataApiClient</span>
<span class="hljs-keyword">from</span> pydantic <span class="hljs-keyword">import</span> <span class="hljs-title class_">ValidationError</span>

<span class="hljs-keyword">from</span> models <span class="hljs-keyword">import</span> <span class="hljs-title class_">NewsAPIModel</span>, <span class="hljs-title class_">NewsDataIOModel</span>
<span class="hljs-keyword">from</span> settings <span class="hljs-keyword">import</span> settings

logging.<span class="hljs-title function_">basicConfig</span>(level=logging.<span class="hljs-property">INFO</span>)
logger = logging.<span class="hljs-title function_">getLogger</span>(__name__)
logger.<span class="hljs-title function_">setLevel</span>(logging.<span class="hljs-property">DEBUG</span>)


def <span class="hljs-title function_">handle_article_fetching</span>(<span class="hljs-attr">func</span>: <span class="hljs-title class_">Callable</span>) -> <span class="hljs-title class_">Callable</span>:
    <span class="hljs-string">""</span><span class="hljs-string">"
    뉴스 기사 가져오기 기능에 대한 예외 처리를 담당하는 데코레이터입니다.

    이 데코레이터는 기사 가져오기 기능을 감싸서 발생하는 예외를 catch하고 로깅합니다.
    오류가 발생하면 오류를 기록하고 빈 목록을 반환합니다.

    Args:
        func (Callable): 감쌀 기사 가져오기 함수.

    Returns:
        Callable: 감싼 함수.
    "</span><span class="hljs-string">""</span>

    @functools.<span class="hljs-title function_">wraps</span>(func)
    def <span class="hljs-title function_">wrapper</span>(*args, **kwargs):
        <span class="hljs-attr">try</span>:
            <span class="hljs-keyword">return</span> <span class="hljs-title function_">func</span>(*args, **kwargs)
        except <span class="hljs-title class_">ValidationError</span> <span class="hljs-keyword">as</span> <span class="hljs-attr">e</span>:
            logger.<span class="hljs-title function_">error</span>(f<span class="hljs-string">"기사 처리 중 유효성 검사 오류 발생: {e}"</span>)
        except <span class="hljs-title class_">Exception</span> <span class="hljs-keyword">as</span> <span class="hljs-attr">e</span>:
            logger.<span class="hljs-title function_">error</span>(f<span class="hljs-string">"소스로부터 데이터를 가져오는 도중 오류 발생: {e}"</span>)
            logger.<span class="hljs-title function_">exception</span>(e)
        <span class="hljs-keyword">return</span> []

    <span class="hljs-keyword">return</span> wrapper

<span class="hljs-keyword">class</span> <span class="hljs-title class_">NewsFetcher</span>:
    <span class="hljs-string">""</span><span class="hljs-string">"
    다양한 API에서 뉴스 기사를 가져오는 클래스입니다.

    속성:
        _newsapi (NewsApiClient): NewsAPI 클라이언트.
        _newsdataapi (NewsDataApiClient): NewsDataAPI 클라이언트.

    메서드:
        fetch_from_newsapi(): NewsAPI로부터 기사 가져오기.
        fetch_from_newsdataapi(): NewsDataAPI로부터 기사 가져오기.
        sources: 호출 가능한 가져오기 함수 목록을 반환합니다.
    "</span><span class="hljs-string">""</span>

    def <span class="hljs-title function_">__init__</span>(self):
        self.<span class="hljs-property">_newsapi</span> = <span class="hljs-title class_">NewsApiClient</span>(api_key=settings.<span class="hljs-property">NEWSAPI_KEY</span>)
        self.<span class="hljs-property">_newsdataapi</span> = <span class="hljs-title class_">NewsDataApiClient</span>(apikey=settings.<span class="hljs-property">NEWSDATAIO_KEY</span>)

    @handle_article_fetching
    def <span class="hljs-title function_">fetch_from_newsapi</span>(self) -> <span class="hljs-title class_">List</span>[<span class="hljs-title class_">Dict</span>]:
        <span class="hljs-string">""</span><span class="hljs-string">"NewsAPI에서 상위 뉴스 가져오기."</span><span class="hljs-string">""</span>
        response = self.<span class="hljs-property">_newsapi</span>.<span class="hljs-title function_">get_everything</span>(
            q=settings.<span class="hljs-property">NEWS_TOPIC</span>,
            language=<span class="hljs-string">"en"</span>,
            page=settings.<span class="hljs-property">ARTICLES_BATCH_SIZE</span>,
            page_size=settings.<span class="hljs-property">ARTICLES_BATCH_SIZE</span>,
        )
        <span class="hljs-keyword">return</span> [
            <span class="hljs-title class_">NewsAPIModel</span>(**article).<span class="hljs-title function_">to_common</span>()
            <span class="hljs-keyword">for</span> article <span class="hljs-keyword">in</span> response.<span class="hljs-title function_">get</span>(<span class="hljs-string">"articles"</span>, [])
        ]

    @handle_article_fetching
    def <span class="hljs-title function_">fetch_from_newsdataapi</span>(self) -> <span class="hljs-title class_">List</span>[<span class="hljs-title class_">Dict</span>]:
        <span class="hljs-string">""</span><span class="hljs-string">"NewsDataAPI에서 뉴스 데이터 가져오기."</span><span class="hljs-string">""</span>
        response = self.<span class="hljs-property">_newsdataapi</span>.<span class="hljs-title function_">news_api</span>(
            q=settings.<span class="hljs-property">NEWS_TOPIC</span>,
            language=<span class="hljs-string">"en"</span>,
            size=settings.<span class="hljs-property">ARTICLES_BATCH_SIZE</span>,
        )
        <span class="hljs-keyword">return</span> [
            <span class="hljs-title class_">NewsDataIOModel</span>(**article).<span class="hljs-title function_">to_common</span>()
            <span class="hljs-keyword">for</span> article <span class="hljs-keyword">in</span> response.<span class="hljs-title function_">get</span>(<span class="hljs-string">"results"</span>, [])
        ]

    @property
    def <span class="hljs-title function_">sources</span>(self) -> <span class="hljs-title class_">List</span>[callable]:
        <span class="hljs-string">""</span><span class="hljs-string">"뉴스 가져오기 함수 목록입니다."</span><span class="hljs-string">""</span>
        <span class="hljs-keyword">return</span> [self.<span class="hljs-property">fetch_from_newsapi</span>, self.<span class="hljs-property">fetch_from_newsdataapi</span>]

&#x3C;div <span class="hljs-keyword">class</span>=<span class="hljs-string">"content-ad"</span>>&#x3C;/div>

이 구현에서 고려해야 할 몇 가지 중요 사항이 있습니다:

- <span class="hljs-title class_">NewsAPIModel</span>과 <span class="hljs-title class_">NewsDataIOModel</span>은 특정 페이로드 형식에 익숙한 <span class="hljs-title class_">Pydantic</span> 모델입니다.
- 우리는 handle_article_fetching 데코레이터를 사용하여 원시 페이로드를 <span class="hljs-title class_">Pydantic</span> 모델로 변환할 때 유효성 오류나 더 넓은 예외를 잡습니다.
- 우리에게는 <span class="hljs-variable constant_">API</span>를 쿼리하는 callable 메서드를 반환하는 sources라는 속성이 있습니다. 이것은 데이터 수집 모듈 내에서 사용될 것이며 멀티 프로듀서 스레드를 생성하여 <span class="hljs-title class_">Kafka</span> 클러스터로 메시지를 전송합니다. 다음에 이어서 설명하겠습니다.

## <span class="hljs-title class_">Kafka</span> 메시지 생성

다음에 우리가 구현할 작업 흐름입니다:

<span class="xml"><span class="hljs-tag">&#x3C;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"content-ad"</span>></span><span class="hljs-tag">&#x3C;/<span class="hljs-name">div</span>></span></span>

<span class="xml"><span class="hljs-tag">&#x3C;<span class="hljs-name">img</span> <span class="hljs-attr">src</span>=<span class="hljs-string">"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_7.png"</span> /></span></span>

여기서 중요한 포인트들입니다:

- <span class="hljs-variable constant_">API</span>에서 가져오는 작업에 별도 스레드를 사용합니다.
- 메시지를 보내기 위해 공통 카프카 프로듀서 인스턴스를 공유합니다.
- 데이터 교환을 보증하기 위해 <span class="hljs-title class_">Pydantic</span> 모델을 사용합니다.

기사를 가져오는 데 별도 스레드를 사용하고, 클러스터로 메시지를 보내기 위해 단일 카프카 프로듀서 인스턴스를 사용하는 것이 우리의 사용 사례에서 권장되는 방법입니다. 그 이유는 다음과 같습니다:

<span class="xml"><span class="hljs-tag">&#x3C;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"content-ad"</span>></span><span class="hljs-tag">&#x3C;/<span class="hljs-name">div</span>></span></span>

- 효율성 및 성능: <span class="hljs-title class_">KafkaProducer</span>는 스레드 안전합니다. 새 인스턴스를 만드는 것은 네트워크 연결과 일부 설정이 필요합니다. 여러 스레드 간에 하나의 단일 인스턴스를 공유하면 이러한 작업과 관련된 오버헤드를 줄일 수 있습니다.
- 처리량: 단일 프로듀서 인스턴스는 메시지를 <span class="hljs-title class_">Kafka</span> 클러스터로 보내기 전에 메시지를 일괄 처리합니다.
- 자원: 사용 사례에 완전히 적용되지는 않지만, 우리는 오직 <span class="hljs-number">2</span>개의 프로듀서 스레드만 가지고 있기 때문에 인스턴스 수를 제한함으로써 시스템 자원 이용률을 최적화할 수 있습니다.

여기 <span class="hljs-title class_">Kafka</span>로 메시지 처리를 담당하는 주요 기능이 있습니다:

def <span class="hljs-title function_">run</span>(self) -> <span class="hljs-title class_">NoReturn</span>:
        <span class="hljs-string">""</span><span class="hljs-string">"지속적으로 Kafka 주제로 메시지를 가져와 보냅니다."</span><span class="hljs-string">""</span>
        <span class="hljs-keyword">while</span> self.<span class="hljs-property">running</span>.<span class="hljs-title function_">is_set</span>():
            <span class="hljs-attr">try</span>:
                <span class="hljs-attr">messages</span>: <span class="hljs-title class_">List</span>[<span class="hljs-title class_">CommonDocument</span>] = self.<span class="hljs-title function_">fetch_function</span>()
                <span class="hljs-keyword">if</span> <span class="hljs-attr">messages</span>:
                    messages = [msg.<span class="hljs-title function_">to_kafka_payload</span>() <span class="hljs-keyword">for</span> msg <span class="hljs-keyword">in</span> messages]
                    self.<span class="hljs-property">producer</span>.<span class="hljs-title function_">send</span>(self.<span class="hljs-property">topic</span>, value=messages)
                    self.<span class="hljs-property">producer</span>.<span class="hljs-title function_">flush</span>()
                logger.<span class="hljs-title function_">info</span>(
                    f<span class="hljs-string">"프로듀서 : {self.producer_id}이(가) {len(messages)}개의 메시지를 전송함."</span>
                )
                time.<span class="hljs-title function_">sleep</span>(self.<span class="hljs-property">wait_window_sec</span>)
            except <span class="hljs-title class_">Exception</span> <span class="hljs-keyword">as</span> <span class="hljs-attr">e</span>:
                logger.<span class="hljs-title function_">error</span>(f<span class="hljs-string">"프로듀서 작업자 {self.producer_id}에서 오류 발생: {e}"</span>)
                self.<span class="hljs-property">running</span>.<span class="hljs-title function_">clear</span>()  # 오류 시 스레드를 중지합니다

구현에서 고려해야 할 중요 사항:

<span class="xml"><span class="hljs-tag">&#x3C;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"content-ad"</span>></span><span class="hljs-tag">&#x3C;/<span class="hljs-name">div</span>></span></span>

- 우리는 fetch sources의 수만큼 <span class="hljs-title class_">KafkaProducerThread</span> 인스턴스가 생성됩니다.
- 우리는 모든 스레드를 <span class="hljs-title class_">KafkaProducerSwarm</span> 아래에 랩합니다.
- 모든 스레드 사이에서 단일 <span class="hljs-title class_">KafkaProducer</span> 인스턴스를 공유하며, 이는 클러스터와 통신할 것입니다.
- 우리는 N개의 fetching 스레드로 확장할 수 있지만 여전히 단일 <span class="hljs-title class_">KafkaProducer</span> 인스턴스를 유지하기 위해 싱글톤 디자인 패턴을 따릅니다.

## <span class="hljs-title class_">Pydantic</span>을 사용한 데이터 교환 모델

위에서 제시한 코드 스니펫 구현에서, 이전에 설명되지 않았던 *<span class="hljs-title class_">Document</span>, *<span class="hljs-title class_">Model</span> 객체의 사용을 관찰했을 수 있습니다. 이 섹션에서 이들이 무엇인지 자세히 살펴보겠습니다.

이들은 데이터 교환을 위한 <span class="hljs-title class_">Pydantic</span> 모델들이며, 우리가 구축 중인 응용 프로그램 내에서 이러한 모델들은:

<span class="xml"><span class="hljs-tag">&#x3C;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"content-ad"</span>></span><span class="hljs-tag">&#x3C;/<span class="hljs-name">div</span>></span></span>

- <span class="hljs-title class_">NewsDataIOModel</span>: <span class="hljs-title class_">NewsData</span> <span class="hljs-variable constant_">API</span>에서 가져온 원시 페이로드를 래핑하고 포맷합니다.
- <span class="hljs-title class_">NewsAPIModel</span>: <span class="hljs-title class_">NewsAPI</span> <span class="hljs-variable constant_">API</span>에서 가져온 원시 페이로드를 래핑하고 포맷합니다.
- <span class="hljs-title class_">CommonDocument</span>: 위에서 언급한 다양한 뉴스 형식 사이의 공통 형식을 설정합니다.
- <span class="hljs-title class_">RefinedDocument</span>: metadata 아래에 유용한 필드를 그룹화하고 기사 설명 텍스트와 같은 주요 필드를 강조하는 공통 형식을 필터링합니다.
- <span class="hljs-title class_">ChunkedDocument</span>: 텍스트를 청크로 나누고 chunk_id와 document_id 사이의 계보를 보장합니다.
- <span class="hljs-title class_">EmbeddedDocument</span>: 청크를 임베드하여 chunk_id와 document_id 사이의 계보를 보장합니다.

예를 들어, 위 <span class="hljs-title class_">CommonDocument</span> 모델은 다양한 뉴스 페이로드 형식 사이의 연결 역할을 나타내므로 이와 같이 구성됩니다:

<span class="hljs-keyword">class</span> <span class="hljs-title class_">CommonDocument</span>(<span class="hljs-title class_">BaseModel</span>):
    <span class="hljs-attr">article_id</span>: str = <span class="hljs-title class_">Field</span>(default_factory=<span class="hljs-attr">lambda</span>: <span class="hljs-title function_">str</span>(<span class="hljs-title function_">uuid4</span>()))
    <span class="hljs-attr">title</span>: str = <span class="hljs-title class_">Field</span>(default_factory=<span class="hljs-attr">lambda</span>: <span class="hljs-string">"N/A"</span>)
    <span class="hljs-attr">url</span>: str = <span class="hljs-title class_">Field</span>(default_factory=<span class="hljs-attr">lambda</span>: <span class="hljs-string">"N/A"</span>)
    <span class="hljs-attr">published_at</span>: str = <span class="hljs-title class_">Field</span>(
        default_factory=<span class="hljs-attr">lambda</span>: datetime.<span class="hljs-title function_">now</span>().<span class="hljs-title function_">strftime</span>(<span class="hljs-string">"%Y-%m-%d %H:%M:%S"</span>)
    )
    <span class="hljs-attr">source_name</span>: str = <span class="hljs-title class_">Field</span>(default_factory=<span class="hljs-attr">lambda</span>: <span class="hljs-string">"Unknown"</span>)
    <span class="hljs-attr">image_url</span>: <span class="hljs-title class_">Optional</span>[str] = <span class="hljs-title class_">Field</span>(default_factory=<span class="hljs-attr">lambda</span>: <span class="hljs-title class_">None</span>)
    <span class="hljs-attr">author</span>: <span class="hljs-title class_">Optional</span>[str] = <span class="hljs-title class_">Field</span>(default_factory=<span class="hljs-attr">lambda</span>: <span class="hljs-string">"Unknown"</span>)
    <span class="hljs-attr">description</span>: <span class="hljs-title class_">Optional</span>[str] = <span class="hljs-title class_">Field</span>(default_factory=<span class="hljs-attr">lambda</span>: <span class="hljs-title class_">None</span>)
    <span class="hljs-attr">content</span>: <span class="hljs-title class_">Optional</span>[str] = <span class="hljs-title class_">Field</span>(default_factory=<span class="hljs-attr">lambda</span>: <span class="hljs-title class_">None</span>)

    @<span class="hljs-title function_">field_validator</span>(<span class="hljs-string">"title"</span>, <span class="hljs-string">"description"</span>, <span class="hljs-string">"content"</span>)
    def <span class="hljs-title function_">clean_text_fields</span>(cls, v):
        <span class="hljs-keyword">if</span> v is <span class="hljs-title class_">None</span> or v == <span class="hljs-string">""</span>:
            <span class="hljs-keyword">return</span> <span class="hljs-string">"N/A"</span>
        <span class="hljs-keyword">return</span> <span class="hljs-title function_">clean_full</span>(v)

    @<span class="hljs-title function_">field_validator</span>(<span class="hljs-string">"url"</span>, <span class="hljs-string">"image_url"</span>)
    def <span class="hljs-title function_">clean_url_fields</span>(cls, v):
        <span class="hljs-keyword">if</span> v is <span class="hljs-title class_">None</span>:
            <span class="hljs-keyword">return</span> <span class="hljs-string">"N/A"</span>
        v = <span class="hljs-title function_">remove_html_tags</span>(v)
        v = <span class="hljs-title function_">normalize_whitespace</span>(v)
        <span class="hljs-keyword">return</span> v

    @<span class="hljs-title function_">field_validator</span>(<span class="hljs-string">"published_at"</span>)
    def <span class="hljs-title function_">clean_date_field</span>(cls, v):
        <span class="hljs-attr">try</span>:
            parsed_date = parser.<span class="hljs-title function_">parse</span>(v)
            <span class="hljs-keyword">return</span> parsed_date.<span class="hljs-title function_">strftime</span>(<span class="hljs-string">"%Y-%m-%d %H:%M:%S"</span>)
        except (<span class="hljs-title class_">ValueError</span>, <span class="hljs-title class_">TypeError</span>):
            logger.<span class="hljs-title function_">error</span>(f<span class="hljs-string">"Error parsing date: {v}, using current date instead."</span>)

    @classmethod
    def <span class="hljs-title function_">from_json</span>(cls, <span class="hljs-attr">data</span>: dict) -> <span class="hljs-string">"CommonDocument"</span>:
        <span class="hljs-string">""</span><span class="hljs-string">"JSON 객체에서 CommonDocument를 만듭니다."</span><span class="hljs-string">""</span>
        <span class="hljs-keyword">return</span> <span class="hljs-title function_">cls</span>(**data)

    def <span class="hljs-title function_">to_kafka_payload</span>(self) -> <span class="hljs-attr">dict</span>:
        <span class="hljs-string">""</span><span class="hljs-string">"Kafka 페이로드의 공통 표현을 준비합니다."</span><span class="hljs-string">""</span>
        <span class="hljs-keyword">return</span> self.<span class="hljs-title function_">model_dump</span>(exclude_none=<span class="hljs-title class_">False</span>)

해석해보겠습니다:

<span class="xml"><span class="hljs-tag">&#x3C;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"content-ad"</span>></span><span class="hljs-tag">&#x3C;/<span class="hljs-name">div</span>></span></span>

- 뉴스 기사 형식에 공통 속성 시리즈가 포함되어 있습니다.
- 각 필드를 유효성 검사하거나 field_validator 데코레이터를 사용하여 기본값을 지정합니다.
- to_kafka_payload 메서드는 메시지 직렬화를 보장하여 <span class="hljs-title class_">Kafka</span> 클러스터로 전송하기 전에 처리합니다.

## 텍스트 필드 클린업 프로세스

클린업 프로세스는 간단합니다. 텍스트를 정리하고 다음을 보장하기 위해 메서드를 사용합니다:

- 끝에 있는 공백이나 \n, \t를 제거합니다.
- ul/li 목록 항목을 제거합니다.
- 텍스트 내에 <span class="hljs-variable constant_">HTML</span> 태그가 있으면 제거합니다.

&#x3C;div <span class="hljs-keyword">class</span>=<span class="hljs-string">"content-ad"</span>>&#x3C;/div>

우리는 이러한 변환을 간소화하기 위해 구조화되지 않은 [<span class="hljs-number">7</span>] <span class="hljs-title class_">Python</span> 라이브러리를 사용하고 있습니다.

## <span class="hljs-title class_">KafkaProducers</span> 실행

지금까지 다음 모듈을 수행/구현했습니다:

- 필요한 모든 서비스에 등록
- <span class="hljs-title class_">Kafka</span> 클러스터 및 벡터 데이터베이스 생성
- 뉴스 기사 검색 핸들러 구현
- 데이터 교환을 위한 <span class="hljs-title class_">Pydantic</span> 모델 구현
- <span class="hljs-title class_">KafkaProducer</span> 로직 구현

&#x3C;div <span class="hljs-keyword">class</span>=<span class="hljs-string">"content-ad"</span>>&#x3C;/div>

작업이 완료되면 이제 안전하게 우리의 파이프라인에서 생산 단계를 실행하고 <span class="hljs-title class_">Upstash</span>의 <span class="hljs-title class_">KafkaCluster</span>에서 메시지를 확인할 수 있습니다.

그럼 시작해봐요!
프로젝트의 루트 디렉토리에서, <span class="hljs-title class_">Makefile</span>에 데이터 수집을 실행하는 명령어가 있습니다:

....

<span class="hljs-attr">run_producers</span>:
 @echo <span class="hljs-string">"$(GREEN) [실행 중] 데이터 수집 파이프라인 Kafka 프로듀서 $(RESET)"</span>
 @bash -c <span class="hljs-string">"poetry run python -m src.producer"</span>

...

이 🔗<span class="hljs-title class_">Makefile</span>은 우리가 구축 중인 솔루션과 상호작용하기 위한 유용한 명령어가 포함되어 있습니다. 이 경우에는 make run_producers를 사용하여 run_producers를 실행해야 합니다. 이렇게 하면 <span class="hljs-title class_">KafkaSwarm</span>이 시작되고 <span class="hljs-title class_">NewsAPIs</span>에서 기사를 가져와 형식을 지정한 다음 <span class="hljs-title class_">Kafka</span> 클러스터로 보내는 스레드를 다룰 것입니다.

&#x3C;div <span class="hljs-keyword">class</span>=<span class="hljs-string">"content-ad"</span>>&#x3C;/div>

</code></pre>
<p><img src="/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_8.png" alt="이미지"></p>
<p>로그를 통해 프로듀서 스레드가 각각 5개의 메시지를 보냈다는 것을 확인했습니다. 메시지들이 클러스터에 도달했는지 확인하려면 Upstash 콘솔로 이동하여 Kafka 클러스터 → 메시지를 확인하십시오. 다음과 같은 화면이 나타날 것입니다:</p>
<p><img src="/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_9.png" alt="이미지"></p>
<p>이 시점에서는 API에서 뉴스 기사를 가져와 형식을 맞춘 후 Kafka로 메시지를 보내는 데이터 수집 파이프라인의 구현 및 테스트가 완료되었습니다. 다음으로는 Kafka에서 새 메시지를 처리하는 "컨슈머" 또는 적재 파이프라인을 구현할 것입니다.</p>
<div class="content-ad"></div>
<h1>데이터 수집 파이프라인</h1>
<p>우리가 Kafka 주제에서 메시지를 받았다는 것을 확인한 후에는 "소비자" 파이프라인을 구현해야 합니다. 이는 다음을 의미합니다:</p>
<ul>
<li>Kafka 주제에서 메시지 읽기</li>
<li>파싱, 형식 지정, 청크화, 임베딩 생성</li>
<li>벡터 객체 생성 및 Upstash Vector Index에 업서트</li>
</ul>
<p><img src="/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_10.png" alt="이미지"></p>
<div class="content-ad"></div>
<p>이를 위해 Bytewax [4]를 사용하여 이러한 단계를 올바른 순서로 연결하는 DataFlow를 정의할 것입니다.</p>
<p>바로 구현에 들어가서 주요 개념을 설명해보겠습니다!</p>
<ul>
<li>Bytewax Flow에 입력으로 Kafka Source를 정의합니다.</li>
</ul>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> json
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-title class_">List</span>

<span class="hljs-keyword">from</span> bytewax.<span class="hljs-property">connectors</span>.<span class="hljs-property">kafka</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">KafkaSinkMessage</span>, <span class="hljs-title class_">KafkaSource</span>

<span class="hljs-keyword">from</span> logger <span class="hljs-keyword">import</span> get_logger
<span class="hljs-keyword">from</span> models <span class="hljs-keyword">import</span> <span class="hljs-title class_">CommonDocument</span>
<span class="hljs-keyword">from</span> settings <span class="hljs-keyword">import</span> settings

logger = <span class="hljs-title function_">get_logger</span>(__name__)

def <span class="hljs-title function_">build_kafka_stream_client</span>():
    <span class="hljs-string">""</span><span class="hljs-string">"
    Build a Kafka stream client to read messages from the Upstash Kafka topic using the ByteWax KafkaSource connector.
    "</span><span class="hljs-string">""</span>
    kafka_config = {
        <span class="hljs-string">"bootstrap.servers"</span>: settings.<span class="hljs-property">UPSTASH_KAFKA_ENDPOINT</span>,
        <span class="hljs-string">"security.protocol"</span>: <span class="hljs-string">"SASL_SSL"</span>,
        <span class="hljs-string">"sasl.mechanisms"</span>: <span class="hljs-string">"SCRAM-SHA-256"</span>,
        <span class="hljs-string">"sasl.username"</span>: settings.<span class="hljs-property">UPSTASH_KAFKA_UNAME</span>,
        <span class="hljs-string">"sasl.password"</span>: settings.<span class="hljs-property">UPSTASH_KAFKA_PASS</span>,
        <span class="hljs-string">"auto.offset.reset"</span>: <span class="hljs-string">"earliest"</span>,  # <span class="hljs-title class_">Start</span> reading at the earliest message
    }
    kafka_input = <span class="hljs-title class_">KafkaSource</span>(
        topics=[settings.<span class="hljs-property">UPSTASH_KAFKA_TOPIC</span>],
        brokers=[settings.<span class="hljs-property">UPSTASH_KAFKA_ENDPOINT</span>],
        add_config=kafka_config,
    )
    logger.<span class="hljs-title function_">info</span>(<span class="hljs-string">"KafkaSource client created successfully."</span>)
    <span class="hljs-keyword">return</span> kafka_input

def <span class="hljs-title function_">process_message</span>(<span class="hljs-attr">message</span>: <span class="hljs-title class_">KafkaSinkMessage</span>):
    <span class="hljs-string">""</span><span class="hljs-string">"
    On a Kafka message, process the message and return a list of CommonDocuments.
    - message: KafkaSinkMessage(key, value) where value is the message payload.
    "</span><span class="hljs-string">""</span>
    <span class="hljs-attr">documents</span>: <span class="hljs-title class_">List</span>[<span class="hljs-title class_">CommonDocument</span>] = []
    <span class="hljs-attr">try</span>:
        json_str = message.<span class="hljs-property">value</span>.<span class="hljs-title function_">decode</span>(<span class="hljs-string">"utf-8"</span>)
        data = json.<span class="hljs-title function_">loads</span>(json_str)
        documents = [<span class="hljs-title class_">CommonDocument</span>.<span class="hljs-title function_">from_json</span>(obj) <span class="hljs-keyword">for</span> obj <span class="hljs-keyword">in</span> data]
        logger.<span class="hljs-title function_">info</span>(f<span class="hljs-string">"Decoded into {len(documents)} CommonDocuments"</span>)
        <span class="hljs-keyword">return</span> documents
    except <span class="hljs-title class_">StopIteration</span>:
        logger.<span class="hljs-title function_">info</span>(<span class="hljs-string">"No more documents to fetch from the client."</span>)
    except <span class="hljs-title class_">KeyError</span> <span class="hljs-keyword">as</span> <span class="hljs-attr">e</span>:
        logger.<span class="hljs-title function_">error</span>(f<span class="hljs-string">"Key error in processing document batch: {e}"</span>)
    except json.<span class="hljs-property">JSONDecodeError</span> <span class="hljs-keyword">as</span> <span class="hljs-attr">e</span>:
        logger.<span class="hljs-title function_">error</span>(f<span class="hljs-string">"Error decoding JSON from message: {e}"</span>)
        raise
    except <span class="hljs-title class_">Exception</span> <span class="hljs-keyword">as</span> <span class="hljs-attr">e</span>:
        logger.<span class="hljs-title function_">exception</span>(f<span class="hljs-string">"Unexpected error in next_batch: {e}"</span>)
</code></pre>
<div class="content-ad"></div>
<p>이 구현에서 중요한 점들:</p>
<ul>
<li>build_kafka_stream_client : 미리 정의된 Bytewax KafkaSource 커넥터를 사용하여 KafkaConsumer의 인스턴스를 생성합니다.</li>
<li>process_message : Kafka Topic에서 메시지를 처리할 콜백 함수입니다.</li>
</ul>
<ol start="2">
<li>Bytewax 플로우의 출력으로 Upstash Vector (Index)를 정의합니다.</li>
</ol>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-title class_">Optional</span>, <span class="hljs-title class_">List</span>

<span class="hljs-keyword">from</span> bytewax.<span class="hljs-property">outputs</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">DynamicSink</span>, <span class="hljs-title class_">StatelessSinkPartition</span>
<span class="hljs-keyword">from</span> upstash_vector <span class="hljs-keyword">import</span> <span class="hljs-title class_">Index</span>, <span class="hljs-title class_">Vector</span>
<span class="hljs-keyword">from</span> models <span class="hljs-keyword">import</span> <span class="hljs-title class_">EmbeddedDocument</span>
<span class="hljs-keyword">from</span> settings <span class="hljs-keyword">import</span> settings
<span class="hljs-keyword">from</span> logger <span class="hljs-keyword">import</span> get_logger


logger = <span class="hljs-title function_">get_logger</span>(__name__)


<span class="hljs-keyword">class</span> <span class="hljs-title class_">UpstashVectorOutput</span>(<span class="hljs-title class_">DynamicSink</span>):
    <span class="hljs-string">""</span><span class="hljs-string">"Upstash 벡터 출력을 나타내는 클래스입니다.

    이 클래스는 at-least-once 처리를 지원하는 동적 출력 유형인 Upstash 벡터 출력을 생성하는 데 사용됩니다.
    resume 이후의 메시지는 resume 즉각적으로 복제됩니다.

    Args:
        vector_size (int): 벡터의 크기.
        collection_name (str, optional): 컬렉션의 이름입니다. 기본값은 constants.VECTOR_DB_OUTPUT_COLLECTION_NAME입니다.
        client (Optional[UpstashClient], optional): Upstash 클라이언트입니다. 기본값은 None입니다.
    "</span><span class="hljs-string">""</span>

    def <span class="hljs-title function_">__init__</span>(
        self,
        <span class="hljs-attr">vector_size</span>: int = settings.<span class="hljs-property">EMBEDDING_MODEL_MAX_INPUT_LENGTH</span>,
        <span class="hljs-attr">collection_name</span>: str = settings.<span class="hljs-property">UPSTASH_VECTOR_TOPIC</span>,
        <span class="hljs-attr">client</span>: <span class="hljs-title class_">Optional</span>[<span class="hljs-title class_">Index</span>] = <span class="hljs-title class_">None</span>,
    ):
        self.<span class="hljs-property">_collection_name</span> = collection_name
        self.<span class="hljs-property">_vector_size</span> = vector_size

        <span class="hljs-keyword">if</span> <span class="hljs-attr">client</span>:
            self.<span class="hljs-property">client</span> = client
        <span class="hljs-attr">else</span>:
            self.<span class="hljs-property">client</span> = <span class="hljs-title class_">Index</span>(
                url=settings.<span class="hljs-property">UPSTASH_VECTOR_ENDPOINT</span>,
                token=settings.<span class="hljs-property">UPSTASH_VECTOR_KEY</span>,
                retries=settings.<span class="hljs-property">UPSTASH_VECTOR_RETRIES</span>,
                retry_interval=settings.<span class="hljs-property">UPSTASH_VECTOR_WAIT_INTERVAL</span>,
            )

    def <span class="hljs-title function_">build</span>(
        self, <span class="hljs-attr">step_id</span>: str, <span class="hljs-attr">worker_index</span>: int, <span class="hljs-attr">worker_count</span>: int
    ) -> <span class="hljs-title class_">StatelessSinkPartition</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-title class_">UpstashVectorSink</span>(self.<span class="hljs-property">client</span>, self.<span class="hljs-property">_collection_name</span>)


<span class="hljs-keyword">class</span> <span class="hljs-title class_">UpstashVectorSink</span>(<span class="hljs-title class_">StatelessSinkPartition</span>):
    <span class="hljs-string">""</span><span class="hljs-string">"
    Upstash Vector 데이터베이스 컬렉션에 문서 임베딩을 작성하는 싱크입니다.
    이 구현은 효율성을 높이기 위해 배치 업서트를 활용하며, 오류 처리 및 로깅을 향상시키고 가독성 및 유지 보수성을 위해 Pythonic한 모법을 따릅니다.

    Args:
        client (Index): 쓰기에 사용할 Upstash Vector 클라이언트입니다.
        collection_name (str, optional): 쓸 컬렉션의 이름입니다. 기본값은 UPSTASH_VECTOR_TOPIC 환경 변수의 값입니다.
    "</span><span class="hljs-string">""</span>

    def <span class="hljs-title function_">__init__</span>(
        self,
        <span class="hljs-attr">client</span>: <span class="hljs-title class_">Index</span>,
        <span class="hljs-attr">collection_name</span>: str = <span class="hljs-title class_">None</span>,
    ):
        self.<span class="hljs-property">_client</span> = client
        self.<span class="hljs-property">_collection_name</span> = collection_name
        self.<span class="hljs-property">_upsert_batch_size</span> = settings.<span class="hljs-property">UPSTASH_VECTOR_UPSERT_BATCH_SIZE</span>

    def <span class="hljs-title function_">write_batch</span>(self, <span class="hljs-attr">documents</span>: <span class="hljs-title class_">List</span>[<span class="hljs-title class_">EmbeddedDocument</span>]):
        <span class="hljs-string">""</span><span class="hljs-string">"
        구성된 Upstash Vector 데이터베이스 컬렉션에 문서 임베딩의 배치를 작성합니다.

        Args:
            documents (List[EmbeddedDocument]): 쓸 문서들입니다.
        "</span><span class="hljs-string">""</span>
        vectors = [
            <span class="hljs-title class_">Vector</span>(id=doc.<span class="hljs-property">doc_id</span>, vector=doc.<span class="hljs-property">embeddings</span>, metadata=doc.<span class="hljs-property">metadata</span>)
            <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> documents
        ]

        # 효율성을 위한 배치 업서트
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-number">0</span>, <span class="hljs-title function_">len</span>(vectors), self.<span class="hljs-property">_upsert_batch_size</span>):
            batch_vectors = vectors[i : i + self.<span class="hljs-property">_upsert_batch_size</span>]
            <span class="hljs-attr">try</span>:
                self.<span class="hljs-property">_client</span>.<span class="hljs-title function_">upsert</span>(vectors=batch_vectors)
            except <span class="hljs-title class_">Exception</span> <span class="hljs-keyword">as</span> <span class="hljs-attr">e</span>:
                logger.<span class="hljs-title function_">error</span>(f<span class="hljs-string">"배치 업서트 중 예외 발생 {e}"</span>)
</code></pre>
<div class="content-ad"></div>
<p>이 구현에서 중요한 사항들입니다:</p>
<ul>
<li>UpstashVectorOutput: 다양한 대상으로 데이터를 전달하기 위해 설계된 Bytewax DynamicSink 추상화를 인스턴스화합니다. 우리의 경우, 이는 Upstash Vector Index 클라이언트 연결 위에 래핑될 것입니다.</li>
<li>UpstashVectorSink: 우리의 DynamicSink을 래핑하고 업서트 벡터를 우리의 VectorDatabase에 처리하는 기능을 담당합니다. 이 StatelessSinkPartition은 DynamicSink가 어떠한 상태도 유지하지 않고 우리의 Sink에 대한 모든 입력을 write_batch 기능 구현에 따라 처리합니다.</li>
</ul>
<h2>나머지 Bytewax Flow 빌드하기</h2>
<p>여기 Upstash Kafka Topic에서 메시지를 가져와 정제, 수정, 분할, 삽입하고 Upstash Vector Index에 벡터를 업서트하는 저희 DataFlow의 전체 구현입니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-string">""</span><span class="hljs-string">"
    이 스크립트는 Upstash 사용 사례에 대한 ByteWax 데이터플로 구현을 정의합니다.
    데이터플로에는 다음 단계가 포함되어 있습니다:
        1. 입력: 카프카 스트림에서 데이터를 읽기
        2. 정제: 입력 데이터를 공통 형식으로 변환 
        3. 청크 분리: 입력 데이터를 더 작은 청크로 분리
        4. 임베드: 입력 데이터에 대한 임베딩 생성
        5. 출력: 출력 데이터를 Upstash 벡터 데이터베이스에 쓰기
"</span><span class="hljs-string">""</span>

<span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> <span class="hljs-title class_">Path</span>
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-title class_">Optional</span>

<span class="hljs-keyword">import</span> bytewax.<span class="hljs-property">operators</span> <span class="hljs-keyword">as</span> op
<span class="hljs-keyword">from</span> vector <span class="hljs-keyword">import</span> <span class="hljs-title class_">UpstashVectorOutput</span>
<span class="hljs-keyword">from</span> consumer <span class="hljs-keyword">import</span> process_message, build_kafka_stream_client
<span class="hljs-keyword">from</span> bytewax.<span class="hljs-property">connectors</span>.<span class="hljs-property">kafka</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">KafkaSource</span>
<span class="hljs-keyword">from</span> bytewax.<span class="hljs-property">dataflow</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Dataflow</span>
<span class="hljs-keyword">from</span> bytewax.<span class="hljs-property">outputs</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">DynamicSink</span>
<span class="hljs-keyword">from</span> embeddings <span class="hljs-keyword">import</span> <span class="hljs-title class_">TextEmbedder</span>
<span class="hljs-keyword">from</span> models <span class="hljs-keyword">import</span> <span class="hljs-title class_">ChunkedDocument</span>, <span class="hljs-title class_">EmbeddedDocument</span>, <span class="hljs-title class_">RefinedDocument</span>
<span class="hljs-keyword">from</span> logger <span class="hljs-keyword">import</span> get_logger

logger = <span class="hljs-title function_">get_logger</span>(__name__)


def <span class="hljs-title function_">build</span>(
    <span class="hljs-attr">model_cache_dir</span>: <span class="hljs-title class_">Optional</span>[<span class="hljs-title class_">Path</span>] = <span class="hljs-title class_">None</span>,
) -> <span class="hljs-title class_">Dataflow</span>:
    <span class="hljs-string">""</span><span class="hljs-string">"
    Upstash 사용 사례에 대한 ByteWax 데이터플로를 구축합니다.
    다음과 같은 데이터플로를 따릅니다:
        * 1. Tag: ['kafka_input']   = KafkaSource에서 입력 데이터 읽기
        * 2. Tag: ['map_kinp']      = KafkaSource에서 CommonDocument로 메시지 처리
            * 2.1 [Optional] Tag ['dbg_map_kinp'] = ['map_kinp'] 후 디버깅
        * 3. Tag: ['refine']        = 메시지를 정제된 문서 형식으로 변환
            * 3.1 [Optional] Tag ['dbg_refine'] = ['refine'] 후 디버깅
        * 4. Tag: ['chunkenize']    = 정제된 문서를 더 작은 청크로 나누기
            * 4.1 [Optional] Tag ['dbg_chunkenize'] = ['chunkenize'] 후 디버깅
        * 5. Tag: ['embed']         = 청크에 대한 임베딩 생성
            * 5.1 [Optional] Tag ['dbg_embed'] = ['embed'] 후 디버깅
        * 6. Tag: ['output']        = 임베딩을 Upstash 벡터 데이터베이스에 쓰기
    노트:
        각 선택적 태그는 문제 해결을 위해 활성화할 수 있는 디버깅 단계입니다.
    "</span><span class="hljs-string">""</span>
    model = <span class="hljs-title class_">TextEmbedder</span>(cache_dir=model_cache_dir)

    dataflow = <span class="hljs-title class_">Dataflow</span>(flow_id=<span class="hljs-string">"news-to-upstash"</span>)
    stream = op.<span class="hljs-title function_">input</span>(
        step_id=<span class="hljs-string">"kafka_input"</span>,
        flow=dataflow,
        source=<span class="hljs-title function_">_build_input</span>(),
    )
    stream = op.<span class="hljs-title function_">flat_map</span>(<span class="hljs-string">"map_kinp"</span>, stream, process_message)
    # _ = op.<span class="hljs-title function_">inspect</span>(<span class="hljs-string">"dbg_map_kinp"</span>, stream)
    stream = op.<span class="hljs-title function_">map</span>(<span class="hljs-string">"refine"</span>, stream, <span class="hljs-title class_">RefinedDocument</span>.<span class="hljs-property">from_common</span>)
    # _ = op.<span class="hljs-title function_">inspect</span>(<span class="hljs-string">"dbg_refine"</span>, stream)
    stream = op.<span class="hljs-title function_">flat_map</span>(
        <span class="hljs-string">"chunkenize"</span>,
        stream,
        lambda <span class="hljs-attr">refined_doc</span>: <span class="hljs-title class_">ChunkedDocument</span>.<span class="hljs-title function_">from_refined</span>(refined_doc, model),
    )
    # _ = op.<span class="hljs-title function_">inspect</span>(<span class="hljs-string">"dbg_chunkenize"</span>, stream)
    stream = op.<span class="hljs-title function_">map</span>(
        <span class="hljs-string">"embed"</span>,
        stream,
        lambda <span class="hljs-attr">chunked_doc</span>: <span class="hljs-title class_">EmbeddedDocument</span>.<span class="hljs-title function_">from_chunked</span>(chunked_doc, model),
    )
    # _ = op.<span class="hljs-title function_">inspect</span>(<span class="hljs-string">"dbg_embed"</span>, stream)
    stream = op.<span class="hljs-title function_">output</span>(<span class="hljs-string">"output"</span>, stream, <span class="hljs-title function_">_build_output</span>())
    logger.<span class="hljs-title function_">info</span>(<span class="hljs-string">"성공적으로 ByteWax 데이터플로를 생성했습니다."</span>)
    logger.<span class="hljs-title function_">info</span>(
        <span class="hljs-string">"\t단계: Kafka 입력 -> 매핑 -> 정제 -> 청크 분리 -> 임베딩 -> 업로드"</span>
    )
    <span class="hljs-keyword">return</span> dataflow


def <span class="hljs-title function_">_build_input</span>() -> <span class="hljs-title class_">KafkaSource</span>:
    <span class="hljs-keyword">return</span> <span class="hljs-title function_">build_kafka_stream_client</span>()


def <span class="hljs-title function_">_build_output</span>() -> <span class="hljs-title class_">DynamicSink</span>:
    <span class="hljs-keyword">return</span> <span class="hljs-title class_">UpstashVectorOutput</span>()
</code></pre>
<div class="content-ad"></div>
<ul>
<li>kafka_input: 카프카 메시지를 가져와 CommonDocument Pydantic 형식으로 변환하는 단계입니다.</li>
<li>map_kinp: 카프카 입력을 의미하며, 수신된 메시지에 flat map을 적용하여 List[CommonDocument] Pydantic 객체를 생성합니다.</li>
<li>refine: List[CommonDocument]를 순회하고 RefinedDocument 인스턴스를 생성하는 단계입니다.</li>
<li>chunkenize: List[RefinedDocument]를 순회하고 ChunkedDocument 인스턴스를 생성하는 단계입니다.</li>
<li>embed: List[ChunkedDocuments]를 순회하고 EmbeddedDocument 인스턴스를 생성하는 단계입니다.</li>
<li>output: List[EmbeddedDocument]를 순회하고 Vector 객체를 생성하여 Upstash Vector Index에 업서트하는 단계입니다.</li>
</ul>
<h1>파이프라인 시작</h1>
<p>지금까지 구현한 것은 다음과 같습니다:</p>
<ul>
<li>데이터 수집 파이프라인: 주기적으로 NewsAPI에서 원시 페이로드를 가져와 형식을 지정한 뒤, 카프카 토픽으로 메시지를 전송하는 단계입니다.</li>
<li>인제션 파이프라인: 이는 Bytewax DataFlow로, 카프카 토픽에 연결되어 메시지를 소비하고, 최종적으로 벡터를 업서트하는 Vector 데이터베이스에 업데이트합니다.</li>
</ul>
<div class="content-ad"></div>
<p>프로젝트 루트에있는 Makefile에서 미리 정의된 명령을 사용하여 이 두 서비스를 모두 시작할 수 있습니다:</p>
<pre><code class="hljs language-js"># 카프카 메시지를 생성하기 위해 데이터 수집 파이프라인 실행
make run_producers

# 카프카 메시지를 소비하고 벡터를 업데이트하기 위해 인제스처리 파이프라인 실행
make run_pipeline
</code></pre>
<p>그리고... 완료되었습니다!
성공적으로 프로듀서/컨슈머 서비스를 시작했습니다.
남은 모듈은 UI입니다. 벡터 데이터베이스와 뉴스 기사를 검색하는 데 상호 작용하는 데 사용됩니다.</p>
<h1>사용자 인터페이스</h1>
<div class="content-ad"></div>
<p>UI는 다음과 같은 기능을 갖춘 기본 Streamlit [8] 애플리케이션입니다:</p>
<ul>
<li>텍스트 검색 창</li>
<li>벡터 데이터베이스에서 가져온 기사로 채워진 카드가 표시되는 div 섹션</li>
</ul>
<p>카드에는 다음과 같은 데이터 필드가 포함되어 있습니다:</p>
<ul>
<li>발행일</li>
<li>유사도 점수</li>
<li>기사 이미지</li>
<li>SeeMore 버튼을 클릭하면 원본 기사 URL로 이동합니다.</li>
</ul>
<div class="content-ad"></div>
<p>한번 메시지/질문을 텍스트 상자에 입력하면 입력이 정리되고 (소문자로 변환, 비ASCII 문자 제거 등) 그리고 삽입됩니다. 새로운 삽입물을 사용하여 벡터 데이터베이스를 쿼리하여 가장 유사한 항목을 가져옵니다. 그 결과는 구성되어 렌더링될 것입니다.</p>
<p>다음은 예시입니다:</p>
<p><img src="/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_11.png" alt="How to build a real-time News Search Engine using VectorDBs - Part 1"></p>
<p><img src="/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_12.png" alt="How to build a real-time News Search Engine using VectorDBs - Part 2"></p>
<div class="content-ad"></div>
<h1>결론</h1>
<p>축하합니다!</p>
<p>성공했습니다! 멋진 프로젝트만큼이나 라이브로 출시할 준비가 된 뉴스 검색 엔진을 만들었습니다. 단순히 무작정 던지는 것이 아니라, 우리는 최고의 소프트웨어 개발 관행을 따르기도 했습니다.</p>
<p>Pydantic을 사용하여 데이터를 잘 처리했고, 유닛 테스트를 작성하고, 스레딩을 활용하여 작업을 가속화했으며, Upstash의 서버리스 카프카와 벡터 데이터베이스를 활용하여 파이프라인을 쉽게 설정할 뿐만 아니라 빠르고 확장 가능하며 오류 대비 가능하도록 만들었습니다.</p>
<div class="content-ad"></div>
<p>이제 당신은 이 청사진을 대부분의 데이터 기반 아이디어에 적용할 수 있는 능력을 갖게 되었어요. 이건 이 프로젝트뿐만 아니라 앞으로 만들게 될 멋진 것들을 위한 큰 승리에요.</p>
<h1>참고 자료</h1>
<p>[1] News Search Engine using Upstash Vector — Decoding ML Github (2024)
[2] Upstash Serverless Kafka
[3] Upstash Serverless Vector Database
[4] Bytewax Stream Processing with Python
[5] Singleton Pattern
[6] sentence-transformers/all-MiniLM-L6-v2
[7] unstructured Python Library
[8] Streamlit Python</p>
<h1>더 읽을 거리</h1>
<div class="content-ad"></div>
<p>이 글과 관련성 순으로 정렬되었습니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"벡터 DB를 사용하여 실시간 뉴스 검색 엔진을 구축하는 방법","description":"","date":"2024-06-19 16:12","slug":"2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs","content":"\n\n아파치 카프카, 바이트왁스, 그리고 업스태시 벡터 데이터베이스를 활용한 라이브 뉴스 집계 스트리밍 파이프라인 구현을 위한 실용적인 안내서입니다.\n\nearthweb.com에서 실시한 연구에 따르면, 매일 온라인 및 오프라인에서 유입되는 뉴스 기사는 200-300만 건 사이에 있다고 합니다!\n\n모든 방향에서 우리에게 쏟아지는 뉴스로 인해 정신없을 때가 많습니다. 그래서 우리는 실제로 관심 있는 뉴스를 빠르게 받아볼 수 있는 짧고 빠른 방법을 찾고 있습니다.\n\n본 문서에서는 이러한 문제를 해결하고자 합니다! 좀 더 구체적으로, 여러 출처로부터 데이터를 수집하고 해당 정보 채널을 당신의 관심사에 맞는 종단점으로 좁힐 수 있는 시스템을 구축할 것입니다 — 뉴스 검색 엔진입니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이론에 대해서만 이야기하거나 이러한 시스템을 구현하는 방법을 알려주는 것이 아니라, 우리는 단계별로 설명하고 보여줄 거예요!\n\n시작하기 전에, 이 기사에서 제공하는 모든 것은 Decoding ML Articles GitHub 저장소에서 완전한 코드 지원을 받습니다.\n\n# 목차\n\n- 아키텍처 개요\n- 도구 고려 사항\n- 전제 조건\n3.1 새로운 Upstash Kafka 클러스터 생성\n3.2 새로운 Upstash Vector 인덱스 생성\n3.3 2개의 라이브 뉴스 API에 등록\n3.4 설치\n- 데이터 수집\n4.1 기사 가져오기 관리자\n4.2 Kafka 메시지 제작\n4.3 Pydantic을 사용한 데이터 교환 모델\n4.4 KafkaProducers 실행\n- 수집 파이프라인\n5.1 Kafka에서 메시지 수신\n5.2 Bytewax 데이터플로 구현\n5.3 기사 정제, 형식 지정, 청크화, 삽입\n5.4 벡터 작성 및 VectorDB에 업서트\n- 파이프라인 시작\n- 사용자 인터페이스\n- 결론\n- 참고문헌\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 아키텍처 개요\n\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_0.png)\n\n요약하자면, 우리는 뉴스 API에서 뉴스 기사를 가져와서 생생한 시스템을 구축할 것입니다. 가져온 데이터를 정의된 형식으로 파싱하고 포맷팅한 다음 첫 번째 단계로 Kafka 토픽에 메시지를 직렬화하고 스트리밍할 것입니다. \n두 번째 단계에서는 Bytewax를 사용하여 Kafka 토픽에서 메시지를 더 청소, 파싱, 청크, 임베드, 벡터를 업서팅하여 벡터 데이터베이스로 보내고, 데이터베이스와 상호 작용할 수 있는 UI로 마무리됩니다.\n\n# 툴 고려 사항\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n올바른 도구를 선택하는 것이 고성능, 확장성, 및 구현 용이성을 달성하는 핵심이었습니다. 프로젝트 전체에서 사용된 도구를 살펴보겠습니다:\n\n- Upstash Serverless Kafka: 인프라 관리에 대해 걱정할 필요 없이 강력하고 확장 가능한 이벤트 스트리밍을 위해 사용됩니다.\n- Python 스레딩: 여러 뉴스 API에서 동시에 데이터를 가져오면서 스레드 안전한 Kafka Producer 인스턴스를 공유하여 메모리 풋프린트와 성능을 최적화합니다.\n- Pydantic 모델: 일관된 및 유효한 데이터 교환 구조를 보장하기 위해 사용됩니다.\n- Bytewax: 스트리밍 데이터를 처리하고 변환하는 데 간편하고 빠른 속도를 제공하기 때문에 사용됩니다.\n- Upstash Vector Database: Serverless로 구성이 쉽고 Python, JS, 및 GO 내에서 쉽게 통합됩니다. UI 콘솔 대시보드에서 풍부한 탐색 옵션과 실시간 상태 메트릭을 제공하는 것이 큰 장점입니다.\n\n하드웨어 요구 사항에 따르면 GPU는 필요하지 않습니다.\n\n비용은 얼마입니까? — 무료입니다.\n이 안내서는 무료 티어 플랜만 사용하도록 설정했으므로 사용한 플랫폼에 대해 지불할 필요가 없습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 준비 사항\n\n어떠한 구현을 하기 전에, 각 서비스에 접근할 수 있는지 확인해야 합니다. 따라서 다음을 설정해야 합니다:\n\n- 새로운 Upstash Kafka 클러스터\n- 새로운 Upstash Vector Index\n- 뉴스 API 등록\n- 환경 설치\n\n처음 시작할 때는 약 5분 정도 걸립니다. 함께 해보세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 새로운 Upstash Kafka 클러스터 생성\n\n먼저 Upstash에 등록해야 합니다. 로그인 후에 다음과 같은 대시보드가 나타납니다:\n\n![대시보드 이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_1.png)\n\n다음으로 상단 바에서 Kafka를 선택하고 + 클러스터 생성 버튼을 클릭하여 새 클러스터를 만들어야 합니다. 클릭하면 다음 모달이 나타납니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_2.png\" /\u003e\n\n클러스터에 이름을 지정하고 본인의 위치와 가장 가까운 지역을 선택한 후, 클러스터 생성을 클릭하여 완료하세요. 완료되면 새로운 Kafka 클러스터가 아래에 표시됩니다. 새로운 Kafka 클러스터를 선택하고 아래 화면으로 이동하게 됩니다:\n\n\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_3.png\" /\u003e\n\n이 뷰에서 주요 구성 요소를 살펴보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 상세 정보: 클러스터 개요 및 Upstash가 제공하는 기능을 보여줍니다.\n- 사용량: 생성/소비된 메시지 수, 비용 영향 등을 보여주는 차트입니다.\n- 주제: 이 탭에서는 Kafka 주제를 만들고 세부 정보를 모니터링할 수 있습니다.\n\n클러스터를 생성한 다음 해야 할 다음 단계는 메시지를 생성(보내기)하고 소비(받기)할 수 있는 주제를 정의하는 것입니다.\n\n주제 탭 아래에서 \"주제 생성\"을 선택하면 다음과 같은 화면이 나타납니다:\n\n\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_4.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주제 이름을 지정하고, 나머지는 기본 상태로 두어서 Create를 클릭하세요.\n\n카프카 클러스터를 성공적으로 생성했습니다. 이제 클러스터에 연결하는 데 도움이 되는 환경 변수를 복사하고 설정해야 합니다. 이를 위해 클러스터 대시보드로 이동하여 세부 정보 탭에서 엔드포인트, 사용자 이름 및 비밀번호를 복사하여 .env 파일에 붙여넣으세요.\n그 후, Topics로 이동하여 카프카 토픽 이름을 복사하세요.\n\n지금까지 .env 파일이어야 하는 모습은 다음과 같습니다:\n\n```js\nUPSTASH_KAFKA_UNAME=\"[사용자 이름]\"\nUPSTASH_KAFKA_PASS=\"[비밀번호]\"\nUPSTASH_KAFKA_ENDPOINT=\"[엔드포인트]\"\nUPSTASH_KAFKA_TOPIC=\"[토픽 이름]\"\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 새 Upstash Vector 색인 만들기\n\n이제 새로운 Vector 데이터베이스를 만들어 보겠습니다. 이를 위해 대시보드에서 Vector를 선택하고 + Index 작성을 클릭하세요. 그러면 다음 뷰로 이동됩니다:\n\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_5.png)\n\n벡터 데이터베이스에 이름을 할당하고, 위치에 가장 가까운 지역을 선택한 다음 Embedding을 생성할 때 사용할 모델로 sentence-transformers/all-MiniLM-L6-v2을 선택하세요. 뉴스 기사의 임베딩을 생성할 때 사용할 모델과 벡터 간 거리 비교에 코사인 유사도 메트릭을 사용할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n새로운 Vector Index를 만든 후에는 Kafka Cluster와 동일한 작업 흐름을 따를 수 있습니다. Index Name, Endpoint, Token을 복사하고 .env 파일에 붙여넣기하세요.\n\n현재 .env 파일은 다음과 같이 보여야 합니다:\n\n```js\nUPSTASH_KAFKA_UNAME=\"[여기에 사용자명 입력]\"\nUPSTASH_KAFKA_PASS=\"[여기에 비밀번호 입력]\"\nUPSTASH_KAFKA_ENDPOINT=\"[여기에 엔드포인트 입력]\"\nUPSTASH_KAFKA_TOPIC=\"[여기에 토픽 이름 입력]\"\n\nUPSTASH_VECTOR_ENDPOINT=\"[Vector 엔드포인트 입력]\"\nUPSTASH_VECTOR_TOPIC=\"[Vector 이름 입력]\"\nUPSTASH_VECTOR_KEY=\"[Vector 토큰 입력]\"\n``` \n\n## 뉴스 API에 등록하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 APIs를 사용하여 기사를 가져올 예정입니다:\n\n- 🔗 NewsAPI\n\n하루에 100번의 API 호출을 할 수 있는 무료 개발자 플랜을 제공합니다.\n\n2. 🔗 NewsData\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n무료 요금제가 제공되며 하루에 200개의 크레딧을 받습니다. 각 크레딧은 10개의 기사와 동일하며, 이는 하루에 총 2000개의 기사를 가져올 수 있다는 것을 의미합니다.\n\n저희 현재의 사용 사례에는 이 API들이 충분한 기능을 제공하여 구축 중인 뉴스 검색 엔진을 구현하고 유효성을 검사할 수 있습니다. 동시에 기존 워크플로우가 동일하게 유지되므로 개선 및 확장할 여지도 남겨두고 있습니다.\n무료 요금제에 따른 유일한 제약은 타임드-배치 페치를 수행할 수 없다는 것입니다. 즉, 이 API들을 쿼리할 때 from_date, to_date를 사용할 수 없습니다. 하지만 이는 문제가 되지 않습니다.\n대신 페치 호출 간의 대기 시간을 이용하여 이 동작을 모방할 예정입니다.\n\n다음 단계는 두 플랫폼에 등록하는 것입니다 — 걱정 마세요, 가능한 간단합니다.\n\n- NewsAPI에 등록한 후, /account로 이동하여 API_KEY 필드를 확인한 후 이를 .env 파일의 NEWSAPI_KEY에 복사하여 붙여넣으십시오.\n- NewsData에 등록한 후, /api-key로 이동하여 API KEY를 확인한 후 이를 .env 파일의 NEWSDATAIO_KEY에 복사하여 붙여넣으십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지루한 부분은 끝났습니다. 이제 이러한 API에 액세스할 수 있고, 기사를 가져올 수 있습니다. 각 API에서 페이로드가 어떻게 보이는지 살펴봅시다:\n\n![image](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_6.png)\n\n## 사전 준비 조치 요약\n\nKafka 클러스터를 생성하고, 벡터 인덱스를 생성하고, 뉴스 API에 등록하는 이 3단계를 모두 마친 후에 .env 파일은 다음과 같은 모습이어야 합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nUPSTASH_KAFKA_UNAME=\"[여기에 사용자 이름 입력]\"\nUPSTASH_KAFKA_PASS=\"[여기에 암호 입력]\"\nUPSTASH_KAFKA_ENDPOINT=\"[여기에 엔드포인트 입력]\"\nUPSTASH_KAFKA_TOPIC=\"[토픽 이름 입력]\"\n\nUPSTASH_VECTOR_ENDPOINT=\"[벡터 엔드포인트 입력]\"\nUPSTASH_VECTOR_TOPIC=\"[벡터 이름 입력]\"\nUPSTASH_VECTOR_KEY=\"[벡터 토큰 입력]\"\n\nNEWSAPI_KEY=\"[NEWSAPI 키 입력]\"\nNEWSDATAIO_KEY=\"[NEWSDATA 키 입력]\"\nNEWS_TOPIC = \"news\" # 이것은 가져올 기사의 카테고리입니다\n\n다음 단계는 구현 세부 정보에 들어가기 전에 환경과 필수 패키지를 설치하는 것입니다.\n다음은 Makefile 설치 단계의 모습입니다:\n\n# Makefile\n...\ninstall:\n @echo \"$(GREEN) [CONDA] [$(ENV_NAME)] 파이썬 환경 생성 $(RESET)\"\n conda create --name $(ENV_NAME) python=3.9 -y\n @echo \"환경 활성화 중...\"\n @bash -c \"source $$(conda info --base)/etc/profile.d/conda.sh \u0026\u0026 conda activate $(ENV_NAME) \\\n   \u0026\u0026 pip install poetry \\\n   poetry env use $(which python)\"\n @echo \"패키지 설치 중\"\n @echo \"pyproject.toml 위치로 변경 중...\"\n @bash -c \" PYTHON_KEYRING_BACKEND=keyring.backends.fail.Keyring poetry install\"\n...\n\n환경을 준비하려면 make install을 실행하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 이 소스로부터 기사를 가져오는 핸들러 구현을 조사해 봅시다.\n\n# 데이터 수집\n\n이 모듈의 목적은 두 API를 쿼리하는 기능을 캡슐화하고, 페이로드를 구문 분석하여 두 페이로드에 모두 존재하는 속성을 사용하여 공통 문서 형식으로 포매팅하고, 클러스터로 메시지를 보내기 위해 공유 KafkaProducer 인스턴스를 사용하는 것입니다.\n\n자세히 살펴볼 내용은 다음 하위 모듈들입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Articles Fetching Manager Class\n- 카프카 클러스터로 메시지를 보내는 방법\n- Pydantic 데이터 모델\n- 파이프라인 실행\n\n## Articles Fetching Manager Class\n\n구현 내용에 대해 알아보겠습니다:\n\nimport datetime\nimport functools\nimport logging\nfrom typing import Callable, Dict, List\n\nfrom newsapi import NewsApiClient\nfrom newsdataapi import NewsDataApiClient\nfrom pydantic import ValidationError\n\nfrom models import NewsAPIModel, NewsDataIOModel\nfrom settings import settings\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n\ndef handle_article_fetching(func: Callable) -\u003e Callable:\n    \"\"\"\n    뉴스 기사 가져오기 기능에 대한 예외 처리를 담당하는 데코레이터입니다.\n\n    이 데코레이터는 기사 가져오기 기능을 감싸서 발생하는 예외를 catch하고 로깅합니다.\n    오류가 발생하면 오류를 기록하고 빈 목록을 반환합니다.\n\n    Args:\n        func (Callable): 감쌀 기사 가져오기 함수.\n\n    Returns:\n        Callable: 감싼 함수.\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except ValidationError as e:\n            logger.error(f\"기사 처리 중 유효성 검사 오류 발생: {e}\")\n        except Exception as e:\n            logger.error(f\"소스로부터 데이터를 가져오는 도중 오류 발생: {e}\")\n            logger.exception(e)\n        return []\n\n    return wrapper\n\nclass NewsFetcher:\n    \"\"\"\n    다양한 API에서 뉴스 기사를 가져오는 클래스입니다.\n\n    속성:\n        _newsapi (NewsApiClient): NewsAPI 클라이언트.\n        _newsdataapi (NewsDataApiClient): NewsDataAPI 클라이언트.\n\n    메서드:\n        fetch_from_newsapi(): NewsAPI로부터 기사 가져오기.\n        fetch_from_newsdataapi(): NewsDataAPI로부터 기사 가져오기.\n        sources: 호출 가능한 가져오기 함수 목록을 반환합니다.\n    \"\"\"\n\n    def __init__(self):\n        self._newsapi = NewsApiClient(api_key=settings.NEWSAPI_KEY)\n        self._newsdataapi = NewsDataApiClient(apikey=settings.NEWSDATAIO_KEY)\n\n    @handle_article_fetching\n    def fetch_from_newsapi(self) -\u003e List[Dict]:\n        \"\"\"NewsAPI에서 상위 뉴스 가져오기.\"\"\"\n        response = self._newsapi.get_everything(\n            q=settings.NEWS_TOPIC,\n            language=\"en\",\n            page=settings.ARTICLES_BATCH_SIZE,\n            page_size=settings.ARTICLES_BATCH_SIZE,\n        )\n        return [\n            NewsAPIModel(**article).to_common()\n            for article in response.get(\"articles\", [])\n        ]\n\n    @handle_article_fetching\n    def fetch_from_newsdataapi(self) -\u003e List[Dict]:\n        \"\"\"NewsDataAPI에서 뉴스 데이터 가져오기.\"\"\"\n        response = self._newsdataapi.news_api(\n            q=settings.NEWS_TOPIC,\n            language=\"en\",\n            size=settings.ARTICLES_BATCH_SIZE,\n        )\n        return [\n            NewsDataIOModel(**article).to_common()\n            for article in response.get(\"results\", [])\n        ]\n\n    @property\n    def sources(self) -\u003e List[callable]:\n        \"\"\"뉴스 가져오기 함수 목록입니다.\"\"\"\n        return [self.fetch_from_newsapi, self.fetch_from_newsdataapi]\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 구현에서 고려해야 할 몇 가지 중요 사항이 있습니다:\n\n- NewsAPIModel과 NewsDataIOModel은 특정 페이로드 형식에 익숙한 Pydantic 모델입니다.\n- 우리는 handle_article_fetching 데코레이터를 사용하여 원시 페이로드를 Pydantic 모델로 변환할 때 유효성 오류나 더 넓은 예외를 잡습니다.\n- 우리에게는 API를 쿼리하는 callable 메서드를 반환하는 sources라는 속성이 있습니다. 이것은 데이터 수집 모듈 내에서 사용될 것이며 멀티 프로듀서 스레드를 생성하여 Kafka 클러스터로 메시지를 전송합니다. 다음에 이어서 설명하겠습니다.\n\n## Kafka 메시지 생성\n\n다음에 우리가 구현할 작업 흐름입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_7.png\" /\u003e\n\n여기서 중요한 포인트들입니다:\n\n- API에서 가져오는 작업에 별도 스레드를 사용합니다.\n- 메시지를 보내기 위해 공통 카프카 프로듀서 인스턴스를 공유합니다.\n- 데이터 교환을 보증하기 위해 Pydantic 모델을 사용합니다.\n\n기사를 가져오는 데 별도 스레드를 사용하고, 클러스터로 메시지를 보내기 위해 단일 카프카 프로듀서 인스턴스를 사용하는 것이 우리의 사용 사례에서 권장되는 방법입니다. 그 이유는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 효율성 및 성능: KafkaProducer는 스레드 안전합니다. 새 인스턴스를 만드는 것은 네트워크 연결과 일부 설정이 필요합니다. 여러 스레드 간에 하나의 단일 인스턴스를 공유하면 이러한 작업과 관련된 오버헤드를 줄일 수 있습니다.\n- 처리량: 단일 프로듀서 인스턴스는 메시지를 Kafka 클러스터로 보내기 전에 메시지를 일괄 처리합니다.\n- 자원: 사용 사례에 완전히 적용되지는 않지만, 우리는 오직 2개의 프로듀서 스레드만 가지고 있기 때문에 인스턴스 수를 제한함으로써 시스템 자원 이용률을 최적화할 수 있습니다.\n\n여기 Kafka로 메시지 처리를 담당하는 주요 기능이 있습니다:\n\ndef run(self) -\u003e NoReturn:\n        \"\"\"지속적으로 Kafka 주제로 메시지를 가져와 보냅니다.\"\"\"\n        while self.running.is_set():\n            try:\n                messages: List[CommonDocument] = self.fetch_function()\n                if messages:\n                    messages = [msg.to_kafka_payload() for msg in messages]\n                    self.producer.send(self.topic, value=messages)\n                    self.producer.flush()\n                logger.info(\n                    f\"프로듀서 : {self.producer_id}이(가) {len(messages)}개의 메시지를 전송함.\"\n                )\n                time.sleep(self.wait_window_sec)\n            except Exception as e:\n                logger.error(f\"프로듀서 작업자 {self.producer_id}에서 오류 발생: {e}\")\n                self.running.clear()  # 오류 시 스레드를 중지합니다\n\n구현에서 고려해야 할 중요 사항:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 우리는 fetch sources의 수만큼 KafkaProducerThread 인스턴스가 생성됩니다.\n- 우리는 모든 스레드를 KafkaProducerSwarm 아래에 랩합니다.\n- 모든 스레드 사이에서 단일 KafkaProducer 인스턴스를 공유하며, 이는 클러스터와 통신할 것입니다.\n- 우리는 N개의 fetching 스레드로 확장할 수 있지만 여전히 단일 KafkaProducer 인스턴스를 유지하기 위해 싱글톤 디자인 패턴을 따릅니다.\n\n## Pydantic을 사용한 데이터 교환 모델\n\n위에서 제시한 코드 스니펫 구현에서, 이전에 설명되지 않았던 *Document, *Model 객체의 사용을 관찰했을 수 있습니다. 이 섹션에서 이들이 무엇인지 자세히 살펴보겠습니다.\n\n이들은 데이터 교환을 위한 Pydantic 모델들이며, 우리가 구축 중인 응용 프로그램 내에서 이러한 모델들은:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- NewsDataIOModel: NewsData API에서 가져온 원시 페이로드를 래핑하고 포맷합니다.\n- NewsAPIModel: NewsAPI API에서 가져온 원시 페이로드를 래핑하고 포맷합니다.\n- CommonDocument: 위에서 언급한 다양한 뉴스 형식 사이의 공통 형식을 설정합니다.\n- RefinedDocument: metadata 아래에 유용한 필드를 그룹화하고 기사 설명 텍스트와 같은 주요 필드를 강조하는 공통 형식을 필터링합니다.\n- ChunkedDocument: 텍스트를 청크로 나누고 chunk_id와 document_id 사이의 계보를 보장합니다.\n- EmbeddedDocument: 청크를 임베드하여 chunk_id와 document_id 사이의 계보를 보장합니다.\n\n예를 들어, 위 CommonDocument 모델은 다양한 뉴스 페이로드 형식 사이의 연결 역할을 나타내므로 이와 같이 구성됩니다:\n\nclass CommonDocument(BaseModel):\n    article_id: str = Field(default_factory=lambda: str(uuid4()))\n    title: str = Field(default_factory=lambda: \"N/A\")\n    url: str = Field(default_factory=lambda: \"N/A\")\n    published_at: str = Field(\n        default_factory=lambda: datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    )\n    source_name: str = Field(default_factory=lambda: \"Unknown\")\n    image_url: Optional[str] = Field(default_factory=lambda: None)\n    author: Optional[str] = Field(default_factory=lambda: \"Unknown\")\n    description: Optional[str] = Field(default_factory=lambda: None)\n    content: Optional[str] = Field(default_factory=lambda: None)\n\n    @field_validator(\"title\", \"description\", \"content\")\n    def clean_text_fields(cls, v):\n        if v is None or v == \"\":\n            return \"N/A\"\n        return clean_full(v)\n\n    @field_validator(\"url\", \"image_url\")\n    def clean_url_fields(cls, v):\n        if v is None:\n            return \"N/A\"\n        v = remove_html_tags(v)\n        v = normalize_whitespace(v)\n        return v\n\n    @field_validator(\"published_at\")\n    def clean_date_field(cls, v):\n        try:\n            parsed_date = parser.parse(v)\n            return parsed_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n        except (ValueError, TypeError):\n            logger.error(f\"Error parsing date: {v}, using current date instead.\")\n\n    @classmethod\n    def from_json(cls, data: dict) -\u003e \"CommonDocument\":\n        \"\"\"JSON 객체에서 CommonDocument를 만듭니다.\"\"\"\n        return cls(**data)\n\n    def to_kafka_payload(self) -\u003e dict:\n        \"\"\"Kafka 페이로드의 공통 표현을 준비합니다.\"\"\"\n        return self.model_dump(exclude_none=False)\n\n해석해보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 뉴스 기사 형식에 공통 속성 시리즈가 포함되어 있습니다.\n- 각 필드를 유효성 검사하거나 field_validator 데코레이터를 사용하여 기본값을 지정합니다.\n- to_kafka_payload 메서드는 메시지 직렬화를 보장하여 Kafka 클러스터로 전송하기 전에 처리합니다.\n\n## 텍스트 필드 클린업 프로세스\n\n클린업 프로세스는 간단합니다. 텍스트를 정리하고 다음을 보장하기 위해 메서드를 사용합니다:\n\n- 끝에 있는 공백이나 \\n, \\t를 제거합니다.\n- ul/li 목록 항목을 제거합니다.\n- 텍스트 내에 HTML 태그가 있으면 제거합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 이러한 변환을 간소화하기 위해 구조화되지 않은 [7] Python 라이브러리를 사용하고 있습니다.\n\n## KafkaProducers 실행\n\n지금까지 다음 모듈을 수행/구현했습니다:\n\n- 필요한 모든 서비스에 등록\n- Kafka 클러스터 및 벡터 데이터베이스 생성\n- 뉴스 기사 검색 핸들러 구현\n- 데이터 교환을 위한 Pydantic 모델 구현\n- KafkaProducer 로직 구현\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n작업이 완료되면 이제 안전하게 우리의 파이프라인에서 생산 단계를 실행하고 Upstash의 KafkaCluster에서 메시지를 확인할 수 있습니다.\n\n그럼 시작해봐요!\n프로젝트의 루트 디렉토리에서, Makefile에 데이터 수집을 실행하는 명령어가 있습니다:\n\n....\n\nrun_producers:\n @echo \"$(GREEN) [실행 중] 데이터 수집 파이프라인 Kafka 프로듀서 $(RESET)\"\n @bash -c \"poetry run python -m src.producer\"\n\n...\n\n이 🔗Makefile은 우리가 구축 중인 솔루션과 상호작용하기 위한 유용한 명령어가 포함되어 있습니다. 이 경우에는 make run_producers를 사용하여 run_producers를 실행해야 합니다. 이렇게 하면 KafkaSwarm이 시작되고 NewsAPIs에서 기사를 가져와 형식을 지정한 다음 Kafka 클러스터로 보내는 스레드를 다룰 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_8.png)\n\n로그를 통해 프로듀서 스레드가 각각 5개의 메시지를 보냈다는 것을 확인했습니다. 메시지들이 클러스터에 도달했는지 확인하려면 Upstash 콘솔로 이동하여 Kafka 클러스터 → 메시지를 확인하십시오. 다음과 같은 화면이 나타날 것입니다:\n\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_9.png)\n\n이 시점에서는 API에서 뉴스 기사를 가져와 형식을 맞춘 후 Kafka로 메시지를 보내는 데이터 수집 파이프라인의 구현 및 테스트가 완료되었습니다. 다음으로는 Kafka에서 새 메시지를 처리하는 \"컨슈머\" 또는 적재 파이프라인을 구현할 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 데이터 수집 파이프라인\n\n우리가 Kafka 주제에서 메시지를 받았다는 것을 확인한 후에는 \"소비자\" 파이프라인을 구현해야 합니다. 이는 다음을 의미합니다:\n\n- Kafka 주제에서 메시지 읽기\n- 파싱, 형식 지정, 청크화, 임베딩 생성\n- 벡터 객체 생성 및 Upstash Vector Index에 업서트\n\n![이미지](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이를 위해 Bytewax [4]를 사용하여 이러한 단계를 올바른 순서로 연결하는 DataFlow를 정의할 것입니다.\n\n바로 구현에 들어가서 주요 개념을 설명해보겠습니다!\n\n- Bytewax Flow에 입력으로 Kafka Source를 정의합니다.\n\n```js\nimport json\nfrom typing import List\n\nfrom bytewax.connectors.kafka import KafkaSinkMessage, KafkaSource\n\nfrom logger import get_logger\nfrom models import CommonDocument\nfrom settings import settings\n\nlogger = get_logger(__name__)\n\ndef build_kafka_stream_client():\n    \"\"\"\n    Build a Kafka stream client to read messages from the Upstash Kafka topic using the ByteWax KafkaSource connector.\n    \"\"\"\n    kafka_config = {\n        \"bootstrap.servers\": settings.UPSTASH_KAFKA_ENDPOINT,\n        \"security.protocol\": \"SASL_SSL\",\n        \"sasl.mechanisms\": \"SCRAM-SHA-256\",\n        \"sasl.username\": settings.UPSTASH_KAFKA_UNAME,\n        \"sasl.password\": settings.UPSTASH_KAFKA_PASS,\n        \"auto.offset.reset\": \"earliest\",  # Start reading at the earliest message\n    }\n    kafka_input = KafkaSource(\n        topics=[settings.UPSTASH_KAFKA_TOPIC],\n        brokers=[settings.UPSTASH_KAFKA_ENDPOINT],\n        add_config=kafka_config,\n    )\n    logger.info(\"KafkaSource client created successfully.\")\n    return kafka_input\n\ndef process_message(message: KafkaSinkMessage):\n    \"\"\"\n    On a Kafka message, process the message and return a list of CommonDocuments.\n    - message: KafkaSinkMessage(key, value) where value is the message payload.\n    \"\"\"\n    documents: List[CommonDocument] = []\n    try:\n        json_str = message.value.decode(\"utf-8\")\n        data = json.loads(json_str)\n        documents = [CommonDocument.from_json(obj) for obj in data]\n        logger.info(f\"Decoded into {len(documents)} CommonDocuments\")\n        return documents\n    except StopIteration:\n        logger.info(\"No more documents to fetch from the client.\")\n    except KeyError as e:\n        logger.error(f\"Key error in processing document batch: {e}\")\n    except json.JSONDecodeError as e:\n        logger.error(f\"Error decoding JSON from message: {e}\")\n        raise\n    except Exception as e:\n        logger.exception(f\"Unexpected error in next_batch: {e}\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 구현에서 중요한 점들:\n\n- build_kafka_stream_client : 미리 정의된 Bytewax KafkaSource 커넥터를 사용하여 KafkaConsumer의 인스턴스를 생성합니다.\n- process_message : Kafka Topic에서 메시지를 처리할 콜백 함수입니다.\n\n2. Bytewax 플로우의 출력으로 Upstash Vector (Index)를 정의합니다.\n\n```js\nfrom typing import Optional, List\n\nfrom bytewax.outputs import DynamicSink, StatelessSinkPartition\nfrom upstash_vector import Index, Vector\nfrom models import EmbeddedDocument\nfrom settings import settings\nfrom logger import get_logger\n\n\nlogger = get_logger(__name__)\n\n\nclass UpstashVectorOutput(DynamicSink):\n    \"\"\"Upstash 벡터 출력을 나타내는 클래스입니다.\n\n    이 클래스는 at-least-once 처리를 지원하는 동적 출력 유형인 Upstash 벡터 출력을 생성하는 데 사용됩니다.\n    resume 이후의 메시지는 resume 즉각적으로 복제됩니다.\n\n    Args:\n        vector_size (int): 벡터의 크기.\n        collection_name (str, optional): 컬렉션의 이름입니다. 기본값은 constants.VECTOR_DB_OUTPUT_COLLECTION_NAME입니다.\n        client (Optional[UpstashClient], optional): Upstash 클라이언트입니다. 기본값은 None입니다.\n    \"\"\"\n\n    def __init__(\n        self,\n        vector_size: int = settings.EMBEDDING_MODEL_MAX_INPUT_LENGTH,\n        collection_name: str = settings.UPSTASH_VECTOR_TOPIC,\n        client: Optional[Index] = None,\n    ):\n        self._collection_name = collection_name\n        self._vector_size = vector_size\n\n        if client:\n            self.client = client\n        else:\n            self.client = Index(\n                url=settings.UPSTASH_VECTOR_ENDPOINT,\n                token=settings.UPSTASH_VECTOR_KEY,\n                retries=settings.UPSTASH_VECTOR_RETRIES,\n                retry_interval=settings.UPSTASH_VECTOR_WAIT_INTERVAL,\n            )\n\n    def build(\n        self, step_id: str, worker_index: int, worker_count: int\n    ) -\u003e StatelessSinkPartition:\n        return UpstashVectorSink(self.client, self._collection_name)\n\n\nclass UpstashVectorSink(StatelessSinkPartition):\n    \"\"\"\n    Upstash Vector 데이터베이스 컬렉션에 문서 임베딩을 작성하는 싱크입니다.\n    이 구현은 효율성을 높이기 위해 배치 업서트를 활용하며, 오류 처리 및 로깅을 향상시키고 가독성 및 유지 보수성을 위해 Pythonic한 모법을 따릅니다.\n\n    Args:\n        client (Index): 쓰기에 사용할 Upstash Vector 클라이언트입니다.\n        collection_name (str, optional): 쓸 컬렉션의 이름입니다. 기본값은 UPSTASH_VECTOR_TOPIC 환경 변수의 값입니다.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Index,\n        collection_name: str = None,\n    ):\n        self._client = client\n        self._collection_name = collection_name\n        self._upsert_batch_size = settings.UPSTASH_VECTOR_UPSERT_BATCH_SIZE\n\n    def write_batch(self, documents: List[EmbeddedDocument]):\n        \"\"\"\n        구성된 Upstash Vector 데이터베이스 컬렉션에 문서 임베딩의 배치를 작성합니다.\n\n        Args:\n            documents (List[EmbeddedDocument]): 쓸 문서들입니다.\n        \"\"\"\n        vectors = [\n            Vector(id=doc.doc_id, vector=doc.embeddings, metadata=doc.metadata)\n            for doc in documents\n        ]\n\n        # 효율성을 위한 배치 업서트\n        for i in range(0, len(vectors), self._upsert_batch_size):\n            batch_vectors = vectors[i : i + self._upsert_batch_size]\n            try:\n                self._client.upsert(vectors=batch_vectors)\n            except Exception as e:\n                logger.error(f\"배치 업서트 중 예외 발생 {e}\")\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 구현에서 중요한 사항들입니다:\n\n- UpstashVectorOutput: 다양한 대상으로 데이터를 전달하기 위해 설계된 Bytewax DynamicSink 추상화를 인스턴스화합니다. 우리의 경우, 이는 Upstash Vector Index 클라이언트 연결 위에 래핑될 것입니다.\n- UpstashVectorSink: 우리의 DynamicSink을 래핑하고 업서트 벡터를 우리의 VectorDatabase에 처리하는 기능을 담당합니다. 이 StatelessSinkPartition은 DynamicSink가 어떠한 상태도 유지하지 않고 우리의 Sink에 대한 모든 입력을 write_batch 기능 구현에 따라 처리합니다.\n\n## 나머지 Bytewax Flow 빌드하기\n\n여기 Upstash Kafka Topic에서 메시지를 가져와 정제, 수정, 분할, 삽입하고 Upstash Vector Index에 벡터를 업서트하는 저희 DataFlow의 전체 구현입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n\"\"\"\n    이 스크립트는 Upstash 사용 사례에 대한 ByteWax 데이터플로 구현을 정의합니다.\n    데이터플로에는 다음 단계가 포함되어 있습니다:\n        1. 입력: 카프카 스트림에서 데이터를 읽기\n        2. 정제: 입력 데이터를 공통 형식으로 변환 \n        3. 청크 분리: 입력 데이터를 더 작은 청크로 분리\n        4. 임베드: 입력 데이터에 대한 임베딩 생성\n        5. 출력: 출력 데이터를 Upstash 벡터 데이터베이스에 쓰기\n\"\"\"\n\nfrom pathlib import Path\nfrom typing import Optional\n\nimport bytewax.operators as op\nfrom vector import UpstashVectorOutput\nfrom consumer import process_message, build_kafka_stream_client\nfrom bytewax.connectors.kafka import KafkaSource\nfrom bytewax.dataflow import Dataflow\nfrom bytewax.outputs import DynamicSink\nfrom embeddings import TextEmbedder\nfrom models import ChunkedDocument, EmbeddedDocument, RefinedDocument\nfrom logger import get_logger\n\nlogger = get_logger(__name__)\n\n\ndef build(\n    model_cache_dir: Optional[Path] = None,\n) -\u003e Dataflow:\n    \"\"\"\n    Upstash 사용 사례에 대한 ByteWax 데이터플로를 구축합니다.\n    다음과 같은 데이터플로를 따릅니다:\n        * 1. Tag: ['kafka_input']   = KafkaSource에서 입력 데이터 읽기\n        * 2. Tag: ['map_kinp']      = KafkaSource에서 CommonDocument로 메시지 처리\n            * 2.1 [Optional] Tag ['dbg_map_kinp'] = ['map_kinp'] 후 디버깅\n        * 3. Tag: ['refine']        = 메시지를 정제된 문서 형식으로 변환\n            * 3.1 [Optional] Tag ['dbg_refine'] = ['refine'] 후 디버깅\n        * 4. Tag: ['chunkenize']    = 정제된 문서를 더 작은 청크로 나누기\n            * 4.1 [Optional] Tag ['dbg_chunkenize'] = ['chunkenize'] 후 디버깅\n        * 5. Tag: ['embed']         = 청크에 대한 임베딩 생성\n            * 5.1 [Optional] Tag ['dbg_embed'] = ['embed'] 후 디버깅\n        * 6. Tag: ['output']        = 임베딩을 Upstash 벡터 데이터베이스에 쓰기\n    노트:\n        각 선택적 태그는 문제 해결을 위해 활성화할 수 있는 디버깅 단계입니다.\n    \"\"\"\n    model = TextEmbedder(cache_dir=model_cache_dir)\n\n    dataflow = Dataflow(flow_id=\"news-to-upstash\")\n    stream = op.input(\n        step_id=\"kafka_input\",\n        flow=dataflow,\n        source=_build_input(),\n    )\n    stream = op.flat_map(\"map_kinp\", stream, process_message)\n    # _ = op.inspect(\"dbg_map_kinp\", stream)\n    stream = op.map(\"refine\", stream, RefinedDocument.from_common)\n    # _ = op.inspect(\"dbg_refine\", stream)\n    stream = op.flat_map(\n        \"chunkenize\",\n        stream,\n        lambda refined_doc: ChunkedDocument.from_refined(refined_doc, model),\n    )\n    # _ = op.inspect(\"dbg_chunkenize\", stream)\n    stream = op.map(\n        \"embed\",\n        stream,\n        lambda chunked_doc: EmbeddedDocument.from_chunked(chunked_doc, model),\n    )\n    # _ = op.inspect(\"dbg_embed\", stream)\n    stream = op.output(\"output\", stream, _build_output())\n    logger.info(\"성공적으로 ByteWax 데이터플로를 생성했습니다.\")\n    logger.info(\n        \"\\t단계: Kafka 입력 -\u003e 매핑 -\u003e 정제 -\u003e 청크 분리 -\u003e 임베딩 -\u003e 업로드\"\n    )\n    return dataflow\n\n\ndef _build_input() -\u003e KafkaSource:\n    return build_kafka_stream_client()\n\n\ndef _build_output() -\u003e DynamicSink:\n    return UpstashVectorOutput()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- kafka_input: 카프카 메시지를 가져와 CommonDocument Pydantic 형식으로 변환하는 단계입니다.\n- map_kinp: 카프카 입력을 의미하며, 수신된 메시지에 flat map을 적용하여 List[CommonDocument] Pydantic 객체를 생성합니다.\n- refine: List[CommonDocument]를 순회하고 RefinedDocument 인스턴스를 생성하는 단계입니다.\n- chunkenize: List[RefinedDocument]를 순회하고 ChunkedDocument 인스턴스를 생성하는 단계입니다.\n- embed: List[ChunkedDocuments]를 순회하고 EmbeddedDocument 인스턴스를 생성하는 단계입니다.\n- output: List[EmbeddedDocument]를 순회하고 Vector 객체를 생성하여 Upstash Vector Index에 업서트하는 단계입니다.\n\n# 파이프라인 시작\n\n지금까지 구현한 것은 다음과 같습니다:\n\n- 데이터 수집 파이프라인: 주기적으로 NewsAPI에서 원시 페이로드를 가져와 형식을 지정한 뒤, 카프카 토픽으로 메시지를 전송하는 단계입니다.\n- 인제션 파이프라인: 이는 Bytewax DataFlow로, 카프카 토픽에 연결되어 메시지를 소비하고, 최종적으로 벡터를 업서트하는 Vector 데이터베이스에 업데이트합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n프로젝트 루트에있는 Makefile에서 미리 정의된 명령을 사용하여 이 두 서비스를 모두 시작할 수 있습니다:\n\n```js\n# 카프카 메시지를 생성하기 위해 데이터 수집 파이프라인 실행\nmake run_producers\n\n# 카프카 메시지를 소비하고 벡터를 업데이트하기 위해 인제스처리 파이프라인 실행\nmake run_pipeline\n```\n\n그리고... 완료되었습니다!\n성공적으로 프로듀서/컨슈머 서비스를 시작했습니다.\n남은 모듈은 UI입니다. 벡터 데이터베이스와 뉴스 기사를 검색하는 데 상호 작용하는 데 사용됩니다.\n\n# 사용자 인터페이스\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nUI는 다음과 같은 기능을 갖춘 기본 Streamlit [8] 애플리케이션입니다:\n\n- 텍스트 검색 창\n- 벡터 데이터베이스에서 가져온 기사로 채워진 카드가 표시되는 div 섹션\n\n카드에는 다음과 같은 데이터 필드가 포함되어 있습니다:\n\n- 발행일\n- 유사도 점수\n- 기사 이미지\n- SeeMore 버튼을 클릭하면 원본 기사 URL로 이동합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한번 메시지/질문을 텍스트 상자에 입력하면 입력이 정리되고 (소문자로 변환, 비ASCII 문자 제거 등) 그리고 삽입됩니다. 새로운 삽입물을 사용하여 벡터 데이터베이스를 쿼리하여 가장 유사한 항목을 가져옵니다. 그 결과는 구성되어 렌더링될 것입니다.\n\n다음은 예시입니다:\n\n\n![How to build a real-time News Search Engine using VectorDBs - Part 1](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_11.png)\n\n![How to build a real-time News Search Engine using VectorDBs - Part 2](/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_12.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n축하합니다!\n\n성공했습니다! 멋진 프로젝트만큼이나 라이브로 출시할 준비가 된 뉴스 검색 엔진을 만들었습니다. 단순히 무작정 던지는 것이 아니라, 우리는 최고의 소프트웨어 개발 관행을 따르기도 했습니다.\n\nPydantic을 사용하여 데이터를 잘 처리했고, 유닛 테스트를 작성하고, 스레딩을 활용하여 작업을 가속화했으며, Upstash의 서버리스 카프카와 벡터 데이터베이스를 활용하여 파이프라인을 쉽게 설정할 뿐만 아니라 빠르고 확장 가능하며 오류 대비 가능하도록 만들었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 당신은 이 청사진을 대부분의 데이터 기반 아이디어에 적용할 수 있는 능력을 갖게 되었어요. 이건 이 프로젝트뿐만 아니라 앞으로 만들게 될 멋진 것들을 위한 큰 승리에요.\n\n# 참고 자료\n\n[1] News Search Engine using Upstash Vector — Decoding ML Github (2024)\n[2] Upstash Serverless Kafka\n[3] Upstash Serverless Vector Database\n[4] Bytewax Stream Processing with Python\n[5] Singleton Pattern\n[6] sentence-transformers/all-MiniLM-L6-v2\n[7] unstructured Python Library\n[8] Streamlit Python\n\n# 더 읽을 거리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글과 관련성 순으로 정렬되었습니다.","ogImage":{"url":"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_0.png"},"coverImage":"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_0.png","tag":["Tech"],"readingTime":28},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e아파치 카프카, 바이트왁스, 그리고 업스태시 벡터 데이터베이스를 활용한 라이브 뉴스 집계 스트리밍 파이프라인 구현을 위한 실용적인 안내서입니다.\u003c/p\u003e\n\u003cp\u003eearthweb.com에서 실시한 연구에 따르면, 매일 온라인 및 오프라인에서 유입되는 뉴스 기사는 200-300만 건 사이에 있다고 합니다!\u003c/p\u003e\n\u003cp\u003e모든 방향에서 우리에게 쏟아지는 뉴스로 인해 정신없을 때가 많습니다. 그래서 우리는 실제로 관심 있는 뉴스를 빠르게 받아볼 수 있는 짧고 빠른 방법을 찾고 있습니다.\u003c/p\u003e\n\u003cp\u003e본 문서에서는 이러한 문제를 해결하고자 합니다! 좀 더 구체적으로, 여러 출처로부터 데이터를 수집하고 해당 정보 채널을 당신의 관심사에 맞는 종단점으로 좁힐 수 있는 시스템을 구축할 것입니다 — 뉴스 검색 엔진입니다!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이론에 대해서만 이야기하거나 이러한 시스템을 구현하는 방법을 알려주는 것이 아니라, 우리는 단계별로 설명하고 보여줄 거예요!\u003c/p\u003e\n\u003cp\u003e시작하기 전에, 이 기사에서 제공하는 모든 것은 Decoding ML Articles GitHub 저장소에서 완전한 코드 지원을 받습니다.\u003c/p\u003e\n\u003ch1\u003e목차\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e아키텍처 개요\u003c/li\u003e\n\u003cli\u003e도구 고려 사항\u003c/li\u003e\n\u003cli\u003e전제 조건\n3.1 새로운 Upstash Kafka 클러스터 생성\n3.2 새로운 Upstash Vector 인덱스 생성\n3.3 2개의 라이브 뉴스 API에 등록\n3.4 설치\u003c/li\u003e\n\u003cli\u003e데이터 수집\n4.1 기사 가져오기 관리자\n4.2 Kafka 메시지 제작\n4.3 Pydantic을 사용한 데이터 교환 모델\n4.4 KafkaProducers 실행\u003c/li\u003e\n\u003cli\u003e수집 파이프라인\n5.1 Kafka에서 메시지 수신\n5.2 Bytewax 데이터플로 구현\n5.3 기사 정제, 형식 지정, 청크화, 삽입\n5.4 벡터 작성 및 VectorDB에 업서트\u003c/li\u003e\n\u003cli\u003e파이프라인 시작\u003c/li\u003e\n\u003cli\u003e사용자 인터페이스\u003c/li\u003e\n\u003cli\u003e결론\u003c/li\u003e\n\u003cli\u003e참고문헌\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e아키텍처 개요\u003c/h1\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e요약하자면, 우리는 뉴스 API에서 뉴스 기사를 가져와서 생생한 시스템을 구축할 것입니다. 가져온 데이터를 정의된 형식으로 파싱하고 포맷팅한 다음 첫 번째 단계로 Kafka 토픽에 메시지를 직렬화하고 스트리밍할 것입니다.\n두 번째 단계에서는 Bytewax를 사용하여 Kafka 토픽에서 메시지를 더 청소, 파싱, 청크, 임베드, 벡터를 업서팅하여 벡터 데이터베이스로 보내고, 데이터베이스와 상호 작용할 수 있는 UI로 마무리됩니다.\u003c/p\u003e\n\u003ch1\u003e툴 고려 사항\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e올바른 도구를 선택하는 것이 고성능, 확장성, 및 구현 용이성을 달성하는 핵심이었습니다. 프로젝트 전체에서 사용된 도구를 살펴보겠습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUpstash Serverless Kafka: 인프라 관리에 대해 걱정할 필요 없이 강력하고 확장 가능한 이벤트 스트리밍을 위해 사용됩니다.\u003c/li\u003e\n\u003cli\u003ePython 스레딩: 여러 뉴스 API에서 동시에 데이터를 가져오면서 스레드 안전한 Kafka Producer 인스턴스를 공유하여 메모리 풋프린트와 성능을 최적화합니다.\u003c/li\u003e\n\u003cli\u003ePydantic 모델: 일관된 및 유효한 데이터 교환 구조를 보장하기 위해 사용됩니다.\u003c/li\u003e\n\u003cli\u003eBytewax: 스트리밍 데이터를 처리하고 변환하는 데 간편하고 빠른 속도를 제공하기 때문에 사용됩니다.\u003c/li\u003e\n\u003cli\u003eUpstash Vector Database: Serverless로 구성이 쉽고 Python, JS, 및 GO 내에서 쉽게 통합됩니다. UI 콘솔 대시보드에서 풍부한 탐색 옵션과 실시간 상태 메트릭을 제공하는 것이 큰 장점입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e하드웨어 요구 사항에 따르면 GPU는 필요하지 않습니다.\u003c/p\u003e\n\u003cp\u003e비용은 얼마입니까? — 무료입니다.\n이 안내서는 무료 티어 플랜만 사용하도록 설정했으므로 사용한 플랫폼에 대해 지불할 필요가 없습니다!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e준비 사항\u003c/h1\u003e\n\u003cp\u003e어떠한 구현을 하기 전에, 각 서비스에 접근할 수 있는지 확인해야 합니다. 따라서 다음을 설정해야 합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e새로운 Upstash Kafka 클러스터\u003c/li\u003e\n\u003cli\u003e새로운 Upstash Vector Index\u003c/li\u003e\n\u003cli\u003e뉴스 API 등록\u003c/li\u003e\n\u003cli\u003e환경 설치\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e처음 시작할 때는 약 5분 정도 걸립니다. 함께 해보세요!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e새로운 Upstash Kafka 클러스터 생성\u003c/h2\u003e\n\u003cp\u003e먼저 Upstash에 등록해야 합니다. 로그인 후에 다음과 같은 대시보드가 나타납니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_1.png\" alt=\"대시보드 이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e다음으로 상단 바에서 Kafka를 선택하고 + 클러스터 생성 버튼을 클릭하여 새 클러스터를 만들어야 합니다. 클릭하면 다음 모달이 나타납니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_2.png\"\u003e\n\u003cp\u003e클러스터에 이름을 지정하고 본인의 위치와 가장 가까운 지역을 선택한 후, 클러스터 생성을 클릭하여 완료하세요. 완료되면 새로운 Kafka 클러스터가 아래에 표시됩니다. 새로운 Kafka 클러스터를 선택하고 아래 화면으로 이동하게 됩니다:\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_3.png\"\u003e\n\u003cp\u003e이 뷰에서 주요 구성 요소를 살펴보겠습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e상세 정보: 클러스터 개요 및 Upstash가 제공하는 기능을 보여줍니다.\u003c/li\u003e\n\u003cli\u003e사용량: 생성/소비된 메시지 수, 비용 영향 등을 보여주는 차트입니다.\u003c/li\u003e\n\u003cli\u003e주제: 이 탭에서는 Kafka 주제를 만들고 세부 정보를 모니터링할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e클러스터를 생성한 다음 해야 할 다음 단계는 메시지를 생성(보내기)하고 소비(받기)할 수 있는 주제를 정의하는 것입니다.\u003c/p\u003e\n\u003cp\u003e주제 탭 아래에서 \"주제 생성\"을 선택하면 다음과 같은 화면이 나타납니다:\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_4.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e주제 이름을 지정하고, 나머지는 기본 상태로 두어서 Create를 클릭하세요.\u003c/p\u003e\n\u003cp\u003e카프카 클러스터를 성공적으로 생성했습니다. 이제 클러스터에 연결하는 데 도움이 되는 환경 변수를 복사하고 설정해야 합니다. 이를 위해 클러스터 대시보드로 이동하여 세부 정보 탭에서 엔드포인트, 사용자 이름 및 비밀번호를 복사하여 .env 파일에 붙여넣으세요.\n그 후, Topics로 이동하여 카프카 토픽 이름을 복사하세요.\u003c/p\u003e\n\u003cp\u003e지금까지 .env 파일이어야 하는 모습은 다음과 같습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_KAFKA_UNAME\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[사용자 이름]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_KAFKA_PASS\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[비밀번호]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_KAFKA_ENDPOINT\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[엔드포인트]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_KAFKA_TOPIC\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[토픽 이름]\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e새 Upstash Vector 색인 만들기\u003c/h2\u003e\n\u003cp\u003e이제 새로운 Vector 데이터베이스를 만들어 보겠습니다. 이를 위해 대시보드에서 Vector를 선택하고 + Index 작성을 클릭하세요. 그러면 다음 뷰로 이동됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_5.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e벡터 데이터베이스에 이름을 할당하고, 위치에 가장 가까운 지역을 선택한 다음 Embedding을 생성할 때 사용할 모델로 sentence-transformers/all-MiniLM-L6-v2을 선택하세요. 뉴스 기사의 임베딩을 생성할 때 사용할 모델과 벡터 간 거리 비교에 코사인 유사도 메트릭을 사용할 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e새로운 Vector Index를 만든 후에는 Kafka Cluster와 동일한 작업 흐름을 따를 수 있습니다. Index Name, Endpoint, Token을 복사하고 .env 파일에 붙여넣기하세요.\u003c/p\u003e\n\u003cp\u003e현재 .env 파일은 다음과 같이 보여야 합니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_KAFKA_UNAME\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[여기에 사용자명 입력]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_KAFKA_PASS\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[여기에 비밀번호 입력]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_KAFKA_ENDPOINT\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[여기에 엔드포인트 입력]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_KAFKA_TOPIC\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[여기에 토픽 이름 입력]\"\u003c/span\u003e\n\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_VECTOR_ENDPOINT\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[Vector 엔드포인트 입력]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_VECTOR_TOPIC\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[Vector 이름 입력]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_VECTOR_KEY\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[Vector 토큰 입력]\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e뉴스 API에 등록하기\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다음 APIs를 사용하여 기사를 가져올 예정입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e🔗 NewsAPI\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e하루에 100번의 API 호출을 할 수 있는 무료 개발자 플랜을 제공합니다.\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e🔗 NewsData\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e무료 요금제가 제공되며 하루에 200개의 크레딧을 받습니다. 각 크레딧은 10개의 기사와 동일하며, 이는 하루에 총 2000개의 기사를 가져올 수 있다는 것을 의미합니다.\u003c/p\u003e\n\u003cp\u003e저희 현재의 사용 사례에는 이 API들이 충분한 기능을 제공하여 구축 중인 뉴스 검색 엔진을 구현하고 유효성을 검사할 수 있습니다. 동시에 기존 워크플로우가 동일하게 유지되므로 개선 및 확장할 여지도 남겨두고 있습니다.\n무료 요금제에 따른 유일한 제약은 타임드-배치 페치를 수행할 수 없다는 것입니다. 즉, 이 API들을 쿼리할 때 from_date, to_date를 사용할 수 없습니다. 하지만 이는 문제가 되지 않습니다.\n대신 페치 호출 간의 대기 시간을 이용하여 이 동작을 모방할 예정입니다.\u003c/p\u003e\n\u003cp\u003e다음 단계는 두 플랫폼에 등록하는 것입니다 — 걱정 마세요, 가능한 간단합니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNewsAPI에 등록한 후, /account로 이동하여 API_KEY 필드를 확인한 후 이를 .env 파일의 NEWSAPI_KEY에 복사하여 붙여넣으십시오.\u003c/li\u003e\n\u003cli\u003eNewsData에 등록한 후, /api-key로 이동하여 API KEY를 확인한 후 이를 .env 파일의 NEWSDATAIO_KEY에 복사하여 붙여넣으십시오.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e지루한 부분은 끝났습니다. 이제 이러한 API에 액세스할 수 있고, 기사를 가져올 수 있습니다. 각 API에서 페이로드가 어떻게 보이는지 살펴봅시다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_6.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch2\u003e사전 준비 조치 요약\u003c/h2\u003e\n\u003cp\u003eKafka 클러스터를 생성하고, 벡터 인덱스를 생성하고, 뉴스 API에 등록하는 이 3단계를 모두 마친 후에 .env 파일은 다음과 같은 모습이어야 합니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_KAFKA_UNAME\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[여기에 사용자 이름 입력]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_KAFKA_PASS\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[여기에 암호 입력]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_KAFKA_ENDPOINT\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[여기에 엔드포인트 입력]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_KAFKA_TOPIC\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[토픽 이름 입력]\"\u003c/span\u003e\n\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_VECTOR_ENDPOINT\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[벡터 엔드포인트 입력]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_VECTOR_TOPIC\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[벡터 이름 입력]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eUPSTASH_VECTOR_KEY\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[벡터 토큰 입력]\"\u003c/span\u003e\n\n\u003cspan class=\"hljs-variable constant_\"\u003eNEWSAPI_KEY\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[NEWSAPI 키 입력]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eNEWSDATAIO_KEY\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"[NEWSDATA 키 입력]\"\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eNEWS_TOPIC\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e\"news\"\u003c/span\u003e # 이것은 가져올 기사의 카테고리입니다\n\n다음 단계는 구현 세부 정보에 들어가기 전에 환경과 필수 패키지를 설치하는 것입니다.\n다음은 \u003cspan class=\"hljs-title class_\"\u003eMakefile\u003c/span\u003e 설치 단계의 모습입니다:\n\n# \u003cspan class=\"hljs-title class_\"\u003eMakefile\u003c/span\u003e\n...\n\u003cspan class=\"hljs-attr\"\u003einstall\u003c/span\u003e:\n @echo \u003cspan class=\"hljs-string\"\u003e\"$(GREEN) [CONDA] [$(ENV_NAME)] 파이썬 환경 생성 $(RESET)\"\u003c/span\u003e\n conda create --name $(\u003cspan class=\"hljs-variable constant_\"\u003eENV_NAME\u003c/span\u003e) python=\u003cspan class=\"hljs-number\"\u003e3.9\u003c/span\u003e -y\n @echo \u003cspan class=\"hljs-string\"\u003e\"환경 활성화 중...\"\u003c/span\u003e\n @bash -c \u003cspan class=\"hljs-string\"\u003e\"source $$(conda info --base)/etc/profile.d/conda.sh \u0026#x26;\u0026#x26; conda activate $(ENV_NAME) \\\n   \u0026#x26;\u0026#x26; pip install poetry \\\n   poetry env use $(which python)\"\u003c/span\u003e\n @echo \u003cspan class=\"hljs-string\"\u003e\"패키지 설치 중\"\u003c/span\u003e\n @echo \u003cspan class=\"hljs-string\"\u003e\"pyproject.toml 위치로 변경 중...\"\u003c/span\u003e\n @bash -c \u003cspan class=\"hljs-string\"\u003e\" PYTHON_KEYRING_BACKEND=keyring.backends.fail.Keyring poetry install\"\u003c/span\u003e\n...\n\n환경을 준비하려면 make install을 실행하세요.\n\n\u0026#x3C;div \u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"content-ad\"\u003c/span\u003e\u003e\u0026#x3C;/div\u003e\n\n이제 이 소스로부터 기사를 가져오는 핸들러 구현을 조사해 봅시다.\n\n# 데이터 수집\n\n이 모듈의 목적은 두 \u003cspan class=\"hljs-variable constant_\"\u003eAPI\u003c/span\u003e를 쿼리하는 기능을 캡슐화하고, 페이로드를 구문 분석하여 두 페이로드에 모두 존재하는 속성을 사용하여 공통 문서 형식으로 포매팅하고, 클러스터로 메시지를 보내기 위해 공유 \u003cspan class=\"hljs-title class_\"\u003eKafkaProducer\u003c/span\u003e 인스턴스를 사용하는 것입니다.\n\n자세히 살펴볼 내용은 다음 하위 모듈들입니다:\n\n\u003cspan class=\"xml\"\u003e\u003cspan class=\"hljs-tag\"\u003e\u0026#x3C;\u003cspan class=\"hljs-name\"\u003ediv\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eclass\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"content-ad\"\u003c/span\u003e\u003e\u003c/span\u003e\u003cspan class=\"hljs-tag\"\u003e\u0026#x3C;/\u003cspan class=\"hljs-name\"\u003ediv\u003c/span\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\n- \u003cspan class=\"hljs-title class_\"\u003eArticles\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eFetching\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eManager\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eClass\u003c/span\u003e\n- 카프카 클러스터로 메시지를 보내는 방법\n- \u003cspan class=\"hljs-title class_\"\u003ePydantic\u003c/span\u003e 데이터 모델\n- 파이프라인 실행\n\n## \u003cspan class=\"hljs-title class_\"\u003eArticles\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eFetching\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eManager\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eClass\u003c/span\u003e\n\n구현 내용에 대해 알아보겠습니다:\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e datetime\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e functools\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e logging\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e typing \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eCallable\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eDict\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eList\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e newsapi \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eNewsApiClient\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e newsdataapi \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eNewsDataApiClient\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e pydantic \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eValidationError\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e models \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eNewsAPIModel\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eNewsDataIOModel\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e settings \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e settings\n\nlogging.\u003cspan class=\"hljs-title function_\"\u003ebasicConfig\u003c/span\u003e(level=logging.\u003cspan class=\"hljs-property\"\u003eINFO\u003c/span\u003e)\nlogger = logging.\u003cspan class=\"hljs-title function_\"\u003egetLogger\u003c/span\u003e(__name__)\nlogger.\u003cspan class=\"hljs-title function_\"\u003esetLevel\u003c/span\u003e(logging.\u003cspan class=\"hljs-property\"\u003eDEBUG\u003c/span\u003e)\n\n\ndef \u003cspan class=\"hljs-title function_\"\u003ehandle_article_fetching\u003c/span\u003e(\u003cspan class=\"hljs-attr\"\u003efunc\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eCallable\u003c/span\u003e) -\u003e \u003cspan class=\"hljs-title class_\"\u003eCallable\u003c/span\u003e:\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    뉴스 기사 가져오기 기능에 대한 예외 처리를 담당하는 데코레이터입니다.\n\n    이 데코레이터는 기사 가져오기 기능을 감싸서 발생하는 예외를 catch하고 로깅합니다.\n    오류가 발생하면 오류를 기록하고 빈 목록을 반환합니다.\n\n    Args:\n        func (Callable): 감쌀 기사 가져오기 함수.\n\n    Returns:\n        Callable: 감싼 함수.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    @functools.\u003cspan class=\"hljs-title function_\"\u003ewraps\u003c/span\u003e(func)\n    def \u003cspan class=\"hljs-title function_\"\u003ewrapper\u003c/span\u003e(*args, **kwargs):\n        \u003cspan class=\"hljs-attr\"\u003etry\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003efunc\u003c/span\u003e(*args, **kwargs)\n        except \u003cspan class=\"hljs-title class_\"\u003eValidationError\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ee\u003c/span\u003e:\n            logger.\u003cspan class=\"hljs-title function_\"\u003eerror\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"기사 처리 중 유효성 검사 오류 발생: {e}\"\u003c/span\u003e)\n        except \u003cspan class=\"hljs-title class_\"\u003eException\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ee\u003c/span\u003e:\n            logger.\u003cspan class=\"hljs-title function_\"\u003eerror\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"소스로부터 데이터를 가져오는 도중 오류 발생: {e}\"\u003c/span\u003e)\n            logger.\u003cspan class=\"hljs-title function_\"\u003eexception\u003c/span\u003e(e)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e []\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e wrapper\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eNewsFetcher\u003c/span\u003e:\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    다양한 API에서 뉴스 기사를 가져오는 클래스입니다.\n\n    속성:\n        _newsapi (NewsApiClient): NewsAPI 클라이언트.\n        _newsdataapi (NewsDataApiClient): NewsDataAPI 클라이언트.\n\n    메서드:\n        fetch_from_newsapi(): NewsAPI로부터 기사 가져오기.\n        fetch_from_newsdataapi(): NewsDataAPI로부터 기사 가져오기.\n        sources: 호출 가능한 가져오기 함수 목록을 반환합니다.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self):\n        self.\u003cspan class=\"hljs-property\"\u003e_newsapi\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eNewsApiClient\u003c/span\u003e(api_key=settings.\u003cspan class=\"hljs-property\"\u003eNEWSAPI_KEY\u003c/span\u003e)\n        self.\u003cspan class=\"hljs-property\"\u003e_newsdataapi\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eNewsDataApiClient\u003c/span\u003e(apikey=settings.\u003cspan class=\"hljs-property\"\u003eNEWSDATAIO_KEY\u003c/span\u003e)\n\n    @handle_article_fetching\n    def \u003cspan class=\"hljs-title function_\"\u003efetch_from_newsapi\u003c/span\u003e(self) -\u003e \u003cspan class=\"hljs-title class_\"\u003eList\u003c/span\u003e[\u003cspan class=\"hljs-title class_\"\u003eDict\u003c/span\u003e]:\n        \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"NewsAPI에서 상위 뉴스 가져오기.\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n        response = self.\u003cspan class=\"hljs-property\"\u003e_newsapi\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eget_everything\u003c/span\u003e(\n            q=settings.\u003cspan class=\"hljs-property\"\u003eNEWS_TOPIC\u003c/span\u003e,\n            language=\u003cspan class=\"hljs-string\"\u003e\"en\"\u003c/span\u003e,\n            page=settings.\u003cspan class=\"hljs-property\"\u003eARTICLES_BATCH_SIZE\u003c/span\u003e,\n            page_size=settings.\u003cspan class=\"hljs-property\"\u003eARTICLES_BATCH_SIZE\u003c/span\u003e,\n        )\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e [\n            \u003cspan class=\"hljs-title class_\"\u003eNewsAPIModel\u003c/span\u003e(**article).\u003cspan class=\"hljs-title function_\"\u003eto_common\u003c/span\u003e()\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e article \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e response.\u003cspan class=\"hljs-title function_\"\u003eget\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"articles\"\u003c/span\u003e, [])\n        ]\n\n    @handle_article_fetching\n    def \u003cspan class=\"hljs-title function_\"\u003efetch_from_newsdataapi\u003c/span\u003e(self) -\u003e \u003cspan class=\"hljs-title class_\"\u003eList\u003c/span\u003e[\u003cspan class=\"hljs-title class_\"\u003eDict\u003c/span\u003e]:\n        \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"NewsDataAPI에서 뉴스 데이터 가져오기.\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n        response = self.\u003cspan class=\"hljs-property\"\u003e_newsdataapi\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003enews_api\u003c/span\u003e(\n            q=settings.\u003cspan class=\"hljs-property\"\u003eNEWS_TOPIC\u003c/span\u003e,\n            language=\u003cspan class=\"hljs-string\"\u003e\"en\"\u003c/span\u003e,\n            size=settings.\u003cspan class=\"hljs-property\"\u003eARTICLES_BATCH_SIZE\u003c/span\u003e,\n        )\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e [\n            \u003cspan class=\"hljs-title class_\"\u003eNewsDataIOModel\u003c/span\u003e(**article).\u003cspan class=\"hljs-title function_\"\u003eto_common\u003c/span\u003e()\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e article \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e response.\u003cspan class=\"hljs-title function_\"\u003eget\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"results\"\u003c/span\u003e, [])\n        ]\n\n    @property\n    def \u003cspan class=\"hljs-title function_\"\u003esources\u003c/span\u003e(self) -\u003e \u003cspan class=\"hljs-title class_\"\u003eList\u003c/span\u003e[callable]:\n        \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"뉴스 가져오기 함수 목록입니다.\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e [self.\u003cspan class=\"hljs-property\"\u003efetch_from_newsapi\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003efetch_from_newsdataapi\u003c/span\u003e]\n\n\u0026#x3C;div \u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"content-ad\"\u003c/span\u003e\u003e\u0026#x3C;/div\u003e\n\n이 구현에서 고려해야 할 몇 가지 중요 사항이 있습니다:\n\n- \u003cspan class=\"hljs-title class_\"\u003eNewsAPIModel\u003c/span\u003e과 \u003cspan class=\"hljs-title class_\"\u003eNewsDataIOModel\u003c/span\u003e은 특정 페이로드 형식에 익숙한 \u003cspan class=\"hljs-title class_\"\u003ePydantic\u003c/span\u003e 모델입니다.\n- 우리는 handle_article_fetching 데코레이터를 사용하여 원시 페이로드를 \u003cspan class=\"hljs-title class_\"\u003ePydantic\u003c/span\u003e 모델로 변환할 때 유효성 오류나 더 넓은 예외를 잡습니다.\n- 우리에게는 \u003cspan class=\"hljs-variable constant_\"\u003eAPI\u003c/span\u003e를 쿼리하는 callable 메서드를 반환하는 sources라는 속성이 있습니다. 이것은 데이터 수집 모듈 내에서 사용될 것이며 멀티 프로듀서 스레드를 생성하여 \u003cspan class=\"hljs-title class_\"\u003eKafka\u003c/span\u003e 클러스터로 메시지를 전송합니다. 다음에 이어서 설명하겠습니다.\n\n## \u003cspan class=\"hljs-title class_\"\u003eKafka\u003c/span\u003e 메시지 생성\n\n다음에 우리가 구현할 작업 흐름입니다:\n\n\u003cspan class=\"xml\"\u003e\u003cspan class=\"hljs-tag\"\u003e\u0026#x3C;\u003cspan class=\"hljs-name\"\u003ediv\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eclass\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"content-ad\"\u003c/span\u003e\u003e\u003c/span\u003e\u003cspan class=\"hljs-tag\"\u003e\u0026#x3C;/\u003cspan class=\"hljs-name\"\u003ediv\u003c/span\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"xml\"\u003e\u003cspan class=\"hljs-tag\"\u003e\u0026#x3C;\u003cspan class=\"hljs-name\"\u003eimg\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003esrc\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_7.png\"\u003c/span\u003e /\u003e\u003c/span\u003e\u003c/span\u003e\n\n여기서 중요한 포인트들입니다:\n\n- \u003cspan class=\"hljs-variable constant_\"\u003eAPI\u003c/span\u003e에서 가져오는 작업에 별도 스레드를 사용합니다.\n- 메시지를 보내기 위해 공통 카프카 프로듀서 인스턴스를 공유합니다.\n- 데이터 교환을 보증하기 위해 \u003cspan class=\"hljs-title class_\"\u003ePydantic\u003c/span\u003e 모델을 사용합니다.\n\n기사를 가져오는 데 별도 스레드를 사용하고, 클러스터로 메시지를 보내기 위해 단일 카프카 프로듀서 인스턴스를 사용하는 것이 우리의 사용 사례에서 권장되는 방법입니다. 그 이유는 다음과 같습니다:\n\n\u003cspan class=\"xml\"\u003e\u003cspan class=\"hljs-tag\"\u003e\u0026#x3C;\u003cspan class=\"hljs-name\"\u003ediv\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eclass\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"content-ad\"\u003c/span\u003e\u003e\u003c/span\u003e\u003cspan class=\"hljs-tag\"\u003e\u0026#x3C;/\u003cspan class=\"hljs-name\"\u003ediv\u003c/span\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\n- 효율성 및 성능: \u003cspan class=\"hljs-title class_\"\u003eKafkaProducer\u003c/span\u003e는 스레드 안전합니다. 새 인스턴스를 만드는 것은 네트워크 연결과 일부 설정이 필요합니다. 여러 스레드 간에 하나의 단일 인스턴스를 공유하면 이러한 작업과 관련된 오버헤드를 줄일 수 있습니다.\n- 처리량: 단일 프로듀서 인스턴스는 메시지를 \u003cspan class=\"hljs-title class_\"\u003eKafka\u003c/span\u003e 클러스터로 보내기 전에 메시지를 일괄 처리합니다.\n- 자원: 사용 사례에 완전히 적용되지는 않지만, 우리는 오직 \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e개의 프로듀서 스레드만 가지고 있기 때문에 인스턴스 수를 제한함으로써 시스템 자원 이용률을 최적화할 수 있습니다.\n\n여기 \u003cspan class=\"hljs-title class_\"\u003eKafka\u003c/span\u003e로 메시지 처리를 담당하는 주요 기능이 있습니다:\n\ndef \u003cspan class=\"hljs-title function_\"\u003erun\u003c/span\u003e(self) -\u003e \u003cspan class=\"hljs-title class_\"\u003eNoReturn\u003c/span\u003e:\n        \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"지속적으로 Kafka 주제로 메시지를 가져와 보냅니다.\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003ewhile\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003erunning\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eis_set\u003c/span\u003e():\n            \u003cspan class=\"hljs-attr\"\u003etry\u003c/span\u003e:\n                \u003cspan class=\"hljs-attr\"\u003emessages\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eList\u003c/span\u003e[\u003cspan class=\"hljs-title class_\"\u003eCommonDocument\u003c/span\u003e] = self.\u003cspan class=\"hljs-title function_\"\u003efetch_function\u003c/span\u003e()\n                \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003emessages\u003c/span\u003e:\n                    messages = [msg.\u003cspan class=\"hljs-title function_\"\u003eto_kafka_payload\u003c/span\u003e() \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e msg \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e messages]\n                    self.\u003cspan class=\"hljs-property\"\u003eproducer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003esend\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003etopic\u003c/span\u003e, value=messages)\n                    self.\u003cspan class=\"hljs-property\"\u003eproducer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eflush\u003c/span\u003e()\n                logger.\u003cspan class=\"hljs-title function_\"\u003einfo\u003c/span\u003e(\n                    f\u003cspan class=\"hljs-string\"\u003e\"프로듀서 : {self.producer_id}이(가) {len(messages)}개의 메시지를 전송함.\"\u003c/span\u003e\n                )\n                time.\u003cspan class=\"hljs-title function_\"\u003esleep\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ewait_window_sec\u003c/span\u003e)\n            except \u003cspan class=\"hljs-title class_\"\u003eException\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ee\u003c/span\u003e:\n                logger.\u003cspan class=\"hljs-title function_\"\u003eerror\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"프로듀서 작업자 {self.producer_id}에서 오류 발생: {e}\"\u003c/span\u003e)\n                self.\u003cspan class=\"hljs-property\"\u003erunning\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eclear\u003c/span\u003e()  # 오류 시 스레드를 중지합니다\n\n구현에서 고려해야 할 중요 사항:\n\n\u003cspan class=\"xml\"\u003e\u003cspan class=\"hljs-tag\"\u003e\u0026#x3C;\u003cspan class=\"hljs-name\"\u003ediv\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eclass\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"content-ad\"\u003c/span\u003e\u003e\u003c/span\u003e\u003cspan class=\"hljs-tag\"\u003e\u0026#x3C;/\u003cspan class=\"hljs-name\"\u003ediv\u003c/span\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\n- 우리는 fetch sources의 수만큼 \u003cspan class=\"hljs-title class_\"\u003eKafkaProducerThread\u003c/span\u003e 인스턴스가 생성됩니다.\n- 우리는 모든 스레드를 \u003cspan class=\"hljs-title class_\"\u003eKafkaProducerSwarm\u003c/span\u003e 아래에 랩합니다.\n- 모든 스레드 사이에서 단일 \u003cspan class=\"hljs-title class_\"\u003eKafkaProducer\u003c/span\u003e 인스턴스를 공유하며, 이는 클러스터와 통신할 것입니다.\n- 우리는 N개의 fetching 스레드로 확장할 수 있지만 여전히 단일 \u003cspan class=\"hljs-title class_\"\u003eKafkaProducer\u003c/span\u003e 인스턴스를 유지하기 위해 싱글톤 디자인 패턴을 따릅니다.\n\n## \u003cspan class=\"hljs-title class_\"\u003ePydantic\u003c/span\u003e을 사용한 데이터 교환 모델\n\n위에서 제시한 코드 스니펫 구현에서, 이전에 설명되지 않았던 *\u003cspan class=\"hljs-title class_\"\u003eDocument\u003c/span\u003e, *\u003cspan class=\"hljs-title class_\"\u003eModel\u003c/span\u003e 객체의 사용을 관찰했을 수 있습니다. 이 섹션에서 이들이 무엇인지 자세히 살펴보겠습니다.\n\n이들은 데이터 교환을 위한 \u003cspan class=\"hljs-title class_\"\u003ePydantic\u003c/span\u003e 모델들이며, 우리가 구축 중인 응용 프로그램 내에서 이러한 모델들은:\n\n\u003cspan class=\"xml\"\u003e\u003cspan class=\"hljs-tag\"\u003e\u0026#x3C;\u003cspan class=\"hljs-name\"\u003ediv\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eclass\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"content-ad\"\u003c/span\u003e\u003e\u003c/span\u003e\u003cspan class=\"hljs-tag\"\u003e\u0026#x3C;/\u003cspan class=\"hljs-name\"\u003ediv\u003c/span\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\n- \u003cspan class=\"hljs-title class_\"\u003eNewsDataIOModel\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eNewsData\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eAPI\u003c/span\u003e에서 가져온 원시 페이로드를 래핑하고 포맷합니다.\n- \u003cspan class=\"hljs-title class_\"\u003eNewsAPIModel\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eNewsAPI\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eAPI\u003c/span\u003e에서 가져온 원시 페이로드를 래핑하고 포맷합니다.\n- \u003cspan class=\"hljs-title class_\"\u003eCommonDocument\u003c/span\u003e: 위에서 언급한 다양한 뉴스 형식 사이의 공통 형식을 설정합니다.\n- \u003cspan class=\"hljs-title class_\"\u003eRefinedDocument\u003c/span\u003e: metadata 아래에 유용한 필드를 그룹화하고 기사 설명 텍스트와 같은 주요 필드를 강조하는 공통 형식을 필터링합니다.\n- \u003cspan class=\"hljs-title class_\"\u003eChunkedDocument\u003c/span\u003e: 텍스트를 청크로 나누고 chunk_id와 document_id 사이의 계보를 보장합니다.\n- \u003cspan class=\"hljs-title class_\"\u003eEmbeddedDocument\u003c/span\u003e: 청크를 임베드하여 chunk_id와 document_id 사이의 계보를 보장합니다.\n\n예를 들어, 위 \u003cspan class=\"hljs-title class_\"\u003eCommonDocument\u003c/span\u003e 모델은 다양한 뉴스 페이로드 형식 사이의 연결 역할을 나타내므로 이와 같이 구성됩니다:\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eCommonDocument\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eBaseModel\u003c/span\u003e):\n    \u003cspan class=\"hljs-attr\"\u003earticle_id\u003c/span\u003e: str = \u003cspan class=\"hljs-title class_\"\u003eField\u003c/span\u003e(default_factory=\u003cspan class=\"hljs-attr\"\u003elambda\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003estr\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003euuid4\u003c/span\u003e()))\n    \u003cspan class=\"hljs-attr\"\u003etitle\u003c/span\u003e: str = \u003cspan class=\"hljs-title class_\"\u003eField\u003c/span\u003e(default_factory=\u003cspan class=\"hljs-attr\"\u003elambda\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"N/A\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-attr\"\u003eurl\u003c/span\u003e: str = \u003cspan class=\"hljs-title class_\"\u003eField\u003c/span\u003e(default_factory=\u003cspan class=\"hljs-attr\"\u003elambda\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"N/A\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-attr\"\u003epublished_at\u003c/span\u003e: str = \u003cspan class=\"hljs-title class_\"\u003eField\u003c/span\u003e(\n        default_factory=\u003cspan class=\"hljs-attr\"\u003elambda\u003c/span\u003e: datetime.\u003cspan class=\"hljs-title function_\"\u003enow\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003estrftime\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"%Y-%m-%d %H:%M:%S\"\u003c/span\u003e)\n    )\n    \u003cspan class=\"hljs-attr\"\u003esource_name\u003c/span\u003e: str = \u003cspan class=\"hljs-title class_\"\u003eField\u003c/span\u003e(default_factory=\u003cspan class=\"hljs-attr\"\u003elambda\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"Unknown\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-attr\"\u003eimage_url\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eOptional\u003c/span\u003e[str] = \u003cspan class=\"hljs-title class_\"\u003eField\u003c/span\u003e(default_factory=\u003cspan class=\"hljs-attr\"\u003elambda\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e)\n    \u003cspan class=\"hljs-attr\"\u003eauthor\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eOptional\u003c/span\u003e[str] = \u003cspan class=\"hljs-title class_\"\u003eField\u003c/span\u003e(default_factory=\u003cspan class=\"hljs-attr\"\u003elambda\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"Unknown\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-attr\"\u003edescription\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eOptional\u003c/span\u003e[str] = \u003cspan class=\"hljs-title class_\"\u003eField\u003c/span\u003e(default_factory=\u003cspan class=\"hljs-attr\"\u003elambda\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e)\n    \u003cspan class=\"hljs-attr\"\u003econtent\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eOptional\u003c/span\u003e[str] = \u003cspan class=\"hljs-title class_\"\u003eField\u003c/span\u003e(default_factory=\u003cspan class=\"hljs-attr\"\u003elambda\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e)\n\n    @\u003cspan class=\"hljs-title function_\"\u003efield_validator\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"title\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"description\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"content\"\u003c/span\u003e)\n    def \u003cspan class=\"hljs-title function_\"\u003eclean_text_fields\u003c/span\u003e(cls, v):\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e v is \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e or v == \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"N/A\"\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eclean_full\u003c/span\u003e(v)\n\n    @\u003cspan class=\"hljs-title function_\"\u003efield_validator\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"url\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"image_url\"\u003c/span\u003e)\n    def \u003cspan class=\"hljs-title function_\"\u003eclean_url_fields\u003c/span\u003e(cls, v):\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e v is \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"N/A\"\u003c/span\u003e\n        v = \u003cspan class=\"hljs-title function_\"\u003eremove_html_tags\u003c/span\u003e(v)\n        v = \u003cspan class=\"hljs-title function_\"\u003enormalize_whitespace\u003c/span\u003e(v)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e v\n\n    @\u003cspan class=\"hljs-title function_\"\u003efield_validator\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"published_at\"\u003c/span\u003e)\n    def \u003cspan class=\"hljs-title function_\"\u003eclean_date_field\u003c/span\u003e(cls, v):\n        \u003cspan class=\"hljs-attr\"\u003etry\u003c/span\u003e:\n            parsed_date = parser.\u003cspan class=\"hljs-title function_\"\u003eparse\u003c/span\u003e(v)\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e parsed_date.\u003cspan class=\"hljs-title function_\"\u003estrftime\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"%Y-%m-%d %H:%M:%S\"\u003c/span\u003e)\n        except (\u003cspan class=\"hljs-title class_\"\u003eValueError\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eTypeError\u003c/span\u003e):\n            logger.\u003cspan class=\"hljs-title function_\"\u003eerror\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"Error parsing date: {v}, using current date instead.\"\u003c/span\u003e)\n\n    @classmethod\n    def \u003cspan class=\"hljs-title function_\"\u003efrom_json\u003c/span\u003e(cls, \u003cspan class=\"hljs-attr\"\u003edata\u003c/span\u003e: dict) -\u003e \u003cspan class=\"hljs-string\"\u003e\"CommonDocument\"\u003c/span\u003e:\n        \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"JSON 객체에서 CommonDocument를 만듭니다.\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ecls\u003c/span\u003e(**data)\n\n    def \u003cspan class=\"hljs-title function_\"\u003eto_kafka_payload\u003c/span\u003e(self) -\u003e \u003cspan class=\"hljs-attr\"\u003edict\u003c/span\u003e:\n        \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"Kafka 페이로드의 공통 표현을 준비합니다.\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.\u003cspan class=\"hljs-title function_\"\u003emodel_dump\u003c/span\u003e(exclude_none=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\n\n해석해보겠습니다:\n\n\u003cspan class=\"xml\"\u003e\u003cspan class=\"hljs-tag\"\u003e\u0026#x3C;\u003cspan class=\"hljs-name\"\u003ediv\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eclass\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"content-ad\"\u003c/span\u003e\u003e\u003c/span\u003e\u003cspan class=\"hljs-tag\"\u003e\u0026#x3C;/\u003cspan class=\"hljs-name\"\u003ediv\u003c/span\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\n- 뉴스 기사 형식에 공통 속성 시리즈가 포함되어 있습니다.\n- 각 필드를 유효성 검사하거나 field_validator 데코레이터를 사용하여 기본값을 지정합니다.\n- to_kafka_payload 메서드는 메시지 직렬화를 보장하여 \u003cspan class=\"hljs-title class_\"\u003eKafka\u003c/span\u003e 클러스터로 전송하기 전에 처리합니다.\n\n## 텍스트 필드 클린업 프로세스\n\n클린업 프로세스는 간단합니다. 텍스트를 정리하고 다음을 보장하기 위해 메서드를 사용합니다:\n\n- 끝에 있는 공백이나 \\n, \\t를 제거합니다.\n- ul/li 목록 항목을 제거합니다.\n- 텍스트 내에 \u003cspan class=\"hljs-variable constant_\"\u003eHTML\u003c/span\u003e 태그가 있으면 제거합니다.\n\n\u0026#x3C;div \u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"content-ad\"\u003c/span\u003e\u003e\u0026#x3C;/div\u003e\n\n우리는 이러한 변환을 간소화하기 위해 구조화되지 않은 [\u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e] \u003cspan class=\"hljs-title class_\"\u003ePython\u003c/span\u003e 라이브러리를 사용하고 있습니다.\n\n## \u003cspan class=\"hljs-title class_\"\u003eKafkaProducers\u003c/span\u003e 실행\n\n지금까지 다음 모듈을 수행/구현했습니다:\n\n- 필요한 모든 서비스에 등록\n- \u003cspan class=\"hljs-title class_\"\u003eKafka\u003c/span\u003e 클러스터 및 벡터 데이터베이스 생성\n- 뉴스 기사 검색 핸들러 구현\n- 데이터 교환을 위한 \u003cspan class=\"hljs-title class_\"\u003ePydantic\u003c/span\u003e 모델 구현\n- \u003cspan class=\"hljs-title class_\"\u003eKafkaProducer\u003c/span\u003e 로직 구현\n\n\u0026#x3C;div \u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"content-ad\"\u003c/span\u003e\u003e\u0026#x3C;/div\u003e\n\n작업이 완료되면 이제 안전하게 우리의 파이프라인에서 생산 단계를 실행하고 \u003cspan class=\"hljs-title class_\"\u003eUpstash\u003c/span\u003e의 \u003cspan class=\"hljs-title class_\"\u003eKafkaCluster\u003c/span\u003e에서 메시지를 확인할 수 있습니다.\n\n그럼 시작해봐요!\n프로젝트의 루트 디렉토리에서, \u003cspan class=\"hljs-title class_\"\u003eMakefile\u003c/span\u003e에 데이터 수집을 실행하는 명령어가 있습니다:\n\n....\n\n\u003cspan class=\"hljs-attr\"\u003erun_producers\u003c/span\u003e:\n @echo \u003cspan class=\"hljs-string\"\u003e\"$(GREEN) [실행 중] 데이터 수집 파이프라인 Kafka 프로듀서 $(RESET)\"\u003c/span\u003e\n @bash -c \u003cspan class=\"hljs-string\"\u003e\"poetry run python -m src.producer\"\u003c/span\u003e\n\n...\n\n이 🔗\u003cspan class=\"hljs-title class_\"\u003eMakefile\u003c/span\u003e은 우리가 구축 중인 솔루션과 상호작용하기 위한 유용한 명령어가 포함되어 있습니다. 이 경우에는 make run_producers를 사용하여 run_producers를 실행해야 합니다. 이렇게 하면 \u003cspan class=\"hljs-title class_\"\u003eKafkaSwarm\u003c/span\u003e이 시작되고 \u003cspan class=\"hljs-title class_\"\u003eNewsAPIs\u003c/span\u003e에서 기사를 가져와 형식을 지정한 다음 \u003cspan class=\"hljs-title class_\"\u003eKafka\u003c/span\u003e 클러스터로 보내는 스레드를 다룰 것입니다.\n\n\u0026#x3C;div \u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"content-ad\"\u003c/span\u003e\u003e\u0026#x3C;/div\u003e\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_8.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e로그를 통해 프로듀서 스레드가 각각 5개의 메시지를 보냈다는 것을 확인했습니다. 메시지들이 클러스터에 도달했는지 확인하려면 Upstash 콘솔로 이동하여 Kafka 클러스터 → 메시지를 확인하십시오. 다음과 같은 화면이 나타날 것입니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_9.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e이 시점에서는 API에서 뉴스 기사를 가져와 형식을 맞춘 후 Kafka로 메시지를 보내는 데이터 수집 파이프라인의 구현 및 테스트가 완료되었습니다. 다음으로는 Kafka에서 새 메시지를 처리하는 \"컨슈머\" 또는 적재 파이프라인을 구현할 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e데이터 수집 파이프라인\u003c/h1\u003e\n\u003cp\u003e우리가 Kafka 주제에서 메시지를 받았다는 것을 확인한 후에는 \"소비자\" 파이프라인을 구현해야 합니다. 이는 다음을 의미합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKafka 주제에서 메시지 읽기\u003c/li\u003e\n\u003cli\u003e파싱, 형식 지정, 청크화, 임베딩 생성\u003c/li\u003e\n\u003cli\u003e벡터 객체 생성 및 Upstash Vector Index에 업서트\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_10.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이를 위해 Bytewax [4]를 사용하여 이러한 단계를 올바른 순서로 연결하는 DataFlow를 정의할 것입니다.\u003c/p\u003e\n\u003cp\u003e바로 구현에 들어가서 주요 개념을 설명해보겠습니다!\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBytewax Flow에 입력으로 Kafka Source를 정의합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e json\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e typing \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eList\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e bytewax.\u003cspan class=\"hljs-property\"\u003econnectors\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ekafka\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eKafkaSinkMessage\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eKafkaSource\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e logger \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e get_logger\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e models \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eCommonDocument\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e settings \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e settings\n\nlogger = \u003cspan class=\"hljs-title function_\"\u003eget_logger\u003c/span\u003e(__name__)\n\ndef \u003cspan class=\"hljs-title function_\"\u003ebuild_kafka_stream_client\u003c/span\u003e():\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    Build a Kafka stream client to read messages from the Upstash Kafka topic using the ByteWax KafkaSource connector.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    kafka_config = {\n        \u003cspan class=\"hljs-string\"\u003e\"bootstrap.servers\"\u003c/span\u003e: settings.\u003cspan class=\"hljs-property\"\u003eUPSTASH_KAFKA_ENDPOINT\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"security.protocol\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"SASL_SSL\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"sasl.mechanisms\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"SCRAM-SHA-256\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"sasl.username\"\u003c/span\u003e: settings.\u003cspan class=\"hljs-property\"\u003eUPSTASH_KAFKA_UNAME\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"sasl.password\"\u003c/span\u003e: settings.\u003cspan class=\"hljs-property\"\u003eUPSTASH_KAFKA_PASS\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"auto.offset.reset\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"earliest\"\u003c/span\u003e,  # \u003cspan class=\"hljs-title class_\"\u003eStart\u003c/span\u003e reading at the earliest message\n    }\n    kafka_input = \u003cspan class=\"hljs-title class_\"\u003eKafkaSource\u003c/span\u003e(\n        topics=[settings.\u003cspan class=\"hljs-property\"\u003eUPSTASH_KAFKA_TOPIC\u003c/span\u003e],\n        brokers=[settings.\u003cspan class=\"hljs-property\"\u003eUPSTASH_KAFKA_ENDPOINT\u003c/span\u003e],\n        add_config=kafka_config,\n    )\n    logger.\u003cspan class=\"hljs-title function_\"\u003einfo\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"KafkaSource client created successfully.\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e kafka_input\n\ndef \u003cspan class=\"hljs-title function_\"\u003eprocess_message\u003c/span\u003e(\u003cspan class=\"hljs-attr\"\u003emessage\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eKafkaSinkMessage\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    On a Kafka message, process the message and return a list of CommonDocuments.\n    - message: KafkaSinkMessage(key, value) where value is the message payload.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003edocuments\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eList\u003c/span\u003e[\u003cspan class=\"hljs-title class_\"\u003eCommonDocument\u003c/span\u003e] = []\n    \u003cspan class=\"hljs-attr\"\u003etry\u003c/span\u003e:\n        json_str = message.\u003cspan class=\"hljs-property\"\u003evalue\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003edecode\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"utf-8\"\u003c/span\u003e)\n        data = json.\u003cspan class=\"hljs-title function_\"\u003eloads\u003c/span\u003e(json_str)\n        documents = [\u003cspan class=\"hljs-title class_\"\u003eCommonDocument\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_json\u003c/span\u003e(obj) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e obj \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e data]\n        logger.\u003cspan class=\"hljs-title function_\"\u003einfo\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"Decoded into {len(documents)} CommonDocuments\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e documents\n    except \u003cspan class=\"hljs-title class_\"\u003eStopIteration\u003c/span\u003e:\n        logger.\u003cspan class=\"hljs-title function_\"\u003einfo\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"No more documents to fetch from the client.\"\u003c/span\u003e)\n    except \u003cspan class=\"hljs-title class_\"\u003eKeyError\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ee\u003c/span\u003e:\n        logger.\u003cspan class=\"hljs-title function_\"\u003eerror\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"Key error in processing document batch: {e}\"\u003c/span\u003e)\n    except json.\u003cspan class=\"hljs-property\"\u003eJSONDecodeError\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ee\u003c/span\u003e:\n        logger.\u003cspan class=\"hljs-title function_\"\u003eerror\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"Error decoding JSON from message: {e}\"\u003c/span\u003e)\n        raise\n    except \u003cspan class=\"hljs-title class_\"\u003eException\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ee\u003c/span\u003e:\n        logger.\u003cspan class=\"hljs-title function_\"\u003eexception\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"Unexpected error in next_batch: {e}\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 구현에서 중요한 점들:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebuild_kafka_stream_client : 미리 정의된 Bytewax KafkaSource 커넥터를 사용하여 KafkaConsumer의 인스턴스를 생성합니다.\u003c/li\u003e\n\u003cli\u003eprocess_message : Kafka Topic에서 메시지를 처리할 콜백 함수입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eBytewax 플로우의 출력으로 Upstash Vector (Index)를 정의합니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e typing \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOptional\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eList\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e bytewax.\u003cspan class=\"hljs-property\"\u003eoutputs\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eDynamicSink\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eStatelessSinkPartition\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e upstash_vector \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eIndex\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eVector\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e models \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eEmbeddedDocument\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e settings \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e settings\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e logger \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e get_logger\n\n\nlogger = \u003cspan class=\"hljs-title function_\"\u003eget_logger\u003c/span\u003e(__name__)\n\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eUpstashVectorOutput\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eDynamicSink\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"Upstash 벡터 출력을 나타내는 클래스입니다.\n\n    이 클래스는 at-least-once 처리를 지원하는 동적 출력 유형인 Upstash 벡터 출력을 생성하는 데 사용됩니다.\n    resume 이후의 메시지는 resume 즉각적으로 복제됩니다.\n\n    Args:\n        vector_size (int): 벡터의 크기.\n        collection_name (str, optional): 컬렉션의 이름입니다. 기본값은 constants.VECTOR_DB_OUTPUT_COLLECTION_NAME입니다.\n        client (Optional[UpstashClient], optional): Upstash 클라이언트입니다. 기본값은 None입니다.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\n        self,\n        \u003cspan class=\"hljs-attr\"\u003evector_size\u003c/span\u003e: int = settings.\u003cspan class=\"hljs-property\"\u003eEMBEDDING_MODEL_MAX_INPUT_LENGTH\u003c/span\u003e,\n        \u003cspan class=\"hljs-attr\"\u003ecollection_name\u003c/span\u003e: str = settings.\u003cspan class=\"hljs-property\"\u003eUPSTASH_VECTOR_TOPIC\u003c/span\u003e,\n        \u003cspan class=\"hljs-attr\"\u003eclient\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eOptional\u003c/span\u003e[\u003cspan class=\"hljs-title class_\"\u003eIndex\u003c/span\u003e] = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e,\n    ):\n        self.\u003cspan class=\"hljs-property\"\u003e_collection_name\u003c/span\u003e = collection_name\n        self.\u003cspan class=\"hljs-property\"\u003e_vector_size\u003c/span\u003e = vector_size\n\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eclient\u003c/span\u003e:\n            self.\u003cspan class=\"hljs-property\"\u003eclient\u003c/span\u003e = client\n        \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n            self.\u003cspan class=\"hljs-property\"\u003eclient\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eIndex\u003c/span\u003e(\n                url=settings.\u003cspan class=\"hljs-property\"\u003eUPSTASH_VECTOR_ENDPOINT\u003c/span\u003e,\n                token=settings.\u003cspan class=\"hljs-property\"\u003eUPSTASH_VECTOR_KEY\u003c/span\u003e,\n                retries=settings.\u003cspan class=\"hljs-property\"\u003eUPSTASH_VECTOR_RETRIES\u003c/span\u003e,\n                retry_interval=settings.\u003cspan class=\"hljs-property\"\u003eUPSTASH_VECTOR_WAIT_INTERVAL\u003c/span\u003e,\n            )\n\n    def \u003cspan class=\"hljs-title function_\"\u003ebuild\u003c/span\u003e(\n        self, \u003cspan class=\"hljs-attr\"\u003estep_id\u003c/span\u003e: str, \u003cspan class=\"hljs-attr\"\u003eworker_index\u003c/span\u003e: int, \u003cspan class=\"hljs-attr\"\u003eworker_count\u003c/span\u003e: int\n    ) -\u003e \u003cspan class=\"hljs-title class_\"\u003eStatelessSinkPartition\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eUpstashVectorSink\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003eclient\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003e_collection_name\u003c/span\u003e)\n\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eUpstashVectorSink\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eStatelessSinkPartition\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    Upstash Vector 데이터베이스 컬렉션에 문서 임베딩을 작성하는 싱크입니다.\n    이 구현은 효율성을 높이기 위해 배치 업서트를 활용하며, 오류 처리 및 로깅을 향상시키고 가독성 및 유지 보수성을 위해 Pythonic한 모법을 따릅니다.\n\n    Args:\n        client (Index): 쓰기에 사용할 Upstash Vector 클라이언트입니다.\n        collection_name (str, optional): 쓸 컬렉션의 이름입니다. 기본값은 UPSTASH_VECTOR_TOPIC 환경 변수의 값입니다.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\n        self,\n        \u003cspan class=\"hljs-attr\"\u003eclient\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eIndex\u003c/span\u003e,\n        \u003cspan class=\"hljs-attr\"\u003ecollection_name\u003c/span\u003e: str = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e,\n    ):\n        self.\u003cspan class=\"hljs-property\"\u003e_client\u003c/span\u003e = client\n        self.\u003cspan class=\"hljs-property\"\u003e_collection_name\u003c/span\u003e = collection_name\n        self.\u003cspan class=\"hljs-property\"\u003e_upsert_batch_size\u003c/span\u003e = settings.\u003cspan class=\"hljs-property\"\u003eUPSTASH_VECTOR_UPSERT_BATCH_SIZE\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003ewrite_batch\u003c/span\u003e(self, \u003cspan class=\"hljs-attr\"\u003edocuments\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eList\u003c/span\u003e[\u003cspan class=\"hljs-title class_\"\u003eEmbeddedDocument\u003c/span\u003e]):\n        \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n        구성된 Upstash Vector 데이터베이스 컬렉션에 문서 임베딩의 배치를 작성합니다.\n\n        Args:\n            documents (List[EmbeddedDocument]): 쓸 문서들입니다.\n        \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n        vectors = [\n            \u003cspan class=\"hljs-title class_\"\u003eVector\u003c/span\u003e(id=doc.\u003cspan class=\"hljs-property\"\u003edoc_id\u003c/span\u003e, vector=doc.\u003cspan class=\"hljs-property\"\u003eembeddings\u003c/span\u003e, metadata=doc.\u003cspan class=\"hljs-property\"\u003emetadata\u003c/span\u003e)\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e doc \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e documents\n        ]\n\n        # 효율성을 위한 배치 업서트\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(vectors), self.\u003cspan class=\"hljs-property\"\u003e_upsert_batch_size\u003c/span\u003e):\n            batch_vectors = vectors[i : i + self.\u003cspan class=\"hljs-property\"\u003e_upsert_batch_size\u003c/span\u003e]\n            \u003cspan class=\"hljs-attr\"\u003etry\u003c/span\u003e:\n                self.\u003cspan class=\"hljs-property\"\u003e_client\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eupsert\u003c/span\u003e(vectors=batch_vectors)\n            except \u003cspan class=\"hljs-title class_\"\u003eException\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ee\u003c/span\u003e:\n                logger.\u003cspan class=\"hljs-title function_\"\u003eerror\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"배치 업서트 중 예외 발생 {e}\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 구현에서 중요한 사항들입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUpstashVectorOutput: 다양한 대상으로 데이터를 전달하기 위해 설계된 Bytewax DynamicSink 추상화를 인스턴스화합니다. 우리의 경우, 이는 Upstash Vector Index 클라이언트 연결 위에 래핑될 것입니다.\u003c/li\u003e\n\u003cli\u003eUpstashVectorSink: 우리의 DynamicSink을 래핑하고 업서트 벡터를 우리의 VectorDatabase에 처리하는 기능을 담당합니다. 이 StatelessSinkPartition은 DynamicSink가 어떠한 상태도 유지하지 않고 우리의 Sink에 대한 모든 입력을 write_batch 기능 구현에 따라 처리합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e나머지 Bytewax Flow 빌드하기\u003c/h2\u003e\n\u003cp\u003e여기 Upstash Kafka Topic에서 메시지를 가져와 정제, 수정, 분할, 삽입하고 Upstash Vector Index에 벡터를 업서트하는 저희 DataFlow의 전체 구현입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    이 스크립트는 Upstash 사용 사례에 대한 ByteWax 데이터플로 구현을 정의합니다.\n    데이터플로에는 다음 단계가 포함되어 있습니다:\n        1. 입력: 카프카 스트림에서 데이터를 읽기\n        2. 정제: 입력 데이터를 공통 형식으로 변환 \n        3. 청크 분리: 입력 데이터를 더 작은 청크로 분리\n        4. 임베드: 입력 데이터에 대한 임베딩 생성\n        5. 출력: 출력 데이터를 Upstash 벡터 데이터베이스에 쓰기\n\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e pathlib \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003ePath\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e typing \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOptional\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e bytewax.\u003cspan class=\"hljs-property\"\u003eoperators\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e op\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e vector \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eUpstashVectorOutput\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e consumer \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e process_message, build_kafka_stream_client\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e bytewax.\u003cspan class=\"hljs-property\"\u003econnectors\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ekafka\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eKafkaSource\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e bytewax.\u003cspan class=\"hljs-property\"\u003edataflow\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eDataflow\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e bytewax.\u003cspan class=\"hljs-property\"\u003eoutputs\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eDynamicSink\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e embeddings \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTextEmbedder\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e models \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eChunkedDocument\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eEmbeddedDocument\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eRefinedDocument\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e logger \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e get_logger\n\nlogger = \u003cspan class=\"hljs-title function_\"\u003eget_logger\u003c/span\u003e(__name__)\n\n\ndef \u003cspan class=\"hljs-title function_\"\u003ebuild\u003c/span\u003e(\n    \u003cspan class=\"hljs-attr\"\u003emodel_cache_dir\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eOptional\u003c/span\u003e[\u003cspan class=\"hljs-title class_\"\u003ePath\u003c/span\u003e] = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e,\n) -\u003e \u003cspan class=\"hljs-title class_\"\u003eDataflow\u003c/span\u003e:\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    Upstash 사용 사례에 대한 ByteWax 데이터플로를 구축합니다.\n    다음과 같은 데이터플로를 따릅니다:\n        * 1. Tag: ['kafka_input']   = KafkaSource에서 입력 데이터 읽기\n        * 2. Tag: ['map_kinp']      = KafkaSource에서 CommonDocument로 메시지 처리\n            * 2.1 [Optional] Tag ['dbg_map_kinp'] = ['map_kinp'] 후 디버깅\n        * 3. Tag: ['refine']        = 메시지를 정제된 문서 형식으로 변환\n            * 3.1 [Optional] Tag ['dbg_refine'] = ['refine'] 후 디버깅\n        * 4. Tag: ['chunkenize']    = 정제된 문서를 더 작은 청크로 나누기\n            * 4.1 [Optional] Tag ['dbg_chunkenize'] = ['chunkenize'] 후 디버깅\n        * 5. Tag: ['embed']         = 청크에 대한 임베딩 생성\n            * 5.1 [Optional] Tag ['dbg_embed'] = ['embed'] 후 디버깅\n        * 6. Tag: ['output']        = 임베딩을 Upstash 벡터 데이터베이스에 쓰기\n    노트:\n        각 선택적 태그는 문제 해결을 위해 활성화할 수 있는 디버깅 단계입니다.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    model = \u003cspan class=\"hljs-title class_\"\u003eTextEmbedder\u003c/span\u003e(cache_dir=model_cache_dir)\n\n    dataflow = \u003cspan class=\"hljs-title class_\"\u003eDataflow\u003c/span\u003e(flow_id=\u003cspan class=\"hljs-string\"\u003e\"news-to-upstash\"\u003c/span\u003e)\n    stream = op.\u003cspan class=\"hljs-title function_\"\u003einput\u003c/span\u003e(\n        step_id=\u003cspan class=\"hljs-string\"\u003e\"kafka_input\"\u003c/span\u003e,\n        flow=dataflow,\n        source=\u003cspan class=\"hljs-title function_\"\u003e_build_input\u003c/span\u003e(),\n    )\n    stream = op.\u003cspan class=\"hljs-title function_\"\u003eflat_map\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"map_kinp\"\u003c/span\u003e, stream, process_message)\n    # _ = op.\u003cspan class=\"hljs-title function_\"\u003einspect\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"dbg_map_kinp\"\u003c/span\u003e, stream)\n    stream = op.\u003cspan class=\"hljs-title function_\"\u003emap\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"refine\"\u003c/span\u003e, stream, \u003cspan class=\"hljs-title class_\"\u003eRefinedDocument\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003efrom_common\u003c/span\u003e)\n    # _ = op.\u003cspan class=\"hljs-title function_\"\u003einspect\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"dbg_refine\"\u003c/span\u003e, stream)\n    stream = op.\u003cspan class=\"hljs-title function_\"\u003eflat_map\u003c/span\u003e(\n        \u003cspan class=\"hljs-string\"\u003e\"chunkenize\"\u003c/span\u003e,\n        stream,\n        lambda \u003cspan class=\"hljs-attr\"\u003erefined_doc\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eChunkedDocument\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_refined\u003c/span\u003e(refined_doc, model),\n    )\n    # _ = op.\u003cspan class=\"hljs-title function_\"\u003einspect\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"dbg_chunkenize\"\u003c/span\u003e, stream)\n    stream = op.\u003cspan class=\"hljs-title function_\"\u003emap\u003c/span\u003e(\n        \u003cspan class=\"hljs-string\"\u003e\"embed\"\u003c/span\u003e,\n        stream,\n        lambda \u003cspan class=\"hljs-attr\"\u003echunked_doc\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eEmbeddedDocument\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_chunked\u003c/span\u003e(chunked_doc, model),\n    )\n    # _ = op.\u003cspan class=\"hljs-title function_\"\u003einspect\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"dbg_embed\"\u003c/span\u003e, stream)\n    stream = op.\u003cspan class=\"hljs-title function_\"\u003eoutput\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"output\"\u003c/span\u003e, stream, \u003cspan class=\"hljs-title function_\"\u003e_build_output\u003c/span\u003e())\n    logger.\u003cspan class=\"hljs-title function_\"\u003einfo\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"성공적으로 ByteWax 데이터플로를 생성했습니다.\"\u003c/span\u003e)\n    logger.\u003cspan class=\"hljs-title function_\"\u003einfo\u003c/span\u003e(\n        \u003cspan class=\"hljs-string\"\u003e\"\\t단계: Kafka 입력 -\u003e 매핑 -\u003e 정제 -\u003e 청크 분리 -\u003e 임베딩 -\u003e 업로드\"\u003c/span\u003e\n    )\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e dataflow\n\n\ndef \u003cspan class=\"hljs-title function_\"\u003e_build_input\u003c/span\u003e() -\u003e \u003cspan class=\"hljs-title class_\"\u003eKafkaSource\u003c/span\u003e:\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ebuild_kafka_stream_client\u003c/span\u003e()\n\n\ndef \u003cspan class=\"hljs-title function_\"\u003e_build_output\u003c/span\u003e() -\u003e \u003cspan class=\"hljs-title class_\"\u003eDynamicSink\u003c/span\u003e:\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eUpstashVectorOutput\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003ekafka_input: 카프카 메시지를 가져와 CommonDocument Pydantic 형식으로 변환하는 단계입니다.\u003c/li\u003e\n\u003cli\u003emap_kinp: 카프카 입력을 의미하며, 수신된 메시지에 flat map을 적용하여 List[CommonDocument] Pydantic 객체를 생성합니다.\u003c/li\u003e\n\u003cli\u003erefine: List[CommonDocument]를 순회하고 RefinedDocument 인스턴스를 생성하는 단계입니다.\u003c/li\u003e\n\u003cli\u003echunkenize: List[RefinedDocument]를 순회하고 ChunkedDocument 인스턴스를 생성하는 단계입니다.\u003c/li\u003e\n\u003cli\u003eembed: List[ChunkedDocuments]를 순회하고 EmbeddedDocument 인스턴스를 생성하는 단계입니다.\u003c/li\u003e\n\u003cli\u003eoutput: List[EmbeddedDocument]를 순회하고 Vector 객체를 생성하여 Upstash Vector Index에 업서트하는 단계입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e파이프라인 시작\u003c/h1\u003e\n\u003cp\u003e지금까지 구현한 것은 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e데이터 수집 파이프라인: 주기적으로 NewsAPI에서 원시 페이로드를 가져와 형식을 지정한 뒤, 카프카 토픽으로 메시지를 전송하는 단계입니다.\u003c/li\u003e\n\u003cli\u003e인제션 파이프라인: 이는 Bytewax DataFlow로, 카프카 토픽에 연결되어 메시지를 소비하고, 최종적으로 벡터를 업서트하는 Vector 데이터베이스에 업데이트합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e프로젝트 루트에있는 Makefile에서 미리 정의된 명령을 사용하여 이 두 서비스를 모두 시작할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 카프카 메시지를 생성하기 위해 데이터 수집 파이프라인 실행\nmake run_producers\n\n# 카프카 메시지를 소비하고 벡터를 업데이트하기 위해 인제스처리 파이프라인 실행\nmake run_pipeline\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그리고... 완료되었습니다!\n성공적으로 프로듀서/컨슈머 서비스를 시작했습니다.\n남은 모듈은 UI입니다. 벡터 데이터베이스와 뉴스 기사를 검색하는 데 상호 작용하는 데 사용됩니다.\u003c/p\u003e\n\u003ch1\u003e사용자 인터페이스\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eUI는 다음과 같은 기능을 갖춘 기본 Streamlit [8] 애플리케이션입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e텍스트 검색 창\u003c/li\u003e\n\u003cli\u003e벡터 데이터베이스에서 가져온 기사로 채워진 카드가 표시되는 div 섹션\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e카드에는 다음과 같은 데이터 필드가 포함되어 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e발행일\u003c/li\u003e\n\u003cli\u003e유사도 점수\u003c/li\u003e\n\u003cli\u003e기사 이미지\u003c/li\u003e\n\u003cli\u003eSeeMore 버튼을 클릭하면 원본 기사 URL로 이동합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e한번 메시지/질문을 텍스트 상자에 입력하면 입력이 정리되고 (소문자로 변환, 비ASCII 문자 제거 등) 그리고 삽입됩니다. 새로운 삽입물을 사용하여 벡터 데이터베이스를 쿼리하여 가장 유사한 항목을 가져옵니다. 그 결과는 구성되어 렌더링될 것입니다.\u003c/p\u003e\n\u003cp\u003e다음은 예시입니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_11.png\" alt=\"How to build a real-time News Search Engine using VectorDBs - Part 1\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs_12.png\" alt=\"How to build a real-time News Search Engine using VectorDBs - Part 2\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e축하합니다!\u003c/p\u003e\n\u003cp\u003e성공했습니다! 멋진 프로젝트만큼이나 라이브로 출시할 준비가 된 뉴스 검색 엔진을 만들었습니다. 단순히 무작정 던지는 것이 아니라, 우리는 최고의 소프트웨어 개발 관행을 따르기도 했습니다.\u003c/p\u003e\n\u003cp\u003ePydantic을 사용하여 데이터를 잘 처리했고, 유닛 테스트를 작성하고, 스레딩을 활용하여 작업을 가속화했으며, Upstash의 서버리스 카프카와 벡터 데이터베이스를 활용하여 파이프라인을 쉽게 설정할 뿐만 아니라 빠르고 확장 가능하며 오류 대비 가능하도록 만들었습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제 당신은 이 청사진을 대부분의 데이터 기반 아이디어에 적용할 수 있는 능력을 갖게 되었어요. 이건 이 프로젝트뿐만 아니라 앞으로 만들게 될 멋진 것들을 위한 큰 승리에요.\u003c/p\u003e\n\u003ch1\u003e참고 자료\u003c/h1\u003e\n\u003cp\u003e[1] News Search Engine using Upstash Vector — Decoding ML Github (2024)\n[2] Upstash Serverless Kafka\n[3] Upstash Serverless Vector Database\n[4] Bytewax Stream Processing with Python\n[5] Singleton Pattern\n[6] sentence-transformers/all-MiniLM-L6-v2\n[7] unstructured Python Library\n[8] Streamlit Python\u003c/p\u003e\n\u003ch1\u003e더 읽을 거리\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 글과 관련성 순으로 정렬되었습니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-Howtobuildareal-timeNewsSearchEngineusingVectorDBs"},"buildId":"kkckKPyxcjJp_o8wb2unW","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>