<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>LLM의 내부 작업 공개 고유 값 관점 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="LLM의 내부 작업 공개 고유 값 관점 | itposting" data-gatsby-head="true"/><meta property="og:title" content="LLM의 내부 작업 공개 고유 값 관점 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective" data-gatsby-head="true"/><meta name="twitter:title" content="LLM의 내부 작업 공개 고유 값 관점 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 20:37" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_buildManifest.js" defer=""></script><script src="/_next/static/8coAiP0lmiEK5aH6nkQkj/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">LLM의 내부 작업 공개 고유 값 관점</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="LLM의 내부 작업 공개 고유 값 관점" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">9<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>Llama3–8B 투영 행렬에 대한 특이값 분해 분석</h2>
<p><img src="/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_0.png" alt="이미지"></p>
<p>LLM이 얼마나 잘 훈련되었는지 생각해 보셨나요? 매개변수의 수가 많은데, 그 매개변수들이 훈련 데이터로부터 정보나 지식을 최대한으로 얻어내고 있는지 궁금해 하시지 않나요? 그렇지 않다면, LLM에서 유용하지 않은 매개변수들을 제거하여 더 효율적으로 만들 수 있을까요?</p>
<p>이 글에서는 Singular Values 관점에서 Llama-3–8B 모델을 깊게 분석하여 이러한 질문에 답해보겠습니다. 더 이상 시간을 낭비하지 말고 편안하게 앉아, SVD를 적용하여 Llama-3–8B 행렬의 품질을 분석해 보세요!</p>
<div class="content-ad"></div>
<h1>SVD 다시 살펴보기</h1>
<p>특이값 분해(SVD)에서 행렬 A는 세 가지 다른 행렬로 분해됩니다:</p>
<p>여기서:</p>
<ul>
<li>A는 원래 행렬입니다.</li>
<li>U는 A의 왼쪽 특이벡터인 열로 이루어진 행렬입니다.</li>
<li>Σ은 A의 특이값을 포함하는 대각행렬입니다. 이 값들은 항상 음이 아닌 값이며 일반적으로 가장 큰 값부터 가장 작은 값 순서로 정렬됩니다.</li>
<li>V_t는 V의 전치행렬이며, V의 열은 A의 오른쪽 특이벡터입니다.</li>
</ul>
<div class="content-ad"></div>
<p>더 간단한 용어로 설명하면, 특이값 분해(SVD)는 행렬의 복잡한 변환을 간단하고 이해하기 쉬운 회전 및 스케일링 과정으로 나누어 줍니다. Σ의 특이값은 스케일링 요소를 알려주고 U와 V_t의 특이벡터는 해당 스케일링이 행렬을 적용하기 전과 후의 방향을 알려줍니다.</p>
<p>특이값은 행렬이 공간에서 다양한 방향으로 얼마나 늘어나거나 줄어드는지를 측정하는 방법으로 생각할 수 있습니다. 각 특이값은 특이벡터 쌍에 해당되며, 하나는 오른쪽 특이벡터(입력 공간에서의 방향), 다른 하나는 왼쪽 특이벡터(출력 공간에서의 방향)입니다.</p>
<p>행렬의 특이값이 급격하게 감소하는 경우(가장 큰 특이값이 작은 것들보다 현저히 큰 경우), 이는 행렬의 유효 랭크(중요한 특이값의 수)가 실제 행렬의 차원보다 훨씬 작다는 것을 의미합니다. 이는 행렬이 낮은 랭크 행렬로 잘 근사될 수 있음을 시사합니다.</p>
<p>LLM(대형 언어 모델)의 맥락에서, 가중치 행렬(예: 어텐션 메커니즘 또는 피드포워드 레이어의 행렬)들은 입력 데이터(예: 단어 임베딩)를 출력 표현으로 변환합니다. 주요한 특이값은 변환에 의해 가장 강조되는 입력 공간의 방향을 나타내며, 모델이 민감하거나 표현력이 강한 방향을 보여줍니다. 작은 특이값은 변환에서 중요하지 않거나 영향력이 적은 방향을 나타냅니다.</p>
<div class="content-ad"></div>
<p>특이값의 분포는 모델의 일반화 능력과 견고성에 영향을 줄 수 있습니다. 느린 감소(많은 큰 특이값)는 과적합을 초래할 수 있으며, 빠른 감소(소수의 큰 특이값)는 과소적합이거나 정보의 손실을 나타낼 수 있습니다.</p>
<h1>Llama-3 아키텍처 재방문</h1>
<p>다음은 meta-llama/Meta-Llama-3-8B-Instructmodel의 config.json 파일입니다. 이 LLM은 8개의 num_key_value_heads를 사용하여 Grouped Query Attention을 활용하며, 이는 그룹 크기가 32/8=4임을 의미합니다.</p>
<pre><code class="hljs language-js">{
  <span class="hljs-string">"architectures"</span>: [
    <span class="hljs-string">"LlamaForCausalLM"</span>
  ],
  <span class="hljs-string">"attention_bias"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-string">"attention_dropout"</span>: <span class="hljs-number">0.0</span>,
  <span class="hljs-string">"bos_token_id"</span>: <span class="hljs-number">128000</span>,
  <span class="hljs-string">"eos_token_id"</span>: <span class="hljs-number">128009</span>,
  <span class="hljs-string">"hidden_act"</span>: <span class="hljs-string">"silu"</span>,
  <span class="hljs-string">"hidden_size"</span>: <span class="hljs-number">4096</span>,
  <span class="hljs-string">"initializer_range"</span>: <span class="hljs-number">0.02</span>,
  <span class="hljs-string">"intermediate_size"</span>: <span class="hljs-number">14336</span>,
  <span class="hljs-string">"max_position_embeddings"</span>: <span class="hljs-number">8192</span>,
  <span class="hljs-string">"model_type"</span>: <span class="hljs-string">"llama"</span>,
  <span class="hljs-string">"num_attention_heads"</span>: <span class="hljs-number">32</span>,
  <span class="hljs-string">"num_hidden_layers"</span>: <span class="hljs-number">32</span>,
  <span class="hljs-string">"num_key_value_heads"</span>: <span class="hljs-number">8</span>,
  <span class="hljs-string">"pretraining_tp"</span>: <span class="hljs-number">1</span>,
  <span class="hljs-string">"rms_norm_eps"</span>: <span class="hljs-number">1e-05</span>,
  <span class="hljs-string">"rope_scaling"</span>: <span class="hljs-literal">null</span>,
  <span class="hljs-string">"rope_theta"</span>: <span class="hljs-number">500000.0</span>,
  <span class="hljs-string">"tie_word_embeddings"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-string">"torch_dtype"</span>: <span class="hljs-string">"bfloat16"</span>,
  <span class="hljs-string">"transformers_version"</span>: <span class="hljs-string">"4.40.0.dev0"</span>,
  <span class="hljs-string">"use_cache"</span>: <span class="hljs-literal">true</span>,
  <span class="hljs-string">"vocab_size"</span>: <span class="hljs-number">128256</span>
}
</code></pre>
<div class="content-ad"></div>
<h1>(Q, K, V, O) 행렬의 특이값 분석</h1>
<p>자, 이제 이 기사의 본격적인 내용으로 들어가 봅시다. Llama-3–8B-Instruct 모델의 (Q, K, V, O) 행렬들을 그들의 특이값을 통해 분석해 보겠습니다!</p>
<h2>코드</h2>
<p>우선, 이 분석에 필요한 모든 패키지를 가져와 봅시다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> transformers
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> <span class="hljs-title class_">AutoConfig</span>, <span class="hljs-title class_">LlamaModel</span>
<span class="hljs-keyword">from</span> safetensors <span class="hljs-keyword">import</span> safe_open
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> matplotlib.<span class="hljs-property">pyplot</span> <span class="hljs-keyword">as</span> plt
</code></pre>
<p>그런 다음, 모델을 다운로드하고 로컬 /tmp디렉토리에 저장합시다.</p>
<pre><code class="hljs language-js"><span class="hljs-variable constant_">MODEL_ID</span> = <span class="hljs-string">"meta-llama/Meta-Llama-3-8B-Instruct"</span>
!huggingface-cli download {<span class="hljs-variable constant_">MODEL_ID</span>} --quiet --local-dir /tmp/{<span class="hljs-variable constant_">MODEL_ID</span>}
</code></pre>
<p>만약 GPU를 많이 가지고 계신 분이시라면, 다음 코드는 관련이 없을 수 있습니다. 그러나 저와 같이 GPU가 부족한 분들에겐, LLama-3–8B 모델의 특정 레이어만 로드하는 데 매우 유용할 것입니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">def <span class="hljs-title function_">load_specific_layers_safetensors</span>(model, model_name, layer_to_load):
    state_dict = {}
    files = [f <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> os.<span class="hljs-title function_">listdir</span>(model_name) <span class="hljs-keyword">if</span> f.<span class="hljs-title function_">endswith</span>(<span class="hljs-string">'.safetensors'</span>)]
    <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> <span class="hljs-attr">files</span>:
        filepath = os.<span class="hljs-property">path</span>.<span class="hljs-title function_">join</span>(model_name, file)
        <span class="hljs-keyword">with</span> <span class="hljs-title function_">safe_open</span>(filepath, framework=<span class="hljs-string">"pt"</span>) <span class="hljs-keyword">as</span> <span class="hljs-attr">f</span>:
            <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> f.<span class="hljs-title function_">keys</span>():
                <span class="hljs-keyword">if</span> f<span class="hljs-string">"layers.{layer_to_load}."</span> <span class="hljs-keyword">in</span> <span class="hljs-attr">key</span>:
                    new_key = key.<span class="hljs-title function_">replace</span>(f<span class="hljs-string">"model.layers.{layer_to_load}."</span>, <span class="hljs-string">'layers.0.'</span>)
                    state_dict[new_key] = f.<span class="hljs-title function_">get_tensor</span>(key)

    missing_keys, unexpected_keys = model.<span class="hljs-title function_">load_state_dict</span>(state_dict, strict=<span class="hljs-title class_">False</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-attr">missing_keys</span>:
        <span class="hljs-title function_">print</span>(f<span class="hljs-string">"Missing keys: {missing_keys}"</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-attr">unexpected_keys</span>:
        <span class="hljs-title function_">print</span>(f<span class="hljs-string">"Unexpected keys: {unexpected_keys}"</span>)
</code></pre>
<p>이렇게 하는 이유는 Google Colab GPU의 무료 티어로는 LLama-3-8B를 fp16 정밀도로도 불러올 수 없기 때문입니다. 또한, 이 분석은 np.linalg.svd가 구축된 방식으로 인해 fp32 정밀도에서 작동해야 합니다. 다음으로, 주어진 matrix_type, layer_number 및 head_number에 대해 특이값을 얻는 메인 함수를 정의할 수 있습니다.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">get_singular_values</span>(model_path, matrix_type, layer_number, head_number):
    <span class="hljs-string">""</span><span class="hljs-string">"
    Llama-3 모델의 지정된 행렬의 특이값을 계산합니다.

    Parameters:
    model_path (str): 모델 경로
    matrix_type (str): 행렬 유형 ('q', 'k', 'v', 'o')
    layer_number (int): 레이어 번호 (0에서 31까지)
    head_number (int): 헤드 번호 (0에서 31까지)

    Returns:
    np.array: 특이값의 배열
    "</span><span class="hljs-string">""</span>
    assert matrix_type <span class="hljs-keyword">in</span> [<span class="hljs-string">'q'</span>, <span class="hljs-string">'k'</span>, <span class="hljs-string">'v'</span>, <span class="hljs-string">'o'</span>], <span class="hljs-string">"잘못된 행렬 유형"</span>
    assert <span class="hljs-number">0</span> &#x3C;= layer_number &#x3C; <span class="hljs-number">32</span>, <span class="hljs-string">"잘못된 레이어 번호"</span>
    assert <span class="hljs-number">0</span> &#x3C;= head_number &#x3C; <span class="hljs-number">32</span>, <span class="hljs-string">"잘못된 헤드 번호"</span>

    # <span class="hljs-variable constant_">RAM</span>이 제한되어 있어 사용한 후에도 fp16을 사용해도 제한된 레이어만을 위해 모델을로드합니다.
    config = <span class="hljs-title class_">AutoConfig</span>.<span class="hljs-title function_">from_pretrained</span>(model_path)
    config.<span class="hljs-property">num_hidden_layers</span> = <span class="hljs-number">1</span>
    model = <span class="hljs-title class_">LlamaModel</span>(config)
    <span class="hljs-title function_">load_specific_layers_safetensors</span>(model, model_path, layer_number)

    # 지정된 레이어에 액세스합니다.
    # 특정 레이어를로드했으므로 항상 인덱스 <span class="hljs-number">0</span>을 사용합니다.
    layer = model.<span class="hljs-property">layers</span>[<span class="hljs-number">0</span>]

    # 각 헤드의 크기 결정합니다.
    num_heads = layer.<span class="hljs-property">self_attn</span>.<span class="hljs-property">num_heads</span>
    head_dim = layer.<span class="hljs-property">self_attn</span>.<span class="hljs-property">head_dim</span>

    # 지정된 행렬에 액세스합니다.
    weight_matrix = <span class="hljs-title function_">getattr</span>(layer.<span class="hljs-property">self_attn</span>, f<span class="hljs-string">"{matrix_type}_proj"</span>).<span class="hljs-property">weight</span>.<span class="hljs-title function_">detach</span>().<span class="hljs-title function_">numpy</span>()
    <span class="hljs-keyword">if</span> matrix_type <span class="hljs-keyword">in</span> [<span class="hljs-string">'q'</span>,<span class="hljs-string">'o'</span>]:
        start = head_number * head_dim
        end = (head_number + <span class="hljs-number">1</span>) * head_dim
    <span class="hljs-attr">else</span>:  # <span class="hljs-string">'k'</span>, <span class="hljs-string">'v'</span> matrices
        # num_key_value_heads로 나눠 헤드 번호를 조절합니다.
        # llama3-8b는 그룹화된 쿼리 어텐션을 사용하기 때문에 수행됩니다.
        num_key_value_groups = num_heads <span class="hljs-comment">// config.num_key_value_heads</span>
        head_number_kv = head_number <span class="hljs-comment">// num_key_value_groups</span>
        start = head_number_kv * head_dim
        end = (head_number_kv + <span class="hljs-number">1</span>) * head_dim

    # 지정된 헤드에 대한 가중치를 추출합니다.
    <span class="hljs-keyword">if</span> matrix_type <span class="hljs-keyword">in</span> [<span class="hljs-string">'q'</span>, <span class="hljs-string">'k'</span>, <span class="hljs-string">'v'</span>]:
        weight_matrix = weight_matrix[<span class="hljs-attr">start</span>:end, :]
    <span class="hljs-attr">else</span>:  # <span class="hljs-string">'o'</span> matrix
        weight_matrix = weight_matrix[:, <span class="hljs-attr">start</span>:end]

    # 특이값 계산합니다.
    singular_values = np.<span class="hljs-property">linalg</span>.<span class="hljs-title function_">svd</span>(weight_matrix, compute_uv=<span class="hljs-title class_">False</span>)

    del model, config

    <span class="hljs-keyword">return</span> <span class="hljs-title function_">list</span>(singular_values)
</code></pre>
<p>HuggingFace에서 구현된 방식으로 인해 K, Q 및 V 행렬에 대한 지정된 헤드의 가중치를 추출할 수 있는 이유는 행별로 슬라이싱을 통해할 수 있습니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_1.png" alt="이미지"></p>
<p>O 행렬의 경우 선형 대수를 통해 O 가중치에서 지정된 헤드에 대한 가중치를 추출하기 위해 열별로 슬라이싱을 할 수 있습니다! 자세한 내용은 다음 그림에서 확인할 수 있습니다.</p>
<p><img src="/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_2.png" alt="이미지"></p>
<h2>결과</h2>
<div class="content-ad"></div>
<p>분석을 위해 다양한 헤드, 레이어 및 행렬 유형에서 get_singular_values() 함수를 실행해야 합니다. 그리고 이러한 다양한 조합을 비교할 수 있도록 분석을 위한 여러 보조 지표도 정의해야 합니다:</p>
<ul>
<li>상위 10개 비율: 상위 10개 특이값의 합과 모든 특이값의 합 사이의 비율</li>
<li>첫 번째/마지막 비율: 가장 높은 특이값과 가장 낮은 특이값 간의 비율</li>
<li>최소 10개 비율: 최소 10개 특이값의 합과 모든 특이값의 합 사이의 비율</li>
</ul>
<p>(레이어 0, 헤드 0) 분석</p>
<ul>
<li>Q(쿼리) 행렬은 초기 최대 특이값(약 10)을 갖고 있으며, 다음으로 K(키) 행렬(약 8)이 있습니다. 이 2개의 행렬은 초기 특이값이 V(값)와 O(출력) 행렬보다 현저히 높습니다.</li>
<li>초기 특이값 뿐만 아니라, Q와 K 행렬의 상위 10개 비율과 첫 번째/마지막 비율을 확인하면, 이 두 행렬이 V와 O 행렬보다 훨씬 높은 값을 갖는다는 것을 알 수 있습니다. 이는 Q와 K 행렬이 대부분의 차원에 집중된 정보를 포함하고 있으며, V와 O 행렬은 정보가 구성요소 전반에 분산되어 있는 것을 시사합니다.</li>
<li>최소 10개 비율을 살펴보면, Q와 K 행렬의 특이값이 거의 0에 가깝고 V와 O 행렬에 비해 상대적으로 훨씬 낮다는 것을 알 수 있습니다. 이는 Q와 K 행렬이 저랭크 구조를 가지고 있음을 나타내는 증거 중 하나이며, 이 차원들이 모델의 전반적인 성능에 미미한 영향을 미칩니다. 이러한 가중치는 구조적으로 제거하여 모델의 정확도에 큰 영향을 미치지 않는 경우가 있을 수 있습니다.</li>
</ul>
<div class="content-ad"></div>
<h2>(레이어 0, 다중 헤드) 분석</h2>
<ul>
<li>헤드 번호가 증가함에 따라 Q 및 K 행렬의 상위 10 비율은 V 및 O 행렬보다 훨씬 빠른 속도로 증가하는 경향이 있습니다. 이 관찰 결과는 Q 및 K 행렬의 최하 10 비율에도 동일하게 적용되며, 헤드 번호가 증가함에 따라 값이 0에 가까워지는 경향을 보입니다. 그러나 V 및 O 행렬에는 해당 경향이 나타나지 않습니다.</li>
<li>이 결과는 헤드 번호가 높은 헤드의 Q 및 K 행렬이 낮은 차원에서 정보를 저장하는 경향이 있다는 것을 나타냅니다. 다시 말해, 헤드 번호가 증가함에 따라 Q 및 K 행렬은 더 적은 차원에서 정보를 저장하려고 합니다.</li>
</ul>
<h2>교차-레이어 분석</h2>
<ul>
<li>더 깊은 레이어로 갈수록, Q 및 K 행렬의 초기값이 감소되는 경향을 발견했지만, 여전히 V 및 O 행렬과 비교하면 비교적 높습니다.</li>
<li>더 깊은 레이어로 갈수록, 특정 헤드의 Q 및 K 행렬의 상위 10 비율 및 첫 번째/마지막 비율에 대한 하락 트렌드 패턴이 나타납니다. 또한 최하 10 비율의 약간의 상승 트렌드 패턴이 있습니다. 이는 더 깊은 레이어의 Q 및 K 행렬이 낮은 레이어와 비교하여 더 잘 훈련된 것으로 나타납니다.</li>
</ul>
<div class="content-ad"></div>
<ul>
<li>"레이어 0, 다중 헤드" 섹션에서 발견한 동일 레이어 내의 헤드 간 패턴은 더 깊은 레이어로 이동할 때 명확하지 않습니다.</li>
</ul>
<p>요약</p>
<ul>
<li>K 및 Q 행렬은 V 및 O 행렬과 비교하여 상대적으로 낮은 순위를 가지고 있습니다. 가지치기(pruning) 또는 차원 축소 방법을 수행하려면 K 및 Q 행렬에 더 집중할 수 있습니다.</li>
<li>레이어가 깊어질수록 모든 (K, Q, V, O) 행렬이 더 잘 훈련됩니다. 가지치기 또는 차원 축소 방법을 수행하려면 낮은 레이어에 더 집중할 수 있습니다.</li>
<li>가지치기 외에도 초기 몇 레이어에서만 전체 미세 조정을 수행하거나 LoRA로도 이를 수행하는 것이 흥미로울 수 있습니다.</li>
</ul>
<h1>마무리 말씀</h1>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_3.png" alt="Image"></p>
<p>이 시점까지 참석해 주셔서 축하드립니다! 이 기사에서 새로운 것을 배우셨으면 좋겠습니다. 선형 대수의 좋은 오래된 개념들을 적용하여, LLM의 훈련이 얼마나 잘 이루어졌는지 이해하는 것은 정말 흥미롭습니다.</p>
<p>이 유형의 콘텐츠를 좋아하신다면, 저의 Medium 계정을 팔로우해주시어 앞으로의 다른 글 알림을 받아보세요.</p>
<h1>저자 소개</h1>
<div class="content-ad"></div>
<p>루이스 오웬은 인도네시아 출신의 데이터 과학자 및 AI 연구 엔지니어로, 항상 새로운 지식에 굶주립니다. 그의 경력 여정을 통해 그는 비영리 단체, 전자 상거래, 대화형 AI, OTA, 스마트 시티 및 핀테크 등 다양한 산업 분야에서 일해 왔습니다. 일 안에서 해외에선, 그는 자신의 기사나 멘토링 세션을 통해 데이터 과학 애호가들이 데이터 과학자로 성장할 수 있도록 시간을 보내는 것을 즐깁니다.</p>
<p>지금은 루이스가 전 세계적인 CX 자동화 플랫폼 인 Yellow.ai의 NLP 연구 엔지니어로 일하고 있습니다. 루이스의 웹사이트를 방문하여 그에 대해 더 알아보세요! 마지막으로, 궁금한 점이나 이야기할 주제가 있으면 망설이지 마시고 LinkedIn을 통해 루이스에게 연락해보세요.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"LLM의 내부 작업 공개 고유 값 관점","description":"","date":"2024-06-19 20:37","slug":"2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective","content":"\n\n## Llama3–8B 투영 행렬에 대한 특이값 분해 분석\n\n![이미지](/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_0.png)\n\nLLM이 얼마나 잘 훈련되었는지 생각해 보셨나요? 매개변수의 수가 많은데, 그 매개변수들이 훈련 데이터로부터 정보나 지식을 최대한으로 얻어내고 있는지 궁금해 하시지 않나요? 그렇지 않다면, LLM에서 유용하지 않은 매개변수들을 제거하여 더 효율적으로 만들 수 있을까요?\n\n이 글에서는 Singular Values 관점에서 Llama-3–8B 모델을 깊게 분석하여 이러한 질문에 답해보겠습니다. 더 이상 시간을 낭비하지 말고 편안하게 앉아, SVD를 적용하여 Llama-3–8B 행렬의 품질을 분석해 보세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# SVD 다시 살펴보기\n\n특이값 분해(SVD)에서 행렬 A는 세 가지 다른 행렬로 분해됩니다:\n\n여기서:\n\n- A는 원래 행렬입니다.\n- U는 A의 왼쪽 특이벡터인 열로 이루어진 행렬입니다.\n- Σ은 A의 특이값을 포함하는 대각행렬입니다. 이 값들은 항상 음이 아닌 값이며 일반적으로 가장 큰 값부터 가장 작은 값 순서로 정렬됩니다.\n- V_t는 V의 전치행렬이며, V의 열은 A의 오른쪽 특이벡터입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 간단한 용어로 설명하면, 특이값 분해(SVD)는 행렬의 복잡한 변환을 간단하고 이해하기 쉬운 회전 및 스케일링 과정으로 나누어 줍니다. Σ의 특이값은 스케일링 요소를 알려주고 U와 V_t의 특이벡터는 해당 스케일링이 행렬을 적용하기 전과 후의 방향을 알려줍니다.\n\n특이값은 행렬이 공간에서 다양한 방향으로 얼마나 늘어나거나 줄어드는지를 측정하는 방법으로 생각할 수 있습니다. 각 특이값은 특이벡터 쌍에 해당되며, 하나는 오른쪽 특이벡터(입력 공간에서의 방향), 다른 하나는 왼쪽 특이벡터(출력 공간에서의 방향)입니다.\n\n행렬의 특이값이 급격하게 감소하는 경우(가장 큰 특이값이 작은 것들보다 현저히 큰 경우), 이는 행렬의 유효 랭크(중요한 특이값의 수)가 실제 행렬의 차원보다 훨씬 작다는 것을 의미합니다. 이는 행렬이 낮은 랭크 행렬로 잘 근사될 수 있음을 시사합니다.\n\nLLM(대형 언어 모델)의 맥락에서, 가중치 행렬(예: 어텐션 메커니즘 또는 피드포워드 레이어의 행렬)들은 입력 데이터(예: 단어 임베딩)를 출력 표현으로 변환합니다. 주요한 특이값은 변환에 의해 가장 강조되는 입력 공간의 방향을 나타내며, 모델이 민감하거나 표현력이 강한 방향을 보여줍니다. 작은 특이값은 변환에서 중요하지 않거나 영향력이 적은 방향을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n특이값의 분포는 모델의 일반화 능력과 견고성에 영향을 줄 수 있습니다. 느린 감소(많은 큰 특이값)는 과적합을 초래할 수 있으며, 빠른 감소(소수의 큰 특이값)는 과소적합이거나 정보의 손실을 나타낼 수 있습니다.\n\n# Llama-3 아키텍처 재방문\n\n다음은 meta-llama/Meta-Llama-3-8B-Instructmodel의 config.json 파일입니다. 이 LLM은 8개의 num_key_value_heads를 사용하여 Grouped Query Attention을 활용하며, 이는 그룹 크기가 32/8=4임을 의미합니다.\n\n```js\n{\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.40.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# (Q, K, V, O) 행렬의 특이값 분석\n\n자, 이제 이 기사의 본격적인 내용으로 들어가 봅시다. Llama-3–8B-Instruct 모델의 (Q, K, V, O) 행렬들을 그들의 특이값을 통해 분석해 보겠습니다!\n\n## 코드\n\n우선, 이 분석에 필요한 모든 패키지를 가져와 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport transformers\nimport torch\nimport numpy as np\nfrom transformers import AutoConfig, LlamaModel\nfrom safetensors import safe_open\nimport os\nimport matplotlib.pyplot as plt\n```\n\n그런 다음, 모델을 다운로드하고 로컬 /tmp디렉토리에 저장합시다.\n\n```js\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n!huggingface-cli download {MODEL_ID} --quiet --local-dir /tmp/{MODEL_ID}\n```\n\n만약 GPU를 많이 가지고 계신 분이시라면, 다음 코드는 관련이 없을 수 있습니다. 그러나 저와 같이 GPU가 부족한 분들에겐, LLama-3–8B 모델의 특정 레이어만 로드하는 데 매우 유용할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef load_specific_layers_safetensors(model, model_name, layer_to_load):\n    state_dict = {}\n    files = [f for f in os.listdir(model_name) if f.endswith('.safetensors')]\n    for file in files:\n        filepath = os.path.join(model_name, file)\n        with safe_open(filepath, framework=\"pt\") as f:\n            for key in f.keys():\n                if f\"layers.{layer_to_load}.\" in key:\n                    new_key = key.replace(f\"model.layers.{layer_to_load}.\", 'layers.0.')\n                    state_dict[new_key] = f.get_tensor(key)\n\n    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n    if missing_keys:\n        print(f\"Missing keys: {missing_keys}\")\n    if unexpected_keys:\n        print(f\"Unexpected keys: {unexpected_keys}\")\n```\n\n이렇게 하는 이유는 Google Colab GPU의 무료 티어로는 LLama-3-8B를 fp16 정밀도로도 불러올 수 없기 때문입니다. 또한, 이 분석은 np.linalg.svd가 구축된 방식으로 인해 fp32 정밀도에서 작동해야 합니다. 다음으로, 주어진 matrix_type, layer_number 및 head_number에 대해 특이값을 얻는 메인 함수를 정의할 수 있습니다.\n\n```js\ndef get_singular_values(model_path, matrix_type, layer_number, head_number):\n    \"\"\"\n    Llama-3 모델의 지정된 행렬의 특이값을 계산합니다.\n\n    Parameters:\n    model_path (str): 모델 경로\n    matrix_type (str): 행렬 유형 ('q', 'k', 'v', 'o')\n    layer_number (int): 레이어 번호 (0에서 31까지)\n    head_number (int): 헤드 번호 (0에서 31까지)\n\n    Returns:\n    np.array: 특이값의 배열\n    \"\"\"\n    assert matrix_type in ['q', 'k', 'v', 'o'], \"잘못된 행렬 유형\"\n    assert 0 \u003c= layer_number \u003c 32, \"잘못된 레이어 번호\"\n    assert 0 \u003c= head_number \u003c 32, \"잘못된 헤드 번호\"\n\n    # RAM이 제한되어 있어 사용한 후에도 fp16을 사용해도 제한된 레이어만을 위해 모델을로드합니다.\n    config = AutoConfig.from_pretrained(model_path)\n    config.num_hidden_layers = 1\n    model = LlamaModel(config)\n    load_specific_layers_safetensors(model, model_path, layer_number)\n\n    # 지정된 레이어에 액세스합니다.\n    # 특정 레이어를로드했으므로 항상 인덱스 0을 사용합니다.\n    layer = model.layers[0]\n\n    # 각 헤드의 크기 결정합니다.\n    num_heads = layer.self_attn.num_heads\n    head_dim = layer.self_attn.head_dim\n\n    # 지정된 행렬에 액세스합니다.\n    weight_matrix = getattr(layer.self_attn, f\"{matrix_type}_proj\").weight.detach().numpy()\n    if matrix_type in ['q','o']:\n        start = head_number * head_dim\n        end = (head_number + 1) * head_dim\n    else:  # 'k', 'v' matrices\n        # num_key_value_heads로 나눠 헤드 번호를 조절합니다.\n        # llama3-8b는 그룹화된 쿼리 어텐션을 사용하기 때문에 수행됩니다.\n        num_key_value_groups = num_heads // config.num_key_value_heads\n        head_number_kv = head_number // num_key_value_groups\n        start = head_number_kv * head_dim\n        end = (head_number_kv + 1) * head_dim\n\n    # 지정된 헤드에 대한 가중치를 추출합니다.\n    if matrix_type in ['q', 'k', 'v']:\n        weight_matrix = weight_matrix[start:end, :]\n    else:  # 'o' matrix\n        weight_matrix = weight_matrix[:, start:end]\n\n    # 특이값 계산합니다.\n    singular_values = np.linalg.svd(weight_matrix, compute_uv=False)\n\n    del model, config\n\n    return list(singular_values)\n```\n\nHuggingFace에서 구현된 방식으로 인해 K, Q 및 V 행렬에 대한 지정된 헤드의 가중치를 추출할 수 있는 이유는 행별로 슬라이싱을 통해할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_1.png)\n\nO 행렬의 경우 선형 대수를 통해 O 가중치에서 지정된 헤드에 대한 가중치를 추출하기 위해 열별로 슬라이싱을 할 수 있습니다! 자세한 내용은 다음 그림에서 확인할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_2.png)\n\n## 결과\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n분석을 위해 다양한 헤드, 레이어 및 행렬 유형에서 get_singular_values() 함수를 실행해야 합니다. 그리고 이러한 다양한 조합을 비교할 수 있도록 분석을 위한 여러 보조 지표도 정의해야 합니다:\n\n- 상위 10개 비율: 상위 10개 특이값의 합과 모든 특이값의 합 사이의 비율\n- 첫 번째/마지막 비율: 가장 높은 특이값과 가장 낮은 특이값 간의 비율\n- 최소 10개 비율: 최소 10개 특이값의 합과 모든 특이값의 합 사이의 비율\n\n(레이어 0, 헤드 0) 분석\n\n- Q(쿼리) 행렬은 초기 최대 특이값(약 10)을 갖고 있으며, 다음으로 K(키) 행렬(약 8)이 있습니다. 이 2개의 행렬은 초기 특이값이 V(값)와 O(출력) 행렬보다 현저히 높습니다.\n- 초기 특이값 뿐만 아니라, Q와 K 행렬의 상위 10개 비율과 첫 번째/마지막 비율을 확인하면, 이 두 행렬이 V와 O 행렬보다 훨씬 높은 값을 갖는다는 것을 알 수 있습니다. 이는 Q와 K 행렬이 대부분의 차원에 집중된 정보를 포함하고 있으며, V와 O 행렬은 정보가 구성요소 전반에 분산되어 있는 것을 시사합니다.\n- 최소 10개 비율을 살펴보면, Q와 K 행렬의 특이값이 거의 0에 가깝고 V와 O 행렬에 비해 상대적으로 훨씬 낮다는 것을 알 수 있습니다. 이는 Q와 K 행렬이 저랭크 구조를 가지고 있음을 나타내는 증거 중 하나이며, 이 차원들이 모델의 전반적인 성능에 미미한 영향을 미칩니다. 이러한 가중치는 구조적으로 제거하여 모델의 정확도에 큰 영향을 미치지 않는 경우가 있을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## (레이어 0, 다중 헤드) 분석\n\n- 헤드 번호가 증가함에 따라 Q 및 K 행렬의 상위 10 비율은 V 및 O 행렬보다 훨씬 빠른 속도로 증가하는 경향이 있습니다. 이 관찰 결과는 Q 및 K 행렬의 최하 10 비율에도 동일하게 적용되며, 헤드 번호가 증가함에 따라 값이 0에 가까워지는 경향을 보입니다. 그러나 V 및 O 행렬에는 해당 경향이 나타나지 않습니다.\n- 이 결과는 헤드 번호가 높은 헤드의 Q 및 K 행렬이 낮은 차원에서 정보를 저장하는 경향이 있다는 것을 나타냅니다. 다시 말해, 헤드 번호가 증가함에 따라 Q 및 K 행렬은 더 적은 차원에서 정보를 저장하려고 합니다.\n\n## 교차-레이어 분석\n\n- 더 깊은 레이어로 갈수록, Q 및 K 행렬의 초기값이 감소되는 경향을 발견했지만, 여전히 V 및 O 행렬과 비교하면 비교적 높습니다.\n- 더 깊은 레이어로 갈수록, 특정 헤드의 Q 및 K 행렬의 상위 10 비율 및 첫 번째/마지막 비율에 대한 하락 트렌드 패턴이 나타납니다. 또한 최하 10 비율의 약간의 상승 트렌드 패턴이 있습니다. 이는 더 깊은 레이어의 Q 및 K 행렬이 낮은 레이어와 비교하여 더 잘 훈련된 것으로 나타납니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- \"레이어 0, 다중 헤드\" 섹션에서 발견한 동일 레이어 내의 헤드 간 패턴은 더 깊은 레이어로 이동할 때 명확하지 않습니다. \n\n요약\n\n- K 및 Q 행렬은 V 및 O 행렬과 비교하여 상대적으로 낮은 순위를 가지고 있습니다. 가지치기(pruning) 또는 차원 축소 방법을 수행하려면 K 및 Q 행렬에 더 집중할 수 있습니다.\n- 레이어가 깊어질수록 모든 (K, Q, V, O) 행렬이 더 잘 훈련됩니다. 가지치기 또는 차원 축소 방법을 수행하려면 낮은 레이어에 더 집중할 수 있습니다.\n- 가지치기 외에도 초기 몇 레이어에서만 전체 미세 조정을 수행하거나 LoRA로도 이를 수행하는 것이 흥미로울 수 있습니다.\n\n# 마무리 말씀\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_3.png)\n\n이 시점까지 참석해 주셔서 축하드립니다! 이 기사에서 새로운 것을 배우셨으면 좋겠습니다. 선형 대수의 좋은 오래된 개념들을 적용하여, LLM의 훈련이 얼마나 잘 이루어졌는지 이해하는 것은 정말 흥미롭습니다.\n\n이 유형의 콘텐츠를 좋아하신다면, 저의 Medium 계정을 팔로우해주시어 앞으로의 다른 글 알림을 받아보세요.\n\n# 저자 소개\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n루이스 오웬은 인도네시아 출신의 데이터 과학자 및 AI 연구 엔지니어로, 항상 새로운 지식에 굶주립니다. 그의 경력 여정을 통해 그는 비영리 단체, 전자 상거래, 대화형 AI, OTA, 스마트 시티 및 핀테크 등 다양한 산업 분야에서 일해 왔습니다. 일 안에서 해외에선, 그는 자신의 기사나 멘토링 세션을 통해 데이터 과학 애호가들이 데이터 과학자로 성장할 수 있도록 시간을 보내는 것을 즐깁니다.\n\n지금은 루이스가 전 세계적인 CX 자동화 플랫폼 인 Yellow.ai의 NLP 연구 엔지니어로 일하고 있습니다. 루이스의 웹사이트를 방문하여 그에 대해 더 알아보세요! 마지막으로, 궁금한 점이나 이야기할 주제가 있으면 망설이지 마시고 LinkedIn을 통해 루이스에게 연락해보세요.","ogImage":{"url":"/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_0.png"},"coverImage":"/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_0.png","tag":["Tech"],"readingTime":9},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003eLlama3–8B 투영 행렬에 대한 특이값 분해 분석\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eLLM이 얼마나 잘 훈련되었는지 생각해 보셨나요? 매개변수의 수가 많은데, 그 매개변수들이 훈련 데이터로부터 정보나 지식을 최대한으로 얻어내고 있는지 궁금해 하시지 않나요? 그렇지 않다면, LLM에서 유용하지 않은 매개변수들을 제거하여 더 효율적으로 만들 수 있을까요?\u003c/p\u003e\n\u003cp\u003e이 글에서는 Singular Values 관점에서 Llama-3–8B 모델을 깊게 분석하여 이러한 질문에 답해보겠습니다. 더 이상 시간을 낭비하지 말고 편안하게 앉아, SVD를 적용하여 Llama-3–8B 행렬의 품질을 분석해 보세요!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003eSVD 다시 살펴보기\u003c/h1\u003e\n\u003cp\u003e특이값 분해(SVD)에서 행렬 A는 세 가지 다른 행렬로 분해됩니다:\u003c/p\u003e\n\u003cp\u003e여기서:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA는 원래 행렬입니다.\u003c/li\u003e\n\u003cli\u003eU는 A의 왼쪽 특이벡터인 열로 이루어진 행렬입니다.\u003c/li\u003e\n\u003cli\u003eΣ은 A의 특이값을 포함하는 대각행렬입니다. 이 값들은 항상 음이 아닌 값이며 일반적으로 가장 큰 값부터 가장 작은 값 순서로 정렬됩니다.\u003c/li\u003e\n\u003cli\u003eV_t는 V의 전치행렬이며, V의 열은 A의 오른쪽 특이벡터입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e더 간단한 용어로 설명하면, 특이값 분해(SVD)는 행렬의 복잡한 변환을 간단하고 이해하기 쉬운 회전 및 스케일링 과정으로 나누어 줍니다. Σ의 특이값은 스케일링 요소를 알려주고 U와 V_t의 특이벡터는 해당 스케일링이 행렬을 적용하기 전과 후의 방향을 알려줍니다.\u003c/p\u003e\n\u003cp\u003e특이값은 행렬이 공간에서 다양한 방향으로 얼마나 늘어나거나 줄어드는지를 측정하는 방법으로 생각할 수 있습니다. 각 특이값은 특이벡터 쌍에 해당되며, 하나는 오른쪽 특이벡터(입력 공간에서의 방향), 다른 하나는 왼쪽 특이벡터(출력 공간에서의 방향)입니다.\u003c/p\u003e\n\u003cp\u003e행렬의 특이값이 급격하게 감소하는 경우(가장 큰 특이값이 작은 것들보다 현저히 큰 경우), 이는 행렬의 유효 랭크(중요한 특이값의 수)가 실제 행렬의 차원보다 훨씬 작다는 것을 의미합니다. 이는 행렬이 낮은 랭크 행렬로 잘 근사될 수 있음을 시사합니다.\u003c/p\u003e\n\u003cp\u003eLLM(대형 언어 모델)의 맥락에서, 가중치 행렬(예: 어텐션 메커니즘 또는 피드포워드 레이어의 행렬)들은 입력 데이터(예: 단어 임베딩)를 출력 표현으로 변환합니다. 주요한 특이값은 변환에 의해 가장 강조되는 입력 공간의 방향을 나타내며, 모델이 민감하거나 표현력이 강한 방향을 보여줍니다. 작은 특이값은 변환에서 중요하지 않거나 영향력이 적은 방향을 나타냅니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e특이값의 분포는 모델의 일반화 능력과 견고성에 영향을 줄 수 있습니다. 느린 감소(많은 큰 특이값)는 과적합을 초래할 수 있으며, 빠른 감소(소수의 큰 특이값)는 과소적합이거나 정보의 손실을 나타낼 수 있습니다.\u003c/p\u003e\n\u003ch1\u003eLlama-3 아키텍처 재방문\u003c/h1\u003e\n\u003cp\u003e다음은 meta-llama/Meta-Llama-3-8B-Instructmodel의 config.json 파일입니다. 이 LLM은 8개의 num_key_value_heads를 사용하여 Grouped Query Attention을 활용하며, 이는 그룹 크기가 32/8=4임을 의미합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e{\n  \u003cspan class=\"hljs-string\"\u003e\"architectures\"\u003c/span\u003e: [\n    \u003cspan class=\"hljs-string\"\u003e\"LlamaForCausalLM\"\u003c/span\u003e\n  ],\n  \u003cspan class=\"hljs-string\"\u003e\"attention_bias\"\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003efalse\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"attention_dropout\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"bos_token_id\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e128000\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"eos_token_id\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e128009\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"hidden_act\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"silu\"\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"hidden_size\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e4096\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"initializer_range\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0.02\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"intermediate_size\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e14336\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"max_position_embeddings\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e8192\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"model_type\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"llama\"\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"num_attention_heads\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"num_hidden_layers\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"num_key_value_heads\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"pretraining_tp\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"rms_norm_eps\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1e-05\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"rope_scaling\"\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003enull\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"rope_theta\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e500000.0\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"tie_word_embeddings\"\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003efalse\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"torch_dtype\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"bfloat16\"\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"transformers_version\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"4.40.0.dev0\"\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"use_cache\"\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003etrue\u003c/span\u003e,\n  \u003cspan class=\"hljs-string\"\u003e\"vocab_size\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e128256\u003c/span\u003e\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e(Q, K, V, O) 행렬의 특이값 분석\u003c/h1\u003e\n\u003cp\u003e자, 이제 이 기사의 본격적인 내용으로 들어가 봅시다. Llama-3–8B-Instruct 모델의 (Q, K, V, O) 행렬들을 그들의 특이값을 통해 분석해 보겠습니다!\u003c/p\u003e\n\u003ch2\u003e코드\u003c/h2\u003e\n\u003cp\u003e우선, 이 분석에 필요한 모든 패키지를 가져와 봅시다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e transformers\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAutoConfig\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eLlamaModel\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e safetensors \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e safe_open\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.\u003cspan class=\"hljs-property\"\u003epyplot\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그런 다음, 모델을 다운로드하고 로컬 /tmp디렉토리에 저장합시다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-variable constant_\"\u003eMODEL_ID\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e\"meta-llama/Meta-Llama-3-8B-Instruct\"\u003c/span\u003e\n!huggingface-cli download {\u003cspan class=\"hljs-variable constant_\"\u003eMODEL_ID\u003c/span\u003e} --quiet --local-dir /tmp/{\u003cspan class=\"hljs-variable constant_\"\u003eMODEL_ID\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e만약 GPU를 많이 가지고 계신 분이시라면, 다음 코드는 관련이 없을 수 있습니다. 그러나 저와 같이 GPU가 부족한 분들에겐, LLama-3–8B 모델의 특정 레이어만 로드하는 데 매우 유용할 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eload_specific_layers_safetensors\u003c/span\u003e(model, model_name, layer_to_load):\n    state_dict = {}\n    files = [f \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e f \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e os.\u003cspan class=\"hljs-title function_\"\u003elistdir\u003c/span\u003e(model_name) \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e f.\u003cspan class=\"hljs-title function_\"\u003eendswith\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'.safetensors'\u003c/span\u003e)]\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e file \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003efiles\u003c/span\u003e:\n        filepath = os.\u003cspan class=\"hljs-property\"\u003epath\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ejoin\u003c/span\u003e(model_name, file)\n        \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esafe_open\u003c/span\u003e(filepath, framework=\u003cspan class=\"hljs-string\"\u003e\"pt\"\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ef\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e key \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e f.\u003cspan class=\"hljs-title function_\"\u003ekeys\u003c/span\u003e():\n                \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e f\u003cspan class=\"hljs-string\"\u003e\"layers.{layer_to_load}.\"\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ekey\u003c/span\u003e:\n                    new_key = key.\u003cspan class=\"hljs-title function_\"\u003ereplace\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"model.layers.{layer_to_load}.\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'layers.0.'\u003c/span\u003e)\n                    state_dict[new_key] = f.\u003cspan class=\"hljs-title function_\"\u003eget_tensor\u003c/span\u003e(key)\n\n    missing_keys, unexpected_keys = model.\u003cspan class=\"hljs-title function_\"\u003eload_state_dict\u003c/span\u003e(state_dict, strict=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003emissing_keys\u003c/span\u003e:\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"Missing keys: {missing_keys}\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eunexpected_keys\u003c/span\u003e:\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"Unexpected keys: {unexpected_keys}\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이렇게 하는 이유는 Google Colab GPU의 무료 티어로는 LLama-3-8B를 fp16 정밀도로도 불러올 수 없기 때문입니다. 또한, 이 분석은 np.linalg.svd가 구축된 방식으로 인해 fp32 정밀도에서 작동해야 합니다. 다음으로, 주어진 matrix_type, layer_number 및 head_number에 대해 특이값을 얻는 메인 함수를 정의할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eget_singular_values\u003c/span\u003e(model_path, matrix_type, layer_number, head_number):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    Llama-3 모델의 지정된 행렬의 특이값을 계산합니다.\n\n    Parameters:\n    model_path (str): 모델 경로\n    matrix_type (str): 행렬 유형 ('q', 'k', 'v', 'o')\n    layer_number (int): 레이어 번호 (0에서 31까지)\n    head_number (int): 헤드 번호 (0에서 31까지)\n\n    Returns:\n    np.array: 특이값의 배열\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    assert matrix_type \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e [\u003cspan class=\"hljs-string\"\u003e'q'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'k'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'v'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'o'\u003c/span\u003e], \u003cspan class=\"hljs-string\"\u003e\"잘못된 행렬 유형\"\u003c/span\u003e\n    assert \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e \u0026#x3C;= layer_number \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"잘못된 레이어 번호\"\u003c/span\u003e\n    assert \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e \u0026#x3C;= head_number \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"잘못된 헤드 번호\"\u003c/span\u003e\n\n    # \u003cspan class=\"hljs-variable constant_\"\u003eRAM\u003c/span\u003e이 제한되어 있어 사용한 후에도 fp16을 사용해도 제한된 레이어만을 위해 모델을로드합니다.\n    config = \u003cspan class=\"hljs-title class_\"\u003eAutoConfig\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(model_path)\n    config.\u003cspan class=\"hljs-property\"\u003enum_hidden_layers\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n    model = \u003cspan class=\"hljs-title class_\"\u003eLlamaModel\u003c/span\u003e(config)\n    \u003cspan class=\"hljs-title function_\"\u003eload_specific_layers_safetensors\u003c/span\u003e(model, model_path, layer_number)\n\n    # 지정된 레이어에 액세스합니다.\n    # 특정 레이어를로드했으므로 항상 인덱스 \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e을 사용합니다.\n    layer = model.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n\n    # 각 헤드의 크기 결정합니다.\n    num_heads = layer.\u003cspan class=\"hljs-property\"\u003eself_attn\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003enum_heads\u003c/span\u003e\n    head_dim = layer.\u003cspan class=\"hljs-property\"\u003eself_attn\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ehead_dim\u003c/span\u003e\n\n    # 지정된 행렬에 액세스합니다.\n    weight_matrix = \u003cspan class=\"hljs-title function_\"\u003egetattr\u003c/span\u003e(layer.\u003cspan class=\"hljs-property\"\u003eself_attn\u003c/span\u003e, f\u003cspan class=\"hljs-string\"\u003e\"{matrix_type}_proj\"\u003c/span\u003e).\u003cspan class=\"hljs-property\"\u003eweight\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003edetach\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003enumpy\u003c/span\u003e()\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e matrix_type \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e [\u003cspan class=\"hljs-string\"\u003e'q'\u003c/span\u003e,\u003cspan class=\"hljs-string\"\u003e'o'\u003c/span\u003e]:\n        start = head_number * head_dim\n        end = (head_number + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) * head_dim\n    \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:  # \u003cspan class=\"hljs-string\"\u003e'k'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'v'\u003c/span\u003e matrices\n        # num_key_value_heads로 나눠 헤드 번호를 조절합니다.\n        # llama3-8b는 그룹화된 쿼리 어텐션을 사용하기 때문에 수행됩니다.\n        num_key_value_groups = num_heads \u003cspan class=\"hljs-comment\"\u003e// config.num_key_value_heads\u003c/span\u003e\n        head_number_kv = head_number \u003cspan class=\"hljs-comment\"\u003e// num_key_value_groups\u003c/span\u003e\n        start = head_number_kv * head_dim\n        end = (head_number_kv + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) * head_dim\n\n    # 지정된 헤드에 대한 가중치를 추출합니다.\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e matrix_type \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e [\u003cspan class=\"hljs-string\"\u003e'q'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'k'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'v'\u003c/span\u003e]:\n        weight_matrix = weight_matrix[\u003cspan class=\"hljs-attr\"\u003estart\u003c/span\u003e:end, :]\n    \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:  # \u003cspan class=\"hljs-string\"\u003e'o'\u003c/span\u003e matrix\n        weight_matrix = weight_matrix[:, \u003cspan class=\"hljs-attr\"\u003estart\u003c/span\u003e:end]\n\n    # 특이값 계산합니다.\n    singular_values = np.\u003cspan class=\"hljs-property\"\u003elinalg\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003esvd\u003c/span\u003e(weight_matrix, compute_uv=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\n\n    del model, config\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003elist\u003c/span\u003e(singular_values)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHuggingFace에서 구현된 방식으로 인해 K, Q 및 V 행렬에 대한 지정된 헤드의 가중치를 추출할 수 있는 이유는 행별로 슬라이싱을 통해할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eO 행렬의 경우 선형 대수를 통해 O 가중치에서 지정된 헤드에 대한 가중치를 추출하기 위해 열별로 슬라이싱을 할 수 있습니다! 자세한 내용은 다음 그림에서 확인할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch2\u003e결과\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e분석을 위해 다양한 헤드, 레이어 및 행렬 유형에서 get_singular_values() 함수를 실행해야 합니다. 그리고 이러한 다양한 조합을 비교할 수 있도록 분석을 위한 여러 보조 지표도 정의해야 합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e상위 10개 비율: 상위 10개 특이값의 합과 모든 특이값의 합 사이의 비율\u003c/li\u003e\n\u003cli\u003e첫 번째/마지막 비율: 가장 높은 특이값과 가장 낮은 특이값 간의 비율\u003c/li\u003e\n\u003cli\u003e최소 10개 비율: 최소 10개 특이값의 합과 모든 특이값의 합 사이의 비율\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e(레이어 0, 헤드 0) 분석\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eQ(쿼리) 행렬은 초기 최대 특이값(약 10)을 갖고 있으며, 다음으로 K(키) 행렬(약 8)이 있습니다. 이 2개의 행렬은 초기 특이값이 V(값)와 O(출력) 행렬보다 현저히 높습니다.\u003c/li\u003e\n\u003cli\u003e초기 특이값 뿐만 아니라, Q와 K 행렬의 상위 10개 비율과 첫 번째/마지막 비율을 확인하면, 이 두 행렬이 V와 O 행렬보다 훨씬 높은 값을 갖는다는 것을 알 수 있습니다. 이는 Q와 K 행렬이 대부분의 차원에 집중된 정보를 포함하고 있으며, V와 O 행렬은 정보가 구성요소 전반에 분산되어 있는 것을 시사합니다.\u003c/li\u003e\n\u003cli\u003e최소 10개 비율을 살펴보면, Q와 K 행렬의 특이값이 거의 0에 가깝고 V와 O 행렬에 비해 상대적으로 훨씬 낮다는 것을 알 수 있습니다. 이는 Q와 K 행렬이 저랭크 구조를 가지고 있음을 나타내는 증거 중 하나이며, 이 차원들이 모델의 전반적인 성능에 미미한 영향을 미칩니다. 이러한 가중치는 구조적으로 제거하여 모델의 정확도에 큰 영향을 미치지 않는 경우가 있을 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e(레이어 0, 다중 헤드) 분석\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e헤드 번호가 증가함에 따라 Q 및 K 행렬의 상위 10 비율은 V 및 O 행렬보다 훨씬 빠른 속도로 증가하는 경향이 있습니다. 이 관찰 결과는 Q 및 K 행렬의 최하 10 비율에도 동일하게 적용되며, 헤드 번호가 증가함에 따라 값이 0에 가까워지는 경향을 보입니다. 그러나 V 및 O 행렬에는 해당 경향이 나타나지 않습니다.\u003c/li\u003e\n\u003cli\u003e이 결과는 헤드 번호가 높은 헤드의 Q 및 K 행렬이 낮은 차원에서 정보를 저장하는 경향이 있다는 것을 나타냅니다. 다시 말해, 헤드 번호가 증가함에 따라 Q 및 K 행렬은 더 적은 차원에서 정보를 저장하려고 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e교차-레이어 분석\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e더 깊은 레이어로 갈수록, Q 및 K 행렬의 초기값이 감소되는 경향을 발견했지만, 여전히 V 및 O 행렬과 비교하면 비교적 높습니다.\u003c/li\u003e\n\u003cli\u003e더 깊은 레이어로 갈수록, 특정 헤드의 Q 및 K 행렬의 상위 10 비율 및 첫 번째/마지막 비율에 대한 하락 트렌드 패턴이 나타납니다. 또한 최하 10 비율의 약간의 상승 트렌드 패턴이 있습니다. 이는 더 깊은 레이어의 Q 및 K 행렬이 낮은 레이어와 비교하여 더 잘 훈련된 것으로 나타납니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e\"레이어 0, 다중 헤드\" 섹션에서 발견한 동일 레이어 내의 헤드 간 패턴은 더 깊은 레이어로 이동할 때 명확하지 않습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e요약\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eK 및 Q 행렬은 V 및 O 행렬과 비교하여 상대적으로 낮은 순위를 가지고 있습니다. 가지치기(pruning) 또는 차원 축소 방법을 수행하려면 K 및 Q 행렬에 더 집중할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e레이어가 깊어질수록 모든 (K, Q, V, O) 행렬이 더 잘 훈련됩니다. 가지치기 또는 차원 축소 방법을 수행하려면 낮은 레이어에 더 집중할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e가지치기 외에도 초기 몇 레이어에서만 전체 미세 조정을 수행하거나 LoRA로도 이를 수행하는 것이 흥미로울 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e마무리 말씀\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective_3.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003e이 시점까지 참석해 주셔서 축하드립니다! 이 기사에서 새로운 것을 배우셨으면 좋겠습니다. 선형 대수의 좋은 오래된 개념들을 적용하여, LLM의 훈련이 얼마나 잘 이루어졌는지 이해하는 것은 정말 흥미롭습니다.\u003c/p\u003e\n\u003cp\u003e이 유형의 콘텐츠를 좋아하신다면, 저의 Medium 계정을 팔로우해주시어 앞으로의 다른 글 알림을 받아보세요.\u003c/p\u003e\n\u003ch1\u003e저자 소개\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e루이스 오웬은 인도네시아 출신의 데이터 과학자 및 AI 연구 엔지니어로, 항상 새로운 지식에 굶주립니다. 그의 경력 여정을 통해 그는 비영리 단체, 전자 상거래, 대화형 AI, OTA, 스마트 시티 및 핀테크 등 다양한 산업 분야에서 일해 왔습니다. 일 안에서 해외에선, 그는 자신의 기사나 멘토링 세션을 통해 데이터 과학 애호가들이 데이터 과학자로 성장할 수 있도록 시간을 보내는 것을 즐깁니다.\u003c/p\u003e\n\u003cp\u003e지금은 루이스가 전 세계적인 CX 자동화 플랫폼 인 Yellow.ai의 NLP 연구 엔지니어로 일하고 있습니다. 루이스의 웹사이트를 방문하여 그에 대해 더 알아보세요! 마지막으로, 궁금한 점이나 이야기할 주제가 있으면 망설이지 마시고 LinkedIn을 통해 루이스에게 연락해보세요.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-UnveilingtheInnerWorkingsofLLMsASingularValuePerspective"},"buildId":"8coAiP0lmiEK5aH6nkQkj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>