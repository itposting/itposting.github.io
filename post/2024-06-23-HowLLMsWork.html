<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>LLMs의 작동 원리 - 대형 언어 모델의 비밀 밝혀보기 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-23-HowLLMsWork" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="LLMs의 작동 원리 - 대형 언어 모델의 비밀 밝혀보기 | itposting" data-gatsby-head="true"/><meta property="og:title" content="LLMs의 작동 원리 - 대형 언어 모델의 비밀 밝혀보기 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-23-HowLLMsWork_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-23-HowLLMsWork" data-gatsby-head="true"/><meta name="twitter:title" content="LLMs의 작동 원리 - 대형 언어 모델의 비밀 밝혀보기 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-23-HowLLMsWork_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-23 19:16" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_buildManifest.js" defer=""></script><script src="/_next/static/CYQjEY3HhSkRTiL0gewc0/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">LLMs의 작동 원리 - 대형 언어 모델의 비밀 밝혀보기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="LLMs의 작동 원리 - 대형 언어 모델의 비밀 밝혀보기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 23, 2024</span><span class="posts_reading_time__f7YPP">2<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-23-HowLLMsWork&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-06-23-HowLLMsWork_0.png" alt="LLMs operation"></p>
<p>LLMs (Large Language Models) operate by predicting the next token based on a sequence of previous tokens. Each generated token is then used as input to generate the next one, enabling the model’s text generation capabilities.</p>
<h2>Step 1: Prompts</h2>
<p>The process starts with receiving a prompt, which is tokenized and converted into embeddings, or vector representations, of the input text. These embeddings, initially random, are learned during model training and represent a non-contextualized vector form of the input token.</p>
<div class="content-ad"></div>
<h2>단계 2: 인코딩</h2>
<p>이 모델은 층별 어텐션 및 피드포워드 계산을 수행하여 어휘 내 각 단어에 숫자(logit)를 할당합니다(디코더 모델인 GPT-X, LLaMA 등) 또는 컨텍스트화된 임베딩을 출력합니다(버트, 로버타, 일렉트라 등과 같은 인코더 모델).</p>
<h2>단계 3: 정규화</h2>
<p>디코더 모델의 경우, 최종 단계는 Softmax 함수를 사용하여 비정규화된 로짓을 정규화된 확률 분포로 변환하는 것을 포함합니다. 이는 생성된 텍스트에서 다음 단어를 결정합니다.</p>
<div class="content-ad"></div>
<h1>추가 세부 정보</h1>
<h2>토큰화</h2>
<p>원시 입력 텍스트는 토큰화를 통해 종종 하위 단위 또는 단어로 분해됩니다. 이 과정을 통해 입력이 모델의 고정 된 어휘와 일치하도록되어 모델에 인식되도록합니다.</p>
<h2>임베딩</h2>
<div class="content-ad"></div>
<p>각 토큰은 임베딩 행렬을 사용하여 고차원 벡터로 매핑됩니다. 이 벡터 표현은 토큰의 의미적 의미를 포착하고 모델의 후속 계층에 입력으로 작용합니다. 이러한 임베딩에는 위치 인코딩이 추가되어 토큰 순서에 대한 정보를 제공하며, 내재된 시퀀스 인식이 부족한 트랜스포머와 같은 모델에 중요합니다.</p>
<h2>트랜스포머 아키텍처</h2>
<p>현대 LLM의 핵심인 트랜스포머 아키텍처는 여러 계층으로 구성됩니다. 각 계층에는 멀티 헤드 셀프 어텐션 메커니즘과 위치별 피드포워드 네트워크가 포함됩니다.</p>
<p><img src="/assets/img/2024-06-23-HowLLMsWork_1.png" alt="트랜스포머 아키텍처"></p>
<div class="content-ad"></div>
<p>셀프 어텐션 메커니즘은 토큰이 다른 토큰과의 관련성을 평가할 수 있게 하여 모델이 입력의 관련 부분에 집중할 수 있게 합니다. 그 결과로 얻는 정보는 각 위치에서 독립적으로 피드포워드 신경망을 통해 처리됩니다.</p>
<p>셀프-어텐션 또는 피드포워드 네트워크 등 각 서브 레이어에는 잔차 연결 후 레이어 정규화가 이어집니다. 이 설정은 활성화를 안정화시키고 학습을 가속화하는 데 도움이 됩니다.</p>
<p>트랜스포머 레이어를 통과한 후 각 토큰의 최종 표현은 로짓 벡터로 변환됩니다. 각 로짓은 모델 어휘 중 단어에 해당하며 해당 단어가 시퀀스에서 다음 단어일 가능성을 나타냅니다.</p>
<p>소프트맥스 함수는 로짓에 적용되어 확률로 변환됩니다. 이 정규화에 의해 확률은 합이 1이 되도록 보장되며 각 확률은 0과 1 사이에 있습니다. 가장 높은 확률을 가진 단어가 시퀀스에서 다음 단어로 선택됩니다.</p>
<div class="content-ad"></div>
<p>이 테이블 태그를 마크다운 형식으로 변경해 주세요.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"LLMs의 작동 원리 - 대형 언어 모델의 비밀 밝혀보기","description":"","date":"2024-06-23 19:16","slug":"2024-06-23-HowLLMsWork","content":"\n\n\n![LLMs operation](/assets/img/2024-06-23-HowLLMsWork_0.png)\n\nLLMs (Large Language Models) operate by predicting the next token based on a sequence of previous tokens. Each generated token is then used as input to generate the next one, enabling the model’s text generation capabilities.\n\n## Step 1: Prompts\n\nThe process starts with receiving a prompt, which is tokenized and converted into embeddings, or vector representations, of the input text. These embeddings, initially random, are learned during model training and represent a non-contextualized vector form of the input token.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 2: 인코딩\n\n이 모델은 층별 어텐션 및 피드포워드 계산을 수행하여 어휘 내 각 단어에 숫자(logit)를 할당합니다(디코더 모델인 GPT-X, LLaMA 등) 또는 컨텍스트화된 임베딩을 출력합니다(버트, 로버타, 일렉트라 등과 같은 인코더 모델).\n\n## 단계 3: 정규화\n\n디코더 모델의 경우, 최종 단계는 Softmax 함수를 사용하여 비정규화된 로짓을 정규화된 확률 분포로 변환하는 것을 포함합니다. 이는 생성된 텍스트에서 다음 단어를 결정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 추가 세부 정보\n\n## 토큰화\n\n원시 입력 텍스트는 토큰화를 통해 종종 하위 단위 또는 단어로 분해됩니다. 이 과정을 통해 입력이 모델의 고정 된 어휘와 일치하도록되어 모델에 인식되도록합니다.\n\n## 임베딩\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 토큰은 임베딩 행렬을 사용하여 고차원 벡터로 매핑됩니다. 이 벡터 표현은 토큰의 의미적 의미를 포착하고 모델의 후속 계층에 입력으로 작용합니다. 이러한 임베딩에는 위치 인코딩이 추가되어 토큰 순서에 대한 정보를 제공하며, 내재된 시퀀스 인식이 부족한 트랜스포머와 같은 모델에 중요합니다.\n\n## 트랜스포머 아키텍처\n\n현대 LLM의 핵심인 트랜스포머 아키텍처는 여러 계층으로 구성됩니다. 각 계층에는 멀티 헤드 셀프 어텐션 메커니즘과 위치별 피드포워드 네트워크가 포함됩니다.\n\n![트랜스포머 아키텍처](/assets/img/2024-06-23-HowLLMsWork_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n셀프 어텐션 메커니즘은 토큰이 다른 토큰과의 관련성을 평가할 수 있게 하여 모델이 입력의 관련 부분에 집중할 수 있게 합니다. 그 결과로 얻는 정보는 각 위치에서 독립적으로 피드포워드 신경망을 통해 처리됩니다.\n\n셀프-어텐션 또는 피드포워드 네트워크 등 각 서브 레이어에는 잔차 연결 후 레이어 정규화가 이어집니다. 이 설정은 활성화를 안정화시키고 학습을 가속화하는 데 도움이 됩니다.\n\n트랜스포머 레이어를 통과한 후 각 토큰의 최종 표현은 로짓 벡터로 변환됩니다. 각 로짓은 모델 어휘 중 단어에 해당하며 해당 단어가 시퀀스에서 다음 단어일 가능성을 나타냅니다.\n\n소프트맥스 함수는 로짓에 적용되어 확률로 변환됩니다. 이 정규화에 의해 확률은 합이 1이 되도록 보장되며 각 확률은 0과 1 사이에 있습니다. 가장 높은 확률을 가진 단어가 시퀀스에서 다음 단어로 선택됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 테이블 태그를 마크다운 형식으로 변경해 주세요.","ogImage":{"url":"/assets/img/2024-06-23-HowLLMsWork_0.png"},"coverImage":"/assets/img/2024-06-23-HowLLMsWork_0.png","tag":["Tech"],"readingTime":2},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-HowLLMsWork_0.png\" alt=\"LLMs operation\"\u003e\u003c/p\u003e\n\u003cp\u003eLLMs (Large Language Models) operate by predicting the next token based on a sequence of previous tokens. Each generated token is then used as input to generate the next one, enabling the model’s text generation capabilities.\u003c/p\u003e\n\u003ch2\u003eStep 1: Prompts\u003c/h2\u003e\n\u003cp\u003eThe process starts with receiving a prompt, which is tokenized and converted into embeddings, or vector representations, of the input text. These embeddings, initially random, are learned during model training and represent a non-contextualized vector form of the input token.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e단계 2: 인코딩\u003c/h2\u003e\n\u003cp\u003e이 모델은 층별 어텐션 및 피드포워드 계산을 수행하여 어휘 내 각 단어에 숫자(logit)를 할당합니다(디코더 모델인 GPT-X, LLaMA 등) 또는 컨텍스트화된 임베딩을 출력합니다(버트, 로버타, 일렉트라 등과 같은 인코더 모델).\u003c/p\u003e\n\u003ch2\u003e단계 3: 정규화\u003c/h2\u003e\n\u003cp\u003e디코더 모델의 경우, 최종 단계는 Softmax 함수를 사용하여 비정규화된 로짓을 정규화된 확률 분포로 변환하는 것을 포함합니다. 이는 생성된 텍스트에서 다음 단어를 결정합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e추가 세부 정보\u003c/h1\u003e\n\u003ch2\u003e토큰화\u003c/h2\u003e\n\u003cp\u003e원시 입력 텍스트는 토큰화를 통해 종종 하위 단위 또는 단어로 분해됩니다. 이 과정을 통해 입력이 모델의 고정 된 어휘와 일치하도록되어 모델에 인식되도록합니다.\u003c/p\u003e\n\u003ch2\u003e임베딩\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e각 토큰은 임베딩 행렬을 사용하여 고차원 벡터로 매핑됩니다. 이 벡터 표현은 토큰의 의미적 의미를 포착하고 모델의 후속 계층에 입력으로 작용합니다. 이러한 임베딩에는 위치 인코딩이 추가되어 토큰 순서에 대한 정보를 제공하며, 내재된 시퀀스 인식이 부족한 트랜스포머와 같은 모델에 중요합니다.\u003c/p\u003e\n\u003ch2\u003e트랜스포머 아키텍처\u003c/h2\u003e\n\u003cp\u003e현대 LLM의 핵심인 트랜스포머 아키텍처는 여러 계층으로 구성됩니다. 각 계층에는 멀티 헤드 셀프 어텐션 메커니즘과 위치별 피드포워드 네트워크가 포함됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-HowLLMsWork_1.png\" alt=\"트랜스포머 아키텍처\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e셀프 어텐션 메커니즘은 토큰이 다른 토큰과의 관련성을 평가할 수 있게 하여 모델이 입력의 관련 부분에 집중할 수 있게 합니다. 그 결과로 얻는 정보는 각 위치에서 독립적으로 피드포워드 신경망을 통해 처리됩니다.\u003c/p\u003e\n\u003cp\u003e셀프-어텐션 또는 피드포워드 네트워크 등 각 서브 레이어에는 잔차 연결 후 레이어 정규화가 이어집니다. 이 설정은 활성화를 안정화시키고 학습을 가속화하는 데 도움이 됩니다.\u003c/p\u003e\n\u003cp\u003e트랜스포머 레이어를 통과한 후 각 토큰의 최종 표현은 로짓 벡터로 변환됩니다. 각 로짓은 모델 어휘 중 단어에 해당하며 해당 단어가 시퀀스에서 다음 단어일 가능성을 나타냅니다.\u003c/p\u003e\n\u003cp\u003e소프트맥스 함수는 로짓에 적용되어 확률로 변환됩니다. 이 정규화에 의해 확률은 합이 1이 되도록 보장되며 각 확률은 0과 1 사이에 있습니다. 가장 높은 확률을 가진 단어가 시퀀스에서 다음 단어로 선택됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 테이블 태그를 마크다운 형식으로 변경해 주세요.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-23-HowLLMsWork"},"buildId":"CYQjEY3HhSkRTiL0gewc0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>