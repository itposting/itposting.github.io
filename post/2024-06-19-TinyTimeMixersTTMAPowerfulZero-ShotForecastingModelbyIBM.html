<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>작은 시간 혼합기 TTM IBM의 강력한 제로샷 예측 모델 | itposting</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://itposting.github.io///post/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="작은 시간 혼합기 TTM IBM의 강력한 제로샷 예측 모델 | itposting" data-gatsby-head="true"/><meta property="og:title" content="작은 시간 혼합기 TTM IBM의 강력한 제로샷 예측 모델 | itposting" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://itposting.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://itposting.github.io///post/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM" data-gatsby-head="true"/><meta name="twitter:title" content="작은 시간 혼합기 TTM IBM의 강력한 제로샷 예측 모델 | itposting" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | itposting" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 03:07" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-23YXDLKDCL"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-23YXDLKDCL');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-d849684d6d83f07a.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_buildManifest.js" defer=""></script><script src="/_next/static/kkckKPyxcjJp_o8wb2unW/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Posting</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">작은 시간 혼합기 TTM IBM의 강력한 제로샷 예측 모델</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="작은 시간 혼합기 TTM IBM의 강력한 제로샷 예측 모델" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Posting</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">9<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_0.png" alt="이미지"></p>
<p>LLM(Large Language Models)에 대한 최신 연구를 따라가보면 주로 두 가지 주요 접근 방식을 볼 수 있어요:</p>
<p>첫째, 연구자들은 가능한 가장 큰 모델을 구축하는 데 주력합니다. 단어 예측을 통한 사전 학습은 성능 향상에 중요한 역할을 합니다(그리고 수백만 달러가 소비되는 곳이기도 합니다!).</p>
<p>둘째, 연구자들은 양자화와 같은 기술을 사용하여 작고 빠른 모델을 만들어냅니다 — 강력한 일반적인 성능을 유지하면서요.</p>
<div class="content-ad"></div>
<p>그러나 일부 작업에서 더 작은 모델이 훨씬 큰 모델보다 더 나은 성과를 내는 흥미로운 일이 발생합니다. 예를 들어, Llama 3-8B는 MMLU 작업에서 더 큰 Llama 2-70B보다 우수한 성과를 냈습니다!</p>
<p>IBM에서 소개한 Tiny Time Mixers (TTM)[1]은 두 번째 접근 방식을 따릅니다. 더 큰 SOTA 모델 — MOIRAI를 포함하여 —을 능가하는 가벼운 모델로, M4 데이터셋에서 우수한 성과를 거둡니다. 게다가, 이는 오픈 소스입니다!</p>
<p>이 기사에서는 다음을 논의합니다:</p>
<ul>
<li>TTM의 아키텍처 및 기능.</li>
<li>TTM을 특별하게 만드는 혁신적인 기능.</li>
<li>다른 모델과의 벤치마킹 결과를 비교한 결과.</li>
</ul>
<div class="content-ad"></div>
<p>시작해요!</p>
<h1>Enter Tiny Time Mixer (TTM)</h1>
<p>TTM의 주요 특징은 다음과 같습니다:</p>
<ul>
<li>Non-Transformer Architecture: TTM은 Attention 메커니즘을 사용하지 않기 때문에 매우 빠릅니다. 완전 연결된 NN 계층만 사용합니다.</li>
<li>TSMixer Foundation: TTM은 아키텍처에서 TSMixer[2] (IBM의 혁신적인 시계열 모델)을 활용합니다.</li>
<li>다양한 입력: 다변량 예측이 가능한 TTM은 추가 채널, 외부 변수 및 알려진 미래 입력을 수용하여 예측 다양성을 향상시킵니다.</li>
<li>빠르고 강력함: TTM은 Monash 데이터 세트의 244백만개 샘플로 사전 훈련되었으며, 6대의 A100 GPU를 사용하여 8시간 이내에 훈련되었습니다.</li>
<li>우수한 제로샷 예측: TTM은 사전 훈련되어 있으며, 미처 본 적 없는 데이터에 대한 우수한 제로샷 예측을 수행하여 큰 SOTA 모델을 능가합니다.</li>
</ul>
<div class="content-ad"></div>
<p>중요한 사항:</p>
<h1>TTM 혁신</h1>
<p>TTM은 여러 혁신적인 기능을 소개합니다:</p>
<ul>
<li>다중 수준 모델링: TTM은 먼저 채널 독립적 방식(일변량 시퀀스)으로 사전 훈련을 받은 후, 세밀 조정 중에 여러 변수 종속성을 학습하기 위해 교차 채널 혼합을 사용합니다.</li>
<li>적응형 패치 적용: 단일 패치 길이 대신 TTM은 서로 다른 레이어 간에 여러 패치 길이를 학습합니다. 각 시계열이 특정 패치 길이에서 최적으로 작동하기 때문에 적응형 패치는 모델이 다양한 데이터에 대해 더 잘 일반화되도록 도와줍니다.</li>
<li>해상도 접두사 튜닝: 다른 주파수(예: 주간, 일별 데이터)는 전통적인 시계열 모델에 어려운 부분입니다. TTM은 시계열 주파수를 인코딩하기 위한 추가 임베딩 레이어를 사용하여 모델이 신호의 주파수에 따라 정확하게 예측을 조건부로 설정할 수 있도록 합니다.</li>
</ul>
<div class="content-ad"></div>
<h1>Tiny Time Mixers — 아키텍처</h1>
<p>TSMixer은 TTM의 전신입니다. TSMixer는 견고한 모델이지만, 기본 모델로 사용하거나 외부 변수를 처리하는 데 사용할 수는 없습니다.</p>
<p>TTM은 TSMixer를 구성 요소로 사용하여 새로운 기능을 도입함으로써, 저자들이 보지 못한 데이터에 대해 일반화된 비-트랜스포머 모델을 만들었습니다.</p>
<p>TTM의 아키텍처는 그림 1에 나와 있습니다. 우리는 두 단계, 사전 훈련(왼쪽)과 파인튜닝(오른쪽)에 대해 설명할 것입니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_1.png" alt="image"></p>
<p><strong>의미론적 지식</strong>: sl=context_size, fl=forecasting_length, c = 입력 기능의 채널 수, c’= 예측 채널의 수.</p>
<h2>사전 훈련</h2>
<ul>
<li>사전 훈련 중에는 모델이 단변량 시계열로만 학습됩니다.</li>
<li>먼저 개별 시계열을 정규화합니다. 마지막 출력은 역정규화됩니다 (표준적인 방법).</li>
<li>패칭은 시계열에서 널리 성공한 기술이며 여기서도 사용됩니다. 단변량 시퀀스를 크기가 pl인 n 패치로 나눕니다.</li>
<li>TTM 백본 모듈은 적응형 패칭을 적용하고 패치를 크기 p에서 hf로 사상합니다. TTM 백본은 TTM의 핵심이며 나중에 자세히 설명하겠습니다.</li>
<li>TTM 디코더는 TTM 백본과 동일한 아키텍처를 갖고 있지만 훨씬 작아서 매개변수가 80% 적습니다.</li>
<li>예측 선형 헤드에는 1개의 완전 연결 계층이 있으며 최종 예측을 생성합니다 (그런 다음 역정규화됨).</li>
<li>MSE 손실은 예측 기간 fl 동안 계산됩니다.</li>
</ul>
<div class="content-ad"></div>
<h2>Feat-Tuning</h2>
<ul>
<li>여기서는 TTM 백본이 동결되어 있고 TTM 디코더 및 Forecast 선형 헤드의 가중치만 업데이트됩니다.</li>
<li>우리는 소수 데이터만으로 학습하는 후속 예측(few-shot forecasting) 또는 전체 데이터셋을 사용하는 후속 예측(full-shot forecasting)을 수행할 수 있습니다.</li>
<li>Feat-Tuning 단계에서는 다변량 데이터셋을 사용할 수 있습니다. 이 경우 TTM 디코더에서 채널 혼합이 활성화됩니다.</li>
<li>선택적으로, 미래의 알려진 변수를 모델링하기 위해 외생 혼합 블록(그림 1에 나와 있음)을 활성화할 수도 있습니다.</li>
</ul>
<h1>TTM 백본</h1>
<p>TTM의 핵심 구성 요소는 TTM 백본입니다. 이는 Resolution Prefix Tuning과 Adaptive Patching을 가능하게 합니다.</p>
<div class="content-ad"></div>
<p>이 컴포넌트를 자세히 살펴보자면 그 기능을 이해할 수 있어요 (그림 2에 표시됨):</p>
<p><img src="/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_2.png" alt="이미지"></p>
<ul>
<li>임베딩 레이어는 크기 pl에서 패치를 투영하여 크기 hf의 입력 임베딩을 만든답니다.</li>
<li>Resolution Prefix Tuning 모듈은 시간-주파수/해상도를 나타내는 hf 크기의 임베딩을 만들고 이를 입력 임베딩에 연결합니다 (그림 2의 n=n+1 연산을 주목해주세요).</li>
<li>TTM 블록은 3개의 하위 모듈을 포함합니다: 패치 분할 모듈, 베니라 TSMixer 블록 및 패치 병합 블록:</li>
<li>패치 분할 모듈은 패치 수를 K만큼 증가시키고 패치 길이를 다시 K만큼 감소시킵니다. 예를 들어, 첫 번째 수준에서 크기 [c,n, hf]의 입력은 [c, 4*n, hf//4]로 변화합니다.</li>
<li>TSMixer 블록이 변환된 입력에 적용되며 패치 병합 블록이 [c, 4*n, hf//4] 입력을 다시 [c,n, hf]로 변형합니다.</li>
</ul>
<h1>외부 믹서</h1>
<div class="content-ad"></div>
<p>미래의 알려진 변수가 있는 경우, Exogenous Mixer를 활성화할 수 있습니다. 이 모듈은 Figure 3에 표시되어 있으며, TTM 아키텍처에서의 위치는 Figure 1에 표시되어 있습니다:</p>
<p><img src="/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_3.png" alt="Exogenous Mixer"></p>
<p>Exogenous Mixer 블록은 간단합니다: 시계열의 미래 값(y3와 y4; Figure 3, 녹색)이 알려진 경우, 이를 사용하여 대상 변수(y1과 y2; Figure 4, 보라색)의 예측을 안내합니다.</p>
<h1>TTM 교육 세부 정보 및 데이터세트</h1>
<div class="content-ad"></div>
<p>저자들은 다양한 문맥과 예측 길이에 대해 5가지 TTM 버전을 만들었습니다. 이는 (512,96), (512,192), (512, 336), (512,720), (96,24) 입니다.</p>
<p>교육에 관해서, 저자들은 모델 사전 훈련을 위해 Monash 데이터베이스의 하위 집합(244k 샘플)을 사용했고, 파인튜닝 성능을 평가하기 위해 Informer 데이터셋을 사용했습니다. 또한, 저자들은 외부 혼합기 블록의 효과를 평가하고 알려진 미래 변수를 추가함으로써 성능이 얼마나 향상되는지 조사하기 위해 다른 데이터셋을 사용했습니다.</p>
<p>이러한 데이터셋에 대해 더 자세한 내용은 원본 논문에서 확인할 수 있습니다. 아래는 (512,96) 변형을 위한 교육 하이퍼파라미터입니다:</p>
<ul>
<li>pl(패치 길이) = 64</li>
<li>백본 수준 수 = 6</li>
<li>각 수준 당 TTM 블록 수 = 2</li>
<li>배치 크기 = 3천</li>
<li>에폭 = 20</li>
</ul>
<div class="content-ad"></div>
<p>학습 및 파인 튜닝 구성에 대한 자세한 내용은 원 논문을 참조해 주세요.</p>
<h1>평가 벤치마크</h1>
<p>TTM 대 최신 기법 모델</p>
<p>그 다음, 저자들은 Zero-shot 및 5% Few-shot 버전의 TTM을 다른 최신 기법 모델과 비교했습니다. 이때 사용된 평가 메트릭은 MSE였습니다. 결과는 다음과 같은 표 1에서 확인할 수 있습니다:</p>
<div class="content-ad"></div>
<p>아래는 너무 인상적입니다:</p>
<p>평균적으로, Few-shot TTM이 모든 다른 모델을 능가했습니다. 심지어 Zero-shot TTM이 일부 모델을 능가할 수 있었습니다! 기억하세요, Zero-shot TTM은 이러한 데이터에 대해 학습을 받지 않고 예측을 생성합니다.</p>
<p>또한 TTM은 작년에 소개된 새로운 기반 시계열 모델인 GPT4TS를 앞서 나갔습니다.</p>
<div class="content-ad"></div>
<p>TMT 이외에도 다음으로 높은 순위의 모델은 GPT4TS, PatchTST 및 TSMixer입니다. 모두 패치(patching)를 활용합니다. 최근 시계열 예측 연구에서 패치(patching)가 매우 유익한 기술임이 입증되었습니다.</p>
<p>TTM 대 foundation 모델</p>
<p>저자들은 TTM을 독립적으로 평가하며 특히 GPT4TS와 LLMTime과 비교합니다.</p>
<ul>
<li>LLMTime은 GPT-3과 LLaMa-2를 사용하여 시계열 예측을 위해 특정 수정을 가한 모델입니다.</li>
<li>GPT4TS는 다양한 작업(예측, 분류 등)을 위해 범용 시계열 모델이며, 기본 모델로 GPT-2를 사용합니다.</li>
</ul>
<div class="content-ad"></div>
<p>비교 결과는 표시됩니다. Table 2 (LLMTime)와 Table 3 (GPT4TS):</p>
<p><img src="/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_5.png" alt="LLMTime"></p>
<p><img src="/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_6.png" alt="GPT4TS"></p>
<p>LLMTime은 제로샷 예측 시나리오에서 평가되었고, GPT4TS는 퓨샷 예측기로 동작했습니다.</p>
<div class="content-ad"></div>
<ul>
<li>두 가지 비교에서 TTM이 명확한 승자입니다.</li>
<li>게다가 TTM은 훨씬 빠르며 리소스를 상당히 적게 필요로 합니다. 이는 TTM이 GPT4TS와 같은 무거운 트랜스포머 계산을 사용하지 않기 때문에 예상된 결과입니다.</li>
</ul>
<h2>외생 변수의 효과성</h2>
<p>현대 실제 세계 데이터셋은 가능한 경우 외생 변수를 사용하므로 예측 애플리케이션에서 이를 활용하는 것이 합리적입니다.</p>
<p>TTM의 저자들은 이와 같은 변수를 사용함으로써 TTM이 어떻게 향상되는지 조사했습니다 (해당하는 경우). 구체적으로 제로샷 TTM, 일반 TTM 및 외생 변수를 사용하는 채널 혼합 (TTM-CM)을 비교했습니다.</p>
<div class="content-ad"></div>
<p>그들은 TSMixer와 그 채널 혼합 변형을 평가했습니다. 결과는 다음과 같이 Table 4에 표시되어 있습니다:</p>
<p><img src="/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_7.png" alt="Table 4"></p>
<p>다시 한 번, 결과는 매우 흥미로워요: 먼저, TTM-CM이 1위를 차지하여 외생 변수가 모델에 도움이 되는 것을 의미합니다.</p>
<p>채널 혼합 속성을 사용하는 TSMixer 변형은 2위를 차지했습니다. 또한, 제로-샷 TTM이 최악의 성능을 보입니다. 보조 변수가 있는 경우 모델 성능을 향상시키는 데 사용되어야 함이 명백합니다.</p>
<div class="content-ad"></div>
<h1>Tiny Time Mixers 실무 활용</h1>
<p>모델 버전 512-96과 1024-96의 가중치를 HuggingFace에서 다운로드하여 다음과 같이 세밀 조정할 수 있습니다:</p>
<pre><code class="hljs language-js">!git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/IBM/tsfm.git</span>
!pip install transformers
!pip install datasets

<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> matplotlib.<span class="hljs-property">pyplot</span> <span class="hljs-keyword">as</span> plt

<span class="hljs-keyword">from</span> tsfm_public.<span class="hljs-property">models</span>.<span class="hljs-property">tinytimemixer</span>.<span class="hljs-property">utils</span> <span class="hljs-keyword">import</span> (
    count_parameters,
    plot_preds,
)

<span class="hljs-keyword">from</span> tsfm_public.<span class="hljs-property">models</span>.<span class="hljs-property">tinytimemixer</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">TinyTimeMixerForPrediction</span>
<span class="hljs-keyword">from</span> tsfm_public.<span class="hljs-property">toolkit</span>.<span class="hljs-property">callbacks</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">TrackingCallback</span>

zeroshot_model = <span class="hljs-title class_">TinyTimeMixerForPrediction</span>.<span class="hljs-title function_">from_pretrained</span>(<span class="hljs-string">"ibm/TTM"</span>, revision=<span class="hljs-string">'main'</span>)
finetune_forecast_model = <span class="hljs-title class_">TinyTimeMixerForPrediction</span>.<span class="hljs-title function_">from_pretrained</span>(<span class="hljs-string">"ibm/TTM"</span>, revision=<span class="hljs-string">'main'</span>, head_dropout=<span class="hljs-number">0.0</span>,dropout=<span class="hljs-number">0.0</span>,loss=<span class="hljs-string">"mse"</span>)
</code></pre>
<p>따라서 transformers 라이브러리의 익숙한 Trainer 모듈을 사용하여 TTM을 세밀 조정할 수 있습니다:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">finetune_forecast_trainer = <span class="hljs-title class_">Trainer</span>(
model=finetune_forecast_model,
args=finetune_forecast_args,
train_dataset=train_dataset,
eval_dataset=valid_dataset,
callbacks=[early_stopping_callback, tracking_callback],
optimizers=(optimizer, scheduler))

# <span class="hljs-title class_">Fine</span> tune
finetune_forecast_trainer.<span class="hljs-title function_">train</span>()
</code></pre>
<img src="/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_8.png">
<p>predictions_test = finetune_forecast_trainer.predict(test_dataset)</p>
<p>이후에는 사적 데이터셋을 통해 예측을 받은 후 결과를 플롯합니다:</p>
<div class="content-ad"></div>
<p>아래는 테이블 태그를 마크다운 형식으로 변경하도록 했습니다.</p>
<img src="/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_9.png">
<h1>마무리 말씀</h1>
<p>Tiny Time Mixer (TTM)은 다른 접근 방식을 따른 혁신적인 모델로서, 더 작지만 효율적인 모델들을 위한 길을 열어두었습니다.</p>
<p>특히, TTM은 어텐션을 사용하지 않았고 여전히 강력한 시계열(Time Series) 기반 모델을 구축할 수 있다는 것을 입증했습니다.</p>
<div class="content-ad"></div>
<p>최초로 MLP만 사용한 메타러닝 기능을 갖춘 시계열 모델은 N-BEATS와 N-HITS였어요. 이 트렌드가 어떻게 이어지는지 한번 살펴봐요.</p>
<p>최근에는 NLP 모델에서도 이러한 트렌드를 관측하고 있어요. 우리는 Mamba(State Space) xLSTM(기반 RNN)과 Hyena(CNN 기반)을 보았는데, 이들은 언어 모델이지만 트랜스포머는 아니며 다양한 벤치마크에서 인상적인 결과를 얻고 있어요.</p>
<p>시계열 모델에 대한 이런 접근 방식이 어떻게 전개될지도 한번 살펴봅시다. 결국, 시계열에 대한 기초 모델 연구는 아직 새로운 상황이에요!</p>
<h1>읽어주셔서 감사합니다!</h1>
<div class="content-ad"></div>
<ul>
<li>제 LinkedIn 팔로우해 주세요!</li>
<li>제 뉴스레터, AI Horizon Forecast를 구독해 주세요!</li>
</ul>
<h2>참고 자료</h2>
<p>[1] Ekambaram 등, Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series (2024년 4월)</p>
<p>[2] Ekambaram 등, TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting (2023년 6월)</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"작은 시간 혼합기 TTM IBM의 강력한 제로샷 예측 모델","description":"","date":"2024-06-19 03:07","slug":"2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM","content":"\n\n![이미지](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_0.png)\n\nLLM(Large Language Models)에 대한 최신 연구를 따라가보면 주로 두 가지 주요 접근 방식을 볼 수 있어요:\n\n첫째, 연구자들은 가능한 가장 큰 모델을 구축하는 데 주력합니다. 단어 예측을 통한 사전 학습은 성능 향상에 중요한 역할을 합니다(그리고 수백만 달러가 소비되는 곳이기도 합니다!).\n\n둘째, 연구자들은 양자화와 같은 기술을 사용하여 작고 빠른 모델을 만들어냅니다 — 강력한 일반적인 성능을 유지하면서요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 일부 작업에서 더 작은 모델이 훨씬 큰 모델보다 더 나은 성과를 내는 흥미로운 일이 발생합니다. 예를 들어, Llama 3-8B는 MMLU 작업에서 더 큰 Llama 2-70B보다 우수한 성과를 냈습니다!\n\nIBM에서 소개한 Tiny Time Mixers (TTM)[1]은 두 번째 접근 방식을 따릅니다. 더 큰 SOTA 모델 — MOIRAI를 포함하여 —을 능가하는 가벼운 모델로, M4 데이터셋에서 우수한 성과를 거둡니다. 게다가, 이는 오픈 소스입니다!\n\n이 기사에서는 다음을 논의합니다:\n\n- TTM의 아키텍처 및 기능.\n- TTM을 특별하게 만드는 혁신적인 기능.\n- 다른 모델과의 벤치마킹 결과를 비교한 결과.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시작해요!\n\n# Enter Tiny Time Mixer (TTM)\n\nTTM의 주요 특징은 다음과 같습니다:\n\n- Non-Transformer Architecture: TTM은 Attention 메커니즘을 사용하지 않기 때문에 매우 빠릅니다. 완전 연결된 NN 계층만 사용합니다.\n- TSMixer Foundation: TTM은 아키텍처에서 TSMixer[2] (IBM의 혁신적인 시계열 모델)을 활용합니다.\n- 다양한 입력: 다변량 예측이 가능한 TTM은 추가 채널, 외부 변수 및 알려진 미래 입력을 수용하여 예측 다양성을 향상시킵니다.\n- 빠르고 강력함: TTM은 Monash 데이터 세트의 244백만개 샘플로 사전 훈련되었으며, 6대의 A100 GPU를 사용하여 8시간 이내에 훈련되었습니다.\n- 우수한 제로샷 예측: TTM은 사전 훈련되어 있으며, 미처 본 적 없는 데이터에 대한 우수한 제로샷 예측을 수행하여 큰 SOTA 모델을 능가합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n중요한 사항:\n\n# TTM 혁신\n\nTTM은 여러 혁신적인 기능을 소개합니다:\n\n- 다중 수준 모델링: TTM은 먼저 채널 독립적 방식(일변량 시퀀스)으로 사전 훈련을 받은 후, 세밀 조정 중에 여러 변수 종속성을 학습하기 위해 교차 채널 혼합을 사용합니다.\n- 적응형 패치 적용: 단일 패치 길이 대신 TTM은 서로 다른 레이어 간에 여러 패치 길이를 학습합니다. 각 시계열이 특정 패치 길이에서 최적으로 작동하기 때문에 적응형 패치는 모델이 다양한 데이터에 대해 더 잘 일반화되도록 도와줍니다.\n- 해상도 접두사 튜닝: 다른 주파수(예: 주간, 일별 데이터)는 전통적인 시계열 모델에 어려운 부분입니다. TTM은 시계열 주파수를 인코딩하기 위한 추가 임베딩 레이어를 사용하여 모델이 신호의 주파수에 따라 정확하게 예측을 조건부로 설정할 수 있도록 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Tiny Time Mixers — 아키텍처\n\nTSMixer은 TTM의 전신입니다. TSMixer는 견고한 모델이지만, 기본 모델로 사용하거나 외부 변수를 처리하는 데 사용할 수는 없습니다.\n\nTTM은 TSMixer를 구성 요소로 사용하여 새로운 기능을 도입함으로써, 저자들이 보지 못한 데이터에 대해 일반화된 비-트랜스포머 모델을 만들었습니다.\n\nTTM의 아키텍처는 그림 1에 나와 있습니다. 우리는 두 단계, 사전 훈련(왼쪽)과 파인튜닝(오른쪽)에 대해 설명할 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_1.png)\n\n**의미론적 지식**: sl=context_size, fl=forecasting_length, c = 입력 기능의 채널 수, c’= 예측 채널의 수.\n\n## 사전 훈련\n\n- 사전 훈련 중에는 모델이 단변량 시계열로만 학습됩니다.\n- 먼저 개별 시계열을 정규화합니다. 마지막 출력은 역정규화됩니다 (표준적인 방법).\n- 패칭은 시계열에서 널리 성공한 기술이며 여기서도 사용됩니다. 단변량 시퀀스를 크기가 pl인 n 패치로 나눕니다.\n- TTM 백본 모듈은 적응형 패칭을 적용하고 패치를 크기 p에서 hf로 사상합니다. TTM 백본은 TTM의 핵심이며 나중에 자세히 설명하겠습니다.\n- TTM 디코더는 TTM 백본과 동일한 아키텍처를 갖고 있지만 훨씬 작아서 매개변수가 80% 적습니다.\n- 예측 선형 헤드에는 1개의 완전 연결 계층이 있으며 최종 예측을 생성합니다 (그런 다음 역정규화됨).\n- MSE 손실은 예측 기간 fl 동안 계산됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Feat-Tuning\n\n- 여기서는 TTM 백본이 동결되어 있고 TTM 디코더 및 Forecast 선형 헤드의 가중치만 업데이트됩니다.\n- 우리는 소수 데이터만으로 학습하는 후속 예측(few-shot forecasting) 또는 전체 데이터셋을 사용하는 후속 예측(full-shot forecasting)을 수행할 수 있습니다.\n- Feat-Tuning 단계에서는 다변량 데이터셋을 사용할 수 있습니다. 이 경우 TTM 디코더에서 채널 혼합이 활성화됩니다.\n- 선택적으로, 미래의 알려진 변수를 모델링하기 위해 외생 혼합 블록(그림 1에 나와 있음)을 활성화할 수도 있습니다.\n\n# TTM 백본\n\nTTM의 핵심 구성 요소는 TTM 백본입니다. 이는 Resolution Prefix Tuning과 Adaptive Patching을 가능하게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 컴포넌트를 자세히 살펴보자면 그 기능을 이해할 수 있어요 (그림 2에 표시됨):\n\n![이미지](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_2.png)\n\n- 임베딩 레이어는 크기 pl에서 패치를 투영하여 크기 hf의 입력 임베딩을 만든답니다.\n- Resolution Prefix Tuning 모듈은 시간-주파수/해상도를 나타내는 hf 크기의 임베딩을 만들고 이를 입력 임베딩에 연결합니다 (그림 2의 n=n+1 연산을 주목해주세요).\n- TTM 블록은 3개의 하위 모듈을 포함합니다: 패치 분할 모듈, 베니라 TSMixer 블록 및 패치 병합 블록:\n- 패치 분할 모듈은 패치 수를 K만큼 증가시키고 패치 길이를 다시 K만큼 감소시킵니다. 예를 들어, 첫 번째 수준에서 크기 [c,n, hf]의 입력은 [c, 4*n, hf//4]로 변화합니다.\n- TSMixer 블록이 변환된 입력에 적용되며 패치 병합 블록이 [c, 4*n, hf//4] 입력을 다시 [c,n, hf]로 변형합니다.\n\n# 외부 믹서\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n미래의 알려진 변수가 있는 경우, Exogenous Mixer를 활성화할 수 있습니다. 이 모듈은 Figure 3에 표시되어 있으며, TTM 아키텍처에서의 위치는 Figure 1에 표시되어 있습니다:\n\n![Exogenous Mixer](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_3.png)\n\nExogenous Mixer 블록은 간단합니다: 시계열의 미래 값(y3와 y4; Figure 3, 녹색)이 알려진 경우, 이를 사용하여 대상 변수(y1과 y2; Figure 4, 보라색)의 예측을 안내합니다.\n\n# TTM 교육 세부 정보 및 데이터세트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저자들은 다양한 문맥과 예측 길이에 대해 5가지 TTM 버전을 만들었습니다. 이는 (512,96), (512,192), (512, 336), (512,720), (96,24) 입니다.\n\n교육에 관해서, 저자들은 모델 사전 훈련을 위해 Monash 데이터베이스의 하위 집합(244k 샘플)을 사용했고, 파인튜닝 성능을 평가하기 위해 Informer 데이터셋을 사용했습니다. 또한, 저자들은 외부 혼합기 블록의 효과를 평가하고 알려진 미래 변수를 추가함으로써 성능이 얼마나 향상되는지 조사하기 위해 다른 데이터셋을 사용했습니다.\n\n이러한 데이터셋에 대해 더 자세한 내용은 원본 논문에서 확인할 수 있습니다. 아래는 (512,96) 변형을 위한 교육 하이퍼파라미터입니다:\n\n- pl(패치 길이) = 64\n- 백본 수준 수 = 6\n- 각 수준 당 TTM 블록 수 = 2\n- 배치 크기 = 3천\n- 에폭 = 20\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n학습 및 파인 튜닝 구성에 대한 자세한 내용은 원 논문을 참조해 주세요.\n\n# 평가 벤치마크\n\nTTM 대 최신 기법 모델\n\n그 다음, 저자들은 Zero-shot 및 5% Few-shot 버전의 TTM을 다른 최신 기법 모델과 비교했습니다. 이때 사용된 평가 메트릭은 MSE였습니다. 결과는 다음과 같은 표 1에서 확인할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 너무 인상적입니다:\n\n평균적으로, Few-shot TTM이 모든 다른 모델을 능가했습니다. 심지어 Zero-shot TTM이 일부 모델을 능가할 수 있었습니다! 기억하세요, Zero-shot TTM은 이러한 데이터에 대해 학습을 받지 않고 예측을 생성합니다.\n\n또한 TTM은 작년에 소개된 새로운 기반 시계열 모델인 GPT4TS를 앞서 나갔습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTMT 이외에도 다음으로 높은 순위의 모델은 GPT4TS, PatchTST 및 TSMixer입니다. 모두 패치(patching)를 활용합니다. 최근 시계열 예측 연구에서 패치(patching)가 매우 유익한 기술임이 입증되었습니다.\n\nTTM 대 foundation 모델\n\n저자들은 TTM을 독립적으로 평가하며 특히 GPT4TS와 LLMTime과 비교합니다.\n- LLMTime은 GPT-3과 LLaMa-2를 사용하여 시계열 예측을 위해 특정 수정을 가한 모델입니다.\n- GPT4TS는 다양한 작업(예측, 분류 등)을 위해 범용 시계열 모델이며, 기본 모델로 GPT-2를 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비교 결과는 표시됩니다. Table 2 (LLMTime)와 Table 3 (GPT4TS):\n\n![LLMTime](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_5.png)\n\n![GPT4TS](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_6.png)\n\nLLMTime은 제로샷 예측 시나리오에서 평가되었고, GPT4TS는 퓨샷 예측기로 동작했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 두 가지 비교에서 TTM이 명확한 승자입니다.\n- 게다가 TTM은 훨씬 빠르며 리소스를 상당히 적게 필요로 합니다. 이는 TTM이 GPT4TS와 같은 무거운 트랜스포머 계산을 사용하지 않기 때문에 예상된 결과입니다.\n\n## 외생 변수의 효과성\n\n현대 실제 세계 데이터셋은 가능한 경우 외생 변수를 사용하므로 예측 애플리케이션에서 이를 활용하는 것이 합리적입니다.\n\nTTM의 저자들은 이와 같은 변수를 사용함으로써 TTM이 어떻게 향상되는지 조사했습니다 (해당하는 경우). 구체적으로 제로샷 TTM, 일반 TTM 및 외생 변수를 사용하는 채널 혼합 (TTM-CM)을 비교했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그들은 TSMixer와 그 채널 혼합 변형을 평가했습니다. 결과는 다음과 같이 Table 4에 표시되어 있습니다:\n\n![Table 4](/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_7.png)\n\n다시 한 번, 결과는 매우 흥미로워요: 먼저, TTM-CM이 1위를 차지하여 외생 변수가 모델에 도움이 되는 것을 의미합니다.\n\n채널 혼합 속성을 사용하는 TSMixer 변형은 2위를 차지했습니다. 또한, 제로-샷 TTM이 최악의 성능을 보입니다. 보조 변수가 있는 경우 모델 성능을 향상시키는 데 사용되어야 함이 명백합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Tiny Time Mixers 실무 활용\n\n모델 버전 512-96과 1024-96의 가중치를 HuggingFace에서 다운로드하여 다음과 같이 세밀 조정할 수 있습니다:\n\n```js\n!git clone https://github.com/IBM/tsfm.git\n!pip install transformers\n!pip install datasets\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tsfm_public.models.tinytimemixer.utils import (\n    count_parameters,\n    plot_preds,\n)\n\nfrom tsfm_public.models.tinytimemixer import TinyTimeMixerForPrediction\nfrom tsfm_public.toolkit.callbacks import TrackingCallback\n\nzeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\"ibm/TTM\", revision='main')\nfinetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\"ibm/TTM\", revision='main', head_dropout=0.0,dropout=0.0,loss=\"mse\")\n```\n\n따라서 transformers 라이브러리의 익숙한 Trainer 모듈을 사용하여 TTM을 세밀 조정할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfinetune_forecast_trainer = Trainer(\nmodel=finetune_forecast_model,\nargs=finetune_forecast_args,\ntrain_dataset=train_dataset,\neval_dataset=valid_dataset,\ncallbacks=[early_stopping_callback, tracking_callback],\noptimizers=(optimizer, scheduler))\n\n# Fine tune\nfinetune_forecast_trainer.train()\n```\n\n\u003cimg src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_8.png\" /\u003e\n\npredictions_test = finetune_forecast_trainer.predict(test_dataset)\n\n이후에는 사적 데이터셋을 통해 예측을 받은 후 결과를 플롯합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 테이블 태그를 마크다운 형식으로 변경하도록 했습니다.\n\n\n\u003cimg src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_9.png\" /\u003e\n\n# 마무리 말씀\n\nTiny Time Mixer (TTM)은 다른 접근 방식을 따른 혁신적인 모델로서, 더 작지만 효율적인 모델들을 위한 길을 열어두었습니다.\n\n특히, TTM은 어텐션을 사용하지 않았고 여전히 강력한 시계열(Time Series) 기반 모델을 구축할 수 있다는 것을 입증했습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최초로 MLP만 사용한 메타러닝 기능을 갖춘 시계열 모델은 N-BEATS와 N-HITS였어요. 이 트렌드가 어떻게 이어지는지 한번 살펴봐요.\n\n최근에는 NLP 모델에서도 이러한 트렌드를 관측하고 있어요. 우리는 Mamba(State Space) xLSTM(기반 RNN)과 Hyena(CNN 기반)을 보았는데, 이들은 언어 모델이지만 트랜스포머는 아니며 다양한 벤치마크에서 인상적인 결과를 얻고 있어요.\n\n시계열 모델에 대한 이런 접근 방식이 어떻게 전개될지도 한번 살펴봅시다. 결국, 시계열에 대한 기초 모델 연구는 아직 새로운 상황이에요!\n\n# 읽어주셔서 감사합니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 제 LinkedIn 팔로우해 주세요!\n- 제 뉴스레터, AI Horizon Forecast를 구독해 주세요!\n\n## 참고 자료\n\n[1] Ekambaram 등, Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series (2024년 4월)\n\n[2] Ekambaram 등, TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting (2023년 6월)","ogImage":{"url":"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_0.png"},"coverImage":"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_0.png","tag":["Tech"],"readingTime":9},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eLLM(Large Language Models)에 대한 최신 연구를 따라가보면 주로 두 가지 주요 접근 방식을 볼 수 있어요:\u003c/p\u003e\n\u003cp\u003e첫째, 연구자들은 가능한 가장 큰 모델을 구축하는 데 주력합니다. 단어 예측을 통한 사전 학습은 성능 향상에 중요한 역할을 합니다(그리고 수백만 달러가 소비되는 곳이기도 합니다!).\u003c/p\u003e\n\u003cp\u003e둘째, 연구자들은 양자화와 같은 기술을 사용하여 작고 빠른 모델을 만들어냅니다 — 강력한 일반적인 성능을 유지하면서요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그러나 일부 작업에서 더 작은 모델이 훨씬 큰 모델보다 더 나은 성과를 내는 흥미로운 일이 발생합니다. 예를 들어, Llama 3-8B는 MMLU 작업에서 더 큰 Llama 2-70B보다 우수한 성과를 냈습니다!\u003c/p\u003e\n\u003cp\u003eIBM에서 소개한 Tiny Time Mixers (TTM)[1]은 두 번째 접근 방식을 따릅니다. 더 큰 SOTA 모델 — MOIRAI를 포함하여 —을 능가하는 가벼운 모델로, M4 데이터셋에서 우수한 성과를 거둡니다. 게다가, 이는 오픈 소스입니다!\u003c/p\u003e\n\u003cp\u003e이 기사에서는 다음을 논의합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTTM의 아키텍처 및 기능.\u003c/li\u003e\n\u003cli\u003eTTM을 특별하게 만드는 혁신적인 기능.\u003c/li\u003e\n\u003cli\u003e다른 모델과의 벤치마킹 결과를 비교한 결과.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e시작해요!\u003c/p\u003e\n\u003ch1\u003eEnter Tiny Time Mixer (TTM)\u003c/h1\u003e\n\u003cp\u003eTTM의 주요 특징은 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNon-Transformer Architecture: TTM은 Attention 메커니즘을 사용하지 않기 때문에 매우 빠릅니다. 완전 연결된 NN 계층만 사용합니다.\u003c/li\u003e\n\u003cli\u003eTSMixer Foundation: TTM은 아키텍처에서 TSMixer[2] (IBM의 혁신적인 시계열 모델)을 활용합니다.\u003c/li\u003e\n\u003cli\u003e다양한 입력: 다변량 예측이 가능한 TTM은 추가 채널, 외부 변수 및 알려진 미래 입력을 수용하여 예측 다양성을 향상시킵니다.\u003c/li\u003e\n\u003cli\u003e빠르고 강력함: TTM은 Monash 데이터 세트의 244백만개 샘플로 사전 훈련되었으며, 6대의 A100 GPU를 사용하여 8시간 이내에 훈련되었습니다.\u003c/li\u003e\n\u003cli\u003e우수한 제로샷 예측: TTM은 사전 훈련되어 있으며, 미처 본 적 없는 데이터에 대한 우수한 제로샷 예측을 수행하여 큰 SOTA 모델을 능가합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e중요한 사항:\u003c/p\u003e\n\u003ch1\u003eTTM 혁신\u003c/h1\u003e\n\u003cp\u003eTTM은 여러 혁신적인 기능을 소개합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e다중 수준 모델링: TTM은 먼저 채널 독립적 방식(일변량 시퀀스)으로 사전 훈련을 받은 후, 세밀 조정 중에 여러 변수 종속성을 학습하기 위해 교차 채널 혼합을 사용합니다.\u003c/li\u003e\n\u003cli\u003e적응형 패치 적용: 단일 패치 길이 대신 TTM은 서로 다른 레이어 간에 여러 패치 길이를 학습합니다. 각 시계열이 특정 패치 길이에서 최적으로 작동하기 때문에 적응형 패치는 모델이 다양한 데이터에 대해 더 잘 일반화되도록 도와줍니다.\u003c/li\u003e\n\u003cli\u003e해상도 접두사 튜닝: 다른 주파수(예: 주간, 일별 데이터)는 전통적인 시계열 모델에 어려운 부분입니다. TTM은 시계열 주파수를 인코딩하기 위한 추가 임베딩 레이어를 사용하여 모델이 신호의 주파수에 따라 정확하게 예측을 조건부로 설정할 수 있도록 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003eTiny Time Mixers — 아키텍처\u003c/h1\u003e\n\u003cp\u003eTSMixer은 TTM의 전신입니다. TSMixer는 견고한 모델이지만, 기본 모델로 사용하거나 외부 변수를 처리하는 데 사용할 수는 없습니다.\u003c/p\u003e\n\u003cp\u003eTTM은 TSMixer를 구성 요소로 사용하여 새로운 기능을 도입함으로써, 저자들이 보지 못한 데이터에 대해 일반화된 비-트랜스포머 모델을 만들었습니다.\u003c/p\u003e\n\u003cp\u003eTTM의 아키텍처는 그림 1에 나와 있습니다. 우리는 두 단계, 사전 훈련(왼쪽)과 파인튜닝(오른쪽)에 대해 설명할 것입니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_1.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e의미론적 지식\u003c/strong\u003e: sl=context_size, fl=forecasting_length, c = 입력 기능의 채널 수, c’= 예측 채널의 수.\u003c/p\u003e\n\u003ch2\u003e사전 훈련\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e사전 훈련 중에는 모델이 단변량 시계열로만 학습됩니다.\u003c/li\u003e\n\u003cli\u003e먼저 개별 시계열을 정규화합니다. 마지막 출력은 역정규화됩니다 (표준적인 방법).\u003c/li\u003e\n\u003cli\u003e패칭은 시계열에서 널리 성공한 기술이며 여기서도 사용됩니다. 단변량 시퀀스를 크기가 pl인 n 패치로 나눕니다.\u003c/li\u003e\n\u003cli\u003eTTM 백본 모듈은 적응형 패칭을 적용하고 패치를 크기 p에서 hf로 사상합니다. TTM 백본은 TTM의 핵심이며 나중에 자세히 설명하겠습니다.\u003c/li\u003e\n\u003cli\u003eTTM 디코더는 TTM 백본과 동일한 아키텍처를 갖고 있지만 훨씬 작아서 매개변수가 80% 적습니다.\u003c/li\u003e\n\u003cli\u003e예측 선형 헤드에는 1개의 완전 연결 계층이 있으며 최종 예측을 생성합니다 (그런 다음 역정규화됨).\u003c/li\u003e\n\u003cli\u003eMSE 손실은 예측 기간 fl 동안 계산됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003eFeat-Tuning\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e여기서는 TTM 백본이 동결되어 있고 TTM 디코더 및 Forecast 선형 헤드의 가중치만 업데이트됩니다.\u003c/li\u003e\n\u003cli\u003e우리는 소수 데이터만으로 학습하는 후속 예측(few-shot forecasting) 또는 전체 데이터셋을 사용하는 후속 예측(full-shot forecasting)을 수행할 수 있습니다.\u003c/li\u003e\n\u003cli\u003eFeat-Tuning 단계에서는 다변량 데이터셋을 사용할 수 있습니다. 이 경우 TTM 디코더에서 채널 혼합이 활성화됩니다.\u003c/li\u003e\n\u003cli\u003e선택적으로, 미래의 알려진 변수를 모델링하기 위해 외생 혼합 블록(그림 1에 나와 있음)을 활성화할 수도 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eTTM 백본\u003c/h1\u003e\n\u003cp\u003eTTM의 핵심 구성 요소는 TTM 백본입니다. 이는 Resolution Prefix Tuning과 Adaptive Patching을 가능하게 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 컴포넌트를 자세히 살펴보자면 그 기능을 이해할 수 있어요 (그림 2에 표시됨):\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e임베딩 레이어는 크기 pl에서 패치를 투영하여 크기 hf의 입력 임베딩을 만든답니다.\u003c/li\u003e\n\u003cli\u003eResolution Prefix Tuning 모듈은 시간-주파수/해상도를 나타내는 hf 크기의 임베딩을 만들고 이를 입력 임베딩에 연결합니다 (그림 2의 n=n+1 연산을 주목해주세요).\u003c/li\u003e\n\u003cli\u003eTTM 블록은 3개의 하위 모듈을 포함합니다: 패치 분할 모듈, 베니라 TSMixer 블록 및 패치 병합 블록:\u003c/li\u003e\n\u003cli\u003e패치 분할 모듈은 패치 수를 K만큼 증가시키고 패치 길이를 다시 K만큼 감소시킵니다. 예를 들어, 첫 번째 수준에서 크기 [c,n, hf]의 입력은 [c, 4*n, hf//4]로 변화합니다.\u003c/li\u003e\n\u003cli\u003eTSMixer 블록이 변환된 입력에 적용되며 패치 병합 블록이 [c, 4*n, hf//4] 입력을 다시 [c,n, hf]로 변형합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e외부 믹서\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e미래의 알려진 변수가 있는 경우, Exogenous Mixer를 활성화할 수 있습니다. 이 모듈은 Figure 3에 표시되어 있으며, TTM 아키텍처에서의 위치는 Figure 1에 표시되어 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_3.png\" alt=\"Exogenous Mixer\"\u003e\u003c/p\u003e\n\u003cp\u003eExogenous Mixer 블록은 간단합니다: 시계열의 미래 값(y3와 y4; Figure 3, 녹색)이 알려진 경우, 이를 사용하여 대상 변수(y1과 y2; Figure 4, 보라색)의 예측을 안내합니다.\u003c/p\u003e\n\u003ch1\u003eTTM 교육 세부 정보 및 데이터세트\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e저자들은 다양한 문맥과 예측 길이에 대해 5가지 TTM 버전을 만들었습니다. 이는 (512,96), (512,192), (512, 336), (512,720), (96,24) 입니다.\u003c/p\u003e\n\u003cp\u003e교육에 관해서, 저자들은 모델 사전 훈련을 위해 Monash 데이터베이스의 하위 집합(244k 샘플)을 사용했고, 파인튜닝 성능을 평가하기 위해 Informer 데이터셋을 사용했습니다. 또한, 저자들은 외부 혼합기 블록의 효과를 평가하고 알려진 미래 변수를 추가함으로써 성능이 얼마나 향상되는지 조사하기 위해 다른 데이터셋을 사용했습니다.\u003c/p\u003e\n\u003cp\u003e이러한 데이터셋에 대해 더 자세한 내용은 원본 논문에서 확인할 수 있습니다. 아래는 (512,96) 변형을 위한 교육 하이퍼파라미터입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epl(패치 길이) = 64\u003c/li\u003e\n\u003cli\u003e백본 수준 수 = 6\u003c/li\u003e\n\u003cli\u003e각 수준 당 TTM 블록 수 = 2\u003c/li\u003e\n\u003cli\u003e배치 크기 = 3천\u003c/li\u003e\n\u003cli\u003e에폭 = 20\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e학습 및 파인 튜닝 구성에 대한 자세한 내용은 원 논문을 참조해 주세요.\u003c/p\u003e\n\u003ch1\u003e평가 벤치마크\u003c/h1\u003e\n\u003cp\u003eTTM 대 최신 기법 모델\u003c/p\u003e\n\u003cp\u003e그 다음, 저자들은 Zero-shot 및 5% Few-shot 버전의 TTM을 다른 최신 기법 모델과 비교했습니다. 이때 사용된 평가 메트릭은 MSE였습니다. 결과는 다음과 같은 표 1에서 확인할 수 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래는 너무 인상적입니다:\u003c/p\u003e\n\u003cp\u003e평균적으로, Few-shot TTM이 모든 다른 모델을 능가했습니다. 심지어 Zero-shot TTM이 일부 모델을 능가할 수 있었습니다! 기억하세요, Zero-shot TTM은 이러한 데이터에 대해 학습을 받지 않고 예측을 생성합니다.\u003c/p\u003e\n\u003cp\u003e또한 TTM은 작년에 소개된 새로운 기반 시계열 모델인 GPT4TS를 앞서 나갔습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eTMT 이외에도 다음으로 높은 순위의 모델은 GPT4TS, PatchTST 및 TSMixer입니다. 모두 패치(patching)를 활용합니다. 최근 시계열 예측 연구에서 패치(patching)가 매우 유익한 기술임이 입증되었습니다.\u003c/p\u003e\n\u003cp\u003eTTM 대 foundation 모델\u003c/p\u003e\n\u003cp\u003e저자들은 TTM을 독립적으로 평가하며 특히 GPT4TS와 LLMTime과 비교합니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLLMTime은 GPT-3과 LLaMa-2를 사용하여 시계열 예측을 위해 특정 수정을 가한 모델입니다.\u003c/li\u003e\n\u003cli\u003eGPT4TS는 다양한 작업(예측, 분류 등)을 위해 범용 시계열 모델이며, 기본 모델로 GPT-2를 사용합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e비교 결과는 표시됩니다. Table 2 (LLMTime)와 Table 3 (GPT4TS):\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_5.png\" alt=\"LLMTime\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_6.png\" alt=\"GPT4TS\"\u003e\u003c/p\u003e\n\u003cp\u003eLLMTime은 제로샷 예측 시나리오에서 평가되었고, GPT4TS는 퓨샷 예측기로 동작했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e두 가지 비교에서 TTM이 명확한 승자입니다.\u003c/li\u003e\n\u003cli\u003e게다가 TTM은 훨씬 빠르며 리소스를 상당히 적게 필요로 합니다. 이는 TTM이 GPT4TS와 같은 무거운 트랜스포머 계산을 사용하지 않기 때문에 예상된 결과입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e외생 변수의 효과성\u003c/h2\u003e\n\u003cp\u003e현대 실제 세계 데이터셋은 가능한 경우 외생 변수를 사용하므로 예측 애플리케이션에서 이를 활용하는 것이 합리적입니다.\u003c/p\u003e\n\u003cp\u003eTTM의 저자들은 이와 같은 변수를 사용함으로써 TTM이 어떻게 향상되는지 조사했습니다 (해당하는 경우). 구체적으로 제로샷 TTM, 일반 TTM 및 외생 변수를 사용하는 채널 혼합 (TTM-CM)을 비교했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그들은 TSMixer와 그 채널 혼합 변형을 평가했습니다. 결과는 다음과 같이 Table 4에 표시되어 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_7.png\" alt=\"Table 4\"\u003e\u003c/p\u003e\n\u003cp\u003e다시 한 번, 결과는 매우 흥미로워요: 먼저, TTM-CM이 1위를 차지하여 외생 변수가 모델에 도움이 되는 것을 의미합니다.\u003c/p\u003e\n\u003cp\u003e채널 혼합 속성을 사용하는 TSMixer 변형은 2위를 차지했습니다. 또한, 제로-샷 TTM이 최악의 성능을 보입니다. 보조 변수가 있는 경우 모델 성능을 향상시키는 데 사용되어야 함이 명백합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003eTiny Time Mixers 실무 활용\u003c/h1\u003e\n\u003cp\u003e모델 버전 512-96과 1024-96의 가중치를 HuggingFace에서 다운로드하여 다음과 같이 세밀 조정할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e!git clone \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/IBM/tsfm.git\u003c/span\u003e\n!pip install transformers\n!pip install datasets\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e pd\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.\u003cspan class=\"hljs-property\"\u003epyplot\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e tsfm_public.\u003cspan class=\"hljs-property\"\u003emodels\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003etinytimemixer\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eutils\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e (\n    count_parameters,\n    plot_preds,\n)\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e tsfm_public.\u003cspan class=\"hljs-property\"\u003emodels\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003etinytimemixer\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTinyTimeMixerForPrediction\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e tsfm_public.\u003cspan class=\"hljs-property\"\u003etoolkit\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ecallbacks\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTrackingCallback\u003c/span\u003e\n\nzeroshot_model = \u003cspan class=\"hljs-title class_\"\u003eTinyTimeMixerForPrediction\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"ibm/TTM\"\u003c/span\u003e, revision=\u003cspan class=\"hljs-string\"\u003e'main'\u003c/span\u003e)\nfinetune_forecast_model = \u003cspan class=\"hljs-title class_\"\u003eTinyTimeMixerForPrediction\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"ibm/TTM\"\u003c/span\u003e, revision=\u003cspan class=\"hljs-string\"\u003e'main'\u003c/span\u003e, head_dropout=\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e,dropout=\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e,loss=\u003cspan class=\"hljs-string\"\u003e\"mse\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e따라서 transformers 라이브러리의 익숙한 Trainer 모듈을 사용하여 TTM을 세밀 조정할 수 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003efinetune_forecast_trainer = \u003cspan class=\"hljs-title class_\"\u003eTrainer\u003c/span\u003e(\nmodel=finetune_forecast_model,\nargs=finetune_forecast_args,\ntrain_dataset=train_dataset,\neval_dataset=valid_dataset,\ncallbacks=[early_stopping_callback, tracking_callback],\noptimizers=(optimizer, scheduler))\n\n# \u003cspan class=\"hljs-title class_\"\u003eFine\u003c/span\u003e tune\nfinetune_forecast_trainer.\u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cimg src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_8.png\"\u003e\n\u003cp\u003epredictions_test = finetune_forecast_trainer.predict(test_dataset)\u003c/p\u003e\n\u003cp\u003e이후에는 사적 데이터셋을 통해 예측을 받은 후 결과를 플롯합니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래는 테이블 태그를 마크다운 형식으로 변경하도록 했습니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM_9.png\"\u003e\n\u003ch1\u003e마무리 말씀\u003c/h1\u003e\n\u003cp\u003eTiny Time Mixer (TTM)은 다른 접근 방식을 따른 혁신적인 모델로서, 더 작지만 효율적인 모델들을 위한 길을 열어두었습니다.\u003c/p\u003e\n\u003cp\u003e특히, TTM은 어텐션을 사용하지 않았고 여전히 강력한 시계열(Time Series) 기반 모델을 구축할 수 있다는 것을 입증했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e최초로 MLP만 사용한 메타러닝 기능을 갖춘 시계열 모델은 N-BEATS와 N-HITS였어요. 이 트렌드가 어떻게 이어지는지 한번 살펴봐요.\u003c/p\u003e\n\u003cp\u003e최근에는 NLP 모델에서도 이러한 트렌드를 관측하고 있어요. 우리는 Mamba(State Space) xLSTM(기반 RNN)과 Hyena(CNN 기반)을 보았는데, 이들은 언어 모델이지만 트랜스포머는 아니며 다양한 벤치마크에서 인상적인 결과를 얻고 있어요.\u003c/p\u003e\n\u003cp\u003e시계열 모델에 대한 이런 접근 방식이 어떻게 전개될지도 한번 살펴봅시다. 결국, 시계열에 대한 기초 모델 연구는 아직 새로운 상황이에요!\u003c/p\u003e\n\u003ch1\u003e읽어주셔서 감사합니다!\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e제 LinkedIn 팔로우해 주세요!\u003c/li\u003e\n\u003cli\u003e제 뉴스레터, AI Horizon Forecast를 구독해 주세요!\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e참고 자료\u003c/h2\u003e\n\u003cp\u003e[1] Ekambaram 등, Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series (2024년 4월)\u003c/p\u003e\n\u003cp\u003e[2] Ekambaram 등, TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting (2023년 6월)\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-TinyTimeMixersTTMAPowerfulZero-ShotForecastingModelbyIBM"},"buildId":"kkckKPyxcjJp_o8wb2unW","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>